{
  "name" : "2021.acl-long.60.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "From Discourse to Narrative: Knowledge Projection for Event Relation Extraction",
    "authors" : [ "Jialong Tang", "Hongyu Lin", "Meng Liao", "Yaojie Lu", "Xianpei Han", "Le Sun", "Weijian Xie", "Jin Xu" ],
    "emails" : [ "jialong2019@iscas.ac.cn", "hongyu@iscas.ac.cn", "xianpei@iscas.ac.cn", "sunle@iscas.ac.cn", "jinxxu}@tencent.com" ],
    "sections" : [ {
      "heading" : null,
      "text" : "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 732–742\nAugust 1–6, 2021. ©2021 Association for Computational Linguistics\n732"
    }, {
      "heading" : "1 Introduction",
      "text" : "Event-centric knowledge graphs (EventKGs) model the narratives of the world by representing events and identifying relations between them, which are critical for machine understanding and can benefit many downstream tasks, such as question answering (Costa et al., 2020), news reading (Vossen, 2018), commonsense knowledge acquisition (Zhang et al., 2020a) and so on.\nRecently, semi-automatically constructing EventKGs have gained much attention (Tandon et al., 2015; Rospocher et al., 2016; Gottschalk and Demidova, 2018; Zhang et al., 2020b). These methods extract event knowledge from massive raw corpora with or without little human intervention, which makes them scalable solutions to build large-scale\n∗Corresponding authors.\nEventKGs. Commonly, each node in EventKGs represents an event, and each edge represents a predefined relation between an event pair1. Currently, event relations are majorly extracted based on the explicit connectives between them. For example, in Figure 1, a Reason relation is extracted between E2: “PER orders two hamburgers” and E3: “PER is so hungry” using the explicit connective “because” between them.\nUnfortunately, the connective-based approaches face the critical coverage problem due to the sparsity of connectives. That is, a large proportion of event pairs are not connected with explicit connectives, but with underlying event relations. We denote them as implicit event relations. Further-\n1Computational and cognitive studies define nodes as eventualities, which include activities, states and events. In this paper, we simplify the definition of each node to “event” due to its popularity.\nmore, the related events can even not close to each other in a document. For the example in Figure 1, the implicit relation Reason between E1: “PER goes to the restaurant” and E3: “PER is so hungry” can not be extracted due to the absence of explicit connective as well as the discontinuity between these two clauses. The common practice in previous connective-based approaches is to ignore all these implicit instances (Zhang et al., 2020b). As a result, the coverage of EventKGs is significantly undermined. Besides, because the scale of the existed event relation corpus (Hong et al., 2016) is limited, it is also impractical to build effective event relation classifiers via supervised learning.\nIn this paper, we propose a new paradigm for event relation extraction — knowledge projection. Instead of relying on sparse connectives or building classifiers starting from scratch, we project discourse knowledge to event narratives by exploiting the anthropological linguistic connections between them. Enlightened by Livholts and Tamboukou (2015); Altshuler (2016); Reyes and Wortham (2017), discourses and narratives have significant associations, and their knowledge are shared at different levels: 1) token-level knowledge: discourses and narratives share similar lexical and syntactic structures, 2) semantic-level knowledge: the semantics entailed in discourse pairs and event pairs are analogical, e.g., E3-Reason→E1 and D3Cause→D1 in Figure 1., and 3) label-level knowledge: heterogeneous event and discourse relations have the same coarse categories, e.g., both the event relation Reason and the discourse relation Cause are included in the coarse-grained relation Contingency. By exploiting the rich knowledge in manually labelled discourse corpus and projecting them into event relation extraction models, the performance of event relation extraction can be significantly improved, and the data requirement can be dramatically reduced.\nSpecifically, we design Multi-tier Knowledge Projection Network (MKPNet), which can leverage multi-tier discourse knowledge effectively for event relation extraction. MKPNet introduces three kinds of adaptors to project knowledge from discourses into narratives: (a) token adaptor for tokenlevel knowledge projection; (b) semantic adaptor for semantic-level knowledge projection; (c) coarse category adaptor for label-level knowledge projection. By sharing the parameters of these three adaptors, the commonalities between discourses and\nnarratives at various levels can be effectively explored. Therefore, we can obtain more general token representations, more accurate semantic representations, and more credible coarse category representations to better predict event relations.\nWe conduct intrinsic experiments on ASER (Zhang et al., 2020b), one of the representative EventKGs, and extrinsic experiments on Winograd Scheme Challenge (WSC) (Levesque et al., 2012), one of the representative natural language understanding benchmarks. Intrinsic experimental results show that the proposed MKPNet significantly outperforms the state-of-the-art (SoA) baselines, and extrinsic experimental results verify the value of the extracted event relations2.\nThe main contributions of this paper are:\n• We propose a new knowledge projection paradigm, which can effectively leverage the commonalities between discourses and narratives for event relation extraction.\n• We design MKPNet, which can effectively leverage multi-tier discourse knowledge for event relation extraction via token adaptor, semantic adaptor and coarse category adaptor.\n• Our method achieves the new SotAevent relation extraction performance, and an enriched EventKG is released by extracting both explicit and implicit event relations. We believe it can benefit many downstream NLP tasks."
    }, {
      "heading" : "2 Background",
      "text" : "Event Relation Extraction (ERE). Given an existing EventKG G = {E ,R}, where nodes E are events and edges R are their relations. Y ex ∈ R are explicit event relations extracted by connectivebased methods, and Y im /∈ R are implicit event relations without connectives. Commonly, implicit event relation extraction (IERE) takes two events E1 = {e11, ..., e1|E1|}, E\n2 = {e21, ..., e2|E2|}, E1, E2 ∈ E as inputs, then uses a neural network to classify their underlying relation.\nDiscourse Relation Recognition (DRR). DRR aims to recognize the relation of two discourse arguments. Discourse relations can be explicit or implicit, where explicit relations are revealed by\n2Our source codes with corresponding experimental datasets and the enhanced EventKG are openly available at https://github.com/TangJiaLong/ Knowledge-Projection-for-ERE.\nconnectives, while implicit relations lack these surface cues. To resolve the implicit discourse relation recognition (IDRR) task, researchers construct high-quality labelled datasets (Prasad et al., 2008) and design elaborate models (Zhang et al., 2016b; Bai and Zhao, 2018; Kishimoto et al., 2020).\nAssociations between Discourse and Narrative. Recent NLP studies have proved that discourse and narratives closely interact with each other, and leveraging discourse knowledge benefits narrative analysis significantly, such as subevents detection (Aldawsari and Finlayson, 2019) and main event relevant identification (Choubey et al., 2020). Motivated by the above observation, this paper leverages the knowledge of discourse by a knowledge projection paradigm. Blessed with the associations at token-, semantic- and coarse category-levels, the discourse corpora and knowledge can be effectively exploited for event relation extraction."
    }, {
      "heading" : "3 Multi-tier Knowledge Projection Network for Event Relation Extraction",
      "text" : "In this section, we describe how to learn an effective event relation extractor by projecting resourcerich discourse knowledge to the resource-poor narrative task. Specifically, we propose Multi-tier Knowledge Projection Network (MKPNet) which can effectively leverage multi-tier discourse knowledge for implicit event relation extraction. Figure 2 shows an overview of MKPNet, which uses token adaptor, semantic adaptor and coarse category adaptor to fully exploit discourse knowledge at different levels. In the following, we first describe the neural architecture of MKPNet and then describe the details of three adaptors."
    }, {
      "heading" : "3.1 Neural Architecture of MKPNet",
      "text" : "For knowledge projection, we model both event relation extraction (ERE) and discourse relation recognition (DRR) as an instance-pair classification task (Devlin et al., 2019; Kishimoto et al., 2020). For ERE, the input is an event pair such as <E1: “PER goes to the restaurant”, E3: “PER is so hungry”> and the output is an event relation such as Reason. For DRR, the input is a clause pair such as <D1: “Tom goes to the restaurant”, D3:“he is so hungry”> and the output is a discourse relation such as Cause.\nSpecifically, MKPNet extends the SotADRR model — BERT-CLS (Kishimoto et al., 2020) by the VAE-based semantic encoder and the coarse category encoder to model knowledge tier-by-tier (Pan et al., 2016; Guo et al., 2019; Kang et al., 2020; Li et al., 2020b). It 1) first utilizes the BERT-based token encoder to encodes an instance pair as a token representation h[CLS]; 2) then obtains the semantic representation hz via a VAE-based semantic encoder; 3) predicts the coarse-grained label and embeddings it as the coarse category representation hY c ; 4) finally classifies its relation with the guidance of the aggregate instance-pair representation:\nY = ClassifierFine([h[CLS] ⊕ hz ⊕ hY c ]) (1)\nwhere ⊕ means the concatenation operation. In this way, the parameters of MKPNet can be grouped by {θBERT , θSemantic, θCoarse, θFine}, where θBERT for BERT-based token encoder, θSemantic for VAE-based semantic encoder, θCoarse for coarse category encoder and θFine for the final relation classifier layer respectively."
    }, {
      "heading" : "3.2 Token Adaptor",
      "text" : "Recent studies have shown that similar tasks usually share similar lexical and syntactic structures and therefore lead to similar token representations (Pennington et al., 2014; Peters et al., 2018). The token adaptor tries to improve the token encoding for ERE by sharing the parameters θBERT of the BERT-based encoders with DRR. In this way, the encoder is more effective due to the more supervision signals and is more general due to the multi-task settings.\nSpecifically, given an event pair <E1, E2>, we represent it as a sequence:\n[CLS], e11, ..., e 1 |E1|, [SEP ], e 2 1, ..., e 2 |E2|, [SEP ]\nwhere[CLS] and [SEP] are special tokens. For each token in the input, its representation is constructed by concatenating the corresponding token, segment and position embeddings. Then, the event pair representation will be inputted into BERT architecture (Devlin et al., 2019) and updated by multilayer Transformer blocks (Vaswani et al., 2017). Finally, we obtain the hidden state corresponding to the special [CLS] token in the last layer as the token-level event pair representation:\nhe[CLS] = BERT (E 1, E2) (2)\nThe token-level discourse pair representation hd[CLS] can be obtained in the same way for DRR.\nTo project the token-level knowledge, we use the same BERT for event pair and discourse pair encoding. During the optimization process, it is fine-tuned using the supervision signals from both ERE and DRR."
    }, {
      "heading" : "3.3 Semantic Adaptor",
      "text" : "Because narrative and discourse analyses need to accurately represent the deeper semantic of the instance pairs, the shallow token-level knowledge captured by the BERT-based token encoder is not enough. However, BERT always induces a nonsmooth anisotropic semantic space which is adverse for semantic modelling of large-grained linguistic units (Li et al., 2020a).\nTo address this issue, we introduce an variational autoencoder-based (VAE-based) semantic encoder to represent the semantics of both events and clauses by transforming the anisotropic semantic distribution to a smooth and isotropic Gaussian distribution (Kingma and Welling, 2014; Rezende\net al., 2014; Sohn et al., 2015). To better learn the semantic encoder, the semantic adaptor shares the parameters θSemantic of it between ERE and DRR and train it using both classification supervision signals and KL divergence.\nSpecifically, VAE is a directed graphical model with the generative model P and the variational model Q, which learns the semantic representation hz of the input by an autoencoder framework. Figure 3 illustrates the graphic representation of the semantic encoder. Specifically, we assume that there exists a continuous latent variable hz ∼ N (µ, diag(σ2)), where µ and σ2 are mean and variance of the Gaussian distribution respectively. With this assumption, the original conditional probability of the event/discourse relations can be expressed by the following formula:\np(hY |h[CLS]) = ∫ hz p(hY |h[CLS], hz)\np(hz|h[CLS])dhz (3)\nThe posterior approximation is q(hz|h[CLS], hY ), where h[CLS] can be he[CLS] or hd[CLS] and hY can be h e Y or h d Y according to the different tasks. We 1) first obtain the inputand output-side representations via the shared BERT-based token encoder and the individual relation embedding networks, i.e., h[CLS] and hY ; 2) then perform a non-linear transformation that project them onto the semantic space:\nh′z = tanh(Wz[h[CLS];hY ] + bz) (4)\n3) obtain the above-mentioned Gaussian parameters µ and logσ2 through linear regression:\nµ =Wµh ′ z + bµ, logσ 2 =Wσh ′ z + bσ (5)\nwhere W and b are the parameter matrix and bias term respectively; 4) use a reparameteriza-\ntion trick (Kingma and Welling, 2014; Sohn et al., 2015) to get the final semantic representation:\nhz = µ+ σ (6)\nwhere ∼ N (0, I) and hz can be hez or hdz . The neural model for the prior p(hz|h[CLS]) is the same as that for the posterior q(hz|h[CLS], hY ), except for the absence of hY . Besides, those two models have parameters independent of each other.\nDuring testing, due to the absence of the outputside representation hY , we set hz to be the mean of p(hz|h[CLS]) (Zhang et al., 2016a), i.e., µ. During training, we minimize the Kullback-Leibler divergence KL(P ||Q) between the generation model P and the inference model Q. Intuitively, KL divergence connects these two models:\nKL(q(hz|h[CLS], hY )||p(hz|h[CLS])) (7)\nTo project the semantic-level knowledge, we use the same VAE for both event pair and discourse pair. Therefore, the commonalities of event semantics and discourse semantics can be captured more accurately."
    }, {
      "heading" : "3.4 Coarse Category Adaptor",
      "text" : "The token adaptor and the semantic adaptor commendably cover the knowledge entailed on the input-side. In addition, we found that ERE and DRR share the same coarse-grained categories: Temporal, Contingency, Comparison and Expansion (Prasad et al., 2008; Zhang et al., 2020b), although they have different fine-grained categories.\nTo this end, we design the coarse category adaptor in a coarse-to-fine framework (Petrov, 2009) to bridge the gap between the heterogeneous finegrained targets. Specifically, we share the parameters θCoarse of the coarse-grained classifier and the coarse label embedding network to obtain more credible coarse category representations.\nSpecifically, we first use the token representation h[CLS] and the semantic representation hz to predict the coarse-grained labels:\nY c = ClassifierCoarse(h[CLS], hz) (8)\nwhere Y c ∈ {Temporal, Contingency, Comparison, Expansion}. After that, we use the coarse label embedding network to obtain the corresponding coarse-grained label embedding hY c , which is referred as the coarse category representation.\nTo project that label-level knowledge, we use the same coarse-grained classifier and the same coarse\nlabel embedding network. During the optimization process, both event instances and discourse instances can be used to train this coarse category encoder. The more supervision signals make it more effective."
    }, {
      "heading" : "3.5 Full Model Training",
      "text" : "In this paper, we utilize multi-task learning (Caruana, 1997) to implement the knowledge projection from discourse to narrative. It expects correlative tasks (ERE and DRR) can help each other to learn better by sharing the parameters of three adaptors. Given ERE and DRR training datasets, an alternate optimization approach (Dong et al., 2015) is used to optimizate MKPNet:\nL(θ) =α(L(θ;Y ) + λKL(P ||Q)) + (1− α)L(θ;Y c)\n(9)\nwhere Y can be Y im or Y d according to the different tasks, λ, α are two hyperparameters, KL(P ||Q)) is the KL divergence in the semantic encoder, L(θ;Y ) and L(θ;Y c) are fine-grained and coarse-grained objectives respectively:\nL(θ;Y ) = log p(Y |h[CLS], hz, hY c) (10) L(θ;Y c) = log p(Y c|h[CLS], hz) (11)\nIt should be noticed that in MKPNet, {θBERT , θSemantic, θCoarse} are the shared parameters of the BERT-based token encoder, the VAE-based semantic encoder and the coarse category encoder between ERE and DRR. And {θFine} are separated parameters of the fine-grained ERE and DRR classifiers."
    }, {
      "heading" : "4 Experiments",
      "text" : "We conduct intrinsic experiments on ASER (Zhang et al., 2020b) to assess the effectiveness of the proposed MKPNet, and extrinsic experiments on WSC (Levesque et al., 2012) to verify the value of the extracted event relations."
    }, {
      "heading" : "4.1 Intrinsic Experiments",
      "text" : "Datasets. For discourse relation recognition (DRR), we use PDTB 2.0 (Prasad et al., 2008) with the same splits of Ji and Eisenstein (2015): sections 2-20/0-1/21-22 respectively for train/dev/test. For event relation extraction (ERE), because there is no labelled training corpus, we construct a new dataset by removing the connectives of the explicit event\nrelation instances in ASER core version3 and retaining at most 2200 instances with the highest confidence scores for each category4. In this way, we obtain 23,181/1400/1400 train/dev/test instances – we denoted it as implicit event relation extraction (IERE) dataset.\nImplementation. We implement our model based on pytorch-transformers (Wolf et al., 2020). We use BERT-base and set all hyper-parameters using the default settings of the SotADRR model (Kishimoto et al., 2020).\nBaselines. For ERE, we compare the proposed MKPNet with the following baselines:\n• Baselines w/o Discourse Knowledge are only trained on IERE training set. We choose the BERT-CLS as the representative of them due to its SotAperformance.\n• Baselines with Discourse Knowledge improve the learning of ERE via transfer learning (Pan and Yang, 2009; Pan et al., 2010) from discourse models, i.e., first pre-train a parameter prior on PDTB 2.0 and then fine-tune it on IERE –– we denote it as BERT-Transfer.\nFor DRR, we compare the proposed MKPNet with the following baselines:"
    }, {
      "heading" : "4.1.1 Overall Results",
      "text" : "1. Based on MKPNet, we enrich the original ASER by abundant implicit event relations. Considering the computational complexity, we classify the event pairs co-occurrence in the same document\n3https://hkust-knowcomp.github.io/ASER 4Higher confidence score means more credible instance.\nand filter them by confidence scores. Specifically, we compute the confidence score by multiplying the classification probability and the frequency of the event pair. Integrating with the original explicit event relations, we can obtain the enriched EventKGs ASER++ (core/high/full) with the different threshold confidences (3/2/1). Table 1 shows that when compared with existing event-related resources, ASER++ has an overwhelming advantage in the number of event relations.\n2. The proposed MKPNet achieves SotAperformance for ERE. MKPNet can significantly outperform the BERT-Transfer and achieves 55.86 accuracy and 55.36 F1. MKPNet w/o KP obtains considerable performance improvements when com-\npared with BERT-CLS. We believe this is because MKPNet fully explores the knowledge on different tiers, and modelling knowledge tier-by-tier is effective.\n3. By projecting knowledge at token-level, semantic level and label level, all three adaptors are useful and are complementary with each other. When compared with the full model MKPNet, its four variants show declined performance in different degrees. MKPNet outperforms MKPNet w/o CA 0.72 accuracy and 0.94 F1, which indicates that our coarse category adaptor successfully bridges the gap of heterogeneous fine-grained targets. MKPNet outperforms MKPNet w/o SA 0.57 accuracy and 0.44 F1, and therefore we believe that our latent semantic adaptor is helpful for capture the semantic-level commonalities. Finally, there is a significant decline between MKPNet w/o KP and MKPNet w/o SA & CA, which means that token adaptor is indispensable. The insight in those observations is that the commonalities between discourses and narratives under the hierarchical structure, thus projecting them at different levels is effective, and three adaptors can be complementary with each other.\n4. The commonalities between discourses and narratives are beneficial for both ERE and DRR. Compared with the baselines w/o discourse knowledge — BERT-CLS and MKPNet w/o KP, both the naive transfer method — BERT-Transfer and our MKPNet achieve significant performance improvements: BERT-Transfer gains 1.29 accuracy and 1.20 F1 when compared to BERT-CLS, and MKPNet gains 1.92 accuracy and 1.84 F1 when compared to MKPNet w/o KP. Besides, for DRR, our method MKPNet also substantially outperforms the other baselines and its variant MKPNet w/o KP. These results verified the commonalities between discourse knowledge and narrative knowledge.\n4.1.2 Detailed Analysis Effects of Semantic-level Knowledge and Labellevel Knowledge. In these experiments, we compare the performance of our models, MKPNet, MKPNet w/o CA and MKPNet w/o SA with or without knowledge projection to find out the effects of semantic-level knowledge and label-level knowledge. From Table 4, we can see that: (1) Compared with their counterparts, MKPNet, MKPNet w/o CA and MKPNet w/o SA with knowledge projection lead to significant improvements. Thus, it is convincing that the performance improvements\nmainly come from the discourse knowledge rather than the neural architecture; (2) Current knowledge projection can be further improved by exploiting more accurate discourse knowledge: MKPNet w/o SA*, which uses golden coarse categories, achieves striking performance (Acc 70.50; F1 70.32).\nTradeoff between Dataset Quality and Size. As described above, the IERE training dataset is constructed using the most confident instances in ASER core version. We can construct a larger but lower quality dataset by incorporating more instances with lower confidence, i.e., the quality-size tradeoff problem. To analyze the tradeoff between the quality and size, we construct a set of datasets with different sizes/qualities, and Figure 4 shows the corresponding results of MKPNet on the development set. We can see that the size is the main factors for performance improvements at the beginning: every 5,000 additional instances can result in a significant improvement (about 2 to 3 F1 gain). When the size is large (more than 20,000 instances in our experiments), more instances will not result in performance improvements, and the low-quality instances will hurt the performance."
    }, {
      "heading" : "4.2 Extrinsic Experiments",
      "text" : "The above intrinsic experiments verified the effectiveness of the proposed MKPNet for ERE. In this section, we use the core version of our enriched EventKGs — ASER++, and then conduct extrinsic experiments on Winograd Schema Challenge (WSC) (Levesque et al., 2012) to verify the effect of ASET++.\nWSC Implementation. WSC is challenging since its schema is a pair of sentences that differ only in one or two words and that contain a referential ambiguity that is resolved in opposite directions in the two sentences. According to Certu et al. (2019), fine-tuning pre-trained language models on WSC-schema style training sets is a robust method to tackle WSC. Therefore, as Figure 5 shows, we transform ASER++ to WSC-schema style training data in the same way as Zhang et al. (2020b) and fine-tune BERT on it, which we refer to as BERT (ASER++). We compare BERT (ASER++) with these baselines:\n• Pure Knowledge-based Methods are heuristical rule-based methods, such as Knowledge Hunting (Emami et al., 2018) and String Match (Zhang et al., 2020b).\n• Language Model-based Methods use language model trained on large-scale corpus and tuned specifically for the WSC task, such as LM (Trinh and Le, 2018).\n• External Knowledge Enhanced Methods are models based on BERT and trained with the different external knowledge resource, e.g., WscR (Ng, 2012; Certu et al., 2019)\nWe implement our model based on pytorchtransformers (Wolf et al., 2020). BERT-large is used. All hyper-parameters are default settings as Certu et al. (2019).\nExtrinsic Results. Table 5 shows the overall results of extrinsic experiments. We can see that: By fine-tuning BERT on our enriched EventKG — ASER++, the WSC performance can be significantly improved. BERT (ASER++) and BERT (ASER++ & WscR) outperform BERT (ASER) and BERT (ASER & WscR) respectively, which verified the effectiveness of ASER++ and implicit event relations are beneficial for downstream NLU tasks."
    }, {
      "heading" : "5 Related Work",
      "text" : "Event-centric Knowledge Graphs. Knowledge graphs have come from entity-centric ones (Banko et al., 2007; Suchanek et al., 2007; Bollacker et al., 2008; Wu et al., 2012) to event-centric ones. However, the construction of traditional KGs takes domain experts much effort and time, which are often with limited size and cannot effectively resolve realworld applications, e.g., FrameNet (Baker et al., 1998). Recently, many modern and large-scale KGs have been built semi-automatically, which focus on events (Tandon et al., 2015; Rospocher et al., 2016; Gottschalk and Demidova, 2018; Zhang et al., 2020b) and commonsense (Speer et al., 2017; Smith et al., 2018; Huang et al., 2018; Sap et al., 2019). Specifically, Yu et al. (2020) proposes an approach to extract entailment relations between eventualities, e.g., “I eat an apple” entails “I eat fruit”, and release an event entailment graph (EEG). Different from EEG, this paper focuses on implicit event relations which are not extracted due to the absences of the connectives and discontinuity.\nKnowledge Transfer. Due to the data scarcity problem, many knowledge transfer studies have been proposed, including multi-task learning (Caru-\nana, 1997), transfer learning (Pan and Yang, 2009; Pan et al., 2010), and knowledge distillation (Hinton et al., 2014). Recently, researchers are interested in training/sharing/transferring/distilling models layer by layer to fully excavate the knowledge (Pan et al., 2016; Guo et al., 2019; Kang et al., 2020; Li et al., 2020b). In this paper, we propose a knowledge projection method which can project discourse knowledge to narraties on different tiers."
    }, {
      "heading" : "6 Conclusions",
      "text" : "In this paper, we propose a knowledge projection paradigm for event relation extraction and Multitier Knowledge Projection Network (MKPNet) is designed to leverage multi-tier discourse knowledge. By effectively projecting knowledge from discourses to narratives, MKPNet achieves the new state-of-the-art event relation extraction performance, and extrinsic experimental results verify the value of the extracted event relations. For future work, we want to design new data-efficient algorithms to learn effective models using low-quality and heterogeneous knowledge."
    }, {
      "heading" : "Acknowledgments",
      "text" : "This work is supported by the Strategic Priority Research Program of Chinese Academy of Sciences, Grant No. XDA27020200, the National Natural Science Foundation of China under Grants no. U1936207, and in part by the Youth Innovation Promotion Association CAS (2018141)."
    } ],
    "references" : [ {
      "title" : "Detecting subevents using discourse and narrative features",
      "author" : [ "Mohammed Aldawsari", "Mark Finlayson." ],
      "venue" : "Proceedings of ACL 2019.",
      "citeRegEx" : "Aldawsari and Finlayson.,? 2019",
      "shortCiteRegEx" : "Aldawsari and Finlayson.",
      "year" : 2019
    }, {
      "title" : "Events, States and Times: An Essay on Narrative Discourse in English",
      "author" : [ "Daniel Altshuler." ],
      "venue" : "Walter de Gruyter GmbH & Co KG.",
      "citeRegEx" : "Altshuler.,? 2016",
      "shortCiteRegEx" : "Altshuler.",
      "year" : 2016
    }, {
      "title" : "Deep enhanced representation for implicit discourse relation cecognition",
      "author" : [ "Hongxiao Bai", "Hai Zhao." ],
      "venue" : "Proceedings of ICCL 2018.",
      "citeRegEx" : "Bai and Zhao.,? 2018",
      "shortCiteRegEx" : "Bai and Zhao.",
      "year" : 2018
    }, {
      "title" : "The berkeley framenet project",
      "author" : [ "Collin F. Baker", "Charles J. Fillmore", "John B. Lowe." ],
      "venue" : "Proceedings of COLING-ACL 1998.",
      "citeRegEx" : "Baker et al\\.,? 1998",
      "shortCiteRegEx" : "Baker et al\\.",
      "year" : 1998
    }, {
      "title" : "Open information extraction from the web",
      "author" : [ "Michele Banko", "Michael J. Cafarella", "Stephen Soderland", "Matthew Broadhead", "Oren Etzioni." ],
      "venue" : "Proceedings of IJCAI 2007.",
      "citeRegEx" : "Banko et al\\.,? 2007",
      "shortCiteRegEx" : "Banko et al\\.",
      "year" : 2007
    }, {
      "title" : "Freebase: A collaboratively created graph database for structuring human knowledge",
      "author" : [ "Kurt D. Bollacker", "Colin Evans", "Praveen Paritosh", "Tim Sturge", "Jamie Taylor." ],
      "venue" : "Proceedings of SIGMOD 2008.",
      "citeRegEx" : "Bollacker et al\\.,? 2008",
      "shortCiteRegEx" : "Bollacker et al\\.",
      "year" : 2008
    }, {
      "title" : "Multitask learning",
      "author" : [ "Rich Caruana." ],
      "venue" : "Machine learning, 28(1).",
      "citeRegEx" : "Caruana.,? 1997",
      "shortCiteRegEx" : "Caruana.",
      "year" : 1997
    }, {
      "title" : "A surprisingly robust trick for the winograd schema challenge",
      "author" : [ "Vid Kocijanand Ana-Maria Certu", "Oana-Maria Camburu", "Yordan Yordanov", "Thomas Lukasiewicz." ],
      "venue" : "Proceedings of ACL 2019, pages 4837–4842.",
      "citeRegEx" : "Certu et al\\.,? 2019",
      "shortCiteRegEx" : "Certu et al\\.",
      "year" : 2019
    }, {
      "title" : "Discourse as a function of event: Profiling discourse structure in news articles around the main event",
      "author" : [ "Prafulla Kumar Choubey", "Aaron Lee", "Ruihong Huang", "Lu Wang." ],
      "venue" : "Proceedings of ACL 2020.",
      "citeRegEx" : "Choubey et al\\.,? 2020",
      "shortCiteRegEx" : "Choubey et al\\.",
      "year" : 2020
    }, {
      "title" : "Event-qa: A dataset for eventcentric question answering over knowledge graphs. arXiv preprint arXiv:2004.11861",
      "author" : [ "Tarcı́sio Souza Costa", "Simon Gottschalk", "Elena Demidova" ],
      "venue" : null,
      "citeRegEx" : "Costa et al\\.,? \\Q2020\\E",
      "shortCiteRegEx" : "Costa et al\\.",
      "year" : 2020
    }, {
      "title" : "Bert: Pre-training of deep bidirectional transformers for language understanding",
      "author" : [ "Jacob Devlin", "Ming-Wei Chang", "Kenton Lee", "Kristina Toutanova." ],
      "venue" : "Proceedings of NAACL 2019.",
      "citeRegEx" : "Devlin et al\\.,? 2019",
      "shortCiteRegEx" : "Devlin et al\\.",
      "year" : 2019
    }, {
      "title" : "Multi-task learning for multiple language translation",
      "author" : [ "Daxiang Dong", "Hua Wu", "Wei He", "Dianhai Yu", "Haifeng Wang." ],
      "venue" : "Proceedings of ACLIJCNLP 2015.",
      "citeRegEx" : "Dong et al\\.,? 2015",
      "shortCiteRegEx" : "Dong et al\\.",
      "year" : 2015
    }, {
      "title" : "A knowledge hunting framework for common sense reasoning",
      "author" : [ "Ali Emami", "Noelia De La Cruz", "Adam Trischler", "Kaheer Suleman", "Jackie Chi Kit Cheung." ],
      "venue" : "Proceedings of EMNLP 2018, pages 1949–1958.",
      "citeRegEx" : "Emami et al\\.,? 2018",
      "shortCiteRegEx" : "Emami et al\\.",
      "year" : 2018
    }, {
      "title" : "Eventkg: A multilingual event-centric temporal knowledge graph",
      "author" : [ "Simon Gottschalk", "Elena Demidova." ],
      "venue" : "Proceedings of ESWC 2018.",
      "citeRegEx" : "Gottschalk and Demidova.,? 2018",
      "shortCiteRegEx" : "Gottschalk and Demidova.",
      "year" : 2018
    }, {
      "title" : "Spottune: Transfer learning through adaptive finetuning",
      "author" : [ "Yunhui Guo", "Honghui Shi", "Abhishek Kumar", "Kristen Grauman", "Tajana Rosing", "Rogerio Feris." ],
      "venue" : "Proceedings of CVPR 2019.",
      "citeRegEx" : "Guo et al\\.,? 2019",
      "shortCiteRegEx" : "Guo et al\\.",
      "year" : 2019
    }, {
      "title" : "Distilling the knowledge in a neural network",
      "author" : [ "Geoffrey Hinton", "Oriol Vinyals", "Jeff Dea." ],
      "venue" : "Proceedings of The Workshop on NIPs 2014.",
      "citeRegEx" : "Hinton et al\\.,? 2014",
      "shortCiteRegEx" : "Hinton et al\\.",
      "year" : 2014
    }, {
      "title" : "Building a cross-document event-event relation corpus",
      "author" : [ "Yu Hong", "Tongtao Zhang", "Tim O’Gorman", "Sharone Horowit-Hendler", "Heng Ji", "Martha Palmer" ],
      "venue" : "In Proceedings of ACL 2016",
      "citeRegEx" : "Hong et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Hong et al\\.",
      "year" : 2016
    }, {
      "title" : "Tracking state changes in procedural text: A challenge dataset and models for process paragraph comprehension",
      "author" : [ "Bhavana Dalviand Lifu Huang", "Niket Tandon", "Wen tau Yih", "Peter Clark." ],
      "venue" : "Proceedings of NAACL-HLT 2018.",
      "citeRegEx" : "Huang et al\\.,? 2018",
      "shortCiteRegEx" : "Huang et al\\.",
      "year" : 2018
    }, {
      "title" : "One vector is not enough: Entity-augmented distributed semantics for discourse relations",
      "author" : [ "Yangfeng Ji", "Jacob Eisenstein." ],
      "venue" : "Proceedings of TACL 2015.",
      "citeRegEx" : "Ji and Eisenstein.,? 2015",
      "shortCiteRegEx" : "Ji and Eisenstein.",
      "year" : 2015
    }, {
      "title" : "Decoupling representation and classifier for long-tailed recognition",
      "author" : [ "Bingyi Kang", "Saining Xie", "Marcus Rohrbach", "Zhicheng Yan", "Albert Gordo", "Jiashi Feng", "Yannis Kalantidis." ],
      "venue" : "Proceedings of ICLR 2020.",
      "citeRegEx" : "Kang et al\\.,? 2020",
      "shortCiteRegEx" : "Kang et al\\.",
      "year" : 2020
    }, {
      "title" : "Auto-encoding variational bayes",
      "author" : [ "Kingma", "Welling." ],
      "venue" : "Proceedings of ICLR 2014.",
      "citeRegEx" : "Kingma and Welling.,? 2014",
      "shortCiteRegEx" : "Kingma and Welling.",
      "year" : 2014
    }, {
      "title" : "Adapting bert to implicit discourse relation classification with a focus on discourse connectives",
      "author" : [ "Yudai Kishimoto", "Yugo Murawaki", "Sadao Kurohashi." ],
      "venue" : "Proceedings of LREC 2020.",
      "citeRegEx" : "Kishimoto et al\\.,? 2020",
      "shortCiteRegEx" : "Kishimoto et al\\.",
      "year" : 2020
    }, {
      "title" : "The winograd schema challenge",
      "author" : [ "Hector Levesque", "Ernest Davis", "Leora Morgenstern." ],
      "venue" : "Proceedings of KR 2012.",
      "citeRegEx" : "Levesque et al\\.,? 2012",
      "shortCiteRegEx" : "Levesque et al\\.",
      "year" : 2012
    }, {
      "title" : "On the sentence embeddings from pre-trained language models",
      "author" : [ "Bohan Li", "Hao Zhou", "Junxian He", "Mingxuan Wang", "Yiming Yang", "Lei Li." ],
      "venue" : "Proceedings of EMNLP 2020.",
      "citeRegEx" : "Li et al\\.,? 2020a",
      "shortCiteRegEx" : "Li et al\\.",
      "year" : 2020
    }, {
      "title" : "Bertemd: Many-to-many layer mapping for bert compression with earth mover’s distance",
      "author" : [ "Jianquan Li", "Xiaokang Liu", "Honghong Zhao", "Ruifeng Xu", "Min Yang", "Yaohong Jin." ],
      "venue" : "Proceedings of EMNLP 2020.",
      "citeRegEx" : "Li et al\\.,? 2020b",
      "shortCiteRegEx" : "Li et al\\.",
      "year" : 2020
    }, {
      "title" : "Discourse and Narrative Methods: Theoretical Departures, Analytical Strategies and Situated Writings",
      "author" : [ "Mona Livholts", "Maria Tamboukou." ],
      "venue" : "Sage.",
      "citeRegEx" : "Livholts and Tamboukou.,? 2015",
      "shortCiteRegEx" : "Livholts and Tamboukou.",
      "year" : 2015
    }, {
      "title" : "Resolving complex cases of definite pronouns: the winograd schema challenge",
      "author" : [ "Altaf Rahmanand Vincent Ng." ],
      "venue" : "Proceedings of EMNLP–CoNLL 2012, pages 777–789.",
      "citeRegEx" : "Ng.,? 2012",
      "shortCiteRegEx" : "Ng.",
      "year" : 2012
    }, {
      "title" : "Domain adaptation via multi-layer transfer learning",
      "author" : [ "Jianhan Pan", "Xuegang Hu", "Peipei Li", "Huizong Li", "Wei He", "Yuhong Zhang", "Yaojin Lin." ],
      "venue" : "Neurocomputing.",
      "citeRegEx" : "Pan et al\\.,? 2016",
      "shortCiteRegEx" : "Pan et al\\.",
      "year" : 2016
    }, {
      "title" : "Domain adaptation via transfer component analysis",
      "author" : [ "Sinno Jialin Pan", "Ivor W Tsang", "James T Kwok", "Qiang Yang." ],
      "venue" : "IEEE Transactions on Neural Networks, 22(2).",
      "citeRegEx" : "Pan et al\\.,? 2010",
      "shortCiteRegEx" : "Pan et al\\.",
      "year" : 2010
    }, {
      "title" : "A survey on transfer learning",
      "author" : [ "Sinno Jialin Pan", "Qiang Yang." ],
      "venue" : "TKDE, 22(10).",
      "citeRegEx" : "Pan and Yang.,? 2009",
      "shortCiteRegEx" : "Pan and Yang.",
      "year" : 2009
    }, {
      "title" : "Glove: Global vectors for word representation",
      "author" : [ "Jeffrey Pennington", "Richard Socher", "Christopher Manning." ],
      "venue" : "Proceedings of EMNLP 2014.",
      "citeRegEx" : "Pennington et al\\.,? 2014",
      "shortCiteRegEx" : "Pennington et al\\.",
      "year" : 2014
    }, {
      "title" : "Deep contextualized word representations",
      "author" : [ "Matthew Peters", "Mark Neumann", "Mohit Iyyer", "Matt Gardner", "Christopher Clark", "Kenton Lee", "Luke Zettlemoyer." ],
      "venue" : "Proceedings of NAACL 2018.",
      "citeRegEx" : "Peters et al\\.,? 2018",
      "shortCiteRegEx" : "Peters et al\\.",
      "year" : 2018
    }, {
      "title" : "Coarse-to-Fine Natural Language Processing",
      "author" : [ "Slav Orlinov Petrov." ],
      "venue" : "Ph.D. thesis, EECS Department, University of California, Berkeley.",
      "citeRegEx" : "Petrov.,? 2009",
      "shortCiteRegEx" : "Petrov.",
      "year" : 2009
    }, {
      "title" : "The penn discourse treebank",
      "author" : [ "Rashmi Prasad", "Nikhil Dinesh", "Alan Lee", "Eleni Miltsakaki", "Livio Robaldo", "Aravind K Joshi", "Bonnie L Webber" ],
      "venue" : null,
      "citeRegEx" : "Prasad et al\\.,? \\Q2008\\E",
      "shortCiteRegEx" : "Prasad et al\\.",
      "year" : 2008
    }, {
      "title" : "Discourse and Education, chapter Discourse Analysis Across Events",
      "author" : [ "Angela Reyes", "Stanton Wortham." ],
      "venue" : "Springer International Publishing.",
      "citeRegEx" : "Reyes and Wortham.,? 2017",
      "shortCiteRegEx" : "Reyes and Wortham.",
      "year" : 2017
    }, {
      "title" : "Stochastic backpropagation and approximate inference in deep generative models",
      "author" : [ "Danilo Jimenez Rezende", "Shakir Mohamed", "Daan Wierstra." ],
      "venue" : "Proceedings of ICML 2014.",
      "citeRegEx" : "Rezende et al\\.,? 2014",
      "shortCiteRegEx" : "Rezende et al\\.",
      "year" : 2014
    }, {
      "title" : "Building event-centric knowledge graphs from news",
      "author" : [ "Marco Rospocher", "Marieke van Erp", "Piek Vossen", "Antske Fokkens", "Itziar Aldabe", "German Rigau", "Aitor Soroa", "Thomas Ploeger", "Tessel Bogaard." ],
      "venue" : "Journal of Web Semantics.",
      "citeRegEx" : "Rospocher et al\\.,? 2016",
      "shortCiteRegEx" : "Rospocher et al\\.",
      "year" : 2016
    }, {
      "title" : "Atomic: An atlas of machine commonsense for ifthen reasoning",
      "author" : [ "Maarten Sap", "Ronan LeBras", "Emily Allaway", "Chandra Bhagavatula", "Nicholas Lourie", "Hannah Rashkin", "Brendan Roof", "Noah A. Smith", "Yejin Choi." ],
      "venue" : "Proceedings of AAAI 2019.",
      "citeRegEx" : "Sap et al\\.,? 2019",
      "shortCiteRegEx" : "Sap et al\\.",
      "year" : 2019
    }, {
      "title" : "Event2mind: Commonsense inference on events, intents, and reactions",
      "author" : [ "Noah A. Smith", "Yejin Choi", "Maarten Sap", "Hannah Rashkin", "Emily Allaway." ],
      "venue" : "Proceedings of ACL 2018.",
      "citeRegEx" : "Smith et al\\.,? 2018",
      "shortCiteRegEx" : "Smith et al\\.",
      "year" : 2018
    }, {
      "title" : "Learning structured output representation using deep conditional generative models",
      "author" : [ "Kihyuk Sohn", "Honglak Lee", "Xinchen Yan." ],
      "venue" : "Proceedings of NIPS 2015.",
      "citeRegEx" : "Sohn et al\\.,? 2015",
      "shortCiteRegEx" : "Sohn et al\\.",
      "year" : 2015
    }, {
      "title" : "Conceptnet 5.5: An open multilingual graph of general knowledge",
      "author" : [ "Robert Speer", "Joshua Chin", "Catherine Havasi" ],
      "venue" : "In Proceedings of AAAI 2017",
      "citeRegEx" : "Speer et al\\.,? \\Q2017\\E",
      "shortCiteRegEx" : "Speer et al\\.",
      "year" : 2017
    }, {
      "title" : "Yago: A core of semantic knowledge",
      "author" : [ "Fabian M. Suchanek", "Gjergji Kasneci", "Gerhard Weikum." ],
      "venue" : "Proceedings of WWW 2007.",
      "citeRegEx" : "Suchanek et al\\.,? 2007",
      "shortCiteRegEx" : "Suchanek et al\\.",
      "year" : 2007
    }, {
      "title" : "Knowlywood: Mining activity knowledge from hollywood narratives",
      "author" : [ "Niket Tandon", "Gerard De Melo", "Abir De", "Gerhard Weikum." ],
      "venue" : "Proceedings of CIKM 2015.",
      "citeRegEx" : "Tandon et al\\.,? 2015",
      "shortCiteRegEx" : "Tandon et al\\.",
      "year" : 2015
    }, {
      "title" : "A simple method for commonsense reasoning",
      "author" : [ "Trieu H. Trinh", "Quoc V. Le." ],
      "venue" : "arXiv preprint arXiv:1806.02847.",
      "citeRegEx" : "Trinh and Le.,? 2018",
      "shortCiteRegEx" : "Trinh and Le.",
      "year" : 2018
    }, {
      "title" : "Attention is all you need",
      "author" : [ "Ashish Vaswani", "Noam Shazeer", "Niki Parmar", "Jakob Uszkoreit", "Llion Jones", "Aidan N Gomezand Łukasz Kaiser", "Illia Polosukhin." ],
      "venue" : "Proceedings of NIPS 2017.",
      "citeRegEx" : "Vaswani et al\\.,? 2017",
      "shortCiteRegEx" : "Vaswani et al\\.",
      "year" : 2017
    }, {
      "title" : "Newsreader at semeval-2018 task 5: Counting events by reasoning over event-centricknowledge-graphs",
      "author" : [ "Piek Vossen." ],
      "venue" : "Proceedings of The 12th International Workshop on Semantic Evaluation.",
      "citeRegEx" : "Vossen.,? 2018",
      "shortCiteRegEx" : "Vossen.",
      "year" : 2018
    }, {
      "title" : "Transformers: State-of-the-art natural language processing",
      "author" : [ "Scao", "Sylvain Gugger", "Mariama Drame", "Quentin Lhoest", "Alexander M. Rush." ],
      "venue" : "Proceedings of EMNLP 2020: System Demonstrations.",
      "citeRegEx" : "Scao et al\\.,? 2020",
      "shortCiteRegEx" : "Scao et al\\.",
      "year" : 2020
    }, {
      "title" : "Probase: A probabilistic taxonomy for text understanding",
      "author" : [ "Wentao Wu", "Hongsong Li", "Haixun Wang", "Kenny Q. Zhu." ],
      "venue" : "Proceedings of SIGMOD 2012.",
      "citeRegEx" : "Wu et al\\.,? 2012",
      "shortCiteRegEx" : "Wu et al\\.",
      "year" : 2012
    }, {
      "title" : "Enriching largescale eventuality knowledge graph with entailment relations",
      "author" : [ "Changlong Yu", "Hongming Zhang", "Yangqiu Song", "Wilfred Ng", "Lifeng Shang." ],
      "venue" : "Proceedings of AKBC 2020.",
      "citeRegEx" : "Yu et al\\.,? 2020",
      "shortCiteRegEx" : "Yu et al\\.",
      "year" : 2020
    }, {
      "title" : "Variational neural machine translation",
      "author" : [ "Biao Zhang", "Deyi Xiong", "Jinsong Su", "Hong Duan", "Min Zhang." ],
      "venue" : "Proceedings of EMNLP 2016.",
      "citeRegEx" : "Zhang et al\\.,? 2016a",
      "shortCiteRegEx" : "Zhang et al\\.",
      "year" : 2016
    }, {
      "title" : "Variational neural discourse relation recognizer",
      "author" : [ "Biao Zhang", "Deyi Xiong", "Jinsong Su", "Qun Liu", "Rongrong Ji", "Hong Duan", "Min Zhang." ],
      "venue" : "Proceedings of EMNLP 2016.",
      "citeRegEx" : "Zhang et al\\.,? 2016b",
      "shortCiteRegEx" : "Zhang et al\\.",
      "year" : 2016
    }, {
      "title" : "Transomcs: From linguistic graphs to commonsense knowledge",
      "author" : [ "Hongming Zhang", "Daniel Khashabi", "Yangqiu Song", "Dan Roth." ],
      "venue" : "Proceedings of IJCAI 2020.",
      "citeRegEx" : "Zhang et al\\.,? 2020a",
      "shortCiteRegEx" : "Zhang et al\\.",
      "year" : 2020
    }, {
      "title" : "Aser: A largescale eventuality knowledge graph",
      "author" : [ "Hongming Zhang", "Xin Liu", "Haojie Pan", "Yangqiu Song", "Cane Wing-Ki Leung." ],
      "venue" : "Proceedings of WWW 2020.",
      "citeRegEx" : "Zhang et al\\.,? 2020b",
      "shortCiteRegEx" : "Zhang et al\\.",
      "year" : 2020
    } ],
    "referenceMentions" : [ {
      "referenceID" : 9,
      "context" : "Event-centric knowledge graphs (EventKGs) model the narratives of the world by representing events and identifying relations between them, which are critical for machine understanding and can benefit many downstream tasks, such as question answering (Costa et al., 2020), news reading (Vossen, 2018), commonsense knowledge acquisition (Zhang et al.",
      "startOffset" : 250,
      "endOffset" : 270
    }, {
      "referenceID" : 45,
      "context" : ", 2020), news reading (Vossen, 2018), commonsense knowledge acquisition (Zhang et al.",
      "startOffset" : 22,
      "endOffset" : 36
    }, {
      "referenceID" : 51,
      "context" : ", 2020), news reading (Vossen, 2018), commonsense knowledge acquisition (Zhang et al., 2020a) and so on.",
      "startOffset" : 72,
      "endOffset" : 93
    }, {
      "referenceID" : 42,
      "context" : "Recently, semi-automatically constructing EventKGs have gained much attention (Tandon et al., 2015; Rospocher et al., 2016; Gottschalk and Demidova, 2018; Zhang et al., 2020b).",
      "startOffset" : 78,
      "endOffset" : 175
    }, {
      "referenceID" : 36,
      "context" : "Recently, semi-automatically constructing EventKGs have gained much attention (Tandon et al., 2015; Rospocher et al., 2016; Gottschalk and Demidova, 2018; Zhang et al., 2020b).",
      "startOffset" : 78,
      "endOffset" : 175
    }, {
      "referenceID" : 13,
      "context" : "Recently, semi-automatically constructing EventKGs have gained much attention (Tandon et al., 2015; Rospocher et al., 2016; Gottschalk and Demidova, 2018; Zhang et al., 2020b).",
      "startOffset" : 78,
      "endOffset" : 175
    }, {
      "referenceID" : 52,
      "context" : "Recently, semi-automatically constructing EventKGs have gained much attention (Tandon et al., 2015; Rospocher et al., 2016; Gottschalk and Demidova, 2018; Zhang et al., 2020b).",
      "startOffset" : 78,
      "endOffset" : 175
    }, {
      "referenceID" : 52,
      "context" : "The common practice in previous connective-based approaches is to ignore all these implicit instances (Zhang et al., 2020b).",
      "startOffset" : 102,
      "endOffset" : 123
    }, {
      "referenceID" : 16,
      "context" : "Besides, because the scale of the existed event relation corpus (Hong et al., 2016) is limited, it is also impractical to build effective event relation classifiers via supervised learning.",
      "startOffset" : 64,
      "endOffset" : 83
    }, {
      "referenceID" : 52,
      "context" : "We conduct intrinsic experiments on ASER (Zhang et al., 2020b), one of the representative EventKGs, and extrinsic experiments on Winograd Scheme Challenge (WSC) (Levesque et al.",
      "startOffset" : 41,
      "endOffset" : 62
    }, {
      "referenceID" : 22,
      "context" : ", 2020b), one of the representative EventKGs, and extrinsic experiments on Winograd Scheme Challenge (WSC) (Levesque et al., 2012), one of the representative natural language understanding benchmarks.",
      "startOffset" : 107,
      "endOffset" : 130
    }, {
      "referenceID" : 33,
      "context" : "To resolve the implicit discourse relation recognition (IDRR) task, researchers construct high-quality labelled datasets (Prasad et al., 2008) and design elaborate models (Zhang et al.",
      "startOffset" : 121,
      "endOffset" : 142
    }, {
      "referenceID" : 50,
      "context" : ", 2008) and design elaborate models (Zhang et al., 2016b; Bai and Zhao, 2018; Kishimoto et al., 2020).",
      "startOffset" : 36,
      "endOffset" : 101
    }, {
      "referenceID" : 2,
      "context" : ", 2008) and design elaborate models (Zhang et al., 2016b; Bai and Zhao, 2018; Kishimoto et al., 2020).",
      "startOffset" : 36,
      "endOffset" : 101
    }, {
      "referenceID" : 21,
      "context" : ", 2008) and design elaborate models (Zhang et al., 2016b; Bai and Zhao, 2018; Kishimoto et al., 2020).",
      "startOffset" : 36,
      "endOffset" : 101
    }, {
      "referenceID" : 0,
      "context" : "Recent NLP studies have proved that discourse and narratives closely interact with each other, and leveraging discourse knowledge benefits narrative analysis significantly, such as subevents detection (Aldawsari and Finlayson, 2019) and main event relevant identification (Choubey et al.",
      "startOffset" : 201,
      "endOffset" : 232
    }, {
      "referenceID" : 8,
      "context" : "Recent NLP studies have proved that discourse and narratives closely interact with each other, and leveraging discourse knowledge benefits narrative analysis significantly, such as subevents detection (Aldawsari and Finlayson, 2019) and main event relevant identification (Choubey et al., 2020).",
      "startOffset" : 272,
      "endOffset" : 294
    }, {
      "referenceID" : 10,
      "context" : "For knowledge projection, we model both event relation extraction (ERE) and discourse relation recognition (DRR) as an instance-pair classification task (Devlin et al., 2019; Kishimoto et al., 2020).",
      "startOffset" : 153,
      "endOffset" : 198
    }, {
      "referenceID" : 21,
      "context" : "For knowledge projection, we model both event relation extraction (ERE) and discourse relation recognition (DRR) as an instance-pair classification task (Devlin et al., 2019; Kishimoto et al., 2020).",
      "startOffset" : 153,
      "endOffset" : 198
    }, {
      "referenceID" : 21,
      "context" : "Specifically, MKPNet extends the SotADRR model — BERT-CLS (Kishimoto et al., 2020) by the VAE-based semantic encoder and the coarse category encoder to model knowledge tier-by-tier (Pan et al.",
      "startOffset" : 58,
      "endOffset" : 82
    }, {
      "referenceID" : 27,
      "context" : ", 2020) by the VAE-based semantic encoder and the coarse category encoder to model knowledge tier-by-tier (Pan et al., 2016; Guo et al., 2019; Kang et al., 2020; Li et al., 2020b).",
      "startOffset" : 106,
      "endOffset" : 179
    }, {
      "referenceID" : 14,
      "context" : ", 2020) by the VAE-based semantic encoder and the coarse category encoder to model knowledge tier-by-tier (Pan et al., 2016; Guo et al., 2019; Kang et al., 2020; Li et al., 2020b).",
      "startOffset" : 106,
      "endOffset" : 179
    }, {
      "referenceID" : 19,
      "context" : ", 2020) by the VAE-based semantic encoder and the coarse category encoder to model knowledge tier-by-tier (Pan et al., 2016; Guo et al., 2019; Kang et al., 2020; Li et al., 2020b).",
      "startOffset" : 106,
      "endOffset" : 179
    }, {
      "referenceID" : 24,
      "context" : ", 2020) by the VAE-based semantic encoder and the coarse category encoder to model knowledge tier-by-tier (Pan et al., 2016; Guo et al., 2019; Kang et al., 2020; Li et al., 2020b).",
      "startOffset" : 106,
      "endOffset" : 179
    }, {
      "referenceID" : 30,
      "context" : "ally share similar lexical and syntactic structures and therefore lead to similar token representations (Pennington et al., 2014; Peters et al., 2018).",
      "startOffset" : 104,
      "endOffset" : 150
    }, {
      "referenceID" : 31,
      "context" : "ally share similar lexical and syntactic structures and therefore lead to similar token representations (Pennington et al., 2014; Peters et al., 2018).",
      "startOffset" : 104,
      "endOffset" : 150
    }, {
      "referenceID" : 10,
      "context" : "Then, the event pair representation will be inputted into BERT architecture (Devlin et al., 2019) and updated by multilayer Transformer blocks (Vaswani et al.",
      "startOffset" : 76,
      "endOffset" : 97
    }, {
      "referenceID" : 44,
      "context" : ", 2019) and updated by multilayer Transformer blocks (Vaswani et al., 2017).",
      "startOffset" : 53,
      "endOffset" : 75
    }, {
      "referenceID" : 23,
      "context" : "However, BERT always induces a nonsmooth anisotropic semantic space which is adverse for semantic modelling of large-grained linguistic units (Li et al., 2020a).",
      "startOffset" : 142,
      "endOffset" : 160
    }, {
      "referenceID" : 20,
      "context" : "736 tion trick (Kingma and Welling, 2014; Sohn et al., 2015) to get the final semantic representation:",
      "startOffset" : 15,
      "endOffset" : 60
    }, {
      "referenceID" : 39,
      "context" : "736 tion trick (Kingma and Welling, 2014; Sohn et al., 2015) to get the final semantic representation:",
      "startOffset" : 15,
      "endOffset" : 60
    }, {
      "referenceID" : 49,
      "context" : "During testing, due to the absence of the outputside representation hY , we set hz to be the mean of p(hz|h[CLS]) (Zhang et al., 2016a), i.",
      "startOffset" : 114,
      "endOffset" : 135
    }, {
      "referenceID" : 33,
      "context" : "In addition, we found that ERE and DRR share the same coarse-grained categories: Temporal, Contingency, Comparison and Expansion (Prasad et al., 2008; Zhang et al., 2020b), although they have different fine-grained categories.",
      "startOffset" : 129,
      "endOffset" : 171
    }, {
      "referenceID" : 52,
      "context" : "In addition, we found that ERE and DRR share the same coarse-grained categories: Temporal, Contingency, Comparison and Expansion (Prasad et al., 2008; Zhang et al., 2020b), although they have different fine-grained categories.",
      "startOffset" : 129,
      "endOffset" : 171
    }, {
      "referenceID" : 32,
      "context" : "To this end, we design the coarse category adaptor in a coarse-to-fine framework (Petrov, 2009) to bridge the gap between the heterogeneous finegrained targets.",
      "startOffset" : 81,
      "endOffset" : 95
    }, {
      "referenceID" : 6,
      "context" : "In this paper, we utilize multi-task learning (Caruana, 1997) to implement the knowledge projection from discourse to narrative.",
      "startOffset" : 46,
      "endOffset" : 61
    }, {
      "referenceID" : 11,
      "context" : "Given ERE and DRR training datasets, an alternate optimization approach (Dong et al., 2015) is used to optimizate MKPNet:",
      "startOffset" : 72,
      "endOffset" : 91
    }, {
      "referenceID" : 52,
      "context" : "We conduct intrinsic experiments on ASER (Zhang et al., 2020b) to assess the effectiveness of the proposed MKPNet, and extrinsic experiments on WSC (Levesque et al.",
      "startOffset" : 41,
      "endOffset" : 62
    }, {
      "referenceID" : 22,
      "context" : ", 2020b) to assess the effectiveness of the proposed MKPNet, and extrinsic experiments on WSC (Levesque et al., 2012) to verify the value of the extracted event relations.",
      "startOffset" : 94,
      "endOffset" : 117
    }, {
      "referenceID" : 33,
      "context" : "0 (Prasad et al., 2008) with the same splits of Ji and Eisenstein (2015): sections 2-20/0-1/21-22 respectively for train/dev/test.",
      "startOffset" : 2,
      "endOffset" : 23
    }, {
      "referenceID" : 21,
      "context" : "We use BERT-base and set all hyper-parameters using the default settings of the SotADRR model (Kishimoto et al., 2020).",
      "startOffset" : 94,
      "endOffset" : 118
    }, {
      "referenceID" : 29,
      "context" : "• Baselines with Discourse Knowledge improve the learning of ERE via transfer learning (Pan and Yang, 2009; Pan et al., 2010) from discourse models, i.",
      "startOffset" : 87,
      "endOffset" : 125
    }, {
      "referenceID" : 28,
      "context" : "• Baselines with Discourse Knowledge improve the learning of ERE via transfer learning (Pan and Yang, 2009; Pan et al., 2010) from discourse models, i.",
      "startOffset" : 87,
      "endOffset" : 125
    }, {
      "referenceID" : 3,
      "context" : "FrameNet (Baker et al., 1998) 1,709 ConceptNet (Speer et al.",
      "startOffset" : 9,
      "endOffset" : 29
    }, {
      "referenceID" : 40,
      "context" : ", 1998) 1,709 ConceptNet (Speer et al., 2017) 116,097 Event2Mind (Smith et al.",
      "startOffset" : 25,
      "endOffset" : 45
    }, {
      "referenceID" : 38,
      "context" : ", 2017) 116,097 Event2Mind (Smith et al., 2018) 57,097 ATOMIC (Sap et al.",
      "startOffset" : 27,
      "endOffset" : 47
    }, {
      "referenceID" : 37,
      "context" : ", 2018) 57,097 ATOMIC (Sap et al., 2019) 877,108 Knowlywood (Tandon et al.",
      "startOffset" : 22,
      "endOffset" : 40
    }, {
      "referenceID" : 42,
      "context" : ", 2019) 877,108 Knowlywood (Tandon et al., 2015) 2,644,415 ASER (Zhang et al.",
      "startOffset" : 27,
      "endOffset" : 48
    }, {
      "referenceID" : 22,
      "context" : "(WSC) (Levesque et al., 2012) to verify the effect of ASET++.",
      "startOffset" : 6,
      "endOffset" : 29
    }, {
      "referenceID" : 12,
      "context" : "• Pure Knowledge-based Methods are heuristical rule-based methods, such as Knowledge Hunting (Emami et al., 2018) and String Match (Zhang et al.",
      "startOffset" : 93,
      "endOffset" : 113
    }, {
      "referenceID" : 43,
      "context" : "• Language Model-based Methods use language model trained on large-scale corpus and tuned specifically for the WSC task, such as LM (Trinh and Le, 2018).",
      "startOffset" : 132,
      "endOffset" : 152
    }, {
      "referenceID" : 12,
      "context" : "Pure Knowledge-based Methods Knowledge Hunting (Emami et al., 2018) 57.",
      "startOffset" : 47,
      "endOffset" : 67
    }, {
      "referenceID" : 43,
      "context" : "6 Language Model-based Methods LM (Single) (Trinh and Le, 2018) 54.",
      "startOffset" : 43,
      "endOffset" : 63
    }, {
      "referenceID" : 4,
      "context" : "Knowledge graphs have come from entity-centric ones (Banko et al., 2007; Suchanek et al., 2007; Bollacker et al., 2008; Wu et al., 2012) to event-centric ones.",
      "startOffset" : 52,
      "endOffset" : 136
    }, {
      "referenceID" : 41,
      "context" : "Knowledge graphs have come from entity-centric ones (Banko et al., 2007; Suchanek et al., 2007; Bollacker et al., 2008; Wu et al., 2012) to event-centric ones.",
      "startOffset" : 52,
      "endOffset" : 136
    }, {
      "referenceID" : 5,
      "context" : "Knowledge graphs have come from entity-centric ones (Banko et al., 2007; Suchanek et al., 2007; Bollacker et al., 2008; Wu et al., 2012) to event-centric ones.",
      "startOffset" : 52,
      "endOffset" : 136
    }, {
      "referenceID" : 47,
      "context" : "Knowledge graphs have come from entity-centric ones (Banko et al., 2007; Suchanek et al., 2007; Bollacker et al., 2008; Wu et al., 2012) to event-centric ones.",
      "startOffset" : 52,
      "endOffset" : 136
    }, {
      "referenceID" : 42,
      "context" : "Recently, many modern and large-scale KGs have been built semi-automatically, which focus on events (Tandon et al., 2015; Rospocher et al., 2016; Gottschalk and Demidova, 2018; Zhang et al., 2020b) and commonsense (Speer et al.",
      "startOffset" : 100,
      "endOffset" : 197
    }, {
      "referenceID" : 36,
      "context" : "Recently, many modern and large-scale KGs have been built semi-automatically, which focus on events (Tandon et al., 2015; Rospocher et al., 2016; Gottschalk and Demidova, 2018; Zhang et al., 2020b) and commonsense (Speer et al.",
      "startOffset" : 100,
      "endOffset" : 197
    }, {
      "referenceID" : 13,
      "context" : "Recently, many modern and large-scale KGs have been built semi-automatically, which focus on events (Tandon et al., 2015; Rospocher et al., 2016; Gottschalk and Demidova, 2018; Zhang et al., 2020b) and commonsense (Speer et al.",
      "startOffset" : 100,
      "endOffset" : 197
    }, {
      "referenceID" : 52,
      "context" : "Recently, many modern and large-scale KGs have been built semi-automatically, which focus on events (Tandon et al., 2015; Rospocher et al., 2016; Gottschalk and Demidova, 2018; Zhang et al., 2020b) and commonsense (Speer et al.",
      "startOffset" : 100,
      "endOffset" : 197
    }, {
      "referenceID" : 29,
      "context" : "740 ana, 1997), transfer learning (Pan and Yang, 2009; Pan et al., 2010), and knowledge distillation (Hinton et al.",
      "startOffset" : 34,
      "endOffset" : 72
    }, {
      "referenceID" : 28,
      "context" : "740 ana, 1997), transfer learning (Pan and Yang, 2009; Pan et al., 2010), and knowledge distillation (Hinton et al.",
      "startOffset" : 34,
      "endOffset" : 72
    }, {
      "referenceID" : 15,
      "context" : ", 2010), and knowledge distillation (Hinton et al., 2014).",
      "startOffset" : 36,
      "endOffset" : 57
    }, {
      "referenceID" : 27,
      "context" : "Recently, researchers are interested in training/sharing/transferring/distilling models layer by layer to fully excavate the knowledge (Pan et al., 2016; Guo et al., 2019; Kang et al., 2020; Li et al., 2020b).",
      "startOffset" : 135,
      "endOffset" : 208
    }, {
      "referenceID" : 14,
      "context" : "Recently, researchers are interested in training/sharing/transferring/distilling models layer by layer to fully excavate the knowledge (Pan et al., 2016; Guo et al., 2019; Kang et al., 2020; Li et al., 2020b).",
      "startOffset" : 135,
      "endOffset" : 208
    }, {
      "referenceID" : 19,
      "context" : "Recently, researchers are interested in training/sharing/transferring/distilling models layer by layer to fully excavate the knowledge (Pan et al., 2016; Guo et al., 2019; Kang et al., 2020; Li et al., 2020b).",
      "startOffset" : 135,
      "endOffset" : 208
    }, {
      "referenceID" : 24,
      "context" : "Recently, researchers are interested in training/sharing/transferring/distilling models layer by layer to fully excavate the knowledge (Pan et al., 2016; Guo et al., 2019; Kang et al., 2020; Li et al., 2020b).",
      "startOffset" : 135,
      "endOffset" : 208
    } ],
    "year" : 2021,
    "abstractText" : "Current event-centric knowledge graphs highly rely on explicit connectives to mine relations between events. Unfortunately, due to the sparsity of connectives, these methods severely undermine the coverage of EventKGs. The lack of high-quality labelled corpora further exacerbates that problem. In this paper, we propose a knowledge projection paradigm for event relation extraction: projecting discourse knowledge to narratives by exploiting the commonalities between them. Specifically, we propose Multi-tier Knowledge Projection Network (MKPNet), which can leverage multitier discourse knowledge effectively for event relation extraction.In this way, the labelled data requirement is significantly reduced, and implicit event relations can be effectively extracted. Intrinsic experimental results show that MKPNet achieves the new state-of-the-art performance, and extrinsic experimental results verify the value of the extracted event relations.",
    "creator" : "LaTeX with hyperref"
  }
}