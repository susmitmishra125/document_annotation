{
  "name" : "2021.acl-long.395.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "COINS: Dynamically Generating COntextualized Inference Rules for Narrative Story Completion",
    "authors" : [ "Debjit Paul", "Anette Frank" ],
    "emails" : [ "paul@cl.uni-heidelberg.de", "frank@cl.uni-heidelberg.de" ],
    "sections" : [ {
      "heading" : null,
      "text" : "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 5086–5099\nAugust 1–6, 2021. ©2021 Association for Computational Linguistics\n5086"
    }, {
      "heading" : "1 Introduction",
      "text" : "Narrative story understanding, and similarly story generation, requires the ability to construe meaning that is not explicitly stated through commonsense reasoning over events in the story (Rashkin et al., 2018a). Previous work in modeling narrative stories has focused on learning scripts1 (Schank and Abelson, 1977; Mooney and DeJong, 1985) and learning narrative schemas using corpus statis-\n1Scripts are structured knowledge about stereotypical event sequences together with their participants.\ntics (Chambers and Jurafsky, 2009; Balasubramanian et al., 2013; Nguyen et al., 2015). Recently, large pretrained language models (LMs) such as GPT-2 have shown remarkable performance on various generation tasks. While these pretrained LMs learn probabilistic associations between words and sentences, they still have difficulties in modeling causality (Mostafazadeh et al., 2020). Also, in narrative story generation, models need to be consistent with everyday commonsense norms. Hence, to address a story generation task, i) models need to be equipped with suitable knowledge, ii) they need effective knowledge integration and reasoning methods, and ideally iii) we want to be able to make the effectiveness of these methods transparent.\nIn this work we focus on the aspects i) to iii), by investigating new methods that build on pretrained LMs to generate missing sentences from an incomplete narrative story. Specifically, we focus on Narrative Story Completion (NSC), a new task setting for story generation. Given an incomplete story, specified only through its beginning and ending, the task is to generate the missing sentences to complete the story (see Figure 1). Our hypothesis is that in order to obtaining a consistent and coherent\nnarrative story, the task requires a model’s ability to perform commonsense inference about events and entities in a story. Unlike other existing tasks, NSC requires: i) generating multiple sentences to complete a story, and ii) ensuring that the generated sentences are coherent with respect to both beginning and ending of the story. Hence, the NSC task offers a challenging setup for investigating the reasoning capacities of a story generation model.\nHumans excel in drawing inferences and constructing causal chains that explain the connection between events (Kintsch and Dijk, 1978). Figure 1 illustrates this with an example from our NSC task.2 From Janie was excited to see her sister’s play in theatre(s1). Janie got a call from her boss about new work(s2) and the outcome Janie watched a video of the play later.(s5) – we can construct inference rules in forward and backward direction: forward via EFFECT: SomeoneB (boss) gave work to SomeoneA (Janie); backward via CAUSE: SomeoneA (Janie) wasn’t able to go SomewhereB (to the theatre). By combining these inferences, we can obtain a representation from which to generate a connection that completes the story, e.g., Janie’s boss wanted her to look after the issue(s3). She missed the theatre play(s4).\nIn this work, we propose COINS: a recursive model that jointly learns to i) dynamically generate commonsense inference rules3 grounded in the context and to ii) perform controled and coherent story generation, using the generated inferences as a guide. We hypothesize that jointly learning to generate contextualized inference rules from dynamically predicted contextualized inference rules and learning to generate story sentences incrementally while taking the inferences into account, will improve the quality of both the predicted inference rules and of generated story sentences. Moreover, the recursive nature of the model and the individuation of the inference prediction and sentence generation tasks make the process more interpretable: the generated inference rules can be viewed as intermediate representations, and can serve as explanations of how the dynamically produced inferences influence the quality of generated story sentences.\nOur main contributions are as follows: 1) We propose a new setting for a Narrative Story Completion task, which asks a system to complete a narrative story given its beginning and ending,\n2We use the ROCstories dataset to frame the NSC task. 3In this paper, similar to Mostafazadeh et al. (2020), we\nwill use “inference rule” and “explanation” interchangeably.\nwith the aim of examining the reasoning capacities of a model that solves the task.\n2) We propose an integrated reasoning and NL generation model, COINS, that based on its current context generates contextualized commonsense inference rules and follow-up sentences, in a stepwise recurrent process.\n3) We conduct extensive experiments with automatic and human evaluation. Automatic evaluations show that COINS outperforms strong baselines (+2.2 BLEU score). Human evaluation shows that compared to strong baselines, our model yields better sentence generations with respect to coherence (+50.5%) and grammaticality (+20.5%).\n4) We show that COINS generates better inference rules (+2.3 BLEU score) compared to a finetuned GPT-2 model, and that jointly learning to generate inferences and story sentences improves the quality of the generated inference rules.\nOur code is made publicly available.4"
    }, {
      "heading" : "2 Related Work",
      "text" : "Sentence-level Commonsense Inference and Beyond. Recent research in this area has focused on commonsense knowledge acquisition (Sap et al., 2019; Zhang et al., 2020; Speer et al., 2017; Malaviya et al., 2020) and commonsense reasoning (Zellers et al., 2019; Talmor et al., 2018). In our work, we focus on inferential knowledge about events, and entities participating in such events. Rashkin et al. (2018b) introduced a knowledge resource of commonsense inferences regarding people’s intents and reactions towards a diverse set of events. With COMET, Bosselut et al. (2019) have shown that pre-trained neural language models can be fine-tuned using large knowledge bases (such as ATOMIC, Sap et al. (2019)) to generate inferences for a given event or sentence. However, the generated knowledge from COMET is noncontextualized and hence, can be inconsistent. Recently, Mostafazadeh et al. (2020) proposed GLUCOSE, a new resource and dataset that offers semistructured commonsense inference rules that are grounded in sentences of specific stories. They show that fine-tuning a pre-trained LM on the GLUCOSE dataset helps the model to better generate inferrable commonsense explanations given a complete story. In concurrent work, Gabriel et al. (2021) proposed PARA-COMET, a model that in-\n4https://github.com/Heidelberg-NLP/ COINS\ncorporates paragraph-level information to generate coherent commonsense inferences from narratives. In this work, we investigate how well a neural model can generate contextualized commonsense inference rules for an incomplete story. Learning to predict iterative inference steps for successive events in a narration using semi-structured knowledge rules is still a difficult and underexplored task. We propose a model that learns to iteratively generate a coherent completion of an incomplete narrative story utilizing semi-structured knowledge as offered by the GLUCOSE framework.\nCommonsense Reasoning in Narrative Stories. Early work on narrative events focused on script learning, by defining stereotypical event sequences together with their participants (Schank and Abelson, 1977). In later works, Chambers and Jurafsky (2008, 2009); Balasubramanian et al. (2013); Nguyen et al. (2015); Pichotta and Mooney (2014) proposed methods to learn narrative event chains using a simpler event representation that allows for efficient learning and inference. Chambers and Jurafsky (2009) acquired Narrative Event Schemata from corpora and established the Narrative Cloze Task (Chambers and Jurafsky, 2008) that evaluates script knowledge by predicting a missing event (verb and its arguments) in a sequence of observed events. More recently, Mostafazadeh et al. (2016) proposed the story cloze task that selects a plausible (right) over an implausible (wrong) story ending. Bhagavatula et al. (2020) proposed an abductive reasoning task to test a model’s ability to generate plausible explanations for an incomplete set of observations. Paul and Frank (2020) proposed a multi-head knowledge attention method to dynamically incorporate non-contextualized inferential knowledge to address the abductive reasoning task. Qin et al. (2020) proposed an unsupervised decoding algorithm that can flexibly incorporate both the past and future contexts using only off-the-shelf language models to generate plausible explanations. Concurrent to our work, Paul and Frank (2021) presented a method for addressing the abductive reasoning task by explicitly learning what events could follow other events in a hypothetical scenario. In our work, we make use of the ROCStories dataset (Mostafazadeh et al., 2016) to build a Narrative Story Completion task that tests a model’s ability of generating missing sentences in a story. We propose a model that aims to produce coherent narrative stories by performing iterative\ncommonsense inference steps. Narrative Story Generation. Much existing work on story generation relied on symbolic planning methods (Lebowitz, 1987; PÉrez and Sharples, 2001; Józefowicz et al., 2016). With the advances of Seq2Seq models, several works applied them in automatic story generation tasks (Roemmele, 2016; Jain et al., 2017). Fan et al. (2018) proposed a hierarchical approach to generate short stories from initial prompts. Recently, many works have focused on integrating external commonsense knowledge from large static knowledge bases like ATOMIC (Sap et al., 2019) or ConceptNet (Speer et al., 2017) for different tasks such as story ending generation (Ji et al., 2020; Guan et al., 2019) or story generation (Guan et al., 2020; Xu et al., 2020). In concurrent work, Ammanabrolu et al. (2021) look into causality for a commonsense plot generation task. In our work, we model the assumption that contextualized inference rules provide inferred information that can guide a system in generating both contextually grounded and coherent follow-up sentences in a story generation task."
    }, {
      "heading" : "3 Task Definition",
      "text" : "We formulate the Narrative Story Completion task (NSC) as follows: given an incomplete story (S= s1, s2, sn) as a sequence of tokens t = {t1, t2, ..., tSEP , ..., tm} (with tSEP a mask token delimiting s2 and sn), the goal is to generate the missing sentences (s3, ..., sn−1) as a sequence of tokens ysi={ysi1 , y si 2 , ..., y si v } (with i = 3, ..., n−1 and v the maximum length of each sentence). In the setting of the NSC task, we expect the completed story to be coherent. That is, the generated sentences should exhibit reasonable logical connections, causal relationships, and temporal dependencies with each other and the given beginning and ending of the story. In this paper, we define a discourse to be coherent if successive sentences that are about the same entities, and the reported events involving them can be construed to reflect common knowledge about how events are typically connected in a temporal sequence or by causal relations. Similar to Hobbs (1985), the criteria to conclude that discourse is coherent include require that there are reflections of causality in the text.\nOur take on this task is to incrementally generate contextualized inference rules from the given context, and to make use of this knowledge to generate missing story sentences."
    }, {
      "heading" : "4 Discourse-Aware Inference Rules",
      "text" : "This section details how we construct training data for the NSC task, by enriching stories with automatically predicted contextualized inferences.5 We utilize the GLUCOSE (Mostafazadeh et al., 2020) dataset, which contains implicit commonsense knowledge in form of semi-structured general and specific inference rules6 (cf. Table 1) that are grounded in the context of individual stories from ROCStories. In GLUCOSE, given a story S and a selected sentence X from the story, the authors define ten dimensions d of commonsense causal explanations related to X , inspired by human cognitive psychology. Only a small part of ROCStories is annotated with GLUCOSE inferences (Table 3).\nGiven the amount of commonsense knowledge needed for real-world tasks, a static knowledge resource is always incomplete. Thus, we fine-tune a pre-trained GPT-2 model on the annotated part of GLUCOSE to dynamically generate inference rules for each sentence Xi of each story Si from the underlying ROCStories data. We fine-tune two separate language models CSIgen and CSIspec for general and specific rules, respectively (Table 2).\nThe 10 dimensions d in GLUCOSE cover im5For testing we rely on GLUCOSE’s manually validated inference rules on a small subset of the ROCStories corpus. 6Specific means rules grounded in a given context and general corresponds to rules that are applicable to other contexts.\nplicit causes and effects of a sentence X in a given story. In our work, we are interested in inference rules that explain a sentence’s causes and effects, to study the impact of such inferences on narrative story completion. We therefore cluster all dimensions d into the two categories EFFECT vs. CAUSE (Table 1) and aggregate all rules from the respective categories (preserving their dimensions). Once our models (CSIgen, CSIspec) are trained, we apply them to our NSC task training data, to enrich it with inference rules for each sentence and story."
    }, {
      "heading" : "5 COINS: COntextualized Inference and Narrative Story Completion Model",
      "text" : "In this section we introduce a recursively operating reasoning and sentence generation model: COINS. An overview is given in Figure 2. In each iteration, the model applies two consecutive steps: (1) Inference Step: Given an incomplete story context S′= X ⊕ Si and relation r, an inference model CSI (gen or spec) generates COntextualized inference rules of type r. (2) Generation Step: a sentence generator reads the generated inference rules concatenated with the current context S′ and generates the next story sentence si+1. The context S′ is updated with si+1 and steps (1) and (2) are repeated (cf. Algorithm 1).\nThis formulation allows us to i) examine inference and generation capabilities separately from each other, ii) helps determine the impact of inferential knowledge on story generation, and iii) can give us insight into how knowledge can guide story generation in a recursive inference framework.\nInference Step. We define the initial story context S′ = {s1, s2,[SEP], sn}, a selected sentence as si, and relation type r ∈ {EFFECT, CAUSE}, where i ∈ [2, . . . n-1], si={wsi1 , .., wsiv }. We adopt a pretrained GPT-2 (base) (Radford et al., 2019) transformer model with multiple Transformer blocks of multi-head self-attention and fully connected layers. During training, in each iteration the input to the model is a concatenation of the current source (S′, si, r) and target sequence i.e., the inference\nrules (Ei or Ci). Eq. (1) defines the inference rule (IR) generation model:\nh0p = ep + Pp,\nhlp = block(h l−1 <p ), l ∈ [1, L]\np(yp|y<p, p) = softmax(hLpW T ) (1)\nwhere h0p is a summation of token embedding ep and position embedding Pp for the p-th token; hlp is the l-th layer’s output at position p, computed through transformer blocks with the masked multi-head self attention mechanism; hLp is the final layer’s hidden state and y<p indicates the left context of position p. The softmax layer defines the model to output the most probable target sequence: the most likely inference rules (Ei and Ci) for each relation type (cf. Algorithm Line 4-5).\nDuring training, we minimize the objective (2) LI(β) = − m+N∑ k=m log p(Eki |S′, si, EFFECT)\n− m+N∑ k=m log p(Cki |S′, sn,CAUSE) (2)\nwhere m,N denote the number of tokens in the source (S′, si, r) and target sequence (inference rules) respectively; β refers to model parameters.\nIn this work, we focus on the NSC task, which requires our model to capture temporal dependencies and causal relationships between events. While we designed our sentence generation model in such a way that it can utilize inference rules from both forward and backward directions for each sentence, we here trigger the generation of CAUSE inference rules for sn, since we expect that events, motivations or attributes that cause sn will be relevant for generating the preceding sentences [s3, . . . sn−1].\nAlgorithm 1 COINS Input: Initial Context (S′ = {s1, s2, [SEP ], sn}) 1: MemIR← empty 2: GenS ← empty list 3: for i← 2 to n− 1 do 4: Ei = GenInferenceRules(S′, si, EFFECT) 5: Ci = GenInferenceRules(S′, sn, CAUSE) 6: Ii = Ei ⊕ Ci 7: si+1 = GenNewSentence(Ii, S′) 8: GenS := GenS + si+1 9: MemIR := MemIR ⊕ Ii 10: LS += −logp(θ)(si+1|Ii, S ′) −logp(β)(Ii|S ′) 11: LIR += −logp(θ)(si+1|Ii, S ′) −logp(β)(Ii|S ′) 12: S′ := {s1, s2, si+1, [SEP ], sn} 13: end for 14: return GenS, MemIR\nSimilarly, we generate EFFECT relations for si, assuming that an event, changes of emotion or changes of attribute that are possible effects caused by si will be most relevant for generating the missing follow-up sentences. In principle, however, for NSC and other story generation tasks, we may consider CAUSE and EFFECT relations for all sentences, letting the model freely choose from the full space of inferences.\nWe concatenate the generated inference rules (Ii = Ei ⊕ Ci)7 and store the last hidden representation in MemIR ∈ IRN×L×H , where N is the number of sentences, L the maximum inference sequence length and H the hidden state dimensions. MemIR is updated with the hidden representations of inference rules in each iteration. Hence, MemIR could act as an intermediate representation, and as a basis for providing explanations for observed story sentence generations. MemIR may also be used as a memory for long-form text generation tasks, to keep track of implicit knowledge triggered by previously generated text, and could support flexible discourse serialization patterns.8\nGeneration Step. Given the generated inference rules Ii (in form of tokens) and the incomplete story context S′, we aim to generate the next missing sentence. We pass the input through another pretrained GPT-2 (base) model (cf. Equation 1). The loss function for the sentence generator is\nLS(θ) = − v∑\nk=1\nlog P (y si+1 k |Ii, [EOK], S ′) (3)\nwhere yk denotes the k-th token and v the maximum length of the generated sentence;\n7We use [SEP ] token to delimit the individual Ei and Ci when concatenating them.\n8We leave such extensions to future work.\ni ∈ [2, n− 1] ; [EOK] denotes the end of knowledge rule tokens, and θ refers to model parameters.\nUpdate Story Context. In the final step we update the story context by inserting the generated sentence si+1 into the previous story context (cf. Algorithm 1, line 12).\nTraining and Inference. We add the losses LI for inference generation and LS for sentence generation to make the models dependent on each other (Algorithm 1, line. 10-11). For both the inference and the generation step model, we minimize the negative log likelihood loss of the respective target sequence."
    }, {
      "heading" : "6 Experiments",
      "text" : ""
    }, {
      "heading" : "6.1 Dataset",
      "text" : "We apply COINS to the NSC and the Story Ending Generation tasks.9 For data statistics see Table 3. Narrative Story Completion. We follow the task definition as introduced in §3. Data Collection. We construct the NSC dataset on the basis of the ROCStories corpus (Mostafazadeh et al., 2016), which contains 98,162 five-sentence stories with a clear beginning and ending, thus making it a good choice for this task. We choose the first two sentences (s1, s2) as beginning rather than just s1 because the first sentence (s1) tends to be short in length, and usually introduces characters or sets the scene (Mostafazadeh et al., 2016), wherease the second sentence (s2) provides more information about the initial story."
    }, {
      "heading" : "6.2 Hyperparameter Details",
      "text" : "Parameter size. For GPT-2 we use the GPT-2 small checkpoint (117M parameters) based on the implementation of HuggingFace (Wolf et al., 2020). Decoding Strategy. In the inference stage, we adopt beam search decoding with a beam size of 5 for all our models and all baselines we produce. We used the following set of hyperparameters for our COINS model: batch size: {2, 4}; epochs: {3, 5}; learning rate: {1e-5, 5e-6}. We use Adam Optimizer, and dropout rate = 0.1. We ran our experiments with GPU sizes of 11GB and 24GB."
    }, {
      "heading" : "6.3 Baselines",
      "text" : "We compare our COINS model to the following baselines:\n9The results for Story Ending Generation will corroborate our results for NSC. All details are given in the Appendix.\n(a) GPT-2 (Radford et al., 2018) (with 12-layer, 768-hidden, 12-heads), trained with an objective to predict the next word. The input to the GPT-2 model is the concatenation of the source and the target story sequence. We follow the standard procedure to fine-tune GPT-2 on the NSC task during training and minimize the loss function:\n−log(s3, s4|[SOS]s1, s2, [SEP ], s5[EOS]) (4)\n(b) Knowledge-Enhanced GPT-2 (KE) (Guan et al., 2020) is the current SOTA for ROCStories generation. It first fine-tunes a pre-trained GPT-2 (small) model with knowledge triples from commonsense datasets (ConceptNet [CN] Speer et al. (2017) and ATOMIC [AT] Sap et al. (2020)). The knowledge triples were converted to sentences using templates. A multitask learning framework further fine-tunes this model on both the Story Ending Generation task and classifying corrupted stories from real ones. As our baseline we choose the version without multi-tasking, since the corrupted story setting is not applicable for the NSC task.\n(c) GRF (Ji et al., 2020) is the current SOTA for the Abductive Reasoning and the Story Ending Generation tasks. GRF enables pre-trained models (GPT-2 small) with dynamic multi-hop reasoning on multi-relational paths extracted from the external ConceptNet commonsense knowledge graph.\n(d) GLUCOSE-GPT-2 Similar to Guan et al. (2020), we fine-tune pretrained GPT-2 (small) on the GLUCOSE dataset using general rules (GR). We follow the same procedure as Guan et al. (2020) and (i) first fine-tune a pre-trained GPT-2 , but here on the GLUCOSE dataset, with the following loss:\n−log(Ii|S, si, r), (5)\nwhere r: CAUSE/EFFECT, Ii: Inference rules. (ii) Then we fine-tune the above model again on the NSC dataset with the following loss:\n−log(s3, s4|[SOS]s1, s2, [SEP ], s5[EOS]) (6)\nThe main difference between GLUCOSE-GPT-2 and COINS is: COINS explicitly learns to generate (contextualized) inference rules on the fly during the inference step and incorporates them in the story generation step."
    }, {
      "heading" : "6.4 Automatic Evaluation Metric",
      "text" : "For automatic evaluation in the NSC task we use as metrics Perplexity (indicates fluency of text generation), BLEU-1/2 (Papineni et al., 2002) and ROUGEL (Lin, 2004). We report performance on the test\nsets by averaging results obtained for 5 different seeds. All improvements across all model variants are statistically significant at p < 0.05)."
    }, {
      "heading" : "7 Results",
      "text" : "Our experimental results are summarised in Tables 4 and 6. NSC task. Table 4 shows the results for the models described in §6.3 and evaluated as per §6.4. We observe the following: (i) COINS outperforms all strong baseline models that utilize pre-trained language models and incorporate external commonsense knowledge with respect to all automatic evaluation metrics. Note that GLUCOSE-GPT2 and COINS are using the same knowledge resource, hence the clear performance increase of COINS (+4.92 BLEU score) indicates that jointly learning to generate contextualized inferences rules and missing sentences in a recursive manner can enhance generation quality.10 (ii) Similar to Ji et al. (2020) we observe that fine-tuning GPT2 over knowledge triples ([CN], [AT]OMIC or [GL]UCOSE) doesn’t improve the overall performance by much (Table 4, line 2: [CN+AT] vs. line 3: [GL] vs. line 1: [no CSK]). (iii) For COINS, general rules (GR) boost performance more than specific rules, indicating that the sentence generation model generalizes well. (iv) In the oracle settings at inference time we provide the model with the silver inference rules (generated as per §4) that use the complete story context as background. The result indicates that SR performs better than GR when the model sees the full story context.\nIn general we observe that story generation benefits from higher-quality, contextualized inference\n10Since GRF’s architecture is specific for ConceptNet, we cannot exclude that the better performance of COINS (+2.2 BLEU) is in part due to differences in the used knowledge.\nrules from GLUCOSE (for COINS).11 The improvement of COINS over GLUCOSE-GPT-2 indicates that our model is well able to utilize and profit from the inference rules. In the oracle setting, SR performs much better than GR. This is expected, since oracle rules with access to the full context will deliver more contextually-relevant inferences, while GR rules may diverge more from the story context. However, in the realistic NSC task setting (Table 4, lines 5,6) GR outperforms SR, which again underlines the generalization capacities of COINS.\nImpact of different inputs for the Generation Step. In Table 5 we investigate the performance of COINS with different inputs to the sentence generation component at inference time: (i) When only inference rules (from the inference step) are given to the model without any story context (S′ = {s1, s2,[SEP], sn}) (IR only), sentence generation benefits when specific rules are used. This is expected since the specific rules contain statements with concrete character names and paraphrased events from the story. (ii) When only the story beginning (s1,2) is provided to the sentence generation model without the ending sentence sn (w/oSE) nor inference rules (w/oIR) we observe that the performance drops compared to models given the full incomplete context (S′), indicating that knowing the story ending helps the model to generate missing sentences that are coherent with the story. However, (iii) when adding inference rules IR (from the inference step i.e., Ei + Ci) to the context (s1,2) without ending sentence (w/oSE), performance again improves (+5.85 BLEU scores). Note that the inference rule contains the CAUSE relation for sn. This indicates that the model is able to utilize inference rules for story generation.12\n11Automatic (silver) GLUCOSE inference rules (cf. §4) of type GR yield 60.8 BLEU score i.e., performance of CSIgen (avg. of both relation types).\n12Here, we report the results with generalized rules as GR works better than SR when context is given (cf. Table. 4).\nPerformance of inference rule generation. We now investigate how difficult it is to generate contextualized inference rules (specific and general) when multiple sentences are missing from a story. For this we compare COINS to a GPT-2 model fine-tuned on GLUCOSE data to generate inference rules (cf. §4). We study the impact of jointly and dynamically learning sentence and inference rule generation (in COINS) on the inference generation task – while the fine-tuned GPT-2 model only learns to generate inference rules conditioned on the static story context. We specifically examine the difficulty of generating inference rules for two consecutive sentences (s3 and s4) in a 5-sentence context, as opposed to shorter sequences, in three different scenarios: i) when the complete story context S is given; ii) when the incomplete context S′ (i.e., s1, s2 and s5) is given, plus either s3 or s4 (1-missing sentence), and iii) when S′ is given, but neither of the intermediate sentences s3 and s4 (2-missing sentences). In each setting, we generate EFFECT and CAUSE rules for the targeted sentences s3, s4, and compare their quality. The results are reported in Table 6. We observe that in the 2-missing sentences setting, COINS outperforms GPT-2 (by +2.3 BLEU score on average). This indicates that learning to perform inference rule generation jointly with sentence generation is beneficial for filling-in multiple story sentences. Interestingly, for increasing numbers of missing sentences, performance drops drastically for CAUSE (as opposed to EFFECT), but less so for COINS as opposed to GPT-2. A possible reason for this may be the conditional, uni-directional nature of the underlying GPT-2 language model, which is trained to predict follow-up words in forward direction. This may favor future-directed EFFECT rules – as opposed to CAUSE relations. The milder effect on COINS could indicate that the concurrent inference model supports the sentence generation model to overcome this weakness.13"
    }, {
      "heading" : "8 Manual Evaluation",
      "text" : "Automatic metrics can give us some indication of NLG quality, however, these metrics do not necessarily reflect the coherence of generated story sentences. We thus conduct a human evaluation focusing on the grammaticality and coherence of the generated sentences in their story context. We\n13In future work, we will test the above hypothesis by experimenting with a bi-directional transformer generation model.\nconduct pairwise comparisons for randomly sampled 100 instances of our best model, i.e., COINS with GR (according to automatic metrics) with four strong baseline models (GPT-2, GLUCOSEGPT-2, GRF, KE). For each pair of instances (one from COINS, the other from a baseline model), we present the generated sentences in their story context, and asked three annotators to give a preference rating (win, tie, lose) according to the criteria grammaticality and coherence. For grammaticality, we present each sentence in isolation and ask the annotators to rate which sentence is more fluent, readable, and compliant with the English standard usage. For coherence, we ask the annotators to assess which of the two generated sentences are more logically coherent with each other and the story beginning and ending, in terms of causal and temporal dependencies. We applied majority voting among the three annotators to obtain final decisions. More details about the annotation are given in Appendix.\nThe human evaluation results are presented in\nTable 7.14 The results show that our model produces more coherent and more grammatically correct sentences compared to all baselines. This indicates that with support of learned contextualized inference rules based on GLUCOSE knowledge, our model generates more coherent story sentences that are causally and temporally well connected.\nRelevance of Generated Inferences Rules. We further conduct human evaluation to validate the effectiveness and relevance of the generated inference rules. We randomly select 50 instances from the NSC dev set. We asked three annotators to evaluate the (GR) inference rules15. We define an inference rule to be relevant if (a) it captures im-\n14We report inter-annotator agreement scores calculated with Fless’ kappa κ (Fleiss, 1971), calculated for each comparison. We find moderate or fair agreement.\n15We report only COINS (GR), our best model according to automatic metrics.\nplicit causes and effects of a selected sentence X given an incomplete story S′, and (b) it is providing useful explanations for the incomplete story S′. The result for this evaluation is shown in Fig.3, for EFFECT and CAUSE relations. We find that in 36% and 34% of cases for effects and causes, respectively (computed on the basis of majority agreement), our algorithm was able to generate relevant inference rules. Our annotations yielded fair inter-annotator agreement of Fleiss’ κ = 0.45.\nCase Study. We provide an example from NSC with different generation outputs (Table 8). Note that the generated sentences are grounded to the inference rules obtained from the inference step. Hence, the rules provide both an intermediate representation and explanations for how knowledge can guide or influence story generation. We provide more qualitative examples in the Appendix."
    }, {
      "heading" : "9 Conclusion",
      "text" : "We addressed a Narrative Story Completion task that allows us to probe the coherence capabilities of a neural generation model. We proposed COINS, a model that iteratively generates commonsense inference rules grounded in the context and generates story sentences, using the generated inferences as a guide. Human and automatic eval-\nuations show that the model outperforms strong commonsense knowledge-based generation models. By individuating the inference rule and sentence generation steps, COINS can make the contribution of commonsense knowledge on story generation transparent. The recursive nature of the inference-driven generation model holds potential for knowledge-driven control in the generation of longer sequences. In future work we will explore how an enhanced memory of generated inferences can realize more complex narrative patterns that diverge from strictly ordered narrative sequences."
    }, {
      "heading" : "Acknowledgements",
      "text" : "This work has been supported by the German Research Foundation as part of the Research Training Group “Adaptive Preparation of Information from Heterogeneous Sources” (AIPHES) under grant No. GRK 1994/1. We thank our annotators for their valuable annotations. We also thank NVIDIA Corporation for donating GPUs used in this research."
    }, {
      "heading" : "A Supplementary",
      "text" : "A.1 Manual Evaluation.\nWe perform an error analysis to better understand the generation quality. We ask our annotators to assess whether the generated text contains any pieces of information that are contradicting the given incomplete story or not. Our annotations were performed by three annotators with a linguistic background. Figure 5, shows a screenshot of the annotation guidelines. Figure 4 depicts the result, we observe the that our COINS models produce less contradicting missing sentences compare to other baselines.\nA.2 Hyperparameter Details\nParameter size. For GPT-2 we use the GPT-2 small checkpoint (117M parameters) based on the implementation of HuggingFace (Wolf et al., 2020) at: https: //github.com/huggingface/transformers/\ntree/master/src/transformers/models/gpt2\nDecoding Strategy. In the inference stage, we adopt beam search decoding with a beam size of 5 for all our models and all baselines we produce. We used the following set of hyperparameters for our COINS model: batch size: {2, 4}; epochs: {3, 5}; learning rate: {1e-5, 5e-6}. We use Adam Optimizer, and dropout rate = 0.1. We ran our experiments with GPU sizes of 11GB and 24GB.\nTraining Details. Our training time is ≈24 hours. The original ROCStories Corpus can be found at: https://cs.rochester.edu/nlp/ rocstories/\nA.3 Story Ending Generation Task\nData. This task is to generate a reasonable ending given a four-sentence story context (Guan et al., 2019). The stories are from ROCStories (Mostafazadeh et al., 2016). We use the same data splits as Guan et al. (2019).\nSEG task. We also investigate how COINS performs when applied to the task of generating a story ending when given a 4-sentence story (SEG). In this task our model takes only one iteration step to generate the story ending, where in the inference\nstep it generates EFFECT inference rules for sentence (s4). As seen in Table 9, the COINS model outperforms all previous strong baselines, including GPT2-GLUCOSE that uses the same knowledge resource. Interestingly, we also observe that fine-tuning on GLUCOSE or ConceptNet knowledge improves the text generation diversity, indicating that the models leverage concepts and event knowledge during generation (cf. Table 9 line.4-8).\nAutomatic Metrics. For Story Ending Generation (SEG) we follow the metrics used in Guan et al. (2019); Ji et al. (2020): they use BLEU-1/2 to measure n-gram overlap between generated and human-written story endings, and Distinct-n (Li et al., 2016) to measure the generation diversity using maximum mutual information.\nBaselines. For the Story Ending Generation task, we compare COINS to the IE+GA model (Guan et al., 2019). It is based on incremental encoding and multi-source graph attention (Guan et al.,\n2019). We also compare to a Seq2Seq model (Luong et al., 2015) based on gated recurrent units (GRU) and attention mechanism."
    } ],
    "references" : [ {
      "title" : "Automated storytelling via causal, commonsense plot ordering",
      "author" : [ "Prithviraj Ammanabrolu", "Wesley Cheung", "William Broniec", "Mark O. Riedl." ],
      "venue" : "Proceedings of the AAAI Conference on Artificial Intelligence.",
      "citeRegEx" : "Ammanabrolu et al\\.,? 2021",
      "shortCiteRegEx" : "Ammanabrolu et al\\.",
      "year" : 2021
    }, {
      "title" : "Generating coherent event schemas at scale",
      "author" : [ "Niranjan Balasubramanian", "Stephen Soderland", "Mausam", "Oren Etzioni." ],
      "venue" : "Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1721–1731, Seattle,",
      "citeRegEx" : "Balasubramanian et al\\.,? 2013",
      "shortCiteRegEx" : "Balasubramanian et al\\.",
      "year" : 2013
    }, {
      "title" : "Abductive commonsense reasoning",
      "author" : [ "Chandra Bhagavatula", "Ronan Le Bras", "Chaitanya Malaviya", "Keisuke Sakaguchi", "Ari Holtzman", "Hannah Rashkin", "Doug Downey", "Wen tau Yih", "Yejin Choi." ],
      "venue" : "International Conference on Learning Representa-",
      "citeRegEx" : "Bhagavatula et al\\.,? 2020",
      "shortCiteRegEx" : "Bhagavatula et al\\.",
      "year" : 2020
    }, {
      "title" : "COMET: Commonsense transformers for automatic knowledge graph construction",
      "author" : [ "Antoine Bosselut", "Hannah Rashkin", "Maarten Sap", "Chaitanya Malaviya", "Asli Celikyilmaz", "Yejin Choi." ],
      "venue" : "Proceedings of the 57th Annual Meeting of the Association",
      "citeRegEx" : "Bosselut et al\\.,? 2019",
      "shortCiteRegEx" : "Bosselut et al\\.",
      "year" : 2019
    }, {
      "title" : "Unsupervised learning of narrative event chains",
      "author" : [ "Nathanael Chambers", "Dan Jurafsky." ],
      "venue" : "Proceedings of ACL-08: HLT, pages 789–797, Columbus, Ohio. Association for Computational Linguistics.",
      "citeRegEx" : "Chambers and Jurafsky.,? 2008",
      "shortCiteRegEx" : "Chambers and Jurafsky.",
      "year" : 2008
    }, {
      "title" : "Unsupervised learning of narrative schemas and their participants",
      "author" : [ "Nathanael Chambers", "Dan Jurafsky." ],
      "venue" : "Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language",
      "citeRegEx" : "Chambers and Jurafsky.,? 2009",
      "shortCiteRegEx" : "Chambers and Jurafsky.",
      "year" : 2009
    }, {
      "title" : "Hierarchical neural story generation",
      "author" : [ "Angela Fan", "Mike Lewis", "Yann Dauphin." ],
      "venue" : "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 889–898, Melbourne, Australia. Association",
      "citeRegEx" : "Fan et al\\.,? 2018",
      "shortCiteRegEx" : "Fan et al\\.",
      "year" : 2018
    }, {
      "title" : "Measuring nominal scale agreement among many raters",
      "author" : [ "Joseph L Fleiss." ],
      "venue" : "Psychological bulletin, 76(5):378.",
      "citeRegEx" : "Fleiss.,? 1971",
      "shortCiteRegEx" : "Fleiss.",
      "year" : 1971
    }, {
      "title" : "Paragraph-level commonsense transformers with recurrent memory",
      "author" : [ "Saadia Gabriel", "Chandra Bhagavatula", "Vered Shwartz", "Ronan Le Bras", "Maxwell Forbes", "Yejin Choi." ],
      "venue" : "AAAI.",
      "citeRegEx" : "Gabriel et al\\.,? 2021",
      "shortCiteRegEx" : "Gabriel et al\\.",
      "year" : 2021
    }, {
      "title" : "A knowledge-enhanced pretraining model for commonsense story generation",
      "author" : [ "Jian Guan", "Fei Huang", "Zhihao Zhao", "Xiaoyan Zhu", "Minlie Huang." ],
      "venue" : "Transactions of the Association for Computational Linguistics, 8:93–108.",
      "citeRegEx" : "Guan et al\\.,? 2020",
      "shortCiteRegEx" : "Guan et al\\.",
      "year" : 2020
    }, {
      "title" : "Story ending generation with incremental encoding and commonsense knowledge",
      "author" : [ "Jian Guan", "Yansen Wang", "Minlie Huang." ],
      "venue" : "Proceedings of the AAAI Conference on Artificial Intelligence, 33(01):6473–6480.",
      "citeRegEx" : "Guan et al\\.,? 2019",
      "shortCiteRegEx" : "Guan et al\\.",
      "year" : 2019
    }, {
      "title" : "On the coherence and structure of discourse",
      "author" : [ "Jerry R Hobbs" ],
      "venue" : null,
      "citeRegEx" : "Hobbs.,? \\Q1985\\E",
      "shortCiteRegEx" : "Hobbs.",
      "year" : 1985
    }, {
      "title" : "Story generation from sequence of independent short descriptions",
      "author" : [ "Parag Jain", "Priyanka Agrawal", "A. Mishra", "M. Sukhwani", "Anirban Laha", "K. Sankaranarayanan." ],
      "venue" : "abs/1707.05501.",
      "citeRegEx" : "Jain et al\\.,? 2017",
      "shortCiteRegEx" : "Jain et al\\.",
      "year" : 2017
    }, {
      "title" : "Language generation with multi-hop reasoning on commonsense knowledge graph",
      "author" : [ "Haozhe Ji", "Pei Ke", "Shaohan Huang", "Furu Wei", "Xiaoyan Zhu", "Minlie Huang." ],
      "venue" : "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Process-",
      "citeRegEx" : "Ji et al\\.,? 2020",
      "shortCiteRegEx" : "Ji et al\\.",
      "year" : 2020
    }, {
      "title" : "Exploring the limits of language modeling",
      "author" : [ "R. Józefowicz", "Oriol Vinyals", "Mike Schuster", "Noam Shazeer", "Y. Wu." ],
      "venue" : "ArXiv, abs/1602.02410.",
      "citeRegEx" : "Józefowicz et al\\.,? 2016",
      "shortCiteRegEx" : "Józefowicz et al\\.",
      "year" : 2016
    }, {
      "title" : "Toward a model of text comprehension and production",
      "author" : [ "W. Kintsch", "T.A. Dijk." ],
      "venue" : "Psychological Review, 85:363–394.",
      "citeRegEx" : "Kintsch and Dijk.,? 1978",
      "shortCiteRegEx" : "Kintsch and Dijk.",
      "year" : 1978
    }, {
      "title" : "Planning stories",
      "author" : [ "Michael Lebowitz." ],
      "venue" : "Proceedings of the 9th annual conference of the cognitive science society, pages 234–242.",
      "citeRegEx" : "Lebowitz.,? 1987",
      "shortCiteRegEx" : "Lebowitz.",
      "year" : 1987
    }, {
      "title" : "A diversity-promoting objective function for neural conversation models",
      "author" : [ "Jiwei Li", "Michel Galley", "Chris Brockett", "Jianfeng Gao", "William B Dolan." ],
      "venue" : "Proceedings of the 2016 Conference of the North American Chapter of the Association for Computa-",
      "citeRegEx" : "Li et al\\.,? 2016",
      "shortCiteRegEx" : "Li et al\\.",
      "year" : 2016
    }, {
      "title" : "ROUGE: A package for automatic evaluation of summaries",
      "author" : [ "Chin-Yew Lin." ],
      "venue" : "Text Summarization Branches Out, pages 74–81, Barcelona, Spain. Association for Computational Linguistics.",
      "citeRegEx" : "Lin.,? 2004",
      "shortCiteRegEx" : "Lin.",
      "year" : 2004
    }, {
      "title" : "Effective approaches to attentionbased neural machine translation",
      "author" : [ "Minh-Thang Luong", "Hieu Pham", "Christopher D Manning." ],
      "venue" : "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 1412–1421.",
      "citeRegEx" : "Luong et al\\.,? 2015",
      "shortCiteRegEx" : "Luong et al\\.",
      "year" : 2015
    }, {
      "title" : "Commonsense knowledge base completion with structural and semantic context",
      "author" : [ "Chaitanya Malaviya", "Chandra Bhagavatula", "Antoine Bosselut", "Yejin Choi." ],
      "venue" : "Proceedings of the 34th AAAI Conference on Artificial Intelligence.",
      "citeRegEx" : "Malaviya et al\\.,? 2020",
      "shortCiteRegEx" : "Malaviya et al\\.",
      "year" : 2020
    }, {
      "title" : "Learning schemata for natural language processing",
      "author" : [ "Raymond J Mooney", "Gerald DeJong." ],
      "venue" : "IJCAI, pages 681–687.",
      "citeRegEx" : "Mooney and DeJong.,? 1985",
      "shortCiteRegEx" : "Mooney and DeJong.",
      "year" : 1985
    }, {
      "title" : "A corpus and cloze evaluation for deeper understanding of commonsense stories",
      "author" : [ "Pushmeet Kohli", "James Allen." ],
      "venue" : "Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human",
      "citeRegEx" : "Kohli and Allen.,? 2016",
      "shortCiteRegEx" : "Kohli and Allen.",
      "year" : 2016
    }, {
      "title" : "GLUCOSE: GeneraLized and COntextualized story explanations",
      "author" : [ "Nasrin Mostafazadeh", "Aditya Kalyanpur", "Lori Moon", "David Buchanan", "Lauren Berkowitz", "Or Biran", "Jennifer Chu-Carroll." ],
      "venue" : "Proceedings of the 2020 Conference on Empirical",
      "citeRegEx" : "Mostafazadeh et al\\.,? 2020",
      "shortCiteRegEx" : "Mostafazadeh et al\\.",
      "year" : 2020
    }, {
      "title" : "Generative event schema induction with entity disambiguation",
      "author" : [ "Kiem-Hieu Nguyen", "Xavier Tannier", "Olivier Ferret", "Romaric Besançon." ],
      "venue" : "Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th In-",
      "citeRegEx" : "Nguyen et al\\.,? 2015",
      "shortCiteRegEx" : "Nguyen et al\\.",
      "year" : 2015
    }, {
      "title" : "Bleu: a method for automatic evaluation of machine translation",
      "author" : [ "Kishore Papineni", "Salim Roukos", "Todd Ward", "WeiJing Zhu." ],
      "venue" : "Proceedings of the 40th annual meeting of the Association for Computational Linguistics, pages 311–318.",
      "citeRegEx" : "Papineni et al\\.,? 2002",
      "shortCiteRegEx" : "Papineni et al\\.",
      "year" : 2002
    }, {
      "title" : "Social commonsense reasoning with multi-head knowledge attention",
      "author" : [ "Debjit Paul", "Anette Frank." ],
      "venue" : "Findings of the Association for Computational Linguistics: EMNLP 2020, pages 2969–2980, Online. Association for Computational Linguistics.",
      "citeRegEx" : "Paul and Frank.,? 2020",
      "shortCiteRegEx" : "Paul and Frank.",
      "year" : 2020
    }, {
      "title" : "Generating hypothetical events for abductive inference",
      "author" : [ "Debjit Paul", "Anette Frank." ],
      "venue" : "Proceedings of the Tenth Joint Conference on Lexical and Computational Semantics, Online. Association for Computational Linguistics.",
      "citeRegEx" : "Paul and Frank.,? 2021",
      "shortCiteRegEx" : "Paul and Frank.",
      "year" : 2021
    }, {
      "title" : "Statistical script learning with multi-argument events",
      "author" : [ "Karl Pichotta", "Raymond Mooney." ],
      "venue" : "Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 220–229.",
      "citeRegEx" : "Pichotta and Mooney.,? 2014",
      "shortCiteRegEx" : "Pichotta and Mooney.",
      "year" : 2014
    }, {
      "title" : "Mexica: A computer model of a cognitive account of creative writing",
      "author" : [ "Rafael PÉrez Ý PÉrez", "Mike Sharples." ],
      "venue" : "Journal of Experimental & Theoretical Artificial Intelligence, 13(2):119–139.",
      "citeRegEx" : "PÉrez and Sharples.,? 2001",
      "shortCiteRegEx" : "PÉrez and Sharples.",
      "year" : 2001
    }, {
      "title" : "Back to the future: Unsupervised backprop-based decoding for counterfactual and abductive commonsense reasoning",
      "author" : [ "Lianhui Qin", "Vered Shwartz", "Peter West", "Chandra Bhagavatula", "Jena D. Hwang", "Ronan Le Bras", "Antoine Bosselut", "Yejin Choi." ],
      "venue" : "In",
      "citeRegEx" : "Qin et al\\.,? 2020",
      "shortCiteRegEx" : "Qin et al\\.",
      "year" : 2020
    }, {
      "title" : "Improving language understanding by generative pre-training",
      "author" : [ "Alec Radford", "Karthik Narasimhan", "Tim Salimans", "Ilya Sutskever" ],
      "venue" : null,
      "citeRegEx" : "Radford et al\\.,? \\Q2018\\E",
      "shortCiteRegEx" : "Radford et al\\.",
      "year" : 2018
    }, {
      "title" : "Language models are unsupervised multitask learners",
      "author" : [ "Alec Radford", "Jeff Wu", "Rewon Child", "David Luan", "Dario Amodei", "Ilya Sutskever" ],
      "venue" : null,
      "citeRegEx" : "Radford et al\\.,? \\Q2019\\E",
      "shortCiteRegEx" : "Radford et al\\.",
      "year" : 2019
    }, {
      "title" : "Modeling naive psychology of characters in simple commonsense stories",
      "author" : [ "Hannah Rashkin", "Antoine Bosselut", "Maarten Sap", "Kevin Knight", "Yejin Choi." ],
      "venue" : "Proceedings of the 56th Annual Meeting of the Association for Computational Lin-",
      "citeRegEx" : "Rashkin et al\\.,? 2018a",
      "shortCiteRegEx" : "Rashkin et al\\.",
      "year" : 2018
    }, {
      "title" : "Event2Mind: Commonsense inference on events, intents, and reactions",
      "author" : [ "Hannah Rashkin", "Maarten Sap", "Emily Allaway", "Noah A. Smith", "Yejin Choi." ],
      "venue" : "Proceedings of the 56th Annual Meeting of the Association for Computa-",
      "citeRegEx" : "Rashkin et al\\.,? 2018b",
      "shortCiteRegEx" : "Rashkin et al\\.",
      "year" : 2018
    }, {
      "title" : "Writing stories with help from recurrent neural networks",
      "author" : [ "Melissa Roemmele." ],
      "venue" : "Proceedings of the AAAI Conference on Artificial Intelligence, volume 30.",
      "citeRegEx" : "Roemmele.,? 2016",
      "shortCiteRegEx" : "Roemmele.",
      "year" : 2016
    }, {
      "title" : "ATOMIC: an atlas of machine commonsense for if-then reasoning",
      "author" : [ "Maarten Sap", "Ronan Le Bras", "Emily Allaway", "Chandra Bhagavatula", "Nicholas Lourie", "Hannah Rashkin", "Brendan Roof", "Noah A. Smith", "Yejin Choi." ],
      "venue" : "The Thirty-Third AAAI Con-",
      "citeRegEx" : "Sap et al\\.,? 2019",
      "shortCiteRegEx" : "Sap et al\\.",
      "year" : 2019
    }, {
      "title" : "Commonsense reasoning for natural language processing",
      "author" : [ "Maarten Sap", "Vered Shwartz", "Antoine Bosselut", "Yejin Choi", "Dan Roth." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics: Tutorial Abstracts,",
      "citeRegEx" : "Sap et al\\.,? 2020",
      "shortCiteRegEx" : "Sap et al\\.",
      "year" : 2020
    }, {
      "title" : "Scripts, plans, goals, and understanding : an inquiry into human knowledge structures",
      "author" : [ "Roger C. Schank", "Robert P. Abelson." ],
      "venue" : "Hillsdale, N.J. : Lawrence Erlbaum Associates.",
      "citeRegEx" : "Schank and Abelson.,? 1977",
      "shortCiteRegEx" : "Schank and Abelson.",
      "year" : 1977
    }, {
      "title" : "Conceptnet 5.5: An open multilingual graph of general knowledge",
      "author" : [ "Robyn Speer", "Joshua Chin", "Catherine Havasi" ],
      "venue" : "In Thirty-First AAAI Conference on Artificial Intelligence",
      "citeRegEx" : "Speer et al\\.,? \\Q2017\\E",
      "shortCiteRegEx" : "Speer et al\\.",
      "year" : 2017
    }, {
      "title" : "Commonsenseqa: A question answering challenge targeting commonsense knowledge",
      "author" : [ "Alon Talmor", "Jonathan Herzig", "Nicholas Lourie", "Jonathan Berant." ],
      "venue" : "NAACL-HLT.",
      "citeRegEx" : "Talmor et al\\.,? 2018",
      "shortCiteRegEx" : "Talmor et al\\.",
      "year" : 2018
    }, {
      "title" : "Transformers: State-of-the-art natural language processing",
      "author" : [ "Teven Le Scao", "Sylvain Gugger", "Mariama Drame", "Quentin Lhoest", "Alexander Rush." ],
      "venue" : "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing:",
      "citeRegEx" : "Scao et al\\.,? 2020",
      "shortCiteRegEx" : "Scao et al\\.",
      "year" : 2020
    }, {
      "title" : "MEGATRON-CNTRL: Controllable story generation with external knowledge using large-scale language models",
      "author" : [ "Peng Xu", "Mostofa Patwary", "Mohammad Shoeybi", "Raul Puri", "Pascale Fung", "Anima Anandkumar", "Bryan Catanzaro." ],
      "venue" : "Proceedings of",
      "citeRegEx" : "Xu et al\\.,? 2020",
      "shortCiteRegEx" : "Xu et al\\.",
      "year" : 2020
    }, {
      "title" : "Hellaswag: Can a machine really finish your sentence",
      "author" : [ "Rowan Zellers", "Ari Holtzman", "Yonatan Bisk", "Ali Farhadi", "Yejin Choi" ],
      "venue" : "In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics,",
      "citeRegEx" : "Zellers et al\\.,? \\Q2019\\E",
      "shortCiteRegEx" : "Zellers et al\\.",
      "year" : 2019
    }, {
      "title" : "Aser: A largescale eventuality knowledge graph",
      "author" : [ "Hongming Zhang", "Xin Liu", "Haojie Pan", "Yangqiu Song", "Cane Wing-Ki Leung." ],
      "venue" : "Proceedings of The Web Conference 2020, pages 201–211.",
      "citeRegEx" : "Zhang et al\\.,? 2020",
      "shortCiteRegEx" : "Zhang et al\\.",
      "year" : 2020
    }, {
      "title" : "Automatic Metrics. For Story Ending Generation (SEG) we follow the metrics used in Guan et al",
      "author" : [ "Ji" ],
      "venue" : null,
      "citeRegEx" : "Ji,? \\Q2020\\E",
      "shortCiteRegEx" : "Ji",
      "year" : 2020
    } ],
    "referenceMentions" : [ {
      "referenceID" : 33,
      "context" : "Narrative story understanding, and similarly story generation, requires the ability to construe meaning that is not explicitly stated through commonsense reasoning over events in the story (Rashkin et al., 2018a).",
      "startOffset" : 189,
      "endOffset" : 212
    }, {
      "referenceID" : 38,
      "context" : "Previous work in modeling narrative stories has focused on learning scripts1 (Schank and Abelson, 1977; Mooney and DeJong, 1985) and learning narrative schemas using corpus statis-",
      "startOffset" : 77,
      "endOffset" : 128
    }, {
      "referenceID" : 21,
      "context" : "Previous work in modeling narrative stories has focused on learning scripts1 (Schank and Abelson, 1977; Mooney and DeJong, 1985) and learning narrative schemas using corpus statis-",
      "startOffset" : 77,
      "endOffset" : 128
    }, {
      "referenceID" : 23,
      "context" : "learn probabilistic associations between words and sentences, they still have difficulties in modeling causality (Mostafazadeh et al., 2020).",
      "startOffset" : 113,
      "endOffset" : 140
    }, {
      "referenceID" : 15,
      "context" : "Humans excel in drawing inferences and constructing causal chains that explain the connection between events (Kintsch and Dijk, 1978).",
      "startOffset" : 109,
      "endOffset" : 133
    }, {
      "referenceID" : 36,
      "context" : "commonsense knowledge acquisition (Sap et al., 2019; Zhang et al., 2020; Speer et al., 2017; Malaviya et al., 2020) and commonsense reasoning (Zellers et al.",
      "startOffset" : 34,
      "endOffset" : 115
    }, {
      "referenceID" : 44,
      "context" : "commonsense knowledge acquisition (Sap et al., 2019; Zhang et al., 2020; Speer et al., 2017; Malaviya et al., 2020) and commonsense reasoning (Zellers et al.",
      "startOffset" : 34,
      "endOffset" : 115
    }, {
      "referenceID" : 39,
      "context" : "commonsense knowledge acquisition (Sap et al., 2019; Zhang et al., 2020; Speer et al., 2017; Malaviya et al., 2020) and commonsense reasoning (Zellers et al.",
      "startOffset" : 34,
      "endOffset" : 115
    }, {
      "referenceID" : 20,
      "context" : "commonsense knowledge acquisition (Sap et al., 2019; Zhang et al., 2020; Speer et al., 2017; Malaviya et al., 2020) and commonsense reasoning (Zellers et al.",
      "startOffset" : 34,
      "endOffset" : 115
    }, {
      "referenceID" : 43,
      "context" : ", 2020) and commonsense reasoning (Zellers et al., 2019; Talmor et al., 2018).",
      "startOffset" : 34,
      "endOffset" : 77
    }, {
      "referenceID" : 40,
      "context" : ", 2020) and commonsense reasoning (Zellers et al., 2019; Talmor et al., 2018).",
      "startOffset" : 34,
      "endOffset" : 77
    }, {
      "referenceID" : 38,
      "context" : "Early work on narrative events focused on script learning, by defining stereotypical event sequences together with their participants (Schank and Abelson, 1977).",
      "startOffset" : 134,
      "endOffset" : 160
    }, {
      "referenceID" : 4,
      "context" : "Schemata from corpora and established the Narrative Cloze Task (Chambers and Jurafsky, 2008) that evaluates script knowledge by predicting a missing event (verb and its arguments) in a sequence of observed events.",
      "startOffset" : 63,
      "endOffset" : 92
    }, {
      "referenceID" : 16,
      "context" : "work on story generation relied on symbolic planning methods (Lebowitz, 1987; PÉrez and Sharples, 2001; Józefowicz et al., 2016).",
      "startOffset" : 61,
      "endOffset" : 128
    }, {
      "referenceID" : 29,
      "context" : "work on story generation relied on symbolic planning methods (Lebowitz, 1987; PÉrez and Sharples, 2001; Józefowicz et al., 2016).",
      "startOffset" : 61,
      "endOffset" : 128
    }, {
      "referenceID" : 14,
      "context" : "work on story generation relied on symbolic planning methods (Lebowitz, 1987; PÉrez and Sharples, 2001; Józefowicz et al., 2016).",
      "startOffset" : 61,
      "endOffset" : 128
    }, {
      "referenceID" : 36,
      "context" : "ATOMIC (Sap et al., 2019) or ConceptNet (Speer et al.",
      "startOffset" : 7,
      "endOffset" : 25
    }, {
      "referenceID" : 39,
      "context" : ", 2019) or ConceptNet (Speer et al., 2017) for different tasks such as story ending generation (Ji et al.",
      "startOffset" : 22,
      "endOffset" : 42
    }, {
      "referenceID" : 13,
      "context" : ", 2017) for different tasks such as story ending generation (Ji et al., 2020; Guan et al., 2019) or story generation (Guan et al.",
      "startOffset" : 60,
      "endOffset" : 96
    }, {
      "referenceID" : 10,
      "context" : ", 2017) for different tasks such as story ending generation (Ji et al., 2020; Guan et al., 2019) or story generation (Guan et al.",
      "startOffset" : 60,
      "endOffset" : 96
    }, {
      "referenceID" : 23,
      "context" : "Table 1: Causal Relation types and their mapped relations (Mostafazadeh et al., 2020).",
      "startOffset" : 58,
      "endOffset" : 85
    }, {
      "referenceID" : 23,
      "context" : "5 We utilize the GLUCOSE (Mostafazadeh et al., 2020) dataset, which contains implicit commonsense knowledge in form of semi-structured general and specific inference rules6 (cf.",
      "startOffset" : 25,
      "endOffset" : 52
    }, {
      "referenceID" : 32,
      "context" : "We adopt a pretrained GPT-2 (base) (Radford et al., 2019) transformer model with multiple Transformer blocks of multi-head self-attention and fully connected layers.",
      "startOffset" : 35,
      "endOffset" : 57
    }, {
      "referenceID" : 9,
      "context" : "(b) Knowledge-Enhanced GPT-2 (KE) (Guan et al., 2020) is the current SOTA for ROCStories generation.",
      "startOffset" : 34,
      "endOffset" : 53
    }, {
      "referenceID" : 13,
      "context" : "(c) GRF (Ji et al., 2020) is the current SOTA for the Abductive Reasoning and the Story Ending Generation tasks.",
      "startOffset" : 8,
      "endOffset" : 25
    }, {
      "referenceID" : 25,
      "context" : "For automatic evaluation in the NSC task we use as metrics Perplexity (indicates fluency of text generation), BLEU-1/2 (Papineni et al., 2002) and ROUGEL (Lin, 2004).",
      "startOffset" : 119,
      "endOffset" : 142
    }, {
      "referenceID" : 7,
      "context" : "We report inter-annotator agreement scores calculated with Fless’ kappa κ (Fleiss, 1971), calculated for each comparison.",
      "startOffset" : 74,
      "endOffset" : 88
    } ],
    "year" : 2021,
    "abstractText" : "Despite recent successes of large pre-trained language models in solving reasoning tasks, their inference capabilities remain opaque. We posit that such models can be made more interpretable by explicitly generating interim inference rules, and using them to guide the generation of task-specific textual outputs. In this paper we present COINS, a recursive inference framework that i) iteratively reads context sentences, ii) dynamically generates contextualized inference rules, encodes them, and iii) uses them to guide task-specific output generation. We apply COINS to a Narrative Story Completion task that asks a model to complete a story with missing sentences, to produce a coherent story with plausible logical connections, causal relationships, and temporal dependencies. By modularizing inference and sentence generation steps in a recurrent model, we aim to make reasoning steps and their effects on next sentence generation transparent. Our automatic and manual evaluations show that the model generates better story sentences than SOTA baselines, especially in terms of coherence. We further demonstrate improved performance over strong pre-trained LMs in generating commonsense inference rules. The recursive nature of COINS holds the potential for controlled generation of longer sequences.",
    "creator" : "LaTeX with hyperref"
  }
}