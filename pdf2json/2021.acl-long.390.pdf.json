{
  "name" : "2021.acl-long.390.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "Few-Shot Text Ranking with Meta Adapted Synthetic Weak Supervision",
    "authors" : [ "Si Sun", "Yingzhuo Qian", "Zhenghao Liu", "Chenyan Xiong", "Kaitao Zhang", "Jie Bao", "Zhiyuan Liu", "Paul Bennett" ],
    "emails" : [ "zkt18}@mails.tsinghua.edu.cn", "liuzy}@tsinghua.edu.cn", "Paul.N.Bennett}@microsoft.com" ],
    "sections" : [ {
      "heading" : null,
      "text" : "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 5030–5043\nAugust 1–6, 2021. ©2021 Association for Computational Linguistics\n5030\nFew-Shot Text Ranking with Meta Adapted Synthetic Weak Supervision Si Sun1, Yingzhuo Qian2, Zhenghao Liu2, Chenyan Xiong3,\nKaitao Zhang2, Jie Bao1, Zhiyuan Liu2, Paul Bennett3 1Department of Electronic Engineering, Tsinghua University, Beijing, China\n2Department of Computer Science and Technology, Tsinghua University, Beijing, China Institute for Artificial Intelligence, Tsinghua University, Beijing, China\nBeijing National Research Center for Information Science and Technology, China 3Microsoft Research, Redmond, USA {s-sun17, qyz17, liu-zh16, zkt18}@mails.tsinghua.edu.cn {bao, liuzy}@tsinghua.edu.cn\n{chenyan.xiong, Paul.N.Bennett}@microsoft.com Abstract\nThe effectiveness of Neural Information Retrieval (Neu-IR) often depends on a large scale of in-domain relevance training signals, which are not always available in real-world ranking scenarios. To democratize the benefits of NeuIR, this paper presents MetaAdaptRank, a domain adaptive learning method that generalizes Neu-IR models from label-rich source domains to few-shot target domains. Drawing on source-domain massive relevance supervision, MetaAdaptRank contrastively synthesizes a large number of weak supervision signals for target domains and meta-learns to reweight these synthetic “weak” data based on their benefits to the target-domain ranking accuracy of Neu-IR models. Experiments on three TREC benchmarks in the web, news, and biomedical domains show that MetaAdaptRank significantly improves the few-shot ranking accuracy of Neu-IR models. Further analyses indicate that MetaAdaptRank thrives from both its contrastive weak data synthesis and metareweighted data selection. The code and data of this paper can be obtained from https: //github.com/thunlp/MetaAdaptRank."
    }, {
      "heading" : "1 Introduction",
      "text" : "Text retrieval aims to rank documents to either directly satisfy users’ search needs or find textual information for later processing components, e.g., question answering (Chen et al., 2017) and fact verification (Liu et al., 2020). Neural information retrieval (Neu-IR) models have recently shown advanced results in many ranking scenarios where massive relevance labels or clickthrough data are available (Mitra et al., 2018; Craswell et al., 2020).\nThe flip side is that the “data-hungry” nature of Neu-IR models yields mixed results in few-shot ranking scenarios that suffer from the shortage of labeled data and implicit user feedback (Lin, 2019; Yang et al., 2019). On ranking benchmarks with\nonly hundreds of labeled queries, there have been debates about whether Neu-IR, even with billions of pre-trained parameters (Zhang et al., 2020a), really outperforms traditional IR techniques such as feature-based models and latent semantic indexing (Yang et al., 2019; Roberts et al., 2020). In fact, many real-world ranking scenarios are fewshot, e.g., tail web queries that innately lack large supervision (Downey et al., 2007), applications with strong privacy constraints like personal and enterprise search (Chirita et al., 2005; Hawking, 2004), and domains where labeling requires professional expertise such as biomedical and legal search (Roberts et al., 2020; Arora et al., 2018).\nTo broaden the benefits of Neu-IR to few-shot scenarios, we present an adaptive learning method MetaAdaptRank that meta-learns to adapt Neu-IR models to target domains with synthetic weak supervision. For synthesizing weak supervision, we take inspiration from the work (Ma et al., 2021) that generates related queries for unlabeled documents in a zero-shot way, but we generate discriminative queries based on contrastive pairs of relevant (positive) and irrelevant (negative) documents. By introducing the negative contrast, MetaAdaptRank can subtly capture the difference between documents to synthesize more ranking-aware weak supervision signals. Given that synthetic weak supervision inevitably contains noises, MetaAdaptRank metalearns to reweight these synthetic weak data and trains Neu-IR models to achieve the best accuracy on a small volume of target data. In this way, neural rankers can distinguish more useful synthetic weak supervision based on the similarity of the gradient directions of synthetic data and target data (Ren et al., 2018) instead of manual heuristics or trialand-error data selection (Zhang et al., 2020b).\nWe conduct experiments on three TREC benchmarks, ClueWeb09, Robust04, and TREC-COVID, which come from the web, news, and biomedi-\ncal domains, respectively. MetaAdaptRank significantly improves the few-shot ranking accuracy of Neu-IR models across all benchmarks. We also empirically indicate that both contrastive weak data synthesis and meta-reweighted data selection contribute to MetaAdaptRank’s effectiveness. Compared to prior work (Ma et al., 2021; Zhang et al., 2020b), MetaAdaptRank not only synthesizes more informative queries and effective weak relevance signals but customizes more diverse and fine-grained weights on synthetic weak data to better adapt neural rankers to target few-shot domains."
    }, {
      "heading" : "2 Related Work",
      "text" : "Recent Neu-IR methods have achieved promising results in modeling relevance matching patterns between queries and documents (Guo et al., 2016; Hui et al., 2017; Mitra et al., 2018). They have been extensively employed in ad-hoc text retrieval (Xiong et al., 2017b; Dai et al., 2018; Nogueira and Cho, 2019; Xiong et al., 2021) and later natural language processing (NLP) tasks (Lee et al., 2019; Liu et al., 2020; Qu et al., 2020).\nThe effectiveness of Neu-IR methods heavily relies on the end-to-end training with a large number of relevance supervision signals, e.g., relevance labels or user clicks. Nevertheless, such supervision signals are often insufficient in many ranking scenarios. The less availability of relevance supervision pushes some Neu-IR methods to freeze their embeddings to avoid overfitting (Yates et al., 2020). The powerful deep pre-trained language models, such as BERT (Devlin et al., 2019), also do not effectively alleviate the dependence of Neu-IR on a large scale of relevance training signals. Recent research even observes that BERT-based neural rankers might require more training data than shallow neural ranking models (Hofstätter et al., 2020; Craswell et al., 2020). Moreover, they may often be overly confident and more unstable in the learning process (Qiao et al., 2019).\nA promising direction to alleviate the dependence of Neu-IR models on large-scale relevance supervision is to leverage weak supervision signals that are noisy but available at mass quantity (Zheng et al., 2019b; Dehghani et al., 2017; Yu et al., 2020). Through IR history, various weak supervision sources have been used to approximate querydocument relevance signals, e.g., pseudo relevance labels generated by unsupervised retrieval methods (Dehghani et al., 2017; Zheng et al., 2019b),\nand title-document pairs (MacAvaney et al., 2019). Recently, Zhang et al. (2020b) treat paired anchor texts and linked pages as weak relevance signals and propose a reinforcement-based data selection method ReInfoSelect, which learns to filter noisy anchor signals with trial-and-error policy gradients. Despite their convincing results, anchor signals are only available in web domains. Directly applying them to non-web domains may suffer from suboptimal outcomes due to domain gaps. To obtain weak supervision that adapts arbitrary domains, Ma et al. (2021) present a synthetic query generation method, which can be trained with source-domain relevance signals and applied on target-domain documents to generate related queries.\nMore recently, a novel meta-learning technique has shown encouraging progress on solving data noises and label biases in computer vision (Ren et al., 2018; Shu et al., 2019; Zheng et al., 2019a) and some NLP tasks (Zheng et al., 2019a; Wang et al., 2020b). To the best of our knowledge, this novel technique has not been well utilized in information retrieval and synthetic supervision settings."
    }, {
      "heading" : "3 Methodology",
      "text" : "This section first recaps the preliminary of Neu-IR and then introduces our proposed MetaAdaptRank. The framework of our method is shown in Figure 1."
    }, {
      "heading" : "3.1 Preliminary of Neu-IR",
      "text" : "The ad-hoc retrieval task is to calculate a ranking score f(q, d; θ) for a query q and a document d from a document set. In Neu-IR, the ranking score f(·; θ) is calculated by a neural model, e.g., BERT, with parameters θ. The query q and the document d are encoded to the token-level representations H:\nH = BERT([CLS] ◦ q ◦ [SEP] ◦ d ◦ [SEP]), (1)\nwhere ◦ represents the concatenation operation. [CLS] and [SEP] are special tokens. The first token (“[CLS]”) representation H0 is regarded as the representation of the q-d pair. Then the ranking score f(q, d; θ) of the pair can be calculated as:\nf(q, d; θ) = tanh(Linear(H0)). (2)\nThe standard learning to rank loss li(θ) (Liu, 2009), e.g., pairwise loss, can be used to optimize the neural model with relevance supervision signals {(qi, d+i , d − i ), 1 ≤ i ≤M}:\nli(θ) = relu(1− (f(qi, d+i ; θ)− f(qi, d − i ; θ))), (3)\nwhere d+i and d − i denote the relevant (positive) and irrelevant (negative) documents of the query qi. In few-shot ranking scenarios, the number of relevance supervision signals (M ) is limited, making it difficult to train an accurate Neu-IR model.\nTo mitigate the few-shot challenge in Neu-IR, MetaAdaptRank first transfers source-domain supervision signals to target-domain weak supervision signals (Sec 3.2); then meta-learns to reweight the synthetic weak supervision (Sec 3.3) for selectively training Neu-IR models (Sec 3.4)."
    }, {
      "heading" : "3.2 Contrastive Synthetic Supervision",
      "text" : "MetaAdaptRank transfers the relevance supervision signals from source domains to few-shot target domains in a zero-shot way. In this way, a natural language generation (NLG) model is trained on source domain relevance signals (Source-domain NLG Training) and is employed in target domains to synthesize weak supervision signals (Targetdomain NLG Inference). We will first recap the previous synthetic method (Ma et al., 2021) and then introduce our contrastive synthetic approach.\nPreliminary of Synthetic Supervision. Given a large volume of source-domain relevance pairs (q, d+), previous synthetic method (Ma et al., 2021) trains a NLG model such as T5 (Raffel et al., 2020) that learns to generate a query q based on its relevant document d+:\nq = T5-NLG([POS] ◦ d+ ◦ [SEP]), (4)\nwhere [POS] and [SEP] are special tokens. In inference, the trained query generator is directly used to generate new queries q∗ for target-domain documents d∗, where d∗ is regarded as the related (posi-\ntive) document of q∗, while the unrelated (negative) document can be sampled from the target corpus.\nDespite some promising results, the vanilla training strategy may cause the NLG model to prefer to generate broad and general queries that are likely related to a crowd of documents in the target corpus. As a consequence, the synthetic relevance supervision does not have enough ranking awareness to train robust Neu-IR models.\nSource-domain NLG Training. To synthesize ranking-aware weak supervision, MetaAdaptRank trains the NLG model to capture the difference between the contrastive document pair (d+, d−) and generate a discriminative query q:\nq = T5-NLG([POS] ◦ d+ ◦ [NEG] ◦ d− ◦ [SEP]), (5)\nwhere [NEG] is another special token. The training instances (q, d+, d−) can be obtained from source domains in which d+ and d− are annotated as the relevant and irrelevant documents for the query q.\nTarget-domain NLG Inference. During inference, we first pick out a mass of confusable document pairs from target domains and then feed them into our trained contrastive query generator (Eq. 5) to synthesize more valuable weak supervision data.\nTo get confusable document pairs, we first generate a seed query q∗ for each target-domain document d∗ using the trained query generator (Eq. 4). Then the seed query is used to retrieve a subset of documents with BM25, where other retrieval methods can also be utilized. The confusable document pairs (d+′, d−′) are pairwise sampled from the retrieved subset without considering their rankings. Given the confusable document pair, we leverage our trained contrastive query generator to generate\na new query q′:\nq′ = T5-NLG([POS] ◦ d+′ ◦ [NEG] ◦ d−′ ◦ [SEP]), (6)\nwhere d+′ and d−′ are regarded as the related (positive) and unrelated (negative) documents of q′. In this way, we can synthesize massive target-domain weak supervision {(qj ′, d+j ′ , d−j ′ ), 1 ≤ j ≤ N}."
    }, {
      "heading" : "3.3 Meta Learning to Reweight",
      "text" : "The synthetic weak data inevitably contain noises. To distinguish more useful training data for neural rankers, MetaAdaptRank meta-learns to reweight these synthetic data, following Ren et al. (2018).\nMeta Learning Objective. Given a large volume of synthetic data {(qj ′, d+j ′ , d−j ′ ), 1 ≤ j ≤ N} and a handful of target data {(qi, d+i , d − i ), 1 ≤ i ≤ M} (M N ), our meta-learning objective is to find the optimal weights w∗ on synthetic data to better train neural rankers. The learning of w∗ involves two nested loops of optimization: initialweighted synthetic data is used to pseudo-optimize the neural ranker; the weights is then optimized by minimizing the neural ranking loss on target data.\nTo be specific, the first loop (Meta-forward Update) incorporates the initial weights w into the learning parameters θ̃(w) instead of truly optimizing the neural ranker:\nθ̃(w) = argmin θ N∑ j=1 wj l ′ j(θ), (7)\nwhere l′j(θ) is the ranking loss on a synthetic instance (qj ′, d+j ′ , d−j ′ ). In the second loop (Metabackward Update), the optimal weights w∗ can be obtained by minimizing the target ranking loss:\nw∗ = argmin w M∑ i=1 li(θ̃(w)), (8)\nwhere li(θ) is the ranking loss on a target instance (qi, d + i , d − i ). The calculation of each loop can be very expensive. In practice, we only perform onestep optimization in the two loops with mini-batch data, consistent with prior work (Ren et al., 2018).\nMeta-forward Update. Taking the t-th training step as an example, we first assign a set of initial weightsw = {wj}nj=1 to the synthetic training data batch and then pseudo-update the neural ranker’s parameters to θ̃t+1(w):\nθ̃t+1(w) = θt − α ∂ ∂(θt) n∑ j=1 wj l ′ j(θ t), (9)\nwhere α is the learning rate. The description here uses vanilla SGD and other optimizers can be used.\nMeta-backward Update. We leverage the neural ranker θ̃t+1(w) to calculate the ranking loss on the target data batch and obtain the optimal weights w∗ = {w∗j}nj=1 through a single optimization step:\nw∗j = wj − η ∂\n∂(wj) m∑ i=1 1 m li(θ̃ t+1(w)), (10)\nwhere η is the learning rate for optimizing weights. The weights are further normalized for stable training. More details are shown in Appendices A.1."
    }, {
      "heading" : "3.4 Training with Meta-Weights",
      "text" : "After obtaining the optimal weights w∗, the optimization of the neural ranker is a standard backpropagation on the weighted loss of synthetic data:\nθt+1 = argmin θt n∑ j=1 w∗j l ′ j(θ t). (11)\nIn each training step, MetaAdaptRank first learns to reweight the synthetic batch based on their metaimpact on the target batch and then updates the neural ranker with the weighted synthetic batch. In this way, the few-shot target data can serve more as a “regularizer” to help the neural ranker to generalize with synthetic data, instead of as direct supervision which requires more labels (Ren et al., 2018)."
    }, {
      "heading" : "4 Experimental Methodology",
      "text" : "This section describes our experimental settings and implementation details.\nDatasets. As shown in Table 1, three standard TREC datasets with different domains are used in our experiments: ClueWeb09-B (Callan et al., 2009), Robust04 (Kwok et al., 2004), and TRECCOVID (Roberts et al., 2020). They are all fewshot ad-hoc retrieval datasets where the number of labeled queries is limited. We leverage the “Complete” version of TREC-COVID whose retrieval document set is the July 16, 2020 release of CORD19 (Wang et al., 2020a), a growing collection of scientific papers on COVID-19 and related research.\nEvaluation Settings. We evaluate supervised IR methods through re-ranking the top 100 documents from the first-stage retrieval with five-fold cross-validation, consistent with prior work (Xiong et al., 2017a; Dai and Callan, 2019; Zhang et al., 2020b). The first-stage retrieval for ClueWeb09-B and Robust04 is the sequential dependence model\n(SDM) (Metzler and Croft, 2005) released by Dai and Callan (2019), and the first-stage retrieval for TREC-COVID is BM25 (Robertson and Zaragoza, 2009) well-tuned by Anserini (Yang et al., 2017).\nMetrics. NDCG@20 is used as the primary metric for all datasets. We also report ERR@20 for ClueWeb09-B and Robust04, which is the same with prior work (Zhang et al., 2020b), and report P@20 for TREC-COVID. Statistic significance is examined by permutation test with p < 0.05.\nBaselines. Two groups of baselines are compared in our experiments, including Traditional IR Baselines and Neural IR Baselines.\nTraditional IR Baselines. Following previous research (Dai and Callan, 2019; Zhang et al., 2020b), we compare four traditional IR methods in our experiments. They are two unsupervised methods, BM25 (Robertson and Zaragoza, 2009) and SDM (Metzler and Croft, 2005), and two learningto-rank (LTR) methods using bag-of-word features, RankSVM (Joachims, 2002) and Coor-Ascent (Coordinate Ascent) (Metzler and Croft, 2007).\nNeural IR Baselines. We also compare seven Neu-IR baselines that utilize different methodologies to train neural rankers. In our experiments, all Neu-IR methods adopt the widely-used BERT ranker (Nogueira and Cho, 2019), BERT-FirstP, which only uses the first paragraph of documents.\nThe vanilla neural baseline only leverages the existing small-scale relevance labels of target datasets to train BERT rankers, which is named Few-shot Supervision. We also compare BERT rankers trained with two large-scale supervision sources: Bing User Click and MS MARCO. Dai and Callan (2019) train BERT rankers with 5 million user click logs in Bing. We borrow their reported results because commercial logs are not publicly available. MS MARCO is a human supervision source (Nguyen et al., 2016), which provides over one million Bing queries with relevance labels.\nFour weak supervision methods are also compared. One baseline is Title Fitler, which treats filtered title-document pairs as weak supervision signals (MacAvaney et al., 2019) for training BERT rankers (Zhang et al., 2020b). Another two baselines are Anchor and ReInfoSelect. Anchor leverages 100k pairs of anchor texts and web pages to train BERT rankers (Zhang et al., 2020b). ReInfoSelect first employs reinforcement learning to select these anchor signals (Zhang et al., 2020b) and then trains BERT rankers. The\nlast baseline SyncSup trains BERT rankers with synthetic weak supervision data, which are synthesized based on the previous work (Ma et al., 2021).\nImplementation Details. This part introduces the implement details of our method and baselines.\nBERT Ranker. For our methods and all Neu-IR baselines, we use the base version of BERT (Devlin et al., 2019) on ClueWeb09-B and Robust04, and PubMedBERT (Base) (Gu et al., 2020) on TRECCOVID. We leverage the OpenMatch (Liu et al., 2021) implementation and obtain the pre-trained weights from Hugging Face (Wolf et al., 2020).\nFor all Neu-IR methods, we first use additional supervision sources such as weak supervision signals to train BERT rankers (except for Few-shot Supervision); then fine-tune the BERT rankers with the training folds of target datasets in the crossvalidation. Following prior work (Dai and Callan, 2019; Zhang et al., 2020b), the ranking features ([CLS] embeddings) of BERT are combined with the first-stage retrieval scores using Coor-Ascent for ClueWeb09-B and Robust04. We set the max input length to 512 and use Adam optimizer with a learning rate of 2e-5 and a batch size of 8.\nContrastive Supervision Synthesis. We use the small version of T5 (60 million parameters) as the NLG models in MetaAdaptRank, and leverage MS MARCO as the training data for T5-NLG models. We set the maximum input length to 512 and use Adam to optimize the T5-NLG models with a learning rate of 2e-5 and a batch size of 4. In inference, the T5-NLG models are applied on target datasets with greedy search. Additionally, we consider CTSyncSup as our ablation baseline, which directly trains BERT rankers on contrastive synthetic supervision data without meta-reweighting.\nMeta Learning to Reweight. The training folds of the target dataset are used as target data to guide the meta-reweighting to synthetic data. We set the batch size of synthetic data (n) and target data (m) to 8. The second-order gradient of the target ranking loss with regard to the initial weight (Eq. 10) is implemented using the automatic differentiation in PyTorch (Paszke et al., 2017)."
    }, {
      "heading" : "5 Evaluation Results",
      "text" : "In this section, we present the evaluation results of MetaAdaptRank and conduct a series of analyses and case studies to study its effectiveness."
    }, {
      "heading" : "5.1 Overall Accuracy",
      "text" : "The ranking results of MetaAdaptRank and baselines are presented in Table 2.\nOn all benchmarks and metrics, MetaAdaptRank outperforms all baselines stably. Compared to the best feature-based LeToR method, Coor-Ascent, MetaAdaptRank outperforms it by more than 15%. MetaAdaptRank even outperforms the strong NeuIR baselines supervised with Bing User Click and MS MARCO, which demonstrates its effectiveness.\nSpecifically, CTSyncSup directly improves the few-shot ranking accuracy of BERT rankers by 3% on all benchmarks. In comparison to other weak supervision sources, filtered title-document relations, Anchor and SyncSup, CTSyncSup shows more stable effectiveness across different benchmarks, revealing its domain-adaption advantages. Moreover, meta-reweighting CTSyncSup brings further improvement and helps MetaAdaptRank outperform the latest selective Neu-IR method ReInfoSelect.\nNext, we go ahead to analyze MetaAdaptRank’s contrastive synthesis and meta-reweighting."
    }, {
      "heading" : "5.2 Effectiveness of Contrastive Synthesis",
      "text" : "We analyze contrastive synthesis’s effectiveness by its effect on ranking results and synthetic quality.\nTable 3 presents the ranking accuracy based on our CTSyncSup and four other supervision sources. CTSyncSup outperforms Anchor and SyncSup stably across all datasets. On Robust04, CTSyncSup even shows better performance than MS MARCO human labels. Besides, combining the sources of MS MARCO and CTSyncSup can further improve the ranking accuracy on ClueWeb09-B and TRECCOVID, revealing that CTSyncSup provides useful supervision signals applicable to various domains.\nWe further evaluate the quality of the queries generated in SyncSup and our CTSyncSup, which are both synthetic methods for generating queries based on target documents. Following previous research (Ma et al., 2021; Yu et al., 2020; Celikyilmaz et al., 2020), eight auto evaluation metrics are used in our evaluation. As shown in Table 4, CTSyncSup outperforms SyncSup on all metrics. The results demonstrate that the contrastive pair of positive and negative documents does help the\nNLG model better approximate the golden queries. In addition, reversing the encoding order of the contrastive document pair causes a dramatic decrease in all evaluation scores of the generated queries. This further shows that our contrastive query generator can extract more specific and representative information from the positive documents, thereby generating more discriminative queries."
    }, {
      "heading" : "5.3 Effectiveness of Meta Reweighting",
      "text" : "To analyze the effectiveness of meta reweighting, we employ MetaAdaptRank on different supervision sources and study its data weighting behaviors in the learning process. The reinforcement data selector ReInfoSelect is used as a comparison, which utilizes the trial-and-error weighting mechanism.\nThe ranking accuracy of MetaAdaptRank and\nReInfoSelect trained with MS MARCO, Anchor, and CTSyncSup is presented in Table 5. For all supervision sources, MetaAdaptRank outperforms ReInfoSelect on all benchmarks. The results show that the meta-reweighting mechanism can more effectively explore the potential of different supervision sources compared to the trial-and-error weighting mechanism. Moreover, the advantages of meta reweighting can be extended to the hybrid supervision source of MS MARCO and CTSyncSup.\nTo further understand the behaviors of meta reweighting, we compare the state of weights assigned to synthetic supervision by MetaAdaptRank and ReInfoSelect in the learning process, using CTSyncSup as synthetic data and ClueWeb09 as target data. The results are shown in Figure 2. Even though each synthetic batch is likely to include both\nuseful and noisy data points, ReInfoSelect always assigns very high weights at the beginning and discards almost all synthetic data points later. Besides, its tight confidence interval reveals that data points in the same batch received almost identical weights. These observations indicate that ReInfoSelect does not effectively distinguish useful synthetic data points from the noisy ones during the learning process. By contrast, MetaAdaptRank assigns higher weights initially and steadily reduces the weights as training goes on. More importantly, its wide confidence interval reveals that the data weights in the same synthetic batch vary significantly, which are thus expected to be more diverse and fine-grained."
    }, {
      "heading" : "5.4 Effectiveness of Hybrid Supervision",
      "text" : "We also analyze MetaAdaptRank’s advantages on the hybrid supervision source of MS MARCO and CTSyncSup. The impact of the hybrid source on its ranking accuracy and meta-reweighting behavior is studied. Besides, we evaluate MetaAdaptRank trained with the hybrid source in Round 5 of the TREC-COVID shared task in which many strong baselines have been well-tuned for four rounds.\nFigure 3a shows the Win/Tie ranking accuracy of MetaAdaptRank trained with MS MARCO and the\nhybrid supervision source. Compared to the single MS MARCO, the hybrid source has more advantages across all benchmarks. Besides, the hybrid advantage seems to be more evident in non-web domain benchmarks, especially on TREC-COVID.\nWe further investigate the weighting behavior of MetaAdaptRank on MS MARCO and the hybrid source, using the same ClueWeb09 target data in previous analyses. Figure 3b illustrates the changes in meta-learned weights of randomly sampled 2k MS MARCO data points before and after merging CTSyncSup source. There are significant weight variations on most MS MARCO data points before and after merging CTSyncSup. Additionally, merging CTSyncSup reduces the weight of more MS MARCO data points, revealing that CTSyncSup data are assigned higher weights. This also reveals that MetaAdaptRank can tailor diversified weights for the same data points in different sources and up-weights more useful training data flexibly.\nLastly, we report the TREC-COVID R5 ranking results of MetaAdaptRank trained with the hybrid source. The top 2 automatic search systems in the R5 leaderboard are compared, which outperforms other systems on the newly added queries in R5. The evaluation of these new queries is fair to our\nmethods and those systems that underwent previous rounds (R1-R4). As shown in Table 6, our single model outperforms the top 2 fusion-based systems on all evaluation of the new, old, and all queries, further showing the effectiveness of MetaAdaptRank with the hybrid supervision source. More details and ranking results are shown in Appendices A.2."
    }, {
      "heading" : "5.5 Case Studies",
      "text" : "Table 7 exhibits some cases of contrastive synthetic data for ClueWeb09 and their meta-learned weights. More cases are shown in Appendices A.3.\nCTSyncSup can extract more specific contents from the positive documents, e.g., “shopping with the planet” and “make a big difference” in the first case; SyncSup captures more general information, e.g., “green energy”. Compared to SyncSup’s queries such as “where is jamestown beach” in the second case, the synthetic queries in CTSyncSup are more informative and discriminative. Noticeably, the second case exhibits the synthetic noise, where the positive document is actually related to “bermuda’s tourism” instead of the query “history of bermuda”. MetaAdaptRank effectively filters this noisy instance by assigning a zero weight to it."
    }, {
      "heading" : "6 Conclusion",
      "text" : "This paper presents MetaAdaptRank, a domain adaption method for few-shot Neu-IR with contrastive weak data synthesis and meta-reweighted data selection. Contrastive synthesis generates informative queries and useful synthetic supervision signals. Meta-learned weights form high-resolution channels between target labels and synthetic signals, providing robust and fine-grained data selection for synthetic weak supervision. Both of them collaborate to significantly improve the neural ranking accuracy in various few-shot search scenarios."
    }, {
      "heading" : "Acknowledgments",
      "text" : "This work is partly supported by the National Key Research and Development Program of China (No. 2020AAA0106501) and Beijing Academy of Artificial Intelligence (BAAI). We thank Zhuyun Dai and Jamie Callan for sharing the SDM results on ClueWeb09-B and Robust04 and thank Shi Yu for discussions in the query generation methodologies."
    }, {
      "heading" : "A Appendices",
      "text" : "A.1 Batch Normalization of Meta-Weights This part elaborates the batch normalization process for meta-learned weights. Following prior research (Ren et al., 2018), we first set the initial weights w to zeros and obtain the new weights w̃:\nw̃j = −η ∂\n∂(wj) m∑ i=1 1 m li(θ̃ t+1(w)) ∣∣∣ wj=0 . (12)\nThen we clip w̃ to get non-negative weights ŵ and further normalize them in the batch to obtain the final weights w∗:\nŵj = max(0, w̃j),\nw∗j = ŵj ( ∑n p=1 ŵp) + δ( ∑n p=1 ŵp) .\n(13) Here δ( ∑n p=1 ŵp) = 1 when ∑n p=1 ŵp = 0, to prevent division errors, otherwise it is 0. With the batch-normalization process, the hyperparameter η can be effectively eliminated. The normalization method is not constrained and other approaches can also be used (Shu et al., 2019; Hu et al., 2019).\nA.2 Supplementary Results of TREC-COVID R5\nThis part supplements our evaluation results in the TREC-COVID R5 shared task. We will first recap the shared task and then present more evaluation results and our implementation details.\nTREC-COVID R5. The TREC-COVID Challenge is an ad-hoc ranking task for COVID-19 literature, consisting of five rounds. TREC-COVID R5 is the last round of this challenge, where the document set is the July 16, 2020 version of CORD-19,\nand the query set contains 50 testing queries. The first 45 queries have been used in previous rounds (R1-R4), and the last five queries are newly added in R5. As in previous rounds, TREC-COVID R5 adopts residual collection evaluation (Salton and Buckley, 1997). In residual collection evaluation, the relevance labels from previous rounds can be used, but any document that has been annotated for a query will be removed before the evaluation. We focus more on the evaluation of R5’s new queries because these queries have no prior relevance labels, which is fairer to our models and those search systems that underwent previous rounds.\nEvaluation Results. Table 8 shows the evaluation results on the new queries of TREC-COVID R5, including three variants of our MetaAdaptRank and the top 10 feedback systems in the R5 leaderboard. Compared with the top 10 feedback systems (many are fusion-based systems), our single model MetaAdaptRank (rerank fusion.2) outperforms all baselines, demonstrating the generalization ability of our method on new queries.\nAdditionally, what catches our attention is that the best and worst of the top 10 feedback systems only have a 5.1% difference in NDCG@20 scores on all queries, while their NDCG@20 scores on the new queries differ by 13.4%. This discrepancy indicates that the residual collection evaluation may have biases between the seen and unseen queries.\nImplementation Details. We next describe the implementation details of the three variants of our MetaAdaptRank in TREC-COVID R5. Consistent with the implementation methods described in Section 4, we rerank the top 100 documents from the first-stage retrieval. We first borrow two retrieval results with different settings provided by Anserini BM25 (Row 7 and 8 of Table Round 51). Then PudMedBERT (Base) is used to rerank these two retrieval results to obtain MetaAdaptRank (rerank fusion.1) and MetaAdaptRank (rerank fusion.2), respectively. MetaAdaptRank (RRF) is the reciprocal rank fusion of these two models. We utilize the open-source library trec-tools (Palotti et al., 2019) to implement RRF and set the fusion weight k to 1.\nTo train MetaAdaptRank, we first synthesize CTSyncSup data based on R5’s document set and leverage the hybrid source of CTSyncSup and MS MARCO as the additional supervision signals. The training process contains two stages. We first train\n1https://github.com/castorini/ anserini/blob/master/docs/ experiments-covid.md\nMetaAdaptRank with the hybrid source and regard the labeled data from previous rounds (R1-R4) as target data in meta-reweighting. Then we continuously train MetaAdaptRank using the labeled data from the previous rounds. In the training processes, we utilize Adam optimizer with a learning rate of 2e-5. Both the batch size and the accumulation step are set to 8. In addition, to ensure a fair comparison with the submitted search systems, we post-process our results according to official guidelines.\nA.3 Supplementary Case Studies Table 9 shows more cases for the other two datasets, Robust04 (News) and TREC-COVID (BioMed), to verify the effectiveness of MetaAdaptRank in different domains. The first three cases are from Robust04, and the rest cases are from TREC-COVID.\nFor the first synthetic cases, our CTSyncSup can extract characteristic keywords, e.g., “radars” and “colombia”, from the positive documents to generate more informative queries, while SyncSup tends to capture general keywords to create broad queries, which may lack the ability to distinguish between different documents. Besides, CTSyncSup can extract some necessary themes from the specific documents, such as the particular time “1993” and the adjective “increased”, as shown in the second case.\nMoreover, cases 4 and 5 show the effectiveness of our contrastive synthesis for biomedical domains. CTSyncSup can capture “lung” and “quarantine prevent” instead of general keywords, such as “sars cov” and “symptoms” often mentioned in COVIDrelated documents. These observations show that CTSyncSup can extract more specific information\nto generate more informative and discriminative queries for different target domains.\nWe further explore those synthetic instances that are assigned zero weights by MetaAdaptRank, such as the third and sixth cases. In the third case, although CTSyncSup captures the two keywords “language” and “osvaldo rodrigrez” from the positive document, its synthetic query is actually less relevant to the main topic of the positive document. For the sixth case, CTSyncSup fails to exclude the\nphrase “covid-19 pandemic” related to both the positive and negative documents, which causes the synthetic query unable to distinguish between them. Fortunately, MetaAdaptRank can effectively identify the synthetic instances whose relevance matching patterns between synthetic queries and positive documents are unclear or non-unique and then precludes such misleading synthetic supervision data by assigning them zero weights."
    } ],
    "references" : [ {
      "title" : "Evaluation of text generation: A survey",
      "author" : [ "Asli Celikyilmaz", "Elizabeth Clark", "Jianfeng Gao." ],
      "venue" : "arXiv preprint arXiv:2006.14799.",
      "citeRegEx" : "Celikyilmaz et al\\.,? 2020",
      "shortCiteRegEx" : "Celikyilmaz et al\\.",
      "year" : 2020
    }, {
      "title" : "Reading wikipedia to answer opendomain questions",
      "author" : [ "Danqi Chen", "Adam Fisch", "Jason Weston", "Antoine Bordes." ],
      "venue" : "Proceedings of ACL, pages 1870–1879.",
      "citeRegEx" : "Chen et al\\.,? 2017",
      "shortCiteRegEx" : "Chen et al\\.",
      "year" : 2017
    }, {
      "title" : "Using ODP metadata to personalize search",
      "author" : [ "Paul Alexandru Chirita", "Wolfgang Nejdl", "Raluca Paiu", "Christian Kohlschütter." ],
      "venue" : "Proceedings of SIGIR, pages 178–185.",
      "citeRegEx" : "Chirita et al\\.,? 2005",
      "shortCiteRegEx" : "Chirita et al\\.",
      "year" : 2005
    }, {
      "title" : "Overview of the TREC 2019 deep learning track",
      "author" : [ "Nick Craswell", "Bhaskar Mitra", "Emine Yilmaz", "Daniel Campos", "Ellen M Voorhees." ],
      "venue" : "arXiv preprint arXiv:2003.07820.",
      "citeRegEx" : "Craswell et al\\.,? 2020",
      "shortCiteRegEx" : "Craswell et al\\.",
      "year" : 2020
    }, {
      "title" : "Deeper text understanding for IR with contextual neural language modeling",
      "author" : [ "Zhuyun Dai", "Jamie Callan." ],
      "venue" : "Proceedings of SIGIR, pages 985–988.",
      "citeRegEx" : "Dai and Callan.,? 2019",
      "shortCiteRegEx" : "Dai and Callan.",
      "year" : 2019
    }, {
      "title" : "Convolutional neural networks for soft-matching n-grams in ad-hoc search",
      "author" : [ "Zhuyun Dai", "Chenyan Xiong", "Jamie Callan", "Zhiyuan Liu." ],
      "venue" : "Proceedings of WSDM, pages 126–134.",
      "citeRegEx" : "Dai et al\\.,? 2018",
      "shortCiteRegEx" : "Dai et al\\.",
      "year" : 2018
    }, {
      "title" : "Neural ranking models with weak supervision",
      "author" : [ "Mostafa Dehghani", "Hamed Zamani", "Aliaksei Severyn", "Jaap Kamps", "W Bruce Croft." ],
      "venue" : "Proceedings of SIGIR, pages 65–74.",
      "citeRegEx" : "Dehghani et al\\.,? 2017",
      "shortCiteRegEx" : "Dehghani et al\\.",
      "year" : 2017
    }, {
      "title" : "BERT: pre-training of deep bidirectional transformers for language understanding",
      "author" : [ "Jacob Devlin", "Ming-Wei Chang", "Kenton Lee", "Kristina Toutanova." ],
      "venue" : "Proceedings of NAACL-HLT, pages 4171–4186.",
      "citeRegEx" : "Devlin et al\\.,? 2019",
      "shortCiteRegEx" : "Devlin et al\\.",
      "year" : 2019
    }, {
      "title" : "Heads and tails: studies of web search with common and rare queries",
      "author" : [ "Doug Downey", "Susan Dumais", "Eric Horvitz." ],
      "venue" : "Proceedings of SIGIR, pages 847–848.",
      "citeRegEx" : "Downey et al\\.,? 2007",
      "shortCiteRegEx" : "Downey et al\\.",
      "year" : 2007
    }, {
      "title" : "Domainspecific language model pretraining for biomedical natural language processing",
      "author" : [ "Yu Gu", "Robert Tinn", "Hao Cheng", "Michael Lucas", "Naoto Usuyama", "Xiaodong Liu", "Tristan Naumann", "Jianfeng Gao", "Hoifung Poon." ],
      "venue" : "arXiv preprint",
      "citeRegEx" : "Gu et al\\.,? 2020",
      "shortCiteRegEx" : "Gu et al\\.",
      "year" : 2020
    }, {
      "title" : "A deep relevance matching model for ad-hoc retrieval",
      "author" : [ "Jiafeng Guo", "Yixing Fan", "Qingyao Ai", "W. Bruce Croft." ],
      "venue" : "Proceedings of CIKM, pages 55–",
      "citeRegEx" : "Guo et al\\.,? 2016",
      "shortCiteRegEx" : "Guo et al\\.",
      "year" : 2016
    }, {
      "title" : "Challenges in enterprise search",
      "author" : [ "David Hawking." ],
      "venue" : "Proceedings of ADC, pages 15–24.",
      "citeRegEx" : "Hawking.,? 2004",
      "shortCiteRegEx" : "Hawking.",
      "year" : 2004
    }, {
      "title" : "Local self-attention over long text for efficient document retrieval",
      "author" : [ "Sebastian Hofstätter", "Hamed Zamani", "Bhaskar Mitra", "Nick Craswell", "Allan Hanbury." ],
      "venue" : "Proceedings of SIGIR, page 2021–2024.",
      "citeRegEx" : "Hofstätter et al\\.,? 2020",
      "shortCiteRegEx" : "Hofstätter et al\\.",
      "year" : 2020
    }, {
      "title" : "Learning data manipulation for augmentation and weighting",
      "author" : [ "Zhiting Hu", "Bowen Tan", "Russ R Salakhutdinov", "Tom M Mitchell", "Eric P Xing." ],
      "venue" : "Proceedings of NeurIPS, pages 15764–15775.",
      "citeRegEx" : "Hu et al\\.,? 2019",
      "shortCiteRegEx" : "Hu et al\\.",
      "year" : 2019
    }, {
      "title" : "PACRR: A position-aware neural IR model for relevance matching",
      "author" : [ "Kai Hui", "Andrew Yates", "Klaus Berberich", "Gerard de Melo." ],
      "venue" : "Proceedings of EMNLP, pages 1049–1058.",
      "citeRegEx" : "Hui et al\\.,? 2017",
      "shortCiteRegEx" : "Hui et al\\.",
      "year" : 2017
    }, {
      "title" : "Optimizing search engines using clickthrough data",
      "author" : [ "Thorsten Joachims." ],
      "venue" : "Proceedings of KDD, pages 133–142.",
      "citeRegEx" : "Joachims.,? 2002",
      "shortCiteRegEx" : "Joachims.",
      "year" : 2002
    }, {
      "title" : "TREC 2004 robust track experiments using PIRCS",
      "author" : [ "Kui-Lam Kwok", "Laszlo Grunfeld", "HL Sun", "Peter Deng", "N Dinstl." ],
      "venue" : "Proceedings of TREC.",
      "citeRegEx" : "Kwok et al\\.,? 2004",
      "shortCiteRegEx" : "Kwok et al\\.",
      "year" : 2004
    }, {
      "title" : "Latent retrieval for weakly supervised open domain question answering",
      "author" : [ "Kenton Lee", "Ming-Wei Chang", "Kristina Toutanova." ],
      "venue" : "Proceedings of ACL, pages 6086–6096.",
      "citeRegEx" : "Lee et al\\.,? 2019",
      "shortCiteRegEx" : "Lee et al\\.",
      "year" : 2019
    }, {
      "title" : "The neural hype and comparisons against weak baselines",
      "author" : [ "Jimmy Lin." ],
      "venue" : "ACM SIGIR Forum, volume 52, pages 40–51.",
      "citeRegEx" : "Lin.,? 2019",
      "shortCiteRegEx" : "Lin.",
      "year" : 2019
    }, {
      "title" : "Learning to rank for information retrieval",
      "author" : [ "Tie-Yan Liu." ],
      "venue" : "Foundations and Trends in Information Retrieval, 3(3):225–331.",
      "citeRegEx" : "Liu.,? 2009",
      "shortCiteRegEx" : "Liu.",
      "year" : 2009
    }, {
      "title" : "Fine-grained fact verification with kernel graph attention network",
      "author" : [ "Zhenghao Liu", "Chenyan Xiong", "Maosong Sun", "Zhiyuan Liu." ],
      "venue" : "Proceedings of ACL, pages 7342–7351.",
      "citeRegEx" : "Liu et al\\.,? 2020",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2020
    }, {
      "title" : "OpenMatch: An open source library for Neu-IR research",
      "author" : [ "Zhenghao Liu", "Kaitao Zhang", "Chenyan Xiong", "Zhiyuan Liu", "Maosong Sun." ],
      "venue" : "arXiv preprint arXiv:2102.00166.",
      "citeRegEx" : "Liu et al\\.,? 2021",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2021
    }, {
      "title" : "Zero-shot neural passage retrieval via domain-targeted synthetic question generation",
      "author" : [ "Ji Ma", "Ivan Korotkov", "Yinfei Yang", "Keith Hall", "Ryan McDonald." ],
      "venue" : "Proceedings of EACL, pages 1075–1088.",
      "citeRegEx" : "Ma et al\\.,? 2021",
      "shortCiteRegEx" : "Ma et al\\.",
      "year" : 2021
    }, {
      "title" : "Content-based weak supervision for ad-hoc re-ranking",
      "author" : [ "Sean MacAvaney", "Andrew Yates", "Kai Hui", "Ophir Frieder." ],
      "venue" : "Proceedings of SIGIR, pages 993–996.",
      "citeRegEx" : "MacAvaney et al\\.,? 2019",
      "shortCiteRegEx" : "MacAvaney et al\\.",
      "year" : 2019
    }, {
      "title" : "A markov random field model for term dependencies",
      "author" : [ "Donald Metzler", "W Bruce Croft." ],
      "venue" : "Proceedings of SIGIR, pages 472–479.",
      "citeRegEx" : "Metzler and Croft.,? 2005",
      "shortCiteRegEx" : "Metzler and Croft.",
      "year" : 2005
    }, {
      "title" : "Linear feature-based models for information retrieval",
      "author" : [ "Donald Metzler", "W Bruce Croft." ],
      "venue" : "Information Retrieval, 10(3):257–274.",
      "citeRegEx" : "Metzler and Croft.,? 2007",
      "shortCiteRegEx" : "Metzler and Croft.",
      "year" : 2007
    }, {
      "title" : "An introduction to neural information retrieval. Foundations and Trends® in Information Retrieval, 13(1):1–126",
      "author" : [ "Bhaskar Mitra", "Nick Craswell" ],
      "venue" : null,
      "citeRegEx" : "Mitra and Craswell,? \\Q2018\\E",
      "shortCiteRegEx" : "Mitra and Craswell",
      "year" : 2018
    }, {
      "title" : "MS MARCO: A human generated machine reading comprehension dataset",
      "author" : [ "Tri Nguyen", "Mir Rosenberg", "Xia Song", "Jianfeng Gao", "Saurabh Tiwary", "Rangan Majumder", "Li Deng." ],
      "venue" : "Proceedings of CoCo@ NIPS, volume 1773.",
      "citeRegEx" : "Nguyen et al\\.,? 2016",
      "shortCiteRegEx" : "Nguyen et al\\.",
      "year" : 2016
    }, {
      "title" : "Passage re-ranking with BERT",
      "author" : [ "Rodrigo Nogueira", "Kyunghyun Cho." ],
      "venue" : "arXiv preprint arXiv:1901.04085.",
      "citeRegEx" : "Nogueira and Cho.,? 2019",
      "shortCiteRegEx" : "Nogueira and Cho.",
      "year" : 2019
    }, {
      "title" : "TrecTools: an open-source Python library for information retrieval practitioners involved in TREC-like campaigns",
      "author" : [ "Joao Palotti", "Harrisen Scells", "Guido Zuccon." ],
      "venue" : "Proceedings of SIGIR, pages 1325– 1328.",
      "citeRegEx" : "Palotti et al\\.,? 2019",
      "shortCiteRegEx" : "Palotti et al\\.",
      "year" : 2019
    }, {
      "title" : "Automatic differentiation in PyTorch",
      "author" : [ "Adam Paszke", "Sam Gross", "Soumith Chintala", "Gregory Chanan", "Edward Yang", "Zachary DeVito", "Zeming Lin", "Alban Desmaison", "Luca Antiga", "Adam Lerer." ],
      "venue" : "Proceedings of NeurIPS Autodiff Workshop.",
      "citeRegEx" : "Paszke et al\\.,? 2017",
      "shortCiteRegEx" : "Paszke et al\\.",
      "year" : 2017
    }, {
      "title" : "Understanding the behaviors of BERT in ranking",
      "author" : [ "Yifan Qiao", "Chenyan Xiong", "Zheng-Hao Liu", "Zhiyuan Liu." ],
      "venue" : "arXiv preprint arXiv:1904.07531.",
      "citeRegEx" : "Qiao et al\\.,? 2019",
      "shortCiteRegEx" : "Qiao et al\\.",
      "year" : 2019
    }, {
      "title" : "Open-retrieval conversational question answering",
      "author" : [ "Chen Qu", "Liu Yang", "Cen Chen", "Minghui Qiu", "W Bruce Croft", "Mohit Iyyer." ],
      "venue" : "Proceedings of SIGIR, pages 539–548.",
      "citeRegEx" : "Qu et al\\.,? 2020",
      "shortCiteRegEx" : "Qu et al\\.",
      "year" : 2020
    }, {
      "title" : "Exploring the limits of transfer learning with a unified text-to-text transformer",
      "author" : [ "Colin Raffel", "Noam Shazeer", "Adam Roberts", "Katherine Lee", "Sharan Narang", "Michael Matena", "Yanqi Zhou", "Wei Li", "Peter J Liu." ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "Raffel et al\\.,? 2020",
      "shortCiteRegEx" : "Raffel et al\\.",
      "year" : 2020
    }, {
      "title" : "Learning to reweight examples for robust deep learning",
      "author" : [ "Mengye Ren", "Wenyuan Zeng", "Bin Yang", "Raquel Urtasun." ],
      "venue" : "Proceedings of ICML, pages 4331–4340.",
      "citeRegEx" : "Ren et al\\.,? 2018",
      "shortCiteRegEx" : "Ren et al\\.",
      "year" : 2018
    }, {
      "title" : "TREC-COVID: rationale and structure of an information retrieval shared task for COVID-19",
      "author" : [ "Kirk Roberts", "Tasmeer Alam", "Steven Bedrick", "Dina Demner-Fushman", "Kyle Lo", "Ian Soboroff", "Ellen Voorhees", "Lucy Lu Wang", "William R Hersh" ],
      "venue" : null,
      "citeRegEx" : "Roberts et al\\.,? \\Q2020\\E",
      "shortCiteRegEx" : "Roberts et al\\.",
      "year" : 2020
    }, {
      "title" : "The probabilistic relevance framework: BM25 and beyond",
      "author" : [ "Stephen Robertson", "Hugo Zaragoza." ],
      "venue" : "Foundations and Trends® in Information Retrieval, 3(4):333–389.",
      "citeRegEx" : "Robertson and Zaragoza.,? 2009",
      "shortCiteRegEx" : "Robertson and Zaragoza.",
      "year" : 2009
    }, {
      "title" : "Improving retrieval performance by relevance feedback",
      "author" : [ "G. Salton", "Chris Buckley." ],
      "venue" : "Journal of the Association for Information Science and Technology, 41(4):355–364.",
      "citeRegEx" : "Salton and Buckley.,? 1997",
      "shortCiteRegEx" : "Salton and Buckley.",
      "year" : 1997
    }, {
      "title" : "Meta-WeightNet: learning an explicit mapping for sample weighting",
      "author" : [ "Jun Shu", "Qi Xie", "Lixuan Yi", "Qian Zhao", "Sanping Zhou", "Zongben Xu", "Deyu Meng." ],
      "venue" : "Proceedings of NeurIPS, pages 1919–1930.",
      "citeRegEx" : "Shu et al\\.,? 2019",
      "shortCiteRegEx" : "Shu et al\\.",
      "year" : 2019
    }, {
      "title" : "2020a. CORD-19: the covid-19 open research dataset",
      "author" : [ "Lucy Lu Wang", "Kyle Lo", "Yoganand Chandrasekhar", "Russell Reas", "Jiangjiang Yang", "Darrin Eide", "Kathryn Funk", "Rodney Kinney", "Ziyang Liu", "William Merrill" ],
      "venue" : "In Proceedings of the 1st Workshop on NLP",
      "citeRegEx" : "Wang et al\\.,? \\Q2020\\E",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2020
    }, {
      "title" : "Balancing training for multilingual neural machine translation",
      "author" : [ "Xinyi Wang", "Yulia Tsvetkov", "Graham Neubig." ],
      "venue" : "Proceedings of ACL, pages 8526–8537.",
      "citeRegEx" : "Wang et al\\.,? 2020b",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2020
    }, {
      "title" : "Transformers: State-of-theart natural language processing",
      "author" : [ "Thomas Wolf", "Julien Chaumond", "Lysandre Debut", "Victor Sanh", "Clement Delangue", "Anthony Moi", "Pierric Cistac", "Morgan Funtowicz", "Joe Davison", "Sam Shleifer" ],
      "venue" : null,
      "citeRegEx" : "Wolf et al\\.,? \\Q2020\\E",
      "shortCiteRegEx" : "Wolf et al\\.",
      "year" : 2020
    }, {
      "title" : "Word-entity duet representations for document ranking",
      "author" : [ "Chenyan Xiong", "Jamie Callan", "Tie-Yan Liu." ],
      "venue" : "Proceedings of SIGIR, pages 763–772.",
      "citeRegEx" : "Xiong et al\\.,? 2017a",
      "shortCiteRegEx" : "Xiong et al\\.",
      "year" : 2017
    }, {
      "title" : "End-to-end neural ad-hoc ranking with kernel pooling",
      "author" : [ "Chenyan Xiong", "Zhuyun Dai", "Jamie Callan", "Zhiyuan Liu", "Russell Power." ],
      "venue" : "Proceedings of SIGIR, pages 55–64.",
      "citeRegEx" : "Xiong et al\\.,? 2017b",
      "shortCiteRegEx" : "Xiong et al\\.",
      "year" : 2017
    }, {
      "title" : "Approximate nearest neighbor negative contrastive learning for dense text retrieval",
      "author" : [ "Lee Xiong", "Chenyan Xiong", "Ye Li", "Kwok-Fung Tang", "Jialin Liu", "Paul N. Bennett", "Junaid Ahmed", "Arnold Overwijk." ],
      "venue" : "Proceedings of ICLR.",
      "citeRegEx" : "Xiong et al\\.,? 2021",
      "shortCiteRegEx" : "Xiong et al\\.",
      "year" : 2021
    }, {
      "title" : "Anserini: Enabling the use of lucene for information retrieval research",
      "author" : [ "Peilin Yang", "Hui Fang", "Jimmy Lin." ],
      "venue" : "Proceedings of SIGIR, pages 1253– 1256.",
      "citeRegEx" : "Yang et al\\.,? 2017",
      "shortCiteRegEx" : "Yang et al\\.",
      "year" : 2017
    }, {
      "title" : "Critically examining the ”neural hype”: Weak baselines and the additivity of effectiveness gains from neural ranking models",
      "author" : [ "Wei Yang", "Kuang Lu", "Peilin Yang", "Jimmy Lin." ],
      "venue" : "Proceedings of SIGIR, pages 1129–1132.",
      "citeRegEx" : "Yang et al\\.,? 2019",
      "shortCiteRegEx" : "Yang et al\\.",
      "year" : 2019
    }, {
      "title" : "Capreolus: A toolkit for end-to-end neural ad hoc retrieval",
      "author" : [ "Andrew Yates", "Siddhant Arora", "Xinyu Zhang", "Wei Yang", "Kevin Martin Jose", "Jimmy Lin." ],
      "venue" : "Proceedings of WSDM, pages 861–864.",
      "citeRegEx" : "Yates et al\\.,? 2020",
      "shortCiteRegEx" : "Yates et al\\.",
      "year" : 2020
    }, {
      "title" : "Fewshot generative conversational query rewriting",
      "author" : [ "Shi Yu", "Jiahua Liu", "Jingqin Yang", "Chenyan Xiong", "Paul Bennett", "Jianfeng Gao", "Zhiyuan Liu." ],
      "venue" : "Proceedings of SIGIR, page 1933–1936.",
      "citeRegEx" : "Yu et al\\.,? 2020",
      "shortCiteRegEx" : "Yu et al\\.",
      "year" : 2020
    }, {
      "title" : "Rapidly deploying a neural search engine for the COVID19 open research dataset",
      "author" : [ "Edwin Zhang", "Nikhil Gupta", "Rodrigo Nogueira", "Kyunghyun Cho", "Jimmy Lin." ],
      "venue" : "Proceedings of the 1st Workshop on NLP for COVID-19 at ACL 2020.",
      "citeRegEx" : "Zhang et al\\.,? 2020a",
      "shortCiteRegEx" : "Zhang et al\\.",
      "year" : 2020
    }, {
      "title" : "Selective weak supervision for neural information retrieval",
      "author" : [ "Kaitao Zhang", "Chenyan Xiong", "Zhenghao Liu", "Zhiyuan Liu." ],
      "venue" : "Proceedings of WWW, pages 474–485.",
      "citeRegEx" : "Zhang et al\\.,? 2020b",
      "shortCiteRegEx" : "Zhang et al\\.",
      "year" : 2020
    }, {
      "title" : "Meta label correction for learning with weak supervision",
      "author" : [ "Guoqing Zheng", "Ahmed Hassan Awadallah", "Susan Dumais." ],
      "venue" : "arXiv preprint arXiv:1911.03809.",
      "citeRegEx" : "Zheng et al\\.,? 2019a",
      "shortCiteRegEx" : "Zheng et al\\.",
      "year" : 2019
    }, {
      "title" : "Investigating weak supervision in deep ranking",
      "author" : [ "Yukun Zheng", "Yiqun Liu", "Zhi-Qiang Fan", "Cheng Luo", "Qingyao Ai", "Min Zhang", "Shaoping Ma." ],
      "venue" : "Data and Information Management, 3:155– 164.",
      "citeRegEx" : "Zheng et al\\.,? 2019b",
      "shortCiteRegEx" : "Zheng et al\\.",
      "year" : 2019
    }, {
      "title" : "In residual collection evaluation, the relevance labels from previous rounds can be used, but any document that has been annotated for a query will be removed before the evaluation",
      "author" : [ "Buckley" ],
      "venue" : null,
      "citeRegEx" : "Buckley and 1997..,? \\Q1997\\E",
      "shortCiteRegEx" : "Buckley and 1997..",
      "year" : 1997
    } ],
    "referenceMentions" : [ {
      "referenceID" : 1,
      "context" : ", question answering (Chen et al., 2017) and fact verification (Liu et al.",
      "startOffset" : 21,
      "endOffset" : 40
    }, {
      "referenceID" : 3,
      "context" : "Neural information retrieval (Neu-IR) models have recently shown advanced results in many ranking scenarios where massive relevance labels or clickthrough data are available (Mitra et al., 2018; Craswell et al., 2020).",
      "startOffset" : 174,
      "endOffset" : 217
    }, {
      "referenceID" : 18,
      "context" : "The flip side is that the “data-hungry” nature of Neu-IR models yields mixed results in few-shot ranking scenarios that suffer from the shortage of labeled data and implicit user feedback (Lin, 2019; Yang et al., 2019).",
      "startOffset" : 188,
      "endOffset" : 218
    }, {
      "referenceID" : 46,
      "context" : "The flip side is that the “data-hungry” nature of Neu-IR models yields mixed results in few-shot ranking scenarios that suffer from the shortage of labeled data and implicit user feedback (Lin, 2019; Yang et al., 2019).",
      "startOffset" : 188,
      "endOffset" : 218
    }, {
      "referenceID" : 49,
      "context" : "On ranking benchmarks with only hundreds of labeled queries, there have been debates about whether Neu-IR, even with billions of pre-trained parameters (Zhang et al., 2020a), really outperforms traditional IR techniques such as feature-based models and latent semantic index-",
      "startOffset" : 152,
      "endOffset" : 173
    }, {
      "referenceID" : 8,
      "context" : ", tail web queries that innately lack large supervision (Downey et al., 2007), applications with strong privacy constraints like personal and",
      "startOffset" : 56,
      "endOffset" : 77
    }, {
      "referenceID" : 2,
      "context" : "enterprise search (Chirita et al., 2005; Hawking, 2004), and domains where labeling requires professional expertise such as biomedical and legal search (Roberts et al.",
      "startOffset" : 18,
      "endOffset" : 55
    }, {
      "referenceID" : 11,
      "context" : "enterprise search (Chirita et al., 2005; Hawking, 2004), and domains where labeling requires professional expertise such as biomedical and legal search (Roberts et al.",
      "startOffset" : 18,
      "endOffset" : 55
    }, {
      "referenceID" : 35,
      "context" : ", 2005; Hawking, 2004), and domains where labeling requires professional expertise such as biomedical and legal search (Roberts et al., 2020; Arora et al., 2018).",
      "startOffset" : 119,
      "endOffset" : 161
    }, {
      "referenceID" : 22,
      "context" : "For synthesizing weak supervision, we take inspiration from the work (Ma et al., 2021) that generates related queries for unlabeled documents",
      "startOffset" : 69,
      "endOffset" : 86
    }, {
      "referenceID" : 34,
      "context" : "In this way, neural rankers can distinguish more useful synthetic weak supervision based on the similarity of the gradient directions of synthetic data and target data (Ren et al., 2018) instead of manual heuristics or trialand-error data selection (Zhang et al.",
      "startOffset" : 168,
      "endOffset" : 186
    }, {
      "referenceID" : 50,
      "context" : ", 2018) instead of manual heuristics or trialand-error data selection (Zhang et al., 2020b).",
      "startOffset" : 70,
      "endOffset" : 91
    }, {
      "referenceID" : 22,
      "context" : "Compared to prior work (Ma et al., 2021; Zhang et al., 2020b), MetaAdaptRank not only synthesizes more informative queries and effective weak relevance signals but customizes more diverse and fine-grained weights on synthetic weak data to better adapt neural rankers to target few-shot domains.",
      "startOffset" : 23,
      "endOffset" : 61
    }, {
      "referenceID" : 50,
      "context" : "Compared to prior work (Ma et al., 2021; Zhang et al., 2020b), MetaAdaptRank not only synthesizes more informative queries and effective weak relevance signals but customizes more diverse and fine-grained weights on synthetic weak data to better adapt neural rankers to target few-shot domains.",
      "startOffset" : 23,
      "endOffset" : 61
    }, {
      "referenceID" : 10,
      "context" : "Recent Neu-IR methods have achieved promising results in modeling relevance matching patterns between queries and documents (Guo et al., 2016; Hui et al., 2017; Mitra et al., 2018).",
      "startOffset" : 124,
      "endOffset" : 180
    }, {
      "referenceID" : 14,
      "context" : "Recent Neu-IR methods have achieved promising results in modeling relevance matching patterns between queries and documents (Guo et al., 2016; Hui et al., 2017; Mitra et al., 2018).",
      "startOffset" : 124,
      "endOffset" : 180
    }, {
      "referenceID" : 43,
      "context" : "tensively employed in ad-hoc text retrieval (Xiong et al., 2017b; Dai et al., 2018; Nogueira and Cho, 2019; Xiong et al., 2021) and later natural language processing (NLP) tasks (Lee et al.",
      "startOffset" : 44,
      "endOffset" : 127
    }, {
      "referenceID" : 5,
      "context" : "tensively employed in ad-hoc text retrieval (Xiong et al., 2017b; Dai et al., 2018; Nogueira and Cho, 2019; Xiong et al., 2021) and later natural language processing (NLP) tasks (Lee et al.",
      "startOffset" : 44,
      "endOffset" : 127
    }, {
      "referenceID" : 28,
      "context" : "tensively employed in ad-hoc text retrieval (Xiong et al., 2017b; Dai et al., 2018; Nogueira and Cho, 2019; Xiong et al., 2021) and later natural language processing (NLP) tasks (Lee et al.",
      "startOffset" : 44,
      "endOffset" : 127
    }, {
      "referenceID" : 44,
      "context" : "tensively employed in ad-hoc text retrieval (Xiong et al., 2017b; Dai et al., 2018; Nogueira and Cho, 2019; Xiong et al., 2021) and later natural language processing (NLP) tasks (Lee et al.",
      "startOffset" : 44,
      "endOffset" : 127
    }, {
      "referenceID" : 17,
      "context" : ", 2021) and later natural language processing (NLP) tasks (Lee et al., 2019; Liu et al., 2020; Qu et al., 2020).",
      "startOffset" : 58,
      "endOffset" : 111
    }, {
      "referenceID" : 20,
      "context" : ", 2021) and later natural language processing (NLP) tasks (Lee et al., 2019; Liu et al., 2020; Qu et al., 2020).",
      "startOffset" : 58,
      "endOffset" : 111
    }, {
      "referenceID" : 32,
      "context" : ", 2021) and later natural language processing (NLP) tasks (Lee et al., 2019; Liu et al., 2020; Qu et al., 2020).",
      "startOffset" : 58,
      "endOffset" : 111
    }, {
      "referenceID" : 47,
      "context" : "The less availability of relevance supervision pushes some Neu-IR methods to freeze their embeddings to avoid overfitting (Yates et al., 2020).",
      "startOffset" : 122,
      "endOffset" : 142
    }, {
      "referenceID" : 7,
      "context" : "The powerful deep pre-trained language models, such as BERT (Devlin et al., 2019), also do not effectively alleviate the dependence of Neu-IR on a large scale of relevance training signals.",
      "startOffset" : 60,
      "endOffset" : 81
    }, {
      "referenceID" : 12,
      "context" : "Recent research even observes that BERT-based neural rankers might require more training data than shallow neural ranking models (Hofstätter et al., 2020; Craswell et al., 2020).",
      "startOffset" : 129,
      "endOffset" : 177
    }, {
      "referenceID" : 3,
      "context" : "Recent research even observes that BERT-based neural rankers might require more training data than shallow neural ranking models (Hofstätter et al., 2020; Craswell et al., 2020).",
      "startOffset" : 129,
      "endOffset" : 177
    }, {
      "referenceID" : 31,
      "context" : "Moreover, they may often be overly confident and more unstable in the learning process (Qiao et al., 2019).",
      "startOffset" : 87,
      "endOffset" : 106
    }, {
      "referenceID" : 52,
      "context" : "A promising direction to alleviate the dependence of Neu-IR models on large-scale relevance supervision is to leverage weak supervision signals that are noisy but available at mass quantity (Zheng et al., 2019b; Dehghani et al., 2017; Yu et al., 2020).",
      "startOffset" : 190,
      "endOffset" : 251
    }, {
      "referenceID" : 6,
      "context" : "A promising direction to alleviate the dependence of Neu-IR models on large-scale relevance supervision is to leverage weak supervision signals that are noisy but available at mass quantity (Zheng et al., 2019b; Dehghani et al., 2017; Yu et al., 2020).",
      "startOffset" : 190,
      "endOffset" : 251
    }, {
      "referenceID" : 48,
      "context" : "A promising direction to alleviate the dependence of Neu-IR models on large-scale relevance supervision is to leverage weak supervision signals that are noisy but available at mass quantity (Zheng et al., 2019b; Dehghani et al., 2017; Yu et al., 2020).",
      "startOffset" : 190,
      "endOffset" : 251
    }, {
      "referenceID" : 6,
      "context" : ", pseudo relevance labels generated by unsupervised retrieval methods (Dehghani et al., 2017; Zheng et al., 2019b), and title-document pairs (MacAvaney et al.",
      "startOffset" : 70,
      "endOffset" : 114
    }, {
      "referenceID" : 52,
      "context" : ", pseudo relevance labels generated by unsupervised retrieval methods (Dehghani et al., 2017; Zheng et al., 2019b), and title-document pairs (MacAvaney et al.",
      "startOffset" : 70,
      "endOffset" : 114
    }, {
      "referenceID" : 23,
      "context" : ", 2019b), and title-document pairs (MacAvaney et al., 2019).",
      "startOffset" : 35,
      "endOffset" : 59
    }, {
      "referenceID" : 34,
      "context" : "More recently, a novel meta-learning technique has shown encouraging progress on solving data noises and label biases in computer vision (Ren et al., 2018; Shu et al., 2019; Zheng et al., 2019a)",
      "startOffset" : 137,
      "endOffset" : 194
    }, {
      "referenceID" : 38,
      "context" : "More recently, a novel meta-learning technique has shown encouraging progress on solving data noises and label biases in computer vision (Ren et al., 2018; Shu et al., 2019; Zheng et al., 2019a)",
      "startOffset" : 137,
      "endOffset" : 194
    }, {
      "referenceID" : 51,
      "context" : "More recently, a novel meta-learning technique has shown encouraging progress on solving data noises and label biases in computer vision (Ren et al., 2018; Shu et al., 2019; Zheng et al., 2019a)",
      "startOffset" : 137,
      "endOffset" : 194
    }, {
      "referenceID" : 19,
      "context" : "The standard learning to rank loss li(θ) (Liu, 2009), e.",
      "startOffset" : 41,
      "endOffset" : 52
    }, {
      "referenceID" : 22,
      "context" : "We will first recap the previous synthetic method (Ma et al., 2021) and then introduce our contrastive synthetic approach.",
      "startOffset" : 50,
      "endOffset" : 67
    }, {
      "referenceID" : 22,
      "context" : "Given a large volume of source-domain relevance pairs (q, d+), previous synthetic method (Ma et al., 2021) trains a NLG model such as T5 (Raffel et al.",
      "startOffset" : 89,
      "endOffset" : 106
    }, {
      "referenceID" : 33,
      "context" : ", 2021) trains a NLG model such as T5 (Raffel et al., 2020)",
      "startOffset" : 38,
      "endOffset" : 59
    }, {
      "referenceID" : 34,
      "context" : "In practice, we only perform onestep optimization in the two loops with mini-batch data, consistent with prior work (Ren et al., 2018).",
      "startOffset" : 116,
      "endOffset" : 134
    }, {
      "referenceID" : 34,
      "context" : "In this way, the few-shot target data can serve more as a “regularizer” to help the neural ranker to generalize with synthetic data, instead of as direct supervision which requires more labels (Ren et al., 2018).",
      "startOffset" : 193,
      "endOffset" : 211
    }, {
      "referenceID" : 16,
      "context" : ", 2009), Robust04 (Kwok et al., 2004), and TRECCOVID (Roberts et al.",
      "startOffset" : 18,
      "endOffset" : 37
    }, {
      "referenceID" : 42,
      "context" : "We evaluate supervised IR methods through re-ranking the top 100 documents from the first-stage retrieval with five-fold cross-validation, consistent with prior work (Xiong et al., 2017a; Dai and Callan, 2019; Zhang et al., 2020b).",
      "startOffset" : 166,
      "endOffset" : 230
    }, {
      "referenceID" : 4,
      "context" : "We evaluate supervised IR methods through re-ranking the top 100 documents from the first-stage retrieval with five-fold cross-validation, consistent with prior work (Xiong et al., 2017a; Dai and Callan, 2019; Zhang et al., 2020b).",
      "startOffset" : 166,
      "endOffset" : 230
    }, {
      "referenceID" : 50,
      "context" : "We evaluate supervised IR methods through re-ranking the top 100 documents from the first-stage retrieval with five-fold cross-validation, consistent with prior work (Xiong et al., 2017a; Dai and Callan, 2019; Zhang et al., 2020b).",
      "startOffset" : 166,
      "endOffset" : 230
    }, {
      "referenceID" : 36,
      "context" : "and Callan (2019), and the first-stage retrieval for TREC-COVID is BM25 (Robertson and Zaragoza, 2009) well-tuned by Anserini (Yang et al.",
      "startOffset" : 72,
      "endOffset" : 102
    }, {
      "referenceID" : 45,
      "context" : "and Callan (2019), and the first-stage retrieval for TREC-COVID is BM25 (Robertson and Zaragoza, 2009) well-tuned by Anserini (Yang et al., 2017).",
      "startOffset" : 126,
      "endOffset" : 145
    }, {
      "referenceID" : 50,
      "context" : "We also report ERR@20 for ClueWeb09-B and Robust04, which is the same with prior work (Zhang et al., 2020b), and report P@20 for TREC-COVID.",
      "startOffset" : 86,
      "endOffset" : 107
    }, {
      "referenceID" : 36,
      "context" : "They are two unsupervised methods, BM25 (Robertson and Zaragoza, 2009) and SDM (Metzler and Croft, 2005), and two learningto-rank (LTR) methods using bag-of-word features,",
      "startOffset" : 40,
      "endOffset" : 70
    }, {
      "referenceID" : 24,
      "context" : "They are two unsupervised methods, BM25 (Robertson and Zaragoza, 2009) and SDM (Metzler and Croft, 2005), and two learningto-rank (LTR) methods using bag-of-word features,",
      "startOffset" : 79,
      "endOffset" : 104
    }, {
      "referenceID" : 15,
      "context" : "RankSVM (Joachims, 2002) and Coor-Ascent (Coordinate Ascent) (Metzler and Croft, 2007).",
      "startOffset" : 8,
      "endOffset" : 24
    }, {
      "referenceID" : 25,
      "context" : "RankSVM (Joachims, 2002) and Coor-Ascent (Coordinate Ascent) (Metzler and Croft, 2007).",
      "startOffset" : 61,
      "endOffset" : 86
    }, {
      "referenceID" : 28,
      "context" : "In our experiments, all Neu-IR methods adopt the widely-used BERT ranker (Nogueira and Cho, 2019), BERT-FirstP, which only uses the first paragraph of documents.",
      "startOffset" : 73,
      "endOffset" : 97
    }, {
      "referenceID" : 27,
      "context" : "MS MARCO is a human supervision source (Nguyen et al., 2016), which provides over one million Bing queries with relevance labels.",
      "startOffset" : 39,
      "endOffset" : 60
    }, {
      "referenceID" : 23,
      "context" : "One baseline is Title Fitler, which treats filtered title-document pairs as weak supervision signals (MacAvaney et al., 2019) for training BERT rankers (Zhang et al.",
      "startOffset" : 101,
      "endOffset" : 125
    }, {
      "referenceID" : 50,
      "context" : ", 2019) for training BERT rankers (Zhang et al., 2020b).",
      "startOffset" : 34,
      "endOffset" : 55
    }, {
      "referenceID" : 50,
      "context" : "Anchor leverages 100k pairs of anchor texts and web pages to train BERT rankers (Zhang et al., 2020b).",
      "startOffset" : 80,
      "endOffset" : 101
    }, {
      "referenceID" : 50,
      "context" : "ReInfoSelect first employs reinforcement learning to select these anchor signals (Zhang et al., 2020b) and then trains BERT rankers.",
      "startOffset" : 81,
      "endOffset" : 102
    }, {
      "referenceID" : 22,
      "context" : "last baseline SyncSup trains BERT rankers with synthetic weak supervision data, which are synthesized based on the previous work (Ma et al., 2021).",
      "startOffset" : 129,
      "endOffset" : 146
    }, {
      "referenceID" : 7,
      "context" : "For our methods and all Neu-IR baselines, we use the base version of BERT (Devlin et al., 2019) on ClueWeb09-B and Robust04, and PubMedBERT (Base) (Gu et al.",
      "startOffset" : 74,
      "endOffset" : 95
    }, {
      "referenceID" : 9,
      "context" : ", 2019) on ClueWeb09-B and Robust04, and PubMedBERT (Base) (Gu et al., 2020) on TRECCOVID.",
      "startOffset" : 59,
      "endOffset" : 76
    }, {
      "referenceID" : 41,
      "context" : "2021) implementation and obtain the pre-trained weights from Hugging Face (Wolf et al., 2020).",
      "startOffset" : 74,
      "endOffset" : 93
    }, {
      "referenceID" : 4,
      "context" : "Following prior work (Dai and Callan, 2019; Zhang et al., 2020b), the ranking features ([CLS] embeddings) of BERT are combined with the first-stage retrieval scores using Coor-Ascent for ClueWeb09-B and Robust04.",
      "startOffset" : 21,
      "endOffset" : 64
    }, {
      "referenceID" : 50,
      "context" : "Following prior work (Dai and Callan, 2019; Zhang et al., 2020b), the ranking features ([CLS] embeddings) of BERT are combined with the first-stage retrieval scores using Coor-Ascent for ClueWeb09-B and Robust04.",
      "startOffset" : 21,
      "endOffset" : 64
    }, {
      "referenceID" : 30,
      "context" : "10) is implemented using the automatic differentiation in PyTorch (Paszke et al., 2017).",
      "startOffset" : 66,
      "endOffset" : 87
    }, {
      "referenceID" : 45,
      "context" : "5035 Methods ClueWeb09-B (Web) Robust04 (News) TREC-COVID (BioMed) NDCG@20 ERR@20 NDCG@20 ERR@20 NDCG@20 P@20 BM25 (Yang et al., 2017) 0.",
      "startOffset" : 115,
      "endOffset" : 134
    }, {
      "referenceID" : 27,
      "context" : "Supervision Sources ClueWeb09-B (Web) Robust04 (News) TREC-COVID (BioMed) NDCG@20 ERR@20 NDCG@20 ERR@20 NDCG@20 P@20 (a) MS MARCO (Nguyen et al., 2016) 0.",
      "startOffset" : 130,
      "endOffset" : 151
    }, {
      "referenceID" : 22,
      "context" : "Following previous research (Ma et al., 2021; Yu et al., 2020; Celikyilmaz et al., 2020), eight auto evaluation metrics are used in our evaluation.",
      "startOffset" : 28,
      "endOffset" : 88
    }, {
      "referenceID" : 48,
      "context" : "Following previous research (Ma et al., 2021; Yu et al., 2020; Celikyilmaz et al., 2020), eight auto evaluation metrics are used in our evaluation.",
      "startOffset" : 28,
      "endOffset" : 88
    }, {
      "referenceID" : 0,
      "context" : "Following previous research (Ma et al., 2021; Yu et al., 2020; Celikyilmaz et al., 2020), eight auto evaluation metrics are used in our evaluation.",
      "startOffset" : 28,
      "endOffset" : 88
    }, {
      "referenceID" : 22,
      "context" : "5036 Synthetic Methods BLEU-1 BLEU-2 ROUGE-1 ROUGE-2 ROUGE-L NIST@1 NIST@2 METEOR SyncSup (Ma et al., 2021) 0.",
      "startOffset" : 90,
      "endOffset" : 107
    } ],
    "year" : 2021,
    "abstractText" : "and the 11th International Joint Conference on Natural Language Processing, pages 5030–5043 August 1–6, 2021. ©2021 Association for Computational Linguistics 5030 Few-Shot Text Ranking with Meta Adapted Synthetic Weak Supervision Si Sun, Yingzhuo Qian, Zhenghao Liu, Chenyan Xiong, Kaitao Zhang, Jie Bao, Zhiyuan Liu, Paul Bennett Department of Electronic Engineering, Tsinghua University, Beijing, China Department of Computer Science and Technology, Tsinghua University, Beijing, China Institute for Artificial Intelligence, Tsinghua University, Beijing, China Beijing National Research Center for Information Science and Technology, China Microsoft Research, Redmond, USA {s-sun17, qyz17, liu-zh16, zkt18}@mails.tsinghua.edu.cn {bao, liuzy}@tsinghua.edu.cn {chenyan.xiong, Paul.N.Bennett}@microsoft.com Abstract",
    "creator" : "LaTeX with hyperref"
  }
}