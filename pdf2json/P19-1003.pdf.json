{
  "name" : "P19-1003.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "Improving Multi-turn Dialogue Modelling with Utterance ReWriter",
    "authors" : [ "Hui Su", "Xiaoyu Shen", "Rongzhi Zhang", "Fei Sun", "Pengwei Hu", "Cheng Niu", "Jie Zhou" ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : null,
      "text" : "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 22–31 Florence, Italy, July 28 - August 2, 2019. c©2019 Association for Computational Linguistics\n22"
    }, {
      "heading" : "1 Introduction",
      "text" : "Dialogue systems have made dramatic progress in recent years, especially in single-turn chit-chat and FAQ matching (Shang et al., 2015; Ghazvininejad et al., 2018; Molino et al., 2018; Chen et al., 2019). Nonethless, multi-turn dialogue modelling still remains extremely challenging (Vinyals and Le, 2015; Serban et al., 2016, 2017; Shen et al., 2018a,b). The challenge is multi-sided. One most important difficulty is the frequently occurred coreference and information omission in our daily conversations, especially in pro-drop languages like Chinese or Japanese. From our preliminary study of 2,000 Chinese multi-turn con-\n∗Both authors contributed equally. 1The code is available on https://github.com/\nchin-gyou/dialogue-utterance-rewriter.\nversations, different degrees of coreference and omission exist in more than 70% of the utterances. Capturing the hidden intention beneath them requires deeper understanding of the dialogue context, which is difficult for current neural networkbased systems. Table 1 shows two typical examples in multi-turn dialogues. “他”(he) from Context 1 is a coreference to “梅西”(Messi) and “为什 么”(Why) from Context 2 omits the further question of “为什么最喜欢泰坦尼克”(Why do you like Tatanic most)?. Without expanding the coreference or omission to recover the full information, the chatbot has no idea how to continue the talk.\nTo address this concern, we propose simplifying the multi-turn dialogue modelling into a singleturn problem by rewriting the current utterance. The utterance rewriter is expected to perform (1) coreference resolution and (2) information completion to recover all coreferred and omitted mentions. In the two examples from Table 1, each utterance 3 will be rewritten into utterance 3′. Afterwards, the system will generate a reply by only looking into the utterance 3′ without considering the previous turns utterance 1 and 2. This simplification shortens the length of dialogue con-\ntext while still maintaining necessary information needed to provide proper responses, which we believe will help ease the difficulty of multi-turn dialogue modelling. Compared with other methods like memory networks (Sukhbaatar et al., 2015) or explicit belief tracking (Mrkšić et al., 2017), the trained utterance rewriter is model-agnostic and can be easily integrated into other black-box dialogue systems. It is also more memory-efficient because the dialogue history information is reflected in a single rewritten utterance.\nTo get supervised training data for the utterance rewriting, we construct a Chinese dialogue dataset containing 20k multi-turn dialogues. Each utterance is paired with corresponding manually annotated rewritings. We model this problem as an extractive generation problem using the Pointer Network (Vinyals et al., 2015). The rewritten utterance is generated by copying words from either the dialogue history or the current utterance based on the attention mechanism (Bahdanau et al., 2014). Inspired by the recently proposed Transformer architecture (Vaswani et al., 2017) in machine translation which can capture better intra-sentence word dependencies, we modify the Transformer architecture to include the pointer network mechanism. The resulting model outperforms the recurrent neural network (RNN) and original Transformer models, achieving an F1 score of over 0.85 for both the coreference resolution and information completion. Furthermore, we integrate our trained utterance rewriter into two online chatbot platforms and find it leads to more accurate intention detection and improves the user engagement. In summary, our contributions are:\n1. We collect a high-quality annotated dataset for coreference resolution and information completion in multi-turn dialogues, which might benefit future related research.\n2. We propose a highly effective Transformerbased utterance rewriter outperforming several strong baselines.\n3. The trained utterance rewriter, when integrated into two real-life online chatbots, is shown to bring significant improvement over the original system.\nIn the next section, we will first go over some related work. Afterwards, in Section 3 and 4, our collected dataset and proposed model are introduced. The experiment results and analysis are\npresented in Section 5. Finally, some conclusions are drawn in Section 6."
    }, {
      "heading" : "2 Related Work",
      "text" : ""
    }, {
      "heading" : "2.1 Sentence Rewriting",
      "text" : "Sentence rewriting has been widely adopted in various NLP tasks. In machine translation, people have used it to refine the output generations from seq2seq models (Niehues et al., 2016; JunczysDowmunt and Grundkiewicz, 2017; Grangier and Auli, 2017; Gu et al., 2017). In text summarization, reediting the retrieved candidates can provide more accurate and abstractive summaries (See et al., 2017; Chen and Bansal, 2018; Cao et al., 2018). In dialogue modelling, Weston et al. (2018) applied it to rewrite outputs from a retrieval model, but they pay no attention to recovering the hidden information under the coreference and omission. Concurrent with our work, Rastogi et al. (2019) adopts a similar idea on English conversations to simplify the downstream SLU task by reformulating the original utterance. Rewriting the source input into some easy-to-process standard format has also gained significant improvements in information retrieval (Riezler and Liu, 2010), semantic parsing (Chen et al., 2016) or question answering (Abujabal et al., 2018), but most of them adopt a simple dictionary or template based rewriting strategy. For multi-turn dialogues, due to the complexity of human languages, designing suitable template-based rewriting rules would be timeconsuming."
    }, {
      "heading" : "2.2 Coreference Resolution",
      "text" : "Coreference resolution aims to link an antecedent for each possible mention. Traditional approaches often adopt a pipeline structure which first identify all pronouns and entities then run clustering algorithms (Haghighi and Klein, 2009; Lee et al., 2011; Durrett and Klein, 2013; Björkelund and Kuhn, 2014). At both stages, they rely heavily on complicated, fine-grained features. Recently, several neural coreference resolution systems (Clark and Manning, 2016a,b) utilize distributed representations to reduce human labors. Lee et al. (2017) reported state-of-the-art results with an end-to-end neural coreference resolution system. However, it requires computing the scores for all possible spans, which is computationally inefficient on online dialogue systems. The recently proposed Transformer adopted the self-\nattention mechanism which could implicitly capture inter-word dependencies in an unsupervised way (Vaswani et al., 2017). However, when multiple coreferences occur, it has problems properly distinguishing them. Our proposed architecture is built upon the Transformer architecture, but perform coreference resolution in a supervised setting to help deal with ambiguous mentions."
    }, {
      "heading" : "3 Dataset",
      "text" : "To get parallel training data for the sentence rewriting, we crawled 200k candidate multi-turn conversational data from several popular Chinese social media platforms for human annotators to work on. Sensitive information is filtered beforehand for later processing. Before starting the annotation, we randomly sample 2,000 conversational data and analyze how often coreference and omission occurs in multi-turn dialogues. Table 2 lists the statistics. As can be seen, only less than 30% utterances have neither coreference nor omission and quite a few utterances have both. This further validates the importance of addressing the these situations in multi-turn dialogues.\nIn the annotation process, human annotators need to identify these two situations then rewrite the utterance to cover all hidden information. An example is shown in Table 1. Annotators are required to provide the rewritten utterance 3′ given the original conversation [utterance 1,2 and 3]. To ensure the annotation quality, 10% of the annotations from each annotator are daily examined by a project manager and feedbacks are provided. The annotation is considered valid only when the accuracy of examined results surpasses 95%. Apart from the accuracy examination, the project manage is also required to (1) select topics that are more likely to be talked about in daily conversations, (2) try to cover broader domains and (3) balance the proportion of different coreference and omission patterns. The whole annotation takes 4 months to finish. In the end, we get 40k high-\nquality parallel samples. Half of them are negative samples which do not need any rewriting. The other half are positive samples where rewriting is needed. Table 3 lists the statistics. The rewritten utterance contains 10.5 tokens in average, reducing the context length by 80%."
    }, {
      "heading" : "4 Model",
      "text" : ""
    }, {
      "heading" : "4.1 Problem Formalization",
      "text" : "We denote each training sample as (H,Un → R). H = {U1, U2, . . . , Un−1} represents the dialogue history containing the first n − 1 turn of utterances. Un is the nth turn of utterance, the one that needs to be rewritten. R is the rewritten utterance after recovering all corefernced and omitted information in Un. R could be identical to Un if no coreference or omission is detected (negative sample). Our goal is to learn a mapping function p(R|(H,Un)) that can automatically rewrite Un based on the history information H . The process is to first encode (H,Un) into s sequence of vectors, then decode R using the pointer network. The next section will explain the steps in order."
    }, {
      "heading" : "4.2 Encoder",
      "text" : "We unfold all tokens in (H,Un) into (w1, w2, . . . , wm). m is the number of tokens in the whole dialogue. An end-of-turn delimiter is inserted between each two turns. The unfolded sequence of tokens are then encoded with Transformer. We concatenate all tokens in (H,Un) as the input, in hope that the Transformer can learn rudimentary coreference information within them by means of the self-attention mechanism. For each token wi, the input embedding is the sum of its word embedding, position embedding and turn embedding:\nI(wi) =WE(wi) + PE(wi) + TE(wi)\nThe word embedding WE(wi) and position embedding PE(wi) are the same as in normal Transformer architectures (Vaswani et al., 2017). We\nadd an additional turn embedding TE(wi) to indicate which turn each token belongs to. Tokens from the same turn will share the same turn embedding. The input embeddings are then forwarded into L stacked encoders to get the final encoding representations. Each encoder contains a self-attention layer followed by a feedforward neural network.:\nE(0) = [ I(w1), I(w2), . . . , I(wm) ] E(l) = FNN(MultiHead(E(l−1),E(l−1),E(l−1)))\nFNN is the feedforward neural network and MultiHead(Q,K,V) is a multi-head attention function taking a query matrix Q, a key matrix K, and a value matrix V as inputs. Each self-attention and feedforward component comes with a residual connection and layer-normalization step, which we refer to Vaswani et al. (2017) for more details. The final encodings are the output from the Lth encoder E(L)."
    }, {
      "heading" : "4.3 Decoder",
      "text" : "The decoder also contains L layers, each layer is composed of three sub-layers. The first sub-layer is a multi-head self-attention:\nMl = MultiHead(D(l−1),D(l−1),D(l−1))\nD(0) = R. The second sub-layer is encoderdecoder attention that integrates E(L) into the decoder. In our task, asH andUn serve different purposes, we use separate key-value matrix for tokens\ncoming from the dialogue history H and those coming from Un. The encoded sequence E(L) obtained from the last section is split into E(L)H (encodings of tokens from H) and E(L)Un (encodings of tokens from Un) then processed separately. The encoder-decoder vectors are computed as follows:\nC(H)l = MultiHead(M(l),E (L) H ,E (L) H )\nC(Un) l = MultiHead(M(l),E (L) Un ,E (L) Un )\nThe third sub-layer is a position-wise fully connected feed-forward neural network:\nD(l) = FNN([C(H)l ◦C(Un)l])\nwhere ◦ denotes vector concatenation."
    }, {
      "heading" : "4.4 Output Distribution",
      "text" : "In the decoding process, we hope our model could learn whether to copy words from H or Un at different steps. Therefore, we impose a soft gating weight λ to make the decision. The decoding probability is computed by combining the atten-\ntion distribution from the last decoding layer:\np(Rt=w|H,Un, R<t)=λ ∑\ni:(wi=w)∧(wi∈H)\nat,i\n+(1−λ) ∑\nj:(wj=w)∧(wj∈Un)\na′t,j\na = Attention(M(L),E (L) Un ) a′ = Attention(M(L),E (L) H )\nλ = σ ( w>d D L t +w > HC(H) L t +w > UC(Un) L t ) a and a′ are the attention distribution over tokens in H and Un respectively. wd,wH , and wU are parameters to be learned, σ is the sigmoid function to output a value between 0 and 1. The gating weight λ works like a sentinel to inform the decoder whether to extract information from the dialogue history H or directly copy from Un. If Un contains neither coreference nor information omission. λ would be always 1 to copy the originalUn as the output. Otherwise λ becomes 0 when a coreference or omission is detected. The attention mechanism is then responsible of finding the proper coreferred or omitted information from the dialogue history. The whole model is trained endto-end by maximizing p(R|H,Un)."
    }, {
      "heading" : "5 Experiments",
      "text" : "We train our model to perform the utterance rewriting task on our collected dataset. In this section, we focus on answering the following two questions: (1) How accurately our proposed model can perform coreference resolution and information completion respectively and (2) How good the trained utterance rewriter is at helping off-theshelf dialogue systems provide more appropriate responses. To answer the first question, we compare our models with several strong baselines and test them by both automatic evaluation and human judgement. For the second question, we integrate our rewriting model to two online dialogue systems and analyze how it affects the humancomputer interactions. The following section will first introduce the compared models and basic settings, then report our evaluation results."
    }, {
      "heading" : "5.1 Compared Models",
      "text" : "When choosing compared models, we are mainly curious to see (1) whether the self-attention based Transformer architecture is superior to other networks like LSTMs, (2) whether the pointer-based\ngenerator is better than pure generation-based models and (3) whether it is preferred to split the attention by a coefficient λ as in our model. With these intentions, we implement the following four types of models for comparison:\n1. (L/T)-Gen: Pure generation-based model. Words are generated from a fixed vocabulary.\n2. (L/T)-Ptr-Net: Pure pointer-based model as in Vinyals et al. (2015). Words can only be copied from the input.\n3. (L/T)-Ptr-Gen: Hybrid pointer+generation model as in See et al. (2017). Words can be either copied from the input or generated from a fixed vocabulary.\n4. (L/T)-Ptr-λ: Our proposed model which split the attention by a coefficient λ.\n(L/T) denotes the encoder-decoder structure is the LSTM or Transformer. For the first three types of models, we unfold all tokens from the dialogue as the input. No difference is made between the dialogue history and the utterance to be rewritten."
    }, {
      "heading" : "5.2 Experiment Settings",
      "text" : "Transformer-based models We set the hidden size as 512. The attention has 8 individual heads and the encoder/decoder have 6 individual stacked layers. Models are optimized with the Adam optimizer. The initial learning rate is 0.0001 and batch size is 64. All hyperparameters are tuned base on the performance on the validation data.\nLSTM-based Models We encode words with a single-layer bidirectional LSTM and decode with a uni-directional LSTM. We use 128-dimensional word embeddings and 256-dimensional hidden states for both the encoder and decoder.2 The batch size is set as 128. Models are trained using Adagrad with learning rate 0.15 and initial accumulator value 0.1, same as in See et al. (2017).\nGeneral Setup We built our vocabulary based on character-based segmentation for Chinese scripts. For non-Chinese characters, like frequently mentioned entity names “Kobe” and “NBA”, we split them by space and keep all unique tokens which appear more than twice. The resulting vocabulary size is 5629 (4813 Chinese\n2We tried increasing the dimension but find it degrades the performance.\ncharacters and 816 other tokens), including the end-of-turn delimiter and a special UNK token for all unknown words. In the testing stage, all models decode words by beam search with beam size set to 4."
    }, {
      "heading" : "5.3 Quality of Sentence ReWriting",
      "text" : "Accuracy of Generation We first evaluate the accuracy of generation leveraging three metrics: BLEU, ROUGE, and the exact match score(EM) (the percentage of decoded sequences that exactly match the human references). For the EM score, we report separately on the positive and negative samples to see the difference. We report BLEU-1, 2, 4 scores and the F1 scores of ROUGE-1, 2, L. The results are listed in Table 4. We can have several observations in response to the three questions proposed in the beginning of Section 5.1:\n1. Transformer-based models lead to signif-\nicant improvement compare with LSTMbased counterparts. This implies the selfattention mechanism is helpful in identifying coreferred and omitted information. More analysis on how it helps coreference resolution can be seen in the next section.\n2. The generation mode does not work well in our setting since all words can be retrieved from either H or Un. Pointer-based models outperform the more complex generationbased and hybrid ones.\n3. Separately processing H and Un then combine their attention with a learned λ performs better than treating the whole dialogue tokens as s single input, though the improvement is less significant compared with previous two mentions.\nOverall our proposed model achieves remarkably good performance, with 55.84% of its generations exactly matches the human reference on the positive samples. For negative samples, our model properly copied the the original utterances in 98.14% of the cases. It suggests our model is already able to identify the utterances that do not need rewriting. Future work should work on improving the rewriting ability on positive samples.\nCoreference Resolution Apart from the standard metrics for text generation, we specifically test the precision, recall and F1 score of coreference resolution on our task. A pronoun or a noun is considered as properly coreferred if the rewritten utterance contains the correct mention in the corresponding referent. The result is shown in Table 5. To compare with current state-of-the-\nart models. We train the model from Lee et al. (2017) on our task and report the results on the first row. The result is quite consistent with the findings from the last section. Our final model outperforms the others by a large margin, reaching a precision score of 93% and recall score of 90%. It implies our model is already quite good at finding the proper coreference. Future challenges would be more about information completion. Figure 2 further provides an examples of how the Transformer can help implicitly learn the coreference resolution through the self-attention mechanism. The same example is also shown in Table 1. The pronoun “他”(he) in the utterance is properly aligned to the mention “梅西”(Messi) in the dialogue history, also partially to “球员”(player) which is the occupation of him. The implicitly learned coreference relation should be part of the reason that Transformers outperform LSTM models on the coreference resolution task.\nInformation Completion Similar as coreference resolution, we evaluate the quality of information completeness separately. One omitted information is considered as properly completed if the rewritten utterance recovers the omitted words. Since it inserts new words to the original utterance, we further conduct a human evaluation to measure the fluency of rewritten utterances. We randomly sample 600 samples from our positive test set. Three participants were asked to judge whether the rewritten utterance is a fluent sentence with the score 1(not fluent)-5(fluent). The fluency score for each model is averaged over all human evaluated scores.\nThe results are shown in Table 7. Basically the condition is similar as in Table 5. T-Ptr-λ achieves the best performance, with the F1 score of 0.86. The performance is slightly worse than coreference resolution since information omission is more implicit. Retrieving all hidden information is sometimes difficult even for humans. Moreover, the fluency of our model’s generations is very good, only slightly worse than the human reference (4.90 vs 4.97). Information completeness does not have much effects on the fluency. Exam-\nples of rewritten utterances are shown in Table 6."
    }, {
      "heading" : "5.4 Integration Testing",
      "text" : "In this section, we study how the proposed utterance rewriter can be integrated into off-the-shelf online chatbots to improve the quality of generated responses. We use our best model T-Ptr-λ to rewrite each utterance based on the dialogue context. The rewritten utterance is then forwarded to the system for response generation. We apply on both a task-oriented and chitchat setting. The results are compared with the original system having no utterance rewriter.\nTask-oriented Our task-oriented dialogue system contains an intention classifier built on FastText(Bojanowski et al., 2017) and a set of templates that perform policy decision and slot-value filling sequentially. Intention detection is a most important component in task-oriented dialogues and its accuracy will affect all the following steps. We define 30 intention classes like weather, hotel booking and shopping. The training data contains 35,447 human annotations. With the combination of our rewriter, the intention classier is able\nto achieve a precision of 89.91%, outperforming the original system by over 9%. The improved intention classification further lead to better conversations. An example is shown in Table 8, a multiturn conversation about the weather. The user first asks “How is the weather in Beijing”, then follows with a further question about “Then what clothes are suitable to wear”. The original system wrongly classified the user intention as shopping since this is a common conversational pattern in shopping. In contrast, our utterance rewriter is able to recover the omitted information “under the weather in Beijing”. Based on the rewritten utterance, the classifier is able to correctly detect the intention and provide proper responses.\nChitchat Our social chatbot contains two separate engines for multi-turn and single-turn dialogues. Each engine is a hybrid retrieval and generation model. In real-life applications, a user query would be simultaneously distributed to these two engines. The returned candidate responses are then reranked to provide the final response. Generally the model is already able to provide rather high-quality responses under the single-turn condition, but under multi-turn conversations, the complex context dependency makes the generation difficult. We integrate our utterance rewriter into the single-turn engine and compare with the original model by conducting the online A/B test. Specifically, we randomly split the users into two groups. One talks with the original system and the other talks with the system integrated with the utterance rewriter. All users are unconscious of the\ndetails about our system. The whole test lasted one month. Table 9 shows the Conversation-turns Per Session (CPS), which is the average number of conversation-turns between the chatbot and the user in a session. The utterance rewriter increases the average CPS from 6.3 to 7.7, indicating the user is more engaged with the integrated model. Table 8 shows an example of how the utterance rewriter helps with the generation. After the rewriting, the model can better understand the dialogue is about the NBA team Warriors, but the original model feels confused and only provides a generic response."
    }, {
      "heading" : "6 Conclusion",
      "text" : "In this paper, we propose improving multi-turn dialogue modelling by imposing a separate utterance rewriter. The rewriter is trained to recover the coreferred and omitted information of user utterances. We collect a high-quality manually annotated dataset and designed a Transformer-pointer based architecture to train the utterance rewriter. The trained utterance rewriter performs remarkably well and, when integrated into two online chatbot applications, significantly improves the intention detection and user engagement. We hope the collected dataset and proposed model can benefit future related research."
    }, {
      "heading" : "Acknowledgments",
      "text" : "We thank all anonymous reviewers and the dialogue system team of Wechat AI for valuable comments. Xiaoyu Shen is supported by IMPRS-CS fellowship."
    } ],
    "references" : [ {
      "title" : "Never-ending learning for open-domain question answering over knowledge bases",
      "author" : [ "Abdalghani Abujabal", "Rishiraj Saha Roy", "Mohamed Yahya", "Gerhard Weikum." ],
      "venue" : "Proceedings of the 2018 World Wide Web Conference on World Wide Web, pages",
      "citeRegEx" : "Abujabal et al\\.,? 2018",
      "shortCiteRegEx" : "Abujabal et al\\.",
      "year" : 2018
    }, {
      "title" : "Neural machine translation by jointly learning to align and translate",
      "author" : [ "Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio." ],
      "venue" : "arXiv preprint arXiv:1409.0473.",
      "citeRegEx" : "Bahdanau et al\\.,? 2014",
      "shortCiteRegEx" : "Bahdanau et al\\.",
      "year" : 2014
    }, {
      "title" : "Learning structured perceptrons for coreference resolution with latent antecedents and non-local features",
      "author" : [ "Anders Björkelund", "Jonas Kuhn." ],
      "venue" : "In",
      "citeRegEx" : "Björkelund and Kuhn.,? 2014",
      "shortCiteRegEx" : "Björkelund and Kuhn.",
      "year" : 2014
    }, {
      "title" : "Enriching word vectors with subword information",
      "author" : [ "Piotr Bojanowski", "Edouard Grave", "Armand Joulin", "Tomas Mikolov." ],
      "venue" : "Transactions of the Association for Computational Linguistics, 5:135–146.",
      "citeRegEx" : "Bojanowski et al\\.,? 2017",
      "shortCiteRegEx" : "Bojanowski et al\\.",
      "year" : 2017
    }, {
      "title" : "Retrieve, rerank and rewrite: Soft template based neural summarization",
      "author" : [ "Ziqiang Cao", "Wenjie Li", "Sujian Li", "Furu Wei." ],
      "venue" : "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), vol-",
      "citeRegEx" : "Cao et al\\.,? 2018",
      "shortCiteRegEx" : "Cao et al\\.",
      "year" : 2018
    }, {
      "title" : "Sentence rewriting for semantic parsing",
      "author" : [ "Bo Chen", "Le Sun", "Xianpei Han", "Bo An." ],
      "venue" : "CoRR, abs/1901.02998.",
      "citeRegEx" : "Chen et al\\.,? 2016",
      "shortCiteRegEx" : "Chen et al\\.",
      "year" : 2016
    }, {
      "title" : "Driven answer generation for product-related questions in e-commerce",
      "author" : [ "Shiqian Chen", "Chenliang Li", "Feng Ji", "Wei Zhou", "Haiqing Chen." ],
      "venue" : "Proceedings of the Twelfth ACM International Conference on Web Search and Data Mining, pages 411–",
      "citeRegEx" : "Chen et al\\.,? 2019",
      "shortCiteRegEx" : "Chen et al\\.",
      "year" : 2019
    }, {
      "title" : "Fast abstractive summarization with reinforce-selected sentence rewriting",
      "author" : [ "Yen-Chun Chen", "Mohit Bansal." ],
      "venue" : "arXiv preprint arXiv:1805.11080.",
      "citeRegEx" : "Chen and Bansal.,? 2018",
      "shortCiteRegEx" : "Chen and Bansal.",
      "year" : 2018
    }, {
      "title" : "Deep reinforcement learning for mention-ranking coreference models",
      "author" : [ "Kevin Clark", "Christopher D Manning." ],
      "venue" : "arXiv preprint arXiv:1609.08667.",
      "citeRegEx" : "Clark and Manning.,? 2016a",
      "shortCiteRegEx" : "Clark and Manning.",
      "year" : 2016
    }, {
      "title" : "Improving coreference resolution by learning entitylevel distributed representations",
      "author" : [ "Kevin Clark", "Christopher D Manning." ],
      "venue" : "arXiv preprint arXiv:1606.01323.",
      "citeRegEx" : "Clark and Manning.,? 2016b",
      "shortCiteRegEx" : "Clark and Manning.",
      "year" : 2016
    }, {
      "title" : "Easy victories and uphill battles in coreference resolution",
      "author" : [ "Greg Durrett", "Dan Klein." ],
      "venue" : "Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1971–1982.",
      "citeRegEx" : "Durrett and Klein.,? 2013",
      "shortCiteRegEx" : "Durrett and Klein.",
      "year" : 2013
    }, {
      "title" : "A knowledge-grounded neural conversation model",
      "author" : [ "Marjan Ghazvininejad", "Chris Brockett", "Ming-Wei Chang", "Bill Dolan", "Jianfeng Gao", "Wen-tau Yih", "Michel Galley." ],
      "venue" : "Thirty-Second AAAI Conference on Artificial Intelligence.",
      "citeRegEx" : "Ghazvininejad et al\\.,? 2018",
      "shortCiteRegEx" : "Ghazvininejad et al\\.",
      "year" : 2018
    }, {
      "title" : "Quickedit: Editing text & translations via simple delete actions",
      "author" : [ "David Grangier", "Michael Auli." ],
      "venue" : "arXiv preprint arXiv:1711.04805.",
      "citeRegEx" : "Grangier and Auli.,? 2017",
      "shortCiteRegEx" : "Grangier and Auli.",
      "year" : 2017
    }, {
      "title" : "Search engine guided nonparametric neural machine translation",
      "author" : [ "Jiatao Gu", "Yong Wang", "Kyunghyun Cho", "Victor OK Li." ],
      "venue" : "arXiv preprint arXiv:1705.07267.",
      "citeRegEx" : "Gu et al\\.,? 2017",
      "shortCiteRegEx" : "Gu et al\\.",
      "year" : 2017
    }, {
      "title" : "Simple coreference resolution with rich syntactic and semantic features",
      "author" : [ "Aria Haghighi", "Dan Klein." ],
      "venue" : "Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing: Volume 3-Volume 3, pages 1152–1161. Asso-",
      "citeRegEx" : "Haghighi and Klein.,? 2009",
      "shortCiteRegEx" : "Haghighi and Klein.",
      "year" : 2009
    }, {
      "title" : "An exploration of neural sequence-tosequence architectures for automatic post-editing",
      "author" : [ "Marcin Junczys-Dowmunt", "Roman Grundkiewicz." ],
      "venue" : "arXiv preprint arXiv:1706.04138.",
      "citeRegEx" : "Junczys.Dowmunt and Grundkiewicz.,? 2017",
      "shortCiteRegEx" : "Junczys.Dowmunt and Grundkiewicz.",
      "year" : 2017
    }, {
      "title" : "Stanford’s multi-pass sieve coreference resolution system at the conll-2011 shared task",
      "author" : [ "Heeyoung Lee", "Yves Peirsman", "Angel Chang", "Nathanael Chambers", "Mihai Surdeanu", "Dan Jurafsky." ],
      "venue" : "Proceedings of the fifteenth conference on com-",
      "citeRegEx" : "Lee et al\\.,? 2011",
      "shortCiteRegEx" : "Lee et al\\.",
      "year" : 2011
    }, {
      "title" : "End-to-end neural coreference resolution",
      "author" : [ "Kenton Lee", "Luheng He", "Mike Lewis", "Luke Zettlemoyer." ],
      "venue" : "arXiv preprint arXiv:1707.07045.",
      "citeRegEx" : "Lee et al\\.,? 2017",
      "shortCiteRegEx" : "Lee et al\\.",
      "year" : 2017
    }, {
      "title" : "Cota: Improving the speed and accuracy of customer support through ranking and deep networks",
      "author" : [ "Piero Molino", "Huaixiu Zheng", "Yi-Chia Wang." ],
      "venue" : "Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery",
      "citeRegEx" : "Molino et al\\.,? 2018",
      "shortCiteRegEx" : "Molino et al\\.",
      "year" : 2018
    }, {
      "title" : "Neural belief tracker: Data-driven dialogue state tracking",
      "author" : [ "Nikola Mrkšić", "Diarmuid Ó Séaghdha", "Tsung-Hsien Wen", "Blaise Thomson", "Steve Young." ],
      "venue" : "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics",
      "citeRegEx" : "Mrkšić et al\\.,? 2017",
      "shortCiteRegEx" : "Mrkšić et al\\.",
      "year" : 2017
    }, {
      "title" : "Pre-translation for neural machine translation",
      "author" : [ "Jan Niehues", "Eunah Cho", "Thanh-Le Ha", "Alex Waibel." ],
      "venue" : "arXiv preprint arXiv:1610.05243.",
      "citeRegEx" : "Niehues et al\\.,? 2016",
      "shortCiteRegEx" : "Niehues et al\\.",
      "year" : 2016
    }, {
      "title" : "Scaling multi-domain dialogue state tracking via query reformulation",
      "author" : [ "Pushpendre Rastogi", "Arpit Gupta", "Tongfei Chen", "Lambert Mathias." ],
      "venue" : "NAACL.",
      "citeRegEx" : "Rastogi et al\\.,? 2019",
      "shortCiteRegEx" : "Rastogi et al\\.",
      "year" : 2019
    }, {
      "title" : "Query rewriting using monolingual statistical machine translation",
      "author" : [ "Stefan Riezler", "Yi Liu." ],
      "venue" : "Computational Linguistics, 36(3):569–582.",
      "citeRegEx" : "Riezler and Liu.,? 2010",
      "shortCiteRegEx" : "Riezler and Liu.",
      "year" : 2010
    }, {
      "title" : "Get to the point: Summarization with pointer-generator networks",
      "author" : [ "Abigail See", "Peter J Liu", "Christopher D Manning." ],
      "venue" : "arXiv preprint arXiv:1704.04368.",
      "citeRegEx" : "See et al\\.,? 2017",
      "shortCiteRegEx" : "See et al\\.",
      "year" : 2017
    }, {
      "title" : "Building end-to-end dialogue systems using generative hierarchical neural network models",
      "author" : [ "Iulian V Serban", "Alessandro Sordoni", "Yoshua Bengio", "Aaron Courville", "Joelle Pineau." ],
      "venue" : "AAAI.",
      "citeRegEx" : "Serban et al\\.,? 2016",
      "shortCiteRegEx" : "Serban et al\\.",
      "year" : 2016
    }, {
      "title" : "A hierarchical latent variable encoder-decoder model for generating dialogues",
      "author" : [ "Iulian Vlad Serban", "Alessandro Sordoni", "Ryan Lowe", "Laurent Charlin", "Joelle Pineau", "Aaron Courville", "Yoshua Bengio." ],
      "venue" : "AAAI.",
      "citeRegEx" : "Serban et al\\.,? 2017",
      "shortCiteRegEx" : "Serban et al\\.",
      "year" : 2017
    }, {
      "title" : "Neural responding machine for short-text conversation",
      "author" : [ "Lifeng Shang", "Zhengdong Lu", "Hang Li." ],
      "venue" : "arXiv preprint arXiv:1503.02364.",
      "citeRegEx" : "Shang et al\\.,? 2015",
      "shortCiteRegEx" : "Shang et al\\.",
      "year" : 2015
    }, {
      "title" : "Nexus network: Connecting the preceding and the following in dialogue generation",
      "author" : [ "Xiaoyu Shen", "Hui Su", "Wenjie Li", "Dietrich Klakow." ],
      "venue" : "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 4316–",
      "citeRegEx" : "Shen et al\\.,? 2018a",
      "shortCiteRegEx" : "Shen et al\\.",
      "year" : 2018
    }, {
      "title" : "Improving variational encoder-decoders in dialogue generation",
      "author" : [ "Xiaoyu Shen", "Hui Su", "Shuzi Niu", "Vera Demberg." ],
      "venue" : "Thirty-Second AAAI Conference on Artificial Intelligence.",
      "citeRegEx" : "Shen et al\\.,? 2018b",
      "shortCiteRegEx" : "Shen et al\\.",
      "year" : 2018
    }, {
      "title" : "End-to-end memory networks. In Advances in neural information processing systems, pages 2440–2448",
      "author" : [ "Sainbayar Sukhbaatar", "Jason Weston", "Rob Fergus" ],
      "venue" : null,
      "citeRegEx" : "Sukhbaatar et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Sukhbaatar et al\\.",
      "year" : 2015
    }, {
      "title" : "Attention is all you need",
      "author" : [ "Ashish Vaswani", "Noam Shazeer", "Niki Parmar", "Jakob Uszkoreit", "Llion Jones", "Aidan N Gomez", "Łukasz Kaiser", "Illia Polosukhin." ],
      "venue" : "Advances in Neural Information Processing Systems, pages 5998–6008.",
      "citeRegEx" : "Vaswani et al\\.,? 2017",
      "shortCiteRegEx" : "Vaswani et al\\.",
      "year" : 2017
    }, {
      "title" : "Pointer networks",
      "author" : [ "Oriol Vinyals", "Meire Fortunato", "Navdeep Jaitly." ],
      "venue" : "Advances in Neural Information Processing Systems, pages 2692–2700.",
      "citeRegEx" : "Vinyals et al\\.,? 2015",
      "shortCiteRegEx" : "Vinyals et al\\.",
      "year" : 2015
    }, {
      "title" : "A neural conversational model",
      "author" : [ "Oriol Vinyals", "Quoc Le." ],
      "venue" : "arXiv preprint arXiv:1506.05869.",
      "citeRegEx" : "Vinyals and Le.,? 2015",
      "shortCiteRegEx" : "Vinyals and Le.",
      "year" : 2015
    }, {
      "title" : "Retrieve and refine: Improved sequence generation models for dialogue",
      "author" : [ "Jason Weston", "Emily Dinan", "Alexander Miller." ],
      "venue" : "Proceedings of the 2018 EMNLP Workshop SCAI: The 2nd International Workshop on Search-Oriented Conversational",
      "citeRegEx" : "Weston et al\\.,? 2018",
      "shortCiteRegEx" : "Weston et al\\.",
      "year" : 2018
    } ],
    "referenceMentions" : [ {
      "referenceID" : 26,
      "context" : "Dialogue systems have made dramatic progress in recent years, especially in single-turn chit-chat and FAQ matching (Shang et al., 2015; Ghazvininejad et al., 2018; Molino et al., 2018; Chen et al., 2019).",
      "startOffset" : 115,
      "endOffset" : 203
    }, {
      "referenceID" : 11,
      "context" : "Dialogue systems have made dramatic progress in recent years, especially in single-turn chit-chat and FAQ matching (Shang et al., 2015; Ghazvininejad et al., 2018; Molino et al., 2018; Chen et al., 2019).",
      "startOffset" : 115,
      "endOffset" : 203
    }, {
      "referenceID" : 18,
      "context" : "Dialogue systems have made dramatic progress in recent years, especially in single-turn chit-chat and FAQ matching (Shang et al., 2015; Ghazvininejad et al., 2018; Molino et al., 2018; Chen et al., 2019).",
      "startOffset" : 115,
      "endOffset" : 203
    }, {
      "referenceID" : 6,
      "context" : "Dialogue systems have made dramatic progress in recent years, especially in single-turn chit-chat and FAQ matching (Shang et al., 2015; Ghazvininejad et al., 2018; Molino et al., 2018; Chen et al., 2019).",
      "startOffset" : 115,
      "endOffset" : 203
    }, {
      "referenceID" : 29,
      "context" : "Compared with other methods like memory networks (Sukhbaatar et al., 2015) or explicit belief tracking (Mrkšić et al.",
      "startOffset" : 49,
      "endOffset" : 74
    }, {
      "referenceID" : 19,
      "context" : ", 2015) or explicit belief tracking (Mrkšić et al., 2017), the trained utterance rewriter is model-agnostic and can be easily integrated into other black-box dialogue systems.",
      "startOffset" : 36,
      "endOffset" : 57
    }, {
      "referenceID" : 1,
      "context" : "The rewritten utterance is generated by copying words from either the dialogue history or the current utterance based on the attention mechanism (Bahdanau et al., 2014).",
      "startOffset" : 145,
      "endOffset" : 168
    }, {
      "referenceID" : 30,
      "context" : "architecture (Vaswani et al., 2017) in machine translation which can capture better intra-sentence word dependencies, we modify the Transformer architecture to include the pointer network mechanism.",
      "startOffset" : 13,
      "endOffset" : 35
    }, {
      "referenceID" : 20,
      "context" : "In machine translation, people have used it to refine the output generations from seq2seq models (Niehues et al., 2016; JunczysDowmunt and Grundkiewicz, 2017; Grangier and Auli, 2017; Gu et al., 2017).",
      "startOffset" : 97,
      "endOffset" : 200
    }, {
      "referenceID" : 12,
      "context" : "In machine translation, people have used it to refine the output generations from seq2seq models (Niehues et al., 2016; JunczysDowmunt and Grundkiewicz, 2017; Grangier and Auli, 2017; Gu et al., 2017).",
      "startOffset" : 97,
      "endOffset" : 200
    }, {
      "referenceID" : 13,
      "context" : "In machine translation, people have used it to refine the output generations from seq2seq models (Niehues et al., 2016; JunczysDowmunt and Grundkiewicz, 2017; Grangier and Auli, 2017; Gu et al., 2017).",
      "startOffset" : 97,
      "endOffset" : 200
    }, {
      "referenceID" : 23,
      "context" : "In text summarization, reediting the retrieved candidates can provide more accurate and abstractive summaries (See et al., 2017; Chen and Bansal, 2018; Cao et al., 2018).",
      "startOffset" : 110,
      "endOffset" : 169
    }, {
      "referenceID" : 7,
      "context" : "In text summarization, reediting the retrieved candidates can provide more accurate and abstractive summaries (See et al., 2017; Chen and Bansal, 2018; Cao et al., 2018).",
      "startOffset" : 110,
      "endOffset" : 169
    }, {
      "referenceID" : 4,
      "context" : "In text summarization, reediting the retrieved candidates can provide more accurate and abstractive summaries (See et al., 2017; Chen and Bansal, 2018; Cao et al., 2018).",
      "startOffset" : 110,
      "endOffset" : 169
    }, {
      "referenceID" : 22,
      "context" : "input into some easy-to-process standard format has also gained significant improvements in information retrieval (Riezler and Liu, 2010), semantic parsing (Chen et al.",
      "startOffset" : 114,
      "endOffset" : 137
    }, {
      "referenceID" : 5,
      "context" : "input into some easy-to-process standard format has also gained significant improvements in information retrieval (Riezler and Liu, 2010), semantic parsing (Chen et al., 2016) or question answering (Abujabal et al.",
      "startOffset" : 156,
      "endOffset" : 175
    }, {
      "referenceID" : 0,
      "context" : ", 2016) or question answering (Abujabal et al., 2018), but most of them",
      "startOffset" : 30,
      "endOffset" : 53
    }, {
      "referenceID" : 14,
      "context" : "Traditional approaches often adopt a pipeline structure which first identify all pronouns and entities then run clustering algorithms (Haghighi and Klein, 2009; Lee et al., 2011; Durrett and Klein, 2013; Björkelund and Kuhn, 2014).",
      "startOffset" : 134,
      "endOffset" : 230
    }, {
      "referenceID" : 16,
      "context" : "Traditional approaches often adopt a pipeline structure which first identify all pronouns and entities then run clustering algorithms (Haghighi and Klein, 2009; Lee et al., 2011; Durrett and Klein, 2013; Björkelund and Kuhn, 2014).",
      "startOffset" : 134,
      "endOffset" : 230
    }, {
      "referenceID" : 10,
      "context" : "Traditional approaches often adopt a pipeline structure which first identify all pronouns and entities then run clustering algorithms (Haghighi and Klein, 2009; Lee et al., 2011; Durrett and Klein, 2013; Björkelund and Kuhn, 2014).",
      "startOffset" : 134,
      "endOffset" : 230
    }, {
      "referenceID" : 2,
      "context" : "Traditional approaches often adopt a pipeline structure which first identify all pronouns and entities then run clustering algorithms (Haghighi and Klein, 2009; Lee et al., 2011; Durrett and Klein, 2013; Björkelund and Kuhn, 2014).",
      "startOffset" : 134,
      "endOffset" : 230
    }, {
      "referenceID" : 30,
      "context" : "24 attention mechanism which could implicitly capture inter-word dependencies in an unsupervised way (Vaswani et al., 2017).",
      "startOffset" : 101,
      "endOffset" : 123
    }, {
      "referenceID" : 30,
      "context" : "The word embedding WE(wi) and position embedding PE(wi) are the same as in normal Transformer architectures (Vaswani et al., 2017).",
      "startOffset" : 108,
      "endOffset" : 130
    }, {
      "referenceID" : 3,
      "context" : "Task-oriented Our task-oriented dialogue system contains an intention classifier built on FastText(Bojanowski et al., 2017) and a set of templates that perform policy decision and slot-value filling sequentially.",
      "startOffset" : 98,
      "endOffset" : 123
    } ],
    "year" : 2019,
    "abstractText" : "Recent research has made impressive progress in single-turn dialogue modelling. In the multi-turn setting, however, current models are still far from satisfactory. One major challenge is the frequently occurred coreference and information omission in our daily conversation, making it hard for machines to understand the real intention. In this paper, we propose rewriting the human utterance as a pre-process to help multi-turn dialgoue modelling. Each utterance is first rewritten to recover all coreferred and omitted information. The next processing steps are then performed based on the rewritten utterance. To properly train the utterance rewriter, we collect a new dataset with human annotations and introduce a Transformer-based utterance rewriting architecture using the pointer network. We show the proposed architecture achieves remarkably good performance on the utterance rewriting task. The trained utterance rewriter can be easily integrated into online chatbots and brings general improvement over different domains.1",
    "creator" : "LaTeX with hyperref package"
  }
}