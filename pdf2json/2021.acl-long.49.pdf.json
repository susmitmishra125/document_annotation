{
  "name" : "2021.acl-long.49.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "OoMMix: Out-of-manifold Regularization in Contextual Embedding Space for Text Classification",
    "authors" : [ "Seonghyeon Lee", "Dongha Lee", "Hwanjo Yu" ],
    "emails" : [ "sh0416@postech.ac.kr", "dongha.lee@postech.ac.kr", "hwanjoyu@postech.ac.kr" ],
    "sections" : [ {
      "heading" : null,
      "text" : "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 590–599\nAugust 1–6, 2021. ©2021 Association for Computational Linguistics\n590"
    }, {
      "heading" : "1 Introduction",
      "text" : "Neural networks with a word embedding table have been the most popular approach to a wide range of NLP applications. The great success of transformer-based contextual embeddings as well as masked language models (Devlin et al., 2019; Liu et al., 2019b; Raffel et al., 2020) makes it possible to exploit the pre-trained weights, fully optimized by using large-scale corpora, and it brought a major breakthrough to many problems. For this reason, most recent work on text classification has achieved state-of-the-art performances by finetuning the network initialized with the pre-trained\n∗ Corresponding author\nweight (Devlin et al., 2019). However, they suffer from extreme over-parameterization due to the large pre-trained weight, which allows them to be easily overfitted to its relatively small training data.\nAlong with outstanding performances of the pretrained weight, researchers have tried to reveal the underlying structure encoded in its embedding space (Rogers et al., 2021). One of the important findings is that the contextual embeddings computed from words usually form a low-dimensional manifold (Ethayarajh, 2019). In particular, a quantitative analysis on the space (Cai et al., 2021), which measured the effective dimension size of BERT after applying PCA on its contextual embedding vectors, showed that 33% of dimensions covers 80% of the variance. In other words, only the low-dimensional subspace is utilized for finetuning BERT, although a high-dimensional space (i.e., model weights with a high capacity) is provided for training. Based on this finding on contextual embedding space, we aim to regularize the contextual embedding space for addressing the problem of over-parameterization, while focusing on the outside of the manifold (i.e., out-of-manifold) that cannot be accessed through the words.\nIn this work, we propose a novel approach to discovering and leveraging the out-of-manifold for contextual embedding regularization. The key idea of our out-of-manifold regularization is to produce the embeddings that are located outside the manifold and utilize them to fine-tune the network for a target task. To effectively interact with the contextual embedding of BERT, we adopt two additional modules, named as embedding generator and manifold discriminator. Specifically, 1) the generator synthesizes the out-of-manifold embeddings by linearly interpolating two input embeddings computed from actually-observed words, and 2) the discriminator identifies whether an input embedding comes from the generator (i.e., the synthesized embed-\nding) or the sequence of words (i.e., the actual embedding). The joint optimization encourages the generator to output the out-of-manifold embeddings that can be easily distinguished from the actual embeddings by the discriminator, and the discriminator to learn the decision boundary between the in-manifold and out-of-manifold embeddings. In the end, the fine-tuning on the synthesized out-of-manifold embeddings tightly regularizes the contextual embedding space of BERT.\nThe experimental results on several text classification benchmarks validate the effectiveness of our approach. In particular, our approach using a parameterized generator significantly outperforms the state-of-the-art mixup approach whose mixing strategy needs to be manually given by a programmer. Furthermore, our approach shows good compatibility with various data augmentation techniques, since the target space we focus on for regularization (i.e., out-of-manifold) does not overlap with the space the data augmentation techniques have paid attention to (i.e., in-manifold). The in-depth analyses on our modules provide an insight into how the out-of-manifold regularization manipulates the contextual embedding space of BERT."
    }, {
      "heading" : "2 Related Work",
      "text" : "In this section, we briefly review two approaches to regularizing over-parameterized network based on auxiliary tasks and auxiliary data."
    }, {
      "heading" : "2.1 Regularization using Auxiliary Tasks",
      "text" : "Regularization is an essential tool for good generalization capability of neural networks. One representative regularization approach relies on designing auxiliary tasks. Liu et al. (2019a) firstly showed promising results by unifying a bunch of heterogeneous tasks and training a single unified model for all the tasks. In particular, the synthesized task that encodes desirable features or removes undesirable features turns out to be helpful for network regularization. Devlin et al. (2019) introduced the task which restores masked sentences, termed as masked language model, to encode the distributional semantic in the network; this considerably boosts the overall performance of NLP applications. In addition, Clark et al. (2020) regularized the network by discriminating generated tokens from a language model, and Gong et al. (2018) utilized an additional discriminator to remove the information about word frequency implicitly encoded in the\nword embeddings."
    }, {
      "heading" : "2.2 Regularization using Auxiliary Data",
      "text" : "Another approach to network regularization is to take advantage of auxiliary data, mainly obtained by data augmentation, which eventually supplements the input data space. Inspired by (Bengio et al., 2011) that additionally trained the network with noised (i.e., augmented) images in computer vision, Wei and Zou (2019) simply augmented sentences by adding a small perturbation to the original sentences, such as adding, deleting, and swapping words within the sentences. Recent work tried to further exploit the knowledge from a pretrained model for augmenting the sentences: sentence back translation by using a pre-trained translation model (Xie et al., 2019), and masked sentence reconstruction by using a pre-trained masked language model (Ng et al., 2020).\nMixup (Zhang et al., 2018) is also a kind of data augmentation but differs in that it performs linear interpolation on multiple input sentences and their corresponding labels. Verma et al. (2019) validated that mixup in the hidden space (instead of the input space) is also effective for regularization, and Guo et al. (2019b) found that mixup of images can regularize the out-of-manifold in image representations. In the case of NLP domain, Guo et al. (2019a) and Guo (2020) firstly adopted mixup to text data for text classification, using the traditional networks such as CNN and LSTM; they sample their mixing coefficients from the beta distribution at the sentence-level and at the word-level, respectively. To fully utilize the contextual embedding of transformer-based networks, Chen et al. (2020) applied mixup in the word-level contextual embedding space using a pre-trained language model (i.e., BERT), whereas Sun et al. (2020) focused on mixup in the sentence-level embedding space specifically for improving GLUE score."
    }, {
      "heading" : "3 Method",
      "text" : "In this section, we propose a novel mixup approach, termed as OoMMix, to regularize the outof-manifold in contextual embedding space for text classification. We first briefly remind the architecture of BERT, then introduce two modules used for out-of-manifold regularization, which are embedding generator and manifold discriminator."
    }, {
      "heading" : "3.1 Preliminary",
      "text" : "BERT is a stack of M transformer encoders pretrained on the objective of the masked language model (Devlin et al., 2019). First, a raw sentence is split into the sequence of tokens x ∈ {0, ..., |V |}L using a tokenizer with the vocabulary V , where L is the sequence length. Each token is mapped into a D-dimensional vector based on the embedding table. The sequence of embedding vectors h(0) ∈ RL×D is transformed into the m-th contextual embedding h(m) ∈ RL×D by m transformer layers (Vaswani et al., 2017).\nWe fine-tune the pre-trained weight to classify input texts into C classes. A classifier produces the classification probability vector o ∈ RC using the last contextual embedding h(M). Then, the optimization problem is defined based on a labeled dataset D = {(x1, y1) , ..., (xN , yN )}.\nminimize wf E (x,y)∈D\n[ LC (x, y) ] LC (x, y) := Lkl (f (x) , ey)\nwhere Lkl is the Kullback-Leibler divergence and ey ∈ RC is a one-hot vector representing the label y. The function f is the whole process from h(0) to o, called a target model, and wf is the trainable parameters for the function f , including the pretrained weight of BERT and the parameters in the classifier. For notation, f can be split into several sub-processes f(x) = (fm′ ◦ hm ′ m ◦ hm0 )(x) where hm ′\nm (x) maps the m-th contextual embedding into the m′-th contextual embedding through the layers."
    }, {
      "heading" : "3.2 Embedding Generator",
      "text" : "The goal of our generator network G is to synthesize an artificial contextual embedding by taking\ntwo contextual embeddings (obtained from layer mg) as its input. We use linear interpolation so that the new embedding belongs to the line segment defined by the two input embeddings. Since we limit the search space, the generator produces a single scalar value λ ∈ [0, 1], called a mixing coefficient.\nG ( h (mg) 1 ,h (mg) 2 ) = λ · h(mg)1 + (1− λ) · h (mg) 2\nλ = g ( h (mg) 1 ,h (mg) 2 ) We introduce the distribution of the mixing coefficient to model its uncertainty. To this end, our generator network produces the lower bound α and the interval ∆ by using h(mg)1 and h (mg) 2 , so as to sample the mixing coefficient from the uniform distribution U (α, α+ ∆).\nTo avoid massive computational overhead incurred by the concatenation of two input sequences (Reimers and Gurevych, 2019), we adopt the Siamese architecture that uses the shared weights on two different inputs. The generator first transforms each sequence of contextual embedding vectors by using a single transformer layer, then obtains the sentence-level embedding by averaging all the embedding vectors in the sequence. From the two sentence-level embeddings s1, s2 ∈ RD, the generator obtains the concatenated embedding s = s1 ⊕ s2 ∈ R2D and calculates α and ∆ by using a two-layer fully-connected network with the softmax normalization. Specifically, the last fully-connected layer outputs a normalized 3-dimensional vector, whose first and second values become α and ∆, thereby the range of sampling distribution (α, α + ∆) lies in [0, 1]. In this work, we consider the structure of the generator to efficiently process the sequential input,\nbut any other structures focusing on different aspects (e.g. the network that enlarges the search space) can be used as well. For effective optimization of λ sampled from U (α, α+ ∆), we apply the re-parameterization trick which decouples the sampling process from the computational graph (Kingma and Welling, 2014). That is, we compute the mixing coefficient by using γ ∼ U (0, 1).\nλ = α+ γ ×∆\nThe optimization problem for text classification can be extended to the new embeddings and their labels, provided by the generator network.\nminimize wfmg ,wg E (x1,y1)∈D\n[ LG (x1, y1) ] (1)\nLG (x1, y1) := E (x2,y2)∈D\n[ Lkl(fmg(h̃), ỹ) ] λ ∼ g ( h mg 0 (x1) , h mg 0 (x2)\n) h̃ := λ · hmg0 (x1) + (1− λ) · h mg 0 (x2)\nỹ := λ · ey1 + (1− λ) · ey2 where wfmg is the trainable parameters of the function fmg (i.e., the process from h\n(mg) to o), and wG is the ones for the generator. Similar to other mixup techniques, we impose the mixed label on the generated embedding."
    }, {
      "heading" : "3.3 Manifold Discriminator",
      "text" : "We found that the supervision from the objective (1) is not enough to train the generator. The objective optimizes the generator to produce the embeddings that are helpful for the target classification. However, since the over-parameterized network tends to memorize all training data, the target model also simply memorizes the original data to minimize Equation (1). In this situation, the generator is more likely to mimic the embeddings seen in the training set (memorized by the target model) rather than generate novel embeddings. For this reason, we need more useful supervision for the generator, to make it output the out-of-manifold embeddings.\nTo tackle this challenge, we define an additional task that identifies whether a contextual embedding comes from the generator or actual words. The purpose of this task is to learn the discriminative features between actual embeddings and generated embeddings, in order that we can easily discover the subspace which cannot be accessed through the actually-observed words. For this task, we introduce a discriminator network D that serves as a\nbinary classifier in the contextual embedding space of the md-th transformer layer.\nThe discriminator takes a contextual embedding h(md) and calculates the score s ∈ [0, 1] which indicates the probability that h(md) comes from an actual sentence (i.e., h(md) is located inside the manifold). Its network structure is similar to that of the generator, except that the concatenation is not needed and the output of the two-layer fully connected network produces a single scalar value. As discussed in Section 3.2, any network structures for focusing on different aspects can be employed.\nThe optimization of the generator and discriminator for this task is described as follows.\nminimize wg ,wd E (x1,y1)∈D\n[ LD (x1) ] (2)\nLD (x1) := E (x2,y2)∈D\n[ Lbce(D(hmdmg(h̃)), 0)\n+Lbce (D (hmd0 (x)) , 1) ]\nwhereLbce is the binary cross entropy loss. By minimizing this objective, our generator can produce the out-of-manifold embeddings that are clearly distinguished from the actual (in-manifold) contextual embeddings by the discriminator."
    }, {
      "heading" : "3.4 Training",
      "text" : "We jointly optimize the two objectives to train the embedding generator. Equation (1) encourages the generator to produce the embeddings which are helpful for the target task, while Equation (2) makes the generator produce the new embeddings different from the contextual embeddings obtained from the words. The final objective is defined by\nE (x,y)∼D [LC (x, y) + LG (x, y) + eLD(x)]\nwhere e regulates the two objectives. The generator and discriminator collaboratively search out informative out-of-manifold embeddings for the target task while being optimized with the target model, thereby the generated embeddings can effectively regularize the out-of-manifold."
    }, {
      "heading" : "4 Experiments",
      "text" : "In this section, we present the experimental results supporting the superiority of OoMMix among the recent mixup approaches in text classification. Also, we investigate its compatibility with other data augmentation techniques. Finally, we provide in-depth analyses on our approach to further validate the effect of out-of-manifold regularization."
    }, {
      "heading" : "4.1 Experimental setup",
      "text" : "Our experiments consider 4 sentence classification benchmarks (Zhang et al., 2015) of various scales. The statistics of the datasets are summarized in Table 1. We follow the experimental setup used in (Chen et al., 2020) to directly compare the results with ours. Specifically, we split the whole training set into training/validation sets, while leaving out the official test set for evaluation. We choose the classification accuracy as the evaluation metric, considering the datasets are already class-balanced. For the various sizes of training set from 0.5K to 35K, we apply stratified sampling to preserve the balanced class distributions.\nIn terms of optimization, we use BERT provided by huggingface for the classification tasks.1 The Adam optimizer is used to fine-tune BERT with the linear warm-up for the first 1000 iterations, and the initial learning rates for the pre-trained weight and the target classifier are set to 2e-5 and 1e-3, respectively. We set the batch size to 12 and the dropout probability to 0.1. We attach the generator and discriminator at the third layer (mg = 3) and the last layer (md = 12), respectively. The two objectives equally contribute to training the generator, e = 1, but we increase the e value if the\n1In our experiments, we use the checkpoint bert-baseuncased as the pre-trained weight.\ndiscriminator fails to discriminate the embeddings. The accuracy is evaluated on validation set every 200 iterations, and stop training when the accuracy does not increase for 10 consecutive evaluations. We report the classification accuracy on the test set at the best validation checkpoint and repeat the experiment three times with different random seeds to report the average with its standard deviation. We implement the code using PyTorch and use NVIDIA Titan Xp for parallel computation. In our environment, the training spends about 30 minutes to 3 hours depending on the dataset."
    }, {
      "heading" : "4.2 Comparision with Mixup Approaches",
      "text" : "We compare OoMMix with existing mixup techniques. All the existing methods manually set the mixing coefficient, whereas we parameterize the linear interpolation by the embedding generator, optimized to produce out-of-manifold embeddings.\n• NonlinearMix (Guo, 2020) samples mixing coefficients for each word from the beta distribution, while using neural networks to produce the mixing coefficient for the label. We apply this approach to BERT.\n• mixup-transformer (Sun et al., 2020) linearly interpolates the sentence-level embedding with a fixed mixing coefficient. The mixing coefficient is 0.5 as the paper suggested.\n• TMix (Chen et al., 2020) performs linear interpolation on the word-level contextual embedding space and samples a mixing coefficient from the beta distribution. We select the best accuracy among different alpha configurations {0.05, 0.1} for the beta distribution.\n• MixText (Chen et al., 2020) additionally utilizes unlabeled data by combining TMix with its pseudo-labeling technique.\nTable 2 reports the accuracy on various sentence classification benchmarks. In most cases, OoMMix achieves the best performance among all the competing mixup approaches. In the case of NonlinearMix, it sometimes shows worse performance than the baseline (i.e., fine-tuning only on original data), because its mixup strategy introduces a large degree of freedom in the search space, which loses useful semantic encoded in the pre-trained weight. The state-of-the-art mixup approaches, TMix and mixup-transformer, slightly improves the accuracy over the baseline, while showing the effectiveness of the mixup approach. Finally, OoMMix beats all the previous mixup approaches, which strongly indicates that the embeddings mixed by the generator are more effective for regularization, compared to the embeddings manually mixed by the existing approaches. It is worth noting that OoMMix obtains a comparable performance to MixText, even without utilizing additional unlabeled data. In conclusion, discovering the out-of-manifold and applying mixup for such subspace are beneficial in\ncontextual embedding space."
    }, {
      "heading" : "4.3 Compatibility with Data Augmentations",
      "text" : "To demonstrate that the regularization effect of OoMMix does not conflict with that of existing data augmentation techniques, we investigate the performance of BERT that adopts both OoMMix and other data augmentations together. Using three popular data augmentation approaches in the NLP community, we replicate the dataset as large as the original one to use them for fine-tuning.\n• EDA (Wei and Zou, 2019) is a simple augmentation approach that randomly inserts/deletes words or swaps two words in a sentence. We used the official codes2 with the default insertion/deletion/swap ratio the author provided.\n• BT (Xie et al., 2019) uses the back-translation for data augmentation. A sentence is translated into another language, then translated back into the original one. We use the code implemented in the MixText repository3 with the checkpoint fairseq provided.4\n• SSMBA (Ng et al., 2020) makes use of the pre-trained masked language model. They mask the original sentence and reconstruct it by filling in the masked portion. We use the codes provided by the authors5 with default\n2https://github.com/jasonwei20/eda_nlp 3https://github.com/GT-SALT/MixText 4transformer.wmt19.{en-ru,ru-en}.single\nmodel are provided through the official torch hub. 5https://github.com/nng555/ssmba\nmasked proportion and the pre-trained weight.\nFigure 2 shows the effectiveness of OoMMix when being used with the data augmentation techniques. For all the cases, OoMMix shows consistent improvement. Especially for the Amazon Review dataset, the data augmentation and our mixup strategy independently bring the improvement of the accuracy, because the subspaces targeted by the data augmentation and OoMMix do not overlap with each other. That is, OoMMix finds out out-of-manifold embedding, which cannot be generated from the actual sentences, whereas the data augmentations (i.e., EDA, BT, and SSMBA) focus on augmenting the sentences whose embeddings are located inside the manifold. Therefore, jointly applying the two techniques allows to tightly regularize the contextual embedding space, including both in-manifold and out-of-manifold.\nMoreover, OoMMix has additional advantages over the data augmentations. First, OoMMix is still effective in the case that large training data are available. The data augmentation techniques result in less performance gain as the size of training data becomes larger, because there is less room for enhancing the manifold constructed by enough training data. Second, the class label of the augmented sentences given by the data augmentation techniques (i.e., the same label with the original sentences) can be noisy for sentence classification, compared to the label of out-of-manifold embeddings generated by OoMMix. This is because the assumption that the augmented sentences have the\nsame label with their original sentences is not always valid. On the contrary, there do not exist actual (or ground truth) labels for out-of-manifold embeddings, as they do not correspond to actual sentences; this allows our mixup label to be less noisy for text classification."
    }, {
      "heading" : "4.4 Effect of the Manifold Discriminator",
      "text" : "We also investigate how the manifold discriminator affects the training of the embedding generator. Precisely, we compare the distributions of mixing coefficients, obtained from two different generators; they are optimized with/without the manifold discriminator, respectively (Figure 3 Upper/Lower). We partition the training process into two phases (i.e., the first and second half), and plot a histogram of the mixing coefficients in each phase.\nThe embedding generator without the discriminator gradually moves the distribution of the mixing coefficients toward zero, which means that the generated embedding becomes similar to the actual embedding. Therefore, training the generator without the discriminator fails to produce novel embeddings, which cannot be seen in the original data. In contrast, in the case of the generator with the discriminator, most of the mixing coefficients are located around 0.5, which implies that the generator produces the embeddings which are far from both the two actual embeddings to some extent. We also observe that the average objective value for our discrimination task (Equation (2)) is 0.208 for the last 20 mini-batches; this is much lower than 0.693 at the initial point. It indicates that the generated embeddings are quite clearly distinguished from the ones computed from actual sentences."
    }, {
      "heading" : "4.5 Effect of Different Embedding Layers",
      "text" : "We further examine the effect of the location of our generator and discriminator (i.e., mg and md) on the final classification performance. Figure 4 illustrates the changes of the classification accuracy with respect to the target contextual embedding layers the modules are attached to. To sum up, BERT achieves high accuracy when the generator is attached to the contextual embedding lower than the sixth layer while the discriminator works for a higher layer. It makes our out-of-manifold regularization affect more parameters in overall layers, which eventually leads to higher accuracy. On the other hand, in case that we use both the generator and discriminator in the same layer, the gradient of the loss for manifold discrimination cannot guide the generator to output out-of-manifold embeddings, and as a result, the generator is not able to generate useful embeddings."
    }, {
      "heading" : "4.6 Manifold Visualization",
      "text" : "Finally, we visualize our contextual embedding space to qualitatively show that OoMMix discovers and leverages the space outside the manifold for regularization. We apply Isomap (Tenenbaum et al., 2000), a neighborhood-based kernel PCA for dimensionality reduction, to both the actual sentence embeddings and generated embeddings. We simply use the Isomap function provided by scikit-learn, and set the number of the neighbors to 15. Figure 5 shows the yz-plane and xy-plane of our embedding space, whose dimensionality is reduced to 3 (i.e., x, y, and z). We use different colors to represent the class of the actual embeddings as well as the predicted class of the generated embeddings.\nIn the yz-plane, the actual sentence embeddings form multiple clusters, optimized for the text clas-\nsification task. At the same time, the generated embeddings are located in the different region from the space enclosing most of the actual embeddings. In the second plot, we colorize the generated embeddings with their predicted class. The predicted class of out-of-manifold embeddings are well-aligned with that of the actual embeddings, which means that OoMMix imposes the classification capability on the out-of-manifold region as well. We change the camera view to xy-plane and repeat the same process to show the alignment of class distribution clearly (in the third/fourth plots). By imposing the classification capability on the extended dimension/subspace (i.e., out-of-manifold), OoMMix significantly improves the classification performance for the original dimension/subspace (i.e., in-manifold)."
    }, {
      "heading" : "5 Conclusion",
      "text" : "This paper proposes OoMMix to regularize out-ofmanifold in the contextual embedding space. Our main motivation is that the embeddings computed from the words only utilize a low-dimensional manifold while a high-dimensional space is available for the model capacity. Therefore, OoMMix discovers the embeddings that are useful for the target task but cannot be accessed through the words. With the help of the manifold discriminator, the embedding generator successfully produces out-of-manifold embeddings with their labels. We demonstrate the effectiveness of OoMMix and its compatibility with the existing data augmentation techniques.\nOur approach is a bit counter-intuitive in that the embeddings that cannot be accessed through the actual words are helpful for the target model. As the discrete features from texts (i.e., words), embedded into the high-dimensional continuous space where\ntheir contexts are encoded, cannot cover the whole space, the uncovered space also should be carefully considered for any target tasks. In this sense, we need to regularize the out-of-manifold to prevent anomalous behavior in that space, which is especially important for a large pre-trained contextual embedding space."
    }, {
      "heading" : "Acknowledgments",
      "text" : "This work was supported by the NRF grant funded by the MSIT (No. 2020R1A2B5B03097210), and the IITP grant funded by the MSIT (No. 2018-0- 00584, 2019-0-01906)."
    }, {
      "heading" : "A Preprocessing decisions, model parameters and other details",
      "text" : "We list the minor implementation details but useful for reproducing our experiments.\n• The train/validation split is implemented using train_test_split function in scikitlearn with seed 42.\n• The bert-base-uncased tokenizer provided by huggingface is used to split the sentence.\n• We take the first 256 tokens for the sentence which length is longer than 256.\n• The embedding table in the pre-trained weight is frozen for all experiments.\n• The data cleaning process in EDA deteriorates the performance, so we omit that process.\n• Due to the different optimization variables for the two objectives, we perform the backward process twice and update the parameter."
    }, {
      "heading" : "B Hyper-parameter search",
      "text" : "Since performing grid search on all datasets is intolerable due to the lack of computational resources, we perform different configurations on one small dataset. The candidate for the embedding layer for the generator (mg) is [0, 2, 4, 6, 8, 10] and the candidate for the embedding layer for the discriminator (md) is [0, 2, 4, 6, 8, 10, 12]. For the case the discriminator could not be trained well, e.g. the discriminator loss does not decrease at all, we increase e to give more weight to the discriminator loss. For all the experiments, we fix themg andmd and manually change the e to make the discriminator classify the embedding. The hyper-parameter choices are summarized in Table 3."
    } ],
    "references" : [ {
      "title" : "Deep learners benefit more from out-of-distribution examples",
      "author" : [ "van Pascanu", "Salah Rifai", "François Savard", "Guillaume Sicard" ],
      "venue" : "In Proceedings of the Fourteenth International Conference on Artificial Intelligence and Statistics,",
      "citeRegEx" : "Pascanu et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Pascanu et al\\.",
      "year" : 2011
    }, {
      "title" : "Isotropy in the contextual embedding space: Clusters and manifolds",
      "author" : [ "Xingyu Cai", "Jiaji Huang", "Yuchen Bian", "Kenneth Church." ],
      "venue" : "International Conference on Learning Representations.",
      "citeRegEx" : "Cai et al\\.,? 2021",
      "shortCiteRegEx" : "Cai et al\\.",
      "year" : 2021
    }, {
      "title" : "MixText: Linguistically-informed interpolation of hidden space for semi-supervised text classification",
      "author" : [ "Jiaao Chen", "Zichao Yang", "Diyi Yang." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 2147–",
      "citeRegEx" : "Chen et al\\.,? 2020",
      "shortCiteRegEx" : "Chen et al\\.",
      "year" : 2020
    }, {
      "title" : "Electra: Pretraining text encoders as discriminators rather than generators",
      "author" : [ "Kevin Clark", "Minh-Thang Luong", "Quoc V. Le", "Christopher D. Manning." ],
      "venue" : "International Conference on Learning Representations.",
      "citeRegEx" : "Clark et al\\.,? 2020",
      "shortCiteRegEx" : "Clark et al\\.",
      "year" : 2020
    }, {
      "title" : "BERT: Pre-training of deep bidirectional transformers for language understanding",
      "author" : [ "Jacob Devlin", "Ming-Wei Chang", "Kenton Lee", "Kristina Toutanova." ],
      "venue" : "Proceedings of the 2019 Conference of the North American Chapter of the Association",
      "citeRegEx" : "Devlin et al\\.,? 2019",
      "shortCiteRegEx" : "Devlin et al\\.",
      "year" : 2019
    }, {
      "title" : "How contextual are contextualized word representations? comparing the geometry of BERT, ELMo, and GPT-2 embeddings",
      "author" : [ "Kawin Ethayarajh." ],
      "venue" : "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the",
      "citeRegEx" : "Ethayarajh.,? 2019",
      "shortCiteRegEx" : "Ethayarajh.",
      "year" : 2019
    }, {
      "title" : "Frage: Frequency-agnostic word representation",
      "author" : [ "Chengyue Gong", "Di He", "Xu Tan", "Tao Qin", "Liwei Wang", "Tie-Yan Liu." ],
      "venue" : "Advances in Neural Information Processing Systems, volume 31, pages 1334– 1345. Curran Associates, Inc.",
      "citeRegEx" : "Gong et al\\.,? 2018",
      "shortCiteRegEx" : "Gong et al\\.",
      "year" : 2018
    }, {
      "title" : "Nonlinear mixup: Out-ofmanifold data augmentation for text classification",
      "author" : [ "Hongyu Guo." ],
      "venue" : "Proceedings of the AAAI Conference on Artificial Intelligence, volume 34, pages 4044–4051.",
      "citeRegEx" : "Guo.,? 2020",
      "shortCiteRegEx" : "Guo.",
      "year" : 2020
    }, {
      "title" : "Augmenting data with mixup for sentence classification: An empirical study",
      "author" : [ "Hongyu Guo", "Yongyi Mao", "Richong Zhang." ],
      "venue" : "arXiv preprint arXiv:1905.08941.",
      "citeRegEx" : "Guo et al\\.,? 2019a",
      "shortCiteRegEx" : "Guo et al\\.",
      "year" : 2019
    }, {
      "title" : "Mixup as locally linear out-of-manifold regularization",
      "author" : [ "Hongyu Guo", "Yongyi Mao", "Richong Zhang." ],
      "venue" : "Proceedings of the AAAI Conference on Artificial Intelligence, volume 33, pages 3714–3722.",
      "citeRegEx" : "Guo et al\\.,? 2019b",
      "shortCiteRegEx" : "Guo et al\\.",
      "year" : 2019
    }, {
      "title" : "AutoEncoding Variational Bayes",
      "author" : [ "Diederik P. Kingma", "Max Welling." ],
      "venue" : "2nd International Conference on Learning Representations, ICLR 2014, Banff, AB, Canada, April 14-16, 2014, Conference Track Proceedings.",
      "citeRegEx" : "Kingma and Welling.,? 2014",
      "shortCiteRegEx" : "Kingma and Welling.",
      "year" : 2014
    }, {
      "title" : "Multi-task deep neural networks for natural language understanding",
      "author" : [ "Xiaodong Liu", "Pengcheng He", "Weizhu Chen", "Jianfeng Gao." ],
      "venue" : "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 4487–4496, Flo-",
      "citeRegEx" : "Liu et al\\.,? 2019a",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2019
    }, {
      "title" : "Roberta: A robustly optimized bert pretraining approach",
      "author" : [ "Yinhan Liu", "Myle Ott", "Naman Goyal", "Jingfei Du", "Mandar Joshi", "Danqi Chen", "Omer Levy", "Mike Lewis", "Luke Zettlemoyer", "Veselin Stoyanov." ],
      "venue" : "arXiv preprint arXiv:1907.11692.",
      "citeRegEx" : "Liu et al\\.,? 2019b",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2019
    }, {
      "title" : "SSMBA: Self-supervised manifold based data augmentation for improving out-of-domain robustness",
      "author" : [ "Nathan Ng", "Kyunghyun Cho", "Marzyeh Ghassemi." ],
      "venue" : "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Process-",
      "citeRegEx" : "Ng et al\\.,? 2020",
      "shortCiteRegEx" : "Ng et al\\.",
      "year" : 2020
    }, {
      "title" : "Exploring the limits of transfer learning with a unified text-totext transformer",
      "author" : [ "Colin Raffel", "Noam Shazeer", "Adam Roberts", "Katherine Lee", "Sharan Narang", "Michael Matena", "Yanqi Zhou", "Wei Li", "Peter J. Liu." ],
      "venue" : "Journal of Machine Learning Re-",
      "citeRegEx" : "Raffel et al\\.,? 2020",
      "shortCiteRegEx" : "Raffel et al\\.",
      "year" : 2020
    }, {
      "title" : "SentenceBERT: Sentence embeddings using Siamese BERTnetworks",
      "author" : [ "Nils Reimers", "Iryna Gurevych." ],
      "venue" : "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natu-",
      "citeRegEx" : "Reimers and Gurevych.,? 2019",
      "shortCiteRegEx" : "Reimers and Gurevych.",
      "year" : 2019
    }, {
      "title" : "A primer in bertology: What we know about how bert works",
      "author" : [ "Anna Rogers", "Olga Kovaleva", "Anna Rumshisky." ],
      "venue" : "Transactions of the Association for Computational Linguistics, 8:842–866.",
      "citeRegEx" : "Rogers et al\\.,? 2021",
      "shortCiteRegEx" : "Rogers et al\\.",
      "year" : 2021
    }, {
      "title" : "Mixuptransformer: Dynamic data augmentation for NLP tasks",
      "author" : [ "Lichao Sun", "Congying Xia", "Wenpeng Yin", "Tingting Liang", "Philip Yu", "Lifang He." ],
      "venue" : "Proceedings of the 28th International Conference on Computational Linguistics, pages 3436–",
      "citeRegEx" : "Sun et al\\.,? 2020",
      "shortCiteRegEx" : "Sun et al\\.",
      "year" : 2020
    }, {
      "title" : "A global geometric framework for nonlinear dimensionality reduction",
      "author" : [ "Joshua B Tenenbaum", "Vin De Silva", "John C Langford." ],
      "venue" : "science, 290(5500):2319–2323.",
      "citeRegEx" : "Tenenbaum et al\\.,? 2000",
      "shortCiteRegEx" : "Tenenbaum et al\\.",
      "year" : 2000
    }, {
      "title" : "Attention is all you need",
      "author" : [ "Ashish Vaswani", "Noam Shazeer", "Niki Parmar", "Jakob Uszkoreit", "Llion Jones", "Aidan N Gomez", "Ł ukasz Kaiser", "Illia Polosukhin." ],
      "venue" : "Advances in Neural Information Processing Systems, volume 30, pages 5998–6008. Cur-",
      "citeRegEx" : "Vaswani et al\\.,? 2017",
      "shortCiteRegEx" : "Vaswani et al\\.",
      "year" : 2017
    }, {
      "title" : "Manifold mixup: Better representations by interpolating hidden states",
      "author" : [ "Vikas Verma", "Alex Lamb", "Christopher Beckham", "Amir Najafi", "Ioannis Mitliagkas", "David Lopez-Paz", "Yoshua Bengio." ],
      "venue" : "Proceedings of the 36th International Conference on",
      "citeRegEx" : "Verma et al\\.,? 2019",
      "shortCiteRegEx" : "Verma et al\\.",
      "year" : 2019
    }, {
      "title" : "EDA: Easy data augmentation techniques for boosting performance on text classification tasks",
      "author" : [ "Jason Wei", "Kai Zou." ],
      "venue" : "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International",
      "citeRegEx" : "Wei and Zou.,? 2019",
      "shortCiteRegEx" : "Wei and Zou.",
      "year" : 2019
    }, {
      "title" : "Unsupervised data augmentation for consistency training",
      "author" : [ "Qizhe Xie", "Zihang Dai", "Eduard Hovy", "Minh-Thang Luong", "Quoc V Le." ],
      "venue" : "arXiv preprint arXiv:1904.12848.",
      "citeRegEx" : "Xie et al\\.,? 2019",
      "shortCiteRegEx" : "Xie et al\\.",
      "year" : 2019
    }, {
      "title" : "mixup: Beyond empirical risk minimization",
      "author" : [ "Hongyi Zhang", "Moustapha Cisse", "Yann N. Dauphin", "David Lopez-Paz." ],
      "venue" : "International Conference on Learning Representations.",
      "citeRegEx" : "Zhang et al\\.,? 2018",
      "shortCiteRegEx" : "Zhang et al\\.",
      "year" : 2018
    }, {
      "title" : "Character-level convolutional networks for text classification",
      "author" : [ "Xiang Zhang", "Junbo Zhao", "Yann LeCun." ],
      "venue" : "Advances in Neural Information Processing Systems, volume 28, pages 649–657. Curran Associates, Inc.",
      "citeRegEx" : "Zhang et al\\.,? 2015",
      "shortCiteRegEx" : "Zhang et al\\.",
      "year" : 2015
    } ],
    "referenceMentions" : [ {
      "referenceID" : 4,
      "context" : "The great success of transformer-based contextual embeddings as well as masked language models (Devlin et al., 2019; Liu et al., 2019b; Raffel et al., 2020) makes it possible to exploit the pre-trained weights, fully optimized by using large-scale corpora, and it brought a major breakthrough to many problems.",
      "startOffset" : 95,
      "endOffset" : 156
    }, {
      "referenceID" : 12,
      "context" : "The great success of transformer-based contextual embeddings as well as masked language models (Devlin et al., 2019; Liu et al., 2019b; Raffel et al., 2020) makes it possible to exploit the pre-trained weights, fully optimized by using large-scale corpora, and it brought a major breakthrough to many problems.",
      "startOffset" : 95,
      "endOffset" : 156
    }, {
      "referenceID" : 14,
      "context" : "The great success of transformer-based contextual embeddings as well as masked language models (Devlin et al., 2019; Liu et al., 2019b; Raffel et al., 2020) makes it possible to exploit the pre-trained weights, fully optimized by using large-scale corpora, and it brought a major breakthrough to many problems.",
      "startOffset" : 95,
      "endOffset" : 156
    }, {
      "referenceID" : 16,
      "context" : "the underlying structure encoded in its embedding space (Rogers et al., 2021).",
      "startOffset" : 56,
      "endOffset" : 77
    }, {
      "referenceID" : 5,
      "context" : "One of the important findings is that the contextual embeddings computed from words usually form a low-dimensional manifold (Ethayarajh, 2019).",
      "startOffset" : 124,
      "endOffset" : 142
    }, {
      "referenceID" : 1,
      "context" : "titative analysis on the space (Cai et al., 2021), which measured the effective dimension size of BERT after applying PCA on its contextual embedding vectors, showed that 33% of dimensions covers 80% of the variance.",
      "startOffset" : 31,
      "endOffset" : 49
    }, {
      "referenceID" : 22,
      "context" : "tence back translation by using a pre-trained translation model (Xie et al., 2019), and masked sentence reconstruction by using a pre-trained masked language model (Ng et al.",
      "startOffset" : 64,
      "endOffset" : 82
    }, {
      "referenceID" : 13,
      "context" : ", 2019), and masked sentence reconstruction by using a pre-trained masked language model (Ng et al., 2020).",
      "startOffset" : 89,
      "endOffset" : 106
    }, {
      "referenceID" : 23,
      "context" : "Mixup (Zhang et al., 2018) is also a kind of data augmentation but differs in that it performs linear interpolation on multiple input sentences and their corresponding labels.",
      "startOffset" : 6,
      "endOffset" : 26
    }, {
      "referenceID" : 4,
      "context" : "trained on the objective of the masked language model (Devlin et al., 2019).",
      "startOffset" : 54,
      "endOffset" : 75
    }, {
      "referenceID" : 19,
      "context" : "The sequence of embedding vectors h(0) ∈ RL×D is transformed into the m-th contextual embedding h(m) ∈ RL×D by m transformer layers (Vaswani et al., 2017).",
      "startOffset" : 132,
      "endOffset" : 154
    }, {
      "referenceID" : 15,
      "context" : "To avoid massive computational overhead incurred by the concatenation of two input sequences (Reimers and Gurevych, 2019), we adopt the Siamese architecture that uses the shared weights on two different inputs.",
      "startOffset" : 93,
      "endOffset" : 121
    }, {
      "referenceID" : 2,
      "context" : "TMix† and MixText† report the scores presented in (Chen et al., 2020), where the sizes of domain-related unlabeled data are described in the parenthesis.",
      "startOffset" : 50,
      "endOffset" : 69
    }, {
      "referenceID" : 24,
      "context" : "Our experiments consider 4 sentence classification benchmarks (Zhang et al., 2015) of various scales.",
      "startOffset" : 62,
      "endOffset" : 82
    }, {
      "referenceID" : 2,
      "context" : "in (Chen et al., 2020) to directly compare the results with ours.",
      "startOffset" : 3,
      "endOffset" : 22
    }, {
      "referenceID" : 7,
      "context" : "• NonlinearMix (Guo, 2020) samples mixing coefficients for each word from the beta distribution, while using neural networks to produce the mixing coefficient for the label.",
      "startOffset" : 15,
      "endOffset" : 26
    }, {
      "referenceID" : 17,
      "context" : "• mixup-transformer (Sun et al., 2020) linearly interpolates the sentence-level embedding with a fixed mixing coefficient.",
      "startOffset" : 20,
      "endOffset" : 38
    }, {
      "referenceID" : 2,
      "context" : "• TMix (Chen et al., 2020) performs linear interpolation on the word-level contextual embedding space and samples a mixing coeffi-",
      "startOffset" : 7,
      "endOffset" : 26
    }, {
      "referenceID" : 2,
      "context" : "• MixText (Chen et al., 2020) additionally utilizes unlabeled data by combining TMix with its pseudo-labeling technique.",
      "startOffset" : 10,
      "endOffset" : 29
    }, {
      "referenceID" : 21,
      "context" : "• EDA (Wei and Zou, 2019) is a simple augmentation approach that randomly inserts/deletes words or swaps two words in a sentence.",
      "startOffset" : 6,
      "endOffset" : 25
    }, {
      "referenceID" : 22,
      "context" : "• BT (Xie et al., 2019) uses the back-translation for data augmentation.",
      "startOffset" : 5,
      "endOffset" : 23
    }, {
      "referenceID" : 13,
      "context" : "• SSMBA (Ng et al., 2020) makes use of the pre-trained masked language model.",
      "startOffset" : 8,
      "endOffset" : 25
    } ],
    "year" : 2021,
    "abstractText" : "Recent studies on neural networks with pretrained weights (i.e., BERT) have mainly focused on a low-dimensional subspace, where the embedding vectors computed from input words (or their contexts) are located. In this work, we propose a new approach, called OoMMix, to finding and regularizing the remainder of the space, referred to as out-ofmanifold, which cannot be accessed through the words. Specifically, we synthesize the outof-manifold embeddings based on two embeddings obtained from actually-observed words, to utilize them for fine-tuning the network. A discriminator is trained to detect whether an input embedding is located inside the manifold or not, and simultaneously, a generator is optimized to produce new embeddings that can be easily identified as out-of-manifold by the discriminator. These two modules successfully collaborate in a unified and end-to-end manner for regularizing the out-of-manifold. Our extensive evaluation on various text classification benchmarks demonstrates the effectiveness of our approach, as well as its good compatibility with existing data augmentation techniques which aim to enhance the manifold.",
    "creator" : "LaTeX with hyperref"
  }
}