{
  "name" : "2021.acl-long.425.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "Article Reranking by Memory-Enhanced Key Sentence Matching for Detecting Previously Fact-Checked Claims",
    "authors" : [ "Qiang Sheng", "Juan Cao", "Xueyao Zhang", "Xirong Li", "Lei Zhong" ],
    "emails" : [ "shengqiang18z@ict.ac.cn", "caojuan@ict.ac.cn", "zhangxueyao19s@ict.ac.cn", "zhonglei18s@ict.ac.cn", "xirong@ruc.edu.cn" ],
    "sections" : [ {
      "heading" : null,
      "text" : "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 5468–5481\nAugust 1–6, 2021. ©2021 Association for Computational Linguistics\n5468"
    }, {
      "heading" : "1 Introduction",
      "text" : "Social media posts with false claims have led to real-world threats on many aspects such as politics (Fisher et al., 2016), social order (Wang and Li, 2011), and personal health (Chen, 2020).\nTo tackle this issue, over 300 fact-checking projects have been launched, such as Snopes1 and Jiaozhen2 (Duke Reporters’ Lab, 2020). Meanwhile, automatic systems have been developed for detecting suspicious claims on social media (Zhou et al., 2015; Popat et al., 2018a). This is however not the end. A considerable amount of false claims continually spread, even though they are already proved false. According to a recent report (Xinhua Net, 2019), around 12% of false claims published on Chinese social media, are actually “old”, as they have been debunked previously. Hence, detecting previously fact-checked claims is an important\n1https://www.snopes.com 2https://fact.qq.com/\ntask.\nAccording to the seminal work by Shaar et al. (2020), the task is tackled by a two-stage information retrieval approach. Its typical workflow is illustrated in Figure 1(a). Given a claim as a query, in the first stage a basic searcher (e.g., BM25 Robertson and Zaragoza, 2009) searches for candidate articles from a collection of fact-checking articles (FC-articles). In the second stage, a more powerful model (e.g., BERT, Devlin et al., 2019) reranks the candidates to provide evidence for manual or automatic detection. Existing works focus on the reranking stage: Vo and Lee (2020) model the interactions between a claim and the whole candidate articles, while Shaar et al. (2020) extract several semantically similar sentences from FC-articles as a proxy. Nevertheless, these methods treat FCarticles as general documents and ignore characteristics of FC-articles. Figure 1(b) shows three sentences from candidate articles for the given claim. Among them, S1 is more friendly to semantic matching than S2 and S3 because the whole S1 focuses on describing its topic and does not contain tokens irrelevant to the given claim, e.g., ”has spread over years” in S2. Thus, a semantic-based model does not require to have strong filtering capability. If we use only general methods on this task, the relevant S2 and S3 may be neglected while irrelevant S1 is focused. To let the model focus on key sentences (i.e., sentences as a good proxy of article-level relevance) like S2 and S3, we need to consider two characteristics of FC-articles besides semantics: C1. Claims are often quoted to describe the checked events (e.g., the underlined text in S2); C2. Event-irrelevant patterns to introduce or debunk claims are common in FC-articles (e.g., bold texts in S2 and S3).\nBased on the observations, we propose a novel reranker, MTM (Memory-enhanced Transformers for Matching). The reranker identifies key sentences per article using claim- and pattern-sentence relevance, and then integrates information from the claim, key sentences, and patterns for article-level relevance prediction. In particular, regarding C1, we propose ROUGE-guided Transformer (ROT) to score claim-sentence relevance literally and semantically. As for C2, we obtain the pattern vectors by clustering the difference of sentence and claim vectors for scoring pattern-sentence relevance and store them in the Pattern Memory Bank (PMB). The joint use of ROT and PMB allows us to iden-\ntify key sentences that reflect the two characteristics of FC-articles. Subsequently, fine-grained interactions among claims and key sentences are modeled by the multi-layer Transformer and aggregated with patterns to obtain an article-level feature representation. The article feature is fed into a Multi-layer Perceptron (MLP) to predict the claim-article relevance.\nTo validate the effectiveness of our method, we built the first Chinese dataset for this task with 11,934 claims collected from Chinese Weibo3 and 27,505 fact-checking articles from multiple sources. 39,178 claim-article pairs are annotated as relevant. Experiments on the English dataset and the newly built Chinese dataset show that MTM outperforms existing methods. Further human evaluation and case studies prove that MTM finds key sentences as explanations. Our main contributions are as follows:\n• We propose a novel reranker MTM for factchecked claim detection, which can better identify key sentences in fact-checking articles by exploiting their characteristics. • We design ROUGE-guided Transformer to combine lexical and semantic information and propose a memory mechanism to capture and exploit common patterns in fact-checking articles. • Experiments on two real-world datasets show that MTM outperforms existing methods. Further human evaluation and case studies prove that our model finds key sentences as good explanations. • We built the first Chinese dataset for factchecked claim detection with fact-checking articles from diverse sources."
    }, {
      "heading" : "2 Related Work",
      "text" : "To defend against false information, researchers are mainly devoted to two threads: (1) Automatic fact-checking methods mainly retrieve relevant factual information from designated sources and judge the claim’s veracity. Thorne et al. (2018) use Wikipedia as a fact tank and build a shared task for automatic fact-checking, while Popat et al. (2018b) and Wang et al. (2018) retrieve webpages as evidence and use their stances on claims for veracity prediction. (2) Fake news detection methods often use non-factual signals, such as styles (Przybyla, 2020; Qi et al., 2019), emotions (Ajao\n3https://weibo.com\net al., 2019; Zhang et al., 2021), source credibility (Nguyen et al., 2020), user response (Shu et al., 2019) and diffusion network (Liu and Wu, 2018; Rosenfeld et al., 2020). However, these methods mainly aim at newly emerged claims and do not address those claims that have been fact-checked but continually spread. Our work is in a new thread, detecting previously fact-checked claims. Vo and Lee (2020) models interaction between claims and FC-articles by combining GloVe (Pennington et al., 2014) and ELMo embeddings (Peters et al., 2018). Shaar et al. (2020) train a RankSVM with scores from BM25 and Sentence-BERT for relevance prediction. These methods ignore the characteristics of FC-articles, which limits the ranking performance and explainability."
    }, {
      "heading" : "3 Proposed Method",
      "text" : "Given a claim q and a candidate set of k1 FCarticles D obtained by a standard full-text retrieval model (BM25), we aim to rerank FC-articles truly relevant w.r.t. q at the top by modeling fine-grained relevance between q and each article d ∈ D. This is accomplished by Memory-enhanced Transformers for Matching (MTM), which conceptually has two steps, (1) Key Sentence Identification and (2) Article Relevance Prediction, see Figure 2. For an article of l sentences, let S = {s1, ..., sl} be its\nsentence set. In Step (1), for each sentence, we derive claim-sentence relevance score from ROUGEguided Transformer (ROT) and pattern-sentence relevance score from Pattern Memory Bank (PMB). The scores indicate how similar the sentence is to the claim and pattern vectors, i.e., how possible to be a key sentence. Top k2 sentences are selected for more complicated interactions and aggregation with the claim and pattern vectors in Step (2). The aggregated vector is used for the final prediction. We detail the components and then summarize the training procedure below."
    }, {
      "heading" : "3.1 Key Sentence Identification",
      "text" : "3.1.1 ROUGE-guided Transformer (ROT) ROT (left top of Figure. 2) is used to evaluate the relevance between q and each sentence s in {Si}k1i=1, both lexically and semantically. Inspired by (Gao et al., 2020), we choose to “inject” the ability to consider lexical relevance into the semantic model. As the BERT is proved to capture and evaluate semantic relevance (Zhang et al., 2020), we use a one-layer Transformer initialized with the first block of pretrained BERT to obtain the initial semantic representation of q and s:\nzq,s = Transformer ([CLS] q [SEP] s) (1)\nwhere [CLS] and [SEP] are preserved tokens and zq,s is the output representation.\nTo force ROT to consider the lexical relevance, we finetune the pretrained Transformer with the guidance of ROUGE (Lin, 2004), a widely-used metric to evaluate the lexical similarity of two segments in summarization and translation tasks. The intuition is that lexical relevance can be characterized by token overlapping, which ROUGE exactly measures. We minimize the mean square error between the prediction and the precision and recall of ROUGE-2 between q and s (R2 ∈ R2) to optimize the ROT:\nR̂(q, s) = MLP ( zq,s([CLS]) ) (2)\nLR = ‖R̂(q, s)− R2(q, s)‖22 + λR‖∆θ‖22 (3)\nwhere the first term is the regression loss and the second is to constraint the change of parameters as the ability to capture semantic relevance should be maintained. λR is a control factor and ∆θ represents the change of parameters."
    }, {
      "heading" : "3.1.2 Pattern Memory Bank (PMB)",
      "text" : "The Pattern Memory Bank (PMB) is to generate, store, and update the vectors which represent the common patterns in FC-articles. The vectors in PMB will be used to evaluate pattern-sentence relevance (see Section 3.1.3). Here we detail how to formulate, initialize, and update these patterns below. Formulation. Intuitively, one can summarize the templates, like “...has been debunked by...”, and explicitly do exact matching, but the templates are costly to obtain and hard to integrate into neural models. Instead, we implicitly represent the common patterns using vectors derived from embeddings of our model, ROT. Inspired by (Wu et al., 2018), we use a memory bankM to store K common patterns (as vectors), i.e.,M = {mi}Ki=1. Initialization. We first represent each q in the training set and s in the corresponding articles by averaging its token embeddings (from the embedding layer of ROT). Considering that a pattern vector should be event-irrelevant, we heuristically remove the event-related part in s as possible by calculating the residual embeddings rs,q, i.e., subtracting q from s. We rule out the residual embeddings that do not satisfy tlow < ‖rs,q‖2 < thigh, because they are unlikely to contain good pattern information: ‖rs,q‖2 ≤ tlow indicates q and s are highly similar and thus leave little pattern information, while\n‖rs,q‖2 ≥ thigh indicates s may not align with q in terms of the event, so the corresponding rs,q is of little sense. Finally, we aggregate the valid residual embeddings into K clusters using K-means and obtain the initial memory bankM:\nM = K-means ( {rvalids,q } ) ={m1, ...,mK} (4)\nwhere {rvalids,q } is the set of valid residual embeddings. Update. As the initial K vectors may not accurately represent common patterns, we update the memory bank according to the feedbacks of results during training: If the model predicts rightly, the key sentence, say s, should be used to update its nearest pattern vector m. To maintain stability, we use an epoch-wise update instead of an iterationwise update.\nTake updating m as an example. After an epoch, we extract all n key sentences whose nearest pattern vector is m and their n corresponding claims, which is denoted as a tuple set (S,Q)m. Then (S,Q)m is separated into two subsets, Rm and Wm, which contain nr and nw sentence-claim tuples from the rightly and wrongly predicted samples, respectively. The core of our update mechanism (Figure 3) is to draw m closer to the residual embeddings inRm and push it away from those in Wm. We denote the ith residual embedding from the two subsets as rRmi and rWmi , respectively.\nTo determine the update direction, we calculate a weighted sum of residual embeddings according to the predicted matching scores. For (s, q), suppose MTM output ŷs,q ∈ [0, 1] as the predicted matching score of q and d (whose key sentence is s), the weight of rs,q is |ŷs,q − 0.5| (denoted as ws,q). Weighted residual embeddings are respectively summed and normalized as the components\nof the direction vector (Eq. 5):\numr = ( nr∑ i=1 wRmi rRmi ) ,umw = ( nw∑ i=1 wWmi rWmi ) (5) where umr and umw are the aggregated residual embeddings. The direction is determined by Eq. 6:\num = wr (u mr −m)︸ ︷︷ ︸\ndraw closer +ww (m− umw)︸ ︷︷ ︸ push away (6)\nwhere wr and ww are the normalized sum of corresponding weights used in Eq. 5 (wr + ww = 1). The pattern vector m is updated with:\nmnew = mold + λm‖mold‖2 um\n‖um‖2 (7)\nwhere mold and mnew are the memory vector m before and after updating; the constant λm and ‖mold‖2 jointly control the step size."
    }, {
      "heading" : "3.1.3 Key Sentence Selection",
      "text" : "Whether a sentence is selected as a key sentence is determined by combining claim- and patternsentence relevance scores. The former is calculated with the distance of q and s trained with ROT (Eq. 8) and the latter uses the distance between the nearest pattern vector in PMB and the residual embedding (Eq. 9). The scores are scaled to [0, 1]. For each sentence s in d, the relevance score with q is calculated by Eq. 10:\nscrQ(q, s) = Scale(‖rs,q‖2) (8)\nscrP (q, s) = Scale(‖mu − rs,q‖2) (9)\nscr(q, s) = λQscrQ(q, s) + λP scrP (q, s) (10)\nwhere Scale(x)=1− x−minmax−min and max and min are the maximum and minimum distance of s in d, respectively. u = arg mini ‖mi − rs,q‖2, and λQ and λP are hyperparameters whose sum is 1.\nFinally, sentences with top-k2 scores, denoted as K = {skeyi (q, d)} k2 i=1, are selected as the key sentences in d for the claim q."
    }, {
      "heading" : "3.2 Article Relevance Prediction (ARP)",
      "text" : "Sentence representation. We model more complicated interactions between the claim and the key sentences by feeding each zq,skey (derived from ROT) into a multi-layer Transformer (MultiTransformer):\nz′q,skey = MultiTransformer(zq,skey) (11)\nFollowing (Reimers and Gurevych, 2019), we respectively compute the mean of all output token vectors of q and s in z′\nq,skey to obtain the fixed sized\nsentence vectors q′ ∈ Rdim and skey′ ∈ Rdim, where dim is the dimension of a token in Transformers. Weighted memory-aware aggregation. For final prediction, we use a score-weighted memory-aware aggregation. To make the predictor aware of the pattern information, we append the corresponding nearest pattern vectors to the claim and key sentence vectors:\nvi = [q ′, skey′i (q, d),mj ] (12)\nwhere i=1, ..., k2. j=arg mink ∥∥∥mk−rskeyi ,q∥∥∥2.\nIntuitively, a sentence with higher score should be attended more. Thus, the concatenated vectors (Eq. 12) are weighted by the relevance scores from Eq. 10 (normalized across the top-k2 sentences). The weighted aggregating vector is fed into a MLP which outputs the probability that d fact-checks q:\nscr′(q, skeyi ) = Normalize ( scr(q, skeyi ) ) (13)\nŷq,d = MLP ( k2∑\ni=1\nscr′(q, skeyi )vi\n) (14)\nwhere ŷq,d ∈ [0, 1]. If ŷq,d > 0.5, the model predicts that d fact-checks q, otherwise does not. The loss function is cross entropy:\nLM = CrossEntropy(ŷq,d, yq,d) (15)\nwhere yq,d ∈ {0, 1} is the ground truth label. yq,d = 1 if d fact-checks q and 0 otherwise. The predicted values are used to rank all k1 candidate articles retrieved in the first stage.\n3.3 Training MTM We summarize the training procedure of MTM in Algorithm 1, including the pretraining of ROT, the initialization of PMB, the training of ARP, and the epoch-wise update of PMB.\nAlgorithm 1 MTM Training Procedure Input: Training set T = [(q0, d00), ..., (q0, d0k1),\n..., (qn, dnk1)] where the k1 candidate articles for each claim are retrieved by BM25. 1: Pre-train ROUGE-guided Transformer. 2: Initialize the Pattern Memory Bank (PMB). 3: for each epoch do 4: for (q, d) in T do 5: // Key Sentence Identification 6: Calculate scrQ(q, s) via ROT and scrP (q, s) via PMB. 7: Calculate scr(q, s) using Eq.10. 8: Select key sentences K. 9: // Article Relevance Prediction (ARP)\n10: Calculate v for each s in K and ŷq,d. 11: Update the ARP to minimize LM . 12: end for 13: Update the PMB using Eq. 7. 14: end for"
    }, {
      "heading" : "4 Experiments",
      "text" : "In this section, we mainly answer the following experimental questions: EQ1: Can MTM improve the ranking performance of FC-articles given a claim? EQ2: How effective are the components of MTM, including ROUGE-guided Transformer, Pattern Memory Bank, and weighted memory-aware aggregation in Article Relevance Prediction? EQ3: To what extent can MTM identify key sentences in the articles, especially in the longer ones?"
    }, {
      "heading" : "4.1 Data",
      "text" : "We conducted the experiments on two real-world datasets. Table 1 shows the statistics of the two datasets. The details are as follows: Twitter Dataset\nThe Twitter4 dataset is originated from (Vo and Lee, 2019) and processed by Vo and Lee (2020). The dataset pairs the claims (tweets) with the corresponding FC-articles from Snopes. For tweets with images, it appends the OCR results to the tweets. We remove the manually normalized claims in Snopes’ FC-articles to adapt to more general scenarios. The data split is the same as that in (Vo and Lee, 2020). Weibo Dataset\nWe built the first Chinese dataset for the task of detecting previously fact-checked claims in this ar-\n4https://twitter.com\nticle. The claims are collected from Weibo and the FC-articles are from multiple fact-checking sources including Jiaozhen, Zhuoyaoji5, etc. We recruited annotators to match claims and FC-articles based on basic search results. Appendix A introduce the details."
    }, {
      "heading" : "4.2 Baseline Methods",
      "text" : "BERT-based rankers from general IR tasks BERT (Devlin et al., 2019): A method of pretraining language representations with a family of pretrained models, which has been used in general document reranking to predict the relevance. (Nogueira and Cho, 2019; Akkalyoncu Yilmaz et al., 2019)\nDuoBERT (Nogueira et al., 2019): A popular BERT-based reranker for multi-stage document ranking. Its input is a query and a pair of documents. The pairwise scores are aggregated for final document ranking. Our first baseline, BERT (trained with query-article pairs), provides the inputs for DuoBERT.\nBERT(Transfer): As no sentence-level labels are provided in most document retrieval datasets, Yang et al. (2019) finetune BERT with short text matching data and then apply to score the relevance between query and each sentence in documents. The three highest scores are combined with BM25 score for document-level prediction. Rankers from related works of our task\nSentence-BERT: Shaar et al. (2020) use pretrained Sentence-BERT models to calculate cosine similarity between each sentence and the given claim. Then the top similarity scores are fed into a neural network to predict document relevance.\nRankSVM: A pairwise RankSVM model for reranking using the scores from BM25 and sentence-BERT (mentioned above), which achieves the best results in (Shaar et al., 2020).\n5https://piyao.sina.cn\nCTM (Vo and Lee, 2020): This method leverages GloVe and ELMo to jointly represent the claims and the FC-articles for predicting the relevance scores. Its multi-modal version is not included as MTM focuses on key textual information."
    }, {
      "heading" : "4.3 Experimental Setup",
      "text" : "Evaluation Metrics. As this is a binary retrieval task, we follow Shaar et al. (2020) and report Mean Reciprocal Rank (MRR), Mean Average Precision@k (MAP@k, k = 1, 3, 5) and HIT@k (k = 3, 5). See equations in Appendix B. Implementation Details. In MTM, the ROT and ARP components have one and eleven Transformer layers, respectively. The initial parameters are obtained from pretrained BERT models6. Other parameters are randomly initialized. The dimension of claim and sentence representation in ARP and pattern vectors are 768. Number of Clusters in PMB K is 20. Following (Shaar et al., 2020) and (Vo and Lee, 2020), we use k1 = 50 candidates retrieved by BM25. k2 = 3 (Weibo, hereafter, W) / 5 (Twitter, hereafter, T) key sentences are selected. We use Adam (P. Kingma and Ba, 2015) for optimization with = 10−6, β1 = 0.9, β2 = 0.999. The learning rates are 5× 10−6 (W) and 1× 10−4 (T). The batch size is 512 for pretraining ROT, 64 for the main task. According to the quantiles on\n6We use bert-base-chinese for Weibo and bert-base-uncased for Twitter.\ntraining sets, we set tlow = 0.252 (W) / 0.190 (T), thigh = 0.295 (W) / 0.227 (T). The following hyperparameters are selected according to the best validation performance: λR = 0.01 (W) / 0.05 (T), λQ = 0.6, λP = 0.4, and λm = 0.3. The maximum epoch is 5. All experiments were conducted on NVIDIA V100 GPUs with PyTorch (Paszke et al., 2019). The implementation details of baselines are in Appendix C."
    }, {
      "heading" : "4.4 Performance Comparison",
      "text" : "To answer EQ1, we compared the performance of baselines and our method on the two datasets, as shown in Table 2. We see that: (1) MTM ourperforms all compared methods on the two datasets (the exception is only the MAP@1 on Twitter), which indicates that it can effectively find related FC-articles and provide evidence for determining if a claim is previously fact-checked. (2) For all methods, the performance on Weibo is worse than that on Twitter because the Weibo dataset contains more claim-sentence pairs (from multiple sources) than Twitter and is more challenging. Despite this, MTM’s improvement is significant. (3) BERT(Transfer), Sentence-BERT and RankSVM use transferred sentence-level knowledge from other pretext tasks but did not outperform the document-level BERT. This is because FCarticles have their own characteristics, which may not be covered by transferred knowledge. In con-\ntrast, our observed characteristics help MTM achieve good performance. Moreover, MTM is also efficiency compared to BERT(Transfer), which also uses 12-layer BERT and selects sentences, because our model uses only one layer for all sentences (other 11 layers are for key sentences), while all sentences are fed into the 12 layers in BERT(Transfer)."
    }, {
      "heading" : "4.5 Ablation Study",
      "text" : "To answer EQ2, we evaluated three ablation groups of MTM’s variants (AG1∼AG3) to investigate the effectiveness of the model design.7 Table 3 shows the performance of variants and MTM.\nAG1: With vs. Without ROUGE. The variant removes the guidance of ROUGE (MTM w/o ROUGE guidance) to check the effectiveness of ROUGEguided finetuning. The variant performs worse on Weibo, but MAP@1 slightly increases on Twitter. This is probably because there are more lexical overlapping between claims and FC-articles in the Weibo dataset, while most of the FC-articles in the Twitter dataset choose to summarize the claims to fact-check.\nAG2: Cluster-based Initialization vs. Random Initialization vs. Without update vs. Without PMB. The first variant (MTM w/ rand mem init) uses random initialization and the second (MTM w/o mem update) uses pattern vectors without updating. The last one (MTM w/o PMB) removes the PMB. We see that the variants all perform worse than MTM on MRR, of which w/ rand mem init performs the worst. This indicates that cluster-based initialization provides a good start and facilitates the following updates while the random one may harm further learning.\nAG3: Score-weighted Pooling vs. Average pooling, and With vs. Without pattern vector. The first variant, MTM w/ avg. pool, replace the score-weighted pooling with average pooling. The comparison in terms of MRR and MAP shows the effectiveness of using relevance scores as weights. The second, MTM w/o pattern aggr., does not append the pattern vector to claim and sentence vectors before aggregation. It yields worse results, indicating the patterns should be taken into consideration for final prediction.\n7We do not run MTM without sentence selection due to its high computational overhead which makes it unfeasible for training and inference."
    }, {
      "heading" : "4.6 Visualization of Memorized Patterns",
      "text" : "To probe what the PMB summarizes and memorizes, we selected and analyzed the key sentences corresponding to the residual embeddings around pattern vectors. Figure 4 shows example sentences where highly frequent words are in boldface. These examples indicate that the pattern vectors do cluster key sentences with common patterns like “...spread in WeChat Moments”."
    }, {
      "heading" : "4.7 Human Evaluation and Case Study",
      "text" : "The quality of selected sentences cannot be automatically evaluated due to the lack of sentencelevel labels. To answer EQ3, we conducted a human evaluation. We randomly sampled 370 claimarticle pairs whose articles were with over 20 sentences from the Weibo dataset. Then we showed each claim and top three sentences selected from the corresponding FC-article by MTM. Three anno-\ntators were asked to check if an auto-selected sentence helped match the given query and the source article (i.e., key sentences). Figure 5 shows (a) MTM hit at least one key sentence in 83.0% of the articles; (b) 73.0% of the sentences at Rank 1 are key sentences, followed by 65.1% at Rank 2 and 56.8% at Rank 3. This proves that MTM can find the key sentences in long FC-articles and provide helpful explanations. We also show the positional distribution in Figure 5(c), where key sentences are scattered throughout the articles. Using MTM to find key sentences can save fact-checkers’ time to scan these long articles for determining whether the given claim was fact-checked.\nAdditionally, we exhibit two cases in the evaluation set in Figure 6. These cases prove that MTM found the key sentences that correspond to the characteristics described in Section 1. Please refer to Appendix D for further case analysis."
    }, {
      "heading" : "5 Conclusions",
      "text" : "We propose MTM to select from fact-checked articles key sentences that introduce or debunk claims. These auto-selected sentences are exploited in an end-to-end network for estimating the relevance of the fact-checked articles w.r.t. a given claim. Experiments on the public Twitter dataset and the private Weibo dataset show that MTM outperforms the state of the art. Moreover, human evaluation and case studies demonstrate that the selected sentences provide helpful explanations of the results."
    }, {
      "heading" : "Acknowledgments",
      "text" : "The authors thank Guang Yang, Tianyun Yang, Peng Qi and anonymous reviewers for their insightful comments. Also, we thank Rundong Li, Qiong Nan, and other annotators for their efforts. This work was supported by the National Key Research and Development Program of China (2017YFC0820604), the National Natural Science Foundation of China (U1703261), and the Fundamental Research Funds for the Central Universities and the Research Funds of Renmin University of China (No. 18XNLG19). The corresponding authors are Juan Cao and Xirong Li.\nBroader Impact Statement\nOur work involves two scenarios that need the ability to detect previously fact-checked claims: (1) For social media platforms, our method can check whether a newly published post contains false claims that have been debunked. The platform may help the users to be aware of the text’s veracity by providing the key sentences selected from fact-checking articles and their links. (2) For manual or automatic fact-checking systems, it can be a filter to avoid redundant fact-checking work. When functioning well, it can assist platforms, users, and fact-checkers to maintain more credible cyberspace. But in the failure cases, some well-disguised claims may escape. This method functions with reliance on the used fact-checking article databases. Thus, authority and credibility need to be carefully considered in practice. We did our best to make the new Weibo dataset for academic purpose reliable. Appendix A introduces more details."
    }, {
      "heading" : "A Constructing the New Weibo Dataset",
      "text" : "To construct datasets for fact-checked claim detection on social media, we need to (1) collect the fact-checked claims (social media posts); (2) collect fact-checking articles (FC-articles); and (3) generate claim-article pairs.\nCollection. In Step (1), we used posts whose labels are fake from the datasets for fake news detection (Zhang et al., 2021; Zhou et al., 2015), because their labels were determined by fact-checking. In Step (2), we crawled fact-checking articles from multiple sources to enrich the article base. The sources are partially listed in Table 4 due to the space limit. For the claims and articles which contained much text in the attached images, we recognized the text using OCR service on Baidu AI platform8. Note that we only crawled the claims and articles that were publicly available at the crawling time. To protect privacy, the publishers’ names were removed. However, we preserved names and offensive words in the main text because they were crucial for summarizing the events and performing the matching process.\n8https://ai.baidu.com/tech/ocr\nAnnotation. In Step (3), we performed a modelassisted human annotation. We first duplicated the data collected in Step (1) and (2) and then used BM25 to retrieve the relevant FC-articles as candidates with the claims as queries. Twenty-six annotators (postgraduates) were instructed (by a Chinese guideline with examples written by the first author) to check whether the candidates did fact-check the given claims. We dropped the claims that are annotated as irrelevant to all candidates. For claims that were with highly overlapping candidates but different annotation results, the authors manually checked and corrected the wrongly annotated samples."
    }, {
      "heading" : "B Calculation of Evaluation Metrics",
      "text" : "Assume that query setQ has |Q| queries and the ith query has ni relevant documents. We calculate the evaluation metrics using the following equations:\nMRR = 1\n|Q| |Q|∑ i=1 1 ranki (16)\nwhere ranki refers to the rank position of the first relevant answer for the ith query in the corresponding retrieving result. (Wikipedia, 2021)\nMAP@k = 1\n|Q| |Q|∑ i=1 1 ni ni∑ j=1 Pi(j)reli(j) (17)\nwhere Pi(j) is the proportion of returned documents in the top-j set for the ith query that are relevant. reli(j) is an indicator function equaling 1 if the document at rank j in the returned list for the ith query is relevant and 0 otherwise. (Li et al., 2016)\nHIT@k = 1\n|Q| |Q|∑ i=1 hasi(k) (18)\nwhere hasi(k) is an indicator function equaling 1 if ranki ≤ k and 0 otherwise. (Yang et al., 2012)\nNote that we guarantee that a query has at least one relevant document in its candidate list, so the corner case of empty ground truth set is ignored.\nC Implementation of BM25 and Baselines\nBM25: The articles were indexed with gensim (Řehůřek and Sojka, 2010).\nBERT: We finetuned the last Transformer layer of bert-base-chinese for Chinese and bert-base-uncased for English. Following the commonly used strategy (e.g., Xie et al., 2020), we truncated the sequences to the maximum length of 512. The maximum length of claims is the same as MTM and the rest tokens are from articles.\nDuoBERT : We used top 20 articles from the results of BERT as candidates to construct article pairs. For each article, the score is obtained by summing its pairwise scores. The used pretrained models are the same as BERT (mentioned above) and we finetuned the layers except the embedding layer and the first Transformer layer.\nBERT(Transfer): For the Twitter data, we used the models provided in Birch (Akkalyoncu Yilmaz et al., 2019) that was finetuned on TREC Microblog Track data (Lin et al., 2014); for the Weibo data, we used LCQMC dataset (Liu et al., 2018) containing 260,068 text pairs to finetune bert-based-chinese for 20 epochs. Considering the value difference between BM25 and BERT scores, the weight of BM25 score was learned by grid search in [0, 1] but the weights of others were in [0, 5]. The step size was 0.1. We got the best results with BM25 weight = 0.2 (Weibo) / 0.1 (Twitter) and the weights of top-3 sentences = 1.2, 0.4, 0.9 (Weibo) / 4.8, 4, 2.5 (Twitter), respectively.\nSentence-BERT: We used the base versions in Sentence-Transformers (Reimers and Gurevych, 2019) to obtain the embeddings against the claims and sentences. Specifically, we\nused stsb-xlm-r-multilingual (Reimers and Gurevych, 2020) for the Weibo data and stsb-bert-base for Twitter9 . According to Shaar et al. (2020), we calculated the cosine similarity of each claim-sentence pair and fed the top-5 scores into a simple neural network (20-ReLU-10ReLU) for classification. We trained the model for 20 epochs with class weighted cross entropy as the loss function. The class weights were calculated across the dataset (TensorFlow, 2021).\nRankSVM: We combined the scores and their reciprocal ranks obtained from Sentence-BERT models and BM25. Then we fed them into a RankSVM10 (Joachims, 2006) for classification. We used Sentence-BERT models trained with {3, 4, 5, 6} sentences for Twitter and those trained with {6, 7, 8, 9} sentences for Weibo. We kept the default settings in the package.\nCTM: For the Twitter dataset, we followed (Vo and Lee, 2020) to use glove.6B11 (Pennington et al., 2014) and the ELMo Original (5.5B)12 (Peters et al., 2018); for the Weibo data, we used sgns.weibo.bigram-char13 (Li et al., 2018) and simplified-Chinese\n9https://www.sbert.net/docs/ pretrained_models.html\n10http://www.cs.cornell.edu/people/tj/ svm_light/svm_rank.html\n11https://nlp.stanford.edu/projects/ glove/\n12https://allennlp.org/elmo 13https://github.com/Embedding/\nChinese-Word-Vectors\nELMo14 (Che et al., 2018; Fares et al., 2017). We kept the default settings provided by the authors15."
    }, {
      "heading" : "D Further Case Analysis",
      "text" : "We reviewed the fact-checking articles in the set for human evaluation wherein MTM hit less than two key sentences. We here exhibit two situations that make MTM did not perform well: (1) In Figure 7, the claim is about where Cao Cao was born. MTM found three sentences with significant patterns (shown in boldface). However, only\n14https://github.com/HIT-SCIR/ ELMoForManyLangs\n15https://github.com/nguyenvo09/ EMNLP2020\nS1 is related to the claim. S2 and S3 introduce similar but irrelevant claims. This is because that the fact-checking article is actually a collection of rumors about South Korea on the Chinese social media. The claims in this article are all similar to each other, and thus, to differentiate them needs more delicate semantic understanding. (2) Figure 8 shows a case where MTM found no key sentence from the article. We append the key sentences selected manually below. We speculate that the failure is due to the length of the given claim. The claim is longer than general posts on Weibo and contains many details, making the model lose focus on the key elements of the event description. Thus, S1 describing another news about MH370’s activity in Vietnam was selected, instead of the ground truth sentences. To achieve better performance, future work may consider improving the semantic modeling and summarizing key information from both fact-checking articles and claims."
    } ],
    "references" : [ {
      "title" : "Sentiment Aware Fake News Detection on Online Social Networks",
      "author" : [ "Oluwaseun Ajao", "Deepayan Bhowmik", "Shahrzad Zargari." ],
      "venue" : "2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 2507–2511.",
      "citeRegEx" : "Ajao et al\\.,? 2019",
      "shortCiteRegEx" : "Ajao et al\\.",
      "year" : 2019
    }, {
      "title" : "Applying BERT to Document Retrieval with Birch",
      "author" : [ "Zeynep Akkalyoncu Yilmaz", "Shengjin Wang", "Wei Yang", "Haotian Zhang", "Jimmy Lin." ],
      "venue" : "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th In-",
      "citeRegEx" : "Yilmaz et al\\.,? 2019",
      "shortCiteRegEx" : "Yilmaz et al\\.",
      "year" : 2019
    }, {
      "title" : "Towards Better UD Parsing",
      "author" : [ "Wanxiang Che", "Yijia Liu", "Yuxuan Wang", "Bo Zheng", "Ting Liu" ],
      "venue" : null,
      "citeRegEx" : "Che et al\\.,? \\Q2018\\E",
      "shortCiteRegEx" : "Che et al\\.",
      "year" : 2018
    }, {
      "title" : "Coronavirus rumors trigger irrational behaviors among chinese netizens [online",
      "author" : [ "Qingqing Chen" ],
      "venue" : null,
      "citeRegEx" : "Chen.,? \\Q2020\\E",
      "shortCiteRegEx" : "Chen.",
      "year" : 2020
    }, {
      "title" : "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
      "author" : [ "Jacob Devlin", "Ming-Wei Chang", "Kenton Lee", "Kristina Toutanova." ],
      "venue" : "Proceedings of the 2019 Conference of the North American Chapter of the Association",
      "citeRegEx" : "Devlin et al\\.,? 2019",
      "shortCiteRegEx" : "Devlin et al\\.",
      "year" : 2019
    }, {
      "title" : "Fact-checking count tops 300 for the first time [online",
      "author" : [ "Duke Reporters’ Lab" ],
      "venue" : null,
      "citeRegEx" : "Lab.,? \\Q2020\\E",
      "shortCiteRegEx" : "Lab.",
      "year" : 2020
    }, {
      "title" : "Word vectors, reuse, and replicability: Towards a community repository of large-text resources",
      "author" : [ "Murhaf Fares", "Andrey Kutuzov", "Stephan Oepen", "Erik Velldal." ],
      "venue" : "Proceedings of the 21st Nordic Conference on Computational Linguistics, pages 271–",
      "citeRegEx" : "Fares et al\\.,? 2017",
      "shortCiteRegEx" : "Fares et al\\.",
      "year" : 2017
    }, {
      "title" : "Pizzagate: From rumor, to hashtag, to gunfire in DC",
      "author" : [ "Marc Fisher", "John Woodrow Cox", "Peter Hermann." ],
      "venue" : "Washington Post, 6.",
      "citeRegEx" : "Fisher et al\\.,? 2016",
      "shortCiteRegEx" : "Fisher et al\\.",
      "year" : 2016
    }, {
      "title" : "Complementing Lexical Retrieval with Semantic Residual Embedding",
      "author" : [ "Luyu Gao", "Zhuyun Dai", "Zhen Fan", "Jamie Callan." ],
      "venue" : "arXiv, arXiv:2004.13969. Version 2.",
      "citeRegEx" : "Gao et al\\.,? 2020",
      "shortCiteRegEx" : "Gao et al\\.",
      "year" : 2020
    }, {
      "title" : "Training Linear SVMs in Linear Time",
      "author" : [ "Thorsten Joachims." ],
      "venue" : "Proceedings of the 12th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining, pages 217–226, New York, NY, USA. Association for Computing Machin-",
      "citeRegEx" : "Joachims.,? 2006",
      "shortCiteRegEx" : "Joachims.",
      "year" : 2006
    }, {
      "title" : "Analogical Reasoning on Chinese Morphological and Semantic Relations",
      "author" : [ "Shen Li", "Zhe Zhao", "Renfen Hu", "Wensi Li", "Tao Liu", "Xiaoyong Du." ],
      "venue" : "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short",
      "citeRegEx" : "Li et al\\.,? 2018",
      "shortCiteRegEx" : "Li et al\\.",
      "year" : 2018
    }, {
      "title" : "Socializing the Semantic Gap: A Comparative Survey on Image Tag Assignment, Refinement, and Retrieval",
      "author" : [ "Xirong Li", "Tiberio Uricchio", "Lamberto Ballan", "Marco Bertini", "Cees G.M. Snoek", "Alberto Del Bimbo." ],
      "venue" : "ACM Computing Surveys, 49(1).",
      "citeRegEx" : "Li et al\\.,? 2016",
      "shortCiteRegEx" : "Li et al\\.",
      "year" : 2016
    }, {
      "title" : "ROUGE: A Package for Automatic Evaluation of Summaries",
      "author" : [ "Chin-Yew Lin." ],
      "venue" : "Text Summarization Branches Out, pages 74–81, Barcelona, Spain. Association for Computational Linguistics.",
      "citeRegEx" : "Lin.,? 2004",
      "shortCiteRegEx" : "Lin.",
      "year" : 2004
    }, {
      "title" : "Overview of the TREC-2014 Microblog Track",
      "author" : [ "Jimmy Lin", "Miles Efron", "Yulu Wang", "Garrick Sherman." ],
      "venue" : "Twenty-Third Text REtrieval Conference (TREC 2014) Proceedings.",
      "citeRegEx" : "Lin et al\\.,? 2014",
      "shortCiteRegEx" : "Lin et al\\.",
      "year" : 2014
    }, {
      "title" : "LCQMC:A Large-scale Chinese Question Matching Corpus",
      "author" : [ "Xin Liu", "Qingcai Chen", "Chong Deng", "Huajun Zeng", "Jing Chen", "Dongfang Li", "Buzhou Tang." ],
      "venue" : "Proceedings of the 27th International Conference on Computational Linguistics, pages",
      "citeRegEx" : "Liu et al\\.,? 2018",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2018
    }, {
      "title" : "Early Detection of Fake News on Social Media through Propagation Path Classification with Recurrent and Convolutional Networks",
      "author" : [ "Yang Liu", "Yi-Fang Wu." ],
      "venue" : "Proceedings of the AAAI Conference on Artificial Intelligence, volume 32, pages",
      "citeRegEx" : "Liu and Wu.,? 2018",
      "shortCiteRegEx" : "Liu and Wu.",
      "year" : 2018
    }, {
      "title" : "FANG: Leveraging Social Context for Fake News Detection Using Graph Representation",
      "author" : [ "Van-Hoang Nguyen", "Kazunari Sugiyama", "Preslav Nakov", "Min-Yen Kan." ],
      "venue" : "Proceedings of the 29th ACM International Conference on Information",
      "citeRegEx" : "Nguyen et al\\.,? 2020",
      "shortCiteRegEx" : "Nguyen et al\\.",
      "year" : 2020
    }, {
      "title" : "Passage Re-ranking with BERT",
      "author" : [ "Rodrigo Nogueira", "Kyunghyun Cho." ],
      "venue" : "arXiv, arXiv:1901.04085. Version 5.",
      "citeRegEx" : "Nogueira and Cho.,? 2019",
      "shortCiteRegEx" : "Nogueira and Cho.",
      "year" : 2019
    }, {
      "title" : "Multi-stage document ranking with BERT",
      "author" : [ "Rodrigo Nogueira", "Wei Yang", "Kyunghyun Cho", "Jimmy Lin." ],
      "venue" : "arXiv, arXiv:1910.14424. Version 1.",
      "citeRegEx" : "Nogueira et al\\.,? 2019",
      "shortCiteRegEx" : "Nogueira et al\\.",
      "year" : 2019
    }, {
      "title" : "Adam: A Method for Stochastic Optimization",
      "author" : [ "Diederik P. Kingma", "Jimmy Ba." ],
      "venue" : "International Conference on Learning Representations.",
      "citeRegEx" : "Kingma and Ba.,? 2015",
      "shortCiteRegEx" : "Kingma and Ba.",
      "year" : 2015
    }, {
      "title" : "Pytorch: An Imperative Style, High-Performance Deep Learning Library",
      "author" : [ "jani", "Sasank Chilamkurthy", "Benoit Steiner", "Lu Fang", "Junjie Bai", "Soumith Chintala" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "jani et al\\.,? \\Q2019\\E",
      "shortCiteRegEx" : "jani et al\\.",
      "year" : 2019
    }, {
      "title" : "GloVe: Global Vectors for Word Representation",
      "author" : [ "Jeffrey Pennington", "Richard Socher", "Christopher Manning." ],
      "venue" : "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1532–1543, Doha,",
      "citeRegEx" : "Pennington et al\\.,? 2014",
      "shortCiteRegEx" : "Pennington et al\\.",
      "year" : 2014
    }, {
      "title" : "Deep Contextualized Word Representations",
      "author" : [ "Matthew Peters", "Mark Neumann", "Mohit Iyyer", "Matt Gardner", "Christopher Clark", "Kenton Lee", "Luke Zettlemoyer." ],
      "venue" : "Proceedings of the 2018 Conference of the North American Chapter of the Associ-",
      "citeRegEx" : "Peters et al\\.,? 2018",
      "shortCiteRegEx" : "Peters et al\\.",
      "year" : 2018
    }, {
      "title" : "CredEye: A Credibility Lens for Analyzing and Explaining Misinformation",
      "author" : [ "Kashyap Popat", "Subhabrata Mukherjee", "Jannik Strötgen", "Gerhard Weikum." ],
      "venue" : "Companion Proceedings of the The Web Conference 2018, pages 155–158,",
      "citeRegEx" : "Popat et al\\.,? 2018a",
      "shortCiteRegEx" : "Popat et al\\.",
      "year" : 2018
    }, {
      "title" : "DeClarE: Debunking Fake News and False Claims using Evidence-Aware Deep Learning",
      "author" : [ "Kashyap Popat", "Subhabrata Mukherjee", "Andrew Yates", "Gerhard Weikum." ],
      "venue" : "Proceedings of the 2018 Conference on Empirical Methods in Natural Language",
      "citeRegEx" : "Popat et al\\.,? 2018b",
      "shortCiteRegEx" : "Popat et al\\.",
      "year" : 2018
    }, {
      "title" : "Capturing the Style of Fake News",
      "author" : [ "Piotr Przybyla." ],
      "venue" : "Proceedings of the AAAI Conference on Artificial Intelligence, volume 34, pages 490–497. AAAI Press.",
      "citeRegEx" : "Przybyla.,? 2020",
      "shortCiteRegEx" : "Przybyla.",
      "year" : 2020
    }, {
      "title" : "Exploiting Multi-domain Visual Information for Fake News Detection",
      "author" : [ "Peng Qi", "Juan Cao", "Tianyun Yang", "Junbo Guo", "Jintao Li." ],
      "venue" : "2019 IEEE International Conference on Data Mining (ICDM), pages 518–527. IEEE.",
      "citeRegEx" : "Qi et al\\.,? 2019",
      "shortCiteRegEx" : "Qi et al\\.",
      "year" : 2019
    }, {
      "title" : "Software Framework for Topic Modelling with Large Corpora",
      "author" : [ "Radim Řehůřek", "Petr Sojka." ],
      "venue" : "Proceedings of the LREC 2010 Workshop on New Challenges for NLP Frameworks, pages 45–50, Valletta, Malta. ELRA.",
      "citeRegEx" : "Řehůřek and Sojka.,? 2010",
      "shortCiteRegEx" : "Řehůřek and Sojka.",
      "year" : 2010
    }, {
      "title" : "SentenceBERT: Sentence Embeddings using Siamese BERTNetworks",
      "author" : [ "Nils Reimers", "Iryna Gurevych." ],
      "venue" : "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on",
      "citeRegEx" : "Reimers and Gurevych.,? 2019",
      "shortCiteRegEx" : "Reimers and Gurevych.",
      "year" : 2019
    }, {
      "title" : "Making Monolingual Sentence Embeddings Multilingual using Knowledge Distillation",
      "author" : [ "Nils Reimers", "Iryna Gurevych." ],
      "venue" : "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 4512–4525,",
      "citeRegEx" : "Reimers and Gurevych.,? 2020",
      "shortCiteRegEx" : "Reimers and Gurevych.",
      "year" : 2020
    }, {
      "title" : "The Probabilistic Relevance Framework: BM25 and Beyond",
      "author" : [ "Stephen Robertson", "Hugo Zaragoza." ],
      "venue" : "Foundations and Trends in Information Retrieval, 3(4):333–389.",
      "citeRegEx" : "Robertson and Zaragoza.,? 2009",
      "shortCiteRegEx" : "Robertson and Zaragoza.",
      "year" : 2009
    }, {
      "title" : "A Kernel of Truth: Determining Rumor Veracity on Twitter by Diffusion Pattern Alone",
      "author" : [ "Nir Rosenfeld", "Aron Szanto", "David C. Parkes." ],
      "venue" : "Proceedings of The Web Conference 2020, page 1018–1028, New York, NY, USA. Association for",
      "citeRegEx" : "Rosenfeld et al\\.,? 2020",
      "shortCiteRegEx" : "Rosenfeld et al\\.",
      "year" : 2020
    }, {
      "title" : "dEFEND: Explainable Fake",
      "author" : [ "Huan Liu" ],
      "venue" : null,
      "citeRegEx" : "Liu.,? \\Q2019\\E",
      "shortCiteRegEx" : "Liu.",
      "year" : 2019
    }, {
      "title" : "Radiation fears",
      "author" : [ "Linguistics. Jingqiong Wang", "Xinzhu Li" ],
      "venue" : null,
      "citeRegEx" : "Wang and Li.,? \\Q2011\\E",
      "shortCiteRegEx" : "Wang and Li.",
      "year" : 2011
    }, {
      "title" : "Relevant Document Discovery for Fact",
      "author" : [ "Korn" ],
      "venue" : null,
      "citeRegEx" : "2018.,? \\Q2018\\E",
      "shortCiteRegEx" : "2018.",
      "year" : 2018
    }, {
      "title" : "Unsupervised Data Aug",
      "author" : [ "ong", "Quoc V Le" ],
      "venue" : null,
      "citeRegEx" : "ong and Le.,? \\Q2020\\E",
      "shortCiteRegEx" : "ong and Le.",
      "year" : 2020
    }, {
      "title" : "Three common types of internet rumors present a new trend of visual spread [online",
      "author" : [ "Xinhua Net" ],
      "venue" : null,
      "citeRegEx" : "Net.,? \\Q2019\\E",
      "shortCiteRegEx" : "Net.",
      "year" : 2019
    }, {
      "title" : "Simple Applications of BERT for Ad Hoc Document Retrieval",
      "author" : [ "Wei Yang", "Haotian Zhang", "Jimmy Lin." ],
      "venue" : "arXiv, arXiv:1903.10972. Version 1.",
      "citeRegEx" : "Yang et al\\.,? 2019",
      "shortCiteRegEx" : "Yang et al\\.",
      "year" : 2019
    }, {
      "title" : "On Top-k Recommendation Using Social Networks",
      "author" : [ "Xiwang Yang", "Harald Steck", "Yang Guo", "Yong Liu." ],
      "venue" : "Proceedings of the Sixth ACM Conference on Recommender Systems, pages 67–74, New York, NY, USA. Association for Computing Machinery.",
      "citeRegEx" : "Yang et al\\.,? 2012",
      "shortCiteRegEx" : "Yang et al\\.",
      "year" : 2012
    }, {
      "title" : "BERTScore: Evaluating text generation with BERT",
      "author" : [ "Tianyi Zhang", "Varsha Kishore", "Felix Wu", "Kilian Q. Weinberger", "Yoav Artzi." ],
      "venue" : "International Conference on Learning Representations.",
      "citeRegEx" : "Zhang et al\\.,? 2020",
      "shortCiteRegEx" : "Zhang et al\\.",
      "year" : 2020
    }, {
      "title" : "Mining Dual Emotion for Fake News Detection",
      "author" : [ "Xueyao Zhang", "Juan Cao", "Xirong Li", "Qiang Sheng", "Lei Zhong", "Kai Shu." ],
      "venue" : "Proceedings of the The Web Conference 2021. International World Wide Web Conferences Steering Committee.",
      "citeRegEx" : "Zhang et al\\.,? 2021",
      "shortCiteRegEx" : "Zhang et al\\.",
      "year" : 2021
    }, {
      "title" : "Real-Time News Certification System on Sina Weibo",
      "author" : [ "Xing Zhou", "Juan Cao", "Zhiwei Jin", "Fei Xie", "Yu Su", "Dafeng Chu", "Xuehui Cao", "Junqiang Zhang." ],
      "venue" : "Proceedings of the 24th International Conference on World Wide Web, WWW ’15",
      "citeRegEx" : "Zhou et al\\.,? 2015",
      "shortCiteRegEx" : "Zhou et al\\.",
      "year" : 2015
    }, {
      "title" : "2020) for the Weibo data and stsb-bert-base for Twitter9",
      "author" : [ "Gurevych" ],
      "venue" : null,
      "citeRegEx" : "Gurevych,? \\Q2020\\E",
      "shortCiteRegEx" : "Gurevych",
      "year" : 2020
    }, {
      "title" : "2014) and the ELMo Original (5.5B)12 (Peters et al., 2018); for the Weibo",
      "author" : [ "ton" ],
      "venue" : null,
      "citeRegEx" : "ton,? \\Q2018\\E",
      "shortCiteRegEx" : "ton",
      "year" : 2018
    } ],
    "referenceMentions" : [ {
      "referenceID" : 7,
      "context" : "Social media posts with false claims have led to real-world threats on many aspects such as politics (Fisher et al., 2016), social order (Wang and Li, 2011), and personal health (Chen, 2020).",
      "startOffset" : 101,
      "endOffset" : 122
    }, {
      "referenceID" : 33,
      "context" : ", 2016), social order (Wang and Li, 2011), and personal health (Chen, 2020).",
      "startOffset" : 22,
      "endOffset" : 41
    }, {
      "referenceID" : 3,
      "context" : ", 2016), social order (Wang and Li, 2011), and personal health (Chen, 2020).",
      "startOffset" : 63,
      "endOffset" : 75
    }, {
      "referenceID" : 41,
      "context" : "Meanwhile, automatic systems have been developed for detecting suspicious claims on social media (Zhou et al., 2015; Popat et al., 2018a).",
      "startOffset" : 97,
      "endOffset" : 137
    }, {
      "referenceID" : 23,
      "context" : "Meanwhile, automatic systems have been developed for detecting suspicious claims on social media (Zhou et al., 2015; Popat et al., 2018a).",
      "startOffset" : 97,
      "endOffset" : 137
    }, {
      "referenceID" : 25,
      "context" : "(2) Fake news detection methods often use non-factual signals, such as styles (Przybyla, 2020; Qi et al., 2019), emotions (Ajao",
      "startOffset" : 78,
      "endOffset" : 111
    }, {
      "referenceID" : 26,
      "context" : "(2) Fake news detection methods often use non-factual signals, such as styles (Przybyla, 2020; Qi et al., 2019), emotions (Ajao",
      "startOffset" : 78,
      "endOffset" : 111
    }, {
      "referenceID" : 16,
      "context" : ", 2021), source credibility (Nguyen et al., 2020), user response (Shu et al.",
      "startOffset" : 28,
      "endOffset" : 49
    }, {
      "referenceID" : 21,
      "context" : "Vo and Lee (2020) models interaction between claims and FC-articles by combining GloVe (Pennington et al., 2014) and ELMo embeddings (Peters et al.",
      "startOffset" : 87,
      "endOffset" : 112
    }, {
      "referenceID" : 8,
      "context" : "Inspired by (Gao et al., 2020), we choose to “inject” the ability to consider lexical relevance into the semantic model.",
      "startOffset" : 12,
      "endOffset" : 30
    }, {
      "referenceID" : 39,
      "context" : "As the BERT is proved to capture and evaluate semantic relevance (Zhang et al., 2020), we use a one-layer Transformer initialized with the first block of pretrained BERT to obtain the initial semantic representation of q and s:",
      "startOffset" : 65,
      "endOffset" : 85
    }, {
      "referenceID" : 12,
      "context" : "5471 To force ROT to consider the lexical relevance, we finetune the pretrained Transformer with the guidance of ROUGE (Lin, 2004), a widely-used metric to evaluate the lexical similarity of two segments in summarization and translation tasks.",
      "startOffset" : 119,
      "endOffset" : 130
    }, {
      "referenceID" : 28,
      "context" : "Following (Reimers and Gurevych, 2019), we respectively compute the mean of all output token vectors of q and s in z′ q,skey to obtain the fixed sized sentence vectors q′ ∈ Rdim and skey′ ∈ Rdim, where dim is the dimension of a token in Transformers.",
      "startOffset" : 10,
      "endOffset" : 38
    }, {
      "referenceID" : 4,
      "context" : "BERT-based rankers from general IR tasks BERT (Devlin et al., 2019): A method of pre-",
      "startOffset" : 46,
      "endOffset" : 67
    }, {
      "referenceID" : 18,
      "context" : "DuoBERT (Nogueira et al., 2019): A popular BERT-based reranker for multi-stage document ranking.",
      "startOffset" : 8,
      "endOffset" : 31
    } ],
    "year" : 2021,
    "abstractText" : "False claims that have been previously factchecked can still spread on social media. To mitigate their continual spread, detecting previously fact-checked claims is indispensable. Given a claim, existing works retrieve fact-checking articles (FC-articles) for detection and focus on reranking candidate articles in the typical two-stage retrieval framework. However, their performance may be limited as they ignore the following characteristics of FC-articles: (1) claims are often quoted to describe the checked events, providing lexical information besides semantics; and (2) sentence templates to introduce or debunk claims are common across articles, providing pattern information. In this paper, we propose a novel reranker, MTM (Memoryenhanced Transformers for Matching), to rank FC-articles using key sentences selected using event (lexical and semantic) and pattern information. For event information, we propose to finetune the Transformer with regression of ROUGE. For pattern information, we generate pattern vectors as a memory bank to match with the parts containing patterns. By fusing event and pattern information, we select key sentences to represent an article and then predict if the article fact-checks the given claim using the claim, key sentences, and patterns. Experiments on two real-world datasets show that MTM outperforms existing methods. Human evaluation proves that MTM can capture key sentences for explanations. The code and the dataset are at https://github.com/ ICTMCG/MTM.",
    "creator" : "LaTeX with hyperref"
  }
}