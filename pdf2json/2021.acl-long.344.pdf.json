{
  "name" : "2021.acl-long.344.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "Dependency-driven Relation Extraction with Attentive Graph Convolutional Networks",
    "authors" : [ "Yuanhe Tian", "Guimin Chen", "Yan Song", "Xiang Wan" ],
    "emails" : [ "yhtian@uw.edu", "}chenguimin@foxmail.com", "songyan@cuhk.edu.cn", "~wanxiang@sribd.cn" ],
    "sections" : [ {
      "heading" : null,
      "text" : "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 4458–4471\nAugust 1–6, 2021. ©2021 Association for Computational Linguistics\n4458"
    }, {
      "heading" : "1 Introduction",
      "text" : "Relation extraction (RE), which aims to detect the relationship between entity mentions from raw text, is one of the most important tasks in information extraction and retrieval, and plays a crucial role in supporting many downstream natural language processing (NLP) applications such as text mining (Distiawan et al., 2019), sentiment analysis (Sun\n*Equal contribution. †Corresponding author. 1The code and models involved in this paper are released\nat https://github.com/cuhksz-nlp/RE-AGCN.\net al., 2019), question answering (Xu et al., 2016a), and summarization (Wang and Cardie, 2012).\nRecently, neural RE methods (Zeng et al., 2014; Zhang and Wang, 2015; Xu et al., 2015; dos Santos et al., 2015; Zhang et al., 2015; Wang et al., 2016; Zhou et al., 2016; Zhang et al., 2017) with powerful encoders (such as CNN, RNN, and Transformers) have significantly improved model performance for RE without requiring any elaborately designed systems or manually constructed features. These methods are superior in capturing contextual information and thus enable RE systems to better understand the text and identify relations between entities in the given text. Adopting neural models to help RE is not only straightforward and effective, but is also expected to incorporate more diverse and informative knowledge into RE systems. Among all different knowledge sources, syntactic information, especially the dependency trees, have been demonstrated to be beneficial in many studies (Miwa and Bansal, 2016; Zhang et al., 2018; Sun et al., 2020; Chen et al., 2021) because they provide long-distance word connections between useful words and thus accordingly guide the system to better extract relations between entity pairs.\nHowever, intensively leveraging dependency information may not always lead to good RE performance, because the noise in the dependency tree can potentially introduce confusions to relation classification (Xu et al., 2015; Yu et al., 2020),\nespecially when those trees are automatically generated. For example, Figure 1 shows an example sentence with its dependency tree, where the dependency connection between “pumpkin mixture” and “bowl” may introduce noise when the object is to predict the relation between “milk” and “pumpkin mixture”. Therefore, previous studies have always required necessary pruning strategies before encoding the dependency information through a particular model such as LSTM (Xu et al., 2015) or graph convolutional networks (GCN) (Zhang et al., 2018). Because fixed pruning strategies are not guaranteed to result in a sub-tree with all important contextual information included and with all noise filtered out, it is necessary to design an appropriate way for distinguishing the noise in the dependency tree and modelling them accordingly.\nIn this paper, we propose a dependency-driven neural approach for RE, where attentive graph neural network (A-GCN) is proposed to distinguish the important contextual information for this task. Furthermore, given that the dependency types (e.g., nominal subject) that associate with dependency connections are also potentially useful for RE since they contain the syntactic instruction among connected words, we further improve A-GCN by introducing type information into it. Specifically, we first obtain the dependency tree of an input sentence from an off-the-shelf toolkit, then build the graph over the dependency tree, and assign different weights to different labeled dependency connections between any two words, with the weights computed based on the connections and their dependency types, lastly predict relations by the AGCN according to the learned weights. In doing so, not only is A-GCN able to distinguish important contextual information from dependency trees and leverage them accordingly, such that reliance on pruning strategies is unnecessary, but A-GCN can also leverage the dependency type information that is omitted by most previous studies (in particular, the studies that also use attention mechanism (Guo et al., 2019)). Experimental results on two English benchmark datasets, i.e., ACE2005EN and SemEval 2010 Task 8, demonstrate the effectiveness of our approach to RE through A-GCN equipped with dependency type information. State-of-the-art performance is observed on both datasets."
    }, {
      "heading" : "2 The Proposed Approach",
      "text" : "RE is conventionally performed as a typical classification task. Our approach follows this paradigm\nby using A-GCN and incorporates dependency information to improve model performance, where the overall architecture of our model is illustrated in Figure 2. Specifically, given an unstructured input sentence X = x1, · · · , xn with n words and let E1 and E2 denote two entities in X , our approach predicts the relation br between E1 and E2 by\nbr = argmax r2R p (r|A-GCN (X , TX )) (1)\nwhere TX is the dependency tree of X obtained from an off-the-shelf toolkit, R is the relation type set; p computes the probability of a particular relation r 2 R given the two entities and br the output of A-GCN, which takes X and TX as the input. Following texts start with a brief introduction of the standard GCN model, then elaborate our proposed A-GCN equipped with dependency type information, and lastly illustrate the process of applying A-GCN to the classification paradigm for RE."
    }, {
      "heading" : "2.1 Standard Graph Convolutional Networks",
      "text" : "Generally, a good text representation is a prerequisite to achieve outstanding model performance (Song et al., 2017; Bojanowski et al., 2017; Song et al., 2018; Song and Shi, 2018; Hajdik et al., 2019). To enhance the text representation and thus obtain a good understanding of the running text, many studies (Song et al., 2009, 2012; Song and Xia, 2013; Xu et al., 2015; Miwa and Bansal, 2016; Zhang et al., 2019; Mandya et al., 2020; Nie et al., 2020) tried to leverage contextual features, such as n-grams and syntactic information, through different model architectures. Among all these architecture choices, graph convolutional networks (GCN) is a widely used architecture to encode the information in a graph, where in each GCN layer, information in each node communicates to its neighbors through the connections between them. The effectiveness of GCN models to encode the contextual information over a graph of an input sentence has been demonstrated by many previous studies (Zhang et al., 2018; Guo et al., 2019; Sun et al., 2020; Chen et al., 2020; Yu et al., 2020; Mandya et al., 2020; Tian et al., 2020c, 2021a). Normally, the graph in the standard GCN model is built from word dependencies and is represented by an adjacency matrix A = (ai,j)n⇥n where ai,j = 1 if i = j or there is a dependency connection2 (arc) between two words xi and xj in the dependency tree TX and ai,j = 0 otherwise. Based on A, for\n2Normally the direction of the connection is ignored.\neach word xi 2 X , the l-th GCN layer gathers the information carried by its context words in TX and computes the output representation h(l)i for xi by:\nh(l)i =\nnX\nj=1\nai,j ⇣ W(l) · h(l 1)j +b\n(l) ⌘!\n(2)\nwhere h(l 1)j denotes the output representation of xj from the (l-1)-th GCN layer3, W(l) and b(l) are trainable matrices and the bias for the l-th GCN layer, respectively, and is the ReLU activation."
    }, {
      "heading" : "2.2 A-GCN with Dependency Type",
      "text" : "It is noted that in standard GCN (e.g., Eq. (2)), the connections among words are treated equally (i.e., ai,j is either 0 or 1). Therefore, GCN-based models for RE are not able to distinguish the importance of different connections and thus pruning on them is of great importance for RE. Therefore, we propose A-GCN for this task, which uses an attention mechanism to compute the weights for different connections so that the model is able to\n3h(0)j is the output of the encoder for xj .\nleverage different dependency connections accordingly. In addition, the standard GCN and most previous studies omit the dependency types associated with the dependency connections, where those types contain highly useful information for RE and are introduced into A-GCN in this work. Specifically, we firstly represent dependency types in TX by a type matrix T = (ti,j)n⇥n, where ti,j is the dependency type (e.g., nsubj) associated with the directed dependency connection4 between xi and xj . Next, we map each type ti,j to its embedding eti,j . Then, at the l-th GCN layer, the weight for the connection between xi and xj is computed by\np(l)i,j = ai,j · exp\n⇣ s(l)i · s (l) j ⌘\nPn j=1 ai,j · exp ⇣ s(l)i · s (l) j ⌘ (3)\nwhere ai,j 2 A, “·” denotes inner production, and s(l)i and s (l) i are two intermediate vectors for xi and\n4It means ti,j and tj,i are represented in different dependency types to model directions of connections between xi and xj . For example, if ti,j is nsubj, then tj,i is #nsubj.\nxj , respectively, which are computed by\ns(l)i = h (l 1) i e t i,j (4)\nand s(l)j = h (l 1) j e t i,j (5)\nwith “ ” denoting the vector concatenation operation. Afterwards, we apply the weight p(l)i,j to the associated dependency connection between xi and xj and obtain the output representation of xi by\nh(l)i =\nnX\nj=1\np(l)i,j ⇣ W(l) · eh(l 1)j + b\n(l) ⌘!\n(6)\nwith , W(l), and b(l) following the same notations in Eq. (2) for standard GCN, and eh(l 1)j (a typeenhanced representation for xj) computed by\neh(l 1)j = h (l 1) j +W (l) T · e t i,j (7)\nwhere W(l)T maps the dependency type embedding eti,j to the same dimension as h (l 1) j .\nCompared with standard GCN (i.e., Eq. (2)), our approach uses numerical weighting (i.e., p(l)i,j 2 [0, 1]) rather than a binary choice for ai,j , to distinguish the importance of different connections so as to leverage them accordingly. In addition, we integrate the dependency type information into both the computed weight (i.e., p(l)i,j) and the output representation of xi (i.e., h (l) i ), which is not considered in most previous studies."
    }, {
      "heading" : "2.3 Relation Extraction with A-GCN",
      "text" : "Before applying A-GCN for RE, we firstly encode the input X into hidden vectors by BERT (Devlin et al., 2019) with h(0)i denoting the hidden vector for xi. Next, we feed h (0) i to our proposed A-GCN model with L layers and obtain the corresponding output h(L)i . Then, we apply the max pooling mechanism to two text spans: the first is on all h(L)i to obtain the global sentence representation hX by\nhX = MaxPooling({h(L)1 , · · · ,h (L) n }) (8)\nand the second is on h(L)i of those words that belongs to an entity mention (i.e., Ek, k = 1, 2) to compute the representation for entity hEk by\nhEk = MaxPooling({h (L) i |xi 2 Ek}) (9)\nAfterwards, we concatenate the representations of the sentence (i.e., hX ) and two entities (i.e., hE1 and hE2) and apply a trainable matrix WR to the\ncomputed vector to map it to the output space by\no = WR · (hX hE1 hE2) (10)\nwhere o is a |R|-dimensional vector with each of its value referring to a relation type in the relation type set R. Finally, we apply a softmax function of o to predict the relation br between E1 and E2 by\nbr = argmax exp (o u)\nP|R| u=1 exp (o u) (11)\nwith ou representing the value at dimension u in o."
    }, {
      "heading" : "3 Experimental Settings",
      "text" : ""
    }, {
      "heading" : "3.1 Datasets",
      "text" : "In the experiments, we use two English benchmark datasets for RE, namely, ACE2005EN (ACE05)5 and SemEval 2010 Task 8 (SemEval)6 (Hendrickx et al., 2010). For ACE05, we use its English section and follow previous studies (Miwa and Bansal, 2016; Christopoulou et al., 2018; Ye et al., 2019) to pre-process it (two small subsets cts and un are removed) and split the documents into training, development, and test sets7. For SemEval, we use its official train/test split8. The numbers of unique relation types in ACE05 and SemEval are 7 and 19, respectively. We report the number of instances (i.e., entity pairs), for train/dev/test sets of ACE05 and SemEval benchmark datasets in Table 1."
    }, {
      "heading" : "3.2 Dependency Graph Construction",
      "text" : "To construct graphs for A-GCN, we use Standard CoreNLP Toolkits (SCT)9 to obtain the dependency tree TX for each input sentence X . Although our approach is able to distinguish the importance of different dependency connections through the attention mechanism, it is still beneficial if we can filter\n5We obtain the official data (LDC2006T06) from https: //catalog.ldc.upenn.edu/LDC2006T06.\n6The data is downloaded from http://docs.google. com/View?docid=dfvxd49s_36c28v9pmw.\n7We follow the train/dev/test splits specified by Miwa and Bansal (2016) at https://github.com/tticoin/ LSTM-ER/tree/master/data/ace2005/split\n8SemEval only includes the training and test sets. 9We download the version 3.9.2 from https://\nstanfordnlp.github.io/CoreNLP/.\nout those dependency connections that bring confusions to RE through particular pruning strategies. Motivated by previous studies (Xu et al., 2015; Zhang et al., 2018; Yu et al., 2020), in this paper, we construct the graph for A-GCN by including two groups of dependency connections, namely, the local connections and the global connections. In detail, local connections include all dependencies that directly connect to the heads of two entities and global connections include all dependencies along the shortest dependency path (SDP) between the head of two entities, where in many cases words that do not directly connected to the two entities are also involved. With an example sentence including two entities (i.e., “company” and benchmarking), Figure 3 illustrates the two groups of dependency connections and the resulted adjacency matrix, which is built with the connections from the two groups10. It is worth noting that, when the SDP is short, there might be more connections in the local group than that in the global one."
    }, {
      "heading" : "3.3 Implementation",
      "text" : "Following Soares et al. (2019), we insert four special tokens (i.e., “<e1>”, “</e1>”, “<e2>”, and “</e2>”) into the input sentence to mark the boundary11 of the two entities to be investigated, which allows the encoder to distinguish the position of entities during encoding and thus improves model performance. For the encoder, we try BERT (Devlin et al., 2019), because it is a powerful pre-trained language model which and whose variants have achieved state-of-the-art performance in many NLP tasks (Wu and He, 2019; Soares et al., 2019; Wu et al., 2019; Diao et al., 2020; Song et al., 2020;\n10We do not distinguish the two groups of connections in A-GCN once they are represented by the adjacency matrix.\n11For example, “<e1>” and “</e1>” are respectively inserted right before and after the entity E1 in the input X .\nAntoun et al., 2020; Tian et al., 2020a,b,d, 2021b; Qin et al., 2021; Song et al., 2021). Specifically, we use the uncased version of BERT-base and BERT-large12 following the default settings (e.g., for BERT-base, we use 12 layers of multi-head attentions with 768-dimensional hidden vectors; for BERT-large, we use 24 layers of multi-head attentions with 1024-dimensional hidden vectors). For A-GCN, we randomly initialize all trainable parameters and the dependency type embeddings. For evaluation, we follow previous studies to use the standard micro-F1 scores13 for ACE05 and use the macro-averaged F1 scores14 for SemEval. In our experiments, we try different combinations of hyper-parameters, and tune them on the dev set, then evaluate on the test set by the model that achieves the highest F1 score on the dev set.15"
    }, {
      "heading" : "4 Results",
      "text" : ""
    }, {
      "heading" : "4.1 Overall Results",
      "text" : "In the experiments, we run our A-GCN models using BERT-base and BERT-large encoder on graphs with and without applying dependency pruning strategies, which correspond to the graph built upon the combined local and global connections (“L + G”), as well as the one constructed by the full dependency graph (“Full”), respectively. We also run baselines with standard GCN and standard graph attentive networks (GAT) (Veličković et al., 2017) with the same graph. For both standard GCN and AGCN, we try different numbers of layers (i.e. 1 to 3\n12We download different BERT models from https:// github.com/huggingface/transformers.\n13We use the evaluation script from sklearn framework. 14We use the official evaluation script downloaded from\nhttp://semeval2.fbk.eu/scorers/task08/ SemEval2010_task8_scorer-v1.2.zip.\n15We report the hyper-parameter settings of different models with their size and running speed in Appendix A and B."
    }, {
      "heading" : "1 BERT-BASE 75.31 87.87",
      "text" : ""
    }, {
      "heading" : "2 + GAT (FULL) 76.16 88.39",
      "text" : ""
    }, {
      "heading" : "3 + GAT (L + G) 75.79 88.53",
      "text" : ""
    }, {
      "heading" : "4 + 1 GCN LAYER (FULL) 74.91 87.58",
      "text" : ""
    }, {
      "heading" : "5 + 1 A-GCN LAYER (FULL) 76.63 88.34",
      "text" : ""
    }, {
      "heading" : "6 + 1 GCN LAYER (L + G) 75.51 88.64",
      "text" : ""
    }, {
      "heading" : "7 + 1 A-GCN LAYER (L + G) 77.10 89.03",
      "text" : ""
    }, {
      "heading" : "8 + 2 GCN LAYERS (FULL) 75.09 88.66",
      "text" : ""
    }, {
      "heading" : "9 + 2 A-GCN LAYERS (FULL) 77.25 88.70",
      "text" : ""
    }, {
      "heading" : "12 + 3 GCN LAYERS (FULL) 75.69 88.54",
      "text" : ""
    }, {
      "heading" : "13 + 3 A-GCN LAYERS (FULL) 76.26 88.63",
      "text" : ""
    }, {
      "heading" : "14 + 3 GCN LAYERS (L + G) 76.85 88.33",
      "text" : ""
    }, {
      "heading" : "1 BERT-LARGE 76.79 89.02",
      "text" : ""
    }, {
      "heading" : "2 + GAT (FULL) 78.25 89.39",
      "text" : ""
    }, {
      "heading" : "3 + GAT (L + G) 78.71 89.44",
      "text" : ""
    }, {
      "heading" : "4 + 1 GCN LAYER (FULL) 77.63 88.98",
      "text" : ""
    }, {
      "heading" : "5 + 1 A-GCN LAYER (FULL) 78.53 89.54",
      "text" : ""
    }, {
      "heading" : "6 + 1 GCN LAYER (L + G) 77.49 89.11",
      "text" : ""
    }, {
      "heading" : "7 + 1 A-GCN LAYER (L + G) 78.48 89.69",
      "text" : ""
    }, {
      "heading" : "8 + 2 GCN LAYERS (FULL) 78.67 89.43",
      "text" : ""
    }, {
      "heading" : "9 + 2 A-GCN LAYERS (FULL) 78.91 89.70",
      "text" : "layers). In addition, we try BERT-base and BERTlarge baselines without using any dependency information. Table 2 shows the F1 scores of our A-GCN models and all the aforementioned baselines on the test set of ACE05 and SemEval.16\nThere are several observations. First, A-GCN functions well when using BERT-base or BERTlarge as encoder, where the consistent improvement is observed over the BERT-only baselines (ID: 1) across two benchmark datasets, even though the BERT baselines have already achieve good performance. Second, for both datasets, A-GCN outperforms GAT (ID: 2, 3) and standard GCN baselines (ID: 4, 6, 8, 10, 12, 14) with the same graph (i.e., either “L + G” or “Full”) and equal number of layers. Particularly, when full dependency graph is used, it is noted that, in some cases (e.g., ID: 8 for BERT-base on ACE05), standard GCN obtains very limited improvements (or even worse results) over the BERT-only baseline (ID: 1), whereas our A-GCN models (e.g., ID: 9 for BERT-base) is able to consistently outperform the BERT-only baseline and achieve higher performance. We attribute this observation to the attention mechanism used to weigh different dependency connections, which allows A-GCN to distinguish the noise in the graph and thus leverage useful dependency information accordingly. Third, among the models with different numbers of A-GCN layers, the ones (e.g., ID: 11 for BERT-base and ID: 11 for BERT-large)\n16For the same group of models, we report the F1 scores on the development sets in Appendix C and the mean and standard deviation of their test set results in Appendix D.\nwith two A-GCN layers achieves the highest scores, where similar tread is observed from the standard GCN baselines. Besides, we find that our A-GCN models (as well as the standard GCN baselines) with the local and global connections (i.e., “L + G”) consistently outperform the ones with full dependency graph (i.e., “Full”). These observations are relatively intuitive since the dependency information may introduce more noise to RE when it is leveraged in an intensive way (e.g., by using more layers or the full dependency tree without pruning)."
    }, {
      "heading" : "4.2 Comparison with Previous Studies",
      "text" : "In addition, we compare our best models (with “L + G” or “Full” graphs) using BERT-large encoder and two A-GCN layers (ID: 9 and 11) with previous studies. The test results (F1 scores) are reported in Table 3, where our model with both local and global connections (i.e., “L + G”) outperforms all previous studies and achieves state-ofthe-art performance on the two benchmark datasets. Specifically, compared with Guo et al. (2019) who proposed an graph-based approach with attentions to leverage dependency connections, our approach leverages both dependency connections and dependency types among all input words and thus provides a better way to comprehensively leverage the dependency information. In addition, although Mandya et al. (2020) proposed an approach to leverage both dependency connections and dependency types through attentions, they added the dependency type directly to the input word embeddings along with POS embeddings, and the attention in\ntheir approach is a separate stand-alone module which is added on the top of the GCN layer. On the contrary, in our approach, the dependency type is added to each A-GCN layer and the attention mechanism is directly applied to each dependency connection in the A-GCN layer. Therefore, compared with Mandya et al. (2020), our A-GCN encodes the dependency connections and dependency types in a more intensive manner and thus can better leverage them to guide the process of predicting the relations between the given entities."
    }, {
      "heading" : "5 Analyses",
      "text" : ""
    }, {
      "heading" : "5.1 The Effect of A-GCN",
      "text" : "Dependency information is supposed to be beneficial for RE because it contains long-distance wordword relations, which could be extremely useful when the given two entities are far away from each other in the input sentence. To explore the effect of A-GCN in capturing such long-distance wordword relations to help with RE, we split the test instances into different groups according to their entities’ distances (i.e., the number of words between the two entities) and run models on these groups to test their performance. Figure 4 shows the performance of our best performing A-GCN model with BERT-large (ID: 11 in Table 2) and its corresponding standard GCN and BERT-large baselines on the three groups of test instances from the test set of SemEval, where the category name indicates the range of the entity distance.17 It is observed that, A-GCN outperforms the two baselines on all groups of test instances and the improvement becomes larger when the entity distance increases.\n17For example, a test sentence whose distance in between two entities is 7 will fall into the group (5, 10].\nThis observation confirms that our approach is able to leverage dependency information and capture long-distance word-word relations to improve RE."
    }, {
      "heading" : "5.2 The Effect of Graph Construction",
      "text" : "In the main experiments, we try A-GCN with the graph built upon the combined local and global connections (“L + G”). To explore the effect of the local connections and the global connections for AGCN, we run our approach using two A-GCN layers with the graph constructed by local connections (“L”) or global connections (“G”) alone. Table 4 presents the experimental results (F1 scores) of different models with BERT-base and BERT-large encoders, where the results from BERT-only baselines, A-GCN (L + G), and A-GCN (Full) are also copied from Table 2 for reference. Compared to A-GCN (L + G), models with the graph constructed by either local connections (i.e., A-GCN (L)) or global connections (i.e., A-GCN (G)) achieve lower performance, which complies with our intuition because both groups of connections contain important contextual features for RE. Interestingly, it is found that A-GCN (L) outperforms A-GCN (G) with both BERT-base and BERT-large encoders. A possible explanation could be the following. There are overlaps between local and global connections (e.g., the connection between “range” and “restrictions” in Figure 3). Therefore, A-GCN (L) can not only leverage the contextual information associated with the entities themselves, but is also partially18 benefited from the overlapping connections on the SDP between the two entities, which leads A-GCN (L) to achieve a higher performance than A-GCN (G).\n18When there is only one word on the shortest dependency path between two entities, all global connections are included in local ones, e.g., “defamation” and “bishop” in Figure 2."
    }, {
      "heading" : "ID MODELS ACE2005 SEMEVAL",
      "text" : ""
    }, {
      "heading" : "5.3 Ablation Study",
      "text" : "Compared with the standard GCN, A-GCN enhances it from two aspects: (1) using an attention mechanism to weigh different dependency connections and (2) introducing dependency types to the process to encode more detailed dependency information. To better investigate the effect of each individual enhancement (i.e., the attention mechanism or the dependency type information), we conduct an ablation study on our best model, i.e., two layers of A-GCN (L + G) with BERT-base and BERTlarge encoder. Table 5 reports the experimental results of different models, where the performance of BERT-only baseline and the standard GCN baseline (i.e., the one uses neither the attention mechanism nor dependency types) are also reported for reference. The results clearly indicate that, the ablation of either enhancement (i.e., the attention mechanism or the dependency type information) could result in worse results (compared with full A-GCN). Between the two enhancements, the ablation of the attention mechanism hurts A-GCN more, which indicates the ability of distinguishing important connections and leveraging them accordingly plays a more important role in RE."
    }, {
      "heading" : "5.4 Case Study",
      "text" : "To explore in detail that how A-GCN leverages dependency connections and types to improve RE, we conduct a case study with our A-GCN models with different dependency graphs (i.e., two layers of A-GCN (Full) and A-GCN (L + G) with BERTlarge encoder) on an example sentence “A central vacuum is a vacuum motor and filtration system\nbuilt inside a canister.”. Figure 5 shows the sentence where both the two models correctly predict the relation between “motor” (E1) and “canister” (E2) (highlighted in the red color) to be “ContentContainer”, whereas the baseline GCN (Full) and GCN (L + G) models fail to do so. We also visualize the attention weights assigned to different dependency connections extracted from the last AGCN layer, with darker and thicker lines referring to higher weights. In this example, for A-GCN (Full), we observe that the connection between “built” and “canister” along SDP and the connection between “inside” and “canister” receive the highest weights, where this is valid because the dependency type, i.e., obl (oblique nominal), associated with the connection (between “built” and “canister”) reveals that “canister” could be the position where the action (i.e., build) takes place, and is further confirmed by another dependency connection and type (i.e., case) between “inside” and “canister”. Therefore, it is proved that our model learn from the contextual information carried by such important connections and results in correct RE prediction. Similarly, A-GCN (L + G) also correctly perform RE on this case by highlighting the same dependency connections as those from the A-GCN (Full) with much higher weights (because many dependency connections are filtered out)."
    }, {
      "heading" : "6 Related Work",
      "text" : "Recently, neural networks with integrating external knowledge or resources play important roles in RE because of their superiority in better capturing contextual information (Shen and Huang,\n2016; Soares et al., 2019). Particularly, as one kind of such knowledge, dependency parses show their effectiveness in supporting RE for its ability in capturing long-distance word relations (Zhang et al., 2018; Guo et al., 2019). However, intensively leveraging dependency information could introduce confusions to RE (Xu et al., 2016b; Yu et al., 2020) so that necessary pruning is required to alleviate this problem. E.g., Xu et al. (2015) proposed to use the connections along the shortest dependency path between the two entities and apply LSTM to model them; Miwa and Bansal (2016) proposed to prune the original dependency tree into the lowest common ancestor subtree. However, these pruning strategies are either too aggressive or modest, so that the resulted graph might lose some important contexts or filled with more noise. Zhang et al. (2018) adopted GCN to model the dependencies and proposed a trade-off pruning strategy in between Xu et al. (2015) and Miwa and Bansal (2016). Besides, there are other graphbased models for RE that utilize layers of multihead attentions (Guo et al., 2019), dynamic pruning (Yu et al., 2020), and additional attention layers (Mandya et al., 2020) to encode dependency trees. Compared with the aforementioned methods, especially the graph-based ones, our approach offers an alternative to enhance RE with A-GCN by using attention mechanism and dependency type, which are effective and efficient improvement to standard GCN without requiring complicated model design."
    }, {
      "heading" : "7 Conclusion",
      "text" : "In this paper, we propose A-GCN to leverage dependency information for relation extraction, where an attention mechanism is applied to dependency connections to applying weighting on both connections and types so as to better distin-\nguish the important dependency information and leverage them accordingly. In doing so, A-GCN is able to dynamically learn from different dependency connections so that less-informative dependencies are smartly pruned. Experimental results and analyses on two English benchmark datasets for relation extraction demonstrate the effectiveness of our approach, especially for entities with long word-sequence distances, where state-of-theart performance is obtained on both datasets."
    }, {
      "heading" : "Acknowledgements",
      "text" : "This work is supported by Chinese Key-Area Research and Development Program of Guangdong Province (2020B0101350001) and NSFC under the project “The Essential Algorithms and Technologies for Standardized Analytics of Clinical Texts” (12026610). This work is also partially supported by Shenzhen Institute of Artificial Intelligence and Robotics for Society under the project “Automatic Knowledge Enhanced Natural Language Understanding and Its Applications” (AC01202101001). We also thank Mr. Peilin Zhou for providing the first version of the model architecture figure."
    }, {
      "heading" : "Appendix A. Hyper-parameter Settings",
      "text" : "Table 6 reports the hyper-parameters tested in training our models. We test all combinations of them for each model and use the one achieving the highest F1 score in our final experiments. The best hyper-parameter setting is highlighted in boldface."
    }, {
      "heading" : "Appendix B. Model Size and Running Speed",
      "text" : "Table 7 reports the number of trainable parameters and the inference speed (sentences per second) of\nthe baseline (i.e., BERT, BERT + GAT and BERT + GCN) and our models (i.e., BERT + A-GCN) on ACE2005 and SemEval datasets. All models are performed on an NVIDIA Tesla V100 GPU."
    }, {
      "heading" : "Appendix C. Experimental Results on the Development Set",
      "text" : "Table 8 reports the F1 scores of different models on the development set of ACE2005.19"
    }, {
      "heading" : "Appendix D. Mean and Deviation of the Results",
      "text" : "In the experiments, we test models with different configurations. For each model, we train it with the best hyper-parameter setting using five different random seeds. We report the mean (µ) and standard deviation ( ) of the F1 scores on the test set of ACE2005 and SemEval in Table 9.\n19SemEval does not have an official dev set."
    } ],
    "references" : [ {
      "title" : "AraBERT: Transformer-based Model for Arabic Language Understanding",
      "author" : [ "Wissam Antoun", "Fady Baly", "Hazem Hajj." ],
      "venue" : "arXiv preprint arXiv:2003.00104.",
      "citeRegEx" : "Antoun et al\\.,? 2020",
      "shortCiteRegEx" : "Antoun et al\\.",
      "year" : 2020
    }, {
      "title" : "Enriching Word Vectors with Subword Information",
      "author" : [ "Piotr Bojanowski", "Edouard Grave", "Armand Joulin", "Tomas Mikolov." ],
      "venue" : "Transactions of the Association for Computational Linguistics, 5:135–146.",
      "citeRegEx" : "Bojanowski et al\\.,? 2017",
      "shortCiteRegEx" : "Bojanowski et al\\.",
      "year" : 2017
    }, {
      "title" : "Joint Aspect Extraction and Sentiment Analysis with Directional Graph Convolutional Networks",
      "author" : [ "Guimin Chen", "Yuanhe Tian", "Yan Song." ],
      "venue" : "Proceedings of the 28th International Conference on Computational Linguistics, pages 272–279.",
      "citeRegEx" : "Chen et al\\.,? 2020",
      "shortCiteRegEx" : "Chen et al\\.",
      "year" : 2020
    }, {
      "title" : "Relation Extraction with Type-aware Map Memories of Word Dependencies",
      "author" : [ "Guimin Chen", "Yuanhe Tian", "Yan Song", "Xiang Wan." ],
      "venue" : "Findings of the Association for Computational Linguistics: ACLIJCNLP 2021.",
      "citeRegEx" : "Chen et al\\.,? 2021",
      "shortCiteRegEx" : "Chen et al\\.",
      "year" : 2021
    }, {
      "title" : "A Walk-based Model on Entity Graphs for Relation Extraction",
      "author" : [ "Fenia Christopoulou", "Makoto Miwa", "Sophia Ananiadou." ],
      "venue" : "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pages",
      "citeRegEx" : "Christopoulou et al\\.,? 2018",
      "shortCiteRegEx" : "Christopoulou et al\\.",
      "year" : 2018
    }, {
      "title" : "2019. BERT: Pre-training",
      "author" : [ "Jacob Devlin", "Ming-Wei Chang", "Kenton Lee", "Kristina Toutanova" ],
      "venue" : null,
      "citeRegEx" : "Devlin et al\\.,? \\Q2019\\E",
      "shortCiteRegEx" : "Devlin et al\\.",
      "year" : 2019
    }, {
      "title" : "ZEN: Pre-training Chinese Text Encoder Enhanced by N-gram Representations",
      "author" : [ "Shizhe Diao", "Jiaxin Bai", "Yan Song", "Tong Zhang", "Yonggang Wang." ],
      "venue" : "Findings of the Association for Computational Linguistics: EMNLP 2020, pages 4729–4740.",
      "citeRegEx" : "Diao et al\\.,? 2020",
      "shortCiteRegEx" : "Diao et al\\.",
      "year" : 2020
    }, {
      "title" : "Neural Relation Extraction for Knowledge Base Enrichment",
      "author" : [ "Bayu Distiawan", "Gerhard Weikum", "Jianzhong Qi", "Rui Zhang." ],
      "venue" : "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 229–240.",
      "citeRegEx" : "Distiawan et al\\.,? 2019",
      "shortCiteRegEx" : "Distiawan et al\\.",
      "year" : 2019
    }, {
      "title" : "Attention Guided Graph Convolutional Networks for Relation Extraction",
      "author" : [ "Zhijiang Guo", "Yan Zhang", "Wei Lu." ],
      "venue" : "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 241–251.",
      "citeRegEx" : "Guo et al\\.,? 2019",
      "shortCiteRegEx" : "Guo et al\\.",
      "year" : 2019
    }, {
      "title" : "Neural Text Generation from Rich Semantic Representations",
      "author" : [ "Valerie Hajdik", "Jan Buys", "Michael Wayne Goodman", "Emily M. Bender." ],
      "venue" : "Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational",
      "citeRegEx" : "Hajdik et al\\.,? 2019",
      "shortCiteRegEx" : "Hajdik et al\\.",
      "year" : 2019
    }, {
      "title" : "SemEval-2010 Task 8: Multi-Way Classification of Semantic Relations",
      "author" : [ "Iris Hendrickx", "Su Nam Kim", "Zornitsa Kozareva", "Preslav Nakov", "Diarmuid Ó Séaghdha", "Sebastian Padó", "Marco Pennacchiotti", "Lorenza Romano", "Stan Szpakowicz" ],
      "venue" : null,
      "citeRegEx" : "Hendrickx et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Hendrickx et al\\.",
      "year" : 2010
    }, {
      "title" : "Graph Convolution over Multiple Dependency Sub-graphs for Relation Extraction",
      "author" : [ "Angrosh Mandya", "Danushka Bollegala", "Frans Coenen." ],
      "venue" : "Proceedings of the 28th International Conference on Computational Linguistics, pages 6424–6435.",
      "citeRegEx" : "Mandya et al\\.,? 2020",
      "shortCiteRegEx" : "Mandya et al\\.",
      "year" : 2020
    }, {
      "title" : "End-to-End Relation Extraction using LSTMs on Sequences and Tree Structures",
      "author" : [ "Makoto Miwa", "Mohit Bansal." ],
      "venue" : "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1105–",
      "citeRegEx" : "Miwa and Bansal.,? 2016",
      "shortCiteRegEx" : "Miwa and Bansal.",
      "year" : 2016
    }, {
      "title" : "Improving Named Entity Recognition with Attentive Ensemble of Syntactic Information",
      "author" : [ "Yuyang Nie", "Yuanhe Tian", "Yan Song", "Xiang Ao", "Xiang Wan." ],
      "venue" : "Findings of the Association for Computational Linguistics: EMNLP 2020, pages 4231–4245.",
      "citeRegEx" : "Nie et al\\.,? 2020",
      "shortCiteRegEx" : "Nie et al\\.",
      "year" : 2020
    }, {
      "title" : "Improving Arabic Diacritization with Regularized Decoding and Adversarial Training",
      "author" : [ "Han Qin", "Guimin Chen", "Yuanhe Tian", "Yan Song." ],
      "venue" : "Proceedings of the Joint Conference of the 59th Annual",
      "citeRegEx" : "Qin et al\\.,? 2021",
      "shortCiteRegEx" : "Qin et al\\.",
      "year" : 2021
    }, {
      "title" : "Classifying Relations by Ranking with Convolutional Neural Networks",
      "author" : [ "Cı́cero dos Santos", "Bing Xiang", "Bowen Zhou" ],
      "venue" : "In Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint",
      "citeRegEx" : "Santos et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Santos et al\\.",
      "year" : 2015
    }, {
      "title" : "Attentionbased Convolutional Neural Network for Semantic Relation Extraction",
      "author" : [ "Yatian Shen", "Xuanjing Huang." ],
      "venue" : "Proceedings of COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers, pages 2526–",
      "citeRegEx" : "Shen and Huang.,? 2016",
      "shortCiteRegEx" : "Shen and Huang.",
      "year" : 2016
    }, {
      "title" : "Matching the Blanks: Distributional Similarity for Relation Learning",
      "author" : [ "Livio Baldini Soares", "Nicholas FitzGerald", "Jeffrey Ling", "Tom Kwiatkowski." ],
      "venue" : "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics,",
      "citeRegEx" : "Soares et al\\.,? 2019",
      "shortCiteRegEx" : "Soares et al\\.",
      "year" : 2019
    }, {
      "title" : "Transliteration of Name Entity via Improved Statistical Translation on Character Sequences",
      "author" : [ "Yan Song", "Chunyu Kit", "Xiao Chen." ],
      "venue" : "Proceedings of the 2009 Named Entities Workshop: Shared Task on Transliteration (NEWS 2009), pages 57–60.",
      "citeRegEx" : "Song et al\\.,? 2009",
      "shortCiteRegEx" : "Song et al\\.",
      "year" : 2009
    }, {
      "title" : "Entropy-based Training Data Selection for Domain Adaptation",
      "author" : [ "Yan Song", "Prescott Klassen", "Fei Xia", "Chunyu Kit." ],
      "venue" : "Proceedings of COLING 2012: Posters, pages 1191–1200.",
      "citeRegEx" : "Song et al\\.,? 2012",
      "shortCiteRegEx" : "Song et al\\.",
      "year" : 2012
    }, {
      "title" : "Learning Word Representations with Regularization from Prior Knowledge",
      "author" : [ "Yan Song", "Chia-Jung Lee", "Fei Xia." ],
      "venue" : "Proceedings of the 21st Conference on Computational Natural Language Learning (CoNLL 2017), pages 143–152.",
      "citeRegEx" : "Song et al\\.,? 2017",
      "shortCiteRegEx" : "Song et al\\.",
      "year" : 2017
    }, {
      "title" : "Complementary Learning of Word Embeddings",
      "author" : [ "Yan Song", "Shuming Shi." ],
      "venue" : "Proceedings of the Twenty-Seventh International Joint Conference on Artificial Intelligence, IJCAI-18, pages 4368– 4374.",
      "citeRegEx" : "Song and Shi.,? 2018",
      "shortCiteRegEx" : "Song and Shi.",
      "year" : 2018
    }, {
      "title" : "Joint Learning Embeddings for Chinese Words and Their Components via Ladder Structured Networks",
      "author" : [ "Yan Song", "Shuming Shi", "Jing Li." ],
      "venue" : "Proceedings of the 27th International Joint Conference on Artificial Intelligence, pages 4375–4381.",
      "citeRegEx" : "Song et al\\.,? 2018",
      "shortCiteRegEx" : "Song et al\\.",
      "year" : 2018
    }, {
      "title" : "Summarizing Medical Conversations via Identifying Important Utterances",
      "author" : [ "Yan Song", "Yuanhe Tian", "Nan Wang", "Fei Xia." ],
      "venue" : "Proceedings of the 28th International Conference on Computational Linguistics, pages 717–729.",
      "citeRegEx" : "Song et al\\.,? 2020",
      "shortCiteRegEx" : "Song et al\\.",
      "year" : 2020
    }, {
      "title" : "A Common Case of Jekyll and Hyde: The Synergistic Effect of Using Divided Source Training Data for Feature Augmentation",
      "author" : [ "Yan Song", "Fei Xia." ],
      "venue" : "Proceedings of the Sixth International Joint Conference on Natural Language Processing, pages",
      "citeRegEx" : "Song and Xia.,? 2013",
      "shortCiteRegEx" : "Song and Xia.",
      "year" : 2013
    }, {
      "title" : "ZEN 2.0: Continue Training and Adaption for N-gram Enhanced Text Encoders",
      "author" : [ "Yan Song", "Tong Zhang", "Yonggang Wang", "Kai-Fu Lee" ],
      "venue" : "arXiv preprint arXiv:2105.01279",
      "citeRegEx" : "Song et al\\.,? \\Q2021\\E",
      "shortCiteRegEx" : "Song et al\\.",
      "year" : 2021
    }, {
      "title" : "Relation Extraction with Convolutional Network over Learnable SyntaxTransport Graph",
      "author" : [ "Kai Sun", "Richong Zhang", "Yongyi Mao", "Samuel Mensah", "Xudong Liu." ],
      "venue" : "AAAI, pages 8928–8935.",
      "citeRegEx" : "Sun et al\\.,? 2020",
      "shortCiteRegEx" : "Sun et al\\.",
      "year" : 2020
    }, {
      "title" : "Aspect-level Sentiment Analysis via Convolution over Dependency Tree",
      "author" : [ "Kai Sun", "Richong Zhang", "Samuel Mensah", "Yongyi Mao", "Xudong Liu." ],
      "venue" : "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing",
      "citeRegEx" : "Sun et al\\.,? 2019",
      "shortCiteRegEx" : "Sun et al\\.",
      "year" : 2019
    }, {
      "title" : "Aspect-based Sentiment Analysis with Type-aware Graph Convolutional Networks and Layer Ensemble",
      "author" : [ "Yuanhe Tian", "Guimin Chen", "Yan Song." ],
      "venue" : "Proceedings of the 2021 Conference of the North American Chapter of the Association for Computa-",
      "citeRegEx" : "Tian et al\\.,? 2021a",
      "shortCiteRegEx" : "Tian et al\\.",
      "year" : 2021
    }, {
      "title" : "Enhancing Aspect-level Sentiment Analysis with Word Dependencies",
      "author" : [ "Yuanhe Tian", "Guimin Chen", "Yan Song." ],
      "venue" : "Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume, pages",
      "citeRegEx" : "Tian et al\\.,? 2021b",
      "shortCiteRegEx" : "Tian et al\\.",
      "year" : 2021
    }, {
      "title" : "Improving Biomedical Named Entity Recognition with Syntactic Information",
      "author" : [ "Yuanhe Tian", "Wang Shen", "Yan Song", "Fei Xia", "Min He", "Kenli Li." ],
      "venue" : "BMC Bioinformatics, 21:1471–2105.",
      "citeRegEx" : "Tian et al\\.,? 2020a",
      "shortCiteRegEx" : "Tian et al\\.",
      "year" : 2020
    }, {
      "title" : "Joint Chinese Word Segmentation and Part-of-speech Tagging via Multi-channel Attention of Character Ngrams",
      "author" : [ "Yuanhe Tian", "Yan Song", "Fei Xia." ],
      "venue" : "Proceedings of the 28th International Conference on Computational Linguistics, pages",
      "citeRegEx" : "Tian et al\\.,? 2020b",
      "shortCiteRegEx" : "Tian et al\\.",
      "year" : 2020
    }, {
      "title" : "Supertagging Combinatory Categorial Grammar with Attentive Graph Convolutional Networks",
      "author" : [ "Yuanhe Tian", "Yan Song", "Fei Xia." ],
      "venue" : "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages",
      "citeRegEx" : "Tian et al\\.,? 2020c",
      "shortCiteRegEx" : "Tian et al\\.",
      "year" : 2020
    }, {
      "title" : "Improving Constituency Parsing with Span Attention",
      "author" : [ "Yuanhe Tian", "Yan Song", "Fei Xia", "Tong Zhang." ],
      "venue" : "Findings of the Association for Computational Linguistics: EMNLP 2020, pages 1691– 1703.",
      "citeRegEx" : "Tian et al\\.,? 2020d",
      "shortCiteRegEx" : "Tian et al\\.",
      "year" : 2020
    }, {
      "title" : "Graph Attention Networks",
      "author" : [ "Petar Veličković", "Guillem Cucurull", "Arantxa Casanova", "Adriana Romero", "Pietro Lio", "Yoshua Bengio." ],
      "venue" : "arXiv preprint arXiv:1710.10903.",
      "citeRegEx" : "Veličković et al\\.,? 2017",
      "shortCiteRegEx" : "Veličković et al\\.",
      "year" : 2017
    }, {
      "title" : "Relation Classification via Multi-Level Attention CNNs",
      "author" : [ "Linlin Wang", "Zhu Cao", "Gerard De Melo", "Zhiyuan Liu." ],
      "venue" : "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1298–",
      "citeRegEx" : "Wang et al\\.,? 2016",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2016
    }, {
      "title" : "Focused Meeting Summarization via Unsupervised Relation Extraction",
      "author" : [ "Lu Wang", "Claire Cardie." ],
      "venue" : "Proceedings of the 13th Annual Meeting of the Special Interest Group on Discourse and Dialogue, pages 304–313.",
      "citeRegEx" : "Wang and Cardie.,? 2012",
      "shortCiteRegEx" : "Wang and Cardie.",
      "year" : 2012
    }, {
      "title" : "Enriching Pretrained Language Model with Entity Information for Relation Classification",
      "author" : [ "Shanchan Wu", "Yifan He." ],
      "venue" : "Proceedings of the 28th ACM International Conference on Information and Knowledge Management, pages 2361–2364.",
      "citeRegEx" : "Wu and He.,? 2019",
      "shortCiteRegEx" : "Wu and He.",
      "year" : 2019
    }, {
      "title" : "WTMED at MEDIQA 2019: A Hybrid Approach to Biomedical Natural Language Inference",
      "author" : [ "Zhaofeng Wu", "Yan Song", "Sicong Huang", "Yuanhe Tian", "Fei Xia." ],
      "venue" : "Proceedings of the 18th BioNLP Workshop and Shared Task, pages 415–426, Florence,",
      "citeRegEx" : "Wu et al\\.,? 2019",
      "shortCiteRegEx" : "Wu et al\\.",
      "year" : 2019
    }, {
      "title" : "Question Answering on Freebase via Relation Extraction and Textual Evidence",
      "author" : [ "Kun Xu", "Siva Reddy", "Yansong Feng", "Songfang Huang", "Dongyan Zhao." ],
      "venue" : "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Vol-",
      "citeRegEx" : "Xu et al\\.,? 2016a",
      "shortCiteRegEx" : "Xu et al\\.",
      "year" : 2016
    }, {
      "title" : "Question answering on Freebase via relation extraction and textual evidence",
      "author" : [ "Kun Xu", "Siva Reddy", "Yansong Feng", "Songfang Huang", "Dongyan Zhao." ],
      "venue" : "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume",
      "citeRegEx" : "Xu et al\\.,? 2016b",
      "shortCiteRegEx" : "Xu et al\\.",
      "year" : 2016
    }, {
      "title" : "Classifying Relations via Long Short Term Memory Networks Along Shortest Dependency Paths",
      "author" : [ "Yan Xu", "Lili Mou", "Ge Li", "Yunchuan Chen", "Hao Peng", "Zhi Jin." ],
      "venue" : "Proceedings of the 2015 conference on empirical methods in natural language",
      "citeRegEx" : "Xu et al\\.,? 2015",
      "shortCiteRegEx" : "Xu et al\\.",
      "year" : 2015
    }, {
      "title" : "Exploiting Entity BIO Tag Embeddings and Multi-task Learning for Relation Extraction with Imbalanced Data",
      "author" : [ "Wei Ye", "Bo Li", "Rui Xie", "Zhonghao Sheng", "Long Chen", "Shikun Zhang." ],
      "venue" : "Proceedings of the 57th Annual Meeting of the Association for",
      "citeRegEx" : "Ye et al\\.,? 2019",
      "shortCiteRegEx" : "Ye et al\\.",
      "year" : 2019
    }, {
      "title" : "Learning to Prune Dependency Trees with Rethinking for Neural Relation Extraction",
      "author" : [ "Bowen Yu", "Mengge Xue", "Zhenyu Zhang", "Tingwen Liu", "Wang Yubin", "Bin Wang." ],
      "venue" : "Proceedings of the 28th International Conference on Computational Linguis-",
      "citeRegEx" : "Yu et al\\.,? 2020",
      "shortCiteRegEx" : "Yu et al\\.",
      "year" : 2020
    }, {
      "title" : "Relation Classification via Convolutional Deep Neural Network",
      "author" : [ "Daojian Zeng", "Kang Liu", "Siwei Lai", "Guangyou Zhou", "Jun Zhao." ],
      "venue" : "Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Pa-",
      "citeRegEx" : "Zeng et al\\.,? 2014",
      "shortCiteRegEx" : "Zeng et al\\.",
      "year" : 2014
    }, {
      "title" : "Relation Classification via Recurrent Neural Network",
      "author" : [ "Dongxu Zhang", "Dong Wang." ],
      "venue" : "arXiv preprint arXiv:1508.01006.",
      "citeRegEx" : "Zhang and Wang.,? 2015",
      "shortCiteRegEx" : "Zhang and Wang.",
      "year" : 2015
    }, {
      "title" : "Incorporating Context and External Knowledge for Pronoun Coreference Resolution",
      "author" : [ "Hongming Zhang", "Yan Song", "Yangqiu Song." ],
      "venue" : "Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics:",
      "citeRegEx" : "Zhang et al\\.,? 2019",
      "shortCiteRegEx" : "Zhang et al\\.",
      "year" : 2019
    }, {
      "title" : "Bidirectional Long Short-Term Memory Networks for Relation Classification",
      "author" : [ "Shu Zhang", "Dequan Zheng", "Xinchen Hu", "Ming Yang." ],
      "venue" : "Proceedings of the 29th Pacific Asia Conference on Language, Information and Computation, pages 73–78.",
      "citeRegEx" : "Zhang et al\\.,? 2015",
      "shortCiteRegEx" : "Zhang et al\\.",
      "year" : 2015
    }, {
      "title" : "Graph Convolution over Pruned Dependency Trees Improves Relation Extraction",
      "author" : [ "Yuhao Zhang", "Peng Qi", "Christopher D. Manning." ],
      "venue" : "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 2205–2215.",
      "citeRegEx" : "Zhang et al\\.,? 2018",
      "shortCiteRegEx" : "Zhang et al\\.",
      "year" : 2018
    }, {
      "title" : "Positionaware Attention and Supervised Data Improve Slot Filling",
      "author" : [ "Yuhao Zhang", "Victor Zhong", "Danqi Chen", "Gabor Angeli", "Christopher D. Manning." ],
      "venue" : "Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing,",
      "citeRegEx" : "Zhang et al\\.,? 2017",
      "shortCiteRegEx" : "Zhang et al\\.",
      "year" : 2017
    }, {
      "title" : "Attention-Based Bidirectional Long Short-Term Memory Networks for Relation Classification",
      "author" : [ "Peng Zhou", "Wei Shi", "Jun Tian", "Zhenyu Qi", "Bingchen Li", "Hongwei Hao", "Bo Xu." ],
      "venue" : "Proceedings of the 54th Annual Meeting of the Association for Compu-",
      "citeRegEx" : "Zhou et al\\.,? 2016",
      "shortCiteRegEx" : "Zhou et al\\.",
      "year" : 2016
    } ],
    "referenceMentions" : [ {
      "referenceID" : 7,
      "context" : "in supporting many downstream natural language processing (NLP) applications such as text mining (Distiawan et al., 2019), sentiment analysis (Sun",
      "startOffset" : 97,
      "endOffset" : 121
    }, {
      "referenceID" : 39,
      "context" : ", 2019), question answering (Xu et al., 2016a), and summarization (Wang and Cardie, 2012).",
      "startOffset" : 28,
      "endOffset" : 46
    }, {
      "referenceID" : 44,
      "context" : "Recently, neural RE methods (Zeng et al., 2014; Zhang and Wang, 2015; Xu et al., 2015; dos Santos et al., 2015; Zhang et al., 2015; Wang et al., 2016; Zhou et al., 2016; Zhang et al., 2017) with powerful encoders (such as CNN, RNN, and Transformers) have significantly improved model performance for RE without requiring any elaborately designed systems or manually constructed features.",
      "startOffset" : 28,
      "endOffset" : 189
    }, {
      "referenceID" : 45,
      "context" : "Recently, neural RE methods (Zeng et al., 2014; Zhang and Wang, 2015; Xu et al., 2015; dos Santos et al., 2015; Zhang et al., 2015; Wang et al., 2016; Zhou et al., 2016; Zhang et al., 2017) with powerful encoders (such as CNN, RNN, and Transformers) have significantly improved model performance for RE without requiring any elaborately designed systems or manually constructed features.",
      "startOffset" : 28,
      "endOffset" : 189
    }, {
      "referenceID" : 41,
      "context" : "Recently, neural RE methods (Zeng et al., 2014; Zhang and Wang, 2015; Xu et al., 2015; dos Santos et al., 2015; Zhang et al., 2015; Wang et al., 2016; Zhou et al., 2016; Zhang et al., 2017) with powerful encoders (such as CNN, RNN, and Transformers) have significantly improved model performance for RE without requiring any elaborately designed systems or manually constructed features.",
      "startOffset" : 28,
      "endOffset" : 189
    }, {
      "referenceID" : 47,
      "context" : "Recently, neural RE methods (Zeng et al., 2014; Zhang and Wang, 2015; Xu et al., 2015; dos Santos et al., 2015; Zhang et al., 2015; Wang et al., 2016; Zhou et al., 2016; Zhang et al., 2017) with powerful encoders (such as CNN, RNN, and Transformers) have significantly improved model performance for RE without requiring any elaborately designed systems or manually constructed features.",
      "startOffset" : 28,
      "endOffset" : 189
    }, {
      "referenceID" : 35,
      "context" : "Recently, neural RE methods (Zeng et al., 2014; Zhang and Wang, 2015; Xu et al., 2015; dos Santos et al., 2015; Zhang et al., 2015; Wang et al., 2016; Zhou et al., 2016; Zhang et al., 2017) with powerful encoders (such as CNN, RNN, and Transformers) have significantly improved model performance for RE without requiring any elaborately designed systems or manually constructed features.",
      "startOffset" : 28,
      "endOffset" : 189
    }, {
      "referenceID" : 50,
      "context" : "Recently, neural RE methods (Zeng et al., 2014; Zhang and Wang, 2015; Xu et al., 2015; dos Santos et al., 2015; Zhang et al., 2015; Wang et al., 2016; Zhou et al., 2016; Zhang et al., 2017) with powerful encoders (such as CNN, RNN, and Transformers) have significantly improved model performance for RE without requiring any elaborately designed systems or manually constructed features.",
      "startOffset" : 28,
      "endOffset" : 189
    }, {
      "referenceID" : 49,
      "context" : "Recently, neural RE methods (Zeng et al., 2014; Zhang and Wang, 2015; Xu et al., 2015; dos Santos et al., 2015; Zhang et al., 2015; Wang et al., 2016; Zhou et al., 2016; Zhang et al., 2017) with powerful encoders (such as CNN, RNN, and Transformers) have significantly improved model performance for RE without requiring any elaborately designed systems or manually constructed features.",
      "startOffset" : 28,
      "endOffset" : 189
    }, {
      "referenceID" : 12,
      "context" : "ies (Miwa and Bansal, 2016; Zhang et al., 2018; Sun et al., 2020; Chen et al., 2021) because they provide long-distance word connections between useful words and thus accordingly guide the system to better extract relations between entity pairs.",
      "startOffset" : 4,
      "endOffset" : 84
    }, {
      "referenceID" : 48,
      "context" : "ies (Miwa and Bansal, 2016; Zhang et al., 2018; Sun et al., 2020; Chen et al., 2021) because they provide long-distance word connections between useful words and thus accordingly guide the system to better extract relations between entity pairs.",
      "startOffset" : 4,
      "endOffset" : 84
    }, {
      "referenceID" : 26,
      "context" : "ies (Miwa and Bansal, 2016; Zhang et al., 2018; Sun et al., 2020; Chen et al., 2021) because they provide long-distance word connections between useful words and thus accordingly guide the system to better extract relations between entity pairs.",
      "startOffset" : 4,
      "endOffset" : 84
    }, {
      "referenceID" : 3,
      "context" : "ies (Miwa and Bansal, 2016; Zhang et al., 2018; Sun et al., 2020; Chen et al., 2021) because they provide long-distance word connections between useful words and thus accordingly guide the system to better extract relations between entity pairs.",
      "startOffset" : 4,
      "endOffset" : 84
    }, {
      "referenceID" : 41,
      "context" : "tree can potentially introduce confusions to relation classification (Xu et al., 2015; Yu et al., 2020),",
      "startOffset" : 69,
      "endOffset" : 103
    }, {
      "referenceID" : 43,
      "context" : "tree can potentially introduce confusions to relation classification (Xu et al., 2015; Yu et al., 2020),",
      "startOffset" : 69,
      "endOffset" : 103
    }, {
      "referenceID" : 41,
      "context" : "Therefore, previous studies have always required necessary pruning strategies before encoding the dependency information through a particular model such as LSTM (Xu et al., 2015) or graph convolutional networks (GCN) (Zhang et al.",
      "startOffset" : 161,
      "endOffset" : 178
    }, {
      "referenceID" : 48,
      "context" : ", 2015) or graph convolutional networks (GCN) (Zhang et al., 2018).",
      "startOffset" : 46,
      "endOffset" : 66
    }, {
      "referenceID" : 20,
      "context" : "Generally, a good text representation is a prerequisite to achieve outstanding model performance (Song et al., 2017; Bojanowski et al., 2017; Song et al., 2018; Song and Shi, 2018; Hajdik et al., 2019).",
      "startOffset" : 97,
      "endOffset" : 201
    }, {
      "referenceID" : 1,
      "context" : "Generally, a good text representation is a prerequisite to achieve outstanding model performance (Song et al., 2017; Bojanowski et al., 2017; Song et al., 2018; Song and Shi, 2018; Hajdik et al., 2019).",
      "startOffset" : 97,
      "endOffset" : 201
    }, {
      "referenceID" : 22,
      "context" : "Generally, a good text representation is a prerequisite to achieve outstanding model performance (Song et al., 2017; Bojanowski et al., 2017; Song et al., 2018; Song and Shi, 2018; Hajdik et al., 2019).",
      "startOffset" : 97,
      "endOffset" : 201
    }, {
      "referenceID" : 21,
      "context" : "Generally, a good text representation is a prerequisite to achieve outstanding model performance (Song et al., 2017; Bojanowski et al., 2017; Song et al., 2018; Song and Shi, 2018; Hajdik et al., 2019).",
      "startOffset" : 97,
      "endOffset" : 201
    }, {
      "referenceID" : 9,
      "context" : "Generally, a good text representation is a prerequisite to achieve outstanding model performance (Song et al., 2017; Bojanowski et al., 2017; Song et al., 2018; Song and Shi, 2018; Hajdik et al., 2019).",
      "startOffset" : 97,
      "endOffset" : 201
    }, {
      "referenceID" : 24,
      "context" : "To enhance the text representation and thus obtain a good understanding of the running text, many studies (Song et al., 2009, 2012; Song and Xia, 2013; Xu et al., 2015; Miwa and Bansal, 2016; Zhang et al., 2019; Mandya et al., 2020; Nie et al., 2020) tried to leverage contextual features, such as n-grams and syntactic information, through different model architectures.",
      "startOffset" : 106,
      "endOffset" : 250
    }, {
      "referenceID" : 41,
      "context" : "To enhance the text representation and thus obtain a good understanding of the running text, many studies (Song et al., 2009, 2012; Song and Xia, 2013; Xu et al., 2015; Miwa and Bansal, 2016; Zhang et al., 2019; Mandya et al., 2020; Nie et al., 2020) tried to leverage contextual features, such as n-grams and syntactic information, through different model architectures.",
      "startOffset" : 106,
      "endOffset" : 250
    }, {
      "referenceID" : 12,
      "context" : "To enhance the text representation and thus obtain a good understanding of the running text, many studies (Song et al., 2009, 2012; Song and Xia, 2013; Xu et al., 2015; Miwa and Bansal, 2016; Zhang et al., 2019; Mandya et al., 2020; Nie et al., 2020) tried to leverage contextual features, such as n-grams and syntactic information, through different model architectures.",
      "startOffset" : 106,
      "endOffset" : 250
    }, {
      "referenceID" : 46,
      "context" : "To enhance the text representation and thus obtain a good understanding of the running text, many studies (Song et al., 2009, 2012; Song and Xia, 2013; Xu et al., 2015; Miwa and Bansal, 2016; Zhang et al., 2019; Mandya et al., 2020; Nie et al., 2020) tried to leverage contextual features, such as n-grams and syntactic information, through different model architectures.",
      "startOffset" : 106,
      "endOffset" : 250
    }, {
      "referenceID" : 11,
      "context" : "To enhance the text representation and thus obtain a good understanding of the running text, many studies (Song et al., 2009, 2012; Song and Xia, 2013; Xu et al., 2015; Miwa and Bansal, 2016; Zhang et al., 2019; Mandya et al., 2020; Nie et al., 2020) tried to leverage contextual features, such as n-grams and syntactic information, through different model architectures.",
      "startOffset" : 106,
      "endOffset" : 250
    }, {
      "referenceID" : 13,
      "context" : "To enhance the text representation and thus obtain a good understanding of the running text, many studies (Song et al., 2009, 2012; Song and Xia, 2013; Xu et al., 2015; Miwa and Bansal, 2016; Zhang et al., 2019; Mandya et al., 2020; Nie et al., 2020) tried to leverage contextual features, such as n-grams and syntactic information, through different model architectures.",
      "startOffset" : 106,
      "endOffset" : 250
    }, {
      "referenceID" : 5,
      "context" : "Before applying A-GCN for RE, we firstly encode the input X into hidden vectors by BERT (Devlin et al., 2019) with h i denoting the hidden vector for xi.",
      "startOffset" : 88,
      "endOffset" : 109
    }, {
      "referenceID" : 10,
      "context" : "In the experiments, we use two English benchmark datasets for RE, namely, ACE2005EN (ACE05)5 and SemEval 2010 Task 8 (SemEval)6 (Hendrickx et al., 2010).",
      "startOffset" : 128,
      "endOffset" : 152
    }, {
      "referenceID" : 12,
      "context" : "For ACE05, we use its English section and follow previous studies (Miwa and Bansal, 2016; Christopoulou et al., 2018; Ye et al., 2019) to pre-process it (two small subsets cts and un are removed) and split the documents into training, development, and test sets7.",
      "startOffset" : 66,
      "endOffset" : 134
    }, {
      "referenceID" : 4,
      "context" : "For ACE05, we use its English section and follow previous studies (Miwa and Bansal, 2016; Christopoulou et al., 2018; Ye et al., 2019) to pre-process it (two small subsets cts and un are removed) and split the documents into training, development, and test sets7.",
      "startOffset" : 66,
      "endOffset" : 134
    }, {
      "referenceID" : 42,
      "context" : "For ACE05, we use its English section and follow previous studies (Miwa and Bansal, 2016; Christopoulou et al., 2018; Ye et al., 2019) to pre-process it (two small subsets cts and un are removed) and split the documents into training, development, and test sets7.",
      "startOffset" : 66,
      "endOffset" : 134
    }, {
      "referenceID" : 41,
      "context" : "Motivated by previous studies (Xu et al., 2015; Zhang et al., 2018; Yu et al., 2020), in this paper, we construct the graph for A-GCN by including two groups of dependency connections, namely, the local connections and the global connections.",
      "startOffset" : 30,
      "endOffset" : 84
    }, {
      "referenceID" : 48,
      "context" : "Motivated by previous studies (Xu et al., 2015; Zhang et al., 2018; Yu et al., 2020), in this paper, we construct the graph for A-GCN by including two groups of dependency connections, namely, the local connections and the global connections.",
      "startOffset" : 30,
      "endOffset" : 84
    }, {
      "referenceID" : 43,
      "context" : "Motivated by previous studies (Xu et al., 2015; Zhang et al., 2018; Yu et al., 2020), in this paper, we construct the graph for A-GCN by including two groups of dependency connections, namely, the local connections and the global connections.",
      "startOffset" : 30,
      "endOffset" : 84
    }, {
      "referenceID" : 34,
      "context" : "baselines with standard GCN and standard graph attentive networks (GAT) (Veličković et al., 2017) with the same graph.",
      "startOffset" : 72,
      "endOffset" : 97
    }, {
      "referenceID" : 48,
      "context" : "Particularly, as one kind of such knowledge, dependency parses show their effectiveness in supporting RE for its ability in capturing long-distance word relations (Zhang et al., 2018; Guo et al., 2019).",
      "startOffset" : 163,
      "endOffset" : 201
    }, {
      "referenceID" : 8,
      "context" : "Particularly, as one kind of such knowledge, dependency parses show their effectiveness in supporting RE for its ability in capturing long-distance word relations (Zhang et al., 2018; Guo et al., 2019).",
      "startOffset" : 163,
      "endOffset" : 201
    }, {
      "referenceID" : 40,
      "context" : "However, intensively leveraging dependency information could introduce confusions to RE (Xu et al., 2016b; Yu et al., 2020) so that necessary pruning is required to alleviate this problem.",
      "startOffset" : 88,
      "endOffset" : 123
    }, {
      "referenceID" : 43,
      "context" : "However, intensively leveraging dependency information could introduce confusions to RE (Xu et al., 2016b; Yu et al., 2020) so that necessary pruning is required to alleviate this problem.",
      "startOffset" : 88,
      "endOffset" : 123
    }, {
      "referenceID" : 8,
      "context" : "Besides, there are other graphbased models for RE that utilize layers of multihead attentions (Guo et al., 2019), dynamic pruning (Yu et al.",
      "startOffset" : 94,
      "endOffset" : 112
    }, {
      "referenceID" : 43,
      "context" : ", 2019), dynamic pruning (Yu et al., 2020), and additional attention layers (Mandya et al.",
      "startOffset" : 25,
      "endOffset" : 42
    }, {
      "referenceID" : 11,
      "context" : ", 2020), and additional attention layers (Mandya et al., 2020) to encode dependency trees.",
      "startOffset" : 41,
      "endOffset" : 62
    } ],
    "year" : 2021,
    "abstractText" : "Syntactic information, especially dependency trees, has been widely used by existing studies to improve relation extraction with better semantic guidance for analyzing the context information associated with the given entities. However, most existing studies suffer from the noise in the dependency trees, especially when they are automatically generated, so that intensively leveraging dependency information may introduce confusions to relation classification and necessary pruning is of great importance in this task. In this paper, we propose a dependency-driven approach for relation extraction with attentive graph convolutional networks (A-GCN). In this approach, an attention mechanism upon graph convolutional networks is applied to different contextual words in the dependency tree obtained from an offthe-shelf dependency parser, to distinguish the importance of different word dependencies. Consider that dependency types among words also contain important contextual guidance, which is potentially helpful for relation extraction, we also include the type information in A-GCN modeling. Experimental results on two English benchmark datasets demonstrate the effectiveness of our A-GCN, which outperforms previous studies and achieves state-ofthe-art performance on both datasets.1",
    "creator" : "Preview"
  }
}