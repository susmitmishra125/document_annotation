{
  "name" : "2021.acl-long.467.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "POS-Constrained Parallel Decoding for Non-autoregressive Generation",
    "authors" : [ "Kexin Yang", "Wenqiang Lei", "Dayiheng Liu", "Weizhen Qi", "Jiancheng Lv" ],
    "emails" : [ "wenqianglei}@gmail.com" ],
    "sections" : [ {
      "heading" : null,
      "text" : "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 5990–6000\nAugust 1–6, 2021. ©2021 Association for Computational Linguistics\n5990"
    }, {
      "heading" : "1 Introduction",
      "text" : "Unlike autoregressive generation (AG) that generates tokens step-by-step, non-autoregressive generation (NAG) parallelly generates all tokens in one time step and thus the inference could be significantly speeded up (Ma et al., 2019; Ran et al., 2020; Susanto et al., 2020). Despite the computational advantage of NAG, it has faced the multimodality problem (Gu et al., 2018) caused by the conditionally independent decoding. A typical example of the problem is illustrated in Figure 1, where either\n∗ Correspondence to Wenqiang Lei.\nof “Thank you.” and “Many Thanks.” is the correct translation (i.e., generation modes). In this example, a mixed mode “Many you.” / “Thank Thanks.” will be generated by NAG. It is because the conditional dependence among target words will be broken in parallel decoding. A typical manifestation is that words are usually missing (e.g., “Many you.”) and repeating (e.g., “Thank Thanks.”) in NAG’s sentences. To solve this problem, the key is helping NAG models to deal with various generation modes.\nTo date, one of the most widely used solutions is sequence-level knowledge distillation (Kim and Rush, 2016) which aims to reduce the generation modes of the raw data (Zhou et al., 2019). Taking machine translation as an example, the knowledge distillation based methods rebuild the target sequence in the training set by employing an AG model to translate the training samples. The assumption is that the target sentences generated by one AG model tend to have less modality. Despite the success of the above studies, there are still two major limitations: (1) Most existing works mainly focus on machine translation where the performance of AG is generally assumed to be better\nthan NAG. Clearly, such a solution will degrade the performance of NAG on the task where the AG model cannot obtain a better result. As demonstrated in our experiments (See § 4.5), there are a number of such tasks beyond the assumption like text summarization and story ending generation. (2) The knowledge distillation based methods may cost a tremendous amount of time to rebuild a largescale training set with AG, which runs counter to the initial goal of NAG to improve the speed.\nTo overcome the aforementioned limitations, we explore to alleviate the multimodality problem in a different manner. In short, we aim to constrain NAG generation modes in the inference stage, rather than directly reducing generation modes in the training stage. More specifically, our basic idea is that the linguistic structure of the target sentence could be helpful to alleviate the multimodality problem. In this paper, we show that the Part-of-Speech (POS) sequence, one of most simple solutions in modeling the linguistic structure (Cutting et al., 1992), could effectively verify our idea and show promising performance in four different tasks. In more details, the proposed POS-constrained Parallel Decoding (POSPD) trains a POS predictor to obtain POS tags of target sequences. In the inference stage, POSPD constrains NAG models to choose the final outputs that satisfy the pre-specified POS sequence. As the POS predictor with a shallow decoder is separately trained, our POSPD could act as a plug-and-play method to assistant NAG models with negligible extra time. Meanwhile, it also shows the speed advantage of our method even considering the time cost in building the POS dataset, since POS tagging is much faster than sentence generating due to the small POS dictionary.\nTo conduct a comprehensive empirical evaluation, we examine the generalizability of POSPD by applying it to two widely-used NAG models (i.e., CMLM and DisCo) over four text generation tasks, including text summarization, story ending generation, question generation, and machine translation. Experiments demonstrate that POSPD significantly and consistently improves the two NAG models and beats the sequence-level knowledge distillation with a considerable performance gap. The main contributions of this work could be summarized as follows:\n• For the first time, we experimentally reveal that the implicit assumption of knowledge distillation does not always hold for the tasks\n(e.g., text summarization, story ending generation, as demonstrated in our experiments). In other words, AG cannot guarantee better performance than NAG, thus resulting in the undesirable performance of NAG if using knowledge distillation to alleviate the multimodality problem. This empirical result could provide novel insight to revisiting the role of the knowledge distillation in NAG.\n• To alleviate the multimodality problem in various tasks, we propose POSPD by employing POS sequences to constrain the NAG generation modes in the inference stage. It is simple but effective, being able to act as a plugand-play assistant for NAG models. Such a linguistic structure based solution shows an effective and efficient alternative to the knowledge distillation paradigm in alleviating the multimodality problem1."
    }, {
      "heading" : "2 Related Works",
      "text" : "In this section, we first analyze related works on alleviating the multimodality problem. Then, we review some representative works which introduce the linguistic structure into some text generation scenarios."
    }, {
      "heading" : "2.1 The Multimodality Problem in NAG",
      "text" : "Recently, various attempts have been made to alleviate the multimodality problem, which can be roughly divided into two types: (1) Reducing the diversity of generation modes in training; (2) Helping models select one generation mode in inference. The first type usually trains the NAG model under the guidance of an AG model (called teacher AG), e.g., sequence-level knowledge distillation (Kim and Rush, 2016), learning from AG model’s hidden state (Li et al., 2019) and the curriculum learning with AG model (Liu et al., 2020d; Guo et al., 2020a). However, these methods implicitly assume that the teacher AG can achieve better performance than NAG models, otherwise it may degrade the performance of the NAG models. As two typical methods of the second type, iterative and dynamic programming methods have achieved promising performance. In short, iterative models generate the target sentence by iteratively refining the latest output (Ghazvininejad et al., 2019; Kasai et al.,\n1The source code and dataset are available at https: //github.com/yangkexin/POSPD\n2020a; Guo et al., 2020b). Alternatively, dynamic programming methods use a heuristic searching strategy to select a better output from multiple decoded candidates (Sun et al., 2019; Saharia et al., 2020; Ghazvininejad et al., 2020). The biggest difference is prespecifying the linguistic structure to constrain the generation of NAG in a plug-and-play way. Extensive experiments verify the effectiveness and efficiency of our idea."
    }, {
      "heading" : "2.2 Leveraging the Linguistic Structure",
      "text" : "Text generation involves multiple tasks, such as style transfer (Liu et al., 2020a) and text filling (Liu et al., 2019). Dating back to the period of statistical machine translation (Liu et al., 2006; Galley et al., 2006), linguistic structure prediction has long been investigated for it. Previous works often model and leverage syntactic structures on the decoder side, such as modeling long-distance word correspondence by syntactic dependency trees (Wu et al., 2017), implicitly incorporate linguistic prior in decoder (Eriguchi et al., 2017) and joint decoding with syntactic structure (Feng et al., 2020). In NAG, linguistic structures can also be helpful. As a global pattern of target sentence, it could serve as the complementary to the parallel decoding by helping models capture words dependency. However, directly incorporating aforementioned methods into NAG are less portable for current NAG models, since they are originally designed for AG. In comparison, POSPD can act as a plug-and-play component that uses a separate POS predictor to constrain NAG models during inference. Therefore, the NAG model can enjoy the benefits of the syntactical structure constraining while retaining\nits original model structure."
    }, {
      "heading" : "3 Methodology",
      "text" : "In this section, we elaborate our POSPD for the NAG model. To ease of presentation, we start from a toy example to illustrate the overview of POSPD in § 3.1, and then give a detailed explanation of the implementation in § 3.2. After that, we present the training details of POSPD in § 3.3."
    }, {
      "heading" : "3.1 Overview",
      "text" : "An overview of our POSPD method is demonstrated in Figure 2, where a toy example of machine translation is used as a showcase. To be exact, the German sentence “Vielen Dank.” is fed simultaneously into both the POS predictor and the NAG model, and then the POS predictor generates a POS sequence JJ NNS PCT which is further converted into a binarized mask matrix through a conversion dictionary. Meanwhile, the NAG model generates the primary probability distributions through a softmax layer. Here, from Figure 1, words “Many” and “you” get the highest probability, resulting in the mix mode “Many you” if following the primary distribution. To avoid such an undesirable result, our POSPD automatically adjusts the probability according to the binarized mask matrix. For example, the probability of “you” is adjusted to 0, since the POS tag of “you” is PRP rather than NNS. As a result, “Many Thanks.” gets the highest probability hence to be generated as the output."
    }, {
      "heading" : "3.2 POSPD in Details",
      "text" : "In this part, we detail the POSPD by introducing the conversion dictionary building, the workflow of POSPD, and the core module—the POS predictor. Building a Conversion Dictionary The key idea of POSPD is filtering out the words that dissatisfy the prespecified POS sequence in the primary results of NAG. To implement our idea, we need a conversion dictionaryDc that contains the mapping from POS tags to words. Given a target vocabulary Vw with the length of |Vw| and a POS tag set Vs, each key ofDc is a POS tag in Vs and the value is a set of words that can be assigned to this POS tag. It is worth noting that a word may have multiple POS tags. Therefore, one word may appear in multiple sets in Dc. The POSPD Workflow The workflow of POSPD is as follows: given a source sentence x, POSPD feeds it into both the NAG model’s encoder and the POS predictor. After that, the POS predictor outputs a POS sequence s = (s1, s2, ..., sL) for the target sentence. Meanwhile, the decoder of the NAG model generates a preliminary distribution matrix D = (d1,d2, ...,dL), where di represents the distribution of all words2 in the i-th position. Note that, the sentence length follows the length of the predicted POS tag L.\nFor the ease of implementation, the POS sequence s is converted into a binarized mask matrixM = (m1,m2, ...,mL). In details, for each POS tag si, the corresponding binarized vector is mi = (m 1 i ,m 2 i , ...,m |Vw| i ) and the j-th position mji is defined as:\nmji = { 1, wj ∈ Dsic ; 0, wj /∈ Dsi,c\n(1)\nwhere wj is the j-th word token in Vw. As a result, the POS sequence s is replaced byM. Finally, we get the new generation results by:\ny = argmax(M ·D). (2)\nThe POS Predictor As the core module of the POSPD, our POS predictor is dedicated to output the POS tag sequence of the target sentence when accepting the source sentence as the input. To train the POS predictor, we need to create a POS dataset where each sample is a pair consisting of a source sentence and a POS sequence of the target\n2The length of di is |Vw|.\nsentence3. As shown in Figure 3, the architecture of our POS predictor is a variant of the standard Transformer (Vaswani et al., 2017). As shown in the gray arrow flow, the main difference between our POS predictor and the vanilla Transformer is the layer number of encoder and decoder. To be specific, unlike the vanilla Transformer which contains six layers for both encoder and decoder, we use a multi-layer encoder and a one-layer decoder to reduce the inference time, because the complexity for decoding the POS sequence is much lower than that for the original sentence.\nPOS Predictor Optimization To optimize the POS predictor, we take a multi-task learning (Evgeniou and Pontil, 2004) paradigm to jointly decode the word sequence and POS sequence on the target side. The underlying hypothesis is that the target word sentence is highly related to the POS sequence. Given a source sentence x, a POS sequence s and a target sentence y = (y1, y2, ..., yL), the learning objective is then defined as the sum of the POS tagging loss (the first term) and the sentence prediction loss (the second term):\nL = Lpos + Lword, (3)\nwhere the POS sentence prediction loss can be written as:\nLpos = L∑\nt=1\nlogP (st|s<t,x), (4)\nand the target sentence prediction loss is:\nLword = L∑\nt=1\nlogP (yt|s<t,x). (5)\n3We use NLTK POS tagger to create the POS sequence, which can be found at https://www.nltk.org/book/ ch05.html.\nIn our method, the POS predictor uses an extra linear layer after the decoder to generate the target sentence, as shown in Figure 3. After training, we only need the POS predicting linear layer for inference, thus enjoying the better performance for the POS sequence prediction."
    }, {
      "heading" : "3.3 Training under the BPE Condition",
      "text" : "Almost all NAG models use the Byte Pair Encoding (BPE) (Sennrich et al., 2016) technique to build the word vocabulary with subword-level tokens. However, these tokens cannot be tagged by the mainstream POS Taggers (Yarowsky and Ngai, 2001), which makes difficulties in building the POS dataset. To address this issue, we propose a simple but effective subword-level POS tagging method for our POS predictor. A simple example is demonstrated in Table 1, the NLTK toolkit tags the word “gutacht” as NN in the original sentence but cannot handle the BPE form “gut ##ach ##t”. Intuitively, we can assign the BPE form to have the POS tag as “gutacht” (i.e. NN NN NN). However, this method increases the number of repeated tokens in generation sentences of NAG models and even worsens the performance. The possible reason is that the aforementioned method cannot explicitly distinguish whether a POS tag is associated with a BPE token or a complete word. In contrast, our method tags the BPE form as NN1 NN2 NN3. As a result, the conversion dictionary is more sparse while improving the mapping between the POS tag and the corresponding words. In addition, the word “question” is tagged as NN, since it doesn’t have any sub-word tokens after the BPE."
    }, {
      "heading" : "4 Experiments",
      "text" : "In this section, we use multiple text generation datasets to comprehensively evaluate the effectiveness and efficiency of the proposed POSPD. For\nan extensive comparison, we compare our POSPD with the sequence-level knowledge distillation, and provide detailed analyzes in alleviating the multimodality problem and the time cost in dataset building."
    }, {
      "heading" : "4.1 Datasets",
      "text" : "We conduct experiments on four widely-used benchmark datasets to evaluate POSPD: XSUM for text summarization, ROCStories corpus for story ending generation, SQuAD 1.1 for question generation, and WMT14 (DE-EN) for machine translation. Meanwhile, we use BERT-based BPE tokenizer4 for all datasets. The details are as follows: XSUM (Narayan et al., 2018) includes the 227k British Broadcasting Corporation (BBC) online articles and the corresponding single-sentence summaries. The average sentence lengths are 358.5 words for input and 21.1 words for output. ROCStories Corpus5 (Mostafazadeh et al., 2016) contains 98k five-sentence stories. For each story, we use the last sentence as the target output while the other four sentences as the source input. We randomly sample 90k/4k stories for training/validation, and the remaining 4160 for testing. The average sentence lengths are 39.64 words for input and 10.72 words for output. SQuAD 1.16 (Rajpurkar et al., 2016) is a machine reading comprehension data set containing 98K passage-question-answer triples (Liu et al., 2020b). After processing, we obtain a question generation dataset. Following GLGE (Liu et al., 2020c), the input sentence is formatted as 〈answer [SEP] passage〉. The average sentence lengths are 149.4 words for input and 11.5 words for output. WMT14 (DE-EN)7 contains 4.5M translation pairs and 3k/3k pairs for validation/testing. The average sentence lengths are 25.07 words for input and 26.53 words for output."
    }, {
      "heading" : "4.2 Evaluation Metrics",
      "text" : "Follow GLGE (Liu et al., 2020c), we use ROUGE-1 (R-1), ROUGE-2 (R-2), and ROUGE-L (R-L) (Lin, 2004) as evaluation metrics for text summarization,\n4https://pypi.org/project/ transformers/.\n5https://cs.rochester.edu/nlp/ rocstories/\n6https://rajpurkar.github.io/ SQuAD-explorer/\n7https://www.statmt.org/wmt14/ translation-task.html\nwhile BLEU-4 (B-4) (Papineni et al., 2002), Meteor (Denkowski and Lavie, 2014), and R-L are used in question generation and story ending generation. Meanwhile, BLEU-4 is also the evaluation metric for machine translation to keep in line with previous works (Gu et al., 2018)."
    }, {
      "heading" : "4.3 Baselines and Comparison",
      "text" : "In this work, we focus on using iteration-based NAG models as backbones, because they are one of the mainstream NAG structures in current works and perform competitively to AG models without any external system (Kasai et al., 2020b). Specifically, we use two representative iteration-based NAG models from recent work, i.e., CMLM (Ghazvininejad et al., 2019) and DisCo (Kasai et al., 2020a). The details are as follows: CMLM The conditional masked language model randomly masks some target tokens and predicts them with the remaining ones. In inference, it masks several tokens with the lower “confidence” and retains other tokens with higher “confidence” during iterations, which is called mask-predict inference. Following Ghazvininejad et al. (2019), we use same settings for all generation tasks8. DisCo The disentangled context transformer aims to use different context information when predicting each token, being regarded as an effective improvement of CMLM. For better comparison, we also use mask-predict inference as same as CMLM. Meanwhile, we use the model settings described in Kasai et al. (2020a) for all generation tasks9. Knowledge Distillation Following Gu et al. (2018) which uses a standard transformer (Vaswani et al., 2017) as the teacher model to regenerate training set in the greedy method for NAG models (hereinafter described as “Transformer-1 (6- 6)”), we report NAG models’ performances on all text generation task when using the distilled training dataset. In the following discussion, the “Transformer-1” and “Transformer-4” denote the beam size of 1 and 4 in the beam search, respectively. Meanwhile, we also report the results of different Transformer model structures, where the “(6-6)” and “(12-1)” denote the version of six encoder layers, six decoder layers and the version of 12 encoder layers, one decoder layers, respectively.\n8https://github.com/facebookresearch/ Mask-Predict\n9https://github.com/facebookresearch/ DisCo"
    }, {
      "heading" : "4.4 Experimental Settings",
      "text" : "We follow the hyperparameters for standard Transformer in (Vaswani et al., 2017) for our POS predictor. One minor difference is the layers of encoder and decoder are set to 12 and 1 to make a fair comparison with AG models, respectively. All of the models are implemented based on Fairseq (Ott et al., 2019), and we follow the other specific parameter settings for both AG and NAG models in (Kasai et al., 2020b). In inference, the length beam, length penalty, and batch size are all set to 1 to calculate the main results (without any postprocessing) and latency. The latency is calculated through using the built-in time statistics function in Fairseq, which is tested on a single NVIDIA Tesla P100 GPU to keep in line with previous works (Gu et al., 2018). Meanwhile, the beam size of our POS predictor is set to 5. For the number of iterations, we report the iterations when the NAG model results are converged. In practice, the iterations of two NAG models are 4, 3, 3 and 10 on XSUM, SQuAD1.1, ROCStories and WMT14 (DE-EN)."
    }, {
      "heading" : "4.5 Main Results",
      "text" : "We evaluate the performance of two NAG models (CMLM and DisCo) on four text generation datasets, and further provide the results when using sequence-level data distillation (i.e., “+Distill”) and the POSPD (i.e., “+POSPD”), respectively. We report the main results in Table 2 and the inference time comparison in Table 3, from which we can make the following conclusions: 1. POSPD consistently improve NAG models on four text generation dataset to a greater extent compared to knowledge distillation. POSPD consistently improve NAG models on four text generation tasks while knowledge distillation may even degrade performances of the NAG models such as XSUM (row 5 vs. row 6) and SQuAD 1.1 (row 8 vs. row 9). More importantly, although the knowledge distillation improves NAG models by 1.04/1.56 (row 5 vs. row 6, row 8 vs. row 9) on BLEU-4 in WMT14 (DE-EN), POSPD still beats the knowledge distillation version by 0.24/0.19 (row 6 vs. row 7, row 9 vs. row 10) on BLEU-4. 2. Knowledge distillation does not always improve the NAG model as the AG models may get worse performance than NAG. In both text summarization (XSUM) and story ending generation (ROCStories) tasks, the two original NAG models CMLM and DisCo outperform the AG model.\nIt is obvious that the adoption of sequence-level knowledge distillation limits the performance of NAG models in these case. More interestingly, in question generation, the AG model outperforms the NAG models with BLUE-4 by 0.4/0.5 (row 3 vs. row 5/row 8), but knowledge distillation degrades NAG models’ performance with BLEU-4 by 0.46/0.13 (row 5 vs. row 6, row 8 vs. row 9). 3. POSPD does not bring significant extra time in constraining NAG models’ generation while decoding. POSPD maintains its advantage in highspeed inference across all data sets. For example, on the dataset SQuAD 1.1, the inference latency of POSPD is much lower than NAG models (1.00× vs. 0.62×/0.66×). Meanwhile, on the WMT14 (DEEN) that has the longest average length of the target sentence, POSPD still maintains its advantage in the inference speed. Therefore, our POSPD could constrain the NAG model with the negligible extra time, since POSPD and the NAG model predict sequences (i.e., POS sequence and target sentence) in parallel."
    }, {
      "heading" : "4.6 Further Discussions",
      "text" : "There is a loose ending towards the discussion of our POSPD solution. In this section, we conduct discussions to shed light on other interesting properties of POSPD. The discussions are guided by the following three research questions: Q1: How does POSPD alleviate the multimodality problem?\nQ2: Is it time-consuming to build the POS dataset on the new task? Q3: Does multi-tasking learning object help the POS tag prediction?"
    }, {
      "heading" : "4.6.1 Discussion on Generated Results (Q1)",
      "text" : "To further analyze the role of POSPD and the sequence-level knowledge distillation in alleviating the multimodality problem, we conduct further statistical analyses on the generated results of four datasets. Considering the multimodality problem usually manifests as repeating or missing tokens in the generation sentences, we use two indicators, i.e., the repetition rate and the total number of tokens, to quantify them separately. Concretely, we refer to a “single-token repeat” metric (Welleck et al., 2020) and define the repetition rate here as the percentage of the repeated times between two adjacent tokens in the total number of tokens in a sentence, and then average it over the dataset.\nThe results are shown in Table 4, from which we can see both knowledge distillation and POSPD can reduce the repetition rate in NAG models on four datasets, and they are more effective on XSUM datasets with longer sentences. While in token numbers, using knowledge distillation significantly reduces the number of tokens generated by NAG models on XSUM. In contrast, using POSPD remarkably make the length of generated sentences by NAG models close to the reference without increasing the repetition rate."
    }, {
      "heading" : "4.6.2 Time Cost in Building Datasets (Q2)",
      "text" : "Considering that both POSPD and knowledge distillation require the processing of the training dataset when it comes to a new task/dataset (i.e., building the POS data set for POSPD / regenerating the training set for knowledge distillation), we further analyze the time consumption of the two processing steps. As shown in Table 5, POSPD has a significant advantage over knowledge distillation in the time consuming of dataset building. Especially on the larger dataset WMT14 (DE-EN), it can save even more time in building datasets, which is beneficial for rapid deployment on new tasks."
    }, {
      "heading" : "4.6.3 Multi-task Learning Strategy (Q3)",
      "text" : "In this part, we analyze the impact of using a multitask learning strategy in POSPD’s training stage. For lack of space, we take the ablation study on two datasets of different sizes, i.e., SQuAD 1.1 and XSUM. The results are shown in Table 6. Interestingly, predicting the POS sequence directly from the original sentence (i.e., “POSPD w/o”) can also improve the performance of the NAG models. More importantly, multi-task learning strategy can improve the performance of POSPD in two datasets with a tiny increase in model parameters (only one linear layer). Meanwhile, it is only used during the POSPD’s training stage and does not affect the inference time of POSPD."
    }, {
      "heading" : "5 Conclusion",
      "text" : "In this paper, we revisit the role of the knowledge distillation in alleviating the multimodality problem of NAG. In brief, we experimentally reflect that the basic assumption of these knowledge distillation methods, the AG model is superior to NAG model, does not always hold for all text generation tasks. To alleviate the multimodality problem, we show a different solution by incorporating linguistic structure into NAG. Extensive experiments demonstrate that our POSPD significantly and consistently improves the NAG models in effectiveness and computational efficacy.\nAs we tentatively give a successful implementation of leveraging one of the simplest linguistic structures to benefit the NAG models in inference, such paradigm deserves a closer and more detailed exploration. Thus in the future, we will investigate to make the NAG models enjoy the benefits of incorporating diverse and abundant linguistic structures in a more superior way. In addition, our\nexperimental results suggest that future work might need to consider wider ranges of generation tasks instead of only machine translation when assessing the performance of NAG models."
    }, {
      "heading" : "Acknowledgments",
      "text" : "This work was supported in part by the National Key RD Program of China under Grant 2020YFB1406702, in part by NFSC under Grant 61625204, 61836006, and the Science and Technology Major Project of Sichuan province under Grant 2020YFG0478."
    } ],
    "references" : [ {
      "title" : "A practical part-of-speech tagger",
      "author" : [ "Douglass Cutting", "Julian Kupiec", "Jan Pedersen", "Penelope Sibun." ],
      "venue" : "Third Conference on Applied Natural Language Processing, pages 133–140.",
      "citeRegEx" : "Cutting et al\\.,? 1992",
      "shortCiteRegEx" : "Cutting et al\\.",
      "year" : 1992
    }, {
      "title" : "Meteor universal: Language specific translation evaluation for any target language",
      "author" : [ "Michael J. Denkowski", "Alon Lavie." ],
      "venue" : "Proceedings of the Ninth Workshop on Statistical Machine Translation, WMT@ACL 2014, June 26-27, 2014, Balti-",
      "citeRegEx" : "Denkowski and Lavie.,? 2014",
      "shortCiteRegEx" : "Denkowski and Lavie.",
      "year" : 2014
    }, {
      "title" : "Learning to parse and translate improves neural machine translation",
      "author" : [ "Akiko Eriguchi", "Yoshimasa Tsuruoka", "Kyunghyun Cho." ],
      "venue" : "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, ACL 2017, Vancouver, Canada,",
      "citeRegEx" : "Eriguchi et al\\.,? 2017",
      "shortCiteRegEx" : "Eriguchi et al\\.",
      "year" : 2017
    }, {
      "title" : "Regularized multi–task learning",
      "author" : [ "Theodoros Evgeniou", "Massimiliano Pontil." ],
      "venue" : "Proceedings of the tenth ACM SIGKDD international conference on Knowledge discovery and data mining, pages 109– 117.",
      "citeRegEx" : "Evgeniou and Pontil.,? 2004",
      "shortCiteRegEx" : "Evgeniou and Pontil.",
      "year" : 2004
    }, {
      "title" : "Enhanced neural machine translation by joint decoding with word and pos-tagging sequences",
      "author" : [ "Xiaocheng Feng", "Zhangyin Feng", "Wanlong Zhao", "Bing Qin", "Ting Liu." ],
      "venue" : "Mob. Networks Appl., 25(5):1722–1728.",
      "citeRegEx" : "Feng et al\\.,? 2020",
      "shortCiteRegEx" : "Feng et al\\.",
      "year" : 2020
    }, {
      "title" : "Scalable inference and training of context-rich syntactic translation models",
      "author" : [ "Michel Galley", "Jonathan Graehl", "Kevin Knight", "Daniel Marcu", "Steve DeNeefe", "Wei Wang", "Ignacio Thayer." ],
      "venue" : "Proceedings of the 21st International Conference on",
      "citeRegEx" : "Galley et al\\.,? 2006",
      "shortCiteRegEx" : "Galley et al\\.",
      "year" : 2006
    }, {
      "title" : "Aligned cross entropy for non-autoregressive machine translation",
      "author" : [ "Marjan Ghazvininejad", "Vladimir Karpukhin", "Luke Zettlemoyer", "Omer Levy." ],
      "venue" : "Proceedings of the 37th International Conference",
      "citeRegEx" : "Ghazvininejad et al\\.,? 2020",
      "shortCiteRegEx" : "Ghazvininejad et al\\.",
      "year" : 2020
    }, {
      "title" : "Mask-predict: Parallel decoding of conditional masked language models",
      "author" : [ "Marjan Ghazvininejad", "Omer Levy", "Yinhan Liu", "Luke Zettlemoyer." ],
      "venue" : "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and",
      "citeRegEx" : "Ghazvininejad et al\\.,? 2019",
      "shortCiteRegEx" : "Ghazvininejad et al\\.",
      "year" : 2019
    }, {
      "title" : "Nonautoregressive neural machine translation",
      "author" : [ "Jiatao Gu", "James Bradbury", "Caiming Xiong", "Victor O.K. Li", "Richard Socher." ],
      "venue" : "6th International Conference on Learning Representations, ICLR 2018, Vancouver, BC, Canada, April 30",
      "citeRegEx" : "Gu et al\\.,? 2018",
      "shortCiteRegEx" : "Gu et al\\.",
      "year" : 2018
    }, {
      "title" : "Fine-tuning by curriculum learning for non-autoregressive neural machine translation",
      "author" : [ "Junliang Guo", "Xu Tan", "Linli Xu", "Tao Qin", "Enhong Chen", "Tie-Yan Liu." ],
      "venue" : "Proceedings of the AAAI Conference on Artificial Intelligence, volume 34, pages",
      "citeRegEx" : "Guo et al\\.,? 2020a",
      "shortCiteRegEx" : "Guo et al\\.",
      "year" : 2020
    }, {
      "title" : "Jointly masked sequence-to-sequence model for nonautoregressive neural machine translation",
      "author" : [ "Junliang Guo", "Linli Xu", "Enhong Chen." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 376–385.",
      "citeRegEx" : "Guo et al\\.,? 2020b",
      "shortCiteRegEx" : "Guo et al\\.",
      "year" : 2020
    }, {
      "title" : "Non-autoregressive machine translation with disentangled context transformer",
      "author" : [ "Jungo Kasai", "James Cross", "Marjan Ghazvininejad", "Jiatao Gu." ],
      "venue" : "Proceedings of the 37th International Conference on Machine Learning, ICML 2020, 13-18 July 2020,",
      "citeRegEx" : "Kasai et al\\.,? 2020a",
      "shortCiteRegEx" : "Kasai et al\\.",
      "year" : 2020
    }, {
      "title" : "Deep encoder, shallow decoder: Reevaluating the speed-quality tradeoff in machine translation",
      "author" : [ "Jungo Kasai", "Nikolaos Pappas", "Hao Peng", "James Cross", "Noah A. Smith." ],
      "venue" : "CoRR, abs/2006.10369.",
      "citeRegEx" : "Kasai et al\\.,? 2020b",
      "shortCiteRegEx" : "Kasai et al\\.",
      "year" : 2020
    }, {
      "title" : "Sequencelevel knowledge distillation",
      "author" : [ "Yoon Kim", "Alexander M. Rush." ],
      "venue" : "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, EMNLP 2016, Austin, Texas, USA, November 1-4, 2016, pages 1317–1327. The",
      "citeRegEx" : "Kim and Rush.,? 2016",
      "shortCiteRegEx" : "Kim and Rush.",
      "year" : 2016
    }, {
      "title" : "Hint-based training for non-autoregressive machine translation",
      "author" : [ "Zhuohan Li", "Zi Lin", "Di He", "Fei Tian", "Tao Qin", "Liwei Wang", "Tie-Yan Liu." ],
      "venue" : "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and",
      "citeRegEx" : "Li et al\\.,? 2019",
      "shortCiteRegEx" : "Li et al\\.",
      "year" : 2019
    }, {
      "title" : "ROUGE: A package for automatic evaluation of summaries",
      "author" : [ "Chin-Yew Lin." ],
      "venue" : "Text Summarization Branches Out, pages 74–81, Barcelona, Spain. Association for Computational Linguistics.",
      "citeRegEx" : "Lin.,? 2004",
      "shortCiteRegEx" : "Lin.",
      "year" : 2004
    }, {
      "title" : "Tigs: An inference algorithm for text infilling with gradient search",
      "author" : [ "Dayiheng Liu", "Jie Fu", "Pengfei Liu", "Jiancheng Lv." ],
      "venue" : "ACL.",
      "citeRegEx" : "Liu et al\\.,? 2019",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2019
    }, {
      "title" : "Revision in continuous space: Unsupervised text style transfer without adversarial learning",
      "author" : [ "Dayiheng Liu", "Jie Fu", "Yidan Zhang", "Chris Pal", "Jiancheng Lv." ],
      "venue" : "AAAI.",
      "citeRegEx" : "Liu et al\\.,? 2020a",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2020
    }, {
      "title" : "Tell me how to ask again: Question data augmentation with controllable rewriting in continuous space",
      "author" : [ "Dayiheng Liu", "Yeyun Gong", "Jie Fu", "Yu Yan", "Jiusheng Chen", "Jiancheng Lv", "Nan Duan", "Ming Zhou." ],
      "venue" : "Proceedings of the 2020 Confer-",
      "citeRegEx" : "Liu et al\\.,? 2020b",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2020
    }, {
      "title" : "2020c. GLGE: A",
      "author" : [ "Dayiheng Liu", "Yu Yan", "Yeyun Gong", "Weizhen Qi", "Hang Zhang", "Jian Jiao", "Weizhu Chen", "Jie Fu", "Linjun Shou", "Ming Gong", "Pengcheng Wang", "Jiusheng Chen", "Daxin Jiang", "Jiancheng Lv", "Ruofei Zhang", "Winnie Wu", "Ming Zhou", "Nan Duan" ],
      "venue" : null,
      "citeRegEx" : "Liu et al\\.,? \\Q2020\\E",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2020
    }, {
      "title" : "Task-level curriculum learning for non-autoregressive neural machine translation",
      "author" : [ "Jinglin Liu", "Yi Ren", "Xu Tan", "Chen Zhang", "Tao Qin", "Zhou Zhao", "Tie-Yan Liu." ],
      "venue" : "Proceedings of the TwentyNinth International Joint Conference on Artificial In-",
      "citeRegEx" : "Liu et al\\.,? 2020d",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2020
    }, {
      "title" : "Treeto-string alignment template for statistical machine translation",
      "author" : [ "Yang Liu", "Qun Liu", "Shouxun Lin." ],
      "venue" : "Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Compu-",
      "citeRegEx" : "Liu et al\\.,? 2006",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2006
    }, {
      "title" : "Flowseq: Nonautoregressive conditional sequence generation with generative flow",
      "author" : [ "Xuezhe Ma", "Chunting Zhou", "Xian Li", "Graham Neubig", "Eduard H. Hovy." ],
      "venue" : "Proceedings of the 2019 Conference on Empirical Methods in Natural Language",
      "citeRegEx" : "Ma et al\\.,? 2019",
      "shortCiteRegEx" : "Ma et al\\.",
      "year" : 2019
    }, {
      "title" : "A corpus and cloze evaluation for deeper understanding of commonsense stories",
      "author" : [ "Nasrin Mostafazadeh", "Nathanael Chambers", "Xiaodong He", "Devi Parikh", "Dhruv Batra", "Lucy Vanderwende", "Pushmeet Kohli", "James F. Allen." ],
      "venue" : "NAACL HLT 2016, The",
      "citeRegEx" : "Mostafazadeh et al\\.,? 2016",
      "shortCiteRegEx" : "Mostafazadeh et al\\.",
      "year" : 2016
    }, {
      "title" : "Don’t give me the details, just the summary! topic-aware convolutional neural networks for extreme summarization",
      "author" : [ "Shashi Narayan", "Shay B. Cohen", "Mirella Lapata." ],
      "venue" : "Proceedings of the 2018 Conference on Empirical Methods in Natural Lan-",
      "citeRegEx" : "Narayan et al\\.,? 2018",
      "shortCiteRegEx" : "Narayan et al\\.",
      "year" : 2018
    }, {
      "title" : "fairseq: A fast, extensible toolkit for sequence modeling",
      "author" : [ "Myle Ott", "Sergey Edunov", "Alexei Baevski", "Angela Fan", "Sam Gross", "Nathan Ng", "David Grangier", "Michael Auli." ],
      "venue" : "Proceedings of NAACL-HLT 2019: Demonstrations.",
      "citeRegEx" : "Ott et al\\.,? 2019",
      "shortCiteRegEx" : "Ott et al\\.",
      "year" : 2019
    }, {
      "title" : "Bleu: a method for automatic evaluation of machine translation",
      "author" : [ "Kishore Papineni", "Salim Roukos", "Todd Ward", "WeiJing Zhu." ],
      "venue" : "Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics, July 6-12, 2002, Philadelphia,",
      "citeRegEx" : "Papineni et al\\.,? 2002",
      "shortCiteRegEx" : "Papineni et al\\.",
      "year" : 2002
    }, {
      "title" : "Squad: 100, 000+ questions for machine comprehension of text",
      "author" : [ "Pranav Rajpurkar", "Jian Zhang", "Konstantin Lopyrev", "Percy Liang." ],
      "venue" : "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, EMNLP 2016, Austin,",
      "citeRegEx" : "Rajpurkar et al\\.,? 2016",
      "shortCiteRegEx" : "Rajpurkar et al\\.",
      "year" : 2016
    }, {
      "title" : "Learning to recover from multi-modality errors for non-autoregressive neural machine translation",
      "author" : [ "Qiu Ran", "Yankai Lin", "Peng Li", "Jie Zhou." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, ACL 2020,",
      "citeRegEx" : "Ran et al\\.,? 2020",
      "shortCiteRegEx" : "Ran et al\\.",
      "year" : 2020
    }, {
      "title" : "Non-autoregressive machine translation with latent alignments",
      "author" : [ "Chitwan Saharia", "William Chan", "Saurabh Saxena", "Mohammad Norouzi." ],
      "venue" : "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, EMNLP 2020, On-",
      "citeRegEx" : "Saharia et al\\.,? 2020",
      "shortCiteRegEx" : "Saharia et al\\.",
      "year" : 2020
    }, {
      "title" : "Neural machine translation of rare words with subword units",
      "author" : [ "Rico Sennrich", "Barry Haddow", "Alexandra Birch." ],
      "venue" : "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, ACL 2016, August 7-12, 2016, Berlin, Ger-",
      "citeRegEx" : "Sennrich et al\\.,? 2016",
      "shortCiteRegEx" : "Sennrich et al\\.",
      "year" : 2016
    }, {
      "title" : "Fast structured decoding for sequence models",
      "author" : [ "Zhiqing Sun", "Zhuohan Li", "Haoqing Wang", "Di He", "Zi Lin", "Zhi-Hong Deng." ],
      "venue" : "Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems",
      "citeRegEx" : "Sun et al\\.,? 2019",
      "shortCiteRegEx" : "Sun et al\\.",
      "year" : 2019
    }, {
      "title" : "Lexically constrained neural machine translation with levenshtein transformer",
      "author" : [ "Raymond Hendy Susanto", "Shamil Chollampatt", "Liling Tan." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, ACL 2020,",
      "citeRegEx" : "Susanto et al\\.,? 2020",
      "shortCiteRegEx" : "Susanto et al\\.",
      "year" : 2020
    }, {
      "title" : "Attention is all you need",
      "author" : [ "Ashish Vaswani", "Noam Shazeer", "Niki Parmar", "Jakob Uszkoreit", "Llion Jones", "Aidan N. Gomez", "Lukasz Kaiser", "Illia Polosukhin." ],
      "venue" : "Advances in Neural Information Processing Systems 30: Annual Conference on Neural",
      "citeRegEx" : "Vaswani et al\\.,? 2017",
      "shortCiteRegEx" : "Vaswani et al\\.",
      "year" : 2017
    }, {
      "title" : "Neural text generation with unlikelihood training",
      "author" : [ "Sean Welleck", "Ilia Kulikov", "Stephen Roller", "Emily Dinan", "Kyunghyun Cho", "Jason Weston." ],
      "venue" : "8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April",
      "citeRegEx" : "Welleck et al\\.,? 2020",
      "shortCiteRegEx" : "Welleck et al\\.",
      "year" : 2020
    }, {
      "title" : "Sequence-to-dependency neural machine translation",
      "author" : [ "Shuangzhi Wu", "Dongdong Zhang", "Nan Yang", "Mu Li", "Ming Zhou." ],
      "venue" : "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, ACL 2017, Vancouver, Canada,",
      "citeRegEx" : "Wu et al\\.,? 2017",
      "shortCiteRegEx" : "Wu et al\\.",
      "year" : 2017
    }, {
      "title" : "Inducing multilingual pos taggers and np bracketers via robust projection across aligned corpora",
      "author" : [ "David Yarowsky", "Grace Ngai." ],
      "venue" : "Second Meeting of the North American Chapter of the Association for Computational Linguistics.",
      "citeRegEx" : "Yarowsky and Ngai.,? 2001",
      "shortCiteRegEx" : "Yarowsky and Ngai.",
      "year" : 2001
    }, {
      "title" : "Understanding knowledge distillation in non-autoregressive machine translation",
      "author" : [ "Chunting Zhou", "Graham Neubig", "Jiatao Gu." ],
      "venue" : "CoRR, abs/1911.02727.",
      "citeRegEx" : "Zhou et al\\.,? 2019",
      "shortCiteRegEx" : "Zhou et al\\.",
      "year" : 2019
    } ],
    "referenceMentions" : [ {
      "referenceID" : 8,
      "context" : "Despite the computational advantage of NAG, it has faced the multimodality problem (Gu et al., 2018) caused by the conditionally independent decoding.",
      "startOffset" : 83,
      "endOffset" : 100
    }, {
      "referenceID" : 37,
      "context" : "Rush, 2016) which aims to reduce the generation modes of the raw data (Zhou et al., 2019).",
      "startOffset" : 70,
      "endOffset" : 89
    }, {
      "referenceID" : 13,
      "context" : ", sequence-level knowledge distillation (Kim and Rush, 2016), learning from AG model’s hidden state (Li et al.",
      "startOffset" : 40,
      "endOffset" : 60
    }, {
      "referenceID" : 14,
      "context" : ", sequence-level knowledge distillation (Kim and Rush, 2016), learning from AG model’s hidden state (Li et al., 2019) and the curriculum learning with AG model (Liu et al.",
      "startOffset" : 100,
      "endOffset" : 117
    }, {
      "referenceID" : 20,
      "context" : ", 2019) and the curriculum learning with AG model (Liu et al., 2020d; Guo et al., 2020a).",
      "startOffset" : 50,
      "endOffset" : 88
    }, {
      "referenceID" : 9,
      "context" : ", 2019) and the curriculum learning with AG model (Liu et al., 2020d; Guo et al., 2020a).",
      "startOffset" : 50,
      "endOffset" : 88
    }, {
      "referenceID" : 17,
      "context" : "Text generation involves multiple tasks, such as style transfer (Liu et al., 2020a) and text filling (Liu",
      "startOffset" : 64,
      "endOffset" : 83
    }, {
      "referenceID" : 21,
      "context" : "Dating back to the period of statistical machine translation (Liu et al., 2006; Galley et al., 2006), linguistic structure prediction has long been investigated for it.",
      "startOffset" : 61,
      "endOffset" : 100
    }, {
      "referenceID" : 5,
      "context" : "Dating back to the period of statistical machine translation (Liu et al., 2006; Galley et al., 2006), linguistic structure prediction has long been investigated for it.",
      "startOffset" : 61,
      "endOffset" : 100
    }, {
      "referenceID" : 35,
      "context" : "Previous works often model and leverage syntactic structures on the decoder side, such as modeling long-distance word correspondence by syntactic dependency trees (Wu et al., 2017), implicitly incorporate linguistic prior in decoder (Eriguchi et al.",
      "startOffset" : 163,
      "endOffset" : 180
    }, {
      "referenceID" : 2,
      "context" : ", 2017), implicitly incorporate linguistic prior in decoder (Eriguchi et al., 2017) and joint decoding with syntactic structure (Feng et al.",
      "startOffset" : 60,
      "endOffset" : 83
    }, {
      "referenceID" : 4,
      "context" : ", 2017) and joint decoding with syntactic structure (Feng et al., 2020).",
      "startOffset" : 52,
      "endOffset" : 71
    }, {
      "referenceID" : 33,
      "context" : "of our POS predictor is a variant of the standard Transformer (Vaswani et al., 2017).",
      "startOffset" : 62,
      "endOffset" : 84
    }, {
      "referenceID" : 3,
      "context" : "POS predictor, we take a multi-task learning (Evgeniou and Pontil, 2004) paradigm to jointly decode the word sequence and POS sequence on the target side.",
      "startOffset" : 45,
      "endOffset" : 72
    }, {
      "referenceID" : 30,
      "context" : "Almost all NAG models use the Byte Pair Encoding (BPE) (Sennrich et al., 2016) technique to build the word vocabulary with subword-level tokens.",
      "startOffset" : 55,
      "endOffset" : 78
    }, {
      "referenceID" : 36,
      "context" : "However, these tokens cannot be tagged by the mainstream POS Taggers (Yarowsky and Ngai, 2001), which makes difficulties in building the POS dataset.",
      "startOffset" : 69,
      "endOffset" : 94
    }, {
      "referenceID" : 24,
      "context" : "XSUM (Narayan et al., 2018) includes the 227k British Broadcasting Corporation (BBC) online articles and the corresponding single-sentence summaries.",
      "startOffset" : 5,
      "endOffset" : 27
    }, {
      "referenceID" : 23,
      "context" : "ROCStories Corpus5 (Mostafazadeh et al., 2016) contains 98k five-sentence stories.",
      "startOffset" : 19,
      "endOffset" : 46
    }, {
      "referenceID" : 27,
      "context" : "16 (Rajpurkar et al., 2016) is a machine reading comprehension data set containing 98K passage-question-answer triples (Liu et al.",
      "startOffset" : 3,
      "endOffset" : 27
    }, {
      "referenceID" : 18,
      "context" : ", 2016) is a machine reading comprehension data set containing 98K passage-question-answer triples (Liu et al., 2020b).",
      "startOffset" : 99,
      "endOffset" : 118
    }, {
      "referenceID" : 15,
      "context" : ", 2020c), we use ROUGE-1 (R-1), ROUGE-2 (R-2), and ROUGE-L (R-L) (Lin, 2004) as evaluation metrics for text summarization,",
      "startOffset" : 65,
      "endOffset" : 76
    }, {
      "referenceID" : 1,
      "context" : "teor (Denkowski and Lavie, 2014), and R-L are used in question generation and story ending generation.",
      "startOffset" : 5,
      "endOffset" : 32
    }, {
      "referenceID" : 8,
      "context" : "Meanwhile, BLEU-4 is also the evaluation metric for machine translation to keep in line with previous works (Gu et al., 2018).",
      "startOffset" : 108,
      "endOffset" : 125
    }, {
      "referenceID" : 12,
      "context" : "NAG models as backbones, because they are one of the mainstream NAG structures in current works and perform competitively to AG models without any external system (Kasai et al., 2020b).",
      "startOffset" : 163,
      "endOffset" : 184
    }, {
      "referenceID" : 33,
      "context" : "(2018) which uses a standard transformer (Vaswani et al., 2017) as the teacher model to regenerate training set in the greedy method for NAG models (hereinafter described as “Transformer-1 (66)”), we report NAG models’ performances on",
      "startOffset" : 41,
      "endOffset" : 63
    }, {
      "referenceID" : 33,
      "context" : "We follow the hyperparameters for standard Transformer in (Vaswani et al., 2017) for our POS predictor.",
      "startOffset" : 58,
      "endOffset" : 80
    }, {
      "referenceID" : 25,
      "context" : "All of the models are implemented based on Fairseq (Ott et al., 2019), and we follow the other specific parameter settings for both AG and NAG models in (Kasai et al.",
      "startOffset" : 51,
      "endOffset" : 69
    }, {
      "referenceID" : 12,
      "context" : ", 2019), and we follow the other specific parameter settings for both AG and NAG models in (Kasai et al., 2020b).",
      "startOffset" : 91,
      "endOffset" : 112
    }, {
      "referenceID" : 8,
      "context" : "Fairseq, which is tested on a single NVIDIA Tesla P100 GPU to keep in line with previous works (Gu et al., 2018).",
      "startOffset" : 95,
      "endOffset" : 112
    }, {
      "referenceID" : 34,
      "context" : "Concretely, we refer to a “single-token repeat” metric (Welleck et al., 2020) and define the repetition rate here as",
      "startOffset" : 55,
      "endOffset" : 77
    } ],
    "year" : 2021,
    "abstractText" : "The multimodality problem has become a major challenge of existing non-autoregressive generation (NAG) systems. A common solution often resorts to sequence-level knowledge distillation by rebuilding the training dataset through autoregressive generation (hereinafter known as “teacher AG”). The success of such methods may largely depend on a latent assumption, i.e., the teacher AG is superior to the NAG model. However, in this work, we experimentally reveal that this assumption does not always hold for the text generation tasks like text summarization and story ending generation. To provide a feasible solution to the multimodality problem of NAG, we propose incorporating linguistic structure (Part-of-Speech sequence in particular) into NAG inference instead of relying on teacher AG. More specifically, the proposed POS-constrained Parallel Decoding (POSPD) method aims at providing a specific POS sequence to constrain the NAG model during decoding. Our experiments demonstrate that POSPD consistently improves NAG models on four text generation tasks to a greater extent compared to knowledge distillation. This observation validates the necessity of exploring the alternatives for sequence-level knowledge distillation.",
    "creator" : "LaTeX with hyperref"
  }
}