{
  "name" : "2021.acl-long.435.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "Maria: A Visual Experience Powered Conversational Agent",
    "authors" : [ "Zujie Liang", "Huang Hu", "Can Xu", "Chongyang Tao", "Xiubo Geng", "Yining Chen", "Fan Liang", "Daxin Jiang" ],
    "emails" : [ "{liangzj9@mail2.sysu.edu.cn,", "isslf@mail.sysu.edu.cn}", "huahu@microsoft.com", "caxu@microsoft.com", "chotao@microsoft.com", "xigeng@microsoft.com", "yinichen@microsoft.com", "djiang@microsoft.com" ],
    "sections" : [ {
      "heading" : null,
      "text" : "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 5596–5611\nAugust 1–6, 2021. ©2021 Association for Computational Linguistics\n5596"
    }, {
      "heading" : "1 Introduction",
      "text" : "Building intelligent conversational agents that can not only converse freely with human but also have the ability to perceive the physical world, has been one of the longest standing goals of natural language processing (NLP) and artificial intelligence (AI). Although the recent large-scale conversation models trained on text-only corpora, such as Meena\n∗Work performed during the internship at Microsoft. †Equal contribution. ‡Corresponding author.\n(Adiwardana et al., 2020), Blender (Roller et al., 2020) and DialoGPT (Zhang et al., 2020), have shown the compelling performance, they are still lack of the perception ability to our physical world. A recent study (Bisk et al., 2020) points out the successful linguistic communication relies on a shared experience of the world that makes language really meaningful. The visual perception is a rich signal for modeling a vastness of experiences in the world that cannot be documented by text alone (Harnad, 1990). On the other hand, human-human conversations involve their understandings of context, the background knowledge they had, and perhaps most importantly the experiences of the world they shared, e.g., what they have seen before.\nFigure 1 shows a conversation between humans. Human-A recalls his/her past experience of playing volleyball or having BBQ on the beach when human-B talks about vacation on the beach of Hawaii. However, the association relationship between beach and volleyball (or BBQ) is hard to capture in traditional knowledge bases, such as knowledge graph. Motivated by this, we select a common word “pizza” and collect the top 17 words that mostly co-occur with “pizza” on Google\nKnowledge Graph1 and MS-COCO images2 (Lin et al., 2014). As shown in Figure 2, the words cooccurring with “pizza” on knowledge graph tend to be the abstract concepts, while the co-occurrence relationship of object tags on images reflects some commonsense of our physical world, e.g., “pizza” is usually on the “dining table”, people usually use “knife” when eating “pizza”. Interestingly, we found the “pizza” also co-occurs with “cell phone” and even “plotted plant”. This indicates when people eat pizza, they sometimes would put their cell phones aside on the table, or there might exist some plotted plants in the restaurant. Thus, empowering conversational agents to have the visual perception ability about the physical world is a key way for them to exhibit the human-like intelligence.\nThe existing works (Mostafazadeh et al., 2017; Huber et al., 2018; Shuster et al., 2020) focus on exploring the multimodal dialog models that ground the conversation on a given image. Recently, Yang et al. (2020) propose to learn the dialog generation model with both image-grounded dialogs and textual dialogs by resorting to text-to-image synthesis techniques (Xu et al., 2018; Qiao et al., 2019) to restore a latent image for the text-only dialog. Even so, these works are still constrained by the assumption that the dialog is conducted center around a given (or synthesized) image.\nIn this paper, we take a step further to extend the assumption of image-grounded conversation to a fully open-ended setting where no imagedialog pairs are assumed available. Specifically, we present Maria, a neural conversational agent powered by visual world experiences which are retrieved from a pre-built image index, e.g., the\n1https://developers.google.com/ knowledge-graph/\n2We calculate the co-occurrence distribution of object tags from the images in MS-COCO dataset. More examples could be found in Appendices.\nOpen Images Dataset (Kuznetsova et al., 2018). Maria consists of three components: text-toimage retriever, visual concept detector, and visualknowledge-grounded response generator. The retriever is responsible for retrieving a piece of visual world experiences, e.g., a correlated image to the dialog from an image index. The visual concept detector utilizes the object detector from UpDown (Anderson et al., 2018) to extract the regions features (i.e., bboxes) and the corresponding visual concepts (i.e., tags) from the retrieval images. Hence, we can construct (bboxes, tags, context, response) 4-tuple as the training data. Finally, these constructed 4-tuples are used to train the visualknowledge-grounded response generator, which is built on the top of a multi-layer Transformer architecture (Vaswani et al., 2017). To effectively inject the visual knowledge into the response generator, we carry out the Masked Concept Prediction and Visual Knowledge Bias besides the response generation objective. The former aims to align the semantic representations between textual words and image regions, while the latter tries to provide more visual knowledge to facilitate the dialog generation. The experimental results on Reddit Conversation Corpus (Dziri et al., 2019a) demonstrate that Maria significantly outperforms previous state-of-the-art methods, and can generate informative responses with visual commonsense of our physical world.\nOverall, the contributions of this paper are summarized as follows:\n• We explore the task of image-grounded dialog generation under a fully open-ended setting where no specific image-dialog pairs are assumed available, i.e., zero-resource imagegrounded conversation. To the best of our knowledge, this is the first work to connect dialog corpus with the unpaired image data;\n• We present Maria, a neural conversational\nagent consisting of three flexible components, which can effectively capture the visual commonsense from images and accordingly generate informative and vivid responses;\n• Extensive experiments on the widely used Reddit Conversation Corpus are conducted to justify the effectiveness of Maria."
    }, {
      "heading" : "2 Related Work",
      "text" : "Vision and Language In the research of vision and language, various tasks have been extensively studied, such as image captioning (Vinyals et al., 2015; Lu et al., 2017; Hu et al., 2020), visual question answering (Antol et al., 2015; Anderson et al., 2018), visual dialog (Das et al., 2017a,b). Popular benchmark datasets in this area include MS-COCO (Lin et al., 2014), VisDial (Das et al., 2017a) and Visual Genome (Krishna et al., 2017). Visual dialog is a task to answer the questions about the factual content of the image in a multi-turn manner. Differently, image-grounded conversation studies how to reply to a dialog context and a given image with proper responses in an open-ended way.\nDialog Generation Encouraged by the success of the neural sequence-to-sequence architecture (Sutskever et al., 2014) on machine translation, end-to-end neural approaches on open-domain dialog generation (Vinyals and Le, 2015; Shang et al., 2015; Serban et al., 2016; Sordoni et al., 2015; Xing et al., 2017; Wu et al., 2018; Zhang et al., 2020; Xu et al., 2019; Adiwardana et al., 2020) have been widely studied in literature. Recently, there is an emerging trend towards grounding the dialog generation models on the external knowledge, such as knowledge graphs (Zhou et al., 2018), documents (Ghazvininejad et al., 2018; Dinan et al., 2019; Kim et al., 2020; Zhao et al., 2020a,b; Li et al., 2020) and images (Mostafazadeh et al., 2017; Shuster et al., 2020; Yang et al., 2020). Different from the previous work on knowledge-grounded conversation that connects dialogs with unpaired document knowledge (Li et al., 2020), our work lies in the research of image-grounded conversation where a response is generated with a dialog context and a given image. Existing works (Mostafazadeh et al., 2017; Shuster et al., 2020; Yang et al., 2020) in this direction assume there is a given (or synthesized) image for the dialog and explore the multimodal dialog models. In contrast to these works, we study the image-grounded conversation under\na fully open-ended assumption where no paired dialog and image are assumed available, i.e., zeroresource image-grounded conversation."
    }, {
      "heading" : "3 Problem Formalization",
      "text" : "Suppose we have a dialog set D = {(Ci, Ri)}ni=1, where ∀i ∈ {1, . . . , n}, Ci refers to a dialog context and Ri is a response to Ci. We assume there is a set of images V = {Vj}mj=1, where ∀j ∈ {1, . . . ,m}, Vj denotes an image. ∀C ∈ D, we assume that there is an image V that triggered by the given dialog context C and response R. Our goal is to estimate a generation model P (R|V,C) from D and V . Thus, given a new dialog context C associated with an image V , the model can generate a response R according to P (R|V,C)."
    }, {
      "heading" : "4 Methodology",
      "text" : "To learn such a generation model P (R|V,C), we need to tackle several challenges: (1) How to bridge the gap between unpaired dialog corpus and image data; (2) After obtaining the correlated images, how to extract the detailed visual features and concepts; (3) How to effectively inject the visual knowledge into response generator and enable it to generate responses that are visual-knowledge-grounded. Figure 3 illustrates the framework of our approach. We first build a large-scale image dataset and leverage a cross-modal matching model to retrieve a correlated image using the content of the dialog. Then an off-the-shelf object detector is applied to extracting the object features and visual concepts from the retrieval image. Finally, the response generator is trained to generate the target response conditioned\non the context, extracted object features, and visual concepts. In the rest of this section, we will elaborate these three modules."
    }, {
      "heading" : "4.1 Text-to-Image Retriever",
      "text" : "In this section, we develop a retrieval model that assigns each dialog with a correlated image V . Specifically, we train a text-to-image matching model from image captioning dataset and utilize it to construct the (C,R, V ) triple data.\nModeling To improve the efficiency of crossmodal retrieval model on large-scale dialog corpus and image dataset, we adopt a two-tower architecture (Lu et al., 2019) to accelerate the retrieval process where the image features can be pre-extracted offline. The model takes a sentence T and an image V as input, and predicts the relevance score s(T, V ) between the sentence and the image. We use a text encoder and an image encoder to produce the representations of T and V , respectively. The text encoder is a pre-trained BERT-base model (Devlin et al., 2019) and we use the hidden state of special token [CLS] as the embedding of T :\net = BERT (T ) (1)\nThen a Multi-Layer Perceptron (MLP) projects the sentence embedding into the cross-modal space. We follow Tan and Bansal (2020) to perform L2normalization on the last output features, by which we can simplify the nearest neighbor search problem in the euclidean space to the Maximum Inner Product problem (Mussmann and Ermon, 2016):\nft (T ) = Ht (et)\n‖Ht (et)‖ (2)\nSimilarly, the image encoder is composed of a pretrained ResNeXt backbone (Xie et al., 2017) and a MLP with L2 normalization:\nfv (V ) = Hv (ev)\n‖Hv (ev)‖ , ev = ResNeXt(V ) (3)\nThus, we define the relevance score s(T, V ) as an inner product of the language feature representation ft (T ) and image feature representation fv (V ):\ns(T, V ) = ft (T ) > fv (V ) (4)\nTraining We train the cross-modal matching model on MS-COCO image captioning dataset (Lin et al., 2014), where each image is paired with 5 sentences describing its visual content. The model is\noptimized by minimizing the hinge loss so that the relevance score s (T, V ) of the positive imagesentence pair can be larger than the negative pair s (T, V −) by at least a margin M :\nLhinge ( T, V, V − ) =\nl∑ i=1 max{0,M − s (T, V ) +s ( T, V − )} (5) Inference Given the trained retrieval model, we can now assign each dialog with a correlated image V . To ensure the diversity and richness of the retrieval results, we fetch 500,000 images from the large-scale Open Images dataset (Kuznetsova et al., 2018) as our image set V . The image Vi ∈ V with the maximum relevance score is paired with the given dialog (Ci, Ri) ∈ D. Note that for the dialog in the training set, we use both the contextC and responseR are concatenated as the query for retrieval (i.e., T = (C,R)), which is beneficial to retrieving an image with the related visual knowledge. On the other hand, for the validation/test set of the dialog corpus, the query is only the context (i.e., T = C) so as to keep consistent with the real-world setting where the response is unavailable and need to be generated at inference."
    }, {
      "heading" : "4.2 Visual Concept Detector",
      "text" : "Given the correlated image Vi to the dialog as the visual clue, we can now extract the visual knowledge from it. One naive approach is to utilize the CNN-based models to extract the latent image features. However, this approach does not consider the fine-grained representation modeling for images, which is crucial for the dialog model to understand the local visual features in images. To address this issue, we adopt an object detection model (Anderson et al., 2018) pre-trained on Visual Genome (Krishna et al., 2017) to extract a set of salient object features O = {ok}Kk=1, where each object feature ok is a 2048-dimensional vector. These features represent the images at the level of objects and other salient regions, which has proven to be vital in many high-level image understanding tasks. Besides, the same detector is used to extract a set of visual concepts Q = {qm}Km=1, where each concept qm is the high-precision textual label of the visual region, e.g., “sunset”, “melon”, etc. In this manner, we simultaneously obtain the fine-grained image representations and the necessary visual concepts for the subsequent dialog generation."
    }, {
      "heading" : "4.3 Visual-Knowledge-Grounded Response Generator",
      "text" : "In this section, we propose a unified architecture to effectively inject a set of region features and corresponding visual concepts into the response generation model. In following parts, we describe the model design and training objectives in detail."
    }, {
      "heading" : "4.3.1 Model Architecture",
      "text" : "Figure 4 shows the architecture of our response generation model, which is a multi-layer transformer network for both bidirectional vision/context (O,Q,C) encoding, and unidirectional response R decoding, via the flexible self-attention masks inspired by (Dong et al., 2019)."
    }, {
      "heading" : "4.3.2 Input Representation",
      "text" : "For each token, the final input representation to the multi-layer transformer network is the elementwise summation of four kinds of embeddings, including token-level, turn-level, position-level, and segment-level. Then, we concatenate all the input representations to one sequence for model training.\nToken-Level The token-level embeddings are the concatenation of (Ow, Qw, Cw, Rw), which denote the token embedding sequence of visual objects, visual concepts, contexts and response respectively. Note that Ow is the object embedding transformed by a linear layer into the same dimension as word embedding.\nTurn-Level Since the dialog is multi-turn, we encode this turn order with a relative turn embedding (Bao et al., 2020). Specifically, the turn number is counted from the last utterance of the dialogue to the beginning. Note that as for the tokens corresponding to O and Q, we simply set them the same as the first utterance of C.\nPosition-Level Positional embedding encodes the signal of the token order in the total input sequence, which is the same as positional encoding of the original transformer (Vaswani et al., 2017).\nSegment-Level Segment embedding is employed to differentiate which segment the token is in, i.e., O,Q,C or R."
    }, {
      "heading" : "4.3.3 Masked Concept Prediction",
      "text" : "Due to the inherent gap between visual modality and textual modality, directly optimizing the model by response generation objective may result in the insufficient utilization of the visual knowledge. To\nalign the semantic representations of two modalities, we devise Masked Concept Prediction (MCP) objective. 15% of the visual concepts are randomly replaced with [MASK] tokens in each training instance, which need to be predicted by the model. However, one problem still remains, i.e., the visual concepts have no specific order when extracting from images. In other words, we need to model MCP as a matching problem of set, which does not need to consider the order of predicted concepts when there are more than two concepts masked out simultaneously. To tackle this, inspired by Hu et al. (2020), we adopt the Hungarian Matching Loss (Stewart et al., 2016; Carion et al., 2020) to estimate an optimal mapping α so that the prediction for each masked position is assigned one of the target concepts. Here we denote the set of all input as X = (O,Q,C,R), the set of the bidirectional self-attention part of X as B = (O,Q,C), the set of masked concepts as Q̂, the set of unmasked tokens as B\\Q̂, and the prediction probabilities of the corresponding representations in the final layer of transformer as H = {hi}mi=1 where hi is the probability distribution of the i-th masked position. Hence, the MCP loss can be defined as:\nLMCP(Q̂,H, α) = − ∑\nqα(i)∈Q̂\nlog hi ( qα(i) | B\\Q̂ ) (6)\nwhere α(i) is the index of the target concept assigned to the i-th prediction. When predicting a masked concept, the model will have to resort to visual region features, dialog contexts and other unmasked visual concepts. This would help the model to align the cross-modal representations between text and visual regions."
    }, {
      "heading" : "4.3.4 Masked Response Prediction",
      "text" : "Encouraged by the success of UniLM (Dong et al., 2019) in Seq2Seq tasks, we adopt the Masked Response Prediction (MRP) objective to model the response generation. During training, 70% of the tokens in R are randomly masked with the special token [MASK]. The model is optimized to recover the masked tokens. The masked response tokens and other unmasked tokens in the whole input sequence can be denoted as R̂ andX\\R̂, respectively. Suppose that pi is the conditional probability distribution of the i-th token in R, the MRP loss is the Negative Log-Likelihood (NLL) of the masked\nPosition-Level\nToken-Level\nNetwork\nTurn-Level\nSegment-Level\nMulti-Layer Transformer\nvis\nResponse (\uD835\uDC79)Dialog Context (\uD835\uDC6A)Visual Concepts (\uD835\uDC78)Region Features (\uD835\uDC76)\nvis vis vis vis vis tag tag tag tag usr usr usr usr usr usr usr usr usr usr sys sys sys sys sys sys sys sys sys sys\n0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29\n-2 -2 -2 -2 -2 -2 -2 -2 -2 -2 -2 -2 -2 -2 -2 -2 -1 -1 -1 -1 0 0 0 0 0 0 0 0 0 0\ndid you eat ? . is a for !SEP SEP SEP SEP BOS EOSMASK MASK MASK MASKhamwhat pizzatablecola cola\npizza match pizzaperfect\n\uD835\uDC42 \uD835\uDC44 \uD835\uDC36 \uD835\uDC45\n\uD835\uDC42 \uD835\uDC44 \uD835\uDC36\n\uD835\uDC45\n\uD835\uDC42,\uD835\uDC44, \uD835\uDC36: bidirectional attention \uD835\uDC45: attend to left content\nprevent from attending\nFigure 4: The overview of the response generation model. There are four kinds of inputs, i.e., image region features O, extracted visual concepts Q, dialog context C and response R. The self-attention mask in R is unidirectional, i.e., can only attend to the left context, while the self-attention mask in other segments is bidirectional.\nresponse tokens as follow:\nLMRP(X, R̂) = − ∑ wi∈R̂ log pi ( wi | X\\R̂ ) (7)\nNote that the self-attention mask in R is left-toright, but the rest are bidirectional. In other words, the tokens in O,Q and C can attend to each other from both directions, while the tokens in R can attend all tokens inO,Q,C and the leftward tokens in R including itself. MRP implicitly encourages the model to generate responses by learning the relationship among all input tokens.\nFor decoding, we first encode the image regions, visual concepts, dialog contexts, and a special token [BOS] as input. Then the model starts the generation by feeding a [MASK] token and samples a word from the predicted distribution over vocabulary. Then, the [MASK] token is replaced by the generated token and a new [MASK] is appended to the input sequence for next word prediction. The generation process terminates when the model predicts [EOS] token or reaches the pre-defined maximum length.\nVisual Knowledge Bias Normally, the top projection layer of generation model produces a probability distribution over the vocabulary:\np = softmax(Wer + b), (8)\nwhere the er ∈ Rd, W ∈ R|V |×d and b ∈ R|V | are the last output of the transformer network, weight and bias parameters of the decoding head, respectively. |V | denotes the vocabulary size. So far, the visual world knowledge is introduced into the response generation model by the shared-parameter self-attention layers. To further inject the visual knowledge into the generation model, we design a simple but effective strategy, namely Visual Knowledge Bias (VKB). Concretely, an additional visual\nvocabulary bias bq is first calculated as follow:\nbq = Fq(e q avg) (9)\nwhere Fq : Rd → R|V | is a projection layer. eqavg denotes the average pooling on all hidden representations of visual concepts, i.e., eqavg = AvgPooling(Eq) where Eq = (eq1, ..., e q K). Then, we mask non-visual-concept tokens in the vocabulary and the masked vocabulary bias b̂q ∈ R|V | is added to the top layer of generation model to get the final distribution over vocabulary:\np̂ = softmax(Wer + b+ b̂q) (10)\nWe leverage this final vocabulary distribution to calculate the MRP loss in Eq. 7 to optimize the model. This visual knowledge bias would encourage the model to generate more visual knowledge related tokens in the response.\nTo sum up, the final objective of our response generation model is to minimize the integrated loss:\nL = LMRP + LMCP (11)"
    }, {
      "heading" : "5 Experimental Setup",
      "text" : ""
    }, {
      "heading" : "5.1 Datasets",
      "text" : "To evaluate the performance of Maria, we conduct comprehensive experiments on the Reddit dataset released by Yang et al. (2020), which is a large-scale and high-quality multi-turn conversations extracted from Reddit Conversation Corpus (Dziri et al., 2019b). Each dialog has 3 to 5 utterances, and the training/validation/test set has 1M/20K/20K dialogs respectively.\nWe train and validate the retrieval model using the Karpathy’s split3 of the MS-COCO image captioning data, where the images are split into\n3https://cs.stanford.edu/people/ karpathy/deepimagesent\n113.2K/5K/5K samples as training/validation/test set, respectively. After the retrieval model is trained, we fetch 500K images from the Open Images dataset as the image index, and then retrieve images from it by dialog context and response to construct the training data for response generator."
    }, {
      "heading" : "5.2 Evaluation Metrics",
      "text" : "Both automatic metrics and human evaluation are employed to assess the performance of Maria and baselines. Automatic metrics include: (1) Fluency: perplexity (PPL) measures the confidence of the generated responses; (2) Relevance: BLEU-1 (Papineni et al., 2002), Rouge-L (Lin, 2004), and we follow Serban et al. (2017) to utilize Embedding Average cosine similarity, Vector Extrema cosine similarity, and Embedding Greedy Matching score. All this metrics are calculated by running the public NLG evaluation script4; (3) Diversity: Distinct-1 (Dist-1) and Distinct-2 (Dist-2) (Li et al., 2016) are defined as the number of distinct uni-grams or bi-grams divided by the total amount of words.\nIn human evaluation, we randomly select 100 dialogue contexts and the corresponding generated responses for Maria and compared baselines. Three human annotators are asked to score the response quality on a scale of {0, 1, 2} from three aspects, including Fluency, Relevance and Richness. The higher score means the better. Since each response receives 3 scores on each aspect, we report the average scores over annotators and responses. The inter-annotator agreement is measured by Fleiss’ Kappa(Fleiss and Cohen, 1973)."
    }, {
      "heading" : "5.3 Implementation Details",
      "text" : "For the retrieval model, ResNeXt-101-32x8d feature is used as the visual embedding, while the concatenation of the last 4 layers of BERT’s outputs is used as the textual embedding. Both embeddings are then respectively fed into an MLP composed of three layers of size (1024, 1024, 512). When training the retrieval model, we set the margin M = 0.5 for the hinge loss, and only tune the parameters of both MLPs while freezing the parameters of ResNeXt and BERT. The total training epoch is 20. At inference, the FAISS (Johnson et al., 2019) library is utilized to accelerate the inner product search by batch processing. We use the off-the-shelf object detector from UpDown (Anderson et al., 2018) to extract top-k (k=36) image\n4https://github.com/Maluuba/nlg-eval\nregion features and the corresponding visual concepts. The detector is a Faster R-CNN (Ren et al., 2015) model trained on the Visual Genome dataset (Krishna et al., 2017).\nFor the response generation model, we set the number of transformer layers L = 12 and the hidden embedding dimension D = 768. Besides, the network parameters are initialized by UniLM. The maximum sequence lengths of context and response are set to 110 and 40, respectively. The sequence lengths of region features and concept tokens are both set to 36. The batch size is 64. We use the Adam Optimizer (Kingma and Ba, 2015) with a learning rate 3e-5 to train the response generation model. The training is conducted on 4 Nvidia Tesla P40 24G GPU cards for 20 epochs."
    }, {
      "heading" : "5.4 Baselines",
      "text" : "We compare the following baselines in the experiments: (1) Seq2Seq: A standard Sequence to Seqence model with attention mechanism (Bahdanau et al., 2015). (2) HRED: A Hierarchical Recurrent Encoder-Decoder neural network (Serban et al., 2016). (3) VHRED: A variation of HRED that introduces latent variables into the generation (Serban et al., 2017). (4) ReCoSa: A hierarchical transformer-based model (Zhang et al., 2019) that achieves the state-of-the-art performance on benchmarks of dialog generation. (5) ImgVAE: A dialog generation model (Yang et al., 2020) that is trained on both textual dialogs and image-grounded dialogs by recovering a latent image behind the textual dialog within a conditional variational autoencoding framework. (6) DialoGPT: An opendomain dialog model (Zhang et al., 2020) that fine-tunes GPT-2 (Radford et al., 2019) on massive Reddit data. Since DialoGPT is a dialog generation model trained on the text-only corpus, we introduce it as an auxiliary baseline. For a fair comparison, we choose the same model size (L=12,D=768) of DialoGPT (117M) as our model."
    }, {
      "heading" : "6 Experimental Results",
      "text" : ""
    }, {
      "heading" : "6.1 Automatic and Human Evaluations",
      "text" : "We summarize the experimental results of automatic evaluations in Table 1. Maria achieves the substantial performance improvements over baselines on all metrics except for the comparison to DialoGPT. Especially, Maria significantly surpasses ImgVAE on Dist-1/2, which indicates introducing richer visual knowledge, i.e., image region features\nand the corresponding visual concepts, is beneficial to generating more diverse and informative responses. This also reflects in human evaluation of Table 2 that the richness score of Maria is higher than that of ImgVAE. Besides, in terms of relevance metrics including BLEU-1, Rouge-L, Average, Extrema and Greedy, Maria outperforms all baselines and even performs better than DialoGPT. This indicates introducing the extra visual knowledge related to dialog context can further force the model to produce more relevant responses.\nOn the other hand, the discrepancy of data distributions between the training data (i.e., ImageChat (Shuster et al., 2020) dataset) and test data (i.e., Reddit conversation dataset) of the text-toimage synthesis model in ImgVAE limits its performance in practice. Besides, constrained by the capability of the text-to-image synthesis model, the richness and diversity of the synthesized images are undesirable, while Maria can retrieve a variety of images from the large-scale image index. That may be the reason why ImgVAE consistently underperforms our Maria on relevance including automatic evaluation and human judgement, which also shows the superiority of the retrieval method for the zero-resource image-grounded conversation. Another observation is that Maria slightly underperforms DialoGPT on PPL and Dist-1/2. Since DialoGPT is a large-scale pre-training based dialog generation model and introduces the extra mutual\ninformation maximization objective to improve the informativeness of generated responses, which is consistent in human evaluation with respect to fluency and richness."
    }, {
      "heading" : "6.2 Ablation Study",
      "text" : "We conduct extensive ablation experiments over different model variants and input components to better understand their relative importance to the dialog generation task. As shown in Table 1, training the simplified versions of Maria or removing any visual signals from input components leads to worse performance in terms of relevance and diversity. In particular, the results on the ablation study validate that: (1) The performance improvement of dialog generation benefits from the MCP’s effectiveness in aligning the representations of text and vision; (2) When training Maria, introducing VKB can further improve the quality and diversity of generated responses; (3) Rich visual knowledge, i.e., image region features and visual concepts, play a significant role in improving the performance of dialog generation. Especially, removing the visual concepts leads to a dramatic performance drop on diversity. The phenomenon is due to the lack of necessary visual concepts, Maria can not well understand the visual world knowledge when only learning from the visual features."
    }, {
      "heading" : "6.3 Case Analysis",
      "text" : "To further investigate the quality of responses generated by Maria, we put an example of generated responses in Figure 5. As we can see from Figure 5, when the context talks about the supermarket “Aldi”, Maria can retrieve a “pizza” related image and generate the informative response grounded on\nit, i.e., “the pizza at Aldi is the best in the world”. This implies the commonsense that the supermarket usually has the pizza to sell. It is also observed that Maria pays more attention to the relevant image regions when generating the word “pizza”, which demonstrates that Maria could capture useful visual knowledge from the image and subsequently leverage it to generate commonsense-aware responses. More cases are demonstrated in Appendices."
    }, {
      "heading" : "7 Conclusions",
      "text" : "In this paper, we present Maria, a neural conversational agent powered by the visual world experiences. It is able to retrieve the visual world experiences with users and generate human-like responses with some visual commonsense. Extensive experiments demonstrate Maria achieves substantial improvements over the state-of-the-art methods in automatic and human evaluation. The future works could include: (1) Design a more precise and comprehensive image retriever to include multiple retrieval images; (2) Combining the retrieve module and dialog generation into an end-to-end model, instead of learning them individually; (3) Explore more efficient neural architectures to inject the visual knowledge into response generation."
    }, {
      "heading" : "A Appendices",
      "text" : "In this section, we show more examples of word co-occurrence distributions on Google knowledge graph and MS-COCO images. Besides, some conversation samples produced by Maria and the baselines are also presented in Section A.2.\nA.1 Word Co-occurrence Distribution Examples\nIn Figure 6, we present some supplementary examples of the word co-occurrence distribution on Google knowledge graph and MS-COCO images, including “traffic light”, “bed”, “book”, and “pot plant”. Figure 6 (a) shows the co-occurrence distributions of “traffic light” and other words on knowledge graph and images, respectively. As we can see, most of the co-occurred words with “traffic light” are the related concepts such as “smart traffic light”, “traffic light protocol”, “traffic light rating system”, etc. While the co-occurred words on images are usually “car”, “person”, “truck”, “bus”, etc, which we often see when walking by the traffic lights. Interestingly, we found “umbrella” and “clock” also co-occurs with “traffic light” in some images. For the former, the picture we can imagine is that people were holding the “umbrellas” when they walked through a zebra crossing under the “traffic light”. For the latter, the possible picture is that we can see both the “traffic light” and the “clock” on the top of a high building from a certain angle when walking on the street. Similar observations can be also seen in other examples.\n(a) traffic light\nMost of the co-occurrence words on knowledge graph are logically-related concepts. However, the co-occurrence relationship of object tags on images reflects some commonsense of our physical world, which implies some pictures that we human could easily imagine. This kind of knowledge is unique and inherent in images, but it can hardly be captured in the traditional knowledge bases, such as knowledge graph.\nA.2 Case Analysis Figure 7 shows some cases from the test set of Reddit data. We observe that the responses generated by Maria are more commonsensical and vivid than those of the baseline methods, which is consistent with our automatic and human evaluation results. Interestingly, Maria is able to retrieve correlated images using the dialog contexts, which makes its response more human-like. For instance, case (a) shows that when the dialog context marvels at “the pass of the world cup”, Maria recalls a football player and compliments him “the best player in the world”; case (b) shows that when the dialog context chats about the “Canada weather”, Maria is aware of the fact that “Canada” is often “snowy” and then talks about “Canada” in a funny tone, “I’ve never been to a place that doesn’t have snow”; case (c) shows that Maria understands that “swan” is sometimes “dangerous” when they are on the “beach”; case (d) shows that when the dialog context tries to guess one type of game, Maria recalls a ping-pong “ball” game and describes it; and etc.\nDialog Context:\nMaria\nA:the pass of the world cup (james rodriguez)\nB: absoleutely gorgeous.\nImgVAE: I love the world cup.\nDialoGPT: it 's not a pass of the world cup. Baseline Response:\nDetected Concepts: ball, socks, field, shirt, grass, man, shorts, player, sock\nResponse: he's the best player in the world\nDialog Context:\nMaria\nA: how is a swan gonna break your arm?\nB: smack you with their wings\nImgVAE: i 'm a little nervous about this.\nDialoGPT: i 'm a swan. Baseline Response:\nDetected Concepts: duck, picture, background, land, beach, border, line, ground, bird,\nsky, water, birds, frame\nResponse: i'm not sure if you're joking or not, but i'm sure they're going to be a bit more dangerous if they are on the beach\nDialog Context:\nMaria\nA: you 've just ruined my canada weather fetish , that it would be ideal place for me to live:\nfreezing cold winters in mild summers.\nB: ya , there are parts of canada with very nice mild summers , but those same places have mild winters too.\nImgVAE: i 'm from canada , but i 've never seen a cold winters winters in cold winters winters .\nDialoGPT: i 'm from the u.s. and i 've never heard of this.\nBaseline Response:\nDetected Concepts: person, fence, ground, tree, building, snow, border, pole, gate\nResponse: i'm from canada and i've never been to a place that doesn't have snow\n(a)\n(b)\n(c)\nDialog Context:\nMaria\nA: we all played this game at one point.\nB: what game is this ?\nImgVAE: it 's a game game game , game of thrones.\nDialoGPT: it 's a meme Baseline Response:\nDetected Concepts: shorts, background, heart, skateboard, woman, ball, bench, counter, shirt, hair, wall, net, stands, people\nResponse: it's a game where you play the ball with your hands\nDialog Context:\nMaria\nA: what 's the coldest you 've ever been?\nB: almost got hypothermia near the arctic circle because my rain jacket failed to be waterproof and all my layers were soaked.\nC: what kind of jacket?\nImgVAE: the one where you ski and the jacket?\nDialoGPT: it 's a winter jacket. Baseline Response:\nDetected Concepts: pants, sky, snow, person, clouds\nResponse: it's a snow jacket\nDialog Context:\nMaria\nA: what cool gadget under $200 can be life changing?\nB: a smartphone.\nImgVAE: i have a smartphone and i 'm a phone.\nDialoGPT: that 's not cool.\nBaseline Response:\nDetected Concepts: button, hand, wall, line, hair, phone, table, fingers, handle, thumb\nResponse: i already have a phone.\n(d)\n(e)\n(f)\nFigure 7: Case Study on the Reddit data from test split."
    } ],
    "references" : [ {
      "title" : "Towards a human-like open-domain chatbot",
      "author" : [ "Daniel Adiwardana", "Minh-Thang Luong", "David R So", "Jamie Hall", "Noah Fiedel", "Romal Thoppilan", "Zi Yang", "Apoorv Kulshreshtha", "Gaurav Nemade", "Yifeng Lu" ],
      "venue" : "arXiv preprint arXiv:2001.09977",
      "citeRegEx" : "Adiwardana et al\\.,? \\Q2020\\E",
      "shortCiteRegEx" : "Adiwardana et al\\.",
      "year" : 2020
    }, {
      "title" : "Bottom-up and top-down attention for image captioning and visual question answering",
      "author" : [ "Teney", "Mark Johnson", "Stephen Gould", "Lei Zhang." ],
      "venue" : "2018 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2018, Salt Lake City, UT,",
      "citeRegEx" : "Teney et al\\.,? 2018",
      "shortCiteRegEx" : "Teney et al\\.",
      "year" : 2018
    }, {
      "title" : "VQA: visual question answering",
      "author" : [ "Stanislaw Antol", "Aishwarya Agrawal", "Jiasen Lu", "Margaret Mitchell", "Dhruv Batra", "C. Lawrence Zitnick", "Devi Parikh." ],
      "venue" : "2015 IEEE International Conference on Computer Vision, ICCV 2015, Santiago, Chile, De-",
      "citeRegEx" : "Antol et al\\.,? 2015",
      "shortCiteRegEx" : "Antol et al\\.",
      "year" : 2015
    }, {
      "title" : "Neural machine translation by jointly learning to align and translate",
      "author" : [ "Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio." ],
      "venue" : "3rd International Conference on Learning Representations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015,",
      "citeRegEx" : "Bahdanau et al\\.,? 2015",
      "shortCiteRegEx" : "Bahdanau et al\\.",
      "year" : 2015
    }, {
      "title" : "PLATO: Pre-trained dialogue generation model with discrete latent variable",
      "author" : [ "Siqi Bao", "Huang He", "Fan Wang", "Hua Wu", "Haifeng Wang." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 85–96, Online.",
      "citeRegEx" : "Bao et al\\.,? 2020",
      "shortCiteRegEx" : "Bao et al\\.",
      "year" : 2020
    }, {
      "title" : "Experience grounds language",
      "author" : [ "Yonatan Bisk", "Ari Holtzman", "Jesse Thomason", "Jacob Andreas", "Yoshua Bengio", "Joyce Chai", "Mirella Lapata", "Angeliki Lazaridou", "Jonathan May", "Aleksandr Nisnevich", "Nicolas Pinto", "Joseph Turian." ],
      "venue" : "Proceedings of the",
      "citeRegEx" : "Bisk et al\\.,? 2020",
      "shortCiteRegEx" : "Bisk et al\\.",
      "year" : 2020
    }, {
      "title" : "End-to-end object detection with transformers",
      "author" : [ "Nicolas Carion", "Francisco Massa", "Gabriel Synnaeve", "Nicolas Usunier", "Alexander Kirillov", "Sergey Zagoruyko." ],
      "venue" : "European Conference on Computer Vision, pages 213–229. Springer.",
      "citeRegEx" : "Carion et al\\.,? 2020",
      "shortCiteRegEx" : "Carion et al\\.",
      "year" : 2020
    }, {
      "title" : "Visual dialog",
      "author" : [ "Abhishek Das", "Satwik Kottur", "Khushi Gupta", "Avi Singh", "Deshraj Yadav", "José M.F. Moura", "Devi Parikh", "Dhruv Batra." ],
      "venue" : "2017 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2017, Honolulu, HI, USA,",
      "citeRegEx" : "Das et al\\.,? 2017a",
      "shortCiteRegEx" : "Das et al\\.",
      "year" : 2017
    }, {
      "title" : "Learning cooperative visual dialog agents with deep reinforcement learning",
      "author" : [ "Abhishek Das", "Satwik Kottur", "José M.F. Moura", "Stefan Lee", "Dhruv Batra." ],
      "venue" : "IEEE International Conference on Computer Vision, ICCV 2017, Venice, Italy, October 22-",
      "citeRegEx" : "Das et al\\.,? 2017b",
      "shortCiteRegEx" : "Das et al\\.",
      "year" : 2017
    }, {
      "title" : "BERT: Pre-training of deep bidirectional transformers for language understanding",
      "author" : [ "Jacob Devlin", "Ming-Wei Chang", "Kenton Lee", "Kristina Toutanova." ],
      "venue" : "Proceedings of the 2019 Conference of the North American Chapter of the Association",
      "citeRegEx" : "Devlin et al\\.,? 2019",
      "shortCiteRegEx" : "Devlin et al\\.",
      "year" : 2019
    }, {
      "title" : "Wizard of wikipedia: Knowledge-powered conversational agents",
      "author" : [ "Emily Dinan", "Stephen Roller", "Kurt Shuster", "Angela Fan", "Michael Auli", "Jason Weston." ],
      "venue" : "7th International Conference on Learning Representations, ICLR 2019, New Orleans, LA,",
      "citeRegEx" : "Dinan et al\\.,? 2019",
      "shortCiteRegEx" : "Dinan et al\\.",
      "year" : 2019
    }, {
      "title" : "Unified language model pre-training for natural language understanding and generation",
      "author" : [ "Li Dong", "Nan Yang", "Wenhui Wang", "Furu Wei", "Xiaodong Liu", "Yu Wang", "Jianfeng Gao", "Ming Zhou", "Hsiao-Wuen Hon." ],
      "venue" : "Advances in Neural Infor-",
      "citeRegEx" : "Dong et al\\.,? 2019",
      "shortCiteRegEx" : "Dong et al\\.",
      "year" : 2019
    }, {
      "title" : "Augmenting neural response generation with context-aware topical attention",
      "author" : [ "Nouha Dziri", "Ehsan Kamalloo", "Kory Mathewson", "Osmar Zaiane." ],
      "venue" : "Proceedings of the First Workshop on NLP for Conversational AI, pages 18–31, Florence, Italy. Associ-",
      "citeRegEx" : "Dziri et al\\.,? 2019a",
      "shortCiteRegEx" : "Dziri et al\\.",
      "year" : 2019
    }, {
      "title" : "Augmenting neural response generation with context-aware topical attention",
      "author" : [ "Nouha Dziri", "Ehsan Kamalloo", "Kory Mathewson", "Osmar Zaiane." ],
      "venue" : "Proceedings of the First Workshop on NLP for Conversational AI, pages 18–31, Florence, Italy. Associ-",
      "citeRegEx" : "Dziri et al\\.,? 2019b",
      "shortCiteRegEx" : "Dziri et al\\.",
      "year" : 2019
    }, {
      "title" : "The equivalence of weighted kappa and the intraclass correlation coefficient as measures of reliability",
      "author" : [ "Joseph L Fleiss", "Jacob Cohen." ],
      "venue" : "Educational and psychological measurement, 33(3):613– 619.",
      "citeRegEx" : "Fleiss and Cohen.,? 1973",
      "shortCiteRegEx" : "Fleiss and Cohen.",
      "year" : 1973
    }, {
      "title" : "A knowledge-grounded neural",
      "author" : [ "Michel Galley" ],
      "venue" : null,
      "citeRegEx" : "Galley.,? \\Q2018\\E",
      "shortCiteRegEx" : "Galley.",
      "year" : 2018
    }, {
      "title" : "The open images dataset v4",
      "author" : [ "Kolesnikov" ],
      "venue" : null,
      "citeRegEx" : "Kolesnikov,? \\Q2018\\E",
      "shortCiteRegEx" : "Kolesnikov",
      "year" : 2018
    }, {
      "title" : "Zero-resource knowledge-grounded dialogue generation",
      "author" : [ "Linxiao Li", "Can Xu", "Wei Wu", "Yufan Zhao", "Xueliang Zhao", "Chongyang Tao." ],
      "venue" : "Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Pro-",
      "citeRegEx" : "Li et al\\.,? 2020",
      "shortCiteRegEx" : "Li et al\\.",
      "year" : 2020
    }, {
      "title" : "ROUGE: A package for automatic evaluation of summaries",
      "author" : [ "Chin-Yew Lin." ],
      "venue" : "Text Summarization Branches Out, pages 74–81, Barcelona, Spain. Association for Computational Linguistics.",
      "citeRegEx" : "Lin.,? 2004",
      "shortCiteRegEx" : "Lin.",
      "year" : 2004
    }, {
      "title" : "Microsoft coco: Common objects in context",
      "author" : [ "Tsung-Yi Lin", "Michael Maire", "Serge Belongie", "James Hays", "Pietro Perona", "Deva Ramanan", "Piotr Dollár", "C Lawrence Zitnick." ],
      "venue" : "European conference on computer vision, pages 740–755. Springer.",
      "citeRegEx" : "Lin et al\\.,? 2014",
      "shortCiteRegEx" : "Lin et al\\.",
      "year" : 2014
    }, {
      "title" : "Vilbert: Pretraining task-agnostic visiolinguistic representations for vision-and-language tasks",
      "author" : [ "Jiasen Lu", "Dhruv Batra", "Devi Parikh", "Stefan Lee." ],
      "venue" : "Advances in Neural Information Processing Systems 32: Annual Conference on Neural Infor-",
      "citeRegEx" : "Lu et al\\.,? 2019",
      "shortCiteRegEx" : "Lu et al\\.",
      "year" : 2019
    }, {
      "title" : "Knowing when to look: Adaptive attention via a visual sentinel for image captioning",
      "author" : [ "Jiasen Lu", "Caiming Xiong", "Devi Parikh", "Richard Socher." ],
      "venue" : "2017 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2017, Honolulu, HI, USA,",
      "citeRegEx" : "Lu et al\\.,? 2017",
      "shortCiteRegEx" : "Lu et al\\.",
      "year" : 2017
    }, {
      "title" : "Image-grounded conversations: Multimodal context for natural question and response generation",
      "author" : [ "Nasrin Mostafazadeh", "Chris Brockett", "Bill Dolan", "Michel Galley", "Jianfeng Gao", "Georgios Spithourakis", "Lucy Vanderwende." ],
      "venue" : "Proceedings of",
      "citeRegEx" : "Mostafazadeh et al\\.,? 2017",
      "shortCiteRegEx" : "Mostafazadeh et al\\.",
      "year" : 2017
    }, {
      "title" : "Learning and inference via maximum inner product search",
      "author" : [ "Stephen Mussmann", "Stefano Ermon." ],
      "venue" : "Proceedings of the 33nd International Conference on Machine Learning, ICML 2016, New York City, NY, USA, June 19-24, 2016, volume 48 of JMLR",
      "citeRegEx" : "Mussmann and Ermon.,? 2016",
      "shortCiteRegEx" : "Mussmann and Ermon.",
      "year" : 2016
    }, {
      "title" : "Bleu: a method for automatic evaluation of machine translation",
      "author" : [ "Kishore Papineni", "Salim Roukos", "Todd Ward", "WeiJing Zhu." ],
      "venue" : "Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics, pages 311–318, Philadelphia,",
      "citeRegEx" : "Papineni et al\\.,? 2002",
      "shortCiteRegEx" : "Papineni et al\\.",
      "year" : 2002
    }, {
      "title" : "Mirrorgan: Learning text-to-image generation by redescription",
      "author" : [ "Tingting Qiao", "Jing Zhang", "Duanqing Xu", "Dacheng Tao." ],
      "venue" : "IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2019, Long Beach, CA, USA, June 16-20, 2019,",
      "citeRegEx" : "Qiao et al\\.,? 2019",
      "shortCiteRegEx" : "Qiao et al\\.",
      "year" : 2019
    }, {
      "title" : "Language models are unsupervised multitask learners",
      "author" : [ "Alec Radford", "Jeffrey Wu", "Rewon Child", "David Luan", "Dario Amodei", "Ilya Sutskever." ],
      "venue" : "OpenAI blog, 1(8):9.",
      "citeRegEx" : "Radford et al\\.,? 2019",
      "shortCiteRegEx" : "Radford et al\\.",
      "year" : 2019
    }, {
      "title" : "Faster R-CNN: towards real-time object detection with region proposal networks",
      "author" : [ "Shaoqing Ren", "Kaiming He", "Ross B. Girshick", "Jian Sun." ],
      "venue" : "Advances in Neural Information Processing Systems 28: Annual Conference on Neural Information Pro-",
      "citeRegEx" : "Ren et al\\.,? 2015",
      "shortCiteRegEx" : "Ren et al\\.",
      "year" : 2015
    }, {
      "title" : "Recipes for building an open-domain chatbot",
      "author" : [ "Stephen Roller", "Emily Dinan", "Naman Goyal", "Da Ju", "Mary Williamson", "Yinhan Liu", "Jing Xu", "Myle Ott", "Kurt Shuster", "Eric M Smith" ],
      "venue" : "arXiv preprint arXiv:2004.13637",
      "citeRegEx" : "Roller et al\\.,? \\Q2020\\E",
      "shortCiteRegEx" : "Roller et al\\.",
      "year" : 2020
    }, {
      "title" : "Building end-to-end dialogue systems using generative hierarchical neural network models",
      "author" : [ "Iulian Vlad Serban", "Alessandro Sordoni", "Yoshua Bengio", "Aaron C. Courville", "Joelle Pineau." ],
      "venue" : "Proceedings of the Thirtieth AAAI Conference on Arti-",
      "citeRegEx" : "Serban et al\\.,? 2016",
      "shortCiteRegEx" : "Serban et al\\.",
      "year" : 2016
    }, {
      "title" : "A hierarchical latent variable encoder-decoder model for generating dialogues",
      "author" : [ "Iulian Vlad Serban", "Alessandro Sordoni", "Ryan Lowe", "Laurent Charlin", "Joelle Pineau", "Aaron C. Courville", "Yoshua Bengio." ],
      "venue" : "Proceedings of the Thirty-First AAAI",
      "citeRegEx" : "Serban et al\\.,? 2017",
      "shortCiteRegEx" : "Serban et al\\.",
      "year" : 2017
    }, {
      "title" : "Neural responding machine for short-text conversation",
      "author" : [ "Lifeng Shang", "Zhengdong Lu", "Hang Li." ],
      "venue" : "Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Lan-",
      "citeRegEx" : "Shang et al\\.,? 2015",
      "shortCiteRegEx" : "Shang et al\\.",
      "year" : 2015
    }, {
      "title" : "Image-chat: Engaging grounded conversations",
      "author" : [ "Kurt Shuster", "Samuel Humeau", "Antoine Bordes", "Jason Weston." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 2414–2429, Online. Association for",
      "citeRegEx" : "Shuster et al\\.,? 2020",
      "shortCiteRegEx" : "Shuster et al\\.",
      "year" : 2020
    }, {
      "title" : "A neural network approach to context-sensitive generation of conversational responses",
      "author" : [ "Alessandro Sordoni", "Michel Galley", "Michael Auli", "Chris Brockett", "Yangfeng Ji", "Margaret Mitchell", "Jian-Yun Nie", "Jianfeng Gao", "Bill Dolan." ],
      "venue" : "Proceedings",
      "citeRegEx" : "Sordoni et al\\.,? 2015",
      "shortCiteRegEx" : "Sordoni et al\\.",
      "year" : 2015
    }, {
      "title" : "End-to-end people detection in crowded scenes",
      "author" : [ "Russell Stewart", "Mykhaylo Andriluka", "Andrew Y. Ng." ],
      "venue" : "2016 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2016, Las Vegas, NV, USA, June 27-30, 2016, pages 2325–2333.",
      "citeRegEx" : "Stewart et al\\.,? 2016",
      "shortCiteRegEx" : "Stewart et al\\.",
      "year" : 2016
    }, {
      "title" : "Sequence to sequence learning with neural networks",
      "author" : [ "Ilya Sutskever", "Oriol Vinyals", "Quoc V. Le." ],
      "venue" : "Advances in Neural Information Processing Systems 27: Annual Conference on Neural Information Processing Systems 2014, December 8-13 2014,",
      "citeRegEx" : "Sutskever et al\\.,? 2014",
      "shortCiteRegEx" : "Sutskever et al\\.",
      "year" : 2014
    }, {
      "title" : "Vokenization: Improving language understanding via contextualized, visually-grounded supervision",
      "author" : [ "Hao Tan", "Mohit Bansal." ],
      "venue" : "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 2066–",
      "citeRegEx" : "Tan and Bansal.,? 2020",
      "shortCiteRegEx" : "Tan and Bansal.",
      "year" : 2020
    }, {
      "title" : "Attention is all you need",
      "author" : [ "Ashish Vaswani", "Noam Shazeer", "Niki Parmar", "Jakob Uszkoreit", "Llion Jones", "Aidan N. Gomez", "Lukasz Kaiser", "Illia Polosukhin." ],
      "venue" : "Advances in Neural Information Processing Systems 30: Annual Conference on Neural",
      "citeRegEx" : "Vaswani et al\\.,? 2017",
      "shortCiteRegEx" : "Vaswani et al\\.",
      "year" : 2017
    }, {
      "title" : "A neural conversational model",
      "author" : [ "Oriol Vinyals", "Quoc Le." ],
      "venue" : "arXiv preprint arXiv:1506.05869.",
      "citeRegEx" : "Vinyals and Le.,? 2015",
      "shortCiteRegEx" : "Vinyals and Le.",
      "year" : 2015
    }, {
      "title" : "Show and tell: A neural image caption generator",
      "author" : [ "Oriol Vinyals", "Alexander Toshev", "Samy Bengio", "Dumitru Erhan." ],
      "venue" : "IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2015, Boston, MA, USA, June 7-12, 2015, pages 3156–",
      "citeRegEx" : "Vinyals et al\\.,? 2015",
      "shortCiteRegEx" : "Vinyals et al\\.",
      "year" : 2015
    }, {
      "title" : "Neural response generation with dynamic vocabularies",
      "author" : [ "Yu Wu", "Wei Wu", "Dejian Yang", "Can Xu", "Zhoujun Li." ],
      "venue" : "Proceedings of the Thirty-Second AAAI Conference on Artificial Intelligence, (AAAI18), the 30th innovative Applications of Artificial In-",
      "citeRegEx" : "Wu et al\\.,? 2018",
      "shortCiteRegEx" : "Wu et al\\.",
      "year" : 2018
    }, {
      "title" : "Aggregated residual transformations for deep neural networks",
      "author" : [ "Saining Xie", "Ross B. Girshick", "Piotr Dollár", "Zhuowen Tu", "Kaiming He." ],
      "venue" : "2017 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2017, Honolulu, HI, USA, July",
      "citeRegEx" : "Xie et al\\.,? 2017",
      "shortCiteRegEx" : "Xie et al\\.",
      "year" : 2017
    }, {
      "title" : "Neural response generation with meta-words",
      "author" : [ "Can Xu", "Wei Wu", "Chongyang Tao", "Huang Hu", "Matt Schuerman", "Ying Wang." ],
      "venue" : "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 5416–5426, Florence,",
      "citeRegEx" : "Xu et al\\.,? 2019",
      "shortCiteRegEx" : "Xu et al\\.",
      "year" : 2019
    }, {
      "title" : "Attngan: Fine-grained text to image generation with attentional generative adversarial networks",
      "author" : [ "Tao Xu", "Pengchuan Zhang", "Qiuyuan Huang", "Han Zhang", "Zhe Gan", "Xiaolei Huang", "Xiaodong He." ],
      "venue" : "2018 IEEE Conference on Computer Vision and",
      "citeRegEx" : "Xu et al\\.,? 2018",
      "shortCiteRegEx" : "Xu et al\\.",
      "year" : 2018
    }, {
      "title" : "Open domain dialogue generation with latent images",
      "author" : [ "Ze Yang", "Wei Wu", "Huang Hu", "Can Xu", "Zhoujun Li." ],
      "venue" : "arXiv preprint arXiv:2004.01981.",
      "citeRegEx" : "Yang et al\\.,? 2020",
      "shortCiteRegEx" : "Yang et al\\.",
      "year" : 2020
    }, {
      "title" : "ReCoSa: Detecting the relevant contexts with self-attention for multi-turn dialogue generation",
      "author" : [ "Hainan Zhang", "Yanyan Lan", "Liang Pang", "Jiafeng Guo", "Xueqi Cheng." ],
      "venue" : "Proceedings of the 57th Annual Meeting of the Association for Computational",
      "citeRegEx" : "Zhang et al\\.,? 2019",
      "shortCiteRegEx" : "Zhang et al\\.",
      "year" : 2019
    }, {
      "title" : "DIALOGPT : Largescale generative pre-training for conversational response generation",
      "author" : [ "Yizhe Zhang", "Siqi Sun", "Michel Galley", "Yen-Chun Chen", "Chris Brockett", "Xiang Gao", "Jianfeng Gao", "Jingjing Liu", "Bill Dolan." ],
      "venue" : "Proceedings of the 58th An-",
      "citeRegEx" : "Zhang et al\\.,? 2020",
      "shortCiteRegEx" : "Zhang et al\\.",
      "year" : 2020
    }, {
      "title" : "Low-resource knowledge-grounded dialogue generation",
      "author" : [ "Xueliang Zhao", "Wei Wu", "Chongyang Tao", "Can Xu", "Dongyan Zhao", "Rui Yan." ],
      "venue" : "8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-",
      "citeRegEx" : "Zhao et al\\.,? 2020a",
      "shortCiteRegEx" : "Zhao et al\\.",
      "year" : 2020
    }, {
      "title" : "Knowledgegrounded dialogue generation with pre-trained language models",
      "author" : [ "Xueliang Zhao", "Wei Wu", "Can Xu", "Chongyang Tao", "Dongyan Zhao", "Rui Yan." ],
      "venue" : "Proceedings of the 2020 Conference on Empirical Methods in Natural Language",
      "citeRegEx" : "Zhao et al\\.,? 2020b",
      "shortCiteRegEx" : "Zhao et al\\.",
      "year" : 2020
    }, {
      "title" : "Commonsense knowledge aware conversation generation with graph attention",
      "author" : [ "Hao Zhou", "Tom Young", "Minlie Huang", "Haizhou Zhao", "Jingfang Xu", "Xiaoyan Zhu." ],
      "venue" : "Proceedings of the TwentySeventh International Joint Conference on Artificial",
      "citeRegEx" : "Zhou et al\\.,? 2018",
      "shortCiteRegEx" : "Zhou et al\\.",
      "year" : 2018
    } ],
    "referenceMentions" : [ {
      "referenceID" : 28,
      "context" : ", 2020), Blender (Roller et al., 2020) and DialoGPT (Zhang et al.",
      "startOffset" : 17,
      "endOffset" : 38
    }, {
      "referenceID" : 46,
      "context" : ", 2020) and DialoGPT (Zhang et al., 2020), have shown the compelling performance, they are still lack of the perception ability to our physical world.",
      "startOffset" : 21,
      "endOffset" : 41
    }, {
      "referenceID" : 5,
      "context" : "A recent study (Bisk et al., 2020) points out the successful linguistic communication relies on a shared experience of the world that makes language really meaningful.",
      "startOffset" : 15,
      "endOffset" : 34
    }, {
      "referenceID" : 19,
      "context" : "Knowledge Graph1 and MS-COCO images2 (Lin et al., 2014).",
      "startOffset" : 37,
      "endOffset" : 55
    }, {
      "referenceID" : 22,
      "context" : "The existing works (Mostafazadeh et al., 2017; Huber et al., 2018; Shuster et al., 2020) focus on exploring the multimodal dialog models that ground the conversation on a given image.",
      "startOffset" : 19,
      "endOffset" : 88
    }, {
      "referenceID" : 32,
      "context" : "The existing works (Mostafazadeh et al., 2017; Huber et al., 2018; Shuster et al., 2020) focus on exploring the multimodal dialog models that ground the conversation on a given image.",
      "startOffset" : 19,
      "endOffset" : 88
    }, {
      "referenceID" : 43,
      "context" : "(2020) propose to learn the dialog generation model with both image-grounded dialogs and textual dialogs by resorting to text-to-image synthesis techniques (Xu et al., 2018; Qiao et al., 2019) to restore a latent image for the text-only dialog.",
      "startOffset" : 156,
      "endOffset" : 192
    }, {
      "referenceID" : 25,
      "context" : "(2020) propose to learn the dialog generation model with both image-grounded dialogs and textual dialogs by resorting to text-to-image synthesis techniques (Xu et al., 2018; Qiao et al., 2019) to restore a latent image for the text-only dialog.",
      "startOffset" : 156,
      "endOffset" : 192
    }, {
      "referenceID" : 37,
      "context" : "Finally, these constructed 4-tuples are used to train the visualknowledge-grounded response generator, which is built on the top of a multi-layer Transformer architecture (Vaswani et al., 2017).",
      "startOffset" : 171,
      "endOffset" : 193
    }, {
      "referenceID" : 12,
      "context" : "The experimental results on Reddit Conversation Corpus (Dziri et al., 2019a) demonstrate that Maria significantly outperforms previous state-of-the-art methods, and can generate informative responses with visual commonsense of our physical world.",
      "startOffset" : 55,
      "endOffset" : 76
    }, {
      "referenceID" : 39,
      "context" : "Vision and Language In the research of vision and language, various tasks have been extensively studied, such as image captioning (Vinyals et al., 2015; Lu et al., 2017; Hu et al., 2020), visual question answering (Antol et al.",
      "startOffset" : 130,
      "endOffset" : 186
    }, {
      "referenceID" : 21,
      "context" : "Vision and Language In the research of vision and language, various tasks have been extensively studied, such as image captioning (Vinyals et al., 2015; Lu et al., 2017; Hu et al., 2020), visual question answering (Antol et al.",
      "startOffset" : 130,
      "endOffset" : 186
    }, {
      "referenceID" : 2,
      "context" : ", 2020), visual question answering (Antol et al., 2015; Anderson et al., 2018), visual dialog (Das et al.",
      "startOffset" : 35,
      "endOffset" : 78
    }, {
      "referenceID" : 19,
      "context" : "Popular benchmark datasets in this area include MS-COCO (Lin et al., 2014), VisDial (Das et al.",
      "startOffset" : 56,
      "endOffset" : 74
    }, {
      "referenceID" : 7,
      "context" : ", 2014), VisDial (Das et al., 2017a) and Visual Genome (Krishna et al.",
      "startOffset" : 17,
      "endOffset" : 36
    }, {
      "referenceID" : 35,
      "context" : "Dialog Generation Encouraged by the success of the neural sequence-to-sequence architecture (Sutskever et al., 2014) on machine translation, end-to-end neural approaches on open-domain dialog generation (Vinyals and Le, 2015; Shang et al.",
      "startOffset" : 92,
      "endOffset" : 116
    }, {
      "referenceID" : 38,
      "context" : ", 2014) on machine translation, end-to-end neural approaches on open-domain dialog generation (Vinyals and Le, 2015; Shang et al., 2015; Serban et al., 2016; Sordoni et al., 2015; Xing et al., 2017; Wu et al., 2018; Zhang et al., 2020; Xu et al., 2019; Adiwardana et al., 2020) have been widely studied in literature.",
      "startOffset" : 94,
      "endOffset" : 277
    }, {
      "referenceID" : 31,
      "context" : ", 2014) on machine translation, end-to-end neural approaches on open-domain dialog generation (Vinyals and Le, 2015; Shang et al., 2015; Serban et al., 2016; Sordoni et al., 2015; Xing et al., 2017; Wu et al., 2018; Zhang et al., 2020; Xu et al., 2019; Adiwardana et al., 2020) have been widely studied in literature.",
      "startOffset" : 94,
      "endOffset" : 277
    }, {
      "referenceID" : 29,
      "context" : ", 2014) on machine translation, end-to-end neural approaches on open-domain dialog generation (Vinyals and Le, 2015; Shang et al., 2015; Serban et al., 2016; Sordoni et al., 2015; Xing et al., 2017; Wu et al., 2018; Zhang et al., 2020; Xu et al., 2019; Adiwardana et al., 2020) have been widely studied in literature.",
      "startOffset" : 94,
      "endOffset" : 277
    }, {
      "referenceID" : 33,
      "context" : ", 2014) on machine translation, end-to-end neural approaches on open-domain dialog generation (Vinyals and Le, 2015; Shang et al., 2015; Serban et al., 2016; Sordoni et al., 2015; Xing et al., 2017; Wu et al., 2018; Zhang et al., 2020; Xu et al., 2019; Adiwardana et al., 2020) have been widely studied in literature.",
      "startOffset" : 94,
      "endOffset" : 277
    }, {
      "referenceID" : 40,
      "context" : ", 2014) on machine translation, end-to-end neural approaches on open-domain dialog generation (Vinyals and Le, 2015; Shang et al., 2015; Serban et al., 2016; Sordoni et al., 2015; Xing et al., 2017; Wu et al., 2018; Zhang et al., 2020; Xu et al., 2019; Adiwardana et al., 2020) have been widely studied in literature.",
      "startOffset" : 94,
      "endOffset" : 277
    }, {
      "referenceID" : 46,
      "context" : ", 2014) on machine translation, end-to-end neural approaches on open-domain dialog generation (Vinyals and Le, 2015; Shang et al., 2015; Serban et al., 2016; Sordoni et al., 2015; Xing et al., 2017; Wu et al., 2018; Zhang et al., 2020; Xu et al., 2019; Adiwardana et al., 2020) have been widely studied in literature.",
      "startOffset" : 94,
      "endOffset" : 277
    }, {
      "referenceID" : 42,
      "context" : ", 2014) on machine translation, end-to-end neural approaches on open-domain dialog generation (Vinyals and Le, 2015; Shang et al., 2015; Serban et al., 2016; Sordoni et al., 2015; Xing et al., 2017; Wu et al., 2018; Zhang et al., 2020; Xu et al., 2019; Adiwardana et al., 2020) have been widely studied in literature.",
      "startOffset" : 94,
      "endOffset" : 277
    }, {
      "referenceID" : 0,
      "context" : ", 2014) on machine translation, end-to-end neural approaches on open-domain dialog generation (Vinyals and Le, 2015; Shang et al., 2015; Serban et al., 2016; Sordoni et al., 2015; Xing et al., 2017; Wu et al., 2018; Zhang et al., 2020; Xu et al., 2019; Adiwardana et al., 2020) have been widely studied in literature.",
      "startOffset" : 94,
      "endOffset" : 277
    }, {
      "referenceID" : 49,
      "context" : "Recently, there is an emerging trend towards grounding the dialog generation models on the external knowledge, such as knowledge graphs (Zhou et al., 2018), documents (Ghazvininejad et al.",
      "startOffset" : 136,
      "endOffset" : 155
    }, {
      "referenceID" : 10,
      "context" : ", 2018), documents (Ghazvininejad et al., 2018; Dinan et al., 2019; Kim et al., 2020; Zhao et al., 2020a,b; Li et al., 2020) and images (Mostafazadeh et al.",
      "startOffset" : 19,
      "endOffset" : 124
    }, {
      "referenceID" : 17,
      "context" : ", 2018), documents (Ghazvininejad et al., 2018; Dinan et al., 2019; Kim et al., 2020; Zhao et al., 2020a,b; Li et al., 2020) and images (Mostafazadeh et al.",
      "startOffset" : 19,
      "endOffset" : 124
    }, {
      "referenceID" : 17,
      "context" : "Different from the previous work on knowledge-grounded conversation that connects dialogs with unpaired document knowledge (Li et al., 2020), our work lies in the research of image-grounded conversation where a response is generated with a dialog context and a given image.",
      "startOffset" : 123,
      "endOffset" : 140
    }, {
      "referenceID" : 22,
      "context" : "Existing works (Mostafazadeh et al., 2017; Shuster et al., 2020; Yang et al., 2020) in this direction assume there is a given (or synthesized) image for the dialog and explore the multimodal dialog models.",
      "startOffset" : 15,
      "endOffset" : 83
    }, {
      "referenceID" : 32,
      "context" : "Existing works (Mostafazadeh et al., 2017; Shuster et al., 2020; Yang et al., 2020) in this direction assume there is a given (or synthesized) image for the dialog and explore the multimodal dialog models.",
      "startOffset" : 15,
      "endOffset" : 83
    }, {
      "referenceID" : 44,
      "context" : "Existing works (Mostafazadeh et al., 2017; Shuster et al., 2020; Yang et al., 2020) in this direction assume there is a given (or synthesized) image for the dialog and explore the multimodal dialog models.",
      "startOffset" : 15,
      "endOffset" : 83
    }, {
      "referenceID" : 20,
      "context" : "Modeling To improve the efficiency of crossmodal retrieval model on large-scale dialog corpus and image dataset, we adopt a two-tower architecture (Lu et al., 2019) to accelerate the retrieval process where the image features can be pre-extracted offline.",
      "startOffset" : 147,
      "endOffset" : 164
    }, {
      "referenceID" : 9,
      "context" : "The text encoder is a pre-trained BERT-base model (Devlin et al., 2019) and we use the hidden state of special token [CLS] as the embedding of T :",
      "startOffset" : 50,
      "endOffset" : 71
    }, {
      "referenceID" : 23,
      "context" : "We follow Tan and Bansal (2020) to perform L2normalization on the last output features, by which we can simplify the nearest neighbor search problem in the euclidean space to the Maximum Inner Product problem (Mussmann and Ermon, 2016):",
      "startOffset" : 209,
      "endOffset" : 235
    }, {
      "referenceID" : 41,
      "context" : "Similarly, the image encoder is composed of a pretrained ResNeXt backbone (Xie et al., 2017) and a MLP with L2 normalization:",
      "startOffset" : 74,
      "endOffset" : 92
    }, {
      "referenceID" : 19,
      "context" : "Training We train the cross-modal matching model on MS-COCO image captioning dataset (Lin et al., 2014), where each image is paired with 5 sentences describing its visual content.",
      "startOffset" : 85,
      "endOffset" : 103
    }, {
      "referenceID" : 11,
      "context" : "Figure 4 shows the architecture of our response generation model, which is a multi-layer transformer network for both bidirectional vision/context (O,Q,C) encoding, and unidirectional response R decoding, via the flexible self-attention masks inspired by (Dong et al., 2019).",
      "startOffset" : 255,
      "endOffset" : 274
    }, {
      "referenceID" : 4,
      "context" : "Turn-Level Since the dialog is multi-turn, we encode this turn order with a relative turn embedding (Bao et al., 2020).",
      "startOffset" : 100,
      "endOffset" : 118
    }, {
      "referenceID" : 37,
      "context" : "Position-Level Positional embedding encodes the signal of the token order in the total input sequence, which is the same as positional encoding of the original transformer (Vaswani et al., 2017).",
      "startOffset" : 172,
      "endOffset" : 194
    }, {
      "referenceID" : 34,
      "context" : "(2020), we adopt the Hungarian Matching Loss (Stewart et al., 2016; Carion et al., 2020) to estimate an optimal mapping α so that the prediction for each masked position is assigned one of the target concepts.",
      "startOffset" : 45,
      "endOffset" : 88
    }, {
      "referenceID" : 6,
      "context" : "(2020), we adopt the Hungarian Matching Loss (Stewart et al., 2016; Carion et al., 2020) to estimate an optimal mapping α so that the prediction for each masked position is assigned one of the target concepts.",
      "startOffset" : 45,
      "endOffset" : 88
    }, {
      "referenceID" : 11,
      "context" : "Encouraged by the success of UniLM (Dong et al., 2019) in Seq2Seq tasks, we adopt the Masked Response Prediction (MRP) objective to model the response generation.",
      "startOffset" : 35,
      "endOffset" : 54
    }, {
      "referenceID" : 13,
      "context" : "(2020), which is a large-scale and high-quality multi-turn conversations extracted from Reddit Conversation Corpus (Dziri et al., 2019b).",
      "startOffset" : 115,
      "endOffset" : 136
    }, {
      "referenceID" : 24,
      "context" : "Automatic metrics include: (1) Fluency: perplexity (PPL) measures the confidence of the generated responses; (2) Relevance: BLEU-1 (Papineni et al., 2002), Rouge-L (Lin, 2004), and we follow Serban et al.",
      "startOffset" : 131,
      "endOffset" : 154
    }, {
      "referenceID" : 18,
      "context" : ", 2002), Rouge-L (Lin, 2004), and we follow Serban et al.",
      "startOffset" : 17,
      "endOffset" : 28
    }, {
      "referenceID" : 14,
      "context" : "The inter-annotator agreement is measured by Fleiss’ Kappa(Fleiss and Cohen, 1973).",
      "startOffset" : 58,
      "endOffset" : 82
    }, {
      "referenceID" : 27,
      "context" : "The detector is a Faster R-CNN (Ren et al., 2015) model trained on the Visual Genome dataset (Krishna et al.",
      "startOffset" : 31,
      "endOffset" : 49
    }, {
      "referenceID" : 3,
      "context" : "We compare the following baselines in the experiments: (1) Seq2Seq: A standard Sequence to Seqence model with attention mechanism (Bahdanau et al., 2015).",
      "startOffset" : 130,
      "endOffset" : 153
    }, {
      "referenceID" : 29,
      "context" : "(2) HRED: A Hierarchical Recurrent Encoder-Decoder neural network (Serban et al., 2016).",
      "startOffset" : 66,
      "endOffset" : 87
    }, {
      "referenceID" : 30,
      "context" : "(3) VHRED: A variation of HRED that introduces latent variables into the generation (Serban et al., 2017).",
      "startOffset" : 84,
      "endOffset" : 105
    }, {
      "referenceID" : 45,
      "context" : "(4) ReCoSa: A hierarchical transformer-based model (Zhang et al., 2019) that achieves the state-of-the-art performance on benchmarks of dialog generation.",
      "startOffset" : 51,
      "endOffset" : 71
    }, {
      "referenceID" : 44,
      "context" : "(5) ImgVAE: A dialog generation model (Yang et al., 2020) that is trained on both textual dialogs and image-grounded dialogs by recovering a latent image behind the textual dialog within a conditional variational autoencoding framework.",
      "startOffset" : 38,
      "endOffset" : 57
    }, {
      "referenceID" : 46,
      "context" : "(6) DialoGPT: An opendomain dialog model (Zhang et al., 2020) that fine-tunes GPT-2 (Radford et al.",
      "startOffset" : 41,
      "endOffset" : 61
    }, {
      "referenceID" : 26,
      "context" : ", 2020) that fine-tunes GPT-2 (Radford et al., 2019) on massive Reddit data.",
      "startOffset" : 30,
      "endOffset" : 52
    }, {
      "referenceID" : 46,
      "context" : "Numbers with underline refer to the best results except for the comparison to DialoGPT (Zhang et al., 2020).",
      "startOffset" : 87,
      "endOffset" : 107
    }, {
      "referenceID" : 32,
      "context" : ", ImageChat (Shuster et al., 2020) dataset) and test data (i.",
      "startOffset" : 12,
      "endOffset" : 34
    } ],
    "year" : 2021,
    "abstractText" : "Arguably, the visual perception of conversational agents to the physical world is a key way for them to exhibit the human-like intelligence. Image-grounded conversation is thus proposed to address this challenge. Existing works focus on exploring the multimodal dialog models that ground the conversation on a given image. In this paper, we take a step further to study image-grounded conversation under a fully open-ended setting where no paired dialog and image are assumed available. Specifically, we present Maria, a neural conversation agent powered by the visual world experiences which are retrieved from a large-scale image index. Maria consists of three flexible components, i.e., text-to-image retriever, visual concept detector and visual-knowledge-grounded response generator. The retriever aims to retrieve a correlated image to the dialog from an image index, while the visual concept detector extracts rich visual knowledge from the image. Then, the response generator is grounded on the extracted visual knowledge and dialog context to generate the target response. Extensive experiments demonstrate Maria outperforms previous state-of-the-art methods on automatic metrics and human evaluation, and can generate informative responses that have some visual commonsense of the physical world.",
    "creator" : "LaTeX with hyperref"
  }
}