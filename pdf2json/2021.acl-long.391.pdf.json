{
  "name" : "2021.acl-long.391.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "Semi-Supervised Text Classification with Balanced Deep Representation Distributions",
    "authors" : [ "Changchun Li", "Ximing Li", "Jihong Ouyang" ],
    "emails" : [ "changchunli93@gmail.com,", "liximing86@gmail.com,", "ouyj@jlu.edu.cn" ],
    "sections" : [ {
      "heading" : null,
      "text" : "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 5044–5053\nAugust 1–6, 2021. ©2021 Association for Computational Linguistics\n5044"
    }, {
      "heading" : "1 Introduction",
      "text" : "Semi-Supervised Learning (SSL) refers to the paradigm of learning with labeled as well as unlabeled data to perform certain applications (van Engelen and Hoos, 2020). Especially, developing effective SSL models for classifying text data has long been a goal for the studies of natural language processing, because labeled texts are difficult to collect in many real-world scenarios. Formally, this ∗ Contributing equally with the first author. † Corresponding author.\nresearch topic is termed as Semi-Supervised Text Classification (SSTC), which nowadays draws much attention from the community (Clark et al., 2018; Gururangan et al., 2019; Chen et al., 2020).\nTo our knowledge, the most recent SSTC methods mainly borrow ideas from the successful patterns of supervised deep learning, such as pretraining and fine-tuning (Dai and Le, 2015; Howard and Ruder, 2018; Peters et al., 2018; Gururangan et al., 2019; Devlin et al., 2019). Generally, those methods perform deep representation learning on unlabeled texts followed by supervised learning on labeled texts. However, a drawback is that they separately learn from the labeled and unlabeled texts, where, specifically, the deep representations are trained without using the labeling information, resulting in potentially less discriminative representations as well as worse performance.\nTo avoid this problem, other SSTC methods combine the traditional spirit of self-training with deep learning, which jointly learn the deep representation and classifier using both labeled and unlabeled texts in a unified framework (Miyato et al., 2017, 2019; Sachan et al., 2019; Xie et al., 2020; Chen\net al., 2020). To be specific, this kind of methods initializes a deep classifier, e.g., BERT (Devlin et al., 2019) with Angular Margin (AM) loss (Wang et al., 2018), by training over labeled texts only; and then it alternatively predicts unlabeled texts as their pseudo-labels and trains the deep classifier over the mixture of labeled and pseudo-labeled texts. Accordingly, both labeled and unlabeled texts can directly contribute to the deep classifier training.\nGenerally speaking, for deep self-training methods, one significant factor of performance is the accuracy of pseudo-labels for unlabeled texts. Unfortunately, they often suffer from low accuracy, where one major reason is the margin bias problem. To interpret this problem, we look around the AM loss with respect to the label angle, i.e., the angles between deep representations of texts and weight vectors of labels. For unlabeled texts, the pseudo-labels are predicted by only ranking the label angles, but neglecting the difference between label angle variances, i.e., the variance of label angles of texts within the same label, which might be much large in SSL as illustrated in Fig.1. In this context, the boundary of AM loss is actually not the optimal one, potentially resulting in lower accuracy for pseudo-labels (see Fig.2(a)).\nTo alleviate the aforementioned problem, we propose a novel SSTC method built on BERT with AM loss, namely Semi-Supervised Text Classification with Balanced Deep representation Distributions (S2TC-BDD). Most specifically, in S2TC-BDD, we suppose that the label angles are drawn from each label-specific Gaussian distribution. Therefore, for each text we can apply linear transformation operations to balance the label angle variances. This is equivalent to moving the boundary to the optimal one, so as to eliminate the margin bias (see examples in Fig.2(b)). We can estimate each label angle variance over both labeled and pseudo-labeled texts during the self-training loops. We evaluate the proposed S2TC-BDD method by comparing the most recent deep SSTC methods. Experimental results indicate the superior performance of S2TC-BDD even with very few labeled texts."
    }, {
      "heading" : "2 Related Work",
      "text" : "The pre-training and fine-tuning framework has lately shown impressive effectiveness on a variety of tasks (Dai and Le, 2015; Radford et al., 2019a; Howard and Ruder, 2018; Peters et al., 2018; De-\nvlin et al., 2019; Yang et al., 2019; Chen et al., 2019; Akbik et al., 2019; Radford et al., 2019b; Brown et al., 2020; Chen et al., 2020). They mainly perform deep representation learning on generic data, followed by supervised learning for downstream tasks. Several SSTC methods are built on this framework (Dai and Le, 2015; Howard and Ruder, 2018; Peters et al., 2018; Gururangan et al., 2019; Devlin et al., 2019). For instance, the VAriational Methods for Pretraining In Resourcelimited Environments (VAMPIRE) (Gururangan et al., 2019) first pre-trains a Variational AutoEncoder (VAE) model on unlabeled texts, and then trains a classifier on the augmentation representations of labeled texts computed by the pre-trained VAE. However, the VAE model is trained without using the labeling information, resulting in potentially less discriminative representations for labeled texts.\nRecent works on SSTC mainly focus on deep self-training (Miyato et al., 2017; Clark et al., 2018; Sachan et al., 2019; Miyato et al., 2019; Xie et al., 2020; Chen et al., 2020), which can jointly learn deep representation and classifier using both labeled and unlabeled texts in a unified framework. It is implemented by performing an alternative process, in which the pseudo-labels of unlabeled texts are updated by the current deep classifier, and then the deep classifier is retrained over both labeled and pseudo-labeled texts. For example, the Virtual Adversarial Training (VAT) method (Miyato et al., 2017, 2019) follows the philosophy of making the classifier robust against random and local perturbation. It first generates the predictions of original texts with the current deep classifier and then trains the deep classifier by utilizing a consistency loss\nbetween the original predictions and the outputs of deep classifier over noise texts by applying local perturbations to the embeddings of original texts. Further, the work in (Sachan et al., 2019) combines maximum likelihood, adversarial training, virtual adversarial training, and entropy minimization in a unified objective. Furthermore, rather than applying local perturbations, Unsupervised Data Augmentation (UDA) (Xie et al., 2020) employs consistency loss between the predictions of unlabeled texts and corresponding augmented texts by data augmentation techniques such as back translations and tf-idf word replacements. The work (Clark et al., 2018) exploits cross-view training by matching the predictions of auxiliary prediction modules over the restricted views of unlabeled texts (e.g., only part of sentence) with ones of primary prediction module over the corresponding full views.\nOrthogonal to the aforementioned self-training SSTC methods, our S2TC-BDD further considers the margin bias problem by balancing the label angle variances. This is beneficial for more accurate pseudo-labels for unlabeled texts, so as to boost the performance of SSTC tasks.\n3 The Proposed S2TC-BDD Method\nIn this section, we describe the proposed deep selftraining SSTC method, namely Semi-Supervised Text Classification with Balanced Deep representation Distributions (S2TC-BDD).\nFormulation of SSTC Consider a training dataset D consisting of a limited labeled text set Dl = {(xli,yli)} i=Nl i=1 and a large unlabeled text set Du = {xuj } j=Nu j=1 . Specifically, let x l i and x u j denote the word sequences of labeled and unlabeled texts, respectively; and let yli ∈ {0, 1}K denote the corresponding one-hot label vector of xli, where ylik = 1 if the text is associated with the k-th label, or ylik = 0 otherwise. We declare that Nl, Nu, and K denote the numbers of labeled texts, unlabeled texts and category labels, respectively. In this paper, we focus on the paradigm of inductive SSTC, whose goal is to learn a classifier from the training dataset D with both labeled and unlabeled texts. The important notations are described in Table 1.\n3.1 Overview of S2TC-BDD Overall speaking, our S2TC-BDD performs a selftraining procedure for SSTC. Given a training dataset, it first trains a fine-tuned deep classifier based on the pre-trained BERT model (Devlin et al.,\n2019) with AM loss (Wang et al., 2018). During the self-training loops, we employ the current deep classifier to predict unlabeled texts as pseudolabels, and then update it over both labeled and pseudo-labeled texts. In particular, we develop a Balanced Deep representation Distribution (BDD) loss, aiming at more accurate pseudo-labels for unlabeled texts. The overall framework of S2TC-BDD is shown in Fig.3. We now present the important details of S2TC-BDD.\nBDD Loss Formally, our BDD loss is extended from the AM loss (Wang et al., 2018). For clarity, we first describe the AM loss with respect to angles. Given a training example (xi,yi), it can be formulated below:\nLam(xi,yi;φ) =\n− K∑ k=1 yik log es(cos(θik)−yikm)∑K j=1 e s(cos(θij)−yijm) , (1)\nwhere φ denotes the model parameters,\ncos(θik) = f>i Wk\n‖fi‖2‖Wk‖2 ,\n‖·‖2 is the `2-norm of vectors; fi and Wk denote the deep representation of text xi and the weight vector of label k, respectively; θik is the angle between fi and Wk; s and m are the parameters used to control the rescaled norm and magnitude of cosine margin, respectively.\nReviewing Eq.1, we observe that it directly measures the loss by label angles of texts only. We kindly argue that it corresponds to nonoptimal decision boundary in SSTC, where the difference between label angle variances is much larger than supervised learning. To alleviate this problem, we suppose that the label angles are drawn from each label-specific Gaussian distri-\nbution {N (µk, σ2k)}k=Kk=1 . Thanks to the properties of Gaussian distribution, we can easily transfer them into the ones with balanced variances {N (µk, σ̂2)}k=Kk=1 , σ̂2 = ∑K k=1 σ 2 k\nK by performing the following linear transformations to the angles:\nψk(θik) = akθik + bk, ∀k ∈ [K], (2)\nwhere\nak = σ̂\nσk , bk = (1− ak)µk. (3)\nWith these linear transformations {ψk(·)}k=Kk=1 , all angles become the samples from balanced angular distributions with the same variances, e.g., ψk(θik) ∼ N (µk, σ̂2). Accordingly, the angular loss of Eq.1 can be rewritten as the following BDD loss:\nLbdd(xi,yi;φ) =\n− K∑ k=1 yik log es(cos(ψk(θik))−yikm)∑K j=1 e s(cos(ψj(θij))−yijm) .\n(4)\nSupervised Angular Loss Applying the BDD loss Lbdd of Eq.4 to the labeled text set Dl = {(xli,yli)} i=Nl i=1 , we can formulate the following supervised angular loss:\nLl(Dl;φ) = 1\nNl Nl∑ i=1 Lbdd(x l i,y l i;φ). (5)\nUnsupervised Angular Loss Under the selftraining paradigm, we form the loss with unlabeled texts and pseudo-labels. Specifically, we\ndenote the pseudo-label as the output probability of the deep classifier. It is computed by normalizing {cos(ψk(θik))}k=Kk=1 with the softmax function: p(k|xi,φ) = ecos(ψk(θik))∑K j=1 e cos(ψj(θij)) , yi, ∀k ∈ [K].\nFor each unlabeled text xui the pseudo-label distribution is given by p(k|xui , φ̃) , yui with the fixed copy φ̃ of the current model parameter φ during self-training loops. Besides, to avoid those pseudolabel distributions {yui } Nu i=1 too uniform, we employ a sharpen function with a temperature T over them:\nyui = Sharpen(y u i , T ) =\n(yui ) 1/T\n‖(yui )1/T ‖1 ,∀i ∈ [Nu],\nwhere ‖·‖1 is the `1-norm of vectors. When T → 0, the pseudo-label distribution tends to be the onehot vector.\nApplying the BDD loss of Eq.4 to the unlabeled text set Du = {xuj } j=Nu j=1 and pseudo-label distributions {yui } Nu i=1, we can formulate the following unsupervised angular loss:\nLu(Du, {yui }Nui=1;φ) = 1\nNu Nu∑ i=1 Lbdd(x u i ,y u i ;φ).\n(6)\nEntropy Regularization Further, we employ the conditional entropy of p(y|xi,φ) as an additional regularization term:\nR(Dl,Du;φ) =\n− 1 Nl +Nu ∑ xi∈Dl,Du K∑ k=1 p(k|xi,φ) log p(k|xi,φ).\n(7)\nThis conditional entropy regularization is introduced by (Grandvalet and Bengio, 2004), and also utilized in (Sajjadi et al., 2016; Miyato et al., 2019; Sachan et al., 2019). It also sharpens the output probability of the deep classifier.\nFull Objective of S2TC-BDD Combining the supervised angular loss Eq.(5), unsupervised angular loss Eq.(6), and entropy regularization Eq.(7), the full objective of S2TC-BDD can be formulated below:\nL(Dl,Du;φ) = Ll(Dl;φ) + λ1Lu(Du, {yui } Nu i=1;φ) + λ2R(Dl,Du;φ),\n(8)\nwhere λ1 and λ2 are regularization parameters."
    }, {
      "heading" : "3.2 Implementations of Label Angle Variances",
      "text" : "In this section, we describe implementations of label angle variances. As mentioned before, what we concern is the estimations of angular distributions {N (µk, σ2k)}k=Kk=1 , where their draws are the angles between deep representations of texts and label prototypes denoted by {ck}k=Kk=1 . Both {(µk, σ2k)}k=Kk=1 and {ck}k=Kk=1 are estimated over both labeled and pseudo-labeled texts during self-training loops. In the following, we describe their learning processes in more detail.\nWithin the framework of stochastic optimization, we update the {(µk, σ2k)}k=Kk=1 and {ck}k=Kk=1 perepoch. For convenience, we denote Ω as the index set of labeled and unlabeled texts in one epoch, {fi}i∈Ω and {yi}i∈Ω as the deep representations of texts and corresponding label or pseudo-label vectors (i.e., yli or y u i ) in the current epoch, respectively.\nEstimating Label Prototypes Given the current {fi}i∈Ω and {yi}i∈Ω, we calculate the label prototypes {ck}k=Kk=1 by the weighted average of {fi}i∈Ω, formulated below:\nck = ∑ i∈Ω yikfi∑ i∈Ω yik , ∀k ∈ [K]. (9)\nTo avoid the misleading affect of some mislabeled texts, inspired by (Liu et al., 2020), we update {ck}k=Kk=1 by employing the moving average with a learning rate γ:\nc (t) k ← (1− γ)c (t) k + γc (t−1) k .\nEstimating Label Angle Variances Given {fi}i∈Ω and {ck}k=Kk=1 , the angles between them can be calculated by:\nβik = arccos ( f>i ck ‖fi‖2‖ck‖2 ) , ∀i ∈ Ω, k ∈ [K].\n(10) Accordingly, we can compute the estimations of {µk}k=Kk=1 and {σ2k}k=Kk=1 as follows:\nµk = ∑ i∈Ω yikβik∑ i∈Ω yik , (11)\nσ2k =\n∑ i∈Ω yik(βik − µk)2∑\ni∈Ω yik − 1 . (12)\nFurther, the moving average is also used to the updates below:\nµ (t) k ← (1− γ)µ (t) k + γµ (t−1) k ,\n(σ2k) (t) ← (1− γ)(σ2k)(t) + γ(σ2k)(t−1)."
    }, {
      "heading" : "4 Experiment",
      "text" : ""
    }, {
      "heading" : "4.1 Experimental Settings",
      "text" : "Datasets To conduct the experiments, we employ three widely used benchmark datasets for text classification: AG News (Zhang et al., 2015), Yelp (Zhang et al., 2015), and Yahoo (Chang et al., 2008). For all datasets, we form the unlabeled training set Du, labeled training set Dl and development set by randomly drawing from the corresponding original training datasets, and utilize the original test sets for prediction evaluation. The dataset statistics and split information are described in Table 2.\nBaseline Models To evaluate the effectiveness of S2TC-BDD, we choose five existing SSTC algorithms for comparison. The details of baseline methods are given below.\n• NB+EM (Nigam et al., 2000): A semisupervised text classification method combining a Naive Bayes classifier (NB) and Expectation-Maximization (EM). In experiments, we pre-process texts following (Gururangan et al., 2019) and use tf-idfs as the representations of texts.\n• BERT (Devlin et al., 2019): A supervised text classification method built on the pre-trained BERT-based-uncased model1 and fine-tuned with the supervised softmax loss on labeled texts."
    }, {
      "heading" : "1 https://pypi.org/project/",
      "text" : "pytorch-transformers/\n• BERT+AM: A semi-supervised text classification method built on the pre-trained BERTbased-uncased1 and fine-tuned following the self-training spirit with the AM loss on both labeled and unlabeled texts.\n• VAMPIRE (Gururangan et al., 2019): A semi-supervised text classification method based on variational pre-training. The code is available on the net.2 In experiments, the default parameters are utilized.\n• VAT (Miyato et al., 2019): A semi-supervised text classification method based on virtual adversarial training. [parameter configuration: perturbation size = 5.0, regularization coefficient α = 1.0, hyperparameter for finite difference ξ = 0.1]\n• UDA (Xie et al., 2020): A semi-supervised text classification method based on unsupervised data augmentation with back translation. The code is available on the net.3 In experiments, we utilize the default parameters, and generate the augmented unlabeled data by using FairSeq4 with German as the intermediate language.\nFor S2TC-BDD, BERT, BERT+AM, VAT and UDA, we utilize BERT-based-uncased tokenizer to tokenize texts; average pooling over BERT-baseduncased model as text encoder to encode texts; and a two-layer MLP, whose hidden size and activation function are 128 and tanh respectively, as the classifier to predict labels. We set the max sentence length as 256 and remain the first 256 tokens for texts exceeding the length limit. For optimization, we utilize the Adam optimizer with learning rates of 5e-6 for BERT encoder and 1e-3 for MLP classifier. For BERT, we set the batch size of labeled tests as 8. For S2TC-BDD, BERT+AM, VAT and UDA, the batch sizes of labeled and unlabeled tests are 4 and 8, respectively. For all datasets, we iterate 20 epochs, where each one contains 200 inner loops. All experiments are carried on a Linux server with two NVIDIA GeForce RTX 2080Ti GPUs, Intel Xeon E5-2640 v4 CPU and 64G memory.\nParameter Settings For S2TC-BDD, in our experiments, its parameters are mostly set as: λ1 ="
    }, {
      "heading" : "2 https://github.com/allenai/vampire",
      "text" : "3 https://github.com/google-research/uda 4 https://github.com/pytorch/fairseq\n1.0, λ2 = 1.0, s = 1.0, m = 0.01. Specifically, for Yelp we set m = 0.3. For the sharpening temperature T , we set 0.5 for AG News and Yahoo, 0.3 for Yelp. The learning rate γ of label prototypes and label angle variances is set to 0.1.\nMetrics We utilize two metrics of Micro-F1 and Macro-F1, which are two different types of the averaged F1 scores. In experiments, we employ the implementation of Micro-F1 and Macro-F1 in the public Scikit-Learn (Pedregosa et al., 2011) tool.5"
    }, {
      "heading" : "4.2 Results",
      "text" : "For all datasets, we perform each method with five random seeds, and report the average scores."
    }, {
      "heading" : "4.2.1 Varying Number of Labeled Texts",
      "text" : "We first evaluate the classification performance of S2TC-BDD with different amounts of labeled texts. For all methods, we conduct the experiments by varying the number of labeled texts Nl over the set {100, 1000, 10000} with the number of unlabeled texts Nu = 20000 for AG News and Yelp, and Nu = 40000 for Yahoo. The classification results of both Micro-F1 and Macro-F1 over all datasets are shown in Table 3, in which the best scores among all comparing baselines are highlighted in boldface. Generally speaking, our proposed S2TC-BDD outperforms the baselines in most cases. Across all datasets and evaluation metrics, S2TC-BDD ranks 1.1 in average. Several observations are made below.\n• Comparing S2TC-BDD against baselines: First, we can observe that S2TC-BDD consistently dominates the pre-training methods (including BERT and VAMPIRE) on both MicroF1 and Macro-F1 scores by a big margin, especially when labeled texts are scarce. For example, when Nl = 100, the Macro-F1 scores\n5 https://scikit-learn.org/stable/\nTable 3: Experimental results of Micro-F1 and Macro-F1 varying the number of labeled texts Nl. The best results are highlighted in boldface.\nMetric Dataset Nl NB+EM BERT BERT+AM VAMPIRE VAT UDA S2TC-BDD\nMicro-F1\nAG News 100 0.834 0.839 0.856 0.705 0.868 0.855 0.872\n1,000 0.855 0.878 0.879 0.833 0.886 0.883 0.889 10,000 0.874 0.905 0.901 0.876 0.898 0.906 0.907\nYelp 100 0.300 0.344 0.399 0.227 0.244 0.387 0.417\n1,000 0.355 0.538 0.544 0.476 0.551 0.554 0.552 10,000 0.404 0.583 0.574 0.551 0.566 0.580 0.583\nYahoo 100 0.529 0.564 0.589 0.389 0.534 0.576 0.618\n1,000 0.624 0.676 0.679 0.547 0.685 0.672 0.687 10,000 0.659 0.713 0.706 0.644 0.701 0.707 0.713\nMacro-F1\nAG News 100 0.833 0.840 0.856 0.698 0.867 0.855 0.872\n1,000 0.855 0.878 0.879 0.833 0.886 0.883 0.889 10,000 0.873 0.905 0.900 0.876 0.897 0.906 0.907\nYelp 100 0.250 0.324 0.371 0.144 0.197 0.357 0.403\n1,000 0.329 0.532 0.535 0.476 0.548 0.550 0.550 10,000 0.397 0.586 0.562 0.553 0.569 0.576 0.586\nYahoo 100 0.489 0.550 0.573 0.356 0.542 0.567 0.595\n1,000 0.616 0.671 0.672 0.545 0.675 0.666 0.680 10,000 0.653 0.708 0.695 0.644 0.697 0.704 0.709\nAverage Rank 6.2 3.6 3.4 6.7 3.8 3.0 1.1\nof S2TC-BDD are even about 0.17, 0.26 and 0.24 higher than VAMPIRE on the datasets of AG News, Yelp and Yahoo, respectively. Second, when labeled texts are very scarce (i.e., when Nl = 100), S2TC-BDD performs better than other self-training baseline methods (i.e., NB+EM, BERT+AM, VAT and UDA) on all datasets, e.g., for Micro-F1 about 0.08 higher than VAT on Yahoo. Otherwise, when labeled texts are large, S2TC-BDD can also achieve the competitive performance, even perform better across all datasets.\n• Comparing S2TC-BDD against BERT+AM and BERT: Our S2TC-BDD method consistently outperforms BERT+AM and BERT across all datasets and metrics. For example, when Nl = 100 the Micro-F1 scores of S2TCBDD beat those of BERT+AM by 0.01 ∼ 0.03 and those of BERT by 0.03 ∼ 0.05 across all datasets. That is because S2TC-BDD employs both labeled and unlabeled texts for training and can predict more accurate pseudo-labels of unlabeled texts than BERT+AM, benefiting for the classifier training. This result is expected since S2TC-BDD performs a Gaussian linear transformation to balance the label angel variances, so as to eliminate the margin bias, leading to more accurate predicted pseudo-labels of unlabeled texts. Besides, these results empirically prove that unlabeled\ntexts are beneficial to the classification performance.\n• Comparing BERT based methods against NB+EM and VAMPIRE: All BERT based methods (i.e., BERT, BERT+AM, VAT, UDA and S2TC-BDD) consistently dominate baselines based on small models (i.e., NB+EM, VAMPIRE). For example, when Nl = 10000, the Micro-F1 and Macro-F1 scores of BERT are about 0.03, 0.18 and 0.05 higher than those of NB+EM on the datasets of AG News, Yelp and Yahoo, respectively. The observation is expected because BERT is a bigger model, hence can extract more discriminative representations of texts than those from the VAE model used in VAMPIRE and tf-idfs used in NB+EM."
    }, {
      "heading" : "4.2.2 Varying Number of Unlabeled Texts",
      "text" : "For NB+EM, BERT+AM, VAMPIRE, VAT, UDA and S2TC-BDD, we also perform the experiments with 100 labeled texts and varying the number of unlabeled texts Nu over the set {0, 200, 2000, 20000} for AG News and Yelp, and {0, 400, 4000, 40000} for Yahoo. Note that VAMPIRE needs unlabeled texts for pre-training, thus we omit the experiments for VAMPIRE with Nu = 0. The classification results are reported in Table 4. Roughly, for all methods the classification\nperformance becomes better as the amount of unlabeled texts increasing. For instance, the MicroF1 scores of S2TC-BDD on all datasets gain about 0.3 improvement as the number of unlabeled texts increasing. These results prove the effectiveness of unlabeled texts in riching the limited supervision from scarce labeled texts and improving the classification performance. Besides, an obvious observation is that the self-training methods (i.e., NB+EM, BERT+AM, VAT, UDA and S2TC-BDD) consistently outperform the pre-training method (i.e., VAMPIRE), especially when unlabeled texts are fewer. The possible reason is that the pretraining methods need more unlabeled texts for pre-training while the self-training methods do not have the requirement."
    }, {
      "heading" : "4.3 Ablation Study",
      "text" : "We perform ablation studies by stripping each component each time to examine the effectiveness of each component in S2TC-BDD. Here, we denote BDD as balanced deep representation angular loss Lbdd in Eq.4. Stripping BDD means that we replace the proposed loss Lbdd with the AM loss Lam in Eq.1. The results are displayed in Table 5. Overall, the classification performance will drop when removing any component of S2TC-BDD, suggesting that all parts make contributions to the final performance of S2TC-BDD. Besides, removing unlabeled texts brings the most significant drop of the performance. This result is expected because label angle variances approximated only with very scarce labeled texts will have lower accuracy, resulting in worse performance. Further, in contrast to entropy regularization, the performance after stripping BDD decrease more. Note that the difference between the proposed Lbdd and Lam is whether constraining the label angle variances to be balanced or not. This result indicates that the balanced constraint of label angle variances brings a better deep classifier as well as more accurate pseudolabels for unlabeled texts, especially when labeled texts are limited, and also empirically prove the\neffectiveness of our balanced label angle variances."
    }, {
      "heading" : "4.4 Efficiency Comparison",
      "text" : "To evaluate the efficiency of our S2TC-BDD, we perform efficiency comparisons over BERT, BERT+AM and S2TC-BDD on all benchmark datasets. To be fair, for all methods and datasets we set the batch sizes of labeled and unlabeled texts to 4 and 8 respectively, and iterate 100 epochs, where each one consists of 200 inner loops. The average per-epoch running time results are shown in Table 6. Generally speaking, the per-epoch running time of our proposed S2TC-BDD is close to those of BERT and BERT+AM. This result means that Gaussian linear transformation and estimation of label angle variances in our S2TC-BDD only introduce very few computation costs. That is expected since they merely require very few simple linear operations, which are very efficient."
    }, {
      "heading" : "5 Conclusion",
      "text" : "In this paper, we propose a novel self-training SSTC method, namely S2TC-BDD. Our S2TC-BDD addresses the margin bias problem in SSTC by balancing the label angle variances, i.e., the variance of label angles of texts within the same label. We estimate the label angle variances with both labeled and unlabeled texts during the self-training loops. To constrain the label angle variances to be balanced, we design several Gaussian linear transformations and incorporate them into a well established AM loss. Our S2TC-BDD empirically outperforms the existing SSTC baseline methods."
    }, {
      "heading" : "Acknowledgments",
      "text" : "We would like to acknowledge support for this project from the National Natural Science Foundation of China (NSFC) (No.61876071, No.62006094), the Key R&D Projects of Science and Technology Department of Jilin Province of China (No.20180201003SF, No.20190701031GH)."
    } ],
    "references" : [ {
      "title" : "Pooled contextualized embeddings for named entity recognition",
      "author" : [ "Alan Akbik", "Tanja Bergmann", "Roland Vollgraf." ],
      "venue" : "Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages",
      "citeRegEx" : "Akbik et al\\.,? 2019",
      "shortCiteRegEx" : "Akbik et al\\.",
      "year" : 2019
    }, {
      "title" : "Language models are few-shot learners",
      "author" : [ "Amodei." ],
      "venue" : "arXiv preprint arXiv:2005.14165.",
      "citeRegEx" : "Amodei.,? 2020",
      "shortCiteRegEx" : "Amodei.",
      "year" : 2020
    }, {
      "title" : "Importance of semantic representation: Dataless classification",
      "author" : [ "Ming-Wei Chang", "Lev-Arie Ratinov", "Dan Roth", "Vivek Srikumar." ],
      "venue" : "AAAI Conference on Artificial Intelligence, pages 830–835.",
      "citeRegEx" : "Chang et al\\.,? 2008",
      "shortCiteRegEx" : "Chang et al\\.",
      "year" : 2008
    }, {
      "title" : "Incorporating structured commonsense knowledge in story completion",
      "author" : [ "Jiaao Chen", "Jianshu Chen", "Zhou Yu." ],
      "venue" : "AAAI Conference on Artificial Intelligence, pages 6244–6251.",
      "citeRegEx" : "Chen et al\\.,? 2019",
      "shortCiteRegEx" : "Chen et al\\.",
      "year" : 2019
    }, {
      "title" : "Mixtext: Linguistically-informed interpolation of hidden space for semi-supervised text classification",
      "author" : [ "Jiaao Chen", "Zichao Yang", "Diyi Yang." ],
      "venue" : "Annual Meeting of the Association for Computational Linguistics, pages 2147–2157.",
      "citeRegEx" : "Chen et al\\.,? 2020",
      "shortCiteRegEx" : "Chen et al\\.",
      "year" : 2020
    }, {
      "title" : "Semi-supervised sequence modeling with cross-view training",
      "author" : [ "Kevin Clark", "Minh-Thang Luong", "Christopher D. Manning", "Quoc V. Le." ],
      "venue" : "Conference on Empirical Methods in Natural Language Processing, pages 1914–1925.",
      "citeRegEx" : "Clark et al\\.,? 2018",
      "shortCiteRegEx" : "Clark et al\\.",
      "year" : 2018
    }, {
      "title" : "Semisupervised sequence learning",
      "author" : [ "Andrew M. Dai", "Quoc V. Le." ],
      "venue" : "Neural Information Processing Systems, pages 3079–3087.",
      "citeRegEx" : "Dai and Le.,? 2015",
      "shortCiteRegEx" : "Dai and Le.",
      "year" : 2015
    }, {
      "title" : "BERT: pre-training of deep bidirectional transformers for language understanding",
      "author" : [ "Jacob Devlin", "Ming-Wei Chang", "Kenton Lee", "Kristina Toutanova." ],
      "venue" : "Conference of the North American Chapter of the Association for Computational Lin-",
      "citeRegEx" : "Devlin et al\\.,? 2019",
      "shortCiteRegEx" : "Devlin et al\\.",
      "year" : 2019
    }, {
      "title" : "A survey on semi-supervised learning",
      "author" : [ "Jesper E. van Engelen", "Holger H. Hoos." ],
      "venue" : "Machine Learning, 109(2):373–440.",
      "citeRegEx" : "Engelen and Hoos.,? 2020",
      "shortCiteRegEx" : "Engelen and Hoos.",
      "year" : 2020
    }, {
      "title" : "Semisupervised learning by entropy minimization",
      "author" : [ "Yves Grandvalet", "Yoshua Bengio." ],
      "venue" : "Neural Information Processing Systems, pages 529– 536.",
      "citeRegEx" : "Grandvalet and Bengio.,? 2004",
      "shortCiteRegEx" : "Grandvalet and Bengio.",
      "year" : 2004
    }, {
      "title" : "Variational pretraining for semi-supervised text classification",
      "author" : [ "Suchin Gururangan", "Tam Dang", "Dallas Card", "Noah A. Smith." ],
      "venue" : "Annual Meeting of the Association for Computational Linguistics, pages 5880–5894.",
      "citeRegEx" : "Gururangan et al\\.,? 2019",
      "shortCiteRegEx" : "Gururangan et al\\.",
      "year" : 2019
    }, {
      "title" : "Universal language model fine-tuning for text classification",
      "author" : [ "Jeremy Howard", "Sebastian Ruder." ],
      "venue" : "Annual Meeting of the Association for Computational Linguistics, pages 328–339.",
      "citeRegEx" : "Howard and Ruder.,? 2018",
      "shortCiteRegEx" : "Howard and Ruder.",
      "year" : 2018
    }, {
      "title" : "Deep representation learning on long-tailed data: A learnable embedding augmentation perspective",
      "author" : [ "Jialun Liu", "Yifan Sun", "Chuchu Han", "Zhaopeng Dou", "Wenhui Li." ],
      "venue" : "IEEE Conference on Computer Vision and Pattern Recognition, pages 2970–",
      "citeRegEx" : "Liu et al\\.,? 2020",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2020
    }, {
      "title" : "Adversarial training methods for semisupervised text classification",
      "author" : [ "Takeru Miyato", "Andrew M. Dai", "Ian J. Goodfellow." ],
      "venue" : "International Conference on Learning Representations.",
      "citeRegEx" : "Miyato et al\\.,? 2017",
      "shortCiteRegEx" : "Miyato et al\\.",
      "year" : 2017
    }, {
      "title" : "Virtual adversarial training: A regularization method for supervised and semi-supervised learning",
      "author" : [ "Takeru Miyato", "Shin-ichi Maeda", "Masanori Koyama", "Shin Ishii." ],
      "venue" : "IEEE Transactions Pattern Analysis and Machine Intelligence, 41(8):1979–",
      "citeRegEx" : "Miyato et al\\.,? 2019",
      "shortCiteRegEx" : "Miyato et al\\.",
      "year" : 2019
    }, {
      "title" : "Text classification from labeled and unlabeled documents using EM",
      "author" : [ "Kamal Nigam", "Andrew McCallum", "Sebastian Thrun", "Tom M. Mitchell." ],
      "venue" : "Machine Learning, 39(2):103–134.",
      "citeRegEx" : "Nigam et al\\.,? 2000",
      "shortCiteRegEx" : "Nigam et al\\.",
      "year" : 2000
    }, {
      "title" : "Scikit-learn: Machine learning in python",
      "author" : [ "Matthieu Perrot", "Edouard Duchesnay." ],
      "venue" : "Journal of Machine Learning Research, 12:2825–2830.",
      "citeRegEx" : "Perrot and Duchesnay.,? 2011",
      "shortCiteRegEx" : "Perrot and Duchesnay.",
      "year" : 2011
    }, {
      "title" : "Deep contextualized word representations",
      "author" : [ "Matthew E. Peters", "Mark Neumann", "Mohit Iyyer", "Matt Gardner", "Christopher Clark", "Kenton Lee", "Luke Zettlemoyer." ],
      "venue" : "Conference of the North American Chapter of the Association for Computational",
      "citeRegEx" : "Peters et al\\.,? 2018",
      "shortCiteRegEx" : "Peters et al\\.",
      "year" : 2018
    }, {
      "title" : "Improving language understanding by generative pre-training",
      "author" : [ "Alec Radford", "Karthik Narasimhan", "Tim Salimans", "Ilya Sutskever" ],
      "venue" : null,
      "citeRegEx" : "Radford et al\\.,? \\Q2019\\E",
      "shortCiteRegEx" : "Radford et al\\.",
      "year" : 2019
    }, {
      "title" : "2019b. Language models are unsupervised multitask learners",
      "author" : [ "Alec Radford", "Jeffrey Wu", "Rewon Child", "David Luan", "Dario Amodei", "Ilya Sutskever" ],
      "venue" : null,
      "citeRegEx" : "Radford et al\\.,? \\Q2019\\E",
      "shortCiteRegEx" : "Radford et al\\.",
      "year" : 2019
    }, {
      "title" : "Revisiting LSTM networks for semi-supervised text classification via mixed objective function",
      "author" : [ "Devendra Singh Sachan", "Manzil Zaheer", "Ruslan Salakhutdinov." ],
      "venue" : "AAAI Conference on Artificial Intelligence, pages 6940–6948.",
      "citeRegEx" : "Sachan et al\\.,? 2019",
      "shortCiteRegEx" : "Sachan et al\\.",
      "year" : 2019
    }, {
      "title" : "Regularization with stochastic transformations and perturbations for deep semi-supervised learning",
      "author" : [ "Mehdi Sajjadi", "Mehran Javanmardi", "Tolga Tasdizen." ],
      "venue" : "Neural Information Processing Systems, pages 1171–1179.",
      "citeRegEx" : "Sajjadi et al\\.,? 2016",
      "shortCiteRegEx" : "Sajjadi et al\\.",
      "year" : 2016
    }, {
      "title" : "Cosface: Large margin cosine loss for deep face recognition",
      "author" : [ "Hao Wang", "Yitong Wang", "Zheng Zhou", "Xing Ji", "Dihong Gong", "Jingchao Zhou", "Zhifeng Li", "Wei Liu." ],
      "venue" : "IEEE Conference on Computer Vision and Pattern Recognition, pages 5265–5274.",
      "citeRegEx" : "Wang et al\\.,? 2018",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2018
    }, {
      "title" : "Unsupervised data augmentation for consistency training",
      "author" : [ "Qizhe Xie", "Zihang Dai", "Eduard H. Hovy", "Thang Luong", "Quoc Le." ],
      "venue" : "Neural Information Processing Systems.",
      "citeRegEx" : "Xie et al\\.,? 2020",
      "shortCiteRegEx" : "Xie et al\\.",
      "year" : 2020
    }, {
      "title" : "Xlnet: Generalized autoregressive pretraining for language understanding",
      "author" : [ "Zhilin Yang", "Zihang Dai", "Yiming Yang", "Jaime G. Carbonell", "Ruslan Salakhutdinov", "Quoc V. Le." ],
      "venue" : "Neural Information Processing Systems, pages 5753–5763.",
      "citeRegEx" : "Yang et al\\.,? 2019",
      "shortCiteRegEx" : "Yang et al\\.",
      "year" : 2019
    }, {
      "title" : "Character-level convolutional networks for text classification",
      "author" : [ "Xiang Zhang", "Junbo Jake Zhao", "Yann LeCun." ],
      "venue" : "Neural Information Processing Systems, pages 649–657.",
      "citeRegEx" : "Zhang et al\\.,? 2015",
      "shortCiteRegEx" : "Zhang et al\\.",
      "year" : 2015
    } ],
    "referenceMentions" : [ {
      "referenceID" : 5,
      "context" : "research topic is termed as Semi-Supervised Text Classification (SSTC), which nowadays draws much attention from the community (Clark et al., 2018; Gururangan et al., 2019; Chen et al., 2020).",
      "startOffset" : 127,
      "endOffset" : 191
    }, {
      "referenceID" : 10,
      "context" : "research topic is termed as Semi-Supervised Text Classification (SSTC), which nowadays draws much attention from the community (Clark et al., 2018; Gururangan et al., 2019; Chen et al., 2020).",
      "startOffset" : 127,
      "endOffset" : 191
    }, {
      "referenceID" : 4,
      "context" : "research topic is termed as Semi-Supervised Text Classification (SSTC), which nowadays draws much attention from the community (Clark et al., 2018; Gururangan et al., 2019; Chen et al., 2020).",
      "startOffset" : 127,
      "endOffset" : 191
    }, {
      "referenceID" : 6,
      "context" : "To our knowledge, the most recent SSTC methods mainly borrow ideas from the successful patterns of supervised deep learning, such as pretraining and fine-tuning (Dai and Le, 2015; Howard and Ruder, 2018; Peters et al., 2018; Gururangan et al., 2019; Devlin et al., 2019).",
      "startOffset" : 161,
      "endOffset" : 270
    }, {
      "referenceID" : 11,
      "context" : "To our knowledge, the most recent SSTC methods mainly borrow ideas from the successful patterns of supervised deep learning, such as pretraining and fine-tuning (Dai and Le, 2015; Howard and Ruder, 2018; Peters et al., 2018; Gururangan et al., 2019; Devlin et al., 2019).",
      "startOffset" : 161,
      "endOffset" : 270
    }, {
      "referenceID" : 17,
      "context" : "To our knowledge, the most recent SSTC methods mainly borrow ideas from the successful patterns of supervised deep learning, such as pretraining and fine-tuning (Dai and Le, 2015; Howard and Ruder, 2018; Peters et al., 2018; Gururangan et al., 2019; Devlin et al., 2019).",
      "startOffset" : 161,
      "endOffset" : 270
    }, {
      "referenceID" : 10,
      "context" : "To our knowledge, the most recent SSTC methods mainly borrow ideas from the successful patterns of supervised deep learning, such as pretraining and fine-tuning (Dai and Le, 2015; Howard and Ruder, 2018; Peters et al., 2018; Gururangan et al., 2019; Devlin et al., 2019).",
      "startOffset" : 161,
      "endOffset" : 270
    }, {
      "referenceID" : 7,
      "context" : "To our knowledge, the most recent SSTC methods mainly borrow ideas from the successful patterns of supervised deep learning, such as pretraining and fine-tuning (Dai and Le, 2015; Howard and Ruder, 2018; Peters et al., 2018; Gururangan et al., 2019; Devlin et al., 2019).",
      "startOffset" : 161,
      "endOffset" : 270
    }, {
      "referenceID" : 7,
      "context" : ", BERT (Devlin et al., 2019) with Angular Margin (AM) loss (Wang et al.",
      "startOffset" : 7,
      "endOffset" : 28
    }, {
      "referenceID" : 22,
      "context" : ", 2019) with Angular Margin (AM) loss (Wang et al., 2018), by training over labeled texts only; and then it alternatively predicts unlabeled texts as their pseudo-labels and trains the deep classifier over the mixture of labeled and pseudo-labeled texts.",
      "startOffset" : 38,
      "endOffset" : 57
    }, {
      "referenceID" : 6,
      "context" : "Several SSTC methods are built on this framework (Dai and Le, 2015; Howard and Ruder, 2018; Peters et al., 2018; Gururangan et al., 2019; Devlin et al., 2019).",
      "startOffset" : 49,
      "endOffset" : 158
    }, {
      "referenceID" : 11,
      "context" : "Several SSTC methods are built on this framework (Dai and Le, 2015; Howard and Ruder, 2018; Peters et al., 2018; Gururangan et al., 2019; Devlin et al., 2019).",
      "startOffset" : 49,
      "endOffset" : 158
    }, {
      "referenceID" : 17,
      "context" : "Several SSTC methods are built on this framework (Dai and Le, 2015; Howard and Ruder, 2018; Peters et al., 2018; Gururangan et al., 2019; Devlin et al., 2019).",
      "startOffset" : 49,
      "endOffset" : 158
    }, {
      "referenceID" : 10,
      "context" : "Several SSTC methods are built on this framework (Dai and Le, 2015; Howard and Ruder, 2018; Peters et al., 2018; Gururangan et al., 2019; Devlin et al., 2019).",
      "startOffset" : 49,
      "endOffset" : 158
    }, {
      "referenceID" : 7,
      "context" : "Several SSTC methods are built on this framework (Dai and Le, 2015; Howard and Ruder, 2018; Peters et al., 2018; Gururangan et al., 2019; Devlin et al., 2019).",
      "startOffset" : 49,
      "endOffset" : 158
    }, {
      "referenceID" : 10,
      "context" : "For instance, the VAriational Methods for Pretraining In Resourcelimited Environments (VAMPIRE) (Gururangan et al., 2019) first pre-trains a Variational AutoEncoder (VAE) model on unlabeled texts, and then trains a classifier on the augmentation representations of labeled texts computed by the pre-trained VAE.",
      "startOffset" : 96,
      "endOffset" : 121
    }, {
      "referenceID" : 13,
      "context" : "Recent works on SSTC mainly focus on deep self-training (Miyato et al., 2017; Clark et al., 2018; Sachan et al., 2019; Miyato et al., 2019; Xie et al., 2020; Chen et al., 2020), which can jointly learn deep representation and classifier using both labeled and unlabeled texts in a unified framework.",
      "startOffset" : 56,
      "endOffset" : 176
    }, {
      "referenceID" : 5,
      "context" : "Recent works on SSTC mainly focus on deep self-training (Miyato et al., 2017; Clark et al., 2018; Sachan et al., 2019; Miyato et al., 2019; Xie et al., 2020; Chen et al., 2020), which can jointly learn deep representation and classifier using both labeled and unlabeled texts in a unified framework.",
      "startOffset" : 56,
      "endOffset" : 176
    }, {
      "referenceID" : 20,
      "context" : "Recent works on SSTC mainly focus on deep self-training (Miyato et al., 2017; Clark et al., 2018; Sachan et al., 2019; Miyato et al., 2019; Xie et al., 2020; Chen et al., 2020), which can jointly learn deep representation and classifier using both labeled and unlabeled texts in a unified framework.",
      "startOffset" : 56,
      "endOffset" : 176
    }, {
      "referenceID" : 14,
      "context" : "Recent works on SSTC mainly focus on deep self-training (Miyato et al., 2017; Clark et al., 2018; Sachan et al., 2019; Miyato et al., 2019; Xie et al., 2020; Chen et al., 2020), which can jointly learn deep representation and classifier using both labeled and unlabeled texts in a unified framework.",
      "startOffset" : 56,
      "endOffset" : 176
    }, {
      "referenceID" : 23,
      "context" : "Recent works on SSTC mainly focus on deep self-training (Miyato et al., 2017; Clark et al., 2018; Sachan et al., 2019; Miyato et al., 2019; Xie et al., 2020; Chen et al., 2020), which can jointly learn deep representation and classifier using both labeled and unlabeled texts in a unified framework.",
      "startOffset" : 56,
      "endOffset" : 176
    }, {
      "referenceID" : 4,
      "context" : "Recent works on SSTC mainly focus on deep self-training (Miyato et al., 2017; Clark et al., 2018; Sachan et al., 2019; Miyato et al., 2019; Xie et al., 2020; Chen et al., 2020), which can jointly learn deep representation and classifier using both labeled and unlabeled texts in a unified framework.",
      "startOffset" : 56,
      "endOffset" : 176
    }, {
      "referenceID" : 20,
      "context" : "Further, the work in (Sachan et al., 2019) combines maximum likelihood, adversarial training, virtual adversarial training, and entropy minimization in a unified objective.",
      "startOffset" : 21,
      "endOffset" : 42
    }, {
      "referenceID" : 23,
      "context" : "Furthermore, rather than applying local perturbations, Unsupervised Data Augmentation (UDA) (Xie et al., 2020) employs consistency loss between the predictions of unlabeled texts and corresponding augmented texts by data augmentation techniques such as back translations and tf-idf word replacements.",
      "startOffset" : 92,
      "endOffset" : 110
    }, {
      "referenceID" : 5,
      "context" : "The work (Clark et al., 2018) exploits cross-view training by matching the predictions of auxiliary prediction modules over the restricted views of unlabeled texts (e.",
      "startOffset" : 9,
      "endOffset" : 29
    }, {
      "referenceID" : 22,
      "context" : "BDD Loss Formally, our BDD loss is extended from the AM loss (Wang et al., 2018).",
      "startOffset" : 61,
      "endOffset" : 80
    }, {
      "referenceID" : 9,
      "context" : "5048 This conditional entropy regularization is introduced by (Grandvalet and Bengio, 2004), and also utilized in (Sajjadi et al.",
      "startOffset" : 62,
      "endOffset" : 91
    }, {
      "referenceID" : 21,
      "context" : "5048 This conditional entropy regularization is introduced by (Grandvalet and Bengio, 2004), and also utilized in (Sajjadi et al., 2016; Miyato et al., 2019; Sachan et al., 2019).",
      "startOffset" : 114,
      "endOffset" : 178
    }, {
      "referenceID" : 14,
      "context" : "5048 This conditional entropy regularization is introduced by (Grandvalet and Bengio, 2004), and also utilized in (Sajjadi et al., 2016; Miyato et al., 2019; Sachan et al., 2019).",
      "startOffset" : 114,
      "endOffset" : 178
    }, {
      "referenceID" : 20,
      "context" : "5048 This conditional entropy regularization is introduced by (Grandvalet and Bengio, 2004), and also utilized in (Sajjadi et al., 2016; Miyato et al., 2019; Sachan et al., 2019).",
      "startOffset" : 114,
      "endOffset" : 178
    }, {
      "referenceID" : 12,
      "context" : "To avoid the misleading affect of some mislabeled texts, inspired by (Liu et al., 2020), we update {ck} k=1 by employing the moving average with a learning rate γ:",
      "startOffset" : 69,
      "endOffset" : 87
    }, {
      "referenceID" : 25,
      "context" : "Datasets To conduct the experiments, we employ three widely used benchmark datasets for text classification: AG News (Zhang et al., 2015), Yelp (Zhang et al.",
      "startOffset" : 117,
      "endOffset" : 137
    }, {
      "referenceID" : 25,
      "context" : ", 2015), Yelp (Zhang et al., 2015), and Yahoo (Chang et al.",
      "startOffset" : 14,
      "endOffset" : 34
    }, {
      "referenceID" : 15,
      "context" : "• NB+EM (Nigam et al., 2000): A semisupervised text classification method combining a Naive Bayes classifier (NB) and Expectation-Maximization (EM).",
      "startOffset" : 8,
      "endOffset" : 28
    }, {
      "referenceID" : 10,
      "context" : "In experiments, we pre-process texts following (Gururangan et al., 2019) and use tf-idfs as the representations of texts.",
      "startOffset" : 47,
      "endOffset" : 72
    }, {
      "referenceID" : 7,
      "context" : "• BERT (Devlin et al., 2019): A supervised text classification method built on the pre-trained BERT-based-uncased model1 and fine-tuned with the supervised softmax loss on labeled texts.",
      "startOffset" : 7,
      "endOffset" : 28
    }, {
      "referenceID" : 10,
      "context" : "• VAMPIRE (Gururangan et al., 2019): A semi-supervised text classification method based on variational pre-training.",
      "startOffset" : 10,
      "endOffset" : 35
    }, {
      "referenceID" : 14,
      "context" : "• VAT (Miyato et al., 2019): A semi-supervised text classification method based on virtual adversarial training.",
      "startOffset" : 6,
      "endOffset" : 27
    }, {
      "referenceID" : 23,
      "context" : "• UDA (Xie et al., 2020): A semi-supervised text classification method based on unsupervised data augmentation with back translation.",
      "startOffset" : 6,
      "endOffset" : 24
    } ],
    "year" : 2021,
    "abstractText" : "Semi-Supervised Text Classification (SSTC) mainly works under the spirit of self-training. They initialize the deep classifier by training over labeled texts; and then alternatively predict unlabeled texts as their pseudo-labels and train the deep classifier over the mixture of labeled and pseudo-labeled texts. Naturally, their performance is largely affected by the accuracy of pseudo-labels for unlabeled texts. Unfortunately, they often suffer from low accuracy because of the margin bias problem caused by the large difference between representation distributions of labels in SSTC. To alleviate this problem, we apply the angular margin loss, and perform Gaussian linear transformation to achieve balanced label angle variances, i.e., the variance of label angles of texts within the same label. More accuracy of predicted pseudo-labels can be achieved by constraining all label angle variances balanced, where they are estimated over both labeled and pseudo-labeled texts during self-training loops. With this insight, we propose a novel SSTC method, namely Semi-Supervised Text Classification with Balanced Deep representation Distributions (STC-BDD). To evaluate STCBDD, we compare it against the state-of-theart SSTC methods. Empirical results demonstrate the effectiveness of STC-BDD, especially when the labeled texts are scarce.",
    "creator" : "LaTeX with hyperref"
  }
}