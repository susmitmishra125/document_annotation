{
  "name" : "2021.acl-long.82.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "Robust Knowledge Graph Completion with Stacked Convolutions and a Student Re-Ranking Network",
    "authors" : [ "Justin Lovelace", "Denis Newman-Griffis", "Shikhar Vashishth", "Jill Fain Lehman", "Carolyn Penstein Rosé" ],
    "emails" : [ "cpa3}@cs.cmu.edu,", "dnewmangriffis@pitt.edu", "t-svashishth@microsoft.com" ],
    "sections" : [ {
      "heading" : null,
      "text" : "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 1016–1029\nAugust 1–6, 2021. ©2021 Association for Computational Linguistics\n1016"
    }, {
      "heading" : "1 Introduction",
      "text" : "Knowledge graphs (KGs) have been shown to be useful for a wide range of NLP tasks, such as question answering (Bordes et al., 2014a,b), dialog systems (Ma et al., 2015), relation extraction (Mintz et al., 2009; Vashishth et al., 2018), and recommender systems (Zhang et al., 2016). However, because scaling the collection of facts to provide coverage for all the true relations that hold between entities is difficult, most existing KGs are incomplete (Dong et al., 2014), limiting their utility for downstream applications. Because of this problem, KG completion (KGC) has come to be a widely studied task (Yang et al., 2015; Trouillon et al., 2016; Shang et al., 2018; Dettmers et al., 2018;\n∗Work performed while at Carnegie Mellon University. 1https://github.com/justinlovelace/\nrobust-kg-completion\nSun et al., 2019; Balazevic et al., 2019; Malaviya et al., 2020; Vashishth et al., 2020a).\nThe increased interest in KGC has led to the curation of a number of benchmark datasets such as FB15K (Bordes et al., 2013), WN18 (Bordes et al., 2013), FB15k-237 (Toutanova and Chen, 2015), and YAGO3-10 (Rebele et al., 2016) that have been the focus of most of the work in this area. However, these benchmark datasets are often curated in such a way as to produce densely connected networks that simplify the task and are not representative of real KGs. For instance, FB15K includes only entities with at least 100 links in Freebase, while YAGO3-10 is limited to only include entities in YAGO3 (Rebele et al., 2016) that have at least 10 relations.\nReal KGs are not as uniformly dense as these benchmark datasets and have many sparsely connected entities (Pujara et al., 2017). This can pose a challenge to typical KGC methods that learn entity representations solely from the knowledge that already exists in the graph.\nTextual entity identifiers can be used to develop entity embeddings that are more robust to sparsity (Malaviya et al., 2020). It has also been shown that textual triplet representations can be used with BERT for triplet classification (Yao et al., 2019). Such an approach can be extended to the more common ranking paradigm through the exhaustive evaluation of candidate triples, but that does not scale to large KG datasets.\nIn our work, we found that existing neural KGC models lack the complexity to effectively fit the training data when used with the pre-trained textual embeddings that are necessary for representing sparsely connected entities. We develop an expressive deep convolutional model that utilizes textual entity representations more effectively and improves sparse KGC. We also develop a student reranking model that is trained using knowledge dis-\ntilled from our original ranking model and demonstrate that the re-ranking procedure is particularly effective for sparsely connected entities. Through these innovations, we develop a KGC pipeline that is more robust to the realities of real KGs. Our contributions can be summarized as follows. • We develop a deep convolutional architecture that\nutilizes textual embeddings more effectively than existing neural KGC models and significantly improves performance for sparse KGC.\n• We develop a re-ranking procedure that distills knowledge from our ranking model into a student network that re-ranks promising candidate entities.\n• We curate two sparse KG datasets containing biomedical and encyclopedic knowledge to study KGC in the setting where dense connectivity is not guaranteed. We release the encyclopedic dataset and the code to derive the biomedical dataset to encourage future work."
    }, {
      "heading" : "2 Related Work",
      "text" : "Knowledge Graph Completion: KGC models typically learn entity and relation embeddings based on known facts (Nickel et al., 2011; Bordes et al., 2013; Yang et al., 2015) and use the learned embeddings to score potential candidate triples. Recent work includes both non-neural (Nickel et al., 2016; Trouillon et al., 2016; Liu et al., 2017; Sun et al., 2019) and neural (Socher et al., 2013; Dong et al., 2014; Dettmers et al., 2018; Vashishth et al., 2020b) approaches for embedding KGs. However, most of them only demonstrate their efficacy on artificially dense benchmark datasets. Pujara et al. (2017) show that the performance of such methods varies drastically with sparse, unreliable data. We compare our proposed method against the existing approaches in a realistic setting where the KG is not uniformly dense.\nPrior work has effectively utilized entity names or descriptions to aid KGC (Socher et al., 2013; Ruobing Xie, 2016; Xiao et al., 2016). In more recent work, Malaviya et al. (2020) explore the problem of KGC using commonsense KGs, which are much sparser than standard benchmark datasets. They adapt an existing KGC model to utilize BERT (Devlin et al., 2019) embeddings. In this paper, we develop a deep convoluational architecture that is more effective than adapting existing shallow models which we find to be underpowerered for large KG datasets.\nYao et al. (2019) developed a triplet classification model by directly fine-tuning BERT with textual entity representations and reported strong classification results. They also adapted their triplet classification model to the ranking paradigm by exhaustively evaluating all possible triples for a given query, (e1, r, ?). However, the ranking performance was not competitive2, and such an approach is not scalable to large KG datasets like those explored in this work. Exhaustively applying BERT to compute all rankings for the test set for our largest dataset would take over two months. In our re-ranking setting, we reduce the number of triples that need to be evaluated by over 7700×, reducing the evaluation time to less than 15 minutes.\nBERT as a Knowledge Base: Recent work (Petroni et al., 2019; Jiang et al., 2020; Rogers et al., 2020) has utilized the masked-languagemodeling (MLM) objective to probe the knowledge contained within pre-trained models using fill-in-the-blank prompts (e.g. “Dante was born in [MASK]”). This body of work has found that pre-trained language models such as BERT capture some of the relational knowledge contained within their pre-training corpora. This motivates us to utilize these models to develop entity representations that are well-suited for KGC.\nRe-Ranking: Wang et al. (2011) introduced cascade re-ranking for document retrieval. This approach applies inexpensive models to develop an initial ranking and utilizes expensive models to improve the ranking of the top-k candidates. Reranking has since been successfully applied across many retrieval tasks (Matsubara et al., 2020; Pei et al., 2019; Nogueira and Cho, 2019). Despite re-ranking’s widespread success, recent KGC work utilizes a single ranking model. We develop an entity re-ranking procedure and demonstrate the effectiveness of the re-ranking paradigm for KGC.\nKnowledge Distillation: Knowledge distillation is a popular technique that is often used for model compression where a large, high-capacity teacher is used to train a simpler student network (Hinton et al., 2015). However, knowledge distillation has since been shown to be useful for improving model performance beyond the original setting of model compression. Li et al. (2017) demonstrated that knowledge distillation improved image classification performance in a setting with noisy\n2Their reported Hits@10 for FB15K-237 was .420 which is lower than all of the models evaluated in this work.\nlabels. The incompleteness of KGs leads to noisy training labels which motivates us to use knowledge distillation to train a student re-ranking model that is more robust to the label noise."
    }, {
      "heading" : "3 Datasets",
      "text" : "We examine KGC in the realistic setting where KGs have many sparsely connected entities. We utilize a commonsense KG dataset that has been used in past work and curate two additional sparse KG datasets containing biomedical and encyclopedic knowledge. We release the encyclopedic dataset and the code to derive the biomedical dataset to encourage future work in this challenging setting. The summary statistics for all datasets are presented in Table 1 and we visualize the connectivity of the datasets in Figure 1."
    }, {
      "heading" : "3.1 SNOMED CT Core",
      "text" : "For constructing SNOMED CT Core, we use the knowledge graph defined by SNOMED CT (Donnelly, 2006), which is contained within the Unified Medical Language System (UMLS) (Bodenreider, 2004). SNOMED CT is well-maintained and is one of the most comprehensive knowledge bases contained within the UMLS (Jiménez-Ruiz et al., 2011; Jiang and Chute, 2009). We first extract the UMLS3 concepts found in the CORE Problem List Subset of the SNOMED CT knowledge base. This subset is intended to contain the concepts most useful for documenting clinical information. We\n3We work with the 2020AA release of the UMLS.\nthen expand the graph to include all concepts that are directly linked to those in the CORE Problem List Subset according to the relations defined by the SNOMED CT KG. Our final KG consists of this set of concepts and the SNOMED CT relations connecting them. Importantly, we do not filter out rare entities from the KG, as is commonly done during the curation of benchmark datasets.\nTo avoid leaking data from inverse, or otherwise informative, relations, we divide the facts into training, validation, and testing sets based on unordered tuples of entities {e1, e2} so that all relations between any two entities are confined to a single split. Unlike some other KG datasets that filter out inverse relations, we divide our dataset in such a way that this is not necessary; our dataset already includes inverse relations, and they do not need to be manually added for training and evaluation as is standard practice (Dettmers et al., 2018; Malaviya et al., 2020).\nBecause we represent entities using textual descriptions in this work, we also mine the entities’ preferred concept names (e.g. “Traumatic hematoma of left kidney”) from the UMLS."
    }, {
      "heading" : "3.2 FB15k-237-Sparse",
      "text" : "The FB15k-237 (Toutanova and Chen, 2015) dataset contains encyclopedic knowledge about the world, e.g. (Barack Obama, placeOfBirth, Honolulu). Although the dataset is very densely connected, that density is artificial. FB15K (Bordes et al., 2013), the precursor to FB15k-237, was curated to only include entities with at least 100 links in Freebase (Bollacker et al., 2008).\nThe dense connectivity of FB15k-237 does allow us to to ablate the effect of this density. We utilize the FB15k-237 dataset and also develop a new dataset, denoted FB15k-237-Sparse, by randomly downsampling the facts in the training set of FB15k-237 to match the average in-degree of the ConceptNet-100K dataset. We use this to directly evaluate the effect of increased sparsity.\nFor the FB15k-237 dataset, we use the textual identifiers released by Ruobing Xie (2016). They released both entity names (e.g. “Jason Frederick Kidd”) as well as brief textual descriptions (e.g. “Jason Frederick Kidd is a retired American professional basketball player. . . ”) for most entities. We utilize the textual descriptions when available."
    }, {
      "heading" : "3.3 ConceptNet-100K",
      "text" : "ConceptNet (Speer and Havasi, 2013) is a KG that contains commonsense knowledge about the world such as the fact (go to dentist, motivatedBy, prevent tooth decay). We utilize ConceptNet-100k (CN-100K) (Li et al., 2016) which consists of the Open Mind Common Sense entries in the ConceptNet dataset. This KG is much sparser than benchmark datasets like FB15k-237, which makes it well-suited for our purpose. We use the training, validation, and testing splits of Malaviya et al. (2020) to allow for direct comparison. We also use the textual descriptions released by Malaviya et al. (2020) to represent the KG entities."
    }, {
      "heading" : "4 Methods",
      "text" : "We provide an overview of our model architecture in Figure 2. We first extract feature representations from BERT (Devlin et al., 2019) to develop textual entity embeddings. Motivated by our observation that existing neural KG architectures are underpowered in our setting, we develop a deep convolutional network utilizing architectural innovations from deep convolutional vision models. Our model’s design improves its ability to fit complex relationships in the training data which leads to downstream performance improvements.\nFinally, we distill our ranking model’s knowledge into a student re-ranking network that adjusts the rankings of promising candidates. In doing so, we demonstrate the effectiveness of the re-ranking\nparadigm for KGC and develop a KGC pipeline with greater robustness to the sparsity of real KGs."
    }, {
      "heading" : "4.1 Entity Ranking",
      "text" : "We follow the standard formulation for KGC. We represent a KG as a set of entity-relationentity facts (e1, r, e2). Given an incomplete fact, (e1, r, ?), our model computes a score for all candidate entities ei that exist in the graph. An effective KGC model should assign greater scores to correct entities than incorrect ones. We follow recent work (Dettmers et al., 2018; Malaviya et al., 2020) and consider both forward and inverse relations (e.g. treats and treated by) in this work. For the datasets that do not already include inverse relations, we introduce an inverse fact, (e2, r−1, e1), for every fact, (e1, r, e2), in the dataset."
    }, {
      "heading" : "4.1.1 Textual Entity Representations",
      "text" : "We utilize BERT (Devlin et al., 2019) to develop entity embeddings that are invariant to the connectivity of the KG. We follow the work of Malaviya et al. (2020) and adapt BERT to each KG’s naming style by fine-tuning BERT using the MLM objective with the set of entity identifiers in the KG.\nFor CN-100K and FB15k-237, we utilize the BERT-base uncased model. For SNOMED CT Core KG, we utilize PubMedBERT (Gu et al., 2020) which is better suited for the biomedical terminology in the UMLS.\nWe apply BERT to the textual entity identifiers and mean-pool across the token representations\nfrom all BERT layers to obtain a summary feature vector for the concept name. We fix these embeddings during training because we must compute scores for a large number of potential candidate entities for each training example. This makes finetuning BERT prohibitively expensive."
    }, {
      "heading" : "4.1.2 Deep Convolutional Architecture",
      "text" : "Inspired by the success of deep convolutional models in computer vision (Krizhevsky et al., 2012; Simonyan and Zisserman, 2015; He et al., 2016; Huang et al., 2019, 2017), we develop a knowledge base completion model based on the seminal ResNet architecture (He et al., 2016) that is sufficiently expressive to model complex interactions between the BERT feature space and the relation embeddings.\nGiven an incomplete triple (ei, rj , ?), we begin by stacking the precomputed entity embedding e ∈ R1×d with the learned relation embedding of the same dimension r ∈ R1×d to produce a feature vector of length dwith two channels q ∈ R2×d. We then apply a one-dimensional convolution with a kernel of width 1 along the length of the feature vector to project each position i to a two-dimensional spatial feature map xi ∈ Rf×f where the convolution has f × f filters. Thus the convolution produces a two-dimensional spatial feature map X ∈ Rf×f×d with d channels, representing the incomplete query triple (ei, rj , ?).\nThe spatial feature map, X ∈ Rf×f×d, is analogous to a square image with a side length of f and d channels, allowing for the straightforward application of deep convolutional models such as ResNet. We apply a sequence of 3N bottleneck blocks to the spatial feature map where N is a hyperparameter that controls the depth of the network. A bottleneck block consists of three consecutive convolutions: a 1× 1 convolution, a 3× 3 convolution, and then another 1 × 1 convolution. The first 1× 1 convolution reduces the feature map dimensionality by a factor of 4 and then the second 1× 1 convolution restores the feature map dimensionality. This design reduces the dimensionality of the expensive 3× 3 convolutions and allows us to increase the depth of our model without dramatically increasing its parameterization. We double the feature dimensionality of the bottleneck blocks after N and 2N blocks so the dimensionality of the final feature map produced by the sequence of convolutions is 4d.\nWe add residual connections to each bottleneck\nblock which improves training for deep networks (He et al., 2016). If we let F(X) represent the application of the bottleneck convolutions, then the output of the bottleneck block is Y = F(X) +X . We apply batch normalization followed by a ReLU nonlinearity (Nair and Hinton, 2010) before each convolutional layer (He et al., 2016) .\nWe utilize circular padding (Wang et al., 2018; Vashishth et al., 2020a) with the 3×3 convolutions to maintain the spatial size of the feature map and use a stride of 1 for all convolutions. For the bottleneck blocks that double the dimensionality of the feature map, we utilize a projection shortcut for the residual connection (He et al., 2016)."
    }, {
      "heading" : "4.1.3 Entity Scoring",
      "text" : "Given an incomplete fact (ei, rj , ?), our convolutional architecture produces a feature map X̂ ∈ Rf×f×4d. We average pool this feature representation over the spatial dimension which produces a summary feature vector x̂ ∈ R4d. We then apply a fully connected layer followed by a PReLU nonlinearity (He et al., 2015) to project the feature vector back to the original embedding dimensionality d. We denote this final vector ê and compute scores for candidate entities using the dot product with candidate entity embeddings. The scores can be efficiently computed for all entities simultaneously using a matrix-vector product with the embedding matrix y = êET where E ∈ Rm×d stores the embeddings for all m entities in the KG."
    }, {
      "heading" : "4.1.4 Training",
      "text" : "Adopting the terminology used by Ruffinelli et al. (2020), we utilize a 1vsAll training strategy with the binary cross-entropy loss function. We treat every fact in our dataset, (ei, rj , ek), as a training sample where (ei, rj , ?) is the input to the model. We compute scores for all entities as described previously and apply a sigmoid operator to induce a probability for each entity. We treat all entities other than ek as negative candidates and then compute the binary cross-entropy loss.\nWe train our model using the Adam optimizer (Kingma and Ba, 2015) with decoupled weight decay regularization (Loshchilov and Hutter, 2019) and label smoothing. We train our models for a maximum of 200 epochs and terminate training early if the validation Mean Reciprocal Rank (MRR) has not improved for 20 epochs. We trained all of the models used in this work using a single NVIDIA GeForce GTX 1080 Ti."
    }, {
      "heading" : "4.2 Entity Re-Ranking",
      "text" : ""
    }, {
      "heading" : "4.2.1 Re-Ranking Network",
      "text" : "We use our convolutional network to extract the top-k entities for every unique training query and then train a re-ranking network to rank these entities. We design our student re-ranking network as a triplet classification model that utilizes the full candidate fact, (ei, rj , ek), instead of an incomplete fact, (ei, rj , ?). This allows the network to model interactions between all elements of the triple. The re-ranking setting also enables us to directly finetune BERT which often improves performance (Peters et al., 2019).\nWe introduce relation tokens4 for each relation in the knowledge graph and construct the textual input by prepending the head and tail entities with the relation token and then concatenating the two sequences. Thus the triple (“head name”, ri, “tail name”) would be represented as “[CLS] [REL i] head name [SEP] [REL i] tail name [SEP]”. We use a learned linear combination of the [CLS] embedding from each layer as the final feature representation for the prediction."
    }, {
      "heading" : "4.2.2 Knowledge Distillation",
      "text" : "A sufficiently performant ranking model can provide an informative prior that can be used to smooth the noisy training labels and improve our re-ranking model. For each training query i, we normalize the logits produced by our teacher ranking model, fT (xi), for the k candidate triples, fT (xi)0:k, as\nsik:(i+1)k = softmax(fT (xi)0:k/T )\nwhere T is the temperature (Hinton et al., 2015). Our training objective for our student model, fS(xi), is a weighted average of the binary cross entropy loss, Lbce, using the teacher’s normalized logits, s, and the noisy training labels, y.\nLKD(yi, xi) = λLbce(si, fS(xi)) + (λ− 1)Lbce(yi, fS(xi)) = Lbce((λ− 1)yi + λsi, fS(xi))\n4We use relation tokens instead of free-text relation representations because the relation identifiers for our datasets are not all well-formed using natural language, and the different styles would introduce a confounding factor that would complicate our evaluation. Utilizing appropriate free-text relation identifiers may improve performance, but we leave that to future work.\nWe select λ ∈ {.25, .5, .75, 1} to optimize the balance between the two objectives using validation performance."
    }, {
      "heading" : "4.2.3 Training",
      "text" : "For our experiments, we extract the top k = 10 candidates produced by our ranking model for every query in the training set. We train our student network using the Adam optimizer (Kingma and Ba, 2015) with decoupled weight decay regularization (Loshchilov and Hutter, 2019). We fine-tune BERT for a maximum of 10 epochs and terminate training early if the Mean Reciprocal Rank (MRR) on validation data has not improved for 3 epochs."
    }, {
      "heading" : "4.2.4 Student-Teacher Ensemble",
      "text" : "For every query, we apply our re-ranking network to the top k = 10 triples and compute the final ranking using an ensemble of the teacher and student networks. The final ranking are computed with\nŝik:(i+1)k = α(softmax(fS(xik:(i+1)k)))\n+ (1− α)(softmax(fT (xi)0:k)))\nwhere 0 ≤ α ≤ 1 controls the impact of the student re-ranker. The cost of computing ŝik:(i+1)k is negligible, so we sweep over [0, 1] in increments of .01 and select the α that achieves the best validation MRR."
    }, {
      "heading" : "5 Experiments",
      "text" : ""
    }, {
      "heading" : "5.1 Baselines",
      "text" : "We utilize the same representative selection of KG models from Malaviya et al. (2020) as baselines: DistMult (Yang et al., 2015), ComplEx (Trouillon et al., 2016) ConvE (Dettmers et al., 2018), and ConvTransE (Shang et al., 2018). This is not an exhaustive selection of all recent KG methods, but a recent replication study by Ruffinelli et al. (2020) found that the baselines that we use are competitive with the state-of-the-art and often outperform more recent models when trained appropriately.\nWe develop additional baselines by adapting the shallow convolutional KGC models to use BERT embeddings to evaluate the benefits of utilizing our proposed convolutional architecture instead of simply repurposing existing KGC models. We refer to these models as BERT-ConvE and BERT-ConvTransE. Malaviya et al. (2020) used BERT embeddings in conjunction with ConvTransE for commonsense KGC, but their model was prohibitively large to reproduce. We refer to\ntheir model as BERT-Large-ConvTransE and compare directly against their reported results.\nWe also develop a deep convolutional baseline, termed BERT-DeepConv, to evaluate the effect of the architectural innovations used in our model. BERT-DeepConv transforms the input embeddings to a spatial feature map like our proposed model, but it then applies a stack of 3 × 3 convolutions instead of a sequence of bottleneck blocks with residual connections. We select hyperparameters (detailed in the Appendix) for all of our BERT baselines so that they have a comparable number of trainable parameters to our proposed model. We discuss the size of these models in detail in in Section 6.4.\nTo evaluate the impact of our re-ranking stage, we ablate the use of knowledge distillation and ensembling. Thus we conduct experiments where our\nre-ranker uses only knowledge distillation, uses only ensembling, and uses neither. This means that in the most naive setting, we train the re-ranker using the hard training labels and re-rank the candidates using only the re-ranker."
    }, {
      "heading" : "5.2 Evaluation",
      "text" : "We report standard ranking metrics: Mean Rank (MR), Mean Reciprocal Rank (MRR), Hits at 1 (H@1), Hits at 3 (H@3), and Hits at 10 (H@10). We follow past work and use the filtered setting (Bordes et al., 2013), removing all positive entities other than the target entity before calculating the target entity’s rank.\nWe utilize paired bootstrap significance testing (Berg-Kirkpatrick et al., 2012) with the MRR to validate the statistical significance of improvements. To account for the large number of comparisons\nbeing performed, we apply the Holm–Bonferroni method (Holm, 1979) to correct for multiple hypothesis testing. We define families for the three primary hypotheses that we tested with our experiments. They are as follows: (1) The deep convolutional BERT models outperform the shallow convolutional BERT models. (2) BERT-ResNet improves upon our BERT-DeepConv baseline. (3) The re-ranking procedure improves the original rankings.\nThis selection has the benefit of allowing for a more granular analysis of each conclusion while significantly reducing the number of hypotheses. The first family includes all pairwise comparisons between the two deep convolutional models and the two shallow convolutional models. The second family involves all comparisons between BERTResNet and BERT-DeepConv. The third family includes comparisons between all re-ranking configurations and the original rankings. We note that the p-value for each family bounds the strict condition that we report any spurious finding within the family."
    }, {
      "heading" : "6 Results and Discussion",
      "text" : ""
    }, {
      "heading" : "6.1 Ranking Performance",
      "text" : "We report results across all of our datasets in Table 2. Our ranking model, BERT-ResNet, outperforms the previously published models and our baselines across all of the sparse datasets. We find that for all sparse datasets, the models that use free text entity representations outperform the models that learn the entity embeddings during training. Among the models utilizing textual information, the deep convolutional methods generally outperform the adaptations of existing neural KG models. BERTResNet outperforms BERT-DeepConv across all datasets, demonstrating that the architectural innovations do improve downstream performance.\nOn the full FB15k-237 dataset, our proposed model is able to achieve competitive results compared to strong baselines. However, the focus of this work is not to achieve state-of-the-art performance on densely connected benchmark datasets such as FB15k-237. These results do, however, allow us to observe the outsized impact of sparsity on models that do not utilize textual information."
    }, {
      "heading" : "6.2 Re-Ranking Performance",
      "text" : "Re-ranking entities without knowledge distillation or ensembling leads to poor results, degrading the\nMRR across most datasets. We note that the performance of our re-ranking model could be limited by our use of a pointwise loss function. Further exploration of pairwise or listwise learning learningto-rank methods is a promising direction for future exploration that could lead to further improvements Guo et al. (2020).\nThe inclusion of either knowledge distillation or ensembling improves performance. Ensembling is particularly important, achieving a statistically significant improvement over the initial rankings across most datasets. Our final setting using both knowledge distillation and ensembling is the only setting to achieve a statistically significant improvement across all four datasets, although using both does not consistently improve performance over ensembling alone.\nA plausible explanation for this is that knowledge distillation improves performance by reducing the divergence between the re-ranker and the teacher, but ensembling can already achieve a similar effect by simply increasing the weight of the teacher in the final prediction. We observe that the weight of the teacher is reduced across all four datasets when knowledge distillation is used which would be consistent with this explanation. Knowledge distillation has also been shown to be useful in situations with noisy labels (Li et al., 2017) which may explain why it was particularly effective for our sparsest dataset, CN-100K, where training with the hard labels led to particularly poor performance."
    }, {
      "heading" : "6.3 Effect of Re-Ranking",
      "text" : "We bin test examples by the in-degree of the tail nodes and compute the MRR within these bins for our model before and after re-ranking. We report this breakdown for the SNOMED CT Core dataset in Figure 3. Our re-ranking stage improves performance uniformly across all levels of sparsity, but it is particularly useful for entities that are rarely seen during training. This is also consistent with the comparatively smaller topline improvement for the densely connected FB15k-237 dataset."
    }, {
      "heading" : "6.4 Model Capacity",
      "text" : "We report the number of trainable parameters for the models that use textual representations along with the train and test set MRR for SNOMED CT Core in Table 3. We observe a monotonic relationship between training and testing performance and note that the shallow models fail to achieve\nour model’s test performance on the training set. This demonstrates that the shallow models lack the complexity to adequately fit the training data. A similar trend held for all datasets except for FB15k237-Sparse whose smaller size reduces the risk of underfitting. This explains the smaller performance improvement for that dataset.\nMalaviya et al. (2020) scaled up BERT-LargeConvTransE to use over 524M trainable parameters, and their model did outperform our smaller BERT-ConvTransE baseline. However, their model still fails to match the performance of either of our deep convolutional models despite using over 15× the number of trainable parameters."
    }, {
      "heading" : "7 Conclusion",
      "text" : "KGs often include many sparsely connected entities where the use of textual entity embeddings is necessary for strong performance. We develop a deep convolutional network that is better-suited for this setting than existing neural models developed on artificially dense benchmark KGs. We also introduce a re-ranking procedure to distill the knowledge from our convolutional model into a student re-ranking network and demonstrate that our procedure is particularly effective at improving the ranking of sparse candidates. We utilize these innovations to develop a KGC pipeline with greater robustness to the realities of KGs and demonstrate\nthe generalizability of our improvements across biomedical, commonsense, and encyclopedic KGs."
    }, {
      "heading" : "Acknowledgments",
      "text" : "This work was supported by the National Science Foundation grant IIS 1917955 and the National Library Medicine of the National Institutes of Health under award number T15 LM007059."
    }, {
      "heading" : "B Evaluation Metrics",
      "text" : "We provide a mathematical formulation for our evaluation metrics. If we denote the set of all facts in the test set as T , then the Mean Rank (MR) is simply computed as\nMR = 1 |T | ∑ xi∈T rank(xi)\nThe Mean Reciprocal Rank (MRR) is computed as\nMRR = 1 |T | ∑ xi∈T\n1\nrank(xi)\nThe Hits at k (H@k) is calculated as\nH@k = 1 |T | ∑ xi∈T I[rank(xi) ≤ k]\nwhere I[P ] is 1 if the condition P is true and is 0 otherwise. When computing rank(xi), we first filter out all positive samples other than the target entity xi. This is commonly referred to as the filtered setting."
    }, {
      "heading" : "C Supplementary Tables",
      "text" : ""
    } ],
    "references" : [ {
      "title" : "TuckER: Tensor factorization for knowledge graph completion",
      "author" : [ "Ivana Balazevic", "Carl Allen", "Timothy Hospedales." ],
      "venue" : "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Confer-",
      "citeRegEx" : "Balazevic et al\\.,? 2019",
      "shortCiteRegEx" : "Balazevic et al\\.",
      "year" : 2019
    }, {
      "title" : "An empirical investigation of statistical significance in NLP",
      "author" : [ "Taylor Berg-Kirkpatrick", "David Burkett", "Dan Klein." ],
      "venue" : "Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Nat-",
      "citeRegEx" : "Berg.Kirkpatrick et al\\.,? 2012",
      "shortCiteRegEx" : "Berg.Kirkpatrick et al\\.",
      "year" : 2012
    }, {
      "title" : "The unified medical language system (UMLS): integrating biomedical terminology",
      "author" : [ "Olivier Bodenreider." ],
      "venue" : "Nucleic acids research, 32:D267–D270.",
      "citeRegEx" : "Bodenreider.,? 2004",
      "shortCiteRegEx" : "Bodenreider.",
      "year" : 2004
    }, {
      "title" : "Freebase: A collaboratively created graph database for structuring human knowledge",
      "author" : [ "Kurt Bollacker", "Colin Evans", "Praveen Paritosh", "Tim Sturge", "Jamie Taylor." ],
      "venue" : "Proceedings of the 2008 ACM SIGMOD International Conference on Management",
      "citeRegEx" : "Bollacker et al\\.,? 2008",
      "shortCiteRegEx" : "Bollacker et al\\.",
      "year" : 2008
    }, {
      "title" : "Question answering with subgraph embeddings",
      "author" : [ "Antoine Bordes", "Sumit Chopra", "Jason Weston." ],
      "venue" : "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 615–620. Association for Compu-",
      "citeRegEx" : "Bordes et al\\.,? 2014a",
      "shortCiteRegEx" : "Bordes et al\\.",
      "year" : 2014
    }, {
      "title" : "Translating embeddings for modeling multirelational data",
      "author" : [ "Antoine Bordes", "Nicolas Usunier", "Alberto GarciaDuran", "Jason Weston", "Oksana Yakhnenko." ],
      "venue" : "Advances in neural information processing systems, pages 2787–2795.",
      "citeRegEx" : "Bordes et al\\.,? 2013",
      "shortCiteRegEx" : "Bordes et al\\.",
      "year" : 2013
    }, {
      "title" : "Open question answering with weakly supervised embedding models",
      "author" : [ "Antoine Bordes", "Jason Weston", "Nicolas Usunier." ],
      "venue" : "Machine Learning and Knowledge Discovery in Databases, pages 165–180, Berlin, Heidelberg. Springer Berlin Heidelberg.",
      "citeRegEx" : "Bordes et al\\.,? 2014b",
      "shortCiteRegEx" : "Bordes et al\\.",
      "year" : 2014
    }, {
      "title" : "BERT: Pre-training of deep bidirectional transformers for language understanding",
      "author" : [ "Jacob Devlin", "Ming-Wei Chang", "Kenton Lee", "Kristina Toutanova." ],
      "venue" : "Proceedings of the 2019 Conference of the North American Chapter of the Association",
      "citeRegEx" : "Devlin et al\\.,? 2019",
      "shortCiteRegEx" : "Devlin et al\\.",
      "year" : 2019
    }, {
      "title" : "Knowledge vault: A web-scale approach to probabilistic knowledge fusion",
      "author" : [ "Xin Dong", "Evgeniy Gabrilovich", "Geremy Heitz", "Wilko Horn", "Ni Lao", "Kevin Murphy", "Thomas Strohmann", "Shaohua Sun", "Wei Zhang." ],
      "venue" : "Proceedings of the 20th ACM",
      "citeRegEx" : "Dong et al\\.,? 2014",
      "shortCiteRegEx" : "Dong et al\\.",
      "year" : 2014
    }, {
      "title" : "SNOMED-CT: The advanced terminology and coding system for eHealth",
      "author" : [ "Kevin Donnelly." ],
      "venue" : "Studies in health technology and informatics, 121:279.",
      "citeRegEx" : "Donnelly.,? 2006",
      "shortCiteRegEx" : "Donnelly.",
      "year" : 2006
    }, {
      "title" : "Domain-specific language model pretraining for biomedical natural language processing",
      "author" : [ "Yu Gu", "Robert Tinn", "Hao Cheng", "Michael Lucas", "Naoto Usuyama", "Xiaodong Liu", "Tristan Naumann", "Jianfeng Gao", "Hoifung Poon." ],
      "venue" : "ArXiv,",
      "citeRegEx" : "Gu et al\\.,? 2020",
      "shortCiteRegEx" : "Gu et al\\.",
      "year" : 2020
    }, {
      "title" : "A deep look into neural ranking models for information retrieval",
      "author" : [ "Jiafeng Guo", "Yixing Fan", "Liang Pang", "Liu Yang", "Qingyao Ai", "Hamed Zamani", "Chen Wu", "W. Bruce Croft", "Xueqi Cheng." ],
      "venue" : "Information Processing Management, 57(6):102067.",
      "citeRegEx" : "Guo et al\\.,? 2020",
      "shortCiteRegEx" : "Guo et al\\.",
      "year" : 2020
    }, {
      "title" : "Delving deep into rectifiers: Surpassing human-level performance on imagenet classification",
      "author" : [ "K. He", "X. Zhang", "S. Ren", "J. Sun." ],
      "venue" : "2015 IEEE International Conference on Computer Vision (ICCV), pages 1026–1034.",
      "citeRegEx" : "He et al\\.,? 2015",
      "shortCiteRegEx" : "He et al\\.",
      "year" : 2015
    }, {
      "title" : "Deep residual learning for image recognition",
      "author" : [ "K. He", "X. Zhang", "S. Ren", "J. Sun." ],
      "venue" : "2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 770–778.",
      "citeRegEx" : "He et al\\.,? 2016",
      "shortCiteRegEx" : "He et al\\.",
      "year" : 2016
    }, {
      "title" : "Identity mappings in deep residual networks",
      "author" : [ "Kaiming He", "Xiangyu Zhang", "Shaoqing Ren", "Jian Sun." ],
      "venue" : "2016 European Conference on Computer Vision (ECCV).",
      "citeRegEx" : "He et al\\.,? 2016",
      "shortCiteRegEx" : "He et al\\.",
      "year" : 2016
    }, {
      "title" : "Distilling the knowledge in a neural network",
      "author" : [ "Geoffrey Hinton", "Oriol Vinyals", "Jeffrey Dean." ],
      "venue" : "NIPS Deep Learning and Representation Learning Workshop.",
      "citeRegEx" : "Hinton et al\\.,? 2015",
      "shortCiteRegEx" : "Hinton et al\\.",
      "year" : 2015
    }, {
      "title" : "A simple sequentially rejective multiple test procedure",
      "author" : [ "Sture Holm." ],
      "venue" : "Scandinavian Journal of Statistics, 6(2):65–70.",
      "citeRegEx" : "Holm.,? 1979",
      "shortCiteRegEx" : "Holm.",
      "year" : 1979
    }, {
      "title" : "Densely connected convolutional networks",
      "author" : [ "Gao Huang", "Zhuang Liu", "Laurens van der Maaten", "Kilian Q Weinberger." ],
      "venue" : "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition.",
      "citeRegEx" : "Huang et al\\.,? 2017",
      "shortCiteRegEx" : "Huang et al\\.",
      "year" : 2017
    }, {
      "title" : "Convolutional networks with dense connectivity",
      "author" : [ "Gao Huang", "Zhuang Liu", "Geoff Pleiss", "Laurens Van Der Maaten", "Kilian Weinberger." ],
      "venue" : "IEEE Transactions on Pattern Analysis and Machine Intelligence.",
      "citeRegEx" : "Huang et al\\.,? 2019",
      "shortCiteRegEx" : "Huang et al\\.",
      "year" : 2019
    }, {
      "title" : "Auditing the Semantic Completeness of SNOMED CT Using Formal Concept Analysis",
      "author" : [ "Guoqian Jiang", "Christopher G. Chute." ],
      "venue" : "Journal of the American Medical Informatics Association, 16(1):89–102.",
      "citeRegEx" : "Jiang and Chute.,? 2009",
      "shortCiteRegEx" : "Jiang and Chute.",
      "year" : 2009
    }, {
      "title" : "How can we know what language models know? Transactions of the Association for Computational Linguistics, 8:423–438",
      "author" : [ "Zhengbao Jiang", "Frank F. Xu", "Jun Araki", "Graham Neubig" ],
      "venue" : null,
      "citeRegEx" : "Jiang et al\\.,? \\Q2020\\E",
      "shortCiteRegEx" : "Jiang et al\\.",
      "year" : 2020
    }, {
      "title" : "Logic-based assessment of the compatibility of UMLS ontology sources",
      "author" : [ "Ernesto Jiménez-Ruiz", "Bernardo Cuenca Grau", "Ian Horrocks", "Rafael Berlanga." ],
      "venue" : "Journal of Biomedical Semantics, 2(1):S2.",
      "citeRegEx" : "Jiménez.Ruiz et al\\.,? 2011",
      "shortCiteRegEx" : "Jiménez.Ruiz et al\\.",
      "year" : 2011
    }, {
      "title" : "Adam: A method for stochastic optimization",
      "author" : [ "Diederik P. Kingma", "Jimmy Ba." ],
      "venue" : "3rd International Conference on Learning Representations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings.",
      "citeRegEx" : "Kingma and Ba.,? 2015",
      "shortCiteRegEx" : "Kingma and Ba.",
      "year" : 2015
    }, {
      "title" : "Imagenet classification with deep convolutional neural networks",
      "author" : [ "Alex Krizhevsky", "Ilya Sutskever", "Geoffrey E. Hinton." ],
      "venue" : "Proceedings of the 25th International Conference on Neural Information Processing Systems - Volume 1, NIPS’12, page",
      "citeRegEx" : "Krizhevsky et al\\.,? 2012",
      "shortCiteRegEx" : "Krizhevsky et al\\.",
      "year" : 2012
    }, {
      "title" : "Commonsense knowledge base completion",
      "author" : [ "Xiang Li", "Aynaz Taheri", "Lifu Tu", "Kevin Gimpel." ],
      "venue" : "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1445–1455, Berlin, Germany.",
      "citeRegEx" : "Li et al\\.,? 2016",
      "shortCiteRegEx" : "Li et al\\.",
      "year" : 2016
    }, {
      "title" : "Learning from noisy labels with distillation",
      "author" : [ "Y. Li", "J. Yang", "Y. Song", "L. Cao", "J. Luo", "L. Li." ],
      "venue" : "2017 IEEE International Conference on Computer Vision (ICCV), pages 1928–1936.",
      "citeRegEx" : "Li et al\\.,? 2017",
      "shortCiteRegEx" : "Li et al\\.",
      "year" : 2017
    }, {
      "title" : "Analogical inference for multi-relational embeddings",
      "author" : [ "Hanxiao Liu", "Yuexin Wu", "Yiming Yang." ],
      "venue" : "Proceedings of the 34th International Conference on Machine Learning, volume 70 of Proceedings of Machine Learning Research, pages",
      "citeRegEx" : "Liu et al\\.,? 2017",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2017
    }, {
      "title" : "Decoupled weight decay regularization",
      "author" : [ "Ilya Loshchilov", "Frank Hutter." ],
      "venue" : "International Conference on Learning Representations.",
      "citeRegEx" : "Loshchilov and Hutter.,? 2019",
      "shortCiteRegEx" : "Loshchilov and Hutter.",
      "year" : 2019
    }, {
      "title" : "Knowledge graph inference for spoken dialog systems",
      "author" : [ "Y. Ma", "P.A. Crook", "R. Sarikaya", "E. Fosler-Lussier." ],
      "venue" : "2015 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 5346–5350.",
      "citeRegEx" : "Ma et al\\.,? 2015",
      "shortCiteRegEx" : "Ma et al\\.",
      "year" : 2015
    }, {
      "title" : "Commonsense knowledge base completion with structural and semantic context",
      "author" : [ "Chaitanya Malaviya", "Chandra Bhagavatula", "Antoine Bosselut", "Yejin Choi." ],
      "venue" : "Proceedings of the 34th AAAI Conference on Artificial Intelligence.",
      "citeRegEx" : "Malaviya et al\\.,? 2020",
      "shortCiteRegEx" : "Malaviya et al\\.",
      "year" : 2020
    }, {
      "title" : "Reranking for Efficient TransformerBased Answer Selection, page 1577–1580",
      "author" : [ "Yoshitomo Matsubara", "Thuy Vu", "Alessandro Moschitti." ],
      "venue" : "Association for Computing Machinery, New York, NY, USA.",
      "citeRegEx" : "Matsubara et al\\.,? 2020",
      "shortCiteRegEx" : "Matsubara et al\\.",
      "year" : 2020
    }, {
      "title" : "Distant supervision for relation extraction without labeled data",
      "author" : [ "Mike Mintz", "Steven Bills", "Rion Snow", "Daniel Jurafsky." ],
      "venue" : "Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference",
      "citeRegEx" : "Mintz et al\\.,? 2009",
      "shortCiteRegEx" : "Mintz et al\\.",
      "year" : 2009
    }, {
      "title" : "Rectified linear units improve restricted boltzmann machines",
      "author" : [ "Vinod Nair", "Geoffrey E. Hinton." ],
      "venue" : "Proceedings of the 27th International Conference on International Conference on Machine Learning, ICML’10, pages 807–814, USA. Omnipress.",
      "citeRegEx" : "Nair and Hinton.,? 2010",
      "shortCiteRegEx" : "Nair and Hinton.",
      "year" : 2010
    }, {
      "title" : "Holographic embeddings of knowledge graphs",
      "author" : [ "Maximilian Nickel", "Lorenzo Rosasco", "Tomaso Poggio." ],
      "venue" : "Proceedings of the Thirtieth AAAI Conference on Artificial Intelligence, AAAI’16, pages 1955–1961. AAAI Press.",
      "citeRegEx" : "Nickel et al\\.,? 2016",
      "shortCiteRegEx" : "Nickel et al\\.",
      "year" : 2016
    }, {
      "title" : "A three-way model for collective learning on multi-relational data",
      "author" : [ "Maximilian Nickel", "Volker Tresp", "Hans-Peter Kriegel." ],
      "venue" : "Proceedings of the 28th International Conference on International Conference on Machine Learning, ICML’11, page",
      "citeRegEx" : "Nickel et al\\.,? 2011",
      "shortCiteRegEx" : "Nickel et al\\.",
      "year" : 2011
    }, {
      "title" : "Passage re-ranking with BERT",
      "author" : [ "Rodrigo Nogueira", "Kyunghyun Cho." ],
      "venue" : "arXiv preprint arXiv:1901.04085.",
      "citeRegEx" : "Nogueira and Cho.,? 2019",
      "shortCiteRegEx" : "Nogueira and Cho.",
      "year" : 2019
    }, {
      "title" : "Personalized re-ranking for recommendation",
      "author" : [ "Changhua Pei", "Yi Zhang", "Yongfeng Zhang", "Fei Sun", "Xiao Lin", "Hanxiao Sun", "Jian Wu", "Peng Jiang", "Junfeng Ge", "Wenwu Ou", "Dan Pei." ],
      "venue" : "Proceedings of the 13th ACM Conference on Recommender",
      "citeRegEx" : "Pei et al\\.,? 2019",
      "shortCiteRegEx" : "Pei et al\\.",
      "year" : 2019
    }, {
      "title" : "To tune or not to tune? adapting pretrained representations to diverse tasks",
      "author" : [ "Matthew E. Peters", "Sebastian Ruder", "Noah A. Smith." ],
      "venue" : "Proceedings of the 4th Workshop on Representation Learning for NLP (RepL4NLP-2019), pages 7–14, Flo-",
      "citeRegEx" : "Peters et al\\.,? 2019",
      "shortCiteRegEx" : "Peters et al\\.",
      "year" : 2019
    }, {
      "title" : "Language models as knowledge bases",
      "author" : [ "Fabio Petroni", "Tim Rocktäschel", "Sebastian Riedel", "Patrick Lewis", "Anton Bakhtin", "Yuxiang Wu", "Alexander Miller" ],
      "venue" : "In Proceedings of the 2019 Conference on Empirical Methods in Natural Language",
      "citeRegEx" : "Petroni et al\\.,? \\Q2019\\E",
      "shortCiteRegEx" : "Petroni et al\\.",
      "year" : 2019
    }, {
      "title" : "Sparsity and noise: Where knowledge graph embeddings fall short",
      "author" : [ "Jay Pujara", "Eriq Augustine", "Lise Getoor." ],
      "venue" : "Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 1751–1756, Copenhagen, Den-",
      "citeRegEx" : "Pujara et al\\.,? 2017",
      "shortCiteRegEx" : "Pujara et al\\.",
      "year" : 2017
    }, {
      "title" : "Yago: A multilingual knowledge base from wikipedia, wordnet, and geonames",
      "author" : [ "Thomas Rebele", "Fabian Suchanek", "Johannes Hoffart", "Joanna Biega", "Erdal Kuzey", "Gerhard Weikum." ],
      "venue" : "International semantic web conference, pages 177–185. Springer.",
      "citeRegEx" : "Rebele et al\\.,? 2016",
      "shortCiteRegEx" : "Rebele et al\\.",
      "year" : 2016
    }, {
      "title" : "A primer in bertology: What we know about how bert works",
      "author" : [ "Anna Rogers", "O. Kovaleva", "Anna Rumshisky." ],
      "venue" : "ArXiv, abs/2002.12327.",
      "citeRegEx" : "Rogers et al\\.,? 2020",
      "shortCiteRegEx" : "Rogers et al\\.",
      "year" : 2020
    }, {
      "title" : "You can teach an old dog new tricks! on training knowledge graph embeddings",
      "author" : [ "Daniel Ruffinelli", "Samuel Broscheit", "Rainer Gemulla." ],
      "venue" : "International Conference on Learning Representations.",
      "citeRegEx" : "Ruffinelli et al\\.,? 2020",
      "shortCiteRegEx" : "Ruffinelli et al\\.",
      "year" : 2020
    }, {
      "title" : "Representation learning of knowledge graphs with entity descriptions",
      "author" : [ "Jia Jia Huanbo Luan Maosong Sun Ruobing Xie", "Zhiyuan Liu." ],
      "venue" : "The 30th AAAI Conference on Artificial Intelligence.",
      "citeRegEx" : "Xie and Liu.,? 2016",
      "shortCiteRegEx" : "Xie and Liu.",
      "year" : 2016
    }, {
      "title" : "End-to-end structure-aware convolutional networks for knowledge base completion",
      "author" : [ "Chao Shang", "Yun Tang", "Jing Huang", "Jinbo Bi", "Xiaodong He", "Bowen Zhou." ],
      "venue" : "CoRR, abs/1811.04441.",
      "citeRegEx" : "Shang et al\\.,? 2018",
      "shortCiteRegEx" : "Shang et al\\.",
      "year" : 2018
    }, {
      "title" : "Very deep convolutional networks for large-scale image recognition",
      "author" : [ "Karen Simonyan", "Andrew Zisserman." ],
      "venue" : "3rd International Conference on Learning Representations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceed-",
      "citeRegEx" : "Simonyan and Zisserman.,? 2015",
      "shortCiteRegEx" : "Simonyan and Zisserman.",
      "year" : 2015
    }, {
      "title" : "Reasoning with neural tensor networks for knowledge base completion",
      "author" : [ "Richard Socher", "Danqi Chen", "Christopher D. Manning", "Andrew Y. Ng." ],
      "venue" : "Proceedings of the 26th International Conference on Neural Information Processing Systems - Volume 1,",
      "citeRegEx" : "Socher et al\\.,? 2013",
      "shortCiteRegEx" : "Socher et al\\.",
      "year" : 2013
    }, {
      "title" : "ConceptNet 5: A Large Semantic Network for Relational Knowledge, pages 161–176",
      "author" : [ "Robyn Speer", "Catherine Havasi." ],
      "venue" : "Springer Berlin Heidelberg, Berlin, Heidelberg.",
      "citeRegEx" : "Speer and Havasi.,? 2013",
      "shortCiteRegEx" : "Speer and Havasi.",
      "year" : 2013
    }, {
      "title" : "Rotate: Knowledge graph embedding by relational rotation in complex space",
      "author" : [ "Zhiqing Sun", "Zhi-Hong Deng", "Jian-Yun Nie", "Jian Tang." ],
      "venue" : "International Conference on Learning Representations.",
      "citeRegEx" : "Sun et al\\.,? 2019",
      "shortCiteRegEx" : "Sun et al\\.",
      "year" : 2019
    }, {
      "title" : "Efficient object localization using convolutional networks",
      "author" : [ "J. Tompson", "R. Goroshin", "A. Jain", "Y. LeCun", "C. Bregler." ],
      "venue" : "2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 648–656.",
      "citeRegEx" : "Tompson et al\\.,? 2015",
      "shortCiteRegEx" : "Tompson et al\\.",
      "year" : 2015
    }, {
      "title" : "Observed versus latent features for knowledge base and text inference",
      "author" : [ "Kristina Toutanova", "Danqi Chen." ],
      "venue" : "Proceedings of the 3rd Workshop on Continuous Vector Space Models and their Compositionality, pages 57–66, Beijing, China. Association",
      "citeRegEx" : "Toutanova and Chen.,? 2015",
      "shortCiteRegEx" : "Toutanova and Chen.",
      "year" : 2015
    }, {
      "title" : "Complex embeddings for simple link prediction",
      "author" : [ "Théo Trouillon", "Johannes Welbl", "Sebastian Riedel", "Éric Gaussier", "Guillaume Bouchard." ],
      "venue" : "Proceedings of the 33rd International Conference on International Conference on Machine Learning - Volume",
      "citeRegEx" : "Trouillon et al\\.,? 2016",
      "shortCiteRegEx" : "Trouillon et al\\.",
      "year" : 2016
    }, {
      "title" : "Reside: Improving distantly-supervised neural relation extraction using side information",
      "author" : [ "Shikhar Vashishth", "Rishabh Joshi", "Sai Suman Prayaga", "Chiranjib Bhattacharyya", "Partha Talukdar." ],
      "venue" : "Proceedings of the 2018 Conference on Empirical Methods",
      "citeRegEx" : "Vashishth et al\\.,? 2018",
      "shortCiteRegEx" : "Vashishth et al\\.",
      "year" : 2018
    }, {
      "title" : "Interacte: Improving convolution-based knowledge graph embeddings by increasing feature interactions",
      "author" : [ "Shikhar Vashishth", "Soumya Sanyal", "Vikram Nitin", "Nilesh Agrawal", "Partha Talukdar." ],
      "venue" : "Proceedings of the 34th AAAI Conference on Ar-",
      "citeRegEx" : "Vashishth et al\\.,? 2020a",
      "shortCiteRegEx" : "Vashishth et al\\.",
      "year" : 2020
    }, {
      "title" : "Composition-based multirelational graph convolutional networks",
      "author" : [ "Shikhar Vashishth", "Soumya Sanyal", "Vikram Nitin", "Partha Talukdar." ],
      "venue" : "International Conference on Learning Representations.",
      "citeRegEx" : "Vashishth et al\\.,? 2020b",
      "shortCiteRegEx" : "Vashishth et al\\.",
      "year" : 2020
    }, {
      "title" : "A cascade ranking model for efficient ranked retrieval",
      "author" : [ "Lidan Wang", "Jimmy Lin", "Donald Metzler." ],
      "venue" : "Proceedings of the 34th International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR ’11, page 105–114, New",
      "citeRegEx" : "Wang et al\\.,? 2011",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2011
    }, {
      "title" : "Omnidirectional CNN for visual place recognition and navigation",
      "author" : [ "Tsun-Hsuan Wang", "Hung-Jui Huang", "Juan-Ting Lin", "Chan-Wei Hu", "Kuo-Hao Zeng", "Min Sun." ],
      "venue" : "arXiv preprint arXiv:1803.04228.",
      "citeRegEx" : "Wang et al\\.,? 2018",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2018
    }, {
      "title" : "Transformers: State-of-the-art natural language processing",
      "author" : [ "Teven Le Scao", "Sylvain Gugger", "Mariama Drame", "Quentin Lhoest", "Alexander Rush." ],
      "venue" : "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing:",
      "citeRegEx" : "Scao et al\\.,? 2020",
      "shortCiteRegEx" : "Scao et al\\.",
      "year" : 2020
    }, {
      "title" : "Ssp: Semantic space projection for knowledge graph embedding with text descriptions",
      "author" : [ "Han Xiao", "Minlie Huang", "Xiaoyan Zhu" ],
      "venue" : null,
      "citeRegEx" : "Xiao et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Xiao et al\\.",
      "year" : 2016
    }, {
      "title" : "Embedding entities and relations for learning and inference in knowledge bases",
      "author" : [ "Bishan Yang", "Scott Wen-tau Yih", "Xiaodong He", "Jianfeng Gao", "Li Deng." ],
      "venue" : "Proceedings of the International Conference on Learning Representations (ICLR) 2015.",
      "citeRegEx" : "Yang et al\\.,? 2015",
      "shortCiteRegEx" : "Yang et al\\.",
      "year" : 2015
    }, {
      "title" : "KG-BERT: BERT for knowledge graph completion",
      "author" : [ "Liang Yao", "Chengsheng Mao", "Yuan Luo." ],
      "venue" : "CoRR, abs/1909.03193.",
      "citeRegEx" : "Yao et al\\.,? 2019",
      "shortCiteRegEx" : "Yao et al\\.",
      "year" : 2019
    }, {
      "title" : "Collaborative knowledge base embedding for recommender systems",
      "author" : [ "Fuzheng Zhang", "Nicholas Jing Yuan", "Defu Lian", "Xing Xie", "Wei-Ying Ma." ],
      "venue" : "Proceedings of the 22Nd ACM SIGKDD International Conference on Knowledge Discovery and",
      "citeRegEx" : "Zhang et al\\.,? 2016",
      "shortCiteRegEx" : "Zhang et al\\.",
      "year" : 2016
    }, {
      "title" : "2019) and train the model with a learning rate of 1e-3. We use label smoothing with a value of 0.1, clip gradients to a max value of 1, and regularize the model using weight decay with a weight of 1e-4",
      "author" : [ "Hutter" ],
      "venue" : null,
      "citeRegEx" : "Hutter,? \\Q2019\\E",
      "shortCiteRegEx" : "Hutter",
      "year" : 2019
    }, {
      "title" : "We utilize the hyperparameters reported in the original papers and conduct a grid search to tune the embedding dimension from [100",
      "author" : [ "Dettmers" ],
      "venue" : null,
      "citeRegEx" : "Dettmers,? \\Q2020\\E",
      "shortCiteRegEx" : "Dettmers",
      "year" : 2020
    } ],
    "referenceMentions" : [ {
      "referenceID" : 28,
      "context" : ", 2014a,b), dialog systems (Ma et al., 2015), relation extraction (Mintz et al.",
      "startOffset" : 27,
      "endOffset" : 44
    }, {
      "referenceID" : 31,
      "context" : ", 2015), relation extraction (Mintz et al., 2009; Vashishth et al., 2018), and recommender systems (Zhang et al.",
      "startOffset" : 29,
      "endOffset" : 73
    }, {
      "referenceID" : 52,
      "context" : ", 2015), relation extraction (Mintz et al., 2009; Vashishth et al., 2018), and recommender systems (Zhang et al.",
      "startOffset" : 29,
      "endOffset" : 73
    }, {
      "referenceID" : 8,
      "context" : "However, because scaling the collection of facts to provide coverage for all the true relations that hold between entities is difficult, most existing KGs are incomplete (Dong et al., 2014), limiting their utility for downstream applications.",
      "startOffset" : 170,
      "endOffset" : 189
    }, {
      "referenceID" : 5,
      "context" : "The increased interest in KGC has led to the curation of a number of benchmark datasets such as FB15K (Bordes et al., 2013), WN18 (Bordes et al.",
      "startOffset" : 102,
      "endOffset" : 123
    }, {
      "referenceID" : 5,
      "context" : ", 2013), WN18 (Bordes et al., 2013), FB15k-237 (Toutanova and Chen, 2015), and YAGO3-10 (Rebele et al.",
      "startOffset" : 14,
      "endOffset" : 35
    }, {
      "referenceID" : 50,
      "context" : ", 2013), FB15k-237 (Toutanova and Chen, 2015), and YAGO3-10 (Rebele et al.",
      "startOffset" : 19,
      "endOffset" : 45
    }, {
      "referenceID" : 40,
      "context" : ", 2013), FB15k-237 (Toutanova and Chen, 2015), and YAGO3-10 (Rebele et al., 2016) that have been the focus of most of the work in this area.",
      "startOffset" : 60,
      "endOffset" : 81
    }, {
      "referenceID" : 40,
      "context" : "For instance, FB15K includes only entities with at least 100 links in Freebase, while YAGO3-10 is limited to only include entities in YAGO3 (Rebele et al., 2016) that have at least 10 relations.",
      "startOffset" : 140,
      "endOffset" : 161
    }, {
      "referenceID" : 39,
      "context" : "Real KGs are not as uniformly dense as these benchmark datasets and have many sparsely connected entities (Pujara et al., 2017).",
      "startOffset" : 106,
      "endOffset" : 127
    }, {
      "referenceID" : 29,
      "context" : "Textual entity identifiers can be used to develop entity embeddings that are more robust to sparsity (Malaviya et al., 2020).",
      "startOffset" : 101,
      "endOffset" : 124
    }, {
      "referenceID" : 60,
      "context" : "It has also been shown that textual triplet representations can be used with BERT for triplet classification (Yao et al., 2019).",
      "startOffset" : 109,
      "endOffset" : 127
    }, {
      "referenceID" : 34,
      "context" : "Knowledge Graph Completion: KGC models typically learn entity and relation embeddings based on known facts (Nickel et al., 2011; Bordes et al., 2013; Yang et al., 2015) and use the learned embeddings to score potential candidate triples.",
      "startOffset" : 107,
      "endOffset" : 168
    }, {
      "referenceID" : 5,
      "context" : "Knowledge Graph Completion: KGC models typically learn entity and relation embeddings based on known facts (Nickel et al., 2011; Bordes et al., 2013; Yang et al., 2015) and use the learned embeddings to score potential candidate triples.",
      "startOffset" : 107,
      "endOffset" : 168
    }, {
      "referenceID" : 59,
      "context" : "Knowledge Graph Completion: KGC models typically learn entity and relation embeddings based on known facts (Nickel et al., 2011; Bordes et al., 2013; Yang et al., 2015) and use the learned embeddings to score potential candidate triples.",
      "startOffset" : 107,
      "endOffset" : 168
    }, {
      "referenceID" : 33,
      "context" : "cent work includes both non-neural (Nickel et al., 2016; Trouillon et al., 2016; Liu et al., 2017; Sun et al., 2019) and neural (Socher et al.",
      "startOffset" : 35,
      "endOffset" : 116
    }, {
      "referenceID" : 51,
      "context" : "cent work includes both non-neural (Nickel et al., 2016; Trouillon et al., 2016; Liu et al., 2017; Sun et al., 2019) and neural (Socher et al.",
      "startOffset" : 35,
      "endOffset" : 116
    }, {
      "referenceID" : 26,
      "context" : "cent work includes both non-neural (Nickel et al., 2016; Trouillon et al., 2016; Liu et al., 2017; Sun et al., 2019) and neural (Socher et al.",
      "startOffset" : 35,
      "endOffset" : 116
    }, {
      "referenceID" : 48,
      "context" : "cent work includes both non-neural (Nickel et al., 2016; Trouillon et al., 2016; Liu et al., 2017; Sun et al., 2019) and neural (Socher et al.",
      "startOffset" : 35,
      "endOffset" : 116
    }, {
      "referenceID" : 46,
      "context" : ", 2019) and neural (Socher et al., 2013; Dong et al., 2014; Dettmers et al., 2018; Vashishth et al., 2020b) approaches for embedding KGs.",
      "startOffset" : 19,
      "endOffset" : 107
    }, {
      "referenceID" : 8,
      "context" : ", 2019) and neural (Socher et al., 2013; Dong et al., 2014; Dettmers et al., 2018; Vashishth et al., 2020b) approaches for embedding KGs.",
      "startOffset" : 19,
      "endOffset" : 107
    }, {
      "referenceID" : 54,
      "context" : ", 2019) and neural (Socher et al., 2013; Dong et al., 2014; Dettmers et al., 2018; Vashishth et al., 2020b) approaches for embedding KGs.",
      "startOffset" : 19,
      "endOffset" : 107
    }, {
      "referenceID" : 46,
      "context" : "Prior work has effectively utilized entity names or descriptions to aid KGC (Socher et al., 2013; Ruobing Xie, 2016; Xiao et al., 2016).",
      "startOffset" : 76,
      "endOffset" : 135
    }, {
      "referenceID" : 58,
      "context" : "Prior work has effectively utilized entity names or descriptions to aid KGC (Socher et al., 2013; Ruobing Xie, 2016; Xiao et al., 2016).",
      "startOffset" : 76,
      "endOffset" : 135
    }, {
      "referenceID" : 7,
      "context" : "They adapt an existing KGC model to utilize BERT (Devlin et al., 2019) embeddings.",
      "startOffset" : 49,
      "endOffset" : 70
    }, {
      "referenceID" : 38,
      "context" : "BERT as a Knowledge Base: Recent work (Petroni et al., 2019; Jiang et al., 2020; Rogers et al., 2020) has utilized the masked-languagemodeling (MLM) objective to probe the knowledge contained within pre-trained models using fill-in-the-blank prompts (e.",
      "startOffset" : 38,
      "endOffset" : 101
    }, {
      "referenceID" : 20,
      "context" : "BERT as a Knowledge Base: Recent work (Petroni et al., 2019; Jiang et al., 2020; Rogers et al., 2020) has utilized the masked-languagemodeling (MLM) objective to probe the knowledge contained within pre-trained models using fill-in-the-blank prompts (e.",
      "startOffset" : 38,
      "endOffset" : 101
    }, {
      "referenceID" : 41,
      "context" : "BERT as a Knowledge Base: Recent work (Petroni et al., 2019; Jiang et al., 2020; Rogers et al., 2020) has utilized the masked-languagemodeling (MLM) objective to probe the knowledge contained within pre-trained models using fill-in-the-blank prompts (e.",
      "startOffset" : 38,
      "endOffset" : 101
    }, {
      "referenceID" : 30,
      "context" : "Reranking has since been successfully applied across many retrieval tasks (Matsubara et al., 2020; Pei et al., 2019; Nogueira and Cho, 2019).",
      "startOffset" : 74,
      "endOffset" : 140
    }, {
      "referenceID" : 36,
      "context" : "Reranking has since been successfully applied across many retrieval tasks (Matsubara et al., 2020; Pei et al., 2019; Nogueira and Cho, 2019).",
      "startOffset" : 74,
      "endOffset" : 140
    }, {
      "referenceID" : 35,
      "context" : "Reranking has since been successfully applied across many retrieval tasks (Matsubara et al., 2020; Pei et al., 2019; Nogueira and Cho, 2019).",
      "startOffset" : 74,
      "endOffset" : 140
    }, {
      "referenceID" : 15,
      "context" : "Knowledge Distillation: Knowledge distillation is a popular technique that is often used for model compression where a large, high-capacity teacher is used to train a simpler student network (Hinton et al., 2015).",
      "startOffset" : 191,
      "endOffset" : 212
    }, {
      "referenceID" : 9,
      "context" : "For constructing SNOMED CT Core, we use the knowledge graph defined by SNOMED CT (Donnelly, 2006), which is contained within the Unified Medical Language System (UMLS) (Bodenreider, 2004).",
      "startOffset" : 81,
      "endOffset" : 97
    }, {
      "referenceID" : 2,
      "context" : "For constructing SNOMED CT Core, we use the knowledge graph defined by SNOMED CT (Donnelly, 2006), which is contained within the Unified Medical Language System (UMLS) (Bodenreider, 2004).",
      "startOffset" : 168,
      "endOffset" : 187
    }, {
      "referenceID" : 21,
      "context" : "SNOMED CT is well-maintained and is one of the most comprehensive knowledge bases contained within the UMLS (Jiménez-Ruiz et al., 2011; Jiang and Chute, 2009).",
      "startOffset" : 108,
      "endOffset" : 158
    }, {
      "referenceID" : 19,
      "context" : "SNOMED CT is well-maintained and is one of the most comprehensive knowledge bases contained within the UMLS (Jiménez-Ruiz et al., 2011; Jiang and Chute, 2009).",
      "startOffset" : 108,
      "endOffset" : 158
    }, {
      "referenceID" : 29,
      "context" : "Unlike some other KG datasets that filter out inverse relations, we divide our dataset in such a way that this is not necessary; our dataset already includes inverse relations, and they do not need to be manually added for training and evaluation as is standard practice (Dettmers et al., 2018; Malaviya et al., 2020).",
      "startOffset" : 271,
      "endOffset" : 317
    }, {
      "referenceID" : 50,
      "context" : "The FB15k-237 (Toutanova and Chen, 2015) dataset contains encyclopedic knowledge about the world, e.",
      "startOffset" : 14,
      "endOffset" : 40
    }, {
      "referenceID" : 5,
      "context" : "FB15K (Bordes et al., 2013), the precursor to FB15k-237, was curated to only include entities with at least 100 links in Freebase (Bollacker et al.",
      "startOffset" : 6,
      "endOffset" : 27
    }, {
      "referenceID" : 3,
      "context" : ", 2013), the precursor to FB15k-237, was curated to only include entities with at least 100 links in Freebase (Bollacker et al., 2008).",
      "startOffset" : 110,
      "endOffset" : 134
    }, {
      "referenceID" : 47,
      "context" : "ConceptNet (Speer and Havasi, 2013) is a KG that contains commonsense knowledge about the world such as the fact (go to dentist, motivatedBy, prevent tooth decay).",
      "startOffset" : 11,
      "endOffset" : 35
    }, {
      "referenceID" : 24,
      "context" : "We utilize ConceptNet-100k (CN-100K) (Li et al., 2016) which consists of the",
      "startOffset" : 37,
      "endOffset" : 54
    }, {
      "referenceID" : 7,
      "context" : "We first extract feature representations from BERT (Devlin et al., 2019) to develop textual entity embeddings.",
      "startOffset" : 51,
      "endOffset" : 72
    }, {
      "referenceID" : 29,
      "context" : "We follow recent work (Dettmers et al., 2018; Malaviya et al., 2020) and consider both forward and inverse relations (e.",
      "startOffset" : 22,
      "endOffset" : 68
    }, {
      "referenceID" : 7,
      "context" : "We utilize BERT (Devlin et al., 2019) to develop entity embeddings that are invariant to the connectivity of the KG.",
      "startOffset" : 16,
      "endOffset" : 37
    }, {
      "referenceID" : 10,
      "context" : "For SNOMED CT Core KG, we utilize PubMedBERT (Gu et al., 2020) which is better suited for the biomedical terminology in the UMLS.",
      "startOffset" : 45,
      "endOffset" : 62
    }, {
      "referenceID" : 23,
      "context" : "Inspired by the success of deep convolutional models in computer vision (Krizhevsky et al., 2012; Simonyan and Zisserman, 2015; He et al., 2016; Huang et al., 2019, 2017), we develop a knowledge base completion model based on the seminal ResNet architecture (He et al.",
      "startOffset" : 72,
      "endOffset" : 170
    }, {
      "referenceID" : 45,
      "context" : "Inspired by the success of deep convolutional models in computer vision (Krizhevsky et al., 2012; Simonyan and Zisserman, 2015; He et al., 2016; Huang et al., 2019, 2017), we develop a knowledge base completion model based on the seminal ResNet architecture (He et al.",
      "startOffset" : 72,
      "endOffset" : 170
    }, {
      "referenceID" : 13,
      "context" : "Inspired by the success of deep convolutional models in computer vision (Krizhevsky et al., 2012; Simonyan and Zisserman, 2015; He et al., 2016; Huang et al., 2019, 2017), we develop a knowledge base completion model based on the seminal ResNet architecture (He et al.",
      "startOffset" : 72,
      "endOffset" : 170
    }, {
      "referenceID" : 13,
      "context" : ", 2019, 2017), we develop a knowledge base completion model based on the seminal ResNet architecture (He et al., 2016) that is sufficiently expressive to model complex interactions between the BERT feature space and the relation embeddings.",
      "startOffset" : 101,
      "endOffset" : 118
    }, {
      "referenceID" : 13,
      "context" : "We add residual connections to each bottleneck block which improves training for deep networks (He et al., 2016).",
      "startOffset" : 95,
      "endOffset" : 112
    }, {
      "referenceID" : 32,
      "context" : "We apply batch normalization followed by a ReLU nonlinearity (Nair and Hinton, 2010) before each convolutional layer (He et al.",
      "startOffset" : 61,
      "endOffset" : 84
    }, {
      "referenceID" : 13,
      "context" : "We apply batch normalization followed by a ReLU nonlinearity (Nair and Hinton, 2010) before each convolutional layer (He et al., 2016) .",
      "startOffset" : 117,
      "endOffset" : 134
    }, {
      "referenceID" : 56,
      "context" : "We utilize circular padding (Wang et al., 2018; Vashishth et al., 2020a) with the 3×3 convolutions to maintain the spatial size of the feature map and use a stride of 1 for all convolutions.",
      "startOffset" : 28,
      "endOffset" : 72
    }, {
      "referenceID" : 53,
      "context" : "We utilize circular padding (Wang et al., 2018; Vashishth et al., 2020a) with the 3×3 convolutions to maintain the spatial size of the feature map and use a stride of 1 for all convolutions.",
      "startOffset" : 28,
      "endOffset" : 72
    }, {
      "referenceID" : 13,
      "context" : "For the bottleneck blocks that double the dimensionality of the feature map, we utilize a projection shortcut for the residual connection (He et al., 2016).",
      "startOffset" : 138,
      "endOffset" : 155
    }, {
      "referenceID" : 12,
      "context" : "We then apply a fully connected layer followed by a PReLU nonlinearity (He et al., 2015) to project the feature vector",
      "startOffset" : 71,
      "endOffset" : 88
    }, {
      "referenceID" : 22,
      "context" : "We train our model using the Adam optimizer (Kingma and Ba, 2015) with decoupled weight decay regularization (Loshchilov and Hutter, 2019) and label smoothing.",
      "startOffset" : 44,
      "endOffset" : 65
    }, {
      "referenceID" : 27,
      "context" : "We train our model using the Adam optimizer (Kingma and Ba, 2015) with decoupled weight decay regularization (Loshchilov and Hutter, 2019) and label smoothing.",
      "startOffset" : 109,
      "endOffset" : 138
    }, {
      "referenceID" : 37,
      "context" : "The re-ranking setting also enables us to directly finetune BERT which often improves performance (Peters et al., 2019).",
      "startOffset" : 98,
      "endOffset" : 119
    }, {
      "referenceID" : 22,
      "context" : "We train our student network using the Adam optimizer (Kingma and Ba, 2015) with decoupled weight decay regularization (Loshchilov and Hutter, 2019).",
      "startOffset" : 54,
      "endOffset" : 75
    }, {
      "referenceID" : 27,
      "context" : "We train our student network using the Adam optimizer (Kingma and Ba, 2015) with decoupled weight decay regularization (Loshchilov and Hutter, 2019).",
      "startOffset" : 119,
      "endOffset" : 148
    }, {
      "referenceID" : 59,
      "context" : "(2020) as baselines: DistMult (Yang et al., 2015), ComplEx (Trouillon et al.",
      "startOffset" : 30,
      "endOffset" : 49
    }, {
      "referenceID" : 51,
      "context" : ", 2015), ComplEx (Trouillon et al., 2016) ConvE (Dettmers et al.",
      "startOffset" : 17,
      "endOffset" : 41
    }, {
      "referenceID" : 5,
      "context" : "We follow past work and use the filtered setting (Bordes et al., 2013), removing all positive entities other than the target entity before calculating the target entity’s rank.",
      "startOffset" : 49,
      "endOffset" : 70
    }, {
      "referenceID" : 1,
      "context" : "We utilize paired bootstrap significance testing (Berg-Kirkpatrick et al., 2012) with the MRR to validate the statistical significance of improvements.",
      "startOffset" : 49,
      "endOffset" : 80
    }, {
      "referenceID" : 16,
      "context" : "1023 being performed, we apply the Holm–Bonferroni method (Holm, 1979) to correct for multiple hypothesis testing.",
      "startOffset" : 58,
      "endOffset" : 70
    }, {
      "referenceID" : 25,
      "context" : "Knowledge distillation has also been shown to be useful in situations with noisy labels (Li et al., 2017) which may explain why it was particularly effective for our sparsest dataset, CN-100K, where training with the hard labels led to particularly poor performance.",
      "startOffset" : 88,
      "endOffset" : 105
    } ],
    "year" : 2021,
    "abstractText" : "Knowledge Graph (KG) completion research usually focuses on densely connected benchmark datasets that are not representative of real KGs. We curate two KG datasets that include biomedical and encyclopedic knowledge and use an existing commonsense KG dataset to explore KG completion in the more realistic setting where dense connectivity is not guaranteed. We develop a deep convolutional network that utilizes textual entity representations and demonstrate that our model outperforms recent KG completion methods in this challenging setting. We find that our model’s performance improvements stem primarily from its robustness to sparsity. We then distill the knowledge from the convolutional network into a student network that re-ranks promising candidate entities. This re-ranking stage leads to further improvements in performance and demonstrates the effectiveness of entity re-ranking for KG completion.1",
    "creator" : "LaTeX with hyperref"
  }
}