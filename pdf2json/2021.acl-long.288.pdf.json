{
  "name" : "2021.acl-long.288.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "CDRNN: Discovering Complex Dynamics in Human Language Processing",
    "authors" : [ "Cory Shain" ],
    "emails" : [ "shain.3@osu.edu" ],
    "sections" : [ {
      "heading" : null,
      "text" : "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 3718–3734\nAugust 1–6, 2021. ©2021 Association for Computational Linguistics\n3718"
    }, {
      "heading" : "1 Introduction",
      "text" : "Central questions in psycholinguistics concern the mental processes involved in incremental human language understanding: which representations are computed when, by what mental algorithms (Frazier and Fodor, 1978; Just and Carpenter, 1980; Abney and Johnson, 1991; Tanenhaus et al., 1995; Almor, 1999; Gibson, 2000; Coltheart et al., 2001; Hale, 2001; Lewis and Vasishth, 2005; Levy, 2008, inter alia)? Such questions are often studied by caching out a theory of language processing in an experimental stimulus, collecting human responses, and fitting a regression model to test whether measures show the expected effects (e.g. Grodner and Gibson, 2005). Regression techniques have grown in sophistication, from ANOVA (e.g. Pickering and Branigan, 1998) to newer linear mixed-effects approaches (LME, Bates et al., 2015) that enable\ndirect word-by-word analysis of effects in naturalistic human language processing (e.g. Demberg and Keller, 2008; Frank and Bod, 2011). However, these methods struggle to account for delayed effects. Because the human mind operates in real time and experiences computational bottlenecks of various kinds (Bouma and De Voogd, 1974; Just and Carpenter, 1980; Ehrlich and Rayner, 1981; Mollica and Piantadosi, 2017), delayed effects may be pervasive, and, if left uncontrolled, can yield misleading results (Shain and Schuler, 2018).\nContinuous-time deconvolutional regression (CDR) is a recently proposed technique to address delayed effects in measures of human cognition (Shain and Schuler, 2018, 2021). CDR fits parametric continuous-time impulse response functions (IRFs) that mediate between word features and response measures. An IRF maps the time elapsed between a stimulus and a response to a weight describing the expected influence of the stimulus on the response. CDR models the response as an IRF-weighted sum of preceding stimuli, thus directly accounting for effect latencies. Empirically, CDR reveals fine-grained processing dynamics and generalizes better to human reading and fMRI responses than established alternatives. However, CDR retains a number of simplifying assumptions (e.g. that the IRF is fixed over time) that may not hold of the human language processing system.\nDeep neural networks (DNNs), widely used in natural language processing (NLP), can relax these strict assumptions. Indeed, psycholinguistic regression analyses and NLP systems share a common structure: both fit a function from word features to some quantity of interest. However, psycholinguistic regression models face an additional constraint: they must be interpretable enough to allow researchers to study relationships between variables in the model. This requirement may be one reason why black box DNNs are not generally\nused to analyze psycholinguistic data, despite the tremendous gains DNNs have enabled in natural language tasks (Peters et al., 2018; Devlin et al., 2019; Radford et al., 2019; Brown et al., 2020, inter alia), in part by better approximating the complex dynamics of human cognition as encoded in natural language (Linzen et al., 2016; Gulordava et al., 2018; Tenney et al., 2019; Hewitt and Manning, 2019; Wilcox et al., 2019; Schrimpf et al., 2020).\nThis study proposes an attempt to leverage the flexibility of DNNs for psycholinguistic data analysis. The continuous-time deconvolutional regressive neural network (CDRNN) is an extension of CDR that reimplements the impulse response function as a DNN describing the expected influence of preceding events (e.g. words) on future responses (e.g. reading times) as a function of their properties and timing. CDRNN retains the deconvolutional design of CDR while relaxing many of its simplifying assumptions (linearity, additivity, homosketasticity, stationarity, and context-independence, see Section 2), resulting in a highly flexible model. Nevertheless, CDRNN is interpretable and can shed light on the underlying data generating process. Results on reading and fMRI measures show substantial generalization improvements from CDRNN over baselines, along with detailed insights about the underlying dynamics that cannot easily be obtained from existing methods.1"
    }, {
      "heading" : "2 Background",
      "text" : "Psycholinguists have been aware for decades that processing effects may lag behind the words that trigger them (Morton, 1964; Bouma and De Voogd, 1974; Rayner, 1977; Erlich and Rayner, 1983; Mitchell, 1984; Rayner, 1998; Vasishth and Lewis, 2006; Smith and Levy, 2013), possibly because cognitive “buffers” may exist to allow higher-level information processing to catch up with the input (Bouma and De Voogd, 1974; Baddeley et al., 1975; Just and Carpenter, 1980; Ehrlich and Rayner, 1981; Mollica and Piantadosi, 2017). They have also recognized the potential for non-linear, interactive, and/or time-varying relationships between word features and language processing (Smith and Levy, 2013; Baayen et al., 2017, 2018). No prior regression method can jointly address these\n1Because of page constraints, additional replication details and synthetic results are provided in an external supplement, available here: https://osf.io/z89vn/.\nconcerns in non-uniform time series (e.g. words with variable duration) like naturalistic psycholinguistic experiments. Discrete-time methods (e.g. lagged/spillover regression, Sims, 1971; Erlich and Rayner, 1983; Mitchell, 1984) ignore potentially meaningful variation in event duration, even if some (e.g. generalized additive models, or GAMs, Hastie and Tibshirani, 1986; Wood, 2006) permit non-linear and non-stationary (time-varying) feature interactions (Baayen et al., 2017). CDR (Shain and Schuler, 2018, 2021) addresses this limitation by fitting continuous-time IRFs, but assumes that the IRF is stationary (time invariant), that features scale linearly and combine additively, and that the response variance is constant (homoskedastic). By implementing the IRF as a time-varying neural network, CDRNN relaxes all of these assumptions, incorporating the featural flexibility of GAMs while retaining the temporal flexibility of CDR.\nPrevious studies have investigated latency and non-linearity in human sentence processing. For example, Smith and Levy (2013) attach theoretical significance to the functional form of the relationship between word surprisal and processing cost, using GAMs to show that this relationship is linear and arguing on this basis that language processing is highly incremental. This claim is under active debate (Brothers and Kuperberg, 2021), underlining the importance of methods that can investigate questions of functional form. Smith and Levy (2013) also investigate the timecourse of surprisal effects using spillover and find a more delayed surprisal response in self-paced reading (SPR) than in eye-tracking. Shain and Schuler (2021) support the latter finding using CDR, and in addition show evidence of strong inertia effects in SPR, such that participants who have been reading quickly in the recent past also read more quickly now. However, this outcome may be an artifact of the stationarity assumption: CDR may be exploiting its estimates of rate effects in order to capture broad non-linear negative trends (e.g. task adaptation, Prasad and Linzen, 2019) in a stationary model. Similarly, the generally null word frequency estimates reported in Shain and Schuler (2021) may be due in part to the assumption of additive effects: word frequency and surprisal are related, and they may coordinate interactively to determine processing costs (Norris, 2006). Thus, in general, prior findings on the timecourse and functional form of effects in human sentence processing may be influenced by method-\nological limitations: the GAM models of Smith and Levy (2013) ignore variable event duration, the CDR models of Shain and Schuler (2021) ignore non-linearity, and both approaches assume stationarity, context-independence, constant variance, and additive effects. By jointly relaxing these potentially problematic assumptions, CDRNN stands to support more reliable conclusions about human language comprehension, while also possibly enabling new insights into cognitive dynamics."
    }, {
      "heading" : "3 Model",
      "text" : ""
    }, {
      "heading" : "3.1 Architecture",
      "text" : "This section presents a high-level description of the model design (for formal definition, see Appendix A). The CDRNN architecture is represented schematically in Figure 1. The primary goal of estimation is to identify the deep neural IRF g(τ) (top) that computes the influence of a preceding event on the predictive distribution over a subsequent response as a function of their distance in time τ . As shown, the IRF is a feedforward projection of τ into a matrix that defines a weighted sum over the values of input vector x, which is concatenated with a bias to capture general effects of stimulus timing (rate). This matrix multiplication determines the contribution of the stimulus event to the parameters of the predictive distribution (e.g. the mean and variance parameters of a Gaussian predictive distribution). Defining the IRF as a function of τ ensures\nthat the model has a continuous-time definition. To capture non-linear effects of stimulus features, the IRF projection is itself parameterized by a projection of a hidden state h. The dependence on h permits non-linear influences of the properties of the stimulus sequence on the IRF itself. To generate h, the predictors x are concatenated with their timestamps t and submitted to the model as input. Inputs are cast to a hidden state for each preceding event as the sum of three quantities: a feedforward projection hin of each input, a forwarddirectional RNN projection hRNN of the events up to and including each input, and random effects hZ containing offsets for the relevant random effects level(s) (e.g. for each participant in an experiment). In this study, the recurrent component is treated as optional (gray arrows). Without the RNN, the model is non-stationary (via input t) but cannot capture contextual influences on the IRF.\nThe summation over IRF outputs at the top of the figure ensures that the model is deconvolutional: each preceding input contributes to the response in some proportion, with that proportion determined by the features, context, and relative timing of that input. Because the IRF depends on a deep neural projection of the current stimulus as well as (optionally) the entire sequence of preceding stimuli, it implicitly estimates all interactions between these variables in governing the response. Predictors may thus coordinate in a non-linear, non-additive, and time-varying manner.\nThe CDRNN IRF describes the influence over time of predictors on all parameters of the predictive distribution (in these experiments, the mean and variance parameters of a Gaussian predictive distribution). Such a design (i.e. modeling dependencies on the predictors of all parameters of the predictive distribution) has previously been termed distributional regression (Bürkner, 2018).\nDespite their flexibility and task performance (Section 5), CDRNN models used in this study have few parameters (Table A1) by current deep learning standards because they are relatively shallow and small (Supplement S1)."
    }, {
      "heading" : "3.2 Objective and Regularization",
      "text" : "Given (1) an input configuration C containing predictors X, input timestamps t, and response timestamps t′, (2) CDRNN parameter vector w, (3) output distribution p, (4) random effects vector z, and (5) response vector y, the model uses gradient de-\nscent to minimize the following objective:\nL (y | C;w, z) def= − log p (y | C;w, z)+ (1) λz||z||22 + Lreg\nIn addition to random effects shrinkage governed by λz and any arbitrary additional regularization penalties Lreg (see Supplement S1), models are regularized using dropout (Srivastava et al., 2014) with drop rate dh at the outputs of all feedforward hidden layers. Random effects are also dropped at rate dz, which is intended to encourage the model to find population-level estimates that accurately reflect central tendency. Finally, the recurrent contribution to the CDRNN hidden state (hRNN above) is dropped at rate dr, which is intended to encourage accurate IRF estimation even when context is unavailable."
    }, {
      "heading" : "3.3 Effect Estimation",
      "text" : "Because it is a DNN, CDRNN lacks parameters that selectively describe the size and shape of the response to a specific predictor (unlike CDR), and indeed individual parameters (e.g. individual biases or connection weights) are not readily interpretable. Thus, from a scientific perspective, the quantity of general interest is not a distribution over parameters, but rather over the effect of a predictor on the response. The current study proposes to accomplish this using perturbation analysis (e.g. Ribeiro et al., 2016; Petsiuk et al., 2018), manipulating the input configuration and quantifying the influence of this manipulation on the predicted response.2 For example, to obtain an estimate of rate effects (i.e. the base response or “deconvolutional intercept,” see Shain and Schuler, 2021), a reference stimulus can be constructed, and the response to it can be queried at each timepoint over some interval of interest. To obtain CDR-like estimates of predictor-wise IRFs, the reference stimulus can be increased by 1 in the predictor dimension of interest (e.g. word surprisal) and requeried, taking the difference between the obtained response and the reference response to reveal the influence of an extra unit of the predictor.3 This study uses the\n2Perturbation analyses is one of a growing suite of tools for black box interpretation. It is used here because it straightforwardly links properties of the input to changes in the estimated response, providing a highly general method for querying aspects of the the non-linear, non-stationary, non-additive IRF defined by the CDRNN equations.\n3Note that 1 is used here to maintain comparability of effect estimates to those generated by methods that assume\ntraining set mean of x and t as a reference, since this represents the response of the system to an average stimulus. The model also supports arbitrary additional kinds of queries, including of the curvature of an effect in the IRF over time and of the interaction between two effects at a point in time. Indeed, the IRF can be queried with respect to any combination of values for predictors, t, and τ , yielding an open-ended space of queries that can be constructed as needed by the researcher.\nBecause the estimates of interest all derive from the model’s predictive distribution, uncertainty about them can be measured with Monte Carlo techniques as long as training involves a stochastic component, such as dropout (Srivastava et al., 2014) or batch normalization (Ioffe and Szegedy, 2015). This study estimates uncertainty using Monte Carlo dropout (Gal and Ghahramani, 2016), which recasts training neural networks with dropout as variational Bayesian approximation of deep Gaussian process models (Damianou and Lawrence, 2013). At inference time, an empirical distribution over responses to an input is constructed by resampling the model (i.e. sampling different dropout masks).4 As argued by Shain and Schuler (2021) for CDR, in addition to intervals-based tests, common hypothesis tests (e.g. for the presence of an effect) can be performed in a CDRNN framework via bootstrap model comparison on held out data (e.g. of models with and without the effect of interest)."
    }, {
      "heading" : "4 Methods",
      "text" : "Following Shain and Schuler (2021), CDRNN is applied to naturalistic human language processing data from three experimental modalities: the Natural Stories self-paced reading corpus (∼1M instances, Futrell et al., 2020), the Dundee eye-tracking corpus (∼200K instances, Kennedy\nlinearity of effects (especially CDR), but that 1 has no special meaning in the non-linear setting of CDRNN modeling, and effects can be queried at any offset from any reference. Results here show that deflections move relatively smoothly away from the reference, even at smaller steps than 1, and that IRFs queried at 1 are similar to those obtained from (linear) CDR, indicating that this method of effect estimation is reliable. Note finally that because predictors are underlyingly rescaled by their training set standard deviations (though plotted at the original scale for clarity), 1 here corresponds to 1 standard unit, as was the case with the CDR estimates discussed in Shain and Schuler (2021).\n4Initial experiments also explored uncertainty quantification by implemententing CDRNN as a variational Bayesian DNN. Compared to the methods advocated here, the variational approach was more prone to instability, achieved worse fit, and yielded implausibly narrow credible intervals.\net al., 2003), and the Natural Stories fMRI corpus (∼200K instances, Shain et al., 2020), using the train/dev/test splits for these corpora defined in Shain and Schuler (2021). Further details about datasets and preprocessing are given in Supplement S2.\nFor reading data, CDRNN is compared to CDR as well as lagged LME and GAM baselines equipped with four spillover positions for each predictor (values from the current word, plus three preceding words), since LME and GAM are well established analysis methods in psycholinguistics (e.g. Baayen et al., 2007; Demberg and Keller, 2008; Frank and Bod, 2011; Smith and Levy, 2013; Baayen et al., 2017; Goodkind and Bicknell, 2018, inter alia). Because the distribution of reading times is heavy-tailed (Frank et al., 2013), following Shain and Schuler (2021) models are fitted to both raw and log-transformed reading times. For fMRI data, CDRNN is compared to CDR as well as four existing techniques for analyzing naturalistic fMRI data: pre-convolution with the canonical hemodynamic response function (HRF, Brennan et al., 2012; Willems et al., 2015; Henderson et al., 2015, 2016; Lopopolo et al., 2017), linear interpolation (Shain and Schuler, 2021), binning (Wehbe et al., 2020), and Lanczos interpolation (Huth et al., 2016). Statistical model comparisons use paired permutation tests of test set error (Demšar, 2006).\nModels use predictors established by prior psycholinguistic research (e.g. Rayner, 1998; Demberg and Keller, 2008; van Schijndel and Schuler, 2013; Staub, 2015; Shain and Schuler, 2018, inter alia): unigram and 5-gram surprisal, word length (reading only), saccade length (eye-tracking only), and previous was fixated (eye-tracking only). Predictor definitions are given in Appendix C. The deconvolutional intercept term rate (Shain and Schuler, 2018, 2021), an estimate of the general influence of observing a stimulus at a point in time, independently of its properties, is implicit in CDRNN (unlike CDR) and is therefore reported in all results. Reading models include random effects by subject,\nwhile fMRI models include random effects by subject and by functional region of interest (fROI). Unlike LME, where random effects capture linear differences in effect size between e.g. subjects, random effects in CDRNN capture differences in overall dynamics between subjects, including differences in size, IRF shape, functional form (e.g. linearity), contextual influences on the IRF, and interactions with other effects.\nTwo CDRNN variants are considered in all experiments: the full model (CDRNN-RNN) containing an RNN over the predictor sequence, and a feedforward only model (CDRNN-FF) with the RNN ablated (gray arrows removed in Figure 1). This manipulation is of interest because CDRNN-FF is both more parsimonious (fewer parameters) and faster to train, and may therefore be preferred in the absence of prior expectation that the IRF is sensitive to context. All plots show means and 95% credible intervals. Code and documentation are available at https://github.com/coryshain/cdr."
    }, {
      "heading" : "5 Results",
      "text" : "Since CDRNN is designed for scientific modeling, the principal output of interest is the IRF itself and the light it might shed on questions of cognitive dynamics, rather than on performance in some task (predicting reading latencies or fMRI measures are not widely targeted engineering goals). However, predictive performance can help establish the trustworthiness of the IRF estimates. To this end, as a sanity check, this section first evaluates predictive performance on human data relative to existing regression techniques. While results may resemble “bake-off” comparisons familiar from machine learning (and indeed CDRNN does outperform all baselines), their primary purpose is to establish that the CDRNN estimates are trustworthy, since they describe the phenomenon of interest in a way that generalizes accurately to an unseen sample. Baseline models, including CDR, are as reported\nin Shain and Schuler (2021).5"
    }, {
      "heading" : "5.1 Model Validation: Baseline Comparisons",
      "text" : "Table 1 gives mean squared error by dataset of CDRNN vs. baseline models on reading times from both Natural Stories and Dundee. Both versions of CDRNN outperform all baselines on the dev partition of all datasets except for raw (ms) latencies in Natural Stories (SPR), where CDRNN is edged out by CDR6 but still substantially outperforms the non-CDR baselines. Nonetheless, results indicate that CDRNN estimates of Natural Stories (ms) are similarly reliable to those of CDR, and, as discussed in Section 5.2, CDRNN largely replicates the CDR estimates on Natural Stories while offering advantages for analysis.\nAlthough CDR struggles against GAM baselines on Dundee, CDRNN has closed the gap. This is noteworthy in light of speculation in Shain and Schuler (2021) that CDR’s poorer performance on Dundee might be due in part to non-linear effects, which GAM can estimate but CDR cannot. CDRNN performance supports this conjecture: once the model can account for non-linearities, it overtakes GAMs.\nResults from fMRI are shown in Table 2, where both CDRNN variants yield substantial improvements to training, dev, and test set error. These results indicate that the relaxed assumptions afforded by CDRNN are beneficial for describing the fMRI response, which is known to saturate over time (Friston et al., 2000; Wager et al., 2005; Vazquez et al., 2006; Lindquist et al., 2009).\nFollowing Shain and Schuler (2021), model error is statistically compared using a paired permu-\n5For all datasets, the CDR baseline used here is the variant that was deployed on the test set in Shain and Schuler (2021).\n6Note that a major advantage of CDRNN is its ability to model dynamics in response variance, which are not reflected in squared error. For example, although CDRNN-FF achieves worse test set error than CDR on the Natural Stories (ms) task, it affords a 31,040 point log likelihood improvement.\ntation test that pools across all datasets covered by a given baseline (reading data for LME and GAM, fMRI data for canonical HRF, linearly interpolated, averaged, and Lanczos interpolated, and both for CDR).7 Results are given in Table 3. As shown, both variants of CDRNN significantly improve over all baselines, and CDRNN-RNN significantly improves over CDRNN-FF. Notwithstanding, CDRNN-FF may be preferred in applications: simpler, faster to train, better at recovering synthetic models (Supplement S3), more reliable in noisy domains like fMRI, and close in performance to CDRNN-RNN. Results overall support the reliability of patterns revealed by CDRNN’s estimated IRF, which is now used to explore and visualize sentence processing dynamics."
    }, {
      "heading" : "5.2 Effect Latencies in CDRNN vs. CDR",
      "text" : "CDR-like IRF estimates can be obtained by increasing a predictor by 1 (standard deviation) relative to the reference and observing the change in the response over time. Visualizations using this approach are presented in Figure 2 alongside CDR estimates from Shain and Schuler (2021). In general, CDRNN finds similar patterns to CDR. This suggests both (1) that CDRNN is capable of recovering estimates from a preceding state-of-the-art deconvolutional model for these domains, and (2) that CDR estimates in these domains are not driven by artifacts introduced by its simplifying assumptions, since a model that lacks those assumptions and has a qualitatively different architecture largely recovers them. Nonetheless there are differences. For example, Dundee estimates decay more quickly over time in CDRNN than in CDR, indicating an even less pronounced influence of temporal diffusion in\n7The comparison rescales each pair of error vectors by their joint standard deviation in order to enable comparability across datasets with different error variances.\neye-tracking than CDR had previously suggested. Estimates from CDRNN-FF and CDRNN-RNN roughly agree, except that CDRNN-RNN estimates for fMRI are more attenuated. CDR shows little uncertainty in the fMRI domain despite its inherent noise (Shain et al., 2020), while CDRNN more plausibly shows more uncertainty in its estimates for the noisier fMRI data.\nAs noted in Section 2, Shain and Schuler (2021) report negative rate effects in reading — i.e., a local decrease in subsequent reading time at each word, especially in SPR. This was interpreted as an inertia effect (faster recent reading engenders faster current reading), but it might also be an artifact of non-linear decreases in latency over time (due to task habituation, e.g. Baayen et al., 2017; Harrington Stack et al., 2018; Prasad and Linzen, 2019) that CDR cannot model. CDRNN estimates nonetheless thus support the prior interpretation of rate effects as inertia, at least in SPR: a model that can flexibly adapt to non-linear habituation trends finds SPR rate estimates that are similar in shape and magnitude to those estimated by CDR.\nIn addition, CDRNN finds a slower response to word surprisal in self-paced reading than in eye-tracking. This result converges with word-\ndiscretized timecourses reported in Smith and Levy (2013), who find more extensive spillover of surprisal effects in SPR than in eye-tracking. Results thus reveal important hidden dynamics in the reading response (inertia effects), continuous-time delays in processing effects, and influences of modality the continuous dynamics of sentence processing, all of which are difficult to estimate using existing regression techniques. Greater response latency and more pronounced inertia effects in self-paced reading may be due to the fact that a gross motor task (paging via button presses) is overlaid on the sentence comprehension task. While the motor task is not generally of interest to psycholinguistic theories, controlling for its effects is crucial when using self-paced reading to study sentence comprehension (Mitchell, 1984)."
    }, {
      "heading" : "5.3 Linearity of Surprisal Effects",
      "text" : "CDRNN also allows the analyst to explore other aspects of the IRF, such as functional curvature at a point in time. For example, in the context of reading, Smith and Levy (2013) argue for a linear increase in processing cost as a function of word surprisal. The present study allows this claim to be assessed across modalities by checking the curva-\nture of the 5-gram surprisal response (in raw ms) at a timepoint of interest (0ms for reading and∼5s for fMRI). As shown in the top row of Figure 3, reading estimates are consistent with a linear response (the credible interval contains a straight line), as predicted, but are highly non-linear in fMRI, with a rapid peak above the mean (zero-crossing) followed by a sharp dip and plateau, and even an estimated increased response at values below the mean (though estimates at the extremes have high uncertainty). This may be due in part to ceiling effects: blood oxygen levels measured by fMRI are bounded, but reading times are not. While this is again a property of experimental modality rather than sentence comprehension itself, understanding such influences is important for drawing scientific conclusions from experimental data. For example, due to the possibility of saturation, fMRI may not be an ideal modality for testing scientific claims about the functional form of effects, and the linearity assumptions of e.g. CDR and LME may be particularly constraining.\nThe curvature of effects can also be queried over time. If an effect is temporally diffuse but linear, its curvature should be roughly linear at any delay of interest. The second row of Figure 3 shows visualizations to this effect. These plots in fact subsume the kinds of univariate plots shown above: univariate IRFs to 5-gram surprisal like those plotted in Figure 2 are simply slices taken at a predictor value (1 sample standard deviation above the mean), whereas curvature estimates in the first row of Figure 3 are simply slices taken at a time value (0s for reading and 5s for fMRI). Plots are consistent with the linearity hypothesis for reading, but again show strong non-linearities in the fMRI domain that are consistent with saturation effects\nas discussed above."
    }, {
      "heading" : "5.4 Effect Interactions",
      "text" : "In addition to exploring multivariate relationships of a predictor with time, relationships between predictors can also be studied. Such relationships constitute “interactions” in a CDRNN model, though they are not constrained (cf. interactions in linear models) to be strictly multiplicative — indeed, a major advantage of CDRNN is that interactions come “for free”, along with estimates of their functional form. To explore effect interactions, a CDRNN-FF version of the full model in Shain et al. (2020) is fitted to the fMRI dataset. The model contains more predictors to explore than models considered above, including surprisal computed from a probabilistic context-free grammar (PCFG surprisal, see Appendix C for details). Univariate IRFs are shown in the top left panel of Figure 4, and pairwise interaction surfaces at a delay of 5s (near the peak response) are shown in the remaining panels. Plots show that the response at any value of the other predictors is roughly flat as a function of sound power (i.e. signal power of the auditory stimulus, middle row). This accords with prior arguments that the cortical language system, whose activity is measured here, does not strongly register low-level perceptual effects (Fedorenko et al., 2010; Braze et al., 2011).\nThe estimate for unigram surprisal (middle left) shows an unexpected non-linearity: although activity increases with higher surprisal (lower frequency words), it also increases at lower surprisal (higher frequency words), suggesting the existence of high frequency items that nonetheless engender a large response. The interaction between PCFG surprisal and unigram surprisal possibly sheds light on this outcome, since it shows a sharper increase in the PCFG surprisal response in higher frequency (lower unigram surprisal) regions. This may be because the most frequent words in English tend to be function words that play an outsized role in syntactic structure building (e.g. prepositional phrase attachment decisions).\nIn addition, 5-gram surprisal interacts with PCFG surprisal, showing a non-linear increase in response for words that are high on both measures. This is consistent with a unitary predictive mechanism that experiences strong error signals when both string-level (5-gram) and structural (PCFG) cues are poor. All these interactions should be interpreted with caution, since the uncertainty interval covers much weaker degrees of interaction."
    }, {
      "heading" : "5.5 IRFs of the Response Variance",
      "text" : "As discussed in Section 3, CDRNN implements distributional regression and thus also contains an IRF describing the influence of predictors on the variance of the predictive distribution as a function of time. IRFs of the variance can be visualized identically to IRFs of the mean.\nFor example, Figure 5 shows the estimated change in the standard deviation of the predictive distribution over time from observing a stimulus.8 Estimates show stimulus-dependent changes\n8Because standard deviation is a bounded variable and the IRF applies before the constraint function (softplus), the relationship between the standard deviation and the y axis of the plots is not straightforward. Estimates nonetheless clearly indicate the shape and relative contribution to the response\nin variance across datasets whose shapes are not straightforwardly related to that of the IRFs of the mean (Figure 2). For example, both reading datasets (left and center) generally show mean and standard deviation traveling together, with increases in the mean corresponding to increases in standard deviation. In Dundee, the shapes of these changes resemble each other strongly, whereas in Natural Stories the IRFs of the standard deviation (especially rate) differ substantially from the IRFs of the mean. By contrast, in fMRI (right), the IRFs of the standard deviation look roughly like inverted HRFs (especially for rate and 5-gram surprisal), indicating that BOLD variance tends to decrease with larger values of the predictors. While detailed interpretation of these patterns is left to future work, these results demonstrate the utility of CDRNN for analyzing a range of links between predictors and response that are otherwise difficult to study."
    }, {
      "heading" : "6 Conclusion",
      "text" : "This study proposed and evaluated CDRNN, a deep neural extension of continuous-time deconvolutional regression that relaxes implausible simplifying assumptions made by widely used regression techniques in psycholinguistics. In so doing, CDRNN provides detailed estimates of human language processing dynamics that are difficult to obtain using other measures. Results showed plausible estimates from human data that generalize better than alternatives and can illuminate hitherto understudied properties of the human sentence processing response. This outcome suggests that CDRNN may play a valuable role in analyzing human experimental data."
    }, {
      "heading" : "A Mathematical Definition",
      "text" : "This appendix formally defines the CDRNN model. CDRNN assumes the following quantities as input:9\n• X ∈ N: Number of predictor observations (e.g. word exposures)\n• Y ∈ N: Number of response observations (e.g. fMRI scans)\n• Z ∈ N: Number of random grouping factor levels (e.g. distinct participants)\n• K ∈ N: Number of predictors • X ∈ RX×K : Design matrix of X predictor\nobservations of K dimensions each.\n• y ∈ RY : Vector of Y response observations • Z ∈ {0, 1}Y×Z : Boolean matrix indicating\nrandom grouping factor levels associated with each response observation\n• t ∈ RX : Vector of timestamps associated with each observation in X\n• t′ ∈ RY : Vectors of timestamps associated with each observation in y\n• S ∈ N: Number of parameters in predictive distribution (e.g. 2 for a normal distribution: mean and variance)\nFor simplicity of exposition, X and y are assumed to contain data from a single time series (e.g. a single participant performing a single experiment).\n9Throughout these definitions, vectors and matrices are notated in bold lowercase and uppercase, respectively (e.g. u, U). Objects with indexed names are designated using subscripts (e.g. vr). Vector and matrix indexing operations are notated using subscript square brackets, and slice operations are notated using ∗ (e.g. X[∗,k] denotes the kth column of matrix X). Hadamard (pointwise) products are notated using . The notations 0 and 1 designate conformable column vectors of 0’s and 1’s, respectively. Superscripts are used for indexation and do not denote exponentiation.\nThe definition below can be applied without loss of generality to data containing multiple time series by concatenating the output of the model as applied to multiple X, y pairs. X,y and their associated satellite data Z, t, t′ must be temporally sorted.\nGiven these inputs, CDRNN estimates a latent impulse response function that relates timestamped predictors to all parameters of the assumed predictive distribution. For example, assuming a univariate normally distributed response, CDRNN learns an IRF with two output dimensions, one for the predictive mean, and one for the predictive variance. Regressing all parameters of the predictive distribution in this way has previously been called distributional regression (Bürkner, 2018).\nCDRNN contains a recurrent neural network (RNN), neural projections that map inputs and RNN states to a hidden state for each preceding event, and neural projections that map the hidden states to predictions about (1) the influence of each event on the response (IRF) and (2) the parameter(s) of the error distribution (e.g. the variance of a Gaussian error). The definition assumes the following quantities:\n• Lin, LRNN, LIRF ∈ N: Number of layers in the input projection, RNN, and IRF, respectively\n• Din(`), DRNN(`), Dh, DIRF(`) ∈ N: Number of output dimensions in the `th layer of the input projection, RNN, hidden state, and IRF, respectively\nThe following values are deterministically assigned:\n• DIRF(LIRF) = S(K + 1) (the IRF generates a convolution weight for every predictor dimension, plus the timestamp, for each parameter of the predictive distribution)\n• Din(0) = K + 1 (input is predictors + time)\n• Din(Lin) = Dh\nIn these definitions, integers x, y respectively refer to row indices of X, y. Let zy be the vector( Z[y,∗] )> of random effects associated with the response at y. Let Wh,Z ∈ RDh×Z , WIRF(1),Z ∈ R2DIRF(1)×Z , and Ws,Z ∈ RS×Z be an embedding matrix for zy. Random effects offsets at response step y for the hidden state (hZy ), the weights and biases of the first layer of the IRF (wIRF(1),Zy , b IRF(1),Z y ), and the parameters of the predictive distribution (eZy , i.e. random intercepts and variance\nparameters) are generated as follows:\nhZy def = Wh,Zzy (2)[\nw IRF(1),Z y b IRF(1),Z y\n] def = WIRF(1),Zzy (3)\nsZy def = Ws,Zzy (4)\nFollowing prior work in mixed effects models (Bates et al., 2015), to ensure that population-level estimates reliably encode central tendency, each output dimension of Wh,Z, WIRF(1),Z, and Ws,Z is constrained to have mean 0 across the levels of each random grouping factor (e.g. across participants in the study).\nThe neural IRF is applied to a temporal offset τ representing the delay at which to query the response to an input (e.g. τ = 1 queries the response to an input 1s after the input occurred). The output of the neural IRF g`x,y(τ) ∈ RDIRF(`) applied to τ at layer ` is defined as:\ng(1)x,y(τ) def = sIRF(1) ( wIRF(1)x,y τ + b IRF(1) x,y ) (5)\ng(`)x,y(τ) def = sIRF(`) ( WIRF(`)g(`−1)x,y (τ) + b IRF(`) ) ,\n(6)\n` > 1\nwIRF(1)x,y def = wIRF(1) +wIRF(1),Zy +W IRF(1) ∆ hx,y\n(7)\nbIRF(1)x,y def = bIRF(1) + bIRF(1),Zy +B IRF(1) ∆ hx,y\n(8)\nW IRF(`) x,y , b IRF(`) x,y , and sIRF(`) are respectively the `th IRF layer’s weight matrix at predictor timestep x and response timestep y, bias vector at time x, y, and squashing function, and g(0)x,y(τ) = τ . wIRF(1), bIRF(1) are respectively globally applied initial weight and bias vectors for the first layer of the IRF, which transforms scalar τ , each of which is shifted by its corresponding random effects. WIRF(1)∆ , B IRF(1) ∆ are respectively weight matrices used to compute additive modifications to WIRF(1) from CDRNN hidden state hx,y, similar in spirit to a residual network (He et al., 2016). Non-initial IRF layers are treated as stationary (i.e. their parameters are independent of x, y). The final output of the IRF is given by:\ngx,y(τ) def = reshape ( g(LIRF)x,y (τ), (S,K + 1) ) (9)\nThe hidden state hx,y is computed as the squashed sum of several quantities: a global bias hbias, random effects hZ, a neural projection hinx,y of the inputs at x, y, and a neural projection hRNNx,y of the hidden state of an RNN over the sequence of predictors up to and including timestep x:\nhx,y def = sh ( hbias + hZy + h in x,y + h RNN x,y ) (10)\nThe IRF gx,y is therefore feature-dependent via the neural projection hinx,y of the input at x, y and context-dependent via the neural projection hRNNx,y of an RNN over the input up to x for the response at y. This design relaxes stationarity assumptions while also sharing structure across timepoints. The definitions of hinx,y and h RNN x,y are given below.\nLet tx be the element t[x] and xx be the xth predictor vector ( X[x,∗] )>. The inputs h(0)x,y to the CDRNN model are defined as the vertical concatenation of the predictors xx and the event timestamp tx:\nh(0)x,y def = [ xx tx ] (11)\nThe output of the input projection at layer l and time x, y is defined as:\nhin(`)x,y def = sin(`) ( Win(`)hin(`−1)x,y + b in(`) ) (12)\nwhere hin(0)x,y def = h (0) x,y. At the final layer, sin(Lin) is identity and bin(Lin) = 0, since hx,y already has a bias. The final output of the input projection is given by:\nhinx,y def = hin(Lin)x,y (13)\nNote that hinx,y is already non-stationary by virtue of its dependence on the event timestamp t[x], which allows the IRF to differ between timepoints (see e.g. Baayen et al., 2017, for development of a similar idea using generalized additive models). While this model of non-stationarity can be complex and non-linear, it is still limited by contextindependence. That is, the change in the IRF over time depends only on the amount of time elapsed since the start of the time series, independently of which events preceded. However, it is possible that the contents of the events in a time series may influence the IRF, above any deterministic change in response over time (for example, if several difficult preceding words have already taxed the processing buffer, additional processing costs may become larger). To account for this possibility, an RNN\nis built into the CDRNN design.10 Any variant of RNN can be used (this study uses a long shortterm memory network, or LSTM, Hochreiter and Schmidhuber, 1997). The `th RNN hidden state at x, y is designated by hRNN(`)x,y . To account for the possibility of random variation in sensitivity to context, the initial hidden and cell states hRNN(`)0,y , c RNN(`) 0,y depend on the random effects:\nh RNN(`) 0,y def = h RNN(`) 0 +W RNNh(`) Z zy (14)\nc RNN(`) 0,y def = c RNN(`) 0 +W RNNc(`) Z zy (15)\nwhere hRNN(`)0 , c RNN(`) 0 are global biases and W RNNh(`) Z , W RNNc(`) Z are constrained to have mean 0 within each random grouping factor. Non-initial RNN states are computed via a standard LSTM update:[ hRNN(`)x,y , c RNN(`) x,y ] def =LSTM ( h RNN(`) x−1,y , (16)\nc RNN(`) x−1,y ,h RNN(`−1) x,y ) The hidden state of the final RNN layer is linearly projected to the dimensionality of the CDRNN hidden state:\nhRNNx,y def = WRNNprojhRNN(LRNN)x,y (17)\nTo apply the CDRNN model to data, a mask F ∈ {0, 1}Y×X admits only those observations in X that precede each y[y]:\nF[y,x] def = { 1 t[x] ≤ t′[y] 0 otherwise\n(18)\nLetting τx,y denote the temporal offset between the predictors at x and the response at y, i.e. τx,y def = t′[y]− t[x]. A total of S(K+1) sparse convolution matrices Gs,k ∈ RY×X are defined to contain the predicted response to each preceding event for the kth dimension of h(0)x,y and the sth parameter of the predictive distribution, masked by F:\nGs,k def =  g1,1(τ1,1)[s,k] · · · gX,1(τX,1)[s,k]... . . . ... g1,Y (τ1,Y )[s,k] · · · gX,Y (τX,Y )[s,k]  F (19)\n10The experiments in this study also consider a variant without the RNN component, which is mathematically equivalent to setting hRNNx,y = 0.\nThe convolved design matrix X′(s) ∈ RY×(K+1) for the sth parameter of the predictive distribution is then computed as:\nX ′(s) [∗,k] def = Gs,k [X, t][∗,k] (20)\nVector s ∈ RS contains global, population-level estimates of the parameters of the predictive distribution. Under the univariate normal predictive distribution assumed in this study, s contains the predictive mean (µ, i.e. the intercept) and variance (σ2):\ns def = [ µ σ2 ] (21)\nMatrix SZ contains random predictive distribution parameter estimates for the yth response sZy :\nSZ def = s Z 1 >\n... sZY >  (22) The vector of values for each response y for the sth predictive distribution parameter is given by summing the population value, random effects values, and convolved response values:\nS[∗,s] def = fconstraint(s) ( X′(s)1+ SZ[∗,s] + s[s] ) (23) where fconstraint(s) enforces any required constraints on the sth parameter of the predictive distribution. In the Gaussian predictive distribution assumed here, fconstraint(1) (the constraint function for the mean) is identity and fconstraint(2) (the constraint function for the variance) is the softplus bijection:\nsoftplus(x) def= ln(ex + 1) (24)\nGiven an assumed distributional family F (here assumed to be univariate normal), the response in the CDRNN model is distributed as:\ny ∼ F ( S[∗,1], . . . ,S[∗,S] ) (25)"
    }, {
      "heading" : "B Asynchronously Measured Predictor Dimensions",
      "text" : "As discussed in Shain and Schuler (2018, 2021), CDR applies straightforwardly to time series with asynchronous predictor vectors and response values (i.e. measured at different times, such as word onsets that do not align with fMRI scan times). The CDR implementation of Shain and Schuler\n(2021) also supports asynchronously measured dimensions of the predictor matrix, simply by providing each predictor dimension with its own vector of timestamps. This allows e.g. Shain et al. (2020) to regress linguistic features (which are word-aligned) and sound power (which in their definition is measured at regular 100ms intervals) in the same model. Supporting asynchronously measured predictor dimensions is more challenging in CDRNN, especially if the RNN component is used. The solution used in CDR is not available because input dimensions that do not align in time are (1) arbitrarily grouped together and (2) erroneously treated as steps in the RNN input sequence. A more principled solution is to interleave the predictors in time order and pad irrelevant dimensions with zeros. For example, in a model with predictor A and predictor B that are sampled at different times, the values of A and B are temporally sorted together into a single time series, with the B value of A events set to zero and the A value of B events set to zero. This approach carries a computational cost: unlike CDR, the number of inputs to the convolution scales linearly on the number of asynchronously measured sets of predictors in the model."
    }, {
      "heading" : "C Predictors",
      "text" : "The following predictors are common to all models presented here:\n• Rate (CDR/NN only): The deconvolutional intercept, i.e. the base response to a stimulus, independent of its features. In CDR, rate is estimated explicitly by fitting an IRF to intercept vector (Shain and Schuler, 2021) (i.e., implicitly, the response when all predictors are 0). In CDRNN, rate is a reference response, computed by taking the response to an average stimulus (since the zero vector may unlikely for a given input distribution, using it as a reference may not reliably reflect the model’s domain knowledge). In this study, all other IRF queries subtract out rate in order to show deviation from the reference.\n• Unigram surprisal: The negative log of the smoothed context-independent probability of a word according to a unigram KenLM model (Heafield et al., 2013) trained on Gigaword 3 (Graff et al., 2007). While this quantity is typically treated on a frequency or log probability scale in psycholinguistics, it is treated here on\na surprisal (negative log prob) scale simply for easy of comparison with 5-gram surprisal (below), even though it is not a good estimate of the quantity typically targeted by surprisal (contextual predictability), since context is ignored.\n• 5-gram surprisal: The negative log of the smoothed probability of a word given the four preceding words according to a 5-gram KenLM model (Heafield et al., 2013) trained on Gigaword 3 (Graff et al., 2007).\nThe following predictor is used in all reading models:\n• Word length: The length of the word in characters.\nThe following predictors are used in eye-tracking models:\n• Saccade length: The length in words of the incoming saccade (eye movement), including the current word.\n• Previous was fixated: Indicator for whether the most recent fixation was to the immediately preceding word.\nReplications of Shain et al. (2020) use the following additional predictors:\n• PCFG surprisal: Lexicalized probabilistic context-free grammar surprisal computed using the incremental left-corner parser of van Schijndel et al. (2013) trained on a generalized categorial grammar (Nguyen et al., 2012) reannotation of Wall Street Journal sections 2 through 21 of the Penn Treebank (Marcus et al., 1993).\n• Sound power: Stimulus sound power (root mean squared energy), averaged over 250ms intervals. This implementation differs slightly from that of Shain et al. (2020), who sampled the measure every 100ms. The longer interval is designed to provide coverage over the extent of the HRF in this study, which uses a shorter history window for computational reasons (128 timesteps instead of 256). Both for computational reasons, especially under CDRNN-RNN (Appendix B) and because prior sound power estimates in this dataset have been weak (Shain et al., 2020), sound power is omitted from models used in the main comparison."
    } ],
    "references" : [ {
      "title" : "TensorFlow: Large-Scale Machine Learning on Heterogeneous Distributed Systems",
      "author" : [ "Fernanda Viégas", "Oriol Vinyals", "Pete Warden", "Martin Wattenberg", "Martin Wicke", "Yuan Yu", "Xiaoqiang Zheng" ],
      "venue" : null,
      "citeRegEx" : "Viégas et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Viégas et al\\.",
      "year" : 2015
    }, {
      "title" : "Memory Requirements and Local Ambiguities of Parsing Strategies",
      "author" : [ "Steven P Abney", "Mark Johnson." ],
      "venue" : "J.\\ Psycholinguistic Research, 20(3):233–250.",
      "citeRegEx" : "Abney and Johnson.,? 1991",
      "shortCiteRegEx" : "Abney and Johnson.",
      "year" : 1991
    }, {
      "title" : "Noun-Phrase Anaphora and Focus: The Informational Load Hypothesis",
      "author" : [ "Amit Almor." ],
      "venue" : "Psychological Review, 106(4):748–765.",
      "citeRegEx" : "Almor.,? 1999",
      "shortCiteRegEx" : "Almor.",
      "year" : 1999
    }, {
      "title" : "The cave of shadows: Addressing the human factor with generalized additive mixed models",
      "author" : [ "Harald Baayen", "Shravan Vasishth", "Reinhold Kliegl", "Douglas Bates." ],
      "venue" : "Journal of Memory and Language, 94(Supplement C):206–234.",
      "citeRegEx" : "Baayen et al\\.,? 2017",
      "shortCiteRegEx" : "Baayen et al\\.",
      "year" : 2017
    }, {
      "title" : "Mixed effects modelling with crossed random effects for subjects and items",
      "author" : [ "R Harald Baayen", "Doug J Davidson", "Douglas M Bates." ],
      "venue" : "manuscript.",
      "citeRegEx" : "Baayen et al\\.,? 2007",
      "shortCiteRegEx" : "Baayen et al\\.",
      "year" : 2007
    }, {
      "title" : "Autocorrelated errors in experimental data in the language sciences: Some solutions offered by Generalized Additive Mixed Models",
      "author" : [ "R Harald Baayen", "Jacolien van Rij", "Cecile de Cat", "Simon Wood." ],
      "venue" : "Dirk Speelman, Kris Heylen, and Dirk Geer-",
      "citeRegEx" : "Baayen et al\\.,? 2018",
      "shortCiteRegEx" : "Baayen et al\\.",
      "year" : 2018
    }, {
      "title" : "Word length and the structure of short term memory",
      "author" : [ "Alan D Baddeley", "Neil Thomson", "Mary Buchanan." ],
      "venue" : "Journal of Verbal Learning and Verbal Behavior, 15(6):575–589.",
      "citeRegEx" : "Baddeley et al\\.,? 1975",
      "shortCiteRegEx" : "Baddeley et al\\.",
      "year" : 1975
    }, {
      "title" : "Fitting linear mixed-effects models using lme4",
      "author" : [ "Douglas Bates", "Martin Mächler", "Ben Bolker", "Steve Walker." ],
      "venue" : "Journal of Statistical Software, 67(1):1–",
      "citeRegEx" : "Bates et al\\.,? 2015",
      "shortCiteRegEx" : "Bates et al\\.",
      "year" : 2015
    }, {
      "title" : "On the control of eye saccades in reading",
      "author" : [ "H Bouma", "A H De Voogd." ],
      "venue" : "Vision Research, 14(4):273–284.",
      "citeRegEx" : "Bouma and Voogd.,? 1974",
      "shortCiteRegEx" : "Bouma and Voogd.",
      "year" : 1974
    }, {
      "title" : "Unification of sentence processing via ear and eye: An fMRI study",
      "author" : [ "David Braze", "W Einar Mencl", "Whitney Tabor", "Kenneth R Pugh", "R Todd Constable", "Robert K Fulbright", "James S Magnuson", "Julie A Van Dyke", "Donald P Shankweiler." ],
      "venue" : "cortex,",
      "citeRegEx" : "Braze et al\\.,? 2011",
      "shortCiteRegEx" : "Braze et al\\.",
      "year" : 2011
    }, {
      "title" : "Syntactic structure building in the anterior temporal lobe during natural story listening",
      "author" : [ "Jonathan Brennan", "Yuval Nir", "Uri Hasson", "Rafael Malach", "David J Heeger", "Liina Pylkkänen." ],
      "venue" : "Brain and Language, 120(2):163–173.",
      "citeRegEx" : "Brennan et al\\.,? 2012",
      "shortCiteRegEx" : "Brennan et al\\.",
      "year" : 2012
    }, {
      "title" : "Word predictability effects are linear, not logarithmic: Implications for probabilistic models of sentence comprehension",
      "author" : [ "Trevor Brothers", "Gina R Kuperberg." ],
      "venue" : "Journal of Memory and Language, 116:104174.",
      "citeRegEx" : "Brothers and Kuperberg.,? 2021",
      "shortCiteRegEx" : "Brothers and Kuperberg.",
      "year" : 2021
    }, {
      "title" : "Language models are few-shot learners",
      "author" : [ "Sigler", "Mateusz Litwin", "Scott Gray", "Benjamin Chess", "Jack Clark", "Christopher Berner", "Dario Amodei." ],
      "venue" : "Proceedings of Advances in Neural Information Processing Systems 33.",
      "citeRegEx" : "Sigler et al\\.,? 2020",
      "shortCiteRegEx" : "Sigler et al\\.",
      "year" : 2020
    }, {
      "title" : "Advanced Bayesian Multilevel Modeling with the R Package brms",
      "author" : [ "Paul-Christian Bürkner." ],
      "venue" : "R Journal, 10(1).",
      "citeRegEx" : "Bürkner.,? 2018",
      "shortCiteRegEx" : "Bürkner.",
      "year" : 2018
    }, {
      "title" : "DRC: a dual route cascaded model of visual word recognition and reading aloud",
      "author" : [ "Max Coltheart", "Kathleen Rastle", "Conrad Perry", "Robyn Langdon", "Johannes Ziegler." ],
      "venue" : "Psychological review, 108(1):204.",
      "citeRegEx" : "Coltheart et al\\.,? 2001",
      "shortCiteRegEx" : "Coltheart et al\\.",
      "year" : 2001
    }, {
      "title" : "Deep gaussian processes",
      "author" : [ "Andreas Damianou", "Neil D Lawrence." ],
      "venue" : "Artificial intelligence and statistics, pages 207–215. PMLR.",
      "citeRegEx" : "Damianou and Lawrence.,? 2013",
      "shortCiteRegEx" : "Damianou and Lawrence.",
      "year" : 2013
    }, {
      "title" : "Data from eyetracking corpora as evidence for theories of syntactic processing complexity",
      "author" : [ "Vera Demberg", "Frank Keller." ],
      "venue" : "Cognition, 109(2):193–210.",
      "citeRegEx" : "Demberg and Keller.,? 2008",
      "shortCiteRegEx" : "Demberg and Keller.",
      "year" : 2008
    }, {
      "title" : "Statistical comparisons of classifiers over multiple data sets",
      "author" : [ "Janez Demšar." ],
      "venue" : "Journal of Machine Learning Research, 7(Jan):1–30.",
      "citeRegEx" : "Demšar.,? 2006",
      "shortCiteRegEx" : "Demšar.",
      "year" : 2006
    }, {
      "title" : "Bert: Pre-training of deep bidirectional transformers for language understanding",
      "author" : [ "Jacob Devlin", "Ming-Wei Chang", "Kenton Lee", "Kristina Toutanova." ],
      "venue" : "NAACL19.",
      "citeRegEx" : "Devlin et al\\.,? 2019",
      "shortCiteRegEx" : "Devlin et al\\.",
      "year" : 2019
    }, {
      "title" : "Contextual effects on word perception and eye movements during reading",
      "author" : [ "Susan F Ehrlich", "Keith Rayner." ],
      "venue" : "Journal of verbal learning and verbal behavior, 20(6):641–655.",
      "citeRegEx" : "Ehrlich and Rayner.,? 1981",
      "shortCiteRegEx" : "Ehrlich and Rayner.",
      "year" : 1981
    }, {
      "title" : "Pronoun assignment and semantic integration during reading: Eye movements and immediacy of processing",
      "author" : [ "Kate Erlich", "Keith Rayner." ],
      "venue" : "Journal of Verbal Learning & Verbal Behavior, 22:75–87.",
      "citeRegEx" : "Erlich and Rayner.,? 1983",
      "shortCiteRegEx" : "Erlich and Rayner.",
      "year" : 1983
    }, {
      "title" : "New method for fMRI investigations of language: defining ROIs functionally in individual subjects",
      "author" : [ "Evelina Fedorenko", "Po-Jang Hsieh", "Alfonso NietoCastañón", "Susan Whitfield-Gabrieli", "Nancy Kanwisher." ],
      "venue" : "Journal of Neurophysiology,",
      "citeRegEx" : "Fedorenko et al\\.,? 2010",
      "shortCiteRegEx" : "Fedorenko et al\\.",
      "year" : 2010
    }, {
      "title" : "Sequential vs",
      "author" : [ "Victoria Fossum", "Roger Levy." ],
      "venue" : "Hierarchical Syntactic Models of Human Incremental Sentence Processing. In Proceedings of {{CMCL}} 2012. Association for Computational Linguistics.",
      "citeRegEx" : "Fossum and Levy.,? 2012",
      "shortCiteRegEx" : "Fossum and Levy.",
      "year" : 2012
    }, {
      "title" : "Insensitivity of the human sentence-processing system to hierarchical structure",
      "author" : [ "Stefan Frank", "Rens Bod." ],
      "venue" : "Psychological Science.",
      "citeRegEx" : "Frank and Bod.,? 2011",
      "shortCiteRegEx" : "Frank and Bod.",
      "year" : 2011
    }, {
      "title" : "Reading time data for evaluating broad-coverage models of English sentence processing",
      "author" : [ "Stefan L Frank", "Irene Fernandez Monsalve", "Robin L Thompson", "Gabriella Vigliocco." ],
      "venue" : "Behavior Research Methods, 45(4):1182–1190.",
      "citeRegEx" : "Frank et al\\.,? 2013",
      "shortCiteRegEx" : "Frank et al\\.",
      "year" : 2013
    }, {
      "title" : "The sausage machine: a new two-stage parsing model",
      "author" : [ "Lyn Frazier", "Jerry D Fodor." ],
      "venue" : "Cognition, 6:291–325.",
      "citeRegEx" : "Frazier and Fodor.,? 1978",
      "shortCiteRegEx" : "Frazier and Fodor.",
      "year" : 1978
    }, {
      "title" : "Nonlinear responses in fMRI: The Balloon model, Volterra kernels, and other hemodynamics",
      "author" : [ "Karl J Friston", "Andrea Mechelli", "Robert Turner", "Cathy J Price." ],
      "venue" : "NeuroImage, 12(4):466–477.",
      "citeRegEx" : "Friston et al\\.,? 2000",
      "shortCiteRegEx" : "Friston et al\\.",
      "year" : 2000
    }, {
      "title" : "The Natural Stories corpus: a reading-time corpus of English texts containing rare syntactic constructions",
      "author" : [ "Richard Futrell", "Edward Gibson", "Harry J Tily", "Idan Blank", "Anastasia Vishnevetsky", "Steven T Piantadosi", "Evelina Fedorenko." ],
      "venue" : "Language Re-",
      "citeRegEx" : "Futrell et al\\.,? 2020",
      "shortCiteRegEx" : "Futrell et al\\.",
      "year" : 2020
    }, {
      "title" : "Dropout as a Bayesian approximation: Representing model uncertainty in deep learning",
      "author" : [ "Yarin Gal", "Zoubin Ghahramani." ],
      "venue" : "international conference on machine learning, pages 1050–1059. PMLR.",
      "citeRegEx" : "Gal and Ghahramani.,? 2016",
      "shortCiteRegEx" : "Gal and Ghahramani.",
      "year" : 2016
    }, {
      "title" : "The Dependency Locality Theory: A distance-based theory of linguistic complexity",
      "author" : [ "Edward Gibson." ],
      "venue" : "Alec Marantz, Yasushi Miyashita, and Wayne O’Neil, editors, Image, language, brain, pages 95– 106. MIT Press, Cambridge.",
      "citeRegEx" : "Gibson.,? 2000",
      "shortCiteRegEx" : "Gibson.",
      "year" : 2000
    }, {
      "title" : "Predictive power of word surprisal for reading times is a linear function of language model quality",
      "author" : [ "Adam Goodkind", "Klinton Bicknell." ],
      "venue" : "Proceedings of the 8th Workshop on Cognitive Modeling and Computational Linguistics (CMCL 2018), pages",
      "citeRegEx" : "Goodkind and Bicknell.,? 2018",
      "shortCiteRegEx" : "Goodkind and Bicknell.",
      "year" : 2018
    }, {
      "title" : "Consequences of the serial nature of linguistic input",
      "author" : [ "Daniel J Grodner", "Edward Gibson." ],
      "venue" : "Cognitive Science, 29:261–291.",
      "citeRegEx" : "Grodner and Gibson.,? 2005",
      "shortCiteRegEx" : "Grodner and Gibson.",
      "year" : 2005
    }, {
      "title" : "Colorless Green Recurrent Networks Dream Hierarchically",
      "author" : [ "Kristina Gulordava", "Piotr Bojanowski", "Édouard Grave", "Tal Linzen", "Marco Baroni." ],
      "venue" : "Proceedings of the 2018 Conference of the North American Chapter of the Association for Computa-",
      "citeRegEx" : "Gulordava et al\\.,? 2018",
      "shortCiteRegEx" : "Gulordava et al\\.",
      "year" : 2018
    }, {
      "title" : "A Probabilistic Earley Parser as a Psycholinguistic Model",
      "author" : [ "John Hale." ],
      "venue" : "Proceedings of the second meeting of the North American chapter of the Association for Computational Linguistics, pages 159– 166, Pittsburgh, PA.",
      "citeRegEx" : "Hale.,? 2001",
      "shortCiteRegEx" : "Hale.",
      "year" : 2001
    }, {
      "title" : "A failure to replicate rapid syntactic adaptation in comprehension",
      "author" : [ "Caoimhe M Harrington Stack", "Ariel N James", "Duane G Watson." ],
      "venue" : "Memory & cognition, 46(6):864–877.",
      "citeRegEx" : "Stack et al\\.,? 2018",
      "shortCiteRegEx" : "Stack et al\\.",
      "year" : 2018
    }, {
      "title" : "Generalized additive models",
      "author" : [ "Trevor Hastie", "Robert Tibshirani." ],
      "venue" : "Statist. Sci., 1(3):297–310.",
      "citeRegEx" : "Hastie and Tibshirani.,? 1986",
      "shortCiteRegEx" : "Hastie and Tibshirani.",
      "year" : 1986
    }, {
      "title" : "Deep residual learning for image recognition",
      "author" : [ "Kaiming He", "Xiangyu Zhang", "Shaoqing Ren", "Jian Sun." ],
      "venue" : "Proceedings of the IEEE conference on computer vision and pattern recognition, pages 770– 778.",
      "citeRegEx" : "He et al\\.,? 2016",
      "shortCiteRegEx" : "He et al\\.",
      "year" : 2016
    }, {
      "title" : "Scalable modified KneserNey language model estimation",
      "author" : [ "Kenneth Heafield", "Ivan Pouzyrevsky", "Jonathan H Clark", "Philipp Koehn." ],
      "venue" : "Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 690–696, Sofia, Bul-",
      "citeRegEx" : "Heafield et al\\.,? 2013",
      "shortCiteRegEx" : "Heafield et al\\.",
      "year" : 2013
    }, {
      "title" : "Language structure in the brain: A fixation-related fMRI study of syntactic surprisal in reading",
      "author" : [ "John M Henderson", "Wonil Choi", "Matthew W Lowder", "Fernanda Ferreira." ],
      "venue" : "Neuroimage, 132:293–300.",
      "citeRegEx" : "Henderson et al\\.,? 2016",
      "shortCiteRegEx" : "Henderson et al\\.",
      "year" : 2016
    }, {
      "title" : "Neural correlates of fixation duration in natural reading: evidence from fixationrelated fMRI",
      "author" : [ "John M Henderson", "Wonil Choi", "Steven G Luke", "Rutvik H Desai." ],
      "venue" : "NeuroImage, 119:390–397.",
      "citeRegEx" : "Henderson et al\\.,? 2015",
      "shortCiteRegEx" : "Henderson et al\\.",
      "year" : 2015
    }, {
      "title" : "Gaussian error linear units (GELUs)",
      "author" : [ "Dan Hendrycks", "Kevin Gimpel." ],
      "venue" : "arXiv preprint arXiv:1606.08415.",
      "citeRegEx" : "Hendrycks and Gimpel.,? 2016",
      "shortCiteRegEx" : "Hendrycks and Gimpel.",
      "year" : 2016
    }, {
      "title" : "A structural probe for finding syntax in word representations",
      "author" : [ "John Hewitt", "Christopher D Manning." ],
      "venue" : "Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Tech-",
      "citeRegEx" : "Hewitt and Manning.,? 2019",
      "shortCiteRegEx" : "Hewitt and Manning.",
      "year" : 2019
    }, {
      "title" : "Long Short-Term Memory",
      "author" : [ "Sepp Hochreiter", "Jürgen Schmidhuber." ],
      "venue" : "Neural Comput., 9(8):1735– 1780.",
      "citeRegEx" : "Hochreiter and Schmidhuber.,? 1997",
      "shortCiteRegEx" : "Hochreiter and Schmidhuber.",
      "year" : 1997
    }, {
      "title" : "Natural speech reveals the semantic maps that tile human cerebral cortex",
      "author" : [ "Alexander G Huth", "Wendy A de Heer", "Thomas L Griffiths", "Frédéric E Theunissen", "Jack L Gallant." ],
      "venue" : "Nature, 532(7600):453.",
      "citeRegEx" : "Huth et al\\.,? 2016",
      "shortCiteRegEx" : "Huth et al\\.",
      "year" : 2016
    }, {
      "title" : "Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift",
      "author" : [ "Sergey Ioffe", "Christian Szegedy." ],
      "venue" : "International Conference on Machine Learning, pages 448–456.",
      "citeRegEx" : "Ioffe and Szegedy.,? 2015",
      "shortCiteRegEx" : "Ioffe and Szegedy.",
      "year" : 2015
    }, {
      "title" : "A theory of reading: From eye fixations to comprehension",
      "author" : [ "Marcel Adam Just", "Patricia A Carpenter." ],
      "venue" : "Psychological Review, 87(4):329–354.",
      "citeRegEx" : "Just and Carpenter.,? 1980",
      "shortCiteRegEx" : "Just and Carpenter.",
      "year" : 1980
    }, {
      "title" : "The Dundee corpus",
      "author" : [ "Alan Kennedy", "James Pynte", "Robin Hill." ],
      "venue" : "Proceedings of the 12th European conference on eye movement.",
      "citeRegEx" : "Kennedy et al\\.,? 2003",
      "shortCiteRegEx" : "Kennedy et al\\.",
      "year" : 2003
    }, {
      "title" : "Adam: A Method for Stochastic Optimization",
      "author" : [ "Diederik P Kingma", "Jimmy Ba." ],
      "venue" : "CoRR, abs/1412.6.",
      "citeRegEx" : "Kingma and Ba.,? 2014",
      "shortCiteRegEx" : "Kingma and Ba.",
      "year" : 2014
    }, {
      "title" : "Expectation-based syntactic comprehension",
      "author" : [ "Roger Levy." ],
      "venue" : "Cognition, 106(3):1126–1177.",
      "citeRegEx" : "Levy.,? 2008",
      "shortCiteRegEx" : "Levy.",
      "year" : 2008
    }, {
      "title" : "An activation-based model of sentence processing as skilled memory retrieval",
      "author" : [ "Richard L Lewis", "Shravan Vasishth." ],
      "venue" : "Cognitive Science, 29(3):375–419.",
      "citeRegEx" : "Lewis and Vasishth.,? 2005",
      "shortCiteRegEx" : "Lewis and Vasishth.",
      "year" : 2005
    }, {
      "title" : "Modeling the hemodynamic response function in fMRI: Efficiency, bias and mismodeling",
      "author" : [ "Martin A Lindquist", "Ji Meng Loh", "Lauren Y Atlas", "Tor D Wager." ],
      "venue" : "NeuroImage, 45(1, Supplement 1):S187 – S198.",
      "citeRegEx" : "Lindquist et al\\.,? 2009",
      "shortCiteRegEx" : "Lindquist et al\\.",
      "year" : 2009
    }, {
      "title" : "Assessing the ability of LSTMs to learn syntax-sensitive dependencies",
      "author" : [ "Tal Linzen", "Emmanuel Dupoux", "Yoav Goldberg." ],
      "venue" : "Transactions of the Association for Computational Linguistics, 4:521– 535.",
      "citeRegEx" : "Linzen et al\\.,? 2016",
      "shortCiteRegEx" : "Linzen et al\\.",
      "year" : 2016
    }, {
      "title" : "Using stochastic language models (SLM) to map lexical, syntactic, and phonological information processing in the brain",
      "author" : [ "Alessandro Lopopolo", "Stefan L Frank", "Antal den Bosch", "Roel M Willems." ],
      "venue" : "PloS one, 12(5):e0177794.",
      "citeRegEx" : "Lopopolo et al\\.,? 2017",
      "shortCiteRegEx" : "Lopopolo et al\\.",
      "year" : 2017
    }, {
      "title" : "Building a large annotated corpus of English: the Penn Treebank",
      "author" : [ "Mitchell P Marcus", "Beatrice Santorini", "Mary Ann Marcinkiewicz." ],
      "venue" : "Computational Linguistics, 19(2):313–330.",
      "citeRegEx" : "Marcus et al\\.,? 1993",
      "shortCiteRegEx" : "Marcus et al\\.",
      "year" : 1993
    }, {
      "title" : "An evaluation of subject-paced reading tasks and other methods for investigating immediate processes in reading",
      "author" : [ "Don C Mitchell." ],
      "venue" : "New methods in reading comprehension research, pages 69–89.",
      "citeRegEx" : "Mitchell.,? 1984",
      "shortCiteRegEx" : "Mitchell.",
      "year" : 1984
    }, {
      "title" : "An incremental information-theoretic buffer supports sentence processing",
      "author" : [ "Francis Mollica", "Steve Piantadosi." ],
      "venue" : "Proceedings of the 39th Annual Cognitive Science Society Meeting.",
      "citeRegEx" : "Mollica and Piantadosi.,? 2017",
      "shortCiteRegEx" : "Mollica and Piantadosi.",
      "year" : 2017
    }, {
      "title" : "The effects of context upon speed of reading, eye movements and eye-voice span",
      "author" : [ "John Morton." ],
      "venue" : "Quarterly Journal of Experimental Psychology, 16(4):340–354.",
      "citeRegEx" : "Morton.,? 1964",
      "shortCiteRegEx" : "Morton.",
      "year" : 1964
    }, {
      "title" : "Accurate Unbounded Dependency Recovery using Generalized Categorial Grammars",
      "author" : [ "Luan Nguyen", "Marten van Schijndel", "William Schuler." ],
      "venue" : "Proceedings of COLING 2012.",
      "citeRegEx" : "Nguyen et al\\.,? 2012",
      "shortCiteRegEx" : "Nguyen et al\\.",
      "year" : 2012
    }, {
      "title" : "The Bayesian Reader: Explaining word recognition as an optimal Bayesian decision process",
      "author" : [ "Dennis Norris." ],
      "venue" : "Psychological review, 113(2):327.",
      "citeRegEx" : "Norris.,? 2006",
      "shortCiteRegEx" : "Norris.",
      "year" : 2006
    }, {
      "title" : "Deep contextualized word representations",
      "author" : [ "Matthew E Peters", "Mark Neumann", "Mohit Iyyer", "Matt Gardner", "Christopher Clark", "Kenton Lee", "Luke Zettlemoyer." ],
      "venue" : "arXiv preprint arXiv:1802.05365.",
      "citeRegEx" : "Peters et al\\.,? 2018",
      "shortCiteRegEx" : "Peters et al\\.",
      "year" : 2018
    }, {
      "title" : "RISE: Randomized Input Sampling for Explanation of Black-box Models",
      "author" : [ "Vitali Petsiuk", "Abir Das", "Kate Saenko." ],
      "venue" : "Proceedings of the British Machine Vision Conference (BMVC).",
      "citeRegEx" : "Petsiuk et al\\.,? 2018",
      "shortCiteRegEx" : "Petsiuk et al\\.",
      "year" : 2018
    }, {
      "title" : "The representation of verbs: Evidence from syntactic priming in language production",
      "author" : [ "Martin J Pickering", "Holly P Branigan." ],
      "venue" : "Journal of Memory and language, 39(4):633–651.",
      "citeRegEx" : "Pickering and Branigan.,? 1998",
      "shortCiteRegEx" : "Pickering and Branigan.",
      "year" : 1998
    }, {
      "title" : "Acceleration of stochastic approximation by averaging",
      "author" : [ "Boris T Polyak", "Anatoli B Juditsky." ],
      "venue" : "SIAM Journal on Control and Optimization, 30(4):838–855.",
      "citeRegEx" : "Polyak and Juditsky.,? 1992",
      "shortCiteRegEx" : "Polyak and Juditsky.",
      "year" : 1992
    }, {
      "title" : "Rapid syntactic adaptation in self-paced reading: detectable, but requires many participants",
      "author" : [ "Grusha Prasad", "Tal Linzen" ],
      "venue" : null,
      "citeRegEx" : "Prasad and Linzen.,? \\Q2019\\E",
      "shortCiteRegEx" : "Prasad and Linzen.",
      "year" : 2019
    }, {
      "title" : "Language models are unsupervised multitask learners",
      "author" : [ "Alec Radford", "Jeffrey Wu", "Rewon Child", "David Luan", "Dario Amodei", "Ilya Sutskever." ],
      "venue" : "OpenAI Blog, 1(8):9.",
      "citeRegEx" : "Radford et al\\.,? 2019",
      "shortCiteRegEx" : "Radford et al\\.",
      "year" : 2019
    }, {
      "title" : "Visual attention in reading: Eye movements reflect cognitive processes",
      "author" : [ "Keith Rayner." ],
      "venue" : "Memory \\& Cognition, 5(4):443–448.",
      "citeRegEx" : "Rayner.,? 1977",
      "shortCiteRegEx" : "Rayner.",
      "year" : 1977
    }, {
      "title" : "Eye Movements in Reading and Information Processing: 20 Years of Research",
      "author" : [ "Keith Rayner." ],
      "venue" : "Psychological Bulletin, 124(3):372–422.",
      "citeRegEx" : "Rayner.,? 1998",
      "shortCiteRegEx" : "Rayner.",
      "year" : 1998
    }, {
      "title" : "Why should I trust you?” Explaining the predictions of any classifier",
      "author" : [ "Marco Tulio Ribeiro", "Sameer Singh", "Carlos Guestrin." ],
      "venue" : "Proceedings of the 22nd ACM SIGKDD international conference on knowledge discovery and data mining,",
      "citeRegEx" : "Ribeiro et al\\.,? 2016",
      "shortCiteRegEx" : "Ribeiro et al\\.",
      "year" : 2016
    }, {
      "title" : "A model of language processing as hierarchic sequential prediction",
      "author" : [ "Marten van Schijndel", "Andy Exley", "William Schuler." ],
      "venue" : "Topics in Cognitive Science, 5(3):522–540.",
      "citeRegEx" : "Schijndel et al\\.,? 2013",
      "shortCiteRegEx" : "Schijndel et al\\.",
      "year" : 2013
    }, {
      "title" : "An Analysis of Frequency- and Memory-Based Processing Costs",
      "author" : [ "Marten van Schijndel", "William Schuler." ],
      "venue" : "Proceedings of NAACL-HLT 2013. Association for Computational Linguistics.",
      "citeRegEx" : "Schijndel and Schuler.,? 2013",
      "shortCiteRegEx" : "Schijndel and Schuler.",
      "year" : 2013
    }, {
      "title" : "Hierarchic syntax improves reading time prediction",
      "author" : [ "Marten van Schijndel", "William Schuler." ],
      "venue" : "Proceedings of NAACL-HLT 2015. Association for Computational Linguistics.",
      "citeRegEx" : "Schijndel and Schuler.,? 2015",
      "shortCiteRegEx" : "Schijndel and Schuler.",
      "year" : 2015
    }, {
      "title" : "Artificial Neural Networks Accurately Predict Language Processing in the Brain",
      "author" : [ "Martin Schrimpf", "Idan A Blank", "Greta Tuckute", "Carina Kauf", "Eghbal A Hosseini", "Nancy G Kanwisher", "Joshua B Tenenbaum", "Evelina Fedorenko." ],
      "venue" : "BioRxiv.",
      "citeRegEx" : "Schrimpf et al\\.,? 2020",
      "shortCiteRegEx" : "Schrimpf et al\\.",
      "year" : 2020
    }, {
      "title" : "fMRI reveals language-specific predictive coding during naturalistic sentence comprehension",
      "author" : [ "Cory Shain", "Idan Blank", "Marten van Schijndel", "William Schuler", "Evelina Fedorenko." ],
      "venue" : "Neuropsychologia, 138.",
      "citeRegEx" : "Shain et al\\.,? 2020",
      "shortCiteRegEx" : "Shain et al\\.",
      "year" : 2020
    }, {
      "title" : "Deconvolutional time series regression: A technique for modeling temporally diffuse effects",
      "author" : [ "Cory Shain", "William Schuler." ],
      "venue" : "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing.",
      "citeRegEx" : "Shain and Schuler.,? 2018",
      "shortCiteRegEx" : "Shain and Schuler.",
      "year" : 2018
    }, {
      "title" : "ContinuousTime Deconvolutional Regression for Psycholinguistic Modeling",
      "author" : [ "Cory Shain", "William Schuler." ],
      "venue" : "Cognition.",
      "citeRegEx" : "Shain and Schuler.,? 2021",
      "shortCiteRegEx" : "Shain and Schuler.",
      "year" : 2021
    }, {
      "title" : "Discrete approximations to continuous time distributed lags in econometrics",
      "author" : [ "Christopher A Sims." ],
      "venue" : "Econometrica: Journal of the Econometric Society, pages 545–563.",
      "citeRegEx" : "Sims.,? 1971",
      "shortCiteRegEx" : "Sims.",
      "year" : 1971
    }, {
      "title" : "The effect of word predictability on reading time is logarithmic",
      "author" : [ "Nathaniel J Smith", "Roger Levy." ],
      "venue" : "Cognition, 128:302–319.",
      "citeRegEx" : "Smith and Levy.,? 2013",
      "shortCiteRegEx" : "Smith and Levy.",
      "year" : 2013
    }, {
      "title" : "Dropout: A simple way to prevent neural networks from overfitting",
      "author" : [ "Nitish Srivastava", "Geoffrey Hinton", "Alex Krizhevsky", "Ilya Sutskever", "Ruslan Salakhutdinov." ],
      "venue" : "The Journal of Machine Learning Research, 15(1):1929–1958.",
      "citeRegEx" : "Srivastava et al\\.,? 2014",
      "shortCiteRegEx" : "Srivastava et al\\.",
      "year" : 2014
    }, {
      "title" : "The effect of lexical predictability on eye movements in reading: Critical review and theoretical interpretation",
      "author" : [ "Adrian Staub." ],
      "venue" : "Language and Linguistics Compass, 9(8):311–327.",
      "citeRegEx" : "Staub.,? 2015",
      "shortCiteRegEx" : "Staub.",
      "year" : 2015
    }, {
      "title" : "Integration of visual and linguistic information in spoken language comprehension",
      "author" : [ "Michael K Tanenhaus", "Michael J Spivey-Knowlton", "Kathleen M Eberhard", "Julie C E Sedivy." ],
      "venue" : "Science, 268:1632–1634.",
      "citeRegEx" : "Tanenhaus et al\\.,? 1995",
      "shortCiteRegEx" : "Tanenhaus et al\\.",
      "year" : 1995
    }, {
      "title" : "BERT rediscovers the classical NLP pipeline",
      "author" : [ "Ian Tenney", "Dipanjan Das", "Ellie Pavlick." ],
      "venue" : "ACL19.",
      "citeRegEx" : "Tenney et al\\.,? 2019",
      "shortCiteRegEx" : "Tenney et al\\.",
      "year" : 2019
    }, {
      "title" : "Argument-head distance and processing complexity: Explaining both locality and antilocality effects",
      "author" : [ "Shravan Vasishth", "Richard L Lewis." ],
      "venue" : "Language, 82(4):767–794.",
      "citeRegEx" : "Vasishth and Lewis.,? 2006",
      "shortCiteRegEx" : "Vasishth and Lewis.",
      "year" : 2006
    }, {
      "title" : "Vascular dynamics and BOLD fMRI: CBF level effects and analysis considerations",
      "author" : [ "Alberto L Vazquez", "Eric R Cohen", "Vikas Gulani", "Luis Hernandez-Garcia", "Ying Zheng", "Gregory R Lee", "Seong-Gi Kim", "James B Grotberg", "Douglas C Noll." ],
      "venue" : "Neu-",
      "citeRegEx" : "Vazquez et al\\.,? 2006",
      "shortCiteRegEx" : "Vazquez et al\\.",
      "year" : 2006
    }, {
      "title" : "Accounting for nonlinear BOLD effects in fMRI: parameter estimates and a model for prediction in rapid event-related studies",
      "author" : [ "Tor D Wager", "Alberto Vazquez", "Luis Hernandez", "Douglas C Noll." ],
      "venue" : "NeuroImage, 25(1):206–218.",
      "citeRegEx" : "Wager et al\\.,? 2005",
      "shortCiteRegEx" : "Wager et al\\.",
      "year" : 2005
    }, {
      "title" : "Incremental language comprehension difficulty predicts activity in the language",
      "author" : [ "Leila Wehbe", "Idan A Blank", "Cory Shain", "Richard Futrell", "Roger Levy", "Titus von der Malsburg", "Nathaniel Smith", "Edward Gibson", "Evelina Fedorenko" ],
      "venue" : null,
      "citeRegEx" : "Wehbe et al\\.,? \\Q2020\\E",
      "shortCiteRegEx" : "Wehbe et al\\.",
      "year" : 2020
    }, {
      "title" : "Hierarchical Representation in Neural Language Models: Suppression and Recovery of Expectations",
      "author" : [ "Ethan Wilcox", "Roger Levy", "Richard Futrell." ],
      "venue" : "Proceedings of the 2019 ACL Workshop BlackboxNLP: Analyzing and Interpreting Neural Net-",
      "citeRegEx" : "Wilcox et al\\.,? 2019",
      "shortCiteRegEx" : "Wilcox et al\\.",
      "year" : 2019
    }, {
      "title" : "Prediction during natural language comprehension",
      "author" : [ "Roel M Willems", "Stefan L Frank", "Annabel D Nijhof", "Peter Hagoort", "Antal den Bosch." ],
      "venue" : "Cerebral Cortex, 26(6):2506–2516.",
      "citeRegEx" : "Willems et al\\.,? 2015",
      "shortCiteRegEx" : "Willems et al\\.",
      "year" : 2015
    }, {
      "title" : "Generalized Additive Models: An Introduction with R",
      "author" : [ "Simon N Wood." ],
      "venue" : "Chapman and Hall/CRC, Boca Raton.",
      "citeRegEx" : "Wood.,? 2006",
      "shortCiteRegEx" : "Wood.",
      "year" : 2006
    }, {
      "title" : "Appendix B) and because prior sound power estimates in this dataset have been weak (Shain et al., 2020), sound power is omitted from models used in the main comparison",
      "author" : [ "der CDRNN-RNN" ],
      "venue" : null,
      "citeRegEx" : "CDRNN.RNN,? \\Q2020\\E",
      "shortCiteRegEx" : "CDRNN.RNN",
      "year" : 2020
    } ],
    "referenceMentions" : [ {
      "referenceID" : 23,
      "context" : ", 2015) that enable direct word-by-word analysis of effects in naturalistic human language processing (e.g. Demberg and Keller, 2008; Frank and Bod, 2011).",
      "startOffset" : 102,
      "endOffset" : 154
    }, {
      "referenceID" : 45,
      "context" : "time and experiences computational bottlenecks of various kinds (Bouma and De Voogd, 1974; Just and Carpenter, 1980; Ehrlich and Rayner, 1981; Mollica and Piantadosi, 2017), delayed effects may be pervasive, and, if left uncontrolled, can yield",
      "startOffset" : 64,
      "endOffset" : 172
    }, {
      "referenceID" : 19,
      "context" : "time and experiences computational bottlenecks of various kinds (Bouma and De Voogd, 1974; Just and Carpenter, 1980; Ehrlich and Rayner, 1981; Mollica and Piantadosi, 2017), delayed effects may be pervasive, and, if left uncontrolled, can yield",
      "startOffset" : 64,
      "endOffset" : 172
    }, {
      "referenceID" : 55,
      "context" : "time and experiences computational bottlenecks of various kinds (Bouma and De Voogd, 1974; Just and Carpenter, 1980; Ehrlich and Rayner, 1981; Mollica and Piantadosi, 2017), delayed effects may be pervasive, and, if left uncontrolled, can yield",
      "startOffset" : 64,
      "endOffset" : 172
    }, {
      "referenceID" : 51,
      "context" : ", 2020, inter alia), in part by better approximating the complex dynamics of human cognition as encoded in natural language (Linzen et al., 2016; Gulordava et al., 2018; Tenney et al., 2019; Hewitt and Manning, 2019; Wilcox et al., 2019; Schrimpf et al., 2020).",
      "startOffset" : 124,
      "endOffset" : 260
    }, {
      "referenceID" : 32,
      "context" : ", 2020, inter alia), in part by better approximating the complex dynamics of human cognition as encoded in natural language (Linzen et al., 2016; Gulordava et al., 2018; Tenney et al., 2019; Hewitt and Manning, 2019; Wilcox et al., 2019; Schrimpf et al., 2020).",
      "startOffset" : 124,
      "endOffset" : 260
    }, {
      "referenceID" : 80,
      "context" : ", 2020, inter alia), in part by better approximating the complex dynamics of human cognition as encoded in natural language (Linzen et al., 2016; Gulordava et al., 2018; Tenney et al., 2019; Hewitt and Manning, 2019; Wilcox et al., 2019; Schrimpf et al., 2020).",
      "startOffset" : 124,
      "endOffset" : 260
    }, {
      "referenceID" : 41,
      "context" : ", 2020, inter alia), in part by better approximating the complex dynamics of human cognition as encoded in natural language (Linzen et al., 2016; Gulordava et al., 2018; Tenney et al., 2019; Hewitt and Manning, 2019; Wilcox et al., 2019; Schrimpf et al., 2020).",
      "startOffset" : 124,
      "endOffset" : 260
    }, {
      "referenceID" : 85,
      "context" : ", 2020, inter alia), in part by better approximating the complex dynamics of human cognition as encoded in natural language (Linzen et al., 2016; Gulordava et al., 2018; Tenney et al., 2019; Hewitt and Manning, 2019; Wilcox et al., 2019; Schrimpf et al., 2020).",
      "startOffset" : 124,
      "endOffset" : 260
    }, {
      "referenceID" : 71,
      "context" : ", 2020, inter alia), in part by better approximating the complex dynamics of human cognition as encoded in natural language (Linzen et al., 2016; Gulordava et al., 2018; Tenney et al., 2019; Hewitt and Manning, 2019; Wilcox et al., 2019; Schrimpf et al., 2020).",
      "startOffset" : 124,
      "endOffset" : 260
    }, {
      "referenceID" : 56,
      "context" : "Psycholinguists have been aware for decades that processing effects may lag behind the words that trigger them (Morton, 1964; Bouma and De Voogd, 1974; Rayner, 1977; Erlich and Rayner, 1983; Mitchell, 1984; Rayner, 1998; Vasishth and Lewis, 2006; Smith and Levy, 2013), possibly because cognitive “buffers” may exist to allow higher-level information processing to catch up with the input (Bouma and De Voogd, 1974; Baddeley et al.",
      "startOffset" : 111,
      "endOffset" : 268
    }, {
      "referenceID" : 65,
      "context" : "Psycholinguists have been aware for decades that processing effects may lag behind the words that trigger them (Morton, 1964; Bouma and De Voogd, 1974; Rayner, 1977; Erlich and Rayner, 1983; Mitchell, 1984; Rayner, 1998; Vasishth and Lewis, 2006; Smith and Levy, 2013), possibly because cognitive “buffers” may exist to allow higher-level information processing to catch up with the input (Bouma and De Voogd, 1974; Baddeley et al.",
      "startOffset" : 111,
      "endOffset" : 268
    }, {
      "referenceID" : 20,
      "context" : "Psycholinguists have been aware for decades that processing effects may lag behind the words that trigger them (Morton, 1964; Bouma and De Voogd, 1974; Rayner, 1977; Erlich and Rayner, 1983; Mitchell, 1984; Rayner, 1998; Vasishth and Lewis, 2006; Smith and Levy, 2013), possibly because cognitive “buffers” may exist to allow higher-level information processing to catch up with the input (Bouma and De Voogd, 1974; Baddeley et al.",
      "startOffset" : 111,
      "endOffset" : 268
    }, {
      "referenceID" : 54,
      "context" : "Psycholinguists have been aware for decades that processing effects may lag behind the words that trigger them (Morton, 1964; Bouma and De Voogd, 1974; Rayner, 1977; Erlich and Rayner, 1983; Mitchell, 1984; Rayner, 1998; Vasishth and Lewis, 2006; Smith and Levy, 2013), possibly because cognitive “buffers” may exist to allow higher-level information processing to catch up with the input (Bouma and De Voogd, 1974; Baddeley et al.",
      "startOffset" : 111,
      "endOffset" : 268
    }, {
      "referenceID" : 66,
      "context" : "Psycholinguists have been aware for decades that processing effects may lag behind the words that trigger them (Morton, 1964; Bouma and De Voogd, 1974; Rayner, 1977; Erlich and Rayner, 1983; Mitchell, 1984; Rayner, 1998; Vasishth and Lewis, 2006; Smith and Levy, 2013), possibly because cognitive “buffers” may exist to allow higher-level information processing to catch up with the input (Bouma and De Voogd, 1974; Baddeley et al.",
      "startOffset" : 111,
      "endOffset" : 268
    }, {
      "referenceID" : 81,
      "context" : "Psycholinguists have been aware for decades that processing effects may lag behind the words that trigger them (Morton, 1964; Bouma and De Voogd, 1974; Rayner, 1977; Erlich and Rayner, 1983; Mitchell, 1984; Rayner, 1998; Vasishth and Lewis, 2006; Smith and Levy, 2013), possibly because cognitive “buffers” may exist to allow higher-level information processing to catch up with the input (Bouma and De Voogd, 1974; Baddeley et al.",
      "startOffset" : 111,
      "endOffset" : 268
    }, {
      "referenceID" : 76,
      "context" : "Psycholinguists have been aware for decades that processing effects may lag behind the words that trigger them (Morton, 1964; Bouma and De Voogd, 1974; Rayner, 1977; Erlich and Rayner, 1983; Mitchell, 1984; Rayner, 1998; Vasishth and Lewis, 2006; Smith and Levy, 2013), possibly because cognitive “buffers” may exist to allow higher-level information processing to catch up with the input (Bouma and De Voogd, 1974; Baddeley et al.",
      "startOffset" : 111,
      "endOffset" : 268
    }, {
      "referenceID" : 6,
      "context" : "Psycholinguists have been aware for decades that processing effects may lag behind the words that trigger them (Morton, 1964; Bouma and De Voogd, 1974; Rayner, 1977; Erlich and Rayner, 1983; Mitchell, 1984; Rayner, 1998; Vasishth and Lewis, 2006; Smith and Levy, 2013), possibly because cognitive “buffers” may exist to allow higher-level information processing to catch up with the input (Bouma and De Voogd, 1974; Baddeley et al., 1975; Just and Carpenter, 1980; Ehrlich and Rayner, 1981; Mollica and Piantadosi, 2017).",
      "startOffset" : 389,
      "endOffset" : 520
    }, {
      "referenceID" : 45,
      "context" : "Psycholinguists have been aware for decades that processing effects may lag behind the words that trigger them (Morton, 1964; Bouma and De Voogd, 1974; Rayner, 1977; Erlich and Rayner, 1983; Mitchell, 1984; Rayner, 1998; Vasishth and Lewis, 2006; Smith and Levy, 2013), possibly because cognitive “buffers” may exist to allow higher-level information processing to catch up with the input (Bouma and De Voogd, 1974; Baddeley et al., 1975; Just and Carpenter, 1980; Ehrlich and Rayner, 1981; Mollica and Piantadosi, 2017).",
      "startOffset" : 389,
      "endOffset" : 520
    }, {
      "referenceID" : 19,
      "context" : "Psycholinguists have been aware for decades that processing effects may lag behind the words that trigger them (Morton, 1964; Bouma and De Voogd, 1974; Rayner, 1977; Erlich and Rayner, 1983; Mitchell, 1984; Rayner, 1998; Vasishth and Lewis, 2006; Smith and Levy, 2013), possibly because cognitive “buffers” may exist to allow higher-level information processing to catch up with the input (Bouma and De Voogd, 1974; Baddeley et al., 1975; Just and Carpenter, 1980; Ehrlich and Rayner, 1981; Mollica and Piantadosi, 2017).",
      "startOffset" : 389,
      "endOffset" : 520
    }, {
      "referenceID" : 55,
      "context" : "Psycholinguists have been aware for decades that processing effects may lag behind the words that trigger them (Morton, 1964; Bouma and De Voogd, 1974; Rayner, 1977; Erlich and Rayner, 1983; Mitchell, 1984; Rayner, 1998; Vasishth and Lewis, 2006; Smith and Levy, 2013), possibly because cognitive “buffers” may exist to allow higher-level information processing to catch up with the input (Bouma and De Voogd, 1974; Baddeley et al., 1975; Just and Carpenter, 1980; Ehrlich and Rayner, 1981; Mollica and Piantadosi, 2017).",
      "startOffset" : 389,
      "endOffset" : 520
    }, {
      "referenceID" : 76,
      "context" : "They have also recognized the potential for non-linear, interactive, and/or time-varying relationships between word features and language processing (Smith and Levy, 2013; Baayen et al., 2017, 2018).",
      "startOffset" : 149,
      "endOffset" : 198
    }, {
      "referenceID" : 20,
      "context" : "Discrete-time methods (e.g. lagged/spillover regression, Sims, 1971; Erlich and Rayner, 1983; Mitchell, 1984) ignore potentially meaningful variation in event duration, even if some (e.",
      "startOffset" : 22,
      "endOffset" : 109
    }, {
      "referenceID" : 54,
      "context" : "Discrete-time methods (e.g. lagged/spillover regression, Sims, 1971; Erlich and Rayner, 1983; Mitchell, 1984) ignore potentially meaningful variation in event duration, even if some (e.",
      "startOffset" : 22,
      "endOffset" : 109
    }, {
      "referenceID" : 87,
      "context" : "lagged/spillover regression, Sims, 1971; Erlich and Rayner, 1983; Mitchell, 1984) ignore potentially meaningful variation in event duration, even if some (e.g. generalized additive models, or GAMs, Hastie and Tibshirani, 1986; Wood, 2006) permit non-linear and non-stationary (time-varying) feature interactions (Baayen et al.",
      "startOffset" : 154,
      "endOffset" : 238
    }, {
      "referenceID" : 3,
      "context" : "generalized additive models, or GAMs, Hastie and Tibshirani, 1986; Wood, 2006) permit non-linear and non-stationary (time-varying) feature interactions (Baayen et al., 2017).",
      "startOffset" : 152,
      "endOffset" : 173
    }, {
      "referenceID" : 11,
      "context" : "This claim is under active debate (Brothers and Kuperberg, 2021), underlining the importance of methods that can investi-",
      "startOffset" : 34,
      "endOffset" : 64
    }, {
      "referenceID" : 58,
      "context" : "Similarly, the generally null word frequency estimates reported in Shain and Schuler (2021) may be due in part to the assumption of additive effects: word frequency and surprisal are related, and they may coordinate interactively to determine processing costs (Norris, 2006).",
      "startOffset" : 260,
      "endOffset" : 274
    }, {
      "referenceID" : 13,
      "context" : "modeling dependencies on the predictors of all parameters of the predictive distribution) has previously been termed distributional regression (Bürkner, 2018).",
      "startOffset" : 143,
      "endOffset" : 158
    }, {
      "referenceID" : 77,
      "context" : "In addition to random effects shrinkage governed by λz and any arbitrary additional regularization penalties Lreg (see Supplement S1), models are regularized using dropout (Srivastava et al., 2014) with drop rate dh at the outputs of all feedforward hidden layers.",
      "startOffset" : 172,
      "endOffset" : 197
    }, {
      "referenceID" : 60,
      "context" : "plish this using perturbation analysis (e.g. Ribeiro et al., 2016; Petsiuk et al., 2018), manipulating the input configuration and quantifying the influence of this manipulation on the predicted response.",
      "startOffset" : 39,
      "endOffset" : 88
    }, {
      "referenceID" : 77,
      "context" : "about them can be measured with Monte Carlo techniques as long as training involves a stochastic component, such as dropout (Srivastava et al., 2014) or batch normalization (Ioffe and Szegedy, 2015).",
      "startOffset" : 124,
      "endOffset" : 149
    }, {
      "referenceID" : 28,
      "context" : "dropout (Gal and Ghahramani, 2016), which recasts training neural networks with dropout as variational Bayesian approximation of deep Gaussian process models (Damianou and Lawrence, 2013).",
      "startOffset" : 8,
      "endOffset" : 34
    }, {
      "referenceID" : 15,
      "context" : "dropout (Gal and Ghahramani, 2016), which recasts training neural networks with dropout as variational Bayesian approximation of deep Gaussian process models (Damianou and Lawrence, 2013).",
      "startOffset" : 158,
      "endOffset" : 187
    }, {
      "referenceID" : 24,
      "context" : "Because the distribution of reading times is heavy-tailed (Frank et al., 2013), following Shain and Schuler (2021) models are fitted to",
      "startOffset" : 58,
      "endOffset" : 78
    }, {
      "referenceID" : 74,
      "context" : ", 2017), linear interpolation (Shain and Schuler, 2021), binning (Wehbe et al.",
      "startOffset" : 30,
      "endOffset" : 55
    }, {
      "referenceID" : 84,
      "context" : ", 2017), linear interpolation (Shain and Schuler, 2021), binning (Wehbe et al., 2020), and Lanczos interpolation (Huth et al.",
      "startOffset" : 65,
      "endOffset" : 85
    }, {
      "referenceID" : 43,
      "context" : ", 2020), and Lanczos interpolation (Huth et al., 2016).",
      "startOffset" : 35,
      "endOffset" : 54
    }, {
      "referenceID" : 17,
      "context" : "Statistical model comparisons use paired permutation tests of test set error (Demšar, 2006).",
      "startOffset" : 77,
      "endOffset" : 91
    }, {
      "referenceID" : 26,
      "context" : "These results indicate that the relaxed assumptions afforded by CDRNN are beneficial for describing the fMRI response, which is known to saturate over time (Friston et al., 2000; Wager et al., 2005; Vazquez et al., 2006; Lindquist et al., 2009).",
      "startOffset" : 156,
      "endOffset" : 244
    }, {
      "referenceID" : 83,
      "context" : "These results indicate that the relaxed assumptions afforded by CDRNN are beneficial for describing the fMRI response, which is known to saturate over time (Friston et al., 2000; Wager et al., 2005; Vazquez et al., 2006; Lindquist et al., 2009).",
      "startOffset" : 156,
      "endOffset" : 244
    }, {
      "referenceID" : 82,
      "context" : "These results indicate that the relaxed assumptions afforded by CDRNN are beneficial for describing the fMRI response, which is known to saturate over time (Friston et al., 2000; Wager et al., 2005; Vazquez et al., 2006; Lindquist et al., 2009).",
      "startOffset" : 156,
      "endOffset" : 244
    }, {
      "referenceID" : 50,
      "context" : "These results indicate that the relaxed assumptions afforded by CDRNN are beneficial for describing the fMRI response, which is known to saturate over time (Friston et al., 2000; Wager et al., 2005; Vazquez et al., 2006; Lindquist et al., 2009).",
      "startOffset" : 156,
      "endOffset" : 244
    }, {
      "referenceID" : 72,
      "context" : "CDR shows little uncertainty in the fMRI domain despite its inherent noise (Shain et al., 2020), while CDRNN more plausibly shows more uncertainty in its estimates",
      "startOffset" : 75,
      "endOffset" : 95
    }, {
      "referenceID" : 63,
      "context" : "This was interpreted as an inertia effect (faster recent reading engenders faster current reading), but it might also be an artifact of non-linear decreases in latency over time (due to task habituation, e.g. Baayen et al., 2017; Harrington Stack et al., 2018; Prasad and Linzen, 2019) that CDR cannot model.",
      "startOffset" : 178,
      "endOffset" : 285
    }, {
      "referenceID" : 54,
      "context" : "While the motor task is not generally of interest to psycholinguistic theories, controlling for its effects is crucial when using self-paced reading to study sentence comprehension (Mitchell, 1984).",
      "startOffset" : 181,
      "endOffset" : 197
    }, {
      "referenceID" : 21,
      "context" : "This accords with prior arguments that the cortical language system, whose activity is measured here, does not strongly register low-level perceptual effects (Fedorenko et al., 2010; Braze et al., 2011).",
      "startOffset" : 158,
      "endOffset" : 202
    }, {
      "referenceID" : 9,
      "context" : "This accords with prior arguments that the cortical language system, whose activity is measured here, does not strongly register low-level perceptual effects (Fedorenko et al., 2010; Braze et al., 2011).",
      "startOffset" : 158,
      "endOffset" : 202
    } ],
    "year" : 2021,
    "abstractText" : "The human mind is a dynamical system, yet many analysis techniques used to study it are limited in their ability to capture the complex dynamics that may characterize mental processes. This study proposes the continuoustime deconvolutional regressive neural network (CDRNN), a deep neural extension of continuous-time deconvolutional regression (CDR, Shain and Schuler, 2021) that jointly captures time-varying, non-linear, and delayed influences of predictors (e.g. word surprisal) on the response (e.g. reading time). Despite this flexibility, CDRNN is interpretable and able to illuminate patterns in human cognition that are otherwise difficult to study. Behavioral and fMRI experiments reveal detailed and plausible estimates of human language processing dynamics that generalize better than CDR and other baselines, supporting a potential role for CDRNN in studying human language processing.",
    "creator" : "pdftk-java 3.0.9"
  }
}