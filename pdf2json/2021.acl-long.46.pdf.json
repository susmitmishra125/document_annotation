{
  "name" : "2021.acl-long.46.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "Structural Knowledge Distillation: Tractably Distilling Information for Structured Predictor",
    "authors" : [ "Xinyu Wang", "Yong Jiang", "Zhaohui Yan", "Zixia Jia", "Nguyen Bach", "Tao Wang", "Zhongqiang Huang", "Fei Huang", "Kewei Tu" ],
    "emails" : [ "wangxy1@shanghaitech.edu.cn", "jiazx@shanghaitech.edu.cn", "yanzhh@shanghaitech.edu.cn", "tukw@shanghaitech.edu.cn", "yongjiang.jy@alibaba-inc.com", "nguyen.bach@alibaba-inc.com", "leeo.wangt@alibaba-inc.com", "z.huang@alibaba-inc.com", "f.huang@alibaba-inc.com" ],
    "sections" : [ {
      "heading" : null,
      "text" : "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 550–564\nAugust 1–6, 2021. ©2021 Association for Computational Linguistics\n550"
    }, {
      "heading" : "1 Introduction",
      "text" : "Deeper and larger neural networks have led to significant improvement in accuracy in various tasks, but they are also more computationally expensive and unfit for resource-constrained scenarios such\n∗Yong Jiang and Kewei Tu are the corresponding authors. ♠: Equal contributions. ‡: This work was conducted when Xinyu Wang was interning at Alibaba DAMO Academy.\n1Our code is publicly available at https://github. com/Alibaba-NLP/StructuralKD.\nas online serving. An interesting and viable solution to this problem is knowledge distillation (KD) (Buciluǎ et al., 2006; Ba and Caruana, 2014; Hinton et al., 2015), which can be used to transfer the knowledge of a large model (the teacher) to a smaller model (the student). In the field of natural language processing (NLP), for example, KD has been successfully applied to compress massive pretrained language models such as BERT (Devlin et al., 2019) and XLM-R (Conneau et al., 2020) into much smaller and faster models without significant loss in accuracy (Tang et al., 2019; Sanh et al., 2019; Tsai et al., 2019; Mukherjee and Hassan Awadallah, 2020).\nA typical approach to KD is letting the student mimic the teacher model’s output probability distributions on the training data by using the cross-entropy objective. For structured prediction problems, however, the output space is exponentially large, making the cross-entropy objective intractable to compute and optimize directly. Take sequence labeling for example. If the size of the label set is L, then there are Ln possible label sequences for a sentence of n words and it is infeasible to compute the cross-entropy by enumerating the label sequences. Previous approaches to structural KD either choose to perform KD on local decisions or substructures instead of on the full output structure, or resort to Top-K approximation of the objective (Kim and Rush, 2016; Kuncoro et al., 2016; Wang et al., 2020a).\nIn this paper, we derive a factorized form of the structural KD objective based on the fact that almost all the structured prediction models factorize the scoring function of the output structure into scores of substructures. If the student’s substructure space is polynomial in size and the teacher’s\nmarginal distributions over these substructures can be tractably estimated, then we can tractably compute and optimize the factorized form of the structural KD objective. As will be shown in the paper, many widely used structured prediction models satisfy the assumptions and hence are amenable to tractable KD. In particular, we show the feasibility and empirical effectiveness of structural KD with different combinations of teacher and student models, including those with incompatible factorization forms. We apply this technique to structural KD between sequence labeling and dependency parsing models under four different scenarios.\n1. The teacher and student share the same factorization form of the output structure scoring function.\n2. The student factorization produces more finegrained substructures than the teacher factorization.\n3. The teacher factorization produces more finegrained substructures than the student factorization.\n4. The factorization forms from the teacher and the student are incompatible.\nIn all the cases, we empirically show that our structural KD approaches can improve the student models. In the few cases where previous KD approaches are applicable, we show our approaches outperform these previous approaches. With unlabeled data, our approaches can further improve student models’ performance. In a zero-shot crosslingual transfer case, we show that with sufficient unlabeled data, student models trained by our approaches can even outperform the teacher models."
    }, {
      "heading" : "2 Background",
      "text" : ""
    }, {
      "heading" : "2.1 Structured Prediction",
      "text" : "Structured prediction aims to predict a structured output such as a sequence, a tree or a graph. In this paper, we focus on structured prediction problems with a discrete output space, which include most of the structured prediction tasks in NLP (e.g., chunking, named entity recognition, and dependency parsing) and many structured prediction tasks in computer vision (e.g., image segmentation). We further assume that the scoring function of the output structure can be factorized into scores of a polynomial number of substructures. Consequently,\nwe can calculate the conditional probability of the output structure y given an input x as follows:\nP (y|x) = exp (Score(y,x))∑ y′∈Y(x) exp (Score(y′,x))\n=\n∏ u∈y exp (Score(u,x))\nZ(x) (1)\nwhere Y(x) represents all possible output structures given the input x, Score(y,x) is the scoring function that evaluates the quality of the output y, Z(x) is the partition function, and u ∈ y denotes that u is a substructure of y. We define the substructure space U(x) = ⋃ y∈Y(x){u|u ∈ y}as the set of substructures of all possible output structures given input x.\nTake sequence labeling for example. Given a sentence x , the output space Y(x) contains all possible label sequences of x. In linear-chain CRF, a popular model for sequence labeling, the scoring function Score(y,x) is computed by summing up all the transition scores and emission scores where i ranges over all the positions in sentence x, and the substructure space U(x) contains all possible position-specific labels {yi} and label pairs {(yi−1, yi)}."
    }, {
      "heading" : "2.2 Knowledge Distillation",
      "text" : "Knowledge distillation is a technique that trains a small student model by encouraging it to imitate the output probability distribution of a large teacher model. The typical KD objective function is the cross-entropy between the output distributions predicted by the teacher model and the student model:\nLKD = − ∑\ny∈Y(x)\nPt(y|x) logPs(y|x) (2)\nwhere Pt and Ps are the teacher’s and the student’s distributions respectively.\nDuring training, the student jointly learns from the gold targets and the distributions predicted by the teacher by optimizing the following objective function:\nLstudent = λLKD + (1− λ)Ltarget\nwhere λ is an interpolation coefficient between the target loss Ltarget and the structural KD loss LKD. Following Clark et al. (2019); Wang et al. (2020a), one may apply teacher annealing in training by decreasing λ linearly from 1 to 0. Because KD does not require gold labels, unlabeled data can also be used in the KD loss."
    }, {
      "heading" : "3 Structural Knowledge Distillation",
      "text" : "When performing knowledge distillation on structured prediction, a major challenge is that the structured output space is exponential in size, leading to intractable computation of the KD objective in Eq. 2. However, if the scoring function of the student model can be factorized into scores of substructures (Eq. 1), then we can derive the following factorized form of the structural KD objective.\nLKD = − ∑\ny∈Y(x)\nPt(y|x)logPs(y|x)\n=− ∑\ny∈Y(x)\nPt(y|x) ∑ u∈y Scores(u,x)+logZs(x)\n=− ∑\ny∈Y(x)\nPt(y|x) ∑\nu∈Us(x)\n1u∈yScores(u,x)+logZs(x)\n=− ∑∑\nu∈Us(x),y∈Y(x)\nPt(y|x)1u∈yScores(u,x)+logZs(x)\n=− ∑\nu∈Us(x)\nPt(u|x)Scores(u,x)+logZs(x) (3)\nwhere 1condition is 1 if the condition is true and 0 otherwise. From Eq. 3, we see that if Us(x) is polynomial in size and Pt(u|x) can be tractably estimated, then the structural KD objective can be tractably computed and optimized. In the rest of this section, we will show that this is indeed the case for some of the most widely used models in sequence labeling and dependency parsing, two representative structured prediction tasks in NLP. Based on the difference in score factorization between the teacher and student models, we divide our discussion into four scenarios."
    }, {
      "heading" : "3.1 Teacher and Student Share the Same Factorization Form",
      "text" : "Case 1a: Linear-Chain CRF ⇒ Linear-Chain CRF In this case, both the teacher and the student are linear-chain CRF models. An example application is to compress a state-of-the-art CRF model for named entity recognition (NER) that is based on large pretrained contextualized embeddings to a smaller CRF model with static embeddings that is more suitable for fast online serving.\nFor a CRF student model described in section 2.1, if we absorb the emission score Se(yi,x) into the transition score St((yi−1, yi),x) at each position i, then the substructure space Us(x) contains every two adjacent labels {(yi−1, yi)} for i=1, . . . , n, with n be-\ning the sequence length, and the substructure score is defined as Score((yi−1, yi),x) = St((yi−1, yi),x) + Se(yi,x). The substructure marginal Pt((yi−1, yi)|x) of the teacher model can be computed by:\nPt((yi−1, yi)|x) ∝ α(yi−1)× β(yi) × exp(Score((yi−1, yi),x))\n(4)\nwhere α(yi−1) and β(yi) are forward and backward scores that can be tractably calculated using the classical forward-backward algorithm.\nComparing with the Posterior KD and Top-K KD of linear-chain CRFs proposed by Wang et al. (2020a), our approach calculates and optimizes the KD objective exactly, while their two KD approaches perform KD either heuristically or approximately. At the formulation level, our approach is based on the marginal distributions of two adjacent labels, while the Posterior KD is based on the marginal distributions of a single label.\nCase 1b: Graph-based Dependency Parsing⇒ Dependency Parsing as Sequence Labeling In this case, we use the biaffine parser proposed by Dozat et al. (2017) as the teacher and the sequence labeling approach proposed by Strzyz et al. (2019) as the student for the dependency parsing task. The biaffine parser is one of the state-of-the-art models, while the sequence labeling parser provides a good speed-accuracy tradeoff. There is a big gap in accuracy between the two models and therefore KD can be used to improve the accuracy of the sequence labeling parser.\nHere we follow the head-selection formulation of dependency parsing without the tree constraint. The dependency parse tree y is represented by 〈y1, . . . , yn〉, where n is the sentence length and yi = (hi, li) denotes the dependency head of the i-th token of the input sentence, with hi being the index of the head token and li being the dependency label. The biaffine parser predicts the dependency head for each token independently. It models separately the probability distribution of the head index Pt(hi|x) and the probability distribution of the label Pt(li|x). The sequence labeling parser is a MaxEnt model that also predicts the head of each token independently. It computes Score((hi, li),x) for each token and applies a softmax function to produce the distribution Ps((hi, li)|x).\nTherefore, these two models share the same factorization in which each substructure is a depen-\ndency arc specified by yi. Us(x) thus contains all possible dependency arcs among tokens of the input sentence x. The substructure marginal predicted by the teacher can be easily derived as:\nPt((hi, li)|x) = Pt(hi|x)× Pt(li|x) (5)\nNote that in this case, the sequence labeling parser uses a MaxEnt decoder, which is locally normalized for each substructure. Therefore, the structural KD objective in Eq. 3 can be reduced to the following form without the need for calculating the student partition function Zs(x).\nLKD = − ∑\nu∈Us(x)\nPt(u|x)× logPs(u|x) (6)\nIn all the cases except Case 1a and Case 3, the student model is locally normalized and hence we can follow this form of objective."
    }, {
      "heading" : "3.2 Student Factorization Produces More Fine-grained Substructures than Teacher Factorization",
      "text" : "Case 2a: Linear-Chain CRF ⇒ MaxEnt In this case, we use a linear-chain CRF model as the teacher and a MaxEnt model as the student. Previous work (Yang et al., 2018; Wang et al., 2020a) shows that a linear-chain CRF decoder often leads to better performance than a MaxEnt decoder for many sequence labeling tasks. Still, the simplicity and efficiency of the MaxEnt model is desirable. Therefore, it makes sense to perform KD from a linear-chain CRF to a MaxEnt model.\nAs mentioned in Case 1a, the substructures of a linear-chain CRF model are consecutive labels {(yi−1, yi)}. In contrast, a MaxEnt model predicts the label probability distribution Ps(yi|x) of each token independently and hence the substructure space Us(x) consists of every individual label {yi}. To calculate the substructure marginal of the teacher Pt(yi|x), we can again utilize the forwardbackward algorithm:\nPt(yi|x) ∝ α(yi)× β(yi) (7)\nwhere α(yi) and β(yi) are forward and backward scores.\nCase 2b: Second-Order Dependency Parsing ⇒ Dependency Parsing as Sequence Labeling The biaffine parser is a first-order dependency parser, which scores each dependency arc in a\nparse tree independently. A second-order dependency parser scores pairs of dependency arcs with a shared token. The substructures of second-order parsing are therefore all the dependency arc pairs with a shared token. It has been found that secondorder extensions of the biaffine parser often have higher parsing accuracy (Wang et al., 2019; Zhang et al., 2020; Wang et al., 2020d; Wang and Tu, 2020). Therefore, we may take a second-order dependency parser as the teacher to improve a sequence labeling parser.\nHere we consider the second-order dependency parser of Wang and Tu (2020). It employs mean field variational inference to estimate the probabilities of arc existence Pt(hi|x) and uses a first-order biaffine model to estimate the probabilities of arc labels Pt(li|x). Therefore, the substructure marginal can be calculated in the same way as Eq. 5."
    }, {
      "heading" : "3.3 Teacher Factorization Produces More Fine-grained Substructures than Student Factorization",
      "text" : "Case 3: MaxEnt ⇒ Linear-Chain CRF Here we consider KD in the opposite direction of Case 2a. An example application is zero-shot crosslingual NER. Previous work (Pires et al., 2019; Wu and Dredze, 2019) has shown that multilingual BERT (M-BERT) has strong zero-shot crosslingual transferability in NER tasks. Many such models employ a MaxEnt decoder. In scenarios requiring fast speed and low computation cost, however, we may want to distill knowledge from such models to a model with much cheaper static monolingual embeddings while compensating the performance loss with a linear-chain CRF decoder.\nAs described in Case 1a, the substructures of a linear-chain CRF model are consecutive labels {(yi−1, yi)}. Because of the label independence and local normalization in the MaxEnt model, the substructure marginal of the MaxEnt teacher is calculated by:\nPt((yi−1, yi)|x) = Pt(yi−1|x)Pt(yi|x) (8)"
    }, {
      "heading" : "3.4 Factorization Forms From Teacher and Student are Incompatible",
      "text" : "Case 4: NER as Parsing ⇒ MaxEnt Very recently, Yu et al. (2020) propose to solve the NER task as graph-based dependency parsing and achieve state-of-the-art performance. They represent each named entity with a dependency arc from\nthe first token to the last token of the named entity, and represent the entity type with the arc label. However, for the flat NER task (i.e., there is no overlapping between entity spans), the time complexity of this method is higher than commonly used sequence labeling NER methods. In this case, we take a parsing-based NER model as our teacher and a MaxEnt model with the BIOES label scheme as our student.\nThe two models adopt very different representations of NER output structures. The parsing-based teacher model represents an NER output of a sentence with a set of labeled dependency arcs and defines its score as the sum of arc scores. The MaxEnt model represents an NER output of a sentence with a sequence of BIOES labels and defines its score as the sum of token-wise label scores. Therefore, the factorization forms of these two models are incompatible.\nComputing the substructure marginal of the teacher Pt(yi|x), where yi ∈ {Bl, Il, El, Sl, O|l ∈ L} and L is the set of entity types, is much more complicated than in the previous cases. Take yi = Bl for example. Pt(yi = Bl|x) represents the probability of the i-th word being the beginning of a multi-word entity of type ‘l’. In the parsing-based teacher model, this probability is proportional to the summation of exponentiated scores of all the output structures that contain a dependency arc of label ‘l’ with the i-th word as its head and with its length larger than 1. It is intractable to compute such marginal probabilities by enumerating all the output structures, but we can tractably compute them using dynamic programming. See supplementary material for a detailed description of our dynamic programming method."
    }, {
      "heading" : "4 Experiments",
      "text" : "We evaluate our approaches described in Section 3 on NER (Case 1a, 2a, 3, 4) and dependency parsing (Case 1b, 2b)."
    }, {
      "heading" : "4.1 Settings",
      "text" : "Datasets We use CoNLL 2002/2003 datasets (Tjong Kim Sang, 2002; Tjong Kim Sang and De Meulder, 2003) for Case 1a, 2a and 4, and use WikiAnn datasets (Pan et al., 2017) for Case 1a, 2a, 3, and 4. The CoNLL datasets contain the corpora of four Indo-European languages. We use the same four languages from the WikiAnn datasets. For cross-lingual transfer in Case 3, we\nuse the four Indo-European languages as the source for the teacher model and additionally select four languages from different language families as the target for the student models.2\nWe use the standard training/development/test split for the CoNLL datasets. For WikiAnn, we follow the sampling of Wang et al. (2020a) with 12000 sentences for English and 5000 sentences for each of the other languages. We split the datasets by 3:1:1 for training/development/test. For Case 1b and 2b, we use Penn Treebank (PTB) 3.0 and follow the same pre-processing pipeline as in Ma et al. (2018). For unlabeled data, we sample sentences that belong to the same languages of the labeled data from the WikiAnn datasets for Case 1a, 2a and 4 and we sample sentences from the target languages of WikiAnn datasets for Case 3. We use the BLLIP corpus3 as the unlabeled data for Case 1b and 2b.\nModels For the student models in all the cases, we use fastText (Bojanowski et al., 2017) word embeddings and character embeddings as the word representation. For Case 1a, 2a and 4, we concatenate the multilingual BERT, Flair (Akbik et al., 2018), fastText embeddings and character embeddings (Santos and Zadrozny, 2014) as the word representations for stronger monolingual teacher models (Wang et al., 2020c). For Case 3, we use MBERT embeddings for the teacher. Also for Case 3, we fine-tune the teacher model on the training set of the four Indo-European languages from the WikiAnn dataset and train student models on the four additional languages. For the teacher models in Case 1b and 2b, we simply use the same embeddings as the student because there is already huge performance gap between the teacher and student in these settings and hence we do not need strong embeddings for the teacher to demonstrate the utility of KD.\nBaselines We compare our Structural KD (Struct. KD) with training without KD (w/o KD) as well as existing KD approaches. In Case 1a, the Pos. KD baseline is the Posterior KD approach for linearchain CRFs proposed by Wang et al. (2020a). They\n2The four languages from the CoNLL datasets are Dutch, English, German and Spanish and the four target languages for Case 3 are Basque, Hebrew, Persian and Tamil. We use ISO 639-1 language codes (https://en.wikipedia.org/ wiki/List_of_ISO_639-1_codes) to represent each language.\n3Brown Laboratory for Linguistic Information Processing (BLLIP) 1987-89 WSJ Corpus Release 1.\nalso propose Top-K KD but have shown that it is inferior to Pos. KD. For experiments using unlabeled data in all the cases, in addition to labeled data, we use the teacher’s prediction on the unlabeled data as pseudo labeled data to train the student models. This can be seen as the Top-1 KD method4. In Case 2a and 3, where we perform KD between CRF and MaxEnt models, we run a reference baseline that replaces the CRF teacher or student model with a MaxEnt model and performs token-level KD (Token KD) of MaxEnt models that optimizes the cross entropy between the teacher and student label distributions at each position.\nTraining For MaxEnt and linear-chain CRF models, we use the same hyper-parameters as in Akbik et al. (2018). For dependency parsing, we use the same hyper-parameters as in Wang and Tu (2020) for teacher models and Strzyz et al. (2019) for student models. For M-BERT fine-tuning in Case 3, we mix the training data of the four source datasets and train the teacher model with the AdamW optimizer (Loshchilov and Hutter, 2018) with a learning rate of 5×10−5 for 10 epochs. We tune the KD temperature in {1, 2, 3, 4, 5} and the loss interpolation annealing rate in {0.5, 1.0, 1.5}. For all experiments, we train the models for 5 runs with a fixed random seed for each run.\n4We do not predict pseudo labels for the labeled data, because we find that the teacher models’ predictions on the labeled training data have approximately 100% accuracy in most of the cases."
    }, {
      "heading" : "4.2 Results",
      "text" : "Table 1 shows the experimental results with labeled data only and 2 shows the experimental results with additional 3000 unlabeled sentences. The results show that our structural KD approaches outperform the baselines in all the cases. Table 3 compares Struct. KD with Token KD, the reference baseline based on MaxEnt models. For Case 2a, which involves a MaxEnt student, Struct. KD with a CRF teacher achieves better results than Token KD with a MaxEnt teacher. For Case 3, which involves a MaxEnt teacher, Struct. KD with a CRF student achieves better results than Token KD with a MaxEnt student. These results are to be expected because Struct. KD makes it possible to apply exact knowledge distillation with a more capable teacher or student. In all the experiments, we run Almost Stochastic Dominance proposed by Dror et al. (2019) with a significance level of 0.05 and find that the advantages of our structural KD approaches are significant. Please refer to Appendix for more detailed results."
    }, {
      "heading" : "4.3 Multilingual NER Experiments",
      "text" : "There is a recent increase of interest in training multilingual NER models (Tsai et al., 2019; Mukherjee and Hassan Awadallah, 2020) because of the strong\ngeneralizability of M-BERT on multiple languages. Existing work explored knowledge distillation approaches to train fast and effective multilingual NER models with the help of monolingual teachers (Wang et al., 2020a). To show the effectiveness of structural KD in the multilingual NER setting, we compare our approaches with those reported by Wang et al. (2020a). Specifically, the monolingual teachers are always CRF models, and the multilingual student is either a CRF model (Case 1a) or a MaxEnt model (Case 2a). Wang et al. (2020a) report results of the Top-WK KD (a weighted version of Top-K KD) and Pos. KD approaches for Case 1a and the reference baseline Token KD (with a MaxEnt teacher) for Case 2a. We follow their experimental settings when running our approach.\nThe experimental results in Table 4 show the effectiveness of Struct. KD in both cases. In Case 1a, our approach is stronger than both Top-WK KD and Pos. KD as well as the mixture of the two approaches on average. In Case 2a, Struct. KD not only outperforms Token KD, but also makes the MaxEnt student competitive with the CRF student without KD (87.32 vs. 87.36)."
    }, {
      "heading" : "5 Analysis",
      "text" : ""
    }, {
      "heading" : "5.1 Amount of Unlabeled Data",
      "text" : "We compare our approaches with the baselines with different amounts of unlabeled data for Case 1a, 1b and 3, which are cases that apply in-domain unlabeled data for NER and dependency parsing, and cross-lingual unlabeled data for NER. We experiment with more unlabeled data for Case 1b than for the other two cases because the labeled training data of PTB is more than 10 times larger than the labeled NER training data in Case 1a and 3. Results are shown in Figure 1. The experimental results show that our approaches consistently outperform the baselines, though the performance gaps between them become smaller when the amount of unlabeled data increases. Comparing the performance of the students with the teachers, we can see that in Case 1a and 1b, the gap between the teacher and the student remains large even with the largest amount of unlabeled data. This is unsurprising considering the difference in model capacity between the teacher and the student. In Case 3, however, we find that when using 30,000 unlabeled sentences, the CRF student models can even outperform the MaxEnt teacher model, which shows the effectiveness of CRF models on NER."
    }, {
      "heading" : "5.2 Temperature in Structural Knowledge Distillation",
      "text" : "A frequently used KD technique is dividing the logits of probability distributions of both the teacher and the student by a temperature in the KD objective (Hinton et al., 2015). Using a higher temperature produces softer probability distributions and often results in higher KD accuracy. In structural KD, there are two approaches to applying the temperature to the teacher model, either globally to the logit of Pt(y|x) (i.e., Scoret(y,x)) of the full structure y, or locally to the logit of Pt(u|x) of each student substructure u. We empirically compare these two approaches in Case 1a with the same setting as in Section 4.1. Table 5 shows that the local approach results in better accuracy for all the languages. Therefore, we use the local approach by default in all the experiments."
    }, {
      "heading" : "5.3 Comparison of Teachers",
      "text" : "In Case 2a and Case 4, we use the same MaxEnt student model but different types of teacher models. Our structural KD approaches in both cases compute the marginal distribution Pt(yi|x) of the teacher at each position i following the substructures of the MaxEnt student, which is then used to train the student substructure scores. We can evaluate the quality of the marginal distributions by taking their modes as label predictions and evaluating their accuracy. In Table 6, we compare the accuracy of the CRF teacher and its marginal distributions from Case 2a, the NER-as-parsing teacher and its marginal distributions from Case 4, and the MaxEnt teacher which is the KD baseline in Case 2a. First, we observe that for both\nCRF and NER-as-parsing, predicting labels from the marginal distributions leads to lower accuracy. This is to be expected because such predictions do not take into account correlations between adjacent labels. While predictions from marginal distributions of the CRF teacher still outperform MaxEnt, those of the NER-as-parsing teacher clearly underperform MaxEnt. This provides an explanation as to why Struct. KD in Case 4 has equal or even lower accuracy than the Token KD baseline in Case 2a in Table 3."
    }, {
      "heading" : "6 Related Work",
      "text" : ""
    }, {
      "heading" : "6.1 Structured Prediction",
      "text" : "In this paper, we use sequence labeling and dependency parsing as two example structured prediction tasks. In sequence labeling, a lot of work applied the linear-chain CRF and achieved state-of-the-art performance in various tasks (Ma and Hovy, 2016; Akbik et al., 2018; Liu et al., 2019b; Yu et al., 2020; Wei et al., 2020; Wang et al., 2021a,b). Meanwhile, a lot of other work used the MaxEnt layer instead of the CRF for sequence labeling (Devlin et al., 2019; Conneau et al., 2020; Wang et al., 2020b) because MaxEnt makes it easier to fine-tune pretrained contextual embeddings in training. Another advantage of MaxEnt in comparison with CRF is its speed. Yang et al. (2018) showed that models equipped with the CRF are about two times slower than models with the MaxEnt layer in sequence labeling. In dependency parsing, recent work shows that second-order CRF parsers achieve significantly higher accuracy than first-order parsers (Wang et al., 2019; Zhang et al., 2020). However, the inference speed of second-order parsers is much slower. Zhang et al. (2020) showed that secondorder parsing is four times slower than the simple head-selection first-order approach (Dozat and\nManning, 2017). Such speed-accuracy tradeoff as seen in sequence labeling and dependency parsing also occurs in many other structured prediction tasks. This makes KD an interesting and very useful technique that can be used to circumvent this tradeoff to some extent."
    }, {
      "heading" : "6.2 Knowledge Distillation in Structured Prediction",
      "text" : "KD has been applied in many structured prediction tasks in the fields of NLP, speech recognition and computer vision, with applications such as neural machine translation (Kim and Rush, 2016; Tan et al., 2019), sequence labeling (Tu and Gimpel, 2019; Wang et al., 2020a), connectionist temporal classification (Huang et al., 2018), image semantic segmentation (Liu et al., 2019a) and so on. In KD for structured prediction tasks, how to handle the exponential number of structured outputs is a main challenge. To address this difficult problem, recent work resorts to approximation of the KD objective. Kim and Rush (2016) proposed sequence-level distillation through predicting K-best sequences of the teacher in neural machine translation. Kuncoro et al. (2016) proposed to use multiple greedy parsers as teachers and generate the probability distribution at each position through voting. Very recently, Wang et al. (2020a) proposed structure-level knowledge distillation for linear-chain CRF models in multilingual sequence labeling. During the distillation process, teacher models predict the Top-K label sequences as the global structure information or the posterior label distribution at each position as the local structural information, which is then used to train the student. Besides approximate approaches, an alternative way is using models that make local decisions and performing KD on these local decisions. Anderson and Gómez-Rodrı́guez (2020) formulated dependency parsing as a head-\nselection problem and distilled the distribution of the head node at each position. Tsai et al. (2019) proposed MiniBERT through distilling the output distributions of M-BERT models of the MaxEnt classifier. Besides the output distribution, Mukherjee and Hassan Awadallah (2020) further distilled the hidden representations of teachers."
    }, {
      "heading" : "7 Conclusion",
      "text" : "In this paper, we propose structural knowledge distillation, which transfers knowledge between structured prediction models. We derive a factorized form of the structural KD objective and make it tractable to compute and optimize for many typical choices of teacher and student models. We apply our approach to four KD scenarios with six cases for sequence labeling and dependency parsing. Empirical results show that our approach outperforms baselines without KD as well as previous KD approaches. With sufficient unlabeled data, our approach can even boost the students to outperform the teachers in zero-shot cross-lingual transfer."
    }, {
      "heading" : "Acknowledgments",
      "text" : "This work was supported by the National Natural Science Foundation of China (61976139) and by Alibaba Group through Alibaba Innovative Research Program."
    }, {
      "heading" : "A Dynamic Programming for Case 4",
      "text" : "We describe how the marginal distribution over BIOES labels at each position of the input sentence can be tractably computed based on the NERas-parsing teacher model using dynamic programming.\nGiven an input sentence x with n words, we first define the following functions.\n• −→ DP(i, l) represents the summation of scores of all possible labeling sequences of the subsentence from the first token to the i-th token while a span ends with the i-th token with a label l.\n• −→ DP(i,F) represents the summation of scores of all possible labeling sequences of the subsentence from the first token to the i-th token while there is no arc pointing to the i-th token.\n• ←− DP(i, l) represents the summation of scores of all possible labeling sequences of the subsentence from the i-th toke to the last token while a span starts with the i-th token with a label l.\n• ←− DP(i,F) represents the summation of scores of all possible labeling sequences of the subsentence from the i-th toke to the last token while there is no arc coming from the i-th token.\nWe can compute the values of these functions for all values of i and l using dynamic programming. The base cases are:\n−→ DP(1,F) = 1\n←− DP(n,F) = 1\nThe recursive formulation of these functions are:\n−→ DP(i, l) = i∑ k=1 exp(Score(yk,i = l)) ∗ −→ DP(k,F) −→ DP(i,F) = −→ DP(i− 1,F) +\n∑ l∈L −→ DP(i− 1, l)\n←− DP(i, l) = n∑ j=i exp(Score(yi,j = l)) ∗ ←− DP(j,F) ←− DP(i,F) = ←− DP(i+ 1,F) +\n∑ l∈L ←− DP(i+ 1, l)\nwhere Score(yi,j = l) is the score assigned by the teacher model to the dependency arc from i to j\nwith label l. After dynamic programming, we can compute the substructure marginals of the teacher Pt(yi|x) as follows:\nPt(yi = Bl|x) = DP(Bl, i)/Z(x)\n= −→ DP(i,F) ∗ n∑ j=i+1 exp(Score(yi,j = l))\n∗←−DP(j,F)/Z(x)\nPt(yi = Il|x) = DP(Il, i)/Z(x)\n= i−1∑ k=1 n∑ j=i+1 exp(Score(yk,j = l)) ∗ −→ DP(k,F)\n∗←−DP(j,F)/Z(x)\nPt(yi = El|x) = DP(El, i)/Z(x)\n= ←− DP(i,F) ∗ i−1∑ k=1 exp(Score(yk,i = l))\n∗ −→DP(k, F)/Z(x)\nPt(yi = O|x) = DP(O, i)/Z(x)\n= −→ DP(i,F) ∗←−DP(i,F)/Z(x)\nPt(yi = Sl|x) = DP(Sl, i)/Z(x)\n= −→ DP(i,F)∗ exp(Score(yi,i = l))∗ ←− DP(i,F)/Z(x)\nwhere\n• DP(X, i) represents the summation of scores of all possible labeling sequences in which the i-th token is labeled as X . X can be one of ‘Bl, Il, El, O, Sl’.\n• Z(x) represents the summation of scores of all possible labeling sequences given the input sentence x. yi,j = l represents that there is a dependency arc of label ‘l’ from the i-th word to the j-th word. We can calculate Z(x) by −→ DP(n, l) + −→ DP(n, F ) or ←− DP(1, l) + ←− DP(1, F )\nThe edge cases are:\nPt(yn = Bl|x) = 0 Pt(y1 = Il|x) = Pt(yn = Il|x) = 0 Pt(y1 = El|x) = 0"
    }, {
      "heading" : "B Additional Analysis",
      "text" : "B.1 Comparison of Speed and Model Size\nAn important goal of KD is to produce faster and smaller models. In Table 7, we show a comparison on the running speed and model size between the teacher and student models on the CoNLL English test set from Case 2a. It can be seen that the student model is about 24 times faster and 25 times smaller than the teacher model."
    }, {
      "heading" : "C Detailed Experimental Results",
      "text" : "In this section, we present detailed experimental results. Table 8, 9 and 10 show the results of NER task, while table 11 and 12 show the results of Parsing. We evaluate the significance based on Almost Stochastic Dominance (ASD) (Dror et al., 2019), which is a high quality comparison between deep neural networks. We evaluate with a significance level of 0.05. For the significance test over averaged scores, we averaged over the same random seed of each language as a sample of averaged score. In tables, we use † to represent our approaches are significantly stronger than the models training without KD or with Top-1 KD. We use ‡ to represent that our approaches are significantly stronger than other KD approaches.\nC.1 Results of NER task\nTable 8, 9 and 10 represent the KD results of experiments with labeled and unlabeled datasets. Our approaches outperform the baselines significantly in most of the cases. Note that in some cases, our approaches perform slightly inferior to other approaches (for example, de dataset in Case 1a in Table 9 with 30k unlabeled sentences) while our approaches are still stronger than these approaches according to the ASD test. The possible reason is that the variances of our approaches are much larger than the other approaches and ASD indicates our approaches is possibly better than the other approaches.\nC.2 Results of Parsing task Tabel 11 and 12 represent the results of experiments of Parsing. Our structural KD approaches significantly outperform the other approaches in all cases. UAS and LAS in these tables were dependency parsing metrics, and they refer to unlabeled attachment score and labeled attachment score respectively."
    } ],
    "references" : [ {
      "title" : "Contextual string embeddings for sequence labeling",
      "author" : [ "Alan Akbik", "Duncan Blythe", "Roland Vollgraf." ],
      "venue" : "Proceedings of the 27th International Conference on Computational Linguistics, pages 1638–1649, Santa Fe, New Mexico, USA. Associ-",
      "citeRegEx" : "Akbik et al\\.,? 2018",
      "shortCiteRegEx" : "Akbik et al\\.",
      "year" : 2018
    }, {
      "title" : "Distilling neural networks for greener and faster dependency parsing",
      "author" : [ "Mark Anderson", "Carlos Gómez-Rodrı́guez" ],
      "venue" : "In Proceedings of the 16th International Conference on Parsing Technologies and the IWPT 2020 Shared Task on Parsing into En-",
      "citeRegEx" : "Anderson and Gómez.Rodrı́guez.,? \\Q2020\\E",
      "shortCiteRegEx" : "Anderson and Gómez.Rodrı́guez.",
      "year" : 2020
    }, {
      "title" : "Do deep nets really need to be deep? In Z",
      "author" : [ "Jimmy Ba", "Rich Caruana." ],
      "venue" : "Ghahramani, M. Welling, C. Cortes, N. D. Lawrence, and K. Q. Weinberger, editors, Advances in Neural Information Processing Systems 27, pages 2654–2662. Curran Associates,",
      "citeRegEx" : "Ba and Caruana.,? 2014",
      "shortCiteRegEx" : "Ba and Caruana.",
      "year" : 2014
    }, {
      "title" : "Enriching word vectors with subword information",
      "author" : [ "Piotr Bojanowski", "Edouard Grave", "Armand Joulin", "Tomas Mikolov." ],
      "venue" : "Transactions of the Association for Computational Linguistics, 5:135–146.",
      "citeRegEx" : "Bojanowski et al\\.,? 2017",
      "shortCiteRegEx" : "Bojanowski et al\\.",
      "year" : 2017
    }, {
      "title" : "Model compression",
      "author" : [ "Cristian Buciluǎ", "Rich Caruana", "Alexandru Niculescu-Mizil." ],
      "venue" : "Proceedings of the 12th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD ’06, pages 535–541, New York, NY, USA.",
      "citeRegEx" : "Buciluǎ et al\\.,? 2006",
      "shortCiteRegEx" : "Buciluǎ et al\\.",
      "year" : 2006
    }, {
      "title" : "BAM! born-again multi-task networks for natural language understanding",
      "author" : [ "Kevin Clark", "Minh-Thang Luong", "Urvashi Khandelwal", "Christopher D. Manning", "Quoc V. Le." ],
      "venue" : "Proceedings of the 57th Annual Meeting of the Association for Computa-",
      "citeRegEx" : "Clark et al\\.,? 2019",
      "shortCiteRegEx" : "Clark et al\\.",
      "year" : 2019
    }, {
      "title" : "Unsupervised cross-lingual representation learning at scale",
      "author" : [ "Alexis Conneau", "Kartikay Khandelwal", "Naman Goyal", "Vishrav Chaudhary", "Guillaume Wenzek", "Francisco Guzmán", "Edouard Grave", "Myle Ott", "Luke Zettlemoyer", "Veselin Stoyanov." ],
      "venue" : "In",
      "citeRegEx" : "Conneau et al\\.,? 2020",
      "shortCiteRegEx" : "Conneau et al\\.",
      "year" : 2020
    }, {
      "title" : "BERT: Pre-training of deep bidirectional transformers for language understanding",
      "author" : [ "Jacob Devlin", "Ming-Wei Chang", "Kenton Lee", "Kristina Toutanova." ],
      "venue" : "Proceedings of the 2019 Conference of the North American Chapter of the Association",
      "citeRegEx" : "Devlin et al\\.,? 2019",
      "shortCiteRegEx" : "Devlin et al\\.",
      "year" : 2019
    }, {
      "title" : "Deep biaffine attention for neural dependency parsing",
      "author" : [ "Timothy Dozat", "Christopher D Manning." ],
      "venue" : "International Conference on Learning Representations.",
      "citeRegEx" : "Dozat and Manning.,? 2017",
      "shortCiteRegEx" : "Dozat and Manning.",
      "year" : 2017
    }, {
      "title" : "Stanford’s graph-based neural dependency parser at the CoNLL 2017 shared task",
      "author" : [ "Timothy Dozat", "Peng Qi", "Christopher D. Manning." ],
      "venue" : "Proceedings of the CoNLL 2017 Shared Task: Multilingual Parsing from Raw Text to Universal Dependencies,",
      "citeRegEx" : "Dozat et al\\.,? 2017",
      "shortCiteRegEx" : "Dozat et al\\.",
      "year" : 2017
    }, {
      "title" : "Deep dominance - how to properly compare deep neural models",
      "author" : [ "Rotem Dror", "Segev Shlomov", "Roi Reichart." ],
      "venue" : "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 2773–2785, Florence, Italy. Associa-",
      "citeRegEx" : "Dror et al\\.,? 2019",
      "shortCiteRegEx" : "Dror et al\\.",
      "year" : 2019
    }, {
      "title" : "Distilling the knowledge in a neural network",
      "author" : [ "Geoffrey Hinton", "Oriol Vinyals", "Jeffrey Dean." ],
      "venue" : "NIPS Deep Learning and Representation Learning Workshop.",
      "citeRegEx" : "Hinton et al\\.,? 2015",
      "shortCiteRegEx" : "Hinton et al\\.",
      "year" : 2015
    }, {
      "title" : "Knowledge distillation for sequence model",
      "author" : [ "Mingkun Huang", "Yongbin You", "Zhehuai Chen", "Yanmin Qian", "Kai Yu." ],
      "venue" : "Proc. Interspeech 2018, pages 3703–3707.",
      "citeRegEx" : "Huang et al\\.,? 2018",
      "shortCiteRegEx" : "Huang et al\\.",
      "year" : 2018
    }, {
      "title" : "Sequencelevel knowledge distillation",
      "author" : [ "Yoon Kim", "Alexander M. Rush." ],
      "venue" : "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 1317–1327, Austin, Texas. Association for Computational Linguistics.",
      "citeRegEx" : "Kim and Rush.,? 2016",
      "shortCiteRegEx" : "Kim and Rush.",
      "year" : 2016
    }, {
      "title" : "Distilling an ensemble of greedy dependency parsers into one MST parser",
      "author" : [ "Adhiguna Kuncoro", "Miguel Ballesteros", "Lingpeng Kong", "Chris Dyer", "Noah A. Smith." ],
      "venue" : "Proceedings of the 2016 Conference on Empirical Methods in Natural Language",
      "citeRegEx" : "Kuncoro et al\\.,? 2016",
      "shortCiteRegEx" : "Kuncoro et al\\.",
      "year" : 2016
    }, {
      "title" : "Structured knowledge distillation for semantic segmentation",
      "author" : [ "Yifan Liu", "Ke Chen", "Chris Liu", "Zengchang Qin", "Zhenbo Luo", "Jingdong Wang." ],
      "venue" : "2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages",
      "citeRegEx" : "Liu et al\\.,? 2019a",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2019
    }, {
      "title" : "GCDT: A global context enhanced deep transition architecture for sequence labeling",
      "author" : [ "Yijin Liu", "Fandong Meng", "Jinchao Zhang", "Jinan Xu", "Yufeng Chen", "Jie Zhou." ],
      "venue" : "Proceedings of the 57th Annual Meeting of the Association for Computational Lin-",
      "citeRegEx" : "Liu et al\\.,? 2019b",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2019
    }, {
      "title" : "Decoupled weight decay regularization",
      "author" : [ "Ilya Loshchilov", "Frank Hutter." ],
      "venue" : "International Conference on Learning Representations.",
      "citeRegEx" : "Loshchilov and Hutter.,? 2018",
      "shortCiteRegEx" : "Loshchilov and Hutter.",
      "year" : 2018
    }, {
      "title" : "End-to-end sequence labeling via bi-directional LSTM-CNNsCRF",
      "author" : [ "Xuezhe Ma", "Eduard Hovy." ],
      "venue" : "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1064–1074, Berlin, Ger-",
      "citeRegEx" : "Ma and Hovy.,? 2016",
      "shortCiteRegEx" : "Ma and Hovy.",
      "year" : 2016
    }, {
      "title" : "Stackpointer networks for dependency parsing",
      "author" : [ "Xuezhe Ma", "Zecong Hu", "Jingzhou Liu", "Nanyun Peng", "Graham Neubig", "Eduard Hovy." ],
      "venue" : "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long",
      "citeRegEx" : "Ma et al\\.,? 2018",
      "shortCiteRegEx" : "Ma et al\\.",
      "year" : 2018
    }, {
      "title" : "XtremeDistil: Multi-stage distillation for massive multilingual models",
      "author" : [ "Subhabrata Mukherjee", "Ahmed Hassan Awadallah." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 2221–2234, Online. As-",
      "citeRegEx" : "Mukherjee and Awadallah.,? 2020",
      "shortCiteRegEx" : "Mukherjee and Awadallah.",
      "year" : 2020
    }, {
      "title" : "Crosslingual name tagging and linking for 282 languages",
      "author" : [ "Xiaoman Pan", "Boliang Zhang", "Jonathan May", "Joel Nothman", "Kevin Knight", "Heng Ji." ],
      "venue" : "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume",
      "citeRegEx" : "Pan et al\\.,? 2017",
      "shortCiteRegEx" : "Pan et al\\.",
      "year" : 2017
    }, {
      "title" : "How multilingual is multilingual BERT",
      "author" : [ "Telmo Pires", "Eva Schlinger", "Dan Garrette" ],
      "venue" : null,
      "citeRegEx" : "Pires et al\\.,? \\Q2019\\E",
      "shortCiteRegEx" : "Pires et al\\.",
      "year" : 2019
    }, {
      "title" : "Distilbert, a distilled version of bert: smaller, faster, cheaper and lighter",
      "author" : [ "Victor Sanh", "Lysandre Debut", "Julien Chaumond", "Thomas Wolf." ],
      "venue" : "The 5th Workshop on Energy Efficient Machine Learning and Cognitive Computing - NeurIPS 2019.",
      "citeRegEx" : "Sanh et al\\.,? 2019",
      "shortCiteRegEx" : "Sanh et al\\.",
      "year" : 2019
    }, {
      "title" : "Learning character-level representations for part-of-speech tagging",
      "author" : [ "Cicero D Santos", "Bianca Zadrozny." ],
      "venue" : "Proceedings of the 31st international conference on machine learning (ICML-14), pages 1818–1826.",
      "citeRegEx" : "Santos and Zadrozny.,? 2014",
      "shortCiteRegEx" : "Santos and Zadrozny.",
      "year" : 2014
    }, {
      "title" : "Viable dependency parsing as sequence labeling",
      "author" : [ "Michalina Strzyz", "David Vilares", "Carlos GómezRodrı́guez" ],
      "venue" : "In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Lan-",
      "citeRegEx" : "Strzyz et al\\.,? \\Q2019\\E",
      "shortCiteRegEx" : "Strzyz et al\\.",
      "year" : 2019
    }, {
      "title" : "Multilingual neural machine translation with knowledge distillation",
      "author" : [ "Xu Tan", "Yi Ren", "Di He", "Tao Qin", "Tie-Yan Liu." ],
      "venue" : "International Conference on Learning Representations.",
      "citeRegEx" : "Tan et al\\.,? 2019",
      "shortCiteRegEx" : "Tan et al\\.",
      "year" : 2019
    }, {
      "title" : "Distilling taskspecific knowledge from bert into simple neural networks",
      "author" : [ "Raphael Tang", "Yao Lu", "Linqing Liu", "Lili Mou", "Olga Vechtomova", "Jimmy Lin." ],
      "venue" : "arXiv preprint arXiv:1903.12136.",
      "citeRegEx" : "Tang et al\\.,? 2019",
      "shortCiteRegEx" : "Tang et al\\.",
      "year" : 2019
    }, {
      "title" : "Introduction to the CoNLL-2002 shared task: Language-independent named entity recognition",
      "author" : [ "Erik F. Tjong Kim Sang." ],
      "venue" : "COLING-02: The 6th Conference on Natural Language Learning 2002 (CoNLL-2002).",
      "citeRegEx" : "Sang.,? 2002",
      "shortCiteRegEx" : "Sang.",
      "year" : 2002
    }, {
      "title" : "Introduction to the CoNLL-2003 shared task: Language-independent named entity recognition",
      "author" : [ "Erik F. Tjong Kim Sang", "Fien De Meulder." ],
      "venue" : "Proceedings of the Seventh Conference on Natural Language Learning at HLT-NAACL 2003, pages",
      "citeRegEx" : "Sang and Meulder.,? 2003",
      "shortCiteRegEx" : "Sang and Meulder.",
      "year" : 2003
    }, {
      "title" : "Small and practical BERT models for sequence labeling",
      "author" : [ "Henry Tsai", "Jason Riesa", "Melvin Johnson", "Naveen Arivazhagan", "Xin Li", "Amelia Archer." ],
      "venue" : "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the",
      "citeRegEx" : "Tsai et al\\.,? 2019",
      "shortCiteRegEx" : "Tsai et al\\.",
      "year" : 2019
    }, {
      "title" : "Benchmarking approximate inference methods for neural structured prediction",
      "author" : [ "Lifu Tu", "Kevin Gimpel." ],
      "venue" : "Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language",
      "citeRegEx" : "Tu and Gimpel.,? 2019",
      "shortCiteRegEx" : "Tu and Gimpel.",
      "year" : 2019
    }, {
      "title" : "Second-order semantic dependency parsing with end-to-end neural networks",
      "author" : [ "Xinyu Wang", "Jingxian Huang", "Kewei Tu." ],
      "venue" : "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 4609–4618, Florence,",
      "citeRegEx" : "Wang et al\\.,? 2019",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2019
    }, {
      "title" : "Structure-level knowledge distillation for multilingual sequence labeling",
      "author" : [ "Xinyu Wang", "Yong Jiang", "Nguyen Bach", "Tao Wang", "Fei Huang", "Kewei Tu." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics,",
      "citeRegEx" : "Wang et al\\.,? 2020a",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2020
    }, {
      "title" : "AIN: Fast and accurate sequence labeling with approximate inference network",
      "author" : [ "Xinyu Wang", "Yong Jiang", "Nguyen Bach", "Tao Wang", "Zhongqiang Huang", "Fei Huang", "Kewei Tu." ],
      "venue" : "Proceedings of the 2020 Conference on Empirical Methods",
      "citeRegEx" : "Wang et al\\.,? 2020b",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2020
    }, {
      "title" : "Automated Concatenation of Embeddings for Structured Prediction",
      "author" : [ "Xinyu Wang", "Yong Jiang", "Nguyen Bach", "Tao Wang", "Zhongqiang Huang", "Fei Huang", "Kewei Tu." ],
      "venue" : "the Joint Conference of the 59th Annual Meeting of the Association",
      "citeRegEx" : "Wang et al\\.,? 2021a",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2021
    }, {
      "title" : "Improving Named Entity Recognition by External Context Retrieving and Cooperative Learning",
      "author" : [ "Xinyu Wang", "Yong Jiang", "Nguyen Bach", "Tao Wang", "Zhongqiang Huang", "Fei Huang", "Kewei Tu." ],
      "venue" : "the Joint Conference of the 59th Annual Meet-",
      "citeRegEx" : "Wang et al\\.,? 2021b",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2021
    }, {
      "title" : "2020c. More embeddings, better sequence labelers",
      "author" : [ "Xinyu Wang", "Yong Jiang", "Nguyen Bach", "Tao Wang", "Huang Zhongqiang", "Fei Huang", "Kewei Tu" ],
      "venue" : "In Findings of EMNLP, Online",
      "citeRegEx" : "Wang et al\\.,? \\Q2020\\E",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2020
    }, {
      "title" : "Enhanced Universal Dependency parsing with secondorder inference and mixture of training data",
      "author" : [ "Xinyu Wang", "Yong Jiang", "Kewei Tu." ],
      "venue" : "Proceedings of the 16th International Conference on Parsing Technologies and the IWPT 2020 Shared",
      "citeRegEx" : "Wang et al\\.,? 2020d",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2020
    }, {
      "title" : "Second-order neural dependency parsing with message passing and endto-end training",
      "author" : [ "Xinyu Wang", "Kewei Tu." ],
      "venue" : "Proceedings of the 1st Conference of the Asia-Pacific Chapter of the Association",
      "citeRegEx" : "Wang and Tu.,? 2020",
      "shortCiteRegEx" : "Wang and Tu.",
      "year" : 2020
    }, {
      "title" : "Don’t eclipse your arts due to small discrepancies: Boundary repositioning with a pointer network for aspect extraction",
      "author" : [ "Zhenkai Wei", "Yu Hong", "Bowei Zou", "Meng Cheng", "Jianmin Yao." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the Asso-",
      "citeRegEx" : "Wei et al\\.,? 2020",
      "shortCiteRegEx" : "Wei et al\\.",
      "year" : 2020
    }, {
      "title" : "Beto, bentz, becas: The surprising cross-lingual effectiveness of BERT",
      "author" : [ "Shijie Wu", "Mark Dredze." ],
      "venue" : "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natu-",
      "citeRegEx" : "Wu and Dredze.,? 2019",
      "shortCiteRegEx" : "Wu and Dredze.",
      "year" : 2019
    }, {
      "title" : "Design challenges and misconceptions in neural sequence labeling",
      "author" : [ "Jie Yang", "Shuailong Liang", "Yue Zhang." ],
      "venue" : "Proceedings of the 27th International Conference on Computational Linguistics, pages 3879–3889, Santa Fe, New Mexico, USA. As-",
      "citeRegEx" : "Yang et al\\.,? 2018",
      "shortCiteRegEx" : "Yang et al\\.",
      "year" : 2018
    }, {
      "title" : "Named entity recognition as dependency parsing",
      "author" : [ "Juntao Yu", "Bernd Bohnet", "Massimo Poesio." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 6470– 6476, Online. Association for Computational Lin-",
      "citeRegEx" : "Yu et al\\.,? 2020",
      "shortCiteRegEx" : "Yu et al\\.",
      "year" : 2020
    }, {
      "title" : "Efficient second-order TreeCRF for neural dependency parsing",
      "author" : [ "Yu Zhang", "Zhenghua Li", "Min Zhang." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 3295–3305, Online. Association for Computa-",
      "citeRegEx" : "Zhang et al\\.,? 2020",
      "shortCiteRegEx" : "Zhang et al\\.",
      "year" : 2020
    } ],
    "referenceMentions" : [ {
      "referenceID" : 4,
      "context" : "An interesting and viable solution to this problem is knowledge distillation (KD) (Buciluǎ et al., 2006; Ba and Caruana, 2014; Hinton et al., 2015), which can be used to transfer the knowledge of a large model (the teacher) to a smaller model (the student).",
      "startOffset" : 82,
      "endOffset" : 147
    }, {
      "referenceID" : 2,
      "context" : "An interesting and viable solution to this problem is knowledge distillation (KD) (Buciluǎ et al., 2006; Ba and Caruana, 2014; Hinton et al., 2015), which can be used to transfer the knowledge of a large model (the teacher) to a smaller model (the student).",
      "startOffset" : 82,
      "endOffset" : 147
    }, {
      "referenceID" : 11,
      "context" : "An interesting and viable solution to this problem is knowledge distillation (KD) (Buciluǎ et al., 2006; Ba and Caruana, 2014; Hinton et al., 2015), which can be used to transfer the knowledge of a large model (the teacher) to a smaller model (the student).",
      "startOffset" : 82,
      "endOffset" : 147
    }, {
      "referenceID" : 7,
      "context" : "In the field of natural language processing (NLP), for example, KD has been successfully applied to compress massive pretrained language models such as BERT (Devlin et al., 2019) and XLM-R (Conneau et al.",
      "startOffset" : 157,
      "endOffset" : 178
    }, {
      "referenceID" : 6,
      "context" : ", 2019) and XLM-R (Conneau et al., 2020) into much smaller and faster models without significant loss in accuracy (Tang et al.",
      "startOffset" : 18,
      "endOffset" : 40
    }, {
      "referenceID" : 27,
      "context" : ", 2020) into much smaller and faster models without significant loss in accuracy (Tang et al., 2019; Sanh et al., 2019; Tsai et al., 2019; Mukherjee and Hassan Awadallah, 2020).",
      "startOffset" : 81,
      "endOffset" : 176
    }, {
      "referenceID" : 23,
      "context" : ", 2020) into much smaller and faster models without significant loss in accuracy (Tang et al., 2019; Sanh et al., 2019; Tsai et al., 2019; Mukherjee and Hassan Awadallah, 2020).",
      "startOffset" : 81,
      "endOffset" : 176
    }, {
      "referenceID" : 30,
      "context" : ", 2020) into much smaller and faster models without significant loss in accuracy (Tang et al., 2019; Sanh et al., 2019; Tsai et al., 2019; Mukherjee and Hassan Awadallah, 2020).",
      "startOffset" : 81,
      "endOffset" : 176
    }, {
      "referenceID" : 13,
      "context" : "Previous approaches to structural KD either choose to perform KD on local decisions or substructures instead of on the full output structure, or resort to Top-K approximation of the objective (Kim and Rush, 2016; Kuncoro et al., 2016; Wang et al., 2020a).",
      "startOffset" : 192,
      "endOffset" : 254
    }, {
      "referenceID" : 14,
      "context" : "Previous approaches to structural KD either choose to perform KD on local decisions or substructures instead of on the full output structure, or resort to Top-K approximation of the objective (Kim and Rush, 2016; Kuncoro et al., 2016; Wang et al., 2020a).",
      "startOffset" : 192,
      "endOffset" : 254
    }, {
      "referenceID" : 33,
      "context" : "Previous approaches to structural KD either choose to perform KD on local decisions or substructures instead of on the full output structure, or resort to Top-K approximation of the objective (Kim and Rush, 2016; Kuncoro et al., 2016; Wang et al., 2020a).",
      "startOffset" : 192,
      "endOffset" : 254
    }, {
      "referenceID" : 42,
      "context" : "Previous work (Yang et al., 2018; Wang et al., 2020a) shows that a linear-chain CRF decoder often leads to better performance than a MaxEnt decoder for many sequence labeling tasks.",
      "startOffset" : 14,
      "endOffset" : 53
    }, {
      "referenceID" : 33,
      "context" : "Previous work (Yang et al., 2018; Wang et al., 2020a) shows that a linear-chain CRF decoder often leads to better performance than a MaxEnt decoder for many sequence labeling tasks.",
      "startOffset" : 14,
      "endOffset" : 53
    }, {
      "referenceID" : 32,
      "context" : "It has been found that secondorder extensions of the biaffine parser often have higher parsing accuracy (Wang et al., 2019; Zhang et al., 2020; Wang et al., 2020d; Wang and Tu, 2020).",
      "startOffset" : 104,
      "endOffset" : 182
    }, {
      "referenceID" : 44,
      "context" : "It has been found that secondorder extensions of the biaffine parser often have higher parsing accuracy (Wang et al., 2019; Zhang et al., 2020; Wang et al., 2020d; Wang and Tu, 2020).",
      "startOffset" : 104,
      "endOffset" : 182
    }, {
      "referenceID" : 38,
      "context" : "It has been found that secondorder extensions of the biaffine parser often have higher parsing accuracy (Wang et al., 2019; Zhang et al., 2020; Wang et al., 2020d; Wang and Tu, 2020).",
      "startOffset" : 104,
      "endOffset" : 182
    }, {
      "referenceID" : 39,
      "context" : "It has been found that secondorder extensions of the biaffine parser often have higher parsing accuracy (Wang et al., 2019; Zhang et al., 2020; Wang et al., 2020d; Wang and Tu, 2020).",
      "startOffset" : 104,
      "endOffset" : 182
    }, {
      "referenceID" : 22,
      "context" : "Previous work (Pires et al., 2019; Wu and Dredze, 2019) has shown that multilingual BERT (M-BERT) has strong zero-shot crosslingual transferability in NER tasks.",
      "startOffset" : 14,
      "endOffset" : 55
    }, {
      "referenceID" : 41,
      "context" : "Previous work (Pires et al., 2019; Wu and Dredze, 2019) has shown that multilingual BERT (M-BERT) has strong zero-shot crosslingual transferability in NER tasks.",
      "startOffset" : 14,
      "endOffset" : 55
    }, {
      "referenceID" : 21,
      "context" : "Datasets We use CoNLL 2002/2003 datasets (Tjong Kim Sang, 2002; Tjong Kim Sang and De Meulder, 2003) for Case 1a, 2a and 4, and use WikiAnn datasets (Pan et al., 2017) for Case 1a, 2a, 3, and 4.",
      "startOffset" : 149,
      "endOffset" : 167
    }, {
      "referenceID" : 3,
      "context" : "Models For the student models in all the cases, we use fastText (Bojanowski et al., 2017) word embeddings and character embeddings as the word representation.",
      "startOffset" : 64,
      "endOffset" : 89
    }, {
      "referenceID" : 0,
      "context" : "For Case 1a, 2a and 4, we concatenate the multilingual BERT, Flair (Akbik et al., 2018), fastText embeddings and character embeddings (Santos and Zadrozny, 2014) as the word representations for stronger monolingual teacher models (Wang et al.",
      "startOffset" : 67,
      "endOffset" : 87
    }, {
      "referenceID" : 24,
      "context" : ", 2018), fastText embeddings and character embeddings (Santos and Zadrozny, 2014) as the word representations for stronger monolingual teacher models (Wang et al.",
      "startOffset" : 54,
      "endOffset" : 81
    }, {
      "referenceID" : 17,
      "context" : "For M-BERT fine-tuning in Case 3, we mix the training data of the four source datasets and train the teacher model with the AdamW optimizer (Loshchilov and Hutter, 2018) with a learning rate of 5×10−5 for 10 epochs.",
      "startOffset" : 140,
      "endOffset" : 169
    }, {
      "referenceID" : 30,
      "context" : "There is a recent increase of interest in training multilingual NER models (Tsai et al., 2019; Mukherjee and Hassan Awadallah, 2020) because of the strong",
      "startOffset" : 75,
      "endOffset" : 132
    }, {
      "referenceID" : 33,
      "context" : "Existing work explored knowledge distillation approaches to train fast and effective multilingual NER models with the help of monolingual teachers (Wang et al., 2020a).",
      "startOffset" : 147,
      "endOffset" : 167
    }, {
      "referenceID" : 11,
      "context" : "A frequently used KD technique is dividing the logits of probability distributions of both the teacher and the student by a temperature in the KD objective (Hinton et al., 2015).",
      "startOffset" : 156,
      "endOffset" : 177
    }, {
      "referenceID" : 7,
      "context" : "Meanwhile, a lot of other work used the MaxEnt layer instead of the CRF for sequence labeling (Devlin et al., 2019; Conneau et al., 2020; Wang et al., 2020b) because MaxEnt makes it easier to fine-tune pretrained contextual embeddings in training.",
      "startOffset" : 94,
      "endOffset" : 157
    }, {
      "referenceID" : 6,
      "context" : "Meanwhile, a lot of other work used the MaxEnt layer instead of the CRF for sequence labeling (Devlin et al., 2019; Conneau et al., 2020; Wang et al., 2020b) because MaxEnt makes it easier to fine-tune pretrained contextual embeddings in training.",
      "startOffset" : 94,
      "endOffset" : 157
    }, {
      "referenceID" : 34,
      "context" : "Meanwhile, a lot of other work used the MaxEnt layer instead of the CRF for sequence labeling (Devlin et al., 2019; Conneau et al., 2020; Wang et al., 2020b) because MaxEnt makes it easier to fine-tune pretrained contextual embeddings in training.",
      "startOffset" : 94,
      "endOffset" : 157
    }, {
      "referenceID" : 32,
      "context" : "In dependency parsing, recent work shows that second-order CRF parsers achieve significantly higher accuracy than first-order parsers (Wang et al., 2019; Zhang et al., 2020).",
      "startOffset" : 134,
      "endOffset" : 173
    }, {
      "referenceID" : 44,
      "context" : "In dependency parsing, recent work shows that second-order CRF parsers achieve significantly higher accuracy than first-order parsers (Wang et al., 2019; Zhang et al., 2020).",
      "startOffset" : 134,
      "endOffset" : 173
    }, {
      "referenceID" : 8,
      "context" : "(2020) showed that secondorder parsing is four times slower than the simple head-selection first-order approach (Dozat and Manning, 2017).",
      "startOffset" : 112,
      "endOffset" : 137
    }, {
      "referenceID" : 13,
      "context" : "KD has been applied in many structured prediction tasks in the fields of NLP, speech recognition and computer vision, with applications such as neural machine translation (Kim and Rush, 2016; Tan et al., 2019), sequence labeling (Tu and Gimpel, 2019; Wang et al.",
      "startOffset" : 171,
      "endOffset" : 209
    }, {
      "referenceID" : 26,
      "context" : "KD has been applied in many structured prediction tasks in the fields of NLP, speech recognition and computer vision, with applications such as neural machine translation (Kim and Rush, 2016; Tan et al., 2019), sequence labeling (Tu and Gimpel, 2019; Wang et al.",
      "startOffset" : 171,
      "endOffset" : 209
    }, {
      "referenceID" : 31,
      "context" : ", 2019), sequence labeling (Tu and Gimpel, 2019; Wang et al., 2020a), connectionist temporal classification (Huang et al.",
      "startOffset" : 27,
      "endOffset" : 68
    }, {
      "referenceID" : 33,
      "context" : ", 2019), sequence labeling (Tu and Gimpel, 2019; Wang et al., 2020a), connectionist temporal classification (Huang et al.",
      "startOffset" : 27,
      "endOffset" : 68
    }, {
      "referenceID" : 12,
      "context" : ", 2020a), connectionist temporal classification (Huang et al., 2018), image semantic segmentation (Liu et al.",
      "startOffset" : 48,
      "endOffset" : 68
    }, {
      "referenceID" : 15,
      "context" : ", 2018), image semantic segmentation (Liu et al., 2019a) and so on.",
      "startOffset" : 37,
      "endOffset" : 56
    } ],
    "year" : 2021,
    "abstractText" : "Knowledge distillation is a critical technique to transfer knowledge between models, typically from a large model (the teacher) to a more fine-grained one (the student). The objective function of knowledge distillation is typically the cross-entropy between the teacher and the student’s output distributions. However, for structured prediction problems, the output space is exponential in size; therefore, the cross-entropy objective becomes intractable to compute and optimize directly. In this paper, we derive a factorized form of the knowledge distillation objective for structured prediction, which is tractable for many typical choices of the teacher and student models. In particular, we show the tractability and empirical effectiveness of structural knowledge distillation between sequence labeling and dependency parsing models under four different scenarios: 1) the teacher and student share the same factorization form of the output structure scoring function; 2) the student factorization produces more fine-grained substructures than the teacher factorization; 3) the teacher factorization produces more fine-grained substructures than the student factorization; 4) the factorization forms from the teacher and the student are incompatible.1",
    "creator" : "LaTeX with hyperref"
  }
}