{
  "name" : "2021.acl-long.242.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "Online Learning Meets Machine Translation Evaluation: Finding the Best Systems with the Least Human Effort",
    "authors" : [ "Vânia Mendonça", "Ricardo Rei", "Luı́sa Coheur", "Alberto Sardinha", "Ana Lúcia Santos" ],
    "emails" : [ "jose.alberto.sardinha}@tecnico.ulisboa.pt,", "ricardo.rei@unbabel.com,", "als@letras.ulisboa.pt" ],
    "sections" : [ {
      "heading" : null,
      "text" : "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 3105–3117\nAugust 1–6, 2021. ©2021 Association for Computational Linguistics\n3105"
    }, {
      "heading" : "1 Introduction",
      "text" : "In Machine Translation (MT), measuring the quality of a large amount of automatic translations can be a challenge. Automatic metrics like BLEU (Papineni et al., 2002) remain popular due to their fast and free computations. Yet, in the last few years we have seen that, as MT quality improves, automatic metrics become less reliable (Ma et al., 2019; Mathur et al., 2020). For example, in the Conference on Machine Translation (WMT)’19 News Translation shared task, the winning system according to human annotators was not even in the top-5 according to BLEU (Barrault et al., 2019). On the other hand, using human assessments can be expensive, especially when evaluating multiple systems. In a real world scenario, given an arbitrary number of MT systems, one would need to evaluate them individually to find the best systems for a given language pair. However, that requires a considerable effort and there may not be enough human annotators to evaluate all the systems’ translations. For\ninstance, in the aforementioned WMT’19 shared task, many translations from the competing systems did not receive any human assessment.\nGiven an ensemble of competing, independent MT systems, how can we dynamically find the best ones for a given language pair, while making the most of existing human feedback? To address this question, we present a novel application of online learning to MT: each MT system in the ensemble is assigned to a weight, and the systems’ weights are updated considering human feedback regarding the quality of their translations at each iteration. We use online learning algorithms with theoretical performance guarantees, under the frameworks of prediction with expert advice (Cesa-Bianchi and Lugosi, 2006) and multi-armed bandits (Robbins, 1952; Lai and Robbins, 1985).\nWe contribute with an online MT ensemble that allows to reduce human effort by immediately incorporating human feedback in order to dynamically converge to the best systems1. Our experiments on WMT’19 News Translation test sets show that our online approaches indeed converge to the shared task’s official top-3 systems (or to a subset of them) in just a few hundred iterations for all the language pairs experimented. Moreover, it does so while coping with the aforementioned lack of human assessments for many translations, through the use of fallback metrics."
    }, {
      "heading" : "2 Online learning frameworks",
      "text" : "To provide some background on our proposal, we start by describing the online learning frameworks that we apply in this paper: prediction with expert advice and multi-armed bandits.\nA problem of prediction with expert advice can be described as an iterative game between a fore-\n1The code for our experiments can be found in https: //github.com/vania-mendonca/MTOL\ncaster and the environment, in which the forecaster seeks advice from different sources (experts) in order to provide the best forecast (Cesa-Bianchi and Lugosi, 2006). At each iteration t, the forecaster consults the predictions p̂j,t, j = 1 . . . J, made by a set of J weighted experts, in the decision space D. Considering these predictions, the forecaster makes its own prediction, p̂f,t ∈ D. At the same time, the environment reveals an outcome yt in the decision space Y (which may not necessarily be the same as D).\nA well-established algorithm to learn the experts’ weights in this framework is Exponentially Weighted Average Forecaster (EWAF) (CesaBianchi and Lugosi, 2006). In EWAF, the prediction made by the forecaster is randomly selected following the probability distribution based on the experts’ weights ω1,t−1 . . . ωJ,t−1:\np̂f,t = ∑J j=1 ωj,t−1pj,t∑J j=1 ωj,t−1 . (1)\nAt the end of each iteration, the forecaster and each of the experts receive a non-negative loss based on the outcome yt revealed by the environment (`f,t and `j,t, respectively). The weight ωj,t of each expert j = 1 . . . J is then updated according to the loss received by each expert, as follows:\nωj,t = ωj,t−1e −η`j,t (2)\nIf the parameter η is set to √\n8 log J T , it can be\nshown that the forecaster quickly converges to the performance of the best expert after T iterations (Cesa-Bianchi and Lugosi, 2006).\nPrediction with expert advice assumes that both the forecaster and all the experts receive a loss once the environment’s outcome is revealed. However, this assumption may not always hold (i.e., there may not always be an environment’s explicit feedback or a way to obtain the loss for all the experts). Thus, we consider a related class of problems, multi-armed bandits, in which the environment’s outcome is unknown (Robbins, 1952; Lai and Robbins, 1985). In this class of problems, one starts by attempting to estimate the means of the loss distributions for each expert (also known as arm) in the first iterations (the exploration phase), and when the forecaster has a high level of confidence in the estimated values, one may keep choosing the prediction with the smallest estimated loss (the exploitation phase).\nA popular online algorithm for adversarial multiarmed bandits is Exponential-weighting for Exploration and Exploitation (EXP3) (Auer et al., 1995). At each iteration t, the forecaster’s action is randomly selected according to the probability distribution given by the weights of each arm j:\np̂f,t = ωj∑J j′=1 ωj′\n(3)\nIn this framework, the forecaster is only able to measure the loss of the action it selects at each iteration, but it cannot measure the loss of other possible actions. Thus, only the weight of the arm associated with this action is updated, as follows:\nωj,t = ωj,t−1e −η ˆ̀j,t (4)\nwhere ˆ̀j,t = `j,t pj,t\nand pj,t is the probability of choosing arm j at iteration t. By setting η to√\n2logJ T |A| (where |A| is the number the actions available, and may be the same as the number of arms J), it can be shown that the forecaster quickly converges to the performance of the best arm.\nBoth of these frameworks are relatively underexplored in NLP, despite their potential to converge to the best performing approach available in scenarios where feedback is naturally present. Therefore, we propose to apply them in order to find the best MT models with little human feedback."
    }, {
      "heading" : "3 Machine Translation with Online Learning",
      "text" : "In this work, we consider the following scenario as the starting point: there is an ensemble composed of an arbitrary number of MT systems; given a segment from a source language corpus, each system outputs a translation in the target language; then, the quality of the translations produced by each of the available systems is assessed by one or more human evaluators with a score reflecting their quality.\nWe frame this scenario as an online learning problem under two different frameworks: (i) prediction with expert advice (using EWAF as the learning algorithm), and (ii) multi-armed bandits (using EXP3 as the learning algorithm). The decision on whether to use one or another framework in an MT scenario depends on whether there is human feedback available for the translations outputted by all the available systems or only for the final choice of the ensemble of systems.\nAn overview of the online learning process is shown in Fig.1, and can be summed up as follows. Each MT system is an expert (or arm) j = 1 . . . J , associated with a weight ωj (all the systems start with same weights). At each iteration t, a segment srct is selected from the source language corpus and handed to all the MT systems. Each system outputs a translation translj,t in the target language, and one of these translations is selected as the forecaster’s action according to the probability distribution given by the systems’ weights (Eq.1 for EWAF and Eq. 3 for EXP3). The chosen translation translf,t (when using EXP3) or the translations outputted by all the systems (when using EWAF) receive a human assessment score2 scorej,t, from which the loss `j,t is derived for the respective MT system. Finally, the weight of the chosen system or the weights of all the systems are updated as a function of the loss received, according to Eq.4 (when using EXP3) and Eq.2 (when using EWAF), respectively (where `j,t = −scorej,t)."
    }, {
      "heading" : "4 Experimental setup",
      "text" : "To validate our proposal, we designed an experiment using data from an MT shared task. The main questions addressed by our experiment are: (i) whether an online learning approach can give a greater weight to the top performing systems for each language pair according to the shared task’s official ranking, and (ii) if so, how quickly (i.e., how many translations need to be assessed by human evaluators in order to find the best system).\nBelow we detail the datasets used (Section 4.1) and the feedback sources considered (Section 4.2), as well as other experimental decisions (Section 4.3).\n2If multiple human assessments were made for the same translation, scorej,t is the average of the scores received."
    }, {
      "heading" : "4.1 Datasets",
      "text" : "We used the test datasets made available by the WMT’19 News Translation shared task (Barrault et al., 2019). For each language pair, each source segment is associated with the following information:\n• A reference translation in the target language (produced specifically for the task);\n• The automatic translation outputted by each system competing in the task for that language pair;\n• The average score obtained by each automatic translation, according to human assessments made by one or more human evaluators, in two formats: a raw score in [0;100] and a zscore in [−∞; +∞]. Not all the automatic translations received a human assessment;\n• The number of human evaluators for each automatic translation (if there were any).\nFor brevity, we focused on five language pairs, listed in Table 1. The official top 3 systems for each pair, according to the average z-score, are shown in Table 2. Our choice of language pairs attempts to capture as many different phenomena as possible with the fewest pairs:\n• English→ German (en-de): This is the language pair with the most competitors and does not have a clear winning system (the winner differs depending on whether one considers the z-score or the raw score);\n• French → German (fr-de): Unlike most language pairs, this pair features two languages other than English. Moreover, there is a strong imbalance between translations lacking human assessments and translations that received at least one assessment;\n• German→ Czech (de-cs): Besides featuring two languages other than English, this pair stands out as it was devised as an unsupervised task (i.e., English was used as a “hub” language);\n• Gujarati→ English (gu-en): This is one of the task’s low-resource language pairs (i.e., whose test set is half the size of most languagepairs in the task), and is one where there may\nbe more linguistic differences between the source and the target languages (e.g., different writing systems). Unlike en-de, there is a clear winner considering both raw and z-score. Moreover, three of the competing systems did not receive any human assessment on their translations;\n• Lithuanian→ English (lt-en): This is another low-resource language pair, with a rather competitive top 3. Unlike most language pairs, all the translations submitted by the competing systems for this pair received a human assessment.\nFor all these language pairs (except English→ German), each segment was given an assessment score considering only the reference translation (and without access to the segment’s context within the document to which it belongs). For English→ German, scores were given considering the source segment instead of the reference, and evaluators\nhad access to the segment’s context within the document."
    }, {
      "heading" : "4.2 Human feedback",
      "text" : "A key condition for applying online learning to this scenario is the availability of feedback. We use the human assessment raw scores3 present in the test sets as a feedback source to compute the loss and update the weight of each MT system, as already suggested in Section 3. However, not all translations received human assessments (recall Table 1). To cope with this issue, we designed different variants of this loss function, following different fallback strategies:\n• human-zero: If there is no human assessment for the current translation, a score of zero is returned (leading to an unchanged weight on that iteration);\n3Although we assume an absolute scale of scores in [0;100] in our experiments, our approach could be applied to any other level of granularity.\n• human-avg: If there is no human assessment for the current translation, the average of the previous scores received by the system behind that translation is returned as the current score;\n• human-comet: If there is no human assessment for the current translation, the COMET score (Rei et al., 2020a) between the translation and the pair source/reference available in the corpus is returned as the current score. We pre-trained4 this automatic metric on the datasets of previous shared tasks (WMT’17 (Bojar et al., 2017) and WMT’18 (Bojar et al., 2018)). Thus, for most translations, it displays a small difference regarding the existing human scores (see Fig. 2 for the case of en-de). Moreover, this metric correlates better with ratings by professional translators than the WMT scores (Freitag et al., 2021)."
    }, {
      "heading" : "4.3 Experimental design",
      "text" : "For each language pair, we shuffled the test set once, so that the performance of the online algorithms would not be biased by the original order of the segments in the test set. We ran EWAF once for each loss function, and we ran EXP3 10 times per loss function and report the average weights obtained across runs, since EXP3’s weight evolution is critically influenced by the random choice of an arm at each iteration. We normalized the translation scores scorej,t to be in the interval [0, 1] and rounded them to two decimal places, to avoid exploding weight values due to the exponential update rule.\n4We trained this metric from scratch following the hyperparameters described in Rei et al. (2020b), except that we used the raw scores instead of the z-normalized scores."
    }, {
      "heading" : "5 Results and discussion",
      "text" : "In order to observe whether (and how soon) our online approach converges to the best systems, we report the overlap between the top n = 1, 3 systems with greatest weights according to our approaches, ŝn, and the top n = 1, 3 systems according to the shared task’s official ranking, s∗n, at specific iterations:\ntopn = |ŝn ∩ s∗n|\nn , n = 1, 3 (5)\nWe preferred this metric over a rank correlation metric, as we are focused on whether our online approach follows the performance of the best MT systems. In a realistic scenario (e.g., a Web MT service), a user would most likely rely solely on the main translation returned, or would at most consider one or two alternative translations. Moreover, due to the lack of a large enough coverage of human assessments, the scores obtained in the shared task are not reliable enough to discriminate between similarly performing systems.\nStarting with en-de (Table 3), this was the language pair for which our approach appears to be the least successful, since, for most of the iterations examined, it failed to converge to the best system. Even so, it managed to converge to the top 3 systems, doing so particularly early in the learning process (50 iterations) when using EWAF with human-avg and human-comet as loss functions (i.e., when using fallback scores). Recall that, for this language pair, there were different official winning systems depending on whether one considers the z-score or the raw score (recall Table 2); since we use the raw score as the loss function, it is expectable that our approach does not necessarily converge to the winner according to the z-score.\nFor fr-de (Table 4), our online approach often converges to the top 3 systems (or a subset of them) throughout the learning process (even at just 10 iterations), and it also converges to the best system when using EWAF with human-comet. This is a particularly interesting result if we recall that, out of the five pairs considered, fr-de had the lowest coverage of human assessments by far (see Table 1), thus suggesting that using COMET may be an adequate fallback strategy.\nFor de-cs (Table 5), we can see that, regardless of the algorithm and loss function used, there is an overlap of at least one system between our top 3 and the shared task’s official top 3, after going through\nonly as few as 10 iterations (despite a considerable lack of human assessments in this language pair). We can also see that the human-comet loss function is the most successful overall, which reinforces the idea that COMET may be an appropriate fallback metric in the absence of human scores for a given translation. Since this is the language pair for which there seems to be a more similar performance across different algorithms and loss functions, we also report the weight evolution plots for this pair in order to inspect what changes depending on the algorithm and fallback strategy used5. Looking at EWAF combined with the human-zero loss function (Fig. 3), one can see a rather irregular evolution for the weights of the top systems, which may be explained by the distribution of the translations lacking human assessments across different systems and learning iterations. Using the human-avg loss function (Fig.4) allows for a more monotonous evolution, by rewarding the sys-\n5The plots for the remaining pairs can be found in App. A.\ntems that were doing better overall in the absence of human assessments. Using the human-comet loss function (Fig. 5) paints a similar picture, as the COMET scores for this language pair seem to be in line with the official ranking (although they appear to benefit the third best system in detriment of the second best). Finally, using EXP3 instead of EWAF (Fig. 6), combined with human-zero, leads to much less pronounced weights, but still in line with the official ranking. Recall that, for EXP3, these weights are averaged across different runs: since each run may lead to different top systems, the difference between the averaged weights ends up being more smooth, i.e., there is a great variance across runs (this happens regardless of the language pair or loss function).\nAs for gu-en (Table 6), our approach (using EWAF with human-zero) converges to the best system and to a subset of the top 3 within just 10 iterations; on the other hand, using human-comet does not do as well as not using a fallback strategy, at least when combined with EWAF. However, recall that, for this pair, there were systems that did not receive any human assessments at all for their translations (that being the reason why we do not report human-avg for this pair: the resulting weights end up being the same as when using human-zero). One of the systems that did not receive any human assessments, online-B, ended\nup receiving high COMET scores, thus leading to a weaker overlap between the online approach ranking and the official ranking.\nFinally, for lt-en (Table 7) we only report the human-zero loss function, since this is the only pair for which there are human assessments for all translations. Interestingly, the online approaches do not do well as quickly as for other pairs, but eventually get there (within 100 to 500 iterations).\nTo sum up these results: although factors like the coverage of human assessments or the combinations of online algorithm and loss function used influence how well our approach does, we can still conclude that using an online learning approach allows to converge to the top 3 systems according to the official ranking (or at least to a subset of them) in just a few hundred iterations (and, in some cases, in just a few dozens of iterations) for all the language pairs considered."
    }, {
      "heading" : "6 Related work",
      "text" : ""
    }, {
      "heading" : "6.1 WMT’19 News Translation Shared Task",
      "text" : "Every year, since 2006, the Conference on Machine Translation (WMT) is responsible for organizing several shared tasks where participants push the limits of MT and MT evaluation (Koehn and Monz, 2006; Barrault et al., 2020). In the News Translation shared task, participants submit the out-\nputs of their systems that are then evaluated by a community of human evaluators using Direct Assessment scores (Graham et al., 2013). Thus, the winner is the system that achieves the highest average score. For WMT’19 (Barrault et al., 2019), most of the competing systems followed a Transformer architecture (Vaswani et al., 2017), with the\nmain differences among them being: (i) whether they considered document-level or only sentencelevel information; (ii) whether they were trained only on the training data provided by the shared task, or on additional sources as well; (iii) whether they consisted of a single model or an ensemble."
    }, {
      "heading" : "6.2 Online learning for Machine Translation",
      "text" : "There has been a number of online learning approaches applied to MT in the past, mainly in Interactive MT and/or post-editing MT systems. However, most approaches aim at learning the parameters or feature weights of an MT model (Mathur et al., 2013; Denkowski et al., 2014; OrtizMartı́nez, 2016; Sokolov et al., 2016; Nguyen et al., 2017; Lam et al., 2018) or fine-tuning a pretrained model for domain adaptation (Turchi et al., 2017; Karimova et al., 2018; Peris and Casacuberta, 2019). Even in cases where the MT model is composed of several sub-models (e.g., Ortiz-Martı́nez (2016)), the goal is to online learn each sub-model’s specific parameters (while our learning goal is the weights of each system in an ensemble). Another key difference between these approaches and ours is that most of them use human post-edited translations as a source of feedback. The exceptions to this are the systems competing for WMT’17 shared task on online bandit learning for MT (Sokolov et al., 2017), as well as Lam et al. (2018), who use (simulated) quality judgments.\nThe most similar proposal to ours is that of Naradowsky et al. (2020), who ensemble different MT systems and dynamically select the best one for a given MT task or domain using stochastic multiarmed bandits and contextual bandits. The bandit algorithms learn from feedback simulated using a sentence-level BLEU score between the selected automatic translation and a reference translation.\nThus, to the best of our knowledge, we are the first to frame the MT problem as a problem of prediction with expert advice and adversarial multiarmed bandits in order to combine different systems into an ensemble that converges to the performance of the best individual systems, simulating the human-in-the-loop by using actual human assessments (when available)."
    }, {
      "heading" : "7 Conclusions and future work",
      "text" : "We proposed an online learning approach to address the issue of finding the best MT systems among an ensemble, while making the most of existing human feedback. In our experiments on WMT’19 News Translation datasets, our approach converged to the top-3 systems (or a subset of them) according to the official shared task’s ranking in just a few hundred iterations for all the language pairs considered (and just a few dozens in some cases), despite the lack of human assessments for\nmany translations. This is a promising result, not only for the purpose of reducing the human evaluations required to find the best systems in a shared task, but also for any MT application that has access to an ensemble of multiple independent systems and to a source of feedback from which it can learn iteratively (e.g., Web translation services).\nYet, our approach is limited by the quality of the collected human judgments. For future work, we plan to combine online learning with a more reliable human metric, such as the Multidimensional Quality Metric (MQM) (Lommel et al., 2014), so that we can focus on the quality of the assessments instead of their quantity."
    }, {
      "heading" : "Acknowledgments",
      "text" : "This work was supported by: Fundação para a Ciência e a Tecnologia (FCT) under references UIDB/50021/2020 (INESC-ID multi-annual funding) and UIDB/00214/2020 (CLUL), as well as under the HOTSPOT project with reference PTDC/CCI-COM/7203/2020; Air Force Office of Scientific Research under award number FA955019-1-0020; P2020 program, supervised by Agência Nacional de Inovação (ANI), under the project CMU-PT Ref. 045909 (MAIA). Vânia Mendonça was funded by an FCT grant with reference SFRH/BD/121443/2016.\nThe authors would like to thank the reviewers for their valuable comments, and to Soraia M. Alarcão for kindly proof-reading this document."
    }, {
      "heading" : "A Weight evolution (all language pairs)",
      "text" : "Here we present the weight evolution per MT system for all the combinations of language pairs, learning algorithms (EWAF or EXP3), and loss functions (human-zero, human-avg, or human-comet, when applicable) – except for those combinations that are already part of the main document.\nA.1 English→ German (en-de)\nA.2 French→ German (fr-de)\nA.3 German→ Czech (de-cs)\n0 100 200 300 400 500 600 700 800 900 1000 1100 1200 1300 1400 1500 1600 1700 1800 1900 Iteration #\n0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0 W ei gh ts\nonline-Y.0 online-B.0 NICT.6938 CAiRE.6949 online-G.0 online-A.0 lmu-unsup-nmt-de-cs.6845 NEU_KingSoft.6766 CUNI-Unsupervised-NER-post.6934 Unsupervised.de-cs.6935 Unsupervised.de-cs.6929\nFigure 19: EXP3 with human-avg loss. Recall that, for this language pair, the official top 3 systems were online-Y, online-B, and NICT.\n0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0 W ei gh ts\nA.4 Gujarati→ English (gu-en)\nA.5 Lithuanian→ English (lt-en)"
    } ],
    "references" : [ {
      "title" : "Gambling in a rigged casino: the adversarial multi-armed bandit problem",
      "author" : [ "Peter Auer", "Nicolo Cesa-Bianchi", "Yoav Freund", "Robert E. Schapire." ],
      "venue" : "Annual Symposium on Foundations of Computer Science - Proceedings, pages 322–331.",
      "citeRegEx" : "Auer et al\\.,? 1995",
      "shortCiteRegEx" : "Auer et al\\.",
      "year" : 1995
    }, {
      "title" : "Findings of the 2020 conference on machine translation (WMT20)",
      "author" : [ "Monz", "Makoto Morishita", "Masaaki Nagata", "Toshiaki Nakazawa", "Santanu Pal", "Matt Post", "Marcos Zampieri." ],
      "venue" : "Proceedings of the Fifth Conference on Machine Translation, pages",
      "citeRegEx" : "Monz et al\\.,? 2020",
      "shortCiteRegEx" : "Monz et al\\.",
      "year" : 2020
    }, {
      "title" : "Findings of the 2019 Conference on Machine Translation (WMT19)",
      "author" : [ "Shervin Malmasi", "Christof Monz", "Mathias Müller", "Santanu Pal", "Matt Post", "Marcos Zampieri." ],
      "venue" : "Proceedings of the Fourth Conference on Machine Translation (Volume 2: Shared",
      "citeRegEx" : "Malmasi et al\\.,? 2019",
      "shortCiteRegEx" : "Malmasi et al\\.",
      "year" : 2019
    }, {
      "title" : "The University of Edinburgh’s Submissions to the WMT19 News Translation Task",
      "author" : [ "Rachel Bawden", "Nikolay Bogoychev", "Ulrich Germann", "Roman Grundkiewicz", "Faheem Kirefu", "Antonio Valerio Miceli Barone", "Alexandra Birch." ],
      "venue" : "Proceedings of",
      "citeRegEx" : "Bawden et al\\.,? 2019",
      "shortCiteRegEx" : "Bawden et al\\.",
      "year" : 2019
    }, {
      "title" : "GTCOM Neural Machine Translation Systems for WMT19",
      "author" : [ "Chao Bei", "Hao Zong", "Conghu Yuan", "Qingming Liu", "Baoyong Fan." ],
      "venue" : "Proceedings of the Fourth Conference on Machine Translation (Volume 2: Shared Task Papers, Day 1), pages 116–",
      "citeRegEx" : "Bei et al\\.,? 2019",
      "shortCiteRegEx" : "Bei et al\\.",
      "year" : 2019
    }, {
      "title" : "Findings of the 2018 Conference on Machine Translation (WMT18)",
      "author" : [ "Ondřej Bojar", "Christian Federmann", "Mark Fishel", "Yvette Graham", "Barry Haddow", "Philipp Koehn", "Christof Monz." ],
      "venue" : "Proceedings of the Third Conference on Machine Trans-",
      "citeRegEx" : "Bojar et al\\.,? 2018",
      "shortCiteRegEx" : "Bojar et al\\.",
      "year" : 2018
    }, {
      "title" : "LIUM’s Contributions to the WMT2019 News Translation Task: Data and Systems for German-French Language Pairs",
      "author" : [ "Fethi Bougares", "Jane Wottawa", "Anne Baillot", "Loı̈c Barrault", "Adrien Bardet" ],
      "venue" : "In Proceedings of the Fourth Conference on Machine",
      "citeRegEx" : "Bougares et al\\.,? \\Q2019\\E",
      "shortCiteRegEx" : "Bougares et al\\.",
      "year" : 2019
    }, {
      "title" : "Prediction, Learning and Games",
      "author" : [ "N. Cesa-Bianchi", "G. Lugosi." ],
      "venue" : "Cambridge University Press.",
      "citeRegEx" : "Cesa.Bianchi and Lugosi.,? 2006",
      "shortCiteRegEx" : "Cesa.Bianchi and Lugosi.",
      "year" : 2006
    }, {
      "title" : "NICT’s Supervised Neural Machine Translation Systems for the WMT19 News Translation Task",
      "author" : [ "Raj Dabre", "Kehai Chen", "Benjamin Marie", "Rui Wang", "Atsushi Fujita", "Masao Utiyama", "Eiichiro Sumita." ],
      "venue" : "Proceedings of the Fourth Conference on",
      "citeRegEx" : "Dabre et al\\.,? 2019",
      "shortCiteRegEx" : "Dabre et al\\.",
      "year" : 2019
    }, {
      "title" : "Learning from post-editing: Online model adaptation for statistical machine translation",
      "author" : [ "Michael Denkowski", "Chris Dyer", "Alon Lavie." ],
      "venue" : "Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics",
      "citeRegEx" : "Denkowski et al\\.,? 2014",
      "shortCiteRegEx" : "Denkowski et al\\.",
      "year" : 2014
    }, {
      "title" : "Experts, Errors, and Context: A Large-Scale Study of Human Evaluation for Machine Translation",
      "author" : [ "Markus Freitag", "George Foster", "David Grangier", "Viresh Ratnakar", "Qijun Tan", "Wolfgang Macherey" ],
      "venue" : null,
      "citeRegEx" : "Freitag et al\\.,? \\Q2021\\E",
      "shortCiteRegEx" : "Freitag et al\\.",
      "year" : 2021
    }, {
      "title" : "Continuous measurement scales in human evaluation of machine translation",
      "author" : [ "Yvette Graham", "Timothy Baldwin", "Alistair Moffat", "Justin Zobel." ],
      "venue" : "Proceedings of the 7th Linguistic Annotation Workshop and Interoperability with Discourse, pages 33–41,",
      "citeRegEx" : "Graham et al\\.,? 2013",
      "shortCiteRegEx" : "Graham et al\\.",
      "year" : 2013
    }, {
      "title" : "Microsoft Translator at WMT 2019: Towards Large-Scale DocumentLevel Neural Machine Translation",
      "author" : [ "Marcin Junczys-Dowmunt." ],
      "venue" : "Proceedings of the Fourth Conference on Machine Translation (Volume 2: Shared Task Papers, Day 1), pages 225–",
      "citeRegEx" : "Junczys.Dowmunt.,? 2019",
      "shortCiteRegEx" : "Junczys.Dowmunt.",
      "year" : 2019
    }, {
      "title" : "A user-study on online adaptation of neural machine translation to human post-edits",
      "author" : [ "Sariya Karimova", "Patrick Simianer", "Stefan Riezler." ],
      "venue" : "Machine Translation, 32(4):309–324.",
      "citeRegEx" : "Karimova et al\\.,? 2018",
      "shortCiteRegEx" : "Karimova et al\\.",
      "year" : 2018
    }, {
      "title" : "Manual and automatic evaluation of machine translation between European languages",
      "author" : [ "Philipp Koehn", "Christof Monz." ],
      "venue" : "Proceedings on the Workshop on Statistical Machine Translation, pages 102– 121, New York City. Association for Computational",
      "citeRegEx" : "Koehn and Monz.,? 2006",
      "shortCiteRegEx" : "Koehn and Monz.",
      "year" : 2006
    }, {
      "title" : "Asymptotically efficient adaptive allocation rules",
      "author" : [ "T.L. Lai", "Herbert Robbins." ],
      "venue" : "Advances in Applied Mathematics, 6(1):4–22.",
      "citeRegEx" : "Lai and Robbins.,? 1985",
      "shortCiteRegEx" : "Lai and Robbins.",
      "year" : 1985
    }, {
      "title" : "A reinforcement learning approach to interactivepredictive neural machine translation",
      "author" : [ "Tsz Kin Lam", "Julia Kreutzer", "Stefan Riezler" ],
      "venue" : null,
      "citeRegEx" : "Lam et al\\.,? \\Q2018\\E",
      "shortCiteRegEx" : "Lam et al\\.",
      "year" : 2018
    }, {
      "title" : "The NiuTrans Machine Translation",
      "author" : [ "Bei Li", "Yinqiao Li", "Chen Xu", "Ye Lin", "Jiqiang Liu", "Hui Liu", "Ziyang Wang", "Yuhao Zhang", "Nuo Xu", "Zeyang Wang", "Kai Feng", "Hexuan Chen", "Tengbo Liu", "Yanyang Li", "Qiang Wang", "Tong Xiao", "Jingbo Zhu" ],
      "venue" : null,
      "citeRegEx" : "Li et al\\.,? \\Q2019\\E",
      "shortCiteRegEx" : "Li et al\\.",
      "year" : 2019
    }, {
      "title" : "Multidimensional quality metrics (MQM): A framework for declaring and describing translation quality metrics",
      "author" : [ "Arle Lommel", "Aljoscha Burchardt", "Hans Uszkoreit." ],
      "venue" : "Tradumàtica: tecnologies de la traducció, 0:455–463.",
      "citeRegEx" : "Lommel et al\\.,? 2014",
      "shortCiteRegEx" : "Lommel et al\\.",
      "year" : 2014
    }, {
      "title" : "Results of the WMT19 metrics shared task: Segment-level and strong MT systems pose big challenges",
      "author" : [ "Qingsong Ma", "Johnny Wei", "Ondřej Bojar", "Yvette Graham." ],
      "venue" : "Proceedings of the Fourth Conference on Machine Translation (Volume",
      "citeRegEx" : "Ma et al\\.,? 2019",
      "shortCiteRegEx" : "Ma et al\\.",
      "year" : 2019
    }, {
      "title" : "Tangled up in BLEU: Reevaluating the evaluation of automatic machine translation evaluation metrics",
      "author" : [ "Nitika Mathur", "Timothy Baldwin", "Trevor Cohn." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics,",
      "citeRegEx" : "Mathur et al\\.,? 2020",
      "shortCiteRegEx" : "Mathur et al\\.",
      "year" : 2020
    }, {
      "title" : "Online Learning Approaches in Computer Assisted Translation",
      "author" : [ "Prashant Mathur", "Mauro Cettolo", "Marcello Federico." ],
      "venue" : "Proceedings of the Eighth Workshop on Statistical Machine Translation, pages 301–308.",
      "citeRegEx" : "Mathur et al\\.,? 2013",
      "shortCiteRegEx" : "Mathur et al\\.",
      "year" : 2013
    }, {
      "title" : "Machine Translation System Selection from Bandit Feedback",
      "author" : [ "Jason Naradowsky", "Xuan Zhang", "Kevin Duh." ],
      "venue" : "Proceedings of the 14th Conference of the Association for Machine Translation in the Americas, pages 50–63.",
      "citeRegEx" : "Naradowsky et al\\.,? 2020",
      "shortCiteRegEx" : "Naradowsky et al\\.",
      "year" : 2020
    }, {
      "title" : "Facebook FAIR’s WMT19 News Translation Task Submission",
      "author" : [ "Nathan Ng", "Kyra Yee", "Alexei Baevski", "Myle Ott", "Michael Auli", "Sergey Edunov." ],
      "venue" : "Proceedings of the Fourth Conference on Machine Translation (Volume 2: Shared Task Papers,",
      "citeRegEx" : "Ng et al\\.,? 2019",
      "shortCiteRegEx" : "Ng et al\\.",
      "year" : 2019
    }, {
      "title" : "Reinforcement Learning for Bandit Neural Machine Translation with Simulated Human Feedback",
      "author" : [ "Khanh Nguyen", "Hal Daumé III", "Jordan BoydGraber." ],
      "venue" : "Proceedings of the 2017 Conference on Empirical Methods in Natural Language Process-",
      "citeRegEx" : "Nguyen et al\\.,? 2017",
      "shortCiteRegEx" : "Nguyen et al\\.",
      "year" : 2017
    }, {
      "title" : "eTranslation’s Submissions to the WMT 2019 News Translation Task",
      "author" : [ "Csaba Oravecz", "Katina Bontcheva", "Adrien Lardilleux", "László Tihanyi", "Andreas Eisele." ],
      "venue" : "Proceedings of the Fourth Conference on Machine Translation (Volume 2: Shared Task Pa-",
      "citeRegEx" : "Oravecz et al\\.,? 2019",
      "shortCiteRegEx" : "Oravecz et al\\.",
      "year" : 2019
    }, {
      "title" : "Online Learning for Statistical Machine Translation",
      "author" : [ "Daniel Ortiz-Martı́nez" ],
      "venue" : "Computational Linguistics,",
      "citeRegEx" : "Ortiz.Martı́nez.,? \\Q2016\\E",
      "shortCiteRegEx" : "Ortiz.Martı́nez.",
      "year" : 2016
    }, {
      "title" : "Bleu: a method for automatic evaluation of machine translation",
      "author" : [ "Kishore Papineni", "Salim Roukos", "Todd Ward", "WeiJing Zhu." ],
      "venue" : "Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics, pages 311–318, Philadelphia,",
      "citeRegEx" : "Papineni et al\\.,? 2002",
      "shortCiteRegEx" : "Papineni et al\\.",
      "year" : 2002
    }, {
      "title" : "Online learning for effort reduction in interactive neural machine translation",
      "author" : [ "Álvaro Peris", "Francisco Casacuberta." ],
      "venue" : "Computer Speech and Language, 58:98–126.",
      "citeRegEx" : "Peris and Casacuberta.,? 2019",
      "shortCiteRegEx" : "Peris and Casacuberta.",
      "year" : 2019
    }, {
      "title" : "Tilde’s Machine Translation Systems for WMT 2019",
      "author" : [ "Marcis Pinnis", "Rihards Krišlauks", "Matı̄ss Rikters" ],
      "venue" : "In Proceedings of the Fourth Conference on Machine Translation",
      "citeRegEx" : "Pinnis et al\\.,? \\Q2019\\E",
      "shortCiteRegEx" : "Pinnis et al\\.",
      "year" : 2019
    }, {
      "title" : "COMET: A neural framework for MT evaluation",
      "author" : [ "Ricardo Rei", "Craig Stewart", "Ana C Farinha", "Alon Lavie." ],
      "venue" : "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 2685–2702, Online. Associa-",
      "citeRegEx" : "Rei et al\\.,? 2020a",
      "shortCiteRegEx" : "Rei et al\\.",
      "year" : 2020
    }, {
      "title" : "Unbabel’s participation in the WMT20 metrics shared task",
      "author" : [ "Ricardo Rei", "Craig Stewart", "Ana C Farinha", "Alon Lavie." ],
      "venue" : "Proceedings of the Fifth Conference on Machine Translation, pages 911–920, Online. Association for Computational",
      "citeRegEx" : "Rei et al\\.,? 2020b",
      "shortCiteRegEx" : "Rei et al\\.",
      "year" : 2020
    }, {
      "title" : "Some Aspects of the Sequential Design of Experiments",
      "author" : [ "Herbert Robbins." ],
      "venue" : "Bulletin of the American Mathematical Society, 58(5):527–535.",
      "citeRegEx" : "Robbins.,? 1952",
      "shortCiteRegEx" : "Robbins.",
      "year" : 1952
    }, {
      "title" : "Learning structured predictors from bandit feedback for interactive NLP",
      "author" : [ "Artem Sokolov", "Julia Kreutzer", "Christopher Lo", "Stefan Riezler." ],
      "venue" : "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 1610–",
      "citeRegEx" : "Sokolov et al\\.,? 2016",
      "shortCiteRegEx" : "Sokolov et al\\.",
      "year" : 2016
    }, {
      "title" : "A Shared Task on Bandit Learning for Machine Translation",
      "author" : [ "Artem Sokolov", "Julia Kreutzer", "Kellen Sunderland", "Pavel Danchenko", "Witold Szymaniak", "Hagen Fürstenau", "Stefan Riezler." ],
      "venue" : "Proceedings of the Conference on Machine Translation",
      "citeRegEx" : "Sokolov et al\\.,? 2017",
      "shortCiteRegEx" : "Sokolov et al\\.",
      "year" : 2017
    }, {
      "title" : "Continuous Learning from Human Post-Edits for Neural Machine Translation",
      "author" : [ "Marco Turchi", "Matteo Negri", "M. Amin Farajian", "Marcello Federico." ],
      "venue" : "The Prague Bulletin of Mathematical Linguistics, 108(1):233–244.",
      "citeRegEx" : "Turchi et al\\.,? 2017",
      "shortCiteRegEx" : "Turchi et al\\.",
      "year" : 2017
    }, {
      "title" : "Attention is all you need",
      "author" : [ "Ashish Vaswani", "Noam Shazeer", "Niki Parmar", "Jakob Uszkoreit", "Llion Jones", "Aidan N. Gomez", "Łukasz Kaiser", "Illia Polosukhin." ],
      "venue" : "31st Conference on Neural Information Processing Systems (NIPS 2017), pages 5999–6009.",
      "citeRegEx" : "Vaswani et al\\.,? 2017",
      "shortCiteRegEx" : "Vaswani et al\\.",
      "year" : 2017
    }, {
      "title" : "Microsoft Research Asia’s Systems for WMT19",
      "author" : [ "Yingce Xia", "Xu Tan", "Fei Tian", "Fei Gao", "Weicong Chen", "Yang Fan", "Linyuan Gong", "Yichong Leng", "Renqian Luo", "Yiren Wang", "Lijun Wu", "Jinhua Zhu", "Tao Qin", "Tie-Yan Liu." ],
      "venue" : "Proceedings of the",
      "citeRegEx" : "Xia et al\\.,? 2019",
      "shortCiteRegEx" : "Xia et al\\.",
      "year" : 2019
    } ],
    "referenceMentions" : [ {
      "referenceID" : 27,
      "context" : "Automatic metrics like BLEU (Papineni et al., 2002) remain popular due to their fast and free computations.",
      "startOffset" : 28,
      "endOffset" : 51
    }, {
      "referenceID" : 19,
      "context" : "Yet, in the last few years we have seen that, as MT quality improves, automatic metrics become less reliable (Ma et al., 2019; Mathur et al., 2020).",
      "startOffset" : 109,
      "endOffset" : 147
    }, {
      "referenceID" : 20,
      "context" : "Yet, in the last few years we have seen that, as MT quality improves, automatic metrics become less reliable (Ma et al., 2019; Mathur et al., 2020).",
      "startOffset" : 109,
      "endOffset" : 147
    }, {
      "referenceID" : 7,
      "context" : "performance guarantees, under the frameworks of prediction with expert advice (Cesa-Bianchi and Lugosi, 2006) and multi-armed bandits (Robbins, 1952; Lai and Robbins, 1985).",
      "startOffset" : 78,
      "endOffset" : 109
    }, {
      "referenceID" : 32,
      "context" : "performance guarantees, under the frameworks of prediction with expert advice (Cesa-Bianchi and Lugosi, 2006) and multi-armed bandits (Robbins, 1952; Lai and Robbins, 1985).",
      "startOffset" : 134,
      "endOffset" : 172
    }, {
      "referenceID" : 15,
      "context" : "performance guarantees, under the frameworks of prediction with expert advice (Cesa-Bianchi and Lugosi, 2006) and multi-armed bandits (Robbins, 1952; Lai and Robbins, 1985).",
      "startOffset" : 134,
      "endOffset" : 172
    }, {
      "referenceID" : 7,
      "context" : "3106 caster and the environment, in which the forecaster seeks advice from different sources (experts) in order to provide the best forecast (Cesa-Bianchi and Lugosi, 2006).",
      "startOffset" : 141,
      "endOffset" : 172
    }, {
      "referenceID" : 7,
      "context" : "8 log J T , it can be shown that the forecaster quickly converges to the performance of the best expert after T iterations (Cesa-Bianchi and Lugosi, 2006).",
      "startOffset" : 123,
      "endOffset" : 154
    }, {
      "referenceID" : 32,
      "context" : "Thus, we consider a related class of problems, multi-armed bandits, in which the environment’s outcome is unknown (Robbins, 1952; Lai and Robbins, 1985).",
      "startOffset" : 114,
      "endOffset" : 152
    }, {
      "referenceID" : 15,
      "context" : "Thus, we consider a related class of problems, multi-armed bandits, in which the environment’s outcome is unknown (Robbins, 1952; Lai and Robbins, 1985).",
      "startOffset" : 114,
      "endOffset" : 152
    }, {
      "referenceID" : 0,
      "context" : "A popular online algorithm for adversarial multiarmed bandits is Exponential-weighting for Exploration and Exploitation (EXP3) (Auer et al., 1995).",
      "startOffset" : 127,
      "endOffset" : 146
    }, {
      "referenceID" : 30,
      "context" : "• human-comet: If there is no human assessment for the current translation, the COMET score (Rei et al., 2020a) between the translation and the pair source/reference available in the corpus is returned as the current score.",
      "startOffset" : 92,
      "endOffset" : 111
    }, {
      "referenceID" : 14,
      "context" : "Every year, since 2006, the Conference on Machine Translation (WMT) is responsible for organizing several shared tasks where participants push the limits of MT and MT evaluation (Koehn and Monz, 2006; Barrault et al., 2020).",
      "startOffset" : 178,
      "endOffset" : 223
    }, {
      "referenceID" : 11,
      "context" : "puts of their systems that are then evaluated by a community of human evaluators using Direct Assessment scores (Graham et al., 2013).",
      "startOffset" : 112,
      "endOffset" : 133
    }, {
      "referenceID" : 36,
      "context" : ", 2019), most of the competing systems followed a Transformer architecture (Vaswani et al., 2017), with the main differences among them being: (i) whether they considered document-level or only sentencelevel information; (ii) whether they were trained only on the training data provided by the shared task, or on additional sources as well; (iii) whether they consisted of a single model or an ensemble.",
      "startOffset" : 75,
      "endOffset" : 97
    }, {
      "referenceID" : 21,
      "context" : "However, most approaches aim at learning the parameters or feature weights of an MT model (Mathur et al., 2013; Denkowski et al., 2014; OrtizMartı́nez, 2016; Sokolov et al., 2016; Nguyen et al., 2017; Lam et al., 2018) or fine-tuning a pretrained model for domain adaptation (Turchi et al.",
      "startOffset" : 90,
      "endOffset" : 218
    }, {
      "referenceID" : 9,
      "context" : "However, most approaches aim at learning the parameters or feature weights of an MT model (Mathur et al., 2013; Denkowski et al., 2014; OrtizMartı́nez, 2016; Sokolov et al., 2016; Nguyen et al., 2017; Lam et al., 2018) or fine-tuning a pretrained model for domain adaptation (Turchi et al.",
      "startOffset" : 90,
      "endOffset" : 218
    }, {
      "referenceID" : 33,
      "context" : "However, most approaches aim at learning the parameters or feature weights of an MT model (Mathur et al., 2013; Denkowski et al., 2014; OrtizMartı́nez, 2016; Sokolov et al., 2016; Nguyen et al., 2017; Lam et al., 2018) or fine-tuning a pretrained model for domain adaptation (Turchi et al.",
      "startOffset" : 90,
      "endOffset" : 218
    }, {
      "referenceID" : 24,
      "context" : "However, most approaches aim at learning the parameters or feature weights of an MT model (Mathur et al., 2013; Denkowski et al., 2014; OrtizMartı́nez, 2016; Sokolov et al., 2016; Nguyen et al., 2017; Lam et al., 2018) or fine-tuning a pretrained model for domain adaptation (Turchi et al.",
      "startOffset" : 90,
      "endOffset" : 218
    }, {
      "referenceID" : 16,
      "context" : "However, most approaches aim at learning the parameters or feature weights of an MT model (Mathur et al., 2013; Denkowski et al., 2014; OrtizMartı́nez, 2016; Sokolov et al., 2016; Nguyen et al., 2017; Lam et al., 2018) or fine-tuning a pretrained model for domain adaptation (Turchi et al.",
      "startOffset" : 90,
      "endOffset" : 218
    }, {
      "referenceID" : 35,
      "context" : ", 2018) or fine-tuning a pretrained model for domain adaptation (Turchi et al., 2017; Karimova et al., 2018; Peris and Casacuberta, 2019).",
      "startOffset" : 64,
      "endOffset" : 137
    }, {
      "referenceID" : 13,
      "context" : ", 2018) or fine-tuning a pretrained model for domain adaptation (Turchi et al., 2017; Karimova et al., 2018; Peris and Casacuberta, 2019).",
      "startOffset" : 64,
      "endOffset" : 137
    }, {
      "referenceID" : 28,
      "context" : ", 2018) or fine-tuning a pretrained model for domain adaptation (Turchi et al., 2017; Karimova et al., 2018; Peris and Casacuberta, 2019).",
      "startOffset" : 64,
      "endOffset" : 137
    }, {
      "referenceID" : 34,
      "context" : "task on online bandit learning for MT (Sokolov et al., 2017), as well as Lam et al.",
      "startOffset" : 38,
      "endOffset" : 60
    }, {
      "referenceID" : 18,
      "context" : "For future work, we plan to combine online learning with a more reliable human metric, such as the Multidimensional Quality Metric (MQM) (Lommel et al., 2014), so that we can focus on the quality of the assessments instead of their quantity.",
      "startOffset" : 137,
      "endOffset" : 158
    } ],
    "year" : 2021,
    "abstractText" : "In Machine Translation, assessing the quality of a large amount of automatic translations can be challenging. Automatic metrics are not reliable when it comes to high performing systems. In addition, resorting to human evaluators can be expensive, especially when evaluating multiple systems. To overcome the latter challenge, we propose a novel application of online learning that, given an ensemble of Machine Translation systems, dynamically converges to the best systems, by taking advantage of the human feedback available. Our experiments on WMT’19 datasets show that our online approach quickly converges to the top-3 ranked systems for the language pairs considered, despite the lack of human feedback for many translations.",
    "creator" : "LaTeX with hyperref"
  }
}