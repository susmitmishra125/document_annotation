{
  "name" : "2021.acl-long.218.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "A Large-Scale Chinese Multimodal NER Dataset with Speech Clues",
    "authors" : [ "Dianbo Sui", "Zhengkun Tian", "Yubo Chen", "Kang Liu", "Jun Zhao", "Daqiao Jiang" ],
    "emails" : [ "jzhao}@nlpr.ia.ac.cn" ],
    "sections" : [ {
      "heading" : null,
      "text" : "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 2807–2818\nAugust 1–6, 2021. ©2021 Association for Computational Linguistics\n2807"
    }, {
      "heading" : "1 Introduction",
      "text" : "“Speech is a part of thought.” — Oliver Sacks, Seeing Voices\nAs a fundamental subtask of information extraction, named entity recognition (NER) aims to locate and classify named entities mentioned in unstructured texts into predefined semantic categories, such as person names, locations and organizations. NER plays a crucial role in many natural language processing (NLP) tasks, including relation extraction (Zelenko et al., 2003), question answering (Mollá et al., 2006) and summarization (Aramaki et al., 2009).\nMost of the research on NER, such as Lample et al. (2016); Ma and Hovy (2016); Chiu and Nichols (2016), only relies on the textual modality to infer tags. However, when texts are noisy or short, and it is not sufficient to locate and classify named entities accurately only based on textual information (Baldwin et al., 2015; Lu et al., 2018). One promising solution is to introduce other modalities as the supplement of the textual modality. So far, some studies on multimodal NER, such as Moon et al. (2018); Zhang et al. (2018); Lu et al. (2018); Arshad et al. (2019); Asgari-Chenaghlu et al. (2020); Yu et al. (2020); Chen et al. (2020); Sun et al. (2020), have attempted to couple the textual modality with the visual modality and witnessed a stable improvement.\nIn this work, we also focus on multimodal NER. But differently from previous studies, we pay special attention to Chinese multimodal NER with both textual and acoustic contents. The motivation comes from two aspects:\nFirst, despite much recent success in multimodal NER, current studies on this topic are limited in English, and totally skirt other languages. Meanwhile, previous work on Chinese NER, such as Xu et al. (2013); Peng and Dredze (2016a); Zhang and Yang (2018); Cao et al. (2018); Sui et al. (2019); Gui et al. (2019); Ma et al. (2020); Li et al. (2020), totally ignores valuable multimodal information. With around 1.3 billion native speakers and the wide spread of short-form video apps in China, it is necessary and urgent to carry out research on Chinese multimodal NER.\nSecond, unlike the static visual modality, the time-varying acoustic modality plays a unique role in Chinese NER, especially in providing precise word segmentation information. In detail, different from English, Chinese is an ideographic language featured by no word delimiter between words in written. This language characteristic is one of the major roadblocks in Chinese NER, since named entity boundaries are usually word boundaries (Zhang and Yang, 2018). Fortunately, cues contained in the fluent acoustic modality, especially pauses between adjacent words, are able to aid the NER model in discovering word boundaries. A classic example shown in Figure 1 can perfectly illustrate this point. In this example, the sentence with ambiguous word segmentation would be disambiguated with the aid of the acoustic modality, which would absolutely assist the model to infer correct NER tags.\nIn this work, we make the following efforts to advance multimodal NER:\nFirst, we construct a large-scale humanannotated Chinese NER dataset with Textual and Acoustic contents, named CNERTA. Specifically, we annotate all occurrences of 3 entity types (person name, location and organization) in 42,987 sentences originating from the transcripts of Aishell-1 (Bu et al., 2017), a corpus that has been widely employed in Mandarin speech recognition research in recent years (Shan et al., 2019; Li et al., 2019; Tian et al., 2020). In particular, unlike previous multimodal NER datasets (Moon et al., 2018; Zhang et al., 2018; Lu et al., 2018) are all flatly annotated, not only the topmost entities but also nested entities are annotated in CNERTA.\nSecond, based on CNERTA, we establish a family of strong and representative baselines. In detail, we first investigate the performance of several classic text-only models on our dataset, including BiLSTM-CRF (Lample et al., 2016) and BERT-\nCRF (Devlin et al., 2019). Then, since introducing a lexicon has been proven as an effective way to incorporate word information in Chinese NER (Zhang and Yang, 2018), we implement several lexicon-enhanced models, such as Lattice-LSTM (Zhang and Yang, 2018) and ZEN (Diao et al., 2020), to explore whether the acoustic modality can provide word information beyond the lexicon. Finally, to verify the effectiveness of introducing the acoustic modality, we test some widely used multimodal models, such as CMA (Tsai et al., 2019) and MMI (Yu et al., 2020), on our dataset.\nThird, upon these strong baselines, we further propose a simple Multi-Modal Multi-Task model (short for M3T) to make better use of the pause information in the acoustic modality. Specifically, different from coupling the visual modality with the textual modality, there is a monotonic alignment between the acoustic modality and the textual modality. Armed with such an alignment, the position of each Chinese character in the continuous speech would be determined, which would make it easy to discover pauses between adjacent words. Therefore, to automatically estimate this desired alignment, we introduce a speech-to-text alignment auxiliary task and propose a hybrid CTC/Tagging loss. In the hybrid loss, a masked CTC loss (Graves et al., 2006) is designed for enforcing a monotonic alignment between speech and text sequences.\nThe primary contributions of this work can be summarized as follows:\n• We construct CNERTA, the first humanannotated Chinese multimodal NER dataset, where each annotated sentence is paired with its corresponding speech data. To our best knowledge, this dataset is not only the largest multimodal NER dataset, but also the largest Chinese nested NER dataset.\n• We establish a family of baselines to leverage textual features or multimodal features. Through various experiments, we observe consistent performance boosts originating from acoustic features, which verifies the significant merits of integrating acoustic features for Chinese NER.\n• We further propose a multimodal multitask method by introducing a speech-to-text alignment auxiliary task. By jointly solving the tagging task and the alignment task, the proposed method can yield SoTA results on CNERTA."
    }, {
      "heading" : "2 Related Work",
      "text" : "Mutlimodal NER: As multimedia technology evolves, processing multimodal data is becoming a burning issue. As a basic NLP tool, multimodal NER attracts increasing attention in recent years. Most of studies on multimodal NER focus on leveraging the associate images to better identify the named entities contained in the text. Specifically, Moon et al. (2018) propose a multimodal NER network with modality attention to fuse textual and visual information. To model inter-modal interactions and filter out the noise in the visual context, Zhang et al. (2018) propose an adaptive co-attention network and a gated visual attention mechanism for multimodal NER. As transformer-based models (Vaswani et al., 2017; Devlin et al., 2019) become the mainstream method in NLP, researchers turn to study how to fuse visual clues in transformers structure. Chen et al. (2020) use captions to represent images as text and adopt transformer-based sequence labeling models to connect multimodal information. Yu et al. (2020) propose a Multimodal Transformer model, which empowers transformer with a multimodal interaction module to capture the inter-modality dynamics between words and images. But different from them, we aim to explore an unexplored territory in this work, which is Chinese multimodal NER with both speech and textual contents.\nChinese NER: Compared with English NER, Chinese NER is more complicated since the written text in Chinese is not naturally segmented. Therefore, how to incorporate word information is the key challenge in Chinese NER. There are three main ways to fuse word information in Chinese NER. The first one is the pipeline method. In the pipeline method, Chinese word segmentation (CWS) is first applied and then a word-based NER model is used. The second one is to learn CWS and NER tasks jointly (Xu et al., 2013; Peng and Dredze, 2016b; Cao et al., 2018; Wu et al., 2019). In such a way, the word boundary information in the CWS task can be transferred to the NER model. The third one is to resort to an automatically constructed lexicon (Zhang and Yang, 2018; Ding et al., 2019; Liu et al., 2019a; Sui et al., 2019; Gui et al., 2019; Li et al., 2020; Ma et al., 2020; Xue et al., 2020). Different from all previous studies, we focus on use speech clues to incorporate word information in Chinese NER."
    }, {
      "heading" : "3 Dataset Acquisition and Comparison",
      "text" : "In this work, we aim to explore Chinese NER with both speech and textual clues. But we are not aware of any such existing corpus, hence we are motivated to collect one. In this section, we will discuss the data acquisition process, subsequently present statistics of the dataset and compare the annotated dataset with other widely-used NER datasets."
    }, {
      "heading" : "3.1 Dataset Acquisition",
      "text" : "The main challenge in data acquisition is to find a large-scale dataset, which includes texts and the corresponding speech data. One possible way is to attach speech data to current existing Chinese NER datasets. However, it is costly to gather hundreds of participants in the recording. Therefore, we take a different way, manually annotating NER tags on a speech recognition dataset from scratch. In detail, our annotated dataset is based on Aishell1 (Bu et al., 2017) dataset, which is a large-scale Mandarin automatic speech recognition dataset. In this dataset, text transcriptions are chosen from five domains: “Finance”, “Science and Technology”, “Sport”, “Entertainments” and “News”. There are 400 participants in the recording, and the gender of participants is balanced with 47% male and 53% female. Speech utterances are recorded via three categories of devices in parallel, which are a high fidelity microphone working at 44.1 kHz, 16-bit, Android phones working at 16 kHz, 16-bit, and Apple iPhones working at 16 kHz, 16-bit.\nTo ensure the quality of annotation, we design two rounds in the annotation procedure. In the first\nDataset # Train # Dev # Test # Total Language Structure Modality\nMSRA 46,364 - 4,365 50,729 Chinese Flat Text OntoNotes 15,724 4301 4,346 24,371 Chinese Flat Text\nWeibo NER 1,350 271 270 1,891 Chinese Flat Text Resume 3,821 463 477 4,761 Chinese Flat Text\nGENIA 15,022 1,669 1,854 18,545 English Nested Text JNLPBA 20,546 - 4,260 24,806 English Nested Text ACE-2004 6,198 742 809 7,749 English Nested Text ACE-2005 7,285 968 1,058 9,311 English Nested Text\nTwitter-2015 4,000 1,000 3,257 8,257 English Flat Text + Image Twitter-2017 3,373 723 723 4,819 English Flat Text + Image\nround, we use Brat (Stenetorp et al., 2012) as the annotation tool and ask 3 internal annotators (including the first author of this paper) to perform annotation, who are very familiar with this task. They independently identify and classify named entities in the transcriptions with more than 17 characters. Cohen’s kappa coefficient (Cohen, 1960) is used to measure the inter-annotator agreements. After the first round, κ = 0.965, which shows the quality of CNERTA is satisfactory. But there are still some sentences for which annotators give out different annotations. For those sentences, the annotators check the disagreed annotations carefully and discuss to reach the agreements for all cases.\nAfter we finish the annotation process, we split the dataset into three parts: training, development, and test set. Table 1 shows the high level statistics of data splits for CNERTA."
    }, {
      "heading" : "3.2 Dataset Comparison",
      "text" : "We compare CNERTA with several widely used NER datasets in Table 2. Specifically, we first compare our corpus with some Chinese NER datasets, such as MSRA (Levow, 2006), OntoNotes (Weischedel et al., 2011), Weibo NER (Peng and Dredze, 2016a) and Resume (Zhang and Yang, 2018). Then, we compare our corpus with several widely used nested NER datasets, like GENIA (Kim et al., 2003), JNLPBA (Collier and Kim, 2004), ACE-2004 (Doddington et al., 2004) and ACE-2005 (Walker et al., 2004). Finally, multimodal NER datasets, including Twitter-2015 (Zhang et al., 2018) and Twitter-2017 (Lu et al., 2018), are compared with our corpus.\nFrom Table 2, we observe that our corpus has unique value compared with the existing datasets. The value is reflected in the following aspects: (1) CNERTA is a large-scale dataset; (2) CNERTA is the first Chinese multimodal dataset; (3) Not only the topmost entities but also nested entities are annotated; (4) Among these datasets, the acoustic modality is only introduced in CNERTA."
    }, {
      "heading" : "4 Preliminaries",
      "text" : ""
    }, {
      "heading" : "4.1 Task Description",
      "text" : "Given a text X = x1, x2, ..., xn and its corresponding speech S = s1, s2, ..., st, where xi denotes the i-th Chinese character and sj denotes the j-th waveform frame, the goal of the task is to leverage textual and speech clues to identify and classify all named entities contained in the text."
    }, {
      "heading" : "4.2 Nested Structure Linearization",
      "text" : "Unlike flat NER, named entities may overlap and also be labeled with more than one label in nested NER. To solve nested NER, we follow Straková et al. (2019) to encode the nested entity structure into a CoNLL-like, per-character BIO encoding (Ramshaw and Marcus, 1995). There are two rules to guide the linearization: (1) entity mentions starting earlier have priority over entities starting later, and (2) for mentions with the same beginning, longer entity mentions have priority over shorter ones. A multilabel for a given Chinese character is a concatenation of all intersecting entity mentions, from the highest priority to the lowest. For more details, we refer readers to Straková et al. (2019)."
    }, {
      "heading" : "4.3 Acoustic Encoder",
      "text" : "The acoustic encoder is used to map raw speech signals into continuous space. There are three parts in the proposed acoustic encoder: a speech processing layer, a convolution front end and a transformerbased encoder.\nSpecifically, in the speech processing layer, a speech signal first goes through a pre-emphasis filter; then gets sliced into frames and a window function is applied to each frame; afterwards, a Short-Time Fourier transform (Kwok and Jones, 2000) is employed on each frame and the power spectrum is calculated; and subsequently, the filter banks (Ravindran et al., 2003) are computed. Then, we use a convolution front end to down-sample the long acoustic features. In the convolution front end, following Dong et al. (2018); Tian et al. (2020), two 3×3 CNN layers with stride 2 are stacked for both time and frequency dimensions. Afterwards, in order to enable the acoustic encoder to attend by relative positions, the positional encoding is added to the output of the convolution front end. Finally, to effectively capture long-term dependencies, down-sampled acoustic features flow through the transformer-based encoder (Vaswani et al., 2017). The transformer-based encoder is a stack of 6 identical layers, each of which is composed of a self-attention sub-layer and a feedforward network."
    }, {
      "heading" : "5 Baselines",
      "text" : "Based on the annotated dataset, a family of strong and representative baselines is established, including (1) text-only models presented in Section 5.1, (2) lexicon-enhanced models shown in Section 5.2 and (3) multimodal models introduced in Section 5.3."
    }, {
      "heading" : "5.1 Text-Only Model",
      "text" : "Open-Source NLP Toolkit: Many open-source NLP toolkits, such as spaCy (Honnibal et al., 2020) and Stanza (Qi et al., 2020), support Chinese NER. In spaCy, a multitask CNN is employed. In Stanza, a contextualized string representation based tagger from Akbik et al. (2018) is adopted. In both spaCy and Stanza, the tagger is trained on OntoNote (Weischedel et al., 2011). To map the output of taggers to CNERTA’s label space, expert-designed rules are used, such as PERSON → PER. Since these toolkits are only designed for flat structure, we do not evaluate these toolkits in nested settings.\nBiLSTM-CRF: Featured by a bidirectional LSTM (Hochreiter and Schmidhuber, 1997) as the textual encoder and conditional random fields (CRF) (Lafferty et al., 2001) as the decoder, the widely used BiLSTM-CRF (Lample et al., 2016) is adopted as an important baseline.\nPLM-CRF: Instead of training a model from scratch, we also adopt the framework of fine-tuning a pretrained language model (PLM) on a downstream task (Radford et al., 2018). In this framework, we adopt BERT (Devlin et al., 2019) as the textual encoder and use CRF as the decoder. In addition to initializing the textual encoder with the original pretrained BERT model, a SoTA Chinese pretrained language model, called MacBERT (Cui et al., 2020), is used. Compared with BERT, MacBERT is built upon RoBERTa (Liu et al., 2019b) and the original MLM task in BERT is replaced with the MLM as correction task. For more details, we refer readers to Cui et al. (2020)."
    }, {
      "heading" : "5.2 Lexicon-Enhanced Model:",
      "text" : "A drawback of the text-only methods mentioned above is that explicit word and word sequence information is not fully exploited, which can be potentially useful. With this consideration, we also adopt lexicon-enhance models to incorporate word lexicons. (1) Lattice-LSTM (Zhang and Yang, 2018) is a classic method that can encode a sequence of input characters as well as all potential words that match a lexicon. (2) ZEN (Diao et al., 2020) is a pretrained Chinese text encoder enhanced by an n-gram lexicon. In ZEN, n-gram contexts are extracted, encoded and integrated with the character encoder. For more details about Lattice-LSTM and ZEN, we refer readers to Zhang and Yang (2018) and Diao et al. (2020)."
    }, {
      "heading" : "5.3 Multimodal Model",
      "text" : "To leverage the acoustic modality, several multimodal models are introduced. In these models, fusion modules are built on the top of the acoustic encoder and the textual encoder, which are designed for capturing the interaction between the textual hidden representations X = [x1, x2, ..., xn]; xi ∈ Rd and the acoustic representations S = [s1, s2, ..., st′ ]; sj ∈ Rd. We present two representative fusion modules, which are Cross-Modal Attention (CMA) module (Tsai et al., 2019) and Multimodal Interaction (MMI) module (Yu et al., 2020).\nCross-Modal Attention Module (CMA): Given the textual hidden representations X ∈ Rd×n and the acoustic representations S ∈ Rd×t′ , we first employ a m-head cross-modal attention mechanism (Tsai et al., 2019), by treating X as queries, and S as keys and values:\nCAi(X,S) = softmax( [WqiX]T[WkiS]√\nd/m )[WviS]\nMH-CA(X,S) = W′[CA1(X,S), ...,CAm(X,S)]\nwhere CAi refers to the i-th head of cross-modal attention, and {Wqi ,Wki ,Wvi} ∈ Rd/m×d, W′ ∈ Rd×d denote the weight matrices for the query, key, value and multi-head attention, respectively. Then, we stack the following sub-layers on top:\nF̂ = LN(X + MH-CA(X,S)) F = LN(F̂ + FFN(F̂)) (1)\nwhere LN means layer normalization (Ba et al., 2016) and FFN means a fully connected feedforward network, which consists of two linear transformation with a ReLU activation (Nair and Hinton, 2010). Finally, the new textual representations F ∈ Rd×n, which are enhanced by acoustic features, are fed into the CRF decoder to infer NER tags.\nMultimodal Interaction Module (MMI): A stack of cross-modal attention layer mentioned above makes up the multimodal interaction module. Since the architecture of MMI is too complex and is not the core of this paper, we will not introduce it in the main text. For more details about MMI, we refer readers to Yu et al. (2020)."
    }, {
      "heading" : "6 Proposed Method",
      "text" : "Previous multimodal methods ignore a natural monotonic alignment between the acoustic modality and the textual modality. To capture this alignment, we propose a multimodal multitask model, called M3T. The framework of the proposed method is shown in Figure 2.\nIn the M3T model, we adopt the CMA module to fuse acoustic information into the textual representations. Besides, a CTC project layer is built upon the acoustic encoder, and the loss function is a combination of masked CTC loss and CRF loss. Specifically, through the CTC project layer, each acoustic representation si ∈ Rd is first mapped to the total size of model units (in this paper, the\nmodel unit is the Chinese character) and then is passed through a logit function:\nG = logit(WTv S) (2)\nwhere Wv ∈ Rd×|V | and |V | is the total size of Chinese characters. Unlike automatic speech recognition, only the characters in the given text need to be aligned rather than the entire model units. Therefore, we only keep these rows unchanged, whose corresponding characters are contained in the given text, and fill the other rows in G ∈ R|V |×t′ with the value −∞. The masked tensor G is then fed into CTC loss. Finally, to jointly solve the tagging task and the alignment task, a hybrid loss of combining the masked CTC loss with the CRF loss is used:\nL = Lcrf + λLctc (3)\nwhere λ is a hyperparameter."
    }, {
      "heading" : "7 Experiments",
      "text" : "In this section, we carry out various experiments to investigate the effectiveness of introducing the acoustic modality. In addition, we empirically compare the proposed model and these baselines under different settings. Following previous studies in NER (Zhang and Yang, 2018), standard precision (P), recall (R) and F1-score (F1) are used as evaluation metrics."
    }, {
      "heading" : "7.1 Implementation Details",
      "text" : "LSTM-Based Baselines: We use the 50- dimensional character embeddings, which are pretrained on Chinese Giga-Word * using word2vec (Mikolov et al., 2013). The dimensionality of LSTM hidden states is set to 300 and the initial learning rate is set to 0.001. We train the models using 100 epochs with a batch size of 16.\nLexicon: The lexicon used in Lattice-LSTM is the same as Zhang and Yang (2018) and the lexicon used in ZEN is the same as Diao et al. (2020). Due to low speed in training and inference, we only employ Lattice-LSTM in unimodal settings.\nPretrained Language Model Fine-Tuning: We use the base models of BERT (Devlin et al., 2019), MacBERT (Cui et al., 2020) and ZEN (Diao et al., 2020). The initial learning rate of pretrained language model is set to 1× 10−5. We fine-tune models using 10 epochs with a batch size of 16.\nComputing Infrastructure: All experiments are conducted on an NVIDIA GeForce RTX 2080 Ti (11 GB of memory).\n*https://catalog.ldc.upenn.edu/ LDC2011T13"
    }, {
      "heading" : "7.2 Main Results",
      "text" : "Table 3 shows the results of baselines and our proposed model on CNERTA. From the table, we find:\n(1) Introducing the acoustic modality can significantly boost the performance of the characterbased models, such as BiLSTM-CRF, BERT-CRF and MacBERT-CRF. With the simple CMA module to introduce the acoustic modality, there is a more than 1.6% improvement in both flat NER and nested NER. Furthermore, by using the M3T model to leverage the acoustic modality, a more than 3% improvement can be brought in all cases. These experimental results demonstrate the effectiveness of introducing the acoustic modality in characterbased NER models.\n(2) Introducing the acoustic modality can improve the performance of lexicon-based models, such as ZEN-CRF. By introducing the acoustic modes in ZEN-CRF with the CMA module, the performance in flat NER and nested NER can be improved by 1.38% and 1.73%, respectively. Armed with the M3T model, the performance in flat NER and nested NER can be further improved by 2.93% and 3.19%. Although not as significant as the improvement of the character-based models, these\nresults still prove that the acoustic modality can provide lexicon-based models with some information that does not contain in the large-scale lexicon.\n(3) Our proposed method (M3T) can achieve the SoTA results on CNERTA. Compared with CMA (Tsai et al., 2019) and MMI (Yu et al., 2020), there is a significant improvement. We conjecture that is due to that the monotonic alignment between the acoustic modality and the textual modality is captured by the masked CTC loss and armed with this alignment, precise word boundary information contained in speech is leveraged by the model."
    }, {
      "heading" : "7.3 Error Analysis",
      "text" : "As NER models established here are not yet as accurate as one would hope, some analyses of the errors that occur in the output of NER models are\nperformed. We divide the error into type error and boundary error. The type error is defined as that the boundary of the predicted entity is correct but the predicted type is wrong, and the other errors are classified as boundary errors. The statistics of boundary errors and type errors are shown in Table 5. From the table, we find that: (1) Errors are mainly caused by mistakenly locating boundaries of entities. Therefore, discovering entity boundaries is the main challenge in Chinese NER. (2) Leveraging the acoustic modality can effectively reduce boundary errors. In nested NER, the number of errors decreases from 906 to 848, totally owning to the reduction of boundary errors, but the number of type errors increases, which may be due to overfitting or some random factors."
    }, {
      "heading" : "7.4 Case Studies",
      "text" : "To visually show the effectiveness of introducing the acoustic modality, case studies on comparing the output of BERT-CRF and BERT-M3T are present in Table 4. From the table, we can observe that: without the acoustic modality, BERT-CRF is prone to locate some ambiguous entities mistakenly, such as “沙特阿拉伯” (Saudi Arabia), “首都 机场”(Capital Airport), “国际米兰” (Inter Milan). But armed with the acoustic modality, these entities are located with complete accuracy. In the last case, BERT-M3T makes some mistakes. We listen to the corresponding audio clip and find that there is a long pause between “毕尔” and “巴鄂”."
    }, {
      "heading" : "8 Conclusion and Future Work",
      "text" : "In this paper, we explore Chinese multimodal NER with both textual and acoustic contents. To achieve this, we construct a large-scale manually annotated multimodal NER dataset，named CNERTA. Based on this dataset, we establish a family of baseline models. Furthermore, we propose a simple multimodal multitask method by introducing a speechto-text alignment auxiliary task. Through extensive experiments, we prove that Chinese NER models can benefit from introducing the acoustic modality and our proposed model is effective.\nIn the future, we are interested in mining other information contained in speech, such as rhythm, emotion, pitch, accent and stress, to boost NER. Meanwhile, we will also work on designing some speech-text pretraining tasks for building a largescale pretrained model with multimodal capabilities."
    }, {
      "heading" : "Acknowledgments",
      "text" : "We thank the anonymous reviewers for their insightful comments. We also thank Zhixing Tian, Tao Wang and Ye Bai for helpful suggestions.\nThis work is supported by the National Key Research and Development Program of China (Grant No. 2020AAA0106400), the National Natural Science Foundation of China (Grant No. 61922085 and Grant No. 61976211) and Beijing Academy of Artificial Intelligence (Grant No. BAAI2019QN0301)."
    } ],
    "references" : [ {
      "title" : "Contextual string embeddings for sequence labeling",
      "author" : [ "Alan Akbik", "Duncan Blythe", "Roland Vollgraf." ],
      "venue" : "Proceedings of the 27th International Conference on Computational Linguistics.",
      "citeRegEx" : "Akbik et al\\.,? 2018",
      "shortCiteRegEx" : "Akbik et al\\.",
      "year" : 2018
    }, {
      "title" : "TEXT2TABLE: Medical text summarization system based on named entity recognition and modality identification",
      "author" : [ "Eiji Aramaki", "Yasuhide Miura", "Masatsugu Tonoike", "Tomoko Ohkuma", "Hiroshi Mashuichi", "Kazuhiko Ohe." ],
      "venue" : "Proceedings of the",
      "citeRegEx" : "Aramaki et al\\.,? 2009",
      "shortCiteRegEx" : "Aramaki et al\\.",
      "year" : 2009
    }, {
      "title" : "Aiding intra-text representations with visual context for multimodal named entity recognition",
      "author" : [ "Omer Arshad", "Ignazio Gallo", "Shah Nawaz", "Alessandro Calefati." ],
      "venue" : "2019 International Conference on Document Analysis and Recognition (IC-",
      "citeRegEx" : "Arshad et al\\.,? 2019",
      "shortCiteRegEx" : "Arshad et al\\.",
      "year" : 2019
    }, {
      "title" : "Layer normalization",
      "author" : [ "Jimmy Lei Ba", "Jamie Ryan Kiros", "Geoffrey E Hinton." ],
      "venue" : "arXiv preprint arXiv:1607.06450.",
      "citeRegEx" : "Ba et al\\.,? 2016",
      "shortCiteRegEx" : "Ba et al\\.",
      "year" : 2016
    }, {
      "title" : "Shared tasks of the 2015 workshop on noisy user-generated text: Twitter lexical normalization and named entity recognition",
      "author" : [ "Timothy Baldwin", "Marie Catherine de Marneffe", "Bo Han", "Young-Bum Kim", "Alan Ritter", "Wei Xu." ],
      "venue" : "Proceedings",
      "citeRegEx" : "Baldwin et al\\.,? 2015",
      "shortCiteRegEx" : "Baldwin et al\\.",
      "year" : 2015
    }, {
      "title" : "Aishell-1: An open-source mandarin speech corpus and a speech recognition baseline",
      "author" : [ "Hui Bu", "Jiayu Du", "Xingyu Na", "Bengu Wu", "Hao Zheng." ],
      "venue" : "2017 20th Conference of the Oriental Chapter of the International Coordinating Committee on Speech",
      "citeRegEx" : "Bu et al\\.,? 2017",
      "shortCiteRegEx" : "Bu et al\\.",
      "year" : 2017
    }, {
      "title" : "Adversarial transfer learning for Chinese named entity recognition with selfattention mechanism",
      "author" : [ "Pengfei Cao", "Yubo Chen", "Kang Liu", "Jun Zhao", "Shengping Liu." ],
      "venue" : "Proceedings of the 2018 Conference on Empirical Methods in Natural Lan-",
      "citeRegEx" : "Cao et al\\.,? 2018",
      "shortCiteRegEx" : "Cao et al\\.",
      "year" : 2018
    }, {
      "title" : "A caption is worth a thousand images: Investigating image captions for multimodal named entity recognition",
      "author" : [ "Shuguang Chen", "Gustavo Aguilar", "Leonardo Neves", "Thamar Solorio." ],
      "venue" : "arXiv preprint arXiv:2010.12712.",
      "citeRegEx" : "Chen et al\\.,? 2020",
      "shortCiteRegEx" : "Chen et al\\.",
      "year" : 2020
    }, {
      "title" : "Named entity recognition with bidirectional LSTM-CNNs",
      "author" : [ "Jason P.C. Chiu", "Eric Nichols." ],
      "venue" : "Transactions of the Association for Computational Linguistics.",
      "citeRegEx" : "Chiu and Nichols.,? 2016",
      "shortCiteRegEx" : "Chiu and Nichols.",
      "year" : 2016
    }, {
      "title" : "A coefficient of agreement for nominal scales",
      "author" : [ "Jacob Cohen." ],
      "venue" : "Educational and psychological measurement.",
      "citeRegEx" : "Cohen.,? 1960",
      "shortCiteRegEx" : "Cohen.",
      "year" : 1960
    }, {
      "title" : "Introduction to the bio-entity recognition task at JNLPBA",
      "author" : [ "Nigel Collier", "Jin-Dong Kim." ],
      "venue" : "Proceedings of the International Joint Workshop on Natural Language Processing in Biomedicine and its Applications (NLPBA/BioNLP).",
      "citeRegEx" : "Collier and Kim.,? 2004",
      "shortCiteRegEx" : "Collier and Kim.",
      "year" : 2004
    }, {
      "title" : "Revisiting pretrained models for Chinese natural language processing",
      "author" : [ "Yiming Cui", "Wanxiang Che", "Ting Liu", "Bing Qin", "Shijin Wang", "Guoping Hu." ],
      "venue" : "Findings of the Association for Computational Linguistics: EMNLP 2020.",
      "citeRegEx" : "Cui et al\\.,? 2020",
      "shortCiteRegEx" : "Cui et al\\.",
      "year" : 2020
    }, {
      "title" : "BERT: Pre-training of deep bidirectional transformers for language understanding",
      "author" : [ "Jacob Devlin", "Ming-Wei Chang", "Kenton Lee", "Kristina Toutanova." ],
      "venue" : "Proceedings of the 2019 Conference of the North American Chapter of the Association for",
      "citeRegEx" : "Devlin et al\\.,? 2019",
      "shortCiteRegEx" : "Devlin et al\\.",
      "year" : 2019
    }, {
      "title" : "ZEN: Pre-training Chinese text encoder enhanced by n-gram representations",
      "author" : [ "Shizhe Diao", "Jiaxin Bai", "Yan Song", "Tong Zhang", "Yonggang Wang." ],
      "venue" : "Findings of the Association for Computational Linguistics: EMNLP 2020.",
      "citeRegEx" : "Diao et al\\.,? 2020",
      "shortCiteRegEx" : "Diao et al\\.",
      "year" : 2020
    }, {
      "title" : "A neural multi-digraph model for chinese ner with gazetteers",
      "author" : [ "Ruixue Ding", "Pengjun Xie", "Xiaoyan Zhang", "Wei Lu", "Linlin Li", "Luo Si." ],
      "venue" : "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics.",
      "citeRegEx" : "Ding et al\\.,? 2019",
      "shortCiteRegEx" : "Ding et al\\.",
      "year" : 2019
    }, {
      "title" : "The automatic content extraction (ACE) program – tasks, data, and evaluation",
      "author" : [ "George Doddington", "Alexis Mitchell", "Mark Przybocki", "Lance Ramshaw", "Stephanie Strassel", "Ralph Weischedel." ],
      "venue" : "Proceedings of the Fourth International Conference on",
      "citeRegEx" : "Doddington et al\\.,? 2004",
      "shortCiteRegEx" : "Doddington et al\\.",
      "year" : 2004
    }, {
      "title" : "Speechtransformer: a no-recurrence sequence-to-sequence model for speech recognition",
      "author" : [ "Linhao Dong", "Shuang Xu", "Bo Xu." ],
      "venue" : "2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE.",
      "citeRegEx" : "Dong et al\\.,? 2018",
      "shortCiteRegEx" : "Dong et al\\.",
      "year" : 2018
    }, {
      "title" : "Connectionist temporal classification: labelling unsegmented sequence data with recurrent neural networks",
      "author" : [ "Alex Graves", "Santiago Fernández", "Faustino Gomez", "Jürgen Schmidhuber." ],
      "venue" : "Proceedings of the 23rd international conference on Ma-",
      "citeRegEx" : "Graves et al\\.,? 2006",
      "shortCiteRegEx" : "Graves et al\\.",
      "year" : 2006
    }, {
      "title" : "A lexicon-based graph neural network for Chinese NER",
      "author" : [ "Tao Gui", "Yicheng Zou", "Qi Zhang", "Minlong Peng", "Jinlan Fu", "Zhongyu Wei", "Xuanjing Huang." ],
      "venue" : "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing",
      "citeRegEx" : "Gui et al\\.,? 2019",
      "shortCiteRegEx" : "Gui et al\\.",
      "year" : 2019
    }, {
      "title" : "Long short-term memory",
      "author" : [ "Sepp Hochreiter", "Jürgen Schmidhuber." ],
      "venue" : "Neural computation.",
      "citeRegEx" : "Hochreiter and Schmidhuber.,? 1997",
      "shortCiteRegEx" : "Hochreiter and Schmidhuber.",
      "year" : 1997
    }, {
      "title" : "spaCy: Industrial-strength Natural Language Processing in Python",
      "author" : [ "Matthew Honnibal", "Ines Montani", "Sofie Van Landeghem", "Adriane Boyd" ],
      "venue" : null,
      "citeRegEx" : "Honnibal et al\\.,? \\Q2020\\E",
      "shortCiteRegEx" : "Honnibal et al\\.",
      "year" : 2020
    }, {
      "title" : "Genia corpus—a semantically annotated corpus for bio-textmining",
      "author" : [ "J-D Kim", "Tomoko Ohta", "Yuka Tateisi", "Jun’ichi Tsujii" ],
      "venue" : null,
      "citeRegEx" : "Kim et al\\.,? \\Q2003\\E",
      "shortCiteRegEx" : "Kim et al\\.",
      "year" : 2003
    }, {
      "title" : "Improved instantaneous frequency estimation using an adaptive short-time fourier transform",
      "author" : [ "Henry K Kwok", "Douglas L Jones." ],
      "venue" : "IEEE transactions on signal processing.",
      "citeRegEx" : "Kwok and Jones.,? 2000",
      "shortCiteRegEx" : "Kwok and Jones.",
      "year" : 2000
    }, {
      "title" : "Conditional random fields: Probabilistic models for segmenting and labeling sequence data",
      "author" : [ "John D. Lafferty", "Andrew McCallum", "Fernando C.N. Pereira." ],
      "venue" : "Proceedings of the Eighteenth International Conference on Machine Learning.",
      "citeRegEx" : "Lafferty et al\\.,? 2001",
      "shortCiteRegEx" : "Lafferty et al\\.",
      "year" : 2001
    }, {
      "title" : "Neural architectures for named entity recognition",
      "author" : [ "Guillaume Lample", "Miguel Ballesteros", "Sandeep Subramanian", "Kazuya Kawakami", "Chris Dyer" ],
      "venue" : null,
      "citeRegEx" : "Lample et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Lample et al\\.",
      "year" : 2016
    }, {
      "title" : "The third international Chinese language processing bakeoff: Word segmentation and named entity recognition",
      "author" : [ "Gina-Anne Levow." ],
      "venue" : "Proceedings of the Fifth SIGHAN Workshop on Chinese Language Processing.",
      "citeRegEx" : "Levow.,? 2006",
      "shortCiteRegEx" : "Levow.",
      "year" : 2006
    }, {
      "title" : "Endto-end speech recognition with adaptive computation steps",
      "author" : [ "Mohan Li", "Min Liu", "Hattori Masanori." ],
      "venue" : "ICASSP 2019-2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP).",
      "citeRegEx" : "Li et al\\.,? 2019",
      "shortCiteRegEx" : "Li et al\\.",
      "year" : 2019
    }, {
      "title" : "FLAT: Chinese NER using flat-lattice transformer",
      "author" : [ "Xiaonan Li", "Hang Yan", "Xipeng Qiu", "Xuanjing Huang." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics.",
      "citeRegEx" : "Li et al\\.,? 2020",
      "shortCiteRegEx" : "Li et al\\.",
      "year" : 2020
    }, {
      "title" : "An encoding strategy based word-character LSTM for Chinese NER",
      "author" : [ "Wei Liu", "Tongge Xu", "Qinghua Xu", "Jiayu Song", "Yueran Zu." ],
      "venue" : "Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational",
      "citeRegEx" : "Liu et al\\.,? 2019a",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2019
    }, {
      "title" : "2019b. Roberta: A robustly optimized bert pretraining approach",
      "author" : [ "Yinhan Liu", "Myle Ott", "Naman Goyal", "Jingfei Du", "Mandar Joshi", "Danqi Chen", "Omer Levy", "Mike Lewis", "Luke Zettlemoyer", "Veselin Stoyanov" ],
      "venue" : null,
      "citeRegEx" : "Liu et al\\.,? \\Q2019\\E",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2019
    }, {
      "title" : "Visual attention model for name tagging in multimodal social media",
      "author" : [ "Di Lu", "Leonardo Neves", "Vitor Carvalho", "Ning Zhang", "Heng Ji." ],
      "venue" : "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics.",
      "citeRegEx" : "Lu et al\\.,? 2018",
      "shortCiteRegEx" : "Lu et al\\.",
      "year" : 2018
    }, {
      "title" : "Simplify the usage of lexicon in Chinese NER",
      "author" : [ "Ruotian Ma", "Minlong Peng", "Qi Zhang", "Zhongyu Wei", "Xuanjing Huang." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics.",
      "citeRegEx" : "Ma et al\\.,? 2020",
      "shortCiteRegEx" : "Ma et al\\.",
      "year" : 2020
    }, {
      "title" : "End-to-end sequence labeling via bi-directional LSTM-CNNsCRF",
      "author" : [ "Xuezhe Ma", "Eduard Hovy." ],
      "venue" : "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics.",
      "citeRegEx" : "Ma and Hovy.,? 2016",
      "shortCiteRegEx" : "Ma and Hovy.",
      "year" : 2016
    }, {
      "title" : "Distributed representations of words and phrases and their compositionality",
      "author" : [ "Tomas Mikolov", "Ilya Sutskever", "Kai Chen", "Greg S Corrado", "Jeff Dean." ],
      "venue" : "Advances in Neural Information Processing Systems.",
      "citeRegEx" : "Mikolov et al\\.,? 2013",
      "shortCiteRegEx" : "Mikolov et al\\.",
      "year" : 2013
    }, {
      "title" : "Named entity recognition for question answering",
      "author" : [ "Diego Mollá", "Menno van Zaanen", "Daniel Smith." ],
      "venue" : "Proceedings of the Australasian Language Technology Workshop 2006.",
      "citeRegEx" : "Mollá et al\\.,? 2006",
      "shortCiteRegEx" : "Mollá et al\\.",
      "year" : 2006
    }, {
      "title" : "Multimodal named entity recognition for short social media posts",
      "author" : [ "Seungwhan Moon", "Leonardo Neves", "Vitor Carvalho." ],
      "venue" : "Proceedings of the",
      "citeRegEx" : "Moon et al\\.,? 2018",
      "shortCiteRegEx" : "Moon et al\\.",
      "year" : 2018
    }, {
      "title" : "Rectified linear units improve restricted boltzmann machines",
      "author" : [ "Vinod Nair", "Geoffrey E. Hinton." ],
      "venue" : "Proceedings of the 27th International Conference on International Conference on Machine Learning.",
      "citeRegEx" : "Nair and Hinton.,? 2010",
      "shortCiteRegEx" : "Nair and Hinton.",
      "year" : 2010
    }, {
      "title" : "Improving named entity recognition for Chinese social media with word segmentation representation learning",
      "author" : [ "Nanyun Peng", "Mark Dredze." ],
      "venue" : "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 2:",
      "citeRegEx" : "Peng and Dredze.,? 2016a",
      "shortCiteRegEx" : "Peng and Dredze.",
      "year" : 2016
    }, {
      "title" : "Improving named entity recognition for Chinese social media with word segmentation representation learning",
      "author" : [ "Nanyun Peng", "Mark Dredze." ],
      "venue" : "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 2:",
      "citeRegEx" : "Peng and Dredze.,? 2016b",
      "shortCiteRegEx" : "Peng and Dredze.",
      "year" : 2016
    }, {
      "title" : "Stanza: A python natural language processing toolkit for many human languages",
      "author" : [ "Peng Qi", "Yuhao Zhang", "Yuhui Zhang", "Jason Bolton", "Christopher D. Manning." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the Association for Computational",
      "citeRegEx" : "Qi et al\\.,? 2020",
      "shortCiteRegEx" : "Qi et al\\.",
      "year" : 2020
    }, {
      "title" : "Improving language understanding by generative pre-training",
      "author" : [ "Alec Radford", "Karthik Narasimhan", "Tim Salimans", "Ilya Sutskever." ],
      "venue" : "Preprint.",
      "citeRegEx" : "Radford et al\\.,? 2018",
      "shortCiteRegEx" : "Radford et al\\.",
      "year" : 2018
    }, {
      "title" : "Text chunking using transformation-based learning",
      "author" : [ "Lance Ramshaw", "Mitch Marcus." ],
      "venue" : "Third Workshop on Very Large Corpora.",
      "citeRegEx" : "Ramshaw and Marcus.,? 1995",
      "shortCiteRegEx" : "Ramshaw and Marcus.",
      "year" : 1995
    }, {
      "title" : "Speech recognition using filter-bank features",
      "author" : [ "Sourabh Ravindran", "C Demirogulu", "David V Anderson." ],
      "venue" : "The Thrity-Seventh Asilomar Conference on Signals, Systems & Computers, 2003.",
      "citeRegEx" : "Ravindran et al\\.,? 2003",
      "shortCiteRegEx" : "Ravindran et al\\.",
      "year" : 2003
    }, {
      "title" : "Component fusion: Learning replaceable language model component for end-to-end speech recognition system",
      "author" : [ "Changhao Shan", "Chao Weng", "Guangsen Wang", "Dan Su", "Min Luo", "Dong Yu", "Lei Xie." ],
      "venue" : "ICASSP 2019-2019 IEEE International Con-",
      "citeRegEx" : "Shan et al\\.,? 2019",
      "shortCiteRegEx" : "Shan et al\\.",
      "year" : 2019
    }, {
      "title" : "brat: a web-based tool for NLP-assisted text annotation",
      "author" : [ "Pontus Stenetorp", "Sampo Pyysalo", "Goran Topić", "Tomoko Ohta", "Sophia Ananiadou", "Jun’ichi Tsujii" ],
      "venue" : "In Proceedings of the Demonstrations Session at EACL",
      "citeRegEx" : "Stenetorp et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Stenetorp et al\\.",
      "year" : 2012
    }, {
      "title" : "Neural architectures for nested NER through linearization",
      "author" : [ "Jana Straková", "Milan Straka", "Jan Hajic." ],
      "venue" : "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics.",
      "citeRegEx" : "Straková et al\\.,? 2019",
      "shortCiteRegEx" : "Straková et al\\.",
      "year" : 2019
    }, {
      "title" : "Leverage lexical knowledge",
      "author" : [ "Dianbo Sui", "Yubo Chen", "Kang Liu", "Jun Zhao", "Shengping Liu" ],
      "venue" : null,
      "citeRegEx" : "Sui et al\\.,? \\Q2019\\E",
      "shortCiteRegEx" : "Sui et al\\.",
      "year" : 2019
    }, {
      "title" : "RIVA: A pre-trained tweet multimodal model based on text-image relation for multimodal NER",
      "author" : [ "Lin Sun", "Jiquan Wang", "Yindu Su", "Fangsheng Weng", "Yuxuan Sun", "Zengwei Zheng", "Yuanyi Chen." ],
      "venue" : "Proceedings of the 28th International Conference",
      "citeRegEx" : "Sun et al\\.,? 2020",
      "shortCiteRegEx" : "Sun et al\\.",
      "year" : 2020
    }, {
      "title" : "Synchronous transformers for end-to-end speech recognition",
      "author" : [ "Zhengkun Tian", "Jiangyan Yi", "Ye Bai", "Jianhua Tao", "Shuai Zhang", "Zhengqi Wen." ],
      "venue" : "ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Pro-",
      "citeRegEx" : "Tian et al\\.,? 2020",
      "shortCiteRegEx" : "Tian et al\\.",
      "year" : 2020
    }, {
      "title" : "Multimodal transformer for unaligned multimodal language sequences",
      "author" : [ "Yao-Hung Hubert Tsai", "Shaojie Bai", "Paul Pu Liang", "J. Zico Kolter", "Louis-Philippe Morency", "Ruslan Salakhutdinov." ],
      "venue" : "Proceedings of the 57th Annual Meeting of the Associa-",
      "citeRegEx" : "Tsai et al\\.,? 2019",
      "shortCiteRegEx" : "Tsai et al\\.",
      "year" : 2019
    }, {
      "title" : "Attention is all you need",
      "author" : [ "Ashish Vaswani", "Noam Shazeer", "Niki Parmar", "Jakob Uszkoreit", "Llion Jones", "Aidan N Gomez", "Łukasz Kaiser", "Illia Polosukhin." ],
      "venue" : "Advances in neural information processing systems.",
      "citeRegEx" : "Vaswani et al\\.,? 2017",
      "shortCiteRegEx" : "Vaswani et al\\.",
      "year" : 2017
    }, {
      "title" : "ACE 2005 multilingual training corpus",
      "author" : [ "Christopher Walker", "Stephanie Strassel", "Julie Medero", "Kazuaki Maeda." ],
      "venue" : "LDC.",
      "citeRegEx" : "Walker et al\\.,? 2004",
      "shortCiteRegEx" : "Walker et al\\.",
      "year" : 2004
    }, {
      "title" : "Neural chinese named entity recognition via cnn-lstm-crf and joint training with word segmentation",
      "author" : [ "Fangzhao Wu", "Junxin Liu", "Chuhan Wu", "Yongfeng Huang", "Xing Xie." ],
      "venue" : "The World Wide Web Conference.",
      "citeRegEx" : "Wu et al\\.,? 2019",
      "shortCiteRegEx" : "Wu et al\\.",
      "year" : 2019
    }, {
      "title" : "Joint segmentation and named entity recognition using dual decomposition in chinese discharge summaries",
      "author" : [ "Yan Xu", "Yining Wang", "Tianren Liu", "Jiahua Liu", "Yubo Fan", "Yi Qian", "Junichi Tsujii", "Eric I Chang." ],
      "venue" : "Journal of the American Medical Informat-",
      "citeRegEx" : "Xu et al\\.,? 2013",
      "shortCiteRegEx" : "Xu et al\\.",
      "year" : 2013
    }, {
      "title" : "Porous lattice transformer encoder for Chinese NER",
      "author" : [ "Mengge Xue", "Bowen Yu", "Tingwen Liu", "Yue Zhang", "Erli Meng", "Bin Wang." ],
      "venue" : "Proceedings of the 28th International Conference on Computational Linguistics.",
      "citeRegEx" : "Xue et al\\.,? 2020",
      "shortCiteRegEx" : "Xue et al\\.",
      "year" : 2020
    }, {
      "title" : "Improving multimodal named entity recognition via entity span detection with unified multimodal transformer",
      "author" : [ "Jianfei Yu", "Jing Jiang", "Li Yang", "Rui Xia." ],
      "venue" : "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics.",
      "citeRegEx" : "Yu et al\\.,? 2020",
      "shortCiteRegEx" : "Yu et al\\.",
      "year" : 2020
    }, {
      "title" : "Kernel methods for relation extraction",
      "author" : [ "Dmitry Zelenko", "Chinatsu Aone", "Anthony Richardella." ],
      "venue" : "Journal of machine learning research.",
      "citeRegEx" : "Zelenko et al\\.,? 2003",
      "shortCiteRegEx" : "Zelenko et al\\.",
      "year" : 2003
    }, {
      "title" : "Adaptive co-attention network for named entity recognition in tweets",
      "author" : [ "Qi Zhang", "Jinlan Fu", "Xiaoyu Liu", "Xuanjing Huang." ],
      "venue" : "Proceedings of the AAAI conference on artificial intelligence.",
      "citeRegEx" : "Zhang et al\\.,? 2018",
      "shortCiteRegEx" : "Zhang et al\\.",
      "year" : 2018
    }, {
      "title" : "Chinese NER using lattice LSTM",
      "author" : [ "Yue Zhang", "Jie Yang." ],
      "venue" : "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics.",
      "citeRegEx" : "Zhang and Yang.,? 2018",
      "shortCiteRegEx" : "Zhang and Yang.",
      "year" : 2018
    } ],
    "referenceMentions" : [ {
      "referenceID" : 56,
      "context" : "NER plays a crucial role in many natural language processing (NLP) tasks, including relation extraction (Zelenko et al., 2003), question answering (Mollá et al.",
      "startOffset" : 104,
      "endOffset" : 126
    }, {
      "referenceID" : 34,
      "context" : ", 2003), question answering (Mollá et al., 2006) and summarization (Aramaki et al.",
      "startOffset" : 28,
      "endOffset" : 48
    }, {
      "referenceID" : 4,
      "context" : "However, when texts are noisy or short, and it is not sufficient to locate and classify named entities accurately only based on textual information (Baldwin et al., 2015; Lu et al., 2018).",
      "startOffset" : 148,
      "endOffset" : 187
    }, {
      "referenceID" : 30,
      "context" : "However, when texts are noisy or short, and it is not sufficient to locate and classify named entities accurately only based on textual information (Baldwin et al., 2015; Lu et al., 2018).",
      "startOffset" : 148,
      "endOffset" : 187
    }, {
      "referenceID" : 58,
      "context" : "This language characteristic is one of the major roadblocks in Chinese NER, since named entity boundaries are usually word boundaries (Zhang and Yang, 2018).",
      "startOffset" : 134,
      "endOffset" : 156
    }, {
      "referenceID" : 5,
      "context" : "Specifically, we annotate all occurrences of 3 entity types (person name, location and organization) in 42,987 sentences originating from the transcripts of Aishell-1 (Bu et al., 2017), a corpus that has been widely employed in Mandarin speech recognition research in recent years (Shan et al.",
      "startOffset" : 167,
      "endOffset" : 184
    }, {
      "referenceID" : 43,
      "context" : ", 2017), a corpus that has been widely employed in Mandarin speech recognition research in recent years (Shan et al., 2019; Li et al., 2019; Tian et al., 2020).",
      "startOffset" : 104,
      "endOffset" : 159
    }, {
      "referenceID" : 26,
      "context" : ", 2017), a corpus that has been widely employed in Mandarin speech recognition research in recent years (Shan et al., 2019; Li et al., 2019; Tian et al., 2020).",
      "startOffset" : 104,
      "endOffset" : 159
    }, {
      "referenceID" : 48,
      "context" : ", 2017), a corpus that has been widely employed in Mandarin speech recognition research in recent years (Shan et al., 2019; Li et al., 2019; Tian et al., 2020).",
      "startOffset" : 104,
      "endOffset" : 159
    }, {
      "referenceID" : 35,
      "context" : "In particular, unlike previous multimodal NER datasets (Moon et al., 2018; Zhang et al., 2018; Lu et al., 2018) are all flatly annotated, not only the topmost entities but also nested entities are annotated in CNERTA.",
      "startOffset" : 55,
      "endOffset" : 111
    }, {
      "referenceID" : 57,
      "context" : "In particular, unlike previous multimodal NER datasets (Moon et al., 2018; Zhang et al., 2018; Lu et al., 2018) are all flatly annotated, not only the topmost entities but also nested entities are annotated in CNERTA.",
      "startOffset" : 55,
      "endOffset" : 111
    }, {
      "referenceID" : 30,
      "context" : "In particular, unlike previous multimodal NER datasets (Moon et al., 2018; Zhang et al., 2018; Lu et al., 2018) are all flatly annotated, not only the topmost entities but also nested entities are annotated in CNERTA.",
      "startOffset" : 55,
      "endOffset" : 111
    }, {
      "referenceID" : 24,
      "context" : "In detail, we first investigate the performance of several classic text-only models on our dataset, including BiLSTM-CRF (Lample et al., 2016) and BERTCRF (Devlin et al.",
      "startOffset" : 121,
      "endOffset" : 142
    }, {
      "referenceID" : 58,
      "context" : "Then, since introducing a lexicon has been proven as an effective way to incorporate word information in Chinese NER (Zhang and Yang, 2018), we implement several lexicon-enhanced models, such as Lattice-LSTM (Zhang and Yang, 2018) and ZEN (Diao et al.",
      "startOffset" : 117,
      "endOffset" : 139
    }, {
      "referenceID" : 58,
      "context" : "Then, since introducing a lexicon has been proven as an effective way to incorporate word information in Chinese NER (Zhang and Yang, 2018), we implement several lexicon-enhanced models, such as Lattice-LSTM (Zhang and Yang, 2018) and ZEN (Diao et al.",
      "startOffset" : 208,
      "endOffset" : 230
    }, {
      "referenceID" : 13,
      "context" : "Then, since introducing a lexicon has been proven as an effective way to incorporate word information in Chinese NER (Zhang and Yang, 2018), we implement several lexicon-enhanced models, such as Lattice-LSTM (Zhang and Yang, 2018) and ZEN (Diao et al., 2020), to explore whether the acoustic modality can provide word information beyond the lexicon.",
      "startOffset" : 239,
      "endOffset" : 258
    }, {
      "referenceID" : 49,
      "context" : "Finally, to verify the effectiveness of introducing the acoustic modality, we test some widely used multimodal models, such as CMA (Tsai et al., 2019) and MMI (Yu et al.",
      "startOffset" : 131,
      "endOffset" : 150
    }, {
      "referenceID" : 17,
      "context" : "In the hybrid loss, a masked CTC loss (Graves et al., 2006) is designed for enforcing a monotonic alignment between speech and text sequences.",
      "startOffset" : 38,
      "endOffset" : 59
    }, {
      "referenceID" : 50,
      "context" : "As transformer-based models (Vaswani et al., 2017; Devlin et al., 2019) become the mainstream method in NLP, researchers turn to study how to fuse visual clues in transformers structure.",
      "startOffset" : 28,
      "endOffset" : 71
    }, {
      "referenceID" : 12,
      "context" : "As transformer-based models (Vaswani et al., 2017; Devlin et al., 2019) become the mainstream method in NLP, researchers turn to study how to fuse visual clues in transformers structure.",
      "startOffset" : 28,
      "endOffset" : 71
    }, {
      "referenceID" : 53,
      "context" : "The second one is to learn CWS and NER tasks jointly (Xu et al., 2013; Peng and Dredze, 2016b; Cao et al., 2018; Wu et al., 2019).",
      "startOffset" : 53,
      "endOffset" : 129
    }, {
      "referenceID" : 38,
      "context" : "The second one is to learn CWS and NER tasks jointly (Xu et al., 2013; Peng and Dredze, 2016b; Cao et al., 2018; Wu et al., 2019).",
      "startOffset" : 53,
      "endOffset" : 129
    }, {
      "referenceID" : 6,
      "context" : "The second one is to learn CWS and NER tasks jointly (Xu et al., 2013; Peng and Dredze, 2016b; Cao et al., 2018; Wu et al., 2019).",
      "startOffset" : 53,
      "endOffset" : 129
    }, {
      "referenceID" : 52,
      "context" : "The second one is to learn CWS and NER tasks jointly (Xu et al., 2013; Peng and Dredze, 2016b; Cao et al., 2018; Wu et al., 2019).",
      "startOffset" : 53,
      "endOffset" : 129
    }, {
      "referenceID" : 58,
      "context" : "The third one is to resort to an automatically constructed lexicon (Zhang and Yang, 2018; Ding et al., 2019; Liu et al., 2019a; Sui et al., 2019; Gui et al., 2019; Li et al., 2020; Ma et al., 2020; Xue et al., 2020).",
      "startOffset" : 67,
      "endOffset" : 215
    }, {
      "referenceID" : 14,
      "context" : "The third one is to resort to an automatically constructed lexicon (Zhang and Yang, 2018; Ding et al., 2019; Liu et al., 2019a; Sui et al., 2019; Gui et al., 2019; Li et al., 2020; Ma et al., 2020; Xue et al., 2020).",
      "startOffset" : 67,
      "endOffset" : 215
    }, {
      "referenceID" : 28,
      "context" : "The third one is to resort to an automatically constructed lexicon (Zhang and Yang, 2018; Ding et al., 2019; Liu et al., 2019a; Sui et al., 2019; Gui et al., 2019; Li et al., 2020; Ma et al., 2020; Xue et al., 2020).",
      "startOffset" : 67,
      "endOffset" : 215
    }, {
      "referenceID" : 46,
      "context" : "The third one is to resort to an automatically constructed lexicon (Zhang and Yang, 2018; Ding et al., 2019; Liu et al., 2019a; Sui et al., 2019; Gui et al., 2019; Li et al., 2020; Ma et al., 2020; Xue et al., 2020).",
      "startOffset" : 67,
      "endOffset" : 215
    }, {
      "referenceID" : 18,
      "context" : "The third one is to resort to an automatically constructed lexicon (Zhang and Yang, 2018; Ding et al., 2019; Liu et al., 2019a; Sui et al., 2019; Gui et al., 2019; Li et al., 2020; Ma et al., 2020; Xue et al., 2020).",
      "startOffset" : 67,
      "endOffset" : 215
    }, {
      "referenceID" : 27,
      "context" : "The third one is to resort to an automatically constructed lexicon (Zhang and Yang, 2018; Ding et al., 2019; Liu et al., 2019a; Sui et al., 2019; Gui et al., 2019; Li et al., 2020; Ma et al., 2020; Xue et al., 2020).",
      "startOffset" : 67,
      "endOffset" : 215
    }, {
      "referenceID" : 31,
      "context" : "The third one is to resort to an automatically constructed lexicon (Zhang and Yang, 2018; Ding et al., 2019; Liu et al., 2019a; Sui et al., 2019; Gui et al., 2019; Li et al., 2020; Ma et al., 2020; Xue et al., 2020).",
      "startOffset" : 67,
      "endOffset" : 215
    }, {
      "referenceID" : 54,
      "context" : "The third one is to resort to an automatically constructed lexicon (Zhang and Yang, 2018; Ding et al., 2019; Liu et al., 2019a; Sui et al., 2019; Gui et al., 2019; Li et al., 2020; Ma et al., 2020; Xue et al., 2020).",
      "startOffset" : 67,
      "endOffset" : 215
    }, {
      "referenceID" : 5,
      "context" : "In detail, our annotated dataset is based on Aishell1 (Bu et al., 2017) dataset, which is a large-scale Mandarin automatic speech recognition dataset.",
      "startOffset" : 54,
      "endOffset" : 71
    }, {
      "referenceID" : 44,
      "context" : "round, we use Brat (Stenetorp et al., 2012) as the annotation tool and ask 3 internal annotators (including the first author of this paper) to perform annotation, who are very familiar with this task.",
      "startOffset" : 19,
      "endOffset" : 43
    }, {
      "referenceID" : 9,
      "context" : "Cohen’s kappa coefficient (Cohen, 1960) is used to measure the inter-annotator agreements.",
      "startOffset" : 26,
      "endOffset" : 39
    }, {
      "referenceID" : 25,
      "context" : "Specifically, we first compare our corpus with some Chinese NER datasets, such as MSRA (Levow, 2006), OntoNotes (Weischedel et al.",
      "startOffset" : 87,
      "endOffset" : 100
    }, {
      "referenceID" : 37,
      "context" : ", 2011), Weibo NER (Peng and Dredze, 2016a) and Resume (Zhang and Yang, 2018).",
      "startOffset" : 19,
      "endOffset" : 43
    }, {
      "referenceID" : 58,
      "context" : ", 2011), Weibo NER (Peng and Dredze, 2016a) and Resume (Zhang and Yang, 2018).",
      "startOffset" : 55,
      "endOffset" : 77
    }, {
      "referenceID" : 21,
      "context" : "Then, we compare our corpus with several widely used nested NER datasets, like GENIA (Kim et al., 2003), JNLPBA (Collier and Kim, 2004), ACE-2004 (Doddington et al.",
      "startOffset" : 85,
      "endOffset" : 103
    }, {
      "referenceID" : 10,
      "context" : ", 2003), JNLPBA (Collier and Kim, 2004), ACE-2004 (Doddington et al.",
      "startOffset" : 16,
      "endOffset" : 39
    }, {
      "referenceID" : 15,
      "context" : ", 2003), JNLPBA (Collier and Kim, 2004), ACE-2004 (Doddington et al., 2004) and ACE-2005 (Walker et al.",
      "startOffset" : 50,
      "endOffset" : 75
    }, {
      "referenceID" : 57,
      "context" : "Finally, multimodal NER datasets, including Twitter-2015 (Zhang et al., 2018) and Twitter-2017 (Lu et al.",
      "startOffset" : 57,
      "endOffset" : 77
    }, {
      "referenceID" : 30,
      "context" : ", 2018) and Twitter-2017 (Lu et al., 2018), are compared with our corpus.",
      "startOffset" : 25,
      "endOffset" : 42
    }, {
      "referenceID" : 41,
      "context" : "(2019) to encode the nested entity structure into a CoNLL-like, per-character BIO encoding (Ramshaw and Marcus, 1995).",
      "startOffset" : 91,
      "endOffset" : 117
    }, {
      "referenceID" : 22,
      "context" : "Specifically, in the speech processing layer, a speech signal first goes through a pre-emphasis filter; then gets sliced into frames and a window function is applied to each frame; afterwards, a Short-Time Fourier transform (Kwok and Jones, 2000) is employed on each frame and the power spectrum is calculated; and subsequently, the filter banks (Ravindran et al.",
      "startOffset" : 224,
      "endOffset" : 246
    }, {
      "referenceID" : 42,
      "context" : "Specifically, in the speech processing layer, a speech signal first goes through a pre-emphasis filter; then gets sliced into frames and a window function is applied to each frame; afterwards, a Short-Time Fourier transform (Kwok and Jones, 2000) is employed on each frame and the power spectrum is calculated; and subsequently, the filter banks (Ravindran et al., 2003) are computed.",
      "startOffset" : 346,
      "endOffset" : 370
    }, {
      "referenceID" : 50,
      "context" : "Finally, to effectively capture long-term dependencies, down-sampled acoustic features flow through the transformer-based encoder (Vaswani et al., 2017).",
      "startOffset" : 130,
      "endOffset" : 152
    }, {
      "referenceID" : 20,
      "context" : "Open-Source NLP Toolkit: Many open-source NLP toolkits, such as spaCy (Honnibal et al., 2020) and Stanza (Qi et al.",
      "startOffset" : 70,
      "endOffset" : 93
    }, {
      "referenceID" : 39,
      "context" : ", 2020) and Stanza (Qi et al., 2020), support Chinese NER.",
      "startOffset" : 19,
      "endOffset" : 36
    }, {
      "referenceID" : 19,
      "context" : "BiLSTM-CRF: Featured by a bidirectional LSTM (Hochreiter and Schmidhuber, 1997) as the textual encoder and conditional random fields (CRF) (Lafferty et al.",
      "startOffset" : 45,
      "endOffset" : 79
    }, {
      "referenceID" : 23,
      "context" : "BiLSTM-CRF: Featured by a bidirectional LSTM (Hochreiter and Schmidhuber, 1997) as the textual encoder and conditional random fields (CRF) (Lafferty et al., 2001) as the decoder, the widely used BiLSTM-CRF (Lample et al.",
      "startOffset" : 139,
      "endOffset" : 162
    }, {
      "referenceID" : 24,
      "context" : ", 2001) as the decoder, the widely used BiLSTM-CRF (Lample et al., 2016) is adopted as an important baseline.",
      "startOffset" : 51,
      "endOffset" : 72
    }, {
      "referenceID" : 40,
      "context" : "PLM-CRF: Instead of training a model from scratch, we also adopt the framework of fine-tuning a pretrained language model (PLM) on a downstream task (Radford et al., 2018).",
      "startOffset" : 149,
      "endOffset" : 171
    }, {
      "referenceID" : 12,
      "context" : "In this framework, we adopt BERT (Devlin et al., 2019) as the textual encoder and use CRF as the decoder.",
      "startOffset" : 33,
      "endOffset" : 54
    }, {
      "referenceID" : 11,
      "context" : "In addition to initializing the textual encoder with the original pretrained BERT model, a SoTA Chinese pretrained language model, called MacBERT (Cui et al., 2020), is used.",
      "startOffset" : 146,
      "endOffset" : 164
    }, {
      "referenceID" : 58,
      "context" : "(1) Lattice-LSTM (Zhang and Yang, 2018) is a classic method that can encode a sequence of input characters as well as all potential words that match a lexicon.",
      "startOffset" : 17,
      "endOffset" : 39
    }, {
      "referenceID" : 13,
      "context" : "(2) ZEN (Diao et al., 2020) is a pretrained Chinese text encoder enhanced by an n-gram lexicon.",
      "startOffset" : 8,
      "endOffset" : 27
    }, {
      "referenceID" : 49,
      "context" : "We present two representative fusion modules, which are Cross-Modal Attention (CMA) module (Tsai et al., 2019) and Multimodal Interaction (MMI) module (Yu et al.",
      "startOffset" : 91,
      "endOffset" : 110
    }, {
      "referenceID" : 55,
      "context" : ", 2019) and Multimodal Interaction (MMI) module (Yu et al., 2020).",
      "startOffset" : 48,
      "endOffset" : 65
    }, {
      "referenceID" : 49,
      "context" : "2812 Cross-Modal Attention Module (CMA): Given the textual hidden representations X ∈ Rd×n and the acoustic representations S ∈ Rd×t , we first employ a m-head cross-modal attention mechanism (Tsai et al., 2019), by treating X as queries, and S as keys and values:",
      "startOffset" : 192,
      "endOffset" : 211
    }, {
      "referenceID" : 3,
      "context" : "where LN means layer normalization (Ba et al., 2016) and FFN means a fully connected feedforward network, which consists of two linear transformation with a ReLU activation (Nair and Hinton, 2010).",
      "startOffset" : 35,
      "endOffset" : 52
    }, {
      "referenceID" : 36,
      "context" : ", 2016) and FFN means a fully connected feedforward network, which consists of two linear transformation with a ReLU activation (Nair and Hinton, 2010).",
      "startOffset" : 128,
      "endOffset" : 151
    }, {
      "referenceID" : 58,
      "context" : "Following previous studies in NER (Zhang and Yang, 2018), standard precision (P), recall (R) and F1-score (F1) are used as evaluation metrics.",
      "startOffset" : 34,
      "endOffset" : 56
    }, {
      "referenceID" : 33,
      "context" : "LSTM-Based Baselines: We use the 50dimensional character embeddings, which are pretrained on Chinese Giga-Word * using word2vec (Mikolov et al., 2013).",
      "startOffset" : 128,
      "endOffset" : 150
    }, {
      "referenceID" : 12,
      "context" : "Pretrained Language Model Fine-Tuning: We use the base models of BERT (Devlin et al., 2019), MacBERT (Cui et al.",
      "startOffset" : 70,
      "endOffset" : 91
    }, {
      "referenceID" : 11,
      "context" : ", 2019), MacBERT (Cui et al., 2020) and ZEN (Diao et al.",
      "startOffset" : 17,
      "endOffset" : 35
    }, {
      "referenceID" : 49,
      "context" : "Compared with CMA (Tsai et al., 2019) and MMI (Yu et al.",
      "startOffset" : 18,
      "endOffset" : 37
    }, {
      "referenceID" : 55,
      "context" : ", 2019) and MMI (Yu et al., 2020), there is a significant improvement.",
      "startOffset" : 16,
      "endOffset" : 33
    } ],
    "year" : 2021,
    "abstractText" : "In this paper, we aim to explore an uncharted territory, which is Chinese multimodal named entity recognition (NER) with both textual and acoustic contents. To achieve this, we construct a large-scale human-annotated Chinese multimodal NER dataset, named CNERTA. Our corpus totally contains 42,987 annotated sentences accompanying by 71 hours of speech data. Based on this dataset, we propose a family of strong and representative baseline models, which can leverage textual features or multimodal features. Upon these baselines, to capture the natural monotonic alignment between the textual modality and the acoustic modality, we further propose a simple multimodal multitask model by introducing a speech-to-text alignment auxiliary task. Through extensive experiments, we observe that: (1) Progressive performance boosts as we move from unimodal to multimodal, verifying the necessity of integrating speech clues into Chinese NER. (2) Our proposed model yields state-of-the-art (SoTA) results on CNERTA, demonstrating its effectiveness. For further research, the annotated dataset is publicly available at http://github.com/DianboWork/ CNERTA.",
    "creator" : "LaTeX with hyperref"
  }
}