section TITLE
id pdf2json/2021.acl-long.4.pdf.json
hatecheck O
: O
functional O
tests O
for O
hate O
speech O
detection O
models O

section ABSTRACT
id pdf2json/2021.acl-long.4.pdf.json
detecting O
online O
hate O
is O
a O
difficult O
task O
that O
even O
state-of-the-art O
models O
struggle O
with O
. O

section ABSTRACT
id pdf2json/2021.acl-long.4.pdf.json
typically O
, O
hate O
speech O
detection O
models O
are O
evaluated O
by O
measuring O
their O
performance O
on O
held-out O
test O
data O
using O
metrics O
such O
as O
accuracy O
and O
f1 O
score O
. O

section ABSTRACT
id pdf2json/2021.acl-long.4.pdf.json
however O
, O
this O
approach O
makes O
it O
difficult O
to O
identify O
specific O
model O
weak O
points O
. O

section ABSTRACT
id pdf2json/2021.acl-long.4.pdf.json
it O
also O
risks O
overestimating O
generalisable O
model O
performance O
due O
to O
increasingly O
well-evidenced O
systematic O
gaps O
and O
biases O
in O
hate O
speech O
datasets O
. O

section ABSTRACT
id pdf2json/2021.acl-long.4.pdf.json
to O
enable O
more O
targeted O
diagnostic O
insights O
, O
we O
introduce O
hatecheck O
, O
a O
suite O
of O
functional O
tests O
for O
hate O
speech O
detection O
models O
. O

section ABSTRACT
id pdf2json/2021.acl-long.4.pdf.json
we O
specify O
29 O
model O
functionalities O
motivated O
by O
a O
review O
of O
previous O
research O
and O
a O
series O
of O
interviews O
with O
civil O
society O
stakeholders O
. O

section ABSTRACT
id pdf2json/2021.acl-long.4.pdf.json
we O
craft O
test O
cases O
for O
each O
functionality O
and O
validate O
their O
quality O
through O
a O
structured O
annotation O
process O
. O

section ABSTRACT
id pdf2json/2021.acl-long.4.pdf.json
to O
illustrate O
hatecheck O
’ O
s O
utility O
, O
we O
test O
near-state-of-the-art O
transformer O
models O
as O
well O
as O
two O
popular O
commercial O
models O
, O
revealing O
critical O
model O
weaknesses O
. O

section 0
id pdf2json/2021.acl-long.4.pdf.json
proceedings O
of O
the O
59th O
annual O
meeting O
of O
the O
association O
for O
computational O
linguistics O
and O
the O
11th O
international O
joint O
conference O
on O
natural O
language O
processing O
, O
pages O
41–58 O
august O
1–6 O
, O
2021 O
. O

section 0
id pdf2json/2021.acl-long.4.pdf.json
©2021 O
association O
for O
computational O
linguistics O
41 O

section 1
id pdf2json/2021.acl-long.4.pdf.json
hate O
speech O
detection O
models O
play O
an O
important O
role O
in O
online O
content O
moderation O
and O
enable O
scientific O
analyses O
of O
online O
hate O
more O
generally O
. O

section 1
id pdf2json/2021.acl-long.4.pdf.json
this O
has O
motivated O
much O
research O
in O
nlp O
and O
the O
social O
sciences O
. O

section 1
id pdf2json/2021.acl-long.4.pdf.json
however O
, O
even O
state-of-the-art O
models O
exhibit O
substantial O
weaknesses O
( O
see O
schmidt O
and O
wiegand O
, O
2017 O
; O
fortuna O
and O
nunes O
, O
2018 O
; O
vidgen O
et O
al. O
, O
2019 O
; O
mishra O
et O
al. O
, O
2020 O
, O
for O
reviews O
) O
. O

section 1
id pdf2json/2021.acl-long.4.pdf.json
so O
far O
, O
hate O
speech O
detection O
models O
have O
primarily O
been O
evaluated O
by O
measuring O
held-out O
performance O
on O
a O
small O
set O
of O
widely-used O
hate O
speech O
datasets O
( O
particularly O
waseem O
and O
hovy O
, O
2016 O
; O
davidson O
et O
al. O
, O
2017 O
; O
founta O
et O
al. O
, O
2018 O
) O
, O
but O
recent O
work O
has O
highlighted O
the O
limitations O
of O
this O
evaluation O
paradigm O
. O

section 1
id pdf2json/2021.acl-long.4.pdf.json
aggregate O
performance O
metrics O
offer O
limited O
insight O
into O
specific O
model O
weak- O
nesses O
( O
wu O
et O
al. O
, O
2019 O
) O
. O

section 1
id pdf2json/2021.acl-long.4.pdf.json
further O
, O
if O
there O
are O
systematic O
gaps O
and O
biases O
in O
training O
data O
, O
models O
may O
perform O
deceptively O
well O
on O
corresponding O
held-out O
test O
sets O
by O
learning O
simple O
decision O
rules O
rather O
than O
encoding O
a O
more O
generalisable O
understanding O
of O
the O
task O
( O
e.g O
. O

section 1
id pdf2json/2021.acl-long.4.pdf.json
niven O
and O
kao O
, O
2019 O
; O
geva O
et O
al. O
, O
2019 O
; O
shah O
et O
al. O
, O
2020 O
) O
. O

section 1
id pdf2json/2021.acl-long.4.pdf.json
the O
latter O
issue O
is O
particularly O
relevant O
to O
hate O
speech O
detection O
since O
current O
hate O
speech O
datasets O
vary O
in O
data O
source O
, O
sampling O
strategy O
and O
annotation O
process O
( O
vidgen O
and O
derczynski O
, O
2020 O
; O
poletto O
et O
al. O
, O
2020 O
) O
, O
and O
are O
known O
to O
exhibit O
annotator O
biases O
( O
waseem O
, O
2016 O
; O
waseem O
et O
al. O
, O
2018 O
; O
sap O
et O
al. O
, O
2019 O
) O
as O
well O
as O
topic O
and O
author O
biases O
( O
wiegand O
et O
al. O
, O
2019 O
; O
nejadgholi O
and O
kiritchenko O
, O
2020 O
) O
. O

section 1
id pdf2json/2021.acl-long.4.pdf.json
correspondingly O
, O
models O
trained O
on O
such O
datasets O
have O
been O
shown O
to O
be O
overly O
sensitive O
to O
lexical O
features O
such O
as O
group O
identifiers O
( O
park O
et O
al. O
, O
2018 O
; O
dixon O
et O
al. O
, O
2018 O
; O
kennedy O
et O
al. O
, O
2020 O
) O
, O
and O
to O
generalise O
poorly O
to O
other O
datasets O
( O
nejadgholi O
and O
kiritchenko O
, O
2020 O
; O
samory O
et O
al. O
, O
2020 O
) O
. O

section 1
id pdf2json/2021.acl-long.4.pdf.json
therefore O
, O
held-out O
performance O
on O
current O
hate O
speech O
datasets O
is O
an O
incomplete O
and O
potentially O
misleading O
measure O
of O
model O
quality O
. O

section 1
id pdf2json/2021.acl-long.4.pdf.json
to O
enable O
more O
targeted O
diagnostic O
insights O
, O
we O
introduce O
hatecheck O
, O
a O
suite O
of O
functional O
tests O
for O
hate O
speech O
detection O
models O
. O

section 1
id pdf2json/2021.acl-long.4.pdf.json
functional O
testing O
, O
also O
known O
as O
black-box O
testing O
, O
is O
a O
testing O
framework O
from O
software O
engineering O
that O
assesses O
different O
functionalities O
of O
a O
given O
model O
by O
validating O
its O
output O
on O
sets O
of O
targeted O
test O
cases O
( O
beizer O
, O
1995 O
) O
. O

section 1
id pdf2json/2021.acl-long.4.pdf.json
ribeiro O
et O
al O
. O

section 1
id pdf2json/2021.acl-long.4.pdf.json
( O
2020 O
) O
show O
how O
such O
a O
framework O
can O
be O
used O
for O
structured O
model O
evaluation O
across O
diverse O
nlp O
tasks O
. O

section 1
id pdf2json/2021.acl-long.4.pdf.json
hatecheck O
covers O
29 O
model O
functionalities O
, O
the O
selection O
of O
which O
we O
motivate O
through O
a O
series O
of O
interviews O
with O
civil O
society O
stakeholders O
and O
a O
review O
of O
hate O
speech O
research O
. O

section 1
id pdf2json/2021.acl-long.4.pdf.json
each O
functionality O
is O
tested O
by O
a O
separate O
functional O
test O
. O

section 1
id pdf2json/2021.acl-long.4.pdf.json
we O
create O
18 O
functional O
tests O
corresponding O
to O
distinct O
expressions O
of O
hate O
. O

section 1
id pdf2json/2021.acl-long.4.pdf.json
the O
other O
11 O
functional O
tests O
are O
non-hateful O
contrasts O
to O
the O
hateful O
cases O
. O

section 1
id pdf2json/2021.acl-long.4.pdf.json
for O
example O
, O
we O
test O
non-hateful O
reclaimed O
uses O
of O
slurs O
as O
a O
contrast O
to O
their O
hateful O
use O
. O

section 1
id pdf2json/2021.acl-long.4.pdf.json
such O
tests O
are O
particularly O
challenging O
to O
models O
relying O
on O
overly O
simplistic O
decision O
rules O
and O
thus O
enable O
more O
accurate O
evaluation O
of O
true O
model O
functionalities O
( O
gardner O
et O
al. O
, O
2020 O
) O
. O

section 1
id pdf2json/2021.acl-long.4.pdf.json
for O
each O
functional O
test O
, O
we O
hand-craft O
sets O
of O
targeted O
test O
cases O
with O
clear O
gold O
standard O
labels O
, O
which O
we O
validate O
through O
a O
structured O
annotation O
process.1 O
hatecheck O
is O
broadly O
applicable O
across O
english-language O
hate O
speech O
detection O
models O
. O

section 1
id pdf2json/2021.acl-long.4.pdf.json
we O
demonstrate O
its O
utility O
as O
a O
diagnostic O
tool O
by O
evaluating O
two O
bert O
models O
( O
devlin O
et O
al. O
, O
2019 O
) O
, O
which O
have O
achieved O
near O
state-of-the-art O
performance O
on O
hate O
speech O
datasets O
( O
tran O
et O
al. O
, O
2020 O
) O
, O
as O
well O
as O
two O
commercial O
models O
– O
google O
jigsaw O
’ O
s O
perspective O
and O
two O
hat O
’ O
s O
siftninja.2 O
when O
tested O
with O
hatecheck O
, O
all O
models O
appear O
overly O
sensitive O
to O
specific O
keywords O
such O
as O
slurs O
. O

section 1
id pdf2json/2021.acl-long.4.pdf.json
they O
consistently O
misclassify O
negated O
hate O
, O
counter O
speech O
and O
other O
non-hateful O
contrasts O
to O
hateful O
phrases O
. O

section 1
id pdf2json/2021.acl-long.4.pdf.json
further O
, O
the O
bert O
models O
are O
biased O
in O
their O
performance O
across O
target O
groups O
, O
misclassifying O
more O
content O
directed O
at O
some O
groups O
( O
e.g O
. O

section 1
id pdf2json/2021.acl-long.4.pdf.json
women O
) O
than O
at O
others O
. O

section 1
id pdf2json/2021.acl-long.4.pdf.json
for O
practical O
applications O
such O
as O
content O
moderation O
and O
further O
research O
use O
, O
these O
are O
critical O
model O
weaknesses O
. O

section 1
id pdf2json/2021.acl-long.4.pdf.json
we O
hope O
that O
by O
revealing O
such O
weaknesses O
, O
hatecheck O
can O
play O
a O
key O
role O
in O
the O
development O
of O
better O
hate O
speech O
detection O
models O
. O

section 1
id pdf2json/2021.acl-long.4.pdf.json
definition O
of O
hate O
speech O
we O
draw O
on O
previous O
definitions O
of O
hate O
speech O
( O
warner O
and O
hirschberg O
, O
2012 O
; O
davidson O
et O
al. O
, O
2017 O
) O
as O
well O
as O
recent O
typologies O
of O
abusive O
content O
( O
vidgen O
et O
al. O
, O
2019 O
; O
banko O
et O
al. O
, O
2020 O
) O
to O
define O
hate O
speech O
as O
abuse O
that O
is O
targeted O
at O
a O
protected O
group O
or O
at O
its O
members O
for O
being O
a O
part O
of O
that O
group O
. O

section 1
id pdf2json/2021.acl-long.4.pdf.json
we O
define O
protected O
groups O
based O
on O
age O
, O
disability O
, O
gender O
identity O
, O
familial O
status O
, O
pregnancy O
, O
race O
, O
national O
or O
ethnic O
origins O
, O
religion O
, O
sex O
or O
sexual O
orientation O
, O
which O
broadly O
reflects O
international O
legal O
consensus O
( O
particularly O
the O
uk O
’ O
s O
2010 O
equality O
act O
, O
the O
us O
1964 O
civil O
rights O
act O
and O
the O
eu O
’ O
s O
charter O
of O
fundamental O
rights O
) O
. O

section 1
id pdf2json/2021.acl-long.4.pdf.json
based O
on O
these O
definitions O
, O
we O
approach O
hate O
speech O
detection O
as O
the O
binary O
classification O
of O
content O
as O
either O
hateful O
or O
1all O
hatecheck O
test O
cases O
and O
annotations O
are O
available O
on O
https O
: O
//github.com/paul-rottger/hatecheck-data O
. O

section 1
id pdf2json/2021.acl-long.4.pdf.json
2www.perspectiveapi.com O
and O
www.siftninja.com O
non-hateful O
. O

section 1
id pdf2json/2021.acl-long.4.pdf.json
other O
work O
has O
further O
differentiated O
between O
different O
types O
of O
hate O
and O
non-hate O
( O
e.g O
. O

section 1
id pdf2json/2021.acl-long.4.pdf.json
founta O
et O
al. O
, O
2018 O
; O
salminen O
et O
al. O
, O
2018 O
; O
zampieri O
et O
al. O
, O
2019 O
) O
, O
but O
such O
taxonomies O
can O
be O
collapsed O
into O
a O
binary O
distinction O
and O
are O
thus O
compatible O
with O
hatecheck O
. O

section 1
id pdf2json/2021.acl-long.4.pdf.json
content O
warning O
this O
article O
contains O
examples O
of O
hateful O
and O
abusive O
language O
. O

section 1
id pdf2json/2021.acl-long.4.pdf.json
all O
examples O
are O
taken O
from O
hatecheck O
to O
illustrate O
its O
composition O
. O

section 1
id pdf2json/2021.acl-long.4.pdf.json
examples O
are O
quoted O
verbatim O
, O
except O
for O
hateful O
slurs O
and O
profanity O
, O
for O
which O
the O
first O
vowel O
is O
replaced O
with O
an O
asterisk O
. O

section 3
id pdf2json/2021.acl-long.4.pdf.json
in O
software O
engineering O
, O
a O
program O
has O
a O
certain O
functionality O
if O
it O
meets O
a O
specified O
input/output O
behaviour O
( O
iso/iec/ieee O
24765:2017 O
, O
e O
) O
. O

section 3
id pdf2json/2021.acl-long.4.pdf.json
accordingly O
, O
we O
operationalise O
a O
functionality O
of O
a O
hate O
speech O
detection O
model O
as O
its O
ability O
to O
provide O
a O
specified O
classification O
( O
hateful O
or O
non-hateful O
) O
for O
test O
cases O
in O
a O
corresponding O
functional O
test O
. O

section 3
id pdf2json/2021.acl-long.4.pdf.json
for O
instance O
, O
a O
model O
might O
correctly O
classify O
hate O
expressed O
using O
profanity O
( O
e.g O
“ O
f*ck O
all O
black O
people O
” O
) O
but O
misclassify O
non-hateful O
uses O
of O
profanity O
( O
e.g O
. O

section 3
id pdf2json/2021.acl-long.4.pdf.json
“ O
f*cking O
hell O
, O
what O
a O
day O
” O
) O
, O
which O
is O
why O
we O
test O
them O
as O
separate O
functionalities O
. O

section 3
id pdf2json/2021.acl-long.4.pdf.json
since O
both O
functionalities O
relate O
to O
profanity O
usage O
, O
we O
group O
them O
into O
a O
common O
functionality O
class O
. O

section 4
id pdf2json/2021.acl-long.4.pdf.json
to O
generate O
an O
initial O
list O
of O
59 O
functionalities O
, O
we O
reviewed O
previous O
hate O
speech O
detection O
research O
and O
interviewed O
civil O
society O
stakeholders O
. O

section 4
id pdf2json/2021.acl-long.4.pdf.json
review O
of O
previous O
research O
we O
identified O
different O
types O
of O
hate O
in O
taxonomies O
of O
abusive O
content O
( O
e.g O
. O

section 4
id pdf2json/2021.acl-long.4.pdf.json
zampieri O
et O
al. O
, O
2019 O
; O
banko O
et O
al. O
, O
2020 O
; O
kurrek O
et O
al. O
, O
2020 O
) O
. O

section 4
id pdf2json/2021.acl-long.4.pdf.json
we O
also O
identified O
likely O
model O
weaknesses O
based O
on O
error O
analyses O
( O
e.g O
. O

section 4
id pdf2json/2021.acl-long.4.pdf.json
davidson O
et O
al. O
, O
2017 O
; O
van O
aken O
et O
al. O
, O
2018 O
; O
vidgen O
et O
al. O
, O
2020a O
) O
as O
well O
as O
review O
articles O
and O
commentaries O
( O
e.g O
. O

section 4
id pdf2json/2021.acl-long.4.pdf.json
schmidt O
and O
wiegand O
, O
2017 O
; O
fortuna O
and O
nunes O
, O
2018 O
; O
vidgen O
et O
al. O
, O
2019 O
) O
. O

section 4
id pdf2json/2021.acl-long.4.pdf.json
for O
example O
, O
hate O
speech O
detection O
models O
have O
been O
shown O
to O
struggle O
with O
correctly O
classifying O
negated O
phrases O
such O
as O
“ O
i O
don O
’ O
t O
hate O
trans O
people O
” O
( O
hosseini O
et O
al. O
, O
2017 O
; O
dinan O
et O
al. O
, O
2019 O
) O
. O

section 4
id pdf2json/2021.acl-long.4.pdf.json
we O
therefore O
included O
functionalities O
for O
negation O
in O
hateful O
and O
non-hateful O
content O
. O

section 4
id pdf2json/2021.acl-long.4.pdf.json
interviews O
we O
interviewed O
21 O
employees O
from O
16 O
british O
, O
german O
and O
american O
ngos O
whose O
work O
directly O
relates O
to O
online O
hate O
. O

section 4
id pdf2json/2021.acl-long.4.pdf.json
most O
of O
the O
ngos O
are O
involved O
in O
monitoring O
and O
reporting O
online O
hate O
, O
often O
with O
“ O
trusted O
flagger O
” O
status O
on O
platforms O
such O
as O
twitter O
and O
facebook O
. O

section 4
id pdf2json/2021.acl-long.4.pdf.json
several O
ngos O
provide O
legal O
advocacy O
and O
victim O
support O
or O
otherwise O
represent O
communities O
that O
are O
often O
targeted O
by O
online O
hate O
, O
such O
as O
muslims O
or O
lgbt+ O
people O
. O

section 4
id pdf2json/2021.acl-long.4.pdf.json
the O
vast O
majority O
of O
interviewees O
do O
not O
have O
a O
technical O
background O
, O
but O
extensive O
practical O
experience O
engaging O
with O
online O
hate O
and O
content O
moderation O
systems O
. O

section 4
id pdf2json/2021.acl-long.4.pdf.json
they O
have O
a O
variety O
of O
ethnic O
and O
cultural O
backgrounds O
, O
and O
most O
of O
them O
have O
been O
targeted O
by O
online O
hate O
themselves O
. O

section 4
id pdf2json/2021.acl-long.4.pdf.json
the O
interviews O
were O
semi-structured O
. O

section 4
id pdf2json/2021.acl-long.4.pdf.json
in O
a O
typical O
interview O
, O
we O
would O
first O
ask O
open-ended O
questions O
about O
online O
hate O
( O
e.g O
. O

section 4
id pdf2json/2021.acl-long.4.pdf.json
“ O
what O
do O
you O
think O
are O
the O
biggest O
challenges O
in O
tackling O
online O
hate O
? O
” O
) O
and O
then O
about O
hate O
speech O
detection O
models O
, O
particularly O
their O
perceived O
weaknesses O
( O
e.g O
. O

section 4
id pdf2json/2021.acl-long.4.pdf.json
“ O
what O
sort O
of O
content O
have O
you O
seen O
moderation O
systems O
get O
wrong O
? O
” O
) O
and O
potential O
improvements O
, O
unbounded O
by O
technical O
feasibility O
( O
e.g O
. O

section 4
id pdf2json/2021.acl-long.4.pdf.json
“ O
if O
you O
could O
design O
an O
ideal O
hate O
detection O
system O
, O
what O
would O
it O
be O
able O
to O
do O
? O
” O
) O
. O

section 4
id pdf2json/2021.acl-long.4.pdf.json
using O
a O
grounded O
theory O
approach O
( O
corbin O
and O
strauss O
, O
1990 O
) O
, O
we O
identified O
emergent O
themes O
in O
the O
interview O
responses O
and O
translated O
them O
into O
model O
functionalities O
. O

section 4
id pdf2json/2021.acl-long.4.pdf.json
for O
example O
, O
several O
interviewees O
raised O
concerns O
around O
the O
misclassification O
of O
counter O
speech O
, O
i.e O
. O

section 4
id pdf2json/2021.acl-long.4.pdf.json
direct O
responses O
to O
hateful O
content O
( O
e.g O
. O

section 4
id pdf2json/2021.acl-long.4.pdf.json
i4 O
: O
“ O
people O
will O
be O
quoting O
someone O
, O
calling O
that O
person O
out O
[ O
... O
] O
but O
that O
will O
get O
picked O
up O
by O
the O
system O
” O
) O
.3 O
we O
therefore O
included O
functionalities O
for O
counter O
speech O
that O
quotes O
or O
references O
hate O
. O

section 4
id pdf2json/2021.acl-long.4.pdf.json
selection O
criteria O
from O
the O
initial O
list O
of O
59 O
functionalities O
, O
we O
select O
those O
in O
hatecheck O
based O
on O
two O
practical O
considerations O
. O

section 4
id pdf2json/2021.acl-long.4.pdf.json
first O
, O
we O
restrict O
hatecheck O
’ O
s O
scope O
to O
individual O
english O
language O
text O
documents O
. O

section 4
id pdf2json/2021.acl-long.4.pdf.json
this O
is O
due O
to O
practical O
constraints O
, O
and O
because O
most O
hate O
speech O
detection O
models O
are O
developed O
for O
such O
data O
( O
poletto O
et O
al. O
, O
2020 O
; O
vidgen O
and O
derczynski O
, O
2020 O
) O
. O

section 4
id pdf2json/2021.acl-long.4.pdf.json
thus O
, O
hatecheck O
does O
not O
test O
functionalities O
that O
relate O
to O
other O
modalities O
( O
e.g O
. O

section 4
id pdf2json/2021.acl-long.4.pdf.json
images O
) O
3when O
quoting O
anonymised O
responses O
throughout O
this O
article O
, O
we O
identify O
each O
interview O
participant O
by O
a O
unique O
id O
. O

section 4
id pdf2json/2021.acl-long.4.pdf.json
we O
can O
not O
release O
full O
interview O
transcripts O
due O
to O
the O
sensitive O
nature O
of O
work O
in O
this O
area O
, O
the O
confidentiality O
terms O
agreed O
with O
our O
participants O
and O
our O
ethics O
clearance O
. O

section 4
id pdf2json/2021.acl-long.4.pdf.json
or O
languages O
, O
or O
that O
require O
context O
( O
e.g O
. O

section 4
id pdf2json/2021.acl-long.4.pdf.json
conversational O
or O
social O
) O
beyond O
individual O
documents O
. O

section 4
id pdf2json/2021.acl-long.4.pdf.json
second O
, O
we O
only O
test O
functionalities O
for O
which O
we O
can O
construct O
test O
cases O
with O
clear O
gold O
standard O
labels O
. O

section 4
id pdf2json/2021.acl-long.4.pdf.json
therefore O
, O
we O
do O
not O
test O
functionalities O
that O
lack O
broad O
consensus O
in O
our O
interviews O
and O
the O
literature O
regarding O
what O
is O
and O
is O
not O
hateful O
. O

section 4
id pdf2json/2021.acl-long.4.pdf.json
the O
use O
of O
humour O
, O
for O
instance O
, O
has O
been O
highlighted O
as O
an O
important O
challenge O
for O
hate O
speech O
research O
( O
van O
aken O
et O
al. O
, O
2018 O
; O
qian O
et O
al. O
, O
2018 O
; O
vidgen O
et O
al. O
, O
2020a O
) O
. O

section 4
id pdf2json/2021.acl-long.4.pdf.json
however O
, O
whether O
humorous O
statements O
are O
hateful O
is O
heavily O
contingent O
on O
normative O
claims O
( O
e.g O
. O

section 4
id pdf2json/2021.acl-long.4.pdf.json
i5 O
: O
“ O
it O
’ O
s O
a O
value O
judgment O
thing O
” O
) O
, O
which O
is O
why O
we O
do O
not O
test O
them O
in O
hatecheck O
. O

section 5
id pdf2json/2021.acl-long.4.pdf.json
hatecheck O
comprises O
29 O
functional O
tests O
grouped O
into O
11 O
classes O
. O

section 5
id pdf2json/2021.acl-long.4.pdf.json
each O
test O
evaluates O
one O
functionality O
and O
is O
associated O
with O
one O
gold O
standard O
label O
( O
hateful O
or O
non-hateful O
) O
. O

section 5
id pdf2json/2021.acl-long.4.pdf.json
each O
functional O
test O
has O
a O
set O
of O
corresponding O
test O
cases O
. O

section 5
id pdf2json/2021.acl-long.4.pdf.json
18 O
functional O
tests O
for O
hateful O
content O
in O
hatecheck O
cover O
distinct O
expressions O
of O
hate O
. O

section 5
id pdf2json/2021.acl-long.4.pdf.json
they O
are O
distinct O
in O
the O
sense O
that O
we O
minimise O
overlap O
between O
them O
, O
for O
instance O
by O
testing O
slurs O
( O
“ O
f*g O
” O
) O
and O
profanity O
( O
“ O
f*ck O
” O
) O
in O
separate O
functional O
tests O
rather O
than O
jointly O
( O
“ O
f*cking O
f*g O
” O
) O
, O
so O
that O
each O
test O
isolates O
one O
particular O
type O
of O
expression O
. O

section 5
id pdf2json/2021.acl-long.4.pdf.json
the O
other O
11 O
functional O
tests O
for O
non-hateful O
content O
cover O
contrastive O
non-hate O
, O
i.e O
. O

section 5
id pdf2json/2021.acl-long.4.pdf.json
content O
which O
shares O
linguistic O
features O
with O
hateful O
expressions O
. O

section 5
id pdf2json/2021.acl-long.4.pdf.json
the O
challenges O
posed O
by O
such O
content O
are O
a O
key O
theme O
in O
our O
interviews O
and O
the O
literature O
. O

section 5
id pdf2json/2021.acl-long.4.pdf.json
we O
construct O
every O
non-hateful O
test O
case O
as O
a O
direct O
contrast O
to O
a O
hateful O
test O
case O
, O
making O
only O
minimal O
changes O
. O

section 5
id pdf2json/2021.acl-long.4.pdf.json
for O
instance O
, O
“ O
i O
love O
immigrants O
” O
is O
a O
test O
case O
in O
f19 O
: O
positive O
statements O
using O
a O
protected O
group O
identifier O
. O

section 5
id pdf2json/2021.acl-long.4.pdf.json
it O
directly O
contrasts O
the O
test O
case O
“ O
i O
hate O
immigrants O
” O
in O
f1 O
: O
strong O
negative O
emotions O
explicitly O
expressed O
about O
a O
protected O
group O
. O

section 5
id pdf2json/2021.acl-long.4.pdf.json
in O
the O
following O
, O
we O
give O
a O
brief O
overview O
of O
the O
different O
functional O
tests O
in O
hatecheck O
. O

section 5
id pdf2json/2021.acl-long.4.pdf.json
table O
1 O
provides O
corresponding O
example O
test O
cases O
. O

section 5
id pdf2json/2021.acl-long.4.pdf.json
each O
individual O
test O
is O
grounded O
in O
direct O
references O
to O
previous O
work O
and/or O
our O
interviews O
. O

section 5
id pdf2json/2021.acl-long.4.pdf.json
these O
references O
are O
detailed O
in O
appendix O
b O
. O

section 5
id pdf2json/2021.acl-long.4.pdf.json
distinct O
expressions O
of O
hate O
hatecheck O
tests O
different O
types O
of O
derogatory O
hate O
speech O
( O
f1-4 O
) O
and O
hate O
expressed O
through O
threatening O
language O
( O
f5/6 O
) O
. O

section 5
id pdf2json/2021.acl-long.4.pdf.json
it O
tests O
hate O
expressed O
using O
slurs O
( O
f7 O
) O
and O
profanity O
( O
f10 O
) O
. O

section 5
id pdf2json/2021.acl-long.4.pdf.json
it O
also O
tests O
hate O
expressed O
through O
pronoun O
reference O
( O
f12/13 O
) O
, O
negation O
( O
f14 O
) O
and O
phrasing O
variants O
, O
specifically O
questions O
and O
opinions O
( O
f16/17 O
) O
. O

section 5
id pdf2json/2021.acl-long.4.pdf.json
lastly O
, O
it O
tests O
hate O
containing O
spelling O
variations O
such O
as O
missing O
characters O
or O
leet O
speak O
( O
f25-29 O
) O
. O

section 5
id pdf2json/2021.acl-long.4.pdf.json
contrastive O
non-hate O
hatecheck O
tests O
non-hateful O
contrasts O
for O
slurs O
, O
particularly O
slur O
homonyms O
and O
reclaimed O
slurs O
( O
f8/9 O
) O
, O
as O
well O
as O
for O
profanity O
( O
f11 O
) O
. O

section 5
id pdf2json/2021.acl-long.4.pdf.json
it O
tests O
nonhateful O
contrasts O
that O
use O
negation O
, O
i.e O
. O

section 5
id pdf2json/2021.acl-long.4.pdf.json
negated O
hate O
( O
f15 O
) O
. O

section 5
id pdf2json/2021.acl-long.4.pdf.json
it O
also O
tests O
non-hateful O
contrasts O
around O
protected O
group O
identifiers O
( O
f18/19 O
) O
. O

section 5
id pdf2json/2021.acl-long.4.pdf.json
it O
tests O
contrasts O
in O
which O
hate O
speech O
is O
quoted O
or O
referenced O
to O
non-hateful O
effect O
, O
specifically O
counter O
speech O
, O
i.e O
. O

section 5
id pdf2json/2021.acl-long.4.pdf.json
direct O
responses O
to O
hate O
speech O
which O
seek O
to O
act O
against O
it O
( O
f20/21 O
) O
. O

section 5
id pdf2json/2021.acl-long.4.pdf.json
lastly O
, O
it O
tests O
non-hateful O
contrasts O
which O
target O
out-of-scope O
entities O
such O
as O
objects O
( O
f22-24 O
) O
rather O
than O
a O
protected O
group O
. O

section 6
id pdf2json/2021.acl-long.4.pdf.json
for O
each O
functionality O
in O
hatecheck O
, O
we O
handcraft O
sets O
of O
test O
cases O
– O
short O
english-language O
text O
documents O
that O
clearly O
correspond O
to O
just O
one O
gold O
standard O
label O
. O

section 6
id pdf2json/2021.acl-long.4.pdf.json
within O
each O
functionality O
, O
we O
aim O
to O
use O
diverse O
vocabulary O
and O
syntax O
to O
reduce O
similarity O
between O
test O
cases O
, O
which O
zhou O
et O
al O
. O

section 6
id pdf2json/2021.acl-long.4.pdf.json
( O
2020 O
) O
suggest O
as O
a O
likely O
cause O
of O
performance O
instability O
for O
diagnostic O
datasets O
. O

section 6
id pdf2json/2021.acl-long.4.pdf.json
to O
generate O
test O
cases O
at O
scale O
, O
we O
use O
templates O
( O
dixon O
et O
al. O
, O
2018 O
; O
garg O
et O
al. O
, O
2019 O
; O
ribeiro O
et O
al. O
, O
2020 O
) O
, O
in O
which O
we O
replace O
tokens O
for O
protected O
group O
identifiers O
( O
e.g O
. O

section 6
id pdf2json/2021.acl-long.4.pdf.json
“ O
i O
hate O
[ O
identity O
] O
. O
” O
) O
and O
slurs O
( O
e.g O
. O

section 6
id pdf2json/2021.acl-long.4.pdf.json
“ O
you O
are O
just O
a O
[ O
slur O
] O
to O
me. O
” O
) O
. O

section 6
id pdf2json/2021.acl-long.4.pdf.json
this O
also O
ensures O
that O
hatecheck O
has O
an O
equal O
number O
of O
cases O
targeted O
at O
different O
protected O
groups O
. O

section 6
id pdf2json/2021.acl-long.4.pdf.json
hatecheck O
covers O
seven O
protected O
groups O
: O
women O
( O
gender O
) O
, O
trans O
people O
( O
gender O
identity O
) O
, O
gay O
people O
( O
sexual O
orientation O
) O
, O
black O
people O
( O
race O
) O
, O
disabled O
people O
( O
disability O
) O
, O
muslims O
( O
religion O
) O
and O
immigrants O
( O
national O
origin O
) O
. O

section 6
id pdf2json/2021.acl-long.4.pdf.json
for O
details O
on O
which O
slurs O
are O
covered O
by O
hatecheck O
and O
how O
they O
were O
selected O
, O
see O
appendix O
c. O
in O
total O
, O
we O
generate O
3,901 O
cases O
, O
3,495 O
of O
which O
come O
from O
460 O
templates O
. O

section 6
id pdf2json/2021.acl-long.4.pdf.json
the O
other O
406 O
cases O
do O
not O
use O
template O
tokens O
( O
e.g O
. O

section 6
id pdf2json/2021.acl-long.4.pdf.json
“ O
sh*t O
, O
i O
forgot O
my O
keys O
” O
) O
and O
are O
thus O
crafted O
individually O
. O

section 6
id pdf2json/2021.acl-long.4.pdf.json
the O
average O
length O
of O
cases O
is O
8.87 O
words O
( O
std O
. O

section 6
id pdf2json/2021.acl-long.4.pdf.json
dev O
. O

section 6
id pdf2json/2021.acl-long.4.pdf.json
= O
3.33 O
) O
or O
48.26 O
characters O
( O
std O
. O

section 6
id pdf2json/2021.acl-long.4.pdf.json
dev O
. O

section 6
id pdf2json/2021.acl-long.4.pdf.json
= O
16.88 O
) O
. O

section 6
id pdf2json/2021.acl-long.4.pdf.json
2,659 O
of O
the O
3,901 O
cases O
( O
68.2 O
% O
) O
are O
hateful O
and O
1,242 O
( O
31.8 O
% O
) O
are O
non-hateful O
. O

section 6
id pdf2json/2021.acl-long.4.pdf.json
secondary O
labels O
in O
addition O
to O
the O
primary O
label O
( O
hateful O
or O
non-hateful O
) O
we O
provide O
up O
to O
two O
secondary O
labels O
for O
all O
cases O
. O

section 6
id pdf2json/2021.acl-long.4.pdf.json
for O
cases O
targeted O
at O
or O
referencing O
a O
particular O
protected O
group O
, O
we O
provide O
a O
label O
for O
the O
group O
that O
is O
targeted O
. O

section 6
id pdf2json/2021.acl-long.4.pdf.json
for O
hateful O
cases O
, O
we O
also O
label O
whether O
they O
are O
targeted O
at O
a O
group O
in O
general O
or O
at O
individuals O
, O
which O
is O
a O
common O
distinction O
in O
taxonomies O
of O
abuse O
( O
e.g O
. O

section 6
id pdf2json/2021.acl-long.4.pdf.json
waseem O
et O
al. O
, O
2017 O
; O
zampieri O
et O
al. O
, O
2019 O
) O
. O

section 7
id pdf2json/2021.acl-long.4.pdf.json
to O
validate O
gold O
standard O
primary O
labels O
of O
test O
cases O
in O
hatecheck O
, O
we O
recruited O
and O
trained O
ten O
annotators.4 O
in O
addition O
to O
the O
binary O
annotation O
task O
, O
we O
also O
gave O
annotators O
the O
option O
to O
flag O
cases O
as O
unrealistic O
( O
e.g O
. O

section 7
id pdf2json/2021.acl-long.4.pdf.json
nonsensical O
) O
to O
further O
confirm O
data O
quality O
. O

section 7
id pdf2json/2021.acl-long.4.pdf.json
each O
annotator O
was O
randomly O
assigned O
approximately O
2,000 O
test O
cases O
, O
so O
that O
each O
of O
the O
3,901 O
cases O
was O
annotated O
by O
exactly O
five O
annotators O
. O

section 7
id pdf2json/2021.acl-long.4.pdf.json
we O
use O
fleiss O
’ O
kappa O
to O
measure O
inter-annotator O
agreement O
( O
hallgren O
, O
2012 O
) O
and O
obtain O
a O
score O
of O
0.93 O
, O
which O
indicates O
“ O
almost O
perfect O
” O
agreement O
( O
landis O
and O
koch O
, O
1977 O
) O
. O

section 7
id pdf2json/2021.acl-long.4.pdf.json
for O
3,879 O
( O
99.4 O
% O
) O
of O
the O
3,901 O
cases O
, O
at O
least O
four O
out O
of O
five O
annotators O
agreed O
with O
our O
gold O
standard O
label O
. O

section 7
id pdf2json/2021.acl-long.4.pdf.json
for O
22 O
cases O
, O
agreement O
was O
less O
than O
four O
out O
of O
five O
. O

section 7
id pdf2json/2021.acl-long.4.pdf.json
to O
ensure O
that O
the O
label O
of O
each O
hatecheck O
case O
is O
unambiguous O
, O
we O
exclude O
these O
22 O
cases O
. O

section 7
id pdf2json/2021.acl-long.4.pdf.json
we O
also O
exclude O
all O
cases O
generated O
from O
the O
same O
templates O
as O
these O
22 O
cases O
to O
avoid O
biases O
in O
target O
coverage O
, O
as O
otherwise O
hate O
against O
some O
protected O
groups O
would O
be O
less O
well O
represented O
than O
hate O
against O
others O
. O

section 7
id pdf2json/2021.acl-long.4.pdf.json
in O
total O
, O
we O
exclude O
173 O
cases O
, O
reducing O
the O
size O
of O
the O
dataset O
to O
3,728 O
test O
cases.5 O
only O
23 O
cases O
were O
flagged O
as O
unrealistic O
by O
one O
annotator O
, O
and O
none O
were O
flagged O
by O
more O
than O
one O
annotator O
. O

section 7
id pdf2json/2021.acl-long.4.pdf.json
thus O
, O
we O
do O
not O
exclude O
any O
test O
cases O
for O
being O
unrealistic O
. O

section 9
id pdf2json/2021.acl-long.4.pdf.json
as O
a O
suite O
of O
black-box O
tests O
, O
hatecheck O
is O
broadly O
applicable O
across O
english-language O
hate O
speech O
detection O
models O
. O

section 9
id pdf2json/2021.acl-long.4.pdf.json
users O
can O
compare O
different O
architectures O
trained O
on O
different O
datasets O
and O
even O
commercial O
models O
for O
which O
public O
information O
on O
architecture O
and O
training O
data O
is O
limited O
. O

section 9
id pdf2json/2021.acl-long.4.pdf.json
4for O
information O
on O
annotator O
training O
, O
their O
background O
and O
demographics O
, O
see O
the O
data O
statement O
in O
appendix O
a O
. O

section 9
id pdf2json/2021.acl-long.4.pdf.json
5we O
make O
data O
on O
annotation O
outcomes O
available O
for O
all O
cases O
we O
generated O
, O
including O
the O
ones O
not O
in O
hatecheck O
. O

section 9
id pdf2json/2021.acl-long.4.pdf.json
( O
2,563 O
in O
18 O
functional O
tests O
) O
are O
labelled O
hateful O
, O
31.2 O
% O
( O
1,165 O
in O
11 O
functional O
tests O
) O
are O
labelled O
non-hateful O
. O

section 9
id pdf2json/2021.acl-long.4.pdf.json
the O
right-most O
columns O
report O
accuracy O
( O
% O
) O
on O
each O
functional O
test O
for O
the O
models O
described O
in O
§3.1 O
. O

section 9
id pdf2json/2021.acl-long.4.pdf.json
best O
performance O
on O
each O
functional O
test O
is O
bolded O
. O

section 9
id pdf2json/2021.acl-long.4.pdf.json
below O
random O
choice O
performance O
( O
< O
50 O
% O
) O
is O
highlighted O
in O
cursive O
red O
. O

section 9
id pdf2json/2021.acl-long.4.pdf.json
pre-trained O
transformer O
models O
we O
test O
an O
uncased O
bert-base O
model O
( O
devlin O
et O
al. O
, O
2019 O
) O
, O
which O
has O
been O
shown O
to O
achieve O
near O
state-of-theart O
performance O
on O
several O
abuse O
detection O
tasks O
( O
tran O
et O
al. O
, O
2020 O
) O
. O

section 9
id pdf2json/2021.acl-long.4.pdf.json
we O
fine-tune O
bert O
on O
two O
widely-used O
hate O
speech O
datasets O
from O
davidson O
et O
al O
. O

section 9
id pdf2json/2021.acl-long.4.pdf.json
( O
2017 O
) O
and O
founta O
et O
al O
. O

section 9
id pdf2json/2021.acl-long.4.pdf.json
( O
2018 O
) O
. O

section 9
id pdf2json/2021.acl-long.4.pdf.json
the O
davidson O
et O
al O
. O

section 9
id pdf2json/2021.acl-long.4.pdf.json
( O
2017 O
) O
dataset O
contains O
24,783 O
tweets O
annotated O
as O
either O
hateful O
, O
offensive O
or O
neither O
. O

section 9
id pdf2json/2021.acl-long.4.pdf.json
the O
founta O
et O
al O
. O

section 9
id pdf2json/2021.acl-long.4.pdf.json
( O
2018 O
) O
dataset O
comprises O
99,996 O
tweets O
annotated O
as O
hateful O
, O
abusive O
, O
spam O
and O
normal O
. O

section 9
id pdf2json/2021.acl-long.4.pdf.json
for O
both O
datasets O
, O
we O
collapse O
labels O
other O
than O
hateful O
into O
a O
single O
non-hateful O
label O
to O
match O
hatecheck O
’ O
s O
binary O
format O
. O

section 9
id pdf2json/2021.acl-long.4.pdf.json
this O
is O
aligned O
with O
the O
original O
multi-label O
setup O
of O
the O
two O
datasets O
. O

section 9
id pdf2json/2021.acl-long.4.pdf.json
davidson O
et O
al O
. O

section 9
id pdf2json/2021.acl-long.4.pdf.json
( O
2017 O
) O
, O
for O
instance O
, O
explicitly O
characterise O
offensive O
content O
in O
their O
dataset O
as O
non-hateful O
. O

section 9
id pdf2json/2021.acl-long.4.pdf.json
respectively O
, O
hateful O
cases O
make O
up O
5.8 O
% O
and O
5.0 O
% O
of O
the O
datasets O
. O

section 9
id pdf2json/2021.acl-long.4.pdf.json
details O
on O
both O
datasets O
and O
pre-processing O
steps O
can O
be O
found O
in O
appendix O
d. O
in O
the O
following O
, O
we O
denote O
bert O
fine-tuned O
on O
binary O
davidson O
et O
al O
. O

section 9
id pdf2json/2021.acl-long.4.pdf.json
( O
2017 O
) O
data O
by O
b-d O
and O
bert O
fine-tuned O
on O
binary O
founta O
et O
al O
. O

section 9
id pdf2json/2021.acl-long.4.pdf.json
( O
2018 O
) O
data O
by O
b-f. O
to O
account O
for O
class O
imbalance O
, O
we O
use O
class O
weights O
emphasising O
the O
hateful O
minority O
class O
( O
he O
and O
garcia O
, O
2009 O
) O
. O

section 9
id pdf2json/2021.acl-long.4.pdf.json
for O
both O
datasets O
, O
we O
use O
a O
stratified O
80/10/10 O
train/dev/test O
split O
. O

section 9
id pdf2json/2021.acl-long.4.pdf.json
macro O
f1 O
on O
the O
held-out O
test O
sets O
is O
70.8 O
for O
b-d O
and O
70.3 O
for O
b-f.6 O
details O
on O
model O
training O
and O
parameters O
can O
be O
found O
in O
appendix O
e. O
commercial O
models O
we O
test O
google O
jigsaw O
’ O
s O
perspective O
( O
p O
) O
and O
two O
hat O
’ O
s O
siftninja O
( O
sn O
) O
.7 O
both O
are O
popular O
models O
for O
content O
moderation O
developed O
by O
major O
tech O
companies O
that O
can O
be O
accessed O
by O
registered O
users O
via O
an O
api O
. O

section 9
id pdf2json/2021.acl-long.4.pdf.json
for O
a O
given O
input O
text O
, O
p O
provides O
percentage O
scores O
across O
attributes O
such O
as O
“ O
toxicity O
” O
and O
“ O
profanity O
” O
. O

section 9
id pdf2json/2021.acl-long.4.pdf.json
we O
use O
“ O
identity O
attack O
” O
, O
which O
aims O
at O
identifying O
“ O
negative O
or O
hateful O
comments O
targeting O
someone O
because O
of O
their O
identity O
” O
and O
thus O
aligns O
closely O
with O
our O
definition O
of O
hate O
speech O
( O
§1 O
) O
. O

section 9
id pdf2json/2021.acl-long.4.pdf.json
we O
convert O
the O
percentage O
score O
to O
a O
binary O
label O
using O
a O
cutoff O
of O
50 O
% O
. O

section 9
id pdf2json/2021.acl-long.4.pdf.json
we O
tested O
p O
in O
december O
2020 O
. O

section 9
id pdf2json/2021.acl-long.4.pdf.json
for O
sn O
, O
we O
use O
its O
‘ O
hate O
speech O
’ O
attribute O
( O
“ O
attacks O
[ O
on O
] O
a O
person O
or O
group O
on O
the O
basis O
of O
personal O
6for O
better O
comparability O
to O
previous O
work O
, O
we O
also O
finetuned O
unweighted O
versions O
of O
our O
models O
on O
the O
original O
multiclass O
d O
and O
f O
data O
. O

section 9
id pdf2json/2021.acl-long.4.pdf.json
their O
performance O
matches O
sota O
results O
( O
mozafari O
et O
al. O
, O
2019 O
; O
cao O
et O
al. O
, O
2020 O
) O
. O

section 9
id pdf2json/2021.acl-long.4.pdf.json
details O
in O
appx O
. O

section 9
id pdf2json/2021.acl-long.4.pdf.json
f. O
7www.perspectiveapi.com O
and O
www.siftninja.com O
attributes O
or O
identities O
” O
) O
, O
which O
distinguishes O
between O
‘ O
mild O
’ O
, O
‘ O
bad O
’ O
, O
‘ O
severe O
’ O
and O
‘ O
no O
’ O
hate O
. O

section 9
id pdf2json/2021.acl-long.4.pdf.json
we O
mark O
all O
but O
‘ O
no O
’ O
hate O
as O
‘ O
hateful O
’ O
to O
obtain O
binary O
labels O
. O

section 9
id pdf2json/2021.acl-long.4.pdf.json
we O
tested O
sn O
in O
january O
2021 O
. O

section 10
id pdf2json/2021.acl-long.4.pdf.json
we O
assess O
model O
performance O
on O
hatecheck O
using O
accuracy O
, O
i.e O
. O

section 10
id pdf2json/2021.acl-long.4.pdf.json
the O
proportion O
of O
correctly O
classified O
test O
cases O
. O

section 10
id pdf2json/2021.acl-long.4.pdf.json
when O
reporting O
accuracy O
in O
tables O
, O
we O
bolden O
the O
best O
performance O
across O
models O
and O
highlight O
performance O
below O
a O
random O
choice O
baseline O
, O
i.e O
. O

section 10
id pdf2json/2021.acl-long.4.pdf.json
50 O
% O
for O
our O
binary O
task O
, O
in O
cursive O
red O
. O

section 10
id pdf2json/2021.acl-long.4.pdf.json
performance O
across O
labels O
all O
models O
show O
clear O
performance O
deficits O
when O
tested O
on O
hateful O
and O
non-hateful O
cases O
in O
hatecheck O
( O
table O
2 O
) O
. O

section 10
id pdf2json/2021.acl-long.4.pdf.json
b-d O
, O
b-f O
and O
p O
are O
relatively O
more O
accurate O
on O
hateful O
cases O
but O
misclassify O
most O
non-hateful O
cases O
. O

section 10
id pdf2json/2021.acl-long.4.pdf.json
in O
total O
, O
p O
performs O
best O
. O

section 10
id pdf2json/2021.acl-long.4.pdf.json
sn O
performs O
worst O
and O
is O
strongly O
biased O
towards O
classifying O
all O
cases O
as O
non-hateful O
, O
making O
it O
highly O
accurate O
on O
nonhateful O
cases O
but O
misclassify O
most O
hateful O
cases O
. O

section 10
id pdf2json/2021.acl-long.4.pdf.json
performance O
across O
functional O
tests O
evaluating O
models O
on O
each O
functional O
test O
( O
table O
1 O
) O
reveals O
specific O
model O
weaknesses O
. O

section 10
id pdf2json/2021.acl-long.4.pdf.json
b-d O
and O
b-f O
, O
respectively O
, O
are O
less O
than O
50 O
% O
accurate O
on O
8 O
and O
4 O
out O
of O
the O
11 O
functional O
tests O
for O
non-hate O
in O
hatecheck O
. O

section 10
id pdf2json/2021.acl-long.4.pdf.json
in O
particular O
, O
the O
models O
misclassify O
most O
cases O
of O
reclaimed O
slurs O
( O
f9 O
, O
39.5 O
% O
and O
33.3 O
% O
correct O
) O
, O
negated O
hate O
( O
f15 O
, O
12.8 O
% O
and O
12.0 O
% O
correct O
) O
and O
counter O
speech O
( O
f20/21 O
, O
26.6 O
% O
/29.1 O
% O
and O
32.9 O
% O
/29.8 O
% O
correct O
) O
. O

section 10
id pdf2json/2021.acl-long.4.pdf.json
b-d O
is O
slightly O
more O
accurate O
than O
b-f O
on O
most O
functional O
tests O
for O
hate O
while O
b-f O
is O
more O
accurate O
on O
most O
tests O
for O
non-hate O
. O

section 10
id pdf2json/2021.acl-long.4.pdf.json
both O
models O
generally O
do O
better O
on O
hateful O
than O
non-hateful O
cases O
, O
although O
they O
struggle O
, O
for O
instance O
, O
with O
spelling O
variations O
, O
particularly O
added O
spaces O
between O
characters O
( O
f28 O
, O
43.9 O
% O
and O
37.6 O
% O
correct O
) O
and O
leet O
speak O
spellings O
( O
f29 O
, O
48.0 O
% O
and O
43.9 O
% O
correct O
) O
. O

section 10
id pdf2json/2021.acl-long.4.pdf.json
p O
performs O
better O
than O
b-d O
and O
b-f O
on O
most O
functional O
tests O
. O

section 10
id pdf2json/2021.acl-long.4.pdf.json
it O
is O
over O
95 O
% O
accurate O
on O
11 O
out O
of O
18 O
functional O
tests O
for O
hate O
and O
substantially O
more O
accurate O
than O
b-d O
and O
b-f O
on O
spelling O
variations O
( O
f25-29 O
) O
. O

section 10
id pdf2json/2021.acl-long.4.pdf.json
however O
, O
it O
performs O
even O
worse O
than O
b-d O
and O
b-f O
on O
non-hateful O
functional O
tests O
for O
reclaimed O
slurs O
( O
f9 O
, O
28.4 O
% O
correct O
) O
, O
negated O
hate O
( O
f15 O
, O
3.8 O
% O
correct O
) O
and O
counter O
speech O
( O
f20/21 O
, O
15.6 O
% O
/18.4 O
% O
correct O
) O
. O

section 10
id pdf2json/2021.acl-long.4.pdf.json
due O
to O
its O
bias O
towards O
classifying O
all O
cases O
as O
non-hateful O
, O
sn O
misclassifies O
most O
hateful O
cases O
and O
is O
near-perfectly O
accurate O
on O
non-hateful O
functional O
tests O
. O

section 10
id pdf2json/2021.acl-long.4.pdf.json
exceptions O
to O
the O
latter O
are O
counter O
speech O
( O
f20/21 O
, O
79.8 O
% O
/79.4 O
% O
correct O
) O
and O
nonhateful O
slur O
usage O
( O
f8/9 O
, O
33.3 O
% O
/18.5 O
% O
correct O
) O
. O

section 10
id pdf2json/2021.acl-long.4.pdf.json
performance O
on O
individual O
functional O
tests O
individual O
functional O
tests O
can O
be O
investigated O
further O
to O
show O
more O
granular O
model O
weaknesses O
. O

section 10
id pdf2json/2021.acl-long.4.pdf.json
to O
illustrate O
, O
table O
3 O
reports O
model O
accuracy O
on O
test O
cases O
for O
non-hateful O
reclaimed O
slurs O
( O
f9 O
) O
grouped O
by O
the O
reclaimed O
slur O
that O
is O
used O
. O

section 10
id pdf2json/2021.acl-long.4.pdf.json
claimed O
slurs O
( O
f9 O
, O
non-hateful O
) O
by O
which O
slur O
is O
used O
. O

section 10
id pdf2json/2021.acl-long.4.pdf.json
performance O
varies O
across O
models O
and O
is O
strikingly O
poor O
on O
individual O
slurs O
. O

section 10
id pdf2json/2021.acl-long.4.pdf.json
b-dmisclassifies O
all O
instances O
of O
“ O
f*g O
” O
, O
“ O
f*ggot O
” O
and O
“ O
q*eer O
” O
. O

section 10
id pdf2json/2021.acl-long.4.pdf.json
b-f O
and O
p O
perform O
better O
for O
“ O
q*eer O
” O
, O
but O
fail O
on O
“ O
n*gga O
” O
. O

section 10
id pdf2json/2021.acl-long.4.pdf.json
sn O
fails O
on O
all O
cases O
but O
reclaimed O
uses O
of O
“ O
b*tch O
” O
. O

section 10
id pdf2json/2021.acl-long.4.pdf.json
performance O
across O
target O
groups O
hatecheck O
can O
test O
whether O
models O
exhibit O
‘ O
unintended O
biases O
’ O
( O
dixon O
et O
al. O
, O
2018 O
) O
by O
comparing O
their O
performance O
on O
cases O
which O
target O
different O
groups O
. O

section 10
id pdf2json/2021.acl-long.4.pdf.json
to O
illustrate O
, O
table O
4 O
shows O
model O
accuracy O
on O
all O
test O
cases O
created O
from O
[ O
identity O
] O
templates O
, O
which O
only O
differ O
in O
the O
group O
identifier O
. O

section 10
id pdf2json/2021.acl-long.4.pdf.json
b-d O
misclassifies O
test O
cases O
targeting O
women O
twice O
as O
often O
as O
those O
targeted O
at O
other O
groups O
. O

section 10
id pdf2json/2021.acl-long.4.pdf.json
b-f O
also O
performs O
relatively O
worse O
for O
women O
and O
fails O
on O
most O
test O
cases O
targeting O
disabled O
people O
. O

section 10
id pdf2json/2021.acl-long.4.pdf.json
by O
contrast O
, O
p O
is O
consistently O
around O
80 O
% O
and O
sn O
around O
25 O
% O
accurate O
across O
target O
groups O
. O

section 11
id pdf2json/2021.acl-long.4.pdf.json
hatecheck O
reveals O
functional O
weaknesses O
in O
all O
four O
models O
that O
we O
test O
. O

section 11
id pdf2json/2021.acl-long.4.pdf.json
first O
, O
all O
models O
are O
overly O
sensitive O
to O
specific O
keywords O
in O
at O
least O
some O
contexts O
. O

section 11
id pdf2json/2021.acl-long.4.pdf.json
b-d O
, O
b-f O
and O
p O
perform O
well O
for O
both O
hateful O
and O
non-hateful O
cases O
of O
profanity O
( O
f10/11 O
) O
, O
which O
shows O
that O
they O
can O
distinguish O
between O
different O
uses O
of O
certain O
profanity O
terms O
. O

section 11
id pdf2json/2021.acl-long.4.pdf.json
however O
, O
all O
models O
perform O
very O
poorly O
on O
reclaimed O
slurs O
( O
f9 O
) O
compared O
to O
hateful O
slurs O
( O
f7 O
) O
. O

section 11
id pdf2json/2021.acl-long.4.pdf.json
thus O
, O
it O
appears O
that O
the O
models O
to O
some O
extent O
encode O
overly O
simplistic O
keyword-based O
decision O
rules O
( O
e.g O
. O

section 11
id pdf2json/2021.acl-long.4.pdf.json
that O
slurs O
are O
hateful O
) O
rather O
than O
capturing O
the O
relevant O
linguistic O
phenomena O
( O
e.g O
. O

section 11
id pdf2json/2021.acl-long.4.pdf.json
that O
slurs O
can O
have O
non-hateful O
reclaimed O
uses O
) O
. O

section 11
id pdf2json/2021.acl-long.4.pdf.json
second O
, O
b-d O
, O
b-f O
and O
p O
struggle O
with O
nonhateful O
contrasts O
to O
hateful O
phrases O
. O

section 11
id pdf2json/2021.acl-long.4.pdf.json
in O
particular O
, O
they O
misclassify O
most O
cases O
of O
negated O
hate O
( O
f15 O
) O
and O
counter O
speech O
( O
f20/21 O
) O
. O

section 11
id pdf2json/2021.acl-long.4.pdf.json
thus O
, O
they O
appear O
to O
not O
sufficiently O
register O
linguistic O
signals O
that O
reframe O
hateful O
phrases O
into O
clearly O
non-hateful O
ones O
( O
e.g O
. O

section 11
id pdf2json/2021.acl-long.4.pdf.json
“ O
no O
muslim O
deserves O
to O
die O
” O
) O
. O

section 11
id pdf2json/2021.acl-long.4.pdf.json
third O
, O
b-d O
and O
b-f O
are O
biased O
in O
their O
target O
coverage O
, O
classifying O
hate O
directed O
against O
some O
protected O
groups O
( O
e.g O
. O

section 11
id pdf2json/2021.acl-long.4.pdf.json
women O
) O
less O
accurately O
than O
equivalent O
cases O
directed O
at O
others O
( O
table O
4 O
) O
. O

section 11
id pdf2json/2021.acl-long.4.pdf.json
for O
practical O
applications O
such O
as O
content O
moderation O
, O
these O
are O
critical O
weaknesses O
. O

section 11
id pdf2json/2021.acl-long.4.pdf.json
models O
that O
misclassify O
reclaimed O
slurs O
penalise O
the O
very O
communities O
that O
are O
commonly O
targeted O
by O
hate O
speech O
. O

section 11
id pdf2json/2021.acl-long.4.pdf.json
models O
that O
misclassify O
counter O
speech O
undermine O
positive O
efforts O
to O
fight O
hate O
speech O
. O

section 11
id pdf2json/2021.acl-long.4.pdf.json
models O
that O
are O
biased O
in O
their O
target O
coverage O
are O
likely O
to O
create O
and O
entrench O
biases O
in O
the O
protections O
afforded O
to O
different O
groups O
. O

section 11
id pdf2json/2021.acl-long.4.pdf.json
as O
a O
suite O
of O
black-box O
tests O
, O
hatecheck O
only O
offers O
indirect O
insights O
into O
the O
source O
of O
these O
weaknesses O
. O

section 11
id pdf2json/2021.acl-long.4.pdf.json
poor O
performance O
on O
functional O
tests O
can O
be O
a O
consequence O
of O
systematic O
gaps O
and O
biases O
in O
model O
training O
data O
. O

section 11
id pdf2json/2021.acl-long.4.pdf.json
it O
can O
also O
indicate O
a O
more O
fundamental O
inability O
of O
the O
model O
’ O
s O
architecture O
to O
capture O
relevant O
linguistic O
phenomena O
. O

section 11
id pdf2json/2021.acl-long.4.pdf.json
b-d O
and O
b-f O
share O
the O
same O
architecture O
but O
differ O
in O
performance O
on O
functional O
tests O
and O
in O
target O
coverage O
. O

section 11
id pdf2json/2021.acl-long.4.pdf.json
this O
reflects O
the O
importance O
of O
training O
data O
composition O
, O
which O
previous O
hate O
speech O
research O
has O
emphasised O
( O
wiegand O
et O
al. O
, O
2019 O
; O
nejadgholi O
and O
kiritchenko O
, O
2020 O
) O
. O

section 11
id pdf2json/2021.acl-long.4.pdf.json
future O
work O
could O
investigate O
the O
provenance O
of O
model O
weaknesses O
in O
more O
detail O
, O
for O
instance O
by O
using O
test O
cases O
from O
hatecheck O
to O
“ O
inoculate O
” O
training O
data O
( O
liu O
et O
al. O
, O
2019 O
) O
. O

section 11
id pdf2json/2021.acl-long.4.pdf.json
if O
poor O
model O
performance O
does O
stem O
from O
biased O
training O
data O
, O
models O
could O
be O
improved O
through O
targeted O
data O
augmentation O
( O
gardner O
et O
al. O
, O
2020 O
) O
. O

section 11
id pdf2json/2021.acl-long.4.pdf.json
hatecheck O
users O
could O
, O
for O
instance O
, O
sample O
or O
construct O
additional O
training O
cases O
to O
resemble O
test O
cases O
from O
functional O
tests O
that O
their O
model O
was O
inaccurate O
on O
, O
bearing O
in O
mind O
that O
this O
additional O
data O
might O
introduce O
other O
unforeseen O
biases O
. O

section 11
id pdf2json/2021.acl-long.4.pdf.json
the O
models O
we O
tested O
would O
likely O
benefit O
from O
training O
on O
additional O
cases O
of O
negated O
hate O
, O
reclaimed O
slurs O
and O
counter O
speech O
. O

section 13
id pdf2json/2021.acl-long.4.pdf.json
good O
performance O
on O
a O
functional O
test O
in O
hatecheck O
only O
reveals O
the O
absence O
of O
a O
particular O
weakness O
, O
rather O
than O
necessarily O
characterising O
a O
generalisable O
model O
strength O
. O

section 13
id pdf2json/2021.acl-long.4.pdf.json
this O
negative O
predictive O
power O
( O
gardner O
et O
al. O
, O
2020 O
) O
is O
common O
, O
to O
some O
extent O
, O
to O
all O
finite O
test O
sets O
. O

section 13
id pdf2json/2021.acl-long.4.pdf.json
thus O
, O
claims O
about O
model O
quality O
should O
not O
be O
overextended O
based O
on O
positive O
hatecheck O
results O
. O

section 13
id pdf2json/2021.acl-long.4.pdf.json
in O
model O
development O
, O
hatecheck O
offers O
targeted O
diagnostic O
insights O
as O
a O
complement O
to O
rather O
than O
a O
substitute O
for O
evaluation O
on O
held-out O
test O
sets O
of O
real-world O
hate O
speech O
. O

section 14
id pdf2json/2021.acl-long.4.pdf.json
each O
test O
case O
in O
hatecheck O
is O
a O
separate O
english-language O
text O
document O
. O

section 14
id pdf2json/2021.acl-long.4.pdf.json
thus O
, O
hatecheck O
does O
not O
test O
functionalities O
related O
to O
context O
outside O
individual O
documents O
, O
modalities O
other O
than O
text O
or O
languages O
other O
than O
english O
. O

section 14
id pdf2json/2021.acl-long.4.pdf.json
future O
research O
could O
expand O
hatecheck O
to O
include O
functional O
tests O
covering O
such O
aspects O
. O

section 14
id pdf2json/2021.acl-long.4.pdf.json
functional O
tests O
in O
hatecheck O
cover O
distinct O
expressions O
of O
hate O
and O
non-hate O
. O

section 14
id pdf2json/2021.acl-long.4.pdf.json
future O
work O
could O
test O
more O
complex O
compound O
statements O
, O
such O
as O
cases O
combining O
slurs O
and O
profanity O
. O

section 14
id pdf2json/2021.acl-long.4.pdf.json
further O
, O
hatecheck O
is O
static O
and O
thus O
does O
not O
test O
functionalities O
related O
to O
language O
change O
. O

section 14
id pdf2json/2021.acl-long.4.pdf.json
this O
could O
be O
addressed O
by O
“ O
live O
” O
datasets O
, O
such O
as O
dynamic O
adversarial O
benchmarks O
( O
nie O
et O
al. O
, O
2020 O
; O
vidgen O
et O
al. O
, O
2020b O
; O
kiela O
et O
al. O
, O
2021 O
) O
. O

section 15
id pdf2json/2021.acl-long.4.pdf.json
future O
research O
could O
expand O
hatecheck O
to O
cover O
additional O
protected O
groups O
. O

section 15
id pdf2json/2021.acl-long.4.pdf.json
we O
also O
suggest O
the O
addition O
of O
intersectional O
characteristics O
, O
which O
interviewees O
highlighted O
as O
a O
neglected O
dimension O
of O
online O
hate O
( O
e.g O
. O

section 15
id pdf2json/2021.acl-long.4.pdf.json
i17 O
: O
“ O
as O
a O
black O
woman O
, O
i O
receive O
abuse O
that O
is O
racialised O
and O
gendered O
” O
) O
. O

section 15
id pdf2json/2021.acl-long.4.pdf.json
similarly O
, O
future O
research O
could O
include O
hateful O
slurs O
beyond O
those O
covered O
by O
hatecheck O
. O

section 15
id pdf2json/2021.acl-long.4.pdf.json
lastly O
, O
future O
research O
could O
craft O
test O
cases O
using O
more O
platform- O
or O
community-specific O
language O
than O
hatecheck O
’ O
s O
more O
general O
test O
cases O
. O

section 15
id pdf2json/2021.acl-long.4.pdf.json
it O
could O
also O
test O
hate O
that O
is O
more O
specific O
to O
particular O
target O
groups O
, O
such O
as O
misogynistic O
tropes O
. O

section 16
id pdf2json/2021.acl-long.4.pdf.json
targeted O
diagnostic O
datasets O
like O
the O
sets O
of O
test O
cases O
in O
hatecheck O
have O
been O
used O
for O
model O
evaluation O
across O
a O
wide O
range O
of O
nlp O
tasks O
, O
such O
as O
natural O
language O
inference O
( O
naik O
et O
al. O
, O
2018 O
; O
mccoy O
et O
al. O
, O
2019 O
) O
, O
machine O
translation O
( O
isabelle O
et O
al. O
, O
2017 O
; O
belinkov O
and O
bisk O
, O
2018 O
) O
and O
language O
modelling O
( O
marvin O
and O
linzen O
, O
2018 O
; O
ettinger O
, O
2020 O
) O
. O

section 16
id pdf2json/2021.acl-long.4.pdf.json
for O
hate O
speech O
detection O
, O
however O
, O
they O
have O
seen O
very O
limited O
use O
. O

section 16
id pdf2json/2021.acl-long.4.pdf.json
palmer O
et O
al O
. O

section 16
id pdf2json/2021.acl-long.4.pdf.json
( O
2020 O
) O
compile O
three O
datasets O
for O
evaluating O
model O
performance O
on O
what O
they O
call O
complex O
offensive O
language O
, O
specifically O
the O
use O
of O
reclaimed O
slurs O
, O
adjective O
nominalisation O
and O
linguistic O
distancing O
. O

section 16
id pdf2json/2021.acl-long.4.pdf.json
they O
select O
test O
cases O
from O
other O
datasets O
sampled O
from O
social O
media O
, O
which O
introduces O
substantial O
disagreement O
between O
annotators O
on O
labels O
in O
their O
data O
. O

section 16
id pdf2json/2021.acl-long.4.pdf.json
dixon O
et O
al O
. O

section 16
id pdf2json/2021.acl-long.4.pdf.json
( O
2018 O
) O
use O
templates O
to O
generate O
synthetic O
sets O
of O
toxic O
and O
non-toxic O
cases O
, O
which O
resembles O
our O
method O
for O
test O
case O
creation O
. O

section 16
id pdf2json/2021.acl-long.4.pdf.json
they O
focus O
primarily O
on O
evaluating O
biases O
around O
the O
use O
of O
group O
identifiers O
and O
do O
not O
validate O
the O
labels O
in O
their O
dataset O
. O

section 16
id pdf2json/2021.acl-long.4.pdf.json
compared O
to O
both O
approaches O
, O
hatecheck O
covers O
a O
much O
larger O
range O
of O
model O
functionalities O
, O
and O
all O
test O
cases O
, O
which O
we O
generated O
specifically O
to O
fit O
a O
given O
functionality O
, O
have O
clear O
gold O
standard O
labels O
, O
which O
are O
validated O
by O
near-perfect O
agreement O
between O
annotators O
. O

section 16
id pdf2json/2021.acl-long.4.pdf.json
in O
its O
use O
of O
contrastive O
cases O
for O
model O
eval- O
uation O
, O
hatecheck O
builds O
on O
a O
long O
history O
of O
minimally-contrastive O
pairs O
in O
nlp O
( O
e.g O
. O

section 16
id pdf2json/2021.acl-long.4.pdf.json
levesque O
et O
al. O
, O
2012 O
; O
sennrich O
, O
2017 O
; O
glockner O
et O
al. O
, O
2018 O
; O
warstadt O
et O
al. O
, O
2020 O
) O
. O

section 16
id pdf2json/2021.acl-long.4.pdf.json
most O
relevantly O
, O
kaushik O
et O
al O
. O

section 16
id pdf2json/2021.acl-long.4.pdf.json
( O
2020 O
) O
and O
gardner O
et O
al O
. O

section 16
id pdf2json/2021.acl-long.4.pdf.json
( O
2020 O
) O
propose O
augmenting O
nlp O
datasets O
with O
contrastive O
cases O
for O
training O
more O
generalisable O
models O
and O
enabling O
more O
meaningful O
evaluation O
. O

section 16
id pdf2json/2021.acl-long.4.pdf.json
we O
built O
on O
their O
approaches O
to O
generate O
non-hateful O
contrast O
cases O
in O
our O
test O
suite O
, O
which O
is O
the O
first O
application O
of O
this O
kind O
for O
hate O
speech O
detection O
. O

section 16
id pdf2json/2021.acl-long.4.pdf.json
in O
terms O
of O
its O
structure O
, O
hatecheck O
is O
most O
directly O
influenced O
by O
the O
checklist O
framework O
proposed O
by O
ribeiro O
et O
al O
. O

section 16
id pdf2json/2021.acl-long.4.pdf.json
( O
2020 O
) O
. O

section 16
id pdf2json/2021.acl-long.4.pdf.json
however O
, O
while O
they O
focus O
on O
demonstrating O
its O
general O
applicability O
across O
nlp O
tasks O
, O
we O
put O
more O
emphasis O
on O
motivating O
the O
selection O
of O
functional O
tests O
as O
well O
as O
constructing O
and O
validating O
targeted O
test O
cases O
specifically O
for O
the O
task O
of O
hate O
speech O
detection O
. O

section 17
id pdf2json/2021.acl-long.4.pdf.json
in O
this O
article O
, O
we O
introduced O
hatecheck O
, O
a O
suite O
of O
functional O
tests O
for O
hate O
speech O
detection O
models O
. O

section 17
id pdf2json/2021.acl-long.4.pdf.json
we O
motivated O
the O
selection O
of O
functional O
tests O
through O
interviews O
with O
civil O
society O
stakeholders O
and O
a O
review O
of O
previous O
hate O
speech O
research O
, O
which O
grounds O
our O
approach O
in O
both O
practical O
and O
academic O
applications O
of O
hate O
speech O
detection O
models O
. O

section 17
id pdf2json/2021.acl-long.4.pdf.json
we O
designed O
the O
functional O
tests O
to O
offer O
contrasts O
between O
hateful O
and O
non-hateful O
content O
that O
are O
challenging O
to O
detection O
models O
, O
which O
enables O
more O
accurate O
evaluation O
of O
their O
true O
functionalities O
. O

section 17
id pdf2json/2021.acl-long.4.pdf.json
for O
each O
functional O
test O
, O
we O
crafted O
sets O
of O
targeted O
test O
cases O
with O
clear O
gold O
standard O
labels O
, O
which O
we O
validated O
through O
a O
structured O
annotation O
process O
. O

section 17
id pdf2json/2021.acl-long.4.pdf.json
we O
demonstrated O
the O
utility O
of O
hatecheck O
as O
a O
diagnostic O
tool O
by O
testing O
near-state-of-the-art O
transformer O
models O
as O
well O
as O
two O
commercial O
models O
for O
hate O
speech O
detection O
. O

section 17
id pdf2json/2021.acl-long.4.pdf.json
hatecheck O
showed O
critical O
weaknesses O
for O
all O
models O
. O

section 17
id pdf2json/2021.acl-long.4.pdf.json
specifically O
, O
models O
appeared O
overly O
sensitive O
to O
particular O
keywords O
and O
phrases O
, O
as O
evidenced O
by O
poor O
performance O
on O
tests O
for O
reclaimed O
slurs O
, O
counter O
speech O
and O
negated O
hate O
. O

section 17
id pdf2json/2021.acl-long.4.pdf.json
the O
transformer O
models O
also O
exhibited O
strong O
biases O
in O
target O
coverage O
. O

section 17
id pdf2json/2021.acl-long.4.pdf.json
online O
hate O
is O
a O
deeply O
harmful O
phenomenon O
, O
and O
detection O
models O
are O
integral O
to O
tackling O
it O
. O

section 17
id pdf2json/2021.acl-long.4.pdf.json
typically O
, O
models O
have O
been O
evaluated O
on O
held-out O
test O
data O
, O
which O
has O
made O
it O
difficult O
to O
assess O
their O
generalisability O
and O
identify O
specific O
weaknesses O
. O

section 17
id pdf2json/2021.acl-long.4.pdf.json
we O
hope O
that O
hatecheck O
’ O
s O
targeted O
diagnostic O
insights O
help O
address O
this O
issue O
by O
contributing O
to O
our O
understanding O
of O
models O
’ O
limitations O
, O
thus O
aiding O
the O
development O
of O
better O
models O
in O
the O
future O
. O

section 18
id pdf2json/2021.acl-long.4.pdf.json
we O
thank O
all O
interviewees O
for O
their O
participation O
. O

section 18
id pdf2json/2021.acl-long.4.pdf.json
we O
also O
thank O
reviewers O
for O
their O
constructive O
feedback O
. O

section 18
id pdf2json/2021.acl-long.4.pdf.json
paul O
röttger O
was O
funded O
by O
the O
german O
academic O
scholarship O
foundation O
. O

section 18
id pdf2json/2021.acl-long.4.pdf.json
bertram O
vidgen O
and O
helen O
margetts O
were O
supported O
by O
wave O
1 O
of O
the O
ukri O
strategic O
priorities O
fund O
under O
the O
epsrc O
grant O
ep/t001569/1 O
, O
particularly O
the O
“ O
criminal O
justice O
system O
” O
theme O
within O
that O
grant O
, O
and O
the O
“ O
hate O
speech O
: O
measures O
& O
counter-measures O
” O
project O
at O
the O
alan O
turing O
institute O
. O

section 18
id pdf2json/2021.acl-long.4.pdf.json
dong O
nguyen O
was O
supported O
by O
the O
“ O
digital O
society O
- O
the O
informed O
citizen O
” O
research O
programme O
, O
which O
is O
( O
partly O
) O
financed O
by O
the O
dutch O
research O
council O
( O
nwo O
) O
, O
project O
410.19.007 O
. O

section 18
id pdf2json/2021.acl-long.4.pdf.json
zeerak O
waseem O
was O
supported O
in O
part O
by O
the O
canada O
150 O
research O
chair O
program O
and O
the O
uk-canada O
ai O
artificial O
intelligence O
initiative O
. O

section 18
id pdf2json/2021.acl-long.4.pdf.json
janet O
b. O
pierrehumbert O
was O
supported O
by O
epsrc O
grant O
ep/t023333/1 O
. O

section 18
id pdf2json/2021.acl-long.4.pdf.json
impact O
statement O
this O
supplementary O
section O
addresses O
relevant O
ethical O
considerations O
that O
were O
not O
explicitly O
discussed O
in O
the O
main O
body O
of O
our O
article O
. O

section 18
id pdf2json/2021.acl-long.4.pdf.json
interview O
participant O
rights O
all O
interviewees O
gave O
explicit O
consent O
for O
their O
participation O
after O
being O
informed O
in O
detail O
about O
the O
research O
use O
of O
their O
responses O
. O

section 18
id pdf2json/2021.acl-long.4.pdf.json
in O
all O
research O
output O
, O
quotes O
from O
interview O
responses O
were O
anonymised O
. O

section 18
id pdf2json/2021.acl-long.4.pdf.json
we O
also O
did O
not O
reveal O
specific O
participant O
demographics O
or O
affiliations O
. O

section 18
id pdf2json/2021.acl-long.4.pdf.json
our O
interview O
approach O
was O
approved O
by O
the O
alan O
turing O
institute O
’ O
s O
ethics O
review O
board O
. O

section 18
id pdf2json/2021.acl-long.4.pdf.json
intellectual O
property O
rights O
the O
test O
cases O
in O
hatecheck O
were O
crafted O
by O
the O
authors O
. O

section 18
id pdf2json/2021.acl-long.4.pdf.json
as O
synthetic O
data O
, O
they O
pose O
no O
risk O
of O
violating O
intellectual O
property O
rights O
. O

section 18
id pdf2json/2021.acl-long.4.pdf.json
annotator O
compensation O
we O
employed O
a O
team O
of O
ten O
annotators O
to O
validate O
the O
quality O
of O
the O
hatecheck O
dataset O
. O

section 18
id pdf2json/2021.acl-long.4.pdf.json
annotators O
were O
compensated O
at O
a O
rate O
of O
£16 O
per O
hour O
. O

section 18
id pdf2json/2021.acl-long.4.pdf.json
the O
rate O
was O
set O
50 O
% O
above O
the O
local O
living O
wage O
( O
£10.85 O
) O
, O
although O
all O
work O
was O
completed O
remotely O
. O

section 18
id pdf2json/2021.acl-long.4.pdf.json
all O
training O
time O
and O
meetings O
were O
paid O
. O

section 18
id pdf2json/2021.acl-long.4.pdf.json
intended O
use O
hatecheck O
’ O
s O
intended O
use O
is O
as O
an O
evaluative O
tool O
for O
hate O
speech O
detection O
models O
, O
providing O
structured O
and O
targeted O
diagnostic O
insights O
into O
model O
functionalities O
. O

section 18
id pdf2json/2021.acl-long.4.pdf.json
we O
demonstrated O
this O
use O
of O
hatecheck O
in O
§3 O
. O

section 18
id pdf2json/2021.acl-long.4.pdf.json
we O
also O
briefly O
discussed O
alternative O
uses O
of O
hatecheck O
, O
e.g O
. O

section 18
id pdf2json/2021.acl-long.4.pdf.json
as O
a O
starting O
point O
for O
data O
augmentation O
. O

section 18
id pdf2json/2021.acl-long.4.pdf.json
these O
uses O
aim O
at O
aiding O
the O
development O
of O
better O
hate O
speech O
detection O
models O
. O

section 18
id pdf2json/2021.acl-long.4.pdf.json
potential O
misuse O
researchers O
might O
overextend O
claims O
about O
the O
functionalities O
of O
their O
models O
based O
on O
their O
test O
performance O
, O
which O
we O
would O
consider O
a O
misuse O
of O
hatecheck O
. O

section 18
id pdf2json/2021.acl-long.4.pdf.json
we O
directly O
addressed O
this O
concern O
by O
highlighting O
hatecheck O
’ O
s O
negative O
predictive O
power O
, O
i.e O
. O

section 18
id pdf2json/2021.acl-long.4.pdf.json
the O
fact O
that O
it O
primarily O
reveals O
model O
weaknesses O
rather O
than O
necessarily O
characterising O
generalisable O
model O
strengths O
, O
as O
one O
of O
its O
limitations O
. O

section 18
id pdf2json/2021.acl-long.4.pdf.json
for O
the O
same O
reason O
, O
we O
emphasised O
the O
limits O
to O
hatecheck O
’ O
s O
coverage O
, O
e.g O
. O

section 18
id pdf2json/2021.acl-long.4.pdf.json
in O
terms O
of O
slurs O
and O
identity O
terms O
. O

section 19
id pdf2json/2021.acl-long.4.pdf.json
following O
bender O
and O
friedman O
( O
2018 O
) O
, O
we O
provide O
a O
data O
statement O
, O
which O
documents O
the O
generation O
and O
provenance O
of O
test O
cases O
in O
hatecheck O
. O

section 19
id pdf2json/2021.acl-long.4.pdf.json
a. O
curation O
rationale O
in O
order O
to O
construct O
hatecheck O
, O
a O
first O
suite O
of O
functional O
tests O
for O
hate O
speech O
detection O
models O
, O
we O
generated O
3,901 O
short O
english-language O
text O
documents O
by O
hand O
and O
by O
using O
simple O
templates O
for O
group O
identifiers O
and O
slurs O
( O
§2.4 O
) O
. O

section 19
id pdf2json/2021.acl-long.4.pdf.json
each O
document O
corresponds O
to O
one O
functional O
test O
and O
a O
binary O
gold O
standard O
label O
( O
hateful O
or O
non-hateful O
) O
. O

section 19
id pdf2json/2021.acl-long.4.pdf.json
in O
order O
to O
validate O
the O
gold O
standard O
labels O
, O
we O
trained O
a O
team O
of O
ten O
annotators O
, O
assigning O
five O
of O
them O
to O
each O
document O
, O
and O
asked O
them O
to O
provide O
independent O
labels O
( O
§2.5 O
) O
. O

section 19
id pdf2json/2021.acl-long.4.pdf.json
to O
further O
improve O
data O
quality O
, O
we O
also O
gave O
annotators O
the O
option O
to O
flag O
cases O
they O
felt O
were O
unrealistic O
( O
e.g O
. O

section 19
id pdf2json/2021.acl-long.4.pdf.json
nonsensical O
) O
, O
but O
this O
flag O
was O
not O
used O
for O
any O
one O
hatecheck O
case O
by O
more O
than O
one O
annotator O
. O

section 19
id pdf2json/2021.acl-long.4.pdf.json
b O
. O

section 19
id pdf2json/2021.acl-long.4.pdf.json
language O
variety O
hatecheck O
only O
covers O
english-language O
text O
documents O
. O

section 19
id pdf2json/2021.acl-long.4.pdf.json
we O
opted O
for O
english O
language O
since O
this O
maximises O
hatecheck O
’ O
s O
relevance O
to O
previous O
and O
current O
work O
in O
hate O
speech O
detection O
, O
which O
is O
mostly O
concerned O
with O
english-language O
data O
. O

section 19
id pdf2json/2021.acl-long.4.pdf.json
our O
language O
choice O
also O
reflects O
the O
expertise O
of O
authors O
and O
annotators O
. O

section 19
id pdf2json/2021.acl-long.4.pdf.json
we O
discuss O
the O
lack O
of O
language O
variety O
as O
a O
limitation O
of O
hatecheck O
in O
§4.2 O
and O
suggest O
expansion O
to O
other O
languages O
as O
a O
priority O
for O
future O
research O
. O

section 19
id pdf2json/2021.acl-long.4.pdf.json
c. O
speaker O
demographics O
since O
all O
test O
cases O
in O
hatecheck O
were O
hand-crafted O
, O
the O
speakers O
are O
the O
same O
as O
the O
authors O
. O

section 19
id pdf2json/2021.acl-long.4.pdf.json
test O
cases O
in O
the O
test O
suite O
were O
primarily O
generated O
by O
the O
lead O
author O
, O
who O
is O
a O
researcher O
at O
a O
uk O
university O
. O

section 19
id pdf2json/2021.acl-long.4.pdf.json
the O
lead O
author O
is O
not O
a O
native O
english O
speaker O
but O
has O
lived O
in O
english-speaking O
countries O
for O
more O
than O
five O
years O
and O
has O
extensively O
engaged O
with O
english-language O
hate O
speech O
in O
previous O
research O
. O

section 19
id pdf2json/2021.acl-long.4.pdf.json
all O
test O
cases O
were O
also O
reviewed O
by O
two O
co-authors O
, O
both O
of O
whom O
have O
worked O
with O
english-language O
hate O
speech O
data O
for O
more O
than O
five O
years O
and O
one O
of O
whom O
is O
a O
native O
english O
speaker O
from O
the O
uk O
. O

section 19
id pdf2json/2021.acl-long.4.pdf.json
d. O
annotator O
demographics O
we O
recruited O
a O
team O
of O
ten O
annotators O
to O
work O
for O
two O
weeks O
. O

section 19
id pdf2json/2021.acl-long.4.pdf.json
30 O
% O
were O
male O
and O
70 O
% O
were O
female O
. O

section 19
id pdf2json/2021.acl-long.4.pdf.json
60 O
% O
were O
18-29 O
and O
40 O
% O
were O
30-39 O
. O

section 19
id pdf2json/2021.acl-long.4.pdf.json
20 O
% O
were O
educated O
to O
high O
school O
level O
, O
10 O
% O
to O
undergraduate O
, O
60 O
% O
to O
taught O
masters O
and O
10 O
% O
to O
research O
degree O
( O
i.e O
. O

section 19
id pdf2json/2021.acl-long.4.pdf.json
phd O
) O
. O

section 19
id pdf2json/2021.acl-long.4.pdf.json
70 O
% O
were O
native O
english O
speakers O
and O
30 O
% O
were O
non-native O
but O
fluent O
. O

section 19
id pdf2json/2021.acl-long.4.pdf.json
annotators O
had O
a O
range O
of O
nationalities O
: O
60 O
% O
were O
british O
and O
10 O
% O
each O
were O
polish O
, O
spanish O
, O
argentinian O
and O
irish O
. O

section 19
id pdf2json/2021.acl-long.4.pdf.json
most O
annotators O
identified O
as O
ethnically O
white O
( O
70 O
% O
) O
, O
followed O
by O
middle O
eastern O
( O
20 O
% O
) O
and O
a O
mixed O
ethnic O
background O
( O
10 O
% O
) O
. O

section 19
id pdf2json/2021.acl-long.4.pdf.json
annotators O
all O
used O
social O
media O
regularly O
, O
and O
60 O
% O
used O
it O
more O
than O
once O
per O
day O
. O

section 19
id pdf2json/2021.acl-long.4.pdf.json
all O
annotators O
had O
seen O
other O
people O
targeted O
by O
online O
abuse O
before O
, O
and O
80 O
% O
had O
been O
targeted O
personally O
. O

section 19
id pdf2json/2021.acl-long.4.pdf.json
all O
annotators O
had O
previously O
completed O
annotation O
work O
on O
at O
least O
one O
other O
hate O
speech O
dataset O
. O

section 19
id pdf2json/2021.acl-long.4.pdf.json
in O
the O
first O
week O
, O
we O
introduced O
the O
binary O
annotation O
task O
to O
them O
in O
an O
onboarding O
session O
and O
tested O
their O
understanding O
on O
a O
set O
of O
100 O
cases O
, O
which O
we O
then O
provided O
individual O
feedback O
on O
. O

section 19
id pdf2json/2021.acl-long.4.pdf.json
in O
the O
second O
week O
, O
we O
asked O
each O
annotator O
to O
annotate O
around O
2,000 O
test O
cases O
so O
that O
each O
case O
in O
our O
test O
suite O
was O
annotated O
by O
varied O
sets O
of O
exactly O
five O
annotators O
. O

section 19
id pdf2json/2021.acl-long.4.pdf.json
throughout O
the O
process O
, O
we O
communicated O
with O
annotators O
in O
real-time O
over O
a O
messaging O
platform O
. O

section 19
id pdf2json/2021.acl-long.4.pdf.json
we O
also O
followed O
guidance O
for O
protecting O
and O
monitoring O
annotator O
well-being O
provided O
by O
vidgen O
et O
al O
. O

section 19
id pdf2json/2021.acl-long.4.pdf.json
( O
2019 O
) O
. O

section 19
id pdf2json/2021.acl-long.4.pdf.json
e. O
speech O
situation O
all O
test O
cases O
were O
created O
between O
the O
23rd O
of O
november O
and O
the O
13th O
of O
december O
2020 O
. O

section 19
id pdf2json/2021.acl-long.4.pdf.json
f. O
text O
characteristics O
the O
composition O
of O
the O
dataset O
, O
including O
primary O
label O
and O
secondary O
labels O
, O
is O
described O
in O
detail O
in O
§2.3 O
and O
§2.4 O
of O
the O
article O
. O

section 20
id pdf2json/2021.acl-long.4.pdf.json
f1 O
– O
strong O
negative O
emotions O
explicitly O
expressed O
about O
a O
protected O
group O
or O
its O
members O
: O
resembles O
“ O
expressed O
hatred O
” O
( O
davidson O
et O
al. O
, O
2017 O
) O
and O
“ O
identity O
attack O
” O
( O
banko O
et O
al. O
, O
2020 O
) O
. O

section 20
id pdf2json/2021.acl-long.4.pdf.json
f2 O
– O
explicit O
descriptions O
of O
a O
protected O
group O
or O
its O
members O
using O
very O
negative O
attributes O
: O
refines O
more O
general O
“ O
insult O
” O
categories O
( O
davidson O
et O
al. O
, O
2017 O
; O
zampieri O
et O
al. O
, O
2019 O
) O
. O

section 20
id pdf2json/2021.acl-long.4.pdf.json
f3 O
– O
explicit O
dehumanisation O
of O
a O
protected O
group O
or O
its O
members O
: O
prevalent O
form O
of O
hate O
( O
mendelsohn O
et O
al. O
, O
2020 O
; O
banko O
et O
al. O
, O
2020 O
; O
vidgen O
et O
al. O
, O
2020a O
) O
. O

section 20
id pdf2json/2021.acl-long.4.pdf.json
highlighted O
in O
our O
interviews O
( O
e.g O
. O

section 20
id pdf2json/2021.acl-long.4.pdf.json
i18 O
: O
“ O
hate O
crime O
[ O
often O
claims O
] O
people O
are O
inferior O
and O
subhuman. O
” O
) O
. O

section 20
id pdf2json/2021.acl-long.4.pdf.json
f4 O
– O
implicit O
derogation O
of O
a O
protected O
group O
or O
its O
members O
: O
closely O
resembles O
“ O
implied O
bias O
” O
( O
sap O
et O
al. O
, O
2020 O
) O
and O
“ O
implicit O
abuse O
” O
( O
waseem O
et O
al. O
, O
2017 O
; O
zhang O
and O
luo O
, O
2019 O
) O
. O

section 20
id pdf2json/2021.acl-long.4.pdf.json
highlighted O
in O
our O
interviews O
( O
e.g O
. O

section 20
id pdf2json/2021.acl-long.4.pdf.json
i16 O
: O
“ O
hate O
has O
always O
been O
expressed O
idiomatically O
” O
) O
. O

section 21
id pdf2json/2021.acl-long.4.pdf.json
its O
members O
: O
core O
element O
of O
several O
hate O
speech O
taxonomies O
( O
golbeck O
et O
al. O
, O
2017 O
; O
zampieri O
et O
al. O
, O
2019 O
; O
vidgen O
et O
al. O
, O
2020a O
; O
banko O
et O
al. O
, O
2020 O
) O
f6 O
– O
threats O
expressed O
as O
normative O
statements O
: O
highlighted O
by O
an O
interviewee O
as O
a O
way O
of O
avoiding O
legal O
consequences O
to O
hate O
speech O
( O
i1 O
: O
“ O
[ O
normative O
threats O
] O
are O
extremely O
hateful O
, O
but O
[ O
legally O
] O
okay O
” O
) O
. O

section 21
id pdf2json/2021.acl-long.4.pdf.json
f7 O
– O
hate O
expressed O
using O
slurs O
: O
prevalent O
way O
of O
expressing O
hate O
( O
palmer O
et O
al. O
, O
2020 O
; O
banko O
et O
al. O
, O
2020 O
; O
kurrek O
et O
al. O
, O
2020 O
) O
. O

section 21
id pdf2json/2021.acl-long.4.pdf.json
f8 O
– O
non-hateful O
homonyms O
of O
slur O
: O
relevant O
alternative O
use O
of O
slurs O
( O
kurrek O
et O
al. O
, O
2020 O
) O
. O

section 21
id pdf2json/2021.acl-long.4.pdf.json
f9 O
– O
use O
of O
reclaimed O
slurs O
: O
likely O
source O
of O
classification O
error O
( O
palmer O
et O
al. O
, O
2020 O
) O
. O

section 21
id pdf2json/2021.acl-long.4.pdf.json
highlighted O
in O
our O
interviews O
( O
e.g O
. O

section 21
id pdf2json/2021.acl-long.4.pdf.json
i7 O
: O
“ O
a O
lot O
of O
lgbt O
people O
use O
slurs O
to O
identify O
themselves O
, O
like O
reclaim O
the O
word O
queer O
, O
and O
people O
[ O
... O
] O
report O
that O
and O
then O
that O
will O
get O
hidden O
” O
) O
. O

section 21
id pdf2json/2021.acl-long.4.pdf.json
f10 O
– O
hate O
expressed O
using O
profanity O
: O
refines O
more O
general O
“ O
insult O
” O
categories O
( O
davidson O
et O
al. O
, O
2017 O
; O
zampieri O
et O
al. O
, O
2019 O
) O
. O

section 21
id pdf2json/2021.acl-long.4.pdf.json
f11 O
– O
non-hateful O
uses O
of O
profanity O
: O
oversensitiveness O
of O
hate O
speech O
detection O
models O
to O
profanity O
( O
davidson O
et O
al. O
, O
2017 O
; O
malmasi O
and O
zampieri O
, O
2018 O
; O
van O
aken O
et O
al. O
, O
2018 O
) O
. O

section 21
id pdf2json/2021.acl-long.4.pdf.json
f12 O
– O
hate O
expressed O
through O
pronoun O
reference O
in O
subsequent O
clauses O
: O
syntactic O
relationships O
and O
long-range O
dependencies O
as O
model O
weak O
points O
( O
burnap O
and O
williams O
, O
2015 O
; O
vidgen O
et O
al. O
, O
2019 O
) O
. O

section 21
id pdf2json/2021.acl-long.4.pdf.json
f13 O
– O
hate O
expressed O
through O
pronoun O
reference O
in O
subsequent O
sentences O
: O
see O
f12 O
. O

section 21
id pdf2json/2021.acl-long.4.pdf.json
f14 O
– O
hate O
expressed O
using O
negated O
positive O
statements O
: O
negation O
as O
an O
effective O
adversary O
for O
hate O
speech O
detection O
models O
( O
hosseini O
et O
al. O
, O
2017 O
; O
dinan O
et O
al. O
, O
2019 O
) O
. O

section 21
id pdf2json/2021.acl-long.4.pdf.json
f15 O
– O
non-hate O
expressed O
using O
negated O
hateful O
statements O
: O
see O
f14 O
. O

section 21
id pdf2json/2021.acl-long.4.pdf.json
f16 O
– O
hate O
phrased O
as O
a O
question O
: O
likely O
source O
of O
classification O
error O
( O
van O
aken O
et O
al. O
, O
2018 O
) O
. O

section 21
id pdf2json/2021.acl-long.4.pdf.json
f17 O
– O
hate O
phrased O
as O
an O
opinion O
: O
highlighted O
by O
an O
interviewee O
as O
a O
way O
of O
avoiding O
legal O
consequences O
to O
hate O
speech O
( O
i1 O
: O
“ O
if O
you O
start O
a O
sentence O
by O
saying O
‘ O
i O
think O
that O
’ O
[ O
... O
] O
, O
the O
limits O
of O
what O
you O
can O
say O
are O
much O
bigger O
” O
) O
. O

section 21
id pdf2json/2021.acl-long.4.pdf.json
f18 O
– O
neutral O
statements O
using O
protected O
group O
identifiers O
: O
oversensitiveness O
of O
hate O
speech O
de- O
tection O
models O
to O
terms O
such O
as O
“ O
black O
” O
and O
“ O
gay O
” O
( O
dixon O
et O
al. O
, O
2018 O
; O
park O
et O
al. O
, O
2018 O
; O
kennedy O
et O
al. O
, O
2020 O
) O
. O

section 21
id pdf2json/2021.acl-long.4.pdf.json
also O
highlighted O
in O
our O
interviews O
( O
e.g O
. O

section 21
id pdf2json/2021.acl-long.4.pdf.json
i7 O
: O
“ O
i O
have O
seen O
the O
algorithm O
get O
it O
wrong O
, O
if O
someone O
’ O
s O
saying O
something O
like O
‘ O
i O
’ O
m O
so O
gay O
’ O
. O
” O
) O
. O

section 21
id pdf2json/2021.acl-long.4.pdf.json
f19 O
– O
positive O
statements O
using O
protected O
group O
identifiers O
: O
see O
f18 O
. O

section 21
id pdf2json/2021.acl-long.4.pdf.json
f20 O
– O
denouncements O
of O
hate O
that O
quote O
it O
: O
counter O
speech O
as O
a O
source O
of O
classification O
error O
( O
warner O
and O
hirschberg O
, O
2012 O
; O
van O
aken O
et O
al. O
, O
2018 O
; O
vidgen O
et O
al. O
, O
2020a O
) O
. O

section 21
id pdf2json/2021.acl-long.4.pdf.json
most O
mentioned O
concern O
in O
our O
interviews O
( O
e.g O
. O

section 21
id pdf2json/2021.acl-long.4.pdf.json
i4 O
: O
“ O
people O
will O
be O
quoting O
someone O
, O
calling O
that O
person O
out O
[ O
... O
] O
but O
that O
will O
get O
picked O
up O
by O
the O
system O
” O
) O
. O

section 21
id pdf2json/2021.acl-long.4.pdf.json
f21 O
– O
denouncements O
of O
hate O
that O
make O
direct O
reference O
to O
it O
: O
see O
f20 O
. O

section 21
id pdf2json/2021.acl-long.4.pdf.json
f22 O
– O
abuse O
targeted O
at O
objects O
: O
distinct O
from O
hate O
speech O
since O
it O
targets O
out-of-scope O
entities O
( O
wulczyn O
et O
al. O
, O
2017 O
; O
zampieri O
et O
al. O
, O
2019 O
) O
. O

section 21
id pdf2json/2021.acl-long.4.pdf.json
f23 O
– O
abuse O
targeted O
at O
individuals O
not O
referencing O
membership O
in O
a O
protected O
group O
: O
see O
f22 O
. O

section 21
id pdf2json/2021.acl-long.4.pdf.json
f24 O
– O
abuse O
targeted O
at O
non-protected O
groups O
( O
e.g O
. O

section 21
id pdf2json/2021.acl-long.4.pdf.json
professions O
) O
: O
see O
f22 O
. O

section 21
id pdf2json/2021.acl-long.4.pdf.json
f25 O
– O
swaps O
of O
adjacent O
characters O
: O
simple O
misspellings O
can O
be O
challenging O
for O
detection O
models O
( O
van O
aken O
et O
al. O
, O
2018 O
; O
qian O
et O
al. O
, O
2018 O
) O
. O

section 21
id pdf2json/2021.acl-long.4.pdf.json
particularly O
relevant O
to O
hate O
speech O
since O
they O
can O
reflect O
intentional O
behaviour O
of O
users O
looking O
to O
avoid O
detection O
( O
hosseini O
et O
al. O
, O
2017 O
; O
gröndahl O
et O
al. O
, O
2018 O
; O
vidgen O
et O
al. O
, O
2019 O
) O
. O

section 21
id pdf2json/2021.acl-long.4.pdf.json
f26 O
– O
missing O
characters O
: O
highlighted O
in O
our O
interviews O
( O
e.g O
. O

section 21
id pdf2json/2021.acl-long.4.pdf.json
i7 O
: O
“ O
it O
could O
be O
a O
misspelling O
of O
a O
word O
like O
‘ O
f*ggot O
’ O
, O
and O
someone O
’ O
s O
put O
one O
‘ O
g O
’ O
instead O
of O
two O
” O
) O
. O

section 21
id pdf2json/2021.acl-long.4.pdf.json
f27 O
– O
missing O
word O
boundaries O
: O
effective O
adversary O
for O
a O
hate O
speech O
detection O
model O
( O
gröndahl O
et O
al. O
, O
2018 O
) O
. O

section 21
id pdf2json/2021.acl-long.4.pdf.json
resembles O
the O
use O
of O
hashtags O
on O
social O
media O
( O
i2 O
: O
“ O
there O
have O
been O
a O
highly O
islamophobic O
hashtags O
going O
around O
” O
) O
. O

section 21
id pdf2json/2021.acl-long.4.pdf.json
f28 O
– O
added O
spaces O
between O
characters O
: O
effective O
adversary O
for O
a O
hate O
speech O
detection O
model O
( O
gröndahl O
et O
al. O
, O
2018 O
) O
. O

section 21
id pdf2json/2021.acl-long.4.pdf.json
highlighted O
in O
our O
interviews O
( O
e.g O
. O

section 21
id pdf2json/2021.acl-long.4.pdf.json
i5 O
: O
“ O
misspellings O
, O
missing O
letters O
or O
additional O
spaces O
between O
the O
letters. O
” O
) O
. O

section 21
id pdf2json/2021.acl-long.4.pdf.json
f29 O
– O
leet O
speak O
: O
resembles O
“ O
obfuscations O
” O
( O
nobata O
et O
al. O
, O
2016 O
; O
van O
aken O
et O
al. O
, O
2018 O
) O
. O

section 21
id pdf2json/2021.acl-long.4.pdf.json
highlighted O
in O
our O
interviews O
( O
e.g O
. O

section 21
id pdf2json/2021.acl-long.4.pdf.json
i14 O
: O
“ O
[ O
hate O
speakers O
] O
replace O
letters O
with O
numbers O
” O
) O
. O

section 22
id pdf2json/2021.acl-long.4.pdf.json
for O
each O
of O
the O
seven O
protected O
groups O
covered O
by O
hatecheck O
, O
we O
searched O
hatebase.org O
, O
a O
crowdsourced O
hate O
speech O
lexicon O
, O
for O
slurs O
which O
target O
that O
group O
. O

section 22
id pdf2json/2021.acl-long.4.pdf.json
from O
these O
slurs O
, O
we O
selected O
the O
three O
that O
were O
most O
often O
logged O
by O
users O
of O
the O
site O
( O
e.g O
. O

section 22
id pdf2json/2021.acl-long.4.pdf.json
“ O
wh*re O
” O
, O
“ O
b*tch O
” O
and O
“ O
sl*t O
” O
for O
women O
) O
, O
except O
for O
when O
the O
third-most O
sighted O
slur O
was O
logged O
substantially O
less O
often O
than O
the O
second O
, O
in O
which O
case O
we O
selected O
the O
top O
two O
( O
e.g O
. O

section 22
id pdf2json/2021.acl-long.4.pdf.json
“ O
tr*nny O
” O
and O
“ O
sh*male O
” O
for O
trans O
people O
) O
. O

section 22
id pdf2json/2021.acl-long.4.pdf.json
for O
immigration O
status O
, O
which O
is O
not O
a O
target O
category O
on O
hatebase.org O
, O
we O
chose O
“ O
r*pefugee O
” O
, O
a O
slur O
for O
refugees O
used O
by O
the O
european O
far O
right O
, O
and O
“ O
w*tback O
” O
, O
a O
slur O
for O
mexican O
immigrants O
to O
the O
us O
, O
which O
was O
logged O
similarly O
often O
as O
other O
slurs O
in O
hatecheck O
. O

section 22
id pdf2json/2021.acl-long.4.pdf.json
for O
reclaimed O
slurs O
( O
f9 O
) O
, O
we O
focus O
on O
slurs O
reclaimed O
by O
black O
communities O
( O
particularly O
“ O
n*gga O
” O
) O
, O
gay O
communities O
( O
“ O
f*g O
” O
, O
“ O
f*ggot O
” O
, O
“ O
q*eer O
” O
) O
and O
by O
women O
( O
“ O
b*tch O
” O
) O
, O
reflecting O
the O
concerns O
highlighted O
by O
our O
interview O
participants O
( O
e.g O
. O

section 22
id pdf2json/2021.acl-long.4.pdf.json
i4 O
: O
“ O
n*gga O
would O
often O
get O
[ O
wrongly O
] O
picked O
up O
by O
[ O
moderation O
] O
systems O
” O
) O
. O

section 22
id pdf2json/2021.acl-long.4.pdf.json
ahead O
of O
the O
structured O
annotation O
process O
( O
§2.5 O
) O
and O
only O
for O
test O
cases O
with O
reclaimed O
slurs O
, O
we O
asked O
selfidentifying O
members O
of O
the O
relevant O
groups O
in O
our O
personal O
networks O
whether O
they O
would O
consider O
the O
test O
cases O
to O
contain O
valid O
and O
realistic O
reclaimed O
slur O
uses O
, O
which O
held O
true O
for O
all O
test O
cases O
. O

section 23
id pdf2json/2021.acl-long.4.pdf.json
d.1 O
davidson O
et O
al O
. O

section 23
id pdf2json/2021.acl-long.4.pdf.json
( O
2017 O
) O
data O
sampling O
davidson O
et O
al O
. O

section 23
id pdf2json/2021.acl-long.4.pdf.json
( O
2017 O
) O
searched O
twitter O
for O
tweets O
containing O
keywords O
from O
a O
list O
they O
compiled O
from O
hatebase.org O
, O
which O
yielded O
a O
sample O
of O
tweets O
from O
33,458 O
users O
. O

section 23
id pdf2json/2021.acl-long.4.pdf.json
they O
then O
randomly O
sampled O
25,000 O
tweets O
from O
all O
tweets O
of O
these O
users O
. O

section 23
id pdf2json/2021.acl-long.4.pdf.json
annotation O
the O
authors O
hired O
crowd O
workers O
from O
crowdflower O
to O
annotate O
each O
tweet O
as O
hateful O
, O
offensive O
or O
neither O
. O

section 23
id pdf2json/2021.acl-long.4.pdf.json
92.0 O
% O
of O
tweets O
were O
annotated O
by O
three O
crowd O
workers O
, O
the O
remainder O
by O
at O
least O
four O
and O
up O
to O
nine O
. O

section 23
id pdf2json/2021.acl-long.4.pdf.json
for O
inter-annotator O
agreement O
, O
the O
authors O
report O
a O
“ O
crowdflower O
score O
” O
of O
92 O
% O
. O

section 23
id pdf2json/2021.acl-long.4.pdf.json
data O
we O
used O
24,783 O
annotated O
tweets O
made O
available O
by O
the O
authors O
on O
github.com/tdavidson/hate-speech-and-offensive-language O
. O

section 23
id pdf2json/2021.acl-long.4.pdf.json
1,430 O
tweets O
( O
5.8 O
% O
) O
are O
labelled O
hateful O
, O
19,190 O
( O
77.4 O
% O
) O
offensive O
and O
4,163 O
( O
16.8 O
% O
) O
neither O
. O

section 23
id pdf2json/2021.acl-long.4.pdf.json
we O
collapse O
the O
latter O
two O
labels O
into O
a O
single O
non-hateful O
label O
to O
match O
hatecheck O
’ O
s O
binary O
format O
, O
resulting O
in O
1,430 O
tweets O
( O
5.8 O
% O
) O
labelled O
hateful O
and O
23,353 O
( O
94.2 O
% O
) O
labelled O
non-hateful O
. O

section 23
id pdf2json/2021.acl-long.4.pdf.json
definition O
of O
hate O
speech O
“ O
language O
that O
is O
used O
to O
expresses O
hatred O
towards O
a O
targeted O
group O
or O
is O
intended O
to O
be O
derogatory O
, O
to O
humiliate O
, O
or O
to O
insult O
the O
members O
of O
the O
group O
” O
. O

section 23
id pdf2json/2021.acl-long.4.pdf.json
d.2 O
founta O
et O
al O
. O

section 23
id pdf2json/2021.acl-long.4.pdf.json
( O
2018 O
) O
data O
sampling O
founta O
et O
al O
. O

section 23
id pdf2json/2021.acl-long.4.pdf.json
( O
2018 O
) O
initially O
collected O
a O
random O
set O
of O
32 O
million O
tweets O
from O
twitter O
. O

section 23
id pdf2json/2021.acl-long.4.pdf.json
they O
then O
used O
a O
boosted O
random O
sampling O
procedure O
based O
on O
negative O
sentiment O
and O
occurrence O
of O
offensive O
words O
as O
selected O
from O
hatebase.org O
to O
augment O
a O
random O
subset O
of O
this O
initial O
sample O
with O
tweets O
they O
expected O
to O
be O
more O
likely O
to O
be O
hateful O
or O
abusive O
. O

section 23
id pdf2json/2021.acl-long.4.pdf.json
annotation O
the O
authors O
hired O
crowd O
workers O
from O
crowdflower O
to O
annotate O
each O
tweet O
as O
hateful O
, O
abusive O
, O
spam O
or O
normal O
. O

section 23
id pdf2json/2021.acl-long.4.pdf.json
all O
tweets O
were O
annotated O
by O
five O
crowd O
workers O
. O

section 23
id pdf2json/2021.acl-long.4.pdf.json
for O
inter-annotator O
agreement O
, O
the O
authors O
report O
that O
55.9 O
% O
of O
tweets O
had O
four O
out O
of O
five O
annotators O
agreeing O
on O
a O
label O
. O

section 23
id pdf2json/2021.acl-long.4.pdf.json
data O
the O
authors O
provided O
us O
access O
to O
the O
full O
text O
versions O
of O
99,996 O
annotated O
tweets O
. O

section 23
id pdf2json/2021.acl-long.4.pdf.json
these O
correspond O
to O
the O
tweet O
ids O
made O
available O
by O
the O
authors O
on O
github.com/encaseh2020/hatespeechtwitter O
. O

section 23
id pdf2json/2021.acl-long.4.pdf.json
4,965 O
tweets O
( O
5.0 O
% O
) O
are O
labelled O
hateful O
, O
27,150 O
( O
27.2 O
% O
) O
abusive O
, O
14,030 O
( O
14.0 O
% O
) O
spam O
and O
53,851 O
( O
53.9 O
% O
) O
normal O
. O

section 23
id pdf2json/2021.acl-long.4.pdf.json
we O
collapse O
the O
latter O
three O
labels O
into O
a O
single O
non-hateful O
label O
to O
match O
hatecheck O
’ O
s O
binary O
format O
, O
resulting O
in O
4,965 O
tweets O
( O
5.0 O
% O
) O
labelled O
hateful O
and O
95,031 O
tweets O
( O
95.0 O
% O
) O
labelled O
non-hateful O
. O

section 23
id pdf2json/2021.acl-long.4.pdf.json
definition O
of O
hate O
speech O
“ O
language O
used O
to O
express O
hatred O
towards O
a O
targeted O
individual O
or O
group O
, O
or O
is O
intended O
to O
be O
derogatory O
, O
to O
humiliate O
, O
or O
to O
insult O
the O
members O
of O
the O
group O
, O
on O
the O
basis O
of O
attributes O
such O
as O
race O
, O
religion O
, O
ethnic O
origin O
, O
sexual O
orientation O
, O
disability O
, O
or O
gender O
” O
. O

section 23
id pdf2json/2021.acl-long.4.pdf.json
d.3 O
pre-processing O
before O
using O
the O
datasets O
for O
fine-tuning O
, O
we O
lowercase O
all O
text O
and O
remove O
newline O
and O
tab O
characters O
. O

section 23
id pdf2json/2021.acl-long.4.pdf.json
we O
replace O
urls O
, O
user O
mentions O
and O
emojis O
with O
[ O
url O
] O
, O
[ O
user O
] O
and O
[ O
emoji O
] O
tokens O
. O

section 23
id pdf2json/2021.acl-long.4.pdf.json
we O
also O
split O
hashtags O
into O
separate O
tokens O
using O
the O
wordsegment O
python O
package O
. O

section 24
id pdf2json/2021.acl-long.4.pdf.json
model O
architecture O
we O
implemented O
uncased O
bert-base O
models O
( O
devlin O
et O
al. O
, O
2019 O
) O
using O
the O
transformers O
python O
library O
( O
wolf O
et O
al. O
, O
2020 O
) O
. O

section 24
id pdf2json/2021.acl-long.4.pdf.json
uncased O
bert-base O
, O
which O
is O
trained O
on O
lower-cased O
english O
text O
, O
has O
12 O
layers O
, O
a O
hidden O
layer O
size O
of O
768 O
, O
12 O
attention O
heads O
and O
a O
total O
of O
110 O
million O
parameters O
. O

section 24
id pdf2json/2021.acl-long.4.pdf.json
for O
sequence O
classification O
, O
we O
added O
a O
linear O
layer O
with O
softmax O
output O
. O

section 24
id pdf2json/2021.acl-long.4.pdf.json
fine-tuning O
b-d O
was O
fine-tuned O
on O
binary O
davidson O
et O
al O
. O

section 24
id pdf2json/2021.acl-long.4.pdf.json
( O
2017 O
) O
data O
and O
b-f O
on O
binary O
founta O
et O
al O
. O

section 24
id pdf2json/2021.acl-long.4.pdf.json
( O
2018 O
) O
data O
. O

section 24
id pdf2json/2021.acl-long.4.pdf.json
for O
both O
datasets O
, O
we O
used O
a O
stratified O
80/10/10 O
train/dev/test O
split O
. O

section 24
id pdf2json/2021.acl-long.4.pdf.json
models O
were O
trained O
for O
three O
epochs O
each O
. O

section 24
id pdf2json/2021.acl-long.4.pdf.json
training O
batch O
size O
was O
16 O
. O

section 24
id pdf2json/2021.acl-long.4.pdf.json
we O
used O
cross-entropy O
loss O
with O
class O
weights O
emphasising O
the O
hateful O
minority O
class O
. O

section 24
id pdf2json/2021.acl-long.4.pdf.json
weights O
were O
set O
to O
the O
relative O
proportion O
of O
the O
other O
class O
in O
the O
training O
data O
, O
meaning O
that O
for O
a O
1:9 O
hateful O
: O
non-hateful O
case O
split O
, O
loss O
on O
hateful O
cases O
would O
be O
multiplied O
by O
9 O
. O

section 24
id pdf2json/2021.acl-long.4.pdf.json
the O
optimiser O
was O
adamw O
( O
loshchilov O
and O
hutter O
, O
2019 O
) O
with O
a O
5e-5 O
learning O
rate O
and O
a O
0.01 O
weight O
decay O
. O

section 24
id pdf2json/2021.acl-long.4.pdf.json
for O
regularisation O
, O
we O
set O
a O
10 O
% O
dropout O
probability O
. O

section 24
id pdf2json/2021.acl-long.4.pdf.json
hyperparameter O
tuning O
the O
number O
of O
finetuning O
epochs O
, O
the O
learning O
rate O
and O
the O
training O
batch O
size O
were O
determined O
by O
exhaustive O
grid O
search O
. O

section 24
id pdf2json/2021.acl-long.4.pdf.json
we O
used O
the O
range O
of O
possible O
values O
recommended O
by O
devlin O
et O
al O
. O

section 24
id pdf2json/2021.acl-long.4.pdf.json
( O
2019 O
) O
: O
[ O
2 O
, O
3 O
, O
4 O
] O
for O
epochs O
, O
[ O
2e-5 O
, O
3e-5 O
, O
5e-5 O
] O
for O
learning O
rate O
and O
[ O
16 O
, O
32 O
] O
for O
batch O
size O
. O

section 24
id pdf2json/2021.acl-long.4.pdf.json
there O
were O
18 O
training/evaluation O
runs O
for O
each O
model O
. O

section 24
id pdf2json/2021.acl-long.4.pdf.json
the O
best O
configuration O
was O
selected O
based O
on O
loss O
on O
the O
10 O
% O
development O
set O
. O

section 24
id pdf2json/2021.acl-long.4.pdf.json
held-out O
performance O
micro/macro O
f1 O
scores O
on O
the O
held-out O
test O
sets O
corresponding O
to O
their O
training O
data O
are O
91.5/70.8 O
for O
b-d O
( O
davidson O
et O
al. O
, O
2017 O
) O
and O
92.9/70.3 O
for O
b-f O
( O
founta O
et O
al. O
, O
2018 O
) O
. O

section 24
id pdf2json/2021.acl-long.4.pdf.json
computation O
we O
ran O
all O
computations O
on O
a O
microsoft O
azure O
“ O
standard O
nc24 O
” O
server O
equipped O
with O
two O
nvidia O
tesla O
k80 O
gpu O
cards O
. O

section 24
id pdf2json/2021.acl-long.4.pdf.json
the O
average O
wall O
time O
for O
each O
hyperparameter O
tuning O
trial O
of O
b-d O
was O
around O
17 O
minutes O
, O
and O
for O
b-f O
around O
70 O
minutes O
. O

section 24
id pdf2json/2021.acl-long.4.pdf.json
source O
code O
our O
code O
is O
available O
on O
github.com/paul-rottger/hatecheck-experiments O
. O

section 25
id pdf2json/2021.acl-long.4.pdf.json
most O
previous O
work O
that O
trains O
and O
evaluates O
models O
on O
davidson O
et O
al O
. O

section 25
id pdf2json/2021.acl-long.4.pdf.json
( O
2017 O
) O
and O
founta O
et O
al O
. O

section 25
id pdf2json/2021.acl-long.4.pdf.json
( O
2018 O
) O
data O
uses O
their O
original O
multiclass O
label O
format O
. O

section 25
id pdf2json/2021.acl-long.4.pdf.json
in O
the O
multiclass O
case O
, O
the O
relative O
size O
of O
the O
hateful O
class O
compared O
to O
the O
non-hateful O
classes O
is O
larger O
than O
in O
the O
binary O
case O
, O
which O
is O
likely O
why O
most O
models O
do O
not O
use O
class O
weights O
. O

section 25
id pdf2json/2021.acl-long.4.pdf.json
for O
comparability O
, O
we O
thus O
fine-tuned O
unweighted O
multiclass O
versions O
of O
b-d O
and O
b-f O
, O
using O
the O
same O
model O
parameters O
described O
in O
appendix O
e. O
on O
multiclass O
davidson O
et O
al O
. O

section 25
id pdf2json/2021.acl-long.4.pdf.json
( O
2017 O
) O
data O
, O
mozafari O
et O
al O
. O

section 25
id pdf2json/2021.acl-long.4.pdf.json
( O
2019 O
) O
report O
a O
weighted-average O
f1 O
score O
of O
91 O
for O
their O
bert-base O
model O
and O
92 O
for O
bertbase O
combined O
with O
a O
cnn O
. O

section 25
id pdf2json/2021.acl-long.4.pdf.json
cao O
et O
al O
. O

section 25
id pdf2json/2021.acl-long.4.pdf.json
( O
2020 O
) O
report O
a O
micro O
f1 O
of O
89.9 O
for O
their O
ensemble-like O
“ O
deephate O
” O
classifier O
. O

section 25
id pdf2json/2021.acl-long.4.pdf.json
our O
unweighted O
multiclass O
bert-base O
model O
achieves O
90.7 O
weighted-average O
f1 O
and O
91.1 O
micro O
f1 O
. O

section 25
id pdf2json/2021.acl-long.4.pdf.json
on O
multiclass O
founta O
et O
al O
. O

section 25
id pdf2json/2021.acl-long.4.pdf.json
( O
2018 O
) O
data O
, O
cao O
et O
al O
. O

section 25
id pdf2json/2021.acl-long.4.pdf.json
( O
2020 O
) O
report O
a O
micro O
f1 O
of O
79.1 O
for O
“ O
deephate O
” O
. O

section 25
id pdf2json/2021.acl-long.4.pdf.json
our O
unweighted O
multiclass O
bert-base O
model O
achieves O
81.7 O
micro O
f1 O
. O

section 25
id pdf2json/2021.acl-long.4.pdf.json
tran O
et O
al O
. O

section 25
id pdf2json/2021.acl-long.4.pdf.json
( O
2020 O
) O
recently O
achieved O
sota O
on O
several O
other O
hate O
speech O
datasets O
with O
their O
habertor O
model O
. O

section 25
id pdf2json/2021.acl-long.4.pdf.json
they O
also O
find O
that O
bertbase O
consistently O
performs O
very O
near O
their O
sota O
. O

section 25
id pdf2json/2021.acl-long.4.pdf.json
however O
, O
they O
do O
not O
evaluate O
their O
models O
on O
davidson O
et O
al O
. O

section 25
id pdf2json/2021.acl-long.4.pdf.json
( O
2017 O
) O
or O
founta O
et O
al O
. O

section 25
id pdf2json/2021.acl-long.4.pdf.json
( O
2018 O
) O
data O
. O

section TITLE
id pdf2json/2021.acl-long.378.pdf.json
parameter-efficient O
transfer O
learning O
with O
diff O
pruning O

section ABSTRACT
id pdf2json/2021.acl-long.378.pdf.json
the O
large O
size O
of O
pretrained O
networks O
makes O
them O
difficult O
to O
deploy O
for O
multiple O
tasks O
in O
storage-constrained O
settings O
. O

section ABSTRACT
id pdf2json/2021.acl-long.378.pdf.json
diff O
pruning O
enables O
parameter-efficient O
transfer O
learning O
that O
scales O
well O
with O
new O
tasks O
. O

section ABSTRACT
id pdf2json/2021.acl-long.378.pdf.json
the O
approach O
learns O
a O
task-specific O
“ O
diff O
” O
vector O
that O
extends O
the O
original O
pretrained O
parameters O
. O

section ABSTRACT
id pdf2json/2021.acl-long.378.pdf.json
this O
diff O
vector O
is O
adaptively O
pruned O
during O
training O
with O
a O
differentiable O
approximation O
to O
the O
l0-norm O
penalty O
to O
encourage O
sparsity O
. O

section ABSTRACT
id pdf2json/2021.acl-long.378.pdf.json
as O
the O
number O
of O
tasks O
increases O
, O
diff O
pruning O
remains O
parameter-efficient O
, O
as O
it O
requires O
storing O
only O
a O
small O
diff O
vector O
for O
each O
task O
. O

section ABSTRACT
id pdf2json/2021.acl-long.378.pdf.json
since O
it O
does O
not O
require O
access O
to O
all O
tasks O
during O
training O
, O
it O
is O
attractive O
in O
on-device O
deployment O
settings O
where O
tasks O
arrive O
in O
stream O
or O
even O
from O
different O
providers O
. O

section ABSTRACT
id pdf2json/2021.acl-long.378.pdf.json
diff O
pruning O
can O
match O
the O
performance O
of O
finetuned O
baselines O
on O
the O
glue O
benchmark O
while O
only O
modifying O
0.5 O
% O
of O
the O
pretrained O
model O
’ O
s O
parameters O
per O
task O
and O
scales O
favorably O
in O
comparison O
to O
popular O
pruning O
approaches O
. O

section 0
id pdf2json/2021.acl-long.378.pdf.json
proceedings O
of O
the O
59th O
annual O
meeting O
of O
the O
association O
for O
computational O
linguistics O
and O
the O
11th O
international O
joint O
conference O
on O
natural O
language O
processing O
, O
pages O
4884–4896 O
august O
1–6 O
, O
2021 O
. O

section 0
id pdf2json/2021.acl-long.378.pdf.json
©2021 O
association O
for O
computational O
linguistics O
4884 O

section 1
id pdf2json/2021.acl-long.378.pdf.json
task-specific O
finetuning O
of O
pretrained O
deep O
networks O
is O
the O
dominant O
paradigm O
in O
contemporary O
nlp O
, O
achieving O
state-of-the-art O
results O
across O
a O
suite O
of O
natural O
language O
understanding O
tasks O
( O
devlin O
et O
al. O
, O
2019 O
; O
liu O
et O
al. O
, O
2019c O
; O
yang O
et O
al. O
, O
2019 O
; O
lan O
et O
al. O
, O
2020 O
) O
. O

section 1
id pdf2json/2021.acl-long.378.pdf.json
while O
straightforward O
and O
empirically O
effective O
, O
this O
approach O
is O
difficult O
to O
scale O
to O
multi-task O
, O
memory-constrained O
settings O
( O
e.g O
. O

section 1
id pdf2json/2021.acl-long.378.pdf.json
for O
on-device O
applications O
) O
, O
as O
it O
requires O
shipping O
and O
storing O
a O
full O
set O
of O
model O
parameters O
for O
each O
task O
. O

section 1
id pdf2json/2021.acl-long.378.pdf.json
inasmuch O
as O
these O
models O
are O
learning O
generalizable O
, O
task-agnostic O
language O
representations O
through O
self-supervised O
pretraining O
, O
finetuning O
the O
entire O
model O
for O
each O
task O
seems O
especially O
profligate O
. O

section 1
id pdf2json/2021.acl-long.378.pdf.json
code O
: O
https O
: O
//github.com/dguo98/diffpruning O
a O
popular O
approach O
to O
parameter-efficiency O
is O
to O
learn O
smaller O
compressed O
models O
for O
each O
task O
( O
gordon O
et O
al. O
, O
2020 O
; O
sajjad O
et O
al. O
, O
2020 O
; O
zhao O
et O
al. O
, O
2020 O
; O
sanh O
et O
al. O
, O
2020 O
) O
. O

section 1
id pdf2json/2021.acl-long.378.pdf.json
such O
approaches O
face O
a O
steep O
sparsity/performance O
tradeoff O
and O
keep O
a O
substantial O
amount O
of O
nonzero O
parameters O
per O
task O
( O
e.g O
. O

section 1
id pdf2json/2021.acl-long.378.pdf.json
10 O
% O
-30 O
% O
) O
. O

section 1
id pdf2json/2021.acl-long.378.pdf.json
multi-task O
learning O
and O
featurebased O
transfer O
allow O
for O
more O
parameter-efficient O
transfer O
learning O
per O
task O
( O
liu O
et O
al. O
, O
2019b O
; O
clark O
et O
al. O
, O
2019 O
; O
stickland O
& O
murray O
, O
2019 O
; O
reimers O
& O
gurevych O
, O
2019 O
) O
. O

section 1
id pdf2json/2021.acl-long.378.pdf.json
these O
methods O
train O
a O
small O
number O
of O
additional O
parameters O
( O
e.g O
. O

section 1
id pdf2json/2021.acl-long.378.pdf.json
a O
linear O
layer O
) O
on O
top O
of O
a O
shared O
model O
. O

section 1
id pdf2json/2021.acl-long.378.pdf.json
however O
, O
multi-task O
learning O
generally O
requires O
access O
to O
all O
tasks O
during O
training O
to O
prevent O
catastrophic O
forgetting O
( O
french O
, O
1999 O
) O
, O
while O
feature-based O
transfer O
learning O
( O
e.g O
. O

section 1
id pdf2json/2021.acl-long.378.pdf.json
based O
on O
task-agnostic O
sentence O
representations O
) O
is O
typically O
outperformed O
by O
finetuning O
( O
howard O
& O
ruder O
, O
2018 O
) O
. O

section 1
id pdf2json/2021.acl-long.378.pdf.json
an O
appealing O
middle O
ground O
is O
to O
finetune O
an O
extension O
of O
the O
base O
model O
for O
specific O
tasks O
. O

section 1
id pdf2json/2021.acl-long.378.pdf.json
this O
approach O
captures O
the O
training O
benefits O
of O
finetuning O
while O
maintaining O
the O
task O
modularity O
of O
feature-based O
transfer O
. O

section 1
id pdf2json/2021.acl-long.378.pdf.json
for O
example O
, O
adapters O
( O
rebuffi O
et O
al. O
, O
2018 O
) O
use O
smaller O
, O
task-specific O
modules O
that O
are O
inserted O
between O
layers O
of O
a O
model O
this O
approach O
does O
not O
require O
access O
to O
all O
tasks O
during O
training O
, O
targeting O
realistic O
settings O
where O
as O
new O
tasks O
arrive O
in O
stream O
( O
houlsby O
et O
al. O
, O
2019 O
; O
pfeiffer O
et O
al. O
, O
2020a O
, O
b O
, O
c O
) O
. O

section 1
id pdf2json/2021.acl-long.378.pdf.json
houlsby O
et O
al O
. O

section 1
id pdf2json/2021.acl-long.378.pdf.json
( O
2019 O
) O
find O
that O
adapter O
layers O
can O
match O
the O
performance O
of O
fully O
finetuned O
bert O
on O
the O
glue O
benchmark O
while O
requiring O
3.6 O
% O
additional O
parameters O
( O
on O
average O
) O
per O
task O
. O

section 1
id pdf2json/2021.acl-long.378.pdf.json
diff O
pruning O
is O
a O
new O
extension O
to O
pretrained O
models O
with O
the O
goal O
of O
even O
more O
parameterefficient O
transfer O
learning O
. O

section 1
id pdf2json/2021.acl-long.378.pdf.json
instead O
of O
modifying O
the O
architecture O
of O
the O
model O
, O
diff O
pruning O
extends O
the O
base O
model O
through O
a O
task-specific O
difference O
vector O
. O

section 1
id pdf2json/2021.acl-long.378.pdf.json
in O
order O
to O
learn O
this O
vector O
, O
we O
reparameterize O
the O
task-specific O
model O
parameters O
as O
θtask O
= O
θpretrained O
+ O
δtask O
, O
where O
the O
pretrained O
parameter O
vector O
θpretrained O
is O
fixed O
and O
the O
task-specific O
diff O
vector O
δtask O
is O
finetuned O
. O

section 1
id pdf2json/2021.acl-long.378.pdf.json
the O
diff O
vector O
is O
regularized O
with O
a O
differentiable O
approximation O
to O
the O
l0-norm O
penalty O
( O
louizos O
et O
al. O
, O
2018 O
) O
to O
encourage O
sparsity O
. O

section 1
id pdf2json/2021.acl-long.378.pdf.json
diff O
pruning O
can O
become O
extremely O
parameterefficient O
, O
as O
it O
only O
requires O
storing O
the O
nonzero O
positions O
and O
weights O
of O
the O
diff O
vector O
for O
each O
task O
. O

section 1
id pdf2json/2021.acl-long.378.pdf.json
the O
cost O
of O
storing O
the O
shared O
pretrained O
model O
remains O
constant O
and O
is O
amortized O
across O
multiple O
tasks O
. O

section 1
id pdf2json/2021.acl-long.378.pdf.json
on O
the O
glue O
benchmark O
( O
wang O
et O
al. O
, O
2019a O
) O
, O
diff O
pruning O
can O
match O
the O
performance O
of O
the O
fully O
finetuned O
bert O
baselines O
while O
finetuning O
only O
0.5 O
% O
of O
the O
pretrained O
parameters O
per O
task O
. O

section 1
id pdf2json/2021.acl-long.378.pdf.json
as O
the O
number O
of O
tasks O
increase O
, O
diff O
pruning O
outperforms O
popular O
pruning-based O
methods O
in O
amount O
of O
storage O
required O
. O

section 2
id pdf2json/2021.acl-long.378.pdf.json
transfer O
learning O
in O
nlp O
mostly O
uses O
a O
pretrainand-finetune O
paradigm O
, O
which O
initializes O
a O
subset O
of O
the O
model O
parameters O
for O
all O
tasks O
from O
a O
pretrained O
model O
and O
then O
finetunes O
on O
a O
task-specific O
objective O
. O

section 2
id pdf2json/2021.acl-long.378.pdf.json
pretraining O
objectives O
include O
context O
prediction O
( O
mikolov O
et O
al. O
, O
2013 O
) O
, O
autoencoding O
( O
dai O
& O
le O
, O
2015 O
) O
, O
machine O
translation O
( O
mccann O
et O
al. O
, O
2017 O
) O
, O
and O
more O
recently O
, O
variants O
of O
language O
modeling O
( O
peters O
et O
al. O
, O
2018 O
; O
radford O
et O
al. O
, O
2018 O
; O
devlin O
et O
al. O
, O
2019 O
) O
objectives O
. O

section 2
id pdf2json/2021.acl-long.378.pdf.json
here O
we O
consider O
applying O
transfer O
learning O
to O
multiple O
tasks O
. O

section 2
id pdf2json/2021.acl-long.378.pdf.json
we O
consider O
a O
setting O
with O
a O
potentially O
unknown O
set O
of O
tasks O
( O
which O
may O
arrive O
in O
stream O
) O
, O
where O
each O
task O
τ O
∈ O
t O
has O
an O
associated O
training O
set O
dτ O
= O
{ O
x O
( O
n O
) O
τ O
, O
y O
( O
n O
) O
τ O
} O
nn=1 O
. O

section 2
id pdf2json/2021.acl-long.378.pdf.json
for O
all O
tasks O
, O
the O
goal O
is O
to O
produce O
( O
possibly O
tied O
) O
model O
parameters O
θτ O
to O
minimize O
the O
empirical O
risk O
, O
min O
θτ O
1 O
n O
n∑ O
n=1 O
c O
( O
fτ O
( O
x O
( O
n O
) O
τ O
; O
θτ O
) O
, O
y O
( O
n O
) O
τ O
) O
+ O
λr O
( O
θτ O
) O
where O
fτ O
( O
· O
; O
θτ O
) O
is O
a O
parameterized O
function O
over O
the O
input O
( O
e.g O
. O

section 2
id pdf2json/2021.acl-long.378.pdf.json
a O
neural O
network O
) O
, O
c O
( O
· O
, O
· O
) O
is O
a O
loss O
function O
( O
e.g O
. O

section 2
id pdf2json/2021.acl-long.378.pdf.json
cross-entropy O
) O
,1 O
and O
r O
( O
· O
) O
is O
an O
optional O
regularizer O
with O
hyperparameter O
λ O
. O

section 2
id pdf2json/2021.acl-long.378.pdf.json
we O
can O
use O
the O
pretrain-finetune O
approach O
by O
simply O
learning O
independent O
parameters O
for O
each O
1while O
the O
loss O
function O
can O
be O
in O
principle O
task-specific O
, O
in O
practice O
we O
use O
cross O
entropy O
for O
all O
tasks O
and O
hence O
omit O
the O
subscript O
in O
c O
( O
· O
, O
· O
) O
. O

section 2
id pdf2json/2021.acl-long.378.pdf.json
task O
. O

section 2
id pdf2json/2021.acl-long.378.pdf.json
however O
, O
the O
large O
size O
of O
pretrained O
models O
makes O
this O
approach O
exceedingly O
parameter O
inefficient O
. O

section 2
id pdf2json/2021.acl-long.378.pdf.json
for O
example O
, O
widely-adopted O
models O
such O
as O
bertbase O
and O
bertlarge O
have O
110m O
and O
340m O
parameters O
respectively O
, O
while O
their O
contemporaries O
have O
parameter O
counts O
in O
the O
billions O
( O
raffel O
et O
al. O
, O
2020 O
; O
shoeybi O
et O
al. O
, O
2019 O
; O
rajbhandari O
et O
al. O
, O
2019 O
) O
. O

section 2
id pdf2json/2021.acl-long.378.pdf.json
storing O
the O
fully O
finetuned O
models O
therefore O
becomes O
difficult O
even O
for O
a O
moderate O
number O
of O
tasks.2 O
a O
classic O
approach O
to O
tackling O
this O
parameter-inefficiencyis O
to O
train O
a O
single O
shared O
model O
( O
along O
with O
a O
task-specific O
output O
layer O
) O
against O
multiple O
tasks O
through O
joint O
training O
( O
caruana O
, O
1997 O
) O
. O

section 2
id pdf2json/2021.acl-long.378.pdf.json
however O
, O
the O
usual O
formulation O
of O
multi-task O
learning O
requires O
the O
set O
of O
tasks O
t O
to O
be O
known O
in O
advance O
in O
order O
to O
prevent O
catastrophic O
forgetting O
( O
french O
, O
1999 O
) O
,3 O
making O
it O
unsuitable O
for O
applications O
in O
which O
the O
set O
of O
tasks O
is O
unknown O
or O
when O
tasks O
arrive O
in O
stream O
. O

section 3
id pdf2json/2021.acl-long.378.pdf.json
diff O
pruning O
formulates O
task-specific O
finetuning O
as O
learning O
a O
diff O
vector O
δτ O
that O
is O
added O
to O
the O
pretrained O
model O
parameters O
θ O
, O
which O
remain O
fixed O
. O

section 3
id pdf2json/2021.acl-long.378.pdf.json
we O
first O
reparameterize O
the O
task-specific O
model O
parameters O
, O
θτ O
= O
θ O
+ O
δτ O
, O
which O
results O
in O
the O
following O
empirical O
risk O
minimization O
problem O
, O
min O
δτ O
l O
( O
dτ O
, O
fτ O
, O
θ O
+ O
δτ O
) O
+ O
λr O
( O
θ O
+ O
δτ O
) O
, O
where O
for O
brevity O
we O
define O
l O
( O
dτ O
, O
fτ O
, O
θτ O
) O
as O
l O
( O
dτ O
, O
fτ O
, O
θτ O
) O
= O
1 O
n O
n∑ O
n=1 O
c O
( O
fτ O
( O
x O
( O
n O
) O
τ O
; O
θτ O
) O
, O
y O
( O
n O
) O
τ O
) O
. O

section 3
id pdf2json/2021.acl-long.378.pdf.json
this O
trivial O
reparameterization O
shows O
that O
the O
cost O
of O
storing O
the O
pretrained O
parameters O
θ O
is O
amortized O
across O
tasks O
, O
and O
the O
only O
marginal O
cost O
for O
new O
tasks O
is O
the O
diff O
vector O
. O

section 3
id pdf2json/2021.acl-long.378.pdf.json
if O
we O
can O
regularize O
δτ O
to O
be O
sparse O
such O
that O
‖δτ‖0 O
‖θ‖0 O
, O
then O
this O
approach O
can O
become O
more O
parameter-efficient O
as O
2an O
intriguing O
line O
of O
work O
suggests O
that O
large-scale O
language O
models O
can O
be O
used O
without O
finetuning O
for O
a O
variety O
of O
tasks O
if O
given O
the O
appropriate O
context O
( O
radford O
et O
al. O
, O
2019 O
; O
brown O
et O
al. O
, O
2020 O
) O
. O

section 3
id pdf2json/2021.acl-long.378.pdf.json
while O
interesting O
, O
these O
models O
generally O
underperform O
task-specific O
models O
and O
require O
billions O
of O
parameters O
, O
though O
recent O
work O
suggests O
that O
they O
can O
be O
made O
substantially O
smaller O
( O
schick O
& O
schutze O
, O
2020 O
) O
. O

section 3
id pdf2json/2021.acl-long.378.pdf.json
3however O
, O
work O
on O
continual O
learning O
mitigates O
these O
issues O
to O
an O
extent O
( O
shin O
et O
al. O
, O
2017 O
; O
lopez-paz O
& O
ranzato O
, O
2017 O
; O
lee O
et O
al. O
, O
2017 O
; O
kirkpatrick O
et O
al. O
, O
2017 O
) O
. O

section 3
id pdf2json/2021.acl-long.378.pdf.json
the O
number O
of O
tasks O
increases O
. O

section 3
id pdf2json/2021.acl-long.378.pdf.json
we O
can O
specify O
this O
goal O
with O
an O
l0-norm O
penalty O
on O
the O
diff O
vector O
, O
r O
( O
θ O
+ O
δτ O
) O
= O
‖δτ‖0 O
= O
d∑ O
i=1 O
1 O
{ O
δτ O
, O
i O
6= O
0 O
} O
. O

section 4
id pdf2json/2021.acl-long.378.pdf.json
l0-norm O
this O
regularizer O
is O
difficult O
to O
optimize O
as O
it O
is O
nondifferentiable O
. O

section 4
id pdf2json/2021.acl-long.378.pdf.json
in O
order O
to O
approximate O
this O
l0 O
objective O
, O
we O
follow O
an O
approach O
for O
gradient-based O
learning O
with O
l0 O
sparsity O
using O
a O
relaxed O
mask O
vector O
( O
louizos O
et O
al. O
, O
2018 O
) O
. O

section 4
id pdf2json/2021.acl-long.378.pdf.json
this O
approach O
involves O
relaxing O
a O
binary O
vector O
into O
continuous O
space O
, O
and O
then O
multiplying O
it O
with O
a O
dense O
weight O
vector O
to O
determine O
how O
much O
of O
the O
weight O
vector O
is O
applied O
during O
training O
. O

section 4
id pdf2json/2021.acl-long.378.pdf.json
after O
training O
, O
the O
mask O
is O
made O
deterministic O
, O
and O
a O
large O
portion O
of O
the O
diff O
vector O
is O
zero.4 O
to O
apply O
this O
method O
we O
first O
decompose O
δτ O
into O
a O
binary O
mask O
vector O
multiplied O
with O
a O
dense O
vector O
, O
δτ O
= O
zτ O
wτ O
, O
zτ O
∈ O
{ O
0 O
, O
1 O
} O
d O
, O
wτ O
∈ O
rd O
. O

section 4
id pdf2json/2021.acl-long.378.pdf.json
we O
now O
lower O
bound O
the O
true O
objective O
and O
optimize O
an O
expectation O
with O
respect O
to O
zτ O
, O
whose O
distribution O
p O
( O
zτ O
; O
ατ O
) O
is O
initially O
bernoulli O
with O
introduced O
parameters O
ατ O
, O
min O
ατ O
, O
wτ O
ezτ∼p O
( O
zτ O
; O
ατ O
) O
[ O
l O
( O
dτ O
, O
fτ O
, O
θ O
+ O
δτ O
) O
+ O
λ‖δτ‖0 O
] O
. O

section 4
id pdf2json/2021.acl-long.378.pdf.json
this O
objective O
is O
still O
complicated O
by O
the O
discrete O
nature O
of O
zτ O
’ O
s O
, O
but O
the O
expectation O
provides O
some O
guidance O
for O
empirically O
effective O
relaxations O
. O

section 4
id pdf2json/2021.acl-long.378.pdf.json
we O
follow O
prior O
work O
( O
louizos O
et O
al. O
, O
2018 O
; O
wang O
et O
al. O
, O
2019b O
) O
and O
relax O
zτ O
into O
continuous O
space O
[ O
0 O
, O
1 O
] O
d O
with O
a O
stretched O
hard-concrete O
distribution O
( O
jang O
et O
al. O
, O
2017 O
; O
maddison O
et O
al. O
, O
2017 O
) O
, O
which O
allows O
for O
the O
use O
of O
pathwise O
gradient O
estimators O
. O

section 4
id pdf2json/2021.acl-long.378.pdf.json
specifically O
, O
zτ O
is O
now O
defined O
to O
be O
a O
deterministic O
and O
( O
sub O
) O
differentiable O
function O
of O
a O
sample O
u O
from O
a O
uniform O
distribution O
, O
u O
∼ O
u O
( O
0,1 O
) O
, O
sτ O
= O
σ O
( O
logu− O
log O
( O
1− O
u O
) O
+ O
ατ O
) O
, O
s̄τ O
= O
sτ O
× O
( O
r O
− O
l O
) O
+ O
l O
, O
zτ O
= O
min O
( O
1 O
, O
max O
( O
0 O
, O
s̄τ O
) O
) O
. O

section 4
id pdf2json/2021.acl-long.378.pdf.json
here O
l O
< O
0 O
and O
r O
> O
1 O
are O
two O
constants O
used O
to O
stretch O
sτ O
into O
the O
interval O
( O
l O
, O
r O
) O
d O
before O
it O
is O
4it O
is O
also O
possible O
to O
learn O
sparse O
diff O
vectors O
through O
other O
penalties O
such O
as O
the O
l1-norm O
. O

section 4
id pdf2json/2021.acl-long.378.pdf.json
we O
chose O
to O
work O
with O
the O
relaxed O
l0-norm O
formulation O
as O
past O
work O
has O
shown O
that O
sgd-based O
optimization O
works O
well O
in O
this O
setting O
. O

section 4
id pdf2json/2021.acl-long.378.pdf.json
clamped O
to O
[ O
0 O
, O
1 O
] O
d O
with O
the O
min O
( O
1 O
, O
max O
( O
0 O
, O
· O
) O
) O
operation O
. O

section 4
id pdf2json/2021.acl-long.378.pdf.json
in O
this O
case O
we O
have O
a O
differentiable O
closedform O
expression O
for O
the O
expected O
l0-norm O
, O
e O
[ O
‖δτ‖0 O
] O
= O
d∑ O
i=1 O
σ O
( O
ατ O
, O
i O
− O
log O
−l O
r O
) O
. O

section 4
id pdf2json/2021.acl-long.378.pdf.json
thus O
the O
final O
optimization O
problem O
is O
given O
by O
, O
min O
ατ O
, O
wτ O
eu∼u O
[ O
0,1 O
] O
[ O
l O
( O
dτ O
, O
fτ O
, O
θ O
+ O
zτ O
wτ O
) O
] O
+λ O
d∑ O
i=1 O
σ O
( O
ατ O
, O
i O
− O
log O
−l O
r O
) O
, O
and O
we O
can O
now O
utilize O
pathwise O
gradient O
estimators O
to O
optimize O
the O
first O
term O
with O
respect O
to O
ατ O
since O
the O
expectation O
no O
longer O
depends O
on O
it.5 O
after O
training O
we O
obtain O
the O
final O
diff O
vector O
δτ O
by O
sampling O
u O
once O
to O
obtain O
zτ O
( O
which O
is O
not O
necessarily O
a O
binary O
vector O
but O
has O
a O
significant O
number O
of O
dimensions O
equal O
to O
exactly O
zero O
due O
to O
the O
clamping O
function O
) O
, O
then O
setting O
δτ O
= O
zτ O
wτ O
.6 O
3.2 O
l0-ball O
projection O
with O
magnitude O
pruning O
for O
sparsity O
control O
differentiable O
l0 O
regularization O
allows O
us O
to O
achieve O
a O
high O
sparsity O
rate O
. O

section 4
id pdf2json/2021.acl-long.378.pdf.json
however O
, O
it O
would O
be O
ideal O
to O
set O
an O
exact O
sparsity O
rate O
, O
especially O
considering O
applications O
which O
require O
parameter O
budgets O
. O

section 4
id pdf2json/2021.acl-long.378.pdf.json
as O
the O
regularization O
coefficient O
λ O
is O
a O
lagrangian O
multiplier O
for O
the O
constraint O
e O
[ O
‖δτ‖0 O
] O
< O
η O
for O
some O
η O
, O
this O
could O
be O
achieved O
in O
principle O
by O
searching O
over O
different O
values O
of O
λ O
. O

section 4
id pdf2json/2021.acl-long.378.pdf.json
however O
we O
found O
it O
more O
efficient O
and O
empirically O
effective O
to O
achieve O
an O
exact O
sparsity O
rate O
by O
projecting O
onto O
a O
target O
l0-ball O
after O
training O
. O

section 4
id pdf2json/2021.acl-long.378.pdf.json
specifically O
, O
we O
use O
magnitude O
pruning O
on O
the O
diff O
vector O
δτ O
and O
target O
a O
sparsity O
rate O
t O
% O
by O
only O
keeping O
the O
top O
t O
% O
× O
d O
values O
in O
δτ O
.7 O
note O
that O
unlike O
standard O
magnitude O
pruning O
, O
this O
is O
based O
on O
the O
magnitude O
of O
the O
diff O
vector O
values O
and O
not O
the O
model O
parameters O
. O

section 4
id pdf2json/2021.acl-long.378.pdf.json
we O
found O
it O
important O
to O
further O
finetune O
δτ O
with O
the O
nonzero O
masks O
fixed O
to O
maintain O
good O
performance O
, O
as O
is O
often O
the O
case O
5to O
reduce O
notation O
clutter O
we O
subsume O
the O
parameters O
of O
the O
task-specific O
output O
layer O
, O
which O
is O
not O
pretrained O
, O
into O
θ O
. O

section 4
id pdf2json/2021.acl-long.378.pdf.json
we O
do O
not O
apply O
the O
l0-norm O
penalty O
on O
these O
parameters O
during O
training O
. O

section 4
id pdf2json/2021.acl-long.378.pdf.json
6we O
found O
sampling O
once O
to O
work O
as O
well O
as O
other O
alternatives O
( O
e.g O
. O

section 4
id pdf2json/2021.acl-long.378.pdf.json
based O
on O
multiple O
samples O
) O
. O

section 4
id pdf2json/2021.acl-long.378.pdf.json
7wang O
et O
al O
. O

section 4
id pdf2json/2021.acl-long.378.pdf.json
( O
2019b O
) O
show O
that O
it O
also O
is O
possible O
to O
inject O
such O
a O
constraint O
softly O
into O
the O
training O
objective O
by O
regularizing O
the O
expected O
model O
size O
towards O
a O
certain O
rate O
. O

section 4
id pdf2json/2021.acl-long.378.pdf.json
however O
, O
since O
the O
constraint O
is O
soft O
this O
approach O
also O
makes O
it O
difficult O
to O
target O
an O
exact O
sparsity O
rate O
. O

section 4
id pdf2json/2021.acl-long.378.pdf.json
in O
magnitude O
pruning O
( O
han O
et O
al. O
, O
2016 O
) O
. O

section 4
id pdf2json/2021.acl-long.378.pdf.json
since O
this O
type O
of O
parameter-efficiency O
through O
projection O
onto O
the O
l0-ball O
can O
be O
applied O
without O
adaptive O
diff O
pruning,8 O
such O
an O
approach O
will O
serve O
as O
one O
of O
our O
baselines O
in O
the O
empirical O
study O
. O

section 5
id pdf2json/2021.acl-long.378.pdf.json
to O
allow O
diff O
pruning O
to O
adapt O
to O
the O
model O
architecture O
, O
we O
consider O
a O
structured O
extension O
which O
incorporates O
dependence O
between O
dimensions O
. O

section 5
id pdf2json/2021.acl-long.378.pdf.json
we O
hypothesize O
that O
this O
approach O
can O
allow O
the O
model O
to O
learn O
to O
modify O
parameters O
in O
local O
regions O
, O
as O
opposed O
to O
treating O
each O
parameter O
independently O
. O

section 5
id pdf2json/2021.acl-long.378.pdf.json
we O
modify O
the O
regularizer O
to O
first O
partition O
the O
parameter O
indices O
into O
g O
groups O
{ O
g O
( O
1 O
) O
, O
. O

section 5
id pdf2json/2021.acl-long.378.pdf.json
. O

section 5
id pdf2json/2021.acl-long.378.pdf.json
. O

section 5
id pdf2json/2021.acl-long.378.pdf.json
, O
g O
( O
g O
) O
} O
where O
g O
( O
j O
) O
is O
a O
subset O
of O
parameter O
indices O
governed O
by O
group O
g O
( O
j O
) O
.9 O
we O
then O
introduce O
a O
scalar O
zjτ O
( O
with O
the O
associated O
parameter O
α O
j O
τ O
) O
for O
each O
group O
g O
( O
j O
) O
, O
and O
decompose O
the O
task-specific O
parameter O
for O
index O
i O
∈ O
g O
( O
j O
) O
as O
δjτ O
, O
i O
= O
zτ O
, O
i O
· O
z O
j O
τ O
·wτ O
, O
i O
. O

section 5
id pdf2json/2021.acl-long.378.pdf.json
the O
expected O
l0-norm O
is O
then O
given O
by O
e O
[ O
‖δτ‖0 O
] O
= O
g∑ O
j=1 O
∑ O
i∈g O
( O
j O
) O
e O
[ O
1 O
{ O
zτ O
, O
i O
· O
zgτ O
> O
0 O
} O
] O
= O
g∑ O
j=1 O
∑ O
i∈g O
( O
j O
) O
σ O
( O
ατ O
, O
i O
− O
log O
−l O
r O
) O
· O
σ O
( O
αjτ O
− O
log O
−l O
r O
) O
. O

section 5
id pdf2json/2021.acl-long.378.pdf.json
we O
can O
train O
with O
gradient-based O
optimization O
as O
before O
. O

section 5
id pdf2json/2021.acl-long.378.pdf.json
parameters O
in O
a O
group O
are O
encouraged O
by O
the O
regularizer O
to O
be O
removed O
jointly O
. O

section 7
id pdf2json/2021.acl-long.378.pdf.json
for O
evaluation O
we O
use O
the O
glue O
benchmark O
( O
wang O
et O
al. O
, O
2019b O
) O
as O
well O
as O
the O
squad O
extractive O
question O
answering O
dataset O
( O
rajpurkar O
et O
al. O
, O
2016 O
) O
. O

section 7
id pdf2json/2021.acl-long.378.pdf.json
following O
adapters O
( O
houlsby O
et O
al. O
, O
2019 O
) O
, O
we O
test O
our O
approach O
on O
the O
following O
subset O
of O
the O
glue O
tasks O
: O
multi-genre O
natural O
language O
inference O
( O
mnli O
) O
, O
where O
the O
goal O
is O
two O
predict O
whether O
the O
relationship O
between O
two O
sentences O
is O
entailment O
, O
contradiction O
, O
or O
neutral O
( O
we O
test O
on O
both O
mnlim O
and O
mnlimm O
which O
respectively O
tests O
on O
matched/mismatched O
domains O
) O
; O
quora O
question O
pairs O
( O
qqp O
) O
, O
a O
classification O
task O
to O
predict O
whether O
two O
question O
are O
semantically O
equivalent O
; O
question O
natural O
language O
inference O
( O
qnli O
) O
, O
which O
8concretely O
, O
one O
can O
obtain O
θτ O
through O
usual O
finetuning O
, O
set O
δτ O
= O
θτ O
− O
θ O
, O
and O
then O
apply O
magnitude O
pruning O
followed O
by O
additional O
finetuning O
on O
δτ O
. O

section 7
id pdf2json/2021.acl-long.378.pdf.json
9while O
groups O
can O
be O
defined O
in O
various O
ways O
, O
we O
found O
that O
defining O
groups O
based O
on O
each O
matrix/bias O
vector O
of O
the O
pretrained O
model O
was O
simple O
and O
worked O
well O
enough O
. O

section 7
id pdf2json/2021.acl-long.378.pdf.json
must O
predict O
whether O
a O
sentence O
is O
a O
correct O
answer O
to O
the O
question O
; O
stanford O
sentiment O
treebank O
( O
sst-2 O
) O
, O
a O
sentence O
classification O
task O
to O
predict O
the O
sentiment O
of O
movie O
reviews O
; O
corpus O
of O
linguistic O
acceptability O
( O
cola O
) O
, O
where O
the O
goal O
is O
predict O
whether O
a O
sentence O
is O
linguistically O
acceptable O
or O
not O
; O
semantic O
textual O
similarity O
benchmark O
( O
stsb O
) O
, O
which O
must O
predict O
a O
similarity O
rating O
between O
two O
sentences O
; O
microsoft O
research O
paraphrase O
corpus O
( O
mrpc O
) O
, O
where O
the O
goal O
is O
to O
predict O
whether O
two O
sentences O
are O
semantically O
equivalent O
; O
recognizing O
textual O
entailment O
( O
rte O
) O
, O
which O
must O
predict O
whether O
a O
second O
sentence O
is O
entailed O
by O
the O
first O
. O

section 7
id pdf2json/2021.acl-long.378.pdf.json
the O
benchmark O
uses O
matthew O
’ O
s O
correlation O
for O
cola O
, O
spearman O
for O
sts-b O
, O
f1 O
score O
for O
mrpc/qqp O
, O
and O
accuracy O
for O
mnli/qnli/sst2/rte O
. O

section 7
id pdf2json/2021.acl-long.378.pdf.json
for O
the O
main O
experiments O
and O
analysis O
, O
we O
use O
the O
bertlarge O
model O
from O
devlin O
et O
al O
. O

section 7
id pdf2json/2021.acl-long.378.pdf.json
( O
2019 O
) O
to O
compare O
against O
the O
adapter-based O
approach O
of O
houlsby O
et O
al O
. O

section 7
id pdf2json/2021.acl-long.378.pdf.json
( O
2019 O
) O
. O

section 7
id pdf2json/2021.acl-long.378.pdf.json
our O
implementation O
is O
based O
on O
the O
hugging O
face O
transformer O
library O
( O
wolf O
et O
al. O
, O
2019 O
) O
. O

section 8
id pdf2json/2021.acl-long.378.pdf.json
we O
compare O
both O
structured O
and O
non-structured O
variants O
of O
diff O
pruning O
against O
the O
following O
baselines O
: O
full O
finetuning O
, O
which O
fully O
finetunes O
bertlarge O
as O
usual O
; O
last O
layer O
finetuning O
, O
which O
only O
finetunes O
the O
penultimate O
layer O
( O
along O
with O
the O
final O
output O
layer O
) O
10 O
; O
adapters O
from O
houlsby O
et O
al O
. O

section 8
id pdf2json/2021.acl-long.378.pdf.json
( O
2019 O
) O
, O
which O
train O
task-specific O
bottleneck O
layers O
between O
each O
layer O
of O
a O
pretrained O
model O
, O
where O
parameter-efficiency O
can O
be O
controlled O
by O
varying O
the O
size O
of O
the O
bottleneck O
layers O
; O
and O
non-adaptive O
diff O
pruning O
, O
which O
performs O
diff O
pruning O
just O
based O
on O
magnitude O
pruning O
( O
i.e. O
, O
we O
obtain O
θτ O
through O
usual O
finetuning O
, O
set O
δτ O
= O
θτ O
− O
θ O
, O
and O
then O
apply O
magnitude O
pruning O
followed O
by O
additional O
finetuning O
on O
δτ O
) O
. O

section 8
id pdf2json/2021.acl-long.378.pdf.json
for O
diff O
pruning O
we O
set O
our O
target O
sparsity O
rate O
to O
0.5 O
% O
and O
investigate O
the O
effect O
of O
different O
target O
sparsity O
rates O
in O
section O
6.1 O
. O

section 9
id pdf2json/2021.acl-long.378.pdf.json
diff O
pruning O
introduces O
additional O
hyperparameters O
l O
, O
r O
( O
for O
stretching O
the O
hard-concrete O
distribution O
) O
and O
λ O
( O
for O
weighting O
the O
approximate O
l0norm O
penalty O
) O
. O

section 9
id pdf2json/2021.acl-long.378.pdf.json
we O
found O
l O
= O
−1.5 O
, O
r O
= O
1.5 O
, O
λ O
= O
1.25 O
× O
10−7 O
to O
work O
well O
across O
all O
tasks O
. O

section 9
id pdf2json/2021.acl-long.378.pdf.json
we O
10wu O
et O
al O
. O

section 9
id pdf2json/2021.acl-long.378.pdf.json
( O
2020 O
) O
observe O
that O
finetuning O
later O
layers O
generally O
performs O
better O
than O
finetuning O
earlier O
layers O
also O
initialize O
the O
weight O
vector O
wτ O
to O
0 O
, O
and O
ατ O
to O
a O
positive O
vector O
( O
we O
use O
5 O
) O
to O
encourage O
zτ O
to O
be O
close O
to O
1 O
at O
the O
start O
of O
training.11 O
while O
we O
mainly O
experiment O
with O
bert O
models O
to O
faciliate O
comparison O
against O
existing O
work O
, O
in O
preliminary O
experiments O
we O
found O
these O
hyperparameters O
to O
work O
for O
finetuning O
roberta O
( O
liu O
et O
al. O
, O
2019c O
) O
and O
xlnet O
( O
yang O
et O
al. O
, O
2019 O
) O
models O
as O
well O
. O

section 9
id pdf2json/2021.acl-long.378.pdf.json
for O
all O
tasks O
we O
initially O
train O
for O
3 O
epochs O
and O
perform O
a O
hyperparameter O
search O
over O
batch O
size O
∈ O
{ O
5 O
, O
8 O
, O
12 O
, O
16 O
} O
and O
learning O
rate O
∈ O
{ O
1×10−5 O
, O
2× O
10−5 O
, O
5× O
10−5 O
} O
.12 O
finetuning O
with O
the O
fixed O
mask O
after O
projecting O
onto O
the O
l0-ball O
with O
magnitude O
pruning O
is O
done O
for O
3 O
epochs O
with O
a O
learning O
rate O
of O
5× O
10−5 O
for O
all O
datasets O
except O
for O
mrpc/stsb/rte/sst-2 O
dataset O
, O
where O
we O
finetune O
for O
5 O
epochs O
. O

section 9
id pdf2json/2021.acl-long.378.pdf.json
the O
exact O
hyperparameters O
for O
each O
task O
are O
given O
in O
section O
a.1 O
of O
the O
appendix O
. O

section 9
id pdf2json/2021.acl-long.378.pdf.json
grouping O
for O
the O
structured O
version O
of O
diff O
pruning O
is O
based O
on O
the O
matrix/bias O
vectors O
( O
i.e O
. O

section 9
id pdf2json/2021.acl-long.378.pdf.json
parameters O
that O
belong O
to O
the O
same O
matrix O
or O
bias O
vector O
are O
assumed O
to O
be O
in O
the O
same O
group O
) O
, O
which O
results O
in O
393 O
groups.13 O

section 11
id pdf2json/2021.acl-long.378.pdf.json
our O
main O
results O
on O
the O
glue O
benchmark O
are O
shown O
in O
table O
1 O
. O

section 11
id pdf2json/2021.acl-long.378.pdf.json
structured O
diff O
pruning O
can O
match O
the O
performance O
of O
a O
fully O
finetuned O
bertlarge O
model O
while O
only O
requiring O
0.5 O
% O
ad- O
11these O
values O
were O
found O
via O
by O
a O
light O
hyperparameter O
search O
on O
the O
sst-2 O
validation O
set O
. O

section 11
id pdf2json/2021.acl-long.378.pdf.json
12however O
we O
found O
the O
default O
settings O
used O
for O
regular O
finetuning O
as O
suggested O
in O
the O
original O
bert O
paper O
to O
work O
well O
for O
most O
tasks O
. O

section 11
id pdf2json/2021.acl-long.378.pdf.json
13this O
definition O
of O
groups O
is O
implementation-specific O
since O
it O
depends O
on O
how O
one O
concatenates O
the O
input O
vector O
before O
each O
affine O
layer O
. O

section 11
id pdf2json/2021.acl-long.378.pdf.json
our O
grouping O
is O
based O
on O
hugging O
face O
’ O
s O
bert O
implementation O
at O
commit O
656e1386a296d696327a9db37de2ccccc79e2cc7 O
. O

section 11
id pdf2json/2021.acl-long.378.pdf.json
we O
found O
this O
simple O
definition O
to O
work O
well O
compared O
to O
alternative O
definitions O
( O
e.g O
. O

section 11
id pdf2json/2021.acl-long.378.pdf.json
based O
on O
individual O
neurons O
) O
. O

section 11
id pdf2json/2021.acl-long.378.pdf.json
ditional O
parameters O
per O
task O
. O

section 11
id pdf2json/2021.acl-long.378.pdf.json
diff O
pruning O
without O
structured O
sparsity O
also O
performs O
well O
, O
though O
slightly O
worse O
than O
the O
structured O
approach O
. O

section 11
id pdf2json/2021.acl-long.378.pdf.json
nonadaptive O
diff O
pruning O
, O
which O
magnitude O
prunes O
the O
diff O
vector O
without O
learning O
the O
binary O
mask O
zτ O
, O
performs O
significantly O
worse O
, O
indicating O
the O
importance O
of O
learning O
the O
masking O
vector O
. O

section 11
id pdf2json/2021.acl-long.378.pdf.json
compared O
to O
adapters O
, O
diff O
pruning O
obtains O
similar O
performance O
while O
requiring O
many O
fewer O
parameters O
per O
task O
, O
making O
it O
a O
potential O
alternative O
for O
parameterefficient O
transfer O
learning.14 O

section 12
id pdf2json/2021.acl-long.378.pdf.json
to O
demonstrate O
the O
effectiveness O
of O
our O
approach O
beyond O
the O
glue O
tasks O
, O
we O
additionally O
experiment O
on O
squad O
( O
rajpurkar O
et O
al. O
, O
2016 O
) O
, O
an O
extractive O
question O
answering O
dataset O
where O
the O
model O
has O
to O
select O
the O
answer O
span O
to O
a O
question O
given O
a O
wikipedia O
paragraph O
. O

section 12
id pdf2json/2021.acl-long.378.pdf.json
to O
make O
direct O
comparisons O
with O
houlsby O
et O
al O
. O

section 12
id pdf2json/2021.acl-long.378.pdf.json
( O
2019 O
) O
, O
we O
run O
all O
experiments O
on O
squad O
v1.1 O
. O

section 12
id pdf2json/2021.acl-long.378.pdf.json
for O
diff O
pruning O
, O
we O
use O
the O
same O
general O
hyperparameters O
as O
our O
full O
finetuning O
baseline O
( O
see O
section O
a.1 O
) O
. O

section 12
id pdf2json/2021.acl-long.378.pdf.json
as O
shown O
in O
figure O
1 O
( O
right O
) O
, O
diff O
pruning O
is O
able O
achieve O
comparable O
or O
better O
performance O
with O
only O
1.0 O
% O
additional O
parameters O
. O

section 12
id pdf2json/2021.acl-long.378.pdf.json
interestingly O
, O
diff O
pruning O
measurably O
improves O
the O
upon O
the O
full O
finetuning O
baseline O
while O
modifying O
fewer O
parameters O
, O
which O
indicates O
that O
diff O
pruning O
can O
have O
a O
useful O
regularization O
effect O
on O
top O
of O
parameter-efficiency O
. O

section 14
id pdf2json/2021.acl-long.378.pdf.json
in O
figure O
1 O
( O
left O
) O
, O
we O
plot O
results O
on O
the O
glue O
validation O
set O
averaged O
across O
all O
tasks O
at O
target O
sparsity O
14comparing O
storage O
costs O
is O
a O
bit O
more O
challenging O
as O
it O
is O
implementation-specific O
. O

section 14
id pdf2json/2021.acl-long.378.pdf.json
diff O
pruning O
incurs O
additional O
storage O
cost O
due O
to O
storing O
the O
nonzero O
positions O
of O
the O
diff O
vector O
. O

section 14
id pdf2json/2021.acl-long.378.pdf.json
see O
section O
6.6 O
for O
storage O
comparison O
against O
adapters O
assuming O
float32 O
for O
weights O
and O
int32 O
for O
positions O
. O

section 14
id pdf2json/2021.acl-long.378.pdf.json
rates O
of O
0.1 O
% O
, O
0.25 O
% O
, O
0.5 O
% O
, O
1.0 O
% O
for O
the O
different O
baselines O
. O

section 14
id pdf2json/2021.acl-long.378.pdf.json
structured O
diff O
pruning O
consistently O
outperforms O
non-structured O
and O
and O
non-adaptive O
variants O
across O
different O
sparsity O
rates O
. O

section 14
id pdf2json/2021.acl-long.378.pdf.json
the O
advantage O
of O
adaptive O
methods O
becomes O
more O
pronounced O
at O
extreme O
sparsity O
rates O
. O

section 14
id pdf2json/2021.acl-long.378.pdf.json
in O
table O
2 O
, O
we O
report O
the O
breakdown O
of O
accuracy O
of O
structured O
diff O
pruning O
across O
different O
tasks O
and O
sparsity O
rates O
, O
where O
we O
observe O
that O
different O
tasks O
have O
different O
sensitivity O
to O
target O
sparsity O
rates O
. O

section 14
id pdf2json/2021.acl-long.378.pdf.json
this O
suggests O
that O
we O
can O
obtain O
even O
greater O
parameter-efficiency O
through O
targeting O
task-specific O
sparsity O
rates O
in O
the O
diff O
vector O
. O

section 15
id pdf2json/2021.acl-long.378.pdf.json
structured O
diff O
pruning O
introduces O
an O
additional O
mask O
per O
group O
, O
which O
encourages O
pruning O
of O
entire O
groups O
. O

section 15
id pdf2json/2021.acl-long.378.pdf.json
this O
is O
less O
restrictive O
than O
traditional O
group O
sparsity O
techniques O
that O
have O
been O
used O
with O
l0-norm O
relaxations O
, O
which O
force O
all O
parameters O
in O
a O
group O
to O
share O
the O
same O
mask O
( O
louizos O
et O
al. O
, O
2018 O
; O
wang O
et O
al. O
, O
2019b O
) O
. O

section 15
id pdf2json/2021.acl-long.378.pdf.json
however O
we O
still O
expect O
entire O
groups O
to O
be O
pruned O
out O
more O
often O
, O
which O
might O
bias O
the O
learning O
process O
towards O
either O
eliminating O
completely O
or O
clustering O
together O
nonzero O
diffs O
. O

section 15
id pdf2json/2021.acl-long.378.pdf.json
in O
table O
3 O
, O
we O
indeed O
find O
that O
structured O
diff O
pruning O
leads O
to O
finetuned O
models O
that O
are O
much O
more O
likely O
to O
leave O
entire O
groups O
unchanged O
from O
their O
pretrained O
values O
( O
zero O
diffs O
) O
. O

section 16
id pdf2json/2021.acl-long.378.pdf.json
different O
layers O
of O
pretrained O
models O
have O
been O
argued O
to O
encode O
different O
information O
( O
liu O
et O
al. O
, O
2019a O
; O
tenney O
et O
al. O
, O
2019 O
) O
. O

section 16
id pdf2json/2021.acl-long.378.pdf.json
given O
that O
each O
task O
will O
likely O
recruit O
different O
kinds O
of O
language O
phenomena O
embedded O
in O
the O
hidden O
layers O
, O
we O
hypothesize O
that O
diff O
pruning O
will O
modify O
different O
parts O
of O
the O
pretrained O
model O
through O
task-specific O
finetuning O
. O

section 16
id pdf2json/2021.acl-long.378.pdf.json
figure O
2 O
shows O
the O
percentage O
of O
nonzero O
diff O
parameters O
attributable O
to O
the O
different O
layers O
for O
each O
task O
. O

section 16
id pdf2json/2021.acl-long.378.pdf.json
we O
find O
that O
different O
tasks O
indeed O
modify O
different O
parts O
of O
the O
network O
, O
although O
there O
are O
some O
qualitative O
similarities O
between O
some O
tasks O
, O
for O
example O
between O
qnli O
& O
qqp O
( O
both O
must O
encode O
questions O
) O
, O
and O
mrpc O
& O
sts-b O
( O
both O
must O
predict O
similarity O
between O
sentences O
) O
. O

section 16
id pdf2json/2021.acl-long.378.pdf.json
the O
embedding O
layer O
is O
very O
sparsely O
modified O
for O
all O
tasks O
. O

section 16
id pdf2json/2021.acl-long.378.pdf.json
while O
some O
of O
the O
variations O
in O
the O
sparsity O
distributions O
is O
due O
to O
simple O
randomness O
, O
we O
do O
observe O
some O
level O
of O
consistency O
over O
multiple O
runs O
of O
the O
same O
task O
, O
as O
shown O
in O
section O
a.2 O
of O
the O
appendix O
. O

section 16
id pdf2json/2021.acl-long.378.pdf.json
the O
ability O
to O
modify O
different O
parts O
of O
the O
pretrained O
model O
for O
each O
task O
could O
explain O
the O
improved O
parameter-efficiency O
of O
our O
approach O
compared O
to O
houlsby O
et O
al O
. O

section 16
id pdf2json/2021.acl-long.378.pdf.json
( O
2019 O
) O
’ O
s O
adapters O
, O
which O
can O
only O
read/write O
to O
the O
pretrained O
model O
at O
certain O
points O
of O
the O
computational O
graph.15 O
this O
po- O
15to O
simulate O
this O
restricted O
setting O
, O
we O
tried O
applying O
diff O
pruning O
only O
on O
the O
fully-connected O
layers O
after O
the O
selfattention O
layers O
, O
and O
observed O
much O
worse O
performance O
. O

section 16
id pdf2json/2021.acl-long.378.pdf.json
tentially O
suggests O
that O
adapters O
with O
more O
finegrained O
access O
into O
model O
internals O
( O
e.g O
. O

section 16
id pdf2json/2021.acl-long.378.pdf.json
adapters O
for O
key/value/query O
transformations O
) O
might O
result O
in O
even O
greater O
parameter-efficiency O
. O

section 16
id pdf2json/2021.acl-long.378.pdf.json
while O
left O
as O
future O
work O
, O
we O
also O
note O
that O
diff O
pruning O
can O
be O
applied O
in O
conjunction O
with O
adapters O
, O
which O
might O
further O
improve O
results O
. O

section 16
id pdf2json/2021.acl-long.378.pdf.json
6.4 O
effect O
of O
l0-ball O
projection O
applying O
magnitude O
pruning O
to O
project O
onto O
the O
l0ball O
was O
crucial O
in O
achieving O
exact O
sparsity O
targets O
. O

section 16
id pdf2json/2021.acl-long.378.pdf.json
as O
shown O
in O
table O
4 O
, O
we O
observed O
little O
loss O
in O
performance O
through O
this O
approach O
. O

section 16
id pdf2json/2021.acl-long.378.pdf.json
we O
reiterate O
that O
it O
was O
crucial O
to O
finetune O
with O
a O
fixed O
mask O
, O
even O
for O
the O
approach O
which O
does O
not O
apply O
magnitude O
pruning.16 O

section 17
id pdf2json/2021.acl-long.378.pdf.json
direct O
bert O
compression O
methods O
also O
provide O
a O
straightforward O
approach O
to O
parameter-efficient O
transfer O
learning O
. O

section 17
id pdf2json/2021.acl-long.378.pdf.json
here O
we O
compare O
diff O
pruning O
against O
existing O
bert O
compression O
methods O
, O
in O
particular O
distilbert O
( O
sanh O
et O
al. O
, O
2019 O
) O
, O
mobilebert O
( O
sun O
et O
al. O
, O
2020b O
) O
and O
tinybert O
( O
jiao O
et O
al. O
, O
2020 O
) O
. O

section 17
id pdf2json/2021.acl-long.378.pdf.json
in O
these O
experiments O
we O
apply O
diff O
pruning O
on O
the O
smaller O
bertbase O
model O
as O
these O
works O
typically O
utilize O
bertbase O
as O
the O
baseline O
. O

section 17
id pdf2json/2021.acl-long.378.pdf.json
as O
shown O
in O
table O
5 O
, O
we O
observe O
that O
diff O
pruning O
is O
more O
parameter-efficient O
when O
considering O
all O
glue O
tasks O
while O
maintaining O
better O
performance O
. O

section 17
id pdf2json/2021.acl-long.378.pdf.json
of O
course O
, O
bert O
compression O
methods O
typically O
have O
faster O
inference O
time O
( O
e.g O
. O

section 17
id pdf2json/2021.acl-long.378.pdf.json
tinybert4 O
is O
9.4× O
faster O
that O
bertbase O
) O
. O

section 17
id pdf2json/2021.acl-long.378.pdf.json
however O
we O
note O
that O
diff O
16without O
fixed-mask O
finetuning O
, O
glue O
performance O
decreases O
from O
84.9 O
to O
81.4. O
pruning O
can O
be O
applied O
on O
these O
methods O
, O
which O
may O
further O
improve O
parameter-efficiency O
while O
maintaining O
fast O
inference O
. O

section 18
id pdf2json/2021.acl-long.378.pdf.json
finally O
, O
table O
6 O
shows O
the O
actual O
memory O
requirements O
for O
diff O
pruning O
compared O
to O
adapters O
for O
a O
python O
implementation O
. O

section 18
id pdf2json/2021.acl-long.378.pdf.json
while O
diff O
pruning O
requires O
storing O
positions O
in O
addition O
to O
the O
weights O
( O
unlike O
adapters O
which O
can O
just O
store O
the O
weights O
) O
, O
diff O
pruning O
is O
still O
more O
storage-efficient O
due O
to O
the O
greater O
parameter-efficiency O
. O

section 19
id pdf2json/2021.acl-long.378.pdf.json
for O
training O
, O
our O
approach O
requires O
more O
memory O
than O
usual O
finetuning O
due O
to O
additionally O
optimizing O
ατ O
and O
wτ O
. O

section 19
id pdf2json/2021.acl-long.378.pdf.json
since O
the O
majority O
of O
gpu O
memory O
is O
typically O
utilized O
by O
a O
minibatch O
’ O
s O
intermediate O
layers O
, O
this O
did O
not O
present O
a O
significant O
challenge O
for O
pretrained O
models O
that O
we O
experimented O
with O
in O
this O
study O
. O

section 19
id pdf2json/2021.acl-long.378.pdf.json
however O
, O
this O
could O
present O
an O
issue O
as O
model O
sizes O
get O
larger O
and O
larger O
. O

section 19
id pdf2json/2021.acl-long.378.pdf.json
after O
training O
, O
storing O
the O
task-specific O
diff O
vector O
requires O
storing O
a O
compressed O
version O
with O
both O
the O
nonzero O
positions O
and O
weights O
, O
which O
incurs O
additional O
storage O
requirements O
. O

section 19
id pdf2json/2021.acl-long.378.pdf.json
finally O
, O
while O
training O
efficiency O
was O
not O
a O
primary O
concern O
of O
this O
work O
, O
diff O
pruning O
was O
also O
approximately O
1.5× O
to O
2× O
slower O
to O
train O
per O
minibatch O
than O
regular O
finetuning O
. O

section 20
id pdf2json/2021.acl-long.378.pdf.json
multi-task O
learning O
multi-task O
learning O
( O
caruana O
, O
1997 O
) O
, O
broadly O
construed O
, O
aims O
to O
learn O
models O
and O
representations O
that O
can O
be O
utilized O
across O
a O
diverse O
range O
of O
tasks O
, O
and O
offers O
a O
natural O
approach O
to O
training O
parameter-efficient O
deep O
models O
. O

section 20
id pdf2json/2021.acl-long.378.pdf.json
several O
works O
have O
shown O
that O
a O
single O
bert O
model O
can O
obtain O
good O
performance O
across O
multiple O
tasks O
when O
jointly O
trained O
( O
liu O
et O
al. O
, O
2019b O
; O
clark O
et O
al. O
, O
2019 O
; O
stickland O
& O
murray O
, O
2019 O
) O
. O

section 20
id pdf2json/2021.acl-long.378.pdf.json
an O
alternative O
approach O
to O
multi-task O
learning O
that O
does O
not O
require O
access O
to O
all O
tasks O
during O
training O
involve O
training O
smaller O
task-specific O
layers O
that O
interact O
with O
a O
fixed O
pretrained O
model O
( O
rebuffi O
et O
al. O
, O
2018 O
; O
zhang O
et O
al. O
, O
2020a O
) O
. O

section 20
id pdf2json/2021.acl-long.378.pdf.json
in O
particular O
, O
adapters O
( O
rebuffi O
et O
al. O
, O
2018 O
) O
, O
which O
learn O
to O
read O
and O
write O
to O
layers O
of O
a O
shared O
model O
, O
have O
been O
applied O
to O
obtain O
parameter-efficient O
bert O
models O
( O
houlsby O
et O
al. O
, O
2019 O
; O
pfeiffer O
et O
al. O
, O
2020a O
, O
b O
, O
c O
) O
. O

section 20
id pdf2json/2021.acl-long.378.pdf.json
in O
recent O
work O
, O
li O
& O
liang O
( O
2021 O
) O
and O
qin O
& O
eisner O
( O
2021 O
) O
explore O
the O
use O
of O
learned O
prompts O
on O
top O
of O
pretrained O
models O
to O
obtain O
task-specific O
models O
. O

section 20
id pdf2json/2021.acl-long.378.pdf.json
yet O
another O
line O
of O
work O
targets O
extreme O
parameterefficiency O
through O
task-agnostic O
sentence O
representations O
that O
can O
be O
used O
without O
finetuning O
for O
downstream O
tasks O
( O
le O
& O
mikolov O
, O
2014 O
; O
kiros O
et O
al. O
, O
2015 O
; O
wieting O
et O
al. O
, O
2016 O
; O
hill O
et O
al. O
, O
2016 O
; O
arora O
et O
al. O
, O
2017 O
; O
conneau O
et O
al. O
, O
2017 O
; O
cer O
et O
al. O
, O
2018 O
; O
zhang O
et O
al. O
, O
2018 O
; O
subramanian O
et O
al. O
, O
2018 O
; O
reimers O
& O
gurevych O
, O
2019 O
; O
zhang O
et O
al. O
, O
2020b O
) O
. O

section 20
id pdf2json/2021.acl-long.378.pdf.json
these O
feature-based O
transfer O
learning O
methods O
are O
however O
generally O
outperformed O
by O
fully O
finetuned O
models O
( O
howard O
& O
ruder O
, O
2018 O
) O
. O

section 20
id pdf2json/2021.acl-long.378.pdf.json
model O
compression O
there O
has O
been O
much O
recent O
work O
on O
compressing O
pretrained O
trained O
with O
selfsupervision O
( O
see O
( O
ganesh O
et O
al. O
, O
2020 O
) O
for O
a O
recent O
survey O
) O
. O

section 20
id pdf2json/2021.acl-long.378.pdf.json
a O
particularly O
promising O
line O
of O
work O
focuses O
on O
obtaining O
smaller O
pretrained O
models O
( O
for O
subsequent O
finetuning O
) O
through O
weight O
pruning O
( O
gordon O
et O
al. O
, O
2020 O
; O
sajjad O
et O
al. O
, O
2020 O
; O
chen O
et O
al. O
, O
2020 O
) O
and/or O
knowledge O
distillation O
( O
sanh O
et O
al. O
, O
2019 O
; O
sun O
et O
al. O
, O
2019 O
; O
turc O
et O
al. O
, O
2019 O
; O
jiao O
et O
al. O
, O
2020 O
; O
sun O
et O
al. O
, O
2020b O
) O
. O

section 20
id pdf2json/2021.acl-long.378.pdf.json
it O
would O
be O
interesting O
to O
see O
whether O
our O
approach O
can O
be O
applied O
on O
top O
of O
these O
smaller O
pretrained O
models O
to O
for O
even O
greater O
parameter-efficiency O
. O

section 20
id pdf2json/2021.acl-long.378.pdf.json
learning O
to O
mask O
our O
work O
is O
closely O
related O
to O
the O
line O
of O
work O
on O
learning O
to O
mask O
parts O
of O
deep O
networks O
with O
differentiable O
relaxations O
of O
binary O
masks O
for O
model O
pruning O
and O
parameter O
sharing O
( O
wang O
et O
al. O
, O
2019b O
; O
zhao O
et O
al. O
, O
2020 O
; O
sanh O
et O
al. O
, O
2020 O
; O
radiya-dixit O
& O
wang O
, O
2020 O
; O
mallya O
et O
al. O
, O
2018 O
; O
guo O
et O
al. O
, O
2019 O
; O
sun O
et O
al. O
, O
2020a O
; O
cao O
et O
al. O
, O
2021 O
) O
. O

section 20
id pdf2json/2021.acl-long.378.pdf.json
while O
these O
works O
also O
enable O
parameterefficient O
transfer O
learning O
, O
they O
generally O
apply O
the O
masks O
directly O
on O
the O
pretrained O
parameters O
instead O
of O
on O
the O
difference O
vector O
as O
in O
the O
present O
work O
. O

section 20
id pdf2json/2021.acl-long.378.pdf.json
regularization O
towards O
pretrained O
models O
finally O
, O
diff O
pruning O
is O
also O
related O
to O
works O
which O
regularize O
the O
learning O
process O
towards O
pre- O
trained/shared O
models O
for O
continual O
learning O
( O
rusu O
et O
al. O
, O
2016 O
; O
kirkpatrick O
et O
al. O
, O
2017 O
; O
schwarz O
et O
al. O
, O
2018 O
) O
, O
domain O
adaptation O
( O
wiese O
et O
al. O
, O
2017 O
; O
miceli O
barone O
et O
al. O
, O
2017 O
) O
, O
and O
stable O
finetuning O
( O
lee O
et O
al. O
, O
2020 O
) O
. O

section 20
id pdf2json/2021.acl-long.378.pdf.json
these O
works O
typically O
do O
not O
utilize O
sparse O
regularizers O
and O
target O
a O
different O
goal O
than O
parameter-efficiency O
. O

section 21
id pdf2json/2021.acl-long.378.pdf.json
we O
propose O
diff O
pruning O
as O
a O
simple O
approach O
for O
parameter-efficient O
transfer O
learning O
with O
pretrained O
models O
. O

section 21
id pdf2json/2021.acl-long.378.pdf.json
experiments O
on O
standard O
nlp O
benchmarks O
and O
models O
show O
that O
diff O
pruning O
can O
match O
the O
performance O
of O
fully O
finetuned O
baselines O
while O
requiring O
only O
a O
few O
additional O
parameters O
per O
task O
, O
and O
can O
sometimes O
have O
a O
regularization O
effect O
and O
improve O
upon O
regular O
finetuning O
. O

section 21
id pdf2json/2021.acl-long.378.pdf.json
we O
also O
propose O
a O
structured O
variant O
of O
diff O
pruning O
which O
provides O
further O
improvements O
. O

section 21
id pdf2json/2021.acl-long.378.pdf.json
avenues O
for O
future O
work O
include O
( O
i O
) O
injecting O
parameter-efficiency O
objectives O
directly O
into O
the O
pretraining O
process O
( O
to O
pretrain O
models O
that O
are O
better O
suited O
towards O
sparse O
transfer O
learning O
) O
, O
and O
( O
ii O
) O
combining O
diff O
pruning O
with O
other O
techniques O
( O
e.g O
. O

section 21
id pdf2json/2021.acl-long.378.pdf.json
adapters O
, O
model O
compression O
) O
to O
achieve O
even O
greater O
parameter-efficiency O
. O

section 22
id pdf2json/2021.acl-long.378.pdf.json
the O
authors O
would O
like O
to O
thank O
the O
anonymous O
reviewers O
for O
their O
valuable O
feedback O
on O
the O
initial O
draft O
. O

section 22
id pdf2json/2021.acl-long.378.pdf.json
amr O
was O
supported O
by O
nsf O
1704834 O
and O
nsf O
career O
2037519 O
. O

section 23
id pdf2json/2021.acl-long.378.pdf.json
a.1 O
hyperparameters O
table O
7 O
shows O
hyperparameters O
we O
used O
for O
training O
glue O
tasks O
. O

section 23
id pdf2json/2021.acl-long.378.pdf.json
for O
squad O
v1.1 O
experiments O
, O
we O
ran O
distributed O
training O
across O
8 O
gpus O
, O
and O
used O
per O
gpu O
batch O
size O
3 O
, O
maximum O
sequence O
length O
384 O
, O
document O
stride O
128 O
, O
learning O
rate O
3× O
10−5 O
, O
number O
of O
initial O
training O
epochs O
2 O
and O
number O
of O
finetuning O
epochs O
2 O
. O

section 23
id pdf2json/2021.acl-long.378.pdf.json
a.2 O
consistency O
of O
nonzero O
parameters O
figure O
3 O
shows O
the O
percentage O
of O
modified O
parameters O
attributable O
to O
each O
layer O
across O
5 O
runs O
of O
sst-2 O
. O

section 23
id pdf2json/2021.acl-long.378.pdf.json
we O
find O
that O
there O
is O
nonotrivial O
variation O
in O
sparsity O
across O
runs O
, O
but O
also O
a O
degree O
of O
consistency O
. O

section 23
id pdf2json/2021.acl-long.378.pdf.json
for O
example O
, O
the O
first O
layer O
is O
modified O
considerably O
more O
than O
other O
layers O
across O
all O
runs O
. O

section TITLE
id pdf2json/2021.acl-long.285.pdf.json
mpc-bert O
: O
a O
pre-trained O
language O
model O
for O
multi-party O
conversation O
understanding O

section ABSTRACT
id pdf2json/2021.acl-long.285.pdf.json
recently O
, O
various O
neural O
models O
for O
multiparty O
conversation O
( O
mpc O
) O
have O
achieved O
impressive O
improvements O
on O
a O
variety O
of O
tasks O
such O
as O
addressee O
recognition O
, O
speaker O
identification O
and O
response O
prediction O
. O

section ABSTRACT
id pdf2json/2021.acl-long.285.pdf.json
however O
, O
these O
existing O
methods O
on O
mpc O
usually O
represent O
interlocutors O
and O
utterances O
individually O
and O
ignore O
the O
inherent O
complicated O
structure O
in O
mpc O
which O
may O
provide O
crucial O
interlocutor O
and O
utterance O
semantics O
and O
would O
enhance O
the O
conversation O
understanding O
process O
. O

section ABSTRACT
id pdf2json/2021.acl-long.285.pdf.json
to O
this O
end O
, O
we O
present O
mpc-bert O
, O
a O
pre-trained O
model O
for O
mpc O
understanding O
that O
considers O
learning O
who O
says O
what O
to O
whom O
in O
a O
unified O
model O
with O
several O
elaborated O
self-supervised O
tasks O
. O

section ABSTRACT
id pdf2json/2021.acl-long.285.pdf.json
particularly O
, O
these O
tasks O
can O
be O
generally O
categorized O
into O
( O
1 O
) O
interlocutor O
structure O
modeling O
including O
reply-to O
utterance O
recognition O
, O
identical O
speaker O
searching O
and O
pointer O
consistency O
distinction O
, O
and O
( O
2 O
) O
utterance O
semantics O
modeling O
including O
masked O
shared O
utterance O
restoration O
and O
shared O
node O
detection O
. O

section ABSTRACT
id pdf2json/2021.acl-long.285.pdf.json
we O
evaluate O
mpcbert O
on O
three O
downstream O
tasks O
including O
addressee O
recognition O
, O
speaker O
identification O
and O
response O
selection O
. O

section ABSTRACT
id pdf2json/2021.acl-long.285.pdf.json
experimental O
results O
show O
that O
mpc-bert O
outperforms O
previous O
methods O
by O
large O
margins O
and O
achieves O
new O
state-of-the-art O
performance O
on O
all O
three O
downstream O
tasks O
at O
two O
benchmarks O
. O

section 0
id pdf2json/2021.acl-long.285.pdf.json
proceedings O
of O
the O
59th O
annual O
meeting O
of O
the O
association O
for O
computational O
linguistics O
and O
the O
11th O
international O
joint O
conference O
on O
natural O
language O
processing O
, O
pages O
3682–3692 O
august O
1–6 O
, O
2021 O
. O

section 0
id pdf2json/2021.acl-long.285.pdf.json
©2021 O
association O
for O
computational O
linguistics O
3682 O

section 1
id pdf2json/2021.acl-long.285.pdf.json
building O
a O
conversational O
agent O
with O
intelligence O
has O
drawn O
significant O
attention O
from O
both O
academia O
and O
industry O
. O

section 1
id pdf2json/2021.acl-long.285.pdf.json
most O
of O
existing O
methods O
have O
studied O
understanding O
conversations O
between O
two O
participants O
, O
aiming O
to O
return O
an O
appropriate O
response O
either O
in O
a O
generation-based O
( O
shang O
et O
al. O
, O
∗work O
done O
during O
the O
internship O
at O
microsoft O
. O

section 1
id pdf2json/2021.acl-long.285.pdf.json
†corresponding O
author O
. O

section 1
id pdf2json/2021.acl-long.285.pdf.json
2015 O
; O
serban O
et O
al. O
, O
2016 O
, O
2017 O
; O
zhang O
et O
al. O
, O
2018b O
, O
2020 O
) O
or O
retrieval-based O
manner O
( O
lowe O
et O
al. O
, O
2015 O
; O
wu O
et O
al. O
, O
2017 O
; O
zhou O
et O
al. O
, O
2018 O
; O
tao O
et O
al. O
, O
2019a O
, O
b O
; O
gu O
et O
al. O
, O
2019a O
, O
b O
, O
2020 O
) O
. O

section 1
id pdf2json/2021.acl-long.285.pdf.json
recently O
, O
researchers O
have O
paid O
more O
attention O
to O
a O
more O
practical O
and O
challenging O
scenario O
involving O
more O
than O
two O
participants O
, O
which O
is O
well O
known O
as O
multiparty O
conversation O
( O
mpc O
) O
( O
ouchi O
and O
tsuboi O
, O
2016 O
; O
zhang O
et O
al. O
, O
2018a O
; O
le O
et O
al. O
, O
2019 O
; O
hu O
et O
al. O
, O
2019 O
) O
. O

section 1
id pdf2json/2021.acl-long.285.pdf.json
table O
1 O
shows O
an O
mpc O
example O
in O
the O
ubuntu O
internet O
relay O
chat O
( O
irc O
) O
channel O
, O
which O
is O
composed O
of O
a O
sequence O
of O
( O
speaker O
, O
utterance O
, O
addressee O
) O
triples O
. O

section 1
id pdf2json/2021.acl-long.285.pdf.json
in O
addition O
to O
returning O
an O
appropriate O
response O
, O
predicting O
who O
will O
be O
the O
next O
speaker O
( O
meng O
et O
al. O
, O
2018 O
) O
and O
who O
is O
the O
addressee O
of O
an O
utterance O
( O
ouchi O
and O
tsuboi O
, O
2016 O
; O
zhang O
et O
al. O
, O
2018a O
; O
le O
et O
al. O
, O
2019 O
) O
are O
unique O
and O
important O
issues O
in O
mpc O
. O

section 1
id pdf2json/2021.acl-long.285.pdf.json
an O
instance O
of O
mpc O
always O
contains O
complicated O
interactions O
between O
interlocutors O
, O
between O
utterances O
and O
between O
an O
interlocutor O
and O
an O
utterance O
. O

section 1
id pdf2json/2021.acl-long.285.pdf.json
therefore O
, O
it O
is O
challenging O
to O
model O
the O
conversation O
flow O
and O
fully O
understand O
the O
dialogue O
content O
. O

section 1
id pdf2json/2021.acl-long.285.pdf.json
existing O
studies O
on O
mpc O
learn O
the O
representations O
of O
interlocutors O
and O
utterances O
with O
neural O
networks O
, O
and O
their O
representation O
spaces O
are O
either O
separate O
( O
ouchi O
and O
tsuboi O
, O
2016 O
) O
or O
interactive O
( O
zhang O
et O
al. O
, O
2018a O
) O
. O

section 1
id pdf2json/2021.acl-long.285.pdf.json
however O
, O
the O
semantics O
contained O
in O
the O
interlocutor O
and O
utterance O
representations O
may O
not O
be O
effectively O
captured O
as O
they O
are O
from O
two O
different O
representation O
spaces O
. O

section 1
id pdf2json/2021.acl-long.285.pdf.json
recently O
, O
to O
take O
advantage O
of O
the O
breakthrough O
in O
pre-training O
language O
models O
( O
plms O
) O
for O
natural O
language O
understanding O
, O
some O
studies O
proposed O
to O
integrate O
the O
speaker O
( O
gu O
et O
al. O
, O
2020 O
) O
or O
topic O
( O
wang O
et O
al. O
, O
2020 O
) O
information O
into O
plms O
. O

section 1
id pdf2json/2021.acl-long.285.pdf.json
despite O
of O
the O
performance O
improvement O
on O
response O
selection O
, O
these O
models O
still O
overlook O
the O
inherent O
relationships O
between O
utterances O
and O
interlocutors O
, O
such O
as O
“ O
address-to O
” O
. O

section 1
id pdf2json/2021.acl-long.285.pdf.json
furthermore O
, O
most O
existing O
studies O
design O
models O
for O
each O
individual O
task O
in O
mpc O
( O
e.g. O
, O
addressee O
recognition O
, O
speaker O
identification O
and O
response O
prediction O
) O
separately O
. O

section 1
id pdf2json/2021.acl-long.285.pdf.json
intuitively O
, O
these O
tasks O
are O
complementary O
among O
each O
other O
. O

section 1
id pdf2json/2021.acl-long.285.pdf.json
making O
use O
of O
these O
tasks O
simultaneously O
may O
produce O
better O
contextualized O
representations O
of O
interlocutors O
and O
utterances O
, O
and O
would O
enhance O
the O
conversation O
understanding O
, O
but O
is O
neglected O
in O
previous O
studies O
. O

section 1
id pdf2json/2021.acl-long.285.pdf.json
on O
account O
of O
above O
issues O
, O
we O
propose O
mpcbert O
which O
jointly O
learns O
who O
says O
what O
to O
whom O
in O
mpc O
by O
designing O
self-supervised O
tasks O
for O
plms O
, O
so O
as O
to O
improve O
the O
ability O
of O
plms O
on O
mpc O
understanding O
. O

section 1
id pdf2json/2021.acl-long.285.pdf.json
specifically O
, O
the O
five O
designed O
tasks O
includes O
reply-to O
utterance O
recognition O
, O
identical O
speaker O
searching O
, O
pointer O
consistency O
distinction O
, O
masked O
shared O
utterance O
restoration O
and O
shared O
node O
detection O
. O

section 1
id pdf2json/2021.acl-long.285.pdf.json
the O
first O
three O
tasks O
are O
designed O
to O
model O
the O
interlocutor O
structure O
in O
mpc O
in O
a O
semantics-to-structure O
manner O
. O

section 1
id pdf2json/2021.acl-long.285.pdf.json
in O
the O
output O
of O
mpc-bert O
, O
an O
interlocutor O
is O
described O
through O
the O
encoded O
representations O
of O
the O
utterances O
it O
says O
. O

section 1
id pdf2json/2021.acl-long.285.pdf.json
thus O
, O
the O
representations O
of O
utterance O
semantics O
are O
utilized O
to O
construct O
the O
conversation O
structure O
in O
these O
three O
tasks O
. O

section 1
id pdf2json/2021.acl-long.285.pdf.json
on O
the O
other O
hand O
, O
the O
last O
two O
tasks O
are O
designed O
to O
model O
the O
utterance O
semantics O
in O
a O
structure-to-semantics O
manner O
. O

section 1
id pdf2json/2021.acl-long.285.pdf.json
intuitively O
, O
the O
conversation O
structure O
influences O
the O
information O
flow O
in O
mpc O
. O

section 1
id pdf2json/2021.acl-long.285.pdf.json
thus O
, O
the O
structure O
information O
can O
also O
be O
used O
to O
strengthen O
the O
representations O
of O
utterance O
semantics O
in O
return O
. O

section 1
id pdf2json/2021.acl-long.285.pdf.json
in O
general O
, O
these O
five O
self-supervised O
tasks O
are O
employed O
to O
jointly O
train O
the O
mpc-bert O
in O
a O
multi-task O
learning O
framework O
, O
which O
helps O
the O
model O
to O
learn O
the O
complementary O
information O
among O
interlocutors O
and O
utterances O
, O
and O
that O
between O
structure O
and O
semantics O
. O

section 1
id pdf2json/2021.acl-long.285.pdf.json
by O
this O
means O
, O
mpc-bert O
can O
produce O
better O
interlocutor O
and O
utterance O
representations O
which O
can O
be O
effectively O
generalized O
to O
multiple O
downstream O
tasks O
of O
mpc O
. O

section 1
id pdf2json/2021.acl-long.285.pdf.json
to O
measure O
the O
effectiveness O
of O
these O
selfsupervised O
tasks O
and O
to O
test O
the O
generalization O
ability O
of O
mpc-bert O
, O
we O
evaluate O
it O
on O
three O
downstream O
tasks O
including O
addressee O
recognition O
, O
speaker O
identification O
and O
response O
selection O
, O
which O
are O
three O
core O
research O
issues O
of O
mpc O
. O

section 1
id pdf2json/2021.acl-long.285.pdf.json
two O
benchmarks O
based O
on O
ubuntu O
irc O
channel O
are O
employed O
for O
evaluation O
. O

section 1
id pdf2json/2021.acl-long.285.pdf.json
one O
was O
released O
by O
hu O
et O
al O
. O

section 1
id pdf2json/2021.acl-long.285.pdf.json
( O
2019 O
) O
. O

section 1
id pdf2json/2021.acl-long.285.pdf.json
the O
other O
was O
released O
by O
ouchi O
and O
tsuboi O
( O
2016 O
) O
and O
has O
three O
experimental O
settings O
according O
to O
session O
lengths O
. O

section 1
id pdf2json/2021.acl-long.285.pdf.json
experimental O
results O
show O
that O
mpc-bert O
outperforms O
the O
current O
state-of-the-art O
models O
by O
margins O
of O
3.51 O
% O
, O
2.86 O
% O
, O
3.28 O
% O
and O
5.36 O
% O
on O
the O
test O
sets O
of O
these O
two O
benchmarks O
respectively O
in O
terms O
of O
the O
session O
accuracy O
of O
addressee O
recognition O
, O
by O
margins O
of O
7.66 O
% O
, O
2.60 O
% O
, O
3.38 O
% O
and O
4.24 O
% O
respectively O
in O
terms O
of O
the O
utterance O
precision O
of O
speaker O
identification O
, O
and O
by O
margins O
of O
3.82 O
% O
, O
2.71 O
% O
, O
2.55 O
% O
and O
3.22 O
% O
respectively O
in O
terms O
of O
the O
response O
recall O
of O
response O
selection O
. O

section 1
id pdf2json/2021.acl-long.285.pdf.json
in O
summary O
, O
our O
contributions O
in O
this O
paper O
are O
three-fold O
: O
( O
1 O
) O
mpc-bert O
, O
a O
plm O
for O
mpc O
understanding O
, O
is O
proposed O
by O
designing O
five O
selfsupervised O
tasks O
based O
on O
the O
interactions O
among O
utterances O
and O
interlocutors O
. O

section 1
id pdf2json/2021.acl-long.285.pdf.json
( O
2 O
) O
three O
downstream O
tasks O
are O
employed O
to O
comprehensively O
evaluate O
the O
effectiveness O
of O
our O
designed O
self-supervised O
tasks O
and O
the O
generalization O
ability O
of O
mpc-bert O
. O

section 1
id pdf2json/2021.acl-long.285.pdf.json
( O
3 O
) O
our O
proposed O
mpc-bert O
achieves O
new O
state-ofthe-art O
performance O
on O
all O
three O
downstream O
tasks O
at O
two O
benchmarks O
. O

section 2
id pdf2json/2021.acl-long.285.pdf.json
existing O
methods O
on O
building O
dialogue O
systems O
can O
be O
generally O
categorized O
into O
studying O
twoparty O
conversations O
and O
multi-party O
conversations O
( O
mpc O
) O
. O

section 2
id pdf2json/2021.acl-long.285.pdf.json
in O
this O
paper O
, O
we O
study O
mpc O
. O

section 2
id pdf2json/2021.acl-long.285.pdf.json
in O
addition O
to O
predicting O
utterances O
, O
identifying O
the O
speaker O
and O
recognizing O
the O
addressee O
of O
an O
utterance O
are O
also O
important O
tasks O
for O
mpc O
. O

section 2
id pdf2json/2021.acl-long.285.pdf.json
ouchi O
and O
tsuboi O
( O
2016 O
) O
first O
proposed O
the O
task O
of O
addressee O
and O
response O
selection O
and O
created O
an O
mpc O
corpus O
for O
studying O
this O
task O
. O

section 2
id pdf2json/2021.acl-long.285.pdf.json
zhang O
et O
al O
. O

section 2
id pdf2json/2021.acl-long.285.pdf.json
( O
2018a O
) O
proposed O
si-rnn O
, O
which O
updated O
speaker O
embeddings O
role-sensitively O
for O
addressee O
and O
response O
selection O
. O

section 2
id pdf2json/2021.acl-long.285.pdf.json
meng O
et O
al O
. O

section 2
id pdf2json/2021.acl-long.285.pdf.json
( O
2018 O
) O
proposed O
a O
task O
of O
speaker O
classification O
as O
a O
surrogate O
task O
for O
speaker O
modeling O
. O

section 2
id pdf2json/2021.acl-long.285.pdf.json
le O
et O
al O
. O

section 2
id pdf2json/2021.acl-long.285.pdf.json
( O
2019 O
) O
proposed O
a O
who-to-whom O
( O
w2w O
) O
model O
to O
recognize O
the O
addressees O
of O
all O
utterances O
. O

section 2
id pdf2json/2021.acl-long.285.pdf.json
hu O
et O
al O
. O

section 2
id pdf2json/2021.acl-long.285.pdf.json
( O
2019 O
) O
proposed O
a O
graph-structured O
network O
( O
gsn O
) O
to O
model O
the O
graphical O
information O
flow O
for O
response O
generation O
. O

section 2
id pdf2json/2021.acl-long.285.pdf.json
wang O
et O
al O
. O

section 2
id pdf2json/2021.acl-long.285.pdf.json
( O
2020 O
) O
proposed O
to O
track O
the O
dynamic O
topic O
for O
response O
selection O
. O

section 2
id pdf2json/2021.acl-long.285.pdf.json
generally O
speaking O
, O
previous O
studies O
on O
mpc O
can O
not O
unify O
the O
representations O
of O
interlocutors O
and O
utterances O
effectively O
. O

section 2
id pdf2json/2021.acl-long.285.pdf.json
also O
, O
they O
are O
limited O
to O
each O
individual O
task O
, O
ignoring O
the O
complementary O
information O
among O
different O
tasks O
. O

section 2
id pdf2json/2021.acl-long.285.pdf.json
to O
the O
best O
of O
our O
knowledge O
, O
this O
paper O
makes O
the O
first O
attempt O
to O
design O
various O
self-supervised O
tasks O
for O
building O
plms O
aiming O
at O
mpc O
understanding O
, O
and O
to O
evaluate O
the O
performance O
of O
plms O
on O
three O
downstream O
tasks O
as O
comprehensively O
as O
possible O
. O

section 3
id pdf2json/2021.acl-long.285.pdf.json
an O
mpc O
instance O
is O
composed O
of O
a O
sequence O
of O
( O
speaker O
, O
utterance O
, O
addressee O
) O
triples O
, O
denoted O
as O
{ O
( O
sn O
, O
un O
, O
an O
) O
} O
nn=1 O
, O
where O
n O
is O
the O
number O
of O
turns O
in O
the O
conversation O
. O

section 3
id pdf2json/2021.acl-long.285.pdf.json
our O
goal O
is O
to O
build O
a O
pre-trained O
language O
model O
for O
universal O
mpc O
understanding O
. O

section 3
id pdf2json/2021.acl-long.285.pdf.json
given O
a O
conversation O
, O
this O
model O
is O
expected O
to O
produce O
embedding O
vectors O
for O
all O
utterances O
which O
contain O
not O
only O
the O
semantic O
information O
of O
each O
utterance O
, O
but O
also O
the O
speaker O
and O
addressee O
structure O
of O
the O
whole O
conversation O
. O

section 3
id pdf2json/2021.acl-long.285.pdf.json
thus O
, O
it O
can O
be O
effectively O
adapted O
to O
various O
downstream O
tasks O
by O
fine-tuning O
model O
parameters O
. O

section 4
id pdf2json/2021.acl-long.285.pdf.json
in O
this O
paper O
, O
bert O
( O
devlin O
et O
al. O
, O
2019 O
) O
is O
chosen O
as O
the O
backbone O
of O
our O
plm O
for O
mpc O
. O

section 4
id pdf2json/2021.acl-long.285.pdf.json
thus O
, O
we O
name O
it O
mpc-bert O
. O

section 4
id pdf2json/2021.acl-long.285.pdf.json
it O
is O
worth O
noting O
that O
our O
proposed O
self-supervised O
tasks O
for O
training O
mpcbert O
can O
also O
be O
applied O
to O
other O
types O
of O
plms O
. O

section 4
id pdf2json/2021.acl-long.285.pdf.json
we O
first O
give O
an O
overview O
of O
the O
input O
representations O
and O
the O
overall O
architectures O
of O
mpc-bert O
. O

section 4
id pdf2json/2021.acl-long.285.pdf.json
when O
constructing O
the O
input O
representations O
, O
in O
order O
to O
consider O
the O
speaker O
information O
of O
each O
utterance O
, O
speaker O
embeddings O
( O
gu O
et O
al. O
, O
2020 O
) O
are O
introduced O
as O
shown O
in O
figure O
1 O
. O

section 4
id pdf2json/2021.acl-long.285.pdf.json
considering O
that O
the O
set O
of O
interlocutors O
are O
inconsistent O
in O
different O
conversations O
, O
a O
position-based O
interlocutor O
embedding O
table O
is O
initialized O
randomly O
at O
first O
and O
updated O
during O
pre-training O
, O
which O
means O
each O
interlocutor O
in O
a O
conversation O
is O
assigned O
with O
an O
embedding O
vector O
according O
to O
the O
order O
it O
appears O
in O
the O
conversation O
. O

section 4
id pdf2json/2021.acl-long.285.pdf.json
then O
, O
the O
speaker O
embeddings O
for O
each O
utterance O
can O
be O
derived O
by O
looking O
up O
this O
embedding O
table O
. O

section 4
id pdf2json/2021.acl-long.285.pdf.json
the O
speaker O
embeddings O
are O
combined O
with O
standard O
token O
, O
position O
and O
segmentation O
embeddings O
and O
are O
then O
encoded O
by O
bert O
. O

section 4
id pdf2json/2021.acl-long.285.pdf.json
the O
output O
embeddings O
of O
bert O
corresponding O
to O
different O
input O
tokens O
are O
utilized O
by O
different O
self-supervised O
tasks O
for O
further O
calculation O
. O

section 5
id pdf2json/2021.acl-long.285.pdf.json
the O
first O
three O
tasks O
follow O
the O
semantics-tostructure O
manner O
. O

section 5
id pdf2json/2021.acl-long.285.pdf.json
in O
mpc-bert O
, O
each O
interlocutor O
is O
described O
through O
the O
encoded O
representations O
of O
the O
utterances O
it O
says O
. O

section 5
id pdf2json/2021.acl-long.285.pdf.json
thus O
, O
the O
representations O
of O
utterance O
semantics O
are O
utilized O
to O
construct O
the O
conversation O
structure O
. O

section 5
id pdf2json/2021.acl-long.285.pdf.json
figure O
1 O
shows O
the O
input O
representations O
and O
the O
model O
architectures O
of O
these O
three O
tasks O
. O

section 5
id pdf2json/2021.acl-long.285.pdf.json
a O
[ O
cls O
] O
token O
is O
inserted O
at O
the O
start O
of O
each O
utterance O
, O
denoting O
its O
utterancelevel O
representation O
. O

section 5
id pdf2json/2021.acl-long.285.pdf.json
then O
, O
all O
utterances O
in O
a O
conversation O
are O
concatenated O
and O
a O
[ O
sep O
] O
token O
is O
inserted O
at O
the O
end O
of O
the O
whole O
sequence O
. O

section 5
id pdf2json/2021.acl-long.285.pdf.json
it O
is O
notable O
that O
these O
three O
tasks O
share O
the O
same O
form O
of O
input O
data O
. O

section 5
id pdf2json/2021.acl-long.285.pdf.json
thus O
, O
the O
input O
only O
needs O
to O
be O
encoded O
once O
by O
bert O
while O
the O
output O
can O
be O
fed O
into O
three O
tasks O
, O
which O
is O
computation-efficient O
. O

section 5
id pdf2json/2021.acl-long.285.pdf.json
as O
shown O
in O
figure O
1 O
, O
a O
task-dependent O
non-linear O
transformation O
layer O
is O
placed O
on O
top O
of O
bert O
in O
order O
to O
adapt O
the O
output O
of O
bert O
to O
different O
tasks O
. O

section 5
id pdf2json/2021.acl-long.285.pdf.json
we O
will O
describe O
the O
details O
of O
these O
tasks O
as O
follows O
. O

section 6
id pdf2json/2021.acl-long.285.pdf.json
to O
enable O
the O
model O
to O
recognize O
the O
addressee O
of O
each O
utterance O
, O
a O
self-supervised O
task O
named O
replyto O
utterance O
recognition O
( O
rur O
) O
is O
proposed O
to O
learn O
which O
preceding O
utterance O
the O
current O
utterance O
replies O
to O
. O

section 6
id pdf2json/2021.acl-long.285.pdf.json
after O
encoded O
by O
bert O
, O
we O
extract O
the O
contextualized O
representations O
for O
each O
[ O
cls O
] O
token O
representing O
individual O
utterances O
. O

section 6
id pdf2json/2021.acl-long.285.pdf.json
next O
, O
a O
non-linear O
transformation O
followed O
by O
a O
layer O
normalization O
are O
performed O
to O
derive O
the O
utterance O
representations O
for O
this O
specific O
task O
{ O
ururi O
} O
ni=1 O
, O
where O
ururi O
∈ O
rd O
and O
d O
= O
768 O
. O

section 6
id pdf2json/2021.acl-long.285.pdf.json
then O
, O
for O
a O
specific O
utterance O
ui O
, O
its O
matching O
scores O
with O
all O
its O
preceding O
utterances O
are O
calculated O
as O
mij O
= O
softmax O
( O
urur O
> O
i O
· O
arur O
· O
ururj O
) O
, O
( O
1 O
) O
where O
arur O
∈ O
rd×d O
is O
a O
linear O
transformation O
, O
mij O
denotes O
the O
matching O
degree O
of O
uj O
being O
the O
replyto O
utterance O
of O
ui O
, O
and O
1 O
≤ O
j O
< O
i O
. O

section 6
id pdf2json/2021.acl-long.285.pdf.json
we O
construct O
a O
set O
s O
by O
sampling O
a O
certain O
number O
of O
utterances O
in O
a O
conversation O
and O
this O
recognition O
operation O
is O
performed O
for O
each O
utterance O
in O
s. O
meanwhile O
, O
a O
dynamic O
sampling O
strategy O
is O
adopted O
so O
that O
models O
can O
see O
more O
samples O
. O

section 6
id pdf2json/2021.acl-long.285.pdf.json
finally O
, O
the O
pretraining O
objective O
of O
this O
self-supervised O
task O
is O
to O
minimize O
the O
cross-entropy O
loss O
as O
lrur O
= O
− O
∑ O
i∈s O
i−1∑ O
j=1 O
yij O
log O
( O
mij O
) O
, O
( O
2 O
) O
where O
yij O
= O
1 O
if O
uj O
is O
the O
reply-to O
utterance O
of O
ui O
and O
yij O
= O
0 O
otherwise O
. O

section 7
id pdf2json/2021.acl-long.285.pdf.json
having O
knowledge O
of O
who O
is O
the O
speaker O
of O
an O
utterance O
is O
also O
important O
for O
mpc O
. O

section 7
id pdf2json/2021.acl-long.285.pdf.json
the O
task O
of O
identical O
speaker O
searching O
( O
iss O
) O
is O
designed O
by O
masking O
the O
speaker O
embedding O
of O
a O
specific O
utterance O
in O
the O
input O
representation O
, O
and O
aims O
to O
predict O
its O
speaker O
given O
the O
conversation O
. O

section 7
id pdf2json/2021.acl-long.285.pdf.json
since O
the O
set O
of O
interlocutors O
vary O
across O
conversations O
, O
the O
task O
of O
predicting O
the O
speaker O
of O
an O
utterance O
is O
reformulated O
as O
searching O
for O
the O
utterances O
sharing O
the O
identical O
speaker O
. O

section 7
id pdf2json/2021.acl-long.285.pdf.json
first O
, O
for O
a O
specific O
utterance O
, O
its O
speaker O
embedding O
is O
masked O
with O
a O
special O
[ O
mask O
] O
interlocutor O
embedding O
to O
avoid O
information O
leakage O
. O

section 7
id pdf2json/2021.acl-long.285.pdf.json
given O
the O
utterance O
representations O
for O
this O
specific O
task O
{ O
uissi O
} O
ni=1 O
where O
uissi O
∈ O
rd O
, O
the O
matching O
scores O
of O
ui O
with O
all O
its O
preceding O
utterances O
are O
calculated O
similarly O
with O
eq O
. O

section 7
id pdf2json/2021.acl-long.285.pdf.json
( O
1 O
) O
. O

section 7
id pdf2json/2021.acl-long.285.pdf.json
here O
, O
mij O
denotes O
the O
matching O
degree O
of O
uj O
sharing O
the O
same O
speaker O
with O
ui O
. O

section 7
id pdf2json/2021.acl-long.285.pdf.json
for O
each O
instance O
in O
the O
dynamic O
sampling O
set O
s O
, O
there O
must O
be O
an O
utterance O
in O
previous O
turns O
sharing O
the O
same O
speaker O
. O

section 7
id pdf2json/2021.acl-long.285.pdf.json
otherwise O
, O
it O
is O
removed O
out O
of O
the O
set O
. O

section 7
id pdf2json/2021.acl-long.285.pdf.json
finally O
, O
the O
pre-training O
objective O
of O
this O
task O
is O
to O
minimize O
the O
cross-entropy O
loss O
similarly O
with O
eq O
. O

section 7
id pdf2json/2021.acl-long.285.pdf.json
( O
2 O
) O
. O

section 7
id pdf2json/2021.acl-long.285.pdf.json
here O
, O
yij O
= O
1 O
if O
uj O
shares O
the O
same O
speaker O
with O
ui O
and O
yij O
= O
0 O
otherwise O
. O

section 8
id pdf2json/2021.acl-long.285.pdf.json
we O
design O
a O
task O
named O
pointer O
consistency O
distinction O
( O
pcd O
) O
to O
jointly O
model O
speakers O
and O
addressees O
in O
mpc O
. O

section 8
id pdf2json/2021.acl-long.285.pdf.json
in O
this O
task O
, O
a O
pair O
of O
utterances O
representing O
the O
“ O
reply-to O
” O
relationship O
is O
defined O
as O
a O
speaker-to-addressee O
pointer O
. O

section 8
id pdf2json/2021.acl-long.285.pdf.json
here O
, O
we O
assume O
that O
the O
representations O
of O
two O
pointers O
directing O
from O
the O
same O
speaker O
to O
the O
same O
addressee O
should O
be O
consistent O
. O

section 8
id pdf2json/2021.acl-long.285.pdf.json
as O
illustrated O
in O
figure O
2 O
( O
a O
) O
, O
speaker O
sm O
speaks O
ui O
and O
uj O
which O
reply O
to O
ui′ O
and O
uj′ O
from O
speaker O
sn O
respectively O
. O

section 8
id pdf2json/2021.acl-long.285.pdf.json
thus O
, O
the O
utterance O
tuples O
( O
ui O
, O
ui′ O
) O
and O
( O
uj O
, O
uj′ O
) O
both O
represent O
the O
pointer O
of O
sm-to-sn O
and O
their O
pointer O
representations O
should O
be O
consistent.. O

section 8
id pdf2json/2021.acl-long.285.pdf.json
given O
the O
utterance O
representations O
for O
this O
specific O
task O
{ O
upcdi O
} O
ni=1 O
where O
u O
pcd O
i O
∈ O
rd O
, O
we O
first O
capture O
the O
pointer O
information O
contained O
in O
each O
utterance O
tuple O
. O

section 8
id pdf2json/2021.acl-long.285.pdf.json
the O
element-wise O
difference O
and O
multiplication O
between O
an O
utterance O
tuple O
( O
ui O
, O
ui′ O
) O
are O
computed O
and O
are O
concatenated O
as O
pii′ O
= O
[ O
u O
pcd O
i O
− O
u O
pcd O
i′ O
; O
u O
pcd O
i O
u O
pcd O
i′ O
] O
, O
( O
3 O
) O
where O
pii′ O
∈ O
r2d O
. O

section 8
id pdf2json/2021.acl-long.285.pdf.json
then O
, O
we O
compress O
pii′ O
and O
obtain O
the O
pointer O
representation O
p̄ii′ O
as O
p̄ii′ O
= O
relu O
( O
pii′ O
·wpcd O
+ O
bpcd O
) O
, O
( O
4 O
) O
where O
wpcd O
∈ O
r2d×d O
and O
bpcd O
∈ O
rd O
are O
parameters O
. O

section 8
id pdf2json/2021.acl-long.285.pdf.json
identically O
, O
a O
consistent O
pointer O
representations O
p̄jj′ O
and O
an O
inconsistent O
one O
p̄kk′ O
sampled O
from O
this O
conversation O
are O
obtained O
. O

section 8
id pdf2json/2021.acl-long.285.pdf.json
the O
similarities O
between O
every O
two O
pointers O
are O
calculated O
as O
mij O
= O
sigmoid O
( O
p̄ O
> O
ii′ O
· O
apcd O
· O
p̄jj′ O
) O
, O
( O
5 O
) O
where O
mij O
denotes O
the O
matching O
degree O
of O
pointer O
p̄ii′ O
being O
consistent O
with O
pointer O
p̄jj′ O
. O

section 8
id pdf2json/2021.acl-long.285.pdf.json
mik O
can O
be O
derived O
accordingly O
. O

section 8
id pdf2json/2021.acl-long.285.pdf.json
finally O
, O
the O
pre-training O
objective O
of O
this O
task O
is O
to O
minimize O
the O
hinge O
loss O
which O
enforces O
mij O
to O
be O
larger O
than O
mik O
by O
at O
least O
a O
margin O
∆ O
as O
lpcd O
= O
max O
{ O
0 O
, O
∆−mij O
+ O
mik O
} O
. O

section 8
id pdf2json/2021.acl-long.285.pdf.json
( O
6 O
) O

section 9
id pdf2json/2021.acl-long.285.pdf.json
intuitively O
, O
the O
conversation O
structure O
might O
influence O
the O
information O
flow O
, O
so O
that O
it O
can O
be O
used O
to O
strengthen O
the O
representations O
of O
utterance O
semantics O
. O

section 9
id pdf2json/2021.acl-long.285.pdf.json
thus O
, O
two O
self-supervised O
tasks O
following O
the O
structure-to-semantics O
manner O
are O
designed O
. O

section 10
id pdf2json/2021.acl-long.285.pdf.json
there O
are O
usually O
several O
utterances O
replying-to O
a O
shared O
utterance O
in O
mpc O
. O

section 10
id pdf2json/2021.acl-long.285.pdf.json
intuitively O
, O
a O
shared O
utterance O
is O
semantically O
relevant O
to O
more O
utterances O
in O
the O
context O
than O
non-shared O
ones O
. O

section 10
id pdf2json/2021.acl-long.285.pdf.json
based O
on O
this O
characteristic O
, O
we O
design O
a O
task O
named O
masked O
shared O
utterance O
restoration O
( O
msur O
) O
. O

section 10
id pdf2json/2021.acl-long.285.pdf.json
we O
first O
randomly O
sample O
an O
utterance O
from O
all O
shared O
utterances O
in O
a O
conversation O
and O
all O
tokens O
in O
this O
sampled O
utterance O
are O
masked O
with O
a O
[ O
mask O
] O
token O
. O

section 10
id pdf2json/2021.acl-long.285.pdf.json
then O
the O
model O
is O
enforced O
to O
restore O
the O
masked O
utterance O
given O
the O
rest O
conversation O
. O

section 10
id pdf2json/2021.acl-long.285.pdf.json
formally O
, O
assuming O
ui O
as O
the O
masked O
shared O
utterance O
and O
li O
as O
the O
number O
of O
tokens O
in O
ui O
. O

section 10
id pdf2json/2021.acl-long.285.pdf.json
given O
the O
token O
representations O
for O
this O
task O
{ O
umsuri O
, O
t O
} O
li O
t=1 O
where O
umsuri O
, O
t O
∈ O
rd O
, O
the O
probability O
distribution O
of O
each O
masked O
token O
can O
be O
calculated O
as O
pui O
, O
t O
= O
softmax O
( O
u O
msur O
i O
, O
t O
·wmsur O
+ O
bmsur O
) O
, O
( O
7 O
) O
where O
wmsur O
∈ O
rd×v O
is O
the O
token O
embedding O
table O
, O
v O
denotes O
the O
vocabulary O
size O
, O
and O
bmsur O
∈ O
rv O
is O
a O
bias O
vector O
. O

section 10
id pdf2json/2021.acl-long.285.pdf.json
finally O
, O
the O
pre-training O
objective O
of O
this O
self-supervised O
task O
is O
to O
minimize O
the O
negative O
log-likelihood O
loss O
as O
lmsur O
= O
− O
1 O
li O
li∑ O
t=1 O
log O
pui O
, O
t O
, O
( O
8 O
) O
where O
pui O
, O
t O
is O
the O
element O
in O
pui O
, O
t O
corresponding O
to O
the O
original O
token O
. O

section 11
id pdf2json/2021.acl-long.285.pdf.json
a O
full O
mpc O
instance O
can O
be O
divided O
into O
several O
sub-conversations O
and O
we O
assume O
that O
the O
representations O
of O
sub-conversations O
under O
the O
same O
parent O
node O
tend O
to O
be O
similar O
. O

section 11
id pdf2json/2021.acl-long.285.pdf.json
as O
illustrated O
in O
figure O
2 O
( O
b O
) O
, O
two O
sub-conversations O
{ O
u3 O
, O
u5 O
, O
u7 O
, O
u8 O
} O
and O
{ O
u4 O
, O
u6 O
, O
u9 O
} O
share O
the O
same O
parent O
node O
u2 O
. O

section 11
id pdf2json/2021.acl-long.285.pdf.json
thus O
, O
they O
should O
be O
semantically O
relevant O
. O

section 11
id pdf2json/2021.acl-long.285.pdf.json
under O
this O
assumption O
, O
we O
design O
a O
self-supervised O
task O
named O
shared O
node O
detection O
( O
snd O
) O
, O
which O
utilizes O
the O
conversation O
structure O
to O
strengthen O
the O
capability O
of O
models O
on O
measuring O
the O
semantic O
relevance O
of O
two O
sub-conversations O
. O

section 11
id pdf2json/2021.acl-long.285.pdf.json
we O
first O
construct O
the O
pre-training O
samples O
for O
this O
task O
. O

section 11
id pdf2json/2021.acl-long.285.pdf.json
empirically O
, O
only O
the O
sub-conversations O
under O
the O
top O
shared O
node O
in O
a O
conversation O
are O
collected O
in O
order O
to O
filter O
out O
the O
sub-conversations O
with O
few O
utterances O
. O

section 11
id pdf2json/2021.acl-long.285.pdf.json
given O
a O
full O
mpc O
, O
the O
two O
sub-conversations O
with O
the O
most O
utterances O
form O
a O
positive O
pair O
. O

section 11
id pdf2json/2021.acl-long.285.pdf.json
for O
each O
positive O
pair O
, O
we O
replace O
one O
of O
its O
elements O
with O
another O
sub-conversation O
randomly O
sampled O
from O
the O
training O
corpus O
to O
form O
a O
negative O
pair O
. O

section 11
id pdf2json/2021.acl-long.285.pdf.json
formally O
, O
given O
two O
sub-conversations O
ci O
and O
cj O
, O
utterances O
in O
each O
sub-conversation O
are O
first O
concatenated O
respectively O
to O
form O
two O
segments O
. O

section 11
id pdf2json/2021.acl-long.285.pdf.json
then O
, O
the O
two O
segments O
are O
concatenated O
with O
a O
[ O
sep O
] O
token O
and O
a O
[ O
cls O
] O
token O
is O
inserted O
at O
the O
beginning O
of O
the O
whole O
sequence O
. O

section 11
id pdf2json/2021.acl-long.285.pdf.json
this O
sequence O
are O
encoded O
by O
bert O
to O
derive O
the O
contextualized O
representation O
for O
the O
[ O
cls O
] O
token O
. O

section 11
id pdf2json/2021.acl-long.285.pdf.json
a O
non-linear O
transformation O
with O
sigmoid O
activation O
is O
further O
applied O
to O
this O
representation O
for O
calculating O
the O
matching O
score O
mij O
, O
i.e. O
, O
the O
probability O
of O
ci O
and O
cj O
sharing O
the O
same O
parent O
node O
. O

section 11
id pdf2json/2021.acl-long.285.pdf.json
finally O
, O
the O
pretraining O
objective O
of O
this O
task O
is O
to O
minimize O
the O
cross-entropy O
loss O
as O
lsnd O
= O
− O
[ O
yijlog O
( O
mij O
) O
+ O
( O
1− O
yij O
) O
log O
( O
1−mij O
) O
] O
, O
( O
9 O
) O
where O
yij O
= O
1 O
if O
ci O
and O
cj O
share O
the O
same O
parent O
node O
and O
yij O
= O
0 O
otherwise O
. O

section 12
id pdf2json/2021.acl-long.285.pdf.json
in O
addition O
, O
we O
also O
adopt O
the O
tasks O
of O
masked O
language O
model O
( O
mlm O
) O
and O
next O
sentence O
prediction O
( O
nsp O
) O
in O
original O
bert O
pre-training O
( O
devlin O
et O
al. O
, O
2019 O
) O
, O
which O
have O
been O
proven O
effective O
for O
incorporating O
domain O
knowledge O
( O
gu O
et O
al. O
, O
2020 O
; O
gururangan O
et O
al. O
, O
2020 O
) O
. O

section 12
id pdf2json/2021.acl-long.285.pdf.json
finally O
, O
mpcbert O
is O
trained O
by O
performing O
multi-task O
learning O
that O
minimizes O
the O
sum O
of O
all O
loss O
functions O
as O
l O
= O
lrur O
+ O
liss O
+ O
lpcd O
+ O
lmsur O
+ O
lsnd O
+ O
lmlm O
+ O
lnsp O
. O

section 12
id pdf2json/2021.acl-long.285.pdf.json
( O
10 O
) O

section 14
id pdf2json/2021.acl-long.285.pdf.json
given O
a O
multi-party O
conversation O
where O
part O
of O
the O
addressees O
are O
unknown O
, O
ouchi O
and O
tsuboi O
( O
2016 O
) O
and O
zhang O
et O
al O
. O

section 14
id pdf2json/2021.acl-long.285.pdf.json
( O
2018a O
) O
recognized O
an O
addressee O
of O
the O
last O
utterance O
. O

section 14
id pdf2json/2021.acl-long.285.pdf.json
le O
et O
al O
. O

section 14
id pdf2json/2021.acl-long.285.pdf.json
( O
2019 O
) O
recognized O
addressees O
of O
all O
utterances O
in O
a O
conversation O
. O

section 14
id pdf2json/2021.acl-long.285.pdf.json
in O
this O
paper O
, O
we O
follow O
the O
more O
challenging O
setting O
in O
le O
et O
al O
. O

section 14
id pdf2json/2021.acl-long.285.pdf.json
( O
2019 O
) O
. O

section 14
id pdf2json/2021.acl-long.285.pdf.json
formally O
, O
models O
are O
asked O
to O
predict O
{ O
ân O
} O
nn=1 O
given O
{ O
( O
sn O
, O
un O
, O
an O
) O
} O
nn=1\ O
{ O
an O
} O
nn=1 O
, O
where O
ân O
is O
selected O
from O
the O
interlocutor O
set O
in O
this O
conversation O
and O
\ O
denotes O
exclusion O
. O

section 14
id pdf2json/2021.acl-long.285.pdf.json
when O
applying O
mpc-bert O
, O
this O
task O
is O
reformulated O
as O
finding O
a O
preceding O
utterance O
from O
the O
same O
addressee O
. O

section 14
id pdf2json/2021.acl-long.285.pdf.json
its O
rur O
matching O
scores O
with O
all O
preceding O
utterances O
are O
calculated O
following O
eq O
. O

section 14
id pdf2json/2021.acl-long.285.pdf.json
( O
1 O
) O
. O

section 14
id pdf2json/2021.acl-long.285.pdf.json
then O
, O
the O
utterance O
with O
the O
highest O
score O
is O
selected O
and O
the O
speaker O
of O
the O
selected O
utterance O
is O
considered O
as O
the O
recognized O
addressee O
. O

section 14
id pdf2json/2021.acl-long.285.pdf.json
finally O
, O
the O
fine-tuning O
objective O
of O
this O
task O
is O
to O
minimize O
the O
crossentropy O
loss O
as O
lar O
= O
− O
n∑ O
i=2 O
i−1∑ O
j=1 O
yij O
log O
( O
mij O
) O
, O
( O
11 O
) O
where O
mij O
is O
defined O
in O
eq O
. O

section 14
id pdf2json/2021.acl-long.285.pdf.json
( O
1 O
) O
, O
yij O
= O
1 O
if O
the O
speaker O
of O
uj O
is O
the O
addressee O
of O
ui O
and O
yij O
= O
0 O
otherwise O
. O

section 15
id pdf2json/2021.acl-long.285.pdf.json
this O
task O
aims O
to O
identify O
the O
speaker O
of O
the O
last O
utterance O
in O
a O
conversation O
. O

section 15
id pdf2json/2021.acl-long.285.pdf.json
formally O
, O
models O
are O
asked O
to O
predict O
ŝn O
given O
{ O
( O
sn O
, O
un O
, O
an O
) O
} O
nn=1\sn O
, O
where O
ŝn O
is O
selected O
from O
the O
interlocutor O
set O
in O
this O
conversation O
. O

section 15
id pdf2json/2021.acl-long.285.pdf.json
when O
applying O
mpc-bert O
, O
this O
task O
is O
reformulated O
as O
identifying O
the O
utterances O
sharing O
the O
same O
speaker O
. O

section 15
id pdf2json/2021.acl-long.285.pdf.json
for O
the O
last O
utterance O
un O
, O
its O
speaker O
embedding O
is O
masked O
and O
its O
iss O
matching O
scores O
mnj O
with O
all O
preceding O
utterances O
are O
calculated O
following O
section O
3.2.2 O
. O

section 15
id pdf2json/2021.acl-long.285.pdf.json
the O
finetuning O
objective O
of O
this O
task O
is O
to O
minimize O
the O
cross-entropy O
loss O
as O
lsi O
= O
− O
n−1∑ O
j=1 O
ynj O
log O
( O
mnj O
) O
, O
( O
12 O
) O
where O
ynj O
= O
1 O
if O
uj O
shares O
the O
same O
speaker O
with O
un O
and O
ynj O
= O
0 O
otherwise O
. O

section 16
id pdf2json/2021.acl-long.285.pdf.json
this O
task O
asks O
models O
to O
select O
ûn O
from O
a O
set O
of O
response O
candidates O
given O
the O
conversation O
context O
{ O
( O
sn O
, O
un O
, O
an O
) O
} O
nn=1\un O
. O

section 16
id pdf2json/2021.acl-long.285.pdf.json
the O
key O
is O
to O
measure O
the O
similarity O
between O
two O
segments O
of O
context O
and O
response O
. O

section 16
id pdf2json/2021.acl-long.285.pdf.json
we O
concatenate O
each O
response O
candidate O
with O
the O
context O
and O
extract O
the O
contextualized O
representation O
e O
[ O
cls O
] O
for O
the O
first O
[ O
cls O
] O
token O
using O
mpc-bert O
. O

section 16
id pdf2json/2021.acl-long.285.pdf.json
then O
, O
e O
[ O
cls O
] O
is O
fed O
into O
a O
nonlinear O
transformation O
with O
sigmoid O
activation O
to O
obtain O
the O
matching O
score O
between O
the O
context O
and O
the O
response O
. O

section 16
id pdf2json/2021.acl-long.285.pdf.json
finally O
, O
the O
fine-tuning O
objective O
of O
this O
task O
is O
to O
minimize O
the O
cross-entropy O
loss O
according O
to O
the O
true/false O
labels O
of O
responses O
in O
the O
training O
set O
as O
lrs O
= O
− O
[ O
ylog O
( O
mcr O
) O
+ O
( O
1−y O
) O
log O
( O
1−mcr O
) O
] O
, O
( O
13 O
) O
where O
y O
= O
1 O
if O
the O
response O
r O
is O
a O
proper O
one O
for O
the O
context O
c O
; O
otherwise O
y O
= O
0 O
. O

section 18
id pdf2json/2021.acl-long.285.pdf.json
we O
evaluated O
our O
proposed O
methods O
on O
two O
ubuntu O
irc O
benchmarks O
. O

section 18
id pdf2json/2021.acl-long.285.pdf.json
one O
was O
released O
by O
hu O
et O
al O
. O

section 18
id pdf2json/2021.acl-long.285.pdf.json
( O
2019 O
) O
, O
in O
which O
both O
speaker O
and O
addressee O
labels O
was O
provided O
for O
each O
utterance O
. O

section 18
id pdf2json/2021.acl-long.285.pdf.json
the O
other O
benchmark O
was O
released O
by O
ouchi O
and O
tsuboi O
( O
2016 O
) O
. O

section 18
id pdf2json/2021.acl-long.285.pdf.json
here O
, O
we O
adopted O
the O
version O
shared O
in O
le O
et O
al O
. O

section 18
id pdf2json/2021.acl-long.285.pdf.json
( O
2019 O
) O
for O
fair O
comparison O
. O

section 18
id pdf2json/2021.acl-long.285.pdf.json
the O
conversation O
sessions O
were O
separated O
into O
three O
categories O
according O
to O
the O
session O
length O
( O
len5 O
, O
len-10 O
and O
len-15 O
) O
following O
the O
splitting O
strategy O
of O
previous O
studies O
( O
ouchi O
and O
tsuboi O
, O
2016 O
; O
zhang O
et O
al. O
, O
2018a O
; O
le O
et O
al. O
, O
2019 O
) O
. O

section 18
id pdf2json/2021.acl-long.285.pdf.json
table O
2 O
presents O
the O
statistics O
of O
the O
two O
benchmarks O
evaluated O
in O
our O
experiments O
. O

section 19
id pdf2json/2021.acl-long.285.pdf.json
non-pre-training-based O
models O
ouchi O
and O
tsuboi O
( O
2016 O
) O
proposed O
a O
dynamic O
model O
drnn O
which O
updated O
speaker O
embeddings O
with O
the O
conversation O
flow O
. O

section 19
id pdf2json/2021.acl-long.285.pdf.json
zhang O
et O
al O
. O

section 19
id pdf2json/2021.acl-long.285.pdf.json
( O
2018a O
) O
improved O
drnn O
to O
si-rnn O
which O
updated O
speaker O
embeddings O
role-sensitively O
. O

section 19
id pdf2json/2021.acl-long.285.pdf.json
le O
et O
al O
. O

section 19
id pdf2json/2021.acl-long.285.pdf.json
( O
2019 O
) O
proposed O
w2w O
which O
jointly O
modeled O
interlocutors O
and O
utterances O
in O
a O
uniform O
framework O
, O
and O
predicted O
all O
addressees O
. O

section 19
id pdf2json/2021.acl-long.285.pdf.json
pre-training-based O
models O
bert O
( O
devlin O
et O
al. O
, O
2019 O
) O
was O
pre-trained O
to O
learn O
general O
language O
representations O
with O
mlm O
and O
nsp O
tasks O
. O

section 19
id pdf2json/2021.acl-long.285.pdf.json
sabert O
( O
gu O
et O
al. O
, O
2020 O
) O
added O
speaker O
embeddings O
and O
further O
pre-trained O
bert O
on O
a O
domain-specific O
corpus O
to O
incorporate O
domain O
knowledge O
. O

section 19
id pdf2json/2021.acl-long.285.pdf.json
we O
re-implemented O
sa-bert O
with O
the O
pre-training O
corpus O
used O
in O
this O
paper O
to O
ensure O
fair O
comparison O
. O

section 20
id pdf2json/2021.acl-long.285.pdf.json
the O
version O
of O
bert-base-uncased O
was O
adopted O
for O
all O
our O
experiments O
. O

section 20
id pdf2json/2021.acl-long.285.pdf.json
for O
pre-training O
, O
gelu O
( O
hendrycks O
and O
gimpel O
, O
2016 O
) O
was O
employed O
as O
the O
activation O
for O
all O
non-linear O
transformations O
. O

section 20
id pdf2json/2021.acl-long.285.pdf.json
the O
adam O
method O
( O
kingma O
and O
ba O
, O
2015 O
) O
was O
employed O
for O
optimization O
. O

section 20
id pdf2json/2021.acl-long.285.pdf.json
the O
learning O
rate O
was O
initialized O
as O
0.00005 O
and O
the O
warmup O
proportion O
was O
set O
to O
0.1 O
. O

section 20
id pdf2json/2021.acl-long.285.pdf.json
we O
pre-trained O
bert O
for O
10 O
epochs O
. O

section 20
id pdf2json/2021.acl-long.285.pdf.json
the O
training O
set O
of O
the O
dateset O
used O
in O
hu O
et O
al O
. O

section 20
id pdf2json/2021.acl-long.285.pdf.json
( O
2019 O
) O
was O
employed O
for O
pre-training O
. O

section 20
id pdf2json/2021.acl-long.285.pdf.json
the O
maximum O
utterance O
number O
was O
set O
to O
7 O
. O

section 20
id pdf2json/2021.acl-long.285.pdf.json
the O
maximum O
sequence O
length O
was O
set O
to O
230 O
. O

section 20
id pdf2json/2021.acl-long.285.pdf.json
the O
maximum O
sampling O
numbers O
for O
each O
example O
were O
set O
to O
4 O
for O
rur O
, O
2 O
for O
iss O
and O
2 O
for O
pcd O
. O

section 20
id pdf2json/2021.acl-long.285.pdf.json
∆ O
in O
eq O
. O

section 20
id pdf2json/2021.acl-long.285.pdf.json
( O
6 O
) O
was O
set O
to O
0.4 O
, O
achieving O
the O
best O
performance O
out O
of O
{ O
0.2 O
, O
0.4 O
, O
0.6 O
, O
0.8 O
} O
on O
the O
validation O
set O
. O

section 20
id pdf2json/2021.acl-long.285.pdf.json
the O
pre-training O
was O
performed O
using O
a O
geforce O
rtx O
2080 O
ti O
gpu O
and O
the O
batch O
size O
was O
set O
to O
4 O
. O

section 20
id pdf2json/2021.acl-long.285.pdf.json
for O
fine-tuning O
, O
some O
configurations O
were O
different O
according O
to O
the O
characteristics O
of O
these O
datasets O
. O

section 20
id pdf2json/2021.acl-long.285.pdf.json
for O
hu O
et O
al O
. O

section 20
id pdf2json/2021.acl-long.285.pdf.json
( O
2019 O
) O
, O
the O
maximum O
utterance O
number O
was O
set O
to O
7 O
and O
the O
maximum O
sequence O
length O
was O
set O
to O
230 O
. O

section 20
id pdf2json/2021.acl-long.285.pdf.json
for O
the O
three O
experimental O
settings O
in O
ouchi O
and O
tsuboi O
( O
2016 O
) O
, O
the O
maximum O
utterance O
numbers O
were O
set O
to O
5 O
, O
10 O
and O
15 O
, O
and O
the O
maximum O
sequence O
lengths O
were O
set O
to O
120 O
, O
220 O
and O
320 O
. O

section 20
id pdf2json/2021.acl-long.285.pdf.json
all O
parameters O
in O
plms O
were O
updated O
. O

section 20
id pdf2json/2021.acl-long.285.pdf.json
the O
learning O
rate O
was O
initialized O
as O
0.00002 O
and O
the O
warmup O
proportion O
was O
set O
to O
0.1 O
. O

section 20
id pdf2json/2021.acl-long.285.pdf.json
for O
hu O
et O
al O
. O

section 20
id pdf2json/2021.acl-long.285.pdf.json
( O
2019 O
) O
, O
the O
fine-tuning O
process O
was O
performed O
for O
10 O
epochs O
for O
addressee O
recognition O
, O
10 O
epochs O
for O
speaker O
identification O
, O
and O
5 O
epochs O
for O
response O
selection O
. O

section 20
id pdf2json/2021.acl-long.285.pdf.json
for O
ouchi O
and O
tsuboi O
( O
2016 O
) O
, O
the O
fine-tuning O
epochs O
were O
set O
to O
5 O
, O
5 O
and O
3 O
respectively O
. O

section 20
id pdf2json/2021.acl-long.285.pdf.json
the O
fine-tuning O
was O
also O
performed O
using O
a O
geforce O
rtx O
2080 O
ti O
gpu O
. O

section 20
id pdf2json/2021.acl-long.285.pdf.json
the O
batch O
sizes O
were O
set O
to O
16 O
for O
hu O
et O
al O
. O

section 20
id pdf2json/2021.acl-long.285.pdf.json
( O
2019 O
) O
, O
and O
40 O
, O
20 O
, O
and O
12 O
for O
the O
three O
experimental O
settings O
in O
ouchi O
and O
tsuboi O
( O
2016 O
) O
respectively O
. O

section 20
id pdf2json/2021.acl-long.285.pdf.json
the O
validation O
set O
was O
used O
to O
select O
the O
best O
model O
for O
testing O
. O

section 20
id pdf2json/2021.acl-long.285.pdf.json
all O
codes O
were O
implemented O
in O
the O
tensorflow O
framework O
( O
abadi O
et O
al. O
, O
2016 O
) O
and O
are O
published O
to O
help O
replicate O
our O
results O
. O

section 20
id pdf2json/2021.acl-long.285.pdf.json
1 O

section 21
id pdf2json/2021.acl-long.285.pdf.json
addressee O
recognition O
we O
followed O
the O
metrics O
of O
previous O
work O
( O
le O
et O
al. O
, O
2019 O
) O
by O
employing O
precision O
@ O
1 O
( O
p O
@ O
1 O
) O
to O
evaluate O
each O
utterance O
with O
ground O
truth O
. O

section 21
id pdf2json/2021.acl-long.285.pdf.json
also O
, O
a O
session O
is O
marked O
as O
positive O
if O
the O
addressees O
of O
all O
its O
utterances O
are O
correctly O
recognized O
, O
which O
is O
calculated O
as O
accuracy O
( O
acc O
. O
) O
. O

section 21
id pdf2json/2021.acl-long.285.pdf.json
table O
3 O
presents O
the O
results O
of O
addressee O
recognition O
. O

section 21
id pdf2json/2021.acl-long.285.pdf.json
it O
shows O
that O
mpc-bert O
outperforms O
the O
best O
performing O
model O
, O
i.e. O
, O
sa-bert O
, O
by O
margins O
of O
3.51 O
% O
, O
2.86 O
% O
, O
3.28 O
% O
and O
5.36 O
% O
on O
these O
test O
sets O
respectively O
in O
terms O
of O
acc. O
, O
verifying O
the O
effectiveness O
of O
the O
proposed O
five O
selfsupervised O
tasks O
as O
a O
whole O
. O

section 21
id pdf2json/2021.acl-long.285.pdf.json
to O
further O
illustrate O
the O
effectiveness O
of O
each O
task O
, O
ablation O
tests O
were O
performed O
as O
shown O
in O
the O
last O
five O
rows O
of O
table O
3 O
. O

section 21
id pdf2json/2021.acl-long.285.pdf.json
we O
can O
observe O
that O
all O
self-supervised O
tasks O
are O
useful O
as O
removing O
any O
of O
them O
causes O
performance O
1https O
: O
//github.com/jasonforjoy/mpc-bert O
drop O
. O

section 21
id pdf2json/2021.acl-long.285.pdf.json
among O
the O
five O
tasks O
, O
rur O
plays O
the O
most O
important O
role O
, O
and O
the O
tasks O
focusing O
on O
modeling O
interlocutor O
structure O
contribute O
more O
than O
those O
for O
utterance O
semantics O
. O

section 21
id pdf2json/2021.acl-long.285.pdf.json
speaker O
identification O
similarly O
, O
p O
@ O
1 O
was O
employed O
as O
the O
evaluation O
metric O
of O
speaker O
identification O
for O
the O
last O
utterance O
of O
a O
conversation O
and O
the O
results O
are O
shown O
in O
table O
4 O
. O

section 21
id pdf2json/2021.acl-long.285.pdf.json
it O
shows O
that O
mpc-bert O
outperforms O
sa-bert O
by O
margins O
of O
7.66 O
% O
, O
2.60 O
% O
, O
3.38 O
% O
and O
4.24 O
% O
respectively O
in O
terms O
of O
p O
@ O
1 O
. O

section 21
id pdf2json/2021.acl-long.285.pdf.json
besides O
, O
from O
the O
ablation O
results O
we O
find O
that O
all O
tasks O
are O
useful O
for O
improving O
the O
performance O
of O
speaker O
identification O
and O
iss O
and O
rur O
contribute O
the O
most O
. O

section 21
id pdf2json/2021.acl-long.285.pdf.json
in O
particular O
, O
removing O
pcd O
, O
msur O
and O
snd O
only O
leads O
to O
slight O
performance O
drop O
. O

section 21
id pdf2json/2021.acl-long.285.pdf.json
the O
reason O
might O
be O
that O
the O
information O
conveyed O
by O
these O
tasks O
is O
redundant O
. O

section 21
id pdf2json/2021.acl-long.285.pdf.json
response O
selection O
the O
rn O
@ O
k O
metrics O
adopted O
by O
previous O
studies O
( O
ouchi O
and O
tsuboi O
, O
2016 O
; O
zhang O
et O
al. O
, O
2018a O
) O
were O
used O
here O
. O

section 21
id pdf2json/2021.acl-long.285.pdf.json
each O
model O
was O
tasked O
with O
selecting O
k O
best-matched O
responses O
from O
n O
available O
candidates O
, O
and O
we O
calculated O
the O
recall O
as O
rn O
@ O
k. O
two O
settings O
were O
followed O
in O
which O
k O
was O
set O
to O
1 O
and O
n O
was O
set O
to O
2 O
or O
10 O
. O

section 21
id pdf2json/2021.acl-long.285.pdf.json
table O
5 O
presents O
the O
results O
of O
response O
selection O
. O

section 21
id pdf2json/2021.acl-long.285.pdf.json
it O
shows O
that O
mpc-bert O
outperforms O
sabert O
by O
margins O
of O
3.82 O
% O
, O
2.71 O
% O
, O
2.55 O
% O
and O
3.22 O
% O
respectively O
in O
terms O
of O
r10 O
@ O
1 O
. O

section 21
id pdf2json/2021.acl-long.285.pdf.json
ablation O
tests O
show O
that O
snd O
is O
the O
most O
useful O
task O
for O
response O
selection O
and O
the O
two O
tasks O
focusing O
on O
the O
utterance O
semantics O
contribute O
more O
than O
those O
focusing O
on O
the O
interlocutor O
structures O
. O

section 22
id pdf2json/2021.acl-long.285.pdf.json
figure O
3 O
illustrates O
how O
the O
performance O
of O
bert O
, O
sa-bert O
and O
mpc-bert O
changed O
with O
respect O
to O
different O
session O
lengths O
on O
the O
test O
sets O
of O
ouchi O
and O
tsuboi O
( O
2016 O
) O
. O

section 22
id pdf2json/2021.acl-long.285.pdf.json
it O
can O
be O
seen O
that O
the O
performance O
of O
addressee O
recognition O
and O
speaker O
identification O
dropped O
as O
the O
session O
length O
increased O
. O

section 22
id pdf2json/2021.acl-long.285.pdf.json
the O
reason O
might O
be O
that O
longer O
sessions O
always O
contain O
more O
interlocutors O
which O
increase O
the O
difficulties O
of O
predicting O
interlocutors O
. O

section 22
id pdf2json/2021.acl-long.285.pdf.json
meanwhile O
, O
the O
performance O
of O
response O
selection O
was O
significantly O
improved O
as O
the O
session O
length O
increased O
. O

section 22
id pdf2json/2021.acl-long.285.pdf.json
it O
can O
be O
attributed O
to O
that O
longer O
sessions O
enrich O
the O
representations O
of O
contexts O
with O
more O
details O
which O
benefit O
response O
selection O
. O

section 22
id pdf2json/2021.acl-long.285.pdf.json
furthermore O
, O
as O
the O
session O
length O
increased O
, O
the O
performance O
of O
mpc-bert O
dropped O
more O
slightly O
than O
that O
of O
sa-bert O
on O
addressee O
recognition O
and O
speaker O
identification O
, O
and O
the O
r10 O
@ O
1 O
gap O
between O
mpc-bert O
and O
sa-bert O
on O
response O
selection O
enlarged O
from O
2.71 O
% O
to O
3.22 O
% O
. O

section 22
id pdf2json/2021.acl-long.285.pdf.json
these O
results O
imply O
the O
superiority O
of O
mpc-bert O
over O
sa-bert O
on O
modeling O
long O
mpcs O
with O
complicated O
structures O
. O

section 23
id pdf2json/2021.acl-long.285.pdf.json
in O
this O
paper O
, O
we O
present O
mpc-bert O
, O
a O
pre-trained O
language O
model O
with O
five O
self-supervised O
tasks O
for O
mpc O
understanding O
. O

section 23
id pdf2json/2021.acl-long.285.pdf.json
these O
tasks O
jointly O
learn O
who O
says O
what O
to O
whom O
in O
mpcs O
. O

section 23
id pdf2json/2021.acl-long.285.pdf.json
experimental O
results O
on O
three O
downstream O
tasks O
show O
that O
mpc-bert O
outperforms O
previous O
methods O
by O
large O
margins O
and O
achieves O
new O
state-of-the-art O
performance O
on O
two O
benchmarks O
. O

section 24
id pdf2json/2021.acl-long.285.pdf.json
we O
thank O
anonymous O
reviewers O
for O
their O
valuable O
comments O
. O

section TITLE
id pdf2json/2021.acl-long.322.pdf.json
learning O
language O
and O
multimodal O
privacy-preserving O
markers O
of O
mood O
from O
mobile O
data O

section ABSTRACT
id pdf2json/2021.acl-long.322.pdf.json
mental O
health O
conditions O
remain O
underdiagnosed O
even O
in O
countries O
with O
common O
access O
to O
advanced O
medical O
care O
. O

section ABSTRACT
id pdf2json/2021.acl-long.322.pdf.json
the O
ability O
to O
accurately O
and O
efficiently O
predict O
mood O
from O
easily O
collectible O
data O
has O
several O
important O
implications O
for O
the O
early O
detection O
, O
intervention O
, O
and O
treatment O
of O
mental O
health O
disorders O
. O

section ABSTRACT
id pdf2json/2021.acl-long.322.pdf.json
one O
promising O
data O
source O
to O
help O
monitor O
human O
behavior O
is O
daily O
smartphone O
usage O
. O

section ABSTRACT
id pdf2json/2021.acl-long.322.pdf.json
however O
, O
care O
must O
be O
taken O
to O
summarize O
behaviors O
without O
identifying O
the O
user O
through O
personal O
( O
e.g. O
, O
personally O
identifiable O
information O
) O
or O
protected O
( O
e.g. O
, O
race O
, O
gender O
) O
attributes O
. O

section ABSTRACT
id pdf2json/2021.acl-long.322.pdf.json
in O
this O
paper O
, O
we O
study O
behavioral O
markers O
of O
daily O
mood O
using O
a O
recent O
dataset O
of O
mobile O
behaviors O
from O
adolescent O
populations O
at O
high O
risk O
of O
suicidal O
behaviors O
. O

section ABSTRACT
id pdf2json/2021.acl-long.322.pdf.json
using O
computational O
models O
, O
we O
find O
that O
language O
and O
multimodal O
representations O
of O
mobile O
typed O
text O
( O
spanning O
typed O
characters O
, O
words O
, O
keystroke O
timings O
, O
and O
app O
usage O
) O
are O
predictive O
of O
daily O
mood O
. O

section ABSTRACT
id pdf2json/2021.acl-long.322.pdf.json
however O
, O
we O
find O
that O
models O
trained O
to O
predict O
mood O
often O
also O
capture O
private O
user O
identities O
in O
their O
intermediate O
representations O
. O

section ABSTRACT
id pdf2json/2021.acl-long.322.pdf.json
to O
tackle O
this O
problem O
, O
we O
evaluate O
approaches O
that O
obfuscate O
user O
identity O
while O
remaining O
predictive O
. O

section ABSTRACT
id pdf2json/2021.acl-long.322.pdf.json
by O
combining O
multimodal O
representations O
with O
privacy-preserving O
learning O
, O
we O
are O
able O
to O
push O
forward O
the O
performanceprivacy O
frontier O
. O

section 0
id pdf2json/2021.acl-long.322.pdf.json
proceedings O
of O
the O
59th O
annual O
meeting O
of O
the O
association O
for O
computational O
linguistics O
and O
the O
11th O
international O
joint O
conference O
on O
natural O
language O
processing O
, O
pages O
4170–4187 O
august O
1–6 O
, O
2021 O
. O

section 0
id pdf2json/2021.acl-long.322.pdf.json
©2021 O
association O
for O
computational O
linguistics O
4170 O

section 1
id pdf2json/2021.acl-long.322.pdf.json
mental O
illnesses O
can O
have O
a O
damaging O
permanent O
impact O
on O
communities O
, O
societies O
, O
and O
economies O
all O
over O
the O
world O
( O
world O
health O
organization O
, O
2003 O
) O
. O

section 1
id pdf2json/2021.acl-long.322.pdf.json
individuals O
often O
do O
not O
realize O
they O
are O
at O
risk O
of O
mental O
disorders O
even O
when O
they O
have O
symptoms O
. O

section 1
id pdf2json/2021.acl-long.322.pdf.json
as O
a O
result O
, O
many O
are O
late O
in O
seeking O
professional O
help O
and O
treatment O
( O
thornicroft O
et O
al. O
, O
2016 O
) O
, O
particularly O
among O
adolescents O
where O
suicide O
is O
the O
second O
leading O
cause O
of O
death O
( O
curtin O
? O
first O
two O
authors O
contributed O
equally O
. O

section 1
id pdf2json/2021.acl-long.322.pdf.json
and O
heron O
, O
2019 O
) O
. O

section 1
id pdf2json/2021.acl-long.322.pdf.json
in O
addition O
to O
deaths O
, O
16 O
% O
of O
high O
school O
students O
report O
having O
serious O
suicidal O
thoughts O
each O
year O
, O
and O
8 O
% O
of O
them O
make O
one O
or O
more O
suicide O
attempts O
( O
cdc O
, O
2015 O
) O
. O

section 1
id pdf2json/2021.acl-long.322.pdf.json
this O
problem O
is O
particularly O
exacerbated O
as O
an O
“ O
echo O
pandemic O
” O
of O
mental O
health O
problems O
have O
arisen O
in O
the O
wake O
of O
the O
covid-19 O
pandemic O
( O
inkster O
et O
al. O
, O
2021 O
; O
saha O
et O
al. O
, O
2020 O
) O
. O

section 1
id pdf2json/2021.acl-long.322.pdf.json
intensive O
monitoring O
of O
behaviors O
via O
adolescents O
’ O
natural O
use O
of O
smartphones O
may O
help O
identify O
realtime O
predictors O
of O
mood O
in O
high-risk O
youth O
as O
a O
proxy O
for O
suicide O
risk O
( O
nahum-shani O
et O
al. O
, O
2018 O
) O
. O

section 1
id pdf2json/2021.acl-long.322.pdf.json
while O
there O
are O
inherent O
limitations O
in O
the O
mismatch O
between O
mood O
prediction O
and O
ultimately O
developing O
real-time O
intervention O
against O
imminent O
suicide O
risk O
( O
coppersmith O
et O
al. O
, O
2018 O
; O
ophir O
et O
al. O
, O
2020 O
) O
, O
we O
believe O
that O
the O
former O
is O
a O
reasonable O
starting O
point O
to O
tackle O
similar O
machine O
learning O
problems O
surrounding O
affective O
computing O
and O
privacy-preserving O
learning O
. O

section 1
id pdf2json/2021.acl-long.322.pdf.json
studying O
mood O
in O
this O
high-risk O
population O
is O
a O
valuable O
goal O
given O
that O
suicide O
attempts O
are O
often O
decided O
within O
a O
short O
time-lapse O
and O
just-in-time O
assessments O
of O
mood O
changes O
can O
be O
a O
stepping O
stone O
in O
this O
direction O
( O
rizk O
et O
al. O
, O
2019 O
; O
oquendo O
et O
al. O
, O
2020 O
) O
. O

section 1
id pdf2json/2021.acl-long.322.pdf.json
technologies O
for O
mood O
prediction O
can O
also O
be O
a O
valuable O
component O
of O
decision O
support O
for O
clinicians O
and O
healthcare O
providers O
during O
their O
assessments O
( O
mann O
et O
al. O
, O
2006 O
; O
cho O
et O
al. O
, O
2019 O
) O
. O

section 1
id pdf2json/2021.acl-long.322.pdf.json
recent O
work O
in O
affective O
computing O
has O
begun O
to O
explore O
the O
potential O
in O
predicting O
mood O
from O
mobile O
data O
. O

section 1
id pdf2json/2021.acl-long.322.pdf.json
studies O
have O
found O
that O
typing O
patterns O
( O
cao O
et O
al. O
, O
2017 O
; O
ghosh O
et O
al. O
, O
2017a O
; O
huang O
et O
al. O
, O
2018 O
; O
zulueta O
et O
al. O
, O
2018 O
) O
, O
self-reporting O
apps O
( O
suhara O
et O
al. O
, O
2017 O
) O
, O
and O
wearable O
sensors O
( O
ghosh O
et O
al. O
, O
2017b O
; O
sano O
et O
al. O
, O
2018 O
) O
are O
particularly O
predictive O
. O

section 1
id pdf2json/2021.acl-long.322.pdf.json
in O
addition O
, O
multimodal O
modeling O
of O
multiple O
sensors O
( O
e.g. O
, O
wearable O
sensors O
and O
smartphone O
apps O
) O
was O
shown O
to O
further O
improve O
performance O
( O
jaques O
et O
al. O
, O
2017 O
; O
taylor O
et O
al. O
, O
2017 O
) O
. O

section 1
id pdf2json/2021.acl-long.322.pdf.json
while O
current O
work O
primarily O
relies O
on O
selfreport O
apps O
for O
long-term O
mood O
assessments O
( O
glenn O
and O
nock O
, O
2014 O
) O
, O
our O
work O
investigates O
mobile O
behaviors O
from O
a O
high-risk O
teenage O
population O
as O
a O
predictive O
signal O
for O
daily O
mood O
( O
franklin O
et O
al. O
, O
2017 O
; O
large O
et O
al. O
, O
2017 O
) O
. O

section 1
id pdf2json/2021.acl-long.322.pdf.json
prior O
work O
has O
also O
shown O
that O
private O
information O
is O
predictable O
from O
digital O
records O
of O
human O
behavior O
( O
kosinski O
et O
al. O
, O
2013 O
) O
, O
which O
is O
dangerous O
especially O
when O
sensitive O
user O
data O
is O
involved O
. O

section 1
id pdf2json/2021.acl-long.322.pdf.json
as O
a O
result O
, O
in O
parallel O
to O
improving O
predictive O
performance O
, O
a O
recent O
focus O
has O
been O
on O
improving O
privacy O
through O
techniques O
such O
as O
differential O
privacy O
( O
dankar O
and O
el O
emam O
, O
2012 O
, O
2013 O
; O
dankar O
et O
al. O
, O
2012 O
) O
and O
federated O
learning O
( O
mcmahan O
et O
al. O
, O
2016 O
; O
geyer O
et O
al. O
, O
2017 O
; O
liang O
et O
al. O
, O
2020b O
) O
, O
especially O
for O
healthcare O
data O
( O
e.g. O
, O
electronic O
health O
records O
( O
xu O
and O
wang O
, O
2019 O
) O
) O
and O
wearable O
devices O
( O
chen O
et O
al. O
, O
2020 O
) O
. O

section 1
id pdf2json/2021.acl-long.322.pdf.json
in O
this O
paper O
, O
as O
a O
step O
towards O
using O
multimodal O
privacy-preserving O
mood O
prediction O
as O
fine-grained O
signals O
to O
aid O
in O
mental O
health O
assessment O
, O
we O
analyze O
a O
recent O
dataset O
of O
mobile O
behaviors O
collected O
from O
adolescent O
populations O
at O
high O
suicidal O
risk O
. O

section 1
id pdf2json/2021.acl-long.322.pdf.json
with O
consent O
from O
participating O
groups O
, O
the O
dataset O
collects O
fine-grained O
features O
spanning O
online O
communication O
, O
keystroke O
patterns O
, O
and O
application O
usage O
. O

section 1
id pdf2json/2021.acl-long.322.pdf.json
participants O
are O
administered O
daily O
questions O
probing O
for O
mood O
scores O
. O

section 1
id pdf2json/2021.acl-long.322.pdf.json
by O
collecting O
and O
working O
on O
ground-truth O
data O
for O
this O
population O
, O
we O
are O
able O
to O
benchmark O
on O
a O
more O
accurate O
indica- O
tor O
of O
mood O
rather O
than O
proxy O
data O
such O
as O
mood O
signals O
inferred O
from O
social O
media O
content O
or O
behavior O
( O
ernala O
et O
al. O
, O
2019 O
) O
. O

section 1
id pdf2json/2021.acl-long.322.pdf.json
this O
unique O
dataset O
presents O
an O
opportunity O
to O
investigate O
a O
different O
medium O
of O
natural O
language O
processing O
- O
typed O
text O
which O
presents O
new O
challenges O
beyond O
conventionally O
studied O
written O
( O
marcus O
et O
al. O
, O
1993 O
) O
and O
spoken O
( O
marslen-wilson O
and O
tyler O
, O
1980 O
) O
text O
. O

section 1
id pdf2json/2021.acl-long.322.pdf.json
we O
propose O
multimodal O
models O
that O
contextualize O
text O
with O
their O
typing O
speeds O
and O
app O
usage O
. O

section 1
id pdf2json/2021.acl-long.322.pdf.json
however O
, O
these O
models O
often O
capture O
private O
user O
identities O
in O
their O
intermediate O
representations O
when O
predicting O
mood O
. O

section 1
id pdf2json/2021.acl-long.322.pdf.json
as O
a O
step O
towards O
privacy-preserving O
learning O
, O
we O
also O
propose O
approaches O
that O
obfuscate O
user O
identity O
while O
remaining O
predictive O
of O
daily O
mood O
. O

section 1
id pdf2json/2021.acl-long.322.pdf.json
by O
combining O
multimodal O
contextualization O
with O
privacy-preserving O
learning O
, O
we O
are O
able O
to O
push O
forward O
the O
performance-privacy O
frontier O
. O

section 1
id pdf2json/2021.acl-long.322.pdf.json
finally O
, O
we O
conclude O
with O
several O
observations O
regarding O
the O
uniqueness O
of O
typed O
text O
as O
an O
opportunity O
for O
nlp O
on O
mobile O
data O
. O

section 2
id pdf2json/2021.acl-long.322.pdf.json
intensive O
monitoring O
of O
behaviors O
via O
adolescents O
’ O
frequent O
use O
of O
smartphones O
may O
shed O
new O
light O
on O
the O
early O
risk O
of O
suicidal O
thoughts O
and O
ideations O
( O
nahum-shani O
et O
al. O
, O
2018 O
) O
. O

section 2
id pdf2json/2021.acl-long.322.pdf.json
smartphones O
provide O
a O
valuable O
and O
natural O
data O
source O
with O
rich O
behavioral O
markers O
spanning O
online O
communication O
, O
keystroke O
patterns O
, O
and O
application O
usage O
. O

section 2
id pdf2json/2021.acl-long.322.pdf.json
learning O
these O
markers O
requires O
large O
datasets O
with O
diversity O
in O
participants O
, O
variety O
in O
features O
, O
and O
accuracy O
in O
annotations O
. O

section 2
id pdf2json/2021.acl-long.322.pdf.json
as O
a O
step O
towards O
this O
goal O
, O
we O
recently O
collected O
a O
dataset O
of O
mobile O
behaviors O
from O
high-risk O
adolescent O
populations O
with O
consent O
from O
participating O
groups O
. O

section 2
id pdf2json/2021.acl-long.322.pdf.json
we O
begin O
with O
a O
brief O
review O
of O
the O
data O
collection O
process O
. O

section 2
id pdf2json/2021.acl-long.322.pdf.json
this O
data O
monitors O
adolescents O
spanning O
( O
a O
) O
recent O
suicide O
attempters O
( O
past O
6 O
months O
) O
with O
current O
suicidal O
ideation O
, O
( O
b O
) O
suicide O
ideators O
with O
no O
past O
suicide O
attempts O
, O
and O
( O
c O
) O
psychiatric O
controls O
with O
no O
history O
of O
suicide O
ideation O
or O
attempts O
. O

section 2
id pdf2json/2021.acl-long.322.pdf.json
passive O
sensing O
data O
is O
collected O
from O
each O
participant O
’ O
s O
smartphone O
across O
a O
duration O
of O
6 O
months O
. O

section 2
id pdf2json/2021.acl-long.322.pdf.json
participants O
are O
administered O
clinical O
interviews O
probing O
for O
suicidal O
thoughts O
and O
behaviors O
( O
stbs O
) O
, O
and O
self-report O
instruments O
regarding O
symptoms O
and O
acute O
events O
( O
e.g. O
, O
suicide O
attempts O
, O
psychiatric O
hospitalizations O
) O
are O
tracked O
weekly O
via O
a O
questionnaire O
. O

section 2
id pdf2json/2021.acl-long.322.pdf.json
all O
users O
have O
given O
consent O
for O
their O
mobile O
data O
to O
be O
collected O
and O
shared O
with O
us O
for O
research O
purposes O
. O

section 2
id pdf2json/2021.acl-long.322.pdf.json
this O
study O
has O
been O
carefully O
reviewed O
and O
approved O
by O
an O
irb O
. O

section 2
id pdf2json/2021.acl-long.322.pdf.json
we O
follow O
the O
nih O
guidelines O
, O
with O
a O
central O
irb O
( O
single O
irb O
) O
linked O
to O
secondary O
sites O
. O

section 2
id pdf2json/2021.acl-long.322.pdf.json
we O
have O
irb O
approval O
for O
the O
central O
institution O
and O
all O
secondary O
sites O
. O

section 3
id pdf2json/2021.acl-long.322.pdf.json
every O
day O
at O
8am O
, O
users O
are O
asked O
to O
respond O
to O
the O
following O
question O
- O
“ O
in O
general O
, O
how O
have O
you O
been O
feeling O
over O
the O
last O
day O
? O
” O
- O
with O
an O
integer O
score O
between O
0 O
and O
100 O
, O
where O
0 O
means O
very O
negative O
and O
100 O
means O
very O
positive O
. O

section 3
id pdf2json/2021.acl-long.322.pdf.json
to O
construct O
our O
prediction O
task O
, O
we O
discretized O
these O
scores O
into O
the O
following O
three O
bins O
: O
negative O
( O
0− O
33 O
) O
, O
neutral O
( O
34− O
66 O
) O
, O
and O
positive O
( O
67− O
100 O
) O
, O
which O
follow O
a O
class O
distribution O
of O
12.43 O
% O
, O
43.63 O
% O
, O
and O
43.94 O
% O
respectively O
. O

section 3
id pdf2json/2021.acl-long.322.pdf.json
for O
our O
3-way O
classification O
task O
, O
participants O
with O
fewer O
than O
50 O
daily O
self-reports O
were O
removed O
since O
these O
participants O
do O
not O
provide O
enough O
data O
to O
train O
an O
effective O
model O
. O

section 3
id pdf2json/2021.acl-long.322.pdf.json
in O
total O
, O
our O
dataset O
consists O
of O
1641 O
samples O
, O
consisting O
of O
data O
coming O
from O
17 O
unique O
participants O
. O

section 4
id pdf2json/2021.acl-long.322.pdf.json
we O
focused O
on O
keyboard O
data O
, O
which O
includes O
the O
time O
of O
data O
capture O
, O
the O
mobile O
application O
used O
, O
and O
the O
text O
entered O
by O
the O
user O
. O

section 4
id pdf2json/2021.acl-long.322.pdf.json
for O
each O
daily O
score O
response O
at O
8am O
, O
we O
use O
information O
collected O
between O
5am O
on O
the O
previous O
day O
to O
5am O
on O
the O
current O
day O
. O

section 4
id pdf2json/2021.acl-long.322.pdf.json
we O
chose O
this O
5am-5am O
window O
by O
looking O
at O
mobile O
activity O
and O
finding O
the O
lowest O
activity O
point O
when O
most O
people O
ended O
their O
day O
: O
5am O
. O

section 4
id pdf2json/2021.acl-long.322.pdf.json
since O
users O
report O
the O
previous O
day O
’ O
s O
mood O
( O
when O
prompted O
at O
8am O
) O
, O
we O
decided O
to O
use O
this O
5am-5am O
time O
period O
to O
summarize O
the O
previous O
day O
’ O
s O
activities O
. O

section 4
id pdf2json/2021.acl-long.322.pdf.json
through O
prototyping O
, O
this O
prompt O
time O
and O
frequency O
were O
found O
to O
give O
reliable O
indicators O
of O
the O
previous O
day O
’ O
s O
mood O
. O

section 4
id pdf2json/2021.acl-long.322.pdf.json
from O
this O
window O
, O
we O
extracted O
the O
following O
features O
to O
characterize O
and O
contextualize O
typed O
text O
. O

section 4
id pdf2json/2021.acl-long.322.pdf.json
text O
: O
after O
removing O
stop-words O
, O
we O
collected O
the O
top O
1000 O
words O
( O
out O
of O
approximately O
3.2 O
million O
) O
used O
across O
all O
users O
in O
our O
dataset O
and O
created O
a O
bag-of-words O
feature O
that O
contains O
the O
daily O
number O
of O
occurrences O
of O
each O
word O
. O

section 4
id pdf2json/2021.acl-long.322.pdf.json
keystrokes O
: O
we O
also O
extracted O
keystroke O
features O
that O
record O
the O
exact O
timing O
that O
each O
character O
was O
typed O
on O
a O
mobile O
keyboard O
( O
including O
alphanumeric O
characters O
, O
special O
characters O
, O
spaces O
, O
backspace O
, O
enter O
, O
and O
autocorrect O
) O
. O

section 4
id pdf2json/2021.acl-long.322.pdf.json
by O
taking O
the O
increase O
in O
recorded O
timing O
after O
each O
keystroke O
, O
we O
obtain O
the O
duration O
that O
each O
key O
was O
pressed O
in O
a O
sequence O
of O
keystrokes O
during O
the O
day O
. O

section 4
id pdf2json/2021.acl-long.322.pdf.json
when O
extracting O
keystrokes O
, O
we O
removed O
all O
small O
timings O
under O
10−2 O
seconds O
. O

section 4
id pdf2json/2021.acl-long.322.pdf.json
app O
usage O
: O
we O
count O
the O
number O
of O
mobile O
applications O
used O
per O
day O
, O
creating O
a O
bag-of-apps O
feature O
for O
each O
day O
. O

section 4
id pdf2json/2021.acl-long.322.pdf.json
we O
discard O
applications O
that O
are O
used O
by O
less O
than O
10 O
% O
of O
the O
participants O
so O
that O
our O
features O
are O
generalizable O
to O
more O
than O
just O
a O
single O
user O
in O
the O
dataset O
, O
resulting O
in O
137 O
total O
apps O
( O
out O
of O
the O
original O
640 O
) O
. O

section 4
id pdf2json/2021.acl-long.322.pdf.json
in O
a O
preliminary O
analysis O
, O
we O
observed O
that O
predictive O
models O
performed O
well O
when O
binarizing O
our O
feature O
vectors O
into O
boolean O
vectors O
, O
which O
signify O
whether O
a O
word O
or O
app O
was O
used O
on O
a O
given O
day O
( O
i.e. O
, O
mapping O
values O
greater O
than O
0 O
to O
1 O
) O
. O

section 4
id pdf2json/2021.acl-long.322.pdf.json
our O
final O
feature O
vectors O
consist O
of O
a O
concatenation O
of O
a O
normalized O
and O
a O
binarized O
feature O
vector O
, O
resulting O
in O
2000 O
and O
274-dimensional O
vectors O
for O
text O
and O
app O
features O
respectively O
. O

section 4
id pdf2json/2021.acl-long.322.pdf.json
for O
keystrokes O
, O
we O
found O
that O
summarizing O
the O
sequence O
of O
timings O
using O
a O
histogram O
( O
i.e. O
, O
defining O
a O
set O
of O
timing O
buckets O
and O
creating O
a O
bag-of-timings O
feature O
) O
for O
each O
day O
performed O
well O
. O

section 4
id pdf2json/2021.acl-long.322.pdf.json
we O
chose O
100 O
fine-grained O
buckets O
, O
resulting O
in O
a O
100-dimensional O
keystroke O
vector O
. O

section 4
id pdf2json/2021.acl-long.322.pdf.json
please O
refer O
to O
appendix O
b O
for O
additional O
details O
about O
the O
dataset O
and O
extracted O
features O
. O

section 5
id pdf2json/2021.acl-long.322.pdf.json
in O
this O
paper O
, O
we O
focus O
on O
studying O
approaches O
for O
learning O
privacy-preserving O
representations O
from O
mobile O
data O
for O
mood O
prediction O
. O

section 5
id pdf2json/2021.acl-long.322.pdf.json
our O
processed O
data O
comes O
in O
the O
form O
of O
{ O
( O
xt O
, O
i O
, O
xk O
, O
i O
, O
xa O
, O
i O
, O
yi O
) O
} O
ni=1 O
with O
xt O
∈ O
n|vt|=2000 O
denoting O
the O
bag-of-words O
features O
, O
xk O
∈ O
n|vk|=100 O
denoting O
the O
bag-oftimings O
features O
, O
and O
xa O
∈ O
n|va|=274 O
denoting O
the O
bag-of-apps O
features O
. O

section 5
id pdf2json/2021.acl-long.322.pdf.json
y O
denotes O
the O
label O
which O
takes O
on O
one O
of O
our O
3 O
mood O
categories O
: O
negative O
, O
neutral O
, O
and O
positive O
. O

section 5
id pdf2json/2021.acl-long.322.pdf.json
in O
parallel O
, O
we O
also O
have O
data O
representing O
the O
corresponding O
( O
one-hot O
) O
user O
identity O
xid O
which O
will O
be O
useful O
when O
learning O
privacypreserving O
representations O
that O
do O
not O
encode O
information O
about O
user O
identity O
xid O
and O
evaluating O
privacy O
performance O
. O

section 6
id pdf2json/2021.acl-long.322.pdf.json
we O
considered O
two O
unimodal O
baselines O
: O
1 O
. O

section 6
id pdf2json/2021.acl-long.322.pdf.json
support O
vector O
machines O
( O
svms O
) O
project O
training O
examples O
to O
a O
chosen O
kernel O
space O
and O
finds O
the O
optimal O
hyperplane O
that O
maximally O
separates O
each O
class O
of O
instances O
. O

section 6
id pdf2json/2021.acl-long.322.pdf.json
we O
apply O
an O
svm O
classifier O
on O
input O
data O
xuni O
∈ O
{ O
xt O
, O
xk O
, O
xa O
} O
and O
use O
supervised O
learning O
to O
predict O
daily O
mood O
labels O
y O
. O

section 6
id pdf2json/2021.acl-long.322.pdf.json
2 O
. O

section 6
id pdf2json/2021.acl-long.322.pdf.json
multilayer O
perceptrons O
( O
mlps O
) O
have O
seen O
widespread O
success O
in O
supervised O
prediction O
tasks O
due O
to O
their O
ability O
in O
modeling O
complex O
nonlinear O
relationships O
. O

section 6
id pdf2json/2021.acl-long.322.pdf.json
because O
of O
the O
small O
size O
of O
our O
dataset O
, O
we O
choose O
a O
simple O
multilayer O
perceptron O
with O
two O
hidden O
layers O
. O

section 6
id pdf2json/2021.acl-long.322.pdf.json
similarly O
, O
we O
apply O
an O
mlp O
classifier O
on O
input O
data O
xuni O
∈ O
{ O
xt O
, O
xk O
, O
xa O
} O
to O
predict O
daily O
mood O
labels O
y O
. O

section 7
id pdf2json/2021.acl-long.322.pdf.json
we O
extend O
both O
svm O
and O
mlp O
classifiers O
using O
early O
fusion O
( O
baltrušaitis O
et O
al. O
, O
2018 O
) O
of O
text O
and O
app O
usage O
to O
model O
multimodal O
interactions O
. O

section 7
id pdf2json/2021.acl-long.322.pdf.json
specifically O
, O
we O
align O
the O
input O
through O
concatenating O
the O
bag-of-words O
, O
bag-of-keystrokes O
, O
and O
bag-of-apps O
features O
for O
each O
day O
resulting O
in O
an O
input O
vector O
xmulti O
= O
xt⊕xk⊕xa O
, O
before O
using O
an O
svm/mlp O
classifier O
for O
prediction O
. O

section 8
id pdf2json/2021.acl-long.322.pdf.json
while O
classifiers O
trained O
with O
traditional O
supervised O
learning O
can O
learn O
useful O
representations O
for O
mood O
prediction O
, O
they O
carry O
the O
risk O
of O
memorizing O
the O
identity O
of O
the O
user O
along O
with O
their O
sensitive O
mobile O
usage O
and O
baseline O
mood O
scores O
, O
and O
possibly O
revealing O
these O
identities O
to O
adversarial O
thirdparties O
( O
abadi O
et O
al. O
, O
2016 O
) O
. O

section 8
id pdf2json/2021.acl-long.322.pdf.json
therefore O
, O
it O
is O
crucial O
to O
perform O
mood O
prediction O
while O
also O
protecting O
the O
privacy O
of O
personal O
identities O
. O

section 8
id pdf2json/2021.acl-long.322.pdf.json
we O
adapt O
the O
selective-additive O
learning O
( O
sal O
) O
framework O
( O
wang O
et O
al. O
, O
2017 O
) O
for O
the O
purpose O
of O
privacy-preserving O
learning O
. O

section 8
id pdf2json/2021.acl-long.322.pdf.json
while O
sal O
was O
originally O
developed O
with O
a O
very O
different O
goal O
in O
mind O
: O
improving O
model O
generalization O
, O
we O
expand O
sal O
to O
a O
very O
important O
problem O
in O
health- O
care O
: O
preserving O
privacy O
. O

section 8
id pdf2json/2021.acl-long.322.pdf.json
we O
adapted O
sal O
to O
learn O
disentangled O
representations O
separated O
into O
identity-dependent O
private O
information O
and O
identityindependent O
population-level O
information O
using O
three O
phases O
: O
( O
1 O
) O
pretrain O
phase O
: O
the O
input O
is O
a O
set O
of O
( O
multimodal O
) O
features O
x O
that O
are O
likely O
to O
contain O
both O
identity-dependent O
and O
independent O
information O
. O

section 8
id pdf2json/2021.acl-long.322.pdf.json
the O
intermediate O
representation O
zfeat O
= O
ffeat O
( O
x O
; O
θ O
∗ O
feat O
) O
is O
obtained O
from O
an O
mlp O
classifier O
pretrained O
for O
mood O
prediction O
. O

section 8
id pdf2json/2021.acl-long.322.pdf.json
ffeat O
denotes O
the O
classifier O
with O
pretrained O
parameters O
θ∗feat O
. O

section 8
id pdf2json/2021.acl-long.322.pdf.json
( O
2 O
) O
selection O
phase O
: O
our O
goal O
is O
to O
now O
disentangle O
the O
identity-dependent O
and O
independent O
information O
within O
zfeat O
. O

section 8
id pdf2json/2021.acl-long.322.pdf.json
we O
hypothesize O
that O
dependent O
and O
independent O
information O
are O
encoded O
in O
separate O
subspaces O
of O
the O
feature O
vector O
zfeat O
. O

section 8
id pdf2json/2021.acl-long.322.pdf.json
this O
allows O
us O
to O
disentangle O
them O
by O
training O
a O
separate O
classifier O
to O
predict O
zfeat O
as O
much O
as O
possible O
given O
only O
the O
user O
identity O
: O
θ∗id O
= O
argmin O
θid O
( O
zfeat O
− O
fid O
( O
xid O
; O
θid O
) O
) O
2 O
+ O
λ||zid||1 O
, O
( O
1 O
) O
where O
xid O
denotes O
a O
one O
hot O
encoding O
of O
user O
identity O
as O
input O
, O
fid O
denotes O
the O
identity O
encoder O
with O
parameters O
θid O
, O
and O
λ O
denotes O
a O
hyperparameter O
that O
controls O
the O
weight O
of O
the O
` O
1 O
regularizer O
. O

section 8
id pdf2json/2021.acl-long.322.pdf.json
fid O
projects O
the O
user O
identity O
encodings O
to O
the O
feature O
space O
learned O
by O
ffeat O
. O

section 8
id pdf2json/2021.acl-long.322.pdf.json
by O
minimizing O
the O
objective O
in O
equation O
( O
1 O
) O
for O
each O
( O
x O
, O
xid O
) O
pair O
, O
fid O
learns O
to O
encode O
user O
identity O
into O
a O
sparse O
vector O
zid O
= O
fid O
( O
xid O
; O
θ O
∗ O
id O
) O
representing O
identity-dependent O
features O
: O
the O
nonzero O
values O
of O
zid O
represent O
dimensions O
of O
the O
identity-dependent O
subspace O
in O
zfeat O
, O
while O
the O
remaining O
dimensions O
belong O
to O
the O
identity-independent O
subspace O
. O

section 8
id pdf2json/2021.acl-long.322.pdf.json
( O
3 O
) O
addition O
phase O
: O
given O
two O
factors O
zfeat O
and O
zid O
, O
to O
ensure O
that O
our O
prediction O
model O
does O
not O
capture O
identity-related O
information O
zid O
, O
we O
add O
multiplicative O
gaussian O
noise O
to O
remove O
information O
from O
the O
identity-related O
subspace O
zid O
while O
repeatedly O
optimizing O
for O
mood O
prediction O
with O
a O
final O
mlp O
classification O
layer O
g O
( O
zfeat O
, O
zid O
; O
δ O
) O
. O

section 8
id pdf2json/2021.acl-long.322.pdf.json
this O
resulting O
model O
should O
only O
retain O
identity-independent O
features O
for O
mood O
prediction O
: O
ŷ O
= O
g O
( O
zfeat O
+ O
zid O
) O
( O
2 O
) O
where O
∼ O
n O
( O
0 O
, O
σ2 O
) O
is O
repeatedly O
sampled O
across O
batches O
and O
training O
epochs O
. O

section 8
id pdf2json/2021.acl-long.322.pdf.json
we O
call O
this O
approach O
noisy O
identity O
mlp O
, O
or O
ni-mlp O
for O
short O
, O
and O
summarize O
the O
final O
algorithm O
in O
figure O
2 O
. O

section 8
id pdf2json/2021.acl-long.322.pdf.json
controlling O
the O
tradeoff O
between O
performance O
and O
privacy O
: O
there O
is O
often O
a O
tradeoff O
between O
privacy O
and O
prediction O
performance O
. O

section 8
id pdf2json/2021.acl-long.322.pdf.json
to O
control O
this O
tradeoff O
, O
we O
vary O
the O
parameter O
σ O
, O
which O
is O
the O
variance O
of O
noise O
added O
to O
the O
identity-dependent O
subspace O
across O
batches O
and O
training O
epochs O
. O

section 8
id pdf2json/2021.acl-long.322.pdf.json
σ O
= O
0 O
recovers O
a O
standard O
mlp O
with O
good O
performance O
but O
reveals O
user O
identities O
, O
while O
large O
σ O
effectively O
protects O
user O
identities O
but O
at O
the O
possible O
expense O
of O
mood O
prediction O
performance O
. O

section 8
id pdf2json/2021.acl-long.322.pdf.json
in O
practice O
, O
the O
optimal O
tradeoff O
between O
privacy O
and O
performance O
varies O
depending O
on O
the O
problem O
. O

section 8
id pdf2json/2021.acl-long.322.pdf.json
for O
our O
purposes O
, O
we O
automatically O
perform O
model O
selection O
using O
this O
performance-privacy O
ratio O
r O
computed O
on O
the O
validation O
set O
, O
where O
r O
= O
smlp O
− O
sni-mlp O
tmlp O
− O
tni-mlp O
( O
3 O
) O
is O
defined O
as O
the O
improvement O
in O
privacy O
per O
unit O
of O
performance O
lost O
. O

section 8
id pdf2json/2021.acl-long.322.pdf.json
here O
, O
s O
is O
defined O
as O
the O
accuracy O
in O
user O
prediction O
and O
t O
is O
defined O
as O
the O
f1 O
score O
on O
mood O
prediction O
. O

section 9
id pdf2json/2021.acl-long.322.pdf.json
we O
perform O
experiments O
to O
test O
the O
utility O
of O
text O
, O
keystroke O
, O
and O
app O
features O
in O
predicting O
daily O
mood O
while O
keeping O
user O
privacy O
in O
mind O
. O

section 10
id pdf2json/2021.acl-long.322.pdf.json
data O
splits O
: O
given O
that O
our O
data O
is O
longitudinal O
, O
we O
split O
our O
data O
into O
10 O
partitions O
ordered O
chronologically O
by O
users O
. O

section 10
id pdf2json/2021.acl-long.322.pdf.json
we O
do O
so O
in O
order O
to O
maintain O
independence O
between O
the O
train O
, O
validation O
, O
and O
test O
splits O
in O
the O
case O
where O
there O
is O
some O
form O
of O
time-level O
dependency O
within O
our O
labels O
. O

section 10
id pdf2json/2021.acl-long.322.pdf.json
evaluation O
: O
for O
each O
model O
, O
we O
run O
a O
nested O
kfold O
cross-validation O
( O
i.e. O
, O
we O
perform O
9-fold O
validation O
within O
10-fold O
testing O
) O
. O

section 10
id pdf2json/2021.acl-long.322.pdf.json
for O
each O
test O
fold O
, O
we O
identify O
the O
optimal O
parameter O
set O
as O
the O
one O
that O
achieves O
the O
highest O
mean O
validation O
score O
over O
the O
validation O
folds O
. O

section 10
id pdf2json/2021.acl-long.322.pdf.json
to O
evaluate O
ni-mlp O
, O
we O
use O
the O
best O
performing O
mlp O
model O
for O
each O
test O
fold O
as O
our O
base O
classifier O
before O
performing O
privacypreserving O
learning O
. O

section 10
id pdf2json/2021.acl-long.322.pdf.json
for O
all O
experiments O
, O
we O
report O
the O
test O
accuracy O
and O
macro O
f1 O
score O
because O
our O
classes O
are O
imbalanced O
. O

section 10
id pdf2json/2021.acl-long.322.pdf.json
given O
the O
low O
number O
of O
cross-validation O
folds O
, O
we O
use O
the O
wilcoxon O
signedrank O
test O
( O
wilcoxon O
, O
1992 O
) O
at O
5 O
% O
significance O
level O
for O
all O
statistical O
comparisons O
( O
see O
appendix O
c O
for O
more O
experimental O
details O
) O
. O

section 11
id pdf2json/2021.acl-long.322.pdf.json
we O
make O
the O
following O
observations O
regarding O
the O
learned O
language O
and O
multimodal O
representations O
for O
mood O
prediction O
: O
observation O
1 O
: O
text O
, O
keystroke O
, O
and O
app O
usage O
features O
are O
individually O
predictive O
of O
mood O
. O

section 11
id pdf2json/2021.acl-long.322.pdf.json
to O
evaluate O
how O
predictive O
our O
extracted O
text O
, O
keystroke O
timings O
, O
and O
app O
usage O
features O
are O
, O
we O
first O
run O
experiments O
using O
svm O
, O
mlp O
, O
and O
nimlp O
on O
each O
individual O
feature O
separately O
. O

section 11
id pdf2json/2021.acl-long.322.pdf.json
since O
we O
have O
unbalanced O
classes O
, O
we O
chose O
a O
majority O
classifier O
( O
i.e. O
, O
most O
common O
class O
in O
the O
training O
set O
) O
as O
our O
baseline O
. O

section 11
id pdf2json/2021.acl-long.322.pdf.json
from O
table O
1 O
, O
we O
observe O
that O
using O
these O
three O
feature O
types O
individually O
outperforms O
the O
baseline O
with O
respect O
to O
accuracy O
and O
f1 O
score O
. O

section 11
id pdf2json/2021.acl-long.322.pdf.json
using O
the O
wilcoxon O
signed-rank O
test O
( O
wilcoxon O
, O
1992 O
) O
at O
5 O
% O
significance O
level O
, O
we O
found O
that O
these O
improvements O
over O
the O
baseline O
in O
both O
f1 O
score O
and O
accuracy O
are O
statistically O
significant O
( O
p-value O
< O
< O
0.05 O
) O
. O

section 11
id pdf2json/2021.acl-long.322.pdf.json
observation O
2 O
: O
pretrained O
sentence O
encoders O
struggle O
on O
this O
task O
. O

section 11
id pdf2json/2021.acl-long.322.pdf.json
we O
also O
applied O
pretrained O
sentence O
encoders O
such O
as O
bert O
( O
devlin O
et O
al. O
, O
2019 O
) O
on O
the O
language O
modality O
for O
mood O
prediction O
. O

section 11
id pdf2json/2021.acl-long.322.pdf.json
surprisingly O
, O
we O
found O
that O
none O
of O
these O
approaches O
performed O
stronger O
than O
a O
simple O
bagof-words O
( O
see O
table O
2 O
) O
. O

section 11
id pdf2json/2021.acl-long.322.pdf.json
we O
provide O
two O
possible O
explanations O
for O
this O
phenomenon O
: O
1 O
. O

section 11
id pdf2json/2021.acl-long.322.pdf.json
bert O
is O
suitable O
for O
written O
text O
on O
the O
web O
( O
wikipedia O
, O
bookcorpus O
, O
carefully O
humanannotated O
datasets O
) O
which O
may O
not O
generalize O
to O
informal O
typed O
text O
that O
contains O
emojis O
, O
typos O
, O
and O
abbreviations O
( O
see O
section O
4.4 O
for O
a O
qualitative O
analysis O
regarding O
the O
predictive O
abilities O
of O
emojis O
and O
keystrokes O
for O
mood O
prediction O
) O
. O

section 11
id pdf2json/2021.acl-long.322.pdf.json
2 O
. O

section 11
id pdf2json/2021.acl-long.322.pdf.json
we O
hypothesize O
that O
it O
is O
difficult O
to O
capture O
such O
long O
sequences O
of O
data O
( O
> O
1000 O
time O
steps O
) O
spread O
out O
over O
a O
day O
. O

section 11
id pdf2json/2021.acl-long.322.pdf.json
current O
work O
has O
shown O
that O
bert O
struggles O
with O
long O
sequence O
lengths O
( O
beltagy O
et O
al. O
, O
2020 O
) O
. O

section 11
id pdf2json/2021.acl-long.322.pdf.json
we O
trained O
two O
extensions O
xlnet O
( O
yang O
et O
al. O
, O
2019 O
) O
and O
longformer O
( O
beltagy O
et O
al. O
, O
2020 O
) O
specifically O
designed O
to O
take O
in O
long-range O
context O
but O
found O
that O
they O
still O
underperform O
as O
compared O
to O
a O
simple O
bag-of-words O
approach O
. O

section 11
id pdf2json/2021.acl-long.322.pdf.json
observation O
3 O
: O
fusing O
both O
text O
and O
keystroke O
timings O
improves O
performance O
. O

section 11
id pdf2json/2021.acl-long.322.pdf.json
this O
dataset O
presents O
a O
unique O
opportunity O
to O
study O
representations O
of O
typed O
text O
as O
an O
alternative O
to O
conventionally O
studied O
written O
or O
spoken O
text O
. O

section 11
id pdf2json/2021.acl-long.322.pdf.json
while O
the O
latter O
two O
use O
language O
alone O
, O
typed O
text O
includes O
keystroke O
features O
providing O
information O
about O
the O
timings O
of O
when O
each O
character O
was O
typed O
. O

section 11
id pdf2json/2021.acl-long.322.pdf.json
in O
table O
1 O
, O
we O
present O
some O
of O
our O
initial O
results O
in O
learning O
text O
and O
keystroke O
representations O
for O
mood O
prediction O
and O
show O
consistent O
improvements O
over O
text O
alone O
. O

section 11
id pdf2json/2021.acl-long.322.pdf.json
we O
further O
study O
the O
uniqueness O
of O
typed O
text O
by O
comparing O
the O
following O
baselines O
: O
1 O
. O

section 11
id pdf2json/2021.acl-long.322.pdf.json
text O
: O
bag-of-words O
only O
. O

section 11
id pdf2json/2021.acl-long.322.pdf.json
2 O
. O

section 11
id pdf2json/2021.acl-long.322.pdf.json
text O
+ O
char O
keystrokes O
: O
bag-of-words O
and O
bagof-timings O
across O
all O
characters O
. O

section 11
id pdf2json/2021.acl-long.322.pdf.json
3 O
. O

section 11
id pdf2json/2021.acl-long.322.pdf.json
text O
+ O
split O
char O
keystrokes O
: O
bag-of-words O
and O
bag-of-timings O
subdivided O
between O
6 O
groups O
: O
alphanumeric O
characters O
, O
symbols O
, O
spacebar O
, O
enter O
, O
delete O
, O
and O
use O
of O
autocorrect O
. O

section 11
id pdf2json/2021.acl-long.322.pdf.json
this O
baseline O
presents O
a O
more O
fine-grained O
decomposition O
of O
the O
typing O
speeds O
across O
different O
semantically O
related O
character O
groups O
. O

section 11
id pdf2json/2021.acl-long.322.pdf.json
4 O
. O

section 11
id pdf2json/2021.acl-long.322.pdf.json
text O
+ O
word O
keystrokes O
: O
bag-of-words O
and O
bagof-timings O
summed O
up O
over O
the O
characters O
in O
each O
word O
. O

section 11
id pdf2json/2021.acl-long.322.pdf.json
this O
presents O
a O
more O
interpretable O
model O
to O
analyze O
the O
relationships O
between O
words O
and O
the O
distribution O
of O
their O
typing O
speeds O
. O

section 11
id pdf2json/2021.acl-long.322.pdf.json
from O
table O
3 O
, O
we O
observe O
that O
keystrokes O
accurately O
contextualize O
text O
, O
especially O
when O
using O
fine-grained O
keystroke O
distributions O
across O
individual O
characters O
. O

section 11
id pdf2json/2021.acl-long.322.pdf.json
other O
methods O
incorporating O
keystroke O
features O
are O
also O
all O
stronger O
than O
unimodal O
models O
. O

section 11
id pdf2json/2021.acl-long.322.pdf.json
different O
ways O
of O
representing O
keystrokes O
also O
provide O
different O
levels O
of O
interpretability O
regarding O
the O
relationships O
between O
words O
, O
characters O
, O
and O
keystrokes O
for O
mood O
prediction O
, O
which O
we O
qualitatively O
analyze O
in O
§4.4 O
. O

section 11
id pdf2json/2021.acl-long.322.pdf.json
observation O
4 O
: O
multimodal O
representation O
learning O
achieves O
the O
best O
performance O
. O

section 11
id pdf2json/2021.acl-long.322.pdf.json
in O
table O
1 O
, O
we O
also O
compare O
the O
performance O
of O
our O
models O
on O
combined O
( O
text O
+ O
keystroke O
+ O
apps O
) O
features O
versus O
the O
performance O
on O
each O
individual O
feature O
set O
. O

section 11
id pdf2json/2021.acl-long.322.pdf.json
for O
both O
metrics O
, O
combining O
all O
features O
gives O
better O
performance O
over O
either O
subset O
. O

section 12
id pdf2json/2021.acl-long.322.pdf.json
despite O
these O
promising O
results O
in O
mood O
prediction O
, O
we O
ask O
an O
important O
question O
: O
does O
the O
model O
capture O
user O
identities O
as O
an O
intermediate O
step O
towards O
predicting O
mood O
? O

section 12
id pdf2json/2021.acl-long.322.pdf.json
to O
answer O
this O
question O
, O
we O
an- O
alyze O
the O
privacy O
of O
raw O
mobile O
data O
and O
trained O
models O
. O

section 12
id pdf2json/2021.acl-long.322.pdf.json
we O
then O
study O
our O
proposed O
method O
of O
learning O
privacy-preserving O
features O
to O
determine O
whether O
it O
can O
obfuscate O
user O
identity O
while O
remaining O
predictive O
of O
daily O
mood O
. O

section 12
id pdf2json/2021.acl-long.322.pdf.json
how O
private O
is O
the O
mobile O
data O
? O

section 12
id pdf2json/2021.acl-long.322.pdf.json
we O
evaluate O
how O
much O
the O
data O
reveal O
user O
identities O
by O
training O
predictive O
models O
with O
typed O
text O
, O
keystroke O
timings O
, O
and O
app O
usage O
as O
input O
and O
user O
identity O
as O
the O
prediction O
target O
. O

section 12
id pdf2json/2021.acl-long.322.pdf.json
from O
table O
4 O
, O
we O
observe O
that O
all O
modalities O
are O
very O
predictive O
of O
user O
identity O
( O
> O
87 O
% O
accuracy O
) O
, O
which O
further O
motivates O
the O
need O
to O
learn O
privacy-preserving O
features O
. O

section 12
id pdf2json/2021.acl-long.322.pdf.json
we O
further O
note O
that O
identifiable O
information O
can O
be O
very O
subtle O
: O
while O
only O
28/1000 O
words O
were O
named O
entities O
, O
it O
was O
possible O
to O
identify O
the O
user O
identity O
with O
> O
87 O
% O
accuracy O
, O
which O
means O
that O
subtle O
word O
choice O
can O
be O
identify O
the O
user O
( O
similarly O
for O
apps O
and O
keystrokes O
) O
. O

section 12
id pdf2json/2021.acl-long.322.pdf.json
how O
private O
are O
the O
learned O
privacy-preserving O
features O
? O

section 12
id pdf2json/2021.acl-long.322.pdf.json
we O
also O
study O
whether O
our O
learned O
features O
are O
correlated O
with O
user O
identity O
through O
both O
visualizations O
and O
quantitative O
evaluations O
. O

section 12
id pdf2json/2021.acl-long.322.pdf.json
visualizations O
: O
we O
use O
t-sne O
( O
van O
der O
maaten O
and O
hinton O
, O
2008 O
) O
to O
reduce O
the O
learned O
features O
from O
trained O
models O
to O
2 O
dimensions O
. O

section 12
id pdf2json/2021.acl-long.322.pdf.json
after O
color-coding O
the O
points O
by O
participant O
identity O
, O
we O
identify O
distinct O
clusters O
in O
figure O
3 O
( O
a O
) O
, O
which O
implies O
that O
mood O
prediction O
can O
be O
strongly O
linked O
to O
identi- O
fying O
the O
person O
, O
therefore O
coming O
at O
the O
price O
of O
losing O
privacy O
. O

section 12
id pdf2json/2021.acl-long.322.pdf.json
as O
an O
attempt O
to O
reduce O
reliance O
on O
user O
identity O
, O
we O
train O
ni-mlp O
which O
is O
designed O
to O
obfuscate O
user-dependent O
features O
. O

section 12
id pdf2json/2021.acl-long.322.pdf.json
after O
training O
ni-mlp O
, O
we O
again O
visualize O
the O
representations O
learned O
in O
figure O
3 O
( O
b O
) O
and O
we O
find O
that O
they O
are O
less O
visually O
separable O
by O
users O
, O
indicating O
that O
ni-mlp O
indeed O
learns O
more O
user-independent O
features O
. O

section 12
id pdf2json/2021.acl-long.322.pdf.json
quantitative O
evaluation O
: O
to O
empirically O
evaluate O
how O
well O
our O
models O
preserve O
privacy O
, O
we O
extracted O
the O
final O
layer O
of O
each O
trained O
model O
and O
fit O
a O
logistic O
regression O
model O
to O
predict O
user O
identity O
using O
these O
final O
layer O
representations O
as O
input O
. O

section 12
id pdf2json/2021.acl-long.322.pdf.json
the O
more O
a O
model O
preserves O
privacy O
, O
the O
harder O
it O
should O
be O
to O
predict O
user O
identity O
. O

section 12
id pdf2json/2021.acl-long.322.pdf.json
from O
table O
5 O
, O
we O
observe O
that O
we O
can O
predict O
user O
identity O
based O
on O
the O
learned O
mlp O
representations O
with O
high O
accuracy O
( O
> O
85 O
% O
) O
using O
the O
most O
sensitive O
app O
usage O
features O
. O

section 12
id pdf2json/2021.acl-long.322.pdf.json
for O
other O
modality O
combinations O
, O
user O
identity O
can O
also O
be O
decoded O
with O
more O
than O
70 O
% O
accuracy O
with O
the O
exception O
of O
keystrokes O
which O
are O
the O
most O
private O
( O
55 O
% O
) O
. O

section 12
id pdf2json/2021.acl-long.322.pdf.json
we O
achieve O
significantly O
more O
privacy O
using O
ni-mlp O
embeddings O
- O
roughly O
35 O
% O
for O
the O
best O
multimodal O
model O
, O
which O
indicates O
the O
possibility O
of O
ni-mlp O
as O
a O
means O
of O
achieving O
privacy-preserving O
mood O
prediction O
. O

section 12
id pdf2json/2021.acl-long.322.pdf.json
understanding O
the O
tradeoff O
between O
performance O
and O
privacy O
: O
ni-mlp O
provides O
a O
tunable O
parameter O
σ O
to O
control O
the O
variance O
of O
noise O
applied O
on O
the O
identity-related O
dimensions O
. O

section 12
id pdf2json/2021.acl-long.322.pdf.json
this O
parameter O
σ O
has O
the O
potential O
to O
give O
a O
tradeoff O
between O
privacy O
and O
prediction O
performance O
. O

section 12
id pdf2json/2021.acl-long.322.pdf.json
in O
figure O
4 O
, O
we O
plot O
this O
tradeoff O
between O
performance O
( O
mood O
prediction O
f1 O
score O
, O
higher O
is O
better O
) O
and O
privacy O
( O
identity O
prediction O
accuracy O
, O
lower O
is O
better O
) O
. O

section 12
id pdf2json/2021.acl-long.322.pdf.json
we O
find O
that O
keystroke O
features O
, O
while O
themselves O
not O
very O
useful O
in O
predicting O
mood O
, O
are O
highly O
private O
features O
. O

section 12
id pdf2json/2021.acl-long.322.pdf.json
it O
is O
important O
to O
note O
that O
keystroke O
features O
show O
strong O
performance O
when O
integrated O
with O
text O
and O
app O
usage O
features O
while O
also O
increasing O
privacy O
, O
thereby O
pushing O
the O
pareto O
front O
outwards O
. O

section 12
id pdf2json/2021.acl-long.322.pdf.json
it O
is O
also O
interesting O
to O
observe O
that O
for O
most O
models O
, O
performance O
stays O
level O
while O
privacy O
improves O
, O
which O
is O
a O
promising O
sign O
for O
the O
real-world O
deployment O
of O
such O
models O
which O
requires O
a O
balance O
between O
both O
desiderata O
. O

section 13
id pdf2json/2021.acl-long.322.pdf.json
to O
further O
shed O
light O
on O
the O
relationships O
between O
mood O
prediction O
performance O
and O
privacy O
, O
we O
performed O
a O
more O
in-depth O
study O
of O
the O
text O
, O
keystroke O
, O
and O
app O
usage O
features O
learned O
by O
the O
model O
( O
see O
appendix O
d.3 O
for O
more O
examples O
) O
. O

section 13
id pdf2json/2021.acl-long.322.pdf.json
understanding O
the O
unimodal O
features O
: O
we O
first O
analyze O
how O
individual O
words O
, O
keystroke O
timings O
, O
and O
app O
usage O
are O
indicative O
of O
positive O
or O
negative O
mood O
for O
different O
users O
. O

section 13
id pdf2json/2021.acl-long.322.pdf.json
text O
: O
we O
find O
that O
several O
words O
are O
particularly O
indicative O
of O
mood O
: O
can O
’ O
t/cant O
, O
don O
’ O
t/don O
’ O
t O
, O
and O
sorry O
are O
negative O
for O
more O
users O
than O
positive O
, O
while O
yes O
is O
overwhelmingly O
positive O
across O
users O
( O
9 O
pos O
, O
1 O
neg O
) O
, O
but O
yeah O
is O
slightly O
negative O
( O
5 O
pos O
, O
7 O
neg O
) O
. O

section 13
id pdf2json/2021.acl-long.322.pdf.json
we O
also O
analyze O
the O
use O
of O
emojis O
in O
typed O
text O
and O
find O
that O
while O
there O
are O
certain O
emojis O
that O
lean O
positive O
( O
e.g. O
, O
) O
, O
there O
are O
ones O
( O
e.g. O
, O
: O
( O
and O
) O
that O
used O
in O
both O
contexts O
depending O
on O
the O
user O
( O
see O
table O
6 O
) O
. O

section 13
id pdf2json/2021.acl-long.322.pdf.json
apps O
: O
in O
table O
7 O
, O
we O
show O
the O
top O
3 O
apps O
associated O
with O
positive O
or O
negative O
moods O
across O
several O
users O
. O

section 13
id pdf2json/2021.acl-long.322.pdf.json
it O
is O
interesting O
to O
observe O
that O
many O
outdoor O
apps O
( O
i.e. O
, O
weather O
, O
myfitnesspal O
, O
uber O
) O
, O
photo O
sharing O
apps O
( O
i.e. O
, O
photos O
, O
snapchat O
) O
, O
and O
calling O
apps O
( O
i.e. O
, O
facetime O
, O
phone O
) O
are O
associated O
with O
positive O
mood O
, O
while O
personal O
apps O
such O
as O
personal O
management O
( O
i.e. O
, O
calendar O
, O
notes O
, O
siri O
) O
, O
web O
browsing O
( O
i.e. O
, O
chrome O
, O
safari O
) O
, O
and O
shopping O
( O
i.e. O
, O
app O
store O
) O
are O
associated O
with O
negative O
mood O
. O

section 13
id pdf2json/2021.acl-long.322.pdf.json
however O
, O
some O
of O
these O
findings O
are O
rather O
userspecific O
( O
e.g. O
, O
phone O
can O
be O
both O
positive O
or O
negative O
depending O
on O
the O
user O
) O
. O

section 13
id pdf2json/2021.acl-long.322.pdf.json
understanding O
the O
multimodal O
features O
: O
we O
also O
analyze O
how O
the O
same O
characters O
and O
words O
can O
contribute O
to O
different O
mood O
predictions O
based O
on O
their O
keystroke O
patterns O
. O

section 13
id pdf2json/2021.acl-long.322.pdf.json
as O
an O
example O
, O
the O
distribution O
of O
keystrokes O
for O
the O
enter O
character O
on O
the O
keyboard O
differs O
according O
to O
the O
daily O
mood O
of O
one O
user O
( O
see O
figure O
5 O
and O
appendix O
d.3 O
for O
more O
users O
) O
. O

section 13
id pdf2json/2021.acl-long.322.pdf.json
in O
table O
8 O
, O
we O
extend O
this O
analysis O
to O
entire O
words O
. O

section 13
id pdf2json/2021.acl-long.322.pdf.json
for O
each O
of O
the O
500 O
most O
common O
words O
, O
we O
aggregated O
their O
accompanying O
keystroke O
timings O
for O
user-reported O
positive O
and O
negative O
mood O
. O

section 13
id pdf2json/2021.acl-long.322.pdf.json
these O
two O
distributions O
tell O
us O
how O
the O
same O
word O
in O
different O
keystroke O
contexts O
can O
indicate O
different O
moods O
. O

section 13
id pdf2json/2021.acl-long.322.pdf.json
we O
performed O
wilcoxon O
rank-sum O
tests O
at O
5 O
% O
significance O
level O
to O
compare O
these O
distributions O
and O
recorded O
the O
words O
in O
which O
either O
faster O
or O
slower O
typing O
was O
statistically O
significantly O
correlated O
with O
either O
mood O
. O

section 13
id pdf2json/2021.acl-long.322.pdf.json
observe O
how O
certain O
semantically O
positive O
words O
like O
love O
, O
thank O
, O
and O
haha O
become O
judged O
as O
more O
positive O
when O
typed O
at O
a O
faster O
speed O
. O

section 13
id pdf2json/2021.acl-long.322.pdf.json
therefore O
, O
contextualizing O
text O
with O
their O
keystroke O
timings O
offers O
additional O
information O
when O
learning O
representations O
of O
typed O
text O
. O

section 14
id pdf2json/2021.acl-long.322.pdf.json
in O
this O
paper O
, O
we O
investigated O
the O
learning O
of O
language O
and O
multimodal O
representations O
of O
typed O
text O
collected O
from O
mobile O
data O
. O

section 14
id pdf2json/2021.acl-long.322.pdf.json
we O
studied O
the O
challenge O
of O
learning O
markers O
of O
daily O
mood O
as O
a O
step O
towards O
early O
detection O
and O
intervention O
of O
mental O
health O
disorders O
for O
social O
good O
. O

section 14
id pdf2json/2021.acl-long.322.pdf.json
our O
method O
also O
shows O
promising O
results O
in O
obfuscating O
user O
identities O
for O
privacy-preserving O
learning O
, O
a O
direction O
crucial O
towards O
real-world O
learning O
from O
sensitive O
mobile O
data O
and O
healthcare O
labels O
. O

section 14
id pdf2json/2021.acl-long.322.pdf.json
in O
addition O
, O
our O
findings O
illustrate O
several O
challenges O
and O
opportunities O
in O
representation O
learning O
from O
typed O
text O
as O
an O
understudied O
area O
in O
nlp O
. O

section 14
id pdf2json/2021.acl-long.322.pdf.json
limitations O
& O
future O
work O
: O
while O
our O
approach O
shows O
promises O
in O
learning O
representations O
for O
mood O
prediction O
, O
several O
future O
directions O
on O
the O
modeling O
and O
nlp O
side O
include O
: O
1 O
) O
better O
models O
and O
pre-training O
algorithms O
for O
nlp O
on O
typed O
text O
, O
2 O
) O
algorithms O
that O
provide O
formal O
guarantees O
of O
privacy O
( O
dwork O
, O
2008 O
) O
, O
and O
3 O
) O
federated O
training O
from O
decentralized O
data O
( O
mcmahan O
et O
al. O
, O
2016 O
) O
to O
improve O
privacy O
( O
geyer O
et O
al. O
, O
2017 O
) O
and O
fairness O
( O
liang O
et O
al. O
, O
2020a O
) O
of O
sensitive O
data O
. O

section 14
id pdf2json/2021.acl-long.322.pdf.json
we O
describe O
more O
limitations O
and O
future O
social O
implications O
of O
our O
work O
in O
our O
broader O
impact O
statement O
in O
appendix O
a O
. O

section 15
id pdf2json/2021.acl-long.322.pdf.json
this O
material O
was O
based O
upon O
work O
partially O
supported O
by O
the O
national O
science O
foundation O
( O
awards O
# O
1750439 O
and O
# O
1734868 O
) O
and O
the O
national O
institutes O
of O
health O
( O
award O
# O
u01mh116923 O
) O
. O

section 15
id pdf2json/2021.acl-long.322.pdf.json
mm O
was O
supported O
by O
the O
swiss O
national O
science O
foundation O
( O
# O
p2gep2_184518 O
) O
. O

section 15
id pdf2json/2021.acl-long.322.pdf.json
rs O
was O
supported O
by O
nsf O
iis1763562 O
and O
onr O
grant O
n000141812861 O
. O

section 15
id pdf2json/2021.acl-long.322.pdf.json
any O
opinions O
, O
findings O
, O
and O
conclusions O
, O
or O
recommendations O
expressed O
in O
this O
material O
are O
those O
of O
the O
author O
( O
s O
) O
and O
do O
not O
necessarily O
reflect O
the O
views O
of O
the O
national O
science O
foundation O
, O
national O
institutes O
of O
health O
, O
or O
office O
of O
naval O
research O
, O
and O
no O
official O
endorsement O
should O
be O
inferred O
. O

section 15
id pdf2json/2021.acl-long.322.pdf.json
we O
would O
also O
like O
to O
acknowledge O
nvidia O
’ O
s O
gpu O
support O
and O
the O
anonymous O
reviewers O
for O
their O
extremely O
helpful O
comments O
. O

section 16
id pdf2json/2021.acl-long.322.pdf.json
learning O
markers O
of O
mood O
from O
mobile O
data O
presents O
an O
opportunity O
for O
large-scale O
adaptive O
interventions O
of O
suicidal O
ideation O
. O

section 16
id pdf2json/2021.acl-long.322.pdf.json
however O
, O
there O
are O
important O
concerns O
regarding O
its O
implications O
to O
society O
and O
policy O
. O

section 16
id pdf2json/2021.acl-long.322.pdf.json
applications O
in O
mental O
health O
: O
suicide O
is O
the O
second O
leading O
cause O
of O
death O
among O
adolescents O
. O

section 16
id pdf2json/2021.acl-long.322.pdf.json
in O
addition O
to O
deaths O
, O
16 O
% O
of O
high O
school O
students O
report O
seriously O
considering O
suicide O
each O
year O
, O
and O
8 O
% O
make O
one O
or O
more O
suicide O
attempts O
( O
cdc O
, O
2015 O
) O
. O

section 16
id pdf2json/2021.acl-long.322.pdf.json
despite O
these O
alarming O
statistics O
, O
there O
is O
little O
consensus O
concerning O
imminent O
risk O
for O
suicide O
( O
franklin O
et O
al. O
, O
2017 O
; O
large O
et O
al. O
, O
2017 O
) O
. O

section 16
id pdf2json/2021.acl-long.322.pdf.json
current O
research O
conducts O
clinical O
interviews O
and O
patient O
self-report O
questionnaires O
that O
provide O
longterm O
assessments O
of O
suicide O
risk O
. O

section 16
id pdf2json/2021.acl-long.322.pdf.json
however O
, O
few O
studies O
have O
focused O
on O
imminent O
suicidal O
risk O
, O
which O
is O
of O
critical O
clinical O
importance O
as O
a O
step O
towards O
adaptive O
real-time O
interventions O
( O
glenn O
and O
nock O
, O
2014 O
; O
schuck O
et O
al. O
, O
2019 O
) O
. O

section 16
id pdf2json/2021.acl-long.322.pdf.json
given O
the O
impact O
of O
suicide O
on O
society O
, O
there O
is O
an O
urgent O
need O
to O
better O
understand O
the O
behavior O
markers O
related O
to O
suicidal O
ideation O
. O

section 16
id pdf2json/2021.acl-long.322.pdf.json
“ O
just-in-time O
” O
adaptive O
interventions O
delivered O
via O
mobile O
health O
applications O
provide O
a O
platform O
of O
exciting O
developments O
in O
low-intensity O
, O
high-impact O
interventions O
( O
nahum-shani O
et O
al. O
, O
2018 O
) O
. O

section 16
id pdf2json/2021.acl-long.322.pdf.json
the O
ability O
to O
intervene O
precisely O
during O
an O
acute O
risk O
for O
suicide O
could O
dramatically O
reduce O
the O
loss O
of O
life O
. O

section 16
id pdf2json/2021.acl-long.322.pdf.json
to O
realize O
this O
goal O
, O
we O
need O
accurate O
and O
timely O
methods O
that O
predict O
when O
interventions O
are O
most O
needed O
. O

section 16
id pdf2json/2021.acl-long.322.pdf.json
monitoring O
( O
with O
participants O
’ O
permission O
) O
mobile O
data O
to O
assess O
mental O
health O
and O
provide O
early O
interventions O
is O
, O
therefore O
, O
a O
rich O
opportunity O
for O
scalable O
deployment O
across O
high-risk O
populations O
. O

section 16
id pdf2json/2021.acl-long.322.pdf.json
our O
data O
collection O
, O
experimental O
study O
, O
and O
computational O
approaches O
provide O
a O
step O
towards O
data-intensive O
longitudinal O
monitoring O
of O
human O
behavior O
. O

section 16
id pdf2json/2021.acl-long.322.pdf.json
however O
, O
one O
must O
take O
care O
to O
summarize O
behaviors O
from O
mobile O
data O
without O
identifying O
the O
user O
through O
personal O
( O
e.g. O
, O
personally O
identifiable O
information O
) O
or O
protected O
attributes O
( O
e.g. O
, O
race O
, O
gender O
) O
. O

section 16
id pdf2json/2021.acl-long.322.pdf.json
this O
form O
of O
anonymity O
is O
critical O
when O
implementing O
these O
technologies O
in O
real-world O
scenarios O
. O

section 16
id pdf2json/2021.acl-long.322.pdf.json
our O
goal O
is O
to O
be O
highly O
predictive O
of O
mood O
while O
remaining O
as O
privacy-preserving O
as O
possible O
. O

section 16
id pdf2json/2021.acl-long.322.pdf.json
we O
outline O
some O
of O
the O
potential O
privacy O
and O
security O
concerns O
below O
. O

section 16
id pdf2json/2021.acl-long.322.pdf.json
limitations O
: O
while O
we O
hope O
that O
our O
research O
can O
provide O
a O
starting O
point O
on O
the O
potential O
of O
detecting O
mood O
unobtrusively O
throughout O
the O
day O
in O
a O
privacy-preserving O
way O
, O
we O
strongly O
acknowledge O
there O
remain O
methodological O
issues O
where O
a O
lot O
more O
research O
needs O
to O
be O
done O
to O
enable O
the O
realworld O
deployment O
of O
such O
technologies O
. O

section 16
id pdf2json/2021.acl-long.322.pdf.json
we O
emphasize O
that O
healthcare O
providers O
and O
mobile O
app O
startups O
should O
not O
attempt O
to O
apply O
our O
approach O
in O
the O
real O
world O
until O
the O
following O
issues O
( O
and O
many O
more O
) O
can O
be O
reliably O
resolved O
: O
1 O
. O

section 16
id pdf2json/2021.acl-long.322.pdf.json
we O
do O
not O
make O
broad O
claims O
across O
teenage O
populations O
from O
only O
17 O
participants O
in O
this O
study O
. O

section 16
id pdf2json/2021.acl-long.322.pdf.json
furthermore O
, O
it O
remains O
challenging O
for O
models O
to O
perform O
person-independent O
prediction O
which O
makes O
it O
hard O
to O
deploy O
across O
large O
populations O
. O

section 16
id pdf2json/2021.acl-long.322.pdf.json
2 O
. O

section 16
id pdf2json/2021.acl-long.322.pdf.json
our O
current O
work O
on O
predicting O
daily O
mood O
is O
still O
a O
long O
way O
from O
predicting O
imminent O
suicide O
risk O
. O

section 16
id pdf2json/2021.acl-long.322.pdf.json
furthermore O
, O
any O
form O
of O
prediction O
is O
still O
significantly O
far O
away O
from O
integrating O
methods O
like O
this O
into O
the O
actual O
practice O
of O
mental O
health O
, O
which O
is O
a O
challenging O
problem O
involving O
a O
broad O
range O
of O
medical O
, O
ethical O
, O
social O
, O
and O
technological O
researchers O
( O
resnik O
et O
al. O
, O
2021 O
; O
lee O
et O
al. O
, O
2021 O
) O
. O

section 16
id pdf2json/2021.acl-long.322.pdf.json
3 O
. O

section 16
id pdf2json/2021.acl-long.322.pdf.json
text O
and O
keystrokes O
can O
differ O
for O
participants O
who O
speak O
multiple O
languages O
or O
non-prestige O
vernaculars O
. O

section 16
id pdf2json/2021.acl-long.322.pdf.json
one O
will O
need O
to O
ensure O
that O
the O
method O
works O
across O
a O
broad O
range O
of O
languages O
to O
ensure O
accessibility O
in O
its O
desired O
outcomes O
. O

section 16
id pdf2json/2021.acl-long.322.pdf.json
4 O
. O

section 16
id pdf2json/2021.acl-long.322.pdf.json
this O
study O
assumes O
that O
participants O
have O
no O
restrictions O
for O
data/network O
connections O
& O
data O
plans O
on O
their O
phones O
, O
which O
may O
leave O
out O
vulnerable O
populations O
that O
do O
not O
meet O
this O
criterion O
. O

section 16
id pdf2json/2021.acl-long.322.pdf.json
privacy O
and O
security O
: O
there O
are O
privacy O
risks O
associated O
with O
making O
predictions O
from O
mobile O
data O
. O

section 16
id pdf2json/2021.acl-long.322.pdf.json
to O
deploy O
these O
algorithms O
across O
at-risk O
populations O
, O
it O
is O
important O
to O
keep O
data O
private O
on O
each O
device O
without O
sending O
it O
to O
other O
locations O
. O

section 16
id pdf2json/2021.acl-long.322.pdf.json
even O
if O
data O
is O
kept O
private O
, O
it O
is O
possible O
to O
decode O
data O
from O
gradients O
( O
zhu O
and O
han O
, O
2020 O
) O
or O
pretrained O
models O
( O
carlini O
et O
al. O
, O
2020 O
) O
. O

section 16
id pdf2json/2021.acl-long.322.pdf.json
in O
addition O
, O
sensitive O
databases O
with O
private O
mobile O
data O
could O
be O
at-risk O
to O
external O
security O
attacks O
from O
adversaries O
( O
lyu O
et O
al. O
, O
2020 O
) O
. O

section 16
id pdf2json/2021.acl-long.322.pdf.json
therefore O
, O
it O
is O
crucial O
to O
obtain O
user O
consent O
before O
collecting O
device O
data O
. O

section 16
id pdf2json/2021.acl-long.322.pdf.json
in O
our O
exper- O
iments O
with O
real-world O
mobile O
data O
, O
all O
participants O
have O
given O
consent O
for O
their O
mobile O
device O
data O
to O
be O
collected O
and O
shared O
with O
us O
for O
research O
purposes O
. O

section 16
id pdf2json/2021.acl-long.322.pdf.json
all O
data O
was O
anonymized O
and O
stripped O
of O
all O
personal O
( O
e.g. O
, O
personally O
identifiable O
information O
) O
and O
protected O
attributes O
( O
e.g. O
, O
race O
, O
gender O
) O
. O

section 16
id pdf2json/2021.acl-long.322.pdf.json
social O
biases O
: O
we O
acknowledge O
that O
there O
is O
a O
risk O
of O
exposure O
bias O
due O
to O
imbalanced O
datasets O
, O
especially O
when O
personal O
mobile O
data O
and O
sensitive O
health O
labels O
( O
e.g. O
, O
daily O
mood O
, O
suicidal O
thoughts O
and O
behaviors O
, O
suicide O
risk O
) O
. O

section 16
id pdf2json/2021.acl-long.322.pdf.json
models O
trained O
on O
biased O
data O
have O
been O
shown O
to O
amplify O
the O
underlying O
social O
biases O
especially O
when O
they O
correlate O
with O
the O
prediction O
targets O
( O
lloyd O
, O
2018 O
) O
. O

section 16
id pdf2json/2021.acl-long.322.pdf.json
this O
leaves O
room O
for O
future O
work O
in O
exploring O
methods O
tailored O
for O
specific O
scenarios O
such O
as O
mitigating O
social O
biases O
in O
words O
( O
bolukbasi O
et O
al. O
, O
2016 O
) O
, O
sentences O
( O
liang O
et O
al. O
, O
2020a O
) O
, O
and O
images O
( O
otterbacher O
et O
al. O
, O
2018 O
) O
. O

section 16
id pdf2json/2021.acl-long.322.pdf.json
future O
research O
should O
also O
focus O
on O
quantifying O
the O
trade-offs O
between O
fairness O
and O
performance O
( O
zhao O
and O
gordon O
, O
2019 O
) O
. O

section 16
id pdf2json/2021.acl-long.322.pdf.json
overall O
, O
we O
believe O
that O
our O
proposed O
approach O
can O
help O
quantify O
the O
tradeoffs O
between O
performance O
and O
privacy O
. O

section 16
id pdf2json/2021.acl-long.322.pdf.json
we O
hope O
that O
this O
brings O
about O
future O
opportunities O
for O
large-scale O
real-time O
analytics O
in O
healthcare O
applications O
. O

section 17
id pdf2json/2021.acl-long.322.pdf.json
the O
mobile O
assessment O
for O
the O
prediction O
of O
suicide O
( O
maps O
) O
dataset O
was O
designed O
to O
elucidate O
real-time O
indicators O
of O
suicide O
risk O
in O
adolescents O
ages O
13 O
− O
18 O
years O
. O

section 17
id pdf2json/2021.acl-long.322.pdf.json
current O
adolescent O
suicide O
ideators O
and O
recent O
suicide O
attempters O
along O
with O
aged-matched O
psychiatric O
controls O
with O
no O
lifetime O
suicidal O
thoughts O
and O
behaviors O
completed O
baseline O
clinical O
assessments O
( O
i.e. O
, O
lifetime O
mental O
disorders O
, O
current O
psychiatric O
symptoms O
) O
. O

section 17
id pdf2json/2021.acl-long.322.pdf.json
following O
the O
baseline O
clinical O
characterization O
, O
a O
smartphone O
app O
, O
the O
effortless O
assessment O
of O
risk O
states O
( O
ears O
) O
, O
was O
installed O
onto O
adolescents O
’ O
phones O
, O
and O
passive O
sensor O
data O
were O
acquired O
for O
6-months O
. O

section 17
id pdf2json/2021.acl-long.322.pdf.json
notably O
, O
during O
ears O
installation O
, O
a O
keyboard O
logger O
is O
configured O
on O
adolescents O
’ O
phones O
, O
which O
then O
tracks O
all O
words O
typed O
into O
the O
phone O
as O
well O
as O
the O
apps O
used O
during O
this O
period O
. O

section 17
id pdf2json/2021.acl-long.322.pdf.json
each O
day O
during O
the O
6- O
month O
follow-up O
, O
participants O
also O
were O
asked O
to O
rate O
their O
mood O
on O
the O
previous O
day O
on O
a O
scale O
ranging O
from O
1− O
100 O
, O
with O
higher O
scores O
indicating O
a O
better O
mood O
. O

section 17
id pdf2json/2021.acl-long.322.pdf.json
after O
extracting O
multimodal O
features O
and O
discretizing O
the O
labels O
( O
see O
section O
2 O
) O
, O
we O
summarize O
the O
final O
dataset O
feature O
and O
label O
statistics O
in O
table O
9 O
. O

section 18
id pdf2json/2021.acl-long.322.pdf.json
we O
provide O
additional O
details O
on O
the O
model O
implementation O
and O
experimental O
setup O
. O

section 18
id pdf2json/2021.acl-long.322.pdf.json
c.1 O
implementation O
details O
all O
models O
and O
analyses O
were O
done O
in O
python O
. O

section 18
id pdf2json/2021.acl-long.322.pdf.json
svm O
models O
were O
implemented O
with O
scikitlearn O
and O
mlp/ni-mlp O
models O
were O
implemented O
with O
pytorch O
. O

section 18
id pdf2json/2021.acl-long.322.pdf.json
bert O
, O
xlnet O
, O
and O
longformer O
models O
were O
fine-tuned O
using O
hugging O
face O
( O
website O
: O
https O
: O
//huggingface.co O
, O
github O
: O
https O
: O
//github.com/huggingface O
) O
. O

section 18
id pdf2json/2021.acl-long.322.pdf.json
c.2 O
hyperparameters O
we O
performed O
a O
small O
hyperparameter O
search O
over O
the O
ranges O
in O
table O
10 O
. O

section 18
id pdf2json/2021.acl-long.322.pdf.json
this O
resulted O
in O
a O
total O
of O
35 O
hyperparameter O
configurations O
for O
svm O
and O
12 O
for O
mlp O
( O
6 O
for O
apps O
only O
) O
. O

section 18
id pdf2json/2021.acl-long.322.pdf.json
by O
choosing O
the O
best-performing O
model O
on O
the O
validation O
set O
, O
we O
selected O
the O
resulting O
hyperparameters O
as O
shown O
in O
table O
10 O
. O

section 18
id pdf2json/2021.acl-long.322.pdf.json
c.3 O
model O
parameters O
each O
model O
has O
about O
two O
million O
parameters O
. O

section 18
id pdf2json/2021.acl-long.322.pdf.json
see O
table O
10 O
for O
exact O
hidden O
dimension O
sizes O
. O

section 18
id pdf2json/2021.acl-long.322.pdf.json
c.4 O
training O
resources O
and O
time O
all O
experiments O
were O
conducted O
on O
a O
geforce O
rtx O
2080 O
ti O
gpu O
with O
12 O
gb O
memory O
. O

section 18
id pdf2json/2021.acl-long.322.pdf.json
see O
table O
11 O
for O
approximate O
running O
times O
. O

section 19
id pdf2json/2021.acl-long.322.pdf.json
we O
present O
several O
additional O
analysis O
of O
the O
data O
and O
empirical O
results O
: O
d.1 O
details O
on O
mood O
prediction O
there O
is O
often O
a O
tradeoff O
between O
privacy O
and O
prediction O
performance O
. O

section 19
id pdf2json/2021.acl-long.322.pdf.json
to O
control O
this O
tradeoff O
, O
we O
vary O
the O
parameter O
σ O
, O
which O
is O
the O
amount O
of O
noise O
added O
to O
the O
identity-dependent O
subspace O
across O
batches O
and O
training O
epochs O
. O

section 19
id pdf2json/2021.acl-long.322.pdf.json
in O
practice O
, O
we O
automatically O
perform O
model O
selection O
using O
this O
performance-privacy O
ratio O
r O
computed O
on O
the O
validation O
set O
, O
where O
r O
= O
smlp O
− O
sni-mlp O
tmlp O
− O
tni-mlp O
( O
4 O
) O
is O
defined O
as O
the O
improvement O
in O
privacy O
per O
unit O
of O
performance O
lost O
. O

section 19
id pdf2json/2021.acl-long.322.pdf.json
here O
, O
s O
is O
defined O
as O
the O
accuracy O
in O
the O
user O
prediction O
task O
and O
t O
is O
defined O
as O
the O
f1 O
score O
on O
the O
mood O
prediction O
task O
. O

section 19
id pdf2json/2021.acl-long.322.pdf.json
in O
the O
rare O
cases O
where O
ni-mlp O
performed O
better O
than O
the O
original O
mlp O
and O
caused O
r O
to O
become O
negative O
, O
we O
found O
this O
improvement O
in O
performance O
always O
came O
at O
the O
expense O
of O
worse O
privacy O
as O
compared O
to O
other O
settings O
of O
λ O
and O
σ O
in O
ni-mlp O
. O

section 19
id pdf2json/2021.acl-long.322.pdf.json
therefore O
, O
models O
with O
negative O
r O
were O
not O
considered O
for O
table O
1 O
. O

section 19
id pdf2json/2021.acl-long.322.pdf.json
d.2 O
details O
on O
preserving O
privacy O
for O
table O
5 O
, O
the O
model O
with O
the O
best O
privacy O
out O
of O
those O
within O
5 O
% O
performance O
of O
the O
original O
mlp O
model O
( O
or O
, O
if O
no O
such O
model O
existed O
, O
the O
model O
with O
the O
best O
performance O
) O
was O
selected O
. O

section 19
id pdf2json/2021.acl-long.322.pdf.json
interestingly O
, O
in O
figure O
4 O
, O
we O
find O
that O
the O
tradeoff O
curve O
on O
a O
model O
trained O
only O
using O
app O
features O
does O
not O
exhibit O
a O
pareto O
tradeoff O
curve O
as O
ex- O
pected O
. O

section 19
id pdf2json/2021.acl-long.322.pdf.json
we O
attribute O
this O
to O
randomness O
in O
predicting O
both O
mood O
and O
identities O
. O

section 19
id pdf2json/2021.acl-long.322.pdf.json
furthermore O
, O
wang O
et O
al O
. O

section 19
id pdf2json/2021.acl-long.322.pdf.json
( O
2017 O
) O
found O
that O
adding O
noise O
to O
the O
identity O
subspace O
can O
sometimes O
improve O
generalization O
by O
reducing O
reliance O
on O
identity-dependent O
confounding O
features O
, O
which O
could O
also O
explain O
occasional O
increased O
performance O
at O
larger O
σ O
values O
. O

section 19
id pdf2json/2021.acl-long.322.pdf.json
note O
that O
we O
do O
not O
include O
privacy O
results O
for O
features O
learned O
by O
svm O
, O
which O
finds O
a O
linear O
separator O
in O
a O
specified O
kernel O
space O
rather O
than O
learning O
a O
representation O
for O
each O
sample O
. O

section 19
id pdf2json/2021.acl-long.322.pdf.json
explicitly O
projecting O
our O
features O
is O
computationally O
infeasible O
due O
to O
the O
high O
dimensionality O
of O
our O
chosen O
kernel O
spaces O
. O

section 19
id pdf2json/2021.acl-long.322.pdf.json
d.3 O
qualitative O
analysis O
in O
this O
section O
, O
we O
provide O
more O
empirical O
analysis O
on O
the O
unimodal O
and O
multimodal O
features O
in O
the O
maps O
dataset O
. O

section 19
id pdf2json/2021.acl-long.322.pdf.json
d.3.1 O
understanding O
the O
unimodal O
features O
text O
: O
we O
begin O
with O
some O
basic O
statistics O
regarding O
word O
distributions O
. O

section 19
id pdf2json/2021.acl-long.322.pdf.json
for O
each O
user O
, O
we O
tallied O
the O
frequencies O
of O
each O
word O
under O
each O
daily O
mood O
category O
( O
positive O
, O
neutral O
, O
and O
negative O
) O
, O
as O
well O
as O
the O
overall O
number O
of O
words O
in O
each O
mood O
category O
. O

section 19
id pdf2json/2021.acl-long.322.pdf.json
we O
define O
“ O
positive O
” O
words O
and O
emojis O
to O
be O
those O
with O
a O
higher O
relative O
frequency O
of O
positive O
mood O
compared O
to O
the O
overall O
positive O
mood O
frequency O
, O
and O
lower O
than O
overall O
negative O
mood O
frequency O
. O

section 19
id pdf2json/2021.acl-long.322.pdf.json
likewise O
, O
“ O
negative O
” O
words O
and O
emojis O
have O
higher O
than O
overall O
negative O
mood O
frequency O
and O
lower O
than O
overall O
positive O
mood O
frequency O
. O

section 19
id pdf2json/2021.acl-long.322.pdf.json
we O
filtered O
out O
words O
for O
specific O
users O
if O
the O
word O
was O
used O
less O
than O
40 O
times O
. O

section 19
id pdf2json/2021.acl-long.322.pdf.json
finally O
, O
we O
ranked O
the O
words O
by O
the O
difference O
in O
relative O
frequency O
( O
i.e. O
, O
a O
word O
is O
“ O
more O
positive O
” O
the O
larger O
the O
difference O
between O
its O
positive O
mood O
relative O
frequency O
and O
the O
user O
’ O
s O
overall O
positive O
mood O
relative O
frequency O
) O
. O

section 19
id pdf2json/2021.acl-long.322.pdf.json
see O
table O
12 O
for O
examples O
of O
top O
positive O
and O
negative O
words O
. O

section 19
id pdf2json/2021.acl-long.322.pdf.json
for O
each O
word O
, O
we O
also O
counted O
the O
number O
of O
users O
for O
which O
the O
word O
was O
positive O
or O
negative O
. O

section 19
id pdf2json/2021.acl-long.322.pdf.json
see O
table O
13 O
for O
the O
words O
with O
the O
highest O
user O
counts O
. O

section 19
id pdf2json/2021.acl-long.322.pdf.json
keystrokes O
: O
we O
show O
some O
sample O
bag-of-timing O
histograms O
in O
figure O
6 O
. O

section 19
id pdf2json/2021.acl-long.322.pdf.json
it O
is O
interesting O
to O
find O
that O
certain O
users O
show O
a O
bimodal O
distribution O
across O
their O
keystroke O
histograms O
with O
one O
peak O
representing O
faster O
typing O
and O
another O
representing O
slower O
typing O
. O

section 19
id pdf2json/2021.acl-long.322.pdf.json
visually O
, O
the O
overall O
keystroke O
histograms O
did O
not O
differ O
that O
much O
across O
users O
which O
might O
explain O
its O
lower O
accuracies O
in O
both O
mood O
and O
user O
prediction O
when O
trained O
with O
ni-mlp O
( O
see O
figure O
4 O
) O
. O

section 19
id pdf2json/2021.acl-long.322.pdf.json
app O
usage O
: O
similar O
to O
“ O
positive O
” O
words O
, O
we O
define O
“ O
positive O
” O
apps O
to O
be O
those O
with O
higher O
than O
overall O
positive O
mood O
relative O
frequency O
and O
lower O
than O
overall O
negative O
mood O
relative O
frequency O
, O
and O
“ O
negative O
” O
apps O
to O
be O
the O
opposite O
. O

section 19
id pdf2json/2021.acl-long.322.pdf.json
apps O
were O
also O
then O
sorted O
by O
difference O
in O
relative O
frequency O
. O

section 19
id pdf2json/2021.acl-long.322.pdf.json
d.3.2 O
understanding O
the O
multimodal O
features O
characters O
with O
keystrokes O
: O
for O
each O
user O
, O
we O
plotted O
histograms O
of O
keystroke O
timings O
of O
alphanumeric O
characters O
, O
symbols O
( O
punctuation O
and O
emojis O
) O
, O
spacebar O
, O
enter O
, O
delete O
, O
and O
use O
of O
autocorrect O
, O
split O
across O
daily O
mood O
categories O
. O

section 19
id pdf2json/2021.acl-long.322.pdf.json
see O
figure O
7 O
for O
examples O
across O
one O
user O
. O

section 19
id pdf2json/2021.acl-long.322.pdf.json
we O
find O
particularly O
interesting O
patterns O
in O
the O
autocorrect O
keys O
and O
symbols O
where O
keystrokes O
are O
quite O
indicative O
of O
mood O
, O
which O
attests O
to O
the O
unique O
nature O
of O
typed O
text O
. O

section 19
id pdf2json/2021.acl-long.322.pdf.json
words O
with O
keystrokes O
: O
for O
each O
user O
, O
we O
plotted O
histograms O
of O
the O
word-level O
keystroke O
timings O
of O
the O
top O
500 O
words O
, O
split O
across O
the O
daily O
mood O
categories O
of O
positive O
, O
neutral O
, O
and O
negative O
. O

section 19
id pdf2json/2021.acl-long.322.pdf.json
we O
also O
performed O
wilcoxon O
rank-sum O
tests O
at O
5 O
% O
signifi- O
cance O
level O
( O
wilcoxon O
, O
1992 O
) O
between O
the O
timings O
of O
positive O
and O
negative O
mood O
for O
each O
user/word O
combination O
to O
determine O
which O
words O
had O
significantly O
different O
timings O
between O
positive O
and O
negative O
mood O
. O

section 20
id pdf2json/2021.acl-long.322.pdf.json
since O
this O
is O
a O
new O
dataset O
, O
we O
explored O
several O
more O
methods O
throughout O
the O
research O
process O
. O

section 20
id pdf2json/2021.acl-long.322.pdf.json
in O
this O
section O
we O
describe O
some O
of O
the O
approaches O
that O
yielded O
initial O
negative O
results O
despite O
them O
working O
well O
for O
standard O
datasets O
: O
1 O
. O

section 20
id pdf2json/2021.acl-long.322.pdf.json
user O
specific O
models O
: O
we O
also O
explored O
the O
setting O
of O
training O
a O
separate O
model O
per O
user O
but O
we O
found O
that O
there O
was O
too O
little O
data O
per O
user O
to O
train O
a O
good O
model O
. O

section 20
id pdf2json/2021.acl-long.322.pdf.json
as O
part O
of O
future O
work O
, O
we O
believe O
that O
if O
ni-mlp O
can O
learn O
a O
user-independent O
classifier O
, O
these O
representations O
can O
then O
be O
used O
for O
further O
finetuning O
or O
few-shot O
learning O
on O
each O
specific O
user O
. O

section 20
id pdf2json/2021.acl-long.322.pdf.json
previous O
work O
in O
federated O
learning O
( O
smith O
et O
al. O
, O
2017 O
; O
liang O
et O
al. O
, O
2020b O
) O
offers O
ways O
of O
learning O
a O
user-specific O
model O
that O
leverages O
other O
users O
’ O
data O
during O
training O
, O
which O
could O
help O
to O
alleviate O
the O
lack O
of O
data O
per O
user O
. O

section 20
id pdf2json/2021.acl-long.322.pdf.json
2 O
. O

section 20
id pdf2json/2021.acl-long.322.pdf.json
user-independent O
data O
splits O
: O
we O
have O
shown O
that O
text O
, O
keystrokes O
, O
and O
app O
usage O
features O
are O
highly O
dependent O
on O
participant O
identities O
. O

section 20
id pdf2json/2021.acl-long.322.pdf.json
consequently O
, O
models O
trained O
on O
these O
features O
would O
perform O
poorly O
when O
evaluated O
on O
a O
user O
not O
found O
in O
the O
training O
set O
. O

section 20
id pdf2json/2021.acl-long.322.pdf.json
we O
would O
like O
to O
evaluate O
if O
better O
learning O
of O
user-independent O
features O
can O
improve O
generalization O
to O
new O
users O
( O
e.g. O
, O
split O
the O
data O
such O
that O
the O
first O
10 O
users O
are O
used O
for O
training O
, O
next O
3 O
for O
validation O
, O
and O
final O
4 O
for O
testing O
) O
. O

section 20
id pdf2json/2021.acl-long.322.pdf.json
our O
initial O
results O
for O
these O
were O
negative O
, O
but O
we O
believe O
that O
combining O
better O
privacy-preserving O
methods O
that O
learn O
user-independent O
features O
could O
help O
in O
this O
regard O
. O

section 20
id pdf2json/2021.acl-long.322.pdf.json
3 O
. O

section 20
id pdf2json/2021.acl-long.322.pdf.json
fine-grained O
multimodal O
fusion O
: O
our O
approach O
of O
combining O
modalities O
was O
only O
at O
the O
input O
level O
( O
i.e. O
, O
early O
fusion O
( O
baltrušaitis O
et O
al. O
, O
2018 O
) O
) O
which O
can O
be O
improved O
upon O
by O
leveraging O
recent O
work O
in O
more O
fine-grained O
fusion O
( O
liang O
et O
al. O
, O
2018 O
) O
. O

section 20
id pdf2json/2021.acl-long.322.pdf.json
one O
such O
example O
could O
be O
to O
align O
each O
keystroke O
feature O
and O
app O
data O
to O
the O
exact O
text O
that O
was O
entered O
in O
, O
which O
provides O
more O
finegrained O
contextualization O
of O
text O
in O
keystroke O
and O
app O
usage O
context O
. O

section TITLE
id pdf2json/2021.acl-long.266.pdf.json
rejuvenating O
low-frequency O
words O
: O
making O
the O
most O
of O
parallel O
data O
in O
non-autoregressive O
translation O

section ABSTRACT
id pdf2json/2021.acl-long.266.pdf.json
knowledge O
distillation O
( O
kd O
) O
is O
commonly O
used O
to O
construct O
synthetic O
data O
for O
training O
non-autoregressive O
translation O
( O
nat O
) O
models O
. O

section ABSTRACT
id pdf2json/2021.acl-long.266.pdf.json
however O
, O
there O
exists O
a O
discrepancy O
on O
lowfrequency O
words O
between O
the O
distilled O
and O
the O
original O
data O
, O
leading O
to O
more O
errors O
on O
predicting O
low-frequency O
words O
. O

section ABSTRACT
id pdf2json/2021.acl-long.266.pdf.json
to O
alleviate O
the O
problem O
, O
we O
directly O
expose O
the O
raw O
data O
into O
nat O
by O
leveraging O
pretraining O
. O

section ABSTRACT
id pdf2json/2021.acl-long.266.pdf.json
by O
analyzing O
directed O
alignments O
, O
we O
found O
that O
kd O
makes O
low-frequency O
source O
words O
aligned O
with O
targets O
more O
deterministically O
but O
fails O
to O
align O
sufficient O
low-frequency O
words O
from O
target O
to O
source O
. O

section ABSTRACT
id pdf2json/2021.acl-long.266.pdf.json
accordingly O
, O
we O
propose O
reverse O
kd O
to O
rejuvenate O
more O
alignments O
for O
lowfrequency O
target O
words O
. O

section ABSTRACT
id pdf2json/2021.acl-long.266.pdf.json
to O
make O
the O
most O
of O
authentic O
and O
synthetic O
data O
, O
we O
combine O
these O
complementary O
approaches O
as O
a O
new O
training O
strategy O
for O
further O
boosting O
nat O
performance O
. O

section ABSTRACT
id pdf2json/2021.acl-long.266.pdf.json
we O
conduct O
experiments O
on O
five O
translation O
benchmarks O
over O
two O
advanced O
architectures O
. O

section ABSTRACT
id pdf2json/2021.acl-long.266.pdf.json
results O
demonstrate O
that O
the O
proposed O
approach O
can O
significantly O
and O
universally O
improve O
translation O
quality O
by O
reducing O
translation O
errors O
on O
low-frequency O
words O
. O

section ABSTRACT
id pdf2json/2021.acl-long.266.pdf.json
encouragingly O
, O
our O
approach O
achieves O
28.2 O
and O
33.9 O
bleu O
points O
on O
the O
wmt14 O
english-german O
and O
wmt16 O
romanian-english O
datasets O
, O
respectively O
. O

section ABSTRACT
id pdf2json/2021.acl-long.266.pdf.json
our O
code O
, O
data O
, O
and O
trained O
models O
are O
available O
at O
https O
: O
//github.com/ O
longyuewangdcu/rlfw-nat O
. O

section 0
id pdf2json/2021.acl-long.266.pdf.json
proceedings O
of O
the O
59th O
annual O
meeting O
of O
the O
association O
for O
computational O
linguistics O
and O
the O
11th O
international O
joint O
conference O
on O
natural O
language O
processing O
, O
pages O
3431–3441 O
august O
1–6 O
, O
2021 O
. O

section 0
id pdf2json/2021.acl-long.266.pdf.json
©2021 O
association O
for O
computational O
linguistics O
3431 O

section 1
id pdf2json/2021.acl-long.266.pdf.json
recent O
years O
have O
seen O
a O
surge O
of O
interest O
in O
nonautoregressive O
translation O
( O
nat O
, O
gu O
et O
al. O
, O
2018 O
) O
, O
which O
can O
improve O
the O
decoding O
efficiency O
by O
predicting O
all O
tokens O
independently O
and O
simultaneously O
. O

section 1
id pdf2json/2021.acl-long.266.pdf.json
the O
non-autoregressive O
factorization O
breaks O
conditional O
dependencies O
among O
output O
tokens O
, O
∗ O
liang O
ding O
and O
longyue O
wang O
contributed O
equally O
to O
this O
work O
. O

section 1
id pdf2json/2021.acl-long.266.pdf.json
work O
was O
done O
when O
liang O
ding O
and O
xuebo O
liu O
were O
interning O
at O
tencent O
ai O
lab O
. O

section 1
id pdf2json/2021.acl-long.266.pdf.json
which O
prevents O
a O
model O
from O
properly O
capturing O
the O
highly O
multimodal O
distribution O
of O
target O
translations O
. O

section 1
id pdf2json/2021.acl-long.266.pdf.json
as O
a O
result O
, O
the O
translation O
quality O
of O
nat O
models O
often O
lags O
behind O
that O
of O
autoregressive O
translation O
( O
at O
, O
vaswani O
et O
al. O
, O
2017 O
) O
models O
. O

section 1
id pdf2json/2021.acl-long.266.pdf.json
to O
balance O
the O
trade-off O
between O
decoding O
speed O
and O
translation O
quality O
, O
knowledge O
distillation O
( O
kd O
) O
is O
widely O
used O
to O
construct O
a O
new O
training O
data O
for O
nat O
models O
( O
gu O
et O
al. O
, O
2018 O
) O
. O

section 1
id pdf2json/2021.acl-long.266.pdf.json
specifically O
, O
target O
sentences O
in O
the O
distilled O
training O
data O
are O
generated O
by O
an O
at O
teacher O
, O
which O
makes O
nat O
easily O
acquire O
more O
deterministic O
knowledge O
and O
achieve O
significant O
improvement O
( O
zhou O
et O
al. O
, O
2020 O
) O
. O

section 1
id pdf2json/2021.acl-long.266.pdf.json
previous O
studies O
have O
shown O
that O
distillation O
may O
lose O
some O
important O
information O
in O
the O
original O
training O
data O
, O
leading O
to O
more O
errors O
on O
predicting O
low-frequency O
words O
. O

section 1
id pdf2json/2021.acl-long.266.pdf.json
to O
alleviate O
this O
problem O
, O
ding O
et O
al O
. O

section 1
id pdf2json/2021.acl-long.266.pdf.json
( O
2021b O
) O
proposed O
to O
augment O
nat O
models O
the O
ability O
to O
learn O
lost O
knowledge O
from O
the O
original O
data O
. O

section 1
id pdf2json/2021.acl-long.266.pdf.json
however O
, O
their O
approach O
relies O
on O
external O
resources O
( O
e.g O
. O

section 1
id pdf2json/2021.acl-long.266.pdf.json
word O
alignment O
) O
and O
human-crafted O
priors O
, O
which O
limits O
the O
applicability O
of O
the O
method O
to O
a O
broader O
range O
of O
tasks O
and O
languages O
. O

section 1
id pdf2json/2021.acl-long.266.pdf.json
accordingly O
, O
we O
turn O
to O
directly O
expose O
the O
raw O
data O
into O
nat O
by O
leveraging O
pretraining O
without O
intensive O
modification O
to O
model O
architectures O
( O
§2.2 O
) O
. O

section 1
id pdf2json/2021.acl-long.266.pdf.json
furthermore O
, O
we O
analyze O
bilingual O
links O
in O
the O
distilled O
data O
from O
two O
alignment O
directions O
( O
i.e O
. O

section 1
id pdf2json/2021.acl-long.266.pdf.json
source-to-target O
and O
target-to-source O
) O
. O

section 1
id pdf2json/2021.acl-long.266.pdf.json
we O
found O
that O
kd O
makes O
low-frequency O
source O
words O
aligned O
with O
targets O
more O
deterministically O
but O
fails O
to O
align O
low-frequency O
words O
from O
target O
to O
source O
due O
to O
information O
loss O
. O

section 1
id pdf2json/2021.acl-long.266.pdf.json
inspired O
by O
this O
finding O
, O
we O
propose O
reverse O
kd O
to O
recall O
more O
alignments O
for O
low-frequency O
target O
words O
( O
§2.3 O
) O
. O

section 1
id pdf2json/2021.acl-long.266.pdf.json
we O
then O
concatenate O
two O
kinds O
of O
distilled O
data O
to O
maintain O
advantages O
of O
deterministic O
knowledge O
and O
low-frequency O
information O
. O

section 1
id pdf2json/2021.acl-long.266.pdf.json
to O
make O
the O
most O
of O
authentic O
and O
synthetic O
data O
, O
we O
combine O
three O
complementary O
approaches O
( O
i.e O
. O

section 1
id pdf2json/2021.acl-long.266.pdf.json
raw O
pretraining O
, O
bidirectional O
distillation O
training O
and O
kd O
finetuning O
) O
as O
a O
new O
training O
strategy O
for O
further O
boosting O
nat O
performance O
( O
§2.4 O
) O
. O

section 1
id pdf2json/2021.acl-long.266.pdf.json
we O
validated O
our O
approach O
on O
five O
translation O
benchmarks O
( O
wmt14 O
en-de O
, O
wmt16 O
ro-en O
, O
wmt17 O
zh-en O
, O
wat17 O
ja-en O
and O
wmt19 O
ende O
) O
over O
two O
advanced O
architectures O
( O
mask O
predict O
, O
ghazvininejad O
et O
al. O
, O
2019 O
; O
levenshtein O
transformer O
, O
gu O
et O
al. O
, O
2019 O
) O
. O

section 1
id pdf2json/2021.acl-long.266.pdf.json
experimental O
results O
show O
that O
the O
proposed O
method O
consistently O
improve O
translation O
performance O
over O
the O
standard O
nat O
models O
across O
languages O
and O
advanced O
nat O
architectures O
. O

section 1
id pdf2json/2021.acl-long.266.pdf.json
extensive O
analyses O
confirm O
that O
the O
performance O
improvement O
indeed O
comes O
from O
the O
better O
lexical O
translation O
accuracy O
especially O
on O
low-frequency O
tokens O
. O

section 1
id pdf2json/2021.acl-long.266.pdf.json
contributions O
our O
main O
contributions O
are O
: O
• O
we O
show O
the O
effectiveness O
of O
rejuvenating O
lowfrequency O
information O
by O
pretraining O
nat O
models O
from O
raw O
data O
. O

section 1
id pdf2json/2021.acl-long.266.pdf.json
• O
we O
provide O
a O
quantitative O
analysis O
of O
bilingual O
links O
to O
demonstrate O
the O
necessity O
to O
improve O
low-frequency O
alignment O
by O
leveraging O
both O
kd O
and O
reverse O
kd O
. O

section 1
id pdf2json/2021.acl-long.266.pdf.json
• O
we O
introduce O
a O
simple O
and O
effective O
training O
recipe O
to O
accomplish O
this O
goal O
, O
which O
is O
robustly O
applicable O
to O
several O
model O
structures O
and O
language O
pairs O
. O

section 3
id pdf2json/2021.acl-long.266.pdf.json
non-autoregressive O
translation O
given O
a O
source O
sentence O
x O
, O
an O
at O
model O
generates O
each O
target O
word O
yt O
conditioned O
on O
previously O
generated O
ones O
y O
< O
t O
, O
leading O
to O
high O
latency O
on O
the O
decoding O
stage O
. O

section 3
id pdf2json/2021.acl-long.266.pdf.json
in O
contrast O
, O
nat O
models O
break O
this O
autoregressive O
factorization O
by O
producing O
target O
words O
in O
parallel O
. O

section 3
id pdf2json/2021.acl-long.266.pdf.json
accordingly O
, O
the O
probability O
of O
generating O
y O
is O
computed O
as O
: O
p O
( O
y|x O
) O
= O
t∏ O
t=1 O
p O
( O
yt|x O
; O
θ O
) O
( O
1 O
) O
where O
t O
is O
the O
length O
of O
the O
target O
sequence O
, O
and O
it O
is O
usually O
predicted O
by O
a O
separate O
conditional O
distribution O
. O

section 3
id pdf2json/2021.acl-long.266.pdf.json
the O
parameters O
θ O
are O
trained O
to O
maximize O
the O
likelihood O
of O
a O
set O
of O
training O
examples O
according O
to O
l O
( O
θ O
) O
= O
arg O
maxθ O
log O
p O
( O
y|x O
; O
θ O
) O
. O

section 3
id pdf2json/2021.acl-long.266.pdf.json
typically O
, O
most O
nat O
models O
are O
implemented O
upon O
the O
framework O
of O
transformer O
( O
vaswani O
et O
al. O
, O
2017 O
) O
. O

section 3
id pdf2json/2021.acl-long.266.pdf.json
knowledge O
distillation O
gu O
et O
al O
. O

section 3
id pdf2json/2021.acl-long.266.pdf.json
( O
2018 O
) O
pointed O
out O
that O
nat O
models O
suffer O
from O
the O
multimodality O
problem O
, O
where O
the O
conditional O
independence O
assumption O
prevents O
a O
model O
from O
properly O
capturing O
the O
highly O
multimodal O
distribution O
of O
target O
translations O
. O

section 3
id pdf2json/2021.acl-long.266.pdf.json
thus O
, O
the O
sequence-level O
knowledge O
distillation O
is O
introduced O
to O
reduce O
the O
modes O
of O
training O
data O
by O
replacing O
their O
original O
target-side O
samples O
with O
sentences O
generated O
by O
an O
at O
teacher O
( O
gu O
et O
al. O
, O
2018 O
; O
zhou O
et O
al. O
, O
2020 O
; O
ren O
et O
al. O
, O
2020 O
) O
. O

section 3
id pdf2json/2021.acl-long.266.pdf.json
formally O
, O
the O
original O
parallel O
data O
raw O
and O
the O
distilled O
data O
−→ O
kd O
can O
be O
defined O
as O
follows O
: O
raw O
= O
{ O
( O
xi O
, O
yi O
) O
} O
ni=1 O
( O
2 O
) O
−→ O
kd O
= O
{ O
( O
xi O
, O
fs O
7→t O
( O
xi O
) O
) O
|xi O
∈ O
raws O
} O
ni=1 O
( O
3 O
) O
where O
fs O
7→t O
represents O
an O
at-based O
translation O
model O
trained O
on O
raw O
data O
for O
translating O
text O
from O
the O
source O
to O
the O
target O
language O
. O

section 3
id pdf2json/2021.acl-long.266.pdf.json
n O
is O
the O
total O
number O
of O
sentence O
pairs O
in O
training O
data O
. O

section 3
id pdf2json/2021.acl-long.266.pdf.json
as O
shown O
in O
figure O
1 O
( O
a O
) O
, O
well-performed O
nat O
models O
are O
generally O
trained O
on O
−→ O
kd O
data O
instead O
of O
raw O
. O

section 4
id pdf2json/2021.acl-long.266.pdf.json
motivation O
gao O
et O
al O
. O

section 4
id pdf2json/2021.acl-long.266.pdf.json
( O
2018 O
) O
showed O
that O
more O
than O
90 O
% O
of O
words O
are O
lower O
than O
10e-4 O
frequency O
in O
wmt14 O
en-de O
dataset O
. O

section 4
id pdf2json/2021.acl-long.266.pdf.json
this O
token O
imbalance O
problem O
biases O
translation O
models O
towards O
overfitting O
to O
frequent O
observations O
while O
neglecting O
those O
low-frequency O
observations O
( O
gong O
et O
al. O
, O
2018 O
; O
nguyen O
and O
chiang O
, O
2018 O
; O
gu O
et O
al. O
, O
2020 O
) O
. O

section 4
id pdf2json/2021.acl-long.266.pdf.json
thus O
, O
the O
at O
teacher O
fs7→t O
tends O
to O
generate O
more O
high-frequency O
tokens O
and O
less O
low-frequency O
tokens O
during O
constructing O
distilled O
data O
−→ O
kd O
. O

section 4
id pdf2json/2021.acl-long.266.pdf.json
on O
the O
one O
hand O
, O
kd O
can O
reduce O
the O
modes O
in O
training O
data O
( O
i.e O
. O

section 4
id pdf2json/2021.acl-long.266.pdf.json
multiple O
lexical O
choices O
for O
a O
source O
word O
) O
, O
which O
lowers O
the O
intrinsic O
uncertainty O
( O
ott O
et O
al. O
, O
2018 O
) O
and O
learning O
difficulty O
for O
nat O
( O
zhou O
et O
al. O
, O
2020 O
; O
ren O
et O
al. O
, O
2020 O
) O
, O
making O
it O
easily O
acquire O
more O
deterministic O
knowledge O
. O

section 4
id pdf2json/2021.acl-long.266.pdf.json
on O
the O
other O
hand O
, O
kd O
aggravates O
the O
imbalance O
of O
high-frequency O
and O
low-frequency O
words O
in O
training O
data O
and O
lost O
some O
important O
information O
originated O
in O
raw O
data O
. O

section 4
id pdf2json/2021.acl-long.266.pdf.json
ding O
et O
al O
. O

section 4
id pdf2json/2021.acl-long.266.pdf.json
( O
2021b O
) O
revealed O
the O
side O
effect O
of O
distilled O
training O
data O
, O
which O
cause O
lexical O
choice O
errors O
for O
low-frequency O
words O
in O
nat O
models O
. O

section 4
id pdf2json/2021.acl-long.266.pdf.json
accordingly O
, O
they O
introduced O
an O
extra O
bilingual O
data-dependent O
prior O
objective O
to O
augments O
nat O
models O
the O
ability O
to O
learn O
the O
lost O
knowledge O
from O
raw O
data O
. O

section 4
id pdf2json/2021.acl-long.266.pdf.json
we O
use O
their O
findings O
as O
our O
departure O
point O
, O
but O
rejuvenate O
low-frequency O
words O
in O
a O
more O
simple O
and O
direct O
way O
: O
directly O
exposing O
raw O
data O
into O
nat O
via O
pretraining O
. O

section 4
id pdf2json/2021.acl-long.266.pdf.json
our O
approach O
many O
studies O
have O
shown O
that O
pretraining O
could O
transfer O
the O
knowledge O
and O
data O
distribution O
, O
especially O
for O
rare O
categories O
, O
hence O
improving O
the O
model O
robustness O
( O
hendrycks O
et O
al. O
, O
2019 O
; O
mathis O
et O
al. O
, O
2021 O
) O
. O

section 4
id pdf2json/2021.acl-long.266.pdf.json
here O
we O
want O
to O
transfer O
the O
distribution O
of O
lost O
information O
, O
e.g O
. O

section 4
id pdf2json/2021.acl-long.266.pdf.json
lowfrequency O
words O
. O

section 4
id pdf2json/2021.acl-long.266.pdf.json
as O
illustrated O
in O
figure O
1 O
( O
b O
) O
, O
we O
propose O
to O
first O
pretrain O
nat O
models O
on O
raw O
data O
and O
then O
continuously O
train O
them O
on O
−→ O
kd O
data O
. O

section 4
id pdf2json/2021.acl-long.266.pdf.json
the O
raw O
data O
maintain O
the O
original O
distribution O
especially O
on O
low-frequency O
words O
. O

section 4
id pdf2json/2021.acl-long.266.pdf.json
although O
it O
is O
difficult O
for O
nat O
to O
learn O
high-mode O
data O
, O
the O
pretraining O
can O
acquire O
general O
knowledge O
from O
authentic O
data O
, O
which O
may O
help O
better O
and O
faster O
learning O
further O
tasks O
. O

section 4
id pdf2json/2021.acl-long.266.pdf.json
thus O
, O
we O
early O
stop O
pretraining O
when O
the O
model O
can O
achieve O
90 O
% O
of O
the O
best O
performance O
of O
raw O
data O
in O
terms O
of O
bleu O
score O
( O
platanios O
et O
al. O
, O
2019 O
) O
1 O
. O

section 4
id pdf2json/2021.acl-long.266.pdf.json
in O
order O
to O
keep O
the O
merits O
of O
low-modes O
, O
1in O
preliminary O
experiments O
, O
we O
tried O
another O
simple O
strategy O
: O
early-stop O
at O
fixed O
step O
according O
to O
the O
size O
of O
training O
data O
( O
e.g O
. O

section 4
id pdf2json/2021.acl-long.266.pdf.json
training O
70k O
en-de O
and O
early O
stop O
at O
20k O
/ O
30k O
/ O
40k O
, O
respectively O
) O
. O

section 4
id pdf2json/2021.acl-long.266.pdf.json
we O
found O
that O
both O
strategies O
achieve O
kd O
” O
and O
“ O
←− O
kd O
” O
indicate O
syntactic O
data O
distilled O
by O
kd O
and O
reverse O
kd O
, O
respectively O
. O

section 4
id pdf2json/2021.acl-long.266.pdf.json
the O
subscript O
“ O
s O
” O
or O
“ O
t O
” O
is O
short O
for O
source- O
or O
target-side O
. O

section 4
id pdf2json/2021.acl-long.266.pdf.json
the O
low-frequency O
words O
are O
highlighted O
with O
colors O
and O
italics O
are O
incorrect O
translations O
. O

section 4
id pdf2json/2021.acl-long.266.pdf.json
we O
further O
train O
the O
pretrained O
model O
on O
distilled O
data O
−→ O
kd O
. O

section 4
id pdf2json/2021.acl-long.266.pdf.json
as O
it O
is O
easy O
for O
nat O
to O
learn O
deterministic O
knowledge O
, O
we O
finetune O
the O
model O
for O
the O
rest O
steps O
. O

section 4
id pdf2json/2021.acl-long.266.pdf.json
for O
fair O
comparison O
, O
the O
total O
training O
steps O
of O
the O
proposed O
method O
are O
same O
as O
the O
traditional O
one O
. O

section 4
id pdf2json/2021.acl-long.266.pdf.json
in O
general O
, O
we O
expect O
that O
this O
training O
recipe O
can O
provide O
a O
good O
trade-off O
between O
raw O
and O
distilled O
data O
( O
i.e O
. O

section 4
id pdf2json/2021.acl-long.266.pdf.json
high-modes O
and O
complete O
vs. O
low-modes O
and O
incomplete O
) O
. O

section 5
id pdf2json/2021.acl-long.266.pdf.json
analyzing O
bilingual O
links O
in O
data O
kd O
simplifies O
the O
training O
data O
by O
replacing O
low-frequency O
target O
words O
with O
high-frequency O
ones O
( O
zhou O
et O
al. O
, O
2020 O
) O
. O

section 5
id pdf2json/2021.acl-long.266.pdf.json
this O
is O
able O
to O
facilitate O
easier O
aligning O
source O
words O
to O
target O
ones O
, O
resulting O
in O
high O
bilingual O
coverage O
( O
jiao O
et O
al. O
, O
2020 O
) O
. O

section 5
id pdf2json/2021.acl-long.266.pdf.json
due O
to O
the O
information O
loss O
, O
we O
argue O
that O
kd O
makes O
lowfrequency O
target O
words O
have O
fewer O
opportunities O
to O
align O
with O
source O
ones O
. O

section 5
id pdf2json/2021.acl-long.266.pdf.json
to O
verify O
this O
, O
we O
propose O
a O
method O
to O
quantitatively O
analyze O
bilingual O
links O
from O
two O
directions O
, O
where O
low-frequency O
words O
similar O
performance O
. O

section 5
id pdf2json/2021.acl-long.266.pdf.json
are O
aligned O
from O
source O
to O
target O
( O
s O
7→ O
t O
) O
or O
in O
an O
opposite O
direction O
( O
t O
7→ O
s O
) O
. O

section 5
id pdf2json/2021.acl-long.266.pdf.json
the O
method O
can O
be O
applied O
to O
different O
types O
of O
data O
. O

section 5
id pdf2json/2021.acl-long.266.pdf.json
here O
we O
take O
s O
7→ O
t O
links O
in O
raw O
data O
as O
an O
example O
to O
illustrate O
the O
algorithm O
. O

section 5
id pdf2json/2021.acl-long.266.pdf.json
given O
the O
wmt14 O
en-de O
parallel O
corpus O
, O
we O
employ O
an O
unsupervised O
word O
alignment O
method2 O
( O
och O
and O
ney O
, O
2003 O
) O
to O
produce O
a O
word O
alignment O
, O
and O
then O
we O
extract O
aligned O
links O
whose O
source O
words O
are O
low-frequency O
( O
called O
s O
7→ O
t O
lfw O
links O
) O
. O

section 5
id pdf2json/2021.acl-long.266.pdf.json
second O
, O
we O
randomly O
select O
a O
number O
of O
samples O
from O
the O
parallel O
corpus O
. O

section 5
id pdf2json/2021.acl-long.266.pdf.json
for O
better O
comparison O
, O
the O
subset O
should O
contains O
the O
same O
i O
in O
equation O
( O
2 O
) O
as O
that O
of O
other O
type O
of O
datasets O
( O
e.g O
. O

section 5
id pdf2json/2021.acl-long.266.pdf.json
i O
in O
equation O
( O
3 O
) O
for O
−→ O
kd O
) O
. O

section 5
id pdf2json/2021.acl-long.266.pdf.json
finally O
, O
we O
calculate O
recall O
, O
precision O
, O
f1 O
scores O
based O
on O
low-frequency O
bilingual O
links O
for O
the O
subset O
. O

section 5
id pdf2json/2021.acl-long.266.pdf.json
recall O
( O
r O
) O
represents O
how O
many O
low-frequency O
source O
words O
can O
be O
aligned O
to O
targets O
. O

section 5
id pdf2json/2021.acl-long.266.pdf.json
precision O
( O
p O
) O
means O
how O
many O
aligned O
low-frequency O
links O
are O
correct O
according O
to O
human O
evaluation O
. O

section 5
id pdf2json/2021.acl-long.266.pdf.json
f1 O
is O
the O
harmonic O
mean O
between O
precision O
and O
recall O
. O

section 5
id pdf2json/2021.acl-long.266.pdf.json
similarly O
, O
we O
can O
analyze O
t O
7→ O
s O
lfw O
links O
by O
considering O
low-frequency O
targets O
. O

section 5
id pdf2json/2021.acl-long.266.pdf.json
table O
1 O
shows O
the O
results O
on O
low-frequency O
links O
. O

section 5
id pdf2json/2021.acl-long.266.pdf.json
compared O
with O
raw O
, O
−→ O
kd O
can O
recall O
more O
s O
7→ O
t O
lfw O
links O
( O
73.4 O
vs. O
66.4 O
) O
with O
more O
accurate O
alignment O
( O
89.2 O
vs. O
73.3 O
) O
. O

section 5
id pdf2json/2021.acl-long.266.pdf.json
this O
demonstrates O
the O
effectiveness O
of O
kd O
for O
nat O
models O
from O
the O
bilingual O
alignment O
perspective O
. O

section 5
id pdf2json/2021.acl-long.266.pdf.json
however O
, O
in O
the O
t O
7→ O
s O
direction O
, O
there O
are O
fewer O
lfw O
links O
( O
69.9 O
vs. O
72.3 O
) O
with O
worse O
alignment O
quality O
( O
79.1 O
vs. O
80.6 O
) O
in−→ O
kd O
than O
those O
in O
raw O
. O

section 5
id pdf2json/2021.acl-long.266.pdf.json
this O
confirms O
our O
claim O
that O
kd O
harms O
nat O
models O
due O
to O
the O
loss O
of O
lowfrequency O
target O
words O
. O

section 5
id pdf2json/2021.acl-long.266.pdf.json
inspired O
by O
these O
findings O
, O
it O
is O
natural O
to O
assume O
that O
reverse O
kd O
exhibits O
complementary O
properties O
. O

section 5
id pdf2json/2021.acl-long.266.pdf.json
accordingly O
, O
we O
conduct O
the O
same O
analysis O
method O
on O
←− O
kd O
data O
, O
and O
found O
better O
t O
7→ O
s O
links O
but O
worse O
s O
7→ O
t O
links O
compared O
with O
raw O
. O

section 5
id pdf2json/2021.acl-long.266.pdf.json
take O
the O
zh-en O
sentence O
pair O
in O
table O
2 O
for O
example O
, O
−→ O
kd O
retains O
the O
source O
side O
lowfrequency O
chinese O
words O
“ O
海克曼 O
” O
( O
raws O
) O
but O
generates O
the O
high-frequency O
english O
words O
“ O
heckman O
” O
instead O
of O
the O
golden O
“ O
hackman O
” O
( O
−→ O
kdt O
) O
. O

section 5
id pdf2json/2021.acl-long.266.pdf.json
on O
the O
other O
hand O
, O
←− O
kd O
preserves O
the O
low-frequency O
english O
words O
“ O
hackman O
” O
( O
rawt O
) O
but O
produces O
the O
high-frequency O
chinese O
words O
“ O
哈克曼 O
” O
( O
←− O
kds O
) O
. O

section 5
id pdf2json/2021.acl-long.266.pdf.json
our O
approach O
based O
on O
analysis O
results O
, O
we O
propose O
to O
train O
nat O
models O
on O
bidirectional O
distil- O
2the O
fastalign O
( O
dyer O
et O
al. O
, O
2013 O
) O
was O
employed O
to O
build O
word O
alignments O
for O
the O
training O
datasets O
. O

section 5
id pdf2json/2021.acl-long.266.pdf.json
lation O
by O
concatenating O
two O
kinds O
of O
distilled O
data O
. O

section 5
id pdf2json/2021.acl-long.266.pdf.json
the O
reverse O
distillation O
is O
to O
replace O
the O
source O
sentences O
in O
the O
original O
training O
data O
with O
synthetic O
ones O
generated O
by O
a O
backward O
at O
teacher.3 O
according O
to O
equation O
3 O
, O
←− O
kd O
can O
be O
formulated O
as O
: O
←− O
kd O
= O
{ O
( O
yi O
, O
ft7→s O
( O
yi O
) O
) O
|yi O
∈ O
rawt O
} O
ni=1 O
( O
4 O
) O
where O
ft7→s O
represents O
an O
at-based O
translation O
model O
trained O
on O
raw O
data O
for O
translating O
text O
from O
the O
target O
to O
the O
source O
language O
. O

section 5
id pdf2json/2021.acl-long.266.pdf.json
figure O
1 O
( O
c O
) O
illustrates O
the O
training O
strategy O
. O

section 5
id pdf2json/2021.acl-long.266.pdf.json
first O
, O
we O
employ O
both O
fs O
7→t O
and O
ft7→s O
at O
models O
to O
generate O
−→ O
kd O
and O
←− O
kd O
data O
, O
respectively O
. O

section 5
id pdf2json/2021.acl-long.266.pdf.json
considering O
complementarity O
of O
two O
distilled O
data O
, O
we O
combine O
−→ O
kd O
and O
←− O
kd O
as O
a O
new O
training O
data O
for O
training O
nat O
models O
. O

section 5
id pdf2json/2021.acl-long.266.pdf.json
we O
expect O
that O
1 O
) O
distilled O
data O
can O
maintain O
advantages O
of O
low-modes O
; O
2 O
) O
bidirectinoal O
distillation O
can O
recall O
more O
lfw O
links O
on O
two O
directions O
with O
better O
alignment O
quality O
, O
leading O
to O
the O
overall O
improvements O
. O

section 5
id pdf2json/2021.acl-long.266.pdf.json
besides O
, O
nguyen O
et O
al O
. O

section 5
id pdf2json/2021.acl-long.266.pdf.json
( O
2020 O
) O
claimed O
that O
combining O
different O
distilled O
data O
( O
generated O
by O
various O
models O
trained O
with O
different O
seeds O
) O
improves O
data O
diversification O
for O
nmt O
, O
and O
we O
leave O
this O
for O
future O
work O
. O

section 6
id pdf2json/2021.acl-long.266.pdf.json
we O
have O
proposed O
two O
parallel O
approaches O
to O
rejuvenate O
low-frequency O
knowledge O
from O
authentic O
( O
§2.2 O
) O
and O
synthetic O
( O
§2.3 O
) O
data O
, O
respectively O
. O

section 6
id pdf2json/2021.acl-long.266.pdf.json
intuitively O
, O
we O
combine O
both O
of O
them O
to O
further O
improve O
the O
model O
performance O
. O

section 6
id pdf2json/2021.acl-long.266.pdf.json
from O
data O
view O
, O
two O
presented O
training O
strategies O
are O
: O
raw→ O
−→kd O
( O
raw O
pretraining O
) O
and O
−→kd O
+←−kd O
( O
bidirectional O
distillation O
training O
) O
. O

section 6
id pdf2json/2021.acl-long.266.pdf.json
considering O
the O
effectiveness O
of O
pretraining O
( O
mathis O
et O
al. O
, O
2021 O
) O
and O
clean O
finetuning O
( O
wu O
et O
al. O
, O
2019 O
) O
, O
we O
introduce O
a O
combined O
pipeline O
: O
raw→ O
−→kd O
+←−kd→ O
−→kd O
as O
out O
best O
training O
strategy O
. O

section 6
id pdf2json/2021.acl-long.266.pdf.json
there O
are O
many O
possible O
ways O
to O
implement O
the O
general O
idea O
of O
combining O
two O
approaches O
. O

section 6
id pdf2json/2021.acl-long.266.pdf.json
the O
aim O
of O
this O
paper O
is O
not O
to O
explore O
the O
whole O
space O
but O
simply O
to O
show O
that O
one O
fairly O
straightforward O
implementation O
works O
well O
and O
the O
idea O
is O
reasonable O
. O

section 6
id pdf2json/2021.acl-long.266.pdf.json
nonetheless O
, O
we O
compare O
possible O
strategies O
of O
combination O
two O
approaches O
as O
well O
as O
demonstrate O
their O
complementarity O
in O
§3.3 O
. O

section 6
id pdf2json/2021.acl-long.266.pdf.json
while O
in O
main O
experiments O
( O
in O
§3.2 O
) O
, O
we O
valid O
the O
combination O
strategy O
, O
namely O
low-frequency O
rejuvenation O
( O
lfr O
) O
. O

section 6
id pdf2json/2021.acl-long.266.pdf.json
3this O
is O
different O
from O
back-translation O
( O
edunov O
et O
al. O
, O
2018 O
) O
, O
which O
is O
an O
alternative O
to O
leverage O
monolingual O
data O
. O

section 8
id pdf2json/2021.acl-long.266.pdf.json
data O
main O
experiments O
are O
conducted O
on O
four O
widely-used O
translation O
datasets O
: O
wmt14 O
englishgerman O
( O
en-de O
, O
vaswani O
et O
al O
. O

section 8
id pdf2json/2021.acl-long.266.pdf.json
2017 O
) O
, O
wmt16 O
romanian-english O
( O
ro-en O
, O
gu O
et O
al O
. O

section 8
id pdf2json/2021.acl-long.266.pdf.json
2018 O
) O
, O
wmt17 O
chinese-english O
( O
zh-en O
, O
hassan O
et O
al O
. O

section 8
id pdf2json/2021.acl-long.266.pdf.json
2018 O
) O
, O
and O
wat17 O
japanese-english O
( O
ja-en O
, O
morishita O
et O
al O
. O

section 8
id pdf2json/2021.acl-long.266.pdf.json
2017 O
) O
, O
which O
consist O
of O
4.5m O
, O
0.6m O
, O
20m O
, O
and O
2m O
sentence O
pairs O
, O
respectively O
. O

section 8
id pdf2json/2021.acl-long.266.pdf.json
we O
use O
the O
same O
validation O
and O
test O
datasets O
with O
previous O
works O
for O
fair O
comparison O
. O

section 8
id pdf2json/2021.acl-long.266.pdf.json
to O
prove O
the O
universality O
of O
our O
approach O
, O
we O
further O
experiment O
on O
different O
data O
volumes O
, O
which O
are O
sampled O
from O
wmt19 O
en-de.4 O
the O
small O
and O
medium O
corpora O
respectively O
consist O
of O
1.0m O
and O
4.5m O
sentence O
pairs O
, O
and O
large O
one O
is O
the O
whole O
dataset O
which O
contains O
36m O
sentence O
pairs O
. O

section 8
id pdf2json/2021.acl-long.266.pdf.json
we O
preprocess O
all O
data O
via O
bpe O
( O
sennrich O
et O
al. O
, O
2016 O
) O
with O
32k O
merge O
operations O
. O

section 8
id pdf2json/2021.acl-long.266.pdf.json
we O
use O
tokenized O
bleu O
( O
papineni O
et O
al. O
, O
2002 O
) O
as O
the O
evaluation O
metric O
, O
and O
sign-test O
( O
collins O
et O
al. O
, O
2005 O
) O
for O
statistical O
significance O
test O
. O

section 8
id pdf2json/2021.acl-long.266.pdf.json
the O
translation O
accuracy O
of O
lowfrequency O
words O
is O
measured O
by O
aolc O
( O
ding O
et O
al. O
, O
2021b O
) O
, O
where O
word O
alignments O
are O
established O
4http O
: O
//www.statmt.org/wmt19/ O
translation-task.html O
based O
on O
the O
widely-used O
automatic O
alignment O
tool O
giza++ O
( O
och O
and O
ney O
, O
2003 O
) O
. O

section 8
id pdf2json/2021.acl-long.266.pdf.json
models O
we O
validated O
our O
research O
hypotheses O
on O
two O
state-of-the-art O
nat O
models O
: O
• O
mask-predict O
( O
maskt O
, O
ghazvininejad O
et O
al O
. O

section 8
id pdf2json/2021.acl-long.266.pdf.json
2019 O
) O
that O
uses O
the O
conditional O
mask O
lm O
( O
devlin O
et O
al. O
, O
2019 O
) O
to O
iteratively O
generate O
the O
target O
sequence O
from O
the O
masked O
input O
. O

section 8
id pdf2json/2021.acl-long.266.pdf.json
we O
followed O
its O
optimal O
settings O
to O
keep O
the O
iteration O
number O
as O
10 O
and O
length O
beam O
as O
5 O
. O

section 8
id pdf2json/2021.acl-long.266.pdf.json
• O
levenshtein O
transformer O
( O
levt O
, O
gu O
et O
al O
. O

section 8
id pdf2json/2021.acl-long.266.pdf.json
2019 O
) O
that O
introduces O
three O
steps O
: O
deletion O
, O
placeholder O
and O
token O
prediction O
. O

section 8
id pdf2json/2021.acl-long.266.pdf.json
the O
decoding O
iterations O
adaptively O
depends O
on O
certain O
conditions O
. O

section 8
id pdf2json/2021.acl-long.266.pdf.json
we O
closely O
followed O
previous O
works O
to O
apply O
sequence-level O
knowledge O
distillation O
to O
nat O
( O
kim O
and O
rush O
, O
2016 O
) O
. O

section 8
id pdf2json/2021.acl-long.266.pdf.json
specifically O
, O
we O
train O
both O
base O
and O
big O
transformer O
as O
the O
at O
teachers O
. O

section 8
id pdf2json/2021.acl-long.266.pdf.json
for O
big O
model O
, O
we O
adopt O
large O
batch O
strategy O
( O
i.e O
. O

section 8
id pdf2json/2021.acl-long.266.pdf.json
458k O
tokens/batch O
) O
to O
optimize O
the O
performance O
. O

section 8
id pdf2json/2021.acl-long.266.pdf.json
most O
nat O
tasks O
employ O
transformer-big O
as O
their O
strong O
teacher O
except O
for O
ro-en O
and O
small O
en-de O
, O
which O
are O
distilled O
by O
transformer-base O
. O

section 8
id pdf2json/2021.acl-long.266.pdf.json
training O
traditionally O
, O
nat O
models O
are O
usually O
trained O
for O
300k O
steps O
on O
regular O
batch O
size O
( O
i.e O
. O

section 8
id pdf2json/2021.acl-long.266.pdf.json
128k O
tokens/batch O
) O
. O

section 8
id pdf2json/2021.acl-long.266.pdf.json
in O
this O
work O
, O
we O
empirically O
adopt O
large O
batch O
strategy O
( O
i.e O
. O

section 8
id pdf2json/2021.acl-long.266.pdf.json
480k O
tokens/batch O
) O
to O
reduce O
the O
training O
steps O
for O
nat O
( O
i.e O
. O

section 8
id pdf2json/2021.acl-long.266.pdf.json
70k O
) O
. O

section 8
id pdf2json/2021.acl-long.266.pdf.json
accordingly O
, O
the O
learning O
rate O
warms O
up O
to O
1× O
10−7 O
for O
10k O
steps O
, O
and O
then O
decays O
for O
60k O
steps O
with O
the O
cosine O
schedule O
( O
ro-en O
models O
only O
need O
4k O
and O
21k O
, O
respectively O
) O
. O

section 8
id pdf2json/2021.acl-long.266.pdf.json
for O
regularization O
, O
we O
tune O
the O
dropout O
rate O
from O
[ O
0.1 O
, O
0.2 O
, O
0.3 O
] O
based O
on O
validation O
performance O
in O
each O
direction O
, O
and O
apply O
weight O
decay O
with O
0.01 O
and O
label O
smoothing O
with O
= O
0.1 O
. O

section 8
id pdf2json/2021.acl-long.266.pdf.json
we O
use O
adam O
optimizer O
( O
kingma O
and O
ba O
, O
2015 O
) O
to O
train O
our O
models O
. O

section 8
id pdf2json/2021.acl-long.266.pdf.json
we O
followed O
the O
common O
practices O
( O
ghazvininejad O
et O
al. O
, O
2019 O
; O
kasai O
et O
al. O
, O
2020 O
) O
to O
evaluate O
the O
performance O
on O
an O
ensemble O
of O
top O
5 O
checkpoints O
to O
avoid O
stochasticity O
. O

section 8
id pdf2json/2021.acl-long.266.pdf.json
note O
that O
the O
total O
training O
steps O
of O
the O
proposed O
approach O
( O
in O
§2.2∼2.4 O
) O
are O
identical O
with O
those O
of O
the O
standard O
training O
( O
in O
§2.1 O
) O
. O

section 8
id pdf2json/2021.acl-long.266.pdf.json
taking O
the O
best O
training O
strategy O
( O
raw O
→ O
−→kd O
+←−kd O
→ O
−→kd O
) O
for O
example O
, O
we O
empirically O
set O
the O
training O
step O
for O
each O
stage O
is O
20k O
, O
20k O
and O
30k O
, O
respectively O
. O

section 8
id pdf2json/2021.acl-long.266.pdf.json
and O
ro-en O
models O
respectively O
need O
8k O
, O
8k O
and O
9k O
steps O
in O
corresponding O
training O
stage O
. O

section 9
id pdf2json/2021.acl-long.266.pdf.json
comparison O
with O
previous O
work O
table O
3 O
lists O
the O
results O
of O
previous O
competitive O
nat O
models O
( O
gu O
et O
al. O
, O
2018 O
; O
lee O
et O
al. O
, O
2018 O
; O
kasai O
et O
al. O
, O
2020 O
; O
gu O
et O
al. O
, O
2019 O
; O
ghazvininejad O
et O
al. O
, O
2019 O
) O
on O
the O
wmt16 O
ro-en O
and O
wmt14 O
en-de O
benchmark O
. O

section 9
id pdf2json/2021.acl-long.266.pdf.json
we O
implemented O
our O
approach O
on O
top O
of O
two O
advanced O
nat O
models O
( O
i.e O
. O

section 9
id pdf2json/2021.acl-long.266.pdf.json
mask-predict O
and O
levenshtein O
transformer O
) O
. O

section 9
id pdf2json/2021.acl-long.266.pdf.json
compared O
with O
standard O
nat O
models O
, O
our O
training O
strategy O
significantly O
and O
consistently O
improves O
translation O
performance O
( O
bleu↑ O
) O
across O
different O
language O
pairs O
and O
nat O
models O
. O

section 9
id pdf2json/2021.acl-long.266.pdf.json
besides O
, O
the O
improvements O
on O
translation O
performance O
are O
mainly O
due O
to O
a O
increase O
of O
translation O
accuracy O
on O
low-frequency O
words O
( O
alf↑ O
) O
, O
which O
reconfirms O
our O
claims O
. O

section 9
id pdf2json/2021.acl-long.266.pdf.json
for O
instance O
, O
our O
method O
significantly O
improves O
the O
standard O
maskpredict O
model O
by O
+0.8 O
bleu O
score O
with O
a O
substantial O
+3.6 O
increase O
in O
alf O
score O
. O

section 9
id pdf2json/2021.acl-long.266.pdf.json
encouragingly O
, O
our O
approach O
push O
the O
existing O
nat O
models O
to O
achieve O
new O
sota O
performances O
( O
i.e O
. O

section 9
id pdf2json/2021.acl-long.266.pdf.json
28.2 O
and O
33.9 O
bleu O
on O
en-de O
and O
ro-en O
, O
respectively O
) O
. O

section 9
id pdf2json/2021.acl-long.266.pdf.json
it O
is O
worth O
noting O
that O
our O
data-level O
approaches O
neither O
modify O
model O
architecture O
nor O
add O
extra O
training O
loss O
, O
thus O
do O
not O
increase O
any O
latency O
( O
“ O
speed O
” O
) O
, O
maintaining O
the O
intrinsic O
advantages O
of O
non-autoregressive O
generation O
. O

section 9
id pdf2json/2021.acl-long.266.pdf.json
we O
must O
admit O
that O
our O
strategy O
indeed O
increase O
the O
amount O
of O
computing O
resources O
due O
to O
that O
we O
should O
train O
ft7→s O
at O
teachers O
for O
building O
←− O
kd O
data O
. O

section 9
id pdf2json/2021.acl-long.266.pdf.json
results O
on O
other O
language O
pairs O
table O
4 O
lists O
the O
results O
of O
nat O
models O
on O
zh-en O
and O
ja-en O
language O
pairs O
, O
which O
belong O
to O
different O
language O
families O
( O
i.e O
. O

section 9
id pdf2json/2021.acl-long.266.pdf.json
indo-european O
, O
sino-tibetan O
and O
japonic O
) O
. O

section 9
id pdf2json/2021.acl-long.266.pdf.json
compared O
with O
baselines O
, O
our O
method O
significantly O
and O
incrementally O
improves O
the O
translation O
quality O
in O
all O
cases O
. O

section 9
id pdf2json/2021.acl-long.266.pdf.json
for O
zh-en O
, O
lfr O
achieves O
on O
average O
+0.8 O
bleu O
improvement O
over O
the O
traditional O
training O
, O
along O
with O
increasing O
on O
average O
+3.0 O
% O
accuracy O
on O
low-frequency O
word O
translation O
. O

section 9
id pdf2json/2021.acl-long.266.pdf.json
for O
long-distance O
language O
pair O
ja-en O
, O
our O
method O
still O
improves O
the O
nat O
model O
by O
on O
average O
+0.7 O
bleu O
point O
with O
on O
average O
+2.2 O
alf O
. O

section 9
id pdf2json/2021.acl-long.266.pdf.json
furthermore O
, O
nat O
models O
with O
the O
proposed O
training O
strategy O
perform O
closely O
to O
their O
at O
teachers O
( O
i.e O
. O

section 9
id pdf2json/2021.acl-long.266.pdf.json
0.2 O
∆bleu O
) O
. O

section 9
id pdf2json/2021.acl-long.266.pdf.json
this O
shows O
the O
effectiveness O
and O
universality O
of O
our O
method O
across O
language O
pairs O
. O

section 9
id pdf2json/2021.acl-long.266.pdf.json
results O
on O
domain O
shift O
scenario O
the O
lexical O
choice O
must O
be O
informed O
by O
linguistic O
knowledge O
of O
how O
the O
translation O
model O
’ O
s O
input O
data O
maps O
onto O
words O
in O
the O
target O
domain O
. O

section 9
id pdf2json/2021.acl-long.266.pdf.json
since O
low-frequency O
words O
get O
lost O
in O
traditional O
nat O
models O
, O
the O
problem O
of O
lexical O
choice O
is O
more O
severe O
under O
domain O
shift O
scenario O
( O
i.e O
. O

section 9
id pdf2json/2021.acl-long.266.pdf.json
models O
are O
trained O
on O
one O
domain O
but O
tested O
on O
other O
domains O
) O
. O

section 9
id pdf2json/2021.acl-long.266.pdf.json
thus O
, O
we O
conduct O
evaluation O
on O
wmt14 O
en-de O
models O
over O
five O
out-of-domain O
test O
sets O
( O
müller O
et O
al. O
, O
2020 O
) O
, O
including O
law O
, O
medicine O
, O
it O
, O
koran O
and O
movie O
subtitle O
domains O
. O

section 9
id pdf2json/2021.acl-long.266.pdf.json
as O
shown O
in O
table O
5 O
, O
standard O
nat O
models O
suffer O
large O
performance O
drops O
in O
terms O
of O
bleu O
score O
( O
i.e O
. O

section 9
id pdf2json/2021.acl-long.266.pdf.json
on O
average O
-2.9 O
bleu O
over O
at O
model O
) O
. O

section 9
id pdf2json/2021.acl-long.266.pdf.json
by O
observing O
these O
outputs O
, O
we O
found O
a O
large O
amount O
of O
translation O
errors O
on O
low-frequency O
words O
, O
most O
of O
which O
are O
domain-specific O
terminologies O
. O

section 9
id pdf2json/2021.acl-long.266.pdf.json
in O
contrast O
, O
our O
approach O
improves O
translation O
quality O
( O
i.e O
. O

section 9
id pdf2json/2021.acl-long.266.pdf.json
on O
average O
-1.4 O
bleu O
over O
at O
model O
) O
by O
rejuvenating O
low-frequency O
words O
to O
a O
certain O
extent O
, O
showing O
that O
lfr O
increases O
the O
domain O
robustness O
of O
nat O
models O
. O

section 9
id pdf2json/2021.acl-long.266.pdf.json
results O
on O
different O
data O
scales O
to O
confirm O
the O
effectiveness O
of O
our O
method O
across O
different O
data O
sizes O
, O
we O
further O
experiment O
on O
three O
en-de O
datasets O
at O
different O
scale O
. O

section 9
id pdf2json/2021.acl-long.266.pdf.json
the O
small- O
and O
mediumscale O
training O
data O
are O
randomly O
sampled O
from O
wm19 O
en-de O
corpus O
, O
containing O
about O
1.0m O
and O
4.5m O
sentence O
pairs O
, O
respectively O
. O

section 9
id pdf2json/2021.acl-long.266.pdf.json
the O
large-scale O
one O
is O
collected O
from O
wmt19 O
, O
which O
consists O
of O
36m O
sentence O
pairs O
. O

section 9
id pdf2json/2021.acl-long.266.pdf.json
we O
report O
the O
bleu O
scores O
on O
same O
testset O
newstest2019 O
for O
fair O
comparison O
. O

section 9
id pdf2json/2021.acl-long.266.pdf.json
we O
employs O
base O
model O
to O
train O
the O
small-scale O
at O
teacher O
, O
and O
big O
model O
with O
large O
batch O
strategy O
( O
i.e O
. O

section 9
id pdf2json/2021.acl-long.266.pdf.json
458k O
tokens/batch O
) O
to O
build O
the O
at O
teachers O
for O
medium- O
and O
large-scale O
. O

section 9
id pdf2json/2021.acl-long.266.pdf.json
as O
seen O
in O
table O
6 O
, O
our O
simple O
training O
recipe O
boost O
performances O
for O
nat O
models O
across O
different O
size O
of O
datasets O
, O
especially O
on O
large O
scale O
( O
+0.9 O
) O
, O
showing O
the O
robustness O
and O
effectiveness O
of O
our O
approach O
. O

section 9
id pdf2json/2021.acl-long.266.pdf.json
complementary O
to O
related O
work O
ding O
et O
al O
. O

section 9
id pdf2json/2021.acl-long.266.pdf.json
( O
2021b O
) O
is O
relevant O
to O
our O
work O
, O
which O
introduced O
an O
extra O
bilingual O
data-dependent O
prior O
objective O
to O
augment O
nat O
models O
the O
ability O
to O
learn O
lowfrequency O
words O
in O
raw O
data O
. O

section 9
id pdf2json/2021.acl-long.266.pdf.json
our O
method O
is O
complementary O
to O
theirs O
due O
to O
that O
we O
only O
change O
data O
and O
training O
strategies O
( O
model-agnostic O
) O
. O

section 9
id pdf2json/2021.acl-long.266.pdf.json
as O
shown O
in O
table O
7 O
, O
two O
approaches O
yield O
comparable O
performance O
in O
terms O
of O
bleu O
and O
alf O
. O

section 9
id pdf2json/2021.acl-long.266.pdf.json
besides O
, O
combination O
can O
further O
improve O
bleu O
as O
well O
as O
alf O
scores O
( O
i.e O
. O

section 9
id pdf2json/2021.acl-long.266.pdf.json
+0.3 O
and O
+0.6 O
) O
. O

section 9
id pdf2json/2021.acl-long.266.pdf.json
this O
illustrates O
the O
complementarity O
of O
model-level O
and O
data-level O
approaches O
on O
rejuvenating O
low-frequency O
knowldege O
for O
nat O
models O
. O

section 10
id pdf2json/2021.acl-long.266.pdf.json
we O
conducted O
extensive O
analyses O
to O
better O
understand O
our O
approach O
. O

section 10
id pdf2json/2021.acl-long.266.pdf.json
all O
results O
are O
reported O
on O
the O
mask-predict O
models O
. O

section 10
id pdf2json/2021.acl-long.266.pdf.json
accuracy O
of O
lexical O
choice O
to O
understand O
where O
the O
performance O
gains O
come O
from O
, O
we O
conduct O
fine-grained O
analysis O
on O
lexical O
choice O
. O

section 10
id pdf2json/2021.acl-long.266.pdf.json
we O
divide O
“ O
all O
” O
tokens O
into O
three O
categories O
based O
on O
their O
frequency O
, O
including O
“ O
high O
” O
, O
“ O
medium O
” O
and O
“ O
low O
” O
. O

section 10
id pdf2json/2021.acl-long.266.pdf.json
following O
ding O
et O
al O
. O

section 10
id pdf2json/2021.acl-long.266.pdf.json
( O
2021b O
) O
, O
we O
measure O
the O
accuracy O
of O
lexical O
choice O
on O
different O
frequency O
of O
words O
. O

section 10
id pdf2json/2021.acl-long.266.pdf.json
table O
8 O
shows O
the O
results O
. O

section 10
id pdf2json/2021.acl-long.266.pdf.json
takeaway O
: O
the O
majority O
of O
improvements O
on O
translation O
accuracy O
is O
from O
the O
low-frequency O
words O
, O
confirming O
our O
hypothesis O
. O

section 10
id pdf2json/2021.acl-long.266.pdf.json
low-frequency O
words O
in O
output O
we O
expect O
to O
recall O
more O
low-frequency O
words O
in O
translation O
output O
. O

section 10
id pdf2json/2021.acl-long.266.pdf.json
as O
shown O
in O
table O
9 O
, O
we O
calculate O
the O
ratio O
of O
low-frequency O
words O
in O
generated O
sentences O
. O

section 10
id pdf2json/2021.acl-long.266.pdf.json
as O
seen O
, O
kd O
biases O
the O
nat O
model O
towards O
gen- O
model O
en-de O
zh-en O
ja-en O
all O
high O
med O
. O

section 10
id pdf2json/2021.acl-long.266.pdf.json
low O
all O
high O
med O
. O

section 10
id pdf2json/2021.acl-long.266.pdf.json
low O
all O
high O
med O
. O

section 10
id pdf2json/2021.acl-long.266.pdf.json
low O
maskt O
( O
raw O
) O
74.3 O
75.9 O
74.6 O
72.5 O
68.5 O
71.5 O
68.3 O
65.1 O
73.1 O
75.5 O
74.7 O
69.1 O
maskt O
( O
kd O
) O
76.3 O
82.4 O
78.3 O
68.4 O
72.7 O
81.4 O
75.2 O
61.5 O
75.3 O
82.8 O
76.3 O
66.9 O
erating O
high-frequency O
tokens O
( O
low O
freq.↓ O
) O
while O
our O
method O
can O
not O
only O
correct O
this O
bias O
( O
on O
average O
+18 O
% O
and O
+26 O
% O
relative O
changes O
for O
+rawpretrain O
and O
+bi-distillation O
) O
, O
but O
also O
enhance O
translation O
( O
bleu↑ O
in O
table O
4 O
) O
. O

section 10
id pdf2json/2021.acl-long.266.pdf.json
takeaway O
: O
our O
method O
generates O
translations O
that O
contain O
more O
low-frequency O
words O
. O

section 10
id pdf2json/2021.acl-long.266.pdf.json
effects O
of O
variant O
training O
strategies O
as O
discussed O
in O
§2.4 O
, O
we O
carefully O
investigate O
alternative O
training O
approaches O
in O
table O
10 O
. O

section 10
id pdf2json/2021.acl-long.266.pdf.json
we O
make O
the O
total O
training O
step O
identical O
to O
that O
of O
vanilla O
nat O
models O
, O
and O
report O
both O
bleu O
and O
alf O
scores O
. O

section 10
id pdf2json/2021.acl-long.266.pdf.json
as O
seen O
, O
all O
variant O
strategies O
perform O
better O
than O
the O
standard O
kd O
method O
in O
terms O
both O
bleu O
and O
alf O
scores O
, O
confirming O
the O
necessity O
of O
our O
work O
. O

section 10
id pdf2json/2021.acl-long.266.pdf.json
takeaway O
: O
1 O
) O
pretraining O
is O
more O
effective O
than O
combination O
on O
utilizing O
data O
manipulation O
strategies O
; O
2 O
) O
raw O
data O
and O
bidirectional O
distilled O
data O
are O
complementary O
to O
each O
other O
; O
3 O
) O
it O
is O
indispensable O
to O
finetune O
models O
on O
−→ O
kd O
in O
the O
last O
stage O
. O

section 10
id pdf2json/2021.acl-long.266.pdf.json
our O
approach O
works O
for O
at O
models O
although O
our O
work O
is O
designed O
for O
nat O
models O
, O
we O
also O
investigated O
whether O
our O
lft O
method O
works O
for O
general O
cases O
, O
e.g O
. O

section 10
id pdf2json/2021.acl-long.266.pdf.json
autoregressive O
models O
. O

section 10
id pdf2json/2021.acl-long.266.pdf.json
we O
used O
transformer-big O
as O
the O
teacher O
model O
. O

section 10
id pdf2json/2021.acl-long.266.pdf.json
for O
fair O
comparison O
, O
we O
leverage O
the O
transformerbase O
as O
the O
student O
model O
, O
which O
shares O
the O
same O
model O
capacity O
with O
nat O
student O
( O
i.e O
. O

section 10
id pdf2json/2021.acl-long.266.pdf.json
maskt O
) O
. O

section 10
id pdf2json/2021.acl-long.266.pdf.json
the O
result O
lists O
in O
table O
11 O
. O

section 10
id pdf2json/2021.acl-long.266.pdf.json
as O
seen O
, O
at O
models O
also O
suffer O
from O
the O
problem O
of O
low-frequency O
words O
when O
using O
knowledge O
distillation O
, O
and O
our O
approach O
also O
works O
for O
them O
. O

section 10
id pdf2json/2021.acl-long.266.pdf.json
takeaway O
: O
our O
method O
works O
well O
for O
general O
cases O
through O
rejuvenating O
more O
low-frequency O
words O
. O

section 11
id pdf2json/2021.acl-long.266.pdf.json
low-frequency O
words O
benefiting O
from O
continuous O
representation O
learned O
from O
the O
training O
data O
, O
nmt O
models O
have O
shown O
the O
promising O
performance O
. O

section 11
id pdf2json/2021.acl-long.266.pdf.json
however O
, O
koehn O
and O
knowles O
( O
2017 O
) O
point O
that O
low-frequency O
words O
translation O
is O
still O
one O
of O
the O
key O
challenges O
for O
nmt O
according O
to O
the O
zipf O
’ O
s O
law O
( O
zipf O
, O
1949 O
) O
. O

section 11
id pdf2json/2021.acl-long.266.pdf.json
for O
at O
models O
, O
arthur O
et O
al O
. O

section 11
id pdf2json/2021.acl-long.266.pdf.json
( O
2016 O
) O
address O
this O
problem O
by O
integrating O
a O
count-based O
lexicon O
, O
and O
nguyen O
and O
chiang O
( O
2018 O
) O
propose O
an O
additional O
lexical O
model O
, O
which O
is O
jointly O
trained O
with O
the O
at O
model O
. O

section 11
id pdf2json/2021.acl-long.266.pdf.json
recently O
, O
gu O
et O
al O
. O

section 11
id pdf2json/2021.acl-long.266.pdf.json
( O
2020 O
) O
adaptively O
re-weight O
the O
rare O
words O
during O
training O
. O

section 11
id pdf2json/2021.acl-long.266.pdf.json
the O
lexical O
choice O
problem O
is O
more O
serious O
for O
nat O
models O
, O
since O
1 O
) O
the O
lexical O
choice O
errors O
( O
low-resource O
words O
in O
particular O
) O
of O
at O
distillation O
will O
propagate O
to O
nat O
models O
; O
and O
2 O
) O
nat O
lacks O
target-side O
dependencies O
thus O
misses O
necessary O
target-side O
context O
. O

section 11
id pdf2json/2021.acl-long.266.pdf.json
in O
this O
work O
, O
we O
alleviate O
this O
problem O
by O
solving O
the O
first O
challenge O
. O

section 11
id pdf2json/2021.acl-long.266.pdf.json
data O
manipulation O
our O
work O
is O
related O
to O
previous O
studies O
on O
manipulating O
training O
data O
for O
nmt O
. O

section 11
id pdf2json/2021.acl-long.266.pdf.json
bogoychev O
and O
sennrich O
( O
2019 O
) O
show O
that O
forwardand O
backward-translations O
( O
ft/ O
bt O
) O
could O
both O
boost O
the O
model O
performances O
, O
where O
ft O
plays O
the O
role O
of O
domain O
adaptation O
and O
bt O
makes O
the O
translation O
fluent O
. O

section 11
id pdf2json/2021.acl-long.266.pdf.json
fadaee O
and O
monz O
( O
2018 O
) O
sample O
the O
monolingual O
data O
with O
more O
difficult O
words O
( O
e.g O
. O

section 11
id pdf2json/2021.acl-long.266.pdf.json
rare O
words O
) O
to O
perform O
bt O
, O
achieving O
significant O
improvements O
compared O
with O
randomly O
sampled O
bt O
. O

section 11
id pdf2json/2021.acl-long.266.pdf.json
nguyen O
et O
al O
. O

section 11
id pdf2json/2021.acl-long.266.pdf.json
( O
2020 O
) O
diversify O
the O
data O
by O
applying O
ft O
and O
bt O
multiply O
times O
. O

section 11
id pdf2json/2021.acl-long.266.pdf.json
however O
, O
different O
from O
at O
, O
the O
prerequisite O
of O
training O
a O
well-performed O
nat O
model O
is O
to O
perform O
kd O
. O

section 11
id pdf2json/2021.acl-long.266.pdf.json
we O
compared O
with O
related O
works O
in O
table O
10 O
and O
found O
that O
our O
approach O
consistently O
outperforms O
them O
. O

section 11
id pdf2json/2021.acl-long.266.pdf.json
note O
that O
all O
the O
ablation O
studies O
focus O
on O
exploiting O
the O
parallel O
data O
without O
augmenting O
additional O
data O
. O

section 11
id pdf2json/2021.acl-long.266.pdf.json
non-autoregressive O
translation O
a O
variety O
of O
approaches O
have O
been O
exploited O
to O
bridge O
the O
performance O
gap O
between O
nat O
and O
at O
models O
. O

section 11
id pdf2json/2021.acl-long.266.pdf.json
some O
researchers O
proposed O
new O
model O
architectures O
( O
lee O
et O
al. O
, O
2018 O
; O
ghazvininejad O
et O
al. O
, O
2019 O
; O
gu O
et O
al. O
, O
2019 O
; O
kasai O
et O
al. O
, O
2020 O
) O
, O
aided O
with O
additional O
signals O
( O
wang O
et O
al. O
, O
2019 O
; O
ran O
et O
al. O
, O
2019 O
; O
ding O
et O
al. O
, O
2020 O
) O
, O
introduced O
sequential O
information O
( O
wei O
et O
al. O
, O
2019 O
; O
shao O
et O
al. O
, O
2019 O
; O
guo O
et O
al. O
, O
2020 O
; O
hao O
et O
al. O
, O
2021 O
) O
, O
and O
explored O
advanced O
training O
objectives O
( O
ghazvininejad O
et O
al. O
, O
2020 O
; O
du O
et O
al. O
, O
2021 O
) O
. O

section 11
id pdf2json/2021.acl-long.266.pdf.json
our O
work O
is O
close O
to O
the O
research O
line O
on O
training O
methods O
. O

section 11
id pdf2json/2021.acl-long.266.pdf.json
ding O
et O
al O
. O

section 11
id pdf2json/2021.acl-long.266.pdf.json
( O
2021b O
) O
revealed O
the O
low-frequency O
word O
problem O
in O
distilled O
training O
data O
, O
and O
introduced O
an O
extra O
kullback-leibler O
divergence O
term O
derived O
by O
comparing O
the O
lexical O
choice O
of O
nat O
model O
and O
that O
embedded O
in O
the O
raw O
data O
. O

section 11
id pdf2json/2021.acl-long.266.pdf.json
ding O
et O
al O
. O

section 11
id pdf2json/2021.acl-long.266.pdf.json
( O
2021a O
) O
propose O
a O
simple O
and O
effective O
training O
strategy O
, O
which O
progressively O
feeds O
different O
granularity O
of O
data O
into O
nat O
models O
by O
leveraging O
curriculum O
learning O
. O

section 12
id pdf2json/2021.acl-long.266.pdf.json
in O
this O
study O
, O
we O
propose O
simple O
and O
effective O
training O
strategies O
to O
rejuvenate O
the O
low-frequency O
information O
in O
the O
raw O
data O
. O

section 12
id pdf2json/2021.acl-long.266.pdf.json
experiments O
show O
that O
our O
approach O
consistently O
and O
significantly O
improves O
translation O
performance O
across O
language O
pairs O
and O
model O
architectures O
. O

section 12
id pdf2json/2021.acl-long.266.pdf.json
notably O
, O
domain O
shift O
is O
an O
extreme O
scenario O
to O
diagnose O
low-frequency O
translation O
, O
and O
our O
method O
significant O
improves O
them O
. O

section 12
id pdf2json/2021.acl-long.266.pdf.json
extensive O
analyses O
reveal O
that O
our O
method O
improves O
the O
accuracy O
of O
lexical O
choices O
for O
low-frequency O
source O
words O
, O
recalling O
more O
low-frequency O
words O
in O
translations O
as O
well O
, O
which O
confirms O
our O
claim O
. O

section 13
id pdf2json/2021.acl-long.266.pdf.json
we O
are O
grateful O
to O
the O
anonymous O
reviewers O
and O
the O
area O
chair O
for O
their O
insightful O
comments O
and O
suggestions O
. O

section 13
id pdf2json/2021.acl-long.266.pdf.json
xuebo O
liu O
and O
derek O
f. O
wong O
were O
supported O
in O
part O
by O
the O
science O
and O
technology O
development O
fund O
, O
macau O
sar O
( O
grant O
no O
. O

section 13
id pdf2json/2021.acl-long.266.pdf.json
0101/2019/a2 O
) O
, O
and O
the O
multi-year O
research O
grant O
from O
the O
university O
of O
macau O
( O
grant O
no O
. O

section 13
id pdf2json/2021.acl-long.266.pdf.json
myrg2020-00054-fst O
) O
. O

section TITLE
id pdf2json/P19-1004.pdf.json
do O
neural O
dialog O
systems O
use O
the O
conversation O
history O
effectively O
? O

section TITLE
id pdf2json/P19-1004.pdf.json
an O
empirical O
study O

section ABSTRACT
id pdf2json/P19-1004.pdf.json
neural O
generative O
models O
have O
been O
become O
increasingly O
popular O
when O
building O
conversational O
agents O
. O

section ABSTRACT
id pdf2json/P19-1004.pdf.json
they O
offer O
flexibility O
, O
can O
be O
easily O
adapted O
to O
new O
domains O
, O
and O
require O
minimal O
domain O
engineering O
. O

section ABSTRACT
id pdf2json/P19-1004.pdf.json
a O
common O
criticism O
of O
these O
systems O
is O
that O
they O
seldom O
understand O
or O
use O
the O
available O
dialog O
history O
effectively O
. O

section ABSTRACT
id pdf2json/P19-1004.pdf.json
in O
this O
paper O
, O
we O
take O
an O
empirical O
approach O
to O
understanding O
how O
these O
models O
use O
the O
available O
dialog O
history O
by O
studying O
the O
sensitivity O
of O
the O
models O
to O
artificially O
introduced O
unnatural O
changes O
or O
perturbations O
to O
their O
context O
at O
test O
time O
. O

section ABSTRACT
id pdf2json/P19-1004.pdf.json
we O
experiment O
with O
10 O
different O
types O
of O
perturbations O
on O
4 O
multi-turn O
dialog O
datasets O
and O
find O
that O
commonly O
used O
neural O
dialog O
architectures O
like O
recurrent O
and O
transformer-based O
seq2seq O
models O
are O
rarely O
sensitive O
to O
most O
perturbations O
such O
as O
missing O
or O
reordering O
utterances O
, O
shuffling O
words O
, O
etc O
. O

section ABSTRACT
id pdf2json/P19-1004.pdf.json
also O
, O
by O
open-sourcing O
our O
code O
, O
we O
believe O
that O
it O
will O
serve O
as O
a O
useful O
diagnostic O
tool O
for O
evaluating O
dialog O
systems O
in O
the O
future O
1 O
. O

section 0
id pdf2json/P19-1004.pdf.json
proceedings O
of O
the O
57th O
annual O
meeting O
of O
the O
association O
for O
computational O
linguistics O
, O
pages O
32–37 O
florence O
, O
italy O
, O
july O
28 O
- O
august O
2 O
, O
2019. O
c©2019 O
association O
for O
computational O
linguistics O
32 O

section 1
id pdf2json/P19-1004.pdf.json
with O
recent O
advancements O
in O
generative O
models O
of O
text O
( O
wu O
et O
al. O
, O
2016 O
; O
vaswani O
et O
al. O
, O
2017 O
; O
radford O
et O
al. O
, O
2018 O
) O
, O
neural O
approaches O
to O
building O
chit-chat O
and O
goal-oriented O
conversational O
agents O
( O
sordoni O
et O
al. O
, O
2015 O
; O
vinyals O
and O
le O
, O
2015 O
; O
serban O
et O
al. O
, O
2016 O
; O
bordes O
and O
weston O
, O
2016 O
; O
serban O
et O
al. O
, O
2017b O
) O
has O
gained O
popularity O
with O
the O
hope O
that O
advancements O
in O
tasks O
like O
machine O
translation O
( O
bahdanau O
et O
al. O
, O
2015 O
) O
, O
abstractive O
summarization O
( O
see O
et O
al. O
, O
2017 O
) O
should O
translate O
to O
dialog O
systems O
as O
well O
. O

section 1
id pdf2json/P19-1004.pdf.json
while O
these O
models O
have O
demonstrated O
the O
ability O
to O
generate O
fluent O
responses O
, O
∗corresponding O
author O
: O
chinnadhurai O
@ O
gmail.com O
1code O
is O
available O
at O
https O
: O
//github.com/ O
chinnadhurai/parlai/ O
they O
still O
lack O
the O
ability O
to O
“ O
understand O
” O
and O
process O
the O
dialog O
history O
to O
produce O
coherent O
and O
interesting O
responses O
. O

section 1
id pdf2json/P19-1004.pdf.json
they O
often O
produce O
boring O
and O
repetitive O
responses O
like O
“ O
thank O
you. O
” O
( O
li O
et O
al. O
, O
2015 O
; O
serban O
et O
al. O
, O
2017a O
) O
or O
meander O
away O
from O
the O
topic O
of O
conversation O
. O

section 1
id pdf2json/P19-1004.pdf.json
this O
has O
been O
often O
attributed O
to O
the O
manner O
and O
extent O
to O
which O
these O
models O
use O
the O
dialog O
history O
when O
generating O
responses O
. O

section 1
id pdf2json/P19-1004.pdf.json
however O
, O
there O
has O
been O
little O
empirical O
investigation O
to O
validate O
these O
speculations O
. O

section 1
id pdf2json/P19-1004.pdf.json
in O
this O
work O
, O
we O
take O
a O
step O
in O
that O
direction O
and O
confirm O
some O
of O
these O
speculations O
, O
showing O
that O
models O
do O
not O
make O
use O
of O
a O
lot O
of O
the O
information O
available O
to O
it O
, O
by O
subjecting O
the O
dialog O
history O
to O
a O
variety O
of O
synthetic O
perturbations O
. O

section 1
id pdf2json/P19-1004.pdf.json
we O
then O
empirically O
observe O
how O
recurrent O
( O
sutskever O
et O
al. O
, O
2014 O
) O
and O
transformer-based O
( O
vaswani O
et O
al. O
, O
2017 O
) O
sequence-to-sequence O
( O
seq2seq O
) O
models O
respond O
to O
these O
changes O
. O

section 1
id pdf2json/P19-1004.pdf.json
the O
central O
premise O
of O
this O
work O
is O
that O
models O
make O
minimal O
use O
of O
certain O
types O
of O
information O
if O
they O
are O
insensitive O
to O
perturbations O
that O
destroy O
them O
. O

section 1
id pdf2json/P19-1004.pdf.json
worryingly O
, O
we O
find O
that O
1 O
) O
both O
recurrent O
and O
transformer-based O
seq2seq O
models O
are O
insensitive O
to O
most O
kinds O
of O
perturbations O
considered O
in O
this O
work O
2 O
) O
both O
are O
particularly O
insensitive O
even O
to O
extreme O
perturbations O
such O
as O
randomly O
shuffling O
or O
reversing O
words O
within O
every O
utterance O
in O
the O
conversation O
history O
( O
see O
table O
1 O
) O
and O
3 O
) O
recurrent O
models O
are O
more O
sensitive O
to O
the O
ordering O
of O
utterances O
within O
the O
dialog O
history O
, O
suggesting O
that O
they O
could O
be O
modeling O
conversation O
dynamics O
better O
than O
transformers O
. O

section 2
id pdf2json/P19-1004.pdf.json
since O
this O
work O
aims O
at O
investigating O
and O
gaining O
an O
understanding O
of O
the O
kinds O
of O
information O
a O
generative O
neural O
response O
model O
learns O
to O
use O
, O
the O
most O
relevant O
pieces O
of O
work O
are O
where O
sim- O
ilar O
analyses O
have O
been O
carried O
out O
to O
understand O
the O
behavior O
of O
neural O
models O
in O
other O
settings O
. O

section 2
id pdf2json/P19-1004.pdf.json
an O
investigation O
into O
how O
lstm O
based O
unconditional O
language O
models O
use O
available O
context O
was O
carried O
out O
by O
khandelwal O
et O
al O
. O

section 2
id pdf2json/P19-1004.pdf.json
( O
2018 O
) O
. O

section 2
id pdf2json/P19-1004.pdf.json
they O
empirically O
demonstrate O
that O
models O
are O
sensitive O
to O
perturbations O
only O
in O
the O
nearby O
context O
and O
typically O
use O
only O
about O
150 O
words O
of O
context O
. O

section 2
id pdf2json/P19-1004.pdf.json
on O
the O
other O
hand O
, O
in O
conditional O
language O
modeling O
tasks O
like O
machine O
translation O
, O
models O
are O
adversely O
affected O
by O
both O
synthetic O
and O
natural O
noise O
introduced O
anywhere O
in O
the O
input O
( O
belinkov O
and O
bisk O
, O
2017 O
) O
. O

section 2
id pdf2json/P19-1004.pdf.json
understanding O
what O
information O
is O
learned O
or O
contained O
in O
the O
representations O
of O
neural O
networks O
has O
also O
been O
studied O
by O
“ O
probing O
” O
them O
with O
linear O
or O
deep O
models O
( O
adi O
et O
al. O
, O
2016 O
; O
subramanian O
et O
al. O
, O
2018 O
; O
conneau O
et O
al. O
, O
2018 O
) O
. O

section 2
id pdf2json/P19-1004.pdf.json
several O
works O
have O
recently O
pointed O
out O
the O
presence O
of O
annotation O
artifacts O
in O
common O
text O
and O
multi-modal O
benchmarks O
. O

section 2
id pdf2json/P19-1004.pdf.json
for O
example O
, O
gururangan O
et O
al O
. O

section 2
id pdf2json/P19-1004.pdf.json
( O
2018 O
) O
demonstrate O
that O
hypothesisonly O
baselines O
for O
natural O
language O
inference O
obtain O
results O
significantly O
better O
than O
random O
guessing O
. O

section 2
id pdf2json/P19-1004.pdf.json
kaushik O
and O
lipton O
( O
2018 O
) O
report O
that O
reading O
comprehension O
systems O
can O
often O
ignore O
the O
entire O
question O
or O
use O
only O
the O
last O
sentence O
of O
a O
document O
to O
answer O
questions O
. O

section 2
id pdf2json/P19-1004.pdf.json
anand O
et O
al O
. O

section 2
id pdf2json/P19-1004.pdf.json
( O
2018 O
) O
show O
that O
an O
agent O
that O
does O
not O
navigate O
or O
even O
see O
the O
world O
around O
it O
can O
answer O
questions O
about O
it O
as O
well O
as O
one O
that O
does O
. O

section 2
id pdf2json/P19-1004.pdf.json
these O
pieces O
of O
work O
suggest O
that O
while O
neural O
methods O
have O
the O
potential O
to O
learn O
the O
task O
specified O
, O
its O
design O
could O
lead O
them O
to O
do O
so O
in O
a O
manner O
that O
doesn O
’ O
t O
use O
all O
of O
the O
available O
information O
within O
the O
task O
. O

section 2
id pdf2json/P19-1004.pdf.json
recent O
work O
has O
also O
investigated O
the O
inductive O
biases O
that O
different O
sequence O
models O
learn O
. O

section 2
id pdf2json/P19-1004.pdf.json
for O
example O
, O
tran O
et O
al O
. O

section 2
id pdf2json/P19-1004.pdf.json
( O
2018 O
) O
find O
that O
recurrent O
models O
are O
better O
at O
modeling O
hierarchical O
structure O
while O
tang O
et O
al O
. O

section 2
id pdf2json/P19-1004.pdf.json
( O
2018 O
) O
find O
that O
feedforward O
architectures O
like O
the O
transformer O
and O
convolutional O
models O
are O
not O
better O
than O
rnns O
at O
modeling O
long-distance O
agreement O
. O

section 2
id pdf2json/P19-1004.pdf.json
transformers O
however O
excel O
at O
word-sense O
disambiguation O
. O

section 2
id pdf2json/P19-1004.pdf.json
we O
analyze O
whether O
the O
choice O
of O
architecture O
and O
the O
use O
of O
an O
attention O
mechanism O
affect O
the O
way O
in O
which O
dialog O
systems O
use O
information O
available O
to O
them O
. O

section 3
id pdf2json/P19-1004.pdf.json
following O
the O
recent O
line O
of O
work O
on O
generative O
dialog O
systems O
, O
we O
treat O
the O
problem O
of O
generating O
an O
appropriate O
response O
given O
a O
conversation O
history O
as O
a O
conditional O
language O
modeling O
problem O
. O

section 3
id pdf2json/P19-1004.pdf.json
specifically O
we O
want O
to O
learn O
a O
conditional O
probability O
distribution O
pθ O
( O
y|x O
) O
where O
y O
is O
a O
reasonable O
response O
given O
the O
conversation O
history O
x O
. O

section 3
id pdf2json/P19-1004.pdf.json
the O
conversation O
history O
is O
typically O
represented O
as O
a O
sequence O
of O
utterances O
x1 O
, O
x2 O
, O
. O

section 3
id pdf2json/P19-1004.pdf.json
. O

section 3
id pdf2json/P19-1004.pdf.json
.xn O
, O
where O
each O
utterance O
xi O
itself O
is O
comprised O
of O
a O
sequence O
of O
words O
xi1 O
, O
xi2 O
. O

section 3
id pdf2json/P19-1004.pdf.json
. O

section 3
id pdf2json/P19-1004.pdf.json
. O

section 3
id pdf2json/P19-1004.pdf.json
xik O
. O

section 3
id pdf2json/P19-1004.pdf.json
the O
response O
y O
is O
a O
single O
utterance O
also O
comprised O
of O
a O
sequence O
of O
words O
y1 O
, O
y2 O
. O

section 3
id pdf2json/P19-1004.pdf.json
. O

section 3
id pdf2json/P19-1004.pdf.json
. O

section 3
id pdf2json/P19-1004.pdf.json
ym O
. O

section 3
id pdf2json/P19-1004.pdf.json
the O
overall O
conditional O
probability O
is O
factorized O
autoregressively O
as O
pθ O
( O
y|x O
) O
= O
n∏ O
i=1 O
pθ O
( O
yi|y O
< O
i O
, O
x1 O
. O

section 3
id pdf2json/P19-1004.pdf.json
. O

section 3
id pdf2json/P19-1004.pdf.json
.xn O
) O
pθ O
, O
in O
this O
work O
, O
is O
parameterized O
by O
a O
recurrent O
or O
transformer-based O
seq2seq O
model O
. O

section 3
id pdf2json/P19-1004.pdf.json
the O
crux O
of O
this O
work O
is O
to O
study O
how O
the O
learned O
probability O
distribution O
behaves O
as O
we O
artificially O
perturb O
the O
conversation O
history O
x1 O
, O
. O

section 3
id pdf2json/P19-1004.pdf.json
. O

section 3
id pdf2json/P19-1004.pdf.json
.xn O
. O

section 3
id pdf2json/P19-1004.pdf.json
we O
measure O
behavior O
by O
looking O
at O
how O
much O
the O
per-token O
perplexity O
increases O
under O
these O
changes O
. O

section 3
id pdf2json/P19-1004.pdf.json
for O
example O
, O
one O
could O
think O
of O
shuffling O
the O
order O
in O
which O
x1 O
. O

section 3
id pdf2json/P19-1004.pdf.json
. O

section 3
id pdf2json/P19-1004.pdf.json
.xn O
is O
presented O
to O
the O
model O
and O
observe O
how O
much O
the O
perplexity O
of O
y O
under O
the O
model O
increases O
. O

section 3
id pdf2json/P19-1004.pdf.json
if O
the O
increase O
is O
only O
minimal O
, O
we O
can O
conclude O
that O
the O
ordering O
of O
x1 O
. O

section 3
id pdf2json/P19-1004.pdf.json
. O

section 3
id pdf2json/P19-1004.pdf.json
.xn O
isn O
’ O
t O
informative O
to O
the O
model O
. O

section 3
id pdf2json/P19-1004.pdf.json
for O
a O
complete O
list O
of O
perturbations O
considered O
in O
this O
work O
, O
please O
refer O
to O
section O
3.2 O
. O

section 3
id pdf2json/P19-1004.pdf.json
all O
models O
are O
trained O
without O
any O
perturbations O
and O
sensitivity O
is O
studied O
only O
at O
test O
time O
. O

section 4
id pdf2json/P19-1004.pdf.json
we O
experiment O
with O
four O
multi-turn O
dialog O
datasets O
. O

section 4
id pdf2json/P19-1004.pdf.json
babi O
dialog O
is O
a O
synthetic O
goal-oriented O
multiturn O
dataset O
( O
bordes O
and O
weston O
, O
2016 O
) O
consisting O
of O
5 O
different O
tasks O
for O
restaurant O
booking O
with O
increasing O
levels O
of O
complexity O
. O

section 4
id pdf2json/P19-1004.pdf.json
we O
consider O
task O
5 O
in O
our O
experiments O
since O
it O
is O
the O
hardest O
and O
is O
a O
union O
of O
all O
four O
tasks O
. O

section 4
id pdf2json/P19-1004.pdf.json
it O
contains O
1k O
dialogs O
with O
an O
average O
of O
13 O
user O
utterances O
per O
dialog O
. O

section 4
id pdf2json/P19-1004.pdf.json
persona O
chat O
is O
an O
open O
domain O
dataset O
( O
zhang O
et O
al. O
, O
2018 O
) O
with O
multi-turn O
chit-chat O
conversations O
between O
turkers O
who O
are O
each O
assigned O
a O
“ O
persona O
” O
at O
random O
. O

section 4
id pdf2json/P19-1004.pdf.json
it O
comprises O
of O
10.9k O
dialogs O
with O
an O
average O
of O
14.8 O
turns O
per O
dialog O
. O

section 4
id pdf2json/P19-1004.pdf.json
dailydialog O
is O
an O
open O
domain O
dataset O
( O
li O
et O
al. O
, O
2017 O
) O
which O
consists O
of O
dialogs O
that O
resemble O
dayto-day O
conversations O
across O
multiple O
topics O
. O

section 4
id pdf2json/P19-1004.pdf.json
it O
comprises O
of O
13k O
dialogs O
with O
an O
average O
of O
7.9 O
turns O
per O
dialog O
. O

section 4
id pdf2json/P19-1004.pdf.json
mutualfriends O
is O
a O
multi-turn O
goal-oriented O
dataset O
( O
he O
et O
al. O
, O
2017 O
) O
where O
two O
agents O
must O
discover O
which O
friend O
of O
theirs O
is O
mutual O
based O
on O
the O
friends O
’ O
attributes O
. O

section 4
id pdf2json/P19-1004.pdf.json
it O
contains O
11k O
dialogs O
with O
an O
average O
of O
11.41 O
utterances O
per O
dialog O
. O

section 5
id pdf2json/P19-1004.pdf.json
we O
experimented O
with O
several O
types O
of O
perturbation O
operations O
at O
the O
utterance O
and O
word O
( O
token O
) O
levels O
. O

section 5
id pdf2json/P19-1004.pdf.json
all O
perturbations O
are O
applied O
in O
isolation O
. O

section 5
id pdf2json/P19-1004.pdf.json
utterance-level O
perturbations O
we O
consider O
the O
following O
operations O
1 O
) O
shuf O
that O
shuffles O
the O
sequence O
of O
utterances O
in O
the O
dialog O
history O
, O
2 O
) O
rev O
that O
reverses O
the O
order O
of O
utterances O
in O
the O
history O
( O
but O
maintains O
word O
order O
within O
each O
utterance O
) O
3 O
) O
drop O
that O
completely O
drops O
certain O
utterances O
and O
4 O
) O
truncate O
that O
truncates O
the O
dialog O
history O
to O
contain O
only O
the O
k O
most O
recent O
utterances O
where O
k O
≤ O
n O
, O
where O
n O
is O
the O
length O
of O
dialog O
history O
. O

section 5
id pdf2json/P19-1004.pdf.json
word-level O
perturbations O
we O
consider O
similar O
operations O
but O
at O
the O
word O
level O
within O
every O
utterance O
1 O
) O
word-shuffle O
that O
randomly O
shuffles O
the O
words O
within O
an O
utterance O
2 O
) O
reverse O
that O
reverses O
the O
ordering O
of O
words O
, O
3 O
) O
word-drop O
that O
drops O
30 O
% O
of O
the O
words O
uniformly O
4 O
) O
noun-drop O
that O
drops O
all O
nouns O
, O
5 O
) O
verb-drop O
that O
drops O
all O
verbs O
. O

section 6
id pdf2json/P19-1004.pdf.json
we O
experimented O
with O
two O
different O
classes O
of O
models O
- O
recurrent O
and O
transformer-based O
sequence-to-sequence O
generative O
models O
. O

section 6
id pdf2json/P19-1004.pdf.json
all O
data O
loading O
, O
model O
implementations O
and O
evaluations O
were O
done O
using O
the O
parlai O
framework O
. O

section 6
id pdf2json/P19-1004.pdf.json
we O
used O
the O
default O
hyper-parameters O
for O
all O
the O
models O
as O
specified O
in O
parlai O
. O

section 6
id pdf2json/P19-1004.pdf.json
recurrent O
models O
we O
trained O
a O
seq2seq O
( O
seq2seq O
lstm O
) O
model O
where O
the O
encoder O
and O
decoder O
are O
parameterized O
as O
lstms O
( O
hochreiter O
and O
schmidhuber O
, O
1997 O
) O
. O

section 6
id pdf2json/P19-1004.pdf.json
we O
also O
experiment O
with O
using O
decoders O
that O
use O
an O
attention O
mechanism O
( O
seq2seq O
lstm O
att O
) O
( O
bahdanau O
et O
al. O
, O
2015 O
) O
. O

section 6
id pdf2json/P19-1004.pdf.json
the O
encoder O
and O
decoder O
lstms O
have O
2 O
layers O
with O
128 O
dimensional O
hidden O
states O
with O
a O
dropout O
rate O
of O
0.1 O
. O

section 6
id pdf2json/P19-1004.pdf.json
transformer O
our O
transformer O
( O
vaswani O
et O
al. O
, O
2017 O
) O
model O
uses O
300 O
dimensional O
embeddings O
and O
hidden O
states O
, O
2 O
layers O
and O
2 O
attention O
heads O
with O
no O
dropout O
. O

section 6
id pdf2json/P19-1004.pdf.json
this O
model O
is O
significantly O
smaller O
than O
the O
ones O
typically O
used O
in O
machine O
translation O
since O
we O
found O
that O
the O
model O
that O
resembled O
vaswani O
et O
al O
. O

section 6
id pdf2json/P19-1004.pdf.json
( O
2017 O
) O
significantly O
overfit O
on O
all O
our O
datasets O
. O

section 6
id pdf2json/P19-1004.pdf.json
while O
the O
models O
considered O
in O
this O
work O
might O
not O
be O
state-of-the-art O
on O
the O
datasets O
considered O
, O
we O
believe O
these O
models O
are O
still O
competitive O
and O
used O
commonly O
enough O
at O
least O
as O
baselines O
, O
that O
the O
community O
will O
benefit O
by O
understanding O
their O
behavior O
. O

section 6
id pdf2json/P19-1004.pdf.json
in O
this O
paper O
, O
we O
use O
early O
stopping O
with O
a O
patience O
of O
10 O
on O
the O
validation O
set O
to O
save O
our O
best O
model O
. O

section 6
id pdf2json/P19-1004.pdf.json
all O
models O
achieve O
close O
to O
the O
perplexity O
numbers O
reported O
for O
generative O
seq2seq O
models O
in O
their O
respective O
papers O
. O

section 7
id pdf2json/P19-1004.pdf.json
our O
results O
are O
presented O
in O
table O
2 O
and O
figure O
1 O
. O

section 7
id pdf2json/P19-1004.pdf.json
table O
2 O
reports O
the O
perplexities O
of O
different O
models O
on O
test O
set O
in O
the O
second O
column O
, O
followed O
by O
the O
increase O
in O
perplexity O
when O
the O
dialog O
history O
is O
perturbed O
using O
the O
method O
specified O
in O
the O
column O
header O
. O

section 7
id pdf2json/P19-1004.pdf.json
rows O
correspond O
to O
models O
trained O
on O
different O
datasets O
. O

section 7
id pdf2json/P19-1004.pdf.json
figure O
1 O
presents O
the O
change O
in O
perplexity O
for O
models O
when O
presented O
only O
with O
the O
k O
most O
recent O
utterances O
from O
the O
dialog O
history O
. O

section 7
id pdf2json/P19-1004.pdf.json
we O
make O
the O
following O
observations O
: O
1 O
. O

section 7
id pdf2json/P19-1004.pdf.json
models O
tend O
to O
show O
only O
tiny O
changes O
in O
perplexity O
in O
most O
cases O
, O
even O
under O
extreme O
changes O
to O
the O
dialog O
history O
, O
suggesting O
that O
they O
use O
far O
from O
all O
the O
information O
that O
is O
available O
to O
them O
. O

section 7
id pdf2json/P19-1004.pdf.json
2 O
. O

section 7
id pdf2json/P19-1004.pdf.json
transformers O
are O
insensitive O
to O
wordreordering O
, O
indicating O
that O
they O
could O
be O
learning O
bag-of-words O
like O
representations O
. O

section 7
id pdf2json/P19-1004.pdf.json
3 O
. O

section 7
id pdf2json/P19-1004.pdf.json
the O
use O
of O
an O
attention O
mechanism O
in O
seq2seq O
lstm O
att O
and O
transformers O
makes O
these O
models O
use O
more O
information O
from O
earlier O
parts O
of O
the O
conversation O
than O
vanilla O
seq2seq O
models O
as O
seen O
from O
increases O
in O
perplexity O
when O
using O
only O
the O
last O
utterance O
. O

section 7
id pdf2json/P19-1004.pdf.json
4 O
. O

section 7
id pdf2json/P19-1004.pdf.json
while O
transformers O
converge O
faster O
and O
to O
lower O
test O
perplexities O
, O
they O
don O
’ O
t O
seem O
to O
capture O
the O
conversational O
dynamics O
across O
utterances O
in O
the O
dialog O
history O
and O
are O
less O
sensitive O
to O
perturbations O
that O
scramble O
this O
structure O
than O
recurrent O
models O
. O

section 8
id pdf2json/P19-1004.pdf.json
this O
work O
studies O
the O
behaviour O
of O
generative O
neural O
dialog O
systems O
in O
the O
presence O
of O
synthetically O
introduced O
perturbations O
to O
the O
dialog O
history O
, O
that O
it O
conditions O
on O
. O

section 8
id pdf2json/P19-1004.pdf.json
we O
find O
that O
both O
recurrent O
and O
transformer-based O
seq2seq O
models O
are O
not O
significantly O
affected O
even O
by O
drastic O
and O
unnatural O
modifications O
to O
the O
dialog O
history O
. O

section 8
id pdf2json/P19-1004.pdf.json
we O
also O
find O
subtle O
differences O
between O
the O
way O
in O
which O
recurrent O
and O
transformer-based O
models O
use O
available O
context O
. O

section 8
id pdf2json/P19-1004.pdf.json
by O
open-sourcing O
our O
code O
, O
we O
believe O
this O
paradigm O
of O
studying O
model O
behavior O
by O
introducing O
perturbations O
that O
destroys O
different O
kinds O
of O
structure O
present O
within O
the O
dialog O
history O
can O
be O
a O
useful O
diagnostic O
tool O
. O

section 8
id pdf2json/P19-1004.pdf.json
we O
also O
foresee O
this O
paradigm O
being O
useful O
when O
building O
new O
dialog O
datasets O
to O
understand O
the O
kinds O
of O
information O
models O
use O
to O
solve O
them O
. O

section 9
id pdf2json/P19-1004.pdf.json
we O
would O
like O
to O
acknowledge O
nvidia O
for O
donating O
gpus O
and O
a O
dgx-1 O
computer O
used O
in O
this O
work O
. O

section 9
id pdf2json/P19-1004.pdf.json
we O
would O
also O
like O
to O
thank O
the O
anonymous O
reviewers O
for O
their O
constructive O
feedback O
. O

section 9
id pdf2json/P19-1004.pdf.json
our O
code O
is O
available O
at O
https O
: O
//github.com/ O
chinnadhurai/parlai/ O
. O

section TITLE
id pdf2json/2021.acl-long.73.pdf.json
xlpt-amr O
: O
cross-lingual O
pre-training O
via O
multi-task O
learning O
for O
zero-shot O
amr O
parsing O
and O
text O
generation O

section ABSTRACT
id pdf2json/2021.acl-long.73.pdf.json
due O
to O
the O
scarcity O
of O
annotated O
data O
, O
abstract O
meaning O
representation O
( O
amr O
) O
research O
is O
relatively O
limited O
and O
challenging O
for O
languages O
other O
than O
english O
. O

section ABSTRACT
id pdf2json/2021.acl-long.73.pdf.json
upon O
the O
availability O
of O
english O
amr O
dataset O
and O
english-tox O
parallel O
datasets O
, O
in O
this O
paper O
we O
propose O
a O
novel O
cross-lingual O
pre-training O
approach O
via O
multi-task O
learning O
( O
mtl O
) O
for O
both O
zeroshot O
amr O
parsing O
and O
amr-to-text O
generation O
. O

section ABSTRACT
id pdf2json/2021.acl-long.73.pdf.json
specifically O
, O
we O
consider O
three O
types O
of O
relevant O
tasks O
, O
including O
amr O
parsing O
, O
amr-to-text O
generation O
, O
and O
machine O
translation O
. O

section ABSTRACT
id pdf2json/2021.acl-long.73.pdf.json
we O
hope O
that O
knowledge O
gained O
while O
learning O
for O
english O
amr O
parsing O
and O
text O
generation O
can O
be O
transferred O
to O
the O
counterparts O
of O
other O
languages O
. O

section ABSTRACT
id pdf2json/2021.acl-long.73.pdf.json
with O
properly O
pretrained O
models O
, O
we O
explore O
four O
different O
finetuning O
methods O
, O
i.e. O
, O
vanilla O
fine-tuning O
with O
a O
single O
task O
, O
one-for-all O
mtl O
fine-tuning O
, O
targeted O
mtl O
fine-tuning O
, O
and O
teacher-studentbased O
mtl O
fine-tuning O
. O

section ABSTRACT
id pdf2json/2021.acl-long.73.pdf.json
experimental O
results O
on O
amr O
parsing O
and O
text O
generation O
of O
multiple O
non-english O
languages O
demonstrate O
that O
our O
approach O
significantly O
outperforms O
a O
strong O
baseline O
of O
pre-training O
approach O
, O
and O
greatly O
advances O
the O
state O
of O
the O
art O
. O

section ABSTRACT
id pdf2json/2021.acl-long.73.pdf.json
in O
detail O
, O
on O
ldc2020t07 O
we O
have O
achieved O
70.45 O
% O
, O
71.76 O
% O
, O
and O
70.80 O
% O
in O
smatch O
f1 O
for O
amr O
parsing O
of O
german O
, O
spanish O
, O
and O
italian O
, O
respectively O
, O
while O
for O
amr-to-text O
generation O
of O
the O
languages O
, O
we O
have O
obtained O
25.69 O
, O
31.36 O
, O
and O
28.42 O
in O
bleu O
respectively O
. O

section ABSTRACT
id pdf2json/2021.acl-long.73.pdf.json
we O
make O
our O
code O
available O
on O
github O
https O
: O
// O
github.com/xdqkid/xlpt-amr O
. O

section 0
id pdf2json/2021.acl-long.73.pdf.json
proceedings O
of O
the O
59th O
annual O
meeting O
of O
the O
association O
for O
computational O
linguistics O
and O
the O
11th O
international O
joint O
conference O
on O
natural O
language O
processing O
, O
pages O
896–907 O
august O
1–6 O
, O
2021 O
. O

section 0
id pdf2json/2021.acl-long.73.pdf.json
©2021 O
association O
for O
computational O
linguistics O
896 O
xlpt-amr O
: O
cross-lingual O
pre-training O
via O
multi-task O
learning O
for O
zero-shot O
amr O
parsing O
and O
text O
generation O
dongqin O
xu1 O
junhui O
li1∗ O
muhua O
zhu2 O
min O
zhang1 O
guodong O
zhou1 O
1school O
of O
computer O
science O
and O
technology O
, O
soochow O
university O
, O
suzhou O
, O
china O
2tencent O
news O
, O
beijing O
, O
china O
xdqck O
@ O
live.com O
, O
{ O
lijunhui O
, O
minzhang O
, O
gdzhou O
} O
@ O
suda.edu.cn O
zhumuhua O
@ O
gmail.com O
abstract O
due O
to O
the O
scarcity O
of O
annotated O
data O
, O
abstract O
meaning O
representation O
( O
amr O
) O
research O
is O
relatively O
limited O
and O
challenging O
for O
languages O
other O
than O
english O
. O

section 0
id pdf2json/2021.acl-long.73.pdf.json
upon O
the O
availability O
of O
english O
amr O
dataset O
and O
english-tox O
parallel O
datasets O
, O
in O
this O
paper O
we O
propose O
a O
novel O
cross-lingual O
pre-training O
approach O
via O
multi-task O
learning O
( O
mtl O
) O
for O
both O
zeroshot O
amr O
parsing O
and O
amr-to-text O
generation O
. O

section 0
id pdf2json/2021.acl-long.73.pdf.json
specifically O
, O
we O
consider O
three O
types O
of O
relevant O
tasks O
, O
including O
amr O
parsing O
, O
amr-to-text O
generation O
, O
and O
machine O
translation O
. O

section 0
id pdf2json/2021.acl-long.73.pdf.json
we O
hope O
that O
knowledge O
gained O
while O
learning O
for O
english O
amr O
parsing O
and O
text O
generation O
can O
be O
transferred O
to O
the O
counterparts O
of O
other O
languages O
. O

section 0
id pdf2json/2021.acl-long.73.pdf.json
with O
properly O
pretrained O
models O
, O
we O
explore O
four O
different O
finetuning O
methods O
, O
i.e. O
, O
vanilla O
fine-tuning O
with O
a O
single O
task O
, O
one-for-all O
mtl O
fine-tuning O
, O
targeted O
mtl O
fine-tuning O
, O
and O
teacher-studentbased O
mtl O
fine-tuning O
. O

section 0
id pdf2json/2021.acl-long.73.pdf.json
experimental O
results O
on O
amr O
parsing O
and O
text O
generation O
of O
multiple O
non-english O
languages O
demonstrate O
that O
our O
approach O
significantly O
outperforms O
a O
strong O
baseline O
of O
pre-training O
approach O
, O
and O
greatly O
advances O
the O
state O
of O
the O
art O
. O

section 0
id pdf2json/2021.acl-long.73.pdf.json
in O
detail O
, O
on O
ldc2020t07 O
we O
have O
achieved O
70.45 O
% O
, O
71.76 O
% O
, O
and O
70.80 O
% O
in O
smatch O
f1 O
for O
amr O
parsing O
of O
german O
, O
spanish O
, O
and O
italian O
, O
respectively O
, O
while O
for O
amr-to-text O
generation O
of O
the O
languages O
, O
we O
have O
obtained O
25.69 O
, O
31.36 O
, O
and O
28.42 O
in O
bleu O
respectively O
. O

section 0
id pdf2json/2021.acl-long.73.pdf.json
we O
make O
our O
code O
available O
on O
github O
https O
: O
// O
github.com/xdqkid/xlpt-amr O
. O

section 1
id pdf2json/2021.acl-long.73.pdf.json
abstract O
meaning O
representation O
( O
amr O
) O
( O
banarescu O
et O
al. O
, O
2013 O
) O
is O
a O
widely O
used O
formalism O
that O
represents O
the O
semantics O
of O
a O
sentence O
with O
a O
directed O
and O
acyclic O
graph O
. O

section 1
id pdf2json/2021.acl-long.73.pdf.json
figure O
1 O
( O
b O
) O
shows O
an O
example O
amr O
graph O
where O
the O
nodes O
such O
as O
∗corresponding O
author O
: O
junhui O
li O
. O

section 1
id pdf2json/2021.acl-long.73.pdf.json
“ O
doctor O
” O
and O
“ O
give-01 O
” O
represent O
concepts O
, O
and O
the O
edges O
such O
as O
“ O
: O
arg0 O
” O
and O
“ O
: O
arg1 O
” O
stand O
for O
semantic O
relations O
between O
two O
connected O
concepts O
. O

section 1
id pdf2json/2021.acl-long.73.pdf.json
recent O
studies O
on O
amr O
mainly O
fall O
in O
two O
directions O
: O
amr O
parsing O
which O
converts O
a O
sentence O
into O
an O
amr O
graph O
( O
flanigan O
et O
al. O
, O
2014 O
; O
wang O
et O
al. O
, O
2015a O
; O
konstas O
et O
al. O
, O
2017 O
, O
to O
name O
a O
few O
) O
and O
its O
inverse O
, O
i.e. O
, O
amr-to-text O
generation O
that O
produces O
a O
sentence O
from O
an O
amr O
graph O
( O
flanigan O
et O
al. O
, O
2016 O
; O
song O
et O
al. O
, O
2017 O
, O
2018 O
, O
to O
name O
a O
few O
) O
. O

section 1
id pdf2json/2021.acl-long.73.pdf.json
restricted O
by O
the O
availability O
of O
annotated O
corpora O
, O
most O
of O
previous O
studies O
on O
amr O
focus O
on O
english O
while O
very O
few O
studies O
are O
for O
chinese O
and O
portuguese O
( O
wang O
et O
al. O
, O
2018 O
; O
sobrevilla O
cabezudo O
et O
al. O
, O
2019 O
; O
anchiêta O
and O
pardo O
, O
2020 O
) O
. O

section 1
id pdf2json/2021.acl-long.73.pdf.json
cross-lingual O
amr O
research O
, O
however O
, O
has O
received O
relatively O
less O
attention O
. O

section 1
id pdf2json/2021.acl-long.73.pdf.json
in O
fact O
, O
crosslingual O
amr O
has O
mainly O
been O
studied O
in O
the O
scope O
of O
annotation O
works O
( O
xue O
et O
al. O
, O
2014 O
; O
hajič O
et O
al. O
, O
2014 O
) O
. O

section 1
id pdf2json/2021.acl-long.73.pdf.json
till O
recently O
, O
damonte O
and O
cohen O
( O
2018 O
) O
demonstrate O
that O
amr O
annotated O
for O
english O
can O
be O
used O
as O
cross-lingual O
semantic O
representations O
, O
and O
propose O
to O
conduct O
cross-lingual O
amr O
parsing O
via O
annotation O
projection O
and O
machine O
translation O
. O

section 1
id pdf2json/2021.acl-long.73.pdf.json
blloshmi O
et O
al O
. O

section 1
id pdf2json/2021.acl-long.73.pdf.json
( O
2020 O
) O
follow O
the O
same O
line O
and O
create O
large-scale O
silver O
data O
to O
boost O
the O
performance O
of O
cross-lingual O
amr O
parsing O
. O

section 1
id pdf2json/2021.acl-long.73.pdf.json
fan O
and O
gardent O
( O
2020 O
) O
focus O
on O
multilingual O
amr-to-text O
generation O
for O
twenty O
one O
different O
languages O
. O

section 1
id pdf2json/2021.acl-long.73.pdf.json
the O
aforementioned O
studies O
consider O
amr O
parsing O
and O
amr-to-text O
generation O
separately O
. O

section 1
id pdf2json/2021.acl-long.73.pdf.json
in O
this O
paper O
, O
we O
formalize O
both O
amr O
parsing O
and O
amr-to-text O
generation O
as O
sequence-tosequence O
( O
seq2seq O
) O
learning O
and O
propose O
a O
novel O
and O
effective O
approach O
to O
cross-lingual O
amr O
, O
which O
is O
illustrated O
in O
figure O
1 O
. O

section 1
id pdf2json/2021.acl-long.73.pdf.json
upon O
the O
availability O
of O
the O
english O
amr O
dataset O
and O
english-tox O
parallel O
datasets O
( O
x O
∈ O
{ O
german O
, O
spanish O
, O
italian O
} O
in O
this O
paper O
) O
, O
our O
purpose O
is O
to O
boost O
the O
performance O
of O
zero-shot O
amr O
parsing O
and O
text O
generation O
in O
x-language O
. O

section 1
id pdf2json/2021.acl-long.73.pdf.json
to O
this O
end O
, O
we O
borrow O
the O
idea O
of O
joint O
pre-training O
from O
xu O
et O
al O
. O

section 1
id pdf2json/2021.acl-long.73.pdf.json
( O
2020 O
) O
and O
explore O
three O
types O
of O
relevant O
tasks O
, O
including O
machine O
translation O
tasks O
, O
amr O
parsing O
and O
amr-to-text O
generation O
tasks O
. O

section 1
id pdf2json/2021.acl-long.73.pdf.json
we O
conjecture O
that O
knowledge O
gained O
while O
learning O
for O
english O
amr O
parsing O
and O
text O
generation O
could O
be O
helpful O
to O
the O
x-language O
counterparts O
, O
and O
machine O
translation O
tasks O
could O
act O
as O
a O
good O
regularizer O
( O
xu O
et O
al. O
, O
2020 O
) O
. O

section 1
id pdf2json/2021.acl-long.73.pdf.json
to O
the O
best O
of O
our O
knowledge O
, O
this O
is O
the O
first O
study O
that O
utilizes O
such O
a O
pre-training O
approach O
in O
cross-lingual O
amr O
research O
. O

section 1
id pdf2json/2021.acl-long.73.pdf.json
we O
also O
explore O
and O
compare O
four O
different O
finetuning O
methods O
to O
answer O
the O
question O
that O
whether O
combining O
amr O
parsing O
and O
amr-to-text O
generation O
tasks O
in O
fine-tuning O
stage O
will O
achieve O
better O
performance O
. O

section 1
id pdf2json/2021.acl-long.73.pdf.json
moreover O
, O
inspired O
by O
the O
teacherstudent O
mechanism O
( O
kim O
and O
rush O
, O
2016 O
; O
chen O
et O
al. O
, O
2017 O
) O
, O
we O
extend O
the O
fine-tuning O
method O
to O
improve O
a O
target O
fine-tuning O
task O
with O
the O
help O
of O
another O
relevant O
yet O
stronger O
task O
. O

section 1
id pdf2json/2021.acl-long.73.pdf.json
experimental O
results O
on O
the O
cross-lingual O
amr O
dataset O
( O
ldc2020t07 O
) O
show O
that O
the O
proposed O
approach O
greatly O
advances O
the O
state O
of O
the O
art O
of O
cross-lingual O
amr O
. O

section 1
id pdf2json/2021.acl-long.73.pdf.json
overall O
, O
we O
make O
the O
following O
contributions O
. O

section 1
id pdf2json/2021.acl-long.73.pdf.json
• O
we O
propose O
an O
effective O
cross-lingual O
pretraining O
approach O
for O
zero-shot O
amr O
parsing O
and O
amr-to-text O
generation O
. O

section 1
id pdf2json/2021.acl-long.73.pdf.json
our O
pre-trained O
models O
could O
be O
used O
for O
both O
amr O
parsing O
and O
amr-to-text O
generation O
. O

section 1
id pdf2json/2021.acl-long.73.pdf.json
• O
we O
explore O
and O
compare O
different O
fine-tuning O
methods O
. O

section 1
id pdf2json/2021.acl-long.73.pdf.json
we O
also O
propose O
a O
teacher-studentbased O
fine-tuning O
method O
that O
achieves O
the O
best O
performance O
. O

section 1
id pdf2json/2021.acl-long.73.pdf.json
• O
we O
evaluate O
our O
approach O
in O
three O
zero-shot O
languages O
of O
amr O
and O
our O
approach O
greatly O
advances O
the O
state O
of O
the O
art O
. O

section 2
id pdf2json/2021.acl-long.73.pdf.json
we O
describe O
related O
studies O
on O
amr O
from O
three O
perspectives O
: O
english O
amr O
parsing O
, O
english O
amrto-text O
generation O
, O
and O
cross-lingual O
amr O
. O

section 2
id pdf2json/2021.acl-long.73.pdf.json
english O
amr O
parsing O
. O

section 2
id pdf2json/2021.acl-long.73.pdf.json
amr O
parsing O
is O
a O
task O
that O
translates O
a O
sentence O
into O
a O
directed O
and O
acyclic O
graph O
( O
banarescu O
et O
al. O
, O
2013 O
) O
. O

section 2
id pdf2json/2021.acl-long.73.pdf.json
according O
to O
the O
approaches O
to O
modeling O
the O
structure O
in O
amr O
graphs O
, O
previous O
studies O
on O
amr O
parsing O
for O
english O
can O
be O
broadly O
grouped O
into O
several O
categories O
, O
which O
are O
tree-based O
approaches O
( O
wang O
et O
al. O
, O
2015b O
; O
groschwitz O
et O
al. O
, O
2018 O
) O
, O
graph-based O
approaches O
( O
flanigan O
et O
al. O
, O
2014 O
; O
werling O
et O
al. O
, O
2015 O
; O
cai O
and O
lam O
, O
2019 O
) O
, O
transition-based O
approaches O
( O
zhou O
et O
al. O
, O
2016 O
; O
damonte O
et O
al. O
, O
2017 O
; O
ballesteros O
and O
al-onaizan O
, O
2017 O
; O
guo O
and O
lu O
, O
2018 O
; O
zhou O
et O
al. O
, O
2021 O
) O
, O
sequence-to-sequence O
( O
seq2seq O
) O
approaches O
( O
peng O
et O
al. O
, O
2017 O
; O
van O
noord O
and O
bos O
, O
2017 O
; O
konstas O
et O
al. O
, O
2017 O
; O
ge O
et O
al. O
, O
2019 O
; O
xu O
et O
al. O
, O
2020 O
; O
bevilacqua O
et O
al. O
, O
2021 O
) O
, O
and O
sequence-to-graph O
( O
seq2graph O
) O
approaches O
( O
lyu O
and O
titov O
, O
2018 O
; O
zhang O
et O
al. O
, O
2019a O
, O
b O
; O
cai O
and O
lam O
, O
2020a O
) O
. O

section 2
id pdf2json/2021.acl-long.73.pdf.json
english O
amr-to-text O
generation O
. O

section 2
id pdf2json/2021.acl-long.73.pdf.json
as O
an O
inverse O
task O
of O
amr O
parsing O
, O
amr-to-text O
generation O
aims O
to O
write O
a O
sentence O
from O
an O
amr O
graph O
. O

section 2
id pdf2json/2021.acl-long.73.pdf.json
early O
studies O
on O
this O
task O
rely O
on O
grammar-based O
approaches O
( O
flanigan O
et O
al. O
, O
2016 O
; O
song O
et O
al. O
, O
2017 O
) O
. O

section 2
id pdf2json/2021.acl-long.73.pdf.json
more O
recent O
studies O
propose O
to O
regard O
amr-totext O
generation O
as O
a O
machine O
translation O
or O
seq2seq O
task O
( O
pourdamghani O
et O
al. O
, O
2016 O
; O
ferreira O
et O
al. O
, O
2017 O
; O
konstas O
et O
al. O
, O
2017 O
; O
cao O
and O
clark O
, O
2019 O
) O
. O

section 2
id pdf2json/2021.acl-long.73.pdf.json
however O
, O
seq2seq O
approaches O
tend O
to O
lose O
structural O
information O
in O
amr O
graphs O
since O
they O
simply O
linearize O
amr O
graphs O
into O
sequences O
before O
feeding O
them O
into O
the O
models O
. O

section 2
id pdf2json/2021.acl-long.73.pdf.json
to O
prevent O
information O
loss O
caused O
by O
linearization O
, O
a O
variety O
of O
graph-tosequence O
approaches O
have O
been O
proposed O
to O
better O
model O
structural O
information O
( O
song O
et O
al. O
, O
2018 O
; O
beck O
et O
al. O
, O
2018 O
; O
damonte O
and O
cohen O
, O
2019 O
; O
guo O
et O
al. O
, O
2019 O
; O
ribeiro O
et O
al. O
, O
2019 O
; O
zhu O
et O
al. O
, O
2019 O
; O
cai O
and O
lam O
, O
2020b O
; O
zhao O
et O
al. O
, O
2020 O
; O
song O
et O
al. O
, O
2020 O
; O
yao O
et O
al. O
, O
2020 O
; O
bai O
et O
al. O
, O
2020 O
) O
. O

section 2
id pdf2json/2021.acl-long.73.pdf.json
by O
taking O
advantages O
of O
strong O
pre-trained O
language O
models O
, O
recent O
studies O
achieve O
new O
state O
of O
the O
art O
( O
mager O
et O
al. O
, O
2020 O
; O
harkous O
et O
al. O
, O
2020 O
; O
ribeiro O
et O
al. O
, O
2020 O
; O
bevilacqua O
et O
al. O
, O
2021 O
) O
. O

section 2
id pdf2json/2021.acl-long.73.pdf.json
cross-lingual O
amr O
. O

section 2
id pdf2json/2021.acl-long.73.pdf.json
all O
above O
related O
studies O
focus O
on O
english O
amr O
research O
. O

section 2
id pdf2json/2021.acl-long.73.pdf.json
relatively O
limited O
efforts O
have O
been O
put O
on O
other O
languages O
due O
to O
the O
lack O
of O
language-specific O
amr O
corpora O
. O

section 2
id pdf2json/2021.acl-long.73.pdf.json
actually O
, O
whether O
amr O
can O
act O
as O
an O
interlingua O
is O
an O
open O
question O
( O
xue O
et O
al. O
, O
2014 O
; O
hajič O
et O
al. O
, O
2014 O
) O
. O

section 2
id pdf2json/2021.acl-long.73.pdf.json
till O
lately O
, O
damonte O
and O
cohen O
( O
2018 O
) O
demonstrate O
that O
a O
simplified O
amr O
can O
be O
used O
across O
languages O
and O
for O
the O
first O
time O
they O
study O
crosslingual O
amr O
parsing O
for O
languages O
rather O
than O
english O
. O

section 2
id pdf2json/2021.acl-long.73.pdf.json
blloshmi O
et O
al O
. O

section 2
id pdf2json/2021.acl-long.73.pdf.json
( O
2020 O
) O
employ O
large-scale O
silver O
parallel O
amr O
data O
to O
bridge O
the O
gap O
between O
different O
languages O
and O
greatly O
advance O
the O
performance O
of O
cross-lingual O
amr O
parsing O
. O

section 2
id pdf2json/2021.acl-long.73.pdf.json
sheth O
et O
al O
. O

section 2
id pdf2json/2021.acl-long.73.pdf.json
( O
2021 O
) O
explore O
annotation O
projection O
to O
leverage O
existing O
english O
amr O
and O
overcome O
resource O
shortage O
in O
the O
target O
language O
. O

section 2
id pdf2json/2021.acl-long.73.pdf.json
furthermore O
, O
fan O
and O
gardent O
( O
2020 O
) O
explore O
cross-lingual O
amr-to-text O
based O
on O
pre-trained O
cross-lingual O
language O
model O
( O
xlm O
) O
( O
lample O
and O
conneau O
, O
2019 O
) O
. O

section 2
id pdf2json/2021.acl-long.73.pdf.json
in O
this O
paper O
we O
build O
strong O
cross-lingual O
pre-trained O
models O
for O
both O
amr O
parsing O
and O
amr-to-text O
generation O
. O

section 2
id pdf2json/2021.acl-long.73.pdf.json
moreover O
, O
a O
nice O
property O
of O
our O
approach O
is O
that O
for O
amr O
parsing O
, O
unlike O
related O
studies O
( O
damonte O
and O
cohen O
, O
2018 O
; O
blloshmi O
et O
al. O
, O
2020 O
) O
, O
we O
do O
not O
need O
to O
perform O
lemmatization O
, O
pos O
tagging O
, O
ner O
, O
or O
re-categorization O
of O
entities O
, O
thus O
require O
no O
language O
specific O
toolkits O
in O
pre-processing O
. O

section 3
id pdf2json/2021.acl-long.73.pdf.json
in O
this O
section O
, O
we O
first O
present O
the O
background O
of O
our O
pre-training O
approach O
( O
section O
3.1 O
) O
, O
followed O
by O
the O
description O
of O
cross-lingual O
pre-training O
tasks O
( O
section O
3.2 O
) O
. O

section 3
id pdf2json/2021.acl-long.73.pdf.json
then O
we O
present O
our O
joint O
pre-training O
( O
section O
3.3 O
) O
. O

section 3
id pdf2json/2021.acl-long.73.pdf.json
for O
simplicity O
, O
in O
the O
following O
we O
use O
german O
as O
a O
representative O
to O
describe O
our O
approach O
to O
german O
amr O
parsing O
and O
amr-to-text O
generation O
. O

section 4
id pdf2json/2021.acl-long.73.pdf.json
transformer-based O
seq2seq O
learning O
. O

section 4
id pdf2json/2021.acl-long.73.pdf.json
our O
models O
are O
built O
on O
the O
transformer O
framework O
( O
vaswani O
et O
al. O
, O
2017 O
) O
. O

section 4
id pdf2json/2021.acl-long.73.pdf.json
the O
encoder O
in O
transformer O
consists O
of O
a O
stack O
of O
multiple O
identical O
layers O
, O
each O
of O
which O
has O
two O
sub-layers O
: O
one O
implements O
the O
multi-head O
self-attention O
mechanism O
and O
the O
other O
is O
a O
position-wise O
fully-connected O
feedforward O
network O
. O

section 4
id pdf2json/2021.acl-long.73.pdf.json
the O
decoder O
is O
also O
composed O
of O
a O
stack O
of O
multiple O
identical O
layers O
. O

section 4
id pdf2json/2021.acl-long.73.pdf.json
each O
layer O
in O
the O
decoder O
consists O
of O
the O
same O
sub-layers O
as O
in O
the O
encoder O
plus O
an O
additional O
sub-layer O
that O
performs O
multi-head O
attention O
to O
the O
distributional O
representation O
produced O
by O
the O
encoder O
. O

section 4
id pdf2json/2021.acl-long.73.pdf.json
see O
vaswani O
et O
al O
. O

section 4
id pdf2json/2021.acl-long.73.pdf.json
( O
2017 O
) O
for O
more O
details O
. O

section 4
id pdf2json/2021.acl-long.73.pdf.json
amr O
graph O
linearization O
and O
recovering O
. O

section 4
id pdf2json/2021.acl-long.73.pdf.json
to O
make O
transformer O
applicable O
to O
amr O
parsing O
and O
amr-to-text O
generation O
, O
on O
the O
one O
hand O
we O
follow O
van O
noord O
and O
bos O
( O
2017 O
) O
to O
linearize O
amr O
graphs O
into O
sequences O
by O
removing O
variables O
, O
wiki O
links O
and O
duplicating O
the O
co-referring O
nodes O
. O

section 4
id pdf2json/2021.acl-long.73.pdf.json
on O
the O
other O
hand O
, O
for O
amr O
parsing O
we O
need O
to O
recover O
the O
graph O
representation O
from O
linearized O
amrs O
by O
assigning O
a O
unique O
variable O
to O
each O
concept O
, O
pruning O
duplicated O
and O
redundant O
materials O
, O
restoring O
co-referring O
nodes O
, O
fixing O
incomplete O
concepts O
and O
performing O
wikification.1 O
in O
this O
paper O
, O
we O
adopt O
linearization O
and O
recovering O
scripts O
provided O
by O
van O
noord O
and O
bos O
( O
2017 O
) O
.2 O

section 5
id pdf2json/2021.acl-long.73.pdf.json
due O
to O
the O
unavailability O
of O
gold O
training O
data O
of O
german O
amr O
parsing O
and O
amr-to-text O
generation O
, O
we O
view O
english O
as O
a O
pivot O
and O
hope O
that O
knowledge O
gained O
while O
learning O
for O
english O
amr O
parsing O
and O
text O
generation O
could O
be O
helpful O
for O
the O
german O
counterparts O
. O

section 5
id pdf2json/2021.acl-long.73.pdf.json
specifically O
, O
given O
an O
en-de O
parallel O
dataset O
( O
t O
en O
, O
t O
de O
) O
, O
we O
use O
an O
english O
amr O
parser O
trained O
on O
annotated O
english O
amrs O
( O
i.e. O
, O
amr2.0 O
) O
to O
parse O
the O
english O
sentences O
into O
amr O
graphs O
, O
thus O
obtain O
a O
trilingual O
parallel O
dataset O
t O
= O
( O
t O
en O
, O
t O
de O
, O
t O
amr O
) O
. O

section 5
id pdf2json/2021.acl-long.73.pdf.json
then O
1we O
extract O
a O
term-wiki O
list O
from O
english O
amr O
training O
dataset O
. O

section 5
id pdf2json/2021.acl-long.73.pdf.json
when O
performing O
wikification O
, O
we O
simply O
just O
look O
up O
the O
list O
. O

section 5
id pdf2json/2021.acl-long.73.pdf.json
2https O
: O
//github.com/rikvn/amr O
on O
the O
trilingual O
parallel O
dataset O
, O
we O
propose O
crosslingual O
pre-training O
via O
multi-task O
learning O
. O

section 5
id pdf2json/2021.acl-long.73.pdf.json
we O
consider O
three O
types O
of O
tasks O
, O
i.e. O
, O
amr O
parsing O
, O
amr-to-text O
generation O
, O
and O
machine O
translation O
. O

section 5
id pdf2json/2021.acl-long.73.pdf.json
amr O
parsing O
tasks O
, O
which O
include O
both O
english O
amr O
parsing O
on O
the O
training O
data O
( O
t O
en O
, O
t O
amr O
) O
and O
german O
amr O
parsing O
on O
( O
t O
de O
, O
t O
amr O
) O
. O

section 5
id pdf2json/2021.acl-long.73.pdf.json
note O
that O
both O
amr O
parsing O
tasks O
are O
trained O
on O
silver O
amr O
graphs O
. O

section 5
id pdf2json/2021.acl-long.73.pdf.json
amr-to-text O
generation O
tasks O
, O
which O
include O
both O
english O
amr-to-text O
generation O
and O
german O
amr-to-text O
generation O
. O

section 5
id pdf2json/2021.acl-long.73.pdf.json
similar O
to O
amr O
parsing O
, O
these O
two O
amr-to-text O
generation O
tasks O
are O
also O
trained O
on O
silver O
amr O
graphs O
( O
t O
amr O
, O
t O
en O
) O
and O
( O
t O
amr O
, O
t O
de O
) O
, O
respectively O
. O

section 5
id pdf2json/2021.acl-long.73.pdf.json
machine O
translation O
tasks O
, O
which O
include O
both O
english-to-german O
and O
german-to-english O
machine O
translation O
tasks O
on O
( O
t O
en O
, O
t O
de O
) O
. O

section 5
id pdf2json/2021.acl-long.73.pdf.json
the O
advantage O
of O
including O
the O
bi-directional O
translation O
tasks O
is O
three-fold O
. O

section 5
id pdf2json/2021.acl-long.73.pdf.json
first O
, O
english-to-german O
translation O
will O
enable O
the O
decoder O
to O
generate O
fluent O
german O
sentence O
, O
which O
is O
beneficial O
to O
german O
amr-to-text O
generation O
. O

section 5
id pdf2json/2021.acl-long.73.pdf.json
second O
, O
german-toenglish O
translation O
will O
enable O
the O
encoder O
to O
capture O
syntax O
and O
semantic O
information O
from O
german O
sentences O
, O
which O
is O
beneficial O
to O
german O
amr O
parsing O
. O

section 5
id pdf2json/2021.acl-long.73.pdf.json
third O
, O
translation O
tasks O
can O
serve O
as O
regularization O
to O
the O
training O
of O
amr O
parsing O
and O
amr-to-text O
generation O
, O
both O
of O
which O
are O
apt O
to O
overfit O
to O
the O
training O
data O
. O

section 5
id pdf2json/2021.acl-long.73.pdf.json
overall O
speaking O
, O
in O
our O
pre-training O
there O
exist O
three O
types O
of O
( O
six O
) O
pre-training O
tasks O
in O
total O
. O

section 5
id pdf2json/2021.acl-long.73.pdf.json
the O
pre-training O
is O
conducted O
on O
a O
trilingual O
parallel O
dataset O
( O
t O
en O
, O
t O
de O
, O
t O
amr O
) O
, O
where O
t O
en O
and O
t O
de O
are O
parallel O
gold O
sentence O
pairs O
while O
t O
amr O
is O
the O
set O
of O
corresponding O
silver O
amr O
graphs O
. O

section 6
id pdf2json/2021.acl-long.73.pdf.json
to O
train O
the O
above O
six O
pre-training O
tasks O
with O
a O
single O
model O
, O
we O
follow O
the O
strategy O
used O
in O
xu O
et O
al O
. O

section 6
id pdf2json/2021.acl-long.73.pdf.json
( O
2020 O
) O
and O
add O
preceding O
language O
tags O
to O
both O
source O
and O
target O
sides O
of O
training O
data O
to O
distinguish O
the O
inputs O
and O
outputs O
of O
each O
training O
task O
. O

section 6
id pdf2json/2021.acl-long.73.pdf.json
as O
illustrated O
in O
table O
1 O
, O
we O
use O
< O
en O
> O
, O
< O
de O
> O
, O
and O
< O
amr O
> O
as O
the O
tags O
of O
begin-of-sentence O
for O
english O
sentences O
, O
german O
sentences O
, O
and O
linearized O
amrs O
, O
respectively O
. O

section 6
id pdf2json/2021.acl-long.73.pdf.json
our O
joint O
pre-training O
on O
multiple O
tasks O
falls O
into O
the O
paradigm O
of O
multi-task O
learning O
( O
mtl O
) O
. O

section 6
id pdf2json/2021.acl-long.73.pdf.json
in O
the O
training O
stage O
, O
we O
take O
turns O
to O
load O
the O
training O
data O
of O
these O
pre-training O
tasks O
. O

section 6
id pdf2json/2021.acl-long.73.pdf.json
for O
example O
, O
we O
update O
model O
parameters O
on O
a O
batch O
of O
training O
instances O
from O
the O
first O
task O
, O
and O
then O
update O
parameters O
on O
a O
batch O
of O
training O
instances O
of O
the O
second O
task O
, O
and O
the O
process O
repeats O
. O

section 6
id pdf2json/2021.acl-long.73.pdf.json
we O
also O
note O
that O
, O
according O
to O
our O
preliminary O
experimentation O
, O
the O
effect O
of O
different O
orders O
of O
carrying O
out O
these O
pre-training O
tasks O
is O
negligible O
. O

section 7
id pdf2json/2021.acl-long.73.pdf.json
to O
fine-tune O
a O
pre-trained O
model O
, O
we O
create O
a O
fine-tuning O
dataset O
from O
english O
annotated O
amrs O
( O
i.e. O
, O
amr2.0 O
) O
. O

section 7
id pdf2json/2021.acl-long.73.pdf.json
given O
english-amr O
parallel O
data O
( O
fen O
, O
famr O
) O
, O
we O
use O
an O
english-to-german O
translator O
to O
translate O
the O
english O
sentences O
into O
german O
sentences O
, O
thus O
obtain O
trilingual O
parallel O
dataset O
f O
= O
( O
fen O
, O
fde O
, O
famr O
) O
. O

section 7
id pdf2json/2021.acl-long.73.pdf.json
as O
our O
goal O
is O
to O
improve O
the O
performance O
of O
zero-shot O
amr O
parsing O
and O
amr-to-text O
generation O
, O
our O
primary O
fine-tuning O
tasks O
are O
german O
amr O
parsing O
and O
amr-to-text O
generation O
. O

section 7
id pdf2json/2021.acl-long.73.pdf.json
moreover O
, O
we O
could O
include O
the O
other O
four O
fine-tuning O
tasks O
as O
auxiliary O
tasks O
when O
necessary O
, O
i.e. O
, O
english O
amr O
parsing O
and O
amr-to-text O
generation O
, O
as O
well O
as O
english-togerman O
and O
german-to-english O
translation O
. O

section 7
id pdf2json/2021.acl-long.73.pdf.json
once O
the O
fine-tuning O
dataset O
is O
ready O
, O
we O
can O
finetune O
a O
pre-trained O
model O
with O
different O
methods O
. O

section 7
id pdf2json/2021.acl-long.73.pdf.json
the O
vanilla O
fine-tuning O
method O
that O
fine-tunes O
a O
pretrained O
model O
on O
the O
dataset O
of O
a O
primary O
task O
is O
a O
natural O
choice O
. O

section 7
id pdf2json/2021.acl-long.73.pdf.json
we O
can O
also O
fine-tune O
a O
pre-trained O
model O
jointly O
over O
all O
fine-tuning O
tasks O
, O
or O
over O
the O
primary O
tasks O
plus O
specifically O
chosen O
fine-tuning O
tasks O
that O
are O
relevant O
. O

section 7
id pdf2json/2021.acl-long.73.pdf.json
in O
the O
following O
we O
explore O
and O
compare O
four O
different O
fine-tuning O
methods O
. O

section 8
id pdf2json/2021.acl-long.73.pdf.json
given O
a O
pre-trained O
model O
, O
vanilla O
fine-tuning O
updates O
the O
parameters O
of O
the O
pre-trained O
model O
solely O
on O
the O
dataset O
of O
the O
downstream O
task O
. O

section 8
id pdf2json/2021.acl-long.73.pdf.json
for O
example O
, O
for O
german O
amr O
parsing O
, O
we O
fine-tune O
the O
pre-trained O
model O
on O
the O
fine-tuning O
dataset O
of O
the O
german O
amr O
parsing O
task O
. O

section 8
id pdf2json/2021.acl-long.73.pdf.json
in O
other O
words O
, O
vanilla O
fine-tuning O
involves O
only O
a O
single-task O
learning O
. O

section 9
id pdf2json/2021.acl-long.73.pdf.json
we O
fine-tune O
a O
pre-trained O
model O
synchronously O
for O
all O
six O
fine-tuning O
tasks O
, O
which O
are O
the O
same O
as O
the O
pre-training O
tasks O
. O

section 9
id pdf2json/2021.acl-long.73.pdf.json
related O
studies O
( O
li O
and O
hoiem O
, O
2018 O
; O
xu O
et O
al. O
, O
2020 O
) O
have O
shown O
that O
it O
is O
important O
to O
optimize O
for O
high O
accuracy O
of O
a O
primary O
fine-tuning O
task O
while O
preserving O
the O
performance O
of O
other O
tasks O
. O

section 9
id pdf2json/2021.acl-long.73.pdf.json
preserving O
the O
performance O
of O
various O
pre-training O
tasks O
could O
be O
viewed O
as O
a O
regularizer O
for O
each O
fine-tuning O
task O
. O

section 9
id pdf2json/2021.acl-long.73.pdf.json
similarly O
to O
joint O
pre-training O
, O
we O
take O
turns O
to O
load O
the O
fine-tuning O
data O
of O
these O
fine-tuning O
tasks O
. O

section 9
id pdf2json/2021.acl-long.73.pdf.json
consequently O
, O
we O
obtain O
a O
single O
fine-tuned O
model O
for O
all O
tasks O
. O

section 10
id pdf2json/2021.acl-long.73.pdf.json
rather O
than O
including O
all O
fine-tuning O
tasks O
within O
a O
single O
model O
, O
we O
can O
selectively O
choose O
relevant O
fine-tuning O
tasks O
. O

section 10
id pdf2json/2021.acl-long.73.pdf.json
for O
german O
amr O
parsing O
, O
we O
use O
amr O
parsing O
on O
german O
as O
the O
primary O
finetuning O
task O
and O
german-to-english O
translation O
as O
an O
auxiliary O
fine-tuning O
task O
. O

section 10
id pdf2json/2021.acl-long.73.pdf.json
the O
auxiliary O
task O
will O
enhance O
the O
encoder O
to O
capture O
semantic O
information O
from O
german O
sentences O
. O

section 10
id pdf2json/2021.acl-long.73.pdf.json
this O
is O
also O
consistent O
with O
the O
fine-tuning O
tasks O
designed O
for O
english O
amr O
parsing O
in O
( O
xu O
et O
al. O
, O
2020 O
) O
. O

section 10
id pdf2json/2021.acl-long.73.pdf.json
for O
german O
amr-to-text O
generation O
, O
we O
choose O
englishto-german O
as O
the O
auxiliary O
fine-tuning O
task O
, O
which O
is O
beneficial O
for O
the O
decoder O
to O
generate O
fluent O
german O
sentences O
. O

section 11
id pdf2json/2021.acl-long.73.pdf.json
one O
notable O
property O
of O
the O
fine-tuning O
dataset O
is O
that O
the O
german O
sentences O
are O
produced O
automatically O
through O
machine O
translation O
. O

section 11
id pdf2json/2021.acl-long.73.pdf.json
noises O
in O
such O
silver O
fine-tuning O
dataset O
may O
degrade O
the O
performance O
of O
fine-tuned O
models O
. O

section 11
id pdf2json/2021.acl-long.73.pdf.json
inspired O
by O
the O
teacher-student O
framework O
( O
kim O
and O
rush O
, O
2016 O
; O
chen O
et O
al. O
, O
2017 O
) O
, O
we O
propose O
to O
solve O
this O
problem O
by O
using O
a O
stronger O
fine-tuning O
task O
to O
help O
improve O
fine-tuning O
tasks O
on O
such O
noisy O
data O
. O

section 11
id pdf2json/2021.acl-long.73.pdf.json
for O
example O
, O
we O
can O
use O
english O
amr O
parsing O
( O
as O
the O
teacher O
) O
to O
help O
german O
amr O
parsing O
( O
as O
the O
student O
) O
, O
since O
english O
amr O
parsing O
that O
is O
fine-tuned O
on O
gold O
data O
tends O
to O
have O
stronger O
performance O
. O

section 11
id pdf2json/2021.acl-long.73.pdf.json
fine-tuning O
for O
german O
amr O
parsing O
. O

section 11
id pdf2json/2021.acl-long.73.pdf.json
we O
use O
e O
, O
g O
, O
a O
to O
denote O
english-side O
, O
german-side O
, O
and O
amr-side O
, O
respectively O
, O
and O
( O
e O
, O
g O
, O
a O
) O
as O
a O
triple O
instance O
. O

section 11
id pdf2json/2021.acl-long.73.pdf.json
for O
german O
amr O
parsing O
( O
i.e. O
, O
g O
→ O
a O
) O
, O
we O
regard O
english O
amr O
parsing O
( O
i.e. O
, O
e O
→ O
a O
) O
as O
its O
teacher O
and O
assume O
that O
the O
probability O
of O
generating O
a O
target O
amr O
token O
ai O
from O
g O
should O
be O
close O
to O
that O
from O
its O
counterpart O
e O
, O
given O
the O
already O
obtained O
partial O
amr O
a O
< O
i O
. O

section 11
id pdf2json/2021.acl-long.73.pdf.json
on O
this O
assumption O
, O
the O
student O
model O
can O
acquire O
knowledge O
from O
the O
teacher O
by O
applying O
word-level O
knowledge O
distillation O
for O
multi-class O
cross-entropy O
with O
the O
following O
joint O
training O
objective O
: O
j O
( O
θg→a O
) O
=∑ O
( O
e O
, O
g O
, O
a O
) O
j O
( O
e O
, O
g O
, O
a O
, O
θ̂e→a O
, O
θg→a O
) O
+ O
lθg→a O
( O
a O
| O
g O
) O
, O
( O
1 O
) O
where O
( O
e O
, O
g O
, O
a O
) O
∈ O
de O
, O
g O
, O
a O
, O
i.e. O
, O
( O
fen O
, O
fde O
, O
famr O
) O
, O
the O
fine-tuning O
data O
for O
english/german O
amr O
parsing O
, O
θ̂e→a O
denotes O
the O
already O
learned O
model O
parameters O
for O
english O
amr O
parsing,3 O
and O
lθg→a O
( O
a O
| O
g O
) O
denotes O
the O
log-likelihood O
function O
for O
translating O
g O
into O
a O
. O

section 11
id pdf2json/2021.acl-long.73.pdf.json
the O
function O
j O
in O
eq O
. O

section 11
id pdf2json/2021.acl-long.73.pdf.json
1 O
is O
defined O
as O
: O
j O
( O
e O
, O
g O
, O
a O
, O
θ̂e→a O
, O
θg→a O
) O
= O
|a|∑ O
i=1 O
kl O
( O
p O
( O
a|e O
, O
a O
< O
i O
; O
θ̂e→a O
) O
‖ O
p O
( O
a|g O
, O
a O
< O
i O
; O
θg→a O
) O
) O
= O
|a|∑ O
i=1 O
∑ O
a∈va O
p O
( O
a|e O
, O
a O
< O
i O
; O
θ̂e→a O
) O
log O
p O
( O
a|e O
, O
a O
< O
i O
; O
θ̂e→a O
) O
p O
( O
a|g O
, O
a O
< O
i O
; O
θg→a O
) O
, O
( O
2 O
) O
where O
kl O
( O
· O
‖ O
· O
) O
denotes O
the O
kl O
divergence O
between O
two O
distributions O
, O
and O
va O
is O
the O
vocabulary O
set.4 O
to O
sum O
up O
, O
in O
mtl O
fine-tuning O
we O
use O
eq O
. O

section 11
id pdf2json/2021.acl-long.73.pdf.json
1 O
as O
the O
objective O
for O
the O
fine-tuning O
task O
of O
german O
amr O
parsing O
while O
we O
still O
use O
the O
log-likelihood O
function O
for O
the O
auxiliary O
fine-tuning O
task O
, O
i.e. O
, O
german-to-english O
translation O
. O

section 11
id pdf2json/2021.acl-long.73.pdf.json
fine-tuning O
for O
german O
amr-to-text O
generation O
. O

section 11
id pdf2json/2021.acl-long.73.pdf.json
considering O
the O
fact O
that O
the O
performance O
of O
english-to-german O
translation O
is O
also O
better O
than O
that O
of O
german O
amr-to-text O
generation O
, O
we O
view O
english-to-german O
translation O
as O
the O
teacher O
and O
assume O
that O
the O
probability O
of O
generating O
a O
target O
german O
token O
gi O
from O
a O
should O
be O
close O
to O
that O
from O
its O
counterpart O
e O
, O
given O
the O
already O
obtained O
partial O
german O
sentence O
g O
< O
i O
. O

section 11
id pdf2json/2021.acl-long.73.pdf.json
the O
joint O
training O
objective O
for O
german O
amr-to-text O
generation O
is O
similar O
to O
the O
aforementioned O
objective O
function O
for O
german O
amr O
parsing O
. O

section 11
id pdf2json/2021.acl-long.73.pdf.json
due O
to O
limited O
space O
, O
we O
omit O
definition O
details O
of O
the O
objective O
function O
. O

section 11
id pdf2json/2021.acl-long.73.pdf.json
3the O
english O
amr O
parser O
is O
learned O
by O
fine-tuning O
the O
pretrained O
model O
on O
fine-tuning O
tasks O
of O
english O
amr O
parsing O
and O
english-to-german O
translation O
. O

section 11
id pdf2json/2021.acl-long.73.pdf.json
4to O
avoid O
overfitting O
, O
the O
method O
additionally O
fine-tunes O
80k O
steps O
on O
the O
pre-training O
dataset O
at O
the O
beginning O
. O

section 12
id pdf2json/2021.acl-long.73.pdf.json
in O
this O
section O
, O
we O
report O
the O
performance O
of O
our O
approach O
to O
amr O
parsing O
and O
amr-to-text O
generation O
for O
non-english O
languages O
, O
including O
german O
( O
de O
) O
, O
spanish O
( O
es O
) O
, O
and O
italian O
( O
it O
) O
. O

section 12
id pdf2json/2021.acl-long.73.pdf.json
the O
models O
are O
pre-trained O
and O
fine-tuned O
on O
english O
data O
and O
one O
of O
either O
de O
, O
es O
, O
or O
it O
, O
and O
are O
evaluated O
in O
the O
target O
language O
. O

section 13
id pdf2json/2021.acl-long.73.pdf.json
pre-training O
datasets O
. O

section 13
id pdf2json/2021.acl-long.73.pdf.json
for O
german O
, O
we O
use O
the O
wmt14 O
english-german O
translation O
dataset O
5 O
which O
consists O
of O
3.9m O
sentence O
pairs O
after O
preprocessing O
. O

section 13
id pdf2json/2021.acl-long.73.pdf.json
for O
spanish O
and O
italian O
, O
we O
use O
europarl O
parallel O
datasets,6 O
which O
consist O
of O
1.9m O
english-spanish O
and O
1.9m O
english-italian O
sentence O
pairs O
, O
respectively O
. O

section 13
id pdf2json/2021.acl-long.73.pdf.json
the O
english O
sentences O
of O
all O
the O
datasets O
are O
all O
parsed O
into O
amr O
graphs O
via O
an O
english O
amr O
parser O
trained O
on O
amr O
2.0 O
( O
ldc2017t10 O
) O
( O
appendix O
a O
provides O
more O
details O
on O
the O
english O
amr O
parser O
) O
. O

section 13
id pdf2json/2021.acl-long.73.pdf.json
we O
merge O
english O
, O
german O
( O
spanish/italian O
) O
sentences O
and O
linearized O
amrs O
together O
and O
segment O
all O
the O
tokens O
into O
subwords O
by O
byte O
pair O
encoding O
( O
bpe O
) O
( O
sennrich O
et O
al. O
, O
2016 O
) O
with O
40k O
( O
or O
30k O
for O
both O
spanish O
and O
italian O
) O
operations O
. O

section 13
id pdf2json/2021.acl-long.73.pdf.json
in O
addition O
, O
we O
also O
train O
nmt O
models O
to O
translate O
english O
into O
german O
, O
spanish O
, O
and O
italian O
on O
above O
parallel O
datasets O
with O
transformer-big O
settings O
( O
vaswani O
et O
al. O
, O
2017 O
) O
. O

section 13
id pdf2json/2021.acl-long.73.pdf.json
these O
nmt O
models O
will O
be O
used O
in O
preparing O
fine-tuning O
datasets O
( O
appendix O
b O
provides O
more O
implementation O
details O
on O
the O
nmt O
models O
) O
. O

section 13
id pdf2json/2021.acl-long.73.pdf.json
fine-tuning O
datasets O
. O

section 13
id pdf2json/2021.acl-long.73.pdf.json
we O
use O
english O
amr2.0 O
which O
contains O
36,521 O
, O
1,368 O
, O
and O
1,371 O
englishamr O
pairs O
for O
training O
, O
development O
, O
and O
testing O
, O
respectively O
. O

section 13
id pdf2json/2021.acl-long.73.pdf.json
we O
translate O
the O
english O
sentences O
into O
german O
, O
spanish O
, O
and O
italian O
, O
respectively O
. O

section 13
id pdf2json/2021.acl-long.73.pdf.json
we O
segment O
all O
the O
tokens O
into O
subwords O
by O
using O
the O
bpe O
model O
trained O
on O
pre-training O
datasets O
. O

section 13
id pdf2json/2021.acl-long.73.pdf.json
pre-training O
and O
fine-tuning O
model O
settings O
. O

section 13
id pdf2json/2021.acl-long.73.pdf.json
we O
implement O
above O
pre-trained O
models O
based O
on O
opennmt-py O
( O
klein O
et O
al. O
, O
2017 O
) O
. O

section 13
id pdf2json/2021.acl-long.73.pdf.json
7 O
for O
simplicity O
, O
we O
use O
the O
same O
hyperparameter O
settings O
to O
train O
all O
the O
models O
in O
both O
pre-training O
and O
fine-tuning O
5https O
: O
//www.statmt.org/wmt14/ O
translation-task.html O
6https O
: O
//www.statmt.org/europarl/index O
. O

section 13
id pdf2json/2021.acl-long.73.pdf.json
html O
7https O
: O
//github.com/opennmt/opennmt-py O
by O
just O
following O
the O
settings O
for O
the O
transformerbase O
model O
in O
vaswani O
et O
al O
. O

section 13
id pdf2json/2021.acl-long.73.pdf.json
( O
2017 O
) O
. O

section 13
id pdf2json/2021.acl-long.73.pdf.json
the O
number O
of O
layers O
in O
encoder O
and O
decoder O
is O
6 O
while O
the O
number O
of O
heads O
is O
8 O
. O

section 13
id pdf2json/2021.acl-long.73.pdf.json
both O
the O
embedding O
size O
and O
the O
hidden O
state O
size O
are O
512 O
while O
the O
size O
of O
feedforward O
network O
is O
2048 O
. O

section 13
id pdf2json/2021.acl-long.73.pdf.json
moreover O
, O
we O
use O
adam O
optimizer O
( O
kingma O
and O
ba O
, O
2015 O
) O
with O
β1 O
of O
0.9 O
and O
β2 O
of O
0.98 O
. O

section 13
id pdf2json/2021.acl-long.73.pdf.json
warm O
up O
step O
, O
learning O
rate O
, O
dropout O
rate O
, O
and O
label O
smoothing O
epsilon O
are O
set O
to O
16000 O
, O
2.0 O
, O
0.1 O
and O
0.1 O
respectively O
. O

section 13
id pdf2json/2021.acl-long.73.pdf.json
we O
set O
the O
batch O
size O
to O
4,096 O
( O
8,196 O
) O
in O
pre-training O
( O
finetuning O
) O
. O

section 13
id pdf2json/2021.acl-long.73.pdf.json
we O
pre-train O
( O
fine-tune O
) O
the O
models O
for O
250k O
( O
10k O
) O
steps O
and O
save O
them O
at O
every O
10k O
( O
1k O
) O
steps O
. O

section 13
id pdf2json/2021.acl-long.73.pdf.json
finally O
, O
we O
obtain O
final O
pre-trained O
( O
finetuned O
) O
models O
by O
averaging O
the O
last O
10 O
checkpoints O
. O

section 13
id pdf2json/2021.acl-long.73.pdf.json
evaluation O
. O

section 13
id pdf2json/2021.acl-long.73.pdf.json
we O
evaluate O
on O
ldc2020t07 O
( O
damonte O
and O
cohen O
, O
2018 O
) O
, O
a O
corpus O
containing O
human O
translations O
of O
the O
test O
portion O
of O
1371 O
sentences O
from O
the O
amr O
2.0 O
, O
in O
german O
, O
spanish O
, O
italian O
, O
and O
chinese O
. O

section 13
id pdf2json/2021.acl-long.73.pdf.json
this O
data O
is O
designed O
for O
use O
in O
cross-lingual O
amr O
research O
. O

section 13
id pdf2json/2021.acl-long.73.pdf.json
following O
fan O
and O
gardent O
( O
2020 O
) O
, O
we O
only O
evaluate O
on O
languages O
of O
german O
, O
spanish O
and O
italian O
where O
we O
have O
training O
data O
from O
europarl O
. O

section 13
id pdf2json/2021.acl-long.73.pdf.json
for O
amr O
parsing O
evaluation O
, O
we O
utilize O
smatch O
and O
other O
fine-grained O
metrics O
( O
cai O
and O
knight O
, O
2013 O
; O
damonte O
et O
al. O
, O
2017 O
) O
. O

section 13
id pdf2json/2021.acl-long.73.pdf.json
for O
amr-to-text O
generation O
, O
we O
report O
performance O
in O
bleu O
( O
papineni O
et O
al. O
, O
2002 O
) O
. O

section 14
id pdf2json/2021.acl-long.73.pdf.json
we O
compare O
the O
performance O
of O
our O
approach O
against O
two O
baseline O
systems O
. O

section 14
id pdf2json/2021.acl-long.73.pdf.json
baselinescratch O
. O

section 14
id pdf2json/2021.acl-long.73.pdf.json
to O
build O
this O
baseline O
system O
, O
we O
directly O
train O
models O
from O
scratch O
on O
the O
finetuning O
datasets O
. O

section 14
id pdf2json/2021.acl-long.73.pdf.json
taking O
german O
amr O
parsing O
as O
example O
, O
we O
train O
the O
model O
on O
its O
fine-tuning O
dataset O
( O
fde O
, O
famr O
) O
to O
get O
baselinescratch O
. O

section 14
id pdf2json/2021.acl-long.73.pdf.json
baselinepre-trained O
. O

section 14
id pdf2json/2021.acl-long.73.pdf.json
rather O
than O
training O
models O
from O
scratch O
, O
we O
pre-train O
the O
models O
on O
largescale O
silver O
datasets O
. O

section 14
id pdf2json/2021.acl-long.73.pdf.json
taking O
german O
amr O
parsing O
as O
example O
, O
we O
first O
pre-train O
the O
model O
on O
the O
pretraining O
dataset O
, O
i.e. O
, O
( O
t O
de O
, O
t O
amr O
) O
, O
then O
we O
finetune O
the O
pre-trained O
model O
on O
the O
corresponding O
fine-tuning O
dataset O
, O
i.e. O
, O
( O
fde O
, O
famr O
) O
. O

section 15
id pdf2json/2021.acl-long.73.pdf.json
table O
2 O
shows O
the O
performance O
of O
amr O
parsing O
and O
amr-to-text O
generation O
for O
german O
( O
de O
) O
, O
spanish O
( O
es O
) O
, O
and O
italian O
( O
it O
) O
. O

section 15
id pdf2json/2021.acl-long.73.pdf.json
from O
the O
performance O
comparison O
of O
the O
two O
baseline O
approaches O
, O
it O
is O
not O
surprising O
to O
find O
out O
that O
pre-training O
on O
silver O
datasets O
is O
a O
very O
effective O
way O
to O
boost O
performance O
( O
konstas O
et O
al. O
, O
2017 O
; O
xu O
et O
al. O
, O
2020 O
) O
. O

section 15
id pdf2json/2021.acl-long.73.pdf.json
by O
using O
silver O
datasets O
, O
we O
obtain O
improvements O
of O
6.80 O
∼ O
7.87 O
smatch O
f1 O
, O
and O
6.21 O
∼ O
10.54 O
bleu O
for O
parsing O
and O
text O
generation O
, O
respectively O
. O

section 15
id pdf2json/2021.acl-long.73.pdf.json
with O
any O
of O
our O
fine-tuning O
methods O
, O
our O
cross-lingual O
pre-training O
approach O
further O
improves O
the O
performance O
over O
the O
strong O
baseline O
baselinepre-trained O
in O
both O
parsing O
and O
generation O
tasks O
over O
all O
languages O
. O

section 15
id pdf2json/2021.acl-long.73.pdf.json
it O
shows O
that O
like O
other O
fine-tuning O
methods O
, O
vanilla O
fine-tuning O
significantly O
boosts O
the O
performance O
of O
both O
parsing O
and O
generation O
. O

section 15
id pdf2json/2021.acl-long.73.pdf.json
however O
, O
it O
still O
underperforms O
any O
of O
the O
mtl O
fine-tuning O
methods O
. O

section 15
id pdf2json/2021.acl-long.73.pdf.json
this O
confirms O
that O
it O
is O
important O
to O
optimize O
for O
high O
accuracy O
of O
a O
certain O
fine-tuning O
task O
while O
preserving O
the O
performance O
of O
other O
pre-training O
. O

section 15
id pdf2json/2021.acl-long.73.pdf.json
the O
performance O
comparison O
between O
xlpt-amrone4all O
and O
xlpt-amrtargeted O
suggests O
that O
selectively O
choosing O
relevant O
fine-tuning O
tasks O
, O
rather O
than O
including O
all O
fine-tuning O
tasks O
, O
could O
further O
boost O
parsing O
and O
generation O
performance O
with O
the O
exception O
of O
spanish O
generation O
task O
. O

section 15
id pdf2json/2021.acl-long.73.pdf.json
the O
xlpt-amrt-s O
models O
perform O
the O
best O
, O
which O
reveals O
that O
using O
the O
teacher-student O
framework O
to O
guide O
the O
decoding O
process O
also O
helps O
the O
student O
task O
. O

section 15
id pdf2json/2021.acl-long.73.pdf.json
this O
is O
owing O
to O
fact O
that O
the O
teacher O
models O
achieve O
better O
performance O
than O
the O
student O
models O
. O

section 15
id pdf2json/2021.acl-long.73.pdf.json
see O
more O
in O
section O
5.4 O
for O
performance O
comparison O
of O
teacher O
and O
student O
models O
. O

section 15
id pdf2json/2021.acl-long.73.pdf.json
finally O
, O
we O
compare O
our O
approach O
to O
the O
previous O
studies O
. O

section 15
id pdf2json/2021.acl-long.73.pdf.json
among O
them O
, O
both O
blloshmi O
et O
al O
. O

section 15
id pdf2json/2021.acl-long.73.pdf.json
( O
2020 O
) O
and O
fan O
and O
gardent O
( O
2020 O
) O
adopt O
pretrained O
models O
which O
cover O
either O
the O
encoder O
part O
, O
or O
the O
decoder O
part O
. O

section 15
id pdf2json/2021.acl-long.73.pdf.json
from O
the O
results O
we O
can O
see O
even O
our O
baseline O
baselinepre-trained O
outperforms O
them O
by O
pre-training O
the O
encoder O
and O
the O
decoder O
simultaneously O
. O

section 15
id pdf2json/2021.acl-long.73.pdf.json
the O
results O
also O
show O
that O
our O
xlpt-amrt-s O
models O
greatly O
advance O
the O
state O
of O
art O
. O

section 15
id pdf2json/2021.acl-long.73.pdf.json
for O
example O
, O
our O
xlpt-amrt-s O
models O
outperform O
sheth O
et O
al O
. O

section 15
id pdf2json/2021.acl-long.73.pdf.json
( O
2021 O
) O
by O
3.4∼7.8 O
smatch O
f1 O
on O
amr O
parsing O
of O
the O
three O
languages O
while O
surpass O
fan O
and O
gardent O
( O
2020 O
) O
by O
around O
10 O
bleu O
on O
amr-to-text O
generation O
. O

section 15
id pdf2json/2021.acl-long.73.pdf.json
table O
3 O
compares O
the O
performance O
of O
finegrained O
metrics O
for O
amr O
parsing O
. O

section 15
id pdf2json/2021.acl-long.73.pdf.json
it O
shows O
that O
our O
xlpt-amrt-s O
models O
achieve O
the O
best O
performance O
on O
all O
the O
metrics O
with O
the O
only O
exception O
of O
concepts O
for O
italian O
amr O
parsing O
. O

section 15
id pdf2json/2021.acl-long.73.pdf.json
it O
shows O
that O
like O
english O
amr O
parsing O
, O
all O
models O
predict O
reentrancies O
poorly O
( O
szubert O
et O
al. O
, O
2020 O
) O
. O

section 15
id pdf2json/2021.acl-long.73.pdf.json
it O
also O
demonstrates O
that O
negations O
is O
another O
metric O
which O
is O
hard O
to O
predict O
. O

section 15
id pdf2json/2021.acl-long.73.pdf.json
in O
future O
work O
, O
we O
will O
pay O
particular O
attention O
to O
the O
two O
metrics O
. O

section 16
id pdf2json/2021.acl-long.73.pdf.json
in O
this O
section O
, O
we O
try O
to O
answer O
the O
following O
three O
questions O
: O
• O
first O
, O
what O
is O
the O
performance O
of O
teacher O
models O
when O
we O
use O
teacher O
models O
to O
guide O
student O
ones O
in O
teacher-student-based O
mtl O
finetuning O
? O

section 16
id pdf2json/2021.acl-long.73.pdf.json
• O
second O
, O
what O
is O
the O
effect O
of O
the O
two O
machine O
translation O
tasks O
in O
pre-training O
? O

section 16
id pdf2json/2021.acl-long.73.pdf.json
• O
third O
, O
in O
our O
approach O
we O
take O
english O
as O
pivot O
language O
by O
taking O
advantage O
of O
large O
scale O
english-to-german O
( O
or O
spanish O
, O
italian O
) O
dataset O
. O

section 16
id pdf2json/2021.acl-long.73.pdf.json
what O
is O
the O
performance O
of O
english O
amr O
parsing O
and O
amt-to-text O
generation O
? O

section 16
id pdf2json/2021.acl-long.73.pdf.json
performance O
of O
teacher O
models O
in O
teacherstudent-based O
mtl O
fine-tuning O
. O

section 16
id pdf2json/2021.acl-long.73.pdf.json
table O
4 O
compares O
the O
performance O
of O
teacher O
and O
student O
models O
. O

section 16
id pdf2json/2021.acl-long.73.pdf.json
it O
shows O
that O
the O
performance O
of O
teacher O
models O
for O
english O
amr O
parsing O
and O
english-to-x O
translation O
is O
much O
higher O
than O
the O
counterparts O
of O
student O
models O
( O
i.e. O
, O
stu O
. O

section 16
id pdf2json/2021.acl-long.73.pdf.json
( O
before O
) O
in O
the O
table O
) O
. O

section 16
id pdf2json/2021.acl-long.73.pdf.json
the O
table O
also O
shows O
that O
the O
student O
models O
beneift O
from O
receiving O
guidance O
from O
the O
teachers O
. O

section 16
id pdf2json/2021.acl-long.73.pdf.json
for O
example O
, O
while O
the O
english O
amr O
parsing O
model O
( O
i.e. O
, O
the O
teacher O
) O
achieves O
78.62 O
smatch O
f1 O
on O
the O
test O
set O
, O
it O
improves O
the O
performance O
of O
the O
german O
amr O
parsing O
model O
( O
i.e. O
, O
the O
student O
) O
from O
68.31 O
smatch O
f1 O
to O
70.45 O
. O

section 16
id pdf2json/2021.acl-long.73.pdf.json
similarly O
, O
while O
the O
englishto-german O
model O
( O
i.e. O
, O
the O
teacher O
) O
achieves O
39.40 O
bleu O
on O
the O
test O
set O
, O
it O
boosts O
the O
performance O
of O
the O
german O
amr-to-text O
generation O
model O
( O
i.e. O
, O
the O
student O
) O
from O
24.15 O
bleu O
to O
25.69 O
. O

section 16
id pdf2json/2021.acl-long.73.pdf.json
effect O
of O
machine O
translation O
tasks O
in O
pretraining O
. O

section 16
id pdf2json/2021.acl-long.73.pdf.json
we O
use O
german O
as O
a O
representative O
. O

section 16
id pdf2json/2021.acl-long.73.pdf.json
note O
that O
when O
machine O
translation O
tasks O
are O
not O
involved O
in O
pre-training O
, O
the O
targeted O
mtl O
finetuning O
method O
is O
not O
applicable O
since O
we O
can O
not O
use O
machine O
translation O
as O
the O
auxiliary O
task O
. O

section 16
id pdf2json/2021.acl-long.73.pdf.json
therefore O
, O
we O
use O
the O
vanilla O
fine-tuning O
method O
to O
finetune O
the O
pre-trained O
models O
. O

section 16
id pdf2json/2021.acl-long.73.pdf.json
table O
5 O
compares O
the O
performance O
with/without O
machine O
translation O
tasks O
in O
pre-training O
. O

section 16
id pdf2json/2021.acl-long.73.pdf.json
from O
it O
, O
we O
observe O
that O
including O
machine O
translation O
tasks O
in O
pre-training O
achieves O
improvements O
of O
2.77 O
smatch O
f1 O
and O
2.46 O
bleu O
on O
german O
amr O
parsing O
and O
text O
generation O
, O
respectively O
. O

section 16
id pdf2json/2021.acl-long.73.pdf.json
this O
suggests O
the O
necessity O
to O
have O
machine O
translation O
tasks O
in O
pre-training O
. O

section 16
id pdf2json/2021.acl-long.73.pdf.json
performance O
of O
english O
amr O
parsing O
and O
amr-to-text O
generation O
. O

section 16
id pdf2json/2021.acl-long.73.pdf.json
based O
on O
the O
pretrained O
models O
, O
we O
take O
the O
targeted O
mtl O
finetuning O
method O
( O
section O
4.3 O
) O
as O
a O
representative O
. O

section 16
id pdf2json/2021.acl-long.73.pdf.json
specifically O
, O
for O
english O
amr O
parsing O
, O
we O
choose O
english-to-x O
( O
x O
∈ O
{ O
german O
, O
spanish O
, O
italian O
} O
) O
as O
the O
auxiliary O
fine-tuning O
task O
while O
for O
english O
test O
generation O
, O
we O
choose O
x-to-english O
as O
the O
auxiliary O
task O
. O

section 16
id pdf2json/2021.acl-long.73.pdf.json
table O
6 O
shows O
that O
the O
performance O
of O
english O
parsing O
and O
generation O
is O
much O
higher O
than O
that O
of O
other O
languages O
. O

section 16
id pdf2json/2021.acl-long.73.pdf.json
moreover O
, O
we O
find O
that O
the O
results O
of O
english O
amr O
parsing O
are O
quite O
close O
when O
combining O
english O
with O
any O
of O
other O
languages O
whereas O
the O
results O
of O
english O
amr-to-text O
generation O
are O
considerably O
different O
. O

section 16
id pdf2json/2021.acl-long.73.pdf.json
one O
possible O
reason O
for O
the O
phenomenon O
is O
that O
english O
amr-to-text O
generation O
is O
relevant O
to O
the O
sizes O
of O
machine O
translation O
datasets O
used O
in O
pre-training O
( O
i.e. O
, O
3.9m O
for O
en-de O
translation O
whereas O
1.9m O
for O
both O
en-es O
and O
enit O
, O
respectively O
) O
while O
english O
parsing O
seems O
to O
be O
less O
affected O
by O
the O
sizes O
of O
( O
silver O
) O
datasets O
. O

section 16
id pdf2json/2021.acl-long.73.pdf.json
it O
indicates O
that O
with O
more O
english O
sentences O
in O
pretraining O
, O
it O
helps O
the O
generation O
models O
to O
generate O
more O
fluent O
and O
correct O
english O
sentences O
. O

section 17
id pdf2json/2021.acl-long.73.pdf.json
in O
this O
paper O
we O
proposed O
a O
cross-lingual O
pretraining O
approach O
via O
multi-task O
learning O
for O
zeroshot O
amr O
parsing O
and O
amr-to-text O
generation O
. O

section 17
id pdf2json/2021.acl-long.73.pdf.json
upon O
english O
amr O
dataset O
and O
english-to-x O
parallel O
datasets O
, O
we O
pre-trained O
models O
on O
three O
types O
of O
relevant O
tasks O
, O
including O
amr O
parsing O
, O
amrto-text O
generation O
, O
and O
machine O
translation O
. O

section 17
id pdf2json/2021.acl-long.73.pdf.json
we O
also O
explored O
and O
compared O
four O
different O
finetuning O
methods O
. O

section 17
id pdf2json/2021.acl-long.73.pdf.json
experimentation O
on O
the O
multilingual O
amr O
dataset O
shows O
that O
our O
approach O
greatly O
advances O
the O
state O
of O
the O
art O
. O

section 18
id pdf2json/2021.acl-long.73.pdf.json
this O
work O
was O
supported O
by O
the O
national O
key O
r O
& O
d O
program O
of O
china O
under O
grant O
no O
. O

section 18
id pdf2json/2021.acl-long.73.pdf.json
2020aaa0108600 O
and O
by O
the O
national O
natural O
science O
foundation O
of O
china O
under O
grant O
no O
. O

section 18
id pdf2json/2021.acl-long.73.pdf.json
61876120 O
. O

section 19
id pdf2json/2021.acl-long.73.pdf.json
our O
english O
amr O
parser O
is O
learned O
in O
a O
seq2seq O
framework O
and O
trained O
on O
amr2.0 O
, O
which O
consists O
of O
36,521 O
training O
amrs O
, O
1,368 O
development O
amrs O
and O
1,371 O
testing O
amrs O
. O

section 19
id pdf2json/2021.acl-long.73.pdf.json
we O
share O
vocabulary O
for O
the O
input O
and O
the O
output O
by O
segmenting O
tokens O
into O
pieces O
by O
byte O
pair O
encoding O
( O
bpe O
) O
with O
20k O
merge O
operations O
. O

section 19
id pdf2json/2021.acl-long.73.pdf.json
we O
use O
opennmt-py O
as O
the O
implementation O
of O
transformer O
. O

section 19
id pdf2json/2021.acl-long.73.pdf.json
in O
model O
setting O
, O
we O
use O
transformer O
base O
model O
setting O
. O

section 19
id pdf2json/2021.acl-long.73.pdf.json
we O
use O
adam O
with O
β1 O
= O
0.9 O
, O
β2 O
= O
0.98 O
for O
optimization O
. O

section 19
id pdf2json/2021.acl-long.73.pdf.json
batch O
size O
, O
learning O
rate O
, O
warm-up O
step O
, O
and O
dropout O
rate O
are O
set O
to O
4096 O
, O
2.0 O
, O
16000 O
and O
0.1 O
respectively O
. O

section 19
id pdf2json/2021.acl-long.73.pdf.json
we O
train O
the O
model O
for O
250k O
steps O
on O
1 O
gpus O
and O
save O
models O
every O
10k O
steps O
. O

section 19
id pdf2json/2021.acl-long.73.pdf.json
finally O
, O
we O
obtain O
final O
model O
by O
averaging O
the O
last O
10 O
checkpoints O
. O

section 19
id pdf2json/2021.acl-long.73.pdf.json
the O
english O
amr O
parser O
achieves O
73.68 O
and O
73.24 O
smatch O
f1 O
on O
the O
dev O
and O
test O
set O
, O
respectively O
. O

section 20
id pdf2json/2021.acl-long.73.pdf.json
in O
pre-processing O
, O
we O
tokenize O
all O
of O
mt O
corpus O
with O
moses O
scripts.8 O
then O
we O
segment O
words O
into O
pieces O
by O
bpe O
with O
32k O
( O
30k O
) O
bpe O
merge O
operations O
for O
en-de O
( O
both O
en-es O
and O
en-it O
) O
. O

section 20
id pdf2json/2021.acl-long.73.pdf.json
after O
filtering O
long O
and O
imbalanced O
pairs O
, O
we O
get O
3.9m O
parallel O
sentence O
pairs O
for O
en-de O
and O
1.9m O
for O
both O
en-es O
and O
en-it O
. O

section 20
id pdf2json/2021.acl-long.73.pdf.json
we O
again O
use O
opennmt-py O
as O
the O
implementation O
of O
transformer O
. O

section 20
id pdf2json/2021.acl-long.73.pdf.json
in O
model O
setting O
, O
we O
use O
transformer O
big O
model O
setting O
. O

section 20
id pdf2json/2021.acl-long.73.pdf.json
we O
use O
adam O
with O
β1 O
= O
0.9 O
, O
β2 O
= O
0.998 O
for O
optimization O
. O

section 20
id pdf2json/2021.acl-long.73.pdf.json
batch O
size O
, O
learning O
rate O
, O
warm-up O
step O
, O
and O
dropout O
rate O
are O
set O
to O
8192 O
, O
2.0 O
, O
8000 O
( O
16000 O
for O
both O
en-es O
and O
en-it O
) O
and O
0.1 O
, O
respectively O
. O

section 20
id pdf2json/2021.acl-long.73.pdf.json
we O
train O
the O
model O
for O
100k O
( O
110k O
for O
en-es O
and O
150k O
for O
en-it O
) O
steps O
on O
4 O
gpus O
and O
save O
models O
very O
5000 O
steps O
. O

section 20
id pdf2json/2021.acl-long.73.pdf.json
for O
each O
translation O
task O
, O
we O
obtain O
final O
model O
by O
8https O
: O
//github.com/moses-smt/ O
mosesdecoder O
averaging O
the O
last O
5 O
( O
20 O
for O
both O
en-es O
and O
en-it O
) O
checkpoints O
. O

section 20
id pdf2json/2021.acl-long.73.pdf.json
for O
evaluation O
, O
we O
use O
case-sensitive O
bleu O
measured O
by O
multi-bleu O
script O
. O

section 20
id pdf2json/2021.acl-long.73.pdf.json
table O
7 O
shows O
the O
performance O
of O
the O
three O
translation O
models O
on O
the O
test O
sets O
, O
i.e. O
, O
newstest2014 O
for O
en-de O
and O
newstest2009 O
for O
both O
en-es O
and O
en-it O
. O

section TITLE
id pdf2json/2021.acl-long.421.pdf.json
matching O
distributions O
between O
model O
and O
data O
: O
cross-domain O
knowledge O
distillation O
for O
unsupervised O
domain O
adaptation O

section ABSTRACT
id pdf2json/2021.acl-long.421.pdf.json
unsupervised O
domain O
adaptation O
( O
uda O
) O
aims O
to O
transfer O
the O
knowledge O
of O
source O
domain O
to O
the O
unlabeled O
target O
domain O
. O

section ABSTRACT
id pdf2json/2021.acl-long.421.pdf.json
existing O
methods O
typically O
require O
to O
learn O
to O
adapt O
the O
target O
model O
by O
exploiting O
the O
source O
data O
and O
sharing O
the O
network O
architecture O
across O
domains O
. O

section ABSTRACT
id pdf2json/2021.acl-long.421.pdf.json
however O
, O
this O
pipeline O
makes O
the O
source O
data O
risky O
and O
is O
inflexible O
for O
deploying O
the O
target O
model O
. O

section ABSTRACT
id pdf2json/2021.acl-long.421.pdf.json
this O
paper O
tackles O
a O
novel O
setting O
where O
only O
a O
trained O
source O
model O
is O
available O
and O
different O
network O
architectures O
can O
be O
adapted O
for O
target O
domain O
in O
terms O
of O
deployment O
environments O
. O

section ABSTRACT
id pdf2json/2021.acl-long.421.pdf.json
we O
propose O
a O
generic O
framework O
named O
crossdomain O
knowledge O
distillation O
( O
cdkd O
) O
without O
needing O
any O
source O
data O
. O

section ABSTRACT
id pdf2json/2021.acl-long.421.pdf.json
cdkd O
matches O
the O
joint O
distributions O
between O
a O
trained O
source O
model O
and O
a O
set O
of O
target O
data O
during O
distilling O
the O
knowledge O
from O
the O
source O
model O
to O
the O
target O
domain O
. O

section ABSTRACT
id pdf2json/2021.acl-long.421.pdf.json
as O
a O
type O
of O
important O
knowledge O
in O
the O
source O
domain O
, O
for O
the O
first O
time O
, O
the O
gradient O
information O
is O
exploited O
to O
boost O
the O
transfer O
performance O
. O

section ABSTRACT
id pdf2json/2021.acl-long.421.pdf.json
experiments O
on O
cross-domain O
text O
classification O
demonstrate O
that O
cdkd O
achieves O
superior O
performance O
, O
which O
verifies O
the O
effectiveness O
in O
this O
novel O
setting O
. O

section 0
id pdf2json/2021.acl-long.421.pdf.json
proceedings O
of O
the O
59th O
annual O
meeting O
of O
the O
association O
for O
computational O
linguistics O
and O
the O
11th O
international O
joint O
conference O
on O
natural O
language O
processing O
, O
pages O
5423–5433 O
august O
1–6 O
, O
2021 O
. O

section 0
id pdf2json/2021.acl-long.421.pdf.json
©2021 O
association O
for O
computational O
linguistics O
5423 O

section 1
id pdf2json/2021.acl-long.421.pdf.json
annotating O
sufficient O
training O
data O
is O
usually O
an O
expensive O
and O
time-consuming O
work O
for O
diverse O
application O
domains O
. O

section 1
id pdf2json/2021.acl-long.421.pdf.json
unsupervised O
domain O
adaptation O
( O
uda O
) O
aims O
at O
solving O
this O
learning O
problem O
in O
the O
unlabeled O
target O
domain O
by O
utilizing O
the O
abundant O
knowledge O
in O
an O
existing O
domain O
called O
source O
domain O
, O
even O
when O
these O
domains O
may O
have O
different O
distributions O
. O

section 1
id pdf2json/2021.acl-long.421.pdf.json
this O
technique O
has O
motivated O
research O
on O
cross-domain O
text O
classification O
( O
chen O
et O
al. O
, O
2019 O
; O
ye O
et O
al. O
, O
2020 O
; O
gururangan O
et O
al. O
, O
2020 O
) O
. O

section 1
id pdf2json/2021.acl-long.421.pdf.json
one O
of O
the O
important O
knowledge O
in O
the O
source O
domain O
is O
the O
labels O
of O
samples O
. O

section 1
id pdf2json/2021.acl-long.421.pdf.json
current O
methods O
mainly O
leverage O
the O
labeled O
source O
∗ O
corresponding O
author O
. O

section 1
id pdf2json/2021.acl-long.421.pdf.json
data O
and O
unlabeled O
target O
data O
to O
learn O
the O
domaininvariant O
features O
( O
tzeng O
et O
al. O
, O
2014 O
; O
ganin O
and O
lempitsky O
, O
2015 O
) O
and O
the O
discriminative O
features O
( O
saito O
et O
al. O
, O
2017 O
; O
ge O
et O
al. O
, O
2020 O
) O
that O
are O
shared O
across O
different O
domains O
. O

section 1
id pdf2json/2021.acl-long.421.pdf.json
unfortunately O
, O
sometimes O
we O
are O
forbidden O
access O
to O
the O
source O
data O
, O
which O
are O
distributed O
on O
different O
devices O
and O
usually O
contain O
private O
information O
, O
e.g. O
, O
user O
profile O
. O

section 1
id pdf2json/2021.acl-long.421.pdf.json
existing O
methods O
can O
not O
solve O
the O
uda O
problem O
without O
the O
source O
data O
yet O
. O

section 1
id pdf2json/2021.acl-long.421.pdf.json
in O
addition O
, O
it O
is O
necessary O
to O
adapt O
the O
target O
domain O
with O
a O
flexible O
network O
architecture O
different O
from O
the O
source O
domain O
in O
terms O
of O
different O
deployment O
requirements O
for O
different O
domains O
. O

section 1
id pdf2json/2021.acl-long.421.pdf.json
but O
most O
of O
works O
( O
liang O
et O
al. O
, O
2020 O
; O
li O
et O
al. O
, O
2020 O
) O
are O
required O
to O
share O
the O
same O
network O
architecture O
between O
different O
domains O
. O

section 1
id pdf2json/2021.acl-long.421.pdf.json
in O
this O
paper O
, O
we O
propose O
a O
novel O
uda O
setting O
: O
only O
a O
trained O
source O
model O
and O
a O
set O
of O
unlabeled O
target O
data O
are O
provided O
, O
and O
the O
target O
model O
is O
allowed O
to O
have O
different O
network O
architectures O
with O
the O
trained O
source O
model O
. O

section 1
id pdf2json/2021.acl-long.421.pdf.json
it O
differs O
from O
the O
vanilla O
uda O
in O
that O
a O
trained O
source O
model O
instead O
of O
source O
data O
is O
provided O
as O
supervision O
to O
the O
unlabeled O
target O
domain O
when O
learning O
to O
adapt O
the O
model O
. O

section 1
id pdf2json/2021.acl-long.421.pdf.json
such O
a O
setting O
satisfies O
privacy O
policy O
and O
effective O
delivery O
, O
and O
helps O
deploy O
the O
target O
model O
flexibly O
according O
to O
the O
target O
application O
. O

section 1
id pdf2json/2021.acl-long.421.pdf.json
our O
setting O
seems O
somewhat O
similar O
to O
knowledge O
distillation O
( O
kd O
) O
( O
hinton O
et O
al. O
, O
2015 O
) O
, O
where O
a O
trained O
teacher O
model O
teaches O
a O
student O
model O
with O
different O
architecture O
on O
the O
same O
task O
over O
a O
set O
of O
unlabeled O
data O
. O

section 1
id pdf2json/2021.acl-long.421.pdf.json
kd O
assumes O
that O
the O
empirical O
distribution O
of O
the O
data O
used O
for O
training O
the O
student O
model O
matches O
the O
distribution O
associated O
with O
the O
trained O
teacher O
model O
. O

section 1
id pdf2json/2021.acl-long.421.pdf.json
nevertheless O
, O
in O
our O
setting O
, O
the O
unlabeled O
data O
and O
teacher O
( O
source O
) O
model O
have O
different O
distributions O
. O

section 1
id pdf2json/2021.acl-long.421.pdf.json
one O
of O
simple O
yet O
generic O
solution O
for O
our O
setting O
is O
to O
match O
the O
distributions O
between O
source O
and O
target O
domains O
under O
the O
process O
of O
distilling O
the O
knowledge O
. O

section 1
id pdf2json/2021.acl-long.421.pdf.json
how- O
ever O
, O
it O
is O
quite O
challenging O
to O
reduce O
the O
shifts O
between O
a O
known O
distribution O
( O
e.g. O
, O
a O
trained O
source O
model O
) O
and O
the O
empirical O
distribution O
of O
data O
( O
e.g. O
, O
target O
data O
) O
. O

section 1
id pdf2json/2021.acl-long.421.pdf.json
prior O
methods O
minimize O
a O
distance O
metric O
of O
domain O
discrepancy O
, O
such O
as O
maximum O
mean O
discrepancy O
( O
mmd O
) O
( O
tzeng O
et O
al. O
, O
2014 O
) O
to O
match O
the O
distributions O
across O
domains O
in O
terms O
of O
the O
source O
and O
target O
data O
. O

section 1
id pdf2json/2021.acl-long.421.pdf.json
unfortunately O
, O
the O
empirical O
evaluation O
of O
these O
metrics O
is O
unavailable O
since O
we O
can O
not O
access O
the O
source O
data O
. O

section 1
id pdf2json/2021.acl-long.421.pdf.json
in O
this O
paper O
, O
we O
propose O
a O
generic O
framework O
named O
cross-domain O
knowledge O
distillation O
( O
cdkd O
) O
. O

section 1
id pdf2json/2021.acl-long.421.pdf.json
specifically O
, O
we O
define O
a O
joint O
kernelized O
stein O
discrepancy O
( O
jksd O
) O
that O
measures O
the O
largest O
discrepancy O
over O
the O
hilbert O
space O
of O
functions O
between O
empirical O
sample O
expectations O
of O
target O
domain O
and O
source O
distribution O
expectations O
. O

section 1
id pdf2json/2021.acl-long.421.pdf.json
inspired O
by O
the O
works O
( O
liu O
et O
al. O
, O
2016 O
) O
, O
the O
source O
distribution O
expectations O
are O
being O
zero O
via O
the O
effect O
of O
stein O
operator O
such O
that O
we O
can O
evaluate O
the O
discrepancy O
of O
joint O
distributions O
without O
any O
source O
data O
. O

section 1
id pdf2json/2021.acl-long.421.pdf.json
we O
embed O
jksd O
criterion O
into O
deep O
network O
where O
multi-view O
features O
including O
activations O
, O
gradients O
and O
class O
probabilities O
in O
the O
source O
model O
are O
exploited O
to O
explore O
the O
domain-invariant O
and O
discriminative O
features O
across O
domains O
. O

section 1
id pdf2json/2021.acl-long.421.pdf.json
in O
addition O
, O
we O
further O
maximize O
jksd O
using O
adversarial O
strategy O
where O
the O
multi-view O
features O
are O
integrated O
into O
domain O
adaptation O
abundantly O
. O

section 1
id pdf2json/2021.acl-long.421.pdf.json
finally O
, O
cdkd O
is O
learnt O
by O
joint O
optimizing O
both O
kd O
objective O
( O
hinton O
et O
al. O
, O
2015 O
) O
and O
jksd O
. O

section 1
id pdf2json/2021.acl-long.421.pdf.json
the O
main O
contributions O
are O
outlined O
as O
, O
• O
we O
propose O
to O
investigate O
the O
problem O
of O
uda O
without O
needing O
source O
data O
by O
exploring O
the O
distribution O
discrepancy O
between O
a O
source O
model O
and O
a O
set O
of O
target O
data O
. O

section 1
id pdf2json/2021.acl-long.421.pdf.json
we O
adapt O
the O
target O
domain O
with O
different O
network O
architecture O
flexibly O
in O
terms O
of O
different O
deployment O
environments O
. O

section 1
id pdf2json/2021.acl-long.421.pdf.json
• O
for O
the O
first O
time O
, O
the O
gradient O
information O
of O
the O
source O
domain O
is O
exploited O
to O
boost O
the O
uda O
performance O
. O

section 1
id pdf2json/2021.acl-long.421.pdf.json
mu O
et O
al O
. O

section 1
id pdf2json/2021.acl-long.421.pdf.json
( O
2020 O
) O
shows O
a O
key O
intuition O
that O
per-sample O
gradients O
contain O
task-relevant O
discriminative O
information O
. O

section 1
id pdf2json/2021.acl-long.421.pdf.json
• O
we O
experiment O
under O
two O
amazon O
review O
datasets O
for O
cross-domain O
text O
classification O
, O
which O
demonstrates O
that O
cdkd O
still O
has O
obvious O
performance O
advantage O
in O
all O
settings O
though O
without O
needing O
any O
source O
data O
. O

section 3
id pdf2json/2021.acl-long.421.pdf.json
uda O
aims O
at O
learning O
a O
model O
which O
can O
generalize O
across O
different O
domains O
following O
different O
probability O
distributions O
. O

section 3
id pdf2json/2021.acl-long.421.pdf.json
existing O
works O
mainly O
focus O
on O
how O
to O
learn O
domain-invariant O
features O
and O
discriminative O
features O
that O
are O
shared O
across O
different O
domains O
. O

section 3
id pdf2json/2021.acl-long.421.pdf.json
moment O
matching O
, O
e.g. O
, O
maximum O
mean O
discrepancy O
( O
mmd O
) O
( O
tzeng O
et O
al. O
, O
2014 O
) O
and O
adversarial O
learning O
( O
ganin O
and O
lempitsky O
, O
2015 O
) O
are O
commonly O
used O
to O
learn O
domain-invariant O
features O
by O
aligning O
the O
marginal O
distributions O
. O

section 3
id pdf2json/2021.acl-long.421.pdf.json
to O
learn O
discriminative O
features O
for O
uda O
, O
self-training O
methods O
( O
saito O
et O
al. O
, O
2017 O
; O
zou O
et O
al. O
, O
2019 O
) O
train O
the O
target O
classifier O
in O
terms O
of O
the O
pseudo O
labels O
of O
target O
data O
. O

section 3
id pdf2json/2021.acl-long.421.pdf.json
these O
works O
committed O
to O
improve O
the O
quality O
of O
pseudo O
labels O
including O
introducing O
mutual O
learning O
( O
ge O
et O
al. O
, O
2020 O
) O
and O
dual O
information O
maximization O
( O
ye O
et O
al. O
, O
2020 O
) O
. O

section 3
id pdf2json/2021.acl-long.421.pdf.json
the O
other O
line O
of O
learning O
discriminative O
features O
is O
to O
match O
the O
conditional O
distributions O
across O
domains O
by O
aligning O
multiple O
domain-specific O
layers O
( O
long O
et O
al. O
, O
2017 O
, O
2018 O
) O
or O
making O
an O
explicit O
hypothesis O
between O
conditional O
distributions O
( O
wang O
et O
al. O
, O
2018 O
; O
yu O
et O
al. O
, O
2019 O
; O
fang O
et O
al. O
, O
2020 O
) O
. O

section 3
id pdf2json/2021.acl-long.421.pdf.json
stn O
( O
yao O
et O
al. O
, O
2019 O
) O
explores O
the O
class-conditional O
distributions O
to O
approximate O
the O
discrepancy O
between O
the O
conditional O
distributions O
via O
soft-mmd O
. O

section 3
id pdf2json/2021.acl-long.421.pdf.json
the O
work O
( O
zhang O
et O
al. O
, O
2021 O
) O
derives O
a O
novel O
criterion O
conditional O
mean O
discrepancy O
( O
cmd O
) O
to O
measure O
the O
shifts O
between O
conditional O
distributions O
in O
tensorproduct O
hilbert O
space O
directly O
. O

section 3
id pdf2json/2021.acl-long.421.pdf.json
however O
, O
these O
methods O
assume O
the O
target O
users O
can O
access O
to O
the O
source O
data O
, O
which O
is O
unsafe O
and O
sometimes O
unpractical O
since O
source O
data O
may O
be O
private O
and O
decentralized O
. O

section 3
id pdf2json/2021.acl-long.421.pdf.json
therefore O
, O
the O
recent O
works O
propose O
to O
generalize O
a O
target O
model O
over O
a O
set O
of O
unlabeled O
target O
data O
only O
in O
terms O
of O
the O
supervision O
of O
a O
trained O
source O
model O
. O

section 3
id pdf2json/2021.acl-long.421.pdf.json
shot O
( O
liang O
et O
al. O
, O
2020 O
) O
learns O
the O
target-specific O
feature O
extraction O
module O
by O
using O
both O
information O
maximization O
and O
self-training O
strategy O
. O

section 3
id pdf2json/2021.acl-long.421.pdf.json
li O
et O
al O
. O

section 3
id pdf2json/2021.acl-long.421.pdf.json
( O
2020 O
) O
improve O
the O
target O
model O
through O
target-style O
data O
based O
on O
generative O
adversarial O
network O
( O
gan O
) O
where O
the O
gan O
and O
the O
target O
model O
are O
collaborated O
without O
source O
data O
. O

section 3
id pdf2json/2021.acl-long.421.pdf.json
unfortunately O
, O
they O
require O
that O
the O
target O
model O
must O
share O
the O
same O
network O
architecture O
with O
the O
source O
model O
. O

section 3
id pdf2json/2021.acl-long.421.pdf.json
meanwhile O
, O
multi-view O
features O
in O
the O
source O
model O
including O
activation O
and O
gradient O
are O
not O
exploited O
which O
also O
contribute O
most O
to O
the O
domain O
adaptation O
. O

section 4
id pdf2json/2021.acl-long.421.pdf.json
kd O
transfers O
the O
knowledge O
from O
a O
cumbersome O
model O
to O
a O
small O
model O
that O
is O
more O
suitable O
for O
deployment O
( O
hinton O
et O
al. O
, O
2015 O
) O
. O

section 4
id pdf2json/2021.acl-long.421.pdf.json
the O
general O
technique O
of O
kd O
involves O
using O
a O
teacher-student O
strategy O
, O
where O
a O
large O
deep O
teacher O
model O
trained O
for O
a O
given O
task O
teaches O
shallower O
student O
model O
on O
the O
same O
task O
( O
yim O
et O
al. O
, O
2017 O
; O
chen O
et O
al. O
, O
2018 O
) O
. O

section 4
id pdf2json/2021.acl-long.421.pdf.json
the O
teacher O
and O
student O
models O
are O
trained O
based O
on O
the O
same O
data O
. O

section 4
id pdf2json/2021.acl-long.421.pdf.json
these O
kd O
methods O
make O
an O
assumption O
that O
the O
training O
data O
and O
the O
distribution O
associated O
with O
the O
teacher O
model O
are O
independent O
and O
identically O
distributed O
. O

section 4
id pdf2json/2021.acl-long.421.pdf.json
however O
, O
sometimes O
we O
are O
required O
to O
train O
a O
student O
model O
in O
a O
new O
domain O
that O
the O
teacher O
model O
is O
not O
familiar O
, O
i.e O
, O
the O
domain O
shifts O
exist O
between O
the O
new O
domain O
and O
the O
domain O
that O
the O
teacher O
model O
is O
trained O
. O

section 4
id pdf2json/2021.acl-long.421.pdf.json
the O
proposed O
cdkd O
is O
able O
to O
relieve O
the O
domain O
shifts O
adaptively O
during O
distilling O
the O
knowledge O
. O

section 5
id pdf2json/2021.acl-long.421.pdf.json
we O
address O
the O
unsupervised O
domain O
adaptation O
( O
uda O
) O
task O
with O
only O
a O
trained O
source O
model O
and O
without O
access O
to O
source O
data O
. O

section 5
id pdf2json/2021.acl-long.421.pdf.json
we O
consider O
k-way O
classification O
. O

section 5
id pdf2json/2021.acl-long.421.pdf.json
formally O
, O
in O
this O
novel O
setting O
, O
we O
are O
given O
a O
trained O
source O
model O
fs O
: O
x O
7→ O
y O
and O
a O
target O
domain O
dt O
= O
{ O
xi O
} O
mi=1 O
⊂ O
x O
with O
m O
unlabeled O
samples O
. O

section 5
id pdf2json/2021.acl-long.421.pdf.json
here O
, O
the O
goal O
of O
cross-domain O
knowledge O
distillation O
( O
cdkd O
) O
is O
to O
learn O
a O
target O
model O
ft O
: O
x O
7→ O
y O
and O
infer O
{ O
yi O
} O
mi=1 O
, O
with O
only O
dt O
and O
fs O
available O
. O

section 5
id pdf2json/2021.acl-long.421.pdf.json
the O
target O
model O
ft O
is O
allowed O
to O
have O
different O
network O
architecture O
with O
fs O
. O

section 5
id pdf2json/2021.acl-long.421.pdf.json
cdkd O
is O
a O
special O
kd O
which O
consists O
of O
a O
trained O
teacher O
model O
fs O
, O
a O
student O
model O
ft O
and O
unlabeled O
data O
dt O
as O
well O
. O

section 5
id pdf2json/2021.acl-long.421.pdf.json
but O
it O
differs O
from O
kd O
in O
that O
the O
empirical O
distribution O
of O
dt O
don O
’ O
t O
match O
the O
distribution O
associated O
with O
the O
trained O
model O
fs O
. O

section 5
id pdf2json/2021.acl-long.421.pdf.json
therefore O
, O
it O
is O
necessary O
to O
introduce O
distribution O
adaptation O
to O
eliminate O
the O
biases O
between O
the O
source O
and O
target O
domains O
during O
distilling O
the O
knowledge O
. O

section 5
id pdf2json/2021.acl-long.421.pdf.json
specifically O
, O
as O
shown O
in O
figure O
1 O
( O
a O
) O
, O
we O
first O
introduce O
kd O
to O
distill O
the O
knowledge O
to O
the O
target O
domain O
in O
terms O
of O
the O
class O
probabilities O
produced O
by O
the O
source O
model O
fs O
. O

section 5
id pdf2json/2021.acl-long.421.pdf.json
then O
, O
we O
introduce O
a O
novel O
criterion O
jksd O
to O
match O
the O
joint O
distributions O
across O
domains O
by O
evaluating O
the O
shift O
between O
a O
known O
distribution O
and O
a O
set O
of O
data O
. O

section 5
id pdf2json/2021.acl-long.421.pdf.json
this O
is O
the O
first O
work O
to O
explore O
the O
distribution O
discrepancy O
between O
a O
model O
and O
a O
set O
of O
data O
in O
uda O
task O
. O

section 6
id pdf2json/2021.acl-long.421.pdf.json
given O
a O
target O
sample O
x O
∈ O
dt O
, O
the O
target O
model O
ft O
: O
x O
7→ O
y O
produces O
class O
probabilities O
by O
using O
a O
“ O
softmax O
” O
output O
layer O
that O
converts O
the O
logits O
p O
= O
( O
p1 O
, O
· O
· O
· O
, O
pk O
) O
into O
a O
probability O
ft O
( O
x O
) O
= O
( O
q1 O
, O
· O
· O
· O
, O
qk O
) O
, O
qi O
= O
exp O
( O
pi/t O
) O
∑ O
j O
exp O
( O
pj/t O
) O
where O
t O
is O
a O
temperature O
used O
for O
generating O
“ O
softer O
” O
class O
probabilities O
. O

section 6
id pdf2json/2021.acl-long.421.pdf.json
we O
optimize O
the O
target O
model O
ft O
by O
minimizing O
the O
following O
objective O
for O
knowledge O
distillation O
, O
lkd O
= O
− O
1 O
m O
∑ O
x∈dt O
fs O
( O
x O
) O
> O
log O
ft O
( O
x O
) O
( O
1 O
) O
in O
our O
paper O
, O
the O
setting O
of O
temperature O
follows O
the O
work O
( O
hinton O
et O
al. O
, O
2015 O
) O
: O
a O
high O
temperature O
t O
is O
adopted O
to O
compute O
ft O
( O
x O
) O
during O
training O
, O
but O
after O
it O
has O
been O
trained O
it O
uses O
a O
temperature O
of O
1 O
. O

section 7
id pdf2json/2021.acl-long.421.pdf.json
in O
traditional O
uda O
setting O
, O
joint O
maximum O
mean O
discrepancy O
( O
jmmd O
) O
( O
long O
et O
al. O
, O
2017 O
) O
has O
been O
applied O
to O
measure O
the O
discrepancy O
in O
joint O
distributions O
of O
different O
domains O
, O
and O
it O
can O
be O
estimated O
empirically O
using O
finite O
samples O
of O
source O
and O
target O
domains O
. O

section 7
id pdf2json/2021.acl-long.421.pdf.json
specifically O
, O
suppose O
k O
: O
x O
×x O
7→ O
r O
and O
l O
: O
y O
× O
y O
7→ O
r O
are O
the O
positive O
definite O
kernels O
with O
feature O
maps O
φ O
( O
· O
) O
: O
x O
7→ O
f O
and O
ψ O
( O
· O
) O
: O
y O
7→ O
g O
for O
domains O
of O
x O
and O
y O
, O
respectively O
that O
corresponds O
to O
reproducing O
kernel O
hilbert O
space O
( O
rkhs O
) O
f O
and O
g O
. O

section 7
id pdf2json/2021.acl-long.421.pdf.json
let O
cpxy O
: O
g O
7→ O
f O
be O
the O
uncentered O
cross O
covariance O
operator O
that O
be O
defined O
as O
cpxy O
= O
e O
( O
x O
, O
y O
) O
∼p O
[ O
φ O
( O
x O
) O
⊗ O
ψ O
( O
y O
) O
] O
. O

section 7
id pdf2json/2021.acl-long.421.pdf.json
jmmd O
measures O
the O
shifts O
in O
joint O
distributions O
p O
( O
x O
, O
y O
) O
and O
q O
( O
x O
, O
y O
) O
by O
j O
( O
p O
, O
q O
) O
= O
sup O
f⊗g∈h O
eq O
( O
f O
( O
x O
) O
g O
( O
y O
) O
) O
− O
ep O
( O
f O
( O
x O
) O
g O
( O
y O
) O
) O
=‖cqxy O
− O
c O
p O
xy O
‖f⊗g O
whereh O
is O
a O
unit O
ball O
in O
f O
⊗ O
g O
. O

section 7
id pdf2json/2021.acl-long.421.pdf.json
in O
our O
setting O
, O
unfortunately O
, O
the O
empirical O
estimation O
of O
jmmd O
is O
unavailable O
since O
we O
can O
not O
access O
the O
source O
data O
ds O
directly O
( O
the O
empirical O
estimation O
of O
jmmd O
is O
in O
appendix O
a.1 O
) O
. O

section 7
id pdf2json/2021.acl-long.421.pdf.json
kernelized O
stein O
discrepancy O
( O
ksd O
) O
as O
a O
statistical O
test O
for O
goodness-of-fit O
can O
test O
whether O
a O
set O
of O
samples O
are O
generated O
from O
a O
marginal O
probability O
( O
chwialkowski O
et O
al. O
, O
2016 O
; O
liu O
et O
al. O
, O
2016 O
) O
. O

section 7
id pdf2json/2021.acl-long.421.pdf.json
inspired O
by O
ksd O
, O
we O
introduce O
joint O
ksd O
( O
jksd O
) O
to O
evaluate O
the O
discrepancy O
between O
a O
known O
distribution O
p O
( O
x O
, O
y O
) O
and O
a O
set O
of O
data O
q̂ O
= O
{ O
xi O
, O
yi O
} O
mi=1 O
obtained O
from O
a O
distribution O
q O
( O
x O
, O
y O
) O
. O

section 7
id pdf2json/2021.acl-long.421.pdf.json
assume O
the O
dimension O
of O
x O
is O
d O
( O
x O
= O
rd O
) O
, O
i.e. O
, O
x O
= O
( O
x1 O
, O
· O
· O
· O
, O
xd O
) O
, O
∀x O
∈ O
x O
. O

section 7
id pdf2json/2021.acl-long.421.pdf.json
we O
denote O
by O
f O
d O
= O
f O
×· O
· O
·f O
the O
hilbert O
space O
of O
d×1 O
vector-valued O
functions O
f O
= O
{ O
f1 O
, O
· O
· O
· O
, O
fd O
} O
with O
fi O
∈ O
f O
, O
and O
with O
an O
inner O
product O
〈f O
, O
f O
′〉fd O
= O
∑d O
i=1 O
〈fi O
, O
f O
′i〉f O
for O
f O
′ O
∈ O
f O
d. O
we O
begin O
by O
defining O
a O
stein O
operator O
ap O
: O
f O
d O
⊗ O
g O
7→ O
f O
d O
⊗ O
g O
acting O
on O
functions O
f O
∈ O
f O
d O
and O
g O
∈ O
g O
( O
ap O
f⊗g O
) O
( O
x O
, O
y O
) O
= O
g O
( O
y O
) O
( O
∇xf O
( O
x O
) O
+f O
( O
x O
) O
∇x O
logp O
( O
x O
, O
y O
) O
) O
> O
1d O
( O
2 O
) O
where O
∇x O
logp O
( O
x O
, O
y O
) O
= O
∇xp O
( O
x O
, O
y O
) O
p O
( O
x O
, O
y O
) O
∈ O
r O
d×1 O
, O
∇xf O
( O
x O
) O
= O
( O
∂f1 O
( O
x O
) O
∂x1 O
, O
· O
· O
· O
, O
∂fd O
( O
x O
) O
∂xd O
) O
∈ O
rd×1 O
for O
x O
= O
( O
x1 O
, O
· O
· O
· O
, O
xd O
) O
and O
1d O
is O
a O
d× O
1 O
vector O
with O
all O
elements O
equal O
to O
1 O
. O

section 7
id pdf2json/2021.acl-long.421.pdf.json
the O
expectation O
of O
stein O
operator O
ap O
over O
the O
distribution O
p O
is O
equal O
to O
0 O
ep O
( O
ap O
f O
⊗ O
g O
) O
( O
x O
, O
y O
) O
= O
0 O
( O
3 O
) O
which O
can O
be O
proved O
easily O
by O
( O
chwialkowski O
et O
al. O
, O
2016 O
, O
lemma O
5.1 O
) O
. O

section 7
id pdf2json/2021.acl-long.421.pdf.json
the O
stein O
operator O
ap O
can O
be O
expressed O
by O
defining O
a O
function O
ξxy O
over O
the O
space O
f O
d O
⊗ O
g O
that O
depends O
on O
gradients O
of O
the O
log-distribution O
and O
the O
kernel O
, O
ξxy O
=∇xφ O
( O
x O
) O
⊗ O
ψ O
( O
y O
) O
+ O
( O
∇x O
logp O
( O
x O
, O
y O
) O
) O
φ O
( O
x O
) O
⊗ O
ψ O
( O
y O
) O
( O
4 O
) O
thus O
, O
( O
ap O
f O
⊗ O
g O
) O
( O
x O
, O
y O
) O
can O
be O
presented O
as O
an O
inner O
product O
, O
i.e. O
, O
〈f O
⊗ O
g O
, O
ξxy〉fd⊗g O
. O

section 7
id pdf2json/2021.acl-long.421.pdf.json
now O
, O
we O
can O
define O
jksd O
and O
express O
it O
in O
the O
rkhs O
by O
replacing O
the O
term O
f O
( O
x O
) O
g O
( O
y O
) O
in O
j O
( O
p O
, O
q O
) O
as O
our O
stein O
operator O
, O
s O
( O
p O
, O
q O
) O
: O
= O
sup O
f⊗g∈h′ O
eq O
( O
ap O
f O
⊗ O
g O
) O
( O
x O
, O
y O
) O
− O
ep O
( O
ap O
f O
⊗ O
g O
) O
( O
x O
, O
y O
) O
= O
supeq O
( O
ap O
f O
⊗ O
g O
) O
( O
x O
, O
y O
) O
= O
sup O
〈f O
⊗ O
g O
, O
eqξxy〉fd⊗g O
=‖eqξxy‖fd⊗g O
where O
h′ O
is O
a O
unit O
ball O
in O
f O
d O
⊗ O
g O
. O

section 7
id pdf2json/2021.acl-long.421.pdf.json
this O
makes O
it O
clear O
why O
eq O
. O

section 7
id pdf2json/2021.acl-long.421.pdf.json
3 O
is O
a O
desirable O
property O
: O
we O
can O
compute O
s O
( O
p O
, O
q O
) O
by O
computing O
the O
hilbertschmidt O
norm O
‖eqξxy‖ O
, O
without O
need O
to O
access O
the O
data O
obtained O
from O
p O
. O

section 7
id pdf2json/2021.acl-long.421.pdf.json
we O
can O
empirically O
estimate O
s2 O
( O
p O
, O
q O
) O
based O
on O
the O
known O
probability O
p O
and O
finite O
samples O
q̂ O
= O
{ O
( O
xi O
, O
yi O
) O
} O
mi=1 O
∼ O
q O
( O
x O
, O
y O
) O
in O
term O
of O
kernel O
tricks O
as O
follows O
, O
ŝ2 O
( O
p O
, O
q O
) O
= O
1 O
m2 O
tr O
( O
∇2kl+ O
2υl+ O
ωl O
) O
( O
5 O
) O
( O
∇2k O
) O
i O
, O
j O
= O
〈 O
∇xiφ O
( O
xi O
) O
, O
∇xjφ O
( O
xj O
) O
〉 O
fd O
υi O
, O
j O
= O
( O
∇xik O
( O
xi O
, O
xj O
) O
) O
> O
∇xj O
logp O
( O
xj O
, O
yj O
) O
ωi O
, O
j O
= O
k O
( O
xi O
, O
xj O
) O
( O
∇xi O
logp O
( O
xi O
, O
yi O
) O
> O
∇xj O
logp O
( O
xj O
, O
yj O
) O
) O
where O
l O
= O
{ O
l O
( O
yi O
, O
yj O
) O
} O
is O
the O
kernel O
gram O
matrix O
, O
〈∇xφ O
( O
x O
) O
, O
∇x′φ O
( O
x′ O
) O
〉fd O
= O
∑d O
i=1 O
∂k O
( O
x O
, O
x′ O
) O
∂xi∂x′i O
, O
all O
the O
matrices O
∇2k O
, O
υ O
, O
ω O
and O
l O
are O
in O
rm×m O
, O
and O
tr O
( O
m O
) O
is O
the O
trace O
of O
the O
matrix O
m. O
( O
refer O
to O
appendix O
a.2 O
for O
detail O
. O
) O

section 7
id pdf2json/2021.acl-long.421.pdf.json
in O
our O
experiments O
, O
we O
adopt O
gaussian O
kernel O
k O
( O
x1 O
, O
x2 O
) O
= O
exp O
( O
− O
1σ2 O
‖x1 O
− O
x2‖ O
2 O
) O
where O
its O
derivative O
∇x1k O
( O
x1 O
, O
x2 O
) O
∈ O
rd O
and O
( O
∇2k O
) O
i O
, O
j O
∈ O
r O
can O
be O
computed O
numerically O
, O
∇x1k O
( O
x1 O
, O
x2 O
) O
= O
k O
( O
x1 O
, O
x2 O
) O
( O
− O
2 O
σ2 O
( O
x1 O
− O
x2 O
) O
) O
( O
∇2k O
) O
i O
, O
j O
= O
k O
( O
x1 O
, O
x2 O
) O
( O
2d O
σ2 O
− O
4‖x1 O
− O
x2‖ O
2 O
σ4 O
) O
remark O
. O

section 7
id pdf2json/2021.acl-long.421.pdf.json
based O
on O
the O
virtue O
of O
goodness-fit O
test O
theory O
, O
we O
will O
have O
s O
( O
p O
, O
q O
) O
= O
0 O
if O
and O
only O
if O
p O
= O
q O
( O
chwialkowski O
et O
al. O
, O
2016 O
) O
. O

section 7
id pdf2json/2021.acl-long.421.pdf.json
instead O
of O
applying O
uniform O
weights O
as O
mmd O
does O
, O
jksd O
applies O
non-uniform O
weights O
βi O
, O
j O
, O
ŝ2 O
( O
p O
, O
q O
) O
= O
∑ O
i O
, O
j O
βi O
, O
jl O
( O
yi O
, O
yj O
) O
where O
βi O
, O
j O
= O
( O
∇2k O
+ O
2υ O
+ O
ω O
) O
i O
, O
j O
is O
, O
in O
turn O
, O
determined O
by O
the O
activation-based O
and O
gradient-based O
features O
of O
the O
known O
probability O
p O
. O

section 7
id pdf2json/2021.acl-long.421.pdf.json
jksd O
computes O
a O
dynamic O
weight O
βi O
, O
j O
to O
decide O
whether O
the O
sample O
i O
shares O
the O
same O
label O
with O
other O
sample O
j O
in O
the O
target O
domain O
. O

section 7
id pdf2json/2021.acl-long.421.pdf.json
different O
from O
cluster-based O
methods O
( O
liang O
et O
al. O
, O
2020 O
) O
, O
jksd O
assigns O
each O
sample O
a O
label O
according O
to O
all O
the O
data O
in O
the O
target O
domain O
instead O
of O
the O
centroid O
of O
each O
category O
. O

section 7
id pdf2json/2021.acl-long.421.pdf.json
the O
computation O
of O
centroid O
severely O
suffers O
from O
the O
noise O
due O
to O
the O
domain O
shifts O
. O

section 7
id pdf2json/2021.acl-long.421.pdf.json
in O
contrast O
, O
our O
solution O
is O
more O
suitable O
for O
uda O
because O
we O
avoid O
to O
use O
the O
untrusted O
intermediate O
results O
( O
i.e. O
, O
the O
centroid O
of O
each O
category O
) O
to O
infer O
the O
labels O
. O

section 8
id pdf2json/2021.acl-long.421.pdf.json
the O
pipeline O
of O
our O
cdkd O
framework O
is O
shown O
in O
figure O
1 O
( O
b O
) O
. O

section 8
id pdf2json/2021.acl-long.421.pdf.json
the O
source O
model O
parameterized O
by O
a O
dnn O
consists O
of O
two O
modules O
: O
a O
feature O
extractor O
ts O
: O
x O
7→ O
zs O
and O
a O
classifier O
gs O
: O
zs O
7→ O
y O
, O
i.e. O
, O
fs O
( O
x O
) O
= O
gs O
( O
ts O
( O
x O
) O
) O
. O

section 8
id pdf2json/2021.acl-long.421.pdf.json
the O
target O
model O
ft O
= O
tt◦gt O
also O
has O
two O
modules O
where O
we O
use O
parallel O
notations O
tt O
( O
· O
; O
θt O
) O
: O
x O
7→ O
zt O
and O
gt O
( O
· O
; O
θg O
) O
: O
zt O
7→ O
y O
for O
target O
model O
. O

section 8
id pdf2json/2021.acl-long.421.pdf.json
note O
here O
in O
our O
experiments O
, O
the O
dimension O
of O
the O
latent O
representations O
of O
source O
model O
is O
set O
equal O
to O
the O
target O
model O
, O
i.e. O
, O
zs O
= O
zt O
= O
rd O
. O

section 8
id pdf2json/2021.acl-long.421.pdf.json
the O
extractors O
ts O
and O
tt O
are O
allowed O
to O
adopt O
different O
network O
architectures O
. O

section 8
id pdf2json/2021.acl-long.421.pdf.json
the O
input O
space O
x O
is O
usually O
highly O
sparse O
where O
the O
kernel O
function O
can O
not O
capture O
sufficient O
features O
to O
measure O
the O
similarity O
. O

section 8
id pdf2json/2021.acl-long.421.pdf.json
therefore O
, O
we O
evaluate O
jksd O
based O
on O
latent O
representations O
of O
target O
samples O
, O
i.e. O
, O
q̂ O
= O
{ O
( O
z O
, O
y O
) O
|z O
= O
tt O
( O
x O
) O
, O
y O
= O
gt O
( O
z O
) O
, O
x O
∈ O
dt O
} O
∼ O
q O
( O
z O
, O
y O
) O
. O

section 8
id pdf2json/2021.acl-long.421.pdf.json
in O
eq O
. O

section 8
id pdf2json/2021.acl-long.421.pdf.json
5 O
, O
it O
is O
required O
to O
evaluate O
the O
joint O
probability O
p O
( O
y O
= O
y O
, O
z O
= O
z O
) O
= O
p O
( O
y|z O
) O
p O
( O
z O
) O
over O
a O
sample O
( O
z O
, O
y O
) O
obtained O
from O
q̂ O
. O

section 8
id pdf2json/2021.acl-long.421.pdf.json
the O
probability O
p O
( O
y|z O
) O
that O
the O
sample O
follows O
conditional O
distribution O
of O
the O
source O
domain O
p O
( O
y|z O
) O
can O
be O
evaluated O
as O
p O
( O
y|z O
) O
= O
y O
> O
gs O
( O
z O
) O
. O

section 8
id pdf2json/2021.acl-long.421.pdf.json
similarly O
, O
the O
term O
p O
( O
z O
) O
represents O
the O
probability O
that O
the O
target O
representation O
z O
follows O
the O
marginal O
distribution O
p O
( O
z O
) O
of O
the O
source O
domain O
. O

section 8
id pdf2json/2021.acl-long.421.pdf.json
since O
we O
can O
not O
access O
the O
source O
marginal O
distribution O
directly O
, O
we O
approximate O
it O
by O
evaluating O
the O
cosine O
similarity O
of O
the O
representations O
outputted O
from O
the O
source O
model O
and O
target O
model O
, O
i.e. O
, O
p O
( O
z O
) O
= O
1 O
2 O
cos O
( O
z O
, O
ts O
( O
x O
) O
) O
+ O
1 O
2 O
where O
x O
= O
t−1t O
( O
z O
) O
is O
the O
sample O
corresponding O
to O
z O
for O
any O
z O
∈ O
q̂ O
. O

section 8
id pdf2json/2021.acl-long.421.pdf.json
formally O
, O
the O
term∇z O
logp O
( O
z O
, O
y O
) O
in O
eq O
. O

section 8
id pdf2json/2021.acl-long.421.pdf.json
5 O
can O
be O
computed O
as O
∇z O
logp O
( O
z O
, O
y O
) O
= O
1 O
p O
( O
y|z O
) O
y O
> O
∇zgs O
( O
z O
) O
+ O
∇zp O
( O
z O
) O
p O
( O
z O
) O
where O
∇zgs O
( O
z O
) O
∈ O
rk×d O
is O
a O
jacobian O
matrix O
of O
the O
target O
latent O
representation O
with O
respect O
to O
the O
source O
classifier O
gs O
. O

section 8
id pdf2json/2021.acl-long.421.pdf.json
we O
propose O
to O
train O
the O
target O
model O
ft O
by O
jointly O
distilling O
the O
knowledge O
from O
the O
source O
domain O
and O
reducing O
the O
shifts O
in O
the O
joint O
distributions O
via O
jksd O
, O
min O
θt O
, O
θg O
lkd O
+ O
µŝ O
2 O
( O
p O
, O
q O
) O
where O
µ O
> O
0 O
is O
a O
tradeoff O
parameter O
for O
jksd O
. O

section 8
id pdf2json/2021.acl-long.421.pdf.json
in O
order O
to O
maximize O
the O
test O
power O
of O
jksd O
, O
we O
require O
the O
class O
of O
functions O
h O
∈ O
f O
d⊗g O
to O
be O
rich O
enough O
. O

section 8
id pdf2json/2021.acl-long.421.pdf.json
meanwhile O
, O
kernel-based O
metrics O
usually O
suffer O
from O
vanishing O
gradients O
for O
low-bandwidth O
kernels O
. O

section 8
id pdf2json/2021.acl-long.421.pdf.json
we O
are O
enlightened O
by O
( O
long O
et O
al. O
, O
2017 O
) O
which O
introduces O
the O
adversarial O
training O
to O
circumvent O
these O
issues O
. O

section 8
id pdf2json/2021.acl-long.421.pdf.json
specifically O
, O
we O
multiple O
fully O
connected O
layers O
u O
and O
v O
parameterized O
by O
θu O
and O
θv O
to O
jksd O
, O
i.e. O
, O
k O
( O
xi O
, O
xj O
) O
and O
l O
( O
yi O
, O
yj O
) O
are O
replaced O
as O
k O
( O
u O
( O
xi O
) O
, O
u O
( O
xj O
) O
) O
and O
l O
( O
v O
( O
yi O
) O
, O
v O
( O
yj O
) O
) O
in O
eq O
. O

section 8
id pdf2json/2021.acl-long.421.pdf.json
5 O
. O

section 8
id pdf2json/2021.acl-long.421.pdf.json
we O
maximize O
jksd O
with O
respect O
to O
the O
new O
parameters O
θu O
and O
θv O
to O
maximize O
the O
test O
power O
of O
jksd O
such O
that O
the O
samples O
in O
the O
target O
domain O
are O
made O
more O
discriminative O
by O
abundantly O
exploiting O
the O
activation O
and O
gradient O
features O
in O
the O
source O
domain O
. O

section 8
id pdf2json/2021.acl-long.421.pdf.json
as O
shown O
in O
figure O
1 O
( O
c O
) O
, O
the O
target O
model O
ft O
can O
be O
optimized O
by O
the O
following O
adversarial O
objective O
, O
min O
θt O
, O
θg O
max O
θu O
, O
θv O
lkd O
+ O
µŝ O
2 O
( O
p O
, O
q O
) O
( O
6 O
) O

section 10
id pdf2json/2021.acl-long.421.pdf.json
to O
testify O
its O
versatility O
, O
we O
evaluate O
the O
proposed O
model O
in O
two O
tasks O
including O
uda O
and O
knowledge O
distillation O
. O

section 10
id pdf2json/2021.acl-long.421.pdf.json
amazon-review1 O
is O
a O
benchmark O
dataset O
for O
domain O
adaptation O
in O
text O
classification O
task O
. O

section 10
id pdf2json/2021.acl-long.421.pdf.json
two O
versions O
of O
amazon O
review O
datasets O
are O
used O
to O
evaluate O
models O
. O

section 10
id pdf2json/2021.acl-long.421.pdf.json
the O
work O
provides O
a O
simplified O
amazon-review O
dataset O
( O
amazon-feature O
) O
collected O
from O
four O
distinct O
domains O
: O
books O
( O
b O
) O
, O
dvd O
( O
d O
) O
, O
electronics O
( O
e O
) O
and O
kitchen O
( O
k O
) O
. O

section 10
id pdf2json/2021.acl-long.421.pdf.json
each O
domain O
comprises O
4,000 O
samples O
with O
400d O
feature O
representations O
and O
2 O
categories O
( O
positive O
and O
negative O
) O
. O

section 10
id pdf2json/2021.acl-long.421.pdf.json
zhang O
et O
al O
. O

section 10
id pdf2json/2021.acl-long.421.pdf.json
( O
2021 O
) O
collected O
a O
larger O
dataset O
called O
amazon-text O
from O
amazon-review O
with O
the O
same O
domains O
in O
amazon-feature O
to O
test O
the O
model O
performance O
for O
large-scale O
transfer O
learning O
. O

section 10
id pdf2json/2021.acl-long.421.pdf.json
the O
review O
texts O
are O
divided O
into O
two O
categories O
according O
to O
user O
rating O
, O
i.e. O
, O
positive O
( O
5 O
stars O
) O
and O
negative O
( O
1 O
star O
) O
. O

section 10
id pdf2json/2021.acl-long.421.pdf.json
there O
are O
10,000 O
original O
review O
texts O
in O
each O
category O
and O
20,000 O
texts O
in O
each O
domain O
. O

section 10
id pdf2json/2021.acl-long.421.pdf.json
the O
notation O
s→t O
represents O
the O
transfer O
learning O
from O
the O
source O
domain O
s O
to O
the O
target O
domain O
t. O
baselines O
. O

section 10
id pdf2json/2021.acl-long.421.pdf.json
for O
the O
bulk O
of O
experiments O
the O
following O
baselines O
are O
evaluated O
. O

section 10
id pdf2json/2021.acl-long.421.pdf.json
the O
source-only O
model O
is O
trained O
only O
over O
source O
domain O
and O
tested O
over O
target-domain O
data O
while O
train-on-target O
model O
is O
trained O
and O
tested O
over O
target-domain O
data O
directly O
. O

section 10
id pdf2json/2021.acl-long.421.pdf.json
we O
compare O
with O
conventional O
domain O
adaptation O
methods O
: O
transfer O
component O
analysis O
( O
tca O
) O
( O
pan O
et O
al. O
, O
2010 O
) O
, O
balanced O
distribution O
adaptation O
( O
bda O
) O
( O
wang O
et O
al. O
, O
2017 O
) O
, O
geodesic O
flow O
kernel O
( O
gfk O
) O
( O
gong O
et O
al. O
, O
2012 O
) O
, O
deep O
domain O
confusion O
( O
ddc O
) O
( O
tzeng O
et O
al. O
, O
2014 O
) O
, O
domain O
adversarial O
neural O
networks O
( O
revgrad O
) O
( O
ganin O
and O
lempitsky O
, O
2015 O
) O
and O
dynamic O
adversarial O
adaptation O
network O
( O
daan O
) O
( O
yu O
et O
al. O
, O
2019 O
) O
. O

section 10
id pdf2json/2021.acl-long.421.pdf.json
we O
compare O
with O
shot O
( O
liang O
et O
al. O
, O
2020 O
) O
for O
the O
uda O
task O
without O
the O
source O
data O
. O

section 10
id pdf2json/2021.acl-long.421.pdf.json
1http O
: O
//jmcauley.ucsd.edu/data/amazon/ O
we O
also O
compare O
with O
the O
knowledge O
distillation O
method O
( O
kd O
) O
( O
hinton O
et O
al. O
, O
2015 O
) O
in O
our O
setting O
. O

section 10
id pdf2json/2021.acl-long.421.pdf.json
in O
our O
experiments O
, O
three O
different O
extractors O
are O
selected O
. O

section 10
id pdf2json/2021.acl-long.421.pdf.json
for O
amazon-feature O
dataset O
, O
the O
extractor O
is O
simply O
modeled O
as O
a O
typical O
3-layer O
fully O
connected O
network O
( O
mlp O
) O
to O
transform O
400d O
inputs O
into O
50d O
latent O
feature O
vectors O
. O

section 10
id pdf2json/2021.acl-long.421.pdf.json
two O
types O
of O
networks O
are O
leveraged O
for O
amazon-text O
dataset O
to O
encode O
the O
original O
review O
texts O
, O
i.e. O
, O
textcnn O
and O
bertgru O
. O

section 10
id pdf2json/2021.acl-long.421.pdf.json
textcnn O
( O
kim O
, O
2014 O
) O
is O
a O
text O
convolutional O
network O
that O
consists O
of O
150 O
convolutional O
filters O
with O
3 O
different O
window O
sizes O
. O

section 10
id pdf2json/2021.acl-long.421.pdf.json
we O
also O
evaluate O
the O
performance O
of O
cross-domain O
text O
classification O
on O
a O
pre-trained O
language O
model O
, O
i.e. O
, O
bert O
( O
devlin O
et O
al. O
, O
2019 O
) O
. O

section 10
id pdf2json/2021.acl-long.421.pdf.json
we O
freeze O
bert O
model O
and O
construct O
a O
2-layer O
bi-directional O
gru O
( O
cho O
et O
al. O
, O
2014 O
) O
to O
learn O
from O
the O
representations O
produced O
by O
bert O
. O

section 10
id pdf2json/2021.acl-long.421.pdf.json
the O
classifier O
is O
modeled O
as O
a O
2-layer O
fully O
connected O
network O
for O
all O
the O
settings O
. O

section 10
id pdf2json/2021.acl-long.421.pdf.json
for O
cdkd O
, O
we O
consider O
to O
learn O
the O
source O
model O
fs O
by O
minimizing O
the O
standard O
cross-entropy O
loss O
. O

section 10
id pdf2json/2021.acl-long.421.pdf.json
we O
randomly O
specify O
a O
0.7/0.3 O
split O
in O
the O
source O
dataset O
and O
generate O
the O
optimal O
source O
model O
based O
on O
the O
validation O
split O
. O

section 10
id pdf2json/2021.acl-long.421.pdf.json
u O
and O
v O
are O
modeled O
as O
weight O
matrices O
. O

section 10
id pdf2json/2021.acl-long.421.pdf.json
we O
implement O
all O
deep O
methods O
based O
on O
pytorch O
framework O
, O
and O
bert O
model O
is O
implemented O
and O
pre-trained O
by O
pytorch-transformers2 O
. O

section 10
id pdf2json/2021.acl-long.421.pdf.json
we O
adopt O
gaussian O
kernel O
with O
bandwidth O
set O
to O
median O
pairwise O
squared O
distances O
on O
the O
training O
data O
( O
gretton O
et O
al. O
, O
2012 O
) O
. O

section 10
id pdf2json/2021.acl-long.421.pdf.json
the O
temperature O
t O
is O
set O
to O
10 O
during O
training O
. O

section 10
id pdf2json/2021.acl-long.421.pdf.json
we O
use O
adamw O
optimizer O
( O
loshchilov O
and O
hutter O
, O
2019 O
) O
with O
batch O
size O
of O
128 O
and O
the O
learning O
rate O
annealing O
strategy O
in O
( O
long O
et O
al. O
, O
2017 O
) O
: O
it O
is O
adjusted O
during O
back O
propagation O
using O
the O
following O
formula O
: O
2https O
: O
//github.com/huggingface/ O
transformers O
models O
e→b O
k→b O
b→d O
e→d O
k→d O
b→e O
d→e O
d→k O
avg O
textcnn O
69.5 O
67.4 O
79.7 O
72.9 O
71.2 O
70.2 O
64.6 O
65.5 O
70.1 O
bertgru O
83.8 O
84.4 O
91.3 O
87.0 O
88.6 O
84.8 O
79.1 O
79.7 O
84.8 O
kd O
( O
hinton O
et O
al. O
, O
2015 O
) O
83.1 O
81.8 O
87.0 O
86.3 O
85.8 O
82.6 O
78.5 O
78.2 O
82.9 O
cdkd O
( O
our O
) O
83.8 O
83.5 O
87.9 O
86.7 O
86.6 O
83.9 O
82.3 O
81.8 O
84.6 O
ηp O
= O
η0 O
( O
1+10p O
) O
0.75 O
where O
p O
is O
the O
training O
progress O
linearly O
changing O
from O
0 O
to O
1 O
and O
η0 O
is O
set O
to O
0.001 O
. O

section 10
id pdf2json/2021.acl-long.421.pdf.json
we O
apply O
the O
same O
strategy O
in O
( O
ganin O
and O
lempitsky O
, O
2015 O
) O
to O
adjust O
the O
factor O
µ O
dynamically O
, O
i.e. O
, O
we O
gradually O
change O
it O
from O
0 O
to O
1 O
by O
a O
progressive O
schedule O
: O
µp O
= O
21+exp O
( O
−10p O
) O
− O
1 O
. O

section 11
id pdf2json/2021.acl-long.421.pdf.json
in O
the O
first O
experiment O
, O
we O
compare O
with O
the O
conventional O
domain O
adaptation O
methods O
where O
the O
source O
model O
and O
target O
model O
share O
the O
same O
network O
architectures O
. O

section 11
id pdf2json/2021.acl-long.421.pdf.json
the O
classification O
accuracy O
results O
on O
the O
amazon-feature O
dataset O
for O
domain O
adaptation O
based O
on O
mlp O
are O
shown O
in O
table O
1 O
. O

section 11
id pdf2json/2021.acl-long.421.pdf.json
some O
of O
the O
observations O
and O
analysis O
are O
listed O
as O
follows O
. O

section 11
id pdf2json/2021.acl-long.421.pdf.json
( O
1 O
) O
the O
performance O
of O
traditional O
uda O
methods O
( O
e.g. O
, O
tca O
, O
gfk O
and O
bda O
) O
is O
worse O
than O
source-only O
model O
, O
i.e. O
, O
negative O
transfer O
learning O
occurs O
in O
all O
transfer O
tasks O
. O

section 11
id pdf2json/2021.acl-long.421.pdf.json
these O
models O
directly O
define O
kernel O
over O
sparse O
input O
vectors O
such O
that O
the O
kernel O
function O
can O
not O
capture O
sufficient O
features O
to O
measure O
the O
similarity O
. O

section 11
id pdf2json/2021.acl-long.421.pdf.json
the O
deep O
transfer O
methods O
outperform O
all O
the O
traditional O
methods O
, O
suggesting O
that O
embedding O
domain O
adaptation O
modules O
into O
deep O
network O
can O
reduce O
domain O
discrepancy O
significantly O
. O

section 11
id pdf2json/2021.acl-long.421.pdf.json
( O
2 O
) O
the O
average O
accuracy O
of O
cdkd O
is O
slightly O
1.0 O
% O
higher O
than O
other O
deep O
transfer O
methods O
( O
ddc O
, O
revgrad O
, O
daan O
and O
shot O
) O
overall O
. O

section 11
id pdf2json/2021.acl-long.421.pdf.json
it O
verifies O
the O
positive O
effect O
of O
transferring O
the O
knowledge O
from O
trained O
source O
model O
without O
accessing O
the O
source O
data O
. O

section 11
id pdf2json/2021.acl-long.421.pdf.json
table O
2 O
shows O
the O
classification O
performance O
of O
deep O
uda O
models O
based O
on O
textcnn O
and O
bertgru O
over O
a O
large O
dataset O
amazon-text O
. O

section 11
id pdf2json/2021.acl-long.421.pdf.json
for O
textcnn O
extractor O
, O
we O
have O
following O
analysis O
. O

section 11
id pdf2json/2021.acl-long.421.pdf.json
cdkd O
achieves O
superior O
performance O
over O
prior O
methods O
by O
larger O
margins O
compared O
to O
small O
dataset O
amazon-feature O
. O

section 11
id pdf2json/2021.acl-long.421.pdf.json
compared O
to O
ddc O
and O
revgrad O
that O
obtains O
the O
domain-invariant O
features O
, O
cdkd O
can O
learn O
discriminative O
information O
from O
the O
source O
model O
by O
minimizing O
jksd O
criterion O
. O

section 11
id pdf2json/2021.acl-long.421.pdf.json
shot O
assumes O
that O
the O
target O
outputs O
should O
be O
similar O
to O
one-hot O
encoding O
. O

section 11
id pdf2json/2021.acl-long.421.pdf.json
however O
, O
the O
onehot O
encoding O
used O
in O
shot O
is O
noisy O
and O
untrusted O
due O
to O
the O
domain O
shifts O
. O

section 11
id pdf2json/2021.acl-long.421.pdf.json
different O
from O
shot O
, O
we O
match O
the O
joint O
distributions O
across O
domains O
in O
terms O
of O
multi-view O
features O
rather O
than O
only O
class O
probabilities O
when O
adapting O
the O
target O
model O
. O

section 11
id pdf2json/2021.acl-long.421.pdf.json
by O
going O
from O
textcnn O
to O
extremely O
deep O
bertgru O
, O
we O
attain O
a O
more O
in-depth O
understanding O
of O
feature O
transferability O
. O

section 11
id pdf2json/2021.acl-long.421.pdf.json
bertgru-based O
models O
outperform O
textcnn-based O
models O
significantly O
, O
which O
shows O
bert O
enables O
learning O
more O
transferable O
representations O
for O
uda O
. O

section 11
id pdf2json/2021.acl-long.421.pdf.json
our O
cdkd O
has O
a O
slight O
advantage O
compared O
to O
other O
models O
overall O
under O
the O
powerful O
transferability O
of O
bertgru O
. O

section 11
id pdf2json/2021.acl-long.421.pdf.json
it O
reveals O
the O
necessity O
of O
designing O
a O
moment O
matching O
approach O
to O
incorporate O
activation O
and O
gradient O
features O
into O
domain O
adaptation O
for O
reducing O
the O
losses O
caused O
by O
the O
lack O
of O
source O
data O
. O

section 11
id pdf2json/2021.acl-long.421.pdf.json
in O
the O
second O
experiment O
, O
we O
compare O
with O
the O
kd O
model O
where O
the O
knowledge O
in O
bertgru O
is O
distilled O
to O
the O
textcnn-based O
model O
. O

section 11
id pdf2json/2021.acl-long.421.pdf.json
we O
generate O
the O
optimal O
bertgru O
as O
the O
teacher O
model O
based O
on O
the O
source O
dataset O
. O

section 11
id pdf2json/2021.acl-long.421.pdf.json
the O
textcnn O
model O
uses O
bert O
tokenizer O
tool O
to O
guarantee O
the O
same O
input O
space O
between O
two O
models O
. O

section 11
id pdf2json/2021.acl-long.421.pdf.json
we O
randomly O
specify O
a O
0.5/0.2/0.3 O
split O
in O
the O
target O
dataset O
where O
we O
train O
and O
select O
textcnn-based O
model O
based O
on O
the O
train O
split O
and O
validation O
split O
respectively O
. O

section 11
id pdf2json/2021.acl-long.421.pdf.json
the O
result O
is O
reported O
in O
table O
3 O
in O
terms O
of O
the O
test O
split O
. O

section 11
id pdf2json/2021.acl-long.421.pdf.json
the O
average O
accuracy O
of O
cdkd O
is O
1.6 O
% O
higher O
than O
original O
kd O
and O
approaches O
to O
the O
teacher O
model O
bertgru O
. O

section 11
id pdf2json/2021.acl-long.421.pdf.json
significantly O
, O
the O
accuracy O
scores O
of O
tasks O
d O
→ O
e O
and O
d O
→ O
k O
are O
higher O
than O
bertgru O
. O

section 11
id pdf2json/2021.acl-long.421.pdf.json
this O
is O
attributed O
to O
distribution O
adaptation O
where O
extra O
performance O
is O
also O
gained O
from O
jksd O
besides O
the O
guidance O
of O
the O
teacher O
model O
. O

section 12
id pdf2json/2021.acl-long.421.pdf.json
ablation O
study O
. O

section 12
id pdf2json/2021.acl-long.421.pdf.json
we O
conduct O
the O
ablation O
experiments O
to O
see O
the O
contributions O
of O
gradient O
information O
( O
g O
) O
and O
the O
adversarial O
strategy O
( O
a O
) O
, O
which O
are O
evaluated O
with O
textcnn O
extractor O
for O
uda O
task O
. O

section 12
id pdf2json/2021.acl-long.421.pdf.json
by O
ablating O
cdkd O
, O
we O
have O
two O
baselines O
of O
cdkd-g O
( O
w/o O
g O
) O
and O
cdkd-a O
( O
w/o O
a O
) O
. O

section 12
id pdf2json/2021.acl-long.421.pdf.json
for O
cdkd-g O
, O
we O
set O
the O
gradient O
of O
log-distribution O
∇xj O
logp O
( O
xj O
, O
yj O
) O
∈ O
rd×1 O
to O
a O
constant O
, O
i.e. O
, O
1 O
d O
( O
1 O
, O
1 O
, O
... O
, O
1 O
) O
> O
while O
we O
optimize O
cdkd O
without O
adversarial O
strategy O
for O
cdkd-a O
. O

section 12
id pdf2json/2021.acl-long.421.pdf.json
from O
the O
results O
in O
figure O
2 O
, O
cdkd-g O
and O
cdkd-a O
perform O
worse O
than O
cdkd O
but O
still O
better O
than O
kd O
, O
suggesting O
that O
gradient O
information O
and O
the O
adversarial O
strategy O
both O
contribute O
to O
the O
improvements O
of O
our O
model O
. O

section 12
id pdf2json/2021.acl-long.421.pdf.json
the O
gradient O
information O
is O
one O
type O
of O
important O
knowledge O
in O
the O
source O
domain O
, O
but O
all O
previous O
methods O
ignore O
its O
importance O
for O
uda O
. O

section 12
id pdf2json/2021.acl-long.421.pdf.json
effects O
of O
source O
model O
accuracy O
. O

section 12
id pdf2json/2021.acl-long.421.pdf.json
here O
we O
study O
how O
the O
performance O
of O
target O
model O
are O
influenced O
by O
the O
source O
model O
accuracy O
, O
which O
are O
analyzed O
based O
on O
b O
→ O
e O
task O
using O
textcnn O
extractor O
. O

section 12
id pdf2json/2021.acl-long.421.pdf.json
we O
randomly O
obtain O
9 O
optimal O
source O
models O
using O
different O
seeds O
over O
b O
dataset O
, O
and O
train O
cdkd O
and O
kd O
models O
based O
on O
different O
source O
models O
for O
b O
→ O
e O
task O
. O

section 12
id pdf2json/2021.acl-long.421.pdf.json
figure O
3 O
shows O
the O
classification O
accuracy O
of O
cdkd O
and O
kd O
by O
varying O
accuracy O
of O
source O
models O
tested O
over O
e O
dataset O
. O

section 12
id pdf2json/2021.acl-long.421.pdf.json
cdkd O
obtains O
similar O
performance O
under O
different O
source O
models O
, O
indicating O
that O
cdkd O
is O
not O
very O
sensitive O
to O
the O
quality O
of O
source O
models O
. O

section 12
id pdf2json/2021.acl-long.421.pdf.json
however O
, O
the O
curves O
of O
kd O
is O
unstable O
, O
i.e. O
, O
the O
performance O
of O
kd O
is O
vulnerable O
to O
the O
impact O
of O
the O
source O
models O
, O
because O
different O
source O
models O
follow O
the O
different O
distributions O
. O

section 12
id pdf2json/2021.acl-long.421.pdf.json
obviously O
, O
jksd O
plays O
a O
crucial O
role O
in O
determining O
the O
effects O
of O
alleviating O
this O
distribution O
discrepancy O
among O
different O
source O
models O
. O

section 12
id pdf2json/2021.acl-long.421.pdf.json
effects O
of O
batch O
size O
. O

section 12
id pdf2json/2021.acl-long.421.pdf.json
batch O
size O
is O
a O
key O
parameter O
to O
optimize O
jksd O
metric O
because O
it O
is O
required O
to O
compute O
kernel O
over O
a O
min-batch O
of O
data O
. O

section 12
id pdf2json/2021.acl-long.421.pdf.json
figure O
4 O
shows O
the O
classification O
accuracy O
of O
cdkd O
by O
varying O
batch O
size O
in O
{ O
64 O
, O
128 O
, O
256 O
, O
512 O
} O
. O

section 12
id pdf2json/2021.acl-long.421.pdf.json
the O
experiment O
shows O
that O
cdkd O
is O
not O
sensitive O
to O
batch O
size O
when O
batch O
size O
is O
larger O
than O
64 O
, O
suggesting O
that O
cdkd O
don O
’ O
t O
need O
a O
very O
large O
batch O
size O
for O
accurate O
estimation O
of O
jksd O
. O

section 13
id pdf2json/2021.acl-long.421.pdf.json
in O
this O
paper O
, O
we O
shed O
a O
new O
light O
on O
the O
challenges O
of O
uda O
without O
needing O
source O
data O
. O

section 13
id pdf2json/2021.acl-long.421.pdf.json
specifically O
, O
we O
provided O
a O
generic O
framework O
named O
cdkd O
to O
learn O
a O
classification O
model O
over O
a O
set O
of O
unlabeled O
target O
data O
by O
making O
use O
of O
the O
knowledge O
of O
the O
activation O
and O
gradient O
information O
in O
the O
trained O
source O
model O
. O

section 13
id pdf2json/2021.acl-long.421.pdf.json
cdkd O
learned O
the O
collective O
knowledge O
across O
different O
domains O
including O
domain-invariant O
and O
discriminative O
features O
by O
matching O
the O
joint O
distributions O
between O
a O
trained O
source O
model O
and O
a O
set O
of O
target O
data O
. O

section 13
id pdf2json/2021.acl-long.421.pdf.json
experiments O
for O
cross-domain O
text O
classification O
testified O
that O
cdkd O
still O
achieves O
advantages O
for O
uda O
task O
though O
without O
any O
source O
data O
and O
improves O
the O
performance O
of O
kd O
task O
when O
the O
trained O
teacher O
model O
doesn O
’ O
t O
match O
the O
training O
data O
. O

section 14
id pdf2json/2021.acl-long.421.pdf.json
this O
work O
was O
supported O
in O
part O
by O
national O
natural O
science O
foundation O
of O
china O
under O
grant O
62001309 O
, O
in O
part O
by O
the O
opening O
project O
of O
beijing O
key O
laboratory O
of O
internet O
culture O
and O
digital O
dissemination O
research O
and O
in O
part O
by O
the O
open O
research O
fund O
from O
shenzhen O
research O
institute O
of O
big O
data O
( O
no O
. O

section 14
id pdf2json/2021.acl-long.421.pdf.json
2019orf01012 O
) O
. O

section 15
id pdf2json/2021.acl-long.421.pdf.json
a.1 O
empirical O
evaluation O
of O
jmmd O
jmmd O
j O
( O
p O
, O
q O
) O
measures O
the O
shifts O
in O
joint O
distributions O
p O
( O
x O
, O
y O
) O
and O
q O
( O
x O
, O
y O
) O
by O
sup O
f⊗g∈h O
eq O
( O
f O
( O
x O
) O
g O
( O
y O
) O
) O
− O
ep O
( O
f O
( O
x O
) O
g O
( O
y O
) O
) O
= O
supeq O
( O
〈f O
⊗ O
g O
, O
φ O
( O
x O
) O
⊗ O
ψ O
( O
y O
) O
〉 O
) O
− O
ep O
( O
〈f O
⊗ O
g O
, O
φ O
( O
x O
) O
⊗ O
ψ O
( O
y O
) O
〉 O
) O
= O
sup O
〈 O
f O
⊗ O
g O
, O
cqxy O
− O
c O
p O
xy O
〉 O
f⊗g O
= O
∥∥∥cqxy O
− O
cpxy O
∥∥∥ O
f⊗g O
given O
a O
source O
domain O
ds O
= O
{ O
( O
xsi O
, O
ysi O
) O
} O
ni=1 O
∼ O
p O
( O
x O
, O
y O
) O
and O
a O
target O
domain O
dt O
= O
{ O
( O
xtj O
, O
ytj O
) O
} O
mj=1 O
∼ O
q O
( O
x O
, O
y O
) O
, O
the O
empirical O
estimation O
of O
jmmd O
is O
, O
ĵ2 O
( O
p O
, O
q O
) O
= O
1 O
n2 O
tr O
( O
ksslss O
) O
+ O
1 O
m2 O
tr O
( O
kttltt O
) O
− O
2 O
mn O
tr O
( O
kstlts O
) O
( O
7 O
) O
where O
( O
kst O
) O
i O
, O
j O
= O
k O
( O
xsi O
, O
x O
t O
j O
) O
and O
( O
lst O
) O
i O
, O
j O
= O
l O
( O
ysi O
, O
y O
t O
j O
) O
are O
gram O
matrices O
, O
and O
tr O
( O
a O
) O
is O
the O
trace O
of O
the O
matrix O
a O
. O

section 15
id pdf2json/2021.acl-long.421.pdf.json
the O
eq O
. O

section 15
id pdf2json/2021.acl-long.421.pdf.json
7 O
applies O
the O
source O
data O
kst O
, O
kss O
, O
lst O
and O
lss O
to O
compute O
the O
score O
of O
jmmd O
, O
which O
can O
not O
adapt O
to O
our O
new O
setting O
obviously O
. O

section 15
id pdf2json/2021.acl-long.421.pdf.json
note O
here O
that O
the O
jmmd O
used O
in O
our O
paper O
is O
a O
simplified O
version O
of O
( O
long O
et O
al. O
, O
2017 O
) O
, O
where O
we O
only O
consider O
two O
variables O
. O

section 15
id pdf2json/2021.acl-long.421.pdf.json
a.2 O
empirical O
evaluation O
of O
jksd O
denote O
λxy O
= O
∇xφ O
( O
x O
) O
+ O
φ O
( O
x O
) O
( O
∇x O
logp O
( O
x O
, O
y O
) O
) O
where O
ξxy O
can O
be O
represented O
as O
ξxy O
= O
λxy⊗ψ O
( O
y O
) O
. O

section 15
id pdf2json/2021.acl-long.421.pdf.json
the O
empirical O
evaluation O
of O
jksd O
can O
be O
computed O
as O
, O
‖eqξxy‖2 O
= O
〈eqξxy O
, O
eqξxy〉 O
=eqeq′ O
〈 O
λxy O
⊗ O
ψ O
( O
y O
) O
, O
λx′y′ O
⊗ O
ψ O
( O
y′ O
) O
〉 O
=eqeq′ O
〈 O
λxy O
, O
λx′y′ O
〉 O
fd O
〈 O
ψ O
( O
y O
) O
, O
ψ O
( O
y′ O
) O
〉 O
g O
=eqeq′ O
〈 O
λxy O
, O
λx′y′ O
〉 O
fd O
l O
( O
y O
, O
y′ O
) O
where O
eq′ O
[ O
· O
] O
refers O
to O
e O
( O
x′ O
, O
y′ O
) O
∼q O
[ O
· O
] O
. O

section 15
id pdf2json/2021.acl-long.421.pdf.json
for O
f O
= O
( O
f1 O
, O
· O
· O
· O
, O
fd O
) O
∈ O
f O
d O
and O
g O
= O
( O
g1 O
, O
· O
· O
· O
, O
gd O
) O
∈ O
f O
d O
, O
the O
inner O
product O
between O
f O
and O
g O
is O
defined O
as O
〈f O
, O
g〉 O
= O
∑d O
i=1 O
〈fi O
, O
gi〉f O
. O

section 15
id pdf2json/2021.acl-long.421.pdf.json
based O
on O
this O
definition O
, O
the O
inner O
product O
〈∇xφ O
( O
x O
) O
, O
∇x′φ O
( O
x′ O
) O
〉fd O
can O
be O
evaluated O
as O
d∑ O
i=1 O
〈 O
∂φ O
( O
x O
) O
∂xi O
, O
∂φ O
( O
x′ O
) O
∂x′i O
〉 O
f O
= O
d∑ O
i=1 O
∂k O
( O
x O
, O
x′ O
) O
∂xi∂x′i O
similar O
to O
( O
chwialkowski O
et O
al. O
, O
2016 O
) O
, O
we O
can O
compute O
h O
( O
x O
, O
y O
, O
x′ O
, O
y′ O
) O
= O
〈 O
λxy O
, O
λx′y′ O
〉 O
fd O
as O
, O
∇x O
logp O
( O
x O
, O
y O
) O
> O
∇x′ O
logp O
( O
x′ O
, O
y′ O
) O
k O
( O
x O
, O
x′ O
) O
+∇x O
logp O
( O
x O
, O
y O
) O
> O
∇x′k O
( O
x O
, O
x′ O
) O
+∇x′ O
logp O
( O
x′ O
, O
y′ O
) O
> O
∇xk O
( O
x O
, O
x′ O
) O
+ O
〈 O
∇xφ O
( O
x O
) O
, O
∇x′φ O
( O
x′ O
) O
〉 O
fd O
thus O
, O
jksd O
s2 O
( O
p O
, O
q O
) O
is O
the O
expectation O
of O
h O
( O
x O
, O
y O
, O
x′ O
, O
y′ O
) O
l O
( O
y O
, O
y′ O
) O
over O
the O
distribution O
q O
, O
s2 O
( O
p O
, O
q O
) O
= O
eqeq′h O
( O
x O
, O
y O
, O
x′ O
, O
y′ O
) O
l O
( O
y O
, O
y′ O
) O
given O
a O
set O
of O
samples O
dt O
= O
{ O
( O
xi O
, O
yi O
) O
} O
mi=1 O
∼ O
q O
( O
x O
, O
y O
) O
, O
we O
can O
evaluate O
s2 O
( O
p O
, O
q O
) O
as O
1 O
m2 O
∑ O
x O
, O
y O
∑ O
x′ O
, O
y′ O
h O
( O
x O
, O
y O
, O
x′ O
, O
y′ O
) O
l O
( O
y O
, O
y′ O
) O
which O
can O
be O
represented O
in O
the O
matrix O
form O
as O
shown O
in O
eq O
. O

section 15
id pdf2json/2021.acl-long.421.pdf.json
5 O
. O

section TITLE
id pdf2json/2021.acl-long.212.pdf.json
assessing O
the O
representations O
of O
idiomaticity O
in O
vector O
models O
with O
a O
noun O
compound O
dataset O
labeled O
at O
type O
and O
token O
levels O

section ABSTRACT
id pdf2json/2021.acl-long.212.pdf.json
accurate O
assessment O
of O
the O
ability O
of O
embedding O
models O
to O
capture O
idiomaticity O
may O
require O
evaluation O
at O
token O
rather O
than O
type O
level O
, O
to O
account O
for O
degrees O
of O
idiomaticity O
and O
possible O
ambiguity O
between O
literal O
and O
idiomatic O
usages O
. O

section ABSTRACT
id pdf2json/2021.acl-long.212.pdf.json
however O
, O
most O
existing O
resources O
with O
annotation O
of O
idiomaticity O
include O
ratings O
only O
at O
type O
level O
. O

section ABSTRACT
id pdf2json/2021.acl-long.212.pdf.json
this O
paper O
presents O
the O
noun O
compound O
type O
and O
token O
idiomaticity O
( O
nctti O
) O
dataset O
, O
with O
human O
annotations O
for O
280 O
noun O
compounds O
in O
english O
and O
180 O
in O
portuguese O
at O
both O
type O
and O
token O
level O
. O

section ABSTRACT
id pdf2json/2021.acl-long.212.pdf.json
we O
compiled O
8,725 O
and O
5,091 O
token O
level O
annotations O
for O
english O
and O
portuguese O
, O
respectively O
, O
which O
are O
strongly O
correlated O
with O
the O
corresponding O
scores O
obtained O
at O
type O
level O
. O

section ABSTRACT
id pdf2json/2021.acl-long.212.pdf.json
the O
nctti O
dataset O
is O
used O
to O
explore O
how O
vector O
space O
models O
reflect O
the O
variability O
of O
idiomaticity O
across O
sentences O
. O

section ABSTRACT
id pdf2json/2021.acl-long.212.pdf.json
several O
experiments O
using O
state-of-the-art O
contextualised O
models O
suggest O
that O
their O
representations O
are O
not O
capturing O
the O
noun O
compounds O
idiomaticity O
as O
human O
annotators O
. O

section ABSTRACT
id pdf2json/2021.acl-long.212.pdf.json
this O
new O
multilingual O
resource O
also O
contains O
suggestions O
for O
paraphrases O
of O
the O
noun O
compounds O
both O
at O
type O
and O
token O
levels O
, O
with O
uses O
for O
lexical O
substitution O
or O
disambiguation O
in O
context O
. O

section 0
id pdf2json/2021.acl-long.212.pdf.json
proceedings O
of O
the O
59th O
annual O
meeting O
of O
the O
association O
for O
computational O
linguistics O
and O
the O
11th O
international O
joint O
conference O
on O
natural O
language O
processing O
, O
pages O
2730–2741 O
august O
1–6 O
, O
2021 O
. O

section 0
id pdf2json/2021.acl-long.212.pdf.json
©2021 O
association O
for O
computational O
linguistics O
2730 O

section 1
id pdf2json/2021.acl-long.212.pdf.json
multiword O
expressions O
( O
mwes O
) O
such O
as O
noun O
compounds O
( O
ncs O
) O
, O
have O
been O
considered O
a O
challenge O
for O
nlp O
( O
sag O
et O
al. O
, O
2002 O
) O
. O

section 1
id pdf2json/2021.acl-long.212.pdf.json
this O
is O
partly O
due O
to O
the O
wide O
range O
of O
idiomaticity O
that O
they O
display O
, O
from O
more O
literal O
to O
idiomatic O
combinations O
( O
olive O
oil O
vs. O
shrinking O
violet O
) O
. O

section 1
id pdf2json/2021.acl-long.212.pdf.json
the O
task O
of O
identifying O
the O
degree O
of O
idiomaticity O
of O
mwes O
has O
been O
investigated O
at O
type O
level O
, O
to O
determine O
the O
potential O
of O
an O
mwe O
to O
be O
idiomatic O
in O
general O
. O

section 1
id pdf2json/2021.acl-long.212.pdf.json
some O
of O
these O
approaches O
are O
based O
on O
the O
assumption O
that O
the O
* O
equal O
contribution O
. O

section 1
id pdf2json/2021.acl-long.212.pdf.json
distance O
between O
the O
representation O
of O
an O
mwe O
as O
a O
unit O
and O
the O
representation O
of O
the O
compositional O
combination O
of O
its O
components O
is O
an O
indication O
of O
the O
degree O
of O
idiomaticity O
: O
they O
are O
closer O
if O
the O
mwe O
is O
more O
compositional O
. O

section 1
id pdf2json/2021.acl-long.212.pdf.json
good O
performances O
are O
obtained O
even O
with O
non-contextualised O
word O
embeddings O
like O
word2vec O
( O
mikolov O
et O
al. O
, O
2013 O
) O
, O
and O
vector O
operations O
like O
addition O
and O
multiplication O
( O
mitchell O
and O
lapata O
, O
2010 O
; O
reddy O
et O
al. O
, O
2011 O
; O
cordeiro O
et O
al. O
, O
2019 O
) O
. O

section 1
id pdf2json/2021.acl-long.212.pdf.json
additionally O
, O
for O
some O
mwes O
, O
there O
is O
a O
potential O
ambiguity O
between O
an O
idiomatic O
and O
a O
literal O
sense O
, O
like O
in O
the O
potentially O
idiomatic O
mwe O
brass O
ring O
which O
can O
be O
ambiguous O
between O
the O
more O
literal O
meaning O
a O
ring O
made O
of O
brass O
and O
the O
more O
idiomatic O
sense O
of O
a O
prize O
. O

section 1
id pdf2json/2021.acl-long.212.pdf.json
considering O
that O
these O
mwes O
can O
have O
both O
idiomatic O
and O
literal O
senses O
, O
a O
related O
task O
of O
token-level O
identification O
evaluates O
whether O
in O
a O
particular O
context O
an O
mwe O
is O
idiomatic O
or O
not O
. O

section 1
id pdf2json/2021.acl-long.212.pdf.json
for O
this O
task O
, O
models O
that O
incorporate O
the O
context O
in O
which O
an O
mwe O
occurs O
tend O
to O
be O
better O
equipped O
to O
distinguish O
idiomatic O
from O
literal O
occurrences O
( O
sporleder O
and O
li O
, O
2009 O
; O
king O
and O
cook O
, O
2018 O
; O
salton O
et O
al. O
, O
2016 O
) O
. O

section 1
id pdf2json/2021.acl-long.212.pdf.json
contextualised O
embedding O
models O
, O
like O
bert O
( O
devlin O
et O
al. O
, O
2019 O
) O
, O
brought O
significant O
advances O
to O
a O
variety O
of O
downstream O
tasks O
( O
e.g O
. O

section 1
id pdf2json/2021.acl-long.212.pdf.json
zhu O
et O
al O
. O

section 1
id pdf2json/2021.acl-long.212.pdf.json
( O
2020 O
) O
for O
machine O
translation O
and O
jiang O
and O
de O
marneffe O
( O
2019 O
) O
for O
natural O
language O
inference O
) O
. O

section 1
id pdf2json/2021.acl-long.212.pdf.json
they O
also O
seem O
to O
benefit O
tasks O
like O
idiomaticity O
and O
metaphor O
identification O
( O
gao O
et O
al. O
, O
2018 O
) O
, O
since O
their O
interpretation O
is O
often O
dependent O
on O
contextual O
clues O
. O

section 1
id pdf2json/2021.acl-long.212.pdf.json
nonetheless O
, O
previous O
work O
found O
that O
non-contextualised O
models O
seem O
to O
still O
bring O
informative O
clues O
for O
these O
tasks O
( O
king O
and O
cook O
, O
2018 O
) O
, O
and O
their O
combination O
with O
contextualised O
models O
could O
improve O
results O
( O
e.g O
. O

section 1
id pdf2json/2021.acl-long.212.pdf.json
for O
metaphor O
identification O
( O
mao O
et O
al. O
, O
2019 O
) O
) O
. O

section 1
id pdf2json/2021.acl-long.212.pdf.json
this O
complementarity O
between O
non-contextualised O
and O
contex- O
tualised O
models O
may O
be O
an O
indication O
that O
enough O
core O
idiomatic O
information O
may O
already O
be O
available O
at O
type O
level O
. O

section 1
id pdf2json/2021.acl-long.212.pdf.json
moreover O
, O
type-based O
compositionality O
prediction O
measures O
that O
perform O
well O
with O
static O
embeddings O
may O
also O
perform O
well O
for O
token-based O
prediction O
with O
contextualised O
models O
. O

section 1
id pdf2json/2021.acl-long.212.pdf.json
to O
address O
these O
questions O
, O
in O
this O
paper O
, O
we O
present O
the O
noun O
compound O
type O
and O
token O
idiomaticity O
( O
nctti O
) O
dataset O
, O
containing O
280 O
ncs O
in O
english O
and O
180 O
in O
portuguese O
, O
annotated O
with O
the O
degree O
of O
idiomaticity O
perceived O
by O
human O
annotators O
, O
at O
type O
and O
token O
level.1 O
nctti O
contains O
a O
total O
of O
8,725 O
annotations O
in O
840 O
different O
sentences O
in O
english O
, O
and O
5,091 O
annotations O
in O
540 O
sentences O
in O
portuguese O
. O

section 1
id pdf2json/2021.acl-long.212.pdf.json
moreover O
, O
nctti O
has O
several O
paraphrases O
for O
each O
nc O
which O
are O
classified O
as O
either O
type O
level O
or O
token O
level O
equivalents O
. O

section 1
id pdf2json/2021.acl-long.212.pdf.json
to O
control O
for O
the O
level O
of O
idiomaticity O
, O
the O
nctti O
dataset O
has O
a O
balanced O
amount O
of O
compositional O
, O
partly O
compositional O
and O
idiomatic O
items O
. O

section 1
id pdf2json/2021.acl-long.212.pdf.json
as O
the O
importance O
of O
context O
to O
determine O
interpretation O
may O
be O
related O
to O
factors O
like O
the O
degree O
of O
idiomaticity O
, O
association O
strength O
or O
the O
frequency O
of O
an O
nc O
, O
we O
present O
an O
illustrative O
analysis O
of O
their O
impact O
for O
the O
performance O
of O
different O
models O
in O
capturing O
idiomaticity O
. O

section 1
id pdf2json/2021.acl-long.212.pdf.json
we O
also O
examine O
how O
the O
performance O
obtained O
for O
human O
idiomaticity O
judgments O
per O
type O
differs O
from O
the O
performance O
obtained O
per O
token O
. O

section 1
id pdf2json/2021.acl-long.212.pdf.json
our O
contributions O
can O
be O
summarised O
as O
: O
( O
1 O
) O
building O
the O
nctti O
dataset O
with O
information O
about O
type O
and O
token O
idiomaticity O
for O
ncs O
in O
two O
languages O
, O
( O
2 O
) O
evaluating O
to O
what O
extent O
models O
are O
able O
to O
detect O
idiomaticity O
at O
type O
and O
token O
level O
, O
analysing O
different O
levels O
of O
contextualisation O
and O
( O
3 O
) O
proposing O
two O
new O
measures O
of O
idiomaticity O
. O

section 1
id pdf2json/2021.acl-long.212.pdf.json
moreover O
, O
the O
paraphrases O
provided O
for O
each O
nc O
at O
type O
and O
token O
level O
make O
nctti O
a O
useful O
resource O
for O
enhancing O
paraphrase O
datasets O
( O
e.g O
. O

section 1
id pdf2json/2021.acl-long.212.pdf.json
ppdb O
( O
ganitkevitch O
et O
al. O
, O
2013 O
) O
) O
, O
for O
tasks O
involving O
lexical O
substitution O
( O
mccarthy O
and O
navigli O
, O
2007 O
; O
mihalcea O
et O
al. O
, O
2010 O
) O
, O
or O
for O
improving O
the O
results O
of O
downstream O
tasks O
, O
such O
as O
text O
simplification O
( O
paetzold O
, O
2016 O
; O
alva-manchego O
et O
al. O
, O
2020 O
) O
. O

section 1
id pdf2json/2021.acl-long.212.pdf.json
such O
paraphrases O
may O
also O
be O
useful O
for O
improving O
the O
task O
of O
machine O
translation O
, O
avoiding O
the O
need O
for O
parallel O
mwe O
corpora O
( O
zaninello O
and O
birch O
, O
2020 O
) O
. O

section 1
id pdf2json/2021.acl-long.212.pdf.json
section O
2 O
gives O
an O
overview O
of O
existing O
idiomaticity O
datasets O
. O

section 1
id pdf2json/2021.acl-long.212.pdf.json
section O
3 O
presents O
the O
nctti O
dataset O
and O
the O
annotations O
, O
and O
section O
4 O
discusses O
1type O
level O
annotations O
come O
from O
cordeiro O
et O
al O
. O

section 1
id pdf2json/2021.acl-long.212.pdf.json
( O
2019 O
) O
, O
the O
dataset O
used O
as O
source O
for O
the O
nctti O
. O

section 1
id pdf2json/2021.acl-long.212.pdf.json
the O
evaluation O
of O
the O
performance O
of O
different O
word O
embeddings O
in O
detecting O
idiomaticity O
. O

section 2
id pdf2json/2021.acl-long.212.pdf.json
datasets O
with O
type-level O
annotations O
are O
available O
for O
ncs O
in O
english O
( O
farahmand O
et O
al. O
, O
2015 O
; O
reddy O
et O
al. O
, O
2011 O
; O
ramisch O
et O
al. O
, O
2016 O
; O
kruszewski O
and O
baroni O
, O
2014 O
) O
, O
german O
( O
roller O
et O
al. O
, O
2013 O
; O
schulte O
im O
walde O
et O
al. O
, O
2016 O
) O
, O
french O
( O
cordeiro O
et O
al. O
, O
2019 O
) O
and O
portuguese O
( O
cordeiro O
et O
al. O
, O
2019 O
) O
. O

section 2
id pdf2json/2021.acl-long.212.pdf.json
however O
, O
datasets O
with O
idiomatic O
information O
at O
token O
level O
are O
scarce O
, O
e.g. O
, O
the O
vnc-tokens O
( O
cook O
et O
al. O
, O
2008 O
) O
, O
containing O
almost O
3k O
annotations O
for O
53 O
verb-noun O
combinations O
in O
english O
. O

section 2
id pdf2json/2021.acl-long.212.pdf.json
regarding O
the O
use O
of O
contextualised O
embeddings O
to O
model O
idiomaticity O
, O
nandakumar O
et O
al O
. O

section 2
id pdf2json/2021.acl-long.212.pdf.json
( O
2019 O
) O
compared O
different O
static O
and O
contextualised O
embeddings O
to O
predict O
the O
ncs O
compositionality O
, O
obtaining O
better O
results O
with O
static O
vectors O
learnt O
individually O
for O
each O
nc O
. O

section 2
id pdf2json/2021.acl-long.212.pdf.json
shwartz O
and O
dagan O
( O
2019 O
) O
train O
various O
classifiers O
initialised O
with O
static O
and O
contextualised O
embeddings O
for O
different O
compositional O
tasks O
, O
achieving O
the O
best O
results O
with O
bert O
embeddings O
. O

section 2
id pdf2json/2021.acl-long.212.pdf.json
yu O
and O
ettinger O
( O
2020 O
) O
, O
using O
partially O
idiomatic O
expressions O
of O
the O
bird O
dataset O
( O
asaadi O
et O
al. O
, O
2019 O
) O
, O
show O
that O
contextualised O
embeddings O
from O
language O
models O
heavily O
rely O
on O
word O
content O
, O
missing O
additional O
information O
provided O
by O
compositional O
operations O
. O

section 2
id pdf2json/2021.acl-long.212.pdf.json
in O
this O
paper O
we O
take O
advantage O
of O
the O
nctti O
dataset O
to O
observe O
whether O
vector O
representations O
obtained O
with O
different O
strategies O
correlate O
with O
human O
annotations O
at O
both O
type O
and O
token O
levels O
. O

section 3
id pdf2json/2021.acl-long.212.pdf.json
this O
section O
describes O
the O
procedure O
to O
create O
the O
nctti O
dataset O
and O
its O
main O
characteristics.2 O

section 4
id pdf2json/2021.acl-long.212.pdf.json
we O
used O
as O
basis O
the O
english O
and O
portuguese O
subsets O
of O
the O
nc O
compositionality O
dataset O
( O
cordeiro O
et O
al. O
, O
2019 O
) O
, O
which O
contain O
compositionality O
scores O
for O
280 O
two-word O
ncs O
in O
english O
( O
90 O
of O
which O
came O
from O
reddy O
et O
al O
. O

section 4
id pdf2json/2021.acl-long.212.pdf.json
( O
2011 O
) O
) O
, O
and O
180 O
in O
portuguese O
, O
all O
of O
them O
labeled O
at O
type O
level O
: O
i.e. O
, O
the O
annotators O
provided O
a O
compositionality O
value O
for O
a O
compound O
( O
from O
0 O
–fully O
idiomatic– O
to O
5 O
, O
fully O
2the O
nccti O
dataset O
can O
be O
downloaded O
from O
the O
following O
url O
: O
https O
: O
//github.com/marcospln/nctti O
. O

section 4
id pdf2json/2021.acl-long.212.pdf.json
compositional O
) O
after O
reading O
various O
sentences O
with O
this O
nc O
. O

section 4
id pdf2json/2021.acl-long.212.pdf.json
to O
obtain O
more O
fine-grained O
compatible O
tokenlevel O
annotations O
about O
the O
impact O
of O
different O
contexts O
in O
the O
interpretation O
of O
ncs O
, O
we O
used O
the O
same O
original O
sentences O
as O
in O
the O
source O
dataset O
( O
three O
sentences O
per O
compound O
with O
the O
same O
sense O
were O
selected O
from O
reddy O
et O
al O
. O

section 4
id pdf2json/2021.acl-long.212.pdf.json
( O
2011 O
) O
dataset O
) O
.3 O
language O
experts O
classified O
each O
noun O
compound O
regarding O
their O
semantic O
compositionality O
as O
idiomatic O
( O
e.g. O
, O
gravy O
train O
) O
, O
partially O
idiomatic O
( O
e.g. O
, O
grandfather O
clock O
) O
, O
or O
compositional O
( O
e.g. O
, O
research O
project O
) O
. O

section 4
id pdf2json/2021.acl-long.212.pdf.json
for O
english O
, O
this O
resulted O
in O
103 O
, O
88 O
, O
and O
89 O
idiomatic O
, O
partially O
idiomatic O
, O
and O
compositional O
compounds O
. O

section 4
id pdf2json/2021.acl-long.212.pdf.json
for O
portuguese O
, O
each O
class O
has O
60 O
compounds O
, O
as O
the O
selection O
had O
been O
balanced O
when O
the O
source O
dataset O
was O
created O
. O

section 5
id pdf2json/2021.acl-long.212.pdf.json
we O
used O
the O
same O
protocol O
as O
reddy O
et O
al O
. O

section 5
id pdf2json/2021.acl-long.212.pdf.json
( O
2011 O
) O
and O
cordeiro O
et O
al O
. O

section 5
id pdf2json/2021.acl-long.212.pdf.json
( O
2019 O
) O
, O
asking O
each O
participant O
to O
give O
0 O
to O
5 O
scores O
for O
an O
nc O
and O
its O
components O
in O
a O
specific O
sentence O
( O
e.g. O
, O
glass O
ceiling O
in O
“ O
women O
are O
continuing O
to O
slowly O
break O
through O
the O
glass O
ceiling O
of O
uk O
business O
[ O
. O

section 5
id pdf2json/2021.acl-long.212.pdf.json
. O

section 5
id pdf2json/2021.acl-long.212.pdf.json
. O

section 5
id pdf2json/2021.acl-long.212.pdf.json
] O
” O
) O
. O

section 5
id pdf2json/2021.acl-long.212.pdf.json
in O
particular O
, O
we O
asked O
participants O
for O
: O
( O
i O
) O
the O
contribution O
of O
the O
head O
to O
the O
meaning O
of O
the O
nc O
( O
e.g. O
, O
is O
a O
glass O
ceiling O
literally O
a O
ceiling O
? O

section 5
id pdf2json/2021.acl-long.212.pdf.json
) O
; O
( O
ii O
) O
the O
contribution O
of O
the O
modifier O
to O
the O
meaning O
of O
the O
nc O
( O
e.g. O
, O
is O
a O
glass O
ceiling O
literally O
of O
glass O
? O

section 5
id pdf2json/2021.acl-long.212.pdf.json
) O
; O
and O
( O
iii O
) O
the O
degree O
of O
compositionality O
of O
the O
compound O
( O
i.e. O
, O
to O
what O
extent O
the O
meaning O
of O
the O
nc O
can O
be O
seen O
as O
a O
combination O
of O
its O
parts O
) O
. O

section 5
id pdf2json/2021.acl-long.212.pdf.json
additionally O
, O
we O
asked O
for O
up O
to O
three O
synonyms O
of O
the O
nc O
in O
that O
particular O
sentence O
( O
e.g. O
, O
synonyms O
at O
token O
level O
) O
. O

section 5
id pdf2json/2021.acl-long.212.pdf.json
we O
used O
amazon O
mechanical O
turk O
to O
obtain O
the O
annotations O
for O
english O
, O
and O
a O
dedicated O
online O
platform O
for O
the O
questionnaire O
in O
portuguese,4 O
as O
we O
could O
not O
find O
a O
suitable O
number O
of O
annotators O
for O
this O
language O
in O
amt.5 O
taking O
this O
into O
account O
, O
the O
numbers O
of O
the O
portuguese O
annotations O
are O
in O
general O
lower O
to O
those O
obtained O
for O
english O
. O

section 5
id pdf2json/2021.acl-long.212.pdf.json
for O
each O
language O
, O
we O
have O
included O
the O
three O
sentences O
of O
every O
compound O
in O
the O
dataset O
( O
840 O
sentences O
in O
english O
, O
and O
540 O
in O
portuguese O
) O
, O
which O
were O
randomly O
submitted O
to O
the O
annotators O
. O

section 5
id pdf2json/2021.acl-long.212.pdf.json
3some O
contexts O
are O
spans O
of O
tokens O
instead O
of O
sentences O
, O
but O
usually O
enough O
to O
interpret O
the O
meaning O
of O
the O
nc O
. O

section 5
id pdf2json/2021.acl-long.212.pdf.json
4the O
platform O
was O
provided O
by O
cordeiro O
et O
al O
. O

section 5
id pdf2json/2021.acl-long.212.pdf.json
( O
2019 O
) O
. O

section 5
id pdf2json/2021.acl-long.212.pdf.json
5the O
annotation O
process O
was O
approved O
by O
the O
ethics O
committee O
of O
the O
university O
of O
sheffield O
. O

section 5
id pdf2json/2021.acl-long.212.pdf.json
this O
is O
a O
thorough O
evaluation O
process O
peer-reviewed O
by O
three O
ethical O
reviewers O
. O

section 5
id pdf2json/2021.acl-long.212.pdf.json
the O
monetary O
compensation O
was O
deemed O
appropriate O
for O
the O
task O
. O

section 5
id pdf2json/2021.acl-long.212.pdf.json
for O
english O
, O
we O
compiled O
at O
least O
10 O
annotations O
per O
sentence O
, O
resulting O
in O
8,725 O
annotations O
( O
10.4 O
annotations O
per O
sentence O
on O
average O
) O
. O

section 5
id pdf2json/2021.acl-long.212.pdf.json
a O
total O
of O
412 O
annotators O
have O
taken O
part O
in O
the O
process O
, O
and O
on O
average O
, O
each O
participant O
labeled O
21 O
instances O
. O

section 5
id pdf2json/2021.acl-long.212.pdf.json
for O
portuguese O
we O
set O
the O
threshold O
in O
5 O
annotations O
per O
sentence O
: O
we O
got O
5,091 O
annotations O
by O
33 O
participants O
, O
so O
that O
each O
sentence O
has O
a O
mean O
of O
9.4 O
annotations O
and O
each O
annotator O
labeled O
on O
average O
154 O
sentences O
. O

section 6
id pdf2json/2021.acl-long.212.pdf.json
inter-annotator O
agreement O
: O
we O
computed O
the O
inter-annotator O
agreements O
for O
two O
and O
three O
annotators O
with O
the O
largest O
number O
of O
sentences O
in O
common O
( O
table O
1 O
) O
. O

section 6
id pdf2json/2021.acl-long.212.pdf.json
for O
english O
, O
we O
obtained O
krippendorff O
’ O
s O
α O
( O
krippendorff O
, O
2011 O
) O
values O
of O
0.30 O
for O
two O
annotators O
( O
199 O
sentences O
) O
and O
0.22 O
for O
three O
annotators O
( O
76 O
sentences O
) O
. O

section 6
id pdf2json/2021.acl-long.212.pdf.json
the O
α O
values O
for O
portuguese O
were O
of O
0.52 O
for O
two O
annotators O
( O
131 O
sentences O
) O
and O
0.44 O
for O
three O
annotators O
( O
60 O
sentences O
) O
. O

section 6
id pdf2json/2021.acl-long.212.pdf.json
overall O
, O
and O
using O
the O
divisions O
proposed O
by O
landis O
and O
koch O
( O
1977 O
) O
, O
the O
agreement O
results O
can O
be O
classified O
as O
‘ O
fair O
’ O
( O
for O
english O
) O
, O
and O
‘ O
moderate O
’ O
( O
for O
portuguese O
) O
. O

section 6
id pdf2json/2021.acl-long.212.pdf.json
correlation O
token O
vs. O
type O
scores O
: O
then O
, O
we O
calculated O
the O
correlations O
( O
spearman O
ρ O
) O
between O
the O
average O
compositionality O
scores O
of O
the O
nctti O
dataset O
and O
those O
of O
the O
original O
resource O
( O
nc O
compositionality O
dataset O
) O
. O

section 6
id pdf2json/2021.acl-long.212.pdf.json
table O
2 O
contains O
the O
correlation O
results O
for O
each O
language O
and O
compositionality O
class O
. O

section 6
id pdf2json/2021.acl-long.212.pdf.json
the O
strong O
to O
very O
strong O
significant O
correlations O
confirm O
the O
robustness O
between O
type-level O
and O
token-level O
human O
compositionality O
annotations O
for O
these O
two O
datasets.6 O
idiomaticity O
values O
: O
with O
regards O
to O
the O
idiomaticity O
values O
of O
each O
class O
, O
table O
3 O
displays O
both O
the O
average O
scores O
and O
the O
standard O
deviation O
in O
both O
languages O
. O

section 6
id pdf2json/2021.acl-long.212.pdf.json
as O
expected O
, O
for O
the O
whole O
compounds O
, O
partially O
idiomatic O
ncs O
are O
those O
with O
higher O
standard O
deviations O
, O
and O
their O
mean O
compositionality O
values O
are O
in O
the O
middle O
of O
the O
scale O
( O
2.34 O
and O
2.46 O
) O
. O

section 6
id pdf2json/2021.acl-long.212.pdf.json
in O
english O
, O
the O
results O
of O
both O
idiomatic O
and O
compositional O
compounds O
are O
more O
homogeneous O
, O
as O
they O
are O
clearly O
located O
on O
the O
margins O
of O
the O
scale O
( O
< O
1 O
and O
> O
4 O
, O
respectively O
) O
with O
lower O
deviations O
. O

section 6
id pdf2json/2021.acl-long.212.pdf.json
this O
is O
not O
the O
case O
in O
portuguese O
, O
where O
the O
average O
values O
are O
> O
1 O
and O
< O
4 O
for O
idiomatic O
and O
compositional O
ncs O
, O
respectively O
, O
placing O
even O
the O
idiomatic O
cases O
closer O
towards O
the O
middle O
of O
the O
scale O
. O

section 6
id pdf2json/2021.acl-long.212.pdf.json
with O
respect O
to O
the O
average O
values O
for O
the O
heads O
and O
modifiers O
, O
we O
can O
highlight O
the O
following O
observations O
: O
first O
, O
both O
head O
and O
modifier O
scores O
are O
consistently O
higher O
than O
the O
means O
for O
the O
whole O
compound O
in O
every O
scenario O
also O
suggesting O
at O
least O
a O
partial O
compositionality O
in O
their O
token O
occurrences O
. O

section 6
id pdf2json/2021.acl-long.212.pdf.json
second O
, O
for O
idiomatic O
ncs O
, O
the O
scores O
of O
the O
modifiers O
are O
higher O
than O
those O
of O
the O
heads O
, O
while O
for O
partially O
compositional O
ncs O
the O
results O
are O
the O
opposite.7 O
finally O
, O
regarding O
the O
compositional O
level O
, O
the O
modifier O
values O
are O
higher O
in O
english O
, O
while O
in O
portuguese O
the O
heads O
seem O
to O
contribute O
more O
to O
the O
meaning O
of O
the O
nc O
. O

section 6
id pdf2json/2021.acl-long.212.pdf.json
6removing O
annotators O
with O
low O
agreement O
( O
spearman O
ρ O
< O
0.2 O
, O
and O
ρ O
< O
0.4 O
) O
resulted O
in O
almost O
identical O
correlations O
. O

section 6
id pdf2json/2021.acl-long.212.pdf.json
7the O
results O
for O
partially O
idiomatic O
compounds O
are O
expected O
to O
some O
extent O
as O
the O
head O
tends O
to O
bear O
more O
semantic O
load O
about O
the O
whole O
expression O
( O
e.g. O
, O
as O
in O
collocations O
) O
. O

section 6
id pdf2json/2021.acl-long.212.pdf.json
observing O
the O
variability O
across O
the O
annotations O
, O
we O
found O
some O
divergence O
in O
a O
few O
compounds O
( O
e.g. O
, O
brass O
ring O
labeled O
as O
idiomatic O
for O
a O
compositional O
occurrence O
“ O
three O
drawers O
, O
each O
with O
a O
brass O
ring O
pull O
, O
provide O
plenty O
of O
storage O
whatever O
you O
use O
it O
for. O
” O
) O
, O
which O
hints O
at O
possible O
interference O
from O
a O
salient O
meaning O
( O
giora O
, O
1999 O
) O
. O

section 6
id pdf2json/2021.acl-long.212.pdf.json
however O
, O
further O
investigation O
is O
needed O
. O

section 6
id pdf2json/2021.acl-long.212.pdf.json
paraphrases O
: O
as O
mentioned O
, O
we O
asked O
the O
participants O
to O
provide O
synonyms O
or O
paraphrases O
for O
the O
noun O
compounds O
in O
each O
particular O
context O
. O

section 6
id pdf2json/2021.acl-long.212.pdf.json
in O
this O
respect O
, O
it O
is O
worth O
noting O
that O
while O
some O
suggestions O
may O
be O
applicable O
across O
all O
the O
sentences O
for O
an O
nc O
( O
e.g O
. O

section 6
id pdf2json/2021.acl-long.212.pdf.json
spun O
sugar O
for O
cotton O
candy O
, O
considered O
as O
a O
type O
level O
synonym O
) O
, O
others O
are O
more O
dependent O
on O
context O
and O
differ O
for O
specific O
sentences O
( O
e.g O
. O

section 6
id pdf2json/2021.acl-long.212.pdf.json
flight O
recorder O
and O
unknown O
process O
, O
for O
black O
box O
, O
which O
can O
be O
considered O
as O
token O
level O
paraphrases O
) O
. O

section 6
id pdf2json/2021.acl-long.212.pdf.json
we O
have O
classified O
the O
paraphrases O
as O
type O
or O
token O
level O
using O
the O
following O
procedure O
: O
to O
organise O
the O
large O
set O
of O
paraphrases O
provided O
by O
the O
annotators O
( O
see O
below O
) O
, O
we O
performed O
an O
automatic O
classification O
as O
follows O
: O
we O
labeled O
as O
type O
level O
synonyms O
those O
paraphrases O
proposed O
for O
the O
three O
sentences O
of O
each O
compound O
, O
and O
those O
suggested O
for O
two O
sentences O
with O
a O
frequency O
> O
= O
3 O
; O
token O
level O
synonyms O
are O
those O
proposed O
only O
for O
one O
sentence O
with O
a O
frequency O
> O
= O
2 O
. O

section 6
id pdf2json/2021.acl-long.212.pdf.json
in O
english O
, O
9,690 O
different O
paraphrases O
were O
proposed O
by O
the O
annotators O
( O
average O
34.60 O
per O
nc O
) O
, O
and O
3,554 O
were O
suggested O
by O
at O
least O
5 O
participants O
( O
average O
of O
12.70 O
per O
nc O
) O
. O

section 6
id pdf2json/2021.acl-long.212.pdf.json
out O
of O
them O
, O
1,506 O
were O
classified O
as O
type O
level O
( O
5.4 O
synonyms O
per O
nc O
, O
on O
average O
) O
, O
and O
353 O
at O
token O
level O
( O
0.42 O
per O
sentence O
, O
1.3 O
per O
nc O
) O
. O

section 6
id pdf2json/2021.acl-long.212.pdf.json
overall O
, O
118 O
ncs O
have O
token O
level O
synonyms O
for O
one O
sentence O
, O
69 O
for O
two O
sentences O
, O
and O
16 O
for O
the O
three O
sentences O
. O

section 6
id pdf2json/2021.acl-long.212.pdf.json
for O
portuguese O
, O
the O
annotators O
suggested O
a O
total O
of O
6,579 O
paraphrases O
( O
314 O
by O
at O
least O
5 O
participants O
and O
764 O
by O
> O
= O
3 O
, O
average O
of O
4.2 O
per O
nc O
) O
. O

section 6
id pdf2json/2021.acl-long.212.pdf.json
743 O
synonyms O
were O
proposed O
for O
the O
180 O
compounds O
( O
an O
average O
of O
4.1 O
per O
nc O
) O
, O
being O
classified O
as O
type O
level O
. O

section 6
id pdf2json/2021.acl-long.212.pdf.json
concerning O
token O
level O
synonyms O
, O
we O
have O
collected O
192 O
synonyms O
( O
1.1 O
per O
nc O
, O
on O
average O
) O
. O

section 6
id pdf2json/2021.acl-long.212.pdf.json
in O
this O
case O
the O
total O
number O
of O
annotations O
was O
lower O
, O
and O
the O
final O
resource O
contains O
61 O
ncs O
with O
token O
level O
synonyms O
for O
one O
sentence O
, O
38 O
for O
two O
sentences O
, O
and O
6 O
compounds O
have O
token O
level O
synonyms O
for O
the O
three O
sentences O
. O

section 6
id pdf2json/2021.acl-long.212.pdf.json
the O
collection O
of O
paraphrases O
included O
in O
the O
nctti O
make O
this O
dataset O
a O
valuable O
resource O
for O
different O
evaluations O
, O
such O
as O
lexical O
substitution O
tasks O
and O
assessments O
of O
the O
performance O
of O
embedding O
models O
to O
correctly O
identify O
contextualised O
synonyms O
of O
ncs O
with O
different O
degrees O
of O
idiomaticity O
. O

section 6
id pdf2json/2021.acl-long.212.pdf.json
table O
4 O
shows O
an O
annotation O
example O
for O
the O
nc O
disc O
jockey O
, O
in O
english O
. O

section 6
id pdf2json/2021.acl-long.212.pdf.json
it O
includes O
the O
three O
sentences O
together O
with O
the O
average O
idiomaticity O
score O
and O
both O
token-level O
and O
type-level O
paraphrases O
. O

section 7
id pdf2json/2021.acl-long.212.pdf.json
this O
section O
displays O
some O
of O
the O
comparative O
analyses O
for O
the O
relevance O
of O
type O
and O
token O
annotation O
for O
idiomaticity O
detection O
. O

section 7
id pdf2json/2021.acl-long.212.pdf.json
first O
, O
we O
adapt O
the O
type O
level O
compositionality O
prediction O
approaches O
used O
on O
static O
word O
vectors O
( O
mitchell O
and O
lapata O
, O
2010 O
) O
to O
contextualised O
models O
( O
nandakumar O
et O
al. O
, O
2019 O
) O
, O
here O
computing O
the O
correlation O
also O
at O
token O
level O
. O

section 7
id pdf2json/2021.acl-long.212.pdf.json
in O
particular O
, O
the O
assumption O
is O
that O
compositionality O
can O
be O
approximated O
as O
the O
distance O
between O
the O
representation O
for O
an O
nc O
and O
the O
representation O
for O
the O
compositional O
combination O
of O
its O
individual O
components O
. O

section 7
id pdf2json/2021.acl-long.212.pdf.json
then O
, O
we O
measure O
whether O
the O
vector O
representations O
reflect O
the O
variability O
of O
the O
human O
annotators O
, O
who O
capture O
different O
nuances O
of O
the O
ncs O
depending O
on O
the O
sentences O
in O
which O
they O
occur O
. O

section 7
id pdf2json/2021.acl-long.212.pdf.json
similarly O
, O
in O
a O
third O
experiment O
we O
use O
the O
standard O
deviations O
of O
the O
idiomaticity O
scores O
in O
the O
three O
contexts O
to O
observe O
how O
the O
interpretation O
of O
the O
ncs O
varies O
across O
sentences O
, O
and O
whether O
this O
correlates O
with O
the O
contextualised O
representations O
produced O
by O
various O
models O
. O

section 7
id pdf2json/2021.acl-long.212.pdf.json
more O
specifically O
, O
we O
assume O
that O
, O
if O
models O
adequately O
incorporate O
contextual O
information O
, O
the O
standard O
deviations O
of O
the O
similarities O
between O
the O
ncs O
in O
different O
contexts O
should O
be O
correlated O
with O
those O
of O
the O
human O
annotators O
. O

section 8
id pdf2json/2021.acl-long.212.pdf.json
we O
evaluate O
four O
contextualised O
models O
: O
three O
bert O
variants O
, O
based O
on O
the O
transformers O
architecture O
( O
vaswani O
et O
al. O
, O
2017 O
) O
, O
and O
elmo O
, O
which O
learns O
word O
vectors O
using O
bidirectional O
lstms O
( O
peters O
et O
al. O
, O
2018 O
) O
. O

section 8
id pdf2json/2021.acl-long.212.pdf.json
for O
english O
we O
used O
the O
elmo O
small O
model O
provided O
by O
peters O
et O
al O
. O

section 8
id pdf2json/2021.acl-long.212.pdf.json
( O
2018 O
) O
, O
bert-large O
uncased O
( O
devlin O
et O
al. O
, O
2019 O
) O
, O
distilbert O
( O
sanh O
et O
al. O
, O
2019 O
) O
, O
based O
on O
bert-base O
and O
distilled O
on O
squad O
dataset O
, O
and O
sentencebert O
( O
reimers O
and O
gurevych O
, O
2019 O
) O
, O
trained O
on O
bert-large O
and O
both O
multinli O
and O
snli.8 O
for O
portuguese O
we O
selected O
the O
elmo O
pre-trained O
weights O
provided O
by O
quinta O
de O
castro O
et O
al O
. O

section 8
id pdf2json/2021.acl-long.212.pdf.json
( O
2018 O
) O
and O
the O
multilingual O
versions O
of O
the O
models O
used O
for O
english O
, O
namely O
mbert O
( O
base O
cased O
) O
, O
and O
both O
multilingual O
distilbert O
and O
sentence-bert O
( O
reimers O
and O
gurevych O
, O
2020 O
) O
. O

section 8
id pdf2json/2021.acl-long.212.pdf.json
as O
a O
static O
noncontextualised O
baseline O
we O
used O
glove O
( O
pennington O
et O
al. O
, O
2014 O
) O
( O
the O
english O
official O
models O
with O
300 O
dimensions O
and O
trained O
on O
840 O
billion O
tokens O
, O
and O
the O
equivalent O
portuguese O
model O
released O
by O
hartmann O
et O
al O
. O

section 8
id pdf2json/2021.acl-long.212.pdf.json
( O
2017 O
) O
) O
. O

section 8
id pdf2json/2021.acl-long.212.pdf.json
the O
vector O
representations O
were O
obtained O
with O
the O
flairnlp O
framework O
( O
akbik O
et O
al. O
, O
2019 O
) O
using O
the O
models O
provided O
by O
the O
transformers O
library O
( O
wolf O
et O
al. O
, O
2020 O
) O
. O

section 8
id pdf2json/2021.acl-long.212.pdf.json
the O
representations O
of O
ncs O
( O
and O
their O
sentences O
) O
were O
obtained O
by O
averaging O
the O
word O
( O
or O
subword O
, O
if O
adopted O
by O
the O
model O
) O
embeddings O
. O

section 8
id pdf2json/2021.acl-long.212.pdf.json
we O
used O
the O
concatenation O
of O
the O
three O
layers O
for O
elmo O
and O
of O
8https O
: O
//www.nyu.edu/projects/bowman/ O
multinli/ O
https O
: O
//nlp.stanford.edu/projects/snli/ O
the O
last O
four O
hidden O
layers O
for O
the O
bert O
models O
. O

section 8
id pdf2json/2021.acl-long.212.pdf.json
in O
glove O
, O
words O
which O
are O
not O
in O
the O
vocabulary O
were O
skipped O
. O

section 9
id pdf2json/2021.acl-long.212.pdf.json
unsupervised O
type O
idiomaticity O
identification O
with O
static O
non-contextualised O
word O
embeddings O
often O
assumes O
that O
the O
similarity O
between O
the O
nc O
embedding O
and O
the O
compositional O
embedding O
of O
the O
component O
words O
( O
e.g O
. O

section 9
id pdf2json/2021.acl-long.212.pdf.json
police O
car O
vs. O
police O
and O
car O
) O
is O
an O
indication O
of O
idiomaticity O
( O
mitchell O
and O
lapata O
, O
2010 O
) O
: O
the O
more O
similar O
they O
are O
the O
more O
compositional O
the O
nc O
is O
. O

section 9
id pdf2json/2021.acl-long.212.pdf.json
to O
approximate O
this O
with O
contextualised O
models O
, O
we O
calculate O
the O
cosine O
similarities O
between O
the O
contextualised O
vector O
of O
the O
nc O
in O
each O
sentence O
with O
two O
types O
of O
noncontextualised O
vectors O
. O

section 9
id pdf2json/2021.acl-long.212.pdf.json
the O
first O
evaluates O
if O
even O
in O
the O
absence O
of O
an O
informative O
sentence O
context O
, O
each O
of O
the O
component O
words O
would O
be O
enough O
of O
a O
trigger O
to O
cue O
the O
nc O
meaning O
( O
e.g O
. O

section 9
id pdf2json/2021.acl-long.212.pdf.json
eager O
for O
eager O
beaver O
) O
. O

section 9
id pdf2json/2021.acl-long.212.pdf.json
this O
is O
implemented O
as O
the O
vector O
for O
the O
nc O
out O
of O
context O
, O
obtained O
by O
feeding O
the O
model O
only O
with O
the O
compound O
, O
dubbed O
nc O
out.9 O
the O
second O
non-contextualised O
vector O
evaluates O
if O
the O
representations O
for O
the O
individual O
words O
have O
enough O
information O
to O
reconstruct O
the O
meaning O
of O
the O
nc O
in O
the O
absence O
of O
context O
and O
of O
the O
collocated O
component O
. O

section 9
id pdf2json/2021.acl-long.212.pdf.json
it O
is O
implemented O
as O
the O
sum O
of O
the O
individual O
vectors O
of O
the O
nc O
components O
, O
where O
each O
nc O
component O
is O
fed O
individually O
to O
the O
model O
as O
a O
sentence O
, O
referred O
to O
as O
nc O
outcomp O
. O

section 9
id pdf2json/2021.acl-long.212.pdf.json
on O
each O
case O
, O
we O
calculate O
two O
spearman O
correlations O
with O
human O
judgments O
: O
at O
token O
level O
, O
using O
all O
the O
sentences O
for O
each O
language O
; O
and O
at O
type O
level O
, O
comparing O
the O
average O
cosine O
similarities O
of O
each O
nc O
with O
their O
compositionality O
scores O
at O
type O
level O
. O

section 9
id pdf2json/2021.acl-long.212.pdf.json
we O
also O
compute O
correlations O
between O
the O
similarities O
and O
frequency-based O
data O
, O
namely O
the O
nc O
raw O
frequency O
, O
and O
the O
ppmi O
( O
church O
and O
hanks O
, O
1990 O
) O
between O
its O
component O
words O
, O
to O
verify O
whether O
they O
have O
any O
impact O
in O
these O
measures O
of O
idiomaticity O
. O

section 9
id pdf2json/2021.acl-long.212.pdf.json
the O
frequency O
data O
were O
obtained O
from O
ukwac O
, O
with O
2.25b O
tokens O
in O
english O
( O
baroni O
et O
al. O
, O
2009 O
) O
, O
and O
brwac O
, O
containing O
2.7b O
tokens O
in O
portuguese O
( O
wagner O
filho O
et O
al. O
, O
2018 O
) O
. O

section 9
id pdf2json/2021.acl-long.212.pdf.json
the O
results O
by O
cordeiro O
et O
al O
. O

section 9
id pdf2json/2021.acl-long.212.pdf.json
( O
2019 O
) O
suggested O
that O
if O
the O
two O
components O
of O
an O
nc O
are O
processed O
as O
a O
single O
token O
unit O
( O
for O
instance O
, O
by O
explic- O
9this O
representation O
equivalent O
to O
the O
avg O
phrase O
used O
by O
yu O
and O
ettinger O
( O
2020 O
) O
. O

section 9
id pdf2json/2021.acl-long.212.pdf.json
itly O
linking O
them O
with O
an O
underscore O
) O
the O
resulting O
static O
representation O
captures O
the O
nc O
idiomatic O
meaning O
. O

section 9
id pdf2json/2021.acl-long.212.pdf.json
this O
is O
not O
surprising O
since O
by O
linking O
the O
two O
components O
we O
create O
a O
new O
word O
that O
would O
be O
treated O
by O
the O
model O
as O
completely O
independent O
of O
the O
preexisting O
component O
words O
. O

section 9
id pdf2json/2021.acl-long.212.pdf.json
but O
such O
preprocessing O
may O
not O
be O
desirable O
or O
even O
feasible O
. O

section 9
id pdf2json/2021.acl-long.212.pdf.json
in O
this O
sense O
the O
contextualised O
models O
would O
be O
a O
good O
promise O
, O
since O
we O
expected O
that O
by O
processing O
a O
sentence O
with O
an O
idiomatic O
nc O
, O
the O
context O
would O
be O
enough O
to O
lead O
the O
model O
into O
linking O
the O
component O
words O
and O
assigning O
the O
corresponding O
idiomatic O
meaning O
. O

section 9
id pdf2json/2021.acl-long.212.pdf.json
figuratively O
speaking O
, O
the O
contextualised O
models O
would O
put O
the O
underscore O
for O
us O
. O

section 9
id pdf2json/2021.acl-long.212.pdf.json
therefore O
, O
if O
contextualised O
models O
capture O
idiomaticity O
, O
the O
similarity O
between O
nc O
and O
nc O
outcomp O
( O
or O
nc O
out O
) O
should O
have O
strong O
correlations O
with O
the O
idiomaticity O
scores O
of O
the O
ncs O
. O

section 9
id pdf2json/2021.acl-long.212.pdf.json
table O
5 O
shows O
the O
significant O
correlations O
in O
english O
( O
top O
rows O
) O
and O
portuguese O
( O
bottom O
) O
. O

section 9
id pdf2json/2021.acl-long.212.pdf.json
these O
results O
indicate O
at O
best O
weak O
( O
nc O
outcomp O
) O
to O
moderate O
( O
nc O
out O
) O
correlations O
between O
models O
’ O
predictions O
and O
human O
judgments O
, O
both O
at O
type O
and O
token O
levels O
. O

section 9
id pdf2json/2021.acl-long.212.pdf.json
moreover O
, O
the O
correlations O
obtained O
are O
much O
smaller O
than O
those O
found O
by O
the O
static O
models O
used O
by O
cordeiro O
et O
al O
. O

section 9
id pdf2json/2021.acl-long.212.pdf.json
( O
2019 O
) O
. O

section 9
id pdf2json/2021.acl-long.212.pdf.json
for O
english O
, O
the O
best O
correlations O
( O
0.37 O
) O
were O
obtained O
by O
bert O
, O
while O
elmo O
and O
sentence-bert O
achieved O
the O
best O
performance O
in O
portuguese O
( O
0.27 O
and O
0.26 O
, O
respectively O
) O
. O

section 9
id pdf2json/2021.acl-long.212.pdf.json
in O
both O
languages O
, O
the O
lower O
values O
were O
those O
of O
distilbert O
. O

section 9
id pdf2json/2021.acl-long.212.pdf.json
it O
is O
worth O
noting O
that O
a O
direct O
comparison O
between O
the O
bert O
models O
in O
both O
languages O
should O
not O
be O
done O
, O
as O
they O
are O
monolingual O
( O
for O
english O
) O
and O
multilingual O
( O
for O
portuguese O
) O
. O

section 9
id pdf2json/2021.acl-long.212.pdf.json
for O
ppmi O
, O
only O
weak O
positive O
correlations O
were O
found O
for O
elmo O
and O
distilbert O
, O
indicating O
that O
for O
them O
higher O
cosine O
values O
weakly O
imply O
ncs O
with O
stronger O
association O
scores O
. O

section 9
id pdf2json/2021.acl-long.212.pdf.json
moreover O
, O
weak O
to O
moderate O
negative O
correlations O
with O
frequency O
were O
found O
for O
the O
bert O
models O
, O
suggesting O
that O
cosine O
similarity O
is O
higher O
for O
less O
frequent O
ncs O
. O

section 9
id pdf2json/2021.acl-long.212.pdf.json
the O
differences O
between O
nc O
out O
and O
nc O
outcomp O
indicate O
the O
importance O
of O
some O
degree O
of O
contextualisation O
( O
also O
found O
by O
yu O
and O
ettinger O
( O
2020 O
) O
) O
, O
even O
if O
only O
as O
one O
component O
contextualising O
the O
other O
in O
nc O
out O
, O
which O
may O
not O
be O
retrievable O
from O
the O
combination O
of O
the O
context-independent O
vectors O
of O
the O
components O
( O
nc O
outcomp O
) O
. O

section 9
id pdf2json/2021.acl-long.212.pdf.json
this O
is O
in O
line O
with O
the O
original O
strategy O
used O
with O
static O
embeddings O
, O
which O
learns O
the O
distribution O
of O
the O
ncs O
pre-identified O
as O
single O
tokens O
in O
corpora O
and O
that O
resulted O
in O
significantly O
better O
correlations O
per O
type O
than O
any O
of O
the O
contextualised O
models O
( O
cordeiro O
et O
al. O
, O
2019 O
) O
. O

section 9
id pdf2json/2021.acl-long.212.pdf.json
to O
make O
a O
fairer O
comparison O
between O
both O
approaches O
, O
we O
injected O
into O
the O
bert O
models O
single O
representations O
for O
the O
ncs O
, O
learnt O
from O
the O
referred O
ukwac O
and O
brwac O
corpora O
. O

section 9
id pdf2json/2021.acl-long.212.pdf.json
we O
first O
annotated O
as O
single O
tokens O
in O
the O
corpus O
those O
ncs O
present O
in O
the O
dataset O
, O
and O
used O
attentive O
mimicking O
with O
one-token-approximation O
( O
schick O
and O
schütze O
, O
2019 O
, O
2020b O
) O
to O
learn O
up O
to O
500 O
contexts O
for O
each O
compound O
. O

section 9
id pdf2json/2021.acl-long.212.pdf.json
after O
that O
, O
we O
injected O
these O
type O
level O
vectors O
into O
the O
bert O
models O
using O
bertram O
( O
schick O
and O
schütze O
, O
2020a O
) O
. O

section 9
id pdf2json/2021.acl-long.212.pdf.json
for O
english O
, O
these O
new O
representations O
obtained O
lower O
results O
than O
the O
original O
bert O
in O
nc O
out O
( O
e.g. O
, O
0.37 O
vs. O
0.28 O
at O
type O
level O
) O
, O
but O
higher O
in O
nc O
outcomp O
( O
0.16 O
vs. O
0.33 O
at O
type O
level O
) O
. O

section 9
id pdf2json/2021.acl-long.212.pdf.json
for O
portuguese O
, O
including O
single O
representations O
for O
the O
ncs O
in O
bert O
improved O
the O
correlations O
in O
three O
of O
the O
four O
scenarios O
( O
except O
for O
nc O
out O
at O
token O
level O
) O
, O
but O
the O
best O
results O
were O
almost O
identical O
to O
those O
of O
elmo O
( O
see O
the O
full O
results O
in O
the O
bottom O
rows O
of O
table O
5 O
) O
. O

section 9
id pdf2json/2021.acl-long.212.pdf.json
regarding O
the O
results O
reported O
by O
nandakumar O
et O
al O
. O

section 9
id pdf2json/2021.acl-long.212.pdf.json
( O
2019 O
) O
, O
for O
english O
, O
our O
experiments O
yielded O
higher O
correlations O
for O
bert O
and O
lower O
for O
elmo O
( O
≈ O
0.3 O
in O
both O
cases O
, O
depending O
on O
the O
setting O
) O
, O
which O
may O
be O
due O
to O
differences O
in O
how O
the O
vectors O
are O
generated O
( O
e.g. O
, O
the O
use O
of O
different O
input O
sentences O
, O
hidden O
layers O
or O
compositional O
operations O
) O
. O

section 9
id pdf2json/2021.acl-long.212.pdf.json
in O
sum O
, O
the O
results O
of O
these O
evaluations O
suggest O
that O
the O
use O
of O
a O
straightforward O
adaptation O
of O
a O
compositionality O
prediction O
approach O
that O
led O
to O
good O
performance O
with O
static O
models O
was O
not O
as O
successful O
with O
contextualised O
models O
. O

section 10
id pdf2json/2021.acl-long.212.pdf.json
we O
analyse O
whether O
models O
are O
able O
to O
capture O
differences O
in O
idiomaticity O
perceived O
by O
human O
annotators O
across O
the O
sentences O
in O
which O
an O
nc O
occurs O
. O

section 10
id pdf2json/2021.acl-long.212.pdf.json
that O
is O
, O
if O
an O
nc O
is O
found O
to O
be O
more O
idiomatic O
in O
one O
sentence O
than O
in O
others O
. O

section 10
id pdf2json/2021.acl-long.212.pdf.json
for O
that O
, O
we O
created O
an O
annotator O
’ O
s O
vector O
for O
each O
sentence O
, O
combining O
the O
human O
scores O
to O
create O
a O
three O
dimensional O
vector O
representation O
, O
where O
the O
first O
dimension O
is O
the O
average O
nc O
compositionality O
, O
and O
the O
second O
and O
third O
are O
the O
average O
scores O
of O
the O
contributions O
of O
the O
head O
and O
of O
the O
modifier O
. O

section 10
id pdf2json/2021.acl-long.212.pdf.json
for O
representing O
the O
sentence O
we O
obtain O
an O
embedding O
by O
averaging O
their O
( O
sub O
) O
words O
. O

section 10
id pdf2json/2021.acl-long.212.pdf.json
we O
calculated O
the O
euclidean O
distances O
between O
( O
i O
) O
the O
annotators O
’ O
vectors O
and O
( O
ii O
) O
the O
cosine O
similarities O
between O
sentence O
embeddings O
of O
each O
of O
the O
possible O
combinations O
of O
the O
three O
sentences O
associated O
to O
each O
nc O
. O

section 10
id pdf2json/2021.acl-long.212.pdf.json
then O
, O
we O
measured O
the O
correlations O
between O
these O
values O
using O
spearman O
ρ O
. O

section 10
id pdf2json/2021.acl-long.212.pdf.json
we O
aim O
to O
assess O
if O
annotations O
and O
models O
indicate O
the O
same O
relative O
differences.10 O
the O
results O
were O
averaged O
for O
the O
280 O
( O
english O
) O
and O
180 O
( O
portuguese O
) O
ncs O
. O

section 10
id pdf2json/2021.acl-long.212.pdf.json
table O
6 O
shows O
the O
results O
for O
the O
whole O
datasets O
and O
divided O
by O
compositionality O
level O
. O

section 10
id pdf2json/2021.acl-long.212.pdf.json
as O
we O
compare O
euclidean O
distances O
with O
cosine O
similarities O
negative O
values O
are O
actually O
positive O
correlations O
and O
vice O
versa O
. O

section 10
id pdf2json/2021.acl-long.212.pdf.json
the O
average O
ρ O
is O
close O
to O
0 O
suggesting O
that O
the O
embedding O
models O
do O
not O
capture O
the O
nuances O
in O
idiomaticity O
perceived O
by O
the O
annotators O
between O
the O
different O
sentences O
per O
nc O
. O

section 11
id pdf2json/2021.acl-long.212.pdf.json
we O
also O
analysed O
the O
similarity O
among O
the O
annotations O
for O
each O
nc O
in O
the O
three O
sentences O
, O
computing O
the O
standard O
deviations O
of O
the O
average O
compositionality O
scores O
given O
by O
the O
annotators O
. O

section 11
id pdf2json/2021.acl-long.212.pdf.json
in O
contrast O
to O
the O
previous O
experiment O
, O
here O
we O
represent O
the O
human O
annotations O
using O
only O
the O
idiomaticity O
scores O
of O
the O
whole O
ncs O
and O
the O
models O
’ O
output O
as O
the O
contextualised O
embedding O
of O
the O
ncs O
in O
each O
sentence O
. O

section 11
id pdf2json/2021.acl-long.212.pdf.json
at O
token O
level O
most O
compounds O
( O
85.7 O
% O
in O
english O
and O
91.1 O
% O
in O
portuguese O
) O
have O
mean O
idiomaticity O
scores O
with O
less O
than O
0.6 O
of O
standard O
deviation O
. O

section 11
id pdf2json/2021.acl-long.212.pdf.json
very O
few O
ncs O
have O
deviations O
higher O
than O
1 O
: O
five O
in O
english O
and O
four O
in O
portuguese O
. O

section 11
id pdf2json/2021.acl-long.212.pdf.json
looking O
at O
the O
contexts O
in O
which O
they O
occur O
, O
the O
variability O
seems O
to O
be O
due O
to O
the O
different O
topics O
to O
which O
the O
sentences O
refer O
. O

section 11
id pdf2json/2021.acl-long.212.pdf.json
for O
instance O
, O
the O
annotators O
have O
identified O
two O
senses O
of O
firing O
line O
: O
one O
, O
more O
idiomatic O
, O
referring O
to O
a O
position O
in O
which O
someone O
is O
criticised O
( O
mean O
score O
of O
1.25 O
) O
, O
and O
a O
second O
one O
( O
partially O
compositional O
, O
with O
an O
average O
of O
2.7 O
) O
referring O
to O
a O
specific O
position O
in O
an O
armed O
conflict O
. O

section 11
id pdf2json/2021.acl-long.212.pdf.json
in O
portuguese O
, O
céu O
aberto O
( O
‘ O
open-air O
’ O
, O
lit O
. O

section 11
id pdf2json/2021.acl-long.212.pdf.json
‘ O
open-sky O
’ O
) O
was O
interpreted O
as O
less O
compositional O
( O
1.2 O
) O
when O
describing O
urban O
settings O
( O
e.g. O
, O
open-air O
shopping O
centers O
) O
than O
when O
referring O
to O
wild O
places O
( O
e.g. O
, O
lobas O
que O
lutavam O
a O
céu O
aberto O
, O
‘ O
wolves O
fighting O
in O
the O
open O
’ O
) O
, O
with O
a O
mean O
idiomaticity O
score O
of O
3 O
. O

section 11
id pdf2json/2021.acl-long.212.pdf.json
10spearman O
ρ O
is O
not O
used O
here O
as O
a O
statistical O
test O
but O
as O
a O
measure O
to O
evaluate O
if O
the O
sentence O
comparisons O
with O
two O
different O
metrics O
yield O
the O
same O
relative O
differences O
. O

section 11
id pdf2json/2021.acl-long.212.pdf.json
as O
there O
are O
only O
three O
sentences O
to O
compare O
, O
ρ O
assumes O
only O
four O
values O
±0.5 O
or O
±1 O
. O

section 11
id pdf2json/2021.acl-long.212.pdf.json
to O
observe O
whether O
language O
models O
capture O
these O
differences O
across O
sentences O
, O
we O
calculated O
the O
cosine O
similarities O
between O
the O
ncs O
in O
the O
three O
sentences O
and O
the O
standard O
deviation O
of O
these O
three O
values O
. O

section 11
id pdf2json/2021.acl-long.212.pdf.json
we O
then O
computed O
the O
spearman O
correlations O
between O
these O
deviations O
obtained O
from O
the O
models O
’ O
representations O
and O
those O
of O
the O
human O
annotations O
: O
all O
correlations O
were O
very O
low O
and O
not O
significant O
, O
suggesting O
that O
the O
vector O
representations O
do O
not O
capture O
the O
variability O
perceived O
by O
the O
annotators O
. O

section 11
id pdf2json/2021.acl-long.212.pdf.json
finally O
, O
we O
have O
also O
selected O
two O
ncs O
in O
english O
with O
a O
combination O
of O
idiomatic O
and O
compositional O
meanings O
( O
brick O
wall O
, O
and O
gold O
mine O
) O
. O

section 11
id pdf2json/2021.acl-long.212.pdf.json
in O
these O
examples O
, O
we O
found O
that O
for O
bert O
( O
our O
best O
model O
) O
the O
cosine O
similarities O
between O
the O
idiomatic O
meanings O
were O
higher O
( O
0.83 O
in O
both O
cases O
) O
than O
between O
idiomatic O
and O
compositional O
senses O
( O
0.68 O
and O
0.7 O
, O
respectively O
) O
, O
suggesting O
that O
they O
are O
somehow O
identifying O
the O
different O
senses O
. O

section 11
id pdf2json/2021.acl-long.212.pdf.json
however O
, O
since O
the O
highest O
standard O
deviations O
were O
achieved O
with O
ncs O
representing O
the O
same O
sense O
in O
all O
contexts O
( O
e.g. O
, O
big O
wig O
and O
grass O
root O
) O
, O
further O
analysis O
is O
needed O
. O

section 11
id pdf2json/2021.acl-long.212.pdf.json
as O
neither O
the O
cosine O
similarities O
obtained O
with O
bert-based O
models O
nor O
the O
standard O
deviations O
between O
them O
were O
correlated O
with O
the O
variation O
in O
the O
human O
scores O
, O
these O
analyses O
suggest O
that O
stateof-the-art O
contextualised O
models O
still O
do O
not O
model O
semantic O
compositionality O
as O
human O
annotators O
do O
. O

section 11
id pdf2json/2021.acl-long.212.pdf.json
the O
experiments O
performed O
in O
this O
section O
have O
shown O
, O
on O
the O
one O
hand O
, O
some O
of O
the O
possibilities O
of O
a O
multilingual O
dataset O
labeled O
at O
type O
and O
token O
level O
; O
on O
the O
other O
hand O
, O
the O
results O
also O
suggest O
that O
capturing O
idiomaticity O
is O
a O
hard O
task O
for O
current O
language O
models O
, O
as O
only O
some O
of O
them O
show O
moderate O
correlations O
with O
human O
annotations O
in O
some O
scenarios O
. O

section 12
id pdf2json/2021.acl-long.212.pdf.json
this O
paper O
presented O
the O
nctti O
, O
a O
dataset O
of O
ncs O
in O
english O
and O
portuguese O
annotated O
at O
type O
and O
token O
level O
with O
human O
judgments O
about O
idiomaticity O
, O
and O
with O
suggestions O
of O
paraphrases O
. O

section 12
id pdf2json/2021.acl-long.212.pdf.json
the O
very O
strong O
correlations O
found O
between O
type O
and O
token O
judgments O
confirm O
the O
robustness O
of O
the O
scores O
, O
while O
the O
paraphrases O
provide O
further O
validation O
of O
the O
interpretation O
of O
the O
ncs O
. O

section 12
id pdf2json/2021.acl-long.212.pdf.json
moreover O
, O
evaluations O
involving O
embedding O
models O
with O
different O
levels O
of O
contextualisation O
suggest O
that O
they O
are O
still O
far O
from O
providing O
accurate O
estimates O
of O
nc O
idiomaticity O
, O
at O
least O
using O
the O
measures O
proposed O
and O
analysed O
in O
the O
paper O
. O

section 12
id pdf2json/2021.acl-long.212.pdf.json
mwes O
are O
still O
a O
pain O
in O
the O
neck O
for O
nlp O
, O
and O
datasets O
like O
the O
nctti O
can O
contribute O
towards O
finding O
better O
representations O
for O
them O
and O
better O
measures O
for O
idiomaticity O
identification O
. O

section 12
id pdf2json/2021.acl-long.212.pdf.json
future O
work O
includes O
using O
these O
ncs O
as O
seeds O
in O
cross-lingual O
representations O
for O
enriching O
the O
dataset O
with O
nc O
equivalents O
in O
different O
languages O
. O

section 12
id pdf2json/2021.acl-long.212.pdf.json
besides O
, O
we O
also O
plan O
to O
enlarge O
the O
datasets O
including O
a O
subset O
of O
sentences O
with O
ambiguous O
ncs O
having O
idiomatic O
and O
compositional O
interpretations O
depending O
on O
the O
context O
. O

section 13
id pdf2json/2021.acl-long.212.pdf.json
aline O
villavicencio O
and O
carolina O
scarton O
are O
funded O
by O
the O
epsrc O
project O
mia O
: O
modeling O
idiomaticity O
in O
human O
and O
artificial O
language O
processing O
( O
ep/t02450x/1 O
) O
. O

section 13
id pdf2json/2021.acl-long.212.pdf.json
marcos O
garcia O
is O
funded O
by O
the O
consellerı́a O
de O
cultura O
, O
educación O
e O
ordenación O
universitaria O
of O
the O
galician O
government O
( O
erdf O
2014-2020 O
: O
call O
ed431g O
2019/04 O
) O
, O
and O
by O
a O
ramón O
y O
cajal O
grant O
( O
ryc2019-028473-i O
) O
. O

section TITLE
id pdf2json/2021.acl-long.6.pdf.json
deeprapper O
: O
neural O
rap O
generation O
with O
rhyme O
and O
rhythm O
modeling O

section ABSTRACT
id pdf2json/2021.acl-long.6.pdf.json
rap O
generation O
, O
which O
aims O
to O
produce O
lyrics O
and O
corresponding O
singing O
beats O
, O
needs O
to O
model O
both O
rhymes O
and O
rhythms O
. O

section ABSTRACT
id pdf2json/2021.acl-long.6.pdf.json
previous O
works O
for O
rap O
generation O
focused O
on O
rhyming O
lyrics O
but O
ignored O
rhythmic O
beats O
, O
which O
are O
important O
for O
rap O
performance O
. O

section ABSTRACT
id pdf2json/2021.acl-long.6.pdf.json
in O
this O
paper O
, O
we O
develop O
deeprapper O
, O
a O
transformer-based O
rap O
generation O
system O
that O
can O
model O
both O
rhymes O
and O
rhythms O
. O

section ABSTRACT
id pdf2json/2021.acl-long.6.pdf.json
since O
there O
is O
no O
available O
rap O
dataset O
with O
rhythmic O
beats O
, O
we O
develop O
a O
data O
mining O
pipeline O
to O
collect O
a O
largescale O
rap O
dataset O
, O
which O
includes O
a O
large O
number O
of O
rap O
songs O
with O
aligned O
lyrics O
and O
rhythmic O
beats O
. O

section ABSTRACT
id pdf2json/2021.acl-long.6.pdf.json
second O
, O
we O
design O
a O
transformerbased O
autoregressive O
language O
model O
which O
carefully O
models O
rhymes O
and O
rhythms O
. O

section ABSTRACT
id pdf2json/2021.acl-long.6.pdf.json
specifically O
, O
we O
generate O
lyrics O
in O
the O
reverse O
order O
with O
rhyme O
representation O
and O
constraint O
for O
rhyme O
enhancement O
and O
insert O
a O
beat O
symbol O
into O
lyrics O
for O
rhythm/beat O
modeling O
. O

section ABSTRACT
id pdf2json/2021.acl-long.6.pdf.json
to O
our O
knowledge O
, O
deeprapper O
is O
the O
first O
system O
to O
generate O
rap O
with O
both O
rhymes O
and O
rhythms O
. O

section ABSTRACT
id pdf2json/2021.acl-long.6.pdf.json
both O
objective O
and O
subjective O
evaluations O
demonstrate O
that O
deeprapper O
generates O
creative O
and O
high-quality O
raps O
with O
rhymes O
and O
rhythms O
. O

section 0
id pdf2json/2021.acl-long.6.pdf.json
proceedings O
of O
the O
59th O
annual O
meeting O
of O
the O
association O
for O
computational O
linguistics O
and O
the O
11th O
international O
joint O
conference O
on O
natural O
language O
processing O
, O
pages O
69–81 O
august O
1–6 O
, O
2021 O
. O

section 0
id pdf2json/2021.acl-long.6.pdf.json
©2021 O
association O
for O
computational O
linguistics O
69 O

section 1
id pdf2json/2021.acl-long.6.pdf.json
rap O
is O
a O
musical O
form O
originating O
from O
america O
in O
1970s O
, O
and O
has O
quickly O
developed O
as O
one O
of O
the O
mainstream O
music O
genres O
in O
the O
world O
( O
keyes O
, O
2004 O
) O
. O

section 1
id pdf2json/2021.acl-long.6.pdf.json
with O
the O
rapid O
development O
of O
artificial O
intelligence O
, O
automatic O
rap O
lyrics O
generation O
has O
drawn O
attention O
from O
academia O
( O
potash O
et O
al. O
, O
2015 O
; O
malmi O
et O
al. O
, O
2016 O
; O
liang O
et O
al. O
, O
2018 O
; O
nikolov O
et O
al. O
, O
2020 O
) O
. O

section 1
id pdf2json/2021.acl-long.6.pdf.json
generally O
speaking O
, O
rap O
lyrics O
need O
to O
be O
semantically O
meaningful O
and O
fashionable O
to O
convey O
interesting O
stories O
or O
express O
feelings O
. O

section 1
id pdf2json/2021.acl-long.6.pdf.json
different O
from O
natural O
language O
or O
other O
artistic O
genres O
( O
e.g. O
, O
∗ O
corresponding O
author O
: O
xu O
tan O
, O
xuta O
@ O
microsoft.com O
lyrics O
or O
poetry O
) O
, O
rap O
has O
distinctive O
characteristics O
: O
1 O
) O
it O
usually O
contains O
complex O
rhyme O
patterns O
among O
several O
consecutive O
sentences O
, O
which O
are O
the O
key O
to O
form O
a O
good O
flow O
; O
2 O
) O
it O
needs O
to O
align O
with O
the O
singing O
beat O
since O
rap O
lyrics O
are O
usually O
rapped O
according O
to O
some O
rhythmic O
accompaniments O
. O

section 1
id pdf2json/2021.acl-long.6.pdf.json
therefore O
, O
how O
to O
generate O
rap O
lyrics O
with O
good O
rhymes O
and O
rhythms O
is O
a O
troublesome O
problem O
. O

section 1
id pdf2json/2021.acl-long.6.pdf.json
previous O
works O
( O
potash O
et O
al. O
, O
2015 O
; O
malmi O
et O
al. O
, O
2016 O
; O
liang O
et O
al. O
, O
2018 O
; O
nikolov O
et O
al. O
, O
2020 O
) O
for O
rap O
generation O
mainly O
focused O
on O
lyric O
generation O
and O
some O
of O
them O
developed O
strategies O
for O
rhyme O
modeling O
. O

section 1
id pdf2json/2021.acl-long.6.pdf.json
potash O
et O
al O
. O

section 1
id pdf2json/2021.acl-long.6.pdf.json
( O
2015 O
) O
directly O
added O
a O
“ O
< O
endline O
> O
” O
token O
at O
the O
end O
of O
verse O
lines O
and O
expected O
to O
learn O
rhyme O
patterns O
implicitly O
. O

section 1
id pdf2json/2021.acl-long.6.pdf.json
nikolov O
et O
al O
. O

section 1
id pdf2json/2021.acl-long.6.pdf.json
( O
2020 O
) O
applied O
a O
two-step O
strategy O
, O
which O
first O
generates O
rap O
lyrics O
and O
then O
adds O
rhyme O
tokens O
to O
the O
end O
of O
generated O
lyrics O
. O

section 1
id pdf2json/2021.acl-long.6.pdf.json
however O
, O
these O
methods O
can O
not O
guarantee O
the O
rhyme O
patterns O
for O
every O
lyric O
line O
and O
only O
care O
the O
rhyme O
on O
the O
last O
token O
. O

section 1
id pdf2json/2021.acl-long.6.pdf.json
although O
many O
works O
have O
studied O
rhyming O
modeling O
in O
other O
artistic O
genres O
( O
e.g. O
, O
poetry O
) O
( O
li O
et O
al. O
, O
2020 O
; O
van O
de O
cruys O
, O
2020 O
; O
liu O
et O
al. O
, O
2020 O
) O
, O
they O
are O
not O
suitable O
for O
rap O
generation O
due O
to O
the O
complex O
rhyme O
structure O
in O
rap O
. O

section 1
id pdf2json/2021.acl-long.6.pdf.json
for O
example O
, O
poetry O
needs O
to O
rhyme O
with O
only O
the O
last O
word O
in O
each O
sentence O
, O
while O
rap O
rhymes O
with O
multiple O
consecutive O
tokens O
at O
the O
end O
of O
each O
sentence O
. O

section 1
id pdf2json/2021.acl-long.6.pdf.json
no O
previous O
works O
have O
studied O
rhythm O
modeling O
( O
i.e. O
, O
beats O
in O
rap O
) O
, O
to O
our O
knowledge O
. O

section 1
id pdf2json/2021.acl-long.6.pdf.json
one O
of O
the O
main O
reasons O
is O
the O
lack O
of O
rap O
datasets O
with O
beat-lyric O
alignment O
. O

section 1
id pdf2json/2021.acl-long.6.pdf.json
consequently O
, O
the O
generation O
of O
lyrics O
without O
rhythmic O
beats O
can O
not O
be O
regarded O
as O
a O
full O
rap O
generation O
. O

section 1
id pdf2json/2021.acl-long.6.pdf.json
in O
this O
paper O
, O
we O
develop O
deeprapper O
, O
a O
transformer O
( O
vaswani O
et O
al. O
, O
2017 O
) O
based O
rap O
generation O
system O
which O
can O
model O
both O
rhymes O
and O
rhythms O
. O

section 1
id pdf2json/2021.acl-long.6.pdf.json
to O
build O
the O
system O
, O
since O
there O
is O
no O
available O
rap O
datasets O
with O
aligned O
rhythmic O
beats O
, O
we O
design O
a O
data O
mining O
pipeline O
and O
collect O
a O
large-scale O
rap O
dataset O
for O
rhythm O
modeling O
. O

section 1
id pdf2json/2021.acl-long.6.pdf.json
specifically O
, O
we O
first O
crawl O
many O
rap O
songs O
, O
each O
song O
with O
both O
rap O
lyrics O
and O
audios O
, O
from O
the O
web O
. O

section 1
id pdf2json/2021.acl-long.6.pdf.json
for O
each O
crawled O
rap O
song O
, O
we O
perform O
a O
series O
of O
data O
preprocessing O
steps O
to O
extract O
rhythmic O
beats O
as O
well O
as O
beat-lyric O
alignment O
. O

section 1
id pdf2json/2021.acl-long.6.pdf.json
to O
better O
model O
rhyme O
, O
we O
generate O
the O
words O
in O
a O
rap O
sentence O
from O
right O
to O
left O
in O
an O
autoregressive O
manner O
. O

section 1
id pdf2json/2021.acl-long.6.pdf.json
doing O
so O
we O
can O
easily O
identify O
the O
last O
few O
words O
of O
a O
sentence O
( O
now O
become O
the O
first O
words O
of O
the O
reverse O
sentence O
) O
to O
rhyme O
with O
. O

section 1
id pdf2json/2021.acl-long.6.pdf.json
additionally O
, O
we O
incorporate O
several O
rhymerelated O
representations O
into O
our O
language O
model O
to O
further O
improve O
the O
rhyming O
quality O
, O
and O
encourage O
the O
n O
-gram O
rhyme O
in O
generated O
rap O
lyrics O
through O
rhyme O
constraint O
during O
inference O
. O

section 1
id pdf2json/2021.acl-long.6.pdf.json
we O
use O
a O
special O
token O
[ O
beat O
] O
to O
represent O
the O
rhythmic O
beat O
and O
insert O
it O
into O
lyrics O
right O
before O
the O
corresponding O
word O
. O

section 1
id pdf2json/2021.acl-long.6.pdf.json
in O
this O
way O
, O
we O
can O
model O
the O
beat O
in O
the O
lyric O
sequence O
both O
in O
training O
and O
generation O
. O

section 1
id pdf2json/2021.acl-long.6.pdf.json
inspired O
by O
the O
success O
of O
pre-trained O
language O
models O
( O
devlin O
et O
al. O
, O
2019 O
; O
radford O
et O
al. O
, O
2018 O
; O
yang O
et O
al. O
, O
2019 O
; O
song O
et O
al. O
, O
2019 O
; O
liu O
et O
al. O
, O
2019 O
) O
, O
we O
incorporate O
pre-training O
into O
our O
system O
. O

section 1
id pdf2json/2021.acl-long.6.pdf.json
to O
obtain O
large-scale O
data O
for O
pre-training O
, O
we O
also O
use O
our O
data O
mining O
pipeline O
to O
collect O
another O
two O
datasets O
: O
1 O
) O
non-rap O
songs O
with O
aligned O
beats O
, O
which O
can O
be O
larger O
than O
rap O
dataset O
since O
non-rap O
songs O
are O
more O
general O
than O
rap O
songs O
; O
2 O
) O
pure O
lyrics O
, O
which O
can O
be O
even O
larger O
than O
non-rap O
songs O
. O

section 1
id pdf2json/2021.acl-long.6.pdf.json
in O
the O
pre-training O
stage O
, O
we O
pre-train O
our O
deeprapper O
model O
based O
on O
the O
above O
two O
datasets O
. O

section 1
id pdf2json/2021.acl-long.6.pdf.json
then O
we O
fine-tune O
our O
pre-trained O
model O
on O
the O
rap O
songs O
with O
aligned O
beats O
. O

section 1
id pdf2json/2021.acl-long.6.pdf.json
the O
fine-tuned O
model O
is O
used O
for O
final O
rap O
generation O
. O

section 1
id pdf2json/2021.acl-long.6.pdf.json
both O
objective O
and O
subjective O
evaluations O
verify O
the O
advantages O
of O
deeprapper O
in O
generating O
rap O
lyrics O
with O
rhymes O
and O
rhythms O
. O

section 1
id pdf2json/2021.acl-long.6.pdf.json
our O
main O
contributions O
can O
be O
summarized O
as O
follows O
: O
• O
to O
model O
rhythms O
in O
rap O
generation O
, O
we O
develop O
a O
data O
mining O
pipeline O
to O
create O
rap O
datasets O
with O
aligned O
rhythmic O
beats O
. O

section 1
id pdf2json/2021.acl-long.6.pdf.json
• O
to O
better O
model O
rhymes O
, O
we O
design O
an O
autoregressive O
language O
model O
to O
generate O
rap O
lyrics O
from O
right O
to O
left O
with O
rhyme O
constraint O
. O

section 1
id pdf2json/2021.acl-long.6.pdf.json
as O
far O
as O
we O
know O
, O
deeprapper O
is O
the O
first O
to O
explicitly O
model O
n O
-gram O
rhymes O
. O

section 1
id pdf2json/2021.acl-long.6.pdf.json
• O
we O
elaborately O
insert O
the O
beat O
token O
inside O
lyrics O
to O
model O
the O
rhythmic O
beats O
. O

section 1
id pdf2json/2021.acl-long.6.pdf.json
to O
our O
knowledge O
, O
deeprapper O
is O
the O
first O
system O
that O
models O
rhythms O
for O
rap O
generation O
. O

section 2
id pdf2json/2021.acl-long.6.pdf.json
since O
deeprapper O
generates O
rap O
lyrics O
with O
both O
rhyme O
and O
rhythm O
modeling O
, O
in O
this O
section O
, O
we O
briefly O
introduce O
the O
related O
background O
: O
lyric O
generation O
, O
rhyme O
modeling O
and O
rhythm O
modeling O
. O

section 2
id pdf2json/2021.acl-long.6.pdf.json
lyric O
generation O
broadly O
speaking O
, O
lyric O
generation O
can O
cover O
rap O
lyric O
generation O
( O
potash O
et O
al. O
, O
2015 O
; O
nikolov O
et O
al. O
, O
2020 O
; O
liang O
et O
al. O
, O
2018 O
) O
, O
song O
lyric O
generation O
( O
watanabe O
et O
al. O
, O
2018 O
; O
lu O
et O
al. O
, O
2019 O
; O
chen O
and O
lerch O
, O
2020 O
; O
sheng O
et O
al. O
, O
2020 O
) O
, O
general O
poetry O
generation O
( O
zhang O
and O
lapata O
, O
2014 O
; O
lau O
et O
al. O
, O
2018 O
; O
li O
et O
al. O
, O
2020 O
; O
liu O
et O
al. O
, O
2020 O
) O
and O
etc O
. O

section 2
id pdf2json/2021.acl-long.6.pdf.json
different O
from O
previous O
works O
that O
leverage O
language O
model O
to O
generate O
lyrics O
similar O
to O
natural O
language O
, O
in O
this O
paper O
, O
we O
introduce O
a O
novel O
language O
model O
for O
rap O
generation O
, O
with O
well-designed O
rhyme O
and O
rhythm O
modeling O
to O
fit O
the O
characteristics O
of O
rap O
lyrics O
. O

section 2
id pdf2json/2021.acl-long.6.pdf.json
additionally O
, O
inspired O
by O
the O
successes O
of O
pre-trained O
language O
models O
( O
devlin O
et O
al. O
, O
2019 O
; O
yang O
et O
al. O
, O
2019 O
; O
liu O
et O
al. O
, O
2019 O
; O
radford O
et O
al. O
, O
2019 O
; O
song O
et O
al. O
, O
2019 O
) O
in O
nlp O
applications O
, O
we O
also O
incorporate O
pre-training O
into O
our O
model O
to O
further O
improve O
the O
quality O
of O
rap O
generation O
. O

section 2
id pdf2json/2021.acl-long.6.pdf.json
rhyme O
modeling O
rhyme O
modeling O
plays O
an O
important O
role O
in O
rap O
generation O
, O
which O
requires O
the O
last O
few O
tokens O
in O
consecutive O
sentences O
to O
have O
the O
same O
rhyme O
pattern O
. O

section 2
id pdf2json/2021.acl-long.6.pdf.json
existing O
rap O
generation O
systems O
either O
directly O
add O
a O
special O
token O
“ O
< O
endline O
> O
” O
at O
the O
end O
of O
rap O
lyric O
to O
encourage O
the O
model O
to O
learn O
rhyme O
structure O
( O
potash O
et O
al. O
, O
2015 O
) O
, O
or O
introduce O
a O
two-step O
strategy O
for O
rhyme O
modeling O
that O
first O
generates O
rap O
lyrics O
and O
then O
adds O
rhyme O
tokens O
after O
the O
generated O
lyrics O
( O
nikolov O
et O
al. O
, O
2020 O
) O
. O

section 2
id pdf2json/2021.acl-long.6.pdf.json
however O
, O
these O
works O
only O
focused O
on O
unigram O
rhyme O
while O
rap O
appreciates O
more O
for O
n-gram O
rhyme O
. O

section 2
id pdf2json/2021.acl-long.6.pdf.json
although O
a O
lot O
of O
works O
have O
explored O
rhyme O
modeling O
in O
other O
genres O
, O
most O
of O
them O
can O
not O
be O
directly O
used O
for O
rap O
generation O
. O

section 2
id pdf2json/2021.acl-long.6.pdf.json
for O
example O
, O
poetry O
generation O
( O
lau O
et O
al. O
, O
2018 O
; O
zhipeng O
et O
al. O
, O
2019 O
; O
liao O
et O
al. O
, O
2019 O
; O
li O
et O
al. O
, O
2020 O
) O
usually O
used O
pre-defined O
format O
to O
control O
the O
rhyme O
pattern O
since O
poetry O
usually O
has O
fixed O
number O
of O
words O
and O
only O
cares O
the O
rhyme O
pattern O
for O
the O
last O
word O
. O

section 2
id pdf2json/2021.acl-long.6.pdf.json
however O
, O
rap O
lyrics O
have O
diverse O
rhyme O
structures O
across O
multiple O
consecutive O
sentences O
and O
most O
importantly O
multiple O
con- O
secutive O
words O
. O

section 2
id pdf2json/2021.acl-long.6.pdf.json
therefore O
, O
we O
introduce O
n O
-gram O
rhyme O
modeling O
in O
deeprapper O
to O
handle O
the O
distinctive O
rhyme O
patterns O
in O
rap O
. O

section 2
id pdf2json/2021.acl-long.6.pdf.json
besides O
, O
we O
also O
train O
our O
language O
model O
in O
a O
reverse O
order O
( O
i.e. O
, O
right O
to O
left O
) O
, O
similar O
to O
previous O
works O
( O
van O
de O
cruys O
, O
2020 O
) O
, O
to O
better O
model O
rhymes O
since O
they O
always O
occur O
at O
the O
end O
of O
sentence O
. O

section 2
id pdf2json/2021.acl-long.6.pdf.json
rhythm O
modeling O
rhythm O
modeling O
is O
usually O
used O
in O
music O
generation O
( O
zhu O
et O
al. O
, O
2018 O
; O
huang O
and O
yang O
, O
2020 O
; O
ren O
et O
al. O
, O
2020 O
) O
which O
generates O
the O
duration O
of O
notes O
along O
with O
the O
note O
pitch O
to O
form O
rhythmic O
beats O
in O
melody O
and O
accompaniment O
generation O
. O

section 2
id pdf2json/2021.acl-long.6.pdf.json
different O
from O
music O
generation O
, O
rap O
cares O
more O
about O
rhythmic O
beats O
instead O
of O
note O
pitches O
( O
i.e O
. O

section 2
id pdf2json/2021.acl-long.6.pdf.json
melody O
) O
. O

section 2
id pdf2json/2021.acl-long.6.pdf.json
in O
this O
way O
, O
the O
generated O
rap O
lyrics O
need O
to O
align O
with O
the O
corresponding O
rhythmic O
beats O
in O
order O
to O
be O
rapped O
, O
otherwise O
it O
can O
not O
be O
regarded O
as O
a O
complete O
rap O
. O

section 2
id pdf2json/2021.acl-long.6.pdf.json
however O
, O
to O
the O
best O
of O
our O
knowledge O
, O
none O
of O
previous O
works O
have O
studied O
the O
rhythm O
modeling O
in O
rap O
generation O
. O

section 2
id pdf2json/2021.acl-long.6.pdf.json
in O
this O
paper O
, O
we O
introduce O
a O
novel O
beat O
modeling O
strategy O
in O
deeprapper O
for O
rhythm O
generation O
. O

section 3
id pdf2json/2021.acl-long.6.pdf.json
previous O
works O
( O
potash O
et O
al. O
, O
2015 O
; O
liang O
et O
al. O
, O
2018 O
; O
nikolov O
et O
al. O
, O
2020 O
) O
for O
rap O
generation O
usually O
used O
rap O
datasets O
with O
only O
lyrics O
, O
without O
considering O
the O
rhythmic O
beat O
information O
. O

section 3
id pdf2json/2021.acl-long.6.pdf.json
to O
model O
rhythm O
in O
rap O
generation O
, O
the O
rap O
dataset O
should O
contain O
lyrics O
with O
aligned O
rhythmic O
beats O
. O

section 3
id pdf2json/2021.acl-long.6.pdf.json
however O
, O
beat O
alignments O
are O
quite O
difficult O
to O
obtain O
, O
since O
their O
annotations O
require O
musicians O
with O
professional O
knowledge O
to O
identify O
stressing O
syllable O
in O
rap O
songs O
. O

section 3
id pdf2json/2021.acl-long.6.pdf.json
to O
handle O
this O
problem O
, O
we O
design O
a O
data O
mining O
pipeline O
to O
automatically O
extract O
beatlyric O
alignments O
. O

section 3
id pdf2json/2021.acl-long.6.pdf.json
in O
this O
section O
, O
we O
introduce O
the O
details O
of O
the O
data O
mining O
pipeline O
and O
our O
mined O
dataset O
based O
on O
this O
pipeline O
. O

section 4
id pdf2json/2021.acl-long.6.pdf.json
figure O
1 O
overviews O
our O
data O
mining O
pipeline O
, O
which O
consists O
of O
5 O
steps O
: O
data O
crawling O
, O
vocal O
and O
accompaniment O
separation O
, O
vocal O
and O
lyric O
alignment O
, O
beat O
detection O
, O
and O
lyric O
and O
beat O
alignment O
. O

section 4
id pdf2json/2021.acl-long.6.pdf.json
data O
crawling O
to O
mine O
a O
large-scale O
rap O
dataset O
, O
we O
first O
crawl O
a O
large O
amount O
of O
rap O
songs O
with O
both O
lyrics O
and O
singing O
audios O
from O
the O
web O
. O

section 4
id pdf2json/2021.acl-long.6.pdf.json
to O
ensure O
the O
lyric O
and O
audio O
can O
be O
aligned O
in O
the O
sentence O
level O
which O
is O
beneficial O
for O
our O
later O
word-level O
beat O
alignment O
, O
we O
also O
crawl O
the O
start O
and O
end O
time O
of O
each O
lyric O
sentence O
corresponding O
to O
the O
audio O
. O

section 4
id pdf2json/2021.acl-long.6.pdf.json
vocal O
and O
accompaniment O
separation O
for O
each O
rap O
song O
, O
we O
utilize O
spleeter O
( O
hennequin O
et O
al. O
, O
2020 O
) O
1 O
, O
a O
public O
music O
separation O
tool O
, O
to O
separate O
the O
vocal O
( O
containing O
rap O
singing O
) O
and O
accompaniment O
( O
containing O
rhythmic O
beats O
) O
from O
the O
crawled O
rap O
audio O
. O

section 4
id pdf2json/2021.acl-long.6.pdf.json
vocal O
and O
lyric O
alignment O
we O
split O
the O
separated O
vocals O
into O
the O
sentence O
level O
according O
to O
the O
crawled O
start O
and O
end O
time O
of O
each O
lyric O
sentence O
, O
and O
thus O
we O
can O
get O
the O
vocal-lyric O
alignments O
in O
the O
sentence O
level O
. O

section 4
id pdf2json/2021.acl-long.6.pdf.json
we O
convert O
lyrics O
into O
phonemes O
via O
phonemizer O
2 O
and O
utilize O
montreal O
forced O
aligner O
3 O
to O
obtain O
vocal-lyric O
alignments O
in O
the O
phoneme O
level O
. O

section 4
id pdf2json/2021.acl-long.6.pdf.json
based O
on O
these O
phoneme-level O
vocal-lyric O
alignments O
, O
we O
obtain O
the O
corresponding O
timestamp O
of O
each O
word O
in O
the O
singing O
audio O
. O

section 4
id pdf2json/2021.acl-long.6.pdf.json
beat O
detection O
to O
obtain O
the O
alignments O
between O
lyrics O
and O
beats O
, O
we O
need O
to O
know O
the O
timestamp O
of O
each O
beat O
. O

section 4
id pdf2json/2021.acl-long.6.pdf.json
therefore O
, O
we O
use O
a O
beat O
track O
detection O
tool O
, O
librosa O
( O
mcfee O
et O
al. O
, O
2020 O
) O
4 O
, O
to O
track O
the O
timestamp O
of O
each O
beat O
from O
the O
separated O
accompaniment O
that O
obtained O
from O
the O
second O
step O
. O

section 4
id pdf2json/2021.acl-long.6.pdf.json
lyric O
and O
beat O
alignment O
after O
we O
obtain O
the O
timestamp O
of O
each O
word O
and O
each O
beat O
, O
we O
can O
align O
them O
together O
according O
to O
their O
timestamps O
. O

section 4
id pdf2json/2021.acl-long.6.pdf.json
however O
, O
since O
a O
rapper O
may O
not O
sing O
a O
word O
exactly O
following O
the O
beat O
, O
directly O
using O
the O
timestamp O
to O
exactly O
match O
the O
word O
and O
beat O
is O
inappropriate O
. O

section 4
id pdf2json/2021.acl-long.6.pdf.json
therefore O
, O
we O
propose O
an O
approximate O
method O
to O
align O
them O
. O

section 4
id pdf2json/2021.acl-long.6.pdf.json
denote O
the O
word O
sequence O
of O
a O
lyric O
1https O
: O
//github.com/deezer/spleeter O
2https O
: O
//github.com/bootphon/phonemizer O
3https O
: O
//github.com/montrealcorpustools/montreal- O
forced-aligner O
4https O
: O
//github.com/librosa/librosa O
sentence O
as O
w O
= O
{ O
w1 O
, O
w2 O
, O
· O
· O
· O
, O
w|w| O
} O
, O
and O
its O
beat O
sequence O
as O
b O
= O
{ O
b1 O
, O
b2 O
, O
· O
· O
· O
, O
b|b| O
} O
, O
where O
wi O
and O
bj O
represent O
i-th O
word O
and O
j-th O
beat O
. O

section 4
id pdf2json/2021.acl-long.6.pdf.json
we O
use O
twi O
and O
tbj O
to O
represent O
the O
timestamps O
of O
wi O
and O
bj O
respectively O
. O

section 4
id pdf2json/2021.acl-long.6.pdf.json
for O
each O
beat O
bj O
, O
we O
first O
filter O
out O
a O
word O
set O
w̃ O
= O
{ O
w O
: O
∣∣tbj O
− O
tw∣∣ O
≤ O
r/2 O
, O
w O
∈ O
w O
} O
, O
where O
r O
represents O
the O
average O
duration O
of O
each O
word O
in O
the O
song O
( O
i.e. O
, O
the O
total O
duration O
divides O
the O
number O
of O
words O
) O
. O

section 4
id pdf2json/2021.acl-long.6.pdf.json
next O
, O
word O
wi O
is O
aligned O
with O
beat O
bj O
if O
it O
satisfies O
the O
following O
condition O
: O
wi O
= O
min O
w O
|tbj O
− O
tw| O
, O
w O
∈ O
w̃ O
. O

section 4
id pdf2json/2021.acl-long.6.pdf.json
( O
1 O
) O

section 5
id pdf2json/2021.acl-long.6.pdf.json
using O
the O
above O
data O
mining O
pipeline O
, O
we O
obtain O
a O
rap O
lyric O
dataset O
with O
aligned O
beats O
( O
named O
as O
d-rap O
, O
where O
d O
represents O
“ O
dataset O
” O
) O
, O
which O
satisfies O
the O
requirements O
of O
building O
a O
rap O
generation O
system O
with O
both O
rhyme O
and O
rhythm O
modeling O
. O

section 5
id pdf2json/2021.acl-long.6.pdf.json
we O
split O
the O
d-rap O
dataset O
into O
the O
training O
and O
validation O
set O
with O
a O
ratio O
of O
4:1 O
. O

section 5
id pdf2json/2021.acl-long.6.pdf.json
since O
rap O
is O
only O
one O
of O
music O
genres O
and O
the O
number O
of O
rap O
songs O
is O
usually O
smaller O
compared O
with O
more O
general O
songs O
, O
we O
also O
mine O
another O
two O
datasets O
to O
pre-train O
our O
deeprapper O
model O
with O
the O
same O
mining O
pipeline O
: O
1 O
) O
non-rap O
songs O
with O
aligned O
beats O
( O
named O
as O
d-song O
) O
; O
2 O
) O
pure O
lyrics O
without O
aligned O
beats O
( O
named O
as O
d-lyric O
) O
. O

section 5
id pdf2json/2021.acl-long.6.pdf.json
we O
summarize O
the O
statistics O
of O
the O
three O
datasets O
in O
table O
1 O
and O
show O
a O
rap O
song O
with O
aligned O
beats O
from O
d-rap O
in O
figure O
2 O
. O

section 6
id pdf2json/2021.acl-long.6.pdf.json
in O
this O
section O
, O
we O
introduce O
the O
architecture O
of O
our O
rap O
generation O
model O
, O
and O
the O
details O
of O
its O
rhyme O
modeling O
and O
rhythm O
modeling O
. O

section 7
id pdf2json/2021.acl-long.6.pdf.json
figure O
3 O
illustrates O
the O
detailed O
architecture O
of O
our O
rap O
generation O
model O
. O

section 7
id pdf2json/2021.acl-long.6.pdf.json
we O
use O
transformer O
( O
vaswani O
et O
al. O
, O
2017 O
) O
to O
build O
an O
autoregressive O
language O
model O
( O
radford O
et O
al. O
, O
2018 O
, O
2019 O
) O
for O
rap O
generation O
, O
and O
introduce O
several O
new O
designs O
: O
1 O
) O
to O
better O
model O
rhymes O
, O
our O
model O
generates O
a O
sentence O
from O
right O
to O
left O
, O
since O
rhyming O
words O
are O
always O
at O
the O
end O
of O
the O
sentence O
; O
2 O
) O
as O
aforementioned O
, O
rhythms O
are O
critical O
for O
rap O
performance O
, O
so O
we O
insert O
a O
special O
token O
[ O
beat O
] O
for O
explicit O
beat O
modeling O
; O
3 O
) O
unlike O
original O
transformer O
with O
only O
word O
embedding O
and O
positional O
embedding O
, O
we O
add O
multiple O
additional O
embeddings O
to O
better O
model O
rhymes O
and O
rhythms O
. O

section 7
id pdf2json/2021.acl-long.6.pdf.json
next O
, O
we O
introduce O
our O
rhyme O
modeling O
in O
subsection O
4.2 O
and O
rhythm O
modeling O
in O
subsection O
4.3 O
. O

section 8
id pdf2json/2021.acl-long.6.pdf.json
rhymes O
are O
the O
key O
to O
form O
a O
good O
rap O
flow O
. O

section 8
id pdf2json/2021.acl-long.6.pdf.json
in O
deeprapper O
, O
we O
model O
rhymes O
with O
three O
components O
: O
1 O
) O
reverse-order O
language O
model O
; O
2 O
) O
rhyme O
representation O
; O
and O
3 O
) O
rhyme O
constraint O
. O

section 9
id pdf2json/2021.acl-long.6.pdf.json
rhyming O
words O
usually O
occur O
at O
the O
end O
of O
each O
lyric O
sentence O
. O

section 9
id pdf2json/2021.acl-long.6.pdf.json
if O
using O
a O
standard O
autoregressive O
language O
model O
and O
generating O
tokens O
from O
left O
to O
right O
, O
we O
need O
to O
identify O
whether O
the O
current O
generation O
step O
is O
the O
end O
of O
a O
sentence O
, O
which O
decides O
whether O
to O
generate O
rhyming O
words O
to O
be O
consistent O
with O
that O
in O
previous O
sentences O
. O

section 9
id pdf2json/2021.acl-long.6.pdf.json
therefore O
, O
to O
better O
model O
rhymes O
, O
we O
use O
a O
reverse-order O
language O
model O
to O
generate O
sentences O
from O
right O
to O
left O
, O
as O
shown O
in O
figure O
3 O
. O

section 9
id pdf2json/2021.acl-long.6.pdf.json
doing O
so O
we O
can O
easily O
identify O
the O
last O
few O
words O
of O
a O
sentence O
( O
now O
become O
the O
first O
few O
words O
of O
the O
reverse O
sentence O
) O
to O
control O
their O
rhymes O
. O

section 9
id pdf2json/2021.acl-long.6.pdf.json
note O
that O
we O
only O
reverse O
words O
inside O
a O
sentence O
, O
and O
still O
generate O
different O
sentences O
in O
the O
original O
order O
. O

section 9
id pdf2json/2021.acl-long.6.pdf.json
figure O
4 O
compares O
the O
sentences O
in O
left-to-right O
order O
and O
right-to-left O
order O
, O
from O
which O
we O
can O
see O
that O
rhyming O
words O
of O
each O
sentence O
share O
the O
same O
relative O
positions O
( O
offset O
to O
the O
first O
token O
) O
in O
the O
reverse O
order O
, O
and O
are O
easy O
to O
model O
and O
control O
. O

section 10
id pdf2json/2021.acl-long.6.pdf.json
rhyming O
words O
have O
two O
important O
features O
: O
1 O
) O
its O
vowel O
that O
used O
for O
rhyming O
and O
2 O
) O
its O
relative O
position O
in O
a O
sentence O
to O
decide O
the O
correspondence O
between O
the O
rhyming O
words O
in O
consecutive O
sentences O
( O
e.g. O
, O
in O
the O
reverse O
order O
setting O
, O
the O
first/second O
word O
of O
the O
current O
sentence O
should O
be O
rhymed O
with O
the O
first/second O
word O
in O
the O
previous O
sentence O
) O
. O

section 10
id pdf2json/2021.acl-long.6.pdf.json
we O
use O
the O
vowel O
in O
the O
pinyin O
5 O
of O
chinese O
characters O
to O
represent O
their O
rhymes O
. O

section 10
id pdf2json/2021.acl-long.6.pdf.json
to O
this O
end O
, O
we O
5pinyin O
is O
the O
standard O
phoneme O
for O
chinese O
. O

section 10
id pdf2json/2021.acl-long.6.pdf.json
build O
a O
vowel O
dictionary O
f O
( O
· O
) O
to O
identify O
the O
vowel O
of O
each O
word O
. O

section 10
id pdf2json/2021.acl-long.6.pdf.json
as O
shown O
in O
figure O
3 O
, O
we O
add O
an O
additional O
vowel O
embedding O
f O
and O
an O
intra-sentence O
relative O
positional O
embedding O
r O
to O
enhance O
rhyme O
representation O
for O
each O
token O
. O

section 10
id pdf2json/2021.acl-long.6.pdf.json
besides O
, O
to O
better O
identify O
different O
sentences O
, O
we O
introduce O
a O
sentence O
embedding O
s O
to O
differentiate O
different O
sentences O
. O

section 11
id pdf2json/2021.acl-long.6.pdf.json
in O
addition O
to O
reverse-order O
language O
model O
and O
rhyme O
representation O
, O
we O
also O
introduce O
rhyme O
constraint O
to O
improve O
the O
quality O
of O
rhyme O
generation O
in O
inference O
. O

section 11
id pdf2json/2021.acl-long.6.pdf.json
as O
shown O
in O
figure O
4 O
, O
sentences O
in O
rap O
lyrics O
not O
only O
rhyme O
with O
the O
last O
token O
, O
but O
also O
with O
multiple O
consecutive O
tokens O
at O
the O
end O
. O

section 11
id pdf2json/2021.acl-long.6.pdf.json
we O
call O
this O
phenomenon O
as O
n O
-gram O
rhymes O
, O
which O
mean O
the O
current O
sentence O
and O
the O
previous O
sentence O
keep O
the O
same O
rhyme O
for O
the O
last O
n O
consecutive O
tokens O
. O

section 11
id pdf2json/2021.acl-long.6.pdf.json
to O
our O
knowledge O
, O
no O
previous O
work O
has O
investigated O
n O
-gram O
rhymes O
( O
n O
> O
1 O
) O
, O
although O
it O
is O
important O
to O
improve O
rap O
quality O
. O

section 11
id pdf2json/2021.acl-long.6.pdf.json
our O
proposed O
rhyme O
constraint O
enables O
our O
model O
to O
adjust O
the O
probability O
of O
next O
predicted O
token O
to O
further O
encourage O
n O
-gram O
rhyme O
generation O
. O

section 11
id pdf2json/2021.acl-long.6.pdf.json
the O
constraint O
is O
introduced O
as O
follows O
. O

section 11
id pdf2json/2021.acl-long.6.pdf.json
to O
generate O
the O
i-th O
word O
wi O
in O
the O
standard O
inference O
procedure O
, O
we O
usually O
choose O
the O
predicted O
token O
with O
the O
maximum O
probability O
, O
i.e. O
, O
wi O
= O
argmax O
p O
( O
w|w O
< O
i O
; O
θ O
) O
, O
where O
w O
< O
i O
denotes O
the O
words O
before O
position O
i O
in O
the O
reverse O
sentence O
and O
θ O
is O
the O
model O
. O

section 11
id pdf2json/2021.acl-long.6.pdf.json
when O
the O
words O
before O
posi- O
tion O
i O
of O
the O
current O
and O
previous O
sentence O
have O
the O
same O
rhyme O
pattern O
, O
we O
will O
use O
an O
adjusted O
probability O
distribution O
p̃ O
( O
w|w O
< O
i O
; O
θ O
) O
to O
encourage O
the O
i-th O
generated O
word O
to O
be O
rhymed O
according O
to O
the O
i-th O
word O
in O
the O
previous O
sentence O
, O
so O
as O
to O
form O
n O
-gram O
rhymes O
. O

section 11
id pdf2json/2021.acl-long.6.pdf.json
the O
adjusted O
probability O
distribution O
p̃ O
( O
w|w O
< O
i O
; O
θ O
) O
is O
: O
p̃ O
( O
w|w O
< O
i O
; O
θ O
) O
= O
α O
· O
p O
( O
w|w O
< O
i O
; O
θ O
) O
+ O
( O
1− O
α O
) O
· O
π O
( O
w O
) O
( O
2 O
) O
where O
π O
( O
w O
) O
is O
a O
vowel O
check O
function O
and O
α O
is O
a O
hyper-parameter O
to O
balance O
the O
two O
terms O
. O

section 11
id pdf2json/2021.acl-long.6.pdf.json
here O
, O
π O
( O
w O
) O
is O
1 O
if O
the O
predicted O
w O
has O
the O
same O
vowel O
with O
the O
i-th O
token O
in O
the O
previous O
sentence O
, O
otherwise O
0 O
. O

section 11
id pdf2json/2021.acl-long.6.pdf.json
in O
other O
words O
, O
when O
predicting O
i-th O
token O
( O
i O
≤ O
n O
) O
, O
we O
encourage O
our O
model O
to O
pay O
more O
attention O
for O
these O
words O
with O
same O
vowel O
with O
the O
i-th O
token O
in O
the O
previous O
sentence O
. O

section 11
id pdf2json/2021.acl-long.6.pdf.json
in O
this O
way O
, O
the O
model O
tends O
to O
generate O
n O
-gram O
rhymes O
with O
large O
n O
. O

section 12
id pdf2json/2021.acl-long.6.pdf.json
generating O
lyrics O
with O
aligned O
beats O
is O
necessary O
since O
rap O
lyrics O
need O
to O
be O
rapped O
with O
rhythmic O
beats O
. O

section 12
id pdf2json/2021.acl-long.6.pdf.json
therefore O
, O
we O
model O
and O
generate O
rhythmic O
beats O
along O
with O
the O
lyrics O
with O
a O
specific O
symbol O
: O
we O
regard O
beat O
as O
a O
special O
token O
[ O
beat O
] O
and O
insert O
it O
into O
lyric O
sequences O
for O
model O
training O
. O

section 12
id pdf2json/2021.acl-long.6.pdf.json
as O
shown O
in O
figure O
3 O
, O
we O
insert O
[ O
beat O
] O
before O
its O
aligned O
words O
like O
the O
following O
examples O
: O
“ O
我 O
[ O
beat O
] O
抬 O
头 O
[ O
beat O
] O
仰望。天空 O
[ O
beat O
] O
的苍 O
[ O
beat O
] O
茫。 O
” O
. O

section 12
id pdf2json/2021.acl-long.6.pdf.json
rap O
usually O
contains O
different O
beat O
frequencies O
, O
i.e. O
, O
the O
ratios O
between O
the O
total O
number O
of O
words O
and O
the O
total O
number O
of O
beats O
in O
a O
rap O
song O
. O

section 12
id pdf2json/2021.acl-long.6.pdf.json
to O
explicitly O
model O
and O
generate O
rap O
with O
different O
beat O
frequencies O
, O
we O
use O
three O
tokens O
[ O
s O
] O
, O
[ O
m O
] O
, O
and O
[ O
f O
] O
to O
represent O
the O
slow O
, O
medium O
and O
fast O
beat O
frequencies O
and O
add O
the O
corresponding O
tokens O
at O
the O
start O
of O
a O
rap O
song O
for O
training O
and O
inference O
. O

section 12
id pdf2json/2021.acl-long.6.pdf.json
in O
our O
d-rap O
dataset O
, O
the O
distribution O
of O
beat O
frequency O
is O
displayed O
in O
figure O
5 O
. O

section 12
id pdf2json/2021.acl-long.6.pdf.json
according O
to O
the O
distribution O
, O
we O
assign O
[ O
s O
] O
, O
[ O
m O
] O
, O
and O
[ O
f O
] O
to O
songs O
with O
beat O
frequency O
less O
than O
3 O
, O
equal O
to O
3 O
, O
and O
greater O
than O
3 O
respectively O
. O

section 14
id pdf2json/2021.acl-long.6.pdf.json
our O
deeprapper O
model O
is O
built O
on O
the O
autoregressive O
transformer O
decoder O
( O
vaswani O
et O
al. O
, O
2017 O
; O
radford O
et O
al. O
, O
2018 O
, O
2019 O
) O
, O
where O
the O
hidden O
size O
, O
the O
number O
of O
attention O
heads O
and O
the O
number O
of O
transformer O
layers O
are O
set O
as O
768 O
, O
12 O
, O
12 O
. O

section 14
id pdf2json/2021.acl-long.6.pdf.json
the O
dimension O
of O
all O
different O
kinds O
of O
embedding O
in O
deeprapper O
is O
set O
as O
768 O
. O

section 14
id pdf2json/2021.acl-long.6.pdf.json
considering O
there O
is O
no O
existing O
pre-trained O
language O
model O
in O
reverse O
order O
, O
we O
do O
not O
utilize O
any O
pre-trained O
language O
models O
for O
initialization O
. O

section 14
id pdf2json/2021.acl-long.6.pdf.json
instead O
, O
we O
first O
pre-train O
our O
model O
on O
d-lyric O
and O
d-song O
for O
2 O
millions O
steps O
, O
and O
then O
fine-tune O
our O
model O
on O
d-rap O
with O
3k O
steps O
as O
the O
size O
of O
d-rap O
is O
smaller O
than O
our O
pre-training O
corpus O
. O

section 14
id pdf2json/2021.acl-long.6.pdf.json
we O
convert O
each O
song O
to O
a O
sequence O
with O
a O
length O
of O
1024 O
tokens O
by O
cutting O
longer O
sequence O
or O
padding O
shorter O
sequence O
. O

section 14
id pdf2json/2021.acl-long.6.pdf.json
our O
model O
is O
trained O
with O
a O
batch O
size O
of O
8 O
songs O
on O
4 O
nvidia O
titan O
v O
gpus O
. O

section 14
id pdf2json/2021.acl-long.6.pdf.json
we O
use O
adam O
optimizer O
with O
a O
learning O
rate O
of O
0.00015 O
, O
β1 O
= O
0.9 O
, O
β2 O
= O
0.999 O
, O
and O
= O
10−6 O
. O

section 14
id pdf2json/2021.acl-long.6.pdf.json
we O
set O
the O
maximum O
value O
of O
n O
-gram O
rhyme O
as O
3 O
and O
the O
hyperparameter O
α O
in O
equation O
2 O
as O
0.95 O
. O

section 14
id pdf2json/2021.acl-long.6.pdf.json
samples O
are O
generated O
conditioned O
on O
a O
given O
sentence O
in O
reference O
. O

section 15
id pdf2json/2021.acl-long.6.pdf.json
in O
this O
subsection O
, O
we O
introduce O
the O
objective O
and O
subjective O
metrics O
to O
evaluate O
the O
quality O
of O
the O
generated O
raps O
. O

section 15
id pdf2json/2021.acl-long.6.pdf.json
objective O
evaluation O
we O
evaluate O
the O
generated O
raps O
in O
terms O
of O
the O
quality O
of O
language O
, O
rhyme O
and O
rhythm O
. O

section 15
id pdf2json/2021.acl-long.6.pdf.json
we O
choose O
five O
metrics O
to O
evaluate O
our O
model O
: O
1 O
) O
perplexity O
( O
ppl O
) O
, O
a O
standard O
metric O
to O
evaluate O
the O
quality O
of O
a O
language O
model O
; O
2 O
) O
rhyme O
accuracy O
( O
ra O
) O
, O
the O
ratio O
of O
sentences O
that O
have O
correctly O
predicted O
rhymes O
; O
3 O
) O
rhyme O
density O
( O
rd O
) O
, O
the O
longest O
rhyme O
of O
a O
song O
, O
averaged O
over O
all O
songs O
, O
which O
is O
introduced O
by O
malmi O
et O
al O
. O

section 15
id pdf2json/2021.acl-long.6.pdf.json
( O
2016 O
) O
to O
measure O
the O
quality O
of O
rhyming O
fluency O
; O
4 O
) O
combo-n O
, O
the O
maximum O
number O
of O
consecutive O
sentences O
with O
the O
same O
n O
-gram O
rhyme O
in O
a O
rap O
song O
, O
averaged O
over O
all O
songs O
, O
where O
we O
study O
n O
= O
1 O
, O
2 O
, O
3 O
; O
5 O
) O
beat O
accuracy O
( O
ba O
) O
, O
the O
accuracy O
of O
our O
model O
in O
beat O
prediction O
, O
under O
the O
teacherforcing O
mode O
. O

section 15
id pdf2json/2021.acl-long.6.pdf.json
subjective O
evaluation O
similar O
to O
previous O
works O
( O
zhang O
and O
lapata O
, O
2014 O
; O
nikolov O
et O
al. O
, O
2020 O
) O
in O
artistic O
creation O
, O
we O
also O
use O
human O
evaluation O
to O
accurately O
evaluate O
the O
quality O
of O
the O
generated O
raps O
. O

section 15
id pdf2json/2021.acl-long.6.pdf.json
we O
invite O
10 O
participants O
with O
professional O
knowledge O
in O
music O
as O
human O
annotators O
to O
evaluate O
100 O
sampled O
raps O
. O

section 15
id pdf2json/2021.acl-long.6.pdf.json
each O
annotator O
is O
required O
to O
score O
from O
1 O
( O
poor O
) O
to O
5 O
( O
perfect O
) O
on O
the O
following O
perspectives O
: O
1 O
) O
the O
clearness O
of O
the O
theme O
of O
the O
rap O
lyrics O
; O
2 O
) O
the O
fluency O
of O
the O
rap O
lyrics O
; O
3 O
) O
the O
quality O
of O
the O
rhyme O
; O
4 O
) O
the O
diversity O
of O
the O
rhyme O
. O

section 15
id pdf2json/2021.acl-long.6.pdf.json
the O
averaged O
score O
of O
all O
annotators O
on O
all O
sampled O
raps O
is O
used O
as O
the O
evaluation O
score O
for O
each O
perspective O
. O

section 16
id pdf2json/2021.acl-long.6.pdf.json
results O
table O
2 O
shows O
the O
objective O
and O
subjective O
results O
of O
deeprapper O
compared O
with O
two O
baselines O
: O
1 O
) O
baseline O
: O
a O
standard O
autoregressive O
language O
model O
with O
the O
same O
model O
configuration O
with O
deeprapper O
but O
without O
our O
proposed O
rhyme O
and O
rhythm O
modeling O
; O
2 O
) O
baseline O
+ O
pt O
, O
using O
pretraining O
on O
baseline O
. O

section 16
id pdf2json/2021.acl-long.6.pdf.json
we O
have O
several O
observations O
from O
table O
2 O
: O
1 O
) O
deeprapper O
achieves O
better O
perplexity O
, O
rhyme O
accuracy O
and O
rhyme O
density O
than O
the O
two O
baselines O
, O
which O
demonstrates O
the O
advantages O
of O
our O
method O
in O
generating O
high-quality O
rap O
lyrics O
with O
accurate O
and O
diverse O
rhymes O
. O

section 16
id pdf2json/2021.acl-long.6.pdf.json
2 O
) O
deeprapper O
achieves O
better O
scores O
in O
all O
subjective O
metrics O
, O
demonstrating O
that O
deeprapper O
can O
generate O
highquality O
and O
rhyming O
raps O
that O
accord O
with O
human O
taste O
. O

section 16
id pdf2json/2021.acl-long.6.pdf.json
3 O
) O
pre-training O
improves O
the O
performance O
of O
baseline O
in O
both O
objective O
and O
subjective O
metrics O
, O
which O
indicates O
the O
importance O
of O
pre-training O
. O

section 16
id pdf2json/2021.acl-long.6.pdf.json
however O
, O
its O
performance O
is O
still O
worse O
than O
deeprapper O
. O

section 16
id pdf2json/2021.acl-long.6.pdf.json
ablation O
studies O
to O
further O
validate O
the O
necessity O
of O
each O
component O
in O
deeprapper O
, O
we O
conduct O
a O
series O
of O
ablation O
studies O
, O
including O
remov- O
ing O
rhyme O
modeling O
, O
rhythm O
modeling O
and O
pretraining O
, O
respectively O
. O

section 16
id pdf2json/2021.acl-long.6.pdf.json
the O
results O
are O
reported O
in O
table O
3 O
. O

section 16
id pdf2json/2021.acl-long.6.pdf.json
we O
have O
several O
observations O
: O
1 O
) O
removing O
rhyme O
modeling O
affects O
rhyme O
quality O
a O
lot O
as O
it O
results O
in O
a O
dramatic O
drop O
in O
rhyme O
accuracy O
and O
rhyme O
density O
; O
2 O
) O
removing O
each O
specific O
design O
in O
rhyme O
modeling O
( O
i.e. O
, O
ro O
: O
reverse O
order O
language O
model O
, O
ve O
: O
vowel O
embedding O
, O
ipe O
: O
intrasentence O
position O
embedding O
, O
se O
: O
sentence O
embedding O
) O
causes O
worse O
rhyme O
accuracy O
and O
rhyme O
density O
. O

section 16
id pdf2json/2021.acl-long.6.pdf.json
specifically O
, O
while O
removing O
ro O
leads O
to O
a O
better O
ppl O
since O
left-to-right O
order O
can O
be O
more O
easily O
modeled O
than O
right-to-left O
order O
according O
to O
the O
analysis O
in O
wu O
et O
al O
. O

section 16
id pdf2json/2021.acl-long.6.pdf.json
( O
2018 O
) O
, O
it O
causes O
large O
accuracy O
drop O
in O
rhyme O
quality O
. O

section 16
id pdf2json/2021.acl-long.6.pdf.json
3 O
) O
apparently O
, O
deeprapper O
without O
rhythm O
modeling O
can O
not O
produce O
any O
beat O
information O
; O
4 O
) O
deeprapper O
without O
pre-training O
affects O
the O
perplexity O
and O
rhyme O
accuracy O
a O
lot O
, O
however O
, O
obtains O
a O
higher O
rhyme O
density O
. O

section 16
id pdf2json/2021.acl-long.6.pdf.json
the O
reason O
is O
that O
without O
pre-training O
, O
deeprapper O
tends O
to O
copy O
previous O
rhyme O
tokens O
due O
to O
the O
lack O
of O
generalization O
( O
larger O
ppl O
) O
. O

section 16
id pdf2json/2021.acl-long.6.pdf.json
to O
verify O
this O
, O
we O
count O
the O
repetitive O
rate O
of O
rhyming O
words O
and O
found O
that O
the O
rate O
of O
deeprapper O
is O
23.8 O
% O
while O
without O
pre-training O
is O
42.5 O
% O
, O
which O
is O
higher O
than O
using O
pre-training O
. O

section 16
id pdf2json/2021.acl-long.6.pdf.json
the O
above O
results O
verify O
the O
effectiveness O
of O
each O
component O
in O
deeprapper O
. O

section 16
id pdf2json/2021.acl-long.6.pdf.json
n O
-gram O
rhyme O
to O
highlight O
the O
advantage O
of O
deeprapper O
in O
modeling O
n-gram O
rhyme O
, O
we O
use O
combo-n O
to O
measure O
the O
ability O
of O
each O
design O
in O
deeprapper O
to O
model O
n-gram O
rhyme O
. O

section 16
id pdf2json/2021.acl-long.6.pdf.json
the O
results O
are O
reported O
in O
table O
4 O
. O

section 16
id pdf2json/2021.acl-long.6.pdf.json
we O
can O
find O
that O
1 O
) O
the O
model O
without O
rhyme O
modeling O
can O
hardly O
generate O
good O
rhyme O
, O
regardless O
of O
the O
value O
of O
n O
in O
n-gram O
; O
2 O
) O
removing O
rhyme O
constraint O
also O
weakens O
the O
capacity O
of O
generating O
n-gram O
rhyme O
. O

section 16
id pdf2json/2021.acl-long.6.pdf.json
these O
results O
further O
demonstrate O
the O
importance O
of O
our O
rhyme O
modeling O
and O
rhyme O
constraint O
in O
generating O
multiple O
consecutive O
rhymes O
. O

section 16
id pdf2json/2021.acl-long.6.pdf.json
beat O
frequency O
to O
better O
measure O
the O
beat O
quality O
, O
we O
randomly O
generate O
about O
5,000 O
samples O
by O
deeprapper O
and O
deeprapper O
with O
beat O
frequency O
control O
. O

section 16
id pdf2json/2021.acl-long.6.pdf.json
we O
propose O
the O
first O
order O
distribution O
( O
fod O
) O
and O
the O
second O
order O
distribution O
( O
sod O
) O
and O
measure O
the O
distance O
( O
via O
wasserstein O
distance O
( O
vallender O
, O
1974 O
) O
) O
of O
these O
distributions O
between O
the O
generated O
samples O
and O
our O
drap O
dataset O
. O

section 16
id pdf2json/2021.acl-long.6.pdf.json
we O
define O
the O
interval O
of O
the O
current O
[ O
beat O
] O
as O
the O
number O
of O
words O
between O
the O
current O
[ O
beat O
] O
and O
the O
next O
[ O
beat O
] O
. O

section 16
id pdf2json/2021.acl-long.6.pdf.json
therefore O
, O
the O
fod O
is O
defined O
as O
the O
distribution O
of O
the O
interval O
of O
the O
current O
[ O
beat O
] O
. O

section 16
id pdf2json/2021.acl-long.6.pdf.json
similarly O
, O
the O
sod O
is O
defined O
the O
distribution O
of O
the O
difference O
between O
the O
interval O
of O
the O
current O
[ O
beat O
] O
and O
the O
next O
[ O
beat O
] O
. O

section 16
id pdf2json/2021.acl-long.6.pdf.json
the O
results O
of O
the O
distance O
are O
normalized O
into O
[ O
0 O
, O
1 O
] O
and O
are O
reported O
in O
table O
5 O
. O

section 16
id pdf2json/2021.acl-long.6.pdf.json
it O
can O
be O
seen O
that O
deeprapper O
with O
beat O
frequency O
control O
achieves O
better O
performance O
in O
beat O
modeling O
, O
which O
indicates O
the O
importance O
of O
beat O
frequency O
control O
in O
beat O
modeling O
. O

section 16
id pdf2json/2021.acl-long.6.pdf.json
case O
analyses O
on O
generated O
raps O
we O
list O
a O
sample O
case O
from O
our O
generated O
raps O
in O
figure O
6 O
to O
demonstrate O
the O
good O
quality O
of O
the O
raps O
generated O
by O
deeprapper O
. O

section 16
id pdf2json/2021.acl-long.6.pdf.json
the O
sample O
is O
generated O
by O
feeding O
the O
first O
sentence O
of O
the O
example O
in O
figure O
2 O
to O
deeprapper O
. O

section 16
id pdf2json/2021.acl-long.6.pdf.json
as O
we O
can O
see O
, O
the O
generated O
sample O
exhibits O
good O
theme O
, O
fluency O
and O
rhyme O
. O

section 16
id pdf2json/2021.acl-long.6.pdf.json
the O
sample O
is O
a O
rap O
with O
a O
number O
of O
1- O
gram O
, O
2-gram O
, O
3-gram O
, O
and O
even O
4-gram O
rhyme O
. O

section 16
id pdf2json/2021.acl-long.6.pdf.json
the O
generated O
lyrics O
depicts O
the O
fond O
memories O
of O
childhood O
and O
the O
beautiful O
visions O
for O
the O
futures O
. O

section 16
id pdf2json/2021.acl-long.6.pdf.json
we O
also O
provide O
a O
group O
of O
samples O
generated O
with O
beat O
frequency O
control O
. O

section 16
id pdf2json/2021.acl-long.6.pdf.json
to O
save O
space O
, O
we O
put O
them O
and O
the O
translation O
of O
all O
the O
samples O
to O
appendix O
. O

section 16
id pdf2json/2021.acl-long.6.pdf.json
more O
samples O
are O
provided O
in O
https O
: O
//deeprapper.github.io O
. O

section 17
id pdf2json/2021.acl-long.6.pdf.json
in O
this O
paper O
, O
we O
develop O
deeprapper O
, O
a O
novel O
transformer-based O
rap O
generation O
system O
, O
which O
leverages O
rhyme O
modeling O
, O
rhythm O
modeling O
and O
pre-training O
for O
rap O
generation O
. O

section 17
id pdf2json/2021.acl-long.6.pdf.json
considering O
there O
is O
no O
available O
rap O
dataset O
with O
aligned O
rhythmic O
beats O
for O
rhythm O
modeling O
, O
we O
propose O
a O
data O
mining O
pipeline O
to O
mine O
a O
rap O
dataset O
with O
beat-lyric O
alignments O
. O

section 17
id pdf2json/2021.acl-long.6.pdf.json
we O
leverage O
right-to-left O
generation O
, O
rhyme O
representation O
and O
rhyme O
constraint O
to O
better O
model O
rhyme O
and O
encourage O
n-gram O
rhyme O
, O
and O
explicitly O
model O
beat O
information O
by O
insert O
beat O
token O
beside O
the O
corresponding O
word O
in O
the O
lyric O
sequence O
. O

section 17
id pdf2json/2021.acl-long.6.pdf.json
to O
our O
knowledge O
, O
deeprapper O
is O
the O
first O
system O
to O
generate O
rap O
with O
both O
rhymes O
and O
rhythms O
. O

section 17
id pdf2json/2021.acl-long.6.pdf.json
both O
objective O
and O
subjective O
evaluations O
demonstrate O
that O
deeprapper O
generates O
high-quality O
raps O
with O
good O
rhymes O
and O
rhythms O
. O

section 17
id pdf2json/2021.acl-long.6.pdf.json
thanks O
to O
the O
design O
of O
deeprapper O
, O
we O
can O
further O
build O
another O
rap O
singing O
system O
to O
sing O
out O
the O
raps O
according O
to O
the O
rhymes O
and O
rhythms O
, O
which O
we O
leave O
as O
future O
work O
. O

section 17
id pdf2json/2021.acl-long.6.pdf.json
we O
also O
leave O
multilingual O
deeprapper O
as O
future O
work O
. O

section 18
id pdf2json/2021.acl-long.6.pdf.json
we O
would O
like O
to O
acknowledge O
the O
anonymous O
reviewers O
for O
their O
insightful O
comments O
. O

section 18
id pdf2json/2021.acl-long.6.pdf.json
research O
on O
this O
paper O
was O
supported O
by O
hong O
kong O
research O
grants O
council O
under O
grant O
16204920 O
. O

section 18
id pdf2json/2021.acl-long.6.pdf.json
ethical O
considerations O
the O
proposed O
framework O
can O
be O
considered O
a O
novel O
language O
model O
for O
rap O
generation O
in O
automatic O
artistic O
creation O
. O

section 18
id pdf2json/2021.acl-long.6.pdf.json
specifically O
, O
the O
proposed O
framework O
has O
been O
configured O
with O
novel O
rhyme O
modeling O
as O
rhyme O
is O
quite O
important O
in O
music O
genres O
. O

section 18
id pdf2json/2021.acl-long.6.pdf.json
therefore O
, O
our O
proposed O
framework O
is O
also O
beneficial O
for O
generating O
other O
music O
genres O
. O

section 18
id pdf2json/2021.acl-long.6.pdf.json
on O
the O
other O
hand O
, O
although O
we O
collect O
large-scale O
lyric O
data O
for O
pre-training O
, O
it O
still O
can O
not O
fully O
utilize O
the O
potential O
of O
pre-training O
. O

section 18
id pdf2json/2021.acl-long.6.pdf.json
in O
the O
future O
, O
we O
expect O
to O
employ O
more O
large-scale O
data O
in O
the O
open O
domain O
plus O
the O
music O
domain O
for O
pre-training O
to O
improve O
the O
capacity O
of O
the O
language O
model O
. O

section 18
id pdf2json/2021.acl-long.6.pdf.json
in O
addition O
, O
our O
training O
datasets O
may O
have O
biases O
, O
which O
may O
bring O
some O
potential O
risks O
of O
model O
bias O
. O

section 18
id pdf2json/2021.acl-long.6.pdf.json
hence O
, O
we O
encourage O
future O
works O
to O
study O
how O
to O
apply O
other O
techniques O
in O
mitigating O
similar O
problems O
in O
our O
framework O
. O

section 19
id pdf2json/2021.acl-long.6.pdf.json
we O
provide O
a O
comparison O
between O
deeprapper O
and O
ghosterwriter O
( O
potash O
et O
al. O
, O
2015 O
) O
in O
table O
6 O
. O

section 19
id pdf2json/2021.acl-long.6.pdf.json
the O
results O
show O
that O
both O
deeprapper O
and O
baselines O
outperform O
ghosterwriter O
in O
terms O
of O
ppl O
, O
rhyme O
accuracy O
, O
and O
rhyme O
density O
on O
rap O
generation O
tasks O
. O

section 20
id pdf2json/2021.acl-long.6.pdf.json
fast O
figure O
7 O
provides O
a O
rap O
generated O
by O
deeprapper O
with O
fast O
beat O
frequency O
, O
which O
the O
frequency O
is O
4.3 O
. O

section 20
id pdf2json/2021.acl-long.6.pdf.json
the O
rap O
express O
ones O
beat O
wished O
to O
his/her O
lover O
. O

section 20
id pdf2json/2021.acl-long.6.pdf.json
the O
following O
is O
the O
translation O
of O
texts O
in O
figure O
7 O
. O

section 20
id pdf2json/2021.acl-long.6.pdf.json
我长大的地方像一个简朴的寨 O
the O
place O
where O
i O
grew O
up O
is O
like O
a O
simple O
village O
遥望远方镜子里的海 O
looking O
into O
the O
distance O
, O
the O
sea O
is O
in O
the O
mirror O
看见理想很实际的说一句拜拜 O
see O
my O
dream O
and O
say O
goodbye O
这世界在我眼里和千里之外 O
the O
world O
is O
sometimes O
in O
my O
eyes O
and O
sometimes O
thousands O
of O
miles O
away O
穿过河流沙漠和人海 O
across O
rivers O
, O
deserts O
and O
crowds O
一山万水渡过岁月冲洗我的爱 O
a O
mountain O
and O
a O
million O
rivers O
wash O
my O
love O
through O
the O
years O
和那亲爱的你一起去踩一路的尘埃 O
step O
on O
the O
dust O
all O
the O
way O
with O
dear O
you O
我一生中最重要的你我壹一都在 O
the O
most O
important O
you O
in O
my O
life O
, O
i O
’ O
ll O
always O
by O
your O
side O
点点轻地落在你那冰封的大门外 O
little O
by O
little O
, O
it O
falls O
outside O
your O
frozen O
gate O
我在默默的祝福你 O
i O
am O
blessing O
you O
silently O
我在无数个夜里默默地祝福你 O
i O
have O
secretly O
blessed O
you O
for O
countless O
nights O
远远地保护着你我的爱 O
protecting O
you O
and O
my O
love O
from O
a O
distance O
medium O
figure O
8 O
provides O
a O
rap O
generated O
by O
deeprapper O
with O
medium O
beat O
frequency O
, O
which O
the O
frequency O
is O
2.6 O
. O

section 20
id pdf2json/2021.acl-long.6.pdf.json
the O
rap O
praises O
the O
times O
we O
live O
in O
. O

section 20
id pdf2json/2021.acl-long.6.pdf.json
the O
following O
is O
the O
translation O
of O
texts O
in O
figure O
8 O
. O

section 20
id pdf2json/2021.acl-long.6.pdf.json
我长大的地方像一个简朴的寨 O
the O
place O
where O
i O
grew O
up O
is O
like O
a O
simple O
village O
简朴的看着简朴的海 O
simply O
looking O
at O
the O
simple O
sea O
爸爸拿着一个简朴的麦 O
dad O
holding O
a O
simple O
wheat O
有人真实的努力就有人背负着爱 O
someone O
takes O
effort O
, O
somebody O
is O
carrying O
love O
那生活的美好让人人们热爱 O
the O
beauty O
of O
life O
makes O
people O
love O
这世界的美好纯粹是意外 O
the O
beauty O
of O
this O
world O
is O
pure O
accident O
而我长大的地方是个简朴的寨 O
and O
the O
place O
where O
i O
grew O
up O
is O
a O
simple O
village O
让我们在这里开心的喝彩 O
let O
’ O
s O
cheer O
happily O
here O
伟大母亲怀抱着爱 O
great O
mother O
embrace O
love O
看着幸福的人们敞开淳朴的怀 O
watching O
happy O
people O
open O
their O
simple O
arms O
我们最美好的这个快乐海 O
we O
are O
in O
the O
most O
beautiful O
happy O
sea O
唱出我们的时代 O
sing O
our O
time O
slow O
figure O
9 O
provides O
a O
rap O
generated O
by O
deeprapper O
with O
slow O
beat O
frequency O
, O
where O
the O
frequency O
is O
2.1 O
. O

section 20
id pdf2json/2021.acl-long.6.pdf.json
the O
rap O
express O
ones O
relief O
from O
life O
. O

section 20
id pdf2json/2021.acl-long.6.pdf.json
the O
following O
is O
the O
translation O
of O
texts O
in O
figure O
9 O
. O

section 20
id pdf2json/2021.acl-long.6.pdf.json
我长大的地方像一个简朴的寨 O
the O
place O
where O
i O
grew O
up O
is O
like O
a O
simple O
village O
快到有一天看见了父母的爱 O
almost O
one O
day O
i O
saw O
the O
love O
of O
my O
parents O
我的时间你实在不用去考虑自己多坏 O
you O
don O
’ O
t O
have O
to O
think O
about O
how O
bad O
you O
are O
in O
my O
time O
当我脚步在外从没过的这么可爱 O
i O
’ O
ve O
never O
been O
so O
cute O
when O
i O
’ O
m O
out O
我只是一次旅行 O
i O
’ O
m O
just O
a O
trip O
to O
your O
life O
你现在的校服我也想换 O
i O
want O
to O
change O
your O
current O
school O
uniform O
我曾经追你 O
i O
used O
to O
chase O
you O
你的运气也不摔 O
your O
luck O
won O
’ O
t O
fall O
毕竟上次 O
after O
all O
last O
time O
你爱的姑娘你也想看 O
you O
want O
to O
see O
the O
girl O
you O
love O
她们和你一定要分离 O
they O
must O
be O
separated O
from O
you O
你就这样子一笑而去 O
you O
just O
leave O
with O
a O
smile O

section 21
id pdf2json/2021.acl-long.6.pdf.json
words O
in O
red O
are O
rhymes O
. O

section 21
id pdf2json/2021.acl-long.6.pdf.json
translation O
of O
chinese O
in O
figure O
2 O
我长大的地方像一个简朴的寨 O
the O
place O
where O
i O
grew O
up O
is O
like O
a O
simple O
village O
简朴的人吃着最简朴的菜 O
simple O
people O
eat O
the O
simplest O
dishes O
简朴的话包含着简朴的爱 O
simple O
words O
contain O
simple O
love O
简朴的道理传给一代又一代 O
simple O
principles O
are O
passed O
on O
from O
generation O
to O
generation O
难以忘记的画面不需相机 O
unforgettable O
picture O
do O
not O
need O
camera O
to O
capture O
难以再闻到的是巷子里的香气 O
what O
is O
hard O
to O
smell O
is O
the O
aroma O
in O
the O
alley O
常常想起外婆家的躺椅 O
i O
often O
think O
of O
grandma O
’ O
s O
recliner O
最珍贵的椰奶往往藏在床底 O
the O
most O
precious O
coconut O
milk O
is O
often O
hidden O
under O
the O
bed O
先填饱肚子再想那些背不下的书 O
fill O
your O
stomach O
first O
, O
then O
think O
about O
the O
books O
that O
you O
can O
’ O
t O
remember O
外婆做的火腿肠比外面炸的酥 O
grandma O
’ O
s O
ham O
sausage O
is O
crispier O
than O
fried O
outside O
油烟的香味弥漫不那么大的屋 O
the O
smell O
of O
lampblack O
pervades O
the O
not O
so O
big O
house O
外婆的故事总会让大人笑着哭 O
grandma O
’ O
s O
stories O
always O
make O
adults O
laugh O
and O
cry O
translation O
of O
chinese O
in O
figure O
3 O
我抬头仰望。天空的苍茫。 O
i O
looked O
up O
. O

section 21
id pdf2json/2021.acl-long.6.pdf.json
the O
sky O
is O
vast O
. O

section 21
id pdf2json/2021.acl-long.6.pdf.json
translation O
of O
chinese O
in O
figure O
4 O
是这座城市的气象 O
it O
is O
the O
weather O
of O
this O
city O
让你感受生命的力量 O
makes O
you O
feel O
the O
power O
of O
living O
translation O
of O
chinese O
in O
figure O
6 O
我长大的地方像一个简朴的寨 O
the O
place O
where O
i O
grew O
up O
is O
like O
a O
simple O
village O
公里也许大的远方简直是个小小的寨 O
a O
small O
far O
away O
village O
偶尔穿件毛衣那样子很可爱 O
it O
is O
cute O
to O
wear O
a O
sweater O
occasionally O
远方可单纯的姑娘还是单纯的孩 O
is O
it O
a O
simple O
girl O
or O
a O
simple O
child O
far O
away O
是放不下的故事大声的喝彩 O
cheers O
loudly O
for O
the O
unforgettable O
story O
像快乐的小孩莫名的敞着怀 O
happy O
kids O
like O
happy O
kids O
几百公里我们相约到未来 O
through O
hundreds O
of O
kilometers O
, O
we O
meet O
in O
the O
future O
在那无尽的沙漠和海 O
in O
the O
endless O
desert O
and O
sea O
看着温暖花开 O
watching O
the O
warm O
flowers O
bloom O
花一样的在 O
like O
flowers O
be O
there O
写动人的天籁 O
write O
moving O
sounds O
of O
nature跟着自由自在 O
feeling O
the O
freedom O
消沉在那片海 O
sometimes O
depressed O
in O
the O
sea O
不懂儿时的他们不懂什么是爱 O
i O
don O
’ O
t O
understand O
their O
childish O
. O

section 21
id pdf2json/2021.acl-long.6.pdf.json
i O
don O
’ O
t O
know O
what O
love O
is O
到现在你看来 O
till O
now O
you O
see O
最真的迷彩 O
it O
is O
the O
most O
true O
fantasy O

section TITLE
id pdf2json/2021.acl-long.517.pdf.json
on O
the O
efficacy O
of O
adversarial O
data O
collection O
for O
question O
answering O
: O
results O
from O
a O
large-scale O
randomized O
study O

section ABSTRACT
id pdf2json/2021.acl-long.517.pdf.json
in O
adversarial O
data O
collection O
( O
adc O
) O
, O
a O
human O
workforce O
interacts O
with O
a O
model O
in O
real O
time O
, O
attempting O
to O
produce O
examples O
that O
elicit O
incorrect O
predictions O
. O

section ABSTRACT
id pdf2json/2021.acl-long.517.pdf.json
researchers O
hope O
that O
models O
trained O
on O
these O
more O
challenging O
datasets O
will O
rely O
less O
on O
superficial O
patterns O
, O
and O
thus O
be O
less O
brittle O
. O

section ABSTRACT
id pdf2json/2021.acl-long.517.pdf.json
however O
, O
despite O
adc O
’ O
s O
intuitive O
appeal O
, O
it O
remains O
unclear O
when O
training O
on O
adversarial O
datasets O
produces O
more O
robust O
models O
. O

section ABSTRACT
id pdf2json/2021.acl-long.517.pdf.json
in O
this O
paper O
, O
we O
conduct O
a O
large-scale O
controlled O
study O
focused O
on O
question O
answering O
, O
assigning O
workers O
at O
random O
to O
compose O
questions O
either O
( O
i O
) O
adversarially O
( O
with O
a O
model O
in O
the O
loop O
) O
; O
or O
( O
ii O
) O
in O
the O
standard O
fashion O
( O
without O
a O
model O
) O
. O

section ABSTRACT
id pdf2json/2021.acl-long.517.pdf.json
across O
a O
variety O
of O
models O
and O
datasets O
, O
we O
find O
that O
models O
trained O
on O
adversarial O
data O
usually O
perform O
better O
on O
other O
adversarial O
datasets O
but O
worse O
on O
a O
diverse O
collection O
of O
out-of-domain O
evaluation O
sets O
. O

section ABSTRACT
id pdf2json/2021.acl-long.517.pdf.json
finally O
, O
we O
provide O
a O
qualitative O
analysis O
of O
adversarial O
( O
vs O
standard O
) O
data O
, O
identifying O
key O
differences O
and O
offering O
guidance O
for O
future O
research.1 O

section 0
id pdf2json/2021.acl-long.517.pdf.json
proceedings O
of O
the O
59th O
annual O
meeting O
of O
the O
association O
for O
computational O
linguistics O
and O
the O
11th O
international O
joint O
conference O
on O
natural O
language O
processing O
, O
pages O
6618–6633 O
august O
1–6 O
, O
2021 O
. O

section 0
id pdf2json/2021.acl-long.517.pdf.json
©2021 O
association O
for O
computational O
linguistics O
6618 O

section 1
id pdf2json/2021.acl-long.517.pdf.json
across O
such O
diverse O
natural O
language O
processing O
( O
nlp O
) O
tasks O
as O
natural O
language O
inference O
( O
nli O
; O
poliak O
et O
al. O
, O
2018 O
; O
gururangan O
et O
al. O
, O
2018 O
) O
, O
question O
answering O
( O
qa O
; O
kaushik O
and O
lipton O
, O
2018 O
) O
, O
and O
sentiment O
analysis O
( O
kaushik O
et O
al. O
, O
2020 O
) O
, O
researchers O
have O
discovered O
that O
models O
can O
succeed O
on O
popular O
benchmarks O
by O
exploiting O
spurious O
associations O
that O
characterize O
a O
particular O
dataset O
but O
do O
not O
hold O
more O
widely O
. O

section 1
id pdf2json/2021.acl-long.517.pdf.json
despite O
performing O
well O
on O
independent O
and O
identically O
distributed O
( O
i.i.d O
. O
) O

section 1
id pdf2json/2021.acl-long.517.pdf.json
data O
, O
these O
models O
are O
liable O
under O
plausible O
domain O
shifts O
. O

section 1
id pdf2json/2021.acl-long.517.pdf.json
with O
the O
goal O
of O
providing O
more O
challenging O
benchmarks O
that O
require O
this O
stronger O
form O
of O
generalization O
, O
an O
emerging O
line O
of O
research O
has O
1data O
collected O
during O
this O
study O
is O
publicly O
available O
at O
https O
: O
//github.com/facebookresearch/aqa-study O
. O

section 1
id pdf2json/2021.acl-long.517.pdf.json
investigated O
adversarial O
data O
collection O
( O
adc O
) O
, O
a O
scheme O
in O
which O
a O
worker O
interacts O
with O
a O
model O
( O
in O
real O
time O
) O
, O
attempting O
to O
produce O
examples O
that O
elicit O
incorrect O
predictions O
( O
e.g. O
, O
dua O
et O
al. O
, O
2019 O
; O
nie O
et O
al. O
, O
2020 O
) O
. O

section 1
id pdf2json/2021.acl-long.517.pdf.json
the O
hope O
is O
that O
by O
identifying O
parts O
of O
the O
input O
domain O
where O
the O
model O
fails O
one O
might O
make O
the O
model O
more O
robust O
. O

section 1
id pdf2json/2021.acl-long.517.pdf.json
researchers O
have O
shown O
that O
models O
trained O
on O
adc O
perform O
better O
on O
such O
adversarially O
collected O
data O
and O
that O
with O
successive O
rounds O
of O
adc O
, O
crowdworkers O
are O
less O
able O
to O
fool O
the O
models O
( O
dinan O
et O
al. O
, O
2019 O
) O
. O

section 1
id pdf2json/2021.acl-long.517.pdf.json
while O
adversarial O
data O
may O
indeed O
provide O
more O
challenging O
benchmarks O
, O
the O
process O
and O
its O
actual O
benefits O
vis-a-vis O
tasks O
of O
interest O
remain O
poorly O
understood O
, O
raising O
several O
key O
questions O
: O
( O
i O
) O
do O
the O
resulting O
models O
typically O
generalize O
better O
out O
of O
distribution O
compared O
to O
standard O
data O
collection O
( O
sdc O
) O
? O

section 1
id pdf2json/2021.acl-long.517.pdf.json
; O
( O
ii O
) O
how O
much O
can O
differences O
between O
adc O
and O
sdc O
be O
attributed O
to O
the O
way O
workers O
behave O
when O
attempting O
to O
fool O
models O
, O
regardless O
of O
whether O
they O
are O
successful O
? O

section 1
id pdf2json/2021.acl-long.517.pdf.json
and O
( O
iii O
) O
what O
is O
the O
impact O
of O
training O
models O
on O
adversarial O
data O
only O
, O
versus O
using O
it O
as O
a O
data O
augmentation O
strategy O
? O

section 1
id pdf2json/2021.acl-long.517.pdf.json
in O
this O
paper O
, O
we O
conduct O
a O
large-scale O
randomized O
controlled O
study O
to O
address O
these O
questions O
. O

section 1
id pdf2json/2021.acl-long.517.pdf.json
focusing O
our O
study O
on O
span-based O
question O
answering O
and O
a O
variant O
of O
the O
natural O
questions O
dataset O
( O
nq O
; O
lee O
et O
al. O
, O
2019 O
; O
karpukhin O
et O
al. O
, O
2020 O
) O
, O
we O
work O
with O
two O
popular O
pretrained O
transformer O
architectures—bertlarge O
( O
devlin O
et O
al. O
, O
2019 O
) O
and O
electralarge O
( O
clark O
et O
al. O
, O
2020 O
) O
— O
each O
fine-tuned O
on O
23.1k O
examples O
. O

section 1
id pdf2json/2021.acl-long.517.pdf.json
to O
eliminate O
confounding O
factors O
when O
assessing O
the O
impact O
of O
adc O
, O
we O
randomly O
assign O
the O
crowdworkers O
tasked O
with O
generating O
questions O
to O
one O
of O
three O
groups O
: O
( O
i O
) O
with O
an O
incentive O
to O
fool O
the O
bert O
model O
; O
( O
ii O
) O
with O
an O
incentive O
to O
fool O
the O
electra O
model O
; O
and O
( O
iii O
) O
a O
standard O
, O
non-adversarial O
setting O
( O
no O
model O
in O
the O
loop O
) O
. O

section 1
id pdf2json/2021.acl-long.517.pdf.json
the O
pool O
of O
contexts O
is O
the O
same O
for O
each O
group O
and O
each O
worker O
is O
asked O
to O
generate O
five O
questions O
for O
each O
context O
that O
they O
see O
. O

section 1
id pdf2json/2021.acl-long.517.pdf.json
workers O
are O
shown O
similar O
instructions O
( O
with O
minimal O
changes O
) O
, O
and O
paid O
the O
same O
base O
amount O
. O

section 1
id pdf2json/2021.acl-long.517.pdf.json
we O
fine-tune O
three O
models O
( O
bert O
, O
roberta O
, O
and O
electra O
) O
on O
resulting O
datasets O
and O
evaluate O
them O
on O
held-out O
test O
sets O
, O
adversarial O
test O
sets O
from O
prior O
work O
( O
bartolo O
et O
al. O
, O
2020 O
) O
, O
and O
12 O
mrqa O
( O
fisch O
et O
al. O
, O
2019 O
) O
datasets O
. O

section 1
id pdf2json/2021.acl-long.517.pdf.json
for O
all O
models O
, O
we O
find O
that O
while O
fine-tuning O
on O
adversarial O
data O
usually O
leads O
to O
better O
performance O
on O
( O
previously O
collected O
) O
adversarial O
data O
, O
it O
typically O
leads O
to O
worse O
performance O
on O
a O
large O
, O
diverse O
collection O
of O
out-of-domain O
datasets O
( O
compared O
to O
fine-tuning O
on O
standard O
data O
) O
. O

section 1
id pdf2json/2021.acl-long.517.pdf.json
we O
observe O
a O
similar O
pattern O
when O
augmenting O
the O
existing O
dataset O
with O
the O
adversarial O
data O
. O

section 1
id pdf2json/2021.acl-long.517.pdf.json
results O
on O
an O
extensive O
collection O
of O
out-of-domain O
evaluation O
sets O
suggest O
that O
adc O
training O
data O
does O
not O
offer O
clear O
benefits O
vis-à-vis O
robustness O
under O
distribution O
shift O
. O

section 1
id pdf2json/2021.acl-long.517.pdf.json
to O
study O
the O
differences O
between O
adversarial O
and O
standard O
data O
, O
we O
perform O
a O
qualitative O
analysis O
, O
categorizing O
questions O
based O
on O
a O
taxonomy O
( O
hovy O
et O
al. O
, O
2000 O
) O
. O

section 1
id pdf2json/2021.acl-long.517.pdf.json
we O
notice O
that O
more O
questions O
in O
the O
adc O
dataset O
require O
numerical O
reasoning O
compared O
to O
the O
sdc O
sample O
. O

section 1
id pdf2json/2021.acl-long.517.pdf.json
these O
qualitative O
insights O
may O
offer O
additional O
guidance O
to O
future O
researchers O
. O

section 2
id pdf2json/2021.acl-long.517.pdf.json
in O
an O
early O
example O
of O
model-in-the-loop O
data O
collection O
, O
zweig O
and O
burges O
( O
2012 O
) O
use O
n-gram O
lan- O
guage O
models O
to O
suggest O
candidate O
incorrect O
answers O
for O
a O
fill-in-the-blank O
task O
. O

section 2
id pdf2json/2021.acl-long.517.pdf.json
richardson O
et O
al O
. O

section 2
id pdf2json/2021.acl-long.517.pdf.json
( O
2013 O
) O
suggested O
adc O
for O
qa O
as O
proposed O
future O
work O
, O
speculating O
that O
it O
might O
challenge O
state-ofthe-art O
models O
. O

section 2
id pdf2json/2021.acl-long.517.pdf.json
in O
the O
build O
it O
break O
it O
, O
the O
language O
edition O
shared O
task O
( O
ettinger O
et O
al. O
, O
2017 O
) O
, O
teams O
worked O
as O
builders O
( O
training O
models O
) O
and O
breakers O
( O
creating O
challenging O
examples O
for O
subsequent O
training O
) O
for O
sentiment O
analysis O
and O
qa-srl O
. O

section 2
id pdf2json/2021.acl-long.517.pdf.json
research O
on O
adc O
has O
picked O
up O
recently O
, O
with O
chen O
et O
al O
. O

section 2
id pdf2json/2021.acl-long.517.pdf.json
( O
2019 O
) O
tasking O
crowdworkers O
to O
construct O
multiple-choice O
questions O
to O
fool O
a O
bert O
model O
and O
wallace O
et O
al O
. O

section 2
id pdf2json/2021.acl-long.517.pdf.json
( O
2019 O
) O
employing O
quizbowl O
community O
members O
to O
write O
jeopardystyle O
questions O
to O
compete O
against O
qa O
models O
. O

section 2
id pdf2json/2021.acl-long.517.pdf.json
zhang O
et O
al O
. O

section 2
id pdf2json/2021.acl-long.517.pdf.json
( O
2018 O
) O
automatically O
generated O
questions O
from O
news O
articles O
, O
keeping O
only O
those O
questions O
that O
were O
incorrectly O
answered O
by O
a O
qa O
model O
. O

section 2
id pdf2json/2021.acl-long.517.pdf.json
dua O
et O
al O
. O

section 2
id pdf2json/2021.acl-long.517.pdf.json
( O
2019 O
) O
and O
dasigi O
et O
al O
. O

section 2
id pdf2json/2021.acl-long.517.pdf.json
( O
2019 O
) O
required O
crowdworkers O
to O
submit O
only O
questions O
that O
qa O
models O
answered O
incorrectly O
. O

section 2
id pdf2json/2021.acl-long.517.pdf.json
to O
construct O
fever O
2.0 O
( O
thorne O
et O
al. O
, O
2019 O
) O
, O
crowdworkers O
were O
required O
to O
fool O
a O
fact-verification O
system O
trained O
on O
the O
fever O
( O
thorne O
et O
al. O
, O
2018 O
) O
dataset O
. O

section 2
id pdf2json/2021.acl-long.517.pdf.json
some O
works O
explore O
adc O
over O
multiple O
rounds O
, O
with O
adversarial O
data O
from O
one O
round O
used O
to O
train O
models O
in O
the O
subsequent O
round O
. O

section 2
id pdf2json/2021.acl-long.517.pdf.json
yang O
et O
al O
. O

section 2
id pdf2json/2021.acl-long.517.pdf.json
( O
2018b O
) O
ask O
workers O
to O
generate O
challenging O
datasets O
working O
first O
as O
adversaries O
and O
later O
as O
collaborators O
. O

section 2
id pdf2json/2021.acl-long.517.pdf.json
dinan O
et O
al O
. O

section 2
id pdf2json/2021.acl-long.517.pdf.json
( O
2019 O
) O
build O
on O
their O
work O
, O
employing O
adc O
to O
address O
offensive O
lan- O
guage O
identification O
. O

section 2
id pdf2json/2021.acl-long.517.pdf.json
they O
find O
that O
over O
successive O
rounds O
of O
training O
, O
models O
trained O
on O
adc O
data O
are O
harder O
for O
humans O
to O
fool O
than O
those O
trained O
on O
standard O
data O
. O

section 2
id pdf2json/2021.acl-long.517.pdf.json
nie O
et O
al O
. O

section 2
id pdf2json/2021.acl-long.517.pdf.json
( O
2020 O
) O
applied O
adc O
for O
an O
nli O
task O
over O
three O
rounds O
, O
finding O
that O
training O
for O
more O
rounds O
improves O
model O
performance O
on O
adversarial O
data O
, O
and O
observing O
improvements O
on O
the O
original O
evaluations O
set O
when O
training O
on O
a O
mixture O
of O
original O
and O
adversarial O
training O
data O
. O

section 2
id pdf2json/2021.acl-long.517.pdf.json
williams O
et O
al O
. O

section 2
id pdf2json/2021.acl-long.517.pdf.json
( O
2020 O
) O
conducted O
an O
error O
analysis O
of O
model O
predictions O
on O
the O
datasets O
collected O
by O
nie O
et O
al O
. O

section 2
id pdf2json/2021.acl-long.517.pdf.json
( O
2020 O
) O
. O

section 2
id pdf2json/2021.acl-long.517.pdf.json
bartolo O
et O
al O
. O

section 2
id pdf2json/2021.acl-long.517.pdf.json
( O
2020 O
) O
studied O
the O
empirical O
efficacy O
of O
adc O
for O
squad O
( O
rajpurkar O
et O
al. O
, O
2016 O
) O
, O
observing O
improved O
performance O
on O
adversarial O
test O
sets O
but O
noting O
that O
trends O
vary O
depending O
on O
the O
models O
used O
to O
collect O
data O
and O
to O
train O
. O

section 2
id pdf2json/2021.acl-long.517.pdf.json
previously O
, O
lowell O
et O
al O
. O

section 2
id pdf2json/2021.acl-long.517.pdf.json
( O
2019 O
) O
observed O
similar O
issues O
in O
active O
learning O
, O
when O
the O
models O
used O
to O
acquire O
data O
and O
for O
subsequent O
training O
differ O
. O

section 2
id pdf2json/2021.acl-long.517.pdf.json
yang O
et O
al O
. O

section 2
id pdf2json/2021.acl-long.517.pdf.json
( O
2018a O
) O
; O
zellers O
et O
al O
. O

section 2
id pdf2json/2021.acl-long.517.pdf.json
( O
2018 O
, O
2019 O
) O
first O
collect O
datasets O
and O
then O
filter O
examples O
based O
on O
predictions O
from O
a O
model O
. O

section 2
id pdf2json/2021.acl-long.517.pdf.json
paperno O
et O
al O
. O

section 2
id pdf2json/2021.acl-long.517.pdf.json
( O
2016 O
) O
apply O
a O
similar O
procedure O
to O
generate O
a O
language O
modeling O
dataset O
( O
lambada O
) O
. O

section 2
id pdf2json/2021.acl-long.517.pdf.json
kaushik O
et O
al O
. O

section 2
id pdf2json/2021.acl-long.517.pdf.json
( O
2020 O
, O
2021 O
) O
collect O
counterfactually O
augmented O
data O
( O
cad O
) O
by O
asking O
crowdworkers O
to O
edit O
existing O
documents O
to O
make O
counterfactual O
labels O
applicable O
, O
showing O
that O
models O
trained O
on O
cad O
generalize O
better O
out-of-domain O
. O

section 2
id pdf2json/2021.acl-long.517.pdf.json
absent O
further O
assumptions O
, O
learning O
classifiers O
robust O
to O
distribution O
shift O
is O
impossible O
( O
bendavid O
et O
al. O
, O
2010 O
) O
. O

section 2
id pdf2json/2021.acl-long.517.pdf.json
while O
few O
nlp O
papers O
on O
the O
matter O
make O
their O
assumptions O
explicit O
, O
they O
typically O
proceed O
under O
the O
implicit O
assumptions O
that O
the O
labeling O
function O
is O
deterministic O
( O
there O
is O
one O
right O
answer O
) O
, O
and O
that O
covariate O
shift O
( O
shimodaira O
, O
2000 O
) O
applies O
( O
the O
labeling O
function O
p O
( O
y|x O
) O
is O
invariant O
across O
domains O
) O
. O

section 2
id pdf2json/2021.acl-long.517.pdf.json
note O
that O
neither O
condition O
is O
generally O
true O
of O
prediction O
problems O
. O

section 2
id pdf2json/2021.acl-long.517.pdf.json
for O
example O
, O
faced O
with O
label O
shift O
( O
schölkopf O
et O
al. O
, O
2012 O
; O
lipton O
et O
al. O
, O
2018 O
) O
p O
( O
y|x O
) O
can O
change O
across O
distributions O
, O
requiring O
one O
to O
adapt O
the O
predictor O
to O
each O
environment O
. O

section 3
id pdf2json/2021.acl-long.517.pdf.json
in O
our O
study O
of O
adc O
for O
qa O
, O
each O
crowdworker O
is O
shown O
a O
short O
passage O
and O
asked O
to O
create O
5 O
questions O
and O
highlight O
answers O
( O
spans O
in O
the O
passage O
, O
see O
fig O
. O

section 3
id pdf2json/2021.acl-long.517.pdf.json
1 O
) O
. O

section 3
id pdf2json/2021.acl-long.517.pdf.json
we O
provide O
all O
workers O
with O
the O
same O
base O
pay O
and O
for O
those O
assigned O
to O
adc O
, O
pay O
out O
an O
additional O
bonus O
for O
each O
question O
that O
fools O
the O
qa O
model O
. O

section 3
id pdf2json/2021.acl-long.517.pdf.json
finally O
, O
we O
field O
a O
different O
set O
of O
workers O
to O
validate O
the O
generated O
examples O
. O

section 3
id pdf2json/2021.acl-long.517.pdf.json
context O
passages O
for O
context O
passages O
, O
we O
use O
the O
first O
100 O
words O
of O
wikipedia O
articles O
. O

section 3
id pdf2json/2021.acl-long.517.pdf.json
truncating O
the O
articles O
keeps O
the O
task O
of O
generating O
questions O
from O
growing O
unwieldy O
. O

section 3
id pdf2json/2021.acl-long.517.pdf.json
these O
segments O
typically O
contain O
an O
overview O
, O
providing O
ample O
material O
for O
factoid O
questions O
. O

section 3
id pdf2json/2021.acl-long.517.pdf.json
we O
restrict O
the O
pool O
of O
candidate O
contexts O
by O
leveraging O
a O
variant O
of O
the O
natural O
questions O
dataset O
( O
kwiatkowski O
et O
al. O
, O
2019 O
; O
lee O
et O
al. O
, O
2019 O
) O
. O

section 3
id pdf2json/2021.acl-long.517.pdf.json
we O
first O
keep O
only O
a O
subset O
of O
23.1k O
question/answer O
pairs O
for O
which O
the O
context O
passages O
are O
the O
first O
100 O
words O
of O
wikipedia O
articles2 O
. O

section 3
id pdf2json/2021.acl-long.517.pdf.json
from O
these O
passages O
, O
we O
sample O
10k O
at O
random O
for O
our O
study O
. O

section 3
id pdf2json/2021.acl-long.517.pdf.json
models O
in O
the O
loop O
we O
use O
bertlarge O
( O
devlin O
et O
al. O
, O
2019 O
) O
and O
electralarge O
( O
clark O
et O
al. O
, O
2020 O
) O
models O
as O
our O
adversarial O
models O
in O
the O
loop O
, O
using O
the O
implementations O
provided O
by O
wolf O
et O
al O
. O

section 3
id pdf2json/2021.acl-long.517.pdf.json
( O
2020 O
) O
. O

section 3
id pdf2json/2021.acl-long.517.pdf.json
we O
fine-tune O
these O
models O
for O
span-based O
question-answering O
, O
using O
the O
23.1k O
training O
examples O
( O
subsampled O
previously O
) O
for O
20 O
epochs O
, O
with O
early-stopping O
based O
on O
word-overlap O
f13 O
over O
the O
validation O
set O
. O

section 3
id pdf2json/2021.acl-long.517.pdf.json
our O
bert O
model O
achieves O
an O
em O
score O
of O
73.1 O
and O
an O
f1 O
score O
of O
80.5 O
on O
an O
i.i.d O
. O

section 3
id pdf2json/2021.acl-long.517.pdf.json
validation O
set O
. O

section 3
id pdf2json/2021.acl-long.517.pdf.json
the O
electra O
model O
performs O
slightly O
better O
, O
obtaining O
an O
74.2 O
em O
and O
81.2 O
f1 O
on O
the O
same O
set O
. O

section 3
id pdf2json/2021.acl-long.517.pdf.json
crowdsourcing O
protocol O
we O
build O
our O
crowdsourcing O
platform O
on O
the O
dynabench O
interface O
( O
kiela O
et O
al. O
, O
2021 O
) O
and O
use O
amazon O
’ O
s O
mechanical O
turk O
to O
recruit O
workers O
to O
write O
questions O
. O

section 3
id pdf2json/2021.acl-long.517.pdf.json
to O
ensure O
high O
quality O
, O
we O
restricted O
the O
pool O
to O
u.s. O
residents O
who O
had O
already O
completed O
at O
least O
1000 O
hits O
and O
had O
over O
98 O
% O
hit O
approval O
rate O
. O

section 3
id pdf2json/2021.acl-long.517.pdf.json
for O
each O
task O
, O
we O
conducted O
several O
pilot O
studies O
to O
gather O
feedback O
from O
crowdworkers O
on O
the O
task O
and O
interface O
. O

section 3
id pdf2json/2021.acl-long.517.pdf.json
we O
identified O
median O
time O
taken O
by O
workers O
to O
complete O
the O
task O
in O
our O
pilot O
studies O
and O
used O
that O
to O
design O
the O
incentive O
structure O
for O
the O
main O
task O
. O

section 3
id pdf2json/2021.acl-long.517.pdf.json
we O
also O
conducted O
multiple O
studies O
with O
different O
variants O
of O
instructions O
to O
observe O
trends O
in O
the O
quality O
of O
questions O
and O
refined O
our O
instructions O
based O
on O
feedback O
from O
crowdworkers O
. O

section 3
id pdf2json/2021.acl-long.517.pdf.json
feedback O
from O
the O
pilots O
also O
guided O
improvements O
to O
2we O
used O
the O
data O
prepared O
by O
karpukhin O
et O
al O
. O

section 3
id pdf2json/2021.acl-long.517.pdf.json
( O
2020 O
) O
, O
available O
at O
https O
: O
//www.github.com/facebookresearch/dpr O
. O

section 3
id pdf2json/2021.acl-long.517.pdf.json
3word-overlap O
f1 O
and O
exact O
match O
( O
em O
) O
metrics O
introduced O
in O
rajpurkar O
et O
al O
. O

section 3
id pdf2json/2021.acl-long.517.pdf.json
( O
2016 O
) O
are O
commonly O
used O
to O
evaluate O
performance O
of O
passage-based O
qa O
systems O
, O
where O
the O
correct O
answer O
is O
a O
span O
in O
the O
given O
passage O
. O

section 3
id pdf2json/2021.acl-long.517.pdf.json
our O
crowdsourcing O
interface O
. O

section 3
id pdf2json/2021.acl-long.517.pdf.json
in O
total O
, O
984 O
workers O
took O
part O
in O
the O
study O
, O
with O
741 O
creating O
questions O
. O

section 3
id pdf2json/2021.acl-long.517.pdf.json
in O
our O
final O
study O
, O
we O
randomly O
assigned O
workers O
to O
generate O
questions O
in O
the O
following O
ways O
: O
( O
i O
) O
to O
fool O
the O
bert O
baseline O
; O
( O
ii O
) O
to O
fool O
the O
electra O
baseline O
; O
or O
( O
iii O
) O
without O
a O
model O
in O
the O
loop O
. O

section 3
id pdf2json/2021.acl-long.517.pdf.json
before O
beginning O
the O
task O
, O
each O
worker O
completes O
an O
onboarding O
process O
to O
familiarize O
them O
with O
the O
platform O
. O

section 3
id pdf2json/2021.acl-long.517.pdf.json
we O
present O
the O
same O
set O
of O
passages O
to O
workers O
regardless O
of O
which O
group O
they O
are O
assigned O
to O
, O
tasking O
them O
with O
generating O
5 O
questions O
for O
each O
passage O
. O

section 3
id pdf2json/2021.acl-long.517.pdf.json
incentive O
structure O
during O
our O
pilot O
studies O
, O
we O
found O
that O
workers O
spend O
≈ O
2–3 O
minutes O
to O
generate O
5 O
questions O
. O

section 3
id pdf2json/2021.acl-long.517.pdf.json
we O
provide O
workers O
with O
the O
same O
base O
pay— O
$ O
0.75 O
per O
hit— O
( O
to O
ensure O
compensation O
at O
a O
$ O
15/hour O
rate O
) O
. O

section 3
id pdf2json/2021.acl-long.517.pdf.json
for O
tasks O
involving O
a O
model O
in O
the O
loop O
, O
we O
define O
a O
model O
prediction O
to O
be O
incorrect O
if O
its O
f1 O
score O
is O
less O
than O
40 O
% O
, O
following O
the O
threshold O
set O
by O
bartolo O
et O
al O
. O

section 3
id pdf2json/2021.acl-long.517.pdf.json
( O
2020 O
) O
. O

section 3
id pdf2json/2021.acl-long.517.pdf.json
workers O
tasked O
with O
fooling O
the O
model O
receive O
bonus O
pay O
of O
$ O
0.15 O
for O
every O
question O
that O
leads O
to O
an O
incorrect O
model O
prediction O
. O

section 3
id pdf2json/2021.acl-long.517.pdf.json
this O
way O
, O
a O
worker O
can O
double O
their O
pay O
if O
all O
5 O
of O
their O
generated O
questions O
induce O
incorrect O
model O
predictions O
. O

section 3
id pdf2json/2021.acl-long.517.pdf.json
quality O
control O
upon O
completion O
of O
each O
batch O
of O
our O
data O
collection O
process O
, O
we O
presented≈ O
20 O
% O
of O
the O
collected O
questions O
to O
a O
fourth O
group O
of O
crowdworkers O
who O
were O
tasked O
with O
validating O
whether O
the O
questions O
were O
answerable O
and O
the O
answers O
were O
correctly O
labeled O
. O

section 3
id pdf2json/2021.acl-long.517.pdf.json
in O
addition O
, O
we O
manually O
verified O
a O
small O
fraction O
of O
the O
collected O
question-answer O
pairs O
. O

section 3
id pdf2json/2021.acl-long.517.pdf.json
if O
validations O
of O
at O
least O
20 O
% O
of O
the O
examples O
generated O
by O
a O
particular O
worker O
were O
incorrect O
, O
their O
work O
was O
discarded O
in O
its O
entirety O
. O

section 3
id pdf2json/2021.acl-long.517.pdf.json
the O
entire O
process O
, O
including O
the O
pilot O
studies O
cost O
≈ O
$ O
50k O
and O
spanned O
a O
period O
of O
seven O
months O
. O

section 3
id pdf2json/2021.acl-long.517.pdf.json
through O
this O
process O
, O
we O
collected O
over O
150k O
question-answer O
pairs O
corresponding O
to O
the O
10k O
contexts O
( O
50k O
from O
each O
group O
) O
but O
the O
final O
datasets O
are O
much O
smaller O
, O
as O
we O
explain O
below O
. O

section 4
id pdf2json/2021.acl-long.517.pdf.json
our O
study O
allows O
us O
to O
answer O
three O
questions O
: O
( O
i O
) O
how O
well O
do O
models O
fine-tuned O
on O
adc O
data O
generalize O
to O
unseen O
distributions O
compared O
to O
finetuning O
on O
sdc O
? O

section 4
id pdf2json/2021.acl-long.517.pdf.json
( O
ii O
) O
among O
the O
differences O
between O
adc O
and O
sdc O
, O
how O
many O
are O
due O
to O
workers O
trying O
to O
fool O
the O
model O
regardless O
of O
whether O
they O
are O
successful O
? O

section 4
id pdf2json/2021.acl-long.517.pdf.json
and O
( O
iii O
) O
what O
is O
the O
impact O
of O
training O
on O
adversarial O
data O
only O
versus O
using O
it O
as O
a O
data O
augmentation O
strategy O
? O

section 4
id pdf2json/2021.acl-long.517.pdf.json
datasets O
for O
both O
bert O
and O
electra O
, O
we O
first O
identify O
contexts O
for O
which O
at O
least O
one O
question O
elicited O
an O
incorrect O
model O
prediction O
. O

section 4
id pdf2json/2021.acl-long.517.pdf.json
note O
that O
this O
set O
of O
contexts O
is O
different O
for O
bert O
and O
electra O
. O

section 4
id pdf2json/2021.acl-long.517.pdf.json
for O
each O
such O
context O
c O
, O
we O
identify O
the O
number O
of O
questions O
kc O
( O
out O
of O
5 O
) O
that O
successfully O
fooled O
the O
model O
. O

section 4
id pdf2json/2021.acl-long.517.pdf.json
we O
then O
create O
3 O
datasets O
per O
model O
by O
, O
for O
each O
context O
, O
( O
i O
) O
choosing O
precisely O
those O
kc O
questions O
that O
fooled O
the O
model O
( O
bertfooled O
and O
electrafooled O
) O
; O
( O
ii O
) O
randomly O
choosing O
kc O
questions O
( O
out O
of O
5 O
) O
from O
adc O
data O
without O
replacement O
( O
bertrandom O
and O
electrarandom O
) O
—regardless O
of O
whether O
they O
fooled O
the O
model O
; O
and O
( O
iii O
) O
randomly O
choosing O
kc O
questions O
( O
out O
of O
5 O
) O
from O
the O
sdc O
data O
without O
replacement O
. O

section 4
id pdf2json/2021.acl-long.517.pdf.json
thus O
, O
we O
create O
6 O
datasets O
, O
where O
all O
3 O
bert O
datasets O
have O
the O
same O
number O
of O
questions O
per O
context O
( O
and O
11.3k O
total O
training O
examples O
) O
, O
while O
all O
3 O
electra O
datasets O
likewise O
share O
the O
same O
number O
of O
questions O
per O
context O
( O
and O
14.7k O
total O
training O
examples O
) O
. O

section 4
id pdf2json/2021.acl-long.517.pdf.json
see O
table O
1 O
for O
details O
on O
the O
number O
of O
passages O
and O
question-answer O
pairs O
used O
in O
the O
different O
splits O
. O

section 4
id pdf2json/2021.acl-long.517.pdf.json
models O
for O
our O
empirical O
analysis O
, O
we O
fine-tune O
bert O
( O
devlin O
et O
al. O
, O
2019 O
) O
, O
roberta O
( O
liu O
et O
al. O
, O
2019 O
) O
, O
and O
electra O
( O
clark O
et O
al. O
, O
2020 O
) O
models O
on O
all O
six O
datasets O
generated O
as O
part O
of O
our O
study O
( O
four O
datasets O
via O
adc O
: O
bertfooled O
, O
bertrandom O
, O
electrafooled O
, O
electrarandom O
, O
and O
the O
two O
datasets O
via O
sdc O
) O
. O

section 4
id pdf2json/2021.acl-long.517.pdf.json
we O
also O
fine-tune O
these O
models O
after O
augmenting O
the O
original O
data O
to O
collected O
datasets O
. O

section 4
id pdf2json/2021.acl-long.517.pdf.json
we O
report O
the O
means O
and O
standard O
deviations O
( O
in O
subscript O
) O
of O
em O
and O
f1 O
scores O
following O
10 O
runs O
of O
each O
experiment O
. O

section 4
id pdf2json/2021.acl-long.517.pdf.json
models O
fine-tuned O
on O
all O
adc O
datasets O
typically O
perform O
better O
on O
their O
held-out O
test O
sets O
than O
those O
trained O
on O
sdc O
data O
and O
vice-versa O
( O
table O
2 O
and O
appendix O
table O
5 O
) O
. O

section 4
id pdf2json/2021.acl-long.517.pdf.json
roberta O
fine-tuned O
on O
the O
bertfooled O
training O
set O
obtains O
em O
and O
f1 O
scores O
of O
49.2 O
and O
71.2 O
, O
respectively O
, O
on O
the O
bertfooled O
test O
set O
, O
outperforming O
roberta O
models O
fine-tuned O
on O
bertrandom O
( O
em O
: O
48.0 O
, O
f1 O
: O
69.8 O
) O
and O
sdc O
( O
em O
: O
42.0 O
, O
f1 O
: O
65.3 O
) O
. O

section 4
id pdf2json/2021.acl-long.517.pdf.json
performance O
on O
the O
original O
dev O
set O
( O
karpukhin O
et O
al. O
, O
2020 O
) O
is O
generally O
comparable O
across O
all O
models O
. O

section 4
id pdf2json/2021.acl-long.517.pdf.json
out-of-domain O
generalization O
to O
adversarial O
data O
we O
evaluate O
these O
models O
on O
adversarial O
test O
sets O
constructed O
with O
bidaf O
( O
dbidaf O
) O
, O
bert O
( O
dbert O
) O
and O
roberta O
( O
droberta O
) O
in O
the O
loop O
( O
bartolo O
et O
al. O
, O
2020 O
) O
. O

section 4
id pdf2json/2021.acl-long.517.pdf.json
prior O
work O
suggests O
that O
training O
on O
adc O
data O
leads O
to O
models O
that O
perform O
better O
on O
similarly O
constructed O
adversarial O
evaluation O
sets O
. O

section 4
id pdf2json/2021.acl-long.517.pdf.json
both O
bert O
and O
roberta O
models O
fine-tuned O
on O
adversarial O
data O
generally O
outperform O
models O
fine-tuned O
on O
sdc O
data O
( O
or O
when O
either O
datasets O
are O
augmented O
to O
the O
original O
data O
) O
on O
all O
three O
evaluation O
sets O
( O
table O
3 O
and O
appendix O
table O
6 O
) O
. O

section 4
id pdf2json/2021.acl-long.517.pdf.json
a O
roberta O
model O
fine-tuned O
on O
bertfooled O
outperforms O
a O
roberta O
model O
fine-tuned O
on O
sdc O
by O
9.1 O
, O
9.3 O
, O
and O
6.2 O
em O
points O
on O
droberta O
, O
dbert O
, O
and O
dbidaf O
, O
respectively O
. O

section 4
id pdf2json/2021.acl-long.517.pdf.json
we O
observe O
similar O
trends O
on O
electra O
models O
fine-tuned O
on O
adc O
data O
versus O
sdc O
data O
, O
but O
these O
gains O
disappear O
when O
the O
same O
models O
are O
finetuned O
on O
augmented O
data O
. O

section 4
id pdf2json/2021.acl-long.517.pdf.json
for O
instance O
, O
while O
electra O
fine-tuned O
on O
bertrandom O
obtains O
an O
em O
score O
of O
14.8 O
on O
droberta O
, O
outperforming O
an O
electra O
fine-tuned O
on O
sdc O
data O
by≈ O
3 O
pts O
, O
the O
difference O
is O
no O
longer O
significant O
when O
respective O
models O
are O
fine-tuned O
after O
original O
data O
is O
augmented O
to O
these O
datasets O
. O

section 4
id pdf2json/2021.acl-long.517.pdf.json
electra O
models O
fine-tuned O
on O
adc O
data O
with O
electra O
in O
the O
loop O
perform O
no O
better O
than O
those O
trained O
on O
sdc O
. O

section 4
id pdf2json/2021.acl-long.517.pdf.json
fine-tuning O
electra O
on O
sdc O
augmented O
to O
original O
data O
leads O
to O
an O
≈ O
1 O
pt O
improvement O
on O
both O
metrics O
compared O
to O
augmenting O
adc O
. O

section 4
id pdf2json/2021.acl-long.517.pdf.json
overall O
, O
we O
find O
that O
models O
fine-tuned O
on O
adc O
data O
typically O
generalize O
better O
to O
out-ofdomain O
adversarial O
test O
sets O
than O
models O
fine-tuned O
on O
sdc O
data O
, O
confirming O
the O
findings O
by O
dinan O
et O
al O
. O

section 4
id pdf2json/2021.acl-long.517.pdf.json
( O
2019 O
) O
. O

section 4
id pdf2json/2021.acl-long.517.pdf.json
out-of-domain O
generalization O
to O
mrqa O
we O
further O
evaluate O
these O
models O
on O
12 O
out-of-domain O
datasets O
used O
in O
the O
2019 O
mrqa O
shared O
task4 O
( O
table O
4 O
and O
appendix O
table O
7 O
) O
.5 O
notably O
, O
for O
bert O
, O
fine-tuning O
on O
sdc O
data O
leads O
to O
significantly O
better O
performance O
( O
as O
compared O
to O
fine-tuning O
on O
4the O
mrqa O
2019 O
shared O
task O
includes O
hotpotqa O
( O
yang O
et O
al. O
, O
2018a O
) O
, O
natural O
questions O
( O
kwiatkowski O
et O
al. O
, O
2019 O
) O
, O
searchqa O
( O
dunn O
et O
al. O
, O
2017 O
) O
, O
squad O
( O
rajpurkar O
et O
al. O
, O
2016 O
) O
, O
triviaqa O
( O
joshi O
et O
al. O
, O
2017 O
) O
, O
bioasq O
( O
tsatsaronis O
et O
al. O
, O
2015 O
) O
, O
drop O
( O
dua O
et O
al. O
, O
2019 O
) O
, O
duorc O
( O
saha O
et O
al. O
, O
2018 O
) O
, O
relationextraction O
( O
levy O
et O
al. O
, O
2017 O
) O
, O
race O
( O
lai O
et O
al. O
, O
2017 O
) O
, O
and O
textbookqa O
( O
kembhavi O
et O
al. O
, O
2017 O
) O
. O

section 4
id pdf2json/2021.acl-long.517.pdf.json
5interestingly O
, O
roberta O
appears O
to O
perform O
better O
compared O
to O
bert O
and O
electra O
. O

section 4
id pdf2json/2021.acl-long.517.pdf.json
prior O
works O
have O
hypothesized O
that O
the O
bigger O
size O
and O
increased O
diversity O
of O
the O
pretraining O
corpus O
of O
roberta O
( O
compared O
to O
those O
of O
bert O
and O
electra O
) O
might O
somehow O
be O
responsible O
for O
roberta O
’ O
s O
better O
out-of-domain O
generalization O
, O
( O
baevski O
et O
al. O
, O
2019 O
; O
hendrycks O
et O
al. O
, O
2020 O
; O
tu O
et O
al. O
, O
2020 O
) O
. O

section 4
id pdf2json/2021.acl-long.517.pdf.json
adc O
data O
collected O
with O
bert O
) O
on O
9 O
out O
of O
12 O
mrqa O
datasets O
, O
with O
gains O
of O
more O
than O
10 O
em O
pts O
on O
6 O
of O
them O
. O

section 4
id pdf2json/2021.acl-long.517.pdf.json
on O
bioasq O
, O
bert O
fine-tuned O
on O
bertfooled O
obtains O
em O
and O
f1 O
scores O
of O
23.5 O
and O
30.3 O
, O
respectively O
. O

section 4
id pdf2json/2021.acl-long.517.pdf.json
by O
comparison O
, O
fine-tuning O
on O
sdc O
data O
yields O
markedly O
higher O
em O
and O
f1 O
scores O
of O
35.1 O
and O
55.7 O
, O
respectively O
. O

section 4
id pdf2json/2021.acl-long.517.pdf.json
similar O
trends O
hold O
across O
models O
and O
datasets O
. O

section 4
id pdf2json/2021.acl-long.517.pdf.json
interestingly O
, O
adc O
fine-tuning O
often O
improves O
performance O
on O
drop O
compared O
to O
sdc O
. O

section 4
id pdf2json/2021.acl-long.517.pdf.json
for O
instance O
, O
roberta O
finetuned O
on O
electrarandom O
outperforms O
roberta O
fine-tuned O
on O
sdc O
by O
≈ O
7 O
pts O
. O

section 4
id pdf2json/2021.acl-long.517.pdf.json
note O
that O
drop O
itself O
was O
adversarially O
constructed O
. O

section 4
id pdf2json/2021.acl-long.517.pdf.json
on O
natural O
questions O
, O
models O
fine-tuned O
on O
adc O
data O
generally O
perform O
comparably O
to O
those O
fine-tuned O
on O
sdc O
data O
. O

section 4
id pdf2json/2021.acl-long.517.pdf.json
roberta O
fine-tuned O
on O
bertrandom O
obtains O
em O
and O
f1 O
scores O
of O
48.1 O
and O
62.6 O
, O
respectively O
, O
whereas O
roberta O
fine-tuned O
on O
sdc O
data O
obtains O
scores O
of O
47.9 O
and O
61.7 O
, O
respectively O
. O

section 4
id pdf2json/2021.acl-long.517.pdf.json
it O
is O
worth O
noting O
that O
passages O
sourced O
to O
construct O
both O
adc O
and O
sdc O
datasets O
come O
from O
the O
natural O
questions O
dataset O
, O
which O
could O
be O
one O
reason O
why O
models O
fine-tuned O
on O
adc O
datasets O
perform O
similar O
to O
those O
fine-tuned O
on O
sdc O
datasets O
when O
evaluated O
on O
natural O
questions O
. O

section 4
id pdf2json/2021.acl-long.517.pdf.json
on O
the O
the O
adversarial O
process O
versus O
adversarial O
success O
we O
notice O
that O
models O
fine-tuned O
on O
bertrandom O
and O
electrarandom O
typically O
outperform O
models O
fine-tuned O
on O
bertfooled O
and O
electrafooled O
, O
respectively O
, O
on O
adversarial O
test O
data O
collected O
in O
prior O
work O
( O
bartolo O
et O
al. O
, O
2020 O
) O
, O
as O
well O
as O
on O
mrqa O
. O

section 4
id pdf2json/2021.acl-long.517.pdf.json
similar O
observation O
can O
be O
made O
when O
the O
adc O
data O
is O
augmented O
with O
the O
original O
training O
data O
. O

section 4
id pdf2json/2021.acl-long.517.pdf.json
these O
trends O
suggest O
that O
the O
adc O
process O
( O
regardless O
of O
the O
outcome O
) O
explains O
our O
results O
more O
than O
successfully O
fooling O
a O
model O
. O

section 4
id pdf2json/2021.acl-long.517.pdf.json
furthermore O
, O
models O
fine-tuned O
only O
on O
sdc O
data O
tend O
to O
outperform O
adc-only O
fine-tuned O
models O
; O
however O
, O
following O
augmentation O
, O
adc O
fine-tuning O
achieves O
comparable O
performance O
on O
more O
datasets O
than O
before O
, O
showcasing O
generalization O
following O
augmentation O
. O

section 4
id pdf2json/2021.acl-long.517.pdf.json
notice O
that O
augmenting O
adc O
data O
to O
original O
data O
may O
not O
always O
help O
. O

section 4
id pdf2json/2021.acl-long.517.pdf.json
bert O
fine-tuned O
on O
original O
23.1k O
examples O
achieves O
an O
em O
11.3 O
on O
searchqa O
. O

section 4
id pdf2json/2021.acl-long.517.pdf.json
when O
fine-tuned O
on O
bertfooled O
augmented O
to O
the O
original O
data O
, O
this O
drops O
to O
8.7 O
, O
and O
when O
fine-tuned O
on O
bertrandom O
augmented O
to O
the O
original O
data O
, O
it O
drops O
to O
11.2 O
. O

section 4
id pdf2json/2021.acl-long.517.pdf.json
fine-tuning O
on O
sdc O
augmented O
to O
the O
original O
data O
, O
however O
, O
results O
in O
em O
of O
13.6 O
. O

section 5
id pdf2json/2021.acl-long.517.pdf.json
finally O
, O
we O
perform O
a O
qualitative O
analysis O
over O
the O
collected O
data O
, O
revealing O
profound O
differences O
with O
models O
in O
( O
versus O
out O
of O
) O
the O
loop O
. O

section 5
id pdf2json/2021.acl-long.517.pdf.json
recall O
that O
be- O
cause O
these O
datasets O
were O
constructed O
in O
a O
randomized O
study O
, O
any O
observed O
differences O
are O
attributable O
to O
the O
model-in-the O
loop O
collection O
scheme O
. O

section 5
id pdf2json/2021.acl-long.517.pdf.json
to O
begin O
, O
we O
analyze O
100 O
questions O
from O
each O
dataset O
and O
categorize O
them O
using O
the O
taxonomy O
introduced O
by O
hovy O
et O
al O
. O

section 5
id pdf2json/2021.acl-long.517.pdf.json
( O
2000 O
) O
.6 O
we O
also O
look O
at O
6this O
taxonomy O
can O
be O
accessed O
at O
https O
: O
//www.isi.edu/nat O
ural-language/projects/webclopedia/taxonomy/taxonomy O
the O
first O
word O
of O
the O
wh-type O
questions O
in O
each O
dev O
set O
( O
fig O
. O

section 5
id pdf2json/2021.acl-long.517.pdf.json
3 O
) O
and O
observe O
key O
qualitative O
differences O
between O
data O
via O
adc O
and O
sdc O
for O
both O
models O
. O

section 5
id pdf2json/2021.acl-long.517.pdf.json
in O
case O
of O
adc O
with O
bert O
( O
and O
associated O
sdc O
) O
, O
while O
we O
observe O
that O
most O
questions O
in O
the O
dev O
sets O
start O
with O
what O
, O
adc O
has O
a O
higher O
proportion O
compared O
to O
sdc O
( O
587 O
in O
bertfooled O
and O
492 O
in O
bertrandom O
versus O
416 O
in O
sdc O
) O
. O

section 5
id pdf2json/2021.acl-long.517.pdf.json
furthermore O
, O
we O
notice O
that O
compared O
to O
bertfooled O
dev O
set O
, O
sdc O
has O
more O
when- O
( O
148 O
) O
and O
who-type O
( O
220 O
) O
questions O
, O
the O
answers O
to O
which O
typically O
refer O
to O
dates O
, O
places O
and O
people O
( O
or O
organizations O
) O
, O
respectively O
. O

section 5
id pdf2json/2021.acl-long.517.pdf.json
this O
is O
also O
reflected O
in O
the O
taxonomy O
categorization O
. O

section 5
id pdf2json/2021.acl-long.517.pdf.json
interestingly O
, O
the O
bertrandom O
dev O
set O
has O
more O
when- O
and O
who-type O
questions O
than O
bertfooled O
( O
103 O
and O
182 O
versus O
50 O
and O
159 O
, O
respectively O
) O
. O

section 5
id pdf2json/2021.acl-long.517.pdf.json
this O
indicates O
that O
the O
bert O
model O
could O
have O
been O
better O
at O
answering O
questions O
related O
to O
dates O
and O
people O
( O
or O
organizations O
) O
, O
which O
could O
have O
further O
incentivized O
workers O
not O
to O
generate O
toplevel.html O
such O
questions O
upon O
observing O
these O
patterns O
. O

section 5
id pdf2json/2021.acl-long.517.pdf.json
similarly O
, O
in O
the O
100-question O
samples O
, O
we O
find O
that O
a O
larger O
proportion O
of O
questions O
in O
adc O
are O
categorized O
as O
requiring O
numerical O
reasoning O
( O
11 O
and O
18 O
in O
bertfooled O
and O
bertrandom O
, O
respectively O
) O
compared O
to O
sdc O
( O
7 O
) O
. O

section 5
id pdf2json/2021.acl-long.517.pdf.json
it O
is O
possible O
that O
the O
model O
’ O
s O
performance O
on O
numerical O
reasoning O
( O
as O
also O
demonstrated O
by O
its O
lower O
performance O
on O
drop O
compared O
to O
fine-tuning O
on O
adc O
or O
sdc O
) O
would O
have O
incentivized O
workers O
to O
generate O
more O
questions O
requiring O
numerical O
reasoning O
and O
as O
a O
result O
, O
skewed O
the O
distribution O
towards O
such O
questions O
. O

section 5
id pdf2json/2021.acl-long.517.pdf.json
similarly O
, O
with O
electra O
, O
we O
observe O
that O
what-type O
questions O
constitute O
most O
of O
the O
questions O
in O
the O
development O
sets O
for O
both O
adc O
and O
sdc O
, O
although O
data O
collected O
via O
adc O
has O
a O
higher O
proportion O
of O
these O
( O
641 O
in O
electrafooled O
and O
619 O
in O
electrarandom O
versus O
542 O
in O
sdc O
) O
. O

section 5
id pdf2json/2021.acl-long.517.pdf.json
we O
also O
notice O
more O
how-type O
questions O
in O
adc O
( O
126 O
in O
electrarandom O
) O
vs O
101 O
in O
sdc O
, O
and O
that O
the O
sdc O
sample O
has O
more O
questions O
that O
relate O
to O
dates O
( O
223 O
) O
but O
the O
number O
is O
lower O
in O
the O
adc O
samples O
( O
157 O
and O
86 O
in O
electrarandom O
and O
electrafooled O
, O
respectively O
) O
. O

section 5
id pdf2json/2021.acl-long.517.pdf.json
as O
with O
bert O
, O
the O
electra O
model O
was O
likely O
better O
at O
identifying O
answers O
about O
dates O
or O
years O
which O
could O
have O
further O
incentivized O
workers O
to O
generate O
less O
questions O
of O
such O
types O
. O

section 5
id pdf2json/2021.acl-long.517.pdf.json
however O
, O
unlike O
with O
bert O
, O
we O
observe O
that O
the O
electra O
adc O
and O
sdc O
100-question O
samples O
contain O
similar O
numbers O
of O
questions O
involving O
numerical O
answers O
( O
8 O
, O
9 O
and O
10 O
in O
electrafooled O
, O
electrarandom O
and O
sdc O
respectively O
) O
. O

section 5
id pdf2json/2021.acl-long.517.pdf.json
lastly O
, O
despite O
explicit O
instructions O
not O
to O
generate O
questions O
about O
passage O
structure O
( O
fig O
. O

section 5
id pdf2json/2021.acl-long.517.pdf.json
1 O
) O
, O
a O
small O
number O
of O
workers O
nevertheless O
created O
such O
questions O
. O

section 5
id pdf2json/2021.acl-long.517.pdf.json
for O
instance O
, O
one O
worker O
wrote O
, O
“ O
what O
is O
the O
number O
in O
the O
passage O
that O
is O
one O
digit O
less O
than O
the O
largest O
number O
in O
the O
passage O
? O
” O
while O
most O
such O
questions O
were O
discarded O
during O
validation O
, O
some O
of O
these O
are O
present O
in O
the O
final O
data O
. O

section 5
id pdf2json/2021.acl-long.517.pdf.json
overall O
, O
we O
notice O
considerable O
differences O
between O
adc O
and O
sdc O
data O
, O
particularly O
vis-avis O
what O
kind O
of O
questions O
workers O
generate O
. O

section 5
id pdf2json/2021.acl-long.517.pdf.json
our O
qualitative O
analysis O
offers O
additional O
insights O
that O
suggest O
that O
adc O
would O
skew O
the O
distribution O
of O
questions O
workers O
create O
, O
as O
the O
incentives O
align O
with O
quickly O
creating O
more O
questions O
that O
can O
fool O
the O
model O
. O

section 5
id pdf2json/2021.acl-long.517.pdf.json
this O
is O
reflected O
in O
all O
our O
adc O
datasets O
. O

section 5
id pdf2json/2021.acl-long.517.pdf.json
one O
remedy O
could O
be O
to O
provide O
workers O
with O
initial O
questions O
, O
asking O
them O
to O
edit O
those O
questions O
to O
elicit O
incorrect O
model O
predictions O
. O

section 5
id pdf2json/2021.acl-long.517.pdf.json
similar O
strategies O
were O
employed O
in O
( O
ettinger O
et O
al. O
, O
2017 O
) O
, O
where O
breakers O
minimally O
edited O
original O
data O
to O
elicit O
incorrect O
predictions O
from O
the O
models O
built O
by O
builders O
, O
as O
well O
as O
in O
recently O
introduced O
adversarial O
benchmarks O
for O
sentiment O
analysis O
( O
potts O
et O
al. O
, O
2020 O
) O
. O

section 6
id pdf2json/2021.acl-long.517.pdf.json
in O
this O
paper O
, O
we O
demonstrated O
that O
across O
a O
variety O
of O
models O
and O
datasets O
, O
training O
on O
adversarial O
data O
leads O
to O
better O
performance O
on O
evaluation O
sets O
created O
in O
a O
similar O
fashion O
, O
but O
tends O
to O
yield O
worse O
performance O
on O
out-of-domain O
evaluation O
sets O
not O
created O
adversarially O
. O

section 6
id pdf2json/2021.acl-long.517.pdf.json
additionally O
, O
our O
results O
suggest O
that O
the O
adc O
process O
( O
regardless O
of O
the O
outcome O
) O
might O
matter O
more O
than O
successfully O
fooling O
a O
model O
. O

section 6
id pdf2json/2021.acl-long.517.pdf.json
we O
also O
identify O
key O
qualitative O
differences O
between O
data O
generated O
via O
adc O
and O
sdc O
, O
particularly O
the O
kinds O
of O
questions O
created O
. O

section 6
id pdf2json/2021.acl-long.517.pdf.json
overall O
, O
our O
work O
investigates O
adc O
in O
a O
con- O
trolled O
setting O
, O
offering O
insights O
that O
can O
guide O
future O
research O
in O
this O
direction O
. O

section 6
id pdf2json/2021.acl-long.517.pdf.json
these O
findings O
are O
particularly O
important O
given O
that O
adc O
is O
more O
timeconsuming O
and O
expensive O
than O
sdc O
, O
with O
workers O
requiring O
additional O
financial O
incentives O
. O

section 6
id pdf2json/2021.acl-long.517.pdf.json
we O
believe O
that O
a O
remedy O
to O
these O
issues O
could O
be O
to O
ask O
workers O
to O
edit O
questions O
rather O
than O
to O
generate O
them O
. O

section 6
id pdf2json/2021.acl-long.517.pdf.json
in O
the O
future O
, O
we O
would O
like O
to O
extend O
this O
study O
and O
investigate O
the O
efficacy O
of O
various O
constraints O
on O
question O
creation O
, O
and O
the O
role O
of O
other O
factors O
such O
as O
domain O
complexity O
, O
passage O
length O
, O
and O
incentive O
structure O
, O
among O
others O
. O

section 7
id pdf2json/2021.acl-long.517.pdf.json
the O
authors O
thank O
max O
bartolo O
, O
robin O
jia O
, O
tanya O
marwah O
, O
sanket O
vaibhav O
mehta O
, O
sina O
fazelpour O
, O
kundan O
krishna O
, O
shantanu O
gupta O
, O
simran O
kaur O
, O
and O
aishwarya O
kamath O
for O
their O
valuable O
feedback O
on O
the O
crowdsourcing O
platform O
and O
the O
paper O
. O

section 7
id pdf2json/2021.acl-long.517.pdf.json
ethical O
considerations O
the O
passages O
in O
our O
datasets O
are O
sourced O
from O
the O
datasets O
released O
by O
karpukhin O
et O
al O
. O

section 7
id pdf2json/2021.acl-long.517.pdf.json
( O
2020 O
) O
under O
a O
creative O
commons O
license O
. O

section 7
id pdf2json/2021.acl-long.517.pdf.json
as O
described O
in O
main O
text O
, O
we O
designed O
our O
incentive O
structure O
to O
ensure O
that O
crowdworkers O
were O
paid O
$ O
15/hour O
, O
which O
is O
twice O
the O
us O
federal O
minimum O
wage O
. O

section 7
id pdf2json/2021.acl-long.517.pdf.json
our O
datasets O
focus O
on O
the O
english O
language O
, O
and O
are O
not O
collected O
for O
the O
purpose O
of O
designing O
nlp O
applications O
but O
to O
conduct O
a O
human O
study O
. O

section 7
id pdf2json/2021.acl-long.517.pdf.json
we O
share O
our O
dataset O
to O
allow O
the O
community O
to O
replicate O
our O
findings O
and O
do O
not O
foresee O
any O
risks O
associated O
with O
the O
use O
of O
this O
data O
. O

section TITLE
id pdf2json/2021.acl-long.486.pdf.json
prgc O
: O
potential O
relation O
and O
global O
correspondence O
based O
joint O
relational O
triple O
extraction O

section ABSTRACT
id pdf2json/2021.acl-long.486.pdf.json
joint O
extraction O
of O
entities O
and O
relations O
from O
unstructured O
texts O
is O
a O
crucial O
task O
in O
information O
extraction O
. O

section ABSTRACT
id pdf2json/2021.acl-long.486.pdf.json
recent O
methods O
achieve O
considerable O
performance O
but O
still O
suffer O
from O
some O
inherent O
limitations O
, O
such O
as O
redundancy O
of O
relation O
prediction O
, O
poor O
generalization O
of O
span-based O
extraction O
and O
inefficiency O
. O

section ABSTRACT
id pdf2json/2021.acl-long.486.pdf.json
in O
this O
paper O
, O
we O
decompose O
this O
task O
into O
three O
subtasks O
, O
relation O
judgement O
, O
entity O
extraction O
and O
subject-object O
alignment O
from O
a O
novel O
perspective O
and O
then O
propose O
a O
joint O
relational O
triple O
extraction O
framework O
based O
on O
potential O
relation O
and O
global O
correspondence O
( O
prgc O
) O
. O

section ABSTRACT
id pdf2json/2021.acl-long.486.pdf.json
specifically O
, O
we O
design O
a O
component O
to O
predict O
potential O
relations O
, O
which O
constrains O
the O
following O
entity O
extraction O
to O
the O
predicted O
relation O
subset O
rather O
than O
all O
relations O
; O
then O
a O
relation-specific O
sequence O
tagging O
component O
is O
applied O
to O
handle O
the O
overlapping O
problem O
between O
subjects O
and O
objects O
; O
finally O
, O
a O
global O
correspondence O
component O
is O
designed O
to O
align O
the O
subject O
and O
object O
into O
a O
triple O
with O
low-complexity O
. O

section ABSTRACT
id pdf2json/2021.acl-long.486.pdf.json
extensive O
experiments O
show O
that O
prgc O
achieves O
state-of-the-art O
performance O
on O
public O
benchmarks O
with O
higher O
efficiency O
and O
delivers O
consistent O
performance O
gain O
on O
complex O
scenarios O
of O
overlapping O
triples.1 O

section 0
id pdf2json/2021.acl-long.486.pdf.json
proceedings O
of O
the O
59th O
annual O
meeting O
of O
the O
association O
for O
computational O
linguistics O
and O
the O
11th O
international O
joint O
conference O
on O
natural O
language O
processing O
, O
pages O
6225–6235 O
august O
1–6 O
, O
2021 O
. O

section 0
id pdf2json/2021.acl-long.486.pdf.json
©2021 O
association O
for O
computational O
linguistics O
6225 O

section 1
id pdf2json/2021.acl-long.486.pdf.json
identifying O
entity O
mentions O
and O
their O
relations O
which O
are O
in O
the O
form O
of O
a O
triple O
( O
subject O
, O
relation O
, O
object O
) O
from O
unstructured O
texts O
is O
an O
important O
task O
in O
information O
extraction O
. O

section 1
id pdf2json/2021.acl-long.486.pdf.json
some O
previous O
works O
proposed O
to O
address O
the O
task O
with O
pipelined O
approaches O
which O
include O
two O
steps O
: O
named O
entity O
recognition O
( O
tjong O
kim O
sang O
and O
de O
meulder O
, O
2003 O
; O
ratinov O
and O
roth O
, O
2009 O
) O
and O
relation O
prediction O
( O
zelenko O
et O
al. O
, O
2002 O
; O
bunescu O
and O
mooney O
, O
*corresponding O
author O
. O

section 1
id pdf2json/2021.acl-long.486.pdf.json
1the O
source O
code O
and O
data O
are O
released O
at O
https O
: O
//github.com/hy-struggle/prgc O
. O

section 1
id pdf2json/2021.acl-long.486.pdf.json
2005 O
; O
pawar O
et O
al. O
, O
2017 O
; O
wang O
et O
al. O
, O
2020b O
) O
. O

section 1
id pdf2json/2021.acl-long.486.pdf.json
recent O
end-to-end O
methods O
, O
which O
are O
based O
on O
either O
multi-task O
learning O
( O
wei O
et O
al. O
, O
2020 O
) O
or O
singlestage O
framework O
( O
wang O
et O
al. O
, O
2020a O
) O
, O
achieved O
promising O
performance O
and O
proved O
their O
effectiveness O
, O
but O
lacked O
in-depth O
study O
of O
the O
task O
. O

section 1
id pdf2json/2021.acl-long.486.pdf.json
to O
better O
comprehend O
the O
task O
and O
advance O
the O
state O
of O
the O
art O
, O
we O
propose O
a O
novel O
perspective O
to O
decompose O
the O
task O
into O
three O
subtasks O
: O
i O
) O
relation O
judgement O
which O
aims O
to O
identify O
relations O
in O
a O
sentence O
, O
ii O
) O
entity O
extraction O
which O
aims O
to O
extract O
all O
subjects O
and O
objects O
in O
the O
sentence O
and O
iii O
) O
subject-object O
alignment O
which O
aims O
to O
align O
the O
subject-object O
pair O
into O
a O
triple O
. O

section 1
id pdf2json/2021.acl-long.486.pdf.json
on O
the O
basis O
, O
we O
review O
two O
end-to-end O
methods O
in O
table O
1 O
. O

section 1
id pdf2json/2021.acl-long.486.pdf.json
for O
the O
multi-task O
method O
named O
casrel O
( O
wei O
et O
al. O
, O
2020 O
) O
, O
the O
relational O
triple O
extraction O
is O
performed O
in O
two O
stages O
which O
applies O
object O
extraction O
to O
all O
relations O
. O

section 1
id pdf2json/2021.acl-long.486.pdf.json
obviously O
, O
the O
way O
to O
identify O
relations O
is O
redundant O
which O
contains O
numerous O
invalid O
operations O
, O
and O
the O
span-based O
extraction O
scheme O
which O
just O
pays O
attention O
to O
start/end O
position O
of O
an O
entity O
leads O
to O
poor O
generalization O
. O

section 1
id pdf2json/2021.acl-long.486.pdf.json
meanwhile O
, O
it O
is O
restricted O
to O
process O
one O
subject O
at O
a O
time O
due O
to O
its O
subject-object O
alignment O
mechanism O
, O
which O
is O
inefficient O
and O
difficult O
to O
deploy O
. O

section 1
id pdf2json/2021.acl-long.486.pdf.json
for O
the O
single-stage O
framework O
named O
tplinker O
( O
wang O
et O
al. O
, O
2020a O
) O
, O
in O
order O
to O
avoid O
the O
exposure O
bias O
in O
subject-object O
alignment O
, O
it O
exploits O
a O
rather O
complicated O
decoder O
which O
leads O
to O
sparse O
label O
and O
low O
convergence O
rate O
while O
the O
problems O
of O
relation O
redundancy O
and O
poor O
generalization O
of O
span-based O
extraction O
are O
still O
unsolved O
. O

section 1
id pdf2json/2021.acl-long.486.pdf.json
to O
address O
aforementioned O
issues O
, O
we O
propose O
an O
end-to-end O
framework O
which O
consists O
of O
three O
components O
: O
potential O
relation O
prediction O
, O
relation-specific O
sequence O
tagging O
and O
global O
correspondence O
, O
which O
fulfill O
the O
three O
subtasks O
accordingly O
as O
shown O
in O
table O
1 O
. O

section 1
id pdf2json/2021.acl-long.486.pdf.json
for O
relation O
judgement O
, O
we O
predict O
potential O
relations O
by O
the O
potential O
relation O
prediction O
component O
rather O
than O
preserve O
all O
redundant O
relations O
, O
which O
reduces O
computational O
complexity O
and O
achieves O
better O
performance O
, O
especially O
when O
there O
are O
many O
relations O
in O
the O
dataset.2 O
for O
entity O
extraction O
, O
we O
use O
a O
more O
robust O
relationspecific O
sequence O
tagging O
component O
( O
rel-spec O
sequence O
tagging O
for O
short O
) O
to O
extract O
subjects O
and O
objects O
separately O
, O
to O
naturally O
handle O
overlapping O
between O
subjects O
and O
objects O
. O

section 1
id pdf2json/2021.acl-long.486.pdf.json
for O
subjectobject O
alignment O
, O
unlike O
tplinker O
which O
uses O
a O
relation-based O
token-pair O
matrix O
, O
we O
design O
a O
relation-independent O
global O
correspondence O
matrix O
to O
determine O
whether O
a O
specific O
subject-object O
pair O
is O
valid O
in O
a O
triple O
. O

section 1
id pdf2json/2021.acl-long.486.pdf.json
given O
a O
sentence O
, O
prgc O
first O
predicts O
a O
subset O
of O
potential O
relations O
and O
a O
global O
matrix O
which O
contains O
the O
correspondence O
score O
between O
all O
subjects O
and O
objects O
; O
then O
performs O
sequence O
tagging O
to O
extract O
subjects O
and O
objects O
for O
each O
potential O
relation O
in O
parallel O
; O
finally O
enumerates O
all O
predicted O
entity O
pairs O
, O
which O
are O
then O
pruned O
by O
the O
global O
correspondence O
matrix O
. O

section 1
id pdf2json/2021.acl-long.486.pdf.json
it O
is O
worth O
to O
note O
that O
the O
experiment O
( O
described O
in O
section O
5.2.1 O
) O
shows O
that O
the O
potential O
relation O
prediction O
component O
of O
prgc O
is O
overall O
beneficial O
, O
even O
though O
it O
introduces O
the O
exposure O
bias O
that O
is O
usually O
mentioned O
in O
prior O
single-stage O
methods O
to O
prove O
their O
advantages O
. O

section 1
id pdf2json/2021.acl-long.486.pdf.json
experimental O
results O
show O
that O
prgc O
outperforms O
the O
state-of-the-art O
methods O
on O
public O
benchmarks O
with O
higher O
efficiency O
and O
fewer O
parameters O
. O

section 1
id pdf2json/2021.acl-long.486.pdf.json
detailed O
experiments O
on O
complex O
scenarios O
such O
as O
various O
overlapping O
patterns O
, O
which O
contain O
the O
single O
entity O
overlap O
( O
seo O
) O
, O
entity O
pair O
overlap O
2for O
example O
, O
the O
webnlg O
dataset O
( O
gardent O
et O
al. O
, O
2017 O
) O
has O
hundreds O
of O
relations O
but O
only O
seven O
valid O
relations O
for O
one O
sentence O
mostly O
. O

section 1
id pdf2json/2021.acl-long.486.pdf.json
( O
epo O
) O
and O
subject O
object O
overlap O
( O
soo O
) O
types3 O
show O
that O
our O
method O
owns O
consistent O
advantages O
. O

section 1
id pdf2json/2021.acl-long.486.pdf.json
the O
main O
contributions O
of O
this O
paper O
are O
as O
follows O
: O
1 O
. O

section 1
id pdf2json/2021.acl-long.486.pdf.json
we O
tackle O
the O
relational O
triple O
extraction O
task O
from O
a O
novel O
perspective O
which O
decomposes O
the O
task O
into O
three O
subtasks O
: O
relation O
judgement O
, O
entity O
extraction O
and O
subject-object O
alignment O
, O
and O
previous O
works O
are O
compared O
on O
the O
basis O
of O
the O
proposed O
paradigm O
as O
shown O
in O
table O
1 O
. O

section 1
id pdf2json/2021.acl-long.486.pdf.json
2 O
. O

section 1
id pdf2json/2021.acl-long.486.pdf.json
following O
our O
perspective O
, O
we O
propose O
a O
novel O
end-to-end O
framework O
and O
design O
three O
components O
with O
respect O
to O
the O
subtasks O
which O
greatly O
alleviate O
the O
problems O
of O
redundant O
relation O
judgement O
, O
poor O
generalization O
of O
spanbased O
extraction O
and O
inefficient O
subject-object O
alignment O
, O
respectively O
. O

section 1
id pdf2json/2021.acl-long.486.pdf.json
3 O
. O

section 1
id pdf2json/2021.acl-long.486.pdf.json
we O
conduct O
extensive O
experiments O
on O
several O
public O
benchmarks O
, O
which O
indicate O
that O
our O
method O
achieves O
state-of-the-art O
performance O
, O
especially O
for O
complex O
scenarios O
of O
overlapping O
triples O
. O

section 1
id pdf2json/2021.acl-long.486.pdf.json
further O
ablation O
studies O
and O
analyses O
confirm O
the O
effectiveness O
of O
each O
component O
in O
our O
model O
. O

section 1
id pdf2json/2021.acl-long.486.pdf.json
4 O
. O

section 1
id pdf2json/2021.acl-long.486.pdf.json
in O
addition O
to O
higher O
accuracy O
, O
experiments O
show O
that O
our O
method O
owns O
significant O
advantages O
in O
complexity O
, O
number O
of O
parameters O
, O
floating O
point O
operations O
( O
flops O
) O
and O
inference O
time O
compared O
with O
previous O
works O
. O

section 2
id pdf2json/2021.acl-long.486.pdf.json
traditionally O
, O
relational O
triple O
extraction O
has O
been O
studied O
as O
two O
separated O
tasks O
: O
entity O
extraction O
and O
relation O
prediction O
. O

section 2
id pdf2json/2021.acl-long.486.pdf.json
early O
works O
( O
zelenko O
et O
al. O
, O
2002 O
; O
chan O
and O
roth O
, O
2011 O
) O
apply O
the O
pipelined O
methods O
to O
perform O
relation O
classification O
between O
entity O
pairs O
after O
extracting O
all O
the O
entities O
. O

section 2
id pdf2json/2021.acl-long.486.pdf.json
to O
establish O
the O
correlation O
between O
these O
two O
tasks O
, O
joint O
models O
have O
attracted O
much O
attention O
. O

section 2
id pdf2json/2021.acl-long.486.pdf.json
prior O
featurebased O
joint O
models O
( O
yu O
and O
lam O
, O
2010 O
; O
li O
and O
ji O
, O
2014 O
; O
miwa O
and O
sasaki O
, O
2014 O
; O
ren O
et O
al. O
, O
2017 O
) O
require O
a O
complicated O
process O
of O
feature O
engineering O
and O
rely O
on O
various O
nlp O
tools O
with O
cumbersome O
manual O
operations O
. O

section 2
id pdf2json/2021.acl-long.486.pdf.json
recently O
, O
the O
neural O
network O
model O
which O
reduces O
manual O
involvement O
occupies O
the O
main O
part O
of O
the O
research O
. O

section 2
id pdf2json/2021.acl-long.486.pdf.json
zheng O
et O
al O
. O

section 2
id pdf2json/2021.acl-long.486.pdf.json
( O
2017 O
) O
proposed O
a O
3more O
details O
about O
overlapping O
patterns O
are O
shown O
in O
appendix O
a. O
novel O
tagging O
scheme O
that O
unified O
the O
role O
of O
the O
entity O
and O
the O
relation O
between O
entities O
in O
the O
annotations O
, O
thus O
the O
joint O
extraction O
task O
was O
converted O
to O
a O
sequence O
labeling O
task O
but O
it O
failed O
to O
solve O
the O
overlapping O
problems O
. O

section 2
id pdf2json/2021.acl-long.486.pdf.json
bekoulis O
et O
al O
. O

section 2
id pdf2json/2021.acl-long.486.pdf.json
( O
2018 O
) O
proposed O
to O
first O
extract O
all O
candidate O
entities O
, O
then O
predict O
the O
relation O
of O
every O
entity O
pair O
as O
a O
multihead O
selection O
problem O
, O
which O
shared O
parameters O
but O
did O
not O
decode O
jointly O
. O

section 2
id pdf2json/2021.acl-long.486.pdf.json
nayak O
and O
ng O
( O
2020 O
) O
employed O
an O
encoder-decoder O
architecture O
and O
a O
pointer O
network O
based O
decoding O
approach O
where O
an O
entire O
triple O
was O
generated O
at O
each O
time O
step O
. O

section 2
id pdf2json/2021.acl-long.486.pdf.json
to O
handle O
the O
problems O
mentioned O
above O
, O
wei O
et O
al O
. O

section 2
id pdf2json/2021.acl-long.486.pdf.json
( O
2020 O
) O
presented O
a O
cascade O
framework O
, O
which O
first O
identified O
all O
possible O
subjects O
in O
a O
sentence O
, O
then O
for O
each O
subject O
, O
applied O
span-based O
taggers O
to O
identify O
the O
corresponding O
objects O
based O
on O
each O
relation O
. O

section 2
id pdf2json/2021.acl-long.486.pdf.json
this O
method O
leads O
to O
redundancy O
on O
relation O
judgement O
, O
and O
is O
not O
robust O
due O
to O
the O
span-based O
scheme O
on O
entity O
extraction O
. O

section 2
id pdf2json/2021.acl-long.486.pdf.json
meanwhile O
, O
the O
alignment O
scheme O
of O
subjects O
and O
objects O
limits O
its O
parallelization O
. O

section 2
id pdf2json/2021.acl-long.486.pdf.json
in O
order O
to O
represent O
the O
relation O
of O
triple O
explicitly O
, O
yuan O
et O
al O
. O

section 2
id pdf2json/2021.acl-long.486.pdf.json
( O
2020 O
) O
presented O
a O
relationspecific O
attention O
to O
assign O
different O
weights O
to O
the O
words O
in O
context O
under O
each O
relation O
, O
but O
it O
applied O
a O
naive O
heuristic O
nearest O
neighbor O
principle O
to O
combine O
the O
entity O
pairs O
which O
means O
the O
nearest O
subject O
and O
object O
entities O
will O
be O
combined O
into O
a O
triple O
. O

section 2
id pdf2json/2021.acl-long.486.pdf.json
this O
is O
obviously O
not O
in O
accordance O
with O
intuition O
and O
fact O
. O

section 2
id pdf2json/2021.acl-long.486.pdf.json
meanwhile O
, O
it O
is O
also O
redundant O
on O
relation O
judgement O
. O

section 2
id pdf2json/2021.acl-long.486.pdf.json
the O
state-of-the-art O
method O
named O
tplinker O
( O
wang O
et O
al. O
, O
2020a O
) O
employs O
a O
token O
pair O
linking O
scheme O
which O
performs O
twoo O
( O
n2 O
) O
matrix O
operations O
for O
extracting O
entities O
and O
aligning O
subjects O
with O
objects O
under O
each O
relation O
of O
a O
sentence O
, O
causing O
extreme O
redundancy O
on O
relation O
judgement O
and O
complexity O
on O
subject-object O
alignment O
, O
respectively O
. O

section 2
id pdf2json/2021.acl-long.486.pdf.json
and O
it O
also O
suffers O
from O
the O
disadvantage O
of O
span-based O
extraction O
scheme O
. O

section 3
id pdf2json/2021.acl-long.486.pdf.json
in O
this O
section O
, O
we O
first O
introduce O
our O
perspective O
of O
relational O
triple O
extraction O
task O
with O
a O
principled O
problem O
definition O
, O
then O
elaborate O
each O
component O
of O
the O
prgc O
model O
. O

section 3
id pdf2json/2021.acl-long.486.pdf.json
an O
overview O
illustration O
of O
prgc O
is O
shown O
in O
figure O
1 O
. O

section 4
id pdf2json/2021.acl-long.486.pdf.json
the O
input O
is O
a O
sentence O
s O
= O
{ O
x1 O
, O
x2 O
, O
... O
, O
xn O
} O
with O
n O
tokens O
. O

section 4
id pdf2json/2021.acl-long.486.pdf.json
the O
desired O
outputs O
are O
relational O
triples O
as O
t O
( O
s O
) O
= O
{ O
( O
s O
, O
r O
, O
o O
) O
|s O
, O
o O
∈ O
e O
, O
r O
∈ O
r O
} O
, O
where O
e O
and O
r O
are O
the O
entity O
and O
relation O
sets O
, O
respectively O
. O

section 4
id pdf2json/2021.acl-long.486.pdf.json
in O
this O
paper O
, O
the O
problem O
is O
decomposed O
into O
three O
subtasks O
: O
relation O
judgement O
for O
the O
given O
sentence O
s O
, O
this O
subtask O
predicts O
potential O
relations O
it O
contains O
. O

section 4
id pdf2json/2021.acl-long.486.pdf.json
the O
output O
of O
this O
task O
is O
yr O
( O
s O
) O
= O
{ O
r1 O
, O
r2 O
, O
... O
, O
rm|ri O
∈ O
r O
} O
, O
where O
m O
is O
the O
size O
of O
potential O
relation O
subset O
. O

section 4
id pdf2json/2021.acl-long.486.pdf.json
entity O
extraction O
for O
the O
given O
sentence O
s O
and O
a O
predicted O
potential O
relation O
ri O
, O
this O
subtask O
identifies O
the O
tag O
of O
each O
token O
with O
bio O
( O
i.e. O
, O
begin O
, O
inside O
and O
outside O
) O
tag O
scheme O
( O
tjong O
kim O
sang O
and O
veenstra O
, O
1999 O
; O
ratinov O
and O
roth O
, O
2009 O
) O
. O

section 4
id pdf2json/2021.acl-long.486.pdf.json
let O
tj O
denote O
the O
tag O
. O

section 4
id pdf2json/2021.acl-long.486.pdf.json
the O
output O
of O
this O
task O
is O
ye O
( O
s O
, O
ri|ri O
∈ O
r O
) O
= O
{ O
t1 O
, O
t2 O
, O
... O
, O
tn O
} O
. O

section 4
id pdf2json/2021.acl-long.486.pdf.json
subject-object O
alignment O
for O
the O
given O
sentence O
s O
, O
this O
subtask O
predicts O
the O
correspondence O
score O
between O
the O
start O
tokens O
of O
subjects O
and O
objects O
. O

section 4
id pdf2json/2021.acl-long.486.pdf.json
that O
means O
only O
the O
pair O
of O
start O
tokens O
of O
a O
true O
triple O
has O
a O
high O
score O
, O
while O
the O
other O
token O
pairs O
have O
a O
low O
score O
. O

section 4
id pdf2json/2021.acl-long.486.pdf.json
let O
m O
denote O
the O
global O
correspondence O
matrix O
. O

section 4
id pdf2json/2021.acl-long.486.pdf.json
the O
output O
of O
this O
task O
is O
ys O
( O
s O
) O
= O
m O
∈ O
rn×n O
. O

section 5
id pdf2json/2021.acl-long.486.pdf.json
the O
output O
of O
prgc O
encoder O
is O
yenc O
( O
s O
) O
= O
{ O
h1 O
, O
h2 O
, O
... O
, O
hn|hi O
∈ O
rd×1 O
} O
, O
where O
d O
is O
the O
embedding O
dimension O
, O
and O
n O
is O
the O
number O
of O
tokens O
. O

section 5
id pdf2json/2021.acl-long.486.pdf.json
we O
use O
a O
pre-trained O
bert O
model4 O
( O
devlin O
et O
al. O
, O
2019 O
) O
to O
encode O
the O
input O
sentence O
for O
a O
fair O
comparison O
, O
but O
theoretically O
it O
can O
be O
extended O
to O
other O
encoders O
, O
such O
as O
glove O
( O
pennington O
et O
al. O
, O
2014 O
) O
and O
roberta O
( O
liu O
et O
al. O
, O
2019 O
) O
. O

section 6
id pdf2json/2021.acl-long.486.pdf.json
in O
this O
section O
, O
we O
describe O
the O
instantiation O
of O
prgc O
decoder O
that O
consists O
of O
three O
components O
. O

section 7
id pdf2json/2021.acl-long.486.pdf.json
this O
component O
is O
shown O
as O
the O
orange O
box O
in O
figure O
1 O
where O
rpot O
is O
the O
potential O
relations O
. O

section 7
id pdf2json/2021.acl-long.486.pdf.json
different O
from O
previous O
works O
( O
wei O
et O
al. O
, O
2020 O
; O
yuan O
et O
al. O
, O
2020 O
; O
wang O
et O
al. O
, O
2020a O
) O
which O
redundantly O
perform O
entity O
extraction O
to O
every O
relation O
, O
given O
a O
sentence O
, O
we O
first O
predict O
a O
subset O
of O
potential O
relations O
that O
possibly O
exist O
in O
the O
sentence O
, O
and O
then O
the O
entity O
extraction O
only O
needs O
to O
be O
applied O
to O
these O
potential O
relations O
. O

section 7
id pdf2json/2021.acl-long.486.pdf.json
given O
the O
embedding O
h O
∈ O
rn×d O
of O
a O
sentence O
with O
n O
tokens O
, O
each O
element O
of O
this O
component O
is O
obtained O
as O
: O
havg O
= O
avgpool O
( O
h O
) O
∈ O
rd×1 O
prel O
= O
σ O
( O
wrh O
avg O
+ O
br O
) O
( O
1 O
) O
where O
avgpool O
is O
the O
average O
pooling O
operation O
( O
lin O
et O
al. O
, O
2014 O
) O
, O
wr O
∈ O
rd×1 O
is O
a O
trainable O
weight O
and O
σ O
denotes O
the O
sigmoid O
function O
. O

section 7
id pdf2json/2021.acl-long.486.pdf.json
4please O
refer O
to O
the O
original O
paper O
( O
devlin O
et O
al. O
, O
2019 O
) O
for O
detailed O
descriptions O
. O

section 7
id pdf2json/2021.acl-long.486.pdf.json
we O
model O
it O
as O
a O
multi-label O
binary O
classification O
task O
, O
and O
the O
corresponding O
relation O
will O
be O
assigned O
with O
tag O
1 O
if O
the O
probability O
exceeds O
a O
certain O
threshold O
λ1 O
or O
with O
tag O
0 O
otherwise O
( O
as O
shown O
in O
figure O
1 O
) O
, O
so O
next O
we O
just O
need O
to O
apply O
the O
relation-specific O
sequence O
tagging O
to O
the O
predicted O
relations O
rather O
than O
all O
relations O
. O

section 8
id pdf2json/2021.acl-long.486.pdf.json
as O
shown O
in O
figure O
1 O
, O
we O
obtain O
several O
relationspecific O
sentence O
representations O
of O
potential O
relations O
described O
in O
section O
3.3.1 O
. O

section 8
id pdf2json/2021.acl-long.486.pdf.json
then O
, O
we O
perform O
two O
sequence O
tagging O
operations O
to O
extract O
subjects O
and O
objects O
, O
respectively O
. O

section 8
id pdf2json/2021.acl-long.486.pdf.json
the O
reason O
why O
we O
extract O
subjects O
and O
objects O
separately O
is O
to O
handle O
the O
special O
overlapping O
pattern O
named O
subject O
object O
overlap O
( O
soo O
) O
. O

section 8
id pdf2json/2021.acl-long.486.pdf.json
we O
can O
also O
simplify O
it O
to O
one O
sequence O
tagging O
operation O
with O
two O
types O
of O
entities O
if O
there O
are O
no O
soo O
patterns O
in O
the O
dataset.5 O
for O
the O
sake O
of O
simplicity O
and O
fairness O
, O
we O
abandon O
the O
traditional O
lstm-crf O
( O
panchendrarajan O
and O
amaresan O
, O
2018 O
) O
network O
but O
adopt O
the O
simple O
fully O
connected O
neural O
network O
. O

section 8
id pdf2json/2021.acl-long.486.pdf.json
detailed O
operations O
of O
this O
component O
on O
each O
token O
are O
as O
follows O
: O
psubi O
, O
j O
= O
softmax O
( O
wsub O
( O
hi O
+ O
uj O
) O
+ O
bsub O
) O
pobji O
, O
j O
= O
softmax O
( O
wobj O
( O
hi O
+ O
uj O
) O
+ O
bobj O
) O
( O
2 O
) O
where O
uj O
∈ O
rd×1 O
is O
the O
j-th O
relation O
representation O
in O
a O
trainable O
embedding O
matrix O
u O
∈ O
rd×nr O
where O
nr O
is O
the O
size O
of O
full O
relation O
set O
, O
hi O
∈ O
rd×1 O
is O
the O
encoded O
representation O
of O
the O
i-th O
token O
, O
and O
wsub O
, O
wobj O
∈ O
rd×3 O
are O
trainable O
weights O
where O
the O
size O
of O
tag O
set O
{ O
b O
, O
i O
, O
o O
} O
is O
3 O
. O

section 9
id pdf2json/2021.acl-long.486.pdf.json
after O
sequence O
tagging O
, O
we O
acquire O
all O
possible O
subjects O
and O
objects O
with O
respect O
to O
a O
relation O
of O
the O
sentence O
, O
then O
we O
use O
a O
global O
correspondence O
matrix O
to O
determine O
the O
correct O
pairs O
of O
the O
subjects O
and O
objects O
. O

section 9
id pdf2json/2021.acl-long.486.pdf.json
it O
should O
be O
noted O
that O
the O
global O
correspondence O
matrix O
can O
be O
learned O
simultaneously O
with O
potential O
relation O
prediction O
since O
it O
is O
independent O
of O
relations O
. O

section 9
id pdf2json/2021.acl-long.486.pdf.json
the O
detailed O
process O
is O
as O
follows O
: O
first O
we O
enumerate O
all O
the O
possible O
subjectobject O
pairs O
; O
then O
we O
check O
the O
corresponding O
score O
in O
the O
global O
matrix O
for O
each O
pair O
, O
retain O
it O
if O
the O
value O
exceeds O
a O
certain O
threshold O
λ2 O
or O
filter O
it O
out O
otherwise O
. O

section 9
id pdf2json/2021.acl-long.486.pdf.json
5for O
example O
, O
the O
soo O
pattern O
is O
rare O
in O
the O
nyt O
( O
riedel O
et O
al. O
, O
2010 O
) O
dataset O
. O

section 9
id pdf2json/2021.acl-long.486.pdf.json
as O
shown O
in O
the O
green O
matrix O
m O
in O
figure O
1 O
, O
given O
a O
sentence O
with O
n O
tokens O
, O
the O
shape O
of O
global O
correspondence O
matrix O
will O
be O
rn×n O
. O

section 9
id pdf2json/2021.acl-long.486.pdf.json
each O
element O
of O
this O
matrix O
is O
about O
the O
start O
position O
of O
a O
paired O
subject O
and O
object O
, O
which O
represents O
the O
confidence O
level O
of O
a O
subject-object O
pair O
, O
the O
higher O
the O
value O
, O
the O
higher O
the O
confidence O
level O
that O
the O
pair O
belongs O
to O
a O
triple O
. O

section 9
id pdf2json/2021.acl-long.486.pdf.json
for O
example O
, O
the O
value O
about O
“ O
tom O
” O
and O
“ O
jerry O
” O
at O
row O
1 O
, O
column O
3 O
will O
be O
high O
if O
they O
are O
in O
a O
correct O
triple O
such O
as O
“ O
( O
tom O
, O
like O
, O
jerry O
) O
” O
. O

section 9
id pdf2json/2021.acl-long.486.pdf.json
the O
value O
of O
each O
element O
in O
the O
matrix O
is O
obtained O
as O
follows O
: O
pisub O
, O
jobj O
= O
σ O
( O
wg O
[ O
h O
sub O
i O
; O
h O
obj O
j O
] O
+ O
bg O
) O
( O
3 O
) O
where O
hsubi O
, O
h O
obj O
j O
∈ O
rd×1 O
are O
the O
encoded O
representation O
of O
the O
i-th O
token O
and O
j-th O
token O
in O
the O
input O
sentence O
forming O
a O
potential O
pair O
of O
subject O
and O
object O
, O
wg O
∈ O
r2d×1 O
is O
a O
trainable O
weight O
, O
and O
σ O
is O
the O
sigmoid O
function O
. O

section 10
id pdf2json/2021.acl-long.486.pdf.json
we O
train O
the O
model O
jointly O
, O
optimize O
the O
combined O
objective O
function O
during O
training O
time O
and O
share O
the O
parameters O
of O
the O
prgc O
encoder O
. O

section 10
id pdf2json/2021.acl-long.486.pdf.json
the O
total O
loss O
can O
be O
divided O
into O
three O
parts O
as O
follows O
: O
lrel O
= O
− O
1 O
nr O
nr∑ O
i=1 O
( O
yi O
logprel O
+ O
( O
1− O
yi O
) O
log O
( O
1− O
prel O
) O
) O
( O
4 O
) O
lseq O
= O
− O
1 O
2× O
n× O
npotr O
∑ O
t∈ O
{ O
sub O
, O
obj O
} O
npotr∑ O
j=1 O
n∑ O
i=1 O
yti O
, O
j O
logp O
t O
i O
, O
j O
( O
5 O
) O
lglobal O
=− O
1 O
n2 O
n∑ O
i=1 O
n∑ O
j=1 O
( O
yi O
, O
j O
logpisub O
, O
jobj O
+ O
( O
1− O
yi O
, O
j O
) O
log O
( O
1− O
pisub O
, O
jobj O
) O
) O
( O
6 O
) O
where O
nr O
is O
the O
size O
of O
full O
relation O
set O
and O
n O
pot O
r O
is O
the O
size O
of O
potential O
relation O
subset O
of O
the O
sentence O
. O

section 10
id pdf2json/2021.acl-long.486.pdf.json
the O
total O
loss O
is O
the O
sum O
of O
these O
three O
parts O
, O
ltotal O
= O
αlrel O
+ O
βlseq O
+ O
γlglobal O
. O

section 10
id pdf2json/2021.acl-long.486.pdf.json
( O
7 O
) O
performance O
might O
be O
better O
by O
carefully O
tuning O
the O
weight O
of O
each O
sub-loss O
, O
but O
we O
just O
assign O
equal O
weights O
for O
simplicity O
( O
i.e. O
, O
α O
= O
β O
= O
γ O
= O
1 O
) O
. O

section 12
id pdf2json/2021.acl-long.486.pdf.json
for O
fair O
and O
comprehensive O
comparison O
, O
we O
follow O
yu O
et O
al O
. O

section 12
id pdf2json/2021.acl-long.486.pdf.json
( O
2019 O
) O
and O
wang O
et O
al O
. O

section 12
id pdf2json/2021.acl-long.486.pdf.json
( O
2020a O
) O
to O
evaluate O
our O
model O
on O
two O
public O
datasets O
nyt O
( O
riedel O
et O
al. O
, O
2010 O
) O
and O
webnlg O
( O
gardent O
et O
al. O
, O
2017 O
) O
, O
both O
of O
which O
have O
two O
versions O
, O
respectively O
. O

section 12
id pdf2json/2021.acl-long.486.pdf.json
we O
denote O
the O
different O
versions O
as O
nyt* O
, O
nyt O
and O
webnlg* O
, O
webnlg O
. O

section 12
id pdf2json/2021.acl-long.486.pdf.json
note O
that O
nyt* O
and O
webnlg* O
annotate O
the O
last O
word O
of O
entities O
, O
while O
nyt O
and O
webnlg O
annotate O
the O
whole O
entity O
span O
. O

section 12
id pdf2json/2021.acl-long.486.pdf.json
the O
statistics O
of O
the O
datasets O
are O
described O
in O
table O
2 O
. O

section 12
id pdf2json/2021.acl-long.486.pdf.json
following O
wei O
et O
al O
. O

section 12
id pdf2json/2021.acl-long.486.pdf.json
( O
2020 O
) O
, O
we O
further O
characterize O
the O
test O
set O
w.r.t O
. O

section 12
id pdf2json/2021.acl-long.486.pdf.json
the O
overlapping O
patterns O
and O
the O
number O
of O
triples O
per O
sentence O
. O

section 12
id pdf2json/2021.acl-long.486.pdf.json
following O
prior O
works O
mentioned O
above O
, O
an O
extracted O
relational O
triple O
is O
regarded O
as O
correct O
only O
if O
it O
is O
an O
exact O
match O
with O
ground O
truth O
, O
which O
means O
the O
last O
word O
of O
entities O
or O
the O
whole O
entity O
span O
( O
depending O
on O
the O
annotation O
protocol O
) O
of O
both O
subject O
and O
object O
and O
the O
relation O
are O
all O
correct O
. O

section 12
id pdf2json/2021.acl-long.486.pdf.json
meanwhile O
, O
we O
report O
the O
standard O
micro O
precision O
( O
prec O
. O

section 12
id pdf2json/2021.acl-long.486.pdf.json
) O
, O
recall O
( O
rec O
. O
) O

section 12
id pdf2json/2021.acl-long.486.pdf.json
and O
f1-score O
for O
all O
the O
baselines O
. O

section 12
id pdf2json/2021.acl-long.486.pdf.json
the O
implementation O
details O
are O
shown O
in O
appendix O
b O
. O

section 12
id pdf2json/2021.acl-long.486.pdf.json
we O
compare O
prgc O
with O
eight O
strong O
baseline O
models O
and O
the O
state-of-the-art O
models O
casrel O
( O
wei O
et O
al. O
, O
2020 O
) O
and O
tplinker O
( O
wang O
et O
al. O
, O
2020a O
) O
. O

section 12
id pdf2json/2021.acl-long.486.pdf.json
all O
the O
experimental O
results O
of O
the O
baseline O
models O
are O
directly O
taken O
from O
wang O
et O
al O
. O

section 12
id pdf2json/2021.acl-long.486.pdf.json
( O
2020a O
) O
unless O
specified O
. O

section 13
id pdf2json/2021.acl-long.486.pdf.json
in O
this O
section O
, O
we O
present O
the O
overall O
results O
and O
the O
results O
of O
complex O
scenarios O
, O
while O
the O
results O
on O
different O
subtasks O
corresponding O
to O
different O
components O
in O
our O
model O
are O
described O
in O
appendix O
c O
. O

section 14
id pdf2json/2021.acl-long.486.pdf.json
table O
3 O
shows O
the O
results O
of O
our O
model O
against O
other O
baseline O
methods O
on O
four O
datasets O
. O

section 14
id pdf2json/2021.acl-long.486.pdf.json
our O
prgc O
method O
outperforms O
them O
in O
respect O
of O
almost O
all O
evaluation O
metrics O
even O
if O
compared O
with O
the O
recent O
strongest O
baseline O
( O
wang O
et O
al. O
, O
2020a O
) O
which O
is O
quite O
complicated O
. O

section 14
id pdf2json/2021.acl-long.486.pdf.json
at O
the O
same O
time O
, O
we O
implement O
prgcrandom O
to O
validate O
the O
utility O
of O
our O
prgc O
decoder O
, O
where O
all O
parameters O
of O
the O
encoder O
bert O
are O
randomly O
initialized O
. O

section 14
id pdf2json/2021.acl-long.486.pdf.json
the O
performance O
of O
prgcrandom O
demonstrates O
that O
our O
decoder O
framework O
( O
which O
obtains O
7 O
% O
improvements O
than O
casrelrandom O
) O
is O
still O
more O
competitive O
and O
robust O
than O
others O
even O
without O
taking O
advantage O
of O
the O
pre-trained O
bert O
language O
model O
. O

section 14
id pdf2json/2021.acl-long.486.pdf.json
it O
is O
important O
to O
note O
that O
even O
though O
tplinkerbert O
has O
more O
parameters O
than O
casrelbert O
, O
it O
only O
obtains O
0.1 O
% O
improvements O
on O
the O
webnlg* O
dataset O
, O
and O
the O
authors O
attributed O
this O
to O
problems O
with O
the O
dataset O
itself O
. O

section 14
id pdf2json/2021.acl-long.486.pdf.json
however O
, O
our O
model O
achieves O
a O
10× O
improvements O
than O
tplinker O
on O
the O
webnlg* O
dataset O
and O
a O
significant O
promotion O
on O
the O
webnlg O
dataset O
. O

section 14
id pdf2json/2021.acl-long.486.pdf.json
the O
reason O
behind O
this O
is O
that O
the O
relation O
judgement O
component O
of O
our O
model O
greatly O
reduces O
redundant O
relations O
particularly O
in O
the O
versions O
of O
webnlg O
which O
contain O
hundreds O
of O
relations O
. O

section 14
id pdf2json/2021.acl-long.486.pdf.json
in O
other O
words O
, O
the O
reduction O
in O
negative O
relations O
provides O
an O
additional O
boost O
compared O
to O
the O
models O
that O
perform O
entity O
extraction O
under O
every O
relation O
. O

section 15
id pdf2json/2021.acl-long.486.pdf.json
following O
previous O
works O
( O
wei O
et O
al. O
, O
2020 O
; O
yuan O
et O
al. O
, O
2020 O
; O
wang O
et O
al. O
, O
2020a O
) O
, O
to O
verify O
the O
capability O
of O
our O
model O
in O
handling O
different O
overlapping O
patterns O
and O
sentences O
with O
different O
numbers O
of O
triples O
, O
we O
conduct O
further O
experiments O
on O
nyt* O
and O
webnlg* O
datasets O
. O

section 15
id pdf2json/2021.acl-long.486.pdf.json
as O
shown O
in O
table O
4 O
, O
our O
model O
exceeds O
all O
the O
baselines O
in O
all O
overlapping O
patterns O
in O
both O
datasets O
except O
the O
soo O
pattern O
in O
the O
nyt* O
dataset O
. O

section 15
id pdf2json/2021.acl-long.486.pdf.json
actually O
, O
the O
observation O
on O
the O
latter O
scenario O
is O
not O
reliable O
due O
to O
the O
very O
low O
percentage O
of O
soo O
in O
nyt* O
( O
i.e. O
, O
45 O
out O
of O
8,110 O
as O
shown O
in O
table O
2 O
) O
. O

section 15
id pdf2json/2021.acl-long.486.pdf.json
as O
shown O
in O
table O
5 O
, O
the O
performance O
of O
our O
model O
is O
better O
than O
others O
almost O
in O
every O
subset O
regardless O
of O
the O
number O
of O
triples O
. O

section 15
id pdf2json/2021.acl-long.486.pdf.json
in O
general O
, O
these O
two O
further O
experiments O
adequately O
show O
the O
advantages O
of O
our O
model O
in O
complex O
scenarios O
. O

section 17
id pdf2json/2021.acl-long.486.pdf.json
as O
shown O
in O
table O
6 O
, O
we O
evaluate O
the O
model O
ef- O
ficiency O
with O
respect O
to O
complexity O
, O
floating O
point O
operations O
( O
flops O
) O
( O
molchanov O
et O
al. O
, O
2017 O
) O
, O
parameters O
of O
the O
decoder O
( O
paramsdecoder O
) O
and O
inference O
time6 O
of O
casrel O
, O
tplinker O
and O
prgc O
in O
two O
datasets O
which O
have O
quite O
different O
characteristics O
in O
the O
size O
of O
relation O
set O
, O
the O
average O
number O
of O
relations O
per O
sentence O
and O
the O
average O
number O
of O
subjects O
per O
sentence O
. O

section 17
id pdf2json/2021.acl-long.486.pdf.json
all O
experiments O
are O
conducted O
with O
the O
same O
hardware O
configuration O
. O

section 17
id pdf2json/2021.acl-long.486.pdf.json
because O
the O
number O
of O
subjects O
in O
a O
sentence O
varies O
, O
it O
is O
difficult O
for O
casrel O
to O
predict O
objects O
in O
a O
heterogeneous O
batch O
, O
and O
it O
is O
restricted O
to O
set O
batch O
size O
to O
1 O
in O
the O
official O
implementation O
( O
wang O
et O
al. O
, O
2020a O
) O
. O

section 17
id pdf2json/2021.acl-long.486.pdf.json
for O
the O
sake O
of O
fair O
comparison O
, O
we O
set O
batch O
size O
to O
1 O
and O
24 O
to O
verify O
the O
single-thread O
decoding O
speed O
and O
parallel O
processing O
capability O
, O
respectively O
. O

section 17
id pdf2json/2021.acl-long.486.pdf.json
the O
results O
indicate O
that O
the O
single-thread O
decoding O
speed O
of O
prgc O
is O
2× O
as O
casrel O
and O
3× O
as O
tplinker O
, O
and O
our O
model O
is O
significantly O
better O
than O
tplinker O
in O
terms O
of O
parallel O
processing O
. O

section 17
id pdf2json/2021.acl-long.486.pdf.json
note O
that O
the O
model O
efficiency O
of O
casrel O
and O
tplinker O
decreases O
as O
the O
size O
of O
relation O
set O
increases O
but O
our O
model O
is O
not O
affected O
by O
the O
size O
of O
relation O
set O
, O
thus O
prgc O
overwhelmingly O
outperforms O
both O
models O
in O
terms O
of O
all O
the O
indicators O
of O
efficiency O
in O
the O
webnlg* O
dataset O
. O

section 17
id pdf2json/2021.acl-long.486.pdf.json
compared O
with O
the O
stateof-the-art O
model O
tplinker O
, O
prgc O
is O
an O
order O
of O
magnitude O
lower O
in O
complexity O
and O
the O
flops O
is O
even O
200 O
times O
lower O
, O
thus O
prgc O
has O
fewer O
parameters O
and O
obtains O
3× O
speedup O
in O
the O
inference O
phase O
while O
the O
f1-score O
is O
improved O
by O
1.1 O
% O
. O

section 17
id pdf2json/2021.acl-long.486.pdf.json
even O
though O
casrel O
has O
lower O
complexity O
and O
flops O
in O
the O
nyt* O
dataset O
, O
prgc O
still O
has O
significant O
advantages O
and O
obtains O
a O
5× O
speedup O
in O
the O
inference O
time O
and O
3 O
% O
improvements O
in O
f1-score O
. O

section 17
id pdf2json/2021.acl-long.486.pdf.json
meanwhile O
, O
figure O
2 O
proves O
our O
advantage O
in O
convergence O
rate O
. O

section 17
id pdf2json/2021.acl-long.486.pdf.json
these O
all O
confirm O
the O
efficiency O
of O
6the O
flops O
and O
paramsdecoder O
are O
calculated O
via O
: O
https O
: O
//github.com/sovrasov/flops-counter.pytorch O
. O

section 17
id pdf2json/2021.acl-long.486.pdf.json
our O
model O
. O

section 18
id pdf2json/2021.acl-long.486.pdf.json
in O
this O
section O
, O
we O
conduct O
ablation O
experiments O
to O
demonstrate O
the O
effectiveness O
of O
each O
component O
in O
prgc O
with O
results O
reported O
in O
table O
7 O
. O

section 19
id pdf2json/2021.acl-long.486.pdf.json
we O
use O
each O
relation O
in O
the O
relation O
set O
to O
perform O
sequence O
tagging O
when O
we O
remove O
the O
potential O
relation O
prediction O
component O
to O
avoid O
the O
exposure O
bias O
. O

section 19
id pdf2json/2021.acl-long.486.pdf.json
as O
shown O
in O
table O
7 O
, O
the O
precision O
significantly O
decreases O
without O
this O
component O
, O
because O
the O
number O
of O
predicted O
triples O
increases O
due O
to O
relations O
not O
presented O
in O
the O
sentences O
, O
especially O
in O
the O
webnlg* O
dataset O
where O
the O
size O
of O
relation O
set O
is O
much O
bigger O
and O
brings O
tremendous O
relation O
redundancy O
. O

section 19
id pdf2json/2021.acl-long.486.pdf.json
meanwhile O
, O
with O
the O
increase O
of O
relation O
number O
in O
sentences O
, O
the O
training O
and O
inference O
time O
increases O
three O
to O
four O
times O
. O

section 19
id pdf2json/2021.acl-long.486.pdf.json
through O
this O
experiment O
, O
the O
validity O
of O
this O
component O
that O
aims O
to O
predict O
a O
potential O
relation O
subset O
is O
proved O
, O
which O
is O
not O
only O
beneficial O
to O
model O
accuracy O
, O
but O
also O
to O
efficiency O
. O

section 20
id pdf2json/2021.acl-long.486.pdf.json
as O
a O
comparison O
for O
sequence O
tagging O
scheme O
, O
following O
wei O
et O
al O
. O

section 20
id pdf2json/2021.acl-long.486.pdf.json
( O
2020 O
) O
and O
wang O
et O
al O
. O

section 20
id pdf2json/2021.acl-long.486.pdf.json
( O
2020a O
) O
, O
we O
perform O
binary O
classification O
to O
detect O
start O
and O
end O
positions O
of O
an O
entity O
with O
the O
span-based O
scheme O
. O

section 20
id pdf2json/2021.acl-long.486.pdf.json
as O
shown O
in O
table O
7 O
, O
span-based O
scheme O
brings O
significant O
decline O
of O
performance O
. O

section 20
id pdf2json/2021.acl-long.486.pdf.json
through O
the O
case O
study O
shown O
in O
figure O
3 O
, O
we O
observe O
that O
the O
span-based O
scheme O
tends O
to O
extract O
long O
entities O
and O
identify O
the O
correct O
subject-object O
pairs O
but O
ignore O
their O
relation O
. O

section 20
id pdf2json/2021.acl-long.486.pdf.json
that O
is O
because O
the O
model O
is O
inclined O
to O
remember O
the O
position O
of O
an O
entity O
rather O
than O
understand O
the O
underlying O
semantics O
. O

section 20
id pdf2json/2021.acl-long.486.pdf.json
however O
, O
the O
sequence O
tagging O
scheme O
used O
by O
prgc O
performs O
well O
in O
both O
cases O
, O
and O
experimental O
results O
prove O
that O
our O
tagging O
scheme O
is O
more O
robust O
and O
generalizable O
. O

section 21
id pdf2json/2021.acl-long.486.pdf.json
for O
comparison O
, O
we O
exploit O
the O
heuristic O
nearest O
neighbor O
principle O
to O
combine O
the O
subject-object O
pairs O
which O
was O
used O
by O
zheng O
et O
al O
. O

section 21
id pdf2json/2021.acl-long.486.pdf.json
( O
2017 O
) O
and O
yuan O
et O
al O
. O

section 21
id pdf2json/2021.acl-long.486.pdf.json
( O
2020 O
) O
. O

section 21
id pdf2json/2021.acl-long.486.pdf.json
as O
shown O
in O
table O
7 O
, O
the O
precision O
also O
significantly O
decreases O
without O
global O
correspondence O
, O
because O
the O
number O
of O
predicted O
triples O
increases O
with O
many O
mismatched O
pairs O
when O
the O
model O
loses O
the O
constraint O
imposed O
by O
this O
component O
. O

section 21
id pdf2json/2021.acl-long.486.pdf.json
this O
experiment O
proves O
that O
the O
global O
correspondence O
component O
is O
effective O
and O
greatly O
outperforms O
the O
heuristic O
nearest O
neighbor O
principle O
in O
the O
subject-object O
alignment O
task O
. O

section 22
id pdf2json/2021.acl-long.486.pdf.json
in O
this O
paper O
, O
we O
presented O
a O
brand-new O
perspective O
and O
introduced O
a O
novel O
joint O
relational O
extraction O
framework O
based O
on O
potential O
relation O
and O
global O
correspondence O
, O
which O
greatly O
alleviates O
the O
problems O
of O
redundant O
relation O
judgement O
, O
poor O
generalization O
of O
span-based O
extraction O
and O
inefficient O
subject-object O
alignment O
. O

section 22
id pdf2json/2021.acl-long.486.pdf.json
experimental O
results O
showed O
that O
our O
model O
achieved O
the O
stateof-the-art O
performance O
in O
the O
public O
datasets O
and O
successfully O
handled O
many O
complex O
scenarios O
with O
higher O
efficiency O
. O

section 23
id pdf2json/2021.acl-long.486.pdf.json
as O
shown O
in O
figure O
4 O
, O
the O
normal O
, O
seo O
and O
epo O
patterns O
are O
usually O
mentioned O
in O
prior O
works O
( O
nayak O
and O
ng O
, O
2020 O
; O
wei O
et O
al. O
, O
2020 O
; O
yuan O
et O
al. O
, O
2020 O
; O
wang O
et O
al. O
, O
2020a O
) O
, O
and O
soo O
is O
a O
special O
pattern O
we O
identified O
and O
addressed O
. O

section 23
id pdf2json/2021.acl-long.486.pdf.json
b O
implementation O
details O
we O
implement O
our O
model O
with O
pytorch O
and O
optimize O
the O
parameters O
by O
adam O
( O
kingma O
and O
ba O
, O
2015 O
) O
with O
batch O
size O
of O
64/6 O
for O
nyt/webnlg O
. O

section 23
id pdf2json/2021.acl-long.486.pdf.json
the O
encoder O
learning O
rate O
for O
bert O
is O
set O
as O
5 O
× O
10−5 O
, O
and O
the O
decoder O
learning O
rate O
is O
set O
as O
0.001 O
in O
order O
to O
converge O
rapidly O
. O

section 23
id pdf2json/2021.acl-long.486.pdf.json
we O
also O
conduct O
weight O
decay O
( O
loshchilov O
and O
hutter O
, O
2017 O
) O
with O
a O
rate O
of O
0.01 O
. O

section 23
id pdf2json/2021.acl-long.486.pdf.json
for O
fair O
comparison O
, O
we O
use O
the O
bert-basecased O
english O
model7 O
as O
our O
encoder O
, O
and O
set O
the O
max O
length O
of O
an O
input O
sentence O
to O
100 O
, O
which O
is O
the O
same O
as O
previous O
works O
( O
wei O
et O
al. O
, O
2020 O
; O
wang O
et O
al. O
, O
2020a O
) O
. O

section 23
id pdf2json/2021.acl-long.486.pdf.json
our O
experiments O
are O
conducted O
on O
the O
workstation O
with O
an O
intel O
xeon O
e5 O
2.40 O
ghz O
cpu O
, O
128 O
gb O
memory O
, O
an O
nvidia O
tesla O
v100 O
gpu O
, O
and O
centos O
7.2 O
. O

section 23
id pdf2json/2021.acl-long.486.pdf.json
we O
train O
the O
model O
for O
100 O
epochs O
and O
choose O
the O
last O
model O
. O

section 23
id pdf2json/2021.acl-long.486.pdf.json
the O
performance O
will O
be O
better O
if O
the O
higher O
the O
threshold O
of O
potential O
relation O
prediction O
( O
λ1 O
) O
, O
but O
tuning O
the O
threshold O
of O
global O
correspondence O
( O
λ2 O
) O
will O
not O
help O
which O
is O
consistent O
with O
the O
analysis O
in O
appendix O
c. O
7available O
at O
https O
: O
//huggingface.co/bert-base-cased O
. O

section 24
id pdf2json/2021.acl-long.486.pdf.json
to O
further O
verify O
the O
results O
of O
the O
three O
subtasks O
in O
our O
new O
perspective O
and O
the O
performance O
of O
each O
component O
in O
our O
model O
, O
we O
present O
more O
detailed O
evaluations O
on O
nyt* O
and O
webnlg* O
datasets O
in O
table O
8 O
. O

section 24
id pdf2json/2021.acl-long.486.pdf.json
relation O
judgement O
we O
evaluate O
outputs O
of O
the O
potential O
relation O
prediction O
component O
which O
are O
potential O
relations O
contained O
in O
a O
sentence O
. O

section 24
id pdf2json/2021.acl-long.486.pdf.json
recall O
is O
more O
important O
for O
this O
task O
because O
if O
a O
true O
relation O
is O
missed O
, O
it O
will O
not O
be O
recovered O
in O
the O
following O
steps O
. O

section 24
id pdf2json/2021.acl-long.486.pdf.json
we O
get O
high O
recall O
in O
this O
task O
and O
the O
results O
show O
that O
effectiveness O
of O
potential O
relation O
prediction O
component O
is O
not O
affected O
by O
the O
size O
of O
relation O
set O
. O

section 24
id pdf2json/2021.acl-long.486.pdf.json
entity O
extraction O
this O
task O
is O
related O
to O
the O
relation-specific O
sequence O
tagging O
component O
, O
and O
we O
evaluate O
it O
as O
a O
named O
entity O
recognition O
( O
ner O
) O
task O
with O
two O
types O
of O
entities O
: O
subjects O
and O
objects O
. O

section 24
id pdf2json/2021.acl-long.486.pdf.json
the O
predicted O
entities O
are O
from O
all O
potential O
relations O
of O
a O
sentence O
, O
and O
recall O
is O
more O
important O
for O
this O
task O
because O
most O
false O
negatives O
can O
be O
filtered O
out O
by O
subject-object O
alignment O
. O

section 24
id pdf2json/2021.acl-long.486.pdf.json
experimental O
results O
show O
that O
we O
extract O
almost O
all O
correct O
entities O
, O
and O
it O
further O
proves O
that O
the O
influence O
of O
the O
exposure O
bias O
is O
negligible O
. O

section 24
id pdf2json/2021.acl-long.486.pdf.json
subject-object O
alignment O
this O
task O
is O
related O
to O
the O
global O
correspondence O
component O
, O
and O
we O
just O
evaluate O
the O
entity O
pair O
in O
a O
triple O
and O
ignore O
the O
relation O
. O

section 24
id pdf2json/2021.acl-long.486.pdf.json
both O
recall O
and O
precision O
are O
important O
for O
this O
component O
, O
experimental O
results O
indicate O
that O
our O
alignment O
scheme O
is O
useful O
but O
still O
can O
be O
further O
improved O
, O
especially O
in O
the O
recall O
. O

section 24
id pdf2json/2021.acl-long.486.pdf.json
overall O
, O
the O
combination O
of O
three O
components O
in O
our O
model O
accomplishes O
the O
relational O
triple O
extraction O
task O
with O
a O
fine-grained O
perspective O
, O
and O
achieves O
better O
and O
solid O
results O
. O

section TITLE
id pdf2json/2021.acl-long.506.pdf.json
beyond O
offline O
mapping O
: O
learning O
cross-lingual O
word O
embeddings O
through O
context O
anchoring O

section ABSTRACT
id pdf2json/2021.acl-long.506.pdf.json
recent O
research O
on O
cross-lingual O
word O
embeddings O
has O
been O
dominated O
by O
unsupervised O
mapping O
approaches O
that O
align O
monolingual O
embeddings O
. O

section ABSTRACT
id pdf2json/2021.acl-long.506.pdf.json
such O
methods O
critically O
rely O
on O
those O
embeddings O
having O
a O
similar O
structure O
, O
but O
it O
was O
recently O
shown O
that O
the O
separate O
training O
in O
different O
languages O
causes O
departures O
from O
this O
assumption O
. O

section ABSTRACT
id pdf2json/2021.acl-long.506.pdf.json
in O
this O
paper O
, O
we O
propose O
an O
alternative O
approach O
that O
does O
not O
have O
this O
limitation O
, O
while O
requiring O
a O
weak O
seed O
dictionary O
( O
e.g. O
, O
a O
list O
of O
identical O
words O
) O
as O
the O
only O
form O
of O
supervision O
. O

section ABSTRACT
id pdf2json/2021.acl-long.506.pdf.json
rather O
than O
aligning O
two O
fixed O
embedding O
spaces O
, O
our O
method O
works O
by O
fixing O
the O
target O
language O
embeddings O
, O
and O
learning O
a O
new O
set O
of O
embeddings O
for O
the O
source O
language O
that O
are O
aligned O
with O
them O
. O

section ABSTRACT
id pdf2json/2021.acl-long.506.pdf.json
to O
that O
end O
, O
we O
use O
an O
extension O
of O
skip-gram O
that O
leverages O
translated O
context O
words O
as O
anchor O
points O
, O
and O
incorporates O
self-learning O
and O
iterative O
restarts O
to O
reduce O
the O
dependency O
on O
the O
initial O
dictionary O
. O

section ABSTRACT
id pdf2json/2021.acl-long.506.pdf.json
our O
approach O
outperforms O
conventional O
mapping O
methods O
on O
bilingual O
lexicon O
induction O
, O
and O
obtains O
competitive O
results O
in O
the O
downstream O
xnli O
task O
. O

section 0
id pdf2json/2021.acl-long.506.pdf.json
proceedings O
of O
the O
59th O
annual O
meeting O
of O
the O
association O
for O
computational O
linguistics O
and O
the O
11th O
international O
joint O
conference O
on O
natural O
language O
processing O
, O
pages O
6479–6489 O
august O
1–6 O
, O
2021 O
. O

section 0
id pdf2json/2021.acl-long.506.pdf.json
©2021 O
association O
for O
computational O
linguistics O
6479 O
dings O
has O
been O
dominated O
by O
unsupervised O
mapping O
approaches O
that O
align O
monolingual O
embeddings O
. O

section 0
id pdf2json/2021.acl-long.506.pdf.json
such O
methods O
critically O
rely O
on O
those O
embeddings O
having O
a O
similar O
structure O
, O
but O
it O
was O
recently O
shown O
that O
the O
separate O
training O
in O
different O
languages O
causes O
departures O
from O
this O
assumption O
. O

section 0
id pdf2json/2021.acl-long.506.pdf.json
in O
this O
paper O
, O
we O
propose O
an O
alternative O
approach O
that O
does O
not O
have O
this O
limitation O
, O
while O
requiring O
a O
weak O
seed O
dictionary O
( O
e.g. O
, O
a O
list O
of O
identical O
words O
) O
as O
the O
only O
form O
of O
supervision O
. O

section 0
id pdf2json/2021.acl-long.506.pdf.json
rather O
than O
aligning O
two O
fixed O
embedding O
spaces O
, O
our O
method O
works O
by O
fixing O
the O
target O
language O
embeddings O
, O
and O
learning O
a O
new O
set O
of O
embeddings O
for O
the O
source O
language O
that O
are O
aligned O
with O
them O
. O

section 0
id pdf2json/2021.acl-long.506.pdf.json
to O
that O
end O
, O
we O
use O
an O
extension O
of O
skip-gram O
that O
leverages O
translated O
context O
words O
as O
anchor O
points O
, O
and O
incorporates O
self-learning O
and O
iterative O
restarts O
to O
reduce O
the O
dependency O
on O
the O
initial O
dictionary O
. O

section 0
id pdf2json/2021.acl-long.506.pdf.json
our O
approach O
outperforms O
conventional O
mapping O
methods O
on O
bilingual O
lexicon O
induction O
, O
and O
obtains O
competitive O
results O
in O
the O
downstream O
xnli O
task O
. O

section 1
id pdf2json/2021.acl-long.506.pdf.json
cross-lingual O
word O
embeddings O
( O
clwes O
) O
represent O
words O
from O
two O
or O
more O
languages O
in O
a O
shared O
space O
, O
so O
that O
semantically O
similar O
words O
in O
different O
languages O
are O
close O
to O
each O
other O
. O

section 1
id pdf2json/2021.acl-long.506.pdf.json
early O
work O
focused O
on O
jointly O
learning O
clwes O
in O
two O
languages O
, O
relying O
on O
a O
strong O
cross-lingual O
supervision O
in O
the O
form O
of O
parallel O
corpora O
( O
luong O
et O
al. O
, O
2015 O
; O
gouws O
et O
al. O
, O
2015 O
) O
or O
bilingual O
dictionaries O
( O
gouws O
and O
søgaard O
, O
2015 O
; O
duong O
et O
al. O
, O
2016 O
) O
. O

section 1
id pdf2json/2021.acl-long.506.pdf.json
however O
, O
these O
approaches O
were O
later O
superseded O
by O
offline O
mapping O
methods O
, O
which O
separately O
train O
word O
embeddings O
in O
different O
languages O
and O
align O
them O
in O
an O
unsupervised O
manner O
through O
self-learning O
( O
artetxe O
et O
al. O
, O
2018 O
; O
hoshen O
and O
wolf O
, O
2018 O
) O
or O
adversarial O
training O
( O
zhang O
et O
al. O
, O
2017 O
; O
conneau O
et O
al. O
, O
2018a O
) O
. O

section 1
id pdf2json/2021.acl-long.506.pdf.json
despite O
the O
advantage O
of O
not O
requiring O
any O
parallel O
resources O
, O
mapping O
methods O
critically O
rely O
on O
the O
underlying O
embeddings O
having O
a O
similar O
structure O
, O
which O
is O
known O
as O
the O
isometry O
assumption O
. O

section 1
id pdf2json/2021.acl-long.506.pdf.json
several O
authors O
have O
observed O
that O
this O
assumption O
does O
not O
generally O
hold O
, O
severely O
hindering O
the O
performance O
of O
these O
methods O
( O
søgaard O
et O
al. O
, O
2018 O
; O
nakashole O
and O
flauger O
, O
2018 O
; O
patra O
et O
al. O
, O
2019 O
) O
. O

section 1
id pdf2json/2021.acl-long.506.pdf.json
in O
later O
work O
, O
ormazabal O
et O
al O
. O

section 1
id pdf2json/2021.acl-long.506.pdf.json
( O
2019 O
) O
showed O
that O
this O
issue O
arises O
from O
trying O
to O
align O
separately O
trained O
embeddings O
, O
as O
joint O
learning O
methods O
are O
not O
susceptible O
to O
it O
. O

section 1
id pdf2json/2021.acl-long.506.pdf.json
in O
this O
paper O
, O
we O
propose O
an O
alternative O
approach O
that O
does O
not O
have O
this O
limitation O
, O
but O
can O
still O
work O
without O
any O
parallel O
resources O
. O

section 1
id pdf2json/2021.acl-long.506.pdf.json
the O
core O
idea O
of O
our O
method O
is O
to O
fix O
the O
target O
language O
embeddings O
, O
and O
learn O
aligned O
embeddings O
for O
the O
source O
language O
from O
scratch O
. O

section 1
id pdf2json/2021.acl-long.506.pdf.json
this O
prevents O
structural O
mismatches O
that O
result O
from O
independently O
training O
embeddings O
in O
different O
languages O
, O
as O
the O
learning O
of O
the O
source O
embeddings O
is O
tailored O
to O
each O
particular O
set O
of O
target O
embeddings O
. O

section 1
id pdf2json/2021.acl-long.506.pdf.json
for O
that O
purpose O
, O
we O
use O
an O
extension O
of O
skip-gram O
that O
leverages O
translated O
context O
words O
as O
anchor O
points O
. O

section 1
id pdf2json/2021.acl-long.506.pdf.json
so O
as O
to O
translate O
the O
context O
words O
, O
we O
start O
with O
a O
weak O
initial O
dictionary O
, O
which O
is O
iteratively O
improved O
through O
self-learning O
, O
and O
we O
further O
incorporate O
a O
restarting O
procedure O
to O
make O
our O
method O
more O
robust O
. O

section 1
id pdf2json/2021.acl-long.506.pdf.json
thanks O
to O
this O
, O
our O
approach O
can O
effectively O
work O
without O
any O
human-crafted O
bilingual O
resources O
, O
relying O
on O
simple O
heuristics O
( O
automatically O
generated O
lists O
of O
numerals O
or O
identical O
words O
) O
or O
an O
existing O
unsupervised O
mapping O
method O
to O
build O
the O
initial O
dictionary O
. O

section 1
id pdf2json/2021.acl-long.506.pdf.json
our O
experiments O
confirm O
the O
effectiveness O
of O
our O
approach O
, O
outperforming O
previous O
mapping O
methods O
on O
bilingual O
dictionary O
induction O
and O
obtaining O
competitive O
results O
on O
zero-shot O
crosslingual O
transfer O
learning O
on O
xnli O
. O

section 2
id pdf2json/2021.acl-long.506.pdf.json
word O
embeddings O
. O

section 2
id pdf2json/2021.acl-long.506.pdf.json
embedding O
methods O
learn O
static O
word O
representations O
based O
on O
co-occurrence O
statistics O
from O
a O
corpus O
. O

section 2
id pdf2json/2021.acl-long.506.pdf.json
most O
approaches O
use O
two O
different O
matrices O
to O
represent O
the O
words O
and O
the O
contexts O
, O
which O
are O
known O
as O
the O
input O
and O
output O
vectors O
, O
respectively O
( O
mikolov O
et O
al. O
, O
2013 O
; O
pennington O
et O
al. O
, O
2014 O
; O
bojanowski O
et O
al. O
, O
2017 O
) O
. O

section 2
id pdf2json/2021.acl-long.506.pdf.json
the O
output O
vectors O
play O
an O
auxiliary O
role O
, O
being O
discarded O
after O
training O
. O

section 2
id pdf2json/2021.acl-long.506.pdf.json
our O
method O
takes O
advantage O
of O
this O
fact O
, O
leveraging O
translated O
output O
vectors O
as O
anchor O
points O
to O
learn O
cross-lingual O
embeddings O
. O

section 2
id pdf2json/2021.acl-long.506.pdf.json
to O
that O
end O
, O
we O
build O
on O
the O
skip-gram O
with O
negative O
sampling O
( O
sgns O
) O
algorithm O
( O
mikolov O
et O
al. O
, O
2013 O
) O
, O
which O
trains O
a O
binary O
classifier O
to O
distinguish O
whether O
each O
output O
word O
co-occurs O
with O
the O
given O
input O
word O
in O
the O
training O
corpus O
or O
was O
instead O
sampled O
from O
a O
noise O
distribution O
. O

section 2
id pdf2json/2021.acl-long.506.pdf.json
mapping O
clwe O
methods O
. O

section 2
id pdf2json/2021.acl-long.506.pdf.json
offline O
mapping O
methods O
separately O
train O
word O
embeddings O
for O
each O
language O
, O
and O
then O
learn O
a O
mapping O
to O
align O
them O
into O
a O
shared O
space O
. O

section 2
id pdf2json/2021.acl-long.506.pdf.json
most O
of O
these O
methods O
align O
the O
embeddings O
through O
a O
linear O
map—often O
enforcing O
orthogonality O
constraints—and O
, O
as O
such O
, O
they O
rely O
on O
the O
assumption O
that O
the O
geometric O
structure O
of O
the O
separately O
learned O
embeddings O
is O
similar O
. O

section 2
id pdf2json/2021.acl-long.506.pdf.json
this O
assumption O
has O
been O
shown O
to O
fail O
under O
unfavorable O
conditions O
, O
severely O
hindering O
the O
performance O
of O
these O
methods O
( O
søgaard O
et O
al. O
, O
2018 O
; O
vulić O
et O
al. O
, O
2020 O
) O
. O

section 2
id pdf2json/2021.acl-long.506.pdf.json
existing O
attempts O
to O
mitigate O
this O
issue O
include O
learning O
non-linear O
maps O
in O
a O
latent O
space O
( O
mohiuddin O
et O
al. O
, O
2020 O
) O
, O
employing O
maps O
that O
are O
only O
locally O
linear O
( O
nakashole O
, O
2018 O
) O
, O
or O
learning O
a O
separate O
map O
for O
each O
word O
( O
glavaš O
and O
vulić O
, O
2020 O
) O
. O

section 2
id pdf2json/2021.acl-long.506.pdf.json
however O
, O
all O
these O
methods O
are O
supervised O
, O
and O
have O
the O
same O
fundamental O
limitation O
of O
aligning O
a O
set O
of O
separately O
trained O
embeddings O
( O
ormazabal O
et O
al. O
, O
2019 O
) O
. O

section 2
id pdf2json/2021.acl-long.506.pdf.json
self-learning O
. O

section 2
id pdf2json/2021.acl-long.506.pdf.json
while O
early O
mapping O
methods O
relied O
on O
a O
bilingual O
dictionary O
to O
learn O
the O
alignment O
, O
this O
requirement O
was O
alleviated O
thanks O
to O
selflearning O
, O
which O
iteratively O
re-induces O
the O
dictionary O
during O
training O
. O

section 2
id pdf2json/2021.acl-long.506.pdf.json
this O
enabled O
learning O
clwes O
in O
a O
semi-supervised O
fashion O
starting O
from O
a O
weak O
initial O
dictionary O
( O
artetxe O
et O
al. O
, O
2017 O
) O
, O
or O
in O
a O
completely O
unsupervised O
manner O
when O
combined O
with O
adversarial O
training O
( O
conneau O
et O
al. O
, O
2018a O
) O
or O
initialization O
heuristics O
( O
artetxe O
et O
al. O
, O
2018 O
; O
hoshen O
and O
wolf O
, O
2018 O
) O
. O

section 2
id pdf2json/2021.acl-long.506.pdf.json
our O
proposed O
method O
also O
incorporates O
a O
self-learning O
procedure O
, O
showing O
that O
this O
technique O
can O
also O
be O
effective O
with O
non-mapping O
methods O
. O

section 2
id pdf2json/2021.acl-long.506.pdf.json
joint O
clwe O
methods O
. O

section 2
id pdf2json/2021.acl-long.506.pdf.json
before O
the O
popularization O
of O
offline O
mapping O
, O
most O
clwe O
methods O
extended O
monolingual O
embedding O
algorithms O
by O
either O
incorporating O
an O
explicit O
cross-lingual O
term O
in O
their O
learning O
objective O
, O
or O
directly O
replacing O
words O
with O
their O
translation O
equivalents O
in O
the O
training O
corpus O
. O

section 2
id pdf2json/2021.acl-long.506.pdf.json
for O
that O
purpose O
, O
these O
methods O
relied O
on O
some O
form O
of O
cross-lingual O
supervision O
, O
ranging O
from O
bilingual O
dictionaries O
( O
gouws O
and O
søgaard O
, O
2015 O
; O
duong O
et O
al. O
, O
2016 O
) O
to O
parallel O
or O
documentaligned O
corpora O
( O
luong O
et O
al. O
, O
2015 O
; O
gouws O
et O
al. O
, O
2015 O
; O
vulić O
and O
moens O
, O
2016 O
) O
. O

section 2
id pdf2json/2021.acl-long.506.pdf.json
more O
recently O
, O
lample O
et O
al O
. O

section 2
id pdf2json/2021.acl-long.506.pdf.json
( O
2018 O
) O
reported O
positive O
results O
learning O
regular O
word O
embeddings O
over O
concatenated O
monolingual O
corpora O
in O
different O
languages O
, O
relying O
on O
identical O
words O
as O
anchor O
points O
. O

section 2
id pdf2json/2021.acl-long.506.pdf.json
wang O
et O
al O
. O

section 2
id pdf2json/2021.acl-long.506.pdf.json
( O
2019 O
) O
further O
improved O
this O
approach O
by O
applying O
a O
conventional O
mapping O
method O
afterwards O
. O

section 2
id pdf2json/2021.acl-long.506.pdf.json
as O
shown O
later O
in O
our O
experiments O
, O
our O
approach O
outperforms O
theirs O
by O
a O
large O
margin O
. O

section 2
id pdf2json/2021.acl-long.506.pdf.json
freezing O
. O

section 2
id pdf2json/2021.acl-long.506.pdf.json
artetxe O
et O
al O
. O

section 2
id pdf2json/2021.acl-long.506.pdf.json
( O
2020 O
) O
showed O
that O
it O
is O
possible O
to O
transfer O
an O
english O
transformer O
to O
a O
new O
language O
by O
freezing O
all O
the O
inner O
parameters O
of O
the O
network O
and O
learning O
a O
new O
set O
of O
embeddings O
for O
the O
new O
language O
through O
masked O
language O
modeling O
. O

section 2
id pdf2json/2021.acl-long.506.pdf.json
this O
works O
because O
the O
frozen O
transformer O
parameters O
constrain O
the O
resulting O
representations O
to O
be O
aligned O
with O
english O
. O

section 2
id pdf2json/2021.acl-long.506.pdf.json
similarly O
, O
our O
proposed O
approach O
uses O
frozen O
output O
vectors O
in O
the O
target O
language O
as O
anchor O
points O
to O
learn O
aligned O
embeddings O
in O
the O
source O
language O
. O

section 3
id pdf2json/2021.acl-long.506.pdf.json
let O
xi O
and O
x̃i O
be O
the O
input O
and O
output O
vectors O
of O
the O
ith O
word O
in O
the O
source O
language O
, O
and O
yj O
and O
ỹj O
be O
their O
analogous O
in O
the O
target O
language O
. O

section 3
id pdf2json/2021.acl-long.506.pdf.json
1 O
in O
addition O
, O
let O
d O
be O
a O
bilingual O
dictionary O
, O
where O
d O
( O
i O
) O
= O
j O
denotes O
that O
the O
ith O
word O
in O
the O
source O
language O
is O
translated O
as O
the O
jth O
word O
in O
the O
target O
language O
. O

section 3
id pdf2json/2021.acl-long.506.pdf.json
our O
approach O
first O
learns O
the O
target O
language O
embeddings O
{ O
yi O
} O
and O
{ O
ỹi O
} O
monolingually O
using O
regular O
sgns O
. O

section 3
id pdf2json/2021.acl-long.506.pdf.json
having O
done O
that O
, O
we O
learn O
the O
source O
language O
embeddings O
{ O
xi O
} O
and O
{ O
x̃i O
} O
, O
constraining O
them O
to O
be O
aligned O
with O
the O
target O
language O
embeddings O
according O
to O
the O
dictionary O
d. O
for O
that O
purpose O
, O
we O
propose O
an O
extension O
of O
1recall O
that O
{ O
x̃i O
} O
and O
{ O
ỹj O
} O
are O
auxiliary O
, O
and O
the O
goal O
is O
to O
learn O
aligned O
{ O
xi O
} O
and O
{ O
yj O
} O
( O
see O
§2 O
) O
. O

section 3
id pdf2json/2021.acl-long.506.pdf.json
algorithm O
1 O
proposed O
method O
input O
: O
d O
( O
dictionary O
) O
, O
csrc O
( O
src O
corpus O
) O
, O
ctgt O
( O
tgt O
corpus O
) O
output O
: O
{ O
xi O
} O
, O
{ O
yi O
} O
( O
aligned O
src O
and O
tgt O
embs O
) O
hparams O
: O
t O
( O
updates O
) O
, O
r O
( O
restarts O
) O
, O
k O
( O
re-inductions O
) O
1 O
: O
{ O
yi O
} O
, O
{ O
ỹi O
} O
← O
sgns O
( O
ctgt O
) O
⊲ O
learn O
target O
embedings O
2 O
: O
for O
r O
← O
1 O
to O
r O
do O
⊲ O
iterative O
restart O
( O
§3.3 O
) O
3 O
: O
{ O
xi O
} O
, O
{ O
x̃i O
} O
← O
random O
init O
( O
) O
4 O
: O
for O
it← O
1 O
to O
t O
do O
5 O
: O
( O
wi O
, O
wj O
) O
← O
next O
instance O
( O
csrc O
) O
6 O
: O
backprop O
( O
l O
( O
wi O
, O
wj O
) O
) O
⊲ O
core O
method O
( O
§3.1 O
) O
7 O
: O
if O
it O
mod O
( O
t/k O
) O
= O
0 O
then O
⊲ O
self-learn O
( O
§3.2 O
) O
8 O
: O
d O
← O
reinduce O
( O
{ O
xi O
} O
, O
{ O
yi O
} O
) O
9 O
: O
end O
if O
10 O
: O
end O
for O
11 O
: O
end O
for O
sgns O
that O
replaces O
the O
output O
vectors O
in O
the O
source O
language O
with O
their O
translation O
equivalents O
in O
the O
target O
language O
, O
which O
act O
as O
anchor O
points O
( O
§3.1 O
) O
. O

section 3
id pdf2json/2021.acl-long.506.pdf.json
so O
as O
to O
make O
our O
method O
more O
robust O
to O
a O
weak O
initial O
dictionary O
, O
we O
incorporate O
a O
self-learning O
procedure O
that O
re-estimates O
the O
dictionary O
during O
training O
( O
§3.2 O
) O
, O
and O
perform O
iterative O
restarts O
( O
§3.3 O
) O
. O

section 3
id pdf2json/2021.acl-long.506.pdf.json
algorithm O
1 O
summarizes O
our O
method O
. O

section 4
id pdf2json/2021.acl-long.506.pdf.json
given O
a O
pair O
of O
words O
( O
wi O
, O
wj O
) O
co-occurring O
in O
the O
source O
language O
corpus O
, O
we O
define O
a O
generalized O
sgns O
objective O
as O
follows O
: O
l O
( O
wi O
, O
wj O
) O
= O
log O
σ O
( O
xwi O
· O
ctx O
( O
wj O
) O
) O
+ O
k O
∑ O
i=1 O
ewn∼pn O
( O
w O
) O
[ O
log O
σ O
( O
−xwi O
· O
ctx O
( O
wn O
) O
) O
] O
where O
k O
is O
the O
number O
of O
negative O
samples O
, O
pn O
( O
w O
) O
is O
the O
noise O
distribution O
, O
and O
ctx O
( O
wt O
) O
is O
a O
function O
that O
returns O
the O
output O
vector O
to O
be O
used O
for O
wt O
. O

section 4
id pdf2json/2021.acl-long.506.pdf.json
in O
regular O
sgns O
, O
this O
function O
would O
simply O
return O
the O
output O
vector O
of O
the O
corresponding O
word O
, O
so O
that O
ctx O
( O
wt O
) O
= O
x̃wt O
. O

section 4
id pdf2json/2021.acl-long.506.pdf.json
in O
contrast O
, O
our O
approach O
replaces O
it O
with O
its O
counterpart O
in O
the O
target O
language O
if O
wt O
is O
in O
the O
dictionary O
: O
ctx O
( O
wt O
) O
= O
{ O
ỹd O
( O
wt O
) O
if O
wt O
∈ O
d O
x̃wt O
otherwise O
during O
training O
, O
the O
replaced O
vectors O
{ O
ỹi O
} O
are O
kept O
frozen O
, O
acting O
as O
anchor O
points O
so O
that O
the O
resulting O
embeddings O
{ O
xi O
} O
are O
aligned O
with O
their O
counterparts O
{ O
yi O
} O
in O
the O
target O
language O
. O

section 5
id pdf2json/2021.acl-long.506.pdf.json
as O
shown O
later O
in O
our O
experiments O
, O
the O
performance O
of O
our O
basic O
method O
is O
largely O
dependent O
on O
the O
quality O
of O
the O
bilingual O
dictionary O
itself O
. O

section 5
id pdf2json/2021.acl-long.506.pdf.json
however O
, O
this O
is O
not O
different O
for O
conventional O
mapping O
methods O
, O
which O
also O
rely O
on O
a O
bilingual O
dictionary O
to O
align O
separately O
trained O
embeddings O
in O
different O
languages O
. O

section 5
id pdf2json/2021.acl-long.506.pdf.json
so O
as O
to O
overcome O
this O
issue O
, O
modern O
mapping O
approaches O
rely O
on O
self-learning O
, O
which O
alternates O
between O
aligning O
the O
embeddings O
and O
re-inducing O
the O
dictionary O
in O
an O
iterative O
fashion O
( O
artetxe O
et O
al. O
, O
2017 O
) O
. O

section 5
id pdf2json/2021.acl-long.506.pdf.json
we O
adopt O
a O
similar O
strategy O
, O
and O
re-induce O
the O
dictionary O
d O
a O
total O
of O
k O
times O
during O
training O
, O
where O
k O
is O
a O
hyperparameter O
. O

section 5
id pdf2json/2021.acl-long.506.pdf.json
to O
that O
end O
, O
we O
first O
obtain O
the O
translations O
for O
each O
source O
word O
using O
csls O
retrieval O
( O
conneau O
et O
al. O
, O
2018a O
) O
: O
d O
( O
i O
) O
= O
argmax O
j O
csls O
( O
xi O
, O
yj O
) O
having O
done O
that O
, O
we O
discard O
all O
entries O
that O
do O
not O
satisfy O
the O
following O
cyclic O
consistency O
condition:2 O
i O
∈ O
d O
⇐⇒ O
i O
= O
argmax O
k O
cos O
( O
xk O
, O
yargmaxj O
cos O
( O
xi O
, O
yj O
) O
) O

section 6
id pdf2json/2021.acl-long.506.pdf.json
while O
self-learning O
is O
able O
to O
improve O
a O
weak O
initial O
dictionary O
throughout O
training O
, O
the O
method O
is O
still O
susceptible O
to O
poor O
local O
optima O
. O

section 6
id pdf2json/2021.acl-long.506.pdf.json
this O
can O
be O
further O
exacerbated O
by O
the O
learning O
rate O
decay O
commonly O
used O
with O
sgns O
, O
which O
makes O
it O
increasingly O
difficult O
to O
recover O
from O
a O
poor O
solution O
as O
training O
progresses O
. O

section 6
id pdf2json/2021.acl-long.506.pdf.json
so O
as O
to O
overcome O
this O
issue O
, O
we O
sequentially O
run O
the O
entire O
sgns O
training O
r O
times O
, O
where O
r O
is O
a O
hyperparameter O
of O
the O
method O
. O

section 6
id pdf2json/2021.acl-long.506.pdf.json
we O
use O
the O
output O
from O
the O
previous O
run O
as O
the O
initial O
dictionary O
, O
but O
all O
the O
other O
parameters O
are O
reset O
and O
the O
full O
training O
process O
is O
run O
from O
scratch O
. O

section 7
id pdf2json/2021.acl-long.506.pdf.json
we O
next O
describe O
the O
systems O
explored O
in O
our O
experiments O
( O
§4.1 O
) O
, O
the O
data O
and O
procedure O
used O
to O
train O
them O
( O
§4.2 O
) O
, O
and O
the O
evaluation O
tasks O
( O
§4.3 O
) O
. O

section 8
id pdf2json/2021.acl-long.506.pdf.json
we O
compare O
3 O
model O
families O
in O
our O
experiments O
: O
offline O
mapping O
. O

section 8
id pdf2json/2021.acl-long.506.pdf.json
this O
approach O
learns O
monolingual O
embeddings O
in O
each O
of O
the O
languages O
separately O
, O
which O
are O
then O
mapped O
into O
a O
common O
space O
2we O
define O
our O
cyclic O
consistency O
condition O
over O
cosine O
similarity O
, O
which O
we O
found O
to O
be O
more O
restrictive O
than O
csls O
( O
in O
that O
it O
discards O
more O
entries O
) O
and O
work O
better O
in O
our O
preliminary O
experiments O
. O

section 8
id pdf2json/2021.acl-long.506.pdf.json
through O
a O
linear O
transformation O
. O

section 8
id pdf2json/2021.acl-long.506.pdf.json
we O
experiment O
with O
3 O
popular O
methods O
from O
the O
literature O
: O
muse O
( O
conneau O
et O
al. O
, O
2018a O
) O
, O
icp O
( O
hoshen O
and O
wolf O
, O
2018 O
) O
and O
vecmap O
( O
artetxe O
et O
al. O
, O
2018 O
) O
. O

section 8
id pdf2json/2021.acl-long.506.pdf.json
we O
use O
the O
original O
implementation O
of O
each O
method O
in O
their O
unsupervised O
mode O
with O
default O
hyperparameters O
. O

section 8
id pdf2json/2021.acl-long.506.pdf.json
joint O
learning O
+ O
offline O
mapping O
. O

section 8
id pdf2json/2021.acl-long.506.pdf.json
this O
approach O
jointly O
learns O
word O
embeddings O
for O
two O
languages O
over O
their O
concatenated O
monolingual O
corpora O
, O
where O
identical O
words O
act O
as O
anchor O
points O
( O
lample O
et O
al. O
, O
2018 O
) O
. O

section 8
id pdf2json/2021.acl-long.506.pdf.json
having O
done O
that O
, O
the O
vocabulary O
is O
partitioned O
into O
one O
shared O
and O
two O
language O
specific O
subsets O
, O
which O
are O
further O
aligned O
through O
an O
offline O
mapping O
method O
( O
wang O
et O
al. O
, O
2019 O
) O
. O

section 8
id pdf2json/2021.acl-long.506.pdf.json
we O
use O
the O
joint O
align O
implementation O
from O
the O
authors O
with O
default O
hyperparameters O
, O
which O
relies O
on O
fasttext O
for O
the O
joint O
learning O
step O
and O
muse O
for O
the O
mapping O
step.3 O
cross-lingual O
anchoring O
. O

section 8
id pdf2json/2021.acl-long.506.pdf.json
our O
proposed O
method O
, O
described O
in O
section O
3 O
. O

section 8
id pdf2json/2021.acl-long.506.pdf.json
we O
explore O
3 O
alternatives O
to O
obtain O
the O
initial O
dictionary O
: O
( O
i O
) O
identical O
words O
, O
where O
di O
= O
j O
if O
the O
ith O
source O
word O
and O
the O
jth O
target O
word O
are O
identically O
spelled O
, O
( O
ii O
) O
numerals O
, O
a O
subset O
of O
the O
former O
where O
identical O
words O
are O
further O
restricted O
to O
be O
sequences O
of O
digits O
, O
and O
( O
iii O
) O
unsupervised O
mapping O
, O
where O
we O
use O
the O
baseline O
vecmap O
system O
described O
above O
to O
induce O
the O
initial O
dictionary.4 O
the O
first O
two O
variants O
make O
assumptions O
on O
the O
writing O
system O
of O
different O
languages O
, O
which O
is O
usually O
regarded O
as O
a O
weak O
form O
of O
supervision O
( O
artetxe O
et O
al. O
, O
2017 O
; O
søgaard O
et O
al. O
, O
2018 O
) O
, O
whereas O
the O
latter O
is O
strictly O
unsupervised O
, O
yet O
dependant O
on O
an O
additional O
system O
from O
a O
different O
family O
. O

section 9
id pdf2json/2021.acl-long.506.pdf.json
we O
learn O
clwes O
between O
english O
and O
six O
other O
languages O
: O
german O
, O
spanish O
, O
french O
, O
finnish O
, O
russian O
and O
chinese O
. O

section 9
id pdf2json/2021.acl-long.506.pdf.json
following O
common O
practice O
, O
we O
3the O
original O
implementation O
only O
supports O
the O
supervised O
mode O
with O
rcsls O
mapping O
, O
so O
we O
modified O
it O
to O
use O
muse O
in O
the O
unsupervised O
setting O
as O
described O
in O
the O
original O
paper O
. O

section 9
id pdf2json/2021.acl-long.506.pdf.json
4we O
use O
csls O
retrieval O
and O
apply O
the O
cyclic O
consistency O
restriction O
as O
described O
in O
section O
3.2. O
use O
wikipedia O
as O
our O
training O
corpus,5 O
which O
we O
preprocessed O
using O
standard O
moses O
scripts O
, O
and O
restrict O
our O
vocabulary O
to O
the O
most O
frequent O
200k O
tokens O
per O
language O
. O

section 9
id pdf2json/2021.acl-long.506.pdf.json
in O
the O
case O
of O
chinese O
, O
word O
segmentation O
was O
done O
using O
the O
stanford O
segmenter O
. O

section 9
id pdf2json/2021.acl-long.506.pdf.json
table O
1 O
summarizes O
the O
statistics O
of O
the O
resulting O
corpora O
, O
while O
table O
2 O
reports O
the O
sizes O
of O
the O
initial O
dictionaries O
derived O
from O
it O
for O
our O
proposed O
method O
. O

section 9
id pdf2json/2021.acl-long.506.pdf.json
for O
joint O
align O
, O
we O
directly O
run O
the O
official O
implementation O
over O
our O
tokenized O
corpus O
as O
described O
above O
. O

section 9
id pdf2json/2021.acl-long.506.pdf.json
all O
the O
other O
systems O
take O
monolingual O
embeddings O
as O
input O
, O
which O
we O
learn O
using O
the O
sgns O
implementation O
in O
word2vec.6 O
for O
our O
proposed O
method O
, O
we O
set O
english O
as O
the O
target O
language O
, O
fix O
the O
corresponding O
monolingual O
embeddings O
, O
and O
learn O
aligned O
embeddings O
in O
the O
source O
language O
using O
our O
extension O
of O
sgns O
( O
§3 O
) O
.7 O
we O
set O
the O
number O
of O
restarts O
r O
to O
3 O
, O
the O
number O
of O
reinductions O
per O
restart O
k O
to O
50 O
, O
and O
the O
number O
of O
epochs O
to O
10 O
# O
trg O
sents O
# O
src O
sents O
, O
which O
makes O
sure O
that O
the O
source O
language O
gets O
a O
similar O
number O
of O
updates O
to O
the O
10 O
epochs O
done O
for O
english.8 O
for O
all O
the O
other O
hyperparameters O
, O
we O
use O
the O
same O
values O
as O
for O
the O
monolingual O
embeddings O
. O

section 9
id pdf2json/2021.acl-long.506.pdf.json
we O
made O
all O
of O
our O
development O
decisions O
based O
on O
preliminary O
experiments O
on O
english-finnish O
, O
without O
any O
systematic O
hyperparameter O
exploration O
. O

section 9
id pdf2json/2021.acl-long.506.pdf.json
our O
implementation O
runs O
on O
cpu O
, O
except O
for O
the O
dictionary O
reinduction O
steps O
, O
which O
run O
on O
a O
single O
gpu O
for O
around O
one O
5we O
extracted O
the O
corpus O
from O
the O
february O
2019 O
dump O
using O
the O
wikiextractor O
tool O
. O

section 9
id pdf2json/2021.acl-long.506.pdf.json
6we O
use O
10 O
negative O
samples O
, O
a O
sub-sampling O
threshold O
of O
1e-5 O
, O
300 O
dimensions O
, O
and O
10 O
epochs O
. O

section 9
id pdf2json/2021.acl-long.506.pdf.json
note O
that O
joint O
align O
also O
learns O
300-dimensional O
vectors O
, O
but O
runs O
fasttext O
with O
default O
hyperparameters O
under O
the O
hood O
. O

section 9
id pdf2json/2021.acl-long.506.pdf.json
7in O
our O
preliminary O
experiments O
, O
we O
observed O
our O
proposed O
method O
to O
be O
quite O
sensitive O
to O
which O
language O
is O
the O
source O
and O
which O
one O
is O
the O
target O
. O

section 9
id pdf2json/2021.acl-long.506.pdf.json
we O
find O
the O
language O
with O
the O
largest O
corpus O
to O
perform O
best O
as O
the O
target O
, O
presumably O
because O
the O
corresponding O
monolingual O
embeddings O
are O
better O
estimated O
, O
so O
it O
is O
more O
appropriate O
to O
fix O
them O
and O
learn O
aligned O
embeddings O
for O
the O
other O
language O
. O

section 9
id pdf2json/2021.acl-long.506.pdf.json
following O
this O
observation O
, O
we O
set O
english O
as O
the O
target O
language O
for O
all O
pairs O
, O
as O
it O
is O
the O
language O
with O
the O
largest O
corpus O
. O

section 9
id pdf2json/2021.acl-long.506.pdf.json
8for O
a O
fair O
comparison O
, O
we O
also O
tried O
using O
the O
same O
number O
of O
epochs O
for O
the O
baseline O
systems O
, O
but O
this O
performed O
worse O
than O
the O
reported O
numbers O
with O
10 O
epochs O
. O

section 9
id pdf2json/2021.acl-long.506.pdf.json
hour O
in O
total O
. O

section 10
id pdf2json/2021.acl-long.506.pdf.json
as O
described O
next O
, O
we O
evaluate O
our O
method O
on O
two O
tasks O
: O
bilingual O
lexicon O
induction O
( O
bli O
) O
and O
cross-lingual O
natural O
language O
inference O
( O
xnli O
) O
. O

section 10
id pdf2json/2021.acl-long.506.pdf.json
bli O
. O

section 10
id pdf2json/2021.acl-long.506.pdf.json
following O
common O
practice O
, O
we O
induce O
a O
bilingual O
dictionary O
through O
csls O
retrieval O
( O
conneau O
et O
al. O
, O
2018a O
) O
for O
each O
set O
of O
cross-lingual O
embeddings O
, O
and O
evaluate O
the O
precision O
at O
1 O
( O
p O
@ O
1 O
) O
with O
respect O
to O
the O
gold O
standard O
test O
dictionary O
from O
the O
muse O
dataset O
( O
conneau O
et O
al. O
, O
2018a O
) O
. O

section 10
id pdf2json/2021.acl-long.506.pdf.json
for O
the O
few O
out-of-vocabulary O
source O
words O
, O
we O
revert O
to O
copying O
as O
a O
back-off O
strategy,9 O
so O
our O
reported O
numbers O
are O
directly O
comparable O
to O
prior O
work O
in O
terms O
of O
coverage O
. O

section 10
id pdf2json/2021.acl-long.506.pdf.json
xnli O
. O

section 10
id pdf2json/2021.acl-long.506.pdf.json
we O
train O
an O
english O
natural O
language O
inference O
model O
on O
multinli O
( O
williams O
et O
al. O
, O
2018 O
) O
, O
and O
evaluate O
the O
zero-shot O
cross-lingual O
transfer O
performance O
on O
the O
xnli O
test O
set O
( O
conneau O
et O
al. O
, O
2018b O
) O
for O
the O
subset O
of O
our O
languages O
covered O
by O
it O
. O

section 10
id pdf2json/2021.acl-long.506.pdf.json
to O
that O
end O
, O
we O
follow O
glavaš O
et O
al O
. O

section 10
id pdf2json/2021.acl-long.506.pdf.json
( O
2019 O
) O
and O
train O
an O
enhanced O
sequential O
inference O
model O
( O
esim O
) O
on O
top O
of O
our O
original O
english O
embeddings O
, O
which O
are O
kept O
frozen O
during O
training O
. O

section 10
id pdf2json/2021.acl-long.506.pdf.json
at O
test O
time O
, O
we O
transfer O
into O
the O
rest O
of O
the O
languages O
by O
plugging O
in O
the O
corresponding O
aligned O
embeddings O
. O

section 10
id pdf2json/2021.acl-long.506.pdf.json
note O
that O
we O
use O
the O
exact O
same O
english O
model O
for O
our O
proposed O
method O
and O
the O
baseline O
muse O
and O
icp O
systems,10 O
which O
only O
differ O
in O
the O
set O
of O
aligned O
9this O
has O
a O
negligible O
impact O
in O
practice O
, O
as O
it O
accounts O
for O
less O
than O
1.4 O
% O
of O
the O
test O
cases O
. O

section 10
id pdf2json/2021.acl-long.506.pdf.json
moreover O
, O
all O
of O
our O
systems O
use O
the O
same O
underlying O
vocabulary O
, O
so O
they O
are O
affected O
in O
the O
exact O
same O
way O
. O

section 10
id pdf2json/2021.acl-long.506.pdf.json
10this O
is O
possible O
because O
they O
all O
fix O
the O
target O
language O
embeddings O
( O
english O
in O
this O
case O
) O
and O
align O
the O
embeddings O
embeddings O
used O
for O
cross-lingual O
transfer O
. O

section 10
id pdf2json/2021.acl-long.506.pdf.json
in O
contrast O
, O
vecmap O
and O
joint O
align O
also O
manipulate O
the O
target O
english O
embeddings O
, O
which O
would O
require O
training O
a O
separate O
model O
for O
each O
language O
pair O
, O
so O
we O
decide O
to O
exclude O
them O
from O
this O
set O
of O
experiments.11 O

section 11
id pdf2json/2021.acl-long.506.pdf.json
we O
next O
discuss O
our O
main O
results O
on O
bli O
( O
§5.1 O
) O
and O
xnli O
( O
§5.2 O
) O
, O
followed O
by O
our O
ablation O
study O
( O
§5.3 O
) O
and O
error O
analysis O
( O
§5.4 O
) O
on O
bli O
. O

section 12
id pdf2json/2021.acl-long.506.pdf.json
table O
3 O
comprises O
our O
main O
bli O
results O
. O

section 12
id pdf2json/2021.acl-long.506.pdf.json
we O
observe O
that O
our O
method O
obtains O
the O
best O
results O
in O
all O
directions O
( O
matched O
by O
vecmap O
in O
russianenglish O
) O
, O
outperforming O
the O
strongest O
baseline O
by O
2.4 O
points O
on O
average O
for O
the O
mapping O
based O
initialization O
. O

section 12
id pdf2json/2021.acl-long.506.pdf.json
our O
improvements O
are O
more O
pronounced O
in O
the O
backward O
direction O
( O
3.1 O
points O
on O
average O
) O
but O
still O
substantial O
in O
the O
forward O
direction O
( O
1.7 O
points O
on O
average O
) O
. O

section 12
id pdf2json/2021.acl-long.506.pdf.json
it O
is O
worth O
noting O
that O
some O
systems O
fail O
to O
converge O
to O
a O
good O
solution O
for O
the O
most O
challenging O
language O
pairs O
. O

section 12
id pdf2json/2021.acl-long.506.pdf.json
this O
includes O
our O
proposed O
method O
in O
the O
case O
of O
chinese-english O
when O
using O
the O
numeral-based O
initialization O
, O
which O
we O
attribute O
to O
the O
smaller O
size O
of O
the O
initial O
dictionary O
( O
only O
244 O
entries O
, O
see O
table O
2 O
) O
. O

section 12
id pdf2json/2021.acl-long.506.pdf.json
other O
than O
that O
, O
we O
observe O
that O
our O
approach O
obtains O
very O
similar O
results O
regardless O
of O
the O
initial O
dictionary O
. O

section 12
id pdf2json/2021.acl-long.506.pdf.json
quite O
remarkably O
, O
in O
the O
source O
language O
with O
them O
, O
either O
through O
mapping O
( O
muse O
, O
icp O
) O
or O
learning O
from O
scratch O
( O
ours O
) O
. O

section 12
id pdf2json/2021.acl-long.506.pdf.json
11in O
addition O
to O
the O
computational O
overhead O
, O
having O
separate O
models O
introduces O
some O
variance O
, O
while O
our O
comparison O
is O
more O
direct O
. O

section 12
id pdf2json/2021.acl-long.506.pdf.json
the O
variant O
using O
vecmap O
for O
initialization O
( O
mapping O
init O
) O
is O
substantially O
stronger O
than O
vecmap O
itself O
despite O
not O
using O
any O
additional O
training O
signal O
. O

section 12
id pdf2json/2021.acl-long.506.pdf.json
so O
as O
to O
put O
our O
results O
into O
perspective O
, O
table O
4 O
compares O
them O
to O
previous O
numbers O
reported O
in O
the O
literature O
. O

section 12
id pdf2json/2021.acl-long.506.pdf.json
note O
that O
the O
numbers O
are O
comparable O
in O
terms O
of O
coverage O
and O
all O
systems O
use O
wikipedia O
as O
the O
training O
corpus O
, O
although O
they O
might O
differ O
on O
the O
specific O
dump O
used O
and O
the O
preprocessing O
details.12 O
as O
it O
can O
be O
seen O
, O
our O
approach O
obtains O
the O
best O
results O
by O
a O
substantial O
margin.13 O

section 13
id pdf2json/2021.acl-long.506.pdf.json
we O
report O
our O
xnli O
results O
in O
table O
5 O
. O

section 13
id pdf2json/2021.acl-long.506.pdf.json
we O
observe O
that O
our O
method O
is O
competitive O
with O
the O
baseline O
12in O
particular O
, O
most O
mapping O
methods O
use O
the O
official O
wikipedia O
embeddings O
from O
fasttext O
. O

section 13
id pdf2json/2021.acl-long.506.pdf.json
unfortunately O
, O
the O
preprocessed O
corpus O
used O
to O
train O
these O
embeddings O
is O
not O
public O
, O
so O
works O
that O
explore O
other O
approaches O
, O
like O
ours O
, O
need O
to O
use O
their O
own O
pre-processed O
copy O
of O
wikipedia O
. O

section 13
id pdf2json/2021.acl-long.506.pdf.json
13artetxe O
et O
al O
. O

section 13
id pdf2json/2021.acl-long.506.pdf.json
( O
2019 O
) O
report O
even O
stronger O
results O
based O
on O
unsupervised O
machine O
translation O
instead O
of O
direct O
retrieval O
with O
clwes O
. O

section 13
id pdf2json/2021.acl-long.506.pdf.json
note O
, O
however O
, O
that O
their O
method O
still O
relies O
on O
cross-lingual O
embeddings O
to O
build O
the O
underlying O
phrase-table O
, O
so O
our O
improvements O
should O
be O
largely O
orthogonal O
to O
theirs O
. O

section 13
id pdf2json/2021.acl-long.506.pdf.json
mapping O
systems O
, O
achieving O
the O
best O
results O
on O
3 O
out O
of O
the O
5 O
transfer O
languages O
by O
a O
small O
margin O
. O

section 13
id pdf2json/2021.acl-long.506.pdf.json
nevertheless O
, O
it O
significantly O
lags O
behind O
muse O
on O
chinese O
, O
even O
if O
the O
exact O
same O
set O
of O
crosslingual O
embeddings O
performed O
better O
than O
muse O
at O
bli O
. O

section 13
id pdf2json/2021.acl-long.506.pdf.json
while O
striking O
, O
similar O
discrepancies O
between O
bli O
and O
xnli O
performance O
where O
also O
observed O
in O
previous O
studies O
( O
glavaš O
et O
al. O
, O
2019 O
) O
. O

section 13
id pdf2json/2021.acl-long.506.pdf.json
finally O
, O
we O
observe O
that O
the O
initial O
dictionary O
has O
a O
negligible O
impact O
in O
the O
performance O
of O
our O
proposed O
method O
, O
which O
supports O
the O
idea O
that O
our O
approach O
converges O
to O
a O
similar O
solution O
given O
any O
reasonable O
initialization O
. O

section 14
id pdf2json/2021.acl-long.506.pdf.json
so O
as O
to O
understand O
the O
role O
of O
self-learning O
and O
the O
iterative O
restarts O
in O
our O
approach O
, O
we O
perform O
an O
ablation O
study O
and O
report O
our O
results O
in O
table O
6 O
. O

section 14
id pdf2json/2021.acl-long.506.pdf.json
we O
observe O
that O
the O
contribution O
of O
these O
components O
is O
greatly O
dependant O
on O
the O
initial O
dictionary O
. O

section 14
id pdf2json/2021.acl-long.506.pdf.json
for O
the O
numeral O
initialization O
, O
the O
basic O
method O
works O
poorly O
, O
and O
both O
extensions O
bring O
large O
improvements O
. O

section 14
id pdf2json/2021.acl-long.506.pdf.json
in O
contrast O
, O
the O
identical O
initialization O
does O
not O
benefit O
from O
iterative O
restarts O
, O
but O
selflearning O
still O
plays O
a O
major O
role O
. O

section 14
id pdf2json/2021.acl-long.506.pdf.json
in O
the O
case O
of O
the O
mapping-based O
initialization O
, O
the O
basic O
method O
is O
already O
very O
competitive O
. O

section 14
id pdf2json/2021.acl-long.506.pdf.json
this O
suggests O
that O
both O
the O
self-learning O
and O
the O
iterative O
restarts O
are O
helpful O
to O
make O
the O
method O
more O
robust O
to O
a O
weak O
initialization O
, O
and O
have O
a O
minor O
impact O
otherwise O
. O

section 14
id pdf2json/2021.acl-long.506.pdf.json
in O
order O
to O
better O
understand O
the O
underlying O
learning O
dynamics O
, O
we O
analyze O
the O
learning O
curves O
for O
finnish-english O
in O
figure O
1 O
. O

section 14
id pdf2json/2021.acl-long.506.pdf.json
we O
observe O
that O
, O
when O
the O
initial O
dictionary O
is O
strong O
, O
our O
method O
surpasses O
the O
baseline O
and O
stabilizes O
early O
. O

section 14
id pdf2json/2021.acl-long.506.pdf.json
in O
contrast O
, O
convergence O
is O
much O
slower O
when O
using O
the O
weak O
numeral-based O
initialization O
, O
and O
the O
iterative O
restarts O
are O
critical O
to O
escape O
poor O
local O
optima O
. O

section 15
id pdf2json/2021.acl-long.506.pdf.json
so O
as O
to O
better O
understand O
where O
our O
improvements O
in O
bli O
are O
coming O
from O
, O
we O
perform O
an O
error O
analysis O
on O
the O
spanish-english O
direction O
. O

section 15
id pdf2json/2021.acl-long.506.pdf.json
to O
that O
end O
, O
we O
manually O
inspect O
the O
69 O
instances O
for O
which O
our O
method O
( O
with O
mapping-based O
initialization O
) O
produced O
a O
correct O
translation O
while O
vecmap O
failed O
according O
to O
the O
gold O
standard O
, O
as O
well O
as O
the O
26 O
instances O
for O
which O
the O
opposite O
was O
true O
. O

section 15
id pdf2json/2021.acl-long.506.pdf.json
we O
then O
categorize O
these O
errors O
into O
several O
types O
, O
which O
are O
summarized O
in O
table O
7 O
. O

section 15
id pdf2json/2021.acl-long.506.pdf.json
we O
observe O
that O
, O
in O
52.6 O
% O
of O
the O
95 O
analyzed O
instances O
, O
the O
translation O
produced O
by O
our O
method O
is O
identical O
to O
the O
source O
word O
, O
while O
this O
percentage O
goes O
down O
to O
4.2 O
% O
for O
vecmap O
. O

section 15
id pdf2json/2021.acl-long.506.pdf.json
this O
tendency O
of O
our O
approach O
to O
copy O
its O
input O
is O
striking O
, O
as O
the O
model O
has O
no O
notion O
about O
the O
words O
being O
identically O
spelled.14 O
a O
large O
portion O
of O
these O
cases O
14the O
variants O
of O
our O
system O
with O
identical O
or O
numeral O
initialization O
do O
indirectly O
see O
this O
signal O
, O
but O
the O
one O
analyzed O
here O
is O
initialized O
with O
the O
vecmap O
mapping O
. O

section 15
id pdf2json/2021.acl-long.506.pdf.json
correspond O
to O
named O
entities O
, O
where O
copying O
is O
the O
right O
behavior O
, O
while O
vecmap O
outputs O
a O
different O
proper O
noun O
. O

section 15
id pdf2json/2021.acl-long.506.pdf.json
there O
are O
also O
some O
instances O
where O
the O
input O
word O
is O
in O
the O
target O
language,15 O
which O
can O
be O
considered O
an O
artifact O
of O
the O
dataset O
, O
but O
copying O
also O
seems O
the O
most O
reasonable O
behavior O
in O
these O
cases O
. O

section 15
id pdf2json/2021.acl-long.506.pdf.json
finally O
, O
there O
are O
also O
a O
few O
cases O
where O
the O
input O
word O
is O
present O
in O
the O
target O
vocabulary O
, O
which O
is O
selected O
by O
our O
method O
and O
counted O
as O
an O
error O
. O

section 15
id pdf2json/2021.acl-long.506.pdf.json
once O
again O
, O
we O
consider O
these O
to O
be O
an O
artifact O
of O
the O
dataset O
, O
as O
copying O
seems O
a O
reasonable O
choice O
if O
the O
input O
word O
is O
considered O
to O
be O
part O
of O
the O
target O
language O
vocabulary O
. O

section 15
id pdf2json/2021.acl-long.506.pdf.json
the O
remaining O
cases O
where O
neither O
method O
copies O
mostly O
correspond O
to O
common O
errors O
, O
where O
one O
of O
the O
systems O
( O
most O
often O
vecmap O
) O
outputs O
a O
semantically O
related O
but O
incorrect O
translation O
. O

section 15
id pdf2json/2021.acl-long.506.pdf.json
however O
, O
there O
are O
also O
a O
few O
instances O
where O
both O
translations O
are O
correct O
, O
but O
one O
of O
them O
is O
missing O
in O
the O
gold O
standard O
. O

section 15
id pdf2json/2021.acl-long.506.pdf.json
with O
the O
aim O
to O
understand O
the O
impact O
of O
identical O
words O
in O
our O
original O
results O
, O
we O
re-evaluated O
the O
systems O
using O
a O
filtered O
version O
of O
the O
muse O
gold O
standard O
dictionaries O
, O
where O
we O
removed O
all O
source O
words O
that O
were O
included O
in O
the O
set O
of O
candidate O
translations O
. O

section 15
id pdf2json/2021.acl-long.506.pdf.json
in O
order O
to O
be O
fair O
, O
we O
filtered O
out O
identical O
words O
from O
the O
output O
of O
the O
system O
, O
reverting O
to O
the O
second O
highest-ranked O
translation O
whenever O
the O
first O
one O
is O
identical O
to O
the O
source O
word O
. O

section 15
id pdf2json/2021.acl-long.506.pdf.json
the O
results O
for O
the O
strongest O
system O
in O
each O
family O
are O
shown O
in O
table O
8 O
. O

section 15
id pdf2json/2021.acl-long.506.pdf.json
even O
if O
the O
margin O
of O
improvement O
is O
reduced O
compared O
to O
table O
3 O
, O
the O
best O
results O
are O
still O
obtained O
by O
our O
proposed O
method O
, O
bringing O
an O
average O
improvement O
15english O
words O
will O
often O
appear O
in O
other O
languages O
as O
part O
of O
named O
entities O
( O
e.g. O
, O
“ O
pink O
” O
as O
part O
of O
“ O
pink O
floyd O
” O
) O
, O
which O
explains O
the O
presence O
of O
such O
words O
in O
the O
spanish O
vocabulary O
. O

section 15
id pdf2json/2021.acl-long.506.pdf.json
of O
1.1 O
points O
. O

section 15
id pdf2json/2021.acl-long.506.pdf.json
it O
is O
also O
worth O
noting O
that O
joint O
align O
, O
which O
shares O
a O
portion O
of O
the O
vocabulary O
for O
both O
languages O
( O
and O
will O
thus O
translate O
all O
words O
in O
the O
shared O
vocabulary O
identically O
) O
, O
suffers O
a O
large O
drop O
in O
performance O
. O

section 15
id pdf2json/2021.acl-long.506.pdf.json
this O
highlights O
the O
importance O
of O
accompanying O
quantitative O
bli O
evaluation O
with O
an O
error O
analysis O
as O
urged O
by O
previous O
studies O
( O
kementchedjhieva O
et O
al. O
, O
2019 O
) O
. O

section 16
id pdf2json/2021.acl-long.506.pdf.json
our O
approach O
for O
learning O
clwes O
addresses O
the O
main O
limitations O
of O
both O
offline O
mapping O
and O
joint O
learning O
methods O
. O

section 16
id pdf2json/2021.acl-long.506.pdf.json
different O
from O
mapping O
approaches O
, O
it O
does O
not O
suffer O
from O
structural O
mismatches O
arising O
from O
independently O
training O
embeddings O
in O
different O
languages O
, O
as O
it O
works O
by O
constraining O
the O
learning O
of O
the O
source O
embeddings O
so O
they O
are O
aligned O
with O
the O
target O
ones O
. O

section 16
id pdf2json/2021.acl-long.506.pdf.json
at O
the O
same O
time O
, O
unlike O
previous O
joint O
methods O
, O
our O
system O
can O
work O
without O
any O
parallel O
resources O
, O
relying O
on O
numerals O
, O
identical O
words O
or O
an O
existing O
mapping O
method O
for O
the O
initialization O
. O

section 16
id pdf2json/2021.acl-long.506.pdf.json
we O
achieve O
this O
by O
combining O
cross-lingual O
anchoring O
with O
self-learning O
and O
iterative O
restarts O
. O

section 16
id pdf2json/2021.acl-long.506.pdf.json
while O
recent O
research O
on O
clwes O
has O
been O
dominated O
by O
mapping O
approaches O
, O
our O
work O
shows O
that O
the O
fundamental O
techniques O
that O
popularized O
these O
methods O
( O
e.g. O
, O
the O
use O
of O
self-learning O
to O
relax O
the O
need O
for O
crosslingual O
supervision O
) O
can O
also O
be O
effective O
beyond O
this O
paradigm O
. O

section 16
id pdf2json/2021.acl-long.506.pdf.json
despite O
its O
simplicity O
, O
our O
experiments O
on O
bli O
show O
the O
superiority O
of O
our O
method O
when O
compared O
to O
previous O
mapping O
systems O
. O

section 16
id pdf2json/2021.acl-long.506.pdf.json
we O
complement O
these O
results O
with O
additional O
experiments O
on O
a O
downstream O
task O
, O
where O
our O
method O
obtains O
competitive O
results O
, O
as O
well O
as O
an O
ablation O
study O
and O
a O
systematic O
error O
analysis O
. O

section 16
id pdf2json/2021.acl-long.506.pdf.json
we O
identify O
a O
striking O
tendency O
of O
our O
method O
to O
translate O
words O
identically O
, O
even O
if O
it O
has O
no O
notion O
of O
the O
words O
being O
identically O
spelled O
. O

section 16
id pdf2json/2021.acl-long.506.pdf.json
thanks O
to O
this O
, O
our O
method O
is O
particularly O
strong O
at O
translating O
named O
entities O
, O
but O
we O
show O
that O
our O
improvements O
are O
not O
limited O
to O
this O
phenomenon O
. O

section 16
id pdf2json/2021.acl-long.506.pdf.json
these O
insights O
confirm O
the O
value O
of O
accompanying O
quantitative O
results O
on O
bli O
with O
qualitative O
evaluation O
( O
kementchedjhieva O
et O
al. O
, O
2019 O
) O
and/or O
other O
tasks O
( O
glavaš O
et O
al. O
, O
2019 O
) O
. O

section 16
id pdf2json/2021.acl-long.506.pdf.json
in O
the O
future O
, O
we O
would O
like O
to O
further O
explore O
clwe O
methods O
that O
go O
beyond O
the O
currently O
dominant O
mapping O
paradigm O
. O

section 16
id pdf2json/2021.acl-long.506.pdf.json
in O
particular O
, O
we O
would O
like O
to O
remove O
the O
requirement O
of O
a O
seed O
dictionary O
altogether O
by O
using O
adversarial O
learning O
, O
and O
explore O
more O
elaborated O
context O
translation O
and O
dictionary O
re-induction O
schemes O
. O

section 17
id pdf2json/2021.acl-long.506.pdf.json
aitor O
ormazabal O
, O
aitor O
soroa O
, O
gorka O
labaka O
and O
eneko O
agirre O
were O
supported O
by O
the O
basque O
government O
( O
excellence O
research O
group O
it1343-19 O
and O
deeptext O
project O
kk-2020/00088 O
) O
, O
project O
bigknowledge O
( O
ayudas O
fundación O
bbva O
a O
equipos O
de O
investigación O
cientı́fica O
2018 O
) O
and O
the O
spanish O
mineco O
( O
project O
domino O
pgc2018-102041b-i00 O
mciu/aei/feder O
, O
ue O
) O
. O

section 17
id pdf2json/2021.acl-long.506.pdf.json
aitor O
ormazabal O
was O
supported O
by O
a O
doctoral O
grant O
from O
the O
spanish O
mecd O
. O

section TITLE
id pdf2json/2021.acl-long.404.pdf.json
a O
cognitive O
regularizer O
for O
language O
modeling O

section ABSTRACT
id pdf2json/2021.acl-long.404.pdf.json
the O
uniform O
information O
density O
( O
uid O
) O
hypothesis O
, O
which O
posits O
that O
speakers O
behaving O
optimally O
tend O
to O
distribute O
information O
uniformly O
across O
a O
linguistic O
signal O
, O
has O
gained O
traction O
in O
psycholinguistics O
as O
an O
explanation O
for O
certain O
syntactic O
, O
morphological O
, O
and O
prosodic O
choices O
. O

section ABSTRACT
id pdf2json/2021.acl-long.404.pdf.json
in O
this O
work O
, O
we O
explore O
whether O
the O
uid O
hypothesis O
can O
be O
operationalized O
as O
an O
inductive O
bias O
for O
statistical O
language O
modeling O
. O

section ABSTRACT
id pdf2json/2021.acl-long.404.pdf.json
specifically O
, O
we O
augment O
the O
canonical O
mle O
objective O
for O
training O
language O
models O
with O
a O
regularizer O
that O
encodes O
uid O
. O

section ABSTRACT
id pdf2json/2021.acl-long.404.pdf.json
in O
experiments O
on O
ten O
languages O
spanning O
five O
language O
families O
, O
we O
find O
that O
using O
uid O
regularization O
consistently O
improves O
perplexity O
in O
language O
models O
, O
having O
a O
larger O
effect O
when O
training O
data O
is O
limited O
. O

section ABSTRACT
id pdf2json/2021.acl-long.404.pdf.json
moreover O
, O
via O
an O
analysis O
of O
generated O
sequences O
, O
we O
find O
that O
uid-regularized O
language O
models O
have O
other O
desirable O
properties O
, O
e.g. O
, O
they O
generate O
text O
that O
is O
more O
lexically O
diverse O
. O

section ABSTRACT
id pdf2json/2021.acl-long.404.pdf.json
our O
results O
not O
only O
suggest O
that O
uid O
is O
a O
reasonable O
inductive O
bias O
for O
language O
modeling O
, O
but O
also O
provide O
an O
alternative O
validation O
of O
the O
uid O
hypothesis O
using O
modern-day O
nlp O
tools O
. O

section 0
id pdf2json/2021.acl-long.404.pdf.json
proceedings O
of O
the O
59th O
annual O
meeting O
of O
the O
association O
for O
computational O
linguistics O
and O
the O
11th O
international O
joint O
conference O
on O
natural O
language O
processing O
, O
pages O
5191–5202 O
august O
1–6 O
, O
2021 O
. O

section 0
id pdf2json/2021.acl-long.404.pdf.json
©2021 O
association O
for O
computational O
linguistics O
5191 O

section 1
id pdf2json/2021.acl-long.404.pdf.json
language O
has O
been O
hypothesized O
to O
follow O
certain O
information-theoretic O
constraints O
. O

section 1
id pdf2json/2021.acl-long.404.pdf.json
one O
of O
the O
most O
famous O
of O
these O
constraints O
is O
the O
uniform O
information O
density O
( O
uid O
) O
hypothesis O
( O
fenk O
and O
fenk O
, O
1980 O
; O
jaeger O
, O
2010 O
) O
, O
which O
states O
that O
, O
subject O
to O
the O
rules O
of O
the O
grammar O
, O
speakers O
aim O
to O
distribute O
information O
density O
across O
a O
linguistic O
signal O
as O
uniformly O
as O
possible O
. O

section 1
id pdf2json/2021.acl-long.404.pdf.json
that O
is O
, O
speakers O
behaving O
optimally O
should O
structure O
their O
utterances O
such O
that O
the O
differences O
between O
the O
peaks O
and O
troughs O
in O
information O
are O
minimized O
. O

section 1
id pdf2json/2021.acl-long.404.pdf.json
in O
the O
psycholinguistics O
literature O
, O
the O
uid O
hypothesis O
has O
been O
used O
to O
explain O
a O
variety O
of O
linguistic O
phenomena O
ranging O
from O
how O
we O
shorten O
the O
phonetic O
duration O
of O
more-predictable O
linguistic O
units O
( O
aylett O
and O
turk O
, O
2004 O
) O
to O
when O
we O
decide O
to O
use O
optional O
syntactic O
relativizers O
( O
levy O
and O
jaeger O
, O
2007 O
) O
, O
among O
other O
phenomena O
( O
bell O
et O
al. O
, O
2003 O
; O
frank O
and O
jaeger O
, O
2008 O
) O
. O

section 1
id pdf2json/2021.acl-long.404.pdf.json
these O
studies O
often O
use O
language O
models O
to O
estimate O
the O
information O
density O
of O
linguistic O
units O
, O
taking O
observations O
of O
low O
variation O
of O
information O
density O
in O
well-formed O
utterances O
as O
evidence O
for O
the O
uid O
hypothesis O
. O

section 1
id pdf2json/2021.acl-long.404.pdf.json
in O
this O
paper O
, O
we O
propose O
a O
new O
experimental O
paradigm O
that O
uses O
modern-day O
nlp O
models O
to O
test O
the O
uid O
hypothesis O
. O

section 1
id pdf2json/2021.acl-long.404.pdf.json
whereas O
prior O
work O
has O
used O
language O
modeling O
as O
a O
tool O
for O
observing O
uid,1 O
we O
explore O
the O
converse—can O
uid O
be O
used O
as O
a O
tool O
to O
train O
better O
language O
models O
? O

section 1
id pdf2json/2021.acl-long.404.pdf.json
specifically O
, O
if O
the O
uid O
hypothesis O
is O
true O
, O
then O
we O
should O
be O
able O
to O
operationalize O
uid O
as O
a O
regularizer O
to O
help O
train O
language O
models O
. O

section 1
id pdf2json/2021.acl-long.404.pdf.json
moreover O
, O
observing O
lower O
perplexity O
in O
language O
models O
trained O
with O
this O
regularization O
would O
imply O
that O
the O
concept O
of O
uid O
is O
a O
good O
inductive O
bias O
for O
language O
modeling O
, O
thereby O
providing O
a O
new O
type O
of O
evidence O
for O
the O
uid O
hypothesis O
at O
scale O
. O

section 1
id pdf2json/2021.acl-long.404.pdf.json
in O
experiments O
, O
we O
indeed O
find O
such O
evidence O
: O
across O
a O
variety O
of O
languages O
and O
dataset O
sizes O
, O
uid O
regularization O
consistently O
improves O
performance O
, O
having O
a O
larger O
effect O
when O
training O
data O
is O
limited O
. O

section 1
id pdf2json/2021.acl-long.404.pdf.json
moreover O
, O
we O
observe O
that—in O
comparison O
with O
their O
unregularized O
counterparts—uidregularized O
language O
models O
are O
( O
1 O
) O
higher O
entropy O
while O
achieving O
the O
same O
( O
or O
better O
) O
test O
set O
perplexity O
and O
( O
2 O
) O
generate O
text O
that O
is O
longer O
and O
more O
lexically O
diverse O
. O

section 1
id pdf2json/2021.acl-long.404.pdf.json
our O
work O
is O
the O
first O
to O
explore O
the O
interaction O
between O
uid O
and O
training O
modernday O
neural O
language O
models O
, O
and O
our O
findings—that O
a O
cognitively O
motivated O
objective O
can O
improve O
language O
model O
performance—open O
up O
new O
avenues O
for O
testing O
other O
psycholinguistic O
hypotheses O
in O
a O
similar O
framework O
. O

section 2
id pdf2json/2021.acl-long.404.pdf.json
the O
task O
of O
language O
modeling O
aims O
to O
estimate O
a O
model O
of O
the O
probability O
of O
observing O
any O
given O
string O
in O
( O
a O
subset O
of O
) O
natural O
language O
. O

section 2
id pdf2json/2021.acl-long.404.pdf.json
formally O
, O
a O
language O
model O
p O
is O
an O
( O
unconditional O
) O
probability O
distribution O
over O
sequences O
of O
words O
w O
= O
〈w1 O
, O
w2 O
, O
. O

section 2
id pdf2json/2021.acl-long.404.pdf.json
. O

section 2
id pdf2json/2021.acl-long.404.pdf.json
. O

section 2
id pdf2json/2021.acl-long.404.pdf.json
〉 O
, O
where O
w O
consists O
of O
tokens O
from O
some O
vocabulary O
and O
begins O
and O
ends O
with O
special O
tokens O
bos O
and O
eos O
, O
respectively O
. O

section 2
id pdf2json/2021.acl-long.404.pdf.json
today O
’ O
s O
language O
models O
are O
typically O
parameterized O
by O
neural O
networks O
( O
e.g. O
, O
transformers O
( O
vaswani O
et O
al. O
, O
2017 O
) O
) O
, O
that O
follow O
a O
localnormalization O
scheme O
. O

section 2
id pdf2json/2021.acl-long.404.pdf.json
specifically O
, O
the O
model O
provides O
a O
conditional O
distribution O
over O
the O
vocabulary O
at O
each O
time O
step O
; O
we O
can O
then O
compute O
the O
proba- O
1on O
its O
own O
, O
the O
term O
‘ O
uid O
’ O
is O
formally O
an O
attribute O
of O
a O
linguistic O
signal O
. O

section 2
id pdf2json/2021.acl-long.404.pdf.json
we O
also O
use O
it O
throughout O
this O
work O
to O
refer O
to O
the O
concept O
that O
uid O
is O
a O
desirable O
property O
. O

section 2
id pdf2json/2021.acl-long.404.pdf.json
bility O
of O
an O
entire O
sequence O
w O
as O
: O
pθ O
( O
w O
) O
= O
|w|∏ O
t=1 O
pθ O
( O
wt O
| O
w O
< O
t O
) O
( O
1 O
) O
where O
θ O
are O
the O
parameters O
of O
the O
model O
and O
we O
use O
w O
< O
t O
to O
represent O
the O
first O
t O
− O
1 O
tokens O
of O
w. O
parameters O
are O
estimated O
by O
optimizing O
over O
some O
objective O
l O
( O
θ O
) O
. O

section 2
id pdf2json/2021.acl-long.404.pdf.json
the O
standard O
objective O
for O
language O
modeling O
is O
the O
negative O
log-likelihood O
of O
a O
datasetw O
under O
the O
model O
: O
l O
( O
θ O
) O
= O
− O
∑ O
w∈w O
log O
pθ O
( O
w O
) O
( O
2 O
) O
subsequently O
, O
we O
drop O
explicit O
dependence O
on O
θ O
when O
it O
is O
obvious O
from O
context O
. O

section 2
id pdf2json/2021.acl-long.404.pdf.json
to O
assess O
the O
goodness O
of O
fit O
of O
a O
model O
p O
, O
we O
typically O
evaluate O
its O
perplexity O
on O
some O
held-out O
datasetwtest O
, O
where O
perplexity O
( O
ppl O
) O
is O
defined O
as O
ppl O
( O
p O
) O
= O
exp O
( O
− O
∑ O
w∈wtest O
1 O
|w| O
log O
p O
( O
w O
) O
) O
( O
3 O
) O
note O
that O
under O
this O
definition O
of O
perplexity O
, O
our O
evaluation O
metric O
is O
slightly O
different O
than O
the O
training O
objective O
; O
the O
former O
computes O
an O
average O
over O
each O
sequence O
while O
the O
later O
treats O
all O
tokens O
equally O
, O
regardless O
of O
the O
length O
of O
the O
sequence O
in O
which O
they O
are O
present O
. O

section 3
id pdf2json/2021.acl-long.404.pdf.json
communication O
via O
natural O
language O
is O
a O
complicated O
and O
nuanced O
process O
that O
takes O
place O
under O
a O
host O
of O
cognitive O
and O
environmental O
constraints O
. O

section 3
id pdf2json/2021.acl-long.404.pdf.json
as O
a O
result O
, O
speakers O
have O
to O
make O
( O
perhaps O
subconscious O
) O
choices O
to O
best O
navigate O
this O
communicative O
dance O
. O

section 3
id pdf2json/2021.acl-long.404.pdf.json
a O
rational O
speaker O
would O
use O
these O
choices O
to O
optimize O
the O
communicative O
properties O
of O
their O
utterances O
. O

section 3
id pdf2json/2021.acl-long.404.pdf.json
one O
such O
locus O
of O
optimization O
is O
outlined O
by O
the O
uniform O
information O
density O
( O
uid O
) O
hypothesis O
. O

section 4
id pdf2json/2021.acl-long.404.pdf.json
at O
its O
core O
, O
the O
uid O
hypothesis O
aims O
to O
explain O
certain O
phenomena O
in O
human O
language O
processing O
using O
an O
information-theoretic O
approach O
: O
we O
can O
view O
language O
as O
a O
transfer O
of O
information O
, O
which O
is O
transmitted O
with O
a O
certain O
density O
through O
a O
communication O
channel O
. O

section 4
id pdf2json/2021.acl-long.404.pdf.json
the O
uid O
hypothesis O
posits O
that O
speakers O
that O
behave O
optimally O
will O
structure O
their O
utterances O
to O
avoid O
peaks O
and O
troughs O
in O
this O
information O
density O
( O
aylett O
and O
turk O
, O
2004 O
; O
levy O
and O
jaeger O
, O
2007 O
; O
jaeger O
, O
2010 O
) O
. O

section 4
id pdf2json/2021.acl-long.404.pdf.json
more O
formally O
stated O
: O
“ O
within O
the O
bounds O
defined O
by O
grammar O
, O
speakers O
prefer O
utterances O
that O
distribute O
information O
uniformly O
across O
the O
signal O
( O
information O
density O
) O
. O

section 4
id pdf2json/2021.acl-long.404.pdf.json
where O
speakers O
have O
a O
choice O
between O
several O
variants O
to O
encode O
their O
message O
, O
they O
prefer O
the O
variant O
with O
more-uniform O
information O
density O
( O
ceteris O
paribus O
) O
” O
( O
jaeger O
, O
2010 O
) O
. O

section 5
id pdf2json/2021.acl-long.404.pdf.json
to O
better O
understand O
the O
uid O
hypothesis O
, O
consider O
the O
concrete O
example O
of O
syntactic O
reduction O
( O
thatmentioning O
) O
from O
jaeger O
( O
2010 O
) O
, O
which O
we O
show O
graphically O
in O
figure O
1 O
and O
also O
describe O
below O
. O

section 5
id pdf2json/2021.acl-long.404.pdf.json
ex O
. O

section 5
id pdf2json/2021.acl-long.404.pdf.json
a O
. O

section 5
id pdf2json/2021.acl-long.404.pdf.json
my O
boss O
confirmed O
[ O
that O
] O
we O
are O
crazy O
. O

section 5
id pdf2json/2021.acl-long.404.pdf.json
ex O
. O

section 5
id pdf2json/2021.acl-long.404.pdf.json
b O
. O

section 5
id pdf2json/2021.acl-long.404.pdf.json
my O
boss O
thinks O
[ O
that O
] O
i O
am O
crazy O
. O

section 5
id pdf2json/2021.acl-long.404.pdf.json
in O
both O
these O
sentences O
, O
the O
use O
of O
the O
relativizer O
that O
is O
syntactically O
optional—at O
the O
onset O
of O
a O
relative O
clause O
( O
rc O
) O
, O
speakers O
can O
, O
but O
do O
not O
have O
to O
, O
include O
the O
relativizer O
. O

section 5
id pdf2json/2021.acl-long.404.pdf.json
many O
speakers O
, O
however O
, O
would O
argue O
that O
the O
sentence O
flows O
better O
with O
the O
relativizer O
included O
in O
example O
a O
and O
the O
relativizer O
omitted O
in O
example O
b O
. O

section 5
id pdf2json/2021.acl-long.404.pdf.json
the O
uid O
hypothesis O
provides O
a O
potential O
explanation O
for O
this O
phenomenon O
. O

section 5
id pdf2json/2021.acl-long.404.pdf.json
when O
a O
rc O
is O
used O
without O
a O
relativizer O
, O
the O
first O
word O
of O
the O
rc O
conveys O
two O
pieces O
of O
information O
: O
both O
the O
onset O
of O
the O
rc O
, O
as O
well O
as O
part O
of O
the O
rc O
’ O
s O
internal O
contents O
. O

section 5
id pdf2json/2021.acl-long.404.pdf.json
in O
example O
a O
, O
many O
speakers O
would O
find O
that O
the O
information O
density O
of O
the O
first O
word O
in O
the O
rc O
, O
we O
, O
is O
high O
, O
and O
so O
adding O
in O
the O
relative O
clause O
distributes O
the O
information O
over O
two O
words O
, O
making O
it O
easier O
to O
parse O
. O

section 5
id pdf2json/2021.acl-long.404.pdf.json
in O
example O
b O
, O
the O
information O
density O
of O
the O
first O
word O
in O
the O
rc O
, O
i O
, O
is O
lower O
relatively O
, O
and O
so O
we O
do O
not O
need O
to O
( O
or O
it O
is O
not O
as O
beneficial O
to O
) O
include O
the O
relativizer O
. O

section 6
id pdf2json/2021.acl-long.404.pdf.json
now O
that O
we O
better O
understand O
what O
the O
uid O
hypothesis O
attempts O
to O
explain O
, O
how O
might O
we O
operationalize O
uid O
and O
find O
quantitative O
evidence O
of O
the O
pressure O
for O
it O
in O
language O
? O

section 6
id pdf2json/2021.acl-long.404.pdf.json
first O
, O
to O
quantify O
the O
amount O
of O
information O
conveyed O
by O
a O
word O
, O
we O
turn O
to O
the O
most O
basic O
information-theoretic O
definition O
: O
the O
information O
conveyed O
by O
a O
word O
w O
in O
context O
is O
its O
shannon O
information O
content O
( O
shannon O
, O
1948 O
) O
, O
also O
called O
surprisal O
. O

section 6
id pdf2json/2021.acl-long.404.pdf.json
ideally O
, O
this O
surprisal O
would O
be O
measured O
using O
the O
“ O
true O
” O
distribution O
over O
human O
language O
. O

section 6
id pdf2json/2021.acl-long.404.pdf.json
because O
we O
do O
not O
have O
access O
to O
such O
a O
distribution O
, O
we O
often O
estimate O
it O
using O
a O
statistical O
language O
model O
. O

section 6
id pdf2json/2021.acl-long.404.pdf.json
that O
is O
, O
given O
a O
statistical O
language O
model O
p O
, O
which O
estimates O
the O
probability O
of O
a O
word O
given O
its O
context O
, O
the O
surprisal O
u O
( O
wt O
) O
of O
word O
wt O
is O
defined O
as O
the O
following O
: O
u O
( O
wt O
) O
= O
− O
log O
p O
( O
wt O
| O
w O
< O
t O
) O
( O
4 O
) O
this O
setup O
provides O
a O
natural O
approach O
to O
exploring O
how O
uid O
might O
manifest—if O
the O
uid O
hypothesis O
is O
true O
, O
then O
we O
should O
observe O
that O
variation O
in O
surprisal O
, O
as O
estimated O
by O
a O
language O
model O
, O
is O
minimized O
in O
natural O
language O
. O

section 6
id pdf2json/2021.acl-long.404.pdf.json
using O
this O
approach O
, O
prior O
work O
has O
accumulated O
evidence O
for O
uid O
across O
various O
levels O
of O
linguistic O
representation O
( O
pluymaekers O
et O
al. O
, O
2005 O
; O
bell O
et O
al. O
, O
2009 O
, O
inter O
alia O
) O
. O

section 6
id pdf2json/2021.acl-long.404.pdf.json
as O
some O
of O
the O
earliest O
examples O
, O
aylett O
and O
turk O
( O
2004 O
) O
showed O
that O
linguistic O
units O
that O
had O
high O
surprisal O
according O
to O
a O
tri-gram O
language O
model O
were O
uttered O
with O
longer O
syllable O
durations O
, O
and O
levy O
and O
jaeger O
( O
2007 O
) O
found O
that O
for O
rcs O
in O
which O
the O
first O
word O
had O
higher O
surprisal O
, O
relativizers O
were O
more O
likely O
to O
be O
used O
in O
the O
rc O
during O
actual O
speech O
. O

section 6
id pdf2json/2021.acl-long.404.pdf.json
further O
examples O
are O
given O
in O
our O
related O
work O
section O
( O
§7 O
) O
. O

section 7
id pdf2json/2021.acl-long.404.pdf.json
while O
prior O
work O
has O
shown O
evidence O
that O
uid O
can O
help O
explain O
many O
of O
the O
choices O
we O
make O
when O
generating O
language O
, O
to O
the O
best O
of O
our O
knowledge O
, O
operationalizations O
of O
uid O
have O
not O
been O
explicitly O
employed O
as O
part O
of O
the O
training O
objective O
in O
modern-day O
nlp O
models O
. O

section 7
id pdf2json/2021.acl-long.404.pdf.json
this O
raises O
the O
simple O
question O
that O
is O
central O
to O
our O
paper O
: O
can O
uid O
serve O
as O
an O
inductive O
bias O
for O
training O
statistical O
language O
models O
? O

section 7
id pdf2json/2021.acl-long.404.pdf.json
in O
an O
effort O
to O
answer O
this O
question O
, O
we O
present O
a O
scheme O
for O
incorporating O
operationalizations O
of O
uid O
into O
the O
language O
model O
training O
objective O
. O

section 7
id pdf2json/2021.acl-long.404.pdf.json
formally O
, O
we O
augment O
the O
canonical O
maximum O
likelihood O
estimation O
objective2 O
in O
eq O
. O

section 7
id pdf2json/2021.acl-long.404.pdf.json
( O
2 O
) O
with O
uid O
2note O
that O
the O
maximum O
likelihood O
estimation O
objective O
minimizes O
( O
over O
w O
∈ O
w O
) O
− O
log O
p O
( O
wt O
| O
w O
< O
t O
) O
, O
i.e. O
, O
surprisal O
. O

section 7
id pdf2json/2021.acl-long.404.pdf.json
although O
such O
an O
objective O
may O
indirectly O
minimize O
peaks O
and O
dips O
in O
surprisal O
across O
a O
sequence O
simply O
by O
pushing O
them O
towards O
0 O
, O
it O
does O
not O
explicitly O
include O
any O
sequence O
level O
penalty O
for O
even O
surprisal O
distribution O
. O

section 7
id pdf2json/2021.acl-long.404.pdf.json
operationalizations O
as O
regularizers O
r. O
under O
this O
new O
objective O
, O
we O
minimize O
lr O
( O
θ O
) O
= O
l O
( O
θ O
) O
+ O
β O
· O
r O
( O
θ O
) O
( O
5 O
) O
where O
β O
> O
0 O
is O
the O
strength O
coefficient O
of O
the O
regularizer O
. O

section 7
id pdf2json/2021.acl-long.404.pdf.json
we O
consider O
two O
natural O
operationalizations O
of O
uid—inspired O
by O
collins O
( O
2014 O
) O
—as O
regularizers O
for O
training O
language O
models O
: O
variance O
regularizer O
. O

section 7
id pdf2json/2021.acl-long.404.pdf.json
uid O
concerns O
the O
distribution O
of O
information O
in O
language O
production O
, O
and O
so O
a O
natural O
measure O
of O
this O
behavior O
is O
the O
variance O
of O
surprisals O
. O

section 7
id pdf2json/2021.acl-long.404.pdf.json
thus O
, O
we O
first O
consider O
a O
regularizer O
that O
penalizes O
high O
variance O
among O
the O
surprisals O
of O
words O
in O
a O
given O
sequence O
: O
r O
( O
θ O
) O
= O
1 O
|w| O
|w|∑ O
t=1 O
( O
u O
( O
wt O
) O
− O
µ O
) O
2 O
( O
6 O
) O
where O
µ O
= O
1|w| O
∑|w| O
t=1 O
u O
( O
wt O
) O
. O

section 7
id pdf2json/2021.acl-long.404.pdf.json
note O
that O
here O
, O
and O
in O
our O
subsequent O
regularizers O
, O
we O
estimate O
u O
( O
· O
) O
via O
eq O
. O

section 7
id pdf2json/2021.acl-long.404.pdf.json
( O
4 O
) O
using O
our O
model O
pθ O
. O

section 7
id pdf2json/2021.acl-long.404.pdf.json
local O
consistency O
. O

section 7
id pdf2json/2021.acl-long.404.pdf.json
next O
, O
we O
consider O
a O
local O
consistency O
regularizer O
that O
encourages O
the O
surprisals O
of O
adjacent O
words O
to O
have O
similar O
magnitude O
: O
r O
( O
θ O
) O
= O
1 O
|w|−1 O
|w|−1∑ O
t=1 O
( O
u O
( O
wt O
) O
− O
u O
( O
wt+1 O
) O
) O
2 O
( O
7 O
) O
this O
regularizer O
is O
also O
a O
reasonable O
operationalization O
of O
uid—if O
every O
surprisal O
is O
similar O
to O
its O
neighbor O
, O
then O
the O
density O
of O
information O
in O
the O
sequence O
will O
be O
close O
to O
uniform O
. O

section 7
id pdf2json/2021.acl-long.404.pdf.json
though O
we O
focus O
on O
these O
two O
regularizers O
, O
other O
operationalizations O
of O
uid O
certainly O
exist O
. O

section 7
id pdf2json/2021.acl-long.404.pdf.json
for O
example O
, O
a O
similar O
variant O
of O
the O
above O
regularizers O
is O
the O
max O
regularizer O
( O
meister O
et O
al. O
, O
2020a O
) O
, O
which O
penalizes O
the O
highest O
surprisal O
in O
a O
sentence.3 O
furthermore O
, O
uid O
may O
also O
be O
defined O
in O
terms O
of O
parse O
steps O
( O
hale O
, O
2001 O
) O
or O
structural O
integrations O
( O
gibson O
, O
2000 O
) O
, O
as O
well O
as O
in O
spoken O
language O
in O
the O
form O
of O
filler O
words O
like O
uh O
and O
um O
or O
word O
repetition O
during O
challenging O
lexical O
retrieval O
. O

section 7
id pdf2json/2021.acl-long.404.pdf.json
we O
consider O
these O
operationalizations O
( O
as O
well O
as O
the O
broader O
discussion O
of O
how O
to O
operationalize O
uid O
) O
as O
future O
work O
. O

section 7
id pdf2json/2021.acl-long.404.pdf.json
3we O
also O
tried O
this O
operationalization O
in O
preliminary O
experiments O
, O
but O
results O
were O
not O
as O
strong O
as O
the O
variance O
or O
local O
consistency O
regularizers O
. O

section 8
id pdf2json/2021.acl-long.404.pdf.json
to O
empirically O
evaluate O
uid O
regularization O
, O
we O
train O
various O
language O
models O
with O
the O
uidregularized O
objective O
( O
eq O
. O

section 8
id pdf2json/2021.acl-long.404.pdf.json
( O
5 O
) O
) O
using O
the O
following O
experimental O
setup O
. O

section 8
id pdf2json/2021.acl-long.404.pdf.json
datasets O
. O

section 8
id pdf2json/2021.acl-long.404.pdf.json
we O
employ O
datasets O
from O
multiple O
languages O
and O
of O
varying O
sizes O
. O

section 8
id pdf2json/2021.acl-long.404.pdf.json
we O
use O
the O
europarl O
corpus O
( O
koehn O
, O
2005 O
) O
—a O
multi-lingual O
dataset O
of O
discussions O
from O
the O
european O
parliament O
that O
has O
been O
commonly O
used O
for O
language O
modeling O
( O
cotterell O
et O
al. O
, O
2018 O
; O
mielke O
et O
al. O
, O
2019 O
) O
—since O
it O
is O
roughly O
semantically O
controlled O
in O
that O
all O
utterances O
are O
presumably O
about O
the O
same O
topics O
. O

section 8
id pdf2json/2021.acl-long.404.pdf.json
we O
use O
europarl O
v7 O
download O
from O
the O
acl O
2014 O
smt O
workshop4 O
and O
perform O
a O
80–10–10 O
traindev-test O
split O
on O
all O
five O
languages—czech O
, O
english O
, O
french O
, O
german O
, O
and O
spanish—which O
yields O
46.7 O
, O
42.2 O
, O
47.2 O
, O
51.3 O
, O
and O
12.4 O
million O
training O
tokens O
for O
each O
language O
respectively O
. O

section 8
id pdf2json/2021.acl-long.404.pdf.json
moreover O
, O
we O
experiment O
on O
languages O
from O
several O
language O
families O
; O
the O
five O
languages O
in O
europarl O
that O
we O
consider O
are O
all O
indo-european O
, O
and O
so O
we O
look O
to O
wiki-40b O
( O
guo O
et O
al. O
, O
2020 O
) O
, O
which O
contains O
wikipedia O
dumps O
of O
a O
wide O
range O
of O
languages O
. O

section 8
id pdf2json/2021.acl-long.404.pdf.json
we O
choose O
a O
set O
of O
diverse O
languages O
with O
training O
set O
sizes O
relatively O
similar O
to O
that O
of O
europarl O
: O
finnish O
( O
a O
uralic O
language O
; O
59.3m O
training O
tokens O
) O
, O
indonesian O
( O
an O
austronesian O
language O
; O
45.7m O
training O
tokens O
) O
, O
and O
turkish O
( O
a O
turkic O
language O
; O
38.1m O
training O
tokens O
) O
. O

section 8
id pdf2json/2021.acl-long.404.pdf.json
to O
explore O
performance O
on O
lower-resource O
languages O
, O
we O
additionally O
experiment O
with O
swahili5 O
( O
a O
niger-congo O
language O
; O
6.3m O
training O
tokens O
) O
and O
tagalog O
( O
an O
austronesian O
language O
; O
4.2m O
training O
tokens O
) O
. O

section 8
id pdf2json/2021.acl-long.404.pdf.json
for O
all O
languages O
, O
we O
performed O
tokenization O
using O
the O
mosestokenizer.6 O
train O
, O
dev O
, O
and O
test O
set O
splits O
are O
shown O
in O
table O
5 O
in O
the O
appendix O
. O

section 8
id pdf2json/2021.acl-long.404.pdf.json
model O
framework O
and O
architecture O
. O

section 8
id pdf2json/2021.acl-long.404.pdf.json
for O
our O
experiments O
, O
we O
use O
the O
fairseq O
library O
( O
ott O
et O
al. O
, O
2019 O
) O
, O
a O
standard O
sequence O
modeling O
toolkit O
in O
pytorch O
. O

section 8
id pdf2json/2021.acl-long.404.pdf.json
as O
our O
model O
, O
we O
use O
fairseq O
’ O
s O
default O
transformer O
( O
with O
six O
decoder O
layers O
and O
eight O
4http O
: O
//statmt.org/wmt14/ O
translation-task.html O
5since O
there O
are O
no O
niger-congo O
languages O
in O
wiki-40b O
, O
we O
perform O
a O
80-10-10 O
split O
on O
swahili O
wikidumps O
( O
see O
https O
: O
//github.com/google-research/bert/ O
blob/master/multilingual.md O
) O
. O

section 8
id pdf2json/2021.acl-long.404.pdf.json
6https O
: O
//pypi.org/project/ O
mosestokenizer/ O
attention O
heads O
) O
, O
which O
achieves O
competitive7 O
language O
modeling O
performance O
( O
although O
the O
purpose O
of O
our O
paper O
is O
not O
to O
achieve O
or O
compare O
with O
the O
state O
of O
the O
art O
) O
. O

section 8
id pdf2json/2021.acl-long.404.pdf.json
for O
all O
experiments O
, O
we O
followed O
the O
data-preprocessing O
scripts O
and O
recommended O
hyperparameters O
provided O
in O
fairseq O
’ O
s O
language O
modeling O
module O
; O
more O
detailed O
information O
can O
be O
found O
on O
the O
github O
page.8 O
uid O
regularizers O
. O

section 8
id pdf2json/2021.acl-long.404.pdf.json
for O
uid O
regularization O
, O
we O
experiment O
with O
the O
variance O
( O
eq O
. O

section 8
id pdf2json/2021.acl-long.404.pdf.json
( O
6 O
) O
) O
and O
local O
consistency O
regularizers O
( O
eq O
. O

section 8
id pdf2json/2021.acl-long.404.pdf.json
( O
7 O
) O
) O
. O

section 8
id pdf2json/2021.acl-long.404.pdf.json
we O
found O
in O
preliminary O
experiments O
that O
effective O
regularization O
strengths O
were O
often O
near O
β O
= O
0.01 O
, O
and O
so O
we O
performed O
a O
grid O
search O
over O
values O
within O
an O
order O
of O
magnitude O
around O
β O
= O
0.01 O
: O
β O
∈ O
{ O
0.006 O
, O
0.008 O
, O
0.01 O
, O
0.02 O
, O
0.03 O
, O
0.04 O
, O
0.05 O
} O
. O

section 8
id pdf2json/2021.acl-long.404.pdf.json
we O
choose O
the O
model O
with O
the O
lowest O
dev O
loss O
to O
evaluate O
on O
the O
test O
set O
. O

section 9
id pdf2json/2021.acl-long.404.pdf.json
in O
this O
section O
, O
we O
report O
results O
for O
models O
trained O
under O
the O
uid-regularized O
objective O
. O

section 9
id pdf2json/2021.acl-long.404.pdf.json
we O
find O
that O
uid O
regularization O
consistently O
improves O
perplexity O
for O
models O
trained O
on O
various O
languages O
( O
§6.1 O
) O
and O
dataset O
sizes O
( O
§6.2 O
) O
. O

section 9
id pdf2json/2021.acl-long.404.pdf.json
additionally O
, O
we O
examine O
properties O
of O
text O
generated O
by O
uid-regularized O
models O
( O
§6.3 O
) O
and O
analyze O
the O
relationship O
between O
our O
operationalization O
of O
uid O
and O
perplexity O
( O
§6.4 O
) O
. O

section 10
id pdf2json/2021.acl-long.404.pdf.json
table O
1 O
shows O
the O
results O
of O
uid-regularized O
language O
models O
trained O
on O
various O
languages O
from O
europarl O
and O
wiki-40b O
, O
and O
includes O
statistical O
significance O
of O
changes O
in O
perplexity O
, O
as O
compared O
with O
baselines O
, O
computed O
using O
permutation O
tests9 O
( O
efron O
and O
tibshirani O
, O
1994 O
) O
. O

section 10
id pdf2json/2021.acl-long.404.pdf.json
for O
all O
languages O
, O
uid O
regularization O
significantly O
improves O
perplexity O
for O
at O
least O
one O
of O
the O
two O
regularizers O
. O

section 10
id pdf2json/2021.acl-long.404.pdf.json
further- O
7on O
wikitext-103 O
, O
the O
largest O
dataset O
we O
train O
on O
( O
103 O
million O
tokens O
) O
, O
we O
achieve O
a O
competitive O
perplexity O
of O
29.89 O
( O
c.f O
. O

section 10
id pdf2json/2021.acl-long.404.pdf.json
merity O
et O
al O
. O

section 10
id pdf2json/2021.acl-long.404.pdf.json
( O
2018 O
) O
) O
. O

section 10
id pdf2json/2021.acl-long.404.pdf.json
for O
smaller O
datasets O
, O
we O
tried O
a O
smaller O
transformer O
architecture O
of O
four O
decoder O
layers O
and O
four O
attention O
heads O
, O
but O
it O
did O
not O
perform O
better O
than O
the O
six O
decoder O
layer O
and O
eight O
attention O
heads O
version O
, O
suggesting O
that O
this O
architecture O
was O
not O
too O
large O
for O
the O
datasets O
we O
use O
in O
this O
paper O
( O
even O
the O
tagalog O
dataset O
we O
use O
is O
larger O
than O
the O
commonly O
used O
penn O
treebank O
and O
wikitext-2 O
) O
. O

section 10
id pdf2json/2021.acl-long.404.pdf.json
8https O
: O
//github.com/pytorch/fairseq/ O
tree/master/examples/language_model O
9http O
: O
//www2.stat.duke.edu/~ar182/rr/ O
examples-gallery/permutationtest.html O
more O
, O
uid O
regularization O
( O
under O
the O
best O
performing O
β O
) O
never O
leads O
to O
worse O
perplexity O
. O

section 10
id pdf2json/2021.acl-long.404.pdf.json
these O
results O
suggest O
that O
incorporating O
uid O
operationalizations O
into O
a O
model O
’ O
s O
training O
objective O
leads O
to O
a O
better O
model O
of O
language O
, O
substantiating O
uniform O
information O
density O
as O
a O
valid O
inductive O
bias O
. O

section 10
id pdf2json/2021.acl-long.404.pdf.json
moreover O
, O
the O
improvement O
for O
many O
languages O
corroborates O
the O
expectation O
that O
uid O
should O
, O
due O
to O
its O
information O
theoretic O
nature O
, O
hold O
across O
languages O
( O
jaeger O
and O
tily O
, O
2011 O
) O
. O

section 11
id pdf2json/2021.acl-long.404.pdf.json
notably O
, O
we O
observe O
the O
largest O
improvements O
( O
1.6–2.9 O
% O
) O
in O
perplexity O
in O
table O
1 O
for O
the O
lowest O
resource O
languages O
, O
tagalog O
and O
swahili O
( O
with O
4.2 O
and O
6.3 O
million O
training O
tokens O
respectively O
) O
. O

section 11
id pdf2json/2021.acl-long.404.pdf.json
conversely O
, O
improvement O
was O
most O
marginal O
( O
0.2– O
0.5 O
% O
) O
on O
the O
highest-resource O
languages O
, O
french O
and O
finnish O
( O
51.3 O
and O
59.3 O
million O
training O
tokens O
respectively O
) O
. O

section 11
id pdf2json/2021.acl-long.404.pdf.json
to O
remove O
language O
as O
a O
confounding O
factor O
from O
this O
observation O
, O
we O
perform O
a O
controlled O
analysis O
of O
the O
effects O
of O
uid O
regularization O
as O
a O
function O
of O
dataset O
size O
. O

section 11
id pdf2json/2021.acl-long.404.pdf.json
we O
focus O
on O
english O
; O
in O
addition O
to O
the O
result O
on O
english O
europarl O
2014 O
from O
table O
1 O
, O
which O
contains O
47.0 O
million O
training O
tokens O
, O
we O
experiment O
with O
the O
smaller O
monolingual O
english O
dataset O
from O
the O
2006 O
naacl O
workshop O
on O
statistical O
machine O
translation O
( O
wmt O
’ O
06 O
) O
,10 O
which O
has O
17.0m O
tokens O
in O
its O
training O
set O
, O
as O
well O
as O
the O
larger O
wikitext-103 O
benchmark O
( O
merity O
et O
al. O
, O
2017 O
) O
, O
which O
contains O
103 O
million O
tokens O
in O
its O
training O
set O
. O

section 11
id pdf2json/2021.acl-long.404.pdf.json
table O
2 O
shows O
the O
perplexities O
for O
models O
with O
and O
without O
uid O
regulariztion O
for O
these O
three O
datasets O
. O

section 11
id pdf2json/2021.acl-long.404.pdf.json
as O
suggested O
by O
earlier O
results O
, O
improvements O
were O
strongest O
for O
the O
wmt O
’ O
06 O
dataset O
, O
with O
an O
improvement O
of O
1.4 O
perplexity O
points O
for O
the O
variance O
regularizer O
and O
0.9 O
ppl O
points O
for O
local O
consistency O
. O

section 11
id pdf2json/2021.acl-long.404.pdf.json
for O
the O
larger O
europarl O
and O
wt-103 O
datasets O
, O
on O
the O
other O
hand O
, O
improvement O
was O
more O
modest O
, O
ranging O
from O
0.1 O
to O
0.3 O
perplexity O
points O
. O

section 11
id pdf2json/2021.acl-long.404.pdf.json
as O
further O
confirmation O
that O
uid O
regularization O
has O
a O
greater O
impact O
on O
smaller O
datasets O
, O
we O
perform O
an O
ablation O
study O
that O
roughly O
controls O
for O
language O
content O
by O
training O
models O
on O
the O
subsets O
of O
the O
same O
dataset O
. O

section 11
id pdf2json/2021.acl-long.404.pdf.json
for O
this O
ablation O
, O
we O
take O
subsets O
of O
2 O
, O
4 O
, O
8 O
, O
12 O
, O
16 O
, O
24 O
, O
and O
32 O
million O
sentences O
from O
the O
47 O
million O
sentences O
in O
english O
europarl O
, O
10we O
downloaded O
the O
given O
train-dev-test O
splits O
from O
https O
: O
//www.statmt.org/wmt06/ O
. O

section 11
id pdf2json/2021.acl-long.404.pdf.json
and O
observe O
how O
much O
the O
uid O
regularizers O
improve O
perplexity O
for O
each O
training O
dataset O
size O
. O

section 11
id pdf2json/2021.acl-long.404.pdf.json
as O
shown O
in O
figure O
2 O
, O
the O
results O
tell O
the O
same O
story O
as O
table O
2—uid O
regularization O
improves O
perplexity O
more O
for O
smaller O
datasets O
. O

section 11
id pdf2json/2021.acl-long.404.pdf.json
these O
results O
are O
consistent O
with O
the O
expectation O
that O
models O
trained O
on O
smaller O
datasets O
are O
more O
likely O
to O
overfit O
and O
could O
therefore O
benefit O
more O
from O
regularization O
( O
melis O
et O
al. O
, O
2018 O
) O
. O

section 11
id pdf2json/2021.acl-long.404.pdf.json
as O
it O
is O
possible O
that O
the O
models O
trained O
on O
smaller O
datasets O
could O
benefit O
from O
any O
kind O
of O
regularization O
, O
we O
experiment O
with O
label O
smoothing O
( O
szegedy O
et O
al. O
, O
2016 O
) O
, O
another O
regularization O
technique O
that O
similarly O
augments O
the O
training O
objective O
with O
a O
penalty O
. O

section 11
id pdf2json/2021.acl-long.404.pdf.json
table O
4 O
shows O
these O
results O
for O
models O
trained O
on O
wmt O
’ O
06 O
and O
europarl O
with O
label O
smoothing—our O
experiments O
indicate O
that O
, O
across O
the O
board O
, O
label O
smoothing O
leads O
to O
worse O
perplexity O
compared O
with O
baseline O
models.11 O
we O
take O
this O
result O
as O
further O
evidence O
that O
the O
improvement O
from O
uid O
regularization O
stems O
from O
the O
uid O
hypothesis O
as O
a O
valid O
inductive O
bias O
, O
rather O
than O
simply O
a O
need O
for O
any O
kind O
of O
regularization O
when O
training O
on O
smaller O
datasets O
. O

section 11
id pdf2json/2021.acl-long.404.pdf.json
11this O
negative O
result O
for O
applying O
label O
smoothing O
to O
language O
modeling O
is O
consistent O
with O
prior O
empirical O
findings O
( O
müller O
et O
al. O
, O
2019 O
; O
gao O
et O
al. O
, O
2020 O
; O
meister O
et O
al. O
, O
2020b O
) O
. O

section 12
id pdf2json/2021.acl-long.404.pdf.json
unconditional O
models O
of O
language O
have O
been O
observed O
to O
produce O
generic O
text O
that O
can O
be O
short O
, O
bland O
, O
or O
repetitive O
( O
fan O
et O
al. O
, O
2018 O
; O
kulikov O
et O
al. O
, O
2019 O
; O
holtzman O
et O
al. O
, O
2020 O
) O
, O
and O
so O
in O
this O
subsection O
we O
investigate O
how O
uid O
regularization O
might O
affect O
these O
characteristics O
in O
generated O
text O
. O

section 12
id pdf2json/2021.acl-long.404.pdf.json
for O
these O
experiments O
, O
we O
consider O
the O
baseline O
model O
, O
the O
variance-regularized O
model O
, O
and O
the O
local O
consistency-regularized O
model O
trained O
on O
english O
europarl O
. O

section 12
id pdf2json/2021.acl-long.404.pdf.json
to O
obtain O
text O
samples O
, O
we O
generate O
samples O
by O
sequentially O
sampling O
tokens O
according O
to O
the O
model O
’ O
s O
predicted O
distribution O
until O
the O
endof-sequence O
( O
eos O
) O
token O
is O
sampled O
, O
i.e. O
, O
ancestral O
sampling O
. O

section 12
id pdf2json/2021.acl-long.404.pdf.json
note O
that O
for O
language O
model O
p O
, O
this O
sampling O
scheme O
is O
equivalent O
to O
directly O
sampling O
y O
∼ O
p. O
we O
obtain O
10,000 O
samples O
for O
each O
model O
and O
report O
statistics O
in O
table O
3 O
. O

section 12
id pdf2json/2021.acl-long.404.pdf.json
we O
analyze O
each O
set O
of O
generated O
sentences O
for O
several O
metrics O
. O

section 12
id pdf2json/2021.acl-long.404.pdf.json
first O
, O
we O
compute O
the O
average O
length O
of O
generated O
sentences O
. O

section 12
id pdf2json/2021.acl-long.404.pdf.json
next O
, O
we O
evaluate O
the O
lexical O
diversity O
of O
generated O
texts O
by O
computing O
the O
percent O
of O
unique O
n-grams O
for O
n O
∈ O
{ O
2 O
, O
3 O
, O
4 O
} O
. O

section 12
id pdf2json/2021.acl-long.404.pdf.json
finally O
, O
sampling O
from O
a O
model O
also O
gives O
us O
a O
means O
for O
estimating O
the O
language O
model O
’ O
s O
entropy O
: O
h O
( O
p O
) O
= O
− O
∑ O
y∈supp O
( O
p O
) O
p O
( O
y O
) O
log O
p O
( O
y O
) O
( O
8 O
) O
= O
−ey∼p O
( O
log O
p O
( O
y O
) O
) O
( O
9 O
) O
in O
the O
case O
of O
language O
models O
, O
supp O
( O
p O
) O
is O
the O
set O
of O
all O
strings O
that O
can O
be O
generated O
from O
the O
model O
’ O
s O
vocabulary O
v O
. O

section 12
id pdf2json/2021.acl-long.404.pdf.json
as O
this O
is O
exponentially O
large O
in O
|v| O
, O
directly O
computing O
h O
( O
p O
) O
is O
intractable O
. O

section 12
id pdf2json/2021.acl-long.404.pdf.json
we O
can O
use O
its O
equivalence O
to O
eq O
. O

section 12
id pdf2json/2021.acl-long.404.pdf.json
( O
9 O
) O
, O
however O
, O
to O
estimate O
h O
( O
p O
) O
with O
a O
simple O
monte-carlo O
estimator O
: O
ĥ O
( O
p O
) O
= O
− O
1 O
k O
k∑ O
k=1 O
log O
p O
( O
y O
( O
k O
) O
) O
( O
10 O
) O
where O
we O
sample O
y O
( O
k O
) O
∼ O
p O
for O
k O
= O
1 O
, O
. O

section 12
id pdf2json/2021.acl-long.404.pdf.json
. O

section 12
id pdf2json/2021.acl-long.404.pdf.json
. O

section 12
id pdf2json/2021.acl-long.404.pdf.json
, O
k. O
table O
3 O
shows O
results O
from O
uid-regularized O
models O
compared O
with O
the O
baseline O
. O

section 12
id pdf2json/2021.acl-long.404.pdf.json
the O
models O
trained O
with O
the O
variance O
and O
local O
consistency O
regularizers O
exhibit O
a O
preference O
for O
longer O
sequence O
length O
and O
higher O
lexical O
diversity O
. O

section 12
id pdf2json/2021.acl-long.404.pdf.json
additionally O
, O
the O
entropy O
estimates O
of O
these O
models O
are O
notably O
higher O
, O
which O
, O
following O
the O
principle O
of O
maximum O
entropy O
( O
jaynes O
, O
1957 O
) O
,12 O
can O
be O
seen O
as O
an O
additional O
advantage O
of O
uid-regularized O
models O
over O
their O
unregularized O
counterparts O
. O

section 13
id pdf2json/2021.acl-long.404.pdf.json
to O
take O
a O
closer O
look O
at O
how O
uid O
regularization O
affects O
language O
models O
, O
we O
examine O
the O
relationship O
between O
minimizing O
perplexity O
and O
uid O
behavior O
, O
where O
we O
quantify O
uid O
behavior O
as O
the O
variance O
of O
models O
’ O
surprisals O
. O

section 13
id pdf2json/2021.acl-long.404.pdf.json
we O
consider O
models O
trained O
on O
the O
english O
europarl O
dataset O
with O
the O
variance O
regularizer O
at O
strengths O
β O
∈ O
{ O
0.01 O
, O
0.03 O
, O
0.05 O
, O
0.07 O
, O
0.09 O
} O
and O
our O
baseline O
( O
which O
is O
equivalent O
to O
β O
= O
0 O
) O
, O
for O
further O
comparison O
, O
we O
also O
train O
a O
model O
with O
β O
= O
−0.01 O
to O
observe O
the O
effects O
of O
penalizing O
uid O
behavior O
. O

section 13
id pdf2json/2021.acl-long.404.pdf.json
we O
report O
results O
on O
the O
europarl O
test O
set O
in O
figure O
3 O
. O

section 13
id pdf2json/2021.acl-long.404.pdf.json
we O
observe O
that O
the O
model O
trained O
with O
a O
uid O
penalty O
( O
negative O
β O
) O
indeed O
exhibits O
worse O
perplexity O
and O
uid O
behavior O
( O
variance O
of O
surprisals O
) O
on O
the O
test O
set O
. O

section 13
id pdf2json/2021.acl-long.404.pdf.json
and O
as O
we O
might O
expect O
, O
models O
trained O
with O
higher O
β O
exhibit O
uid O
behavior O
more O
strongly O
, O
as O
our O
quantification O
is O
part O
of O
their O
training O
objective O
. O

section 13
id pdf2json/2021.acl-long.404.pdf.json
overall O
, O
from O
β O
= O
0.01 O
to O
β O
= O
0.05 O
, O
both O
12the O
principle O
of O
maximum O
entropy O
states O
that O
the O
probability O
distribution O
that O
best O
represents O
the O
current O
knowledge O
state O
is O
the O
one O
with O
the O
largest O
entropy O
. O

section 13
id pdf2json/2021.acl-long.404.pdf.json
perplexity O
and O
uid O
behavior O
are O
positively O
correlated O
with O
β O
, O
but O
when O
we O
optimize O
too O
much O
for O
uid O
( O
β O
≥ O
0.07 O
) O
, O
there O
is O
a O
trade-off O
in O
which O
model O
perplexity O
begins O
to O
increase O
. O

section 13
id pdf2json/2021.acl-long.404.pdf.json
we O
also O
observe O
an O
intriguing O
phenomenon O
in O
figure O
3 O
. O

section 13
id pdf2json/2021.acl-long.404.pdf.json
models O
that O
achieve O
similar O
perplexity O
can O
have O
substantially O
different O
uid O
behavior O
values O
on O
the O
test O
set O
. O

section 13
id pdf2json/2021.acl-long.404.pdf.json
specifically O
, O
the O
β O
= O
0 O
and O
β O
= O
0.07 O
models O
, O
which O
have O
almost O
the O
same O
perplexity O
, O
have O
variance O
of O
surprisals O
of O
17.8 O
and O
15.8—a O
difference O
of O
more O
than O
ten O
percent O
! O

section 13
id pdf2json/2021.acl-long.404.pdf.json
if O
such O
models O
with O
similar O
perplexity O
can O
have O
varying O
definitions O
of O
what O
constitutes O
good O
uid O
behavior O
, O
then O
prior O
work O
, O
which O
has O
drawn O
conclusions O
on O
uid O
based O
on O
surprisals O
computed O
by O
a O
single O
model O
( O
aylett O
and O
turk O
, O
2004 O
; O
levy O
and O
jaeger O
, O
2007 O
; O
jain O
et O
al. O
, O
2018 O
) O
, O
may O
need O
revisiting O
. O

section 13
id pdf2json/2021.acl-long.404.pdf.json
as O
this O
direction O
is O
outside O
the O
scope O
of O
the O
present O
paper O
, O
we O
leave O
it O
as O
future O
work O
. O

section 14
id pdf2json/2021.acl-long.404.pdf.json
we O
discussed O
how O
operationalizing O
uid O
for O
language O
modeling O
leads O
to O
better O
models O
in O
a O
wide O
variety O
of O
settings O
. O

section 14
id pdf2json/2021.acl-long.404.pdf.json
these O
results O
both O
provide O
a O
new O
form O
of O
evidence O
for O
the O
uid O
hypothesis O
and O
build O
on O
prior O
work O
exploring O
uid O
in O
modern-day O
nlp O
models O
. O

section 14
id pdf2json/2021.acl-long.404.pdf.json
evidence O
for O
the O
uid O
hypothesis O
. O

section 14
id pdf2json/2021.acl-long.404.pdf.json
our O
work O
extends O
the O
body O
of O
psycholinguistic O
research O
on O
uniform O
information O
density O
, O
which O
has O
largely O
corroborated O
the O
uid O
hypothesis O
by O
providing O
evidence O
that O
variation O
in O
surprisal O
, O
as O
estimated O
by O
a O
lan- O
guage O
model O
, O
is O
minimized O
in O
natural O
language O
. O

section 14
id pdf2json/2021.acl-long.404.pdf.json
in O
addition O
to O
early O
studies O
that O
used O
this O
approach O
to O
find O
evidence O
for O
uid O
in O
syntactic O
reduction O
( O
levy O
and O
jaeger O
, O
2007 O
) O
, O
morphosyntactic O
contractions O
( O
frank O
and O
jaeger O
, O
2008 O
) O
, O
and O
prosodic O
structure O
( O
aylett O
and O
turk O
, O
2004 O
) O
, O
the O
same O
line O
of O
reasoning O
has O
been O
used O
by O
more O
recent O
work O
exploring O
a O
variety O
of O
other O
linguistic O
properties O
. O

section 14
id pdf2json/2021.acl-long.404.pdf.json
these O
studies O
have O
found O
that O
word O
duration O
can O
be O
predicted O
by O
syntactic O
surprisal O
( O
demberg O
et O
al. O
, O
2012 O
; O
moorecantwell O
, O
2013 O
) O
, O
construction O
probability O
( O
kuperman O
and O
bresnan O
, O
2012 O
) O
, O
informativity O
( O
seyfarth O
, O
2014 O
) O
, O
and O
contextual O
predictability O
( O
jurafsky O
et O
al. O
, O
2001 O
; O
bell O
et O
al. O
, O
2003 O
; O
gahl O
and O
garnsey O
, O
2004 O
) O
. O

section 14
id pdf2json/2021.acl-long.404.pdf.json
they O
have O
also O
observed O
that O
word O
length O
is O
reflected O
by O
conceptual O
complexity O
( O
lewis O
and O
frank O
, O
2016 O
) O
; O
word O
order O
choice O
can O
be O
predicted O
by O
processing O
cost O
( O
bloem O
, O
2016 O
; O
sikos O
et O
al. O
, O
2017 O
) O
; O
phonological O
patterns O
can O
be O
shaped O
by O
word O
predictability O
( O
hall O
et O
al. O
, O
2018 O
) O
; O
and O
uid O
computed O
at O
the O
sequence O
level O
predicts O
human O
preferences O
for O
syntactic O
alternatives O
of O
the O
same O
sentence O
. O

section 14
id pdf2json/2021.acl-long.404.pdf.json
whereas O
the O
above O
prior O
work O
has O
used O
language O
modeling O
as O
a O
tool O
for O
measuring O
uid O
, O
our O
paper O
has O
explored O
the O
exact O
converse—we O
have O
asked O
whether O
uid O
, O
operationalized O
as O
a O
regularizer O
, O
can O
be O
used O
as O
a O
tool O
for O
training O
better O
language O
models O
. O

section 14
id pdf2json/2021.acl-long.404.pdf.json
we O
argue O
that O
if O
the O
uid O
hypothesis O
holds O
as O
a O
general O
principle O
, O
then O
we O
should O
be O
able O
to O
exploit O
it O
as O
a O
training O
criterion O
that O
improves O
language O
modeling O
. O

section 14
id pdf2json/2021.acl-long.404.pdf.json
and O
accordingly O
, O
our O
results O
show O
that—across O
a O
variety O
of O
languages O
and O
dataset O
sizes—regularization O
for O
uid O
did O
indeed O
improve O
perplexity O
, O
which O
we O
view O
as O
an O
alternative O
kind O
of O
evidence O
for O
the O
uid O
hypothesis O
at O
scale O
. O

section 14
id pdf2json/2021.acl-long.404.pdf.json
notably O
, O
figure O
3 O
at O
first O
could O
appear O
to O
contradict O
the O
uid O
hypothesis O
, O
since O
models O
with O
better O
uid O
behavior O
did O
not O
always O
achieve O
better O
perplexity O
. O

section 14
id pdf2json/2021.acl-long.404.pdf.json
we O
do O
not O
consider O
this O
as O
evidence O
against O
the O
uid O
hypothesis O
, O
however O
. O

section 14
id pdf2json/2021.acl-long.404.pdf.json
rather O
, O
we O
posit O
that O
when O
β O
is O
too O
large O
, O
we O
may O
be O
optimizing O
for O
uid O
to O
the O
point O
of O
tending O
towards O
unnatural O
language—a O
perfectly O
uniform O
dispersion O
of O
information O
across O
an O
utterance O
may O
come O
at O
the O
cost O
of O
strange O
lexical O
choices O
. O

section 14
id pdf2json/2021.acl-long.404.pdf.json
in O
this O
light O
, O
such O
a O
trade-off O
should O
be O
somewhat O
expected O
. O

section 14
id pdf2json/2021.acl-long.404.pdf.json
uid O
in O
modern O
nlp O
. O

section 14
id pdf2json/2021.acl-long.404.pdf.json
in O
addition O
to O
the O
traditional O
line O
of O
psycholinguistic O
work O
, O
there O
have O
also O
been O
more-recent O
studies O
on O
uid O
in O
the O
context O
of O
modern O
nlp O
, O
although O
this O
work O
is O
relatively O
sparse O
. O

section 14
id pdf2json/2021.acl-long.404.pdf.json
rubino O
et O
al O
. O

section 14
id pdf2json/2021.acl-long.404.pdf.json
( O
2016 O
) O
leverage O
infor- O
mation O
density O
encoded O
as O
surprisal O
at O
the O
word O
, O
part O
of O
speech O
, O
and O
syntax O
levels O
to O
help O
build O
a O
state-of-the-art O
model O
for O
mixed-domain O
translationese O
detection O
. O

section 14
id pdf2json/2021.acl-long.404.pdf.json
jain O
et O
al O
. O

section 14
id pdf2json/2021.acl-long.404.pdf.json
( O
2018 O
) O
incorporate O
uid O
measures O
across O
sentences O
into O
models O
designed O
to O
detect O
natural O
versus O
manipulated O
text O
. O

section 14
id pdf2json/2021.acl-long.404.pdf.json
perhaps O
the O
work O
that O
is O
most O
related O
to O
ours O
, O
meister O
et O
al O
. O

section 14
id pdf2json/2021.acl-long.404.pdf.json
( O
2020a O
) O
, O
leverages O
uid O
to O
explain O
why O
beam O
search O
is O
an O
effective O
decoding O
algorithm O
and O
uses O
operationalizations O
of O
uid O
during O
beam O
search O
to O
alleviate O
problems O
with O
decoding O
poorly O
calibrated O
machine O
translation O
models O
. O

section 14
id pdf2json/2021.acl-long.404.pdf.json
whereas O
meister O
et O
al O
. O

section 14
id pdf2json/2021.acl-long.404.pdf.json
( O
2020a O
) O
focuses O
on O
decoding O
, O
our O
work O
shows O
the O
first O
evidence O
that O
uid O
can O
be O
operationalized O
to O
aid O
training O
. O

section 15
id pdf2json/2021.acl-long.404.pdf.json
in O
closing O
, O
we O
have O
proposed O
encoding O
uniform O
information O
density O
as O
a O
regularizer O
for O
training O
language O
models—a O
novel O
manner O
of O
incorporating O
an O
established O
psycholinguistic O
theory O
into O
modern O
statistical O
language O
modeling O
. O

section 15
id pdf2json/2021.acl-long.404.pdf.json
in O
experiments O
on O
a O
range O
of O
languages O
and O
dataset O
sizes O
, O
uid O
regularization O
consistently O
improves O
perplexity O
over O
baselines O
. O

section 15
id pdf2json/2021.acl-long.404.pdf.json
our O
results O
suggest O
that O
uid O
is O
a O
valid O
inductive O
bias O
for O
improving O
the O
canonical O
maximum O
likelihood O
objective O
in O
language O
modeling O
, O
providing O
a O
new O
, O
alternative O
type O
of O
evidence O
that O
supports O
the O
uid O
hypothesis O
at O
scale O
. O

section 15
id pdf2json/2021.acl-long.404.pdf.json
our O
work O
opens O
the O
door O
to O
future O
research O
directions O
such O
as O
using O
similar O
techniques O
to O
validate O
other O
psycholinguistic O
phenomena O
, O
applying O
uid O
regularization O
in O
conditional O
language O
generation O
tasks O
, O
and O
exploring O
how O
uid O
regularized O
models O
perform O
in O
downstream O
nlp O
applications O
. O

section 15
id pdf2json/2021.acl-long.404.pdf.json
ethical O
concerns O
language O
models O
have O
various O
ethical O
, O
environmental O
, O
and O
financial O
concerns O
. O

section 15
id pdf2json/2021.acl-long.404.pdf.json
we O
can O
not O
do O
justice O
to O
them O
here O
, O
but O
do O
see O
bender O
et O
al O
. O

section 15
id pdf2json/2021.acl-long.404.pdf.json
( O
2021 O
) O
for O
a O
pointer O
. O

section 15
id pdf2json/2021.acl-long.404.pdf.json
we O
do O
not O
foresee O
any O
additional O
ethical O
concerns O
with O
the O
contributions O
made O
in O
our O
work O
beyond O
those O
discussed O
in O
bender O
et O
al O
. O

section 15
id pdf2json/2021.acl-long.404.pdf.json
( O
2021 O
) O
. O

section 16
id pdf2json/2021.acl-long.404.pdf.json
we O
thank O
roger O
levy O
for O
feedback O
in O
the O
middle O
stages O
of O
our O
work O
and O
tiago O
pimentel O
, O
david O
reitter O
, O
tal O
linzen O
, O
and O
slav O
petrov O
for O
feedback O
on O
the O
manuscript O
. O

section 17
id pdf2json/2021.acl-long.404.pdf.json
datasets O
. O

section 17
id pdf2json/2021.acl-long.404.pdf.json
table O
5 O
shows O
the O
train O
, O
dev O
, O
and O
test O
set O
splits O
for O
the O
language O
modeling O
datasets O
we O
use O
. O

section 17
id pdf2json/2021.acl-long.404.pdf.json
hyperparameters O
. O

section 17
id pdf2json/2021.acl-long.404.pdf.json
table O
6 O
shows O
the O
optimized O
β O
hyperparameter O
from O
a O
grid-search O
over O
β O
∈ O
{ O
0.006 O
, O
0.008 O
, O
0.01 O
, O
0.02 O
, O
0.03 O
, O
0.04 O
, O
0.05 O
} O
for O
both O
regularizers O
on O
all O
datasets O
we O
use O
. O

section 17
id pdf2json/2021.acl-long.404.pdf.json
notably O
, O
the O
best O
β O
for O
variance O
ranged O
from O
1×10−2 O
to O
5×10−2 O
, O
and O
the O
best O
β O
for O
local O
consistency O
ranged O
from O
6×10−3 O
to O
2×10−2 O
. O

section 17
id pdf2json/2021.acl-long.404.pdf.json
for O
use O
on O
a O
new O
dataset O
, O
we O
recommend O
starting O
with O
1×10−2 O
, O
which O
we O
found O
almost O
always O
improved O
perplexity O
for O
both O
regularizers O
( O
on O
these O
datasets O
, O
at O
least O
) O
. O

section TITLE
id pdf2json/2021.acl-long.146.pdf.json
knowledgeable O
or O
educated O
guess O
? O

section TITLE
id pdf2json/2021.acl-long.146.pdf.json
revisiting O
language O
models O
as O
knowledge O
bases O

section ABSTRACT
id pdf2json/2021.acl-long.146.pdf.json
previous O
literatures O
show O
that O
pre-trained O
masked O
language O
models O
( O
mlms O
) O
such O
as O
bert O
can O
achieve O
competitive O
factual O
knowledge O
extraction O
performance O
on O
some O
datasets O
, O
indicating O
that O
mlms O
can O
potentially O
be O
a O
reliable O
knowledge O
source O
. O

section ABSTRACT
id pdf2json/2021.acl-long.146.pdf.json
in O
this O
paper O
, O
we O
conduct O
a O
rigorous O
study O
to O
explore O
the O
underlying O
predicting O
mechanisms O
of O
mlms O
over O
different O
extraction O
paradigms O
. O

section ABSTRACT
id pdf2json/2021.acl-long.146.pdf.json
by O
investigating O
the O
behaviors O
of O
mlms O
, O
we O
find O
that O
previous O
decent O
performance O
mainly O
owes O
to O
the O
biased O
prompts O
which O
overfit O
dataset O
artifacts O
. O

section ABSTRACT
id pdf2json/2021.acl-long.146.pdf.json
furthermore O
, O
incorporating O
illustrative O
cases O
and O
external O
contexts O
improve O
knowledge O
prediction O
mainly O
due O
to O
entity O
type O
guidance O
and O
golden O
answer O
leakage O
. O

section ABSTRACT
id pdf2json/2021.acl-long.146.pdf.json
our O
findings O
shed O
light O
on O
the O
underlying O
predicting O
mechanisms O
of O
mlms O
, O
and O
strongly O
question O
the O
previous O
conclusion O
that O
current O
mlms O
can O
potentially O
serve O
as O
reliable O
factual O
knowledge O
bases1 O
. O

section 0
id pdf2json/2021.acl-long.146.pdf.json
proceedings O
of O
the O
59th O
annual O
meeting O
of O
the O
association O
for O
computational O
linguistics O
and O
the O
11th O
international O
joint O
conference O
on O
natural O
language O
processing O
, O
pages O
1860–1874 O
august O
1–6 O
, O
2021 O
. O

section 0
id pdf2json/2021.acl-long.146.pdf.json
©2021 O
association O
for O
computational O
linguistics O
1860 O

section 1
id pdf2json/2021.acl-long.146.pdf.json
recently O
, O
pre-trained O
language O
models O
( O
peters O
et O
al. O
, O
2018 O
; O
devlin O
et O
al. O
, O
2019 O
; O
brown O
et O
al. O
, O
2020 O
) O
have O
achieved O
promising O
performance O
on O
many O
nlp O
tasks O
. O

section 1
id pdf2json/2021.acl-long.146.pdf.json
apart O
from O
utilizing O
the O
universal O
representations O
from O
pre-trained O
models O
in O
downstream O
tasks O
, O
some O
literatures O
have O
shown O
the O
potential O
of O
pretrained O
masked O
language O
models O
( O
e.g. O
, O
bert O
( O
devlin O
et O
al. O
, O
2019 O
) O
and O
roberta O
( O
liu O
et O
al. O
, O
2019b O
) O
) O
to O
be O
factual O
knowledge O
bases O
( O
petroni O
et O
al. O
, O
2019 O
; O
bouraoui O
et O
al. O
, O
2020 O
; O
jiang O
et O
al. O
, O
2020b O
; O
shin O
et O
al. O
, O
2020 O
; O
jiang O
et O
al. O
, O
2020a O
; O
wang O
et O
al. O
, O
2020 O
; O
kassner O
and O
schütze O
, O
2020a O
; O
kassner O
et O
al. O
, O
2020 O
) O
. O

section 1
id pdf2json/2021.acl-long.146.pdf.json
for O
example O
, O
to O
extract O
the O
birthplace O
of O
steve O
jobs O
, O
we O
can O
query O
mlms O
like O
bert O
with O
“ O
steve O
jobs O
was O
born O
in O
[ O
mask O
] O
” O
, O
where O
steve O
jobs O
is O
the O
subject O
∗corresponding O
authors O
1we O
openly O
release O
the O
source O
code O
and O
data O
at O
https O
: O
//github.com/c-box/lanka O
of O
the O
fact O
, O
“ O
was O
born O
in O
” O
is O
a O
prompt O
string O
for O
the O
relation O
“ O
place-of-birth O
” O
and O
[ O
mask O
] O
is O
a O
placeholder O
for O
the O
object O
to O
predict O
. O

section 1
id pdf2json/2021.acl-long.146.pdf.json
then O
mlms O
are O
expected O
to O
predict O
the O
correct O
answer O
“ O
california O
” O
at O
the O
[ O
mask O
] O
position O
based O
on O
its O
internal O
knowledge O
. O

section 1
id pdf2json/2021.acl-long.146.pdf.json
to O
help O
mlms O
better O
extract O
knowledge O
, O
the O
query O
may O
also O
be O
enriched O
with O
external O
information O
like O
illustrative O
cases O
( O
e.g. O
, O
( O
obama O
, O
hawaii O
) O
) O
( O
brown O
et O
al. O
, O
2020 O
) O
or O
external O
context O
( O
e.g. O
, O
jobs O
lives O
in O
california O
) O
( O
petroni O
et O
al. O
, O
2020 O
) O
. O

section 1
id pdf2json/2021.acl-long.146.pdf.json
some O
literatures O
have O
shown O
that O
such O
paradigms O
can O
achieve O
decent O
performance O
on O
some O
benchmarks O
like O
lama O
( O
petroni O
et O
al. O
, O
2019 O
) O
. O

section 1
id pdf2json/2021.acl-long.146.pdf.json
despite O
some O
reported O
success O
, O
currently O
there O
is O
no O
rigorous O
study O
looking O
deeply O
into O
the O
underlying O
mechanisms O
behind O
these O
achievements O
. O

section 1
id pdf2json/2021.acl-long.146.pdf.json
besides O
, O
it O
is O
also O
unclear O
whether O
such O
achievements O
depend O
on O
certain O
conditions O
( O
e.g. O
, O
datasets O
, O
domains O
, O
relations O
) O
. O

section 1
id pdf2json/2021.acl-long.146.pdf.json
the O
absence O
of O
such O
kind O
of O
studies O
undermines O
our O
trust O
in O
the O
predictions O
of O
mlms O
. O

section 1
id pdf2json/2021.acl-long.146.pdf.json
we O
could O
neither O
determine O
whether O
the O
predictions O
are O
reliable O
nor O
explain O
why O
mlms O
make O
a O
specific O
prediction O
, O
and O
therefore O
significantly O
limits O
mlms O
’ O
further O
applications O
and O
improvements O
. O

section 1
id pdf2json/2021.acl-long.146.pdf.json
to O
this O
end O
, O
this O
paper O
conducts O
a O
thorough O
study O
on O
whether O
mlms O
could O
be O
reliable O
factual O
knowledge O
bases O
. O

section 1
id pdf2json/2021.acl-long.146.pdf.json
throughout O
our O
investigations O
, O
we O
analyze O
the O
behaviors O
of O
mlms O
, O
figure O
out O
the O
critical O
factors O
for O
mlms O
to O
achieve O
decent O
performance O
, O
and O
demonstrate O
how O
different O
kinds O
of O
external O
information O
influence O
mlms O
’ O
predictions O
. O

section 1
id pdf2json/2021.acl-long.146.pdf.json
specifically O
, O
we O
investigate O
factual O
knowledge O
extraction O
from O
mlms2 O
over O
three O
representative O
factual O
knowledge O
extraction O
paradigms O
, O
as O
shown O
in O
figure O
1 O
: O
• O
prompt-based O
retrieval O
( O
petroni O
et O
al. O
, O
2019 O
; O
jiang O
et O
al. O
, O
2020b O
; O
shin O
et O
al. O
, O
2020 O
) O
, O
which O
queries O
mlm O
for O
object O
answer O
only O
given O
the O
subject O
and O
the O
corresponding O
relation O
prompt O
as O
input O
, O
e.g. O
, O
“ O
jobs O
was O
born O
in O
[ O
mask O
] O
. O
” O
• O
case-based O
analogy O
( O
brown O
et O
al. O
, O
2020 O
; O
madotto O
et O
al. O
, O
2020 O
; O
gao O
et O
al. O
, O
2020 O
) O
, O
which O
enhances O
the O
prompt-based O
retrieval O
with O
several O
illustrative O
cases O
, O
e.g. O
, O
“ O
obama O
was O
born O
in O
hawaii O
. O

section 1
id pdf2json/2021.acl-long.146.pdf.json
[ O
sep O
] O
jobs O
was O
born O
in O
[ O
mask O
] O
. O
” O
• O
context-based O
inference O
( O
petroni O
et O
al. O
, O
2020 O
; O
bian O
et O
al. O
, O
2021 O
) O
, O
which O
augments O
the O
prompt-based O
retrieval O
with O
external O
relevant O
contexts O
, O
e.g. O
, O
“ O
jobs O
lives O
in O
california O
. O

section 1
id pdf2json/2021.acl-long.146.pdf.json
[ O
sep O
] O
jobs O
was O
born O
in O
[ O
mask O
] O
. O
” O
surprisingly O
, O
the O
main O
conclusions O
of O
this O
paper O
somewhat O
diverge O
from O
previous O
findings O
in O
published O
literatures O
, O
which O
are O
summarized O
in O
figure O
1 O
. O

section 1
id pdf2json/2021.acl-long.146.pdf.json
for O
prompt-based O
paradigm O
( O
§ O
3 O
) O
, O
we O
find O
that O
the O
prediction O
distribution O
of O
mlms O
is O
significantly O
prompt-biased O
. O

section 1
id pdf2json/2021.acl-long.146.pdf.json
specifically O
, O
we O
find O
that O
prompt-based O
retrieval O
generates O
similar O
predictions O
on O
totally O
different O
datasets O
. O

section 1
id pdf2json/2021.acl-long.146.pdf.json
and O
predictions O
are O
spuriously O
correlated O
with O
the O
applied O
prompts O
, O
rather O
than O
the O
facts O
we O
want O
to O
extract O
. O

section 1
id pdf2json/2021.acl-long.146.pdf.json
therefore O
, O
previous O
decent O
performance O
mainly O
stems O
from O
the O
prompt O
over-fitting O
the O
dataset O
answer O
distribution O
, O
rather O
than O
mlms O
’ O
knowledge O
extraction O
ability O
. O

section 1
id pdf2json/2021.acl-long.146.pdf.json
our O
findings O
strongly O
question O
the O
conclusions O
of O
previous O
literatures O
, O
and O
demonstrate O
that O
current O
mlms O
can O
not O
serve O
as O
reliable O
knowledge O
bases O
when O
using O
prompt-based O
retrieval O
paradigm O
. O

section 1
id pdf2json/2021.acl-long.146.pdf.json
2this O
paper O
shows O
the O
experimental O
results O
on O
bert-large O
because O
previous O
work O
has O
shown O
that O
it O
can O
achieve O
the O
best O
performance O
on O
factual O
knowledge O
extraction O
among O
all O
mlms O
. O

section 1
id pdf2json/2021.acl-long.146.pdf.json
in O
the O
appendix O
, O
we O
also O
report O
the O
experimental O
results O
on O
roberta-large O
, O
which O
also O
reach O
the O
main O
conclusions O
reported O
in O
the O
paper O
. O

section 1
id pdf2json/2021.acl-long.146.pdf.json
for O
case-based O
paradigm O
( O
§ O
4 O
) O
, O
we O
find O
that O
the O
illustrative O
cases O
mainly O
provide O
a O
“ O
type O
guidance O
” O
for O
mlms O
. O

section 1
id pdf2json/2021.acl-long.146.pdf.json
to O
show O
this O
, O
we O
propose O
a O
novel O
algorithm O
to O
induce O
the O
object O
type O
of O
each O
relation O
based O
on O
wikidata3 O
taxonomy O
. O

section 1
id pdf2json/2021.acl-long.146.pdf.json
according O
to O
the O
induced O
types O
, O
we O
find O
that O
the O
performance O
gain O
brought O
by O
illustrative O
cases O
mainly O
owes O
to O
the O
improvement O
on O
recognizing O
object O
type O
. O

section 1
id pdf2json/2021.acl-long.146.pdf.json
by O
contrast O
, O
it O
can O
not O
help O
mlms O
select O
the O
correct O
answer O
from O
the O
entities O
with O
the O
same O
type O
: O
the O
rank O
of O
answer O
within O
its O
entity O
type O
is O
changed O
randomly O
after O
introducing O
illustrative O
cases O
. O

section 1
id pdf2json/2021.acl-long.146.pdf.json
that O
is O
to O
say O
, O
under O
the O
case-based O
paradigm O
, O
although O
mlms O
can O
effectively O
analogize O
between O
entities O
with O
the O
same O
type O
, O
they O
still O
can O
not O
well O
identify O
the O
exact O
target O
object O
based O
on O
their O
internal O
knowledge O
and O
the O
provided O
illustrative O
cases O
. O

section 1
id pdf2json/2021.acl-long.146.pdf.json
for O
context-based O
paradigm O
( O
§ O
5 O
) O
, O
we O
find O
that O
context O
can O
help O
the O
factual O
knowledge O
extraction O
mainly O
because O
it O
explicitly O
or O
implicitly O
leaks O
the O
correct O
answer O
. O

section 1
id pdf2json/2021.acl-long.146.pdf.json
specifically O
, O
the O
knowledge O
extraction O
performance O
improvement O
mainly O
happens O
when O
the O
introduced O
context O
contains O
the O
answer O
. O

section 1
id pdf2json/2021.acl-long.146.pdf.json
furthermore O
, O
when O
we O
mask O
the O
answer O
in O
the O
context O
, O
the O
performance O
still O
significantly O
improves O
as O
long O
as O
mlms O
can O
correctly O
reconstruct O
the O
masked O
answer O
in O
the O
remaining O
context O
. O

section 1
id pdf2json/2021.acl-long.146.pdf.json
in O
other O
words O
, O
in O
these O
instances O
, O
the O
context O
itself O
servers O
as O
a O
delegator O
of O
the O
masked O
answer O
, O
and O
therefore O
mlms O
can O
still O
obtain O
sufficient O
implicit O
answer O
evidence O
even O
the O
answer O
doesn O
’ O
t O
explicitly O
appear O
. O

section 1
id pdf2json/2021.acl-long.146.pdf.json
all O
the O
above O
findings O
demonstrate O
that O
current O
mlms O
are O
not O
reliable O
in O
factual O
knowledge O
extraction O
. O

section 1
id pdf2json/2021.acl-long.146.pdf.json
furthermore O
, O
this O
paper O
sheds O
some O
light O
on O
the O
underlying O
predicting O
mechanisms O
of O
mlms O
, O
which O
can O
potentially O
benefit O
many O
future O
studies O
. O

section 2
id pdf2json/2021.acl-long.146.pdf.json
the O
great O
success O
of O
pre-trained O
language O
models O
( O
plms O
) O
raises O
the O
question O
of O
whether O
plms O
can O
be O
directly O
used O
as O
reliable O
knowledge O
bases O
. O

section 2
id pdf2json/2021.acl-long.146.pdf.json
petroni O
et O
al O
. O

section 2
id pdf2json/2021.acl-long.146.pdf.json
( O
2019 O
) O
propose O
the O
lama O
benchmark O
, O
which O
probes O
knowledge O
in O
plms O
using O
prompt-based O
retrieval O
. O

section 2
id pdf2json/2021.acl-long.146.pdf.json
jiang O
et O
al O
. O

section 2
id pdf2json/2021.acl-long.146.pdf.json
( O
2020a O
) O
build O
a O
multilingual O
knowledge O
probing O
benchmark O
based O
on O
lama O
. O

section 2
id pdf2json/2021.acl-long.146.pdf.json
there O
are O
many O
studies O
focus O
on O
probing O
specific O
knowledge O
in O
plms O
, O
such O
as O
linguistic O
knowledge O
( O
lin O
et O
al. O
, O
2019 O
; O
tenney O
et O
al. O
, O
2019 O
; O
liu O
et O
al. O
, O
2019a O
; O
htut O
et O
al. O
, O
2019 O
; O
hewitt O
and O
manning O
, O
2019 O
; O
goldberg O
, O
2019 O
; O
warstadt O
et O
al. O
, O
2019 O
) O
, O
3www.wikidata.org O
semantic O
knowledge O
( O
tenney O
et O
al. O
, O
2019 O
; O
wallace O
et O
al. O
, O
2019 O
; O
ettinger O
, O
2020 O
) O
and O
world O
knowledge O
( O
davison O
et O
al. O
, O
2019 O
; O
bouraoui O
et O
al. O
, O
2020 O
; O
forbes O
et O
al. O
, O
2019 O
; O
zhou O
et O
al. O
, O
2019 O
; O
roberts O
et O
al. O
, O
2020 O
; O
lin O
et O
al. O
, O
2020 O
; O
tamborrino O
et O
al. O
, O
2020 O
) O
. O

section 2
id pdf2json/2021.acl-long.146.pdf.json
recently O
, O
some O
studies O
doubt O
the O
reliability O
of O
plms O
as O
knowledge O
base O
by O
discovering O
the O
the O
spurious O
correlation O
to O
surface O
forms O
( O
mccoy O
et O
al. O
, O
2019 O
; O
poerner O
et O
al. O
, O
2020 O
; O
shwartz O
et O
al. O
, O
2020 O
) O
, O
and O
their O
sensitivity O
to O
“ O
negation O
” O
and O
“ O
mispriming O
” O
( O
kassner O
and O
schütze O
, O
2020b O
) O
. O

section 2
id pdf2json/2021.acl-long.146.pdf.json
currently O
, O
there O
are O
three O
main O
paradigms O
for O
knowledge O
extraction O
from O
plms O
: O
prompt-based O
retrieval O
( O
schick O
and O
schütze O
, O
2021 O
; O
li O
and O
liang O
, O
2021 O
) O
, O
case-based O
analogy O
( O
schick O
and O
schütze O
, O
2020a O
, O
b O
) O
, O
and O
context-based O
inference O
. O

section 2
id pdf2json/2021.acl-long.146.pdf.json
for O
promptbased O
retrieval O
, O
current O
studies O
focus O
on O
seeking O
better O
prompts O
by O
either O
mining O
from O
corpus O
( O
jiang O
et O
al. O
, O
2020b O
) O
or O
learning O
using O
labeled O
data O
( O
shin O
et O
al. O
, O
2020 O
) O
. O

section 2
id pdf2json/2021.acl-long.146.pdf.json
for O
case-based O
analogy O
, O
current O
studies O
mostly O
focus O
on O
whether O
good O
cases O
will O
lead O
to O
good O
few-shot O
abilities O
, O
and O
many O
tasks O
are O
tried O
( O
brown O
et O
al. O
, O
2020 O
; O
madotto O
et O
al. O
, O
2020 O
; O
gao O
et O
al. O
, O
2020 O
) O
. O

section 2
id pdf2json/2021.acl-long.146.pdf.json
for O
context-based O
inference O
, O
current O
studies O
focus O
on O
enhancing O
the O
prediction O
by O
seeking O
more O
informative O
contexts O
, O
e.g. O
, O
for O
knowledge O
extraction O
( O
petroni O
et O
al. O
, O
2020 O
) O
and O
commonsenseqa O
( O
bian O
et O
al. O
, O
2021 O
) O
. O

section 2
id pdf2json/2021.acl-long.146.pdf.json
however O
, O
there O
is O
no O
previous O
work O
which O
focuses O
on O
systematically O
study O
the O
underlying O
predicting O
mechanisms O
of O
mlms O
on O
these O
paradigms O
. O

section 3
id pdf2json/2021.acl-long.146.pdf.json
the O
prompt-based O
retrieval O
extracts O
factual O
knowledge O
by O
querying O
mlms O
with O
( O
subject O
, O
prompt O
, O
[ O
mask O
] O
) O
. O

section 3
id pdf2json/2021.acl-long.146.pdf.json
for O
example O
, O
to O
extract O
the O
“ O
place-of-birth O
” O
of O
steve O
jobs O
, O
we O
could O
query O
bert O
with O
“ O
steve O
jobs O
was O
born O
in O
[ O
mask O
] O
. O
” O
and O
the O
predicted O
“ O
california O
” O
would O
be O
regarded O
as O
the O
answer O
. O

section 3
id pdf2json/2021.acl-long.146.pdf.json
we O
consider O
three O
kinds O
of O
prompts O
: O
the O
manually O
prompts O
tman O
created O
by O
petroni O
et O
al O
. O

section 3
id pdf2json/2021.acl-long.146.pdf.json
( O
2019 O
) O
, O
the O
mining-based O
prompts O
tmine O
by O
jiang O
et O
al O
. O

section 3
id pdf2json/2021.acl-long.146.pdf.json
( O
2020b O
) O
and O
the O
automatically O
searched O
prompts O
tauto O
from O
shin O
et O
al O
. O

section 3
id pdf2json/2021.acl-long.146.pdf.json
( O
2020 O
) O
. O

section 4
id pdf2json/2021.acl-long.146.pdf.json
conclusion O
1 O
. O

section 4
id pdf2json/2021.acl-long.146.pdf.json
prompt-based O
retrieval O
is O
promptbiased O
. O

section 4
id pdf2json/2021.acl-long.146.pdf.json
as O
a O
result O
, O
previous O
decent O
performance O
actually O
measures O
how O
well O
the O
applied O
prompts O
fit O
the O
dataset O
answer O
distribution O
, O
rather O
than O
the O
factual O
knowledge O
extraction O
ability O
from O
mlms O
. O

section 4
id pdf2json/2021.acl-long.146.pdf.json
lon O
don O
pari O
s O
rom O
e O
toky O
o O
bos O
ton O
chic O
ago O
mon O
trea O
l O
ber O
lin O
mila O
n O
mos O
cow O
lama O
wiki-uni O
0.1 O
0.2 O
( O
a O
) O
the O
true O
answer O
distributions O
are O
very O
different O
between O
lama O
and O
wiki-uni O
. O

section 4
id pdf2json/2021.acl-long.146.pdf.json
lon O
don O
pari O
s O
rom O
e O
toky O
o O
bos O
ton O
chic O
ago O
mon O
trea O
l O
ber O
lin O
mila O
n O
mos O
cow O
lama O
wiki-uni O
0.2 O
0.4 O
( O
b O
) O
however O
, O
the O
prediction O
distribution O
made O
by O
mlms O
on O
them O
are O
still O
very O
similar O
. O

section 4
id pdf2json/2021.acl-long.146.pdf.json
figure O
2 O
: O
an O
illustration O
example O
of O
the O
vastly O
different O
answer O
distributions O
but O
similar O
prediction O
distributions O
on O
lama O
and O
wiki-uni O
on O
“ O
place-of-birth O
” O
relation O
. O

section 4
id pdf2json/2021.acl-long.146.pdf.json
specifically O
, O
we O
conduct O
studies O
and O
find O
that O
1 O
) O
prompt-based O
retrieval O
will O
generate O
similar O
responses O
given O
quite O
different O
datasets O
. O

section 4
id pdf2json/2021.acl-long.146.pdf.json
to O
show O
this O
, O
we O
construct O
a O
new O
dataset O
from O
wikidata O
– O
wikiuni O
, O
which O
have O
a O
totally O
different O
answer O
distribution O
from O
the O
widely-used O
lama4 O
dataset O
( O
petroni O
et O
al. O
, O
2019 O
) O
. O

section 4
id pdf2json/2021.acl-long.146.pdf.json
however O
, O
we O
find O
that O
the O
prediction O
distributions O
on O
wiki-uni O
and O
lama O
are O
highly O
correlated O
, O
and O
this O
spurious O
correlation O
holds O
across O
different O
prompts O
. O

section 4
id pdf2json/2021.acl-long.146.pdf.json
such O
results O
reveal O
that O
there O
is O
just O
a O
weak O
correlation O
between O
the O
predictions O
of O
mlms O
and O
the O
factual O
answer O
distribution O
of O
the O
dataset O
. O

section 4
id pdf2json/2021.acl-long.146.pdf.json
2 O
) O
the O
prediction O
distribution O
is O
dominated O
by O
the O
prompt O
, O
i.e. O
, O
the O
prediction O
distribution O
using O
only O
( O
prompt O
, O
[ O
mask O
] O
) O
is O
highly O
correlated O
to O
the O
prediction O
distribution O
using O
( O
subject O
, O
prompt O
, O
[ O
mask O
] O
) O
. O

section 4
id pdf2json/2021.acl-long.146.pdf.json
this O
indicates O
that O
it O
is O
the O
applied O
prompts O
, O
rather O
than O
the O
actual O
facts O
, O
determine O
the O
predictions O
of O
mlms O
. O

section 4
id pdf2json/2021.acl-long.146.pdf.json
3 O
) O
the O
performance O
of O
the O
prompt O
can O
be O
predicted O
by O
the O
divergence O
between O
the O
prompt-only O
distribution O
and O
the O
answer O
distribution O
of O
the O
dataset O
. O

section 4
id pdf2json/2021.acl-long.146.pdf.json
all O
these O
findings O
reveal O
that O
previous O
decent O
performance O
in O
this O
field O
actually O
measures O
the O
degree O
of O
prompt-dataset O
fitness O
, O
rather O
than O
the O
universal O
factual O
knowledge O
extraction O
ability O
. O

section 5
id pdf2json/2021.acl-long.146.pdf.json
finding O
1 O
. O

section 5
id pdf2json/2021.acl-long.146.pdf.json
prompt-based O
retrieval O
will O
generate O
similar O
responses O
to O
quite O
different O
datasets O
. O

section 5
id pdf2json/2021.acl-long.146.pdf.json
a O
reliable O
knowledge O
extractor O
should O
generate O
4since O
we O
focus O
on O
factual O
knowledge O
, O
we O
use O
the O
t- O
rex O
( O
elsahar O
et O
al. O
, O
2018 O
) O
subset O
of O
the O
lama O
benchmark O
. O

section 5
id pdf2json/2021.acl-long.146.pdf.json
different O
responses O
to O
different O
knowledge O
queries O
. O

section 5
id pdf2json/2021.acl-long.146.pdf.json
to O
verify O
whether O
mlms O
meet O
this O
standard O
, O
we O
manually O
construct O
a O
new O
dataset O
– O
wiki-uni O
, O
which O
has O
a O
comparable O
size O
but O
totally O
different O
answer O
distribution O
to O
lama O
, O
and O
then O
compare O
the O
prediction O
distributions O
on O
them O
. O

section 5
id pdf2json/2021.acl-long.146.pdf.json
for O
a O
fair O
comparison O
, O
we O
follow O
the O
construction O
criteria O
of O
lama O
: O
we O
use O
the O
same O
41 O
relations O
, O
filter O
out O
the O
queries O
whose O
objects O
are O
not O
in O
the O
mlms O
’ O
vocabulary O
. O

section 5
id pdf2json/2021.acl-long.146.pdf.json
compared O
with O
lama O
, O
the O
major O
difference O
is O
that O
wiki-uni O
has O
a O
uniform O
answer O
distribution O
, O
i.e. O
, O
for O
each O
relation O
, O
we O
keep O
the O
same O
number O
of O
instances O
for O
each O
object O
. O

section 5
id pdf2json/2021.acl-long.146.pdf.json
please O
refer O
to O
appendix O
for O
more O
construction O
details O
. O

section 5
id pdf2json/2021.acl-long.146.pdf.json
figure O
2a O
shows O
the O
answer O
distributions O
of O
lama O
and O
wiki-uni O
on O
relation O
“ O
place-of-birth O
” O
. O

section 5
id pdf2json/2021.acl-long.146.pdf.json
we O
can O
see O
that O
the O
answers O
in O
lama O
are O
highly O
concentrated O
on O
the O
head O
object O
entities O
, O
while O
the O
answers O
in O
wiki-uni O
follow O
a O
uniform O
distribution O
. O

section 5
id pdf2json/2021.acl-long.146.pdf.json
given O
lama O
and O
wiki-uni O
, O
we O
investigate O
the O
predicting O
behaviors O
of O
mlms O
. O

section 5
id pdf2json/2021.acl-long.146.pdf.json
surprisingly O
, O
the O
prediction O
distributions O
on O
these O
two O
totally O
different O
datasets O
are O
highly O
correlated O
. O

section 5
id pdf2json/2021.acl-long.146.pdf.json
figure O
2b O
shows O
an O
example O
. O

section 5
id pdf2json/2021.acl-long.146.pdf.json
we O
can O
see O
that O
the O
prediction O
distribution O
on O
wiki-uni O
is O
very O
similar O
to O
that O
on O
lama O
. O

section 5
id pdf2json/2021.acl-long.146.pdf.json
and O
these O
two O
distributions O
are O
both O
close O
to O
the O
answer O
distribution O
of O
lama O
but O
far O
away O
from O
the O
answer O
distribution O
of O
wiki-uni O
. O

section 5
id pdf2json/2021.acl-long.146.pdf.json
to O
investigate O
whether O
this O
spurious O
correlation O
is O
a O
common O
phenomenon O
, O
we O
analyze O
the O
pearson O
correlation O
coefficient O
between O
prediction O
distributions O
on O
lama O
and O
wiki-uni O
across O
different O
relations O
and O
three O
kinds O
of O
prompts O
. O

section 5
id pdf2json/2021.acl-long.146.pdf.json
the O
boxplot O
in O
figure O
3 O
shows O
the O
very O
significant O
correlation O
between O
the O
prediction O
distributions O
on O
lama O
and O
wiki-uni O
: O
on O
all O
three O
kinds O
of O
prompts O
, O
the O
correlation O
coefficients O
exceed O
0.8 O
in O
more O
than O
half O
of O
relations O
. O

section 5
id pdf2json/2021.acl-long.146.pdf.json
these O
results O
demonstrate O
that O
promptbased O
retrieval O
will O
lead O
to O
very O
similar O
prediction O
distributions O
even O
when O
test O
sets O
have O
vastly O
different O
answer O
distributions O
. O

section 5
id pdf2json/2021.acl-long.146.pdf.json
furthermore O
, O
we O
find O
that O
the O
prediction O
distribution O
obviously O
doesn O
’ O
t O
correspond O
to O
the O
answer O
distribution O
of O
wiki-uni O
. O

section 5
id pdf2json/2021.acl-long.146.pdf.json
from O
table O
1 O
, O
we O
can O
see O
that O
on O
average O
, O
the O
top-5 O
answers O
of O
each O
relation O
in O
wiki-uni O
cover O
only O
7.78 O
% O
instances O
. O

section 5
id pdf2json/2021.acl-long.146.pdf.json
by O
contrast O
, O
the O
top-5 O
predictions O
of O
each O
relation O
in O
wiki-uni O
cover O
more O
than O
52 O
% O
instances O
, O
which O
is O
close O
to O
the O
answer O
distribution O
and O
prediction O
distribution O
on O
lama O
. O

section 5
id pdf2json/2021.acl-long.146.pdf.json
as O
a O
result O
, O
the O
performance O
on O
wiki-uni O
( O
mean O
p O
@ O
1 O
: O
16.47 O
) O
is O
significantly O
worse O
than O
that O
on O
lama O
( O
mean O
p O
@ O
1 O
: O
30.36 O
) O
. O

section 5
id pdf2json/2021.acl-long.146.pdf.json
in O
conclusion O
, O
the O
facts O
of O
a O
dataset O
can O
not O
explain O
the O
predictions O
of O
mlms O
, O
and O
therefore O
previous O
evaluations O
of O
the O
mlms O
’ O
ability O
on O
factual O
knowledge O
extraction O
are O
unreliable O
. O

section 6
id pdf2json/2021.acl-long.146.pdf.json
finding O
2 O
. O

section 6
id pdf2json/2021.acl-long.146.pdf.json
the O
prediction O
distribution O
is O
severely O
prompt-biased O
. O

section 6
id pdf2json/2021.acl-long.146.pdf.json
to O
investigate O
the O
underlying O
factors O
of O
the O
predicting O
behavior O
of O
mlms O
, O
we O
compare O
the O
promptonly O
prediction O
distribution O
using O
only O
( O
prompt O
, O
[ O
mask O
] O
) O
and O
the O
full O
prediction O
distribution O
using O
( O
subject O
, O
prompt O
, O
[ O
mask O
] O
) O
. O

section 6
id pdf2json/2021.acl-long.146.pdf.json
to O
obtain O
the O
promptonly O
distribution O
, O
we O
mask O
the O
subject O
and O
then O
use O
( O
[ O
mask O
] O
, O
prompt O
, O
[ O
mask O
] O
) O
to O
query O
mlms O
( O
e.g. O
, O
[ O
mask O
] O
was O
born O
in O
[ O
mask O
] O
) O
. O

section 6
id pdf2json/2021.acl-long.146.pdf.json
because O
there O
is O
no O
subject O
information O
in O
the O
input O
, O
mlms O
can O
only O
depend O
on O
applied O
prompt O
’ O
s O
information O
to O
make O
the O
prediction O
at O
the O
second O
[ O
mask O
] O
. O

section 6
id pdf2json/2021.acl-long.146.pdf.json
therefore O
, O
we O
regard O
the O
probability O
distribution O
at O
the O
second O
[ O
mask O
] O
symbol O
as O
the O
prompt-only O
distribution O
. O

section 6
id pdf2json/2021.acl-long.146.pdf.json
after O
that O
, O
we O
analyze O
the O
correlations O
between O
the O
prompt-only O
distribution O
and O
the O
prediction O
distribution O
on O
wiki-uni O
dataset O
. O

section 6
id pdf2json/2021.acl-long.146.pdf.json
figure O
4 O
shows O
the O
boxplot O
. O

section 6
id pdf2json/2021.acl-long.146.pdf.json
on O
all O
three O
kinds O
of O
prompts O
, O
correlation O
coefficients O
between O
the O
prompt-only O
distribution O
and O
the O
prediction O
distribution O
on O
wiki-uni O
exceed O
0.6 O
in O
more O
than O
half O
of O
relations O
. O

section 6
id pdf2json/2021.acl-long.146.pdf.json
this O
demonstrates O
that O
in O
these O
relations O
, O
the O
promptonly O
distribution O
dominates O
the O
prediction O
distribution O
. O

section 6
id pdf2json/2021.acl-long.146.pdf.json
combining O
with O
the O
findings O
in O
section O
3.2 O
, O
we O
can O
summarize O
that O
the O
prompt-based O
retrieval O
is O
mainly O
based O
on O
guided O
guessing O
, O
i.e. O
, O
the O
predictions O
are O
generated O
by O
sampling O
from O
the O
promptbiased O
distribution O
guided O
by O
the O
moderate O
impact O
of O
subjects O
. O

section 6
id pdf2json/2021.acl-long.146.pdf.json
note O
that O
among O
a O
minor O
part O
of O
relations O
, O
the O
correlations O
between O
the O
prompt-only O
distribution O
and O
the O
prediction O
distribution O
are O
relatively O
low O
. O

section 6
id pdf2json/2021.acl-long.146.pdf.json
we O
find O
that O
the O
main O
reason O
is O
the O
type O
selectional O
preference O
provided O
by O
the O
subject O
entities O
, O
and O
section O
4 O
will O
further O
discuss O
the O
impact O
of O
this O
type-guidance O
mechanism O
for O
mlms O
. O

section 7
id pdf2json/2021.acl-long.146.pdf.json
finding O
3 O
. O

section 7
id pdf2json/2021.acl-long.146.pdf.json
“ O
better O
” O
prompts O
are O
the O
prompts O
fitting O
the O
answer O
distribution O
better O
, O
rather O
than O
the O
prompts O
with O
better O
retrieval O
ability O
. O

section 7
id pdf2json/2021.acl-long.146.pdf.json
some O
previous O
literatures O
attempt O
to O
find O
better O
prompts O
for O
factual O
knowledge O
extraction O
from O
mlms O
. O

section 7
id pdf2json/2021.acl-long.146.pdf.json
however O
, O
as O
we O
mentioned O
above O
, O
the O
prompt O
itself O
will O
lead O
to O
a O
biased O
prediction O
distribution O
. O

section 7
id pdf2json/2021.acl-long.146.pdf.json
this O
raises O
our O
concern O
that O
whether O
the O
found O
better O
prompts O
are O
really O
with O
better O
knowledge O
extraction O
ability O
, O
or O
the O
better O
performance O
just O
come O
from O
the O
over-fitting O
between O
the O
promptonly O
distribution O
and O
the O
answer O
distribution O
of O
the O
test O
set O
. O

section 7
id pdf2json/2021.acl-long.146.pdf.json
to O
answer O
this O
question O
, O
we O
evaluate O
the O
kl O
divergence O
between O
the O
prompt-only O
distribution O
and O
the O
answer O
distribution O
of O
lama O
on O
different O
kinds O
of O
prompts O
. O

section 7
id pdf2json/2021.acl-long.146.pdf.json
the O
results O
are O
shown O
in O
table O
2 O
. O

section 7
id pdf2json/2021.acl-long.146.pdf.json
we O
find O
that O
the O
kl O
divergence O
is O
a O
strong O
indicator O
of O
the O
performance O
of O
a O
prompt O
, O
i.e. O
, O
the O
smaller O
the O
kl O
divergence O
between O
the O
promptonly O
distribution O
and O
the O
answer O
distribution O
of O
the O
test O
set O
is O
, O
the O
better O
performance O
the O
prompt O
achieve O
. O

section 7
id pdf2json/2021.acl-long.146.pdf.json
furthermore O
, O
table O
3 O
shows O
several O
comparisons O
between O
different O
kinds O
of O
prompts O
and O
their O
performance O
on O
lama O
. O

section 7
id pdf2json/2021.acl-long.146.pdf.json
we O
can O
easily O
observe O
that O
the O
better-performed O
prompts O
are O
actually O
over-fitting O
the O
dataset O
, O
rather O
than O
better O
capturing O
the O
underlying O
semantic O
of O
the O
relation O
. O

section 7
id pdf2json/2021.acl-long.146.pdf.json
as O
a O
result O
, O
previous O
prompt O
searching O
studies O
are O
actually O
optimized O
on O
the O
spurious O
prompt-dataset O
compatibility O
, O
rather O
than O
the O
universal O
factual O
knowledge O
extraction O
ability O
. O

section 8
id pdf2json/2021.acl-long.146.pdf.json
the O
case-based O
analogy O
enhances O
the O
prompt-based O
paradigm O
with O
several O
illustrative O
cases O
. O

section 8
id pdf2json/2021.acl-long.146.pdf.json
for O
example O
, O
if O
we O
want O
to O
know O
the O
“ O
place-of-birth O
” O
of O
steve O
jobs O
, O
we O
would O
first O
sample O
cases O
such O
as O
( O
obama O
, O
place-of-birth O
, O
hawaii O
) O
, O
and O
combine O
them O
with O
the O
original O
query O
. O

section 8
id pdf2json/2021.acl-long.146.pdf.json
in O
this O
way O
, O
we O
will O
use O
“ O
obama O
was O
born O
in O
hawaii O
. O

section 8
id pdf2json/2021.acl-long.146.pdf.json
[ O
sep O
] O
steve O
jobs O
was O
born O
in O
[ O
mask O
] O
. O
” O
to O
query O
mlms O
. O

section 9
id pdf2json/2021.acl-long.146.pdf.json
conclusion O
2 O
. O

section 9
id pdf2json/2021.acl-long.146.pdf.json
illustrative O
cases O
guide O
mlms O
to O
better O
recognizing O
object O
type O
, O
rather O
than O
better O
predicting O
facts O
. O

section 9
id pdf2json/2021.acl-long.146.pdf.json
to O
show O
this O
, O
we O
first O
design O
an O
effective O
algorithm O
to O
induce O
the O
type O
of O
an O
entity O
set O
based O
on O
wikidata O
taxonomy O
, O
which O
can O
identify O
the O
object O
type O
of O
a O
relation O
. O

section 9
id pdf2json/2021.acl-long.146.pdf.json
according O
to O
the O
induced O
types O
, O
we O
find O
that O
the O
benefits O
of O
illustrative O
cases O
mainly O
stem O
from O
the O
promotion O
of O
object O
type O
recognition O
. O

section 9
id pdf2json/2021.acl-long.146.pdf.json
in O
other O
words O
, O
case-based O
analogy O
guides O
mlms O
with O
better O
type O
prediction O
ability O
but O
contributes O
little O
to O
the O
entity O
prediction O
ability O
. O

section 9
id pdf2json/2021.acl-long.146.pdf.json
in O
the O
following O
, O
we O
first O
illustrate O
our O
type O
inducing O
algorithm O
, O
and O
then O
explain O
how O
we O
reach O
the O
conclusion O
. O

section 10
id pdf2json/2021.acl-long.146.pdf.json
to O
induce O
the O
object O
type O
of O
a O
relation O
, O
we O
first O
collect O
all O
its O
objects O
in O
lama O
and O
form O
an O
entity O
set O
. O

section 10
id pdf2json/2021.acl-long.146.pdf.json
then O
we O
induce O
the O
type O
of O
an O
entity O
set O
by O
designing O
a O
simple O
but O
effective O
algorithm O
. O

section 10
id pdf2json/2021.acl-long.146.pdf.json
the O
main O
intuition O
behind O
our O
algorithm O
is O
that O
a O
representative O
type O
should O
be O
the O
finest O
grained O
type O
that O
can O
cover O
a O
sufficient O
number O
of O
the O
instances O
in O
the O
entity O
set O
. O

section 10
id pdf2json/2021.acl-long.146.pdf.json
figure O
5 O
shows O
an O
example O
of O
our O
algorithm O
. O

section 10
id pdf2json/2021.acl-long.146.pdf.json
given O
a O
set O
of O
entities O
in O
wikidata O
, O
we O
first O
construct O
an O
entity O
type O
graph O
( O
etg O
) O
by O
recursively O
introducing O
all O
ancestor O
entity O
types O
according O
to O
the O
instance-of O
and O
subclass-of O
relations O
. O

section 10
id pdf2json/2021.acl-long.146.pdf.json
for O
the O
example O
in O
figure O
5 O
, O
chicago O
is O
in O
the O
entity O
set O
and O
is O
an O
instance-of O
big O
city O
. O

section 10
id pdf2json/2021.acl-long.146.pdf.json
big O
city O
is O
a O
subclass-of O
city O
. O

section 10
id pdf2json/2021.acl-long.146.pdf.json
as O
a O
result O
, O
chicago O
, O
big O
city O
and O
city O
will O
all O
be O
introduced O
into O
etg O
. O

section 10
id pdf2json/2021.acl-long.146.pdf.json
then O
we O
apply O
topological O
sorting O
( O
cook O
, O
1985 O
) O
to O
etg O
to O
obtain O
a O
fine-to-coarse O
entity O
type O
sequence O
. O

section 10
id pdf2json/2021.acl-long.146.pdf.json
finally O
, O
based O
on O
the O
sequence O
, O
we O
select O
the O
first O
type O
which O
covers O
more O
than O
80 O
% O
of O
entities O
in O
the O
entity O
set O
( O
e.g. O
, O
city O
in O
figure O
5 O
) O
. O

section 10
id pdf2json/2021.acl-long.146.pdf.json
table O
4 O
illustrates O
several O
induced O
types O
, O
and O
please O
refer O
to O
the O
appendix O
for O
details O
. O

section 11
id pdf2json/2021.acl-long.146.pdf.json
finding O
4 O
. O

section 11
id pdf2json/2021.acl-long.146.pdf.json
illustrative O
cases O
help O
mlms O
to O
better O
recognize O
the O
type O
of O
objects O
, O
and O
therefore O
improve O
factual O
knowledge O
extraction O
. O

section 11
id pdf2json/2021.acl-long.146.pdf.json
for O
case-based O
analogy O
, O
the O
first O
thing O
we O
want O
to O
know O
is O
whether O
illustrative O
cases O
can O
improve O
the O
knowledge O
extraction O
performance O
. O

section 11
id pdf2json/2021.acl-long.146.pdf.json
to O
this O
end O
, O
for O
each O
( O
subject O
, O
relation O
) O
query O
in O
lama O
, O
we O
randomly O
sample O
10 O
illustrative O
cases O
. O

section 11
id pdf2json/2021.acl-long.146.pdf.json
to O
avoid O
answer O
leakage O
, O
we O
ensure O
the O
objects O
of O
these O
cases O
don O
’ O
t O
contain O
the O
golden O
answer O
of O
the O
query O
. O

section 11
id pdf2json/2021.acl-long.146.pdf.json
then O
we O
use O
( O
cases O
, O
subject O
, O
prompt O
, O
[ O
mask O
] O
) O
as O
the O
analogous O
query O
to O
mlms O
. O

section 11
id pdf2json/2021.acl-long.146.pdf.json
results O
show O
that O
case-based O
analogy O
can O
significantly O
improve O
performance O
. O

section 11
id pdf2json/2021.acl-long.146.pdf.json
after O
introducing O
illustrative O
cases O
, O
the O
mean O
precision O
increases O
from O
30.36 O
% O
to O
36.23 O
% O
. O

section 11
id pdf2json/2021.acl-long.146.pdf.json
besides O
, O
we O
find O
that O
11.81 O
% O
instances O
can O
benefit O
from O
the O
introduced O
cases O
and O
only O
5.94 O
% O
instances O
are O
undermined O
. O

section 11
id pdf2json/2021.acl-long.146.pdf.json
this O
shows O
that O
case-based O
analogy O
really O
helps O
the O
mlms O
to O
make O
better O
predictions O
. O

section 11
id pdf2json/2021.acl-long.146.pdf.json
by O
analyzing O
the O
predicting O
behaviors O
, O
we O
observe O
that O
the O
main O
benefit O
of O
introducing O
illustrative O
cases O
comes O
from O
the O
better O
type O
recognition O
. O

section 11
id pdf2json/2021.acl-long.146.pdf.json
to O
verify O
this O
observation O
, O
we O
investigate O
how O
the O
types O
of O
predictions O
changed O
after O
introducing O
the O
illustrative O
cases O
. O

section 11
id pdf2json/2021.acl-long.146.pdf.json
table O
4 O
shows O
the O
results O
on O
relations O
whose O
precision O
improvement O
is O
more O
than O
10 O
% O
after O
introducing O
illustrative O
cases O
. O

section 11
id pdf2json/2021.acl-long.146.pdf.json
from O
the O
table O
, O
it O
is O
very O
obvious O
that O
illustrative O
cases O
enhance O
the O
factual O
knowledge O
extraction O
by O
improving O
type O
prediction O
: O
1 O
) O
for O
queries O
whose O
predictions O
are O
correctly O
reversed O
( O
from O
wrong O
to O
right O
) O
, O
the O
vast O
majority O
of O
them O
stems O
from O
the O
revised O
type O
prediction O
; O
2 O
) O
even O
for O
queries O
whose O
predictions O
are O
mistakenly O
reversed O
( O
from O
right O
to O
wrong O
) O
, O
the O
type O
of O
the O
majority O
of O
predictions O
still O
remains O
correct O
. O

section 11
id pdf2json/2021.acl-long.146.pdf.json
in O
conclusion O
, O
introducing O
illustrative O
cases O
can O
significantly O
improve O
the O
knowledge O
extraction O
ability O
by O
recognizing O
the O
object O
type O
more O
accurately O
. O

section 11
id pdf2json/2021.acl-long.146.pdf.json
that O
is O
, O
adding O
illustrative O
cases O
will O
provide O
more O
guidance O
for O
object O
type O
. O

section 12
id pdf2json/2021.acl-long.146.pdf.json
finding O
5 O
. O

section 12
id pdf2json/2021.acl-long.146.pdf.json
illustrative O
cases O
are O
of O
limited O
help O
for O
selecting O
the O
answer O
from O
entities O
of O
the O
same O
type O
. O

section 12
id pdf2json/2021.acl-long.146.pdf.json
to O
show O
this O
, O
we O
introduce O
a O
new O
metric O
referred O
as O
in-type O
rank O
, O
which O
is O
the O
rank O
of O
the O
correct O
answer O
within O
the O
entities O
of O
the O
same O
type O
for O
a O
query O
. O

section 12
id pdf2json/2021.acl-long.146.pdf.json
by O
comparing O
the O
in-type O
rank O
in O
prompt-based O
and O
case-based O
paradigm O
, O
we O
can O
evaluate O
whether O
the O
illustrative O
cases O
can O
actually O
help O
better O
entity O
prediction O
apart O
from O
better O
type O
recognition O
. O

section 12
id pdf2json/2021.acl-long.146.pdf.json
figure O
6 O
shows O
the O
percentages O
on O
the O
change O
of O
overall O
rank O
( O
among O
all O
candidates O
) O
and O
the O
in-type O
rank O
( O
among O
candidates O
with O
the O
same O
type O
) O
of O
golden O
answer O
. O

section 12
id pdf2json/2021.acl-long.146.pdf.json
unfortunately O
, O
we O
find O
that O
illustrative O
cases O
are O
of O
limited O
help O
for O
entity O
prediction O
: O
the O
change O
of O
in-type O
rank O
is O
nearly O
random O
. O

section 12
id pdf2json/2021.acl-long.146.pdf.json
the O
percentages O
of O
queries O
with O
raised/unchanged/dropped O
in-type O
rank O
are O
nearly O
the O
same O
: O
33.05 O
% O
vs O
35.47 O
% O
vs O
31.47 O
% O
. O

section 12
id pdf2json/2021.acl-long.146.pdf.json
furthermore O
, O
we O
find O
that O
the O
mrr O
with O
the O
type O
only O
changed O
from O
0.491 O
to O
0.494 O
, O
which O
shows O
little O
improvement O
after O
introducing O
illustrative O
cases O
. O

section 12
id pdf2json/2021.acl-long.146.pdf.json
these O
results O
show O
that O
the O
raises O
of O
overall O
rank O
of O
golden O
answer O
are O
not O
because O
of O
the O
better O
prediction O
inside O
the O
same O
type O
. O

section 12
id pdf2json/2021.acl-long.146.pdf.json
in O
conclusion O
, O
illustrative O
cases O
can O
not O
well O
guide O
the O
entity O
prediction O
, O
and O
they O
mainly O
benefit O
the O
factual O
knowledge O
extraction O
by O
providing O
guidance O
for O
object O
type O
recognition O
. O

section 13
id pdf2json/2021.acl-long.146.pdf.json
the O
context-based O
inference O
augments O
the O
promptbased O
paradigm O
with O
external O
contexts O
. O

section 13
id pdf2json/2021.acl-long.146.pdf.json
for O
example O
, O
if O
we O
want O
to O
know O
the O
“ O
place-of-birth O
” O
of O
steve O
jobs O
, O
we O
can O
use O
the O
external O
context O
“ O
jobs O
was O
from O
california. O
” O
, O
and O
form O
a O
context-enriched O
query O
“ O
jobs O
was O
from O
california O
. O

section 13
id pdf2json/2021.acl-long.146.pdf.json
[ O
sep O
] O
steve O
jobs O
was O
born O
in O
[ O
mask O
] O
. O
” O
to O
query O
mlms O
. O

section 13
id pdf2json/2021.acl-long.146.pdf.json
specifically O
, O
we O
use O
the O
same O
context O
retrieval O
method O
as O
petroni O
et O
al O
. O

section 13
id pdf2json/2021.acl-long.146.pdf.json
( O
2020 O
) O
: O
for O
each O
instance O
, O
given O
the O
subject O
and O
relation O
as O
query O
, O
we O
use O
the O
first O
paragraph O
of O
drqa O
’ O
s O
( O
chen O
et O
al. O
, O
2017 O
) O
retrieved O
document O
as O
external O
contexts O
. O

section 14
id pdf2json/2021.acl-long.146.pdf.json
conclusion O
3 O
. O

section 14
id pdf2json/2021.acl-long.146.pdf.json
additional O
context O
helps O
mlms O
to O
predict O
the O
answer O
because O
they O
contain O
the O
answer O
, O
explicitly O
or O
implicitly O
. O

section 14
id pdf2json/2021.acl-long.146.pdf.json
several O
studies O
( O
petroni O
et O
al. O
, O
2020 O
; O
bian O
et O
al. O
, O
2021 O
) O
show O
that O
external O
context O
can O
help O
knowledge O
extraction O
from O
mlms O
. O

section 14
id pdf2json/2021.acl-long.146.pdf.json
to O
investigate O
the O
underlying O
mechanism O
, O
we O
evaluate O
which O
kinds O
of O
information O
in O
contexts O
contribute O
to O
the O
fact O
prediction O
, O
and O
find O
that O
the O
improvement O
mainly O
comes O
from O
the O
answer O
leakage O
in O
context O
. O

section 14
id pdf2json/2021.acl-long.146.pdf.json
furthermore O
, O
we O
find O
the O
answers O
can O
not O
only O
be O
leaked O
explicitly O
, O
but O
can O
also O
be O
leaked O
implicitly O
if O
the O
context O
provides O
sufficient O
information O
. O

section 15
id pdf2json/2021.acl-long.146.pdf.json
finding O
6 O
. O

section 15
id pdf2json/2021.acl-long.146.pdf.json
explicit O
answer O
leakage O
significantly O
improves O
the O
prediction O
performance O
. O

section 15
id pdf2json/2021.acl-long.146.pdf.json
to O
show O
this O
, O
we O
split O
lama O
into O
two O
parts O
ac- O
cording O
to O
whether O
the O
additional O
context O
contains O
the O
answer O
. O

section 15
id pdf2json/2021.acl-long.146.pdf.json
table O
5 O
shows O
the O
results O
on O
these O
two O
parts O
respectively O
. O

section 15
id pdf2json/2021.acl-long.146.pdf.json
we O
can O
see O
that O
the O
improvements O
on O
these O
two O
parts O
diverge O
significantly O
. O

section 15
id pdf2json/2021.acl-long.146.pdf.json
for O
context O
containing O
the O
answer O
, O
context-based O
inference O
significantly O
improves O
the O
factual O
knowledge O
extraction O
performance O
. O

section 15
id pdf2json/2021.acl-long.146.pdf.json
however O
, O
there O
is O
even O
a O
little O
performance O
drop O
for O
those O
instances O
whose O
context O
does O
not O
contain O
the O
answer O
. O

section 15
id pdf2json/2021.acl-long.146.pdf.json
this O
indicates O
that O
the O
improvement O
of O
factual O
knowledge O
extraction O
is O
mainly O
due O
to O
the O
explicit O
existence O
of O
the O
answer O
in O
the O
context O
. O

section 16
id pdf2json/2021.acl-long.146.pdf.json
finding O
7 O
. O

section 16
id pdf2json/2021.acl-long.146.pdf.json
implicit O
answer O
leakage O
can O
also O
significantly O
improve O
the O
prediction O
performance O
. O

section 16
id pdf2json/2021.acl-long.146.pdf.json
as O
we O
mentioned O
above O
, O
explicit O
answer O
leakage O
significantly O
helps O
the O
answer O
prediction O
. O

section 16
id pdf2json/2021.acl-long.146.pdf.json
the O
answer-leaked O
context O
may O
explicitly O
provide O
the O
answer O
or O
implicitly O
guide O
the O
prediction O
by O
providing O
answer-specific O
information O
. O

section 16
id pdf2json/2021.acl-long.146.pdf.json
to O
understanding O
the O
underlying O
mechanism O
, O
we O
mask O
the O
answer O
in O
the O
context O
and O
verify O
whether O
it O
can O
still O
achieve O
the O
performance O
gain O
. O

section 16
id pdf2json/2021.acl-long.146.pdf.json
table O
6 O
shows O
the O
results O
. O

section 16
id pdf2json/2021.acl-long.146.pdf.json
we O
find O
that O
the O
performance O
gain O
is O
still O
very O
significant O
after O
masking O
the O
answer O
. O

section 16
id pdf2json/2021.acl-long.146.pdf.json
this O
indicates O
that O
the O
contexts O
previously O
containing O
the O
answer O
are O
still O
very O
effective O
even O
the O
answer O
doesn O
’ O
t O
explicitly O
present O
. O

section 16
id pdf2json/2021.acl-long.146.pdf.json
to O
further O
investigate O
the O
reason O
behind O
, O
we O
split O
the O
masked O
version O
of O
answer-leaked O
instances O
into O
two O
groups O
by O
whether O
mlms O
can O
or O
can O
not O
correctly O
reconstruct O
the O
masked O
answer O
from O
the O
re- O
maining O
context O
. O

section 16
id pdf2json/2021.acl-long.146.pdf.json
the O
results O
are O
shown O
in O
table O
7 O
. O

section 16
id pdf2json/2021.acl-long.146.pdf.json
we O
can O
see O
that O
the O
performance O
gain O
significantly O
diverges O
in O
these O
two O
groups O
: O
the O
improvements O
mainly O
come O
from O
the O
instances O
whose O
answer O
in O
context O
can O
be O
reconstructed O
– O
we O
refer O
to O
this O
as O
implicit O
answer O
leakage O
. O

section 16
id pdf2json/2021.acl-long.146.pdf.json
that O
is O
to O
say O
, O
for O
these O
instances O
, O
the O
context O
serves O
as O
a O
sufficient O
delegator O
of O
its O
answer O
, O
and O
therefore O
mlms O
can O
obtain O
sufficient O
answer O
evidence O
even O
the O
answer O
does O
not O
explicitly O
appear O
. O

section 16
id pdf2json/2021.acl-long.146.pdf.json
however O
, O
for O
contexts O
that O
can O
not O
reconstruct O
the O
masked O
answer O
, O
the O
improvements O
are O
relatively O
minor O
. O

section 16
id pdf2json/2021.acl-long.146.pdf.json
in O
conclusion O
, O
the O
real O
efficacy O
of O
context-based O
inference O
comes O
from O
the O
sufficient O
answer O
evidence O
provided O
by O
the O
context O
, O
either O
explicitly O
or O
implicitly O
. O

section 17
id pdf2json/2021.acl-long.146.pdf.json
in O
this O
paper O
, O
we O
thoroughly O
study O
the O
underlying O
mechanisms O
of O
mlms O
on O
three O
representative O
factual O
knowledge O
extraction O
paradigms O
. O

section 17
id pdf2json/2021.acl-long.146.pdf.json
we O
find O
that O
the O
prompt-based O
retrieval O
is O
severely O
promptbiased O
, O
illustrative O
cases O
enhance O
mlms O
mainly O
via O
type O
guidance O
, O
and O
external O
contexts O
help O
knowledge O
prediction O
mostly O
because O
they O
contain O
the O
correct O
answer O
, O
explicitly O
or O
implicitly O
. O

section 17
id pdf2json/2021.acl-long.146.pdf.json
these O
findings O
strongly O
question O
previous O
conclusions O
that O
current O
mlms O
could O
serve O
as O
reliable O
factual O
knowledge O
bases O
. O

section 17
id pdf2json/2021.acl-long.146.pdf.json
the O
findings O
of O
this O
paper O
can O
benefit O
the O
community O
in O
many O
directions O
. O

section 17
id pdf2json/2021.acl-long.146.pdf.json
by O
explaining O
the O
underlying O
predicting O
mechanisms O
of O
mlms O
, O
we O
provide O
reliable O
explanations O
for O
many O
previous O
knowledgeintensive O
techniques O
. O

section 17
id pdf2json/2021.acl-long.146.pdf.json
for O
example O
, O
our O
method O
can O
explain O
why O
and O
how O
incorporating O
external O
contexts O
will O
help O
knowledge O
extraction O
and O
commonsenseqa O
( O
talmor O
et O
al. O
, O
2019 O
) O
. O

section 17
id pdf2json/2021.acl-long.146.pdf.json
our O
findings O
also O
reveal O
why O
plm O
probing O
datasets O
may O
not O
be O
reliable O
and O
how O
the O
evaluation O
can O
be O
promoted O
by O
designing O
de-biased O
evaluation O
datasets O
. O

section 17
id pdf2json/2021.acl-long.146.pdf.json
this O
paper O
also O
sheds O
light O
on O
future O
research O
directions O
. O

section 17
id pdf2json/2021.acl-long.146.pdf.json
for O
instance O
, O
knowing O
the O
main O
benefit O
of O
illustrative O
cases O
comes O
from O
type-guidance O
, O
we O
can O
enhance O
many O
type-centric O
prediction O
tasks O
such O
as O
ner O
( O
lample O
et O
al. O
, O
2016 O
) O
and O
factoid O
qa O
( O
iyyer O
et O
al. O
, O
2014 O
) O
. O

section 17
id pdf2json/2021.acl-long.146.pdf.json
moreover O
, O
based O
on O
the O
mechanism O
of O
incorporating O
external O
contexts O
, O
we O
can O
better O
evaluate O
, O
seek O
, O
and O
denoise O
external O
contexts O
for O
different O
tasks O
using O
mlms O
. O

section 17
id pdf2json/2021.acl-long.146.pdf.json
for O
example O
, O
we O
can O
assess O
and O
select O
appropriate O
facts O
for O
commonsenseqa O
based O
on O
whether O
they O
can O
reconstruct O
the O
candidate O
answers O
. O

section 17
id pdf2json/2021.acl-long.146.pdf.json
this O
paper O
focuses O
on O
masked O
language O
models O
, O
which O
have O
been O
shown O
very O
effective O
and O
are O
widely O
used O
. O

section 17
id pdf2json/2021.acl-long.146.pdf.json
we O
also O
want O
to O
investigate O
another O
representative O
category O
of O
language O
models O
– O
the O
generative O
pre-trained O
models O
( O
e.g. O
, O
gpt2/3 O
( O
radford O
et O
al. O
, O
2019 O
; O
brown O
et O
al. O
, O
2020 O
) O
) O
, O
which O
have O
been O
shown O
to O
have O
quite O
different O
mechanisms O
and O
we O
leave O
it O
for O
future O
work O
due O
to O
page O
limitation O
. O

section 18
id pdf2json/2021.acl-long.146.pdf.json
we O
sincerely O
thank O
all O
anonymous O
reviewers O
for O
their O
insightful O
comments O
and O
valuable O
suggestions O
. O

section 18
id pdf2json/2021.acl-long.146.pdf.json
this O
work O
is O
supported O
by O
the O
national O
key O
research O
and O
development O
program O
of O
china O
( O
no O
. O

section 18
id pdf2json/2021.acl-long.146.pdf.json
2020aaa0106400 O
) O
, O
the O
national O
natural O
science O
foundation O
of O
china O
under O
grants O
no O
. O

section 18
id pdf2json/2021.acl-long.146.pdf.json
u1936207 O
, O
and O
in O
part O
by O
the O
youth O
innovation O
promotion O
association O
cas O
( O
2018141 O
) O
. O

section 19
id pdf2json/2021.acl-long.146.pdf.json
to O
construct O
wiki-uni O
, O
we O
first O
collect O
all O
the O
triples O
which O
belong O
to O
the O
same O
41 O
relations O
with O
lama O
from O
wikidata O
( O
vrandečić O
and O
krötzsch O
, O
2014 O
) O
, O
then O
we O
randomly O
sample O
50k O
triples O
with O
a O
single-token O
object O
for O
each O
relation O
. O

section 19
id pdf2json/2021.acl-long.146.pdf.json
similar O
to O
lama O
, O
we O
filter O
out O
the O
instances O
whose O
object O
is O
not O
in O
mlms O
’ O
vocabulary O
. O

section 19
id pdf2json/2021.acl-long.146.pdf.json
for O
each O
relation O
, O
we O
group O
the O
instances O
based O
on O
different O
objects O
, O
and O
indicate O
fo O
as O
the O
frequency O
of O
each O
object O
. O

section 19
id pdf2json/2021.acl-long.146.pdf.json
we O
denote O
the O
median O
of O
fo O
with O
fm O
. O

section 19
id pdf2json/2021.acl-long.146.pdf.json
for O
groups O
where O
fo O
> O
fm O
, O
we O
randomly O
sample O
fm O
instances O
, O
and O
delete O
the O
groups O
where O
fo O
< O
fm O
. O

section 19
id pdf2json/2021.acl-long.146.pdf.json
therefore O
, O
we O
acquire O
a O
dataset O
named O
wiki-uni O
with O
a O
uniform O
answer O
distribution O
. O

section 19
id pdf2json/2021.acl-long.146.pdf.json
there O
are O
70k O
facts O
in O
wiki-uni O
and O
34k O
facts O
in O
lama O
. O

section 19
id pdf2json/2021.acl-long.146.pdf.json
since O
bert O
and O
roberta O
have O
a O
different O
vocabulary O
, O
so O
the O
datasets O
for O
their O
evaluation O
are O
slightly O
different O
. O

section 20
id pdf2json/2021.acl-long.146.pdf.json
our O
conclusions O
are O
similar O
on O
bert-large O
and O
roberta-large O
, O
therefore O
, O
we O
report O
the O
results O
of O
bert-large O
in O
the O
article O
and O
results O
of O
robertalarge O
here O
. O

section 20
id pdf2json/2021.acl-long.146.pdf.json
b.1 O
promp-based O
retrieval O
figure O
7 O
shows O
the O
very O
significant O
correlation O
between O
the O
prediction O
distributions O
on O
lama O
and O
wiki-uni O
for O
roberta-large O
: O
on O
all O
three O
kinds O
of O
prompts O
, O
the O
pearson O
correlation O
coefficient O
between O
these O
two O
prediction O
distributions O
exceeds O
0.9 O
in O
most O
relations O
. O

section 20
id pdf2json/2021.acl-long.146.pdf.json
table O
8 O
shows O
the O
percentage O
of O
instances O
that O
the O
topk O
object O
entities O
cover O
for O
roberta-large O
. O

section 20
id pdf2json/2021.acl-long.146.pdf.json
b.2 O
case-based O
analogy O
table O
9 O
shows O
the O
performance O
improvement O
after O
introducing O
illustrative O
cases O
for O
roberta-large O
model O
, O
we O
can O
see O
that O
the O
illustrative O
cases O
could O
also O
significantly O
increase O
the O
knowledge O
extraction O
performance O
for O
roberta-large O
. O

section 20
id pdf2json/2021.acl-long.146.pdf.json
table O
14 O
shows O
how O
the O
entity O
types O
of O
predictions O
changed O
after O
introducing O
the O
illustrative O
cases O
for O
roberta-large O
model O
, O
the O
conclusion O
is O
similar O
with O
bert-large O
. O

section 20
id pdf2json/2021.acl-long.146.pdf.json
figure O
8 O
shows O
the O
percentage O
on O
the O
change O
of O
overall O
rank O
and O
in-type O
rank O
for O
roberta-large O
model O
. O

section 20
id pdf2json/2021.acl-long.146.pdf.json
and O
another O
finding O
is O
that O
bert-large O
has O
a O
better O
type O
prediction O
ability O
than O
roberta-large O
, O
even O
without O
illustrative O
cases O
. O

section 20
id pdf2json/2021.acl-long.146.pdf.json
we O
calculate O
the O
overall O
type O
precision O
over O
prompt-based O
paradigm O
( O
the O
percentage O
of O
predictions O
that O
the O
type O
is O
correct O
) O
. O

section 20
id pdf2json/2021.acl-long.146.pdf.json
and O
the O
type O
precision O
for O
bert-large O
is O
68 O
% O
and O
for O
roberta-large O
is O
only O
51 O
% O
, O
which O
partly O
explains O
why O
performance O
of O
roberta-large O
is O
significantly O
worse O
than O
bert-large O
on O
lama O
dataset O
. O

section 20
id pdf2json/2021.acl-long.146.pdf.json
b.3 O
context-based O
inference O
table O
10 O
shows O
the O
comparison O
of O
contexts O
group O
by O
whether O
the O
contexts O
contain O
the O
answer O
for O
roberta-large O
. O

section 20
id pdf2json/2021.acl-long.146.pdf.json
we O
can O
see O
that O
for O
contexts O
containing O
the O
answer O
, O
context-based O
inference O
significantly O
improves O
the O
factual O
extraction O
performance O
. O

section 20
id pdf2json/2021.acl-long.146.pdf.json
meanwhile O
, O
there O
is O
a O
performance O
drop O
for O
those O
instances O
whose O
context O
does O
not O
contain O
the O
answer O
. O

section 20
id pdf2json/2021.acl-long.146.pdf.json
table O
11 O
shows O
the O
overall O
performance O
improvements O
when O
introducing O
different O
external O
contexts O
for O
roberta-large O
. O

section 20
id pdf2json/2021.acl-long.146.pdf.json
table O
12 O
shows O
the O
comparison O
of O
the O
masked O
contexts O
based O
on O
whether O
they O
can/ O
can O
not O
reconstruct O
the O
masked O
answer O
for O
roberta-large O
. O

section 20
id pdf2json/2021.acl-long.146.pdf.json
the O
improvements O
mainly O
comes O
from O
the O
instances O
whose O
answer O
in O
contexts O
can O
be O
reconstructed O
. O

section 20
id pdf2json/2021.acl-long.146.pdf.json
without O
contexts O
full O
contexts O
masked O
contexts O

section 22
id pdf2json/2021.acl-long.146.pdf.json
table O
13 O
shows O
the O
detailed O
analysis O
of O
all O
relations O
using O
case-based O
analogy O
paradigm O
for O
bertlarge O
and O
table O
14 O
is O
the O
results O
on O
robertalarge O
. O

section 22
id pdf2json/2021.acl-long.146.pdf.json
because O
of O
the O
page O
limit O
, O
another O
finding O
we O
didn O
’ O
t O
mention O
in O
the O
article O
is O
that O
, O
apart O
from O
“ O
type O
guidance O
” O
, O
the O
illustrative O
cases O
could O
also O
provide O
a O
“ O
surface O
form O
guidance O
” O
in O
a O
few O
relations O
( O
e.g. O
, O
part O
of O
, O
applies O
to O
jurisdiction O
, O
subclass O
of O
) O
. O

section 22
id pdf2json/2021.acl-long.146.pdf.json
specifically O
, O
the O
“ O
surface O
form O
” O
indicate O
that O
the O
object O
entity O
name O
( O
e.g. O
, O
apple O
) O
is O
a O
substring O
of O
the O
subject O
entity O
name O
( O
e.g. O
, O
apple O
watch O
) O
. O

section 22
id pdf2json/2021.acl-long.146.pdf.json
such O
phenomenon O
is O
also O
mentioned O
in O
poerner O
et O
al O
. O

section 22
id pdf2json/2021.acl-long.146.pdf.json
( O
2020 O
) O
. O

section TITLE
id pdf2json/2021.acl-long.69.pdf.json
learning O
faithful O
representations O
of O
causal O
graphs O

section ABSTRACT
id pdf2json/2021.acl-long.69.pdf.json
learning O
contextual O
text O
embeddings O
that O
represent O
causal O
graphs O
has O
been O
useful O
in O
improving O
the O
performance O
of O
downstream O
tasks O
like O
causal O
treatment O
effect O
estimation O
. O

section ABSTRACT
id pdf2json/2021.acl-long.69.pdf.json
however O
, O
existing O
causal O
embeddings O
which O
are O
trained O
to O
predict O
direct O
causal O
links O
, O
fail O
to O
capture O
other O
indirect O
causal O
links O
of O
the O
graph O
, O
thus O
leading O
to O
spurious O
correlations O
in O
downstream O
tasks O
. O

section ABSTRACT
id pdf2json/2021.acl-long.69.pdf.json
in O
this O
paper O
, O
we O
define O
the O
faithfulness O
property O
of O
contextual O
embeddings O
to O
capture O
geometric O
distance-based O
properties O
of O
directed O
acyclic O
causal O
graphs O
. O

section ABSTRACT
id pdf2json/2021.acl-long.69.pdf.json
by O
incorporating O
these O
faithfulness O
properties O
, O
we O
learn O
text O
embeddings O
that O
are O
31.3 O
% O
more O
faithful O
to O
human O
validated O
causal O
graphs O
with O
about O
800k O
and O
200k O
causal O
links O
and O
achieve O
21.1 O
% O
better O
precision-recall O
auc O
in O
a O
link O
prediction O
fine-tuning O
task O
. O

section ABSTRACT
id pdf2json/2021.acl-long.69.pdf.json
further O
, O
in O
a O
crowdsourced O
causal O
question-answering O
task O
on O
yahoo O
! O

section ABSTRACT
id pdf2json/2021.acl-long.69.pdf.json
answers O
with O
questions O
of O
the O
form O
“ O
what O
causes O
x O
? O
” O
, O
our O
faithful O
embeddings O
achieved O
a O
precision O
of O
the O
first O
ranked O
answer O
( O
p O
@ O
1 O
) O
of O
41.07 O
% O
, O
outperforming O
the O
existing O
baseline O
by O
10.2 O
% O
. O

section 0
id pdf2json/2021.acl-long.69.pdf.json
proceedings O
of O
the O
59th O
annual O
meeting O
of O
the O
association O
for O
computational O
linguistics O
and O
the O
11th O
international O
joint O
conference O
on O
natural O
language O
processing O
, O
pages O
839–850 O
august O
1–6 O
, O
2021 O
. O

section 0
id pdf2json/2021.acl-long.69.pdf.json
©2021 O
association O
for O
computational O
linguistics O
839 O

section 1
id pdf2json/2021.acl-long.69.pdf.json
learning O
distributed O
word O
representations O
that O
capture O
causal O
relationships O
are O
useful O
for O
real-world O
natural O
language O
processing O
tasks O
( O
roberts O
et O
al. O
, O
2020 O
; O
veitch O
et O
al. O
, O
2020 O
; O
gao O
et O
al. O
, O
2018 O
, O
2019 O
) O
. O

section 1
id pdf2json/2021.acl-long.69.pdf.json
approximating O
the O
notion O
of O
causality O
with O
a O
similarity-based O
distance O
metric O
using O
separate O
vector O
representations O
for O
cause O
and O
effect O
tokens O
has O
led O
to O
significant O
improvement O
in O
the O
performance O
of O
downstream O
tasks O
like O
question O
answering O
, O
but O
can O
be O
too O
restrictive O
to O
generalize O
over O
unobserved O
edges O
in O
larger O
causal O
graphs O
( O
sharp O
et O
al. O
, O
2016 O
) O
. O

section 1
id pdf2json/2021.acl-long.69.pdf.json
in O
downstream O
causal O
reasoning O
based O
tasks O
like O
dialog O
systems O
( O
ning O
et O
al. O
, O
2018 O
) O
, O
explanation O
generation O
( O
grimsley O
et O
al. O
, O
2020 O
) O
, O
question O
answering O
( O
sharp O
et O
al. O
, O
2016 O
) O
, O
it O
is O
important O
to O
align O
the O
models O
with O
the O
corresponding O
causal O
graph O
. O

section 1
id pdf2json/2021.acl-long.69.pdf.json
however O
, O
words O
that O
have O
low O
cosine O
similarity O
capture O
various O
semantic O
similarities O
, O
like O
relatedness O
, O
synonyms O
, O
replaceability O
, O
or O
complementarity O
, O
but O
not O
directionality O
( O
hamilton O
et O
al. O
, O
2017 O
) O
. O

section 1
id pdf2json/2021.acl-long.69.pdf.json
hence O
, O
any O
symmetric O
distance O
in O
an O
embedding O
space O
can O
not O
convey O
the O
directed O
causal O
semantics O
for O
a O
downstream O
task O
( O
mémoli O
et O
al. O
, O
2016 O
) O
. O

section 1
id pdf2json/2021.acl-long.69.pdf.json
in O
this O
paper O
, O
we O
overcome O
these O
two O
shortcomings O
and O
propose O
to O
optimize O
for O
directed O
faithfulness O
( O
spirtes O
et O
al. O
, O
1993 O
) O
that O
word O
embeddings O
have O
to O
satisfy O
towards O
a O
causal O
graph O
. O

section 1
id pdf2json/2021.acl-long.69.pdf.json
prior O
work O
on O
capturing O
sufficient O
information O
for O
causal O
inference O
tasks O
from O
embeddings O
aims O
to O
directly O
use O
them O
for O
average O
treatment O
effect O
estimation O
( O
veitch O
et O
al. O
, O
2020 O
) O
. O

section 1
id pdf2json/2021.acl-long.69.pdf.json
we O
are O
, O
however O
, O
interested O
in O
a O
complementary O
question O
: O
“ O
can O
we O
learn O
word O
embeddings O
based O
on O
a O
distance O
measure O
that O
maps O
the O
directed O
distance O
between O
nodes O
in O
a O
causal O
graph O
to O
that O
in O
the O
embedding O
space O
? O
” O
. O

section 1
id pdf2json/2021.acl-long.69.pdf.json
unlike O
prior O
work O
, O
which O
aims O
to O
learn O
a O
causal O
aware O
embedding O
restricted O
to O
direct O
link O
prediction O
( O
hamilton O
et O
al. O
, O
2017 O
) O
, O
we O
propose O
faithfulness O
constraints O
so O
that O
causal O
word O
embeddings O
aims O
to O
preserve O
the O
partial O
ordering O
over O
pairwise O
distances O
in O
the O
directed O
causal O
graph O
. O

section 1
id pdf2json/2021.acl-long.69.pdf.json
in O
this O
paper O
, O
to O
achieve O
the O
goal O
of O
learning O
faithful O
word O
embeddings O
with O
a O
vocabulary O
of O
more O
than O
100k O
tokens O
, O
we O
minimize O
faithfulness O
violations O
over O
pairwise O
samples O
of O
nodes O
in O
the O
causal O
graph O
. O

section 1
id pdf2json/2021.acl-long.69.pdf.json
through O
this O
constrained O
optimization O
, O
we O
learn O
an O
embedding O
that O
can O
be O
applied O
directly O
for O
causal O
inference O
tasks O
but O
also O
generalizes O
to O
emergent O
causal O
links O
. O

section 1
id pdf2json/2021.acl-long.69.pdf.json
it O
has O
been O
shown O
that O
nlp O
models O
need O
to O
understand O
such O
causal O
links O
that O
persist O
in O
the O
real O
world O
for O
safe O
deployment O
( O
gao O
et O
al. O
, O
2018 O
; O
mishra O
et O
al. O
, O
2019 O
) O
. O

section 1
id pdf2json/2021.acl-long.69.pdf.json
embeddings O
that O
violate O
the O
faithfulness O
property O
, O
can O
lead O
to O
spurious O
correlations O
based O
on O
co-location O
in O
the O
embedding O
space O
. O

section 1
id pdf2json/2021.acl-long.69.pdf.json
for O
example O
, O
in O
a O
yahoo O
! O

section 1
id pdf2json/2021.acl-long.69.pdf.json
causal O
question-answering O
task O
’ O
s O
example O
: O
“ O
what O
causes O
nosebleed O
? O
” O
: O
the O
answers O
were O
“ O
dry O
air O
” O
, O
“ O
heavy O
dust O
” O
, O
“ O
damaged O
nasal O
cells O
” O
and O
“ O
liver O
problems O
” O
. O

section 1
id pdf2json/2021.acl-long.69.pdf.json
if O
we O
were O
to O
only O
rely O
on O
an O
undirected O
association O
based O
embeddings O
, O
the O
causes O
“ O
dry O
air O
” O
and O
“ O
liver O
problems O
” O
might O
be O
nearby O
( O
with O
distance O
of O
2 O
) O
, O
but O
would O
be O
appropriately O
placed O
far O
in O
a O
directed O
causality O
based O
embedding O
space O
. O

section 1
id pdf2json/2021.acl-long.69.pdf.json
to O
capture O
such O
asymmetric O
properties O
, O
we O
aim O
to O
preserve O
alignment O
with O
the O
causal O
graph O
by O
mapping O
causal O
links O
to O
an O
asymmetric O
quasi-pseudo O
distance O
measure O
during O
training O
to O
capture O
directionality O
of O
the O
causal O
graph O
as O
per O
figure O
1 O
. O

section 1
id pdf2json/2021.acl-long.69.pdf.json
since O
human O
validated O
causal O
graphs O
can O
be O
used O
directly O
to O
answer O
questions O
of O
the O
type O
“ O
what O
causes O
x O
? O
” O
, O
we O
demonstrate O
the O
utility O
of O
learning O
faithful O
representations O
by O
using O
our O
distance-based O
features O
to O
solve O
the O
yahoo O
! O

section 1
id pdf2json/2021.acl-long.69.pdf.json
causal O
question-answering O
( O
qa O
) O
task O
. O

section 1
id pdf2json/2021.acl-long.69.pdf.json
a O
causal O
qa O
task O
, O
unlike O
a O
standard O
qa O
task O
, O
can O
directly O
benefit O
from O
incorporating O
a O
causal O
graph O
into O
word O
embeddings O
to O
answer O
anti-causal O
queries O
. O

section 1
id pdf2json/2021.acl-long.69.pdf.json
our O
key O
contributions O
are O
: O
• O
we O
define O
a O
faithfulness O
property O
for O
word O
embeddings O
over O
a O
causal O
graph O
, O
that O
captures O
geometric O
properties O
of O
the O
causal O
graph O
, O
beyond O
the O
direct O
link O
prediction O
by O
ensuring O
global O
proximity O
preservation O
. O

section 1
id pdf2json/2021.acl-long.69.pdf.json
• O
we O
propose O
a O
methodology O
to O
learn O
faithful O
embeddings O
through O
violation O
minimization O
which O
improves O
neighborhood O
detection O
by O
31.3 O
% O
, O
uniformity O
by O
42.6 O
% O
, O
and O
distance O
correlation O
by O
54.2 O
% O
using O
a O
quasi-pseudo O
distance O
metric O
. O

section 1
id pdf2json/2021.acl-long.69.pdf.json
• O
the O
faithful O
bert O
and O
roberta-based O
embeddings O
we O
learn O
, O
when O
used O
as O
inputs O
to O
a O
causal O
qa O
task O
, O
increases O
the O
precision O
of O
the O
first O
ranked O
answer O
( O
p O
@ O
1 O
) O
over O
existing O
baselines O
by O
10.2 O
% O
. O

section 3
id pdf2json/2021.acl-long.69.pdf.json
causal O
inference O
, O
as O
outlined O
in O
( O
pearl O
, O
2009 O
) O
formalizes O
cause O
and O
effects O
discovered O
through O
intervention O
based O
experiments O
and O
communicates O
them O
via O
directed O
acyclic O
graphs O
. O

section 3
id pdf2json/2021.acl-long.69.pdf.json
with O
the O
availability O
of O
large O
observational O
datasets O
for O
machine O
learning O
, O
various O
methods O
and O
assumptions O
have O
been O
proposed O
for O
learning O
causal O
graphs O
( O
schölkopf O
, O
2019 O
) O
, O
data O
fusion O
and O
transportability O
properties O
( O
bareinboim O
and O
pearl O
, O
2016 O
; O
bonner O
and O
vasile O
, O
2017 O
) O
. O

section 3
id pdf2json/2021.acl-long.69.pdf.json
specifically O
, O
our O
work O
closely O
aligns O
with O
the O
assumption O
of O
faithfulness O
( O
spirtes O
et O
al. O
, O
1993 O
) O
, O
which O
requires O
that O
the O
observed O
probability O
distributions O
of O
nodes O
in O
a O
causal O
graph O
are O
conditionally O
independent O
as O
per O
the O
links O
in O
the O
graph O
. O

section 3
id pdf2json/2021.acl-long.69.pdf.json
in O
our O
work O
, O
we O
use O
the O
probability O
distributions O
as O
modeled O
in O
a O
natural O
language O
model O
( O
kuhn O
and O
de O
mori O
, O
1990 O
) O
and O
align O
it O
with O
the O
causal O
links O
in O
a O
graphical O
causal O
model O
. O

section 3
id pdf2json/2021.acl-long.69.pdf.json
we O
extend O
the O
faithfulness O
assumption O
to O
be O
reflected O
in O
embeddings O
learnt O
by O
a O
masked O
language O
model O
( O
devlin O
et O
al. O
, O
2019 O
; O
liu O
et O
al. O
, O
2019b O
) O
for O
downstream O
tasks O
. O

section 3
id pdf2json/2021.acl-long.69.pdf.json
this O
definition O
of O
faithfulness O
is O
different O
from O
the O
one O
proposed O
by O
( O
jacovi O
and O
goldberg O
, O
2020 O
) O
used O
to O
evaluate O
models O
for O
interpretability O
of O
models O
used O
for O
downstream O
tasks O
. O

section 3
id pdf2json/2021.acl-long.69.pdf.json
instead O
, O
our O
work O
builds O
on O
embeddings O
learnt O
in O
( O
sharp O
et O
al. O
, O
2016 O
) O
, O
given O
a O
causal O
model O
and O
learn O
embeddings O
that O
are O
bootstrapped O
using O
a O
small O
set O
of O
cause-effect O
seeds O
. O

section 3
id pdf2json/2021.acl-long.69.pdf.json
causal O
models O
have O
also O
been O
used O
to O
learn O
auxiliary O
tasks O
( O
feder O
et O
al. O
, O
2020 O
) O
using O
adversarial O
training O
to O
ensure O
that O
a O
language O
model O
learns O
causal-inspired O
representations O
. O

section 3
id pdf2json/2021.acl-long.69.pdf.json
such O
approaches O
use O
causal O
models O
to O
learn O
counterfactual O
embeddings O
invariant O
to O
the O
presence O
of O
confounding O
concepts O
in O
a O
sentence O
, O
while O
we O
encode O
the O
geometrical O
properties O
of O
causal O
graphs O
into O
the O
embeddings O
and O
the O
distance O
measure O
to O
maintain O
their O
faithfulness O
. O

section 3
id pdf2json/2021.acl-long.69.pdf.json
in O
principle O
, O
we O
adopt O
a O
similar O
approach O
to O
( O
veitch O
et O
al. O
, O
2020 O
) O
of O
fine-tuning O
towards O
a O
causal O
link O
prediction O
task O
. O

section 3
id pdf2json/2021.acl-long.69.pdf.json
this O
is O
in O
contrast O
with O
approaches O
that O
use O
energy-based O
transition O
vectors O
used O
to O
represent O
the O
cause-to-effect O
and O
effectto-cause O
links O
( O
zhao O
et O
al. O
, O
2017 O
) O
. O

section 3
id pdf2json/2021.acl-long.69.pdf.json
our O
approach O
uses O
regularization O
constraints O
similar O
to O
the O
ones O
proposed O
for O
information O
bottlenecks O
in O
word O
embeddings O
( O
li O
and O
eisner O
, O
2019 O
; O
goyal O
and O
durrett O
, O
2019 O
) O
, O
text-based O
games O
( O
narasimhan O
et O
al. O
, O
2015 O
) O
, O
activation O
links O
in O
neuroscience O
( O
chalupka O
et O
al. O
, O
2016 O
) O
, O
causal O
consistency O
with O
ordinary O
differential O
equations O
( O
rubenstein O
et O
al. O
, O
2017 O
) O
and O
temporal O
granger O
causality O
( O
tank O
et O
al. O
, O
2018 O
) O
. O

section 3
id pdf2json/2021.acl-long.69.pdf.json
for O
an O
extensive O
survey O
of O
using O
text O
for O
causal O
inference O
tasks O
, O
we O
refer O
to O
( O
keith O
et O
al. O
, O
2020 O
) O
. O

section 4
id pdf2json/2021.acl-long.69.pdf.json
learning O
asymmetric O
transitive O
graph O
representations O
which O
generalize O
the O
causal O
graph O
have O
been O
studied O
extensively O
in O
information O
retrieval O
( O
chen O
et O
al. O
, O
2007 O
; O
epasto O
and O
perozzi O
, O
2019 O
; O
li O
et O
al. O
, O
2019 O
; O
grover O
and O
leskovec O
, O
2016 O
) O
. O

section 4
id pdf2json/2021.acl-long.69.pdf.json
they O
either O
utilize O
a O
random O
walk O
learning O
technique O
( O
perozzi O
et O
al. O
, O
2014 O
) O
or O
matrix O
factorization O
techniques O
( O
lee O
and O
seung O
, O
2000 O
; O
tenenbaum O
et O
al. O
, O
2000 O
; O
wang O
et O
al. O
, O
2017 O
; O
mikolov O
et O
al. O
, O
2013 O
) O
to O
incorporate O
priors O
such O
as O
the O
stationary O
transition O
probability O
matrix O
, O
community O
structure O
, O
etc O
. O

section 4
id pdf2json/2021.acl-long.69.pdf.json
more O
recently O
, O
( O
liu O
et O
al. O
, O
2019a O
; O
ostendorff O
et O
al. O
, O
2019 O
; O
lu O
et O
al. O
, O
2020 O
) O
have O
incorporated O
knowledge O
graphs O
in O
bert O
and O
shown O
increased O
accuracy O
in O
knowledgecentric O
nlp O
tasks O
. O

section 4
id pdf2json/2021.acl-long.69.pdf.json
( O
zhou O
et O
al. O
, O
2017 O
; O
gordo O
and O
perronnin O
, O
2011 O
; O
ou O
et O
al. O
, O
2016 O
; O
sun O
et O
al. O
, O
2018 O
; O
tang O
et O
al. O
, O
2015 O
) O
propose O
asymmetric O
higher O
order O
proximity O
preserving O
graph O
embedding O
methods O
by O
learning O
separate O
source O
and O
target O
embeddings O
. O

section 4
id pdf2json/2021.acl-long.69.pdf.json
while O
we O
can O
learn O
faithful O
3-dimension O
embeddings O
for O
any O
fixed O
finite O
undirected O
graph O
deterministically O
( O
cohen O
et O
al. O
, O
1995 O
) O
, O
fine-tuning O
pretrained O
word O
embeddings O
such O
that O
they O
generalize O
over O
all O
sub-graphs O
in O
a O
directed O
graph O
is O
known O
to O
be O
a O
hard O
graph O
kernel O
design O
problem O
that O
scales O
cubically O
with O
the O
number O
of O
nodes O
( O
vishwanathan O
et O
al. O
, O
2010 O
) O
. O

section 4
id pdf2json/2021.acl-long.69.pdf.json
our O
approach O
builds O
on O
efforts O
to O
incorporate O
graph-like O
structure O
in O
bert O
, O
but O
overcomes O
the O
issue O
of O
learning O
dual O
embeddings O
for O
cause-effect O
edges O
by O
learning O
unified O
embeddings O
for O
both O
cause O
and O
effect O
roles O
of O
words O
. O

section 4
id pdf2json/2021.acl-long.69.pdf.json
through O
such O
embeddings O
, O
we O
can O
further O
aid O
causal O
discovery O
that O
is O
not O
yet O
captured O
in O
a O
graphical O
notation O
( O
chen O
et O
al. O
, O
2014 O
) O
. O

section 5
id pdf2json/2021.acl-long.69.pdf.json
recently O
, O
graph O
neural O
networks O
that O
capture O
the O
graph O
neighborhood O
structure O
have O
been O
employed O
in O
link O
prediction O
( O
zhu O
et O
al. O
, O
2020 O
; O
abu-el-haija O
et O
al. O
, O
2017 O
) O
. O

section 5
id pdf2json/2021.acl-long.69.pdf.json
in O
( O
you O
et O
al. O
, O
2018 O
) O
, O
the O
problem O
is O
reduced O
to O
that O
of O
sequence O
prediction O
by O
reducing O
the O
graph O
to O
breadth-first O
search O
based O
deterministic O
sequence O
. O

section 5
id pdf2json/2021.acl-long.69.pdf.json
in O
( O
li O
et O
al. O
, O
2018 O
) O
, O
node O
embeddings O
are O
updated O
after O
several O
rounds O
of O
message O
passing O
, O
while O
in O
( O
tu O
et O
al. O
, O
2016 O
) O
a O
variant O
of O
the O
random O
walk O
is O
incorporated O
with O
a O
max-margin O
discriminative O
constraint O
. O

section 5
id pdf2json/2021.acl-long.69.pdf.json
in O
( O
velikovi O
et O
al. O
, O
2018 O
) O
, O
models O
are O
learned O
by O
attending O
over O
the O
neighborhood O
of O
nodes O
for O
context O
, O
while O
( O
kipf O
and O
welling O
, O
2016 O
) O
apply O
spectral O
graph O
convolutions O
for O
a O
selfsupervised O
learning O
task O
. O

section 5
id pdf2json/2021.acl-long.69.pdf.json
we O
adopt O
the O
incremental O
approach O
proposed O
in O
( O
velikovi O
et O
al. O
, O
2018 O
) O
which O
does O
not O
rely O
on O
knowing O
the O
entire O
graph O
structure O
apriori O
and O
fine-tune O
on O
cause-effect O
pairs O
for O
the O
link O
prediction O
task O
on O
a O
pre-trained O
bert-based O
language O
model O
. O

section 7
id pdf2json/2021.acl-long.69.pdf.json
causal O
inference O
( O
pearl O
, O
2009 O
) O
aims O
to O
understand O
the O
cause O
and O
effect O
relationships O
between O
events O
. O

section 7
id pdf2json/2021.acl-long.69.pdf.json
learning O
purely O
based O
on O
correlations O
in O
observational O
data O
can O
lead O
to O
spurious O
causal O
links O
and O
can O
severely O
impact O
downstream O
tasks O
. O

section 7
id pdf2json/2021.acl-long.69.pdf.json
hence O
, O
intervention-based O
studies O
are O
conducted O
which O
carefully O
study O
the O
impact O
of O
a O
cause O
using O
controlled O
randomized O
experiments O
and O
other O
criterion O
to O
learn O
if O
links O
between O
causes O
and O
effects O
exist O
using O
observed O
data O
under O
specific O
assumptions O
. O

section 7
id pdf2json/2021.acl-long.69.pdf.json
the O
findings O
of O
such O
studies O
are O
formalized O
using O
frameworks O
like O
rubin O
causal O
models O
( O
rubin O
, O
1974 O
) O
, O
structural O
causal O
models O
( O
pearl O
, O
2009 O
) O
, O
etc O
. O

section 7
id pdf2json/2021.acl-long.69.pdf.json
while O
there O
are O
differences O
in O
abstractions O
between O
them O
, O
there O
is O
formal O
equivalence O
( O
galles O
and O
pearl O
, O
1998 O
) O
in O
modeling O
counterfactuals O
( O
“ O
what O
is O
the O
effect O
when O
the O
cause O
is O
intervened O
? O
” O
) O
and O
we O
refer O
the O
reader O
to O
( O
pearl O
and O
mackenzie O
, O
2018 O
) O
for O
a O
primer O
in O
causal O
modeling O
. O

section 7
id pdf2json/2021.acl-long.69.pdf.json
in O
this O
paper O
, O
we O
assume O
a O
graphical O
structural O
causal O
modelc O
( O
pearl O
, O
2009 O
) O
is O
given O
, O
whose O
nodes O
are O
linked O
with O
directed O
edges O
that O
denote O
the O
causeeffect O
relationship O
. O

section 7
id pdf2json/2021.acl-long.69.pdf.json
for O
example O
, O
the O
cause-effect O
of O
“ O
smoking O
” O
causes O
“ O
cancer O
” O
, O
references O
to O
the O
real O
world O
action O
of O
“ O
smoking O
” O
in O
individuals O
that O
leads O
to O
the O
development O
of O
“ O
cancer O
” O
kind O
of O
disease O
in O
those O
individuals O
. O

section 7
id pdf2json/2021.acl-long.69.pdf.json
while O
causal O
models O
have O
a O
close O
relationship O
to O
the O
knowledge O
graph O
, O
the O
links O
of O
the O
causal O
graph O
have O
a O
well-defined O
causal O
interpretation O
that O
can O
be O
validated O
through O
counterfactual O
experiments O
. O

section 7
id pdf2json/2021.acl-long.69.pdf.json
in O
this O
work O
, O
we O
assume O
the O
availability O
of O
such O
a O
causal O
graph O
and O
we O
do O
not O
aim O
to O
build O
one O
. O

section 7
id pdf2json/2021.acl-long.69.pdf.json
instead O
, O
we O
rely O
on O
hu- O
man O
annotators O
who O
with O
the O
help O
of O
web O
crawlers O
( O
heindorf O
et O
al. O
, O
2020a O
) O
and O
other O
information O
retrieval O
tools O
( O
sharp O
et O
al. O
, O
2016 O
) O
produce O
a O
directed O
graphical O
causal O
model O
as O
shown O
in O
figure O
1 O
. O

section 8
id pdf2json/2021.acl-long.69.pdf.json
given O
a O
graphical O
causal O
model O
c O
, O
we O
now O
present O
a O
faithfulness O
property O
an O
embedding O
that O
aims O
to O
closely O
align O
with O
the O
causal O
model O
has O
to O
satisfy O
. O

section 8
id pdf2json/2021.acl-long.69.pdf.json
the O
faithfulness O
property O
was O
first O
proposed O
for O
any O
two O
causal O
spaces O
in O
( O
bombelli O
et O
al. O
, O
2013 O
) O
in O
the O
domain O
of O
quantum O
physics O
with O
the O
space-time O
dimension O
. O

section 8
id pdf2json/2021.acl-long.69.pdf.json
inspired O
by O
this O
, O
we O
propose O
an O
instantiation O
for O
word O
embeddings O
and O
a O
corresponding O
graphical O
causal O
model O
. O

section 8
id pdf2json/2021.acl-long.69.pdf.json
definition O
1 O
( O
faithfulness O
) O
. O

section 8
id pdf2json/2021.acl-long.69.pdf.json
an O
embedding O
f O
: O
c O
→ O
m O
from O
a O
causal O
set O
( O
c O
, O
dc O
) O
to O
a O
vector O
space O
( O
m O
, O
dm O
) O
is O
faithful O
if O
: O
• O
∃λ O
, O
∀x O
, O
y O
∈ O
c O
, O
dc O
( O
x O
, O
y O
) O
= O
1 O
⇔ O
dm O
( O
f O
( O
x O
) O
, O
f O
( O
y O
) O
) O
≤ O
λ O
• O
f O
( O
c O
) O
is O
distributed O
uniformly O
• O
∀x O
, O
y O
, O
w O
, O
z O
∈ O
c O
, O
dc O
( O
x O
, O
y O
) O
≤ O
dc O
( O
w O
, O
z O
) O
⇔ O
dm O
( O
f O
( O
x O
) O
, O
f O
( O
y O
) O
) O
≤ O
dm O
( O
f O
( O
w O
) O
, O
f O
( O
z O
) O
) O
note O
that O
we O
use O
the O
causal O
set O
( O
c O
, O
dc O
) O
as O
a O
tuple O
of O
the O
graphical O
causal O
model O
c O
and O
a O
distance O
measure O
dc O
which O
is O
used O
to O
measure O
the O
directed O
distance O
between O
nodes O
in O
the O
graph O
. O

section 8
id pdf2json/2021.acl-long.69.pdf.json
the O
vector O
space O
in O
which O
we O
map O
our O
embeddings O
is O
also O
characterized O
by O
a O
tuple O
( O
m O
, O
dm O
) O
, O
where O
m O
is O
the O
multidimensional O
real O
number O
space O
rm O
, O
and O
a O
distance O
measure O
dm O
which O
identifies O
nearby O
words O
in O
that O
vector O
space O
. O

section 8
id pdf2json/2021.acl-long.69.pdf.json
the O
three O
conditions O
posed O
by O
the O
faithfulness O
property O
, O
more O
concretely O
specify O
that O
there O
needs O
to O
be O
a O
real O
threshold O
, O
within O
the O
embedding O
space O
, O
which O
can O
cover O
all O
the O
neighboring O
nodes O
of O
a O
word O
, O
the O
embedding O
space O
needs O
to O
be O
uniformly O
distributed O
, O
and O
finally O
, O
any O
inequality O
relationships O
between O
two O
distance O
measures O
in O
the O
causal O
graph O
needs O
to O
hold O
in O
the O
embedding O
space O
too O
. O

section 8
id pdf2json/2021.acl-long.69.pdf.json
an O
embedding O
that O
satisfies O
this O
property O
can O
then O
be O
used O
to O
sufficiently O
represent O
the O
causal O
graph O
in O
downstream O
tasks O
. O

section 9
id pdf2json/2021.acl-long.69.pdf.json
the O
definition O
of O
faithfulness O
is O
dependent O
on O
the O
distance O
measure O
used O
in O
both O
the O
causal O
graph O
and O
the O
embedding O
domains O
. O

section 9
id pdf2json/2021.acl-long.69.pdf.json
in O
this O
work O
, O
we O
assume O
that O
the O
causal O
graph O
is O
a O
directed O
acyclic O
graph O
, O
and O
hence O
we O
measure O
dc O
as O
the O
shortest O
directed O
distance O
( O
number O
of O
edges O
in O
an O
unweighted O
graph O
) O
between O
two O
nodes O
. O

section 9
id pdf2json/2021.acl-long.69.pdf.json
if O
no O
such O
path O
exists O
between O
two O
nodes O
, O
we O
consider O
the O
distance O
to O
be O
a O
large O
number O
, O
which O
in O
the O
case O
of O
an O
unweighted O
graph O
, O
can O
be O
set O
to O
> O
n O
, O
where O
n O
is O
the O
number O
of O
nodes O
in O
the O
acyclic O
graph O
. O

section 9
id pdf2json/2021.acl-long.69.pdf.json
note O
that O
weighted O
graphs O
can O
also O
be O
incorporated O
with O
minor O
changes O
based O
on O
the O
maximum O
path O
in O
the O
graph O
. O

section 9
id pdf2json/2021.acl-long.69.pdf.json
however O
, O
the O
distance O
measure O
in O
the O
embedding O
space O
faces O
challenges O
in O
evaluation O
of O
simple O
supervised O
tasks O
( O
jastrzebski O
et O
al. O
, O
2017 O
) O
. O

section 9
id pdf2json/2021.acl-long.69.pdf.json
to O
overcome O
these O
, O
we O
chose O
a O
distance O
measure O
that O
is O
closely O
tied O
to O
our O
faithfulness O
definition O
. O

section 9
id pdf2json/2021.acl-long.69.pdf.json
we O
chose O
a O
unified O
set O
of O
embeddings O
for O
both O
the O
cause O
u O
and O
effect O
v O
, O
and O
, O
if O
there O
exists O
a O
causal O
edge O
from O
u O
→ O
v O
, O
then O
we O
would O
expect O
that O
dm O
( O
f O
( O
u O
) O
, O
f O
( O
v O
) O
) O
< O
< O
dm O
( O
f O
( O
v O
) O
, O
f O
( O
u O
) O
) O
. O

section 9
id pdf2json/2021.acl-long.69.pdf.json
for O
this O
reason O
, O
symmetric O
distance O
choices O
like O
euclidean O
distance O
, O
cosine O
similarity O
are O
not O
suitable O
. O

section 9
id pdf2json/2021.acl-long.69.pdf.json
our O
chosen O
distance O
measure O
, O
hence O
should O
follow O
the O
properties O
of O
quasi-pseudo O
metrics O
, O
defined O
as O
follows O
in O
( O
moshokoa O
, O
2005 O
) O
: O
definition O
2 O
( O
quasi-pseudo O
metric O
) O
. O

section 9
id pdf2json/2021.acl-long.69.pdf.json
a O
measure O
dm O
: O
x O
×x O
→ O
[ O
0 O
, O
∞ O
) O
is O
a O
quasi-pseudo O
metric O
if O
∀x O
, O
y O
, O
z O
∈ O
x O
, O
• O
dm O
( O
x O
, O
y O
) O
≥ O
0 O
• O
dm O
( O
x O
, O
x O
) O
= O
0 O
, O
but O
dm O
( O
x O
, O
y O
) O
= O
0 O
is O
possible O
for O
x O
6= O
y O
• O
dm O
( O
x O
, O
z O
) O
≤ O
dm O
( O
x O
, O
y O
) O
+ O
dm O
( O
y O
, O
z O
) O
hence O
, O
quasi-psuedo O
metrics O
, O
which O
do O
not O
satisfy O
the O
symmetry O
property O
are O
best O
suited O
to O
measure O
the O
distance O
between O
any O
two O
embeddings O
. O

section 9
id pdf2json/2021.acl-long.69.pdf.json
we O
can O
generate O
such O
metrics O
, O
given O
a O
measure O
d. O
if O
the O
cause O
phrase O
u O
has O
pword O
tokens O
, O
and O
the O
effect O
phrase O
v O
has O
q O
word O
tokens O
, O
we O
choose O
the O
maxmatching O
method O
given O
in O
( O
xie O
and O
mu O
, O
2019 O
) O
in O
our O
definition O
of O
dm O
by O
iterating O
through O
all O
pairs O
of O
words O
( O
vb O
, O
ua O
) O
: O
vb O
6= O
ua O
. O

section 9
id pdf2json/2021.acl-long.69.pdf.json
note O
that O
the O
measure O
d O
computes O
the O
difference O
between O
v O
to O
u O
over O
the O
total O
m O
number O
of O
dimensions O
in O
f O
( O
vb O
) O
, O
f O
( O
ua O
) O
. O

section 9
id pdf2json/2021.acl-long.69.pdf.json
d O
( O
u O
, O
v O
) O
= O
min O
a=1..p O
b=1..q O
vb O
6=ua O
m∑ O
j=1 O
( O
fj O
( O
vb O
) O
− O
fj O
( O
ua O
) O
) O
( O
1 O
) O
dm O
( O
f O
( O
u O
) O
, O
f O
( O
v O
) O
) O
= O
{ O
d O
( O
u O
, O
v O
) O
, O
if O
d O
( O
u O
, O
v O
) O
> O
0 O
10−d O
( O
u O
, O
v O
) O
− O
1 O
, O
otherwise O
( O
2 O
) O
we O
chose O
this O
definition O
, O
as O
it O
is O
differentiable O
( O
except O
at O
0 O
, O
where O
we O
choose O
the O
gradient O
to O
be O
0 O
) O
. O

section 9
id pdf2json/2021.acl-long.69.pdf.json
also O
, O
for O
each O
point O
u O
in O
the O
embedding O
space O
, O
there O
is O
a O
corresponding O
hyperplane O
that O
passes O
through O
it O
that O
defines O
the O
half-space O
which O
separates O
the O
reachable O
nodes O
v O
: O
d O
( O
u O
, O
v O
) O
> O
0 O
- O
nodes O
which O
have O
either O
an O
indirect O
or O
direct O
causal O
link O
and O
the O
unreachable O
nodes O
v O
: O
d O
( O
u O
, O
v O
) O
< O
0 O
. O

section 9
id pdf2json/2021.acl-long.69.pdf.json
also O
, O
by O
the O
property O
of O
d O
( O
u O
, O
v O
) O
= O
−d O
( O
v O
, O
u O
) O
, O
we O
see O
that O
if O
v O
is O
reachable O
from O
u O
, O
then O
u O
is O
not O
reachable O
from O
v O
, O
thus O
affirming O
that O
this O
is O
suitable O
to O
represent O
a O
causal O
graph O
that O
is O
directed O
and O
acyclic O
. O

section 10
id pdf2json/2021.acl-long.69.pdf.json
there O
are O
currently O
many O
approaches O
to O
learning O
causal O
representations O
, O
one O
which O
uses O
a O
masked O
language O
modeling O
approach O
where O
the O
word O
tokens O
in O
the O
cause O
are O
paired O
with O
word O
tokens O
in O
the O
effect O
using O
a O
skip-gram O
technique O
in O
an O
unsupervised O
setting O
. O

section 10
id pdf2json/2021.acl-long.69.pdf.json
in O
the O
supervised O
setting O
, O
models O
align O
the O
cause-effect O
embeddings O
to O
solve O
either O
a O
sequence-to-sequence O
translation O
task O
or O
logistic O
classification O
task O
. O

section 10
id pdf2json/2021.acl-long.69.pdf.json
since O
we O
aim O
to O
capture O
all O
the O
nodes O
of O
the O
causal O
graph O
into O
a O
single O
set O
of O
word O
embeddings O
, O
we O
choose O
this O
approach O
. O

section 10
id pdf2json/2021.acl-long.69.pdf.json
further O
, O
in O
the O
supervised O
setting O
, O
we O
make O
explicit O
the O
causal O
relationship O
between O
cause O
and O
effect O
, O
thereby O
capturing O
the O
directionality O
of O
the O
linkage O
. O

section 10
id pdf2json/2021.acl-long.69.pdf.json
thus O
, O
a O
supervised O
model O
could O
translate O
a O
cause O
to O
an O
effect O
or O
predict O
the O
link O
that O
exists O
from O
a O
cause O
to O
an O
effect O
. O

section 10
id pdf2json/2021.acl-long.69.pdf.json
among O
these O
supervised O
modeling O
choices O
, O
we O
choose O
the O
binary O
classification O
task O
of O
predicting O
if O
a O
directed O
edge O
exists O
between O
two O
nodes O
in O
the O
causal O
graph O
. O

section 10
id pdf2json/2021.acl-long.69.pdf.json
this O
supervised O
learning O
is O
achieved O
by O
following O
the O
technique O
of O
fine-tuning O
as O
proposed O
in O
( O
veitch O
et O
al. O
, O
2020 O
) O
. O

section 10
id pdf2json/2021.acl-long.69.pdf.json
formally O
, O
given O
a O
cause O
phrase O
u O
, O
an O
effect O
phrase O
v O
, O
let O
an O
i O
( O
u O
, O
v O
) O
be O
an O
edge O
indicator O
variable O
i O
( O
u O
, O
v O
) O
= O
1u→v O
that O
takes O
binary O
values O
of O
{ O
0 O
, O
1 O
} O
based O
on O
the O
existence O
of O
an O
edge O
from O
u→ O
v O
in O
the O
causal O
graph O
. O

section 10
id pdf2json/2021.acl-long.69.pdf.json
pre-trained O
contextual O
models O
: O
pre-trained O
models O
based O
on O
transformers O
like O
bert O
( O
devlin O
et O
al. O
, O
2019 O
) O
, O
roberta O
( O
liu O
et O
al. O
, O
2019b O
) O
learn O
contextual O
embeddings O
of O
words O
or O
tokens O
by O
optimizing O
for O
the O
self-supervision O
task O
of O
predicting O
randomly O
masked O
tokens O
in O
a O
sentence O
. O

section 10
id pdf2json/2021.acl-long.69.pdf.json
these O
pretrained O
embeddings O
for O
word O
tokens O
have O
been O
used O
extensively O
for O
fine-tuning O
. O

section 10
id pdf2json/2021.acl-long.69.pdf.json
here O
, O
we O
use O
such O
finetuned O
models O
denoted O
as O
g̃ O
to O
predict O
the O
existence O
of O
an O
edge O
between O
the O
cause O
and O
effect O
u O
, O
v O
, O
by O
embedding O
them O
into O
f O
( O
u O
) O
, O
f O
( O
v O
) O
respectively O
and O
further O
optimizing O
them O
in O
the O
fine-tuning O
stage O
on O
the O
following O
cross-entropy O
classification O
loss O
ls O
= O
eu O
, O
v∼c O
crossent O
( O
i O
( O
u O
, O
v O
) O
, O
g̃ O
( O
u O
, O
v O
) O
) O
( O
3 O
) O

section 11
id pdf2json/2021.acl-long.69.pdf.json
given O
the O
faithfulness O
definition O
, O
our O
goal O
is O
to O
learn O
an O
embedding O
that O
minimizes O
the O
number O
of O
violations O
of O
the O
faithfulness O
property O
. O

section 11
id pdf2json/2021.acl-long.69.pdf.json
for O
each O
of O
the O
3 O
conditions O
present O
in O
the O
faithfulness O
property O
, O
we O
define O
how O
we O
measure O
their O
adherence O
and O
incorporate O
it O
in O
the O
loss O
function O
. O

section 11
id pdf2json/2021.acl-long.69.pdf.json
in O
addition O
to O
the O
causal O
graph O
link O
prediction O
task O
, O
we O
now O
present O
how O
the O
faithfulness O
properties O
are O
incorporated O
through O
regularization O
constraints O
. O

section 12
id pdf2json/2021.acl-long.69.pdf.json
since O
we O
expect O
a O
single O
embedding O
distance O
threshold O
that O
perfectly O
encapsulates O
the O
neighborhood O
of O
a O
node O
, O
we O
can O
measure O
this O
by O
varying O
distance O
thresholds O
for O
neighborhood O
detection O
and O
compute O
the O
area O
under O
the O
curve O
of O
the O
precisionrecall O
curve O
. O

section 12
id pdf2json/2021.acl-long.69.pdf.json
since O
we O
aim O
to O
retain O
all O
the O
neighbors O
of O
a O
node O
in O
the O
causal O
graph O
within O
an O
upper O
bound O
of O
the O
distance O
in O
the O
embedding O
space O
, O
we O
add O
the O
sum O
of O
the O
distance O
between O
the O
nodes O
and O
their O
neighbors O
as O
an O
l1 O
regularization O
loss O
. O

section 12
id pdf2json/2021.acl-long.69.pdf.json
ln O
= O
e O
u∼c O
v∈neigh O
( O
u O
) O
|dm O
( O
f O
( O
u O
) O
, O
f O
( O
v O
) O
) O
| O
( O
4 O
) O

section 13
id pdf2json/2021.acl-long.69.pdf.json
since O
checking O
for O
true O
uniformity O
can O
be O
computationally O
intractable O
, O
we O
approximate O
by O
computing O
the O
per-dimension O
aggregate O
of O
all O
the O
word O
embeddings O
and O
compute O
the O
wasserstein O
distance O
( O
olkin O
and O
pukelsheim O
, O
1982 O
) O
between O
the O
observed O
distribution O
and O
the O
expected O
uniform O
distribution O
centered O
around O
zero O
( O
0m O
) O
. O

section 13
id pdf2json/2021.acl-long.69.pdf.json
since O
, O
in O
the O
uniformity O
constraint O
, O
we O
would O
expect O
that O
the O
embeddings O
are O
centered O
around O
zero O
, O
the O
mean O
of O
the O
embeddings O
should O
be O
close O
to O
zero O
. O

section 13
id pdf2json/2021.acl-long.69.pdf.json
we O
measure O
the O
distance O
from O
this O
expected O
centroid O
and O
penalize O
the O
model O
for O
a O
high O
distance O
. O

section 13
id pdf2json/2021.acl-long.69.pdf.json
if O
cb O
denote O
the O
set O
of O
nodes O
chosen O
in O
a O
batch O
b O
, O
with O
size O
|b| O
, O
and O
fj O
( O
p O
) O
denote O
the O
jth O
dimension O
of O
the O
embedding O
of O
node O
p O
, O
then O
we O
present O
the O
uniformity O
regularization O
loss O
: O
lu O
= O
m∑ O
j=1 O
1 O
|b| O
∑ O
p∈cb O
fj O
( O
p O
) O
( O
5 O
) O

section 14
id pdf2json/2021.acl-long.69.pdf.json
to O
measure O
if O
inequalities O
between O
two O
distances O
in O
the O
causal O
graph O
hold O
in O
the O
embedding O
space O
, O
we O
measure O
the O
pearson O
correlation O
coefficient O
between O
samples O
of O
distances O
between O
words O
in O
the O
causal O
graph O
and O
that O
of O
the O
embeddings O
. O

section 14
id pdf2json/2021.acl-long.69.pdf.json
to O
ensure O
that O
any O
two O
distances O
sampled O
from O
the O
causal O
graph O
maintain O
the O
same O
inequality O
in O
the O
embedding O
space O
, O
we O
sample O
random O
nodes O
from O
the O
causal O
graph O
and O
compute O
the O
empirical O
pearson O
correlation O
coefficient O
of O
their O
distances O
in O
the O
embedding O
space O
. O

section 14
id pdf2json/2021.acl-long.69.pdf.json
a O
perfect O
correlation O
would O
lead O
to O
a O
coefficient O
of O
+1 O
, O
so O
we O
penalize O
any O
deviation O
from O
that O
ideal O
correlation O
and O
present O
the O
distance O
correlation O
loss O
: O
lc O
= O
1− O
ρdc O
, O
dm O
= O
1− O
cov O
( O
dc O
, O
dm O
) O
σdcσdm O
( O
6 O
) O
note O
that O
all O
the O
above O
constraints O
are O
at O
a O
batch O
level O
and O
hence O
is O
added O
on O
to O
the O
batch O
crossentropy O
loss O
during O
every O
back-propagation O
step O
. O

section 14
id pdf2json/2021.acl-long.69.pdf.json
since O
the O
losses O
are O
differentiable O
, O
we O
have O
used O
the O
auto-diff O
capability O
available O
in O
tensorflow O
. O

section 14
id pdf2json/2021.acl-long.69.pdf.json
the O
contribution O
of O
each O
of O
the O
above O
losses O
are O
combined O
using O
the O
augmented O
lagrangian O
method O
( O
hestenes O
, O
1969 O
) O
and O
controlled O
using O
3 O
parameters O
α O
, O
β O
, O
γ O
as O
follows O
: O
l O
= O
( O
1− O
α− O
β O
− O
γ O
) O
ls O
+ O
αln O
+ O
βlu O
+ O
γlc O
( O
7 O
) O
the O
values O
of O
these O
hyperparameters O
were O
chosen O
to O
be O
0.1 O
, O
0.15 O
, O
0.1 O
respectively O
after O
crossvalidation O
to O
optimize O
causal O
link O
prediction O
accuracy O
and O
faithfulness O
metrics O
. O

section 14
id pdf2json/2021.acl-long.69.pdf.json
a O
summary O
of O
our O
approach O
is O
outlined O
in O
algorithm O
1 O
. O

section 14
id pdf2json/2021.acl-long.69.pdf.json
the O
learning O
rate O
a O
= O
0.01 O
, O
lu O
, O
lc O
are O
computed O
per O
batch O
by O
maintaining O
the O
required O
variables O
f O
( O
u O
) O
, O
f O
( O
v O
) O
, O
dc O
( O
u O
, O
v O
) O
, O
dm O
( O
f O
( O
u O
) O
, O
f O
( O
v O
) O
) O
in O
memory O
. O

section 14
id pdf2json/2021.acl-long.69.pdf.json
these O
are O
implemented O
using O
tensorflow O
’ O
s O
eager O
execution O
framework O
. O

section 16
id pdf2json/2021.acl-long.69.pdf.json
the O
causal O
evidence O
graphs O
we O
use O
contain O
phrases O
like O
“ O
heavy O
rainfall O
” O
as O
causes O
and O
effects O
, O
which O
require O
us O
to O
learn O
the O
combined O
embeddings O
of O
the O
phrases O
. O

section 16
id pdf2json/2021.acl-long.69.pdf.json
restricting O
ourselves O
to O
just O
individual O
words O
would O
leave O
out O
the O
context O
required O
to O
understand O
the O
context O
to O
understand O
the O
causeeffect O
pairs O
. O

section 16
id pdf2json/2021.acl-long.69.pdf.json
for O
example O
, O
the O
kind O
of O
effects O
“ O
heavy O
algorithm O
1 O
faithful O
embedding O
training O
1 O
: O
input O
: O
pre-trained O
bert O
based O
model O
g̃ O
, O
causal O
graph O
c O
, O
distance O
measures O
: O
dc O
, O
dm O
, O
2 O
: O
for O
e=1..epochs O
do O
3 O
: O
l O
= O
0 O
4 O
: O
for O
j=1..b O
do O
5 O
: O
u O
, O
v O
∼ O
c O
: O
∑ O
1i O
( O
u O
, O
v O
) O
=0 O
= O
∑ O
1i O
( O
u O
, O
v O
) O
=1 O
6 O
: O
ls O
+= O
crossent O
( O
i O
( O
u O
, O
v O
) O
, O
g̃ O
( O
u O
, O
v O
) O
) O
7 O
: O
ln O
+= O
∑ O
w∈neigh O
( O
u O
) O
dm O
( O
f O
( O
u O
) O
, O
f O
( O
w O
) O
) O
8 O
: O
store O
f O
( O
u O
) O
, O
f O
( O
v O
) O
to O
update O
lu O
9 O
: O
store O
dc O
( O
u O
, O
v O
) O
, O
dm O
( O
f O
( O
u O
) O
, O
f O
( O
v O
) O
) O
to O
up- O
date O
lc O
10 O
: O
end O
for O
11 O
: O
update O
lu O
, O
lc O
and O
compute O
l O
( O
eqn O
7 O
) O
12 O
: O
backprop O
g̃ O
← O
g̃ O
− O
a O
( O
∂l∂g̃ O
) O
13 O
: O
end O
for O
rainfall O
” O
might O
have O
could O
be O
different O
from O
just O
“ O
rainfall O
” O
. O

section 16
id pdf2json/2021.acl-long.69.pdf.json
we O
thus O
utilize O
the O
contextual O
embedding O
framework O
used O
to O
learn O
language O
models O
in O
bert O
( O
devlin O
et O
al. O
, O
2019 O
) O
, O
as O
a O
way O
to O
learn O
contextual O
embeddings O
that O
align O
with O
a O
given O
graphical O
causal O
model O
. O

section 16
id pdf2json/2021.acl-long.69.pdf.json
note O
that O
there O
may O
be O
more O
than O
one O
causal O
model O
provided O
by O
experts O
based O
on O
their O
domains O
, O
and O
it O
is O
important O
to O
view O
our O
contribution O
as O
a O
way O
to O
align O
with O
domain O
expertise O
( O
for O
example O
, O
medical O
, O
legal O
, O
privacy O
, O
etc O
) O
with O
their O
respective O
causal O
models O
as O
a O
common O
mechanism O
to O
represent O
the O
said O
domain O
knowledge O
. O

section 16
id pdf2json/2021.acl-long.69.pdf.json
we O
use O
two O
causal O
graphs O
to O
construct O
their O
respective O
faithful O
embeddings O
, O
and O
demonstrate O
the O
utility O
of O
the O
embeddings O
in O
downstream O
tasks O
. O

section 16
id pdf2json/2021.acl-long.69.pdf.json
the O
first O
causal O
graph O
we O
use O
is O
identical O
to O
the O
one O
used O
in O
( O
sharp O
et O
al. O
, O
2016 O
) O
, O
which O
uses O
the O
815,233 O
cause-effect O
pairs O
extracted O
from O
the O
annotated O
gigaword O
and O
wikipedia O
dataset O
, O
and O
an O
equal O
number O
of O
random O
relation O
pairs O
that O
are O
not O
causal O
as O
negative O
samples O
. O

section 16
id pdf2json/2021.acl-long.69.pdf.json
the O
second O
causal O
graph O
is O
extracted O
from O
the O
web O
by O
( O
heindorf O
et O
al. O
, O
2020b O
) O
, O
who O
use O
a O
bootstrapping O
approach O
with O
the O
initial O
pattern O
of O
“ O
a O
causes O
b O
” O
and O
apply O
it O
to O
the O
clueweb12 O
web O
crawl O
dataset O
with O
733,019,372 O
english O
web O
pages O
, O
between O
february O
and O
may O
2012 O
. O

section 16
id pdf2json/2021.acl-long.69.pdf.json
from O
this O
web O
crawl O
, O
they O
provide O
a O
causal O
graph O
with O
80,223 O
concept O
nodes O
and O
199,803 O
causal O
links O
between O
the O
nodes O
. O

section 16
id pdf2json/2021.acl-long.69.pdf.json
this O
graph O
has O
been O
sampled O
and O
validated O
by O
human O
annotators O
with O
over O
96 O
% O
precision O
. O

section 16
id pdf2json/2021.acl-long.69.pdf.json
for O
our O
indirect O
evaluation O
based O
on O
downstream O
question O
answering O
tasks O
, O
we O
use O
the O
3031 O
causal O
questions O
from O
yahoo O
! O

section 16
id pdf2json/2021.acl-long.69.pdf.json
answers O
corpus O
( O
sharp O
et O
al. O
, O
2016 O
) O
. O

section 16
id pdf2json/2021.acl-long.69.pdf.json
these O
questions O
are O
of O
the O
form O
“ O
what O
causes O
x O
? O
” O
, O
and O
we O
use O
our O
faithful O
embeddings O
as O
a O
drop-in O
replacement O
for O
this O
causal O
qa O
task O
. O

section 17
id pdf2json/2021.acl-long.69.pdf.json
evaluating O
embeddings O
intrinsically O
has O
often O
led O
to O
varying O
leaderboards O
( O
jastrzebski O
et O
al. O
, O
2017 O
) O
, O
hence O
we O
evaluate O
our O
embeddings O
based O
on O
their O
ability O
to O
map O
to O
the O
cause-effect O
relationship O
directly O
. O

section 17
id pdf2json/2021.acl-long.69.pdf.json
we O
measure O
the O
faithfulness O
of O
the O
trained O
embeddings O
, O
using O
3 O
metrics O
, O
one O
per O
property O
as O
per O
eqns O
4 O
, O
5 O
, O
6 O
. O

section 17
id pdf2json/2021.acl-long.69.pdf.json
for O
the O
neighborhood O
condition O
, O
we O
measure O
the O
area O
under O
the O
precision-recall O
curve O
as O
we O
choose O
multiple O
thresholds O
to O
define O
the O
neighborhood O
in O
the O
embedding O
space O
to O
correspondingly O
identify O
the O
relevant O
neighbors O
in O
the O
causal O
graph O
. O

section 17
id pdf2json/2021.acl-long.69.pdf.json
for O
the O
uniformity O
condition O
, O
we O
measure O
the O
means O
of O
the O
per-dimension O
values O
of O
the O
word O
embeddings O
and O
compute O
the O
1st O
wasserstein O
( O
olkin O
and O
pukelsheim O
, O
1982 O
) O
distance O
from O
the O
expected O
centroid O
of O
zero O
. O

section 17
id pdf2json/2021.acl-long.69.pdf.json
we O
also O
perform O
a O
statistical O
test O
for O
uniform O
distribution O
, O
which O
measures O
the O
mean O
kolmogorov-smirnov O
( O
k-s O
) O
test O
statistic O
( O
daniel O
, O
1990 O
) O
by O
bucketing O
embedding O
each O
dimension O
into O
10 O
buckets O
. O

section 17
id pdf2json/2021.acl-long.69.pdf.json
since O
each O
dimension O
’ O
s O
test O
statistic O
can O
either O
pass O
or O
fail O
the O
test O
based O
on O
the O
significance O
level O
, O
we O
present O
the O
total O
number O
of O
dimensions O
that O
pass O
the O
test O
at O
α O
= O
0.05 O
significance O
level O
. O

section 17
id pdf2json/2021.acl-long.69.pdf.json
finally O
, O
to O
measure O
the O
distance O
correlation O
property O
, O
we O
report O
the O
pearson O
correlation O
coefficient O
between O
distances O
in O
the O
causal O
graph O
and O
the O
embeddings O
on O
a O
held-out O
part O
of O
the O
causal O
graph O
. O

section 17
id pdf2json/2021.acl-long.69.pdf.json
for O
the O
qa O
task O
, O
we O
report O
the O
precision-at-one O
( O
p O
@ O
1 O
) O
, O
the O
fraction O
of O
test O
samples O
where O
the O
highest O
ranked O
answer O
is O
relevant O
and O
the O
mean O
reciprocal O
rank O
( O
mrr O
) O
( O
manning O
et O
al. O
, O
2008 O
) O
, O
the O
inverse O
of O
the O
position O
of O
the O
correct O
answer O
in O
our O
ranking O
on O
the O
held-out O
question O
set O
provided O
by O
( O
sharp O
et O
al. O
, O
2015 O
) O
. O

section 18
id pdf2json/2021.acl-long.69.pdf.json
we O
evaluate O
our O
faithful O
embeddings O
by O
comparing O
them O
against O
two O
state-of-the-art O
approaches O
described O
in O
( O
sharp O
et O
al. O
, O
2016 O
) O
and O
( O
veitch O
et O
al. O
, O
2020 O
) O
. O

section 18
id pdf2json/2021.acl-long.69.pdf.json
cembedbi O
uses O
a O
bi-directional O
model O
, O
with O
the O
task O
of O
predicting O
the O
masked O
cause O
and O
effect O
word O
tokens O
. O

section 18
id pdf2json/2021.acl-long.69.pdf.json
this O
approach O
uses O
separate O
embeddings O
for O
words O
used O
as O
causes O
and O
effects O
. O

section 18
id pdf2json/2021.acl-long.69.pdf.json
causal O
{ O
bert O
, O
roberta O
} O
( O
veitch O
et O
al. O
, O
2020 O
) O
uses O
the O
fine-tuning O
technique O
for O
the O
binary O
classification O
of O
edge O
detection O
, O
similar O
to O
ours O
, O
on O
the O
pre-trained O
large-uncased O
model O
. O

section 18
id pdf2json/2021.acl-long.69.pdf.json
we O
can O
thus O
compare O
the O
gains O
we O
get O
by O
incorporating O
faithfulness O
conditions O
on O
the O
embeddings O
in O
downstream O
tasks O
. O

section 20
id pdf2json/2021.acl-long.69.pdf.json
as O
shown O
in O
tables O
1 O
and O
2 O
, O
our O
faithful-roberta O
model O
outperforms O
causal- O
{ O
bert O
, O
roberta O
} O
and O
cembedbi O
( O
sharp O
et O
al. O
, O
2016 O
) O
on O
each O
of O
the O
three O
properties O
of O
faithfulness O
, O
namely O
the O
neighborhood O
, O
uniformity O
, O
and O
distance O
correlation O
, O
by O
more O
than O
30 O
% O
. O

section 20
id pdf2json/2021.acl-long.69.pdf.json
additionally O
, O
we O
report O
the O
correlation O
for O
euclidean O
and O
cosine O
similarity O
, O
despite O
not O
using O
it O
to O
optimize O
at O
training O
time O
. O

section 20
id pdf2json/2021.acl-long.69.pdf.json
faithful O
versions O
of O
the O
bert O
and O
roberta O
models O
increase O
the O
area O
under O
the O
curve O
of O
the O
precision-recall O
curve O
in O
detecting O
neighboring O
nodes O
of O
the O
gigaword O
and O
causenet O
causal O
graphs O
by O
21-23 O
% O
and O
17-20 O
% O
respectively O
. O

section 20
id pdf2json/2021.acl-long.69.pdf.json
in O
figure O
2 O
, O
we O
present O
the O
precision-recall O
curve O
when O
we O
use O
the O
models O
for O
ranking O
causal O
pairs O
above O
non-causal O
pairs O
on O
the O
semeval O
task O
8 O
tuples O
( O
hendrickx O
et O
al. O
, O
2007 O
) O
by O
varying O
the O
distance O
threshold O
in O
the O
embedding O
space O
which O
outlines O
the O
boundary O
of O
the O
neighboring O
nodes O
in O
the O
causal O
graph O
. O

section 20
id pdf2json/2021.acl-long.69.pdf.json
this O
increase O
in O
accuracy O
for O
neighborhood O
detection O
indicates O
that O
incorporating O
the O
constraints O
during O
training O
time O
with O
our O
asymmetric O
causal O
embedding O
distance O
provides O
benefits O
in O
aligning O
the O
contextual O
embeddings O
as O
per O
the O
causal O
graph O
. O

section 21
id pdf2json/2021.acl-long.69.pdf.json
to O
evaluate O
if O
learning O
faithful O
embeddings O
is O
useful O
for O
causal O
aligned O
downstream O
tasks O
, O
we O
evaluate O
the O
fine-tuned O
embeddings O
to O
be O
directly O
used O
for O
question O
answering O
. O

section 21
id pdf2json/2021.acl-long.69.pdf.json
as O
used O
in O
( O
fried O
et O
al. O
, O
2015 O
) O
, O
we O
use O
the O
maximum O
, O
minimum O
, O
average O
distance O
between O
words O
of O
the O
question O
and O
answer O
words O
and O
the O
overall O
distance O
between O
the O
composite O
question O
and O
answer O
vectors O
from O
the O
embedding O
. O

section 21
id pdf2json/2021.acl-long.69.pdf.json
note O
that O
since O
both O
cembedbi O
and O
causal- O
{ O
bert O
, O
roberta O
} O
are O
trained O
with O
cosine O
similarity O
in O
mind O
, O
we O
use O
the O
cosine O
similarity O
, O
but O
for O
our O
faithful- O
{ O
bert O
, O
roberta O
} O
models O
, O
the O
distance O
measure O
used O
to O
rank O
is O
the O
quasi-pseudo O
metric O
defined O
in O
def O
2 O
. O

section 21
id pdf2json/2021.acl-long.69.pdf.json
we O
use O
these O
4 O
features O
to O
train O
an O
svm O
ranker O
to O
re-rank O
candidate O
answers O
provided O
by O
the O
candidate O
retrieval O
tool O
( O
jansen O
et O
al. O
, O
2014 O
) O
. O

section 21
id pdf2json/2021.acl-long.69.pdf.json
we O
see O
in O
table O
3 O
that O
faithful-roberta O
increases O
both O
the O
precision O
of O
the O
first O
answer O
predicted O
by O
10.2 O
% O
, O
and O
the O
mean O
reciprocal O
rank O
by O
10.8 O
% O
. O

section 21
id pdf2json/2021.acl-long.69.pdf.json
this O
means O
that O
not O
only O
is O
the O
first O
ranked O
answer O
more O
causally O
correct O
, O
but O
the O
retrieval O
of O
the O
correct O
answer O
in O
the O
top-k O
positions O
has O
improved O
. O

section 21
id pdf2json/2021.acl-long.69.pdf.json
this O
improvement O
in O
an O
out-of-domain O
qa O
task O
by O
aligning O
the O
embeddings O
to O
an O
externally O
available O
causal O
graph O
demonstrates O
that O
benefits O
of O
faithfulness O
transfer O
to O
downstream O
tasks O
. O

section 22
id pdf2json/2021.acl-long.69.pdf.json
to O
understand O
the O
reason O
behind O
the O
improved O
performance O
, O
we O
performed O
a O
qualitative O
inspection O
of O
100 O
randomly O
sampled O
word O
pairs O
from O
the O
gigaword O
causal O
graph O
1 O
that O
are O
at O
varying O
distances O
in O
the O
original O
pre-trained O
embedding O
and O
trace O
1https O
: O
//github.com/ananthnyu/faithful-causal-rep/ O
how O
they O
have O
re-aligned O
after O
fine-tuning O
with O
the O
faithfulness O
objective O
. O

section 22
id pdf2json/2021.acl-long.69.pdf.json
we O
annotate O
each O
of O
these O
word-pairs O
as O
being O
either O
causal O
or O
not O
as O
shown O
in O
the O
confusion O
matrix O
with O
examples O
in O
table O
4 O
. O

section 22
id pdf2json/2021.acl-long.69.pdf.json
in O
figure O
3 O
, O
we O
see O
re-alignment O
of O
these O
word O
pairs O
from O
association O
based O
roberta O
embeddings O
to O
the O
causally O
aligned O
faithful-roberta O
embedding O
space O
, O
that O
is O
, O
causal O
word O
pairs O
( O
blue O
and O
orange O
) O
move O
closer O
, O
and O
non-causal O
word O
pairs O
( O
green O
and O
red O
) O
move O
further O
based O
on O
the O
quasi-pseudo O
metric O
dm O
. O

section 22
id pdf2json/2021.acl-long.69.pdf.json
specifically O
, O
the O
associative O
but O
non-causal O
word O
pairs O
( O
green O
) O
have O
moved O
further O
in O
faithfulroberta O
, O
while O
the O
non-associative O
but O
causal O
word O
pairs O
( O
orange O
) O
have O
moved O
closer O
. O

section 22
id pdf2json/2021.acl-long.69.pdf.json
we O
see O
that O
in O
the O
cosine-similarity O
based O
roberta O
, O
the O
causal O
word O
pairs O
had O
a O
mean O
distance O
of O
0.48 O
, O
while O
in O
the O
quasi-pseudo O
metric O
based O
faithfulroberta O
, O
the O
mean O
distance O
between O
the O
causal O
word O
pairs O
reduced O
to O
0.28 O
. O

section 22
id pdf2json/2021.acl-long.69.pdf.json
the O
distances O
are O
normalized O
between O
0 O
and O
1 O
based O
on O
the O
maximum O
and O
minimum O
values O
of O
distances O
( O
cosine O
or O
dm O
) O
in O
the O
sampled O
word-pairs O
. O

section 22
id pdf2json/2021.acl-long.69.pdf.json
we O
further O
analyzed O
how O
these O
associative O
and O
causal O
re-alignments O
impacted O
the O
causal O
qa O
task O
by O
categorizing O
the O
word O
pairs O
into O
three O
types O
of O
variables O
- O
mediators O
, O
colliders O
and O
confounders O
. O

section 22
id pdf2json/2021.acl-long.69.pdf.json
mediators O
: O
for O
the O
question O
, O
“ O
what O
causes O
a O
tornado O
? O
” O
, O
the O
answer O
involves O
“ O
thunderstorms O
” O
, O
which O
is O
a O
mediator O
caused O
by O
“ O
high O
pressure O
” O
. O

section 22
id pdf2json/2021.acl-long.69.pdf.json
we O
see O
that O
“ O
high O
pressure O
” O
is O
now O
much O
closer O
to O
“ O
tornado O
” O
in O
faithful-roberta O
than O
baseline O
embeddings O
. O

section 22
id pdf2json/2021.acl-long.69.pdf.json
colliders O
: O
for O
the O
question O
, O
“ O
what O
causes O
persistent O
cough O
? O
” O
, O
the O
colliders O
“ O
smoking O
” O
and O
“ O
asthma O
” O
have O
moved O
further O
based O
on O
dm O
in O
faithful-roberta O
. O

section 22
id pdf2json/2021.acl-long.69.pdf.json
confounders O
: O
for O
questions O
with O
confounders O
like O
, O
“ O
what O
causes O
indigestion O
? O
” O
, O
the O
confounding O
links O
“ O
anxiety→ O
indigestion O
” O
, O
and O
“ O
anxiety→ O
insomnia O
” O
are O
near O
, O
but O
“ O
insomnia→ O
indigestion O
” O
, O
is O
far O
. O

section 22
id pdf2json/2021.acl-long.69.pdf.json
this O
further O
demonstrates O
the O
utility O
of O
incorporating O
faithfulness O
over O
multiple O
nodes O
of O
the O
graph O
, O
in O
addition O
to O
pairwise O
causal O
link O
prediction O
. O

section 23
id pdf2json/2021.acl-long.69.pdf.json
we O
show O
that O
the O
faithfulness O
of O
text O
embeddings O
to O
a O
causal O
graph O
is O
important O
for O
causal O
inferencealigned O
downstream O
tasks O
. O

section 23
id pdf2json/2021.acl-long.69.pdf.json
by O
incorporating O
the O
three O
faithfulness O
properties O
of O
neighborhood O
, O
uniformity O
, O
and O
distance O
correlation O
through O
regularization O
constraints O
while O
learning O
embeddings O
, O
we O
improve O
the O
precision O
of O
the O
first O
ranked O
answer O
in O
the O
causal O
qa O
task O
by O
10.2 O
% O
. O

section 23
id pdf2json/2021.acl-long.69.pdf.json
we O
show O
that O
this O
is O
due O
to O
causal O
re-alignment O
of O
embeddings O
as O
per O
an O
asymmetric O
pseudo-distance O
metric O
. O

section 24
id pdf2json/2021.acl-long.69.pdf.json
we O
thank O
sam O
bowman O
for O
his O
feedback O
to O
the O
draft O
version O
of O
this O
manuscript O
. O

section TITLE
id pdf2json/2021.acl-long.520.pdf.json
question O
answering O
over O
temporal O
knowledge O
graphs O

section ABSTRACT
id pdf2json/2021.acl-long.520.pdf.json
temporal O
knowledge O
graphs O
( O
temporal O
kgs O
) O
extend O
regular O
knowledge O
graphs O
by O
providing O
temporal O
scopes O
( O
e.g. O
, O
start O
and O
end O
times O
) O
on O
each O
edge O
in O
the O
kg O
. O

section ABSTRACT
id pdf2json/2021.acl-long.520.pdf.json
while O
question O
answering O
over O
kg O
( O
kgqa O
) O
has O
received O
some O
attention O
from O
the O
research O
community O
, O
qa O
over O
temporal O
kgs O
( O
temporal O
kgqa O
) O
is O
a O
relatively O
unexplored O
area O
. O

section ABSTRACT
id pdf2json/2021.acl-long.520.pdf.json
lack O
of O
broadcoverage O
datasets O
has O
been O
another O
factor O
limiting O
progress O
in O
this O
area O
. O

section ABSTRACT
id pdf2json/2021.acl-long.520.pdf.json
we O
address O
this O
challenge O
by O
presenting O
cronquestions O
, O
the O
largest O
known O
temporal O
kgqa O
dataset O
, O
clearly O
stratified O
into O
buckets O
of O
structural O
complexity O
. O

section ABSTRACT
id pdf2json/2021.acl-long.520.pdf.json
cronquestions O
expands O
the O
only O
known O
previous O
dataset O
by O
a O
factor O
of O
340× O
. O

section ABSTRACT
id pdf2json/2021.acl-long.520.pdf.json
we O
find O
that O
various O
state-of-the-art O
kgqa O
methods O
fall O
far O
short O
of O
the O
desired O
performance O
on O
this O
new O
dataset O
. O

section ABSTRACT
id pdf2json/2021.acl-long.520.pdf.json
in O
response O
, O
we O
also O
propose O
cronkgqa O
, O
a O
transformerbased O
solution O
that O
exploits O
recent O
advances O
in O
temporal O
kg O
embeddings O
, O
and O
achieves O
performance O
superior O
to O
all O
baselines O
, O
with O
an O
increase O
of O
120 O
% O
in O
accuracy O
over O
the O
next O
best O
performing O
method O
. O

section ABSTRACT
id pdf2json/2021.acl-long.520.pdf.json
through O
extensive O
experiments O
, O
we O
give O
detailed O
insights O
into O
the O
workings O
of O
cronkgqa O
, O
as O
well O
as O
situations O
where O
significant O
further O
improvements O
appear O
possible O
. O

section ABSTRACT
id pdf2json/2021.acl-long.520.pdf.json
in O
addition O
to O
the O
dataset O
, O
we O
have O
released O
our O
code O
as O
well O
. O

section 0
id pdf2json/2021.acl-long.520.pdf.json
proceedings O
of O
the O
59th O
annual O
meeting O
of O
the O
association O
for O
computational O
linguistics O
and O
the O
11th O
international O
joint O
conference O
on O
natural O
language O
processing O
, O
pages O
6663–6676 O
august O
1–6 O
, O
2021 O
. O

section 0
id pdf2json/2021.acl-long.520.pdf.json
©2021 O
association O
for O
computational O
linguistics O
6663 O

section 1
id pdf2json/2021.acl-long.520.pdf.json
temporal O
knowledge O
graphs O
( O
temporal O
kgs O
) O
are O
multi-relational O
graph O
where O
each O
edge O
is O
associated O
with O
a O
time O
duration O
. O

section 1
id pdf2json/2021.acl-long.520.pdf.json
this O
is O
in O
contrast O
to O
a O
regular O
kg O
where O
no O
time O
annotation O
is O
present O
. O

section 1
id pdf2json/2021.acl-long.520.pdf.json
for O
example O
, O
a O
regular O
kg O
may O
contain O
a O
fact O
such O
as O
( O
barack O
obama O
, O
held O
position O
, O
president O
of O
usa O
) O
, O
while O
a O
temporal O
kg O
would O
contain O
the O
start O
and O
end O
time O
as O
well O
— O
( O
barack O
obama O
, O
held O
position O
, O
president O
of O
usa O
, O
2008 O
, O
2016 O
) O
. O

section 1
id pdf2json/2021.acl-long.520.pdf.json
edges O
may O
be O
associated O
with O
a O
set O
of O
non-contiguous O
time O
intervals O
as O
well O
. O

section 1
id pdf2json/2021.acl-long.520.pdf.json
these O
temporal O
scopes O
on O
facts O
can O
be O
either O
automatically O
estimated O
( O
talukdar O
et O
al. O
, O
2012 O
) O
or O
user O
contributed O
. O

section 1
id pdf2json/2021.acl-long.520.pdf.json
several O
such O
temporal O
kgs O
have O
been O
proposed O
in O
the O
literature O
, O
where O
the O
focus O
is O
on O
kg O
completion O
( O
dasgupta O
et O
al O
. O

section 1
id pdf2json/2021.acl-long.520.pdf.json
2018 O
; O
garcı́a-durán O
et O
al O
. O

section 1
id pdf2json/2021.acl-long.520.pdf.json
2018 O
; O
leetaru O
and O
schrodt O
2013 O
; O
lacroix O
et O
al O
. O

section 1
id pdf2json/2021.acl-long.520.pdf.json
2020 O
; O
jain O
et O
al O
. O

section 1
id pdf2json/2021.acl-long.520.pdf.json
2020 O
) O
. O

section 1
id pdf2json/2021.acl-long.520.pdf.json
the O
task O
of O
knowledge O
graph O
question O
answering O
( O
kgqa O
) O
is O
to O
answer O
natural O
language O
questions O
using O
a O
kg O
as O
the O
knowledge O
base O
. O

section 1
id pdf2json/2021.acl-long.520.pdf.json
this O
is O
in O
contrast O
to O
reading O
comprehension-based O
question O
answering O
, O
where O
typically O
the O
question O
is O
accompanied O
by O
a O
context O
( O
e.g. O
, O
text O
passage O
) O
and O
the O
answer O
is O
either O
one O
of O
multiple O
choices O
( O
rajpurkar O
et O
al. O
, O
2016 O
) O
or O
a O
piece O
of O
text O
from O
the O
context O
( O
yang O
et O
al. O
, O
2018 O
) O
. O

section 1
id pdf2json/2021.acl-long.520.pdf.json
in O
kgqa O
, O
the O
answer O
is O
usually O
an O
entity O
( O
node O
) O
in O
the O
kg O
, O
and O
the O
reasoning O
required O
to O
answer O
questions O
is O
either O
single-fact O
based O
( O
bordes O
et O
al. O
, O
2015 O
) O
, O
multi-hop O
( O
yih O
et O
al O
. O

section 1
id pdf2json/2021.acl-long.520.pdf.json
2015 O
, O
zhang O
et O
al O
. O

section 1
id pdf2json/2021.acl-long.520.pdf.json
2017 O
) O
or O
conjunction/comparison O
based O
reasoning O
( O
talmor O
and O
berant O
, O
2018 O
) O
. O

section 1
id pdf2json/2021.acl-long.520.pdf.json
temporal O
kgqa O
takes O
this O
a O
step O
further O
where O
: O
1 O
. O

section 1
id pdf2json/2021.acl-long.520.pdf.json
the O
underlying O
kg O
is O
a O
temporal O
kg O
. O

section 1
id pdf2json/2021.acl-long.520.pdf.json
2 O
. O

section 1
id pdf2json/2021.acl-long.520.pdf.json
the O
answer O
is O
either O
an O
entity O
or O
time O
duration O
. O

section 1
id pdf2json/2021.acl-long.520.pdf.json
3 O
. O

section 1
id pdf2json/2021.acl-long.520.pdf.json
complex O
temporal O
reasoning O
might O
be O
needed O
. O

section 1
id pdf2json/2021.acl-long.520.pdf.json
kg O
embeddings O
are O
low-dimensional O
dense O
vector O
representations O
of O
entities O
and O
relations O
in O
a O
kg O
. O

section 1
id pdf2json/2021.acl-long.520.pdf.json
several O
methods O
have O
been O
proposed O
in O
the O
literature O
to O
embed O
kgs O
( O
bordes O
et O
al O
. O

section 1
id pdf2json/2021.acl-long.520.pdf.json
2013 O
, O
trouillon O
et O
al O
. O

section 1
id pdf2json/2021.acl-long.520.pdf.json
2016 O
, O
vashishth O
et O
al O
. O

section 1
id pdf2json/2021.acl-long.520.pdf.json
2020 O
) O
. O

section 1
id pdf2json/2021.acl-long.520.pdf.json
these O
embeddings O
were O
originally O
proposed O
for O
the O
task O
of O
kg O
completion O
i.e. O
, O
predicting O
missing O
edges O
in O
the O
kg O
, O
since O
most O
real O
world O
kgs O
are O
incomplete O
. O

section 1
id pdf2json/2021.acl-long.520.pdf.json
recently O
, O
however O
, O
they O
have O
also O
been O
applied O
to O
the O
task O
of O
kgqa O
where O
they O
have O
been O
shown O
to O
increase O
performance O
the O
settings O
of O
both O
of O
complete O
and O
incomplete O
kgs O
( O
saxena O
et O
al O
. O

section 1
id pdf2json/2021.acl-long.520.pdf.json
2020 O
; O
sun O
et O
al O
. O

section 1
id pdf2json/2021.acl-long.520.pdf.json
2020 O
) O
. O

section 1
id pdf2json/2021.acl-long.520.pdf.json
temporal O
kg O
embeddings O
are O
another O
upcoming O
area O
where O
entities O
, O
relations O
and O
timestamps O
in O
a O
temporal O
kg O
are O
embedded O
in O
a O
low-dimensional O
vector O
space O
( O
dasgupta O
et O
al O
. O

section 1
id pdf2json/2021.acl-long.520.pdf.json
2018 O
, O
lacroix O
et O
al O
. O

section 1
id pdf2json/2021.acl-long.520.pdf.json
2020 O
, O
jain O
et O
al O
. O

section 1
id pdf2json/2021.acl-long.520.pdf.json
2020 O
, O
goel O
et O
al O
. O

section 1
id pdf2json/2021.acl-long.520.pdf.json
2019 O
) O
. O

section 1
id pdf2json/2021.acl-long.520.pdf.json
here O
too O
, O
the O
main O
application O
so O
far O
has O
been O
temporal O
kg O
completion O
. O

section 1
id pdf2json/2021.acl-long.520.pdf.json
in O
our O
work O
, O
we O
investigate O
whether O
temporal O
kg O
embeddings O
can O
be O
applied O
to O
the O
task O
of O
temporal O
kgqa O
, O
and O
how O
they O
fare O
compared O
to O
non-temporal O
embeddings O
or O
off-the-shelf O
methods O
without O
any O
kg O
embeddings O
. O

section 1
id pdf2json/2021.acl-long.520.pdf.json
in O
this O
paper O
we O
propose O
cronquestions O
, O
a O
new O
dataset O
for O
temporal O
kgqa O
. O

section 1
id pdf2json/2021.acl-long.520.pdf.json
cronquestions O
consists O
of O
both O
a O
temporal O
kg O
and O
accompanying O
natural O
language O
questions O
. O

section 1
id pdf2json/2021.acl-long.520.pdf.json
there O
were O
three O
main O
guiding O
principles O
while O
creating O
this O
dataset O
: O
1 O
. O

section 1
id pdf2json/2021.acl-long.520.pdf.json
the O
associated O
kg O
must O
provide O
temporal O
an- O
notations O
. O

section 1
id pdf2json/2021.acl-long.520.pdf.json
2 O
. O

section 1
id pdf2json/2021.acl-long.520.pdf.json
questions O
must O
involve O
an O
element O
of O
temporal O
reasoning O
. O

section 1
id pdf2json/2021.acl-long.520.pdf.json
3 O
. O

section 1
id pdf2json/2021.acl-long.520.pdf.json
the O
number O
of O
labeled O
instances O
must O
be O
large O
enough O
that O
it O
can O
be O
used O
for O
training O
models O
, O
rather O
than O
for O
evaluation O
alone O
. O

section 1
id pdf2json/2021.acl-long.520.pdf.json
guided O
by O
the O
above O
principles O
, O
we O
present O
a O
dataset O
consisting O
of O
a O
temporal O
kg O
with O
125k O
entities O
and O
328k O
facts O
, O
along O
with O
a O
set O
of O
410k O
natural O
language O
questions O
that O
require O
temporal O
reasoning O
. O

section 1
id pdf2json/2021.acl-long.520.pdf.json
on O
this O
new O
dataset O
, O
we O
apply O
approaches O
based O
on O
deep O
language O
models O
( O
lm O
) O
alone O
, O
such O
as O
t5 O
( O
raffel O
et O
al. O
, O
2020 O
) O
, O
bert O
( O
devlin O
et O
al. O
, O
2019 O
) O
, O
and O
knowbert O
( O
peters O
et O
al. O
, O
2019 O
) O
, O
and O
also O
hybrid O
lm+kg O
embedding O
approaches O
, O
such O
as O
entities-as-experts O
( O
févry O
et O
al. O
, O
2020 O
) O
and O
embedkgqa O
( O
saxena O
et O
al. O
, O
2020 O
) O
. O

section 1
id pdf2json/2021.acl-long.520.pdf.json
we O
find O
that O
these O
baselines O
are O
not O
suited O
to O
temporal O
reasoning O
. O

section 1
id pdf2json/2021.acl-long.520.pdf.json
in O
response O
, O
we O
propose O
cronkgqa O
, O
an O
enhancement O
of O
embedkgqa O
, O
which O
outperforms O
baselines O
across O
all O
question O
types O
. O

section 1
id pdf2json/2021.acl-long.520.pdf.json
cronkgqa O
achieves O
very O
high O
accuracy O
on O
simple O
temporal O
reasoning O
questions O
, O
but O
falls O
short O
when O
it O
comes O
to O
questions O
requiring O
more O
complex O
reasoning O
. O

section 1
id pdf2json/2021.acl-long.520.pdf.json
thus O
, O
although O
we O
get O
promising O
early O
results O
, O
cronquestions O
leaves O
ample O
scope O
to O
improve O
complex O
temporal O
kgqa O
. O

section 1
id pdf2json/2021.acl-long.520.pdf.json
our O
source O
code O
along O
with O
the O
cronquestions O
dataset O
can O
be O
found O
at O
https O
: O
//github.com/apoorvumang/cronkgqa O
. O

section 3
id pdf2json/2021.acl-long.520.pdf.json
there O
have O
been O
several O
kgqa O
datasets O
proposed O
in O
the O
literature O
( O
table O
1 O
) O
. O

section 3
id pdf2json/2021.acl-long.520.pdf.json
in O
simplequestions O
( O
bordes O
et O
al. O
, O
2015 O
) O
one O
needs O
to O
extract O
just O
a O
single O
fact O
from O
the O
kg O
to O
answer O
a O
question O
. O

section 3
id pdf2json/2021.acl-long.520.pdf.json
metaqa O
( O
zhang O
et O
al. O
, O
2017 O
) O
and O
webquestionssp O
( O
yih O
et O
al. O
, O
2015 O
) O
require O
multi-hop O
reasoning O
, O
where O
one O
must O
traverse O
over O
multiple O
edges O
in O
the O
kg O
to O
reach O
the O
answer O
. O

section 3
id pdf2json/2021.acl-long.520.pdf.json
complexwebquestions O
( O
talmor O
and O
berant O
, O
2018 O
) O
contains O
both O
multi-hop O
and O
conjunction/comparison O
type O
questions O
. O

section 3
id pdf2json/2021.acl-long.520.pdf.json
however O
, O
none O
of O
these O
are O
aimed O
at O
temporal O
reasoning O
, O
and O
the O
kg O
they O
are O
based O
on O
is O
non-temporal O
. O

section 3
id pdf2json/2021.acl-long.520.pdf.json
temporal O
qa O
datasets O
have O
mostly O
been O
studied O
in O
the O
area O
of O
reading O
comprehension O
. O

section 3
id pdf2json/2021.acl-long.520.pdf.json
one O
such O
dataset O
is O
torque O
( O
ning O
et O
al. O
, O
2020 O
) O
, O
where O
the O
system O
is O
given O
a O
question O
along O
with O
some O
context O
( O
a O
text O
passage O
) O
and O
is O
asked O
to O
answer O
a O
multiple O
choice O
question O
with O
five O
choices O
. O

section 3
id pdf2json/2021.acl-long.520.pdf.json
this O
is O
in O
contrast O
to O
kgqa O
, O
where O
there O
is O
no O
context O
, O
and O
the O
answer O
is O
one O
of O
potentially O
hundreds O
of O
thousands O
of O
entities O
. O

section 3
id pdf2json/2021.acl-long.520.pdf.json
tempquestions O
( O
jia O
et O
al. O
, O
2018a O
) O
is O
a O
kgqa O
dataset O
specifically O
aimed O
at O
temporal O
qa O
. O

section 3
id pdf2json/2021.acl-long.520.pdf.json
it O
consists O
of O
a O
subset O
of O
questions O
from O
webquestions O
, O
free917 O
( O
cai O
and O
yates O
, O
2013 O
) O
and O
complexquestions O
( O
bao O
et O
al. O
, O
2016 O
) O
that O
are O
temporal O
in O
nature O
. O

section 3
id pdf2json/2021.acl-long.520.pdf.json
they O
gave O
a O
definition O
for O
“ O
temporal O
question O
” O
and O
used O
certain O
trigger O
words O
( O
for O
example O
‘ O
before O
’ O
, O
‘ O
after O
’ O
) O
along O
with O
other O
constraints O
to O
filter O
out O
questions O
from O
these O
datasets O
that O
fell O
under O
this O
definition O
. O

section 3
id pdf2json/2021.acl-long.520.pdf.json
however O
, O
this O
dataset O
contains O
only O
1271 O
questions O
— O
useful O
only O
for O
evaluation O
— O
and O
the O
kg O
on O
which O
it O
is O
based O
( O
a O
subset O
of O
freebase O
( O
bollacker O
et O
al. O
, O
2008 O
) O
) O
is O
not O
a O
temporal O
kg O
. O

section 3
id pdf2json/2021.acl-long.520.pdf.json
another O
drawback O
is O
that O
freebase O
has O
not O
been O
under O
active O
development O
since O
2015 O
, O
therefore O
some O
information O
stored O
in O
it O
is O
outdated O
and O
this O
is O
a O
potential O
source O
of O
inaccuracy O
. O

section 4
id pdf2json/2021.acl-long.520.pdf.json
to O
the O
best O
of O
our O
knowledge O
, O
recent O
kgqa O
algorithms O
( O
miller O
et O
al O
. O

section 4
id pdf2json/2021.acl-long.520.pdf.json
2016 O
; O
sun O
et O
al O
. O

section 4
id pdf2json/2021.acl-long.520.pdf.json
2019 O
; O
cohen O
et O
al O
. O

section 4
id pdf2json/2021.acl-long.520.pdf.json
2020 O
; O
sun O
et O
al O
. O

section 4
id pdf2json/2021.acl-long.520.pdf.json
2020 O
) O
work O
with O
nontemporal O
kgs O
, O
i.e. O
, O
kgs O
containing O
facts O
of O
the O
form O
( O
subject O
, O
relation O
, O
object O
) O
. O

section 4
id pdf2json/2021.acl-long.520.pdf.json
extending O
these O
to O
temporal O
kgs O
containing O
facts O
of O
the O
form O
( O
subject O
, O
relation O
, O
object O
, O
start O
time O
, O
end O
time O
) O
is O
a O
non-trivial O
task O
. O

section 4
id pdf2json/2021.acl-long.520.pdf.json
tequila O
( O
jia O
et O
al. O
, O
2018b O
) O
is O
one O
method O
aimed O
specifically O
at O
temporal O
kgqa O
. O

section 4
id pdf2json/2021.acl-long.520.pdf.json
tequila O
decomposes O
and O
rewrites O
the O
question O
into O
nontemporal O
sub-questions O
and O
temporal O
constraints O
. O

section 4
id pdf2json/2021.acl-long.520.pdf.json
answers O
to O
sub-questions O
are O
then O
retrieved O
using O
any O
kgqa O
engine O
. O

section 4
id pdf2json/2021.acl-long.520.pdf.json
finally O
, O
tequila O
uses O
constraint O
reasoning O
on O
temporal O
intervals O
to O
compute O
final O
answers O
to O
the O
full O
question O
. O

section 4
id pdf2json/2021.acl-long.520.pdf.json
a O
major O
drawback O
of O
this O
approach O
is O
the O
use O
of O
pre-specified O
templates O
for O
decomposition O
, O
as O
well O
as O
the O
assumption O
of O
having O
temporal O
constraints O
on O
entities O
. O

section 4
id pdf2json/2021.acl-long.520.pdf.json
also O
, O
since O
it O
is O
made O
for O
non-temporal O
kgs O
, O
there O
is O
no O
direct O
way O
of O
applying O
it O
to O
temporal O
kgs O
where O
facts O
are O
temporally O
scoped O
. O

section 5
id pdf2json/2021.acl-long.520.pdf.json
cronquestions O
, O
our O
temporal O
kgqa O
dataset O
consists O
of O
two O
parts O
: O
a O
kg O
with O
temporal O
annotations O
, O
and O
a O
set O
of O
natural O
language O
questions O
requiring O
temporal O
reasoning O
. O

section 6
id pdf2json/2021.acl-long.520.pdf.json
to O
prepare O
our O
temporal O
kg O
, O
we O
started O
by O
taking O
all O
facts O
with O
temporal O
annotations O
from O
the O
wikidata O
subset O
proposed O
by O
lacroix O
et O
al O
. O

section 6
id pdf2json/2021.acl-long.520.pdf.json
( O
2020 O
) O
. O

section 6
id pdf2json/2021.acl-long.520.pdf.json
we O
removed O
some O
instances O
of O
the O
predicate O
“ O
member O
of O
sports O
team O
” O
in O
order O
to O
balance O
out O
the O
kg O
since O
this O
predicate O
constituted O
over O
50 O
percent O
of O
the O
facts O
. O

section 6
id pdf2json/2021.acl-long.520.pdf.json
timestamps O
were O
discretized O
to O
years O
. O

section 6
id pdf2json/2021.acl-long.520.pdf.json
this O
resulted O
in O
a O
kg O
with O
323k O
facts O
, O
125k O
entities O
and O
203 O
relations O
. O

section 6
id pdf2json/2021.acl-long.520.pdf.json
however O
, O
this O
filtering O
of O
facts O
misses O
out O
on O
important O
world O
events O
. O

section 6
id pdf2json/2021.acl-long.520.pdf.json
for O
example O
, O
the O
kg O
subset O
created O
using O
the O
aforementioned O
technique O
contains O
the O
entity O
world O
war O
ii O
but O
no O
associated O
fact O
that O
tells O
us O
when O
world O
war O
ii O
started O
or O
ended O
. O

section 6
id pdf2json/2021.acl-long.520.pdf.json
this O
knowledge O
is O
needed O
to O
answer O
questions O
such O
as O
“ O
who O
was O
the O
president O
of O
the O
usa O
during O
world O
war O
ii O
? O
. O
” O
to O
overcome O
this O
shortcoming O
, O
we O
first O
extracted O
entities O
from O
wikidata O
that O
have O
a O
“ O
start O
time O
” O
and O
“ O
end O
time O
” O
annotation O
. O

section 6
id pdf2json/2021.acl-long.520.pdf.json
from O
this O
set O
, O
we O
then O
removed O
entities O
which O
were O
game O
shows O
, O
movies O
or O
television O
series O
( O
since O
these O
are O
not O
important O
world O
events O
, O
but O
do O
have O
a O
start O
and O
end O
time O
annotation O
) O
, O
and O
then O
removed O
entities O
with O
less O
than O
50 O
associated O
facts O
. O

section 6
id pdf2json/2021.acl-long.520.pdf.json
this O
final O
set O
of O
entitities O
was O
then O
added O
as O
facts O
in O
the O
format O
( O
wwii O
, O
significant O
event O
, O
occurred O
, O
1939 O
, O
1945 O
) O
. O

section 6
id pdf2json/2021.acl-long.520.pdf.json
the O
final O
temporal O
kg O
consisted O
of O
328k O
facts O
out O
of O
which O
5k O
are O
event-facts O
. O

section 7
id pdf2json/2021.acl-long.520.pdf.json
to O
generate O
the O
qa O
dataset O
, O
we O
started O
with O
a O
set O
of O
templates O
for O
temporal O
reasoning O
. O

section 7
id pdf2json/2021.acl-long.520.pdf.json
these O
were O
made O
using O
the O
five O
most O
frequent O
relations O
from O
our O
wikidata O
subset O
, O
namely O
• O
member O
of O
sports O
team O
• O
position O
held O
• O
award O
received O
• O
spouse O
• O
employer O
this O
resulted O
in O
30 O
unique O
seed O
templates O
over O
five O
relations O
and O
five O
different O
reasoning O
structures O
( O
please O
see O
table O
2 O
for O
some O
examples O
) O
. O

section 7
id pdf2json/2021.acl-long.520.pdf.json
each O
of O
these O
templates O
has O
a O
corresponding O
procedure O
that O
could O
be O
executed O
over O
the O
temporal O
kg O
to O
extract O
all O
possible O
answers O
for O
that O
template O
. O

section 7
id pdf2json/2021.acl-long.520.pdf.json
however O
, O
similar O
to O
zhang O
et O
al O
. O

section 7
id pdf2json/2021.acl-long.520.pdf.json
( O
2017 O
) O
, O
we O
chose O
not O
to O
make O
this O
procedure O
a O
part O
of O
the O
dataset O
, O
to O
remove O
unwelcome O
dependence O
of O
qa O
systems O
on O
such O
formal O
candidate O
collection O
methods O
. O

section 7
id pdf2json/2021.acl-long.520.pdf.json
this O
also O
allows O
easy O
augmentation O
of O
the O
dataset O
, O
since O
only O
question-answer O
pairs O
are O
needed O
. O

section 7
id pdf2json/2021.acl-long.520.pdf.json
in O
the O
same O
spirit O
as O
complexwebquestions O
, O
we O
then O
asked O
human O
annotators O
to O
paraphrase O
these O
templates O
in O
order O
to O
generate O
more O
linguistic O
diversity O
. O

section 7
id pdf2json/2021.acl-long.520.pdf.json
annotators O
were O
given O
slot-filled O
templates O
with O
dummy O
entities O
and O
times O
, O
and O
asked O
to O
rephrase O
the O
question O
such O
that O
the O
dummy O
entities/times O
were O
present O
in O
the O
paraphrase O
and O
the O
question O
meaning O
did O
not O
change O
. O

section 7
id pdf2json/2021.acl-long.520.pdf.json
this O
resulted O
in O
246 O
unique O
templates O
. O

section 7
id pdf2json/2021.acl-long.520.pdf.json
we O
then O
used O
the O
monolingual O
paraphraser O
developed O
by O
hu O
et O
al O
. O

section 7
id pdf2json/2021.acl-long.520.pdf.json
( O
2019 O
) O
to O
automatically O
generate O
paraphrases O
using O
these O
246 O
templates O
. O

section 7
id pdf2json/2021.acl-long.520.pdf.json
after O
verifying O
their O
correctness O
through O
annotators O
, O
we O
ended O
up O
with O
654 O
templates O
. O

section 7
id pdf2json/2021.acl-long.520.pdf.json
these O
templates O
were O
then O
filled O
using O
entity O
aliases O
from O
wikidata O
to O
generate O
410k O
unique O
question-answer O
pairs O
. O

section 7
id pdf2json/2021.acl-long.520.pdf.json
finally O
, O
while O
splitting O
the O
data O
into O
train/test O
folds O
, O
we O
ensured O
that O
1 O
. O

section 7
id pdf2json/2021.acl-long.520.pdf.json
paraphrases O
of O
train O
questions O
are O
not O
present O
in O
test O
questions O
. O

section 7
id pdf2json/2021.acl-long.520.pdf.json
2 O
. O

section 7
id pdf2json/2021.acl-long.520.pdf.json
there O
is O
no O
entity O
overlap O
between O
test O
questions O
and O
train O
questions O
. O

section 7
id pdf2json/2021.acl-long.520.pdf.json
event O
overlap O
is O
allowed O
. O

section 7
id pdf2json/2021.acl-long.520.pdf.json
the O
second O
requirement O
implies O
that O
, O
if O
the O
question O
“ O
who O
was O
president O
before O
obama O
” O
is O
present O
in O
the O
train O
set O
, O
the O
test O
set O
can O
not O
contain O
any O
question O
that O
mentions O
the O
entity O
‘ O
obama O
’ O
. O

section 7
id pdf2json/2021.acl-long.520.pdf.json
while O
this O
policy O
may O
appear O
like O
an O
overabundance O
of O
caution O
, O
it O
ensures O
that O
models O
are O
doing O
temporal O
reasoning O
rather O
than O
guessing O
from O
entities O
seen O
during O
training O
. O

section 7
id pdf2json/2021.acl-long.520.pdf.json
lewis O
et O
al O
. O

section 7
id pdf2json/2021.acl-long.520.pdf.json
( O
2020 O
) O
noticed O
an O
issue O
in O
webquestions O
where O
they O
found O
that O
almost O
30 O
% O
of O
test O
questions O
overlapped O
with O
training O
questions O
. O

section 7
id pdf2json/2021.acl-long.520.pdf.json
the O
issue O
has O
been O
seen O
in O
the O
metaqa O
dataset O
as O
well O
, O
where O
there O
is O
significant O
overlap O
between O
test/train O
entities O
and O
test/train O
question O
paraphrases O
, O
leading O
to O
suspiciously O
high O
performance O
on O
baseline O
methods O
even O
with O
partial O
kg O
data O
( O
saxena O
et O
al. O
, O
2020 O
) O
, O
which O
suggests O
that O
models O
that O
apparently O
perform O
well O
are O
not O
necessarily O
performing O
the O
desired O
reasoning O
over O
the O
kg O
. O

section 7
id pdf2json/2021.acl-long.520.pdf.json
a O
drawback O
of O
our O
data O
creation O
protocol O
is O
that O
question/answer O
pairs O
are O
generated O
automatically O
. O

section 7
id pdf2json/2021.acl-long.520.pdf.json
therefore O
, O
the O
question O
distribution O
is O
artificial O
from O
a O
semantic O
perspective O
. O

section 7
id pdf2json/2021.acl-long.520.pdf.json
( O
complexwebquestions O
has O
a O
similar O
limitation O
. O
) O

section 7
id pdf2json/2021.acl-long.520.pdf.json
however O
, O
since O
developing O
models O
that O
are O
capable O
of O
temporal O
reasoning O
is O
an O
important O
direction O
for O
natural O
language O
understanding O
, O
we O
feel O
that O
our O
dataset O
provides O
an O
opportunity O
to O
both O
train O
and O
evaluate O
kgqa O
models O
because O
of O
its O
large O
size O
, O
notwithstanding O
its O
lower-than-natural O
linguistic O
variety O
. O

section 7
id pdf2json/2021.acl-long.520.pdf.json
in O
section O
6.4 O
, O
we O
show O
the O
effect O
that O
training O
data O
size O
has O
on O
model O
performance O
. O

section 7
id pdf2json/2021.acl-long.520.pdf.json
summarizing O
, O
each O
of O
our O
examples O
contains O
1 O
. O

section 7
id pdf2json/2021.acl-long.520.pdf.json
a O
paraphrased O
natural O
language O
question O
. O

section 7
id pdf2json/2021.acl-long.520.pdf.json
2 O
. O

section 7
id pdf2json/2021.acl-long.520.pdf.json
a O
set O
of O
entities/times O
in O
the O
question O
. O

section 7
id pdf2json/2021.acl-long.520.pdf.json
3 O
. O

section 7
id pdf2json/2021.acl-long.520.pdf.json
a O
set O
of O
‘ O
gold O
’ O
answers O
( O
entity O
or O
time O
) O
. O

section 7
id pdf2json/2021.acl-long.520.pdf.json
the O
entities O
are O
specified O
as O
wikidata O
ids O
( O
e.g. O
, O
q219237 O
) O
, O
and O
times O
are O
years O
( O
e.g. O
, O
1991 O
) O
. O

section 7
id pdf2json/2021.acl-long.520.pdf.json
we O
include O
the O
set O
of O
entities/times O
in O
the O
test O
questions O
as O
well O
since O
similar O
to O
other O
kgqa O
datasets O
( O
metaqa O
, O
webquestions O
, O
complexwebquestions O
) O
and O
methods O
that O
use O
these O
datasets O
( O
pullnet O
, O
emql O
) O
, O
entity O
linking O
is O
considered O
as O
a O
separate O
problem O
and O
complete O
entity O
linking O
is O
as- O
sumed O
. O

section 7
id pdf2json/2021.acl-long.520.pdf.json
we O
also O
include O
the O
seed O
template O
and O
head/tail/time O
annotation O
in O
the O
train O
fold O
, O
but O
omit O
these O
from O
the O
test O
fold O
. O

section 8
id pdf2json/2021.acl-long.520.pdf.json
in O
order O
to O
aid O
analysis O
, O
we O
categorize O
questions O
into O
“ O
simple O
reasoning O
” O
and O
“ O
complex O
reasoning O
” O
questions O
( O
please O
refer O
to O
table O
4 O
for O
the O
distribution O
statistics O
) O
. O

section 8
id pdf2json/2021.acl-long.520.pdf.json
simple O
reasoning O
: O
these O
questions O
require O
a O
sin- O
gle O
fact O
to O
answer O
, O
where O
the O
answer O
can O
be O
either O
an O
entity O
or O
a O
time O
instance O
. O

section 8
id pdf2json/2021.acl-long.520.pdf.json
for O
example O
the O
question O
“ O
who O
was O
the O
president O
of O
the O
united O
states O
in O
2008 O
? O
” O
requires O
a O
single O
fact O
to O
answer O
the O
question O
, O
namely O
( O
barack O
obama O
, O
held O
position O
, O
president O
of O
usa O
, O
2008 O
, O
2016 O
) O
complex O
reasoning O
: O
these O
questions O
require O
multiple O
facts O
to O
answer O
and O
can O
be O
more O
varied O
. O

section 8
id pdf2json/2021.acl-long.520.pdf.json
for O
example O
“ O
who O
was O
the O
first O
president O
of O
the O
united O
states O
? O
” O
this O
requires O
reasoning O
over O
multiple O
facts O
pertaining O
to O
the O
entity O
“ O
president O
of O
the O
united O
states O
” O
. O

section 8
id pdf2json/2021.acl-long.520.pdf.json
in O
our O
dataset O
, O
all O
questions O
that O
are O
not O
“ O
simple O
reasoning O
” O
questions O
are O
considered O
complex O
questions O
. O

section 8
id pdf2json/2021.acl-long.520.pdf.json
these O
are O
further O
categorized O
into O
the O
types O
“ O
before/after O
‘ O
’ O
, O
“ O
first/last O
” O
and O
“ O
time O
join O
” O
— O
please O
refer O
table O
2 O
for O
examples O
of O
these O
questions O
. O

section 9
id pdf2json/2021.acl-long.520.pdf.json
we O
investigate O
how O
we O
can O
use O
kg O
embeddings O
, O
both O
temporal O
and O
non-temporal O
, O
along O
with O
pretrained O
language O
models O
to O
perform O
temporal O
kgqa O
. O

section 9
id pdf2json/2021.acl-long.520.pdf.json
we O
will O
first O
briefly O
describe O
the O
specific O
kg O
embedding O
models O
we O
use O
, O
and O
then O
go O
on O
to O
show O
how O
we O
use O
them O
in O
our O
qa O
models O
. O

section 9
id pdf2json/2021.acl-long.520.pdf.json
in O
all O
cases O
, O
the O
scores O
are O
turned O
into O
suitable O
losses O
with O
regard O
to O
positive O
and O
negative O
tuples O
in O
an O
incomplete O
kg O
, O
and O
these O
losses O
minimized O
to O
train O
the O
entity O
, O
time O
and O
relation O
representations O
. O

section 10
id pdf2json/2021.acl-long.520.pdf.json
complex O
( O
trouillon O
et O
al. O
, O
2016 O
) O
represents O
each O
entity O
e O
as O
a O
complex O
vector O
ue O
∈ O
cd O
. O

section 10
id pdf2json/2021.acl-long.520.pdf.json
each O
relation O
r O
is O
represented O
as O
a O
complex O
vector O
vr O
∈ O
cd O
as O
well O
. O

section 10
id pdf2json/2021.acl-long.520.pdf.json
the O
score O
φ O
of O
a O
claimed O
fact O
( O
s O
, O
r O
, O
o O
) O
is O
φ O
( O
s O
, O
r O
, O
o O
) O
= O
< O
( O
〈us O
, O
vr O
, O
u O
? O
o〉 O
) O
= O
< O
( O
∑d O
d=1 O
us O
[ O
d O
] O
vr O
[ O
d O
] O
uo O
[ O
d O
] O
? O
) O

section 10
id pdf2json/2021.acl-long.520.pdf.json
( O
1 O
) O
where O
< O
( O
· O
) O
denotes O
the O
real O
part O
and O
c O
? O

section 10
id pdf2json/2021.acl-long.520.pdf.json
is O
the O
complex O
conjugate O
. O

section 10
id pdf2json/2021.acl-long.520.pdf.json
despite O
further O
developments O
, O
complex O
, O
along O
with O
refined O
training O
protocols O
( O
lacroix O
et O
al. O
, O
2018 O
) O
remains O
among O
the O
strongest O
kb O
embedding O
approaches O
( O
ruffinelli O
et O
al. O
, O
2020 O
) O
. O

section 11
id pdf2json/2021.acl-long.520.pdf.json
lacroix O
et O
al O
. O

section 11
id pdf2json/2021.acl-long.520.pdf.json
( O
2020 O
) O
took O
an O
early O
step O
to O
extend O
complex O
with O
time O
. O

section 11
id pdf2json/2021.acl-long.520.pdf.json
each O
timestamp O
t O
is O
also O
represented O
as O
a O
complex O
vector O
wt O
∈ O
cd O
. O

section 11
id pdf2json/2021.acl-long.520.pdf.json
for O
a O
claimed O
fact O
( O
s O
, O
r O
, O
o O
, O
t O
) O
, O
their O
tcomplex O
scoring O
function O
is O
φ O
( O
s O
, O
r O
, O
o O
, O
t O
) O
= O
< O
( O
〈us O
, O
vr O
, O
u O
? O
o O
, O
wt〉 O
) O
( O
2 O
) O
their O
tntcomplex O
scoring O
function O
uses O
two O
representations O
of O
relations O
r O
: O
vtr O
, O
which O
is O
sensitive O
to O
time O
, O
and O
vr O
, O
which O
is O
not O
. O

section 11
id pdf2json/2021.acl-long.520.pdf.json
the O
scoring O
function O
is O
the O
sum O
of O
a O
time-sensitive O
and O
a O
time-insensitive O
part O
: O
< O
( O
〈us O
, O
vtr O
, O
u O
? O
o O
, O
wt〉+ O
〈us O
, O
vr O
, O
u O
? O
o,1〉 O
) O
. O

section 12
id pdf2json/2021.acl-long.520.pdf.json
timeplex O
( O
jain O
et O
al. O
, O
2020 O
) O
augmented O
complex O
with O
embeddings O
ut O
∈ O
cd O
for O
discretized O
time O
instants O
t. O
to O
incorporate O
time O
, O
timeplex O
uses O
three O
representations O
for O
each O
relation O
r O
, O
viz. O
, O
( O
vsor O
, O
v O
st O
r O
, O
v O
ot O
r O
) O
and O
writes O
the O
base O
score O
of O
a O
tuple O
( O
s O
, O
r O
, O
o O
, O
t O
) O
as O
φ O
( O
s O
, O
r O
, O
o O
, O
t O
) O
= O
〈us O
, O
vsor O
, O
u O
? O
o〉+ O
α O
〈us O
, O
vstr O
, O
u O
? O
t O
〉 O
+ O
β O
〈uo O
, O
votr O
, O
u O
? O
t O
〉+ O
γ O
〈us O
, O
uo O
, O
u O
? O
t O
〉 O
, O
( O
3 O
) O
where O
α O
, O
β O
, O
γ O
are O
hyperparameters O
. O

section 13
id pdf2json/2021.acl-long.520.pdf.json
we O
start O
with O
a O
temporal O
kg O
, O
apply O
a O
time-agnostic O
or O
time-sensitive O
kg O
embedding O
algorithm O
( O
complex O
, O
tcomplex O
, O
or O
timeplex O
) O
to O
it O
, O
and O
obtain O
entity O
, O
relation O
, O
and O
timestamp O
embeddings O
for O
the O
temporal O
kg O
. O

section 13
id pdf2json/2021.acl-long.520.pdf.json
we O
will O
use O
the O
following O
notation O
. O

section 13
id pdf2json/2021.acl-long.520.pdf.json
• O
e O
is O
the O
matrix O
of O
entity O
embeddings O
• O
t O
is O
the O
matrix O
of O
timestamp O
embeddings O
• O
e O
.t O
is O
the O
concatenation O
of O
e O
and O
t O
matrices O
. O

section 13
id pdf2json/2021.acl-long.520.pdf.json
this O
is O
used O
for O
scoring O
answers O
, O
since O
the O
answer O
can O
be O
either O
an O
entity O
or O
timestamp O
. O

section 13
id pdf2json/2021.acl-long.520.pdf.json
in O
case O
entity/timestamp O
embeddings O
are O
complex O
valued O
vectors O
in O
cd O
, O
we O
expand O
them O
to O
real O
valued O
vectors O
of O
size O
2d O
, O
where O
the O
first O
half O
is O
the O
real O
part O
and O
the O
second O
half O
is O
the O
complex O
part O
of O
the O
original O
vector O
. O

section 13
id pdf2json/2021.acl-long.520.pdf.json
we O
first O
apply O
embedkgqa O
( O
saxena O
et O
al. O
, O
2020 O
) O
directly O
to O
the O
task O
of O
temporal O
kgqa O
. O

section 13
id pdf2json/2021.acl-long.520.pdf.json
in O
its O
original O
implementation O
, O
embedkgqa O
uses O
complex O
( O
section O
4.1 O
) O
embeddings O
and O
can O
only O
deal O
with O
non-temporal O
kgs O
and O
single O
entity O
questions O
. O

section 13
id pdf2json/2021.acl-long.520.pdf.json
in O
order O
to O
apply O
it O
to O
cronquestions O
, O
we O
set O
the O
first O
entity O
encountered O
in O
the O
question O
as O
the O
“ O
head O
entity O
” O
needed O
by O
embedkgqa O
. O

section 13
id pdf2json/2021.acl-long.520.pdf.json
along O
with O
this O
, O
we O
set O
the O
entity O
embedding O
matrix O
e O
to O
be O
the O
complex O
embedding O
of O
our O
kg O
entities O
, O
and O
initialize O
t O
to O
a O
random O
learnable O
matrix O
. O

section 13
id pdf2json/2021.acl-long.520.pdf.json
embedkgqa O
then O
performs O
prediction O
over O
e O
.t O
. O

section 13
id pdf2json/2021.acl-long.520.pdf.json
next O
, O
we O
modify O
embedkgqa O
so O
that O
it O
can O
use O
temporal O
kg O
embeddings O
. O

section 13
id pdf2json/2021.acl-long.520.pdf.json
we O
use O
tcomplex O
( O
section O
4.2 O
) O
for O
getting O
entity O
and O
timestamp O
embeddings O
. O

section 13
id pdf2json/2021.acl-long.520.pdf.json
cronkgqa O
( O
figure O
1 O
) O
utilizes O
two O
scoring O
functions O
, O
one O
for O
predicting O
entity O
and O
one O
for O
predicting O
time O
. O

section 13
id pdf2json/2021.acl-long.520.pdf.json
using O
a O
pre-trained O
lm O
( O
bert O
in O
our O
case O
) O
cronkgqa O
finds O
a O
question O
embedding O
qe O
. O

section 13
id pdf2json/2021.acl-long.520.pdf.json
this O
is O
then O
projected O
to O
get O
two O
embeddings O
, O
qeent O
and O
qetime O
, O
which O
are O
question O
embeddings O
for O
entity O
and O
time O
prediction O
respectively O
. O

section 13
id pdf2json/2021.acl-long.520.pdf.json
entity O
scoring O
function O
: O
we O
extract O
a O
subject O
en- O
tity O
s O
and O
a O
timestamp O
t O
from O
the O
question O
. O

section 13
id pdf2json/2021.acl-long.520.pdf.json
if O
either O
is O
missing O
, O
we O
use O
a O
dummy O
entity/time O
. O

section 13
id pdf2json/2021.acl-long.520.pdf.json
then O
, O
using O
the O
scoring O
function O
φ O
( O
s O
, O
r O
, O
o O
, O
t O
) O
from O
equation O
2 O
, O
we O
calculate O
a O
score O
for O
each O
entity O
e O
∈ O
e O
as O
φent O
( O
e O
) O
= O
< O
( O
〈us O
, O
qeent O
, O
u O
? O
e O
, O
wt〉 O
) O
( O
4 O
) O
where O
e O
is O
the O
set O
of O
entities O
in O
the O
kg O
. O

section 13
id pdf2json/2021.acl-long.520.pdf.json
this O
gives O
us O
a O
score O
for O
each O
entity O
being O
an O
answer O
. O

section 13
id pdf2json/2021.acl-long.520.pdf.json
time O
scoring O
function O
: O
similarly O
, O
we O
extract O
a O
subject O
entity O
s O
and O
object O
entity O
o O
from O
the O
question O
, O
using O
dummy O
entities O
if O
none O
are O
present O
. O

section 13
id pdf2json/2021.acl-long.520.pdf.json
then O
, O
using O
2 O
, O
we O
calculate O
a O
score O
for O
each O
times- O
tamp O
t O
∈ O
t O
as O
φtime O
( O
t O
) O
= O
< O
( O
〈us O
, O
qetime O
, O
u O
? O
o O
, O
wt〉 O
) O
( O
5 O
) O
the O
scores O
for O
all O
entities O
and O
times O
are O
concatenated O
, O
and O
softmax O
is O
used O
to O
calculate O
answer O
probabilities O
over O
this O
combined O
score O
vector O
. O

section 13
id pdf2json/2021.acl-long.520.pdf.json
the O
model O
is O
trained O
using O
cross O
entropy O
loss O
. O

section 14
id pdf2json/2021.acl-long.520.pdf.json
in O
this O
section O
, O
we O
aim O
to O
answer O
the O
following O
questions O
: O
1 O
. O

section 14
id pdf2json/2021.acl-long.520.pdf.json
how O
do O
baselines O
and O
cronkgqa O
perform O
on O
the O
cronquestions O
task O
? O

section 14
id pdf2json/2021.acl-long.520.pdf.json
( O
section O
6.2 O
. O
) O

section 14
id pdf2json/2021.acl-long.520.pdf.json
2 O
. O

section 14
id pdf2json/2021.acl-long.520.pdf.json
do O
some O
methods O
perform O
better O
than O
others O
on O
specific O
reasoning O
tasks O
? O

section 14
id pdf2json/2021.acl-long.520.pdf.json
( O
section O
6.3 O
. O
) O

section 14
id pdf2json/2021.acl-long.520.pdf.json
3 O
. O

section 14
id pdf2json/2021.acl-long.520.pdf.json
how O
much O
does O
the O
training O
dataset O
size O
( O
num- O
ber O
of O
questions O
) O
affect O
the O
performance O
of O
a O
model O
? O

section 14
id pdf2json/2021.acl-long.520.pdf.json
( O
section O
6.4 O
. O
) O

section 14
id pdf2json/2021.acl-long.520.pdf.json
4 O
. O

section 14
id pdf2json/2021.acl-long.520.pdf.json
do O
temporal O
kg O
embeddings O
confer O
any O
advantage O
over O
non-temporal O
kg O
embeddings O
? O

section 14
id pdf2json/2021.acl-long.520.pdf.json
( O
section O
6.5 O
. O
) O

section 15
id pdf2json/2021.acl-long.520.pdf.json
it O
has O
been O
shown O
by O
petroni O
et O
al O
. O

section 15
id pdf2json/2021.acl-long.520.pdf.json
( O
2019 O
) O
and O
raffel O
et O
al O
. O

section 15
id pdf2json/2021.acl-long.520.pdf.json
( O
2020 O
) O
that O
large O
lms O
, O
such O
as O
bert O
and O
its O
variants O
, O
capture O
real O
world O
knowledge O
( O
collected O
from O
their O
massive O
, O
encyclopedic O
training O
corpus O
) O
and O
can O
directly O
be O
applied O
to O
tasks O
such O
as O
qa O
. O

section 15
id pdf2json/2021.acl-long.520.pdf.json
in O
these O
baselines O
, O
we O
do O
not O
specifically O
feed O
our O
version O
of O
the O
temporal O
kg O
to O
the O
model O
— O
we O
instead O
expect O
the O
model O
to O
have O
the O
real O
world O
knowledge O
to O
compute O
the O
answer O
. O

section 15
id pdf2json/2021.acl-long.520.pdf.json
bert O
: O
we O
experiment O
with O
bert O
, O
roberta O
( O
liu O
et O
al. O
, O
2019 O
) O
and O
knowbert O
( O
peters O
et O
al. O
, O
2019 O
) O
which O
is O
a O
variant O
of O
bert O
where O
information O
from O
knowledge O
bases O
such O
as O
wikidata O
and O
wordnet O
has O
been O
injected O
into O
bert O
. O

section 15
id pdf2json/2021.acl-long.520.pdf.json
we O
add O
a O
prediction O
head O
on O
top O
of O
the O
[ O
cls O
] O
token O
of O
the O
final O
layer O
and O
do O
a O
softmax O
over O
it O
to O
predict O
the O
answer O
probabilities O
. O

section 15
id pdf2json/2021.acl-long.520.pdf.json
t5 O
: O
in O
order O
to O
apply O
t5 O
( O
raffel O
et O
al. O
, O
2020 O
) O
to O
temporal O
qa O
, O
we O
transform O
each O
question O
in O
our O
dataset O
to O
the O
form O
‘ O
temporal O
question O
: O
〈question〉 O
? O
’ O
. O

section 15
id pdf2json/2021.acl-long.520.pdf.json
for O
evaluation O
there O
are O
two O
cases O
: O
1 O
. O

section 15
id pdf2json/2021.acl-long.520.pdf.json
time O
answer O
: O
we O
do O
exact O
string O
matching O
between O
t5 O
output O
and O
correct O
answer O
. O

section 15
id pdf2json/2021.acl-long.520.pdf.json
2 O
. O

section 15
id pdf2json/2021.acl-long.520.pdf.json
entity O
answer O
: O
we O
compare O
the O
system O
output O
to O
the O
aliases O
of O
all O
entities O
in O
the O
kg O
. O

section 15
id pdf2json/2021.acl-long.520.pdf.json
the O
entity O
having O
an O
alias O
with O
the O
smallest O
edit O
distance O
( O
levenshtein O
, O
1966 O
) O
to O
the O
predicted O
text O
output O
is O
taken O
as O
the O
predicted O
entity O
. O

section 15
id pdf2json/2021.acl-long.520.pdf.json
entities O
as O
experts O
: O
févry O
et O
al O
. O

section 15
id pdf2json/2021.acl-long.520.pdf.json
( O
2020 O
) O
proposed O
eae O
, O
a O
model O
which O
aims O
to O
integrate O
entity O
knowledge O
into O
a O
transformer-based O
language O
model O
. O

section 15
id pdf2json/2021.acl-long.520.pdf.json
for O
temporal O
kgqa O
on O
cronquestions O
, O
we O
assume O
that O
all O
grounded O
entity O
and O
time O
mention O
spans O
are O
marked O
in O
the O
question1 O
. O

section 15
id pdf2json/2021.acl-long.520.pdf.json
we O
will O
refer O
to O
this O
model O
as O
t-eae-add O
. O

section 15
id pdf2json/2021.acl-long.520.pdf.json
we O
try O
another O
variant O
of O
eae O
, O
t-eae-replace O
, O
where O
instead O
of O
adding O
the O
entity/time O
and O
bert O
token O
embeddings O
, O
we O
replace O
the O
bert O
embeddings O
with O
the O
entity/time O
embeddings O
for O
entity/time O
mentions.2 O
1this O
assumption O
can O
be O
removed O
by O
using O
eae O
’ O
s O
early O
transformer O
stages O
as O
ne O
spotters O
and O
disambiguators O
. O

section 15
id pdf2json/2021.acl-long.520.pdf.json
2appendix O
a.1 O
gives O
details O
of O
our O
eae O
implementation O
. O

section 16
id pdf2json/2021.acl-long.520.pdf.json
table O
5 O
shows O
the O
results O
of O
various O
methods O
on O
our O
dataset O
. O

section 16
id pdf2json/2021.acl-long.520.pdf.json
we O
see O
that O
methods O
based O
on O
large O
pre-trained O
lms O
alone O
( O
bert O
, O
roberta O
, O
t5 O
) O
, O
as O
well O
as O
knowbert O
, O
perform O
significantly O
worse O
than O
methods O
that O
are O
augmented O
with O
kg O
embeddings O
( O
temporal O
or O
non-temporal O
) O
. O

section 16
id pdf2json/2021.acl-long.520.pdf.json
this O
is O
probably O
because O
having O
kg O
embeddings O
specific O
to O
our O
temporal O
kg O
helps O
the O
model O
to O
focus O
on O
those O
entities/timestamps O
. O

section 16
id pdf2json/2021.acl-long.520.pdf.json
in O
our O
experiments O
, O
bert O
performs O
slightly O
better O
than O
knowbert O
, O
even O
though O
knowbert O
has O
entity O
knowledge O
in O
its O
parameters O
. O

section 16
id pdf2json/2021.acl-long.520.pdf.json
t5-3b O
performs O
the O
best O
among O
the O
lms O
we O
tested O
, O
possibly O
because O
of O
the O
large O
number O
of O
parameters O
and O
pre-training O
. O

section 16
id pdf2json/2021.acl-long.520.pdf.json
even O
among O
methods O
that O
use O
kg O
embeddings O
, O
cronkgqa O
performs O
the O
best O
on O
all O
metrics O
, O
followed O
by O
t-eae-replace O
. O

section 16
id pdf2json/2021.acl-long.520.pdf.json
since O
embedkgqa O
has O
non-temporal O
embeddings O
, O
its O
performance O
on O
questions O
where O
the O
answer O
is O
a O
time O
is O
very O
low O
— O
comparable O
to O
bert O
— O
which O
is O
the O
lm O
used O
in O
our O
embedkgqa O
implementation O
. O

section 16
id pdf2json/2021.acl-long.520.pdf.json
another O
interesting O
thing O
to O
note O
is O
the O
performance O
on O
simple O
reasoning O
questions O
. O

section 16
id pdf2json/2021.acl-long.520.pdf.json
cronkgqa O
far O
outperforms O
baselines O
for O
simple O
questions O
, O
achieving O
close O
to O
0.99 O
hits O
@ O
1 O
, O
which O
is O
much O
lower O
for O
t-eae O
( O
0.329 O
) O
. O

section 16
id pdf2json/2021.acl-long.520.pdf.json
we O
believe O
there O
might O
be O
a O
few O
reasons O
that O
contribute O
to O
this O
: O
1 O
. O

section 16
id pdf2json/2021.acl-long.520.pdf.json
there O
is O
the O
inductive O
bias O
of O
combining O
em- O
beddings O
using O
tcomplex O
scoring O
function O
in O
cronkgqa O
, O
which O
is O
the O
same O
one O
used O
in O
creating O
the O
entity O
and O
time O
embeddings O
, O
thus O
making O
the O
simple O
questions O
straightforward O
to O
answer O
. O

section 16
id pdf2json/2021.acl-long.520.pdf.json
however O
, O
not O
relying O
on O
a O
scoring O
function O
means O
that O
t-eae O
can O
be O
extended O
to O
any O
kg O
embedding O
, O
whereas O
cronkgqa O
can O
not O
. O

section 16
id pdf2json/2021.acl-long.520.pdf.json
2 O
. O

section 16
id pdf2json/2021.acl-long.520.pdf.json
another O
contributing O
reason O
could O
be O
that O
there O
are O
fewer O
parameters O
to O
be O
trained O
in O
cronkgqa O
while O
a O
6-layer O
transformer O
encoder O
needs O
to O
be O
trained O
from O
scratch O
in O
t-eae O
. O

section 16
id pdf2json/2021.acl-long.520.pdf.json
transformers O
typically O
require O
large O
amounts O
of O
varied O
data O
to O
train O
successfully O
. O

section 17
id pdf2json/2021.acl-long.520.pdf.json
table O
6 O
shows O
the O
performance O
of O
kg O
embedding O
based O
models O
across O
different O
types O
of O
reasoning O
. O

section 17
id pdf2json/2021.acl-long.520.pdf.json
as O
stated O
above O
in O
section O
6.2 O
, O
cronkgqa O
performs O
very O
well O
on O
simple O
reasoning O
questions O
( O
simple O
entity O
, O
simple O
time O
) O
. O

section 17
id pdf2json/2021.acl-long.520.pdf.json
among O
complex O
question O
types O
, O
all O
models O
( O
except O
embedkgqa O
) O
perform O
the O
best O
on O
time O
join O
questions O
( O
e.g. O
, O
‘ O
who O
played O
with O
roberto O
dinamite O
on O
the O
brazil O
national O
football O
team O
’ O
) O
. O

section 17
id pdf2json/2021.acl-long.520.pdf.json
this O
is O
because O
such O
questions O
typically O
have O
multiple O
answers O
( O
such O
as O
all O
the O
players O
when O
roberto O
dinamite O
was O
playing O
for O
brazil O
) O
, O
which O
makes O
it O
easier O
for O
the O
model O
to O
make O
a O
correct O
prediction O
. O

section 17
id pdf2json/2021.acl-long.520.pdf.json
in O
the O
other O
two O
question O
types O
, O
the O
answer O
is O
always O
a O
single O
entity/time O
. O

section 17
id pdf2json/2021.acl-long.520.pdf.json
before/after O
questions O
seem O
most O
challenging O
for O
all O
methods O
, O
with O
the O
best O
method O
achieving O
only O
0.288 O
hits O
@ O
1 O
. O

section 18
id pdf2json/2021.acl-long.520.pdf.json
figure O
2 O
shows O
the O
effect O
of O
training O
dataset O
size O
on O
model O
performance O
. O

section 18
id pdf2json/2021.acl-long.520.pdf.json
as O
we O
can O
see O
, O
for O
t-eae-add O
, O
increasing O
the O
training O
dataset O
size O
from O
10 O
% O
to O
100 O
% O
steadily O
increases O
its O
performance O
for O
both O
simple O
and O
complex O
reasoning O
type O
questions O
. O

section 18
id pdf2json/2021.acl-long.520.pdf.json
this O
effect O
is O
somewhat O
present O
in O
cronkgqa O
for O
complex O
reasoning O
, O
but O
not O
so O
for O
simple O
reasoning O
type O
questions O
. O

section 18
id pdf2json/2021.acl-long.520.pdf.json
we O
hypothesize O
that O
this O
is O
because O
t-eae O
has O
more O
trainable O
parameters O
— O
it O
has O
a O
6-layer O
transformer O
that O
needs O
to O
be O
trained O
from O
scratch O
— O
in O
contrast O
to O
cronkgqa O
that O
needs O
to O
merely O
fine O
tune O
bert O
and O
train O
some O
shallow O
projection O
layers O
. O

section 18
id pdf2json/2021.acl-long.520.pdf.json
these O
results O
affirm O
our O
hypothesis O
that O
having O
a O
large O
, O
even O
if O
synthetic O
, O
dataset O
is O
useful O
for O
training O
temporal O
reasoning O
models O
. O

section 19
id pdf2json/2021.acl-long.520.pdf.json
we O
conducted O
further O
experiments O
to O
study O
the O
effect O
of O
temporal O
vs. O
non-temporal O
kg O
embeddings O
. O

section 19
id pdf2json/2021.acl-long.520.pdf.json
we O
replaced O
the O
temporal O
entity O
embeddings O
in O
t-eae-replace O
with O
complex O
embeddings O
, O
and O
treated O
timestamps O
as O
regular O
tokens O
( O
not O
associated O
with O
any O
entity/time O
mentions O
) O
. O

section 19
id pdf2json/2021.acl-long.520.pdf.json
cronkgqacx O
is O
the O
same O
as O
embedkgqa O
. O

section 19
id pdf2json/2021.acl-long.520.pdf.json
the O
results O
can O
be O
seen O
in O
table O
7 O
. O

section 19
id pdf2json/2021.acl-long.520.pdf.json
as O
we O
can O
see O
, O
for O
both O
cronkgqa O
and O
t-eae-replace O
, O
using O
temporal O
kge O
( O
tcomplex O
) O
gives O
a O
significant O
boost O
in O
performance O
compared O
to O
non-temporal O
kge O
( O
complex O
) O
. O

section 19
id pdf2json/2021.acl-long.520.pdf.json
cronkgqa O
receives O
a O
much O
larger O
boost O
in O
performance O
compared O
to O
t-eae-replace O
, O
probably O
because O
the O
scoring O
function O
has O
been O
modeled O
after O
tcomplex O
and O
not O
complex O
, O
while O
there O
is O
no O
such O
embedding-specific O
engineering O
in O
teae-replace O
. O

section 19
id pdf2json/2021.acl-long.520.pdf.json
another O
observation O
is O
that O
questions O
having O
temporal O
answers O
achieve O
very O
low O
accuracy O
( O
0.057 O
and O
0.062 O
respectively O
) O
in O
both O
cronkgqa-cx O
and O
t-eae-replace-cx O
, O
which O
is O
much O
lower O
than O
what O
these O
models O
achieve O
with O
tcomplex O
. O

section 19
id pdf2json/2021.acl-long.520.pdf.json
this O
shows O
that O
having O
temporal O
kg O
embeddings O
is O
essential O
for O
achieving O
good O
performance O
for O
kg O
embedding-based O
methods O
. O

section 20
id pdf2json/2021.acl-long.520.pdf.json
in O
this O
paper O
we O
introduce O
cronquestions O
, O
a O
new O
dataset O
for O
temporal O
knowledge O
graph O
question O
answering O
. O

section 20
id pdf2json/2021.acl-long.520.pdf.json
while O
there O
exist O
some O
temporal O
kgqa O
datasets O
, O
they O
are O
all O
based O
on O
non-temporal O
kgs O
( O
e.g. O
, O
freebase O
) O
and O
have O
relatively O
few O
questions O
. O

section 20
id pdf2json/2021.acl-long.520.pdf.json
our O
dataset O
consists O
of O
both O
a O
temporal O
kg O
as O
well O
as O
a O
large O
set O
of O
temporal O
questions O
requiring O
various O
structures O
of O
reasoning O
. O

section 20
id pdf2json/2021.acl-long.520.pdf.json
in O
order O
to O
develop O
such O
a O
large O
dataset O
, O
we O
used O
a O
synthetic O
generation O
procedure O
, O
leading O
to O
a O
question O
distribution O
that O
is O
artificial O
from O
a O
semantic O
perspective O
. O

section 20
id pdf2json/2021.acl-long.520.pdf.json
however O
, O
having O
a O
large O
dataset O
provides O
an O
opportunity O
to O
train O
models O
, O
rather O
than O
just O
evaluate O
them O
. O

section 20
id pdf2json/2021.acl-long.520.pdf.json
we O
experimentally O
show O
that O
increasing O
the O
training O
dataset O
size O
steadily O
improves O
the O
performance O
of O
certain O
methods O
on O
the O
tkgqa O
task O
. O

section 20
id pdf2json/2021.acl-long.520.pdf.json
we O
first O
apply O
large O
pre-trained O
lm O
based O
qa O
methods O
on O
our O
new O
dataset O
. O

section 20
id pdf2json/2021.acl-long.520.pdf.json
then O
we O
inject O
kg O
embeddings O
, O
both O
temporal O
and O
non-temporal O
, O
into O
these O
lms O
and O
observe O
significant O
improvement O
in O
performance O
. O

section 20
id pdf2json/2021.acl-long.520.pdf.json
we O
also O
propose O
a O
new O
method O
, O
cronkgqa O
, O
that O
is O
able O
to O
leverage O
temporal O
kg O
embeddings O
to O
perform O
tkgqa O
. O

section 20
id pdf2json/2021.acl-long.520.pdf.json
in O
our O
experiments O
, O
cronkgqa O
outperforms O
all O
baselines O
. O

section 20
id pdf2json/2021.acl-long.520.pdf.json
these O
results O
suggest O
that O
kg O
embeddings O
can O
be O
effectively O
used O
to O
perform O
temporal O
kgqa O
, O
although O
there O
remains O
significant O
scope O
for O
improvement O
when O
it O
comes O
to O
complex O
reasoning O
questions O
. O

section 21
id pdf2json/2021.acl-long.520.pdf.json
we O
would O
like O
to O
thank O
the O
anonymous O
reviewers O
for O
their O
constructive O
feedback O
, O
and O
pat O
verga O
and O
william O
cohen O
from O
google O
research O
for O
their O
insightful O
comments O
. O

section 21
id pdf2json/2021.acl-long.520.pdf.json
we O
would O
also O
like O
to O
thank O
chitrank O
gupta O
( O
iit O
bombay O
) O
for O
his O
help O
in O
debugging O
the O
source O
code O
and O
dataset O
. O

section 21
id pdf2json/2021.acl-long.520.pdf.json
this O
work O
is O
supported O
in O
part O
by O
a O
gift O
from O
google O
research O
, O
india O
and O
a O
jagadish O
bose O
fellowship O
. O

section 22
id pdf2json/2021.acl-long.520.pdf.json
a.1 O
entities O
as O
experts O
( O
eae O
) O
the O
model O
architecture O
follows O
transformer O
( O
vaswani O
et O
al. O
, O
2017 O
) O
interleaved O
with O
an O
entity O
memory O
layer O
. O

section 22
id pdf2json/2021.acl-long.520.pdf.json
it O
has O
two O
embedding O
matrices O
, O
for O
tokens O
and O
entities O
. O

section 22
id pdf2json/2021.acl-long.520.pdf.json
it O
works O
on O
the O
input O
sequence O
x O
as O
follows O
. O

section 22
id pdf2json/2021.acl-long.520.pdf.json
x0 O
= O
tokenembed O
( O
x O
) O
x1 O
= O
transformer0 O
( O
x O
0 O
, O
num O
layers O
= O
l0 O
) O
x2 O
= O
entitymemory O
( O
x1 O
) O
x3 O
= O
layernorm O
( O
x2 O
+x1 O
) O
x4 O
= O
transformer1 O
( O
x O
3 O
, O
num O
layers O
= O
l1 O
) O
x5 O
= O
taskspecificheads O
( O
x4 O
) O
( O
6 O
) O
the O
whole O
model O
( O
transformers O
, O
token O
and O
entity O
embeddings O
, O
and O
task-specific O
heads O
) O
is O
trained O
end O
to O
end O
using O
losses O
for O
entity O
linking O
, O
mention O
detection O
and O
masked O
language O
modeling O
. O

section 22
id pdf2json/2021.acl-long.520.pdf.json
a.2 O
eae O
for O
temporal O
kgqa O
cronquestions O
does O
not O
provide O
a O
text O
corpus O
for O
training O
language O
models O
. O

section 22
id pdf2json/2021.acl-long.520.pdf.json
therefore O
, O
we O
use O
bert O
( O
devlin O
et O
al. O
, O
2019 O
) O
for O
transformer0 O
as O
well O
as O
tokenembed O
( O
eqn O
. O

section 22
id pdf2json/2021.acl-long.520.pdf.json
6 O
) O
. O

section 22
id pdf2json/2021.acl-long.520.pdf.json
for O
entitymemory O
, O
we O
use O
tcomplex/timeplex O
embeddings O
of O
entities O
and O
timestamps O
that O
have O
been O
pre-trained O
using O
the O
cronquestions O
kg O
( O
please O
refer O
to O
section O
4 O
for O
details O
on O
kg O
embeddings O
) O
. O

section 22
id pdf2json/2021.acl-long.520.pdf.json
the O
modified O
model O
is O
as O
follows O
: O
x1 O
= O
bert O
( O
x O
) O
x2 O
= O
entitytimeembedding O
( O
x1 O
) O
x3 O
= O
layernorm O
( O
x2 O
+x1 O
) O
x4 O
= O
transformer1 O
( O
x O
3 O
, O
num O
layers O
= O
6 O
) O
x5 O
= O
predictionhead O
( O
x4 O
) O
( O
7 O
) O
for O
simplicity O
, O
we O
assume O
that O
all O
grounded O
entity O
and O
time O
mention O
spans O
are O
marked O
in O
the O
question O
, O
i.e. O
, O
for O
each O
token O
, O
we O
know O
. O

section 22
id pdf2json/2021.acl-long.520.pdf.json
which O
entity O
or O
timestamp O
it O
belongs O
to O
( O
or O
if O
it O
doesn O
’ O
t O
belong O
to O
any O
) O
. O

section 22
id pdf2json/2021.acl-long.520.pdf.json
thus O
, O
for O
each O
token O
xi O
in O
the O
input O
x O
, O
• O
x1 O
[ O
i O
] O
contains O
the O
contextual O
bert O
embedding O
of O
xi O
• O
for O
x2 O
[ O
i O
] O
there O
are O
3 O
cases O
. O

section 22
id pdf2json/2021.acl-long.520.pdf.json
– O
xi O
is O
a O
mention O
of O
entity O
e. O
then O
x2 O
[ O
i O
] O
= O
e O
[ O
e O
] O
. O

section 22
id pdf2json/2021.acl-long.520.pdf.json
– O
xi O
is O
a O
mention O
of O
timestamp O
t. O
then O
x2 O
[ O
i O
] O
= O
t O
[ O
t O
] O
. O

section 22
id pdf2json/2021.acl-long.520.pdf.json
– O
xi O
is O
not O
a O
mention O
. O

section 22
id pdf2json/2021.acl-long.520.pdf.json
then O
x2 O
[ O
i O
] O
is O
the O
zero O
vector O
. O

section 22
id pdf2json/2021.acl-long.520.pdf.json
predictionhead O
takes O
the O
final O
output O
from O
transformer1 O
of O
the O
token O
corresponding O
to O
the O
[ O
cls O
] O
token O
of O
bert O
as O
the O
predicted O
answer O
embedding O
. O

section 22
id pdf2json/2021.acl-long.520.pdf.json
this O
answer O
embedding O
is O
scored O
against O
e O
.t O
using O
dot O
product O
to O
get O
a O
score O
for O
each O
possible O
answer O
, O
and O
softmax O
is O
taken O
to O
get O
answer O
probabilities O
. O

section 22
id pdf2json/2021.acl-long.520.pdf.json
the O
model O
is O
trained O
on O
the O
qa O
dataset O
using O
cross-entropy O
loss O
. O

section 22
id pdf2json/2021.acl-long.520.pdf.json
we O
will O
refer O
to O
this O
model O
as O
t-eae-add O
since O
we O
are O
taking O
element-wise O
sum O
of O
bert O
and O
entity/time O
embeddings O
. O

section 22
id pdf2json/2021.acl-long.520.pdf.json
t-eae-replace O
instead O
of O
adding O
entity/time O
and O
bert O
embeddings O
, O
we O
replace O
the O
bert O
embeddings O
with O
the O
entity/time O
embeddings O
for O
entity/time O
mentions O
. O

section 22
id pdf2json/2021.acl-long.520.pdf.json
specifically O
, O
before O
feeding O
to O
transformer1 O
in O
step O
4 O
of O
eqn O
. O

section 22
id pdf2json/2021.acl-long.520.pdf.json
7 O
, O
1. O
if O
xi O
is O
not O
an O
entity O
or O
time O
mention O
, O
x3 O
[ O
i O
] O
= O
bert O
( O
x1 O
[ O
i O
] O
) O
2. O
if O
xi O
is O
an O
entity O
or O
time O
mention O
, O
x3 O
[ O
i O
] O
= O
entitytimeembedding O
( O
x1 O
[ O
i O
] O
) O
the O
rest O
of O
the O
model O
remains O
the O
same O
. O

section 22
id pdf2json/2021.acl-long.520.pdf.json
a.3 O
examples O
tables O
8 O
to O
12 O
contain O
some O
example O
questions O
from O
the O
validation O
set O
of O
cronquestions O
, O
along O
with O
the O
top O
5 O
predictions O
of O
the O
models O
we O
experimented O
with O
. O

section 22
id pdf2json/2021.acl-long.520.pdf.json
t5-3b O
has O
a O
single O
prediction O
since O
it O
is O
a O
text-to-text O
model O
. O

section 22
id pdf2json/2021.acl-long.520.pdf.json
question O
who O
held O
the O
position O
of O
prime O
minister O
of O
sweden O
before O
2nd O
world O
war O
question O
type O
before/after O
gold O
answer O
( O
s O
) O
per O
albin O
hansson O
bert O
emil O
stang O
, O
sr. O
, O
sigurd O
ibsen O
, O
johan O
nygaardsvold O
, O
laila O
freivalds O
, O
j. O
s. O
woodsworth O
knowbert O
benito O
mussolini O
, O
östen O
undén O
, O
hans-dietrich O
genscher O
, O
winston O
churchill O
, O
lutz O
graf O
schwerin O
von O
krosigk O
t5-3b O
bo O
osten O
unden O
embedkgqa O
per O
albin O
hansson O
, O
tage O
erlander O
, O
carl O
gustaf O
ekman O
, O
arvid O
lindman O
, O
hjalmar O
branting O
t-eae-add O
per O
albin O
hansson O
, O
manuel O
roxas O
, O
arthur O
sauvé O
, O
konstantinos O
demertzis O
, O
karl O
renner O
t-eae-replace O
per O
albin O
hansson O
, O
tage O
erlander O
, O
arvid O
lindman O
, O
valère O
bernard O
, O
vladko O
maček O
cronkgqa O
per O
albin O
hansson O
, O
tage O
erlander O
, O
arvid O
lindman O
, O
carl O
gustaf O
ekman O
, O
hjalmar O
branting O
question O
who O
did O
john O
alan O
lasseter O
work O
with O
while O
employed O
at O
pixar O
question O
type O
time O
join O
gold O
answer O
( O
s O
) O
floyd O
norman O
bert O
tim O
cook O
, O
eleanor O
winsor O
leach O
, O
david O
r. O
williams O
, O
robert O
m. O
boynton O
, O
jules O
steeg O
knowbert O
1994 O
, O
1997 O
, O
walt O
disney O
animation O
studios O
, O
christiane O
kubrick O
, O
1989 O
t5-3b O
john O
alan O
lasseter O
embedkgqa O
john O
lasseter O
, O
floyd O
norman O
, O
duncan O
marjoribanks O
, O
glen O
keane O
, O
theodore O
ty O
t-eae-add O
john O
lasseter O
, O
anne O
marie O
bardwell O
, O
will O
finn O
, O
floyd O
norman O
, O
rejean O
bourdages O
t-eae-replace O
john O
lasseter O
, O
will O
finn O
, O
floyd O
norman O
, O
nik O
ranieri O
, O
ken O
duncan O
cronkgqa O
john O
lasseter O
, O
floyd O
norman O
, O
duncan O
marjoribanks O
, O
david O
pruiksma O
, O
theodore O
ty O
table O
10 O
: O
time O
join O
type O
question O
. O

section 22
id pdf2json/2021.acl-long.520.pdf.json
question O
where O
did O
john O
hubley O
work O
before O
working O
for O
industrial O
films O
question O
type O
before/after O
gold O
answer O
( O
s O
) O
the O
walt O
disney O
studios O
bert O
the O
walt O
disney O
studios O
, O
warner O
bros. O
cartoons O
, O
pixar O
, O
microsoft O
, O
united O
states O
navy O
knowbert O
école O
polytechnique O
, O
pitié-salpêtrière O
hospital O
, O
the O
walt O
disney O
studios O
, O
elisabeth O
buddenbrook O
, O
yale O
university O
t5-3b O
london O
film O
school O
embedkgqa O
the O
walt O
disney O
studios O
, O
collège O
de O
france O
, O
warner O
bros. O
cartoons O
, O
university O
of O
naples O
federico O
ii O
, O
eth O
zurich O
t-eae-add O
the O
walt O
disney O
studios O
, O
fleischer O
studios O
, O
upa O
, O
walter O
lantz O
productions O
, O
wellesley O
college O
t-eae-replace O
the O
walt O
disney O
studios O
, O
city O
college O
of O
new O
york O
, O
upa O
, O
yale O
university O
, O
indiana O
university O
cronkgqa O
the O
walt O
disney O
studios O
, O
upa O
, O
saint O
petersburg O
state O
university O
, O
warner O
bros. O
cartoons O
, O
collège O
de O
france O
table O
11 O
: O
before/after O
reasoning O
type O
question O
. O

section 22
id pdf2json/2021.acl-long.520.pdf.json
question O
the O
last O
person O
that O
naomi O
foner O
gyllenhaal O
was O
married O
to O
was O
question O
type O
first/last O
gold O
answer O
( O
s O
) O
stephen O
gyllenhaal O
bert O
1928 O
, O
jennifer O
lash O
, O
stephen O
mallory O
, O
martin O
landau O
, O
bayerische O
verfassungsmedaille O
in O
gold O
knowbert O
nadia O
benois O
, O
eugenia O
zukerman O
, O
germany O
national O
football O
team O
, O
talulah O
riley O
, O
lola O
landau O
t5-3b O
gyllenhaal O
embedkgqa O
stephen O
gyllenhaal O
, O
naomi O
foner O
gyllenhaal O
, O
wolfhard O
von O
boeselager O
, O
heinrich O
schweiger O
, O
bruce O
paltrow O
t-eae-add O
stephen O
gyllenhaal O
, O
marianne O
zoff O
, O
cotter O
smith O
, O
douglas O
wilder O
, O
gerd O
vespermann O
t-eae-replace O
stephen O
gyllenhaal O
, O
hetty O
broedelet-henkes O
, O
naomi O
foner O
gyllenhaal O
, O
miles O
copeland O
, O
jr. O
, O
member O
of O
the O
chamber O
of O
representatives O
of O
colombia O
cronkgqa O
stephen O
gyllenhaal O
, O
antonia O
fraser O
, O
bruce O
paltrow O
, O
naomi O
foner O
gyllenhaal O
, O
wolfhard O
von O
boeselager O
table O
12 O
: O
first/last O
reasoning O
type O
question O
. O

section TITLE
id pdf2json/2021.acl-long.154.pdf.json
uxla O
: O
a O
robust O
unsupervised O
data O
augmentation O
framework O
for O
zero-resource O
cross-lingual O
nlp O

section ABSTRACT
id pdf2json/2021.acl-long.154.pdf.json
transfer O
learning O
has O
yielded O
state-of-the-art O
( O
sota O
) O
results O
in O
many O
supervised O
nlp O
tasks O
. O

section ABSTRACT
id pdf2json/2021.acl-long.154.pdf.json
however O
, O
annotated O
data O
for O
every O
target O
task O
in O
every O
target O
language O
is O
rare O
, O
especially O
for O
low-resource O
languages O
. O

section ABSTRACT
id pdf2json/2021.acl-long.154.pdf.json
we O
propose O
uxla O
a O
novel O
unsupervised O
data O
augmentation O
framework O
for O
zero-resource O
transfer O
learning O
scenarios O
. O

section ABSTRACT
id pdf2json/2021.acl-long.154.pdf.json
in O
particular O
, O
uxla O
aims O
to O
solve O
crosslingual O
adaptation O
problems O
from O
a O
source O
language O
task O
distribution O
to O
an O
unknown O
target O
language O
task O
distribution O
, O
assuming O
no O
training O
label O
in O
the O
target O
language O
. O

section ABSTRACT
id pdf2json/2021.acl-long.154.pdf.json
at O
its O
core O
, O
uxla O
performs O
simultaneous O
selftraining O
with O
data O
augmentation O
and O
unsupervised O
sample O
selection O
. O

section ABSTRACT
id pdf2json/2021.acl-long.154.pdf.json
to O
show O
its O
effectiveness O
, O
we O
conduct O
extensive O
experiments O
on O
three O
diverse O
zero-resource O
cross-lingual O
transfer O
tasks O
. O

section ABSTRACT
id pdf2json/2021.acl-long.154.pdf.json
uxla O
achieves O
sota O
results O
in O
all O
the O
tasks O
, O
outperforming O
the O
baselines O
by O
a O
good O
margin O
. O

section ABSTRACT
id pdf2json/2021.acl-long.154.pdf.json
with O
an O
in-depth O
framework O
dissection O
, O
we O
demonstrate O
the O
cumulative O
contributions O
of O
different O
components O
to O
its O
success O
. O

section 0
id pdf2json/2021.acl-long.154.pdf.json
proceedings O
of O
the O
59th O
annual O
meeting O
of O
the O
association O
for O
computational O
linguistics O
and O
the O
11th O
international O
joint O
conference O
on O
natural O
language O
processing O
, O
pages O
1978–1992 O
august O
1–6 O
, O
2021 O
. O

section 0
id pdf2json/2021.acl-long.154.pdf.json
©2021 O
association O
for O
computational O
linguistics O
1978 O

section 1
id pdf2json/2021.acl-long.154.pdf.json
self-supervised O
learning O
in O
the O
form O
of O
pretrained O
language O
models O
( O
lm O
) O
has O
been O
the O
driving O
force O
in O
developing O
state-of-the-art O
nlp O
systems O
in O
recent O
years O
. O

section 1
id pdf2json/2021.acl-long.154.pdf.json
these O
methods O
typically O
follow O
two O
basic O
steps O
, O
where O
a O
supervised O
task-specific O
finetuning O
follows O
a O
large-scale O
lm O
pretraining O
( O
radford O
et O
al. O
, O
2019 O
) O
. O

section 1
id pdf2json/2021.acl-long.154.pdf.json
however O
, O
getting O
labeled O
data O
for O
every O
target O
task O
in O
every O
target O
language O
is O
difficult O
, O
especially O
for O
low-resource O
languages O
. O

section 1
id pdf2json/2021.acl-long.154.pdf.json
recently O
, O
the O
pretrain-finetune O
paradigm O
has O
also O
been O
extended O
to O
multi-lingual O
setups O
to O
train O
effective O
multi-lingual O
models O
that O
can O
be O
used O
for O
zero-shot O
cross-lingual O
transfer O
. O

section 1
id pdf2json/2021.acl-long.154.pdf.json
jointly O
trained O
deep O
multi-lingual O
lms O
like O
mbert O
( O
devlin O
et O
al. O
, O
2019 O
) O
and O
xlm-r O
( O
conneau O
et O
al. O
, O
2020 O
) O
coupled O
∗equal O
contribution O
with O
supervised O
fine-tuning O
in O
the O
source O
language O
have O
been O
quite O
successful O
in O
transferring O
linguistic O
and O
task O
knowledge O
from O
one O
language O
to O
another O
without O
using O
any O
task O
label O
in O
the O
target O
language O
. O

section 1
id pdf2json/2021.acl-long.154.pdf.json
the O
joint O
pretraining O
with O
multiple O
languages O
allows O
these O
models O
to O
generalize O
across O
languages O
. O

section 1
id pdf2json/2021.acl-long.154.pdf.json
despite O
their O
effectiveness O
, O
recent O
studies O
( O
pires O
et O
al. O
, O
2019 O
; O
k O
et O
al. O
, O
2020 O
) O
have O
also O
highlighted O
one O
crucial O
limiting O
factor O
for O
successful O
crosslingual O
transfer O
. O

section 1
id pdf2json/2021.acl-long.154.pdf.json
they O
all O
agree O
that O
the O
crosslingual O
generalization O
ability O
of O
the O
model O
is O
limited O
by O
the O
( O
lack O
of O
) O
structural O
similarity O
between O
the O
source O
and O
target O
languages O
. O

section 1
id pdf2json/2021.acl-long.154.pdf.json
for O
example O
, O
for O
transferring O
mbert O
from O
english O
, O
k O
et O
al O
. O

section 1
id pdf2json/2021.acl-long.154.pdf.json
( O
2020 O
) O
report O
about O
23.6 O
% O
accuracy O
drop O
in O
hindi O
( O
structurally O
dissimilar O
) O
compared O
to O
9 O
% O
drop O
in O
spanish O
( O
structurally O
similar O
) O
in O
cross-lingual O
natural O
language O
inference O
( O
xnli O
) O
. O

section 1
id pdf2json/2021.acl-long.154.pdf.json
the O
difficulty O
level O
of O
transfer O
is O
further O
exacerbated O
if O
the O
( O
dissimilar O
) O
target O
language O
is O
low-resourced O
, O
as O
the O
joint O
pretraining O
step O
may O
not O
have O
seen O
many O
instances O
from O
this O
language O
in O
the O
first O
place O
. O

section 1
id pdf2json/2021.acl-long.154.pdf.json
in O
our O
experiments O
( O
§3.2 O
) O
, O
in O
cross-lingual O
ner O
( O
xner O
) O
, O
we O
report O
f1 O
reductions O
of O
28.3 O
% O
in O
urdu O
and O
30.4 O
% O
in O
burmese O
for O
xlm-r O
, O
which O
is O
trained O
on O
a O
much O
larger O
multilingual O
dataset O
than O
mbert O
. O

section 1
id pdf2json/2021.acl-long.154.pdf.json
one O
attractive O
way O
to O
improve O
cross-lingual O
generalization O
is O
to O
perform O
data O
augmentation O
( O
simard O
et O
al. O
, O
1998 O
) O
, O
and O
train O
the O
model O
on O
examples O
that O
are O
similar O
but O
different O
from O
the O
labeled O
data O
in O
the O
source O
language O
. O

section 1
id pdf2json/2021.acl-long.154.pdf.json
formalized O
by O
the O
vicinal O
risk O
minimization O
( O
vrm O
) O
principle O
( O
chapelle O
et O
al. O
, O
2001 O
) O
, O
such O
data O
augmentation O
methods O
have O
shown O
impressive O
results O
in O
vision O
( O
zhang O
et O
al. O
, O
2018 O
; O
berthelot O
et O
al. O
, O
2019 O
) O
. O

section 1
id pdf2json/2021.acl-long.154.pdf.json
these O
methods O
enlarge O
the O
support O
of O
the O
training O
distribution O
by O
generating O
new O
data O
points O
from O
a O
vicinity O
distribution O
around O
each O
training O
example O
. O

section 1
id pdf2json/2021.acl-long.154.pdf.json
for O
images O
, O
the O
vicinity O
of O
a O
training O
image O
can O
be O
defined O
by O
a O
set O
of O
operations O
like O
rotation O
and O
scaling O
, O
or O
by O
linear O
mixtures O
of O
features O
and O
labels O
( O
zhang O
et O
al. O
, O
2018 O
) O
. O

section 1
id pdf2json/2021.acl-long.154.pdf.json
however O
, O
when O
it O
comes O
to O
text O
, O
such O
unsupervised O
augmentation O
methods O
have O
rarely O
been O
successful O
. O

section 1
id pdf2json/2021.acl-long.154.pdf.json
the O
main O
reason O
is O
that O
unlike O
images O
, O
linguistic O
units O
are O
discrete O
and O
a O
smooth O
change O
in O
their O
embeddings O
may O
not O
result O
in O
a O
plausible O
linguistic O
unit O
that O
has O
similar O
meanings O
. O

section 1
id pdf2json/2021.acl-long.154.pdf.json
in O
nlp O
, O
to O
the O
best O
of O
our O
knowledge O
, O
the O
most O
successful O
augmentation O
method O
has O
so O
far O
been O
back-translation O
( O
sennrich O
et O
al. O
, O
2016 O
) O
which O
paraphrases O
an O
input O
sentence O
through O
round-trip O
translation O
. O

section 1
id pdf2json/2021.acl-long.154.pdf.json
however O
, O
it O
requires O
parallel O
data O
to O
train O
effective O
machine O
translation O
systems O
, O
acquiring O
which O
can O
be O
more O
expensive O
for O
low-resource O
languages O
than O
annotating O
the O
target O
language O
data O
. O

section 1
id pdf2json/2021.acl-long.154.pdf.json
furthermore O
, O
back-translation O
is O
only O
applicable O
in O
a O
supervised O
setup O
and O
to O
tasks O
where O
it O
is O
possible O
to O
find O
the O
alignments O
between O
the O
original O
labeled O
entities O
and O
the O
back-translated O
entities O
, O
such O
as O
in O
question O
answering O
( O
yu O
et O
al. O
, O
2018 O
) O
. O

section 1
id pdf2json/2021.acl-long.154.pdf.json
other O
related O
work O
includes O
contextual O
augmentation O
( O
kobayashi O
, O
2018 O
) O
, O
conditional O
bert O
( O
wu O
et O
al. O
, O
2018 O
) O
and O
aug-bert O
( O
shi O
et O
al. O
, O
2019 O
) O
. O

section 1
id pdf2json/2021.acl-long.154.pdf.json
these O
methods O
use O
a O
constrained O
augmentation O
that O
alters O
a O
pretrained O
lm O
to O
a O
label-conditional O
lm O
for O
a O
specific O
task O
. O

section 1
id pdf2json/2021.acl-long.154.pdf.json
since O
they O
rely O
on O
labels O
, O
their O
application O
is O
limited O
by O
the O
availability O
of O
enough O
task O
labels O
. O

section 1
id pdf2json/2021.acl-long.154.pdf.json
in O
this O
work O
, O
we O
propose O
uxla O
, O
a O
robust O
unsupervised O
cross-lingual O
augmentation O
framework O
for O
improving O
cross-lingual O
generalization O
of O
multilingual O
lms O
. O

section 1
id pdf2json/2021.acl-long.154.pdf.json
uxla O
augments O
data O
from O
the O
unlabeled O
training O
examples O
in O
the O
target O
language O
as O
well O
as O
from O
the O
virtual O
input O
samples O
generated O
from O
the O
vicinity O
distribution O
of O
the O
source O
and O
target O
language O
sentences O
. O

section 1
id pdf2json/2021.acl-long.154.pdf.json
with O
the O
augmented O
data O
, O
it O
performs O
simultaneous O
self-learning O
with O
an O
effective O
distillation O
strategy O
to O
learn O
a O
strongly O
adapted O
cross-lingual O
model O
from O
noisy O
( O
pseudo O
) O
labels O
for O
the O
target O
language O
task O
. O

section 1
id pdf2json/2021.acl-long.154.pdf.json
we O
propose O
novel O
ways O
to O
generate O
virtual O
sentences O
using O
a O
multilingual O
masked O
lm O
( O
conneau O
et O
al. O
, O
2020 O
) O
, O
and O
get O
reliable O
task O
labels O
by O
simultaneous O
multilingual O
co-training O
. O

section 1
id pdf2json/2021.acl-long.154.pdf.json
this O
co-training O
employs O
a O
twostage O
co-distillation O
process O
to O
ensure O
robust O
transfer O
to O
dissimilar O
and/or O
low-resource O
languages O
. O

section 1
id pdf2json/2021.acl-long.154.pdf.json
we O
validate O
the O
effectiveness O
and O
robustness O
of O
uxla O
by O
performing O
extensive O
experiments O
on O
three O
diverse O
zero-resource O
cross-lingual O
transfer O
tasks–xner O
, O
xnli O
, O
and O
paws-x O
, O
which O
posit O
different O
sets O
of O
challenges O
, O
and O
across O
many O
( O
14 O
in O
total O
) O
language O
pairs O
comprising O
languages O
that O
are O
similar/dissimilar/low-resourced O
. O

section 1
id pdf2json/2021.acl-long.154.pdf.json
uxla O
yields O
impressive O
results O
on O
xner O
, O
setting O
sota O
in O
all O
tested O
languages O
outperforming O
the O
baselines O
by O
a O
good O
margin O
. O

section 1
id pdf2json/2021.acl-long.154.pdf.json
the O
relative O
gains O
for O
uxla O
are O
particularly O
higher O
for O
structurally O
dissimilar O
and/or O
low-resource O
languages O
: O
28.54 O
% O
, O
16.05 O
% O
, O
and O
9.25 O
% O
absolute O
improvements O
for O
urdu O
, O
burmese O
, O
and O
arabic O
, O
respectively O
. O

section 1
id pdf2json/2021.acl-long.154.pdf.json
for O
xnli O
, O
with O
only O
5 O
% O
labeled O
data O
in O
the O
source O
, O
it O
gets O
comparable O
results O
to O
the O
baseline O
that O
uses O
all O
the O
labeled O
data O
, O
and O
surpasses O
the O
standard O
baseline O
by O
2.55 O
% O
on O
average O
when O
it O
uses O
all O
the O
labeled O
data O
in O
the O
source O
. O

section 1
id pdf2json/2021.acl-long.154.pdf.json
we O
also O
have O
similar O
findings O
in O
paws-x O
. O

section 1
id pdf2json/2021.acl-long.154.pdf.json
we O
provide O
a O
comprehensive O
analysis O
of O
the O
factors O
that O
contribute O
to O
uxla O
’ O
s O
performance O
. O

section 1
id pdf2json/2021.acl-long.154.pdf.json
we O
open-source O
our O
framework O
at O
https O
: O
//ntunlpsg.github.io/project/uxla/ O
. O

section 2
id pdf2json/2021.acl-long.154.pdf.json
while O
recent O
cross-lingual O
transfer O
learning O
efforts O
have O
relied O
almost O
exclusively O
on O
multi-lingual O
pretraining O
and O
zero-shot O
transfer O
of O
a O
fine-tuned O
source O
model O
, O
we O
believe O
there O
is O
a O
great O
potential O
for O
more O
elaborate O
methods O
that O
can O
leverage O
the O
unlabeled O
data O
better O
. O

section 2
id pdf2json/2021.acl-long.154.pdf.json
motivated O
by O
this O
, O
we O
present O
uxla O
, O
our O
unsupervised O
data O
augmentation O
framework O
for O
zero-resource O
cross-lingual O
task O
adaptation O
. O

section 2
id pdf2json/2021.acl-long.154.pdf.json
figure O
1 O
gives O
an O
overview O
of O
uxla O
. O

section 2
id pdf2json/2021.acl-long.154.pdf.json
let O
ds O
= O
( O
xs O
, O
ys O
) O
and O
dt O
= O
( O
xt O
) O
denote O
the O
training O
data O
for O
a O
source O
language O
s O
and O
a O
target O
language O
t O
, O
respectively O
. O

section 2
id pdf2json/2021.acl-long.154.pdf.json
uxla O
augments O
data O
from O
various O
origins O
at O
different O
stages O
of O
training O
. O

section 2
id pdf2json/2021.acl-long.154.pdf.json
in O
the O
initial O
stage O
( O
epoch O
1 O
) O
, O
it O
uses O
the O
augmented O
training O
samples O
from O
the O
target O
language O
( O
d′t O
) O
along O
with O
the O
original O
source O
( O
ds O
) O
. O

section 2
id pdf2json/2021.acl-long.154.pdf.json
in O
later O
stages O
( O
epoch O
2-3 O
) O
, O
it O
uses O
vicinal O
sentences O
generated O
from O
the O
vicinity O
distribution O
of O
source O
and O
target O
examples O
: O
ϑ O
( O
x̃sn|xsn O
) O
and O
ϑ O
( O
x̃tn|xtn O
) O
, O
where O
xsn O
∼ O
xs O
and O
xtn O
∼ O
xt O
. O

section 2
id pdf2json/2021.acl-long.154.pdf.json
it O
performs O
self-training O
on O
the O
augmented O
data O
to O
acquire O
the O
corresponding O
pseudo O
labels O
. O

section 2
id pdf2json/2021.acl-long.154.pdf.json
to O
avoid O
confirmation O
bias O
with O
self-training O
where O
the O
model O
accumulates O
its O
own O
errors O
, O
it O
simultaneously O
trains O
three O
task O
models O
to O
generate O
virtual O
training O
data O
through O
data O
augmentation O
and O
filtering O
of O
potential O
label O
noises O
via O
multi-epoch O
co-teaching O
( O
zhou O
and O
li O
, O
2005 O
) O
. O

section 2
id pdf2json/2021.acl-long.154.pdf.json
in O
each O
epoch O
, O
the O
co-teaching O
process O
first O
performs O
co-distillation O
, O
where O
two O
peer O
task O
models O
are O
used O
to O
select O
“ O
reliable O
” O
training O
examples O
to O
train O
the O
third O
model O
. O

section 2
id pdf2json/2021.acl-long.154.pdf.json
the O
selected O
samples O
with O
pseudo O
labels O
are O
then O
added O
to O
the O
target O
task O
model O
’ O
s O
training O
data O
by O
taking O
the O
agreement O
from O
the O
other O
two O
models O
, O
a O
process O
we O
refer O
to O
as O
coguessing O
. O

section 2
id pdf2json/2021.acl-long.154.pdf.json
the O
co-distillation O
and O
co-guessing O
mechanism O
ensure O
robustness O
of O
uxla O
to O
out-of-domain O
distributions O
that O
can O
occur O
in O
a O
multilingual O
setup O
, O
e.g. O
, O
due O
to O
a O
structurally O
dissimilar O
and/or O
lowresource O
target O
language O
. O

section 2
id pdf2json/2021.acl-long.154.pdf.json
algorithm O
1 O
gives O
a O
pseudocode O
of O
the O
overall O
training O
method O
. O

section 2
id pdf2json/2021.acl-long.154.pdf.json
each O
of O
the O
task O
models O
in O
uxla O
is O
an O
instance O
of O
xlm-r O
finetuned O
on O
the O
source O
language O
task O
( O
e.g. O
, O
english O
ner O
) O
, O
whereas O
the O
pretrained O
masked O
lm O
parameterized O
by O
θmlm O
( O
i.e. O
, O
before O
fine-tuning O
) O
is O
used O
to O
define O
the O
vicinity O
distribution O
ϑ O
( O
x̃n|xn O
, O
θmlm O
) O
around O
each O
selected O
example O
xn O
. O

section 2
id pdf2json/2021.acl-long.154.pdf.json
in O
the O
following O
, O
we O
describe O
the O
steps O
in O
algorithm O
1 O
. O

section 3
id pdf2json/2021.acl-long.154.pdf.json
we O
first O
train O
three O
instances O
of O
the O
xlm-r O
model O
( O
θ O
( O
1 O
) O
, O
θ O
( O
2 O
) O
, O
θ O
( O
3 O
) O
) O
with O
an O
additional O
task-specific O
linear O
layer O
on O
the O
source O
language O
( O
english O
) O
labeled O
data O
. O

section 3
id pdf2json/2021.acl-long.154.pdf.json
each O
model O
has O
the O
same O
architecture O
( O
xlmr O
large O
) O
but O
is O
initialized O
with O
different O
random O
seeds O
. O

section 3
id pdf2json/2021.acl-long.154.pdf.json
for O
token-level O
prediction O
tasks O
( O
e.g. O
, O
ner O
) O
, O
the O
token-level O
representations O
are O
fed O
into O
the O
classification O
layer O
, O
whereas O
for O
sentence-level O
tasks O
( O
e.g. O
, O
xnli O
) O
, O
the O
[ O
cls O
] O
representation O
is O
used O
as O
input O
to O
the O
classification O
layer O
. O

section 3
id pdf2json/2021.acl-long.154.pdf.json
training O
with O
confidence O
penalty O
our O
goal O
is O
to O
train O
the O
task O
models O
so O
that O
they O
can O
be O
used O
reliably O
for O
self-training O
on O
a O
target O
language O
that O
is O
potentially O
dissimilar O
and O
low-resourced O
. O

section 3
id pdf2json/2021.acl-long.154.pdf.json
in O
such O
situations O
, O
an O
overly O
confident O
( O
overfitted O
) O
model O
may O
produce O
more O
noisy O
pseudo O
labels O
, O
and O
the O
noise O
will O
then O
accumulate O
as O
the O
training O
progresses O
. O

section 3
id pdf2json/2021.acl-long.154.pdf.json
overly O
confident O
predictions O
may O
also O
im- O
pose O
difficulties O
on O
our O
distillation O
methods O
( O
§2.3 O
) O
in O
isolating O
good O
samples O
from O
noisy O
ones O
. O

section 3
id pdf2json/2021.acl-long.154.pdf.json
however O
, O
training O
with O
the O
standard O
cross-entropy O
( O
ce O
) O
loss O
may O
result O
in O
overfitted O
models O
that O
produce O
overly O
confident O
predictions O
( O
low O
entropy O
) O
, O
especially O
when O
the O
class O
distribution O
is O
not O
balanced O
. O

section 3
id pdf2json/2021.acl-long.154.pdf.json
we O
address O
this O
by O
adding O
a O
negative O
entropy O
term O
−h O
to O
the O
ce O
loss O
as O
follows O
. O

section 3
id pdf2json/2021.acl-long.154.pdf.json
l O
( O
θ O
) O
= O
c∑ O
c=1 O
[ O
− O
yc O
log O
pcθ O
( O
x O
) O
︸ O
︷︷ O
︸ O
ce O
+ O
pcθ O
( O
x O
) O
log O
p O
c O
θ O
( O
x O
) O
︸ O
︷︷ O
︸ O
−h O
] O
( O
1 O
) O
where O
x O
is O
the O
representation O
that O
goes O
to O
the O
output O
layer O
, O
and O
yc O
and O
pcθ O
( O
x O
) O
are O
respectively O
the O
ground O
truth O
label O
and O
model O
predictions O
with O
respect O
to O
class O
c. O
such O
regularizer O
of O
output O
distribution O
has O
been O
shown O
to O
be O
effective O
for O
training O
large O
models O
( O
pereyra O
et O
al. O
, O
2017 O
) O
. O

section 3
id pdf2json/2021.acl-long.154.pdf.json
we O
also O
report O
significant O
gains O
with O
confidence O
penalty O
in O
§3 O
. O

section 3
id pdf2json/2021.acl-long.154.pdf.json
appendix O
b O
shows O
visualizations O
on O
why O
confidence O
penalty O
is O
helpful O
for O
distillation O
. O

section 4
id pdf2json/2021.acl-long.154.pdf.json
our O
augmentated O
sentences O
come O
from O
two O
different O
sources O
: O
the O
original O
target O
language O
samples O
xt O
, O
and O
the O
virtual O
samples O
generated O
from O
the O
vicinity O
distribution O
of O
the O
source O
and O
target O
samples O
: O
ϑ O
( O
x̃sn|xsn O
, O
θmlm O
) O
and O
ϑ O
( O
x̃tn|xtn O
, O
θmlm O
) O
with O
xsn O
∼ O
xs O
and O
xtn O
∼ O
xt O
. O

section 4
id pdf2json/2021.acl-long.154.pdf.json
it O
has O
been O
shown O
that O
contextual O
lms O
pretrained O
on O
large-scale O
datasets O
capture O
useful O
linguistic O
features O
and O
can O
be O
used O
to O
generate O
fluent O
grammatical O
texts O
( O
hewitt O
and O
manning O
, O
2019 O
) O
. O

section 4
id pdf2json/2021.acl-long.154.pdf.json
we O
use O
xlm-r O
masked O
lm O
( O
conneau O
et O
al. O
, O
2020 O
) O
as O
our O
vicinity O
model O
θmlm O
, O
which O
is O
trained O
on O
massive O
multilingual O
corpora O
( O
2.5 O
tb O
of O
common-crawl O
data O
in O
100 O
languages O
) O
. O

section 4
id pdf2json/2021.acl-long.154.pdf.json
the O
algorithm O
1 O
uxla O
: O
a O
robust O
unsupervised O
data O
augmentation O
framework O
for O
cross-lingual O
nlp O
input O
: O
source O
( O
s O
) O
and O
target O
( O
t O
) O
language O
datasets O
: O
ds O
= O
( O
xs O
, O
ys O
) O
, O
dt O
= O
( O
xt O
) O
; O
task O
models O
: O
θ O
( O
1 O
) O
, O
θ O
( O
2 O
) O
, O
θ O
( O
3 O
) O
, O
pre-trained O
masked O
lm O
θmlm O
, O
mask O
ratio O
p O
, O
diversification O
factor O
δ O
, O
sampling O
factor O
α O
, O
and O
distillation O
factor O
η O
output O
: O
models O
trained O
on O
augmented O
data O
1 O
: O
θ O
( O
1 O
) O
, O
θ O
( O
2 O
) O
, O
θ O
( O
3 O
) O
= O
warmup O
( O
ds O
, O
θ O
( O
1 O
) O
, O
θ O
( O
2 O
) O
, O
θ O
( O
3 O
) O
) O
. O

section 4
id pdf2json/2021.acl-long.154.pdf.json
warm O
up O
with O
conf O
. O

section 4
id pdf2json/2021.acl-long.154.pdf.json
penalty O
. O

section 4
id pdf2json/2021.acl-long.154.pdf.json
2 O
: O
for O
e O
∈ O
[ O
1 O
: O
3 O
] O
do O
. O

section 4
id pdf2json/2021.acl-long.154.pdf.json
e O
denotes O
epoch O
. O

section 4
id pdf2json/2021.acl-long.154.pdf.json
3 O
: O
for O
k O
∈ O
{ O
1 O
, O
2 O
, O
3 O
} O
do O
4 O
: O
x O
( O
k O
) O
t O
, O
y O
( O
k O
) O
t O
= O
distil O
( O
xt O
, O
ηe O
, O
θ O
( O
k O
) O
) O
. O

section 4
id pdf2json/2021.acl-long.154.pdf.json
infer O
and O
select O
tgt O
training O
data O
for O
augmentation O
. O

section 4
id pdf2json/2021.acl-long.154.pdf.json
5 O
: O
for O
j O
∈ O
{ O
1 O
, O
2 O
, O
3 O
} O
do O
6 O
: O
if O
k O
== O
j O
then O
continue O
7 O
: O
/* O
source O
language O
data O
augmentation O
*/ O
8 O
: O
x̃s O
= O
gen-lm O
( O
xs O
, O
θmlm O
, O
p O
, O
δ O
) O
. O

section 4
id pdf2json/2021.acl-long.154.pdf.json
vicinal O
example O
generation O
. O

section 4
id pdf2json/2021.acl-long.154.pdf.json
9 O
: O
x O
( O
k O
) O
s O
, O
y O
( O
k O
) O
s O
= O
distil O
( O
x̃s O
, O
ηe O
, O
θ O
( O
k O
) O
) O
; O
x O
( O
j O
) O
s O
, O
y O
( O
j O
) O
s O
= O
distil O
( O
x̃s O
, O
ηe O
, O
θ O
( O
j O
) O
) O
10 O
: O
d̃s O
= O
agreement O
( O
d O
( O
k O
) O
s O
= O
( O
x O
( O
k O
) O
s O
, O
y O
( O
k O
) O
s O
) O
, O
d O
( O
j O
) O
s O
= O
( O
x O
( O
j O
) O
s O
, O
y O
( O
j O
) O
s O
) O
) O
11 O
: O
/* O
target O
language O
data O
augmentation O
( O
no O
vicinity O
) O
*/ O
12 O
: O
x O
( O
j O
) O
t O
, O
y O
( O
j O
) O
t O
= O
distil O
( O
xt O
, O
ηe O
, O
θ O
( O
j O
) O
) O
13 O
: O
d′t O
= O
agreement O
( O
d O
( O
k O
) O
t O
= O
( O
x O
( O
k O
) O
t O
, O
y O
( O
k O
) O
t O
) O
, O
d O
( O
j O
) O
t O
= O
( O
x O
( O
j O
) O
t O
, O
y O
( O
j O
) O
t O
) O
) O
. O

section 4
id pdf2json/2021.acl-long.154.pdf.json
see O
line O
4 O
14 O
: O
/* O
target O
language O
data O
augmentation O
*/ O
15 O
: O
x̃t O
= O
gen-lm O
( O
xt O
, O
θmlm O
, O
p O
, O
δ O
) O
. O

section 4
id pdf2json/2021.acl-long.154.pdf.json
vicinal O
example O
generation O
. O

section 4
id pdf2json/2021.acl-long.154.pdf.json
16 O
: O
x O
( O
k O
) O
t O
, O
y O
( O
k O
) O
t O
= O
distil O
( O
x̃t O
, O
ηe O
, O
θ O
( O
k O
) O
) O
; O
x O
( O
j O
) O
t O
, O
y O
( O
j O
) O
t O
= O
distil O
( O
x̃t O
, O
ηe O
, O
θ O
( O
j O
) O
) O
17 O
: O
d̃t O
= O
agreement O
( O
d O
( O
k O
) O
t O
= O
( O
x O
( O
k O
) O
t O
, O
y O
( O
k O
) O
t O
) O
, O
d O
( O
j O
) O
t O
= O
( O
x O
( O
j O
) O
t O
, O
y O
( O
j O
) O
t O
) O
) O
18 O
: O
/* O
train O
new O
models O
on O
augmented O
data O
*/ O
19 O
: O
for O
l O
∈ O
{ O
1 O
, O
2 O
, O
3 O
} O
do O
20 O
: O
if O
l O
6= O
j O
and O
l O
6= O
k O
then O
21 O
: O
with O
sampling O
factor O
α O
, O
train O
θ O
( O
l O
) O
on O
d O
, O
. O

section 4
id pdf2json/2021.acl-long.154.pdf.json
train O
progressively O
22 O
: O
where O
d O
= O
{ O
ds1 O
( O
e O
∈ O
{ O
1 O
, O
3 O
} O
) O
∪ O
d′t1 O
( O
e O
∈ O
{ O
1 O
, O
3 O
} O
) O
∪ O
d̃s1 O
( O
e O
= O
3 O
) O
∪ O
d̃t1 O
( O
e O
∈ O
{ O
2 O
, O
3 O
} O
) O
} O
23 O
: O
return O
{ O
θ O
( O
1 O
) O
, O
θ O
( O
2 O
) O
, O
θ O
( O
3 O
) O
} O
vicinity O
model O
is O
a O
disjoint O
pretrained O
entity O
whose O
parameters O
are O
not O
trained O
on O
any O
task O
objective O
. O

section 4
id pdf2json/2021.acl-long.154.pdf.json
in O
order O
to O
generate O
samples O
around O
each O
selected O
example O
, O
we O
first O
randomly O
choose O
p O
% O
of O
the O
input O
tokens O
. O

section 4
id pdf2json/2021.acl-long.154.pdf.json
then O
we O
successively O
( O
one O
at O
a O
time O
) O
mask O
one O
of O
the O
chosen O
tokens O
and O
ask O
xlmr O
masked O
lm O
to O
predict O
a O
token O
in O
that O
masked O
position O
, O
i.e. O
, O
compute O
ϑ O
( O
x̃m|x O
, O
θmlm O
) O
with O
m O
being O
the O
index O
of O
the O
masked O
token O
. O

section 4
id pdf2json/2021.acl-long.154.pdf.json
for O
a O
specific O
mask O
, O
we O
sample O
s O
candidate O
words O
from O
the O
output O
distribution O
, O
and O
generate O
novel O
sentences O
by O
following O
one O
of O
the O
two O
alternative O
approaches O
. O

section 4
id pdf2json/2021.acl-long.154.pdf.json
( O
i O
) O
successive O
max O
in O
this O
approach O
, O
we O
take O
the O
most O
probable O
output O
token O
( O
s O
= O
1 O
) O
at O
each O
prediction O
step O
, O
o∗m O
= O
argmaxo O
ϑ O
( O
x̃m O
= O
o|x O
, O
θmlm O
) O
. O

section 4
id pdf2json/2021.acl-long.154.pdf.json
a O
new O
sentence O
is O
constructed O
by O
p O
% O
newly O
generated O
tokens O
. O

section 4
id pdf2json/2021.acl-long.154.pdf.json
we O
generate O
δ O
( O
diversification O
factor O
) O
virtual O
samples O
for O
each O
original O
example O
x O
, O
by O
randomly O
masking O
p O
% O
tokens O
each O
time O
. O

section 4
id pdf2json/2021.acl-long.154.pdf.json
( O
ii O
) O
successive O
cross O
in O
this O
approach O
, O
we O
divide O
each O
original O
( O
multi-sentence O
) O
sample O
x O
into O
two O
parts O
and O
use O
successive O
max O
to O
create O
two O
sets O
of O
augmented O
samples O
of O
size O
δ1 O
and O
δ2 O
, O
respectively O
. O

section 4
id pdf2json/2021.acl-long.154.pdf.json
we O
then O
take O
the O
cross O
of O
these O
two O
sets O
to O
generate O
δ1 O
× O
δ2 O
augmented O
samples O
. O

section 4
id pdf2json/2021.acl-long.154.pdf.json
augmentation O
of O
sentences O
through O
successive O
max O
or O
cross O
is O
carried O
out O
within O
the O
gen-lm O
( O
generate O
via O
lm O
) O
module O
in O
algorithm O
1 O
. O

section 4
id pdf2json/2021.acl-long.154.pdf.json
for O
tasks O
involving O
a O
single O
sequence O
( O
e.g. O
, O
xner O
) O
, O
we O
directly O
use O
successive O
max O
. O

section 4
id pdf2json/2021.acl-long.154.pdf.json
pairwise O
tasks O
like O
xnli O
and O
paws-x O
have O
pairwise O
dependencies O
: O
dependencies O
between O
a O
premise O
and O
a O
hypothesis O
in O
xnli O
or O
dependencies O
between O
a O
sentence O
and O
its O
possible O
paraphrase O
in O
paws-x O
. O

section 4
id pdf2json/2021.acl-long.154.pdf.json
to O
model O
such O
dependencies O
, O
we O
use O
successive O
cross O
, O
which O
uses O
cross-product O
of O
two O
successive O
max O
applied O
independently O
to O
each O
component O
. O

section 5
id pdf2json/2021.acl-long.154.pdf.json
due O
to O
discrete O
nature O
of O
texts O
, O
vrm O
based O
augmentation O
methods O
that O
are O
successful O
for O
images O
such O
as O
mixmatch O
( O
berthelot O
et O
al. O
, O
2019 O
) O
that O
generates O
new O
samples O
and O
their O
labels O
as O
simple O
linear O
interpolation O
, O
have O
not O
been O
successful O
in O
nlp O
. O

section 5
id pdf2json/2021.acl-long.154.pdf.json
the O
meaning O
of O
a O
sentence O
can O
change O
entirely O
even O
with O
minor O
variations O
in O
the O
original O
sentence O
. O

section 5
id pdf2json/2021.acl-long.154.pdf.json
for O
example O
, O
consider O
the O
following O
example O
generated O
by O
our O
vicinity O
model O
. O

section 5
id pdf2json/2021.acl-long.154.pdf.json
original O
: O
eu O
rejects O
german O
call O
to O
boycott O
british O
lamb O
. O

section 5
id pdf2json/2021.acl-long.154.pdf.json
masked O
: O
< O
mask O
> O
rejects O
german O
call O
to O
boycott O
british O
lamb O
. O

section 5
id pdf2json/2021.acl-long.154.pdf.json
xlm-r O
: O
trump O
rejects O
german O
call O
to O
boycott O
british O
lamb O
. O

section 5
id pdf2json/2021.acl-long.154.pdf.json
here O
, O
eu O
is O
an O
organization O
whereas O
the O
newly O
predicted O
word O
trump O
is O
a O
person O
( O
different O
name O
type O
) O
. O

section 5
id pdf2json/2021.acl-long.154.pdf.json
therefore O
, O
we O
need O
to O
relabel O
the O
augmented O
sentences O
no O
matter O
whether O
the O
original O
sentence O
has O
labels O
( O
source O
) O
or O
not O
( O
target O
) O
. O

section 5
id pdf2json/2021.acl-long.154.pdf.json
however O
, O
the O
relabeling O
process O
can O
induce O
noise O
, O
especially O
for O
dissimilar/low-resource O
languages O
, O
since O
the O
base O
task O
model O
may O
not O
be O
adapted O
fully O
in O
the O
early O
training O
stages O
. O

section 5
id pdf2json/2021.acl-long.154.pdf.json
we O
propose O
a O
2-stage O
sample O
distillation O
process O
to O
filter O
out O
noisy O
augmented O
data O
. O

section 5
id pdf2json/2021.acl-long.154.pdf.json
stage O
1 O
: O
distillation O
by O
single-model O
the O
first O
stage O
of O
distillation O
involves O
predictions O
from O
a O
single O
model O
for O
which O
we O
propose O
two O
alternatives O
: O
( O
i O
) O
distillation O
by O
model O
confidence O
: O
in O
this O
approach O
, O
we O
select O
samples O
based O
on O
the O
model O
’ O
s O
prediction O
confidence O
. O

section 5
id pdf2json/2021.acl-long.154.pdf.json
this O
method O
is O
similar O
in O
spirit O
to O
the O
selection O
method O
proposed O
by O
ruder O
and O
plank O
( O
2018a O
) O
. O

section 5
id pdf2json/2021.acl-long.154.pdf.json
for O
sentence-level O
tasks O
( O
e.g. O
, O
xnli O
) O
, O
the O
model O
produces O
a O
single O
class O
distribution O
for O
each O
training O
example O
. O

section 5
id pdf2json/2021.acl-long.154.pdf.json
in O
this O
case O
, O
the O
model O
’ O
s O
confidence O
is O
computed O
by O
p∗ O
= O
maxc∈ O
{ O
1 O
... O
c O
} O
pcθ O
( O
x O
) O
. O

section 5
id pdf2json/2021.acl-long.154.pdf.json
for O
token-level O
sequence O
labeling O
tasks O
( O
e.g. O
, O
ner O
) O
, O
the O
model O
’ O
s O
confidence O
is O
computed O
by O
: O
p∗ O
= O
1 O
t O
∑t O
t=1 O
{ O
maxc∈ O
{ O
1 O
... O
c O
} O
p O
c O
θ O
( O
xt O
) O
} O
, O
where O
t O
is O
the O
length O
of O
the O
sequence O
. O

section 5
id pdf2json/2021.acl-long.154.pdf.json
the O
distillation O
is O
then O
done O
by O
selecting O
the O
top O
η O
% O
samples O
with O
the O
highest O
confidence O
scores O
. O

section 5
id pdf2json/2021.acl-long.154.pdf.json
( O
ii O
) O
sample O
distillation O
by O
clustering O
: O
we O
propose O
this O
method O
based O
on O
the O
finding O
that O
large O
neural O
models O
tend O
to O
learn O
good O
samples O
faster O
than O
noisy O
ones O
, O
leading O
to O
a O
lower O
loss O
for O
good O
samples O
and O
higher O
loss O
for O
noisy O
ones O
( O
han O
et O
al. O
, O
2018 O
; O
arazo O
et O
al. O
, O
2019 O
) O
. O

section 5
id pdf2json/2021.acl-long.154.pdf.json
we O
use O
a O
1d O
twocomponent O
gaussian O
mixture O
model O
( O
gmm O
) O
to O
model O
per-sample O
loss O
distribution O
and O
cluster O
the O
samples O
based O
on O
their O
goodness O
. O

section 5
id pdf2json/2021.acl-long.154.pdf.json
gmms O
provide O
flexibility O
in O
modeling O
the O
sharpness O
of O
a O
distribution O
and O
can O
be O
easily O
fit O
using O
expectationmaximization O
( O
em O
) O
( O
see O
more O
on O
appendix O
c O
) O
. O

section 5
id pdf2json/2021.acl-long.154.pdf.json
the O
loss O
is O
computed O
based O
on O
the O
pseudo O
labels O
predicted O
by O
the O
model O
. O

section 5
id pdf2json/2021.acl-long.154.pdf.json
for O
each O
sample O
x O
, O
its O
goodness O
probability O
is O
the O
posterior O
probability O
p O
( O
z O
= O
g|x O
, O
θgmm O
) O
, O
where O
g O
is O
the O
component O
with O
smaller O
mean O
loss O
. O

section 5
id pdf2json/2021.acl-long.154.pdf.json
here O
, O
distillation O
hyperparameter O
η O
is O
the O
posterior O
probability O
threshold O
based O
on O
which O
samples O
are O
selected O
. O

section 5
id pdf2json/2021.acl-long.154.pdf.json
stage O
2 O
: O
distillation O
by O
model O
agreement O
in O
the O
second O
stage O
of O
distillation O
, O
we O
select O
samples O
by O
taking O
the O
agreement O
( O
co-guess O
) O
of O
two O
different O
peer O
models O
θ O
( O
j O
) O
and O
θ O
( O
k O
) O
to O
train O
the O
third O
θ O
( O
l O
) O
. O

section 5
id pdf2json/2021.acl-long.154.pdf.json
formally O
, O
agreement O
( O
d O
( O
k O
) O
, O
d O
( O
j O
) O
) O
= O
{ O
( O
x O
( O
k O
) O
, O
y O
( O
k O
) O
) O
: O
y O
( O
k O
) O
= O
y O
( O
j O
) O
} O
s.t O
. O

section 5
id pdf2json/2021.acl-long.154.pdf.json
k O
6= O
j O

section 6
id pdf2json/2021.acl-long.154.pdf.json
uxla O
uses O
multi-epoch O
co-teaching O
. O

section 6
id pdf2json/2021.acl-long.154.pdf.json
it O
uses O
ds O
andd′t O
in O
the O
first O
epoch O
. O

section 6
id pdf2json/2021.acl-long.154.pdf.json
in O
epoch O
2 O
, O
it O
uses O
d̃t O
( O
target O
virtual O
) O
, O
and O
finally O
it O
uses O
all O
the O
four O
datasets O
- O
ds O
, O
d′t O
, O
d̃t O
, O
and O
d̃s O
( O
line O
22 O
in O
algorithm O
1 O
) O
. O

section 6
id pdf2json/2021.acl-long.154.pdf.json
the O
datasets O
used O
at O
different O
stages O
can O
be O
of O
different O
sizes O
. O

section 6
id pdf2json/2021.acl-long.154.pdf.json
for O
example O
, O
the O
number O
of O
augmented O
samples O
in O
d̃s O
and O
d̃t O
grow O
polynomially O
with O
the O
successive O
cross O
masking O
method O
. O

section 6
id pdf2json/2021.acl-long.154.pdf.json
also O
, O
the O
co-distillation O
produces O
sample O
sets O
of O
variable O
sizes O
. O

section 6
id pdf2json/2021.acl-long.154.pdf.json
to O
ensure O
that O
our O
model O
does O
not O
overfit O
on O
one O
particular O
dataset O
, O
we O
employ O
a O
balanced O
sampling O
strategy O
. O

section 6
id pdf2json/2021.acl-long.154.pdf.json
for O
n O
number O
of O
datasets O
{ O
di O
} O
ni=1 O
with O
probabilities O
, O
{ O
pi O
} O
ni=1 O
, O
we O
define O
the O
following O
multinomial O
distribution O
to O
sample O
from O
: O
pi O
= O
fαi∑n O
j=1 O
f O
α O
j O
, O
where O
fi O
= O
ni∑n O
j=1 O
nj O
( O
2 O
) O
where O
α O
is O
the O
sampling O
factor O
and O
ni O
is O
the O
total O
number O
of O
samples O
in O
the O
ith O
dataset O
. O

section 6
id pdf2json/2021.acl-long.154.pdf.json
by O
tweaking O
α O
, O
we O
can O
control O
how O
many O
samples O
a O
dataset O
can O
provide O
in O
the O
mix O
. O

section 7
id pdf2json/2021.acl-long.154.pdf.json
we O
consider O
three O
tasks O
in O
the O
zero-resource O
crosslingual O
transfer O
setting O
. O

section 7
id pdf2json/2021.acl-long.154.pdf.json
we O
assume O
labeled O
training O
data O
only O
in O
english O
, O
and O
transfer O
the O
trained O
model O
to O
a O
target O
language O
. O

section 7
id pdf2json/2021.acl-long.154.pdf.json
for O
all O
experiments O
, O
we O
report O
the O
mean O
score O
of O
the O
three O
models O
that O
use O
different O
seeds O
. O

section 8
id pdf2json/2021.acl-long.154.pdf.json
xner O
: O
we O
use O
the O
standard O
conll O
datasets O
( O
sang O
, O
2002 O
; O
sang O
and O
meulder O
, O
2003 O
) O
for O
english O
( O
en O
) O
, O
german O
( O
de O
) O
, O
spanish O
( O
es O
) O
and O
dutch O
( O
nl O
) O
. O

section 8
id pdf2json/2021.acl-long.154.pdf.json
we O
also O
evaluate O
on O
finnish O
( O
fi O
) O
and O
arabic O
( O
ar O
) O
datasets O
collected O
from O
bari O
et O
al O
. O

section 8
id pdf2json/2021.acl-long.154.pdf.json
( O
2020 O
) O
. O

section 8
id pdf2json/2021.acl-long.154.pdf.json
note O
that O
arabic O
is O
structurally O
different O
from O
english O
, O
and O
finnish O
is O
from O
a O
different O
language O
family O
. O

section 8
id pdf2json/2021.acl-long.154.pdf.json
to O
show O
how O
the O
models O
perform O
on O
extremely O
lowresource O
languages O
, O
we O
experiment O
with O
three O
structurally O
different O
languages O
from O
wikiann O
( O
pan O
et O
al. O
, O
2017 O
) O
of O
different O
( O
unlabeled O
) O
training O
data O
sizes O
: O
urdu O
( O
ur-20k O
training O
samples O
) O
, O
bengali O
( O
bn10k O
samples O
) O
, O
and O
burmese O
( O
my-100 O
samples O
) O
. O

section 8
id pdf2json/2021.acl-long.154.pdf.json
xnli O
we O
use O
the O
standard O
dataset O
( O
conneau O
et O
al. O
, O
2018 O
) O
. O

section 8
id pdf2json/2021.acl-long.154.pdf.json
for O
a O
given O
pair O
of O
sentences O
, O
the O
task O
is O
to O
predict O
the O
entailment O
relationship O
between O
the O
two O
sentences O
, O
i.e. O
, O
whether O
the O
second O
sentence O
( O
hypothesis O
) O
is O
an O
entailment O
, O
contradiction O
, O
or O
neutral O
with O
respect O
to O
the O
first O
one O
( O
premise O
) O
. O

section 8
id pdf2json/2021.acl-long.154.pdf.json
we O
experiment O
with O
spanish O
, O
german O
, O
arabic O
, O
swahili O
( O
sw O
) O
, O
hindi O
( O
hi O
) O
and O
urdu O
. O

section 8
id pdf2json/2021.acl-long.154.pdf.json
paws-x O
the O
paraphrase O
adversaries O
from O
word O
scrambling O
cross-lingual O
task O
( O
yang O
et O
al. O
, O
2019 O
) O
requires O
the O
models O
to O
determine O
whether O
two O
sentences O
are O
paraphrases O
. O

section 8
id pdf2json/2021.acl-long.154.pdf.json
we O
evaluate O
on O
all O
the O
six O
( O
typologically O
distinct O
) O
languages O
: O
fr O
, O
es O
, O
de O
, O
chinese O
( O
zh O
) O
, O
japanese O
( O
ja O
) O
, O
and O
korean O
( O
ko O
) O
. O

section 8
id pdf2json/2021.acl-long.154.pdf.json
evaluation O
setup O
our O
goal O
is O
to O
adapt O
a O
task O
model O
from O
a O
source O
language O
distribution O
to O
an O
unknown O
target O
language O
distribution O
assuming O
no O
labeled O
data O
in O
the O
target O
. O

section 8
id pdf2json/2021.acl-long.154.pdf.json
in O
this O
scenario O
, O
there O
might O
be O
two O
different O
distributional O
gaps O
: O
( O
i O
) O
the O
generalization O
gap O
for O
the O
source O
distribution O
, O
and O
( O
ii O
) O
the O
gap O
between O
the O
source O
and O
target O
language O
distribution O
. O

section 8
id pdf2json/2021.acl-long.154.pdf.json
we O
wish O
to O
investigate O
our O
method O
in O
tasks O
that O
exhibit O
such O
properties O
. O

section 8
id pdf2json/2021.acl-long.154.pdf.json
we O
use O
the O
standard O
task O
setting O
for O
xner O
, O
where O
we O
take O
100 O
% O
samples O
from O
the O
datasets O
as O
they O
come O
from O
various O
domains O
and O
sizes O
without O
any O
specific O
bias O
. O

section 8
id pdf2json/2021.acl-long.154.pdf.json
however O
, O
both O
xnli O
and O
paws-x O
training O
data O
come O
with O
machine-translated O
texts O
in O
target O
languages O
. O

section 8
id pdf2json/2021.acl-long.154.pdf.json
thus O
, O
the O
data O
is O
parallel O
and O
lacks O
enough O
diversity O
( O
source O
and O
target O
come O
from O
the O
same O
domain O
) O
. O

section 8
id pdf2json/2021.acl-long.154.pdf.json
cross-lingual O
models O
trained O
in O
this O
setup O
may O
pick O
up O
distributional O
bias O
( O
in O
the O
label O
space O
) O
from O
the O
source O
. O

section 8
id pdf2json/2021.acl-long.154.pdf.json
artetxe O
et O
al O
. O

section 8
id pdf2json/2021.acl-long.154.pdf.json
( O
2020 O
) O
also O
argue O
that O
the O
translation O
process O
can O
induce O
subtle O
artifacts O
that O
may O
have O
a O
notable O
impact O
on O
models O
. O

section 8
id pdf2json/2021.acl-long.154.pdf.json
therefore O
, O
for O
xnli O
and O
paws-x O
, O
we O
experiment O
with O
two O
different O
setups O
. O

section 8
id pdf2json/2021.acl-long.154.pdf.json
first O
, O
to O
ensure O
distributional O
differences O
and O
non-parallelism O
, O
we O
use O
5 O
% O
of O
the O
training O
data O
from O
the O
source O
language O
and O
augment O
a O
different O
( O
nonparallel O
) O
5 O
% O
model O
ur O
bn O
my O
supervised O
results O
xlm-r O
( O
our-impl O
) O
97.1 O
97.8 O
76.8 O
zero-resource O
results O
data O
for O
the O
target O
language O
. O

section 8
id pdf2json/2021.acl-long.154.pdf.json
we O
used O
a O
different O
seed O
each O
time O
to O
retrieve O
this O
5 O
% O
data O
. O

section 8
id pdf2json/2021.acl-long.154.pdf.json
second O
, O
to O
compare O
with O
previous O
methods O
, O
we O
also O
evaluate O
on O
the O
standard O
100 O
% O
setup O
. O

section 8
id pdf2json/2021.acl-long.154.pdf.json
the O
evaluation O
is O
done O
on O
the O
entire O
test O
set O
in O
both O
setups O
. O

section 8
id pdf2json/2021.acl-long.154.pdf.json
we O
will O
refer O
to O
these O
two O
settings O
as O
5 O
% O
and O
100 O
% O
. O

section 8
id pdf2json/2021.acl-long.154.pdf.json
more O
details O
about O
model O
settings O
are O
in O
appendix O
d O
. O

section 9
id pdf2json/2021.acl-long.154.pdf.json
xner O
table O
1 O
reports O
the O
xner O
results O
on O
the O
datasets O
from O
conll O
and O
( O
bari O
et O
al. O
, O
2020 O
) O
, O
where O
we O
also O
evaluate O
an O
ensemble O
by O
averaging O
the O
probabilities O
from O
the O
three O
models O
. O

section 9
id pdf2json/2021.acl-long.154.pdf.json
we O
observe O
that O
after O
performing O
warm-up O
with O
conf-penalty O
( O
§2.1 O
) O
, O
xlm-r O
performs O
better O
than O
mbert O
on O
average O
by O
∼3.8 O
% O
for O
all O
the O
languages O
. O

section 9
id pdf2json/2021.acl-long.154.pdf.json
uxla O
gives O
absolute O
improvements O
of O
3.76 O
% O
, O
4.34 O
% O
, O
6.94 O
% O
, O
8.31 O
% O
, O
and O
4.18 O
% O
for O
es O
, O
nl O
, O
de O
, O
ar O
, O
and O
fi O
, O
respectively O
. O

section 9
id pdf2json/2021.acl-long.154.pdf.json
interestingly O
, O
it O
surpasses O
supervised O
lstm-crf O
for O
nl O
and O
de O
without O
using O
any O
target O
language O
labeled O
data O
. O

section 9
id pdf2json/2021.acl-long.154.pdf.json
it O
also O
produces O
comparable O
results O
for O
es O
. O

section 9
id pdf2json/2021.acl-long.154.pdf.json
in O
table O
2 O
, O
we O
report O
the O
results O
on O
the O
three O
lowresource O
langauges O
from O
wikiann O
. O

section 9
id pdf2json/2021.acl-long.154.pdf.json
from O
these O
results O
and O
the O
results O
of O
ar O
and O
fi O
in O
table O
1 O
, O
we O
see O
that O
uxla O
is O
particularly O
effective O
for O
languages O
that O
are O
structurally O
dissimilar O
and/or O
lowresourced O
, O
especially O
when O
the O
base O
model O
is O
weak O
: O
28.54 O
% O
, O
16.05 O
% O
, O
and O
9.25 O
% O
absolute O
improvements O
for O
ur O
, O
my O
and O
ar O
, O
respectively O
. O

section 9
id pdf2json/2021.acl-long.154.pdf.json
xnli-5 O
% O
from O
table O
3 O
, O
we O
see O
that O
the O
performance O
of O
xlm-r O
trained O
on O
5 O
% O
data O
is O
surprisingly O
good O
compared O
to O
the O
model O
trained O
on O
full O
data O
( O
see O
xlm-r O
( O
our O
imp O
. O

section 9
id pdf2json/2021.acl-long.154.pdf.json
) O
) O
, O
lagging O
by O
only O
5.6 O
% O
on O
average O
. O

section 9
id pdf2json/2021.acl-long.154.pdf.json
in O
our O
single O
gpu O
implementation O
of O
xnli O
, O
we O
could O
not O
reproduce O
the O
reported O
results O
of O
conneau O
et O
al O
. O

section 9
id pdf2json/2021.acl-long.154.pdf.json
( O
2020 O
) O
. O

section 9
id pdf2json/2021.acl-long.154.pdf.json
however O
, O
our O
results O
resemble O
the O
reported O
xlm-r O
results O
of O
xtreme O
( O
hu O
et O
al. O
, O
2020 O
) O
. O

section 9
id pdf2json/2021.acl-long.154.pdf.json
we O
consider O
xtreme O
as O
our O
standard O
baseline O
for O
xnli-100 O
% O
. O

section 9
id pdf2json/2021.acl-long.154.pdf.json
we O
observe O
that O
with O
only O
5 O
% O
labeled O
data O
in O
the O
source O
, O
uxla O
gets O
comparable O
results O
to O
the O
xtreme O
baseline O
that O
uses O
100 O
% O
labeled O
data O
( O
lagging O
behind O
by O
only O
∼0.7 O
% O
on O
avg O
. O

section 9
id pdf2json/2021.acl-long.154.pdf.json
) O
; O
even O
for O
ar O
and O
sw O
, O
we O
get O
0.22 O
% O
and O
1.11 O
% O
improvements O
, O
respectively O
. O

section 9
id pdf2json/2021.acl-long.154.pdf.json
it O
surpasses O
the O
standard O
5 O
% O
baseline O
by O
4.2 O
% O
on O
average O
. O

section 9
id pdf2json/2021.acl-long.154.pdf.json
specifically O
, O
uxla O
gets O
absolute O
improvements O
of O
3.05 O
% O
, O
3.34 O
% O
, O
5.38 O
% O
, O
5.01 O
% O
, O
4.29 O
% O
, O
and O
4.12 O
% O
for O
es O
, O
de O
, O
ar O
, O
sw O
, O
hi O
, O
and O
ur O
, O
respectively O
. O

section 9
id pdf2json/2021.acl-long.154.pdf.json
again O
, O
the O
gains O
are O
relatively O
higher O
for O
low-resource O
and/or O
dissimilar O
languages O
despite O
the O
base O
model O
being O
weak O
in O
such O
cases O
. O

section 9
id pdf2json/2021.acl-long.154.pdf.json
xnli-100 O
% O
now O
, O
considering O
uxla O
’ O
s O
performance O
on O
the O
full O
( O
100 O
% O
) O
labeled O
source O
data O
in O
table O
3 O
, O
we O
see O
that O
it O
achieves O
sota O
results O
for O
all O
of O
the O
languages O
with O
an O
absolute O
improvement O
of O
2.55 O
% O
on O
average O
from O
the O
xtreme O
baseline O
. O

section 9
id pdf2json/2021.acl-long.154.pdf.json
specifically O
, O
uxla O
gets O
absolute O
improvements O
of O
1.95 O
% O
, O
1.68 O
% O
, O
4.30 O
% O
, O
3.50 O
% O
, O
3.24 O
% O
, O
and O
1.65 O
% O
for O
es O
, O
de O
, O
ar O
, O
sw O
, O
hi O
, O
and O
ur O
, O
respectively O
. O

section 9
id pdf2json/2021.acl-long.154.pdf.json
paws-x O
similar O
to O
xnli O
, O
we O
observe O
sizable O
improvements O
for O
uxla O
over O
the O
baselines O
on O
paws-x O
for O
both O
5 O
% O
and O
100 O
% O
settings O
( O
table O
4 O
) O
. O

section 9
id pdf2json/2021.acl-long.154.pdf.json
specifically O
, O
in O
5 O
% O
setting O
, O
uxla O
gets O
absolute O
gains O
of O
5.33 O
% O
, O
5.94 O
% O
, O
5.04 O
% O
, O
6.85 O
% O
, O
7.00 O
% O
, O
and O
5.45 O
% O
for O
de O
, O
es O
, O
fr O
, O
ja O
, O
ko O
, O
and O
zh O
, O
respectively O
, O
while O
in O
100 O
% O
setting O
, O
it O
gets O
2.21 O
% O
, O
2.36 O
% O
, O
2.00 O
% O
, O
3.99 O
% O
, O
4.53 O
% O
, O
and O
4.41 O
% O
improvements O
respectively O
. O

section 9
id pdf2json/2021.acl-long.154.pdf.json
in O
general O
, O
we O
get O
an O
average O
improvements O
of O
5.94 O
% O
and O
3.25 O
% O
in O
paws-x-5 O
% O
and O
pawsx-100 O
% O
settings O
respectively O
. O

section 9
id pdf2json/2021.acl-long.154.pdf.json
moreover O
, O
our O
5 O
% O
setting O
outperforms O
100 O
% O
xlm-r O
baselines O
for O
es O
, O
ja O
, O
and O
zh O
. O

section 9
id pdf2json/2021.acl-long.154.pdf.json
interestingly O
, O
in O
the O
100 O
% O
setup O
, O
our O
uxla O
( O
ensemble O
) O
achieves O
almost O
similar O
accuracies O
compared O
to O
supervised O
finetuning O
of O
xlm-r O
on O
all O
target O
language O
training O
dataset O
. O

section 10
id pdf2json/2021.acl-long.154.pdf.json
in O
this O
section O
, O
we O
analyze O
uxla O
by O
dissecting O
it O
and O
measuring O
the O
contribution O
of O
its O
each O
of O
the O
components O
. O

section 10
id pdf2json/2021.acl-long.154.pdf.json
for O
this O
, O
we O
use O
the O
xner O
task O
and O
analyze O
the O
model O
based O
on O
the O
results O
in O
table O
1 O
. O

section 11
id pdf2json/2021.acl-long.154.pdf.json
model O
confidence O
vs. O
clustering O
we O
first O
analyze O
the O
performance O
of O
our O
single-model O
distillation O
methods O
( O
§2.3 O
) O
to O
see O
which O
of O
the O
two O
alternatives O
works O
better O
. O

section 11
id pdf2json/2021.acl-long.154.pdf.json
from O
table O
5 O
, O
we O
see O
that O
both O
perform O
similarly O
with O
model O
confidence O
being O
slightly O
better O
. O

section 11
id pdf2json/2021.acl-long.154.pdf.json
in O
our O
main O
experiments O
( O
tables O
1-4 O
) O
and O
subsequent O
analysis O
, O
we O
use O
model O
confidence O
for O
distillation O
. O

section 11
id pdf2json/2021.acl-long.154.pdf.json
however O
, O
we O
should O
not O
rule O
out O
the O
clustering O
method O
as O
it O
gives O
a O
more O
general O
solution O
to O
consider O
other O
distillation O
features O
( O
e.g. O
, O
sequence O
length O
, O
language O
) O
than O
model O
prediction O
scores O
, O
which O
we O
did O
not O
explore O
in O
this O
paper O
. O

section 11
id pdf2json/2021.acl-long.154.pdf.json
distillation O
factor O
η O
we O
next O
show O
the O
results O
for O
different O
distillation O
factor O
( O
η O
) O
in O
table O
5 O
. O

section 11
id pdf2json/2021.acl-long.154.pdf.json
here O
100 O
% O
refers O
to O
the O
case O
when O
no O
single-model O
distillation O
is O
done O
based O
on O
model O
confidence O
. O

section 11
id pdf2json/2021.acl-long.154.pdf.json
we O
notice O
that O
the O
best O
results O
for O
each O
of O
the O
languages O
are O
obtained O
for O
values O
other O
than O
100 O
% O
, O
which O
indicates O
that O
distillation O
is O
indeed O
an O
effective O
step O
in O
uxla O
. O

section 11
id pdf2json/2021.acl-long.154.pdf.json
see O
appendix O
b O
for O
more O
analysis O
on O
η. O
two-stage O
distillation O
we O
now O
validate O
whether O
the O
second-stage O
distillation O
( O
distillation O
by O
model O
agreement O
) O
is O
needed O
. O

section 11
id pdf2json/2021.acl-long.154.pdf.json
in O
table O
5 O
, O
we O
also O
compare O
the O
results O
with O
the O
model O
agreement O
( O
shown O
as O
∩ O
) O
to O
the O
results O
without O
using O
any O
agreement O
( O
φ O
) O
. O

section 11
id pdf2json/2021.acl-long.154.pdf.json
we O
observe O
better O
performance O
with O
model O
agreement O
in O
all O
the O
cases O
on O
top O
of O
the O
single-model O
distillation O
which O
validates O
its O
utility O
. O

section 11
id pdf2json/2021.acl-long.154.pdf.json
results O
with O
η O
= O
100 O
, O
agreement O
= O
∩ O
can O
be O
considered O
as O
the O
tri-training O
( O
ruder O
and O
plank O
, O
2018b O
) O
baseline O
. O

section 12
id pdf2json/2021.acl-long.154.pdf.json
figure O
2 O
presents O
the O
effect O
of O
different O
types O
of O
augmented O
data O
used O
by O
different O
epochs O
in O
our O
multi-epoch O
co-teaching O
framework O
. O

section 12
id pdf2json/2021.acl-long.154.pdf.json
we O
observe O
that O
in O
every O
epoch O
, O
there O
is O
a O
significant O
boost O
in O
f1 O
scores O
for O
each O
of O
the O
languages O
. O

section 12
id pdf2json/2021.acl-long.154.pdf.json
arabic O
, O
being O
structural O
dissimilar O
to O
english O
, O
has O
a O
lower O
base O
score O
, O
but O
the O
relative O
improvements O
brought O
by O
uxla O
are O
higher O
for O
arabic O
, O
especially O
in O
epoch O
2 O
when O
it O
gets O
exposed O
to O
the O
target O
language O
virtual O
data O
( O
d̃t O
) O
generated O
by O
the O
vicinity O
distribution O
. O

section 13
id pdf2json/2021.acl-long.154.pdf.json
for O
all O
the O
three O
tasks O
, O
we O
get O
reasonable O
improvements O
over O
the O
baselines O
by O
training O
with O
confidence O
penalty O
( O
§2.1 O
) O
. O

section 13
id pdf2json/2021.acl-long.154.pdf.json
specifically O
, O
we O
get O
0.56 O
% O
, O
0.74 O
% O
, O
1.89 O
% O
, O
and O
1.18 O
% O
improvements O
in O
xner O
, O
xnli-5 O
% O
, O
paws-x-5 O
% O
, O
and O
paws-x-100 O
% O
respectively O
( O
table O
1,3,4 O
) O
. O

section 13
id pdf2json/2021.acl-long.154.pdf.json
the O
improvements O
in O
xnli-100 O
% O
are O
marginal O
and O
inconsistent O
, O
which O
we O
suspect O
due O
to O
the O
balanced O
class O
distribution O
. O

section 13
id pdf2json/2021.acl-long.154.pdf.json
from O
the O
results O
of O
ensemble O
models O
, O
we O
see O
that O
the O
ensemble O
boosts O
the O
baseline O
xlm-r O
. O

section 13
id pdf2json/2021.acl-long.154.pdf.json
however O
, O
our O
regular O
uxla O
still O
outperforms O
the O
ensemble O
baselines O
by O
a O
sizeable O
margin O
. O

section 13
id pdf2json/2021.acl-long.154.pdf.json
moreover O
, O
ensembling O
the O
trained O
models O
from O
uxla O
further O
improves O
the O
performance O
. O

section 13
id pdf2json/2021.acl-long.154.pdf.json
these O
comparisons O
ensure O
that O
the O
capability O
of O
uxla O
through O
co-teaching O
and O
co-distillation O
is O
beyond O
the O
ensemble O
effect O
. O

section 14
id pdf2json/2021.acl-long.154.pdf.json
table O
6 O
shows O
the O
robustness O
of O
the O
fine-tuned O
uxla O
model O
on O
xner O
task O
. O

section 14
id pdf2json/2021.acl-long.154.pdf.json
after O
fine-tuning O
in O
a O
specific O
target O
language O
, O
the O
f1 O
scores O
in O
english O
remain O
almost O
similar O
( O
see O
first O
row O
) O
. O

section 14
id pdf2json/2021.acl-long.154.pdf.json
for O
some O
languages O
, O
uxla O
adaptation O
on O
a O
different O
language O
also O
improves O
the O
performance O
. O

section 14
id pdf2json/2021.acl-long.154.pdf.json
for O
example O
, O
arabic O
gets O
improvements O
for O
all O
uxla-adapted O
models O
( O
compare O
50.88 O
with O
others O
in O
row O
5 O
) O
. O

section 14
id pdf2json/2021.acl-long.154.pdf.json
this O
indicates O
that O
augmentation O
of O
uxla O
does O
not O
overfit O
on O
a O
target O
language O
. O

section 14
id pdf2json/2021.acl-long.154.pdf.json
more O
baselines O
, O
analysis O
and O
visualizations O
are O
added O
in O
appendix O
. O

section 15
id pdf2json/2021.acl-long.154.pdf.json
recent O
years O
have O
witnessed O
significant O
progress O
in O
learning O
multilingual O
pretrained O
models O
. O

section 15
id pdf2json/2021.acl-long.154.pdf.json
notably O
, O
mbert O
( O
devlin O
et O
al. O
, O
2019 O
) O
extends O
( O
english O
) O
bert O
by O
jointly O
training O
on O
102 O
languages O
. O

section 15
id pdf2json/2021.acl-long.154.pdf.json
xlm O
( O
lample O
and O
conneau O
, O
2019 O
) O
extends O
mbert O
with O
a O
conditional O
lm O
and O
a O
translation O
lm O
( O
using O
parallel O
data O
) O
objectives O
. O

section 15
id pdf2json/2021.acl-long.154.pdf.json
conneau O
et O
al O
. O

section 15
id pdf2json/2021.acl-long.154.pdf.json
( O
2020 O
) O
train O
the O
largest O
multilingual O
language O
model O
xlm-r O
with O
roberta O
( O
liu O
et O
al. O
, O
2019 O
) O
. O

section 15
id pdf2json/2021.acl-long.154.pdf.json
wu O
and O
dredze O
( O
2019 O
) O
, O
keung O
et O
al O
. O

section 15
id pdf2json/2021.acl-long.154.pdf.json
( O
2019 O
) O
, O
and O
pires O
et O
al O
. O

section 15
id pdf2json/2021.acl-long.154.pdf.json
( O
2019 O
) O
evaluate O
zero-shot O
cross-lingual O
transferability O
of O
mbert O
on O
several O
tasks O
and O
attribute O
its O
generalization O
capability O
to O
shared O
subword O
units O
. O

section 15
id pdf2json/2021.acl-long.154.pdf.json
pires O
et O
al O
. O

section 15
id pdf2json/2021.acl-long.154.pdf.json
( O
2019 O
) O
also O
found O
structural O
similarity O
( O
e.g. O
, O
word O
order O
) O
to O
be O
another O
important O
factor O
for O
successful O
crosslingual O
transfer O
. O

section 15
id pdf2json/2021.acl-long.154.pdf.json
k O
et O
al O
. O

section 15
id pdf2json/2021.acl-long.154.pdf.json
( O
2020 O
) O
, O
however O
, O
show O
that O
the O
shared O
subword O
has O
a O
minimal O
contribution O
; O
instead O
, O
the O
structural O
similarity O
between O
languages O
is O
more O
crucial O
for O
effective O
transfer O
. O

section 15
id pdf2json/2021.acl-long.154.pdf.json
older O
data O
augmentation O
approaches O
relied O
on O
distributional O
clusters O
( O
täckström O
et O
al. O
, O
2012 O
) O
. O

section 15
id pdf2json/2021.acl-long.154.pdf.json
a O
number O
of O
recent O
methods O
have O
been O
proposed O
using O
contextualized O
lms O
( O
kobayashi O
, O
2018 O
; O
wu O
et O
al. O
, O
2018 O
; O
shi O
et O
al. O
, O
2019 O
; O
ding O
et O
al. O
, O
2020 O
; O
liu O
et O
al. O
, O
2021 O
) O
. O

section 15
id pdf2json/2021.acl-long.154.pdf.json
these O
methods O
rely O
on O
labels O
to O
perform O
label-constrained O
augmentation O
, O
thus O
not O
directly O
comparable O
with O
ours O
. O

section 15
id pdf2json/2021.acl-long.154.pdf.json
also O
, O
there O
are O
fundamental O
differences O
in O
the O
way O
we O
use O
the O
pretrained O
lm O
. O

section 15
id pdf2json/2021.acl-long.154.pdf.json
unlike O
them O
our O
lm O
augmentation O
is O
purely O
unsupervised O
and O
we O
do O
not O
perform O
any O
fine-tuning O
of O
the O
pretrained O
vicinity O
model O
. O

section 15
id pdf2json/2021.acl-long.154.pdf.json
this O
disjoint O
characteristic O
gives O
our O
framework O
the O
flexibility O
to O
replace O
θlm O
even O
with O
a O
better O
monolingual O
lm O
for O
a O
specific O
target O
language O
, O
which O
in O
turn O
makes O
uxla O
extendable O
to O
utilize O
stronger O
lms O
that O
may O
come O
in O
the O
future O
. O

section 15
id pdf2json/2021.acl-long.154.pdf.json
in O
a O
concurrent O
work O
( O
mohiuddin O
et O
al. O
, O
2021 O
) O
, O
we O
propose O
a O
contextualized O
lm O
based O
data O
augmentation O
for O
neural O
machine O
translation O
and O
show O
its O
advantages O
over O
traditional O
back-translation O
gaining O
improved O
performance O
in O
low-resource O
scenarios O
. O

section 16
id pdf2json/2021.acl-long.154.pdf.json
we O
propose O
a O
novel O
data O
augmentation O
framework O
, O
uxla O
, O
for O
zero-resource O
cross-lingual O
task O
adaptation O
. O

section 16
id pdf2json/2021.acl-long.154.pdf.json
it O
performs O
simultaneous O
self-training O
with O
data O
augmentation O
and O
unsupervised O
sample O
selection O
. O

section 16
id pdf2json/2021.acl-long.154.pdf.json
with O
extensive O
experiments O
on O
three O
different O
cross-lingual O
tasks O
spanning O
many O
language O
pairs O
, O
we O
have O
demonstrated O
the O
effectiveness O
of O
uxla O
. O

section 16
id pdf2json/2021.acl-long.154.pdf.json
for O
the O
zero-resource O
xner O
task O
, O
uxla O
sets O
a O
new O
sota O
for O
all O
the O
tested O
languages O
. O

section 16
id pdf2json/2021.acl-long.154.pdf.json
for O
both O
xnli O
and O
paws-x O
tasks O
, O
with O
only O
5 O
% O
labeled O
data O
in O
the O
source O
, O
uxla O
gets O
comparable O
results O
to O
the O
baseline O
that O
uses O
100 O
% O
labeled O
data O
. O

section 17
id pdf2json/2021.acl-long.154.pdf.json
here O
are O
our O
justifications O
for O
various O
design O
principles O
of O
the O
uxla O
framework O
. O

section 17
id pdf2json/2021.acl-long.154.pdf.json
is O
masked O
language O
model O
pre-training O
with O
cross-lingual O
training O
data O
from O
task O
dataset O
useful O
? O

section 17
id pdf2json/2021.acl-long.154.pdf.json
in O
table O
7 O
, O
we O
perform O
language O
model O
finetuning O
on O
xlm-r O
large O
model O
with O
multilingual O
sentences O
of O
ner O
dataset O
and O
perform O
adaptation O
with O
only O
english O
language O
. O

section 17
id pdf2json/2021.acl-long.154.pdf.json
with O
the O
lmfinetuned O
xlm-r O
model O
, O
we O
didn O
’ O
t O
see O
any O
significant O
increase O
in O
cross-lingual O
transfer O
. O

section 17
id pdf2json/2021.acl-long.154.pdf.json
for O
spanish O
, O
arabic O
language O
, O
the O
score O
even O
got O
decreased O
, O
which O
indicates O
possible O
over-fitting O
. O

section 17
id pdf2json/2021.acl-long.154.pdf.json
however O
, O
robustness O
experiment O
in O
table O
6 O
( O
see O
in O
the O
main O
paper O
, O
sec O
4.4 O
) O
indicates O
that O
our O
proposed O
method O
doesn O
’ O
t O
overfit O
on O
target O
language O
rather O
than O
augment O
the O
new O
knowledge O
base O
. O

section 17
id pdf2json/2021.acl-long.154.pdf.json
is O
using O
three O
models O
with O
different O
initialization O
necessary O
? O

section 17
id pdf2json/2021.acl-long.154.pdf.json
yes O
, O
different O
initialization O
ensures O
different O
convergence O
paths O
, O
which O
results O
in O
diversity O
during O
inference O
. O

section 17
id pdf2json/2021.acl-long.154.pdf.json
co-labeling O
( O
section O
3.3 O
) O
utilizes O
this O
property O
. O

section 17
id pdf2json/2021.acl-long.154.pdf.json
there O
could O
be O
some O
other O
ways O
to O
achieve O
the O
same O
thing O
. O

section 17
id pdf2json/2021.acl-long.154.pdf.json
our O
initial O
attempt O
with O
three O
different O
heads O
( O
sharing O
a O
backbone O
network O
) O
didn O
’ O
t O
work O
well O
. O

section 17
id pdf2json/2021.acl-long.154.pdf.json
is O
using O
three O
epochs O
necessary O
? O

section 17
id pdf2json/2021.acl-long.154.pdf.json
we O
utilize O
different O
types O
of O
datasets O
in O
different O
epochs O
. O

section 17
id pdf2json/2021.acl-long.154.pdf.json
while O
pseudo-labeling O
may O
induce O
noise O
, O
the O
model O
’ O
s O
predictions O
for O
in-domain O
cross-lingual O
samples O
are O
usually O
better O
. O

section 17
id pdf2json/2021.acl-long.154.pdf.json
because O
of O
this O
, O
for O
a O
smooth O
transition O
, O
we O
apply O
the O
vicinal O
samples O
in O
the O
second O
epoch O
. O

section 17
id pdf2json/2021.acl-long.154.pdf.json
finally O
, O
inspired O
by O
the O
joint O
training O
of O
the O
cross-lingual O
language O
model O
, O
in O
the O
third O
epoch O
we O
use O
all O
four O
datasets O
. O

section 17
id pdf2json/2021.acl-long.154.pdf.json
we O
also O
include O
the O
labeled O
source O
data O
which O
ensures O
that O
our O
model O
does O
not O
overfit O
on O
target O
distribution O
as O
well O
as O
persists O
the O
generalization O
capability O
of O
the O
source O
distribution O
. O

section 17
id pdf2json/2021.acl-long.154.pdf.json
need O
for O
the O
combination O
of O
co-teaching O
, O
codistillation O
and O
co-guessing O
? O

section 17
id pdf2json/2021.acl-long.154.pdf.json
the O
combination O
of O
these O
helps O
to O
distill O
out O
the O
noisy O
samples O
better O
. O

section 17
id pdf2json/2021.acl-long.154.pdf.json
efficiency O
of O
the O
method O
and O
expensive O
extra O
costs O
for O
large-scale O
pretrained O
models O
it O
is O
a O
common O
practice O
in O
model O
selection O
to O
train O
3-5 O
disjoint O
lm-based O
task O
models O
( O
e.g. O
, O
xlm-r O
on O
ner O
) O
with O
different O
random O
seeds O
and O
report O
the O
ensemble O
score O
or O
score O
of O
the O
best O
( O
validation O
set O
) O
model O
. O

section 17
id pdf2json/2021.acl-long.154.pdf.json
in O
contrast O
, O
uxla O
uses O
3 O
different O
models O
and O
jointly O
trains O
them O
where O
the O
models O
assist O
each O
other O
through O
distillation O
and O
co-labeling O
. O

section 17
id pdf2json/2021.acl-long.154.pdf.json
in O
that O
sense O
, O
the O
extra O
cost O
comes O
from O
distillation O
and O
co-labeling O
, O
which O
is O
not O
significant O
and O
is O
compensated O
by O
the O
significant O
improvements O
that O
uxla O
offers O
. O

section 17
id pdf2json/2021.acl-long.154.pdf.json
b O
visualization O
of O
confidence O
penalty O
b.1 O
effect O
of O
confidence O
penalty O
in O
classification O
in O
figure O
3 O
( O
a-b O
) O
, O
we O
present O
the O
effect O
of O
the O
confidence O
penalty O
( O
eq O
. O

section 17
id pdf2json/2021.acl-long.154.pdf.json
1 O
in O
the O
main O
paper O
) O
in O
the O
target O
language O
( O
spanish O
) O
classification O
on O
the O
xner O
dev O
. O

section 17
id pdf2json/2021.acl-long.154.pdf.json
data O
( O
i.e. O
, O
after O
training O
on O
english O
ner O
) O
. O

section 17
id pdf2json/2021.acl-long.154.pdf.json
we O
show O
the O
class O
distribution O
from O
the O
final O
logits O
( O
on O
the O
target O
language O
) O
using O
t-sne O
plots O
. O

section 17
id pdf2json/2021.acl-long.154.pdf.json
from O
the O
figure O
, O
it O
is O
evident O
that O
the O
use O
of O
confidence O
penalty O
in O
the O
warm-up O
step O
makes O
the O
model O
more O
robust O
to O
unseen O
out-of-distribution O
target O
language O
data O
yielding O
better O
predictions O
, O
which O
in O
turn O
also O
provides O
a O
better O
prior O
for O
self-training O
with O
pseudo O
labels O
. O

section 17
id pdf2json/2021.acl-long.154.pdf.json
b.2 O
effect O
of O
confidence O
penalty O
in O
loss O
distribution O
figures O
3 O
( O
c O
) O
and O
3 O
( O
d O
) O
present O
the O
per-sample O
loss O
( O
i.e. O
, O
mean O
loss O
per O
sentence O
w.r.t O
. O

section 17
id pdf2json/2021.acl-long.154.pdf.json
the O
pseudo O
labels O
) O
distribution O
in O
histogram O
without O
and O
with O
confidence O
penalty O
, O
respectively O
. O

section 17
id pdf2json/2021.acl-long.154.pdf.json
here O
, O
accurate2 O
refers O
to O
the O
sentences O
which O
have O
at O
most O
two O
wrong O
ner O
labels O
, O
and O
sentences O
containing O
more O
than O
two O
errors O
are O
referred O
to O
as O
noisy O
samples O
. O

section 17
id pdf2json/2021.acl-long.154.pdf.json
it O
shows O
that O
without O
confidence O
penalty O
, O
there O
are O
many O
noisy O
samples O
with O
a O
small O
loss O
which O
is O
not O
desired O
. O

section 17
id pdf2json/2021.acl-long.154.pdf.json
in O
addition O
to O
that O
, O
the O
figures O
also O
suggest O
that O
the O
confidence O
penalty O
helps O
to O
separate O
the O
clean O
samples O
from O
the O
noisy O
ones O
either O
by O
clustering O
or O
by O
model O
confidence O
. O

section 17
id pdf2json/2021.acl-long.154.pdf.json
figures O
4 O
( O
a O
) O
and O
4 O
( O
b O
) O
present O
the O
loss O
distribution O
in O
a O
scatter O
plot O
by O
sorting O
the O
sentences O
based O
on O
their O
length O
in O
the O
x-axis O
; O
y-axis O
represents O
the O
loss O
. O

section 17
id pdf2json/2021.acl-long.154.pdf.json
as O
we O
can O
see O
, O
the O
losses O
are O
indeed O
more O
scattered O
when O
we O
train O
the O
model O
with O
confidence O
penalty O
, O
which O
indicates O
higher O
per-sample O
entropy O
, O
as O
expected O
. O

section 17
id pdf2json/2021.acl-long.154.pdf.json
also O
, O
we O
can O
see O
that O
as O
the O
sentence O
length O
increases O
, O
there O
are O
more O
wrong O
predictions O
. O

section 17
id pdf2json/2021.acl-long.154.pdf.json
our O
distillation O
method O
should O
be O
able O
to O
distill O
out O
these O
noisy O
pseudo O
samples O
. O

section 17
id pdf2json/2021.acl-long.154.pdf.json
finally O
, O
figures O
4 O
( O
c O
) O
and O
4 O
( O
d O
) O
show O
the O
length O
distribution O
of O
all O
vs. O
the O
selected O
sentences O
( O
by O
distillation O
by O
model O
confidence O
) O
without O
and O
with O
confidence O
penalty O
. O

section 17
id pdf2json/2021.acl-long.154.pdf.json
bari O
et O
al O
. O

section 17
id pdf2json/2021.acl-long.154.pdf.json
( O
2020 O
) O
shows O
that O
cross-lingual O
ner O
inference O
is O
heavily O
dependent O
on O
the O
length O
distribution O
of O
the O
samples O
. O

section 17
id pdf2json/2021.acl-long.154.pdf.json
in O
general O
, O
the O
performance O
of O
the O
lower O
length O
samples O
is O
more O
accurate O
. O

section 17
id pdf2json/2021.acl-long.154.pdf.json
however O
, O
if O
we O
only O
select O
the O
lower O
length O
samples O
we O
will O
easily O
overfit O
. O

section 17
id pdf2json/2021.acl-long.154.pdf.json
from O
these O
plots O
, O
we O
observe O
that O
the O
confidence O
penalty O
also O
helps O
to O
perform O
a O
better O
distillation O
as O
more O
sentences O
are O
selected O
( O
by O
the O
distillation O
procedure O
) O
from O
the O
lower O
length O
distribution O
, O
while O
still O
covering O
the O
entire O
lengths O
. O

section 17
id pdf2json/2021.acl-long.154.pdf.json
this O
shows O
that O
using O
the O
confidence O
penalty O
in O
training O
, O
model O
becomes O
more O
robust O
. O

section 17
id pdf2json/2021.acl-long.154.pdf.json
in O
summary O
, O
comparing O
the O
figures O
3 O
( O
c-d O
) O
- O
4 O
( O
cd O
) O
, O
we O
can O
conclude O
that O
training O
without O
confidence O
penalty O
can O
make O
the O
model O
more O
prone O
to O
over-fitting O
, O
resulting O
in O
more O
noisy O
pseudo O
labels O
. O

section 17
id pdf2json/2021.acl-long.154.pdf.json
training O
with O
confidence O
penalty O
not O
only O
improves O
pseudo O
labeling O
accuracy O
but O
also O
helps O
the O
distillation O
methods O
to O
perform O
better O
noise O
filtering O
. O

section 18
id pdf2json/2021.acl-long.154.pdf.json
one O
limitation O
of O
the O
confidence-based O
( O
singlemodel O
) O
distillation O
is O
that O
it O
does O
not O
consider O
task- O
specific O
information O
. O

section 18
id pdf2json/2021.acl-long.154.pdf.json
apart O
from O
classifier O
confidence O
, O
there O
could O
be O
other O
important O
features O
that O
can O
distinguish O
a O
good O
sample O
from O
a O
noisy O
one O
. O

section 18
id pdf2json/2021.acl-long.154.pdf.json
for O
example O
, O
for O
sequence O
labeling O
, O
sequence O
length O
can O
be O
an O
important O
feature O
as O
the O
models O
tend O
to O
make O
more O
mistakes O
( O
hence O
noisy O
) O
for O
longer O
sequences O
bari O
et O
al O
. O

section 18
id pdf2json/2021.acl-long.154.pdf.json
( O
2020 O
) O
. O

section 18
id pdf2json/2021.acl-long.154.pdf.json
one O
might O
also O
want O
to O
consider O
other O
features O
like O
fluency O
, O
which O
can O
be O
estimated O
by O
a O
pre-trained O
conditional O
lm O
like O
gpt O
radford O
et O
al O
. O

section 18
id pdf2json/2021.acl-long.154.pdf.json
( O
2020 O
) O
. O

section 18
id pdf2json/2021.acl-long.154.pdf.json
in O
the O
following O
, O
we O
introduce O
a O
clustering-based O
method O
that O
can O
consider O
these O
additional O
features O
to O
separate O
good O
samples O
from O
bad O
ones O
. O

section 18
id pdf2json/2021.acl-long.154.pdf.json
here O
our O
goal O
is O
to O
cluster O
the O
samples O
based O
on O
their O
goodness O
. O

section 18
id pdf2json/2021.acl-long.154.pdf.json
it O
has O
been O
shown O
in O
computer O
vision O
that O
deep O
models O
tend O
to O
learn O
good O
samples O
faster O
than O
noisy O
ones O
, O
leading O
to O
a O
lower O
loss O
for O
good O
samples O
and O
higher O
loss O
for O
noisy O
ones O
han O
et O
al O
. O

section 18
id pdf2json/2021.acl-long.154.pdf.json
( O
2018 O
) O
, O
arpit O
et O
al O
. O

section 18
id pdf2json/2021.acl-long.154.pdf.json
( O
2017 O
) O
. O

section 18
id pdf2json/2021.acl-long.154.pdf.json
we O
propose O
to O
model O
per-sample O
loss O
distribution O
( O
along O
with O
other O
task-specific O
features O
) O
with O
a O
mixture O
model O
, O
which O
we O
fit O
using O
an O
expectation-maximization O
( O
em O
) O
algorithm O
. O

section 18
id pdf2json/2021.acl-long.154.pdf.json
however O
, O
contrary O
to O
those O
approaches O
which O
use O
actual O
( O
supervised O
) O
labels O
, O
we O
use O
the O
model O
predicted O
pseudo O
labels O
to O
compute O
the O
loss O
for O
the O
samples O
. O

section 18
id pdf2json/2021.acl-long.154.pdf.json
we O
use O
a O
two-component O
gaussian O
mixture O
model O
( O
gmm O
) O
due O
to O
its O
flexibility O
in O
modeling O
the O
sharpness O
of O
a O
distribution O
li O
et O
al O
. O

section 18
id pdf2json/2021.acl-long.154.pdf.json
( O
2020a O
) O
. O

section 18
id pdf2json/2021.acl-long.154.pdf.json
in O
the O
following O
, O
we O
describe O
the O
em O
training O
of O
the O
gmm O
for O
one O
feature O
, O
i.e. O
, O
per-sample O
loss O
, O
but O
it O
is O
trivial O
to O
extend O
it O
to O
consider O
other O
indicative O
taskspecific O
features O
like O
sequence O
length O
or O
fluency O
score O
( O
see O
any O
textbook O
on O
machine O
learning O
) O
. O

section 18
id pdf2json/2021.acl-long.154.pdf.json
em O
training O
for O
two-component O
gmm O
let O
xi O
∈ O
ir O
denote O
the O
loss O
for O
sample O
xi O
and O
zi O
∈ O
{ O
0 O
, O
1 O
} O
denote O
its O
cluster O
id O
. O

section 18
id pdf2json/2021.acl-long.154.pdf.json
we O
can O
write O
the O
1d O
gmm O
model O
as O
: O
p O
( O
xi|θ O
, O
π O
) O
= O
1∑ O
k=0 O
n O
( O
xi|µk O
, O
σk O
) O
πk O
( O
3 O
) O
where O
θk O
= O
{ O
µk O
, O
σ2k O
} O
are O
the O
parameters O
of O
the O
kth O
mixture O
component O
and O
πk O
= O
p O
( O
zi O
= O
k O
) O
is O
the O
probability O
( O
weight O
) O
of O
the O
k-th O
component O
with O
the O
condition O
0 O
≤ O
πk O
≤ O
1 O
and O
∑ O
k O
πk O
= O
1 O
. O

section 18
id pdf2json/2021.acl-long.154.pdf.json
in O
em O
, O
we O
optimize O
the O
expected O
complete O
data O
log O
likelihood O
q O
( O
θ O
, O
θt−1 O
) O
defined O
as O
: O
q O
( O
θ O
, O
θt−1 O
) O
= O
e O
( O
∑ O
i O
log O
[ O
p O
( O
xi O
, O
zi|θ O
) O
] O
) O
= O
e O
( O
∑ O
i O
∑ O
k O
i O
( O
zi O
= O
k O
) O
log O
[ O
p O
( O
xi|θk O
) O
πk O
] O
) O
= O
∑ O
i O
∑ O
k O
e O
( O
i O
( O
zi O
= O
k O
) O
) O
log O
[ O
p O
( O
xi|θk O
) O
πk O
] O
= O
∑ O
i O
∑ O
k O
p O
( O
zi O
= O
k|xi O
, O
θt−1 O
) O
log O
[ O
p O
( O
xi|θk O
) O
πk O
] O
= O
∑ O
i O
∑ O
k O
ri O
, O
k O
( O
θ O
t−1 O
) O
log O
p O
( O
xi|θk O
) O
+ O
ri O
, O
k O
( O
θt−1 O
) O
log O
πk O
( O
4 O
) O
where O
ri O
, O
k O
( O
θt−1 O
) O
is O
the O
responsibility O
that O
cluster O
k O
takes O
for O
sample O
xi O
, O
which O
is O
computed O
in O
the O
estep O
so O
that O
we O
can O
optimize O
q O
( O
θ O
, O
θt−1 O
) O
( O
eq O
. O

section 18
id pdf2json/2021.acl-long.154.pdf.json
4 O
) O
in O
the O
m-step O
. O

section 18
id pdf2json/2021.acl-long.154.pdf.json
the O
e-step O
and O
m-step O
for O
a O
1d O
gmm O
can O
be O
written O
as O
: O
e-step O
: O
compute O
ri O
, O
k O
( O
θt−1 O
) O
= O
n O
( O
xi|θt−1k O
) O
π O
t−1 O
k∑ O
kn O
( O
xi|θ O
t−1 O
k O
) O
π O
t−1 O
k O
m-step O
: O
optimize O
q O
( O
θ O
, O
θt−1 O
) O
w.r.t O
. O

section 18
id pdf2json/2021.acl-long.154.pdf.json
θ O
and O
π O
• O
πk O
= O
∑ O
i O
ri O
, O
k∑ O
i O
∑ O
k O
ri O
, O
k O
= O
1n O
∑ O
i O
ri O
, O
k O
• O
µk O
= O
∑ O
i O
ri O
, O
kxi∑ O
i O
ri O
, O
k O
; O
σ2k O
= O
∑ O
i O
ri O
, O
k O
( O
xi−µk O
) O
2∑ O
i O
ri O
, O
k O
inference O
for O
a O
sample O
x O
, O
its O
goodness O
probability O
is O
the O
posterior O
probability O
p O
( O
z O
= O
g|x O
, O
θ O
) O
, O
where O
g O
∈ O
{ O
0 O
, O
1 O
} O
is O
the O
component O
with O
smaller O
mean O
loss O
. O

section 18
id pdf2json/2021.acl-long.154.pdf.json
here O
, O
distillation O
hyperparameter O
η O
is O
the O
posterior O
probability O
threshold O
based O
on O
which O
samples O
are O
selected O
. O

section 18
id pdf2json/2021.acl-long.154.pdf.json
relation O
with O
distillation O
by O
model O
confidence O
astute O
readers O
might O
have O
already O
noticed O
that O
per-sample O
loss O
has O
a O
direct O
deterministic O
relation O
with O
the O
model O
confidence O
. O

section 18
id pdf2json/2021.acl-long.154.pdf.json
even O
though O
they O
are O
different O
, O
these O
two O
distillation O
methods O
consider O
the O
same O
source O
of O
information O
. O

section 18
id pdf2json/2021.acl-long.154.pdf.json
however O
, O
as O
mentioned O
, O
the O
clustering-based O
method O
allows O
us O
to O
incorporate O
other O
indicative O
features O
like O
length O
, O
fluency O
, O
etc O
. O

section 18
id pdf2json/2021.acl-long.154.pdf.json
for O
a O
fair O
comparison O
between O
the O
two O
methods O
, O
we O
use O
only O
the O
per-sample O
loss O
in O
our O
primary O
( O
single-model O
) O
distillation O
methods O
. O

section 19
id pdf2json/2021.acl-long.154.pdf.json
we O
present O
the O
hyperparameter O
settings O
for O
xner O
and O
xnli O
tasks O
for O
the O
xla O
framework O
in O
table O
8 O
. O

section 19
id pdf2json/2021.acl-long.154.pdf.json
in O
the O
warm-up O
step O
, O
we O
train O
and O
validate O
the O
task O
models O
with O
english O
data O
. O

section 19
id pdf2json/2021.acl-long.154.pdf.json
however O
, O
for O
cross-lingual O
adaptation O
, O
we O
validate O
( O
for O
model O
selection O
) O
our O
model O
with O
the O
target O
language O
development O
set O
. O

section 19
id pdf2json/2021.acl-long.154.pdf.json
we O
train O
our O
model O
with O
respect O
to O
the O
number O
of O
steps O
instead O
of O
the O
number O
of O
epochs O
. O

section 19
id pdf2json/2021.acl-long.154.pdf.json
in O
the O
case O
of O
a O
given O
number O
of O
epochs O
, O
we O
convert O
it O
to O
a O
total O
number O
of O
steps O
. O

section 19
id pdf2json/2021.acl-long.154.pdf.json
we O
observe O
that O
learning O
rate O
is O
a O
crucial O
hyperparameter O
. O

section 19
id pdf2json/2021.acl-long.154.pdf.json
in O
table O
8 O
, O
lr-warm-up-steps O
refer O
to O
the O
warmup-step O
from O
triangular O
learning O
rate O
scheduling O
. O

section 19
id pdf2json/2021.acl-long.154.pdf.json
this O
hyperparameter O
is O
not O
to O
be O
confused O
with O
warm-up O
step O
of O
the O
uxla O
framework O
. O

section 19
id pdf2json/2021.acl-long.154.pdf.json
in O
our O
experiments O
, O
effective O
batch-size O
is O
another O
crucial O
hyperparameter O
that O
can O
be O
obtained O
by O
gradient O
accumulation O
steps O
. O

section 19
id pdf2json/2021.acl-long.154.pdf.json
we O
fix O
the O
maximum O
sequence O
length O
to O
280 O
for O
xner O
and O
128 O
tokens O
for O
xnli O
. O

section 19
id pdf2json/2021.acl-long.154.pdf.json
for O
each O
of O
the O
experiments O
, O
we O
report O
the O
average O
score O
of O
three O
task O
models O
, O
θ O
( O
1 O
) O
, O
θ O
( O
2 O
) O
, O
θ O
( O
3 O
) O
, O
which O
are O
initialized O
with O
different O
seeds O
. O

section 19
id pdf2json/2021.acl-long.154.pdf.json
we O
perform O
each O
of O
the O
experiments O
in O
a O
single O
gpu O
setup O
with O
float32 O
precision O
. O

section 20
id pdf2json/2021.acl-long.154.pdf.json
vicinal O
risk O
minimization O
. O

section 20
id pdf2json/2021.acl-long.154.pdf.json
one O
of O
the O
fundamental O
challenges O
in O
deep O
learning O
is O
to O
train O
models O
that O
generalize O
well O
to O
examples O
outside O
the O
training O
distribution O
. O

section 20
id pdf2json/2021.acl-long.154.pdf.json
the O
widely O
used O
empirical O
risk O
minimization O
( O
erm O
) O
principle O
where O
models O
are O
trained O
to O
minimize O
the O
average O
training O
error O
has O
been O
shown O
to O
be O
insufficient O
to O
achieve O
generalization O
on O
distributions O
that O
differ O
slightly O
from O
the O
training O
data O
( O
szegedy O
et O
al. O
, O
2014 O
; O
zhang O
et O
al. O
, O
2018 O
) O
. O

section 20
id pdf2json/2021.acl-long.154.pdf.json
data O
augmentation O
supported O
by O
the O
vicinal O
risk O
minimization O
( O
vrm O
) O
principle O
( O
chapelle O
et O
al. O
, O
2001 O
) O
can O
be O
an O
effective O
choice O
for O
achieving O
better O
out-of-training O
generalization O
. O

section 20
id pdf2json/2021.acl-long.154.pdf.json
in O
vrm O
, O
we O
minimize O
the O
empirical O
vicinal O
risk O
defined O
as O
: O
lv O
( O
θ O
) O
= O
1 O
n O
n∑ O
n=1 O
l O
( O
fθ O
( O
x̃n O
) O
, O
ỹn O
) O
( O
5 O
) O
where O
fθ O
denotes O
the O
model O
parameterized O
by O
θ O
, O
and O
daug O
= O
{ O
( O
x̃n O
, O
ỹn O
) O
} O
nn=1 O
is O
an O
augmented O
dataset O
constructed O
by O
sampling O
the O
vicinal O
distribution O
ϑ O
( O
x̃ O
, O
ỹ|xi O
, O
yi O
) O
around O
the O
original O
training O
sample O
( O
xi O
, O
yi O
) O
. O

section 20
id pdf2json/2021.acl-long.154.pdf.json
defining O
vicinity O
is O
however O
challenging O
as O
it O
requires O
to O
extract O
samples O
from O
a O
distribution O
without O
hurting O
the O
labels O
. O

section 20
id pdf2json/2021.acl-long.154.pdf.json
earlier O
methods O
apply O
simple O
rules O
like O
rotation O
and O
scaling O
of O
images O
( O
simard O
et O
al. O
, O
1998 O
) O
. O

section 20
id pdf2json/2021.acl-long.154.pdf.json
recently O
, O
zhang O
et O
al O
. O

section 20
id pdf2json/2021.acl-long.154.pdf.json
( O
2018 O
) O
; O
berthelot O
et O
al O
. O

section 20
id pdf2json/2021.acl-long.154.pdf.json
( O
2019 O
) O
and O
li O
et O
al O
. O

section 20
id pdf2json/2021.acl-long.154.pdf.json
( O
2020 O
) O
show O
impressive O
results O
in O
image O
classification O
with O
simple O
linear O
interpolation O
of O
data O
. O

section 20
id pdf2json/2021.acl-long.154.pdf.json
however O
, O
to O
our O
knowledge O
, O
none O
of O
these O
methods O
has O
so O
far O
been O
successful O
in O
nlp O
due O
to O
the O
discrete O
nature O
of O
texts O
. O

section TITLE
id pdf2json/2021.acl-long.286.pdf.json
best O
of O
both O
worlds O
: O
making O
high O
accuracy O
non-incremental O
transformer-based O
disfluency O
detection O
incremental O

section ABSTRACT
id pdf2json/2021.acl-long.286.pdf.json
while O
transformer-based O
text O
classifiers O
pretrained O
on O
large O
volumes O
of O
text O
have O
yielded O
significant O
improvements O
on O
a O
wide O
range O
of O
computational O
linguistics O
tasks O
, O
their O
implementations O
have O
been O
unsuitable O
for O
live O
incremental O
processing O
thus O
far O
, O
operating O
only O
on O
the O
level O
of O
complete O
sentence O
inputs O
. O

section ABSTRACT
id pdf2json/2021.acl-long.286.pdf.json
we O
address O
the O
challenge O
of O
introducing O
methods O
for O
word-by-word O
left-to-right O
incremental O
processing O
to O
transformers O
such O
as O
bert O
, O
models O
without O
an O
intrinsic O
sense O
of O
linear O
order O
. O

section ABSTRACT
id pdf2json/2021.acl-long.286.pdf.json
we O
modify O
the O
training O
method O
and O
live O
decoding O
of O
non-incremental O
models O
to O
detect O
speech O
disfluencies O
with O
minimum O
latency O
and O
without O
pre-segmentation O
of O
dialogue O
acts O
. O

section ABSTRACT
id pdf2json/2021.acl-long.286.pdf.json
we O
experiment O
with O
several O
decoding O
methods O
to O
predict O
the O
rightward O
context O
of O
the O
word O
currently O
being O
processed O
using O
a O
gpt-2 O
language O
model O
and O
apply O
a O
bert-based O
disfluency O
detector O
to O
sequences O
, O
including O
predicted O
words O
. O

section ABSTRACT
id pdf2json/2021.acl-long.286.pdf.json
we O
show O
our O
method O
of O
incrementalising O
transformers O
maintains O
most O
of O
their O
high O
non-incremental O
performance O
while O
operating O
strictly O
incrementally O
. O

section ABSTRACT
id pdf2json/2021.acl-long.286.pdf.json
we O
also O
evaluate O
our O
models O
’ O
incremental O
performance O
to O
establish O
the O
trade-off O
between O
incremental O
performance O
and O
final O
performance O
, O
using O
different O
prediction O
strategies O
. O

section ABSTRACT
id pdf2json/2021.acl-long.286.pdf.json
we O
apply O
our O
system O
to O
incremental O
speech O
recognition O
results O
as O
they O
arrive O
into O
a O
live O
system O
and O
achieve O
state-of-the-art O
results O
in O
this O
setting O
. O

section 0
id pdf2json/2021.acl-long.286.pdf.json
proceedings O
of O
the O
59th O
annual O
meeting O
of O
the O
association O
for O
computational O
linguistics O
and O
the O
11th O
international O
joint O
conference O
on O
natural O
language O
processing O
, O
pages O
3693–3703 O
august O
1–6 O
, O
2021 O
. O

section 0
id pdf2json/2021.acl-long.286.pdf.json
©2021 O
association O
for O
computational O
linguistics O
3693 O

section 1
id pdf2json/2021.acl-long.286.pdf.json
conversational O
systems O
provide O
a O
significant O
addition O
to O
the O
present O
approaches O
in O
mental O
health O
care O
delivery O
. O

section 1
id pdf2json/2021.acl-long.286.pdf.json
interactions O
with O
these O
conversational O
agents O
have O
been O
shown O
to O
contain O
observable O
indicators O
of O
cognitive O
states O
, O
such O
as O
the O
rate O
of O
filled O
pauses O
and O
different O
temporal O
and O
turnrelated O
features O
( O
gratch O
et O
al. O
, O
2014 O
) O
. O

section 1
id pdf2json/2021.acl-long.286.pdf.json
alzheimer O
’ O
s O
disease O
( O
ad O
) O
patients O
, O
for O
example O
, O
have O
trouble O
performing O
tasks O
that O
leverage O
semantic O
information O
; O
they O
have O
difficulties O
with O
verbal O
fluency O
and O
object O
recognition O
. O

section 1
id pdf2json/2021.acl-long.286.pdf.json
ad O
patients O
speak O
more O
slowly O
with O
long O
pauses O
and O
spend O
extra O
time O
looking O
for O
the O
correct O
word O
, O
which O
leads O
to O
speech O
disfluency O
( O
lópez-de O
ipiña O
et O
al. O
, O
2013 O
; O
nasreen O
et O
al. O
, O
2021 O
) O
. O

section 1
id pdf2json/2021.acl-long.286.pdf.json
disfluency O
markers O
can O
be O
key O
features O
for O
identifying O
certain O
cognitive O
disorders O
for O
application O
in O
conversational O
agents O
( O
rohanian O
et O
al. O
, O
2020 O
) O
. O

section 1
id pdf2json/2021.acl-long.286.pdf.json
such O
conversational O
systems O
are O
primarily O
used O
for O
content O
processing O
, O
which O
is O
then O
analyzed O
offline O
. O

section 1
id pdf2json/2021.acl-long.286.pdf.json
there O
is O
much O
work O
on O
detecting O
disfluencies O
for O
offline O
analysis O
of O
transcripts O
. O

section 1
id pdf2json/2021.acl-long.286.pdf.json
however O
, O
given O
that O
these O
disfluency O
detection O
models O
do O
not O
work O
for O
live O
systems O
and O
depend O
on O
rich O
transcription O
data O
, O
including O
pre-segmentation O
of O
dialogue O
acts O
, O
to O
facilitate O
more O
cost-effective O
analysis O
of O
other O
data O
, O
we O
need O
systems O
capable O
of O
performing O
directly O
and O
incrementally O
off O
the O
speech O
signal O
, O
or O
at O
least O
from O
the O
results O
of O
automatic O
speech O
recognition O
( O
asr O
) O
as O
they O
arrive O
in O
the O
system O
. O

section 1
id pdf2json/2021.acl-long.286.pdf.json
as O
it O
receives O
word-by-word O
data O
, O
an O
incremental O
model O
must O
operate O
with O
minimum O
latency O
and O
do O
so O
without O
changing O
its O
initial O
assumptions O
and O
delivering O
its O
best O
decisions O
as O
early O
as O
possible O
following O
the O
principles O
outlined O
in O
( O
hough O
and O
purver O
, O
2014 O
) O
. O

section 1
id pdf2json/2021.acl-long.286.pdf.json
here O
we O
design O
and O
evaluating O
models O
that O
work O
with O
online O
, O
incremental O
speech O
recognition O
output O
to O
detect O
disfluencies O
with O
varying O
levels O
of O
granularity O
. O

section 1
id pdf2json/2021.acl-long.286.pdf.json
the O
best O
neural O
language O
encoders O
currently O
used O
in O
computational O
linguistics O
consider O
word O
sequences O
as O
a O
whole O
, O
and O
their O
implementations O
have O
been O
unsuitable O
for O
live O
incremental O
processing O
. O

section 1
id pdf2json/2021.acl-long.286.pdf.json
transformers O
( O
vaswani O
et O
al. O
, O
2017 O
) O
, O
for O
instance O
, O
operate O
on O
representations O
that O
do O
not O
naturally O
have O
an O
organizing O
principle O
of O
linear O
word O
order O
. O

section 1
id pdf2json/2021.acl-long.286.pdf.json
we O
analyze O
how O
these O
models O
work O
under O
incremental O
frameworks O
, O
where O
it O
is O
essential O
to O
present O
partial O
output O
relying O
on O
partial O
input O
pro- O
vided O
up O
to O
a O
certain O
time O
step O
that O
may O
occur O
in O
interactive O
healthcare O
systems O
. O

section 1
id pdf2json/2021.acl-long.286.pdf.json
we O
explore O
whether O
we O
can O
adjust O
such O
models O
to O
function O
incrementally O
and O
how O
useful O
they O
are O
in O
terms O
of O
overall O
accuracy O
and O
incremental O
metrics O
. O

section 1
id pdf2json/2021.acl-long.286.pdf.json
to O
further O
enhance O
the O
models O
’ O
incremental O
performance O
, O
we O
use O
two O
general O
strategies O
to O
adjust O
the O
training O
regime O
and O
the O
real-time O
procedure O
: O
incremental O
training O
( O
‘ O
chunk-based O
’ O
training O
and O
add-m O
training O
) O
and O
incremental O
decoding O
( O
constant O
latency O
and O
prophecies O
) O
. O

section 1
id pdf2json/2021.acl-long.286.pdf.json
we O
employ O
three O
prominent O
decoding O
methods O
to O
predict O
the O
rightward O
context O
of O
the O
word O
currently O
being O
processed O
: O
beam O
search O
, O
top-k O
sampling O
, O
and O
top-p O
sampling O
. O

section 1
id pdf2json/2021.acl-long.286.pdf.json
we O
also O
measure O
our O
models O
’ O
incremental O
performance O
to O
set O
the O
trade-off O
between O
incremental O
performance O
and O
final O
performance O
. O

section 2
id pdf2json/2021.acl-long.286.pdf.json
although O
considerable O
work O
has O
been O
done O
on O
detecting O
disfluencies O
, O
much O
of O
this O
work O
uses O
transcripts O
as O
texts O
rather O
than O
live O
speech O
inputs O
, O
with O
the O
goal O
of O
‘ O
cleaning O
’ O
the O
disfluent O
content O
for O
post-processing O
purposes O
. O

section 2
id pdf2json/2021.acl-long.286.pdf.json
they O
are O
almost O
exclusively O
conducted O
on O
pre-segmented O
utterances O
of O
the O
switchboard O
corpus O
of O
telephone O
conversations O
( O
godfrey O
et O
al. O
, O
1992 O
) O
. O

section 2
id pdf2json/2021.acl-long.286.pdf.json
several O
disfluency O
detection O
efforts O
involve O
sentence-based O
parsing O
and O
language O
models O
( O
johnson O
and O
charniak O
, O
2004 O
; O
zwarts O
et O
al. O
, O
2010 O
) O
. O

section 2
id pdf2json/2021.acl-long.286.pdf.json
sequence O
labeling O
models O
with O
start-inside-outside O
( O
bio O
) O
style O
tags O
have O
been O
used O
in O
recent O
neural O
sequence O
approaches O
to O
disfluency O
detection O
based O
on O
bi-directional O
long O
short O
term O
memory O
( O
bilstm O
) O
networks O
and O
transformers O
, O
in O
which O
the O
sequences O
are O
available O
in O
full O
( O
zayats O
et O
al. O
, O
2016 O
; O
lou O
and O
johnson O
, O
2020 O
; O
wang O
et O
al. O
, O
2020 O
) O
. O

section 2
id pdf2json/2021.acl-long.286.pdf.json
such O
offline O
methods O
are O
insufficient O
if O
we O
intend O
to O
infer O
meaning O
from O
repairs O
and O
edit O
words O
for O
disfluency O
detection O
in O
real-time O
, O
which O
is O
beneficial O
in O
a O
healthcare O
domain O
dialogue O
system O
that O
seeks O
to O
get O
a O
consistent O
and O
clear O
understanding O
of O
user O
statements O
and O
the O
user O
’ O
s O
cognitive O
state O
. O

section 2
id pdf2json/2021.acl-long.286.pdf.json
methods O
based O
on O
strictly O
incremental O
operation O
have O
been O
rare O
. O

section 2
id pdf2json/2021.acl-long.286.pdf.json
hough O
and O
purver O
( O
2014 O
) O
used O
a O
line O
of O
classifiers O
and O
language O
model O
features O
in O
a O
strong O
incremental O
operating O
system O
without O
looking O
ahead O
. O

section 2
id pdf2json/2021.acl-long.286.pdf.json
incremental O
dependency O
parsing O
combined O
with O
the O
removal O
of O
disfluency O
was O
also O
studied O
( O
rasooli O
and O
tetreault O
, O
2015 O
) O
. O

section 2
id pdf2json/2021.acl-long.286.pdf.json
some O
studies O
have O
used O
recurrent O
neural O
networks O
for O
live O
dis- O
fluency O
identification O
. O

section 2
id pdf2json/2021.acl-long.286.pdf.json
using O
a O
basic O
elman O
recurrent O
neural O
network O
( O
rnn O
) O
, O
hough O
and O
schlangen O
( O
2015 O
) O
investigated O
incremental O
processing O
, O
with O
an O
objective O
coupling O
detection O
accuracy O
with O
low O
latency O
. O

section 2
id pdf2json/2021.acl-long.286.pdf.json
language O
models O
have O
been O
used O
as O
an O
additional O
task O
for O
the O
identification O
of O
disfluencies O
, O
relying O
on O
the O
intuition O
that O
disfluencies O
can O
be O
detected O
by O
divergences O
from O
clean O
language O
models O
, O
with O
johnson O
and O
charniak O
( O
2004 O
) O
’ O
s O
noisy O
channel O
model O
beginning O
this O
effort O
. O

section 2
id pdf2json/2021.acl-long.286.pdf.json
shalyminov O
et O
al O
. O

section 2
id pdf2json/2021.acl-long.286.pdf.json
( O
2018 O
) O
made O
language O
modelling O
an O
auxiliary O
task O
to O
disfluency O
detection O
in O
a O
deep O
multi-task O
learning O
( O
mtl O
) O
set-up O
, O
gaining O
accuracy O
over O
a O
vanilla O
rnn O
tagger O
. O

section 2
id pdf2json/2021.acl-long.286.pdf.json
pos O
tags O
have O
also O
been O
used O
as O
an O
input O
for O
detecting O
disfluencies O
, O
showing O
slight O
increases O
in O
disfluency O
detection O
over O
using O
word O
values O
alone O
( O
purver O
et O
al. O
, O
2018 O
) O
. O

section 2
id pdf2json/2021.acl-long.286.pdf.json
while O
the O
work O
above O
operates O
only O
on O
transcripts O
pre-segmented O
into O
utterances O
, O
recent O
research O
has O
been O
performed O
on O
combining O
disfluency O
detection O
with O
utterance O
segmentation O
. O

section 2
id pdf2json/2021.acl-long.286.pdf.json
this O
was O
done O
in O
a O
joint O
tagset O
of O
disfluency O
, O
and O
utterance O
segmentation O
tags O
by O
( O
hough O
and O
schlangen O
, O
2017 O
) O
, O
showing O
an O
improvement O
over O
the O
performance O
of O
the O
individual O
tasks O
, O
and O
( O
rohanian O
and O
hough O
, O
2020 O
) O
show O
an O
improvement O
in O
both O
tasks O
when O
framed O
as O
a O
multi-task O
learning O
( O
mtl O
) O
set-up O
with O
a O
long O
short-term O
memory O
network O
( O
lstm O
) O
, O
also O
simultaneously O
doing O
pos-tagging O
and O
language O
modelling O
. O

section 2
id pdf2json/2021.acl-long.286.pdf.json
the O
recent O
live O
incremental O
systems O
fall O
short O
of O
the O
same O
accuracies O
achievable O
on O
pre-segmented O
transcripts O
, O
so O
there O
is O
a O
natural O
interest O
in O
using O
the O
best O
non-incremental O
sequence O
models O
and O
adapting O
them O
for O
incrementality O
. O

section 2
id pdf2json/2021.acl-long.286.pdf.json
madureira O
and O
schlangen O
( O
2020 O
) O
take O
up O
this O
effort O
in O
several O
other O
sequence O
tagging O
and O
classification O
tasks O
, O
showing O
how O
bidirectional O
encoders O
and O
transformers O
can O
be O
modified O
to O
work O
incrementally O
. O

section 2
id pdf2json/2021.acl-long.286.pdf.json
to O
reduce O
the O
impact O
of O
the O
partiality O
of O
the O
input O
, O
the O
models O
predict O
future O
content O
and O
wait O
for O
more O
rightward O
context O
. O

section 2
id pdf2json/2021.acl-long.286.pdf.json
dalvi O
et O
al O
. O

section 2
id pdf2json/2021.acl-long.286.pdf.json
( O
2018 O
) O
also O
use O
truncated O
inputs O
during O
the O
training O
phase O
of O
live O
machine O
translation O
to O
address O
the O
partial O
input O
sentence O
decoding O
problem O
bidirectional O
encoders O
face O
. O

section 2
id pdf2json/2021.acl-long.286.pdf.json
here O
, O
we O
seek O
to O
add O
to O
this O
growing O
effort O
to O
investigate O
the O
trade-off O
of O
incremental O
performance O
against O
the O
final O
output O
quality O
of O
deep O
neural O
network-based O
language O
processing O
, O
applied O
to O
incremental O
disfluency O
detection O
. O

section 3
id pdf2json/2021.acl-long.286.pdf.json
disfluencies O
are O
generally O
assumed O
to O
have O
a O
reparandum-interregnum-repair O
structure O
in O
their O
fullest O
form O
as O
speech O
repairs O
( O
shriberg O
, O
1994 O
; O
meteer O
et O
al. O
, O
1995 O
) O
. O

section 3
id pdf2json/2021.acl-long.286.pdf.json
a O
reparandum O
is O
a O
stretch O
of O
speech O
later O
corrected O
by O
the O
speaker O
; O
the O
corrected O
expression O
is O
a O
repair O
, O
the O
beginning O
of O
which O
is O
referred O
to O
as O
repair O
onset O
. O

section 3
id pdf2json/2021.acl-long.286.pdf.json
an O
interregnum O
word O
is O
a O
filler O
or O
a O
reference O
expression O
between O
the O
repair O
and O
reparandum O
, O
usually O
an O
interruption O
and O
hesitation O
step O
when O
the O
speaker O
expresses O
a O
repair O
, O
giving O
the O
structure O
as O
in O
( O
1 O
) O
. O

section 3
id pdf2json/2021.acl-long.286.pdf.json
john O
[ O
likes︸ O
︷︷ O
︸ O
reparandum O
+ O
{ O
uh O
} O
︸ O
︷︷ O
︸ O
interregnum O
loves O
] O
︸ O
︷︷ O
︸ O
repair O
mary O
( O
1 O
) O
in O
the O
absence O
of O
reparandum O
and O
repair O
, O
the O
disfluency O
is O
reduced O
to O
an O
isolated O
edit O
term O
. O

section 3
id pdf2json/2021.acl-long.286.pdf.json
a O
marked O
, O
lexicalised O
edit O
term O
such O
as O
a O
filled O
pause O
( O
“ O
uh O
” O
or O
“ O
um O
” O
) O
or O
more O
phrasal O
terms O
such O
as O
“ O
i O
mean O
” O
and O
“ O
you O
know O
” O
may O
occur O
. O

section 3
id pdf2json/2021.acl-long.286.pdf.json
the O
identification O
of O
these O
elements O
and O
their O
structure O
is O
then O
the O
task O
of O
disfluency O
detection O
. O

section 3
id pdf2json/2021.acl-long.286.pdf.json
the O
task O
of O
detecting O
incremental O
disfluencies O
adds O
to O
the O
difficulty O
of O
doing O
this O
in O
real-time O
, O
word-by-word O
, O
from O
left O
to O
right O
. O

section 3
id pdf2json/2021.acl-long.286.pdf.json
disfluency O
recognition O
is O
then O
treated O
as O
the O
same O
problem O
that O
a O
human O
processor O
faces O
with O
a O
disfluent O
expression O
: O
only O
when O
an O
interregnum O
is O
detected O
, O
or O
maybe O
even O
when O
a O
repair O
is O
initiated O
, O
does O
it O
become O
clear O
that O
the O
earlier O
content O
is O
now O
to O
be O
regarded O
as O
‘ O
to O
be O
repaired O
, O
’ O
i.e. O
, O
to O
be O
classified O
as O
a O
reparandum O
. O

section 3
id pdf2json/2021.acl-long.286.pdf.json
therefore O
, O
the O
task O
can O
not O
be O
defined O
as O
a O
simple O
sequence O
labeling O
task O
in O
which O
the O
tags O
for O
the O
reparandum O
, O
interregnum O
, O
and O
repair O
phases O
are O
assigned O
left-to-right O
over O
words O
as O
seen O
in O
the O
above O
example O
; O
in O
this O
case O
, O
it O
will O
require O
the O
assumption O
that O
“ O
likes O
” O
would O
be O
repaired O
, O
at O
a O
time O
when O
there O
is O
no O
data O
to O
make O
it O
available O
. O

section 3
id pdf2json/2021.acl-long.286.pdf.json
we O
use O
a O
tag O
set O
that O
encodes O
the O
start O
of O
the O
reparandum O
only O
at O
a O
time O
when O
it O
can O
be O
inferred O
, O
primarily O
when O
the O
repair O
starts O
– O
the O
disfluency O
detection O
task O
is O
to O
tag O
words O
as O
in O
the O
top O
line O
of O
tags O
in O
fig O
. O

section 3
id pdf2json/2021.acl-long.286.pdf.json
1 O
as O
either O
fluent O
( O
f O
) O
an O
edit O
term O
( O
e O
) O
, O
a O
repair O
onset O
word O
( O
rps−n O
for O
the O
reparandum O
starting O
n O
words O
back O
) O
and O
a O
repair O
end O
word O
of O
the O
type O
repeat O
( O
rpnrep O
) O
, O
substitution O
( O
rpnsub O
) O
or O
delete O
( O
rpndel O
) O
. O

section 4
id pdf2json/2021.acl-long.286.pdf.json
to O
incrementalise O
a O
transformer-based O
model O
for O
word-by-word O
disfluency O
detection O
, O
we O
devise O
a O
model O
built O
on O
top O
of O
a O
pre-trained O
bert O
architecture O
( O
devlin O
et O
al. O
, O
2019 O
) O
with O
a O
conditional O
random O
field O
( O
crf O
) O
output O
architecture O
to O
tag O
sequences O
with O
tags O
such O
as O
those O
in O
the O
top O
line O
of O
fig O
. O

section 4
id pdf2json/2021.acl-long.286.pdf.json
1 O
. O

section 4
id pdf2json/2021.acl-long.286.pdf.json
we O
use O
a O
bert-based O
encoder O
and O
try O
different O
strategies O
to O
incrementalise O
the O
system O
’ O
s O
operation O
and O
output O
, O
using O
language O
models O
to O
predict O
future O
word O
sequences O
as O
described O
in O
section O
5 O
while O
maintaining O
bert O
’ O
s O
non-incremental O
quality O
. O

section 4
id pdf2json/2021.acl-long.286.pdf.json
utterance O
segmentation O
our O
models O
are O
designed O
to O
work O
not O
only O
with O
pre-segmented O
data O
but O
also O
on O
raw O
transcripts O
and O
asr O
results O
, O
where O
utterance O
segmentation O
is O
required O
to O
leverage O
the O
use O
of O
sentence-based O
linguistic O
knowledge O
in O
bert O
. O

section 4
id pdf2json/2021.acl-long.286.pdf.json
utterance O
segmentation O
has O
a O
clear O
interdependence O
with O
and O
influence O
on O
the O
detection O
of O
disfluency O
as O
disfluent O
restarts O
and O
repairs O
may O
be O
incorrectly O
predicted O
at O
fluent O
utterance O
boundaries O
without O
segmentation O
. O

section 4
id pdf2json/2021.acl-long.286.pdf.json
in O
this O
paper O
, O
rather O
than O
performing O
utterance O
segmentation O
in O
tandem O
with O
disfluency O
detection O
, O
we O
perform O
it O
on O
words O
as O
they O
arrive O
in O
the O
system O
as O
a O
live O
segmentation O
task O
before O
sending O
the O
current O
prefix O
of O
the O
utterance O
to O
the O
disfluency O
detection O
system O
. O

section 4
id pdf2json/2021.acl-long.286.pdf.json
we O
use O
the O
word-by-word O
segmentation O
system O
from O
( O
rohanian O
and O
hough O
, O
2020 O
) O
where O
four O
output O
tags O
define O
ranges O
of O
transcribed O
words O
or O
word O
hypotheses O
using O
a O
bies O
tag O
scheme O
( O
beginning O
, O
inside O
, O
end O
, O
and O
single O
) O
to O
allow O
for O
the O
prediction O
of O
an O
utterance O
ending O
. O

section 4
id pdf2json/2021.acl-long.286.pdf.json
the O
tagset O
allows O
information O
to O
be O
captured O
from O
the O
context O
of O
the O
word O
to O
decide O
whether O
this O
word O
continues O
a O
current O
utterance O
( O
the O
- O
prefix O
) O
or O
starts O
anew O
( O
the O
. O

section 4
id pdf2json/2021.acl-long.286.pdf.json
prefix O
) O
, O
and O
also O
allows O
live O
prediction O
of O
whether O
the O
next O
word O
will O
continue O
the O
current O
utterance O
( O
the O
- O
suffix O
) O
or O
whether O
the O
current O
word O
finishes O
the O
utterance O
( O
the O
. O

section 4
id pdf2json/2021.acl-long.286.pdf.json
suffix O
) O
. O

section 4
id pdf2json/2021.acl-long.286.pdf.json
an O
example O
of O
the O
scheme O
is O
shown O
in O
the O
second O
line O
of O
fig O
. O

section 4
id pdf2json/2021.acl-long.286.pdf.json
1 O
. O

section 4
id pdf2json/2021.acl-long.286.pdf.json
crf O
we O
use O
a O
crf O
output O
architecture O
to O
predict O
a O
tag O
for O
every O
token O
. O

section 4
id pdf2json/2021.acl-long.286.pdf.json
although O
this O
model O
generates O
predictions O
for O
the O
whole O
sequence O
, O
the O
labels O
are O
outputted O
individually O
. O

section 4
id pdf2json/2021.acl-long.286.pdf.json
there O
are O
important O
dependencies O
between O
adjacent O
labels O
in O
disfluency O
detection O
, O
and O
explicit O
modeling O
of O
these O
relationships O
can O
help O
. O

section 4
id pdf2json/2021.acl-long.286.pdf.json
the O
addition O
of O
the O
crf O
enables O
the O
model O
to O
test O
for O
the O
most O
optimal O
path O
across O
all O
available O
label O
sequences O
. O

section 5
id pdf2json/2021.acl-long.286.pdf.json
in O
addition O
to O
the O
word O
values O
, O
we O
also O
experiment O
with O
two O
other O
inputs O
: O
part-of-speech O
tags O
pos O
tags O
may O
enhance O
the O
identification O
of O
disfluencies O
on O
various O
settings O
. O

section 5
id pdf2json/2021.acl-long.286.pdf.json
pos O
tagging O
helps O
detect O
disfluency O
structure O
as O
the O
parallelism O
between O
the O
reparandum O
and O
repair O
in O
substitutions O
, O
as O
shown O
in O
the O
repeated O
in O
nnp O
sequences O
in O
fig O
. O

section 5
id pdf2json/2021.acl-long.286.pdf.json
1 O
. O

section 5
id pdf2json/2021.acl-long.286.pdf.json
word O
timings O
we O
also O
experiment O
with O
the O
duration O
from O
the O
ending O
of O
the O
previous O
word O
to O
the O
ending O
of O
the O
current O
word O
as O
it O
enters O
the O
system O
, O
either O
from O
ground O
truth O
word O
transcriptions O
or O
from O
asr O
results O
. O

section 6
id pdf2json/2021.acl-long.286.pdf.json
here O
we O
describe O
the O
different O
strategies O
we O
used O
to O
modify O
the O
training O
and O
live O
decoding O
methods O
of O
non-incremental O
models O
to O
detect O
speech O
disfluencies O
word-by-word O
incrementally O
. O

section 6
id pdf2json/2021.acl-long.286.pdf.json
the O
general O
principle O
is O
to O
leverage O
high O
accuracy O
full O
sequence O
classification O
using O
bert O
but O
deploying O
it O
on O
sequences O
, O
including O
future O
predictions O
for O
words O
up O
to O
the O
hypothesised O
end O
of O
the O
current O
utterance O
. O

section 7
id pdf2json/2021.acl-long.286.pdf.json
training O
is O
performed O
on O
full O
sentences/utterances O
, O
but O
the O
decoder O
produces O
outputs O
based O
on O
partial O
input O
data O
at O
the O
test O
time O
. O

section 7
id pdf2json/2021.acl-long.286.pdf.json
this O
disparity O
between O
training O
and O
decoding O
can O
potentially O
affect O
our O
models O
’ O
performance O
. O

section 7
id pdf2json/2021.acl-long.286.pdf.json
based O
on O
( O
dalvi O
et O
al. O
, O
2018 O
) O
, O
we O
present O
two O
methods O
to O
address O
this O
issue O
: O
chunk-based O
training O
and O
add-m O
training O
. O

section 7
id pdf2json/2021.acl-long.286.pdf.json
chunk-based O
training O
in O
chunk-based O
training O
, O
we O
change O
the O
training O
scheme O
by O
removing O
the O
ends O
of O
each O
sentence O
in O
the O
training O
set O
and O
simply O
break O
each O
training O
sentence O
into O
chunks O
of O
n O
tokens O
. O

section 7
id pdf2json/2021.acl-long.286.pdf.json
here O
we O
use O
2 O
and O
3 O
for O
n O
. O

section 7
id pdf2json/2021.acl-long.286.pdf.json
add-m O
training O
we O
begin O
with O
the O
first O
n O
words O
in O
training O
sentences O
in O
add-m O
training O
. O

section 7
id pdf2json/2021.acl-long.286.pdf.json
the O
next O
training O
instances O
are O
then O
generated O
by O
n O
+m O
, O
n O
+2m O
, O
n O
+3m O
... O
words O
before O
the O
end O
of O
the O
sentence O
is O
reached O
. O

section 7
id pdf2json/2021.acl-long.286.pdf.json
in O
our O
experiments O
, O
we O
found O
setting O
n=1 O
and O
m=1 O
worked O
best O
. O

section 8
id pdf2json/2021.acl-long.286.pdf.json
constant O
latency O
the O
technique O
of O
constant O
latency O
requires O
allowing O
certain O
‘ O
future O
’ O
words O
to O
be O
seen O
before O
a O
label O
to O
previous O
words O
is O
given O
. O

section 8
id pdf2json/2021.acl-long.286.pdf.json
it O
is O
a O
form O
of O
look-ahead O
based O
on O
baumann O
et O
al O
. O

section 8
id pdf2json/2021.acl-long.286.pdf.json
( O
2011 O
) O
, O
in O
which O
before O
making O
the O
first O
decision O
with O
respect O
to O
previous O
time O
steps O
, O
the O
processor O
is O
required O
to O
wait O
for O
some O
correct O
context O
. O

section 8
id pdf2json/2021.acl-long.286.pdf.json
we O
explore O
the O
one- O
or O
two-word O
contexts O
of O
our O
input O
. O

section 8
id pdf2json/2021.acl-long.286.pdf.json
this O
suggests O
that O
the O
model O
generates O
the O
first O
label O
for O
word O
t O
after O
the O
word O
t+ O
1 O
is O
seen O
or O
the O
model O
observes O
words O
t O
+ O
1 O
and O
t O
+ O
2 O
before O
tagging O
word O
t. O
this O
has O
an O
inherent O
limit O
on O
the O
latency O
achievable O
, O
and O
we O
use O
this O
as O
a O
baseline O
incremental O
decoding O
system O
. O

section 8
id pdf2json/2021.acl-long.286.pdf.json
prophecy-based O
decoding O
for O
our O
other O
decoding O
strategies O
, O
we O
use O
a O
‘ O
prophecy O
’ O
-based O
approach O
to O
predicting O
future O
word O
sequences O
, O
following O
the O
task O
of O
open-ended O
language O
generation O
, O
which O
, O
given O
an O
input O
text O
passage O
as O
context O
, O
is O
to O
produce O
text O
that O
constitutes O
a O
cohesive O
continuation O
( O
holtzman O
et O
al. O
, O
2019 O
) O
. O

section 8
id pdf2json/2021.acl-long.286.pdf.json
inspired O
by O
( O
madureira O
and O
schlangen O
, O
2020 O
) O
, O
using O
the O
gpt-2 O
language O
model O
( O
radford O
et O
al. O
, O
2019 O
) O
, O
we O
first O
give O
each O
word O
as O
a O
left O
context O
and O
create O
a O
continuation O
until O
the O
end O
of O
an O
utterance O
to O
create O
a O
hypothetical O
complete O
context O
that O
satisfies O
the O
requirements O
of O
the O
models O
’ O
non-incremental O
structure O
. O

section 8
id pdf2json/2021.acl-long.286.pdf.json
formally O
, O
with O
m O
tokens O
x1 O
... O
xm O
as O
our O
context O
, O
the O
task O
is O
to O
create O
the O
next O
n O
continuation O
tokens O
to O
achieve O
the O
completed O
sequence O
x1 O
... O
xm+n O
. O

section 8
id pdf2json/2021.acl-long.286.pdf.json
it O
is O
assumed O
that O
the O
models O
compute O
p O
( O
x1 O
: O
m+n O
) O
using O
a O
standard O
left-to-right O
decomposition O
of O
the O
text O
probability O
as O
in O
( O
2 O
) O
. O

section 8
id pdf2json/2021.acl-long.286.pdf.json
this O
process O
is O
used O
to O
build O
the O
utterance O
continuation O
token-by-token O
using O
a O
specific O
decoding O
technique O
. O

section 8
id pdf2json/2021.acl-long.286.pdf.json
p O
( O
x1 O
: O
m+n O
) O
= O
m+n∏ O
i=1 O
p O
( O
xi|x1 O
... O
xi−1 O
) O
( O
2 O
) O
three O
of O
the O
most O
common O
decoding O
methods O
are O
used O
in O
this O
paper O
: O
beam O
search O
, O
top-k O
sampling O
, O
and O
top-p O
sampling O
. O

section 8
id pdf2json/2021.acl-long.286.pdf.json
example O
word O
sequence O
prophecies O
from O
these O
decoding O
methods O
are O
shown O
in O
fig O
. O

section 8
id pdf2json/2021.acl-long.286.pdf.json
2 O
. O

section 8
id pdf2json/2021.acl-long.286.pdf.json
the O
right-most O
block O
shows O
the O
prediction O
of O
the O
continuation O
of O
the O
word O
sequences O
as O
each O
new O
word O
in O
the O
sequence O
“ O
john O
likes O
uh O
loves O
mary O
” O
is O
fed O
into O
the O
language O
model O
. O

section 8
id pdf2json/2021.acl-long.286.pdf.json
beam O
search O
assuming O
that O
the O
model O
gives O
a O
greater O
likelihood O
to O
better O
quality O
text O
, O
we O
are O
looking O
for O
a O
sequence O
with O
the O
highest O
probability O
. O

section 8
id pdf2json/2021.acl-long.286.pdf.json
during O
the O
search O
, O
a O
group O
of O
stacks O
is O
used O
to O
hold O
hypotheses O
. O

section 8
id pdf2json/2021.acl-long.286.pdf.json
beam O
size O
n O
is O
used O
to O
manage O
the O
search O
space O
by O
expanding O
the O
top O
n O
hypotheses O
in O
the O
existing O
stack O
. O

section 8
id pdf2json/2021.acl-long.286.pdf.json
we O
used O
beam O
size O
10 O
for O
all O
the O
models O
. O

section 8
id pdf2json/2021.acl-long.286.pdf.json
top-k O
sampling O
we O
define O
sampling O
as O
randomly O
choosing O
the O
next O
word O
based O
on O
its O
conditional O
probability O
distribution O
as O
in O
( O
3 O
) O
. O

section 8
id pdf2json/2021.acl-long.286.pdf.json
xi O
∼ O
p O
( O
x|x1 O
: O
i−1 O
) O
( O
3 O
) O
in O
the O
top-k O
sampling O
, O
the O
most O
probable O
next O
k O
words O
are O
extracted O
and O
the O
probability O
mass O
is O
redistributed O
between O
only O
the O
following O
k O
words O
( O
fan O
et O
al. O
, O
2018 O
) O
. O

section 8
id pdf2json/2021.acl-long.286.pdf.json
given O
a O
distribution O
p O
( O
x|x1 O
: O
i−1 O
) O
, O
we O
extract O
its O
top-k O
vocabulary O
v O
( O
k O
) O
⊂ O
v O
as O
the O
set O
of O
size O
k O
which O
maximizes O
∑ O
x∈v O
( O
k O
) O
p O
( O
x|x1 O
: O
i−1 O
) O
. O

section 8
id pdf2json/2021.acl-long.286.pdf.json
after O
an O
initial O
investigation O
, O
we O
set O
k O
to O
50 O
in O
all O
experiments O
. O

section 8
id pdf2json/2021.acl-long.286.pdf.json
top-p O
sampling O
rather O
than O
selecting O
only O
the O
most O
probable O
k O
words O
, O
in O
top-p O
sampling O
, O
we O
select O
the O
smallest O
possible O
range O
of O
words O
with O
their O
total O
likelihood O
exceeds O
the O
probability O
p O
( O
holtzman O
et O
al. O
, O
2019 O
) O
. O

section 8
id pdf2json/2021.acl-long.286.pdf.json
the O
probability O
mass O
is O
then O
redistributed O
between O
this O
set O
of O
words O
. O

section 8
id pdf2json/2021.acl-long.286.pdf.json
with O
this O
method O
, O
the O
size O
of O
the O
word O
set O
will O
dynamically O
adjust O
based O
on O
the O
probability O
distribution O
of O
the O
next O
word O
. O

section 8
id pdf2json/2021.acl-long.286.pdf.json
with O
the O
distribution O
p O
( O
x|x1 O
: O
i−1 O
) O
, O
we O
consider O
its O
top-p O
sequence O
, O
with O
vocabulary O
v O
( O
p O
) O
⊂ O
v O
as O
the O
smallest O
set O
with O
p O
( O
x|x1 O
: O
i−1 O
) O
≥ O
p. O
we O
set O
p O
= O
0.95 O
. O

section 9
id pdf2json/2021.acl-long.286.pdf.json
we O
train O
on O
transcripts O
and O
test O
on O
both O
transcripts O
and O
asr O
hypotheses O
. O

section 9
id pdf2json/2021.acl-long.286.pdf.json
all O
models O
in O
testing O
have O
strictly O
word-by-word O
left O
to O
right O
input O
. O

section 9
id pdf2json/2021.acl-long.286.pdf.json
in O
addition O
to O
using O
the O
latest O
word O
hypothesis O
as O
input O
, O
we O
train O
and O
evaluate O
the O
presented O
models O
with O
two O
kinds O
of O
additional O
inputs O
: O
time O
elapsed O
from O
the O
end O
of O
the O
previous O
word O
( O
hypothesis O
) O
to O
the O
current O
one O
and O
the O
pos O
tag O
of O
the O
current O
word O
. O

section 9
id pdf2json/2021.acl-long.286.pdf.json
results O
on O
the O
development O
set O
were O
used O
to O
find O
the O
best O
model O
to O
be O
evaluated O
on O
the O
test O
set O
. O

section 9
id pdf2json/2021.acl-long.286.pdf.json
we O
used O
the O
data O
from O
( O
hough O
and O
schlangen O
, O
2017 O
) O
for O
asr O
hypotheses O
– O
this O
was O
generated O
by O
a O
free O
trial O
version O
of O
ibm O
’ O
s O
watson O
speechto-text O
service O
for O
incremental O
asr O
. O

section 9
id pdf2json/2021.acl-long.286.pdf.json
the O
service O
offers O
good O
quality O
asr O
on O
noisy O
data-on O
our O
selected O
held-out O
data O
on O
switchboard O
, O
and O
the O
average O
wer O
is O
26.5 O
% O
. O

section 9
id pdf2json/2021.acl-long.286.pdf.json
the O
watson O
service O
, O
crucially O
for O
our O
task O
, O
does O
not O
filter O
out O
hesitation O
markers O
or O
disfluencies O
( O
baumann O
et O
al. O
, O
2017 O
) O
. O

section 9
id pdf2json/2021.acl-long.286.pdf.json
the O
service O
delivers O
results O
incrementally O
, O
so O
silence-based O
endpointing O
is O
not O
used O
. O

section 9
id pdf2json/2021.acl-long.286.pdf.json
it O
also O
outputs O
word O
timings O
, O
which O
are O
close O
enough O
to O
the O
source O
timings O
to O
use O
as O
features O
in O
the O
live O
version O
of O
our O
system O
. O

section 9
id pdf2json/2021.acl-long.286.pdf.json
the O
word O
embedding O
for O
lstm O
was O
initialised O
with O
50-dimensional O
embedding O
trained O
on O
google O
news O
( O
mikolov O
et O
al. O
, O
2013 O
) O
. O

section 9
id pdf2json/2021.acl-long.286.pdf.json
the O
model O
has O
been O
implemented O
using O
tensorflow O
2.1 O
. O

section 9
id pdf2json/2021.acl-long.286.pdf.json
we O
train O
all O
models O
for O
a O
maximum O
of O
50 O
epochs O
; O
otherwise O
, O
stop O
training O
if O
there O
is O
no O
improvement O
on O
the O
best O
score O
on O
the O
validation O
set O
after O
7 O
epochs O
. O

section 9
id pdf2json/2021.acl-long.286.pdf.json
a O
large O
version O
of O
the O
pre-trained O
bert O
is O
used O
with O
340m O
parameters O
( O
24-layer O
blocks O
, O
16 O
self- O
attention O
heads O
, O
and O
1024 O
hidden-size O
) O
for O
the O
model O
. O

section 9
id pdf2json/2021.acl-long.286.pdf.json
in O
our O
analysis O
, O
when O
fine-tuning O
bert O
, O
we O
followed O
the O
hyper-parameters O
of O
( O
devlin O
et O
al. O
, O
2019 O
) O
. O

section 9
id pdf2json/2021.acl-long.286.pdf.json
since O
the O
datasets O
we O
use O
are O
tokenized O
, O
and O
each O
token O
has O
a O
matching O
tag O
, O
we O
adopt O
the O
directions O
provided O
by O
( O
devlin O
et O
al. O
, O
2019 O
) O
to O
deal O
with O
the O
sub-tokenization O
of O
bert O
: O
to O
determine O
its O
label O
, O
the O
scores O
of O
the O
first O
sub-token O
are O
used O
, O
and O
further O
sub-token O
scores O
are O
discarded O
. O

section 9
id pdf2json/2021.acl-long.286.pdf.json
data O
we O
use O
standard O
switchboard O
training O
data O
( O
all O
conversation O
numbers O
starting O
sw2* O
, O
sw3 O
* O
in O
the O
penn O
treebank O
iii O
release O
: O
100k O
utterances O
, O
650k O
words O
) O
and O
use O
standard O
held-out O
data O
( O
ptb O
iii O
files O
sw4 O
[ O
5-9 O
] O
* O
: O
6.4k O
utterances O
, O
49k O
words O
) O
as O
our O
validation O
set O
. O

section 9
id pdf2json/2021.acl-long.286.pdf.json
we O
test O
on O
the O
standard O
test O
data O
( O
ptb O
iii O
files O
4 O
[ O
0-1 O
] O
* O
) O
with O
partial O
words O
and O
punctuation O
stripped O
away O
from O
all O
files O
. O

section 9
id pdf2json/2021.acl-long.286.pdf.json
we O
only O
choose O
a O
subset O
of O
the O
held-out O
and O
test O
data O
for O
the O
asr O
results O
in O
assessment O
, O
whereby O
both O
channels O
achieve O
below O
40 O
percent O
wer O
to O
ensure O
good O
separation- O
this O
left O
us O
with O
18 O
dialogues O
in O
validation O
data O
and O
17 O
dialogues O
for O
test O
data O
. O

section 10
id pdf2json/2021.acl-long.286.pdf.json
we O
calculate O
f1 O
accuracy O
for O
repair O
onset O
detection O
frps O
and O
for O
edit O
term O
words O
fe O
, O
which O
includes O
interregna O
and O
frm O
for O
reparandum O
detection O
. O

section 10
id pdf2json/2021.acl-long.286.pdf.json
performing O
the O
task O
live O
, O
on O
hypotheses O
of O
speech O
recognition O
that O
may O
not O
be O
quite O
equivalent O
to O
the O
annotated O
gold-standard O
transcription O
involves O
the O
use O
of O
time-based O
local O
accuracy O
metrics O
in O
a O
time O
window O
( O
i.e. O
, O
within O
this O
time O
frame O
, O
has O
a O
disfluency O
been O
detected O
, O
even O
if O
not O
on O
the O
identical O
words O
? O

section 10
id pdf2json/2021.acl-long.286.pdf.json
) O
-we O
, O
therefore O
, O
measure O
the O
f1 O
score O
over O
10-second O
windows O
of O
each O
speaker O
’ O
s O
channel O
. O

section 10
id pdf2json/2021.acl-long.286.pdf.json
for O
incremental O
performance O
, O
we O
measure O
latency O
and O
output O
stability O
over O
time O
. O

section 10
id pdf2json/2021.acl-long.286.pdf.json
we O
use O
the O
first O
time O
to O
detection O
( O
ftd O
) O
metric O
of O
( O
zwarts O
et O
al. O
, O
2010 O
) O
for O
latency O
: O
the O
average O
latency O
( O
in O
number O
of O
words O
) O
before O
the O
first O
detection O
of O
a O
gold O
standard O
repair O
onset O
or O
edit O
term O
word O
. O

section 10
id pdf2json/2021.acl-long.286.pdf.json
for O
stability O
, O
we O
evaluate O
the O
edit O
overhead O
( O
eo O
) O
of O
output O
labels O
( O
baumann O
et O
al. O
, O
2011 O
) O
, O
the O
proportion O
of O
the O
unnecessary O
edits O
( O
insertions O
and O
deletions O
) O
required O
to O
achieve O
the O
final O
labels O
produced O
by O
the O
model O
, O
with O
perfect O
performance O
being O
0 O
% O
. O

section 11
id pdf2json/2021.acl-long.286.pdf.json
we O
compare O
our O
incrementalised O
bert O
model O
against O
a O
number O
of O
existing O
baselines O
, O
largely O
from O
existing O
incremental O
disfluency O
detection O
systems O
trained O
and O
tested O
on O
the O
same O
data O
: O
stir O
( O
hp O
’ O
14/hs O
’ O
15/phh O
’ O
18 O
) O
: O
hough O
and O
purver O
( O
2014 O
) O
’ O
s O
strongly O
incremental O
repair O
detection O
( O
stir O
) O
non-deep O
model O
using O
n-gram O
language O
model O
features O
in O
a O
pipeline O
of O
random O
forest O
classifiers O
. O

section 11
id pdf2json/2021.acl-long.286.pdf.json
the O
reparandum O
is O
detected O
by O
a O
backward O
search O
, O
showing O
robustness O
for O
longer O
lengths O
of O
repair O
compared O
to O
deep O
sequence O
tagging O
models O
( O
purver O
et O
al. O
, O
2018 O
) O
. O

section 11
id pdf2json/2021.acl-long.286.pdf.json
a O
state-ofthe-art O
incremental O
model O
on O
pre-segmented O
transcripts O
. O

section 11
id pdf2json/2021.acl-long.286.pdf.json
rnn O
( O
hs O
’ O
15 O
) O
: O
( O
hough O
and O
schlangen O
, O
2015 O
) O
’ O
s O
rnn-based O
model O
, O
the O
first O
deep O
learning-based O
incremental O
disfluency O
detection O
model O
using O
the O
same O
tagset O
as O
in O
our O
model O
. O

section 11
id pdf2json/2021.acl-long.286.pdf.json
results O
from O
purver O
et O
al O
. O

section 11
id pdf2json/2021.acl-long.286.pdf.json
( O
2018 O
) O
are O
used O
, O
which O
reproduced O
the O
model O
with O
some O
degradation O
in O
the O
results O
. O

section 11
id pdf2json/2021.acl-long.286.pdf.json
lstm O
: O
an O
lstm O
version O
of O
hough O
and O
schlangen O
( O
2015 O
) O
on O
pre-segmented O
transcripts O
lstm O
joint O
tagset O
( O
hs O
’ O
17 O
) O
hough O
and O
schlangen O
( O
2017 O
) O
’ O
s O
model O
, O
which O
simultaneously O
predicts O
utterance O
segmentation O
using O
a O
joint O
tag O
set O
of O
utterance O
segmentation O
tags O
and O
disfluency O
tags O
, O
the O
latter O
of O
which O
is O
the O
same O
as O
our O
own O
. O

section 11
id pdf2json/2021.acl-long.286.pdf.json
this O
is O
the O
only O
other O
work O
to O
use O
word O
timing O
information O
and O
to O
be O
testable O
on O
asr O
results O
. O

section 11
id pdf2json/2021.acl-long.286.pdf.json
lstm-mtl O
( O
sel O
’ O
18 O
) O
shalyminov O
et O
al O
. O

section 11
id pdf2json/2021.acl-long.286.pdf.json
( O
2018 O
) O
’ O
s O
multi-task O
learning O
model O
, O
which O
tags O
according O
to O
our O
tag O
set O
but O
simultaneously O
does O
language O
modelling O
by O
predicting O
the O
probability O
of O
the O
current O
word O
given O
the O
history O
. O

section 11
id pdf2json/2021.acl-long.286.pdf.json
also O
adds O
ground-truth O
pos O
tags O
to O
input O
. O

section 11
id pdf2json/2021.acl-long.286.pdf.json
lstm-mtl O
( O
rh O
’ O
20 O
) O
: O
rohanian O
and O
hough O
( O
2020 O
) O
’ O
s O
multi-task O
learning O
model O
, O
which O
simultaneously O
predicts O
utterance O
segmentation O
, O
pos O
tags O
and O
language O
model O
probabilities O
, O
exhibiting O
state-of-the-art O
results O
for O
a O
strictly O
incremental O
deep O
model O
. O

section 11
id pdf2json/2021.acl-long.286.pdf.json
the O
model O
is O
used O
as O
described O
by O
the O
authors O
and O
also O
here O
with O
the O
addition O
of O
timing O
information O
and O
gold O
standard O
pos O
information O
( O
as O
opposed O
to O
simultaneously O
predicted O
pos O
tags O
) O
. O

section 11
id pdf2json/2021.acl-long.286.pdf.json
it O
is O
also O
applied O
to O
asr O
results O
as O
it O
is O
a O
suitable O
model O
to O
do O
so O
. O

section 11
id pdf2json/2021.acl-long.286.pdf.json
this O
same O
model O
provides O
the O
automatic O
live O
utterance O
segmentation O
in O
our O
own O
model O
. O

section 12
id pdf2json/2021.acl-long.286.pdf.json
the O
results O
in O
terms O
of O
the O
final O
output O
of O
our O
best O
performing O
incremental O
bert O
system O
in O
the O
three O
testing O
regimes O
versus O
its O
competitors O
is O
shown O
in O
table O
1.1 O
we O
found O
our O
best O
model O
was O
the O
add-m O
trained O
model O
, O
and O
the O
best O
decoding O
strategy O
was O
using O
top-p O
sampling O
for O
predicting O
future O
words O
. O

section 12
id pdf2json/2021.acl-long.286.pdf.json
disfluency O
detection O
on O
transcripts O
for O
repair O
detection O
, O
our O
system O
’ O
s O
best O
frps O
score O
for O
detecting O
repair O
onsets O
on O
pre-segmented O
transcripts O
at O
0.853 O
beats O
state-of-the-art O
incremental O
systems O
. O

section 12
id pdf2json/2021.acl-long.286.pdf.json
this O
performance O
degrades O
using O
automatic O
segmentation O
to O
0.802 O
, O
a O
state-of-the-art O
result O
for O
this O
setting O
. O

section 12
id pdf2json/2021.acl-long.286.pdf.json
its O
frm O
accuracy O
of O
0.757 O
on O
reparandum O
words O
on O
pre-segmented O
transcripts O
is O
only O
beaten O
by O
hp O
’ O
14/phh O
’ O
18 O
model O
using O
word O
and O
pos O
input O
, O
making O
it O
a O
state-of-the-art O
strictly O
incremental O
deep O
model O
. O

section 12
id pdf2json/2021.acl-long.286.pdf.json
this O
performance O
degrades O
to O
0.678 O
on O
raw O
transcripts O
but O
is O
a O
state-of-the-art O
result O
for O
this O
setting O
. O

section 12
id pdf2json/2021.acl-long.286.pdf.json
in O
terms O
of O
edit O
term O
detection O
, O
stateof-the-art O
detection O
results O
of O
0.960 O
and O
0.944 O
are O
achieved O
on O
the O
pre-segmented O
and O
unsegmented O
settings O
, O
improving O
over O
the O
existing O
benchmarks O
of O
hp O
’ O
14 O
and O
rh O
’ O
20 O
. O

section 12
id pdf2json/2021.acl-long.286.pdf.json
these O
results O
suggest O
we O
have O
achieved O
the O
aim O
of O
a O
strictly O
incremental O
model O
achieving O
high O
final O
accuracies O
. O

section 12
id pdf2json/2021.acl-long.286.pdf.json
disfluency O
detection O
on O
asr O
results O
using O
the O
asr O
results O
from O
hs O
’ O
17 O
for O
comparison O
, O
a O
significant O
improvement O
can O
be O
seen O
over O
the O
previously O
reported O
results O
on O
frps O
and O
fe O
per O
10-second O
window O
, O
improving O
from O
0.557 O
to O
0.605 O
and O
from O
0.727 O
to O
0.809 O
respectively O
. O

section 12
id pdf2json/2021.acl-long.286.pdf.json
given O
the O
previously O
reported O
best O
system O
gave O
strong O
correlations O
in O
terms O
of O
real O
repair O
rates O
, O
this O
is O
encouraging O
that O
our O
system O
could O
be O
very O
useful O
in O
a O
live O
setting O
. O

section 13
id pdf2json/2021.acl-long.286.pdf.json
the O
purpose O
of O
this O
paper O
was O
to O
adapt O
a O
highperforming O
, O
non-incremental O
model O
for O
incremental O
operation O
. O

section 13
id pdf2json/2021.acl-long.286.pdf.json
as O
can O
be O
seen O
in O
table O
2 O
and O
in O
fig O
. O

section 13
id pdf2json/2021.acl-long.286.pdf.json
3 O
, O
while O
our O
bert O
model O
with O
top-p O
sample O
utterance O
prediction O
outperforms O
the O
multi-task O
1experiments O
are O
reproducible O
from O
https O
: O
//github O
. O

section 13
id pdf2json/2021.acl-long.286.pdf.json
com/mortezaro/tr-disfluency O
model O
and O
vanilla O
lstm O
model O
in O
terms O
of O
final O
output O
accuracy O
, O
its O
incremental O
output O
stability O
is O
slightly O
below O
its O
competitors O
, O
with O
the O
best O
edit O
overhead O
of O
63 O
% O
unnecessary O
edits O
versus O
25 O
% O
( O
lstm O
joint O
tagset O
( O
hs O
’ O
17 O
) O
) O
and O
42 O
% O
( O
lstmmtl O
( O
rh O
’ O
20 O
) O
) O
on O
asr O
results O
, O
meaning O
the O
output O
is O
slightly O
, O
though O
not O
severely O
, O
more O
jittery O
. O

section 13
id pdf2json/2021.acl-long.286.pdf.json
of O
the O
prophecy-based O
approaches O
, O
we O
found O
the O
top-p O
sampling O
method O
gave O
the O
most O
stable O
results O
( O
eo=61 O
% O
with O
chunk O
training O
, O
eo=60 O
% O
with O
add-m O
training O
) O
and O
beam O
search O
gave O
the O
least O
stable O
. O

section 13
id pdf2json/2021.acl-long.286.pdf.json
as O
shown O
in O
fig O
. O

section 13
id pdf2json/2021.acl-long.286.pdf.json
3 O
, O
while O
the O
constant O
latency O
approaches O
offer O
large O
advantages O
in O
eo O
over O
prophecy-based O
models O
on O
transcripts O
, O
that O
advantage O
disappears O
on O
asr O
results O
, O
where O
the O
prophecy O
models O
generally O
outperform O
them O
. O

section 13
id pdf2json/2021.acl-long.286.pdf.json
as O
can O
be O
seen O
in O
table O
2 O
, O
there O
is O
a O
slight O
improvement O
in O
stability O
across O
all O
systems O
using O
the O
add-m O
training O
regime O
for O
final O
output O
and O
incremental O
performance O
. O

section 13
id pdf2json/2021.acl-long.286.pdf.json
in O
terms O
of O
latency O
, O
results O
are O
even O
more O
encouraging O
, O
with O
the O
best O
ftd O
for O
rps O
of O
0.31 O
words O
( O
versus O
0.03 O
and O
0.07 O
) O
on O
transcripts O
, O
which O
shows O
a O
relatively O
short O
latency O
of O
detecting O
the O
repair O
for O
the O
first O
time– O
this O
suggests O
a O
responsive O
, O
sensitive O
system O
. O

section 14
id pdf2json/2021.acl-long.286.pdf.json
we O
conduct O
an O
error O
analysis O
in O
terms O
of O
performance O
on O
different O
repair O
types O
and O
in O
terms O
of O
repairs O
with O
different O
lengths O
. O

section 14
id pdf2json/2021.acl-long.286.pdf.json
table O
3 O
shows O
the O
performance O
in O
terms O
of O
frps O
score O
on O
detecting O
repairs O
of O
the O
three O
different O
types O
: O
verbatim O
repeats O
, O
substitutions O
, O
and O
deletes O
( O
restarts O
) O
. O

section 14
id pdf2json/2021.acl-long.286.pdf.json
our O
bert O
model O
performs O
best O
, O
either O
jointly O
or O
uniquely O
, O
across O
all O
three O
types O
, O
with O
a O
gain O
of O
0.06 O
over O
its O
nearest O
competitors O
for O
substitutions O
and O
deletes O
. O

section 14
id pdf2json/2021.acl-long.286.pdf.json
through O
large-scale O
training O
, O
the O
enhanced O
linguistic O
knowledge O
equips O
it O
to O
recognize O
the O
syntactic O
and O
lexical O
parallelism O
in O
more O
complex O
repairs O
while O
retaining O
high O
accuracy O
on O
repeats O
. O

section 14
id pdf2json/2021.acl-long.286.pdf.json
table O
4 O
shows O
the O
degradation O
in O
performance O
in O
detecting O
repairs O
of O
different O
lengths O
. O

section 14
id pdf2json/2021.acl-long.286.pdf.json
with O
add-m O
training O
, O
the O
bert O
model O
degrades O
less O
and O
performs O
( O
joint O
) O
best O
on O
all O
lengths O
and O
nested O
disfluencies O
. O

section 14
id pdf2json/2021.acl-long.286.pdf.json
while O
the O
performance O
on O
length O
five O
repairs O
is O
considerably O
better O
than O
the O
other O
deep O
models O
, O
the O
0.187 O
accuracy O
on O
length O
six O
repairs O
is O
what O
gives O
it O
a O
slight O
disadvantage O
compared O
to O
the O
hp O
’ O
14 O
explicit O
backtracking O
system O
( O
reported O
as O
high O
as O
0.500 O
in O
phh O
’ O
18 O
) O
, O
which O
likely O
accounts O
for O
the O
lower O
frm O
score O
despite O
the O
superior O
frps O
score O
of O
our O
system O
. O

section 15
id pdf2json/2021.acl-long.286.pdf.json
our O
incremental O
gpt-2 O
and O
bert-driven O
system O
performs O
well O
at O
detecting O
repair O
disfluencies O
on O
pre-segmented O
and O
unsegmented O
transcripts O
, O
achieving O
state-of-the-art O
results O
for O
a O
strictly O
incremental O
repair O
onset O
detection O
. O

section 15
id pdf2json/2021.acl-long.286.pdf.json
our O
system O
is O
competitive O
at O
reparadnum O
word O
detection O
and O
achieves O
state-of-the-art O
results O
in O
edit O
term O
detection O
. O

section 15
id pdf2json/2021.acl-long.286.pdf.json
the O
results O
on O
asr O
transcripts O
are O
also O
state-of-the-art O
. O

section 15
id pdf2json/2021.acl-long.286.pdf.json
the O
high O
sequence-final O
performance O
comes O
at O
the O
expense O
of O
marginally O
increased O
jitter O
in O
the O
word-by-word O
output O
, O
but O
with O
sensitive O
and O
fast O
repair O
detection O
, O
on O
average O
first O
detecting O
the O
repair O
under O
a O
third O
of O
a O
second O
after O
the O
end O
of O
the O
repair O
onset O
word O
. O

section 15
id pdf2json/2021.acl-long.286.pdf.json
these O
results O
suggest O
it O
is O
beginning O
to O
enjoy O
the O
best O
of O
both O
worlds O
in O
leveraging O
the O
right-ward O
context O
which O
bert O
uses O
for O
its O
high O
performance O
, O
while O
the O
continuation O
predictions O
from O
the O
gpt-2 O
model O
are O
good O
enough O
to O
allow O
good O
incremental O
performance O
before O
the O
true O
right-ward O
context O
is O
available O
. O

section 15
id pdf2json/2021.acl-long.286.pdf.json
the O
linguistic O
knowledge O
in O
the O
bert O
model O
allows O
it O
to O
recognize O
parallelism O
in O
reparandum O
and O
repair O
phases O
and O
the O
absence O
thereof O
to O
increase O
performance O
on O
detecting O
substitution O
and O
delete O
repairs O
. O

section 15
id pdf2json/2021.acl-long.286.pdf.json
this O
improvement O
to O
existing O
deep O
disfluency O
detection O
models O
, O
and O
, O
with O
appropriate O
use O
of O
open-ended O
language O
generation O
techniques O
with O
a O
gpt-2 O
language O
model O
, O
its O
good O
incremental O
performance O
, O
is O
consistent O
with O
a O
growing O
body O
of O
work O
( O
heeman O
and O
allen O
, O
1999 O
; O
johnson O
and O
charniak O
, O
2004 O
; O
zwarts O
et O
al. O
, O
2010 O
; O
hough O
and O
purver O
, O
2014 O
; O
shalyminov O
et O
al. O
, O
2018 O
; O
rohanian O
and O
hough O
, O
2020 O
) O
, O
showing O
good O
language O
modelling O
can O
lead O
to O
good O
disfluency O
detection O
, O
as O
they O
are O
inherently O
part O
of O
the O
same O
process O
. O

section 15
id pdf2json/2021.acl-long.286.pdf.json
our O
system O
still O
fails O
to O
detect O
longer O
repairs O
compared O
to O
an O
explicit O
backtracking O
mechanism O
like O
( O
hough O
and O
purver O
, O
2014 O
) O
. O

section 15
id pdf2json/2021.acl-long.286.pdf.json
while O
the O
vanishing O
gradient O
problem O
is O
partly O
overcome O
here O
, O
the O
strictly O
left-to-right O
constraint O
on O
decoding O
puts O
memory O
limitations O
on O
any O
repair O
detection O
system O
. O

section 15
id pdf2json/2021.acl-long.286.pdf.json
in O
future O
, O
we O
will O
explore O
efficient O
ways O
to O
navigate O
this O
space O
whilst O
not O
filtering O
out O
rarer O
repair O
forms O
. O

section 15
id pdf2json/2021.acl-long.286.pdf.json
the O
results O
on O
asr O
results O
show O
our O
disfluency O
detection O
system O
is O
ready O
for O
use O
in O
a O
live O
setting O
with O
a O
good O
degree O
of O
accuracy O
, O
and O
work O
is O
currently O
underway O
to O
use O
it O
to O
help O
detect O
a O
variety O
of O
different O
cognitive O
conditions O
, O
including O
alzheimer O
’ O
s O
disease O
, O
in O
a O
live O
diagnostic O
system O
. O

section 16
id pdf2json/2021.acl-long.286.pdf.json
we O
thank O
the O
anonymous O
acl-ijcnlp O
reviewers O
for O
their O
helpful O
comments O
and O
matthew O
purver O
for O
his O
continuous O
support O
and O
supervision O
on O
the O
wider O
project O
. O

section TITLE
id pdf2json/2021.acl-long.397.pdf.json
from O
paraphrasing O
to O
semantic O
parsing O
: O
unsupervised O
semantic O
parsing O
via O
synchronous O
semantic O
decoding O

section ABSTRACT
id pdf2json/2021.acl-long.397.pdf.json
semantic O
parsing O
is O
challenging O
due O
to O
the O
structure O
gap O
and O
the O
semantic O
gap O
between O
utterances O
and O
logical O
forms O
. O

section ABSTRACT
id pdf2json/2021.acl-long.397.pdf.json
in O
this O
paper O
, O
we O
propose O
an O
unsupervised O
semantic O
parsing O
method O
– O
synchronous O
semantic O
decoding O
( O
ssd O
) O
, O
which O
can O
simultaneously O
resolve O
the O
semantic O
gap O
and O
the O
structure O
gap O
by O
jointly O
leveraging O
paraphrasing O
and O
grammarconstrained O
decoding O
. O

section ABSTRACT
id pdf2json/2021.acl-long.397.pdf.json
specifically O
, O
we O
reformulate O
semantic O
parsing O
as O
a O
constrained O
paraphrasing O
problem O
: O
given O
an O
utterance O
, O
our O
model O
synchronously O
generates O
its O
canonical O
utterance1 O
and O
meaning O
representation O
. O

section ABSTRACT
id pdf2json/2021.acl-long.397.pdf.json
during O
synchronous O
decoding O
: O
the O
utterance O
paraphrasing O
is O
constrained O
by O
the O
structure O
of O
the O
logical O
form O
, O
therefore O
the O
canonical O
utterance O
can O
be O
paraphrased O
controlledly O
; O
the O
semantic O
decoding O
is O
guided O
by O
the O
semantics O
of O
the O
canonical O
utterance O
, O
therefore O
its O
logical O
form O
can O
be O
generated O
unsupervisedly O
. O

section ABSTRACT
id pdf2json/2021.acl-long.397.pdf.json
experimental O
results O
show O
that O
ssd O
is O
a O
promising O
approach O
and O
can O
achieve O
competitive O
unsupervised O
semantic O
parsing O
performance O
on O
multiple O
datasets O
. O

section 0
id pdf2json/2021.acl-long.397.pdf.json
proceedings O
of O
the O
59th O
annual O
meeting O
of O
the O
association O
for O
computational O
linguistics O
and O
the O
11th O
international O
joint O
conference O
on O
natural O
language O
processing O
, O
pages O
5110–5121 O
august O
1–6 O
, O
2021 O
. O

section 0
id pdf2json/2021.acl-long.397.pdf.json
©2021 O
association O
for O
computational O
linguistics O
5110 O

section 1
id pdf2json/2021.acl-long.397.pdf.json
semantic O
parsing O
aims O
to O
translate O
natural O
language O
utterances O
to O
their O
formal O
meaning O
representations O
, O
such O
as O
lambda O
calculus O
( O
zettlemoyer O
and O
collins O
, O
2005 O
; O
wong O
and O
mooney O
, O
2007 O
) O
, O
funql O
( O
kate O
et O
al. O
, O
2005 O
; O
lu O
et O
al. O
, O
2008 O
) O
, O
and O
sql O
queries O
. O

section 1
id pdf2json/2021.acl-long.397.pdf.json
currently O
, O
most O
neural O
semantic O
parsers O
( O
dong O
and O
lapata O
, O
2016 O
; O
chen O
et O
al. O
, O
2018b O
; O
zhao O
et O
al. O
, O
2020 O
; O
shao O
et O
al. O
, O
2020 O
) O
model O
semantic O
parsing O
as O
a O
sequence O
to O
sequence O
translation O
task O
via O
encoder-decoder O
framework O
. O

section 1
id pdf2json/2021.acl-long.397.pdf.json
∗corresponding O
author O
1canonical O
utterances O
are O
pseudo-language O
representations O
of O
logical O
forms O
, O
which O
have O
the O
synchronous O
structure O
of O
logical O
forms O
. O

section 1
id pdf2json/2021.acl-long.397.pdf.json
( O
berant O
and O
liang O
, O
2014 O
; O
xiao O
et O
al. O
, O
2016 O
; O
su O
and O
yan O
, O
2017 O
; O
cao O
et O
al. O
, O
2020 O
) O
semantic O
parsing O
is O
a O
challenging O
task O
due O
to O
the O
structure O
gap O
and O
the O
semantic O
gap O
between O
natural O
language O
utterances O
and O
logical O
forms O
. O

section 1
id pdf2json/2021.acl-long.397.pdf.json
for O
structure O
gap O
, O
because O
utterances O
are O
usually O
word O
sequences O
and O
logical O
forms O
are O
usually O
trees/graphs O
constrained O
by O
specific O
grammars O
, O
a O
semantic O
parser O
needs O
to O
learn O
the O
complex O
structure O
transformation O
rules O
between O
them O
. O

section 1
id pdf2json/2021.acl-long.397.pdf.json
for O
semantic O
gap O
, O
because O
the O
flexibility O
of O
natural O
languages O
, O
the O
same O
meaning O
can O
be O
expressed O
using O
very O
different O
utterances O
, O
a O
semantic O
parser O
needs O
be O
able O
to O
map O
various O
expressions O
to O
their O
semantic O
form O
. O

section 1
id pdf2json/2021.acl-long.397.pdf.json
to O
address O
the O
structure O
gap O
and O
the O
semantic O
gap O
, O
current O
semantic O
parsers O
usually O
rely O
on O
a O
large O
amount O
of O
labeled O
data O
, O
often O
resulting O
in O
data O
bottleneck O
problem O
. O

section 1
id pdf2json/2021.acl-long.397.pdf.json
previous O
studies O
have O
found O
that O
the O
structure O
gap O
and O
the O
semantic O
gap O
can O
be O
alleviated O
by O
leveraging O
external O
resources O
, O
therefore O
the O
reliance O
on O
data O
can O
be O
reduced O
. O

section 1
id pdf2json/2021.acl-long.397.pdf.json
for O
structure O
gap O
, O
previous O
studies O
found O
that O
constrained O
decoding O
can O
effectively O
constrain O
the O
output O
structure O
by O
injecting O
grammars O
of O
logical O
forms O
and O
facts O
in O
knowledge O
bases O
during O
inference O
. O

section 1
id pdf2json/2021.acl-long.397.pdf.json
for O
example O
, O
the O
grammar-based O
neural O
semantic O
parsers O
( O
xiao O
et O
al. O
, O
2016 O
; O
yin O
and O
neubig O
, O
2017 O
) O
and O
the O
constrained O
decoding O
algorithm O
( O
krishnamurthy O
et O
al. O
, O
2017 O
) O
. O

section 1
id pdf2json/2021.acl-long.397.pdf.json
for O
semantic O
gap O
, O
previous O
studies O
have O
found O
that O
paraphrasing O
is O
an O
effective O
technique O
for O
resolving O
the O
diversity O
of O
natural O
expressions O
. O

section 1
id pdf2json/2021.acl-long.397.pdf.json
using O
paraphrasing O
, O
semantic O
parsers O
can O
handle O
the O
different O
expressions O
of O
the O
same O
meaning O
, O
therefore O
can O
reduce O
the O
requirement O
of O
labeled O
data O
. O

section 1
id pdf2json/2021.acl-long.397.pdf.json
for O
example O
, O
supervised O
methods O
( O
berant O
and O
liang O
, O
2014 O
; O
su O
and O
yan O
, O
2017 O
) O
use O
the O
paraphrasing O
scores O
between O
canonical O
utterances O
and O
sentences O
to O
re-rank O
logical O
forms O
; O
two-stage O
( O
cao O
et O
al. O
, O
2020 O
) O
rewrites O
utterances O
to O
canonical O
utterances O
which O
can O
be O
easily O
parsed O
. O

section 1
id pdf2json/2021.acl-long.397.pdf.json
the O
main O
drawback O
of O
these O
studies O
is O
that O
they O
use O
constrained O
decoding O
and O
paraphrasing O
independently O
and O
separately O
, O
therefore O
they O
can O
only O
alleviate O
either O
semantic O
gap O
or O
structure O
gap O
. O

section 1
id pdf2json/2021.acl-long.397.pdf.json
in O
this O
paper O
, O
we O
propose O
an O
unsupervised O
semantic O
parsing O
method O
– O
synchronous O
semantic O
decoding O
( O
ssd O
) O
, O
which O
can O
simultaneously O
resolve O
the O
structure O
gap O
and O
the O
semantic O
gap O
by O
jointly O
leveraging O
paraphrasing O
and O
grammarconstrained O
decoding O
. O

section 1
id pdf2json/2021.acl-long.397.pdf.json
specifically O
, O
we O
model O
semantic O
parsing O
as O
a O
constrained O
paraphrasing O
task O
: O
given O
an O
utterance O
, O
we O
synchronously O
decode O
its O
canonical O
utterance O
and O
its O
logical O
form O
using O
a O
general O
paraphrase O
model O
, O
where O
the O
canonical O
utterance O
and O
the O
logical O
form O
share O
the O
same O
underlying O
structure O
. O

section 1
id pdf2json/2021.acl-long.397.pdf.json
based O
on O
the O
synchronous O
decoding O
, O
the O
canonical O
utterance O
generation O
can O
be O
constrained O
by O
the O
structure O
of O
logical O
form O
, O
and O
the O
logical O
form O
generation O
can O
be O
guided O
by O
the O
semantics O
of O
canonical O
form O
. O

section 1
id pdf2json/2021.acl-long.397.pdf.json
by O
modeling O
the O
interdependency O
between O
canonical O
utterance O
and O
logical O
form O
, O
and O
exploiting O
them O
through O
synchronous O
decoding O
, O
our O
method O
can O
perform O
effective O
unsupervised O
semantic O
parsing O
using O
only O
pretrained O
general O
paraphrasing O
model O
– O
no O
annotated O
data O
for O
semantic O
parsing O
is O
needed O
. O

section 1
id pdf2json/2021.acl-long.397.pdf.json
we O
conduct O
experiments O
on O
geo O
and O
overnight O
. O

section 1
id pdf2json/2021.acl-long.397.pdf.json
experimental O
results O
show O
that O
our O
method O
is O
promising O
, O
which O
can O
achieve O
competitive O
unsupervised O
semantic O
parsing O
performance O
, O
and O
can O
be O
further O
improved O
with O
external O
resources O
. O

section 1
id pdf2json/2021.acl-long.397.pdf.json
the O
main O
contributions O
of O
this O
paper O
are O
: O
• O
we O
propose O
an O
unsupervised O
semantic O
parsing O
method O
– O
synchronous O
semantic O
de- O
coding O
, O
which O
can O
simultaneously O
resolve O
the O
semantic O
gap O
and O
the O
structure O
gap O
by O
jointly O
leveraging O
paraphrasing O
and O
grammar-constrained O
semantic O
decoding O
. O

section 1
id pdf2json/2021.acl-long.397.pdf.json
• O
we O
design O
two O
effective O
synchronous O
semantic O
decoding O
algorithms O
– O
rule-level O
inference O
and O
word-level O
inference O
, O
which O
can O
generate O
paraphrases O
under O
the O
grammar O
constraints O
and O
synchronously O
decode O
meaning O
representations O
. O

section 1
id pdf2json/2021.acl-long.397.pdf.json
• O
our O
model O
achieves O
competitive O
unsupervised O
semantic O
parsing O
performance O
on O
geo O
and O
overnight O
datasets O
. O

section 2
id pdf2json/2021.acl-long.397.pdf.json
we O
now O
present O
overview O
of O
our O
synchronous O
semantic O
decoding O
algorithm O
, O
which O
can O
jointly O
leverage O
paraphrasing O
and O
grammar-constrained O
decoding O
for O
unsupervised O
semantic O
parsing O
. O

section 2
id pdf2json/2021.acl-long.397.pdf.json
given O
an O
utterance O
, O
ssd O
reformulates O
semantic O
parsing O
as O
a O
constrained O
paraphrasing O
problem O
, O
and O
synchronously O
generates O
its O
canonical O
utterance O
and O
logical O
form O
. O

section 2
id pdf2json/2021.acl-long.397.pdf.json
for O
example O
in O
fig O
. O

section 2
id pdf2json/2021.acl-long.397.pdf.json
2 O
, O
given O
“ O
how O
many O
rivers O
run O
through O
texas O
” O
, O
ssd O
generates O
“ O
what O
is O
the O
number O
of O
river O
traverse O
state0 O
” O
as O
its O
canonical O
form O
and O
answer O
( O
count O
( O
river O
( O
traverse O
2 O
( O
state0 O
) O
) O
) O
) O
as O
its O
logical O
form O
. O

section 2
id pdf2json/2021.acl-long.397.pdf.json
during O
synchronous O
decoding O
: O
the O
utterance O
paraphrase O
generation O
is O
constrained O
by O
the O
grammar O
of O
logical O
forms O
, O
therefore O
the O
canonical O
utterance O
can O
be O
generated O
controlledly O
; O
the O
logical O
form O
is O
generated O
synchronously O
with O
the O
canonical O
utterance O
via O
synchronous O
grammar O
. O

section 2
id pdf2json/2021.acl-long.397.pdf.json
logical O
form O
generation O
is O
controlled O
by O
the O
semantic O
constraints O
from O
paraphrasing O
and O
structure O
constraints O
from O
grammars O
and O
database O
schemas O
. O

section 2
id pdf2json/2021.acl-long.397.pdf.json
therefore O
the O
logical O
form O
can O
be O
generated O
unsupervisedly O
. O

section 2
id pdf2json/2021.acl-long.397.pdf.json
to O
this O
end O
, O
ssd O
needs O
to O
address O
two O
challenges O
. O

section 2
id pdf2json/2021.acl-long.397.pdf.json
firstly O
, O
we O
need O
to O
design O
paraphrasingbased O
decoding O
algorithms O
which O
can O
effectively O
impose O
grammar O
constraints O
on O
inference O
. O

section 2
id pdf2json/2021.acl-long.397.pdf.json
secondly O
, O
current O
paraphrasing O
models O
are O
trained O
on O
natural O
language O
sentences O
, O
which O
are O
different O
from O
the O
unnatural O
canonical O
utterances O
. O

section 2
id pdf2json/2021.acl-long.397.pdf.json
therefore O
ssd O
needs O
to O
resolve O
this O
style O
bias O
for O
effective O
canonical O
utterance O
generation O
. O

section 2
id pdf2json/2021.acl-long.397.pdf.json
specifically O
, O
we O
first O
propose O
two O
inference O
algorithms O
for O
constrained O
paraphrasing O
based O
syn- O
chronous O
semantic O
decoding O
: O
rule-level O
inference O
and O
word-level O
inference O
. O

section 2
id pdf2json/2021.acl-long.397.pdf.json
then O
we O
resolve O
the O
style O
bias O
of O
paraphrase O
model O
via O
adaptive O
fine-tuning O
and O
utterance O
reranking O
, O
where O
adaptive O
fine-tuning O
can O
adjust O
the O
paraphrase O
model O
to O
generate O
canonical O
utterances O
, O
and O
utterance O
reranking O
resolves O
the O
style O
bias O
by O
focusing O
more O
on O
semantic O
coherence O
. O

section 2
id pdf2json/2021.acl-long.397.pdf.json
in O
sections O
3-5 O
, O
we O
provide O
the O
details O
of O
our O
implementation O
. O

section 3
id pdf2json/2021.acl-long.397.pdf.json
given O
an O
utterance O
x O
, O
we O
turn O
semantic O
parsing O
into O
a O
constrained O
paraphrasing O
task O
. O

section 3
id pdf2json/2021.acl-long.397.pdf.json
concretely O
, O
we O
use O
synchronous O
context-free O
grammar O
as O
our O
synchronous O
grammar O
, O
which O
provides O
a O
one-to-one O
mapping O
from O
a O
logical O
form O
y O
to O
its O
canonical O
utterance O
cy O
. O

section 3
id pdf2json/2021.acl-long.397.pdf.json
the O
parsing O
task O
ŷ O
= O
argmax O
y∈y O
pparse O
( O
y|x O
) O
is O
then O
transferred O
to O
ŷ O
= O
argmax O
y∈y O
pparaphrase O
( O
c O
y|x O
) O
. O

section 3
id pdf2json/2021.acl-long.397.pdf.json
instead O
of O
directly O
parsing O
utterance O
into O
its O
logical O
form O
, O
ssd O
generates O
its O
canonical O
utterance O
and O
obtains O
its O
logical O
form O
based O
on O
the O
one-to-one O
mapping O
relation O
. O

section 3
id pdf2json/2021.acl-long.397.pdf.json
in O
following O
we O
first O
introduce O
the O
grammar O
constraints O
in O
decoding O
, O
and O
then O
present O
two O
inference O
algorithms O
for O
generating O
paraphrases O
under O
the O
grammar O
constraints O
. O

section 4
id pdf2json/2021.acl-long.397.pdf.json
synchronous O
context-free O
grammar O
( O
scfg O
) O
is O
employed O
as O
our O
synchronous O
grammar O
, O
which O
is O
widely O
used O
to O
convert O
a O
meaning O
representation O
into O
an O
unique O
canonical O
utterance O
( O
wang O
et O
al. O
, O
2015 O
; O
jia O
and O
liang O
, O
2016 O
) O
. O

section 4
id pdf2json/2021.acl-long.397.pdf.json
an O
scfg O
consists O
of O
a O
set O
of O
production O
rules O
: O
n O
→ O
〈α O
, O
β〉 O
, O
wheren O
is O
a O
non-terminal O
, O
and O
α O
and O
β O
are O
sequence O
of O
terminal O
and O
non-terminal O
symbols O
. O

section 4
id pdf2json/2021.acl-long.397.pdf.json
each O
non-terminal O
symbol O
in O
α O
is O
aligned O
to O
the O
same O
non-terminal O
symbol O
in O
β O
, O
and O
vice O
versa O
. O

section 4
id pdf2json/2021.acl-long.397.pdf.json
therefore O
, O
an O
scfg O
defines O
a O
set O
of O
joint O
derivations O
of O
aligned O
pairs O
of O
utterances O
and O
logical O
forms O
. O

section 4
id pdf2json/2021.acl-long.397.pdf.json
scfgs O
can O
provide O
useful O
constraints O
for O
semantic O
decoding O
by O
restricting O
the O
decoding O
space O
and O
exploiting O
the O
semantic O
knowledge O
: O
grammar O
constraints O
the O
grammars O
ensure O
the O
generated O
utterances/logical O
forms O
are O
grammar-legal O
. O

section 4
id pdf2json/2021.acl-long.397.pdf.json
in O
this O
way O
the O
search O
space O
can O
be O
greatly O
reduced O
. O

section 4
id pdf2json/2021.acl-long.397.pdf.json
for O
example O
, O
when O
expanding O
the O
non-terminal O
$ O
r O
in O
fig O
2 O
we O
don O
’ O
t O
need O
to O
consider O
the O
words O
“ O
run O
” O
and O
“ O
flow O
” O
, O
because O
they O
are O
not O
in O
the O
candidate O
grammar O
rules O
. O

section 4
id pdf2json/2021.acl-long.397.pdf.json
semantic O
constraints O
like O
the O
type O
checking O
in O
wang O
et O
al O
. O

section 4
id pdf2json/2021.acl-long.397.pdf.json
( O
2015 O
) O
, O
the O
constraints O
of O
knowledge O
base O
schema O
can O
be O
integrated O
to O
further O
refine O
the O
grammar O
. O

section 4
id pdf2json/2021.acl-long.397.pdf.json
the O
semantic O
constraints O
ensure O
the O
generated O
utterances/logical O
forms O
will O
be O
semantically O
valid O
. O

section 6
id pdf2json/2021.acl-long.397.pdf.json
one O
strategy O
to O
generate O
paraphrase O
under O
the O
grammar O
constraint O
is O
taking O
the O
grammar O
rule O
as O
the O
decoding O
unit O
. O

section 6
id pdf2json/2021.acl-long.397.pdf.json
grammar-based O
decoders O
have O
been O
proposed O
to O
output O
sequences O
of O
grammar O
rules O
instead O
of O
words O
( O
yin O
and O
neubig O
, O
2017 O
) O
. O

section 6
id pdf2json/2021.acl-long.397.pdf.json
like O
them O
, O
our O
rule-level O
inference O
method O
takes O
the O
grammar O
rule O
as O
the O
decoding O
unit O
. O

section 6
id pdf2json/2021.acl-long.397.pdf.json
figure O
3 O
( O
a O
) O
shows O
an O
example O
of O
our O
rule O
level O
inference O
method O
. O

section 6
id pdf2json/2021.acl-long.397.pdf.json
loc O
ate O
d O
inwhat O
thatstateis O
largest O
city O
city0 O
lake0 O
the O
located O
located O
answer O
( O
state O
( O
$ O
s O
) O
) O
answer O
( O
state O
( O
loc_1 O
( O
city0 O
) O
) O
) O
answer O
( O
state O
( O
loc_1 O
( O
lake0 O
) O
) O
) O
answer O
( O
state O
( O
loc_1 O
( O
largest O
( O
city O
) O
) O
) O
) O
answer O
( O
$ O
e O
) O
$ O
e O
→ O
$ O
s O
→ O
root O
→ O
what O
is O
$ O
e O
answer O
( O
$ O
e O
) O
state O
$ O
s O
state O
( O
$ O
s O
) O
that O
$ O
c O
located O
in O
loc_1 O
( O
$ O
c O
) O
$ O
c O
→ O
city0 O
city0 O
root O
→ O
< O
what O
is O
$ O
e O
, O
answer O
( O
$ O
e O
) O
> O
$ O
e O
→ O
< O
state O
$ O
s O
, O
state O
( O
$ O
s O
) O
> O
$ O
s O
→ O
< O
that O
$ O
c O
located O
in O
, O
loc_1 O
( O
$ O
c O
) O
> O
$ O
c O
→ O
< O
city0 O
, O
city0 O
> O
( O
a O
) O
rule-level O
inference O
$ O
e O
→ O
$ O
s O
→ O
root O
→ O
what O
is O
$ O
e O
answer O
( O
$ O
e O
) O
state O
$ O
s O
state O
( O
$ O
s O
) O
that O
$ O
c O
located O
in O
loc_1 O
( O
$ O
c O
) O
$ O
c O
→ O
city0 O
city0 O
loc O
ate O
d O
inwhat O
thatstateis O
largest O
city O
city0 O
lake0 O
the O
located O
located O
answer O
( O
state O
( O
$ O
s O
) O
) O
answer O
( O
state O
( O
loc_1 O
( O
city0 O
) O
) O
) O
answer O
( O
state O
( O
loc_1 O
( O
lake0 O
) O
) O
) O
answer O
( O
state O
( O
loc_1 O
( O
largest O
( O
city O
) O
) O
) O
) O
answer O
( O
$ O
e O
) O
$ O
e O
→ O
$ O
s O
→ O
root O
→ O
what O
is O
$ O
e O
answer O
( O
$ O
e O
) O
state O
$ O
s O
state O
( O
$ O
s O
) O
that O
$ O
c O
located O
in O
loc_1 O
( O
$ O
c O
) O
$ O
c O
→ O
city0 O
city0 O
( O
b O
) O
word-level O
inference O
figure O
3 O
: O
from O
the O
utterance O
“ O
which O
state O
is O
city0 O
i O
” O
, O
two O
inference O
methods O
generate O
its O
canonical O
utterance O
“ O
what O
is O
state O
that O
city0 O
located O
in O
” O
and O
its O
logical O
form O
answer O
( O
state O
( O
loc O
1 O
( O
city0 O
) O
) O
) O
. O

section 6
id pdf2json/2021.acl-long.397.pdf.json
the O
ways O
they O
handle O
non-terminal O
$ O
c O
which O
is O
not O
at O
the O
end O
of O
utterance-side O
production O
rule O
are O
represented O
by O
purple O
lines O
. O

section 6
id pdf2json/2021.acl-long.397.pdf.json
algorithm O
1 O
: O
rule-level O
inference O
input O
: O
input O
utterance O
x O
, O
paraphrasing O
model O
para O
, O
beam O
size O
b O
, O
maximum O
output O
length O
l O
, O
scfg O
rules O
r O
, O
maximum O
search O
depth O
k O
; O
1 O
beam0 O
← O
{ O
〈s〉 O
} O
2 O
outputs← O
{ O
} O
3 O
for O
t O
= O
1 O
to O
l O
do O
4 O
for O
hypothesis O
c O
in O
beamt−1 O
do O
5 O
for O
r O
in O
expand O
rules O
for O
c O
do O
6 O
if O
all O
non-terminals O
in O
rβ O
are O
on O
the O
right O
then O
7 O
c′ O
← O
expand O
( O
c O
, O
r O
) O
8 O
beamt O
← O
beamt O
∪ O
{ O
c′ O
} O
9 O
else O
10 O
c′ O
← O
expand O
( O
c O
, O
r O
) O
11 O
beamc O
′ O
t O
← O
{ O
c′ O
} O
12 O
for O
k O
= O
1 O
to O
k O
do O
13 O
for O
hypothesis O
h O
in O
beamc O
′ O
t+k−1 O
do O
14 O
rh← O
expand O
rules O
for O
h O
’ O
s O
first O
non-terminal O
15 O
beamc O
′ O
t+k O
←beamc O
′ O
t+k O
∪ O
expand O
( O
h O
, O
rh O
) O
16 O
beamc O
′ O
t+k O
← O
nbest O
( O
beamc O
′ O
t+k O
, O
b O
) O
17 O
move O
utterances O
from O
beamc O
′ O
t+k O
to O
beamt+k O
, O
if O
non-terminals O
are O
on O
the O
right O
of O
the O
utterances O
. O

section 6
id pdf2json/2021.acl-long.397.pdf.json
18 O
beamt O
← O
nbest O
( O
beamt O
, O
b O
− O
|outputs| O
) O
19 O
move O
full O
utterances O
from O
beamt O
to O
outputs O
20 O
if O
beamt O
is O
empty O
then O
21 O
return O
outputs O
22 O
return O
outputs O
when O
the O
non-terminal O
in O
the O
utterance-side O
production O
rule O
is O
at O
the O
end O
of O
the O
rule O
( O
e.g. O
, O
$ O
e→ O
〈state O
$ O
s O
, O
state O
( O
$ O
s O
) O
〉 O
) O
, O
denoting O
the O
utteranceside O
production O
rule O
as O
rβ O
= O
[ O
w1 O
, O
w2 O
, O
... O
, O
wlr O
, O
n O
] O
, O
we O
can O
simply O
expand O
non-terminals O
in O
canonical O
utterances O
by O
this O
rule O
, O
and O
generate O
the O
canonical O
utterances O
from O
left O
to O
right O
with O
probabilities O
computed O
by O
: O
p O
( O
cy≤t O
|x O
) O
= O
p O
( O
cy O
< O
t O
|x O
) O
lr∏ O
i=1 O
pparaphrase O
( O
wi|x O
, O
cy O
< O
t O
, O
w O
< O
i O
) O
( O
1 O
) O
otherwise O
, O
we O
generate O
the O
next O
production O
rules O
to O
expand O
this O
rule O
( O
i.e. O
, O
rule O
with O
purple O
line O
) O
, O
until O
there O
is O
no O
non-terminal O
on O
the O
left O
of O
words O
, O
or O
the O
generating O
step O
reaches O
the O
depth O
of O
k. O
we O
use O
beam O
search O
during O
the O
inference O
. O

section 6
id pdf2json/2021.acl-long.397.pdf.json
the O
inference O
details O
are O
described O
in O
algorithm O
1 O
. O

section 7
id pdf2json/2021.acl-long.397.pdf.json
except O
for O
rule-level O
inference O
, O
we O
also O
propose O
a O
word-level O
inference O
algorithm O
, O
which O
generates O
paraphrases O
word O
by O
word O
under O
the O
scfg O
constraints O
. O

section 7
id pdf2json/2021.acl-long.397.pdf.json
firstly O
, O
we O
construct O
a O
deterministic O
automaton O
using O
lr O
( O
1 O
) O
parser O
( O
knuth O
, O
1965 O
) O
from O
the O
cfg O
in O
utterance O
side O
. O

section 7
id pdf2json/2021.acl-long.397.pdf.json
the O
automaton O
can O
transit O
from O
one O
state O
to O
another O
in O
response O
to O
an O
input O
. O

section 7
id pdf2json/2021.acl-long.397.pdf.json
the O
inputs O
of O
the O
automaton O
are O
words O
and O
the O
states O
of O
it O
are O
utterance/logical O
form O
segments O
. O

section 7
id pdf2json/2021.acl-long.397.pdf.json
lr O
( O
1 O
) O
parser O
peeks O
ahead O
one O
lookahead O
input O
symbol O
, O
and O
the O
state O
transition O
table O
describes O
the O
acceptable O
inputs O
and O
the O
next O
states O
. O

section 7
id pdf2json/2021.acl-long.397.pdf.json
then O
, O
in O
each O
decoding O
step O
we O
generate O
a O
word O
with O
a O
new O
state O
which O
is O
transited O
from O
previous O
state O
. O

section 7
id pdf2json/2021.acl-long.397.pdf.json
an O
example O
is O
shown O
in O
figure O
3 O
( O
b O
) O
. O

section 7
id pdf2json/2021.acl-long.397.pdf.json
only O
the O
acceptable O
words O
in O
the O
current O
state O
can O
be O
generated O
, O
and O
the O
end-of-sentence O
symbol O
can O
only O
be O
generated O
when O
reaching O
the O
final O
state O
. O

section 7
id pdf2json/2021.acl-long.397.pdf.json
beam O
search O
is O
also O
used O
in O
this O
inference O
. O

section 8
id pdf2json/2021.acl-long.397.pdf.json
the O
above O
decoding O
algorithms O
only O
rely O
on O
a O
paraphrase O
generation O
model O
, O
which O
generates O
canonical O
utterance O
and O
logical O
form O
synchronously O
for O
semantic O
parsing O
. O

section 8
id pdf2json/2021.acl-long.397.pdf.json
we O
can O
directly O
use O
general O
paraphrase O
generation O
models O
such O
as O
gpt-2 O
( O
radford O
et O
al. O
, O
2019 O
) O
, O
t5 O
( O
raffel O
et O
al. O
, O
2020 O
) O
for O
ssd O
. O

section 8
id pdf2json/2021.acl-long.397.pdf.json
however O
, O
as O
described O
in O
above O
, O
there O
exists O
a O
style O
bias O
between O
natural O
language O
sentences O
and O
canonical O
utterances O
, O
which O
hurts O
the O
performance O
of O
unsupervised O
semantic O
par- O
ing O
. O

section 8
id pdf2json/2021.acl-long.397.pdf.json
in O
this O
section O
, O
we O
describe O
how O
to O
alleviate O
this O
bias O
via O
adaptive O
fine-tuning O
. O

section 8
id pdf2json/2021.acl-long.397.pdf.json
given O
a O
text O
generation O
model O
, O
after O
pretraining O
it O
using O
paraphrase O
corpus O
, O
we O
fine-tune O
it O
using O
synthesized O
〈sentence O
, O
canonical O
utterance〉 O
pairs O
. O

section 8
id pdf2json/2021.acl-long.397.pdf.json
previous O
studies O
have O
shown O
that O
the O
pretraining O
on O
synthesized O
data O
can O
significantly O
improve O
the O
performance O
of O
semantic O
parsing O
( O
xu O
et O
al. O
, O
2020a O
; O
marzoev O
et O
al. O
, O
2020 O
; O
yu O
et O
al. O
, O
2020 O
; O
xu O
et O
al. O
, O
2020b O
) O
. O

section 8
id pdf2json/2021.acl-long.397.pdf.json
specifically O
, O
we O
design O
three O
data O
synthesis O
algorithms O
: O
1 O
) O
cus O
we O
sample O
cus O
from O
scfgs O
, O
and O
preserve O
executable O
ones O
. O

section 8
id pdf2json/2021.acl-long.397.pdf.json
as O
we O
do O
not O
have O
the O
paired O
sentences O
, O
we O
only O
fine-tune O
the O
language O
model O
of O
the O
plms O
on O
cus O
. O

section 8
id pdf2json/2021.acl-long.397.pdf.json
2 O
) O
self O
paras O
we O
use O
the O
trained O
paraphrase O
model O
to O
get O
the O
natural O
language O
paraphrases O
of O
the O
sampled O
canonical O
utterances O
to O
form O
〈sentence O
, O
canonical O
utterance〉 O
pairs O
. O

section 8
id pdf2json/2021.acl-long.397.pdf.json
3 O
) O
external O
paras O
we O
also O
use O
external O
paraphrase O
methods O
such O
as O
back O
translation O
to O
get O
the O
pairs O
. O

section 9
id pdf2json/2021.acl-long.397.pdf.json
adaptive O
fine-tuning O
resolves O
the O
style O
bias O
problem O
by O
fitting O
a O
better O
paraphrase O
model O
. O

section 9
id pdf2json/2021.acl-long.397.pdf.json
in O
this O
section O
, O
we O
propose O
an O
utterance O
reranking O
algorithm O
to O
further O
alleviate O
the O
style O
bias O
by O
reranking O
and O
selecting O
the O
best O
canonical O
form O
. O

section 9
id pdf2json/2021.acl-long.397.pdf.json
given O
the O
utterance O
x O
and O
top-n O
parsing O
results O
( O
yn O
, O
cn O
) O
, O
n O
= O
1 O
, O
2 O
, O
... O
, O
n O
, O
we O
rerank O
all O
candidates O
by O
focusing O
on O
semantic O
similarities O
between O
x O
and O
cn O
, O
so O
that O
canonical O
utterances O
can O
be O
effectively O
selected O
. O

section 9
id pdf2json/2021.acl-long.397.pdf.json
reranking O
for O
semantic O
parsing O
has O
been O
exploited O
in O
many O
previous O
studies O
( O
berant O
and O
liang O
, O
2014 O
; O
yin O
and O
neubig O
, O
2019 O
) O
. O

section 9
id pdf2json/2021.acl-long.397.pdf.json
these O
works O
employ O
reranking O
for O
canonical O
utterances O
selection O
. O

section 9
id pdf2json/2021.acl-long.397.pdf.json
differently O
, O
our O
re-ranker O
does O
not O
need O
labeled O
data O
. O

section 9
id pdf2json/2021.acl-long.397.pdf.json
formally O
, O
we O
measure O
two O
similarities O
between O
x O
and O
cn O
and O
the O
final O
reranking O
score O
is O
calculated O
by O
: O
score O
( O
x O
, O
c O
) O
= O
log O
p O
( O
c|x O
) O
+ O
srec O
( O
x O
, O
c O
) O
+ O
sasso O
( O
x O
, O
c O
) O
( O
2 O
) O
reconstruction O
score O
the O
reconstruction O
score O
measures O
the O
coherence O
and O
adequacy O
of O
the O
canonical O
utterances O
, O
using O
the O
probability O
of O
reproducing O
the O
original O
input O
sentence O
x O
from O
c O
with O
the O
trained O
paraphrasing O
model O
: O
srec O
( O
x O
, O
c O
) O
= O
log O
ppr O
( O
x|c O
) O
association O
score O
the O
association O
score O
measures O
whether O
x O
and O
c O
contain O
words O
that O
are O
likely O
to O
be O
paraphrases O
. O

section 9
id pdf2json/2021.acl-long.397.pdf.json
we O
calculate O
it O
as O
: O
sasso O
( O
x O
, O
c O
) O
= O
log O
|c|∏ O
i=1 O
|x|∑ O
j=0 O
p O
( O
ci|xj O
) O
a O
( O
j|i O
) O
+ O
log O
|x|∏ O
j=1 O
|c|∑ O
i=0 O
p O
( O
xj O
|ci O
) O
a O
( O
i|j O
) O
( O
3 O
) O
in O
which O
, O
p O
( O
ci|xj O
) O
means O
the O
paraphrase O
probability O
from O
xj O
to O
ci O
, O
and O
a O
( O
j|i O
) O
means O
the O
alignment O
probability O
. O

section 9
id pdf2json/2021.acl-long.397.pdf.json
the O
paraphrase O
probability O
and O
alignment O
are O
trained O
and O
inferred O
as O
the O
translation O
model O
in O
smt O
ibm O
model O
2 O
. O

section 11
id pdf2json/2021.acl-long.397.pdf.json
datasets O
we O
conduct O
experiments O
on O
three O
datasets O
: O
overnight O
( O
λ-dcs O
) O
, O
geo O
( O
funql O
) O
, O
and O
geogranno O
, O
which O
use O
different O
meaning O
representations O
and O
on O
different O
domains O
. O

section 11
id pdf2json/2021.acl-long.397.pdf.json
our O
implementations O
are O
public O
available2 O
. O

section 11
id pdf2json/2021.acl-long.397.pdf.json
overnight O
this O
is O
a O
multi-domain O
dataset O
, O
which O
contains O
natural O
language O
paraphrases O
paired O
with O
lambda O
dcs O
logical O
forms O
across O
eight O
domains O
. O

section 11
id pdf2json/2021.acl-long.397.pdf.json
we O
use O
the O
same O
train/test O
splits O
as O
wang O
et O
al O
. O

section 11
id pdf2json/2021.acl-long.397.pdf.json
( O
2015 O
) O
. O

section 11
id pdf2json/2021.acl-long.397.pdf.json
geo O
( O
funql O
) O
this O
is O
a O
semantic O
parsing O
benchmark O
about O
u.s. O
geography O
( O
zelle O
and O
mooney O
, O
1996 O
) O
using O
the O
variable-free O
semantic O
representation O
funql O
( O
kate O
et O
al. O
, O
2005 O
) O
. O

section 11
id pdf2json/2021.acl-long.397.pdf.json
we O
extend O
the O
funql O
grammar O
to O
scfg O
for O
this O
dataset O
. O

section 11
id pdf2json/2021.acl-long.397.pdf.json
we O
follow O
the O
standard O
600/280 O
train/test O
splits O
. O

section 11
id pdf2json/2021.acl-long.397.pdf.json
geogranno O
this O
is O
another O
version O
of O
geo O
( O
herzig O
and O
berant O
, O
2019 O
) O
, O
in O
which O
lambda O
dcs O
logical O
forms O
paired O
with O
canonical O
utterances O
are O
produced O
from O
scfg O
. O

section 11
id pdf2json/2021.acl-long.397.pdf.json
instead O
of O
paraphrasing O
sentences O
, O
crowd O
workers O
are O
required O
to O
select O
the O
correct O
canonical O
utterance O
from O
candidate O
list O
. O

section 11
id pdf2json/2021.acl-long.397.pdf.json
we O
follow O
the O
split O
( O
train/valid/test O
487/59/278 O
) O
in O
original O
paper O
. O

section 11
id pdf2json/2021.acl-long.397.pdf.json
paraphrase O
model O
we O
obtain O
the O
paraphrase O
model O
by O
training O
t5 O
and O
gpt2.0 O
on O
wikianswer O
paraphrase3 O
, O
we O
train O
10 O
epochs O
with O
learning O
rate O
as O
1e-5 O
. O

section 11
id pdf2json/2021.acl-long.397.pdf.json
follow O
li O
et O
al O
. O

section 11
id pdf2json/2021.acl-long.397.pdf.json
( O
2019 O
) O
, O
we O
sample O
500k O
pairs O
of O
sentences O
in O
wikianswer O
corpus O
as O
training O
set O
and O
6k O
as O
dev O
set O
. O

section 11
id pdf2json/2021.acl-long.397.pdf.json
we O
generate O
adaptive O
fine-tuning O
datasets O
proportional O
to O
their O
labeled O
datasets O
, O
and O
back-translation O
( O
from O
english O
2https O
: O
//github.com/lingowu/ssd O
3http O
: O
//knowitall.cs.washington.edu/ O
paralex O
to O
chinese O
then O
translate O
back O
) O
is O
used O
to O
obtain O
external O
paraphrases O
data O
. O

section 11
id pdf2json/2021.acl-long.397.pdf.json
on O
average O
, O
we O
sample O
423 O
cus O
per O
domain O
, O
and O
synthesize O
847 O
instances O
per O
domain O
in O
self O
paras O
and O
1252 O
in O
external O
paras O
. O

section 11
id pdf2json/2021.acl-long.397.pdf.json
unsupervised O
settings O
in O
unsupervised O
settings O
, O
we O
do O
not O
use O
any O
annotated O
semantic O
parsing O
data O
. O

section 11
id pdf2json/2021.acl-long.397.pdf.json
the O
paraphrase O
generation O
models O
are O
fixed O
after O
the O
paraphrasing O
pre-training O
and O
the O
adaptive O
fine-tuning O
. O

section 11
id pdf2json/2021.acl-long.397.pdf.json
the O
models O
are O
employed O
to O
generate O
canonical O
utterances O
and O
mrs O
synchronously O
via O
rule-level O
or O
word-level O
inference O
. O

section 11
id pdf2json/2021.acl-long.397.pdf.json
in O
rule-level O
inference O
, O
the O
leftmost O
nonterminators O
are O
eliminated O
by O
cyclically O
expanded O
and O
the O
maximum O
depth O
k O
is O
set O
to O
5 O
, O
the O
beam O
size O
is O
set O
to O
20 O
. O

section 11
id pdf2json/2021.acl-long.397.pdf.json
ssd O
uses O
t5 O
as O
the O
pre-trained O
language O
model O
in O
all O
the O
proposed O
components O
, O
including O
adaptive O
fine-tuning O
, O
reranking O
and O
the O
two O
decoding O
constraints O
. O

section 11
id pdf2json/2021.acl-long.397.pdf.json
ablation O
experiments O
are O
conducted O
over O
all O
components O
with O
rule-level O
inference O
. O

section 11
id pdf2json/2021.acl-long.397.pdf.json
unsupervised O
settings O
( O
with O
external O
nonparallel O
data O
) O
cao O
et O
al O
. O

section 11
id pdf2json/2021.acl-long.397.pdf.json
( O
2020 O
) O
have O
shown O
that O
external O
nonparallel O
data O
( O
including O
nonparallel O
natural O
language O
utterances O
and O
canonical O
utterances O
) O
can O
be O
used O
to O
build O
unsupervised O
semantic O
parsers O
. O

section 11
id pdf2json/2021.acl-long.397.pdf.json
for O
fair O
comparison O
, O
we O
also O
conduct O
unsupervised O
experiments O
with O
external O
unparallel O
data O
. O

section 11
id pdf2json/2021.acl-long.397.pdf.json
specifically O
, O
we O
enhance O
the O
original O
ssd O
using O
the O
samples O
methods O
( O
cao O
et O
al. O
, O
2020 O
) O
: O
we O
label O
each O
input O
sentences O
with O
the O
most O
possible O
outputs O
in O
the O
nonparallel O
corpus O
and O
use O
these O
samples O
as O
peusdo O
training O
data O
– O
we O
denote O
this O
setting O
as O
ssd-samples O
. O

section 11
id pdf2json/2021.acl-long.397.pdf.json
supervised O
settings O
our O
ssd O
method O
can O
be O
further O
enhanced O
using O
annotated O
training O
instances O
. O

section 11
id pdf2json/2021.acl-long.397.pdf.json
specifically O
, O
given O
the O
annotated O
〈utterance O
, O
logical O
form〉 O
instances O
, O
we O
first O
transform O
logical O
form O
to O
its O
canonical O
form O
, O
then O
use O
them O
to O
further O
fine-tune O
our O
paraphrase O
models O
after O
unsupervised O
pre-training O
. O

section 11
id pdf2json/2021.acl-long.397.pdf.json
baselines O
we O
compare O
our O
method O
with O
the O
following O
unsupervised O
baselines O
: O
1 O
) O
cross-domain O
zero O
shot O
( O
herzig O
and O
berant O
, O
2018 O
) O
, O
which O
trains O
on O
other O
source O
domains O
and O
then O
generalizes O
to O
target O
domains O
in O
overnight O
and O
2 O
) O
genovernight O
( O
wang O
et O
al. O
, O
2015 O
) O
in O
which O
models O
are O
trained O
on O
synthesized O
〈cu O
, O
mr〉 O
pairs O
; O
3 O
) O
we O
also O
implement O
seq2seq O
baseline O
on O
the O
synthesized O
data O
as O
synth-seq2seq O
. O

section 11
id pdf2json/2021.acl-long.397.pdf.json
4 O
) O
synthparaseq2seq O
is O
trained O
on O
the O
synthesized O
data O
and O
〈cu O
paraphrase O
, O
mr〉 O
pairs O
, O
the O
paraphrases O
are O
obtained O
in O
the O
same O
way O
in O
section O
4 O
. O

section 13
id pdf2json/2021.acl-long.397.pdf.json
the O
overall O
results O
of O
different O
baselines O
and O
our O
method O
are O
shown O
in O
table O
1 O
and O
table O
3 O
( O
we O
also O
demonstrate O
several O
cases O
in O
appendix O
) O
. O

section 13
id pdf2json/2021.acl-long.397.pdf.json
for O
our O
method O
, O
we O
report O
its O
performances O
on O
three O
settings O
. O

section 13
id pdf2json/2021.acl-long.397.pdf.json
we O
can O
see O
that O
: O
1 O
. O

section 13
id pdf2json/2021.acl-long.397.pdf.json
by O
synchronously O
decoding O
canonical O
utterances O
and O
meaning O
representations O
, O
ssd O
achieves O
competitive O
unsupervised O
semantic O
parsing O
performance O
. O

section 13
id pdf2json/2021.acl-long.397.pdf.json
in O
all O
datasets O
, O
our O
method O
outperforms O
other O
baselines O
in O
the O
unsupervised O
settings O
. O

section 13
id pdf2json/2021.acl-long.397.pdf.json
these O
results O
demonstrate O
that O
unsupervised O
semantic O
parsers O
can O
be O
effectively O
built O
by O
simultaneously O
exploit O
semantic O
and O
structural O
constraints O
, O
without O
the O
need O
of O
labeled O
data O
. O

section 13
id pdf2json/2021.acl-long.397.pdf.json
2 O
. O

section 13
id pdf2json/2021.acl-long.397.pdf.json
our O
model O
can O
achieve O
competitive O
performance O
on O
different O
datasets O
with O
different O
settings O
. O

section 13
id pdf2json/2021.acl-long.397.pdf.json
in O
supervised O
settings O
, O
our O
model O
can O
achieve O
competitive O
performance O
with O
sota O
. O

section 13
id pdf2json/2021.acl-long.397.pdf.json
with O
nonparallel O
data O
, O
our O
model O
can O
outperform O
two-stage O
. O

section 13
id pdf2json/2021.acl-long.397.pdf.json
on O
geo O
( O
funql O
) O
our O
model O
also O
ob- O
tains O
a O
significant O
improvement O
compared O
with O
baselines O
, O
which O
also O
verifies O
that O
our O
method O
is O
not O
limited O
to O
specific O
datasets O
( O
i.e. O
, O
overnight O
and O
geogranno O
, O
which O
are O
constructed O
with O
scfg O
and O
paraphrasing O
. O
) O

section 14
id pdf2json/2021.acl-long.397.pdf.json
inference O
can O
effectively O
generate O
paraphrases O
under O
the O
grammar O
constraints O
. O

section 14
id pdf2json/2021.acl-long.397.pdf.json
the O
rule-level O
inference O
can O
achieve O
better O
performance O
, O
we O
believe O
this O
is O
because O
rule-level O
inference O
is O
more O
compact O
than O
word-level O
inference O
, O
therefore O
the O
rule-level O
inference O
can O
search O
wider O
space O
and O
benefit O
beam O
search O
more O
. O

section 15
id pdf2json/2021.acl-long.397.pdf.json
effect O
of O
decoding O
constraints O
to O
analyze O
the O
effect O
of O
decoding O
constraints O
, O
we O
conduct O
ablation O
experiments O
with O
different O
constraint O
settings O
and O
the O
results O
are O
shown O
in O
table O
2 O
: O
- O
semantic O
denotes O
removing O
the O
semantic O
constraint O
, O
-grammar O
denotes O
all O
constraints O
are O
removed O
at O
the O
same O
time O
, O
the O
decoding O
is O
unrestricted O
. O

section 15
id pdf2json/2021.acl-long.397.pdf.json
we O
can O
see O
that O
the O
constrained O
decoding O
is O
critical O
for O
our O
paraphrasing-based O
semantic O
parsing O
, O
and O
both O
grammar O
constraints O
and O
semantic O
constraints O
contribute O
to O
the O
improvement O
. O

section 15
id pdf2json/2021.acl-long.397.pdf.json
effect O
of O
adaptive O
fine-tuning O
to O
analyze O
the O
effect O
of O
adaptive O
fine-tuning O
, O
we O
show O
the O
results O
with O
different O
settings O
by O
ablating O
a O
finetuning O
corpus O
at O
a O
time O
( O
see O
table O
2 O
) O
. O

section 15
id pdf2json/2021.acl-long.397.pdf.json
we O
can O
see O
that O
adaptive O
fine-tuning O
can O
significantly O
improve O
the O
performance O
. O

section 15
id pdf2json/2021.acl-long.397.pdf.json
and O
the O
paraphrase O
generation O
model O
can O
be O
effectively O
fine-tuned O
only O
using O
cus O
or O
self O
paras O
, O
which O
can O
be O
easily O
constructed O
. O

section 15
id pdf2json/2021.acl-long.397.pdf.json
0 O
10 O
20 O
30 O
40 O
50 O
60 O
70 O
80 O
90 O
0 O
5 O
10 O
15 O
30 O
50 O
100 O
ac O
cu O
ra O
cy O
( O
% O
) O
ssd O
seq2seq O
syth-seq2seq O
effect O
of O
reranking O
to O
analyze O
the O
effect O
of O
reranking O
, O
we O
compare O
the O
settings O
with/without O
reranking O
and O
its O
upper O
bound O
– O
oracle O
, O
which O
can O
always O
select O
the O
correct O
logical O
form O
if O
it O
is O
within O
the O
beam O
. O

section 15
id pdf2json/2021.acl-long.397.pdf.json
experimental O
results O
show O
that O
reranking O
can O
improve O
the O
semantic O
parsing O
performance O
. O

section 15
id pdf2json/2021.acl-long.397.pdf.json
moreover O
, O
there O
is O
still O
a O
large O
margin O
between O
our O
method O
and O
oracle O
, O
i.e. O
, O
the O
unsupervised O
semantic O
parsing O
can O
be O
significantly O
promoted O
by O
designing O
better O
reranking O
algorithms O
. O

section 15
id pdf2json/2021.acl-long.397.pdf.json
effect O
of O
adding O
labeled O
data O
to O
investigate O
the O
effect O
of O
adding O
labeled O
data O
, O
we O
test O
our O
method O
by O
varying O
the O
size O
of O
the O
labeled O
data O
on O
overnight O
from O
0 O
% O
to O
100 O
% O
. O

section 15
id pdf2json/2021.acl-long.397.pdf.json
in O
fig O
. O

section 15
id pdf2json/2021.acl-long.397.pdf.json
4 O
, O
we O
can O
see O
that O
our O
method O
can O
outperform O
baselines O
using O
the O
same O
labeled O
data O
. O

section 15
id pdf2json/2021.acl-long.397.pdf.json
and O
a O
small O
amount O
of O
data O
can O
produce O
a O
good O
performance O
using O
our O
method O
. O

section 15
id pdf2json/2021.acl-long.397.pdf.json
effect O
of O
pretrained O
language O
models O
to O
analyze O
the O
effect O
of O
plms O
, O
we O
show O
the O
results O
with O
different O
plm O
settings O
: O
instead O
of O
t5 O
we O
use O
gpt2 O
or O
randomly O
initialized O
transformers O
to O
construct O
paraphrasing O
models O
. O

section 15
id pdf2json/2021.acl-long.397.pdf.json
experimental O
results O
show O
that O
powerful O
plms O
can O
improve O
the O
performance O
. O

section 15
id pdf2json/2021.acl-long.397.pdf.json
powered O
by O
the O
language O
generation O
models O
to O
do O
semantic O
parsing O
, O
our O
method O
can O
benefit O
from O
the O
rapid O
development O
of O
plms O
. O

section 16
id pdf2json/2021.acl-long.397.pdf.json
data O
scarcity O
in O
semantic O
parsing O
. O

section 16
id pdf2json/2021.acl-long.397.pdf.json
witnessed O
the O
labeled O
data O
bottleneck O
problem O
, O
many O
techniques O
have O
been O
proposed O
to O
reduce O
the O
demand O
for O
labeled O
logical O
forms O
. O

section 16
id pdf2json/2021.acl-long.397.pdf.json
many O
weakly O
supervised O
learning O
are O
proposed O
( O
artzi O
and O
zettlemoyer O
, O
2013 O
; O
berant O
et O
al. O
, O
2013 O
; O
reddy O
et O
al. O
, O
2014 O
; O
agrawal O
et O
al. O
, O
2019 O
; O
chen O
et O
al. O
, O
2020 O
) O
, O
such O
as O
denotation-base O
learning O
( O
pasupat O
and O
liang O
, O
2016 O
; O
goldman O
et O
al. O
, O
2018 O
) O
, O
iterative O
searching O
( O
dasigi O
et O
al. O
, O
2019 O
) O
. O

section 16
id pdf2json/2021.acl-long.397.pdf.json
semi-supervised O
semantic O
parsing O
is O
also O
proposed O
( O
chen O
et O
al. O
, O
2018a O
) O
. O

section 16
id pdf2json/2021.acl-long.397.pdf.json
such O
as O
variational O
auto-encoding O
( O
yin O
et O
al. O
, O
2018 O
) O
, O
dual O
learning O
framework O
for O
semantic O
parsing O
( O
cao O
et O
al. O
, O
2019 O
) O
, O
dual O
information O
maximization O
method O
( O
ye O
et O
al. O
, O
2019 O
) O
, O
and O
backtranslation O
( O
sun O
et O
al. O
, O
2019 O
) O
. O

section 16
id pdf2json/2021.acl-long.397.pdf.json
one O
other O
strategy O
is O
to O
generate O
data O
for O
semantic O
parsing O
, O
e.g. O
, O
wang O
et O
al O
. O

section 16
id pdf2json/2021.acl-long.397.pdf.json
( O
2015 O
) O
construct O
a O
semantic O
parsing O
dataset O
from O
grammar O
rules O
and O
crowdsourcing O
paraphrase O
. O

section 16
id pdf2json/2021.acl-long.397.pdf.json
guo O
et O
al O
. O

section 16
id pdf2json/2021.acl-long.397.pdf.json
( O
2018 O
) O
produce O
pseudolabeled O
data O
. O

section 16
id pdf2json/2021.acl-long.397.pdf.json
jia O
and O
liang O
( O
2016 O
) O
create O
new O
“ O
recombinant O
” O
training O
examples O
with O
scfg O
. O

section 16
id pdf2json/2021.acl-long.397.pdf.json
the O
domain O
transfer O
techniques O
are O
also O
used O
to O
reduce O
the O
cost O
of O
data O
collecting O
for O
the O
unseen O
domain O
( O
su O
and O
yan O
, O
2017 O
; O
herzig O
and O
berant O
, O
2018 O
; O
lu O
et O
al. O
, O
2019 O
; O
zhong O
et O
al. O
, O
2020 O
) O
. O

section 16
id pdf2json/2021.acl-long.397.pdf.json
goldwasser O
et O
al O
. O

section 16
id pdf2json/2021.acl-long.397.pdf.json
( O
2011 O
) O
; O
poon O
and O
domingos O
( O
2009 O
) O
; O
schmitt O
et O
al O
. O

section 16
id pdf2json/2021.acl-long.397.pdf.json
( O
2020 O
) O
leverage O
external O
resources O
or O
techniques O
for O
unsupervised O
learning O
. O

section 16
id pdf2json/2021.acl-long.397.pdf.json
constrained O
decoding O
. O

section 16
id pdf2json/2021.acl-long.397.pdf.json
after O
neural O
parsers O
model O
semantic O
parsing O
as O
a O
sentence O
to O
logical O
form O
translation O
task O
( O
yih O
et O
al. O
, O
2015 O
; O
krishnamurthy O
et O
al. O
, O
2017 O
; O
iyyer O
et O
al. O
, O
2017 O
; O
jie O
and O
lu O
, O
2018 O
; O
lindemann O
et O
al. O
, O
2020 O
) O
, O
many O
constrained O
decoding O
algorithms O
are O
also O
proposed O
, O
such O
as O
type O
constraint-based O
illegal O
token O
filtering O
( O
krishnamurthy O
et O
al. O
, O
2017 O
) O
; O
lisp O
interpreter-based O
method O
( O
liang O
et O
al. O
, O
2017 O
) O
; O
type O
constraints O
for O
generating O
valid O
actions O
( O
iyyer O
et O
al. O
, O
2017 O
) O
. O

section 16
id pdf2json/2021.acl-long.397.pdf.json
paraphrasing O
in O
semantic O
parsing O
. O

section 16
id pdf2json/2021.acl-long.397.pdf.json
paraphrase O
models O
have O
been O
widely O
used O
in O
semantic O
parsing O
. O

section 16
id pdf2json/2021.acl-long.397.pdf.json
parasempre O
( O
berant O
and O
liang O
, O
2014 O
) O
use O
paraphrase O
model O
to O
rerank O
candidate O
logical O
forms O
. O

section 16
id pdf2json/2021.acl-long.397.pdf.json
wang O
et O
al O
. O

section 16
id pdf2json/2021.acl-long.397.pdf.json
( O
2015 O
) O
employ O
scfg O
grammar O
rules O
to O
produce O
mr O
and O
canonical O
utterance O
pairs O
, O
and O
construct O
overnight O
dataset O
by O
paraphrasing O
utterances O
. O

section 16
id pdf2json/2021.acl-long.397.pdf.json
dong O
et O
al O
. O

section 16
id pdf2json/2021.acl-long.397.pdf.json
( O
2017 O
) O
use O
paraphrasing O
to O
expand O
the O
expressions O
of O
query O
sentences O
. O

section 16
id pdf2json/2021.acl-long.397.pdf.json
compared O
with O
these O
methods O
, O
we O
combine O
paraphrasing O
with O
grammar-constrained O
decoding O
, O
therefore O
ssd O
can O
further O
reduce O
the O
requirement O
of O
labeled O
data O
and O
achieve O
unsupervised O
semantic O
parsing O
. O

section 17
id pdf2json/2021.acl-long.397.pdf.json
we O
propose O
an O
unsupervised O
semantic O
parsing O
method O
– O
synchronous O
semantic O
decoding O
, O
which O
leverages O
paraphrasing O
and O
grammar-constrained O
decoding O
to O
simultaneously O
resolve O
the O
semantic O
gap O
and O
the O
structure O
gap O
. O

section 17
id pdf2json/2021.acl-long.397.pdf.json
specifically O
, O
we O
design O
two O
synchronous O
semantic O
decoding O
algorithms O
for O
paraphrasing O
under O
grammar O
constraints O
, O
and O
exploit O
adaptive O
fine-tuning O
and O
utterance O
reranking O
to O
alleviate O
the O
style O
bias O
in O
semantic O
parsing O
. O

section 17
id pdf2json/2021.acl-long.397.pdf.json
experimental O
results O
show O
that O
our O
approach O
can O
achieve O
competitive O
performance O
in O
unsupervised O
settings O
. O

section 18
id pdf2json/2021.acl-long.397.pdf.json
we O
sincerely O
thank O
the O
reviewers O
for O
their O
insightful O
comments O
and O
valuable O
suggestions O
. O

section 18
id pdf2json/2021.acl-long.397.pdf.json
moreover O
, O
this O
work O
is O
supported O
by O
the O
national O
key O
research O
and O
development O
program O
of O
china O
( O
no O
. O

section 18
id pdf2json/2021.acl-long.397.pdf.json
2020aaa0106400 O
) O
, O
the O
national O
natural O
science O
foundation O
of O
china O
under O
grants O
no O
. O

section 18
id pdf2json/2021.acl-long.397.pdf.json
61906182 O
and O
62076233 O
, O
and O
in O
part O
by O
the O
youth O
innovation O
promotion O
association O
cas O
( O
2018141 O
) O
. O

section 19
id pdf2json/2021.acl-long.397.pdf.json
a.1 O
case O
study O
in O
table O
4 O
, O
we O
present O
the O
cases O
generated O
from O
ssd O
. O

section 19
id pdf2json/2021.acl-long.397.pdf.json
cases O
show O
that O
ssd O
can O
output O
semanticssimilar O
and O
grammar-legal O
utterances O
. O

section 19
id pdf2json/2021.acl-long.397.pdf.json
in O
case O
1 O
, O
“ O
take-out O
” O
does O
not O
appear O
in O
paraphrase O
dataset O
, O
we O
can O
still O
efficiently O
generate O
the O
utterances O
containing O
it O
, O
which O
shows O
our O
constrainedparaphrasing O
based O
semantic O
parser O
has O
the O
generalization O
ability O
on O
unseen O
words O
. O

section 19
id pdf2json/2021.acl-long.397.pdf.json
we O
found O
that O
the O
parser O
maintains O
high O
recall O
, O
covering O
the O
correct O
canonical O
utterances O
in O
our O
n-best O
list O
of O
predictions O
. O

section 19
id pdf2json/2021.acl-long.397.pdf.json
as O
case O
2 O
shows O
the O
designed O
utterance O
reranking O
score O
can O
select O
the O
best O
canonical O
utterances O
by O
focusing O
on O
coherence O
and O
adequacy O
. O

section 19
id pdf2json/2021.acl-long.397.pdf.json
with O
adaptive O
fine-tuning O
( O
case O
3 O
) O
, O
our O
model O
can O
generate O
the O
utterances O
focusing O
more O
on O
semantics O
to O
alleviate O
the O
style O
bias O
. O

section TITLE
id pdf2json/2021.acl-long.150.pdf.json
intrinsic O
bias O
metrics O
do O
not O
correlate O
with O
application O
bias O

section ABSTRACT
id pdf2json/2021.acl-long.150.pdf.json
natural O
language O
processing O
( O
nlp O
) O
systems O
learn O
harmful O
societal O
biases O
that O
cause O
them O
to O
amplify O
inequality O
as O
they O
are O
deployed O
in O
more O
and O
more O
situations O
. O

section ABSTRACT
id pdf2json/2021.acl-long.150.pdf.json
to O
guide O
efforts O
at O
debiasing O
these O
systems O
, O
the O
nlp O
community O
relies O
on O
a O
variety O
of O
metrics O
that O
quantify O
bias O
in O
models O
. O

section ABSTRACT
id pdf2json/2021.acl-long.150.pdf.json
some O
of O
these O
metrics O
are O
intrinsic O
, O
measuring O
bias O
in O
word O
embedding O
spaces O
, O
and O
some O
are O
extrinsic O
, O
measuring O
bias O
in O
downstream O
tasks O
that O
the O
word O
embeddings O
enable O
. O

section ABSTRACT
id pdf2json/2021.acl-long.150.pdf.json
do O
these O
intrinsic O
and O
extrinsic O
metrics O
correlate O
with O
each O
other O
? O

section ABSTRACT
id pdf2json/2021.acl-long.150.pdf.json
we O
compare O
intrinsic O
and O
extrinsic O
metrics O
across O
hundreds O
of O
trained O
models O
covering O
different O
tasks O
and O
experimental O
conditions O
. O

section ABSTRACT
id pdf2json/2021.acl-long.150.pdf.json
our O
results O
show O
no O
reliable O
correlation O
between O
these O
metrics O
that O
holds O
in O
all O
scenarios O
across O
tasks O
and O
languages O
. O

section ABSTRACT
id pdf2json/2021.acl-long.150.pdf.json
we O
urge O
researchers O
working O
on O
debiasing O
to O
focus O
on O
extrinsic O
measures O
of O
bias O
, O
and O
to O
make O
using O
these O
measures O
more O
feasible O
via O
creation O
of O
new O
challenge O
sets O
and O
annotated O
test O
data O
. O

section ABSTRACT
id pdf2json/2021.acl-long.150.pdf.json
to O
aid O
this O
effort O
, O
we O
release O
code O
, O
a O
new O
intrinsic O
metric O
, O
and O
an O
annotated O
test O
set O
focused O
on O
gender O
bias O
in O
hate O
speech.1 O

section 0
id pdf2json/2021.acl-long.150.pdf.json
proceedings O
of O
the O
59th O
annual O
meeting O
of O
the O
association O
for O
computational O
linguistics O
and O
the O
11th O
international O
joint O
conference O
on O
natural O
language O
processing O
, O
pages O
1926–1940 O
august O
1–6 O
, O
2021 O
. O

section 0
id pdf2json/2021.acl-long.150.pdf.json
©2021 O
association O
for O
computational O
linguistics O
1926 O

section 1
id pdf2json/2021.acl-long.150.pdf.json
awareness O
of O
bias O
in O
natural O
language O
processing O
( O
nlp O
) O
systems O
has O
rapidly O
increased O
as O
more O
and O
more O
systems O
are O
discovered O
to O
perpetuate O
societal O
unfairness O
at O
massive O
scales O
. O

section 1
id pdf2json/2021.acl-long.150.pdf.json
this O
awareness O
has O
prompted O
a O
surge O
of O
research O
into O
measuring O
and O
mitigating O
bias O
, O
but O
this O
research O
suffers O
from O
lack O
of O
consistent O
metrics O
that O
discover O
and O
measure O
bias O
. O

section 1
id pdf2json/2021.acl-long.150.pdf.json
instead O
, O
work O
on O
bias O
is O
“ O
rife O
with O
unstated O
assumptions O
” O
( O
blodgett O
et O
al. O
, O
2020 O
) O
and O
relies O
on O
metrics O
that O
are O
easy O
to O
measure O
rather O
than O
metrics O
that O
meaningfully O
detect O
bias O
in O
applications O
. O

section 1
id pdf2json/2021.acl-long.150.pdf.json
∗ O
equal O
contribution O
. O

section 1
id pdf2json/2021.acl-long.150.pdf.json
correspondence O
to O
s.tarrant O
@ O
ed.ac.uk O
1https O
: O
//tinyurl.com/serif-embed O
a O
recent O
comprehensive O
survey O
of O
bias O
in O
nlp O
( O
blodgett O
et O
al. O
, O
2020 O
) O
found O
that O
one O
third O
of O
all O
research O
papers O
focused O
on O
bias O
in O
word O
embeddings O
. O

section 1
id pdf2json/2021.acl-long.150.pdf.json
this O
makes O
embeddings O
the O
most O
common O
topic O
in O
studies O
of O
bias O
— O
over O
twice O
as O
common O
as O
any O
other O
topic O
related O
to O
bias O
in O
nlp O
. O

section 1
id pdf2json/2021.acl-long.150.pdf.json
as O
is O
visualised O
in O
figure O
1a O
, O
bias O
in O
embedding O
spaces O
is O
measured O
with O
intrinsic O
metrics O
, O
most O
commonly O
with O
the O
word O
embedding O
association O
test O
( O
weat O
) O
( O
caliskan O
et O
al. O
, O
2017 O
) O
, O
which O
relates O
bias O
to O
the O
geometry O
of O
the O
embedding O
space O
. O

section 1
id pdf2json/2021.acl-long.150.pdf.json
once O
embeddings O
are O
incorporated O
into O
an O
application O
, O
bias O
can O
be O
measured O
via O
extrinsic O
metrics O
( O
figure O
1b O
) O
that O
test O
whether O
the O
application O
performs O
differently O
on O
language O
related O
to O
different O
populations O
. O

section 1
id pdf2json/2021.acl-long.150.pdf.json
hence O
, O
research O
on O
debiasing O
embeddings O
relies O
crucially O
on O
a O
hypothesis O
that O
doing O
so O
will O
remove O
or O
reduce O
bias O
in O
downstream O
applications O
. O

section 1
id pdf2json/2021.acl-long.150.pdf.json
however O
, O
we O
are O
aware O
of O
no O
prior O
research O
that O
confirms O
this O
hypothesis O
. O

section 1
id pdf2json/2021.acl-long.150.pdf.json
this O
untested O
assumption O
leaves O
nlp O
bias O
research O
in O
a O
precarious O
position O
. O

section 1
id pdf2json/2021.acl-long.150.pdf.json
research O
into O
the O
semantics O
of O
word O
embeddings O
has O
already O
shown O
that O
intrinsic O
metrics O
( O
e.g O
. O

section 1
id pdf2json/2021.acl-long.150.pdf.json
using O
analogies O
and O
semantic O
similarity O
, O
as O
in O
hill O
et O
al. O
, O
2015 O
) O
do O
not O
correlate O
well O
with O
extrinsic O
metrics O
( O
faruqui O
et O
al. O
, O
2016 O
) O
. O

section 1
id pdf2json/2021.acl-long.150.pdf.json
research O
into O
the O
bias O
of O
word O
embeddings O
lacks O
the O
same O
type O
of O
systematic O
study O
, O
and O
thus O
as O
a O
field O
we O
are O
exposed O
to O
three O
large O
risks O
: O
1 O
) O
making O
misleading O
claims O
about O
the O
fairness O
of O
our O
systems O
, O
2 O
) O
concentrating O
our O
efforts O
on O
the O
wrong O
problem O
, O
and O
most O
importantly O
, O
3 O
) O
feeling O
a O
false O
sense O
of O
security O
that O
we O
are O
making O
more O
progress O
on O
the O
problem O
than O
we O
are O
. O

section 1
id pdf2json/2021.acl-long.150.pdf.json
our O
bias O
research O
can O
be O
rigorous O
and O
innovative O
, O
but O
unless O
we O
understand O
the O
limitations O
of O
metrics O
we O
use O
to O
evaluate O
it O
, O
it O
might O
have O
no O
impact O
. O

section 1
id pdf2json/2021.acl-long.150.pdf.json
in O
this O
paper O
, O
we O
ask O
: O
does O
the O
commonly O
used O
intrinsic O
metric O
for O
embeddings O
( O
weat O
) O
correlate O
with O
extrinsic O
metrics O
of O
application O
bias O
? O

section 1
id pdf2json/2021.acl-long.150.pdf.json
to O
answer O
this O
question O
, O
we O
analyse O
the O
relationship O
between O
intrinsic O
and O
extrinsic O
bias O
. O

section 1
id pdf2json/2021.acl-long.150.pdf.json
our O
study O
considers O
two O
languages O
( O
english O
and O
spanish O
) O
, O
two O
common O
embedding O
algorithms O
( O
word2vec O
and O
fasttext O
) O
and O
two O
downstream O
tasks O
( O
coreference O
resolution O
and O
hatespeech O
detection O
) O
. O

section 1
id pdf2json/2021.acl-long.150.pdf.json
while O
we O
find O
a O
moderately O
high O
correlation O
between O
these O
metrics O
in O
a O
handful O
of O
conditions O
, O
we O
find O
no O
correlation O
or O
even O
negative O
correlation O
in O
most O
conditions O
. O

section 1
id pdf2json/2021.acl-long.150.pdf.json
therefore O
, O
we O
recommend O
that O
the O
ethical O
scientist O
or O
engineer O
does O
not O
rely O
on O
intrinsic O
metrics O
when O
attempting O
to O
mitigate O
bias O
, O
but O
instead O
focuses O
on O
the O
harms O
of O
specific O
applications O
and O
test O
for O
bias O
directly O
. O

section 1
id pdf2json/2021.acl-long.150.pdf.json
as O
additional O
contributions O
to O
these O
findings O
, O
we O
release O
new O
weat O
metrics O
for O
spanish O
, O
and O
a O
new O
gender-annotated O
test O
set O
for O
hatespeech O
detection O
for O
english O
, O
both O
of O
which O
we O
created O
in O
the O
course O
of O
this O
research O
. O

section 2
id pdf2json/2021.acl-long.150.pdf.json
in O
all O
of O
our O
experiments O
, O
we O
compute O
correlations O
between O
commonly-used O
metrics O
, O
both O
intrinsic O
and O
extrinsic O
. O

section 3
id pdf2json/2021.acl-long.150.pdf.json
intrinsic O
bias O
metrics O
are O
applied O
directly O
to O
word O
embeddings O
, O
formulating O
bias O
in O
terms O
of O
geometric O
relationships O
between O
concepts O
such O
as O
male O
, O
female O
, O
career O
, O
or O
family O
. O

section 3
id pdf2json/2021.acl-long.150.pdf.json
each O
concept O
is O
in O
turn O
represented O
by O
curated O
wordlists O
. O

section 3
id pdf2json/2021.acl-long.150.pdf.json
for O
example O
, O
the O
concept O
male O
is O
represented O
by O
words O
like O
brother O
, O
father O
, O
grandfather O
, O
etc O
. O

section 3
id pdf2json/2021.acl-long.150.pdf.json
while O
the O
concept O
math O
& O
science O
is O
represented O
by O
words O
like O
programmer O
, O
engineer O
, O
etc O
. O

section 3
id pdf2json/2021.acl-long.150.pdf.json
the O
most O
commonly O
used O
metric O
is O
weat O
( O
caliskan O
et O
al. O
, O
2017 O
) O
.2 O
, O
which O
measures O
the O
difference O
in O
mean O
cosine O
similarity O
between O
two O
target O
concepts O
x O
and O
y O
; O
and O
two O
attribute O
concepts O
a O
and O
b O
. O

section 3
id pdf2json/2021.acl-long.150.pdf.json
this O
difference O
represents O
the O
imbalance O
in O
associations O
between O
concepts O
. O

section 3
id pdf2json/2021.acl-long.150.pdf.json
using O
~w O
to O
represent O
the O
embedding O
of O
word O
w O
, O
we O
have O
a O
test O
statistic O
: O
s O
( O
x O
, O
y O
, O
a O
, O
b O
) O
= O
∑ O
x∈x O
s O
( O
x O
, O
a O
, O
b O
) O
− O
∑ O
y∈y O
s O
( O
y O
, O
a O
, O
b O
) O
where O
s O
( O
w O
, O
a O
, O
b O
) O
= O
mean O
a∈a O
cos O
( O
~w O
, O
~a O
) O
−mean O
b∈b O
cos O
( O
~w O
, O
~b O
) O
this O
is O
normalised O
by O
the O
standard O
deviation O
to O
get O
the O
effect O
size O
which O
we O
use O
in O
our O
experiments O
. O

section 3
id pdf2json/2021.acl-long.150.pdf.json
weat O
was O
initially O
developed O
as O
an O
indicator O
of O
bias O
, O
to O
show O
that O
the O
implicit O
association O
test O
( O
iat O
) O
from O
the O
field O
of O
psychology O
( O
greenwald O
et O
al. O
, O
1998 O
) O
can O
be O
replicated O
via O
word O
embeddings O
measurements O
. O

section 3
id pdf2json/2021.acl-long.150.pdf.json
there O
are O
thus O
10 O
original O
tests O
chosen O
to O
replicate O
the O
tests O
presented O
to O
human O
subjects O
in O
iat O
. O

section 3
id pdf2json/2021.acl-long.150.pdf.json
the O
tests O
measure O
different O
kinds O
of O
biased O
associations O
, O
such O
as O
african-american O
names O
vs. O
white O
names O
with O
pleasant O
vs. O
unpleasant O
terms O
, O
and O
female O
terms O
vs. O
male O
terms O
with O
career O
vs. O
family O
words O
. O

section 3
id pdf2json/2021.acl-long.150.pdf.json
weat O
was O
later O
repurposed O
as O
a O
predictor O
of O
bias O
in O
embedding O
spaces O
, O
via O
a O
somewhat O
muddy O
logical O
journey O
. O

section 3
id pdf2json/2021.acl-long.150.pdf.json
it O
has O
since O
been O
translated O
into O
6 O
other O
languages O
( O
xweat O
; O
lauscher O
and O
glavas O
, O
2019 O
) O
, O
and O
extended O
to O
operate O
on O
full O
sentences O
( O
may O
et O
al. O
, O
2019 O
) O
and O
on O
contextual O
language O
models O
( O
kurita O
et O
al. O
, O
2019 O
) O
. O

section 3
id pdf2json/2021.acl-long.150.pdf.json
when O
weat O
is O
used O
as O
a O
metric O
, O
papers O
report O
the O
effect O
size O
of O
the O
subset O
of O
tests O
relevant O
to O
the O
task O
at O
hand O
, O
each O
separately O
. O

section 3
id pdf2json/2021.acl-long.150.pdf.json
there O
are O
known O
issues O
with O
weat O
, O
such O
as O
sensitivity O
to O
corpus O
word O
frequency O
, O
and O
sensitivity O
2we O
count O
34 O
papers O
from O
*cl O
and O
fat* O
conferences O
since O
january O
2020 O
that O
use O
weat O
or O
seat O
( O
may O
et O
al. O
, O
2019 O
) O
in O
their O
methodology O
. O

section 3
id pdf2json/2021.acl-long.150.pdf.json
to O
target O
and O
attribute O
wordlists O
, O
as O
found O
by O
sedoc O
and O
ungar O
( O
2019 O
) O
and O
ethayarajh O
et O
al O
. O

section 3
id pdf2json/2021.acl-long.150.pdf.json
( O
2019 O
) O
. O

section 3
id pdf2json/2021.acl-long.150.pdf.json
the O
latter O
proposes O
an O
alternative O
more O
theoretically O
robust O
metric O
, O
relational O
inner O
product O
association O
( O
ripa O
) O
, O
which O
uses O
the O
principal O
component O
of O
a O
gender O
subspace O
( O
determined O
via O
the O
method O
of O
bolukbasi O
et O
al O
. O

section 3
id pdf2json/2021.acl-long.150.pdf.json
( O
2016 O
) O
) O
to O
directly O
measure O
how O
” O
gendered O
” O
a O
word O
is O
. O

section 3
id pdf2json/2021.acl-long.150.pdf.json
we O
have O
chosen O
to O
use O
the O
most O
common O
version O
of O
weat O
for O
this O
first O
empirical O
study O
, O
since O
it O
is O
most O
widely O
used O
. O

section 3
id pdf2json/2021.acl-long.150.pdf.json
it O
would O
be O
interesting O
to O
test O
ripa O
in O
the O
same O
way O
, O
if O
it O
were O
extended O
to O
more O
types O
of O
bias O
and O
more O
languages O
. O

section 3
id pdf2json/2021.acl-long.150.pdf.json
but O
we O
note O
that O
all O
intrinsic O
metrics O
are O
sensitive O
to O
chosen O
wordlists O
, O
so O
this O
must O
be O
done O
carefully O
, O
especially O
across O
languages O
, O
a O
topic O
we O
will O
return O
to O
in O
section O
4.3 O
. O

section 4
id pdf2json/2021.acl-long.150.pdf.json
extrinsic O
bias O
metrics O
measure O
bias O
in O
applications O
, O
via O
some O
variant O
of O
performance O
disparity O
, O
or O
performance O
gap O
between O
groups O
. O

section 4
id pdf2json/2021.acl-long.150.pdf.json
for O
instance O
, O
a O
speech O
recognition O
system O
is O
unfair O
if O
it O
has O
higher O
error O
rates O
for O
african-american O
dialects O
( O
tatman O
, O
2017 O
) O
, O
meaning O
that O
systems O
perform O
less O
well O
for O
those O
speakers O
. O

section 4
id pdf2json/2021.acl-long.150.pdf.json
a O
hiring O
classification O
system O
is O
unfair O
if O
it O
has O
more O
false O
negatives O
for O
women O
than O
for O
men O
, O
meaning O
that O
more O
qualified O
women O
are O
accidentally O
rejected O
than O
are O
qualified O
men.3 O
there O
are O
two O
commonly O
used O
metrics O
to O
quantify O
this O
possible O
performance O
disparity O
: O
predictive O
parity O
( O
hutchinson O
and O
mitchell O
, O
2019 O
) O
, O
which O
measures O
the O
difference O
in O
precision O
for O
a O
privileged O
and O
non-privileged O
group O
, O
and O
equality O
of O
opportunity O
( O
hardt O
et O
al. O
, O
2016 O
) O
, O
which O
measures O
the O
difference O
in O
recall O
between O
those O
groups O
( O
see O
appendix O
a O
for O
formal O
definitions O
) O
. O

section 4
id pdf2json/2021.acl-long.150.pdf.json
the O
metric O
that O
best O
identifies O
bias O
in O
a O
system O
varies O
based O
on O
the O
task O
. O

section 4
id pdf2json/2021.acl-long.150.pdf.json
for O
different O
applications O
, O
false O
negatives O
may O
be O
more O
harmful O
, O
for O
others O
false O
positives O
may O
be O
. O

section 4
id pdf2json/2021.acl-long.150.pdf.json
for O
our O
first O
task O
of O
coreference O
( O
figure O
1b O
) O
, O
false O
negatives O
— O
where O
the O
system O
fails O
to O
identify O
anti-stereotypical O
coreference O
chains O
( O
e.g O
. O

section 4
id pdf2json/2021.acl-long.150.pdf.json
women O
as O
farmers O
or O
as O
ceos O
) O
— O
are O
more O
harmful O
to O
the O
underprivileged O
class O
than O
false O
positives O
. O

section 4
id pdf2json/2021.acl-long.150.pdf.json
for O
our O
second O
task O
, O
hate O
speech O
detection O
( O
figure O
2 O
) O
, O
both O
can O
be O
harmful O
, O
for O
different O
reasons O
. O

section 4
id pdf2json/2021.acl-long.150.pdf.json
false O
positives O
for O
one O
group O
can O
systematically O
censor O
certain O
content O
, O
as O
has O
been O
found O
for O
hate O
speech O
detection O
applied O
to O
africanamerican O
vernacular O
english O
( O
aave O
) O
( O
sap O
et O
al. O
, O
3https O
: O
//tinyurl.com/y6c6clzu O
2019 O
; O
davidson O
et O
al. O
, O
2019 O
) O
. O

section 4
id pdf2json/2021.acl-long.150.pdf.json
false O
negatives O
permit O
abuse O
of O
minority O
populations O
that O
are O
targets O
of O
hate O
speech O
. O

section 4
id pdf2json/2021.acl-long.150.pdf.json
we O
examine O
performance O
gaps O
in O
both O
precision O
and O
in O
recall O
for O
broad O
coverage O
. O

section 5
id pdf2json/2021.acl-long.150.pdf.json
each O
of O
our O
experiments O
measures O
the O
correlation O
between O
a O
specific O
instance O
of O
weat O
and O
a O
specific O
extrinsic O
bias O
metric O
. O

section 5
id pdf2json/2021.acl-long.150.pdf.json
in O
each O
experiment O
, O
we O
train O
an O
embedding O
, O
measure O
the O
bias O
according O
to O
weat O
, O
and O
measure O
the O
bias O
in O
the O
downstream O
task O
that O
uses O
that O
embedding O
. O

section 5
id pdf2json/2021.acl-long.150.pdf.json
we O
then O
modify O
the O
embeddings O
by O
applying O
an O
algorithm O
to O
either O
debias O
them O
, O
or O
— O
by O
inverting O
the O
algorithm O
’ O
s O
behavior O
— O
to O
overbias O
them O
. O

section 5
id pdf2json/2021.acl-long.150.pdf.json
again O
we O
measure O
weat O
on O
the O
modified O
embedding O
and O
also O
the O
downstream O
bias O
in O
the O
target O
task O
. O

section 5
id pdf2json/2021.acl-long.150.pdf.json
when O
we O
have O
done O
this O
multiple O
times O
until O
we O
reach O
a O
stopping O
condition O
( O
detailed O
below O
) O
, O
we O
compute O
the O
correlation O
between O
the O
two O
metrics O
( O
via O
pearson O
correlation O
and O
analysis O
with O
scatterplots O
) O
. O

section 5
id pdf2json/2021.acl-long.150.pdf.json
rather O
than O
draw O
conclusions O
from O
a O
single O
experiment O
, O
we O
attempt O
to O
draw O
more O
robust O
conclusions O
by O
running O
many O
experiments O
, O
which O
vary O
along O
several O
dimensions O
. O

section 5
id pdf2json/2021.acl-long.150.pdf.json
we O
consider O
two O
common O
embedding O
algorithms O
, O
two O
tasks O
, O
and O
two O
languages O
. O

section 5
id pdf2json/2021.acl-long.150.pdf.json
a O
full O
table O
of O
experiment O
conditions O
can O
be O
found O
in O
table O
1 O
. O

section 6
id pdf2json/2021.acl-long.150.pdf.json
we O
need O
to O
measure O
the O
relationship O
between O
intrinsic O
and O
extrinsic O
metrics O
as O
bias O
changes O
, O
we O
must O
generate O
many O
datapoints O
for O
each O
experiment O
. O

section 6
id pdf2json/2021.acl-long.150.pdf.json
previous O
work O
on O
bias O
in O
embeddings O
studies O
methods O
to O
reduce O
embedding O
bias O
. O

section 6
id pdf2json/2021.acl-long.150.pdf.json
to O
generate O
enough O
data O
points O
, O
we O
take O
the O
novel O
approach O
of O
both O
decreasing O
and O
increasing O
bias O
in O
the O
embeddings O
. O

section 6
id pdf2json/2021.acl-long.150.pdf.json
we O
measure O
the O
baseline O
bias O
level O
, O
via O
weat O
, O
for O
each O
embedding O
trained O
normally O
on O
the O
original O
corpus O
. O

section 6
id pdf2json/2021.acl-long.150.pdf.json
we O
then O
adjust O
the O
bias O
up O
or O
down O
, O
remeasure O
weat O
, O
and O
measure O
the O
change O
in O
the O
downstream O
task O
. O

section 6
id pdf2json/2021.acl-long.150.pdf.json
we O
choose O
two O
methods O
from O
previous O
work O
that O
are O
capable O
of O
both O
debiasing O
and O
overbiasing O
: O
the O
first O
is O
a O
preprocessing O
method O
that O
operates O
on O
the O
training O
data O
before O
training O
, O
the O
second O
is O
a O
postprocessing O
method O
that O
operates O
on O
the O
embedding O
space O
once O
it O
has O
been O
trained O
. O

section 6
id pdf2json/2021.acl-long.150.pdf.json
this O
is O
important O
since O
both O
kinds O
of O
methods O
may O
be O
used O
in O
practice O
: O
a O
large O
company O
with O
proprietary O
data O
will O
train O
embeddings O
from O
scratch O
, O
and O
thus O
may O
use O
a O
preprocessing O
method O
; O
whereas O
a O
small O
company O
may O
rely O
on O
publicly O
available O
pretrained O
embeddings O
, O
and O
thus O
use O
a O
post-processing O
method O
. O

section 6
id pdf2json/2021.acl-long.150.pdf.json
4 O
for O
preprocessing O
, O
we O
use O
dataset O
balancing O
( O
dixon O
et O
al. O
, O
2018 O
) O
, O
which O
consists O
of O
subsampling O
the O
training O
data O
to O
be O
more O
equal O
with O
respect O
to O
some O
attributes O
. O

section 6
id pdf2json/2021.acl-long.150.pdf.json
for O
instance O
, O
if O
we O
are O
adjusting O
gender O
bias O
, O
we O
identify O
pro-stereotypical O
sentences5 O
such O
as O
‘ O
she O
was O
a O
talented O
housekeeper O
’ O
vs. O
anti-stereotypical O
sentences O
, O
such O
as O
‘ O
he O
was O
a O
talented O
housekeeper O
’ O
or O
‘ O
she O
was O
a O
talented O
analyst O
’ O
. O

section 6
id pdf2json/2021.acl-long.150.pdf.json
we O
sub-sample O
and O
reduce O
the O
frequency O
of O
the O
pro-stereotypical O
collocations O
to O
debias O
, O
and O
sub-sample O
the O
anti-stereotypical O
conditions O
to O
overbias O
. O

section 6
id pdf2json/2021.acl-long.150.pdf.json
as O
a O
postprocessing O
method O
for O
already O
trained O
embeddings O
, O
we O
use O
the O
attract-repel O
( O
mrksic O
et O
al. O
, O
2017 O
) O
algorithm O
. O

section 6
id pdf2json/2021.acl-long.150.pdf.json
this O
algorithm O
was O
de- O
4there O
are O
additional O
embedding O
based O
debiasing O
methods O
used O
in O
practice O
, O
based O
on O
identifying O
and O
removing O
a O
gender O
subspace O
during O
training O
or O
as O
postprocessing O
( O
bolukbasi O
et O
al. O
, O
2016 O
; O
zhao O
et O
al. O
, O
2018b O
) O
. O

section 6
id pdf2json/2021.acl-long.150.pdf.json
however O
, O
these O
methods O
do O
not O
change O
a O
word O
’ O
s O
nearest O
neighbour O
clusters O
( O
gonen O
and O
goldberg O
, O
2019 O
) O
, O
and O
so O
we O
would O
expect O
these O
debiasing O
methods O
to O
show O
superficial O
bias O
changes O
in O
weat O
without O
changing O
downstream O
bias O
. O

section 6
id pdf2json/2021.acl-long.150.pdf.json
both O
methods O
that O
we O
select O
modify O
the O
underlying O
word O
distribution O
and O
move O
many O
words O
in O
relation O
to O
each O
other O
. O

section 6
id pdf2json/2021.acl-long.150.pdf.json
we O
verified O
this O
with O
tsne O
visualisation O
as O
in O
figure O
1a O
following O
gonen O
and O
goldberg O
( O
2019 O
) O
and O
find O
that O
our O
bias O
modification O
methods O
do O
change O
word O
clusters O
. O

section 6
id pdf2json/2021.acl-long.150.pdf.json
5stereotypes O
as O
defined O
by O
zhao O
et O
al O
. O

section 6
id pdf2json/2021.acl-long.150.pdf.json
( O
2018a O
) O
and O
by O
caliskan O
et O
al O
. O

section 6
id pdf2json/2021.acl-long.150.pdf.json
( O
2017 O
) O
, O
who O
use O
the O
u.s. O
bureau O
of O
labor O
statistics O
and O
the O
implicit O
association O
test O
, O
respectively O
. O

section 6
id pdf2json/2021.acl-long.150.pdf.json
veloped O
to O
use O
dictionary O
wordlists O
( O
synonyms O
, O
antonyms O
) O
to O
refine O
semantic O
spaces O
. O

section 6
id pdf2json/2021.acl-long.150.pdf.json
it O
aims O
to O
move O
similar O
words O
( O
synonyms O
) O
close O
to O
each O
other O
and O
dissimilar O
words O
( O
antonyms O
) O
farther O
from O
each O
other O
, O
while O
keeping O
a O
regularisation O
term O
to O
preserve O
original O
semantics O
as O
much O
as O
possible O
. O

section 6
id pdf2json/2021.acl-long.150.pdf.json
lauscher O
et O
al O
. O

section 6
id pdf2json/2021.acl-long.150.pdf.json
( O
2020 O
) O
used O
an O
approach O
inspired O
by O
attract-repel O
for O
debiasing O
, O
though O
with O
constraints O
implemented O
somewhat O
differently O
. O

section 6
id pdf2json/2021.acl-long.150.pdf.json
we O
use O
the O
same O
pro- O
and O
anti-stereotypical O
wordlists O
as O
in O
dataset-balancing O
. O

section 6
id pdf2json/2021.acl-long.150.pdf.json
for O
debiasing O
, O
we O
use O
the O
algorithm O
to O
increase O
distance O
between O
prostereotypical O
combinations O
( O
she O
, O
housekeeper O
) O
and O
decrease O
distance O
between O
anti-stereotypical O
combinations O
( O
she O
, O
analyst O
or O
he O
, O
housekeeper O
) O
. O

section 6
id pdf2json/2021.acl-long.150.pdf.json
for O
overbiasing O
we O
do O
the O
reverse.6 O
as O
the O
stopping O
condition O
for O
preprocessing O
, O
we O
constrain O
the O
sub-sampling O
so O
that O
it O
does O
not O
substantially O
change O
the O
dataset O
size O
, O
by O
limiting O
it O
to O
removing O
less O
than O
five O
percent O
of O
the O
original O
data O
. O

section 6
id pdf2json/2021.acl-long.150.pdf.json
for O
postprocessing O
we O
limit O
the O
algorithm O
to O
maximum O
5 O
iterations O
. O

section 7
id pdf2json/2021.acl-long.150.pdf.json
we O
use O
two O
common O
word O
embedding O
algorithms O
: O
fasttext O
( O
bojanowski O
et O
al. O
, O
2017 O
) O
and O
skip-gram O
word2vec O
( O
mikolov O
et O
al. O
, O
2013 O
) O
embeddings O
. O

section 7
id pdf2json/2021.acl-long.150.pdf.json
word O
embeddings O
in O
fasttext O
are O
composed O
from O
embeddings O
of O
both O
the O
word O
and O
its O
subwords O
in O
the O
form O
of O
character O
n-grams O
. O

section 7
id pdf2json/2021.acl-long.150.pdf.json
lauscher O
and O
glavas O
( O
2019 O
) O
suggest O
that O
this O
difference O
may O
cause O
bias O
to O
be O
acquired O
and O
encoded O
differently O
in O
fasttext O
and O
word2vec O
( O
we O
discuss O
this O
in O
more O
detail O
in O
section O
5 O
) O
. O

section 7
id pdf2json/2021.acl-long.150.pdf.json
despite O
recent O
widespread O
interest O
in O
contextual O
embeddings O
( O
e.g O
. O

section 7
id pdf2json/2021.acl-long.150.pdf.json
bert O
; O
devlin O
et O
al. O
, O
2019 O
) O
, O
our O
experiments O
use O
these O
simpler O
contextless O
embed- O
6wordlists O
used O
for O
bias-modification O
and O
configs O
for O
attract-repel O
are O
included O
in O
the O
codebase O
. O

section 7
id pdf2json/2021.acl-long.150.pdf.json
dings O
because O
they O
are O
widely O
available O
in O
many O
toolkits O
and O
used O
in O
many O
real-world O
applications O
. O

section 7
id pdf2json/2021.acl-long.150.pdf.json
their O
design O
simplifies O
our O
experiments O
, O
whereas O
contextual O
embeddings O
would O
add O
significant O
complexity O
. O

section 7
id pdf2json/2021.acl-long.150.pdf.json
however O
, O
we O
know O
that O
bias O
is O
still O
a O
problem O
for O
large O
contextual O
embeddings O
( O
zhao O
et O
al. O
, O
2019 O
, O
2020 O
; O
gehman O
et O
al. O
, O
2020 O
; O
sheng O
et O
al. O
, O
2019 O
) O
, O
so O
our O
work O
remains O
important O
. O

section 7
id pdf2json/2021.acl-long.150.pdf.json
if O
intrinsic O
and O
extrinsic O
measures O
do O
not O
correlate O
with O
simple O
embeddings O
, O
this O
result O
is O
unlikely O
to O
be O
changed O
by O
adding O
more O
architectural O
layers O
and O
configurable O
hyperparameters O
. O

section 8
id pdf2json/2021.acl-long.150.pdf.json
we O
use O
three O
tasks O
that O
appear O
often O
in O
bias O
literature O
: O
coreference O
resolution O
for O
english O
, O
hate O
speech O
detection O
for O
english O
, O
and O
hate O
speech O
detection O
for O
spanish O
. O

section 8
id pdf2json/2021.acl-long.150.pdf.json
to O
make O
the O
scenarios O
as O
realistic O
as O
possible O
, O
we O
use O
a O
common O
, O
easy-to-implement O
and O
high O
performing O
architecture O
for O
each O
task O
: O
the O
end-to-end O
coreference O
system O
of O
lee O
et O
al O
. O

section 8
id pdf2json/2021.acl-long.150.pdf.json
( O
2017 O
) O
and O
the O
the O
cnn O
of O
kim O
( O
2014 O
) O
, O
which O
has O
been O
used O
in O
high-scoring O
systems O
in O
recent O
hate O
speech O
detection O
shared O
tasks O
( O
basile O
et O
al. O
, O
2019 O
) O
. O

section 8
id pdf2json/2021.acl-long.150.pdf.json
for O
each O
task O
, O
we O
feed O
pretrained O
embeddings O
to O
the O
model O
, O
frozen O
, O
and O
then O
train O
the O
model O
using O
the O
standard O
hyperparameters O
published O
for O
each O
model O
and O
task O
. O

section 9
id pdf2json/2021.acl-long.150.pdf.json
we O
experiment O
on O
both O
english O
and O
spanish O
. O

section 9
id pdf2json/2021.acl-long.150.pdf.json
it O
is O
important O
to O
take O
a O
language O
with O
pervasive O
gender-marking O
( O
spanish O
) O
into O
account O
, O
as O
previous O
work O
has O
shown O
that O
grammatical O
gender-marking O
has O
a O
strong O
effect O
on O
gender O
bias O
in O
embeddings O
( O
mccurdy O
and O
serbetci O
, O
2017 O
; O
gonen O
et O
al. O
, O
2019 O
; O
zhou O
et O
al. O
, O
2019 O
) O
. O

section 9
id pdf2json/2021.acl-long.150.pdf.json
we O
use O
spanish O
only O
for O
hate O
speech O
detection O
, O
because O
gender O
marking O
makes O
a O
challenge-set O
style O
coreference O
evaluation O
trivial O
to O
resolve O
and O
not O
a O
candidate O
for O
detection O
of O
gender O
bias.7 O

section 11
id pdf2json/2021.acl-long.150.pdf.json
to O
train O
embeddings O
, O
we O
use O
domain-matched O
data O
for O
each O
downstream O
task O
. O

section 11
id pdf2json/2021.acl-long.150.pdf.json
for O
coreference O
we O
train O
on O
wikipedia O
data O
, O
and O
for O
hatespeech O
detection O
we O
train O
on O
english O
tweets O
or O
spanish O
tweets O
, O
7this O
fact O
is O
the O
premise O
behind O
the O
work O
of O
stanovsky O
et O
al O
. O

section 11
id pdf2json/2021.acl-long.150.pdf.json
( O
2019 O
) O
who O
use O
the O
explicit O
marking O
in O
translation O
to O
reveal O
bias O
. O

section 11
id pdf2json/2021.acl-long.150.pdf.json
consistent O
with O
the O
task.8 O
our O
english O
coreference O
system O
is O
trained O
on O
ontonotes O
( O
weischedel O
et O
al. O
, O
2017 O
) O
and O
evaluated O
on O
winobias O
( O
zhao O
et O
al. O
, O
2018a O
) O
, O
a O
winograd-schema O
style O
challenge O
set O
designed O
to O
measure O
gender O
bias O
in O
coreference O
resolution O
. O

section 11
id pdf2json/2021.acl-long.150.pdf.json
english O
hate O
speech O
detection O
uses O
the O
abusive O
tweets O
dataset O
of O
founta O
et O
al O
. O

section 11
id pdf2json/2021.acl-long.150.pdf.json
( O
2018 O
) O
, O
and O
is O
evaluated O
on O
the O
test O
set O
of O
ten O
thousand O
tweets O
, O
which O
we O
have O
hand O
labelled O
as O
targeted O
male O
, O
female O
, O
and O
neutral O
( O
we O
release O
this O
labelled O
test O
set O
for O
future O
work O
) O
. O

section 11
id pdf2json/2021.acl-long.150.pdf.json
spanish O
hate O
speech O
detection O
uses O
the O
data O
from O
the O
shared O
task O
of O
basile O
et O
al O
. O

section 11
id pdf2json/2021.acl-long.150.pdf.json
( O
2019 O
) O
, O
which O
contains O
labels O
for O
comments O
directed O
at O
women O
and O
directed O
at O
migrants O
. O

section 12
id pdf2json/2021.acl-long.150.pdf.json
both O
weat O
and O
bias O
modification O
methods O
depend O
on O
seed O
wordlists.9 O
these O
wordlists O
are O
closely O
related O
to O
each O
other O
, O
and O
we O
match O
them O
by O
type O
of O
bias O
, O
such O
that O
we O
measure O
weat O
tests O
for O
gender O
bias O
with O
embeddings O
modified O
via O
gender O
bias O
wordlists O
( O
themselves O
derived O
from O
weat O
lists O
, O
as O
detailed O
below O
) O
and O
weat O
tests O
for O
migrant O
bias O
with O
embeddings O
modified O
for O
migrant O
bias O
. O

section 12
id pdf2json/2021.acl-long.150.pdf.json
weat O
wordlists O
are O
standardised O
, O
and O
for O
english O
we O
use O
the O
three O
weat O
test O
wordlists O
( O
numbers O
6,7,8 O
) O
for O
gender.10 O
to O
generate O
bias O
modification O
wordlists O
we O
follow O
the O
approach O
of O
lauscher O
et O
al O
. O

section 12
id pdf2json/2021.acl-long.150.pdf.json
( O
2020 O
) O
and O
use O
a O
pretrained O
set O
of O
embeddings O
( O
from O
spacy.io O
) O
to O
expand O
the O
set O
of O
weat O
words O
to O
their O
100 O
unique O
nearest O
neighbours O
. O

section 12
id pdf2json/2021.acl-long.150.pdf.json
for O
all O
experiments O
, O
we O
take O
the O
union O
of O
all O
weat O
terms O
, O
expand O
them O
, O
and O
use O
this O
expanded O
set O
for O
both O
dataset O
balancing O
and O
for O
attract-repel.11 O
for O
gender O
bias O
in O
coreference O
and O
hate O
speech O
, O
we O
use O
terms O
that O
are O
male O
vs. O
female O
and O
are O
career O
, O
math O
, O
science O
, O
vs. O
family O
, O
art O
. O

section 12
id pdf2json/2021.acl-long.150.pdf.json
for O
gender O
bias O
and O
migrant O
bias O
in O
spanish O
hate O
speech O
, O
we O
compare O
male/female O
identity O
or O
migrant/non-migrant O
identity O
with O
pleasantunpleasant O
term O
expansions.12 O
8details O
of O
datasets O
& O
preprocessing O
are O
in O
appendix O
c. O
9weat O
uses O
wordlists O
to O
measure O
relationships O
between O
words O
in O
the O
space O
, O
and O
bias O
modification O
depends O
on O
identifying O
words O
to O
sub O
or O
supersample O
( O
for O
databalancing O
) O
, O
or O
to O
adjust O
( O
for O
attract-repel O
) O
. O

section 12
id pdf2json/2021.acl-long.150.pdf.json
many O
other O
debiasing O
methods O
that O
we O
did O
not O
use O
( O
e.g O
bolukbasi O
et O
al O
. O

section 12
id pdf2json/2021.acl-long.150.pdf.json
( O
2016 O
) O
) O
also O
use O
wordlists O
. O

section 12
id pdf2json/2021.acl-long.150.pdf.json
10all O
weat O
wordlists O
are O
in O
appendix O
b O
. O

section 12
id pdf2json/2021.acl-long.150.pdf.json
we O
make O
a O
small O
substitution O
of O
general O
gender O
words O
instead O
of O
proper O
names O
in O
weat O
6 O
, O
as O
proper O
names O
by O
design O
do O
not O
appear O
in O
our O
coreference O
task O
. O

section 12
id pdf2json/2021.acl-long.150.pdf.json
11final O
word O
sets O
are O
200-400 O
words O
, O
due O
to O
significant O
overlap O
in O
nearest O
neighbors O
& O
manual O
removal O
of O
odd O
terms O
. O

section 12
id pdf2json/2021.acl-long.150.pdf.json
12we O
did O
additionally O
experiment O
with O
using O
the O
exact O

section 13
id pdf2json/2021.acl-long.150.pdf.json
we O
substantially O
modified O
spanish O
weat O
( O
aka O
xweat O
for O
non-english O
weats O
) O
and O
added O
entirely O
new O
terms O
. O

section 13
id pdf2json/2021.acl-long.150.pdf.json
the O
reason O
for O
this O
is O
that O
the O
original O
xweat O
was O
translated O
from O
english O
very O
literally O
, O
which O
causes O
two O
problems O
. O

section 13
id pdf2json/2021.acl-long.150.pdf.json
the O
first O
problem O
with O
xweat O
is O
that O
many O
of O
the O
terms O
do O
not O
make O
sense O
in O
a O
spanish O
speaking O
community O
— O
names O
included O
in O
the O
original O
, O
like O
amy O
, O
are O
names O
in O
spanish O
and O
thus O
were O
untranslated O
, O
but O
are O
uncommon O
and O
have O
upper O
class O
connotations O
not O
intended O
in O
the O
original O
test O
. O

section 13
id pdf2json/2021.acl-long.150.pdf.json
another O
example O
is O
firearms O
translated O
as O
arma O
de O
fuego O
, O
which O
while O
technically O
a O
correct O
literal O
translation O
, O
is O
not O
commonly O
used O
to O
describe O
weapons.13 O
the O
second O
problem O
with O
xweat O
is O
that O
nouns O
on O
the O
wordlists O
for O
both O
abstract O
math O
and O
science O
concepts O
as O
well O
as O
abstract O
art O
concepts O
are O
almost O
entirely O
grammatically O
female O
. O

section 13
id pdf2json/2021.acl-long.150.pdf.json
for O
instance O
, O
ciencia O
( O
science O
) O
, O
geometrı́a O
( O
geometry O
) O
are O
grammatically O
female O
, O
as O
are O
escultura O
( O
sculpture O
) O
and O
novela O
( O
novel O
) O
. O

section 13
id pdf2json/2021.acl-long.150.pdf.json
it O
is O
well O
established O
that O
for O
languages O
with O
grammatical O
gender O
, O
words O
that O
share O
a O
grammatical O
gender O
have O
embeddings O
that O
are O
closer O
together O
than O
words O
that O
do O
not O
( O
gonen O
et O
al. O
, O
2019 O
; O
mccurdy O
and O
serbetci O
, O
2017 O
) O
. O

section 13
id pdf2json/2021.acl-long.150.pdf.json
so O
, O
when O
weat O
in O
english O
was O
translated O
into O
xweat O
in O
spanish O
( O
glavas O
et O
al. O
, O
2019 O
) O
, O
the O
terms O
were O
imbalanced O
with O
regard O
to O
grammatical O
gender O
, O
which O
makes O
the O
results O
misleading O
. O

section 13
id pdf2json/2021.acl-long.150.pdf.json
we O
balance O
the O
lists O
, O
often O
replacing O
abstract O
nouns O
with O
corresponding O
adjectives O
which O
can O
take O
male O
or O
female O
form O
, O
e.g O
. O

section 13
id pdf2json/2021.acl-long.150.pdf.json
cientı́fico O
and O
cientı́fica O
( O
scientific O
, O
male O
and O
female O
) O
, O
such O
that O
we O
can O
use O
both O
versions O
to O
account O
for O
the O
effect O
of O
grammatical O
gender O
. O

section 13
id pdf2json/2021.acl-long.150.pdf.json
finally O
, O
we O
needed O
a O
metric O
to O
examine O
bias O
against O
migrants O
. O

section 13
id pdf2json/2021.acl-long.150.pdf.json
metrics O
for O
intrinsic O
bias O
must O
be O
targeted O
to O
the O
type O
of O
harm O
expected O
in O
the O
downstream O
application O
, O
and O
there O
is O
not O
an O
out-ofthe-box O
weat O
test O
for O
this O
. O

section 13
id pdf2json/2021.acl-long.150.pdf.json
so O
we O
create O
a O
new O
weat O
test O
for O
bias O
against O
migrants O
in O
spanish O
. O

section 13
id pdf2json/2021.acl-long.150.pdf.json
following O
the O
setup O
of O
tests O
for O
racial O
bias O
in O
original O
weat O
— O
based O
on O
american O
racial O
biases O
in O
english O
— O
we O
have O
lists O
of O
names O
associated O
with O
migrants O
vs. O
non-migrants O
, O
and O
compare O
them O
with O
lists O
of O
pleasant O
and O
unpleasant O
terms O
. O

section 13
id pdf2json/2021.acl-long.150.pdf.json
the O
names O
are O
based O
on O
work O
of O
salamanca O
and O
pereira O
weat O
terms O
for O
debiasing O
, O
and O
found O
the O
trends O
to O
be O
similar O
but O
of O
smaller O
magnitude O
, O
so O
we O
settled O
on O
expanded O
lists O
as O
a O
more O
realistic O
scenario O
. O

section 13
id pdf2json/2021.acl-long.150.pdf.json
13the O
standard O
would O
be O
armas O
. O

section 13
id pdf2json/2021.acl-long.150.pdf.json
arma O
de O
fuego O
is O
also O
composed O
of O
three O
words O
, O
and O
so O
will O
not O
appear O
in O
any O
vocabulary O
. O

section 13
id pdf2json/2021.acl-long.150.pdf.json
( O
2013 O
) O
, O
who O
studied O
ranking O
names O
as O
lower O
vs. O
upper O
class O
; O
class O
status O
is O
closely O
correlated O
with O
whether O
a O
person O
is O
a O
migrant O
. O

section 13
id pdf2json/2021.acl-long.150.pdf.json
we O
select O
a O
subset O
of O
names O
in O
which O
the O
majority O
in O
the O
study O
agree O
on O
the O
class O
. O

section 13
id pdf2json/2021.acl-long.150.pdf.json
pleasant O
and O
unpleasant O
terms O
exist O
in O
weat O
and O
xweat O
, O
but O
we O
again O
modify O
them O
to O
balance O
grammatical O
gender O
. O

section 14
id pdf2json/2021.acl-long.150.pdf.json
figure O
3 O
displays O
data O
for O
all O
tasks O
: O
one O
scatterplot O
per O
triple O
of O
experimental O
variables O
: O
an O
intrinsic O
metric O
, O
an O
extrinsic O
metric O
, O
an O
embedding O
algorithm O
. O

section 14
id pdf2json/2021.acl-long.150.pdf.json
if O
we O
want O
to O
be O
able O
to O
broadly O
use O
weat O
metrics O
for O
any O
given O
bias O
research O
, O
these O
graphs O
should O
each O
show O
a O
clear O
and O
a O
positive O
correlation O
. O

section 14
id pdf2json/2021.acl-long.150.pdf.json
none O
of O
them O
do O
. O

section 14
id pdf2json/2021.acl-long.150.pdf.json
there O
are O
no O
trends O
in O
correlation O
between O
the O
metrics O
that O
hold O
in O
all O
cases O
regardless O
of O
experimental O
detail O
, O
for O
any O
of O
the O
tasks O
. O

section 14
id pdf2json/2021.acl-long.150.pdf.json
we O
have O
additionally O
examined O
whether O
there O
are O
correlations O
within O
one O
bias O
modification O
method O
( O
pre O
or O
postprocessing O
) O
in O
case O
a O
difference O
in O
the O
way O
these O
methods O
modify O
embeddings O
causes O
differences O
in O
trends O
. O

section 14
id pdf2json/2021.acl-long.150.pdf.json
in O
most O
cases O
this O
breakout O
tells O
the O
same O
story O
. O

section 14
id pdf2json/2021.acl-long.150.pdf.json
the O
select O
cases O
where O
positive O
( O
and O
negative O
) O
correlations O
are O
present O
are O
discussed O
below O
. O

section 14
id pdf2json/2021.acl-long.150.pdf.json
further O
breakout O
graphs O
and O
combinations O
are O
included O
in O
appendix O
d. O
coreference O
( O
en O
) O
: O
gender O
the O
coreference O
task O
( O
figure O
3 O
, O
rows O
1-3 O
) O
doesn O
’ O
t O
display O
a O
clear O
correlation O
in O
all O
cases O
, O
and O
yet O
it O
has O
the O
clearest O
relationship O
of O
all O
three O
tasks O
, O
with O
a O
significant O
moderate O
positive O
correlation O
for O
both O
predictive O
parity O
( O
precision O
) O
and O
equality O
of O
opportunity O
( O
recall O
) O
for O
word2vec O
( O
columns O
3 O
& O
4 O
) O
. O

section 14
id pdf2json/2021.acl-long.150.pdf.json
the O
overall O
trends O
are O
muddied O
by O
the O
data O
for O
fasttext O
, O
which O
does O
not O
have O
a O
significant O
correlation O
under O
any O
conditions O
. O

section 14
id pdf2json/2021.acl-long.150.pdf.json
both O
are O
expected O
: O
that O
coreference O
would O
display O
the O
strongest O
trends O
, O
and O
that O
fasttext O
would O
display O
more O
unpredictable O
or O
weaker O
trends O
. O

section 14
id pdf2json/2021.acl-long.150.pdf.json
the O
winobias O
coreference O
task O
is O
as O
directly O
matched O
to O
the O
weat O
tests O
as O
it O
is O
possible O
to O
be O
- O
since O
both O
use O
common O
career O
words O
to O
measure O
bias O
. O

section 14
id pdf2json/2021.acl-long.150.pdf.json
so O
the O
relationship O
between O
the O
two O
metrics O
is O
clearest O
here O
: O
moving O
female O
terms O
closer O
to O
certain O
career O
terms O
most O
directly O
helps O
a O
system O
resolve O
anti-stereotypical O
coreference O
chains O
. O

section 14
id pdf2json/2021.acl-long.150.pdf.json
however O
, O
we O
still O
only O
see O
a O
correlation O
in O
wod2vec O
, O
not O
fasttext O
. O

section 14
id pdf2json/2021.acl-long.150.pdf.json
fasttext O
may O
behave O
less O
predictably O
because O
of O
its O
use O
of O
subwords O
; O
when O
subwords O
are O
used O
, O
word O
representations O
are O
more O
interconnected.14 O
we O
can O
debias O
with O
regard O
to O
a O
specific O
word O
, O
but O
that O
word O
’ O
s O
embedding O
will O
still O
be O
influenced O
by O
all O
other O
words O
that O
share O
its O
character O
ngrams O
. O

section 14
id pdf2json/2021.acl-long.150.pdf.json
it O
is O
difficult O
to O
predict O
how O
changing O
the O
composition O
of O
a O
training O
corpus O
will O
affect O
all O
words O
that O
contain O
a O
certain O
ngram O
( O
e.g O
. O

section 14
id pdf2json/2021.acl-long.150.pdf.json
ch O
) O
in O
them O
. O

section 14
id pdf2json/2021.acl-long.150.pdf.json
for O
this O
reason O
, O
fasttext O
may O
be O
initially O
more O
resistant O
to O
encoding O
biases O
than O
word2vec O
, O
as O
was O
found O
in O
lauscher O
and O
glavas O
( O
2019 O
) O
, O
but O
may O
also O
be O
more O
complex O
to O
debias O
. O

section 14
id pdf2json/2021.acl-long.150.pdf.json
this O
has O
implications O
for O
extending O
this O
work O
to O
contextual O
models O
, O
which O
always O
use O
some O
form O
of O
subword O
unit O
. O

section 14
id pdf2json/2021.acl-long.150.pdf.json
hatespeech O
( O
en O
) O
: O
gender O
hatespeech O
( O
en O
) O
has O
fewer O
and O
more O
restricted O
correlations O
than O
coreference O
, O
as O
can O
be O
seen O
in O
figure O
3 O
, O
rows O
4-6 O
. O

section 14
id pdf2json/2021.acl-long.150.pdf.json
these O
plots O
show O
no O
relationship O
at O
all O
between O
intrinsic O
and O
extrinsic O
metrics O
. O

section 14
id pdf2json/2021.acl-long.150.pdf.json
when O
data O
is O
broken O
out O
by O
bias O
modification O
method O
( O
see O
figure O
4b O
in O
appendix O
d O
) O
, O
it O
becomes O
clear O
that O
there O
is O
a O
moderate O
positive O
correlation O
for O
postprocessing O
for O
recall O
, O
and O
the O
aggregate O
appears O
this O
way O
because O
there O
is O
a O
moderate O
negative O
correlation O
for O
preprocessing O
. O

section 14
id pdf2json/2021.acl-long.150.pdf.json
this O
holds O
for O
both O
embedding O
algorithms O
, O
though O
both O
positive O
and O
negative O
correlations O
are O
stronger O
for O
fasttext O
. O

section 14
id pdf2json/2021.acl-long.150.pdf.json
precision O
displays O
no O
correlation O
. O

section 14
id pdf2json/2021.acl-long.150.pdf.json
note O
that O
the O
absolute O
variance O
in O
recall O
is O
much O
smaller O
than O
for O
precision O
, O
but O
this O
is O
still O
significant O
for O
each O
embedding O
algorithm O
individually O
and O
for O
both O
grouped O
together O
. O

section 14
id pdf2json/2021.acl-long.150.pdf.json
of O
interest O
for O
future O
bias O
research O
is O
that O
the O
baseline O
level O
of O
bias O
( O
premodification O
, O
from O
raw O
twitter O
data O
) O
in O
english O
hatespeech O
differs O
by O
embedding O
type O
, O
but O
only O
for O
precision O
. O

section 14
id pdf2json/2021.acl-long.150.pdf.json
initial O
models O
( O
with O
unmodified O
embeddings O
) O
using O
fasttext O
have O
10 O
additional O
points O
of O
precision O
for O
maletargeted O
hatespeech O
than O
for O
female-targeted O
. O

section 14
id pdf2json/2021.acl-long.150.pdf.json
however O
initial O
models O
using O
word2vec O
have O
the O
opposite O
bias O
and O
have O
4 O
fewer O
points O
of O
precision O
for O
male-targeted O
than O
female O
targeted O
hatespeech O
. O

section 14
id pdf2json/2021.acl-long.150.pdf.json
for O
recall O
, O
the O
two O
embedding O
algorithms O
are O
equivalent O
, O
with O
6 O
fewer O
points O
for O
male-targeted O
hatespeech O
. O

section 14
id pdf2json/2021.acl-long.150.pdf.json
in O
fact O
, O
in O
the O
recall O
metric O
there O
is O
an O
early O
indication O
of O
unreliability O
of O
the O
relationship O
we O
are O
examining O
between O
weat O
and O
extrinsic O
bias O
, O
because O
there O
is O
a O
spread O
of O
different O
weat O
results O
that O
map O
to O
nearly O
the O
same O
difference O
in O
recall O
. O

section 14
id pdf2json/2021.acl-long.150.pdf.json
14for O
example O
, O
the O
representation O
of O
the O
word O
childish O
is O
by O
design O
also O
made O
up O
of O
the O
representations O
for O
child O
and O
ish O
, O
but O
also O
all O
unigrams O
, O
bigrams O
, O
and O
trigrams O
it O
contains O
( O
c O
, O
ch O
, O
chi O
, O
etc O
) O
. O

section 14
id pdf2json/2021.acl-long.150.pdf.json
hatespeech O
( O
es O
) O
: O
gender O
and O
migrant O
for O
hatespeech O
in O
spanish O
, O
we O
examine O
two O
kinds O
of O
bias O
separately O
— O
gender O
bias O
and O
bias O
against O
migrants O
, O
in O
figure O
3 O
, O
rows O
7,8 O
. O

section 14
id pdf2json/2021.acl-long.150.pdf.json
neither O
gender O
bias O
nor O
migrant O
bias O
show O
positive O
correlations O
in O
any O
experimental O
conditions O
. O

section 14
id pdf2json/2021.acl-long.150.pdf.json
gender O
bias O
in O
our O
models O
is O
in O
an O
absolute O
sense O
never O
present O
, O
since O
in O
spanish O
hatespeech O
targeted O
against O
women O
is O
easier O
to O
identify O
than O
against O
others O
( O
with O
f1 O
in O
the O
high O
80s O
) O
.15 O
but O
there O
are O
no O
overall O
trends O
when O
this O
is O
bias O
is O
modified O
to O
be O
more O
or O
less O
extreme O
, O
and O
there O
are O
no O
positive O
correlations O
in O
any O
conditions O
. O

section 14
id pdf2json/2021.acl-long.150.pdf.json
there O
is O
a O
moderate O
negative O
correlation O
for O
precision O
only O
when O
looking O
at O
fasttext O
embeddings O
. O

section 14
id pdf2json/2021.acl-long.150.pdf.json
migrant O
bias O
similarly O
has O
no O
trends O
save O
in O
very O
restricted O
conditions O
broken O
out O
by O
bias O
modification O
type O
. O

section 14
id pdf2json/2021.acl-long.150.pdf.json
in O
contrast O
to O
the O
gender O
case O
, O
hatespeech O
against O
migrants O
is O
clearly O
challenging O
to O
identify O
, O
with O
much O
lower O
f1 O
in O
the O
low O
60s O
. O

section 14
id pdf2json/2021.acl-long.150.pdf.json
there O
is O
a O
positive O
correlation O
between O
migrant O
bias O
and O
performance O
gap O
for O
recall O
with O
preprocessing O
in O
fasttext O
only O
. O

section 14
id pdf2json/2021.acl-long.150.pdf.json
this O
fits O
the O
expectation O
that O
fasttext O
may O
be O
more O
sensitive O
to O
preprocessing O
than O
postprocessing O
due O
to O
subwords O
, O
as O
discussed O
above O
, O
though O
in O
the O
gender O
bias O
case O
with O
negative O
correlation O
it O
is O
equally O
sensitive O
to O
both O
, O
so O
it O
is O
hard O
to O
draw O
conclusions O
. O

section 14
id pdf2json/2021.acl-long.150.pdf.json
given O
the O
smaller O
number O
of O
datapoints O
for O
spanish O
( O
discussed O
below O
) O
this O
is O
likely O
just O
noise O
. O

section 14
id pdf2json/2021.acl-long.150.pdf.json
to O
confuse O
the O
situation O
further O
, O
the O
only O
trends O
in O
precision O
are O
present O
in O
word2vec O
, O
and O
are O
negative O
correlations O
. O

section 14
id pdf2json/2021.acl-long.150.pdf.json
note O
that O
all O
graphs O
for O
spanish O
display O
central O
clusters O
, O
because O
it O
was O
more O
difficult O
to O
get O
an O
even O
spread O
of O
bias O
measures O
, O
and O
because O
spanish O
has O
fewer O
data O
points O
than O
english O
. O

section 14
id pdf2json/2021.acl-long.150.pdf.json
this O
is O
for O
a O
number O
of O
reasons O
that O
compound O
and O
underscore O
the O
difficulty O
of O
expanding O
supposedly O
languageagnostic O
techniques O
beyond O
english O
, O
even O
to O
high O
resourced O
languages O
like O
spanish O
. O

section 14
id pdf2json/2021.acl-long.150.pdf.json
we O
have O
only O
one O
weat O
test O
for O
each O
type O
of O
bias O
, O
since O
we O
made O
our O
own O
that O
carefully O
balanced O
grammatical O
gender O
, O
after O
rectifying O
the O
issues O
with O
the O
existing O
translated O
versions O
( O
see O
section O
4.3 O
) O
. O

section 14
id pdf2json/2021.acl-long.150.pdf.json
bias O
modification O
is O
also O
more O
difficult O
- O
the O
richer O
agreement O
system O
in O
spanish O
means O
that O
there O
are O
more O
surface O
forms O
of O
what O
would O
be O
one O
word O
in O
english O
. O

section 14
id pdf2json/2021.acl-long.150.pdf.json
in O
addition O
, O
the O
language O
model O
used O
for O
nearest O
neighbour O
expansion O
of O
wordlists O
( O
see O
sec- O
15this O
is O
perhaps O
due O
to O
examples O
in O
the O
training O
data O
having O
clearer O
markers O
such O
as O
specific O
anti-female O
slurs O
, O
but O
is O
itself O
an O
interesting O
question O
. O

section 14
id pdf2json/2021.acl-long.150.pdf.json
tion O
4.2 O
) O
produces O
predominantly O
formal O
register O
words O
from O
news O
or O
scientific O
articles O
, O
due O
to O
a O
less O
varied O
makeup O
of O
its O
training O
data O
than O
the O
english O
model O
. O

section 14
id pdf2json/2021.acl-long.150.pdf.json
this O
makes O
them O
less O
well O
suited O
to O
debiasing O
twitter O
data O
specifically O
, O
and O
there O
were O
no O
readily O
available O
models O
that O
had O
more O
casual O
register O
. O

section 14
id pdf2json/2021.acl-long.150.pdf.json
for O
bias O
against O
migrants O
, O
there O
is O
the O
additional O
challenge O
that O
wordlists O
are O
predominantly O
based O
on O
proper O
names O
, O
which O
are O
much O
rarer O
in O
twitter O
( O
which O
tends O
to O
use O
@ O
mentions O
instead O
) O
than O
in O
other O
media O
. O

section 15
id pdf2json/2021.acl-long.150.pdf.json
the O
broad O
result O
of O
this O
research O
is O
that O
changes O
in O
weat O
do O
not O
correlate O
with O
changes O
in O
application O
bias O
, O
and O
therefore O
that O
weat O
should O
not O
be O
used O
to O
measure O
progress O
in O
debiasing O
algorithms O
. O

section 15
id pdf2json/2021.acl-long.150.pdf.json
we O
have O
found O
that O
even O
when O
we O
maximally O
target O
bias O
modification O
of O
an O
embedding O
, O
we O
can O
not O
produce O
a O
reliable O
change O
in O
bias O
direction O
downstream O
. O

section 15
id pdf2json/2021.acl-long.150.pdf.json
there O
was O
no O
pattern O
or O
correlation O
between O
tasks O
, O
for O
the O
same O
task O
in O
different O
languages O
, O
or O
even O
in O
most O
cases O
within O
one O
task O
. O

section 15
id pdf2json/2021.acl-long.150.pdf.json
and O
we O
have O
chosen O
one O
of O
the O
simplest O
possible O
setups O
, O
with O
fullword O
embeddings O
and O
a O
single O
type O
of O
bias O
at O
a O
time O
. O

section 15
id pdf2json/2021.acl-long.150.pdf.json
real O
world O
scenarios O
can O
easily O
be O
more O
complicated O
and O
involve O
multiple O
types O
of O
bias O
or O
subword O
embeddings O
. O

section 15
id pdf2json/2021.acl-long.150.pdf.json
our O
findings O
also O
indicate O
that O
additional O
complexity O
may O
muddy O
the O
relationship O
further O
. O

section 15
id pdf2json/2021.acl-long.150.pdf.json
for O
example O
, O
fasttext O
behaved O
less O
predictably O
than O
word2vec O
across O
experiments O
, O
suggesting O
that O
if O
we O
were O
to O
expand O
to O
larger O
models O
that O
are O
fully O
reliant O
on O
subwords O
the O
patterns O
may O
become O
even O
less O
clear O
. O

section 15
id pdf2json/2021.acl-long.150.pdf.json
the O
implication O
of O
this O
finding O
is O
that O
an O
nlp O
scientist O
or O
engineer O
has O
limited O
options O
when O
investigating O
and O
mitigating O
bias O
. O

section 15
id pdf2json/2021.acl-long.150.pdf.json
they O
must O
a O
) O
find O
the O
specific O
set O
of O
wordlists O
, O
embedding O
algorithms O
, O
downstream O
tasks O
, O
and O
bias O
modification O
methods O
that O
are O
together O
predictive O
of O
bias O
for O
the O
given O
task O
, O
language O
, O
and O
model O
or O
b O
) O
implement O
full O
systems O
to O
test O
application O
bias O
directly O
, O
even O
if O
their O
work O
focuses O
on O
embeddings O
. O

section 15
id pdf2json/2021.acl-long.150.pdf.json
while O
the O
latter O
may O
seem O
onerous O
, O
it O
may O
not O
be O
more O
so O
than O
exhaustively O
searching O
for O
a O
configuration O
where O
intrinsic O
bias O
metrics O
are O
predictive O
. O

section 15
id pdf2json/2021.acl-long.150.pdf.json
this O
underscores O
the O
importance O
of O
making O
good O
downstream O
bias O
measures O
available O
, O
as O
either O
approach O
will O
require O
these O
. O

section 15
id pdf2json/2021.acl-long.150.pdf.json
more O
datasets O
that O
are O
collected O
need O
to O
be O
annotated O
with O
subgroup O
demographic O
and O
identity O
information O
— O
there O
are O
very O
few O
available O
. O

section 15
id pdf2json/2021.acl-long.150.pdf.json
more O
research O
needs O
to O
focus O
on O
creating O
good O
challenge O
sets O
to O
measure O
application O
bias O
. O

section 15
id pdf2json/2021.acl-long.150.pdf.json
additional O
research O
on O
more O
broad O
usage O
of O
unsupervised O
methods O
( O
zhao O
and O
chang O
, O
2020 O
) O
would O
also O
be O
valuable O
, O
though O
those O
also O
would O
benefit O
from O
subgroup O
identity O
annotation O
to O
make O
their O
results O
more O
interpretable O
. O

section 15
id pdf2json/2021.acl-long.150.pdf.json
it O
is O
only O
when O
more O
of O
these O
things O
are O
readily O
available O
that O
we O
can O
see O
the O
true O
measure O
of O
the O
efficacy O
of O
our O
debiasing O
efforts O
. O

section 15
id pdf2json/2021.acl-long.150.pdf.json
we O
do O
note O
a O
limitation O
of O
this O
study O
in O
that O
all O
downstream O
tasks O
are O
discriminative O
classification O
tasks O
. O

section 15
id pdf2json/2021.acl-long.150.pdf.json
bias O
in O
classification O
is O
more O
straightforward O
to O
measure O
, O
with O
well O
established O
metrics O
, O
but O
covers O
allocational O
harms O
( O
performance O
disparity O
) O
, O
whereas O
the O
inclusion O
of O
generative O
models O
could O
better O
cover O
representational O
harms O
( O
misleading O
or O
harmful O
representations/portrayals O
) O
( O
blodgett O
et O
al. O
, O
2020 O
; O
crawford O
, O
2017 O
) O
. O

section 15
id pdf2json/2021.acl-long.150.pdf.json
concurrent O
research O
on O
causal O
mediation O
analysis O
for O
bias O
has O
shown O
that O
the O
embedding O
layer O
in O
open-domain O
generation O
has O
the O
strongest O
effect O
on O
gender O
bias O
( O
as O
compared O
to O
other O
layers O
of O
the O
network O
) O
( O
vig O
et O
al. O
, O
2020 O
) O
. O

section 15
id pdf2json/2021.acl-long.150.pdf.json
further O
work O
could O
investigate O
whether O
generation O
tasks O
have O
display O
the O
same O
or O
different O
relationship O
to O
intrinsic O
metrics O
. O

section 16
id pdf2json/2021.acl-long.150.pdf.json
we O
have O
examined O
the O
relationship O
of O
the O
intrinsic O
bias O
metric O
weat O
to O
the O
extrinsic O
bias O
metrics O
of O
equality O
of O
opportunity O
and O
predictive O
parity O
, O
for O
multiple O
tasks O
and O
languages O
, O
and O
determined O
that O
positive O
correlations O
between O
them O
exist O
only O
in O
very O
restricted O
settings O
. O

section 16
id pdf2json/2021.acl-long.150.pdf.json
in O
many O
cases O
there O
is O
either O
negative O
correlation O
or O
none O
at O
all O
. O

section 16
id pdf2json/2021.acl-long.150.pdf.json
while O
intrinsic O
metrics O
such O
as O
weat O
remain O
good O
descriptive O
metrics O
for O
computational O
social O
science O
, O
and O
for O
examining O
bias O
in O
human O
texts O
, O
we O
advise O
that O
the O
nlp O
community O
not O
rely O
on O
them O
for O
measuring O
model O
bias O
. O

section 16
id pdf2json/2021.acl-long.150.pdf.json
we O
instead O
advise O
that O
they O
focus O
on O
careful O
consideration O
of O
downstream O
applications O
and O
the O
creation O
of O
datasets O
and O
challenge O
sets O
that O
enable O
measurement O
at O
this O
stage O
. O

section 17
id pdf2json/2021.acl-long.150.pdf.json
we O
thank O
andreas O
grivas O
, O
kate O
mccurdy O
, O
yevgen O
matusevych O
, O
elizabeth O
nielsen O
, O
ramon O
sanabria O
, O
ida O
szubert O
, O
sabine O
weber O
, O
björn O
ross O
, O
agostina O
calabrese O
, O
and O
eddie O
ungless O
for O
comments O
on O
earlier O
drafts O
of O
this O
paper O
. O

section 18
id pdf2json/2021.acl-long.150.pdf.json
performance O
gap O
metrics O
measure O
difference O
in O
performance O
across O
different O
demographic O
splits O
of O
the O
data O
, O
and O
are O
in O
our O
case O
( O
and O
most O
commonly O
) O
applied O
to O
classification O
tasks O
. O

section 18
id pdf2json/2021.acl-long.150.pdf.json
where O
a O
is O
a O
demographic O
variable O
( O
race O
, O
gender O
, O
etc O
) O
, O
y O
is O
the O
true O
label O
, O
and O
ŷ O
is O
the O
predicted O
label O
, O
a O
fair O
system O
will O
satisfy O
: O
p O
( O
ŷ O
= O
1|a O
= O
x O
, O
y O
= O
1 O
) O
= O
p O
( O
ŷ O
= O
1|a O
= O
y O
, O
y O
= O
1 O
) O
where O
x O
and O
y O
are O
demographic O
values O
usually O
of O
an O
privileged O
and O
a O
underprivileged O
group O
. O

section 18
id pdf2json/2021.acl-long.150.pdf.json
this O
expresses O
that O
the O
probability O
of O
a O
given O
test O
sample O
being O
correctly O
identified O
as O
a O
true O
positive O
should O
be O
equal O
regardless O
of O
group O
, O
and O
is O
known O
as O
equality O
of O
opportunity O
( O
hardt O
et O
al. O
, O
2016 O
) O
. O

section 18
id pdf2json/2021.acl-long.150.pdf.json
a O
fair O
system O
will O
also O
satisfy O
: O
p O
( O
ŷ O
= O
1|a O
= O
x O
, O
y O
= O
0 O
) O
= O
p O
( O
ŷ O
= O
1|a O
= O
y O
, O
y O
= O
0 O
) O
which O
expresses O
that O
that O
probability O
of O
a O
given O
test O
sample O
being O
incorrectly O
identified O
as O
positive O
is O
equal O
regardless O
of O
group O
. O

section 18
id pdf2json/2021.acl-long.150.pdf.json
this O
is O
known O
as O
predictive O
parity O
and O
when O
combined O
with O
equality O
of O
opportunity O
is O
known O
as O
equalized O
odds O
. O

section 18
id pdf2json/2021.acl-long.150.pdf.json
these O
are O
easily O
measured O
in O
most O
nlp O
systems O
. O

section 18
id pdf2json/2021.acl-long.150.pdf.json
the O
former O
is O
captured O
by O
measuring O
recall O
gap O
, O
where O
if O
x O
is O
the O
privileged O
group O
and O
y O
the O
underprivileged O
, O
unfairness O
is O
captured O
by O
recallx O
− O
recally O
, O
where O
any O
positive O
value O
is O
unfair O
. O

section 18
id pdf2json/2021.acl-long.150.pdf.json
the O
latter O
is O
captured O
by O
precisionx O
− O
precisiony O
, O
again O
where O
positive O
values O
are O
unfair O
. O

section 19
id pdf2json/2021.acl-long.150.pdf.json
b.1 O
english O
weat O
lists O
all O
are O
tests O
for O
gender O
bias O
. O

section 19
id pdf2json/2021.acl-long.150.pdf.json
b.1.1 O
weat O
6 O
weat O
6 O
was O
modified O
to O
use O
the O
general O
gender O
terms O
of O
7,8 O
rather O
than O
proper O
names O
, O
because O
the O
co-reference O
task O
contains O
no O
names O
. O

section 19
id pdf2json/2021.acl-long.150.pdf.json
male O
male O
, O
man O
, O
boy O
, O
brother O
, O
he O
, O
him O
, O
his O
, O
son O
female O
female O
, O
woman O
, O
girl O
, O
sister O
, O
she O
, O
her O
, O
hers O
, O
daughter O
career O
executive O
, O
management O
, O
professional O
, O
corporation O
, O
salary O
, O
office O
, O
business O
, O
career O
family O
home O
, O
parents O
, O
children O
, O
family O
, O
cousins O
, O
marriage O
, O
wedding O
, O
relatives O
the O
original O
weat O
6 O
uses O
the O
following O
male O
and O
female O
names O
as O
the O
gender O
terms O
: O
male O
: O
john O
, O
paul O
, O
mike O
, O
kevin O
, O
steve O
, O
greg O
, O
jeff O
, O
bill O
female O
: O
amy O
, O
joan O
, O
lisa O
, O
sarah O
, O
diana O
, O
kate O
, O
ann O
, O
donna O
. O

section 19
id pdf2json/2021.acl-long.150.pdf.json
b.1.2 O
weat O
7 O
male O
male O
, O
man O
, O
boy O
, O
brother O
, O
he O
, O
him O
, O
his O
, O
son O
female O
female O
, O
woman O
, O
girl O
, O
sister O
, O
she O
, O
her O
, O
hers O
, O
daughter O
math O
math O
, O
algebra O
, O
geometry O
, O
calculus O
, O
equations O
, O
computation O
, O
numbers O
, O
addition O
art O
poetry O
, O
art O
, O
dance O
, O
literature O
, O
novel O
, O
symphony O
, O
drama O
, O
sculpture O
b.1.3 O
weat O
8 O
male O
brother O
, O
father O
, O
uncle O
, O
grandfather O
, O
son O
, O
he O
, O
his O
, O
him O
female O
sister O
, O
mother O
, O
aunt O
, O
grandmother O
, O
daughter O
, O
she O
, O
hers O
, O
her O
science O
science O
, O
technology O
, O
physics O
, O
chemistry O
, O
einstein O
, O
nasa O
, O
experiment O
, O
astronomy O
art O
poetry O
, O
art O
, O
shakespeare O
, O
dance O
, O
literature O
, O
novel O
, O
symphony O
, O
drama O
b.2 O
changes O
to O
english O
list O
we O
modify O
weat O
6 O
to O
use O
the O
gender O
terms O
for O
weat O
7/8 O
as O
the O
terms O
for O
6 O
, O
but O
otherwise O
leave O
terms O
as O
is O
. O

section 19
id pdf2json/2021.acl-long.150.pdf.json
weat O
6 O
( O
career/family O
vs. O
male/female O
) O
uses O
proper O
names O
as O
gender O
terms O
, O
whereas O
the O
other O
two O
tests O
use O
more O
standard O
gender O
terms O
( O
she O
, O
her O
, O
he O
, O
him O
, O
mother O
, O
father O
) O
. O

section 19
id pdf2json/2021.acl-long.150.pdf.json
this O
is O
an O
artifact O
of O
replicating O
iat O
, O
which O
introduces O
a O
confound O
in O
their O
comparability O
– O
if O
the O
weat O
tests O
have O
different O
patterns O
of O
correlation O
, O
we O
don O
’ O
t O
know O
whether O
this O
is O
because O
of O
the O
difference O
in O
the O
way O
gender O
bias O
patterns O
for O
career/family O
vs. O
for O
arts/science O
or O
whether O
it O
patterns O
differently O
because O
of O
proper O
names O
vs. O
gender O
terms O
. O

section 19
id pdf2json/2021.acl-long.150.pdf.json
this O
is O
exacerbated O
in O
our O
case O
where O
proper O
names O
are O
treated O
even O
more O
differently O
than O
usual O
both O
in O
twitter O
( O
where O
@ O
mentions O
stand O
in O
for O
proper O
names O
) O
and O
in O
the O
winobias O
metric O
that O
we O
use O
( O
where O
professions O
are O
used O
instead O
of O
proper O
names O
precisely O
because O
names O
contain O
gender O
information O
and O
the O
challenge O
set O
intends O
to O
be O
ambiguous O
) O
. O

section 19
id pdf2json/2021.acl-long.150.pdf.json
b.3 O
spanish O
weat O
lists O
: O
recall O
that O
we O
created O
these O
ourselves O
, O
the O
gender O
test O
with O
reference O
to O
both O
the O
original O
gender O
focused O
weat O
6,7,8 O
of O
caliskan O
et O
al O
. O

section 19
id pdf2json/2021.acl-long.150.pdf.json
( O
2017 O
) O
and O
the O
translation O
of O
lauscher O
and O
glavas O
( O
2019 O
) O
, O
significantly O
modified O
and O
extended O
to O
balance O
grammatical O
gender O
across O
sets O
of O
words O
. O

section 19
id pdf2json/2021.acl-long.150.pdf.json
the O
migrant O
test O
was O
created O
with O
reference O
to O
the O
tests O
for O
racism O
that O
use O
african-american O
vs. O
european-american O
names O
paired O
with O
pleasant O
vs. O
unpleasant O
terms O
in O
weat O
3 O
, O
4 O
, O
5 O
, O
using O
the O
lists O
of O
european O
spanish O
vs. O
migrant O
spanish O
names O
identified O
by O
salamanca O
and O
pereira O
( O
2013 O
) O
. O

section 19
id pdf2json/2021.acl-long.150.pdf.json
b.3.1 O
gender O
male O
: O
masculino O
, O
hombre O
, O
niño O
, O
hermano O
, O
él O
, O
hijo O
, O
hermano O
, O
padre O
, O
papá O
, O
tı́o O
, O
abuelo O
female O
: O
femenino O
, O
mujer O
, O
niña O
, O
hermana O
, O
ella O
, O
hija O
, O
hermana O
, O
madre O
, O
mamá O
, O
tı́a O
, O
abuela O
science O
: O
cientı́fico O
, O
fı́sico O
, O
quı́mico O
, O
astrónomo O
, O
tecnológico O
, O
biólogo O
, O
cientı́fica O
, O
fı́sica O
, O
quı́mica O
, O
astrónoma O
, O
tecnológica O
, O
bióloga O
art O
: O
arquitecto O
, O
escultor O
, O
pintor O
, O
escritor O
, O
poeta O
, O
baiları́n O
, O
actor O
, O
fotógrafo O
, O
arquitecta O
, O
escultora O
, O
pintora O
, O
escritora O
, O
poetisa O
, O
bailarina O
, O
actora O
, O
fotógrafa O
b.3.2 O
migrants O
european-spanish O
names O
: O
agustina O
, O
martina O
, O
josefa O
, O
antonia O
, O
sofı́a O
, O
isidora O
, O
cristóbal O
, O
sebastián O
, O
agustı́n O
, O
alonso O
, O
joaquı́n O
, O
león O
, O
ignacio O
, O
julieta O
, O
matilde O
migrant-spanish O
names O
: O
shirley O
, O
yamileth O
, O
sharon O
, O
britney O
, O
maryori O
, O
melody O
, O
nayareth O
, O
yaritza O
, O
byron O
, O
brian O
, O
jason O
, O
malcon O
, O
justin O
, O
jeremy O
, O
jordan O
, O
brayan O
, O
yeison O
, O
yeremi O
, O
bairon O
, O
yastin O
pleasant O
terms O
: O
caricia O
, O
libertad O
, O
salud O
, O
amor O
, O
paz O
, O
animar O
, O
amistad O
, O
cielo O
, O
lealtad O
, O
placer O
, O
diamante O
, O
gentil O
, O
honestidad O
, O
suerte O
, O
arcoiris O
, O
diploma O
, O
regalo O
, O
honor O
, O
milagro O
, O
amanecer O
, O
familia O
, O
alegrı́a O
, O
felicidad O
, O
risa O
, O
paraı́so O
, O
vacación O
, O
paz O
, O
maravilloso O
, O
maravillosa O
unpleasant O
terms O
: O
abuso O
, O
choque O
, O
suciedad O
, O
asesinato O
, O
enfermedad O
, O
accidente O
, O
muerte O
, O
sufrimiento O
, O
veneno O
, O
hedor O
, O
apestar O
, O
ataque O
, O
asalto O
, O
desastre O
, O
odio O
, O
contaminación O
, O
tragedia O
, O
divorcio O
, O
cárcel O
, O
pobreza O
, O
fea O
, O
feo O
, O
cáncer O
, O
matar O
, O
vómito O
, O
bomba O
, O
maldad O
, O
podrido O
, O
podrida O
, O
agonı́a O
, O
terrible O
, O
horrible O
, O
guerra O
, O
repugnante O

section 20
id pdf2json/2021.acl-long.150.pdf.json
this O
details O
the O
data O
for O
training O
embeddings O
. O

section 20
id pdf2json/2021.acl-long.150.pdf.json
for O
data O
used O
in O
training O
the O
final O
models O
, O
see O
relevant O
papers O
cited O
in O
section O
4.1 O
. O

section 20
id pdf2json/2021.acl-long.150.pdf.json
c.1 O
wikipedia O
wikipedia O
data O
is O
downloaded O
from O
the O
latest O
wikipedia O
article O
dump O
, O
tokenized O
with O
nltk O
( O
https O
: O
//www.nltk.org/ O
) O
, O
and O
all O
words O
appearing O
less O
than O
10 O
times O
are O
replaced O
with O
< O
unk O
> O
. O

section 20
id pdf2json/2021.acl-long.150.pdf.json
the O
final O
dataset O
has O
439,935,872 O
words O
. O

section 20
id pdf2json/2021.acl-long.150.pdf.json
c.2 O
twitter O
twitter O
data O
is O
from O
2019 O
and O
is O
downloaded O
from O
the O
internet O
archive O
https O
: O
//archive.org/ O
details/twitterstream O
. O

section 20
id pdf2json/2021.acl-long.150.pdf.json
retweets O
are O
removed O
, O
and O
data O
is O
lowercased O
, O
tokenized O
with O
nltk O
tweettokenizer O
, O
and O
hashtags O
and O
@ O
mentions O
are O
replaced O
with O
< O
hash O
> O
and O
< O
mention O
> O
respectively O
. O

section 20
id pdf2json/2021.acl-long.150.pdf.json
all O
words O
appearing O
less O
than O
10 O
times O
are O
replaced O
with O
< O
unk O
> O
. O

section 20
id pdf2json/2021.acl-long.150.pdf.json
english O
twitter O
data O
size O
is O
3,641,306 O
tweets O
with O
38,376,060 O
words O
. O

section 20
id pdf2json/2021.acl-long.150.pdf.json
spanish O
twitter O
data O
size O
is O
10,683,846 O
tweets O
with O
142,715,339 O
words O
. O

section 21
id pdf2json/2021.acl-long.150.pdf.json
below O
are O
breakouts O
of O
graphs O
by O
bias O
modification O
method O
, O
as O
well O
as O
full O
graphs O
with O
metric O
scales O
and O
legends O
. O

section 21
id pdf2json/2021.acl-long.150.pdf.json
figure O
4 O
breaks O
out O
all O
tasks O
by O
bias O
modification O
method O
( O
pre- O
vs. O
post-processing O
) O
. O

section 21
id pdf2json/2021.acl-long.150.pdf.json
the O
main O
interesting O
thing O
to O
note O
here O
is O
for O
hatespeech O
in O
english O
. O

section 21
id pdf2json/2021.acl-long.150.pdf.json
based O
on O
the O
spread O
of O
data O
points O
, O
it O
is O
easy O
to O
see O
that O
there O
is O
overall O
more O
effect O
on O
precision O
gap O
when O
embeddings O
are O
modified O
, O
whereas O
recall O
performance O
gap O
occupies O
a O
narrower O
band O
over O
a O
wide O
spread O
of O
weat O
metrics O
. O

section 21
id pdf2json/2021.acl-long.150.pdf.json
yet O
recall O
is O
the O
only O
metric O
which O
has O
a O
positive O
correlation O
with O
weat O
, O
and O
then O
only O
in O
the O
postprocessing O
condition O
. O

section 21
id pdf2json/2021.acl-long.150.pdf.json
for O
spanish O
it O
is O
also O
visible O
that O
it O
is O
much O
more O
difficult O
to O
modify O
bias O
for O
spanish O
when O
preprocessing O
vs. O
when O
postprocessing O
. O

section 21
id pdf2json/2021.acl-long.150.pdf.json
figure O
5 O
shows O
one O
graph O
for O
each O
task O
and O
bias O
type O
combination O
, O
in O
full O
, O
in O
order O
to O
view O
the O
effect O
of O
not O
controlling O
for O
experimental O
variable O
. O

section 21
id pdf2json/2021.acl-long.150.pdf.json
it O
also O
shows O
the O
scale O
for O
the O
spread O
of O
data O
points O
. O

section 21
id pdf2json/2021.acl-long.150.pdf.json
finally O
, O
for O
interest O
, O
we O
also O
include O
figure O
6 O
, O
which O
displays O
the O
correlation O
broken O
out O
by O
type O
of O
winobias O
test O
( O
which O
differ O
in O
difficulty O
because O
type O
1 O
is O
semantic O
and O
type O
2 O
is O
syntactic O
) O
. O

section TITLE
id pdf2json/2021.acl-long.534.pdf.json
are O
missing O
links O
predictable O
? O

section TITLE
id pdf2json/2021.acl-long.534.pdf.json
an O
inferential O
benchmark O
for O
knowledge O
graph O
completion O

section ABSTRACT
id pdf2json/2021.acl-long.534.pdf.json
we O
present O
inferwiki O
, O
a O
knowledge O
graph O
completion O
( O
kgc O
) O
dataset O
that O
improves O
upon O
existing O
benchmarks O
in O
inferential O
ability O
, O
assumptions O
, O
and O
patterns O
. O

section ABSTRACT
id pdf2json/2021.acl-long.534.pdf.json
first O
, O
each O
testing O
sample O
is O
predictable O
with O
supportive O
data O
in O
the O
training O
set O
. O

section ABSTRACT
id pdf2json/2021.acl-long.534.pdf.json
to O
ensure O
it O
, O
we O
propose O
to O
utilize O
rule-guided O
train/test O
generation O
, O
instead O
of O
conventional O
random O
split O
. O

section ABSTRACT
id pdf2json/2021.acl-long.534.pdf.json
second O
, O
inferwiki O
initiates O
the O
evaluation O
following O
the O
open-world O
assumption O
and O
improves O
the O
inferential O
difficulty O
of O
the O
closed-world O
assumption O
, O
by O
providing O
manually O
annotated O
negative O
and O
unknown O
triples O
. O

section ABSTRACT
id pdf2json/2021.acl-long.534.pdf.json
third O
, O
we O
include O
various O
inference O
patterns O
( O
e.g. O
, O
reasoning O
path O
length O
and O
types O
) O
for O
comprehensive O
evaluation O
. O

section ABSTRACT
id pdf2json/2021.acl-long.534.pdf.json
in O
experiments O
, O
we O
curate O
two O
settings O
of O
inferwiki O
varying O
in O
sizes O
and O
structures O
, O
and O
apply O
the O
construction O
process O
on O
codex O
as O
comparative O
datasets O
. O

section ABSTRACT
id pdf2json/2021.acl-long.534.pdf.json
the O
results O
and O
empirical O
analyses O
demonstrate O
the O
necessity O
and O
high-quality O
of O
inferwiki O
. O

section ABSTRACT
id pdf2json/2021.acl-long.534.pdf.json
nevertheless O
, O
the O
performance O
gap O
among O
various O
inferential O
assumptions O
and O
patterns O
presents O
the O
difficulty O
and O
inspires O
future O
research O
direction O
. O

section ABSTRACT
id pdf2json/2021.acl-long.534.pdf.json
our O
datasets O
can O
be O
found O
in O
https O
: O
//github O
. O

section ABSTRACT
id pdf2json/2021.acl-long.534.pdf.json
com/taominer/inferwiki O
. O

section 0
id pdf2json/2021.acl-long.534.pdf.json
proceedings O
of O
the O
59th O
annual O
meeting O
of O
the O
association O
for O
computational O
linguistics O
and O
the O
11th O
international O
joint O
conference O
on O
natural O
language O
processing O
, O
pages O
6855–6865 O
august O
1–6 O
, O
2021 O
. O

section 0
id pdf2json/2021.acl-long.534.pdf.json
©2021 O
association O
for O
computational O
linguistics O
6855 O
we O
present O
inferwiki O
, O
a O
knowledge O
graph O
completion O
( O
kgc O
) O
dataset O
that O
improves O
upon O
existing O
benchmarks O
in O
inferential O
ability O
, O
assumptions O
, O
and O
patterns O
. O

section 0
id pdf2json/2021.acl-long.534.pdf.json
first O
, O
each O
testing O
sample O
is O
predictable O
with O
supportive O
data O
in O
the O
training O
set O
. O

section 0
id pdf2json/2021.acl-long.534.pdf.json
to O
ensure O
it O
, O
we O
propose O
to O
utilize O
rule-guided O
train/test O
generation O
, O
instead O
of O
conventional O
random O
split O
. O

section 0
id pdf2json/2021.acl-long.534.pdf.json
second O
, O
inferwiki O
initiates O
the O
evaluation O
following O
the O
open-world O
assumption O
and O
improves O
the O
inferential O
difficulty O
of O
the O
closed-world O
assumption O
, O
by O
providing O
manually O
annotated O
negative O
and O
unknown O
triples O
. O

section 0
id pdf2json/2021.acl-long.534.pdf.json
third O
, O
we O
include O
various O
inference O
patterns O
( O
e.g. O
, O
reasoning O
path O
length O
and O
types O
) O
for O
comprehensive O
evaluation O
. O

section 0
id pdf2json/2021.acl-long.534.pdf.json
in O
experiments O
, O
we O
curate O
two O
settings O
of O
inferwiki O
varying O
in O
sizes O
and O
structures O
, O
and O
apply O
the O
construction O
process O
on O
codex O
as O
comparative O
datasets O
. O

section 0
id pdf2json/2021.acl-long.534.pdf.json
the O
results O
and O
empirical O
analyses O
demonstrate O
the O
necessity O
and O
high-quality O
of O
inferwiki O
. O

section 0
id pdf2json/2021.acl-long.534.pdf.json
nevertheless O
, O
the O
performance O
gap O
among O
various O
inferential O
assumptions O
and O
patterns O
presents O
the O
difficulty O
and O
inspires O
future O
research O
direction O
. O

section 0
id pdf2json/2021.acl-long.534.pdf.json
our O
datasets O
can O
be O
found O
in O
https O
: O
//github O
. O

section 0
id pdf2json/2021.acl-long.534.pdf.json
com/taominer/inferwiki O
. O

section 1
id pdf2json/2021.acl-long.534.pdf.json
knowledge O
graph O
completion O
( O
kgc O
) O
aims O
to O
predict O
missing O
links O
in O
kg O
by O
inferring O
new O
knowledge O
from O
existing O
ones O
. O

section 1
id pdf2json/2021.acl-long.534.pdf.json
attributed O
to O
its O
reasoning O
ability O
, O
kgc O
models O
are O
crucial O
in O
alleviating O
the O
kg O
’ O
s O
incompleteness O
issue O
and O
benefiting O
many O
downstream O
applications O
, O
such O
as O
recommendation O
( O
cao O
et O
al. O
, O
2019b O
) O
and O
information O
extraction O
( O
hu O
et O
al. O
, O
2021 O
; O
cao O
et O
al. O
, O
2020a O
) O
. O

section 1
id pdf2json/2021.acl-long.534.pdf.json
however O
, O
the O
kgc O
performance O
on O
existing O
benchmarks O
are O
still O
unsatisfactory O
— O
0.51 O
hit O
ratio O
@ O
1 O
and O
187 O
mean O
rank O
of O
the O
top-ranked O
model O
( O
wang O
et O
al. O
, O
2019 O
) O
on O
the O
widely O
used O
fb15k237 O
( O
toutanova O
and O
chen O
, O
2015 O
) O
. O

section 1
id pdf2json/2021.acl-long.534.pdf.json
do O
we O
have O
a O
slow O
progress O
of O
models O
( O
akrami O
et O
al. O
, O
2020 O
) O
? O

section 1
id pdf2json/2021.acl-long.534.pdf.json
or O
should O
we O
blame O
for O
the O
low-quality O
of O
benchmarks O
? O

section 1
id pdf2json/2021.acl-long.534.pdf.json
in O
this O
paper O
, O
we O
re-think O
the O
task O
of O
kgc O
and O
construct O
a O
new O
benchmark O
dubbed O
inferwiki O
that O
highlights O
three O
fundamental O
objectives O
: O
test O
triples O
should O
be O
inferential O
: O
this O
is O
the O
essential O
requirement O
of O
kgc O
. O

section 1
id pdf2json/2021.acl-long.534.pdf.json
each O
test O
triple O
should O
have O
supportive O
samples O
in O
the O
train O
set O
. O

section 1
id pdf2json/2021.acl-long.534.pdf.json
however O
, O
we O
observe O
two O
major O
issues O
of O
current O
kgc O
datasets O
: O
unpredictable O
and O
meaningless O
test O
triples O
, O
which O
may O
hinder O
evaluating O
and O
advancing O
stateof-the-arts O
. O

section 1
id pdf2json/2021.acl-long.534.pdf.json
as O
shown O
in O
table O
1 O
, O
the O
first O
example O
of O
inferring O
the O
location O
for O
david O
( O
i.e. O
, O
florida O
) O
is O
even O
impossible O
for O
humans O
— O
not O
to O
mention O
machines O
— O
merely O
based O
on O
his O
birthplace O
and O
nationality O
( O
i.e. O
, O
atlanta O
and O
usa O
) O
. O

section 1
id pdf2json/2021.acl-long.534.pdf.json
in O
contrast O
, O
the O
second O
one O
is O
predictable O
but O
meaningless O
to O
find O
the O
missing O
month O
from O
a O
list O
of O
months O
within O
a O
year O
. O

section 1
id pdf2json/2021.acl-long.534.pdf.json
the O
above O
cases O
are O
very O
common O
in O
existing O
datasets O
, O
e.g. O
, O
yago3-10 O
( O
dettmers O
et O
al. O
, O
2018 O
) O
and O
codex O
( O
safavi O
and O
koutra O
, O
2020 O
) O
, O
mainly O
due O
to O
their O
construction O
process O
: O
first O
collecting O
a O
highfrequency O
subset O
of O
entities O
and O
then O
randomly O
splitting O
their O
triples O
into O
train/test O
. O

section 1
id pdf2json/2021.acl-long.534.pdf.json
in O
this O
setting O
, O
kgc O
models O
may O
be O
over- O
or O
under-estimated O
, O
as O
we O
are O
even O
unsure O
if O
a O
human O
can O
perform O
better O
. O

section 1
id pdf2json/2021.acl-long.534.pdf.json
test O
triples O
may O
be O
inferred O
positive O
, O
negative O
, O
or O
unknown O
. O

section 1
id pdf2json/2021.acl-long.534.pdf.json
following O
open-world O
assumption O
: O
what O
is O
not O
observed O
in O
kg O
is O
not O
necessar- O
6856 O
ily O
false O
, O
but O
unknown O
( O
shi O
and O
weninger O
, O
2018 O
) O
. O

section 1
id pdf2json/2021.acl-long.534.pdf.json
however O
, O
existing O
benchmarks O
generate O
unseen O
triples O
as O
negatives O
( O
i.e. O
, O
the O
closed-world O
assumption O
) O
, O
because O
kg O
contains O
only O
positive O
triples O
. O

section 1
id pdf2json/2021.acl-long.534.pdf.json
they O
usually O
randomly O
corrupt O
the O
head O
or O
tail O
entity O
in O
a O
triple O
, O
sometimes O
with O
type O
constraints O
( O
li O
et O
al. O
, O
2019a O
) O
. O

section 1
id pdf2json/2021.acl-long.534.pdf.json
this O
leads O
to O
trivial O
evaluation O
( O
almost O
100 O
% O
accuracy O
in O
triple O
classification O
( O
safavi O
and O
koutra O
, O
2020 O
) O
) O
. O

section 1
id pdf2json/2021.acl-long.534.pdf.json
besides O
, O
the O
lack O
of O
unknown O
test O
ignores O
a O
critical O
inference O
capacity O
and O
may O
cause O
false O
negative O
errors O
in O
knowledge-driven O
tasks O
( O
kotnis O
and O
nastase O
, O
2017 O
) O
. O

section 1
id pdf2json/2021.acl-long.534.pdf.json
inference O
has O
various O
patterns O
. O

section 1
id pdf2json/2021.acl-long.534.pdf.json
concentrating O
on O
limited O
patterns O
in O
evaluation O
may O
bring O
in O
severe O
bias O
. O

section 1
id pdf2json/2021.acl-long.534.pdf.json
domain-specific O
datasets O
kinship O
( O
kemp O
et O
al. O
, O
2006 O
) O
and O
country O
( O
bouchard O
et O
al. O
, O
2015 O
) O
only O
focus O
on O
a O
few O
relations O
and O
are O
nearly O
solved O
( O
das O
et O
al. O
, O
2017 O
) O
. O

section 1
id pdf2json/2021.acl-long.534.pdf.json
general-domain O
wn18rr O
( O
dettmers O
et O
al. O
, O
2018 O
) O
contains O
prevalent O
symmetry O
relation O
types O
, O
which O
incorrectly O
boosts O
the O
performance O
of O
rotate O
( O
abboud O
et O
al. O
, O
2020 O
) O
. O

section 1
id pdf2json/2021.acl-long.534.pdf.json
clearly O
, O
limited O
patterns O
leads O
to O
unfair O
comparisons O
among O
kgc O
models O
. O

section 1
id pdf2json/2021.acl-long.534.pdf.json
to O
this O
end O
, O
we O
curated O
an O
inferential O
kgc O
dataset O
extracted O
from O
wikidata O
and O
establish O
the O
benchmark O
with O
two O
settings O
of O
varying O
in O
sizes O
and O
structures O
: O
inferwiki64k O
and O
inferwiki16k O
. O

section 1
id pdf2json/2021.acl-long.534.pdf.json
instead O
of O
random O
split O
, O
we O
mine O
rules O
via O
anyburl O
( O
meilicke O
et O
al. O
, O
2019 O
) O
to O
guide O
train/test O
generation O
. O

section 1
id pdf2json/2021.acl-long.534.pdf.json
all O
test O
triples O
are O
thus O
guaranteed O
inferential O
from O
training O
data O
. O

section 1
id pdf2json/2021.acl-long.534.pdf.json
to O
avoid O
the O
rule O
leakage O
, O
we O
utilize O
two O
sets O
of O
triples O
: O
a O
large O
set O
for O
high-quality O
rule O
extraction O
and O
a O
small O
set O
for O
train/test O
split O
. O

section 1
id pdf2json/2021.acl-long.534.pdf.json
moreover O
, O
we O
infer O
unseen O
triples O
and O
manually O
annotate O
them O
with O
positive O
, O
negative O
and O
unknown O
labels O
to O
improve O
the O
difficulty O
of O
evaluation O
following O
both O
closed-world O
and O
openworld O
assumptions O
. O

section 1
id pdf2json/2021.acl-long.534.pdf.json
for O
inference O
patterns O
, O
we O
include O
and O
balance O
triples O
with O
different O
reasoning O
path O
length O
, O
relation O
types O
and O
patterns O
( O
e.g. O
, O
symmetry O
and O
composition O
) O
. O

section 1
id pdf2json/2021.acl-long.534.pdf.json
our O
contributions O
can O
be O
summarized O
as O
follows O
: O
• O
we O
summarize O
three O
principles O
of O
kgc O
: O
inferential O
ability O
, O
assumptions O
and O
patterns O
, O
and O
construct O
a O
rule-guided O
dataset O
. O

section 1
id pdf2json/2021.acl-long.534.pdf.json
• O
we O
highlight O
the O
importance O
of O
negatives O
and O
unknowns O
, O
and O
initiate O
open-world O
evaluation O
. O

section 1
id pdf2json/2021.acl-long.534.pdf.json
• O
we O
conduct O
extensive O
experiments O
to O
establish O
the O
benchmark O
. O

section 1
id pdf2json/2021.acl-long.534.pdf.json
the O
results O
and O
deep O
analyses O
verify O
the O
necessity O
and O
challenge O
of O
inferwiki O
, O
providing O
insights O
for O
future O
research O
. O

section 2
id pdf2json/2021.acl-long.534.pdf.json
we O
can O
roughly O
classify O
current O
kgc O
datasets O
into O
two O
groups O
: O
inferential O
and O
non-inferential O
datasets O
. O

section 2
id pdf2json/2021.acl-long.534.pdf.json
the O
first O
group O
is O
usually O
manually O
curated O
to O
ensure O
each O
testing O
sample O
can O
be O
inferred O
from O
training O
data O
through O
reasoning O
paths O
, O
while O
they O
only O
focus O
on O
specific O
relations O
, O
such O
as O
families O
( O
garcia-duran O
et O
al. O
, O
2015 O
) O
, O
kinship O
( O
kemp O
et O
al. O
, O
2006 O
) O
, O
and O
country O
( O
bouchard O
et O
al. O
, O
2015 O
) O
. O

section 2
id pdf2json/2021.acl-long.534.pdf.json
the O
limited O
scale O
and O
inference O
patterns O
make O
them O
not O
challenging O
. O

section 2
id pdf2json/2021.acl-long.534.pdf.json
hole O
( O
nickel O
et O
al. O
, O
2016 O
) O
achieves O
99.7 O
% O
acu-pr O
on O
the O
dataset O
of O
country O
. O

section 2
id pdf2json/2021.acl-long.534.pdf.json
the O
second O
group O
of O
datasets O
are O
automatically O
derived O
from O
public O
kgs O
and O
randomly O
split O
positive O
triples O
into O
train/test O
, O
leading O
to O
a O
risk O
of O
testing O
samples O
non-inferential O
from O
training O
data O
. O

section 2
id pdf2json/2021.acl-long.534.pdf.json
popular O
datasets O
include O
fb15k-237 O
( O
toutanova O
and O
chen O
, O
2015 O
) O
, O
wn18rr O
( O
dettmers O
et O
al. O
, O
2018 O
) O
, O
and O
yago3-10 O
( O
dettmers O
et O
al. O
, O
2018 O
) O
. O

section 2
id pdf2json/2021.acl-long.534.pdf.json
codex O
( O
safavi O
and O
koutra O
, O
2020 O
) O
argues O
the O
scope O
and O
difficulty O
of O
the O
above O
datasets O
, O
thus O
propose O
a O
comprehensive O
dataset O
with O
manually O
verified O
hard O
negatives O
. O

section 2
id pdf2json/2021.acl-long.534.pdf.json
in O
fact O
, O
inference O
is O
an O
important O
ability O
for O
intelligence O
. O

section 2
id pdf2json/2021.acl-long.534.pdf.json
various O
fields O
study O
how O
inference O
is O
done O
in O
practice O
, O
ranging O
from O
logic O
to O
cognitive O
psychology O
. O

section 2
id pdf2json/2021.acl-long.534.pdf.json
inference O
helps O
people O
make O
reliable O
predictions O
, O
which O
is O
also O
an O
expected O
ability O
for O
ai O
models O
. O

section 2
id pdf2json/2021.acl-long.534.pdf.json
indeed O
, O
once O
deployed O
, O
a O
model O
may O
have O
6857 O
to O
make O
a O
prediction O
when O
there O
is O
no O
evidence O
in O
the O
training O
set O
. O

section 2
id pdf2json/2021.acl-long.534.pdf.json
but O
, O
instead O
of O
an O
unreliable O
guess O
, O
we O
highlight O
the O
ability O
to O
know O
unknown O
, O
a.k.a O
. O

section 2
id pdf2json/2021.acl-long.534.pdf.json
open-world O
assumption O
. O

section 2
id pdf2json/2021.acl-long.534.pdf.json
therefore O
, O
we O
aim O
to O
curate O
an O
large-scale O
inferential O
benchmark O
inferwiki O
including O
various O
inference O
patterns O
and O
testing O
samples O
( O
i.e. O
, O
positive O
, O
negative O
, O
and O
unknown O
) O
, O
for O
better O
evaluation O
. O

section 2
id pdf2json/2021.acl-long.534.pdf.json
we O
list O
the O
statistics O
in O
table O
2 O
. O

section 3
id pdf2json/2021.acl-long.534.pdf.json
we O
describe O
our O
dataset O
construction O
that O
comprises O
four O
steps O
: O
data O
preprocessing O
, O
rule O
mining O
, O
ruleguided O
train/test O
generation O
, O
and O
inferred O
test O
labeling O
. O

section 3
id pdf2json/2021.acl-long.534.pdf.json
we O
then O
give O
a O
detailed O
analysis O
. O

section 4
id pdf2json/2021.acl-long.534.pdf.json
more O
and O
more O
studies O
utilize O
wikidata1 O
as O
a O
knowledge O
resource O
due O
to O
its O
high O
quality O
and O
large O
quantity O
. O

section 4
id pdf2json/2021.acl-long.534.pdf.json
we O
utilize O
the O
september O
2019 O
english O
dump O
in O
experiments O
. O

section 4
id pdf2json/2021.acl-long.534.pdf.json
data O
preprocessing O
aims O
to O
define O
relation O
vocabulary O
and O
extract O
two O
sets O
of O
triples O
from O
wikidata O
: O
a O
large O
one O
for O
rule O
mining O
t O
r O
and O
a O
relatively O
small O
one O
for O
dataset O
generation O
t O
d. O
the O
reason O
for O
using O
two O
sets O
is O
to O
avoid O
the O
leakage O
of O
rules O
. O

section 4
id pdf2json/2021.acl-long.534.pdf.json
in O
other O
words O
, O
some O
frequent O
rules O
on O
the O
large O
set O
may O
be O
very O
few O
on O
the O
small O
set O
. O

section 4
id pdf2json/2021.acl-long.534.pdf.json
the O
different O
distributions O
shall O
avoid O
that O
rule O
mining O
methods O
will O
easily O
achieve O
high O
performance O
. O

section 4
id pdf2json/2021.acl-long.534.pdf.json
besides O
, O
more O
triples O
can O
improve O
the O
quality O
of O
mined O
rules O
. O

section 4
id pdf2json/2021.acl-long.534.pdf.json
in O
contrast O
, O
the O
relatively O
small O
set O
is O
enough O
for O
efficient O
kgc O
training O
and O
evaluation O
. O

section 4
id pdf2json/2021.acl-long.534.pdf.json
in O
specific O
, O
we O
first O
extract O
all O
triples O
that O
consist O
of O
two O
entity O
items O
and O
one O
relation O
with O
english O
labels O
. O

section 4
id pdf2json/2021.acl-long.534.pdf.json
we O
then O
remove O
the O
repeated O
triples O
and O
obtain O
40,199,175 O
triples O
with O
7,734,841 O
entities O
and O
1,170 O
different O
relation O
types O
. O

section 4
id pdf2json/2021.acl-long.534.pdf.json
considering O
rule O
mining O
efficiency O
, O
we O
reduce O
the O
relation O
vocabulary O
by O
( O
1 O
) O
manually O
filtering O
out O
meaningless O
relations O
, O
such O
as O
movie O
id O
or O
film O
rating O
, O
( O
2 O
) O
removing O
relations O
of O
instanceof O
and O
subclassof O
following O
existing O
benchmarks O
( O
toutanova O
and O
chen O
, O
2015 O
) O
, O
( O
3 O
) O
select O
the O
most O
frequent O
500 O
relation O
types O
. O

section 4
id pdf2json/2021.acl-long.534.pdf.json
we O
focus O
on O
the O
most O
frequent O
800,000 O
entities O
, O
which O
result O
in O
8,632,777 O
triples O
as O
the O
large O
set O
for O
rule O
mining O
. O

section 4
id pdf2json/2021.acl-long.534.pdf.json
to O
obtain O
the O
small O
set O
for O
dataset O
construction O
, O
we O
further O
select O
the O
most O
frequent O
120,000 O
entities O
and O
300 O
relations O
, O
which O
result O
in O
1,283,246 O
triples O
. O

section 4
id pdf2json/2021.acl-long.534.pdf.json
note O
that O
we O
also O
infer O
new O
triples O
and O
label O
them O
as O
positive O
, O
negative O
, O
or O
unknown O
later O
. O

section 4
id pdf2json/2021.acl-long.534.pdf.json
1https O
: O
//www.wikidata.org/ O

section 5
id pdf2json/2021.acl-long.534.pdf.json
since O
developing O
advanced O
rule O
mining O
models O
is O
not O
the O
focus O
of O
this O
paper O
and O
several O
mature O
tools O
are O
available O
online O
, O
such O
as O
amie+ O
( O
galárraga O
et O
al. O
, O
2015 O
) O
and O
anyburl O
( O
meilicke O
et O
al. O
, O
2019 O
) O
. O

section 5
id pdf2json/2021.acl-long.534.pdf.json
we O
utilize O
anyburl2 O
in O
experiments O
due O
to O
its O
efficiency O
and O
effectiveness O
. O

section 5
id pdf2json/2021.acl-long.534.pdf.json
given O
a O
set O
of O
triples O
( O
i.e. O
, O
the O
large O
set O
t O
r O
) O
, O
this O
step O
aims O
to O
automatically O
learn O
rules O
f O
= O
{ O
( O
fp O
, O
λp O
) O
} O
pp=1 O
, O
where O
fp O
denotes O
a O
horn O
rule O
, O
e.g. O
, O
spouse O
( O
x O
, O
y O
) O
∧ O
father O
( O
x O
, O
z O
) O
⇒ O
mother O
( O
y O
, O
z O
) O
, O
and O
λp O
∈ O
[ O
0 O
, O
1 O
] O
denotes O
the O
confidence O
of O
fp O
. O

section 5
id pdf2json/2021.acl-long.534.pdf.json
for O
each O
rule O
fp O
, O
the O
left O
side O
of⇒ O
is O
called O
the O
premise O
, O
and O
the O
right O
side O
is O
called O
the O
conclusion O
, O
where O
the O
conclusion O
contains O
a O
single O
atom O
and O
the O
premise O
is O
a O
conjunction O
of O
several O
atoms O
in O
the O
horn O
rule O
scheme O
. O

section 5
id pdf2json/2021.acl-long.534.pdf.json
we O
can O
ground O
specific O
entities O
to O
replace O
x O
, O
y O
, O
z O
in O
fp O
, O
which O
shall O
denote O
an O
inferential O
relationship O
between O
premise O
and O
conclusion O
triples O
. O

section 5
id pdf2json/2021.acl-long.534.pdf.json
for O
example O
, O
given O
spouse O
( O
lebron O
james O
, O
savannah O
brinson O
) O
and O
father O
( O
lebron O
james O
, O
bronny O
james O
) O
, O
we O
may O
infer O
a O
new O
triple O
mother O
( O
savannah O
brinson O
, O
bronny O
james O
) O
. O

section 5
id pdf2json/2021.acl-long.534.pdf.json
of O
course O
, O
not O
all O
of O
the O
mined O
rules O
are O
reasonable O
. O

section 5
id pdf2json/2021.acl-long.534.pdf.json
to O
alleviate O
the O
negative O
impacts O
of O
unreasonable O
rules O
, O
we O
rely O
on O
more O
data O
( O
a O
large O
set O
of O
triples O
) O
and O
keep O
high-confidence O
rules O
only O
. O

section 5
id pdf2json/2021.acl-long.534.pdf.json
particularly O
, O
we O
follow O
the O
suggested O
configuration O
of O
anyburl O
. O

section 5
id pdf2json/2021.acl-long.534.pdf.json
we O
run O
it O
for O
500 O
seconds O
to O
ensure O
that O
all O
triples O
can O
be O
traversed O
at O
least O
once O
and O
obtain O
251,317 O
rules O
, O
where O
168,996 O
out O
of O
them O
whose O
confidence O
meets O
λp O
> O
0.1 O
have O
been O
selected O
as O
the O
rule O
set O
to O
guide O
dataset O
construction O
. O

section 6
id pdf2json/2021.acl-long.534.pdf.json
different O
from O
existing O
benchmarks O
, O
inferwiki O
provides O
inferential O
testing O
triples O
with O
supportive O
data O
in O
the O
training O
set O
. O

section 6
id pdf2json/2021.acl-long.534.pdf.json
moreover O
, O
it O
aims O
to O
include O
as O
many O
inference O
patterns O
as O
possible O
and O
these O
patterns O
are O
better O
evenly O
distributed O
to O
avoid O
biased O
evaluation O
. O

section 6
id pdf2json/2021.acl-long.534.pdf.json
thus O
, O
this O
step O
has O
four O
objectives O
: O
ruleguided O
split O
, O
path O
extension O
, O
negative O
supplement O
, O
and O
inference O
pattern O
balance O
. O

section 6
id pdf2json/2021.acl-long.534.pdf.json
rule-guided O
split O
grounds O
the O
mined O
rules O
f O
on O
triples O
t O
d O
to O
obtain O
premise O
triples O
and O
corresponding O
conclusion O
triples O
. O

section 6
id pdf2json/2021.acl-long.534.pdf.json
all O
premise O
triples O
form O
a O
training O
set O
, O
and O
all O
conclusion O
triples O
form O
a O
test O
set O
. O

section 6
id pdf2json/2021.acl-long.534.pdf.json
thus O
, O
they O
are O
naturally O
guaranteed O
to O
be O
inferential O
. O

section 6
id pdf2json/2021.acl-long.534.pdf.json
for O
correctness O
, O
all O
of O
premise O
triples O
2http O
: O
//web.informatik.uni-mannheim O
. O

section 6
id pdf2json/2021.acl-long.534.pdf.json
de/anyburl/ O
6858 O
must O
exist O
in O
the O
given O
triple O
set O
t O
d O
, O
while O
conclusion O
triples O
are O
not O
necessarily O
in O
t O
d O
and O
may O
be O
generated O
for O
further O
annotation O
( O
i.e. O
, O
section O
3.4 O
) O
. O

section 6
id pdf2json/2021.acl-long.534.pdf.json
for O
example O
, O
given O
a O
rule O
spouse O
( O
x O
, O
y O
) O
∧ O
father O
( O
x O
, O
z O
) O
⇒ O
mother O
( O
y O
, O
z O
) O
, O
we O
traverse O
all O
of O
the O
given O
triples O
and O
find O
entities O
lebron O
james O
, O
savannah O
brinson O
, O
and O
bronny O
james O
that O
meet O
the O
premise O
. O

section 6
id pdf2json/2021.acl-long.534.pdf.json
we O
then O
add O
the O
premise O
triples O
spouse O
( O
lebron O
james O
, O
savannah O
brinson O
) O
and O
father O
( O
lebron O
james O
, O
bronny O
james O
) O
into O
the O
training O
set O
, O
and O
generate O
the O
conclusion O
triple O
mother O
( O
savannah O
brinson O
, O
bronny O
james O
) O
for O
testing O
, O
no O
matter O
it O
is O
given O
or O
not O
. O

section 6
id pdf2json/2021.acl-long.534.pdf.json
path O
extension O
aims O
to O
increase O
the O
inference O
path O
patterns O
by O
( O
1 O
) O
adding O
more O
reasoning O
paths O
for O
the O
same O
testing O
triple O
, O
and O
( O
2 O
) O
elongating O
paths O
by O
replacing O
those O
premise O
triples O
that O
have O
reasoning O
paths O
. O

section 6
id pdf2json/2021.acl-long.534.pdf.json
for O
example O
, O
we O
replace O
father O
( O
lebron O
james O
, O
bronny O
james O
) O
with O
two O
triples O
that O
can O
infer O
it O
: O
father O
( O
lebron O
james O
, O
bryce O
james O
) O
and O
brother O
( O
bronny O
james O
, O
bryce O
james O
) O
. O

section 6
id pdf2json/2021.acl-long.534.pdf.json
the O
original O
path O
is O
then O
extended O
by O
one O
hop O
. O

section 6
id pdf2json/2021.acl-long.534.pdf.json
correspondingly O
, O
we O
define O
the O
confidence O
of O
extended O
paths O
as O
the O
multiplication O
of O
all O
involved O
rules O
. O

section 6
id pdf2json/2021.acl-long.534.pdf.json
longer O
paths O
will O
challenge O
long-distance O
reasoning O
ability O
. O

section 6
id pdf2json/2021.acl-long.534.pdf.json
negative O
supplement O
is O
to O
generate O
negative O
triples O
if O
we O
can O
not O
annotate O
the O
same O
number O
of O
negatives O
with O
positive O
triples O
. O

section 6
id pdf2json/2021.acl-long.534.pdf.json
otherwise O
, O
we O
will O
face O
an O
imbalance O
issue O
. O

section 6
id pdf2json/2021.acl-long.534.pdf.json
following O
conventions O
, O
we O
randomly O
corrupt O
the O
head O
or O
tail O
entities O
in O
a O
positive O
triple O
with O
the O
following O
constraints O
: O
( O
1 O
) O
the O
relation O
of O
the O
positive O
triple O
is O
exclusive O
, O
e.g. O
, O
placeofbirth O
, O
if O
the O
ratio O
from O
head O
to O
tail O
entities O
is O
smaller O
than O
a O
threshold O
( O
we O
choose O
1.2 O
heuristically O
in O
experiments O
) O
; O
otherwise O
, O
the O
corrupted O
negative O
triple O
may O
be O
actually O
positive O
, O
leading O
to O
false O
negative O
errors O
. O

section 6
id pdf2json/2021.acl-long.534.pdf.json
( O
2 O
) O
we O
choose O
positive O
triples O
from O
the O
test O
set O
for O
corruption O
to O
improve O
the O
difficulty O
— O
the O
model O
has O
to O
correctly O
infer O
the O
corresponding O
positive O
triple O
from O
training O
data O
, O
then O
classify O
the O
corrupted O
triple O
as O
negative O
through O
the O
confliction O
. O

section 6
id pdf2json/2021.acl-long.534.pdf.json
particularly O
, O
for O
non-exclusive O
relation O
types O
, O
most O
of O
their O
corrupted O
results O
should O
be O
unknown O
following O
open-world O
assumption O
. O

section 6
id pdf2json/2021.acl-long.534.pdf.json
the O
inferred O
test O
set O
covers O
such O
cases O
, O
which O
will O
be O
discussed O
in O
section O
3.4 O
. O

section 6
id pdf2json/2021.acl-long.534.pdf.json
inference O
pattern O
balance O
aims O
to O
balance O
various O
inference O
patterns O
, O
including O
path O
length O
, O
relation O
types O
, O
and O
relation O
patterns O
( O
i.e. O
, O
symmetry O
, O
inversion O
, O
hierarchy O
, O
composition O
, O
and O
others O
) O
. O

section 6
id pdf2json/2021.acl-long.534.pdf.json
this O
is O
because O
concentrating O
on O
some O
patterns O
may O
lead O
to O
severe O
bias O
and O
unfair O
comparison O
between O
kgc O
models O
( O
zhang O
et O
al. O
, O
2020 O
) O
. O

section 6
id pdf2json/2021.acl-long.534.pdf.json
we O
first O
count O
the O
frequency O
of O
testing O
triples O
according O
to O
the O
path O
lengths O
, O
relation O
types O
and O
patterns O
, O
respectively O
. O

section 6
id pdf2json/2021.acl-long.534.pdf.json
for O
each O
of O
them O
, O
we O
rank O
their O
counting O
and O
choose O
highest O
ranked O
groups O
of O
triples O
as O
frequent O
ones O
, O
instead O
of O
setting O
a O
threshold O
. O

section 6
id pdf2json/2021.acl-long.534.pdf.json
we O
then O
carefully O
remove O
some O
frequent O
triples O
randomly O
, O
until O
the O
new O
distributions O
reach O
an O
accepted O
range O
( O
checked O
by O
humans O
) O
. O

section 7
id pdf2json/2021.acl-long.534.pdf.json
different O
from O
existing O
datasets O
, O
inferwiki O
aims O
to O
include O
positive O
, O
negative O
, O
and O
unknown O
testing O
triples O
, O
to O
evaluate O
the O
model O
under O
two O
types O
of O
assumptions O
: O
open-world O
assumption O
and O
closedworld O
assumption O
. O

section 7
id pdf2json/2021.acl-long.534.pdf.json
the O
main O
difference O
between O
them O
is O
whether O
unknown O
triples O
are O
regarded O
as O
negatives O
. O

section 7
id pdf2json/2021.acl-long.534.pdf.json
that O
is O
, O
the O
open-world O
evaluation O
is O
a O
three-class O
classification O
problem O
( O
i.e. O
, O
positive O
, O
negative O
, O
and O
unknown O
) O
. O

section 7
id pdf2json/2021.acl-long.534.pdf.json
the O
closed-world O
evaluation O
targets O
only O
positive O
and O
negative O
triples O
, O
and O
we O
can O
simply O
relabel O
unknown O
triples O
as O
negatives O
without O
changing O
the O
test O
set O
. O

section 7
id pdf2json/2021.acl-long.534.pdf.json
so O
far O
, O
we O
have O
two O
test O
sets O
: O
one O
is O
generated O
via O
rule O
guidance O
, O
and O
the O
other O
contains O
the O
supplemented O
negatives O
. O

section 7
id pdf2json/2021.acl-long.534.pdf.json
this O
section O
aims O
to O
label O
the O
generated O
triples O
. O

section 7
id pdf2json/2021.acl-long.534.pdf.json
first O
, O
we O
automatically O
label O
the O
triples O
with O
positive O
if O
they O
exist O
in O
wikidata O
. O

section 7
id pdf2json/2021.acl-long.534.pdf.json
then O
, O
we O
manually O
annotate O
the O
remaining O
4,053 O
triples O
. O

section 7
id pdf2json/2021.acl-long.534.pdf.json
the O
annotation O
guideline O
can O
be O
found O
in O
appendix O
b O
. O

section 7
id pdf2json/2021.acl-long.534.pdf.json
note O
that O
all O
of O
the O
unknowns O
are O
factually O
incorrect O
but O
not O
inferential O
. O

section 7
id pdf2json/2021.acl-long.534.pdf.json
to O
assess O
the O
quality O
of O
annotations O
, O
we O
verify O
a O
random O
selection O
of O
300 O
test O
triples O
( O
100 O
for O
each O
label O
) O
. O

section 7
id pdf2json/2021.acl-long.534.pdf.json
the O
annotators O
agree O
with O
our O
labels O
84.3 O
% O
of O
the O
time O
. O

section 7
id pdf2json/2021.acl-long.534.pdf.json
we O
further O
investigate O
the O
disagreements O
by O
relabeling O
100 O
samples O
. O

section 7
id pdf2json/2021.acl-long.534.pdf.json
85 O
% O
of O
the O
time O
, O
humans O
prefer O
an O
unknown O
, O
while O
automatic O
labeling O
tends O
to O
assign O
them O
with O
positive O
or O
negative O
labels O
. O

section 7
id pdf2json/2021.acl-long.534.pdf.json
this O
suggests O
the O
inferential O
difference O
between O
humans O
and O
machines O
— O
the O
capacity O
of O
knowing O
unknown O
. O

section 7
id pdf2json/2021.acl-long.534.pdf.json
finally O
, O
we O
remove O
the O
entities O
that O
are O
not O
in O
any O
of O
the O
grounded O
paths O
and O
their O
triples O
. O

section 7
id pdf2json/2021.acl-long.534.pdf.json
we O
randomly O
select O
half O
of O
the O
test O
set O
as O
valid O
. O

section 7
id pdf2json/2021.acl-long.534.pdf.json
this O
forms O
inferwiki64k O
. O

section 7
id pdf2json/2021.acl-long.534.pdf.json
we O
further O
extract O
a O
dense O
subset O
inferwiki16k O
by O
filtering O
out O
the O
positive O
triples O
whose O
confidence O
is O
smaller O
than O
0.6 O
. O

section 7
id pdf2json/2021.acl-long.534.pdf.json
correspondingly O
, O
negative/unknown O
triples O
are O
reduced O
to O
keep O
balance O
. O

section 7
id pdf2json/2021.acl-long.534.pdf.json
the O
statistics O
is O
listed O
in O
table O
2 O
. O

section 7
id pdf2json/2021.acl-long.534.pdf.json
6859 O

section 8
id pdf2json/2021.acl-long.534.pdf.json
table O
3 O
shows O
positive O
, O
negative O
, O
and O
unknown O
examples O
of O
inferwiki O
and O
their O
( O
possible O
) O
supportive O
training O
data O
. O

section 8
id pdf2json/2021.acl-long.534.pdf.json
for O
positives O
, O
their O
paths O
seem O
reasonable O
and O
vary O
in O
length O
, O
relation O
types O
, O
and O
patterns O
. O

section 8
id pdf2json/2021.acl-long.534.pdf.json
the O
7-hop O
path O
of O
the O
sibling O
example O
is O
even O
difficult O
for O
a O
human O
. O

section 8
id pdf2json/2021.acl-long.534.pdf.json
for O
negatives O
and O
unknowns O
, O
they O
are O
indeed O
incorrect O
and O
more O
challenging O
. O

section 8
id pdf2json/2021.acl-long.534.pdf.json
there O
are O
no O
direct O
contradicted O
triples O
in O
the O
train O
set O
— O
the O
model O
is O
encouraged O
to O
reason O
related O
triples O
and O
justify O
if O
there O
is O
a O
confliction O
( O
i.e. O
, O
negative O
) O
or O
not O
( O
i.e. O
, O
unknown O
) O
. O

section 8
id pdf2json/2021.acl-long.534.pdf.json
nevertheless O
, O
there O
are O
two O
minor O
issues O
. O

section 8
id pdf2json/2021.acl-long.534.pdf.json
first O
, O
some O
unreasonable O
paths O
may O
corrupt O
the O
predictability O
. O

section 8
id pdf2json/2021.acl-long.534.pdf.json
we O
thus O
increase O
the O
rule O
confidence O
threshold O
λ O
> O
0.6 O
for O
inferwiki16k O
and O
manually O
annotate O
uncertain O
test O
triples O
for O
the O
correctness O
of O
labels O
. O

section 8
id pdf2json/2021.acl-long.534.pdf.json
more O
advanced O
rule O
mining O
models O
can O
improve O
the O
construction O
pipeline O
. O

section 8
id pdf2json/2021.acl-long.534.pdf.json
we O
leave O
it O
in O
the O
future O
. O

section 8
id pdf2json/2021.acl-long.534.pdf.json
second O
, O
does O
unknown O
triples O
have O
a O
bias O
on O
certain O
relation O
types O
? O

section 8
id pdf2json/2021.acl-long.534.pdf.json
the O
answer O
is O
yes O
but O
not O
exactly O
. O

section 8
id pdf2json/2021.acl-long.534.pdf.json
as O
shown O
in O
table O
3 O
, O
the O
relation O
connectswith O
is O
involved O
in O
both O
positive O
and O
unknown O
triples O
, O
which O
is O
also O
determined O
by O
the O
paths O
. O

section 8
id pdf2json/2021.acl-long.534.pdf.json
next O
, O
we O
analyze O
the O
relation O
patterns O
and O
path O
length O
distribution O
through O
comparisons O
with O
existing O
kgc O
datasets O
. O

section 8
id pdf2json/2021.acl-long.534.pdf.json
due O
to O
the O
different O
construction O
pipelines O
, O
existing O
datasets O
are O
difficult O
to O
offer O
quantitative O
statistics O
. O

section 8
id pdf2json/2021.acl-long.534.pdf.json
we O
thus O
apply O
our O
pipeline O
on O
codex O
( O
safavi O
and O
koutra O
, O
2020 O
) O
. O

section 8
id pdf2json/2021.acl-long.534.pdf.json
only O
inferential O
test O
triples O
remain O
, O
and O
the O
training O
set O
keeps O
unchanged O
, O
namely O
codex-m-infer O
, O
which O
reduces O
the O
test O
and O
valid O
positives O
from O
20,622 O
to O
7,050 O
. O

section 8
id pdf2json/2021.acl-long.534.pdf.json
this O
agree O
with O
the O
original O
paper O
that O
reports O
20.56 O
% O
triples O
are O
symmetry O
or O
compositional O
through O
amie+ O
analysis O
. O

section 8
id pdf2json/2021.acl-long.534.pdf.json
we O
find O
more O
paths O
due O
to O
more O
extensive O
rules O
extracted O
from O
a O
large O
set O
of O
triples O
. O

section 8
id pdf2json/2021.acl-long.534.pdf.json
this O
also O
demonstrates O
the O
necessity O
of O
rule-guided O
train/test O
generation O
— O
most O
test O
triples O
are O
not O
guaranteed O
inferential O
when O
using O
random O
split O
. O

section 8
id pdf2json/2021.acl-long.534.pdf.json
relation O
pattern O
following O
convention O
, O
we O
count O
reasoning O
paths O
for O
various O
patterns O
: O
symmetry O
, O
inversion O
, O
hierarchy O
, O
composition O
, O
and O
others O
, O
whose O
detailed O
explanations O
and O
examples O
can O
be O
found O
in O
appendix O
c. O
if O
a O
triple O
has O
multiple O
paths O
, O
we O
count O
all O
of O
them O
. O

section 8
id pdf2json/2021.acl-long.534.pdf.json
as O
figure O
1 O
shows O
, O
we O
can O
see O
that O
( O
1 O
) O
there O
are O
no O
inversion O
and O
only O
a O
few O
symmetry O
and O
hierarchy O
patterns O
in O
codex-m O
, O
as O
most O
current O
datasets O
remove O
them O
to O
avoid O
train/test O
leakage O
. O

section 8
id pdf2json/2021.acl-long.534.pdf.json
but O
, O
we O
argue O
that O
learning O
and O
remembering O
such O
patterns O
are O
also O
an O
essential O
capacity O
of O
inference O
. O

section 8
id pdf2json/2021.acl-long.534.pdf.json
it O
just O
needs O
to O
control O
their O
numbers O
for O
a O
fair O
comparison O
. O

section 8
id pdf2json/2021.acl-long.534.pdf.json
( O
2 O
) O
the O
patterns O
of O
inferwiki O
is O
more O
evenly O
distributed O
. O

section 8
id pdf2json/2021.acl-long.534.pdf.json
note O
that O
the O
patterns O
6860 O
of O
symmetry O
, O
inversion O
, O
and O
hierarchy O
refer O
to O
1- O
hop O
paths O
, O
while O
composition O
and O
others O
refer O
to O
multi-hop O
paths O
. O

section 8
id pdf2json/2021.acl-long.534.pdf.json
so O
, O
the O
total O
number O
of O
the O
former O
three O
is O
almost O
the O
same O
as O
that O
of O
the O
latter O
two O
, O
to O
balance O
paths O
with O
varying O
lengths O
, O
which O
will O
be O
discussed O
next O
. O

section 8
id pdf2json/2021.acl-long.534.pdf.json
path O
length O
distribution O
the O
reasoning O
paths O
can O
ensure O
test O
triples O
’ O
predictability O
but O
may O
not O
be O
the O
shortest O
ones O
, O
as O
there O
may O
be O
undiscovered O
paths O
connecting O
two O
entities O
. O

section 8
id pdf2json/2021.acl-long.534.pdf.json
thus O
, O
our O
statistics O
concerning O
path O
length O
offer O
a O
conservative O
analysis O
and O
give O
an O
upper O
bound O
. O

section 8
id pdf2json/2021.acl-long.534.pdf.json
for O
a O
test O
triple O
with O
multiple O
paths O
, O
we O
count O
the O
shortest O
one O
. O

section 8
id pdf2json/2021.acl-long.534.pdf.json
as O
shown O
in O
figure O
2 O
, O
we O
can O
see O
that O
inferwiki O
has O
more O
long-distance O
paths O
, O
while O
codex-m-infer O
normally O
concentrates O
on O
maximum O
3-hop O
reasoning O
paths O
. O

section 8
id pdf2json/2021.acl-long.534.pdf.json
in O
specific O
, O
the O
maximum O
path O
length O
of O
inferwiki O
is O
9 O
( O
4 O
before O
path O
extension O
) O
and O
the O
average O
length O
is O
2.9 O
( O
1.5 O
before O
path O
extension O
) O
. O

section 8
id pdf2json/2021.acl-long.534.pdf.json
further O
analysis O
of O
relation O
, O
entity O
and O
neighbor O
distributions O
can O
be O
found O
in O
appendix O
d O
& O
e O
. O

section 9
id pdf2json/2021.acl-long.534.pdf.json
although O
we O
carefully O
design O
the O
construction O
of O
inferwiki O
, O
there O
are O
still O
two O
types O
of O
limitations O
: O
rule O
biases O
and O
dataset O
errors O
, O
that O
can O
to O
be O
addressed O
along O
with O
the O
development O
of O
kg O
techniques O
in O
the O
future O
. O

section 9
id pdf2json/2021.acl-long.534.pdf.json
in O
terms O
of O
rule O
biases O
, O
anyburl O
may O
be O
over-estimated O
due O
to O
its O
role O
in O
the O
construction O
. O

section 9
id pdf2json/2021.acl-long.534.pdf.json
although O
we O
utilize O
two O
triple O
sets O
to O
avoid O
rule O
leakage O
, O
their O
overlap O
may O
still O
bring O
unfair O
performance O
gain O
to O
anyburl O
. O

section 9
id pdf2json/2021.acl-long.534.pdf.json
we O
consider O
synthesize O
several O
rule O
mining O
results O
to O
improve O
inferwiki O
in O
the O
next O
version O
. O

section 9
id pdf2json/2021.acl-long.534.pdf.json
in O
terms O
of O
dataset O
errors O
, O
first O
, O
to O
balance O
positive O
and O
negative O
triples O
in O
the O
larger O
inferwiki64k O
, O
we O
follow O
conventions O
to O
randomly O
sample O
a O
portion O
of O
negatives O
. O

section 9
id pdf2json/2021.acl-long.534.pdf.json
these O
negatives O
may O
be O
unknown O
if O
following O
open-world O
assumption O
. O

section 9
id pdf2json/2021.acl-long.534.pdf.json
we O
manually O
assess O
the O
randomly O
sampled O
negatives O
and O
find O
a O
15.7 O
% O
error O
rate O
. O

section 9
id pdf2json/2021.acl-long.534.pdf.json
therefore O
, O
we O
conduct O
open-world O
experiments O
on O
the O
smaller O
inferwiki16k O
, O
all O
of O
whose O
testing O
negatives O
are O
verified O
by O
humans O
. O

section 9
id pdf2json/2021.acl-long.534.pdf.json
the O
second O
type O
of O
errors O
is O
due O
to O
unreasonable O
rules O
for O
dataset O
split O
, O
which O
is O
caused O
by O
prediction O
errors O
of O
existing O
rule O
mining O
models O
. O

section 9
id pdf2json/2021.acl-long.534.pdf.json
however O
, O
there O
is O
no O
suitable O
evaluation O
in O
this O
field O
to O
provide O
quantitative O
analysis O
. O

section 9
id pdf2json/2021.acl-long.534.pdf.json
our O
ongoing O
work O
aims O
to O
develop O
an O
automatic O
evaluation O
for O
path O
rationality O
to O
improve O
the O
mining O
quality O
, O
and O
thus O
facilitate O
our O
inferential O
pipeline O
. O

section 11
id pdf2json/2021.acl-long.534.pdf.json
we O
benchmark O
performance O
on O
inferwiki O
for O
the O
tasks O
: O
( O
1 O
) O
link O
prediction O
, O
the O
task O
of O
predicting O
the O
missing O
head/tail O
entity O
for O
a O
given O
query O
triple O
( O
? O
, O
r O
, O
t O
) O
or O
( O
h O
, O
r O
, O
? O
) O
. O

section 11
id pdf2json/2021.acl-long.534.pdf.json
models O
are O
encouraged O
to O
rank O
correct O
entities O
higher O
than O
others O
in O
the O
vocabulary O
. O

section 11
id pdf2json/2021.acl-long.534.pdf.json
we O
adopt O
the O
filtering O
setting O
( O
bordes O
et O
al. O
, O
2013 O
) O
that O
excludes O
those O
entities O
, O
if O
the O
predicted O
triples O
have O
been O
seen O
in O
the O
train O
set O
. O

section 11
id pdf2json/2021.acl-long.534.pdf.json
mean O
reciprocal O
rank O
( O
mrr O
) O
and O
hits O
@ O
k O
are O
standard O
metrics O
for O
evaluation O
. O

section 11
id pdf2json/2021.acl-long.534.pdf.json
( O
2 O
) O
triple O
classification O
aims O
to O
predict O
a O
label O
for O
each O
given O
triple O
( O
h O
, O
r O
, O
t O
) O
. O

section 11
id pdf2json/2021.acl-long.534.pdf.json
the O
label O
following O
open-world O
assumption O
is O
trinary O
y O
∈ O
{ O
−1 O
, O
0 O
, O
1 O
} O
and O
becomes O
binary O
y O
∈ O
{ O
−1 O
, O
1 O
} O
when O
adopting O
closed-world O
assumption O
— O
all O
0-label O
triples O
are O
re-labeled O
with O
−1 O
, O
since O
our O
unknown O
triples O
are O
factually O
negative O
yet O
non-inferential O
from O
training O
data O
. O

section 11
id pdf2json/2021.acl-long.534.pdf.json
since O
kgc O
models O
output O
real-value O
scores O
for O
triples O
, O
we O
classify O
scores O
into O
labels O
by O
choosing O
one O
or O
two O
thresholds O
per O
relation O
type O
on O
valid O
. O

section 11
id pdf2json/2021.acl-long.534.pdf.json
accuracy O
, O
precision O
, O
recall O
, O
and O
f1 O
are O
measurements O
. O

section 12
id pdf2json/2021.acl-long.534.pdf.json
for O
comprehensive O
comparison O
, O
we O
choose O
three O
types O
of O
representative O
models O
as O
baselines O
: O
( O
1 O
) O
knowledge O
graph O
embedding O
models O
, O
including O
transe O
( O
bordes O
et O
al. O
, O
2013 O
) O
, O
complex O
( O
trouillon O
et O
al. O
, O
2016 O
) O
, O
rotate O
( O
sun O
et O
al. O
, O
2019 O
) O
, O
conve O
( O
dettmers O
et O
al. O
, O
2018 O
) O
, O
and O
tucker O
( O
balazevic O
et O
al. O
, O
2019 O
) O
, O
( O
2 O
) O
multihop O
reasoning O
model O
multihop O
( O
lin O
et O
al. O
, O
2018 O
) O
, O
and O
( O
3 O
) O
rule-based O
anyburl O
( O
meilicke O
et O
al. O
, O
2019 O
) O
. O

section 12
id pdf2json/2021.acl-long.534.pdf.json
note O
that O
the O
latter O
two O
are O
specially O
designed O
for O
link O
prediction O
. O

section 12
id pdf2json/2021.acl-long.534.pdf.json
the O
detailed O
implementation O
including O
parameters O
and O
thresholds O
can O
be O
found O
in O
appendix O
f O
. O

section 13
id pdf2json/2021.acl-long.534.pdf.json
table O
4 O
shows O
micro O
scores O
for O
triple O
classification O
. O

section 13
id pdf2json/2021.acl-long.534.pdf.json
we O
can O
see O
that O
all O
of O
the O
baselines O
perform O
6861 O
well O
— O
around O
90 O
% O
f1 O
scores O
. O

section 13
id pdf2json/2021.acl-long.534.pdf.json
this O
is O
consistent O
with O
recent O
findings O
that O
triple O
classification O
is O
a O
nearly O
solved O
task O
( O
around O
98 O
% O
f1 O
scores O
) O
( O
safavi O
and O
koutra O
, O
2020 O
) O
. O

section 13
id pdf2json/2021.acl-long.534.pdf.json
nevertheless O
, O
the O
lower O
performance O
demonstrates O
the O
difficulty O
of O
our O
curated O
datasets O
, O
mainly O
due O
to O
the O
manually O
annotated O
hard O
negatives O
of O
inferwiki O
( O
and O
codex O
) O
. O

section 13
id pdf2json/2021.acl-long.534.pdf.json
impacts O
of O
hard O
negatives O
figure O
3 O
presents O
the O
accuracy O
on O
inferwiki16k O
regarding O
various O
types O
of O
triples O
: O
positive O
, O
random O
supplemented O
negatives O
, O
and O
annotated O
negatives O
( O
including O
relabeled O
unknowns O
) O
. O

section 13
id pdf2json/2021.acl-long.534.pdf.json
we O
can O
see O
that O
( O
1 O
) O
random O
negative O
triples O
are O
indeed O
trivial O
for O
all O
of O
baseline O
models O
, O
which O
motivates O
the O
necessity O
of O
harder O
negative O
triples O
to O
push O
this O
research O
direction O
forward O
, O
( O
2 O
) O
positive O
triples O
are O
slightly O
difficult O
to O
judge O
than O
random O
negatives O
, O
and O
( O
3 O
) O
the O
accuracy O
significantly O
drops O
on O
annotation O
negatives O
. O

section 13
id pdf2json/2021.acl-long.534.pdf.json
this O
is O
mainly O
because O
most O
annotated O
triples O
are O
actually O
unknown O
— O
they O
are O
factually O
incorrect O
, O
but O
there O
are O
no O
obvious O
abnormal O
patterns O
. O

section 13
id pdf2json/2021.acl-long.534.pdf.json
such O
non-inferential O
cases O
may O
underestimate O
kgc O
models O
. O

section 13
id pdf2json/2021.acl-long.534.pdf.json
open-world O
assumption O
since O
most O
baselines O
fail O
in O
judging O
unknown O
as O
negative O
, O
we O
now O
investigate O
them O
following O
open-world O
assumption O
to O
see O
their O
ability O
in O
recog- O
nizing O
unknown O
triples O
. O

section 13
id pdf2json/2021.acl-long.534.pdf.json
table O
5 O
shows O
the O
macro O
performance3 O
on O
inferwiki16k O
. O

section 13
id pdf2json/2021.acl-long.534.pdf.json
we O
can O
see O
that O
all O
of O
the O
baseline O
models O
perform O
worse O
than O
those O
under O
the O
closed-world O
assumption O
. O

section 13
id pdf2json/2021.acl-long.534.pdf.json
on O
one O
hand O
, O
the O
trinary O
classification O
is O
intuitively O
more O
difficult O
than O
binary O
classification O
. O

section 13
id pdf2json/2021.acl-long.534.pdf.json
on O
the O
other O
hand O
, O
it O
is O
a O
rather O
straightforward O
method O
to O
search O
two O
decision O
thresholds O
— O
one O
between O
positive O
and O
unknown O
and O
the O
other O
between O
unknown O
and O
negative O
. O

section 13
id pdf2json/2021.acl-long.534.pdf.json
this O
motivates O
us O
future O
works O
on O
advanced O
models O
to O
represent O
kg O
, O
which O
should O
also O
be O
able O
to O
detect O
the O
limitation O
and O
boundaries O
of O
given O
kg O
. O

section 13
id pdf2json/2021.acl-long.534.pdf.json
it O
is O
a O
fundamental O
capacity O
of O
inference O
to O
respond O
“ O
i O
do O
not O
know O
” O
, O
to O
avoid O
false O
negatives O
in O
downstream O
applications O
. O

section 13
id pdf2json/2021.acl-long.534.pdf.json
figure O
4 O
presents O
a O
detailed O
analysis O
of O
each O
model O
regarding O
their O
search O
thresholds O
. O

section 13
id pdf2json/2021.acl-long.534.pdf.json
we O
can O
see O
that O
although O
their O
best O
performance O
seems O
not O
bad O
, O
the O
worst O
scores O
are O
only O
around O
10 O
% O
. O

section 13
id pdf2json/2021.acl-long.534.pdf.json
that O
is O
, O
they O
are O
very O
sensitive O
to O
thresholds O
. O

section 13
id pdf2json/2021.acl-long.534.pdf.json
besides O
, O
most O
of O
the O
time O
, O
the O
average O
f1 O
scores O
of O
complex O
, O
rotate O
, O
and O
tucker O
are O
around O
20 O
% O
, O
while O
transe O
achieves O
higher O
scores O
. O

section 13
id pdf2json/2021.acl-long.534.pdf.json
maybe O
that O
is O
the O
reason O
why O
it O
is O
still O
the O
most O
widely O
used O
kgc O
method O
. O

section 13
id pdf2json/2021.acl-long.534.pdf.json
conve O
stably O
outperforms O
other O
baselines O
, O
no O
matter O
in O
terms O
of O
best O
, O
worst O
, O
or O
average O
performance O
. O

section 14
id pdf2json/2021.acl-long.534.pdf.json
table O
6 O
shows O
the O
average O
scores O
for O
head O
and O
tail O
prediction O
. O

section 14
id pdf2json/2021.acl-long.534.pdf.json
we O
can O
see O
that O
( O
1 O
) O
anyburl O
performs O
the O
best O
most O
of O
the O
time O
, O
but O
the O
performance O
gap O
is O
not O
significant O
. O

section 14
id pdf2json/2021.acl-long.534.pdf.json
this O
is O
mainly O
due O
to O
its O
role O
in O
3micro O
performance O
is O
only O
applicable O
to O
binary O
classification O
, O
while O
open-world O
evaluation O
is O
trinary O
. O

section 14
id pdf2json/2021.acl-long.534.pdf.json
6862 O
dataset O
construction O
, O
although O
we O
utilize O
two O
sets O
of O
triples O
to O
minimize O
rule O
leakage O
. O

section 14
id pdf2json/2021.acl-long.534.pdf.json
actually O
, O
inference O
of O
rules O
may O
be O
more O
important O
than O
we O
thought O
to O
improve O
the O
reliability O
and O
interpretability O
of O
knowledge-driven O
models O
. O

section 14
id pdf2json/2021.acl-long.534.pdf.json
this O
also O
motivates O
us O
to O
incorporate O
rule O
knowledge O
into O
kgc O
training O
for O
advanced O
reasoning O
ability O
( O
guo O
et O
al. O
, O
2018 O
; O
li O
et O
al. O
, O
2019b O
) O
. O

section 14
id pdf2json/2021.acl-long.534.pdf.json
( O
2 O
) O
kgc O
models O
perform O
better O
on O
inferwiki16k O
than O
inferwiki64k O
, O
due O
to O
the O
higher O
structure O
density O
and O
rule O
confidence O
. O

section 14
id pdf2json/2021.acl-long.534.pdf.json
( O
3 O
) O
models O
have O
higher O
hit O
@ O
10 O
and O
lower O
hit O
@ O
1 O
on O
inferwiki O
than O
other O
datasets O
( O
e.g. O
, O
codex O
) O
. O

section 14
id pdf2json/2021.acl-long.534.pdf.json
this O
agrees O
with O
an O
intuition O
that O
most O
entities O
are O
irrelevant O
, O
making O
it O
trivial O
to O
judge O
these O
corrupted O
triples O
as O
in O
triple O
classification O
. O

section 14
id pdf2json/2021.acl-long.534.pdf.json
and O
, O
only O
a O
small O
portion O
of O
entities O
is O
difficult O
to O
predict O
, O
which O
requires O
strong O
inference O
ability O
. O

section 14
id pdf2json/2021.acl-long.534.pdf.json
besides O
, O
hit O
@ O
1 O
varies O
a O
lot O
, O
so O
that O
we O
can O
better O
compare O
among O
models O
. O

section 14
id pdf2json/2021.acl-long.534.pdf.json
impacts O
of O
inferential O
path O
length O
figure O
5 O
presents O
hit O
@ O
1 O
curves O
for O
tail O
prediction O
regarding O
varying O
path O
length O
on O
inferwiki64k4 O
. O

section 14
id pdf2json/2021.acl-long.534.pdf.json
we O
can O
see O
an O
overall O
downwards O
trend O
along O
with O
the O
increasing O
path O
length O
. O

section 14
id pdf2json/2021.acl-long.534.pdf.json
meanwhile O
, O
the O
large O
fluctuation O
may O
be O
due O
to O
two O
possible O
reasons O
: O
( O
1 O
) O
as O
discussed O
in O
section O
3.5 O
, O
the O
inferential O
paths O
ensure O
the O
predictability O
, O
but O
may O
not O
be O
the O
shortest O
ones O
. O

section 14
id pdf2json/2021.acl-long.534.pdf.json
this O
thus O
offers O
a O
conservative O
4multihop O
is O
designed O
for O
tail O
prediction O
, O
and O
hit O
@ O
1 O
on O
inferwiki64k O
is O
more O
distinct O
for O
following O
ablation O
study O
. O

section 14
id pdf2json/2021.acl-long.534.pdf.json
analysis O
and O
give O
an O
upper O
bound O
of O
the O
performance O
concerning O
k-hop O
paths O
. O

section 14
id pdf2json/2021.acl-long.534.pdf.json
our O
paths O
are O
of O
high O
coverage O
and O
quality O
compared O
with O
existing O
datasets O
, O
which O
either O
conduct O
case O
study O
or O
postprocess O
datasets O
via O
rule O
mining O
. O

section 14
id pdf2json/2021.acl-long.534.pdf.json
( O
2 O
) O
relation O
types O
and O
patterns O
also O
have O
significant O
impacts O
. O

section 14
id pdf2json/2021.acl-long.534.pdf.json
shorter O
paths O
contain O
more O
long-tail O
relations O
, O
and O
longer O
paths O
tend O
to O
cover O
many O
common O
relations O
. O

section 14
id pdf2json/2021.acl-long.534.pdf.json
this O
improves O
the O
difficulty O
of O
shorter O
paths O
and O
makes O
longer O
paths O
easier O
. O

section 14
id pdf2json/2021.acl-long.534.pdf.json
impacts O
of O
relation O
patterns O
we O
present O
the O
hit O
@ O
1 O
tail O
prediction O
on O
inferwiki64k O
regarding O
relation O
patterns O
in O
table O
7 O
. O

section 14
id pdf2json/2021.acl-long.534.pdf.json
we O
can O
see O
that O
symmetry O
and O
inversion O
are O
not O
wellsolved O
, O
which O
should O
be O
considered O
into O
evaluation O
but O
limited O
in O
scale O
. O

section 14
id pdf2json/2021.acl-long.534.pdf.json
transe O
performs O
worse O
on O
symmetry O
and O
inversion O
relations O
, O
consistent O
with O
the O
analysis O
in O
abboud O
et O
al O
. O

section 14
id pdf2json/2021.acl-long.534.pdf.json
( O
2020 O
) O
. O

section 14
id pdf2json/2021.acl-long.534.pdf.json
even O
if O
complex O
and O
rotate O
can O
capture O
such O
patterns O
, O
they O
fail O
to O
rank O
corresponding O
entities O
at O
the O
top O
. O

section 14
id pdf2json/2021.acl-long.534.pdf.json
embedding-based O
models O
perform O
well O
on O
hierarchy O
relations O
, O
even O
outperforms O
anyburl O
. O

section 14
id pdf2json/2021.acl-long.534.pdf.json
for O
compositional O
relations O
, O
it O
is O
still O
quite O
challenging O
and O
worthwhile O
further O
investigation O
. O

section 15
id pdf2json/2021.acl-long.534.pdf.json
we O
investigate O
the O
impacts O
of O
rule-based O
train/test O
generatation O
by O
comparing O
codex-m-infer O
with O
6863 O
codex-m O
. O

section 15
id pdf2json/2021.acl-long.534.pdf.json
the O
two O
datasets O
share O
the O
same O
training O
set O
. O

section 15
id pdf2json/2021.acl-long.534.pdf.json
the O
only O
difference O
lies O
in O
how O
we O
obtain O
the O
test O
triples O
, O
either O
using O
our O
proposed O
pipeline O
( O
codex-m-infer O
) O
or O
randomly O
( O
codex-m O
) O
. O

section 15
id pdf2json/2021.acl-long.534.pdf.json
thus O
, O
the O
results O
reflect O
the O
impacts O
of O
inferential O
guarantee O
for O
dataset O
construction O
and O
demonstrate O
the O
necessity O
to O
avoid O
over-estimation O
or O
underestimation O
of O
the O
inferential O
ability O
of O
kgc O
models O
. O

section 15
id pdf2json/2021.acl-long.534.pdf.json
we O
report O
the O
performance O
on O
codex-m O
from O
the O
original O
paper O
( O
safavi O
and O
koutra O
, O
2020 O
) O
. O

section 15
id pdf2json/2021.acl-long.534.pdf.json
we O
can O
see O
that O
all O
of O
models O
perform O
better O
with O
inferential O
path O
guarantee O
on O
codex-m-infer O
than O
codex-m O
, O
except O
complex O
for O
link O
prediction O
. O

section 15
id pdf2json/2021.acl-long.534.pdf.json
this O
is O
because O
rule O
guidance O
elimites O
those O
noninferential O
testing O
triples O
, O
making O
the O
task O
easier O
. O

section 15
id pdf2json/2021.acl-long.534.pdf.json
nevertheless O
, O
the O
scores O
on O
hard O
cases O
are O
actually O
decreased O
( O
as O
discussed O
in O
figure O
3 O
and O
table O
7 O
) O
. O

section 15
id pdf2json/2021.acl-long.534.pdf.json
models O
are O
excepted O
a O
stronger O
reasoning O
ability O
among O
several O
related O
entities O
, O
instead O
of O
trivially O
filtering O
out O
massive O
irrelevant O
entities O
. O

section 15
id pdf2json/2021.acl-long.534.pdf.json
this O
also O
demonstrates O
the O
necessity O
of O
inferwiki O
to O
avoid O
over- O
or O
under- O
estimation O
of O
the O
inferential O
ability O
of O
kgc O
models O
— O
learning O
new O
knowledge O
from O
existing O
ones O
. O

section 16
id pdf2json/2021.acl-long.534.pdf.json
we O
illustrate O
the O
most O
frequent O
relation O
types O
and O
their O
distribution O
of O
inferwiki64k O
and O
inferwiki16k O
in O
figure O
8 O
. O

section 16
id pdf2json/2021.acl-long.534.pdf.json
we O
can O
see O
that O
inferwiki O
has O
a O
diverse O
relation O
types O
that O
are O
not O
limited O
to O
specific O
domains O
. O

section 16
id pdf2json/2021.acl-long.534.pdf.json
besides O
, O
the O
triples O
of O
each O
relation O
type O
are O
well O
balanced O
. O

section 17
id pdf2json/2021.acl-long.534.pdf.json
we O
highlighted O
three O
principles O
for O
kgc O
datasets O
: O
inferential O
ability O
, O
assumptions O
, O
and O
patterns O
, O
and O
contribute O
a O
large-scale O
dataset O
inferwiki O
. O

section 17
id pdf2json/2021.acl-long.534.pdf.json
we O
established O
a O
benchmark O
with O
three O
types O
of O
seven O
kgc O
models O
on O
two O
tasks O
of O
triple O
classification O
and O
link O
prediction O
. O

section 17
id pdf2json/2021.acl-long.534.pdf.json
the O
results O
present O
a O
detailed O
analysis O
regarding O
various O
inference O
patterns O
, O
which O
demonstrates O
the O
necessity O
of O
an O
inferential O
guarantee O
for O
better O
evaluation O
and O
the O
difficulty O
of O
new O
open-world O
triple O
classification O
. O

section 17
id pdf2json/2021.acl-long.534.pdf.json
in O
the O
future O
, O
we O
are O
interested O
in O
cross-kgs O
inference O
and O
transfer O
( O
cao O
et O
al. O
, O
2019a O
) O
, O
and O
investigating O
how O
to O
inject O
knowledge O
into O
deep O
learning O
architectures O
, O
such O
as O
for O
information O
extraction O
( O
tong O
et O
al. O
, O
2020 O
) O
or O
text O
generation O
( O
cao O
et O
al. O
, O
2020b O
) O
. O

section 18
id pdf2json/2021.acl-long.534.pdf.json
this O
research O
was O
conducted O
in O
collaboration O
with O
sensetime O
. O

section 18
id pdf2json/2021.acl-long.534.pdf.json
this O
work O
is O
partially O
supported O
by O
a*star O
through O
the O
industry O
alignment O
fund O
- O
industry O
collaboration O
projects O
grant O
, O
by O
ntu O
( O
ntu–ace2020-01 O
) O
and O
ministry O
of O
education O
( O
rg96/20 O
) O
, O
and O
by O
the O
national O
research O
foundation O
, O
prime O
minister O
’ O
s O
office O
, O
singapore O
under O
its O
energy O
programme O
( O
ep O
award O
no O
. O

section 18
id pdf2json/2021.acl-long.534.pdf.json
nrf2017ewtep003-023 O
) O
administrated O
by O
the O
energy O
market O
authority O
of O
singapore O
. O

section 18
id pdf2json/2021.acl-long.534.pdf.json
this O
work O
is O
partially O
supported O
by O
singapore O
moe O
acrf O
t1 O
. O

section 18
id pdf2json/2021.acl-long.534.pdf.json
6864 O

section 19
id pdf2json/2021.acl-long.534.pdf.json
table O
8 O
lists O
existing O
kgc O
datasets O
. O

section 19
id pdf2json/2021.acl-long.534.pdf.json
we O
can O
roughly O
classify O
them O
into O
two O
groups O
: O
inferential O
and O
noninferential O
datasets O
. O

section 19
id pdf2json/2021.acl-long.534.pdf.json
the O
first O
group O
are O
usually O
manually O
curated O
to O
ensure O
each O
testing O
sample O
can O
be O
inferred O
from O
training O
data O
through O
reasoning O
paths O
. O

section 19
id pdf2json/2021.acl-long.534.pdf.json
families O
( O
garcia-duran O
et O
al. O
, O
2015 O
) O
test O
family O
relationships O
including O
cousin O
, O
ancestor O
, O
marriage O
, O
parent O
, O
sibling O
, O
and O
uncle O
, O
among O
the O
members O
of O
5 O
families O
along O
6 O
generations O
. O

section 19
id pdf2json/2021.acl-long.534.pdf.json
such O
that O
there O
are O
obvious O
compositional O
relationships O
like O
uncle O
≈ O
sibling O
+ O
parent O
or O
parent O
≈ O
married O
+ O
parent O
. O

section 19
id pdf2json/2021.acl-long.534.pdf.json
kinship O
( O
kemp O
et O
al. O
, O
2006 O
) O
contains O
kinship O
relationships O
among O
members O
of O
the O
alyawarra O
tribe O
from O
central O
australia O
, O
while O
country O
( O
bouchard O
et O
al. O
, O
2015 O
) O
contains O
countries O
, O
regions O
, O
and O
subregions O
as O
entities O
and O
is O
carefully O
designed O
to O
explicitly O
test O
the O
location O
relationship O
( O
i.e. O
, O
locatedin O
and O
neighbor O
) O
among O
them O
. O

section 19
id pdf2json/2021.acl-long.534.pdf.json
the O
above O
datasets O
are O
clearly O
limited O
in O
scale O
and O
inference O
patterns O
, O
thus O
become O
not O
challenging O
. O

section 19
id pdf2json/2021.acl-long.534.pdf.json
hole O
( O
nickel O
et O
al. O
, O
2016 O
) O
even O
achieves O
99.7 O
% O
acu-pr O
on O
dataset O
country O
( O
bouchard O
et O
al. O
, O
2015 O
) O
. O

section 19
id pdf2json/2021.acl-long.534.pdf.json
the O
second O
group O
of O
datasets O
are O
automatically O
derived O
from O
public O
kgs O
and O
randomly O
split O
positive O
triples O
into O
train/valid/test O
, O
leading O
to O
a O
risk O
of O
testing O
samples O
non-inferential O
from O
training O
data O
. O

section 19
id pdf2json/2021.acl-long.534.pdf.json
fb13 O
( O
socher O
et O
al. O
, O
2013 O
) O
and O
fb15k O
( O
bordes O
et O
al. O
, O
2013 O
) O
are O
commonly O
used O
benchmark O
from O
freebase O
. O

section 19
id pdf2json/2021.acl-long.534.pdf.json
fb15k401 O
( O
yang O
et O
al. O
, O
2014 O
) O
is O
a O
subset O
of O
fb15k O
containing O
only O
frequent O
relations O
( O
relations O
with O
at O
least O
100 O
training O
examples O
) O
. O

section 19
id pdf2json/2021.acl-long.534.pdf.json
to O
remove O
test O
leakage O
, O
fb15k-237 O
( O
toutanova O
and O
chen O
, O
2015 O
) O
removes O
all O
equivalent O
or O
inverse O
relations O
. O

section 19
id pdf2json/2021.acl-long.534.pdf.json
similarly O
, O
fb5m O
( O
wang O
et O
al. O
, O
2014 O
) O
removes O
all O
the O
entity O
pairs O
that O
appear O
in O
the O
testing O
set O
. O

section 19
id pdf2json/2021.acl-long.534.pdf.json
wn18rr O
( O
dettmers O
et O
al. O
, O
2018 O
) O
is O
the O
challenging O
version O
of O
wn18 O
( O
bordes O
et O
al. O
, O
2013 O
) O
extracted O
from O
wordnet O
. O

section 19
id pdf2json/2021.acl-long.534.pdf.json
textual O
information O
is O
also O
included O
for O
specific O
task O
, O
such O
as O
fb40k O
( O
lin O
et O
al. O
, O
2015 O
) O
targeting O
relation O
extraction O
dataset O
new O
york O
times O
( O
riedel O
et O
al. O
, O
2010 O
) O
. O

section 19
id pdf2json/2021.acl-long.534.pdf.json
fb24k O
( O
lin O
et O
al. O
, O
2016 O
) O
introduce O
attributes O
. O

section 19
id pdf2json/2021.acl-long.534.pdf.json
fb15k+ O
( O
xie O
et O
al. O
, O
2016 O
) O
introduce O
types O
and O
make O
fb15k O
more O
sparse O
by O
only O
filterring O
out O
relation O
with O
a O
frequency O
lower O
than O
one O
. O

section 19
id pdf2json/2021.acl-long.534.pdf.json
another O
popular O
knowledge O
source O
is O
yago O
, O
and O
the O
corresponding O
datasets O
include O
yago3-10 O
( O
dettmers O
et O
al. O
, O
2018 O
) O
and O
yago37 O
( O
guo O
et O
al. O
, O
2018 O
) O
. O

section 19
id pdf2json/2021.acl-long.534.pdf.json
except O
for O
open-domain O
kg O
, O
nell O
( O
wang O
et O
al. O
, O
2015 O
) O
concentrates O
on O
location O
and O
sports O
, O
and O
umls O
( O
kok O
and O
domingos O
, O
2007 O
) O
targets O
medical O
knowledge O
. O

section 19
id pdf2json/2021.acl-long.534.pdf.json
codex O
( O
safavi O
and O
koutra O
, O
2020 O
) O
argues O
the O
quality O
of O
the O
above O
benchmarks O
, O
such O
as O
nell995 O
( O
xiong O
et O
al. O
, O
2017 O
) O
are O
nonsensical O
or O
overly O
generic O
. O

section 19
id pdf2json/2021.acl-long.534.pdf.json
thus O
they O
propose O
a O
comprehensive O
dataset O
consisting O
of O
three O
knowledge O
graphs O
varying O
in O
size O
and O
structure O
, O
entity O
types O
, O
multilingual O
labels O
and O
descriptions O
, O
and O
hard O
negatives O
. O

section 20
id pdf2json/2021.acl-long.534.pdf.json
we O
provide O
the O
following O
annotation O
guidelines O
for O
annotators O
to O
label O
inferred O
triples O
in O
section O
3.4 O
. O

section 20
id pdf2json/2021.acl-long.534.pdf.json
task O
this O
is O
a O
two-step O
annotations O
. O

section 20
id pdf2json/2021.acl-long.534.pdf.json
first O
, O
you O
must O
annotate O
each O
triple O
with O
the O
label O
y O
∈ O
{ O
1 O
, O
−1 O
} O
, O
where O
1 O
denotes O
that O
the O
triple O
is O
correct O
and O
−1 O
denotes O
that O
the O
triple O
is O
incorrect O
. O

section 20
id pdf2json/2021.acl-long.534.pdf.json
you O
can O
find O
the O
answer O
from O
anywhere O
you O
want O
, O
such O
as O
commonsense O
, O
wikipedia O
, O
and O
professional O
websites O
. O

section 20
id pdf2json/2021.acl-long.534.pdf.json
if O
you O
can O
not O
find O
any O
evidence O
to O
support O
the O
statement O
, O
you O
shall O
choose O
label O
−1 O
. O

section 20
id pdf2json/2021.acl-long.534.pdf.json
second O
, O
you O
must O
annotate O
each O
incorrect O
triple O
with O
the O
label O
ŷ O
∈ O
{ O
0 O
, O
−1 O
} O
, O
where O
0 O
denotes O
that O
you O
do O
not O
know O
the O
answer O
. O

section 20
id pdf2json/2021.acl-long.534.pdf.json
now O
, O
you O
can O
find O
the O
answer O
from O
our O
provided O
triples O
. O

section 20
id pdf2json/2021.acl-long.534.pdf.json
if O
you O
can O
not O
find O
any O
evidence O
to O
support O
the O
statement O
, O
you O
shall O
choose O
label O
0 O
. O

section 20
id pdf2json/2021.acl-long.534.pdf.json
examples O
here O
are O
some O
examples O
judged O
using O
three O
types O
of O
knowledge O
sources O
. O

section 20
id pdf2json/2021.acl-long.534.pdf.json
• O
commonsense O
: O
( O
cypriot O
fourth O
division O
, O
haspart O
, O
2018–19 O
cypriot O
third O
division O
) O
is O
clearly O
incorrect O
, O
since O
the O
fourth O
division O
can O
not O
has O
a O
part O
of O
third O
division O
. O

section 20
id pdf2json/2021.acl-long.534.pdf.json
• O
professional O
websites O
: O
to O
annotate O
the O
triple O
( O
bahrain-merida O
2019 O
, O
haspart O
, O
carlos O
betancur O
) O
, O
you O
may O
search O
the O
person O
in O
professional O
websites O
, O
such O
as O
https O
: O
//www.procyclingstats.com/ O
team/bahrain-merida-2019 O
. O

section 20
id pdf2json/2021.acl-long.534.pdf.json
since O
there O
is O
no O
carlos O
betancur O
listed O
in O
that O
website O
, O
please O
choose O
false O
. O

section 20
id pdf2json/2021.acl-long.534.pdf.json
• O
wikipedia O
: O
given O
the O
triples O
( O
tōkaidō O
shinkansen O
, O
connectswith O
, O
osaka O
higashi O
line O
) O
and O
( O
tōkaidō O
shinkansen O
, O
connectswith O
, O
san O
’ O
yō O
main O
line O
) O
, O
you O
can O
find O
related O
station O
information O
from O
the O
page O
of O
tōkaidō O
shinkansen O
. O

section 20
id pdf2json/2021.acl-long.534.pdf.json
you O
can O
find O
that O
osaka O
higashi O
line O
shares O
a O
transfer O
station O
with O
tōkaidō O
shinkansen O
, O
thus O
label O
it O
with O
1 O
. O

section 20
id pdf2json/2021.acl-long.534.pdf.json
and O
, O
san O
’ O
yō O
main O
line O
doesn O
’ O
t O
show O
up O
in O
the O
page O
, O
you O
may O
label O
it O
with O
−1 O
. O

section 21
id pdf2json/2021.acl-long.534.pdf.json
inferwiki O
is O
able O
to O
analyze O
relation O
patterns O
for O
each O
path O
, O
including O
symmetry O
, O
inversion O
, O
hierarchy O
, O
and O
composition O
, O
where O
detailed O
explanations O
and O
examples O
are O
listed O
in O
table O
9 O
. O

section 22
id pdf2json/2021.acl-long.534.pdf.json
we O
illustrate O
the O
most O
frequent O
relation O
types O
and O
their O
distribution O
of O
inferwiki64k O
and O
inferwiki16k O
in O
figure O
8 O
. O

section 23
id pdf2json/2021.acl-long.534.pdf.json
figure O
9 O
shows O
the O
distribution O
of O
entities O
and O
their O
neighbors O
as O
compared O
to O
widely O
used O
datasets O
: O
fb15k237 O
and O
codex-m O
. O

section 24
id pdf2json/2021.acl-long.534.pdf.json
our O
experiments O
are O
run O
on O
the O
server O
with O
the O
following O
configurations O
: O
os O
of O
ubuntu O
16.04.6 O
lts O
, O
cpu O
of O
intel O
( O
r O
) O
xeon O
( O
r O
) O
cpu O
e5-2680 O
v4 O
@ O
2.40ghz O
, O
and O
gpu O
of O
geforce O
rtx O
2080 O
ti O
. O

section 24
id pdf2json/2021.acl-long.534.pdf.json
we O
use O
openke5 O
for O
re-implementing O
transe O
, O
complex O
, O
and O
rotate O
. O

section 24
id pdf2json/2021.acl-long.534.pdf.json
for O
the O
rest O
models O
, O
we O
use O
the O
original O
codes O
for O
conve6 O
, O
tucker O
7 O
, O
multi- O
5https O
: O
//github.com/thunlp/openke O
6https O
: O
//github.com/timdettmers/conve O
7https O
: O
//github.com/ibalazevic/tucker O
hop8 O
, O
and O
anyburl9 O
. O

section 24
id pdf2json/2021.acl-long.534.pdf.json
because O
we O
utilize O
various O
types O
of O
kgc O
models O
including O
embedding-based O
, O
multi-hop O
reasoning O
( O
reinforcement O
learning O
) O
, O
and O
rule-based O
models O
, O
these O
models O
largely O
have O
their O
own O
hyperparameters O
. O

section 24
id pdf2json/2021.acl-long.534.pdf.json
to O
avoid O
exhaustive O
parameter O
search O
in O
a O
large O
range O
, O
we O
conduct O
a O
series O
of O
preliminary O
experiments O
and O
find O
that O
the O
suggested O
parameters O
work O
well O
on O
wikidata-based O
data O
. O

section 24
id pdf2json/2021.acl-long.534.pdf.json
we O
then O
search O
the O
embedding O
size O
in O
the O
range O
of O
{ O
256 O
, O
512 O
} O
, O
number O
of O
negative O
samples O
in O
the O
range O
of O
{ O
15 O
, O
25 O
} O
and O
margin O
in O
the O
range O
of O
{ O
4 O
, O
8 O
} O
. O

section 24
id pdf2json/2021.acl-long.534.pdf.json
the O
optimal O
parameters O
of O
each O
model O
on O
all O
of O
three O
datasets O
are O
listed O
in O
table O
10 O
. O

section 24
id pdf2json/2021.acl-long.534.pdf.json
the O
thresholds O
in O
triples O
classification O
are O
listed O
in O
table O
11 O
8https O
: O
//github.com/salesforce/ O
multihopkg O
9http O
: O
//web.informatik.uni-mannheim O
. O

section 24
id pdf2json/2021.acl-long.534.pdf.json
de/anyburl/ O

section TITLE
id pdf2json/2021.acl-long.449.pdf.json
adapting O
unsupervised O
syntactic O
parsing O
methodology O
for O
discourse O
dependency O
parsing O

section ABSTRACT
id pdf2json/2021.acl-long.449.pdf.json
one O
of O
the O
main O
bottlenecks O
in O
developing O
discourse O
dependency O
parsers O
is O
the O
lack O
of O
annotated O
training O
data O
. O

section ABSTRACT
id pdf2json/2021.acl-long.449.pdf.json
a O
potential O
solution O
is O
to O
utilize O
abundant O
unlabeled O
data O
by O
using O
unsupervised O
techniques O
, O
but O
there O
is O
so O
far O
little O
research O
in O
unsupervised O
discourse O
dependency O
parsing O
. O

section ABSTRACT
id pdf2json/2021.acl-long.449.pdf.json
fortunately O
, O
unsupervised O
syntactic O
dependency O
parsing O
has O
been O
studied O
for O
decades O
, O
which O
could O
potentially O
be O
adapted O
for O
discourse O
parsing O
. O

section ABSTRACT
id pdf2json/2021.acl-long.449.pdf.json
in O
this O
paper O
, O
we O
propose O
a O
simple O
yet O
effective O
method O
to O
adapt O
unsupervised O
syntactic O
dependency O
parsing O
methodology O
for O
unsupervised O
discourse O
dependency O
parsing O
. O

section ABSTRACT
id pdf2json/2021.acl-long.449.pdf.json
we O
apply O
the O
method O
to O
adapt O
two O
state-of-the-art O
unsupervised O
syntactic O
dependency O
parsing O
methods O
. O

section ABSTRACT
id pdf2json/2021.acl-long.449.pdf.json
experimental O
results O
demonstrate O
that O
our O
adaptation O
is O
effective O
. O

section ABSTRACT
id pdf2json/2021.acl-long.449.pdf.json
moreover O
, O
we O
extend O
the O
adapted O
methods O
to O
the O
semi-supervised O
and O
supervised O
setting O
and O
surprisingly O
, O
we O
find O
that O
they O
outperform O
previous O
methods O
specially O
designed O
for O
supervised O
discourse O
parsing O
. O

section ABSTRACT
id pdf2json/2021.acl-long.449.pdf.json
further O
analysis O
shows O
our O
adaptations O
result O
in O
superiority O
not O
only O
in O
parsing O
accuracy O
but O
also O
in O
time O
and O
space O
efficiency O
. O

section 0
id pdf2json/2021.acl-long.449.pdf.json
proceedings O
of O
the O
59th O
annual O
meeting O
of O
the O
association O
for O
computational O
linguistics O
and O
the O
11th O
international O
joint O
conference O
on O
natural O
language O
processing O
, O
pages O
5782–5794 O
august O
1–6 O
, O
2021 O
. O

section 0
id pdf2json/2021.acl-long.449.pdf.json
©2021 O
association O
for O
computational O
linguistics O
5782 O

section 1
id pdf2json/2021.acl-long.449.pdf.json
discourse O
parsing O
, O
aiming O
to O
find O
how O
the O
text O
spans O
in O
a O
document O
relate O
to O
each O
other O
, O
benefits O
various O
down-stream O
tasks O
, O
such O
as O
machine O
translation O
evaluation O
( O
guzmán O
et O
al. O
, O
2014 O
; O
joty O
et O
al. O
, O
2014 O
) O
, O
summarization O
( O
marcu O
, O
2000 O
; O
hirao O
et O
al. O
, O
2013 O
) O
, O
sentiment O
analysis O
( O
bhatia O
et O
al. O
, O
2015 O
; O
huber O
and O
carenini O
, O
2020 O
) O
and O
automated O
essay O
scoring O
( O
miltsakaki O
and O
kukich O
, O
2004 O
; O
burstein O
et O
al. O
, O
2013 O
) O
. O

section 1
id pdf2json/2021.acl-long.449.pdf.json
researchers O
have O
made O
impressive O
progress O
on O
discourse O
parsing O
from O
the O
constituency O
perspective O
, O
which O
presents O
discourse O
structures O
as O
constituency O
trees O
( O
ji O
and O
eisenstein O
, O
2014 O
; O
feng O
and O
hirst O
, O
2014 O
; O
joty O
et O
al. O
, O
2015 O
; O
nishida O
and O
*corresponding O
author O
. O

section 1
id pdf2json/2021.acl-long.449.pdf.json
nakayama O
, O
2020 O
) O
. O

section 1
id pdf2json/2021.acl-long.449.pdf.json
however O
, O
as O
demonstrated O
by O
morey O
et O
al O
. O

section 1
id pdf2json/2021.acl-long.449.pdf.json
( O
2018 O
) O
, O
discourse O
structure O
can O
also O
be O
formulated O
as O
a O
dependency O
structure O
. O

section 1
id pdf2json/2021.acl-long.449.pdf.json
besides O
that O
, O
there O
might O
exist O
ambiguous O
parsing O
in O
terms O
of O
the O
constituency O
perspective O
( O
morey O
et O
al. O
, O
2018 O
) O
. O

section 1
id pdf2json/2021.acl-long.449.pdf.json
all O
of O
these O
suggest O
that O
dependency O
discourse O
parsing O
is O
a O
different O
promising O
approach O
for O
discourse O
parsing O
. O

section 1
id pdf2json/2021.acl-long.449.pdf.json
one O
of O
the O
main O
bottlenecks O
in O
developing O
discourse O
dependency O
parsing O
methods O
is O
the O
lack O
of O
annotated O
training O
data O
since O
the O
labeling O
effort O
is O
labor-intensive O
and O
time-consuming O
, O
and O
needs O
well-trained O
experts O
with O
linguistic O
knowledge O
( O
marcu O
et O
al. O
, O
1999 O
) O
. O

section 1
id pdf2json/2021.acl-long.449.pdf.json
this O
problem O
can O
be O
tackled O
by O
employing O
unsupervised O
and O
semisupervised O
methods O
that O
can O
utilize O
unlabeled O
data O
. O

section 1
id pdf2json/2021.acl-long.449.pdf.json
however O
, O
while O
unsupervised O
methodology O
has O
been O
studied O
for O
decades O
in O
syntactic O
dependency O
parsing O
, O
there O
is O
little O
attention O
paid O
to O
the O
counterpart O
in O
discourse O
dependency O
parsing O
. O

section 1
id pdf2json/2021.acl-long.449.pdf.json
considering O
the O
similarity O
between O
syntactic O
and O
discourse O
dependency O
parsing O
, O
it O
is O
natural O
to O
suggest O
such O
methodology O
can O
be O
adapted O
from O
the O
former O
to O
the O
latter O
. O

section 1
id pdf2json/2021.acl-long.449.pdf.json
in O
this O
paper O
, O
we O
propose O
a O
simple O
yet O
effective O
adaptation O
method O
that O
can O
be O
readily O
applied O
to O
different O
unsupervised O
syntactic O
dependency O
parsing O
approaches O
. O

section 1
id pdf2json/2021.acl-long.449.pdf.json
adaptation O
from O
syntactic O
dependency O
parsing O
to O
discourse O
dependency O
parsing O
has O
two O
challenges O
. O

section 1
id pdf2json/2021.acl-long.449.pdf.json
first O
, O
unlike O
syntactic O
parsing O
which O
has O
a O
finite O
vocabulary O
, O
in O
discourse O
parsing O
, O
the O
number O
of O
elementary O
discourse O
units O
( O
edus O
) O
is O
unlimited O
. O

section 1
id pdf2json/2021.acl-long.449.pdf.json
this O
makes O
it O
difficult O
if O
not O
impossible O
to O
directly O
apply O
syntactic O
approaches O
requiring O
enumeration O
of O
words O
or O
word O
categories O
to O
discourse O
parsing O
. O

section 1
id pdf2json/2021.acl-long.449.pdf.json
second O
, O
in O
a O
discourse O
dependency O
parse O
tree O
, O
the O
dependencies O
within O
a O
sentence O
or O
a O
paragraph O
often O
form O
a O
complete O
subtree O
. O

section 1
id pdf2json/2021.acl-long.449.pdf.json
there O
is O
no O
correspondence O
to O
this O
constraint O
in O
syntactic O
parsing O
approaches O
. O

section 1
id pdf2json/2021.acl-long.449.pdf.json
to O
address O
these O
two O
chal- O
lenges O
, O
we O
cluster O
the O
edus O
to O
produce O
clusters O
resembling O
part-of-speech O
( O
pos O
) O
tags O
in O
syntactic O
parsing O
and O
we O
introduce O
the O
hierarchical O
eisner O
algorithm O
that O
finds O
the O
optimal O
parse O
tree O
conforming O
to O
the O
constraint O
. O

section 1
id pdf2json/2021.acl-long.449.pdf.json
we O
applied O
our O
adaptation O
method O
to O
two O
stateof-the-art O
unsupervised O
syntactic O
dependency O
parsing O
models O
: O
neural O
conditional O
random O
field O
autoencoder O
( O
ncrfae O
, O
li O
and O
tu O
( O
2020 O
) O
) O
and O
variational O
variant O
of O
discriminative O
neural O
dependency O
model O
with O
valences O
( O
v-dndmv O
, O
han O
et O
al O
. O

section 1
id pdf2json/2021.acl-long.449.pdf.json
( O
2019 O
) O
) O
. O

section 1
id pdf2json/2021.acl-long.449.pdf.json
in O
our O
experiments O
, O
the O
adapted O
models O
performs O
better O
than O
the O
baseline O
on O
both O
rst O
discourse O
treebank O
( O
rst-dt O
, O
carlson O
et O
al O
. O

section 1
id pdf2json/2021.acl-long.449.pdf.json
( O
2001 O
) O
) O
and O
scidtb O
( O
yang O
and O
li O
, O
2018 O
) O
in O
the O
unsupervised O
setting O
. O

section 1
id pdf2json/2021.acl-long.449.pdf.json
when O
we O
extend O
the O
two O
models O
to O
the O
semi-supervised O
and O
supervised O
setting O
, O
we O
find O
they O
can O
outperform O
previous O
methods O
specially O
designed O
for O
supervised O
discourse O
parsing O
. O

section 1
id pdf2json/2021.acl-long.449.pdf.json
further O
analysis O
indicates O
that O
the O
hierarchical O
eisner O
algorithm O
shows O
superiority O
not O
only O
in O
parsing O
accuracy O
but O
also O
in O
time O
and O
space O
efficiency O
. O

section 1
id pdf2json/2021.acl-long.449.pdf.json
its O
empirical O
time O
and O
space O
complexity O
is O
close O
to O
o O
( O
n2 O
) O
with O
n O
being O
the O
number O
of O
edus O
, O
while O
the O
unconstrained O
algorithm O
adopted O
by O
most O
previous O
work O
has O
a O
complexity O
of O
o O
( O
n3 O
) O
. O

section 1
id pdf2json/2021.acl-long.449.pdf.json
the O
code O
and O
trained O
models O
can O
be O
found O
at O
: O
https O
: O
//github O
. O

section 1
id pdf2json/2021.acl-long.449.pdf.json
com/ehaschia/discoursedependencyparsing O
. O

section 2
id pdf2json/2021.acl-long.449.pdf.json
unsupervised O
syntactic O
dependency O
parsing O
unsupervised O
syntactic O
dependency O
parsing O
is O
the O
task O
to O
find O
syntactic O
dependency O
relations O
between O
words O
in O
sentences O
without O
guidance O
from O
annotations O
. O

section 2
id pdf2json/2021.acl-long.449.pdf.json
the O
most O
popular O
approaches O
to O
this O
task O
are O
dependency O
model O
with O
valences O
( O
dmv O
, O
klein O
and O
manning O
( O
2004 O
) O
) O
, O
a O
generative O
model O
learning O
the O
grammar O
from O
pos O
tags O
for O
dependency O
predictions O
, O
and O
its O
extensions O
. O

section 2
id pdf2json/2021.acl-long.449.pdf.json
jiang O
et O
al O
. O

section 2
id pdf2json/2021.acl-long.449.pdf.json
( O
2016 O
) O
employ O
neural O
networks O
to O
capture O
the O
similarities O
between O
pos O
tags O
ignored O
by O
vanilla O
dmv O
and O
han O
et O
al O
. O

section 2
id pdf2json/2021.acl-long.449.pdf.json
( O
2019 O
) O
further O
amend O
the O
former O
with O
discriminative O
information O
obtained O
from O
an O
additional O
encoding O
network O
. O

section 2
id pdf2json/2021.acl-long.449.pdf.json
besides O
, O
there O
are O
also O
some O
discriminative O
approaches O
modeling O
the O
conditional O
probability O
or O
score O
of O
the O
dependency O
tree O
given O
the O
sentence O
, O
such O
as O
the O
crf O
autoencoder O
method O
proposed O
by O
cai O
et O
al O
. O

section 2
id pdf2json/2021.acl-long.449.pdf.json
( O
2017 O
) O
. O

section 2
id pdf2json/2021.acl-long.449.pdf.json
discourse O
dependency O
parsing O
there O
is O
limited O
work O
focusing O
on O
discourse O
dependency O
parsing O
. O

section 2
id pdf2json/2021.acl-long.449.pdf.json
li O
et O
al O
. O

section 2
id pdf2json/2021.acl-long.449.pdf.json
( O
2014 O
) O
proposes O
an O
algorithm O
to O
convert O
constituency O
rst O
tree O
to O
dependency O
structure O
. O

section 2
id pdf2json/2021.acl-long.449.pdf.json
in O
their O
algorithm O
, O
each O
non-terminal O
is O
assigned O
with O
a O
head O
edu O
, O
which O
is O
the O
head O
edu O
of O
its O
leftmost O
nucleus O
child O
. O

section 2
id pdf2json/2021.acl-long.449.pdf.json
then O
, O
a O
dependency O
relation O
is O
created O
for O
each O
non-terminal O
from O
its O
head O
to O
its O
dependent O
, O
in O
a O
procedure O
similar O
to O
those O
designed O
for O
syntactic O
parsing O
. O

section 2
id pdf2json/2021.acl-long.449.pdf.json
hirao O
et O
al O
. O

section 2
id pdf2json/2021.acl-long.449.pdf.json
( O
2013 O
) O
proposes O
another O
method O
that O
differs O
from O
the O
previous O
one O
in O
the O
processing O
of O
multinuclear O
relations O
. O

section 2
id pdf2json/2021.acl-long.449.pdf.json
yoshida O
et O
al O
. O

section 2
id pdf2json/2021.acl-long.449.pdf.json
( O
2014 O
) O
proposes O
a O
dependency O
parser O
built O
around O
a O
maximum O
spanning O
tree O
decoder O
and O
trains O
on O
dependency O
trees O
converted O
from O
rstdt O
. O

section 2
id pdf2json/2021.acl-long.449.pdf.json
their O
parser O
achieved O
better O
performance O
on O
the O
summarization O
task O
than O
a O
similar O
constituencybased O
parser O
. O

section 2
id pdf2json/2021.acl-long.449.pdf.json
morey O
et O
al O
. O

section 2
id pdf2json/2021.acl-long.449.pdf.json
( O
2018 O
) O
reviews O
the O
rst O
discourse O
parsing O
from O
the O
dependency O
perspective O
. O

section 2
id pdf2json/2021.acl-long.449.pdf.json
they O
adapt O
the O
the O
best O
discourse O
constituency O
parsing O
models O
until O
2018 O
to O
the O
dependency O
task O
. O

section 2
id pdf2json/2021.acl-long.449.pdf.json
yang O
and O
li O
( O
2018 O
) O
constructs O
a O
discourse O
dependency O
treebank O
scidtb O
for O
scientific O
abstracts O
. O

section 2
id pdf2json/2021.acl-long.449.pdf.json
to O
the O
best O
of O
our O
knowledge O
, O
we O
are O
the O
first O
to O
investigate O
unsupervised O
and O
semi-supervised O
discourse O
dependency O
parsing O
. O

section 2
id pdf2json/2021.acl-long.449.pdf.json
unsupervised O
constituent O
discourse O
parsing O
kobayashi O
et O
al O
. O

section 2
id pdf2json/2021.acl-long.449.pdf.json
( O
2019 O
) O
propose O
two O
unsupervised O
methods O
that O
build O
unlabeled O
constituent O
discourse O
trees O
by O
using O
the O
cky O
dynamic O
programming O
algorithm O
. O

section 2
id pdf2json/2021.acl-long.449.pdf.json
their O
methods O
build O
the O
optimal O
tree O
in O
terms O
of O
a O
similarity O
( O
dissimilarity O
) O
score O
function O
that O
is O
defined O
for O
merging O
( O
splitting O
) O
text O
spans O
into O
larger O
( O
smaller O
) O
ones O
. O

section 2
id pdf2json/2021.acl-long.449.pdf.json
nishida O
et O
al O
. O

section 2
id pdf2json/2021.acl-long.449.pdf.json
( O
2020 O
) O
use O
viterbi O
em O
with O
a O
margin-based O
criterion O
to O
train O
a O
span-based O
neural O
unsupervised O
constituency O
discourse O
parser O
. O

section 2
id pdf2json/2021.acl-long.449.pdf.json
the O
performance O
of O
these O
unsupervised O
methods O
is O
close O
to O
that O
of O
previous O
supervised O
parsers O
. O

section 3
id pdf2json/2021.acl-long.449.pdf.json
we O
propose O
an O
adaptation O
method O
that O
can O
be O
readily O
integrated O
with O
different O
unsupervised O
syntactic O
dependency O
parsing O
approaches O
. O

section 3
id pdf2json/2021.acl-long.449.pdf.json
first O
, O
we O
cluster O
the O
element O
discourse O
units O
( O
edu O
) O
to O
produce O
clusters O
resembling O
pos O
tags O
or O
words O
used O
in O
syntactic O
parsing O
. O

section 3
id pdf2json/2021.acl-long.449.pdf.json
this O
is O
necessary O
because O
many O
unsupervised O
syntactic O
parsers O
require O
enumeration O
of O
words O
or O
word O
categories O
, O
typically O
in O
modeling O
multinomial O
distributions O
as O
we O
shall O
see O
in O
section O
4 O
. O

section 3
id pdf2json/2021.acl-long.449.pdf.json
while O
edus O
, O
which O
are O
sequences O
of O
words O
, O
can O
not O
be O
enumerated O
, O
its O
clusters O
can O
. O

section 3
id pdf2json/2021.acl-long.449.pdf.json
during O
parsing O
, O
we O
apply O
the O
hierarchical O
eisner O
algorithm O
used O
for O
parse O
tree O
, O
a O
novel O
modified O
ver- O
sion O
of O
the O
classic O
eisner O
algorithm O
, O
used O
for O
parse O
tree O
to O
produce O
discourse O
dependency O
parse O
trees O
that O
conform O
to O
the O
constraint O
that O
every O
sentence O
or O
paragraph O
should O
correspond O
to O
a O
complete O
subtree O
. O

section 4
id pdf2json/2021.acl-long.449.pdf.json
given O
an O
input O
document O
represented O
as O
an O
edu O
sequence O
x1 O
, O
x2 O
, O
. O

section 4
id pdf2json/2021.acl-long.449.pdf.json
. O

section 4
id pdf2json/2021.acl-long.449.pdf.json
. O

section 4
id pdf2json/2021.acl-long.449.pdf.json
, O
xn O
, O
we O
can O
use O
word O
embedding O
or O
context O
sensitive O
word O
embedding O
to O
get O
the O
vector O
representation O
xi O
of O
the O
i-th O
edu O
xi O
. O

section 4
id pdf2json/2021.acl-long.449.pdf.json
specifically O
, O
we O
use O
bert O
( O
devlin O
et O
al. O
, O
2019 O
) O
to O
encode O
each O
word O
. O

section 4
id pdf2json/2021.acl-long.449.pdf.json
let O
wi O
be O
the O
encoding O
of O
the O
i-th O
word O
in O
the O
document O
. O

section 4
id pdf2json/2021.acl-long.449.pdf.json
for O
an O
edu O
xi O
spanning O
from O
word O
position O
b O
to O
e O
, O
we O
follow O
toshniwal O
et O
al O
. O

section 4
id pdf2json/2021.acl-long.449.pdf.json
( O
2020 O
) O
and O
concatenate O
the O
encoding O
of O
the O
endpoints O
to O
form O
its O
representation O
: O
xi O
= O
[ O
wb O
; O
we O
] O
. O

section 4
id pdf2json/2021.acl-long.449.pdf.json
with O
the O
representations O
of O
all O
edus O
from O
the O
whole O
training O
corpus O
obtained O
, O
we O
use O
k-means O
( O
lloyd O
, O
1982 O
) O
to O
cluster O
them O
. O

section 4
id pdf2json/2021.acl-long.449.pdf.json
let O
ci O
be O
the O
cluster O
label O
of O
xi O
. O

section 5
id pdf2json/2021.acl-long.449.pdf.json
the O
eisner O
algorithm O
( O
eisner O
, O
1996 O
) O
is O
a O
dynamic O
programming O
algorithm O
widely O
used O
to O
find O
the O
optimal O
syntactic O
dependency O
parse O
tree O
. O

section 5
id pdf2json/2021.acl-long.449.pdf.json
the O
basic O
idea O
of O
it O
is O
to O
parse O
the O
left O
and O
right O
dependents O
of O
an O
token O
independently O
and O
combine O
them O
at O
a O
later O
stage O
. O

section 5
id pdf2json/2021.acl-long.449.pdf.json
algorithm O
1 O
shows O
the O
pseudo-code O
of O
the O
eisner O
algorithm O
. O

section 5
id pdf2json/2021.acl-long.449.pdf.json
here O
ci→j O
represents O
a O
complete O
span O
, O
which O
consists O
of O
a O
head O
token O
i O
and O
all O
of O
its O
descendants O
on O
one O
side O
, O
and O
ii→j O
represent O
an O
incomplete O
span O
, O
which O
consists O
of O
a O
head O
i O
and O
its O
partial O
descendants O
on O
one O
side O
and O
can O
be O
extended O
by O
adding O
more O
descendants O
to O
that O
side O
. O

section 5
id pdf2json/2021.acl-long.449.pdf.json
discourse O
dependency O
parse O
trees O
, O
however O
, O
algorithm O
1 O
eisner O
algorithm O
1 O
: O
inputs O
: O
demonstrate O
structural O
characteristics O
not O
taken O
into O
account O
by O
the O
eisner O
algorithm O
. O

section 5
id pdf2json/2021.acl-long.449.pdf.json
specifically O
, O
a O
document O
has O
a O
hierarchical O
structure O
which O
divides O
the O
document O
into O
paragraphs O
, O
each O
paragraph O
into O
sentences O
, O
and O
finally O
each O
sentence O
into O
edus O
, O
and O
the O
discourse O
parse O
tree O
should O
be O
consistent O
with O
this O
hierarchical O
structure O
. O

section 5
id pdf2json/2021.acl-long.449.pdf.json
equivalently O
, O
in O
a O
discourse O
parse O
tree O
, O
every O
sentence O
or O
paragraph O
should O
be O
exactly O
covered O
by O
a O
complete O
subtree O
, O
like O
figure O
1 O
. O

section 5
id pdf2json/2021.acl-long.449.pdf.json
we O
empirically O
find O
that O
this O
constraint O
is O
satisfied O
by O
most O
of O
the O
gold O
discourse O
parses O
in O
the O
rst O
discourse O
treebank O
( O
rst-dt O
, O
carlson O
et O
al O
. O

section 5
id pdf2json/2021.acl-long.449.pdf.json
( O
2001 O
) O
) O
and O
scidtb O
( O
yang O
and O
li O
, O
2018 O
) O
datasets O
( O
table O
1 O
) O
. O

section 5
id pdf2json/2021.acl-long.449.pdf.json
we O
therefore O
propose O
the O
hierarchical O
eisner O
algorithm O
, O
a O
novel O
modification O
to O
the O
eisner O
algorithm O
that O
incorporates O
the O
constraint O
. O

section 5
id pdf2json/2021.acl-long.449.pdf.json
our O
new O
algorithm O
has O
almost O
the O
same O
state O
transition O
formulas O
as O
the O
eisner O
algorithm O
except O
for O
a O
few O
changes O
brought O
by O
the O
hierarchical O
constraint O
. O

section 5
id pdf2json/2021.acl-long.449.pdf.json
concretely O
, O
our O
algorithm O
finds O
the O
optimal O
parse O
tree O
in O
a O
bottom-up O
way O
and O
divides O
the O
process O
into O
3 O
steps O
: O
intra-sentence O
parsing O
, O
intra-paragraph O
parsing O
, O
and O
intra-document O
parsing O
. O

section 5
id pdf2json/2021.acl-long.449.pdf.json
in O
the O
intra-sentence O
parsing O
step O
, O
we O
run O
the O
original O
eisner O
algorithm O
, O
except O
that O
we O
need O
not O
to O
form O
a O
tree O
. O

section 5
id pdf2json/2021.acl-long.449.pdf.json
then O
in O
the O
algorithm O
2 O
modification O
to O
algorithm O
1 O
6 O
: O
ii→j O
= O
max O
i≤k≤j O
( O
sij O
+ O
ci→k O
+ O
ck+1←j O
) O
7 O
: O
ii←j O
= O
max O
i≤k≤j O
( O
sji O
+ O
ci→k O
+ O
ck+1←j O
) O
8 O
: O
ci→j O
= O
max O
i≤k≤j O
j∈e O
( O
ii→k O
+ O
ck→j O
) O
. O

section 5
id pdf2json/2021.acl-long.449.pdf.json
here O
e O
is O
a O
set O
of O
the O
index O
of O
the O
end O
boundary O
of O
sentences O
. O

section 5
id pdf2json/2021.acl-long.449.pdf.json
9 O
: O
ci←j O
= O
max O
i≤k≤j O
i∈b O
( O
ci←k O
+ O
ik←j O
) O
. O

section 5
id pdf2json/2021.acl-long.449.pdf.json
here O
b O
is O
a O
set O
of O
the O
index O
of O
the O
begin O
boundary O
of O
sentences O
. O

section 5
id pdf2json/2021.acl-long.449.pdf.json
intra-paragraph O
step O
, O
we O
combine O
all O
intra-sentence O
spans O
in O
the O
paragraph O
. O

section 5
id pdf2json/2021.acl-long.449.pdf.json
under O
the O
constraint O
that O
there O
can O
only O
be O
one O
edu O
in O
every O
sentence O
whose O
head O
is O
not O
belong O
to O
this O
sentence O
. O

section 5
id pdf2json/2021.acl-long.449.pdf.json
to O
achieve O
that O
, O
we O
modify O
the O
state O
transition O
equations O
( O
step O
6-9 O
in O
algorithm O
1 O
) O
to O
prune O
invalid O
arcs O
. O

section 5
id pdf2json/2021.acl-long.449.pdf.json
figure O
2 O
shows O
some O
cases O
during O
merge O
across O
sentence O
spans O
. O

section 5
id pdf2json/2021.acl-long.449.pdf.json
case O
1 O
are O
valid O
because O
the O
constraint O
is O
satisfied O
. O

section 5
id pdf2json/2021.acl-long.449.pdf.json
case O
2 O
is O
invalid O
because O
the O
head O
of O
edu O
e6 O
can O
not O
be O
e4 O
or O
e5 O
hence O
the O
constraint O
is O
violated O
. O

section 5
id pdf2json/2021.acl-long.449.pdf.json
from O
these O
cases O
, O
we O
can O
find O
that O
for O
incomplete O
span O
ii→k O
and O
complete O
span O
ck→j O
across O
sentences O
, O
we O
only O
merge O
them O
when O
j O
is O
at O
the O
end O
boundary O
of O
a O
sentence O
as O
algorithm O
2 O
shows O
. O

section 5
id pdf2json/2021.acl-long.449.pdf.json
after O
the O
intra-paragraph O
step O
, O
we O
move O
to O
the O
intra-document O
step O
to O
combine O
paragraph-level O
spans O
following O
the O
same O
procedure O
as O
in O
the O
intraparagraph O
step O
and O
form O
the O
final O
document-level O
tree O
. O

section 5
id pdf2json/2021.acl-long.449.pdf.json
our O
method O
has O
lower O
time O
complexity O
than O
the O
original O
eisner O
algorithm O
. O

section 5
id pdf2json/2021.acl-long.449.pdf.json
suppose O
a O
document O
has O
kp O
paragraphs O
, O
each O
paragraph O
has O
ks O
sentences O
and O
each O
sentence O
has O
ke O
edus O
. O

section 5
id pdf2json/2021.acl-long.449.pdf.json
the O
time O
complexity O
of O
the O
original O
eisner O
algorithm O
is O
o O
( O
k3pk O
3 O
sk O
3 O
e O
) O
while O
the O
time O
complexity O
of O
our O
hierarchical O
eisner O
algorithm O
is O
o O
( O
k2pk O
3 O
sk O
3 O
e O
) O
. O

section 6
id pdf2json/2021.acl-long.449.pdf.json
we O
adapt O
two O
current O
state-of-the-art O
models O
in O
unsupervised O
syntactic O
dependency O
parsing O
for O
discourse O
parsing O
. O

section 6
id pdf2json/2021.acl-long.449.pdf.json
one O
is O
neural O
crf O
autoencoder O
( O
ncrfae O
, O
li O
and O
tu O
( O
2020 O
) O
; O
cai O
et O
al O
. O

section 6
id pdf2json/2021.acl-long.449.pdf.json
( O
2017 O
) O
) O
, O
a O
discriminative O
model O
, O
and O
the O
other O
is O
: O
variational O
variant O
of O
dndmv O
( O
v-dndmv O
, O
han O
et O
al O
. O

section 6
id pdf2json/2021.acl-long.449.pdf.json
( O
2019 O
) O
) O
, O
a O
generative O
model O
. O

section 7
id pdf2json/2021.acl-long.449.pdf.json
a O
crf O
autoencoder O
( O
ammar O
et O
al. O
, O
2014 O
) O
consists O
of O
an O
encoder O
and O
a O
decoder O
. O

section 7
id pdf2json/2021.acl-long.449.pdf.json
the O
encoder O
predicts O
a O
hidden O
structure O
, O
such O
as O
a O
discourse O
dependency O
tree O
in O
our O
task O
, O
from O
the O
input O
and O
the O
decoder O
tries O
to O
reconstruct O
the O
input O
from O
the O
hidden O
structure O
. O

section 7
id pdf2json/2021.acl-long.449.pdf.json
in O
a O
neuralized O
crf O
autoencoder O
, O
we O
employ O
neural O
networks O
as O
the O
encoder O
and/or O
decoder O
. O

section 7
id pdf2json/2021.acl-long.449.pdf.json
we O
use O
the O
widely O
used O
biaffine O
dependency O
parser O
( O
dozat O
and O
manning O
, O
2017 O
) O
as O
the O
encoder O
to O
compute O
the O
hidden O
structure O
distribution O
pφ O
( O
y|x O
) O
, O
parameterized O
with O
φ O
. O

section 7
id pdf2json/2021.acl-long.449.pdf.json
here O
y O
represents O
the O
hidden O
structure O
and O
x O
is O
input O
document O
. O

section 7
id pdf2json/2021.acl-long.449.pdf.json
we O
feed O
the O
input O
document O
x O
into O
a O
bi-lstm O
network O
to O
produce O
the O
contextual O
representation O
of O
each O
edu O
segmentation O
ri O
, O
and O
then O
feed O
ri O
to O
two O
mlp O
networks O
to O
produce O
two O
continuous O
vectors O
v O
( O
head O
) O
i O
and O
v O
( O
dep O
) O
i O
, O
representing O
i-th O
edu O
segmentation O
being O
used O
as O
dependency O
head O
and O
dependent O
respectively O
. O

section 7
id pdf2json/2021.acl-long.449.pdf.json
a O
biaffine O
function O
is O
used O
to O
compute O
the O
score O
matrix O
s. O
each O
matrix O
element O
sij O
, O
the O
score O
for O
a O
dependency O
arc O
pointing O
from O
xi O
to O
xj O
, O
is O
computed O
as O
follows O
: O
sij O
= O
v O
( O
head O
) O
> O
i O
wv O
( O
dep O
) O
i O
+ O
b O
( O
1 O
) O
where O
w O
is O
the O
parameter O
matrix O
and O
b O
is O
the O
bias O
. O

section 7
id pdf2json/2021.acl-long.449.pdf.json
following O
dozat O
and O
manning O
( O
2017 O
) O
we O
formulate O
pφ O
( O
y|x O
) O
as O
a O
head O
selection O
problem O
process O
that O
selects O
the O
dependency O
head O
of O
each O
edu O
in- O
dependently O
: O
pφ O
( O
y|x O
) O
= O
∏ O
i O
p O
( O
hi|x O
) O
( O
2 O
) O
where O
hi O
is O
the O
index O
of O
the O
head O
of O
edu O
xi O
and O
p O
( O
hi|x O
) O
is O
computed O
by O
softmax O
function O
with O
score O
sij O
: O
p O
( O
hi O
= O
j|x O
) O
= O
esji∑n O
k=1 O
e O
ski O
( O
3 O
) O
the O
decoder O
parameterized O
with O
λ O
computes O
pλ O
( O
x̂|y O
) O
, O
the O
probability O
of O
the O
reconstructed O
document O
x̂ O
given O
the O
parse O
tree O
y O
. O

section 7
id pdf2json/2021.acl-long.449.pdf.json
following O
cai O
et O
al O
. O

section 7
id pdf2json/2021.acl-long.449.pdf.json
( O
2017 O
) O
and O
li O
and O
tu O
( O
2020 O
) O
, O
we O
independently O
predict O
each O
edu O
x̂i O
from O
its O
head O
specified O
by O
y O
. O

section 7
id pdf2json/2021.acl-long.449.pdf.json
since O
edus O
can O
not O
be O
enumerated O
, O
we O
reformulate O
the O
process O
as O
predicting O
the O
edu O
cluster O
ĉi O
given O
its O
dependency O
head O
cluster O
chi O
. O

section 7
id pdf2json/2021.acl-long.449.pdf.json
our O
decoder O
simply O
specifies O
a O
categorical O
distribution O
p O
( O
ĉi|chi O
) O
for O
each O
possible O
edu O
cluster O
and O
compute O
the O
reconstruction O
probability O
as O
follows O
: O
pλ O
( O
x̂|y O
) O
= O
∏ O
i O
p O
( O
ĉi|chi O
) O
( O
4 O
) O
we O
achieve O
the O
final O
reconstruction O
distribution O
by O
cascading O
the O
encoder O
and O
decoder O
distribution O
: O
pφ O
, O
λ O
( O
x̂ O
, O
y|x O
) O
= O
pλ O
( O
x̂|y O
) O
pφ O
( O
y|x O
) O
( O
5 O
) O
the O
best O
parsing O
is O
obtained O
by O
maximizing O
pφ O
, O
λ O
( O
x̂ O
, O
y|x O
) O
: O
y∗ O
= O
arg O
max O
y O
pφ O
, O
λ O
( O
x̂ O
, O
y|x O
) O
( O
6 O
) O
we O
consider O
the O
general O
case O
of O
training O
the O
crf O
autoencoder O
with O
dataset O
d O
containing O
both O
labelled O
data O
l O
and O
unlabelled O
data O
u. O
purely O
supervised O
or O
unsupervised O
learning O
can O
be O
seen O
as O
special O
cases O
of O
this O
setting O
. O

section 7
id pdf2json/2021.acl-long.449.pdf.json
the O
loss O
functionl O
( O
d O
) O
consists O
of O
a O
labelled O
loss O
ll O
( O
l O
) O
and O
an O
unlabelled O
loss O
lu O
( O
u O
) O
: O
l O
( O
d O
) O
= O
αll O
( O
l O
) O
+ O
( O
1− O
α O
) O
lu O
( O
u O
) O
( O
7 O
) O
where O
α O
is O
the O
hyperparameter O
weighting O
the O
importance O
of O
the O
two O
parts O
. O

section 7
id pdf2json/2021.acl-long.449.pdf.json
for O
the O
labelled O
data O
, O
where O
the O
gold O
parse O
trees O
y∗ O
are O
known O
, O
labelled O
loss O
is O
: O
ll O
( O
l O
) O
= O
− O
∑ O
x∈l O
logpφ O
, O
λ O
( O
x̂ O
, O
y O
∗|x O
) O
( O
8 O
) O
for O
the O
unlabelled O
data O
where O
the O
gold O
parses O
are O
unknown O
, O
the O
unlabelled O
loss O
is O
: O
lu O
( O
u O
) O
= O
− O
∑ O
x∈u O
max O
y∈y O
( O
x O
) O
logpφ O
, O
λ O
( O
x̂ O
, O
y|x O
) O
( O
9 O
) O
we O
optimize O
the O
encoder O
parameter O
φ O
and O
decoder O
parameter O
λ O
together O
with O
gradient O
descent O
methods O
. O

section 8
id pdf2json/2021.acl-long.449.pdf.json
v-dndmv O
is O
a O
variational O
autoencoder O
model O
composed O
of O
both O
an O
encoder O
and O
a O
decoder O
. O

section 8
id pdf2json/2021.acl-long.449.pdf.json
the O
encoder O
is O
a O
bi-lstm O
that O
takes O
the O
input O
document O
and O
produces O
parameters O
of O
a O
gaussian O
distribution O
from O
which O
a O
continuous O
vector O
s O
summarizing O
the O
document O
sampled O
. O

section 8
id pdf2json/2021.acl-long.449.pdf.json
the O
decoder O
models O
the O
joint O
probability O
of O
the O
document O
and O
its O
discourse O
dependency O
tree O
condition O
on O
s O
with O
a O
generative O
grammar O
. O

section 8
id pdf2json/2021.acl-long.449.pdf.json
the O
grammar O
is O
defined O
on O
a O
finite O
set O
of O
discrete O
symbols O
, O
so O
in O
our O
adapted O
model O
, O
input O
documents O
are O
represented O
by O
edu O
clusters O
instead O
of O
edus O
that O
are O
infinite O
in O
number O
. O

section 8
id pdf2json/2021.acl-long.449.pdf.json
there O
are O
three O
types O
of O
grammar O
rules O
, O
each O
associated O
with O
a O
set O
of O
probabilistic O
distributions O
: O
root O
, O
child O
and O
decision O
. O

section 8
id pdf2json/2021.acl-long.449.pdf.json
to O
generate O
a O
document O
, O
we O
firstly O
sample O
from O
the O
root O
distribution O
proot O
( O
chd|s O
) O
to O
determine O
the O
cluster O
label O
of O
the O
head O
edu O
of O
the O
document O
and O
then O
recursively O
decide O
whether O
to O
generate O
a O
new O
child O
edu O
cluster O
and O
what O
child O
edu O
cluster O
to O
generate O
by O
sampling O
from O
the O
decision O
distribution O
pdecision O
( O
dec|h O
, O
dir O
, O
val O
, O
s O
) O
and O
child O
distribution O
pchild O
( O
chd|h O
, O
dir O
, O
val O
, O
s O
) O
. O

section 8
id pdf2json/2021.acl-long.449.pdf.json
dir O
denotes O
the O
generation O
direction O
( O
i.e O
, O
left O
or O
right O
) O
, O
val O
is O
a O
binary O
variable O
denoting O
whether O
the O
current O
edu O
already O
has O
a O
child O
in O
the O
direction O
dir O
or O
not O
. O

section 8
id pdf2json/2021.acl-long.449.pdf.json
dec O
is O
a O
binary O
variable O
indicating O
whether O
to O
continue O
generating O
a O
child O
edu O
, O
and O
h O
and O
chd O
denote O
the O
parent O
and O
child O
edu O
cluster O
respectively O
. O

section 8
id pdf2json/2021.acl-long.449.pdf.json
we O
use O
neural O
networks O
to O
calculate O
these O
distributions O
. O

section 8
id pdf2json/2021.acl-long.449.pdf.json
the O
input O
of O
the O
networks O
is O
the O
continuous O
vector O
or O
matrix O
representations O
of O
grammar O
rule O
components O
such O
as O
h O
, O
chd O
, O
val O
and O
dir O
as O
well O
as O
document O
vector O
s O
produced O
by O
the O
encoder O
. O

section 8
id pdf2json/2021.acl-long.449.pdf.json
the O
training O
objective O
for O
learning O
the O
model O
is O
the O
probability O
of O
the O
training O
data O
. O

section 8
id pdf2json/2021.acl-long.449.pdf.json
the O
intermediate O
continuous O
vector O
s O
and O
the O
hidden O
variable O
representing O
the O
dependency O
tree O
are O
both O
marginalized O
. O

section 8
id pdf2json/2021.acl-long.449.pdf.json
since O
the O
marginalized O
probability O
can O
not O
be O
calculated O
exactly O
, O
v-dndmv O
maximizes O
the O
evidence O
lower O
bound O
( O
elbo O
) O
, O
a O
lower O
bound O
of O
the O
marginalized O
probability O
. O

section 8
id pdf2json/2021.acl-long.449.pdf.json
elbo O
consists O
of O
the O
conditional O
likelihood O
of O
the O
training O
data O
and O
an O
regularisation O
term O
given O
by O
the O
kl O
divergence O
between O
pθ O
( O
s|x O
) O
and O
p O
( O
s O
) O
( O
which O
is O
a O
standard O
gaussian O
) O
. O

section 8
id pdf2json/2021.acl-long.449.pdf.json
the O
conditional O
likelihood O
is O
shown O
as O
follows O
: O
l O
( O
θ O
) O
= O
1 O
n O
n∑ O
i=1 O
∑ O
y O
( O
i O
) O
∈y O
( O
x O
( O
i O
) O
) O
logpθ O
( O
x O
( O
i O
) O
, O
y O
( O
i O
) O
|s O
( O
i O
) O
) O
( O
10 O
) O
here O
n O
is O
the O
number O
of O
training O
samples O
, O
y O
is O
the O
dependency O
tree O
and O
y O
( O
x O
) O
is O
the O
set O
of O
all O
possible O
dependency O
tree O
in O
x. O
θ O
is O
the O
parameters O
of O
the O
neural O
networks O
. O

section 8
id pdf2json/2021.acl-long.449.pdf.json
we O
can O
rewrite O
the O
conditional O
probability O
as O
following O
: O
pθ O
( O
x O
, O
y|s O
) O
= O
∏ O
r∈ O
( O
x O
, O
y O
) O
p O
( O
r|s O
) O
( O
11 O
) O
where O
r O
is O
the O
grammar O
rule O
involved O
in O
generating O
x O
along O
with O
y O
. O

section 8
id pdf2json/2021.acl-long.449.pdf.json
we O
optimize O
elbo O
using O
the O
expectationmaximization O
( O
em O
) O
algorithm O
, O
alternating O
the O
estep O
and O
the O
m-step O
. O

section 8
id pdf2json/2021.acl-long.449.pdf.json
in O
the O
e-step O
, O
we O
fix O
rule O
parameters O
and O
use O
our O
hierarchical O
eisner O
algorithm O
to O
compute O
the O
expectation O
of O
possible O
dependency O
tree O
y O
, O
which O
gives O
the O
expected O
count O
of O
rules O
used O
in O
the O
training O
samples O
. O

section 8
id pdf2json/2021.acl-long.449.pdf.json
in O
the O
m-step O
, O
expected O
count O
of O
rules O
computed O
in O
the O
e-step O
is O
used O
to O
train O
the O
prediction O
neural O
networks O
with O
gradient O
descent O
methods O
. O

section 8
id pdf2json/2021.acl-long.449.pdf.json
the O
regularisation O
term O
is O
also O
optimized O
using O
gradient O
descent O
methods O
in O
the O
m-step O
. O

section 8
id pdf2json/2021.acl-long.449.pdf.json
after O
training O
, O
the O
parsing O
result O
y∗of O
a O
new O
test O
case O
x O
is O
obtained O
as O
: O
y∗ O
= O
arg O
max O
y∈y O
( O
x O
) O
pθ O
( O
x O
, O
y|s O
) O
( O
12 O
) O

section 10
id pdf2json/2021.acl-long.449.pdf.json
data O
we O
evaluate O
the O
performance O
of O
our O
models O
on O
the O
rst O
discourse O
treebank* O
( O
rst-dt O
, O
carlson O
et O
al O
. O

section 10
id pdf2json/2021.acl-long.449.pdf.json
( O
2001 O
) O
) O
and O
scidtb† O
( O
yang O
and O
li O
, O
2018 O
) O
. O

section 10
id pdf2json/2021.acl-long.449.pdf.json
rst-dt O
consists O
of O
wall O
street O
journal O
articles O
manually O
annotated O
with O
rst O
structures O
( O
mann O
and O
thompson O
, O
1988 O
) O
. O

section 10
id pdf2json/2021.acl-long.449.pdf.json
we O
use O
the O
method O
proposed O
by O
li O
et O
al O
. O

section 10
id pdf2json/2021.acl-long.449.pdf.json
( O
2014 O
) O
to O
convert O
the O
rst O
structure O
samples O
into O
dependency O
structures O
. O

section 10
id pdf2json/2021.acl-long.449.pdf.json
scidtb O
consists O
of O
scientific O
abstracts O
from O
acl O
anthology O
annotated O
with O
dependency O
structures O
. O

section 10
id pdf2json/2021.acl-long.449.pdf.json
*https O
: O
//catalog.ldc.upenn.edu/ O
ldc2002t07 O
†https O
: O
//github.com/pku-tangent/scidtb O
hyper-parameter O
for O
our O
ncrfae O
model O
, O
we O
adopt O
the O
hyper-parameters O
of O
li O
and O
tu O
( O
2020 O
) O
. O

section 10
id pdf2json/2021.acl-long.449.pdf.json
for O
our O
v-ndnmv O
model O
we O
adopt O
the O
hyperparameters O
of O
han O
et O
al O
. O

section 10
id pdf2json/2021.acl-long.449.pdf.json
( O
2019 O
) O
. O

section 10
id pdf2json/2021.acl-long.449.pdf.json
we O
use O
adam O
( O
kingma O
and O
ba O
, O
2015 O
) O
to O
optimize O
our O
objective O
functions O
. O

section 10
id pdf2json/2021.acl-long.449.pdf.json
experimental O
details O
are O
provided O
in O
appendix O
a O
. O

section 11
id pdf2json/2021.acl-long.449.pdf.json
we O
compared O
our O
methods O
with O
the O
following O
baselines O
: O
right O
branching O
( O
rb O
) O
is O
a O
rule O
based O
method O
. O

section 11
id pdf2json/2021.acl-long.449.pdf.json
given O
a O
sequence O
of O
elements O
( O
i.e. O
, O
edus O
or O
subtrees O
) O
, O
rb O
generates O
a O
left O
to O
right O
chain O
structure O
, O
like O
x1 O
→ O
x2 O
, O
x2 O
→ O
x3 O
· O
· O
· O
. O

section 11
id pdf2json/2021.acl-long.449.pdf.json
in O
order O
to O
develop O
a O
strong O
baseline O
, O
we O
include O
the O
hierarchical O
constraint O
introduced O
in O
section O
3.2 O
in O
this O
procedure O
. O

section 11
id pdf2json/2021.acl-long.449.pdf.json
that O
is O
, O
we O
first O
build O
sentence-level O
discourse O
trees O
using O
the O
right O
branching O
method O
based O
on O
sentence O
segmentation O
. O

section 11
id pdf2json/2021.acl-long.449.pdf.json
then O
we O
build O
paragraph-level O
trees O
using O
the O
right O
branching O
method O
to O
form O
a O
left O
to O
right O
chain O
of O
sentencelevel O
subtrees O
. O

section 11
id pdf2json/2021.acl-long.449.pdf.json
finally O
we O
obtain O
document-level O
trees O
in O
the O
same O
way O
. O

section 11
id pdf2json/2021.acl-long.449.pdf.json
since O
this O
method O
has O
three O
stages O
, O
we O
call O
it O
“ O
rb O
rb O
rb O
” O
. O

section 11
id pdf2json/2021.acl-long.449.pdf.json
this O
simple O
procedure O
forms O
a O
strong O
baseline O
in O
terms O
of O
performance O
. O

section 11
id pdf2json/2021.acl-long.449.pdf.json
as O
nishida O
and O
nakayama O
( O
2020 O
) O
reports O
, O
the O
unlabeled O
f1 O
score O
of O
constituent O
structures O
of O
rb O
rb O
rb O
reaches O
79.9 O
on O
rst-dt O
. O

section 11
id pdf2json/2021.acl-long.449.pdf.json
correspondingly O
, O
the O
performance O
of O
the O
supervised O
method O
proposed O
by O
( O
joty O
et O
al. O
, O
2015 O
) O
is O
82.5 O
. O

section 11
id pdf2json/2021.acl-long.449.pdf.json
nishida20 O
is O
a O
neural O
model O
for O
unsupervised O
discourse O
constituency O
parsing O
proposed O
by O
nishida O
and O
nakayama O
( O
2020 O
) O
. O

section 11
id pdf2json/2021.acl-long.449.pdf.json
this O
model O
runs O
a O
cky O
parser O
that O
uses O
a O
bi-lstm O
model O
to O
learn O
representations O
of O
text O
spans O
, O
complemented O
with O
lexical O
, O
syntactic O
and O
structural O
features O
. O

section 11
id pdf2json/2021.acl-long.449.pdf.json
we O
convert O
its O
result O
to O
dependency O
structure O
using O
the O
same O
conversation O
method O
of O
li O
et O
al O
. O

section 11
id pdf2json/2021.acl-long.449.pdf.json
( O
2014 O
) O
. O

section 11
id pdf2json/2021.acl-long.449.pdf.json
to O
make O
a O
fair O
comparison O
, O
we O
use O
rb O
rb O
rb O
to O
initialize O
their O
model O
instead O
of O
rb∗ O
rb O
rb O
as O
in O
their O
paper O
, O
where O
rb∗ O
means O
using O
predicted O
syntactic O
structures O
for O
initialization O
at O
the O
sentence O
level O
. O

section 11
id pdf2json/2021.acl-long.449.pdf.json
compared O
with O
baselines O
, O
our O
two O
adapted O
models O
ncrfae O
and O
v-dndmv O
both O
achieve O
better O
performance O
on O
the O
two O
datasets O
. O

section 11
id pdf2json/2021.acl-long.449.pdf.json
results O
also O
show O
that O
the O
generative O
model O
v-dndmv O
is O
better O
than O
the O
discriminatve O
model O
ncrfae O
in O
the O
unsupervised O
setting O
. O

section 11
id pdf2json/2021.acl-long.449.pdf.json
we O
also O
investigate O
the O
semi-supervised O
setting O
on O
the O
scidtb O
dataset O
of O
our O
adapted O
models O
with O
varied O
ratios O
of O
labeled/unlabeled O
data O
. O

section 11
id pdf2json/2021.acl-long.449.pdf.json
experimental O
results O
are O
shown O
in O
figure O
3 O
, O
which O
indicate O
that O
ncrfae O
outperforms O
v-dndmv O
for O
all O
the O
ratios O
. O

section 11
id pdf2json/2021.acl-long.449.pdf.json
even O
when O
trained O
with O
only O
a O
few O
labeled O
data O
( O
0.01 O
of O
labeled O
data O
in O
scidtb O
, O
only O
about O
7 O
samples O
) O
, O
the O
discriminative O
model O
already O
outperforms O
the O
generative O
model O
significantly O
. O

section 11
id pdf2json/2021.acl-long.449.pdf.json
besides O
that O
, O
we O
also O
find O
our O
semi-supervised O
methods O
reach O
higher O
uas O
scores O
than O
their O
supervised O
versions O
( O
trained O
with O
labeled O
data O
only O
) O
for O
all O
the O
labeled/unlabeled O
data O
ratios O
. O

section 11
id pdf2json/2021.acl-long.449.pdf.json
inspired O
by O
the O
promising O
results O
in O
the O
semisupervised O
setting O
, O
we O
also O
investigate O
the O
performance O
of O
our O
adapted O
ncrfae O
and O
v-dndmv O
in O
the O
fully O
supervised O
setting O
. O

section 11
id pdf2json/2021.acl-long.449.pdf.json
the O
results O
are O
shown O
in O
table O
3 O
. O

section 11
id pdf2json/2021.acl-long.449.pdf.json
we O
evaluate O
our O
models O
on O
the O
rstdt O
and O
scidtb O
datasets O
and O
compare O
them O
with O
eight O
models O
. O

section 11
id pdf2json/2021.acl-long.449.pdf.json
nivre04 O
( O
nivre O
et O
al. O
, O
2004 O
) O
and O
wang17 O
( O
wang O
et O
al. O
, O
2017 O
) O
are O
two O
transitionbased O
models O
for O
dependency O
parsing O
. O

section 11
id pdf2json/2021.acl-long.449.pdf.json
yang O
and O
li O
( O
2018 O
) O
adapts O
them O
to O
discourse O
dependency O
parsing O
. O

section 11
id pdf2json/2021.acl-long.449.pdf.json
feng14 O
( O
feng O
and O
hirst O
, O
2014 O
) O
, O
ji14 O
‡we O
correct O
their O
evaluation O
metrics O
, O
so O
the O
result O
is O
different O
from O
the O
original O
paper O
( O
li O
et O
al. O
, O
2014 O
) O
. O

section 11
id pdf2json/2021.acl-long.449.pdf.json
( O
ji O
and O
eisenstein O
, O
2014 O
) O
, O
joty15 O
( O
joty O
et O
al. O
, O
2015 O
) O
and O
braud17 O
( O
braud O
et O
al. O
, O
2017 O
) O
are O
methods O
for O
discourse O
constituent O
parsing O
and O
they O
are O
adapted O
for O
discourse O
dependency O
parsing O
by O
morey O
et O
al O
. O

section 11
id pdf2json/2021.acl-long.449.pdf.json
( O
2018 O
) O
. O

section 11
id pdf2json/2021.acl-long.449.pdf.json
li14 O
( O
li O
et O
al. O
, O
2014 O
) O
and O
morey18 O
( O
morey O
et O
al. O
, O
2018 O
) O
are O
graph-based O
and O
transition-based O
methods O
specially O
designed O
for O
discourse O
dependency O
parsing O
, O
respectively O
. O

section 11
id pdf2json/2021.acl-long.449.pdf.json
these O
models O
are O
statistical O
or O
simple O
neural O
models O
, O
and O
they O
do O
not O
use O
pretrained O
language O
models O
( O
like O
bert O
, O
elmo O
( O
peters O
et O
al. O
, O
2018 O
) O
) O
to O
extract O
features O
. O

section 11
id pdf2json/2021.acl-long.449.pdf.json
as O
table O
3 O
shows O
, O
the O
performance O
of O
our O
ncrfae O
is O
significantly O
better O
than O
the O
baseline O
models O
. O

section 11
id pdf2json/2021.acl-long.449.pdf.json
especially O
, O
the O
uas O
and O
las O
of O
ncrfae O
are O
8.9 O
points O
and O
11.5 O
points O
higher O
than O
the O
best O
baseline O
models O
on O
the O
scidtb O
dataset O
, O
respectively O
. O

section 11
id pdf2json/2021.acl-long.449.pdf.json
besides O
that O
, O
we O
find O
that O
v-dndmv O
also O
beats O
baselines O
on O
the O
scidtb O
dataset O
and O
reaches O
comparable O
results O
on O
rst-dt O
. O

section 11
id pdf2json/2021.acl-long.449.pdf.json
we O
also O
test O
our O
approaches O
without O
using O
bert O
and O
find O
that O
they O
still O
outperform O
the O
baselines O
. O

section 11
id pdf2json/2021.acl-long.449.pdf.json
for O
example O
, O
the O
performance O
of O
ncrfae O
with O
glove O
( O
pennington O
et O
al. O
, O
2014 O
) O
on O
scidtb O
averaged O
over O
5 O
runs O
is O
: O
uas O
: O
73.9 O
las O
: O
55.5 O
. O

section 11
id pdf2json/2021.acl-long.449.pdf.json
these O
results O
again O
give O
evidence O
for O
our O
success O
in O
adapting O
unsupervised O
syntactic O
dependency O
parsing O
methods O
for O
discourse O
dependency O
parsing O
as O
the O
adapted O
methods O
not O
only O
work O
in O
the O
unsupervised O
setting O
, O
but O
also O
reach O
state-of-the-art O
in O
the O
supervised O
setting O
. O

section 11
id pdf2json/2021.acl-long.449.pdf.json
as O
for O
the O
performance O
gap O
between O
v-dndmv O
and O
ncrfae O
, O
we O
believe O
that O
the O
main O
reason O
is O
their O
different O
abilities O
to O
extract O
contextual O
features O
from O
the O
input O
text O
for O
the O
parsing O
task O
. O

section 11
id pdf2json/2021.acl-long.449.pdf.json
as O
a O
generative O
model O
, O
the O
decoder O
of O
v-dndmv O
follows O
a O
strong O
assumption O
that O
each O
token O
in O
the O
input O
text O
is O
generated O
independently O
, O
which O
prevents O
the O
contextual O
features O
from O
being O
directly O
used O
. O

section 11
id pdf2json/2021.acl-long.449.pdf.json
instead O
, O
contextual O
features O
are O
mixed O
with O
other O
information O
in O
the O
document O
representation O
which O
acts O
as O
the O
condition O
of O
the O
generation O
process O
in O
the O
model O
. O

section 11
id pdf2json/2021.acl-long.449.pdf.json
ncrfae O
, O
on O
the O
other O
hand O
, O
employs O
a O
discriminative O
parser O
to O
leverage O
contextual O
features O
for O
dependency O
structure O
prediction O
directly O
. O

section 11
id pdf2json/2021.acl-long.449.pdf.json
thus O
, O
as O
long O
as O
there O
is O
sufficient O
labeled O
data O
, O
ncrfae O
can O
achieve O
much O
better O
results O
than O
vdndmv O
. O

section 11
id pdf2json/2021.acl-long.449.pdf.json
we O
have O
observed O
a O
similar O
phenomenon O
in O
syntactic O
parsing O
. O

section 11
id pdf2json/2021.acl-long.449.pdf.json
significance O
test O
we O
investigate O
the O
significance O
of O
the O
performance O
improvement O
in O
every O
setting O
. O

section 11
id pdf2json/2021.acl-long.449.pdf.json
for O
unsupervised O
parsing O
, O
we O
perform O
a O
t-test O
between O
the O
strongest O
baseline O
rb O
rb O
rb O
and O
vdndmv O
. O

section 11
id pdf2json/2021.acl-long.449.pdf.json
the O
t-value O
and O
p-value O
calculated O
on O
10 O
runs O
are O
2.86 O
and O
0.00104 O
, O
which O
shows O
the O
significance O
of O
the O
improvement O
. O

section 11
id pdf2json/2021.acl-long.449.pdf.json
for O
the O
semisupervised O
results O
, O
we O
also O
perform O
significance O
tests O
between O
the O
semi-supervised O
and O
supervisedonly O
results O
. O

section 11
id pdf2json/2021.acl-long.449.pdf.json
the O
results O
show O
that O
our O
semisupervised O
method O
significantly O
outperforms O
the O
supervised-only O
method O
. O

section 11
id pdf2json/2021.acl-long.449.pdf.json
for O
example O
, O
on O
the O
0.5:0.5 O
setting O
, O
the O
t-value O
is O
2.13 O
and O
the O
p-value O
is O
0.04767 O
. O

section 11
id pdf2json/2021.acl-long.449.pdf.json
for O
the O
fully O
supervised O
setting O
, O
due O
to O
a O
lack O
of O
code O
from O
previous O
work O
, O
it O
is O
currently O
difficult O
for O
us O
to O
carry O
out O
a O
significance O
analysis O
. O

section 11
id pdf2json/2021.acl-long.449.pdf.json
instead O
, O
we O
show O
that O
our O
models O
are O
very O
stable O
and O
consistently O
outperform O
the O
baselines O
by O
running O
our O
models O
for O
10-times O
. O

section 11
id pdf2json/2021.acl-long.449.pdf.json
for O
example O
, O
our O
ncrfae O
uas O
score O
is O
78.95±0.29 O
on O
the O
scidtb O
dataset O
. O

section 13
id pdf2json/2021.acl-long.449.pdf.json
in O
the O
left O
part O
of O
figure O
4 O
we O
show O
the O
curves O
of O
the O
time O
cost O
of O
the O
hierarchical O
and O
traditional O
eisner O
algorithms O
against O
the O
rst-dt O
document O
length O
. O

section 13
id pdf2json/2021.acl-long.449.pdf.json
the O
experiments O
are O
run O
on O
servers O
equipped O
with O
nvidia O
titan O
v O
gpus O
. O

section 13
id pdf2json/2021.acl-long.449.pdf.json
we O
can O
observe O
clearly O
that O
the O
curve O
of O
the O
hierarchical O
eisner O
algorithm O
always O
stays O
far O
below O
that O
of O
the O
eisner O
algorithm O
, O
which O
verifies O
our O
theoretical O
analysis O
on O
the O
time O
complexity O
of O
the O
hierarchical O
eisner O
algorithm O
in O
section O
3.2 O
. O

section 13
id pdf2json/2021.acl-long.449.pdf.json
the O
right O
part O
of O
figure O
4 O
demonstrates O
a O
similar O
phenomenon O
where O
we O
illustrate O
the O
memory O
usage O
of O
the O
hierarchical O
and O
traditional O
eisner O
algorithms O
against O
the O
training O
document O
length O
in O
the O
same O
computing O
environment O
. O

section 13
id pdf2json/2021.acl-long.449.pdf.json
from O
the O
curves O
of O
these O
two O
figures O
we O
can O
conclude O
that O
our O
hierarchical O
eisner O
algorithm O
has O
advantage O
over O
the O
traditional O
one O
in O
both O
time O
and O
space O
efficiencies O
. O

section 13
id pdf2json/2021.acl-long.449.pdf.json
besides O
the O
superiority O
in O
computational O
efficiency O
, O
our O
experiments O
also O
indicate O
that O
our O
hierarchical O
eisner O
algorithm O
can O
achieve O
better O
performance O
than O
the O
traditional O
one O
. O

section 13
id pdf2json/2021.acl-long.449.pdf.json
with O
other O
conditions O
fixed O
, O
the O
uas O
produced O
by O
hierarchical O
eisner O
is O
79.1 O
in O
the O
task O
of O
supervised O
discourse O
parsing O
on O
the O
scidtb O
dataset O
while O
the O
corresponding O
result O
of O
the O
eisner O
algorithm O
is O
78.6 O
. O

section 14
id pdf2json/2021.acl-long.449.pdf.json
to O
explore O
the O
suitable O
number O
of O
clusters O
of O
edus O
, O
we O
evaluate O
our O
ncrfae O
model O
with O
different O
cluster O
numbers O
from O
10 O
to O
100 O
. O

section 14
id pdf2json/2021.acl-long.449.pdf.json
as O
table O
4 O
shows O
, O
there O
is O
an O
upward O
trend O
while O
the O
number O
of O
clusters O
increases O
from O
10 O
to O
50 O
. O

section 14
id pdf2json/2021.acl-long.449.pdf.json
after O
reaching O
the O
peak O
, O
the O
uas O
decreases O
as O
the O
number O
of O
cluster O
continues O
to O
increase O
. O

section 14
id pdf2json/2021.acl-long.449.pdf.json
we O
thus O
choose O
50 O
for O
our O
experiments O
. O

section 15
id pdf2json/2021.acl-long.449.pdf.json
in O
order O
to O
inspect O
if O
there O
exist O
any O
coherent O
relations O
between O
the O
clusters O
of O
edus O
obtained O
for O
§this O
is O
the O
actual O
evaluation O
result O
and O
the O
theoretical O
result O
should O
be O
0.0 O
adaptation O
in O
discourse O
parsing O
and O
the O
labels O
of O
dependency O
arcs O
, O
similar O
to O
that O
between O
pos O
tags O
and O
syntactic O
dependency O
labels O
, O
we O
compute O
the O
co-appearance O
distribution O
of O
cluster O
labels O
and O
dependency O
arc O
labels O
. O

section 15
id pdf2json/2021.acl-long.449.pdf.json
in O
figure O
5 O
, O
we O
show O
the O
probabilities O
of O
the O
clusters O
being O
used O
as O
heads O
phead O
( O
ck|rm O
) O
and O
children O
pchild O
( O
ck|rm O
) O
given O
different O
dependency O
types O
respectively O
. O

section 15
id pdf2json/2021.acl-long.449.pdf.json
here O
ck O
and O
rm O
represent O
different O
type O
of O
clusters O
and O
relations O
. O

section 15
id pdf2json/2021.acl-long.449.pdf.json
we O
cluster O
edus O
to O
10 O
clusters O
and O
only O
show O
a O
subset O
of O
them O
. O

section 15
id pdf2json/2021.acl-long.449.pdf.json
detailed O
heat-map O
can O
be O
found O
in O
appendix O
b O
. O

section 15
id pdf2json/2021.acl-long.449.pdf.json
by O
observing O
the O
two O
heat-maps O
, O
we O
notice O
obvious O
trends O
that O
for O
each O
dependency O
arc O
label O
, O
the O
co-appearance O
probabilities O
are O
concentrated O
at O
certain O
cluster O
labels O
. O

section 15
id pdf2json/2021.acl-long.449.pdf.json
for O
example O
, O
when O
the O
cluster O
is O
used O
as O
dependency O
heads O
, O
more O
than O
60 O
% O
of O
the O
co-appearance O
probability O
for O
arc O
label O
comparison O
and O
same-unit O
is O
concentrated O
at O
cluster O
type O
9 O
and O
6 O
respectively O
; O
when O
the O
cluster O
is O
used O
as O
dependency O
children O
, O
cluster O
type O
1 O
receives O
more O
than O
40 O
% O
of O
the O
co-appearance O
probability O
for O
certain O
arc O
labels O
. O

section 15
id pdf2json/2021.acl-long.449.pdf.json
the O
property O
displayed O
by O
the O
adaptation O
clusters O
is O
very O
similar O
to O
that O
of O
pos O
tags O
, O
which O
justifies O
our O
clustering O
strategy O
adopted O
for O
discourse O
parsing O
. O

section 15
id pdf2json/2021.acl-long.449.pdf.json
to O
further O
quantify O
the O
coherence O
between O
the O
adaptation O
clusters O
and O
dependency O
arcs O
, O
we O
evaluate O
the O
mutual O
information O
between O
two O
discrete O
random O
variables O
in O
the O
training O
set O
of O
scidtb O
: O
one O
is O
the O
tuple O
consists O
of O
two O
cluster O
labels O
for O
a O
pair O
of O
edus O
in O
the O
training O
sample O
, O
representing O
dependency O
head O
and O
child O
respectively O
; O
and O
the O
other O
is O
the O
binary O
random O
variable O
indicating O
whether O
there O
exists O
a O
dependency O
arc O
between O
a O
edu O
pair O
in O
the O
training O
data O
. O

section 15
id pdf2json/2021.acl-long.449.pdf.json
besides O
our O
adaptation O
clusters O
, O
we O
also O
evaluate O
this O
metric O
for O
two O
other O
clustering O
strategies O
, O
random O
clustering O
and O
nice O
proposed O
by O
he O
et O
al O
. O

section 15
id pdf2json/2021.acl-long.449.pdf.json
( O
2018 O
) O
, O
for O
comparison O
and O
show O
the O
results O
in O
table O
5 O
. O

section 15
id pdf2json/2021.acl-long.449.pdf.json
we O
see O
that O
measured O
by O
mutual O
information O
, O
clusters O
produced O
by O
our O
clustering O
strategy O
is O
much O
more O
coherent O
with O
dependencies O
than O
the O
other O
strategies O
. O

section 16
id pdf2json/2021.acl-long.449.pdf.json
in O
this O
paper O
, O
we O
propose O
a O
method O
to O
adapt O
unsupervised O
syntactic O
parsing O
methods O
for O
discourse O
dependency O
parsing O
. O

section 16
id pdf2json/2021.acl-long.449.pdf.json
first O
, O
we O
cluster O
the O
element O
discourse O
units O
( O
edu O
) O
to O
produce O
clusters O
resembling O
pos O
tags O
. O

section 16
id pdf2json/2021.acl-long.449.pdf.json
second O
, O
we O
modify O
the O
eisner O
algorithm O
used O
for O
finding O
the O
optimal O
parse O
tree O
with O
hierarchical O
constraint O
. O

section 16
id pdf2json/2021.acl-long.449.pdf.json
we O
apply O
the O
adaptations O
to O
two O
unsupervised O
syntactic O
dependency O
parsing O
methods O
. O

section 16
id pdf2json/2021.acl-long.449.pdf.json
experimental O
results O
show O
that O
our O
method O
successfully O
adapts O
the O
two O
models O
for O
discourse O
dependency O
parsing O
, O
which O
demonstrate O
advantages O
in O
both O
parsing O
accuracy O
and O
running O
efficiency O
. O

section 17
id pdf2json/2021.acl-long.449.pdf.json
this O
work O
was O
supported O
by O
the O
national O
natural O
science O
foundation O
of O
china O
( O
61976139 O
) O
. O

section 18
id pdf2json/2021.acl-long.449.pdf.json
we O
implement O
our O
ncrfae O
and O
v-dndmv O
models O
by O
pytorch O
1.6 O
and O
python O
3.8.3 O
. O

section 18
id pdf2json/2021.acl-long.449.pdf.json
we O
run O
our O
experiments O
on O
a O
server O
with O
intel O
( O
r O
) O
xeon O
( O
r O
) O
gold O
5115 O
cpu O
and O
nvidia O
titan O
v O
gpu O
. O

section 18
id pdf2json/2021.acl-long.449.pdf.json
based O
on O
these O
software O
and O
hardware O
environments O
, O
our O
ncrfae O
and O
v-dndmv O
models O
trained O
on O
the O
scidtb O
dataset O
use O
about O
30 O
and O
45 O
minutes O
, O
respectively O
. O

section 18
id pdf2json/2021.acl-long.449.pdf.json
moreover O
, O
our O
ncrfae O
and O
v-dndmv O
models O
trained O
on O
the O
rst-dt O
dataset O
use O
about O
4 O
and O
18 O
hours O
, O
respectively O
. O

section 18
id pdf2json/2021.acl-long.449.pdf.json
the O
number O
of O
parameters O
in O
ncrfae O
is O
about O
8.26 O
million O
, O
and O
the O
number O
of O
parameters O
in O
v-dndmv O
is O
0.47 O
million O
. O

section 18
id pdf2json/2021.acl-long.449.pdf.json
the O
hyperparameter O
configurations O
of O
the O
result O
report O
in O
our O
paper O
are O
shown O
in O
table O
6 O
. O

section 18
id pdf2json/2021.acl-long.449.pdf.json
we O
choose O
the O
hyperparameter O
configurations O
by O
manual O
tuning O
and O
the O
uas O
score O
on O
the O
development O
dataset O
is O
used O
to O
select O
among O
them O
. O

section 18
id pdf2json/2021.acl-long.449.pdf.json
due O
to O
the O
lack O
of O
development O
set O
of O
rst-dt O
, O
we O
prepare O
a O
development O
set O
with O
20 O
instances O
randomly O
sampled O
from O
the O
training O
set O
. O

section 18
id pdf2json/2021.acl-long.449.pdf.json
the O
size O
of O
each O
dataset O
is O
shown O
in O
table O
7 O
. O

section 18
id pdf2json/2021.acl-long.449.pdf.json
b O
full O
heat-maps O

section TITLE
id pdf2json/2021.acl-long.135.pdf.json
a O
sequence-to-sequence O
approach O
to O
dialogue O
state O
tracking O

section ABSTRACT
id pdf2json/2021.acl-long.135.pdf.json
this O
paper O
is O
concerned O
with O
dialogue O
state O
tracking O
( O
dst O
) O
in O
a O
task-oriented O
dialogue O
system O
. O

section ABSTRACT
id pdf2json/2021.acl-long.135.pdf.json
building O
a O
dst O
module O
that O
is O
highly O
effective O
is O
still O
a O
challenging O
issue O
, O
although O
significant O
progresses O
have O
been O
made O
recently O
. O

section ABSTRACT
id pdf2json/2021.acl-long.135.pdf.json
this O
paper O
proposes O
a O
new O
approach O
to O
dialogue O
state O
tracking O
, O
referred O
to O
as O
seq2seqdu O
, O
which O
formalizes O
dst O
as O
a O
sequence-tosequence O
problem O
. O

section ABSTRACT
id pdf2json/2021.acl-long.135.pdf.json
seq2seq-du O
employs O
two O
bert-based O
encoders O
to O
respectively O
encode O
the O
utterances O
in O
the O
dialogue O
and O
the O
descriptions O
of O
schemas O
, O
an O
attender O
to O
calculate O
attentions O
between O
the O
utterance O
embeddings O
and O
the O
schema O
embeddings O
, O
and O
a O
decoder O
to O
generate O
pointers O
to O
represent O
the O
current O
state O
of O
dialogue O
. O

section ABSTRACT
id pdf2json/2021.acl-long.135.pdf.json
seq2seq-du O
has O
the O
following O
advantages O
. O

section ABSTRACT
id pdf2json/2021.acl-long.135.pdf.json
it O
can O
jointly O
model O
intents O
, O
slots O
, O
and O
slot O
values O
; O
it O
can O
leverage O
the O
rich O
representations O
of O
utterances O
and O
schemas O
based O
on O
bert O
; O
it O
can O
effectively O
deal O
with O
categorical O
and O
non-categorical O
slots O
, O
and O
unseen O
schemas O
. O

section ABSTRACT
id pdf2json/2021.acl-long.135.pdf.json
in O
addition O
, O
seq2seq-du O
can O
also O
be O
used O
in O
the O
nlu O
( O
natural O
language O
understanding O
) O
module O
of O
a O
dialogue O
system O
. O

section ABSTRACT
id pdf2json/2021.acl-long.135.pdf.json
experimental O
results O
on O
benchmark O
datasets O
in O
different O
settings O
( O
sgd O
, O
multiwoz2.2 O
, O
multiwoz2.1 O
, O
woz2.0 O
, O
dstc2 O
, O
m2m O
, O
snips O
, O
and O
atis O
) O
show O
that O
seq2seq-du O
outperforms O
the O
existing O
methods O
. O

section 0
id pdf2json/2021.acl-long.135.pdf.json
proceedings O
of O
the O
59th O
annual O
meeting O
of O
the O
association O
for O
computational O
linguistics O
and O
the O
11th O
international O
joint O
conference O
on O
natural O
language O
processing O
, O
pages O
1714–1725 O
august O
1–6 O
, O
2021 O
. O

section 0
id pdf2json/2021.acl-long.135.pdf.json
©2021 O
association O
for O
computational O
linguistics O
1714 O

section 1
id pdf2json/2021.acl-long.135.pdf.json
a O
task-oriented O
dialogue O
system O
usually O
consists O
of O
several O
modules O
: O
natural O
language O
understanding O
( O
nlu O
) O
, O
dialogue O
state O
tracking O
( O
dst O
) O
, O
dialogue O
policy O
( O
policy O
) O
, O
and O
natural O
language O
generation O
( O
nlg O
) O
. O

section 1
id pdf2json/2021.acl-long.135.pdf.json
we O
consider O
dst O
and O
also O
nlu O
in O
this O
paper O
. O

section 1
id pdf2json/2021.acl-long.135.pdf.json
in O
nlu O
, O
a O
semantic O
frame O
representing O
the O
content O
of O
user O
utterance O
is O
created O
in O
each O
turn O
∗the O
work O
was O
done O
when O
the O
first O
author O
was O
an O
intern O
at O
bytedance O
ai O
lab O
. O

section 1
id pdf2json/2021.acl-long.135.pdf.json
of O
dialogue O
. O

section 1
id pdf2json/2021.acl-long.135.pdf.json
in O
dst O
, O
several O
semantic O
frames O
representing O
the O
‘ O
states O
’ O
of O
dialogue O
are O
created O
and O
updated O
in O
multiple O
turns O
of O
dialogue O
. O

section 1
id pdf2json/2021.acl-long.135.pdf.json
domain O
knowledge O
in O
dialogues O
is O
represented O
by O
a O
representation O
referred O
to O
as O
schema O
, O
which O
consists O
of O
possible O
intents O
, O
slots O
, O
and O
slot O
values O
. O

section 1
id pdf2json/2021.acl-long.135.pdf.json
slot O
values O
can O
be O
in O
a O
pre-defined O
set O
, O
with O
the O
corresponding O
slot O
being O
referred O
to O
as O
categorical O
slot O
, O
and O
they O
can O
also O
be O
from O
an O
open O
set O
, O
with O
the O
corresponding O
slot O
being O
referred O
to O
as O
non-categorical O
slot O
. O

section 1
id pdf2json/2021.acl-long.135.pdf.json
figure O
1 O
shows O
an O
example O
of O
dst O
. O

section 1
id pdf2json/2021.acl-long.135.pdf.json
we O
think O
that O
a O
dst O
module O
( O
and O
an O
nlu O
module O
) O
should O
have O
the O
following O
abilities O
. O

section 1
id pdf2json/2021.acl-long.135.pdf.json
( O
1 O
) O
global O
, O
the O
model O
can O
jointly O
represent O
intents O
, O
slots O
, O
and O
slot O
values O
. O

section 1
id pdf2json/2021.acl-long.135.pdf.json
( O
2 O
) O
represenable O
, O
it O
has O
strong O
capa- O
bility O
to O
represent O
knowledge O
for O
the O
task O
, O
on O
top O
of O
a O
pre-trained O
language O
model O
like O
bert O
. O

section 1
id pdf2json/2021.acl-long.135.pdf.json
( O
3 O
) O
scalable O
, O
the O
model O
can O
deal O
with O
categorical O
and O
non-categorical O
slots O
and O
unseen O
schemas O
. O

section 1
id pdf2json/2021.acl-long.135.pdf.json
many O
methods O
have O
been O
proposed O
for O
dst O
( O
wu O
et O
al. O
, O
2019 O
; O
zhong O
et O
al. O
, O
2018 O
; O
mrkšić O
et O
al. O
, O
2017 O
; O
goo O
et O
al. O
, O
2018 O
) O
. O

section 1
id pdf2json/2021.acl-long.135.pdf.json
there O
are O
two O
lines O
of O
relevant O
research O
. O

section 1
id pdf2json/2021.acl-long.135.pdf.json
( O
1 O
) O
to O
enhance O
the O
scalability O
of O
dst O
, O
a O
problem O
formulation O
, O
referred O
to O
as O
schemaguided O
dialogue O
, O
is O
proposed O
. O

section 1
id pdf2json/2021.acl-long.135.pdf.json
in O
the O
setting O
, O
it O
is O
assumed O
that O
descriptions O
on O
schemas O
in O
natural O
language O
across O
multiple O
domains O
are O
given O
and O
utilized O
. O

section 1
id pdf2json/2021.acl-long.135.pdf.json
consequently O
, O
a O
number O
of O
methods O
are O
developed O
to O
make O
use O
of O
schema O
descriptions O
to O
increase O
the O
scalability O
of O
dst O
( O
rastogi O
et O
al. O
, O
2019 O
; O
zang O
et O
al. O
, O
2020 O
; O
noroozi O
et O
al. O
, O
2020 O
) O
. O

section 1
id pdf2json/2021.acl-long.135.pdf.json
the O
methods O
regard O
dst O
as O
a O
classification O
and/or O
an O
extraction O
problem O
and O
independently O
infer O
the O
intent O
and O
slot O
value O
pairs O
for O
the O
current O
turn O
. O

section 1
id pdf2json/2021.acl-long.135.pdf.json
therefore O
, O
the O
proposed O
models O
are O
generally O
representable O
and O
scalable O
, O
but O
not O
global O
. O

section 1
id pdf2json/2021.acl-long.135.pdf.json
( O
2 O
) O
there O
are O
also O
a O
few O
methods O
which O
view O
dst O
as O
a O
sequence O
to O
sequence O
problem O
. O

section 1
id pdf2json/2021.acl-long.135.pdf.json
some O
methods O
sequentially O
infer O
the O
intent O
and O
slot O
value O
pairs O
for O
the O
current O
turn O
on O
the O
basis O
of O
dialogue O
history O
and O
usually O
employ O
a O
hierarchical O
structure O
( O
not O
based O
on O
bert O
) O
for O
the O
inference O
( O
lei O
et O
al. O
, O
2018 O
; O
ren O
et O
al. O
, O
2019 O
; O
chen O
et O
al. O
, O
2020b O
) O
. O

section 1
id pdf2json/2021.acl-long.135.pdf.json
recently O
, O
a O
new O
approach O
is O
proposed O
which O
formalizes O
the O
tasks O
in O
dialogue O
as O
sequence O
prediction O
problems O
using O
a O
unified O
language O
model O
( O
based O
on O
gpt-2 O
) O
( O
hosseini-asl O
et O
al. O
, O
2020 O
) O
. O

section 1
id pdf2json/2021.acl-long.135.pdf.json
the O
method O
can O
not O
deal O
with O
unseen O
schemas O
and O
intents O
, O
however O
, O
and O
thus O
is O
not O
scalable O
. O

section 1
id pdf2json/2021.acl-long.135.pdf.json
we O
propose O
a O
novel O
approach O
to O
dst O
, O
referred O
to O
as O
seq2seq-du O
( O
sequence-to-sequence O
for O
dialogue O
understanding O
) O
, O
which O
combines O
the O
advantages O
of O
the O
existing O
approaches O
. O

section 1
id pdf2json/2021.acl-long.135.pdf.json
to O
the O
best O
of O
our O
knowledge O
, O
there O
was O
no O
previous O
work O
which O
studied O
the O
approach O
. O

section 1
id pdf2json/2021.acl-long.135.pdf.json
we O
think O
that O
dst O
should O
be O
formalized O
as O
a O
sequence O
to O
sequence O
or O
‘ O
translation O
’ O
problem O
in O
which O
the O
utterances O
in O
the O
dialogue O
are O
transformed O
into O
semantic O
frames O
. O

section 1
id pdf2json/2021.acl-long.135.pdf.json
in O
this O
way O
, O
the O
intents O
, O
slots O
, O
and O
slot O
values O
can O
be O
jointly O
modeled O
. O

section 1
id pdf2json/2021.acl-long.135.pdf.json
moreover O
, O
nlu O
can O
also O
be O
viewed O
as O
a O
special O
case O
of O
dst O
and O
thus O
seq2seq-du O
can O
also O
be O
applied O
to O
nlu O
. O

section 1
id pdf2json/2021.acl-long.135.pdf.json
we O
note O
that O
very O
recently O
the O
effectiveness O
of O
the O
sequence O
to O
sequence O
approach O
has O
also O
been O
verified O
in O
other O
language O
understanding O
tasks O
( O
paolini O
et O
al. O
, O
2021 O
) O
. O

section 1
id pdf2json/2021.acl-long.135.pdf.json
seq2seq-du O
comprises O
a O
bert-based O
encoder O
to O
encode O
the O
utterances O
in O
the O
dialogue O
, O
a O
bert O
based O
encoder O
to O
encode O
the O
schema O
descriptions O
, O
an O
attender O
to O
calculate O
attentions O
between O
the O
utterance O
embeddings O
and O
schema O
embeddings O
, O
and O
a O
decoder O
to O
generate O
pointers O
of O
items O
representing O
the O
intents O
and O
slots-value O
pairs O
of O
state O
. O

section 1
id pdf2json/2021.acl-long.135.pdf.json
seq2seq-du O
has O
the O
following O
advantages O
. O

section 1
id pdf2json/2021.acl-long.135.pdf.json
( O
1 O
) O
global O
: O
it O
relies O
on O
the O
sequence O
to O
sequence O
framework O
to O
simultaneously O
model O
the O
intents O
, O
slots O
, O
and O
slot-values O
. O

section 1
id pdf2json/2021.acl-long.135.pdf.json
( O
2 O
) O
representable O
: O
it O
employs O
bert O
( O
devlin O
et O
al. O
, O
2019 O
) O
to O
learn O
and O
utilize O
better O
representations O
of O
not O
only O
the O
current O
utterance O
but O
also O
the O
previous O
utterances O
in O
the O
dialogue O
. O

section 1
id pdf2json/2021.acl-long.135.pdf.json
if O
schema O
descriptions O
are O
available O
, O
it O
also O
employs O
bert O
for O
the O
learning O
and O
utilization O
of O
their O
representations O
. O

section 1
id pdf2json/2021.acl-long.135.pdf.json
( O
3 O
) O
scalable O
: O
it O
uses O
the O
pointer O
generation O
mechanism O
, O
as O
in O
the O
pointer O
network O
( O
vinyals O
et O
al. O
, O
2015 O
) O
, O
to O
create O
representations O
of O
intents O
, O
slots O
, O
and O
slot-values O
, O
no O
matter O
whether O
the O
slots O
are O
categorical O
or O
non-categorical O
, O
and O
whether O
the O
schemas O
are O
unseen O
or O
not O
. O

section 1
id pdf2json/2021.acl-long.135.pdf.json
experimental O
results O
on O
benchmark O
datasets O
show O
that O
seq2seq-du1 O
performs O
much O
better O
than O
the O
baselines O
on O
sgd O
, O
multiwoz2.2 O
, O
and O
multiwoz2.1 O
in O
multi-turn O
dialogue O
with O
schema O
descriptions O
, O
is O
superior O
to O
bert-dst O
on O
woz2.0 O
, O
dstc2 O
, O
and O
m2m O
, O
in O
multi-turn O
dialogue O
without O
schema O
descriptions O
, O
and O
works O
equally O
well O
as O
joint O
bert O
on O
atis O
and O
snips O
in O
single O
turn O
dialogue O
( O
in O
fact O
, O
it O
degenerates O
to O
joint O
bert O
) O
. O

section 2
id pdf2json/2021.acl-long.135.pdf.json
there O
has O
been O
a O
large O
amount O
of O
work O
on O
task-oriented O
dialogue O
, O
especially O
dialogue O
state O
tracking O
and O
natural O
language O
understanding O
( O
eg. O
, O
( O
zhang O
et O
al. O
, O
2020 O
; O
huang O
et O
al. O
, O
2020 O
; O
chen O
et O
al. O
, O
2017 O
) O
) O
. O

section 2
id pdf2json/2021.acl-long.135.pdf.json
table O
1 O
makes O
a O
summary O
of O
existing O
methods O
on O
dst O
. O

section 2
id pdf2json/2021.acl-long.135.pdf.json
we O
also O
indicate O
the O
methods O
on O
which O
we O
make O
comparison O
in O
our O
experiments O
. O

section 3
id pdf2json/2021.acl-long.135.pdf.json
previous O
approaches O
mainly O
focus O
on O
encoding O
of O
the O
dialogue O
context O
and O
employ O
deep O
neural O
networks O
such O
as O
cnn O
, O
rnn O
, O
and O
lstmrnn O
to O
independently O
infer O
the O
values O
of O
slots O
in O
dst O
( O
mrkšić O
et O
al. O
, O
2017 O
; O
xu O
and O
hu O
, O
2018 O
; O
zhong O
et O
al. O
, O
2018 O
; O
ren O
et O
al. O
, O
2018 O
; O
rastogi O
et O
al. O
, O
2017 O
; O
ramadan O
et O
al. O
, O
2018 O
; O
wu O
et O
al. O
, O
2019 O
; O
zhang O
et O
al. O
, O
2019 O
; O
heck O
et O
al. O
, O
2020 O
) O
. O

section 3
id pdf2json/2021.acl-long.135.pdf.json
the O
approaches O
1the O
code O
is O
available O
at O
https O
: O
//github.com/ O
sweetalyssum/seq2seq-du O
. O

section 3
id pdf2json/2021.acl-long.135.pdf.json
can O
not O
deal O
with O
unseen O
schemas O
in O
new O
domains O
, O
however O
. O

section 3
id pdf2json/2021.acl-long.135.pdf.json
to O
cope O
with O
the O
problem O
, O
a O
new O
direction O
called O
schema-guided O
dialogue O
is O
proposed O
recently O
, O
which O
assumes O
that O
natural O
language O
descriptions O
of O
schemas O
are O
provided O
and O
can O
be O
used O
to O
help O
transfer O
knowledge O
across O
domains O
. O

section 3
id pdf2json/2021.acl-long.135.pdf.json
as O
such O
, O
a O
number O
of O
methods O
are O
developed O
in O
the O
recent O
dialogue O
competition O
sgd O
( O
rastogi O
et O
al. O
, O
2019 O
; O
zang O
et O
al. O
, O
2020 O
; O
noroozi O
et O
al. O
, O
2020 O
; O
chen O
et O
al. O
, O
2020a O
) O
. O

section 3
id pdf2json/2021.acl-long.135.pdf.json
our O
work O
is O
partially O
motivated O
by O
the O
sgd O
initiative O
. O

section 3
id pdf2json/2021.acl-long.135.pdf.json
our O
model O
seq2seq-du O
is O
unique O
in O
that O
it O
formalizes O
schema-guided O
dst O
as O
a O
sequence-to-sequence O
problem O
using O
bert O
and O
pointer O
generation O
. O

section 3
id pdf2json/2021.acl-long.135.pdf.json
in O
fact O
, O
sequence-to-sequence O
models O
are O
also O
utilized O
in O
dst O
. O

section 3
id pdf2json/2021.acl-long.135.pdf.json
sequicity O
( O
lei O
et O
al. O
, O
2018 O
) O
is O
a O
two-step O
sequence O
to O
sequence O
model O
which O
first O
encodes O
the O
dialogue O
history O
and O
generates O
a O
belief O
span O
, O
and O
then O
generates O
a O
language O
response O
from O
the O
belief O
span O
. O

section 3
id pdf2json/2021.acl-long.135.pdf.json
comer O
( O
ren O
et O
al. O
, O
2019 O
) O
and O
credit O
( O
chen O
et O
al. O
, O
2020b O
) O
are O
hierarchical O
sequence-to-sequence O
models O
which O
represent O
the O
intents O
and O
slot-value O
pairs O
in O
a O
hierarchical O
way O
, O
and O
employ O
a O
multi-stage O
decoder O
. O

section 3
id pdf2json/2021.acl-long.135.pdf.json
simpletod O
( O
hosseini-asl O
et O
al. O
, O
2020 O
) O
is O
a O
unified O
approach O
to O
task-oriented O
dialogue O
which O
employs O
a O
single O
and O
causal O
language O
model O
to O
perform O
sequence O
prediction O
in O
dst O
, O
policy O
, O
and O
nlg O
. O

section 3
id pdf2json/2021.acl-long.135.pdf.json
our O
proposed O
approach O
also O
uses O
a O
sequence-tosequence O
model O
. O

section 3
id pdf2json/2021.acl-long.135.pdf.json
there O
are O
significant O
differences O
between O
our O
model O
seq2seq-du O
and O
the O
existing O
models O
. O

section 3
id pdf2json/2021.acl-long.135.pdf.json
first O
, O
there O
is O
no O
hierarchy O
in O
decoding O
of O
seq2seq-du O
. O

section 3
id pdf2json/2021.acl-long.135.pdf.json
a O
flat O
structure O
on O
top O
of O
bert O
appears O
to O
be O
sufficient O
for O
jointly O
capturing O
the O
intents O
, O
slots O
, O
and O
values O
. O

section 3
id pdf2json/2021.acl-long.135.pdf.json
second O
, O
the O
decoder O
in O
seq2seq-du O
generates O
pointers O
instead O
of O
tokens O
, O
and O
thus O
can O
easily O
and O
effectively O
handle O
categorical O
slots O
, O
non-categorical O
slots O
, O
as O
well O
as O
unseen O
schemas O
. O

section 4
id pdf2json/2021.acl-long.135.pdf.json
traditionally O
the O
problem O
of O
nlu O
is O
decomposed O
into O
two O
independent O
issues O
, O
namely O
classification O
of O
intents O
and O
sequence O
labeling O
of O
slot-value O
pairs O
( O
liu O
and O
lane O
, O
2016 O
; O
hakkani-tür O
et O
al. O
, O
2016 O
) O
. O

section 4
id pdf2json/2021.acl-long.135.pdf.json
for O
example O
, O
deep O
neural O
network O
combined O
with O
conditional O
random O
field O
is O
employed O
for O
the O
task O
( O
yao O
et O
al. O
, O
2014 O
) O
. O

section 4
id pdf2json/2021.acl-long.135.pdf.json
recently O
the O
pretrained O
language O
model O
bert O
( O
chen O
et O
al. O
, O
2019 O
) O
is O
exploited O
to O
further O
enhance O
the O
accuracy O
. O

section 4
id pdf2json/2021.acl-long.135.pdf.json
methods O
are O
also O
proposed O
which O
can O
jointly O
train O
and O
utilize O
classification O
and O
sequence O
labeling O
models O
( O
chen O
et O
al. O
, O
2019 O
; O
goo O
et O
al. O
, O
2018 O
) O
. O

section 4
id pdf2json/2021.acl-long.135.pdf.json
in O
this O
paper O
, O
we O
view O
nlu O
as O
special O
case O
of O
dst O
and O
employ O
our O
model O
seq2seq-du O
to O
perform O
nlu O
. O

section 4
id pdf2json/2021.acl-long.135.pdf.json
seq2seq-du O
can O
degenerate O
to O
a O
bert O
based O
nlu O
model O
. O

section 5
id pdf2json/2021.acl-long.135.pdf.json
our O
approach O
seq2seq-du O
formalizes O
dialogue O
state O
tracking O
as O
a O
sequence O
to O
sequence O
problem O
using O
bert O
and O
pointer O
generation O
. O

section 5
id pdf2json/2021.acl-long.135.pdf.json
as O
shown O
in O
figure O
2 O
, O
seq2seq-du O
consists O
of O
an O
utterance O
encoder O
, O
a O
schema O
encoder O
, O
an O
utterance O
schema O
attender O
, O
and O
a O
state O
decoder O
. O

section 5
id pdf2json/2021.acl-long.135.pdf.json
in O
each O
turn O
of O
dialogue O
, O
the O
utterance O
encoder O
transforms O
the O
current O
user O
utterance O
and O
the O
previous O
utterances O
in O
the O
dialogue O
into O
a O
sequence O
of O
utterance O
embeddings O
using O
bert O
; O
the O
schema O
encoder O
transforms O
the O
schema O
descriptions O
into O
a O
set O
of O
schema O
embeddings O
also O
using O
bert O
; O
the O
utterance O
schema O
attender O
calculates O
attentions O
between O
the O
utterance O
embeddings O
and O
the O
schema O
embeddings O
to O
create O
attended O
utterance O
and O
schema O
representations O
; O
finally O
, O
the O
state O
decoder O
sequentially O
generates O
a O
state O
representation O
on O
the O
basis O
of O
the O
attended O
representations O
using O
lstm O
and O
pointer O
generation O
. O

section 6
id pdf2json/2021.acl-long.135.pdf.json
the O
utterance O
encoder O
takes O
the O
current O
user O
utterance O
as O
well O
as O
the O
previous O
utterances O
( O
user O
and O
system O
utterances O
) O
in O
the O
dialogue O
( O
a O
sequence O
of O
tokens O
) O
as O
input O
and O
employs O
bert O
to O
construct O
a O
sequence O
of O
utterance O
embeddings O
. O

section 6
id pdf2json/2021.acl-long.135.pdf.json
the O
relations O
between O
the O
current O
utterance O
and O
the O
previous O
utterances O
are O
captured O
by O
the O
encoder O
. O

section 6
id pdf2json/2021.acl-long.135.pdf.json
the O
input O
of O
the O
encoder O
is O
a O
sequence O
of O
tokens O
with O
length O
n O
, O
denoted O
as O
x O
= O
( O
x1 O
, O
... O
, O
xn O
) O
. O

section 6
id pdf2json/2021.acl-long.135.pdf.json
the O
first O
token O
x1 O
is O
[ O
cls O
] O
, O
followed O
by O
the O
tokens O
of O
the O
current O
user O
utterance O
and O
the O
tokens O
of O
the O
previous O
utterances O
, O
separated O
by O
[ O
sep O
] O
. O

section 6
id pdf2json/2021.acl-long.135.pdf.json
the O
output O
is O
a O
sequence O
of O
embeddings O
also O
with O
length O
n O
, O
denoted O
as O
d O
= O
( O
d1 O
, O
... O
, O
dn O
) O
and O
referred O
to O
as O
utterance O
embeddings O
, O
with O
one O
embedding O
for O
each O
token O
. O

section 7
id pdf2json/2021.acl-long.135.pdf.json
the O
schema O
encoder O
takes O
the O
descriptions O
of O
intents O
, O
slots O
, O
and O
categorical O
slot O
values O
( O
a O
set O
of O
combined O
sequences O
of O
tokens O
) O
as O
input O
and O
employs O
bert O
to O
construct O
a O
set O
of O
schema O
embeddings O
. O

section 7
id pdf2json/2021.acl-long.135.pdf.json
suppose O
that O
there O
are O
i O
intents O
, O
s O
slots O
, O
and O
v O
categorical O
slot O
values O
in O
the O
schemas O
. O

section 7
id pdf2json/2021.acl-long.135.pdf.json
each O
schema O
element O
is O
described O
by O
two O
descriptions O
as O
outlined O
in O
table O
2 O
. O

section 7
id pdf2json/2021.acl-long.135.pdf.json
the O
input O
is O
a O
set O
of O
combined O
sequences O
of O
tokens O
, O
denoted O
as O
y O
= O
{ O
y1 O
, O
... O
, O
ym O
} O
. O

section 7
id pdf2json/2021.acl-long.135.pdf.json
note O
that O
m O
= O
i O
+ O
s O
+ O
v O
. O

section 7
id pdf2json/2021.acl-long.135.pdf.json
each O
combined O
sequence O
starts O
with O
[ O
cls O
] O
, O
followed O
by O
the O
tokens O
of O
the O
two O
descriptions O
with O
[ O
sep O
] O
as O
a O
separator O
. O

section 7
id pdf2json/2021.acl-long.135.pdf.json
the O
final O
representation O
of O
[ O
cls O
] O
is O
used O
as O
the O
embedding O
of O
the O
input O
intent O
, O
slot O
, O
or O
slot O
value O
. O

section 7
id pdf2json/2021.acl-long.135.pdf.json
the O
output O
is O
a O
set O
of O
embeddings O
, O
and O
all O
the O
embeddings O
are O
called O
schema O
embeddings O
e O
= O
{ O
e1 O
, O
... O
, O
em O
} O
. O

section 7
id pdf2json/2021.acl-long.135.pdf.json
the O
schema O
encoder O
in O
fact O
adopts O
the O
same O
approach O
of O
schema O
encoding O
as O
in O
( O
rastogi O
et O
al. O
, O
2019 O
) O
. O

section 7
id pdf2json/2021.acl-long.135.pdf.json
there O
are O
two O
advantages O
with O
the O
approach O
. O

section 7
id pdf2json/2021.acl-long.135.pdf.json
first O
, O
the O
encoder O
can O
be O
trained O
across O
different O
domains O
. O

section 7
id pdf2json/2021.acl-long.135.pdf.json
schema O
descriptions O
in O
different O
domains O
can O
be O
utilized O
together O
. O

section 7
id pdf2json/2021.acl-long.135.pdf.json
second O
, O
once O
the O
encoder O
is O
fine-tuned O
, O
it O
can O
be O
used O
to O
process O
unseen O
schemas O
with O
new O
intents O
, O
slots O
, O
and O
slot O
values O
. O

section 8
id pdf2json/2021.acl-long.135.pdf.json
the O
utterance-schema O
attender O
takes O
the O
sequence O
of O
utterance O
embeddings O
and O
the O
set O
of O
schema O
embeddings O
as O
input O
and O
calculates O
schema-attended O
utterance O
representations O
and O
utterance-attended O
schema O
representations O
. O

section 8
id pdf2json/2021.acl-long.135.pdf.json
in O
this O
way O
, O
information O
from O
the O
utterances O
and O
information O
from O
the O
schemas O
are O
fused O
. O

section 8
id pdf2json/2021.acl-long.135.pdf.json
first O
, O
the O
attender O
constructs O
an O
attention O
matrix O
, O
indicating O
the O
similarities O
between O
utterance O
embeddings O
and O
schema O
embeddings O
. O

section 8
id pdf2json/2021.acl-long.135.pdf.json
given O
the O
i-th O
utterance O
token O
embedding O
di O
and O
j-th O
schema O
embedding O
ej O
, O
it O
calculates O
the O
similarity O
as O
follows O
, O
a O
( O
i O
, O
j O
) O
= O
rᵀtanh O
( O
w1di O
+w2ej O
) O
, O
( O
1 O
) O
where O
r O
, O
w1 O
, O
w2 O
are O
trainable O
parameters O
. O

section 8
id pdf2json/2021.acl-long.135.pdf.json
the O
attender O
then O
normalizes O
each O
row O
of O
matrix O
a O
as O
a O
probability O
distribution O
, O
to O
obtain O
matrix O
a O
. O

section 8
id pdf2json/2021.acl-long.135.pdf.json
each O
row O
represents O
the O
attention O
weights O
of O
schema O
elements O
with O
respect O
to O
an O
utterance O
token O
. O

section 8
id pdf2json/2021.acl-long.135.pdf.json
then O
the O
schema-attended O
utterance O
representations O
are O
calculated O
as O
da O
= O
ea O
ᵀ O
. O

section 8
id pdf2json/2021.acl-long.135.pdf.json
the O
attender O
also O
normalizes O
each O
column O
of O
matrix O
a O
as O
a O
probability O
distribution O
, O
to O
obtain O
matrix O
ã O
. O

section 8
id pdf2json/2021.acl-long.135.pdf.json
each O
column O
represents O
the O
attention O
weights O
of O
utterance O
tokens O
with O
respect O
to O
a O
schema O
element O
. O

section 8
id pdf2json/2021.acl-long.135.pdf.json
then O
the O
utterance-attended O
schema O
representations O
are O
calculated O
as O
ea O
= O
dã O
. O

section 9
id pdf2json/2021.acl-long.135.pdf.json
the O
state O
decoder O
sequentially O
generates O
a O
state O
representation O
( O
semantic O
frame O
) O
for O
the O
current O
turn O
, O
which O
is O
represented O
as O
a O
sequence O
of O
pointers O
to O
elements O
of O
the O
schemas O
and O
tokens O
of O
the O
utterances O
( O
cf. O
, O
figure O
1 O
) O
. O

section 9
id pdf2json/2021.acl-long.135.pdf.json
the O
sequence O
can O
then O
be O
either O
re-formalized O
as O
a O
semantic O
frame O
in O
dialogue O
state O
tracking2 O
, O
[ O
intent O
; O
( O
slot1 O
, O
value1 O
) O
; O
( O
slot2 O
, O
value2 O
) O
; O
... O
] O
, O
2for O
simplicity O
, O
we O
assume O
here O
that O
there O
is O
only O
one O
semantic O
frame O
in O
each O
turn O
. O

section 9
id pdf2json/2021.acl-long.135.pdf.json
in O
principle O
, O
there O
can O
be O
multiple O
frames O
. O

section 9
id pdf2json/2021.acl-long.135.pdf.json
or O
a O
sequence O
of O
labels O
in O
nlu O
( O
intent-labeling O
and O
slot-filling O
) O
. O

section 9
id pdf2json/2021.acl-long.135.pdf.json
the O
pointers O
point O
to O
the O
elements O
of O
intents O
, O
slots O
, O
and O
slot O
values O
in O
the O
schema O
descriptions O
( O
categorical O
slot O
values O
) O
, O
as O
well O
as O
the O
tokens O
in O
the O
utterances O
( O
non-categorical O
slot O
values O
) O
. O

section 9
id pdf2json/2021.acl-long.135.pdf.json
the O
elements O
in O
the O
schemas O
can O
be O
either O
words O
or O
phrases O
, O
and O
the O
tokens O
in O
the O
utterances O
form O
spans O
for O
extraction O
of O
slot O
values O
. O

section 9
id pdf2json/2021.acl-long.135.pdf.json
the O
state O
decoder O
is O
an O
lstm O
using O
pointer O
( O
vinyals O
et O
al. O
, O
2015 O
) O
and O
attention O
( O
bahdanau O
et O
al. O
, O
2015 O
) O
. O

section 9
id pdf2json/2021.acl-long.135.pdf.json
it O
takes O
the O
two O
representations O
da O
and O
ea O
as O
input O
. O

section 9
id pdf2json/2021.acl-long.135.pdf.json
at O
each O
decode O
step O
t O
, O
the O
decoder O
receives O
the O
embedding O
of O
the O
previous O
item O
wt−1 O
, O
the O
utterance O
context O
vector O
ut O
, O
the O
schema O
context O
vector O
st O
, O
and O
the O
previous O
hidden O
state O
ht−1 O
, O
and O
produces O
the O
current O
hidden O
state O
ht O
: O
ht O
= O
lstm O
( O
wt−1 O
, O
ht−1 O
, O
ut O
, O
st O
) O
. O

section 9
id pdf2json/2021.acl-long.135.pdf.json
( O
2 O
) O
we O
adopt O
the O
attention O
function O
in O
( O
bahdanau O
et O
al. O
, O
2015 O
) O
to O
calculate O
the O
context O
vectors O
as O
follows O
, O
ut O
= O
attend O
( O
ht−1 O
, O
da O
, O
da O
) O
, O
( O
3 O
) O
st O
= O
attend O
( O
ht−1 O
, O
ea O
, O
ea O
) O
. O

section 9
id pdf2json/2021.acl-long.135.pdf.json
( O
4 O
) O
the O
decoder O
then O
generates O
a O
pointer O
from O
the O
set O
of O
pointers O
in O
the O
schema O
elements O
and O
the O
tokens O
of O
the O
utterances O
on O
the O
basis O
of O
the O
hidden O
state O
ht O
. O

section 9
id pdf2json/2021.acl-long.135.pdf.json
specifically O
, O
it O
generates O
a O
pointer O
of O
item O
w O
according O
to O
the O
following O
distribution O
, O
zw O
= O
q O
ᵀtanh O
( O
u1ht O
+ O
u2kw O
) O
, O
( O
5 O
) O
p O
( O
# O
w O
) O
= O
softmax O
( O
zw O
) O
, O
( O
6 O
) O
where O
# O
w O
is O
the O
pointer O
of O
item O
w O
, O
kw O
is O
the O
representation O
of O
item O
w O
either O
in O
the O
utterance O
representations O
da O
or O
in O
the O
schema O
representations O
ea O
, O
q O
, O
u1 O
, O
and O
u2 O
are O
trainable O
parameters O
, O
and O
softmax O
is O
calculated O
over O
all O
possible O
pointers O
. O

section 9
id pdf2json/2021.acl-long.135.pdf.json
during O
decoding O
, O
the O
decoder O
employs O
beam O
search O
to O
find O
the O
best O
sequences O
of O
pointers O
in O
terms O
of O
probability O
of O
sequence O
. O

section 10
id pdf2json/2021.acl-long.135.pdf.json
the O
training O
of O
seq2seq-du O
follows O
the O
standard O
procedure O
of O
sequence-to-sequence O
. O

section 10
id pdf2json/2021.acl-long.135.pdf.json
the O
only O
difference O
is O
that O
it O
is O
always O
conditioned O
on O
the O
schema O
descriptions O
. O

section 10
id pdf2json/2021.acl-long.135.pdf.json
each O
instance O
in O
training O
consists O
of O
the O
current O
utterance O
and O
the O
previous O
utterances O
, O
and O
the O
state O
representation O
( O
sequence O
of O
pointers O
) O
for O
the O
current O
turn O
. O

section 10
id pdf2json/2021.acl-long.135.pdf.json
two O
pre-trained O
bert O
models O
are O
used O
for O
representations O
of O
utterances O
and O
schema O
descriptions O
respectively O
. O

section 10
id pdf2json/2021.acl-long.135.pdf.json
the O
bert O
models O
are O
then O
fine-tuned O
in O
the O
training O
process O
. O

section 10
id pdf2json/2021.acl-long.135.pdf.json
cross-entropy O
loss O
is O
utilized O
to O
measure O
the O
loss O
of O
generating O
a O
sequence O
. O

section 12
id pdf2json/2021.acl-long.135.pdf.json
we O
conduct O
experiments O
using O
the O
benchmark O
datasets O
on O
task-oriented O
dialogue O
. O

section 12
id pdf2json/2021.acl-long.135.pdf.json
sgd O
( O
rastogi O
et O
al. O
, O
2019 O
) O
and O
multiwoz2.2 O
( O
zang O
et O
al. O
, O
2020 O
) O
are O
datasets O
for O
dst O
; O
they O
include O
schemas O
with O
categorical O
slots O
and O
non-categorical O
slots O
in O
multiple O
domains O
and O
natural O
language O
descriptions O
on O
the O
schemas O
, O
as O
shown O
in O
table O
2 O
. O

section 12
id pdf2json/2021.acl-long.135.pdf.json
in O
particular O
, O
sgd O
includes O
unseen O
schemas O
in O
the O
test O
set O
. O

section 12
id pdf2json/2021.acl-long.135.pdf.json
multiwoz2.1 O
( O
eric O
et O
al. O
, O
2020 O
) O
is O
the O
previous O
version O
of O
multiwoz2.2 O
, O
which O
only O
has O
categorical O
slots O
in O
multiple O
domains O
. O

section 12
id pdf2json/2021.acl-long.135.pdf.json
woz2.0 O
( O
wen O
et O
al. O
, O
2017 O
) O
and O
dstc2 O
( O
henderson O
et O
al. O
, O
2014 O
) O
are O
datasets O
for O
dst O
; O
they O
contain O
schemas O
with O
only O
categorical O
slots O
in O
a O
single O
domain O
. O

section 12
id pdf2json/2021.acl-long.135.pdf.json
m2m O
( O
shah O
et O
al. O
, O
2018 O
) O
is O
a O
dataset O
for O
dst O
and O
it O
has O
span O
annotations O
for O
slot O
values O
in O
multiple O
domains O
. O

section 12
id pdf2json/2021.acl-long.135.pdf.json
atis O
( O
tur O
et O
al. O
, O
2010 O
) O
and O
snips O
( O
coucke O
et O
al. O
, O
2018 O
) O
are O
datasets O
for O
nlu O
in O
single-turn O
dialogues O
in O
a O
single O
domain O
. O

section 12
id pdf2json/2021.acl-long.135.pdf.json
table O
3 O
gives O
the O
statics O
of O
datasets O
in O
the O
experiments O
. O

section 13
id pdf2json/2021.acl-long.135.pdf.json
we O
make O
comparison O
between O
our O
approach O
and O
the O
state-of-the-art O
methods O
on O
the O
datasets O
. O

section 13
id pdf2json/2021.acl-long.135.pdf.json
sgd O
, O
multiwoz2.2 O
and O
multiwoz2.1 O
: O
we O
compare O
seq2seqdu O
with O
six O
state-of-the-art O
methods O
on O
sgd O
, O
multiwoz2.2 O
and O
multiwoz2.1 O
, O
which O
utilize O
schema O
descriptions O
, O
span-based O
and O
candidate-based O
methods O
, O
unified O
seq2seq O
model O
and O
bert O
: O
fastsgt O
( O
noroozi O
et O
al. O
, O
2020 O
) O
, O
sgdbaseline O
( O
rastogi O
et O
al. O
, O
2019 O
) O
, O
trippy O
( O
heck O
et O
al. O
, O
2020 O
) O
, O
simpletod O
( O
hosseini-asl O
et O
al. O
, O
2020 O
) O
, O
trade O
( O
wu O
et O
al. O
, O
2019 O
) O
, O
and O
dsdst O
( O
zhang O
et O
al. O
, O
2019 O
) O
. O

section 13
id pdf2json/2021.acl-long.135.pdf.json
woz2.0 O
and O
dstc2 O
: O
our O
approach O
is O
compared O
against O
the O
state-of-the-art O
methods O
on O
woz2.0 O
and O
dstc2 O
, O
including O
those O
using O
a O
hierarchical O
seq2seq O
model O
and O
bert O
: O
comer O
( O
ren O
et O
al. O
, O
2019 O
) O
, O
bert-dst O
( O
chao O
and O
lane O
, O
2019 O
) O
, O
statenet O
( O
ren O
et O
al. O
, O
2018 O
) O
, O
glad O
( O
zhong O
et O
al. O
, O
2018 O
) O
, O
belief O
tracking O
( O
ramadan O
et O
al. O
, O
2018 O
) O
, O
and O
neural O
belief O
tracker O
( O
mrkšić O
et O
al. O
, O
2017 O
) O
. O

section 13
id pdf2json/2021.acl-long.135.pdf.json
m2m O
: O
we O
evaluate O
our O
approach O
and O
the O
stateof-the-art O
methods O
on O
m2m O
, O
which O
respectively O
employ O
a O
bert-based O
architecture O
and O
a O
jointlytrained O
language O
understanding O
model O
, O
bertdst O
( O
chao O
and O
lane O
, O
2019 O
) O
and O
dst+lu O
( O
rastogi O
et O
al. O
, O
2018 O
) O
. O

section 13
id pdf2json/2021.acl-long.135.pdf.json
atis O
and O
snips O
: O
we O
make O
comparison O
between O
our O
approach O
and O
the O
state-of-the-art O
methods O
on O
atis O
and O
snips O
for O
nlu O
within O
the O
sequence O
labeling O
framework O
, O
including O
joint O
bert O
( O
chen O
et O
al. O
, O
2019 O
) O
, O
slot-gated O
( O
goo O
et O
al. O
, O
2018 O
) O
, O
atten.-birnn O
( O
liu O
and O
lane O
, O
2016 O
) O
, O
and O
rnnlstm O
( O
hakkani-tür O
et O
al. O
, O
2016 O
) O
. O

section 13
id pdf2json/2021.acl-long.135.pdf.json
we O
also O
include O
two O
variants O
of O
seq2seq-du O
. O

section 13
id pdf2json/2021.acl-long.135.pdf.json
the O
differences O
are O
whether O
to O
use O
the O
schema O
descriptions O
, O
and O
the O
formation O
of O
dialogue O
state O
. O

section 13
id pdf2json/2021.acl-long.135.pdf.json
seq2seq-du-w/oschema O
: O
it O
is O
used O
for O
datasets O
that O
do O
not O
have O
schema O
descriptions O
. O

section 13
id pdf2json/2021.acl-long.135.pdf.json
it O
only O
contains O
utterance O
encoder O
and O
state O
decoder O
. O

section 13
id pdf2json/2021.acl-long.135.pdf.json
seq2seq-du-seqlabel O
: O
it O
is O
used O
for O
nlu O
in O
a O
single-turn O
dialogue O
. O

section 13
id pdf2json/2021.acl-long.135.pdf.json
it O
views O
the O
problem O
as O
sequence O
labeling O
, O
and O
only O
contains O
the O
utterance O
encoder O
and O
state O
decoder O
. O

section 14
id pdf2json/2021.acl-long.135.pdf.json
we O
make O
use O
of O
the O
following O
metrics O
in O
evaluation O
. O

section 14
id pdf2json/2021.acl-long.135.pdf.json
intent O
accuracy O
: O
percentage O
of O
turns O
in O
dialogue O
for O
which O
the O
intent O
is O
correctly O
identified O
. O

section 14
id pdf2json/2021.acl-long.135.pdf.json
joint O
goal O
accuracy O
: O
percentage O
of O
turns O
for O
which O
all O
the O
slots O
are O
correctly O
identified O
. O

section 14
id pdf2json/2021.acl-long.135.pdf.json
for O
non- O
categorical O
slots O
, O
a O
fuzzy O
matching O
score O
is O
used O
on O
sgd O
and O
exact O
match O
are O
used O
on O
the O
other O
datasets O
to O
keep O
the O
numbers O
comparable O
with O
other O
works O
. O

section 14
id pdf2json/2021.acl-long.135.pdf.json
slot O
f1 O
: O
f1 O
score O
to O
evaluate O
accuracy O
of O
slot O
sequence O
labeling O
. O

section 15
id pdf2json/2021.acl-long.135.pdf.json
we O
use O
the O
pre-trained O
bert O
model O
( O
[ O
bert-base O
, O
uncased O
] O
) O
, O
which O
has O
12 O
hidden O
layers O
of O
768 O
units O
and O
12 O
self-attention O
heads O
to O
encode O
utterances O
and O
schema O
descriptions O
. O

section 15
id pdf2json/2021.acl-long.135.pdf.json
the O
hidden O
size O
of O
lstm O
decoder O
is O
also O
768 O
. O

section 15
id pdf2json/2021.acl-long.135.pdf.json
the O
dropout O
probability O
is O
0.1 O
. O

section 15
id pdf2json/2021.acl-long.135.pdf.json
we O
also O
use O
beam O
search O
for O
decoding O
, O
with O
a O
beam O
size O
of O
5 O
. O

section 15
id pdf2json/2021.acl-long.135.pdf.json
the O
batch O
size O
is O
set O
to O
8 O
. O

section 15
id pdf2json/2021.acl-long.135.pdf.json
adam O
( O
kingma O
and O
ba O
, O
2014 O
) O
is O
used O
for O
optimization O
with O
an O
initial O
learning O
rate O
of O
1e-4 O
. O

section 15
id pdf2json/2021.acl-long.135.pdf.json
hyper O
parameters O
are O
chosen O
using O
the O
validation O
dataset O
in O
all O
cases O
. O

section 15
id pdf2json/2021.acl-long.135.pdf.json
the O
training O
curves O
of O
all O
models O
are O
shown O
in O
appendix O
a O
. O

section 16
id pdf2json/2021.acl-long.135.pdf.json
tables O
4 O
, O
5 O
, O
6 O
, O
and O
7 O
show O
the O
results O
. O

section 16
id pdf2json/2021.acl-long.135.pdf.json
one O
can O
see O
that O
seq2seq-du O
performs O
significantly O
better O
than O
the O
baselines O
in O
dst O
and O
performs O
equally O
well O
as O
the O
baselines O
in O
nlu O
. O

section 16
id pdf2json/2021.acl-long.135.pdf.json
dst O
is O
carried O
out O
in O
different O
settings O
in O
sgd O
, O
multiwoz2.2 O
, O
multiwoz2.1 O
, O
woz2.0 O
, O
dstc2 O
, O
and O
m2m O
. O

section 16
id pdf2json/2021.acl-long.135.pdf.json
in O
all O
cases O
, O
seq2seq-du O
works O
significantly O
better O
than O
the O
baselines O
. O

section 16
id pdf2json/2021.acl-long.135.pdf.json
the O
results O
indicate O
that O
seq2seq-du O
is O
really O
a O
general O
and O
effective O
model O
for O
dst O
, O
which O
can O
be O
applied O
to O
multiple O
settings O
. O

section 16
id pdf2json/2021.acl-long.135.pdf.json
specifically O
, O
seq2seq-du O
can O
leverage O
the O
schema O
descriptions O
for O
dst O
when O
they O
are O
available O
( O
sgd O
and O
multiwoz2.2 O
, O
multiwoz2.1 O
) O
3 O
. O

section 16
id pdf2json/2021.acl-long.135.pdf.json
it O
can O
work O
well O
in O
zero-shot O
learning O
to O
deal O
with O
unseen O
schemas O
( O
sgd O
) O
. O

section 16
id pdf2json/2021.acl-long.135.pdf.json
it O
can O
also O
effectively O
handle O
categorical O
slots O
( O
multiwoz2.1 O
, O
woz2.0 O
and O
dstc2 O
) O
and O
non-categorical O
slots O
( O
m2m O
) O
. O

section 16
id pdf2json/2021.acl-long.135.pdf.json
it O
appears O
that O
the O
success O
of O
seq2seqdu O
is O
due O
to O
its O
suitable O
architecture O
design O
with O
a O
sequence-to-sequence O
framework O
, O
bert-based O
encoders O
, O
utterance-schema O
attender O
, O
and O
pointer O
generation O
decoder O
. O

section 16
id pdf2json/2021.acl-long.135.pdf.json
nlu O
is O
formalized O
as O
sequence O
labeling O
in O
atis O
and O
snips O
. O

section 16
id pdf2json/2021.acl-long.135.pdf.json
seq2seq-du O
is O
degenerated O
to O
seq2seq-du-seqlabel O
, O
which O
is O
equivalent O
to O
the O
baseline O
of O
joint O
bert O
. O

section 16
id pdf2json/2021.acl-long.135.pdf.json
the O
results O
suggest O
that O
it O
is O
3there O
are O
better O
performing O
systems O
in O
the O
sgd O
competition O
. O

section 16
id pdf2json/2021.acl-long.135.pdf.json
the O
systems O
are O
not O
based O
on O
single O
methods O
and O
thus O
are O
not O
directly O
comparable O
with O
our O
method O
. O

section 16
id pdf2json/2021.acl-long.135.pdf.json
the O
case O
. O

section 16
id pdf2json/2021.acl-long.135.pdf.json
specially O
, O
the O
performances O
of O
seq2seqdu O
are O
comparable O
with O
joint O
bert O
, O
indicating O
that O
seq2seq-du O
can O
also O
be O
employed O
in O
nlu O
. O

section 17
id pdf2json/2021.acl-long.135.pdf.json
we O
also O
conduct O
ablation O
study O
on O
seq2seq-du O
. O

section 17
id pdf2json/2021.acl-long.135.pdf.json
we O
validate O
the O
effects O
of O
three O
factors O
: O
bertbased O
encoder O
, O
utterance-schema O
attention O
, O
and O
pointer O
generation O
decoder O
. O

section 17
id pdf2json/2021.acl-long.135.pdf.json
the O
results O
indicate O
that O
all O
the O
components O
of O
seq2seq-du O
are O
indispensable O
. O

section 17
id pdf2json/2021.acl-long.135.pdf.json
effect O
of O
bert O
to O
investigate O
the O
effectiveness O
of O
using O
bert O
in O
the O
utterance O
encoder O
and O
schema O
encoder O
, O
we O
replace O
bert O
with O
bi-directional O
lstm O
and O
run O
the O
model O
on O
sgd O
and O
multiwoz2.2 O
. O

section 17
id pdf2json/2021.acl-long.135.pdf.json
as O
shown O
in O
figure O
3 O
, O
the O
performance O
of O
the O
bilstm-based O
model O
seq2seq-du-w/obert O
in O
terms O
of O
joint O
ga O
and O
int O
. O

section 17
id pdf2json/2021.acl-long.135.pdf.json
acc O
decreases O
significantly O
compared O
with O
seq2seq-du O
. O

section 17
id pdf2json/2021.acl-long.135.pdf.json
it O
indicates O
that O
the O
bert-based O
encoders O
can O
create O
and O
utilize O
more O
accurate O
representations O
for O
dialogue O
understanding O
. O

section 17
id pdf2json/2021.acl-long.135.pdf.json
effect O
of O
attention O
to O
investigate O
the O
effectiveness O
of O
using O
attention O
, O
we O
compare O
seq2seq-du O
with O
seq2seq-duw/oattention O
which O
eliminates O
the O
attention O
mechanism O
, O
seq2seq-du-w/schemaatt O
which O
only O
contains O
the O
utterance-attended O
schema O
representations O
, O
and O
seq2seq-du-w/utteranceatt O
which O
only O
contains O
the O
schema-attended O
utterance O
representations O
. O

section 17
id pdf2json/2021.acl-long.135.pdf.json
figure O
3 O
shows O
the O
results O
on O
sgd O
and O
multiwoz2.2 O
in O
terms O
of O
joint O
ga O
and O
int O
. O

section 17
id pdf2json/2021.acl-long.135.pdf.json
acc O
. O

section 17
id pdf2json/2021.acl-long.135.pdf.json
one O
can O
observe O
that O
without O
attention O
the O
performances O
deteriorate O
considerably O
. O

section 17
id pdf2json/2021.acl-long.135.pdf.json
in O
addition O
, O
the O
performances O
of O
unidirectional O
attentions O
are O
inferior O
to O
the O
performance O
of O
bidirectional O
attention O
. O

section 17
id pdf2json/2021.acl-long.135.pdf.json
thus O
, O
utilization O
of O
bidirectional O
attention O
between O
utterances O
and O
schema O
descriptions O
is O
desriable O
. O

section 17
id pdf2json/2021.acl-long.135.pdf.json
effect O
of O
pointer O
generation O
to O
investigate O
the O
effectiveness O
of O
the O
pointer O
generation O
mechanism O
, O
we O
directly O
generate O
words O
from O
the O
vocabulary O
instead O
of O
generating O
pointers O
in O
the O
decoding O
process O
. O

section 17
id pdf2json/2021.acl-long.135.pdf.json
figure O
3 O
also O
shows O
the O
results O
of O
seq2seq-du-w/opointer O
on O
sgd O
and O
multiwoz2.2 O
in O
terms O
of O
joint O
ga O
and O
int O
. O

section 17
id pdf2json/2021.acl-long.135.pdf.json
acc O
. O

section 17
id pdf2json/2021.acl-long.135.pdf.json
from O
the O
results O
we O
can O
see O
that O
pointer O
generation O
is O
crucial O
for O
coping O
with O
unseen O
schemas O
. O

section 17
id pdf2json/2021.acl-long.135.pdf.json
in O
sgd O
which O
contains O
a O
large O
number O
of O
unseen O
schemas O
in O
the O
test O
set O
, O
there O
is O
significant O
performance O
degradation O
without O
pointer O
generation O
. O

section 17
id pdf2json/2021.acl-long.135.pdf.json
the O
results O
on O
multiwoz2.2 O
, O
which O
does O
not O
have O
unseen O
schemas O
in O
the O
test O
set O
, O
show O
pointer O
generation O
can O
also O
make O
significant O
improvement O
on O
already O
seen O
schemas O
by O
making O
full O
use O
of O
schema O
descriptions O
. O

section 18
id pdf2json/2021.acl-long.135.pdf.json
case O
study O
we O
make O
qualitative O
analysis O
on O
the O
results O
of O
seq2seq-du O
and O
sgd-baseline O
on O
sgd O
and O
multiwoz2.2 O
. O

section 18
id pdf2json/2021.acl-long.135.pdf.json
we O
find O
that O
seq2seq-du O
can O
make O
more O
accurate O
inference O
of O
dialogue O
states O
by O
leveraging O
the O
relations O
existing O
in O
the O
utterances O
and O
schema O
descriptions O
. O

section 18
id pdf2json/2021.acl-long.135.pdf.json
for O
example O
, O
in O
the O
first O
case O
in O
table O
8 O
, O
the O
user O
wants O
to O
find O
a O
cheap O
guesthouse O
. O

section 18
id pdf2json/2021.acl-long.135.pdf.json
seq2seq-du O
can O
correctly O
infer O
that O
the O
hotel O
type O
is O
“ O
guesthouse O
” O
by O
referring O
to O
the O
relation O
between O
“ O
hotel-pricerange O
” O
and O
“ O
hotel-type O
” O
. O

section 18
id pdf2json/2021.acl-long.135.pdf.json
in O
the O
second O
case O
, O
the O
user O
wants O
to O
rent O
a O
room O
with O
in-unit O
laundry O
. O

section 18
id pdf2json/2021.acl-long.135.pdf.json
in O
the O
dataset O
, O
a O
user O
who O
intends O
to O
rent O
a O
room O
will O
care O
more O
about O
the O
laundry O
property O
. O

section 18
id pdf2json/2021.acl-long.135.pdf.json
seq2seq-du O
can O
effectively O
extract O
the O
relation O
between O
“ O
intent O
” O
and O
“ O
in-unit-laundry O
” O
, O
yielding O
a O
correct O
result O
. O

section 18
id pdf2json/2021.acl-long.135.pdf.json
in O
contrast O
, O
sgd-baseline O
does O
not O
model O
the O
relations O
in O
the O
schemas O
, O
and O
thus O
it O
can O
not O
properly O
infer O
the O
values O
of O
“ O
hoteltype O
” O
and O
“ O
in-unit-laundry O
” O
. O

section 18
id pdf2json/2021.acl-long.135.pdf.json
dealing O
with O
unseen O
schemas O
we O
analyze O
the O
zero-shot O
learning O
ability O
of O
seq2seq-du O
. O

section 18
id pdf2json/2021.acl-long.135.pdf.json
table O
9 O
presents O
the O
accuracies O
of O
seq2seq-du O
in O
different O
domains O
on O
sgd O
. O

section 18
id pdf2json/2021.acl-long.135.pdf.json
( O
note O
that O
only O
sgd O
has O
unseen O
schemas O
in O
test O
set O
. O
) O

section 18
id pdf2json/2021.acl-long.135.pdf.json
we O
observe O
that O
the O
best O
performances O
can O
be O
obtained O
in O
the O
domains O
with O
all O
seen O
schemas O
. O

section 18
id pdf2json/2021.acl-long.135.pdf.json
the O
domains O
that O
have O
more O
partially O
seen O
schemas O
achieve O
higher O
accuracies O
, O
such O
as O
” O
hotels O
” O
, O
” O
movies O
” O
, O
” O
services O
” O
. O

section 18
id pdf2json/2021.acl-long.135.pdf.json
the O
accuracies O
decline O
in O
the O
domains O
with O
more O
unseen O
schemas O
, O
such O
as O
” O
messaging O
” O
and O
” O
rentalcars O
” O
. O

section 18
id pdf2json/2021.acl-long.135.pdf.json
we O
conclude O
that O
seq2seq-du O
can O
perform O
zero-shot O
learning O
across O
domains O
. O

section 18
id pdf2json/2021.acl-long.135.pdf.json
however O
, O
the O
ability O
still O
needs O
enhancement O
. O

section 18
id pdf2json/2021.acl-long.135.pdf.json
categorical O
slots O
and O
non-categorical O
slots O
table O
10 O
shows O
the O
accuracies O
of O
seq2seq-du O
and O
the O
baselines O
with O
respect O
to O
categorical O
and O
noncategorical O
slots O
on O
sgd O
and O
multiwoz2.2 O
. O

section 18
id pdf2json/2021.acl-long.135.pdf.json
( O
we O
did O
not O
compare O
with O
fastsgt O
on O
sgd O
dataset O
due O
to O
unavailability O
of O
the O
codes O
. O
) O

section 18
id pdf2json/2021.acl-long.135.pdf.json
one O
can O
see O
that O
seq2seq-du O
can O
effectively O
deal O
with O
both O
categorical O
and O
non-categorical O
slots O
. O

section 18
id pdf2json/2021.acl-long.135.pdf.json
furthermore O
, O
seq2seq-du O
demonstrates O
higher O
accuracies O
on O
categorical O
slots O
than O
non-categorical O
slots O
. O

section 18
id pdf2json/2021.acl-long.135.pdf.json
we O
conjecture O
that O
it O
is O
due O
to O
the O
co-occurrences O
of O
categorical O
slot O
values O
in O
both O
the O
dialogue O
history O
and O
the O
schema O
descriptions O
. O

section 18
id pdf2json/2021.acl-long.135.pdf.json
the O
utterance-schema O
attention O
can O
more O
easily O
capture O
the O
relations O
between O
the O
values O
. O

section 19
id pdf2json/2021.acl-long.135.pdf.json
we O
have O
proposed O
a O
new O
approach O
to O
dialogue O
state O
tracking O
. O

section 19
id pdf2json/2021.acl-long.135.pdf.json
the O
approach O
, O
referred O
to O
as O
seq2seqdu O
, O
takes O
dialogue O
state O
tracking O
( O
dst O
) O
as O
a O
problem O
of O
transforming O
all O
the O
utterances O
in O
a O
dialogue O
into O
semantic O
frames O
( O
state O
representations O
) O
on O
the O
basis O
of O
schema O
descriptions O
. O

section 19
id pdf2json/2021.acl-long.135.pdf.json
seq2seq-du O
is O
unique O
in O
that O
within O
the O
sequence O
to O
sequence O
framework O
it O
employs O
bert O
in O
encoding O
of O
utterances O
and O
schema O
descriptions O
respectively O
and O
generates O
pointers O
in O
decoding O
of O
dialogue O
state O
. O

section 19
id pdf2json/2021.acl-long.135.pdf.json
seq2seq-du O
is O
a O
global O
, O
reprentable O
, O
and O
scalable O
model O
for O
dst O
as O
well O
as O
nlu O
( O
natural O
language O
understanding O
) O
. O

section 19
id pdf2json/2021.acl-long.135.pdf.json
experimental O
results O
show O
that O
seq2seq-du O
significantly O
outperforms O
the O
state-ofthe-arts O
methods O
in O
dst O
on O
the O
benchmark O
datasets O
of O
sgd O
, O
multiwoz2.2 O
, O
multiwoz2.1 O
, O
woz2.0 O
, O
dstc2 O
, O
m2m O
, O
and O
performs O
as O
well O
as O
the O
stateof-the-arts O
in O
nlu O
on O
the O
benchmark O
datasets O
of O
atis O
and O
snips O
. O

section 20
id pdf2json/2021.acl-long.135.pdf.json
figure O
4 O
shows O
the O
training O
losses O
of O
seq2seq-du O
on O
the O
training O
datasets O
, O
while O
figure O
5 O
shows O
the O
accuracies O
of O
seq2seq-du O
on O
the O
test O
sets O
during O
training O
. O

section 20
id pdf2json/2021.acl-long.135.pdf.json
we O
regard O
training O
convergence O
when O
the O
fluctuation O
of O
loss O
is O
less O
than O
0.01 O
for O
consecutive O
20 O
thousand O
steps O
. O

section 20
id pdf2json/2021.acl-long.135.pdf.json
seq2seq-du O
converges O
at O
the O
180k-th O
step O
on O
sgd O
, O
multiwoz2.2 O
, O
and O
multiwoz2.1 O
. O

section 20
id pdf2json/2021.acl-long.135.pdf.json
seq2seq-du-w/oschema O
converges O
at O
the O
150k-th O
step O
on O
woz2.0 O
and O
at O
the O
140k-th O
step O
on O
dstc2 O
, O
and O
m2m O
. O

section 20
id pdf2json/2021.acl-long.135.pdf.json
furthermore O
, O
seq2seq-duseqlabel O
converges O
at O
the O
130k-th O
step O
on O
atis O
and O
snips O
. O

section 20
id pdf2json/2021.acl-long.135.pdf.json
these O
are O
consistent O
with O
the O
general O
trends O
in O
machine O
learning O
that O
more O
complex O
models O
are O
more O
difficult O
to O
train O
. O

section TITLE
id pdf2json/2021.acl-long.355.pdf.json
continuous O
language O
generative O
flow O

section ABSTRACT
id pdf2json/2021.acl-long.355.pdf.json
recent O
years O
have O
witnessed O
various O
types O
of O
generative O
models O
for O
natural O
language O
generation O
( O
nlg O
) O
, O
especially O
rnns O
or O
transformer O
based O
sequence-to-sequence O
models O
, O
as O
well O
as O
variational O
autoencoder O
( O
vae O
) O
and O
generative O
adversarial O
network O
( O
gan O
) O
based O
models O
. O

section ABSTRACT
id pdf2json/2021.acl-long.355.pdf.json
however O
, O
flow-based O
generative O
models O
, O
which O
achieve O
strong O
performance O
in O
image O
generation O
due O
to O
their O
invertibility O
and O
exact O
density O
estimation O
properties O
, O
have O
been O
less O
explored O
for O
nlg O
. O

section ABSTRACT
id pdf2json/2021.acl-long.355.pdf.json
in O
this O
paper O
, O
we O
propose O
a O
flow-based O
language O
generation O
model O
by O
adapting O
previous O
flow O
generative O
models O
to O
language O
generation O
via O
continuous O
input O
embeddings O
, O
adapted O
affine O
coupling O
structures O
, O
and O
a O
novel O
architecture O
for O
autoregressive O
text O
generation O
. O

section ABSTRACT
id pdf2json/2021.acl-long.355.pdf.json
we O
also O
apply O
our O
framework O
to O
sequence-to-sequence O
generation O
, O
including O
textand O
video-based O
question O
generation O
( O
qg O
) O
and O
neural O
machine O
translation O
( O
nmt O
) O
, O
and O
data O
augmentation O
for O
question O
answering O
( O
qa O
) O
. O

section ABSTRACT
id pdf2json/2021.acl-long.355.pdf.json
we O
use O
our O
language O
flow O
model O
to O
provide O
extra O
input O
features O
for O
qg O
and O
nmt O
, O
which O
achieves O
improvements O
over O
the O
strong O
qg O
baselines O
on O
squad O
and O
tvqa O
and O
nmt O
baseline O
on O
wmt16 O
. O

section ABSTRACT
id pdf2json/2021.acl-long.355.pdf.json
we O
also O
augment O
qa O
data O
with O
new O
context O
by O
injecting O
noise O
to O
the O
latent O
features O
of O
the O
language O
flow O
and O
show O
this O
augmentation O
leads O
to O
a O
large O
performance O
improvement O
from O
strong O
baselines O
on O
squad O
and O
tvqa.1 O

section 0
id pdf2json/2021.acl-long.355.pdf.json
proceedings O
of O
the O
59th O
annual O
meeting O
of O
the O
association O
for O
computational O
linguistics O
and O
the O
11th O
international O
joint O
conference O
on O
natural O
language O
processing O
, O
pages O
4609–4622 O
august O
1–6 O
, O
2021 O
. O

section 0
id pdf2json/2021.acl-long.355.pdf.json
©2021 O
association O
for O
computational O
linguistics O
4609 O

section 1
id pdf2json/2021.acl-long.355.pdf.json
several O
generative O
models O
have O
been O
proposed O
for O
language O
generation O
, O
including O
sequence-tosequence O
models O
based O
on O
rnns O
( O
luong O
et O
al. O
, O
2015 O
) O
and O
transformers O
( O
vaswani O
et O
al. O
, O
2017 O
) O
, O
as O
well O
as O
variational O
autoencoders O
( O
vaes O
) O
to O
generate O
diverse O
texts O
( O
bowman O
et O
al. O
, O
2016 O
; O
jain O
1our O
code O
and O
models O
are O
available O
at O
: O
https O
: O
// O
github.com/zinengtang/continuousflownlg O
et O
al. O
, O
2017 O
) O
, O
plus O
generative O
adversarial O
networks O
( O
gans O
) O
( O
yu O
et O
al. O
, O
2017 O
) O
to O
improve O
intended O
semantic O
fidelity O
. O

section 1
id pdf2json/2021.acl-long.355.pdf.json
another O
line O
of O
the O
generative O
model O
, O
normalizing O
flow O
( O
rezende O
and O
mohamed O
, O
2015 O
) O
, O
is O
widely O
explored O
in O
computer O
vision O
and O
representation O
learning O
but O
less O
explored O
for O
nlg O
tasks O
. O

section 1
id pdf2json/2021.acl-long.355.pdf.json
flow O
models O
have O
been O
shown O
to O
be O
capable O
of O
improving O
probability O
density O
estimation O
, O
including O
variational O
inference O
( O
rezende O
and O
mohamed O
, O
2015 O
) O
and O
exact O
density O
estimation O
( O
dinh O
et O
al. O
, O
2015 O
) O
. O

section 1
id pdf2json/2021.acl-long.355.pdf.json
generative O
flow O
is O
one O
type O
of O
flow O
model O
and O
first O
proposed O
by O
dinh O
et O
al O
. O

section 1
id pdf2json/2021.acl-long.355.pdf.json
( O
2015 O
, O
2017 O
) O
; O
kingma O
and O
dhariwal O
( O
2018 O
) O
. O

section 1
id pdf2json/2021.acl-long.355.pdf.json
taking O
advantage O
of O
its O
invertible O
structure O
, O
it O
can O
perform O
an O
exact O
density O
estimation O
of O
the O
input O
distribution O
. O

section 1
id pdf2json/2021.acl-long.355.pdf.json
thus O
, O
during O
generation O
, O
we O
can O
sample O
from O
its O
latent O
space O
and O
then O
generate O
new O
examples O
through O
its O
invertible O
decoder O
. O

section 1
id pdf2json/2021.acl-long.355.pdf.json
generative O
flow O
shows O
strong O
performance O
on O
image O
generation O
, O
attribute O
manipulation O
, O
and O
latent O
space O
inference O
( O
kingma O
and O
dhariwal O
, O
2018 O
) O
. O

section 1
id pdf2json/2021.acl-long.355.pdf.json
considering O
these O
successful O
applications O
, O
we O
conjecture O
that O
the O
flow O
model O
should O
also O
have O
strong O
potential O
to O
be O
adapted O
for O
language O
generation O
tasks O
. O

section 1
id pdf2json/2021.acl-long.355.pdf.json
therefore O
, O
in O
this O
paper O
, O
we O
introduce O
a O
continuous O
language O
generative O
flow O
model O
that O
can O
deal O
with O
discrete O
language O
data O
in O
continuous O
latent O
space O
. O

section 1
id pdf2json/2021.acl-long.355.pdf.json
we O
propose O
two O
variants O
, O
the O
non-autoregressive O
and O
autoregressive O
models O
, O
and O
show O
that O
they O
both O
can O
perform O
well O
on O
density O
estimation O
tasks O
. O

section 1
id pdf2json/2021.acl-long.355.pdf.json
we O
follow O
the O
architecture O
of O
one O
previous O
generative O
flow O
model O
, O
glow O
( O
kingma O
and O
dhariwal O
, O
2018 O
) O
, O
but O
make O
adaptions O
for O
language O
generation O
tasks O
. O

section 1
id pdf2json/2021.acl-long.355.pdf.json
we O
first O
employ O
glove O
word O
embeddings O
( O
pennington O
et O
al. O
, O
2014 O
) O
to O
map O
the O
discrete O
token O
sequence O
to O
a O
continuous O
embedding O
matrix O
. O

section 1
id pdf2json/2021.acl-long.355.pdf.json
furthermore O
, O
we O
utilize O
two O
components O
: O
time-dimension O
permutation O
and O
affine O
coupling O
with O
rnn O
or O
transformer O
non-linearity O
functions O
, O
which O
allow O
interaction O
between O
words O
in O
a O
se- O
quence O
and O
better O
contextualizes O
language O
semantics O
. O

section 1
id pdf2json/2021.acl-long.355.pdf.json
overall O
, O
these O
proposed O
components O
help O
generate O
texts O
in O
a O
non-autoregressive O
manner O
. O

section 1
id pdf2json/2021.acl-long.355.pdf.json
however O
, O
even O
though O
the O
non-autoregressive O
model O
has O
attracted O
a O
lot O
of O
research O
attention O
because O
of O
its O
fast O
generation O
speed O
, O
it O
still O
hardly O
surpasses O
the O
generation O
quality O
of O
autoregressive O
models O
( O
ren O
et O
al. O
, O
2020 O
) O
. O

section 1
id pdf2json/2021.acl-long.355.pdf.json
therefore O
, O
to O
make O
our O
language O
flow O
model O
learn O
language O
generation O
in O
a O
stronger O
autoregressive O
manner O
, O
we O
change O
the O
flow O
model O
’ O
s O
affine O
coupling O
and O
permutation O
to O
a O
uni-directional O
structure O
, O
i.e. O
, O
each O
timestep O
can O
only O
attend O
to O
previous O
timesteps O
. O

section 1
id pdf2json/2021.acl-long.355.pdf.json
in O
this O
way O
, O
we O
enable O
our O
model O
to O
perform O
text O
generation O
autoregressively O
. O

section 1
id pdf2json/2021.acl-long.355.pdf.json
some O
recent O
works O
have O
developed O
density O
estimation O
models O
targeted O
on O
character-level O
discrete O
data O
( O
discreteflow O
( O
tran O
et O
al. O
, O
2019 O
) O
) O
and O
explored O
using O
the O
flow O
architecture O
as O
an O
extra O
data O
encoder O
that O
provides O
latent O
features O
to O
support O
nonautoregressive O
text O
generation O
( O
flowseq O
( O
ma O
et O
al. O
, O
2019 O
) O
) O
. O

section 1
id pdf2json/2021.acl-long.355.pdf.json
while O
our O
work O
shares O
some O
similar O
characteristics O
, O
we O
explore O
different O
directions O
: O
( O
1 O
) O
discreteflow O
develops O
a O
modulus O
calculation O
method O
to O
process O
discrete O
data O
. O

section 1
id pdf2json/2021.acl-long.355.pdf.json
instead O
, O
we O
use O
word O
embedding O
to O
transform O
the O
discrete O
input O
tokens O
to O
continuous O
features O
, O
which O
is O
simple O
yet O
effective O
. O

section 1
id pdf2json/2021.acl-long.355.pdf.json
( O
2 O
) O
flowseq O
essentially O
leverages O
the O
flow O
architecture O
in O
a O
typical O
encoder-decoder O
model O
to O
support O
non-autoregressive O
generation O
, O
whereas O
our O
models O
follow O
the O
standard O
generative O
flow O
framework O
and O
can O
directly O
generate O
texts O
via O
their O
invertible O
structure O
in O
both O
non-autoregressive O
or O
autoregressive O
manner O
. O

section 1
id pdf2json/2021.acl-long.355.pdf.json
( O
3 O
) O
autoregressive O
flows O
were O
previously O
developed O
( O
papamakarios O
et O
al. O
, O
2017 O
; O
huang O
et O
al. O
, O
2018 O
) O
for O
stronger O
density O
estimation O
ability O
. O

section 1
id pdf2json/2021.acl-long.355.pdf.json
however O
, O
the O
autoregressive O
language O
flow O
model O
we O
develop O
here O
aims O
for O
better O
text O
generation O
quality O
. O

section 1
id pdf2json/2021.acl-long.355.pdf.json
for O
this O
, O
our O
model O
is O
autoregressive O
in O
both O
the O
forward O
stage O
( O
encoding O
an O
input O
to O
a O
latent O
feature O
) O
and O
inverse O
stage O
( O
decoding O
the O
latent O
feature O
to O
the O
input O
) O
with O
an O
uni-directional O
( O
i.e. O
, O
the O
left-to-right O
direction O
) O
structure O
, O
we O
evaluate O
the O
density O
estimation O
ability O
of O
our O
language O
flow O
models O
as O
well O
as O
their O
effectiveness O
for O
three O
downstream O
tasks O
: O
( O
1 O
) O
sequenceto-sequence O
( O
seq-to-seq O
) O
generation O
that O
includes O
question O
generation O
( O
qg O
) O
and O
neural O
machine O
translation O
( O
nmt O
) O
and O
( O
2 O
) O
data O
augmentation O
for O
question O
answering O
( O
qa O
) O
. O

section 1
id pdf2json/2021.acl-long.355.pdf.json
we O
test O
qg O
and O
qa O
data O
augmentation O
on O
two O
large-scale O
qa O
datasets O
: O
( O
a O
) O
squad O
( O
rajpurkar O
et O
al. O
, O
2016 O
) O
, O
a O
widely O
explored O
textual O
qa O
and O
qg O
dataset O
and O
( O
b O
) O
tvqa O
( O
lei O
et O
al. O
, O
2018 O
) O
, O
a O
large-scale O
multimodal O
videodialogue O
qa O
task O
. O

section 1
id pdf2json/2021.acl-long.355.pdf.json
we O
test O
machine O
translation O
on O
wmt16 O
( O
cettolo O
et O
al. O
, O
2012 O
) O
, O
a O
commonly O
used O
nmt O
dataset O
. O

section 1
id pdf2json/2021.acl-long.355.pdf.json
for O
density O
estimation O
, O
we O
compare O
the O
negative O
likelihoods O
of O
our O
models O
against O
a O
baseline O
lstm O
model O
. O

section 1
id pdf2json/2021.acl-long.355.pdf.json
for O
qg O
, O
we O
use O
the O
non-autoregressive O
flow O
model O
to O
provide O
extra O
input O
features O
for O
a O
standard O
encoder-decoder O
text O
generation O
model O
. O

section 1
id pdf2json/2021.acl-long.355.pdf.json
we O
show O
that O
it O
can O
significantly O
improve O
a O
baseline O
qg O
model O
for O
both O
squad O
and O
tvqa O
on O
both O
automatic O
and O
human O
evaluation O
metrics O
. O

section 1
id pdf2json/2021.acl-long.355.pdf.json
aided O
by O
our O
flow O
model O
, O
we O
achieve O
strong O
improvements O
over O
a O
transformer O
baseline O
in O
the O
neural O
machine O
translation O
experiment O
. O

section 1
id pdf2json/2021.acl-long.355.pdf.json
in O
addition O
to O
improving O
language O
generation O
quality O
, O
we O
also O
use O
the O
proposed O
autoregressive O
flow O
model O
for O
data O
augmentation O
. O

section 1
id pdf2json/2021.acl-long.355.pdf.json
for O
this O
, O
we O
focus O
on O
generating O
diverse O
textual O
contexts O
for O
qa O
tasks O
. O

section 1
id pdf2json/2021.acl-long.355.pdf.json
in O
particular O
, O
we O
inject O
noise O
into O
the O
latent O
features O
of O
our O
flow O
models O
( O
encoded O
from O
ground-truth O
contexts O
) O
and O
then O
generate O
new O
contexts O
from O
the O
noise-injected O
features O
. O

section 1
id pdf2json/2021.acl-long.355.pdf.json
experiments O
show O
that O
the O
generated O
contexts O
can O
be O
either O
a O
varied O
expression O
of O
the O
same O
subject O
or O
paraphrasing O
the O
original O
context O
, O
but O
, O
mostly O
keep O
the O
answerability O
of O
the O
original O
question O
( O
see O
examples O
in O
table O
3 O
) O
. O

section 1
id pdf2json/2021.acl-long.355.pdf.json
combined O
with O
data O
augmentation O
strategies O
( O
data O
filtering O
and O
training O
schema O
) O
, O
we O
achieve O
statistically O
significant O
improvements O
on O
both O
squad O
and O
tvqa O
over O
strong O
baselines O
. O

section 1
id pdf2json/2021.acl-long.355.pdf.json
overall O
, O
we O
have O
two O
contributions O
: O
( O
1 O
) O
we O
propose O
two O
continuous O
language O
generative O
flow O
model O
variants O
that O
have O
better O
density O
estimation O
abilities O
than O
an O
lstm O
baseline O
model O
, O
and O
can O
perform O
non-autoregressive O
and O
autoregressive O
generation O
respectively O
; O
( O
2 O
) O
our O
language O
flow O
model O
largely O
improves O
qg O
, O
nmt O
, O
and O
data O
augmentation O
for O
qa O
tasks O
. O

section 2
id pdf2json/2021.acl-long.355.pdf.json
in O
this O
section O
, O
we O
first O
review O
the O
generative O
flow O
model O
proposed O
in O
previous O
works O
( O
dinh O
et O
al. O
, O
2015 O
; O
kingma O
and O
dhariwal O
, O
2018 O
) O
. O

section 2
id pdf2json/2021.acl-long.355.pdf.json
then O
, O
following O
it O
, O
we O
propose O
two O
variants O
of O
our O
continuous O
language O
generative O
flow O
model O
. O

section 3
id pdf2json/2021.acl-long.355.pdf.json
flow-based O
generative O
models O
transform O
simple O
latent O
distributions O
, O
p O
( O
z O
) O
, O
into O
a O
complex O
data O
dis- O
tribution O
( O
language O
text O
in O
our O
case O
) O
, O
p O
( O
x O
) O
, O
through O
a O
chain O
of O
invertible O
transformations O
. O

section 3
id pdf2json/2021.acl-long.355.pdf.json
we O
first O
designate O
a O
true O
data O
distribution O
p O
( O
x O
) O
and O
a O
model O
pθ O
( O
x O
) O
with O
parameters O
θ O
to O
parameterize O
the O
true O
distribution O
p O
( O
x O
) O
. O

section 3
id pdf2json/2021.acl-long.355.pdf.json
the O
latent O
space O
inference O
is O
then O
defined O
as O
: O
xi O
∼ O
p O
( O
x O
) O
( O
1 O
) O
zi O
= O
fθ O
( O
xi O
) O
( O
2 O
) O
where O
xi O
is O
a O
data O
point O
from O
the O
true O
data O
distribution O
and O
zi O
the O
latent O
features O
. O

section 3
id pdf2json/2021.acl-long.355.pdf.json
this O
encoding O
x O
to O
z O
procedure O
is O
usually O
referred O
as O
the O
forward O
stage O
. O

section 3
id pdf2json/2021.acl-long.355.pdf.json
the O
transformation O
fθ O
is O
designed O
to O
be O
invertible O
and O
bijective O
. O

section 3
id pdf2json/2021.acl-long.355.pdf.json
in O
previous O
flow-based O
generative O
models O
( O
dinh O
et O
al. O
, O
2015 O
, O
2017 O
; O
kingma O
and O
dhariwal O
, O
2018 O
) O
, O
the O
generative O
process O
( O
or O
referred O
as O
the O
inverse O
stage O
) O
is O
defined O
as O
: O
zi O
∼ O
pθ O
( O
z O
) O
( O
3 O
) O
xi O
= O
gθ O
( O
zi O
) O
= O
f O
−1 O
θ O
( O
zi O
) O
( O
4 O
) O
where O
zi O
is O
a O
sample O
from O
the O
latent O
space O
distribution O
, O
such O
as O
a O
standard O
gaussian O
distribution O
. O

section 3
id pdf2json/2021.acl-long.355.pdf.json
the O
flow O
mapping O
fθ O
is O
composed O
of O
a O
chain O
of O
transformations O
: O
f O
= O
f1 O
◦ O
f2 O
◦ O
· O
· O
· O
◦ O
fk O
with O
each O
representing O
one O
flow O
step O
. O

section 3
id pdf2json/2021.acl-long.355.pdf.json
then O
, O
the O
log-likelihood O
can O
be O
written O
as O
: O
log O
pθ O
( O
x O
) O
= O
log O
pθ O
( O
z O
) O
+ O
k∑ O
j=1 O
log O
∣∣∣∣det O
( O
dhjdhj−1 O
) O
∣∣∣∣ O
( O
5 O
) O
where O
hj O
is O
the O
output O
of O
each O
flow O
step O
. O

section 3
id pdf2json/2021.acl-long.355.pdf.json
the O
value O
log O
|det O
( O
dhj/dhj−1 O
) O
| O
is O
namely O
the O
logdeterminant O
: O
the O
log O
of O
the O
absolute O
value O
of O
the O
determinant O
of O
the O
jacobian O
matrix O
( O
dhj/dhj−1 O
) O
. O

section 3
id pdf2json/2021.acl-long.355.pdf.json
this O
value O
is O
the O
change O
in O
log-density O
from O
hj−1 O
to O
hj O
under O
transformation O
fj O
. O

section 3
id pdf2json/2021.acl-long.355.pdf.json
this O
equation O
is O
namely O
the O
change O
of O
variable O
formula O
. O

section 3
id pdf2json/2021.acl-long.355.pdf.json
the O
objective O
for O
density O
estimation O
is O
formulated O
as O
: O
l O
( O
d O
) O
= O
1 O
n O
n∑ O
i=1 O
− O
log O
pθ O
( O
x̃i O
) O
−m O
log O
d O
( O
6 O
) O
x̃i O
= O
xi O
+ O
u O
( O
7 O
) O
where O
u O
is O
usually O
sampled O
from O
a O
gaussian O
distribution O
, O
n O
the O
number O
of O
samples O
in O
a O
batch O
, O
and O
d O
( O
= O
128 O
) O
the O
discretization O
level O
of O
the O
data O
and O
m O
the O
dimension O
of O
xi.2 O
2the O
change O
of O
variable O
formula O
, O
eq.5 O
, O
treats O
the O
data O
space O
as O
unbounded O
. O

section 3
id pdf2json/2021.acl-long.355.pdf.json
however O
, O
the O
data O
we O
use O
is O
usually O
within O
range O
-1.0 O
to O
1.0 O
and O
parameter O
d O
( O
the O
discretization O
) O
can O
reduce O
the O
impact O
of O
boundary O
effects O
according O
to O
dinh O
et O
al O
. O

section 3
id pdf2json/2021.acl-long.355.pdf.json
( O
2017 O
) O
. O

section 3
id pdf2json/2021.acl-long.355.pdf.json
each O
flow O
step O
in O
the O
generative O
flow O
model O
includes O
three O
parts O
: O
normalization O
, O
permutation O
, O
and O
affine O
coupling O
. O

section 3
id pdf2json/2021.acl-long.355.pdf.json
( O
1 O
) O
normalization O
is O
designed O
to O
scale O
each O
output O
to O
stabilize O
training O
. O

section 3
id pdf2json/2021.acl-long.355.pdf.json
we O
follow O
glow O
( O
kingma O
and O
dhariwal O
, O
2018 O
) O
to O
use O
actnorm O
. O

section 3
id pdf2json/2021.acl-long.355.pdf.json
( O
2 O
) O
permutation O
makes O
sure O
after O
multiple O
flow O
steps O
, O
each O
channel O
can O
sufficiently O
affect O
other O
dimensions O
. O

section 3
id pdf2json/2021.acl-long.355.pdf.json
the O
glow O
model O
( O
kingma O
and O
dhariwal O
, O
2018 O
) O
proposes O
to O
use O
a O
( O
trainable O
) O
invertible O
1 O
× O
1 O
convolution O
. O

section 3
id pdf2json/2021.acl-long.355.pdf.json
it O
is O
essentially O
a O
flexible O
generalization O
of O
a O
permutation O
operation O
. O

section 3
id pdf2json/2021.acl-long.355.pdf.json
we O
follow O
glow O
and O
also O
use O
its O
lu O
decomposition O
to O
reduce O
determinant O
computation O
cost O
. O

section 3
id pdf2json/2021.acl-long.355.pdf.json
different O
from O
all O
previous O
work O
, O
we O
apply O
1× O
1 O
convolution O
on O
the O
time O
dimension O
rather O
than O
the O
hidden O
dimension O
. O

section 3
id pdf2json/2021.acl-long.355.pdf.json
this O
is O
because O
language O
data O
is O
sequential O
and O
temporal O
. O

section 3
id pdf2json/2021.acl-long.355.pdf.json
this O
change O
is O
crucial O
to O
the O
proposed O
flow O
model O
’ O
s O
performance O
, O
which O
will O
be O
shown O
in O
ablation O
studies O
( O
table O
4 O
) O
. O

section 3
id pdf2json/2021.acl-long.355.pdf.json
( O
3 O
) O
affine O
coupling O
is O
designed O
to O
incorporate O
complex O
nonlinear O
mapping O
but O
still O
keep O
invertibility O
( O
see O
figure O
1 O
) O
. O

section 3
id pdf2json/2021.acl-long.355.pdf.json
z1 O
, O
z2 O
= O
split O
( O
z0 O
, O
dim O
: O
time O
) O
( O
8 O
) O
s O
, O
t O
= O
split O
( O
nn O
( O
z1 O
) O
, O
dim O
: O
hidden O
) O
( O
9 O
) O
ẑ2 O
= O
σ O
( O
s+ O
α O
) O
( O
t+ O
z2 O
) O
( O
10 O
) O
where O
nn O
refers O
to O
nonlinear O
function O
, O
σ O
is O
sigmoid O
activation O
. O

section 3
id pdf2json/2021.acl-long.355.pdf.json
α O
is O
a O
hyperparameter O
that O
prevents O
small O
value O
( O
around O
0 O
) O
from O
resulting O
in O
large O
negative O
value O
by O
log O
. O

section 3
id pdf2json/2021.acl-long.355.pdf.json
note O
that O
, O
in O
the O
first O
equation O
, O
glow O
( O
kingma O
and O
dhariwal O
, O
2018 O
) O
splits O
along O
the O
hidden O
dimension O
. O

section 3
id pdf2json/2021.acl-long.355.pdf.json
however O
, O
we O
split O
along O
time O
dimension O
( O
first O
introduced O
in O
flowseq O
( O
ma O
et O
al. O
, O
2019 O
) O
) O
which O
has O
the O
same O
motivation O
as O
the O
permutation O
module O
. O

section 4
id pdf2json/2021.acl-long.355.pdf.json
we O
first O
present O
our O
non-autoregressive O
language O
flow O
which O
is O
based O
on O
the O
architecture O
introduced O
above O
. O

section 4
id pdf2json/2021.acl-long.355.pdf.json
besides O
the O
permutation/affine O
coupling O
structures O
changes O
introduced O
above O
, O
we O
use O
rnns O
or O
transformer O
as O
the O
nonlinear O
mapping O
, O
propose O
to O
use O
continuous O
input O
embedding O
, O
and O
introduce O
multi-scale O
architecture O
. O

section 4
id pdf2json/2021.acl-long.355.pdf.json
affine O
coupling O
. O

section 4
id pdf2json/2021.acl-long.355.pdf.json
we O
use O
a O
multihead O
selfattention O
module O
in O
transformer O
( O
vaswani O
et O
al. O
, O
2017 O
) O
or O
alternatively O
rnns O
( O
a O
one-layer O
bidirectional O
lstm O
( O
schuster O
and O
paliwal O
, O
1997 O
) O
) O
in O
the O
coupling O
layer O
by O
replacing O
the O
non-linear O
mapping O
of O
affine O
coupling O
, O
nn O
( O
see O
eq.9 O
) O
. O

section 4
id pdf2json/2021.acl-long.355.pdf.json
continuous O
input O
embedding O
. O

section 4
id pdf2json/2021.acl-long.355.pdf.json
the O
language O
flow O
model O
we O
propose O
operates O
on O
continuous O
inputs O
, O
which O
means O
the O
inputs O
are O
not O
discrete O
tokens O
but O
continuous O
word O
embeddings O
. O

section 4
id pdf2json/2021.acl-long.355.pdf.json
we O
implement O
it O
through O
glove O
embeddings O
( O
pennington O
et O
al. O
, O
2014 O
) O
. O

section 4
id pdf2json/2021.acl-long.355.pdf.json
therefore O
, O
the O
density O
estimation O
is O
performed O
for O
the O
distribution O
p O
( O
x O
) O
, O
where O
x O
is O
the O
word O
embeddings O
of O
language O
tokens O
. O

section 4
id pdf2json/2021.acl-long.355.pdf.json
note O
that O
the O
word O
embeddings O
are O
frozen O
. O

section 4
id pdf2json/2021.acl-long.355.pdf.json
in O
the O
inverse O
stage O
, O
we O
compute O
the O
cosine O
similarity O
between O
the O
embedding O
matrix O
and O
decoder O
output O
as O
the O
token O
generation O
probability O
distribution O
, O
so O
that O
all O
tokens O
can O
be O
generated O
in O
parallel O
, O
i.e. O
, O
nonautoregressively O
. O

section 4
id pdf2json/2021.acl-long.355.pdf.json
multi-scale O
architecture O
. O

section 4
id pdf2json/2021.acl-long.355.pdf.json
following O
dinh O
et O
al O
. O

section 4
id pdf2json/2021.acl-long.355.pdf.json
( O
2017 O
) O
, O
we O
use O
a O
multi-scale O
architecture O
( O
see O
figure O
2 O
) O
that O
contains O
multiple O
blocks O
while O
each O
block O
containing O
several O
flow O
steps O
. O

section 4
id pdf2json/2021.acl-long.355.pdf.json
in O
our O
work O
, O
we O
denote O
the O
number O
of O
flow O
steps O
as O
k O
, O
and O
the O
number O
of O
blocks O
as O
l O
that O
each O
contains O
k O
flow O
steps O
. O

section 4
id pdf2json/2021.acl-long.355.pdf.json
we O
denote O
the O
input O
shape O
as O
( O
batch O
size O
b O
, O
sequence O
length O
s O
, O
hidden O
dimension O
h O
) O
. O

section 4
id pdf2json/2021.acl-long.355.pdf.json
at O
the O
start O
of O
each O
block O
, O
the O
tensor O
is O
reshaped O
from O
( O
b O
, O
s O
, O
h O
) O
to O
( O
b O
, O
s2 O
, O
2h O
) O
, O
so O
the O
model O
can O
capture O
more O
local O
features O
; O
and O
at O
the O
end O
of O
each O
block O
( O
except O
the O
last O
block O
) O
, O
the O
latent O
feature O
is O
split O
into O
halves O
via O
channel O
dimension O
with O
one O
as O
the O
output O
, O
zl O
, O
and O
the O
other O
as O
the O
input O
of O
the O
next O
block O
. O

section 4
id pdf2json/2021.acl-long.355.pdf.json
if O
we O
have O
3 O
blocks O
, O
we O
will O
have O
three O
latent O
outputs O
, O
zl O
. O

section 4
id pdf2json/2021.acl-long.355.pdf.json
past O
works O
( O
dinh O
et O
al. O
, O
2017 O
; O
kingma O
and O
dhariwal O
, O
2018 O
; O
ma O
et O
al. O
, O
2019 O
) O
reshape O
in O
this O
manner O
for O
all O
blocks O
. O

section 4
id pdf2json/2021.acl-long.355.pdf.json
however O
, O
we O
do O
not O
reshape O
in O
the O
first O
block O
but O
apply O
the O
same O
for O
the O
following O
blocks O
, O
which O
allows O
the O
model O
to O
better O
process O
the O
original O
input O
text O
with O
intact O
sentence O
structure O
. O

section 5
id pdf2json/2021.acl-long.355.pdf.json
the O
model O
we O
developed O
in O
the O
previous O
subsection O
can O
properly O
operate O
on O
continuous O
word O
embeddings O
, O
have O
exact O
density O
estimation O
, O
and O
perform O
non-autoregressive O
generation O
, O
however O
, O
it O
lacks O
the O
autoregressive O
structure O
that O
is O
commonly O
used O
for O
text O
generation O
. O

section 5
id pdf2json/2021.acl-long.355.pdf.json
previous O
works O
have O
shown O
autoregressive O
generation O
usually O
performs O
better O
than O
non-autoregressive O
generation O
( O
ren O
et O
al. O
, O
2020 O
) O
. O

section 5
id pdf2json/2021.acl-long.355.pdf.json
thus O
, O
we O
develop O
an O
autoregressive O
model O
that O
can O
generate O
text O
from O
left O
to O
right O
in O
the O
inverse O
stage O
. O

section 5
id pdf2json/2021.acl-long.355.pdf.json
to O
achieve O
this O
, O
we O
change O
affine O
coupling O
and O
permutation O
in O
the O
flow O
step O
to O
be O
unidirectional O
, O
i.e. O
, O
each O
timestep O
can O
only O
attend O
to O
timesteps O
that O
precede O
it O
. O

section 5
id pdf2json/2021.acl-long.355.pdf.json
however O
, O
we O
have O
to O
remove O
the O
multi-scale O
architecture O
to O
fulfill O
the O
autoregressive O
requirement O
. O

section 5
id pdf2json/2021.acl-long.355.pdf.json
see O
sample O
outputs O
in O
table O
1 O
for O
comparison O
to O
those O
from O
the O
nonautoregressive O
model O
. O

section 5
id pdf2json/2021.acl-long.355.pdf.json
uni-directional O
permutation O
. O

section 5
id pdf2json/2021.acl-long.355.pdf.json
since O
the O
permutation O
in O
each O
flow O
step O
designed O
in O
our O
nonautoregressive O
flow O
model O
is O
bidirectional O
, O
we O
mask O
the O
1× O
1 O
convolution O
to O
a O
lower O
triangular O
matrix O
. O

section 5
id pdf2json/2021.acl-long.355.pdf.json
therefore O
, O
each O
token O
can O
only O
attend O
to O
previous O
tokens O
in O
the O
permutation O
, O
i.e. O
, O
uni-directional O
permutation O
. O

section 5
id pdf2json/2021.acl-long.355.pdf.json
uni-directional O
affine O
coupling O
. O

section 5
id pdf2json/2021.acl-long.355.pdf.json
we O
then O
introduce O
an O
autoregressive O
version O
of O
affine O
coupling O
, O
shown O
by O
the O
ac-cell O
in O
figure O
3 O
. O

section 5
id pdf2json/2021.acl-long.355.pdf.json
for O
each O
flow O
step O
, O
we O
denote O
the O
input O
sequence O
as O
ẑ O
( O
0 O
) O
: O
( O
t O
) O
k+1 O
= O
[ O
ẑ O
( O
0 O
) O
k+1 O
, O
... O
, O
ẑ O
( O
t O
) O
k+1 O
] O
, O
and O
then O
the O
autoregressive O
coupling O
is O
defined O
as O
: O
r O
( O
t−1 O
) O
= O
nn O
( O
[ O
c O
( O
t−1 O
) O
; O
z O
( O
t−1 O
) O
k+1 O
] O
) O
( O
11 O
) O
c O
( O
t O
) O
= O
ha O
( O
r O
( O
t−1 O
) O
, O
ẑ O
( O
t O
) O
k+1 O
) O
( O
12 O
) O
z O
( O
t O
) O
k+1 O
= O
hb O
( O
r O
( O
t−1 O
) O
, O
c O
( O
t O
) O
) O
( O
13 O
) O
we O
recurrently O
obtain O
the O
outputs O
, O
[ O
z O
( O
1 O
) O
k+1 O
, O
... O
, O
z O
( O
t O
) O
k+1 O
] O
. O

section 5
id pdf2json/2021.acl-long.355.pdf.json
note O
that O
z O
( O
0 O
) O
k+1 O
= O
ẑ O
( O
0 O
) O
k+1 O
, O
so O
the O
computation O
starts O
from O
z O
( O
1 O
) O
k+1 O
. O

section 5
id pdf2json/2021.acl-long.355.pdf.json
when O
computing O
z O
( O
1 O
) O
k+1 O
, O
we O
can O
not O
get O
c O
( O
0 O
) O
, O
so O
we O
set O
it O
to O
be O
zero O
. O

section 5
id pdf2json/2021.acl-long.355.pdf.json
ha O
and O
ha O
are O
both O
affine O
coupling O
structured O
, O
as O
shown O
in O
figure O
4 O
. O

section 5
id pdf2json/2021.acl-long.355.pdf.json
nn O
is O
either O
rnn O
or O
transformer O
. O

section 5
id pdf2json/2021.acl-long.355.pdf.json
in O
the O
inverse O
stage O
, O
to O
obtain O
ẑk+1 O
, O
we O
start O
from O
ẑ O
( O
0 O
) O
k+1 O
= O
z O
( O
0 O
) O
k+1 O
and O
c O
( O
0 O
) O
: O
r O
( O
t−1 O
) O
= O
nn O
( O
[ O
c O
( O
t−1 O
) O
; O
ẑ O
( O
t−1 O
) O
k+1 O
] O
) O
( O
14 O
) O
c O
( O
t O
) O
= O
h−1b O
( O
r O
( O
t−1 O
) O
, O
z O
( O
t O
) O
k+1 O
) O
( O
15 O
) O
ẑ O
( O
t O
) O
k+1 O
= O
h O
−1 O
a O
( O
r O
( O
t−1 O
) O
, O
c O
( O
t O
) O
) O
( O
16 O
) O
since O
both O
decoded O
tokens O
z O
( O
t O
) O
and O
context O
c O
( O
t O
) O
only O
depend O
on O
previous O
tokens O
z O
( O
0 O
) O
: O
( O
t−1 O
) O
, O
we O
can O
perform O
autoregressive O
decoding O
and O
beam O
search O
with O
cosine O
similarity O
as O
the O
probability O
distribution O
of O
output O
tokens O
. O

section 5
id pdf2json/2021.acl-long.355.pdf.json
autoregressive O
flow O
step O
. O

section 5
id pdf2json/2021.acl-long.355.pdf.json
the O
changes O
of O
affine O
coupling O
and O
permutation O
to O
uni-directional O
allow O
the O
flow O
step O
to O
be O
autoregressive O
. O

section 5
id pdf2json/2021.acl-long.355.pdf.json
and O
the O
whole O
autoregressive O
flow O
model O
will O
contain O
k O
such O
flow O
steps O
. O

section 5
id pdf2json/2021.acl-long.355.pdf.json
at O
each O
flow O
step O
, O
the O
log-determinant O
is O
the O
summation O
of O
the O
log-determinant O
of O
all O
time O
steps O
: O
log O
p O
( O
zk+1 O
) O
= O
∑ O
t O
log O
p O
( O
z O
( O
t O
) O
k+1 O
) O
( O
17 O
) O
= O
∑ O
t O
log O
p O
( O
z O
( O
t O
) O
k O
) O
+ O
log O
∣∣∣∣∣det O
( O
dz O
( O
t O
) O
k+1 O
dz O
( O
t O
) O
k O
) O
∣∣∣∣∣ O
( O
18 O
) O

section 6
id pdf2json/2021.acl-long.355.pdf.json
we O
next O
apply O
our O
flow O
model O
to O
several O
downstream O
tasks O
. O

section 6
id pdf2json/2021.acl-long.355.pdf.json
despite O
the O
flow O
’ O
s O
rigid O
model O
structure O
, O
it O
has O
a O
strong O
potential O
in O
density O
estimation O
due O
to O
its O
complex O
transformation O
of O
inputs O
into O
a O
continuous O
latent O
space O
. O

section 6
id pdf2json/2021.acl-long.355.pdf.json
we O
aim O
to O
use O
this O
property O
to O
improve O
standard O
encoder-decoder O
text O
generation O
models O
. O

section 6
id pdf2json/2021.acl-long.355.pdf.json
moreover O
, O
as O
the O
flow O
model O
has O
a O
strong O
ability O
in O
generating O
diverse O
text O
, O
we O
show O
that O
it O
has O
the O
capability O
for O
data O
augmentation O
to O
improve O
qa O
tasks O
. O

section 7
id pdf2json/2021.acl-long.355.pdf.json
squad O
. O

section 7
id pdf2json/2021.acl-long.355.pdf.json
squad O
is O
a O
textual O
question O
answering O
dataset O
containing O
100,000+ O
questions/answers O
with O
corresponding O
short O
articles O
as O
context O
. O

section 7
id pdf2json/2021.acl-long.355.pdf.json
we O
use O
it O
to O
evaluate O
both O
question O
generation O
and O
data O
augmentation O
( O
by O
generating O
new O
articles O
) O
for O
question O
answering O
. O

section 7
id pdf2json/2021.acl-long.355.pdf.json
tvqa O
. O

section 7
id pdf2json/2021.acl-long.355.pdf.json
tvqa O
is O
a O
large-scale O
video O
qa O
dataset O
based O
on O
6 O
tv O
shows O
. O

section 7
id pdf2json/2021.acl-long.355.pdf.json
it O
consists O
of O
152,545 O
qa O
pairs O
from O
21,793 O
video O
clips O
with O
subtitle O
text O
. O

section 7
id pdf2json/2021.acl-long.355.pdf.json
we O
use O
it O
to O
evaluate O
both O
question O
generation O
and O
data O
augmentation O
( O
by O
generating O
new O
subtitles O
) O
for O
question O
answering O
. O

section 7
id pdf2json/2021.acl-long.355.pdf.json
wmt16 O
( O
ro-en O
) O
. O

section 7
id pdf2json/2021.acl-long.355.pdf.json
wmt16 O
( O
ro-en O
) O
is O
a O
machine O
translation O
dataset O
between O
english O
and O
romanian O
with O
around O
610k O
sentence O
pairs O
. O

section 7
id pdf2json/2021.acl-long.355.pdf.json
we O
use O
it O
for O
our O
machine O
translation O
experiment O
and O
only O
test O
for O
the O
romanian O
to O
english O
direction O
. O

section 8
id pdf2json/2021.acl-long.355.pdf.json
similar O
to O
flowseq O
( O
ma O
et O
al. O
, O
2019 O
) O
, O
we O
use O
flow O
as O
an O
extra O
module O
on O
top O
of O
a O
typical O
encoderdecoder O
language O
generation O
model O
and O
test O
on O
question O
generation O
( O
qg O
) O
and O
neural O
machine O
translation O
( O
nmt O
) O
. O

section 8
id pdf2json/2021.acl-long.355.pdf.json
as O
the O
flow O
model O
has O
the O
ability O
for O
exact O
density O
estimation O
, O
it O
provides O
the O
exact O
density O
components O
of O
context O
information O
and O
we O
assume O
that O
it O
provides O
a O
better O
hidden O
representation O
of O
context O
and O
thus O
helps O
with O
language O
generation O
. O

section 8
id pdf2json/2021.acl-long.355.pdf.json
it O
can O
also O
be O
viewed O
as O
a O
self-supervised O
learning O
method O
that O
can O
provide O
new O
features O
for O
downstream O
tasks O
. O

section 8
id pdf2json/2021.acl-long.355.pdf.json
concretely O
, O
while O
the O
original O
qg O
model3 O
is O
formulated O
as O
ui O
= O
e O
( O
xi O
) O
, O
q̂i O
= O
g O
( O
ui O
) O
; O
the O
new O
qg O
model O
with O
flow O
is O
formulated O
as O
: O
ui O
= O
e O
( O
xi O
) O
, O
q̂i O
= O
g O
( O
hatt O
( O
ui O
, O
zi O
) O
) O
( O
19 O
) O
where O
e O
refers O
to O
encoder O
and O
g O
decoder O
. O

section 8
id pdf2json/2021.acl-long.355.pdf.json
zi O
refers O
to O
latent O
features O
of O
the O
non-autoregressive O
flow O
model O
. O

section 8
id pdf2json/2021.acl-long.355.pdf.json
hatt O
is O
essentially O
a O
mlp O
with O
sigmoid O
activation O
. O

section 8
id pdf2json/2021.acl-long.355.pdf.json
the O
loss O
function O
has O
two O
parts O
: O
lgen O
= O
1 O
n O
n∑ O
i=1 O
− O
log O
p O
( O
qi O
) O
( O
20 O
) O
l O
= O
λlnll O
+ O
lgen O
( O
21 O
) O
where O
qi O
represents O
the O
target O
questions O
and O
λ O
is O
a O
hyperparameter O
for O
nll O
loss O
( O
eq O
. O

section 8
id pdf2json/2021.acl-long.355.pdf.json
6 O
) O
3we O
replicate O
zhang O
and O
bansal O
( O
2019 O
) O
’ O
s O
standard O
encoderdecoder O
attention O
qg O
model O
with O
bert O
features O
as O
input O
embeddings O
. O

section 9
id pdf2json/2021.acl-long.355.pdf.json
context O
generation O
. O

section 9
id pdf2json/2021.acl-long.355.pdf.json
we O
propose O
to O
use O
flow O
to O
generate O
diverse O
contexts O
for O
data O
augmentation O
as O
both O
tvqa O
and O
squad O
are O
question O
answering O
tasks O
with O
textual O
context O
. O

section 9
id pdf2json/2021.acl-long.355.pdf.json
we O
generate O
new O
context O
( O
video O
subtitles O
for O
tvqa O
; O
articles O
for O
squad O
) O
by O
injecting O
noise O
to O
the O
hidden O
vector O
of O
the O
original O
context O
, O
zi O
, O
and O
reconstructing O
it O
to O
new O
sentences O
, O
x̂i O
. O

section 9
id pdf2json/2021.acl-long.355.pdf.json
note O
that O
, O
we O
can O
also O
do O
the O
same O
thing O
for O
questions O
, O
however O
, O
we O
find O
that O
changing O
one O
word O
in O
the O
question O
will O
dramatically O
change O
its O
meaning O
, O
so O
we O
limit O
this O
augmentation O
to O
the O
context O
and O
keep O
the O
original O
question O
unchanged O
. O

section 9
id pdf2json/2021.acl-long.355.pdf.json
the O
generation O
process O
is O
formulated O
as O
: O
zi O
= O
fθ O
( O
xi O
) O
( O
22 O
) O
x̂i O
= O
f O
−1 O
θ O
( O
zi O
+ O
z0 O
) O
( O
23 O
) O
where O
fθ O
refers O
to O
the O
flow O
model O
and O
xi O
the O
input O
text O
and O
zi O
the O
latent O
space O
. O

section 9
id pdf2json/2021.acl-long.355.pdf.json
the O
transformation O
is O
performed O
by O
simply O
sampling O
a O
gaussian O
noise O
z0 O
, O
add O
it O
to O
zi O
, O
and O
reconstruct O
the O
new O
context O
x̂i O
in O
the O
reverse O
stage O
. O

section 9
id pdf2json/2021.acl-long.355.pdf.json
in O
this O
task O
, O
we O
use O
the O
autoregressive O
flow O
model O
as O
this O
variant O
is O
designed O
for O
text O
generation O
. O

section 9
id pdf2json/2021.acl-long.355.pdf.json
we O
also O
use O
the O
non-autoregressive O
flow O
model O
additionally O
leveraged O
by O
an O
additional O
autoregressive O
decoder O
, O
as O
an O
alternative O
approach O
. O

section 9
id pdf2json/2021.acl-long.355.pdf.json
while O
the O
standard O
rnn-based O
language O
model O
does O
not O
have O
an O
explicit O
global O
sentence O
representation O
, O
our O
flow O
model O
is O
similar O
to O
bowman O
et O
al O
. O

section 9
id pdf2json/2021.acl-long.355.pdf.json
( O
2016 O
) O
’ O
s O
vae O
framework O
that O
encodes O
the O
sentence O
into O
a O
continuous O
hidden O
vector O
, O
p O
( O
z|x O
) O
. O

section 9
id pdf2json/2021.acl-long.355.pdf.json
and O
sampling O
around O
the O
hidden O
vector O
can O
naturally O
be O
viewed O
as O
injecting O
noise O
without O
changing O
key O
information O
. O

section 9
id pdf2json/2021.acl-long.355.pdf.json
therefore O
, O
we O
do O
not O
aim O
for O
paraphrasing O
the O
original O
context O
because O
the O
flow O
model O
can O
reconstruct O
different O
information O
from O
random O
noise O
injection O
in O
latent O
space O
. O

section 9
id pdf2json/2021.acl-long.355.pdf.json
notably O
, O
this O
method O
has O
the O
risk O
of O
changing O
the O
context O
’ O
s O
meaning O
and O
making O
the O
question O
unanswerable O
, O
however O
, O
empirically O
, O
we O
find O
that O
as O
long O
as O
we O
keep O
the O
noise O
small O
enough O
, O
the O
generation O
will O
be O
either O
paraphrases O
or O
different O
expressions O
of O
the O
same O
subject O
without O
affecting O
the O
answerability O
. O

section 9
id pdf2json/2021.acl-long.355.pdf.json
data O
filtering O
. O

section 9
id pdf2json/2021.acl-long.355.pdf.json
to O
better O
utilize O
the O
generated O
data O
, O
we O
design O
a O
data O
filter O
as O
filtering O
out O
the O
lowquality O
generated O
text O
is O
useful O
in O
helping O
improve O
the O
data O
augmentation O
( O
zhang O
and O
bansal O
, O
2019 O
) O
. O

section 9
id pdf2json/2021.acl-long.355.pdf.json
we O
use O
pretrained O
qa O
baseline O
models O
( O
see O
table O
8 O
baseline O
tvqa+ O
and O
table O
9 O
baseline O
bert O
) O
to O
filter O
out O
the O
low-quality O
context O
. O

section 9
id pdf2json/2021.acl-long.355.pdf.json
the O
generated O
context O
will O
be O
filtered O
out O
if O
the O
model O
performs O
worse O
on O
predicting O
correct O
answers O
when O
original O
context O
is O
replaced O
by O
its O
generated O
counterpart O
. O

section 10
id pdf2json/2021.acl-long.355.pdf.json
we O
follow O
zhang O
and O
bansal O
( O
2019 O
) O
to O
split O
the O
development O
set O
of O
squadv1.1 O
( O
rajpurkar O
et O
al. O
, O
2016 O
) O
into O
two O
halves O
and O
show O
the O
result O
on O
the O
test O
split O
. O

section 10
id pdf2json/2021.acl-long.355.pdf.json
we O
generally O
follow O
previous O
work O
on O
evaluation O
metrics O
. O

section 10
id pdf2json/2021.acl-long.355.pdf.json
for O
density O
estimation O
, O
we O
use O
negative O
log-likelihood O
( O
nll O
) O
for O
comparison O
and O
bits O
per O
dimension O
to O
regularize O
the O
negative O
loglikelihood O
loss O
, O
formulated O
as O
lm O
log O
( O
2 O
) O
, O
where O
m O
represents O
the O
dimension O
of O
input O
. O

section 10
id pdf2json/2021.acl-long.355.pdf.json
we O
evaluate O
qg O
by O
bleu4 O
( O
papineni O
et O
al. O
, O
2002 O
) O
, O
meteor O
( O
lavie O
and O
agarwal O
, O
2007 O
) O
, O
rouge-l O
( O
lin O
, O
2004 O
) O
, O
and O
amazon O
mturk O
human O
evaluation O
. O

section 10
id pdf2json/2021.acl-long.355.pdf.json
we O
use O
the O
bleu O
score O
to O
evaluate O
nmt O
. O

section 10
id pdf2json/2021.acl-long.355.pdf.json
we O
use O
accuracy O
to O
evaluate O
the O
tvqa O
qa O
model O
and O
em O
( O
exact O
match O
) O
and O
f1 O
score O
to O
evaluate O
the O
squad O
qa O
model O
. O

section 10
id pdf2json/2021.acl-long.355.pdf.json
we O
replicate O
zhang O
and O
bansal O
( O
2019 O
) O
’ O
s O
baseline O
qg O
model O
. O

section 10
id pdf2json/2021.acl-long.355.pdf.json
we O
use O
the O
stage O
model O
with O
glove O
embeddings O
developed O
by O
lei O
et O
al O
. O

section 10
id pdf2json/2021.acl-long.355.pdf.json
( O
2020 O
) O
as O
the O
tvqa O
qa O
baseline O
and O
use O
bert O
as O
the O
squad O
qa O
baseline O
. O

section 10
id pdf2json/2021.acl-long.355.pdf.json
see O
appendix O
a O
for O
more O
experiment/reproducibility O
details O
. O

section 12
id pdf2json/2021.acl-long.355.pdf.json
first O
of O
all O
, O
to O
evaluate O
the O
density O
estimation O
ability O
, O
we O
compare O
the O
negative O
log-likelihood O
( O
nll O
, O
eq.6 O
) O
4 O
of O
our O
different O
flow O
models O
on O
the O
context O
data O
of O
squad O
and O
tvqa O
against O
a O
baseline O
model O
( O
a O
3-layer O
bidirectional O
lstm-rnn O
model O
with O
hidden O
size O
300 O
) O
. O

section 12
id pdf2json/2021.acl-long.355.pdf.json
as O
shown O
in O
table O
4 O
, O
the O
flow O
model O
of O
time-dim O
coupling/permutation O
4note O
that O
since O
our O
p O
( O
x O
) O
is O
over O
continuous O
word O
embeddings O
, O
so O
it O
is O
the O
probability O
density O
of O
a O
continuous O
variable O
which O
is O
not O
bounded O
by O
[ O
0,1 O
] O
. O

section 12
id pdf2json/2021.acl-long.355.pdf.json
generally O
outperforms O
the O
baseline O
lstm O
model O
. O

section 12
id pdf2json/2021.acl-long.355.pdf.json
the O
flow O
model O
of O
time-dim O
coupling/permutation O
largely O
outperforms O
the O
flow O
model O
of O
channeldim O
coupling/permutation O
. O

section 12
id pdf2json/2021.acl-long.355.pdf.json
we O
also O
test O
our O
autoregressive O
model O
to O
check O
its O
density O
estimation O
ability O
, O
and O
we O
find O
it O
performs O
well O
and O
even O
sometimes O
slightly O
better O
than O
the O
non-autoregressive O
model O
. O

section 12
id pdf2json/2021.acl-long.355.pdf.json
note O
that O
we O
do O
not O
claim O
the O
autoregressive O
model O
is O
better O
at O
density O
estimation O
than O
the O
non-autoregressive O
version O
, O
instead O
, O
we O
aim O
to O
show O
that O
it O
can O
perform O
reasonably O
with O
the O
proposed O
autoregressive O
adaptation O
. O

section 13
id pdf2json/2021.acl-long.355.pdf.json
question O
generation O
. O

section 13
id pdf2json/2021.acl-long.355.pdf.json
through O
the O
ablation O
studies O
shown O
in O
table O
5 O
and O
table O
6 O
, O
we O
demonstrate O
that O
the O
proposed O
flow-aided O
qg O
model O
significantly O
improves O
the O
qg O
performance O
. O

section 13
id pdf2json/2021.acl-long.355.pdf.json
the O
statistical O
significances O
for O
all O
metric O
improvements O
( O
bleu4 O
, O
rouge-l O
, O
meteor O
) O
are O
p O
< O
0.001 O
for O
both O
tvqa O
qg O
and O
squad O
qg.5 O
we O
also O
conduct O
a O
human O
evaluation O
. O

section 13
id pdf2json/2021.acl-long.355.pdf.json
we O
random O
sample O
200 O
examples6 O
, O
and O
we O
present O
the O
participants O
two O
questions O
per O
example O
generated O
by O
two O
different O
models O
and O
let O
them O
judge O
which O
question O
is O
better O
in O
terms O
of O
answerability O
and O
overall O
quality O
. O

section 13
id pdf2json/2021.acl-long.355.pdf.json
see O
more O
human O
evaluation O
details O
in O
appendix O
a.3 O
. O

section 13
id pdf2json/2021.acl-long.355.pdf.json
we O
compare O
our O
flow O
model O
to O
the O
pure O
encoder-decoder O
baseline O
as O
well O
as O
the O
flowseq O
model O
( O
ma O
et O
al. O
, O
2019 O
) O
in O
human O
evaluation O
. O

section 13
id pdf2json/2021.acl-long.355.pdf.json
as O
shown O
in O
the O
last O
rows O
in O
table O
5 O
and O
table O
6 O
, O
humans O
favor O
our O
model O
more O
than O
the O
baseline O
in O
both O
tasks O
, O
which O
indicates O
our O
flow O
model O
indeed O
provides O
useful O
latent O
features O
for O
better O
generation O
. O

section 13
id pdf2json/2021.acl-long.355.pdf.json
plus O
, O
our O
model O
also O
always O
outperforms O
flowseq O
. O

section 13
id pdf2json/2021.acl-long.355.pdf.json
we O
conjecture O
that O
it O
is O
because O
flowseq O
is O
non-autoregressive O
whereas O
our O
qg O
model O
is O
autoregressive O
. O

section 13
id pdf2json/2021.acl-long.355.pdf.json
neural O
machine O
translation O
. O

section 13
id pdf2json/2021.acl-long.355.pdf.json
we O
also O
test O
the O
effectiveness O
of O
our O
approach O
on O
a O
neural O
machine O
translation O
( O
nmt O
) O
task O
. O

section 13
id pdf2json/2021.acl-long.355.pdf.json
we O
first O
replicate O
lee O
et O
al O
. O

section 13
id pdf2json/2021.acl-long.355.pdf.json
( O
2018 O
) O
’ O
s O
transformer O
autoregressive O
model O
baseline O
, O
and O
then O
we O
add O
our O
flow O
architecture O
on O
top O
of O
it O
. O

section 13
id pdf2json/2021.acl-long.355.pdf.json
as O
shown O
in O
table O
7 O
, O
our O
proposed O
flow-aided O
mt O
model O
can O
improve O
the O
machine O
translation O
performance O
over O
the O
strong O
transformer O
baseline O
on O
the O
wmt16 O
( O
cettolo O
et O
al. O
, O
2012 O
) O
romanian O
to O
english O
translation O
task O
. O

section 13
id pdf2json/2021.acl-long.355.pdf.json
see O
a.7 O
for O
more O
details O
. O

section 13
id pdf2json/2021.acl-long.355.pdf.json
we O
hope O
5statistical O
significance O
is O
computed O
using O
the O
bootstrap O
test O
( O
efron O
and O
tibshirani O
, O
1994 O
) O
. O

section 13
id pdf2json/2021.acl-long.355.pdf.json
6we O
exclude O
those O
examples O
where O
the O
two O
models O
generate O
identical O
questions O
. O

section 13
id pdf2json/2021.acl-long.355.pdf.json
that O
these O
promising O
initial O
nmt O
results O
will O
also O
encourage O
the O
community O
to O
use O
continuous O
flow O
models O
for O
other O
nmt O
and O
nlg O
tasks O
. O

section 14
id pdf2json/2021.acl-long.355.pdf.json
as O
shown O
in O
table O
8 O
and O
table O
9 O
, O
using O
the O
augmented O
data O
generated O
by O
our O
language O
flow O
model O
( O
refers O
to O
our O
autoregressive O
language O
flow O
model O
) O
, O
we O
achieve O
significant O
performance O
improvements O
over O
strong O
baselines O
on O
both O
tvqa O
qa O
( O
lei O
et O
al. O
, O
2020 O
) O
( O
p O
< O
0.0001 O
) O
and O
squad O
qa O
( O
rajpurkar O
et O
al. O
, O
2016 O
) O
( O
p O
< O
0.0005 O
) O
for O
both O
em O
and O
f1 O
. O

section 14
id pdf2json/2021.acl-long.355.pdf.json
furthermore O
, O
when O
we O
add O
an O
lstm O
autoregressive O
decoder O
to O
our O
non-autoregressive O
encoder O
( O
referred O
to O
as O
language O
flow+ O
) O
and O
use O
it O
to O
perform O
data O
augmentation O
, O
we O
observe O
even O
slightly O
better O
results O
. O

section 14
id pdf2json/2021.acl-long.355.pdf.json
this O
may O
indicate O
the O
stronger O
encoding O
ability O
of O
our O
non-autoregressive O
model O
due O
to O
its O
multi-scale O
architecture O
. O

section 14
id pdf2json/2021.acl-long.355.pdf.json
mean- O
while O
, O
we O
compare O
to O
two O
other O
data O
augmentation O
techniques O
: O
paraphrasing O
( O
niu O
and O
bansal O
, O
2018 O
) O
and O
back-translation O
( O
sennrich O
et O
al. O
, O
2016 O
) O
. O

section 14
id pdf2json/2021.acl-long.355.pdf.json
note O
that O
for O
a O
fair O
comparison O
, O
we O
apply O
the O
same O
data O
filter O
and O
training O
schema O
for O
all O
data O
augmentation O
methods O
. O

section 14
id pdf2json/2021.acl-long.355.pdf.json
it O
can O
be O
seen O
that O
both O
methods O
perform O
worse O
than O
our O
language O
flow O
or O
language O
flow+ O
models O
. O

section 15
id pdf2json/2021.acl-long.355.pdf.json
we O
show O
some O
sample O
questions O
generated O
by O
our O
non-autoregressive O
and O
autoregressive O
flow O
models O
in O
table O
1 O
. O

section 15
id pdf2json/2021.acl-long.355.pdf.json
the O
autoregressive O
samples O
are O
better O
organized O
and O
grammatically O
sound O
, O
while O
non-autoregressive O
generation O
fails O
at O
the O
latter O
part O
of O
the O
sentence O
. O

section 15
id pdf2json/2021.acl-long.355.pdf.json
it O
might O
because O
the O
nonautoregressive O
structure O
has O
a O
weaker O
ability O
to O
model O
the O
temporal O
dependency O
during O
generation O
, O
which O
is O
consistent O
with O
the O
observations O
from O
previous O
works O
( O
ren O
et O
al. O
, O
2020 O
) O
. O

section 15
id pdf2json/2021.acl-long.355.pdf.json
to O
show O
that O
our O
model O
generates O
samples O
from O
a O
continuous O
space O
, O
we O
generate O
interpolation O
samples O
from O
our O
autoregressive O
flow O
model O
shown O
in O
table O
2 O
. O

section 15
id pdf2json/2021.acl-long.355.pdf.json
those O
samples O
are O
mostly O
grammatically O
sound O
and O
correctly O
reflect O
the O
intermediate O
content O
of O
the O
two O
interpolated O
sentences O
. O

section 15
id pdf2json/2021.acl-long.355.pdf.json
while O
variational O
autoencoder O
has O
the O
issue O
of O
ignoring O
latent O
space O
( O
li O
et O
al. O
, O
2019 O
) O
, O
our O
models O
do O
not O
suffer O
from O
this O
issue O
. O

section 15
id pdf2json/2021.acl-long.355.pdf.json
we O
introduced O
two O
types O
of O
language O
generation O
models O
in O
the O
paper O
: O
( O
1 O
) O
the O
autoregressive O
flow O
model O
( O
used O
in O
data O
augmentation O
tasks O
) O
and O
( O
2 O
) O
the O
model O
that O
uses O
flow O
latent O
features O
as O
extra O
input O
( O
e.g. O
, O
for O
qg O
tasks O
) O
. O

section 15
id pdf2json/2021.acl-long.355.pdf.json
our O
autoregressive O
flow O
model O
’ O
s O
decoder O
is O
the O
inverted O
version O
of O
its O
encoder O
with O
the O
same O
weights O
, O
so O
it O
ensures O
the O
decoder O
uses O
the O
latent O
features O
. O

section 15
id pdf2json/2021.acl-long.355.pdf.json
when O
we O
use O
flow O
latent O
features O
as O
extra O
inputs O
, O
it O
significantly O
improves O
qa O
performance O
( O
table O
5 O
and O
table O
6 O
) O
, O
which O
implies O
the O
latent O
features O
are O
usefully O
involved O
in O
generation O
. O

section 16
id pdf2json/2021.acl-long.355.pdf.json
we O
have O
proposed O
a O
language O
generative O
flow O
model O
with O
non-autoregressive O
and O
autoregressive O
variants O
. O

section 16
id pdf2json/2021.acl-long.355.pdf.json
the O
non-autoregressive O
flow O
model O
achieves O
strong O
performance O
on O
density O
estimation O
and O
helps O
improve O
question O
generation O
and O
machine O
translation O
by O
providing O
additional O
useful O
latent O
features O
to O
the O
decoder O
. O

section 16
id pdf2json/2021.acl-long.355.pdf.json
moreover O
, O
the O
autoregressive O
variant O
largely O
improves O
question O
answering O
by O
generating O
new O
contexts O
with O
noise O
injection O
. O

section 17
id pdf2json/2021.acl-long.355.pdf.json
we O
thank O
the O
reviewers O
for O
their O
helpful O
feedback O
. O

section 17
id pdf2json/2021.acl-long.355.pdf.json
this O
research O
is O
supported O
by O
nsf-career O
award O
1846185 O
, O
onr O
grant O
n00014-18-1-2871 O
, O
and O
aro-yip O
award O
# O
w911nf18-1-0336 O
. O

section 17
id pdf2json/2021.acl-long.355.pdf.json
the O
views O
contained O
in O
this O
article O
are O
those O
of O
the O
authors O
and O
not O
of O
the O
funding O
agency O
. O

section 19
id pdf2json/2021.acl-long.355.pdf.json
in O
this O
section O
, O
we O
introduce O
our O
experiment O
settings O
ranging O
from O
datasets O
usage O
, O
flow O
implementation O
details O
, O
question O
generation O
model O
, O
and O
data O
augmentation O
settings O
. O

section 19
id pdf2json/2021.acl-long.355.pdf.json
we O
use O
a O
fixed O
seed O
2020 O
for O
pytorch O
random O
seed O
. O

section 20
id pdf2json/2021.acl-long.355.pdf.json
we O
use O
pytorch O
1.5 O
( O
paszke O
et O
al. O
, O
2017 O
) O
to O
build O
our O
model O
. O

section 20
id pdf2json/2021.acl-long.355.pdf.json
we O
use O
nvidia O
geforce O
rtx O
2080ti O
and O
intel O
cpu O
( O
intel O
( O
r O
) O
xeon O
( O
r O
) O
silver O
4114 O
cpu O
@ O
2.20ghz O
) O
built O
on O
ubuntu O
16.01 O
for O
each O
training O
or O
inference O
process O
. O

section 21
id pdf2json/2021.acl-long.355.pdf.json
we O
use O
squadv1.1 O
( O
rajpurkar O
et O
al. O
, O
2016 O
) O
7 O
and O
tvqa O
( O
lei O
et O
al. O
, O
2018 O
) O
8 O
to O
perform O
our O
experiments O
, O
since O
squad O
is O
a O
widely O
explored O
qa O
and O
qg O
dataset O
, O
and O
tvqa O
is O
a O
video-based O
multimodal O
dataset O
with O
rich O
dialogue O
context O
. O

section 21
id pdf2json/2021.acl-long.355.pdf.json
therefore O
, O
question O
generation O
, O
context O
generation O
, O
and O
language O
density O
estimation O
, O
and O
data O
augmentation O
can O
be O
well O
performed O
and O
evaluated O
comprehensively O
on O
these O
two O
datasets O
and O
tasks O
. O

section 21
id pdf2json/2021.acl-long.355.pdf.json
tvqa O
consists O
of O
152,545 O
qa O
pairs O
from O
21,793 O
clips O
, O
spanning O
over O
460 O
hours O
of O
video O
. O

section 21
id pdf2json/2021.acl-long.355.pdf.json
the O
subtitles O
in O
tvqa O
dataset O
has O
time-stamp O
annotations O
of O
localized O
clip O
in O
the O
full O
subtitle O
clip O
. O

section 21
id pdf2json/2021.acl-long.355.pdf.json
the O
localized O
clip O
is O
the O
relevant O
interval O
of O
a O
clip O
for O
question O
answering O
. O

section 21
id pdf2json/2021.acl-long.355.pdf.json
in O
both O
the O
video O
question O
generation O
task O
and O
the O
context O
generation O
task O
for O
data O
augmentation O
, O
we O
use O
localized O
subtitles O
. O

section 21
id pdf2json/2021.acl-long.355.pdf.json
the O
tvqa O
context O
features O
are O
dialogues O
or O
video O
subtitles O
; O
hence O
data O
augmentation O
on O
this O
dataset O
should O
consider O
an O
additional O
frame-level O
dimension O
. O

section 21
id pdf2json/2021.acl-long.355.pdf.json
squad O
has O
over O
100,000 O
questions O
and O
23,215 O
paragraphs O
for O
the O
536 O
articles O
covering O
a O
wide O
range O
of O
topics O
. O

section 21
id pdf2json/2021.acl-long.355.pdf.json
we O
follow O
zhang O
and O
bansal O
( O
2019 O
) O
to O
split O
the O
development O
set O
of O
squadv1.1 O
( O
rajpurkar O
et O
al. O
, O
2016 O
) O
into O
two O
splits O
and O
show O
the O
result O
on O
the O
second O
split O
. O

section 22
id pdf2json/2021.acl-long.355.pdf.json
we O
tokenize O
the O
data O
to O
be O
used O
for O
both O
glove O
embedding O
and O
bert O
features O
extraction O
, O
and O
we O
add O
the O
start O
of O
sentence O
token O
and O
the O
end O
of O
sentence O
token O
for O
every O
input O
. O

section 23
id pdf2json/2021.acl-long.355.pdf.json
we O
generally O
follow O
previous O
work O
on O
evaluation O
metrics O
across O
density O
estimation O
, O
question O
generation O
, O
and O
question O
answering O
augmentation O
. O

section 23
id pdf2json/2021.acl-long.355.pdf.json
flow O
model O
. O

section 23
id pdf2json/2021.acl-long.355.pdf.json
for O
flow O
density O
estimation O
, O
we O
follow O
previous O
work O
( O
kingma O
and O
dhariwal O
, O
2018 O
; O
dinh O
et O
al. O
, O
2017 O
) O
to O
use O
negative O
log-likelihood O
for O
comparison O
. O

section 23
id pdf2json/2021.acl-long.355.pdf.json
question O
generation O
model O
. O

section 23
id pdf2json/2021.acl-long.355.pdf.json
we O
evaluate O
the O
generation O
quality O
by O
bleu4 O
( O
papineni O
et O
al. O
, O
7online O
link O
for O
squad O
: O
rajpurkar.github.io/ O
squad-explorer/explore/1.1/dev/ O
8online O
link O
for O
tvqa O
: O
tvqa.cs.unc.edu/ O
2002 O
) O
, O
meteor O
( O
lavie O
and O
agarwal O
, O
2007 O
) O
, O
and O
rouge-l O
( O
lin O
, O
2004 O
) O
to O
provide O
an O
insight O
into O
the O
performance O
of O
our O
model O
. O

section 23
id pdf2json/2021.acl-long.355.pdf.json
we O
also O
use O
amazon O
turk O
human O
evaluation O
that O
compares O
the O
baseline O
generation O
and O
the O
proposed O
model O
generation O
by O
proving O
a O
suitable O
qa O
context O
. O

section 23
id pdf2json/2021.acl-long.355.pdf.json
for O
squad O
qg O
, O
we O
present O
the O
article O
context O
, O
question O
pairs O
, O
and O
the O
answer O
for O
the O
users O
to O
select O
their O
preference O
in O
terms O
of O
answerability O
and O
overall O
quality O
of O
the O
question O
pair O
. O

section 23
id pdf2json/2021.acl-long.355.pdf.json
for O
tvqa O
qg O
, O
we O
present O
the O
video O
clip O
, O
subtitle O
context O
, O
question O
pairs O
, O
and O
answer O
candidates O
for O
the O
users O
to O
select O
their O
preference O
in O
terms O
of O
answerability O
and O
overall O
quality O
of O
the O
questions O
pair O
. O

section 23
id pdf2json/2021.acl-long.355.pdf.json
machine O
translation O
model O
. O

section 23
id pdf2json/2021.acl-long.355.pdf.json
we O
evaluate O
the O
generation O
quality O
by O
bleu O
( O
papineni O
et O
al. O
, O
2002 O
) O
to O
provide O
an O
insight O
into O
the O
performance O
of O
our O
model O
. O

section 23
id pdf2json/2021.acl-long.355.pdf.json
data O
augmentation O
model O
. O

section 23
id pdf2json/2021.acl-long.355.pdf.json
we O
use O
accuracy O
scores O
to O
evaluate O
tvqa O
qa O
model O
, O
and O
follow O
previous O
work O
( O
rajpurkar O
et O
al. O
, O
2016 O
) O
to O
use O
em O
( O
exact O
match O
) O
and O
f1 O
score O
to O
evaluate O
squad O
qa O
model O
. O

section 24
id pdf2json/2021.acl-long.355.pdf.json
the O
experiment O
on O
base O
flow O
models O
does O
not O
involve O
extensive O
hyperparameter O
search O
trials O
since O
flow O
models O
follow O
the O
principle O
: O
the O
deeper O
, O
the O
better O
. O

section 24
id pdf2json/2021.acl-long.355.pdf.json
we O
use O
small-sized O
flow O
models O
across O
different O
versions O
of O
( O
k=8 O
, O
l=3 O
, O
parameter O
number O
: O
128m O
for O
transformer O
module O
and O
196m O
for O
rnn O
module O
) O
flow O
models O
for O
ablation O
study O
. O

section 24
id pdf2json/2021.acl-long.355.pdf.json
the O
autoregressive O
flow O
model O
has O
k=24 O
, O
l=1 O
with O
approximately O
the O
same O
parameter O
number O
, O
130m O
, O
by O
changing O
the O
nonlinear O
functions O
complexity O
for O
a O
fair O
comparison O
. O

section 24
id pdf2json/2021.acl-long.355.pdf.json
based O
on O
the O
sequence O
length O
distribution O
of O
the O
dataset O
, O
we O
designate O
the O
maximum O
fixed O
flow O
sequence O
length O
for O
tvqa-subtitles O
as O
64 O
, O
squadparagraphs O
as O
256 O
. O

section 24
id pdf2json/2021.acl-long.355.pdf.json
we O
set O
l=3 O
or O
4 O
for O
all O
experiments O
while O
changing O
the O
number O
of O
flow O
steps O
k O
with O
a O
multiple O
of O
8 O
. O

section 24
id pdf2json/2021.acl-long.355.pdf.json
while O
it O
follows O
that O
the O
more O
k O
, O
the O
better O
, O
setting O
l O
to O
a O
reasonable O
value O
is O
essential O
as O
each O
block O
will O
reduce O
sequence O
length O
by O
half O
. O

section 24
id pdf2json/2021.acl-long.355.pdf.json
therefore O
, O
l O
is O
set O
according O
to O
the O
length O
of O
the O
input O
. O

section 24
id pdf2json/2021.acl-long.355.pdf.json
the O
discretization O
, O
d O
, O
in O
the O
negative O
loglikelihood O
loss O
function O
( O
eq.6 O
) O
is O
set O
to O
2n O
, O
where O
n=6 O
. O

section 24
id pdf2json/2021.acl-long.355.pdf.json
noise O
, O
u O
, O
is O
set O
as O
gaussian O
sample O
with O
α O
= O
12m O
, O
where O
m=6 O
. O

section 24
id pdf2json/2021.acl-long.355.pdf.json
we O
follow O
previous O
work O
( O
kingma O
and O
dhariwal O
, O
2018 O
) O
to O
use O
bits O
per O
dimension O
to O
regularize O
the O
negative O
log-likelihood O
loss O
, O
formulated O
as O
l O
( O
m O
log O
( O
2 O
) O
) O
, O
where O
m O
represents O
the O
dimension O
of O
input O
. O

section 24
id pdf2json/2021.acl-long.355.pdf.json
we O
use O
a O
learning O
rate O
between O
1e-4 O
and O
1e5 O
, O
specifically O
5e-5 O
, O
to O
achieve O
stable O
and O
faster O
convergence O
( O
with O
adam O
optimizer O
( O
kingma O
and O
ba O
, O
2015 O
) O
, O
beta1=0.9 O
, O
beta2=0.999 O
) O
. O

section 24
id pdf2json/2021.acl-long.355.pdf.json
with O
prior O
knowledge O
of O
adam O
optimizer O
, O
we O
perform O
5 O
trials O
to O
test O
learning O
rate O
( O
1e-3 O
, O
5e-4 O
. O

section 24
id pdf2json/2021.acl-long.355.pdf.json
1e-4 O
. O

section 24
id pdf2json/2021.acl-long.355.pdf.json
5e-5 O
, O
1e-5 O
) O
to O
find O
the O
fastest O
convergence O
rate O
. O

section 24
id pdf2json/2021.acl-long.355.pdf.json
the O
average O
training O
time O
is O
50 O
epochs O
for O
a O
( O
k=8 O
l=3 O
parameter O
number O
: O
128m O
for O
transformer O
module O
and O
196m O
for O
rnn O
module O
) O
flow O
model O
, O
as O
each O
epoch O
takes O
20 O
minutes O
. O

section 24
id pdf2json/2021.acl-long.355.pdf.json
inference O
for O
one O
sample O
takes O
around O
0.01s O
. O

section 24
id pdf2json/2021.acl-long.355.pdf.json
the O
density O
estimation O
by O
the O
lstm O
model O
we O
use O
for O
baseline O
comparison O
in O
nll O
and O
qg O
models O
is O
designed O
to O
be O
well O
defined O
as O
a O
density O
estimation O
model O
. O

section 24
id pdf2json/2021.acl-long.355.pdf.json
flow O
density O
estimation O
models O
with O
no O
invertibility O
are O
not O
well-defined O
. O

section 24
id pdf2json/2021.acl-long.355.pdf.json
therefore O
, O
we O
mimic O
a O
model O
structure O
that O
the O
transformation O
is O
through O
only O
non-singular O
matrix O
weight O
to O
obtain O
an O
arithmetically O
invertible O
model O
. O

section 25
id pdf2json/2021.acl-long.355.pdf.json
the O
experiment O
on O
question O
generation O
models O
does O
not O
involve O
extensive O
hyperparameter O
search O
trials O
, O
as O
the O
proposed O
model O
has O
stable O
convergence O
under O
varied O
circumstances O
. O

section 25
id pdf2json/2021.acl-long.355.pdf.json
we O
take O
the O
last O
latent O
space O
output O
of O
the O
flow O
model O
as O
the O
features O
used O
for O
the O
qg O
model O
decoder O
or O
attention O
map O
. O

section 25
id pdf2json/2021.acl-long.355.pdf.json
we O
use O
( O
k=16 O
, O
l=3 O
parameter O
number O
: O
256m O
parameters O
for O
transformer O
module O
) O
flow O
models O
with O
transformer O
modules O
without O
autoregressive O
decoding O
for O
all O
the O
qg O
experiments O
. O

section 25
id pdf2json/2021.acl-long.355.pdf.json
the O
loss O
weight O
of O
λ1 O
is O
set O
1.0 O
; O
the O
weight O
will O
not O
significantly O
affect O
the O
result O
as O
long O
as O
it O
is O
set O
to O
a O
reasonably O
large O
value O
. O

section 25
id pdf2json/2021.acl-long.355.pdf.json
we O
use O
gradient O
descent O
with O
momentum O
optimizer O
( O
momentum O
= O
0.8 O
, O
lr O
= O
1e-3 O
) O
for O
both O
base O
model O
and O
flow O
model O
. O

section 25
id pdf2json/2021.acl-long.355.pdf.json
with O
prior O
knowledge O
of O
the O
sgd O
optimizer O
, O
we O
perform O
four O
trials O
to O
test O
the O
learning O
rate O
( O
1e-2 O
, O
5e-3 O
. O

section 25
id pdf2json/2021.acl-long.355.pdf.json
1e-3 O
. O

section 25
id pdf2json/2021.acl-long.355.pdf.json
5e-4 O
) O
to O
find O
the O
fastest O
convergence O
rate O
and O
stable O
training O
. O

section 25
id pdf2json/2021.acl-long.355.pdf.json
we O
employ O
zhang O
and O
bansal O
( O
2019 O
) O
’ O
s O
baseline O
qg O
model O
, O
which O
is O
a O
robust O
encoder-decoder O
attention O
generation O
network O
with O
a O
maxout O
pointer O
network O
and O
self-gated O
attention O
( O
zhao O
et O
al. O
, O
2018 O
) O
for O
both O
tasks.9 O
we O
use O
pretrained O
bert O
( O
devlin O
9maxout O
pointer O
is O
not O
used O
in O
the O
tvqa O
qg O
model O
since O
et O
al. O
, O
2019 O
) O
hidden O
features O
with O
768 O
dimensions O
by O
a O
small O
uncased O
bert O
model O
to O
replace O
glove O
embedding O
to O
make O
the O
baseline O
stronger O
to O
show O
that O
the O
flow O
model O
can O
still O
improve O
well O
on O
a O
strong O
baseline O
. O

section 25
id pdf2json/2021.acl-long.355.pdf.json
the O
average O
training O
time O
is O
20 O
epochs O
for O
the O
joint O
training O
of O
the O
qg O
model O
and O
the O
( O
k=16 O
l=3 O
) O
flow O
model O
, O
as O
each O
epoch O
takes O
50 O
minutes O
. O

section 25
id pdf2json/2021.acl-long.355.pdf.json
inference O
for O
one O
sample O
takes O
around O
0.03s O
. O

section 26
id pdf2json/2021.acl-long.355.pdf.json
for O
the O
machine O
translation O
dataset O
wmt16 O
, O
the O
source O
and O
target O
languages O
share O
the O
same O
set O
of O
subword O
embeddings O
. O

section 26
id pdf2json/2021.acl-long.355.pdf.json
the O
maximum O
text O
length O
is O
set O
to O
64 O
and O
we O
filter O
out O
all O
data O
that O
is O
above O
this O
range O
. O

section 26
id pdf2json/2021.acl-long.355.pdf.json
we O
use O
( O
k=4 O
, O
l=3 O
with O
transformer O
module O
) O
non-autoregressive O
flow O
models O
with O
transformer O
modules O
for O
all O
the O
data O
augmentation O
experiments O
. O

section 26
id pdf2json/2021.acl-long.355.pdf.json
we O
use O
adam O
optimizer O
( O
kingma O
and O
ba O
, O
2015 O
) O
with O
beta1=0.9 O
, O
beta2=0.999 O
, O
and O
a O
learning O
rate O
5e-5 O
for O
flow O
model O
training O
. O

section 27
id pdf2json/2021.acl-long.355.pdf.json
the O
experiment O
on O
context O
generation O
models O
generally O
follows O
empirical O
hyperparameter O
settings O
. O

section 27
id pdf2json/2021.acl-long.355.pdf.json
we O
use O
( O
k=32 O
, O
l=4 O
parameter O
number O
: O
512m O
parameters O
for O
transformer O
module O
) O
autoregressive O
flow O
models O
with O
transformer O
modules O
for O
all O
the O
data O
augmentation O
experiments O
. O

section 27
id pdf2json/2021.acl-long.355.pdf.json
we O
use O
adam O
optimizer O
( O
kingma O
and O
ba O
, O
2015 O
) O
with O
beta1=0.9 O
, O
beta2=0.999 O
, O
and O
a O
learning O
rate O
5e-5 O
for O
flow O
model O
training O
and O
an O
empirically O
stable O
learning O
rate O
3e-4 O
for O
attention O
decoder O
training O
. O

section 27
id pdf2json/2021.acl-long.355.pdf.json
we O
set O
z0 O
to O
a O
gaussian O
noise O
sample O
with O
mean O
0.0 O
and O
variance O
1.0 O
during O
training O
and O
variance O
0.5 O
during O
inference O
. O

section 27
id pdf2json/2021.acl-long.355.pdf.json
for O
inference O
variance O
tuning O
, O
we O
start O
from O
variance O
1.0 O
and O
gradually O
decrease O
by O
0.1 O
until O
0.1 O
to O
manually O
check O
which O
setting O
has O
generated O
samples O
with O
reliable O
quality O
and O
diversity O
suitable O
for O
robust O
data O
augmentation O
. O

section 27
id pdf2json/2021.acl-long.355.pdf.json
the O
average O
training O
time O
is O
100 O
epochs O
for O
the O
( O
k=32 O
l=4 O
parameter O
number O
: O
512m O
) O
augmentation O
flow O
model O
, O
as O
each O
epoch O
takes O
30 O
minutes O
. O

section 27
id pdf2json/2021.acl-long.355.pdf.json
inference O
for O
one O
sample O
takes O
around O
0.5s O
. O

section 27
id pdf2json/2021.acl-long.355.pdf.json
base O
qa O
model O
. O

section 27
id pdf2json/2021.acl-long.355.pdf.json
we O
use O
model O
, O
backbone O
+ O
attn O
. O

section 27
id pdf2json/2021.acl-long.355.pdf.json
sup O
. O

section 27
id pdf2json/2021.acl-long.355.pdf.json
+ O
temp O
. O

section 27
id pdf2json/2021.acl-long.355.pdf.json
sup O
. O

section 27
id pdf2json/2021.acl-long.355.pdf.json
+ O
local O
( O
stage O
) O
with O
glove O
embeddings O
, O
developed O
in O
tvqa+ O
dataset O
( O
lei O
et O
al. O
, O
2020 O
) O
as O
the O
qa O
baseline O
for O
tvqa O
data O
the O
number O
of O
words O
out O
of O
vocabulary O
is O
small O
. O

section 27
id pdf2json/2021.acl-long.355.pdf.json
augmentation O
model O
. O

section 27
id pdf2json/2021.acl-long.355.pdf.json
we O
use O
the O
bert O
baseline O
( O
devlin O
et O
al. O
, O
2019 O
) O
for O
squad O
qa O
( O
rajpurkar O
et O
al. O
, O
2016 O
) O
; O
this O
bert O
baseline O
is O
pretrained O
and O
uncased O
with O
768 O
base O
dimension O
and O
finetuned O
on O
the O
squad O
dataset O
. O

section 27
id pdf2json/2021.acl-long.355.pdf.json
these O
two O
models O
are O
also O
used O
as O
data O
filters O
. O

section 27
id pdf2json/2021.acl-long.355.pdf.json
data O
augmentation O
strategies O
. O

section 27
id pdf2json/2021.acl-long.355.pdf.json
the O
training O
schemes O
are O
crucial O
for O
context O
generation O
since O
the O
tvqa O
model O
has O
heavy O
dependence O
on O
the O
subtitles O
and O
squad O
model O
on O
the O
paragraphs O
: O
similar O
to O
zhang O
and O
bansal O
( O
2019 O
) O
’ O
s O
strategies O
, O
we O
obtain O
approximately O
ten O
times O
the O
amount O
of O
augmented O
data O
than O
the O
original O
amount O
, O
and O
filter O
them O
to O
obtain O
approximately O
40 O
% O
of O
augmented O
data O
to O
be O
used O
for O
training O
. O

section 27
id pdf2json/2021.acl-long.355.pdf.json
we O
set O
a O
probability O
, O
0.5 O
, O
for O
replacing O
the O
original O
data O
with O
newly O
generated O
filtered O
data O
for O
each O
batch O
in O
training O
. O

section 27
id pdf2json/2021.acl-long.355.pdf.json
for O
tvqa O
data O
augmentation O
, O
we O
generate O
localized O
subtitles O
and O
replace O
the O
corresponding O
part O
in O
non-localized O
full-subtitles O
. O

section 27
id pdf2json/2021.acl-long.355.pdf.json
for O
squad O
data O
augmentation O
, O
we O
generate O
trunks O
of O
paragraphs O
that O
do O
not O
contain O
answers O
to O
replace O
the O
corresponding O
trunks O
in O
the O
original O
paragraphs O
. O

section TITLE
id pdf2json/2021.acl-long.102.pdf.json
common O
sense O
beyond O
english O
: O
evaluating O
and O
improving O
multilingual O
language O
models O
for O
commonsense O
reasoning O

section ABSTRACT
id pdf2json/2021.acl-long.102.pdf.json
commonsense O
reasoning O
research O
has O
so O
far O
been O
limited O
to O
english O
. O

section ABSTRACT
id pdf2json/2021.acl-long.102.pdf.json
we O
aim O
to O
evaluate O
and O
improve O
popular O
multilingual O
language O
models O
( O
ml-lms O
) O
to O
help O
advance O
commonsense O
reasoning O
( O
csr O
) O
beyond O
english O
. O

section ABSTRACT
id pdf2json/2021.acl-long.102.pdf.json
we O
collect O
the O
mickey O
corpus O
, O
consisting O
of O
561k O
sentences O
in O
11 O
different O
languages O
, O
which O
can O
be O
used O
for O
analyzing O
and O
improving O
mllms O
. O

section ABSTRACT
id pdf2json/2021.acl-long.102.pdf.json
we O
propose O
mickey O
probe O
, O
a O
languageagnostic O
probing O
task O
for O
fairly O
evaluating O
the O
common O
sense O
of O
popular O
ml-lms O
across O
different O
languages O
. O

section ABSTRACT
id pdf2json/2021.acl-long.102.pdf.json
in O
addition O
, O
we O
also O
create O
two O
new O
datasets O
, O
x-csqa O
and O
x-codah O
, O
by O
translating O
their O
english O
versions O
to O
15 O
other O
languages O
, O
so O
that O
we O
can O
evaluate O
popular O
ml-lms O
for O
cross-lingual O
commonsense O
reasoning O
. O

section ABSTRACT
id pdf2json/2021.acl-long.102.pdf.json
to O
improve O
the O
performance O
beyond O
english O
, O
we O
propose O
a O
simple O
yet O
effective O
method O
— O
multilingual O
contrastive O
pretraining O
( O
mcp O
) O
. O

section ABSTRACT
id pdf2json/2021.acl-long.102.pdf.json
it O
significantly O
enhances O
sentence O
representations O
, O
yielding O
a O
large O
performance O
gain O
on O
both O
benchmarks O
( O
e.g. O
, O
+2.7 O
% O
accuracy O
for O
x-csqa O
over O
xlm-rl O
) O
. O

section 0
id pdf2json/2021.acl-long.102.pdf.json
proceedings O
of O
the O
59th O
annual O
meeting O
of O
the O
association O
for O
computational O
linguistics O
and O
the O
11th O
international O
joint O
conference O
on O
natural O
language O
processing O
, O
pages O
1274–1287 O
august O
1–6 O
, O
2021 O
. O

section 0
id pdf2json/2021.acl-long.102.pdf.json
©2021 O
association O
for O
computational O
linguistics O
1274 O

section 1
id pdf2json/2021.acl-long.102.pdf.json
understanding O
natural O
language O
relies O
heavily O
on O
commonsense O
reasoning O
( O
csr O
) O
, O
which O
is O
the O
process O
of O
making O
inferences O
with O
commonsense O
knowledge O
. O

section 1
id pdf2json/2021.acl-long.102.pdf.json
commonsense O
knowledge O
is O
the O
set O
of O
general O
facts O
that O
reflect O
our O
natural O
understanding O
of O
the O
physical O
world O
and O
human O
behavior O
, O
which O
are O
usually O
seen O
as O
an O
implicit O
background O
when O
people O
communicate O
with O
each O
other O
using O
languages O
. O

section 1
id pdf2json/2021.acl-long.102.pdf.json
it O
is O
thus O
of O
vital O
importance O
to O
evaluate O
and O
improve O
the O
commonsense O
reasoning O
capability O
of O
language O
models O
( O
lms O
) O
, O
towards O
building O
general O
natural O
language O
understanding O
( O
nlu O
) O
systems O
( O
davis O
and O
marcus O
, O
2015 O
) O
. O

section 1
id pdf2json/2021.acl-long.102.pdf.json
1we O
release O
our O
code O
and O
data O
at O
the O
project O
website O
: O
https O
: O
//inklab.usc.edu/xcsr/ O
. O

section 1
id pdf2json/2021.acl-long.102.pdf.json
many O
recent O
benchmark O
datasets O
and O
probing O
methods O
have O
been O
proposed O
to O
evaluate O
machine O
common O
sense O
. O

section 1
id pdf2json/2021.acl-long.102.pdf.json
as O
shown O
in O
figure O
1 O
, O
the O
lama O
probe O
( O
petroni O
et O
al. O
, O
2019 O
) O
is O
for O
analyzing O
lms O
’ O
zero-shot O
commonsense O
recalling O
ability O
; O
commonsenseqa O
( O
csqa O
) O
( O
talmor O
et O
al. O
, O
2019 O
) O
is O
instead O
a O
multiple-choice O
qa O
task O
that O
needs O
fine-tuning O
; O
codah O
( O
chen O
et O
al. O
, O
2019 O
) O
and O
swag O
( O
zellers O
et O
al. O
, O
2018 O
) O
focus O
on O
the O
ability O
to O
complete O
the O
most O
plausible O
scenes O
. O

section 1
id pdf2json/2021.acl-long.102.pdf.json
however O
, O
all O
these O
works O
have O
been O
limited O
only O
to O
english O
. O

section 1
id pdf2json/2021.acl-long.102.pdf.json
consequently O
, O
follow-up O
analysis O
and O
reasoning O
methods O
developed O
( O
lin O
et O
al. O
, O
2019 O
; O
feng O
et O
al. O
, O
2020 O
; O
lin O
et O
al. O
, O
2020 O
) O
also O
focus O
only O
on O
english O
lms O
like O
bert O
( O
devlin O
et O
al. O
, O
2019 O
) O
. O

section 1
id pdf2json/2021.acl-long.102.pdf.json
such O
english-centric O
trend O
of O
commonsense O
reasoning O
studies O
not O
only O
limits O
our O
research O
scope O
, O
but O
also O
tends O
to O
exacerbate O
english-specific O
bias O
that O
might O
prevent O
future O
methods O
from O
generalizing O
beyond O
english O
( O
ponti O
et O
al. O
, O
2020 O
) O
. O

section 1
id pdf2json/2021.acl-long.102.pdf.json
it O
is O
of O
pressing O
urgency O
for O
the O
community O
to O
develop O
nlu O
systems O
that O
can O
serve O
all O
languages O
in O
the O
world O
to O
bridge O
the O
gap O
between O
different O
cultures O
and O
eliminate O
language O
barriers O
( O
hu O
et O
al. O
, O
2020 O
) O
, O
and O
multilingual O
language O
models O
( O
mllms O
) O
, O
such O
as O
xlm-r O
( O
conneau O
et O
al. O
, O
2020 O
) O
, O
are O
among O
the O
most O
promising O
tools O
to O
achieve O
this O
ambitious O
goal O
. O

section 1
id pdf2json/2021.acl-long.102.pdf.json
although O
ml-lms O
have O
been O
evaluated O
in O
a O
few O
nlu O
tasks O
, O
e.g. O
, O
xnli O
( O
conneau O
et O
al. O
, O
2018 O
) O
and O
xtemre O
( O
hu O
et O
al. O
, O
2020 O
) O
, O
it O
is O
still O
relatively O
unclear O
how O
ml-lms O
perform O
in O
commonsense O
reasoning O
tasks O
, O
due O
to O
the O
lack O
of O
1 O
) O
dedicated O
methods O
for O
probing O
common O
sense O
in O
ml-lms O
and O
2 O
) O
multilingual O
benchmark O
datasets O
for O
commonsense O
reasoning O
. O

section 1
id pdf2json/2021.acl-long.102.pdf.json
to O
analyze O
how O
much O
common O
sense O
mllms O
already O
have O
without O
any O
tuning O
, O
we O
propose O
mickeyprobe O
, O
a O
zero-shot O
probing O
task O
. O

section 1
id pdf2json/2021.acl-long.102.pdf.json
it O
tasks O
a O
ml-lm O
to O
rank O
a O
set O
of O
contrastive O
assertions O
( O
i.e. O
, O
declarative O
sentences O
) O
in O
the O
same O
language O
by O
their O
commonsense O
plausibility O
, O
for O
which O
we O
use O
pseudo-likelihood O
( O
pll O
) O
( O
salazar O
et O
al. O
, O
2020 O
) O
as O
a O
proxy O
. O

section 1
id pdf2json/2021.acl-long.102.pdf.json
unlike O
the O
lama O
probe O
, O
it O
can O
study O
multi-token O
concepts O
which O
are O
ubiquitous O
in O
some O
non-english O
languages O
. O

section 1
id pdf2json/2021.acl-long.102.pdf.json
in O
addition O
, O
it O
fairly O
compares O
performance O
across O
different O
languages O
via O
a O
language-invariant O
evaluation O
protocol O
. O

section 1
id pdf2json/2021.acl-long.102.pdf.json
alongside O
the O
probing O
task O
, O
we O
also O
create O
mickeycorpus O
, O
a O
large-scale O
multilingual O
dataset O
, O
consisting O
of O
561k O
sentences O
in O
11 O
different O
languages O
. O

section 1
id pdf2json/2021.acl-long.102.pdf.json
our O
experiments O
reveal O
that O
there O
are O
always O
large O
discrepancies O
across O
different O
languages O
in O
the O
tested O
ml-lms O
, O
and O
different O
mllms O
show O
very O
different O
language O
preferences O
. O

section 1
id pdf2json/2021.acl-long.102.pdf.json
beyond O
supervision-free O
analysis O
of O
ml-lms O
, O
we O
also O
study O
their O
performance O
in O
commonsense O
reasoning O
tasks O
, O
such O
as O
csqa O
and O
codah O
, O
within O
a O
cross-lingual O
transfer O
setting O
( O
i.e. O
, O
trained O
on O
english O
data O
and O
tested O
on O
other O
languages O
) O
. O

section 1
id pdf2json/2021.acl-long.102.pdf.json
we O
find O
that O
existing O
ml-lms O
tend O
to O
have O
much O
lower O
accuracy O
in O
commonsense O
reasoning O
beyond O
english O
. O

section 1
id pdf2json/2021.acl-long.102.pdf.json
we O
conjecture O
a O
major O
common O
weakness O
of O
existing O
ml-lms O
is O
that O
their O
pretraining O
stages O
do O
not O
have O
a O
proper O
sentence-level O
objective O
. O

section 1
id pdf2json/2021.acl-long.102.pdf.json
therefore O
, O
we O
propose O
multilingual O
contrastive O
pre-training O
( O
mcp O
) O
, O
which O
tasks O
a O
mllm O
to O
select O
the O
correct O
assertion O
out O
of O
a O
set O
of O
n O
contrastive O
assertions O
in O
n O
different O
languages O
. O

section 1
id pdf2json/2021.acl-long.102.pdf.json
we O
re-format O
mickeycorpus O
by O
sampling O
across O
languages O
and O
thus O
form O
a O
dedicated O
pre-training O
corpus O
for O
the O
mcp O
task O
. O

section 1
id pdf2json/2021.acl-long.102.pdf.json
to O
fairly O
evaluate O
different O
ml-lms O
and O
validate O
the O
effectiveness O
of O
mcp O
, O
we O
create O
x-csqa O
and O
xcodah O
, O
two O
cross-lingual O
commonsense O
reasoning O
datasets O
by O
translating O
their O
english O
versions O
to O
15 O
other O
languages2 O
, O
including O
low-resource O
ones O
such O
as O
swahili O
( O
sw O
) O
and O
urdu O
( O
ur O
) O
. O

section 1
id pdf2json/2021.acl-long.102.pdf.json
experiments O
show O
that O
the O
proposed O
mcp O
objective O
indeed O
significantly O
improves O
the O
performance O
of O
state-ofthe-art O
ml-lms O
in O
cross-lingual O
commonsense O
reasoning O
. O

section 1
id pdf2json/2021.acl-long.102.pdf.json
our O
contributions O
are O
as O
follows O
: O
• O
resources O
. O

section 1
id pdf2json/2021.acl-long.102.pdf.json
we O
collect O
a O
large O
multilingual O
parallel O
corpus O
, O
mickeycorpus O
, O
consisting O
of O
561k O
sentences O
in O
11 O
languages O
, O
which O
can O
be O
used O
for O
analyzing O
and O
improving O
ml-lms O
. O

section 1
id pdf2json/2021.acl-long.102.pdf.json
we O
also O
create O
x-csqa O
and O
x-codah O
, O
two O
cross-lingual O
csr O
benchmarks O
in O
16 O
languages O
, O
for O
question O
answering O
and O
scene O
completion O
, O
respectively O
. O

section 1
id pdf2json/2021.acl-long.102.pdf.json
• O
evaluation O
and O
analysis O
. O

section 1
id pdf2json/2021.acl-long.102.pdf.json
we O
analyze O
multiple O
popular O
ml-lms O
with O
mickeyprobe O
, O
a O
language-invariant O
, O
zero-shot O
task O
for O
probing O
common O
sense O
in O
ml-lms O
; O
we O
also O
evaluate O
them O
on O
x-csqa O
and O
x-codah O
in O
a O
cross-lingual O
transfer O
setting O
. O

section 1
id pdf2json/2021.acl-long.102.pdf.json
• O
method O
to O
improve O
ml-lms O
. O

section 1
id pdf2json/2021.acl-long.102.pdf.json
we O
propose O
multilingual O
contrastive O
pretraining O
, O
a O
simple O
and O
effective O
sentence-level O
pretext O
task O
for O
enhancing O
ml-lms O
in O
cross-lingual O
commonsense O
reasoning O
, O
which O
significantly O
improves O
the O
state-of-the-art O
ml-lms O
in O
crosslingual O
commonsense O
reasoning O
. O

section 2
id pdf2json/2021.acl-long.102.pdf.json
in O
this O
section O
, O
we O
introduce O
important O
concepts O
, O
background O
knowledge O
, O
and O
related O
work O
before O
we O
present O
our O
work O
in O
following O
sections O
. O

section 3
id pdf2json/2021.acl-long.102.pdf.json
a O
multilingual O
language O
model O
( O
ml-lm O
) O
aims O
to O
produce O
text O
representations O
for O
multiple O
languages O
in O
a O
unified O
embedding O
space O
. O

section 3
id pdf2json/2021.acl-long.102.pdf.json
one O
of O
the O
unique O
advantages O
of O
ml-lms O
is O
their O
potential O
ability O
to O
perform O
zero-shot O
cross-lingual O
transfer O
— O
a O
model O
trained O
( O
or O
fine-tuned O
) O
on O
data O
in O
one O
language O
( O
usually O
english O
) O
can O
be O
directly O
used O
in O
other O
languages O
as O
well O
without O
further O
fine-tuning O
. O

section 3
id pdf2json/2021.acl-long.102.pdf.json
improving O
ml-lms O
is O
thus O
believed O
as O
one O
of O
the O
most O
promising O
approach O
towards O
multilingual O
nlu O
at O
scale O
. O

section 3
id pdf2json/2021.acl-long.102.pdf.json
mbert O
( O
devlin O
2the O
16 O
languages O
for O
x-csqa O
and O
x-codah O
: O
{ O
en O
, O
zh O
, O
de O
, O
es O
, O
fr O
, O
it O
, O
jap O
, O
nl O
, O
pl O
, O
pt O
, O
ru O
, O
ar O
, O
vi O
, O
hi O
, O
sw O
, O
ur O
} O
. O

section 3
id pdf2json/2021.acl-long.102.pdf.json
et O
al. O
, O
2019 O
) O
is O
simply O
the O
bert O
model O
( O
devlin O
et O
al. O
, O
2019 O
) O
trained O
on O
multilingual O
corpora O
without O
specific O
designs O
about O
multilinguality O
. O

section 3
id pdf2json/2021.acl-long.102.pdf.json
the O
distil-mbert O
( O
d-mbert O
) O
( O
sanh O
et O
al. O
, O
2019 O
) O
is O
a O
smaller O
mbert O
trained O
by O
knowledge O
distillation O
. O

section 3
id pdf2json/2021.acl-long.102.pdf.json
conneau O
and O
lample O
( O
2019 O
) O
proposed O
xlm O
( O
-100 O
) O
, O
which O
is O
pretrained O
with O
both O
masked O
language O
modeling O
( O
mlm O
) O
and O
translation O
language O
modeling O
( O
tlm O
) O
. O

section 3
id pdf2json/2021.acl-long.102.pdf.json
conneau O
et O
al O
. O

section 3
id pdf2json/2021.acl-long.102.pdf.json
( O
2020 O
) O
further O
proposed O
xlm-r O
, O
which O
improves O
the O
xlm O
with O
a O
better O
sub-token O
vocabulary O
and O
highquality O
multilingual O
corpora O
( O
cc100 O
) O
. O

section 3
id pdf2json/2021.acl-long.102.pdf.json
we O
leave O
the O
analysis O
of O
recent O
seq2seq O
ml-lms O
, O
such O
as O
mbart O
( O
liu O
et O
al. O
, O
2020 O
) O
and O
mt5 O
( O
xue O
et O
al. O
, O
2021 O
) O
, O
as O
future O
work O
, O
because O
their O
architectures O
are O
significantly O
different O
from O
the O
other O
ml-lms O
. O

section 3
id pdf2json/2021.acl-long.102.pdf.json
note O
that O
the O
above O
ml-lms O
are O
pretrained O
only O
with O
token-level O
training O
objectives O
such O
as O
mlm O
( O
i.e. O
, O
recovering O
masked O
tokens O
in O
monolingual O
text O
) O
and O
tlm O
( O
i.e. O
, O
recovering O
masked O
tokens O
in O
a O
pair O
of O
parallel O
sentences O
in O
two O
different O
languages O
) O
. O

section 3
id pdf2json/2021.acl-long.102.pdf.json
however O
, O
most O
nlu O
tasks O
, O
including O
commonsense O
reasoning O
, O
highly O
rely O
on O
sentence-level O
representations O
. O

section 3
id pdf2json/2021.acl-long.102.pdf.json
we O
argue O
that O
a O
well-designed O
sentence-level O
pre-training O
objective O
should O
improve O
ml-lms O
for O
nlu O
tasks O
. O

section 3
id pdf2json/2021.acl-long.102.pdf.json
this O
intuition O
motivates O
us O
to O
propose O
a O
sentence-level O
pre-training O
objective O
— O
mcp O
( O
section O
5 O
) O
. O

section 4
id pdf2json/2021.acl-long.102.pdf.json
there O
are O
a O
few O
recent O
multilingual O
benchmarks O
for O
nlu O
tasks O
, O
e.g. O
, O
xtreme O
( O
hu O
et O
al. O
, O
2020 O
) O
, O
tydi O
qa O
( O
clark O
et O
al. O
, O
2020 O
) O
, O
and O
xglue O
( O
liang O
et O
al. O
, O
2020 O
) O
. O

section 4
id pdf2json/2021.acl-long.102.pdf.json
xtreme O
and O
xglue O
are O
unified O
large-scale O
multilingual O
multitask O
benchmarks O
, O
while O
ty-di O
qa O
focuses O
on O
the O
qa O
. O

section 4
id pdf2json/2021.acl-long.102.pdf.json
these O
existing O
cross-lingual O
benchmarks O
have O
not O
covered O
commonsense O
reasoning O
tasks O
, O
such O
as O
csqa O
( O
talmor O
et O
al. O
, O
2019 O
) O
, O
swag O
( O
zellers O
et O
al. O
, O
2018 O
) O
, O
and O
codah O
( O
chen O
et O
al. O
, O
2019 O
) O
. O

section 4
id pdf2json/2021.acl-long.102.pdf.json
csqa O
is O
a O
question O
answering O
task O
and O
the O
other O
two O
are O
scene O
completion O
tasks O
, O
while O
all O
have O
a O
multiple-choice O
selection O
objective O
, O
as O
shown O
in O
figure O
1 O
. O

section 4
id pdf2json/2021.acl-long.102.pdf.json
these O
benchmarks O
are O
widely O
used O
to O
evaluate O
lms O
for O
commonsense O
reasoning O
. O

section 4
id pdf2json/2021.acl-long.102.pdf.json
unfortunately O
, O
they O
are O
limited O
to O
english O
, O
not O
applicable O
to O
evaluate O
models O
of O
multilingual O
commonsense O
knowledge O
, O
which O
motivates O
us O
to O
create O
x-csqa O
and O
x-codah O
. O

section 4
id pdf2json/2021.acl-long.102.pdf.json
the O
goal O
of O
the O
recent O
xcopa O
( O
ponti O
et O
al. O
, O
2020 O
) O
dataset O
shares O
a O
similar O
goal O
, O
but O
it O
only O
focused O
on O
event-based O
causal O
reasoning O
in O
the O
scope O
of O
humans O
’ O
social O
behavior O
, O
which O
is O
thus O
arguably O
more O
culturally O
biased O
. O

section 4
id pdf2json/2021.acl-long.102.pdf.json
in O
contrast O
, O
the O
x-csqa O
and O
x-codah O
are O
mainly O
for O
evaluating O
general O
world O
knowledge O
and O
cover O
more O
fine-grained O
types O
of O
reasoning O
( O
e.g. O
, O
quantitative O
, O
negation O
) O
, O
and O
thus O
engage O
a O
more O
language-agnostic O
, O
comprehensive O
understanding O
of O
ml-lms O
about O
common O
sense O
. O

section 5
id pdf2json/2021.acl-long.102.pdf.json
the O
lama O
probe O
( O
petroni O
et O
al. O
, O
2019 O
) O
is O
the O
seminal O
work O
on O
probing O
for O
common O
sense O
in O
( O
english O
) O
language O
models O
. O

section 5
id pdf2json/2021.acl-long.102.pdf.json
it O
has O
a O
straightforward O
intuition O
: O
if O
a O
pretrained O
language O
model O
contains O
more O
commonsense O
knowledge O
, O
then O
it O
should O
be O
better O
at O
recalling O
a O
masked O
token O
in O
a O
commonsense O
assertion O
( O
e.g. O
, O
“ O
birds O
have O
[ O
mask O
] O
” O
) O
. O

section 5
id pdf2json/2021.acl-long.102.pdf.json
specifically O
, O
given O
a O
lama-probe O
sentence O
s O
and O
its O
masked O
token O
wt O
, O
a O
lm O
under O
testing O
uses O
all O
past O
and O
future O
tokens O
— O
s\t O
: O
= O
w1 O
, O
. O

section 5
id pdf2json/2021.acl-long.102.pdf.json
. O

section 5
id pdf2json/2021.acl-long.102.pdf.json
. O

section 5
id pdf2json/2021.acl-long.102.pdf.json
, O
wt O
1 O
, O
wt+1 O
, O
. O

section 5
id pdf2json/2021.acl-long.102.pdf.json
. O

section 5
id pdf2json/2021.acl-long.102.pdf.json
. O

section 5
id pdf2json/2021.acl-long.102.pdf.json
, O
w|s| O
. O

section 5
id pdf2json/2021.acl-long.102.pdf.json
as O
the O
input O
to O
rank O
all O
tokens O
in O
the O
vocabulary O
with O
the O
probability O
p O
wt O
| O
s\t O
via O
zero-shot O
inference O
. O

section 5
id pdf2json/2021.acl-long.102.pdf.json
one O
can O
evaluate O
the O
performance O
of O
recalling O
common O
sense O
by O
measuring O
the O
position O
of O
a O
correct O
token O
“ O
wing O
” O
in O
the O
ranked O
list O
. O

section 5
id pdf2json/2021.acl-long.102.pdf.json
that O
is O
, O
the O
lama O
probe O
method O
uses O
token-level O
probability O
as O
a O
proxy O
to O
probe O
for O
common O
sense O
in O
lms O
via O
ranking O
all O
tokens O
in O
their O
vocabularies O
. O

section 5
id pdf2json/2021.acl-long.102.pdf.json
this O
intuitive O
method O
, O
however O
, O
has O
several O
inherent O
limitations O
. O

section 5
id pdf2json/2021.acl-long.102.pdf.json
first O
, O
in O
many O
other O
languages O
, O
multi-token O
concepts O
are O
ubiquitous O
, O
for O
example O
, O
“ O
˛fü O
” O
( O
“ O
library O
” O
in O
simplified O
chinese O
) O
. O

section 5
id pdf2json/2021.acl-long.102.pdf.json
jiang O
et O
al O
. O

section 5
id pdf2json/2021.acl-long.102.pdf.json
( O
2020 O
) O
present O
several O
methods O
to O
decode O
multi-token O
entities O
so O
that O
they O
can O
adapt O
the O
lama O
probe O
to O
probe O
a O
lm O
for O
language-specific O
analysis O
. O

section 5
id pdf2json/2021.acl-long.102.pdf.json
it O
is O
however O
infeasible O
to O
use O
tokenlevel O
probing O
tasks O
if O
we O
want O
to O
analyze O
ml-lms O
across O
languages O
. O

section 5
id pdf2json/2021.acl-long.102.pdf.json
in O
addition O
, O
the O
evaluation O
metric O
of O
the O
lama O
probe O
could O
be O
unfair O
, O
because O
there O
can O
be O
many O
correct O
words O
for O
a O
masked O
position O
( O
e.g. O
, O
“ O
birds O
have O
legs/eyes O
” O
) O
. O

section 5
id pdf2json/2021.acl-long.102.pdf.json
the O
ranking O
metrics O
of O
the O
lama O
probe O
, O
however O
, O
tend O
to O
ignore O
these O
facts O
, O
resulting O
in O
a O
less O
trustworthy O
analysis O
. O

section 5
id pdf2json/2021.acl-long.102.pdf.json
the O
vocabulary-specific O
ranking O
is O
unfair O
when O
comparing O
across O
different O
languages O
, O
so O
they O
can O
have O
very O
different O
label O
space O
. O

section 5
id pdf2json/2021.acl-long.102.pdf.json
these O
limitations O
of O
the O
lama O
probe O
prevent O
us O
from O
analyzing O
common O
sense O
in O
ml-lm O
across O
topologically O
diverse O
languages O
. O

section 6
id pdf2json/2021.acl-long.102.pdf.json
the O
challenges O
of O
using O
the O
lama O
probe O
for O
probing O
common O
sense O
in O
ml-lms O
motivate O
us O
to O
propose O
a O
more O
suitable O
method O
for O
analyzing O
mllms O
, O
one O
that O
can O
fairly O
compare O
across O
a O
diverse O
set O
of O
languages O
. O

section 6
id pdf2json/2021.acl-long.102.pdf.json
we O
present O
mickeyprobe O
, O
a O
multilingual O
task O
for O
probing O
commonsense O
knowledge O
and O
analysis O
. O

section 6
id pdf2json/2021.acl-long.102.pdf.json
we O
design O
a O
languageagnostic O
probing O
task O
with O
a O
sentence-selection O
objective O
for O
analyzing O
common O
sense O
of O
a O
mllm O
: O
given O
a O
set O
of O
assertions O
( O
i.e. O
, O
declarative O
sentences O
) O
that O
have O
similar O
words O
and O
syntactic O
features O
, O
select O
the O
one O
with O
highest O
commonsense O
plausibility O
. O

section 6
id pdf2json/2021.acl-long.102.pdf.json
we O
present O
the O
task O
formulation O
in O
this O
section O
and O
then O
introduce O
how O
we O
collect O
the O
dedicated O
dataset O
in O
section O
4 O
. O

section 6
id pdf2json/2021.acl-long.102.pdf.json
notations O
. O

section 6
id pdf2json/2021.acl-long.102.pdf.json
we O
define O
a O
mickey O
probe O
m O
as O
a O
set O
of O
k O
assertions O
in O
the O
same O
language O
, O
where O
one O
and O
only O
one O
of O
them O
( O
say O
, O
mi O
) O
is O
the O
truth O
assertion O
with O
better O
commonsense O
plausibility O
than O
the O
other O
k O
1 O
ones O
. O

section 6
id pdf2json/2021.acl-long.102.pdf.json
each O
mickey O
probe O
m O
has O
multiple O
semantically O
equivalent O
versions O
in O
different O
languages O
. O

section 6
id pdf2json/2021.acl-long.102.pdf.json
let O
us O
denote O
a O
language O
by O
l O
2 O
l O
where O
l O
= O
{ O
en O
, O
fr O
, O
ru O
, O
zh O
, O
. O

section 6
id pdf2json/2021.acl-long.102.pdf.json
. O

section 6
id pdf2json/2021.acl-long.102.pdf.json
. O
} O

section 6
id pdf2json/2021.acl-long.102.pdf.json
and O
|l| O
is O
the O
number O
of O
languages O
of O
interest O
. O

section 6
id pdf2json/2021.acl-long.102.pdf.json
then O
, O
m O
l O
is O
the O
probe O
m O
in O
the O
language O
l. O
for O
example O
, O
m O
en O
and O
m O
fr O
denote O
the O
probes O
with O
the O
same O
meaning O
but O
in O
english O
( O
en O
) O
and O
french O
( O
fr O
) O
respectively O
. O

section 6
id pdf2json/2021.acl-long.102.pdf.json
we O
use O
m O
to O
denote O
a O
multilingual O
parallel O
dataset O
for O
mickeyprobe O
, O
which O
consists O
of O
t⇥|l|⇥k O
assertions O
. O

section 6
id pdf2json/2021.acl-long.102.pdf.json
t O
is O
the O
number O
of O
mickeyprobe O
items O
and O
each O
item O
has O
k O
assertions O
and O
|l| O
language O
. O

section 6
id pdf2json/2021.acl-long.102.pdf.json
finally O
, O
we O
can O
formally O
describe O
a O
multilingual O
parallel O
dataset O
m O
for O
mickeyprobe O
: O
8m O
2m O
, O
8 O
( O
lx O
, O
ly O
) O
2 O
l2 O
, O
8i O
2 O
nk O
, O
m O
lxi O
./ O
m O
ly O
i O
. O

section 6
id pdf2json/2021.acl-long.102.pdf.json
( O
1 O
) O
we O
use O
the O
notation O
./ O
to O
indicate O
two O
assertions O
in O
different O
languages O
( O
e.g. O
, O
lx O
and O
ly O
) O
are O
semantically O
equivalent O
to O
each O
other O
. O

section 6
id pdf2json/2021.acl-long.102.pdf.json
we O
leave O
the O
details O
of O
creating O
such O
an O
m O
in O
section O
4 O
. O

section 6
id pdf2json/2021.acl-long.102.pdf.json
commonsense O
probing O
task O
. O

section 6
id pdf2json/2021.acl-long.102.pdf.json
given O
a O
micky O
probe O
m O
in O
the O
dataset O
m O
, O
and O
suppose O
the O
index O
of O
the O
truth O
assertion O
to O
be O
t O
, O
a O
perfect O
multilingual O
language O
model O
would O
produce O
sentence O
probabilities O
such O
that O
it O
always O
gives O
the O
truth O
assertion O
m O
lt O
the O
highest O
probability O
among O
other O
candidates O
for O
every O
language O
. O

section 6
id pdf2json/2021.acl-long.102.pdf.json
8l O
2 O
l O
, O
8i O
2 O
nk O
, O
p O
( O
m O
li O
) O
p O
( O
m O
lt O
) O
. O

section 6
id pdf2json/2021.acl-long.102.pdf.json
( O
2 O
) O
it O
is O
still O
an O
open O
problem O
to O
properly O
compute O
sentence O
probabilities O
from O
masked O
language O
models O
, O
the O
recently O
proposed O
pseudo-loglikelihood O
scoring O
( O
plls O
) O
( O
salazar O
et O
al. O
, O
2020 O
) O
has O
shown O
promising O
results O
in O
many O
downstream O
nlp O
applications O
that O
need O
sentence O
re-ranking O
( O
e.g. O
, O
speech O
recognition O
, O
and O
translation O
) O
, O
suggesting O
it O
is O
a O
promising O
proxy O
of O
sentence O
probability O
. O

section 6
id pdf2json/2021.acl-long.102.pdf.json
given O
a O
sentence O
s O
, O
its O
pll O
is O
defined O
as O
: O
logp O
( O
s O
) O
= O
pll O
( O
s O
) O
: O
= O
|s|x O
i=1 O
logp O
wi O
| O
s\i O
( O
3 O
) O
that O
is O
, O
we O
individually O
mask O
each O
token O
wi O
at O
a O
time O
and O
use O
the O
remaining O
context O
s\i O
to O
get O
the O
probability O
of O
a O
word O
wi O
in O
the O
sentence O
s. O
finally O
, O
we O
aggregate O
them O
to O
approximate O
p O
( O
s O
) O
. O

section 6
id pdf2json/2021.acl-long.102.pdf.json
evaluation O
metric O
. O

section 6
id pdf2json/2021.acl-long.102.pdf.json
the O
evaluation O
metric O
for O
mickeyprobe O
over O
a O
multilingual O
parallel O
dataset O
m O
in O
a O
specific O
language O
l O
is O
defined O
as O
the O
overall O
hit O
@ O
k O
accuracy O
of O
the O
selection O
results O
hit O
@ O
k O
( O
l O
) O
=p O
m2m O
1 O
{ O
truth-rank O
( O
m O
l O
) O
k O
} O
/ O
|m| O
where O
truth-rank O
( O
m O
l O
) O
means O
the O
the O
position O
of O
the O
truth O
assertion O
m O
lt O
in O
m O
l O
sorted O
by O
their O
probabilities O
defined O
in O
eq O
. O

section 6
id pdf2json/2021.acl-long.102.pdf.json
( O
3 O
) O
. O

section 6
id pdf2json/2021.acl-long.102.pdf.json
the O
hit O
@ O
1 O
is O
just O
equivalent O
to O
the O
conventional O
accuracy O
. O

section 6
id pdf2json/2021.acl-long.102.pdf.json
advantages O
of O
mickeyprobe O
. O

section 6
id pdf2json/2021.acl-long.102.pdf.json
there O
are O
two O
key O
advantages O
of O
the O
mickeyprobe O
for O
evaluating O
ml-lms O
: O
( O
1 O
) O
the O
sentence-level O
probability O
can O
be O
more O
generally O
applied O
in O
languages O
besides O
english O
, O
comparing O
with O
the O
lama O
probe O
which O
only O
studies O
single-token O
english O
words O
. O

section 6
id pdf2json/2021.acl-long.102.pdf.json
( O
2 O
) O
the O
task O
formulation O
creates O
a O
relatively O
closed-ended O
setting O
, O
such O
that O
we O
can O
use O
a O
language-independent O
evaluation O
metric O
to O
fairly O
compare O
across O
various O
languages O
within O
a O
mllm O
and O
compare O
across O
various O
ml-lms O
for O
a O
particular O
language O
. O

section 6
id pdf2json/2021.acl-long.102.pdf.json
in O
addition O
, O
we O
can O
see O
lama O
probe O
as O
a O
monolingual O
, O
word-level O
version O
of O
the O
more O
general O
mickeyprobe O
: O
the O
lama O
probe O
is O
when O
l O
= O
{ O
en O
} O
, O
and O
{ O
m O
en O
} O
= O
m O
2 O
m O
is O
a O
huge O
number O
of O
k O
assertions O
( O
i.e. O
, O
the O
vocabulary O
size O
) O
— O
a O
fixed O
[ O
mask O
] O
is O
replaced O
by O
all O
tokens O
in O
the O
vocabulary O
. O

section 7
id pdf2json/2021.acl-long.102.pdf.json
we O
present O
a O
procedure O
for O
automatically O
creating O
a O
multilingual O
parallel O
dataset O
m O
for O
the O
probing O
task O
mickeyprobe O
. O

section 7
id pdf2json/2021.acl-long.102.pdf.json
our O
collected O
corpus O
, O
named O
mickeycorpus O
, O
has O
561k O
sentences O
in O
11 O
languages O
( O
t O
=10.2k O
, O
k=5 O
, O
|l|=11 O
) O
. O

section 8
id pdf2json/2021.acl-long.102.pdf.json
for O
the O
correct O
commonsense O
assertions O
in O
english O
, O
we O
have O
an O
existing O
resource O
, O
the O
omcs O
corpus O
( O
singh O
et O
al. O
, O
2002 O
) O
which O
contains O
humanwritten O
sentences O
in O
english O
that O
describe O
commonsense O
facts O
. O

section 8
id pdf2json/2021.acl-long.102.pdf.json
each O
assertion O
can O
be O
used O
as O
a O
m O
ent O
and O
we O
perform O
perturbations O
on O
it O
to O
create O
the O
other O
k O
1 O
distractor O
assertions O
( O
i.e. O
, O
false O
candidates O
) O
, O
yielding O
an O
m O
en O
example O
. O

section 8
id pdf2json/2021.acl-long.102.pdf.json
inspired O
by O
bert-attack O
method O
( O
li O
et O
al. O
, O
2020 O
) O
, O
we O
use O
a O
simple O
method O
to O
generate O
false O
assertions O
that O
are O
semantically O
related O
and O
syntactically O
similar O
to O
the O
truth O
assertions O
. O

section 8
id pdf2json/2021.acl-long.102.pdf.json
given O
a O
correct O
assertion O
, O
we O
first O
randomly O
sample O
a O
few O
( O
1 O
⇠ O
3 O
) O
words O
with O
a O
part-of-speech O
tag O
as O
noun O
, O
verb O
, O
or O
adjective O
, O
and O
replace O
them O
with O
[ O
mask O
] O
. O

section 8
id pdf2json/2021.acl-long.102.pdf.json
then O
, O
we O
use O
a O
beam-search O
style O
method O
to O
decode O
the O
[ O
mask O
] O
tokens O
one O
by O
one O
from O
left O
to O
right O
. O

section 8
id pdf2json/2021.acl-long.102.pdf.json
to O
ensure O
that O
the O
distractors O
are O
less O
plau- O
sible O
, O
we O
limit O
the O
decoding O
steps O
to O
only O
sample O
tokens O
that O
ranks O
between O
200th⇠300th O
. O

section 8
id pdf2json/2021.acl-long.102.pdf.json
we O
repeat O
the O
above O
procedure O
multiple O
times O
with O
different O
sets O
of O
[ O
mask O
] O
tokens O
. O

section 8
id pdf2json/2021.acl-long.102.pdf.json
then O
, O
we O
use O
stanza O
( O
qi O
et O
al. O
, O
2020 O
) O
to O
remove O
distractors O
that O
have O
sequences O
of O
pos O
tags O
or O
morphological O
features O
different O
from O
the O
truth O
assertions O
. O

section 8
id pdf2json/2021.acl-long.102.pdf.json
finally O
, O
we O
sample O
k O
1 O
of O
them O
as O
the O
distractors O
. O

section 9
id pdf2json/2021.acl-long.102.pdf.json
we O
use O
bidirectional O
translation O
with O
the O
marianmt O
models O
( O
junczys-dowmunt O
et O
al. O
, O
2018 O
) O
pretrained O
on O
the O
opus O
corpora O
( O
tiedemann O
, O
2016 O
) O
. O

section 9
id pdf2json/2021.acl-long.102.pdf.json
we O
translate O
all O
english O
probes O
to O
the O
25 O
languages O
that O
has O
models O
in O
both O
directions O
and O
then O
translate O
them O
back O
to O
english O
. O

section 9
id pdf2json/2021.acl-long.102.pdf.json
as O
the O
outputs O
from O
these O
models O
might O
contain O
noise O
and O
errors O
, O
we O
compute O
the O
semantic O
similarities O
( O
i.e. O
, O
cosine O
similarity O
) O
between O
the O
original O
m O
en O
and O
the O
backtranslated O
mx-en O
via O
the O
sentencebert O
( O
reimers O
and O
gurevych O
, O
2019 O
) O
model O
. O

section 9
id pdf2json/2021.acl-long.102.pdf.json
to O
ensure O
the O
quality O
and O
fair O
comparisons O
, O
we O
set O
a O
similarity O
threshold O
as O
0.75 O
and O
keep O
the O
intersections O
of O
probes O
in O
all O
languages O
. O

section 9
id pdf2json/2021.acl-long.102.pdf.json
considering O
some O
languages O
tend O
to O
have O
translations O
of O
lower O
quality O
, O
we O
finally O
choose O
the O
best O
10 O
languages O
to O
build O
the O
mickey O
probe O
dataset O
for O
our O
analysis O
, O
yielding O
10k O
examples O
in O
each O
language O
and O
10.2k*5*11 O
⇡ O
561k O
sentences O
in O
total O
. O

section 9
id pdf2json/2021.acl-long.102.pdf.json
the O
language O
set O
l O
= O
{ O
en O
, O
de O
, O
fr O
, O
ru O
, O
es O
, O
hi O
, O
vi O
, O
bg O
, O
zh O
, O
nl O
, O
it O
} O
. O

section 9
id pdf2json/2021.acl-long.102.pdf.json
note O
that O
our O
purpose O
of O
checking O
the O
backtranslation O
quality O
here O
is O
mainly O
to O
only O
keep O
the O
high-quality O
translations O
for O
all O
language O
pairs O
that O
we O
considered O
. O

section 9
id pdf2json/2021.acl-long.102.pdf.json
conventional O
metrics O
, O
e.g. O
, O
blue O
score O
( O
papineni O
et O
al. O
, O
2002 O
) O
, O
which O
focus O
on O
the O
exact O
word O
match O
, O
are O
thus O
less O
suitable O
: O
given O
the O
original O
sentence O
“ O
i O
have O
a O
book O
” O
, O
the O
translation O
results O
“ O
i O
have O
a O
novel O
” O
and O
“ O
i O
have O
a O
tool O
” O
will O
be O
seen O
as O
equally O
wrong O
. O

section 9
id pdf2json/2021.acl-long.102.pdf.json
inspired O
by O
bertscore O
( O
zhang O
et O
al. O
, O
2020 O
) O
, O
the O
bt-cosine O
is O
based O
on O
sentencebert O
, O
which O
efficiently O
gives O
a O
higher O
score O
for O
the O
former O
and O
a O
lower O
score O
for O
the O
latter O
, O
due O
to O
the O
semantic O
relatedness O
between O
“ O
novel O
” O
and O
“ O
book. O
” O
we O
observed O
that O
most O
of O
our O
back-translations O
are O
in O
similar O
situations O
, O
and O
thus O
decide O
to O
use O
bt-cosine O
instead O
of O
others O
. O

section 10
id pdf2json/2021.acl-long.102.pdf.json
we O
now O
use O
the O
mickeycorpus O
to O
evaluate O
the O
5 O
pre-trained O
ml-lms O
introduced O
in O
section O
2.1 O
: O
d-mbert O
( O
sanh O
et O
al. O
, O
2019 O
) O
, O
mbert O
( O
devlin O
et O
al. O
, O
2019 O
) O
, O
xlm O
( O
conneau O
and O
lample O
, O
2019 O
) O
, O
xlm-rbase O
, O
and O
xlm-rlarge O
( O
conneau O
et O
al. O
, O
2020 O
) O
. O

section 10
id pdf2json/2021.acl-long.102.pdf.json
all O
these O
ml-lms O
pretraining O
objectives O
contain O
masked-word-prediction O
tasks O
, O
so O
we O
can O
easily O
use O
ppls O
( O
eq O
. O

section 10
id pdf2json/2021.acl-long.102.pdf.json
3 O
) O
to O
probe O
them O
a O
zeroshot O
, O
supervision-free O
manner O
with O
hit O
@ O
1 O
accuracy O
. O

section 10
id pdf2json/2021.acl-long.102.pdf.json
( O
the O
hit O
@ O
2 O
results O
are O
shown O
in O
appendix O
. O
) O

section 10
id pdf2json/2021.acl-long.102.pdf.json
we O
present O
a O
histogram O
in O
figure O
3 O
and O
show O
the O
concrete O
results O
in O
table O
1 O
. O

section 10
id pdf2json/2021.acl-long.102.pdf.json
we O
find O
that O
there O
are O
always O
large O
discrepancies O
across O
different O
languages O
in O
all O
tested O
ml-lms O
, O
which O
motivates O
us O
to O
analyze O
the O
following O
questions O
. O

section 10
id pdf2json/2021.acl-long.102.pdf.json
q1 O
: O
do O
different O
ml-lms O
have O
similar O
language O
preferences O
? O

section 10
id pdf2json/2021.acl-long.102.pdf.json
no O
. O

section 10
id pdf2json/2021.acl-long.102.pdf.json
we O
arrange O
the O
languages O
in O
all O
ml-lms O
with O
the O
same O
order O
for O
figure O
3 O
— O
the O
monotonically O
descending O
order O
of O
xlm-rl O
. O

section 10
id pdf2json/2021.acl-long.102.pdf.json
interestingly O
, O
we O
find O
that O
different O
ml-lms O
are O
good O
for O
different O
languages O
, O
resulting O
in O
a O
very O
diverse O
set O
of O
trends O
. O

section 10
id pdf2json/2021.acl-long.102.pdf.json
for O
example O
, O
xlm-rb O
, O
has O
a O
higher O
performance O
in O
it O
than O
zh O
and O
fr O
, O
unlike O
xlm-r O
l O
which O
are O
pre-trained O
on O
the O
same O
corpora O
with O
the O
same O
objectives O
. O

section 10
id pdf2json/2021.acl-long.102.pdf.json
mbert O
and O
d-mbert O
has O
stronger O
performance O
in O
fr O
than O
nl O
and O
de O
, O
unlike O
xlm O
and O
xlm-r. O
q2 O
: O
does O
length O
influence O
pll O
ranking O
? O

section 10
id pdf2json/2021.acl-long.102.pdf.json
not O
much O
. O

section 10
id pdf2json/2021.acl-long.102.pdf.json
the O
pll O
computation O
indeed O
tends O
to O
prefer O
shorter O
sequences O
( O
see O
eq O
. O

section 10
id pdf2json/2021.acl-long.102.pdf.json
3 O
) O
, O
so O
one O
may O
wonder O
if O
the O
length O
of O
assertions O
would O
influence O
the O
probing O
results O
. O

section 10
id pdf2json/2021.acl-long.102.pdf.json
the O
“ O
shortest O
” O
row O
in O
table O
1 O
presents O
the O
results O
when O
we O
always O
select O
the O
shortest O
assertion O
within O
a O
probe O
, O
instead O
of O
pll O
ranking O
. O

section 10
id pdf2json/2021.acl-long.102.pdf.json
the O
gaps O
between O
these O
scores O
and O
xlmr-l O
’ O
s O
suggest O
that O
the O
probing O
task O
indeed O
uses O
pll O
as O
a O
valid O
proxy O
for O
evaluating O
common O
sense O
based O
on O
sentence-level O
semantics O
. O

section 10
id pdf2json/2021.acl-long.102.pdf.json
q3 O
: O
is O
the O
translation O
quality O
a O
key O
factor O
? O

section 10
id pdf2json/2021.acl-long.102.pdf.json
we O
show O
“ O
bt-cosine O
” O
, O
the O
mean O
of O
the O
cosine O
scores O
between O
the O
original O
english O
sentences O
and O
the O
back-translated O
ones O
, O
and O
sort O
the O
table O
by O
these O
numbers O
. O

section 10
id pdf2json/2021.acl-long.102.pdf.json
the O
first O
5 O
languages O
, O
{ O
de O
, O
it O
, O
es O
, O
fr O
, O
nl O
} O
have O
the O
largest O
bt-cosine O
, O
i.e. O
, O
the O
best O
translation O
quality O
, O
and O
they O
indeed O
have O
better O
performances O
in O
general O
for O
xlm-r O
models O
. O

section 10
id pdf2json/2021.acl-long.102.pdf.json
however O
, O
although O
zh O
has O
a O
worse O
bt-score O
than O
vi O
, O
all O
mllms O
perform O
better O
in O
zh O
than O
vi O
. O

section 10
id pdf2json/2021.acl-long.102.pdf.json
thus O
, O
we O
believe O
the O
translation O
quality O
of O
mickeycorpus O
will O
not O
be O
a O
factor O
to O
influence O
our O
understanding O
of O
ml-lms O
. O

section 10
id pdf2json/2021.acl-long.102.pdf.json
consequently O
, O
this O
suggests O
that O
further O
study O
must O
depend O
on O
pre-training O
corpora O
of O
each O
ml-lm O
in O
different O
languages O
. O

section 10
id pdf2json/2021.acl-long.102.pdf.json
q4 O
: O
does O
the O
size O
of O
pre-training O
corpora O
matter O
? O

section 10
id pdf2json/2021.acl-long.102.pdf.json
we O
list O
the O
size O
of O
the O
monolingual O
corpus O
in O
each O
language O
for O
cc-100 O
that O
xlm-r O
are O
pretrained O
on O
( O
i.e. O
, O
the O
cc-size O
row O
) O
. O

section 10
id pdf2json/2021.acl-long.102.pdf.json
although O
ru O
has O
a O
much O
larger O
corpus O
than O
de O
, O
it O
, O
etc. O
, O
the O
xlmr O
performance O
in O
ru O
is O
much O
worse O
. O

section 10
id pdf2json/2021.acl-long.102.pdf.json
in O
addition O
, O
fr O
and O
nl O
have O
almost O
the O
same O
translation O
quality O
while O
fr O
’ O
s O
cc-size O
is O
twice O
the O
size O
of O
nl O
, O
but O
the O
performance O
in O
fr O
is O
still O
much O
worse O
than O
nl O
. O

section 10
id pdf2json/2021.acl-long.102.pdf.json
we O
conjecture O
this O
would O
be O
either O
due O
to O
the O
design O
of O
sub-token O
vocabulary O
or O
the O
text O
quality O
( O
instead O
of O
the O
size O
) O
of O
the O
cc-100 O
corpora O
. O

section 10
id pdf2json/2021.acl-long.102.pdf.json
further O
implications O
. O

section 10
id pdf2json/2021.acl-long.102.pdf.json
the O
benchmark O
results O
of O
five O
popular O
ml-lms O
on O
the O
mickeyprobe O
task O
over O
the O
mickeycorpus O
offer O
the O
initial O
and O
valuable O
understanding O
with O
a O
closer O
look O
at O
the O
commonsense O
knowledge O
of O
ml-lms O
by O
probing O
them O
in O
a O
unified O
evaluation O
protocol O
. O

section 10
id pdf2json/2021.acl-long.102.pdf.json
one O
can O
either O
compare O
a O
ml-lm O
across O
different O
languages O
or O
compare O
a O
certain O
language O
across O
mllms O
in O
table O
1 O
. O

section 10
id pdf2json/2021.acl-long.102.pdf.json
these O
comparable O
results O
support O
further O
analysis O
that O
can O
benefit O
the O
development O
of O
ml-lms O
in O
the O
future O
. O

section 10
id pdf2json/2021.acl-long.102.pdf.json
after O
all O
, O
even O
the O
best O
ml-lm O
xlm-rl O
also O
degrades O
much O
in O
other O
languages O
, O
and O
also O
perform O
slightly O
worse O
than O
robertal O
in O
en O
( O
93.4 O
% O
) O
. O

section 10
id pdf2json/2021.acl-long.102.pdf.json
we O
argue O
( O
cultureinvariant O
) O
common O
sense O
knowledge O
should O
be O
seen O
as O
an O
important O
way O
to O
connect O
multiple O
languages O
and O
thus O
better O
align O
them O
in O
a O
shared O
embedding O
space O
induced O
by O
a O
ml-lm O
. O

section 11
id pdf2json/2021.acl-long.102.pdf.json
in O
this O
section O
, O
we O
reformat O
the O
mickeyprobe O
so O
that O
we O
can O
reuse O
the O
mickeycorpus O
for O
improving O
the O
pre-trained O
ml-lms O
for O
commonsense O
reasoning O
beyond O
english O
. O

section 11
id pdf2json/2021.acl-long.102.pdf.json
we O
propose O
a O
multilingual O
contrastive O
pre-training O
( O
mcp O
) O
task O
that O
focuses O
on O
enhancing O
the O
sentence-level O
representation O
of O
ml-lms O
. O

section 11
id pdf2json/2021.acl-long.102.pdf.json
mcp O
improves O
a O
mllm O
in O
a O
multilingual O
, O
contrastive O
environment O
, O
where O
the O
model O
learns O
to O
select O
the O
assertion O
with O
the O
best O
commonsense O
plausibility O
from O
a O
set O
of O
contrastive O
sentences O
in O
different O
languages O
. O

section 11
id pdf2json/2021.acl-long.102.pdf.json
each O
mcp O
example O
is O
a O
set O
of O
multilingual O
assertions O
while O
each O
mickey O
probe O
is O
a O
monolingual O
set O
. O

section 11
id pdf2json/2021.acl-long.102.pdf.json
mcp O
dataset O
creation O
from O
m. O
we O
create O
pretraining O
examples O
for O
the O
mcp O
task O
by O
converting O
mickeyprobe O
examples O
, O
as O
shown O
in O
the O
steps O
illustrated O
in O
algorithm O
1 O
. O

section 11
id pdf2json/2021.acl-long.102.pdf.json
simply O
put O
, O
we O
reformat O
a O
k-way O
mickey O
probe O
m O
( O
k O
⇥ O
|l| O
assertions O
) O
to O
a O
mcp O
example O
by O
sampling O
a O
set O
of O
v O
candidate O
assertions O
in O
v O
different O
languages O
. O

section 11
id pdf2json/2021.acl-long.102.pdf.json
we O
convert O
all O
examples O
in O
the O
mickeycorpus O
m O
to O
build O
a O
new O
cross-lingual O
sentence-selection O
dataset O
c O
for O
learning O
the O
mcp O
task O
. O

section 11
id pdf2json/2021.acl-long.102.pdf.json
mcp O
learning O
. O

section 11
id pdf2json/2021.acl-long.102.pdf.json
given O
a O
mcp O
example O
c O
2 O
c O
, O
we O
append O
one O
dense O
linear O
layer O
f O
on O
top O
of O
a O
ml-lm O
with O
parameters O
denoted O
as O
⇥ml-lm O
for O
learning O
to O
predict O
the O
commonsense O
plausibility O
score O
of O
each O
assertion O
ci O
2 O
c O
as O
follows O
: O
hi O
= O
ml-lm O
( O
ci O
) O
. O

section 11
id pdf2json/2021.acl-long.102.pdf.json
[ O
cls O
] O
( O
4 O
) O
oi O
= O
f O
( O
hi O
; O
⇥f O
) O
( O
5 O
) O
zi O
= O
eoi O
pv=|c| O
j=1 O
e O
oj O
( O
6 O
) O
⇢ O
= O
vx O
i=1 O
1i O
log O
( O
zi O
) O
( O
7 O
) O
we O
first O
get O
the O
logit O
oi O
of O
each O
assertion O
by O
projecting O
its O
[ O
cls O
] O
embeddings O
hi O
to O
a O
logit O
oi O
via O
a O
dense O
layer O
f O
with O
parameters O
⇥f O
; O
then O
, O
we O
use O
softmax O
to O
normalize O
the O
logits O
as O
plausibility O
scores O
zi O
; O
finally O
, O
we O
compute O
the O
cross-entropy O
loss O
⇢ O
where O
1i=1 O
if O
ci O
is O
a O
correct O
assertion O
and O
0 O
otherwise O
. O

section 11
id pdf2json/2021.acl-long.102.pdf.json
we O
fine-tune O
{ O
⇥ml-lm O
, O
⇥f O
} O
to O
minimize O
the O
overall O
loss O
over O
the O
mcp O
dataset O
c O
. O

section 12
id pdf2json/2021.acl-long.102.pdf.json
in O
this O
section O
, O
we O
introduce O
the O
datasets O
, O
experimental O
setup O
, O
results O
, O
and O
our O
analysis O
. O

section 12
id pdf2json/2021.acl-long.102.pdf.json
algorithm O
1 O
: O
convert O
a O
mickey O
probe O
m O
to O
an O
example O
for O
the O
mcp O
task O
. O

section 12
id pdf2json/2021.acl-long.102.pdf.json
in O
: O
m O
2m O
/* O
is O
a O
probe O
that O
has O
|l| O
sub-sets O
; O
each O
sub-set O
m O
lx O
is O
a O
set O
of O
k O
assertions O
in O
the O
same O
language O
lx O
2 O
l. O
m O
lxt O
is O
always O
the O
truth O
. O

section 12
id pdf2json/2021.acl-long.102.pdf.json
*/ O
out O
: O
c O
/* O
a O
set O
of O
v O
assertions O
in O
different O
languages O
. O

section 12
id pdf2json/2021.acl-long.102.pdf.json
*/ O
remarks O
: O
n O
( O
x O
) O
is O
a O
function O
to O
randomly O
sample O
n O
unique O
elements O
from O
a O
set O
x O
. O

section 12
id pdf2json/2021.acl-long.102.pdf.json
1 O
la O
1 O
( O
l O
) O
/* O
pick O
an O
anchor O
language O
. O

section 12
id pdf2json/2021.acl-long.102.pdf.json
*/ O
2 O
c O
{ O
m O
lat O
} O
/* O
initiate O
w/ O
the O
truth O
assertion O
. O

section 12
id pdf2json/2021.acl-long.102.pdf.json
*/ O
/* O
iterate O
each O
sampled O
distractor O
language O
li O
. O

section 12
id pdf2json/2021.acl-long.102.pdf.json
*/ O
3 O
foreach O
li O
2 O
v O
1 O
( O
l O
la O
) O
do O
/* O
sample O
an O
index O
of O
distractor O
assertion O
. O

section 12
id pdf2json/2021.acl-long.102.pdf.json
*/ O
4 O
j O
1 O
( O
nk O
{ O
t O
} O
) O
/* O
add O
a O
distractor O
assertion O
as O
a O
candidate O
. O

section 12
id pdf2json/2021.acl-long.102.pdf.json
*/ O
5 O
c.add O
( O
m O
lij O
) O

section 13
id pdf2json/2021.acl-long.102.pdf.json
to O
evaluate O
ml-lms O
for O
commonsense O
reasoning O
in O
a O
cross-lingual O
zero-shot O
transfer O
setting O
, O
we O
create O
two O
benchmark O
datasets O
, O
namely O
x-csqa O
and O
x-codah O
. O

section 13
id pdf2json/2021.acl-long.102.pdf.json
table O
3 O
shows O
the O
statistics O
of O
the O
two O
datasets O
. O

section 13
id pdf2json/2021.acl-long.102.pdf.json
specifically O
, O
we O
use O
online O
commercial O
services O
such O
as O
deepl O
pro O
translate O
to O
collect O
high-quality O
translations O
of O
the O
examples O
in O
csqa O
and O
codah O
for O
15 O
languages O
other O
than O
english O
. O

section 13
id pdf2json/2021.acl-long.102.pdf.json
the O
size O
of O
codah O
is O
small O
( O
only O
2.7k O
) O
, O
so O
we O
use O
7k O
swag O
validation O
examples O
as O
additional O
training O
data O
which O
share O
the O
same O
formulation O
. O

section 13
id pdf2json/2021.acl-long.102.pdf.json
we O
discuss O
the O
reduction O
of O
cultural O
differences O
and O
quality O
control O
of O
automatic O
translations O
as O
well O
as O
other O
details O
in O
ethical O
considerations O
( O
the O
paragraph O
for O
cultural O
bias O
reduction O
) O
and O
appendix O
( O
a O
) O
. O

section 13
id pdf2json/2021.acl-long.102.pdf.json
as O
our O
goal O
is O
to O
evaluate O
different O
ml-lms O
( O
instead O
of O
different O
languages O
) O
in O
a O
unified O
evaluation O
protocol O
for O
cross-lingual O
commonsense O
reasoning O
, O
we O
argue O
that O
such O
automatically O
translated O
examples O
, O
although O
might O
contain O
noise O
, O
can O
serve O
as O
a O
starting O
benchmark O
for O
us O
to O
obtain O
meaningful O
analysis O
before O
more O
humantranslated O
datasets O
will O
be O
available O
in O
the O
future O
. O

section 14
id pdf2json/2021.acl-long.102.pdf.json
we O
focus O
on O
4 O
popular O
ml-lms O
that O
we O
introduced O
in O
section O
2.1 O
: O
mbert O
, O
xlm-100 O
, O
xlmrb O
and O
xlm-rl O
as O
well O
as O
our O
proposed O
mcp O
method O
. O

section 14
id pdf2json/2021.acl-long.102.pdf.json
for O
both O
tasks O
, O
we O
concatenate O
each O
prompt O
( O
the O
question O
or O
first O
sentence O
) O
and O
each O
of O
its O
options O
individually O
in O
the O
form O
of O
“ O
[ O
cls O
] O
prompt O
[ O
sep O
] O
optioni O
[ O
sep O
] O
” O
. O

section 14
id pdf2json/2021.acl-long.102.pdf.json
then O
, O
we O
fine-tune O
ml-lms O
over O
the O
english O
training O
dataset O
and O
test O
them O
on O
other O
languages O
. O

section 14
id pdf2json/2021.acl-long.102.pdf.json
why O
zero-shot O
cross-lingual O
transfer O
? O

section 14
id pdf2json/2021.acl-long.102.pdf.json
it O
is O
almost O
impossible O
to O
collect O
data O
in O
all O
languages O
that O
an O
nlu O
system O
might O
be O
used O
for O
. O

section 14
id pdf2json/2021.acl-long.102.pdf.json
therefore O
, O
prior O
works O
mainly O
focus O
on O
zero-shot O
crosslingual O
transfer O
( O
conneau O
et O
al. O
, O
2018 O
) O
, O
which O
is O
more O
meaningful O
and O
can O
offer O
lower-bound O
performance O
analysis O
. O

section 14
id pdf2json/2021.acl-long.102.pdf.json
it O
is O
also O
an O
ideal O
setting O
for O
studying O
csr O
because O
most O
commonsense O
facts O
are O
language-invariant O
. O

section 14
id pdf2json/2021.acl-long.102.pdf.json
thus O
, O
an O
englishfinetuned O
ml-lm O
for O
csr O
should O
be O
able O
to O
transfer O
its O
ability O
to O
a O
wide O
range O
of O
other O
languages O
as O
well O
. O

section 14
id pdf2json/2021.acl-long.102.pdf.json
furthermore O
, O
our O
goal O
of O
this O
paper O
is O
to O
evaluate O
and O
improve O
ml-lms O
, O
so O
translating O
back O
to O
english O
and O
then O
use O
an O
english-only O
lm O
is O
also O
not O
helpful O
towards O
to O
this O
end O
. O

section 15
id pdf2json/2021.acl-long.102.pdf.json
in O
table O
2 O
, O
we O
present O
the O
empirical O
results O
over O
x-codah O
and O
x-csqa O
for O
the O
ml-lms O
as O
well O
as O
two O
models O
enhanced O
by O
our O
proposed O
mcp O
method O
. O

section 15
id pdf2json/2021.acl-long.102.pdf.json
on O
both O
tasks O
, O
the O
xlm-rl O
performs O
the O
best O
with O
a O
large O
margin O
. O

section 15
id pdf2json/2021.acl-long.102.pdf.json
enhanced O
by O
the O
mcp O
method O
, O
both O
xlm-rb O
and O
xlm-rl O
see O
significant O
improvement O
( O
e.g. O
, O
2.7 O
% O
absolute O
improvement O
for O
xlm-rl O
on O
x-csqa-avg O
) O
. O

section 15
id pdf2json/2021.acl-long.102.pdf.json
can O
mcp O
’ O
s O
improvement O
generalize O
to O
unseen O
, O
low-resource O
languages O
? O

section 15
id pdf2json/2021.acl-long.102.pdf.json
note O
that O
mcp O
dataset O
only O
involves O
9 O
languages O
here O
, O
and O
there O
are O
6 O
languages O
that O
are O
totally O
unseen O
in O
the O
mcp O
training O
( O
i.e. O
, O
{ O
pl O
, O
ar O
, O
ja O
, O
pt O
, O
sw O
, O
ur O
} O
) O
. O

section 15
id pdf2json/2021.acl-long.102.pdf.json
the O
largest O
performance O
gain O
is O
in O
ru O
on O
x-csqa O
and O
vi O
on O
xcodah O
. O

section 15
id pdf2json/2021.acl-long.102.pdf.json
surprisingly O
, O
we O
find O
the O
improvements O
on O
them O
are O
also O
large O
for O
xlm-rl O
( O
e.g. O
, O
48.4 O
! O

section 15
id pdf2json/2021.acl-long.102.pdf.json
52.3 O
for O
ar O
) O
. O

section 15
id pdf2json/2021.acl-long.102.pdf.json
in O
addition O
, O
for O
the O
two O
low-resource O
languages O
sw O
and O
ur O
, O
mcp O
also O
brings O
2 O
⇠ O
3 O
percentage O
points O
of O
improvement O
for O
xlm-rl O
. O

section 15
id pdf2json/2021.acl-long.102.pdf.json
it O
is O
, O
however O
, O
not O
always O
the O
case O
for O
xlm-rb O
, O
which O
we O
conjecture O
tends O
to O
be O
more O
likely O
to O
overfit O
. O

section 15
id pdf2json/2021.acl-long.102.pdf.json
although O
ml-lms O
enjoy O
the O
merits O
of O
zeroshot O
cross-lingual O
transfer O
, O
their O
performances O
are O
usually O
worse O
than O
the O
english-only O
robertal O
on O
the O
en-test O
( O
70.4 O
% O
vs O
66.7 O
% O
for O
x-csqa O
) O
. O

section 15
id pdf2json/2021.acl-long.102.pdf.json
although O
mcp O
can O
mitigate O
the O
gap O
( O
70.4 O
% O
vs O
69.5 O
% O
) O
for O
x-csqa O
, O
there O
is O
still O
a O
large O
gap O
( O
81.6 O
% O
vs O
69.9 O
% O
) O
for O
x-codah O
. O

section 15
id pdf2json/2021.acl-long.102.pdf.json
we O
use O
fig O
. O

section 15
id pdf2json/2021.acl-long.102.pdf.json
4 O
to O
analyze O
how O
different O
categories O
of O
commonsense O
reasoning O
in O
codah O
( O
chen O
et O
al. O
, O
2019 O
) O
are O
diverse O
in O
different O
languages O
. O

section 15
id pdf2json/2021.acl-long.102.pdf.json
we O
find O
that O
others O
, O
reference O
, O
and O
negation O
have O
relatively O
smaller O
variances O
across O
different O
languages O
, O
as O
they O
are O
more O
language-invariant O
. O

section 15
id pdf2json/2021.acl-long.102.pdf.json
however O
, O
a O
few O
polysemous O
, O
idioms O
examples O
can O
be O
englishspecific O
which O
may O
not O
generalize O
to O
other O
languages O
. O

section 15
id pdf2json/2021.acl-long.102.pdf.json
more O
detailed O
analysis O
is O
in O
appendix O
. O

section 15
id pdf2json/2021.acl-long.102.pdf.json
from O
the O
curve O
of O
dev O
accuracy O
in O
figure O
5 O
, O
we O
see O
that O
mcp-enhanced O
xlm-r O
models O
are O
much O
more O
sample O
efficient O
and O
converge O
much O
faster O
than O
vanilla O
versions O
. O

section 15
id pdf2json/2021.acl-long.102.pdf.json
this O
suggests O
that O
the O
mcp O
, O
if O
used O
on O
a O
larger O
corpus O
with O
broader O
topics O
, O
can O
potentially O
produce O
a O
better O
ml-lm O
with O
more O
general O
usage O
, O
especially O
when O
only O
limited O
labelled O
is O
available O
. O

section 15
id pdf2json/2021.acl-long.102.pdf.json
our O
results O
on O
xnli-10 O
% O
( O
using O
10 O
% O
of O
the O
training O
data O
) O
( O
conneau O
et O
al. O
, O
2018 O
) O
show O
that O
mcp-enhanced O
xlm-rl O
has O
1.2 O
percent O
accuracy O
improvement O
on O
the O
average O
of O
15 O
languages O
. O

section 15
id pdf2json/2021.acl-long.102.pdf.json
as O
our O
focus O
in O
this O
paper O
is O
commonsense O
reasoning O
, O
we O
leave O
the O
study O
on O
other O
cross-lingual O
nlu O
tasks O
as O
future O
work O
. O

section 15
id pdf2json/2021.acl-long.102.pdf.json
importantly O
, O
our O
experiments O
imply O
that O
a O
proper O
( O
continual O
) O
pre-training O
task O
that O
has O
a O
( O
contrastive O
) O
sentence-level O
objective O
could O
improve O
both O
the O
final O
performance O
as O
well O
as O
learning O
efficiency O
. O

section 16
id pdf2json/2021.acl-long.102.pdf.json
we O
evaluate O
and O
improve O
popular O
multilingual O
language O
models O
( O
ml-lms O
) O
for O
advancing O
commonsense O
reasoning O
beyond O
english O
. O

section 16
id pdf2json/2021.acl-long.102.pdf.json
we O
propose O
the O
mickeyprobe O
, O
a O
language-agnostic O
probing O
task O
for O
analyzing O
common O
sense O
of O
ml-lms O
in O
a O
zero-shot O
manner O
. O

section 16
id pdf2json/2021.acl-long.102.pdf.json
with O
our O
proposed O
new O
benchmark O
datasets O
via O
automatic O
translation O
, O
x-csqa O
and O
x-codah O
, O
we O
evaluate O
ml-lms O
in O
a O
crosslingual O
transfer O
setting O
for O
commonsense O
reasoning O
. O

section 16
id pdf2json/2021.acl-long.102.pdf.json
we O
also O
improve O
the O
state-of-the-art O
ml-lm O
with O
a O
simple O
yet O
effective O
method O
— O
multilingual O
contrastive O
pre-training O
, O
which O
uses O
a O
sentencelevel O
objective O
to O
enhance O
sentence O
representations O
, O
yielding O
a O
significant O
performance O
gain O
. O

section 16
id pdf2json/2021.acl-long.102.pdf.json
all O
above O
work O
is O
based O
on O
mickeycorpus O
, O
which O
can O
be O
used O
as O
both O
a O
probing O
dataset O
and O
a O
pretraining O
corpus O
for O
analyzing O
and O
improving O
mllms O
. O

section 16
id pdf2json/2021.acl-long.102.pdf.json
we O
hope O
our O
resources O
and O
pre-training O
method O
for O
ml-lms O
can O
help O
the O
community O
advance O
commonsense O
reasoning O
beyond O
english O
. O

section 17
id pdf2json/2021.acl-long.102.pdf.json
this O
research O
is O
supported O
in O
part O
by O
the O
office O
of O
the O
director O
of O
national O
intelligence O
( O
odni O
) O
, O
intelligence O
advanced O
research O
projects O
activity O
( O
iarpa O
) O
, O
via O
contract O
no O
. O

section 17
id pdf2json/2021.acl-long.102.pdf.json
2019-19051600007 O
, O
the O
darpa O
mcs O
program O
under O
contract O
no O
. O

section 17
id pdf2json/2021.acl-long.102.pdf.json
n660011924033 O
with O
the O
united O
states O
office O
of O
naval O
research O
, O
the O
defense O
advanced O
research O
projects O
agency O
with O
award O
w911nf-19-20271 O
, O
and O
nsf O
sma O
18-29268 O
. O

section 17
id pdf2json/2021.acl-long.102.pdf.json
the O
views O
and O
conclusions O
contained O
herein O
are O
those O
of O
the O
authors O
and O
should O
not O
be O
interpreted O
as O
necessarily O
representing O
the O
official O
policies O
, O
either O
expressed O
or O
implied O
, O
of O
odni O
, O
iarpa O
, O
or O
the O
u.s. O
government O
. O

section 17
id pdf2json/2021.acl-long.102.pdf.json
we O
would O
like O
to O
thank O
all O
the O
collaborators O
in O
usc O
ink O
research O
lab O
and O
the O
reviewers O
for O
their O
constructive O
feedback O
on O
the O
work O
. O

section 17
id pdf2json/2021.acl-long.102.pdf.json
* O
ethical O
considerations O
resource O
copyright O
this O
work O
presents O
three O
new O
resources O
: O
mickeycorpus O
, O
x-codah O
, O
and O
x-csqa O
, O
which O
are O
multilingual O
extension O
of O
the O
omcs O
( O
singh O
et O
al. O
, O
2002 O
) O
3 O
, O
csqa O
( O
talmor O
et O
al. O
, O
2019 O
) O
4 O
, O
and O
codah O
( O
chen O
et O
al. O
, O
2019 O
) O
5 O
respectively O
. O

section 17
id pdf2json/2021.acl-long.102.pdf.json
all O
these O
three O
original O
sources O
of O
the O
data O
are O
publicly O
available O
for O
free O
, O
and O
we O
do O
not O
add O
any O
additional O
requirement O
for O
accessing O
our O
resources O
. O

section 17
id pdf2json/2021.acl-long.102.pdf.json
we O
will O
highlight O
the O
original O
sources O
of O
our O
data O
and O
ask O
users O
to O
cite O
the O
original O
papers O
when O
they O
use O
our O
extended O
versions O
for O
research O
. O

section 17
id pdf2json/2021.acl-long.102.pdf.json
3https O
: O
//github.com/commonsense/ O
conceptnet5/wiki/downloads O
4https O
: O
//www.tau-nlp.org/commonsenseqa O
5https O
: O
//github.com/websail-nu/codah O
cultural O
bias O
reduction O
like O
most O
most O
multilingual O
parallel O
resources O
, O
especially O
in O
general O
nlu O
domain O
, O
there O
exists O
potential O
data O
bias O
due O
to O
the O
barrier O
of O
languages O
as O
well O
as O
cultural O
differences O
( O
acharya O
et O
al. O
, O
2020 O
; O
lin O
et O
al. O
, O
2018 O
) O
, O
which O
could O
induce O
the O
labeling O
differences O
on O
the O
same O
situation O
. O

section 17
id pdf2json/2021.acl-long.102.pdf.json
for O
example O
, O
a O
question O
like O
“ O
what O
do O
people O
usually O
drink O
in O
the O
morning O
? O

section 17
id pdf2json/2021.acl-long.102.pdf.json
( O
coffee/tea/milk O
) O
” O
or O
“ O
when O
does O
a O
wedding O
usually O
start O
? O

section 17
id pdf2json/2021.acl-long.102.pdf.json
( O
morning/afternoon/evening O
) O
” O
might O
be O
answered O
very O
differently O
by O
people O
from O
different O
backgrounds O
and O
cultures O
, O
not O
to O
mention O
different O
languages O
. O

section 17
id pdf2json/2021.acl-long.102.pdf.json
the O
prior O
english O
commonsense O
resources O
which O
our O
datasets O
are O
built O
on O
are O
already O
possess O
such O
inherent O
bias O
, O
even O
with O
in O
the O
english O
language O
. O

section 17
id pdf2json/2021.acl-long.102.pdf.json
therefore O
, O
before O
we O
translate O
csqa O
and O
codah O
, O
we O
intentionally O
remove O
the O
examples O
that O
are O
either O
labeled O
as O
non-neutral O
by O
a O
pre-trained O
sentiment O
classifier O
, O
or O
contained O
any O
keywords O
that O
are O
relevant O
to O
social O
behavior O
( O
e.g. O
, O
weddings O
) O
. O

section 17
id pdf2json/2021.acl-long.102.pdf.json
we O
manually O
inspect O
test O
examples O
in O
x-csqa O
and O
x-codah O
in O
the O
english O
and O
chinese O
versions O
and O
have O
a O
strong O
confidence O
there O
is O
few O
strongly O
controversial O
example O
. O

section 17
id pdf2json/2021.acl-long.102.pdf.json
however O
, O
we O
admit O
that O
such O
reduction O
of O
cultural O
differences O
in O
common O
sense O
has O
not O
been O
systematically O
measured O
in O
this O
work O
for O
other O
languages O
. O

section 17
id pdf2json/2021.acl-long.102.pdf.json
application O
risks O
of O
cross-lingual O
csr O
. O

section 17
id pdf2json/2021.acl-long.102.pdf.json
the O
work O
also O
evaluates O
a O
few O
multilingual O
language O
models O
( O
ml-lms O
) O
for O
cross-lingual O
commonsense O
reasoning O
( O
xcsr O
) O
, O
and O
introduced O
a O
new O
model O
which O
outperforms O
them O
. O

section 17
id pdf2json/2021.acl-long.102.pdf.json
this O
raises O
the O
question O
of O
whether O
harm O
might O
arise O
from O
applications O
of O
xcsr—or O
more O
generally O
, O
since O
xcsr O
is O
intended O
as O
a O
step O
toward O
making O
english-only O
csr O
more O
applicable O
in O
other O
languages O
, O
whether O
harm O
might O
arise O
more O
generally O
from O
existing O
ml-lms O
. O

section 17
id pdf2json/2021.acl-long.102.pdf.json
among O
the O
risks O
that O
need O
to O
be O
considered O
in O
any O
deployment O
of O
nlp O
technology O
are O
that O
responses O
may O
be O
wrong O
or O
biased O
, O
in O
ways O
that O
would O
lead O
to O
improperly O
justified O
decisions O
. O

section 17
id pdf2json/2021.acl-long.102.pdf.json
although O
in O
our O
view O
the O
current O
technology O
is O
still O
relatively O
immature O
, O
and O
unlikely O
to O
be O
fielded O
in O
applications O
that O
would O
cause O
harm O
of O
this O
sort O
, O
it O
is O
desirable O
that O
ml-lms O
provide O
audit O
trails O
, O
and O
recourse O
so O
that O
their O
predictions O
can O
be O
explained O
to O
and O
critiqued O
by O
affected O
parties O
. O

section 18
id pdf2json/2021.acl-long.102.pdf.json
before O
we O
start O
the O
translation O
procedure O
, O
we O
first O
re-split O
the O
datasets O
of O
csqa O
and O
codah O
such O
that O
the O
test O
set O
examples O
in O
the O
english O
language O
do O
not O
contain O
controversial O
examples O
or O
culturerelated O
examples O
that O
would O
potentially O
cause O
cultural O
bias O
in O
our O
dataset O
. O

section 18
id pdf2json/2021.acl-long.102.pdf.json
please O
refer O
to O
the O
section O
of O
ethical O
considerations O
( O
following O
the O
conclusion O
) O
in O
the O
main O
paper O
for O
more O
details O
. O

section 18
id pdf2json/2021.acl-long.102.pdf.json
then O
, O
we O
use O
the O
deepl O
pro O
translation O
service O
to O
translate O
the O
10 O
languages O
: O
{ O
de O
, O
fr O
, O
es O
, O
pt O
, O
it O
, O
nl O
, O
pl O
, O
ru O
, O
jap O
, O
zh O
} O
and O
use O
google O
translation O
api O
to O
translate O
the O
others O
{ O
ar O
, O
sw O
, O
ur O
, O
vi O
, O
hi O
} O
. O

section 18
id pdf2json/2021.acl-long.102.pdf.json
we O
agree O
that O
ideally O
we O
should O
use O
human O
experts O
to O
translate O
the O
examples O
in O
csqa O
and O
codah O
, O
but O
the O
cost O
or O
building O
a O
large-scale O
multilingual O
dataset O
with O
the O
same O
scale O
of O
our O
datasets O
is O
extremely O
high O
– O
around O
10k O
usd O
. O

section 18
id pdf2json/2021.acl-long.102.pdf.json
as O
a O
matter O
of O
fact O
, O
most O
of O
the O
examples O
in O
csqa O
and O
codah O
are O
very O
easy O
and O
short O
sentences O
, O
and O
most O
of O
them O
can O
be O
well O
translated O
by O
modern O
commercial O
translation O
apis O
, O
because O
they O
usually O
have O
a O
hybrid O
system O
. O

section 18
id pdf2json/2021.acl-long.102.pdf.json
moreover O
, O
we O
choose O
the O
deepl O
online O
service O
because O
it O
has O
been O
reported O
by O
many O
individual O
media O
as O
the O
best O
choice O
. O

section 18
id pdf2json/2021.acl-long.102.pdf.json
to O
ensure O
the O
quality O
of O
the O
translation O
, O
we O
perform O
the O
translation O
for O
both O
directions O
and O
then O
use O
the O
same O
quality O
control O
method O
as O
we O
discussed O
in O
section O
4 O
for O
removing O
the O
examples O
that O
have O
lower O
cosine O
similarity O
between O
original O
english O
version O
and O
back-translated O
examples O
. O

section 18
id pdf2json/2021.acl-long.102.pdf.json
during O
the O
process O
, O
we O
manually O
went O
through O
the O
chinese O
versions O
to O
find O
a O
suitable O
threshold O
for O
taking O
the O
intersection O
— O
0.85 O
, O
which O
results O
in O
a O
comparable O
bt-cosine O
mean O
to O
the O
xnli O
dataset O
6 O
. O

section 19
id pdf2json/2021.acl-long.102.pdf.json
we O
summarize O
hyper-parameters O
that O
we O
used O
for O
training O
ml-lms O
on O
x-codah O
and O
x-csqa O
in O
6we O
sampled O
1k O
examples O
in O
the O
test O
set O
and O
follow O
the O
same O
procedure O
for O
the O
intersection O
language O
set O
. O

section 19
id pdf2json/2021.acl-long.102.pdf.json
table O
7 O
. O

section 19
id pdf2json/2021.acl-long.102.pdf.json
evaluation O
steps O
are O
equally O
100 O
for O
all O
models O
and O
datasets O
. O

section 19
id pdf2json/2021.acl-long.102.pdf.json
maximum O
sequence O
length O
is O
100 O
for O
x-codah O
and O
64 O
for O
x-csqa O
. O

section 19
id pdf2json/2021.acl-long.102.pdf.json
the O
batch O
size O
here O
refers O
to O
“ O
train O
batch O
size O
per O
device O
⇥ O
# O
gpus O
⇥ O
# O
gradient O
accumulation O
steps O
” O
. O

section 19
id pdf2json/2021.acl-long.102.pdf.json
note O
that O
the O
mcp O
methods O
use O
the O
exactly O
the O
same O
hyper-parameters O
which O
we O
have O
found O
optimal O
by O
tuning O
over O
the O
dev O
set O
. O

section 19
id pdf2json/2021.acl-long.102.pdf.json
the O
learning O
rates O
we O
tried O
for O
all O
models O
are O
from O
the O
range O
{ O
3e-5 O
, O
2e-5 O
, O
1e-5 O
, O
8e-6 O
, O
6e-6 O
, O
5e-6 O
} O
. O

section 19
id pdf2json/2021.acl-long.102.pdf.json
the O
warm O
up O
steps O
are O
selected O
from O
{ O
50 O
, O
100 O
, O
200 O
, O
300 O
, O
500 O
} O
. O

section 20
id pdf2json/2021.acl-long.102.pdf.json
table O
4 O
shows O
the O
model O
architectures O
and O
sizes O
that O
we O
used O
from O
( O
conneau O
et O
al. O
, O
2020 O
) O
. O

section 20
id pdf2json/2021.acl-long.102.pdf.json
we O
show O
the O
tokenization O
( O
tnz O
) O
used O
by O
each O
transformer O
model O
, O
the O
number O
of O
layers O
l O
, O
the O
number O
of O
hidden O
states O
of O
the O
model O
hm O
, O
the O
dimension O
of O
the O
feed-forward O
layer O
hff O
, O
the O
number O
of O
attention O
heads O
a O
, O
the O
size O
of O
the O
vocabulary O
v O
and O
the O
total O
number O
of O
parameters O
# O
params O
. O

section 21
id pdf2json/2021.acl-long.102.pdf.json
d.1 O
hit O
@ O
1 O
accuracy O
in O
histogram O
d.2 O
hit O
@ O
k O
accuracy O
of O
mickey O
probes O
table O
5 O
shows O
the O
hit O
@ O
2 O
accuracy O
of O
the O
five O
mllms O
for O
the O
mickeyprobe O
. O

section 21
id pdf2json/2021.acl-long.102.pdf.json
hit O
@ O
2 O
accuracy O
evaluates O
whether O
the O
models O
can O
rank O
the O
correct O
assertion O
within O
top O
2 O
. O

section 21
id pdf2json/2021.acl-long.102.pdf.json
unlike O
hit O
@ O
1 O
which O
only O
accepts O
best O
predictions O
, O
hit O
@ O
2 O
is O
more O
flexible O
. O

section 21
id pdf2json/2021.acl-long.102.pdf.json
thus O
, O
the O
performances O
in O
hit O
@ O
2 O
increase O
compared O
to O
the O
ones O
in O
hit O
@ O
1 O
. O

section 21
id pdf2json/2021.acl-long.102.pdf.json
we O
can O
see O
that O
the O
discrepancies O
across O
languages O
still O
exist O
. O

section 21
id pdf2json/2021.acl-long.102.pdf.json
d.3 O
categorized O
x-codah O
analysis O
please O
refer O
the O
codah O
( O
chen O
et O
al. O
, O
2019 O
) O
paper O
for O
the O
definition O
and O
concrete O
examples O
in O
each O
category O
. O

section 21
id pdf2json/2021.acl-long.102.pdf.json
we O
show O
benchmark O
results O
of O
mcp O
( O
xlm-rl O
) O
on O
x-codah O
within O
different O
carriages O
in O
table O
6 O
. O

section 21
id pdf2json/2021.acl-long.102.pdf.json
the O
rb O
stands O
for O
using O
the O
roberta-large O
model O
to O
fine-tune O
on O
the O
english O
x-codah O
dataset O
. O

section 21
id pdf2json/2021.acl-long.102.pdf.json
we O
find O
that O
the O
largest O
gaps O
in O
en O
are O
in O
the O
idioms O
and O
the O
others O
. O

section 21
id pdf2json/2021.acl-long.102.pdf.json
interestingly O
, O
we O
find O
that O
the O
quantities O
category O
is O
where O
mcp O
performs O
better O
than O
the O
roberta O
large O
. O

section 21
id pdf2json/2021.acl-long.102.pdf.json
model O
lr O
# O
epoch O
# O
wus O
bsz O
x-codah O
mbert O
3e-05 O
20 O
100 O
128 O
xlm-100 O
1e-05 O
20 O
100 O
64 O
xlm-r-b O
1e-05 O
20 O
100 O
128 O
xlm-r-l O
6e-06 O
10 O
100 O
64 O
mcp O
( O
xlm-r-b O
) O
1e-05 O
20 O
100 O
128 O
mcp O
( O
xlm-r-l O
) O
6e-06 O
10 O
100 O
64 O
x-csqa O

section TITLE
id pdf2json/2021.acl-long.521.pdf.json
language O
model O
augmented O
relevance O
score O

section ABSTRACT
id pdf2json/2021.acl-long.521.pdf.json
although O
automated O
metrics O
are O
commonly O
used O
to O
evaluate O
nlg O
systems O
, O
they O
often O
correlate O
poorly O
with O
human O
judgements O
. O

section ABSTRACT
id pdf2json/2021.acl-long.521.pdf.json
newer O
metrics O
such O
as O
bertscore O
have O
addressed O
many O
weaknesses O
in O
prior O
metrics O
such O
as O
bleu O
and O
rouge O
, O
which O
rely O
on O
=-gram O
matching O
. O

section ABSTRACT
id pdf2json/2021.acl-long.521.pdf.json
these O
newer O
methods O
, O
however O
, O
are O
still O
limited O
in O
that O
they O
do O
not O
consider O
the O
generation O
context O
, O
so O
they O
can O
not O
properly O
reward O
generated O
text O
that O
is O
correct O
but O
deviates O
from O
the O
given O
reference O
. O

section ABSTRACT
id pdf2json/2021.acl-long.521.pdf.json
in O
this O
paper O
, O
we O
propose O
language O
model O
augmented O
relevance O
score O
( O
mars O
) O
, O
a O
new O
context-aware O
metric O
for O
nlg O
evaluation O
. O

section ABSTRACT
id pdf2json/2021.acl-long.521.pdf.json
mars O
leverages O
off-the-shelf O
language O
models O
, O
guided O
by O
reinforcement O
learning O
, O
to O
create O
augmented O
references O
that O
consider O
both O
the O
generation O
context O
and O
available O
human O
references O
, O
which O
are O
then O
used O
as O
additional O
references O
to O
score O
generated O
text O
. O

section ABSTRACT
id pdf2json/2021.acl-long.521.pdf.json
compared O
with O
seven O
existing O
metrics O
in O
three O
common O
nlg O
tasks O
, O
mars O
not O
only O
achieves O
higher O
correlation O
with O
human O
reference O
judgements O
, O
but O
also O
differentiates O
well-formed O
candidates O
from O
adversarial O
samples O
to O
a O
larger O
degree O
. O

section 0
id pdf2json/2021.acl-long.521.pdf.json
proceedings O
of O
the O
59th O
annual O
meeting O
of O
the O
association O
for O
computational O
linguistics O
and O
the O
11th O
international O
joint O
conference O
on O
natural O
language O
processing O
, O
pages O
6677–6690 O
august O
1–6 O
, O
2021 O
. O

section 0
id pdf2json/2021.acl-long.521.pdf.json
©2021 O
association O
for O
computational O
linguistics O
6677 O
in O
this O
paper O
, O
we O
propose O
language O
model O
augmented O
relevance O
score O
( O
mars O
) O
, O
a O
new O
context-aware O
metric O
for O
nlg O
evaluation O
. O

section 0
id pdf2json/2021.acl-long.521.pdf.json
mars O
leverages O
off-the-shelf O
language O
models O
, O
guided O
by O
reinforcement O
learning O
, O
to O
create O
augmented O
references O
that O
consider O
both O
the O
generation O
context O
and O
available O
human O
references O
, O
which O
are O
then O
used O
as O
additional O
references O
to O
score O
generated O
text O
. O

section 0
id pdf2json/2021.acl-long.521.pdf.json
compared O
with O
seven O
existing O
metrics O
in O
three O
common O
nlg O
tasks O
, O
mars O
not O
only O
achieves O
higher O
correlation O
with O
human O
reference O
judgements O
, O
but O
also O
differentiates O
well-formed O
candidates O
from O
adversarial O
samples O
to O
a O
larger O
degree O
. O

section 1
id pdf2json/2021.acl-long.521.pdf.json
automated O
metrics O
such O
as O
bleu O
( O
papineni O
et O
al. O
, O
2002 O
) O
and O
rouge O
( O
lin O
, O
2004 O
) O
are O
popular O
methods O
for O
evaluating O
natural O
language O
generation O
( O
nlg O
) O
systems O
. O

section 1
id pdf2json/2021.acl-long.521.pdf.json
compared O
with O
human O
evaluation O
, O
they O
are O
cheaper O
and O
faster O
, O
and O
accordingly O
, O
they O
often O
serve O
as O
essential O
metrics O
for O
benchmarking O
the O
performance O
of O
nlg O
models O
( O
novikova O
et O
al. O
, O
2017 O
) O
. O

section 1
id pdf2json/2021.acl-long.521.pdf.json
despite O
their O
widespread O
use O
, O
however O
, O
these O
automated O
metrics O
often O
poorly O
correlate O
with O
ratings O
given O
by O
human O
judges O
, O
particularly O
for O
datasets O
in O
which O
only O
a O
single O
human O
reference O
exists O
( O
gupta O
et O
al. O
, O
2019 O
; O
novikova O
et O
al. O
, O
2017 O
) O
. O

section 1
id pdf2json/2021.acl-long.521.pdf.json
moreover O
, O
these O
automated O
metrics O
only O
capture O
similarities O
between O
generated O
sentences O
and O
reference O
candidates O
, O
crucially O
ignoring O
provided O
contexts O
that O
are O
relevant O
for O
evaluating O
the O
answer O
in O
contextual O
nlg O
tasks O
, O
such O
as O
story O
generation O
, O
news O
summarization O
, O
and O
question-answering O
( O
tao O
et O
al. O
, O
2018 O
; O
nema O
and O
khapra O
, O
2018 O
) O
. O

section 1
id pdf2json/2021.acl-long.521.pdf.json
table O
1 O
shows O
a O
story O
generation1 O
example O
that O
exemplifies O
some O
weaknesses O
of O
several O
common O
metrics O
. O

section 1
id pdf2json/2021.acl-long.521.pdf.json
perplexity O
( O
ppl O
) O
( O
brown O
et O
al. O
, O
1992 O
) O
successfully O
detects O
ungrammatical O
sentences O
, O
but O
it O
fails O
to O
distinguish O
legitimate O
novel O
continuations O
and O
copy-and-pasted O
ones O
. O

section 1
id pdf2json/2021.acl-long.521.pdf.json
relying O
on O
surface-level O
=-gram O
matching O
, O
bleu-1 O
and O
rouge-l2 O
can O
not O
detect O
reordering O
effectively O
, O
and O
wrongly O
score O
the O
well-formed O
candidate O
lower O
than O
its O
retrieval-based O
adversarial O
example O
. O

section 1
id pdf2json/2021.acl-long.521.pdf.json
bertscore O
( O
zhang O
et O
al. O
, O
2019 O
) O
leverages O
contextual O
embeddings O
from O
bert O
( O
devlin O
et O
al. O
, O
2019 O
) O
, O
thus O
mitigating O
the O
above O
challenges O
, O
but O
still O
does O
not O
fairly O
evaluate O
candidates O
that O
correctly O
align O
with O
the O
context O
but O
happen O
to O
differ O
1the O
roc O
story O
generation O
task O
asks O
systems O
to O
generate O
a O
legitimate O
ending O
for O
a O
four-sentence O
story O
. O

section 1
id pdf2json/2021.acl-long.521.pdf.json
2l O
stands O
for O
longest O
common O
sequence O
matching O
. O

section 1
id pdf2json/2021.acl-long.521.pdf.json
context O
. O

section 1
id pdf2json/2021.acl-long.521.pdf.json
wendy O
was O
driving O
down O
the O
road O
. O

section 1
id pdf2json/2021.acl-long.521.pdf.json
she O
heard O
her O
car O
making O
a O
noise O
. O

section 1
id pdf2json/2021.acl-long.521.pdf.json
she O
pulled O
over O
to O
examine O
the O
problem O
. O

section 1
id pdf2json/2021.acl-long.521.pdf.json
there O
was O
nothing O
but O
oil O
all O
on O
the O
road O
from O
her O
car O
. O

section 1
id pdf2json/2021.acl-long.521.pdf.json
human O
reference O
. O

section 1
id pdf2json/2021.acl-long.521.pdf.json
she O
called O
for O
help O
and O
waited O
to O
get O
her O
car O
fixed O
. O

section 1
id pdf2json/2021.acl-long.521.pdf.json
ppl O
bleu-1 O
rouge-l O
bertscore O
mars O
ability O
) O
. O

section 1
id pdf2json/2021.acl-long.521.pdf.json
we O
boxed O
the O
cases O
where O
the O
adversarial O
example O
does O
not O
score O
lower O
than O
the O
well-formed O
candidate O
. O

section 1
id pdf2json/2021.acl-long.521.pdf.json
from O
the O
provided O
reference O
example O
. O

section 1
id pdf2json/2021.acl-long.521.pdf.json
in O
our O
example O
, O
the O
candidate O
“ O
... O
her O
engine O
was O
smoking O
” O
is O
reasonable O
but O
deviates O
from O
the O
human O
reference O
, O
and O
so O
bertscore O
rates O
it O
relatively O
low O
( O
0.338 O
out O
of O
1.0 O
) O
, O
thus O
correlating O
poorly O
with O
human O
rating O
, O
which O
was O
high O
( O
5.05 O
out O
of O
6.00 O
) O
. O

section 1
id pdf2json/2021.acl-long.521.pdf.json
to O
address O
the O
above O
issues O
, O
prior O
studies O
have O
proposed O
a O
number O
of O
promising O
remedies O
. O

section 1
id pdf2json/2021.acl-long.521.pdf.json
one O
line O
of O
work O
has O
proposed O
to O
combine O
human O
ratings O
with O
automated O
metrics O
( O
durmus O
et O
al. O
, O
2020 O
; O
chaganty O
et O
al. O
, O
2018 O
, O
inter O
alia O
) O
. O

section 1
id pdf2json/2021.acl-long.521.pdf.json
for O
instance O
, O
in O
huse O
score O
, O
hashimoto O
et O
al O
. O

section 1
id pdf2json/2021.acl-long.521.pdf.json
( O
2019 O
) O
leverages O
the O
differences O
between O
perplexity O
and O
human O
judgements O
to O
consider O
both O
quality O
and O
diversity O
of O
generated O
text O
. O

section 1
id pdf2json/2021.acl-long.521.pdf.json
another O
line O
has O
proposed O
training O
separate O
neural O
models O
to O
aid O
automated O
metrics O
( O
mehri O
and O
eskenazi O
, O
2020 O
; O
yuma O
et O
al. O
, O
2020 O
, O
inter O
alia O
) O
. O

section 1
id pdf2json/2021.acl-long.521.pdf.json
for O
instance O
, O
bleurt O
( O
sellam O
et O
al. O
, O
2020 O
) O
fine-tunes O
bert O
( O
devlin O
et O
al. O
, O
2019 O
) O
on O
synthetic O
reference-candidate O
pairs O
for O
machine O
translation O
. O

section 1
id pdf2json/2021.acl-long.521.pdf.json
these O
methods O
, O
however O
, O
are O
often O
limited O
in O
practical O
use O
, O
because O
the O
high-cost O
human O
ratings O
are O
not O
always O
available O
for O
every O
dataset O
, O
and O
the O
data- O
or O
system-specific O
training O
is O
not O
easily O
extended O
to O
other O
domains O
( O
zhang O
et O
al. O
, O
2019 O
) O
, O
and O
can O
even O
bias O
the O
evaluation O
( O
freitag O
et O
al. O
, O
2020b O
) O
. O

section 1
id pdf2json/2021.acl-long.521.pdf.json
in O
this O
paper O
, O
we O
present O
mars O
( O
language O
model O
augmented O
relevance O
score O
) O
, O
a O
new O
nlg O
evaluation O
metric O
that O
requires O
neither O
supervision O
from O
human O
ratings O
nor O
additional O
training O
on O
specific O
domains O
. O

section 1
id pdf2json/2021.acl-long.521.pdf.json
as O
shown O
in O
figure O
1 O
, O
instead O
of O
comparing O
candidates O
only O
with O
human O
written O
references O
, O
as O
many O
prior O
metrics O
do O
, O
mars O
uses O
a O
mixture O
of O
both O
human O
and O
augmented O
references O
. O

section 1
id pdf2json/2021.acl-long.521.pdf.json
specifically O
, O
mars O
masks O
tokens O
in O
the O
reference O
to O
create O
templates O
, O
and O
then O
uses O
the O
context O
and O
templates O
to O
generate O
augmented O
references O
by O
infilling O
the O
masked O
parts O
with O
an O
lm O
guided O
by O
reinforcement O
learning O
. O

section 1
id pdf2json/2021.acl-long.521.pdf.json
the O
augmented O
references O
thus O
incorporate O
information O
from O
both O
the O
context O
and O
the O
human O
reference O
, O
and O
are O
enriched O
with O
lexical O
and O
syntactic O
diversity O
, O
facilitating O
fairer O
evaluation O
of O
candidates O
. O

section 1
id pdf2json/2021.acl-long.521.pdf.json
finally O
, O
we O
compute O
the O
score O
as O
a O
weighted O
average O
of O
the O
similarity O
between O
the O
candidate O
and O
the O
set O
of O
augmented O
references O
in O
the O
contextual O
embedding O
space O
. O

section 1
id pdf2json/2021.acl-long.521.pdf.json
the O
advantages O
of O
mars O
are O
three-fold O
. O

section 1
id pdf2json/2021.acl-long.521.pdf.json
first O
, O
mars O
correlates O
highly O
with O
human O
judgements O
. O

section 1
id pdf2json/2021.acl-long.521.pdf.json
we O
apply O
mars O
to O
three O
diverse O
nlg O
tasks O
, O
and O
demonstrate O
that O
, O
compared O
with O
seven O
popular O
nlg O
metrics O
, O
mars O
better O
correlates O
with O
human O
judgements O
and O
is O
robust O
against O
adversarial O
attacks O
. O

section 1
id pdf2json/2021.acl-long.521.pdf.json
second O
, O
mars O
is O
context-aware O
. O

section 1
id pdf2json/2021.acl-long.521.pdf.json
unlike O
existing O
metrics O
that O
only O
consider O
the O
given O
human O
reference O
, O
we O
use O
a O
constrained O
nlg O
approach O
to O
incorporate O
the O
generation O
context O
into O
augmented O
references O
, O
thus O
alleviating O
bias O
against O
diverse O
candidates O
. O

section 1
id pdf2json/2021.acl-long.521.pdf.json
third O
, O
mars O
is O
easy O
to O
deploy O
and O
extend O
. O

section 1
id pdf2json/2021.acl-long.521.pdf.json
built O
on O
off-the-shelf O
lms O
, O
mars O
requires O
neither O
human O
supervision O
nor O
additional O
training O
for O
specific O
domains O
, O
and O
can O
therefore O
serve O
as O
a O
general-purpose O
metric O
for O
a O
broad O
range O
of O
nlg O
applications O
, O
as O
we O
will O
demonstrate O
for O
three O
common O
nlg O
tasks O
: O
story O
generation O
, O
news O
summarization O
, O
and O
question-answering O
. O

section 2
id pdf2json/2021.acl-long.521.pdf.json
mars O
comprises O
three O
steps O
. O

section 2
id pdf2json/2021.acl-long.521.pdf.json
first O
, O
we O
mask O
out O
non-important O
tokens O
from O
the O
human O
reference O
to O
produce O
templates O
for O
augmentation O
( O
§2.1 O
) O
. O

section 2
id pdf2json/2021.acl-long.521.pdf.json
second O
, O
we O
guide O
off-the-shelf O
lms O
to O
generate O
reference O
augmentation O
on O
these O
templates O
via O
a O
reinforced O
self-planning O
algorithm O
( O
§2.2 O
) O
. O

section 2
id pdf2json/2021.acl-long.521.pdf.json
finally O
, O
we O
compute O
a O
weighted O
average O
score O
that O
reflects O
the O
overall O
similarity O
between O
the O
candidate O
and O
the O
set O
of O
augmented O
references O
( O
§2.3 O
) O
. O

section 3
id pdf2json/2021.acl-long.521.pdf.json
the O
first O
step O
in O
mars O
is O
to O
take O
in O
the O
given O
human O
reference O
and O
generate O
templates—masked O
versions O
of O
the O
human O
reference—which O
can O
then O
be O
used O
to O
generate O
augmented O
references O
. O

section 3
id pdf2json/2021.acl-long.521.pdf.json
our O
masking O
procedure O
can O
be O
viewed O
as O
a O
reversed O
process O
of O
prior O
insertion- O
and O
template-based O
generation O
approaches O
( O
zhang O
et O
al. O
, O
2020 O
; O
miao O
et O
al. O
, O
2019 O
) O
; O
whereas O
these O
generation O
approaches O
start O
with O
templates O
of O
important O
tokens O
and O
then O
fill O
in O
the O
details O
to O
generate O
complete O
sentences O
, O
our O
masking O
procedure O
starts O
with O
the O
complete O
sentence O
( O
i.e. O
, O
the O
human O
reference O
) O
and O
then O
masks O
out O
unimportant O
tokens O
to O
generate O
templates O
. O

section 3
id pdf2json/2021.acl-long.521.pdf.json
to O
better O
explain O
our O
masking O
procedure O
, O
we O
introduce O
two O
concepts O
, O
mask O
priority O
and O
mask O
cost O
: O
mask O
priority O
. O

section 3
id pdf2json/2021.acl-long.521.pdf.json
we O
compute O
a O
mask O
priority O
e8 O
for O
each O
token O
g8 O
, O
which O
captures O
the O
priority O
of O
masking O
g8 O
, O
where O
non-important O
words O
should O
receive O
higher O
priority O
. O

section 3
id pdf2json/2021.acl-long.521.pdf.json
we O
compute O
e8 O
as O
a O
function O
of O
two O
things O
: O
the O
inverse O
document O
frequency O
( O
idf O
) O
of O
g8 O
, O
and O
the O
part-of-speech O
( O
pos O
) O
of O
g8 O
: O
e8 O
= O
u O
( O
pos O
[ O
g8 O
] O
) O
idf O
( O
g8 O
, O
- O
) O
, O
( O
1 O
) O
where O
u O
is O
a O
function O
that O
assigns O
a O
weight O
to O
each O
pos O
tag.3 O
common O
tokens O
across O
the O
corpus O
- O
( O
e.g. O
, O
stop O
words O
, O
with O
low O
idf O
) O
will O
receive O
high O
mask O
priority O
. O

section 3
id pdf2json/2021.acl-long.521.pdf.json
tokens O
responsible O
for O
description O
details O
will O
also O
be O
assigned O
high O
mask O
priority O
based O
on O
their O
part-of-speech O
( O
e.g. O
, O
adjectives O
are O
mainly O
used O
for O
details O
and O
so O
they O
are O
given O
higher O
priority O
of O
being O
masked O
) O
. O

section 3
id pdf2json/2021.acl-long.521.pdf.json
mask O
cost O
. O

section 3
id pdf2json/2021.acl-long.521.pdf.json
for O
each O
token O
g8 O
, O
we O
also O
compute O
a O
mask O
cost O
f8 O
. O

section 3
id pdf2json/2021.acl-long.521.pdf.json
tokens O
that O
appear O
in O
both O
context O
and O
human O
reference O
should O
have O
high O
masking O
cost O
as O
they O
are O
deemed O
context-carrying O
. O

section 3
id pdf2json/2021.acl-long.521.pdf.json
we O
use O
the O
longest O
common O
sequence O
( O
lcs O
) O
matching O
between O
the O
context O
and O
the O
human O
reference O
to O
identify O
these O
context-carrying O
tokens O
. O

section 3
id pdf2json/2021.acl-long.521.pdf.json
in O
our O
experiments O
, O
we O
set O
the O
f8 O
of O
these O
tokens O
to O
10 O
and O
the O
default O
f8 O
of O
all O
other O
tokens O
to O
1 O
. O

section 3
id pdf2json/2021.acl-long.521.pdf.json
we O
use O
_ O
to O
denote O
the O
ratio O
of O
tokens O
to O
be O
masked O
in O
a O
sentence O
of O
# O
tokens O
, O
and O
define O
, O
max O
= O
_ O
· O
# O
as O
the O
maximum O
cost O
allowed O
. O

section 3
id pdf2json/2021.acl-long.521.pdf.json
3u O
varies O
for O
each O
task O
. O

section 3
id pdf2json/2021.acl-long.521.pdf.json
empirically O
, O
we O
find O
that O
it O
works O
well O
to O
assign O
adjectives O
, O
adverbs O
, O
and O
nouns O
higher O
weights O
than O
other O
parts-of-speech O
. O

section 3
id pdf2json/2021.acl-long.521.pdf.json
for O
our O
setting O
, O
we O
assign O
weights O
of O
4 O
, O
3 O
, O
2 O
to O
the O
above O
three O
types O
. O

section 3
id pdf2json/2021.acl-long.521.pdf.json
dp-based O
token O
masking O
. O

section 3
id pdf2json/2021.acl-long.521.pdf.json
now O
that O
for O
each O
token O
we O
have O
a O
mask O
priority O
and O
a O
mask O
cost O
, O
we O
aim O
to O
choose O
a O
set O
of O
tokens O
to O
mask O
with O
the O
highest O
possible O
sum O
of O
priorities O
for O
which O
the O
sum O
of O
mask O
costs O
is O
not O
greater O
than O
, O
max O
. O

section 3
id pdf2json/2021.acl-long.521.pdf.json
given O
a O
function O
q O
( O
g8 O
) O
= O
{ O
1 O
, O
0 O
} O
where O
1 O
means O
token O
g8 O
is O
masked O
and O
0 O
means O
it O
remains O
, O
the O
objective O
of O
token O
masking O
can O
be O
expressed O
as O
follows O
: O
max O
# O
∑ O
8=1 O
e8 O
· O
q O
( O
g8 O
) O
, O
s.t O
. O

section 3
id pdf2json/2021.acl-long.521.pdf.json
# O
∑ O
8=1 O
f8 O
· O
q O
( O
g8 O
) O
≤ O
, O
max O
. O

section 3
id pdf2json/2021.acl-long.521.pdf.json
( O
2 O
) O
such O
a O
goal O
is O
actually O
a O
np-complete O
combinatorial O
optimization O
problem O
, O
called O
the O
knapsack O
problem O
( O
pisinger O
, O
1995 O
) O
, O
which O
we O
solve O
using O
dynamic-programming O
( O
dp O
) O
. O

section 3
id pdf2json/2021.acl-long.521.pdf.json
in O
general O
, O
the O
masking O
strategy O
aggressively O
harvests O
tokens O
of O
high O
mask O
priority O
while O
keeping O
the O
cost O
of O
masked O
tokens O
from O
exceeding O
the O
mask O
cost O
limitation O
, O
max O
. O

section 3
id pdf2json/2021.acl-long.521.pdf.json
the O
detailed O
dp O
algorithm O
for O
solving O
this O
problem O
is O
shown O
in O
appendix O
a O
. O

section 3
id pdf2json/2021.acl-long.521.pdf.json
2.2 O
self-planning O
cloze O
augmentation O
after O
creating O
the O
templates O
described O
in O
§2.1 O
, O
we O
produce O
augmented O
reference O
examples O
based O
on O
both O
the O
templates O
as O
well O
as O
the O
generation O
context O
. O

section 3
id pdf2json/2021.acl-long.521.pdf.json
this O
procedure O
can O
be O
seen O
as O
a O
mixture O
of O
hardand O
soft-constrained O
nlg O
, O
where O
the O
template O
tokens O
pre-exist O
with O
some O
blanks O
, O
and O
the O
system O
, O
conditioned O
on O
the O
context O
, O
aims O
to O
fill O
in O
the O
blanks O
. O

section 3
id pdf2json/2021.acl-long.521.pdf.json
we O
henceforth O
refer O
this O
process O
of O
creating O
augmented O
references O
as O
cloze4 O
augmentation O
. O

section 3
id pdf2json/2021.acl-long.521.pdf.json
background O
. O

section 3
id pdf2json/2021.acl-long.521.pdf.json
masked O
language O
models O
( O
mlm O
) O
such O
as O
roberta O
( O
liu O
et O
al. O
, O
2019 O
) O
and O
bert O
( O
devlin O
et O
al. O
, O
2019 O
) O
are O
trained O
to O
predict O
masked O
tokens O
within O
sentences O
, O
and O
thus O
are O
able O
to O
do O
cloze O
augmentation O
off-the-shelf O
. O

section 3
id pdf2json/2021.acl-long.521.pdf.json
however O
, O
without O
architecture-level O
modification O
, O
mlms O
are O
only O
able O
to O
infill O
a O
pre-determined O
number O
of O
missing O
tokens O
( O
zhu O
et O
al. O
, O
2019 O
) O
. O

section 3
id pdf2json/2021.acl-long.521.pdf.json
this O
is O
especially O
problematic O
since—if O
they O
are O
directly O
used O
to O
augment O
references—all O
the O
augmented O
references O
will O
have O
the O
same O
number O
of O
tokens O
as O
that O
of O
the O
original O
human O
reference O
. O

section 3
id pdf2json/2021.acl-long.521.pdf.json
we O
believe O
this O
unnecessarily O
constrains O
augmentation O
diversity O
, O
and O
thus O
consider O
it O
as O
a O
naive O
method O
in O
our O
evaluations O
( O
§4 O
) O
. O

section 3
id pdf2json/2021.acl-long.521.pdf.json
4a O
cloze O
test O
( O
taylor O
, O
1953 O
) O
is O
a O
language O
test O
where O
a O
portion O
of O
language O
is O
removed O
and O
the O
participant O
is O
asked O
to O
replace O
the O
missing O
language O
item O
. O

section 3
id pdf2json/2021.acl-long.521.pdf.json
autoregressive O
language O
models O
( O
alm O
) O
such O
as O
gpt-2 O
( O
radford O
et O
al. O
, O
2019 O
) O
, O
on O
the O
other O
hand O
, O
are O
trained O
to O
predict O
current O
step O
token O
given O
past O
tokens O
. O

section 3
id pdf2json/2021.acl-long.521.pdf.json
they O
can O
generate O
sequences O
of O
varying O
lengths O
, O
but O
they O
can O
not O
infill O
missing O
tokens O
within O
sentences O
effectively O
since O
they O
do O
not O
consider O
future O
context O
. O

section 3
id pdf2json/2021.acl-long.521.pdf.json
to O
enable O
alms O
to O
infill O
blanks O
of O
unspecified O
length O
, O
prior O
work O
has O
proposed O
either O
retraining O
a O
new O
lm O
from O
scratch O
( O
shen O
et O
al. O
, O
2020 O
) O
or O
fine-tuning O
on O
specially O
prepared O
data O
( O
donahue O
et O
al. O
, O
2020 O
) O
, O
which O
are O
costly O
and O
not O
easy O
to O
extend O
to O
new O
nlg O
tasks O
. O

section 3
id pdf2json/2021.acl-long.521.pdf.json
as O
shown O
in O
figure O
2 O
, O
we O
take O
a O
reinforcement O
learning O
( O
rl O
) O
approach O
that O
uses O
future O
words O
after O
the O
blank O
to O
guide O
current O
step O
infilling O
generation O
. O

section 3
id pdf2json/2021.acl-long.521.pdf.json
since O
such O
rl O
guidance O
only O
relies O
on O
the O
tokens O
within O
its O
own O
to-be-infilled O
template O
, O
we O
call O
it O
reinforced O
self-planning O
. O

section 3
id pdf2json/2021.acl-long.521.pdf.json
our O
method O
combines O
the O
advantages O
of O
both O
mlms O
and O
alms O
, O
requiring O
neither O
re-training O
nor O
collecting O
new O
data O
, O
and O
thus O
is O
easier O
to O
extend O
to O
other O
off-the-shelf O
lms O
. O

section 3
id pdf2json/2021.acl-long.521.pdf.json
reinforced O
self-planning O
. O

section 3
id pdf2json/2021.acl-long.521.pdf.json
at O
each O
decoding O
step O
during O
generation O
, O
a O
vanilla O
alm O
will O
pick O
the O
token O
gc O
that O
has O
the O
highest O
probability O
by O
applying O
an O
argmax O
over O
the O
softmax O
output O
of O
hidden O
states O
. O

section 3
id pdf2json/2021.acl-long.521.pdf.json
we O
add O
a O
self-planning O
stage O
between O
the O
argmax O
and O
softmax O
function O
. O

section 3
id pdf2json/2021.acl-long.521.pdf.json
following O
the O
rl O
framework O
, O
we O
define O
the O
state O
at O
step O
c O
as O
the O
generated O
sequences O
before O
c O
( O
i.e. O
, O
bc O
= O
g O
< O
c O
) O
, O
and O
the O
action O
at O
step O
c O
as O
the O
c-th O
output O
token O
( O
i.e. O
, O
0c O
= O
gc O
) O
. O

section 3
id pdf2json/2021.acl-long.521.pdf.json
we O
take O
the O
softmax O
output O
of O
the O
last O
hidden O
states O
( O
with O
parameter O
\ O
) O
as O
the O
policy O
c\ O
, O
since O
it O
is O
the O
probability O
of O
picking O
token O
gc O
( O
action O
0c O
) O
given O
the O
state O
bc O
= O
g O
< O
c O
. O

section 3
id pdf2json/2021.acl-long.521.pdf.json
similarly O
, O
we O
denote O
the O
policy O
after O
reinforced O
self-planning O
as O
c\3 O
. O

section 3
id pdf2json/2021.acl-long.521.pdf.json
typically O
, O
the O
rl O
objective O
is O
to O
maximize O
the O
expectation O
of O
total O
reward O
, O
summed O
over O
) O
steps O
on O
the O
trajectory O
g O
induced O
by O
c\ O
: O
( O
\ O
) O
= O
eg∼c\ O
[ O
) O
∑ O
c=0 O
wcac O
] O
, O
( O
3 O
) O
where O
w O
∈ O
( O
0 O
, O
1 O
] O
is O
the O
discounting O
factor O
, O
and O
a O
is O
the O
single-step O
reward O
. O

section 3
id pdf2json/2021.acl-long.521.pdf.json
in O
text O
generation O
, O
however O
, O
such O
a O
reward O
definition O
requires O
sampling O
over O
the O
future O
generated O
sequence O
to O
estimate O
current O
step O
reward O
( O
gong O
et O
al. O
, O
2019 O
) O
, O
which O
may O
cause O
the O
policy O
to O
end O
in O
zero O
reward O
region O
because O
of O
high O
variance O
of O
the O
gradient O
( O
pang O
and O
he O
, O
2021 O
) O
. O

section 3
id pdf2json/2021.acl-long.521.pdf.json
since O
we O
guide O
the O
generation O
in O
every O
step O
of O
decoding O
, O
we O
derive O
the O
c-th O
step O
policy O
gradient O
o\ O
c O
( O
\ O
) O
as O
: O
ecg∼c\ O
[ O
nco\ O
log O
c\ O
( O
0c O
|bc O
) O
· O
a O
( O
g3c O
) O
] O
, O
( O
4 O
) O
with O
importance O
sampling O
weight O
nc O
to O
stabilize O
the O
optimization O
( O
munos O
et O
al. O
, O
2016 O
) O
, O
which O
is O
: O
nc O
= O
c\3 O
( O
0c O
|bc O
) O
c\ O
( O
0c O
|bc O
) O
. O

section 3
id pdf2json/2021.acl-long.521.pdf.json
if O
we O
denote O
a O
certain O
token O
in O
future O
context O
as O
f O
∈ O
{ O
ffuture O
} O
, O
single-step O
self-planning O
reward O
a O
( O
g3c O
) O
can O
be O
approximated O
by O
the O
cosine O
similarity O
between O
c-th O
step O
hidden O
state O
and O
the O
embedded O
vector O
of O
f O
by O
the O
lm O
embedding O
layers O
, O
which O
is O
a O
( O
g3c O
) O
= O
∑ O
f O
∈ffuture O
log O
( O
softmax O
( O
ℎ\3 O
< O
c O
) O
· O
emb O
( O
f O
) O
) O
. O

section 3
id pdf2json/2021.acl-long.521.pdf.json
( O
5 O
) O
given O
all O
above O
definitions O
, O
at O
c-th O
step O
, O
we O
up- O
date O
c\ O
towards O
the O
self-planned O
c\3 O
as O
: O
\3 O
← O
\ O
+ O
[ O
: O
∑ O
8=1 O
o\ O
c O
( O
\3/b O
) O
‖o\ O
c O
( O
\3/b O
) O
‖ O
, O
( O
6 O
) O
where O
[ O
is O
the O
learning O
rate O
and O
b O
is O
the O
temperature O
parameter O
to O
control O
the O
stochastic O
sampling O
during O
token O
decoding O
( O
keskar O
et O
al. O
, O
2019 O
) O
. O

section 3
id pdf2json/2021.acl-long.521.pdf.json
after O
: O
iterations O
of O
reinforced O
self-planning O
, O
the O
updated O
policy O
c\3 O
should O
produce O
tokens O
approaching O
the O
future O
context O
in O
embedding O
space O
, O
since O
future O
context O
contributes O
to O
the O
calculation O
of O
reward O
a O
( O
eq O
. O

section 3
id pdf2json/2021.acl-long.521.pdf.json
5 O
) O
.5 O
more O
details O
about O
how O
we O
handle O
edge O
cases O
during O
reinforced O
self-planning O
are O
presented O
in O
appendix O
b O
. O

section 3
id pdf2json/2021.acl-long.521.pdf.json
5in O
our O
setting O
, O
[ O
, O
b O
and O
: O
are O
0.02 O
, O
1.3 O
, O
and O
3 O
respectively O
. O

section 4
id pdf2json/2021.acl-long.521.pdf.json
after O
generating O
augmented O
reference O
sentences O
, O
the O
final O
mars O
score O
is O
computed O
as O
a O
weighted O
average O
of O
the O
similarity O
between O
the O
candidate O
and O
each O
reference O
in O
the O
augmentation O
set O
( O
including O
the O
original O
human O
reference O
) O
. O

section 4
id pdf2json/2021.acl-long.521.pdf.json
one O
way O
to O
obtain O
similarity O
scores O
is O
using O
bertscore O
( O
zhang O
et O
al. O
, O
2019 O
) O
, O
but O
bertscore O
requires O
training O
on O
external O
resources O
to O
make O
its O
outputs O
more O
readable O
. O

section 4
id pdf2json/2021.acl-long.521.pdf.json
therefore O
, O
in O
order O
to O
keep O
all O
the O
resources O
used O
by O
mars O
off-the-shelf O
, O
we O
utilize O
sentencebert O
( O
reimers O
and O
gurevych O
, O
2019 O
) O
, O
which O
uses O
the O
mean O
of O
all O
token O
embeddings O
in O
a O
sentence O
as O
the O
overall O
sentence-level O
encoding O
. O

section 4
id pdf2json/2021.acl-long.521.pdf.json
as O
the O
sentence O
encoder O
, O
we O
use O
roberta-large O
( O
liu O
et O
al. O
, O
2019 O
) O
, O
a O
common O
choice O
in O
the O
literature O
( O
zhang O
et O
al. O
, O
2019 O
; O
reimers O
and O
gurevych O
, O
2020 O
) O
. O

section 4
id pdf2json/2021.acl-long.521.pdf.json
as O
shown O
in O
eq O
. O

section 4
id pdf2json/2021.acl-long.521.pdf.json
7 O
, O
we O
then O
compute O
mars O
score O
as O
the O
average O
of O
the O
cosine O
similarities O
weighted O
using O
a O
geometric O
progression O
with O
a O
common O
ratio O
@ O
∈ O
( O
0 O
, O
1 O
] O
and O
a O
scale O
factor O
( O
start O
value O
) O
0 O
≠ O
0 O
: O
mars O
= O
# O
_∑ O
8=1 O
0 O
@ O
8−1 O
candt O
· O
ref8−1 O
‖cand‖t O
‖ref8−1‖ O
s.t O
. O

section 4
id pdf2json/2021.acl-long.521.pdf.json
# O
_∑ O
8=1 O
0 O
@ O
8−1 O
= O
1 O
, O
( O
7 O
) O
where O
the O
candidate O
encoding O
is O
cand O
, O
the O
reference O
encodings O
are O
ref8 O
( O
8 O
is O
the O
index O
of O
the O
augmented O
reference O
under O
a O
certain O
_ O
, O
and O
ref0 O
marks O
the O
zeromask O
human O
reference O
) O
, O
and O
# O
_ O
is O
the O
number O
of O
masking O
ratios O
we O
use O
in O
§2.1 O
. O

section 4
id pdf2json/2021.acl-long.521.pdf.json
different O
@ O
values O
, O
as O
defined O
by O
the O
geometric O
progression O
, O
determine O
how O
much O
weight O
each O
reference O
contributes O
. O

section 4
id pdf2json/2021.acl-long.521.pdf.json
by O
default O
, O
eq O
. O

section 4
id pdf2json/2021.acl-long.521.pdf.json
7 O
assigns O
the O
largest O
weight O
to O
the O
human O
reference O
since O
it O
is O
the O
gold O
standard O
. O

section 5
id pdf2json/2021.acl-long.521.pdf.json
we O
evaluated O
mars O
and O
compared O
it O
with O
several O
popular O
nlg O
metrics O
on O
the O
following O
three O
tasks O
: O
story O
generation O
. O

section 5
id pdf2json/2021.acl-long.521.pdf.json
we O
use O
the O
roc O
stories O
dataset6 O
for O
story O
generation O
, O
which O
requires O
candidate O
nlg O
systems O
to O
generate O
coherent O
endings O
to O
four-sentence O
stories O
( O
mostafazadeh O
et O
al. O
, O
2016 O
) O
. O

section 5
id pdf2json/2021.acl-long.521.pdf.json
the O
dataset O
consists O
of O
96,198 O
examples O
of O
partially O
written O
stories O
; O
we O
take O
the O
human-rated O
subset O
( O
# O
=300 O
) O
released O
by O
huse O
( O
hashimoto O
et O
al. O
, O
2019 O
) O
, O
which O
contains O
continuances O
by O
( O
1 O
) O
6https O
: O
//cs.rochester.edu/nlp/rocstories/ O
an O
industry-level O
system O
based O
on O
apache O
solr7 O
, O
and O
( O
2 O
) O
an O
open-nmt O
model O
with O
global O
attention O
( O
mccann O
et O
al. O
, O
2017 O
) O
. O

section 5
id pdf2json/2021.acl-long.521.pdf.json
news O
summarization O
. O

section 5
id pdf2json/2021.acl-long.521.pdf.json
for O
the O
news O
summarization O
task O
, O
we O
use O
the O
newsroom O
summary O
dataset.8 O
this O
dataset O
contains O
1.3 O
million O
articles O
from O
38 O
major O
publications O
( O
grusky O
et O
al. O
, O
2018 O
) O
and O
we O
use O
the O
subset O
with O
human O
ratings O
( O
# O
=540 O
) O
released O
by O
the O
authors.9 O
this O
dataset O
contains O
outputs O
from O
summarization O
models O
: O
( O
1 O
) O
textrank O
: O
a O
sentencelevel O
summarization O
system O
inspired O
by O
google O
pagerank O
( O
page O
et O
al. O
, O
1999 O
) O
, O
( O
2 O
) O
a O
seq2seq O
model O
with O
attention O
( O
rush O
et O
al. O
, O
2015 O
) O
, O
and O
( O
3 O
) O
pointern O
: O
a O
pointer-based O
neural O
model O
( O
see O
et O
al. O
, O
2017 O
) O
trained O
on O
newsroom O
dataset O
. O

section 5
id pdf2json/2021.acl-long.521.pdf.json
question O
answering O
. O

section 5
id pdf2json/2021.acl-long.521.pdf.json
for O
question O
answering O
, O
we O
use O
the O
mocha O
dataset,10 O
which O
includes O
human O
ratings O
on O
outputs O
of O
five O
models O
trained O
on O
six O
qa O
datasets O
( O
chen O
et O
al. O
, O
2020 O
) O
. O

section 5
id pdf2json/2021.acl-long.521.pdf.json
we O
consider O
a O
distributionally-balanced O
subset O
( O
# O
=450 O
) O
of O
these O
outputs O
from O
three O
systems O
: O
( O
1 O
) O
finetuned O
gpt-2 O
( O
radford O
et O
al. O
, O
2019 O
) O
, O
( O
2 O
) O
a O
backtranslation O
model O
( O
sennrich O
et O
al. O
, O
2016 O
) O
, O
and O
( O
3 O
) O
a O
mhpg O
model O
( O
bauer O
et O
al. O
, O
2018 O
) O
trained O
on O
narrativeqa O
( O
kočiskỳ O
et O
al. O
, O
2018 O
) O
and O
mcscript O
( O
ostermann O
et O
al. O
, O
2018 O
) O
datasets O
. O

section 5
id pdf2json/2021.acl-long.521.pdf.json
the O
detailed O
statistics O
of O
these O
three O
datasets O
we O
used O
for O
this O
work O
are O
shown O
in O
table O
2 O
. O

section 5
id pdf2json/2021.acl-long.521.pdf.json
for O
pre-processing O
, O
we O
removed O
hashtags O
and O
urls O
in O
the O
text O
, O
but O
leave O
punctuation O
and O
stop O
words O
, O
which O
can O
affect O
lcs O
matching O
when O
computing O
mask O
costs O
. O

section 5
id pdf2json/2021.acl-long.521.pdf.json
for O
all O
tasks O
, O
we O
use O
gpt-2 O
( O
large O
, O
with O
774m O
parameters O
) O
as O
the O
language O
model O
for O
7https O
: O
//lucene.apache.org/solr O
8http O
: O
//lil.nlp.cornell.edu/newsroom/ O
9the O
subset O
includes O
human O
ratings O
on O
four O
perspectives O
: O
coherence O
, O
fluency O
, O
informative O
and O
relevance O
. O

section 5
id pdf2json/2021.acl-long.521.pdf.json
we O
compute O
the O
average O
of O
the O
four O
scores O
as O
an O
overall O
human O
rating O
. O

section 5
id pdf2json/2021.acl-long.521.pdf.json
10https O
: O
//allennlp.org/mocha O
mars O
, O
and O
roberta-large O
for O
the O
naive O
method O
. O

section 5
id pdf2json/2021.acl-long.521.pdf.json
for O
the O
newsroom O
dataset O
, O
some O
news O
articles O
were O
longer O
than O
the O
max O
sequence O
length O
of O
1024 O
bpe O
, O
and O
so O
we O
cut O
off O
the O
tail O
end O
of O
these O
examples O
. O

section 5
id pdf2json/2021.acl-long.521.pdf.json
with O
a O
single O
rtx-2080 O
gpu O
, O
cloze O
augmentation O
with O
_ O
= O
{ O
0 O
( O
human O
ref O
. O

section 5
id pdf2json/2021.acl-long.521.pdf.json
) O
, O
20 O
% O
, O
40 O
% O
, O
60 O
% O
, O
80 O
% O
} O
takes O
0.8 O
seconds O
on O
average O
per O
reference O
, O
amounting O
to O
a O
total O
augmentation O
time O
of O
17 O
, O
45 O
, O
and O
32 O
minutes O
for O
the O
roc O
, O
newsroom O
and O
mocha O
tasks O
respectively O
. O

section 5
id pdf2json/2021.acl-long.521.pdf.json
we O
show O
how O
we O
pick O
the O
masking O
ratios O
for O
different O
tasks O
in O
§4.3 O
. O

section 7
id pdf2json/2021.acl-long.521.pdf.json
as O
automated O
metrics O
are O
only O
helpful O
if O
they O
correlate O
sufficiently O
with O
human O
judgements O
, O
in O
this O
section O
we O
examine O
how O
mars O
correlates O
with O
human O
judgements O
compared O
with O
prior O
metrics O
. O

section 7
id pdf2json/2021.acl-long.521.pdf.json
system-level O
correlation O
. O

section 7
id pdf2json/2021.acl-long.521.pdf.json
table O
3 O
shows O
the O
correlations O
between O
human O
judgements O
and O
automated O
metrics O
for O
mars O
and O
seven O
other O
unsupervised O
metrics O
, O
across O
all O
nlg O
systems O
studied O
in O
our O
three O
tasks O
. O

section 7
id pdf2json/2021.acl-long.521.pdf.json
compared O
with O
the O
other O
metrics O
, O
mars O
achieves O
the O
highest O
correlation O
with O
human O
judgements O
for O
five O
of O
the O
seven O
systems O
( O
and O
comparable O
with O
the O
top O
in O
the O
other O
two O
systems O
) O
, O
making O
considerable O
improvements O
over O
the O
next-best O
metric O
for O
many O
of O
the O
nlg O
systems O
( O
e.g. O
, O
0.370 O
↑ O
for O
back-translation O
, O
and O
0.231 O
↑ O
for O
solr O
) O
. O

section 7
id pdf2json/2021.acl-long.521.pdf.json
we O
also O
notice O
that O
mars O
has O
greater O
improvements O
on O
more O
open-ended O
tasks O
( O
e.g. O
, O
story O
generation O
, O
which O
has O
low O
ω O
) O
, O
which O
corroborates O
mars O
’ O
s O
original O
objective O
of O
judging O
diverse O
candidates O
more O
fairly O
. O

section 7
id pdf2json/2021.acl-long.521.pdf.json
as O
for O
the O
baselines O
, O
=-gram O
matching O
metrics O
such O
as O
bleu O
correlate O
poorly O
with O
human O
ratings O
on O
such O
open-ended O
tasks O
; O
bertscore O
performs O
better O
on O
short O
candidates O
and O
high-ω O
tasks O
( O
e.g. O
, O
qa O
) O
; O
and O
perplexity O
, O
as O
expected O
, O
correlates O
weakly O
with O
human O
ratings O
. O

section 7
id pdf2json/2021.acl-long.521.pdf.json
the O
naive O
method O
, O
which O
uses O
multiple O
augmented O
references O
of O
the O
same O
length O
, O
improves O
over O
bertscore O
, O
which O
only O
uses O
the O
original O
reference O
. O

section 7
id pdf2json/2021.acl-long.521.pdf.json
ablation O
study O
. O

section 7
id pdf2json/2021.acl-long.521.pdf.json
as O
shown O
in O
the O
lower O
rows O
of O
table O
3 O
, O
we O
see O
that O
the O
performance O
of O
mars O
drops O
substantially O
when O
the O
crucial O
components O
are O
removed O
. O

section 7
id pdf2json/2021.acl-long.521.pdf.json
specifically O
, O
removing O
self-planning O
hurts O
performance O
more O
for O
tasks O
with O
longer O
references O
( O
e.g. O
, O
story O
generation O
) O
since O
self-planning O
is O
more O
helpful O
when O
there O
are O
more O
blanks O
to O
in-fill O
, O
and O
removing O
context O
hurts O
performance O
more O
in O
tasks O
that O
are O
less O
open-ended O
( O
highω O
, O
such O
as O
qa O
) O
because O
there O
is O
no O
adequate O
input O
for O
a O
reasonable O
augmentation O
. O

section 7
id pdf2json/2021.acl-long.521.pdf.json
we O
take O
these O
ablation O
study O
results O
as O
evidence O
that O
the O
techniques O
we O
propose O
in O
mars O
are O
crucial O
for O
improving O
correlation O
with O
human O
judgements O
. O

section 7
id pdf2json/2021.acl-long.521.pdf.json
task-level O
correlation O
visualization O
. O

section 7
id pdf2json/2021.acl-long.521.pdf.json
to O
visualize O
the O
correlation O
between O
automated O
metrics O
roc O
story O
generation O
newsroom O
summarization O
mocha O
question O
answering O
existing O
metrics O
reorder O
( O
δ O
) O
retrieve O
( O
δ O
) O
ref O
. O

section 7
id pdf2json/2021.acl-long.521.pdf.json
reorder O
( O
δ O
) O
retrieve O
( O
δ O
) O
ref O
. O

section 7
id pdf2json/2021.acl-long.521.pdf.json
reorder O
( O
δ O
) O
retrieve O
( O
δ O
) O
ref O
. O

section 7
id pdf2json/2021.acl-long.521.pdf.json
and O
human O
judgements O
, O
we O
consider O
the O
mocha O
qa O
task O
as O
an O
example O
and O
plot O
the O
correlations O
of O
bertscore O
( O
left O
) O
and O
mars O
( O
right O
) O
with O
human O
judgements O
. O

section 7
id pdf2json/2021.acl-long.521.pdf.json
as O
shown O
in O
figure O
3 O
, O
compared O
with O
mars O
, O
bertscore O
has O
more O
candidates O
in O
the O
upper-left O
corner O
of O
the O
plot O
( O
i.e. O
, O
low O
bertscore O
but O
high O
human O
judgement O
) O
. O

section 7
id pdf2json/2021.acl-long.521.pdf.json
many O
of O
these O
are O
generated O
by O
gpt-2 O
and O
mhpg O
, O
which O
, O
based O
on O
manual O
examination O
, O
tend O
to O
provide O
more O
details O
in O
the O
answer O
than O
the O
human O
reference O
. O

section 7
id pdf2json/2021.acl-long.521.pdf.json
for O
instance O
, O
given O
a O
context O
about O
shopping O
, O
one O
question O
is O
“ O
did O
they O
need O
to O
buy O
any O
meat O
? O
” O
. O

section 7
id pdf2json/2021.acl-long.521.pdf.json
the O
human O
reference O
answer O
is O
simply O
“ O
yes O
, O
they O
did. O
” O
, O
but O
gpt-2 O
returns O
“ O
yes O
, O
they O
bought O
chicken O
and O
a O
roast. O
” O
, O
which O
is O
more O
detailed O
, O
even O
containing O
item O
names O
derived O
from O
the O
context O
. O

section 7
id pdf2json/2021.acl-long.521.pdf.json
whereas O
bertscore O
can O
not O
evaluate O
such O
cases O
where O
the O
generated O
candidate O
is O
over-described O
with O
respect O
to O
the O
human O
reference O
, O
mars O
uses O
augmented O
references O
enriched O
with O
information O
from O
the O
context O
to O
provide O
a O
fairer O
judgement O
. O

section 8
id pdf2json/2021.acl-long.521.pdf.json
good O
evaluation O
metrics O
ought O
to O
also O
be O
able O
to O
detect O
adversarial O
examples O
by O
assigning O
them O
lower O
scores O
than O
well-formed O
candidates O
. O

section 8
id pdf2json/2021.acl-long.521.pdf.json
as O
shown O
in O
table O
4 O
, O
uni-gram O
matching O
bleu-1 O
can O
not O
detect O
reordered O
sequences O
, O
while O
rouge-l O
scores O
reordered O
sequence O
higher O
occasionally O
if O
tokenswapping O
leads O
to O
more O
lcs O
. O

section 8
id pdf2json/2021.acl-long.521.pdf.json
sentence O
mover O
’ O
s O
similarity O
combines O
word O
and O
sentence O
embeddings O
and O
thus O
is O
more O
capable O
of O
recognizing O
reordered O
samples O
than O
moverscore O
. O

section 8
id pdf2json/2021.acl-long.521.pdf.json
perplexity O
can O
detect O
reordered O
examples O
effectively O
, O
but O
is O
unable O
to O
detect O
retrieved O
sentences O
, O
as O
they O
are O
usually O
well-formed O
. O

section 8
id pdf2json/2021.acl-long.521.pdf.json
mars O
, O
on O
the O
other O
hand O
, O
has O
the O
best O
robustness O
against O
adversarial O
samples O
, O
possibly O
because O
multiple O
context-infused O
augmented O
references O
help O
mars O
detect O
adversarial O
samples O
more O
reliably O
. O

section 8
id pdf2json/2021.acl-long.521.pdf.json
we O
also O
study O
the O
effects O
of O
contextual O
embeddings O
we O
use O
in O
§2.3—when O
switching O
to O
glove O
embeddings O
( O
pennington O
et O
al. O
, O
2014 O
) O
, O
which O
are O
not O
contextual O
, O
mars O
is O
less O
able O
to O
detect O
adversarial O
samples O
, O
especially O
reordered O
ones O
. O

section 8
id pdf2json/2021.acl-long.521.pdf.json
the O
naive O
method O
, O
which O
by O
default O
uses O
roberta O
embedding O
, O
achieves O
comparable O
robustness O
as O
mars O
but O
its O
task-level O
correlations O
with O
humans O
( O
ref O
. O
) O

section 8
id pdf2json/2021.acl-long.521.pdf.json
are O
generally O
lower O
than O
mars O
, O
potentially O
because O
its O
fixed-length O
cloze O
generation O
limits O
the O
diversity O
of O
augmented O
references O
. O

section 9
id pdf2json/2021.acl-long.521.pdf.json
the O
masking O
ratios O
for O
mars O
are O
set O
using O
the O
hyperparameter O
{ O
_ O
} O
max O
, O
which O
corresponds O
to O
mars O
using O
masking O
ratios O
from O
0 O
% O
to O
{ O
_ O
} O
max O
in O
increments O
of O
20 O
% O
, O
e.g. O
, O
{ O
_ O
} O
max O
= O
40 O
% O
indicates O
_ O
∈ O
{ O
0 O
% O
, O
20 O
% O
, O
40 O
% O
} O
. O

section 9
id pdf2json/2021.acl-long.521.pdf.json
in O
preliminary O
experiments O
, O
we O
observed O
that O
{ O
_ O
} O
max O
varied O
for O
different O
datasets O
. O

section 9
id pdf2json/2021.acl-long.521.pdf.json
thus O
, O
for O
our O
three O
generation O
tasks O
, O
we O
evaluate O
mars O
performance O
given O
different O
{ O
_ O
} O
max O
, O
as O
shown O
in O
table O
5 O
. O

section 9
id pdf2json/2021.acl-long.521.pdf.json
we O
find O
that O
tasks O
that O
were O
more O
open-ended O
( O
low O
ω O
; O
e.g. O
, O
story O
generation O
) O
benefited O
from O
higher O
{ O
_ O
} O
max O
, O
which O
created O
a O
more O
diverse O
set O
of O
augmented O
references O
, O
whereas O
tasks O
that O
were O
less O
open-ended O
( O
high O
ω O
; O
e.g. O
, O
qa O
) O
worked O
better O
with O
lower O
{ O
_ O
} O
max O
, O
which O
kept O
the O
augmented O
references O
more O
similar O
to O
the O
original O
. O

section 10
id pdf2json/2021.acl-long.521.pdf.json
we O
analyzed O
cases O
where O
mars O
score O
substantially O
differed O
from O
human O
judgements O
. O

section 10
id pdf2json/2021.acl-long.521.pdf.json
from O
test O
set O
outputs O
, O
we O
found O
that O
errors O
could O
often O
be O
categorized O
into O
one O
of O
three O
types O
( O
shown O
in O
table O
6 O
) O
: O
( O
1 O
) O
out O
of O
vocabulary O
errors O
, O
often O
induced O
by O
unknown O
tokens O
in O
the O
candidates O
, O
( O
2 O
) O
confusion O
errors O
, O
where O
candidates O
are O
simply O
copied O
from O
context O
, O
and O
( O
3 O
) O
inference O
errors O
, O
where O
the O
candidates O
are O
further O
inferences O
of O
the O
context O
based O
on O
commonsense O
knowledge O
. O

section 10
id pdf2json/2021.acl-long.521.pdf.json
in O
these O
cases O
, O
human O
annotators O
tended O
to O
assign O
higher O
scores O
, O
whereas O
, O
mars O
over-penalized O
them O
. O

section 11
id pdf2json/2021.acl-long.521.pdf.json
we O
conducted O
human O
evaluation O
on O
amazon O
mechanical O
turk O
( O
mturk O
) O
to O
further O
study O
the O
quality O
of O
mars O
augmentation O
. O

section 11
id pdf2json/2021.acl-long.521.pdf.json
in O
total O
150 O
participants O
were O
randomly O
assigned O
to O
evaluate O
the O
three O
tasks O
. O

section 11
id pdf2json/2021.acl-long.521.pdf.json
participants O
( O
61.3 O
% O
male O
and O
38.7 O
% O
female O
) O
were O
all O
from O
the O
united O
states O
and O
above O
18 O
years O
old O
, O
with O
an O
average O
age O
of O
34.7 O
years O
old O
. O

section 11
id pdf2json/2021.acl-long.521.pdf.json
each O
participant O
was O
paid O
75 O
cents O
for O
completing O
14 O
questions O
in O
each O
questionnaire O
( O
average O
completion O
time O
per O
questionnaire O
was O
about O
5.11 O
minutes O
) O
. O

section 11
id pdf2json/2021.acl-long.521.pdf.json
results O
we O
conducted O
paired O
sample O
c-tests O
to O
examine O
how O
much O
the O
augmentation O
samples O
resemble O
the O
original O
human O
references O
regarding O
relevance O
to O
context O
and O
readability O
. O

section 11
id pdf2json/2021.acl-long.521.pdf.json
as O
shown O
in O
table O
7 O
, O
in O
terms O
of O
relevance O
to O
context O
, O
mars O
had O
no O
statistically O
significant O
difference O
compared O
with O
original O
human O
references O
in O
newsroom O
and O
mocha O
datasets O
, O
but O
was O
rated O
as O
even O
more O
relevant O
to O
the O
generation O
context O
than O
the O
human O
reference O
in O
the O
roc O
dataset O
( O
mars O
mean O
= O
5.07 O
> O
human O
ref O
. O

section 11
id pdf2json/2021.acl-long.521.pdf.json
mean O
= O
4.95 O
) O
, O
possibly O
because O
reinforced O
self-planning O
guided O
the O
augmentation O
to O
be O
more O
related O
to O
the O
context O
. O

section 11
id pdf2json/2021.acl-long.521.pdf.json
in O
terms O
of O
readabil- O
ity O
, O
both O
mars O
and O
naive O
were O
rated O
lower O
than O
the O
original O
but O
not O
significantly O
; O
we O
take O
this O
as O
a O
compromise O
of O
cloze O
style O
augmentation O
. O

section 11
id pdf2json/2021.acl-long.521.pdf.json
no O
statistically O
significant O
differences O
were O
seen O
between O
the O
original O
and O
mars O
augmentation O
in O
overall O
ratings O
across O
the O
three O
tasks O
. O

section 11
id pdf2json/2021.acl-long.521.pdf.json
these O
results O
further O
confirm O
that O
augmented O
examples O
from O
mars O
are O
of O
similar O
quality O
to O
the O
original O
human O
references O
. O

section 12
id pdf2json/2021.acl-long.521.pdf.json
unsupervised O
metrics O
. O

section 12
id pdf2json/2021.acl-long.521.pdf.json
in O
addition O
to O
the O
metrics O
we O
directly O
compared O
with O
previously O
, O
other O
unsupervised O
metrics O
have O
also O
been O
proposed O
. O

section 12
id pdf2json/2021.acl-long.521.pdf.json
ter O
( O
snover O
et O
al. O
, O
2006 O
) O
, O
character O
( O
wang O
et O
al. O
, O
2016 O
) O
, O
and O
chrf O
( O
popović O
, O
2017 O
) O
focus O
on O
character-level O
overlaps O
instead O
of O
=-gram O
matching O
. O

section 12
id pdf2json/2021.acl-long.521.pdf.json
similar O
to O
bertscore O
, O
yisi O
( O
lo O
, O
2019 O
) O
and O
bertr O
( O
mathur O
et O
al. O
, O
2019 O
) O
leverage O
pre-trained O
contextual O
embeddings O
to O
better O
capture O
similarity O
. O

section 12
id pdf2json/2021.acl-long.521.pdf.json
δbleu O
( O
galley O
et O
al. O
, O
2015 O
) O
adds O
human O
annotated O
sentences O
as O
negative O
references O
. O

section 12
id pdf2json/2021.acl-long.521.pdf.json
bawden O
et O
al O
. O

section 12
id pdf2json/2021.acl-long.521.pdf.json
( O
2020 O
) O
find O
the O
gain O
from O
multiple O
references O
can O
be O
limited O
by O
inherent O
weaknesses O
in O
bleu O
. O

section 12
id pdf2json/2021.acl-long.521.pdf.json
we O
considered O
lessons O
from O
many O
of O
the O
above O
works O
while O
designing O
mars O
. O

section 12
id pdf2json/2021.acl-long.521.pdf.json
learned O
metrics O
. O

section 12
id pdf2json/2021.acl-long.521.pdf.json
compared O
with O
unsupervised O
metrics O
, O
learned O
metrics O
collect O
human O
supervisions O
( O
freitag O
et O
al. O
, O
2020a O
; O
chaganty O
et O
al. O
, O
2018 O
) O
or O
train O
on O
specially O
prepared O
data O
of O
a O
certain O
domain O
( O
sellam O
et O
al. O
, O
2020 O
; O
rei O
et O
al. O
, O
2020 O
) O
. O

section 12
id pdf2json/2021.acl-long.521.pdf.json
other O
approaches O
train O
on O
related O
tasks O
and O
use O
these O
models O
as O
metrics O
for O
the O
original O
task O
( O
goodrich O
et O
al. O
, O
2019 O
; O
eyal O
et O
al. O
, O
2019 O
) O
. O

section 12
id pdf2json/2021.acl-long.521.pdf.json
whereas O
learned O
metrics O
may O
have O
limited O
applicability O
on O
tasks O
where O
no O
such O
resources O
are O
available O
, O
mars O
fully O
exploits O
the O
few-shot O
learning O
abilities O
of O
off-the-shelf O
lms O
and O
therefore O
does O
not O
require O
additional O
training O
. O

section 12
id pdf2json/2021.acl-long.521.pdf.json
task-specific O
metrics O
. O

section 12
id pdf2json/2021.acl-long.521.pdf.json
finally O
, O
many O
metrics O
have O
been O
proposed O
for O
task-specific O
evaluation O
, O
such O
as O
leic O
( O
cui O
et O
al. O
, O
2018 O
) O
and O
cider O
( O
vedantam O
et O
al. O
, O
2015 O
) O
for O
image O
captioning O
, O
parent O
( O
dhingra O
et O
al. O
, O
2019 O
) O
for O
table-to-text O
, O
and O
easse O
( O
alva-manchego O
et O
al. O
, O
2019 O
) O
for O
sentence O
simplification O
. O

section 12
id pdf2json/2021.acl-long.521.pdf.json
mars O
, O
with O
some O
modifications O
, O
can O
potentially O
be O
extended O
to O
these O
tasks O
. O

section 13
id pdf2json/2021.acl-long.521.pdf.json
mars O
can O
be O
limited O
by O
the O
lm O
that O
it O
uses— O
for O
instance O
, O
the O
total O
length O
of O
context O
+ O
reference/candidate O
is O
limited O
by O
the O
max O
sequence O
length O
of O
the O
lm O
used O
. O

section 13
id pdf2json/2021.acl-long.521.pdf.json
additionally O
, O
our O
work O
has O
focused O
on O
english O
, O
and O
mars O
may O
require O
non-trivial O
modifications O
to O
handle O
cases O
where O
the O
context O
and O
reference/candidate O
are O
in O
different O
languages O
, O
such O
as O
machine O
translation O
. O

section 13
id pdf2json/2021.acl-long.521.pdf.json
future O
work O
, O
could O
potentially O
extend O
mars O
to O
these O
scenarios O
using O
multi-lingual O
sequence-to-sequence O
models O
such O
as O
multilingual-t5 O
( O
xue O
et O
al. O
, O
2020 O
) O
. O

section 13
id pdf2json/2021.acl-long.521.pdf.json
we O
also O
analyzed O
errors O
and O
found O
that O
mars O
sometimes O
under-scores O
candidates O
that O
contained O
unknown O
tokens O
or O
were O
copied O
directly O
from O
the O
context O
( O
see O
appendix O
c O
for O
examples O
and O
further O
analysis O
) O
. O

section 14
id pdf2json/2021.acl-long.521.pdf.json
we O
have O
proposed O
mars O
, O
a O
context-aware O
and O
easy-to-deploy O
nlg O
metric O
built O
upon O
an O
off-theshelf O
language O
model O
( O
gpt-2 O
) O
. O

section 14
id pdf2json/2021.acl-long.521.pdf.json
on O
three O
contextual O
nlg O
tasks O
, O
we O
show O
that O
mars O
better O
correlates O
with O
human O
judgements O
compared O
with O
seven O
other O
unsupervised O
metrics O
. O

section 14
id pdf2json/2021.acl-long.521.pdf.json
requiring O
neither O
costly O
human O
supervision O
nor O
additional O
training O
, O
mars O
can O
be O
applied O
to O
a O
broad O
range O
of O
nlg O
tasks O
. O

section 14
id pdf2json/2021.acl-long.521.pdf.json
ethical O
considerations O
the O
goal O
of O
mars O
is O
to O
aid O
the O
evaluation O
of O
nlg O
models O
, O
and O
hence O
we O
draw O
attention O
to O
several O
ethical O
considerations O
. O

section 14
id pdf2json/2021.acl-long.521.pdf.json
first O
, O
the O
augmented O
references O
of O
mars O
can O
be O
affected O
by O
certain O
biases O
from O
the O
lm O
it O
is O
based O
on O
( O
e.g. O
, O
gpt-2 O
) O
( O
liu O
et O
al. O
, O
2021 O
) O
, O
though O
those O
biases O
may O
be O
partially O
mitigated O
by O
the O
relatively O
narrow O
scope O
of O
cloze O
completion O
and O
by O
generations O
being O
guided O
by O
given O
context O
and O
human O
references O
. O

section 14
id pdf2json/2021.acl-long.521.pdf.json
second O
, O
mars O
facilitates O
evaluation O
and O
therefore O
development O
of O
nlg O
models O
, O
for O
which O
a O
major O
ethical O
consideration O
is O
that O
they O
can O
mimic O
target O
properties O
in O
training O
data O
that O
are O
undesirable O
. O

section 14
id pdf2json/2021.acl-long.521.pdf.json
this O
is O
especially O
true O
of O
models O
trained O
on O
non-contemporary O
data O
that O
does O
not O
represent O
current O
norms O
and O
practices O
. O

section 14
id pdf2json/2021.acl-long.521.pdf.json
these O
biases O
can O
lead O
to O
ethical O
concerns O
if O
users O
or O
deployers O
of O
models O
are O
not O
aware O
of O
these O
issues O
or O
do O
not O
account O
for O
them O
. O

section 14
id pdf2json/2021.acl-long.521.pdf.json
more O
generally O
, O
nlg O
models O
can O
also O
be O
used O
in O
malicious O
ways O
such O
as O
to O
generate O
fake O
news O
or O
spam O
, O
which O
we O
strongly O
discourage O
. O

section 14
id pdf2json/2021.acl-long.521.pdf.json
finally O
, O
our O
experiments O
and O
analysis O
are O
done O
in O
english O
, O
and O
therefore O
we O
do O
not O
claim O
that O
our O
findings O
will O
generalize O
across O
all O
languages O
, O
although O
our O
framework O
has O
potential O
to O
be O
extended O
to O
other O
languages O
with O
necessary O
modifications O
. O

section 15
id pdf2json/2021.acl-long.521.pdf.json
as O
part O
of O
eq.1 O
in O
the O
main O
paper O
, O
we O
define O
the O
idf O
score O
given O
token O
g8 O
and O
a O
corpus O
- O
containing O
`` O
documents O
as O
: O
idf O
( O
g8 O
, O
- O
) O
= O
− O
log O
1 O
`` O
`` O
∑ O
9=1 O
[ O
g8 O
∈ O
- O
9 O
] O
, O
where O
[ O
· O
] O
is O
the O
indicator O
function O
. O

section 15
id pdf2json/2021.acl-long.521.pdf.json
we O
present O
our O
dp-based O
masking O
algorithm O
in O
algorithm O
1 O
: O
algorithm O
1 O
: O
dp-based O
token O
masking O
input O
: O
human O
reference O
{ O
g8 O
} O
# O
8=1 O
, O
masking O
ratio O
_ O
, O
and O
task-specific O
factor O
u. O
compute O
e8 O
for O
each O
g8 O
with O
u O
( O
eq O
. O

section 15
id pdf2json/2021.acl-long.521.pdf.json
1 O
) O
; O
compute O
f8 O
depending O
on O
lcs O
for O
each O
g8 O
; O
init O
dp-table O
) O
[ O
# O
+ O
1 O
] O
[ O
, O
max O
+ O
1 O
] O
with O
all O
0 O
; O
for O
8 O
= O
1 O
, O
2 O
, O
. O

section 15
id pdf2json/2021.acl-long.521.pdf.json
. O

section 15
id pdf2json/2021.acl-long.521.pdf.json
. O

section 15
id pdf2json/2021.acl-long.521.pdf.json
, O
# O
do O
for O
9 O
= O
1 O
, O
2 O
, O
. O

section 15
id pdf2json/2021.acl-long.521.pdf.json
. O

section 15
id pdf2json/2021.acl-long.521.pdf.json
. O

section 15
id pdf2json/2021.acl-long.521.pdf.json
, O
,max O
do O
if O
9 O
− O
f8−1 O
< O
0 O
then O
) O
[ O
8 O
] O
[ O
9 O
] O
= O
) O
[ O
8 O
− O
1 O
] O
[ O
9 O
] O
; O
record O
masking O
choice O
q O
( O
g8 O
) O
; O
else O
) O
[ O
8 O
] O
[ O
9 O
] O
= O
max O
( O
) O
[ O
8 O
− O
1 O
] O
[ O
9 O
] O
, O
) O
[ O
8 O
− O
1 O
] O
[ O
9 O
− O
f8−1 O
] O
+ O
e8−1 O
) O
; O
record O
masking O
choice O
q O
( O
g8 O
) O
; O
end O
end O
end O
{ O
q O
( O
g8 O
) O
# O
8=1 O
} O
← O
backtracking O
via O
records O
; O
return O
best O
masking O
strategy O
{ O
q O
( O
g8 O
) O
# O
8=1 O
} O
; O

section 16
id pdf2json/2021.acl-long.521.pdf.json
the O
complete O
procedure O
for O
augmenting O
human O
references O
is O
presented O
in O
algorithm O
2 O
. O

section 16
id pdf2json/2021.acl-long.521.pdf.json
for O
a O
given O
template O
, O
we O
first O
group O
the O
tokens O
into O
a O
blockby-block O
form O
with O
blank O
blocks O
( O
[ O
b O
] O
) O
and O
text O
blocks O
( O
[ O
t O
] O
) O
. O

section 16
id pdf2json/2021.acl-long.521.pdf.json
then O
, O
we O
generate O
varying O
lengths O
of O
tokens O
, O
iteratively O
concatenating O
them O
with O
next O
text O
block O
, O
and O
judging O
them O
based O
on O
ppl O
, O
and O
finally O
revising O
current O
generations O
accordingly O
. O

section 16
id pdf2json/2021.acl-long.521.pdf.json
we O
use O
the O
language O
modeling O
ability O
of O
lm O
to O
check O
the O
perplexity O
of O
the O
current O
sequence O
, O
and O
set O
a O
hyper-parameter O
f O
to O
control O
the O
maximum O
extended O
generation O
( O
for O
a O
lower O
ppl O
) O
. O

section 16
id pdf2json/2021.acl-long.521.pdf.json
depending O
on O
whether O
there O
is O
a O
subsequent O
text O
block O
, O
the O
generation O
will O
switch O
between O
two O
algorithm O
2 O
: O
generate O
, O
judge O
and O
revise O
input O
: O
template O
{ O
q O
( O
g8 O
) O
} O
# O
8=1 O
, O
max O
guess O
f O
, O
and O
lm O
perplexity O
checker O
ppl O
. O

section 16
id pdf2json/2021.acl-long.521.pdf.json
group O
{ O
q O
( O
g8 O
) O
} O
# O
8=1 O
into O
[ O
b O
] O
and O
[ O
t O
] O
; O
init O
final O
output O
b O
; O
foreach O
block O
do O
8 O
← O
0 O
; O
init O
priority O
queue O
@ O
, O
buffer O
b′ O
; O
if O
[ O
t O
] O
then O
append O
[ O
t O
] O
to O
b O
; O
else O
if O
[ O
b O
] O
then O
while O
8 O
< O
f O
+ O
| O
[ O
b O
] O
| O
do O
if O
next O
is O
[ O
t O
] O
then O
f O
← O
self-planning O
gen. O
; O
else O
f O
← O
open-ended O
gen. O
; O
end O
b′← O
b O
+ O
f O
; O
record O
( O
ppl O
( O
b′ O
+ O
[ O
t O
] O
) O
, O
b′ O
) O
in O
@ O
; O
8 O
← O
8 O
+ O
1 O
; O
end O
b← O
b O
+ O
lowest O
ppl O
b′ O
pop O
from O
@ O
; O
end O
end O
return O
augmented O
reference O
b O
; O
modes O
: O
self-planning O
generation O
( O
if O
there O
is O
future O
context O
) O
and O
open-ended O
generation O
( O
otherwise O
) O
. O

section 16
id pdf2json/2021.acl-long.521.pdf.json
we O
use O
a O
priority O
queue O
to O
store O
each O
step O
generation O
and O
its O
corresponding O
ppl O
for O
quick O
revisions O
afterwards O
. O

section TITLE
id pdf2json/2021.acl-long.337.pdf.json
hierarchy-aware O
label O
semantics O
matching O
network O
for O
hierarchical O
text O
classification O

section ABSTRACT
id pdf2json/2021.acl-long.337.pdf.json
hierarchical O
text O
classification O
is O
an O
important O
yet O
challenging O
task O
due O
to O
the O
complex O
structure O
of O
the O
label O
hierarchy O
. O

section ABSTRACT
id pdf2json/2021.acl-long.337.pdf.json
existing O
methods O
ignore O
the O
semantic O
relationship O
between O
text O
and O
labels O
, O
so O
they O
can O
not O
make O
full O
use O
of O
the O
hierarchical O
information O
. O

section ABSTRACT
id pdf2json/2021.acl-long.337.pdf.json
to O
this O
end O
, O
we O
formulate O
the O
text-label O
semantics O
relationship O
as O
a O
semantic O
matching O
problem O
and O
thus O
propose O
a O
hierarchy-aware O
label O
semantics O
matching O
network O
( O
himatch O
) O
. O

section ABSTRACT
id pdf2json/2021.acl-long.337.pdf.json
first O
, O
we O
project O
text O
semantics O
and O
label O
semantics O
into O
a O
joint O
embedding O
space O
. O

section ABSTRACT
id pdf2json/2021.acl-long.337.pdf.json
we O
then O
introduce O
a O
joint O
embedding O
loss O
and O
a O
matching O
learning O
loss O
to O
model O
the O
matching O
relationship O
between O
the O
text O
semantics O
and O
the O
label O
semantics O
. O

section ABSTRACT
id pdf2json/2021.acl-long.337.pdf.json
our O
model O
captures O
the O
text-label O
semantics O
matching O
relationship O
among O
coarse-grained O
labels O
and O
fine-grained O
labels O
in O
a O
hierarchy-aware O
manner O
. O

section ABSTRACT
id pdf2json/2021.acl-long.337.pdf.json
the O
experimental O
results O
on O
various O
benchmark O
datasets O
verify O
that O
our O
model O
achieves O
state-of-the-art O
results O
. O

section 0
id pdf2json/2021.acl-long.337.pdf.json
proceedings O
of O
the O
59th O
annual O
meeting O
of O
the O
association O
for O
computational O
linguistics O
and O
the O
11th O
international O
joint O
conference O
on O
natural O
language O
processing O
, O
pages O
4370–4379 O
august O
1–6 O
, O
2021 O
. O

section 0
id pdf2json/2021.acl-long.337.pdf.json
©2021 O
association O
for O
computational O
linguistics O
4370 O

section 1
id pdf2json/2021.acl-long.337.pdf.json
hierarchical O
text O
classification O
( O
htc O
) O
is O
widely O
used O
in O
natural O
language O
processing O
( O
nlp O
) O
, O
such O
as O
news O
categorization O
( O
lewis O
et O
al. O
, O
2004 O
) O
and O
scientific O
paper O
classification O
( O
kowsari O
et O
al. O
, O
2017 O
) O
. O

section 1
id pdf2json/2021.acl-long.337.pdf.json
htc O
is O
a O
particular O
multi-label O
text O
classification O
problem O
, O
which O
introduces O
hierarchies O
to O
organize O
label O
structure O
. O

section 1
id pdf2json/2021.acl-long.337.pdf.json
as O
depicted O
in O
figure O
1 O
, O
htc O
models O
predict O
multiple O
labels O
in O
a O
given O
label O
hierarchy O
, O
which O
generally O
construct O
one O
or O
multiple O
paths O
from O
coarse-grained O
labels O
to O
fine-grained O
labels O
in O
a O
top-down O
manner O
( O
aixin O
sun O
and O
ee-peng O
lim O
, O
2001 O
) O
. O

section 1
id pdf2json/2021.acl-long.337.pdf.json
generally O
speaking O
, O
fine-grained O
labels O
are O
the O
most O
appropriate O
labels O
for O
describing O
the O
input O
text O
. O

section 1
id pdf2json/2021.acl-long.337.pdf.json
coarse-grained O
labels O
are O
generally O
the O
parent O
nodes O
of O
coarse- O
or O
fine-grained O
labels O
, O
expressing O
a O
more O
general O
concept O
. O

section 1
id pdf2json/2021.acl-long.337.pdf.json
the O
key O
challenges O
of O
∗*corresponding O
author O
htc O
are O
to O
model O
the O
large-scale O
, O
imbalanced O
, O
and O
structured O
label O
hierarchy O
( O
mao O
et O
al. O
, O
2019 O
) O
. O

section 1
id pdf2json/2021.acl-long.337.pdf.json
input O
text O
: O
`` O
global O
debt O
is O
set O
to O
reach O
$ O
200 O
trillion O
... O
'' O
existing O
work O
in O
htc O
has O
introduced O
various O
methods O
to O
use O
hierarchical O
information O
in O
a O
holistic O
way O
. O

section 1
id pdf2json/2021.acl-long.337.pdf.json
to O
capture O
the O
holistic O
label O
correlation O
features O
, O
some O
researchers O
proposed O
a O
hierarchyaware O
global O
model O
to O
exploit O
the O
prior O
probability O
of O
label O
dependencies O
through O
graph O
convolution O
networks O
( O
gcn O
) O
and O
treelstm O
( O
zhou O
et O
al. O
, O
2020 O
) O
. O

section 1
id pdf2json/2021.acl-long.337.pdf.json
some O
researchers O
also O
introduced O
more O
label O
correlation O
features O
such O
as O
label O
semantic O
similarity O
and O
label O
co-occurrence O
( O
lu O
et O
al. O
, O
2020 O
) O
. O

section 1
id pdf2json/2021.acl-long.337.pdf.json
they O
followed O
the O
traditional O
way O
to O
transform O
htc O
into O
multiple O
binary O
classifiers O
for O
every O
label O
( O
fürnkranz O
et O
al. O
, O
2008 O
) O
. O

section 1
id pdf2json/2021.acl-long.337.pdf.json
however O
, O
they O
ignored O
the O
interaction O
between O
text O
semantics O
and O
label O
semantics O
( O
fürnkranz O
et O
al. O
, O
2008 O
; O
wang O
et O
al. O
, O
2019 O
) O
, O
which O
is O
highly O
useful O
for O
classification O
( O
chen O
et O
al. O
, O
2020 O
) O
. O

section 1
id pdf2json/2021.acl-long.337.pdf.json
hence O
, O
their O
models O
may O
not O
be O
sufficient O
to O
model O
complex O
label O
dependencies O
and O
provide O
comparable O
text-label O
classification O
scores O
( O
wang O
et O
al. O
, O
2019 O
) O
. O

section 1
id pdf2json/2021.acl-long.337.pdf.json
a O
natural O
strategy O
for O
modeling O
the O
interaction O
between O
text O
semantics O
and O
label O
semantics O
is O
to O
introduce O
a O
text-label O
joint O
embedding O
by O
label O
attention O
( O
xiao O
et O
al. O
, O
2019 O
) O
or O
autoencoders O
( O
yeh O
et O
al. O
, O
2017 O
) O
. O

section 1
id pdf2json/2021.acl-long.337.pdf.json
label O
attention-based O
methods O
adopted O
a O
self-attention O
mechanism O
to O
identify O
label-specific O
information O
( O
xiao O
et O
al. O
, O
2019 O
) O
. O

section 1
id pdf2json/2021.acl-long.337.pdf.json
autoencoder-based O
methods O
extended O
the O
vanilla O
canonical O
correlated O
autoencoder O
( O
yeh O
et O
al. O
, O
2017 O
) O
to O
a O
ranking-based O
autoencoder O
architecture O
to O
produce O
comparable O
text-label O
scores O
( O
wang O
et O
al. O
, O
2019 O
) O
. O

section 1
id pdf2json/2021.acl-long.337.pdf.json
however O
, O
these O
methods O
assume O
all O
the O
labels O
are O
independent O
without O
fully O
considering O
the O
correlation O
between O
coarse-grained O
labels O
and O
fine-grained O
labels O
, O
which O
can O
not O
be O
simply O
transferred O
to O
htc O
models O
( O
zhou O
et O
al. O
, O
2020 O
) O
. O

section 1
id pdf2json/2021.acl-long.337.pdf.json
in O
this O
paper O
, O
we O
formulate O
the O
interaction O
between O
text O
and O
label O
as O
a O
semantic O
matching O
problem O
and O
propose O
a O
hierarchy-aware O
label O
semantics O
matching O
network O
( O
himatch O
) O
. O

section 1
id pdf2json/2021.acl-long.337.pdf.json
the O
principal O
idea O
is O
that O
the O
text O
representations O
should O
be O
semantically O
similar O
to O
the O
target O
label O
representations O
( O
especially O
fine-grained O
labels O
) O
, O
while O
they O
should O
be O
semantically O
far O
away O
from O
the O
incorrect O
label O
representations O
. O

section 1
id pdf2json/2021.acl-long.337.pdf.json
first O
, O
we O
adopt O
a O
text O
encoder O
and O
a O
label O
encoder O
( O
shown O
in O
figure O
2 O
) O
to O
extract O
textual O
semantics O
and O
label O
semantics O
, O
respectively O
. O

section 1
id pdf2json/2021.acl-long.337.pdf.json
second O
, O
inspired O
by O
the O
methods O
of O
learning O
common O
embeddings O
( O
wang O
et O
al. O
, O
2019 O
) O
, O
we O
project O
both O
textual O
semantics O
and O
label O
semantics O
into O
a O
text-label O
joint O
embedding O
space O
where O
correlations O
between O
text O
and O
labels O
are O
exploited O
. O

section 1
id pdf2json/2021.acl-long.337.pdf.json
in O
this O
joint O
embedding O
space O
, O
we O
introduce O
a O
joint O
embedding O
loss O
between O
text O
semantics O
and O
target O
label O
semantics O
to O
learn O
a O
text-label O
joint O
embedding O
. O

section 1
id pdf2json/2021.acl-long.337.pdf.json
after O
that O
, O
we O
apply O
a O
matching O
learning O
loss O
to O
capture O
text-label O
matching O
relationships O
in O
a O
hierarchy-aware O
manner O
. O

section 1
id pdf2json/2021.acl-long.337.pdf.json
in O
this O
way O
, O
the O
finegrained O
labels O
are O
semantically O
closest O
to O
the O
text O
semantics O
, O
followed O
by O
the O
coarse-grained O
labels O
, O
while O
the O
incorrect O
labels O
should O
be O
semantically O
far O
away O
from O
the O
text O
semantics O
. O

section 1
id pdf2json/2021.acl-long.337.pdf.json
hence O
, O
we O
propose O
a O
hierarchy-aware O
matching O
learning O
method O
to O
capture O
different O
matching O
relationships O
through O
different O
penalty O
margins O
on O
semantic O
distances O
. O

section 1
id pdf2json/2021.acl-long.337.pdf.json
finally O
, O
we O
employ O
the O
textual O
representations O
guided O
by O
the O
joint O
embedding O
loss O
and O
matching O
learning O
loss O
to O
perform O
the O
hierarchical O
text O
classification O
. O

section 1
id pdf2json/2021.acl-long.337.pdf.json
the O
major O
contributions O
of O
this O
paper O
are O
: O
1 O
. O

section 1
id pdf2json/2021.acl-long.337.pdf.json
by O
considering O
the O
text-label O
semantics O
matching O
relationship O
, O
we O
are O
the O
first O
to O
formulate O
htc O
as O
a O
semantic O
matching O
problem O
rather O
than O
merely O
multiple O
binary O
classification O
tasks O
. O

section 1
id pdf2json/2021.acl-long.337.pdf.json
2 O
. O

section 1
id pdf2json/2021.acl-long.337.pdf.json
we O
propose O
a O
hierarchy-aware O
label O
semantics O
matching O
network O
( O
himatch O
) O
, O
in O
which O
we O
introduce O
a O
joint O
embedding O
loss O
and O
a O
matching O
learn- O
ing O
loss O
to O
learn O
the O
text-label O
semantics O
matching O
relationship O
in O
a O
hierarchy-aware O
manner O
. O

section 1
id pdf2json/2021.acl-long.337.pdf.json
3 O
. O

section 1
id pdf2json/2021.acl-long.337.pdf.json
extensive O
experiments O
( O
with/without O
bert O
) O
on O
various O
datasets O
show O
that O
our O
model O
achieves O
state-of-the-art O
results O
. O

section 3
id pdf2json/2021.acl-long.337.pdf.json
hierarchical O
text O
classification O
is O
a O
particular O
multilabel O
text O
classification O
problem O
, O
where O
the O
classification O
results O
are O
assigned O
to O
one O
or O
more O
nodes O
of O
a O
taxonomic O
hierarchy O
. O

section 3
id pdf2json/2021.acl-long.337.pdf.json
existing O
state-of-the-art O
methods O
focus O
on O
encoding O
hierarchy O
constraint O
in O
a O
global O
view O
such O
as O
directed O
graph O
and O
tree O
structure O
. O

section 3
id pdf2json/2021.acl-long.337.pdf.json
zhou O
et O
al O
. O

section 3
id pdf2json/2021.acl-long.337.pdf.json
( O
2020 O
) O
proposed O
a O
hierarchyaware O
global O
model O
to O
exploit O
the O
prior O
probability O
of O
label O
dependencies O
. O

section 3
id pdf2json/2021.acl-long.337.pdf.json
lu O
et O
al O
. O

section 3
id pdf2json/2021.acl-long.337.pdf.json
( O
2020 O
) O
introduced O
three O
kinds O
of O
label O
knowledge O
graphs O
, O
i.e. O
, O
taxonomy O
graph O
, O
semantic O
similarity O
graph O
, O
and O
cooccurrence O
graph O
to O
benefit O
hierarchical O
text O
classification O
. O

section 3
id pdf2json/2021.acl-long.337.pdf.json
they O
regarded O
hierarchical O
text O
classification O
as O
multiple O
binary O
classification O
tasks O
( O
fürnkranz O
et O
al. O
, O
2008 O
) O
. O

section 3
id pdf2json/2021.acl-long.337.pdf.json
the O
limitation O
is O
that O
these O
models O
did O
not O
consider O
the O
interaction O
of O
label O
semantics O
and O
text O
semantics O
. O

section 3
id pdf2json/2021.acl-long.337.pdf.json
therefore O
, O
they O
failed O
to O
capture O
complex O
label O
dependencies O
and O
can O
not O
provide O
comparable O
text-label O
classification O
scores O
( O
wang O
et O
al. O
, O
2019 O
) O
, O
which O
leads O
to O
restricted O
performance O
( O
chen O
et O
al. O
, O
2020 O
) O
. O

section 3
id pdf2json/2021.acl-long.337.pdf.json
hence O
, O
it O
is O
crucial O
to O
exploit O
the O
relationship O
between O
text O
and O
label O
semantics O
, O
and O
help O
the O
model O
distinguish O
target O
labels O
from O
incorrect O
labels O
in O
a O
comparable O
and O
hierarchy-aware O
manner O
. O

section 3
id pdf2json/2021.acl-long.337.pdf.json
we O
perform O
matching O
learning O
in O
a O
joint O
embedding O
of O
text O
and O
label O
to O
solve O
these O
problems O
in O
this O
work O
. O

section 4
id pdf2json/2021.acl-long.337.pdf.json
to O
determine O
the O
correlation O
between O
text O
and O
label O
, O
researchers O
proposed O
various O
methods O
to O
exploit O
a O
text-label O
joint O
embedding O
such O
as O
( O
xiao O
et O
al. O
, O
2019 O
) O
or O
autoencoder O
( O
yeh O
et O
al. O
, O
2017 O
) O
. O

section 4
id pdf2json/2021.acl-long.337.pdf.json
in O
the O
field O
of O
multi-label O
text O
classification O
, O
xiao O
et O
al O
. O

section 4
id pdf2json/2021.acl-long.337.pdf.json
( O
2019 O
) O
proposed O
a O
label-specific O
attention O
network O
( O
lsan O
) O
to O
learn O
a O
text-label O
joint O
embedding O
by O
label O
semantic O
and O
document O
semantic O
. O

section 4
id pdf2json/2021.acl-long.337.pdf.json
wang O
et O
al O
. O

section 4
id pdf2json/2021.acl-long.337.pdf.json
( O
2019 O
) O
extended O
vanilla O
canonical O
correlated O
autoencoder O
( O
yeh O
et O
al. O
, O
2017 O
) O
to O
a O
ranking-based O
autoencoder O
architecture O
to O
produce O
comparable O
label O
scores O
. O

section 4
id pdf2json/2021.acl-long.337.pdf.json
however O
, O
they O
did O
not O
fully O
consider O
label O
semantics O
and O
holistic O
label O
correlation O
among O
fine-grained O
labels O
, O
coarse-grained O
labels O
, O
and O
incorrect O
labels O
. O

section 4
id pdf2json/2021.acl-long.337.pdf.json
in O
addition O
, O
we O
can O
not O
simply O
transfer O
these O
multi-label O
classification O
methods O
to O
htc O
due O
to O
the O
constraint O
of O
hierarchy O
( O
zhou O
et O
al. O
, O
2020 O
) O
. O

section 5
id pdf2json/2021.acl-long.337.pdf.json
in O
this O
section O
, O
we O
will O
describe O
the O
details O
about O
our O
hierarchy-aware O
label O
semantics O
matching O
network O
. O

section 5
id pdf2json/2021.acl-long.337.pdf.json
figure O
2 O
shows O
the O
overall O
architecture O
of O
our O
proposed O
model O
. O

section 6
id pdf2json/2021.acl-long.337.pdf.json
in O
the O
htc O
task O
, O
given O
the O
input O
sequence O
xseq O
= O
{ O
x1 O
, O
... O
, O
xn O
} O
, O
the O
model O
will O
predict O
the O
label O
y O
= O
{ O
y1 O
, O
... O
, O
yk O
} O
where O
n O
is O
the O
number O
of O
words O
and O
k O
is O
the O
number O
of O
label O
sets O
. O

section 6
id pdf2json/2021.acl-long.337.pdf.json
the O
label O
with O
a O
probability O
higher O
than O
a O
fixed O
threshold O
( O
0.5 O
) O
will O
be O
regarded O
as O
the O
prediction O
result O
. O

section 6
id pdf2json/2021.acl-long.337.pdf.json
the O
sequence O
of O
token O
embeddings O
is O
firstly O
fed O
into O
a O
bidirectional O
gru O
layer O
to O
extract O
contextual O
feature O
h O
= O
{ O
h1 O
, O
... O
, O
hn O
} O
. O

section 6
id pdf2json/2021.acl-long.337.pdf.json
then O
, O
cnn O
layers O
with O
top-k O
max-pooling O
are O
adopted O
for O
generating O
key O
n-gram O
features O
t O
∈ O
rk×dcnn O
where O
dcnn O
indicates O
the O
output O
dimension O
of O
the O
cnn O
layer O
. O

section 6
id pdf2json/2021.acl-long.337.pdf.json
following O
the O
previous O
work O
( O
zhou O
et O
al. O
, O
2020 O
) O
, O
we O
further O
introduce O
a O
hierarchy-aware O
text O
feature O
propagation O
module O
to O
encode O
label O
hierarchy O
information O
. O

section 6
id pdf2json/2021.acl-long.337.pdf.json
we O
define O
a O
hierarchy O
label O
structure O
as O
a O
directed O
graph O
g O
= O
( O
vt O
, O
←− O
e O
, O
−→ O
e O
) O
, O
where O
vt O
indicates O
the O
set O
of O
hierarchy O
structure O
nodes O
. O

section 6
id pdf2json/2021.acl-long.337.pdf.json
←− O
e O
are O
built O
from O
the O
top-down O
hierarchy O
paths O
representing O
the O
prior O
statistical O
probability O
from O
parent O
nodes O
to O
children O
nodes O
. O

section 6
id pdf2json/2021.acl-long.337.pdf.json
−→ O
e O
are O
built O
from O
the O
bottom-up O
hierarchy O
paths O
representing O
the O
connection O
relationship O
from O
children O
nodes O
to O
parent O
nodes O
. O

section 6
id pdf2json/2021.acl-long.337.pdf.json
the O
feature O
size O
of O
graph O
adjacency O
matrix O
← O
e O
and O
→ O
e O
is O
∈ O
rk×k O
, O
where O
k O
is O
the O
number O
of O
label O
sets O
. O

section 6
id pdf2json/2021.acl-long.337.pdf.json
text O
feature O
propagation O
module O
firstly O
projects O
text O
features O
t O
to O
node O
inputs O
vt O
by O
a O
linear O
transformation O
wproj O
∈ O
rk×dcnn×dt O
, O
where O
dt O
represents O
the O
hierarchy O
structure O
node O
dimension O
from O
text O
feature O
. O

section 6
id pdf2json/2021.acl-long.337.pdf.json
then O
a O
graph O
convolution O
network O
( O
gcn O
) O
is O
adopted O
to O
explicitly O
combine O
text O
semantics O
with O
prior O
hierarchical O
information←− O
e O
and O
−→ O
e O
: O
st O
= O
σ O
( O
←− O
e O
· O
vt O
·wg1 O
+ O
−→ O
e O
· O
vt O
·wg2 O
) O
( O
1 O
) O
where O
σ O
is O
the O
activation O
function O
relu O
. O

section 6
id pdf2json/2021.acl-long.337.pdf.json
wg1 O
, O
wg2 O
∈ O
rdt×dt O
are O
the O
weight O
matrix O
of O
gcn O
. O

section 6
id pdf2json/2021.acl-long.337.pdf.json
st O
is O
the O
text O
representation O
aware O
of O
prior O
hierarchy O
paths O
. O

section 7
id pdf2json/2021.acl-long.337.pdf.json
in O
the O
htc O
task O
, O
the O
hierarchical O
label O
structure O
can O
be O
regarded O
as O
a O
directed O
graph O
g O
= O
( O
vl O
, O
←− O
e O
, O
−→ O
e O
) O
, O
where O
vl O
indicates O
the O
set O
of O
hierarchy O
structure O
nodes O
with O
label O
representation O
. O

section 7
id pdf2json/2021.acl-long.337.pdf.json
the O
graph O
g O
in O
label O
encoder O
shares O
the O
same O
structure O
←− O
e O
and O
−→ O
e O
with O
the O
graph O
in O
text O
encoder O
. O

section 7
id pdf2json/2021.acl-long.337.pdf.json
given O
the O
total O
label O
set O
y O
= O
{ O
y1 O
, O
... O
, O
yk O
} O
as O
input O
, O
we O
create O
label O
embeddings O
vl O
∈ O
rdl O
by O
averaging O
of O
pre-trained O
label O
embeddings O
first O
. O

section 7
id pdf2json/2021.acl-long.337.pdf.json
then O
gcn O
could O
be O
utilized O
as O
label O
encoder O
: O
sl O
= O
σ O
( O
←− O
e O
· O
vl O
·wg3 O
+ O
−→ O
e O
· O
vl O
·wg4 O
) O
( O
2 O
) O
where O
σ O
is O
the O
activation O
function O
relu O
. O

section 7
id pdf2json/2021.acl-long.337.pdf.json
wg3 O
, O
wg4 O
∈ O
rdl×dl O
are O
the O
weight O
matrix O
of O
gcn O
. O

section 7
id pdf2json/2021.acl-long.337.pdf.json
sl O
is O
the O
label O
representation O
aware O
of O
prior O
hierarchy O
paths O
. O

section 7
id pdf2json/2021.acl-long.337.pdf.json
it O
must O
be O
noted O
that O
the O
weight O
matrix O
and O
input O
representation O
of O
the O
label O
encoder O
are O
different O
with O
those O
in O
the O
text O
encoder O
. O

section 9
id pdf2json/2021.acl-long.337.pdf.json
in O
this O
section O
, O
we O
will O
introduce O
the O
methods O
of O
learning O
a O
text-label O
joint O
embedding O
and O
hierarchyaware O
matching O
relationship O
. O

section 9
id pdf2json/2021.acl-long.337.pdf.json
for O
joint O
embedding O
learning O
, O
firstly O
, O
we O
project O
text O
semantics O
st O
and O
label O
semantics O
sl O
into O
a O
common O
latent O
space O
as O
follows O
: O
φt O
= O
ffnt O
( O
st O
) O
, O
( O
3 O
) O
φl O
= O
ffnl O
( O
sl O
) O
( O
4 O
) O
where O
ffnt O
and O
ffnl O
are O
independent O
two-layer O
feedforward O
neural O
networks O
. O

section 9
id pdf2json/2021.acl-long.337.pdf.json
φt O
, O
φl O
∈ O
rdϕ O
represent O
text O
semantics O
and O
label O
semantics O
in O
joint O
embedding O
space O
, O
respectively O
. O

section 9
id pdf2json/2021.acl-long.337.pdf.json
dϕ O
indicates O
the O
dimension O
of O
joint O
embedding O
. O

section 9
id pdf2json/2021.acl-long.337.pdf.json
in O
order O
to O
align O
the O
two O
independent O
semantic O
representations O
in O
the O
latent O
space O
, O
we O
employ O
the O
mean O
squared O
loss O
between O
text O
semantics O
and O
target O
labels O
semantics O
: O
ljoint O
= O
∑ O
p∈p O
( O
y O
) O
∥∥φt O
− O
φpl O
∥∥22 O
( O
5 O
) O
where O
p O
( O
y O
) O
is O
target O
label O
sets O
. O

section 9
id pdf2json/2021.acl-long.337.pdf.json
ljoint O
aims O
to O
minimize O
the O
common O
embedding O
loss O
between O
input O
text O
and O
target O
labels O
. O

section 10
id pdf2json/2021.acl-long.337.pdf.json
based O
on O
the O
text-label O
joint O
embedding O
loss O
, O
the O
model O
only O
captures O
the O
correlations O
between O
text O
semantics O
and O
target O
labels O
semantics O
, O
while O
correlations O
among O
different O
granular O
labels O
are O
ignored O
. O

section 10
id pdf2json/2021.acl-long.337.pdf.json
in O
the O
htc O
task O
, O
it O
is O
expected O
that O
the O
matching O
relationship O
between O
text O
semantics O
and O
fine-grained O
labels O
should O
be O
the O
closest O
, O
followed O
by O
coarsegrained O
labels O
. O

section 10
id pdf2json/2021.acl-long.337.pdf.json
text O
semantics O
and O
incorrect O
labels O
semantics O
should O
not O
be O
related O
. O

section 10
id pdf2json/2021.acl-long.337.pdf.json
insight O
of O
these O
, O
we O
propose O
a O
hierarchy-aware O
matching O
loss O
lmatch O
to O
incorporate O
the O
correlations O
among O
text O
semantics O
and O
different O
labels O
semantics O
. O

section 10
id pdf2json/2021.acl-long.337.pdf.json
lmatch O
aims O
to O
penalize O
the O
small O
semantic O
distance O
between O
text O
semantics O
and O
incorrect O
labels O
semantics O
with O
a O
margin O
γ O
: O
lmatch O
= O
max O
( O
0 O
, O
d O
( O
φt O
, O
φ O
p O
l O
) O
−d O
( O
φt O
, O
φnl O
) O
+ O
γ O
) O
( O
6 O
) O
where O
φpl O
represents O
target O
labels O
semantics O
and O
φnl O
represents O
incorrect O
labels O
semantics O
. O

section 10
id pdf2json/2021.acl-long.337.pdf.json
we O
use O
l2-normalized O
euclidean O
distance O
for O
metricd O
and O
γ O
is O
a O
margin O
constant O
for O
margin-based O
triplet O
loss O
. O

section 10
id pdf2json/2021.acl-long.337.pdf.json
we O
take O
the O
average O
of O
all O
the O
losses O
between O
every O
label O
pairs O
as O
the O
margin O
loss O
. O

section 10
id pdf2json/2021.acl-long.337.pdf.json
hierarchy-aware O
margin O
due O
to O
the O
large O
label O
sets O
in O
the O
htc O
task O
, O
it O
is O
time-consuming O
to O
calculate O
every O
label O
’ O
s O
matching O
loss O
. O

section 10
id pdf2json/2021.acl-long.337.pdf.json
therefore O
, O
we O
propose O
hierarchy-aware O
sampling O
to O
alleviate O
the O
problem O
. O

section 10
id pdf2json/2021.acl-long.337.pdf.json
specifically O
, O
we O
sample O
all O
parent O
labels O
( O
coarse-grained O
labels O
) O
, O
one O
sibling O
label O
, O
and O
one O
random O
incorrect O
label O
for O
every O
fine-grained O
label O
to O
obtain O
its O
negative O
label O
sets O
n O
∈ O
n O
( O
y O
) O
. O

section 10
id pdf2json/2021.acl-long.337.pdf.json
it O
is O
also O
unreasonable O
to O
assign O
the O
same O
margin O
for O
different O
label O
pairs O
since O
the O
label O
semantics O
similarity O
is O
quite O
different O
in O
a O
large O
structured O
label O
hierarchy O
. O

section 10
id pdf2json/2021.acl-long.337.pdf.json
our O
basic O
idea O
is O
that O
the O
semantics O
relationship O
should O
be O
closer O
if O
two O
labels O
are O
closer O
in O
the O
hierarchical O
structure O
. O

section 10
id pdf2json/2021.acl-long.337.pdf.json
firstly O
, O
the O
text O
semantics O
should O
match O
fine-grained O
labels O
the O
most O
, O
which O
is O
exploited O
in O
joint O
embedding O
learning O
. O

section 10
id pdf2json/2021.acl-long.337.pdf.json
then O
we O
regard O
the O
pair O
with O
the O
smallest O
semantic O
distance O
( O
d1 O
) O
as O
a O
positive O
pair O
and O
regard O
other O
textlabel O
matching O
pairs O
as O
negative O
pairs O
. O

section 10
id pdf2json/2021.acl-long.337.pdf.json
as O
depicted O
in O
the O
schema O
figure O
3 O
, O
compared O
with O
the O
positive O
pair O
, O
the O
semantics O
matching O
distance O
between O
text O
and O
coarse-grained O
target O
labels O
( O
d2 O
) O
should O
be O
larger O
. O

section 10
id pdf2json/2021.acl-long.337.pdf.json
the O
incorrect O
sibling O
labels O
have O
a O
certain O
semantic O
relationship O
with O
the O
target O
labels O
. O

section 10
id pdf2json/2021.acl-long.337.pdf.json
hence O
, O
the O
semantics O
matching O
distance O
between O
text O
and O
the O
incorrect O
sibling O
labels O
of O
fine-grained O
labels O
( O
d3 O
) O
should O
be O
further O
larger O
, O
while O
the O
semantics O
matching O
distance O
between O
text O
and O
other O
incorrect O
labels O
( O
d4 O
) O
should O
be O
the O
largest O
. O

section 10
id pdf2json/2021.acl-long.337.pdf.json
we O
introduce O
hierarchy-aware O
penalty O
margins O
γ1 O
, O
γ2 O
, O
γ3 O
, O
γ4 O
to O
model O
the O
comparable O
relationship O
. O

section 10
id pdf2json/2021.acl-long.337.pdf.json
the O
penalty O
margin O
is O
smaller O
if O
we O
expect O
the O
semantic O
matching O
distance O
to O
be O
smaller O
. O

section 10
id pdf2json/2021.acl-long.337.pdf.json
we O
neglect O
γ1 O
because O
the O
matching O
relationships O
between O
text O
semantics O
and O
fine-grained O
labels O
are O
exploited O
in O
joint O
embedding O
learning O
. O

section 10
id pdf2json/2021.acl-long.337.pdf.json
γ2 O
, O
γ3 O
, O
γ4 O
are O
penalty O
margins O
compared O
with O
the O
matching O
relationships O
between O
text O
semantics O
and O
fine-grained O
labels O
semantics O
. O

section 10
id pdf2json/2021.acl-long.337.pdf.json
we O
introduce O
two O
hyperparameters O
α O
, O
β O
to O
measure O
different O
matching O
relationships O
of O
γ O
: O
γ2 O
= O
αγ O
; O
γ3 O
= O
βγ O
; O
γ4 O
= O
γ O
( O
7 O
) O
where O
0 O
< O
α O
< O
β O
< O
1 O
. O

section 10
id pdf2json/2021.acl-long.337.pdf.json
the O
proposed O
loss O
captures O
the O
relative O
semantics O
similarity O
rankings O
among O
target O
labels O
and O
incorrect O
labels O
in O
a O
hierarchyaware O
manner O
. O

section 11
id pdf2json/2021.acl-long.337.pdf.json
we O
find O
that O
it O
is O
easier O
to O
overfit O
for O
classification O
learning O
if O
we O
perform O
classification O
learning O
in O
the O
text-label O
joint O
embedding O
directly O
. O

section 11
id pdf2json/2021.acl-long.337.pdf.json
hence O
, O
we O
use O
the O
text O
semantics O
representation O
st O
guided O
by O
joint O
embedding O
loss O
and O
matching O
learning O
loss O
to O
perform O
classification O
learning O
. O

section 11
id pdf2json/2021.acl-long.337.pdf.json
st O
is O
fed O
into O
a O
fully O
connected O
layer O
to O
get O
the O
label O
probability O
ŷ O
for O
prediction O
. O

section 11
id pdf2json/2021.acl-long.337.pdf.json
the O
overall O
objective O
function O
includes O
a O
crossentropy O
category O
loss O
, O
joint O
embedding O
loss O
and O
hierarchy-aware O
matching O
loss O
: O
l O
= O
lcls O
( O
y O
, O
ŷ O
) O
+ O
λ1ljoint O
+ O
λ2lmatch O
( O
8 O
) O
where O
y O
and O
ŷ O
are O
the O
ground-truth O
label O
and O
output O
probability O
, O
respectively O
. O

section 11
id pdf2json/2021.acl-long.337.pdf.json
λ1 O
, O
λ2 O
are O
the O
hyperparameters O
for O
balancing O
the O
joint O
embedding O
loss O
and O
matching O
learning O
loss O
. O

section 11
id pdf2json/2021.acl-long.337.pdf.json
we O
minimize O
the O
above O
function O
by O
gradient O
descent O
during O
training O
. O

section 13
id pdf2json/2021.acl-long.337.pdf.json
datasets O
to O
evaluate O
the O
effectiveness O
of O
our O
model O
, O
we O
conduct O
experiments O
on O
three O
widelystudied O
datasets O
for O
hierarchical O
multi-label O
text O
classification O
. O

section 13
id pdf2json/2021.acl-long.337.pdf.json
statistics O
of O
these O
datasets O
are O
listed O
in O
table O
1 O
. O

section 13
id pdf2json/2021.acl-long.337.pdf.json
rcv1-v2 O
( O
lewis O
et O
al. O
, O
2004 O
) O
is O
a O
news O
categorization O
corpora O
, O
and O
wos O
( O
kowsari O
et O
al. O
, O
2017 O
) O
includes O
abstracts O
of O
published O
papers O
from O
web O
of O
science O
. O

section 13
id pdf2json/2021.acl-long.337.pdf.json
eurlex57k O
is O
a O
large O
hierarchical O
multi-label O
text O
classification O
( O
lmtc O
) O
dataset O
that O
contains O
57k O
english O
eu O
legislative O
documents O
, O
and O
is O
tagged O
with O
about O
4.3k O
labels O
from O
the O
european O
vocabulary O
( O
chalkidis O
et O
al. O
, O
2019 O
) O
. O

section 13
id pdf2json/2021.acl-long.337.pdf.json
the O
label O
sets O
are O
split O
into O
zero-shot O
labels O
, O
few-shot O
labels O
, O
and O
frequent O
labels O
. O

section 13
id pdf2json/2021.acl-long.337.pdf.json
few-shot O
labels O
are O
labels O
whose O
frequencies O
in O
the O
training O
set O
are O
less O
than O
or O
equal O
to O
50 O
. O

section 13
id pdf2json/2021.acl-long.337.pdf.json
frequent O
labels O
are O
labels O
whose O
frequencies O
in O
the O
training O
set O
are O
more O
than O
50 O
. O

section 13
id pdf2json/2021.acl-long.337.pdf.json
the O
label O
setting O
is O
the O
same O
as O
previous O
work O
( O
lu O
et O
al. O
, O
2020 O
) O
. O

section 13
id pdf2json/2021.acl-long.337.pdf.json
in O
eurlex57k O
, O
the O
corpora O
are O
only O
tagged O
with O
fine-grained O
labels O
, O
and O
the O
parent O
labels O
of O
fine-grained O
labels O
are O
not O
tagged O
as O
the O
target O
labels O
. O

section 13
id pdf2json/2021.acl-long.337.pdf.json
evaluation O
metric O
on O
rcv1-v2 O
and O
wos O
datasets O
, O
we O
measure O
the O
experimental O
results O
by O
micro-f1 O
and O
macro-f1 O
. O

section 13
id pdf2json/2021.acl-long.337.pdf.json
micro-f1 O
takes O
the O
overall O
precision O
and O
recall O
of O
all O
the O
instances O
into O
account O
, O
while O
macro-f1 O
equals O
the O
average O
f1score O
of O
labels O
. O

section 13
id pdf2json/2021.acl-long.337.pdf.json
we O
report O
the O
results O
of O
two O
ranking O
metrics O
on O
large O
hierarchical O
multi-label O
text O
classification O
dataset O
eurlex-57k O
, O
including O
recall O
@ O
5 O
and O
ndcg O
@ O
5 O
. O

section 13
id pdf2json/2021.acl-long.337.pdf.json
the O
ranking O
metrics O
are O
preferable O
for O
eurlex-57k O
since O
it O
does O
not O
introduce O
a O
significant O
bias O
towards O
frequent O
labels O
( O
lu O
et O
al. O
, O
2020 O
) O
. O

section 13
id pdf2json/2021.acl-long.337.pdf.json
implementation O
details O
we O
initialize O
the O
word O
embeddings O
with O
300d O
pre-trained O
glove O
vectors O
( O
pennington O
et O
al. O
, O
2014 O
) O
. O

section 13
id pdf2json/2021.acl-long.337.pdf.json
then O
we O
use O
a O
one-layer O
bigru O
with O
hidden O
dimension O
100 O
and O
used O
100 O
filters O
with O
kernel O
size O
[ O
2,3,4 O
] O
to O
setup O
the O
cnns O
. O

section 13
id pdf2json/2021.acl-long.337.pdf.json
the O
dimension O
of O
the O
text O
propagation O
feature O
and O
graph O
convolution O
weight O
matrix O
are O
both O
300 O
. O

section 13
id pdf2json/2021.acl-long.337.pdf.json
the O
hidden O
size O
of O
joint O
embedding O
is O
200 O
. O

section 13
id pdf2json/2021.acl-long.337.pdf.json
the O
matching O
margin O
γ O
is O
set O
to O
0.2 O
on O
rcv1-v2 O
and O
wos O
datasets O
, O
and O
set O
to O
0.5 O
on O
eurlex-57k O
dataset O
. O

section 13
id pdf2json/2021.acl-long.337.pdf.json
we O
set O
the O
value O
of O
hierarchy-aware O
penalty O
hyperparameters O
α O
, O
β O
to O
0.01 O
and O
0.5 O
, O
respectively O
. O

section 13
id pdf2json/2021.acl-long.337.pdf.json
the O
loss O
balancing O
factor O
λ1 O
, O
λ2 O
are O
set O
to O
1 O
. O

section 13
id pdf2json/2021.acl-long.337.pdf.json
for O
fair O
comparisons O
with O
previous O
work O
( O
lu O
et O
al. O
, O
2020 O
; O
chalkidis O
et O
al. O
, O
2019 O
) O
on O
eurlex-57k O
dataset O
, O
firstly O
, O
we O
do O
not O
use O
cnn O
layer O
and O
text O
feature O
propagation O
module O
. O

section 13
id pdf2json/2021.acl-long.337.pdf.json
secondly O
, O
to O
adapt O
to O
the O
zeroshot O
settings O
, O
the O
prediction O
is O
generated O
by O
the O
dot O
product O
similarity O
between O
text O
semantics O
and O
label O
semantics O
. O

section 13
id pdf2json/2021.acl-long.337.pdf.json
our O
model O
is O
optimized O
by O
adam O
with O
a O
learning O
rate O
of O
1e-4 O
. O

section 13
id pdf2json/2021.acl-long.337.pdf.json
for O
pretrained O
language O
model O
bert O
( O
devlin O
et O
al. O
, O
2018 O
) O
, O
we O
use O
the O
top-level O
representation O
hcls O
of O
bert O
’ O
s O
special O
cls O
token O
to O
perform O
classification O
. O

section 13
id pdf2json/2021.acl-long.337.pdf.json
to O
combine O
our O
model O
with O
bert O
, O
we O
replace O
the O
text O
encoder O
of O
himatch O
with O
bert O
, O
and O
the O
label O
representations O
are O
initiated O
by O
pretrained O
bert O
embedding O
. O

section 13
id pdf2json/2021.acl-long.337.pdf.json
the O
batch O
size O
is O
set O
to O
16 O
, O
and O
the O
learning O
rate O
is O
2e-5 O
. O

section 13
id pdf2json/2021.acl-long.337.pdf.json
comparison O
models O
on O
rcv1-v2 O
and O
wos O
datasets O
, O
we O
compare O
our O
model O
with O
three O
types O
of O
strong O
baselines O
: O
1 O
) O
text O
classification O
baselines O
: O
textrcnn O
( O
lai O
et O
al. O
, O
2015 O
) O
, O
textrcnn O
with O
label O
attention O
( O
textrcnn-la O
) O
( O
zhou O
et O
al. O
, O
2020 O
) O
, O
and O
sgm O
( O
yang O
et O
al. O
, O
2018 O
) O
. O

section 13
id pdf2json/2021.acl-long.337.pdf.json
2 O
) O
hierarchy-aware O
models O
: O
he-agcrcnn O
( O
peng O
et O
al. O
, O
2019 O
) O
, O
hmcn O
( O
mao O
et O
al. O
, O
2019 O
) O
, O
htrans O
( O
banerjee O
et O
al. O
, O
2019 O
) O
, O
hilap-rl O
( O
mao O
et O
al. O
, O
2019 O
) O
which O
introduced O
reinforcement O
learning O
to O
simulate O
the O
assignment O
process O
, O
hiagm O
( O
zhou O
et O
al. O
, O
2020 O
) O
which O
exploited O
the O
prior O
probability O
of O
label O
dependecies O
through O
graph O
convolution O
network O
and O
treelstm O
. O

section 13
id pdf2json/2021.acl-long.337.pdf.json
3 O
) O
pretrained O
language O
model O
: O
a O
more O
powerful O
pretrained O
language O
model O
bert O
( O
devlin O
et O
al. O
, O
2018 O
) O
than O
tradition O
text O
classification O
models O
when O
fine-tuned O
on O
downstream O
tasks O
. O

section 13
id pdf2json/2021.acl-long.337.pdf.json
on O
eurlex-57k O
dataset O
, O
we O
compare O
our O
model O
with O
strong O
baselines O
with/without O
zeroshot O
settings O
such O
as O
bigru-att O
, O
bigru-lwan O
( O
chalkidis O
et O
al. O
, O
2019 O
) O
which O
introduced O
labelwise O
attention O
. O

section 13
id pdf2json/2021.acl-long.337.pdf.json
the O
models O
starting O
with O
“ O
zero O
” O
make O
predictions O
by O
calculating O
similarity O
scores O
between O
text O
and O
label O
semantics O
for O
zero-shot O
set- O
tings O
. O

section 13
id pdf2json/2021.acl-long.337.pdf.json
agru-kamg O
( O
lu O
et O
al. O
, O
2020 O
) O
is O
a O
stateof-the-art O
model O
which O
introduced O
various O
label O
knowledge O
. O

section 14
id pdf2json/2021.acl-long.337.pdf.json
table O
2 O
, O
3 O
and O
4 O
report O
the O
performance O
of O
our O
approaches O
against O
other O
methods O
. O

section 14
id pdf2json/2021.acl-long.337.pdf.json
hiagm O
is O
an O
effective O
baseline O
on O
rcv1-v2 O
and O
wos O
due O
to O
the O
introduction O
of O
holistic O
label O
information O
. O

section 14
id pdf2json/2021.acl-long.337.pdf.json
however O
, O
they O
ignored O
the O
semantic O
relationship O
between O
text O
and O
labels O
. O

section 14
id pdf2json/2021.acl-long.337.pdf.json
our O
model O
achieves O
the O
best O
results O
by O
capturing O
the O
matching O
relationships O
among O
text O
and O
labels O
in O
a O
hierarchy-aware O
manner O
, O
which O
achieves O
stronger O
performances O
especially O
on O
macro-f1 O
. O

section 14
id pdf2json/2021.acl-long.337.pdf.json
the O
improvements O
show O
that O
our O
model O
can O
make O
better O
use O
of O
structural O
information O
to O
help O
imbalanced O
htc O
classification O
. O

section 14
id pdf2json/2021.acl-long.337.pdf.json
the O
pretrained O
language O
model O
bert O
is O
an O
effective O
method O
when O
fine-tuned O
on O
downstream O
tasks O
. O

section 14
id pdf2json/2021.acl-long.337.pdf.json
compared O
with O
the O
results O
regarding O
htc O
as O
multiple O
binary O
classifiers O
, O
our O
results O
show O
that O
the O
full O
use O
of O
structured O
label O
hierarchy O
can O
bring O
great O
improvements O
to O
bert O
model O
on O
rcv1-v2 O
and O
wos O
datasets O
. O

section 14
id pdf2json/2021.acl-long.337.pdf.json
on O
eurlex57k O
dataset O
, O
our O
model O
achieves O
the O
best O
results O
on O
different O
matrics O
except O
for O
zeroshot O
labels O
. O

section 14
id pdf2json/2021.acl-long.337.pdf.json
the O
largest O
improvements O
come O
from O
few-shot O
labels O
. O

section 14
id pdf2json/2021.acl-long.337.pdf.json
agru-kamg O
achieves O
the O
best O
results O
on O
zero-shot O
labels O
by O
fusing O
various O
knowledge O
such O
as O
label O
semantics O
similarities O
and O
label O
co-occurrence O
. O

section 14
id pdf2json/2021.acl-long.337.pdf.json
however O
, O
our O
model O
performs O
semantics O
matching O
among O
seen O
labels O
based O
on O
training O
corpora O
, O
which O
is O
not O
designed O
for O
a O
specific O
zero-shot O
learning O
task O
. O

section 16
id pdf2json/2021.acl-long.337.pdf.json
in O
this O
section O
, O
we O
investigate O
to O
study O
the O
independent O
effect O
of O
each O
component O
in O
our O
proposed O
model O
. O

section 16
id pdf2json/2021.acl-long.337.pdf.json
firstly O
, O
we O
validate O
the O
influence O
of O
two O
proposed O
losses O
, O
and O
the O
hierarchy-aware O
sampling O
. O

section 16
id pdf2json/2021.acl-long.337.pdf.json
the O
results O
are O
reported O
in O
table O
5 O
. O

section 16
id pdf2json/2021.acl-long.337.pdf.json
the O
results O
show O
that O
f1 O
will O
decrease O
with O
removing O
joint O
embedding O
loss O
or O
matching O
learning O
loss O
. O

section 16
id pdf2json/2021.acl-long.337.pdf.json
joint O
embedding O
loss O
has O
a O
great O
influence O
since O
label O
semantics O
matching O
relies O
on O
the O
joint O
embedding O
. O

section 16
id pdf2json/2021.acl-long.337.pdf.json
besides O
, O
in O
the O
hierarchy-aware O
margin O
subsection O
, O
we O
perform O
hierarchy-aware O
sampling O
by O
sampling O
coarse-grained O
labels O
, O
incorrect O
sibling O
labels O
, O
and O
other O
incorrect O
labels O
as O
negative O
label O
sets O
. O

section 16
id pdf2json/2021.acl-long.337.pdf.json
when O
we O
remove O
hierarchy-aware O
sampling O
and O
replace O
it O
with O
random O
sampling O
, O
the O
results O
will O
decrease O
, O
which O
shows O
the O
effectiveness O
of O
hierarchy-aware O
sampling O
. O

section 17
id pdf2json/2021.acl-long.337.pdf.json
to O
study O
the O
influence O
of O
the O
hyperparameters O
γ O
, O
α O
, O
and O
β O
, O
we O
conduct O
seven O
experiments O
on O
rcv1v2 O
dataset O
. O

section 17
id pdf2json/2021.acl-long.337.pdf.json
the O
results O
are O
reported O
in O
table O
6 O
. O

section 17
id pdf2json/2021.acl-long.337.pdf.json
the O
first O
experiment O
is O
the O
best O
hyperparameters O
of O
our O
model O
. O

section 17
id pdf2json/2021.acl-long.337.pdf.json
then O
we O
fine-tune O
the O
matching O
learning O
margin O
γ O
in O
experiments O
two O
and O
three O
. O

section 17
id pdf2json/2021.acl-long.337.pdf.json
we O
find O
that O
a O
proper O
margin O
γ O
= O
0.2 O
is O
beneficial O
for O
matching O
learning O
compared O
with O
a O
large O
or O
small O
margin O
. O

section 17
id pdf2json/2021.acl-long.337.pdf.json
furthermore O
, O
we O
validate O
the O
effectiveness O
of O
the O
hierarchy-aware O
margin O
. O

section 17
id pdf2json/2021.acl-long.337.pdf.json
in O
experiment O
four O
, O
the O
performance O
will O
decrease O
if O
we O
violate O
the O
hierarchical O
structure O
by O
setting O
a O
large O
penalty O
margin O
for O
coarse-grained O
labels O
, O
and O
setting O
a O
small O
penalty O
margin O
for O
incorrect O
sibling O
labels O
. O

section 17
id pdf2json/2021.acl-long.337.pdf.json
in O
experiment O
five O
, O
the O
performance O
has O
a O
relatively O
larger O
decrease O
if O
we O
set O
α O
= O
1 O
and O
β O
= O
1 O
, O
which O
ignores O
hierarchical O
structure O
completely O
. O

section 17
id pdf2json/2021.acl-long.337.pdf.json
we O
speculate O
that O
the O
penalty O
margin O
that O
violates O
the O
hierarchical O
structure O
will O
affect O
the O
results O
, O
since O
the O
semantics O
relationship O
should O
be O
closer O
if O
the O
labels O
are O
closer O
in O
the O
hierarchical O
structure O
. O

section 17
id pdf2json/2021.acl-long.337.pdf.json
moreover O
, O
we O
validate O
the O
effectiveness O
of O
different O
penalty O
margins O
among O
different O
granular O
labels O
. O

section 17
id pdf2json/2021.acl-long.337.pdf.json
in O
experiments O
six O
and O
seven O
, O
the O
results O
will O
degrade O
if O
we O
ignore O
the O
relationships O
between O
coarse-grained O
target O
labels O
and O
incorrect O
sibling O
labels O
, O
by O
setting O
the O
same O
margin O
for O
α O
and O
β O
. O

section 17
id pdf2json/2021.acl-long.337.pdf.json
therefore O
, O
it O
is O
necessary O
to O
set O
a O
small O
penalty O
margin O
for O
coarse-grained O
target O
labels O
, O
and O
a O
larger O
penalty O
margin O
for O
incorrect O
sibling O
labels O
. O

section 18
id pdf2json/2021.acl-long.337.pdf.json
we O
plot O
the O
t-sne O
projection O
of O
the O
text O
representations O
and O
label O
representations O
in O
the O
joint O
embedding O
in O
figure O
4 O
. O

section 18
id pdf2json/2021.acl-long.337.pdf.json
figure O
a O
) O
is O
a O
part O
of O
the O
hierarchical O
label O
structure O
in O
rcv1-v2 O
. O

section 18
id pdf2json/2021.acl-long.337.pdf.json
label O
c171 O
and O
c172 O
are O
fine-grained O
labels O
, O
and O
label O
c17 O
is O
coarse-grained O
label O
of O
c171 O
and O
c172 O
. O

section 18
id pdf2json/2021.acl-long.337.pdf.json
gwelf O
and O
e61 O
are O
other O
labels O
with O
different O
semantics O
with O
c17 O
, O
c171 O
and O
c172 O
. O

section 18
id pdf2json/2021.acl-long.337.pdf.json
in O
figure O
b O
) O
, O
by O
introducing O
joint O
embedding O
loss O
, O
we O
can O
see O
that O
the O
text O
representations O
are O
close O
to O
their O
corresponding O
label O
representations O
. O

section 18
id pdf2json/2021.acl-long.337.pdf.json
furthermore O
, O
the O
text O
representations O
of O
labels O
c171 O
and O
c172 O
are O
close O
to O
the O
label O
representation O
of O
their O
coarse-grained O
label O
c17 O
. O

section 18
id pdf2json/2021.acl-long.337.pdf.json
however O
, O
the O
text O
representations O
of O
different O
labels O
may O
overlap O
, O
since O
the O
matching O
relationships O
among O
different O
labels O
are O
ignored O
. O

section 18
id pdf2json/2021.acl-long.337.pdf.json
in O
figure O
c O
) O
, O
by O
introducing O
both O
joint O
embedding O
loss O
and O
matching O
learning O
loss O
, O
the O
text O
representations O
of O
different O
labels O
are O
more O
separable O
. O

section 18
id pdf2json/2021.acl-long.337.pdf.json
other O
unrelated O
text O
representations O
and O
label O
representations O
such O
as O
labels O
gwelf O
, O
e61 O
are O
far O
away O
from O
c17 O
, O
c171 O
, O
c172 O
. O

section 18
id pdf2json/2021.acl-long.337.pdf.json
besides O
, O
the O
text O
representations O
of O
semantically O
similar O
labels O
( O
c171 O
and O
c172 O
) O
are O
far O
away O
relatively O
compared O
with O
figure O
b O
) O
. O

section 18
id pdf2json/2021.acl-long.337.pdf.json
the O
t-sne O
visualization O
shows O
that O
our O
model O
can O
capture O
the O
semantics O
relationship O
among O
texts O
, O
coarsegrained O
labels O
, O
fine-grained O
labels O
and O
unrelated O
labels O
. O

section 19
id pdf2json/2021.acl-long.337.pdf.json
we O
analyze O
the O
performance O
with O
different O
label O
granularity O
based O
on O
their O
hierarchical O
levels O
. O

section 19
id pdf2json/2021.acl-long.337.pdf.json
we O
compute O
level-based O
micro-f1 O
and O
macro-f1 O
scores O
of O
the O
rcv1-v2 O
dataset O
on O
textrcnn O
, O
hiagm O
, O
and O
our O
model O
in O
figure O
5 O
. O

section 19
id pdf2json/2021.acl-long.337.pdf.json
on O
rcv1-v2 O
dataset O
, O
both O
the O
second O
and O
third O
hierarchical O
levels O
contain O
fine-grained O
labels O
( O
leaf O
nodes O
) O
. O

section 19
id pdf2json/2021.acl-long.337.pdf.json
the O
second O
level O
has O
the O
largest O
number O
of O
labels O
and O
contains O
confusing O
labels O
with O
similar O
concepts O
, O
so O
its O
micro-f1 O
is O
relatively O
low O
. O

section 19
id pdf2json/2021.acl-long.337.pdf.json
both O
the O
second O
and O
third O
levels O
contain O
some O
long-tailed O
labels O
, O
so O
their O
macro-f1 O
are O
relatively O
low O
. O

section 19
id pdf2json/2021.acl-long.337.pdf.json
figure O
5 O
shows O
that O
our O
model O
achieves O
a O
better O
performance O
than O
other O
models O
on O
all O
levels O
, O
especially O
among O
deep O
levels O
. O

section 19
id pdf2json/2021.acl-long.337.pdf.json
the O
results O
demonstrate O
that O
our O
model O
has O
a O
better O
ability O
to O
capture O
the O
hierarchical O
label O
semantic O
, O
especially O
on O
fine-grained O
labels O
with O
a O
complex O
hierarchical O
structure O
. O

section 20
id pdf2json/2021.acl-long.337.pdf.json
in O
this O
part O
, O
we O
compare O
the O
computational O
complexity O
between O
hiagm O
and O
our O
model O
. O

section 20
id pdf2json/2021.acl-long.337.pdf.json
for O
time O
complexity O
, O
the O
training O
time O
of O
himatch O
is O
1.11 O
times O
that O
of O
hiagm O
with O
batch O
size O
64 O
. O

section 20
id pdf2json/2021.acl-long.337.pdf.json
for O
space O
complexity O
during O
training O
, O
himatch O
has O
37.4m O
parameters O
, O
while O
hiagm O
has O
27.8m O
. O

section 20
id pdf2json/2021.acl-long.337.pdf.json
the O
increase O
mainly O
comes O
from O
the O
label O
encoder O
with O
large O
label O
sets O
. O

section 20
id pdf2json/2021.acl-long.337.pdf.json
however O
, O
during O
testing O
, O
the O
time O
and O
space O
complexity O
of O
himatch O
is O
the O
same O
as O
hiagm O
. O

section 20
id pdf2json/2021.acl-long.337.pdf.json
the O
reason O
is O
that O
only O
the O
classification O
results O
are O
needed O
, O
and O
we O
can O
remove O
the O
joint O
embedding O
. O

section 20
id pdf2json/2021.acl-long.337.pdf.json
himatch O
achieves O
new O
state-of-the-art O
results O
, O
and O
we O
believe O
that O
the O
increase O
of O
computational O
complexity O
is O
acceptable O
. O

section 21
id pdf2json/2021.acl-long.337.pdf.json
here O
we O
present O
a O
novel O
hierarchical O
text O
classification O
model O
called O
himatch O
that O
can O
capture O
semantic O
relationships O
among O
texts O
and O
labels O
at O
different O
abstraction O
levels O
. O

section 21
id pdf2json/2021.acl-long.337.pdf.json
instead O
of O
treating O
htc O
as O
multiple O
binary O
classification O
tasks O
, O
we O
consider O
the O
text-label O
semantics O
matching O
relationship O
and O
formulate O
it O
as O
a O
semantic O
matching O
problem O
. O

section 21
id pdf2json/2021.acl-long.337.pdf.json
we O
learn O
a O
joint O
semantic O
embedding O
between O
text O
and O
labels O
. O

section 21
id pdf2json/2021.acl-long.337.pdf.json
finally O
, O
we O
propose O
a O
hierarchy-aware O
matching O
strategy O
to O
model O
different O
matching O
relationships O
among O
coarse-grained O
labels O
, O
fine-grained O
labels O
and O
incorrect O
labels O
. O

section 21
id pdf2json/2021.acl-long.337.pdf.json
in O
future O
work O
, O
we O
plan O
to O
extend O
our O
model O
to O
the O
zero-shot O
learning O
scenario O
. O

section 22
id pdf2json/2021.acl-long.337.pdf.json
we O
thank O
the O
anonymous O
reviewers O
for O
their O
helpful O
feedbacks O
. O

section 22
id pdf2json/2021.acl-long.337.pdf.json
the O
work O
described O
in O
this O
paper O
was O
partially O
funded O
by O
the O
national O
natural O
science O
foundation O
of O
china O
( O
grant O
no O
. O

section 22
id pdf2json/2021.acl-long.337.pdf.json
61502174 O
, O
and O
61872148 O
) O
, O
the O
natural O
science O
foundation O
of O
guangdong O
province O
( O
grant O
no O
. O

section 22
id pdf2json/2021.acl-long.337.pdf.json
2017a030313355 O
, O
2019a1515010768 O
and O
2021a1515011496 O
) O
, O
the O
guangzhou O
science O
and O
technology O
planning O
project O
( O
grant O
no O
. O

section 22
id pdf2json/2021.acl-long.337.pdf.json
201704030051 O
, O
and O
201902010020 O
) O
, O
the O
key O
r O
& O
d O
program O
of O
guangdong O
province O
( O
no O
. O

section 22
id pdf2json/2021.acl-long.337.pdf.json
2018b010107002 O
) O
and O
the O
fundamental O
research O
funds O
for O
the O
central O
universities O
. O

