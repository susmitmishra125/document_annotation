section TITLE
id pdf2json/2021.acl-long.4.pdf.json
HATECHECK O
: O
Functional O
Tests O
for O
Hate O
Speech O
Detection O
Models O

section ABSTRACT
id pdf2json/2021.acl-long.4.pdf.json
Detecting O
online O
hate O
is O
a B-PARAMETER
difficult O
task O
that O
even O
state-of-the-art O
models O
struggle O
with O
. O
section ABSTRACT
id pdf2json/2021.acl-long.4.pdf.json
Typically O
, O
hate O
speech O
detection O
models O
are O
evaluated O
by O
measuring O
their O
performance O
on O
held-out O
test O
data O
using O
metrics O
such O
as O
accuracy B-PARAMETER
and O
F1 B-METRIC
score I-METRIC
. O
section ABSTRACT
id pdf2json/2021.acl-long.4.pdf.json
However O
, O
this O
approach O
makes O
it O
difficult O
to O
identify O
specific O
model O
weak O
points O
. O
section ABSTRACT
id pdf2json/2021.acl-long.4.pdf.json
It O
also O
risks O
overestimating O
generalisable O
model O
performance O
due O
to O
increasingly O
well-evidenced O
systematic O
gaps O
and O
biases O
in O
hate O
speech O
datasets O
. O
section ABSTRACT
id pdf2json/2021.acl-long.4.pdf.json
To O
enable O
more O
targeted O
diagnostic O
insights O
, O
we O
introduce O
HATECHECK O
, O
a B-PARAMETER
suite O
of O
functional O
tests O
for O
hate O
speech O
detection O
models O
. O
section ABSTRACT
id pdf2json/2021.acl-long.4.pdf.json
We O
specify O
29 O
model O
functionalities O
motivated O
by O
a B-PARAMETER
review O
of O
previous O
research O
and O
a B-PARAMETER
series O
of O
interviews O
with O
civil O
society O
stakeholders O
. O
section ABSTRACT
id pdf2json/2021.acl-long.4.pdf.json
We O
craft O
test O
cases O
for O
each O
functionality O
and O
validate O
their O
quality O
through O
a B-PARAMETER
structured O
annotation O
process O
. O
section ABSTRACT
id pdf2json/2021.acl-long.4.pdf.json
To O
illustrate O
HATECHECK O
’ O
s O
utility O
, O
we O
test O
near-state-of-the-art O
transformer B-METHOD
models O
as O
well O
as O
two O
popular O
commercial O
models O
, O
revealing O
critical O
model O
weaknesses O
. O

section 0
id pdf2json/2021.acl-long.4.pdf.json
Proceedings O
of O
the O
59th O
Annual O
Meeting O
of O
the O
Association O
for O
Computational O
Linguistics O
and O
the O
11th O
International O
Joint O
Conference O
on O
Natural O
Language O
Processing O
, O
pages O
41–58 O
August O
1–6 O
, O
2021 O
. O
section 0
id pdf2json/2021.acl-long.4.pdf.json
©2021 O
Association O
for O
Computational O
Linguistics O
41 O

section 1
id pdf2json/2021.acl-long.4.pdf.json
Hate O
speech O
detection O
models O
play O
an O
important O
role O
in O
online O
content O
moderation O
and O
enable O
scientific O
analyses O
of O
online O
hate O
more O
generally O
. O
section 1
id pdf2json/2021.acl-long.4.pdf.json
This O
has O
motivated O
much O
research O
in O
NLP O
and O
the O
social O
sciences O
. O
section 1
id pdf2json/2021.acl-long.4.pdf.json
However O
, O
even O
state-of-the-art O
models O
exhibit O
substantial O
weaknesses O
( O
see O
Schmidt B-AUTHOR
and O
Wiegand B-AUTHOR
, O
2017 O
; O
Fortuna B-AUTHOR
and O
Nunes B-AUTHOR
, O
2018 O
; O
Vidgen B-AUTHOR
et O
al. O
, O
2019 O
; O
Mishra B-AUTHOR
et O
al. O
, O
2020 O
, O
for O
reviews O
) O
. O
section 1
id pdf2json/2021.acl-long.4.pdf.json
So O
far O
, O
hate O
speech O
detection O
models O
have O
primarily O
been O
evaluated O
by O
measuring O
held-out O
performance O
on O
a B-PARAMETER
small O
set O
of O
widely-used O
hate O
speech O
datasets O
( O
particularly O
Waseem B-AUTHOR
and O
Hovy B-AUTHOR
, O
2016 O
; O
Davidson B-AUTHOR
et O
al. O
, O
2017 O
; O
Founta B-AUTHOR
et O
al. O
, O
2018 O
) O
, O
but O
recent O
work O
has O
highlighted O
the O
limitations O
of O
this O
evaluation O
paradigm O
. O
section 1
id pdf2json/2021.acl-long.4.pdf.json
Aggregate O
performance O
metrics O
offer O
limited O
insight O
into O
specific O
model O
weak- O
nesses O
( O
Wu B-AUTHOR
et O
al. O
, O
2019 O
) O
. O
section 1
id pdf2json/2021.acl-long.4.pdf.json
Further O
, O
if O
there O
are O
systematic O
gaps O
and O
biases O
in O
training O
data O
, O
models O
may O
perform O
deceptively O
well O
on O
corresponding O
held-out O
test O
sets O
by O
learning O
simple O
decision O
rules O
rather O
than O
encoding O
a B-PARAMETER
more O
generalisable O
understanding O
of O
the O
task O
( O
e.g O
. O
section 1
id pdf2json/2021.acl-long.4.pdf.json
Niven B-AUTHOR
and O
Kao B-AUTHOR
, O
2019 O
; O
Geva B-AUTHOR
et O
al. O
, O
2019 O
; O
Shah B-AUTHOR
et O
al. O
, O
2020 O
) O
. O
section 1
id pdf2json/2021.acl-long.4.pdf.json
The O
latter O
issue O
is O
particularly O
relevant O
to O
hate O
speech O
detection O
since O
current O
hate O
speech O
datasets O
vary O
in O
data O
source O
, O
sampling O
strategy O
and O
annotation O
process O
( O
Vidgen B-AUTHOR
and O
Derczynski B-AUTHOR
, O
2020 O
; O
Poletto B-AUTHOR
et O
al. O
, O
2020 O
) O
, O
and O
are O
known O
to O
exhibit O
annotator O
biases O
( O
Waseem B-AUTHOR
, O
2016 O
; O
Waseem B-AUTHOR
et O
al. O
, O
2018 O
; O
Sap B-AUTHOR
et O
al. O
, O
2019 O
) O
as O
well O
as O
topic B-AUTHOR
and O
author O
biases O
( O
Wiegand B-AUTHOR
et O
al. O
, O
2019 O
; O
Nejadgholi B-AUTHOR
and O
Kiritchenko B-AUTHOR
, O
2020 O
) O
. O
section 1
id pdf2json/2021.acl-long.4.pdf.json
Correspondingly O
, O
models O
trained O
on O
such O
datasets O
have O
been O
shown O
to O
be O
overly O
sensitive O
to O
lexical O
features O
such O
as O
group O
identifiers O
( O
Park B-AUTHOR
et O
al. O
, O
2018 O
; O
Dixon B-AUTHOR
et O
al. O
, O
2018 O
; O
Kennedy B-AUTHOR
et O
al. O
, O
2020 O
) O
, O
and O
to O
generalise O
poorly O
to O
other O
datasets O
( O
Nejadgholi B-AUTHOR
and O
Kiritchenko B-AUTHOR
, O
2020 O
; O
Samory B-AUTHOR
et O
al. O
, O
2020 O
) O
. O
section 1
id pdf2json/2021.acl-long.4.pdf.json
Therefore O
, O
held-out O
performance O
on O
current O
hate O
speech O
datasets O
is O
an O
incomplete O
and O
potentially O
misleading O
measure O
of O
model O
quality O
. O
section 1
id pdf2json/2021.acl-long.4.pdf.json
To O
enable O
more O
targeted O
diagnostic O
insights O
, O
we O
introduce O
HATECHECK O
, O
a B-PARAMETER
suite O
of O
functional O
tests O
for O
hate O
speech O
detection O
models O
. O
section 1
id pdf2json/2021.acl-long.4.pdf.json
Functional O
testing O
, O
also O
known O
as O
black-box O
testing O
, O
is O
a B-PARAMETER
testing O
framework O
from O
software O
engineering O
that O
assesses O
different O
functionalities O
of O
a B-PARAMETER
given O
model O
by O
validating O
its O
output O
on O
sets O
of O
targeted O
test O
cases O
( O
Beizer B-AUTHOR
, O
1995 O
) O
. O
section 1
id pdf2json/2021.acl-long.4.pdf.json
Ribeiro B-AUTHOR
et O
al O
. O
section 1
id pdf2json/2021.acl-long.4.pdf.json
( O
2020 O
) O
show O
how O
such O
a B-PARAMETER
framework O
can O
be O
used O
for O
structured O
model O
evaluation O
across O
diverse O
NLP O
tasks O
. O
section 1
id pdf2json/2021.acl-long.4.pdf.json
HATECHECK O
covers O
29 O
model O
functionalities O
, O
the O
selection O
of O
which O
we O
motivate O
through O
a B-PARAMETER
series O
of O
interviews O
with O
civil O
society O
stakeholders O
and O
a B-PARAMETER
review O
of O
hate O
speech O
research O
. O
section 1
id pdf2json/2021.acl-long.4.pdf.json
Each O
functionality O
is O
tested O
by O
a B-PARAMETER
separate O
functional O
test O
. O
section 1
id pdf2json/2021.acl-long.4.pdf.json
We O
create O
18 O
functional O
tests O
corresponding O
to O
distinct O
expressions O
of O
hate O
. O
section 1
id pdf2json/2021.acl-long.4.pdf.json
The O
other O
11 B-PARAMETER
functional O
tests O
are O
non-hateful O
contrasts O
to O
the O
hateful O
cases O
. O
section 1
id pdf2json/2021.acl-long.4.pdf.json
For O
example O
, O
we O
test O
non-hateful O
reclaimed O
uses O
of O
slurs O
as O
a B-PARAMETER
contrast O
to O
their O
hateful O
use O
. O
section 1
id pdf2json/2021.acl-long.4.pdf.json
Such O
tests O
are O
particularly O
challenging O
to O
models O
relying O
on O
overly O
simplistic O
decision O
rules O
and O
thus O
enable O
more O
accurate O
evaluation O
of O
true O
model O
functionalities O
( O
Gardner B-AUTHOR
et O
al. O
, O
2020 O
) O
. O
section 1
id pdf2json/2021.acl-long.4.pdf.json
For O
each O
functional O
test O
, O
we O
hand-craft O
sets O
of O
targeted O
test O
cases O
with O
clear O
gold B-METRIC
standard I-METRIC
labels I-METRIC
, O
which O
we O
validate O
through O
a B-PARAMETER
structured O
annotation O
process.1 O
HATECHECK O
is O
broadly O
applicable O
across O
English-language O
hate O
speech O
detection O
models O
. O
section 1
id pdf2json/2021.acl-long.4.pdf.json
We O
demonstrate O
its O
utility O
as O
a B-PARAMETER
diagnostic O
tool O
by O
evaluating O
two O
BERT B-SOFTWARE
models O
( O
Devlin B-AUTHOR
et O
al. O
, O
2019 O
) O
, O
which O
have O
achieved O
near O
state-of-the-art O
performance O
on O
hate O
speech O
datasets O
( O
Tran B-AUTHOR
et O
al. O
, O
2020 O
) O
, O
as O
well O
as O
two O
commercial O
models O
– O
Google B-SOFTWARE
Jigsaw O
’ O
s O
Perspective O
and O
Two O
Hat O
’ O
s O
SiftNinja.2 O
When O
tested O
with O
HATECHECK O
, O
all O
models O
appear O
overly O
sensitive O
to O
specific O
keywords O
such O
as O
slurs O
. O
section 1
id pdf2json/2021.acl-long.4.pdf.json
They O
consistently O
misclassify O
negated O
hate O
, O
counter O
speech O
and O
other O
non-hateful O
contrasts O
to O
hateful O
phrases O
. O
section 1
id pdf2json/2021.acl-long.4.pdf.json
Further O
, O
the O
BERT B-SOFTWARE
models O
are O
biased O
in O
their O
performance O
across O
target B-PARAMETER
groups O
, O
misclassifying O
more O
content O
directed O
at O
some O
groups O
( O
e.g O
. O
section 1
id pdf2json/2021.acl-long.4.pdf.json
women O
) O
than O
at O
others O
. O
section 1
id pdf2json/2021.acl-long.4.pdf.json
For O
practical O
applications O
such O
as O
content O
moderation O
and O
further O
research O
use O
, O
these O
are O
critical O
model O
weaknesses O
. O
section 1
id pdf2json/2021.acl-long.4.pdf.json
We O
hope O
that O
by O
revealing O
such O
weaknesses O
, O
HATECHECK O
can O
play O
a B-PARAMETER
key O
role O
in O
the O
development O
of O
better O
hate O
speech O
detection O
models O
. O
section 1
id pdf2json/2021.acl-long.4.pdf.json
Definition O
of O
Hate O
Speech O
We O
draw O
on O
previous O
definitions O
of O
hate O
speech O
( O
Warner B-AUTHOR
and O
Hirschberg B-AUTHOR
, O
2012 O
; O
Davidson B-AUTHOR
et O
al. O
, O
2017 O
) O
as O
well O
as O
recent O
typologies O
of O
abusive O
content O
( O
Vidgen B-AUTHOR
et O
al. O
, O
2019 O
; O
Banko B-AUTHOR
et O
al. O
, O
2020 O
) O
to O
define O
hate O
speech O
as O
abuse O
that O
is O
targeted O
at O
a B-PARAMETER
protected O
group O
or O
at O
its O
members O
for O
being O
a B-PARAMETER
part O
of O
that O
group O
. O
section 1
id pdf2json/2021.acl-long.4.pdf.json
We O
define O
protected O
groups O
based O
on O
age O
, O
disability O
, O
gender O
identity O
, O
familial O
status O
, O
pregnancy O
, O
race O
, O
national O
or O
ethnic O
origins O
, O
religion O
, O
sex O
or O
sexual O
orientation O
, O
which O
broadly O
reflects O
international O
legal O
consensus O
( O
particularly O
the O
UK O
’ O
s O
2010 O
Equality O
Act O
, O
the O
US B-PARAMETER
1964 O
Civil O
Rights O
Act O
and O
the O
EU O
’ O
s O
Charter O
of O
Fundamental O
Rights O
) O
. O
section 1
id pdf2json/2021.acl-long.4.pdf.json
Based O
on O
these O
definitions O
, O
we O
approach O
hate O
speech O
detection O
as O
the O
binary O
classification O
of O
content O
as O
either O
hateful O
or O
1All O
HATECHECK O
test O
cases O
and O
annotations O
are O
available O
on O
https O
: O
//github.com/paul-rottger/hatecheck-data O
. O
section 1
id pdf2json/2021.acl-long.4.pdf.json
2www.perspectiveapi.com O
and O
www.siftninja.com O
non-hateful O
. O
section 1
id pdf2json/2021.acl-long.4.pdf.json
Other O
work O
has O
further O
differentiated O
between O
different O
types O
of O
hate O
and O
non-hate O
( O
e.g O
. O
section 1
id pdf2json/2021.acl-long.4.pdf.json
Founta B-AUTHOR
et O
al. O
, O
2018 O
; O
Salminen B-AUTHOR
et O
al. O
, O
2018 O
; O
Zampieri B-AUTHOR
et O
al. O
, O
2019 O
) O
, O
but O
such O
taxonomies O
can O
be O
collapsed O
into O
a B-PARAMETER
binary O
distinction O
and O
are O
thus O
compatible O
with O
HATECHECK O
. O
section 1
id pdf2json/2021.acl-long.4.pdf.json
Content O
Warning O
This O
article O
contains O
examples O
of O
hateful O
and O
abusive O
language O
. O
section 1
id pdf2json/2021.acl-long.4.pdf.json
All O
examples O
are O
taken O
from O
HATECHECK O
to O
illustrate O
its O
composition O
. O
section 1
id pdf2json/2021.acl-long.4.pdf.json
Examples O
are O
quoted O
verbatim O
, O
except O
for O
hateful O
slurs O
and O
profanity O
, O
for O
which O
the O
first O
vowel O
is O
replaced O
with O
an O
asterisk O
. O


section 3
id pdf2json/2021.acl-long.4.pdf.json
In O
software O
engineering O
, O
a B-PARAMETER
program O
has O
a B-PARAMETER
certain O
functionality O
if O
it O
meets O
a B-PARAMETER
specified O
input/output O
behaviour O
( O
ISO/IEC/IEEE O
24765:2017 O
, O
E O
) O
. O
section 3
id pdf2json/2021.acl-long.4.pdf.json
Accordingly O
, O
we O
operationalise O
a B-PARAMETER
functionality O
of O
a B-PARAMETER
hate O
speech O
detection O
model O
as O
its O
ability O
to O
provide O
a B-PARAMETER
specified O
classification O
( O
hateful O
or O
non-hateful O
) O
for O
test O
cases O
in O
a B-PARAMETER
corresponding O
functional O
test O
. O
section 3
id pdf2json/2021.acl-long.4.pdf.json
For O
instance O
, O
a B-PARAMETER
model O
might O
correctly O
classify O
hate O
expressed O
using O
profanity O
( O
e.g O
“ O
F*ck O
all O
black O
people O
” O
) O
but O
misclassify O
non-hateful O
uses O
of O
profanity O
( O
e.g O
. O
section 3
id pdf2json/2021.acl-long.4.pdf.json
“ O
F*cking O
hell O
, O
what O
a B-PARAMETER
day O
” O
) O
, O
which O
is O
why O
we O
test O
them O
as O
separate O
functionalities O
. O
section 3
id pdf2json/2021.acl-long.4.pdf.json
Since O
both O
functionalities O
relate O
to O
profanity O
usage O
, O
we O
group O
them O
into O
a B-PARAMETER
common O
functionality O
class O
. O

section 4
id pdf2json/2021.acl-long.4.pdf.json
To O
generate O
an O
initial O
list O
of O
59 O
functionalities O
, O
we O
reviewed O
previous O
hate O
speech O
detection O
research O
and O
interviewed O
civil O
society O
stakeholders O
. O
section 4
id pdf2json/2021.acl-long.4.pdf.json
Review O
of O
Previous O
Research O
We O
identified O
different O
types O
of O
hate O
in O
taxonomies O
of O
abusive O
content O
( O
e.g O
. O
section 4
id pdf2json/2021.acl-long.4.pdf.json
Zampieri B-AUTHOR
et O
al. O
, O
2019 O
; O
Banko B-AUTHOR
et O
al. O
, O
2020 O
; O
Kurrek B-AUTHOR
et O
al. O
, O
2020 O
) O
. O
section 4
id pdf2json/2021.acl-long.4.pdf.json
We O
also O
identified O
likely O
model O
weaknesses O
based O
on O
error O
analyses O
( O
e.g O
. O
section 4
id pdf2json/2021.acl-long.4.pdf.json
Davidson B-AUTHOR
et O
al. O
, O
2017 O
; O
van B-AUTHOR
Aken I-AUTHOR
et O
al. O
, O
2018 O
; O
Vidgen B-AUTHOR
et O
al. O
, O
2020a O
) O
as O
well O
as O
review O
articles O
and O
commentaries O
( O
e.g O
. O
section 4
id pdf2json/2021.acl-long.4.pdf.json
Schmidt B-AUTHOR
and O
Wiegand B-AUTHOR
, O
2017 O
; O
Fortuna B-AUTHOR
and O
Nunes B-AUTHOR
, O
2018 O
; O
Vidgen B-AUTHOR
et O
al. O
, O
2019 O
) O
. O
section 4
id pdf2json/2021.acl-long.4.pdf.json
For O
example O
, O
hate O
speech O
detection O
models O
have O
been O
shown O
to O
struggle O
with O
correctly O
classifying O
negated O
phrases O
such O
as O
“ O
I O
don O
’ O
t O
hate O
trans O
people O
” O
( O
Hosseini B-AUTHOR
et O
al. O
, O
2017 O
; O
Dinan B-AUTHOR
et O
al. O
, O
2019 O
) O
. O
section 4
id pdf2json/2021.acl-long.4.pdf.json
We O
therefore O
included O
functionalities O
for O
negation O
in O
hateful O
and O
non-hateful O
content O
. O
section 4
id pdf2json/2021.acl-long.4.pdf.json
Interviews O
We O
interviewed O
21 O
employees O
from O
16 O
British O
, O
German O
and O
American O
NGOs O
whose O
work O
directly O
relates O
to O
online O
hate O
. O
section 4
id pdf2json/2021.acl-long.4.pdf.json
Most O
of O
the O
NGOs O
are O
involved O
in O
monitoring O
and O
reporting O
online O
hate O
, O
often O
with O
“ O
trusted O
flagger O
” O
status O
on O
platforms O
such O
as O
Twitter B-SOFTWARE
and O
Facebook O
. O
section 4
id pdf2json/2021.acl-long.4.pdf.json
Several O
NGOs O
provide O
legal O
advocacy O
and O
victim O
support O
or O
otherwise O
represent O
communities O
that O
are O
often O
targeted O
by O
online O
hate O
, O
such O
as O
Muslims O
or O
LGBT+ O
people O
. O
section 4
id pdf2json/2021.acl-long.4.pdf.json
The O
vast O
majority O
of O
interviewees O
do O
not O
have O
a B-PARAMETER
technical O
background O
, O
but O
extensive O
practical O
experience O
engaging O
with O
online O
hate O
and O
content O
moderation O
systems O
. O
section 4
id pdf2json/2021.acl-long.4.pdf.json
They O
have O
a B-PARAMETER
variety O
of O
ethnic O
and O
cultural O
backgrounds O
, O
and O
most O
of O
them O
have O
been O
targeted O
by O
online O
hate O
themselves O
. O
section 4
id pdf2json/2021.acl-long.4.pdf.json
The O
interviews O
were O
semi-structured O
. O
section 4
id pdf2json/2021.acl-long.4.pdf.json
In O
a B-PARAMETER
typical O
interview O
, O
we O
would O
first O
ask O
open-ended O
questions O
about O
online O
hate O
( O
e.g O
. O
section 4
id pdf2json/2021.acl-long.4.pdf.json
“ O
What O
do O
you O
think O
are O
the O
biggest O
challenges O
in O
tackling O
online O
hate O
? O
” O
) O
and O
then O
about O
hate O
speech O
detection O
models O
, O
particularly O
their O
perceived O
weaknesses O
( O
e.g O
. O
section 4
id pdf2json/2021.acl-long.4.pdf.json
“ O
What O
sort O
of O
content O
have O
you O
seen O
moderation O
systems O
get O
wrong O
? O
” O
) O
and O
potential O
improvements O
, O
unbounded O
by O
technical O
feasibility O
( O
e.g O
. O
section 4
id pdf2json/2021.acl-long.4.pdf.json
“ O
If O
you O
could O
design O
an O
ideal O
hate O
detection O
system O
, O
what O
would O
it O
be O
able O
to O
do O
? O
” O
) O
. O
section 4
id pdf2json/2021.acl-long.4.pdf.json
Using O
a B-PARAMETER
grounded O
theory O
approach O
( O
Corbin B-AUTHOR
and O
Strauss B-AUTHOR
, O
1990 O
) O
, O
we O
identified O
emergent O
themes O
in O
the O
interview O
responses O
and O
translated O
them O
into O
model O
functionalities O
. O
section 4
id pdf2json/2021.acl-long.4.pdf.json
For O
example O
, O
several O
interviewees O
raised O
concerns O
around O
the O
misclassification O
of O
counter O
speech O
, O
i.e O
. O
section 4
id pdf2json/2021.acl-long.4.pdf.json
direct O
responses O
to O
hateful O
content O
( O
e.g O
. O
section 4
id pdf2json/2021.acl-long.4.pdf.json
I4 O
: O
“ O
people O
will O
be O
quoting O
someone O
, O
calling O
that O
person O
out O
[ O
... O
] O
but O
that O
will O
get O
picked O
up O
by O
the O
system O
” O
) O
.3 O
We O
therefore O
included O
functionalities O
for O
counter O
speech O
that O
quotes O
or O
references O
hate O
. O
section 4
id pdf2json/2021.acl-long.4.pdf.json
Selection O
Criteria O
From O
the O
initial O
list O
of O
59 O
functionalities O
, O
we O
select O
those O
in O
HATECHECK O
based O
on O
two O
practical O
considerations O
. O
section 4
id pdf2json/2021.acl-long.4.pdf.json
First O
, O
we O
restrict O
HATECHECK O
’ O
s O
scope O
to O
individual O
English O
language O
text O
documents O
. O
section 4
id pdf2json/2021.acl-long.4.pdf.json
This O
is O
due O
to O
practical O
constraints O
, O
and O
because O
most O
hate O
speech O
detection O
models O
are O
developed O
for O
such O
data O
( O
Poletto B-AUTHOR
et O
al. O
, O
2020 O
; O
Vidgen B-AUTHOR
and O
Derczynski B-AUTHOR
, O
2020 O
) O
. O
section 4
id pdf2json/2021.acl-long.4.pdf.json
Thus O
, O
HATECHECK O
does O
not O
test O
functionalities O
that O
relate O
to O
other O
modalities O
( O
e.g O
. O
section 4
id pdf2json/2021.acl-long.4.pdf.json
images O
) O
3When O
quoting O
anonymised O
responses O
throughout O
this O
article O
, O
we O
identify O
each O
interview O
participant O
by O
a B-PARAMETER
unique O
ID O
. O
section 4
id pdf2json/2021.acl-long.4.pdf.json
We O
can O
not O
release O
full O
interview O
transcripts O
due O
to O
the O
sensitive O
nature O
of O
work O
in O
this O
area O
, O
the O
confidentiality O
terms O
agreed O
with O
our O
participants O
and O
our O
ethics O
clearance O
. O
section 4
id pdf2json/2021.acl-long.4.pdf.json
or O
languages O
, O
or O
that O
require O
context O
( O
e.g O
. O
section 4
id pdf2json/2021.acl-long.4.pdf.json
conversational O
or O
social O
) O
beyond O
individual O
documents O
. O
section 4
id pdf2json/2021.acl-long.4.pdf.json
Second O
, O
we O
only O
test O
functionalities O
for O
which O
we O
can O
construct O
test O
cases O
with O
clear O
gold B-METRIC
standard I-METRIC
labels I-METRIC
. O
section 4
id pdf2json/2021.acl-long.4.pdf.json
Therefore O
, O
we O
do O
not O
test O
functionalities O
that O
lack O
broad O
consensus O
in O
our O
interviews O
and O
the O
literature O
regarding O
what O
is O
and O
is O
not O
hateful O
. O
section 4
id pdf2json/2021.acl-long.4.pdf.json
The O
use O
of O
humour O
, O
for O
instance O
, O
has O
been O
highlighted O
as O
an O
important O
challenge O
for O
hate O
speech O
research O
( O
van B-AUTHOR
Aken I-AUTHOR
et O
al. O
, O
2018 O
; O
Qian B-AUTHOR
et O
al. O
, O
2018 O
; O
Vidgen B-AUTHOR
et O
al. O
, O
2020a O
) O
. O
section 4
id pdf2json/2021.acl-long.4.pdf.json
However O
, O
whether O
humorous O
statements O
are O
hateful O
is O
heavily O
contingent O
on O
normative O
claims O
( O
e.g O
. O
section 4
id pdf2json/2021.acl-long.4.pdf.json
I5 O
: O
“ O
it O
’ O
s O
a B-PARAMETER
value O
judgment O
thing O
” O
) O
, O
which O
is O
why O
we O
do O
not O
test O
them O
in O
HATECHECK O
. O

section 5
id pdf2json/2021.acl-long.4.pdf.json
HATECHECK O
comprises O
29 O
functional O
tests O
grouped O
into O
11 B-PARAMETER
classes O
. O
section 5
id pdf2json/2021.acl-long.4.pdf.json
Each O
test O
evaluates O
one O
functionality O
and O
is O
associated O
with O
one O
gold O
standard O
label O
( O
hateful O
or O
non-hateful O
) O
. O
section 5
id pdf2json/2021.acl-long.4.pdf.json
Each O
functional O
test O
has O
a B-PARAMETER
set O
of O
corresponding O
test O
cases O
. O
section 5
id pdf2json/2021.acl-long.4.pdf.json
18 O
functional O
tests O
for O
hateful O
content O
in O
HATECHECK O
cover O
distinct O
expressions O
of O
hate O
. O
section 5
id pdf2json/2021.acl-long.4.pdf.json
They O
are O
distinct O
in O
the O
sense O
that O
we O
minimise O
overlap O
between O
them O
, O
for O
instance O
by O
testing O
slurs O
( O
“ O
f*g O
” O
) O
and O
profanity O
( O
“ O
f*ck O
” O
) O
in O
separate O
functional O
tests O
rather O
than O
jointly O
( O
“ O
f*cking O
f*g O
” O
) O
, O
so B-PARAMETER
that O
each O
test O
isolates O
one O
particular O
type O
of O
expression O
. O
section 5
id pdf2json/2021.acl-long.4.pdf.json
The O
other O
11 B-PARAMETER
functional O
tests O
for O
non-hateful O
content O
cover O
contrastive O
non-hate O
, O
i.e O
. O
section 5
id pdf2json/2021.acl-long.4.pdf.json
content O
which O
shares O
linguistic O
features O
with O
hateful O
expressions O
. O
section 5
id pdf2json/2021.acl-long.4.pdf.json
The O
challenges O
posed O
by O
such O
content O
are O
a B-PARAMETER
key O
theme O
in O
our O
interviews O
and O
the O
literature O
. O
section 5
id pdf2json/2021.acl-long.4.pdf.json
We O
construct O
every O
non-hateful O
test O
case O
as O
a B-PARAMETER
direct O
contrast O
to O
a B-PARAMETER
hateful O
test O
case O
, O
making O
only O
minimal O
changes O
. O
section 5
id pdf2json/2021.acl-long.4.pdf.json
For O
instance O
, O
“ O
I O
love O
immigrants O
” O
is O
a B-PARAMETER
test O
case O
in O
F19 O
: O
positive O
statements O
using O
a B-PARAMETER
protected O
group O
identifier O
. O
section 5
id pdf2json/2021.acl-long.4.pdf.json
It O
directly O
contrasts O
the O
test O
case O
“ O
I O
hate O
immigrants O
” O
in O
F1 B-METRIC
: O
strong O
negative O
emotions O
explicitly O
expressed O
about O
a B-PARAMETER
protected O
group O
. O
section 5
id pdf2json/2021.acl-long.4.pdf.json
In O
the O
following O
, O
we O
give O
a B-PARAMETER
brief O
overview O
of O
the O
different O
functional O
tests O
in O
HATECHECK O
. O
section 5
id pdf2json/2021.acl-long.4.pdf.json
Table O
1 O
provides O
corresponding O
example O
test O
cases O
. O
section 5
id pdf2json/2021.acl-long.4.pdf.json
Each O
individual O
test O
is O
grounded O
in O
direct O
references O
to O
previous O
work O
and/or O
our O
interviews O
. O
section 5
id pdf2json/2021.acl-long.4.pdf.json
These O
references O
are O
detailed O
in O
Appendix O
B O
. O
section 5
id pdf2json/2021.acl-long.4.pdf.json
Distinct O
Expressions O
of O
Hate O
HATECHECK O
tests O
different O
types O
of O
derogatory O
hate O
speech O
( O
F1-4 O
) O
and O
hate O
expressed O
through O
threatening O
language O
( O
F5/6 O
) O
. O
section 5
id pdf2json/2021.acl-long.4.pdf.json
It O
tests O
hate O
expressed O
using O
slurs O
( O
F7 O
) O
and O
profanity O
( O
F10 O
) O
. O
section 5
id pdf2json/2021.acl-long.4.pdf.json
It O
also O
tests O
hate O
expressed O
through O
pronoun O
reference O
( O
F12/13 O
) O
, O
negation O
( O
F14 O
) O
and O
phrasing O
variants O
, O
specifically O
questions O
and O
opinions O
( O
F16/17 O
) O
. O
section 5
id pdf2json/2021.acl-long.4.pdf.json
Lastly O
, O
it O
tests O
hate O
containing O
spelling O
variations O
such O
as O
missing O
characters O
or O
leet O
speak O
( O
F25-29 O
) O
. O
section 5
id pdf2json/2021.acl-long.4.pdf.json
Contrastive O
Non-Hate O
HATECHECK O
tests O
non-hateful O
contrasts O
for O
slurs O
, O
particularly O
slur O
homonyms O
and O
reclaimed O
slurs O
( O
F8/9 O
) O
, O
as O
well O
as O
for O
profanity O
( O
F11 O
) O
. O
section 5
id pdf2json/2021.acl-long.4.pdf.json
It O
tests O
nonhateful O
contrasts O
that O
use O
negation O
, O
i.e O
. O
section 5
id pdf2json/2021.acl-long.4.pdf.json
negated O
hate O
( O
F15 O
) O
. O
section 5
id pdf2json/2021.acl-long.4.pdf.json
It O
also O
tests O
non-hateful O
contrasts O
around O
protected O
group O
identifiers O
( O
F18/19 O
) O
. O
section 5
id pdf2json/2021.acl-long.4.pdf.json
It O
tests O
contrasts O
in O
which O
hate O
speech O
is O
quoted O
or O
referenced O
to O
non-hateful O
effect O
, O
specifically O
counter O
speech O
, O
i.e O
. O
section 5
id pdf2json/2021.acl-long.4.pdf.json
direct O
responses O
to O
hate O
speech O
which O
seek O
to O
act O
against O
it O
( O
F20/21 O
) O
. O
section 5
id pdf2json/2021.acl-long.4.pdf.json
Lastly O
, O
it O
tests O
non-hateful O
contrasts O
which O
target B-PARAMETER
out-of-scope O
entities O
such O
as O
objects O
( O
F22-24 O
) O
rather O
than O
a B-PARAMETER
protected O
group O
. O

section 6
id pdf2json/2021.acl-long.4.pdf.json
For O
each O
functionality O
in O
HATECHECK O
, O
we O
handcraft O
sets O
of O
test O
cases O
– O
short O
English-language O
text O
documents O
that O
clearly O
correspond O
to O
just O
one O
gold O
standard O
label O
. O
section 6
id pdf2json/2021.acl-long.4.pdf.json
Within O
each O
functionality O
, O
we O
aim O
to O
use O
diverse O
vocabulary O
and O
syntax O
to O
reduce O
similarity O
between O
test O
cases O
, O
which O
Zhou B-AUTHOR
et O
al O
. O
section 6
id pdf2json/2021.acl-long.4.pdf.json
( O
2020 O
) O
suggest O
as O
a B-PARAMETER
likely O
cause O
of O
performance O
instability O
for O
diagnostic O
datasets O
. O
section 6
id pdf2json/2021.acl-long.4.pdf.json
To O
generate O
test O
cases O
at O
scale O
, O
we O
use O
templates O
( O
Dixon B-AUTHOR
et O
al. O
, O
2018 O
; O
Garg B-AUTHOR
et O
al. O
, O
2019 O
; O
Ribeiro B-AUTHOR
et O
al. O
, O
2020 O
) O
, O
in O
which O
we O
replace O
tokens O
for O
protected O
group O
identifiers O
( O
e.g O
. O
section 6
id pdf2json/2021.acl-long.4.pdf.json
“ O
I O
hate O
[ O
IDENTITY O
] O
. O
” O
) O
and O
slurs O
( O
e.g O
. O
section 6
id pdf2json/2021.acl-long.4.pdf.json
“ O
You O
are O
just O
a B-PARAMETER
[ O
SLUR O
] O
to O
me. O
” O
) O
. O
section 6
id pdf2json/2021.acl-long.4.pdf.json
This O
also O
ensures O
that O
HATECHECK O
has O
an O
equal O
number O
of O
cases O
targeted O
at O
different O
protected O
groups O
. O
section 6
id pdf2json/2021.acl-long.4.pdf.json
HATECHECK O
covers O
seven O
protected O
groups O
: O
women O
( O
gender O
) O
, O
trans O
people O
( O
gender O
identity O
) O
, O
gay O
people O
( O
sexual O
orientation O
) O
, O
black O
people O
( O
race O
) O
, O
disabled O
people O
( O
disability O
) O
, O
Muslims O
( O
religion O
) O
and O
immigrants O
( O
national O
origin O
) O
. O
section 6
id pdf2json/2021.acl-long.4.pdf.json
For O
details O
on O
which O
slurs O
are O
covered O
by O
HATECHECK O
and O
how O
they O
were O
selected O
, O
see O
Appendix O
C. O
In O
total O
, O
we O
generate O
3,901 O
cases O
, O
3,495 O
of O
which O
come O
from O
460 O
templates O
. O
section 6
id pdf2json/2021.acl-long.4.pdf.json
The O
other O
406 O
cases O
do O
not O
use O
template O
tokens O
( O
e.g O
. O
section 6
id pdf2json/2021.acl-long.4.pdf.json
“ O
Sh*t O
, O
I O
forgot O
my O
keys O
” O
) O
and O
are O
thus O
crafted O
individually O
. O
section 6
id pdf2json/2021.acl-long.4.pdf.json
The O
average B-METRIC
length B-METRIC
of O
cases O
is O
8.87 O
words O
( O
std O
. O
section 6
id pdf2json/2021.acl-long.4.pdf.json
dev O
. O
section 6
id pdf2json/2021.acl-long.4.pdf.json
= O
3.33 O
) O
or O
48.26 O
characters O
( O
std O
. O
section 6
id pdf2json/2021.acl-long.4.pdf.json
dev O
. O
section 6
id pdf2json/2021.acl-long.4.pdf.json
= O
16.88 O
) O
. O
section 6
id pdf2json/2021.acl-long.4.pdf.json
2,659 O
of O
the O
3,901 O
cases O
( O
68.2 O
% O
) O
are O
hateful O
and O
1,242 O
( O
31.8 O
% O
) O
are O
non-hateful O
. O
section 6
id pdf2json/2021.acl-long.4.pdf.json
Secondary O
Labels O
In O
addition O
to O
the O
primary O
label O
( O
hateful O
or O
non-hateful O
) O
we O
provide O
up O
to O
two O
secondary O
labels O
for O
all O
cases O
. O
section 6
id pdf2json/2021.acl-long.4.pdf.json
For O
cases O
targeted O
at O
or O
referencing O
a B-PARAMETER
particular O
protected O
group O
, O
we O
provide O
a B-PARAMETER
label O
for O
the O
group O
that O
is O
targeted O
. O
section 6
id pdf2json/2021.acl-long.4.pdf.json
For O
hateful O
cases O
, O
we O
also O
label O
whether O
they O
are O
targeted O
at O
a B-PARAMETER
group O
in O
general O
or O
at O
individuals O
, O
which O
is O
a B-PARAMETER
common O
distinction O
in O
taxonomies O
of O
abuse O
( O
e.g O
. O
section 6
id pdf2json/2021.acl-long.4.pdf.json
Waseem B-AUTHOR
et O
al. O
, O
2017 O
; O
Zampieri B-AUTHOR
et O
al. O
, O
2019 O
) O
. O

section 7
id pdf2json/2021.acl-long.4.pdf.json
To O
validate O
gold O
standard O
primary O
labels O
of O
test O
cases O
in O
HATECHECK O
, O
we O
recruited O
and O
trained O
ten O
annotators.4 O
In O
addition O
to O
the O
binary O
annotation O
task O
, O
we O
also O
gave O
annotators O
the O
option O
to O
flag O
cases O
as O
unrealistic O
( O
e.g O
. O
section 7
id pdf2json/2021.acl-long.4.pdf.json
nonsensical O
) O
to O
further O
confirm O
data O
quality O
. O
section 7
id pdf2json/2021.acl-long.4.pdf.json
Each O
annotator O
was O
randomly O
assigned O
approximately O
2,000 O
test O
cases O
, O
so B-PARAMETER
that O
each O
of O
the O
3,901 O
cases O
was O
annotated O
by O
exactly O
five O
annotators O
. O
section 7
id pdf2json/2021.acl-long.4.pdf.json
We O
use O
Fleiss O
’ O
Kappa O
to O
measure O
inter-annotator O
agreement O
( O
Hallgren B-AUTHOR
, O
2012 O
) O
and O
obtain O
a B-PARAMETER
score O
of O
0.93 O
, O
which O
indicates O
“ O
almost O
perfect O
” O
agreement O
( O
Landis B-AUTHOR
and O
Koch B-AUTHOR
, O
1977 O
) O
. O
section 7
id pdf2json/2021.acl-long.4.pdf.json
For O
3,879 O
( O
99.4 O
% O
) O
of O
the O
3,901 O
cases O
, O
at O
least O
four O
out O
of O
five O
annotators O
agreed O
with O
our O
gold O
standard O
label O
. O
section 7
id pdf2json/2021.acl-long.4.pdf.json
For O
22 O
cases O
, O
agreement O
was O
less O
than O
four O
out O
of O
five O
. O
section 7
id pdf2json/2021.acl-long.4.pdf.json
To O
ensure O
that O
the O
label O
of O
each O
HATECHECK O
case O
is O
unambiguous O
, O
we O
exclude O
these O
22 O
cases O
. O
section 7
id pdf2json/2021.acl-long.4.pdf.json
We O
also O
exclude O
all O
cases O
generated O
from O
the O
same O
templates O
as O
these O
22 O
cases O
to O
avoid O
biases O
in O
target B-PARAMETER
coverage O
, O
as O
otherwise O
hate O
against O
some O
protected O
groups O
would O
be O
less O
well O
represented O
than O
hate O
against O
others O
. O
section 7
id pdf2json/2021.acl-long.4.pdf.json
In O
total O
, O
we O
exclude O
173 O
cases O
, O
reducing O
the O
size O
of O
the O
dataset O
to O
3,728 O
test O
cases.5 O
Only O
23 O
cases O
were O
flagged O
as O
unrealistic O
by O
one O
annotator O
, O
and O
none O
were O
flagged O
by O
more O
than O
one O
annotator O
. O
section 7
id pdf2json/2021.acl-long.4.pdf.json
Thus O
, O
we O
do O
not O
exclude O
any O
test O
cases O
for O
being O
unrealistic O
. O


section 9
id pdf2json/2021.acl-long.4.pdf.json
As O
a B-PARAMETER
suite O
of O
black-box O
tests O
, O
HATECHECK O
is O
broadly O
applicable O
across O
English-language O
hate O
speech O
detection O
models O
. O
section 9
id pdf2json/2021.acl-long.4.pdf.json
Users O
can O
compare O
different O
architectures O
trained O
on O
different O
datasets O
and O
even O
commercial O
models O
for O
which O
public O
information O
on O
architecture O
and O
training O
data O
is O
limited O
. O
section 9
id pdf2json/2021.acl-long.4.pdf.json
4For O
information O
on O
annotator O
training O
, O
their O
background O
and O
demographics O
, O
see O
the O
data O
statement O
in O
Appendix O
A O
. O
section 9
id pdf2json/2021.acl-long.4.pdf.json
5We O
make O
data O
on O
annotation O
outcomes O
available O
for O
all O
cases O
we O
generated O
, O
including O
the O
ones O
not O
in O
HATECHECK O
. O
section 9
id pdf2json/2021.acl-long.4.pdf.json
( O
2,563 O
in O
18 O
functional O
tests O
) O
are O
labelled O
hateful O
, O
31.2 O
% O
( O
1,165 O
in O
11 B-PARAMETER
functional O
tests O
) O
are O
labelled O
non-hateful O
. O
section 9
id pdf2json/2021.acl-long.4.pdf.json
The O
right-most O
columns O
report O
accuracy B-PARAMETER
( O
% O
) O
on O
each O
functional O
test O
for O
the O
models O
described O
in O
§3.1 O
. O
section 9
id pdf2json/2021.acl-long.4.pdf.json
Best O
performance O
on O
each O
functional O
test O
is O
bolded O
. O
section 9
id pdf2json/2021.acl-long.4.pdf.json
Below O
random O
choice O
performance O
( O
< O
50 O
% O
) O
is O
highlighted O
in O
cursive O
red O
. O
section 9
id pdf2json/2021.acl-long.4.pdf.json
Pre-Trained O
Transformer B-METHOD
Models O
We O
test O
an O
uncased O
BERT-base B-METHOD
model O
( O
Devlin B-AUTHOR
et O
al. O
, O
2019 O
) O
, O
which O
has O
been O
shown O
to O
achieve O
near O
state-of-theart O
performance O
on O
several O
abuse O
detection O
tasks O
( O
Tran B-AUTHOR
et O
al. O
, O
2020 O
) O
. O
section 9
id pdf2json/2021.acl-long.4.pdf.json
We O
fine-tune O
BERT B-SOFTWARE
on O
two O
widely-used O
hate O
speech O
datasets O
from O
Davidson B-AUTHOR
et O
al O
. O
section 9
id pdf2json/2021.acl-long.4.pdf.json
( O
2017 O
) O
and O
Founta B-AUTHOR
et O
al O
. O
section 9
id pdf2json/2021.acl-long.4.pdf.json
( O
2018 O
) O
. O
section 9
id pdf2json/2021.acl-long.4.pdf.json
The O
Davidson B-AUTHOR
et O
al O
. O
section 9
id pdf2json/2021.acl-long.4.pdf.json
( O
2017 O
) O
dataset O
contains O
24,783 O
tweets O
annotated O
as O
either O
hateful O
, O
offensive O
or O
neither O
. O
section 9
id pdf2json/2021.acl-long.4.pdf.json
The O
Founta B-AUTHOR
et O
al O
. O
section 9
id pdf2json/2021.acl-long.4.pdf.json
( O
2018 O
) O
dataset O
comprises O
99,996 O
tweets O
annotated O
as O
hateful O
, O
abusive O
, O
spam O
and O
normal O
. O
section 9
id pdf2json/2021.acl-long.4.pdf.json
For O
both O
datasets O
, O
we O
collapse O
labels O
other O
than O
hateful O
into O
a B-PARAMETER
single O
non-hateful O
label O
to O
match O
HATECHECK O
’ O
s O
binary O
format O
. O
section 9
id pdf2json/2021.acl-long.4.pdf.json
This O
is O
aligned O
with O
the O
original O
multi-label O
setup O
of O
the O
two O
datasets O
. O
section 9
id pdf2json/2021.acl-long.4.pdf.json
Davidson B-AUTHOR
et O
al O
. O
section 9
id pdf2json/2021.acl-long.4.pdf.json
( O
2017 O
) O
, O
for O
instance O
, O
explicitly O
characterise O
offensive O
content O
in O
their O
dataset O
as O
non-hateful O
. O
section 9
id pdf2json/2021.acl-long.4.pdf.json
Respectively O
, O
hateful O
cases O
make O
up O
5.8 O
% O
and O
5.0 O
% O
of O
the O
datasets O
. O
section 9
id pdf2json/2021.acl-long.4.pdf.json
Details O
on O
both O
datasets O
and O
pre-processing O
steps O
can O
be O
found O
in O
Appendix O
D. O
In O
the O
following O
, O
we O
denote O
BERT B-SOFTWARE
fine-tuned O
on O
binary O
Davidson B-AUTHOR
et O
al O
. O
section 9
id pdf2json/2021.acl-long.4.pdf.json
( O
2017 O
) O
data O
by O
B-D O
and O
BERT B-SOFTWARE
fine-tuned O
on O
binary O
Founta B-AUTHOR
et O
al O
. O
section 9
id pdf2json/2021.acl-long.4.pdf.json
( O
2018 O
) O
data O
by O
B-F. O
To O
account O
for O
class O
imbalance O
, O
we O
use O
class O
weights B-PARAMETER
emphasising O
the O
hateful O
minority O
class O
( O
He B-AUTHOR
and O
Garcia B-AUTHOR
, O
2009 O
) O
. O
section 9
id pdf2json/2021.acl-long.4.pdf.json
For O
both O
datasets O
, O
we O
use O
a B-PARAMETER
stratified O
80/10/10 O
train/dev/test O
split O
. O
section 9
id pdf2json/2021.acl-long.4.pdf.json
Macro B-METRIC
F1 I-METRIC
on O
the O
held-out O
test O
sets O
is O
70.8 O
for O
B-D O
and O
70.3 O
for O
B-F.6 O
Details O
on O
model O
training O
and O
parameters O
can O
be O
found O
in O
Appendix O
E. O
Commercial O
Models O
We O
test O
Google B-SOFTWARE
Jigsaw O
’ O
s O
Perspective O
( O
P O
) O
and O
Two O
Hat O
’ O
s O
SiftNinja O
( O
SN O
) O
.7 O
Both O
are O
popular O
models O
for O
content O
moderation O
developed O
by O
major O
tech O
companies O
that O
can O
be O
accessed O
by O
registered O
users O
via O
an O
API O
. O
section 9
id pdf2json/2021.acl-long.4.pdf.json
For O
a B-PARAMETER
given O
input O
text O
, O
P O
provides O
percentage O
scores O
across O
attributes O
such O
as O
“ O
toxicity O
” O
and O
“ O
profanity O
” O
. O
section 9
id pdf2json/2021.acl-long.4.pdf.json
We O
use O
“ O
identity O
attack O
” O
, O
which O
aims O
at O
identifying O
“ O
negative O
or O
hateful O
comments O
targeting O
someone O
because O
of O
their O
identity O
” O
and O
thus O
aligns O
closely O
with O
our O
definition O
of O
hate O
speech O
( O
§1 O
) O
. O
section 9
id pdf2json/2021.acl-long.4.pdf.json
We O
convert O
the O
percentage O
score O
to O
a B-PARAMETER
binary O
label O
using O
a B-PARAMETER
cutoff O
of O
50 O
% O
. O
section 9
id pdf2json/2021.acl-long.4.pdf.json
We O
tested O
P O
in O
December O
2020 O
. O
section 9
id pdf2json/2021.acl-long.4.pdf.json
For O
SN O
, O
we O
use O
its O
‘ O
hate O
speech O
’ O
attribute O
( O
“ O
attacks O
[ O
on O
] O
a B-PARAMETER
person O
or O
group O
on O
the O
basis O
of O
personal O
6For O
better O
comparability O
to O
previous O
work O
, O
we O
also O
finetuned O
unweighted O
versions O
of O
our O
models O
on O
the O
original O
multiclass O
D O
and O
F O
data O
. O
section 9
id pdf2json/2021.acl-long.4.pdf.json
Their O
performance O
matches O
SOTA B-SOFTWARE
results O
( O
Mozafari B-AUTHOR
et O
al. O
, O
2019 O
; O
Cao B-AUTHOR
et O
al. O
, O
2020 O
) O
. O
section 9
id pdf2json/2021.acl-long.4.pdf.json
Details O
in O
Appx O
. O
section 9
id pdf2json/2021.acl-long.4.pdf.json
F. O
7www.perspectiveapi.com O
and O
www.siftninja.com O
attributes O
or O
identities O
” O
) O
, O
which O
distinguishes O
between O
‘ O
mild O
’ O
, O
‘ O
bad O
’ O
, O
‘ O
severe O
’ O
and O
‘ O
no O
’ O
hate O
. O
section 9
id pdf2json/2021.acl-long.4.pdf.json
We O
mark O
all O
but O
‘ O
no O
’ O
hate O
as O
‘ O
hateful O
’ O
to O
obtain O
binary O
labels O
. O
section 9
id pdf2json/2021.acl-long.4.pdf.json
We O
tested O
SN O
in O
January O
2021 O
. O

section 10
id pdf2json/2021.acl-long.4.pdf.json
We O
assess O
model O
performance O
on O
HATECHECK O
using O
accuracy B-PARAMETER
, O
i.e O
. O
section 10
id pdf2json/2021.acl-long.4.pdf.json
the O
proportion O
of O
correctly O
classified O
test O
cases O
. O
section 10
id pdf2json/2021.acl-long.4.pdf.json
When O
reporting O
accuracy B-PARAMETER
in O
tables O
, O
we O
bolden O
the O
best O
performance O
across O
models O
and O
highlight O
performance O
below O
a B-PARAMETER
random O
choice O
baseline O
, O
i.e O
. O
section 10
id pdf2json/2021.acl-long.4.pdf.json
50 O
% O
for O
our O
binary O
task O
, O
in O
cursive O
red O
. O
section 10
id pdf2json/2021.acl-long.4.pdf.json
Performance O
Across O
Labels O
All O
models O
show O
clear O
performance O
deficits O
when O
tested O
on O
hateful O
and O
non-hateful O
cases O
in O
HATECHECK O
( O
Table O
2 O
) O
. O
section 10
id pdf2json/2021.acl-long.4.pdf.json
B-D O
, O
B-F O
and O
P O
are O
relatively O
more O
accurate O
on O
hateful O
cases O
but O
misclassify O
most O
non-hateful O
cases O
. O
section 10
id pdf2json/2021.acl-long.4.pdf.json
In O
total O
, O
P O
performs O
best O
. O
section 10
id pdf2json/2021.acl-long.4.pdf.json
SN O
performs O
worst O
and O
is O
strongly O
biased O
towards O
classifying O
all O
cases O
as O
non-hateful O
, O
making O
it O
highly O
accurate O
on O
nonhateful O
cases O
but O
misclassify O
most O
hateful O
cases O
. O
section 10
id pdf2json/2021.acl-long.4.pdf.json
Performance O
Across O
Functional O
Tests O
Evaluating O
models O
on O
each O
functional O
test O
( O
Table O
1 O
) O
reveals O
specific O
model O
weaknesses O
. O
section 10
id pdf2json/2021.acl-long.4.pdf.json
B-D O
and O
B-F O
, O
respectively O
, O
are O
less O
than O
50 O
% O
accurate O
on O
8 O
and O
4 O
out O
of O
the O
11 B-PARAMETER
functional O
tests O
for O
non-hate O
in O
HATECHECK O
. O
section 10
id pdf2json/2021.acl-long.4.pdf.json
In O
particular O
, O
the O
models O
misclassify O
most O
cases O
of O
reclaimed O
slurs O
( O
F9 O
, O
39.5 O
% O
and O
33.3 O
% O
correct O
) O
, O
negated O
hate O
( O
F15 O
, O
12.8 O
% O
and O
12.0 O
% O
correct O
) O
and O
counter O
speech O
( O
F20/21 O
, O
26.6 O
% O
/29.1 O
% O
and O
32.9 O
% O
/29.8 O
% O
correct O
) O
. O
section 10
id pdf2json/2021.acl-long.4.pdf.json
B-D O
is O
slightly O
more O
accurate O
than O
B-F O
on O
most O
functional O
tests O
for O
hate O
while O
B-F O
is O
more O
accurate O
on O
most O
tests O
for O
non-hate O
. O
section 10
id pdf2json/2021.acl-long.4.pdf.json
Both O
models O
generally O
do O
better O
on O
hateful O
than O
non-hateful O
cases O
, O
although O
they O
struggle O
, O
for O
instance O
, O
with O
spelling O
variations O
, O
particularly O
added O
spaces O
between O
characters O
( O
F28 O
, O
43.9 O
% O
and O
37.6 O
% O
correct O
) O
and O
leet O
speak O
spellings O
( O
F29 O
, O
48.0 O
% O
and O
43.9 O
% O
correct O
) O
. O
section 10
id pdf2json/2021.acl-long.4.pdf.json
P O
performs O
better O
than O
B-D O
and O
B-F O
on O
most O
functional O
tests O
. O
section 10
id pdf2json/2021.acl-long.4.pdf.json
It O
is O
over O
95 O
% O
accurate O
on O
11 B-PARAMETER
out O
of O
18 O
functional O
tests O
for O
hate O
and O
substantially O
more O
accurate O
than O
B-D O
and O
B-F O
on O
spelling O
variations O
( O
F25-29 O
) O
. O
section 10
id pdf2json/2021.acl-long.4.pdf.json
However O
, O
it O
performs O
even O
worse O
than O
B-D O
and O
B-F O
on O
non-hateful O
functional O
tests O
for O
reclaimed O
slurs O
( O
F9 O
, O
28.4 O
% O
correct O
) O
, O
negated O
hate O
( O
F15 O
, O
3.8 O
% O
correct O
) O
and O
counter O
speech O
( O
F20/21 O
, O
15.6 O
% O
/18.4 O
% O
correct O
) O
. O
section 10
id pdf2json/2021.acl-long.4.pdf.json
Due O
to O
its O
bias B-PARAMETER
towards O
classifying O
all O
cases O
as O
non-hateful O
, O
SN O
misclassifies O
most O
hateful O
cases O
and O
is O
near-perfectly O
accurate O
on O
non-hateful O
functional O
tests O
. O
section 10
id pdf2json/2021.acl-long.4.pdf.json
Exceptions O
to O
the O
latter O
are O
counter O
speech O
( O
F20/21 O
, O
79.8 O
% O
/79.4 O
% O
correct O
) O
and O
nonhateful O
slur O
usage O
( O
F8/9 O
, O
33.3 O
% O
/18.5 O
% O
correct O
) O
. O
section 10
id pdf2json/2021.acl-long.4.pdf.json
Performance O
on O
Individual O
Functional O
Tests O
Individual O
functional O
tests O
can O
be O
investigated O
further O
to O
show O
more O
granular O
model O
weaknesses O
. O
section 10
id pdf2json/2021.acl-long.4.pdf.json
To O
illustrate O
, O
Table O
3 O
reports O
model O
accuracy B-PARAMETER
on O
test O
cases O
for O
non-hateful O
reclaimed O
slurs O
( O
F9 O
) O
grouped O
by O
the O
reclaimed O
slur O
that O
is O
used O
. O
section 10
id pdf2json/2021.acl-long.4.pdf.json
claimed O
slurs O
( O
F9 O
, O
non-hateful O
) O
by O
which O
slur O
is O
used O
. O
section 10
id pdf2json/2021.acl-long.4.pdf.json
Performance O
varies O
across O
models O
and O
is O
strikingly O
poor O
on O
individual O
slurs O
. O
section 10
id pdf2json/2021.acl-long.4.pdf.json
B-Dmisclassifies O
all O
instances O
of O
“ O
f*g O
” O
, O
“ O
f*ggot O
” O
and O
“ O
q*eer O
” O
. O
section 10
id pdf2json/2021.acl-long.4.pdf.json
B-F O
and O
P O
perform O
better O
for O
“ O
q*eer O
” O
, O
but O
fail O
on O
“ O
n*gga O
” O
. O
section 10
id pdf2json/2021.acl-long.4.pdf.json
SN O
fails O
on O
all O
cases O
but O
reclaimed O
uses O
of O
“ O
b*tch O
” O
. O
section 10
id pdf2json/2021.acl-long.4.pdf.json
Performance O
Across O
Target O
Groups O
HATECHECK O
can O
test O
whether O
models O
exhibit O
‘ O
unintended O
biases O
’ O
( O
Dixon B-AUTHOR
et O
al. O
, O
2018 O
) O
by O
comparing O
their O
performance O
on O
cases O
which O
target B-PARAMETER
different O
groups O
. O
section 10
id pdf2json/2021.acl-long.4.pdf.json
To O
illustrate O
, O
Table O
4 O
shows O
model O
accuracy B-PARAMETER
on O
all O
test O
cases O
created O
from O
[ O
IDENTITY O
] O
templates O
, O
which O
only O
differ O
in O
the O
group O
identifier O
. O
section 10
id pdf2json/2021.acl-long.4.pdf.json
B-D O
misclassifies O
test O
cases O
targeting O
women O
twice O
as O
often O
as O
those O
targeted O
at O
other O
groups O
. O
section 10
id pdf2json/2021.acl-long.4.pdf.json
B-F O
also O
performs O
relatively O
worse O
for O
women O
and O
fails O
on O
most O
test O
cases O
targeting O
disabled O
people O
. O
section 10
id pdf2json/2021.acl-long.4.pdf.json
By O
contrast O
, O
P O
is O
consistently O
around O
80 O
% O
and O
SN O
around O
25 O
% O
accurate O
across O
target B-PARAMETER
groups O
. O

section 11
id pdf2json/2021.acl-long.4.pdf.json
HATECHECK O
reveals O
functional O
weaknesses O
in O
all O
four O
models O
that O
we O
test O
. O
section 11
id pdf2json/2021.acl-long.4.pdf.json
First O
, O
all O
models O
are O
overly O
sensitive O
to O
specific O
keywords O
in O
at O
least O
some O
contexts O
. O
section 11
id pdf2json/2021.acl-long.4.pdf.json
B-D O
, O
B-F O
and O
P O
perform O
well O
for O
both O
hateful O
and O
non-hateful O
cases O
of O
profanity O
( O
F10/11 O
) O
, O
which O
shows O
that O
they O
can O
distinguish O
between O
different O
uses O
of O
certain O
profanity O
terms O
. O
section 11
id pdf2json/2021.acl-long.4.pdf.json
However O
, O
all O
models O
perform O
very O
poorly O
on O
reclaimed O
slurs O
( O
F9 O
) O
compared O
to O
hateful O
slurs O
( O
F7 O
) O
. O
section 11
id pdf2json/2021.acl-long.4.pdf.json
Thus O
, O
it O
appears O
that O
the O
models O
to O
some O
extent O
encode O
overly O
simplistic O
keyword-based O
decision O
rules O
( O
e.g O
. O
section 11
id pdf2json/2021.acl-long.4.pdf.json
that O
slurs O
are O
hateful O
) O
rather O
than O
capturing O
the O
relevant O
linguistic O
phenomena O
( O
e.g O
. O
section 11
id pdf2json/2021.acl-long.4.pdf.json
that O
slurs O
can O
have O
non-hateful O
reclaimed O
uses O
) O
. O
section 11
id pdf2json/2021.acl-long.4.pdf.json
Second O
, O
B-D O
, O
B-F O
and O
P O
struggle O
with O
nonhateful O
contrasts O
to O
hateful O
phrases O
. O
section 11
id pdf2json/2021.acl-long.4.pdf.json
In O
particular O
, O
they O
misclassify O
most O
cases O
of O
negated O
hate O
( O
F15 O
) O
and O
counter O
speech O
( O
F20/21 O
) O
. O
section 11
id pdf2json/2021.acl-long.4.pdf.json
Thus O
, O
they O
appear O
to O
not O
sufficiently O
register O
linguistic O
signals O
that O
reframe O
hateful O
phrases O
into O
clearly O
non-hateful O
ones O
( O
e.g O
. O
section 11
id pdf2json/2021.acl-long.4.pdf.json
“ O
No O
Muslim O
deserves O
to O
die O
” O
) O
. O
section 11
id pdf2json/2021.acl-long.4.pdf.json
Third O
, O
B-D O
and O
B-F O
are O
biased O
in O
their O
target B-PARAMETER
coverage O
, O
classifying O
hate O
directed O
against O
some O
protected O
groups O
( O
e.g O
. O
section 11
id pdf2json/2021.acl-long.4.pdf.json
women O
) O
less O
accurately O
than O
equivalent O
cases O
directed O
at O
others O
( O
Table O
4 O
) O
. O
section 11
id pdf2json/2021.acl-long.4.pdf.json
For O
practical O
applications O
such O
as O
content O
moderation O
, O
these O
are O
critical O
weaknesses O
. O
section 11
id pdf2json/2021.acl-long.4.pdf.json
Models O
that O
misclassify O
reclaimed O
slurs O
penalise O
the O
very O
communities O
that O
are O
commonly O
targeted O
by O
hate O
speech O
. O
section 11
id pdf2json/2021.acl-long.4.pdf.json
Models O
that O
misclassify O
counter O
speech O
undermine O
positive O
efforts O
to O
fight O
hate O
speech O
. O
section 11
id pdf2json/2021.acl-long.4.pdf.json
Models O
that O
are O
biased O
in O
their O
target B-PARAMETER
coverage O
are O
likely O
to O
create O
and O
entrench O
biases O
in O
the O
protections O
afforded O
to O
different O
groups O
. O
section 11
id pdf2json/2021.acl-long.4.pdf.json
As O
a B-PARAMETER
suite O
of O
black-box O
tests O
, O
HATECHECK O
only O
offers O
indirect O
insights O
into O
the O
source O
of O
these O
weaknesses O
. O
section 11
id pdf2json/2021.acl-long.4.pdf.json
Poor O
performance O
on O
functional O
tests O
can O
be O
a B-PARAMETER
consequence O
of O
systematic O
gaps O
and O
biases O
in O
model O
training O
data O
. O
section 11
id pdf2json/2021.acl-long.4.pdf.json
It O
can O
also O
indicate O
a B-PARAMETER
more O
fundamental O
inability O
of O
the O
model O
’ O
s O
architecture O
to O
capture O
relevant O
linguistic O
phenomena O
. O
section 11
id pdf2json/2021.acl-long.4.pdf.json
B-D O
and O
B-F O
share O
the O
same O
architecture O
but O
differ O
in O
performance O
on O
functional O
tests O
and O
in O
target B-PARAMETER
coverage O
. O
section 11
id pdf2json/2021.acl-long.4.pdf.json
This O
reflects O
the O
importance O
of O
training O
data O
composition O
, O
which O
previous O
hate O
speech O
research O
has O
emphasised O
( O
Wiegand B-AUTHOR
et O
al. O
, O
2019 O
; O
Nejadgholi B-AUTHOR
and O
Kiritchenko B-AUTHOR
, O
2020 O
) O
. O
section 11
id pdf2json/2021.acl-long.4.pdf.json
Future O
work O
could O
investigate O
the O
provenance O
of O
model O
weaknesses O
in O
more O
detail O
, O
for O
instance O
by O
using O
test O
cases O
from O
HATECHECK O
to O
“ O
inoculate O
” O
training O
data O
( O
Liu B-AUTHOR
et O
al. O
, O
2019 O
) O
. O
section 11
id pdf2json/2021.acl-long.4.pdf.json
If O
poor O
model O
performance O
does O
stem O
from O
biased O
training O
data O
, O
models O
could O
be O
improved O
through O
targeted O
data B-METHOD
augmentation I-METHOD
( O
Gardner B-AUTHOR
et O
al. O
, O
2020 O
) O
. O
section 11
id pdf2json/2021.acl-long.4.pdf.json
HATECHECK O
users O
could O
, O
for O
instance O
, O
sample O
or O
construct O
additional O
training O
cases O
to O
resemble O
test O
cases O
from O
functional O
tests O
that O
their O
model O
was O
inaccurate O
on O
, O
bearing O
in O
mind O
that O
this O
additional O
data O
might O
introduce O
other O
unforeseen O
biases O
. O
section 11
id pdf2json/2021.acl-long.4.pdf.json
The O
models O
we O
tested O
would O
likely O
benefit O
from O
training O
on O
additional O
cases O
of O
negated O
hate O
, O
reclaimed O
slurs O
and O
counter O
speech O
. O


section 13
id pdf2json/2021.acl-long.4.pdf.json
Good O
performance O
on O
a B-PARAMETER
functional O
test O
in O
HATECHECK O
only O
reveals O
the O
absence O
of O
a B-PARAMETER
particular O
weakness O
, O
rather O
than O
necessarily O
characterising O
a B-PARAMETER
generalisable O
model O
strength O
. O
section 13
id pdf2json/2021.acl-long.4.pdf.json
This O
negative O
predictive O
power O
( O
Gardner B-AUTHOR
et O
al. O
, O
2020 O
) O
is O
common O
, O
to O
some O
extent O
, O
to O
all O
finite O
test O
sets O
. O
section 13
id pdf2json/2021.acl-long.4.pdf.json
Thus O
, O
claims O
about O
model O
quality O
should O
not O
be O
overextended O
based O
on O
positive O
HATECHECK O
results O
. O
section 13
id pdf2json/2021.acl-long.4.pdf.json
In O
model O
development O
, O
HATECHECK O
offers O
targeted O
diagnostic O
insights O
as O
a B-PARAMETER
complement O
to O
rather O
than O
a B-PARAMETER
substitute O
for O
evaluation O
on O
held-out O
test O
sets O
of O
real-world O
hate O
speech O
. O

section 14
id pdf2json/2021.acl-long.4.pdf.json
Each O
test O
case O
in O
HATECHECK O
is O
a B-PARAMETER
separate O
English-language O
text O
document O
. O
section 14
id pdf2json/2021.acl-long.4.pdf.json
Thus O
, O
HATECHECK O
does O
not O
test O
functionalities O
related O
to O
context O
outside O
individual O
documents O
, O
modalities O
other O
than O
text O
or O
languages O
other O
than O
English O
. O
section 14
id pdf2json/2021.acl-long.4.pdf.json
Future O
research O
could O
expand O
HATECHECK O
to O
include O
functional O
tests O
covering O
such O
aspects O
. O
section 14
id pdf2json/2021.acl-long.4.pdf.json
Functional O
tests O
in O
HATECHECK O
cover O
distinct O
expressions O
of O
hate O
and O
non-hate O
. O
section 14
id pdf2json/2021.acl-long.4.pdf.json
Future O
work O
could O
test O
more O
complex O
compound O
statements O
, O
such O
as O
cases O
combining O
slurs O
and O
profanity O
. O
section 14
id pdf2json/2021.acl-long.4.pdf.json
Further O
, O
HATECHECK O
is O
static O
and O
thus O
does O
not O
test O
functionalities O
related O
to O
language O
change O
. O
section 14
id pdf2json/2021.acl-long.4.pdf.json
This O
could O
be O
addressed O
by O
“ O
live O
” O
datasets O
, O
such O
as O
dynamic O
adversarial O
benchmarks O
( O
Nie B-AUTHOR
et O
al. O
, O
2020 O
; O
Vidgen B-AUTHOR
et O
al. O
, O
2020b O
; O
Kiela B-AUTHOR
et O
al. O
, O
2021 O
) O
. O

section 15
id pdf2json/2021.acl-long.4.pdf.json
Future O
research O
could O
expand O
HATECHECK O
to O
cover O
additional O
protected O
groups O
. O
section 15
id pdf2json/2021.acl-long.4.pdf.json
We O
also O
suggest O
the O
addition O
of O
intersectional O
characteristics O
, O
which O
interviewees O
highlighted O
as O
a B-PARAMETER
neglected O
dimension O
of O
online O
hate O
( O
e.g O
. O
section 15
id pdf2json/2021.acl-long.4.pdf.json
I17 O
: O
“ O
As O
a B-PARAMETER
black O
woman O
, O
I O
receive O
abuse O
that O
is O
racialised O
and O
gendered O
” O
) O
. O
section 15
id pdf2json/2021.acl-long.4.pdf.json
Similarly O
, O
future O
research O
could O
include O
hateful O
slurs O
beyond O
those O
covered O
by O
HATECHECK O
. O
section 15
id pdf2json/2021.acl-long.4.pdf.json
Lastly O
, O
future O
research O
could O
craft O
test O
cases O
using O
more O
platform- O
or O
community-specific O
language O
than O
HATECHECK O
’ O
s O
more O
general O
test O
cases O
. O
section 15
id pdf2json/2021.acl-long.4.pdf.json
It O
could O
also O
test O
hate O
that O
is O
more O
specific O
to O
particular O
target B-PARAMETER
groups O
, O
such O
as O
misogynistic O
tropes O
. O

section 16
id pdf2json/2021.acl-long.4.pdf.json
Targeted O
diagnostic O
datasets O
like O
the O
sets O
of O
test O
cases O
in O
HATECHECK O
have O
been O
used O
for O
model O
evaluation O
across O
a B-PARAMETER
wide O
range O
of O
NLP O
tasks O
, O
such O
as O
natural O
language O
inference O
( O
Naik B-AUTHOR
et O
al. O
, O
2018 O
; O
McCoy B-AUTHOR
et O
al. O
, O
2019 O
) O
, O
machine O
translation O
( O
Isabelle B-AUTHOR
et O
al. O
, O
2017 O
; O
Belinkov B-AUTHOR
and O
Bisk B-AUTHOR
, O
2018 O
) O
and O
language O
modelling O
( O
Marvin B-AUTHOR
and O
Linzen B-AUTHOR
, O
2018 O
; O
Ettinger O
, O
2020 O
) O
. O
section 16
id pdf2json/2021.acl-long.4.pdf.json
For O
hate O
speech O
detection O
, O
however O
, O
they O
have O
seen O
very O
limited O
use O
. O
section 16
id pdf2json/2021.acl-long.4.pdf.json
Palmer B-AUTHOR
et O
al O
. O
section 16
id pdf2json/2021.acl-long.4.pdf.json
( O
2020 O
) O
compile O
three O
datasets O
for O
evaluating O
model O
performance O
on O
what O
they O
call O
complex O
offensive O
language O
, O
specifically O
the O
use O
of O
reclaimed O
slurs O
, O
adjective O
nominalisation O
and O
linguistic O
distancing O
. O
section 16
id pdf2json/2021.acl-long.4.pdf.json
They O
select O
test O
cases O
from O
other O
datasets O
sampled O
from O
social O
media O
, O
which O
introduces O
substantial O
disagreement O
between O
annotators O
on O
labels O
in O
their O
data O
. O
section 16
id pdf2json/2021.acl-long.4.pdf.json
Dixon B-AUTHOR
et O
al O
. O
section 16
id pdf2json/2021.acl-long.4.pdf.json
( O
2018 O
) O
use O
templates O
to O
generate O
synthetic O
sets O
of O
toxic O
and O
non-toxic O
cases O
, O
which O
resembles O
our O
method O
for O
test O
case O
creation O
. O
section 16
id pdf2json/2021.acl-long.4.pdf.json
They O
focus O
primarily O
on O
evaluating O
biases O
around O
the O
use O
of O
group O
identifiers O
and O
do O
not O
validate O
the O
labels O
in O
their O
dataset O
. O
section 16
id pdf2json/2021.acl-long.4.pdf.json
Compared O
to O
both O
approaches O
, O
HATECHECK O
covers O
a B-PARAMETER
much O
larger O
range O
of O
model O
functionalities O
, O
and O
all O
test O
cases O
, O
which O
we O
generated O
specifically O
to O
fit O
a B-PARAMETER
given O
functionality O
, O
have O
clear O
gold B-METRIC
standard I-METRIC
labels I-METRIC
, O
which O
are O
validated O
by O
near-perfect O
agreement O
between O
annotators O
. O
section 16
id pdf2json/2021.acl-long.4.pdf.json
In O
its O
use O
of O
contrastive O
cases O
for O
model O
eval- O
uation O
, O
HATECHECK O
builds O
on O
a B-PARAMETER
long O
history O
of O
minimally-contrastive O
pairs O
in O
NLP O
( O
e.g O
. O
section 16
id pdf2json/2021.acl-long.4.pdf.json
Levesque B-AUTHOR
et O
al. O
, O
2012 O
; O
Sennrich B-AUTHOR
, O
2017 O
; O
Glockner B-AUTHOR
et O
al. O
, O
2018 O
; O
Warstadt B-AUTHOR
et O
al. O
, O
2020 O
) O
. O
section 16
id pdf2json/2021.acl-long.4.pdf.json
Most O
relevantly O
, O
Kaushik B-AUTHOR
et O
al O
. O
section 16
id pdf2json/2021.acl-long.4.pdf.json
( O
2020 O
) O
and O
Gardner B-AUTHOR
et O
al O
. O
section 16
id pdf2json/2021.acl-long.4.pdf.json
( O
2020 O
) O
propose O
augmenting O
NLP O
datasets O
with O
contrastive O
cases O
for O
training O
more O
generalisable O
models O
and O
enabling O
more O
meaningful O
evaluation O
. O
section 16
id pdf2json/2021.acl-long.4.pdf.json
We O
built O
on O
their O
approaches O
to O
generate O
non-hateful O
contrast O
cases O
in O
our O
test O
suite O
, O
which O
is O
the O
first O
application O
of O
this O
kind O
for O
hate O
speech O
detection O
. O
section 16
id pdf2json/2021.acl-long.4.pdf.json
In O
terms O
of O
its O
structure O
, O
HATECHECK O
is O
most O
directly O
influenced O
by O
the O
CHECKLIST O
framework O
proposed O
by O
Ribeiro B-AUTHOR
et O
al O
. O
section 16
id pdf2json/2021.acl-long.4.pdf.json
( O
2020 O
) O
. O
section 16
id pdf2json/2021.acl-long.4.pdf.json
However O
, O
while O
they O
focus O
on O
demonstrating O
its O
general O
applicability O
across O
NLP O
tasks O
, O
we O
put O
more O
emphasis O
on O
motivating O
the O
selection O
of O
functional O
tests O
as O
well O
as O
constructing O
and O
validating O
targeted O
test O
cases O
specifically O
for O
the O
task O
of O
hate O
speech O
detection O
. O

section 17
id pdf2json/2021.acl-long.4.pdf.json
In O
this O
article O
, O
we O
introduced O
HATECHECK O
, O
a B-PARAMETER
suite O
of O
functional O
tests O
for O
hate O
speech O
detection O
models O
. O
section 17
id pdf2json/2021.acl-long.4.pdf.json
We O
motivated O
the O
selection O
of O
functional O
tests O
through O
interviews O
with O
civil O
society O
stakeholders O
and O
a B-PARAMETER
review O
of O
previous O
hate O
speech O
research O
, O
which O
grounds O
our O
approach O
in O
both O
practical O
and O
academic O
applications O
of O
hate O
speech O
detection O
models O
. O
section 17
id pdf2json/2021.acl-long.4.pdf.json
We O
designed O
the O
functional O
tests O
to O
offer O
contrasts O
between O
hateful O
and O
non-hateful O
content O
that O
are O
challenging O
to O
detection O
models O
, O
which O
enables O
more O
accurate O
evaluation O
of O
their O
true O
functionalities O
. O
section 17
id pdf2json/2021.acl-long.4.pdf.json
For O
each O
functional O
test O
, O
we O
crafted O
sets O
of O
targeted O
test O
cases O
with O
clear O
gold B-METRIC
standard I-METRIC
labels I-METRIC
, O
which O
we O
validated O
through O
a B-PARAMETER
structured O
annotation O
process O
. O
section 17
id pdf2json/2021.acl-long.4.pdf.json
We O
demonstrated O
the O
utility O
of O
HATECHECK O
as O
a B-PARAMETER
diagnostic O
tool O
by O
testing O
near-state-of-the-art O
transformer B-METHOD
models O
as O
well O
as O
two O
commercial O
models O
for O
hate O
speech O
detection O
. O
section 17
id pdf2json/2021.acl-long.4.pdf.json
HATECHECK O
showed O
critical O
weaknesses O
for O
all O
models O
. O
section 17
id pdf2json/2021.acl-long.4.pdf.json
Specifically O
, O
models O
appeared O
overly O
sensitive O
to O
particular O
keywords O
and O
phrases O
, O
as O
evidenced O
by O
poor O
performance O
on O
tests O
for O
reclaimed O
slurs O
, O
counter O
speech O
and O
negated O
hate O
. O
section 17
id pdf2json/2021.acl-long.4.pdf.json
The O
transformer B-METHOD
models O
also O
exhibited O
strong O
biases O
in O
target B-PARAMETER
coverage O
. O
section 17
id pdf2json/2021.acl-long.4.pdf.json
Online O
hate O
is O
a B-PARAMETER
deeply O
harmful O
phenomenon O
, O
and O
detection O
models O
are O
integral O
to O
tackling O
it O
. O
section 17
id pdf2json/2021.acl-long.4.pdf.json
Typically O
, O
models O
have O
been O
evaluated O
on O
held-out O
test O
data O
, O
which O
has O
made O
it O
difficult O
to O
assess O
their O
generalisability O
and O
identify O
specific O
weaknesses O
. O
section 17
id pdf2json/2021.acl-long.4.pdf.json
We O
hope O
that O
HATECHECK O
’ O
s O
targeted O
diagnostic O
insights O
help O
address O
this O
issue O
by O
contributing O
to O
our O
understanding O
of O
models O
’ O
limitations O
, O
thus O
aiding O
the O
development O
of O
better O
models O
in O
the O
future O
. O

section 18
id pdf2json/2021.acl-long.4.pdf.json
We O
thank O
all O
interviewees O
for O
their O
participation O
. O
section 18
id pdf2json/2021.acl-long.4.pdf.json
We O
also O
thank O
reviewers O
for O
their O
constructive O
feedback O
. O
section 18
id pdf2json/2021.acl-long.4.pdf.json
Paul B-AUTHOR
Röttger O
was O
funded O
by O
the O
German O
Academic O
Scholarship O
Foundation O
. O
section 18
id pdf2json/2021.acl-long.4.pdf.json
Bertram B-AUTHOR
Vidgen B-AUTHOR
and O
Helen B-AUTHOR
Margetts I-AUTHOR
were O
supported O
by O
Wave O
1 O
of O
The O
UKRI O
Strategic O
Priorities O
Fund O
under O
the O
EPSRC O
Grant O
EP/T001569/1 O
, O
particularly O
the O
“ O
Criminal O
Justice O
System O
” O
theme O
within O
that O
grant O
, O
and O
the O
“ O
Hate O
Speech O
: O
Measures O
& O
Counter-Measures O
” O
project O
at O
The O
Alan O
Turing O
Institute O
. O
section 18
id pdf2json/2021.acl-long.4.pdf.json
Dong B-AUTHOR
Nguyen B-AUTHOR
was O
supported O
by O
the O
“ O
Digital O
Society O
- O
The O
Informed O
Citizen O
” O
research O
programme O
, O
which O
is O
( O
partly O
) O
financed O
by O
the O
Dutch O
Research O
Council O
( O
NWO O
) O
, O
project O
410.19.007 O
. O
section 18
id pdf2json/2021.acl-long.4.pdf.json
Zeerak B-AUTHOR
Waseem I-AUTHOR
was O
supported O
in O
part O
by O
the O
Canada O
150 O
Research O
Chair O
program O
and O
the O
UK-Canada O
AI O
Artificial O
Intelligence O
Initiative O
. O
section 18
id pdf2json/2021.acl-long.4.pdf.json
Janet B-AUTHOR
B. I-AUTHOR
Pierrehumbert I-AUTHOR
was O
supported O
by O
EPSRC O
Grant O
EP/T023333/1 O
. O
section 18
id pdf2json/2021.acl-long.4.pdf.json
Impact O
Statement O
This O
supplementary O
section O
addresses O
relevant O
ethical O
considerations O
that O
were O
not O
explicitly O
discussed O
in O
the O
main O
body O
of O
our O
article O
. O
section 18
id pdf2json/2021.acl-long.4.pdf.json
Interview O
Participant O
Rights O
All O
interviewees O
gave O
explicit O
consent O
for O
their O
participation O
after O
being O
informed O
in O
detail O
about O
the O
research O
use O
of O
their O
responses O
. O
section 18
id pdf2json/2021.acl-long.4.pdf.json
In O
all O
research O
output O
, O
quotes O
from O
interview O
responses O
were O
anonymised O
. O
section 18
id pdf2json/2021.acl-long.4.pdf.json
We O
also O
did O
not O
reveal O
specific O
participant O
demographics O
or O
affiliations O
. O
section 18
id pdf2json/2021.acl-long.4.pdf.json
Our O
interview O
approach O
was O
approved O
by O
the O
Alan O
Turing O
Institute O
’ O
s O
Ethics O
Review O
Board O
. O
section 18
id pdf2json/2021.acl-long.4.pdf.json
Intellectual O
Property O
Rights O
The O
test O
cases O
in O
HATECHECK O
were O
crafted O
by O
the O
authors O
. O
section 18
id pdf2json/2021.acl-long.4.pdf.json
As O
synthetic O
data O
, O
they O
pose O
no O
risk O
of O
violating O
intellectual O
property O
rights O
. O
section 18
id pdf2json/2021.acl-long.4.pdf.json
Annotator O
Compensation O
We O
employed O
a B-PARAMETER
team O
of O
ten O
annotators O
to O
validate O
the O
quality O
of O
the O
HATECHECK B-DATASET
dataset I-DATASET
. O
section 18
id pdf2json/2021.acl-long.4.pdf.json
Annotators O
were O
compensated O
at O
a B-PARAMETER
rate O
of O
£16 O
per O
hour O
. O
section 18
id pdf2json/2021.acl-long.4.pdf.json
The O
rate O
was O
set O
50 O
% O
above O
the O
local O
living O
wage O
( O
£10.85 O
) O
, O
although O
all O
work O
was O
completed O
remotely O
. O
section 18
id pdf2json/2021.acl-long.4.pdf.json
All O
training O
time O
and O
meetings O
were O
paid O
. O
section 18
id pdf2json/2021.acl-long.4.pdf.json
Intended O
Use O
HATECHECK O
’ O
s O
intended O
use O
is O
as O
an O
evaluative O
tool O
for O
hate O
speech O
detection O
models O
, O
providing O
structured O
and O
targeted O
diagnostic O
insights O
into O
model O
functionalities O
. O
section 18
id pdf2json/2021.acl-long.4.pdf.json
We O
demonstrated O
this O
use O
of O
HATECHECK O
in O
§3 O
. O
section 18
id pdf2json/2021.acl-long.4.pdf.json
We O
also O
briefly O
discussed O
alternative O
uses O
of O
HATECHECK O
, O
e.g O
. O
section 18
id pdf2json/2021.acl-long.4.pdf.json
as O
a B-PARAMETER
starting O
point O
for O
data B-METHOD
augmentation I-METHOD
. O
section 18
id pdf2json/2021.acl-long.4.pdf.json
These O
uses O
aim O
at O
aiding O
the O
development O
of O
better O
hate O
speech O
detection O
models O
. O
section 18
id pdf2json/2021.acl-long.4.pdf.json
Potential O
Misuse O
Researchers O
might O
overextend O
claims O
about O
the O
functionalities O
of O
their O
models O
based O
on O
their O
test O
performance O
, O
which O
we O
would O
consider O
a B-PARAMETER
misuse O
of O
HATECHECK O
. O
section 18
id pdf2json/2021.acl-long.4.pdf.json
We O
directly O
addressed O
this O
concern O
by O
highlighting O
HATECHECK O
’ O
s O
negative O
predictive O
power O
, O
i.e O
. O
section 18
id pdf2json/2021.acl-long.4.pdf.json
the O
fact O
that O
it O
primarily O
reveals O
model O
weaknesses O
rather O
than O
necessarily O
characterising O
generalisable O
model O
strengths O
, O
as O
one O
of O
its O
limitations O
. O
section 18
id pdf2json/2021.acl-long.4.pdf.json
For O
the O
same O
reason O
, O
we O
emphasised O
the O
limits O
to O
HATECHECK O
’ O
s O
coverage O
, O
e.g O
. O
section 18
id pdf2json/2021.acl-long.4.pdf.json
in O
terms O
of O
slurs O
and O
identity O
terms O
. O

section 19
id pdf2json/2021.acl-long.4.pdf.json
Following O
Bender B-AUTHOR
and O
Friedman B-AUTHOR
( O
2018 O
) O
, O
we O
provide O
a B-PARAMETER
data O
statement O
, O
which O
documents O
the O
generation O
and O
provenance O
of O
test O
cases O
in O
HATECHECK O
. O
section 19
id pdf2json/2021.acl-long.4.pdf.json
A. O
CURATION O
RATIONALE O
In O
order O
to O
construct O
HATECHECK O
, O
a B-PARAMETER
first O
suite O
of O
functional O
tests O
for O
hate O
speech O
detection O
models O
, O
we O
generated O
3,901 O
short O
English-language O
text O
documents O
by O
hand O
and O
by O
using O
simple O
templates O
for O
group O
identifiers O
and O
slurs O
( O
§2.4 O
) O
. O
section 19
id pdf2json/2021.acl-long.4.pdf.json
Each O
document O
corresponds O
to O
one O
functional O
test O
and O
a B-PARAMETER
binary O
gold O
standard O
label O
( O
hateful O
or O
non-hateful O
) O
. O
section 19
id pdf2json/2021.acl-long.4.pdf.json
In O
order O
to O
validate O
the O
gold B-METRIC
standard I-METRIC
labels I-METRIC
, O
we O
trained O
a B-PARAMETER
team O
of O
ten O
annotators O
, O
assigning O
five O
of O
them O
to O
each O
document O
, O
and O
asked O
them O
to O
provide O
independent O
labels O
( O
§2.5 O
) O
. O
section 19
id pdf2json/2021.acl-long.4.pdf.json
To O
further O
improve O
data O
quality O
, O
we O
also O
gave O
annotators O
the O
option O
to O
flag O
cases O
they O
felt O
were O
unrealistic O
( O
e.g O
. O
section 19
id pdf2json/2021.acl-long.4.pdf.json
nonsensical O
) O
, O
but O
this O
flag O
was O
not O
used O
for O
any O
one O
HATECHECK O
case O
by O
more O
than O
one O
annotator O
. O
section 19
id pdf2json/2021.acl-long.4.pdf.json
B O
. O
section 19
id pdf2json/2021.acl-long.4.pdf.json
LANGUAGE O
VARIETY O
HATECHECK O
only O
covers O
English-language O
text O
documents O
. O
section 19
id pdf2json/2021.acl-long.4.pdf.json
We O
opted O
for O
English O
language O
since O
this O
maximises O
HATECHECK O
’ O
s O
relevance O
to O
previous O
and O
current O
work O
in O
hate O
speech O
detection O
, O
which O
is O
mostly O
concerned O
with O
English-language O
data O
. O
section 19
id pdf2json/2021.acl-long.4.pdf.json
Our O
language O
choice O
also O
reflects O
the O
expertise O
of O
authors O
and O
annotators O
. O
section 19
id pdf2json/2021.acl-long.4.pdf.json
We O
discuss O
the O
lack O
of O
language O
variety O
as O
a B-PARAMETER
limitation O
of O
HATECHECK O
in O
§4.2 O
and O
suggest O
expansion O
to O
other O
languages O
as O
a B-PARAMETER
priority O
for O
future O
research O
. O
section 19
id pdf2json/2021.acl-long.4.pdf.json
C. O
SPEAKER O
DEMOGRAPHICS O
Since O
all O
test O
cases O
in O
HATECHECK O
were O
hand-crafted O
, O
the O
speakers O
are O
the O
same O
as O
the O
authors O
. O
section 19
id pdf2json/2021.acl-long.4.pdf.json
Test O
cases O
in O
the O
test O
suite O
were O
primarily O
generated O
by O
the O
lead O
author O
, O
who O
is O
a B-PARAMETER
researcher O
at O
a B-PARAMETER
UK O
university O
. O
section 19
id pdf2json/2021.acl-long.4.pdf.json
The O
lead O
author O
is O
not O
a B-PARAMETER
native O
English O
speaker O
but O
has O
lived O
in O
English-speaking O
countries O
for O
more O
than O
five O
years O
and O
has O
extensively O
engaged O
with O
English-language O
hate O
speech O
in O
previous O
research O
. O
section 19
id pdf2json/2021.acl-long.4.pdf.json
All O
test O
cases O
were O
also O
reviewed O
by O
two O
co-authors O
, O
both O
of O
whom O
have O
worked O
with O
English-language O
hate O
speech O
data O
for O
more O
than O
five O
years O
and O
one O
of O
whom O
is O
a B-PARAMETER
native O
English O
speaker O
from O
the O
UK O
. O
section 19
id pdf2json/2021.acl-long.4.pdf.json
D. O
ANNOTATOR O
DEMOGRAPHICS O
We O
recruited O
a B-PARAMETER
team O
of O
ten O
annotators O
to O
work O
for O
two O
weeks O
. O
section 19
id pdf2json/2021.acl-long.4.pdf.json
30 O
% O
were O
male O
and O
70 O
% O
were O
female O
. O
section 19
id pdf2json/2021.acl-long.4.pdf.json
60 O
% O
were O
18-29 O
and O
40 O
% O
were O
30-39 O
. O
section 19
id pdf2json/2021.acl-long.4.pdf.json
20 O
% O
were O
educated O
to O
high O
school O
level O
, O
10 O
% O
to O
undergraduate O
, O
60 O
% O
to O
taught O
masters O
and O
10 O
% O
to O
research O
degree O
( O
i.e O
. O
section 19
id pdf2json/2021.acl-long.4.pdf.json
PhD O
) O
. O
section 19
id pdf2json/2021.acl-long.4.pdf.json
70 O
% O
were O
native O
English O
speakers O
and O
30 O
% O
were O
non-native O
but O
fluent O
. O
section 19
id pdf2json/2021.acl-long.4.pdf.json
Annotators O
had O
a B-PARAMETER
range O
of O
nationalities O
: O
60 O
% O
were O
British O
and O
10 O
% O
each O
were O
Polish O
, O
Spanish O
, O
Argentinian O
and O
Irish O
. O
section 19
id pdf2json/2021.acl-long.4.pdf.json
Most O
annotators O
identified O
as O
ethnically O
White O
( O
70 O
% O
) O
, O
followed O
by O
Middle O
Eastern O
( O
20 O
% O
) O
and O
a B-PARAMETER
mixed O
ethnic O
background O
( O
10 O
% O
) O
. O
section 19
id pdf2json/2021.acl-long.4.pdf.json
Annotators O
all O
used O
social O
media O
regularly O
, O
and O
60 O
% O
used O
it O
more O
than O
once O
per O
day O
. O
section 19
id pdf2json/2021.acl-long.4.pdf.json
All O
annotators O
had O
seen O
other O
people O
targeted O
by O
online O
abuse O
before O
, O
and O
80 O
% O
had O
been O
targeted O
personally O
. O
section 19
id pdf2json/2021.acl-long.4.pdf.json
All O
annotators O
had O
previously O
completed O
annotation O
work O
on O
at O
least O
one O
other O
hate O
speech O
dataset O
. O
section 19
id pdf2json/2021.acl-long.4.pdf.json
In O
the O
first O
week O
, O
we O
introduced O
the O
binary O
annotation O
task O
to O
them O
in O
an O
onboarding O
session O
and O
tested O
their O
understanding O
on O
a B-PARAMETER
set O
of O
100 O
cases O
, O
which O
we O
then O
provided O
individual O
feedback O
on O
. O
section 19
id pdf2json/2021.acl-long.4.pdf.json
In O
the O
second O
week O
, O
we O
asked O
each O
annotator O
to O
annotate O
around O
2,000 O
test O
cases O
so B-PARAMETER
that O
each O
case O
in O
our O
test O
suite O
was O
annotated O
by O
varied O
sets O
of O
exactly O
five O
annotators O
. O
section 19
id pdf2json/2021.acl-long.4.pdf.json
Throughout O
the O
process O
, O
we O
communicated O
with O
annotators O
in O
real-time O
over O
a B-PARAMETER
messaging O
platform O
. O
section 19
id pdf2json/2021.acl-long.4.pdf.json
We O
also O
followed O
guidance O
for O
protecting O
and O
monitoring O
annotator O
well-being O
provided O
by O
Vidgen B-AUTHOR
et O
al O
. O
section 19
id pdf2json/2021.acl-long.4.pdf.json
( O
2019 O
) O
. O
section 19
id pdf2json/2021.acl-long.4.pdf.json
E. O
SPEECH O
SITUATION O
All O
test O
cases O
were O
created O
between O
the O
23rd O
of O
November O
and O
the O
13th O
of O
December O
2020 O
. O
section 19
id pdf2json/2021.acl-long.4.pdf.json
F. O
TEXT O
CHARACTERISTICS O
The O
composition O
of O
the O
dataset O
, O
including O
primary O
label O
and O
secondary O
labels O
, O
is O
described O
in O
detail O
in O
§2.3 O
and O
§2.4 O
of O
the O
article O
. O

section 20
id pdf2json/2021.acl-long.4.pdf.json
F1 B-METRIC
– O
strong O
negative O
emotions O
explicitly O
expressed O
about O
a B-PARAMETER
protected O
group O
or O
its O
members O
: O
Resembles O
“ O
expressed O
hatred O
” O
( O
Davidson B-AUTHOR
et O
al. O
, O
2017 O
) O
and O
“ O
identity O
attack O
” O
( O
Banko B-AUTHOR
et O
al. O
, O
2020 O
) O
. O
section 20
id pdf2json/2021.acl-long.4.pdf.json
F2 O
– O
explicit O
descriptions O
of O
a B-PARAMETER
protected O
group O
or O
its O
members O
using O
very O
negative O
attributes O
: O
Refines O
more O
general O
“ O
insult O
” O
categories O
( O
Davidson B-AUTHOR
et O
al. O
, O
2017 O
; O
Zampieri B-AUTHOR
et O
al. O
, O
2019 O
) O
. O
section 20
id pdf2json/2021.acl-long.4.pdf.json
F3 O
– O
explicit O
dehumanisation O
of O
a B-PARAMETER
protected O
group O
or O
its O
members O
: O
Prevalent O
form O
of O
hate O
( O
Mendelsohn B-AUTHOR
et O
al. O
, O
2020 O
; O
Banko B-AUTHOR
et O
al. O
, O
2020 O
; O
Vidgen B-AUTHOR
et O
al. O
, O
2020a O
) O
. O
section 20
id pdf2json/2021.acl-long.4.pdf.json
Highlighted O
in O
our O
interviews O
( O
e.g O
. O
section 20
id pdf2json/2021.acl-long.4.pdf.json
I18 O
: O
“ O
hate O
crime O
[ O
often O
claims O
] O
people O
are O
inferior O
and O
subhuman. O
” O
) O
. O
section 20
id pdf2json/2021.acl-long.4.pdf.json
F4 O
– O
implicit O
derogation O
of O
a B-PARAMETER
protected O
group O
or O
its O
members O
: O
Closely O
resembles O
“ O
implied O
bias B-PARAMETER
” O
( O
Sap B-AUTHOR
et O
al. O
, O
2020 O
) O
and O
“ O
implicit O
abuse O
” O
( O
Waseem B-AUTHOR
et O
al. O
, O
2017 O
; O
Zhang B-AUTHOR
and O
Luo B-AUTHOR
, O
2019 O
) O
. O
section 20
id pdf2json/2021.acl-long.4.pdf.json
Highlighted O
in O
our O
interviews O
( O
e.g O
. O
section 20
id pdf2json/2021.acl-long.4.pdf.json
I16 O
: O
“ O
hate O
has O
always O
been O
expressed O
idiomatically O
” O
) O
. O

section 21
id pdf2json/2021.acl-long.4.pdf.json
its O
members O
: O
Core O
element O
of O
several O
hate O
speech O
taxonomies O
( O
Golbeck O
et O
al. O
, O
2017 O
; O
Zampieri B-AUTHOR
et O
al. O
, O
2019 O
; O
Vidgen B-AUTHOR
et O
al. O
, O
2020a O
; O
Banko B-AUTHOR
et O
al. O
, O
2020 O
) O
F6 O
– O
threats O
expressed O
as O
normative O
statements O
: O
Highlighted O
by O
an O
interviewee O
as O
a B-PARAMETER
way O
of O
avoiding O
legal O
consequences O
to O
hate O
speech O
( O
I1 O
: O
“ O
[ O
normative O
threats O
] O
are O
extremely O
hateful O
, O
but O
[ O
legally O
] O
okay O
” O
) O
. O
section 21
id pdf2json/2021.acl-long.4.pdf.json
F7 O
– O
hate O
expressed O
using O
slurs O
: O
Prevalent O
way O
of O
expressing O
hate O
( O
Palmer B-AUTHOR
et O
al. O
, O
2020 O
; O
Banko B-AUTHOR
et O
al. O
, O
2020 O
; O
Kurrek B-AUTHOR
et O
al. O
, O
2020 O
) O
. O
section 21
id pdf2json/2021.acl-long.4.pdf.json
F8 O
– O
non-hateful O
homonyms O
of O
slur O
: O
Relevant O
alternative O
use O
of O
slurs O
( O
Kurrek B-AUTHOR
et O
al. O
, O
2020 O
) O
. O
section 21
id pdf2json/2021.acl-long.4.pdf.json
F9 O
– O
use O
of O
reclaimed O
slurs O
: O
Likely O
source O
of O
classification O
error O
( O
Palmer B-AUTHOR
et O
al. O
, O
2020 O
) O
. O
section 21
id pdf2json/2021.acl-long.4.pdf.json
Highlighted O
in O
our O
interviews O
( O
e.g O
. O
section 21
id pdf2json/2021.acl-long.4.pdf.json
I7 O
: O
“ O
A O
lot O
of O
LGBT O
people O
use O
slurs O
to O
identify O
themselves O
, O
like O
reclaim O
the O
word B-METHOD
queer O
, O
and O
people O
[ O
... O
] O
report O
that O
and O
then O
that O
will O
get O
hidden O
” O
) O
. O
section 21
id pdf2json/2021.acl-long.4.pdf.json
F10 O
– O
hate O
expressed O
using O
profanity O
: O
Refines O
more O
general O
“ O
insult O
” O
categories O
( O
Davidson B-AUTHOR
et O
al. O
, O
2017 O
; O
Zampieri B-AUTHOR
et O
al. O
, O
2019 O
) O
. O
section 21
id pdf2json/2021.acl-long.4.pdf.json
F11 O
– O
non-hateful O
uses O
of O
profanity O
: O
Oversensitiveness O
of O
hate O
speech O
detection O
models O
to O
profanity O
( O
Davidson B-AUTHOR
et O
al. O
, O
2017 O
; O
Malmasi O
and O
Zampieri B-AUTHOR
, O
2018 O
; O
van B-AUTHOR
Aken I-AUTHOR
et O
al. O
, O
2018 O
) O
. O
section 21
id pdf2json/2021.acl-long.4.pdf.json
F12 O
– O
hate O
expressed O
through O
pronoun O
reference O
in O
subsequent O
clauses O
: O
Syntactic O
relationships O
and O
long-range O
dependencies O
as O
model O
weak O
points O
( O
Burnap O
and O
Williams B-AUTHOR
, O
2015 O
; O
Vidgen B-AUTHOR
et O
al. O
, O
2019 O
) O
. O
section 21
id pdf2json/2021.acl-long.4.pdf.json
F13 O
– O
hate O
expressed O
through O
pronoun O
reference O
in O
subsequent O
sentences O
: O
See O
F12 O
. O
section 21
id pdf2json/2021.acl-long.4.pdf.json
F14 O
– O
hate O
expressed O
using O
negated O
positive O
statements O
: O
Negation O
as O
an O
effective O
adversary O
for O
hate O
speech O
detection O
models O
( O
Hosseini B-AUTHOR
et O
al. O
, O
2017 O
; O
Dinan B-AUTHOR
et O
al. O
, O
2019 O
) O
. O
section 21
id pdf2json/2021.acl-long.4.pdf.json
F15 O
– O
non-hate O
expressed O
using O
negated O
hateful O
statements O
: O
See O
F14 O
. O
section 21
id pdf2json/2021.acl-long.4.pdf.json
F16 O
– O
hate O
phrased O
as O
a B-PARAMETER
question O
: O
Likely O
source O
of O
classification O
error O
( O
van B-AUTHOR
Aken I-AUTHOR
et O
al. O
, O
2018 O
) O
. O
section 21
id pdf2json/2021.acl-long.4.pdf.json
F17 O
– O
hate O
phrased O
as O
an O
opinion O
: O
Highlighted O
by O
an O
interviewee O
as O
a B-PARAMETER
way O
of O
avoiding O
legal O
consequences O
to O
hate O
speech O
( O
I1 O
: O
“ O
If O
you O
start O
a B-PARAMETER
sentence O
by O
saying O
‘ O
I O
think O
that O
’ O
[ O
... O
] O
, O
the O
limits O
of O
what O
you O
can O
say O
are O
much O
bigger O
” O
) O
. O
section 21
id pdf2json/2021.acl-long.4.pdf.json
F18 O
– O
neutral O
statements O
using O
protected O
group O
identifiers O
: O
Oversensitiveness O
of O
hate O
speech O
de- O
tection O
models O
to O
terms O
such O
as O
“ O
black O
” O
and O
“ O
gay O
” O
( O
Dixon B-AUTHOR
et O
al. O
, O
2018 O
; O
Park B-AUTHOR
et O
al. O
, O
2018 O
; O
Kennedy B-AUTHOR
et O
al. O
, O
2020 O
) O
. O
section 21
id pdf2json/2021.acl-long.4.pdf.json
Also O
highlighted O
in O
our O
interviews O
( O
e.g O
. O
section 21
id pdf2json/2021.acl-long.4.pdf.json
I7 O
: O
“ O
I O
have O
seen O
the O
algorithm O
get O
it O
wrong O
, O
if O
someone O
’ O
s O
saying O
something O
like O
‘ O
I O
’ O
m O
so B-PARAMETER
gay O
’ O
. O
” O
) O
. O
section 21
id pdf2json/2021.acl-long.4.pdf.json
F19 O
– O
positive O
statements O
using O
protected O
group O
identifiers O
: O
See O
F18 O
. O
section 21
id pdf2json/2021.acl-long.4.pdf.json
F20 O
– O
denouncements O
of O
hate O
that O
quote O
it O
: O
Counter O
speech O
as O
a B-PARAMETER
source O
of O
classification O
error O
( O
Warner B-AUTHOR
and O
Hirschberg B-AUTHOR
, O
2012 O
; O
van B-AUTHOR
Aken I-AUTHOR
et O
al. O
, O
2018 O
; O
Vidgen B-AUTHOR
et O
al. O
, O
2020a O
) O
. O
section 21
id pdf2json/2021.acl-long.4.pdf.json
Most O
mentioned O
concern O
in O
our O
interviews O
( O
e.g O
. O
section 21
id pdf2json/2021.acl-long.4.pdf.json
I4 O
: O
“ O
people O
will O
be O
quoting O
someone O
, O
calling O
that O
person O
out O
[ O
... O
] O
but O
that O
will O
get O
picked O
up O
by O
the O
system O
” O
) O
. O
section 21
id pdf2json/2021.acl-long.4.pdf.json
F21 O
– O
denouncements O
of O
hate O
that O
make O
direct O
reference O
to O
it O
: O
See O
F20 O
. O
section 21
id pdf2json/2021.acl-long.4.pdf.json
F22 O
– O
abuse O
targeted O
at O
objects O
: O
Distinct O
from O
hate O
speech O
since O
it O
targets O
out-of-scope O
entities O
( O
Wulczyn O
et O
al. O
, O
2017 O
; O
Zampieri B-AUTHOR
et O
al. O
, O
2019 O
) O
. O
section 21
id pdf2json/2021.acl-long.4.pdf.json
F23 O
– O
abuse O
targeted O
at O
individuals O
not O
referencing O
membership O
in O
a B-PARAMETER
protected O
group O
: O
See O
F22 O
. O
section 21
id pdf2json/2021.acl-long.4.pdf.json
F24 O
– O
abuse O
targeted O
at O
non-protected O
groups O
( O
e.g O
. O
section 21
id pdf2json/2021.acl-long.4.pdf.json
professions O
) O
: O
See O
F22 O
. O
section 21
id pdf2json/2021.acl-long.4.pdf.json
F25 O
– O
swaps O
of O
adjacent O
characters O
: O
Simple O
misspellings O
can O
be O
challenging O
for O
detection O
models O
( O
van B-AUTHOR
Aken I-AUTHOR
et O
al. O
, O
2018 O
; O
Qian B-AUTHOR
et O
al. O
, O
2018 O
) O
. O
section 21
id pdf2json/2021.acl-long.4.pdf.json
Particularly O
relevant O
to O
hate O
speech O
since O
they O
can O
reflect O
intentional O
behaviour O
of O
users O
looking O
to O
avoid O
detection O
( O
Hosseini B-AUTHOR
et O
al. O
, O
2017 O
; O
Gröndahl O
et O
al. O
, O
2018 O
; O
Vidgen B-AUTHOR
et O
al. O
, O
2019 O
) O
. O
section 21
id pdf2json/2021.acl-long.4.pdf.json
F26 O
– O
missing O
characters O
: O
Highlighted O
in O
our O
interviews O
( O
e.g O
. O
section 21
id pdf2json/2021.acl-long.4.pdf.json
I7 O
: O
“ O
it O
could O
be O
a B-PARAMETER
misspelling O
of O
a B-PARAMETER
word B-METHOD
like O
‘ O
f*ggot O
’ O
, O
and O
someone O
’ O
s O
put O
one O
‘ O
g O
’ O
instead O
of O
two O
” O
) O
. O
section 21
id pdf2json/2021.acl-long.4.pdf.json
F27 O
– O
missing O
word B-METHOD
boundaries O
: O
Effective O
adversary O
for O
a B-PARAMETER
hate O
speech O
detection O
model O
( O
Gröndahl O
et O
al. O
, O
2018 O
) O
. O
section 21
id pdf2json/2021.acl-long.4.pdf.json
Resembles O
the O
use O
of O
hashtags O
on O
social O
media O
( O
I2 O
: O
“ O
there O
have O
been O
a B-PARAMETER
highly O
Islamophobic O
hashtags O
going O
around O
” O
) O
. O
section 21
id pdf2json/2021.acl-long.4.pdf.json
F28 O
– O
added O
spaces O
between O
characters O
: O
Effective O
adversary O
for O
a B-PARAMETER
hate O
speech O
detection O
model O
( O
Gröndahl O
et O
al. O
, O
2018 O
) O
. O
section 21
id pdf2json/2021.acl-long.4.pdf.json
Highlighted O
in O
our O
interviews O
( O
e.g O
. O
section 21
id pdf2json/2021.acl-long.4.pdf.json
I5 O
: O
“ O
misspellings O
, O
missing O
letters O
or O
additional O
spaces O
between O
the O
letters. O
” O
) O
. O
section 21
id pdf2json/2021.acl-long.4.pdf.json
F29 O
– O
leet O
speak O
: O
Resembles O
“ O
obfuscations O
” O
( O
Nobata B-AUTHOR
et O
al. O
, O
2016 O
; O
van B-AUTHOR
Aken I-AUTHOR
et O
al. O
, O
2018 O
) O
. O
section 21
id pdf2json/2021.acl-long.4.pdf.json
Highlighted O
in O
our O
interviews O
( O
e.g O
. O
section 21
id pdf2json/2021.acl-long.4.pdf.json
I14 O
: O
“ O
[ O
hate O
speakers O
] O
replace O
letters O
with O
numbers O
” O
) O
. O

section 22
id pdf2json/2021.acl-long.4.pdf.json
For O
each O
of O
the O
seven O
protected O
groups O
covered O
by O
HATECHECK O
, O
we O
searched O
hatebase.org O
, O
a B-PARAMETER
crowdsourced O
hate O
speech O
lexicon O
, O
for O
slurs O
which O
target B-PARAMETER
that O
group O
. O
section 22
id pdf2json/2021.acl-long.4.pdf.json
From O
these O
slurs O
, O
we O
selected O
the O
three O
that O
were O
most O
often O
logged O
by O
users O
of O
the O
site O
( O
e.g O
. O
section 22
id pdf2json/2021.acl-long.4.pdf.json
“ O
wh*re O
” O
, O
“ O
b*tch O
” O
and O
“ O
sl*t O
” O
for O
women O
) O
, O
except O
for O
when O
the O
third-most O
sighted O
slur O
was O
logged O
substantially O
less O
often O
than O
the O
second O
, O
in O
which O
case O
we O
selected O
the O
top O
two O
( O
e.g O
. O
section 22
id pdf2json/2021.acl-long.4.pdf.json
“ O
tr*nny O
” O
and O
“ O
sh*male O
” O
for O
trans O
people O
) O
. O
section 22
id pdf2json/2021.acl-long.4.pdf.json
For O
immigration O
status O
, O
which O
is O
not O
a B-PARAMETER
target B-PARAMETER
category O
on O
hatebase.org O
, O
we O
chose O
“ O
r*pefugee O
” O
, O
a B-PARAMETER
slur O
for O
refugees O
used O
by O
the O
European O
far O
right O
, O
and O
“ O
w*tback O
” O
, O
a B-PARAMETER
slur O
for O
Mexican O
immigrants O
to O
the O
US B-PARAMETER
, O
which O
was O
logged O
similarly O
often O
as O
other O
slurs O
in O
HATECHECK O
. O
section 22
id pdf2json/2021.acl-long.4.pdf.json
For O
reclaimed O
slurs O
( O
F9 O
) O
, O
we O
focus O
on O
slurs O
reclaimed O
by O
black O
communities O
( O
particularly O
“ O
n*gga O
” O
) O
, O
gay O
communities O
( O
“ O
f*g O
” O
, O
“ O
f*ggot O
” O
, O
“ O
q*eer O
” O
) O
and O
by O
women O
( O
“ O
b*tch O
” O
) O
, O
reflecting O
the O
concerns O
highlighted O
by O
our O
interview O
participants O
( O
e.g O
. O
section 22
id pdf2json/2021.acl-long.4.pdf.json
I4 O
: O
“ O
n*gga O
would O
often O
get O
[ O
wrongly O
] O
picked O
up O
by O
[ O
moderation O
] O
systems O
” O
) O
. O
section 22
id pdf2json/2021.acl-long.4.pdf.json
Ahead O
of O
the O
structured O
annotation O
process O
( O
§2.5 O
) O
and O
only O
for O
test O
cases O
with O
reclaimed O
slurs O
, O
we O
asked O
selfidentifying O
members O
of O
the O
relevant O
groups O
in O
our O
personal O
networks O
whether O
they O
would O
consider O
the O
test O
cases O
to O
contain O
valid O
and O
realistic O
reclaimed O
slur O
uses O
, O
which O
held O
true O
for O
all O
test O
cases O
. O

section 23
id pdf2json/2021.acl-long.4.pdf.json
D.1 O
Davidson B-AUTHOR
et O
al O
. O
section 23
id pdf2json/2021.acl-long.4.pdf.json
( O
2017 O
) O
Data O
Sampling O
Davidson B-AUTHOR
et O
al O
. O
section 23
id pdf2json/2021.acl-long.4.pdf.json
( O
2017 O
) O
searched O
Twitter B-SOFTWARE
for O
tweets O
containing O
keywords O
from O
a B-PARAMETER
list O
they O
compiled O
from O
hatebase.org O
, O
which O
yielded O
a B-PARAMETER
sample O
of O
tweets O
from O
33,458 O
users O
. O
section 23
id pdf2json/2021.acl-long.4.pdf.json
They O
then O
randomly O
sampled O
25,000 O
tweets O
from O
all O
tweets O
of O
these O
users O
. O
section 23
id pdf2json/2021.acl-long.4.pdf.json
Annotation O
The O
authors O
hired O
crowd O
workers O
from O
CrowdFlower O
to O
annotate O
each O
tweet O
as O
hateful O
, O
offensive O
or O
neither O
. O
section 23
id pdf2json/2021.acl-long.4.pdf.json
92.0 O
% O
of O
tweets O
were O
annotated O
by O
three O
crowd O
workers O
, O
the O
remainder O
by O
at O
least O
four O
and O
up O
to O
nine O
. O
section 23
id pdf2json/2021.acl-long.4.pdf.json
For O
inter-annotator O
agreement O
, O
the O
authors O
report O
a B-PARAMETER
“ O
CrowdFlower O
score O
” O
of O
92 O
% O
. O
section 23
id pdf2json/2021.acl-long.4.pdf.json
Data O
We O
used O
24,783 O
annotated O
tweets O
made O
available O
by O
the O
authors O
on O
github.com/tdavidson/hate-speech-and-offensive-language O
. O
section 23
id pdf2json/2021.acl-long.4.pdf.json
1,430 O
tweets O
( O
5.8 O
% O
) O
are O
labelled O
hateful O
, O
19,190 O
( O
77.4 O
% O
) O
offensive O
and O
4,163 O
( O
16.8 O
% O
) O
neither O
. O
section 23
id pdf2json/2021.acl-long.4.pdf.json
We O
collapse O
the O
latter O
two O
labels O
into O
a B-PARAMETER
single O
non-hateful O
label O
to O
match O
HATECHECK O
’ O
s O
binary O
format O
, O
resulting O
in O
1,430 O
tweets O
( O
5.8 O
% O
) O
labelled O
hateful O
and O
23,353 O
( O
94.2 O
% O
) O
labelled O
non-hateful O
. O
section 23
id pdf2json/2021.acl-long.4.pdf.json
Definition O
of O
Hate O
Speech O
“ O
Language O
that O
is O
used O
to O
expresses O
hatred O
towards O
a B-PARAMETER
targeted O
group O
or O
is O
intended O
to O
be O
derogatory O
, O
to O
humiliate O
, O
or O
to O
insult O
the O
members O
of O
the O
group O
” O
. O
section 23
id pdf2json/2021.acl-long.4.pdf.json
D.2 O
Founta B-AUTHOR
et O
al O
. O
section 23
id pdf2json/2021.acl-long.4.pdf.json
( O
2018 O
) O
Data O
Sampling O
Founta B-AUTHOR
et O
al O
. O
section 23
id pdf2json/2021.acl-long.4.pdf.json
( O
2018 O
) O
initially O
collected O
a B-PARAMETER
random O
set O
of O
32 O
million O
tweets O
from O
Twitter B-SOFTWARE
. O
section 23
id pdf2json/2021.acl-long.4.pdf.json
They O
then O
used O
a B-PARAMETER
boosted O
random O
sampling O
procedure O
based O
on O
negative O
sentiment O
and O
occurrence O
of O
offensive O
words O
as O
selected O
from O
hatebase.org O
to O
augment O
a B-PARAMETER
random O
subset O
of O
this O
initial O
sample O
with O
tweets O
they O
expected O
to O
be O
more O
likely O
to O
be O
hateful O
or O
abusive O
. O
section 23
id pdf2json/2021.acl-long.4.pdf.json
Annotation O
The O
authors O
hired O
crowd O
workers O
from O
CrowdFlower O
to O
annotate O
each O
tweet O
as O
hateful O
, O
abusive O
, O
spam O
or O
normal O
. O
section 23
id pdf2json/2021.acl-long.4.pdf.json
All O
tweets O
were O
annotated O
by O
five O
crowd O
workers O
. O
section 23
id pdf2json/2021.acl-long.4.pdf.json
For O
inter-annotator O
agreement O
, O
the O
authors O
report O
that O
55.9 O
% O
of O
tweets O
had O
four O
out O
of O
five O
annotators O
agreeing O
on O
a B-PARAMETER
label O
. O
section 23
id pdf2json/2021.acl-long.4.pdf.json
Data O
The O
authors O
provided O
us B-PARAMETER
access O
to O
the O
full O
text O
versions O
of O
99,996 O
annotated O
tweets O
. O
section 23
id pdf2json/2021.acl-long.4.pdf.json
These O
correspond O
to O
the O
tweet O
IDs O
made O
available O
by O
the O
authors O
on O
github.com/ENCASEH2020/hatespeechtwitter O
. O
section 23
id pdf2json/2021.acl-long.4.pdf.json
4,965 O
tweets O
( O
5.0 O
% O
) O
are O
labelled O
hateful O
, O
27,150 O
( O
27.2 O
% O
) O
abusive O
, O
14,030 O
( O
14.0 O
% O
) O
spam O
and O
53,851 O
( O
53.9 O
% O
) O
normal O
. O
section 23
id pdf2json/2021.acl-long.4.pdf.json
We O
collapse O
the O
latter O
three O
labels O
into O
a B-PARAMETER
single O
non-hateful O
label O
to O
match O
HATECHECK O
’ O
s O
binary O
format O
, O
resulting O
in O
4,965 O
tweets O
( O
5.0 O
% O
) O
labelled O
hateful O
and O
95,031 O
tweets O
( O
95.0 O
% O
) O
labelled O
non-hateful O
. O
section 23
id pdf2json/2021.acl-long.4.pdf.json
Definition O
of O
Hate O
Speech O
“ O
Language O
used O
to O
express O
hatred O
towards O
a B-PARAMETER
targeted O
individual O
or O
group O
, O
or O
is O
intended O
to O
be O
derogatory O
, O
to O
humiliate O
, O
or O
to O
insult O
the O
members O
of O
the O
group O
, O
on O
the O
basis O
of O
attributes O
such O
as O
race O
, O
religion O
, O
ethnic O
origin O
, O
sexual O
orientation O
, O
disability O
, O
or O
gender O
” O
. O
section 23
id pdf2json/2021.acl-long.4.pdf.json
D.3 O
Pre-Processing O
Before O
using O
the O
datasets O
for O
fine-tuning O
, O
we O
lowercase O
all O
text O
and O
remove O
newline O
and O
tab O
characters O
. O
section 23
id pdf2json/2021.acl-long.4.pdf.json
We O
replace O
URLs O
, O
user O
mentions O
and O
emojis O
with O
[ O
URL O
] O
, O
[ O
USER O
] O
and O
[ O
EMOJI O
] O
tokens O
. O
section 23
id pdf2json/2021.acl-long.4.pdf.json
We O
also O
split O
hashtags O
into O
separate O
tokens O
using O
the O
wordsegment O
Python O
package O
. O

section 24
id pdf2json/2021.acl-long.4.pdf.json
Model O
Architecture O
We O
implemented O
uncased O
BERT-base B-METHOD
models O
( O
Devlin B-AUTHOR
et O
al. O
, O
2019 O
) O
using O
the O
transformers O
Python O
library O
( O
Wolf B-AUTHOR
et O
al. O
, O
2020 O
) O
. O
section 24
id pdf2json/2021.acl-long.4.pdf.json
Uncased O
BERT-base B-METHOD
, O
which O
is O
trained O
on O
lower-cased O
English O
text O
, O
has O
12 B-PARAMETER
layers I-PARAMETER
, O
a B-PARAMETER
hidden O
layer O
size O
of O
768 B-PARAMETER
, O
12 O
attention B-METHOD
heads O
and O
a B-PARAMETER
total O
of O
110 O
million O
parameters O
. O
section 24
id pdf2json/2021.acl-long.4.pdf.json
For O
sequence O
classification O
, O
we O
added O
a B-PARAMETER
linear O
layer O
with O
softmax B-METRIC
output O
. O
section 24
id pdf2json/2021.acl-long.4.pdf.json
Fine-Tuning O
B-D O
was O
fine-tuned O
on O
binary O
Davidson B-AUTHOR
et O
al O
. O
section 24
id pdf2json/2021.acl-long.4.pdf.json
( O
2017 O
) O
data O
and O
B-F O
on O
binary O
Founta B-AUTHOR
et O
al O
. O
section 24
id pdf2json/2021.acl-long.4.pdf.json
( O
2018 O
) O
data O
. O
section 24
id pdf2json/2021.acl-long.4.pdf.json
For O
both O
datasets O
, O
we O
used O
a B-PARAMETER
stratified O
80/10/10 O
train/dev/test O
split O
. O
section 24
id pdf2json/2021.acl-long.4.pdf.json
Models O
were O
trained O
for O
three O
epochs B-PARAMETER
each O
. O
section 24
id pdf2json/2021.acl-long.4.pdf.json
Training O
batch O
size O
was O
16 O
. O
section 24
id pdf2json/2021.acl-long.4.pdf.json
We O
used O
cross-entropy B-METRIC
loss I-METRIC
with O
class O
weights B-PARAMETER
emphasising O
the O
hateful O
minority O
class O
. O
section 24
id pdf2json/2021.acl-long.4.pdf.json
Weights O
were O
set O
to O
the O
relative O
proportion O
of O
the O
other O
class O
in O
the O
training O
data O
, O
meaning O
that O
for O
a B-PARAMETER
1:9 O
hateful O
: O
non-hateful O
case O
split O
, O
loss O
on O
hateful O
cases O
would O
be O
multiplied O
by O
9 O
. O
section 24
id pdf2json/2021.acl-long.4.pdf.json
The O
optimiser O
was O
AdamW B-SOFTWARE
( O
Loshchilov B-AUTHOR
and O
Hutter B-AUTHOR
, O
2019 O
) O
with O
a B-PARAMETER
5e-5 O
learning B-PARAMETER
rate I-PARAMETER
and O
a B-PARAMETER
0.01 B-PARAMETER
weight I-PARAMETER
decay I-PARAMETER
. O
section 24
id pdf2json/2021.acl-long.4.pdf.json
For O
regularisation O
, O
we O
set O
a B-PARAMETER
10 O
% O
dropout B-METHOD
probability O
. O
section 24
id pdf2json/2021.acl-long.4.pdf.json
Hyperparameter O
Tuning O
The O
number O
of O
finetuning O
epochs B-PARAMETER
, O
the O
learning B-PARAMETER
rate I-PARAMETER
and O
the O
training O
batch O
size O
were O
determined O
by O
exhaustive O
grid O
search O
. O
section 24
id pdf2json/2021.acl-long.4.pdf.json
We O
used O
the O
range O
of O
possible O
values O
recommended O
by O
Devlin B-AUTHOR
et I-AUTHOR
al I-AUTHOR
. I-AUTHOR
section 24
id pdf2json/2021.acl-long.4.pdf.json
( O
2019 O
) O
: O
[ O
2 O
, O
3 O
, O
4 O
] O
for O
epochs B-PARAMETER
, O
[ O
2e-5 O
, O
3e-5 O
, O
5e-5 O
] O
for O
learning B-PARAMETER
rate I-PARAMETER
and O
[ O
16 O
, O
32 O
] O
for O
batch O
size O
. O
section 24
id pdf2json/2021.acl-long.4.pdf.json
There O
were O
18 O
training/evaluation O
runs O
for O
each O
model O
. O
section 24
id pdf2json/2021.acl-long.4.pdf.json
The O
best O
configuration O
was O
selected O
based O
on O
loss O
on O
the O
10 O
% O
development O
set O
. O
section 24
id pdf2json/2021.acl-long.4.pdf.json
Held-Out O
Performance O
Micro/macro O
F1 B-METRIC
scores O
on O
the O
held-out O
test O
sets O
corresponding O
to O
their O
training O
data O
are O
91.5/70.8 O
for O
B-D O
( O
Davidson B-AUTHOR
et O
al. O
, O
2017 O
) O
and O
92.9/70.3 O
for O
B-F O
( O
Founta B-AUTHOR
et O
al. O
, O
2018 O
) O
. O
section 24
id pdf2json/2021.acl-long.4.pdf.json
Computation O
We O
ran O
all O
computations O
on O
a B-PARAMETER
Microsoft O
Azure O
“ O
Standard O
NC24 O
” O
server O
equipped O
with O
two O
NVIDIA O
Tesla O
K80 O
GPU B-SOFTWARE
cards O
. O
section 24
id pdf2json/2021.acl-long.4.pdf.json
The O
average B-METRIC
wall O
time O
for O
each O
hyperparameter O
tuning O
trial O
of O
B-D O
was O
around O
17 O
minutes O
, O
and O
for O
B-F O
around O
70 O
minutes O
. O
section 24
id pdf2json/2021.acl-long.4.pdf.json
Source O
Code O
Our O
code O
is O
available O
on O
github.com/paul-rottger/hatecheck-experiments O
. O

section 25
id pdf2json/2021.acl-long.4.pdf.json
Most O
previous O
work O
that O
trains O
and O
evaluates O
models O
on O
Davidson B-AUTHOR
et O
al O
. O
section 25
id pdf2json/2021.acl-long.4.pdf.json
( O
2017 O
) O
and O
Founta B-AUTHOR
et O
al O
. O
section 25
id pdf2json/2021.acl-long.4.pdf.json
( O
2018 O
) O
data O
uses O
their O
original O
multiclass O
label O
format O
. O
section 25
id pdf2json/2021.acl-long.4.pdf.json
In O
the O
multiclass O
case O
, O
the O
relative O
size O
of O
the O
hateful O
class O
compared O
to O
the O
non-hateful O
classes O
is O
larger O
than O
in O
the O
binary O
case O
, O
which O
is O
likely O
why O
most O
models O
do O
not O
use O
class O
weights B-PARAMETER
. O
section 25
id pdf2json/2021.acl-long.4.pdf.json
For O
comparability O
, O
we O
thus O
fine-tuned O
unweighted O
multiclass O
versions O
of O
B-D O
and O
B-F O
, O
using O
the O
same O
model O
parameters O
described O
in O
Appendix O
E. O
On O
multiclass O
Davidson B-AUTHOR
et O
al O
. O
section 25
id pdf2json/2021.acl-long.4.pdf.json
( O
2017 O
) O
data O
, O
Mozafari B-AUTHOR
et O
al O
. O
section 25
id pdf2json/2021.acl-long.4.pdf.json
( O
2019 O
) O
report O
a B-PARAMETER
weighted-average O
F1 B-METRIC
score I-METRIC
of O
91 O
for O
their O
BERT-base B-METHOD
model O
and O
92 O
for O
BERTbase O
combined O
with O
a B-PARAMETER
CNN B-METHOD
. O
section 25
id pdf2json/2021.acl-long.4.pdf.json
Cao B-AUTHOR
et O
al O
. O
section 25
id pdf2json/2021.acl-long.4.pdf.json
( O
2020 O
) O
report O
a B-PARAMETER
micro O
F1 B-METRIC
of O
89.9 O
for O
their O
ensemble-like O
“ O
DeepHate O
” O
classifier O
. O
section 25
id pdf2json/2021.acl-long.4.pdf.json
Our O
unweighted O
multiclass O
BERT-base B-METHOD
model O
achieves O
90.7 O
weighted-average O
F1 B-METRIC
and O
91.1 O
micro O
F1 B-METRIC
. O
section 25
id pdf2json/2021.acl-long.4.pdf.json
On O
multiclass O
Founta B-AUTHOR
et O
al O
. O
section 25
id pdf2json/2021.acl-long.4.pdf.json
( O
2018 O
) O
data O
, O
Cao B-AUTHOR
et O
al O
. O
section 25
id pdf2json/2021.acl-long.4.pdf.json
( O
2020 O
) O
report O
a B-PARAMETER
micro O
F1 B-METRIC
of O
79.1 O
for O
“ O
DeepHate O
” O
. O
section 25
id pdf2json/2021.acl-long.4.pdf.json
Our O
unweighted O
multiclass O
BERT-base B-METHOD
model O
achieves O
81.7 O
micro O
F1 B-METRIC
. O
section 25
id pdf2json/2021.acl-long.4.pdf.json
Tran B-AUTHOR
et O
al O
. O
section 25
id pdf2json/2021.acl-long.4.pdf.json
( O
2020 O
) O
recently O
achieved O
SOTA B-SOFTWARE
on O
several O
other O
hate O
speech O
datasets O
with O
their O
HABERTOR O
model O
. O
section 25
id pdf2json/2021.acl-long.4.pdf.json
They O
also O
find O
that O
BERTbase O
consistently O
performs O
very O
near O
their O
SOTA B-SOFTWARE
. O
section 25
id pdf2json/2021.acl-long.4.pdf.json
However O
, O
they O
do O
not O
evaluate O
their O
models O
on O
Davidson B-AUTHOR
et O
al O
. O
section 25
id pdf2json/2021.acl-long.4.pdf.json
( O
2017 O
) O
or O
Founta B-AUTHOR
et O
al O
. O
section 25
id pdf2json/2021.acl-long.4.pdf.json
( O
2018 O
) O
data O
. O

section TITLE
id pdf2json/2021.acl-long.378.pdf.json
Parameter-Efficient O
Transfer B-METHOD
Learning I-METHOD
with O
Diff O
Pruning O

section ABSTRACT
id pdf2json/2021.acl-long.378.pdf.json
The O
large O
size O
of O
pretrained O
networks O
makes O
them O
difficult O
to O
deploy O
for O
multiple O
tasks O
in O
storage-constrained O
settings O
. O
section ABSTRACT
id pdf2json/2021.acl-long.378.pdf.json
Diff O
pruning O
enables O
parameter-efficient O
transfer B-METHOD
learning I-METHOD
that O
scales O
well O
with O
new O
tasks O
. O
section ABSTRACT
id pdf2json/2021.acl-long.378.pdf.json
The O
approach O
learns O
a B-PARAMETER
task-specific O
“ O
diff O
” O
vector O
that O
extends O
the O
original O
pretrained O
parameters O
. O
section ABSTRACT
id pdf2json/2021.acl-long.378.pdf.json
This O
diff O
vector O
is O
adaptively O
pruned O
during O
training O
with O
a B-PARAMETER
differentiable O
approximation O
to O
the O
L0-norm O
penalty O
to O
encourage O
sparsity O
. O
section ABSTRACT
id pdf2json/2021.acl-long.378.pdf.json
As O
the O
number O
of O
tasks O
increases O
, O
diff O
pruning O
remains O
parameter-efficient O
, O
as O
it O
requires O
storing O
only O
a B-PARAMETER
small O
diff O
vector O
for O
each O
task O
. O
section ABSTRACT
id pdf2json/2021.acl-long.378.pdf.json
Since O
it O
does O
not O
require O
access O
to O
all O
tasks O
during O
training O
, O
it O
is O
attractive O
in O
on-device O
deployment O
settings O
where O
tasks O
arrive O
in O
stream O
or O
even O
from O
different O
providers O
. O
section ABSTRACT
id pdf2json/2021.acl-long.378.pdf.json
Diff O
pruning O
can O
match O
the O
performance O
of O
finetuned O
baselines O
on O
the O
GLUE O
benchmark O
while O
only O
modifying O
0.5 O
% O
of O
the O
pretrained O
model O
’ O
s O
parameters O
per O
task O
and O
scales O
favorably O
in O
comparison O
to O
popular O
pruning O
approaches O
. O

section 0
id pdf2json/2021.acl-long.378.pdf.json
Proceedings O
of O
the O
59th O
Annual O
Meeting O
of O
the O
Association O
for O
Computational O
Linguistics O
and O
the O
11th O
International O
Joint O
Conference O
on O
Natural O
Language O
Processing O
, O
pages O
4884–4896 O
August O
1–6 O
, O
2021 O
. O
section 0
id pdf2json/2021.acl-long.378.pdf.json
©2021 O
Association O
for O
Computational O
Linguistics O
4884 O

section 1
id pdf2json/2021.acl-long.378.pdf.json
Task-specific O
finetuning O
of O
pretrained O
deep O
networks O
is O
the O
dominant O
paradigm O
in O
contemporary O
NLP O
, O
achieving O
state-of-the-art O
results O
across O
a B-PARAMETER
suite O
of O
natural O
language O
understanding O
tasks O
( O
Devlin B-AUTHOR
et O
al. O
, O
2019 O
; O
Liu B-AUTHOR
et O
al. O
, O
2019c O
; O
Yang B-AUTHOR
et O
al. O
, O
2019 O
; O
Lan B-AUTHOR
et O
al. O
, O
2020 O
) O
. O
section 1
id pdf2json/2021.acl-long.378.pdf.json
While O
straightforward O
and O
empirically O
effective O
, O
this O
approach O
is O
difficult O
to O
scale O
to O
multi-task O
, O
memory-constrained O
settings O
( O
e.g O
. O
section 1
id pdf2json/2021.acl-long.378.pdf.json
for O
on-device O
applications O
) O
, O
as O
it O
requires O
shipping O
and O
storing O
a B-PARAMETER
full O
set O
of O
model O
parameters O
for O
each O
task O
. O
section 1
id pdf2json/2021.acl-long.378.pdf.json
Inasmuch O
as O
these O
models O
are O
learning O
generalizable O
, O
task-agnostic O
language O
representations O
through O
self-supervised O
pretraining O
, O
finetuning O
the O
entire O
model O
for O
each O
task O
seems O
especially O
profligate O
. O
section 1
id pdf2json/2021.acl-long.378.pdf.json
Code O
: O
https O
: O
//github.com/dguo98/DiffPruning O
A O
popular O
approach O
to O
parameter-efficiency O
is O
to O
learn O
smaller O
compressed O
models O
for O
each O
task O
( O
Gordon O
et O
al. O
, O
2020 O
; O
Sajjad O
et O
al. O
, O
2020 O
; O
Zhao B-AUTHOR
et O
al. O
, O
2020 O
; O
Sanh B-AUTHOR
et O
al. O
, O
2020 O
) O
. O
section 1
id pdf2json/2021.acl-long.378.pdf.json
Such O
approaches O
face O
a B-PARAMETER
steep O
sparsity/performance O
tradeoff O
and O
keep O
a B-PARAMETER
substantial O
amount O
of O
nonzero O
parameters O
per O
task O
( O
e.g O
. O
section 1
id pdf2json/2021.acl-long.378.pdf.json
10 O
% O
-30 O
% O
) O
. O
section 1
id pdf2json/2021.acl-long.378.pdf.json
Multi-task O
learning O
and O
featurebased O
transfer O
allow O
for O
more O
parameter-efficient O
transfer B-METHOD
learning I-METHOD
per O
task O
( O
Liu B-AUTHOR
et O
al. O
, O
2019b O
; O
Clark B-AUTHOR
et O
al. O
, O
2019 O
; O
Stickland O
& O
Murray O
, O
2019 O
; O
Reimers O
& O
Gurevych B-AUTHOR
, O
2019 O
) O
. O
section 1
id pdf2json/2021.acl-long.378.pdf.json
These O
methods O
train O
a B-PARAMETER
small O
number O
of O
additional O
parameters O
( O
e.g O
. O
section 1
id pdf2json/2021.acl-long.378.pdf.json
a B-PARAMETER
linear O
layer O
) O
on O
top O
of O
a B-PARAMETER
shared O
model O
. O
section 1
id pdf2json/2021.acl-long.378.pdf.json
However O
, O
multi-task O
learning O
generally O
requires O
access O
to O
all O
tasks O
during O
training O
to O
prevent O
catastrophic O
forgetting O
( O
French O
, O
1999 O
) O
, O
while O
feature-based O
transfer B-METHOD
learning I-METHOD
( O
e.g O
. O
section 1
id pdf2json/2021.acl-long.378.pdf.json
based O
on O
task-agnostic O
sentence O
representations O
) O
is O
typically O
outperformed O
by O
finetuning O
( O
Howard O
& O
Ruder O
, O
2018 O
) O
. O
section 1
id pdf2json/2021.acl-long.378.pdf.json
An B-AUTHOR
appealing O
middle O
ground O
is O
to O
finetune O
an O
extension O
of O
the O
base O
model O
for O
specific O
tasks O
. O
section 1
id pdf2json/2021.acl-long.378.pdf.json
This O
approach O
captures O
the O
training O
benefits O
of O
finetuning O
while O
maintaining O
the O
task O
modularity O
of O
feature-based O
transfer O
. O
section 1
id pdf2json/2021.acl-long.378.pdf.json
For O
example O
, O
Adapters O
( O
Rebuffi O
et O
al. O
, O
2018 O
) O
use O
smaller O
, O
task-specific O
modules O
that O
are O
inserted O
between O
layers O
of O
a B-PARAMETER
model O
This O
approach O
does O
not O
require O
access O
to O
all O
tasks O
during O
training O
, O
targeting O
realistic O
settings O
where O
as O
new O
tasks O
arrive O
in O
stream O
( O
Houlsby O
et O
al. O
, O
2019 O
; O
Pfeiffer O
et O
al. O
, O
2020a O
, O
b O
, O
c B-METRIC
) O
. O
section 1
id pdf2json/2021.acl-long.378.pdf.json
Houlsby O
et O
al O
. O
section 1
id pdf2json/2021.acl-long.378.pdf.json
( O
2019 O
) O
find O
that O
adapter O
layers O
can O
match O
the O
performance O
of O
fully O
finetuned O
BERT B-SOFTWARE
on O
the O
GLUE O
benchmark O
while O
requiring O
3.6 O
% O
additional O
parameters O
( O
on O
average B-METRIC
) O
per O
task O
. O
section 1
id pdf2json/2021.acl-long.378.pdf.json
Diff O
pruning O
is O
a B-PARAMETER
new O
extension O
to O
pretrained O
models O
with O
the O
goal O
of O
even O
more O
parameterefficient O
transfer B-METHOD
learning I-METHOD
. O
section 1
id pdf2json/2021.acl-long.378.pdf.json
Instead O
of O
modifying O
the O
architecture O
of O
the O
model O
, O
diff O
pruning O
extends O
the O
base O
model O
through O
a B-PARAMETER
task-specific O
difference O
vector O
. O
section 1
id pdf2json/2021.acl-long.378.pdf.json
In O
order O
to O
learn O
this O
vector O
, O
we O
reparameterize O
the O
task-specific O
model O
parameters O
as O
θtask O
= O
θpretrained O
+ O
δtask O
, O
where O
the O
pretrained O
parameter O
vector O
θpretrained O
is O
fixed O
and O
the O
task-specific O
diff O
vector O
δtask O
is O
finetuned O
. O
section 1
id pdf2json/2021.acl-long.378.pdf.json
The O
diff O
vector O
is O
regularized O
with O
a B-PARAMETER
differentiable O
approximation O
to O
the O
L0-norm O
penalty O
( O
Louizos O
et O
al. O
, O
2018 O
) O
to O
encourage O
sparsity O
. O
section 1
id pdf2json/2021.acl-long.378.pdf.json
Diff O
pruning O
can O
become O
extremely O
parameterefficient O
, O
as O
it O
only O
requires O
storing O
the O
nonzero O
positions O
and O
weights B-PARAMETER
of O
the O
diff O
vector O
for O
each O
task O
. O
section 1
id pdf2json/2021.acl-long.378.pdf.json
The O
cost O
of O
storing O
the O
shared O
pretrained O
model O
remains O
constant O
and O
is O
amortized O
across O
multiple O
tasks O
. O
section 1
id pdf2json/2021.acl-long.378.pdf.json
On O
the O
GLUE O
benchmark O
( O
Wang B-AUTHOR
et O
al. O
, O
2019a O
) O
, O
diff O
pruning O
can O
match O
the O
performance O
of O
the O
fully O
finetuned O
BERT B-SOFTWARE
baselines O
while O
finetuning O
only O
0.5 O
% O
of O
the O
pretrained O
parameters O
per O
task O
. O
section 1
id pdf2json/2021.acl-long.378.pdf.json
As O
the O
number O
of O
tasks O
increase O
, O
diff O
pruning O
outperforms O
popular O
pruning-based O
methods O
in O
amount O
of O
storage O
required O
. O

section 2
id pdf2json/2021.acl-long.378.pdf.json
Transfer O
learning O
in O
NLP O
mostly O
uses O
a B-PARAMETER
pretrainand-finetune O
paradigm O
, O
which O
initializes O
a B-PARAMETER
subset O
of O
the O
model O
parameters O
for O
all O
tasks O
from O
a B-PARAMETER
pretrained O
model O
and O
then O
finetunes O
on O
a B-PARAMETER
task-specific O
objective O
. O
section 2
id pdf2json/2021.acl-long.378.pdf.json
Pretraining O
objectives O
include O
context O
prediction O
( O
Mikolov B-AUTHOR
et O
al. O
, O
2013 O
) O
, O
autoencoding O
( O
Dai B-AUTHOR
& O
Le B-AUTHOR
, O
2015 O
) O
, O
machine O
translation O
( O
McCann O
et O
al. O
, O
2017 O
) O
, O
and O
more O
recently O
, O
variants O
of O
language O
modeling O
( O
Peters B-AUTHOR
et O
al. O
, O
2018 O
; O
Radford B-AUTHOR
et O
al. O
, O
2018 O
; O
Devlin B-AUTHOR
et O
al. O
, O
2019 O
) O
objectives O
. O
section 2
id pdf2json/2021.acl-long.378.pdf.json
Here O
we O
consider O
applying O
transfer B-METHOD
learning I-METHOD
to O
multiple O
tasks O
. O
section 2
id pdf2json/2021.acl-long.378.pdf.json
We O
consider O
a B-PARAMETER
setting O
with O
a B-PARAMETER
potentially O
unknown O
set O
of O
tasks O
( O
which O
may O
arrive O
in O
stream O
) O
, O
where O
each O
task O
τ O
∈ O
T O
has O
an O
associated O
training O
set O
Dτ O
= O
{ O
x O
( O
n O
) O
τ O
, O
y O
( O
n O
) O
τ O
} O
Nn=1 O
. O
section 2
id pdf2json/2021.acl-long.378.pdf.json
For O
all O
tasks O
, O
the O
goal O
is O
to O
produce O
( O
possibly O
tied O
) O
model O
parameters O
θτ O
to O
minimize O
the O
empirical O
risk O
, O
min O
θτ O
1 O
N O
N∑ O
n=1 O
C B-METHOD
( O
fτ O
( O
x O
( O
n O
) O
τ O
; O
θτ O
) O
, O
y O
( O
n O
) O
τ O
) O
+ O
λR O
( O
θτ O
) O
where O
fτ O
( O
· O
; O
θτ O
) O
is O
a B-PARAMETER
parameterized O
function O
over O
the O
input O
( O
e.g O
. O
section 2
id pdf2json/2021.acl-long.378.pdf.json
a B-PARAMETER
neural B-METHOD
network I-METHOD
) O
, O
C B-METHOD
( O
· O
, O
· O
) O
is O
a B-PARAMETER
loss O
function O
( O
e.g O
. O
section 2
id pdf2json/2021.acl-long.378.pdf.json
cross-entropy O
) O
,1 O
and O
R O
( O
· O
) O
is O
an O
optional O
regularizer O
with O
hyperparameter O
λ O
. O
section 2
id pdf2json/2021.acl-long.378.pdf.json
We O
can O
use O
the O
pretrain-finetune O
approach O
by O
simply O
learning O
independent O
parameters O
for O
each O
1While O
the O
loss O
function O
can O
be O
in O
principle O
task-specific O
, O
in O
practice O
we O
use O
cross O
entropy B-METRIC
for O
all O
tasks O
and O
hence O
omit O
the O
subscript O
in O
C B-METHOD
( O
· O
, O
· O
) O
. O
section 2
id pdf2json/2021.acl-long.378.pdf.json
task O
. O
section 2
id pdf2json/2021.acl-long.378.pdf.json
However O
, O
the O
large O
size O
of O
pretrained O
models O
makes O
this O
approach O
exceedingly O
parameter O
inefficient O
. O
section 2
id pdf2json/2021.acl-long.378.pdf.json
For O
example O
, O
widely-adopted O
models O
such O
as O
BERTBASE B-METHOD
and O
BERTLARGE O
have O
110M O
and O
340M O
parameters O
respectively O
, O
while O
their O
contemporaries O
have O
parameter O
counts O
in O
the O
billions O
( O
Raffel O
et O
al. O
, O
2020 O
; O
Shoeybi O
et O
al. O
, O
2019 O
; O
Rajbhandari O
et O
al. O
, O
2019 O
) O
. O
section 2
id pdf2json/2021.acl-long.378.pdf.json
Storing O
the O
fully O
finetuned O
models O
therefore O
becomes O
difficult O
even O
for O
a B-PARAMETER
moderate O
number O
of O
tasks.2 O
A O
classic O
approach O
to O
tackling O
this O
parameter-inefficiencyis O
to O
train O
a B-PARAMETER
single O
shared O
model O
( O
along O
with O
a B-PARAMETER
task-specific O
output O
layer O
) O
against O
multiple O
tasks O
through O
joint O
training O
( O
Caruana O
, O
1997 O
) O
. O
section 2
id pdf2json/2021.acl-long.378.pdf.json
However O
, O
the O
usual O
formulation O
of O
multi-task O
learning O
requires O
the O
set O
of O
tasks O
T O
to O
be O
known O
in O
advance O
in O
order O
to O
prevent O
catastrophic O
forgetting O
( O
French O
, O
1999 O
) O
,3 O
making O
it O
unsuitable O
for O
applications O
in O
which O
the O
set O
of O
tasks O
is O
unknown O
or O
when O
tasks O
arrive O
in O
stream O
. O

section 3
id pdf2json/2021.acl-long.378.pdf.json
Diff O
pruning O
formulates O
task-specific O
finetuning O
as O
learning O
a B-PARAMETER
diff O
vector O
δτ O
that O
is O
added O
to O
the O
pretrained O
model O
parameters O
θ O
, O
which O
remain O
fixed O
. O
section 3
id pdf2json/2021.acl-long.378.pdf.json
We O
first O
reparameterize O
the O
task-specific O
model O
parameters O
, O
θτ O
= O
θ O
+ O
δτ O
, O
which O
results O
in O
the O
following O
empirical O
risk O
minimization O
problem O
, O
min O
δτ O
L O
( O
Dτ O
, O
fτ O
, O
θ O
+ O
δτ O
) O
+ O
λR O
( O
θ O
+ O
δτ O
) O
, O
where O
for O
brevity O
we O
define O
L O
( O
Dτ O
, O
fτ O
, O
θτ O
) O
as O
L O
( O
Dτ O
, O
fτ O
, O
θτ O
) O
= O
1 O
N O
N∑ O
n=1 O
C B-METHOD
( O
fτ O
( O
x O
( O
n O
) O
τ O
; O
θτ O
) O
, O
y O
( O
n O
) O
τ O
) O
. O
section 3
id pdf2json/2021.acl-long.378.pdf.json
This O
trivial O
reparameterization O
shows O
that O
the O
cost O
of O
storing O
the O
pretrained O
parameters O
θ O
is O
amortized O
across O
tasks O
, O
and O
the O
only O
marginal O
cost O
for O
new O
tasks O
is O
the O
diff O
vector O
. O
section 3
id pdf2json/2021.acl-long.378.pdf.json
If O
we O
can O
regularize O
δτ O
to O
be O
sparse O
such O
that O
‖δτ‖0 O
‖θ‖0 O
, O
then O
this O
approach O
can O
become O
more O
parameter-efficient O
as O
2An O
intriguing O
line O
of O
work O
suggests O
that O
large-scale O
language O
models O
can O
be O
used O
without O
finetuning O
for O
a B-PARAMETER
variety O
of O
tasks O
if O
given O
the O
appropriate O
context O
( O
Radford B-AUTHOR
et O
al. O
, O
2019 O
; O
Brown B-AUTHOR
et O
al. O
, O
2020 O
) O
. O
section 3
id pdf2json/2021.acl-long.378.pdf.json
While O
interesting O
, O
these O
models O
generally O
underperform O
task-specific O
models O
and O
require O
billions O
of O
parameters O
, O
though O
recent O
work O
suggests O
that O
they O
can O
be O
made O
substantially O
smaller O
( O
Schick O
& O
Schutze B-AUTHOR
, O
2020 O
) O
. O
section 3
id pdf2json/2021.acl-long.378.pdf.json
3However O
, O
work O
on O
continual O
learning O
mitigates O
these O
issues O
to O
an O
extent O
( O
Shin B-AUTHOR
et O
al. O
, O
2017 O
; O
Lopez-Paz O
& O
Ranzato O
, O
2017 O
; O
Lee B-AUTHOR
et O
al. O
, O
2017 O
; O
Kirkpatrick B-AUTHOR
et O
al. O
, O
2017 O
) O
. O
section 3
id pdf2json/2021.acl-long.378.pdf.json
the O
number O
of O
tasks O
increases O
. O
section 3
id pdf2json/2021.acl-long.378.pdf.json
We O
can O
specify O
this O
goal O
with O
an O
L0-norm O
penalty O
on O
the O
diff O
vector O
, O
R O
( O
θ O
+ O
δτ O
) O
= O
‖δτ‖0 O
= O
d∑ O
i=1 O
1 O
{ O
δτ O
, O
i O
6= O
0 O
} O
. O

section 4
id pdf2json/2021.acl-long.378.pdf.json
L0-norm O
This O
regularizer O
is O
difficult O
to O
optimize O
as O
it O
is O
nondifferentiable O
. O
section 4
id pdf2json/2021.acl-long.378.pdf.json
In O
order O
to O
approximate O
this O
L0 O
objective O
, O
we O
follow O
an O
approach O
for O
gradient-based O
learning O
with O
L0 O
sparsity O
using O
a B-PARAMETER
relaxed O
mask O
vector O
( O
Louizos O
et O
al. O
, O
2018 O
) O
. O
section 4
id pdf2json/2021.acl-long.378.pdf.json
This O
approach O
involves O
relaxing O
a B-PARAMETER
binary O
vector O
into O
continuous O
space O
, O
and O
then O
multiplying O
it O
with O
a B-PARAMETER
dense O
weight O
vector O
to O
determine O
how O
much O
of O
the O
weight O
vector O
is O
applied O
during O
training O
. O
section 4
id pdf2json/2021.acl-long.378.pdf.json
After O
training O
, O
the O
mask O
is O
made O
deterministic O
, O
and O
a B-PARAMETER
large O
portion O
of O
the O
diff O
vector O
is O
zero.4 O
To O
apply O
this O
method O
we O
first O
decompose O
δτ O
into O
a B-PARAMETER
binary O
mask O
vector O
multiplied O
with O
a B-PARAMETER
dense O
vector O
, O
δτ O
= O
zτ O
wτ O
, O
zτ O
∈ O
{ O
0 O
, O
1 O
} O
d O
, O
wτ O
∈ O
Rd O
. O
section 4
id pdf2json/2021.acl-long.378.pdf.json
We O
now O
lower O
bound O
the O
true O
objective O
and O
optimize O
an O
expectation O
with O
respect O
to O
zτ O
, O
whose O
distribution O
p O
( O
zτ O
; O
ατ O
) O
is O
initially O
Bernoulli O
with O
introduced O
parameters O
ατ O
, O
min O
ατ O
, O
wτ O
Ezτ∼p O
( O
zτ O
; O
ατ O
) O
[ O
L O
( O
Dτ O
, O
fτ O
, O
θ O
+ O
δτ O
) O
+ O
λ‖δτ‖0 O
] O
. O
section 4
id pdf2json/2021.acl-long.378.pdf.json
This O
objective O
is O
still O
complicated O
by O
the O
discrete O
nature O
of O
zτ O
’ O
s O
, O
but O
the O
expectation O
provides O
some O
guidance O
for O
empirically O
effective O
relaxations O
. O
section 4
id pdf2json/2021.acl-long.378.pdf.json
We O
follow O
prior O
work O
( O
Louizos O
et O
al. O
, O
2018 O
; O
Wang B-AUTHOR
et O
al. O
, O
2019b O
) O
and O
relax O
zτ O
into O
continuous O
space O
[ O
0 O
, O
1 O
] O
d O
with O
a B-PARAMETER
stretched O
Hard-Concrete O
distribution O
( O
Jang B-AUTHOR
et O
al. O
, O
2017 O
; O
Maddison O
et O
al. O
, O
2017 O
) O
, O
which O
allows O
for O
the O
use O
of O
pathwise O
gradient O
estimators O
. O
section 4
id pdf2json/2021.acl-long.378.pdf.json
Specifically O
, O
zτ O
is O
now O
defined O
to O
be O
a B-PARAMETER
deterministic O
and O
( O
sub O
) O
differentiable O
function O
of O
a B-PARAMETER
sample O
u O
from O
a B-PARAMETER
uniform O
distribution O
, O
u O
∼ O
U O
( O
0,1 O
) O
, O
sτ O
= O
σ O
( O
logu− O
log O
( O
1− O
u O
) O
+ O
ατ O
) O
, O
s̄τ O
= O
sτ O
× O
( O
r O
− O
l O
) O
+ O
l O
, O
zτ O
= O
min O
( O
1 O
, O
max O
( O
0 O
, O
s̄τ O
) O
) O
. O
section 4
id pdf2json/2021.acl-long.378.pdf.json
Here O
l O
< O
0 O
and O
r O
> O
1 O
are O
two O
constants O
used O
to O
stretch O
sτ O
into O
the O
interval O
( O
l O
, O
r O
) O
d O
before O
it O
is O
4It O
is O
also O
possible O
to O
learn O
sparse O
diff O
vectors O
through O
other O
penalties O
such O
as O
the O
L1-norm O
. O
section 4
id pdf2json/2021.acl-long.378.pdf.json
We O
chose O
to O
work O
with O
the O
relaxed O
L0-norm O
formulation O
as O
past O
work O
has O
shown O
that O
SGD-based O
optimization O
works O
well O
in O
this O
setting O
. O
section 4
id pdf2json/2021.acl-long.378.pdf.json
clamped O
to O
[ O
0 O
, O
1 O
] O
d O
with O
the O
min O
( O
1 O
, O
max O
( O
0 O
, O
· O
) O
) O
operation O
. O
section 4
id pdf2json/2021.acl-long.378.pdf.json
In O
this O
case O
we O
have O
a B-PARAMETER
differentiable O
closedform O
expression O
for O
the O
expected O
L0-norm O
, O
E O
[ O
‖δτ‖0 O
] O
= O
d∑ O
i=1 O
σ O
( O
ατ O
, O
i O
− O
log O
−l O
r O
) O
. O
section 4
id pdf2json/2021.acl-long.378.pdf.json
Thus O
the O
final O
optimization O
problem O
is O
given O
by O
, O
min O
ατ O
, O
wτ O
Eu∼U O
[ O
0,1 O
] O
[ O
L O
( O
Dτ O
, O
fτ O
, O
θ O
+ O
zτ O
wτ O
) O
] O
+λ O
d∑ O
i=1 O
σ O
( O
ατ O
, O
i O
− O
log O
−l O
r O
) O
, O
and O
we O
can O
now O
utilize O
pathwise O
gradient O
estimators O
to O
optimize O
the O
first O
term O
with O
respect O
to O
ατ O
since O
the O
expectation O
no O
longer O
depends O
on O
it.5 O
After O
training O
we O
obtain O
the O
final O
diff O
vector O
δτ O
by O
sampling O
u O
once O
to O
obtain O
zτ O
( O
which O
is O
not O
necessarily O
a B-PARAMETER
binary O
vector O
but O
has O
a B-PARAMETER
significant O
number O
of O
dimensions O
equal O
to O
exactly O
zero O
due O
to O
the O
clamping O
function O
) O
, O
then O
setting O
δτ O
= O
zτ O
wτ O
.6 O
3.2 O
L0-ball O
projection O
with O
magnitude O
pruning O
for O
sparsity O
control O
Differentiable O
L0 O
regularization O
allows O
us B-PARAMETER
to O
achieve O
a B-PARAMETER
high O
sparsity O
rate O
. O
section 4
id pdf2json/2021.acl-long.378.pdf.json
However O
, O
it O
would O
be O
ideal O
to O
set O
an O
exact O
sparsity O
rate O
, O
especially O
considering O
applications O
which O
require O
parameter O
budgets O
. O
section 4
id pdf2json/2021.acl-long.378.pdf.json
As O
the O
regularization O
coefficient O
λ O
is O
a B-PARAMETER
Lagrangian O
multiplier O
for O
the O
constraint O
E O
[ O
‖δτ‖0 O
] O
< O
η O
for O
some O
η O
, O
this O
could O
be O
achieved O
in O
principle O
by O
searching O
over O
different O
values O
of O
λ O
. O
section 4
id pdf2json/2021.acl-long.378.pdf.json
However O
we O
found O
it O
more O
efficient O
and O
empirically O
effective O
to O
achieve O
an O
exact O
sparsity O
rate O
by O
projecting O
onto O
a B-PARAMETER
target B-PARAMETER
L0-ball O
after O
training O
. O
section 4
id pdf2json/2021.acl-long.378.pdf.json
Specifically O
, O
we O
use O
magnitude O
pruning O
on O
the O
diff O
vector O
δτ O
and O
target B-PARAMETER
a B-PARAMETER
sparsity O
rate O
t O
% O
by O
only O
keeping O
the O
top O
t O
% O
× O
d O
values O
in O
δτ O
.7 O
Note O
that O
unlike O
standard O
magnitude O
pruning O
, O
this O
is O
based O
on O
the O
magnitude O
of O
the O
diff O
vector O
values O
and O
not O
the O
model O
parameters O
. O
section 4
id pdf2json/2021.acl-long.378.pdf.json
We O
found O
it O
important O
to O
further O
finetune O
δτ O
with O
the O
nonzero O
masks O
fixed O
to O
maintain O
good O
performance O
, O
as O
is O
often O
the O
case O
5To O
reduce O
notation O
clutter O
we O
subsume O
the O
parameters O
of O
the O
task-specific O
output O
layer O
, O
which O
is O
not O
pretrained O
, O
into O
θ O
. O
section 4
id pdf2json/2021.acl-long.378.pdf.json
We O
do O
not O
apply O
the O
L0-norm O
penalty O
on O
these O
parameters O
during O
training O
. O
section 4
id pdf2json/2021.acl-long.378.pdf.json
6We O
found O
sampling O
once O
to O
work O
as O
well O
as O
other O
alternatives O
( O
e.g O
. O
section 4
id pdf2json/2021.acl-long.378.pdf.json
based O
on O
multiple O
samples O
) O
. O
section 4
id pdf2json/2021.acl-long.378.pdf.json
7Wang O
et O
al O
. O
section 4
id pdf2json/2021.acl-long.378.pdf.json
( O
2019b O
) O
show O
that O
it O
also O
is O
possible O
to O
inject O
such O
a B-PARAMETER
constraint O
softly O
into O
the O
training O
objective O
by O
regularizing O
the O
expected O
model O
size O
towards O
a B-PARAMETER
certain O
rate O
. O
section 4
id pdf2json/2021.acl-long.378.pdf.json
However O
, O
since O
the O
constraint O
is O
soft O
this O
approach O
also O
makes O
it O
difficult O
to O
target B-PARAMETER
an O
exact O
sparsity O
rate O
. O
section 4
id pdf2json/2021.acl-long.378.pdf.json
in O
magnitude O
pruning O
( O
Han B-AUTHOR
et O
al. O
, O
2016 O
) O
. O
section 4
id pdf2json/2021.acl-long.378.pdf.json
Since O
this O
type O
of O
parameter-efficiency O
through O
projection O
onto O
the O
L0-ball O
can O
be O
applied O
without O
adaptive O
diff O
pruning,8 O
such O
an O
approach O
will O
serve O
as O
one O
of O
our O
baselines O
in O
the O
empirical O
study O
. O

section 5
id pdf2json/2021.acl-long.378.pdf.json
To O
allow O
diff O
pruning O
to O
adapt O
to O
the O
model O
architecture O
, O
we O
consider O
a B-PARAMETER
structured O
extension O
which O
incorporates O
dependence O
between O
dimensions O
. O
section 5
id pdf2json/2021.acl-long.378.pdf.json
We O
hypothesize O
that O
this O
approach O
can O
allow O
the O
model O
to O
learn O
to O
modify O
parameters O
in O
local O
regions O
, O
as O
opposed O
to O
treating O
each O
parameter O
independently O
. O
section 5
id pdf2json/2021.acl-long.378.pdf.json
We O
modify O
the O
regularizer O
to O
first O
partition O
the O
parameter O
indices O
into O
G O
groups O
{ O
g O
( O
1 O
) O
, O
. O
section 5
id pdf2json/2021.acl-long.378.pdf.json
. O
section 5
id pdf2json/2021.acl-long.378.pdf.json
. O
section 5
id pdf2json/2021.acl-long.378.pdf.json
, O
g O
( O
G O
) O
} O
where O
g O
( O
j O
) O
is O
a B-PARAMETER
subset O
of O
parameter O
indices O
governed O
by O
group O
g O
( O
j O
) O
.9 O
We O
then O
introduce O
a B-PARAMETER
scalar O
zjτ O
( O
with O
the O
associated O
parameter O
α O
j O
τ O
) O
for O
each O
group O
g O
( O
j O
) O
, O
and O
decompose O
the O
task-specific O
parameter O
for O
index O
i O
∈ O
g O
( O
j O
) O
as O
δjτ O
, O
i O
= O
zτ O
, O
i O
· O
z O
j O
τ O
·wτ O
, O
i O
. O
section 5
id pdf2json/2021.acl-long.378.pdf.json
The O
expected O
L0-norm O
is O
then O
given O
by O
E O
[ O
‖δτ‖0 O
] O
= O
G∑ O
j=1 O
∑ O
i∈g O
( O
j O
) O
E O
[ O
1 O
{ O
zτ O
, O
i O
· O
zgτ O
> O
0 O
} O
] O
= O
G∑ O
j=1 O
∑ O
i∈g O
( O
j O
) O
σ O
( O
ατ O
, O
i O
− O
log O
−l O
r O
) O
· O
σ O
( O
αjτ O
− O
log O
−l O
r O
) O
. O
section 5
id pdf2json/2021.acl-long.378.pdf.json
We O
can O
train O
with O
gradient-based O
optimization O
as O
before O
. O
section 5
id pdf2json/2021.acl-long.378.pdf.json
Parameters O
in O
a B-PARAMETER
group O
are O
encouraged O
by O
the O
regularizer O
to O
be O
removed O
jointly O
. O


section 7
id pdf2json/2021.acl-long.378.pdf.json
For O
evaluation O
we O
use O
the O
GLUE O
benchmark O
( O
Wang B-AUTHOR
et O
al. O
, O
2019b O
) O
as O
well O
as O
the O
SQuAD O
extractive B-METHOD
question O
answering O
dataset O
( O
Rajpurkar O
et O
al. O
, O
2016 O
) O
. O
section 7
id pdf2json/2021.acl-long.378.pdf.json
Following O
Adapters O
( O
Houlsby O
et O
al. O
, O
2019 O
) O
, O
we O
test O
our O
approach O
on O
the O
following O
subset O
of O
the O
GLUE O
tasks O
: O
Multi-Genre O
Natural O
Language O
Inference O
( O
MNLI B-DATASET
) O
, O
where O
the O
goal O
is O
two O
predict O
whether O
the O
relationship O
between O
two O
sentences O
is O
entailment O
, O
contradiction O
, O
or O
neutral O
( O
we O
test O
on O
both O
MNLIm O
and O
MNLImm O
which O
respectively O
tests O
on O
matched/mismatched O
domains O
) O
; O
Quora O
Question O
Pairs O
( O
QQP O
) O
, O
a B-PARAMETER
classification O
task O
to O
predict O
whether O
two O
question O
are O
semantically O
equivalent O
; O
Question O
Natural O
Language O
Inference O
( O
QNLI O
) O
, O
which O
8Concretely O
, O
one O
can O
obtain O
θτ O
through O
usual O
finetuning O
, O
set O
δτ O
= O
θτ O
− O
θ O
, O
and O
then O
apply O
magnitude O
pruning O
followed O
by O
additional O
finetuning O
on O
δτ O
. O
section 7
id pdf2json/2021.acl-long.378.pdf.json
9While O
groups O
can O
be O
defined O
in O
various O
ways O
, O
we O
found O
that O
defining O
groups O
based O
on O
each O
matrix/bias O
vector O
of O
the O
pretrained O
model O
was O
simple O
and O
worked O
well O
enough O
. O
section 7
id pdf2json/2021.acl-long.378.pdf.json
must O
predict O
whether O
a B-PARAMETER
sentence O
is O
a B-PARAMETER
correct O
answer O
to O
the O
question O
; O
Stanford B-DATASET
Sentiment I-DATASET
Treebank I-DATASET
( O
SST-2 B-DATASET
) O
, O
a B-PARAMETER
sentence O
classification O
task O
to O
predict O
the O
sentiment O
of O
movie O
reviews O
; O
Corpus O
of O
Linguistic O
Acceptability O
( O
CoLA O
) O
, O
where O
the O
goal O
is O
predict O
whether O
a B-PARAMETER
sentence O
is O
linguistically O
acceptable O
or O
not O
; O
Semantic O
Textual O
Similarity O
Benchmark O
( O
STSB O
) O
, O
which O
must O
predict O
a B-PARAMETER
similarity O
rating O
between O
two O
sentences O
; O
Microsoft O
Research O
Paraphrase O
Corpus O
( O
MRPC O
) O
, O
where O
the O
goal O
is O
to O
predict O
whether O
two O
sentences O
are O
semantically O
equivalent O
; O
Recognizing O
Textual O
Entailment O
( O
RTE O
) O
, O
which O
must O
predict O
whether O
a B-PARAMETER
second O
sentence O
is O
entailed O
by O
the O
first O
. O
section 7
id pdf2json/2021.acl-long.378.pdf.json
The O
benchmark O
uses O
Matthew O
’ O
s O
correlation O
for O
CoLA O
, O
Spearman O
for O
STS-B O
, O
F1 B-METRIC
score I-METRIC
for O
MRPC/QQP O
, O
and O
accuracy B-PARAMETER
for O
MNLI/QNLI/SST2/RTE O
. O
section 7
id pdf2json/2021.acl-long.378.pdf.json
For O
the O
main O
experiments O
and O
analysis O
, O
we O
use O
the O
BERTLARGE O
model O
from O
Devlin B-AUTHOR
et I-AUTHOR
al I-AUTHOR
. I-AUTHOR
section 7
id pdf2json/2021.acl-long.378.pdf.json
( O
2019 O
) O
to O
compare O
against O
the O
adapter-based O
approach O
of O
Houlsby O
et O
al O
. O
section 7
id pdf2json/2021.acl-long.378.pdf.json
( O
2019 O
) O
. O
section 7
id pdf2json/2021.acl-long.378.pdf.json
Our O
implementation O
is O
based O
on O
the O
Hugging B-SOFTWARE
Face I-SOFTWARE
Transformer B-METHOD
library O
( O
Wolf B-AUTHOR
et O
al. O
, O
2019 O
) O
. O

section 8
id pdf2json/2021.acl-long.378.pdf.json
We O
compare O
both O
structured O
and O
non-structured O
variants O
of O
diff O
pruning O
against O
the O
following O
baselines O
: O
Full O
finetuning O
, O
which O
fully O
finetunes O
BERTLARGE O
as O
usual O
; O
Last O
layer O
finetuning O
, O
which O
only O
finetunes O
the O
penultimate O
layer O
( O
along O
with O
the O
final O
output O
layer O
) O
10 O
; O
Adapters O
from O
Houlsby O
et O
al O
. O
section 8
id pdf2json/2021.acl-long.378.pdf.json
( O
2019 O
) O
, O
which O
train O
task-specific O
bottleneck O
layers O
between O
each O
layer O
of O
a B-PARAMETER
pretrained O
model O
, O
where O
parameter-efficiency O
can O
be O
controlled O
by O
varying O
the O
size O
of O
the O
bottleneck O
layers O
; O
and O
Non-adaptive O
diff O
pruning O
, O
which O
performs O
diff O
pruning O
just O
based O
on O
magnitude O
pruning O
( O
i.e. O
, O
we O
obtain O
θτ O
through O
usual O
finetuning O
, O
set O
δτ O
= O
θτ O
− O
θ O
, O
and O
then O
apply O
magnitude O
pruning O
followed O
by O
additional O
finetuning O
on O
δτ O
) O
. O
section 8
id pdf2json/2021.acl-long.378.pdf.json
For O
diff O
pruning O
we O
set O
our O
target B-PARAMETER
sparsity O
rate O
to O
0.5 O
% O
and O
investigate O
the O
effect O
of O
different O
target B-PARAMETER
sparsity O
rates O
in O
section O
6.1 O
. O

section 9
id pdf2json/2021.acl-long.378.pdf.json
Diff O
pruning O
introduces O
additional O
hyperparameters O
l O
, O
r O
( O
for O
stretching O
the O
Hard-Concrete O
distribution O
) O
and O
λ O
( O
for O
weighting B-METHOD
the O
approximate O
L0norm O
penalty O
) O
. O
section 9
id pdf2json/2021.acl-long.378.pdf.json
We O
found O
l O
= O
−1.5 O
, O
r O
= O
1.5 O
, O
λ O
= O
1.25 O
× O
10−7 O
to O
work O
well O
across O
all O
tasks O
. O
section 9
id pdf2json/2021.acl-long.378.pdf.json
We O
10Wu O
et O
al O
. O
section 9
id pdf2json/2021.acl-long.378.pdf.json
( O
2020 O
) O
observe O
that O
finetuning O
later O
layers O
generally O
performs O
better O
than O
finetuning O
earlier O
layers O
also O
initialize O
the O
weight O
vector O
wτ O
to O
0 O
, O
and O
ατ O
to O
a B-PARAMETER
positive O
vector O
( O
we O
use O
5 O
) O
to O
encourage O
zτ O
to O
be O
close O
to O
1 O
at O
the O
start O
of O
training.11 O
While O
we O
mainly O
experiment O
with O
BERT B-SOFTWARE
models O
to O
faciliate O
comparison O
against O
existing O
work O
, O
in O
preliminary O
experiments O
we O
found O
these O
hyperparameters O
to O
work O
for O
finetuning O
RoBERTa B-METHOD
( O
Liu B-AUTHOR
et O
al. O
, O
2019c O
) O
and O
XLNet O
( O
Yang B-AUTHOR
et O
al. O
, O
2019 O
) O
models O
as O
well O
. O
section 9
id pdf2json/2021.acl-long.378.pdf.json
For O
all O
tasks O
we O
initially O
train O
for O
3 O
epochs B-PARAMETER
and O
perform O
a B-PARAMETER
hyperparameter O
search O
over O
batch O
size O
∈ O
{ O
5 O
, O
8 O
, O
12 O
, O
16 O
} O
and O
learning B-PARAMETER
rate I-PARAMETER
∈ O
{ O
1×10−5 O
, O
2× O
10−5 O
, O
5× O
10−5 O
} O
.12 O
Finetuning O
with O
the O
fixed O
mask O
after O
projecting O
onto O
the O
L0-ball O
with O
magnitude O
pruning O
is O
done O
for O
3 O
epochs B-PARAMETER
with O
a B-PARAMETER
learning B-PARAMETER
rate I-PARAMETER
of O
5× O
10−5 O
for O
all O
datasets O
except O
for O
MRPC/STSB/RTE/SST-2 O
dataset O
, O
where O
we O
finetune O
for O
5 O
epochs B-PARAMETER
. O
section 9
id pdf2json/2021.acl-long.378.pdf.json
The O
exact O
hyperparameters O
for O
each O
task O
are O
given O
in O
section O
A.1 O
of O
the O
appendix O
. O
section 9
id pdf2json/2021.acl-long.378.pdf.json
Grouping O
for O
the O
structured O
version O
of O
diff O
pruning O
is O
based O
on O
the O
matrix/bias O
vectors O
( O
i.e O
. O
section 9
id pdf2json/2021.acl-long.378.pdf.json
parameters O
that O
belong O
to O
the O
same O
matrix O
or O
bias B-PARAMETER
vector O
are O
assumed O
to O
be O
in O
the O
same O
group O
) O
, O
which O
results O
in O
393 O
groups.13 O


section 11
id pdf2json/2021.acl-long.378.pdf.json
Our O
main O
results O
on O
the O
GLUE O
benchmark O
are O
shown O
in O
Table O
1 O
. O
section 11
id pdf2json/2021.acl-long.378.pdf.json
Structured O
diff O
pruning O
can O
match O
the O
performance O
of O
a B-PARAMETER
fully O
finetuned O
BERTLARGE O
model O
while O
only O
requiring O
0.5 O
% O
ad- O
11These O
values O
were O
found O
via O
by O
a B-PARAMETER
light O
hyperparameter O
search O
on O
the O
SST-2 B-DATASET
validation O
set O
. O
section 11
id pdf2json/2021.acl-long.378.pdf.json
12However O
we O
found O
the O
default O
settings O
used O
for O
regular O
finetuning O
as O
suggested O
in O
the O
original O
BERT B-SOFTWARE
paper O
to O
work O
well O
for O
most O
tasks O
. O
section 11
id pdf2json/2021.acl-long.378.pdf.json
13This O
definition O
of O
groups O
is O
implementation-specific O
since O
it O
depends O
on O
how O
one O
concatenates O
the O
input O
vector O
before O
each O
affine O
layer O
. O
section 11
id pdf2json/2021.acl-long.378.pdf.json
Our O
grouping O
is O
based O
on O
Hugging B-SOFTWARE
Face I-SOFTWARE
’ O
s O
BERT B-SOFTWARE
implementation O
at O
commit O
656e1386a296d696327a9db37de2ccccc79e2cc7 O
. O
section 11
id pdf2json/2021.acl-long.378.pdf.json
We O
found O
this O
simple O
definition O
to O
work O
well O
compared O
to O
alternative O
definitions O
( O
e.g O
. O
section 11
id pdf2json/2021.acl-long.378.pdf.json
based O
on O
individual O
neurons O
) O
. O
section 11
id pdf2json/2021.acl-long.378.pdf.json
ditional O
parameters O
per O
task O
. O
section 11
id pdf2json/2021.acl-long.378.pdf.json
Diff O
pruning O
without O
structured O
sparsity O
also O
performs O
well O
, O
though O
slightly O
worse O
than O
the O
structured O
approach O
. O
section 11
id pdf2json/2021.acl-long.378.pdf.json
Nonadaptive O
diff O
pruning O
, O
which O
magnitude O
prunes O
the O
diff O
vector O
without O
learning O
the O
binary O
mask O
zτ O
, O
performs O
significantly O
worse O
, O
indicating O
the O
importance O
of O
learning O
the O
masking O
vector O
. O
section 11
id pdf2json/2021.acl-long.378.pdf.json
Compared O
to O
Adapters O
, O
diff O
pruning O
obtains O
similar O
performance O
while O
requiring O
many O
fewer O
parameters O
per O
task O
, O
making O
it O
a B-PARAMETER
potential O
alternative O
for O
parameterefficient O
transfer O
learning.14 O

section 12
id pdf2json/2021.acl-long.378.pdf.json
To O
demonstrate O
the O
effectiveness O
of O
our O
approach O
beyond O
the O
GLUE O
tasks O
, O
we O
additionally O
experiment O
on O
SQuAD O
( O
Rajpurkar O
et O
al. O
, O
2016 O
) O
, O
an O
extractive B-METHOD
question O
answering O
dataset O
where O
the O
model O
has O
to O
select O
the O
answer O
span O
to O
a B-PARAMETER
question O
given O
a B-PARAMETER
Wikipedia B-DATASET
paragraph O
. O
section 12
id pdf2json/2021.acl-long.378.pdf.json
To O
make O
direct O
comparisons O
with O
Houlsby O
et O
al O
. O
section 12
id pdf2json/2021.acl-long.378.pdf.json
( O
2019 O
) O
, O
we O
run O
all O
experiments O
on O
SQuAD O
v1.1 O
. O
section 12
id pdf2json/2021.acl-long.378.pdf.json
For O
diff O
pruning O
, O
we O
use O
the O
same O
general O
hyperparameters O
as O
our O
full O
finetuning O
baseline O
( O
see O
section O
A.1 O
) O
. O
section 12
id pdf2json/2021.acl-long.378.pdf.json
As O
shown O
in O
Figure O
1 O
( O
right O
) O
, O
diff O
pruning O
is O
able O
achieve O
comparable O
or O
better O
performance O
with O
only O
1.0 O
% O
additional O
parameters O
. O
section 12
id pdf2json/2021.acl-long.378.pdf.json
Interestingly O
, O
diff O
pruning O
measurably O
improves O
the O
upon O
the O
full O
finetuning O
baseline O
while O
modifying O
fewer O
parameters O
, O
which O
indicates O
that O
diff O
pruning O
can O
have O
a B-PARAMETER
useful O
regularization O
effect O
on O
top O
of O
parameter-efficiency O
. O


section 14
id pdf2json/2021.acl-long.378.pdf.json
In O
Figure O
1 O
( O
left O
) O
, O
we O
plot O
results O
on O
the O
GLUE O
validation O
set O
averaged O
across O
all O
tasks O
at O
target B-PARAMETER
sparsity O
14Comparing O
storage O
costs O
is O
a B-PARAMETER
bit O
more O
challenging O
as O
it O
is O
implementation-specific O
. O
section 14
id pdf2json/2021.acl-long.378.pdf.json
Diff O
pruning O
incurs O
additional O
storage O
cost O
due O
to O
storing O
the O
nonzero O
positions O
of O
the O
diff O
vector O
. O
section 14
id pdf2json/2021.acl-long.378.pdf.json
See O
section O
6.6 O
for O
storage O
comparison O
against O
Adapters O
assuming O
float32 O
for O
weights B-PARAMETER
and O
int32 O
for O
positions O
. O
section 14
id pdf2json/2021.acl-long.378.pdf.json
rates O
of O
0.1 O
% O
, O
0.25 O
% O
, O
0.5 O
% O
, O
1.0 O
% O
for O
the O
different O
baselines O
. O
section 14
id pdf2json/2021.acl-long.378.pdf.json
Structured O
diff O
pruning O
consistently O
outperforms O
non-structured O
and O
and O
non-adaptive O
variants O
across O
different O
sparsity O
rates O
. O
section 14
id pdf2json/2021.acl-long.378.pdf.json
The O
advantage O
of O
adaptive O
methods O
becomes O
more O
pronounced O
at O
extreme O
sparsity O
rates O
. O
section 14
id pdf2json/2021.acl-long.378.pdf.json
In O
Table O
2 O
, O
we O
report O
the O
breakdown O
of O
accuracy B-PARAMETER
of O
structured O
diff O
pruning O
across O
different O
tasks O
and O
sparsity O
rates O
, O
where O
we O
observe O
that O
different O
tasks O
have O
different O
sensitivity O
to O
target B-PARAMETER
sparsity O
rates O
. O
section 14
id pdf2json/2021.acl-long.378.pdf.json
This O
suggests O
that O
we O
can O
obtain O
even O
greater O
parameter-efficiency O
through O
targeting O
task-specific O
sparsity O
rates O
in O
the O
diff O
vector O
. O

section 15
id pdf2json/2021.acl-long.378.pdf.json
Structured O
diff O
pruning O
introduces O
an O
additional O
mask O
per O
group O
, O
which O
encourages O
pruning O
of O
entire O
groups O
. O
section 15
id pdf2json/2021.acl-long.378.pdf.json
This O
is O
less O
restrictive O
than O
traditional O
group O
sparsity O
techniques O
that O
have O
been O
used O
with O
L0-norm O
relaxations O
, O
which O
force O
all O
parameters O
in O
a B-PARAMETER
group O
to O
share O
the O
same O
mask O
( O
Louizos O
et O
al. O
, O
2018 O
; O
Wang B-AUTHOR
et O
al. O
, O
2019b O
) O
. O
section 15
id pdf2json/2021.acl-long.378.pdf.json
However O
we O
still O
expect O
entire O
groups O
to O
be O
pruned O
out O
more O
often O
, O
which O
might O
bias B-PARAMETER
the O
learning O
process O
towards O
either O
eliminating O
completely O
or O
clustering B-METHOD
together O
nonzero O
diffs O
. O
section 15
id pdf2json/2021.acl-long.378.pdf.json
In O
Table O
3 O
, O
we O
indeed O
find O
that O
structured O
diff O
pruning O
leads O
to O
finetuned O
models O
that O
are O
much O
more O
likely O
to O
leave O
entire O
groups O
unchanged O
from O
their O
pretrained O
values O
( O
zero O
diffs O
) O
. O

section 16
id pdf2json/2021.acl-long.378.pdf.json
Different O
layers O
of O
pretrained O
models O
have O
been O
argued O
to O
encode O
different O
information O
( O
Liu B-AUTHOR
et O
al. O
, O
2019a O
; O
Tenney B-AUTHOR
et O
al. O
, O
2019 O
) O
. O
section 16
id pdf2json/2021.acl-long.378.pdf.json
Given O
that O
each O
task O
will O
likely O
recruit O
different O
kinds O
of O
language O
phenomena O
embedded O
in O
the O
hidden O
layers O
, O
we O
hypothesize O
that O
diff O
pruning O
will O
modify O
different O
parts O
of O
the O
pretrained O
model O
through O
task-specific O
finetuning O
. O
section 16
id pdf2json/2021.acl-long.378.pdf.json
Figure O
2 O
shows O
the O
percentage O
of O
nonzero O
diff O
parameters O
attributable O
to O
the O
different O
layers O
for O
each O
task O
. O
section 16
id pdf2json/2021.acl-long.378.pdf.json
We O
find O
that O
different O
tasks O
indeed O
modify O
different O
parts O
of O
the O
network O
, O
although O
there O
are O
some O
qualitative O
similarities O
between O
some O
tasks O
, O
for O
example O
between O
QNLI O
& O
QQP O
( O
both O
must O
encode O
questions O
) O
, O
and O
MRPC O
& O
STS-B O
( O
both O
must O
predict O
similarity O
between O
sentences O
) O
. O
section 16
id pdf2json/2021.acl-long.378.pdf.json
The O
embedding O
layer O
is O
very O
sparsely O
modified O
for O
all O
tasks O
. O
section 16
id pdf2json/2021.acl-long.378.pdf.json
While O
some O
of O
the O
variations O
in O
the O
sparsity O
distributions O
is O
due O
to O
simple O
randomness O
, O
we O
do O
observe O
some O
level O
of O
consistency O
over O
multiple O
runs O
of O
the O
same O
task O
, O
as O
shown O
in O
section O
A.2 O
of O
the O
appendix O
. O
section 16
id pdf2json/2021.acl-long.378.pdf.json
The O
ability O
to O
modify O
different O
parts O
of O
the O
pretrained O
model O
for O
each O
task O
could O
explain O
the O
improved O
parameter-efficiency O
of O
our O
approach O
compared O
to O
Houlsby O
et O
al O
. O
section 16
id pdf2json/2021.acl-long.378.pdf.json
( O
2019 O
) O
’ O
s O
Adapters O
, O
which O
can O
only O
read/write O
to O
the O
pretrained O
model O
at O
certain O
points O
of O
the O
computational O
graph.15 O
This O
po- O
15To O
simulate O
this O
restricted O
setting O
, O
we O
tried O
applying O
diff O
pruning O
only O
on O
the O
fully-connected O
layers O
after O
the O
selfattention O
layers O
, O
and O
observed O
much O
worse O
performance O
. O
section 16
id pdf2json/2021.acl-long.378.pdf.json
tentially O
suggests O
that O
Adapters O
with O
more O
finegrained O
access O
into O
model O
internals O
( O
e.g O
. O
section 16
id pdf2json/2021.acl-long.378.pdf.json
Adapters O
for O
key/value/query O
transformations O
) O
might O
result O
in O
even O
greater O
parameter-efficiency O
. O
section 16
id pdf2json/2021.acl-long.378.pdf.json
While O
left O
as O
future O
work O
, O
we O
also O
note O
that O
diff O
pruning O
can O
be O
applied O
in O
conjunction O
with O
Adapters O
, O
which O
might O
further O
improve O
results O
. O
section 16
id pdf2json/2021.acl-long.378.pdf.json
6.4 O
Effect O
of O
L0-ball O
projection O
Applying O
magnitude O
pruning O
to O
project O
onto O
the O
L0ball O
was O
crucial O
in O
achieving O
exact O
sparsity O
targets O
. O
section 16
id pdf2json/2021.acl-long.378.pdf.json
As O
shown O
in O
Table O
4 O
, O
we O
observed O
little O
loss O
in O
performance O
through O
this O
approach O
. O
section 16
id pdf2json/2021.acl-long.378.pdf.json
We O
reiterate O
that O
it O
was O
crucial O
to O
finetune O
with O
a B-PARAMETER
fixed O
mask O
, O
even O
for O
the O
approach O
which O
does O
not O
apply O
magnitude O
pruning.16 O

section 17
id pdf2json/2021.acl-long.378.pdf.json
Direct O
BERT B-SOFTWARE
compression O
methods O
also O
provide O
a B-PARAMETER
straightforward O
approach O
to O
parameter-efficient O
transfer B-METHOD
learning I-METHOD
. O
section 17
id pdf2json/2021.acl-long.378.pdf.json
Here O
we O
compare O
diff O
pruning O
against O
existing O
BERT B-SOFTWARE
compression O
methods O
, O
in O
particular O
DistilBERT B-METHOD
( O
Sanh B-AUTHOR
et O
al. O
, O
2019 O
) O
, O
MobileBERT O
( O
Sun B-AUTHOR
et O
al. O
, O
2020b O
) O
and O
TinyBERT O
( O
Jiao O
et O
al. O
, O
2020 O
) O
. O
section 17
id pdf2json/2021.acl-long.378.pdf.json
In O
these O
experiments O
we O
apply O
diff O
pruning O
on O
the O
smaller O
BERTBASE B-METHOD
model O
as O
these O
works O
typically O
utilize O
BERTBASE B-METHOD
as O
the O
baseline O
. O
section 17
id pdf2json/2021.acl-long.378.pdf.json
As O
shown O
in O
Table O
5 O
, O
we O
observe O
that O
diff O
pruning O
is O
more O
parameter-efficient O
when O
considering O
all O
GLUE O
tasks O
while O
maintaining O
better O
performance O
. O
section 17
id pdf2json/2021.acl-long.378.pdf.json
Of O
course O
, O
BERT B-SOFTWARE
compression O
methods O
typically O
have O
faster O
inference O
time O
( O
e.g O
. O
section 17
id pdf2json/2021.acl-long.378.pdf.json
TinyBERT4 O
is O
9.4× O
faster O
that O
BERTBASE B-METHOD
) O
. O
section 17
id pdf2json/2021.acl-long.378.pdf.json
However O
we O
note O
that O
diff O
16Without O
fixed-mask O
finetuning O
, O
GLUE O
performance O
decreases O
from O
84.9 O
to O
81.4. O
pruning O
can O
be O
applied O
on O
these O
methods O
, O
which O
may O
further O
improve O
parameter-efficiency O
while O
maintaining O
fast O
inference O
. O

section 18
id pdf2json/2021.acl-long.378.pdf.json
Finally O
, O
Table O
6 O
shows O
the O
actual O
memory O
requirements O
for O
diff O
pruning O
compared O
to O
Adapters O
for O
a B-PARAMETER
Python O
implementation O
. O
section 18
id pdf2json/2021.acl-long.378.pdf.json
While O
diff O
pruning O
requires O
storing O
positions O
in O
addition O
to O
the O
weights B-PARAMETER
( O
unlike O
Adapters O
which O
can O
just O
store O
the O
weights B-PARAMETER
) O
, O
diff O
pruning O
is O
still O
more O
storage-efficient O
due O
to O
the O
greater O
parameter-efficiency O
. O

section 19
id pdf2json/2021.acl-long.378.pdf.json
For O
training O
, O
our O
approach O
requires O
more O
memory O
than O
usual O
finetuning O
due O
to O
additionally O
optimizing O
ατ O
and O
wτ O
. O
section 19
id pdf2json/2021.acl-long.378.pdf.json
Since O
the O
majority O
of O
GPU B-SOFTWARE
memory O
is O
typically O
utilized O
by O
a B-PARAMETER
minibatch O
’ O
s O
intermediate O
layers O
, O
this O
did O
not O
present O
a B-PARAMETER
significant O
challenge O
for O
pretrained O
models O
that O
we O
experimented O
with O
in O
this O
study O
. O
section 19
id pdf2json/2021.acl-long.378.pdf.json
However O
, O
this O
could O
present O
an O
issue O
as O
model O
sizes O
get O
larger O
and O
larger O
. O
section 19
id pdf2json/2021.acl-long.378.pdf.json
After O
training O
, O
storing O
the O
task-specific O
diff O
vector O
requires O
storing O
a B-PARAMETER
compressed O
version O
with O
both O
the O
nonzero O
positions O
and O
weights B-PARAMETER
, O
which O
incurs O
additional O
storage O
requirements O
. O
section 19
id pdf2json/2021.acl-long.378.pdf.json
Finally O
, O
while O
training O
efficiency B-METRIC
was O
not O
a B-PARAMETER
primary O
concern O
of O
this O
work O
, O
diff O
pruning O
was O
also O
approximately O
1.5× O
to O
2× O
slower O
to O
train O
per O
minibatch O
than O
regular O
finetuning O
. O

section 20
id pdf2json/2021.acl-long.378.pdf.json
Multi-task O
learning O
Multi-task O
learning O
( O
Caruana O
, O
1997 O
) O
, O
broadly O
construed O
, O
aims O
to O
learn O
models O
and O
representations O
that O
can O
be O
utilized O
across O
a B-PARAMETER
diverse O
range O
of O
tasks O
, O
and O
offers O
a B-PARAMETER
natural O
approach O
to O
training O
parameter-efficient O
deep O
models O
. O
section 20
id pdf2json/2021.acl-long.378.pdf.json
Several O
works O
have O
shown O
that O
a B-PARAMETER
single O
BERT B-SOFTWARE
model O
can O
obtain O
good O
performance O
across O
multiple O
tasks O
when O
jointly O
trained O
( O
Liu B-AUTHOR
et O
al. O
, O
2019b O
; O
Clark B-AUTHOR
et O
al. O
, O
2019 O
; O
Stickland O
& O
Murray O
, O
2019 O
) O
. O
section 20
id pdf2json/2021.acl-long.378.pdf.json
An B-AUTHOR
alternative O
approach O
to O
multi-task O
learning O
that O
does O
not O
require O
access O
to O
all O
tasks O
during O
training O
involve O
training O
smaller O
task-specific O
layers O
that O
interact O
with O
a B-PARAMETER
fixed O
pretrained O
model O
( O
Rebuffi O
et O
al. O
, O
2018 O
; O
Zhang B-AUTHOR
et O
al. O
, O
2020a O
) O
. O
section 20
id pdf2json/2021.acl-long.378.pdf.json
In O
particular O
, O
Adapters O
( O
Rebuffi O
et O
al. O
, O
2018 O
) O
, O
which O
learn O
to O
read O
and O
write O
to O
layers O
of O
a B-PARAMETER
shared O
model O
, O
have O
been O
applied O
to O
obtain O
parameter-efficient O
BERT B-SOFTWARE
models O
( O
Houlsby O
et O
al. O
, O
2019 O
; O
Pfeiffer O
et O
al. O
, O
2020a O
, O
b O
, O
c B-METRIC
) O
. O
section 20
id pdf2json/2021.acl-long.378.pdf.json
In O
recent O
work O
, O
Li B-AUTHOR
& O
Liang B-AUTHOR
( O
2021 O
) O
and O
Qin B-AUTHOR
& O
Eisner B-AUTHOR
( O
2021 O
) O
explore O
the O
use O
of O
learned O
prompts O
on O
top O
of O
pretrained O
models O
to O
obtain O
task-specific O
models O
. O
section 20
id pdf2json/2021.acl-long.378.pdf.json
Yet O
another O
line O
of O
work O
targets O
extreme O
parameterefficiency O
through O
task-agnostic O
sentence O
representations O
that O
can O
be O
used O
without O
finetuning O
for O
downstream O
tasks O
( O
Le B-AUTHOR
& O
Mikolov B-AUTHOR
, O
2014 O
; O
Kiros O
et O
al. O
, O
2015 O
; O
Wieting O
et O
al. O
, O
2016 O
; O
Hill O
et O
al. O
, O
2016 O
; O
Arora O
et O
al. O
, O
2017 O
; O
Conneau B-AUTHOR
et O
al. O
, O
2017 O
; O
Cer O
et O
al. O
, O
2018 O
; O
Zhang B-AUTHOR
et O
al. O
, O
2018 O
; O
Subramanian O
et O
al. O
, O
2018 O
; O
Reimers O
& O
Gurevych B-AUTHOR
, O
2019 O
; O
Zhang B-AUTHOR
et O
al. O
, O
2020b O
) O
. O
section 20
id pdf2json/2021.acl-long.378.pdf.json
These O
feature-based O
transfer B-METHOD
learning I-METHOD
methods O
are O
however O
generally O
outperformed O
by O
fully O
finetuned O
models O
( O
Howard O
& O
Ruder O
, O
2018 O
) O
. O
section 20
id pdf2json/2021.acl-long.378.pdf.json
Model O
compression O
There O
has O
been O
much O
recent O
work O
on O
compressing O
pretrained O
trained O
with O
selfsupervision O
( O
see O
( O
Ganesh O
et O
al. O
, O
2020 O
) O
for O
a B-PARAMETER
recent O
survey O
) O
. O
section 20
id pdf2json/2021.acl-long.378.pdf.json
A O
particularly O
promising O
line O
of O
work O
focuses O
on O
obtaining O
smaller O
pretrained O
models O
( O
for O
subsequent O
finetuning O
) O
through O
weight O
pruning O
( O
Gordon O
et O
al. O
, O
2020 O
; O
Sajjad O
et O
al. O
, O
2020 O
; O
Chen B-AUTHOR
et O
al. O
, O
2020 O
) O
and/or O
knowledge O
distillation O
( O
Sanh B-AUTHOR
et O
al. O
, O
2019 O
; O
Sun B-AUTHOR
et O
al. O
, O
2019 O
; O
Turc O
et O
al. O
, O
2019 O
; O
Jiao O
et O
al. O
, O
2020 O
; O
Sun B-AUTHOR
et O
al. O
, O
2020b O
) O
. O
section 20
id pdf2json/2021.acl-long.378.pdf.json
It O
would O
be O
interesting O
to O
see O
whether O
our O
approach O
can O
be O
applied O
on O
top O
of O
these O
smaller O
pretrained O
models O
to O
for O
even O
greater O
parameter-efficiency O
. O
section 20
id pdf2json/2021.acl-long.378.pdf.json
Learning O
to O
mask O
Our O
work O
is O
closely O
related O
to O
the O
line O
of O
work O
on O
learning O
to O
mask O
parts O
of O
deep O
networks O
with O
differentiable O
relaxations O
of O
binary O
masks O
for O
model O
pruning O
and O
parameter O
sharing O
( O
Wang B-AUTHOR
et O
al. O
, O
2019b O
; O
Zhao B-AUTHOR
et O
al. O
, O
2020 O
; O
Sanh B-AUTHOR
et O
al. O
, O
2020 O
; O
Radiya-Dixit O
& O
Wang B-AUTHOR
, O
2020 O
; O
Mallya O
et O
al. O
, O
2018 O
; O
Guo B-AUTHOR
et O
al. O
, O
2019 O
; O
Sun B-AUTHOR
et O
al. O
, O
2020a O
; O
Cao B-AUTHOR
et O
al. O
, O
2021 O
) O
. O
section 20
id pdf2json/2021.acl-long.378.pdf.json
While O
these O
works O
also O
enable O
parameterefficient O
transfer B-METHOD
learning I-METHOD
, O
they O
generally O
apply O
the O
masks O
directly O
on O
the O
pretrained O
parameters O
instead O
of O
on O
the O
difference O
vector O
as O
in O
the O
present O
work O
. O
section 20
id pdf2json/2021.acl-long.378.pdf.json
Regularization O
towards O
pretrained O
models O
Finally O
, O
diff O
pruning O
is O
also O
related O
to O
works O
which O
regularize O
the O
learning O
process O
towards O
pre- O
trained/shared O
models O
for O
continual O
learning O
( O
Rusu O
et O
al. O
, O
2016 O
; O
Kirkpatrick B-AUTHOR
et O
al. O
, O
2017 O
; O
Schwarz O
et O
al. O
, O
2018 O
) O
, O
domain B-METHOD
adaptation I-METHOD
( O
Wiese O
et O
al. O
, O
2017 O
; O
Miceli O
Barone O
et O
al. O
, O
2017 O
) O
, O
and O
stable O
finetuning O
( O
Lee B-AUTHOR
et O
al. O
, O
2020 O
) O
. O
section 20
id pdf2json/2021.acl-long.378.pdf.json
These O
works O
typically O
do O
not O
utilize O
sparse O
regularizers O
and O
target B-PARAMETER
a B-PARAMETER
different O
goal O
than O
parameter-efficiency O
. O

section 21
id pdf2json/2021.acl-long.378.pdf.json
We O
propose O
diff O
pruning O
as O
a B-PARAMETER
simple O
approach O
for O
parameter-efficient O
transfer B-METHOD
learning I-METHOD
with O
pretrained O
models O
. O
section 21
id pdf2json/2021.acl-long.378.pdf.json
Experiments O
on O
standard O
NLP O
benchmarks O
and O
models O
show O
that O
diff O
pruning O
can O
match O
the O
performance O
of O
fully O
finetuned O
baselines O
while O
requiring O
only O
a B-PARAMETER
few O
additional O
parameters O
per O
task O
, O
and O
can O
sometimes O
have O
a B-PARAMETER
regularization O
effect O
and O
improve O
upon O
regular O
finetuning O
. O
section 21
id pdf2json/2021.acl-long.378.pdf.json
We O
also O
propose O
a B-PARAMETER
structured O
variant O
of O
diff O
pruning O
which O
provides O
further O
improvements O
. O
section 21
id pdf2json/2021.acl-long.378.pdf.json
Avenues O
for O
future O
work O
include O
( O
i O
) O
injecting O
parameter-efficiency O
objectives O
directly O
into O
the O
pretraining O
process O
( O
to O
pretrain O
models O
that O
are O
better O
suited O
towards O
sparse O
transfer B-METHOD
learning I-METHOD
) O
, O
and O
( O
ii O
) O
combining O
diff O
pruning O
with O
other O
techniques O
( O
e.g O
. O
section 21
id pdf2json/2021.acl-long.378.pdf.json
adapters O
, O
model O
compression O
) O
to O
achieve O
even O
greater O
parameter-efficiency O
. O

section 22
id pdf2json/2021.acl-long.378.pdf.json
The O
authors O
would O
like O
to O
thank O
the O
anonymous O
reviewers O
for O
their O
valuable O
feedback O
on O
the O
initial O
draft O
. O
section 22
id pdf2json/2021.acl-long.378.pdf.json
AMR O
was O
supported O
by O
NSF O
1704834 O
and O
NSF O
Career O
2037519 O
. O

section 23
id pdf2json/2021.acl-long.378.pdf.json
A.1 O
Hyperparameters O
Table O
7 O
shows O
hyperparameters O
we O
used O
for O
training O
GLUE O
tasks O
. O
section 23
id pdf2json/2021.acl-long.378.pdf.json
For O
SQuAD O
v1.1 O
experiments O
, O
we O
ran O
distributed O
training O
across O
8 O
GPUs O
, O
and O
used O
per O
gpu O
batch O
size O
3 O
, O
maximum B-METRIC
sequence O
length B-METRIC
384 O
, O
document O
stride O
128 O
, O
learning B-PARAMETER
rate I-PARAMETER
3× O
10−5 O
, O
number O
of O
initial O
training O
epochs B-PARAMETER
2 O
and O
number O
of O
finetuning O
epochs B-PARAMETER
2 O
. O
section 23
id pdf2json/2021.acl-long.378.pdf.json
A.2 O
Consistency O
of O
Nonzero O
Parameters O
Figure O
3 O
shows O
the O
percentage O
of O
modified O
parameters O
attributable O
to O
each O
layer O
across O
5 O
runs O
of O
SST-2 B-DATASET
. O
section 23
id pdf2json/2021.acl-long.378.pdf.json
We O
find O
that O
there O
is O
nonotrivial O
variation O
in O
sparsity O
across O
runs O
, O
but O
also O
a B-PARAMETER
degree O
of O
consistency O
. O
section 23
id pdf2json/2021.acl-long.378.pdf.json
For O
example O
, O
the O
first O
layer O
is O
modified O
considerably O
more O
than O
other O
layers O
across O
all O
runs O
. O

section TITLE
id pdf2json/2021.acl-long.285.pdf.json
MPC-BERT O
: O
A O
Pre-Trained O
Language O
Model O
for O
Multi-Party O
Conversation O
Understanding O

section ABSTRACT
id pdf2json/2021.acl-long.285.pdf.json
Recently O
, O
various O
neural O
models O
for O
multiparty O
conversation O
( O
MPC O
) O
have O
achieved O
impressive O
improvements O
on O
a B-PARAMETER
variety O
of O
tasks O
such O
as O
addressee O
recognition O
, O
speaker O
identification O
and O
response O
prediction O
. O
section ABSTRACT
id pdf2json/2021.acl-long.285.pdf.json
However O
, O
these O
existing O
methods O
on O
MPC O
usually O
represent O
interlocutors O
and O
utterances O
individually O
and O
ignore O
the O
inherent O
complicated O
structure O
in O
MPC O
which O
may O
provide O
crucial O
interlocutor O
and O
utterance O
semantics O
and O
would O
enhance O
the O
conversation O
understanding O
process O
. O
section ABSTRACT
id pdf2json/2021.acl-long.285.pdf.json
To O
this O
end O
, O
we O
present O
MPC-BERT O
, O
a B-PARAMETER
pre-trained O
model O
for O
MPC O
understanding O
that O
considers O
learning O
who O
says O
what O
to O
whom O
in O
a B-PARAMETER
unified O
model O
with O
several O
elaborated O
self-supervised O
tasks O
. O
section ABSTRACT
id pdf2json/2021.acl-long.285.pdf.json
Particularly O
, O
these O
tasks O
can O
be O
generally O
categorized O
into O
( O
1 O
) O
interlocutor O
structure O
modeling O
including O
reply-to O
utterance O
recognition O
, O
identical O
speaker O
searching O
and O
pointer O
consistency O
distinction O
, O
and O
( O
2 O
) O
utterance O
semantics O
modeling O
including O
masked O
shared O
utterance O
restoration O
and O
shared O
node O
detection O
. O
section ABSTRACT
id pdf2json/2021.acl-long.285.pdf.json
We O
evaluate O
MPCBERT O
on O
three O
downstream O
tasks O
including O
addressee O
recognition O
, O
speaker O
identification O
and O
response O
selection O
. O
section ABSTRACT
id pdf2json/2021.acl-long.285.pdf.json
Experimental O
results O
show O
that O
MPC-BERT O
outperforms O
previous O
methods O
by O
large O
margins O
and O
achieves O
new O
state-of-the-art O
performance O
on O
all O
three O
downstream O
tasks O
at O
two O
benchmarks O
. O

section 0
id pdf2json/2021.acl-long.285.pdf.json
Proceedings O
of O
the O
59th O
Annual O
Meeting O
of O
the O
Association O
for O
Computational O
Linguistics O
and O
the O
11th O
International O
Joint O
Conference O
on O
Natural O
Language O
Processing O
, O
pages O
3682–3692 O
August O
1–6 O
, O
2021 O
. O
section 0
id pdf2json/2021.acl-long.285.pdf.json
©2021 O
Association O
for O
Computational O
Linguistics O
3682 O

section 1
id pdf2json/2021.acl-long.285.pdf.json
Building O
a B-PARAMETER
conversational O
agent O
with O
intelligence O
has O
drawn O
significant O
attention B-METHOD
from O
both O
academia O
and O
industry O
. O
section 1
id pdf2json/2021.acl-long.285.pdf.json
Most O
of O
existing O
methods O
have O
studied O
understanding O
conversations O
between O
two O
participants O
, O
aiming O
to O
return O
an O
appropriate O
response O
either O
in O
a B-PARAMETER
generation-based O
( O
Shang O
et O
al. O
, O
∗Work O
done O
during O
the O
internship O
at O
Microsoft O
. O
section 1
id pdf2json/2021.acl-long.285.pdf.json
†Corresponding O
author O
. O
section 1
id pdf2json/2021.acl-long.285.pdf.json
2015 O
; O
Serban B-AUTHOR
et O
al. O
, O
2016 O
, O
2017 O
; O
Zhang B-AUTHOR
et O
al. O
, O
2018b O
, O
2020 O
) O
or O
retrieval-based O
manner O
( O
Lowe B-AUTHOR
et O
al. O
, O
2015 O
; O
Wu B-AUTHOR
et O
al. O
, O
2017 O
; O
Zhou B-AUTHOR
et O
al. O
, O
2018 O
; O
Tao B-AUTHOR
et O
al. O
, O
2019a O
, O
b O
; O
Gu O
et O
al. O
, O
2019a O
, O
b O
, O
2020 O
) O
. O
section 1
id pdf2json/2021.acl-long.285.pdf.json
Recently O
, O
researchers O
have O
paid O
more O
attention B-METHOD
to O
a B-PARAMETER
more O
practical O
and O
challenging O
scenario O
involving O
more O
than O
two O
participants O
, O
which O
is O
well O
known O
as O
multiparty O
conversation O
( O
MPC O
) O
( O
Ouchi B-AUTHOR
and O
Tsuboi B-AUTHOR
, O
2016 O
; O
Zhang B-AUTHOR
et O
al. O
, O
2018a O
; O
Le B-AUTHOR
et O
al. O
, O
2019 O
; O
Hu B-AUTHOR
et O
al. O
, O
2019 O
) O
. O
section 1
id pdf2json/2021.acl-long.285.pdf.json
Table O
1 O
shows O
an O
MPC O
example O
in O
the O
Ubuntu O
Internet O
Relay O
Chat O
( O
IRC O
) O
channel O
, O
which O
is O
composed O
of O
a B-PARAMETER
sequence O
of O
( O
speaker O
, O
utterance O
, O
addressee O
) O
triples O
. O
section 1
id pdf2json/2021.acl-long.285.pdf.json
In O
addition O
to O
returning O
an O
appropriate O
response O
, O
predicting O
who O
will O
be O
the O
next O
speaker O
( O
Meng B-AUTHOR
et O
al. O
, O
2018 O
) O
and O
who O
is O
the O
addressee O
of O
an O
utterance O
( O
Ouchi B-AUTHOR
and O
Tsuboi B-AUTHOR
, O
2016 O
; O
Zhang B-AUTHOR
et O
al. O
, O
2018a O
; O
Le B-AUTHOR
et O
al. O
, O
2019 O
) O
are O
unique O
and O
important O
issues O
in O
MPC O
. O
section 1
id pdf2json/2021.acl-long.285.pdf.json
An B-AUTHOR
instance O
of O
MPC O
always O
contains O
complicated O
interactions O
between O
interlocutors O
, O
between O
utterances O
and O
between O
an O
interlocutor O
and O
an O
utterance O
. O
section 1
id pdf2json/2021.acl-long.285.pdf.json
Therefore O
, O
it O
is O
challenging O
to O
model O
the O
conversation O
flow O
and O
fully O
understand O
the O
dialogue O
content O
. O
section 1
id pdf2json/2021.acl-long.285.pdf.json
Existing O
studies O
on O
MPC O
learn O
the O
representations O
of O
interlocutors O
and O
utterances O
with O
neural O
networks O
, O
and O
their O
representation O
spaces O
are O
either O
separate O
( O
Ouchi B-AUTHOR
and O
Tsuboi B-AUTHOR
, O
2016 O
) O
or O
interactive O
( O
Zhang B-AUTHOR
et O
al. O
, O
2018a O
) O
. O
section 1
id pdf2json/2021.acl-long.285.pdf.json
However O
, O
the O
semantics O
contained O
in O
the O
interlocutor O
and O
utterance O
representations O
may O
not O
be O
effectively O
captured O
as O
they O
are O
from O
two O
different O
representation O
spaces O
. O
section 1
id pdf2json/2021.acl-long.285.pdf.json
Recently O
, O
to O
take O
advantage O
of O
the O
breakthrough O
in O
pre-training O
language O
models O
( O
PLMs B-METHOD
) O
for O
natural O
language O
understanding O
, O
some O
studies O
proposed O
to O
integrate O
the O
speaker O
( O
Gu O
et O
al. O
, O
2020 O
) O
or O
topic B-AUTHOR
( O
Wang B-AUTHOR
et O
al. O
, O
2020 O
) O
information O
into O
PLMs B-METHOD
. O
section 1
id pdf2json/2021.acl-long.285.pdf.json
Despite O
of O
the O
performance O
improvement O
on O
response O
selection O
, O
these O
models O
still O
overlook O
the O
inherent O
relationships O
between O
utterances O
and O
interlocutors O
, O
such O
as O
“ O
address-to O
” O
. O
section 1
id pdf2json/2021.acl-long.285.pdf.json
Furthermore O
, O
most O
existing O
studies O
design O
models O
for O
each O
individual O
task O
in O
MPC O
( O
e.g. O
, O
addressee O
recognition O
, O
speaker O
identification O
and O
response O
prediction O
) O
separately O
. O
section 1
id pdf2json/2021.acl-long.285.pdf.json
Intuitively O
, O
these O
tasks O
are O
complementary O
among O
each O
other O
. O
section 1
id pdf2json/2021.acl-long.285.pdf.json
Making O
use O
of O
these O
tasks O
simultaneously O
may O
produce O
better O
contextualized O
representations O
of O
interlocutors O
and O
utterances O
, O
and O
would O
enhance O
the O
conversation O
understanding O
, O
but O
is O
neglected O
in O
previous O
studies O
. O
section 1
id pdf2json/2021.acl-long.285.pdf.json
On O
account O
of O
above O
issues O
, O
we O
propose O
MPCBERT O
which O
jointly O
learns O
who O
says O
what O
to O
whom O
in O
MPC O
by O
designing O
self-supervised O
tasks O
for O
PLMs B-METHOD
, O
so B-PARAMETER
as O
to O
improve O
the O
ability O
of O
PLMs B-METHOD
on O
MPC O
understanding O
. O
section 1
id pdf2json/2021.acl-long.285.pdf.json
Specifically O
, O
the O
five O
designed O
tasks O
includes O
reply-to O
utterance O
recognition O
, O
identical O
speaker O
searching O
, O
pointer O
consistency O
distinction O
, O
masked O
shared O
utterance O
restoration O
and O
shared O
node O
detection O
. O
section 1
id pdf2json/2021.acl-long.285.pdf.json
The O
first O
three O
tasks O
are O
designed O
to O
model O
the O
interlocutor O
structure O
in O
MPC O
in O
a B-PARAMETER
semantics-to-structure O
manner O
. O
section 1
id pdf2json/2021.acl-long.285.pdf.json
In O
the O
output O
of O
MPC-BERT O
, O
an O
interlocutor O
is O
described O
through O
the O
encoded O
representations O
of O
the O
utterances O
it O
says O
. O
section 1
id pdf2json/2021.acl-long.285.pdf.json
Thus O
, O
the O
representations O
of O
utterance O
semantics O
are O
utilized O
to O
construct O
the O
conversation O
structure O
in O
these O
three O
tasks O
. O
section 1
id pdf2json/2021.acl-long.285.pdf.json
On O
the O
other O
hand O
, O
the O
last O
two O
tasks O
are O
designed O
to O
model O
the O
utterance O
semantics O
in O
a B-PARAMETER
structure-to-semantics O
manner O
. O
section 1
id pdf2json/2021.acl-long.285.pdf.json
Intuitively O
, O
the O
conversation O
structure O
influences O
the O
information O
flow O
in O
MPC O
. O
section 1
id pdf2json/2021.acl-long.285.pdf.json
Thus O
, O
the O
structure O
information O
can O
also O
be O
used O
to O
strengthen O
the O
representations O
of O
utterance O
semantics O
in O
return O
. O
section 1
id pdf2json/2021.acl-long.285.pdf.json
In O
general O
, O
these O
five O
self-supervised O
tasks O
are O
employed O
to O
jointly O
train O
the O
MPC-BERT O
in O
a B-PARAMETER
multi-task O
learning O
framework O
, O
which O
helps O
the O
model O
to O
learn O
the O
complementary O
information O
among O
interlocutors O
and O
utterances O
, O
and O
that O
between O
structure O
and O
semantics O
. O
section 1
id pdf2json/2021.acl-long.285.pdf.json
By O
this O
means O
, O
MPC-BERT O
can O
produce O
better O
interlocutor O
and O
utterance O
representations O
which O
can O
be O
effectively O
generalized O
to O
multiple O
downstream O
tasks O
of O
MPC O
. O
section 1
id pdf2json/2021.acl-long.285.pdf.json
To O
measure O
the O
effectiveness O
of O
these O
selfsupervised O
tasks O
and O
to O
test O
the O
generalization O
ability O
of O
MPC-BERT O
, O
we O
evaluate O
it O
on O
three O
downstream O
tasks O
including O
addressee O
recognition O
, O
speaker O
identification O
and O
response O
selection O
, O
which O
are O
three O
core O
research O
issues O
of O
MPC O
. O
section 1
id pdf2json/2021.acl-long.285.pdf.json
Two O
benchmarks O
based O
on O
Ubuntu O
IRC O
channel O
are O
employed O
for O
evaluation O
. O
section 1
id pdf2json/2021.acl-long.285.pdf.json
One O
was O
released O
by O
Hu B-AUTHOR
et I-AUTHOR
al I-AUTHOR
. O
section 1
id pdf2json/2021.acl-long.285.pdf.json
( O
2019 O
) O
. O
section 1
id pdf2json/2021.acl-long.285.pdf.json
The O
other O
was O
released O
by O
Ouchi B-AUTHOR
and O
Tsuboi B-AUTHOR
( O
2016 O
) O
and O
has O
three O
experimental O
settings O
according O
to O
session O
lengths O
. O
section 1
id pdf2json/2021.acl-long.285.pdf.json
Experimental O
results O
show O
that O
MPC-BERT O
outperforms O
the O
current O
state-of-the-art O
models O
by O
margins O
of O
3.51 O
% O
, O
2.86 O
% O
, O
3.28 O
% O
and O
5.36 O
% O
on O
the O
test O
sets O
of O
these O
two O
benchmarks O
respectively O
in O
terms O
of O
the O
session O
accuracy B-PARAMETER
of O
addressee O
recognition O
, O
by O
margins O
of O
7.66 O
% O
, O
2.60 O
% O
, O
3.38 O
% O
and O
4.24 O
% O
respectively O
in O
terms O
of O
the O
utterance O
precision B-METRIC
of O
speaker O
identification O
, O
and O
by O
margins O
of O
3.82 O
% O
, O
2.71 O
% O
, O
2.55 O
% O
and O
3.22 O
% O
respectively O
in O
terms O
of O
the O
response O
recall B-METRIC
of O
response O
selection O
. O
section 1
id pdf2json/2021.acl-long.285.pdf.json
In O
summary O
, O
our O
contributions O
in O
this O
paper O
are O
three-fold O
: O
( O
1 O
) O
MPC-BERT O
, O
a B-PARAMETER
PLM O
for O
MPC O
understanding O
, O
is O
proposed O
by O
designing O
five O
selfsupervised O
tasks O
based O
on O
the O
interactions O
among O
utterances O
and O
interlocutors O
. O
section 1
id pdf2json/2021.acl-long.285.pdf.json
( O
2 O
) O
Three O
downstream O
tasks O
are O
employed O
to O
comprehensively O
evaluate O
the O
effectiveness O
of O
our O
designed O
self-supervised O
tasks O
and O
the O
generalization O
ability O
of O
MPC-BERT O
. O
section 1
id pdf2json/2021.acl-long.285.pdf.json
( O
3 O
) O
Our O
proposed O
MPC-BERT O
achieves O
new O
state-ofthe-art O
performance O
on O
all O
three O
downstream O
tasks O
at O
two O
benchmarks O
. O

section 2
id pdf2json/2021.acl-long.285.pdf.json
Existing O
methods O
on O
building O
dialogue B-SOFTWARE
systems I-SOFTWARE
can O
be O
generally O
categorized O
into O
studying O
twoparty O
conversations O
and O
multi-party O
conversations O
( O
MPC O
) O
. O
section 2
id pdf2json/2021.acl-long.285.pdf.json
In O
this O
paper O
, O
we O
study O
MPC O
. O
section 2
id pdf2json/2021.acl-long.285.pdf.json
In O
addition O
to O
predicting O
utterances O
, O
identifying O
the O
speaker O
and O
recognizing O
the O
addressee O
of O
an O
utterance O
are O
also O
important O
tasks O
for O
MPC O
. O
section 2
id pdf2json/2021.acl-long.285.pdf.json
Ouchi B-AUTHOR
and O
Tsuboi B-AUTHOR
( O
2016 O
) O
first O
proposed O
the O
task O
of O
addressee O
and O
response O
selection O
and O
created O
an O
MPC O
corpus O
for O
studying O
this O
task O
. O
section 2
id pdf2json/2021.acl-long.285.pdf.json
Zhang B-AUTHOR
et I-AUTHOR
al I-AUTHOR
. O
section 2
id pdf2json/2021.acl-long.285.pdf.json
( O
2018a O
) O
proposed O
SI-RNN B-METHOD
, O
which O
updated O
speaker O
embeddings O
role-sensitively O
for O
addressee O
and O
response O
selection O
. O
section 2
id pdf2json/2021.acl-long.285.pdf.json
Meng B-AUTHOR
et O
al O
. O
section 2
id pdf2json/2021.acl-long.285.pdf.json
( O
2018 O
) O
proposed O
a B-PARAMETER
task O
of O
speaker O
classification O
as O
a B-PARAMETER
surrogate O
task O
for O
speaker O
modeling O
. O
section 2
id pdf2json/2021.acl-long.285.pdf.json
Le B-AUTHOR
et O
al O
. O
section 2
id pdf2json/2021.acl-long.285.pdf.json
( O
2019 O
) O
proposed O
a B-PARAMETER
who-to-whom O
( O
W2W O
) O
model O
to O
recognize O
the O
addressees O
of O
all O
utterances O
. O
section 2
id pdf2json/2021.acl-long.285.pdf.json
Hu B-AUTHOR
et I-AUTHOR
al I-AUTHOR
. O
section 2
id pdf2json/2021.acl-long.285.pdf.json
( O
2019 O
) O
proposed O
a B-PARAMETER
graph-structured O
network O
( O
GSN O
) O
to O
model O
the O
graphical O
information O
flow O
for O
response O
generation O
. O
section 2
id pdf2json/2021.acl-long.285.pdf.json
Wang B-AUTHOR
et O
al O
. O
section 2
id pdf2json/2021.acl-long.285.pdf.json
( O
2020 O
) O
proposed O
to O
track O
the O
dynamic O
topic B-AUTHOR
for O
response O
selection O
. O
section 2
id pdf2json/2021.acl-long.285.pdf.json
Generally O
speaking O
, O
previous O
studies O
on O
MPC O
can O
not O
unify O
the O
representations O
of O
interlocutors O
and O
utterances O
effectively O
. O
section 2
id pdf2json/2021.acl-long.285.pdf.json
Also O
, O
they O
are O
limited O
to O
each O
individual O
task O
, O
ignoring O
the O
complementary O
information O
among O
different O
tasks O
. O
section 2
id pdf2json/2021.acl-long.285.pdf.json
To O
the O
best O
of O
our O
knowledge O
, O
this O
paper O
makes O
the O
first O
attempt O
to O
design O
various O
self-supervised O
tasks O
for O
building O
PLMs B-METHOD
aiming O
at O
MPC O
understanding O
, O
and O
to O
evaluate O
the O
performance O
of O
PLMs B-METHOD
on O
three O
downstream O
tasks O
as O
comprehensively O
as O
possible O
. O

section 3
id pdf2json/2021.acl-long.285.pdf.json
An B-AUTHOR
MPC O
instance O
is O
composed O
of O
a B-PARAMETER
sequence O
of O
( O
speaker O
, O
utterance O
, O
addressee O
) O
triples O
, O
denoted O
as O
{ O
( O
sn O
, O
un O
, O
an O
) O
} O
Nn=1 O
, O
where O
N O
is O
the O
number O
of O
turns O
in O
the O
conversation O
. O
section 3
id pdf2json/2021.acl-long.285.pdf.json
Our O
goal O
is O
to O
build O
a B-PARAMETER
pre-trained O
language O
model O
for O
universal O
MPC O
understanding O
. O
section 3
id pdf2json/2021.acl-long.285.pdf.json
Given O
a B-PARAMETER
conversation O
, O
this O
model O
is O
expected O
to O
produce O
embedding O
vectors O
for O
all O
utterances O
which O
contain O
not O
only O
the O
semantic B-METHOD
information O
of O
each O
utterance O
, O
but O
also O
the O
speaker O
and O
addressee O
structure O
of O
the O
whole O
conversation O
. O
section 3
id pdf2json/2021.acl-long.285.pdf.json
Thus O
, O
it O
can O
be O
effectively O
adapted O
to O
various O
downstream O
tasks O
by O
fine-tuning O
model O
parameters O
. O

section 4
id pdf2json/2021.acl-long.285.pdf.json
In O
this O
paper O
, O
BERT B-SOFTWARE
( O
Devlin B-AUTHOR
et O
al. O
, O
2019 O
) O
is O
chosen O
as O
the O
backbone O
of O
our O
PLM O
for O
MPC O
. O
section 4
id pdf2json/2021.acl-long.285.pdf.json
Thus O
, O
we O
name O
it O
MPC-BERT O
. O
section 4
id pdf2json/2021.acl-long.285.pdf.json
It O
is O
worth O
noting O
that O
our O
proposed O
self-supervised O
tasks O
for O
training O
MPCBERT O
can O
also O
be O
applied O
to O
other O
types O
of O
PLMs B-METHOD
. O
section 4
id pdf2json/2021.acl-long.285.pdf.json
We O
first O
give O
an O
overview O
of O
the O
input O
representations O
and O
the O
overall O
architectures O
of O
MPC-BERT O
. O
section 4
id pdf2json/2021.acl-long.285.pdf.json
When O
constructing O
the O
input O
representations O
, O
in O
order O
to O
consider O
the O
speaker O
information O
of O
each O
utterance O
, O
speaker O
embeddings O
( O
Gu O
et O
al. O
, O
2020 O
) O
are O
introduced O
as O
shown O
in O
Figure O
1 O
. O
section 4
id pdf2json/2021.acl-long.285.pdf.json
Considering O
that O
the O
set O
of O
interlocutors O
are O
inconsistent O
in O
different O
conversations O
, O
a B-PARAMETER
position-based O
interlocutor O
embedding O
table O
is O
initialized O
randomly O
at O
first O
and O
updated O
during O
pre-training O
, O
which O
means O
each O
interlocutor O
in O
a B-PARAMETER
conversation O
is O
assigned O
with O
an O
embedding O
vector O
according O
to O
the O
order O
it O
appears O
in O
the O
conversation O
. O
section 4
id pdf2json/2021.acl-long.285.pdf.json
Then O
, O
the O
speaker O
embeddings O
for O
each O
utterance O
can O
be O
derived O
by O
looking O
up O
this O
embedding O
table O
. O
section 4
id pdf2json/2021.acl-long.285.pdf.json
The O
speaker O
embeddings O
are O
combined O
with O
standard O
token O
, O
position O
and O
segmentation O
embeddings O
and O
are O
then O
encoded O
by O
BERT B-SOFTWARE
. O
section 4
id pdf2json/2021.acl-long.285.pdf.json
The O
output O
embeddings O
of O
BERT B-SOFTWARE
corresponding O
to O
different O
input O
tokens O
are O
utilized O
by O
different O
self-supervised O
tasks O
for O
further O
calculation O
. O

section 5
id pdf2json/2021.acl-long.285.pdf.json
The O
first O
three O
tasks O
follow O
the O
semantics-tostructure O
manner O
. O
section 5
id pdf2json/2021.acl-long.285.pdf.json
In O
MPC-BERT O
, O
each O
interlocutor O
is O
described O
through O
the O
encoded O
representations O
of O
the O
utterances O
it O
says O
. O
section 5
id pdf2json/2021.acl-long.285.pdf.json
Thus O
, O
the O
representations O
of O
utterance O
semantics O
are O
utilized O
to O
construct O
the O
conversation O
structure O
. O
section 5
id pdf2json/2021.acl-long.285.pdf.json
Figure O
1 O
shows O
the O
input O
representations O
and O
the O
model O
architectures O
of O
these O
three O
tasks O
. O
section 5
id pdf2json/2021.acl-long.285.pdf.json
A O
[ O
CLS O
] O
token O
is O
inserted O
at O
the O
start O
of O
each O
utterance O
, O
denoting O
its O
utterancelevel O
representation O
. O
section 5
id pdf2json/2021.acl-long.285.pdf.json
Then O
, O
all O
utterances O
in O
a B-PARAMETER
conversation O
are O
concatenated O
and O
a B-PARAMETER
[ O
SEP O
] O
token O
is O
inserted O
at O
the O
end O
of O
the O
whole O
sequence O
. O
section 5
id pdf2json/2021.acl-long.285.pdf.json
It O
is O
notable O
that O
these O
three O
tasks O
share O
the O
same O
form O
of O
input O
data O
. O
section 5
id pdf2json/2021.acl-long.285.pdf.json
Thus O
, O
the O
input O
only O
needs O
to O
be O
encoded O
once O
by O
BERT B-SOFTWARE
while O
the O
output O
can O
be O
fed B-METRIC
into O
three O
tasks O
, O
which O
is O
computation-efficient O
. O
section 5
id pdf2json/2021.acl-long.285.pdf.json
As O
shown O
in O
Figure O
1 O
, O
a B-PARAMETER
task-dependent O
non-linear O
transformation O
layer O
is O
placed O
on O
top O
of O
BERT B-SOFTWARE
in O
order O
to O
adapt O
the O
output O
of O
BERT B-SOFTWARE
to O
different O
tasks O
. O
section 5
id pdf2json/2021.acl-long.285.pdf.json
We O
will O
describe O
the O
details O
of O
these O
tasks O
as O
follows O
. O

section 6
id pdf2json/2021.acl-long.285.pdf.json
To O
enable O
the O
model O
to O
recognize O
the O
addressee O
of O
each O
utterance O
, O
a B-PARAMETER
self-supervised O
task O
named O
replyto O
utterance O
recognition O
( O
RUR O
) O
is O
proposed O
to O
learn O
which O
preceding O
utterance O
the O
current O
utterance O
replies O
to O
. O
section 6
id pdf2json/2021.acl-long.285.pdf.json
After O
encoded O
by O
BERT B-SOFTWARE
, O
we O
extract O
the O
contextualized O
representations O
for O
each O
[ O
CLS O
] O
token O
representing O
individual O
utterances O
. O
section 6
id pdf2json/2021.acl-long.285.pdf.json
Next O
, O
a B-PARAMETER
non-linear O
transformation O
followed O
by O
a B-PARAMETER
layer B-METHOD
normalization I-METHOD
are O
performed O
to O
derive O
the O
utterance O
representations O
for O
this O
specific O
task O
{ O
ururi O
} O
Ni=1 O
, O
where O
ururi O
∈ O
Rd O
and O
d O
= O
768 B-PARAMETER
. O
section 6
id pdf2json/2021.acl-long.285.pdf.json
Then O
, O
for O
a B-PARAMETER
specific O
utterance O
Ui O
, O
its O
matching O
scores O
with O
all O
its O
preceding O
utterances O
are O
calculated O
as O
mij O
= O
softmax B-METRIC
( O
urur O
> O
i O
· O
Arur O
· O
ururj O
) O
, O
( O
1 O
) O
where O
Arur O
∈ O
Rd×d O
is O
a B-PARAMETER
linear B-METHOD
transformation I-METHOD
, O
mij O
denotes O
the O
matching O
degree O
of O
Uj O
being O
the O
replyto O
utterance O
of O
Ui O
, O
and O
1 O
≤ O
j O
< O
i O
. O
section 6
id pdf2json/2021.acl-long.285.pdf.json
We O
construct O
a B-PARAMETER
set O
S O
by O
sampling O
a B-PARAMETER
certain O
number O
of O
utterances O
in O
a B-PARAMETER
conversation O
and O
this O
recognition O
operation O
is O
performed O
for O
each O
utterance O
in O
S. O
Meanwhile O
, O
a B-PARAMETER
dynamic O
sampling O
strategy O
is O
adopted O
so B-PARAMETER
that O
models O
can O
see O
more O
samples O
. O
section 6
id pdf2json/2021.acl-long.285.pdf.json
Finally O
, O
the O
pretraining O
objective O
of O
this O
self-supervised O
task O
is O
to O
minimize O
the O
cross-entropy B-METRIC
loss I-METRIC
as O
Lrur O
= O
− O
∑ O
i∈S O
i−1∑ O
j=1 O
yij O
log O
( O
mij O
) O
, O
( O
2 O
) O
where O
yij O
= O
1 O
if O
Uj O
is O
the O
reply-to O
utterance O
of O
Ui O
and O
yij O
= O
0 O
otherwise O
. O

section 7
id pdf2json/2021.acl-long.285.pdf.json
Having O
knowledge O
of O
who O
is O
the O
speaker O
of O
an O
utterance O
is O
also O
important O
for O
MPC O
. O
section 7
id pdf2json/2021.acl-long.285.pdf.json
The O
task O
of O
identical O
speaker O
searching O
( O
ISS O
) O
is O
designed O
by O
masking O
the O
speaker O
embedding O
of O
a B-PARAMETER
specific O
utterance O
in O
the O
input O
representation O
, O
and O
aims O
to O
predict O
its O
speaker O
given O
the O
conversation O
. O
section 7
id pdf2json/2021.acl-long.285.pdf.json
Since O
the O
set O
of O
interlocutors O
vary O
across O
conversations O
, O
the O
task O
of O
predicting O
the O
speaker O
of O
an O
utterance O
is O
reformulated O
as O
searching O
for O
the O
utterances O
sharing O
the O
identical O
speaker O
. O
section 7
id pdf2json/2021.acl-long.285.pdf.json
First O
, O
for O
a B-PARAMETER
specific O
utterance O
, O
its O
speaker O
embedding O
is O
masked O
with O
a B-PARAMETER
special O
[ O
Mask O
] O
interlocutor O
embedding O
to O
avoid O
information O
leakage O
. O
section 7
id pdf2json/2021.acl-long.285.pdf.json
Given O
the O
utterance O
representations O
for O
this O
specific O
task O
{ O
uissi O
} O
Ni=1 O
where O
uissi O
∈ O
Rd O
, O
the O
matching O
scores O
of O
Ui O
with O
all O
its O
preceding O
utterances O
are O
calculated O
similarly O
with O
Eq O
. O
section 7
id pdf2json/2021.acl-long.285.pdf.json
( O
1 O
) O
. O
section 7
id pdf2json/2021.acl-long.285.pdf.json
Here O
, O
mij O
denotes O
the O
matching O
degree O
of O
Uj O
sharing O
the O
same O
speaker O
with O
Ui O
. O
section 7
id pdf2json/2021.acl-long.285.pdf.json
For O
each O
instance O
in O
the O
dynamic O
sampling O
set O
S O
, O
there O
must O
be O
an O
utterance O
in O
previous O
turns O
sharing O
the O
same O
speaker O
. O
section 7
id pdf2json/2021.acl-long.285.pdf.json
Otherwise O
, O
it O
is O
removed O
out O
of O
the O
set O
. O
section 7
id pdf2json/2021.acl-long.285.pdf.json
Finally O
, O
the O
pre-training O
objective O
of O
this O
task O
is O
to O
minimize O
the O
cross-entropy B-METRIC
loss I-METRIC
similarly O
with O
Eq O
. O
section 7
id pdf2json/2021.acl-long.285.pdf.json
( O
2 O
) O
. O
section 7
id pdf2json/2021.acl-long.285.pdf.json
Here O
, O
yij O
= O
1 O
if O
Uj O
shares O
the O
same O
speaker O
with O
Ui O
and O
yij O
= O
0 O
otherwise O
. O

section 8
id pdf2json/2021.acl-long.285.pdf.json
We O
design O
a B-PARAMETER
task O
named O
pointer O
consistency O
distinction O
( O
PCD O
) O
to O
jointly O
model O
speakers O
and O
addressees O
in O
MPC O
. O
section 8
id pdf2json/2021.acl-long.285.pdf.json
In O
this O
task O
, O
a B-PARAMETER
pair O
of O
utterances O
representing O
the O
“ O
reply-to O
” O
relationship O
is O
defined O
as O
a B-PARAMETER
speaker-to-addressee O
pointer O
. O
section 8
id pdf2json/2021.acl-long.285.pdf.json
Here O
, O
we O
assume O
that O
the O
representations O
of O
two O
pointers O
directing O
from O
the O
same O
speaker O
to O
the O
same O
addressee O
should O
be O
consistent O
. O
section 8
id pdf2json/2021.acl-long.285.pdf.json
As O
illustrated O
in O
Figure O
2 O
( O
a B-PARAMETER
) O
, O
speaker O
Sm O
speaks O
Ui O
and O
Uj O
which O
reply O
to O
Ui′ O
and O
Uj′ O
from O
speaker O
Sn O
respectively O
. O
section 8
id pdf2json/2021.acl-long.285.pdf.json
Thus O
, O
the O
utterance O
tuples O
( O
Ui O
, O
Ui′ O
) O
and O
( O
Uj O
, O
Uj′ O
) O
both O
represent O
the O
pointer O
of O
Sm-to-Sn O
and O
their O
pointer O
representations O
should O
be O
consistent.. O
section 8
id pdf2json/2021.acl-long.285.pdf.json
Given O
the O
utterance O
representations O
for O
this O
specific O
task O
{ O
upcdi O
} O
Ni=1 O
where O
u O
pcd O
i O
∈ O
Rd O
, O
we O
first O
capture O
the O
pointer O
information O
contained O
in O
each O
utterance O
tuple O
. O
section 8
id pdf2json/2021.acl-long.285.pdf.json
The O
element-wise O
difference O
and O
multiplication O
between O
an O
utterance O
tuple O
( O
Ui O
, O
Ui′ O
) O
are O
computed O
and O
are O
concatenated O
as O
pii′ O
= O
[ O
u O
pcd O
i O
− O
u O
pcd O
i′ O
; O
u O
pcd O
i O
u O
pcd O
i′ O
] O
, O
( O
3 O
) O
where O
pii′ O
∈ O
R2d O
. O
section 8
id pdf2json/2021.acl-long.285.pdf.json
Then O
, O
we O
compress O
pii′ O
and O
obtain O
the O
pointer O
representation O
p̄ii′ O
as O
p̄ii′ O
= O
ReLU B-METHOD
( O
pii′ O
·Wpcd O
+ O
bpcd O
) O
, O
( O
4 O
) O
where O
Wpcd O
∈ O
R2d×d O
and O
bpcd O
∈ O
Rd O
are O
parameters O
. O
section 8
id pdf2json/2021.acl-long.285.pdf.json
Identically O
, O
a B-PARAMETER
consistent O
pointer O
representations O
p̄jj′ O
and O
an O
inconsistent O
one O
p̄kk′ O
sampled O
from O
this O
conversation O
are O
obtained O
. O
section 8
id pdf2json/2021.acl-long.285.pdf.json
The O
similarities O
between O
every O
two O
pointers O
are O
calculated O
as O
mij O
= O
sigmoid B-METHOD
( O
p̄ O
> O
ii′ O
· O
Apcd O
· O
p̄jj′ O
) O
, O
( O
5 O
) O
where O
mij O
denotes O
the O
matching O
degree O
of O
pointer O
p̄ii′ O
being O
consistent O
with O
pointer O
p̄jj′ O
. O
section 8
id pdf2json/2021.acl-long.285.pdf.json
mik O
can O
be O
derived O
accordingly O
. O
section 8
id pdf2json/2021.acl-long.285.pdf.json
Finally O
, O
the O
pre-training O
objective O
of O
this O
task O
is O
to O
minimize O
the O
hinge O
loss O
which O
enforces O
mij O
to O
be O
larger O
than O
mik O
by O
at O
least O
a B-PARAMETER
margin O
∆ O
as O
Lpcd O
= O
max O
{ O
0 O
, O
∆−mij O
+ O
mik O
} O
. O
section 8
id pdf2json/2021.acl-long.285.pdf.json
( O
6 O
) O

section 9
id pdf2json/2021.acl-long.285.pdf.json
Intuitively O
, O
the O
conversation O
structure O
might O
influence O
the O
information O
flow O
, O
so B-PARAMETER
that O
it O
can O
be O
used O
to O
strengthen O
the O
representations O
of O
utterance O
semantics O
. O
section 9
id pdf2json/2021.acl-long.285.pdf.json
Thus O
, O
two O
self-supervised O
tasks O
following O
the O
structure-to-semantics O
manner O
are O
designed O
. O

section 10
id pdf2json/2021.acl-long.285.pdf.json
There O
are O
usually O
several O
utterances O
replying-to O
a B-PARAMETER
shared O
utterance O
in O
MPC O
. O
section 10
id pdf2json/2021.acl-long.285.pdf.json
Intuitively O
, O
a B-PARAMETER
shared O
utterance O
is O
semantically O
relevant O
to O
more O
utterances O
in O
the O
context O
than O
non-shared O
ones O
. O
section 10
id pdf2json/2021.acl-long.285.pdf.json
Based O
on O
this O
characteristic O
, O
we O
design O
a B-PARAMETER
task O
named O
masked O
shared O
utterance O
restoration O
( O
MSUR O
) O
. O
section 10
id pdf2json/2021.acl-long.285.pdf.json
We O
first O
randomly O
sample O
an O
utterance O
from O
all O
shared O
utterances O
in O
a B-PARAMETER
conversation O
and O
all O
tokens O
in O
this O
sampled O
utterance O
are O
masked O
with O
a B-PARAMETER
[ O
MASK O
] O
token O
. O
section 10
id pdf2json/2021.acl-long.285.pdf.json
Then O
the O
model O
is O
enforced O
to O
restore O
the O
masked O
utterance O
given O
the O
rest O
conversation O
. O
section 10
id pdf2json/2021.acl-long.285.pdf.json
Formally O
, O
assuming O
Ui O
as O
the O
masked O
shared O
utterance O
and O
li B-AUTHOR
as O
the O
number O
of O
tokens O
in O
Ui O
. O
section 10
id pdf2json/2021.acl-long.285.pdf.json
Given O
the O
token O
representations O
for O
this O
task O
{ O
umsuri O
, O
t O
} O
li B-AUTHOR
t=1 O
where O
umsuri O
, O
t O
∈ O
Rd O
, O
the O
probability O
distribution O
of O
each O
masked O
token O
can O
be O
calculated O
as O
pui O
, O
t O
= O
softmax B-METRIC
( O
u O
msur O
i O
, O
t O
·Wmsur O
+ O
bmsur O
) O
, O
( O
7 O
) O
where O
Wmsur O
∈ O
Rd×V O
is O
the O
token O
embedding O
table O
, O
V O
denotes O
the O
vocabulary B-METRIC
size I-METRIC
, O
and O
bmsur O
∈ O
RV O
is O
a B-PARAMETER
bias B-PARAMETER
vector O
. O
section 10
id pdf2json/2021.acl-long.285.pdf.json
Finally O
, O
the O
pre-training O
objective O
of O
this O
self-supervised O
task O
is O
to O
minimize O
the O
negative B-METHOD
log-likelihood B-PARAMETER
loss O
as O
Lmsur O
= O
− O
1 O
li B-AUTHOR
li∑ O
t=1 O
log O
pui O
, O
t O
, O
( O
8 O
) O
where O
pui O
, O
t O
is O
the O
element O
in O
pui O
, O
t O
corresponding O
to O
the O
original O
token O
. O

section 11
id pdf2json/2021.acl-long.285.pdf.json
A O
full O
MPC O
instance O
can O
be O
divided O
into O
several O
sub-conversations O
and O
we O
assume O
that O
the O
representations O
of O
sub-conversations O
under O
the O
same O
parent O
node O
tend O
to O
be O
similar O
. O
section 11
id pdf2json/2021.acl-long.285.pdf.json
As O
illustrated O
in O
Figure O
2 O
( O
b O
) O
, O
two O
sub-conversations O
{ O
U3 O
, O
U5 O
, O
U7 O
, O
U8 O
} O
and O
{ O
U4 O
, O
U6 O
, O
U9 O
} O
share O
the O
same O
parent O
node O
U2 O
. O
section 11
id pdf2json/2021.acl-long.285.pdf.json
Thus O
, O
they O
should O
be O
semantically O
relevant O
. O
section 11
id pdf2json/2021.acl-long.285.pdf.json
Under O
this O
assumption O
, O
we O
design O
a B-PARAMETER
self-supervised O
task O
named O
shared O
node O
detection O
( O
SND O
) O
, O
which O
utilizes O
the O
conversation O
structure O
to O
strengthen O
the O
capability O
of O
models O
on O
measuring O
the O
semantic B-METHOD
relevance O
of O
two O
sub-conversations O
. O
section 11
id pdf2json/2021.acl-long.285.pdf.json
We O
first O
construct O
the O
pre-training O
samples O
for O
this O
task O
. O
section 11
id pdf2json/2021.acl-long.285.pdf.json
Empirically O
, O
only O
the O
sub-conversations O
under O
the O
top O
shared O
node O
in O
a B-PARAMETER
conversation O
are O
collected O
in O
order O
to O
filter O
out O
the O
sub-conversations O
with O
few O
utterances O
. O
section 11
id pdf2json/2021.acl-long.285.pdf.json
Given O
a B-PARAMETER
full O
MPC O
, O
the O
two O
sub-conversations O
with O
the O
most O
utterances O
form O
a B-PARAMETER
positive O
pair O
. O
section 11
id pdf2json/2021.acl-long.285.pdf.json
For O
each O
positive O
pair O
, O
we O
replace O
one O
of O
its O
elements O
with O
another O
sub-conversation O
randomly O
sampled O
from O
the O
training O
corpus O
to O
form O
a B-PARAMETER
negative O
pair O
. O
section 11
id pdf2json/2021.acl-long.285.pdf.json
Formally O
, O
given O
two O
sub-conversations O
ci O
and O
cj O
, O
utterances O
in O
each O
sub-conversation O
are O
first O
concatenated O
respectively O
to O
form O
two O
segments O
. O
section 11
id pdf2json/2021.acl-long.285.pdf.json
Then O
, O
the O
two O
segments O
are O
concatenated O
with O
a B-PARAMETER
[ O
SEP O
] O
token O
and O
a B-PARAMETER
[ O
CLS O
] O
token O
is O
inserted O
at O
the O
beginning O
of O
the O
whole O
sequence O
. O
section 11
id pdf2json/2021.acl-long.285.pdf.json
This O
sequence O
are O
encoded O
by O
BERT B-SOFTWARE
to O
derive O
the O
contextualized O
representation O
for O
the O
[ O
CLS O
] O
token O
. O
section 11
id pdf2json/2021.acl-long.285.pdf.json
A O
non-linear O
transformation O
with O
sigmoid B-METHOD
activation I-METHOD
is O
further O
applied O
to O
this O
representation O
for O
calculating O
the O
matching O
score O
mij O
, O
i.e. O
, O
the O
probability O
of O
ci O
and O
cj O
sharing O
the O
same O
parent O
node O
. O
section 11
id pdf2json/2021.acl-long.285.pdf.json
Finally O
, O
the O
pretraining O
objective O
of O
this O
task O
is O
to O
minimize O
the O
cross-entropy B-METRIC
loss I-METRIC
as O
Lsnd O
= O
− O
[ O
yijlog O
( O
mij O
) O
+ O
( O
1− O
yij O
) O
log O
( O
1−mij O
) O
] O
, O
( O
9 O
) O
where O
yij O
= O
1 O
if O
ci O
and O
cj O
share O
the O
same O
parent O
node O
and O
yij O
= O
0 O
otherwise O
. O

section 12
id pdf2json/2021.acl-long.285.pdf.json
In O
addition O
, O
we O
also O
adopt O
the O
tasks O
of O
masked O
language O
model O
( O
MLM O
) O
and O
next O
sentence O
prediction O
( O
NSP O
) O
in O
original O
BERT B-SOFTWARE
pre-training O
( O
Devlin B-AUTHOR
et O
al. O
, O
2019 O
) O
, O
which O
have O
been O
proven O
effective O
for O
incorporating O
domain O
knowledge O
( O
Gu O
et O
al. O
, O
2020 O
; O
Gururangan B-AUTHOR
et O
al. O
, O
2020 O
) O
. O
section 12
id pdf2json/2021.acl-long.285.pdf.json
Finally O
, O
MPCBERT O
is O
trained O
by O
performing O
multi-task O
learning O
that O
minimizes O
the O
sum O
of O
all O
loss O
functions O
as O
L O
= O
Lrur O
+ O
Liss O
+ O
Lpcd O
+ O
Lmsur O
+ O
Lsnd O
+ O
Lmlm O
+ O
Lnsp O
. O
section 12
id pdf2json/2021.acl-long.285.pdf.json
( O
10 O
) O


section 14
id pdf2json/2021.acl-long.285.pdf.json
Given O
a B-PARAMETER
multi-party O
conversation O
where O
part O
of O
the O
addressees O
are O
unknown O
, O
Ouchi B-AUTHOR
and O
Tsuboi B-AUTHOR
( O
2016 O
) O
and O
Zhang B-AUTHOR
et I-AUTHOR
al I-AUTHOR
. O
section 14
id pdf2json/2021.acl-long.285.pdf.json
( O
2018a O
) O
recognized O
an O
addressee O
of O
the O
last O
utterance O
. O
section 14
id pdf2json/2021.acl-long.285.pdf.json
Le B-AUTHOR
et O
al O
. O
section 14
id pdf2json/2021.acl-long.285.pdf.json
( O
2019 O
) O
recognized O
addressees O
of O
all O
utterances O
in O
a B-PARAMETER
conversation O
. O
section 14
id pdf2json/2021.acl-long.285.pdf.json
In O
this O
paper O
, O
we O
follow O
the O
more O
challenging O
setting O
in O
Le B-AUTHOR
et O
al O
. O
section 14
id pdf2json/2021.acl-long.285.pdf.json
( O
2019 O
) O
. O
section 14
id pdf2json/2021.acl-long.285.pdf.json
Formally O
, O
models O
are O
asked O
to O
predict O
{ O
ân O
} O
Nn=1 O
given O
{ O
( O
sn O
, O
un O
, O
an O
) O
} O
Nn=1\ O
{ O
an O
} O
Nn=1 O
, O
where O
ân O
is O
selected O
from O
the O
interlocutor O
set O
in O
this O
conversation O
and O
\ O
denotes O
exclusion O
. O
section 14
id pdf2json/2021.acl-long.285.pdf.json
When O
applying O
MPC-BERT O
, O
this O
task O
is O
reformulated O
as O
finding O
a B-PARAMETER
preceding O
utterance O
from O
the O
same O
addressee O
. O
section 14
id pdf2json/2021.acl-long.285.pdf.json
Its O
RUR O
matching O
scores O
with O
all O
preceding O
utterances O
are O
calculated O
following O
Eq O
. O
section 14
id pdf2json/2021.acl-long.285.pdf.json
( O
1 O
) O
. O
section 14
id pdf2json/2021.acl-long.285.pdf.json
Then O
, O
the O
utterance O
with O
the O
highest O
score O
is O
selected O
and O
the O
speaker O
of O
the O
selected O
utterance O
is O
considered O
as O
the O
recognized O
addressee O
. O
section 14
id pdf2json/2021.acl-long.285.pdf.json
Finally O
, O
the O
fine-tuning O
objective O
of O
this O
task O
is O
to O
minimize O
the O
crossentropy O
loss O
as O
Lar O
= O
− O
N∑ O
i=2 O
i−1∑ O
j=1 O
yij O
log O
( O
mij O
) O
, O
( O
11 B-PARAMETER
) O
where O
mij O
is O
defined O
in O
Eq O
. O
section 14
id pdf2json/2021.acl-long.285.pdf.json
( O
1 O
) O
, O
yij O
= O
1 O
if O
the O
speaker O
of O
Uj O
is O
the O
addressee O
of O
Ui O
and O
yij O
= O
0 O
otherwise O
. O

section 15
id pdf2json/2021.acl-long.285.pdf.json
This O
task O
aims O
to O
identify O
the O
speaker O
of O
the O
last O
utterance O
in O
a B-PARAMETER
conversation O
. O
section 15
id pdf2json/2021.acl-long.285.pdf.json
Formally O
, O
models O
are O
asked O
to O
predict O
ŝN O
given O
{ O
( O
sn O
, O
un O
, O
an O
) O
} O
Nn=1\sN O
, O
where O
ŝN O
is O
selected O
from O
the O
interlocutor O
set O
in O
this O
conversation O
. O
section 15
id pdf2json/2021.acl-long.285.pdf.json
When O
applying O
MPC-BERT O
, O
this O
task O
is O
reformulated O
as O
identifying O
the O
utterances O
sharing O
the O
same O
speaker O
. O
section 15
id pdf2json/2021.acl-long.285.pdf.json
For O
the O
last O
utterance O
UN O
, O
its O
speaker O
embedding O
is O
masked O
and O
its O
ISS O
matching O
scores O
mNj O
with O
all O
preceding O
utterances O
are O
calculated O
following O
Section O
3.2.2 O
. O
section 15
id pdf2json/2021.acl-long.285.pdf.json
The O
finetuning O
objective O
of O
this O
task O
is O
to O
minimize O
the O
cross-entropy B-METRIC
loss I-METRIC
as O
Lsi O
= O
− O
N−1∑ O
j=1 O
yNj O
log O
( O
mNj O
) O
, O
( O
12 O
) O
where O
yNj O
= O
1 O
if O
Uj O
shares O
the O
same O
speaker O
with O
UN O
and O
yNj O
= O
0 O
otherwise O
. O

section 16
id pdf2json/2021.acl-long.285.pdf.json
This O
task O
asks O
models O
to O
select O
ûN O
from O
a B-PARAMETER
set O
of O
response O
candidates O
given O
the O
conversation O
context O
{ O
( O
sn O
, O
un O
, O
an O
) O
} O
Nn=1\uN O
. O
section 16
id pdf2json/2021.acl-long.285.pdf.json
The O
key O
is O
to O
measure O
the O
similarity O
between O
two O
segments O
of O
context O
and O
response O
. O
section 16
id pdf2json/2021.acl-long.285.pdf.json
We O
concatenate O
each O
response O
candidate O
with O
the O
context O
and O
extract O
the O
contextualized O
representation O
e O
[ O
CLS O
] O
for O
the O
first O
[ O
CLS O
] O
token O
using O
MPC-BERT O
. O
section 16
id pdf2json/2021.acl-long.285.pdf.json
Then O
, O
e O
[ O
CLS O
] O
is O
fed B-METRIC
into O
a B-PARAMETER
nonlinear O
transformation O
with O
sigmoid B-METHOD
activation I-METHOD
to O
obtain O
the O
matching O
score O
between O
the O
context O
and O
the O
response O
. O
section 16
id pdf2json/2021.acl-long.285.pdf.json
Finally O
, O
the O
fine-tuning O
objective O
of O
this O
task O
is O
to O
minimize O
the O
cross-entropy B-METRIC
loss I-METRIC
according O
to O
the O
true/false O
labels O
of O
responses O
in O
the O
training O
set O
as O
Lrs O
= O
− O
[ O
ylog O
( O
mcr O
) O
+ O
( O
1−y O
) O
log O
( O
1−mcr O
) O
] O
, O
( O
13 O
) O
where O
y O
= O
1 O
if O
the O
response O
r O
is O
a B-PARAMETER
proper O
one O
for O
the O
context O
c B-METRIC
; O
otherwise O
y O
= O
0 O
. O


section 18
id pdf2json/2021.acl-long.285.pdf.json
We O
evaluated O
our O
proposed O
methods O
on O
two O
Ubuntu O
IRC O
benchmarks O
. O
section 18
id pdf2json/2021.acl-long.285.pdf.json
One O
was O
released O
by O
Hu B-AUTHOR
et I-AUTHOR
al I-AUTHOR
. O
section 18
id pdf2json/2021.acl-long.285.pdf.json
( O
2019 O
) O
, O
in O
which O
both O
speaker O
and O
addressee O
labels O
was O
provided O
for O
each O
utterance O
. O
section 18
id pdf2json/2021.acl-long.285.pdf.json
The O
other O
benchmark O
was O
released O
by O
Ouchi B-AUTHOR
and O
Tsuboi B-AUTHOR
( O
2016 O
) O
. O
section 18
id pdf2json/2021.acl-long.285.pdf.json
Here O
, O
we O
adopted O
the O
version O
shared O
in O
Le B-AUTHOR
et O
al O
. O
section 18
id pdf2json/2021.acl-long.285.pdf.json
( O
2019 O
) O
for O
fair O
comparison O
. O
section 18
id pdf2json/2021.acl-long.285.pdf.json
The O
conversation O
sessions O
were O
separated O
into O
three O
categories O
according O
to O
the O
session O
length B-METRIC
( O
Len5 O
, O
Len-10 O
and O
Len-15 O
) O
following O
the O
splitting O
strategy O
of O
previous O
studies O
( O
Ouchi B-AUTHOR
and O
Tsuboi B-AUTHOR
, O
2016 O
; O
Zhang B-AUTHOR
et O
al. O
, O
2018a O
; O
Le B-AUTHOR
et O
al. O
, O
2019 O
) O
. O
section 18
id pdf2json/2021.acl-long.285.pdf.json
Table O
2 O
presents O
the O
statistics O
of O
the O
two O
benchmarks O
evaluated O
in O
our O
experiments O
. O

section 19
id pdf2json/2021.acl-long.285.pdf.json
Non-pre-training-based O
models O
Ouchi B-AUTHOR
and O
Tsuboi B-AUTHOR
( O
2016 O
) O
proposed O
a B-PARAMETER
dynamic O
model O
DRNN O
which O
updated O
speaker O
embeddings O
with O
the O
conversation O
flow O
. O
section 19
id pdf2json/2021.acl-long.285.pdf.json
Zhang B-AUTHOR
et I-AUTHOR
al I-AUTHOR
. O
section 19
id pdf2json/2021.acl-long.285.pdf.json
( O
2018a O
) O
improved O
DRNN O
to O
SI-RNN B-METHOD
which O
updated O
speaker O
embeddings O
role-sensitively O
. O
section 19
id pdf2json/2021.acl-long.285.pdf.json
Le B-AUTHOR
et O
al O
. O
section 19
id pdf2json/2021.acl-long.285.pdf.json
( O
2019 O
) O
proposed O
W2W O
which O
jointly O
modeled O
interlocutors O
and O
utterances O
in O
a B-PARAMETER
uniform O
framework O
, O
and O
predicted O
all O
addressees O
. O
section 19
id pdf2json/2021.acl-long.285.pdf.json
Pre-training-based O
models O
BERT B-SOFTWARE
( O
Devlin B-AUTHOR
et O
al. O
, O
2019 O
) O
was O
pre-trained O
to O
learn O
general O
language O
representations O
with O
MLM O
and O
NSP O
tasks O
. O
section 19
id pdf2json/2021.acl-long.285.pdf.json
SABERT O
( O
Gu O
et O
al. O
, O
2020 O
) O
added O
speaker O
embeddings O
and O
further O
pre-trained O
BERT B-SOFTWARE
on O
a B-PARAMETER
domain-specific O
corpus O
to O
incorporate O
domain O
knowledge O
. O
section 19
id pdf2json/2021.acl-long.285.pdf.json
We O
re-implemented O
SA-BERT O
with O
the O
pre-training O
corpus O
used O
in O
this O
paper O
to O
ensure O
fair O
comparison O
. O

section 20
id pdf2json/2021.acl-long.285.pdf.json
The O
version O
of O
BERT-base-uncased O
was O
adopted O
for O
all O
our O
experiments O
. O
section 20
id pdf2json/2021.acl-long.285.pdf.json
For O
pre-training O
, O
GELU B-METHOD
( O
Hendrycks O
and O
Gimpel O
, O
2016 O
) O
was O
employed O
as O
the O
activation O
for O
all O
non-linear O
transformations O
. O
section 20
id pdf2json/2021.acl-long.285.pdf.json
The O
Adam B-SOFTWARE
method O
( O
Kingma B-AUTHOR
and O
Ba B-AUTHOR
, O
2015 O
) O
was O
employed O
for O
optimization O
. O
section 20
id pdf2json/2021.acl-long.285.pdf.json
The O
learning B-PARAMETER
rate I-PARAMETER
was O
initialized O
as O
0.00005 O
and O
the O
warmup O
proportion O
was O
set O
to O
0.1 O
. O
section 20
id pdf2json/2021.acl-long.285.pdf.json
We O
pre-trained O
BERT B-SOFTWARE
for O
10 B-PARAMETER
epochs B-PARAMETER
. O
section 20
id pdf2json/2021.acl-long.285.pdf.json
The O
training O
set O
of O
the O
dateset O
used O
in O
Hu B-AUTHOR
et I-AUTHOR
al I-AUTHOR
. O
section 20
id pdf2json/2021.acl-long.285.pdf.json
( O
2019 O
) O
was O
employed O
for O
pre-training O
. O
section 20
id pdf2json/2021.acl-long.285.pdf.json
The O
maximum B-METRIC
utterance O
number O
was O
set O
to O
7 O
. O
section 20
id pdf2json/2021.acl-long.285.pdf.json
The O
maximum B-METRIC
sequence O
length B-METRIC
was O
set O
to O
230 O
. O
section 20
id pdf2json/2021.acl-long.285.pdf.json
The O
maximum B-METRIC
sampling O
numbers O
for O
each O
example O
were O
set O
to O
4 O
for O
RUR O
, O
2 O
for O
ISS O
and O
2 O
for O
PCD O
. O
section 20
id pdf2json/2021.acl-long.285.pdf.json
∆ O
in O
Eq O
. O
section 20
id pdf2json/2021.acl-long.285.pdf.json
( O
6 O
) O
was O
set O
to O
0.4 O
, O
achieving O
the O
best O
performance O
out O
of O
{ O
0.2 O
, O
0.4 O
, O
0.6 O
, O
0.8 O
} O
on O
the O
validation O
set O
. O
section 20
id pdf2json/2021.acl-long.285.pdf.json
The O
pre-training O
was O
performed O
using O
a B-PARAMETER
GeForce B-SOFTWARE
RTX O
2080 O
Ti O
GPU B-SOFTWARE
and O
the O
batch O
size O
was O
set O
to O
4 O
. O
section 20
id pdf2json/2021.acl-long.285.pdf.json
For O
fine-tuning O
, O
some O
configurations O
were O
different O
according O
to O
the O
characteristics O
of O
these O
datasets O
. O
section 20
id pdf2json/2021.acl-long.285.pdf.json
For O
Hu B-AUTHOR
et I-AUTHOR
al I-AUTHOR
. O
section 20
id pdf2json/2021.acl-long.285.pdf.json
( O
2019 O
) O
, O
the O
maximum B-METRIC
utterance O
number O
was O
set O
to O
7 O
and O
the O
maximum B-METRIC
sequence O
length B-METRIC
was O
set O
to O
230 O
. O
section 20
id pdf2json/2021.acl-long.285.pdf.json
For O
the O
three O
experimental O
settings O
in O
Ouchi B-AUTHOR
and O
Tsuboi B-AUTHOR
( O
2016 O
) O
, O
the O
maximum B-METRIC
utterance O
numbers O
were O
set O
to O
5 O
, O
10 O
and O
15 O
, O
and O
the O
maximum B-METRIC
sequence O
lengths O
were O
set O
to O
120 O
, O
220 O
and O
320 O
. O
section 20
id pdf2json/2021.acl-long.285.pdf.json
All O
parameters O
in O
PLMs B-METHOD
were O
updated O
. O
section 20
id pdf2json/2021.acl-long.285.pdf.json
The O
learning B-PARAMETER
rate I-PARAMETER
was O
initialized O
as O
0.00002 O
and O
the O
warmup O
proportion O
was O
set O
to O
0.1 O
. O
section 20
id pdf2json/2021.acl-long.285.pdf.json
For O
Hu B-AUTHOR
et I-AUTHOR
al I-AUTHOR
. O
section 20
id pdf2json/2021.acl-long.285.pdf.json
( O
2019 O
) O
, O
the O
fine-tuning O
process O
was O
performed O
for O
10 B-PARAMETER
epochs B-PARAMETER
for O
addressee O
recognition O
, O
10 B-PARAMETER
epochs B-PARAMETER
for O
speaker O
identification O
, O
and O
5 O
epochs B-PARAMETER
for O
response O
selection O
. O
section 20
id pdf2json/2021.acl-long.285.pdf.json
For O
Ouchi B-AUTHOR
and O
Tsuboi B-AUTHOR
( O
2016 O
) O
, O
the O
fine-tuning O
epochs B-PARAMETER
were O
set O
to O
5 O
, O
5 O
and O
3 O
respectively O
. O
section 20
id pdf2json/2021.acl-long.285.pdf.json
The O
fine-tuning O
was O
also O
performed O
using O
a B-PARAMETER
GeForce B-SOFTWARE
RTX O
2080 O
Ti O
GPU B-SOFTWARE
. O
section 20
id pdf2json/2021.acl-long.285.pdf.json
The O
batch O
sizes O
were O
set O
to O
16 O
for O
Hu B-AUTHOR
et I-AUTHOR
al I-AUTHOR
. O
section 20
id pdf2json/2021.acl-long.285.pdf.json
( O
2019 O
) O
, O
and O
40 O
, O
20 O
, O
and O
12 O
for O
the O
three O
experimental O
settings O
in O
Ouchi B-AUTHOR
and O
Tsuboi B-AUTHOR
( O
2016 O
) O
respectively O
. O
section 20
id pdf2json/2021.acl-long.285.pdf.json
The O
validation O
set O
was O
used O
to O
select O
the O
best O
model O
for O
testing O
. O
section 20
id pdf2json/2021.acl-long.285.pdf.json
All O
codes O
were O
implemented O
in O
the O
TensorFlow O
framework O
( O
Abadi O
et O
al. O
, O
2016 O
) O
and O
are O
published O
to O
help O
replicate O
our O
results O
. O
section 20
id pdf2json/2021.acl-long.285.pdf.json
1 O

section 21
id pdf2json/2021.acl-long.285.pdf.json
Addressee O
recognition O
We O
followed O
the O
metrics O
of O
previous O
work O
( O
Le B-AUTHOR
et O
al. O
, O
2019 O
) O
by O
employing O
precision B-METRIC
@ O
1 O
( O
P O
@ O
1 O
) O
to O
evaluate O
each O
utterance O
with O
ground O
truth O
. O
section 21
id pdf2json/2021.acl-long.285.pdf.json
Also O
, O
a B-PARAMETER
session O
is O
marked O
as O
positive O
if O
the O
addressees O
of O
all O
its O
utterances O
are O
correctly O
recognized O
, O
which O
is O
calculated O
as O
accuracy B-PARAMETER
( O
Acc B-METRIC
. O
) O
. O
section 21
id pdf2json/2021.acl-long.285.pdf.json
Table O
3 O
presents O
the O
results O
of O
addressee O
recognition O
. O
section 21
id pdf2json/2021.acl-long.285.pdf.json
It O
shows O
that O
MPC-BERT O
outperforms O
the O
best O
performing O
model O
, O
i.e. O
, O
SA-BERT O
, O
by O
margins O
of O
3.51 O
% O
, O
2.86 O
% O
, O
3.28 O
% O
and O
5.36 O
% O
on O
these O
test O
sets O
respectively O
in O
terms O
of O
Acc. O
, O
verifying O
the O
effectiveness O
of O
the O
proposed O
five O
selfsupervised O
tasks O
as O
a B-PARAMETER
whole O
. O
section 21
id pdf2json/2021.acl-long.285.pdf.json
To O
further O
illustrate O
the O
effectiveness O
of O
each O
task O
, O
ablation O
tests O
were O
performed O
as O
shown O
in O
the O
last O
five O
rows O
of O
Table O
3 O
. O
section 21
id pdf2json/2021.acl-long.285.pdf.json
We O
can O
observe O
that O
all O
self-supervised O
tasks O
are O
useful O
as O
removing O
any O
of O
them O
causes O
performance O
1https O
: O
//github.com/JasonForJoy/MPC-BERT O
drop O
. O
section 21
id pdf2json/2021.acl-long.285.pdf.json
Among O
the O
five O
tasks O
, O
RUR O
plays O
the O
most O
important O
role O
, O
and O
the O
tasks O
focusing O
on O
modeling O
interlocutor O
structure O
contribute O
more O
than O
those O
for O
utterance O
semantics O
. O
section 21
id pdf2json/2021.acl-long.285.pdf.json
Speaker O
identification O
Similarly O
, O
P O
@ O
1 O
was O
employed O
as O
the O
evaluation O
metric O
of O
speaker O
identification O
for O
the O
last O
utterance O
of O
a B-PARAMETER
conversation O
and O
the O
results O
are O
shown O
in O
Table O
4 O
. O
section 21
id pdf2json/2021.acl-long.285.pdf.json
It O
shows O
that O
MPC-BERT O
outperforms O
SA-BERT O
by O
margins O
of O
7.66 O
% O
, O
2.60 O
% O
, O
3.38 O
% O
and O
4.24 O
% O
respectively O
in O
terms O
of O
P O
@ O
1 O
. O
section 21
id pdf2json/2021.acl-long.285.pdf.json
Besides O
, O
from O
the O
ablation O
results O
we O
find O
that O
all O
tasks O
are O
useful O
for O
improving O
the O
performance O
of O
speaker O
identification O
and O
ISS O
and O
RUR O
contribute O
the O
most O
. O
section 21
id pdf2json/2021.acl-long.285.pdf.json
In O
particular O
, O
removing O
PCD O
, O
MSUR O
and O
SND O
only O
leads O
to O
slight O
performance O
drop O
. O
section 21
id pdf2json/2021.acl-long.285.pdf.json
The O
reason O
might O
be O
that O
the O
information O
conveyed O
by O
these O
tasks O
is O
redundant O
. O
section 21
id pdf2json/2021.acl-long.285.pdf.json
Response O
selection O
The O
Rn O
@ O
k B-METRIC
metrics O
adopted O
by O
previous O
studies O
( O
Ouchi B-AUTHOR
and O
Tsuboi B-AUTHOR
, O
2016 O
; O
Zhang B-AUTHOR
et O
al. O
, O
2018a O
) O
were O
used O
here O
. O
section 21
id pdf2json/2021.acl-long.285.pdf.json
Each O
model O
was O
tasked O
with O
selecting O
k B-METRIC
best-matched O
responses O
from O
n O
available O
candidates O
, O
and O
we O
calculated O
the O
recall B-METRIC
as O
Rn O
@ O
k. O
Two O
settings O
were O
followed O
in O
which O
k B-METRIC
was O
set O
to O
1 O
and O
n O
was O
set O
to O
2 O
or O
10 O
. O
section 21
id pdf2json/2021.acl-long.285.pdf.json
Table O
5 O
presents O
the O
results O
of O
response O
selection O
. O
section 21
id pdf2json/2021.acl-long.285.pdf.json
It O
shows O
that O
MPC-BERT O
outperforms O
SABERT O
by O
margins O
of O
3.82 O
% O
, O
2.71 O
% O
, O
2.55 O
% O
and O
3.22 O
% O
respectively O
in O
terms O
of O
R10 O
@ O
1 O
. O
section 21
id pdf2json/2021.acl-long.285.pdf.json
Ablation O
tests O
show O
that O
SND O
is O
the O
most O
useful O
task O
for O
response O
selection O
and O
the O
two O
tasks O
focusing O
on O
the O
utterance O
semantics O
contribute O
more O
than O
those O
focusing O
on O
the O
interlocutor O
structures O
. O

section 22
id pdf2json/2021.acl-long.285.pdf.json
Figure O
3 O
illustrates O
how O
the O
performance O
of O
BERT B-SOFTWARE
, O
SA-BERT O
and O
MPC-BERT O
changed O
with O
respect O
to O
different O
session O
lengths O
on O
the O
test O
sets O
of O
Ouchi B-AUTHOR
and O
Tsuboi B-AUTHOR
( O
2016 O
) O
. O
section 22
id pdf2json/2021.acl-long.285.pdf.json
It O
can O
be O
seen O
that O
the O
performance O
of O
addressee O
recognition O
and O
speaker O
identification O
dropped O
as O
the O
session O
length B-METRIC
increased O
. O
section 22
id pdf2json/2021.acl-long.285.pdf.json
The O
reason O
might O
be O
that O
longer O
sessions O
always O
contain O
more O
interlocutors O
which O
increase O
the O
difficulties O
of O
predicting O
interlocutors O
. O
section 22
id pdf2json/2021.acl-long.285.pdf.json
Meanwhile O
, O
the O
performance O
of O
response O
selection O
was O
significantly O
improved O
as O
the O
session O
length B-METRIC
increased O
. O
section 22
id pdf2json/2021.acl-long.285.pdf.json
It O
can O
be O
attributed O
to O
that O
longer O
sessions O
enrich O
the O
representations O
of O
contexts O
with O
more O
details O
which O
benefit O
response O
selection O
. O
section 22
id pdf2json/2021.acl-long.285.pdf.json
Furthermore O
, O
as O
the O
session O
length B-METRIC
increased O
, O
the O
performance O
of O
MPC-BERT O
dropped O
more O
slightly O
than O
that O
of O
SA-BERT O
on O
addressee O
recognition O
and O
speaker O
identification O
, O
and O
the O
R10 O
@ O
1 O
gap O
between O
MPC-BERT O
and O
SA-BERT O
on O
response O
selection O
enlarged O
from O
2.71 O
% O
to O
3.22 O
% O
. O
section 22
id pdf2json/2021.acl-long.285.pdf.json
These O
results O
imply O
the O
superiority O
of O
MPC-BERT O
over O
SA-BERT O
on O
modeling O
long O
MPCs O
with O
complicated O
structures O
. O

section 23
id pdf2json/2021.acl-long.285.pdf.json
In O
this O
paper O
, O
we O
present O
MPC-BERT O
, O
a B-PARAMETER
pre-trained O
language O
model O
with O
five O
self-supervised O
tasks O
for O
MPC O
understanding O
. O
section 23
id pdf2json/2021.acl-long.285.pdf.json
These O
tasks O
jointly O
learn O
who O
says O
what O
to O
whom O
in O
MPCs O
. O
section 23
id pdf2json/2021.acl-long.285.pdf.json
Experimental O
results O
on O
three O
downstream O
tasks O
show O
that O
MPC-BERT O
outperforms O
previous O
methods O
by O
large O
margins O
and O
achieves O
new O
state-of-the-art O
performance O
on O
two O
benchmarks O
. O

section 24
id pdf2json/2021.acl-long.285.pdf.json
We O
thank O
anonymous O
reviewers O
for O
their O
valuable O
comments O
. O

section TITLE
id pdf2json/2021.acl-long.322.pdf.json
Learning O
Language O
and O
Multimodal O
Privacy-Preserving O
Markers O
of O
Mood O
from O
Mobile O
Data O

section ABSTRACT
id pdf2json/2021.acl-long.322.pdf.json
Mental O
health O
conditions O
remain O
underdiagnosed O
even O
in O
countries O
with O
common O
access O
to O
advanced O
medical O
care O
. O
section ABSTRACT
id pdf2json/2021.acl-long.322.pdf.json
The O
ability O
to O
accurately O
and O
efficiently O
predict O
mood O
from O
easily O
collectible O
data O
has O
several O
important O
implications O
for O
the O
early O
detection O
, O
intervention O
, O
and O
treatment O
of O
mental O
health O
disorders O
. O
section ABSTRACT
id pdf2json/2021.acl-long.322.pdf.json
One O
promising O
data O
source O
to O
help O
monitor O
human O
behavior O
is O
daily O
smartphone O
usage O
. O
section ABSTRACT
id pdf2json/2021.acl-long.322.pdf.json
However O
, O
care O
must O
be O
taken O
to O
summarize O
behaviors O
without O
identifying O
the O
user O
through O
personal O
( O
e.g. O
, O
personally O
identifiable O
information O
) O
or O
protected O
( O
e.g. O
, O
race O
, O
gender O
) O
attributes O
. O
section ABSTRACT
id pdf2json/2021.acl-long.322.pdf.json
In O
this O
paper O
, O
we O
study O
behavioral O
markers O
of O
daily O
mood O
using O
a B-PARAMETER
recent O
dataset O
of O
mobile O
behaviors O
from O
adolescent O
populations O
at O
high O
risk O
of O
suicidal O
behaviors O
. O
section ABSTRACT
id pdf2json/2021.acl-long.322.pdf.json
Using O
computational O
models O
, O
we O
find O
that O
language O
and O
multimodal O
representations O
of O
mobile O
typed O
text O
( O
spanning O
typed O
characters O
, O
words O
, O
keystroke O
timings O
, O
and O
app O
usage O
) O
are O
predictive O
of O
daily O
mood O
. O
section ABSTRACT
id pdf2json/2021.acl-long.322.pdf.json
However O
, O
we O
find O
that O
models O
trained O
to O
predict O
mood O
often O
also O
capture O
private O
user O
identities O
in O
their O
intermediate O
representations O
. O
section ABSTRACT
id pdf2json/2021.acl-long.322.pdf.json
To O
tackle O
this O
problem O
, O
we O
evaluate O
approaches O
that O
obfuscate O
user O
identity O
while O
remaining O
predictive O
. O
section ABSTRACT
id pdf2json/2021.acl-long.322.pdf.json
By O
combining O
multimodal O
representations O
with O
privacy-preserving O
learning O
, O
we O
are O
able O
to O
push O
forward O
the O
performanceprivacy O
frontier O
. O

section 0
id pdf2json/2021.acl-long.322.pdf.json
Proceedings O
of O
the O
59th O
Annual O
Meeting O
of O
the O
Association O
for O
Computational O
Linguistics O
and O
the O
11th O
International O
Joint O
Conference O
on O
Natural O
Language O
Processing O
, O
pages O
4170–4187 O
August O
1–6 O
, O
2021 O
. O
section 0
id pdf2json/2021.acl-long.322.pdf.json
©2021 O
Association O
for O
Computational O
Linguistics O
4170 O

section 1
id pdf2json/2021.acl-long.322.pdf.json
Mental O
illnesses O
can O
have O
a B-PARAMETER
damaging O
permanent O
impact O
on O
communities O
, O
societies O
, O
and O
economies O
all O
over O
the O
world O
( O
World O
Health O
Organization O
, O
2003 O
) O
. O
section 1
id pdf2json/2021.acl-long.322.pdf.json
Individuals O
often O
do O
not O
realize O
they O
are O
at O
risk O
of O
mental O
disorders O
even O
when O
they O
have O
symptoms O
. O
section 1
id pdf2json/2021.acl-long.322.pdf.json
As O
a B-PARAMETER
result O
, O
many O
are O
late O
in O
seeking O
professional O
help O
and O
treatment O
( O
Thornicroft O
et O
al. O
, O
2016 O
) O
, O
particularly O
among O
adolescents O
where O
suicide O
is O
the O
second O
leading O
cause O
of O
death O
( O
Curtin O
? O
first O
two O
authors O
contributed O
equally O
. O
section 1
id pdf2json/2021.acl-long.322.pdf.json
and O
Heron O
, O
2019 O
) O
. O
section 1
id pdf2json/2021.acl-long.322.pdf.json
In O
addition O
to O
deaths O
, O
16 O
% O
of O
high O
school O
students O
report O
having O
serious O
suicidal O
thoughts O
each O
year O
, O
and O
8 O
% O
of O
them O
make O
one O
or O
more O
suicide O
attempts O
( O
CDC O
, O
2015 O
) O
. O
section 1
id pdf2json/2021.acl-long.322.pdf.json
This O
problem O
is O
particularly O
exacerbated O
as O
an O
“ O
echo O
pandemic O
” O
of O
mental O
health O
problems O
have O
arisen O
in O
the O
wake O
of O
the O
COVID-19 O
pandemic O
( O
Inkster O
et O
al. O
, O
2021 O
; O
Saha B-AUTHOR
et O
al. O
, O
2020 O
) O
. O
section 1
id pdf2json/2021.acl-long.322.pdf.json
Intensive O
monitoring O
of O
behaviors O
via O
adolescents O
’ O
natural O
use O
of O
smartphones O
may O
help O
identify O
realtime O
predictors O
of O
mood O
in O
high-risk O
youth O
as O
a B-PARAMETER
proxy O
for O
suicide O
risk O
( O
Nahum-Shani O
et O
al. O
, O
2018 O
) O
. O
section 1
id pdf2json/2021.acl-long.322.pdf.json
While O
there O
are O
inherent O
limitations O
in O
the O
mismatch O
between O
mood O
prediction O
and O
ultimately O
developing O
real-time O
intervention O
against O
imminent O
suicide O
risk O
( O
Coppersmith O
et O
al. O
, O
2018 O
; O
Ophir O
et O
al. O
, O
2020 O
) O
, O
we O
believe O
that O
the O
former O
is O
a B-PARAMETER
reasonable O
starting O
point O
to O
tackle O
similar O
machine O
learning O
problems O
surrounding O
affective O
computing O
and O
privacy-preserving O
learning O
. O
section 1
id pdf2json/2021.acl-long.322.pdf.json
Studying O
mood O
in O
this O
high-risk O
population O
is O
a B-PARAMETER
valuable O
goal O
given O
that O
suicide O
attempts O
are O
often O
decided O
within O
a B-PARAMETER
short O
time-lapse O
and O
just-in-time O
assessments O
of O
mood O
changes O
can O
be O
a B-PARAMETER
stepping O
stone O
in O
this O
direction O
( O
Rizk O
et O
al. O
, O
2019 O
; O
Oquendo O
et O
al. O
, O
2020 O
) O
. O
section 1
id pdf2json/2021.acl-long.322.pdf.json
Technologies O
for O
mood O
prediction O
can O
also O
be O
a B-PARAMETER
valuable O
component O
of O
decision O
support O
for O
clinicians O
and O
healthcare O
providers O
during O
their O
assessments O
( O
Mann O
et O
al. O
, O
2006 O
; O
Cho B-AUTHOR
et O
al. O
, O
2019 O
) O
. O
section 1
id pdf2json/2021.acl-long.322.pdf.json
Recent O
work O
in O
affective O
computing O
has O
begun O
to O
explore O
the O
potential O
in O
predicting O
mood O
from O
mobile O
data O
. O
section 1
id pdf2json/2021.acl-long.322.pdf.json
Studies O
have O
found O
that O
typing O
patterns O
( O
Cao B-AUTHOR
et O
al. O
, O
2017 O
; O
Ghosh O
et O
al. O
, O
2017a O
; O
Huang B-AUTHOR
et O
al. O
, O
2018 O
; O
Zulueta O
et O
al. O
, O
2018 O
) O
, O
self-reporting O
apps O
( O
Suhara O
et O
al. O
, O
2017 O
) O
, O
and O
wearable O
sensors O
( O
Ghosh O
et O
al. O
, O
2017b O
; O
Sano O
et O
al. O
, O
2018 O
) O
are O
particularly O
predictive O
. O
section 1
id pdf2json/2021.acl-long.322.pdf.json
In O
addition O
, O
multimodal O
modeling O
of O
multiple O
sensors O
( O
e.g. O
, O
wearable O
sensors O
and O
smartphone O
apps O
) O
was O
shown O
to O
further O
improve O
performance O
( O
Jaques O
et O
al. O
, O
2017 O
; O
Taylor B-AUTHOR
et O
al. O
, O
2017 O
) O
. O
section 1
id pdf2json/2021.acl-long.322.pdf.json
While O
current O
work O
primarily O
relies O
on O
selfreport O
apps O
for O
long-term O
mood O
assessments O
( O
Glenn O
and O
Nock O
, O
2014 O
) O
, O
our O
work O
investigates O
mobile O
behaviors O
from O
a B-PARAMETER
high-risk O
teenage O
population O
as O
a B-PARAMETER
predictive O
signal O
for O
daily O
mood O
( O
Franklin O
et O
al. O
, O
2017 O
; O
Large O
et O
al. O
, O
2017 O
) O
. O
section 1
id pdf2json/2021.acl-long.322.pdf.json
Prior O
work O
has O
also O
shown O
that O
private O
information O
is O
predictable O
from O
digital O
records O
of O
human O
behavior O
( O
Kosinski O
et O
al. O
, O
2013 O
) O
, O
which O
is O
dangerous O
especially O
when O
sensitive O
user O
data O
is O
involved O
. O
section 1
id pdf2json/2021.acl-long.322.pdf.json
As O
a B-PARAMETER
result O
, O
in O
parallel O
to O
improving O
predictive O
performance O
, O
a B-PARAMETER
recent O
focus O
has O
been O
on O
improving O
privacy O
through O
techniques O
such O
as O
differential O
privacy O
( O
Dankar O
and O
El O
Emam O
, O
2012 O
, O
2013 O
; O
Dankar O
et O
al. O
, O
2012 O
) O
and O
federated O
learning O
( O
McMahan O
et O
al. O
, O
2016 O
; O
Geyer O
et O
al. O
, O
2017 O
; O
Liang B-AUTHOR
et O
al. O
, O
2020b O
) O
, O
especially O
for O
healthcare O
data O
( O
e.g. O
, O
electronic O
health O
records O
( O
Xu B-AUTHOR
and O
Wang B-AUTHOR
, O
2019 O
) O
) O
and O
wearable O
devices O
( O
Chen B-AUTHOR
et O
al. O
, O
2020 O
) O
. O
section 1
id pdf2json/2021.acl-long.322.pdf.json
In O
this O
paper O
, O
as O
a B-PARAMETER
step O
towards O
using O
multimodal O
privacy-preserving O
mood O
prediction O
as O
fine-grained O
signals O
to O
aid O
in O
mental O
health O
assessment O
, O
we O
analyze O
a B-PARAMETER
recent O
dataset O
of O
mobile O
behaviors O
collected O
from O
adolescent O
populations O
at O
high O
suicidal O
risk O
. O
section 1
id pdf2json/2021.acl-long.322.pdf.json
With O
consent O
from O
participating O
groups O
, O
the O
dataset O
collects O
fine-grained O
features O
spanning O
online O
communication O
, O
keystroke O
patterns O
, O
and O
application O
usage O
. O
section 1
id pdf2json/2021.acl-long.322.pdf.json
Participants O
are O
administered O
daily O
questions O
probing O
for O
mood O
scores O
. O
section 1
id pdf2json/2021.acl-long.322.pdf.json
By O
collecting O
and O
working O
on O
ground-truth O
data O
for O
this O
population O
, O
we O
are O
able O
to O
benchmark O
on O
a B-PARAMETER
more O
accurate O
indica- O
tor O
of O
mood O
rather O
than O
proxy O
data O
such O
as O
mood O
signals O
inferred O
from O
social O
media O
content O
or O
behavior O
( O
Ernala O
et O
al. O
, O
2019 O
) O
. O
section 1
id pdf2json/2021.acl-long.322.pdf.json
This O
unique O
dataset O
presents O
an O
opportunity O
to O
investigate O
a B-PARAMETER
different O
medium O
of O
natural O
language O
processing O
- O
typed O
text O
which O
presents O
new O
challenges O
beyond O
conventionally O
studied O
written O
( O
Marcus B-AUTHOR
et O
al. O
, O
1993 O
) O
and O
spoken O
( O
Marslen-Wilson O
and O
Tyler O
, O
1980 O
) O
text O
. O
section 1
id pdf2json/2021.acl-long.322.pdf.json
We O
propose O
multimodal O
models O
that O
contextualize O
text O
with O
their O
typing O
speeds O
and O
app O
usage O
. O
section 1
id pdf2json/2021.acl-long.322.pdf.json
However O
, O
these O
models O
often O
capture O
private O
user O
identities O
in O
their O
intermediate O
representations O
when O
predicting O
mood O
. O
section 1
id pdf2json/2021.acl-long.322.pdf.json
As O
a B-PARAMETER
step O
towards O
privacy-preserving O
learning O
, O
we O
also O
propose O
approaches O
that O
obfuscate O
user O
identity O
while O
remaining O
predictive O
of O
daily O
mood O
. O
section 1
id pdf2json/2021.acl-long.322.pdf.json
By O
combining O
multimodal O
contextualization O
with O
privacy-preserving O
learning O
, O
we O
are O
able O
to O
push O
forward O
the O
performance-privacy O
frontier O
. O
section 1
id pdf2json/2021.acl-long.322.pdf.json
Finally O
, O
we O
conclude O
with O
several O
observations O
regarding O
the O
uniqueness O
of O
typed O
text O
as O
an O
opportunity O
for O
NLP O
on O
mobile O
data O
. O

section 2
id pdf2json/2021.acl-long.322.pdf.json
Intensive O
monitoring O
of O
behaviors O
via O
adolescents O
’ O
frequent O
use O
of O
smartphones O
may O
shed O
new O
light O
on O
the O
early O
risk O
of O
suicidal O
thoughts O
and O
ideations O
( O
Nahum-Shani O
et O
al. O
, O
2018 O
) O
. O
section 2
id pdf2json/2021.acl-long.322.pdf.json
Smartphones O
provide O
a B-PARAMETER
valuable O
and O
natural O
data O
source O
with O
rich O
behavioral O
markers O
spanning O
online O
communication O
, O
keystroke O
patterns O
, O
and O
application O
usage O
. O
section 2
id pdf2json/2021.acl-long.322.pdf.json
Learning O
these O
markers O
requires O
large O
datasets O
with O
diversity O
in O
participants O
, O
variety O
in O
features O
, O
and O
accuracy B-PARAMETER
in O
annotations O
. O
section 2
id pdf2json/2021.acl-long.322.pdf.json
As O
a B-PARAMETER
step O
towards O
this O
goal O
, O
we O
recently O
collected O
a B-PARAMETER
dataset O
of O
mobile O
behaviors O
from O
high-risk O
adolescent O
populations O
with O
consent O
from O
participating O
groups O
. O
section 2
id pdf2json/2021.acl-long.322.pdf.json
We O
begin O
with O
a B-PARAMETER
brief O
review O
of O
the O
data O
collection O
process O
. O
section 2
id pdf2json/2021.acl-long.322.pdf.json
This O
data O
monitors O
adolescents O
spanning O
( O
a B-PARAMETER
) O
recent O
suicide O
attempters O
( O
past O
6 O
months O
) O
with O
current O
suicidal O
ideation O
, O
( O
b O
) O
suicide O
ideators O
with O
no O
past O
suicide O
attempts O
, O
and O
( O
c B-METRIC
) O
psychiatric O
controls O
with O
no O
history O
of O
suicide O
ideation O
or O
attempts O
. O
section 2
id pdf2json/2021.acl-long.322.pdf.json
Passive O
sensing O
data O
is O
collected O
from O
each O
participant O
’ O
s O
smartphone O
across O
a B-PARAMETER
duration O
of O
6 O
months O
. O
section 2
id pdf2json/2021.acl-long.322.pdf.json
Participants O
are O
administered O
clinical O
interviews O
probing O
for O
suicidal O
thoughts O
and O
behaviors O
( O
STBs O
) O
, O
and O
self-report O
instruments O
regarding O
symptoms O
and O
acute O
events O
( O
e.g. O
, O
suicide O
attempts O
, O
psychiatric O
hospitalizations O
) O
are O
tracked O
weekly O
via O
a B-PARAMETER
questionnaire O
. O
section 2
id pdf2json/2021.acl-long.322.pdf.json
All O
users O
have O
given O
consent O
for O
their O
mobile O
data O
to O
be O
collected O
and O
shared O
with O
us B-PARAMETER
for O
research O
purposes O
. O
section 2
id pdf2json/2021.acl-long.322.pdf.json
This O
study O
has O
been O
carefully O
reviewed O
and O
approved O
by O
an O
IRB O
. O
section 2
id pdf2json/2021.acl-long.322.pdf.json
We O
follow O
the O
NIH O
guidelines O
, O
with O
a B-PARAMETER
central O
IRB O
( O
single O
IRB O
) O
linked O
to O
secondary O
sites O
. O
section 2
id pdf2json/2021.acl-long.322.pdf.json
We O
have O
IRB O
approval O
for O
the O
central O
institution O
and O
all O
secondary O
sites O
. O

section 3
id pdf2json/2021.acl-long.322.pdf.json
Every O
day O
at O
8am O
, O
users O
are O
asked O
to O
respond O
to O
the O
following O
question O
- O
“ O
In O
general O
, O
how O
have O
you O
been O
feeling O
over O
the O
last O
day O
? O
” O
- O
with O
an O
integer O
score O
between O
0 O
and O
100 O
, O
where O
0 O
means O
very O
negative O
and O
100 O
means O
very O
positive O
. O
section 3
id pdf2json/2021.acl-long.322.pdf.json
To O
construct O
our O
prediction O
task O
, O
we O
discretized O
these O
scores O
into O
the O
following O
three O
bins O
: O
negative O
( O
0− O
33 O
) O
, O
neutral O
( O
34− O
66 O
) O
, O
and O
positive O
( O
67− O
100 O
) O
, O
which O
follow O
a B-PARAMETER
class O
distribution O
of O
12.43 O
% O
, O
43.63 O
% O
, O
and O
43.94 O
% O
respectively O
. O
section 3
id pdf2json/2021.acl-long.322.pdf.json
For O
our O
3-way O
classification O
task O
, O
participants O
with O
fewer O
than O
50 O
daily O
self-reports O
were O
removed O
since O
these O
participants O
do O
not O
provide O
enough O
data O
to O
train O
an O
effective O
model O
. O
section 3
id pdf2json/2021.acl-long.322.pdf.json
In O
total O
, O
our O
dataset O
consists O
of O
1641 O
samples O
, O
consisting O
of O
data O
coming O
from O
17 O
unique O
participants O
. O

section 4
id pdf2json/2021.acl-long.322.pdf.json
We O
focused O
on O
keyboard O
data O
, O
which O
includes O
the O
time O
of O
data O
capture O
, O
the O
mobile O
application O
used O
, O
and O
the O
text O
entered O
by O
the O
user O
. O
section 4
id pdf2json/2021.acl-long.322.pdf.json
For O
each O
daily O
score O
response O
at O
8am O
, O
we O
use O
information O
collected O
between O
5am O
on O
the O
previous O
day O
to O
5am O
on O
the O
current O
day O
. O
section 4
id pdf2json/2021.acl-long.322.pdf.json
We O
chose O
this O
5am-5am O
window O
by O
looking O
at O
mobile O
activity O
and O
finding O
the O
lowest O
activity O
point O
when O
most O
people O
ended O
their O
day O
: O
5am O
. O
section 4
id pdf2json/2021.acl-long.322.pdf.json
Since O
users O
report O
the O
previous O
day O
’ O
s O
mood O
( O
when O
prompted O
at O
8am O
) O
, O
we O
decided O
to O
use O
this O
5am-5am O
time O
period O
to O
summarize O
the O
previous O
day O
’ O
s O
activities O
. O
section 4
id pdf2json/2021.acl-long.322.pdf.json
Through O
prototyping O
, O
this O
prompt O
time O
and O
frequency O
were O
found O
to O
give O
reliable O
indicators O
of O
the O
previous O
day O
’ O
s O
mood O
. O
section 4
id pdf2json/2021.acl-long.322.pdf.json
From O
this O
window O
, O
we O
extracted O
the O
following O
features O
to O
characterize O
and O
contextualize O
typed O
text O
. O
section 4
id pdf2json/2021.acl-long.322.pdf.json
Text O
: O
After O
removing O
stop-words O
, O
we O
collected O
the O
top O
1000 O
words O
( O
out O
of O
approximately O
3.2 O
million O
) O
used O
across O
all O
users O
in O
our O
dataset O
and O
created O
a B-PARAMETER
bag-of-words O
feature O
that O
contains O
the O
daily O
number O
of O
occurrences O
of O
each O
word B-METHOD
. O
section 4
id pdf2json/2021.acl-long.322.pdf.json
Keystrokes O
: O
We O
also O
extracted O
keystroke O
features O
that O
record O
the O
exact O
timing O
that O
each O
character O
was O
typed O
on O
a B-PARAMETER
mobile O
keyboard O
( O
including O
alphanumeric O
characters O
, O
special O
characters O
, O
spaces O
, O
backspace O
, O
enter O
, O
and O
autocorrect O
) O
. O
section 4
id pdf2json/2021.acl-long.322.pdf.json
By O
taking O
the O
increase O
in O
recorded O
timing O
after O
each O
keystroke O
, O
we O
obtain O
the O
duration O
that O
each O
key O
was O
pressed O
in O
a B-PARAMETER
sequence O
of O
keystrokes O
during O
the O
day O
. O
section 4
id pdf2json/2021.acl-long.322.pdf.json
When O
extracting O
keystrokes O
, O
we O
removed O
all O
small O
timings O
under O
10−2 O
seconds O
. O
section 4
id pdf2json/2021.acl-long.322.pdf.json
App O
usage O
: O
We O
count O
the O
number O
of O
mobile O
applications O
used O
per O
day O
, O
creating O
a B-PARAMETER
bag-of-apps O
feature O
for O
each O
day O
. O
section 4
id pdf2json/2021.acl-long.322.pdf.json
We O
discard O
applications O
that O
are O
used O
by O
less O
than O
10 O
% O
of O
the O
participants O
so B-PARAMETER
that O
our O
features O
are O
generalizable O
to O
more O
than O
just O
a B-PARAMETER
single O
user O
in O
the O
dataset O
, O
resulting O
in O
137 O
total O
apps O
( O
out O
of O
the O
original O
640 O
) O
. O
section 4
id pdf2json/2021.acl-long.322.pdf.json
In O
a B-PARAMETER
preliminary O
analysis O
, O
we O
observed O
that O
predictive O
models O
performed O
well O
when O
binarizing O
our O
feature O
vectors O
into O
boolean O
vectors O
, O
which O
signify O
whether O
a B-PARAMETER
word B-METHOD
or O
app O
was O
used O
on O
a B-PARAMETER
given O
day O
( O
i.e. O
, O
mapping O
values O
greater O
than O
0 O
to O
1 O
) O
. O
section 4
id pdf2json/2021.acl-long.322.pdf.json
Our O
final O
feature O
vectors O
consist O
of O
a B-PARAMETER
concatenation O
of O
a B-PARAMETER
normalized O
and O
a B-PARAMETER
binarized O
feature O
vector O
, O
resulting O
in O
2000 O
and O
274-dimensional O
vectors O
for O
text O
and O
app O
features O
respectively O
. O
section 4
id pdf2json/2021.acl-long.322.pdf.json
For O
keystrokes O
, O
we O
found O
that O
summarizing O
the O
sequence O
of O
timings O
using O
a B-PARAMETER
histogram O
( O
i.e. O
, O
defining O
a B-PARAMETER
set O
of O
timing O
buckets O
and O
creating O
a B-PARAMETER
bag-of-timings O
feature O
) O
for O
each O
day O
performed O
well O
. O
section 4
id pdf2json/2021.acl-long.322.pdf.json
We O
chose O
100 O
fine-grained O
buckets O
, O
resulting O
in O
a B-PARAMETER
100-dimensional B-PARAMETER
keystroke O
vector O
. O
section 4
id pdf2json/2021.acl-long.322.pdf.json
Please O
refer O
to O
Appendix O
B O
for O
additional O
details O
about O
the O
dataset O
and O
extracted O
features O
. O

section 5
id pdf2json/2021.acl-long.322.pdf.json
In O
this O
paper O
, O
we O
focus O
on O
studying O
approaches O
for O
learning O
privacy-preserving O
representations O
from O
mobile O
data O
for O
mood O
prediction O
. O
section 5
id pdf2json/2021.acl-long.322.pdf.json
Our O
processed O
data O
comes O
in O
the O
form O
of O
{ O
( O
xt O
, O
i O
, O
xk O
, O
i O
, O
xa O
, O
i O
, O
yi O
) O
} O
ni=1 O
with O
xt O
∈ O
N|Vt|=2000 O
denoting O
the O
bag-of-words O
features O
, O
xk O
∈ O
N|Vk|=100 O
denoting O
the O
bag-oftimings O
features O
, O
and O
xa O
∈ O
N|Va|=274 O
denoting O
the O
bag-of-apps O
features O
. O
section 5
id pdf2json/2021.acl-long.322.pdf.json
y O
denotes O
the O
label O
which O
takes O
on O
one O
of O
our O
3 O
mood O
categories O
: O
negative O
, O
neutral O
, O
and O
positive O
. O
section 5
id pdf2json/2021.acl-long.322.pdf.json
In O
parallel O
, O
we O
also O
have O
data O
representing O
the O
corresponding O
( O
one-hot O
) O
user O
identity O
xid O
which O
will O
be O
useful O
when O
learning O
privacypreserving O
representations O
that O
do O
not O
encode O
information O
about O
user O
identity O
xid O
and O
evaluating O
privacy O
performance O
. O

section 6
id pdf2json/2021.acl-long.322.pdf.json
We O
considered O
two O
unimodal O
baselines O
: O
1 O
. O
section 6
id pdf2json/2021.acl-long.322.pdf.json
Support O
Vector O
Machines O
( O
SVMS O
) O
project O
training O
examples O
to O
a B-PARAMETER
chosen O
kernel O
space O
and O
finds O
the O
optimal O
hyperplane O
that O
maximally O
separates O
each O
class O
of O
instances O
. O
section 6
id pdf2json/2021.acl-long.322.pdf.json
We O
apply O
an O
SVM O
classifier O
on O
input O
data O
xuni O
∈ O
{ O
xt O
, O
xk O
, O
xa O
} O
and O
use O
supervised B-METHOD
learning I-METHOD
to O
predict O
daily O
mood O
labels O
y O
. O
section 6
id pdf2json/2021.acl-long.322.pdf.json
2 O
. O
section 6
id pdf2json/2021.acl-long.322.pdf.json
Multilayer O
Perceptrons O
( O
MLPS O
) O
have O
seen O
widespread O
success O
in O
supervised O
prediction O
tasks O
due O
to O
their O
ability O
in O
modeling O
complex O
nonlinear O
relationships O
. O
section 6
id pdf2json/2021.acl-long.322.pdf.json
Because O
of O
the O
small O
size O
of O
our O
dataset O
, O
we O
choose O
a B-PARAMETER
simple O
multilayer O
perceptron O
with O
two O
hidden O
layers O
. O
section 6
id pdf2json/2021.acl-long.322.pdf.json
Similarly O
, O
we O
apply O
an O
MLP B-METHOD
classifier O
on O
input O
data O
xuni O
∈ O
{ O
xt O
, O
xk O
, O
xa O
} O
to O
predict O
daily O
mood O
labels O
y O
. O

section 7
id pdf2json/2021.acl-long.322.pdf.json
We O
extend O
both O
SVM O
and O
MLP B-METHOD
classifiers O
using O
early O
fusion O
( O
Baltrušaitis O
et O
al. O
, O
2018 O
) O
of O
text O
and O
app O
usage O
to O
model O
multimodal O
interactions O
. O
section 7
id pdf2json/2021.acl-long.322.pdf.json
Specifically O
, O
we O
align O
the O
input O
through O
concatenating O
the O
bag-of-words O
, O
bag-of-keystrokes O
, O
and O
bag-of-apps O
features O
for O
each O
day O
resulting O
in O
an O
input O
vector O
xmulti O
= O
xt⊕xk⊕xa O
, O
before O
using O
an O
SVM/MLP O
classifier O
for O
prediction O
. O

section 8
id pdf2json/2021.acl-long.322.pdf.json
While O
classifiers O
trained O
with O
traditional O
supervised B-METHOD
learning I-METHOD
can O
learn O
useful O
representations O
for O
mood O
prediction O
, O
they O
carry O
the O
risk O
of O
memorizing O
the O
identity O
of O
the O
user O
along O
with O
their O
sensitive O
mobile O
usage O
and O
baseline O
mood O
scores O
, O
and O
possibly O
revealing O
these O
identities O
to O
adversarial O
thirdparties O
( O
Abadi O
et O
al. O
, O
2016 O
) O
. O
section 8
id pdf2json/2021.acl-long.322.pdf.json
Therefore O
, O
it O
is O
crucial O
to O
perform O
mood O
prediction O
while O
also O
protecting O
the O
privacy O
of O
personal O
identities O
. O
section 8
id pdf2json/2021.acl-long.322.pdf.json
We O
adapt O
the O
Selective-Additive O
Learning O
( O
SAL B-METHOD
) O
framework O
( O
Wang B-AUTHOR
et O
al. O
, O
2017 O
) O
for O
the O
purpose O
of O
privacy-preserving O
learning O
. O
section 8
id pdf2json/2021.acl-long.322.pdf.json
While O
SAL B-METHOD
was O
originally O
developed O
with O
a B-PARAMETER
very O
different O
goal O
in O
mind O
: O
improving O
model O
generalization O
, O
we O
expand O
SAL B-METHOD
to O
a B-PARAMETER
very O
important O
problem O
in O
health- O
care O
: O
preserving O
privacy O
. O
section 8
id pdf2json/2021.acl-long.322.pdf.json
We O
adapted O
SAL B-METHOD
to O
learn O
disentangled O
representations O
separated O
into O
identity-dependent O
private O
information O
and O
identityindependent O
population-level O
information O
using O
three O
phases O
: O
( O
1 O
) O
Pretrain O
phase O
: O
The O
input O
is O
a B-PARAMETER
set O
of O
( O
multimodal O
) O
features O
x O
that O
are O
likely O
to O
contain O
both O
identity-dependent O
and O
independent O
information O
. O
section 8
id pdf2json/2021.acl-long.322.pdf.json
The O
intermediate O
representation O
zfeat O
= O
ffeat O
( O
x O
; O
θ O
∗ O
feat O
) O
is O
obtained O
from O
an O
MLP B-METHOD
classifier O
pretrained O
for O
mood O
prediction O
. O
section 8
id pdf2json/2021.acl-long.322.pdf.json
ffeat O
denotes O
the O
classifier O
with O
pretrained O
parameters O
θ∗feat O
. O
section 8
id pdf2json/2021.acl-long.322.pdf.json
( O
2 O
) O
Selection O
phase O
: O
Our O
goal O
is O
to O
now O
disentangle O
the O
identity-dependent O
and O
independent O
information O
within O
zfeat O
. O
section 8
id pdf2json/2021.acl-long.322.pdf.json
We O
hypothesize O
that O
dependent O
and O
independent O
information O
are O
encoded O
in O
separate O
subspaces O
of O
the O
feature O
vector O
zfeat O
. O
section 8
id pdf2json/2021.acl-long.322.pdf.json
This O
allows O
us B-PARAMETER
to O
disentangle O
them O
by O
training O
a B-PARAMETER
separate O
classifier O
to O
predict O
zfeat O
as O
much O
as O
possible O
given O
only O
the O
user O
identity O
: O
θ∗id O
= O
argmin O
θid O
( O
zfeat O
− O
fid O
( O
xid O
; O
θid O
) O
) O
2 O
+ O
λ||zid||1 O
, O
( O
1 O
) O
where O
xid O
denotes O
a B-PARAMETER
one O
hot O
encoding O
of O
user O
identity O
as O
input O
, O
fid O
denotes O
the O
identity O
encoder B-METHOD
with O
parameters O
θid O
, O
and O
λ O
denotes O
a B-PARAMETER
hyperparameter O
that O
controls O
the O
weight O
of O
the O
` O
1 O
regularizer O
. O
section 8
id pdf2json/2021.acl-long.322.pdf.json
fid O
projects O
the O
user O
identity O
encodings O
to O
the O
feature O
space O
learned O
by O
ffeat O
. O
section 8
id pdf2json/2021.acl-long.322.pdf.json
By O
minimizing O
the O
objective O
in O
equation O
( O
1 O
) O
for O
each O
( O
x O
, O
xid O
) O
pair O
, O
fid O
learns O
to O
encode O
user O
identity O
into O
a B-PARAMETER
sparse O
vector O
zid O
= O
fid O
( O
xid O
; O
θ O
∗ O
id O
) O
representing O
identity-dependent O
features O
: O
the O
nonzero O
values O
of O
zid O
represent O
dimensions O
of O
the O
identity-dependent O
subspace O
in O
zfeat O
, O
while O
the O
remaining O
dimensions O
belong O
to O
the O
identity-independent O
subspace O
. O
section 8
id pdf2json/2021.acl-long.322.pdf.json
( O
3 O
) O
Addition O
phase O
: O
Given O
two O
factors O
zfeat O
and O
zid O
, O
to O
ensure O
that O
our O
prediction O
model O
does O
not O
capture O
identity-related O
information O
zid O
, O
we O
add O
multiplicative O
Gaussian B-METHOD
noise I-METHOD
to O
remove O
information O
from O
the O
identity-related O
subspace O
zid O
while O
repeatedly O
optimizing O
for O
mood O
prediction O
with O
a B-PARAMETER
final O
MLP B-METHOD
classification O
layer O
g O
( O
zfeat O
, O
zid O
; O
δ O
) O
. O
section 8
id pdf2json/2021.acl-long.322.pdf.json
This O
resulting O
model O
should O
only O
retain O
identity-independent O
features O
for O
mood O
prediction O
: O
ŷ O
= O
g O
( O
zfeat O
+ O
zid O
) O
( O
2 O
) O
where O
∼ O
N O
( O
0 O
, O
σ2 O
) O
is O
repeatedly O
sampled O
across O
batches O
and O
training O
epochs B-PARAMETER
. O
section 8
id pdf2json/2021.acl-long.322.pdf.json
We O
call O
this O
approach O
NOISY O
IDENTITY O
MLP B-METHOD
, O
or O
NI-MLP O
for O
short O
, O
and O
summarize O
the O
final O
algorithm O
in O
Figure O
2 O
. O
section 8
id pdf2json/2021.acl-long.322.pdf.json
Controlling O
the O
tradeoff O
between O
performance O
and O
privacy O
: O
There O
is O
often O
a B-PARAMETER
tradeoff O
between O
privacy O
and O
prediction O
performance O
. O
section 8
id pdf2json/2021.acl-long.322.pdf.json
To O
control O
this O
tradeoff O
, O
we O
vary O
the O
parameter O
σ O
, O
which O
is O
the O
variance B-METRIC
of O
noise O
added O
to O
the O
identity-dependent O
subspace O
across O
batches O
and O
training O
epochs B-PARAMETER
. O
section 8
id pdf2json/2021.acl-long.322.pdf.json
σ O
= O
0 O
recovers O
a B-PARAMETER
standard O
MLP B-METHOD
with O
good O
performance O
but O
reveals O
user O
identities O
, O
while O
large O
σ O
effectively O
protects O
user O
identities O
but O
at O
the O
possible O
expense O
of O
mood O
prediction O
performance O
. O
section 8
id pdf2json/2021.acl-long.322.pdf.json
In O
practice O
, O
the O
optimal O
tradeoff O
between O
privacy O
and O
performance O
varies O
depending O
on O
the O
problem O
. O
section 8
id pdf2json/2021.acl-long.322.pdf.json
For O
our O
purposes O
, O
we O
automatically O
perform O
model O
selection O
using O
this O
performance-privacy O
ratio O
R O
computed O
on O
the O
validation O
set O
, O
where O
R O
= O
sMLP O
− O
sNI-MLP O
tMLP O
− O
tNI-MLP O
( O
3 O
) O
is O
defined O
as O
the O
improvement O
in O
privacy O
per O
unit O
of O
performance O
lost O
. O
section 8
id pdf2json/2021.acl-long.322.pdf.json
Here O
, O
s O
is O
defined O
as O
the O
accuracy B-PARAMETER
in O
user O
prediction O
and O
t O
is O
defined O
as O
the O
F1 B-METRIC
score I-METRIC
on O
mood O
prediction O
. O

section 9
id pdf2json/2021.acl-long.322.pdf.json
We O
perform O
experiments O
to O
test O
the O
utility O
of O
text O
, O
keystroke O
, O
and O
app O
features O
in O
predicting O
daily O
mood O
while O
keeping O
user O
privacy O
in O
mind O
. O

section 10
id pdf2json/2021.acl-long.322.pdf.json
Data O
splits O
: O
Given O
that O
our O
data O
is O
longitudinal O
, O
we O
split O
our O
data O
into O
10 O
partitions O
ordered O
chronologically O
by O
users O
. O
section 10
id pdf2json/2021.acl-long.322.pdf.json
We O
do O
so B-PARAMETER
in O
order O
to O
maintain O
independence O
between O
the O
train O
, O
validation O
, O
and O
test O
splits O
in O
the O
case O
where O
there O
is O
some O
form O
of O
time-level O
dependency O
within O
our O
labels O
. O
section 10
id pdf2json/2021.acl-long.322.pdf.json
Evaluation O
: O
For O
each O
model O
, O
we O
run O
a B-PARAMETER
nested O
kfold O
cross-validation B-METHOD
( O
i.e. O
, O
we O
perform O
9-fold O
validation O
within O
10-fold O
testing O
) O
. O
section 10
id pdf2json/2021.acl-long.322.pdf.json
For O
each O
test O
fold O
, O
we O
identify O
the O
optimal O
parameter O
set O
as O
the O
one O
that O
achieves O
the O
highest O
mean B-METRIC
validation O
score O
over O
the O
validation O
folds O
. O
section 10
id pdf2json/2021.acl-long.322.pdf.json
To O
evaluate O
NI-MLP O
, O
we O
use O
the O
best O
performing O
MLP B-METHOD
model O
for O
each O
test O
fold O
as O
our O
base O
classifier O
before O
performing O
privacypreserving O
learning O
. O
section 10
id pdf2json/2021.acl-long.322.pdf.json
For O
all O
experiments O
, O
we O
report O
the O
test B-METRIC
accuracy B-PARAMETER
and O
macro B-METRIC
F1 I-METRIC
score I-METRIC
because O
our O
classes O
are O
imbalanced O
. O
section 10
id pdf2json/2021.acl-long.322.pdf.json
Given O
the O
low O
number O
of O
cross-validation B-METHOD
folds O
, O
we O
use O
the O
Wilcoxon O
signedrank O
test O
( O
Wilcoxon O
, O
1992 O
) O
at O
5 O
% O
significance O
level O
for O
all O
statistical O
comparisons O
( O
see O
Appendix O
C B-METHOD
for O
more O
experimental O
details O
) O
. O

section 11
id pdf2json/2021.acl-long.322.pdf.json
We O
make O
the O
following O
observations O
regarding O
the O
learned O
language O
and O
multimodal O
representations O
for O
mood O
prediction O
: O
Observation O
1 O
: O
Text O
, O
keystroke O
, O
and O
app O
usage O
features O
are O
individually O
predictive O
of O
mood O
. O
section 11
id pdf2json/2021.acl-long.322.pdf.json
To O
evaluate O
how O
predictive O
our O
extracted O
text O
, O
keystroke O
timings O
, O
and O
app O
usage O
features O
are O
, O
we O
first O
run O
experiments O
using O
SVM O
, O
MLP B-METHOD
, O
and O
NIMLP O
on O
each O
individual O
feature O
separately O
. O
section 11
id pdf2json/2021.acl-long.322.pdf.json
Since O
we O
have O
unbalanced O
classes O
, O
we O
chose O
a B-PARAMETER
majority O
classifier O
( O
i.e. O
, O
most O
common O
class O
in O
the O
training O
set O
) O
as O
our O
baseline O
. O
section 11
id pdf2json/2021.acl-long.322.pdf.json
From O
Table O
1 O
, O
we O
observe O
that O
using O
these O
three O
feature O
types O
individually O
outperforms O
the O
baseline O
with O
respect O
to O
accuracy B-PARAMETER
and O
F1 B-METRIC
score I-METRIC
. O
section 11
id pdf2json/2021.acl-long.322.pdf.json
Using O
the O
Wilcoxon O
signed-rank O
test O
( O
Wilcoxon O
, O
1992 O
) O
at O
5 O
% O
significance O
level O
, O
we O
found O
that O
these O
improvements O
over O
the O
baseline O
in O
both O
F1 B-METRIC
score I-METRIC
and O
accuracy B-PARAMETER
are O
statistically O
significant O
( O
p-value B-METRIC
< O
< O
0.05 O
) O
. O
section 11
id pdf2json/2021.acl-long.322.pdf.json
Observation O
2 O
: O
Pretrained O
sentence O
encoders O
struggle O
on O
this O
task O
. O
section 11
id pdf2json/2021.acl-long.322.pdf.json
We O
also O
applied O
pretrained O
sentence O
encoders O
such O
as O
BERT B-SOFTWARE
( O
Devlin B-AUTHOR
et O
al. O
, O
2019 O
) O
on O
the O
language O
modality O
for O
mood O
prediction O
. O
section 11
id pdf2json/2021.acl-long.322.pdf.json
Surprisingly O
, O
we O
found O
that O
none O
of O
these O
approaches O
performed O
stronger O
than O
a B-PARAMETER
simple O
bagof-words O
( O
see O
Table O
2 O
) O
. O
section 11
id pdf2json/2021.acl-long.322.pdf.json
We O
provide O
two O
possible O
explanations O
for O
this O
phenomenon O
: O
1 O
. O
section 11
id pdf2json/2021.acl-long.322.pdf.json
BERT B-SOFTWARE
is O
suitable O
for O
written O
text O
on O
the O
web O
( O
Wikipedia B-DATASET
, O
BookCorpus B-DATASET
, O
carefully O
humanannotated O
datasets O
) O
which O
may O
not O
generalize O
to O
informal O
typed O
text O
that O
contains O
emojis O
, O
typos O
, O
and O
abbreviations O
( O
see O
Section O
4.4 O
for O
a B-PARAMETER
qualitative O
analysis O
regarding O
the O
predictive O
abilities O
of O
emojis O
and O
keystrokes O
for O
mood O
prediction O
) O
. O
section 11
id pdf2json/2021.acl-long.322.pdf.json
2 O
. O
section 11
id pdf2json/2021.acl-long.322.pdf.json
We O
hypothesize O
that O
it O
is O
difficult O
to O
capture O
such O
long O
sequences O
of O
data O
( O
> O
1000 O
time O
steps O
) O
spread O
out O
over O
a B-PARAMETER
day O
. O
section 11
id pdf2json/2021.acl-long.322.pdf.json
Current O
work O
has O
shown O
that O
BERT B-SOFTWARE
struggles O
with O
long O
sequence O
lengths O
( O
Beltagy B-AUTHOR
et O
al. O
, O
2020 O
) O
. O
section 11
id pdf2json/2021.acl-long.322.pdf.json
We O
trained O
two O
extensions O
XLNet O
( O
Yang B-AUTHOR
et O
al. O
, O
2019 O
) O
and O
LongFormer O
( O
Beltagy B-AUTHOR
et O
al. O
, O
2020 O
) O
specifically O
designed O
to O
take O
in O
long-range O
context O
but O
found O
that O
they O
still O
underperform O
as O
compared O
to O
a B-PARAMETER
simple O
bag-of-words O
approach O
. O
section 11
id pdf2json/2021.acl-long.322.pdf.json
Observation O
3 O
: O
Fusing O
both O
text O
and O
keystroke O
timings O
improves O
performance O
. O
section 11
id pdf2json/2021.acl-long.322.pdf.json
This O
dataset O
presents O
a B-PARAMETER
unique O
opportunity O
to O
study O
representations O
of O
typed O
text O
as O
an O
alternative O
to O
conventionally O
studied O
written O
or O
spoken O
text O
. O
section 11
id pdf2json/2021.acl-long.322.pdf.json
While O
the O
latter O
two O
use O
language O
alone O
, O
typed O
text O
includes O
keystroke O
features O
providing O
information O
about O
the O
timings O
of O
when O
each O
character O
was O
typed O
. O
section 11
id pdf2json/2021.acl-long.322.pdf.json
In O
Table O
1 O
, O
we O
present O
some O
of O
our O
initial O
results O
in O
learning O
text O
and O
keystroke O
representations O
for O
mood O
prediction O
and O
show O
consistent O
improvements O
over O
text O
alone O
. O
section 11
id pdf2json/2021.acl-long.322.pdf.json
We O
further O
study O
the O
uniqueness O
of O
typed O
text O
by O
comparing O
the O
following O
baselines O
: O
1 O
. O
section 11
id pdf2json/2021.acl-long.322.pdf.json
Text O
: O
bag-of-words O
only O
. O
section 11
id pdf2json/2021.acl-long.322.pdf.json
2 O
. O
section 11
id pdf2json/2021.acl-long.322.pdf.json
Text O
+ O
char O
keystrokes O
: O
bag-of-words O
and O
bagof-timings O
across O
all O
characters O
. O
section 11
id pdf2json/2021.acl-long.322.pdf.json
3 O
. O
section 11
id pdf2json/2021.acl-long.322.pdf.json
Text O
+ O
split O
char O
keystrokes O
: O
bag-of-words O
and O
bag-of-timings O
subdivided O
between O
6 O
groups O
: O
alphanumeric O
characters O
, O
symbols O
, O
spacebar O
, O
enter O
, O
delete O
, O
and O
use O
of O
autocorrect O
. O
section 11
id pdf2json/2021.acl-long.322.pdf.json
This O
baseline O
presents O
a B-PARAMETER
more O
fine-grained O
decomposition O
of O
the O
typing O
speeds O
across O
different O
semantically O
related O
character O
groups O
. O
section 11
id pdf2json/2021.acl-long.322.pdf.json
4 O
. O
section 11
id pdf2json/2021.acl-long.322.pdf.json
Text O
+ O
word B-METHOD
keystrokes O
: O
bag-of-words O
and O
bagof-timings O
summed O
up O
over O
the O
characters O
in O
each O
word B-METHOD
. O
section 11
id pdf2json/2021.acl-long.322.pdf.json
This O
presents O
a B-PARAMETER
more O
interpretable O
model O
to O
analyze O
the O
relationships O
between O
words O
and O
the O
distribution O
of O
their O
typing O
speeds O
. O
section 11
id pdf2json/2021.acl-long.322.pdf.json
From O
Table O
3 O
, O
we O
observe O
that O
keystrokes O
accurately O
contextualize O
text O
, O
especially O
when O
using O
fine-grained O
keystroke O
distributions O
across O
individual O
characters O
. O
section 11
id pdf2json/2021.acl-long.322.pdf.json
Other O
methods O
incorporating O
keystroke O
features O
are O
also O
all O
stronger O
than O
unimodal O
models O
. O
section 11
id pdf2json/2021.acl-long.322.pdf.json
Different O
ways O
of O
representing O
keystrokes O
also O
provide O
different O
levels O
of O
interpretability O
regarding O
the O
relationships O
between O
words O
, O
characters O
, O
and O
keystrokes O
for O
mood O
prediction O
, O
which O
we O
qualitatively O
analyze O
in O
§4.4 O
. O
section 11
id pdf2json/2021.acl-long.322.pdf.json
Observation O
4 O
: O
Multimodal O
representation B-METHOD
learning I-METHOD
achieves O
the O
best O
performance O
. O
section 11
id pdf2json/2021.acl-long.322.pdf.json
In O
Table O
1 O
, O
we O
also O
compare O
the O
performance O
of O
our O
models O
on O
combined O
( O
text O
+ O
keystroke O
+ O
apps O
) O
features O
versus O
the O
performance O
on O
each O
individual O
feature O
set O
. O
section 11
id pdf2json/2021.acl-long.322.pdf.json
For O
both O
metrics O
, O
combining O
all O
features O
gives O
better O
performance O
over O
either O
subset O
. O

section 12
id pdf2json/2021.acl-long.322.pdf.json
Despite O
these O
promising O
results O
in O
mood O
prediction O
, O
we O
ask O
an O
important O
question O
: O
Does O
the O
model O
capture O
user O
identities O
as O
an O
intermediate O
step O
towards O
predicting O
mood O
? O
section 12
id pdf2json/2021.acl-long.322.pdf.json
To O
answer O
this O
question O
, O
we O
an- O
alyze O
the O
privacy O
of O
raw O
mobile O
data O
and O
trained O
models O
. O
section 12
id pdf2json/2021.acl-long.322.pdf.json
We O
then O
study O
our O
proposed O
method O
of O
learning O
privacy-preserving O
features O
to O
determine O
whether O
it O
can O
obfuscate O
user O
identity O
while O
remaining O
predictive O
of O
daily O
mood O
. O
section 12
id pdf2json/2021.acl-long.322.pdf.json
How O
private O
is O
the O
mobile O
data O
? O
section 12
id pdf2json/2021.acl-long.322.pdf.json
We O
evaluate O
how O
much O
the O
data O
reveal O
user O
identities O
by O
training O
predictive O
models O
with O
typed O
text O
, O
keystroke O
timings O
, O
and O
app O
usage O
as O
input O
and O
user O
identity O
as O
the O
prediction O
target B-PARAMETER
. O
section 12
id pdf2json/2021.acl-long.322.pdf.json
From O
Table O
4 O
, O
we O
observe O
that O
all O
modalities O
are O
very O
predictive O
of O
user O
identity O
( O
> O
87 O
% O
accuracy B-PARAMETER
) O
, O
which O
further O
motivates O
the O
need O
to O
learn O
privacy-preserving O
features O
. O
section 12
id pdf2json/2021.acl-long.322.pdf.json
We O
further O
note O
that O
identifiable O
information O
can O
be O
very O
subtle O
: O
while O
only O
28/1000 O
words O
were O
named O
entities O
, O
it O
was O
possible O
to O
identify O
the O
user O
identity O
with O
> O
87 O
% O
accuracy B-PARAMETER
, O
which O
means O
that O
subtle O
word B-METHOD
choice O
can O
be O
identify O
the O
user O
( O
similarly O
for O
apps O
and O
keystrokes O
) O
. O
section 12
id pdf2json/2021.acl-long.322.pdf.json
How O
private O
are O
the O
learned O
privacy-preserving O
features O
? O
section 12
id pdf2json/2021.acl-long.322.pdf.json
We O
also O
study O
whether O
our O
learned O
features O
are O
correlated O
with O
user O
identity O
through O
both O
visualizations O
and O
quantitative O
evaluations O
. O
section 12
id pdf2json/2021.acl-long.322.pdf.json
Visualizations O
: O
We O
use O
t-SNE B-METHOD
( O
Van O
der O
Maaten O
and O
Hinton B-AUTHOR
, O
2008 O
) O
to O
reduce O
the O
learned O
features O
from O
trained O
models O
to O
2 O
dimensions O
. O
section 12
id pdf2json/2021.acl-long.322.pdf.json
After O
color-coding O
the O
points O
by O
participant O
identity O
, O
we O
identify O
distinct O
clusters O
in O
Figure O
3 O
( O
a B-PARAMETER
) O
, O
which O
implies O
that O
mood O
prediction O
can O
be O
strongly O
linked O
to O
identi- O
fying O
the O
person O
, O
therefore O
coming O
at O
the O
price O
of O
losing O
privacy O
. O
section 12
id pdf2json/2021.acl-long.322.pdf.json
As O
an O
attempt O
to O
reduce O
reliance O
on O
user O
identity O
, O
we O
train O
NI-MLP O
which O
is O
designed O
to O
obfuscate O
user-dependent O
features O
. O
section 12
id pdf2json/2021.acl-long.322.pdf.json
After O
training O
NI-MLP O
, O
we O
again O
visualize O
the O
representations O
learned O
in O
Figure O
3 O
( O
b O
) O
and O
we O
find O
that O
they O
are O
less O
visually O
separable O
by O
users O
, O
indicating O
that O
NI-MLP O
indeed O
learns O
more O
user-independent O
features O
. O
section 12
id pdf2json/2021.acl-long.322.pdf.json
Quantitative O
evaluation O
: O
To O
empirically O
evaluate O
how O
well O
our O
models O
preserve O
privacy O
, O
we O
extracted O
the O
final O
layer O
of O
each O
trained O
model O
and O
fit O
a B-PARAMETER
logistic B-METHOD
regression I-METHOD
model O
to O
predict O
user O
identity O
using O
these O
final O
layer O
representations O
as O
input O
. O
section 12
id pdf2json/2021.acl-long.322.pdf.json
The O
more O
a B-PARAMETER
model O
preserves O
privacy O
, O
the O
harder O
it O
should O
be O
to O
predict O
user O
identity O
. O
section 12
id pdf2json/2021.acl-long.322.pdf.json
From O
Table O
5 O
, O
we O
observe O
that O
we O
can O
predict O
user O
identity O
based O
on O
the O
learned O
MLP B-METHOD
representations O
with O
high O
accuracy B-PARAMETER
( O
> O
85 O
% O
) O
using O
the O
most O
sensitive O
app O
usage O
features O
. O
section 12
id pdf2json/2021.acl-long.322.pdf.json
For O
other O
modality O
combinations O
, O
user O
identity O
can O
also O
be O
decoded O
with O
more O
than O
70 O
% O
accuracy B-PARAMETER
with O
the O
exception O
of O
keystrokes O
which O
are O
the O
most O
private O
( O
55 O
% O
) O
. O
section 12
id pdf2json/2021.acl-long.322.pdf.json
We O
achieve O
significantly O
more O
privacy O
using O
NI-MLP O
embeddings O
- O
roughly O
35 O
% O
for O
the O
best O
multimodal O
model O
, O
which O
indicates O
the O
possibility O
of O
NI-MLP O
as O
a B-PARAMETER
means O
of O
achieving O
privacy-preserving O
mood O
prediction O
. O
section 12
id pdf2json/2021.acl-long.322.pdf.json
Understanding O
the O
tradeoff O
between O
performance O
and O
privacy O
: O
NI-MLP O
provides O
a B-PARAMETER
tunable O
parameter O
σ O
to O
control O
the O
variance B-METRIC
of O
noise O
applied O
on O
the O
identity-related O
dimensions O
. O
section 12
id pdf2json/2021.acl-long.322.pdf.json
This O
parameter O
σ O
has O
the O
potential O
to O
give O
a B-PARAMETER
tradeoff O
between O
privacy O
and O
prediction O
performance O
. O
section 12
id pdf2json/2021.acl-long.322.pdf.json
In O
Figure O
4 O
, O
we O
plot O
this O
tradeoff O
between O
performance O
( O
mood O
prediction O
F1 B-METRIC
score I-METRIC
, O
higher O
is O
better O
) O
and O
privacy O
( O
identity O
prediction O
accuracy B-PARAMETER
, O
lower O
is O
better O
) O
. O
section 12
id pdf2json/2021.acl-long.322.pdf.json
We O
find O
that O
keystroke O
features O
, O
while O
themselves O
not O
very O
useful O
in O
predicting O
mood O
, O
are O
highly O
private O
features O
. O
section 12
id pdf2json/2021.acl-long.322.pdf.json
It O
is O
important O
to O
note O
that O
keystroke O
features O
show O
strong O
performance O
when O
integrated O
with O
text O
and O
app O
usage O
features O
while O
also O
increasing O
privacy O
, O
thereby O
pushing O
the O
Pareto O
front O
outwards O
. O
section 12
id pdf2json/2021.acl-long.322.pdf.json
It O
is O
also O
interesting O
to O
observe O
that O
for O
most O
models O
, O
performance O
stays O
level O
while O
privacy O
improves O
, O
which O
is O
a B-PARAMETER
promising O
sign O
for O
the O
real-world O
deployment O
of O
such O
models O
which O
requires O
a B-PARAMETER
balance O
between O
both O
desiderata O
. O

section 13
id pdf2json/2021.acl-long.322.pdf.json
To O
further O
shed O
light O
on O
the O
relationships O
between O
mood O
prediction O
performance O
and O
privacy O
, O
we O
performed O
a B-PARAMETER
more O
in-depth O
study O
of O
the O
text O
, O
keystroke O
, O
and O
app O
usage O
features O
learned O
by O
the O
model O
( O
see O
Appendix O
D.3 O
for O
more O
examples O
) O
. O
section 13
id pdf2json/2021.acl-long.322.pdf.json
Understanding O
the O
unimodal O
features O
: O
We O
first O
analyze O
how O
individual O
words O
, O
keystroke O
timings O
, O
and O
app O
usage O
are O
indicative O
of O
positive O
or O
negative O
mood O
for O
different O
users O
. O
section 13
id pdf2json/2021.acl-long.322.pdf.json
Text O
: O
We O
find O
that O
several O
words O
are O
particularly O
indicative O
of O
mood O
: O
can O
’ O
t/cant O
, O
don O
’ O
t/don O
’ O
t O
, O
and O
sorry O
are O
negative O
for O
more O
users O
than O
positive O
, O
while O
yes O
is O
overwhelmingly O
positive O
across O
users O
( O
9 O
pos O
, O
1 O
neg O
) O
, O
but O
yeah O
is O
slightly O
negative O
( O
5 O
pos O
, O
7 O
neg O
) O
. O
section 13
id pdf2json/2021.acl-long.322.pdf.json
We O
also O
analyze O
the O
use O
of O
emojis O
in O
typed O
text O
and O
find O
that O
while O
there O
are O
certain O
emojis O
that O
lean O
positive O
( O
e.g. O
, O
) O
, O
there O
are O
ones O
( O
e.g. O
, O
: O
( O
and O
) O
that O
used O
in O
both O
contexts O
depending O
on O
the O
user O
( O
see O
Table O
6 O
) O
. O
section 13
id pdf2json/2021.acl-long.322.pdf.json
Apps O
: O
In O
Table O
7 O
, O
we O
show O
the O
top O
3 O
apps O
associated O
with O
positive O
or O
negative O
moods O
across O
several O
users O
. O
section 13
id pdf2json/2021.acl-long.322.pdf.json
It O
is O
interesting O
to O
observe O
that O
many O
outdoor O
apps O
( O
i.e. O
, O
Weather O
, O
MyFitnessPal O
, O
Uber O
) O
, O
photo O
sharing O
apps O
( O
i.e. O
, O
Photos O
, O
Snapchat O
) O
, O
and O
calling O
apps O
( O
i.e. O
, O
FaceTime O
, O
Phone O
) O
are O
associated O
with O
positive O
mood O
, O
while O
personal O
apps O
such O
as O
personal O
management O
( O
i.e. O
, O
Calendar O
, O
Notes O
, O
Siri O
) O
, O
web O
browsing O
( O
i.e. O
, O
Chrome O
, O
Safari O
) O
, O
and O
shopping O
( O
i.e. O
, O
App O
Store O
) O
are O
associated O
with O
negative O
mood O
. O
section 13
id pdf2json/2021.acl-long.322.pdf.json
However O
, O
some O
of O
these O
findings O
are O
rather O
userspecific O
( O
e.g. O
, O
Phone O
can O
be O
both O
positive O
or O
negative O
depending O
on O
the O
user O
) O
. O
section 13
id pdf2json/2021.acl-long.322.pdf.json
Understanding O
the O
multimodal O
features O
: O
We O
also O
analyze O
how O
the O
same O
characters O
and O
words O
can O
contribute O
to O
different O
mood O
predictions O
based O
on O
their O
keystroke O
patterns O
. O
section 13
id pdf2json/2021.acl-long.322.pdf.json
As O
an O
example O
, O
the O
distribution O
of O
keystrokes O
for O
the O
enter O
character O
on O
the O
keyboard O
differs O
according O
to O
the O
daily O
mood O
of O
one O
user O
( O
see O
Figure O
5 O
and O
Appendix O
D.3 O
for O
more O
users O
) O
. O
section 13
id pdf2json/2021.acl-long.322.pdf.json
In O
Table O
8 O
, O
we O
extend O
this O
analysis O
to O
entire O
words O
. O
section 13
id pdf2json/2021.acl-long.322.pdf.json
For O
each O
of O
the O
500 O
most O
common O
words O
, O
we O
aggregated O
their O
accompanying O
keystroke O
timings O
for O
user-reported O
positive O
and O
negative O
mood O
. O
section 13
id pdf2json/2021.acl-long.322.pdf.json
These O
two O
distributions O
tell O
us B-PARAMETER
how O
the O
same O
word B-METHOD
in O
different O
keystroke O
contexts O
can O
indicate O
different O
moods O
. O
section 13
id pdf2json/2021.acl-long.322.pdf.json
We O
performed O
Wilcoxon O
rank-sum O
tests O
at O
5 O
% O
significance O
level O
to O
compare O
these O
distributions O
and O
recorded O
the O
words O
in O
which O
either O
faster O
or O
slower O
typing O
was O
statistically O
significantly O
correlated O
with O
either O
mood O
. O
section 13
id pdf2json/2021.acl-long.322.pdf.json
Observe O
how O
certain O
semantically O
positive O
words O
like O
love O
, O
thank O
, O
and O
haha O
become O
judged O
as O
more O
positive O
when O
typed O
at O
a B-PARAMETER
faster O
speed O
. O
section 13
id pdf2json/2021.acl-long.322.pdf.json
Therefore O
, O
contextualizing O
text O
with O
their O
keystroke O
timings O
offers O
additional O
information O
when O
learning O
representations O
of O
typed O
text O
. O

section 14
id pdf2json/2021.acl-long.322.pdf.json
In O
this O
paper O
, O
we O
investigated O
the O
learning O
of O
language O
and O
multimodal O
representations O
of O
typed O
text O
collected O
from O
mobile O
data O
. O
section 14
id pdf2json/2021.acl-long.322.pdf.json
We O
studied O
the O
challenge O
of O
learning O
markers O
of O
daily O
mood O
as O
a B-PARAMETER
step O
towards O
early O
detection O
and O
intervention O
of O
mental O
health O
disorders O
for O
social O
good O
. O
section 14
id pdf2json/2021.acl-long.322.pdf.json
Our O
method O
also O
shows O
promising O
results O
in O
obfuscating O
user O
identities O
for O
privacy-preserving O
learning O
, O
a B-PARAMETER
direction O
crucial O
towards O
real-world O
learning O
from O
sensitive O
mobile O
data O
and O
healthcare O
labels O
. O
section 14
id pdf2json/2021.acl-long.322.pdf.json
In O
addition O
, O
our O
findings O
illustrate O
several O
challenges O
and O
opportunities O
in O
representation B-METHOD
learning I-METHOD
from O
typed O
text O
as O
an O
understudied O
area O
in O
NLP O
. O
section 14
id pdf2json/2021.acl-long.322.pdf.json
Limitations O
& O
future O
work O
: O
While O
our O
approach O
shows O
promises O
in O
learning O
representations O
for O
mood O
prediction O
, O
several O
future O
directions O
on O
the O
modeling O
and O
NLP O
side O
include O
: O
1 O
) O
better O
models O
and O
pre-training O
algorithms O
for O
NLP O
on O
typed O
text O
, O
2 O
) O
algorithms O
that O
provide O
formal O
guarantees O
of O
privacy O
( O
Dwork O
, O
2008 O
) O
, O
and O
3 O
) O
federated O
training O
from O
decentralized O
data O
( O
McMahan O
et O
al. O
, O
2016 O
) O
to O
improve O
privacy O
( O
Geyer O
et O
al. O
, O
2017 O
) O
and O
fairness O
( O
Liang B-AUTHOR
et O
al. O
, O
2020a O
) O
of O
sensitive O
data O
. O
section 14
id pdf2json/2021.acl-long.322.pdf.json
We O
describe O
more O
limitations O
and O
future O
social O
implications O
of O
our O
work O
in O
our O
broader O
impact O
statement O
in O
Appendix O
A O
. O

section 15
id pdf2json/2021.acl-long.322.pdf.json
This O
material O
was O
based O
upon O
work O
partially O
supported O
by O
the O
National O
Science O
Foundation O
( O
Awards O
# O
1750439 O
and O
# O
1734868 O
) O
and O
the O
National O
Institutes O
of O
Health O
( O
Award O
# O
U01MH116923 O
) O
. O
section 15
id pdf2json/2021.acl-long.322.pdf.json
MM O
was O
supported O
by O
the O
Swiss O
National O
Science O
Foundation O
( O
# O
P2GEP2_184518 O
) O
. O
section 15
id pdf2json/2021.acl-long.322.pdf.json
RS O
was O
supported O
by O
NSF O
IIS1763562 O
and O
ONR O
Grant O
N000141812861 O
. O
section 15
id pdf2json/2021.acl-long.322.pdf.json
Any O
opinions O
, O
findings O
, O
and O
conclusions O
, O
or O
recommendations O
expressed O
in O
this O
material O
are O
those O
of O
the O
author O
( O
s O
) O
and O
do O
not O
necessarily O
reflect O
the O
views O
of O
the O
National O
Science O
Foundation O
, O
National O
Institutes O
of O
Health O
, O
or O
Office O
of O
Naval O
Research O
, O
and O
no O
official O
endorsement O
should O
be O
inferred O
. O
section 15
id pdf2json/2021.acl-long.322.pdf.json
We O
would O
also O
like O
to O
acknowledge O
NVIDIA O
’ O
s O
GPU B-SOFTWARE
support O
and O
the O
anonymous O
reviewers O
for O
their O
extremely O
helpful O
comments O
. O

section 16
id pdf2json/2021.acl-long.322.pdf.json
Learning O
markers O
of O
mood O
from O
mobile O
data O
presents O
an O
opportunity O
for O
large-scale O
adaptive O
interventions O
of O
suicidal O
ideation O
. O
section 16
id pdf2json/2021.acl-long.322.pdf.json
However O
, O
there O
are O
important O
concerns O
regarding O
its O
implications O
to O
society O
and O
policy O
. O
section 16
id pdf2json/2021.acl-long.322.pdf.json
Applications O
in O
mental O
health O
: O
Suicide O
is O
the O
second O
leading O
cause O
of O
death O
among O
adolescents O
. O
section 16
id pdf2json/2021.acl-long.322.pdf.json
In O
addition O
to O
deaths O
, O
16 O
% O
of O
high O
school O
students O
report O
seriously O
considering O
suicide O
each O
year O
, O
and O
8 O
% O
make O
one O
or O
more O
suicide O
attempts O
( O
CDC O
, O
2015 O
) O
. O
section 16
id pdf2json/2021.acl-long.322.pdf.json
Despite O
these O
alarming O
statistics O
, O
there O
is O
little O
consensus O
concerning O
imminent O
risk O
for O
suicide O
( O
Franklin O
et O
al. O
, O
2017 O
; O
Large O
et O
al. O
, O
2017 O
) O
. O
section 16
id pdf2json/2021.acl-long.322.pdf.json
Current O
research O
conducts O
clinical O
interviews O
and O
patient O
self-report O
questionnaires O
that O
provide O
longterm O
assessments O
of O
suicide O
risk O
. O
section 16
id pdf2json/2021.acl-long.322.pdf.json
However O
, O
few O
studies O
have O
focused O
on O
imminent O
suicidal O
risk O
, O
which O
is O
of O
critical O
clinical O
importance O
as O
a B-PARAMETER
step O
towards O
adaptive O
real-time O
interventions O
( O
Glenn O
and O
Nock O
, O
2014 O
; O
Schuck O
et O
al. O
, O
2019 O
) O
. O
section 16
id pdf2json/2021.acl-long.322.pdf.json
Given O
the O
impact O
of O
suicide O
on O
society O
, O
there O
is O
an O
urgent O
need O
to O
better O
understand O
the O
behavior O
markers O
related O
to O
suicidal O
ideation O
. O
section 16
id pdf2json/2021.acl-long.322.pdf.json
“ O
Just-in-time O
” O
adaptive O
interventions O
delivered O
via O
mobile O
health O
applications O
provide O
a B-PARAMETER
platform O
of O
exciting O
developments O
in O
low-intensity O
, O
high-impact O
interventions O
( O
Nahum-Shani O
et O
al. O
, O
2018 O
) O
. O
section 16
id pdf2json/2021.acl-long.322.pdf.json
The O
ability O
to O
intervene O
precisely O
during O
an O
acute O
risk O
for O
suicide O
could O
dramatically O
reduce O
the O
loss O
of O
life O
. O
section 16
id pdf2json/2021.acl-long.322.pdf.json
To O
realize O
this O
goal O
, O
we O
need O
accurate O
and O
timely O
methods O
that O
predict O
when O
interventions O
are O
most O
needed O
. O
section 16
id pdf2json/2021.acl-long.322.pdf.json
Monitoring O
( O
with O
participants O
’ O
permission O
) O
mobile O
data O
to O
assess O
mental O
health O
and O
provide O
early O
interventions O
is O
, O
therefore O
, O
a B-PARAMETER
rich O
opportunity O
for O
scalable O
deployment O
across O
high-risk O
populations O
. O
section 16
id pdf2json/2021.acl-long.322.pdf.json
Our O
data O
collection O
, O
experimental O
study O
, O
and O
computational O
approaches O
provide O
a B-PARAMETER
step O
towards O
data-intensive O
longitudinal O
monitoring O
of O
human O
behavior O
. O
section 16
id pdf2json/2021.acl-long.322.pdf.json
However O
, O
one O
must O
take O
care O
to O
summarize O
behaviors O
from O
mobile O
data O
without O
identifying O
the O
user O
through O
personal O
( O
e.g. O
, O
personally O
identifiable O
information O
) O
or O
protected O
attributes O
( O
e.g. O
, O
race O
, O
gender O
) O
. O
section 16
id pdf2json/2021.acl-long.322.pdf.json
This O
form O
of O
anonymity O
is O
critical O
when O
implementing O
these O
technologies O
in O
real-world O
scenarios O
. O
section 16
id pdf2json/2021.acl-long.322.pdf.json
Our O
goal O
is O
to O
be O
highly O
predictive O
of O
mood O
while O
remaining O
as O
privacy-preserving O
as O
possible O
. O
section 16
id pdf2json/2021.acl-long.322.pdf.json
We O
outline O
some O
of O
the O
potential O
privacy O
and O
security O
concerns O
below O
. O
section 16
id pdf2json/2021.acl-long.322.pdf.json
Limitations O
: O
While O
we O
hope O
that O
our O
research O
can O
provide O
a B-PARAMETER
starting O
point O
on O
the O
potential O
of O
detecting O
mood O
unobtrusively O
throughout O
the O
day O
in O
a B-PARAMETER
privacy-preserving O
way O
, O
we O
strongly O
acknowledge O
there O
remain O
methodological O
issues O
where O
a B-PARAMETER
lot O
more O
research O
needs O
to O
be O
done O
to O
enable O
the O
realworld O
deployment O
of O
such O
technologies O
. O
section 16
id pdf2json/2021.acl-long.322.pdf.json
We O
emphasize O
that O
healthcare O
providers O
and O
mobile O
app O
startups O
should O
not O
attempt O
to O
apply O
our O
approach O
in O
the O
real O
world O
until O
the O
following O
issues O
( O
and O
many O
more O
) O
can O
be O
reliably O
resolved O
: O
1 O
. O
section 16
id pdf2json/2021.acl-long.322.pdf.json
We O
do O
not O
make O
broad O
claims O
across O
teenage O
populations O
from O
only O
17 O
participants O
in O
this O
study O
. O
section 16
id pdf2json/2021.acl-long.322.pdf.json
Furthermore O
, O
it O
remains O
challenging O
for O
models O
to O
perform O
person-independent O
prediction O
which O
makes O
it O
hard O
to O
deploy O
across O
large O
populations O
. O
section 16
id pdf2json/2021.acl-long.322.pdf.json
2 O
. O
section 16
id pdf2json/2021.acl-long.322.pdf.json
Our O
current O
work O
on O
predicting O
daily O
mood O
is O
still O
a B-PARAMETER
long O
way O
from O
predicting O
imminent O
suicide O
risk O
. O
section 16
id pdf2json/2021.acl-long.322.pdf.json
Furthermore O
, O
any O
form O
of O
prediction O
is O
still O
significantly O
far O
away O
from O
integrating O
methods O
like O
this O
into O
the O
actual O
practice O
of O
mental O
health O
, O
which O
is O
a B-PARAMETER
challenging O
problem O
involving O
a B-PARAMETER
broad O
range O
of O
medical O
, O
ethical O
, O
social O
, O
and O
technological O
researchers O
( O
Resnik O
et O
al. O
, O
2021 O
; O
Lee B-AUTHOR
et O
al. O
, O
2021 O
) O
. O
section 16
id pdf2json/2021.acl-long.322.pdf.json
3 O
. O
section 16
id pdf2json/2021.acl-long.322.pdf.json
Text O
and O
keystrokes O
can O
differ O
for O
participants O
who O
speak O
multiple O
languages O
or O
non-prestige O
vernaculars O
. O
section 16
id pdf2json/2021.acl-long.322.pdf.json
One O
will O
need O
to O
ensure O
that O
the O
method O
works O
across O
a B-PARAMETER
broad O
range O
of O
languages O
to O
ensure O
accessibility O
in O
its O
desired O
outcomes O
. O
section 16
id pdf2json/2021.acl-long.322.pdf.json
4 O
. O
section 16
id pdf2json/2021.acl-long.322.pdf.json
This O
study O
assumes O
that O
participants O
have O
no O
restrictions O
for O
data/network O
connections O
& O
data O
plans O
on O
their O
phones O
, O
which O
may O
leave O
out O
vulnerable O
populations O
that O
do O
not O
meet O
this O
criterion O
. O
section 16
id pdf2json/2021.acl-long.322.pdf.json
Privacy O
and O
security O
: O
There O
are O
privacy O
risks O
associated O
with O
making O
predictions O
from O
mobile O
data O
. O
section 16
id pdf2json/2021.acl-long.322.pdf.json
To O
deploy O
these O
algorithms O
across O
at-risk O
populations O
, O
it O
is O
important O
to O
keep O
data O
private O
on O
each O
device O
without O
sending O
it O
to O
other O
locations O
. O
section 16
id pdf2json/2021.acl-long.322.pdf.json
Even O
if O
data O
is O
kept O
private O
, O
it O
is O
possible O
to O
decode O
data O
from O
gradients O
( O
Zhu B-AUTHOR
and O
Han B-AUTHOR
, O
2020 O
) O
or O
pretrained O
models O
( O
Carlini O
et O
al. O
, O
2020 O
) O
. O
section 16
id pdf2json/2021.acl-long.322.pdf.json
In O
addition O
, O
sensitive O
databases O
with O
private O
mobile O
data O
could O
be O
at-risk O
to O
external O
security O
attacks O
from O
adversaries O
( O
Lyu O
et O
al. O
, O
2020 O
) O
. O
section 16
id pdf2json/2021.acl-long.322.pdf.json
Therefore O
, O
it O
is O
crucial O
to O
obtain O
user O
consent O
before O
collecting O
device O
data O
. O
section 16
id pdf2json/2021.acl-long.322.pdf.json
In O
our O
exper- O
iments O
with O
real-world O
mobile O
data O
, O
all O
participants O
have O
given O
consent O
for O
their O
mobile O
device O
data O
to O
be O
collected O
and O
shared O
with O
us B-PARAMETER
for O
research O
purposes O
. O
section 16
id pdf2json/2021.acl-long.322.pdf.json
All O
data O
was O
anonymized O
and O
stripped O
of O
all O
personal O
( O
e.g. O
, O
personally O
identifiable O
information O
) O
and O
protected O
attributes O
( O
e.g. O
, O
race O
, O
gender O
) O
. O
section 16
id pdf2json/2021.acl-long.322.pdf.json
Social O
biases O
: O
We O
acknowledge O
that O
there O
is O
a B-PARAMETER
risk O
of O
exposure O
bias B-PARAMETER
due O
to O
imbalanced O
datasets O
, O
especially O
when O
personal O
mobile O
data O
and O
sensitive O
health O
labels O
( O
e.g. O
, O
daily O
mood O
, O
suicidal O
thoughts O
and O
behaviors O
, O
suicide O
risk O
) O
. O
section 16
id pdf2json/2021.acl-long.322.pdf.json
Models O
trained O
on O
biased O
data O
have O
been O
shown O
to O
amplify O
the O
underlying O
social O
biases O
especially O
when O
they O
correlate O
with O
the O
prediction O
targets O
( O
Lloyd O
, O
2018 O
) O
. O
section 16
id pdf2json/2021.acl-long.322.pdf.json
This O
leaves O
room O
for O
future O
work O
in O
exploring O
methods O
tailored O
for O
specific O
scenarios O
such O
as O
mitigating O
social O
biases O
in O
words O
( O
Bolukbasi O
et O
al. O
, O
2016 O
) O
, O
sentences O
( O
Liang B-AUTHOR
et O
al. O
, O
2020a O
) O
, O
and O
images O
( O
Otterbacher O
et O
al. O
, O
2018 O
) O
. O
section 16
id pdf2json/2021.acl-long.322.pdf.json
Future O
research O
should O
also O
focus O
on O
quantifying O
the O
trade-offs O
between O
fairness O
and O
performance O
( O
Zhao B-AUTHOR
and O
Gordon O
, O
2019 O
) O
. O
section 16
id pdf2json/2021.acl-long.322.pdf.json
Overall O
, O
we O
believe O
that O
our O
proposed O
approach O
can O
help O
quantify O
the O
tradeoffs O
between O
performance O
and O
privacy O
. O
section 16
id pdf2json/2021.acl-long.322.pdf.json
We O
hope O
that O
this O
brings O
about O
future O
opportunities O
for O
large-scale O
real-time O
analytics O
in O
healthcare O
applications O
. O

section 17
id pdf2json/2021.acl-long.322.pdf.json
The O
Mobile O
Assessment O
for O
the O
Prediction O
of O
Suicide O
( O
MAPS O
) O
dataset O
was O
designed O
to O
elucidate O
real-time O
indicators O
of O
suicide O
risk O
in O
adolescents O
ages O
13 O
− O
18 O
years O
. O
section 17
id pdf2json/2021.acl-long.322.pdf.json
Current O
adolescent O
suicide O
ideators O
and O
recent O
suicide O
attempters O
along O
with O
aged-matched O
psychiatric O
controls O
with O
no O
lifetime O
suicidal O
thoughts O
and O
behaviors O
completed O
baseline O
clinical O
assessments O
( O
i.e. O
, O
lifetime O
mental O
disorders O
, O
current O
psychiatric O
symptoms O
) O
. O
section 17
id pdf2json/2021.acl-long.322.pdf.json
Following O
the O
baseline O
clinical O
characterization O
, O
a B-PARAMETER
smartphone O
app O
, O
the O
Effortless O
Assessment O
of O
Risk O
States O
( O
EARS O
) O
, O
was O
installed O
onto O
adolescents O
’ O
phones O
, O
and O
passive O
sensor O
data O
were O
acquired O
for O
6-months O
. O
section 17
id pdf2json/2021.acl-long.322.pdf.json
Notably O
, O
during O
EARS O
installation O
, O
a B-PARAMETER
keyboard O
logger O
is O
configured O
on O
adolescents O
’ O
phones O
, O
which O
then O
tracks O
all O
words O
typed O
into O
the O
phone O
as O
well O
as O
the O
apps O
used O
during O
this O
period O
. O
section 17
id pdf2json/2021.acl-long.322.pdf.json
Each O
day O
during O
the O
6- O
month O
follow-up O
, O
participants O
also O
were O
asked O
to O
rate O
their O
mood O
on O
the O
previous O
day O
on O
a B-PARAMETER
scale O
ranging O
from O
1− O
100 O
, O
with O
higher O
scores O
indicating O
a B-PARAMETER
better O
mood O
. O
section 17
id pdf2json/2021.acl-long.322.pdf.json
After O
extracting O
multimodal O
features O
and O
discretizing O
the O
labels O
( O
see O
Section O
2 O
) O
, O
we O
summarize O
the O
final O
dataset O
feature O
and O
label O
statistics O
in O
Table O
9 O
. O

section 18
id pdf2json/2021.acl-long.322.pdf.json
We O
provide O
additional O
details O
on O
the O
model O
implementation O
and O
experimental O
setup O
. O
section 18
id pdf2json/2021.acl-long.322.pdf.json
C.1 O
Implementation O
Details O
All O
models O
and O
analyses O
were O
done O
in O
Python O
. O
section 18
id pdf2json/2021.acl-long.322.pdf.json
SVM O
models O
were O
implemented O
with O
Scikitlearn O
and O
MLP/NI-MLP O
models O
were O
implemented O
with O
PyTorch B-SOFTWARE
. O
section 18
id pdf2json/2021.acl-long.322.pdf.json
BERT B-SOFTWARE
, O
XLNet O
, O
and O
Longformer O
models O
were O
fine-tuned O
using O
Hugging B-SOFTWARE
Face I-SOFTWARE
( O
website O
: O
https O
: O
//huggingface.co O
, O
GitHub B-SOFTWARE
: O
https O
: O
//github.com/huggingface O
) O
. O
section 18
id pdf2json/2021.acl-long.322.pdf.json
C.2 O
Hyperparameters O
We O
performed O
a B-PARAMETER
small O
hyperparameter O
search O
over O
the O
ranges O
in O
Table O
10 O
. O
section 18
id pdf2json/2021.acl-long.322.pdf.json
This O
resulted O
in O
a B-PARAMETER
total O
of O
35 O
hyperparameter O
configurations O
for O
SVM O
and O
12 O
for O
MLP B-METHOD
( O
6 O
for O
apps O
only O
) O
. O
section 18
id pdf2json/2021.acl-long.322.pdf.json
By O
choosing O
the O
best-performing O
model O
on O
the O
validation O
set O
, O
we O
selected O
the O
resulting O
hyperparameters O
as O
shown O
in O
Table O
10 O
. O
section 18
id pdf2json/2021.acl-long.322.pdf.json
C.3 O
Model O
Parameters O
Each O
model O
has O
about O
two O
million O
parameters O
. O
section 18
id pdf2json/2021.acl-long.322.pdf.json
See O
Table O
10 O
for O
exact O
hidden O
dimension O
sizes O
. O
section 18
id pdf2json/2021.acl-long.322.pdf.json
C.4 O
Training O
Resources O
and O
Time O
All O
experiments O
were O
conducted O
on O
a B-PARAMETER
GeForce B-SOFTWARE
RTX O
2080 O
Ti O
GPU B-SOFTWARE
with O
12 O
GB O
memory O
. O
section 18
id pdf2json/2021.acl-long.322.pdf.json
See O
Table O
11 B-PARAMETER
for O
approximate O
running O
times O
. O

section 19
id pdf2json/2021.acl-long.322.pdf.json
We O
present O
several O
additional O
analysis O
of O
the O
data O
and O
empirical O
results O
: O
D.1 O
Details O
on O
Mood O
Prediction O
There O
is O
often O
a B-PARAMETER
tradeoff O
between O
privacy O
and O
prediction O
performance O
. O
section 19
id pdf2json/2021.acl-long.322.pdf.json
To O
control O
this O
tradeoff O
, O
we O
vary O
the O
parameter O
σ O
, O
which O
is O
the O
amount O
of O
noise O
added O
to O
the O
identity-dependent O
subspace O
across O
batches O
and O
training O
epochs B-PARAMETER
. O
section 19
id pdf2json/2021.acl-long.322.pdf.json
In O
practice O
, O
we O
automatically O
perform O
model O
selection O
using O
this O
performance-privacy O
ratio O
R O
computed O
on O
the O
validation O
set O
, O
where O
R O
= O
sMLP O
− O
sNI-MLP O
tMLP O
− O
tNI-MLP O
( O
4 O
) O
is O
defined O
as O
the O
improvement O
in O
privacy O
per O
unit O
of O
performance O
lost O
. O
section 19
id pdf2json/2021.acl-long.322.pdf.json
Here O
, O
s O
is O
defined O
as O
the O
accuracy B-PARAMETER
in O
the O
user O
prediction O
task O
and O
t O
is O
defined O
as O
the O
F1 B-METRIC
score I-METRIC
on O
the O
mood O
prediction O
task O
. O
section 19
id pdf2json/2021.acl-long.322.pdf.json
In O
the O
rare O
cases O
where O
NI-MLP O
performed O
better O
than O
the O
original O
MLP B-METHOD
and O
caused O
R O
to O
become O
negative O
, O
we O
found O
this O
improvement O
in O
performance O
always O
came O
at O
the O
expense O
of O
worse O
privacy O
as O
compared O
to O
other O
settings O
of O
λ O
and O
σ O
in O
NI-MLP O
. O
section 19
id pdf2json/2021.acl-long.322.pdf.json
Therefore O
, O
models O
with O
negative O
R O
were O
not O
considered O
for O
Table O
1 O
. O
section 19
id pdf2json/2021.acl-long.322.pdf.json
D.2 O
Details O
on O
Preserving O
Privacy O
For O
Table O
5 O
, O
the O
model O
with O
the O
best O
privacy O
out O
of O
those O
within O
5 O
% O
performance O
of O
the O
original O
MLP B-METHOD
model O
( O
or O
, O
if O
no O
such O
model O
existed O
, O
the O
model O
with O
the O
best O
performance O
) O
was O
selected O
. O
section 19
id pdf2json/2021.acl-long.322.pdf.json
Interestingly O
, O
in O
Figure O
4 O
, O
we O
find O
that O
the O
tradeoff O
curve O
on O
a B-PARAMETER
model O
trained O
only O
using O
app O
features O
does O
not O
exhibit O
a B-PARAMETER
Pareto O
tradeoff O
curve O
as O
ex- O
pected O
. O
section 19
id pdf2json/2021.acl-long.322.pdf.json
We O
attribute O
this O
to O
randomness O
in O
predicting O
both O
mood O
and O
identities O
. O
section 19
id pdf2json/2021.acl-long.322.pdf.json
Furthermore O
, O
Wang B-AUTHOR
et O
al O
. O
section 19
id pdf2json/2021.acl-long.322.pdf.json
( O
2017 O
) O
found O
that O
adding O
noise O
to O
the O
identity O
subspace O
can O
sometimes O
improve O
generalization O
by O
reducing O
reliance O
on O
identity-dependent O
confounding O
features O
, O
which O
could O
also O
explain O
occasional O
increased O
performance O
at O
larger O
σ O
values O
. O
section 19
id pdf2json/2021.acl-long.322.pdf.json
Note O
that O
we O
do O
not O
include O
privacy O
results O
for O
features O
learned O
by O
SVM O
, O
which O
finds O
a B-PARAMETER
linear O
separator O
in O
a B-PARAMETER
specified O
kernel O
space O
rather O
than O
learning O
a B-PARAMETER
representation O
for O
each O
sample O
. O
section 19
id pdf2json/2021.acl-long.322.pdf.json
Explicitly O
projecting O
our O
features O
is O
computationally O
infeasible O
due O
to O
the O
high O
dimensionality B-PARAMETER
of O
our O
chosen O
kernel O
spaces O
. O
section 19
id pdf2json/2021.acl-long.322.pdf.json
D.3 O
Qualitative O
Analysis O
In O
this O
section O
, O
we O
provide O
more O
empirical O
analysis O
on O
the O
unimodal O
and O
multimodal O
features O
in O
the O
MAPS O
dataset O
. O
section 19
id pdf2json/2021.acl-long.322.pdf.json
D.3.1 O
Understanding O
the O
unimodal O
features O
Text O
: O
We O
begin O
with O
some O
basic O
statistics O
regarding O
word B-METHOD
distributions O
. O
section 19
id pdf2json/2021.acl-long.322.pdf.json
For O
each O
user O
, O
we O
tallied O
the O
frequencies O
of O
each O
word B-METHOD
under O
each O
daily O
mood O
category O
( O
positive O
, O
neutral O
, O
and O
negative O
) O
, O
as O
well O
as O
the O
overall O
number O
of O
words O
in O
each O
mood O
category O
. O
section 19
id pdf2json/2021.acl-long.322.pdf.json
We O
define O
“ O
positive O
” O
words O
and O
emojis O
to O
be O
those O
with O
a B-PARAMETER
higher O
relative O
frequency O
of O
positive O
mood O
compared O
to O
the O
overall O
positive O
mood O
frequency O
, O
and O
lower O
than O
overall O
negative O
mood O
frequency O
. O
section 19
id pdf2json/2021.acl-long.322.pdf.json
Likewise O
, O
“ O
negative O
” O
words O
and O
emojis O
have O
higher O
than O
overall O
negative O
mood O
frequency O
and O
lower O
than O
overall O
positive O
mood O
frequency O
. O
section 19
id pdf2json/2021.acl-long.322.pdf.json
We O
filtered O
out O
words O
for O
specific O
users O
if O
the O
word B-METHOD
was O
used O
less O
than O
40 O
times O
. O
section 19
id pdf2json/2021.acl-long.322.pdf.json
Finally O
, O
we O
ranked O
the O
words O
by O
the O
difference O
in O
relative O
frequency O
( O
i.e. O
, O
a B-PARAMETER
word B-METHOD
is O
“ O
more O
positive O
” O
the O
larger O
the O
difference O
between O
its O
positive O
mood O
relative O
frequency O
and O
the O
user O
’ O
s O
overall O
positive O
mood O
relative O
frequency O
) O
. O
section 19
id pdf2json/2021.acl-long.322.pdf.json
See O
Table O
12 O
for O
examples O
of O
top O
positive O
and O
negative O
words O
. O
section 19
id pdf2json/2021.acl-long.322.pdf.json
For O
each O
word B-METHOD
, O
we O
also O
counted O
the O
number O
of O
users O
for O
which O
the O
word B-METHOD
was O
positive O
or O
negative O
. O
section 19
id pdf2json/2021.acl-long.322.pdf.json
See O
Table O
13 O
for O
the O
words O
with O
the O
highest O
user O
counts O
. O
section 19
id pdf2json/2021.acl-long.322.pdf.json
Keystrokes O
: O
We O
show O
some O
sample O
bag-of-timing O
histograms O
in O
Figure O
6 O
. O
section 19
id pdf2json/2021.acl-long.322.pdf.json
It O
is O
interesting O
to O
find O
that O
certain O
users O
show O
a B-PARAMETER
bimodal O
distribution O
across O
their O
keystroke O
histograms O
with O
one O
peak O
representing O
faster O
typing O
and O
another O
representing O
slower O
typing O
. O
section 19
id pdf2json/2021.acl-long.322.pdf.json
Visually O
, O
the O
overall O
keystroke O
histograms O
did O
not O
differ O
that O
much O
across O
users O
which O
might O
explain O
its O
lower O
accuracies O
in O
both O
mood O
and O
user O
prediction O
when O
trained O
with O
NI-MLP O
( O
see O
Figure O
4 O
) O
. O
section 19
id pdf2json/2021.acl-long.322.pdf.json
App O
usage O
: O
Similar O
to O
“ O
positive O
” O
words O
, O
we O
define O
“ O
positive O
” O
apps O
to O
be O
those O
with O
higher O
than O
overall O
positive O
mood O
relative O
frequency O
and O
lower O
than O
overall O
negative O
mood O
relative O
frequency O
, O
and O
“ O
negative O
” O
apps O
to O
be O
the O
opposite O
. O
section 19
id pdf2json/2021.acl-long.322.pdf.json
Apps O
were O
also O
then O
sorted O
by O
difference O
in O
relative O
frequency O
. O
section 19
id pdf2json/2021.acl-long.322.pdf.json
D.3.2 O
Understanding O
the O
multimodal O
features O
Characters O
with O
keystrokes O
: O
For O
each O
user O
, O
we O
plotted O
histograms O
of O
keystroke O
timings O
of O
alphanumeric O
characters O
, O
symbols O
( O
punctuation O
and O
emojis O
) O
, O
spacebar O
, O
enter O
, O
delete O
, O
and O
use O
of O
autocorrect O
, O
split O
across O
daily O
mood O
categories O
. O
section 19
id pdf2json/2021.acl-long.322.pdf.json
See O
Figure O
7 O
for O
examples O
across O
one O
user O
. O
section 19
id pdf2json/2021.acl-long.322.pdf.json
We O
find O
particularly O
interesting O
patterns O
in O
the O
autocorrect O
keys O
and O
symbols O
where O
keystrokes O
are O
quite O
indicative O
of O
mood O
, O
which O
attests O
to O
the O
unique O
nature O
of O
typed O
text O
. O
section 19
id pdf2json/2021.acl-long.322.pdf.json
Words O
with O
keystrokes O
: O
For O
each O
user O
, O
we O
plotted O
histograms O
of O
the O
word-level O
keystroke O
timings O
of O
the O
top O
500 O
words O
, O
split O
across O
the O
daily O
mood O
categories O
of O
positive O
, O
neutral O
, O
and O
negative O
. O
section 19
id pdf2json/2021.acl-long.322.pdf.json
We O
also O
performed O
Wilcoxon O
rank-sum O
tests O
at O
5 O
% O
signifi- O
cance O
level O
( O
Wilcoxon O
, O
1992 O
) O
between O
the O
timings O
of O
positive O
and O
negative O
mood O
for O
each O
user/word O
combination O
to O
determine O
which O
words O
had O
significantly O
different O
timings O
between O
positive O
and O
negative O
mood O
. O

section 20
id pdf2json/2021.acl-long.322.pdf.json
Since O
this O
is O
a B-PARAMETER
new O
dataset O
, O
we O
explored O
several O
more O
methods O
throughout O
the O
research O
process O
. O
section 20
id pdf2json/2021.acl-long.322.pdf.json
In O
this O
section O
we O
describe O
some O
of O
the O
approaches O
that O
yielded O
initial O
negative O
results O
despite O
them O
working O
well O
for O
standard O
datasets O
: O
1 O
. O
section 20
id pdf2json/2021.acl-long.322.pdf.json
User O
specific O
models O
: O
We O
also O
explored O
the O
setting O
of O
training O
a B-PARAMETER
separate O
model O
per O
user O
but O
we O
found O
that O
there O
was O
too O
little O
data O
per O
user O
to O
train O
a B-PARAMETER
good O
model O
. O
section 20
id pdf2json/2021.acl-long.322.pdf.json
As O
part O
of O
future O
work O
, O
we O
believe O
that O
if O
NI-MLP O
can O
learn O
a B-PARAMETER
user-independent O
classifier O
, O
these O
representations O
can O
then O
be O
used O
for O
further O
finetuning O
or O
few-shot O
learning O
on O
each O
specific O
user O
. O
section 20
id pdf2json/2021.acl-long.322.pdf.json
Previous O
work O
in O
federated O
learning O
( O
Smith B-AUTHOR
et O
al. O
, O
2017 O
; O
Liang B-AUTHOR
et O
al. O
, O
2020b O
) O
offers O
ways O
of O
learning O
a B-PARAMETER
user-specific O
model O
that O
leverages O
other O
users O
’ O
data O
during O
training O
, O
which O
could O
help O
to O
alleviate O
the O
lack O
of O
data O
per O
user O
. O
section 20
id pdf2json/2021.acl-long.322.pdf.json
2 O
. O
section 20
id pdf2json/2021.acl-long.322.pdf.json
User-independent O
data O
splits O
: O
We O
have O
shown O
that O
text O
, O
keystrokes O
, O
and O
app O
usage O
features O
are O
highly O
dependent O
on O
participant O
identities O
. O
section 20
id pdf2json/2021.acl-long.322.pdf.json
Consequently O
, O
models O
trained O
on O
these O
features O
would O
perform O
poorly O
when O
evaluated O
on O
a B-PARAMETER
user O
not O
found O
in O
the O
training O
set O
. O
section 20
id pdf2json/2021.acl-long.322.pdf.json
We O
would O
like O
to O
evaluate O
if O
better O
learning O
of O
user-independent O
features O
can O
improve O
generalization O
to O
new O
users O
( O
e.g. O
, O
split O
the O
data O
such O
that O
the O
first O
10 O
users O
are O
used O
for O
training O
, O
next O
3 O
for O
validation O
, O
and O
final O
4 O
for O
testing O
) O
. O
section 20
id pdf2json/2021.acl-long.322.pdf.json
Our O
initial O
results O
for O
these O
were O
negative O
, O
but O
we O
believe O
that O
combining O
better O
privacy-preserving O
methods O
that O
learn O
user-independent O
features O
could O
help O
in O
this O
regard O
. O
section 20
id pdf2json/2021.acl-long.322.pdf.json
3 O
. O
section 20
id pdf2json/2021.acl-long.322.pdf.json
Fine-grained O
multimodal O
fusion O
: O
Our O
approach O
of O
combining O
modalities O
was O
only O
at O
the O
input O
level O
( O
i.e. O
, O
early O
fusion O
( O
Baltrušaitis O
et O
al. O
, O
2018 O
) O
) O
which O
can O
be O
improved O
upon O
by O
leveraging O
recent O
work O
in O
more O
fine-grained O
fusion O
( O
Liang B-AUTHOR
et O
al. O
, O
2018 O
) O
. O
section 20
id pdf2json/2021.acl-long.322.pdf.json
One O
such O
example O
could O
be O
to O
align O
each O
keystroke O
feature O
and O
app O
data O
to O
the O
exact O
text O
that O
was O
entered O
in O
, O
which O
provides O
more O
finegrained O
contextualization O
of O
text O
in O
keystroke O
and O
app O
usage O
context O
. O

section TITLE
id pdf2json/2021.acl-long.266.pdf.json
Rejuvenating O
Low-Frequency O
Words O
: O
Making O
the O
Most O
of O
Parallel O
Data O
in O
Non-Autoregressive O
Translation O

section ABSTRACT
id pdf2json/2021.acl-long.266.pdf.json
Knowledge O
distillation O
( O
KD O
) O
is O
commonly O
used O
to O
construct O
synthetic O
data O
for O
training O
non-autoregressive O
translation O
( O
NAT O
) O
models O
. O
section ABSTRACT
id pdf2json/2021.acl-long.266.pdf.json
However O
, O
there O
exists O
a B-PARAMETER
discrepancy O
on O
lowfrequency O
words O
between O
the O
distilled O
and O
the O
original O
data O
, O
leading O
to O
more O
errors O
on O
predicting O
low-frequency O
words O
. O
section ABSTRACT
id pdf2json/2021.acl-long.266.pdf.json
To O
alleviate O
the O
problem O
, O
we O
directly O
expose O
the O
raw O
data O
into O
NAT O
by O
leveraging O
pretraining O
. O
section ABSTRACT
id pdf2json/2021.acl-long.266.pdf.json
By O
analyzing O
directed O
alignments O
, O
we O
found O
that O
KD O
makes O
low-frequency O
source O
words O
aligned O
with O
targets O
more O
deterministically O
but O
fails O
to O
align O
sufficient O
low-frequency O
words O
from O
target B-PARAMETER
to O
source O
. O
section ABSTRACT
id pdf2json/2021.acl-long.266.pdf.json
Accordingly O
, O
we O
propose O
reverse O
KD O
to O
rejuvenate O
more O
alignments O
for O
lowfrequency O
target B-PARAMETER
words O
. O
section ABSTRACT
id pdf2json/2021.acl-long.266.pdf.json
To O
make O
the O
most O
of O
authentic O
and O
synthetic O
data O
, O
we O
combine O
these O
complementary O
approaches O
as O
a B-PARAMETER
new O
training O
strategy O
for O
further O
boosting B-METHOD
NAT O
performance O
. O
section ABSTRACT
id pdf2json/2021.acl-long.266.pdf.json
We O
conduct O
experiments O
on O
five O
translation O
benchmarks O
over O
two O
advanced O
architectures O
. O
section ABSTRACT
id pdf2json/2021.acl-long.266.pdf.json
Results O
demonstrate O
that O
the O
proposed O
approach O
can O
significantly O
and O
universally O
improve O
translation O
quality O
by O
reducing O
translation O
errors O
on O
low-frequency O
words O
. O
section ABSTRACT
id pdf2json/2021.acl-long.266.pdf.json
Encouragingly O
, O
our O
approach O
achieves O
28.2 O
and O
33.9 O
BLEU B-METRIC
points O
on O
the O
WMT14 O
English-German O
and O
WMT16 O
Romanian-English O
datasets O
, O
respectively O
. O
section ABSTRACT
id pdf2json/2021.acl-long.266.pdf.json
Our O
code O
, O
data O
, O
and O
trained O
models O
are O
available O
at O
https O
: O
//github.com/ O
longyuewangdcu/RLFW-NAT O
. O

section 0
id pdf2json/2021.acl-long.266.pdf.json
Proceedings O
of O
the O
59th O
Annual O
Meeting O
of O
the O
Association O
for O
Computational O
Linguistics O
and O
the O
11th O
International O
Joint O
Conference O
on O
Natural O
Language O
Processing O
, O
pages O
3431–3441 O
August O
1–6 O
, O
2021 O
. O
section 0
id pdf2json/2021.acl-long.266.pdf.json
©2021 O
Association O
for O
Computational O
Linguistics O
3431 O

section 1
id pdf2json/2021.acl-long.266.pdf.json
Recent O
years O
have O
seen O
a B-PARAMETER
surge O
of O
interest O
in O
nonautoregressive O
translation O
( O
NAT O
, O
Gu O
et O
al. O
, O
2018 O
) O
, O
which O
can O
improve O
the O
decoding O
efficiency B-METRIC
by O
predicting O
all O
tokens O
independently O
and O
simultaneously O
. O
section 1
id pdf2json/2021.acl-long.266.pdf.json
The O
non-autoregressive O
factorization O
breaks O
conditional O
dependencies O
among O
output O
tokens O
, O
∗ O
Liang B-AUTHOR
Ding B-AUTHOR
and O
Longyue O
Wang B-AUTHOR
contributed O
equally O
to O
this O
work O
. O
section 1
id pdf2json/2021.acl-long.266.pdf.json
Work O
was O
done O
when O
Liang B-AUTHOR
Ding B-AUTHOR
and O
Xuebo O
Liu B-AUTHOR
were O
interning O
at O
Tencent O
AI O
Lab O
. O
section 1
id pdf2json/2021.acl-long.266.pdf.json
which O
prevents O
a B-PARAMETER
model O
from O
properly O
capturing O
the O
highly O
multimodal O
distribution O
of O
target B-PARAMETER
translations O
. O
section 1
id pdf2json/2021.acl-long.266.pdf.json
As O
a B-PARAMETER
result O
, O
the O
translation O
quality O
of O
NAT O
models O
often O
lags O
behind O
that O
of O
autoregressive O
translation O
( O
AT O
, O
Vaswani B-AUTHOR
et O
al. O
, O
2017 O
) O
models O
. O
section 1
id pdf2json/2021.acl-long.266.pdf.json
To O
balance O
the O
trade-off O
between O
decoding O
speed O
and O
translation O
quality O
, O
knowledge O
distillation O
( O
KD O
) O
is O
widely O
used O
to O
construct O
a B-PARAMETER
new O
training O
data O
for O
NAT O
models O
( O
Gu O
et O
al. O
, O
2018 O
) O
. O
section 1
id pdf2json/2021.acl-long.266.pdf.json
Specifically O
, O
target B-PARAMETER
sentences O
in O
the O
distilled O
training O
data O
are O
generated O
by O
an O
AT O
teacher O
, O
which O
makes O
NAT O
easily O
acquire O
more O
deterministic O
knowledge O
and O
achieve O
significant O
improvement O
( O
Zhou B-AUTHOR
et O
al. O
, O
2020 O
) O
. O
section 1
id pdf2json/2021.acl-long.266.pdf.json
Previous O
studies O
have O
shown O
that O
distillation O
may O
lose O
some O
important O
information O
in O
the O
original O
training O
data O
, O
leading O
to O
more O
errors O
on O
predicting O
low-frequency O
words O
. O
section 1
id pdf2json/2021.acl-long.266.pdf.json
To O
alleviate O
this O
problem O
, O
Ding B-AUTHOR
et O
al O
. O
section 1
id pdf2json/2021.acl-long.266.pdf.json
( O
2021b O
) O
proposed O
to O
augment O
NAT O
models O
the O
ability O
to O
learn O
lost O
knowledge O
from O
the O
original O
data O
. O
section 1
id pdf2json/2021.acl-long.266.pdf.json
However O
, O
their O
approach O
relies O
on O
external O
resources O
( O
e.g O
. O
section 1
id pdf2json/2021.acl-long.266.pdf.json
word B-METHOD
alignment O
) O
and O
human-crafted O
priors O
, O
which O
limits O
the O
applicability O
of O
the O
method O
to O
a B-PARAMETER
broader O
range O
of O
tasks O
and O
languages O
. O
section 1
id pdf2json/2021.acl-long.266.pdf.json
Accordingly O
, O
we O
turn O
to O
directly O
expose O
the O
raw O
data O
into O
NAT O
by O
leveraging O
pretraining O
without O
intensive O
modification O
to O
model O
architectures O
( O
§2.2 O
) O
. O
section 1
id pdf2json/2021.acl-long.266.pdf.json
Furthermore O
, O
we O
analyze O
bilingual O
links O
in O
the O
distilled O
data O
from O
two O
alignment O
directions O
( O
i.e O
. O
section 1
id pdf2json/2021.acl-long.266.pdf.json
source-to-target O
and O
target-to-source O
) O
. O
section 1
id pdf2json/2021.acl-long.266.pdf.json
We O
found O
that O
KD O
makes O
low-frequency O
source O
words O
aligned O
with O
targets O
more O
deterministically O
but O
fails O
to O
align O
low-frequency O
words O
from O
target B-PARAMETER
to O
source O
due O
to O
information O
loss O
. O
section 1
id pdf2json/2021.acl-long.266.pdf.json
Inspired O
by O
this O
finding O
, O
we O
propose O
reverse O
KD O
to O
recall B-METRIC
more O
alignments O
for O
low-frequency O
target B-PARAMETER
words O
( O
§2.3 O
) O
. O
section 1
id pdf2json/2021.acl-long.266.pdf.json
We O
then O
concatenate O
two O
kinds O
of O
distilled O
data O
to O
maintain O
advantages O
of O
deterministic O
knowledge O
and O
low-frequency O
information O
. O
section 1
id pdf2json/2021.acl-long.266.pdf.json
To O
make O
the O
most O
of O
authentic O
and O
synthetic O
data O
, O
we O
combine O
three O
complementary O
approaches O
( O
i.e O
. O
section 1
id pdf2json/2021.acl-long.266.pdf.json
raw O
pretraining O
, O
bidirectional O
distillation O
training O
and O
KD O
finetuning O
) O
as O
a B-PARAMETER
new O
training O
strategy O
for O
further O
boosting B-METHOD
NAT O
performance O
( O
§2.4 O
) O
. O
section 1
id pdf2json/2021.acl-long.266.pdf.json
We O
validated O
our O
approach O
on O
five O
translation O
benchmarks O
( O
WMT14 O
En-De O
, O
WMT16 O
Ro-En O
, O
WMT17 O
Zh-En O
, O
WAT17 O
Ja-En O
and O
WMT19 O
EnDe O
) O
over O
two O
advanced O
architectures O
( O
Mask O
Predict O
, O
Ghazvininejad O
et O
al. O
, O
2019 O
; O
Levenshtein O
Transformer B-METHOD
, O
Gu O
et O
al. O
, O
2019 O
) O
. O
section 1
id pdf2json/2021.acl-long.266.pdf.json
Experimental O
results O
show O
that O
the O
proposed O
method O
consistently O
improve O
translation O
performance O
over O
the O
standard O
NAT O
models O
across O
languages O
and O
advanced O
NAT O
architectures O
. O
section 1
id pdf2json/2021.acl-long.266.pdf.json
Extensive O
analyses O
confirm O
that O
the O
performance O
improvement O
indeed O
comes O
from O
the O
better O
lexical O
translation O
accuracy B-PARAMETER
especially O
on O
low-frequency O
tokens O
. O
section 1
id pdf2json/2021.acl-long.266.pdf.json
Contributions O
Our O
main O
contributions O
are O
: O
• O
We O
show O
the O
effectiveness O
of O
rejuvenating O
lowfrequency O
information O
by O
pretraining O
NAT O
models O
from O
raw O
data O
. O
section 1
id pdf2json/2021.acl-long.266.pdf.json
• O
We O
provide O
a B-PARAMETER
quantitative O
analysis O
of O
bilingual O
links O
to O
demonstrate O
the O
necessity O
to O
improve O
low-frequency O
alignment O
by O
leveraging O
both O
KD O
and O
reverse O
KD O
. O
section 1
id pdf2json/2021.acl-long.266.pdf.json
• O
We O
introduce O
a B-PARAMETER
simple O
and O
effective O
training O
recipe O
to O
accomplish O
this O
goal O
, O
which O
is O
robustly O
applicable O
to O
several O
model O
structures O
and O
language O
pairs O
. O


section 3
id pdf2json/2021.acl-long.266.pdf.json
Non-Autoregressive O
Translation O
Given O
a B-PARAMETER
source O
sentence O
x O
, O
an O
AT O
model O
generates O
each O
target B-PARAMETER
word B-METHOD
yt O
conditioned O
on O
previously O
generated O
ones O
y O
< O
t O
, O
leading O
to O
high O
latency O
on O
the O
decoding O
stage O
. O
section 3
id pdf2json/2021.acl-long.266.pdf.json
In O
contrast O
, O
NAT O
models O
break O
this O
autoregressive O
factorization O
by O
producing O
target B-PARAMETER
words O
in O
parallel O
. O
section 3
id pdf2json/2021.acl-long.266.pdf.json
Accordingly O
, O
the O
probability O
of O
generating O
y O
is O
computed O
as O
: O
p O
( O
y|x O
) O
= O
T∏ O
t=1 O
p O
( O
yt|x O
; O
θ O
) O
( O
1 O
) O
where O
T O
is O
the O
length B-METRIC
of O
the O
target B-PARAMETER
sequence O
, O
and O
it O
is O
usually O
predicted O
by O
a B-PARAMETER
separate O
conditional O
distribution O
. O
section 3
id pdf2json/2021.acl-long.266.pdf.json
The O
parameters O
θ O
are O
trained O
to O
maximize O
the O
likelihood O
of O
a B-PARAMETER
set O
of O
training O
examples O
according O
to O
L O
( O
θ O
) O
= O
arg O
maxθ O
log O
p O
( O
y|x O
; O
θ O
) O
. O
section 3
id pdf2json/2021.acl-long.266.pdf.json
Typically O
, O
most O
NAT O
models O
are O
implemented O
upon O
the O
framework O
of O
Transformer B-METHOD
( O
Vaswani B-AUTHOR
et O
al. O
, O
2017 O
) O
. O
section 3
id pdf2json/2021.acl-long.266.pdf.json
Knowledge O
Distillation O
Gu B-AUTHOR
et I-AUTHOR
al I-AUTHOR
. O
section 3
id pdf2json/2021.acl-long.266.pdf.json
( O
2018 O
) O
pointed O
out O
that O
NAT O
models O
suffer O
from O
the O
multimodality O
problem O
, O
where O
the O
conditional O
independence O
assumption O
prevents O
a B-PARAMETER
model O
from O
properly O
capturing O
the O
highly O
multimodal O
distribution O
of O
target B-PARAMETER
translations O
. O
section 3
id pdf2json/2021.acl-long.266.pdf.json
Thus O
, O
the O
sequence-level O
knowledge O
distillation O
is O
introduced O
to O
reduce O
the O
modes O
of O
training O
data O
by O
replacing O
their O
original O
target-side O
samples O
with O
sentences O
generated O
by O
an O
AT O
teacher O
( O
Gu O
et O
al. O
, O
2018 O
; O
Zhou B-AUTHOR
et O
al. O
, O
2020 O
; O
Ren B-AUTHOR
et O
al. O
, O
2020 O
) O
. O
section 3
id pdf2json/2021.acl-long.266.pdf.json
Formally O
, O
the O
original O
parallel O
data O
Raw O
and O
the O
distilled O
data O
−→ O
KD O
can O
be O
defined O
as O
follows O
: O
Raw O
= O
{ O
( O
xi O
, O
yi O
) O
} O
Ni=1 O
( O
2 O
) O
−→ O
KD O
= O
{ O
( O
xi O
, O
fs O
7→t O
( O
xi O
) O
) O
|xi O
∈ O
Raws O
} O
Ni=1 O
( O
3 O
) O
where O
fs O
7→t O
represents O
an O
AT-based O
translation O
model O
trained O
on O
Raw O
data O
for O
translating O
text O
from O
the O
source O
to O
the O
target B-PARAMETER
language O
. O
section 3
id pdf2json/2021.acl-long.266.pdf.json
N O
is O
the O
total O
number O
of O
sentence O
pairs O
in O
training O
data O
. O
section 3
id pdf2json/2021.acl-long.266.pdf.json
As O
shown O
in O
Figure O
1 O
( O
a B-PARAMETER
) O
, O
well-performed O
NAT O
models O
are O
generally O
trained O
on O
−→ O
KD O
data O
instead O
of O
Raw O
. O

section 4
id pdf2json/2021.acl-long.266.pdf.json
Motivation O
Gao B-AUTHOR
et I-METHOD
al I-METHOD
. O
section 4
id pdf2json/2021.acl-long.266.pdf.json
( O
2018 O
) O
showed O
that O
more O
than O
90 O
% O
of O
words O
are O
lower O
than O
10e-4 O
frequency O
in O
WMT14 O
En-De O
dataset O
. O
section 4
id pdf2json/2021.acl-long.266.pdf.json
This O
token O
imbalance O
problem O
biases O
translation O
models O
towards O
overfitting O
to O
frequent O
observations O
while O
neglecting O
those O
low-frequency O
observations O
( O
Gong B-AUTHOR
et O
al. O
, O
2018 O
; O
Nguyen B-AUTHOR
and O
Chiang O
, O
2018 O
; O
Gu O
et O
al. O
, O
2020 O
) O
. O
section 4
id pdf2json/2021.acl-long.266.pdf.json
Thus O
, O
the O
AT O
teacher O
fs7→t O
tends O
to O
generate O
more O
high-frequency O
tokens O
and O
less O
low-frequency O
tokens O
during O
constructing O
distilled O
data O
−→ O
KD O
. O
section 4
id pdf2json/2021.acl-long.266.pdf.json
On O
the O
one O
hand O
, O
KD O
can O
reduce O
the O
modes O
in O
training O
data O
( O
i.e O
. O
section 4
id pdf2json/2021.acl-long.266.pdf.json
multiple O
lexical O
choices O
for O
a B-PARAMETER
source O
word B-METHOD
) O
, O
which O
lowers O
the O
intrinsic O
uncertainty O
( O
Ott O
et O
al. O
, O
2018 O
) O
and O
learning O
difficulty O
for O
NAT O
( O
Zhou B-AUTHOR
et O
al. O
, O
2020 O
; O
Ren B-AUTHOR
et O
al. O
, O
2020 O
) O
, O
making O
it O
easily O
acquire O
more O
deterministic O
knowledge O
. O
section 4
id pdf2json/2021.acl-long.266.pdf.json
On O
the O
other O
hand O
, O
KD O
aggravates O
the O
imbalance O
of O
high-frequency O
and O
low-frequency O
words O
in O
training O
data O
and O
lost O
some O
important O
information O
originated O
in O
raw O
data O
. O
section 4
id pdf2json/2021.acl-long.266.pdf.json
Ding B-AUTHOR
et O
al O
. O
section 4
id pdf2json/2021.acl-long.266.pdf.json
( O
2021b O
) O
revealed O
the O
side O
effect O
of O
distilled O
training O
data O
, O
which O
cause O
lexical O
choice O
errors O
for O
low-frequency O
words O
in O
NAT O
models O
. O
section 4
id pdf2json/2021.acl-long.266.pdf.json
Accordingly O
, O
they O
introduced O
an O
extra O
bilingual O
data-dependent O
prior O
objective O
to O
augments O
NAT O
models O
the O
ability O
to O
learn O
the O
lost O
knowledge O
from O
raw O
data O
. O
section 4
id pdf2json/2021.acl-long.266.pdf.json
We O
use O
their O
findings O
as O
our O
departure O
point O
, O
but O
rejuvenate O
low-frequency O
words O
in O
a B-PARAMETER
more O
simple O
and O
direct O
way O
: O
directly O
exposing O
raw O
data O
into O
NAT O
via O
pretraining O
. O
section 4
id pdf2json/2021.acl-long.266.pdf.json
Our O
Approach O
Many O
studies O
have O
shown O
that O
pretraining O
could O
transfer O
the O
knowledge O
and O
data O
distribution O
, O
especially O
for O
rare O
categories O
, O
hence O
improving O
the O
model O
robustness O
( O
Hendrycks O
et O
al. O
, O
2019 O
; O
Mathis O
et O
al. O
, O
2021 O
) O
. O
section 4
id pdf2json/2021.acl-long.266.pdf.json
Here O
we O
want O
to O
transfer O
the O
distribution O
of O
lost O
information O
, O
e.g O
. O
section 4
id pdf2json/2021.acl-long.266.pdf.json
lowfrequency O
words O
. O
section 4
id pdf2json/2021.acl-long.266.pdf.json
As O
illustrated O
in O
Figure O
1 O
( O
b O
) O
, O
we O
propose O
to O
first O
pretrain O
NAT O
models O
on O
Raw O
data O
and O
then O
continuously O
train O
them O
on O
−→ O
KD O
data O
. O
section 4
id pdf2json/2021.acl-long.266.pdf.json
The O
raw O
data O
maintain O
the O
original O
distribution O
especially O
on O
low-frequency O
words O
. O
section 4
id pdf2json/2021.acl-long.266.pdf.json
Although O
it O
is O
difficult O
for O
NAT O
to O
learn O
high-mode O
data O
, O
the O
pretraining O
can O
acquire O
general O
knowledge O
from O
authentic O
data O
, O
which O
may O
help O
better O
and O
faster O
learning O
further O
tasks O
. O
section 4
id pdf2json/2021.acl-long.266.pdf.json
Thus O
, O
we O
early O
stop O
pretraining O
when O
the O
model O
can O
achieve O
90 O
% O
of O
the O
best O
performance O
of O
raw O
data O
in O
terms O
of O
BLEU B-METRIC
score O
( O
Platanios O
et O
al. O
, O
2019 O
) O
1 O
. O
section 4
id pdf2json/2021.acl-long.266.pdf.json
In O
order O
to O
keep O
the O
merits O
of O
low-modes O
, O
1In O
preliminary O
experiments O
, O
we O
tried O
another O
simple O
strategy O
: O
early-stop O
at O
fixed O
step O
according O
to O
the O
size O
of O
training O
data O
( O
e.g O
. O
section 4
id pdf2json/2021.acl-long.266.pdf.json
training O
70K O
En-De O
and O
early O
stop O
at O
20K O
/ O
30K O
/ O
40K O
, O
respectively O
) O
. O
section 4
id pdf2json/2021.acl-long.266.pdf.json
We O
found O
that O
both O
strategies O
achieve O
KD O
” O
and O
“ O
←− O
KD O
” O
indicate O
syntactic O
data O
distilled O
by O
KD O
and O
reverse O
KD O
, O
respectively O
. O
section 4
id pdf2json/2021.acl-long.266.pdf.json
The O
subscript O
“ O
S O
” O
or O
“ O
T O
” O
is O
short O
for O
source- O
or O
target-side O
. O
section 4
id pdf2json/2021.acl-long.266.pdf.json
The O
low-frequency O
words O
are O
highlighted O
with O
colors O
and O
italics O
are O
incorrect O
translations O
. O
section 4
id pdf2json/2021.acl-long.266.pdf.json
we O
further O
train O
the O
pretrained O
model O
on O
distilled O
data O
−→ O
KD O
. O
section 4
id pdf2json/2021.acl-long.266.pdf.json
As O
it O
is O
easy O
for O
NAT O
to O
learn O
deterministic O
knowledge O
, O
we O
finetune O
the O
model O
for O
the O
rest O
steps O
. O
section 4
id pdf2json/2021.acl-long.266.pdf.json
For O
fair O
comparison O
, O
the O
total O
training O
steps O
of O
the O
proposed O
method O
are O
same O
as O
the O
traditional O
one O
. O
section 4
id pdf2json/2021.acl-long.266.pdf.json
In O
general O
, O
we O
expect O
that O
this O
training O
recipe O
can O
provide O
a B-PARAMETER
good O
trade-off O
between O
raw O
and O
distilled O
data O
( O
i.e O
. O
section 4
id pdf2json/2021.acl-long.266.pdf.json
high-modes O
and O
complete O
vs. O
low-modes O
and O
incomplete O
) O
. O

section 5
id pdf2json/2021.acl-long.266.pdf.json
Analyzing O
Bilingual O
Links O
in O
Data O
KD O
simplifies O
the O
training O
data O
by O
replacing O
low-frequency O
target B-PARAMETER
words O
with O
high-frequency O
ones O
( O
Zhou B-AUTHOR
et O
al. O
, O
2020 O
) O
. O
section 5
id pdf2json/2021.acl-long.266.pdf.json
This O
is O
able O
to O
facilitate O
easier O
aligning O
source O
words O
to O
target B-PARAMETER
ones O
, O
resulting O
in O
high O
bilingual O
coverage O
( O
Jiao O
et O
al. O
, O
2020 O
) O
. O
section 5
id pdf2json/2021.acl-long.266.pdf.json
Due O
to O
the O
information O
loss O
, O
we O
argue O
that O
KD O
makes O
lowfrequency O
target B-PARAMETER
words O
have O
fewer O
opportunities O
to O
align O
with O
source O
ones O
. O
section 5
id pdf2json/2021.acl-long.266.pdf.json
To O
verify O
this O
, O
we O
propose O
a B-PARAMETER
method O
to O
quantitatively O
analyze O
bilingual O
links O
from O
two O
directions O
, O
where O
low-frequency O
words O
similar O
performance O
. O
section 5
id pdf2json/2021.acl-long.266.pdf.json
are O
aligned O
from O
source O
to O
target B-PARAMETER
( O
s O
7→ O
t O
) O
or O
in O
an O
opposite O
direction O
( O
t O
7→ O
s O
) O
. O
section 5
id pdf2json/2021.acl-long.266.pdf.json
The O
method O
can O
be O
applied O
to O
different O
types O
of O
data O
. O
section 5
id pdf2json/2021.acl-long.266.pdf.json
Here O
we O
take O
s O
7→ O
t O
links O
in O
Raw O
data O
as O
an O
example O
to O
illustrate O
the O
algorithm O
. O
section 5
id pdf2json/2021.acl-long.266.pdf.json
Given O
the O
WMT14 O
En-De O
parallel O
corpus O
, O
we O
employ O
an O
unsupervised O
word B-METHOD
alignment O
method2 O
( O
Och B-AUTHOR
and O
Ney O
, O
2003 O
) O
to O
produce O
a B-PARAMETER
word B-METHOD
alignment O
, O
and O
then O
we O
extract O
aligned O
links O
whose O
source O
words O
are O
low-frequency O
( O
called O
s O
7→ O
t O
LFW O
Links O
) O
. O
section 5
id pdf2json/2021.acl-long.266.pdf.json
Second O
, O
we O
randomly O
select O
a B-PARAMETER
number O
of O
samples O
from O
the O
parallel O
corpus O
. O
section 5
id pdf2json/2021.acl-long.266.pdf.json
For O
better O
comparison O
, O
the O
subset O
should O
contains O
the O
same O
i O
in O
Equation O
( O
2 O
) O
as O
that O
of O
other O
type O
of O
datasets O
( O
e.g O
. O
section 5
id pdf2json/2021.acl-long.266.pdf.json
i O
in O
Equation O
( O
3 O
) O
for O
−→ O
KD O
) O
. O
section 5
id pdf2json/2021.acl-long.266.pdf.json
Finally O
, O
we O
calculate O
recall B-METRIC
, O
precision B-METRIC
, O
F1 B-METRIC
scores O
based O
on O
low-frequency O
bilingual O
links O
for O
the O
subset O
. O
section 5
id pdf2json/2021.acl-long.266.pdf.json
Recall B-METRIC
( O
R O
) O
represents O
how O
many O
low-frequency O
source O
words O
can O
be O
aligned O
to O
targets O
. O
section 5
id pdf2json/2021.acl-long.266.pdf.json
Precision B-METRIC
( O
P O
) O
means O
how O
many O
aligned O
low-frequency O
links O
are O
correct O
according O
to O
human O
evaluation O
. O
section 5
id pdf2json/2021.acl-long.266.pdf.json
F1 B-METRIC
is O
the O
harmonic O
mean B-METRIC
between O
precision B-METRIC
and O
recall B-METRIC
. O
section 5
id pdf2json/2021.acl-long.266.pdf.json
Similarly O
, O
we O
can O
analyze O
t O
7→ O
s O
LFW O
Links O
by O
considering O
low-frequency O
targets O
. O
section 5
id pdf2json/2021.acl-long.266.pdf.json
Table O
1 O
shows O
the O
results O
on O
low-frequency O
links O
. O
section 5
id pdf2json/2021.acl-long.266.pdf.json
Compared O
with O
Raw O
, O
−→ O
KD O
can O
recall B-METRIC
more O
s O
7→ O
t O
LFW O
links O
( O
73.4 O
vs. O
66.4 O
) O
with O
more O
accurate O
alignment O
( O
89.2 O
vs. O
73.3 O
) O
. O
section 5
id pdf2json/2021.acl-long.266.pdf.json
This O
demonstrates O
the O
effectiveness O
of O
KD O
for O
NAT O
models O
from O
the O
bilingual O
alignment O
perspective O
. O
section 5
id pdf2json/2021.acl-long.266.pdf.json
However O
, O
in O
the O
t O
7→ O
s O
direction O
, O
there O
are O
fewer O
LFW O
links O
( O
69.9 O
vs. O
72.3 O
) O
with O
worse O
alignment O
quality O
( O
79.1 O
vs. O
80.6 O
) O
in−→ O
KD O
than O
those O
in O
Raw O
. O
section 5
id pdf2json/2021.acl-long.266.pdf.json
This O
confirms O
our O
claim O
that O
KD O
harms O
NAT O
models O
due O
to O
the O
loss O
of O
lowfrequency O
target B-PARAMETER
words O
. O
section 5
id pdf2json/2021.acl-long.266.pdf.json
Inspired O
by O
these O
findings O
, O
it O
is O
natural O
to O
assume O
that O
reverse O
KD O
exhibits O
complementary O
properties O
. O
section 5
id pdf2json/2021.acl-long.266.pdf.json
Accordingly O
, O
we O
conduct O
the O
same O
analysis O
method O
on O
←− O
KD O
data O
, O
and O
found O
better O
t O
7→ O
s O
links O
but O
worse O
s O
7→ O
t O
links O
compared O
with O
Raw O
. O
section 5
id pdf2json/2021.acl-long.266.pdf.json
Take O
the O
Zh-En O
sentence O
pair O
in O
Table O
2 O
for O
example O
, O
−→ O
KD O
retains O
the O
source O
side O
lowfrequency O
Chinese O
words O
“ O
海克曼 O
” O
( O
RawS O
) O
but O
generates O
the O
high-frequency O
English O
words O
“ O
Heckman O
” O
instead O
of O
the O
golden O
“ O
Hackman O
” O
( O
−→ O
KDT O
) O
. O
section 5
id pdf2json/2021.acl-long.266.pdf.json
On O
the O
other O
hand O
, O
←− O
KD O
preserves O
the O
low-frequency O
English O
words O
“ O
Hackman O
” O
( O
RawT O
) O
but O
produces O
the O
high-frequency O
Chinese O
words O
“ O
哈克曼 O
” O
( O
←− O
KDS O
) O
. O
section 5
id pdf2json/2021.acl-long.266.pdf.json
Our O
Approach O
Based O
on O
analysis O
results O
, O
we O
propose O
to O
train O
NAT O
models O
on O
bidirectional O
distil- O
2The O
FastAlign O
( O
Dyer O
et O
al. O
, O
2013 O
) O
was O
employed O
to O
build O
word B-METHOD
alignments O
for O
the O
training O
datasets O
. O
section 5
id pdf2json/2021.acl-long.266.pdf.json
lation O
by O
concatenating O
two O
kinds O
of O
distilled O
data O
. O
section 5
id pdf2json/2021.acl-long.266.pdf.json
The O
reverse O
distillation O
is O
to O
replace O
the O
source O
sentences O
in O
the O
original O
training O
data O
with O
synthetic O
ones O
generated O
by O
a B-PARAMETER
backward O
AT O
teacher.3 O
According O
to O
Equation O
3 O
, O
←− O
KD O
can O
be O
formulated O
as O
: O
←− O
KD O
= O
{ O
( O
yi O
, O
ft7→s O
( O
yi O
) O
) O
|yi O
∈ O
Rawt O
} O
Ni=1 O
( O
4 O
) O
where O
ft7→s O
represents O
an O
AT-based O
translation O
model O
trained O
on O
Raw O
data O
for O
translating O
text O
from O
the O
target B-PARAMETER
to O
the O
source O
language O
. O
section 5
id pdf2json/2021.acl-long.266.pdf.json
Figure O
1 O
( O
c B-METRIC
) O
illustrates O
the O
training O
strategy O
. O
section 5
id pdf2json/2021.acl-long.266.pdf.json
First O
, O
we O
employ O
both O
fs O
7→t O
and O
ft7→s O
AT O
models O
to O
generate O
−→ O
KD O
and O
←− O
KD O
data O
, O
respectively O
. O
section 5
id pdf2json/2021.acl-long.266.pdf.json
Considering O
complementarity O
of O
two O
distilled O
data O
, O
we O
combine O
−→ O
KD O
and O
←− O
KD O
as O
a B-PARAMETER
new O
training O
data O
for O
training O
NAT O
models O
. O
section 5
id pdf2json/2021.acl-long.266.pdf.json
We O
expect O
that O
1 O
) O
distilled O
data O
can O
maintain O
advantages O
of O
low-modes O
; O
2 O
) O
bidirectinoal O
distillation O
can O
recall B-METRIC
more O
LFW O
links O
on O
two O
directions O
with O
better O
alignment O
quality O
, O
leading O
to O
the O
overall O
improvements O
. O
section 5
id pdf2json/2021.acl-long.266.pdf.json
Besides O
, O
Nguyen B-AUTHOR
et O
al O
. O
section 5
id pdf2json/2021.acl-long.266.pdf.json
( O
2020 O
) O
claimed O
that O
combining O
different O
distilled O
data O
( O
generated O
by O
various O
models O
trained O
with O
different O
seeds O
) O
improves O
data O
diversification O
for O
NMT O
, O
and O
we O
leave O
this O
for O
future O
work O
. O

section 6
id pdf2json/2021.acl-long.266.pdf.json
We O
have O
proposed O
two O
parallel O
approaches O
to O
rejuvenate O
low-frequency O
knowledge O
from O
authentic O
( O
§2.2 O
) O
and O
synthetic O
( O
§2.3 O
) O
data O
, O
respectively O
. O
section 6
id pdf2json/2021.acl-long.266.pdf.json
Intuitively O
, O
we O
combine O
both O
of O
them O
to O
further O
improve O
the O
model O
performance O
. O
section 6
id pdf2json/2021.acl-long.266.pdf.json
From O
data O
view O
, O
two O
presented O
training O
strategies O
are O
: O
Raw→ O
−→KD O
( O
Raw O
Pretraining O
) O
and O
−→KD O
+←−KD O
( O
Bidirectional O
Distillation O
Training O
) O
. O
section 6
id pdf2json/2021.acl-long.266.pdf.json
Considering O
the O
effectiveness O
of O
pretraining O
( O
Mathis O
et O
al. O
, O
2021 O
) O
and O
clean O
finetuning O
( O
Wu B-AUTHOR
et O
al. O
, O
2019 O
) O
, O
we O
introduce O
a B-PARAMETER
combined O
pipeline O
: O
Raw→ O
−→KD O
+←−KD→ O
−→KD O
as O
out O
best O
training O
strategy O
. O
section 6
id pdf2json/2021.acl-long.266.pdf.json
There O
are O
many O
possible O
ways O
to O
implement O
the O
general O
idea O
of O
combining O
two O
approaches O
. O
section 6
id pdf2json/2021.acl-long.266.pdf.json
The O
aim O
of O
this O
paper O
is O
not O
to O
explore O
the O
whole O
space O
but O
simply O
to O
show O
that O
one O
fairly O
straightforward O
implementation O
works O
well O
and O
the O
idea O
is O
reasonable O
. O
section 6
id pdf2json/2021.acl-long.266.pdf.json
Nonetheless O
, O
we O
compare O
possible O
strategies O
of O
combination O
two O
approaches O
as O
well O
as O
demonstrate O
their O
complementarity O
in O
§3.3 O
. O
section 6
id pdf2json/2021.acl-long.266.pdf.json
While O
in O
main O
experiments O
( O
in O
§3.2 O
) O
, O
we O
valid O
the O
combination O
strategy O
, O
namely O
Low-Frequency O
Rejuvenation O
( O
LFR O
) O
. O
section 6
id pdf2json/2021.acl-long.266.pdf.json
3This O
is O
different O
from O
back-translation O
( O
Edunov O
et O
al. O
, O
2018 O
) O
, O
which O
is O
an O
alternative O
to O
leverage O
monolingual O
data O
. O


section 8
id pdf2json/2021.acl-long.266.pdf.json
Data O
Main O
experiments O
are O
conducted O
on O
four O
widely-used O
translation O
datasets O
: O
WMT14 O
EnglishGerman O
( O
En-De O
, O
Vaswani B-AUTHOR
et O
al O
. O
section 8
id pdf2json/2021.acl-long.266.pdf.json
2017 O
) O
, O
WMT16 O
Romanian-English O
( O
Ro-En O
, O
Gu B-AUTHOR
et I-AUTHOR
al I-AUTHOR
. O
section 8
id pdf2json/2021.acl-long.266.pdf.json
2018 O
) O
, O
WMT17 O
Chinese-English O
( O
Zh-En O
, O
Hassan O
et O
al O
. O
section 8
id pdf2json/2021.acl-long.266.pdf.json
2018 O
) O
, O
and O
WAT17 O
Japanese-English O
( O
Ja-En O
, O
Morishita O
et O
al O
. O
section 8
id pdf2json/2021.acl-long.266.pdf.json
2017 O
) O
, O
which O
consist O
of O
4.5M O
, O
0.6M O
, O
20M O
, O
and O
2M O
sentence O
pairs O
, O
respectively O
. O
section 8
id pdf2json/2021.acl-long.266.pdf.json
We O
use O
the O
same O
validation O
and O
test O
datasets O
with O
previous O
works O
for O
fair O
comparison O
. O
section 8
id pdf2json/2021.acl-long.266.pdf.json
To O
prove O
the O
universality O
of O
our O
approach O
, O
we O
further O
experiment O
on O
different O
data O
volumes O
, O
which O
are O
sampled O
from O
WMT19 O
En-De.4 O
The O
Small O
and O
Medium O
corpora O
respectively O
consist O
of O
1.0M O
and O
4.5M O
sentence O
pairs O
, O
and O
Large O
one O
is O
the O
whole O
dataset O
which O
contains O
36M O
sentence O
pairs O
. O
section 8
id pdf2json/2021.acl-long.266.pdf.json
We O
preprocess O
all O
data O
via O
BPE B-METHOD
( O
Sennrich B-AUTHOR
et O
al. O
, O
2016 O
) O
with O
32K O
merge O
operations O
. O
section 8
id pdf2json/2021.acl-long.266.pdf.json
We O
use O
tokenized O
BLEU B-METRIC
( O
Papineni B-AUTHOR
et O
al. O
, O
2002 O
) O
as O
the O
evaluation O
metric O
, O
and O
sign-test O
( O
Collins O
et O
al. O
, O
2005 O
) O
for O
statistical O
significance O
test O
. O
section 8
id pdf2json/2021.acl-long.266.pdf.json
The O
translation O
accuracy B-PARAMETER
of O
lowfrequency O
words O
is O
measured O
by O
AoLC O
( O
Ding B-AUTHOR
et O
al. O
, O
2021b O
) O
, O
where O
word B-METHOD
alignments O
are O
established O
4http O
: O
//www.statmt.org/wmt19/ O
translation-task.html O
based O
on O
the O
widely-used O
automatic O
alignment O
tool O
GIZA++ O
( O
Och B-AUTHOR
and O
Ney O
, O
2003 O
) O
. O
section 8
id pdf2json/2021.acl-long.266.pdf.json
Models O
We O
validated O
our O
research O
hypotheses O
on O
two O
state-of-the-art O
NAT O
models O
: O
• O
Mask-Predict O
( O
MaskT O
, O
Ghazvininejad O
et O
al O
. O
section 8
id pdf2json/2021.acl-long.266.pdf.json
2019 O
) O
that O
uses O
the O
conditional O
mask O
LM O
( O
Devlin B-AUTHOR
et O
al. O
, O
2019 O
) O
to O
iteratively O
generate O
the O
target B-PARAMETER
sequence O
from O
the O
masked O
input O
. O
section 8
id pdf2json/2021.acl-long.266.pdf.json
We O
followed O
its O
optimal O
settings O
to O
keep O
the O
iteration O
number O
as O
10 O
and O
length B-METRIC
beam O
as O
5 O
. O
section 8
id pdf2json/2021.acl-long.266.pdf.json
• O
Levenshtein O
Transformer B-METHOD
( O
LevT O
, O
Gu B-AUTHOR
et I-AUTHOR
al I-AUTHOR
. O
section 8
id pdf2json/2021.acl-long.266.pdf.json
2019 O
) O
that O
introduces O
three O
steps O
: O
deletion O
, O
placeholder O
and O
token O
prediction O
. O
section 8
id pdf2json/2021.acl-long.266.pdf.json
The O
decoding O
iterations O
adaptively O
depends O
on O
certain O
conditions O
. O
section 8
id pdf2json/2021.acl-long.266.pdf.json
We O
closely O
followed O
previous O
works O
to O
apply O
sequence-level O
knowledge O
distillation O
to O
NAT O
( O
Kim B-AUTHOR
and O
Rush B-AUTHOR
, O
2016 O
) O
. O
section 8
id pdf2json/2021.acl-long.266.pdf.json
Specifically O
, O
we O
train O
both O
BASE B-METHOD
and O
BIG O
Transformer B-METHOD
as O
the O
AT O
teachers O
. O
section 8
id pdf2json/2021.acl-long.266.pdf.json
For O
BIG O
model O
, O
we O
adopt O
large O
batch O
strategy O
( O
i.e O
. O
section 8
id pdf2json/2021.acl-long.266.pdf.json
458K O
tokens/batch O
) O
to O
optimize O
the O
performance O
. O
section 8
id pdf2json/2021.acl-long.266.pdf.json
Most O
NAT O
tasks O
employ O
Transformer-BIG O
as O
their O
strong O
teacher O
except O
for O
Ro-En O
and O
Small O
En-De O
, O
which O
are O
distilled O
by O
Transformer-BASE O
. O
section 8
id pdf2json/2021.acl-long.266.pdf.json
Training O
Traditionally O
, O
NAT O
models O
are O
usually O
trained O
for O
300K O
steps O
on O
regular O
batch O
size O
( O
i.e O
. O
section 8
id pdf2json/2021.acl-long.266.pdf.json
128K O
tokens/batch O
) O
. O
section 8
id pdf2json/2021.acl-long.266.pdf.json
In O
this O
work O
, O
we O
empirically O
adopt O
large O
batch O
strategy O
( O
i.e O
. O
section 8
id pdf2json/2021.acl-long.266.pdf.json
480K O
tokens/batch O
) O
to O
reduce O
the O
training O
steps O
for O
NAT O
( O
i.e O
. O
section 8
id pdf2json/2021.acl-long.266.pdf.json
70K O
) O
. O
section 8
id pdf2json/2021.acl-long.266.pdf.json
Accordingly O
, O
the O
learning B-PARAMETER
rate I-PARAMETER
warms O
up O
to O
1× O
10−7 O
for O
10K O
steps O
, O
and O
then O
decays O
for O
60k O
steps O
with O
the O
cosine O
schedule O
( O
Ro-En O
models O
only O
need O
4K O
and O
21K O
, O
respectively O
) O
. O
section 8
id pdf2json/2021.acl-long.266.pdf.json
For O
regularization O
, O
we O
tune O
the O
dropout B-METHOD
rate O
from O
[ O
0.1 O
, O
0.2 O
, O
0.3 O
] O
based O
on O
validation O
performance O
in O
each O
direction O
, O
and O
apply O
weight O
decay O
with O
0.01 O
and O
label O
smoothing O
with O
= O
0.1 O
. O
section 8
id pdf2json/2021.acl-long.266.pdf.json
We O
use O
Adam B-SOFTWARE
optimizer I-SOFTWARE
( O
Kingma B-AUTHOR
and O
Ba B-AUTHOR
, O
2015 O
) O
to O
train O
our O
models O
. O
section 8
id pdf2json/2021.acl-long.266.pdf.json
We O
followed O
the O
common O
practices O
( O
Ghazvininejad O
et O
al. O
, O
2019 O
; O
Kasai O
et O
al. O
, O
2020 O
) O
to O
evaluate O
the O
performance O
on O
an O
ensemble O
of O
top O
5 O
checkpoints O
to O
avoid O
stochasticity O
. O
section 8
id pdf2json/2021.acl-long.266.pdf.json
Note O
that O
the O
total O
training O
steps O
of O
the O
proposed O
approach O
( O
in O
§2.2∼2.4 O
) O
are O
identical O
with O
those O
of O
the O
standard O
training O
( O
in O
§2.1 O
) O
. O
section 8
id pdf2json/2021.acl-long.266.pdf.json
Taking O
the O
best O
training O
strategy O
( O
Raw O
→ O
−→KD O
+←−KD O
→ O
−→KD O
) O
for O
example O
, O
we O
empirically O
set O
the O
training O
step O
for O
each O
stage O
is O
20K O
, O
20K O
and O
30K O
, O
respectively O
. O
section 8
id pdf2json/2021.acl-long.266.pdf.json
And O
Ro-En O
models O
respectively O
need O
8K O
, O
8K O
and O
9K O
steps O
in O
corresponding O
training O
stage O
. O

section 9
id pdf2json/2021.acl-long.266.pdf.json
Comparison O
with O
Previous O
Work O
Table O
3 O
lists O
the O
results O
of O
previous O
competitive O
NAT O
models O
( O
Gu O
et O
al. O
, O
2018 O
; O
Lee B-AUTHOR
et O
al. O
, O
2018 O
; O
Kasai O
et O
al. O
, O
2020 O
; O
Gu O
et O
al. O
, O
2019 O
; O
Ghazvininejad O
et O
al. O
, O
2019 O
) O
on O
the O
WMT16 O
Ro-En O
and O
WMT14 O
En-De O
benchmark O
. O
section 9
id pdf2json/2021.acl-long.266.pdf.json
We O
implemented O
our O
approach O
on O
top O
of O
two O
advanced O
NAT O
models O
( O
i.e O
. O
section 9
id pdf2json/2021.acl-long.266.pdf.json
Mask-Predict O
and O
Levenshtein O
Transformer B-METHOD
) O
. O
section 9
id pdf2json/2021.acl-long.266.pdf.json
Compared O
with O
standard O
NAT O
models O
, O
our O
training O
strategy O
significantly O
and O
consistently O
improves O
translation O
performance O
( O
BLEU↑ O
) O
across O
different O
language O
pairs O
and O
NAT O
models O
. O
section 9
id pdf2json/2021.acl-long.266.pdf.json
Besides O
, O
the O
improvements O
on O
translation O
performance O
are O
mainly O
due O
to O
a B-PARAMETER
increase O
of O
translation O
accuracy B-PARAMETER
on O
low-frequency O
words O
( O
ALF↑ O
) O
, O
which O
reconfirms O
our O
claims O
. O
section 9
id pdf2json/2021.acl-long.266.pdf.json
For O
instance O
, O
our O
method O
significantly O
improves O
the O
standard O
MaskPredict O
model O
by O
+0.8 O
BLEU B-METRIC
score O
with O
a B-PARAMETER
substantial O
+3.6 O
increase O
in O
ALF O
score O
. O
section 9
id pdf2json/2021.acl-long.266.pdf.json
Encouragingly O
, O
our O
approach O
push O
the O
existing O
NAT O
models O
to O
achieve O
new O
SOTA B-SOFTWARE
performances O
( O
i.e O
. O
section 9
id pdf2json/2021.acl-long.266.pdf.json
28.2 O
and O
33.9 O
BLEU B-METRIC
on O
En-De O
and O
Ro-En O
, O
respectively O
) O
. O
section 9
id pdf2json/2021.acl-long.266.pdf.json
It O
is O
worth O
noting O
that O
our O
data-level O
approaches O
neither O
modify O
model O
architecture O
nor O
add O
extra O
training O
loss O
, O
thus O
do O
not O
increase O
any O
latency O
( O
“ O
Speed O
” O
) O
, O
maintaining O
the O
intrinsic O
advantages O
of O
non-autoregressive O
generation O
. O
section 9
id pdf2json/2021.acl-long.266.pdf.json
We O
must O
admit O
that O
our O
strategy O
indeed O
increase O
the O
amount O
of O
computing O
resources O
due O
to O
that O
we O
should O
train O
ft7→s O
AT O
teachers O
for O
building O
←− O
KD O
data O
. O
section 9
id pdf2json/2021.acl-long.266.pdf.json
Results O
on O
Other O
Language O
Pairs O
Table O
4 O
lists O
the O
results O
of O
NAT O
models O
on O
Zh-En O
and O
Ja-En O
language O
pairs O
, O
which O
belong O
to O
different O
language O
families O
( O
i.e O
. O
section 9
id pdf2json/2021.acl-long.266.pdf.json
Indo-European O
, O
Sino-Tibetan O
and O
Japonic O
) O
. O
section 9
id pdf2json/2021.acl-long.266.pdf.json
Compared O
with O
baselines O
, O
our O
method O
significantly O
and O
incrementally O
improves O
the O
translation O
quality O
in O
all O
cases O
. O
section 9
id pdf2json/2021.acl-long.266.pdf.json
For O
Zh-En O
, O
LFR O
achieves O
on O
average B-METRIC
+0.8 O
BLEU B-METRIC
improvement O
over O
the O
traditional O
training O
, O
along O
with O
increasing O
on O
average B-METRIC
+3.0 O
% O
accuracy B-PARAMETER
on O
low-frequency O
word B-METHOD
translation O
. O
section 9
id pdf2json/2021.acl-long.266.pdf.json
For O
long-distance O
language O
pair O
Ja-En O
, O
our O
method O
still O
improves O
the O
NAT O
model O
by O
on O
average B-METRIC
+0.7 O
BLEU B-METRIC
point O
with O
on O
average B-METRIC
+2.2 O
ALF O
. O
section 9
id pdf2json/2021.acl-long.266.pdf.json
Furthermore O
, O
NAT O
models O
with O
the O
proposed O
training O
strategy O
perform O
closely O
to O
their O
AT O
teachers O
( O
i.e O
. O
section 9
id pdf2json/2021.acl-long.266.pdf.json
0.2 O
∆BLEU O
) O
. O
section 9
id pdf2json/2021.acl-long.266.pdf.json
This O
shows O
the O
effectiveness O
and O
universality O
of O
our O
method O
across O
language O
pairs O
. O
section 9
id pdf2json/2021.acl-long.266.pdf.json
Results O
on O
Domain O
Shift O
Scenario O
The O
lexical O
choice O
must O
be O
informed O
by O
linguistic O
knowledge O
of O
how O
the O
translation O
model O
’ O
s O
input O
data O
maps O
onto O
words O
in O
the O
target B-PARAMETER
domain O
. O
section 9
id pdf2json/2021.acl-long.266.pdf.json
Since O
low-frequency O
words O
get O
lost O
in O
traditional O
NAT O
models O
, O
the O
problem O
of O
lexical O
choice O
is O
more O
severe O
under O
domain O
shift O
scenario O
( O
i.e O
. O
section 9
id pdf2json/2021.acl-long.266.pdf.json
models O
are O
trained O
on O
one O
domain O
but O
tested O
on O
other O
domains O
) O
. O
section 9
id pdf2json/2021.acl-long.266.pdf.json
Thus O
, O
we O
conduct O
evaluation O
on O
WMT14 O
En-De O
models O
over O
five O
out-of-domain O
test O
sets O
( O
Müller O
et O
al. O
, O
2020 O
) O
, O
including O
law O
, O
medicine O
, O
IT O
, O
Koran O
and O
movie O
subtitle O
domains O
. O
section 9
id pdf2json/2021.acl-long.266.pdf.json
As O
shown O
in O
Table O
5 O
, O
standard O
NAT O
models O
suffer O
large O
performance O
drops O
in O
terms O
of O
BLEU B-METRIC
score O
( O
i.e O
. O
section 9
id pdf2json/2021.acl-long.266.pdf.json
on O
average B-METRIC
-2.9 O
BLEU B-METRIC
over O
AT O
model O
) O
. O
section 9
id pdf2json/2021.acl-long.266.pdf.json
By O
observing O
these O
outputs O
, O
we O
found O
a B-PARAMETER
large O
amount O
of O
translation O
errors O
on O
low-frequency O
words O
, O
most O
of O
which O
are O
domain-specific O
terminologies O
. O
section 9
id pdf2json/2021.acl-long.266.pdf.json
In O
contrast O
, O
our O
approach O
improves O
translation O
quality O
( O
i.e O
. O
section 9
id pdf2json/2021.acl-long.266.pdf.json
on O
average B-METRIC
-1.4 O
BLEU B-METRIC
over O
AT O
model O
) O
by O
rejuvenating O
low-frequency O
words O
to O
a B-PARAMETER
certain O
extent O
, O
showing O
that O
LFR O
increases O
the O
domain O
robustness O
of O
NAT O
models O
. O
section 9
id pdf2json/2021.acl-long.266.pdf.json
Results O
on O
Different O
Data O
Scales O
To O
confirm O
the O
effectiveness O
of O
our O
method O
across O
different O
data O
sizes O
, O
we O
further O
experiment O
on O
three O
En-De O
datasets O
at O
different O
scale O
. O
section 9
id pdf2json/2021.acl-long.266.pdf.json
The O
small- O
and O
mediumscale O
training O
data O
are O
randomly O
sampled O
from O
WM19 O
En-De O
corpus O
, O
containing O
about O
1.0M O
and O
4.5M O
sentence O
pairs O
, O
respectively O
. O
section 9
id pdf2json/2021.acl-long.266.pdf.json
The O
large-scale O
one O
is O
collected O
from O
WMT19 O
, O
which O
consists O
of O
36M O
sentence O
pairs O
. O
section 9
id pdf2json/2021.acl-long.266.pdf.json
We O
report O
the O
BLEU B-METRIC
scores I-METRIC
on O
same O
testset O
newstest2019 O
for O
fair O
comparison O
. O
section 9
id pdf2json/2021.acl-long.266.pdf.json
We O
employs O
base O
model O
to O
train O
the O
small-scale O
AT O
teacher O
, O
and O
big O
model O
with O
large O
batch O
strategy O
( O
i.e O
. O
section 9
id pdf2json/2021.acl-long.266.pdf.json
458K O
tokens/batch O
) O
to O
build O
the O
AT O
teachers O
for O
medium- O
and O
large-scale O
. O
section 9
id pdf2json/2021.acl-long.266.pdf.json
As O
seen O
in O
Table O
6 O
, O
our O
simple O
training O
recipe O
boost O
performances O
for O
NAT O
models O
across O
different O
size O
of O
datasets O
, O
especially O
on O
large O
scale O
( O
+0.9 O
) O
, O
showing O
the O
robustness O
and O
effectiveness O
of O
our O
approach O
. O
section 9
id pdf2json/2021.acl-long.266.pdf.json
Complementary O
to O
Related O
Work O
Ding B-AUTHOR
et O
al O
. O
section 9
id pdf2json/2021.acl-long.266.pdf.json
( O
2021b O
) O
is O
relevant O
to O
our O
work O
, O
which O
introduced O
an O
extra O
bilingual O
data-dependent O
prior O
objective O
to O
augment O
NAT O
models O
the O
ability O
to O
learn O
lowfrequency O
words O
in O
raw O
data O
. O
section 9
id pdf2json/2021.acl-long.266.pdf.json
Our O
method O
is O
complementary O
to O
theirs O
due O
to O
that O
we O
only O
change O
data O
and O
training O
strategies O
( O
model-agnostic O
) O
. O
section 9
id pdf2json/2021.acl-long.266.pdf.json
As O
shown O
in O
Table O
7 O
, O
two O
approaches O
yield O
comparable O
performance O
in O
terms O
of O
BLEU B-METRIC
and O
ALF O
. O
section 9
id pdf2json/2021.acl-long.266.pdf.json
Besides O
, O
combination O
can O
further O
improve O
BLEU B-METRIC
as O
well O
as O
ALF O
scores O
( O
i.e O
. O
section 9
id pdf2json/2021.acl-long.266.pdf.json
+0.3 O
and O
+0.6 O
) O
. O
section 9
id pdf2json/2021.acl-long.266.pdf.json
This O
illustrates O
the O
complementarity O
of O
model-level O
and O
data-level O
approaches O
on O
rejuvenating O
low-frequency O
knowldege O
for O
NAT O
models O
. O

section 10
id pdf2json/2021.acl-long.266.pdf.json
We O
conducted O
extensive O
analyses O
to O
better O
understand O
our O
approach O
. O
section 10
id pdf2json/2021.acl-long.266.pdf.json
All O
results O
are O
reported O
on O
the O
Mask-Predict O
models O
. O
section 10
id pdf2json/2021.acl-long.266.pdf.json
Accuracy B-METRIC
of O
Lexical O
Choice O
To O
understand O
where O
the O
performance O
gains O
come O
from O
, O
we O
conduct O
fine-grained O
analysis O
on O
lexical O
choice O
. O
section 10
id pdf2json/2021.acl-long.266.pdf.json
We O
divide O
“ O
All O
” O
tokens O
into O
three O
categories O
based O
on O
their O
frequency O
, O
including O
“ O
High O
” O
, O
“ O
Medium O
” O
and O
“ O
Low O
” O
. O
section 10
id pdf2json/2021.acl-long.266.pdf.json
Following O
Ding B-AUTHOR
et O
al O
. O
section 10
id pdf2json/2021.acl-long.266.pdf.json
( O
2021b O
) O
, O
we O
measure O
the O
accuracy B-PARAMETER
of O
lexical O
choice O
on O
different O
frequency O
of O
words O
. O
section 10
id pdf2json/2021.acl-long.266.pdf.json
Table O
8 O
shows O
the O
results O
. O
section 10
id pdf2json/2021.acl-long.266.pdf.json
Takeaway O
: O
The O
majority O
of O
improvements O
on O
translation O
accuracy B-PARAMETER
is O
from O
the O
low-frequency O
words O
, O
confirming O
our O
hypothesis O
. O
section 10
id pdf2json/2021.acl-long.266.pdf.json
Low-Frequency O
Words O
in O
Output O
We O
expect O
to O
recall B-METRIC
more O
low-frequency O
words O
in O
translation O
output O
. O
section 10
id pdf2json/2021.acl-long.266.pdf.json
As O
shown O
in O
Table O
9 O
, O
we O
calculate O
the O
ratio O
of O
low-frequency O
words O
in O
generated O
sentences O
. O
section 10
id pdf2json/2021.acl-long.266.pdf.json
As O
seen O
, O
KD O
biases O
the O
NAT O
model O
towards O
gen- O
Model O
En-De O
Zh-En O
Ja-En O
All O
High O
Med O
. O
section 10
id pdf2json/2021.acl-long.266.pdf.json
Low O
All O
High O
Med O
. O
section 10
id pdf2json/2021.acl-long.266.pdf.json
Low O
All O
High O
Med O
. O
section 10
id pdf2json/2021.acl-long.266.pdf.json
Low O
MaskT O
( O
Raw O
) O
74.3 O
75.9 O
74.6 O
72.5 O
68.5 O
71.5 O
68.3 O
65.1 O
73.1 O
75.5 O
74.7 O
69.1 O
MaskT O
( O
KD O
) O
76.3 O
82.4 O
78.3 O
68.4 O
72.7 O
81.4 O
75.2 O
61.5 O
75.3 O
82.8 O
76.3 O
66.9 O
erating O
high-frequency O
tokens O
( O
Low O
freq.↓ O
) O
while O
our O
method O
can O
not O
only O
correct O
this O
bias B-PARAMETER
( O
on O
average B-METRIC
+18 O
% O
and O
+26 O
% O
relative O
changes O
for O
+rawpretrain O
and O
+Bi-distillation O
) O
, O
but O
also O
enhance O
translation O
( O
BLEU↑ O
in O
Table O
4 O
) O
. O
section 10
id pdf2json/2021.acl-long.266.pdf.json
Takeaway O
: O
Our O
method O
generates O
translations O
that O
contain O
more O
low-frequency O
words O
. O
section 10
id pdf2json/2021.acl-long.266.pdf.json
Effects O
of O
Variant O
Training O
Strategies O
As O
discussed O
in O
§2.4 O
, O
we O
carefully O
investigate O
alternative O
training O
approaches O
in O
Table O
10 O
. O
section 10
id pdf2json/2021.acl-long.266.pdf.json
We O
make O
the O
total O
training O
step O
identical O
to O
that O
of O
vanilla O
NAT O
models O
, O
and O
report O
both O
BLEU B-METRIC
and O
ALF O
scores O
. O
section 10
id pdf2json/2021.acl-long.266.pdf.json
As O
seen O
, O
all O
variant O
strategies O
perform O
better O
than O
the O
standard O
KD O
method O
in O
terms O
both O
BLEU B-METRIC
and O
ALF O
scores O
, O
confirming O
the O
necessity O
of O
our O
work O
. O
section 10
id pdf2json/2021.acl-long.266.pdf.json
Takeaway O
: O
1 O
) O
Pretraining O
is O
more O
effective O
than O
combination O
on O
utilizing O
data O
manipulation O
strategies O
; O
2 O
) O
raw O
data O
and O
bidirectional O
distilled O
data O
are O
complementary O
to O
each O
other O
; O
3 O
) O
it O
is O
indispensable O
to O
finetune O
models O
on O
−→ O
KD O
in O
the O
last O
stage O
. O
section 10
id pdf2json/2021.acl-long.266.pdf.json
Our O
Approach O
Works O
for O
AT O
Models O
Although O
our O
work O
is O
designed O
for O
NAT O
models O
, O
we O
also O
investigated O
whether O
our O
LFT O
method O
works O
for O
general O
cases O
, O
e.g O
. O
section 10
id pdf2json/2021.acl-long.266.pdf.json
autoregressive O
models O
. O
section 10
id pdf2json/2021.acl-long.266.pdf.json
We O
used O
Transformer-BIG O
as O
the O
teacher O
model O
. O
section 10
id pdf2json/2021.acl-long.266.pdf.json
For O
fair O
comparison O
, O
we O
leverage O
the O
TransformerBASE O
as O
the O
student O
model O
, O
which O
shares O
the O
same O
model O
capacity O
with O
NAT O
student O
( O
i.e O
. O
section 10
id pdf2json/2021.acl-long.266.pdf.json
MaskT O
) O
. O
section 10
id pdf2json/2021.acl-long.266.pdf.json
The O
result O
lists O
in O
Table O
11 B-PARAMETER
. O
section 10
id pdf2json/2021.acl-long.266.pdf.json
As O
seen O
, O
AT O
models O
also O
suffer O
from O
the O
problem O
of O
low-frequency O
words O
when O
using O
knowledge O
distillation O
, O
and O
our O
approach O
also O
works O
for O
them O
. O
section 10
id pdf2json/2021.acl-long.266.pdf.json
Takeaway O
: O
Our O
method O
works O
well O
for O
general O
cases O
through O
rejuvenating O
more O
low-frequency O
words O
. O

section 11
id pdf2json/2021.acl-long.266.pdf.json
Low-Frequency O
Words O
Benefiting O
from O
continuous O
representation O
learned O
from O
the O
training O
data O
, O
NMT O
models O
have O
shown O
the O
promising O
performance O
. O
section 11
id pdf2json/2021.acl-long.266.pdf.json
However O
, O
Koehn O
and O
Knowles O
( O
2017 O
) O
point O
that O
low-frequency O
words O
translation O
is O
still O
one O
of O
the O
key O
challenges O
for O
NMT O
according O
to O
the O
Zipf O
’ O
s O
law O
( O
Zipf O
, O
1949 O
) O
. O
section 11
id pdf2json/2021.acl-long.266.pdf.json
For O
AT O
models O
, O
Arthur O
et O
al O
. O
section 11
id pdf2json/2021.acl-long.266.pdf.json
( O
2016 O
) O
address O
this O
problem O
by O
integrating O
a B-PARAMETER
count-based O
lexicon O
, O
and O
Nguyen B-AUTHOR
and O
Chiang O
( O
2018 O
) O
propose O
an O
additional O
lexical O
model O
, O
which O
is O
jointly O
trained O
with O
the O
AT O
model O
. O
section 11
id pdf2json/2021.acl-long.266.pdf.json
Recently O
, O
Gu B-AUTHOR
et I-AUTHOR
al I-AUTHOR
. O
section 11
id pdf2json/2021.acl-long.266.pdf.json
( O
2020 O
) O
adaptively O
re-weight O
the O
rare O
words O
during O
training O
. O
section 11
id pdf2json/2021.acl-long.266.pdf.json
The O
lexical O
choice O
problem O
is O
more O
serious O
for O
NAT O
models O
, O
since O
1 O
) O
the O
lexical O
choice O
errors O
( O
low-resource O
words O
in O
particular O
) O
of O
AT O
distillation O
will O
propagate O
to O
NAT O
models O
; O
and O
2 O
) O
NAT O
lacks O
target-side O
dependencies O
thus O
misses O
necessary O
target-side O
context O
. O
section 11
id pdf2json/2021.acl-long.266.pdf.json
In O
this O
work O
, O
we O
alleviate O
this O
problem O
by O
solving O
the O
first O
challenge O
. O
section 11
id pdf2json/2021.acl-long.266.pdf.json
Data O
Manipulation O
Our O
work O
is O
related O
to O
previous O
studies O
on O
manipulating O
training O
data O
for O
NMT O
. O
section 11
id pdf2json/2021.acl-long.266.pdf.json
Bogoychev O
and O
Sennrich B-AUTHOR
( O
2019 O
) O
show O
that O
forwardand O
backward-translations O
( O
FT/ O
BT O
) O
could O
both O
boost O
the O
model O
performances O
, O
where O
FT O
plays O
the O
role O
of O
domain B-METHOD
adaptation I-METHOD
and O
BT O
makes O
the O
translation O
fluent O
. O
section 11
id pdf2json/2021.acl-long.266.pdf.json
Fadaee O
and O
Monz O
( O
2018 O
) O
sample O
the O
monolingual O
data O
with O
more O
difficult O
words O
( O
e.g O
. O
section 11
id pdf2json/2021.acl-long.266.pdf.json
rare O
words O
) O
to O
perform O
BT O
, O
achieving O
significant O
improvements O
compared O
with O
randomly O
sampled O
BT O
. O
section 11
id pdf2json/2021.acl-long.266.pdf.json
Nguyen B-AUTHOR
et O
al O
. O
section 11
id pdf2json/2021.acl-long.266.pdf.json
( O
2020 O
) O
diversify O
the O
data O
by O
applying O
FT O
and O
BT O
multiply O
times O
. O
section 11
id pdf2json/2021.acl-long.266.pdf.json
However O
, O
different O
from O
AT O
, O
the O
prerequisite O
of O
training O
a B-PARAMETER
well-performed O
NAT O
model O
is O
to O
perform O
KD O
. O
section 11
id pdf2json/2021.acl-long.266.pdf.json
We O
compared O
with O
related O
works O
in O
Table O
10 O
and O
found O
that O
our O
approach O
consistently O
outperforms O
them O
. O
section 11
id pdf2json/2021.acl-long.266.pdf.json
Note O
that O
all O
the O
ablation O
studies O
focus O
on O
exploiting O
the O
parallel O
data O
without O
augmenting O
additional O
data O
. O
section 11
id pdf2json/2021.acl-long.266.pdf.json
Non-Autoregressive O
Translation O
A O
variety O
of O
approaches O
have O
been O
exploited O
to O
bridge O
the O
performance O
gap O
between O
NAT O
and O
AT O
models O
. O
section 11
id pdf2json/2021.acl-long.266.pdf.json
Some O
researchers O
proposed O
new O
model O
architectures O
( O
Lee B-AUTHOR
et O
al. O
, O
2018 O
; O
Ghazvininejad O
et O
al. O
, O
2019 O
; O
Gu O
et O
al. O
, O
2019 O
; O
Kasai O
et O
al. O
, O
2020 O
) O
, O
aided O
with O
additional O
signals O
( O
Wang B-AUTHOR
et O
al. O
, O
2019 O
; O
Ran O
et O
al. O
, O
2019 O
; O
Ding B-AUTHOR
et O
al. O
, O
2020 O
) O
, O
introduced O
sequential O
information O
( O
Wei B-AUTHOR
et O
al. O
, O
2019 O
; O
Shao O
et O
al. O
, O
2019 O
; O
Guo B-AUTHOR
et O
al. O
, O
2020 O
; O
Hao B-AUTHOR
et O
al. O
, O
2021 O
) O
, O
and O
explored O
advanced O
training O
objectives O
( O
Ghazvininejad O
et O
al. O
, O
2020 O
; O
Du B-AUTHOR
et O
al. O
, O
2021 O
) O
. O
section 11
id pdf2json/2021.acl-long.266.pdf.json
Our O
work O
is O
close O
to O
the O
research O
line O
on O
training O
methods O
. O
section 11
id pdf2json/2021.acl-long.266.pdf.json
Ding B-AUTHOR
et O
al O
. O
section 11
id pdf2json/2021.acl-long.266.pdf.json
( O
2021b O
) O
revealed O
the O
low-frequency O
word B-METHOD
problem O
in O
distilled O
training O
data O
, O
and O
introduced O
an O
extra O
Kullback-Leibler O
divergence O
term O
derived O
by O
comparing O
the O
lexical O
choice O
of O
NAT O
model O
and O
that O
embedded O
in O
the O
raw O
data O
. O
section 11
id pdf2json/2021.acl-long.266.pdf.json
Ding B-AUTHOR
et O
al O
. O
section 11
id pdf2json/2021.acl-long.266.pdf.json
( O
2021a O
) O
propose O
a B-PARAMETER
simple O
and O
effective O
training O
strategy O
, O
which O
progressively O
feeds O
different O
granularity O
of O
data O
into O
NAT O
models O
by O
leveraging O
curriculum O
learning O
. O

section 12
id pdf2json/2021.acl-long.266.pdf.json
In O
this O
study O
, O
we O
propose O
simple O
and O
effective O
training O
strategies O
to O
rejuvenate O
the O
low-frequency O
information O
in O
the O
raw O
data O
. O
section 12
id pdf2json/2021.acl-long.266.pdf.json
Experiments O
show O
that O
our O
approach O
consistently O
and O
significantly O
improves O
translation O
performance O
across O
language O
pairs O
and O
model O
architectures O
. O
section 12
id pdf2json/2021.acl-long.266.pdf.json
Notably O
, O
domain O
shift O
is O
an O
extreme O
scenario O
to O
diagnose O
low-frequency O
translation O
, O
and O
our O
method O
significant O
improves O
them O
. O
section 12
id pdf2json/2021.acl-long.266.pdf.json
Extensive O
analyses O
reveal O
that O
our O
method O
improves O
the O
accuracy B-PARAMETER
of O
lexical O
choices O
for O
low-frequency O
source O
words O
, O
recalling O
more O
low-frequency O
words O
in O
translations O
as O
well O
, O
which O
confirms O
our O
claim O
. O

section 13
id pdf2json/2021.acl-long.266.pdf.json
We O
are O
grateful O
to O
the O
anonymous O
reviewers O
and O
the O
area O
chair O
for O
their O
insightful O
comments O
and O
suggestions O
. O
section 13
id pdf2json/2021.acl-long.266.pdf.json
Xuebo O
Liu B-AUTHOR
and O
Derek O
F. O
Wong O
were O
supported O
in O
part O
by O
the O
Science O
and O
Technology O
Development O
Fund O
, O
Macau O
SAR O
( O
Grant O
No O
. O
section 13
id pdf2json/2021.acl-long.266.pdf.json
0101/2019/A2 O
) O
, O
and O
the O
Multi-year O
Research O
Grant O
from O
the O
University O
of O
Macau O
( O
Grant O
No O
. O
section 13
id pdf2json/2021.acl-long.266.pdf.json
MYRG2020-00054-FST O
) O
. O

section TITLE
id pdf2json/P19-1004.pdf.json
Do O
Neural O
Dialog O
Systems O
Use O
the O
Conversation O
History O
Effectively O
? O
section TITLE
id pdf2json/P19-1004.pdf.json
An B-AUTHOR
Empirical O
Study O

section ABSTRACT
id pdf2json/P19-1004.pdf.json
Neural O
generative O
models O
have O
been O
become O
increasingly O
popular O
when O
building O
conversational O
agents O
. O
section ABSTRACT
id pdf2json/P19-1004.pdf.json
They O
offer O
flexibility O
, O
can O
be O
easily O
adapted O
to O
new O
domains O
, O
and O
require O
minimal O
domain O
engineering O
. O
section ABSTRACT
id pdf2json/P19-1004.pdf.json
A O
common O
criticism O
of O
these O
systems O
is O
that O
they O
seldom O
understand O
or O
use O
the O
available O
dialog O
history O
effectively O
. O
section ABSTRACT
id pdf2json/P19-1004.pdf.json
In O
this O
paper O
, O
we O
take O
an O
empirical O
approach O
to O
understanding O
how O
these O
models O
use O
the O
available O
dialog O
history O
by O
studying O
the O
sensitivity O
of O
the O
models O
to O
artificially O
introduced O
unnatural O
changes O
or O
perturbations O
to O
their O
context O
at O
test O
time O
. O
section ABSTRACT
id pdf2json/P19-1004.pdf.json
We O
experiment O
with O
10 O
different O
types O
of O
perturbations O
on O
4 O
multi-turn O
dialog O
datasets O
and O
find O
that O
commonly O
used O
neural O
dialog O
architectures O
like O
recurrent O
and O
transformer-based O
seq2seq B-METHOD
models O
are O
rarely O
sensitive O
to O
most O
perturbations O
such O
as O
missing O
or O
reordering O
utterances O
, O
shuffling O
words O
, O
etc O
. O
section ABSTRACT
id pdf2json/P19-1004.pdf.json
Also O
, O
by O
open-sourcing O
our O
code O
, O
we O
believe O
that O
it O
will O
serve O
as O
a B-PARAMETER
useful O
diagnostic O
tool O
for O
evaluating O
dialog O
systems O
in O
the O
future O
1 O
. O

section 0
id pdf2json/P19-1004.pdf.json
Proceedings O
of O
the O
57th O
Annual O
Meeting O
of O
the O
Association O
for O
Computational O
Linguistics O
, O
pages O
32–37 O
Florence O
, O
Italy O
, O
July O
28 O
- O
August O
2 O
, O
2019. O
c©2019 O
Association O
for O
Computational O
Linguistics O
32 O

section 1
id pdf2json/P19-1004.pdf.json
With O
recent O
advancements O
in O
generative O
models O
of O
text O
( O
Wu B-AUTHOR
et O
al. O
, O
2016 O
; O
Vaswani B-AUTHOR
et O
al. O
, O
2017 O
; O
Radford B-AUTHOR
et O
al. O
, O
2018 O
) O
, O
neural O
approaches O
to O
building O
chit-chat O
and O
goal-oriented O
conversational O
agents O
( O
Sordoni B-AUTHOR
et O
al. O
, O
2015 O
; O
Vinyals B-AUTHOR
and O
Le B-AUTHOR
, O
2015 O
; O
Serban B-AUTHOR
et O
al. O
, O
2016 O
; O
Bordes B-METHOD
and O
Weston B-AUTHOR
, O
2016 O
; O
Serban B-AUTHOR
et O
al. O
, O
2017b O
) O
has O
gained O
popularity O
with O
the O
hope O
that O
advancements O
in O
tasks O
like O
machine O
translation O
( O
Bahdanau B-AUTHOR
et O
al. O
, O
2015 O
) O
, O
abstractive O
summarization O
( O
See O
et O
al. O
, O
2017 O
) O
should O
translate O
to O
dialog O
systems O
as O
well O
. O
section 1
id pdf2json/P19-1004.pdf.json
While O
these O
models O
have O
demonstrated O
the O
ability O
to O
generate O
fluent O
responses O
, O
∗Corresponding O
author O
: O
chinnadhurai O
@ O
gmail.com O
1Code O
is O
available O
at O
https O
: O
//github.com/ O
chinnadhurai/ParlAI/ O
they O
still O
lack O
the O
ability O
to O
“ O
understand O
” O
and O
process O
the O
dialog O
history O
to O
produce O
coherent O
and O
interesting O
responses O
. O
section 1
id pdf2json/P19-1004.pdf.json
They O
often O
produce O
boring O
and O
repetitive O
responses O
like O
“ O
Thank O
you. O
” O
( O
Li B-AUTHOR
et O
al. O
, O
2015 O
; O
Serban B-AUTHOR
et O
al. O
, O
2017a O
) O
or O
meander O
away O
from O
the O
topic B-AUTHOR
of O
conversation O
. O
section 1
id pdf2json/P19-1004.pdf.json
This O
has O
been O
often O
attributed O
to O
the O
manner O
and O
extent O
to O
which O
these O
models O
use O
the O
dialog O
history O
when O
generating O
responses O
. O
section 1
id pdf2json/P19-1004.pdf.json
However O
, O
there O
has O
been O
little O
empirical O
investigation O
to O
validate O
these O
speculations O
. O
section 1
id pdf2json/P19-1004.pdf.json
In O
this O
work O
, O
we O
take O
a B-PARAMETER
step O
in O
that O
direction O
and O
confirm O
some O
of O
these O
speculations O
, O
showing O
that O
models O
do O
not O
make O
use O
of O
a B-PARAMETER
lot O
of O
the O
information O
available O
to O
it O
, O
by O
subjecting O
the O
dialog O
history O
to O
a B-PARAMETER
variety O
of O
synthetic O
perturbations O
. O
section 1
id pdf2json/P19-1004.pdf.json
We O
then O
empirically O
observe O
how O
recurrent O
( O
Sutskever B-AUTHOR
et O
al. O
, O
2014 O
) O
and O
transformer-based O
( O
Vaswani B-AUTHOR
et O
al. O
, O
2017 O
) O
sequence-to-sequence B-METHOD
( O
seq2seq B-METHOD
) O
models O
respond O
to O
these O
changes O
. O
section 1
id pdf2json/P19-1004.pdf.json
The O
central O
premise O
of O
this O
work O
is O
that O
models O
make O
minimal O
use O
of O
certain O
types O
of O
information O
if O
they O
are O
insensitive O
to O
perturbations O
that O
destroy O
them O
. O
section 1
id pdf2json/P19-1004.pdf.json
Worryingly O
, O
we O
find O
that O
1 O
) O
both O
recurrent O
and O
transformer-based O
seq2seq B-METHOD
models O
are O
insensitive O
to O
most O
kinds O
of O
perturbations O
considered O
in O
this O
work O
2 O
) O
both O
are O
particularly O
insensitive O
even O
to O
extreme O
perturbations O
such O
as O
randomly O
shuffling O
or O
reversing O
words O
within O
every O
utterance O
in O
the O
conversation O
history O
( O
see O
Table O
1 O
) O
and O
3 O
) O
recurrent O
models O
are O
more O
sensitive O
to O
the O
ordering O
of O
utterances O
within O
the O
dialog O
history O
, O
suggesting O
that O
they O
could O
be O
modeling O
conversation O
dynamics O
better O
than O
transformers O
. O

section 2
id pdf2json/P19-1004.pdf.json
Since O
this O
work O
aims O
at O
investigating O
and O
gaining O
an O
understanding O
of O
the O
kinds O
of O
information O
a B-PARAMETER
generative O
neural O
response O
model O
learns O
to O
use O
, O
the O
most O
relevant O
pieces O
of O
work O
are O
where O
sim- O
ilar O
analyses O
have O
been O
carried O
out O
to O
understand O
the O
behavior O
of O
neural O
models O
in O
other O
settings O
. O
section 2
id pdf2json/P19-1004.pdf.json
An B-AUTHOR
investigation O
into O
how O
LSTM B-METHOD
based O
unconditional O
language O
models O
use O
available O
context O
was O
carried O
out O
by O
Khandelwal O
et O
al O
. O
section 2
id pdf2json/P19-1004.pdf.json
( O
2018 O
) O
. O
section 2
id pdf2json/P19-1004.pdf.json
They O
empirically O
demonstrate O
that O
models O
are O
sensitive O
to O
perturbations O
only O
in O
the O
nearby O
context O
and O
typically O
use O
only O
about O
150 O
words O
of O
context O
. O
section 2
id pdf2json/P19-1004.pdf.json
On O
the O
other O
hand O
, O
in O
conditional O
language O
modeling O
tasks O
like O
machine O
translation O
, O
models O
are O
adversely O
affected O
by O
both O
synthetic O
and O
natural O
noise O
introduced O
anywhere O
in O
the O
input O
( O
Belinkov B-AUTHOR
and O
Bisk B-AUTHOR
, O
2017 O
) O
. O
section 2
id pdf2json/P19-1004.pdf.json
Understanding O
what O
information O
is O
learned O
or O
contained O
in O
the O
representations O
of O
neural O
networks O
has O
also O
been O
studied O
by O
“ O
probing O
” O
them O
with O
linear O
or O
deep O
models O
( O
Adi O
et O
al. O
, O
2016 O
; O
Subramanian O
et O
al. O
, O
2018 O
; O
Conneau B-AUTHOR
et O
al. O
, O
2018 O
) O
. O
section 2
id pdf2json/P19-1004.pdf.json
Several O
works O
have O
recently O
pointed O
out O
the O
presence O
of O
annotation O
artifacts O
in O
common O
text O
and O
multi-modal O
benchmarks O
. O
section 2
id pdf2json/P19-1004.pdf.json
For O
example O
, O
Gururangan B-AUTHOR
et O
al O
. O
section 2
id pdf2json/P19-1004.pdf.json
( O
2018 O
) O
demonstrate O
that O
hypothesisonly O
baselines O
for O
natural O
language O
inference O
obtain O
results O
significantly O
better O
than O
random O
guessing O
. O
section 2
id pdf2json/P19-1004.pdf.json
Kaushik B-AUTHOR
and O
Lipton O
( O
2018 O
) O
report O
that O
reading O
comprehension O
systems O
can O
often O
ignore O
the O
entire O
question O
or O
use O
only O
the O
last O
sentence O
of O
a B-PARAMETER
document O
to O
answer O
questions O
. O
section 2
id pdf2json/P19-1004.pdf.json
Anand O
et O
al O
. O
section 2
id pdf2json/P19-1004.pdf.json
( O
2018 O
) O
show O
that O
an O
agent O
that O
does O
not O
navigate O
or O
even O
see O
the O
world O
around O
it O
can O
answer O
questions O
about O
it O
as O
well O
as O
one O
that O
does O
. O
section 2
id pdf2json/P19-1004.pdf.json
These O
pieces O
of O
work O
suggest O
that O
while O
neural O
methods O
have O
the O
potential O
to O
learn O
the O
task O
specified O
, O
its O
design O
could O
lead O
them O
to O
do O
so B-PARAMETER
in O
a B-PARAMETER
manner O
that O
doesn O
’ O
t O
use O
all O
of O
the O
available O
information O
within O
the O
task O
. O
section 2
id pdf2json/P19-1004.pdf.json
Recent O
work O
has O
also O
investigated O
the O
inductive O
biases O
that O
different O
sequence O
models O
learn O
. O
section 2
id pdf2json/P19-1004.pdf.json
For O
example O
, O
Tran B-AUTHOR
et O
al O
. O
section 2
id pdf2json/P19-1004.pdf.json
( O
2018 O
) O
find O
that O
recurrent O
models O
are O
better O
at O
modeling O
hierarchical O
structure O
while O
Tang B-AUTHOR
et O
al O
. O
section 2
id pdf2json/P19-1004.pdf.json
( O
2018 O
) O
find O
that O
feedforward O
architectures O
like O
the O
transformer B-METHOD
and O
convolutional O
models O
are O
not O
better O
than O
RNNs O
at O
modeling O
long-distance O
agreement O
. O
section 2
id pdf2json/P19-1004.pdf.json
Transformers O
however O
excel O
at O
word-sense O
disambiguation O
. O
section 2
id pdf2json/P19-1004.pdf.json
We O
analyze O
whether O
the O
choice O
of O
architecture O
and O
the O
use O
of O
an O
attention B-METHOD
mechanism I-METHOD
affect O
the O
way O
in O
which O
dialog O
systems O
use O
information O
available O
to O
them O
. O

section 3
id pdf2json/P19-1004.pdf.json
Following O
the O
recent O
line O
of O
work O
on O
generative O
dialog O
systems O
, O
we O
treat O
the O
problem O
of O
generating O
an O
appropriate O
response O
given O
a B-PARAMETER
conversation O
history O
as O
a B-PARAMETER
conditional O
language O
modeling O
problem O
. O
section 3
id pdf2json/P19-1004.pdf.json
Specifically O
we O
want O
to O
learn O
a B-PARAMETER
conditional O
probability O
distribution O
Pθ O
( O
y|x O
) O
where O
y O
is O
a B-PARAMETER
reasonable O
response O
given O
the O
conversation O
history O
x O
. O
section 3
id pdf2json/P19-1004.pdf.json
The O
conversation O
history O
is O
typically O
represented O
as O
a B-PARAMETER
sequence O
of O
utterances O
x1 O
, O
x2 O
, O
. O
section 3
id pdf2json/P19-1004.pdf.json
. O
section 3
id pdf2json/P19-1004.pdf.json
.xn O
, O
where O
each O
utterance O
xi O
itself O
is O
comprised O
of O
a B-PARAMETER
sequence O
of O
words O
xi1 O
, O
xi2 O
. O
section 3
id pdf2json/P19-1004.pdf.json
. O
section 3
id pdf2json/P19-1004.pdf.json
. O
section 3
id pdf2json/P19-1004.pdf.json
xik O
. O
section 3
id pdf2json/P19-1004.pdf.json
The O
response O
y O
is O
a B-PARAMETER
single O
utterance O
also O
comprised O
of O
a B-PARAMETER
sequence O
of O
words O
y1 O
, O
y2 O
. O
section 3
id pdf2json/P19-1004.pdf.json
. O
section 3
id pdf2json/P19-1004.pdf.json
. O
section 3
id pdf2json/P19-1004.pdf.json
ym O
. O
section 3
id pdf2json/P19-1004.pdf.json
The O
overall O
conditional O
probability O
is O
factorized O
autoregressively O
as O
Pθ O
( O
y|x O
) O
= O
n∏ O
i=1 O
Pθ O
( O
yi|y O
< O
i O
, O
x1 O
. O
section 3
id pdf2json/P19-1004.pdf.json
. O
section 3
id pdf2json/P19-1004.pdf.json
.xn O
) O
Pθ O
, O
in O
this O
work O
, O
is O
parameterized O
by O
a B-PARAMETER
recurrent O
or O
transformer-based O
seq2seq B-METHOD
model O
. O
section 3
id pdf2json/P19-1004.pdf.json
The O
crux O
of O
this O
work O
is O
to O
study O
how O
the O
learned O
probability O
distribution O
behaves O
as O
we O
artificially O
perturb O
the O
conversation O
history O
x1 O
, O
. O
section 3
id pdf2json/P19-1004.pdf.json
. O
section 3
id pdf2json/P19-1004.pdf.json
.xn O
. O
section 3
id pdf2json/P19-1004.pdf.json
We O
measure O
behavior O
by O
looking O
at O
how O
much O
the O
per-token O
perplexity B-METRIC
increases O
under O
these O
changes O
. O
section 3
id pdf2json/P19-1004.pdf.json
For O
example O
, O
one O
could O
think O
of O
shuffling O
the O
order O
in O
which O
x1 O
. O
section 3
id pdf2json/P19-1004.pdf.json
. O
section 3
id pdf2json/P19-1004.pdf.json
.xn O
is O
presented O
to O
the O
model O
and O
observe O
how O
much O
the O
perplexity B-METRIC
of O
y O
under O
the O
model O
increases O
. O
section 3
id pdf2json/P19-1004.pdf.json
If O
the O
increase O
is O
only O
minimal O
, O
we O
can O
conclude O
that O
the O
ordering O
of O
x1 O
. O
section 3
id pdf2json/P19-1004.pdf.json
. O
section 3
id pdf2json/P19-1004.pdf.json
.xn O
isn O
’ O
t O
informative O
to O
the O
model O
. O
section 3
id pdf2json/P19-1004.pdf.json
For O
a B-PARAMETER
complete O
list O
of O
perturbations O
considered O
in O
this O
work O
, O
please O
refer O
to O
Section O
3.2 O
. O
section 3
id pdf2json/P19-1004.pdf.json
All O
models O
are O
trained O
without O
any O
perturbations O
and O
sensitivity O
is O
studied O
only O
at O
test O
time O
. O

section 4
id pdf2json/P19-1004.pdf.json
We O
experiment O
with O
four O
multi-turn O
dialog O
datasets O
. O
section 4
id pdf2json/P19-1004.pdf.json
bAbI O
dialog O
is O
a B-PARAMETER
synthetic O
goal-oriented O
multiturn O
dataset O
( O
Bordes B-METHOD
and O
Weston B-AUTHOR
, O
2016 O
) O
consisting O
of O
5 O
different O
tasks O
for O
restaurant O
booking O
with O
increasing O
levels O
of O
complexity O
. O
section 4
id pdf2json/P19-1004.pdf.json
We O
consider O
Task O
5 O
in O
our O
experiments O
since O
it O
is O
the O
hardest O
and O
is O
a B-PARAMETER
union O
of O
all O
four O
tasks O
. O
section 4
id pdf2json/P19-1004.pdf.json
It O
contains O
1k O
dialogs O
with O
an O
average B-METRIC
of O
13 O
user O
utterances O
per O
dialog O
. O
section 4
id pdf2json/P19-1004.pdf.json
Persona O
Chat O
is O
an O
open O
domain O
dataset O
( O
Zhang B-AUTHOR
et O
al. O
, O
2018 O
) O
with O
multi-turn O
chit-chat O
conversations O
between O
turkers O
who O
are O
each O
assigned O
a B-PARAMETER
“ O
persona O
” O
at O
random O
. O
section 4
id pdf2json/P19-1004.pdf.json
It O
comprises O
of O
10.9k O
dialogs O
with O
an O
average B-METRIC
of O
14.8 O
turns O
per O
dialog O
. O
section 4
id pdf2json/P19-1004.pdf.json
Dailydialog B-DATASET
is O
an O
open O
domain O
dataset O
( O
Li B-AUTHOR
et O
al. O
, O
2017 O
) O
which O
consists O
of O
dialogs O
that O
resemble O
dayto-day O
conversations O
across O
multiple O
topics O
. O
section 4
id pdf2json/P19-1004.pdf.json
It O
comprises O
of O
13k O
dialogs O
with O
an O
average B-METRIC
of O
7.9 O
turns O
per O
dialog O
. O
section 4
id pdf2json/P19-1004.pdf.json
MutualFriends O
is O
a B-PARAMETER
multi-turn O
goal-oriented O
dataset O
( O
He B-AUTHOR
et O
al. O
, O
2017 O
) O
where O
two O
agents O
must O
discover O
which O
friend O
of O
theirs O
is O
mutual O
based O
on O
the O
friends O
’ O
attributes O
. O
section 4
id pdf2json/P19-1004.pdf.json
It O
contains O
11k O
dialogs O
with O
an O
average B-METRIC
of O
11.41 O
utterances O
per O
dialog O
. O

section 5
id pdf2json/P19-1004.pdf.json
We O
experimented O
with O
several O
types O
of O
perturbation O
operations O
at O
the O
utterance O
and O
word B-METHOD
( O
token O
) O
levels O
. O
section 5
id pdf2json/P19-1004.pdf.json
All O
perturbations O
are O
applied O
in O
isolation O
. O
section 5
id pdf2json/P19-1004.pdf.json
Utterance-level O
perturbations O
We O
consider O
the O
following O
operations O
1 O
) O
Shuf O
that O
shuffles O
the O
sequence O
of O
utterances O
in O
the O
dialog O
history O
, O
2 O
) O
Rev O
that O
reverses O
the O
order O
of O
utterances O
in O
the O
history O
( O
but O
maintains O
word B-METHOD
order O
within O
each O
utterance O
) O
3 O
) O
Drop O
that O
completely O
drops O
certain O
utterances O
and O
4 O
) O
Truncate O
that O
truncates O
the O
dialog O
history O
to O
contain O
only O
the O
k B-METRIC
most O
recent O
utterances O
where O
k B-METRIC
≤ O
n O
, O
where O
n O
is O
the O
length B-METRIC
of O
dialog O
history O
. O
section 5
id pdf2json/P19-1004.pdf.json
Word-level O
perturbations O
We O
consider O
similar O
operations O
but O
at O
the O
word B-METHOD
level O
within O
every O
utterance O
1 O
) O
word-shuffle O
that O
randomly O
shuffles O
the O
words O
within O
an O
utterance O
2 O
) O
reverse O
that O
reverses O
the O
ordering O
of O
words O
, O
3 O
) O
word-drop O
that O
drops O
30 O
% O
of O
the O
words O
uniformly O
4 O
) O
noun-drop O
that O
drops O
all O
nouns O
, O
5 O
) O
verb-drop O
that O
drops O
all O
verbs O
. O

section 6
id pdf2json/P19-1004.pdf.json
We O
experimented O
with O
two O
different O
classes O
of O
models O
- O
recurrent O
and O
transformer-based O
sequence-to-sequence B-METHOD
generative O
models O
. O
section 6
id pdf2json/P19-1004.pdf.json
All O
data O
loading O
, O
model O
implementations O
and O
evaluations O
were O
done O
using O
the O
ParlAI O
framework O
. O
section 6
id pdf2json/P19-1004.pdf.json
We O
used O
the O
default O
hyper-parameters O
for O
all O
the O
models O
as O
specified O
in O
ParlAI O
. O
section 6
id pdf2json/P19-1004.pdf.json
Recurrent O
Models O
We O
trained O
a B-PARAMETER
seq2seq B-METHOD
( O
seq2seq B-METHOD
lstm B-METHOD
) O
model O
where O
the O
encoder B-METHOD
and O
decoder B-METHOD
are O
parameterized O
as O
LSTMs B-METHOD
( O
Hochreiter B-AUTHOR
and O
Schmidhuber B-AUTHOR
, O
1997 O
) O
. O
section 6
id pdf2json/P19-1004.pdf.json
We O
also O
experiment O
with O
using O
decoders O
that O
use O
an O
attention B-METHOD
mechanism I-METHOD
( O
seq2seq B-METHOD
lstm B-METHOD
att O
) O
( O
Bahdanau B-AUTHOR
et O
al. O
, O
2015 O
) O
. O
section 6
id pdf2json/P19-1004.pdf.json
The O
encoder B-METHOD
and O
decoder B-METHOD
LSTMs B-METHOD
have O
2 O
layers O
with O
128 O
dimensional O
hidden O
states O
with O
a B-PARAMETER
dropout B-METHOD
rate O
of O
0.1 O
. O
section 6
id pdf2json/P19-1004.pdf.json
Transformer B-METHOD
Our O
transformer B-METHOD
( O
Vaswani B-AUTHOR
et O
al. O
, O
2017 O
) O
model O
uses O
300 O
dimensional O
embeddings O
and O
hidden O
states O
, O
2 O
layers O
and O
2 O
attention B-METHOD
heads O
with O
no O
dropout B-METHOD
. O
section 6
id pdf2json/P19-1004.pdf.json
This O
model O
is O
significantly O
smaller O
than O
the O
ones O
typically O
used O
in O
machine O
translation O
since O
we O
found O
that O
the O
model O
that O
resembled O
Vaswani B-AUTHOR
et O
al O
. O
section 6
id pdf2json/P19-1004.pdf.json
( O
2017 O
) O
significantly O
overfit O
on O
all O
our O
datasets O
. O
section 6
id pdf2json/P19-1004.pdf.json
While O
the O
models O
considered O
in O
this O
work O
might O
not O
be O
state-of-the-art O
on O
the O
datasets O
considered O
, O
we O
believe O
these O
models O
are O
still O
competitive O
and O
used O
commonly O
enough O
at O
least O
as O
baselines O
, O
that O
the O
community O
will O
benefit O
by O
understanding O
their O
behavior O
. O
section 6
id pdf2json/P19-1004.pdf.json
In O
this O
paper O
, O
we O
use O
early O
stopping O
with O
a B-PARAMETER
patience O
of O
10 O
on O
the O
validation O
set O
to O
save O
our O
best O
model O
. O
section 6
id pdf2json/P19-1004.pdf.json
All O
models O
achieve O
close O
to O
the O
perplexity B-METRIC
numbers O
reported O
for O
generative O
seq2seq B-METHOD
models O
in O
their O
respective O
papers O
. O

section 7
id pdf2json/P19-1004.pdf.json
Our O
results O
are O
presented O
in O
Table O
2 O
and O
Figure O
1 O
. O
section 7
id pdf2json/P19-1004.pdf.json
Table O
2 O
reports O
the O
perplexities O
of O
different O
models O
on O
test O
set O
in O
the O
second O
column O
, O
followed O
by O
the O
increase O
in O
perplexity B-METRIC
when O
the O
dialog O
history O
is O
perturbed O
using O
the O
method O
specified O
in O
the O
column O
header O
. O
section 7
id pdf2json/P19-1004.pdf.json
Rows O
correspond O
to O
models O
trained O
on O
different O
datasets O
. O
section 7
id pdf2json/P19-1004.pdf.json
Figure O
1 O
presents O
the O
change O
in O
perplexity B-METRIC
for O
models O
when O
presented O
only O
with O
the O
k B-METRIC
most O
recent O
utterances O
from O
the O
dialog O
history O
. O
section 7
id pdf2json/P19-1004.pdf.json
We O
make O
the O
following O
observations O
: O
1 O
. O
section 7
id pdf2json/P19-1004.pdf.json
Models O
tend O
to O
show O
only O
tiny O
changes O
in O
perplexity B-METRIC
in O
most O
cases O
, O
even O
under O
extreme O
changes O
to O
the O
dialog O
history O
, O
suggesting O
that O
they O
use O
far O
from O
all O
the O
information O
that O
is O
available O
to O
them O
. O
section 7
id pdf2json/P19-1004.pdf.json
2 O
. O
section 7
id pdf2json/P19-1004.pdf.json
Transformers O
are O
insensitive O
to O
wordreordering O
, O
indicating O
that O
they O
could O
be O
learning O
bag-of-words O
like O
representations O
. O
section 7
id pdf2json/P19-1004.pdf.json
3 O
. O
section 7
id pdf2json/P19-1004.pdf.json
The O
use O
of O
an O
attention B-METHOD
mechanism I-METHOD
in O
seq2seq B-METHOD
lstm B-METHOD
att O
and O
transformers O
makes O
these O
models O
use O
more O
information O
from O
earlier O
parts O
of O
the O
conversation O
than O
vanilla O
seq2seq B-METHOD
models O
as O
seen O
from O
increases O
in O
perplexity B-METRIC
when O
using O
only O
the O
last O
utterance O
. O
section 7
id pdf2json/P19-1004.pdf.json
4 O
. O
section 7
id pdf2json/P19-1004.pdf.json
While O
transformers O
converge O
faster O
and O
to O
lower O
test O
perplexities O
, O
they O
don O
’ O
t O
seem O
to O
capture O
the O
conversational O
dynamics O
across O
utterances O
in O
the O
dialog O
history O
and O
are O
less O
sensitive O
to O
perturbations O
that O
scramble O
this O
structure O
than O
recurrent O
models O
. O

section 8
id pdf2json/P19-1004.pdf.json
This O
work O
studies O
the O
behaviour O
of O
generative O
neural O
dialog O
systems O
in O
the O
presence O
of O
synthetically O
introduced O
perturbations O
to O
the O
dialog O
history O
, O
that O
it O
conditions O
on O
. O
section 8
id pdf2json/P19-1004.pdf.json
We O
find O
that O
both O
recurrent O
and O
transformer-based O
seq2seq B-METHOD
models O
are O
not O
significantly O
affected O
even O
by O
drastic O
and O
unnatural O
modifications O
to O
the O
dialog O
history O
. O
section 8
id pdf2json/P19-1004.pdf.json
We O
also O
find O
subtle O
differences O
between O
the O
way O
in O
which O
recurrent O
and O
transformer-based O
models O
use O
available O
context O
. O
section 8
id pdf2json/P19-1004.pdf.json
By O
open-sourcing O
our O
code O
, O
we O
believe O
this O
paradigm O
of O
studying O
model O
behavior O
by O
introducing O
perturbations O
that O
destroys O
different O
kinds O
of O
structure O
present O
within O
the O
dialog O
history O
can O
be O
a B-PARAMETER
useful O
diagnostic O
tool O
. O
section 8
id pdf2json/P19-1004.pdf.json
We O
also O
foresee O
this O
paradigm O
being O
useful O
when O
building O
new O
dialog O
datasets O
to O
understand O
the O
kinds O
of O
information O
models O
use O
to O
solve O
them O
. O

section 9
id pdf2json/P19-1004.pdf.json
We O
would O
like O
to O
acknowledge O
NVIDIA O
for O
donating O
GPUs O
and O
a B-PARAMETER
DGX-1 O
computer O
used O
in O
this O
work O
. O
section 9
id pdf2json/P19-1004.pdf.json
We O
would O
also O
like O
to O
thank O
the O
anonymous O
reviewers O
for O
their O
constructive O
feedback O
. O
section 9
id pdf2json/P19-1004.pdf.json
Our O
code O
is O
available O
at O
https O
: O
//github.com/ O
chinnadhurai/ParlAI/ O
. O

