positional artefacts propagate throughmasked language model embeddings.
ziyang luo1∗, artur kulmizev1, xiaoxi mao21 department of linguistics and philology, uppsala university, sweden2 fuxi ai lab, netease inc., hangzhou, chinaziyang.luo.9588@student.uu.se, artur.kulmizev@lingfil.uu.semaoxiaoxi@corp.netease.com.
abstract.
in this work, we demonstrate that the contex-tualized word vectors derived from pretrainedmasked language model-based encoders sharea common, perhaps undesirable pattern acrosslayers.
namely, we ﬁnd cases of persistentoutlier neurons within bert and roberta’shidden state vectors that consistently bear thesmallest or largest values in said vectors.
inan attempt to investigate the source of this in-formation, we introduce a neuron-level anal-ysis method, which reveals that the outliersare closely related to information captured bypositional embeddings.
we also pre-train theroberta-base models from scratch and ﬁndthat the outliers disappear without using posi-tional embeddings.
these outliers, we ﬁnd, arethe major cause of anisotropy of encoders’ rawvector spaces, and clipping them leads to in-creased similarity across vectors.
we demon-strate this in practice by showing that clippedvectors can more accurately distinguish wordsenses, as well as lead to better sentence em-beddings when mean pooling.
in three super-vised tasks, we ﬁnd that clipping does not af-fect the performance..1.introduction.
a major area of nlp research in the deep learn-ing era has concerned the representation of wordsin low-dimensional, continuous vector spaces.
traditional methods for achieving this have in-cluded word embedding models such as word2vec(mikolov et al., 2013), glove (pennington et al.,2014), and fasttext (bojanowski et al., 2017).
however, though inﬂuential, such approaches allshare a uniform pitfall in assigning a single, staticvector to a word type.
given that the vast major-ity of words are polysemous (klein and murphy,2001), static word embeddings cannot possibly rep-resent a word’s changing meaning in context..∗ work partly done during internship at netease inc...in recent years, deep language models, likeelmo (peters et al., 2018), bert (devlin et al.,2019) and roberta (liu et al., 2019b), haveachieved great success across many nlp tasks.
such models introduce a new type of word vectors,deemed the contextualized variety, where the repre-sentation is computed with respect to the contextof the target word.
since these vectors are sensitiveto context, they can better address the polysemyproblem that hinders traditional word embeddings.
indeed, studies have shown that replacing staticembeddings (e.g.
word2vec) with contextualizedones (e.g.
bert) can beneﬁt many nlp tasks,including constituency parsing (kitaev and klein,2018), coreference resolution (joshi et al., 2019)and machine translation (liu et al., 2020)..however, despite the major success in deploy-ing these representations across linguistic tasks,there remains little understanding about informa-tion embedded in contextualized vectors and themechanisms that generate them.
indeed, an en-tire research area central to this core issue — theinterpretability of neural nlp models — has re-cently emerged (linzen et al., 2018, 2019; alishahiet al., 2020).
a key theme in this line of workhas been the use of linear probes in investigatingthe linguistic properties of contextualized vectors(tenney et al., 2019; hewitt and manning, 2019).
such studies, among many others, show that con-textualization is an important factor that sets theseembeddings apart from static ones, the latter ofwhich are unreliable in extracting features centralto context or linguistic hierarchy.
nonetheless,much of this work likewise fails to engage withthe raw vector spaces of language models, pre-ferring instead to focus its analysis on the trans-formed vectors.
indeed, the fraction of work thathas done the former has shed some curious insights:that untransformed bert sentence representationsstill lag behind word embeddings across a variety.
proceedingsofthe59thannualmeetingoftheassociationforcomputationallinguisticsandthe11thinternationaljointconferenceonnaturallanguageprocessing,pages5312–5327august1–6,2021.©2021associationforcomputationallinguistics5312of semantic benchmarks (reimers and gurevych,2019) and that the vector spaces of language mod-els are explicitly anisotropic (ethayarajh, 2019;li et al., 2020a).
certainly, an awareness of thepatterns inherent to models’ untransformed vectorspaces — even if shallow — can only beneﬁt thetransformation-based analyses outlined above..in this work, we shed light on a persistent patternthat can be observed for contextualized vectors pro-duced by bert and roberta.
namely, we showthat, across all layers, select neurons in bert androberta consistently bear extremely large values.
we observe this pattern across vectors for all wordsin several datasets, demonstrating that these sin-gleton dimensions serve as major outliers to thedistributions of neuron values in both encoders’representational spaces.
with this insight in mind,the contributions of our work are as follows:.
1. we introduce a neuron-level method for ana-lyzing the origin of a model’s outliers.
usingthis, we show that they are closely related topositional information..2. in investigating the effects of clipping the out-liers (zeroing-out), we show that the degreeof anisotropy in the vector space diminishessigniﬁcantly..3. we show that after clipping the outliers, thebert representations can better distinguishbetween a word’s potential senses in theword-in-context (wic) dataset (pilehvar andcamacho-collados, 2019), as well as lead tobetter sentence embeddings when mean pool-ing..2 finding outliers.
in this section, we demonstrate the existence oflarge-valued vector dimensions across nearly alltokens encoded by bert and roberta.
to illus-trate these patterns, we employ two well-knowndatasets — sst-2 (socher et al., 2013) and qqp1.
sst-2 (60.7k sentences) is a widely-employed sen-timent analysis dataset of movie reviews, whileqqp (727.7k sentences) is a semantic textual sim-ilarity dataset of quora questions, which collectsquestions across many topics.
we choose thesedatasets in order to account for a reasonably widedistributions of domains and topics, but note that.
1https://www.quora.com/q/quoradata/.
first-quora-dataset-release-question-pairs.
figure 1: average vectors for each layer of bert-base..figure 2: average vectors for each layer of roberta-base..any dataset would illustrate our ﬁndings well.
werandomly sample 10k sentences from the trainingsets of both sst-2 and qqp, tokenize them, andencode them via bert-base and roberta-base.
all models are downloaded from the huggingfacetransformers library (wolf et al., 2020), thoughwe replicated our results for bert by loading theprovided model weights via our own loaders..when discounting the input embedding layersof each model, we are left with 3.68m and 3.59mcontextualized token embeddings for bert-baseand roberta-base, respectively.
in order to illus-trate the outlier patterns, we average all subwordvectors for each layer of each model..in examining bert-base, we ﬁnd that the mini-mum value of 96.60% of vectors lies in the 557thdimension.
figure 1 displays the averaged subwordvectors for each layer of bert-base, corroborat-ing that these patterns exist across all layers.
forroberta-base, we likewise ﬁnd that the maximumvalue of all vectors is the 588th element.
interest-ingly, the minimum element of 88.19% of vectors in.
5313roberta-base is the 77th element, implying thatroberta has two such outliers.
figure 2 displaysthe average vectors for each layer of roberta-base..our observations here reveal a curious patternthat is present in the base versions of bert androberta.
we also corroborate the same ﬁndingsfor the large and distilled (sanh et al., 2020) vari-ants of these architectures, which can be found inthe appendix a. indeed, it would be difﬁcult toreach any sort of conclusion about the represen-tational geometry of such models without under-standing the outliers’ origin(s)..3 where do outliers come from?.
in this section, we attempt to trace the source of theoutlier dimensions in bert-base and roberta-base (henceforth bert and roberta).
similarlyto the previous section, we can corroborate the re-sults of the experiments described here (as wellas in the remainder of the paper) for the large anddistilled varieties of each respective architecture.
thus, for reasons of brevity, we focus our forth-coming analyses on the base versions of bertand roberta and include results for the remain-ing models in the appendix b.2 for the interestedreader..in our per-layer analysis in §2, we report thatoutlier dimensions exist across every layer in eachmodel.
upon a closer look at the input layer (whichfeatures a vector sum of positional, segment, andtoken embeddings), we ﬁnd that the same outliersalso exist in positional embeddings.
figure 3 showsthat the 1st positional embedding of bert hastwo such dimensions, where the 557th element islikewise the minimum.
interestingly, this patterndoes not exist in other positional embeddings, norin segment or token embeddings.
furthermore,figure 4 shows that the 4th positional embeddingof roberta has four outliers, which include theaforementioned 77th and 588th dimensions.
wealso ﬁnd that, from the 4th position to the ﬁnal po-sition, the maximum element of 99.8% positionalembeddings is the 588th element..digging deeper, we observe similar patterns inthe layer normalization (ln, ba et al.
(2016)) pa-rameters of both models.
recall that ln has twolearnable parameters — gain (γ) and bias (β) —both of which are 768-dimension vectors (in thecase of the base models).
these are designed asan afﬁne transformation over dimension-wise nor-.
figure 3: the ﬁrst positional embedding of bert-base..figure 4:roberta-base..the fourth positional embedding of.
malized vectors in order to, like most normaliza-tion strategies, improve their expressive ability andto aid in optimization.
every layer of bert androberta applies separate lns post-attention andpre-output.
for bert, the 557th element of the γvector is always among the top-6 largest values forthe ﬁrst ten layers’ ﬁrst ln.
speciﬁcally, it is thelargest value in the ﬁrst three layers.
for roberta,the 588th element of the ﬁrst ln’s β vector is al-ways among the top-2 largest values for all layers —it is largest in the ﬁrst ﬁve layers.
furthermore, the77th element of the second ln’s γ are among thetop-7 largest values from the second to the tenthlayer..it is reasonable to conclude that, after the vectornormalization performed by ln,the outliersobserved in the raw embeddings are lost.
wethese particular neurons arehypothesize thatsomehow important to the network, such that theyretained after scaling the normalized vectors by theafﬁne transformation involving γ and β. indeed,we observe that, in bert, only the 1st position’sembedding has such an outlier.
however, it issubsequently observed in every layer and token.
5314after the ﬁrst ln is applied.
since layernormis trained globally and is not token speciﬁc, ithappens to rescale every vector such that thepositional information is retained.
we corroboratethis by observing that all vectors share the same γ.this effectively guarantees the presence of outliersin the 1st layer, which are then propagated upwardby means of the transformer’s residual connection(he et al., 2015).
also, it is important to note that,in the case of bert, the ﬁrst position’s embeddingis directly tied to the requisite [cls] token, whichis prepended to all sequences as part of the mlmtraining objective.
this has been recently noted toaffect e.g.
attention patterns, where much of theprobability mass is distributed to this particulartoken alone, despite it bearing the smallest normamong all other vectors in a given layer and head(kobayashi et al., 2020)..neuron-level analysisin order to test the extentto which bert and roberta’s outliers are relatedto positional information, we employ a probingtechnique inspired by durrani et al.
(2020).
first,we train a linear probe w ∈ rm ×n without biasto predict the position of a contextualized vectorin a sentence.
in durrani et al.
(2020), the weightsof the classiﬁer are employed as a proxy for select-ing the most relevant neurons to the prediction.
indoing so, they assume that, the larger the absolutevalue of the weight, the more important the corre-sponding neuron.
however, this method disregardsthe magnitudes of the values of neurons, as a largeweights do not necessarily imply that the neuronhas high contribution to the ﬁnal classiﬁcation re-sult.
for example, if the value of a neuron is closeto zero, a large weight also leads to a small contri-bution.
in order to address this issue, we deﬁne thecontribution of the ith neuron as c(i) = abs(wi∗vi)for i = 1, 2, 3, ..., n, where wi is the ith weight andvi is the ith neuron in the contextualized word vec-tor.
we name c = [c(1), c(2), ..., c(n)] as a contri-bution vector.
if a neuron has a high contribution,this means that this neuron is highly relevant to theﬁnal classiﬁcation result..we train, validate, and test our probe on thesplits provided in the sst-2 dataset (as mentionedin §2, we surmise that any dataset would be ade-quate for demonstrating this).
the linear probe isa 768 × 300 matrix, which we train separately foreach layer.
since all sst-2 sentences are shorterthan 300 tokens in length, we set m = 300. we.
use a batch size of 128 and train for 10 epochswith a categorical cross-entropy loss, optimized byadam (kingma and ba, 2017)..figure 5a shows that, while it is possible todecode positional information from the lowestthree layers with almost perfect accuracy, muchof this information is gradually lost higher upin the model.
furthermore, it appears that thehigher layers of roberta contain more positionalinformation than bert.
looking at figure 5b,we see that bert’s outlier neuron has a highercontribution in position prediction than the averagecontribution of all neurons.
we also ﬁnd thatthe contribution values of the same neuron arethe highest in all layers.
combined with theaforementioned pattern of the ﬁrst positionalembedding, we can conclude that the 557th neuronis related to positional information.
likewise,for roberta, figure 5c shows that the 77thand 588th neurons have the highest contributionfor position prediction.
we also ﬁnd that thecontribution values of the 588th neurons are alwayslargest for all layers, which implies that these neu-rons are likewise related to positional information.2.
removing positional embeddingsin order toisolate the relation between outlier neurons andpositional information, we pre-train two roberta-base models (with and without positional embed-dings) from scratch using fairseq (ott et al., 2019).
our pre-training data is the english wikipedia cor-pus3, where we train for 200k steps with a batchsize of 256, optimized by adam.
all models sharethe same hyper-parameters, which are listed in theappendix c.1.
we use four nvidia a100 gpusto pre-train each model, costing about 35 hours permodel..we ﬁnd that, without the help of positional em-beddings, the validation perplexity of roberta-base is very high at 354.0, which is in linewith lee et al.
(2019)’s observation that the self-attention mechanism of transformer encoder isorder-invariant.
in other words, the removal of pesfrom roberta-base makes it a bag-of-word model,whose outputs do not contain any positional infor-mation.
in contrast, the perplexity of robertaequipped with standard positional embeddings ismuch lower at 4.3, which is likewise expected..2we also use heatmaps to show the contribution values in.
appendix b.1..3we randomly select 158.4m sentences for training and.
50k sentences for validation..5315(a) accuracy of position prediction..(b) the contribution value of bert-base’s outlier neuron on position predic-tion..(c) the contribution value of roberta-base’s outlier neurons on position predic-tion..in examining outlier neurons, we employ thesame datasets detailed in §2.
for the roberta-base model with pes, we ﬁnd that the maximumelement of 82.56% of all vectors is the 81st dimen-sion4, similarly to our ﬁndings above.
however, wedo not observe the presence of such outlier neuronsin the roberta-base model without pes, whichindicates that the outlier neurons are tied directlyto positional information.
similar to §2, we displaythe averaged subword vectors for each layer of ourmodels in appendix c.2, which also corroborateour results..4 clipping the outliers.
in §3, we demonstrated that outlier neurons are re-lated to positional information.
in this section, weinvestigate the effects of zeroing out these dimen-sions in contextualized vectors, a process which werefer to as clipping..4.1 vector space geometry.
anisotropy ethayarajh (2019) observe that con-textualized word vectors are anisotropic in all non-input layers, which means that the average cosinesimilarity between uniformly randomly sampledwords is close to 1. to corroborate this ﬁnding, werandomly sample 2000 sentences from the sst-2training set and create 1000 sentence-pairs.
then,we randomly select a token in each sentence, dis-carding all other tokens.
this effectively sets thecorrespondence between the two sentences to twotokens instead.
following this, we compute thecosine similarity between these two tokens to mea-sure the anisotropy of contextualized vectors..in the left plot of figure 6, we can see that con-textualized representations of bert and robertaare more anisotropic in higher layers.
this is espe-.
4different initializations make our models have different.
outlier dimensions..cially true for roberta, where the average cosinesimilarity between random words is larger than 0.5after the ﬁrst non-input layer.
this implies that theinternal representations in bert and robertaoccupy a narrow cone in the vector space..since outlier neurons tend to be valued higheror lower than all other contextualized vectordimensions, we hypothesize that they are the mainculprit behind the degree of observed anisotropy.
to verify our hypothesis, we clip bert androberta’s outliers by setting each neuron’s valueto zero.
the left plot in figure 6 shows that, afterclipping the outliers, their vector spaces becomeclose to isotropic..self-similarity in addition to remarking upon theanisotropic characteristics of contextualized vectorspaces, ethayarajh (2019) introduce several mea-sures to gauge the extent of “contextualization” in-herent models.
one such metric is self-similarity,which the authors employ to compare the similar-ity of a word’s internal representations in differentcontexts.
given a word w and n different sentencess1, s2, ..., sn which contain such word, f il (w) is theinternal representation of w in sentence si in thelth layer.
the average self-similarity of w in the lthlayer is then deﬁned as:.
(cid:80)n.(cid:80)n.i=1.
j=i+1 cos.(cid:16).
l (w), f jf i.
(cid:17)l (w).
selfsiml(w) =.
n(n − 1).
(1)intuitively, a self-similarity score of 1 indicatesthat no contextualization is being performed by themodel (e.g.
static word embeddings), while a scoreof 0 implies that representations for a given wordare maximally different given various contexts..to investigate the effect of outlier neurons on amodel’s self-similarity, we sample 1000 differentwords from sst-2 training set, all of which appearat least in 10 different sentences.
we then com-.
5316figure 6: left: anisotropy measurement of contextualized word vectors in bert and roberta before and afterclipping the outlier dimensions.
right: self-similarity measurement of bert and roberta before and afterclipping..pute the average self-similarity of these words ascontextualized by bert and roberta — beforeand after clipping the outliers.
to adjust for theeffect of anisotropy, we subtract the self-similarityfrom each layer’s anisotropy measurement, as inethayarajh (2019)..the right plot in figure 6 shows that, similarlyto the ﬁndings in (ethayarajh, 2019), a word’s self-similarity is highest in the lower layers, but de-creases in higher layers.
crucially, we also observethat, after clipping the outlier dimensions, the self-similarity increases, indicating that vectors becomecloser to each other in the contextualized space.
this bears some impact on studies attempting tocharacterize the vector spaces of models like bertand roberta, as it is clearly possible to overstatethe degree of “contextualization” without address-ing the effect of positional artefacts..4.2 word sense.
bearing in mind the ﬁndings of the previous sec-tion, we now turn to the question of word sense, ascaptured by contextualized embeddings.
supposethat we have a target word w, which appears in twosentences.
w has the same sense in these two sen-tences, but its contextualized representations arenot identical due to the word appearing in (perhapsslightly) different contexts.
in the previous few sec-tions, we showed that outlier neurons are relatedto positional information and that clipping themcan make a word’s contextualized vectors moresimilar.
here, we hypothesize that clipping suchdimensions can likewise aid in intrinsic semantictasks, like differentiating senses of a word..to test our hypothesis, we analyze contextu-alized vectors using the word-in-context (wic)dataset (pilehvar and camacho-collados, 2019),which is designed to identify the meaning of words.
modelbaselinebefore clippingbertrobertaafter clippingbert-cliproberta-clip.
layer-.
710.
1011.threshold accuracy.
-.
0.70.9.
0.50.6.
50.0%.
67.5%69.0%.
68.4%69.9%.
table 1: the best accuracy scores on wic dataset.
bold indicates that the best result increases after clip-ping..in different contexts.
wic is a binary classiﬁcationtask, where, given a target word and two sentenceswhich contain it, models must determine whetherthe word has the same meaning across the two sen-tences..in order to test how well we can identify differ-ences in word senses using contextualized vectors,we compute the cosine similarity between contex-tualized vectors of target words across pairs of sen-tences, as they appear in the wic dataset.
if thesimilarity value is larger than a speciﬁed threshold,we assign the true label to the sentence pair; other-wise, we assign the false label.
we use this methodto compare the accuracy of bert and robertaon wic before and after clipping the outliers.
sincethis method does not require any training, we testour models on the wic training dataset.5 we com-pare 9 different thresholds from 0.1 to 0.9, as wellas a simple baseline model that assigns the truelabels to all samples..table 1 shows that after clipping outliers, thebest accuracy scores of bert and roberta in-crease about 1%.6 this indicates that these neurons.
5the wic test set does not provide labels and the sizeof validation set is too small (638 sentences pairs).
we thuschoose to use the training dataset (5428 sentences pairs)..6the thresholds are different due to the fact that the cosine.
5317datasetbaselineavg.
glovebefore clippingbertrobertaafter clippingbert-cliproberta-clip.
sts-b.
sick-r.sts-12.
sts-13.
sts-14.
sts-15.
sts-16.
58.02.
53.76.
55.14.
70.66.
59.73.
68.25.
63.66.
58.61(3)56.60(11).
60.78(2)64.68(11).
48.00(1)40.00(1).
61.19(12)58.33(11).
50.10(12)49.79(8).
61.15(1)64.39(9).
62.38(12)64.82(11).
63.06(2)60.61(11).
61.74(2)64.82(11).
50.40(1)43.44(1).
61.44(1)59.72(11).
54.52(2)51.92(3).
67.00(2)66.15(3).
64.18(2)67.14(11).
table 2: experimental results on semantic textual similarity, where the baselines results are published in reimersand gurevych (2019).
we show the best spearman rank correlation between sentence embeddings’ cosine simi-larity and the golden labels.
the results are reported as r × 100. the number in the parenthesis denotes that thisresult belongs to the speciﬁc layer.
bold indicates that the best result increases after clipping..are less related to word sense information and canbe safely clipped for this particular task (if per-formed in an unsupervised fashion)..4.3 sentence embedding.
venturing beyond the word-level, we also hypothe-size that outlier clipping can lead to better sentenceembeddings when relying on the cosine similar-ity metric.
to test this, we follow reimers andgurevych (2019) in evaluating our models on 7semantic textual similarity (sts) datasets, includ-ing the sts-b benchmark (sts-b) (cer et al.,2017), the sick-relatedness (sick-r) dataset(bentivogli et al., 2016) and the sts tasks 2012-2016 (agirre et al., 2012, 2013, 2014, 2015, 2016).
each sentence pair in these datasets is annotatedwith a relatedness score on a 5-point rating scale,as obtained from human judgments.
we load eachdataset using the senteval toolkit (conneau andkiela, 2018)..indeed, the most common approach for com-puting sentence embeddings from contextualizedmodels is simply averaging all subword vectors thatcomprise a given sentence (reimers and gurevych,2019).
we follow this method in obtaining embed-dings for each pair of sentences in the aforemen-tioned tasks, between which we compute the cosinesimilarity.
given a set of similarity and gold relat-edness scores, we then calculate the spearman rankcorrelation.
as a comparison, we also consideraveraged glove embeddings as our baseline..table 2 shows that, after clipping the outliers, thebest spearman rank correlation scores for bertand roberta increase across all datasets, someby a large margin.
this indicates that clipping theoutlier neurons can lead to better sentence embed-dings when mean pooling.
however, like li et al..similarity is inﬂated in the presence of outlier neurons..modelbefore clippingbertrobertaafter clippingbert-cliproberta-clip.
sst-2.
imdb.
sst-5.
85.9%(12)88.4%(8).
86.8%(10)91.5%(9).
46.2%(10)46.9%(7).
85.4%(12)88.7%(8).
86.4%(10)91.6%(9).
46.1%(12)47.0%(7).
table 3: the best accuracy scores on different super-vised tasks.
the number in the parenthesis denotes thatthis result belongs to the speciﬁc layer..(2020b), we also notice that averaged glove em-beddings still manage outperform both bert androberta on all sts 2012-16 tasks.
this impliesthat the post-clipping reduction in anisotropy isonly a partial explanation for why contextualized,mean-pooled sentence embeddings still lag behindstatic word embeddings in capturing the semanticsof a given sentence..4.4 supervised tasks.
in the previous sections, we analyzed the effects ofclipping outlier neurons on various intrinsic seman-tic tasks.
here, we explore the effects of clipping ina supervised scenario, where we hypothesize thata model will learn to discard outlier informationif it is not needed for a given task.
we considertwo binary classiﬁcation tasks, sst-2 and imdb(maas et al., 2011), and a multi-class classiﬁcationtask, sst-5, which is a 5-class version of sst-2.
first, we freeze all the parameters of the pre-trainedmodels and use the same method in §4.3 to get thesentence embedding of each sentence.
then, wetrain a simple linear classiﬁer w ∈ r768×n foreach layer, where n is the number of classes.
weuse different batch sizes for different tasks, 768 forsst-2, 128 for imdb and 1536 for sst-5.
then wetrain for 10 epochs with a categorical cross-entropyloss, optimized by adam..5318table 3 shows that there is little difference inemploying raw vs. clipped vectors in terms of taskperformance.
this indicates that using vectors withclipped outliers does not drastically affect classiﬁeraccuracy when it comes to these common tasks..5 discussion.
the experiments detailed in the previous sectionspoint to the dangers of relying on metrics like co-sine similarity when making observations aboutmodels’ representational spaces.
this is particu-larly salient when the vectors being compared aretaken off-the-shelf and their composition is notwidely understood.
given the presence of modelidiosyncracies like the outliers highlighted here,mean-sensitive, l2 normalized metrics (e.g.
cosinesimilarity or pearson correlation) will inevitablyweigh the comparison of vectors along the highest-valued dimensions.
in the case of positional arte-facts propagating through the bert and robertanetworks, the basis of comparison is inevitablysteered towards whatever information is captured inthose dimensions.
furthermore, since such outliervalues show little variance across vectors, proxymetrics of anisotropy like measuring the averagecosine similarity across random words (detailedin §4.1) will inevitably return an exceedingly highsimilarity, no matter what the context.
when cosinesimilarity is viewed primarily as means of seman-tic comparison between word or sentence vectors,the prospect of calculating cosine similarity fora benchmark like wic or sts-b becomes erro-neous.
though an examination of distance metricsis outside the scope of this study, we acknowledgesimilar points as having been addressed in regardsto static word embeddings (mimno and thomp-son, 2017) as well as contextualized ones (li et al.,2020b).
likewise, we would like to stress that ourmanual clipping operation was performed for il-lustrative purposes and that interested researchersshould employ more systematic post-hoc normal-ization strategies, e.g.
whitening (su et al., 2021),when working with hidden states directly..relatedly, the anisotropic nature of the vectorspace that persists even after clipping the outlierssuggests that positional artefacts are simply part ofthe explanation.
per this point, gao et al.
(2019)prove that, in training any sort of model with likeli-hood loss, the representations learned for tokens be-ing predicted will be naturally be pushed away frommost other tokens in order to achieve a higher like-.
lihood.
they relate this observation to the zipﬁannature of word distributions, where the vast major-ity of words are infrequent.
li et al.
(2020a) extendthis insight speciﬁcally to bert and show that,while high frequency words concentrate densely,low frequency words are much more sparsely dis-tributed.
though we do not attempt to disputethese claims with our ﬁndings, we do hope ourexperiments will highlight the important role thatpositional embeddings play in the representationalgeometry of transformer-based models.
indeed, re-cent work has demonstrated that employing relativepositional embeddings and untying them from thesimultaneously learned word embeddings has leadto impressive gains for bert-based architecturesacross common benchmarks (he et al., 2020; keet al., 2020).
it remains to be seen how such pro-cedures affect the representations of such models,however..beyond this, it is clear that layernorm is thereason positional artefacts propagate though modelrepresentations in the ﬁrst place.
indeed, our exper-iments show that the outlier dimension observed forbert is tied directly to the [cls] token, whichalways occurs at the requisite 1st position —- de-spite having no linguistic bearing on the sequenceof observed tokens being modeled.
however, thefact that roberta (which employs a similar de-limiter) retains outliers originating from differentpositions’ embeddings implies that the issue ofartefact propagation is not simply a relic of taskdesign.
it is possible that whatever positional id-iosyncrasies contribute to a task’s loss are likewiseretained in their respective embeddings.
in the caseof bert, the outlier dimension may be granted alarge negative weight in order to differentiate the(privileged) 1st position between all others.
thisinformation being reconstructed by the layernormparameters, which are shared for all positions in thesequence length, and then propagated up throughthe transformer network is a phenomenon worthyof further attention..6 related work.
in recent years, an explosion of work focused onunderstanding the inner workings of pretrained neu-ral language models has emerged.
one line ofsuch work investigates the self-attention mecha-nism of transformer-based models, aiming to e.g.
characterize its patterns or decode syntactic struc-ture (raganato and tiedemann, 2018; vig, 2019;.
5319mareˇcek and rosa, 2018; voita et al., 2019; clarket al., 2019; kobayashi et al., 2020).
another lineof work analyzes models’ internal representationsusing probes.
these are often linear classiﬁers thattake representations as input and are trained withsupervised tasks in mind, e.g.
pos-tagging, de-pendency parsing (tenney et al., 2019; liu et al.,2019a; lin et al., 2019; hewitt and manning, 2019;zhao et al., 2020).
in such work, high probingaccuracies are often likened to a particular modelhaving “learned” the task in question..most similar to our work, ethayarajh (2019) in-vestigate the extent of “contextualization” in mod-els like bert, elmo, and gpt-2 (radford et al.,2019).
mainly, they demonstrate that the contextu-alized vectors of all words are non-isotropic acrossall models and layers.
however, they do not indi-cate why these models have such properties.
alsorelevant are the studies of dalvi et al.
(2018), whointroduce a neuron-level analysis method, and dur-rani et al.
(2020), who use this method to analyzeindividual neurons in contextualized word vectors.
similarly to our experiment, durrani et al.
(2020)train a linear probe to predict linguistic informationstored in a vector.
they then employ the weightsof the classiﬁer as a proxy to select the most rele-vant neurons to a particular task.
in a similar vein,coenen et al.
(2019) demonstrate the existence ofsyntactic and semantic subspaces in bert repre-sentations..7 conclusion.
in this paper, we called attention to sets of out-lier neurons that appear in bert and roberta’sinternal representations, which bear consistentlylarge values when compared to the distribution ofvalues of all other neurons.
in investigating theorigin of these outliers, we employed a neuron-level analysis method which revealed that they areartefacts derived from positional embeddings andlayer normalization.
furthermore, we found thatoutliers are a major cause for the anisotrophy ofa model’s vector space (ethayarajh, 2019).
clip-ping them, consequently, can make the vector spacemore directionally uniform and increase the similar-ity between words’ contextual representations.
inaddition, we showed that outliers can distort resultswhen investigating word sense within contextual-ized representations as well as obtaining sentenceembeddings via mean pooling, where removingthem leads to uniformly better results.
lastly, we.
ﬁnd that “clipping” does not affect models’ perfor-mance on three supervised tasks..it is important to note that the exact dimensionsat which the outliers occur will vary pending dif-ferent initializations and training procedures (asevidenced by our own roberta model).
as such,future work will aim at investigating strategies formitigating the propagation of these artefacts whenpretraining.
furthermore, given that both bertand roberta are masked language models, it willbe interesting to investigate whether or not similarartefacts occur in e.g.
autoregressive models likegpt-2 (radford et al., 2019) or xlnet (yang et al.,2019).
per the insights of gao et al.
(2019), it isvery likely that the representational spaces of suchmodels are anisotropic, but it is important to gaugethe extent to which this can be traced to positionalartefacts..authors’ note we would like to mention koval-eva et al.
(2021)’s contemporaneous work, whichlikewise draws attention to bert’s outlier neurons.
while our discussion situates outliers in the con-text of positional embeddings and vector spaces,kovaleva et al.
(2021) offer an exhaustive analy-sis of layernorm parameterization and its impacton masked language modeling and ﬁnetuning.
werefer the interested reader to that work for a thor-ough discussion of layernorm’s role in the outlierneuron phenomenon..acknowledgments we would like to thankjoakim nivre and daniel dakota for fruitful dis-cussions and the anonymous reviewers for theirexcellent feedback..references.
eneko agirre, carmen banea, claire cardie, danielcer, mona diab, aitor gonzalez-agirre, weiweiguo, i˜nigo lopez-gazpio, montse maritxalar, radamihalcea, german rigau, larraitz uria, and janycewiebe.
2015. semeval-2015 task 2: semantic tex-tual similarity, english, spanish and pilot on inter-pretability.
in proceedings of the 9th internationalworkshop on semantic evaluation (semeval 2015),pages 252–263, denver, colorado.
association forcomputational linguistics..eneko agirre, carmen banea, claire cardie, danielcer, mona diab, aitor gonzalez-agirre, weiweiguo, rada mihalcea, german rigau, and janycewiebe.
2014. semeval-2014 task 10: multilingualin proceedings of thesemantic textual similarity.
8th international workshop on semantic evaluation.
5320(semeval 2014), pages 81–91, dublin, ireland.
as-sociation for computational linguistics..eneko agirre, carmen banea, daniel cer, mona diab,aitor gonzalez-agirre, rada mihalcea, germanrigau, and janyce wiebe.
2016. semeval-2016task 1: semantic textual similarity, monolingualand cross-lingual evaluation.
in proceedings of the10th international workshop on semantic evalua-tion (semeval-2016), pages 497–511, san diego,california.
association for computational linguis-tics..eneko agirre, daniel cer, mona diab, and aitorgonzalez-agirre.
2012. semeval-2012 task 6: apilot on semantic textual similarity.
in *sem 2012:the first joint conference on lexical and compu-tational semantics – volume 1: proceedings of themain conference and the shared task, and volume2: proceedings of the sixth international workshopon semantic evaluation (semeval 2012), pages 385–393, montr´eal, canada.
association for computa-tional linguistics..eneko agirre, daniel cer, mona diab, aitor gonzalez-agirre, and weiwei guo.
2013.
*sem 2013 sharedin second jointtask: semantic textual similarity.
conference on lexical and computational seman-tics (*sem), volume 1: proceedings of the mainconference and the shared task: semantic textualsimilarity, pages 32–43, atlanta, georgia, usa.
as-sociation for computational linguistics..afra alishahi, yonatan belinkov, grzegorz chrupała,dieuwke hupkes, yuval pinter, and hassan saj-jad, editors.
2020. proceedings of the third black-boxnlp workshop on analyzing and interpretingneural networks for nlp.
association for compu-tational linguistics, online..jimmy lei ba, jamie ryan kiros, and geoffrey e. hin-.
ton.
2016. layer normalization..luisa bentivogli, raffaella bernardi, marco marelli,stefano menini, marco baroni, and roberto zam-parelli.
2016. sick through the semeval glasses: les-son learned from the evaluation of compositionaldistributional semantic models on full sentencesthrough semantic relatedness and textual entailment.
language resources and evaluation, 50..piotr bojanowski, edouard grave, armand joulin, andtomas mikolov.
2017. enriching word vectors withsubword information.
transactions of the associa-tion for computational linguistics, 5:135–146..daniel cer, mona diab, eneko agirre, i˜nigo lopez-gazpio, and lucia specia.
2017. semeval-2017task 1: semantic textual similarity multilingual andin proceedingscrosslingual focused evaluation.
of the 11th international workshop on semanticevaluation (semeval-2017), pages 1–14, vancouver,canada.
association for computational linguistics..kevin clark, urvashi khandelwal, omer levy, andchristopher d. manning.
2019. what does bertin pro-look at?
an analysis of bert’s attention.
ceedings of the 2019 acl workshop blackboxnlp:analyzing and interpreting neural networks fornlp, pages 276–286, florence, italy.
associationfor computational linguistics..andy coenen, emily reif, ann yuan, been kim,adam pearce, fernanda vi´egas, and martin watten-berg.
2019. visualizing and measuring the geometryof bert.
arxiv preprint arxiv:1906.02715..alexis conneau and douwe kiela.
2018. senteval: anevaluation toolkit for universal sentence representa-tions.
in proceedings of the eleventh internationalconference on language resources and evaluation(lrec 2018), miyazaki, japan.
european languageresources association (elra)..fahim dalvi, nadir durrani, hassan sajjad, yonatanbelinkov, anthony bau, and james glass.
2018.what is one grain of sand in the desert?
analyzingindividual neurons in deep nlp models..jacob devlin, ming-wei chang, kenton lee, andkristina toutanova.
2019. bert: pre-training ofdeep bidirectional transformers for language under-in proceedings of the 2019 conferencestanding.
of the north american chapter of the associationfor computational linguistics: human languagetechnologies, volume 1 (long and short papers),pages 4171–4186, minneapolis, minnesota.
associ-ation for computational linguistics..nadir durrani, hassan sajjad, fahim dalvi, andyonatan belinkov.
2020. analyzing individual neu-in proceed-rons in pre-trained language models.
ings of the 2020 conference on empirical methodsin natural language processing (emnlp), pages4865–4880, online.
association for computationallinguistics..kawin ethayarajh.
2019. how contextual are contex-tualized word representations?
comparing the geom-etry of bert, elmo, and gpt-2 embeddings.
inproceedings of the 2019 conference on empiricalmethods in natural language processing and the9th international joint conference on natural lan-guage processing (emnlp-ijcnlp), pages 55–65,hong kong, china.
association for computationallinguistics..jun gao, di he, xu tan, tao qin, liwei wang, and tie-yan liu.
2019. representation degeneration prob-lem in training natural language generation models.
arxiv preprint arxiv:1907.12009..kaiming he, xiangyu zhang, shaoqing ren, and jiansun.
2015. deep residual learning for image recog-nition..pengcheng he, xiaodong liu, jianfeng gao, andweizhu chen.
2020. deberta: decoding-enhancedarxiv preprintbert with disentangled attention.
arxiv:2006.03654..5321john hewitt and christopher d. manning.
2019. astructural probe for ﬁnding syntax in word repre-sentations.
in proceedings of the 2019 conferenceof the north american chapter of the associationfor computational linguistics: human languagetechnologies, volume 1 (long and short papers),pages 4129–4138, minneapolis, minnesota.
associ-ation for computational linguistics..mandar joshi, omer levy, luke zettlemoyer, anddaniel weld.
2019. bert for coreference reso-in proceedings oflution: baselines and analysis.
the 2019 conference on empirical methods in nat-ural language processing and the 9th internationaljoint conference on natural language processing(emnlp-ijcnlp), pages 5803–5808, hong kong,china.
association for computational linguistics..guolin ke, di he, and tie-yan liu.
2020. rethink-ing the positional encoding in language pre-training.
arxiv preprint arxiv:2006.15595..diederik p. kingma and jimmy ba.
2017. adam: a.method for stochastic optimization..nikita kitaev and dan klein.
2018. constituency pars-in proceedingsing with a self-attentive encoder.
of the 56th annual meeting of the association forcomputational linguistics (volume 1: long papers),pages 2676–2686, melbourne, australia.
associa-tion for computational linguistics..devorah e. klein and gregory l. murphy.
2001. thejournal of.
representation of polysemous words.
memory and language, 45(2):259 – 282..goro kobayashi, tatsuki kuribayashi, sho yokoi, andkentaro inui.
2020. attention is not only a weight:analyzing transformers with vector norms.
inproceedings of the 2020 conference on empiricalmethods in natural language processing (emnlp),pages 7057–7075, online.
association for computa-tional linguistics..olga kovaleva, saurabh kulshreshtha, anna rogers,and anna rumshisky.
2021. bert busters: outlierlayernorm dimensions that disrupt bert..juho lee, yoonho lee, jungtaek kim, adam ko-siorek, seungjin choi, and yee whye teh.
2019.set transformer: a framework for attention-basedpermutation-invariant neural networks.
in proceed-ings of the 36th international conference on ma-chine learning, volume 97 of proceedings of ma-chine learning research, pages 3744–3753.
pmlr..bohan li, hao zhou, junxian he, mingxuan wang,yiming yang, and lei li.
2020a.
on the sen-tence embeddings from pre-trained language models.
arxiv preprint arxiv:2011.05864..bohan li, hao zhou, junxian he, mingxuan wang,yiming yang, and lei li.
2020b.
on the sentenceembeddings from pre-trained language models.
inproceedings of the 2020 conference on empiricalmethods in natural language processing (emnlp),.
pages 9119–9130, online.
association for computa-tional linguistics..yongjie lin, yi chern tan, and robert frank.
2019.open sesame: getting inside bert’s linguistic knowl-edge..tal linzen, grzegorz chrupała, and afra alishahi, ed-itors.
2018. proceedings of the 2018 emnlp work-shop blackboxnlp: analyzing and interpreting neu-ral networks for nlp.
association for computa-tional linguistics, brussels, belgium..tal linzen, grzegorz chrupała, yonatan belinkov, anddieuwke hupkes, editors.
2019. proceedings of the2019 acl workshop blackboxnlp: analyzing andinterpreting neural networks for nlp.
associationfor computational linguistics, florence, italy..nelson f. liu, matt gardner, yonatan belinkov,matthew e. peters, and noah a. smith.
2019a.
lin-guistic knowledge and transferability of contextualrepresentations.
in proceedings of the 2019 confer-ence of the north american chapter of the associ-ation for computational linguistics: human lan-guage technologies, volume 1 (long and short pa-pers), pages 1073–1094, minneapolis, minnesota.
association for computational linguistics..yinhan liu, jiatao gu, naman goyal, xian li, sergeyedunov, marjan ghazvininejad, mike lewis, andluke zettlemoyer.
2020. multilingual denoisingpre-training for neural machine translation..yinhan liu, myle ott, naman goyal, jingfei du, man-dar joshi, danqi chen, omer levy, mike lewis,luke zettlemoyer, and veselin stoyanov.
2019b.
roberta: a robustly optimized bert pretraining ap-proach..andrew l. maas, raymond e. daly, peter t. pham,dan huang, andrew y. ng, and christopher potts.
2011. learning word vectors for sentiment analy-sis.
in proceedings of the 49th annual meeting ofthe association for computational linguistics: hu-man language technologies, pages 142–150, port-land, oregon, usa.
association for computationallinguistics..david mareˇcek and rudolf rosa.
2018. extracting syn-tactic trees from transformer encoder self-attentions.
in proceedings of the 2018 emnlp workshop black-boxnlp: analyzing and interpreting neural net-works for nlp, pages 347–349, brussels, belgium.
association for computational linguistics..tomas mikolov, kai chen, greg corrado, and jeffreydean.
2013. efﬁcient estimation of word represen-tations in vector space..david mimno and laure thompson.
2017. the strangegeometry of skip-gram with negative sampling.
inempirical methods in natural language process-ing..5322empirical methods in natural language processing,pages 1631–1642, seattle, washington, usa.
asso-ciation for computational linguistics..jianlin su, jiarun cao, weijie liu, and yangyiwen ou.
2021. whitening sentence representations for bet-ter semantics and faster retrieval.
arxiv preprintarxiv:2103.15316..ian tenney, patrick xia, berlin chen, alex wang,adam poliak, r thomas mccoy, najoung kim,benjamin van durme, sam bowman, dipanjan das,and ellie pavlick.
2019. what do you learn fromcontext?
probing for sentence structure in contextu-in international con-alized word representations.
ference on learning representations..jesse vig.
2019. visualizing attention in transformer-.
based language representation models..elena voita, david talbot, fedor moiseev, rico sen-nrich, and ivan titov.
2019. analyzing multi-headself-attention: specialized heads do the heavy lift-in proceedings of theing, the rest can be pruned.
57th annual meeting of the association for com-putational linguistics, pages 5797–5808, florence,italy.
association for computational linguistics..thomas wolf, lysandre debut, victor sanh, julienchaumond, clement delangue, anthony moi, pier-ric cistac, tim rault, remi louf, morgan funtow-icz, joe davison, sam shleifer, patrick von platen,clara ma, yacine jernite, julien plu, canwen xu,teven le scao, sylvain gugger, mariama drame,quentin lhoest, and alexander rush.
2020. trans-formers: state-of-the-art natural language process-ing.
in proceedings of the 2020 conference on em-pirical methods in natural language processing:system demonstrations, pages 38–45, online.
asso-ciation for computational linguistics..zhilin yang, zihang dai, yiming yang, jaime car-bonell, russ r salakhutdinov, and quoc v le.
2019.xlnet: generalized autoregressive pretraining forin advances in neurallanguage understanding.
information processing systems, volume 32, pages5753–5763.
curran associates, inc..mengjie.
zhao,.
philipp.
yadollahyaghoobzadeh, and hinrich sch¨utze.
2020. quanti-fying the contextualization of word representationswith semantic class probing..dufter,.
myle ott, sergey edunov, alexei baevski, angelafan, sam gross, nathan ng, david grangier, andfairseq: a fast, extensiblemichael auli.
2019.in proceedings oftoolkit for sequence modeling.
the 2019 conference of the north american chap-ter of the association for computational linguistics(demonstrations), pages 48–53, minneapolis, min-nesota.
association for computational linguistics..jeffrey pennington, richard socher, and christophermanning.
2014. glove: global vectors for wordrepresentation.
in proceedings of the 2014 confer-ence on empirical methods in natural languageprocessing (emnlp), pages 1532–1543, doha,qatar.
association for computational linguistics..matthew peters, mark neumann, mohit iyyer, mattgardner, christopher clark, kenton lee, and lukezettlemoyer.
2018. deep contextualized word rep-in proceedings of the 2018 confer-resentations.
ence of the north american chapter of the associ-ation for computational linguistics: human lan-guage technologies, volume 1 (long papers), pages2227–2237, new orleans, louisiana.
associationfor computational linguistics..mohammad taher pilehvar and jose camacho-collados.
2019. wic: the word-in-context datasetfor evaluating context-sensitive meaning represen-in proceedings of the 2019 conferencetations.
of the north american chapter of the associationfor computational linguistics: human languagetechnologies, volume 1 (long and short papers),pages 1267–1273, minneapolis, minnesota.
associ-ation for computational linguistics..alec radford, jeff wu, rewon child, david luan,dario amodei, and ilya sutskever.
2019. languagemodels are unsupervised multitask learners..alessandro raganato and j¨org tiedemann.
2018. ananalysis of encoder representations in transformer-in proceedings of thebased machine translation.
2018 emnlp workshop blackboxnlp: analyzingand interpreting neural networks for nlp, pages287–297, brussels, belgium.
association for com-putational linguistics..nils reimers and iryna gurevych.
2019. sentence-bert: sentence embeddings using siamese bert-networks.
in proceedings of the 2019 conference onempirical methods in natural language processingand the 9th international joint conference on natu-ral language processing (emnlp-ijcnlp), pages3982–3992, hong kong, china.
association forcomputational linguistics..victor sanh, lysandre debut, julien chaumond, andthomas wolf.
2020. distilbert, a distilled version ofbert: smaller, faster, cheaper and lighter..richard socher, alex perelygin, jean wu, jasonchuang, christopher d. manning, andrew ng, andchristopher potts.
2013. recursive deep modelsfor semantic compositionality over a sentiment tree-in proceedings of the 2013 conference onbank..5323figure 7: average vectors for each layer of bert-distil..figure 8: average vectors for each layer of roberta-distil..a outliers of distilled and large models.
for bert-distil, figure 7 shows the patterns ofbert-distil across all layers.
the 557th elementis an outlier.
for roberta-distil, figure 8 showsthe patterns of roberta-distil across all layers.
the 77th and 588th elements are two outliers.
forbert-large, figure 9 shows the patterns of bert-large across all layers.
from the ﬁrst layer to thetenth layer, the 896th element is an outlier.
fromthe tenth layer to the seventeenth layer, the 678thelement is an outlier.
from the sixteenth layer tothe nineteenth layer, the 122nd element is an outlier.
from the nineteenth layer to the twenty-third layer,the 928th element is an outlier.
the ﬁnal layerdoes not have outliers.
for roberta-large, figure.
figure 10: average vectors for each layer of roberta-large..10 shows the patterns of roberta-large acrossall layers.
from the ﬁrst layer to the twenty-thirdlayer, the 673rd element is an outlier.
from theﬁfteenth layer to the ﬁnal layer, the 631st elementis an outlier.
from the ﬁrst layer to the sixth layer,the 981st element is an outlier..b neuron-level analysis.
b.1 heatmaps of base models.
figure 11 and 12 show the heatmaps of the outlierneurons and the highest non-outlier contributionvalues..b.2 distilled and large models.
figure 13 show the accuracy scores of positionprediction of distilled and large models..distil-models figure 14 shows the contributionvalue of distilled models’ outlier neurons onposition prediction..large-models figure 15 shows the contributionvalue of large models’ outlier neurons on positionprediction..c our pre-training models.
c.1 hyper-parameters.
figure 9: average vectors for each layer of bert-large..table 4 shows the hyper-parameters of pre-trainingour roberta-base models..5324figure 11: up: contribution values heatmap of the out-lier neuron of bert-base.
down:the highest non-outlier contribution value of bert-base..figure 13: up: accuracy of position prediction of dis-tilled models.
down: accuracy of position predictionof large models..figure 12: up: contribution values heatmap of the 77thdimension of roberta-base.
mid: contribution val-ues heatmap of the 588th dimension of roberta-base.
down:the highest non-outlier contribution value ofroberta-base..figure 14: the contribution value of distilled models’outlier neurons on position prediction..5325figure 15: the contribution value of large models out-lier neurons on position prediction..figure 16: average vectors for each layer of ourroberta-base w/ or w/o pe..hyper-parameternumber of layershidden sizefnn inner hidden sizeattention headsattention head sizedropoutwarmup stepsmax stepslearning ratesbatch sizeweight decaylearning rate decayadam ((cid:15), β1, β2)gradient clipping.
our roberta-base12768307212640.110k200k1e-42560.01polynomial(1e-6, 0.9, 0.98)0.5.table 4: hyper-parametersroberta-base models..for pre-training our.
c.2 average subword vectors.
figure 16 show the average vectors for each of ourmodels..d clipping the outliers.
d.1 geometry of vector space.
distil-models figure 17 shows the anisotropicmeasurement of distilled models and the self-similarity measurement of distilled models..large-models figure 18 shows the anisotropicmeasurement of large models and figure 19 showsthe self-similarity measurement of large models.
we “clip” different outlier neurons in different lay-ers.
for bert-large, we zero-out the 896th neuronfrom the ﬁrst layer to the tenth layer, the 678thneuron from the tenth layer to the seventeenthlayer, the 122nd neuron from the sixteenth layerto the nineteenth layer and the 928th neuron fromthe nineteenth layer to the twenty-third layer.
forroberta-large, we zero-out the 673rd neuron forall non-input layers, the 981st neuron for the ﬁrst 9layers and the 631st neuron for the last 10 layers..5326modelbaselinebefore clippingbert-distilroberta-distilbert-largeroberta-largeafter clippingbert-distil-cliproberta-distil-clipbert-large-cliproberta-large-clip.
551210.
651216.threshold acc..layer-.
-.
0.90.90.70.9.
0.60.60.60.6.
50.0%.
66.5%63.7%70.2%70.4%.
67.3%66.7%70.3%71.3%.
table 5: the best accuracy scores on wic dataset fordistilled and large models.
bold indicates that the bestresult increases after clipping..dataset.
sts-bsick-rsts-12sts-13sts-14sts-15sts-16.
bertdistil.
59.65(6)62.64(6)42.96(1)59.33(1)53.81(6)61.40(6)61.43(6).
robertadistil.
56.06(5)62.63(5)40.19(1)56.42(5)49.59(6)65.10(5)62.90(5).
bertdistilclip56.62(6)62.42(6)46.47(1)55.74(1)50.57(1)61.48(1)60.75(6).
robertadistilclip58.47(5)62.73(6)42.36(1)60.64(6)52.51(2)65.93(2)64.49(5).
table 6: experimental results on semantic textual simi-larity of distilled models.
the number in the parenthe-sis denotes that this result belongs to the speciﬁc layer.
bold indicates that the best result increases after clip-ping..d.2 word sense.
table 5 shows the accuracy scores of distill-modelsand large-models on wic dataset before and after“clipping the outliers”..d.3 sentence embedding.
table 6 shows the results on semantic textual sim-ilarity tasks of distilled models before and after“clipping the outliers”..table 7 shows the results on semantic textualsimilarity tasks of large models before and after“clipping the outliers”..dataset.
sts-bsick-rsts-12sts-13sts-14sts-15sts-16.
bertlarge.
62.56(1)64.47(24)54.05(1)68.80(2)60.46(1)73.91(1)66.35(17).
robertalarge.
59.71(19)63.08(14)44.72(1)61.68(8)51.39(8)65.98(7)66.50(14).
bertlargeclip66.43(3)65.72(23)56.44(3)71.07(2)63.35(1)76.51(1)71.41(3).
robertalargeclip62.01(23)63.50(16)49.69(1)62.82(10)57.33(1)69.71(1)68.25(11).
table 7: experimental results on semantic textual sim-ilarity of large models.
the number in the parenthe-sis denotes that this result belongs to the speciﬁc layer.
bold indicates that the best result increases after clip-ping..figure 17: up: average cosine similarity between ran-dom words of distil-models.
down: self-similaritymeasurement of bert-distil and roberta-distil (ad-justed by anisotropy) before and after “clipping the out-liers”..figure 18: average cosine similarity between randomwords of large-models..figure 19: self-similarity measurement of bert-largeand roberta-large (adjusted by anisotropy) beforeand after “clipping the outliers”..5327