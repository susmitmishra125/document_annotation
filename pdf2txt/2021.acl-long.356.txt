twag: a topic-guided wikipedia abstract generator.
fangwei zhu1,2, shangqing tu3, jiaxin shi1,2, juanzi li1,2, lei hou1,2∗ and tong cui41dept.
of computer sci.&tech., bnrist, tsinghua university, beijing 100084, china2kirc, institute for artiﬁcial intelligence, tsinghua university3school of computer science and engineering, beihang university4noah’s ark lab, huawei inc.{zfw19@mails.,shijx16@mails,lijuanzi@,houlei@}tsinghua.edu.cntsq@buaa.edu.cn,cuitong5@huawei.com.
abstract.
wikipedia abstract generation aims to distilla wikipedia abstract from web sources andhas met signiﬁcant success by adopting multi-document summarization techniques.
how-ever, previous works generally view the ab-stract as plain text, ignoring the fact that itis a description of a certain entity and can bedecomposed into different topics.
in this pa-per, we propose a two-stage model twag thatguides the abstract generation with topical in-formation.
first, we detect the topic of eachinput paragraph with a classiﬁer trained on ex-isting wikipedia articles to divide input docu-ments into different topics.
then, we predictthe topic distribution of each abstract sentence,and decode the sentence from topic-awarerepresentations with a pointer-generator net-work.
we evaluate our model on the wi-kicatsum dataset, and the results show thattwag outperforms various existing baselinesand is capable of generating comprehensive ab-stracts.
our code and dataset can be accessedat https://github.com/thu-keg/twag.
1.introduction.
wikipedia, one of the most popular crowd-sourcedonline knowledge bases, has been widely used asthe valuable resources in natural language process-ing tasks such as knowledge acquisition (lehmannet al., 2015) and question answering (hewlett et al.,2016; rajpurkar et al., 2016) due to its high qual-ity and wide coverage.
within a wikipedia article,its abstract is the overview of the whole content,and thus becomes the most frequently used part invarious tasks.
however, the abstract is often con-tributed by experts, which is labor-intensive andprone to be incomplete..in this paper, we aim to automatically generatewikipedia abstracts based on the related documents.
∗ corresponding author.
collected from referred websites or search engines,which is essentially a multi-document summariza-tion problem.
this problem is studied in both ex-tractive and abstractive manners..the extractive models attempt to select relevanttextual units from input documents and combinethem into a summary.
graph-based representationsare widely exploited to capture the most salient tex-tual units and enhance the quality of the ﬁnal sum-mary (erkan and radev, 2004; mihalcea and tarau,2004; wan, 2008).
recently, there also emergeneural extractive models (yasunaga et al., 2017;yin et al., 2019) utilizing the graph convolutionalnetwork (kipf and welling, 2017) to better captureinter-document relations.
however, these modelsare not suitable for wikipedia abstract generation.
the reason is that the input documents collectedfrom various sources are often noisy and lack intrin-sic relations (sauper and barzilay, 2009), whichmakes the relation graph hard to build..the abstractive models aim to distill an informa-tive and coherent summary via sentence-fusion andparaphrasing (filippova and strube, 2008; baner-jee et al., 2015; bing et al., 2015), but achieve littlesuccess due to the limited scale of datasets.
liuet al.
(2018) proposes an extractive-then-abstractivemodel and contributes wikisum, a large-scaledataset for wikipedia abstract generation, inspiringa branch of further studies (perez-beltrachini et al.,2019; liu and lapata, 2019; li et al., 2020)..the above models generally view the abstractas plain text, ignoring the fact that wikipedia ab-stracts describe certain entities, and the structureof wikipedia articles could help generate compre-hensive abstracts.
we observe that humans tendto describe entities in a certain domain from sev-eral topics when writing wikipedia abstracts.
asillustrated in figure 1, the abstract of the arcticfox contains its adaption, biology taxonomy andgeographical distribution, which is consistent with.
proceedingsofthe59thannualmeetingoftheassociationforcomputationallinguisticsandthe11thinternationaljointconferenceonnaturallanguageprocessing,pages4623–4635august1–6,2021.©2021associationforcomputationallinguistics46232 related work.
5 distribution and habitat.
5.1 migrations and travel.
2.1 multi-document summarization.
abstract.
content table.
and.
hemisphere.
the arctic fox (vulpes lagopus), also knownas the white fox, polar fox, or snow fox, is asmall fox native to the arctic regions of thenortherncommonthroughout the arctic tundra biome.
it is welladapted to living in cold environments, andis best known for its thick, warm fur that isalso used as camouflage.
it has a large andvery fluffy tail.
in the wild, most individualsdo not live past their first year but someexceptional ones survive up to 11 years.
itsbody length ranges from 46 to 68 cm (18 to27 in), with a generally rounded body shapeto minimize the escape of body heat..2 adaptations.
2.1 sensory modalities2.2 physiology.
3 size.
4 taxonomy4.1 origins4.2 subspecies.
figure 1: an example of wikipedia article arctic fox.
the abstract contains three orthogonal topics about ananimal: description, (cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)taxonomy and distribution.
theright half is part of the article’s content table, showingsection labels related to different topics..the content table.
therefore, given an entity ina speciﬁc domain, generating abstracts from cor-responding topics would reduce redundancy andproduce a more complete summary..in this paper, we try to utilize the topical infor-mation of entities within its domain (wikipedia cat-egories) to improve the quality of the generated ab-stract.
we propose a novel two-stage topic-guidedwikipedia abstract generation model (twag).
twag ﬁrst divides input documents by paragraphand assigns a topic for each paragraph with aclassiﬁer-based topic detector.
then, it generatesthe abstract in a sentence-wise manner, i.e., pre-dicts the topic distribution of each abstract sen-tence to determine its topic-aware representation,and decodes the sentence with a pointer-generatornetwork (see et al., 2017)..we evaluate twag on the wikicatsum (perez-beltrachini et al., 2019) dataset, a subset of thewikisum containing three distinct domains.
ex-perimental results show that it signiﬁcantly im-proves the quality of abstract compared with sev-eral strong baselines..in conclusion, the contributions of our work are.
as follows:.
• we propose twag, a two-stage neural ab-stractive wikipedia abstract generation modelutilizing the topic information in wikipedia,which is capable of generating comprehensiveabstracts..• we simulate the way humans recognize en-tities, using a classiﬁer to divide input doc-uments into topics, and then perform topic-.
aware abstract generation upon the predictedtopic distribution of each abstract sentence..• our experiment results against 4 distinct base-.
lines prove the effectiveness of twag..multi-document summarization is a classic andchallenging problem in natural language process-ing, which aims to distill an informative and co-herent summary from a set of input documents.
compared with single-document summarization,the input documents may contain redundant or evencontradictory information (radev, 2000)..early high-quality multi-document summariza-tion datasets are annotated by humans, e.g.,datasets for document understanding conference(duc) and text analysis conference (tac).
thesedatasets are too small to build neural models,and most of the early works take an extractivemethod, attempting to build graphs with inter-paragraph relations and choose the most salienttextual units.
the graph could be built with var-ious information, e.g., tf-idf similarity (erkanand radev, 2004), discourse relation (mihalceaand tarau, 2004), document-sentence two-layer re-lations (wan, 2008), multi-modal (wan and xiao,2009) and query information (cai and li, 2012).
recently, there emerge attempts to incorporate neu-ral models, e.g., yasunaga et al.
(2017) builds adiscourse graph and represents textual units uponthe graph convolutional network (gcn) (kipf andwelling, 2017), and yin et al.
(2019) adopts theentity linking technique to capture global depen-dencies between sentences and ranks the sentenceswith a neural graph-based model..in contrast, early abstractive models usingsentence-fusion and paraphrasing (filippova andstrube, 2008; banerjee et al., 2015; bing et al.,inspired by the re-2015) achieve less success.
cent success of single-document abstractive mod-els (see et al., 2017; paulus et al., 2018; gehrmannet al., 2018; huang et al., 2020), some works (liuet al., 2018; zhang et al., 2018) try to transfersingle-document models to multi-document set-tings to alleviate the limitations of small-scaledatasets.
speciﬁcally, liu et al.
(2018) deﬁneswikipedia generation problem and contributes thelarge-scale wikisum dataset.
fabbri et al.
(2019)constructs a middle-scale dataset named multi-.
4624news and proposes an extractive-then-abstractivemodel by appending a sequence-to-sequence modelafter the extractive step.
li et al.
(2020) modelsinter-document relations with explicit graph repre-sentations, and incorporates pre-trained languagemodels to better handle long input documents..of size n as input, and outputs a wikipedia abstracts = (s1, s2, .
.
.
, sm) with m sentences.
the goalis to ﬁnd an optimal abstract s ∗ that best concludesthe input, i.e.,.
s ∗ = arg max.
p (s|d).
s.(1).
2.2 wikipedia-related text generation.
sauper and barzilay (2009) is the ﬁrst work focus-ing on wikipedia generation, which uses integerlinear programming (ilp) to select the useful sen-tences for wikipedia abstracts.
banerjee and mitra(2016) further evaluates the coherence of selectedsentences to improve the linguistic quality..liu et al.
(2018) proposes a two-stage extractive-then-abstractive model, which ﬁrst picks para-graphs according to tf-idf weights from websources, then generates the summary with a trans-former model by viewing the input as a long ﬂatsequence.
inspired by this work, perez-beltrachiniet al.
(2019) uses a convolutional encoder and ahierarchical decoder, and utilizes the latent dirich-let allocation model (lda) to render the decodertopic-aware.
hiersumm (liu and lapata, 2019)adopts a learning-based model for the extractivestage, and computes the attention between para-graphs to model the dependencies across multipleparagraphs.
however, these works view wikipediaabstracts as plain text and do not explore the under-lying topical information in wikipedia articles..there are also works that focus on generatingother aspects of wikipedia text.
biadsy et al.
(2008) utilizes the key-value pairs in wikipediainfoboxes to generate high-quality biographies.
hayashi et al.
(2021) investigates the structure ofwikipedia and builds an aspect-based summariza-tion dataset by manually labeling aspects and iden-tifying the aspect of input paragraphs with a ﬁne-tuned roberta model (liu et al., 2019).
ourmodel also utilizes the structure of wikipedia, butwe generate the compact abstract rather than indi-vidual aspects, which requires the fusion of aspectsand poses a greater challenge to understand theconnection and difference among topics..3 problem deﬁnition.
deﬁnition 1 wikipedia abstract generation ac-cepts a set of paragraphs1 d = {d1, d2, .
.
.
, dn}.
1the input documents can be represented by textual unitswith different granularity, and we choose paragraph as it nor-mally expresses relatively complete and compact semantics..previous works generally view s as plain text, ig-noring the semantics in wikipedia articles.
beforeintroducing our idea, let’s review how wikipediaorganizes articles..wikipedia employs a hierarchical open categorysystem to organize millions of articles, and wename the top-level category as domain.
as for awikipedia article, we concern three parts, i.e., theabstract, the content table, and textual contents.
note that the content table is composed of severalsection labels {l}, pairing with corresponding tex-tual contents {p}.
as illustrated in figure 1, thecontent table indicates different aspects (we callthem topics) of the article, and the abstract seman-tically corresponds to these topics, telling us thattopics could beneﬁt the abstract generation..however, general domains like person or animalconsist millions of articles with diverse content ta-bles, making it not feasible to simply treat sectionlabels as topics.
considering that articles in spe-ciﬁc domains often share several salient topics, wemanually merge similar section labels to convertthe sections titles to a set of topics.
formally, the} oftopic set is denoted as t = {t1, t2, ..., tnt}.
, .
.
.
, lmsize nt, where each topic ti = {l1ii., l2i.now, our task can be expressed with a topical.
objective, i.e.,deﬁnition 2 given the input paragraphs d, weintroduce the latent topics z = {z1, z2, .
.
.
, zn},where zi ∈ t is the topic of i-th input paragraph di,and our objective of wikipedia abstract generationis re-written as.
s ∗ = arg max.
p (z|d) arg max.
p (s|d, z)..(2).
z.s.therefore, the abstract generation could be com-pleted with two sub-tasks, i.e., topic detection to op-timize arg maxz p (z|d) and topic-aware abstractgeneration to optimize arg maxs p (s|d, z)..4 the proposed method.
as shown in figure 2, our proposed twag adoptsa two-stage structure.
first, we train a topic detec-tor based on existing wikipedia articles to predictthe topic of input paragraphs.
second, we group the.
4625(cid:1853)(cid:1870)(cid:1859)(cid:1865)(cid:1853)(cid:1876)(cid:2292)(cid:1842)(cid:4666)(cid:2292)(cid:513)(cid:2270)(cid:4667).
(cid:1853)(cid:1870)(cid:1859)(cid:1865)(cid:1853)(cid:1876)(cid:2285)(cid:1842)(cid:4666)(cid:2285)(cid:513)(cid:2270)(cid:481) (cid:2292)(cid:4667).
1.
2.
0.
······.
0.
2.
(cid:1833)(cid:2868) (cid:3404) (cid:4668)(cid:1856)(cid:2871)(cid:481) (cid:485) (cid:1856)(cid:3041)(cid:2879)(cid:2869)(cid:4669)with a broader contour , strong keel and dextral coil ….
(cid:1833)(cid:2869) (cid:3404) (cid:4668)(cid:1856)(cid:2869)(cid:481) (cid:485) (cid:4669)arkive species - partulina snail ( partulina semicarinata ) ….
(cid:1833)(cid:2870) (cid:3404) (cid:4668)(cid:1856)(cid:2870)(cid:481) (cid:485) (cid:481) (cid:1856)(cid:3041)(cid:4669)endemic to the remaining wet forests on the hawaiian island of lanai ….
topic encoder.
(cid:2189)(cid:2868).
(cid:2189)(cid:2869).
fc+softmax.
(step (cid:1872)-(cid:883)).
topic predictor (step (cid:1872)).
(cid:2187)(cid:3047)(cid:2879)(cid:2869)(cid:2190)(cid:3047)(cid:2879)(cid:2869).
gru.
(cid:2190)(cid:3047).
(cid:2199)(cid:3047).
topicselector.
(cid:2187)(cid:3047).
(cid:2189)(cid:2870).
(step (cid:1872)+(cid:883)).
(cid:2187)(cid:3047)(cid:2878)(cid:2869)(cid:2190)(cid:3047)(cid:2878)(cid:2869).
argmax.
······.
······.
albert.
(cid:1856)(cid:2869).
(cid:1856)(cid:2870).
(cid:1856)(cid:2871).
······.
(cid:1856)(cid:3041)(cid:2879)(cid:2869).
(cid:1856)(cid:3041).
(cid:1871)(cid:3047)(cid:2879)(cid:2869).
(cid:1871)(cid:3047) :.
······.
(cid:1871)(cid:3047)(cid:2878)(cid:2869).
sentence decoder.
(cid:2200)(cid:3047).
<start>.
this.
species.
is.
endemic.
to.
hawaii.
figure 2: the twag framework.
we use an example domain with 3 topics for illustration.
the left half is thetopic detector which attempts to ﬁnd a topic for each input paragraph, and the right half is the topic-aware abstractgenerator to generate the abstract by sentence based on input paragraphs and their predicted topics..input paragraphs by detected topics to encode themseparately, and generate the abstract in a sentence-wise manner.
in each step, we predict the topicdistribution of the current sentence, fuse it with theglobal hidden state to get the topic-aware represen-tation, and generate the sentence with a copy-baseddecoder.
next, we will detail each module..the abstract.
speciﬁcally, it contains three modules:a topic encoder to encode the input paragraphs intotopical representations, a topic predictor to predictthe topic distribution of abstract sentences and gen-erate the topic-aware sentence representation, anda sentence decoder to generate abstract sentencesbased on the topic-aware representations..4.1 topic detection.
the topic detector aims to annotate input para-graphs with their optimal corresponding top-ics.
to formalize, given the input paragraphsd, det returns its corresponding topics z ={z1, z2, .
.
.
, zn}, i.e.,.
z = det(d).
(3).
we view topic detection as a classiﬁcation prob-lem.
for each paragraph d ∈ d, we encode itwith albert(lan et al., 2019) and then predictits topic z with a fully-connected layer, i.e.,.
4.2.1 topic encodergiven the input paragraphs d and the detected top-ics z, we concatenate all paragraphs belongingto the same topic tk to form a topic-speciﬁc textgroup (ttg) gk, which contains salient informa-tion about a certain topic of an entity:.
gk = concat({di|zi = tk})..(6).
to further capture hidden semantics, we use a.bidirectional gru to encode the ttgs:.
gk, uk = bigru(gk)..(7).
d = albert(d)z = arg max(linear(d)).
(4).
(5).
gk is the ﬁnal hidden state of the gk, and uk =(u1, u2, .
.
.
, ungk) represents the hidden state ofeach token in gk, where ngk denotes the numberof tokens in gk..where d is the vector representation of d, and weﬁne-tuned the albert model on a pretrained ver-sion..4.2 topic-aware abstract generation.
topic-aware abstract generator utilizes the inputparagraphs d and the detected topics z to generate.
4.2.2 topic predictorafter encoding the topics into hidden states, twagtackles the decoding process in a sentence-wisemanner:.
arg maxs.p (s|d, z) =.
arg maxsi.
p (si|d, z, s<i) (8).
m(cid:2).
i=1.
4626to generate the abstract s, we ﬁrst predict thetopic distribution of every sentence si with a grudecoder.
at each time step t, the topic predictorproduces a global hidden state ht, and then esti-mates the probability distribution qt over topics..ht = gru(ht−1, et−1)qt = softmax(linear(ht)).
(9).
(10).
where et−1 denotes the topical information in thelast step.
e0 is initialized as an all-zero vector, andet could be derived from qt in two ways..the ﬁrst way named hard topic, is to directlyselect the topic with the highest probability, andtake its corresponding representation, i.e.,.
ehardt.= garg maxi(qi)..(11).
the second way named soft topic, is to view everysentence as a mixture of different topics, and takethe weighted sum over topic representations, i.e.,.
for the k-th token, the decoder computes an atten-tion distribution ak over tokens in the input para-graphs, where each element aik could be viewed asthe probability of the i-th token being selected,.
aik = softmax(tanh(wuui +wssk + ba)) (16).
where sk denotes the decoder hidden state withs0 = rt to incorporate the topic-aware representa-tion, and wu, ws, ba are trainable parameters..to generate a token from the vocabulary, weﬁrst use the attention mechanism to calculate theweighted sum of encoder hidden states, known asthe context vector,.
c∗k =.
aikui..(cid:2).
i.
(17).
which is further fed into a two-layer network toobtain the probability distribution over vocabulary,.
esof tt = qt · g.(12).
pvoc = softmax(linear(linear([sk, c∗.
k])))..(18).
where g = (g1, g2, .
.
.
, gnt) is the matrix oftopic representations.
with the observation thatwikipedia abstract sentences normally containmixed topics, we choose the soft topic mechanismfor our model (see section 5.3 for details)..finally, we compute the topic-aware hidden statert by adding up ht and et, which serves as theinitial hidden state of sentence decoder:.
rt = ht + et.
(13).
to switch between these two mechanisms, pgenk, decoder hid-.
is computed from context vector c∗den state sk and decoder input xk:.
pgen = σ(wt.
c c∗.
k + wt.
s sk + wt.
x xk + bp) (19).
where σ represents the sigmoid function andwtx and bp are trainable parameters.
cthe ﬁnal probability distribution of words is2.
, wts., wt.
p (w) = pgenpvoc(w) + (1 − pgen).
aik.(20).
(cid:3).
i:|wwi=w.
additionally, a stop conﬁrmation is executed at.
each time step:.
4.3 training.
pstop = σ(linear(ht)).
(14).
the modules for topic detection and abstract gener-ation are trained separately..where σ represents the sigmoid function.
if pstop >0.5, twag will terminate the decoding processand no more abstract sentences will be generated..4.2.3 sentence decoderour sentence decoder adopts the pointer-generatornetwork (see et al., 2017), which picks tokens bothfrom input paragraphs and vocabulary..to copy a token from the input paragraphs, thedecoder requires the token-wise hidden states u =(u1, u2, .
.
.
, unu) of all nu input tokens, which isobtained by concatenating the token-wise hiddenstates of all ttgs, i.e.,.
4.3.1 topic detector trainingsince there are no public benchmarks for assigninginput paragraphs with wikipedia topics, we con-struct the dataset with existing wikipedia articles.
in each domain, we collect all the label-contentpairs {(l, p)} (deﬁned in section 3), and split thecontent into paragraphs p = (d1, d2, .
.
.
, dnp) toform a set of label-paragraph pairs {(l, d)}.
after-wards, we choose all pairs (l, d) whose sectionlabel l belongs to a particular topic t ∈ t tocomplete the dataset construction, i.e., the topic-paragraph set {(t, d)}.
besides, a noise topic is.
u = [u1, u2, .
.
.
, unu].
(15).
2wwi means the token corresponding to ui..4627set up in each domain, which refers to meaning-less text like scripts and advertisements, and thecorresponding paragraphs are obtained by utilizingregular expressions to match obvious noisy texts.
the details are reported in appendix a..note that the dataset for abstract generation iscollected from non-wikipedia websites (refer tosection 5 for details).
these two datasets are in-dependent of each other, which prevents potentialdata leakage..in the training step, we use the negative log-.
likelihood loss to optimize the topic detector..4.3.2 abstract generator trainingthe loss of topic-aware abstract generation stepconsists of two parts: the ﬁrst part is the averageloss of sentence decoder for each abstract sentencelsent, and the second part is the cross-entropy lossof stop conﬁrmation lstop..following (see et al., 2017), we compute theloss of an abstract sentence by averaging the neg-ative log likelihood of every target word in thatsentence, and achieve lsent via averaging over allm sentences,.
lsent =.
1m.(cid:3).
m(cid:2).
nst(cid:2).
1nst.
t=1.
i=1.
(cid:4).
− log p (wi).
(21).
where nst is the length of the t-th sentence of theabstract.
as for lstop, we adopt the cross-entropyloss, i.e.,.
lstop = −ys log(pstop) − (1 − ys) log(1 − pstop).
(22).
where ys = 1 when t > m and ys = 0 otherwise..5 experiments.
5.1 experimental settings.
dataset.
to evaluate the overall performance ofour model, we use the wikicatsum dataset pro-posed by (perez-beltrachini et al., 2019), whichcontains three distinct domains (company, filmand animal) in wikipedia.
each domain is splitinto train (90%), validation (5%) and test (5%) set.
we build the dataset for training and evaluat-ing the topic detector from the 2019-07-01 englishwikipedia full dump.
for each record in the wiki-catsum dataset, we ﬁnd the article with the sametitle in wikipedia dump, and pick all section label-content pairs {(l, p)} in that article.
we removeall hyperlinks and graphics in contents, split thecontents into paragraphs with the spacy library,.
and follow the steps in section 4.3.1 to completedataset construction.
finally, we conduct an 8:1:1split for train, validation and test..table 1 presents the detailed parameters of used.
datasets..evaluation metrics.
we evaluate the perfor-mance of our model with rouge scores (lin,2004), which is a common metric in comparinggenerated and standard summaries.
consideringthat we do not constrain the length of generated ab-stracts, we choose rouge f1 score that combinesprecision and recall to eliminate the tendency offavoring long or short results..implementation details.
we use the open-source pytorch and transformers library to im-plement our model.
all models are trained onnvidia geforce rtx 2080..in topic detection, we choose the top 20 frequentsection labels in each domain and manually groupthem into different topics (refer to the appendixa for details).
for training, we use the pretrainedalbert-base-v2 model in the transformers library,keep its default parameters and train the modulefor 4 epochs with a learning rate of 3e-5..for abstract generation, we use a single-layerbigru network to encode the ttgs into hiddenstates of 512 dimensions.
the ﬁrst 400 tokensof input paragraphs are retained and transformedinto glove (pennington et al., 2014) embedding of300 dimensions.
the vocabulary size is 50000 andout-of-vocabulary tokens are represented with theaverage embedding of its adjacent 10 tokens.
thismodule is trained for 10 epochs, the learning rateis 1e-4 for the ﬁrst epoch and 1e-5 for the rest..before evaluation, we remove sentences thathave an overlap of over 50% with other sentencesto reduce redundancy..baselines.
we compare our proposed twagwith the following strong baselines:.
• tf-s2s (liu et al., 2018) uses a transformerdecoder and compresses key-value pairs inself-attention with a convolutional layer..• cv-s2d+t (perez-beltrachini et al., 2019)uses a convolutional encoder and a two-layerhierarchical decoder, and introduces lda tomodel topical information..• hiersumm (liu and lapata, 2019) utilizesinter-.
the attention mechanism to model.
4628domain.
#examples r1-r r2-r rl-r.#topics.
train.
valid.
test.
companyfilmanimal.
62,54559,97360,816.
.551.559.541.
.217.243.208.
.438.456.455.
454.
35,506187,22151,009.
1,99910,8012,897.
2,21210,0852,876.table 1: details about used datasets.
the left half shows parameters about the wikicatsum dataset: number ofexamples and rouge 1, 2, l recalls.
the right half shows parameters about the dataset for topic detector: numberof topics and number of topic-paragraph pairs in each split..companyr2.
rl.
r1.
rl.
r1.
animalr2.
model.
tf-s2scv-s2d+thiersummbarttwag (ours).
r1.
.197.275.133.310.341.
.023.106.028.116.119.
.125.214.107.244.316.
.198.380.246.375.408.filmr2.
.065.212.126.199.212.
.172.323.185.325.343.
.252.427.165.376.431.
.099.279.069.226.244.rl.
.210.379.134.335.409.table 2: rouge f1 scores of different models..paragraph relations and then enhances the doc-ument representation with graphs..ﬁtting due to the dataset scale.
this phenomenonalso proves that twag is data-efﬁcient..• bart (lewis et al., 2020) is a pretrainedsequence-to-sequence model that achievedsuccess on various sequence prediction tasks..we ﬁne-tune the pretrained bart-base modelon our dataset and set beam size to 5 for all modelsusing beam search at test time.
the parameters weuse for training and evaluation are identical to thesein corresponding papers..5.2 results and analysis.
table 2 shows the rouge f1 scores of differentmodels.
in all three domains, twag outperformsother baselines.
our model surpasses other mod-els on rouge-1 score by a margin of about 10%,while still retaining advantage on rouge-2 androuge-l scores.
in domain company, our modelboosts the rouge-l f1 score by about 30%, con-sidering that rouge-l score is computed upon thelongest common sequence, the highest rouge-lscore indicates that abstracts generated by twaghave the highest holistic quality..while cvs2d+t and bart retain reasonablescores, tf-s2s and hiersumm do not reach thescores they claim in their papers.
notice that thewikicatsum dataset is a subset of wikisum, whichis used as the training dataset of these two models,we infer that tf-s2s and hiersumm require moretraining data to converge, and suffer from under-.
5.3 ablation study.
learning rate of topic detector.
we tried twolearning rates when training the topic detector mod-ule.
a learning rate of 1e-7 would result in a preci-sion of 0.922 in evaluation, while a learning rate of3e-5 would result in a precision of 0.778. however,choosing the former learning rate causes a dropof about 10% in all rouge scores, which is thereason why we use the latter one in our full model.
we infer that human authors occasionally makemistakes, assigning paragraphs into section labelsthat belong to other topics.
a topic detector withlow learning rate overﬁts these mistakes, harmingthe overall performance of our model..soft or hard topic.
to further investigate theeffectiveness of twag’s soft topic mechanism,we compare the results of soft and hard topic andreport them in table 4, from which we can see thathard topic does quite poorly in this task..topic detector.
hard topicr2.
r1.
rl.
soft topicr2.
r1.
companyfilmanimal.
.266.355.407.
.074.159.223.
.245.333.387.
.341.408.431.
.119.212.244.rl.
.316.343.409.table 4: rouge f1 scores of different topic selectors..a possible reason is that some sentences in the.
4629gold abstract: manjina there ( kannada : (cid:2)unk(cid:3) (cid:2)unk(cid:3) ) is a 1980 indian kannada ﬁlm, directed by bangalore nageshand produced by k. r. narayana murthy.
the ﬁlm stars srinath, manjula, thoogudeepa srinivas and dinesh in leadroles.
the ﬁlm had musical score by upendra kumar.
tf-s2s: chalo ishq larain is a pakistani ﬁlm directed by sajjad gul and was released across pakistan in april 2002starring meera , ali haider and zara sheikh.
cv-s2d+t: (cid:2)unk(cid:3) (cid:2)unk(cid:3) ( kannada : (cid:2)unk(cid:3) (cid:2)unk(cid:3) ) is a 1967 indian kannada ﬁlm, directed by b. mallesh andproduced by b. s. ranga.
the ﬁlm stars rajkumar, udaykumar, narasimharaju and k. s. ashwath in lead roles.
the ﬁlmhad musical score by chellapilla satyam .
the ﬁlm was a remake of hindi ﬁlm (cid:2)unk(cid:3).
hiersumm: ( kannada : (cid:2)unk(cid:3) (cid:2)unk(cid:3) ) is a 1980 indian kannada ﬁlm , directed by bangalore nagesh .
the ﬁlm starssrinath , manjulla , thoogudeepa shreenivaas .
the ﬁlm stars srinath , manjula , manjula , thoogudeepa shreenivaas inlead roles .
the ﬁlm had musical score by upendra kumar .
the ﬁlm is a remake of telugu movie aakashagiri rao .
themovie was remade in telugu as manjina in 1986 .
the movie was remade in telugu as manjina there .
.
.
bart: manjina there is a 1980 kannada family drama ﬁlm directed by bangalore nagesh starring srinath and manjulain the lead roles.
it was released on 14 january 1980.twag: manjina there is a 1980 kannada drama ﬁlm directed by bangalore nagesh.
the ﬁlm stars srinath, vajramuni,manjula and thoogudeepa srinivas in lead roles.
the ﬁlm had musical score by upendra kumar and the ﬁlm opened topositive reviews in 1980. the ﬁlm was a remake of tamil ﬁlm (cid:2)unk(cid:3)..table 3: comparison between wikipedia abstracts generated by different models about the ﬁlm majina there.
non-english characters have been replaced with (cid:3)unk(cid:4) for readability..standard abstract express more than one topic.
as-signing one topic to each sentence will result insemantic loss and thus harm the quality of gen-erated abstract, while the soft topic could bettersimulate the human writing style..number of section labels.
the number of sec-tion labels nt plays a key role in our model: a smallnt would not be informative enough to build topics,while a large one would induce noise.
we can seefrom figure 3 that the frequency of section labelsis long-tailed, thus retaining only a small portion isable to capture the major part of information.
ta-.
figure 3: the frequency of section labels in three do-mains.
when ignoring section labels with extra high orlow frequency, remaining section labels’ frequency andrank generally form a straight line in log scale, whichmatches the zipf’s law for long-tail distributions..ble 5 records the experiment results we conductedon domain company.
nt = 20 reaches a peak onrouge 1, 2 and l scores, indicating that 20 is areasonable number of section labels..5.4 case study.
#labels.
r1.
r2.
rl.
102030.
.337.340.336.
.117.118.117.
.312.315.311.table 5: rouge f1 scores of different nt..can see that the gold abstract contains informationabout three topics: basic information (region, di-rector, and producer), actors, and music..among the models, tf-s2s produces an abstractwith a proper pattern but contains wrong informa-tion and bart misses the musical informationtopic.
cv-s2d+t, hiersumm, and our twagmodel both cover all three topics in the gold ab-stract, however, cv-s2d+t makes several factualerrors like the release date and actors and hier-summ suffers from redundancy.
twag covers allthree topics in the gold abstract and discovers extrafacts, proving itself to be competent in generatingcomprehensive abstracts..5.5 human evaluation.
we follow the experimental setup of (perez-beltrachini et al., 2019) and conduct a human eval-uation consisting of two parts.
a total of 45 exam-ples (15 from each domain) are randomly selectedfrom the test set for evaluation..the ﬁrst part is a question-answering (qa)scheme proposed in (clarke and lapata, 2010) inorder to examine factoid information in summaries.
we create 2-5 questions3 based on the golden sum-.
table 3 shows the generated wikipedia abstractsby different models about ﬁlm majina there.
we.
3example questions are listed in the appendix c, and the.
whole evaluation set is included in the our code repository..4630model.
companyscore non-0.
filmscore non-0.
animalscore non-0.
tf-s2scv-s2d+thiersummbarttwag (ours).
.075.237.255.591.665.
.694.660.896.813.903.
.000.040.213.452.669.
.000.143.327.796.918.
.000.382.000.342.543.
.000.576.000.653.868.table 6: human evaluation results in qa scheme.
score represents the mean score and non-0 represents thepercentage of answered questions..model.
tf-s2scv-s2d+thiersummbarttwag (ours).
c.2.692.422.962.642.91.companyf.s.2.712.362.642.822.87.
2.672.731.693.002.91.filmf.2.712.692.783.023.16.s.c.animalf.2.842.982.043.243.44.
2.222.802.802.783.56.
2.963.183.133.113.58.s.2.763.181.823.003.40.c.1.932.293.132.873.20.table 7: human evaluation results in linguistic quality scoring.
c indicates completeness, f indicates ﬂuency ands indicates succinctness..mary which covers the appeared topics, and invite3 participants to answer the questions by takingautomatically-generated summaries as backgroundinformation.
the more questions a summary cananswer, the better it is.
to quantify the results, weassign a score of 1/0.5/0.1/0 to a correct answer, apartially correct answer, a wrong answer and thosecannot be answered, and report the average scoreover all questions.
notice that we give a score of0.1 even if the participants answer the question in-correctly, because a wrong answer indicates thesummary covers a certain topic and is superior tomissing information.
results in table 6 shows that1) taking summaries generated by twag is capableof answering more questions and giving the correctanswer, 2) tf-s2s and hiersumm perform poorlyin domain film and animal, which is possibly aconsequence of under-ﬁtting in small datasets..the second part is an evaluation over linguisticquality.
we ask the participants to read differentgenerated summaries from 3 perspectives and givea score of 1-5 (larger scores indicates higher qual-ity): completeness (does the summary containsufﬁcient information?
), fluency (is the summaryﬂuent and grammatical?)
and succinctness (doesthe summary avoid redundant sentences?)
speciﬁ-cally, 3 participants are assigned to evaluate eachmodel, and the average scores are taken as the ﬁ-.
nal results.
table 7 presents the comparison results,from which we can see that, the linguistic quality oftwag model outperforms other baseline models,validating its effectiveness..6 conclusion.
in this paper, we propose a novel topic-guided ab-stractive summarization model twag for generat-ing wikipedia abstracts.
it investigates the sectionlabels of wikipedia, dividing the input documentinto different topics to improve the quality of gener-ated abstract.
this approach simulates the way howhuman recognize entities, and experimental resultsshow that our model obviously outperforms exist-ing state-of-the-art models which view wikipediaabstracts as plain text.
our model also demon-strates its high data efﬁciency.
in the future, wewill try to incorporate pretrained language modelsinto the topic-aware abstract generator module, andapply the topic-aware model to other texts rich intopical information like sports match reports..acknowledgments.
we thank the anonymous reviewers for their in-sightful comments.
this work is supported by thenational key research and development programof china (2017yfb1002101), nsfc key project(u1736204) and a grant from huawei inc..4631ethical considerations.
twag could be applied to applications like auto-matically writing new wikipedia abstracts or othertexts rich in topical information.
it can also help hu-man writers to examine whether they have missedinformation about certain important topics..the beneﬁts of using our model include savinghuman writers’ labor and making abstracts morecomprehensive.
there are also important consid-erations when using our model.
input texts mayviolate copyrights when inadequately collected,and misleading texts may lead to factual mistakesin generated abstracts.
to mitigate the risks, re-searches on how to avoid copyright issues whencollecting documents from the internet would help..references.
siddhartha banerjee and prasenjit mitra.
2016. wiki-write: generating wikipedia articles automatically.
in proceedings of the 25th international joint con-ference on artiﬁcial intelligence, pages 2740–2746..siddhartha banerjee, prasenjit mitra, and kazunarisugiyama.
2015. multi-document abstractive sum-marization using ilp based multi-sentence compres-sion.
in proceedings of the 24th international con-ference on artiﬁcial intelligence, pages 1208–1214..fadi biadsy, julia hirschberg, and elena filatova.
2008. an unsupervised approach to biography pro-duction using wikipedia.
in proceedings of acl-08:hlt, pages 807–815..lidong bing, piji li, yi liao, wai lam, weiwei guo,and rebecca j passonneau.
2015. abstractive multi-document summarization via phrase selection andmerging.
in proceedings of the 53rd annual meet-ing of the association for computational linguisticsand the 7th international joint conference on natu-ral language processing, pages 1587–1597..xiaoyan cai and wenjie li.
2012. mutually rein-forced manifold-ranking based relevance propaga-tion model for query-focused multi-document sum-marization.
ieee transactions on audio, speech,and language processing, 20(5):1597–1607..james clarke and mirella lapata.
2010. discourse con-straints for document compression.
computationallinguistics, 36(3):411–441..g¨unes erkan and dragomir r radev.
2004. lexrank:graph-based lexical centrality as salience in textsummarization.
journal of artiﬁcial intelligence re-search, 22:457–479..alexander richard fabbri, irene li, tianwei she, suyili, and dragomir radev.
2019. multi-news: a large-scale multi-document summarization dataset and ab-stractive hierarchical model.
in proceedings of the.
57th annual meeting of the association for compu-tational linguistics, pages 1074–1084..katja filippova and michael strube.
2008. sentencefusion via dependency graph compression.
in pro-ceedings of the 2008 conference on empirical meth-ods in natural language processing, pages 177–185..sebastian gehrmann, yuntian deng, and alexander mrush.
2018. bottom-up abstractive summarization.
in proceedings of the 2018 conference on empiri-cal methods in natural language processing, pages4098–4109..hiroaki hayashi, prashant budania, peng wang, chrisackerson, raj neervannan, and graham neubig.
2021. wikiasp: a dataset for multi-domain aspect-based summarization.
transactions of the associa-tion for computational linguistics, 9:211–225..daniel hewlett, alexandre lacoste, llion jones, illiapolosukhin, andrew fandrianto, jay han, matthewkelcey, and david berthelot.
2016. wikireading: anovel large-scale language understanding task overwikipedia.
in proceedings of the 54th annual meet-ing of the association for computational linguistics,pages 1535–1545..luyang huang, lingfei wu, and lu wang.
2020.knowledge graph-augmented abstractive summa-rization with semantic-driven cloze reward.
in pro-ceedings of the 58th annual meeting of the asso-ciation for computational linguistics, pages 5094–5107..thomas n. kipf and max welling.
2017..semi-supervised classiﬁcation with graph convolutionalin proceedings of the 5th internationalnetworks.
conference on learning representations..zhenzhong lan, mingda chen, sebastian goodman,kevin gimpel, piyush sharma, and radu soricut.
2019. albert: a lite bert for self-supervised learningof language representations.
in international con-ference on learning representations..jens lehmann, robert isele, max jakob, anja jentzsch,dimitris kontokostas, pablo n mendes, sebastianhellmann, mohamed morsey, patrick van kleef,s¨oren auer, et al.
2015. dbpedia–a large-scale, mul-tilingual knowledge base extracted from wikipedia.
semantic web, 6(2):167–195..mike lewis, yinhan liu, naman goyal, mar-jan ghazvininejad, abdelrahman mohamed, omerlevy, veselin stoyanov, and luke zettlemoyer.
2020. bart: denoising sequence-to-sequence pre-training for natural language generation, translation,and comprehension.
in proceedings of the 58th an-nual meeting of the association for computationallinguistics, pages 7871–7880..wei li, xinyan xiao, jiachen liu, hua wu, haifengwang, and junping du.
2020. leveraging graph.
4632to improve abstractive multi-document summariza-in proceedings of the 58th annual meetingtion.
of the association for computational linguistics,pages 6232–6243..chin-yew lin.
2004. rouge: a package for automaticin text summarization.
evaluation of summaries.
branches out, pages 74–81..peter j liu, mohammad saleh, etienne pot, bengoodrich, ryan sepassi, lukasz kaiser, and noamshazeer.
2018. generating wikipedia by summariz-ing long sequences.
in international conference onlearning representations..yang liu and mirella lapata.
2019. hierarchical trans-formers for multi-document summarization.
in pro-ceedings of the 57th annual meeting of the asso-ciation for computational linguistics, pages 5070–5081..yinhan liu, myle ott, naman goyal, jingfei du, man-dar joshi, danqi chen, omer levy, mike lewis,luke zettlemoyer, and veselin stoyanov.
2019.roberta: a robustly optimized bert pretraining ap-proach.
arxiv, pages arxiv–1907..rada mihalcea and paul tarau.
2004. textrank: bring-ing order into text.
in proceedings of the 2004 con-ference on empirical methods in natural languageprocessing, pages 404–411..romain paulus, caiming xiong, and richard socher.
2018. a deep reinforced model for abstractive sum-marization.
in international conference on learn-ing representations..jeffrey pennington, richard socher, and christophermanning.
2014. glove: global vectors for wordrepresentation.
in proceedings of the 2014 confer-ence on empirical methods in natural languageprocessing), pages 1532–1543..laura perez-beltrachini, yang liu, and mirella lapata.
2019. generating summaries with topic templatesand structured convolutional decoders.
in proceed-ings of the 57th annual meeting of the associationfor computational linguistics, pages 5107–5116..dragomir radev.
2000. a common theory of infor-mation fusion from multiple text sources step one:cross-document structure.
in 1st sigdial workshopon discourse and dialogue, pages 74–83..pranav rajpurkar, jian zhang, konstantin lopyrev, andpercy liang.
2016. squad: 100,000+ questions formachine comprehension of text.
in proceedings ofthe 2016 conference on empirical methods in natu-ral language processing, pages 2383–2392..abigail see, peter j liu, and christopher d manning.
2017. get to the point: summarization with pointer-generator networks.
in proceedings of the 55th an-nual meeting of the association for computationallinguistics, pages 1073–1083..xiaojun wan.
2008. an exploration of document im-pact on graph-based multi-document summarization.
in proceedings of the 2008 conference on empiricalmethods in natural language processing, pages 755–762..xiaojun wan and jianguo xiao.
2009. graph-basedmulti-modality learning for topic-focused multi-document summarization.
in proceedings of the21st international jont conference on artiﬁcal intel-ligence, pages 1586–1591..michihiro yasunaga, rui zhang, kshitijh meelu,ayush pareek, krishnan srinivasan, and dragomirradev.
2017. graph-based neural multi-documentsummarization.
in proceedings of the 21st confer-ence on computational natural language learning,pages 452–462..yongjing yin, linfeng song, jinsong su, jiali zeng,chulun zhou, and jiebo luo.
2019. graph-basedneural sentence ordering.
in proceedings of the 28thinternational joint conference on artiﬁcial intelli-gence, pages 5387–5393..jianmin zhang, jiwei tan, and xiaojun wan.
2018.towards a neural network approach to abstractivearxiv preprintmulti-document summarization.
arxiv:1804.09010..a topic allocation.
for each domain, we sort section labels by fre-quency and choose the top nt = 20 frequent sec-tion labels, then manually allocate them into dif-ferent topics.
section labels with little semanticinformation like reference and notes are discardedin allocation to reduce noise.
table 8 shows howwe allocate section labels into topics in domaincompany, film and animal..an additional noise topic is added to each do-main to detect website noises.
we build trainingrecords for noise by ﬁnding noise text in thetraining set of wikicatsum by regular expressions.
for example, we view all text containing “cookie”,“href” or text that seems to be a reference as noise..b trivia about baselines.
christina sauper and regina barzilay.
2009. auto-matically generating wikipedia articles: a structure-aware approach.
in proceedings of the joint confer-ence of the 47th annual meeting of the acl and the4th international joint conference on natural lan-guage processing of the afnlp, pages 208–216..we use bart-base as the baseline for compari-son because bart-large performs poorly in ex-periments.
bart-large starts generating redun-dant results when using only 4% training data, andits training loss also decreases much slower than.
4633bart-base.
we infer that bart-large may overﬁton training data, and bart-base is more competentto be the baseline..c human evaluation example.
table 9 shows an example of gold summary, its cor-responding question set and system outputs.
thefull dataset we used for human evaluation can befound in our code repository..4634domain.
company.
film.
animal.
topichistory.
product.
location.
reception.
castplotproduction.
reception.
box ofﬁcedistributiontaxonomy.
description.
conservation status.
section labelshistory, company history, ownershipproducts, services, destinations,products and services, technologyfleet, operations, subsidiaries , locationsawards, controversies, controversy,criticism, accidents and incidents, receptioncast, castingplot, synopsis, plot summaryproduction, filming, developmentreception, critical reception, critical response,awards, accolades, awards and nominationsbox ofﬁcedistributiontaxonomy, species, subspecies, classiﬁcationdescription, habitat, ecology, behaviour, biology,diet, feeding, breeding, reproduction, life cyclestatus, conservation, conservation status.
table 8: detailed allocation of section labels.
in domain company, orange labels are labels selected when nt = 10,green labels are additional labels selected when nt = 20, and blue labels are additional labels selected whennt = 30..gold abstract: l’avare is a 1980 french comedy ﬁlm written and directed by louis de fun`es and jean girault , andstarring de fun`es.
the english title of the ﬁlm is the miser.
it is an adaptation of moli`ere ’s famous comedy l’avare oul’´ecole du mensonge ( the miser ).
de fun`es tried to draw out the unhappy side of the character.
harpagon , unlovedby humanity, is driven to an obsessive love of money.
questions:1. when and where was l’avare released?
2. who stars in l’avare?
3. who directed l’avare?
4. what is the english name of l’avare?
tf-s2s: der er et yndigt land is a 1983 danish drama ﬁlm directed by morten arnfred.
it was entered into the 33rdberlin international ﬁlm festival , where it won an honourable mention.
cv-s2d+t: &lt;unk&gt; ’s &lt;unk&gt; is a french comedy ﬁlm from 1954, directed by jean girault, starring jeanmarais and louis de fun`es.
it was screened in the un certain regard section at the 2015 cannes ﬁlm festival.
hiersumm:( hangul ; rr : l’am ) is a french drama ﬁlm directed by louis de fun`es.
it is based on a play by moli`ere.
it stars louis de fun`es.
it was entered into the 36th berlin international ﬁlm festival.
the ﬁlm was nominated forthe golden globe award for best foreign language ﬁlm.
it was also nominated for the golden globe award for bestforeign language ﬁlm.
.
.
.
bart: l’avare ( english : the miser ) is a 1980 french drama ﬁlm directed by louis de fun`es and starring jean girault.
it was based on an original screenplay co-written with julien franc¸ois .
twag: the miser ( french : l’avare ) is a 1980 french drama ﬁlm directed by funes de fun`es.
the ﬁlm stars louis defun`es , sanctioning cl´eante , and broach harpagon..table 9: example of gold summary, question set and system outputs for the qa evaluation study..4635