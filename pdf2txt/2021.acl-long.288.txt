cdrnn: discovering complex dynamics in human language processing.
cory shainthe ohio state universityshain.3@osu.edu.
abstract.
the human mind is a dynamical system, yetmany analysis techniques used to study it arelimited in their ability to capture the complexdynamics that may characterize mental pro-cesses.
this study proposes the continuous-time deconvolutional regressive neural net-work (cdrnn), a deep neural extensionof continuous-time deconvolutional regression(cdr, shain and schuler, 2021) that jointlycaptures time-varying, non-linear, and delayedinﬂuences of predictors (e.g.
word surprisal)on the response (e.g.
reading time).
despitethis ﬂexibility, cdrnn is interpretable andable to illuminate patterns in human cognitionthat are otherwise difﬁcult to study.
behavioraland fmri experiments reveal detailed andplausible estimates of human language pro-cessing dynamics that generalize better thancdr and other baselines, supporting a poten-tial role for cdrnn in studying human lan-guage processing..1.introduction.
central questions in psycholinguistics concern themental processes involved in incremental humanlanguage understanding: which representations arecomputed when, by what mental algorithms (fra-zier and fodor, 1978; just and carpenter, 1980;abney and johnson, 1991; tanenhaus et al., 1995;almor, 1999; gibson, 2000; coltheart et al., 2001;hale, 2001; lewis and vasishth, 2005; levy, 2008,inter alia)?
such questions are often studied bycaching out a theory of language processing in anexperimental stimulus, collecting human responses,and ﬁtting a regression model to test whether mea-sures show the expected effects (e.g.
grodner andgibson, 2005).
regression techniques have grownin sophistication, from anova (e.g.
pickering andbranigan, 1998) to newer linear mixed-effects ap-proaches (lme, bates et al., 2015) that enable.
direct word-by-word analysis of effects in natu-ralistic human language processing (e.g.
dembergand keller, 2008; frank and bod, 2011).
however,these methods struggle to account for delayed ef-fects.
because the human mind operates in realtime and experiences computational bottlenecks ofvarious kinds (bouma and de voogd, 1974; justand carpenter, 1980; ehrlich and rayner, 1981;mollica and piantadosi, 2017), delayed effects maybe pervasive, and, if left uncontrolled, can yieldmisleading results (shain and schuler, 2018)..continuous-time deconvolutional.
regression(cdr) is a recently proposed technique to addressdelayed effects in measures of human cognition(shain and schuler, 2018, 2021).
cdr ﬁts para-metric continuous-time impulse response functions(irfs) that mediate between word features and re-sponse measures.
an irf maps the time elapsedbetween a stimulus and a response to a weightdescribing the expected inﬂuence of the stimuluson the response.
cdr models the response as anirf-weighted sum of preceding stimuli, thus di-rectly accounting for effect latencies.
empirically,cdr reveals ﬁne-grained processing dynamics andgeneralizes better to human reading and fmri re-sponses than established alternatives.
however,cdr retains a number of simplifying assumptions(e.g.
that the irf is ﬁxed over time) that may nothold of the human language processing system..deep neural networks (dnns), widely used innatural language processing (nlp), can relax thesestrict assumptions.
indeed, psycholinguistic re-gression analyses and nlp systems share a com-mon structure: both ﬁt a function from word fea-tures to some quantity of interest.
however, psy-cholinguistic regression models face an additionalconstraint: they must be interpretable enough toallow researchers to study relationships betweenvariables in the model.
this requirement may beone reason why black box dnns are not generally.
proceedingsofthe59thannualmeetingoftheassociationforcomputationallinguisticsandthe11thinternationaljointconferenceonnaturallanguageprocessing,pages3718–3734august1–6,2021.©2021associationforcomputationallinguistics3718used to analyze psycholinguistic data, despite thetremendous gains dnns have enabled in naturallanguage tasks (peters et al., 2018; devlin et al.,2019; radford et al., 2019; brown et al., 2020, interalia), in part by better approximating the complexdynamics of human cognition as encoded in natu-ral language (linzen et al., 2016; gulordava et al.,2018; tenney et al., 2019; hewitt and manning,2019; wilcox et al., 2019; schrimpf et al., 2020).
this study proposes an attempt to leverage theﬂexibility of dnns for psycholinguistic data anal-ysis.
the continuous-time deconvolutional regres-sive neural network (cdrnn) is an extensionof cdr that reimplements the impulse responsefunction as a dnn describing the expected in-ﬂuence of preceding events (e.g.
words) on fu-ture responses (e.g.
reading times) as a functionof their properties and timing.
cdrnn retainsthe deconvolutional design of cdr while relax-ing many of its simplifying assumptions (linear-ity, additivity, homosketasticity, stationarity, andcontext-independence, see section 2), resulting ina highly ﬂexible model.
nevertheless, cdrnn isinterpretable and can shed light on the underlyingdata generating process.
results on reading andfmri measures show substantial generalization im-provements from cdrnn over baselines, alongwith detailed insights about the underlying dynam-ics that cannot easily be obtained from existingmethods.1.
2 background.
psycholinguists have been aware for decades thatprocessing effects may lag behind the words thattrigger them (morton, 1964; bouma and de voogd,1974; rayner, 1977; erlich and rayner, 1983;mitchell, 1984; rayner, 1998; vasishth and lewis,2006; smith and levy, 2013), possibly becausecognitive “buffers” may exist to allow higher-levelinformation processing to catch up with the input(bouma and de voogd, 1974; baddeley et al., 1975;just and carpenter, 1980; ehrlich and rayner,1981; mollica and piantadosi, 2017).
they havealso recognized the potential for non-linear, inter-active, and/or time-varying relationships betweenword features and language processing (smithand levy, 2013; baayen et al., 2017, 2018).
noprior regression method can jointly address these.
1because of page constraints, additional replication detailsand synthetic results are provided in an external supplement,available here: https://osf.io/z89vn/..concerns in non-uniform time series (e.g.
wordswith variable duration) like naturalistic psycholin-guistic experiments.
discrete-time methods (e.g.
lagged/spillover regression, sims, 1971; erlich andrayner, 1983; mitchell, 1984) ignore potentiallymeaningful variation in event duration, even ifsome (e.g.
generalized additive models, or gams,hastie and tibshirani, 1986; wood, 2006) permitnon-linear and non-stationary (time-varying) fea-ture interactions (baayen et al., 2017).
cdr (shainand schuler, 2018, 2021) addresses this limitationby ﬁtting continuous-time irfs, but assumes thatthe irf is stationary (time invariant), that featuresscale linearly and combine additively, and that theresponse variance is constant (homoskedastic).
byimplementing the irf as a time-varying neural net-work, cdrnn relaxes all of these assumptions, in-corporating the featural ﬂexibility of gams whileretaining the temporal ﬂexibility of cdr..previous studies have investigated latency andnon-linearity in human sentence processing.
forexample, smith and levy (2013) attach theoreticalsigniﬁcance to the functional form of the relation-ship between word surprisal and processing cost,using gams to show that this relationship is linearand arguing on this basis that language processingis highly incremental.
this claim is under activedebate (brothers and kuperberg, 2021), underlin-ing the importance of methods that can investi-gate questions of functional form.
smith and levy(2013) also investigate the timecourse of surprisaleffects using spillover and ﬁnd a more delayed sur-prisal response in self-paced reading (spr) thanin eye-tracking.
shain and schuler (2021) supportthe latter ﬁnding using cdr, and in addition showevidence of strong inertia effects in spr, such thatparticipants who have been reading quickly in therecent past also read more quickly now.
however,this outcome may be an artifact of the stationarityassumption: cdr may be exploiting its estimatesof rate effects in order to capture broad non-linearnegative trends (e.g.
task adaptation, prasad andlinzen, 2019) in a stationary model.
similarly, thegenerally null word frequency estimates reportedin shain and schuler (2021) may be due in part tothe assumption of additive effects: word frequencyand surprisal are related, and they may coordinateinteractively to determine processing costs (nor-ris, 2006).
thus, in general, prior ﬁndings on thetimecourse and functional form of effects in humansentence processing may be inﬂuenced by method-.
3719(cid:80).
1x.g(τ ).
∼.
1x.g(τ ).
∼.
1x.g(τ ).
∼.
g(τ ).
∼.
1x.s.h.nnr.n.+ i.nnr.n.+ i.nnr.n.+ i.nnr.n.+ i.convolution.
irfτ.rnn.
h(0) =.
(cid:21)(cid:20)xt.figure 1: cdrnn model.
subscripts omitted to re-duce clutter.
the irf g(τ ) at an event computes the ex-pected contribution of each feature of the event vectorh(0) to each element of the parameter vector s of thepredictive distribution for a particular response value.
the ﬁrst layer of the irf depends non-linearly on theproperties of the event via hin and (optionally) on con-text via hrnn, which requires the recurrent connectionsin gray.
elements with random effects have dotted out-lines.
for variable deﬁnitions, see appendix a..ological limitations: the gam models of smithand levy (2013) ignore variable event duration, thecdr models of shain and schuler (2021) ignorenon-linearity, and both approaches assume station-arity, context-independence, constant variance, andadditive effects.
by jointly relaxing these poten-tially problematic assumptions, cdrnn stands tosupport more reliable conclusions about human lan-guage comprehension, while also possibly enablingnew insights into cognitive dynamics..3 model.
3.1 architecture.
this section presents a high-level description ofthe model design (for formal deﬁnition, see ap-pendix a).
the cdrnn architecture is representedschematically in figure 1. the primary goal of esti-mation is to identify the deep neural irf g(τ ) (top)that computes the inﬂuence of a preceding eventon the predictive distribution over a subsequent re-sponse as a function of their distance in time τ .
asshown, the irf is a feedforward projection of τ intoa matrix that deﬁnes a weighted sum over the val-ues of input vector x, which is concatenated with abias to capture general effects of stimulus timing(rate).
this matrix multiplication determines thecontribution of the stimulus event to the parametersof the predictive distribution (e.g.
the mean andvariance parameters of a gaussian predictive distri-bution).
deﬁning the irf as a function of τ ensures.
that the model has a continuous-time deﬁnition..to capture non-linear effects of stimulus fea-tures, the irf projection is itself parameterized bya projection of a hidden state h. the dependenceon h permits non-linear inﬂuences of the proper-ties of the stimulus sequence on the irf itself.
togenerate h, the predictors x are concatenated withtheir timestamps t and submitted to the model asinput.
inputs are cast to a hidden state for eachpreceding event as the sum of three quantities: afeedforward projection hin of each input, a forward-directional rnn projection hrnn of the events upto and including each input, and random effects hzcontaining offsets for the relevant random effectslevel(s) (e.g.
for each participant in an experiment).
in this study, the recurrent component is treatedas optional (gray arrows).
without the rnn, themodel is non-stationary (via input t) but cannotcapture contextual inﬂuences on the irf..the summation over irf outputs at the top ofthe ﬁgure ensures that the model is deconvolutional:each preceding input contributes to the response insome proportion, with that proportion determinedby the features, context, and relative timing of thatinput.
because the irf depends on a deep neuralprojection of the current stimulus as well as (op-tionally) the entire sequence of preceding stimuli, itimplicitly estimates all interactions between thesevariables in governing the response.
predictors maythus coordinate in a non-linear, non-additive, andtime-varying manner..the cdrnn irf describes the inﬂuence overtime of predictors on all parameters of the predic-tive distribution (in these experiments, the meanand variance parameters of a gaussian predictivedistribution).
such a design (i.e.
modeling depen-dencies on the predictors of all parameters of thepredictive distribution) has previously been termeddistributional regression (b¨urkner, 2018)..despite their ﬂexibility and task performance(section 5), cdrnn models used in this study havefew parameters (table a1) by current deep learningstandards because they are relatively shallow andsmall (supplement s1)..3.2 objective and regularization.
given (1) an input conﬁguration c containing pre-dictors x, input timestamps t, and response times-tamps t(cid:48), (2) cdrnn parameter vector w, (3) out-put distribution p, (4) random effects vector z, and(5) response vector y, the model uses gradient de-.
3720scent to minimize the following objective:.
l (y | c; w, z) def= − log p (y | c; w, z) + (1).
λz||z||2.
2 + lreg.
in addition to random effects shrinkage governedby λz and any arbitrary additional regularizationpenalties lreg (see supplement s1), models areregularized using dropout (srivastava et al., 2014)with drop rate dh at the outputs of all feedforwardhidden layers.
random effects are also dropped atrate dz, which is intended to encourage the modelto ﬁnd population-level estimates that accuratelyreﬂect central tendency.
finally, the recurrent con-tribution to the cdrnn hidden state (hrnn above)is dropped at rate dr, which is intended to encour-age accurate irf estimation even when context isunavailable..3.3 effect estimation.
because it is a dnn, cdrnn lacks parametersthat selectively describe the size and shape of theresponse to a speciﬁc predictor (unlike cdr), andindeed individual parameters (e.g.
individual biasesor connection weights) are not readily interpretable.
thus, from a scientiﬁc perspective, the quantity ofgeneral interest is not a distribution over parame-ters, but rather over the effect of a predictor on theresponse.
the current study proposes to accom-plish this using perturbation analysis (e.g.
ribeiroet al., 2016; petsiuk et al., 2018), manipulating theinput conﬁguration and quantifying the inﬂuenceof this manipulation on the predicted response.2for example, to obtain an estimate of rate effects(i.e.
the base response or “deconvolutional inter-cept,” see shain and schuler, 2021), a referencestimulus can be constructed, and the response toit can be queried at each timepoint over some in-terval of interest.
to obtain cdr-like estimatesof predictor-wise irfs, the reference stimulus canbe increased by 1 in the predictor dimension ofinterest (e.g.
word surprisal) and requeried, takingthe difference between the obtained response andthe reference response to reveal the inﬂuence ofan extra unit of the predictor.3 this study uses the.
2perturbation analyses is one of a growing suite of tools forblack box interpretation.
it is used here because it straightfor-wardly links properties of the input to changes in the estimatedresponse, providing a highly general method for querying as-pects of the the non-linear, non-stationary, non-additive irfdeﬁned by the cdrnn equations..3note that 1 is used here to maintain comparability ofeffect estimates to those generated by methods that assume.
training set mean of x and t as a reference, sincethis represents the response of the system to anaverage stimulus.
the model also supports arbi-trary additional kinds of queries, including of thecurvature of an effect in the irf over time and ofthe interaction between two effects at a point intime.
indeed, the irf can be queried with respectto any combination of values for predictors, t, andτ , yielding an open-ended space of queries that canbe constructed as needed by the researcher..because the estimates of interest all derive fromthe model’s predictive distribution, uncertaintyabout them can be measured with monte carlo tech-niques as long as training involves a stochastic com-ponent, such as dropout (srivastava et al., 2014)or batch normalization (ioffe and szegedy, 2015).
this study estimates uncertainty using monte carlodropout (gal and ghahramani, 2016), which re-casts training neural networks with dropout as vari-ational bayesian approximation of deep gaussianprocess models (damianou and lawrence, 2013).
at inference time, an empirical distribution overresponses to an input is constructed by resamplingthe model (i.e.
sampling different dropout masks).4as argued by shain and schuler (2021) for cdr, inaddition to intervals-based tests, common hypothe-sis tests (e.g.
for the presence of an effect) can beperformed in a cdrnn framework via bootstrapmodel comparison on held out data (e.g.
of modelswith and without the effect of interest)..4 methods.
following shain and schuler (2021), cdrnnis applied to naturalistic human language pro-cessing data from three experimental modalities:the natural stories self-paced reading corpus(∼1m instances, futrell et al., 2020), the dundeeeye-tracking corpus (∼200k instances, kennedy.
linearity of effects (especially cdr), but that 1 has no specialmeaning in the non-linear setting of cdrnn modeling, andeffects can be queried at any offset from any reference.
resultshere show that deﬂections move relatively smoothly awayfrom the reference, even at smaller steps than 1, and that irfsqueried at 1 are similar to those obtained from (linear) cdr,indicating that this method of effect estimation is reliable.
note ﬁnally that because predictors are underlyingly rescaledby their training set standard deviations (though plotted at theoriginal scale for clarity), 1 here corresponds to 1 standardunit, as was the case with the cdr estimates discussed inshain and schuler (2021)..4initial experiments also explored uncertainty quantiﬁca-tion by implemententing cdrnn as a variational bayesiandnn.
compared to the methods advocated here, the varia-tional approach was more prone to instability, achieved worseﬁt, and yielded implausibly narrow credible intervals..3721modellmegamcdrcdrnn-ffcdrnn-rnn.
train19980†19873181181833818217.msdev20471†20349183731867718624.natural stories (spr).
test20230†20109182121840118430.train0.0789†0.07840.06460.06440.0636.log-msdev0.0807†0.08020.06520.06510.0647.test0.0803†0.07990.06540.06500.0642.train13112†12882130731276012791.msdev14162†13948141061386313897.dundee.
test14024†13771139601367813717.train0.1507†0.14910.15050.14790.1476.log-msdev0.1532†0.15180.15390.15070.1507.test0.1526†0.15080.15200.14980.1495.table 1: reading.
mean squared error by model.
baselines as reported in shain and schuler (2021).
daggers (†)indicate convergence failures..et al., 2003), and the natural stories fmri cor-pus (∼200k instances, shain et al., 2020), usingthe train/dev/test splits for these corpora deﬁnedin shain and schuler (2021).
further details aboutdatasets and preprocessing are given in supple-ment s2..for reading data, cdrnn is compared tocdr as well as lagged lme and gam baselinesequipped with four spillover positions for each pre-dictor (values from the current word, plus threepreceding words), since lme and gam are wellestablished analysis methods in psycholinguistics(e.g.
baayen et al., 2007; demberg and keller,2008; frank and bod, 2011; smith and levy, 2013;baayen et al., 2017; goodkind and bicknell, 2018,inter alia).
because the distribution of readingtimes is heavy-tailed (frank et al., 2013), follow-ing shain and schuler (2021) models are ﬁtted toboth raw and log-transformed reading times.
forfmri data, cdrnn is compared to cdr as wellas four existing techniques for analyzing naturalis-tic fmri data: pre-convolution with the canonicalhemodynamic response function (hrf, brennanet al., 2012; willems et al., 2015; henderson et al.,2015, 2016; lopopolo et al., 2017), linear interpo-lation (shain and schuler, 2021), binning (wehbeet al., 2020), and lanczos interpolation (huth et al.,2016).
statistical model comparisons use pairedpermutation tests of test set error (demˇsar, 2006).
models use predictors established by prior psy-cholinguistic research (e.g.
rayner, 1998; dembergand keller, 2008; van schijndel and schuler, 2013;staub, 2015; shain and schuler, 2018, inter alia):unigram and 5-gram surprisal, word length (read-ing only), saccade length (eye-tracking only), andprevious was ﬁxated (eye-tracking only).
predictordeﬁnitions are given in appendix c. the decon-volutional intercept term rate (shain and schuler,2018, 2021), an estimate of the general inﬂuenceof observing a stimulus at a point in time, inde-pendently of its properties, is implicit in cdrnn(unlike cdr) and is therefore reported in all results.
reading models include random effects by subject,.
while fmri models include random effects by sub-ject and by functional region of interest (froi).
unlike lme, where random effects capture lin-ear differences in effect size between e.g.
subjects,random effects in cdrnn capture differences inoverall dynamics between subjects, including dif-ferences in size, irf shape, functional form (e.g.
linearity), contextual inﬂuences on the irf, andinteractions with other effects..two cdrnn variants are considered in all ex-periments: the full model (cdrnn-rnn) contain-ing an rnn over the predictor sequence, and a feed-forward only model (cdrnn-ff) with the rnnablated (gray arrows removed in figure 1).
thismanipulation is of interest because cdrnn-ffis both more parsimonious (fewer parameters) andfaster to train, and may therefore be preferred in theabsence of prior expectation that the irf is sensi-tive to context.
all plots show means and 95% cred-ible intervals.
code and documentation are avail-able at https://github.com/coryshain/cdr..5 results.
since cdrnn is designed for scientiﬁc modeling,the principal output of interest is the irf itself andthe light it might shed on questions of cognitivedynamics, rather than on performance in some task(predicting reading latencies or fmri measures arenot widely targeted engineering goals).
however,predictive performance can help establish the trust-worthiness of the irf estimates.
to this end, asa sanity check, this section ﬁrst evaluates predic-tive performance on human data relative to existingregression techniques.
while results may resem-ble “bake-off” comparisons familiar from machinelearning (and indeed cdrnn does outperform allbaselines), their primary purpose is to establishthat the cdrnn estimates are trustworthy, sincethey describe the phenomenon of interest in a waythat generalizes accurately to an unseen sample.
baseline models, including cdr, are as reported.
3722modelcanonical hrflinearly interpolatedaveragedlanczos interpolatedcdrcdrnn-ffcdrnn-rnn.
train11.3548†11.4236†11.3478†11.3536†11.277410.564810.8736.expl11.8263†11.9888†11.9280†11.9059†11.692811.360211.5631.test11.5661†11.6654†11.6090†11.5871†11.536911.304211.3914.table 2: fmri.
mean squared error by model.
base-lines as reported in shain and schuler (2021).
daggers(†) indicate convergence failures..in shain and schuler (2021).5.
5.1 model validation: baseline comparisons.
table 1 gives mean squared error by dataset ofcdrnn vs. baseline models on reading times fromboth natural stories and dundee.
both versionsof cdrnn outperform all baselines on the devpartition of all datasets except for raw (ms) laten-cies in natural stories (spr), where cdrnn isedged out by cdr6 but still substantially outper-forms the non-cdr baselines.
nonetheless, resultsindicate that cdrnn estimates of natural stories(ms) are similarly reliable to those of cdr, and,as discussed in section 5.2, cdrnn largely repli-cates the cdr estimates on natural stories whileoffering advantages for analysis..although cdr struggles against gam baselineson dundee, cdrnn has closed the gap.
this isnoteworthy in light of speculation in shain andschuler (2021) that cdr’s poorer performanceon dundee might be due in part to non-lineareffects, which gam can estimate but cdr can-not.
cdrnn performance supports this conjecture:once the model can account for non-linearities, itovertakes gams..results from fmri are shown in table 2, whereboth cdrnn variants yield substantial improve-ments to training, dev, and test set error.
these re-sults indicate that the relaxed assumptions affordedby cdrnn are beneﬁcial for describing the fmriresponse, which is known to saturate over time(friston et al., 2000; wager et al., 2005; vazquezet al., 2006; lindquist et al., 2009)..following shain and schuler (2021), model er-ror is statistically compared using a paired permu-.
5for all datasets, the cdr baseline used here is the variantthat was deployed on the test set in shain and schuler (2021).
6note that a major advantage of cdrnn is its ability tomodel dynamics in response variance, which are not reﬂectedin squared error.
for example, although cdrnn-ff achievesworse test set error than cdr on the natural stories (ms) task,it affords a 31,040 point log likelihood improvement..baseline modalitylmereadinggam reading.
canonical hrfinterpolatedaveragedlanczoscdrcdrnn-ff.
fmrifmrifmrifmribothboth.
cdrnn.
ffp0.0001***0.0001***0.0001***0.0001***0.0001***0.0001***0.0001***—.
rnnp0.0001***0.0001***0.0001***0.0001***0.0001***0.0001***0.0001***0.0048**.
table 3: permutation test of overall test set perfor-mance improvement from cdrnn variants over eachbaseline..tation test that pools across all datasets coveredby a given baseline (reading data for lme andgam, fmri data for canonical hrf, linearly inter-polated, averaged, and lanczos interpolated, andboth for cdr).7 results are given in table 3. asshown, both variants of cdrnn signiﬁcantly im-prove over all baselines, and cdrnn-rnn signif-icantly improves over cdrnn-ff.
notwithstand-ing, cdrnn-ff may be preferred in applications:simpler, faster to train, better at recovering syn-thetic models (supplement s3), more reliable innoisy domains like fmri, and close in performanceto cdrnn-rnn.
results overall support the relia-bility of patterns revealed by cdrnn’s estimatedirf, which is now used to explore and visualizesentence processing dynamics..5.2 effect latencies in cdrnn vs. cdr.
cdr-like irf estimates can be obtained by increas-ing a predictor by 1 (standard deviation) relativeto the reference and observing the change in theresponse over time.
visualizations using this ap-proach are presented in figure 2 alongside cdr es-timates from shain and schuler (2021).
in general,cdrnn ﬁnds similar patterns to cdr.
this sug-gests both (1) that cdrnn is capable of recoveringestimates from a preceding state-of-the-art decon-volutional model for these domains, and (2) thatcdr estimates in these domains are not driven byartifacts introduced by its simplifying assumptions,since a model that lacks those assumptions and hasa qualitatively different architecture largely recov-ers them.
nonetheless there are differences.
for ex-ample, dundee estimates decay more quickly overtime in cdrnn than in cdr, indicating an evenless pronounced inﬂuence of temporal diffusion in.
7the comparison rescales each pair of error vectors bytheir joint standard deviation in order to enable comparabilityacross datasets with different error variances..3723cdr.
cdrnn-ff.
cdrnn-rnn.
).
rps(.
rotstan.sm-gol.eednud.sm-gol.)irm.f(.
rotstan.dlob.
• rate.
saccade length.
previous was ﬁxated.
word length/sound power.
unigram surprisal.
5-gram surprisal.
figure 2: cdrnn-estimated irfs across datasets, with cdr estimates from shain and schuler (2021) for refer-ence.
sound power omitted from cdrnn fmri models (see appendix c for justiﬁcation)..delay (s).
eye-tracking than cdr had previously suggested.
estimates from cdrnn-ff and cdrnn-rnnroughly agree, except that cdrnn-rnn estimatesfor fmri are more attenuated.
cdr shows littleuncertainty in the fmri domain despite its inher-ent noise (shain et al., 2020), while cdrnn moreplausibly shows more uncertainty in its estimatesfor the noisier fmri data..as noted in section 2, shain and schuler (2021)report negative rate effects in reading — i.e., alocal decrease in subsequent reading time at eachword, especially in spr.
this was interpreted asan inertia effect (faster recent reading engendersfaster current reading), but it might also be an ar-tifact of non-linear decreases in latency over time(due to task habituation, e.g.
baayen et al., 2017;harrington stack et al., 2018; prasad and linzen,2019) that cdr cannot model.
cdrnn estimatesnonetheless thus support the prior interpretation ofrate effects as inertia, at least in spr: a model thatcan ﬂexibly adapt to non-linear habituation trendsﬁnds spr rate estimates that are similar in shapeand magnitude to those estimated by cdr..in addition, cdrnn ﬁnds a slower responseto word surprisal in self-paced reading than ineye-tracking.
this result converges with word-.
discretized timecourses reported in smith and levy(2013), who ﬁnd more extensive spillover of sur-prisal effects in spr than in eye-tracking.
resultsthus reveal important hidden dynamics in the read-ing response (inertia effects), continuous-time de-lays in processing effects, and inﬂuences of modal-ity the continuous dynamics of sentence processing,all of which are difﬁcult to estimate using existingregression techniques.
greater response latencyand more pronounced inertia effects in self-pacedreading may be due to the fact that a gross mo-tor task (paging via button presses) is overlaid onthe sentence comprehension task.
while the motortask is not generally of interest to psycholinguistictheories, controlling for its effects is crucial whenusing self-paced reading to study sentence compre-hension (mitchell, 1984)..5.3 linearity of surprisal effects.
cdrnn also allows the analyst to explore otheraspects of the irf, such as functional curvatureat a point in time.
for example, in the context ofreading, smith and levy (2013) argue for a linearincrease in processing cost as a function of wordsurprisal.
the present study allows this claim to beassessed across modalities by checking the curva-.
3724natstor (spr).
dundee.
natstor (fmri).
suoenatnatsni.i.emtrevo.figure 3: cdrnn-ff-estimated functional curvatureof the 5-gram surprisal response.
in 3d plots, 95%credible intervals shown as vertical gray bars..ture of the 5-gram surprisal response (in raw ms) ata timepoint of interest (0ms for reading and ∼5s forfmri).
as shown in the top row of figure 3, read-ing estimates are consistent with a linear response(the credible interval contains a straight line), aspredicted, but are highly non-linear in fmri, witha rapid peak above the mean (zero-crossing) fol-lowed by a sharp dip and plateau, and even anestimated increased response at values below themean (though estimates at the extremes have highuncertainty).
this may be due in part to ceilingeffects: blood oxygen levels measured by fmri arebounded, but reading times are not.
while this isagain a property of experimental modality ratherthan sentence comprehension itself, understandingsuch inﬂuences is important for drawing scientiﬁcconclusions from experimental data.
for example,due to the possibility of saturation, fmri may notbe an ideal modality for testing scientiﬁc claimsabout the functional form of effects, and the lin-earity assumptions of e.g.
cdr and lme may beparticularly constraining..the curvature of effects can also be queried overtime.
if an effect is temporally diffuse but linear,its curvature should be roughly linear at any de-lay of interest.
the second row of figure 3 showsvisualizations to this effect.
these plots in fact sub-sume the kinds of univariate plots shown above:univariate irfs to 5-gram surprisal like those plot-ted in figure 2 are simply slices taken at a pre-dictor value (1 sample standard deviation abovethe mean), whereas curvature estimates in the ﬁrstrow of figure 3 are simply slices taken at a timevalue (0s for reading and 5s for fmri).
plots areconsistent with the linearity hypothesis for reading,but again show strong non-linearities in the fmridomain that are consistent with saturation effects.
delay (s).
• rate.
sound power.
unigram surprisal.
5-gram surprisal.
pcfg surprisal.
figure 4: effect interactions in a cdrnn-ff repli-cation of shain et al.
(2020).
95% credible intervalsshown as vertical gray bars..as discussed above..5.4 effect interactions.
in addition to exploring multivariate relationshipsof a predictor with time, relationships between pre-dictors can also be studied.
such relationships con-stitute “interactions” in a cdrnn model, thoughthey are not constrained (cf.
interactions in linearmodels) to be strictly multiplicative — indeed, amajor advantage of cdrnn is that interactionscome “for free”, along with estimates of theirfunctional form.
to explore effect interactions, acdrnn-ff version of the full model in shain et al.
(2020) is ﬁtted to the fmri dataset.
the modelcontains more predictors to explore than modelsconsidered above, including surprisal computedfrom a probabilistic context-free grammar (pcfgsurprisal, see appendix c for details).
univariateirfs are shown in the top left panel of figure 4,and pairwise interaction surfaces at a delay of 5s(near the peak response) are shown in the remainingpanels.
plots show that the response at any valueof the other predictors is roughly ﬂat as a functionof sound power (i.e.
signal power of the auditorystimulus, middle row).
this accords with prior ar-guments that the cortical language system, whoseactivity is measured here, does not strongly regis-ter low-level perceptual effects (fedorenko et al.,2010; braze et al., 2011)..3725natstor (spr).
dundee.
natstor (fmri).
delay (s).
• rateword length.
saccade length.
previous was ﬁxated.
unigram surprisal.
5-gram surprisal.
figure 5: cdrnn-ff-estimated irfs of the varianceof the response by dataset..the estimate for unigram surprisal (middle left)shows an unexpected non-linearity: although ac-tivity increases with higher surprisal (lower fre-quency words), it also increases at lower surprisal(higher frequency words), suggesting the existenceof high frequency items that nonetheless engender alarge response.
the interaction between pcfg sur-prisal and unigram surprisal possibly sheds lighton this outcome, since it shows a sharper increasein the pcfg surprisal response in higher frequency(lower unigram surprisal) regions.
this may be be-cause the most frequent words in english tend tobe function words that play an outsized role in syn-tactic structure building (e.g.
prepositional phraseattachment decisions)..in addition, 5-gram surprisal interacts withpcfg surprisal, showing a non-linear increase inresponse for words that are high on both measures.
this is consistent with a unitary predictive mech-anism that experiences strong error signals whenboth string-level (5-gram) and structural (pcfg)cues are poor.
all these interactions should be inter-preted with caution, since the uncertainty intervalcovers much weaker degrees of interaction..5.5.irfs of the response variance.
as discussed in section 3, cdrnn implementsdistributional regression and thus also contains anirf describing the inﬂuence of predictors on thevariance of the predictive distribution as a functionof time.
irfs of the variance can be visualizedidentically to irfs of the mean..for example, figure 5 shows the estimatedchange in the standard deviation of the predic-tive distribution over time from observing a stimu-lus.8 estimates show stimulus-dependent changes.
8because standard deviation is a bounded variable andthe irf applies before the constraint function (softplus), therelationship between the standard deviation and the y axis ofthe plots is not straightforward.
estimates nonetheless clearlyindicate the shape and relative contribution to the response.
in variance across datasets whose shapes are notstraightforwardly related to that of the irfs ofthe mean (figure 2).
for example, both read-ing datasets (left and center) generally show meanand standard deviation traveling together, with in-creases in the mean corresponding to increases instandard deviation.
in dundee, the shapes of thesechanges resemble each other strongly, whereas innatural stories the irfs of the standard deviation(especially rate) differ substantially from the irfsof the mean.
by contrast, in fmri (right), the irfsof the standard deviation look roughly like invertedhrfs (especially for rate and 5-gram surprisal),indicating that bold variance tends to decreasewith larger values of the predictors.
while detailedinterpretation of these patterns is left to future work,these results demonstrate the utility of cdrnn foranalyzing a range of links between predictors andresponse that are otherwise difﬁcult to study..6 conclusion.
this study proposed and evaluated cdrnn, a deepneural extension of continuous-time deconvolu-tional regression that relaxes implausible simpli-fying assumptions made by widely used regres-sion techniques in psycholinguistics.
in so doing,cdrnn provides detailed estimates of human lan-guage processing dynamics that are difﬁcult to ob-tain using other measures.
results showed plau-sible estimates from human data that generalizebetter than alternatives and can illuminate hith-erto understudied properties of the human sentenceprocessing response.
this outcome suggests thatcdrnn may play a valuable role in analyzinghuman experimental data..references.
mart´ın abadi, ashish agarwal, paul barham, eugenebrevdo, zhifeng chen, craig citro, greg corrado,andy davis, jeffrey dean, matthieu devin, san-jay ghemawat, ian goodfellow, andrew harp, ge-offrey irving, michael isard, yangqing jia, rafaljozefowicz, lukasz kaiser, manjunath kudlur, joshlevenberg, dan man´e, rajat monga, sherry moore,derek murray, chris olah, mike schuster, jonathonshlens, benoit steiner, ilya sutskever, kunal talwar,paul tucker, vincent vanhoucke, vijay vasudevan,fernanda vi´egas, oriol vinyals, pete warden, mar-tin wattenberg, martin wicke, yuan yu, and xiao-qiang zheng.
2015. tensorflow: large-scale ma-chine learning on heterogeneous distributed sys-tems..variance of the stimulus features..3726steven p abney and mark johnson.
1991. memory re-quirements and local ambiguities of parsing strate-gies.
j.\ psycholinguistic research, 20(3):233–250..amit almor.
1999. noun-phrase anaphora and focus:the informational load hypothesis.
psychologicalreview, 106(4):748–765..harald baayen, shravan vasishth, reinhold kliegl, anddouglas bates.
2017. the cave of shadows: ad-dressing the human factor with generalized additivemixed models.
journal of memory and language,94(supplement c):206–234..r harald baayen, doug j davidson, and douglas mbates.
2007. mixed effects modelling with crossedrandom effects for subjects and items.
manuscript..r harald baayen, jacolien van rij, cecile de cat, andsimon wood.
2018. autocorrelated errors in exper-imental data in the language sciences: some solu-tions offered by generalized additive mixed mod-els.
in dirk speelman, kris heylen, and dirk geer-aerts, editors, mixed effects regression models inlinguistics.
springer, berlin..alan d baddeley, neil thomson, and mary buchanan.
1975. word length and the structure of short termmemory.
journal of verbal learning and verbal be-havior, 15(6):575–589..douglas bates, martin m¨achler, ben bolker, and stevewalker.
2015. fitting linear mixed-effects modelsusing lme4.
journal of statistical software, 67(1):1–48..h bouma and a h de voogd.
1974. on the con-trol of eye saccades in reading.
vision research,14(4):273–284..david braze, w einar mencl, whitney tabor, ken-neth r pugh, r todd constable, robert k fulbright,james s magnuson, julie a van dyke, and don-ald p shankweiler.
2011. uniﬁcation of sentenceprocessing via ear and eye: an fmri study.
cortex,47(4):416–431..jonathan brennan, yuval nir, uri hasson, rafaelmalach, david j heeger, and liina pylkk¨anen.
2012.syntactic structure building in the anterior temporallobe during natural story listening.
brain and lan-guage, 120(2):163–173..trevor brothers and gina r kuperberg.
2021. wordpredictability effects are linear, not logarithmic: im-plications for probabilistic models of sentence com-journal of memory and language,prehension.
116:104174..tom b brown, benjamin mann, nick ryder,melanie subbiah, jared kaplan, prafulla dhariwal,arvind neelakantan, pranav shyam, girish sastry,amanda askell, sandhini agarwal, ariel herbert-voss, gretchen krueger, tom henighan, rewonchild, aditya ramesh, daniel ziegler, jeffrey wu,clemens winter, chris hesse, mark chen, eric.
sigler, mateusz litwin, scott gray, benjamin chess,jack clark, christopher berner, and dario amodei.
2020. language models are few-shot learners.
inproceedings of advances in neural information pro-cessing systems 33..paul-christian b¨urkner.
2018. advanced bayesianmultilevel modeling with the r package brms.
rjournal, 10(1)..max coltheart, kathleen rastle, conrad perry, robynlangdon, and johannes ziegler.
2001. drc: a dualroute cascaded model of visual word recognition andreading aloud.
psychological review, 108(1):204..andreas damianou and neil d lawrence.
2013. deepin artiﬁcial intelligence and.
gaussian processes.
statistics, pages 207–215.
pmlr..vera demberg and frank keller.
2008. data from eye-tracking corpora as evidence for theories of syntacticprocessing complexity.
cognition, 109(2):193–210..janez demˇsar.
2006. statistical comparisons of clas-siﬁers over multiple data sets.
journal of machinelearning research, 7(jan):1–30..jacob devlin, ming-wei chang, kenton lee, andkristina toutanova.
2019. bert: pre-training of deepbidirectional transformers for language understand-ing.
naacl19..susan f ehrlich and keith rayner.
1981. contextualeffects on word perception and eye movements dur-ing reading.
journal of verbal learning and verbalbehavior, 20(6):641–655..kate erlich and keith rayner.
1983. pronoun assign-ment and semantic integration during reading: eyemovements and immediacy of processing.
journalof verbal learning & verbal behavior, 22:75–87..evelina fedorenko, po-jang hsieh, alfonso nieto-casta˜n´on, susan whitﬁeld-gabrieli, and nancykanwisher.
2010. new method for fmri investi-gations of language: deﬁning rois functionally injournal of neurophysiology,individual subjects.
104(2):1177–1194..victoria fossum and roger levy.
2012. sequentialvs. hierarchical syntactic models of human incre-in proceedings ofmental sentence processing.
{{cmcl}} 2012. association for computationallinguistics..stefan frank and rens bod.
2011. insensitivity of thehuman sentence-processing system to hierarchicalstructure.
psychological science..stefan l frank, irene fernandez monsalve, robin lthompson, and gabriella vigliocco.
2013. read-ing time data for evaluating broad-coverage modelsof english sentence processing.
behavior researchmethods, 45(4):1182–1190..3727lyn frazier and jerry d fodor.
1978. the sausage ma-chine: a new two-stage parsing model.
cognition,6:291–325..karl j friston, andrea mechelli, robert turner, andcathy j price.
2000. nonlinear responses in fmri:the balloon model, volterra kernels, and otherhemodynamics.
neuroimage, 12(4):466–477..richard futrell, edward gibson, harry j tily, idanblank, anastasia vishnevetsky, steven t piantadosi,and evelina fedorenko.
2020. the natural storiescorpus: a reading-time corpus of english texts con-taining rare syntactic constructions.
language re-sources and evaluation, pages 1–15..yarin gal and zoubin ghahramani.
2016. dropout as abayesian approximation: representing model uncer-tainty in deep learning.
in international conferenceon machine learning, pages 1050–1059.
pmlr..edward gibson.
2000. the dependency locality the-ory: a distance-based theory of linguistic complex-ity.
in alec marantz, yasushi miyashita, and wayneo’neil, editors, image, language, brain, pages 95–106. mit press, cambridge..adam goodkind and klinton bicknell.
2018. predic-tive power of word surprisal for reading times is ain pro-linear function of language model quality.
ceedings of the 8th workshop on cognitive modelingand computational linguistics (cmcl 2018), pages10–18..david graff, junbo kong, ke chen, and kazuakienglish gigaword third edition.
maeda.
2007.ldc2007t07..daniel j grodner and edward gibson.
2005. conse-quences of the serial nature of linguistic input.
cog-nitive science, 29:261–291..kristina gulordava, piotr bojanowski, ´edouard grave,tal linzen, and marco baroni.
2018. colorlessgreen recurrent networks dream hierarchically.
in proceedings of the 2018 conference of the northamerican chapter of the association for computa-tional linguistics: human language technologies,volume 1 (long papers), pages 1195–1205..john hale.
2001. a probabilistic earley parser as apsycholinguistic model.
in proceedings of the sec-ond meeting of the north american chapter of the as-sociation for computational linguistics, pages 159–166, pittsburgh, pa..caoimhe m harrington stack, ariel n james, and du-ane g watson.
2018. a failure to replicate rapidsyntactic adaptation in comprehension.
memory &cognition, 46(6):864–877..kaiming he, xiangyu zhang, shaoqing ren, and jiansun.
2016. deep residual learning for image recog-in proceedings of the ieee conference onnition.
computer vision and pattern recognition, pages 770–778..kenneth heaﬁeld, ivan pouzyrevsky, jonathan h clark,and philipp koehn.
2013. scalable modiﬁed kneser-ney language model estimation.
in proceedings ofthe 51st annual meeting of the association for com-putational linguistics, pages 690–696, soﬁa, bul-garia..john m henderson, wonil choi, matthew w lowder,and fernanda ferreira.
2016. language structure inthe brain: a ﬁxation-related fmri study of syntacticsurprisal in reading.
neuroimage, 132:293–300..john m henderson, wonil choi, steven g luke, andrutvik h desai.
2015. neural correlates of ﬁxationduration in natural reading: evidence from ﬁxation-related fmri.
neuroimage, 119:390–397..dan hendrycks and kevin gimpel.
2016. gaus-arxiv preprint.
sian error linear units (gelus).
arxiv:1606.08415..john hewitt and christopher d manning.
2019. astructural probe for ﬁnding syntax in word represen-in proceedings of the 2019 conference oftations.
the north american chapter of the association forcomputational linguistics: human language tech-nologies, volume 1 (long and short papers), pages4129–4138..sepp hochreiter and j¨urgen schmidhuber.
1997. longshort-term memory.
neural comput., 9(8):1735–1780..alexander g huth, wendy a de heer, thomas l grif-ﬁths, fr´ed´eric e theunissen, and jack l gallant.
2016. natural speech reveals the semantic maps thattile human cerebral cortex.
nature, 532(7600):453..sergey ioffe and christian szegedy.
2015. batch nor-malization: accelerating deep network training byreducing internal covariate shift.
in internationalconference on machine learning, pages 448–456..marcel adam just and patricia a carpenter.
1980. atheory of reading: from eye ﬁxations to comprehen-sion.
psychological review, 87(4):329–354..alan kennedy, james pynte, and robin hill.
2003.the dundee corpus.
in proceedings of the 12th eu-ropean conference on eye movement..diederik p kingma and jimmy ba.
2014. adam:corr,.
a method for stochastic optimization.
abs/1412.6..trevor hastie and robert tibshirani.
1986. general-ized additive models.
statist.
sci., 1(3):297–310..roger levy.
2008. expectation-based syntactic com-.
prehension.
cognition, 106(3):1126–1177..3728richard l lewis and shravan vasishth.
2005. anactivation-based model of sentence processing ascognitive science,skilled memory retrieval.
29(3):375–419..boris t polyak and anatoli b juditsky.
1992. ac-celeration of stochastic approximation by averag-ing.
siam journal on control and optimization,30(4):838–855..martin a lindquist, ji meng loh, lauren y atlas, andtor d wager.
2009. modeling the hemodynamic re-sponse function in fmri: efﬁciency, bias and mis-modeling.
neuroimage, 45(1, supplement 1):s187– s198..tal linzen, emmanuel dupoux, and yoav goldberg.
2016. assessing the ability of lstms to learnsyntax-sensitive dependencies.
transactions of theassociation for computational linguistics, 4:521–535..alessandro lopopolo, stefan l frank, antal denbosch, and roel m willems.
2017. using stochas-tic language models (slm) to map lexical, syntac-tic, and phonological information processing in thebrain.
plos one, 12(5):e0177794..mitchell p marcus, beatrice santorini, and mary annmarcinkiewicz.
1993. building a large annotatedthe penn treebank.
computa-corpus of english:tional linguistics, 19(2):313–330..don c mitchell.
1984. an evaluation of subject-pacedreading tasks and other methods for investigating im-mediate processes in reading.
new methods in read-ing comprehension research, pages 69–89..francis mollica and steve piantadosi.
2017. an in-cremental information-theoretic buffer supports sen-tence processing.
in proceedings of the 39th annualcognitive science society meeting..john morton.
1964..the effects of context uponspeed of reading, eye movements and eye-voicespan.
quarterly journal of experimental psychol-ogy, 16(4):340–354..luan nguyen, marten van schijndel, and williamschuler.
2012. accurate unbounded dependencyrecovery using generalized categorial grammars.
in proceedings of coling 2012..dennis norris.
2006. the bayesian reader: explain-ing word recognition as an optimal bayesian deci-sion process.
psychological review, 113(2):327..matthew e peters, mark neumann, mohit iyyer, mattgardner, christopher clark, kenton lee, and lukezettlemoyer.
2018. deep contextualized word repre-sentations.
arxiv preprint arxiv:1802.05365..vitali petsiuk, abir das, and kate saenko.
2018.rise: randomized input sampling for explanationof black-box models.
in proceedings of the britishmachine vision conference (bmvc)..martin j pickering and holly p branigan.
1998. therepresentation of verbs: evidence from syntacticpriming in language production.
journal of memoryand language, 39(4):633–651..grusha prasad and tal linzen.
2019. rapid syntacticadaptation in self-paced reading: detectable, but re-quires many participants..alec radford, jeffrey wu, rewon child, david luan,dario amodei, and ilya sutskever.
2019. languagemodels are unsupervised multitask learners.
openaiblog, 1(8):9..keith rayner.
1977. visual attention in reading: eyemovements reﬂect cognitive processes.
memory \&cognition, 5(4):443–448..keith rayner.
1998. eye movements in reading andinformation processing: 20 years of research.
psy-chological bulletin, 124(3):372–422..marco tulio ribeiro, sameer singh, and carlosguestrin.
2016.
”why should i trust you?” explain-in proceed-ing the predictions of any classiﬁer.
ings of the 22nd acm sigkdd international con-ference on knowledge discovery and data mining,pages 1135–1144..marten van schijndel, andy exley, and williamschuler.
2013. a model of language processing ashierarchic sequential prediction.
topics in cognitivescience, 5(3):522–540..marten van schijndel and william schuler.
2013. ananalysis of frequency- and memory-based process-ing costs.
in proceedings of naacl-hlt 2013. as-sociation for computational linguistics..marten van schijndel and william schuler.
2015. hier-archic syntax improves reading time prediction.
inproceedings of naacl-hlt 2015. association forcomputational linguistics..martin schrimpf, idan a blank, greta tuckute, ca-rina kauf, eghbal a hosseini, nancy g kanwisher,joshua b tenenbaum, and evelina fedorenko.
2020.artiﬁcial neural networks accurately predict lan-guage processing in the brain.
biorxiv..cory shain, idan blank, marten van schijndel, williamschuler, and evelina fedorenko.
2020.fmri re-veals language-speciﬁc predictive coding during nat-uralistic sentence comprehension.
neuropsycholo-gia, 138..cory shain and william schuler.
2018. deconvolu-tional time series regression: a technique for model-ing temporally diffuse effects.
in proceedings of the2018 conference on empirical methods in naturallanguage processing..cory shain and william schuler.
2021. continuous-time deconvolutional regression for psycholinguis-tic modeling.
cognition..3729christopher a sims.
1971. discrete approximationsto continuous time distributed lags in econometrics.
econometrica: journal of the econometric society,pages 545–563..nathaniel j smith and roger levy.
2013. the effect ofword predictability on reading time is logarithmic.
cognition, 128:302–319..nitish srivastava, geoffrey hinton, alex krizhevsky,ilya sutskever, and ruslan salakhutdinov.
2014.dropout: a simple way to prevent neural networksfrom overﬁtting.
the journal of machine learningresearch, 15(1):1929–1958..adrian staub.
2015. the effect of lexical predictabilityon eye movements in reading: critical review andtheoretical interpretation.
language and linguisticscompass, 9(8):311–327..michael k tanenhaus, michael j spivey-knowlton,kathleen m eberhard, and julie c e sedivy.
1995.integration of visual and linguistic informa-tion in spoken language comprehension.
science,268:1632–1634..ian tenney, dipanjan das, and ellie pavlick.
2019.the classical nlp pipeline..bert rediscoversacl19..shravan vasishth and richard l lewis.
2006.argument-head distance and processing complex-ity: explaining both locality and antilocality effects.
language, 82(4):767–794..alberto l vazquez, eric r cohen, vikas gulani, luishernandez-garcia, ying zheng, gregory r lee,seong-gi kim, james b grotberg, and douglas cnoll.
2006. vascular dynamics and bold fmri:cbf level effects and analysis considerations.
neu-roimage, 32(4):1642–1655..tor d wager, alberto vazquez, luis hernandez, anddouglas c noll.
2005. accounting for nonlinearbold effects in fmri: parameter estimates and amodel for prediction in rapid event-related studies.
neuroimage, 25(1):206–218..leila wehbe, idan a blank, cory shain, richardfutrell, roger levy, titus von der malsburg,nathaniel smith, edward gibson, and evelina fe-dorenko.
2020.incremental language comprehen-sion difﬁculty predicts activity in the language net-work but not the multiple demand network.
biorxiv..ethan wilcox, roger levy, and richard futrell.
2019.hierarchical representation in neural languagemodels: suppression and recovery of expectations.
in proceedings of the 2019 acl workshop black-boxnlp: analyzing and interpreting neural net-works for nlp, pages 181–190..roel m willems, stefan l frank, annabel d nijhof,peter hagoort, and antal den bosch.
2015. predic-tion during natural language comprehension.
cere-bral cortex, 26(6):2506–2516..datasetsynthnatstor (spr)dundeenatstor (fmri).
cdr66221,8452,080331.cdrnn-ff7,33022,5466,87013,834.cdrnn-rnn17,05840,40814,83826,058.table a1: number of trainable parameters by modeland dataset..simon n wood.
2006. generalized additive models:an introduction with r. chapman and hall/crc,boca raton..a mathematical deﬁnition.
this appendix formally deﬁnes the cdrnn model.
cdrnn assumes the following quantities as in-put:9.
• x ∈ n: number of predictor observations.
(e.g.
word exposures).
• y ∈ n: number of response observations.
(e.g.
fmri scans).
• z ∈ n: number of random grouping factor.
levels (e.g.
distinct participants).
• k ∈ n: number of predictors• x ∈ rx×k: design matrix of x predictor.
observations of k dimensions each..• y ∈ ry : vector of y response observations• z ∈ {0, 1}y ×z: boolean matrix indicatingrandom grouping factor levels associated witheach response observation.
• t ∈ rx : vector of timestamps associated.
with each observation in x.
• t(cid:48) ∈ ry : vectors of timestamps associated.
with each observation in y.
• s ∈ n: number of parameters in predictivedistribution (e.g.
2 for a normal distribution:mean and variance).
for simplicity of exposition, x and y are assumedto contain data from a single time series (e.g.
asingle participant performing a single experiment)..9throughout these deﬁnitions, vectors and matrices arenotated in bold lowercase and uppercase, respectively (e.g.
u, u).
objects with indexed names are designated usingsubscripts (e.g.
vr).
vector and matrix indexing operations arenotated using subscript square brackets, and slice operationsare notated using ∗ (e.g.
x[∗,k] denotes the kth column ofmatrix x).
hadamard (pointwise) products are notated using(cid:12).
the notations 0 and 1 designate conformable columnvectors of 0’s and 1’s, respectively.
superscripts are used forindexation and do not denote exponentiation..3730the deﬁnition below can be applied without loss ofgenerality to data containing multiple time seriesby concatenating the output of the model as appliedto multiple x, y pairs.
x, y and their associatedsatellite data z, t, t(cid:48) must be temporally sorted..given these inputs, cdrnn estimates a latentimpulse response function that relates timestampedpredictors to all parameters of the assumed predic-tive distribution.
for example, assuming a univari-ate normally distributed response, cdrnn learnsan irf with two output dimensions, one for thepredictive mean, and one for the predictive vari-ance.
regressing all parameters of the predictivedistribution in this way has previously been calleddistributional regression (b¨urkner, 2018)..cdrnn contains a recurrent neural network(rnn), neural projections that map inputs andrnn states to a hidden state for each precedingevent, and neural projections that map the hiddenstates to predictions about (1) the inﬂuence of eachevent on the response (irf) and (2) the parame-ter(s) of the error distribution (e.g.
the varianceof a gaussian error).
the deﬁnition assumes thefollowing quantities:.
• lin, lrnn, lirf ∈ n: number of layers in theinput projection, rnn, and irf, respectively• din((cid:96)), drnn((cid:96)), dh, dirf((cid:96)) ∈ n: numberof output dimensions in the (cid:96)th layer of theinput projection, rnn, hidden state, and irf,respectively.
the following values are deterministically as-signed:.
• dirf(lirf) = s(k + 1) (the irf generates aconvolution weight for every predictor dimen-sion, plus the timestamp, for each parameterof the predictive distribution).
• din(0) = k + 1 (input is predictors + time)• din(lin) = dh.
(cid:1)(cid:62).
in these deﬁnitions, integers x, y respectivelyrefer to row indices of x, y. let zy be the vector(cid:0)z[y,∗]of random effects associated with the re-sponse at y. let wh,z ∈ rdh×z, wirf(1),z ∈r2dirf(1)×z, and ws,z ∈ rs×z be an embed-ding matrix for zy.
random effects offsets at re-sponse step y for the hidden state (hzy ), the weightsand biases of the ﬁrst layer of the irf (wirf(1),z,birf(1),z), and the parameters of the predictive dis-ytribution (ezy , i.e.
random intercepts and variance.
y.parameters) are generated as follows:.
(cid:34).
hzy(cid:35)wirf(1),zbirf(1),zy.y.def= wh,zzy.
def= wirf(1),zzy.
szy.def= ws,zzy.
(2).
(3).
(4).
following prior work in mixed effects models(bates et al., 2015), to ensure that population-levelestimates reliably encode central tendency, eachoutput dimension of wh,z, wirf(1),z, and ws,zis constrained to have mean 0 across the levels ofeach random grouping factor (e.g.
across partici-pants in the study)..the neural irf is applied to a temporal offsetτ representing the delay at which to query the re-sponse to an input (e.g.
τ = 1 queries the responseto an input 1s after the input occurred).
the outputx,y(τ ) ∈ rdirf((cid:96)) applied to τ atof the neural irf g(cid:96)layer (cid:96) is deﬁned as:.
x,y(τ ) def= sirf(1)g(1)x,y(τ ) def= sirf((cid:96))g((cid:96)).
(cid:16).
(cid:16).
wirf(1)x,y.
τ + birf(1).
x,y.
wirf((cid:96))g((cid:96)−1).
x,y.
(cid:17).
(5)(τ ) + birf((cid:96))(cid:17)(6).
,.
(cid:96) > 1.wirf(1)x,y.
def= wirf(1) + wirf(1),z.y.
+ wirf(1)∆.
hx,y.
birf(1)x,y.
def= birf(1) + birf(1),z.y.
+ birf(1)∆.
hx,y.
(7).
(8).
, birf((cid:96))x,y.
wirf((cid:96)), and sirf((cid:96)) are respectively thex,y(cid:96)th irf layer’s weight matrix at predictor timestepx and response timestep y, bias vector at timex, y, and squashing function, and g(0)x,y(τ ) = τ .
wirf(1), birf(1) are respectively globally appliedinitial weight and bias vectors for the ﬁrst layerof the irf, which transforms scalar τ , each ofwhich is shifted by its corresponding random ef-fects.
wirf(1)are respectively weightmatrices used to compute additive modiﬁcationsto wirf(1) from cdrnn hidden state hx,y, simi-lar in spirit to a residual network (he et al., 2016).
non-initial irf layers are treated as stationary (i.e.
their parameters are independent of x, y).
the ﬁnaloutput of the irf is given by:.
, birf(1)∆.
∆.
gx,y(τ ) def= reshape.
(cid:16).
g(lirf)x,y.
(τ ), (s, k + 1).
(9).
(cid:17).
3731the hidden state hx,y is computed as thesquashed sum of several quantities: a global biashbias, random effects hz, a neural projection hinx,yof the inputs at x, y, and a neural projection hrnnx,yof the hidden state of an rnn over the sequence ofpredictors up to and including timestep x:.
hx,y.
def= sh.
(cid:0)hbias + hz.
y + hin.
x,y + hrnnx,y.
(cid:1).
(10).
the irf gx,y is therefore feature-dependent viathe neural projection hinx,y of the input at x, y andcontext-dependent via the neural projection hrnnx,yof an rnn over the input up to x for the responseat y. this design relaxes stationarity assumptionswhile also sharing structure across timepoints.
thedeﬁnitions of hin.
x,y are given below..x,y and hrnn.
(cid:1)(cid:62).
let tx be the element t[x] and xx be the xth pre-dictor vector (cid:0)x[x,∗]x,y to thecdrnn model are deﬁned as the vertical concate-nation of the predictors xx and the event timestamptx:.
.
the inputs h(0).
h(0)x,y.
def=.
(cid:21).
(cid:20)xxtx.
(11).
the output of the input projection at layer l andtime x, y is deﬁned as:.
hin((cid:96))x,y.
def= sin((cid:96)).
win((cid:96))hin((cid:96)−1).
x,y.
(cid:16).
+ bin((cid:96))(cid:17).
(12).
def= h(0).
where hin(0)x,y.
at the ﬁnal layer, sin(lin) isx,yidentity and bin(lin) = 0, since hx,y already hasa bias.
the ﬁnal output of the input projection isgiven by:.
hinx,y.
def= hin(lin)x,y.
(13).
note that hin.
x,y is already non-stationary by virtueof its dependence on the event timestamp t[x],which allows the irf to differ between timepoints(see e.g.
baayen et al., 2017, for development ofa similar idea using generalized additive models).
while this model of non-stationarity can be com-plex and non-linear, it is still limited by context-independence.
that is, the change in the irf overtime depends only on the amount of time elapsedsince the start of the time series, independently ofwhich events preceded.
however, it is possible thatthe contents of the events in a time series may in-ﬂuence the irf, above any deterministic change inresponse over time (for example, if several difﬁcultpreceding words have already taxed the process-ing buffer, additional processing costs may becomelarger).
to account for this possibility, an rnn.
is built into the cdrnn design.10 any variantof rnn can be used (this study uses a long short-term memory network, or lstm, hochreiter andschmidhuber, 1997).
the (cid:96)th rnn hidden stateat x, y is designated by hrnn((cid:96)).
to account forthe possibility of random variation in sensitivity tocontext, the initial hidden and cell states hrnn((cid:96)),crnn((cid:96))depend on the random effects:0,y.
x,y.
0,y.
hrnn((cid:96))0,ycrnn((cid:96))0,y.
def= hrnn((cid:96))0def= crnn((cid:96))0.
+ wrnnh((cid:96))z+ wrnnc((cid:96))z.zy.
zy.
(14).
(15).
where hrnn((cid:96)), crnn((cid:96))00wrnnh((cid:96)), wrnnc((cid:96))zz0 within each random grouping factor..are global biases andare constrained to have mean.
non-initial rnn states are computed via a stan-.
dard lstm update:.
(cid:104).
hrnn((cid:96))x,y.
, crnn((cid:96))x,y.
(cid:16).
hrnn((cid:96))x−1,y ,.
(cid:105) def= lstmcrnn((cid:96))x−1,y , hrnn((cid:96)−1)x,y.
(16)(cid:17).
the hidden state of the ﬁnal rnn layer is linearlyprojected to the dimensionality of the cdrnn hid-den state:.
hrnnx,y.
def= wrnnprojhrnn(lrnn).
x,y.
(17).
to apply the cdrnn model to data, a maskf ∈ {0, 1}y ×x admits only those observations inx that precede each y[y]:.
f[y,x].
def=.
(cid:40).
1 t[x] ≤ t(cid:48)[y]0 otherwise.
(18).
letting τx,y denote the temporal offset between thedef=predictors at x and the response at y, i.e.
τx,yt(cid:48)[y] − t[x].
a total of s(k + 1) sparse convolutionmatrices gs,k ∈ ry ×x are deﬁned to contain thepredicted response to each preceding event for thekth dimension of h(0)x,y and the sth parameter of thepredictive distribution, masked by f:.
gs,k.
def=.
.
.
g1,1(τ1,1)[s,k]...g1,y (τ1,y )[s,k].
· · ·.
.
.
· · ·.
gx,1(τx,1)[s,k]...gx,y (τx,y )[s,k].
.
(cid:12)f.(19).
10the experiments in this study also consider a variant with-out the rnn component, which is mathematically equivalentto setting hrnn.
x,y = 0..3732the convolved design matrix x(cid:48)(s) ∈ ry ×(k+1)for the sth parameter of the predictive distributionis then computed as:.
x(cid:48)(s)[∗,k].
def= gs,k [x, t][∗,k].
(20).
vector s ∈ rs contains global, population-levelestimates of the parameters of the predictive dis-tribution.
under the univariate normal predictivedistribution assumed in this study, s contains thepredictive mean (µ, i.e.
the intercept) and variance(σ2):.
matrix sz contains random predictive distributionparameter estimates for the yth response szy :.
s def=.
(cid:21).
(cid:20) µσ2.
sz def=.
(cid:62).
.
.
(cid:62).
.
.
sz1...szy.
(21).
(22).
the vector of values for each response y for the sthpredictive distribution parameter is given by sum-ming the population value, random effects values,and convolved response values:.
s[∗,s].
def= fconstraint(s).
x(cid:48)(s)1 + sz.
[∗,s] + s[s].
(cid:16).
(cid:17).
(23)where fconstraint(s) enforces any required constraintson the sth parameter of the predictive distribution.
in the gaussian predictive distribution assumedhere, fconstraint(1) (the constraint function for themean) is identity and fconstraint(2) (the constraintfunction for the variance) is the softplus bijection:.
softplus(x) def= ln(ex + 1).
(24).
given an assumed distributional family f (hereassumed to be univariate normal), the response inthe cdrnn model is distributed as:.
y ∼ f (cid:0)s[∗,1], .
.
.
, s[∗,s].
(cid:1).
(25).
b asynchronously measured predictor.
dimensions.
as discussed in shain and schuler (2018, 2021),cdr applies straightforwardly to time series withasynchronous predictor vectors and response val-ues (i.e.
measured at different times, such as wordonsets that do not align with fmri scan times).
the cdr implementation of shain and schuler.
(2021) also supports asynchronously measured di-mensions of the predictor matrix, simply by provid-ing each predictor dimension with its own vector oftimestamps.
this allows e.g.
shain et al.
(2020) toregress linguistic features (which are word-aligned)and sound power (which in their deﬁnition is mea-sured at regular 100ms intervals) in the same model.
supporting asynchronously measured predictor di-mensions is more challenging in cdrnn, espe-cially if the rnn component is used.
the solutionused in cdr is not available because input dimen-sions that do not align in time are (1) arbitrarilygrouped together and (2) erroneously treated assteps in the rnn input sequence.
a more princi-pled solution is to interleave the predictors in timeorder and pad irrelevant dimensions with zeros.
forexample, in a model with predictor a and predictorb that are sampled at different times, the valuesof a and b are temporally sorted together into asingle time series, with the b value of a events setto zero and the a value of b events set to zero.
thisapproach carries a computational cost: unlike cdr,the number of inputs to the convolution scales lin-early on the number of asynchronously measuredsets of predictors in the model..c predictors.
the following predictors are common to all modelspresented here:.
• rate (cdr/nn only): the deconvolutionalintercept, i.e.
the base response to a stimulus,independent of its features.
in cdr, rate is es-timated explicitly by ﬁtting an irf to interceptvector (shain and schuler, 2021) (i.e., implic-itly, the response when all predictors are 0).
in cdrnn, rate is a reference response, com-puted by taking the response to an averagestimulus (since the zero vector may unlikelyfor a given input distribution, using it as areference may not reliably reﬂect the model’sdomain knowledge).
in this study, all otherirf queries subtract out rate in order to showdeviation from the reference..• unigram surprisal: the negative log of thesmoothed context-independent probability ofa word according to a unigram kenlm model(heaﬁeld et al., 2013) trained on gigaword 3(graff et al., 2007).
while this quantity is typi-cally treated on a frequency or log probabilityscale in psycholinguistics, it is treated here on.
3733a surprisal (negative log prob) scale simplyfor easy of comparison with 5-gram surprisal(below), even though it is not a good estimateof the quantity typically targeted by surprisal(contextual predictability), since context is ig-nored..• 5-gram surprisal: the negative log of thesmoothed probability of a word given thefour preceding words according to a 5-gramkenlm model (heaﬁeld et al., 2013) trainedon gigaword 3 (graff et al., 2007)..the following predictor is used in all reading mod-els:.
• word length: the length of the word in char-.
acters..the following predictors are used in eye-trackingmodels:.
• saccade length: the length in words of theincoming saccade (eye movement), includingthe current word..• previous was ﬁxated: indicator for whetherthe most recent ﬁxation was to the immedi-ately preceding word..replications of shain et al.
(2020) use the follow-ing additional predictors:.
• pcfg surprisal: lexicalized probabilisticcontext-free grammar surprisal computed us-ing the incremental left-corner parser of vanschijndel et al.
(2013) trained on a general-ized categorial grammar (nguyen et al., 2012)reannotation of wall street journal sections2 through 21 of the penn treebank (marcuset al., 1993)..• sound power: stimulus sound power (rootmean squared energy), averaged over 250msintervals.
this implementation differs slightlyfrom that of shain et al.
(2020), who sam-pled the measure every 100ms.
the longerinterval is designed to provide coverage overthe extent of the hrf in this study, whichuses a shorter history window for computa-tional reasons (128 timesteps instead of 256).
both for computational reasons, especially un-der cdrnn-rnn (appendix b) and becauseprior sound power estimates in this datasethave been weak (shain et al., 2020), soundpower is omitted from models used in themain comparison..3734