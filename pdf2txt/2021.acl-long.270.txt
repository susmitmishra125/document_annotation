novel slot detection: a benchmark for discovering unknown slot typesin the task-oriented dialogue system.
yanan wu1∗, zhiyuan zeng1∗, keqing he2∗, hong xu1yuanmeng yan1, huixing jiang2, weiran xu1∗1pattern recognition & intelligent system laboratory1beijing university of posts and telecommunications, beijing, china2meituan group, beijing, china{yanan.wu,zengzhiyuan,xuhong,yanyuanmeng,xuweiran}@bupt.edu.cn{hekeqing,jianghuixing}@meituan.com.
abstract.
existing slot ﬁlling models can only recognizepre-deﬁned in-domain slot types from a lim-ited slot set.
in the practical application, areliable dialogue system should know what itdoes not know.
in this paper, we introduce anew task, novel slot detection (nsd), in thetask-oriented dialogue system.
nsd aims todiscover unknown or out-of-domain slot typesto strengthen the capability of a dialogue sys-tem based on in-domain training data.
be-sides, we construct two public nsd datasets,propose several strong nsd baselines, and es-tablish a benchmark for future work.
finally,we conduct exhaustive experiments and quali-tative analysis to comprehend key challengesand provide new guidance for future direc-tions1..1.introduction.
slot ﬁlling plays a vital role to understand userqueries in personal assistants such as amazonalexa, apple siri, google assistant, etc.
it aimsat identifying a sequence of tokens and extractingsemantic constituents from the user queries.
givena large scale pre-collected training corpus, existingneural-based models (mesnil et al., 2015; liu andlane, 2015, 2016; goo et al., 2018; haihong et al.,2019; chen et al., 2019; he et al., 2020b,d; yanet al., 2020; louvan and magnini, 2020; he et al.,2020a) have been actively applied to slot ﬁlling andachieved promising results..existing slot ﬁlling models can only recognizepre-deﬁned entity types from a limited slot set,which is insufﬁcient in the practical applicationscenario.
a reliable slot ﬁlling model should notonly predict the pre-deﬁned slots but also detect po-tential unknown slot types to know what it doesn’t.
∗the ﬁrst three authors contribute equally.
weiran xu is.
the corresponding author..1https://github.com/chestnutwyn/acl20.
21-novel-slot-detection.
figure 1: an example of novel slot detection in thetask-oriented dialogue system.
without nsd, the dia-logue system gives the wrong response since it misun-derstands the unknown slot “is this my world” as the in-domain playlist type.
in contrast, nsd recognizes “isthis my world” as ns and the system gives a fallbackresponse.
meanwhile, with human-in-the-loop annota-tion, the system can increase its functions or skills..know, which we call novel slot detection (nsd) inthis paper.
nsd is particularly crucial in deployedsystems—both to avoid performing the wrong ac-tion and to discover potential new entity types forfuture development and improvement.
we displayan example as fig 1 shows..in this paper, we deﬁne novel slot (ns) as newslot types that are not included in the pre-deﬁnedslot set.
nsd aims to discover potential new orout-of-domain entity types to strengthen the capa-bility of a dialogue system based on in-domain pre-collected training data.
there are two aspects in theprevious work related to nsd, out-of-vocabulary(oov) recognition (liang et al., 2017a; zhao andfeng, 2018; hu et al., 2019; he et al., 2020c,d; yanet al., 2020; he et al., 2020e) and out-of-domain(ood) intent detection (lin and xu, 2019; lar-son et al., 2019; xu et al., 2020a; zeng et al.,2021b,a).
oov means many slot types can have alarge number of new slot values while the trainingset only obtains a tiny part of slot values.
oovaims to recognize unseen slot values in training set.
proceedingsofthe59thannualmeetingoftheassociationforcomputationallinguisticsandthe11thinternationaljointconferenceonnaturallanguageprocessing,pages3484–3494august1–6,2021.©2021associationforcomputationallinguistics3484agent: what can i do for you?user: play is this my world by leo arnaud.play is this my world by leo arnaud.playlistartistdialogue systemdm & nlg moduleis this my world is an unknown slot type (denoted as ns).
it’s the name of leo arnaud’s album.dialogue system without novel slot detectoragent: what can i do for you?user: play is this my world by leo arnaud.dialogue systemdm & nlg moduledialogue system with novel slot detectornlu moduleplay is this my world by leo arnaud.nsartistnovel slot detectorhuman annotatedcollectednovel slotsupdate dialogue systemagent: “is this my world” is probably a novel slot.
the current system can not handle it.agent: you don’t have a playlist called “is this my world”×√utteranceslot filling labelsnovel slot detection labels.
playoo.is.
this.
my.
world.
by.
leo.
b-album i-album i-album i-album o b-artisto b-artist.
ns.
ns.
ns.
ns.
arnaudi-artisti-artist.
table 1: comparison between slot ﬁlling and novel slot detection.
in the novel slot detection labels, we consider“album” as an unknown slot type that is out of the scope of the pre-deﬁned slot set.
meanwhile, “artist” belongingto in-domain slot types still needs to be recognized as the original slot ﬁlling task..for pre-deﬁned slot types, using character embed-ding (liang et al., 2017a), copy mechanism (zhaoand feng, 2018), few/zero-shot learning (hu et al.,2019; he et al., 2020e; shah et al., 2019), trans-fer learning (chen and moschitti, 2019; he et al.,2020c,b) and background knowledge (yang andmitchell, 2017; he et al., 2020d), etc.
comparedto oov recognition, our proposed novel slot detec-tion task focuses on detecting unknown slot types,not just unseen values.
nsd faces the challenges ofboth oov and no sufﬁcient context semantics (seeanalysis in section 6.2), greatly increasing the com-plexity of the task.
another line of related workis ood intent detection (hendrycks and gimpel,2017; lee et al., 2018; lin and xu, 2019; ren et al.,2019; zheng et al., 2020; xu et al., 2020a) whichaims to know when a query falls outside the rangeof predeﬁned supported intents.
the main differ-ence is that nsd detects unknown slot types in thetoken level while ood intent detection identiﬁesout-of-domain intent queries.
nsd requires a deepunderstanding of the query context and is prone tolabel bias of o (see analysis in section 5.3.1), mak-ing it challenging to identify unknown slot types inthe task-oriented dialog system..in this paper, we ﬁrst introduce a new and im-portant task, novel slot detection (nsd), in thetask-oriented dialogue system (section 2.2).
nsdplays a vital role in avoiding performing the wrongaction and discovering potential new entity typesfor the future development of dialogue systems.
then, we construct two public nsd datasets, snips-nsd and atis-nsd, based on the original slotﬁlling datasets, snips (coucke et al., 2018) andatis (hemphill et al., 1990) (section 2.2).
fromthe perspective of practical application, we con-sider three kinds of dataset construction strategies,replace, mask and remove.
replace denotes welabel the novel slot values with all o in the train-ing set.
mask is to label with all o and mask thenovel slot values.
remove is the most strict strat-egy where all the queries containing novel slotsare removed.
we dive into the details of the threedifferent construction strategies in section 3.2 andperform a qualitative analysis in section 5.3.1. be-.
sides, we propose two kinds of evaluation metrics,span-level f1 and token-level f1 in section 3.4,following the slot ﬁlling task.
span f1 consid-ers the exact matching of a novel slot span whiletoken f1 focuses on prediction accuracy on eachword of a novel slot span.
we discuss performancecomparison between the two metrics and proposea new metric, restriction-oriented span evaluation(rose), to combine the advantages of both in sec-tion 5.3.3. then, we establish a fair benchmarkand propose extensive strong baselines for nsd insection 4. finally, we perform exhaustive experi-ments and qualitative analysis to shed light on thechallenges that current approaches faced with nsdin section 5.3 and 6..our contributions are three-fold: (1) we intro-duce a novel slot detection (nsd) task in thetask-oriented dialogue system.
nsd helps avoidperforming the wrong action and discovering po-tential new entity types for increasing functionsof dialogue systems.
(2) we construct two publicnsd datasets and establish a benchmark for futurework.
(3) we conduct exhaustive experiments andqualitative analysis to comprehend key challengesand provide new guidance for future nsd work..2 problem formulation.
2.1 slot filling.
given a sentence x = {x1, ..., xn} with n tokens,the slot ﬁlling task is to predict a corresponding tagsequence y = {y1, ..., yn} in bio format, whereeach yi can take three types of values: b-slot type,i-slot type and o, where “b” and “i” stand for thebeginning and intermediate word of a slot and “o”means the word does not belong to any slot.
here,slot ﬁlling assumes yi ∈ y, where y denotes apre-deﬁned slot set of size m. current approachestypically model slot ﬁlling as a sequence labelingproblem using rnn (liu and lane, 2015, 2016;goo et al., 2018) or pre-trained language models(chen et al., 2019)..2.2 novel slot detection.
we refer to the above training data d as in-domain(ind) data.
novel slot detection aims to identify.
3485original utteranceoriginal slot filling labels.
strategy.
mask.
replace.
remove.
b-album i-album i-album i-album o b-artist.
is.
by.
my.
this.
iso.world.
playoplayoplay mask mask mask mask byo--.
worldo.thiso.myo.o--.
o--.
o--.
o--.
leo.
byo b-artist.
o b-artist--.
--.
leo.
leo.
arnaudi-artistarnaudi-artistarnaudi-artist--.
table 2: comparison between three processing strategies in the training set.
we consider “album” as an unknownslot type and “-” denotes the sentence is removed from the training data..unknown or out-of-domain (ood) slot types viaind data while correctly labeling in-domain data.
we denote unknown slot type as ns and in-domainslot types as ind in the following sections.
notethat we don’t distinguish between b-ns and i-nsand unify them as ns because we empirically ﬁndexisting models hardly discriminate b and i for anunknown slot type.
we provide a detailed analysisin section 5.3.3. we show an example of nsd intable 1. the challenges of recognizing nsd comefrom two aspects, o tags and in-domain slots.
onthe one hand, models need to learn entity infor-mation for distinguishing ns from o tags.
on theother hand, they require discriminating ns fromother slot types in the pre-deﬁned slot set.
weprovide a detailed error analysis in section 6.1..3 dataset.
since there are not existing nsd datasets, we con-struct two new datasets based on the two widelyused slot ﬁlling datasets, snips (coucke et al.,2018) and atis (hemphill et al., 1990).
we ﬁrstbrieﬂy introduce snips and atis, then elaborate ondata construction and processing in detail, and dis-play the statistic of our nsd datasets, snips-nsdand atis-nsd.
finally, we deﬁne two evaluationmetrics for the nsd task, span f1 and token f1..3.1 original slot filling datasetssnips2 is a custom intent engine dataset.
it orig-inally has 13,084 train utterances, 700 and 700test utterances.
atis3 contains audio recordingsof people making ﬂight reservations.
it originallyhas 4,478 train utterances, 500 dev and 893 testutterances.
the full statistic is shown in table 3.note that the vocabulary only contains words inthe training set, and test set words that do not existin the vocabulary are referred to oov words.
thepercentage of oov words represents the portion ofoov words in the test set..2https://github.com/sonos/nlu-.
benchmark/tree/master/2017-06-custom-intent-engines.
3https://github.com/yvchen/jointslu/tree/master/data.
vocabulary sizepercentage of oov wordsnumber of slotstraining set sizedevelopment set sizetesting set size.
snips11,2415.95% 0.77%.
atis722.
3913,084700700.
794,478500893.table 3: statistics of atis and snips datasets..3.2 data construction and processing.
for snips and atis datasets, we keep some slotclasses in training as unknown and integrate themback during testing, following (fei and liu, 2016;shu et al., 2017; lin and xu, 2019).
we randomlyselect part of slot types in snips and atis as un-known slots(5%, 15%, and 30% in this paper).
note that the original train/val/test split is ﬁxed.
considering class imbalance, we perform weightedsampling where the chosen probability is relevantto the number of class examples similar to (lin andxu, 2019).
to avoid randomness of experimentresults, we report the average result over 10 runs..after we choose the unknown slot types, a criti-cal problem is how to handle sentences includingthese unknown slot types in training set.
for oodintent detection, we just need to remove these sen-tences in training and validation set.
however, fornovel slot detection, a sentence perhaps containsboth in-domain slots and unknown slots, whichis nontrivial for tackling unknown slots at the to-ken level.
we need to balance the performanceof recognizing unknown slots and in-domain slots.
therefore, we propose three different processingstrategies as follows: (1) replace: we label theunknown slot values with all o in the training setwhile the original values remain unchanged.
(2)mask: we label the unknown slot values with allo and mask these slot values with a special tokenmask.
(3) remove: all the sentences containingunknown slots are directly removed..we display examples of the above three strate-gies in table 2. for the val and test set, we justlabel the unknown slot values with all ns whilekeeping the in-domain labeling ﬁxed.
note that ns.
3486snips-nsd-15%number of in-domain slotsnumber of unknown slotspercentage of oov wordsnumber of queriesnumber of queriesincluding unknown slotsnumber of slot valuesnumber of unknown slot values.
train336-9,329.
0.val336-700.
192.
23,1760.
1,794210.test3368.51%700.
202.
1,790220.table 4: the detailed statistics of snips-nsd-15%..tags only exist in the val and test set, not in the train-ing set.
besides, we keep original in-domain slotsﬁxed to evaluate the performance of both ns andin-domain slots.
we aim to simulate the practicalscenario where we can hardly know what unknownslots are.
these three strategies all have its practi-cal signiﬁcance.
compared with others, removeis the most suitable strategies for real-world sce-narios.
in practical scenario, dialog systems ﬁrsttrain in the data set labeled by human annotators,and then applied to the actual application.
in theprocess of interaction with the real users, novelslot types appear gradually.
therefore, we considerthat the training set doesn’t contain potential novelslots sentences.
in other words, remove is themost suitable strategy for nsd in real applications.
what’s more, section 5.3.1 demonstrates removeperforms best while the others suffer from severemodel bias by o tags.
therefore, we adopt removeas the main strategy in this paper..3.3 statistic of new nsd datasets.
table 4 shows the detailed statistics of snips-nsd-15% constructed by remove strategy, where wechoose 15% classes in the training data as unknownslots.
4 combining table 3 and table 4, we canﬁnd remove strategy removes 28.70% of queriesin the original snips training set, hence increasesthe percentage of oov word from 5.95% to 8.51%.
and unknown slot values account for 12.29% oftotal slot values in the test set..3.4 metrics.
the traditional slot ﬁlling task uses span f1 5 forevaluation.
span f1 considers the exact span match-ing of an unknown slot span.
however, we ﬁnd insection 5.3.3 that this metric is too strict to nsd.
4since different proportions of unknown slots have differ-ent statistics, here we only display the results of snips-nsd-15% for brevity..5https://www.clips.uantwerpen.be/conl.
l2000/chunking/conlleval.txt.
figure 2: the overall architecture of our approach..models.
in the practical application, we only needto coarsely mine parts of words of unknown slots,then send these queries containing potential un-known slot tokens to human annotators, which haseffectively reduced extensive labor and improvedefﬁciency.
therefore, we deﬁne a more reasonablemetric, token f1 which focuses on the word-levelmatching of a novel slot span.
we also propose anew metric, restriction-oriented span evaluation(rose), for a fair comparison in section 5.3.3..4 methodology.
in this section, we introduce the nsd models pro-posed in this paper and illustrate the differencesbetween the various parallel approaches during thetraining and test stage..4.1 overall framework.
the overall structure of model is shown in fig 2. inthe training stage, we either train a multiple-classclassiﬁer or binary classiﬁer using different train-ing objectives.
we use public bert-large (devlinet al., 2019) embedding layer and bilstm-crf(huang et al., 2015) for token level feature extrac-tion.
then, in the test stage, we use the typicalneural multiple classiﬁer to predict the in-domainslot labels.
meanwhile, we use the detection algo-rithm, msp or gda to ﬁgure out novel slot tokens.
finally, we override the slot token labels whichare detected as ns.
in terms of training objectives,detection algorithms, and distance strategies, wecompare different variants as follows.
training objective.
for in-domain slots, we pro-pose two training objectives.
multiple classiﬁerrefers to the traditional slot ﬁlling objective setting,which performs token-level multiple classiﬁcationson the bio tags (ratinov and roth, 2009) com-bined with different slots.
binary classiﬁer uniﬁesall non-o tags into one class, and the model makes.
3487contextual encoderembedding layerplayisthismyworldbyleoarnaud......softmaxlayermsp/gdain-domainslot typesnovel slot typestrainingtestdetection method.
distance strategy span f1 span f1 token f1 span f1 span f1 token f1 span f1 span f1 token f1.
models.
objectivebinarymultiplebinary+multiplebinarybinarymultiplemultiple.
---differenceminimumdifferenceminimum.
ind.
87.2188.0589.5987.9561.2993.1493.10.
5%.
nsd.
12.3414.0423.5823.8310.3629.7331.67*.
25.1630.5037.5535.8317.0845.9946.97*.
15%.
12.3120.9724.7022.0616.9131.9632.19.nsd.
ind.
nsd.
30%.
8.7325.2630.6632.5015.5636.1638.64*.
40.3846.9152.1044.1333.7854.5555.24*.
39.5040.0245.3243.9931.1053.0253.75*.
58.8878.5279.0878.7248.0785.5686.26*.
table 5: ind and nsd results with different proportions (5%, 15% and 30%) of classes are treated as unknownslots on snips-nsd.
* indicates the signiﬁcant improvement over all baselines (p < 0.05)..detection method.
distance strategy span f1 span f1 token f1 span f1 span f1 token f1 span f1 span f1 token f1.
models.
objectivebinarymultiplebinary+multiplebinarybinarymultiplemultiple.
ind.
nsd.
5%.
19.7327.1532.4927.0215.9047.78*41.74.
29.6331.1643.4834.2120.9651.54*45.91.
---differenceminimumdifferenceminimum.
92.0494.3394.4193.6993.5795.2095.31*.
15%.
nsd.
23.4039.8841.2330.5124.5350.92*43.78.
33.8942.2943.1336.3027.2652.24*46.18.ind.
80.4987.6390.1488.7388.2192.0291.67.
30%.
nsd.
21.8840.4241.7630.9126.4051.26*45.44.
39.1747.6451.8745.6439.8356.59*52.37.table 6: ind and nsd results with different proportions (5%, 15% and 30%) of classes are treated as unknownslots on atis-nsd.
* indicates the signiﬁcant improvement over all baselines (p < 0.05)..ind.
71.4479.7183.7283.6549.1190.0790.18.ind.
91.7492.5493.2992.1390.9893.9293.88.msp.
gda.
msp.
gda.
a token-level binary classiﬁcation of o or non-oon the sequence.
note that in the test stage, for in-domain prediction, we both use the multiple clas-siﬁer.
while, for novel slot detection, we use themultiple classiﬁer, or the binary classiﬁer, or bothof them.
in table 5 and table 6, binary+multiplemeans the token will be labeled as ns only if bothclassiﬁers predict it as ns.
detection algorithm.
msp and gda are detec-tion algorithms in the test stage.
msp (maxi-mum softmax probability) (hendrycks and gim-pel, 2017) applies a threshold on the maximumsoftmax probability, if the maximum falls belowthe threshold, the token will be predicted to bea novel slot token.
gda (gaussian discriminantanalysis) (xu et al., 2020a) is a generative distance-based classiﬁer for out-of-domain detection witheuclidean space.
we treat tokens not belonging toany in-domain slots (including o) as novel slot to-kens for both methods.
for example, with a binaryclassiﬁer, if the softmax probabilities belonging too or non-o are both lower than an msp threshold,then the token is labeled as ns.
distance strategy.
the gda detection is basedon the distances between a target and each slotrepresentation cluster.
in original gda, when theminimum distance is greater than a certain thresh-old, it is predicted to be novel slots.
we proposea novel strategy named difference, which uses themaximum distance minus the minimum distance,when the difference value of a target is less thana threshold, it is predicted as novel slots.
both.
of their thresholds are obtained by optimizing thensd metrics on the validation set..5 experiment and analysis.
5.1.implementation details.
we use the public pre-trained bert-large-uncasedmodel to embed tokens which has 24 layers, 1024hidden states, 16 heads and 336m parameters.
thehidden size for the bilstm layer is set to 128.adam is used for optimization with an initial learn-ing rate of 2e-5.
the dropout value is ﬁxed as 0.5,and the batch size is 64. we train the model onlyon in-domain labeled data.
the training stage hasan early stopping setting with patience equal to10. we use the best f1 scores on the validationset to calculate the msp and gda thresholds adap-tively.
each result of the experiments is tested for10 times under the same setting and reports the av-erage value.
the training stage of our model lastsabout 28 minutes on single tesla t4 gpu(16 gbof memory)..5.2 main results.
table 5 and 6 show the experiment results withseven different models on two benchmark slotﬁlling datasets snips-nsd and atis-nsd con-structed by remove strategy.
we both report nsdand ind results using span f1 and token f1.
wecompare these models from three perspectives, de-tection method, objective and distance strategy inthe following.
the analysis of effect of the propor-.
34885%.
nsd.
strategy.
replacemaskremove.
indspan94.5290.0893.10.span1.9323.1031.67.indtoken span94.335.2786.5237.9146.9790.18.
15%.
span0.6625.0732.19.nsd.
indtoken span94.022.2983.3745.9253.7586.26.
30%.
span0.2732.1438.64.nsd.
token0.8250.6855.24.table 7:processinggda+multiple+minimum..comparison between different datausingstrategies.
snips-nsd.
on.
figure 3: effect of the proportion of unknown slottypes..tion of unknown slot types is described in 5.3.2.detection method: msp vs gda.
under thesame setting of objective, gda performs betterthan msp in both ind and nsd, especially innsd.
we argue that gda models the posteriordistribution on representation spaces of the fea-ture extractor and avoids the issue of overconﬁdentpredictions (guo et al., 2017; liang et al., 2017b,2018).
besides, comparing snips-nsd and atis-nsd, nsd token f1 scores on atis-nsd aremuch higher than snips-nsd but no signiﬁcantdifference exists for nsd span f1 scores.
thereason is that snips-nsd has a higher average en-tity length (1.83) than atis-nsd (1.29), making itharder to detect the exact ns span.
objective: binary vs multiple.
under all set-tings, multiple outperforms binary with a largemargin on two datasets in both ind and nsd met-rics.
for msp, combining multiple and binary gethigher f1 scores.
speciﬁcally, the binary classiﬁeris used to calculate the conﬁdence of a token be-longing to non-o type, which can judge whetherthe token belongs to entities and distinguish nsfrom type o. on the other hand, we use the mul-tiple classiﬁer to calculate the conﬁdence for to-kens that are of type ns, to distinguish ns fromall predeﬁned non-o slot types.
for gda, we donot combine multiple and binary because of poorperformance.
multiple achieves the best resultsfor all the ind and nsd f1 scores.
we supposemulti-class classiﬁcation can better capture seman-tic features than binary classiﬁcation.
distance strategy: minimum vs difference.
we.
ﬁnd under the same setting of binary, differencestrategy outperforms minimum on both datasetsfor nsd metrics.
but under the same setting ofmultiple, there is no consistent superiority betweenthe two distance strategies.
for example, differ-ence outperforms minimum for nsd metrics onatis-nsd, opposite to the results on snips-nsd.
we argue different distance strategies are closelyrelated to objective settings and dataset complexity.
we will leave the theoretical analysis to the future..5.3 qualitative analysis.
5.3.1 effect of different data processing.
strategies.
table 7 displays ind and nsd metrics of three dif-ferent dataset processing strategies on snips-nsdusing the same model gda+multiple+minimum.
in this section, we will dive into the analysis ofthe effects of different data processing strategies.
results show the replace strategy gets poor per-formance in nsd, which proves labeling unknownslots as o tags will severely mislead the model.
the mask and remove strategies are more rea-sonable since they remove unknown slots fromthe training data.
their main difference is thatmask only deletes token-level information, whileremove even eliminates the contextual informa-tion.
for nsd in all datasets, remove gains signif-icantly better performance on both token f1 andspan f1 than mask by 9.06%(5%), 7.83%(15%)and 4.56%(30%) on token f1, and 8.57%(5%),7.12%(15%) and 6.5%(30%) on span f1.
we ar-gue the remaining context is still misleading even ifthe novel slot tokens are not directly trained in themask strategy.
besides, mask does not conform tothe real nsd scenario.
generally, remove is themost suitable strategy for nsd in real applicationsand can achieve the best performance..5.3.2 effect of the proportion of unknown.
slot types.
fig 3 displays the effect of the proportion of un-known slot types using the remove strategy ingda+multiple+minimum.
results show that withthe increase of the proportion of unknown slottypes, the nsd f1 scores get improvements whileind f1 scores decrease.
we suppose fewer in-domain slot types help the model distinguish un-known slots from ind slots, thus nsd f1 scoresget improvements.
however, for in-domain slotdetection, since remove deletes all the sentencescontaining unknown slots in the training data, our.
34895%15%30%32.535.037.540.042.545.047.550.0f1-score (macro)your title namesnips nsd span f1atis nsd span f15%15%30%8688909294snips ind span f1atis ind span f1proportion of unknown slot typestype.
proportion(%) span length token f1 span f1.
top 5.bottom 5.object nametimerangeentity namemusic itemartistcitycountrystatebest ratingyear.
21.4215.2923.1414.8615.298.576.295.546.143.43.
3.712.353.091.052.051.321.571.101.001.00.
55.6453.6548.5646.2345.2618.7214.1913.5511.0410.24.
20.8230.1522.8334.5926.3615.8511.1110.8311.0410.24.table 9: results of single unknown slot..type 1object nametimerangeparty size numbercitystateobject nameobject nameobject nameobject nametimerangecity.
type 2-----timerangeparty size numbercitystateparty size numberstate.
token f1 span f1.
55.6453.6533.4418.7213.5553.8852.8157.9256.3271.27∗29.33∗.
20.8230.1528.5715.8510.8323.3722.3521.4219.2751.03∗27.14∗.
table 10: results of combining multiple unknown slots.
* denotes that nsd performance of the combination oftwo unknown slots is signiﬁcantly better than each sin-gle slot..of the tokens in spans.
to make a comprehensiveevaluation, we deﬁned the rose-mean, namelythe mean of rose-25%, rose-50%, rose-75%,and rose-100%.
we present results on part ofproposed models in table 8..5.3.4 analysis of single unknown slot.
to analyze the relationship between nsd perfor-mance and a single speciﬁc slot, we calculate thetoken and span metrics treating each single slottype as an unknown slot and show the results ofthe top ﬁve and bottom ﬁve for token f1 scoresin table 9. we ﬁnd that the slots with better per-formance often account for a larger percentage ofthe data set, such as object name or entity name.
they also tend to have a larger value space, such astimerange, music item, or artist.
these charac-teristics allow the semantic representation of theseslots to be distributed over a large area rather thanclustered tightly together.
we consider that thisdistribution is more reasonable because in a realapplication scenario, novel slots are diverse and itsdistribution tends to be diffuse.
performance onthese types also proves that the nsd models wepropose can be better generalized to a reasonabledata setting..figure 4: effect of varying degrees of restrictions.
gda+mul.+min.
msp+bin.+mul..rose-meanrose-100%rose-50%.
40.7340.3941.00.
34.7133.7435.46.rose metrics on snips-nsd usingtable 8:gda+multiple+minimum and msp+binary+multiple.
models suffer from the lack of sufﬁcient context torecognize ind slots so ind f1 scores decrease..5.3.3 new metric: rose.
the previous results have shown span f1 is muchlower than the token f1.
the reason is that spanf1 is a strict metric, where the model needs to cor-rectly predict all ns tokens and the correct bound-ary.
this is difﬁcult for nsd models due to thelack of supervised information.
in fact, nsd mod-els only need to mark some tokens in the span ofnovel slots and send the total sequence containingthe ns tokens back to the humans.
a small numberof token omissions or misjudgments are acceptable.
therefore, to meet a reasonable nsd scenario, wepropose a new metric, restriction-oriented spanevaluation (rose), to evaluate the span predictionperformance under different restrictions.
first, wedo not punish the situation where tokens predictionexceeds the span.
then, we consider a span is cor-rect when the number of correctly predicted tokensis greater than a settable proportion p of the spanlength.
we take the average of the rose score andthe original span f1 to avoid the model obtainingan outstanding result through over-long prediction.
the results using snips with 15% of novel slotsare shown in figure 4. as the degree of restrictionincreases, the metrics tend to decline.
it indicatesthat the model can mostly identify more than half.
3490rose-25%rose-50%rose-75%rose-100%span f1metrics152025303540f1-score (macro)msp+bin.msp+mul.msp+bin.+mul.gda+bin.+min.gda+bin.+diff.gda+mul.+min.gda+mul.+diff.
nsd error proportion(%)prediction is nstarget is nssum.
o17.7918.4736.26.
18.847.5426.38.
9.0728.2937.36.sum45.7054.30100.00.open vocabulary slots other slots.
6 discussion.
table 11: relative proportions of several types of er-rors..error type ns.
ns to o.movie name(m name).
ns toopen slot.
album.
ns toother slot.
artist.
o to ns.
artist.
open slotsto ns.
object type.
other slotsto ns.
city.
exampletext: when will paris by night airedtrue: o o b-m name i-m name i-m name opredict: o o ns o ns otext: play the insoc eptrue: o b-album i-album i-albumpredict: o b-object name i-object name nstext: play kurt cobain ballad tunestrue: o b-artist i-artist b-music item opredict: o b-genre i-genre b-music item otext: the workout playlist needs more chris crosstrue: o b-playlist o o o b-artist i-artistpredict: o b-playlist o o ns ns nstext: tell me the actors of the saga awardstrue: o o o b-object name o o b-object type opredict: o o o ns o o ns otext: what is the weather of east portal kstrue: o o o o o b-city i-city b-statepredict: o o o o o ns ns ns.
table 12: error case from nsd prediction..5.3.5 analysis for relationship of multiple.
unknown slots.
in order to explore the effect of inter-slot relation-ships on nsd, we conducted experiments in whichtwo types are mixed as novel slots.
some of the re-sults are shown in table 10. in the ﬁve types shownin the table, object name is an open vocabularyslot with a wide range of values and contains manyoov tokens, timerange and party size numberoften contain numbers, city and state are usuallysimilar in semantics and context.
we found thatwhen the other types combined with object name,nsd performance is often maintained close to treatobject name as a novel slot alone.
the reason, onthe one hand, is that the proportion of other types inthe dataset is relatively small, so the overall impacton the metrics is smaller.
on the other hand, due tothe large semantic distribution range of the open vo-cabulary slot, there is a latent inclusion relationshipfor other types, so the mixing of a single type tendsto have a slight impact on the nsd performance.
we also found that the appropriate combination cansigniﬁcantly improve the efﬁciency of nsd.
suchas timerange with party size number, or city withstate.
this indicates that when the novel slot is sim-ilar to the in-domain slot, the model tends to predictthe novel slot as a similar slot, which leads to errors.
when both are treated as novel slots, these errorscan be mitigated..in this section, we empirically divide all the errorsamples into three categories.
each type of prob-lem contains two aspects, corresponding to nsdprecision and recall, respectively.
we present therelative proportions of several types of errors intable 11, which using snips dataset with 5% novelslots on gda+multiple+minimum model.
for eacherror type, we present an example in table 12 todescribe the characteristics and analyze the causes.
then, we dive into identifying the key challengesand ﬁnally proposed possible solutions for futurework..6.1 error analysis.
tag o. tag o is the largest and most widely dis-tributed type in the dataset, and it generally refers tothe independent function tokens.
therefore, whenidentifying, it is easy to be confused with othertypes, and the confusion is more serious for novelslots without supervised learning.
we observedthat tokens with o label detected as novel slots usu-ally exist near spans, and the function words in thespan labeled as a novel slot have a probability ofbeing predicted as o. we consider that this kindof problem is related to the context.
although theprocessing strategy of remove can effectively re-duce the misleading of o for the novel slots, tago will still be affected by context information ofother in-domain slots.
open vocabulary slots.
we observe that a largenumber of novel slot tokens are mispredicted asopen vocabulary slots, while the reverse situationis much less likely to happen.
this indicates thatin snips, open vocabulary slots tend to overlapor contain most other slots semantically.
even intraditional slot ﬁlling tasks, open vocabulary slotsare often confused with other slots.
we demon-strate this hypothesis in the analysis.
section 5.3.5shows that nsd performs better when open vocab-ulary slots are treated as novel slots, and section5.3.4 shows that there is no signiﬁcant performancechange when open vocabulary slots are mixed withsome semantically concentrated slots.
the reasonfor this problem is that the deﬁnition of the datasetis not reasonable.
slots with a large value rangecan hardly help the personal assistant to give anappropriate reply, and the supervised informationof these slots is usually incomplete.
similar slots.
except for the two cases mentionedabove, predicting novel slots as other in-domain.
3491slots is the most common type of error, in whichsimilar slots account for a large part of it.
due tothe overlap between vocabulary or shared similarcontext, the model often tend to be overconﬁdentto predict similar slot labels, we analyze the phe-nomenon in table 10, when similar types is treatedas a new slot at the same time, nsd efﬁciency willrise signiﬁcantly.
we employ a generative classiﬁ-cation method gda, compared with the traditionalmsp method, to make full use of data features andalleviate the problem..6.2 challenges.
based on the above analysis, we summarize thecurrent challenges faced by the nsd task:function tokens.
articles, prepositions, and so onthat act as connective words in a sequence.
it isusually labeled with type o, but also found in somelong-span slots, such as movie name.
it can leadto confusion between o and novel slot when thiskind of slot is the target of nsd.
insufﬁcient context.
correct slot detection oftendepends on the context, and this supervised infor-mation is missing for novel slots.
models can onlyconduct nsd to tokens using the original embed-dings or representations trained in other contexts,which can lead to bias in the semantic modeling ofthe novel slot.
dependencies between slots.
there are some se-mantic overlaps or inclusion relationships in theslot deﬁnition of the current benchmark slot ﬁllingdatasets.
as a result, the semantic features are notsufﬁciently discriminative, and thus some outlierstokens in in-domain slots are easily confused withthe novel slots.
open vocabulary slots.
open vocabulary slots isa special kind of slot, its deﬁnition is usually macro-scopic and can be further divided, the value rangeis broad.
the representation distribution for openvocabulary slots tends to be diffuse and uneven,which can be misleading to nsd..6.3 future directions.
for tag o, a possible solution is to use a binarymodel to assist identiﬁcation between o and non-ofunction tokens, we provide a simple method inthis paper and leave further optimizing to futurework.
then, to decouple the dependencies betweenslots, it is critical to learn more discriminative fea-tures for in-domain data, using contrastive learningor prototypical network is expected to help.
be-sides, in the traditional slot ﬁlling task, the open.
vocabulary slot problem has been researched fora long time, and accumulate many achievements.
adaptive combination and improvement of rele-vant methods with nsd tasks is also an importantdirection of our future research..7 related work.
oov recognition oov aims to recognize unseenslot values in training set for pre-deﬁned slot types,using character embedding (liang et al., 2017a),copy mechanism (zhao and feng, 2018), few/zero-shot learning (hu et al., 2019; shah et al., 2019),transfer learning (chen and moschitti, 2019; heet al., 2020c) and background knowledge (yangand mitchell, 2017; he et al., 2020d), etc.
ourproposed nsd task focuses on detecting unknownslot types, not just unseen values..ood intent detection lee et al.
(2018); linand xu (2019); xu et al.
(2020a) aim to know whena query falls outside the range of predeﬁned sup-ported intents.
generally, they ﬁrst learn discrimi-native intent representations via in-domain (ind)data, then employs detecting algorithms, such asmaximum softmax probability (msp) (hendrycksand gimpel, 2017), local outlier factor (lof)(lin and xu, 2019), gaussian discriminant analy-sis (gda) (xu et al., 2020b) to compute the simi-larity of features between ood samples and indsamples.
compared to our proposed nsd, the maindifference is that nsd detects unknown slot typesin the token level while ood intent detection iden-tiﬁes sentence-level ood intent queries..8 conclusion.
in this paper, we deﬁned a new task, novel slot de-tection(nsd), then provide two public datasets andestablish a benchmark for it.
further, we analyzethe problems of nsd through multi-angle exper-iments and extract the key challenges of the task.
we provide some strong models for these problemsand offer possible solutions for future work..acknowledgements.
this work was partially supported by national keyr&d program of china no.
2019yff0303300and subject ii no.
2019yff0303302, docomobeijing communications laboratories co., ltd,moe-cmcc ”artiﬁcal intelligence” project no.
mcm20190701..3492broader impact.
dialog systems have demonstrated remarkable per-formance across a wide range of applications, withthe promise of a signiﬁcant positive impact on hu-man production mode and lifeway.
the ﬁrst step ofthe dialog system is to identify users’ key points.
inpractical industrial scenario, users may make unrea-sonable queries which fall outside of the scope ofthe system-supported slot types.
previous dialoguesystems will ignore this problem, which will leadto wrong operations and limit the system’s devel-opment.
in this paper, we ﬁrstly propose to detectnot only pre-deﬁned slot types but also potentialunknown or out-of-domain slot types using mspand gda methods.
according to exhaustive ex-periments and qualitative analysis, we also discussseveral major challenges in novel slot detectionfor future work.
the effectiveness and robustnessof the model are signiﬁcantly improved by addingnovel slot detection, which takes a step towardsthe ultimate goal of enabling the safe real-worlddeployment of dialog systems in safety-critical do-mains.
the experimental results have been reportedon standard benchmark datasets for considerationsof reproducible research..references.
lingzhen chen and alessandro moschitti.
2019. trans-fer learning for sequence labeling using sourcemodel and target data.
arxiv, abs/1902.05309..qian chen, zhu zhuo, and wen wang.
2019. bertfor joint intent classiﬁcation and slot ﬁlling.
arxivpreprint arxiv:1902.10909..a. coucke, a. saade, adrien ball, th´eodore bluche,a. caulier, d. leroy, cl´ement doumouro, thibaultgisselbrecht, f. caltagirone, thibaut lavril, ma¨elprimet, and j. dureau.
2018. snips voice platform:an embedded spoken language understanding sys-tem for private-by-design voice interfaces.
arxiv,abs/1805.10190..j. devlin, ming-wei chang, kenton lee, and kristinatoutanova.
2019. bert: pre-training of deep bidirec-tional transformers for language understanding.
innaacl-hlt..geli fei and b. liu.
2016. breaking the closed worldassumption in text classiﬁcation.
in hlt-naacl..chih-wen goo, guang gao, yun-kai hsu, chih-lihuo, tsung-chieh chen, keng-wei hsu, and yun-nung chen.
2018. slot-gated modeling for jointslot ﬁlling and intent prediction.
in proceedings ofthe 2018 conference of the north american chap-ter of the association for computational linguistics:.
human language technologies, volume 2 (short pa-pers), pages 753–757..chuan guo, geoff pleiss, yu sun, and kilian q. wein-berger.
2017. on calibration of modern neural net-works.
in icml..e haihong, peiqing niu, zhongfu chen, and meinasong.
2019. a novel bi-directional interrelatedmodel for joint intent detection and slot ﬁlling.
inproceedings of the 57th annual meeting of the asso-ciation for computational linguistics, pages 5467–5471..keqing he, shuyu lei, yushu yang, huixing jiang,and zhongyuan wang.
2020a.
syntactic graph con-volutional network for spoken language understand-ing.
in proceedings of the 28th international con-ference on computational linguistics, pages 2728–2738, barcelona, spain (online).
international com-mittee on computational linguistics..keqing he, weiran xu, and yuanmeng yan.
2020b.
multi-level cross-lingual transfer learning with lan-guage shared and speciﬁc knowledge for spokenieee access, 8:29407–language understanding.
29416..keqing he, yuanmeng yan, si hong liu, z. liu, andweiran xu.
2020c.
learning label-relational out-put structure for adaptive sequence labeling.
2020international joint conference on neural networks(ijcnn), pages 1–8..keqing he, yuanmeng yan, and weiran xu.
2020d.
learning to tag oov tokens by integrating contex-tual representation and background knowledge.
inproceedings of the 58th annual meeting of the as-sociation for computational linguistics, pages 619–624, online.
association for computational linguis-tics..keqing he, jinchao zhang, yuanmeng yan, weiran xu,cheng niu, and jie zhou.
2020e.
contrastive zero-shot learning for cross-domain slot ﬁlling with ad-versarial attack.
in coling..c. t. hemphill, j. j. godfrey, and g. doddington.
1990.the atis spoken language systems pilot corpus.
inhlt..dan hendrycks and kevin gimpel.
2017..abaseline for detecting misclassiﬁed and out-of-distribution examples in neural networks.
arxiv,abs/1610.02136..ziniu hu, ting chen, kai-wei chang, and yizhou sun.
2019. few-shot representation learning for out-of-in proceedings of the 57th an-vocabulary words.
nual meeting of the association for computationallinguistics, pages 4102–4112, florence, italy.
asso-ciation for computational linguistics..zhiheng huang, w. xu, and kai yu.
2015. bidirec-tional lstm-crf models for sequence tagging.
arxiv,abs/1508.01991..3493stefan larson, anish mahendran,.
joseph peper,christopher clarke, andrew lee, p. hill, jonathan k.kummerfeld, kevin leach, m. laurenzano, l. tang,and j. mars.
2019. an evaluation dataset for intentclassiﬁcation and out-of-scope prediction.
arxiv,abs/1909.02027..kimin lee, kibok lee, h. lee, and jinwoo shin.
2018. a simple uniﬁed framework for detecting out-of-distribution samples and adversarial attacks.
inneurips..dongyun liang, weiran xu, and yinge zhao.
2017a.
combining word-level and character-level represen-tations for relation classiﬁcation of informal text.
inproceedings of the 2nd workshop on representationlearning for nlp, pages 43–47, vancouver, canada.
association for computational linguistics..shiyu liang, yixuan li, and r. srikant.
2017b.
prin-cipled detection of out-of-distribution examples inneural networks.
arxiv, abs/1706.02690..shiyu liang, yixuan li, and r. srikant.
2018. enhanc-ing the reliability of out-of-distribution image detec-tion in neural networks.
arxiv: learning..ting-en lin and h. xu.
2019. deep unknown intentdetection with margin loss.
arxiv, abs/1906.00434..bing liu and ian lane.
2015. recurrent neural net-work structured output prediction for spoken lan-in proc.
nips workshopguage understanding.
on machine learning for spoken language under-standing and interactions..bing liu and ian lane.
2016. attention-based recur-rent neural network models for joint intent detectionand slot ﬁlling.
arxiv preprint arxiv:1609.01454..samuel louvan and b. magnini.
2020. recent neuralmethods on slot ﬁlling and intent classiﬁcation fortask-oriented dialogue systems: a survey.
in col-ing..darsh j. shah, raghav gupta, a. fayazi, anddilek z. hakkani-t¨ur.
2019. robust zero-shot cross-domain slot ﬁlling with example values.
arxiv,abs/1906.06870..lei shu, hu xu, and bing liu.
2017. doc: deeparxiv,.
open classiﬁcation of text documents.
abs/1709.08716..h. xu, keqing he, yuanmeng yan, si hong liu, z. liu,and weiran xu.
2020a.
a deep generative distance-based classiﬁer for out-of-domain detection with ma-halanobis space.
in coling..hong xu, keqing he, yuanmeng yan, sihong liu, zi-jun liu, and weiran xu.
2020b.
a deep generativedistance-based classiﬁer for out-of-domain detectionwith mahalanobis space.
in proceedings of the 28thinternational conference on computational linguis-tics, pages 1452–1460, barcelona, spain (online).
international committee on computational linguis-tics..yuanmeng yan, keqing he, hong xu, sihong liu,fanyu meng, min hu, and weiran xu.
2020. ad-versarial semantic decoupling for recognizing open-vocabulary slots.
in proceedings of the 2020 con-ference on empirical methods in natural languageprocessing (emnlp), pages 6070–6075, online.
as-sociation for computational linguistics..b. yang and tom michael mitchell.
2017. leverag-ing knowledge bases in lstms for improving machinereading.
in acl..zhiyuan zeng, keqing he, yuanmeng yan, hong xu,and weiran xu.
2021a.
adversarial self-supervisedlearning for out-of-domain detection.
in naacl..zhiyuan zeng, hong xu, keqing he, yuanmeng yan,sihong liu, zijun liu, and weiran xu.
2021b.
ad-versarial generative distance-based classiﬁer for ro-in icassp 2021bust out-of-domain detection.
- 2021 ieee international conference on acous-tics, speech and signal processing (icassp), pages7658–7662..gr´egoire mesnil, yann dauphin, kaisheng yao,yoshua bengio, li deng, dilek z. hakkani-tur, xi-aodong he, larry heck, gokhan tur, dong yu, andgeoffrey zweig.
2015. using recurrent neural net-works for slot ﬁlling in spoken language understand-ing.
ieee/acm transactions on audio, speech, andlanguage processing, 23:530–539..lin zhao and zhe feng.
2018..improving slot ﬁll-ing in spoken language understanding with jointpointer and attention.
in proceedings of the 56th an-nual meeting of the association for computationallinguistics (volume 2: short papers), pages 426–431, melbourne, australia.
association for compu-tational linguistics..lev ratinov and dan roth.
2009..design chal-lenges and misconceptions in named entity recog-in proceedings of the thirteenth confer-nition.
ence on computational natural language learning(conll-2009), pages 147–155..j. ren, peter j. liu, e. fertig, jasper snoek, ryanpoplin, mark a. depristo, joshua v. dillon, and bal-aji lakshminarayanan.
2019. likelihood ratios forout-of-distribution detection.
in neurips..yinhe zheng, guanyi chen, and minlie huang.
2020.out-of-domain detection for natural language under-ieee/acm transac-standing in dialog systems.
tions on audio, speech, and language processing,28:1198–1209..3494