a span-based model for joint overlapped and discontinuousnamed entity recognition.
fei li1 and zhichao lin2 and meishan zhang2 and donghong ji1∗1. department of key laboratory of aerospace information security and trusted computing,ministry of education, school of cyber science and engineering, wuhan university, china2. school of new media and communication, tianjin university, china{lifei csnlp,dhji}@whu.edu.cn and mason.zms@gmail.com.
abstract.
research on overlapped and discontinuousnamed entity recognition (ner) has receivedincreasing attention.
the majority of previ-ous work focuses on either overlapped or dis-continuous entities.
in this paper, we pro-pose a novel span-based model that can rec-ognize both overlapped and discontinuous en-tities jointly.
the model includes two ma-jor steps.
first, entity fragments are recog-nized by traversing over all possible text spans,thus, overlapped entities can be recognized.
second, we perform relation classiﬁcation tojudge whether a given pair of entity fragmentsto be overlapping or succession.
in this way,we can recognize not only discontinuous en-tities, and meanwhile doubly check the over-lapped entities.
as a whole, our model canbe regarded as a relation extraction paradigmessentially.
experimental results on multiplebenchmark datasets (i.e., clef, genia andace05) show that our model is highly compet-itive for overlapped and discontinuous ner..1.introduction.
named entity recognition (ner)(sang andde meulder, 2003) is one fundamental task fornatural language processing (nlp), due to its wideapplication in information extraction and data min-ing (lin et al., 2019b; cao et al., 2019).
tradi-tionally, ner is presented as a sequence labelingproblem and widely solved by conditional randomﬁeld (crf) based models (lafferty et al., 2001).
however, this framework is difﬁcult to handle over-lapped and discontinuous entities (lu and roth,2015; muis and lu, 2016), which we illustrate us-ing two examples as shown in figure 1. the twoentities “pennsylvania” and “pennsylvania radiostation” are nested with each other,1 and the sec-.
∗corresponding author.
1 we consider “nested” as a special case of “overlapped”..ond example shows a discontinuous entity “mitralleaﬂets thickened” involving three fragments..there have been several studies to investigateoverlapped or discontinuous entities (finkel andmanning, 2009; lu and roth, 2015; muis and lu,2017; katiyar and cardie, 2018; wang and lu,2018; ju et al., 2018; wang et al., 2018; fisherand vlachos, 2019; luan et al., 2019; wang andlu, 2019).
the majority of them focus on over-lapped ner, with only several exceptions to thebest of our knowledge.
muis and lu (2016) presenta hypergraph model that is capable of handlingboth overlapped and discontinuous entities.
wangand lu (2019) extend the hypergraph model withlong short-term memories (lstms) (hochreiterand schmidhuber, 1997).
dai et al.
(2020) pro-posed a transition-based neural model for discon-tinuous ner.
by using these models, ner couldbe conducted universally without any assumptionto exclude overlapped or discontinuous entities,which could be more practical in real applications.
the hypergraph (muis and lu, 2016; wang andlu, 2019) and transition-based models (dai et al.,2020) are ﬂexible to be adapted for different tasks,achieving great successes for overlapped or dis-continuous ner.
however, these models need tomanually deﬁne graph nodes, edges and transitionactions.
moreover, these models build graphs orgenerate transitions along the words in the sen-tences gradually, which may lead to error propa-gation (zhang et al., 2016).
in contrast, the span-based scheme might be a good alternative, whichis much simpler including only span-level classiﬁ-cation.
thus, it needs less manual intervention andmeanwhile span-level classiﬁcation can be fullyparallelized without error propagation.
recently,luan et al.
(2019) utilized the span-based modelfor information extraction effectively..in this work, we propose a novel span-basedjoint model to recognize overlapped and discon-.
proceedingsofthe59thannualmeetingoftheassociationforcomputationallinguisticsandthe11thinternationaljointconferenceonnaturallanguageprocessing,pages4814–4828august1–6,2021.©2021associationforcomputationallinguistics4814figure 1: examples to illustrate the differences between the sequence labeling model and our span-based model.
on the left, word fragments marked with the same number belong the same entity.
on the right, blue rectanglesdenote the recognized entity fragments, and solid lines indicate the succession or overlapping relationsbetween them (the two relations are mutually exclusive)..tinuous entities simultaneously in an end-to-endway.
the model utilizes bert (devlin et al.,2019) to produce deep contextualized word rep-resentations, and then enumerates all candidatetext spans (luan et al., 2019), classifying whetherthey are entity fragments.
following, fragmentrelations are predicted by another classiﬁer to de-termine whether two speciﬁc fragments involve acertain relation.
we deﬁne two relations for ourgoal: overlapping or succession, whichare used for overlapped and discontinuous entities,respectively.
in essence, the joint model can beregarded as one kind of relation extraction mod-els, which is adapted for our goal.
to enhanceour model, we utilize the syntax information aswell by using a dependency-guided graph convo-lutional network (kipf and welling, 2017; zhanget al., 2018; jie and lu, 2019; guo et al., 2019)..we evaluate our proposed model on sev-eral benchmark datasets which includes bothoverlapped and discontinuous entities(e.g.,clef (suominen et al., 2013)).
the resultsshow that our model outperforms the hypergraph(muis and lu, 2016; wang and lu, 2019) andtransition-based models (dai et al., 2020).
be-sides, we conduct experiments on two benchmarkdatasets including only overlapped entities (i.e.,genia (kim et al., 2003) and ace05).
experi-mental results show that our model can also obtaincomparable performances with the state-of-the-artmodels (luan et al., 2019; wadden et al., 2019;strakov´a et al., 2019).
in addition, we observe thatour approaches for model enhancement are effec-tive in the benchmark datasets.
our code is avail-able at https://github.com/foxlf823/sodner..2 related work.
in the nlp domain, ner is usually considered asa sequence labeling problem (liu et al., 2018; linet al., 2019b; cao et al., 2019).
with well-designedfeatures, crf-based models have achieved theleading performance (lafferty et al., 2001; finkelet al., 2005; liu et al., 2011).
recently, neuralnetwork models have been exploited for featurerepresentations (chen and manning, 2014; zhouet al., 2015).
moreover, contextualized word rep-resentations such as elmo (peters et al., 2018),flair (akbik et al., 2018) and bert (devlin et al.,2019) have also achieved great success.
as forner, the end-to-end bi-directional lstm crfmodels (lample et al., 2016; ma and hovy, 2016;yang et al., 2018) is one representative architec-ture.
these models are only capable of recognizingregular named entities..for overlapped ner, the earliest model to ourknowledge is proposed by finkel and manning(2009), where they convert overlapped ner as aparsing task.
lu and roth (2015) propose a hy-pergraph model to recognize overlapped entitiesand lead to a number of extensions (muis andlu, 2017; katiyar and cardie, 2018; wang andlu, 2018).
moreover, recurrent neural networks(rnns) are also used for overlapped ner (ju et al.,2018; wang et al., 2018).
other approaches in-clude multi-grained detection (xia et al., 2019),boundary detection (zheng et al., 2019), anchor-region network (lin et al., 2019a) and machinereading comprehension (li et al., 2020).
the state-of-the-art models for overlapped ner include thesequence-to-sequence (seq2seq) model (strakov´aet al., 2019), where the decoder predicts multiple.
4815at issue is the liability of a [[pennsylvania]1radiostation]2under the federal wiretap statute.example 1the[mitral]1valve [leaflets]1are mildly [thickened]1.sequence labeling modelour proposed modelthe/omitral/bvalve/oleaflets/bare/omildly/othickened/b./oat/oissue/ois/othe/oliability/oof/oa/opennsylvania/bradio/istation/iunder/othe/ofederal/owiretap/ostatute/o./opennsylvaniapennsylvania radio stationentity fragment relation graphmitralleafletsthickenedentity fragment relation graphsequence labeling modelour proposed modelexample 2figure 2: the architecture of our model.
the input is “the [mitral]1 valve [leaﬂets]1 are mildly [thickened]1”.
h1denotes the original word representation and h(cid:48)1 denotes the syntax-enhanced word representation.
s1,2 denotesthe span representation.
α and β control the loss weights of two tasks, namely recognizing entity fragments fromtext spans and predicting the relation between each pair of fragments..labels for a word and move to next word until it out-puts the “end of word” label, and the span-basedmodel (luan et al., 2019; wadden et al., 2019),where overlapped entities are recognized by classi-ﬁcation for enumerated spans..compared with the number of related work foroverlapped ner, there are no related studies foronly discontinuous ner, but several related stud-ies for both overlapped and discontinuous ner.
early studies addressed such problem by extendingthe bio label scheme (tang et al., 2013; metke-jimenez and karimi, 2016).
muis and lu (2016)ﬁrst proposed a hypergraph-based model for recog-nizing overlapped and discontinuous entities, andthen wang and lu (2019) utilized deep neural net-works to enhance the model.
very recently, daiet al.
(2020) proposed a transition-based neuralmodel with manually-designed actions for bothoverlapped and discontinuous ner.
in this work,we also aim to design a competitive model forboth overlapped and discontinuous ner.
our dif-ferences are that our model is span-based (luanet al., 2019) and it is also enhanced by dependency-guided graph convolutional network (gcn) (zhanget al., 2018; guo et al., 2019)..to our knowledge, syntax information is com-monly neglected in most previous work for over-lapped or discontinuous ner, except finkel andmanning (2009).
the work employs a constituency.
parser to transform a sentence into a nested entitytree, and syntax information is used naturally tofacilitate ner.
by contrast, syntax information hasbeen utilized in some studies for traditional regu-lar ner.
under the traditional statistical setting,syntax information is used by manually-crafted fea-tures (hacioglu et al., 2005; ling and weld, 2012)or auxiliary tasks (florian et al., 2006) for ner.
recently, jie et al.
(2017) build a semi-crf modelbased on dependency information to optimize theresearch space of ner recognition.
jie and lu(2019) stack the dependency-guided graph convo-lutional network (zhang et al., 2018; guo et al.,2019) on top of the bilstm layer.
these studieshave demonstrated that syntax information couldbe an effective feature source for ner..3 method.
the key idea of our model includes two mecha-nisms.
first, our model enumerates all possibletext spans in a sentence and then exploits a multi-classiﬁcation strategy to determine whether onespan is an entity fragment as well as the entitytype.
based on this mechanism, overlapped entitiescould be recognized.
second, our model performspairwise relation classiﬁcations over all entity frag-ments to recognize their relationships.
we deﬁnethree kinds of relation types:.
4816inputwordrepgraph convolutional networkℎ"ℎ#ℎ$span representationℎ"%ℎ#%ℎ$%&'(#&'($)$,$)$,#…)$,")#,#)","…entity fragment recognitionfragment relation predictiontrainingdependency parsingsyntax informationthemitralthickeneddecoding⨁'(-,.,θ)'1))#'1))$23………mitralleafletsthickenedentity fragment relation graphthemitralvalueleafletsaremildlythickenedthe1001000mitral0101000value0011000leaflets1111001are0000101mildly0000011thickened0001111bertelmoword2vecbidirectional lstm• succession, indicating that the two entityfragments belong to one single named entity.
• overlapping, indicating that the two en-.
tity fragments have overlapped parts..• other, indicating that the two entity frag-ments have other relations or no relations..with the succession relation, we can rec-through theognize discontinuous entities.
overlapping relation, we aim to improve therecognition of overlapped entities with double su-pervision.
the proposed model is essentially arelation extraction model being adapted for ourtask.
the architecture of our model is illustratedin figure 2, where the main components includethe following parts: (1) word representation, (2)graph convolutional network, (3) span representa-tion, and (4) joint decoding, which are introducedby the following subsections, respectively..3.1 word representation.
we exploit bert (devlin et al., 2019) as inputsfor our model, which has demonstrated effectivefor a range of nlp tasks.2 given an input sentencex = {x1, x2, ..., xn }, we convert each word xiinto word pieces and then feed them into a pre-trained bert module.
after the bert calculation,each sentential word may involve vectorial repre-sentations of several pieces.
here we employ therepresentation of the beginning word piece as theﬁnal word representation following (wadden et al.,2019).
for instance, if “fevers” is split into “fever”and “##s”, the representation of “fever” is used asthe whole word representation.
therefore, all thewords in the sentence x correspond to a matrix h= {h1, h2, ..., hn } ∈ rn ×dh, where dh denotesthe dimension of hi..3.2 graph convolutional network.
dependency syntax information has been demon-strated to be useful for ner previously (jie andlu, 2019).
in this work, we also exploit it to en-hance our proposed model.3 graph convolutionalnetwork (gcn) (kipf and welling, 2017) is onerepresentative method to encode dependency-basedgraphs, which has been shown effective in infor-mation extraction (zhang et al., 2018).
thus, wechoose it as one standard strategy to enhance ourword representations.
concretely, we utilize the.
2we also investigate the effects of different word encoders.
in the experiments.
please refer to appendix a.
3some cases are shown in appendix b..the architecture of our graph convolu-figure 3:tional network.
graph convolution: equation 1. self-attention: equation 2..attention-guided gcn (aggcn) (guo et al., 2019)to reach our goal, as it can bring better performancecompared with the standard gcn..in order to illustrate the network of aggcn(figure 3), we start with the standard gcn module.
given the word representations h = {h1, h2, ...,hn }, the standard gcn uses the following equa-tion to update them:.
h(l).
i = σ(.
aijw (l)h(l−1).
j.
+ b(l)),.
(1).
n(cid:88).
j=1.
where w (l) and b(l) are the weight and bias ofthe l-th layer.
a ∈ rn ×n is an adjacency matrixobtained from the dependency graph, where aij =1 indicates there is an edge between the word iand j in the dependency graph.
figure 2 offers anexample of the matrix which is produced by thecorresponding dependency syntax tree..in fact, a can be considered as a form of hardattention in gcn, while aggcn (guo et al., 2019)aims to improve the method by using a in thelower layers and updating a at the higher layersvia multi-head self-attention (vaswani et al., 2017)as below:.
˜at = softmax(.
h tw t.q × (h tw t√dhead.
k)t.),.
(2).
4817self-attentionself-attention!
"#$%&'(&')*+,-./)*+,-graph convolutiongraph convolutionlinear combination&'0.20.3…0.10.40.1…0.2…………0.30.2…0.3ℎ(ℎ1ℎ)ℎ(ℎ1ℎ)……./(11…011…0…………00…1/ℎ(ℎ1ℎ)ℎ(ℎ1ℎ)……0.10.1…0.50.30.1…0.4…………0.10.1…0.2ℎ(ℎ1ℎ)ℎ(ℎ1ℎ)……………q and w twhere w tk are used to project the inputh t ∈ rn ×dhead (dhead = dh) of the t-th headnheadinto a query and a key.
˜at ∈ rn ×n is the updatedadjacency matrix for the t-th head..for each head t, aggcn uses ˜at and a denselyconnected layer to update the word representations,which is similar to the standard gcn as shown inequation 1. the output of the densely connectedlayer is ˜h t ∈ rn ×dh.
then a linear combinationlayer is used to merge the output of each head,namely ˜h = [ ˜h 1, · · · , ˜h nhead]w1, where w1∈ r(nhead×dh)×dh is the weight and ˜h ∈ rn ×dhis the ﬁnal output of aggcn..after that, ˜h is concatenated with the originalword representations h to form ﬁnal word rep-resentations h (cid:48) ∈ rn ×(dh+df ) = [h, ˜hw2],where w2 ∈ rdh×df indicates a linear transfor-mation for dimensionality reduction.4.
3.3 span representation.
we employ span enumeration (luan et al., 2019)to generate text spans.
take the sentence “themitral valve leaﬂets are mildly thickened” in fig-ure 2 as an example, the generated text spans willbe “the”, “the mitral”, “the mitral valve”, ...,“mildly”, “mildly thickened” and “thickened”.
torepresent a text span, we use the concatenation ofword representations of its startpoint and endpoint.
for example, given word representations h = {h1,h2, ..., hn } ∈ rn ×dh (or h (cid:48) = {h(cid:48)n })and a span (i, j) that starts at the position i andends at j, the span representation will be.
2, ..., h(cid:48).
1, h(cid:48).
si,j = [hi, hj, w] or [h(cid:48).
i, h(cid:48).
j, w],.
(3).
where w is a 20-dimensional embedding to repre-sent the span width following previous work (luanet al., 2019; wadden et al., 2019).
thus, the dimen-sion ds of si,j is 2dh + 20 (or 2(dh + df ) + 20)..3.4 decoding.
our decoding consists of two parts.
first, we rec-ognize all valid entity fragments, and then performpairwise classiﬁcations over the fragments to un-cover their relationships.
entity fragment recognition: given a span(i, j) represented as si,j, we utilize one mlp to.
4we employ third-party tools to perform parsing for thecorpora that do not contain gold syntax annotations.
sincesometimes parsing may fail, dependency-guided gcn will benoneffective.
concatenation can remedy such problem sinceh still works even if ˜h is invalid..if isentityfragment(si,j) then.
algorithm 1 decoding algorithm.
input: an input sentence x = {x1, x2, ..., xn }output: the recognized results r1: s = enumeratespan(x) where s = {s1,1, s1,2, ...}2: for si,j in s do3:4:5: for each pair si,j, s˜i,˜j in v do6:7:8: graph g = {v, e}9: for g in findcompletesubgraphs(g) dor ← g10:11: return r.if issuccession(si,j, s˜i,˜j) then.
e ← < si,j, s˜i,˜j >.
v ← si,j.
classify whether the span is an entity fragment andwhat is the entity type, formalized as:.
p1 = softmax(mlp1(si,j)),.
(4).
where p1 indicates the probabilities of entity typessuch as organization, disease and none (i.e., notan entity fragment).
fragment relation prediction: given two entityfragments (i, j) and (˜i, ˜j) represented as si,j ands˜i,˜j, we utilize another mlp to classify their rela-tions:.
p2 = softmax(mlp2([si,j, si,j ∗ s˜i,˜j, s˜i,˜j])),.
(5)where p2 indicates the probabilities of three classes,namely succession, overlapping and other, andthe feature representations are mostly referredfrom luan et al.
(2019) and wadden et al.
(2019).
noticeably, although the overlapped entities canbe recognized at the ﬁrst step, here we use theoverlapping as one auxiliary strategy to fur-ther enhance the model..during decoding (algorithm 1), our model rec-ognizes entity fragments from text spans (lines 2-4)in the input sentence and selects each pair of thesefragments to determine their relations (lines 5-7).
therefore, the prediction results can be consideredas an entity fragment relation graph (line 8), wherea node denotes an entity fragment and an edge de-notes the relation between two entity fragments.5the decoding object is to ﬁnd all the subgraphsin which each node connects with any other node(line 9).
thus, each of such subgraph composes anentity (line 10).
in particular, the entity fragmentthat has no edge with others composes an entity byitself..5we only use the succession relations during de-coding while ignore the overlapping relations.
theoverlapping relations are only used during training..4818# documents or sentences.
% of overlapped entities.
% of discontinuous entities.
clef clef-dis cadec genia ace3704351.
1,599200200.
1792099.
534303430.
875187188.
678.
11138.
293836.
545552.
151413.
11109.
181822.
000.
403739.
000.traindevtest.
traindevtest.
traindevtest.
table 1: dataset statistics.
for the clef, clef-dis, cadec, genia and ace05 datasets, we follow the settingsof dai et al.
(2020), wang and lu (2019), luan et al.
(2019) and lu and roth (2015) respectively.
the statistics ofclef-dis are sentence numbers, others are document numbers..3.5 training.
during training, we employ multi-task learn-ing (caruana, 1997; liu et al., 2017) to jointlytrain different parts of our model.6 the loss func-tion is deﬁned as the negative log-likelihood of thetwo classiﬁcation tasks, namely entity fragmentrecognition and fragment relation prediction:.
(cid:88).
l = −.
α log p1(yent) + β log p2(yrel),.
(6).
where yent and yrel denote the corresponding gold-standard labels for text spans and span pairs, αand β are the weights to control the task impor-tance.
during training, we use the bertadam algo-rithm (devlin et al., 2019) with the learning rate5 × 10−5 to ﬁnetune bert and 1 × 10−3 to ﬁne-tune other parts of our model.
the training processwould terminate if the performance does not in-crease by 15 epochs..4 experimental setup.
datasets: to evaluate our model for simultane-ously recognizing overlapped and discontinuousentities, we follow prior work (muis and lu, 2016;wang and lu, 2019; dai et al., 2020) and employthe data, called clef, from the share/clefehealth evaluation lab 2013 (suominen et al.,2013), which consists of 199 and 99 clinical notesfor training and testing.
note that dai et al.
(2020)used the full clef dataset in their experiments(179 for training, 20 for development and 99 fortesting), while muis and lu (2016) and wang andlu (2019) used a subset of the union of the clefdataset and semeval 2014 task 7 (pradhan et al.,.
6please refer to appendix c for the effect of multi-task.
learning..2014).
concretely, they used the training set andtest set of the share/clef ehealth evaluationlab 2013 as the training and development set, andthey also used the development set of the semeval2014 task 7 as the test set.
in addition, they se-lected only the sentences that contain at least onediscontinuous entity.
finally, the training, devel-opment and test sets contain 534, 303 and 430sentences, respectively.
we call this dataset asclef-dis in this paper.
moreover, we also fol-low dai et al.
(2020) to evaluate models using thecadec dataset proposed by karimi et al.
(2015).
we follow the setting of dai et al.
(2020) to splitthe dataset and conduct experiments..to show our model is comparable with the state-of-the-art models for overlapped ner, we conductexperiments on genia (kim et al., 2003) andace05.
for the genia and ace05 datasets, weemploy the same experimental setting in previousworks (lu and roth, 2015; muis and lu, 2017;wang and lu, 2018; luan et al., 2019), where 80%,10% and 10% sentences in 1,999 genia docu-ments, and the sentences in 370, 43 and 51 ace05documents are used for training, development andtest, respectively.
the statistics of all the datasetswe use in this paper is shown in table 1..evaluation metrics: in terms of evaluation met-rics, we follow prior work (lu and roth, 2015;muis and lu, 2016; wang and lu, 2018, 2019) andemploy the precision (p), recall (r) and f1-score(f1).
a predicted entity is counted as true-positiveif its boundary and type match those of a gold en-tity.
for a discontinuous entity, each span shouldmatch a span of the gold entity.
all f1 scores re-ported in section 5 are the mean values from ﬁveruns of the same setting..4819related worktang et al.
(2013)tang et al.
(2015)dai et al.
(2020).
our model.
methodcrf, biohdcrf, biohd1234transition-based, elmo7span-based, bert0 – dep-guided gcn0 – overlap relation0 – bert.
f175.078.377.7.
83.282.582.278.6.related workbaseline (2016)tang et al.
(2018)dai et al.
(2020).
our model.
methodcrf, biohdlstm-crf, multilabeltransition-based, elmospan-based, bert0 – dep-guided gcn0 – overlap relation0 – bert.
f160.266.369.069.5.
69.969.966.8.table 2: results on the clef dataset..table 4: results on the cadec dataset.
“baseline(2016)” indicates metke-jimenez and karimi (2016)..related workmuis and lu (2016)wang and lu (2019) hypergraph, rnn.
methodhypergraph.
dai et al.
(2020).
our model.
transition-based, elmospan-based, bert0 – dep-guided gcn0 – overlap relation0 – bert.
f152.856.162.9.
63.362.962.656.4.table 3: results on the clef-dis dataset..implementation details: for hyper-parametersand other details, please refer to appendix d..5 results and analyses.
5.1 results on clef.
table 2 shows the results on the clef dataset.
asseen, tang et al.
(2013) and tang et al.
(2015)adapted the crf model, which is usually used forﬂat ner, to overlapped and discontinuous ner.
they modiﬁed the bio label scheme to biohd andbiohd1234, which use “h” to label overlappedentity segments and “d” to label discontinuous en-tity segments.
surprisingly, the recently-proposedtransition-based model (dai et al., 2020) does notperform better than the crf model (tang et al.,2015), which may be because tang et al.
(2015)have conducted elaborate feature engineering fortheir model.
in contrast, our model outperforms allthe strong baselines with at least about 5% marginin f1.
our model does not rely on feature engi-neering or manually-designed transitions, which ismore suitable for modern end-to-end learning..we further perform ablation studies to investi-gate the effect of dependency-guided gcn and theoverlapping relation, which can be removed with-out inﬂuencing our major goal.
as shown in ta-ble 2, after removing either of them, the f1 scores.
7dai et al.
(2020) found that bert did not perform better.
than elmo in their experiments..go down by 0.7% and 1.0%.
the observation sug-gests that both dependency-guided gcn and theoverlapping relation are effective for our model.
moreover, after we replace bert with the wordembeddings pretrained on pubmed (chiu et al.,2016), the f1 score goes down by 4.6%, whichdemonstrates that bert plays an important role inour model..5.2 results on clef-dis.
table 3 shows the results on the clef-dis dataset.
as seen, our model outperforms the previous bestmodel (dai et al., 2020) by 0.4% in f1, whichindicates that our model is very competitive, lead-ing to a new state-of-the-art result on the dataset.
similarly, we further perform ablation studies toinvestigate the effect of dependency-guided gcn,the overlapping relation and bert on this dataset.
as shown, after removing either of the gcn oroverlapping relation, the f1 score decreases by0.4% or 0.7%, which is consistent with the obser-vations in table 2. in addition, to fairly comparewith wang and lu (2019), we also replace bertwith the word embeddings pretrained on pubmed(chiu et al., 2016).
as we can see, our model alsooutperforms their model by 0.3%..5.3 results on cadec.
as shown in table 4, metke-jimenez and karimi(2016) employed the similar method in (tanget al., 2013) by expanding the bio label schemeto biohd.
tang et al.
(2018) also experimentedthe biohd label scheme, but they found that theresult of the biohd-based method was slightlyworse than that of the “multilabel” method (65.5%vs. 66.3% in f1).
compared with the methodin (metke-jimenez and karimi, 2016), the perfor-mance improvement might be mainly because theyused deep neural networks (e.g., lstm) instead ofshallow non-neural models..4820figure 4: result analysis based on entity types (i.e.,(r)egular, (o)verlapped and (d)iscontinuous) on theclef-dis dataset, comparing with bilstm-crf.
8.compared with the above baselines,.
thetransition-based model dai et al.
(2020) is stillthe best.
our full model slightly outperforms thetransition-based model by 0.5%.
in this dataset,we do not observe mutual beneﬁt between thedependency-guided gcn and overlapped relationprediction modules, since our model achieves bet-ter results when using them separately (69.9%) thanusing them jointly (69.5%).
however, when usingthem separately, the f1 is still 0.6% higher thanthe one using neither of them.
without bert, theperformance of our model drops by about 3% butit is still comparable with the performances of themethods without contextualized representations..5.4 result analysis based on entity types.
comparing with bilstm-crf to show thenecessity of building one model to recognize reg-ular, overlapped and discontinuous entities simul-taneously, we analyze the predicted entities in theclef-dis dataset and classify them based on theirtypes, as shown in figure 4. in addition, we com-pare our model with bilstm-crf (lample et al.,2016; ma and hovy, 2016; yang et al., 2018), toshow our model does not inﬂuence the performanceof regular ner signiﬁcantly.
for a fair comparison,we replace bert with glove (pennington et al.,2014) and keep the setting of our model the samewith the setting of the bilstm-crf model usedin previous work (yang et al., 2018)..as seen, if only considering regular entities, the.
8many discontinuous entities are also overlapped, but we.
do not count them as overlapped entities in this ﬁgure..figure 5: result analysis based on entity types on theclef-dis dataset, comparing with dai et al.
(2020)(blue)..bilstm-crf model can achieve a better perfor-mance compared with our model, especially theprecision value is much higher.
one likely reasonmight be that the bilstm-crf model is capa-ble of using the label dependence to detect entityboundaries accurately, ensuring the correctness ofthe recognized entities, which is closely related tothe precision.
nevertheless, our model can lead tohigher recall, which reduces the gap between thetwo models..if considering both regular and overlapped enti-ties, the recall of our model is greatly boosted, andthus the f1 increases concurrently.
if both regularand discontinuous entities are included, the perfor-mance of our model rises signiﬁcantly to 50.9% dueto the large scale of discontinuous entities.
whenall types of entities are concerned, the f1 of ourmodel further increases by 0.8%, indicating theeffectiveness of our model in joint recognition ofoverlapped, discontinuous and regular entities..comparing with the transition-based modelas shown in figure 5, we also compare ourmodel with the transition-based model (dai et al.,2020) based on entity types by analyzing the re-sults from one run of experiments.
note thatsince we do not tune the hyper-parameters ofthe transition-based model elaborately, the per-formance is not as good as the one that theyhave reported.
as seen, our model performsbetter in all of the four groups, namely regular,regular+overlapped, regular+discontinuous, regu-lar+overlapped+discontinuous entity recognition.
however, based on the observation on the bars indifferent groups, we ﬁnd that the main superiority.
4821precisionrecallf167.723.935.342.526.432.641.429.834.653.348.750.951.452.151.7r(bilstm-crf)rr+or+dr+o+drr+or+dr+o+d33.337.857.459.941.144.161.562.9genia ace05.
related workfinkel and manning (2009)lu and roth (2015)muis and lu (2017)katiyar and cardie (2018)wang et al.
(2018)ju et al.
(2018)zheng et al.
(2019)lin et al.
(2019a)wang and lu (2018)xia et al.
(2019).
methodconstituency parsinghypergraphhypergraphhypergraph, rnntransition-based parsing, rnndynamically stacking, rnnboundary detection, rnnanchor-region detection, rnn, cnnhypergraph, rnnmulti-grained detection, rnn, elmo.
fisher and vlachos (2019) merge and label, bert.
luan et al.
(2019)wadden et al.
(2019)strakov´a et al.
(2019).
our model.
span-based, elmo, corefspan-based, bert, corefseq2seq, elmo, bert, flairspan-based, bert0 – dep-guided gcn0 – overlap relation.
70.370.370.873.873.974.774.774.875.1––76.277.9.
78.377.877.477.4.
–58.761.370.573.072.2–74.974.578.282.482.9–.
84.383.082.682.7.table 5: comparisons with prior work on the genia and ace05 datasets..of our model comes from regular entity recogni-tion.
in recognizing overlapped entities, our modelis comparable with the transition-based model, butin recognizing discontinuous entities, our modelperforms slightly worse than the transition-basedmodel.
this suggests that a combination of span-based and transition-based models may be a poten-tial method for future research..5.5 results on genia and ace05.
table 5 shows the results of the genia and ace05datasets, which include only regular and over-lapped entities.
our ﬁnal model achieves 77.8%and 83.0% f1s in the genia and ace05 datasets,respectively.
by removing the dependency-guidedgcn, the model shows an averaged decrease of0.4%, indicating the usefulness of dependency syn-tax information.
the ﬁnding is consistent with thatof the clef dataset.
interestingly, we note thatthe overlapping relation also brings a positive inﬂu-ence in this setting.
actually, the relation extractionarchitecture is not necessary for only regular andoverlapped entities, because the decoding can beﬁnished after the ﬁrst entity fragment recognitionstep.
the observation doubly demonstrates the ad-vantage of our ﬁnal model.
we also compare ourresults with several state-of-the-art results of theprevious work on the two datasets in table 5. onlythe studies with the same training, developmentand test divisions are listed.
we can see that ourmodel can achieve very competitive performances.
on both datasets.
note that luan et al.
(2019) andwadden et al.
(2019) use extra coreference resolu-tion information, and strakov´a et al.
(2019) exploitmuch richer word representations by a combinationof elmo, bert and flair..6 conclusion.
in this work, we proposed an efﬁcient and effectivemodel to recognize both overlapped and discontin-uous entities simultaneously, which can be appliedto any ner dataset theoretically, since no extraassumption is required to limit the type of namedentities.
first, we enumerate all spans in a givensentence to determine whether they are valid en-tity fragments, and then relation classiﬁcations areperformed to check the relationships between allfragment pairs.
the results show that our modelis highly competitive to the state-of-the-art mod-els for overlapped or discontinuous ner.
we haveconducted detailed studies to help comprehensiveunderstanding of our model..acknowledgments.
we thank the reviewers for their commentsand recommendation.
this work is supportedby the national natural science foundation ofchina (no.
61772378), the national key re-search and development program of china (no.
2017yfc1200500), the research foundation ofministry of education of china (no.
18jzd015)..4822references.
alan akbik, duncan blythe, and roland vollgraf.
2018. contextual string embeddings for sequencein proceedings of the 27th internationallabeling.
conference on computational linguistics, pages1638–1649..emily alsentzer, john murphy, william boag, wei-hung weng, di jindi, tristan naumann, andmatthew mcdermott.
2019. publicly available clini-cal bert embeddings.
in proceedings of the 2nd clin-ical natural language processing workshop, pages72–78..iz beltagy, kyle lo, and arman cohan.
2019. scib-ert: a pretrained language model for scientiﬁc text.
in proceedings of the 2019 conference on empiricalmethods in natural language processing and the9th international joint conference on natural lan-guage processing (emnlp-ijcnlp), pages 3615–3620, hong kong, china.
association for computa-tional linguistics..yixin cao, zikun hu, tat-seng chua, zhiyuan liu, andheng ji.
2019. low-resource name tagging learnedin proceedings of thewith weakly labeled data.
2019 conference on emnlp, pages 261–270..rich caruana.
1997. multitask learning.
machine.
learning, 28(1):41–75..danqi chen and christopher d manning.
2014. afast and accurate dependency parser using neural net-in proceedings of the 2014 conference onworks.
empirical methods in natural language processing(emnlp), pages 740–750..billy chiu, gamal crichton, anna korhonen, andsampo pyysalo.
2016. how to train good word em-beddings for biomedical nlp.
in proceedings of the15th workshop on biomedical natural language pro-cessing, pages 166–174..xiang dai, sarvnaz karimi, ben hachey, and cecileparis.
2020. an effective transition-based model fordiscontinuous ner.
in proceedings of the 58th an-nual meeting of the association for computationallinguistics, pages 5860–5870, online.
associationfor computational linguistics..jacob devlin, ming-wei chang, kenton lee, andkristina toutanova.
2019. bert: pre-training ofdeep bidirectional transformers for language under-in proceedings of the 2019 conferencestanding.
of the north american chapter of the associationfor computational linguistics: human languagetechnologies, volume 1 (long and short papers),pages 4171–4186, minneapolis, minnesota.
associ-ation for computational linguistics..jenny rose finkel, trond grenager, and christopherincorporating non-local informa-manning.
2005.tion into information extraction systems by gibbssampling.
in proceedings of the 43rd annual meet-linguistics,ing on association for computational.
pages 363–370.
association for computational lin-guistics..jenny rose finkel and christopher d manning.
2009.nested named entity recognition.
in proceedings ofthe 2009 conference on emnlp, pages 141–150..joseph fisher and andreas vlachos.
2019. merge andlabel: a novel neural network architecture for nestedner.
in proceedings of the 57th annual meeting ofthe acl, pages 5840–5850..radu florian, hongyan jing, nanda kambhatla, andimed zitouni.
2006. factorizing complex models: acase study in mention detection.
in proceedings ofthe 21st international conference on computationallinguistics and the 44th annual meeting of the as-sociation for computational linguistics, pages 473–480. association for computational linguistics..matt gardner, joel grus, mark neumann, oyvindtafjord, pradeep dasigi, nelson f. liu, matthewpeters, michael schmitz, and luke s. zettlemoyer.
2018. allennlp: a deep semantic natural languagein proceedings of workshopprocessing platform.
for nlp open source software (nlp-oss), pages1–6, melbourne, australia.
association for compu-tational linguistics..zhijiang guo, yan zhang, and wei lu.
2019. atten-tion guided graph convolutional networks for rela-tion extraction.
in proceedings of the 57th annualmeeting of the association for computational lin-guistics, pages 241–251..kadri hacioglu, benjamin douglas, and ying chen.
2005. detection of entity mentions occurring in en-in proceedings of the con-glish and chinese text.
ference on human language technology and em-pirical methods in natural language processing,pages 379–386.
association for computational lin-guistics..sepp hochreiter and j¨urgen schmidhuber.
1997.neural computation,.
long short-term memory.
9(8):1735–1780..zhanming jie and wei lu.
2019. dependency-guidedin proceed-lstm-crf for named entity recognition.
ings of the 2019 conference on emnlp, pages3853–3863..zhanming jie, aldrian obaja muis, and wei lu.
2017.efﬁcient dependency-guided named entity recogni-tion.
in thirty-first aaai conference on artiﬁcialintelligence..meizhi ju, makoto miwa, and sophia ananiadou.
2018. a neural layered model for nested named en-in proceedings of the 2018 con-tity recognition.
ference of the north american chapter of the acl,pages 1446–1459..sarvnaz karimi, alejandro metke-jimenez, madonnakemp, and chen wang.
2015. cadec: a corpus ofadverse drug event annotations.
journal of biomedi-cal informatics, 55:73–81..4823arzoo katiyar and claire cardie.
2018. nested namedin proceedings of theentity recognition revisited.
2018 conference of the north american chapter ofthe acl, pages 861–871..wei lu and dan roth.
2015. joint mention extractionand classiﬁcation with mention hypergraphs.
in pro-ceedings of the 2015 conference on emnlp, pages857–867..j-d kim, tomoko ohta, yuka tateisi, and jun’ichitsujii.
2003. genia corpus—a semantically anno-bioinformatics,tated corpus for bio-textmining.
19(suppl 1):i180–i182..thomas n kipf and max welling.
2017..semi-supervised classiﬁcation with graph convolutionalnetworks..john lafferty, andrew mccallum, and fernandopereira.
2001. conditional random ﬁelds: prob-abilistic models for segmenting and labeling se-in proceedings of the eighteenth in-quence data.
ternational conference on machine learning, icml,volume 1, pages 282–289..guillaume lample, miguel ballesteros, sandeep sub-ramanian, kazuya kawakami, and chris dyer.
2016.neural architectures for named entity recognition.
in proceedings of the naacl, pages 260–270..xiaoya li, jingrong feng, yuxian meng, qinghonghan, fei wu, and jiwei li.
2020. a uniﬁed mrcin pro-framework for named entity recognition.
ceedings of the 58th annual meeting of the asso-ciation for computational linguistics, pages 5849–5859, online.
association for computational lin-guistics..hongyu lin, yaojie lu, xianpei han, and le sun.
2019a.
sequence-to-nuggets: nested entity mentionin proceed-detection via anchor-region networks.
ings of the 57th annual meeting of the associationfor computational linguistics, pages 5182–5192..ying lin, liyuan liu, heng ji, dong yu, and ji-awei han.
2019b.
reliability-aware dynamic fea-ture composition for name tagging.
in proceedingsof the 57th annual meeting of the acl, pages 165–174..xiao ling and daniel s weld.
2012. fine-grained en-tity recognition.
in twenty-sixth aaai conferenceon artiﬁcial intelligence..liyuan.
liu,.
xiang.
shang,.
ren,jingbojian peng,frank fangzheng xu, huan gui,empower sequence la-and jiawei han.
2018.beling with task-aware neurallanguage model.
in thirty-second aaai conference on artiﬁcialintelligence..pengfei liu, xipeng qiu, and xuan-jing huang.
2017.adversarial multi-task learning for text classiﬁca-tion.
in proceedings of the 55th annual meeting ofthe acl, pages 1–10..yi luan, dave wadden, luheng he, amy shah, mariostendorf, and hannaneh hajishirzi.
2019. a gen-eral framework for information extraction using dy-namic span graphs.
in proceedings of the 2019 con-ference of the north american chapter of the acl,pages 3036–3046..xuezhe ma and eduard hovy.
2016. end-to-end se-quence labeling via bi-directional lstm-cnns-crf.
inproceedings of the 54th acl, pages 1064–1074..christopher d manning, mihai surdeanu, john bauer,jenny rose finkel, steven bethard, and david mc-closky.
2014. the stanford corenlp natural languageprocessing toolkit.
in proceedings of 52nd annualmeeting of the acl: system demonstrations, pages55–60..alejandro metke-jimenez and sarvnaz karimi.
2016.concept identiﬁcation and normalisation for adversein bm-drug event discovery in medical forums.
did@ iswc.
citeseer..tomas mikolov, ilya sutskever, kai chen, greg s cor-rado, and jeff dean.
2013. distributed representa-tions of words and phrases and their composition-ality.
in c. j. c. burges, l. bottou, m. welling,z. ghahramani, and k. q. weinberger, editors, ad-vances in neural information processing systems26, pages 3111–3119.
curran associates, inc..aldrian obaja muis and wei lu.
2016. learning torecognize discontiguous entities.
in proceedings ofthe 2016 conference on emnlp, pages 75–84..aldrian obaja muis and wei lu.
2017. labeling gapsbetween words: recognizing overlapping mentionswith mention separators.
in proceedings of the 2017conference on emnlp, pages 2608–2618..jeffrey pennington, richard socher, and christopher dmanning.
2014. glove: global vectors for word rep-resentation.
in proceedings of the 2014 conferenceon empirical methods in natural language process-ing (emnlp), pages 1532–1543..matthew peters, mark neumann, mohit iyyer, mattgardner, christopher clark, kenton lee, and lukezettlemoyer.
2018. deep contextualized word repre-sentations.
in proceedings of the 2018 conferenceof the north american chapter of the acl, pages2227–2237..xiaohua liu, shaodian zhang, furu wei, and mingzhou.
2011. recognizing named entities in tweets.
in proceedings of the 49th annual meeting of theassociation for computational linguistics: humanlanguage technologies, pages 359–367..sameer pradhan, wendy chapman, suresh man, andsemeval-2014 task 7:guergana savova.
2014.analysis of clinical text.
in proc.
of the 8th interna-tional workshop on semantic evaluation (semeval2014. citeseer..4824congying xia, chenwei zhang, tao yang, yaliang li,nan du, xian wu, wei fan, fenglong ma, and s yuphilip.
2019. multi-grained named entity recogni-tion.
in proceedings of the acl, pages 1430–1440..jie yang, shuailong liang, and yue zhang.
2018. de-sign challenges and misconceptions in neural se-in proceedings of the 27th inter-quence labeling.
national conference on computational linguistics,pages 3879–3889..meishan zhang, yue zhang, and guohong fu.
2016.transition-based neural word segmentation.
in pro-ceedings of the 54th annual meeting of the associa-tion for computational linguistics (volume 1: longpapers), pages 421–431..yuhao zhang, peng qi, and christopher d manning.
2018. graph convolution over pruned dependencytrees improves relation extraction.
in proceedings ofthe 2018 conference on emnlp, pages 2205–2215..changmeng zheng, yi cai, jingyun xu, ho-fung le-ung, and guandong xu.
2019. a boundary-awareneural model for nested named entity recognition.
in proceedings of the 2019 conference on emnlp,pages 357–366..hao zhou, yue zhang, shujian huang, and jiajuna neural probabilistic structured-chen.
2015.prediction model for transition-based dependencyin proceedings of the 53rd annual meet-parsing.
ing of the association for computational linguisticsand the 7th international joint conference on natu-ral language processing (volume 1: long papers),pages 1213–1222..erik tjong kim sang and fien de meulder.
2003. in-troduction to the conll-2003 shared task: language-independent named entity recognition.
in proceed-ings of the seventh conference on natural languagelearning at hlt-naacl 2003, pages 142–147..jana strakov´a, milan straka, and jan hajic.
2019. neu-ral architectures for nested ner through linearization.
in proceedings of the 57th annual meeting of theacl, pages 5326–5331..hanna suominen, sanna salanter¨a, sumithra velupil-lai, wendy w chapman, guergana savova, noemieelhadad, sameer pradhan, brett r south, danielle lmowery, gareth jf jones, et al.
2013. overview ofthe share/clef ehealth evaluation lab 2013. in inter-national conference of the clef for european lan-guages, pages 212–231..buzhou tang, qingcai chen, xiaolong wang, yonghuiwu, yaoyun zhang, min jiang, jingqi wang, andhua xu.
2015. recognizing disjoint clinical con-cepts in clinical text using machine learning-basedmethods.
in amia annual symposium proceedings,volume 2015, page 1184. american medical infor-matics association..buzhou tang, jianglu hu, xiaolong wang, and qing-cai chen.
2018. recognizing continuous and discon-tinuous adverse drug reaction mentions from socialmedia using lstm-crf.
wireless communications andmobile computing, 2018..buzhou tang, yonghui wu, min jiang, joshua cdenny, and hua xu.
2013. recognizing and encod-ing discorder concepts in clinical text using machinelearning and vector space model.
clef (workingnotes), 665..ashish vaswani, noam shazeer, niki parmar, jakobuszkoreit, llion jones, aidan n gomez, łukaszkaiser, and illia polosukhin.
2017. attention is allyou need.
in nips, pages 5998–6008..david wadden, ulme wennberg, yi luan, and han-naneh hajishirzi.
2019. entity, relation, and eventextraction with contextualized span representations.
in proceedings of the 2019 conference on emnlp,pages 5788–5793..bailin wang and wei lu.
2018. neural segmental hy-inpergraphs for overlapping mention recognition.
proceedings of the 2018 conference on emnlp,pages 204–214..bailin wang and wei lu.
2019. combining spans intoentities: a neural two-stage approach for recogniz-in proceedings of theing discontiguous entities.
2019 conference on emnlp, pages 6217–6225..bailin wang, wei lu, yu wang, and hongxia jin.
2018.a neural transition-based model for nested mentionrecognition.
in proceedings of the 2018 conferenceon emnlp, pages 1011–1017..4825methodword2vecword2vec+bilstmelmoelmo+bilstmbertbert+bilstm.
clef clef-dis68.578.674.277.182.583.2.
43.556.448.155.859.063.3.table 6: results using different word representationmethods..a comparing different settings in the.
word representation layer.
the word representation layer addresses the prob-lem that how to transform a word into a vectorfor the usage of upper layers.
in this paper, weinvestigate several common word encoders in re-cent nlp research to generate word representa-tions, namely word2vec (mikolov et al., 2013)(or its variants such as glove (pennington et al.,2014)), elmo (peters et al., 2018) and bert (de-vlin et al., 2019).
given an input sentence x = {x1,x2, ..., xn }, we use different methods to representthem as vectors based on which word encoders weutilize:.
• if word2vec is used, each word xi will bedirectly transformed into a vector hi accord-ing to the pretrained embedding lookup table.
therefore, all the words in the sentence x cor-respond to a matrix h = {h1, h2, ..., hn }∈ rn ×dh, where dh denotes the dimension ofhi..• if elmo is used, each word xi will ﬁrstbe split into characters and then input intocharacter-level convolutional networks to ob-tain character-level word representations.
fi-nally, all word representations in the sen-tence will be input into 3-layer bilstms togenerate contextualized word representations,which can also be denoted as h = {h1, h2,..., hn }.
• if bert is used, each word xi will be con-verted into word pieces and then fed into a pre-trained bert module.
after the bert calcu-lation, each sentential word may involve vec-torial representations of several pieces.
herewe employ the representation of the beginningword piece as the ﬁnal word representation fol-lowing (wadden et al., 2019).
for instance,.
if “fevers” is split into “fever” and “##s”, therepresentation of “fever” is used as the wholeword representation.
therefore, all the wordsin the sentence x can also be represented as amatrix h = {h1, h2, ..., hn }.
in addition, a bidirectional lstm (bilstm) layercan be stacked on word encoders to further cap-ture contextual information in the sentence, whichis especially helpful for non-contextualized wordrepresentations such as word2vec.
concretely, theword representations h = {h1, h2, ..., hn } will beinput into the bilstm layer and consumed in theforward and backward orders.
assuming that theoutputs of the forward and backward lstms are←−−→−→h 2, ...,h n } andh = {←−h n } respectively.
thus, they can be concatenated←−(e.g., ˆhi = [h i]) to compose the ﬁnal wordrepresentations ˆh = {ˆh1, ˆh2, ..., ˆhn }..−→h 2, ...,.
←−h = {.
−→h 1,.
←−h 1,.
−→h i,.
we investigate the effects of different word en-coders and the bilstm layer in the experiments.
as shown in table 6, we compare the effects ofdifferent word representation methods in the clefand clef-dis datasets, where the size of the for-mer one is much bigger than that of the latter, inorder to also investigate the impact of the data sizeon word representations.
from the table, the ﬁrstobservation is that bert is the most effective wordrepresentation method.
surprisingly, word2vecis more effective than elmo, which may be be-cause elmo is exclusively based on characters andcannot effectively capture the whole meanings ofwords.
therefore, this suggests that it is better touse elmo with word2vec..second, we ﬁnd that bilstm is helpful in allcases, especially for word2vec.
this may be be-cause word2vec is a kind of non-contexualizedword representations, which particularly needs thehelp of bilstm to capture contexual information.
in contrast, bert is not very sensitive to the helpof bilstm as word2vec and elmo, which maybe because the transformer in bert has alreadycaptured contexual information..third, we observe that the effect of bilstm ismore obvious for the clef-dis dataset.
consid-ering the data sizes of the clef and clef-disdatasets, it is more likely that small datasets needthe help of bilstm, while big datasets are less sen-sitive to the bilstm and bert is usually enoughfor them to build word representations..4826examples.
dependency graphs.
this showed a mildly [displaced]1 and[angulated]2 inferior manubrial [[fracture]1]2..[[tone]1]2 was [increased]1 in the left lowerextremity and [decreased]2 in the left upperextremity..table 7: case studies.
bold words with the same number belong to the same entity..methodefrefr(+frp).
p81.281.4.r79.680.1.f180.480.7.table 8: effect of joint training between entity frag-ment recognition (efr) and fragment relation predic-tion (frp) on the clef-dis dataset.
p, r and f1 arethe results for efr..b case studies.
to understand how syntax information helps ourmodel to identify discontinuous or overlapped en-tities, we offer two examples in the clef datasetfor illustration, as shown in table 7. both the twoexamples are failed in the model without usingdependency information, but are correctly recog-nized in our ﬁnal model.
in the ﬁrst example, thefragments “displaced” and “fracture” of the sameentity are far away from each other in the originalsentence, while they are directly connected in thedependency graph.
similarly, in the second exam-ple, the distance between “tone” and “decreased”is 9 in the sentence, while their dependency dis-tance is only 1. these dependency connections canbe directly modeled in dependency-guided gcn,thus, resulting in strong clues for the ner, whichmakes our ﬁnal model work..c effect of joint training.
as mentioned in section 3.5, we employ multi-tasklearning to jointly train our model between twotasks, namely entity fragment recognition and frag-ment relation prediction.
therefore, it is interestingto show the effect of joint training by observingthe performance changes of the entity fragmentrecognition (efr) task before and after adding thefragment relation prediction (frp) task.
as seenin table 8, the f1 of entity fragment recognitionincreases by 0.3% after adding the frp task, whichshows that the frp task could improve the efr.
clef cadec genia ace05.
dhnheadldfds.
mlp layermlp sizeαβ.
400422086011501.01.0.
400422086011501.01.0.
76842641,68421501.01.0.
76841641,68421501.00.6.table 9: main hyper-parameter settings in our modelfor all the datasets.
dh–section 3.1; nhead, l anddf –section 3.2; ds–section 3.3; α and β–section 3.5.note that the hyper-parameter settings in the clef-disdataset is the same as those in the clef dataset..task.
this suggests that the interaction betweenentity fragment recognition and fragment relationprediction could beneﬁt our model, which also in-dicates that end-to-end modeling is more desirable..d implementation details.
is.
our modelimplemented based on al-lennlp (gardner et al., 2018).
the number ofparameters is about 117m plus bert.
we use onegpu of nvidia tesla v100 to train the model,which occupies about 10gb memories.
the train-ing time for one epoch is between 2∼6 minutes ondifferent datasets..table 9 shows the main hyper-parameter valuesin our model.
we tune the hyper-parameters basedon the results of about 5 trials on development sets.
below are the ranges tried for the hyper-parameters:the gcn layer l (1, 2), the gcn head nhead (2,4), the gcn output size df (20, 48, 64), the mlplayer (1, 2), the mlp size (100, 150, 200), the lossweight α and β (0.6, 0.8, 1.0).
since we employthe bertbase, the dimension dh of word repre-sentations is 768 except in the clef and cadec.
4827datasets, where we use a bilstm layer on top ofbert to obtain word representations since we ob-serve performance improvements.
we try 200 and400 hidden units for the bilstm layer..considering the domains of the datasets, weemploy clinical bert1 (alsentzer et al., 2019),scibert2 (beltagy et al., 2019) and googlebert3 (devlin et al., 2019) for the clef (andcadec), genia and ace05 datasets, respec-tively.
in addition, since our model needs syntaxinformation for dependency-guided gcn, but thedatasets do not contain gold syntax annotations,we utilize the stanford corenlp toolkit (manninget al., 2014) to perform dependency parsing..1https://github.com/emilyalsentzer/clinicalbert2https://github.com/allenai/scibert3https://github.com/google-research/bert.
4828