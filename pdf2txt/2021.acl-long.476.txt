robustifying multi-hop question answering throughpseudo-evidentiality training.
kyungjae lee1.
seung-won hwang2∗ sang-eun han1 dohyeon lee1.
1yonsei university.
2seoul national university.
abstract.
this paper studies the bias problem of multi-hop question answering models, of answeringcorrectly without correct reasoning.
one wayto robustify these models is by supervising tonot only answer right, but also with right rea-soning chains.
an existing direction is to an-notate reasoning chains to train models, requir-ing expensive additional annotations.
in con-trast, we propose a new approach to learn evi-dentiality, deciding whether the answer predic-tion is supported by correct evidences, with-out such annotations.
instead, we comparecounterfactual changes in answer conﬁdencewith and without evidence sentences, to gener-ate “pseudo-evidentiality” annotations.
we val-idate our proposed model on an original setand challenge set in hotpotqa, showing thatour method is accurate and robust in multi-hopreasoning..1.introduction.
multi-hop question answering (qa) is a task ofanswering complex questions by connecting infor-mation from several texts.
since the informationis spread over multiple facts, this task requires tocapture multiple relevant facts (which we refer asevidences) and infer an answer based on all theseevidences..however, previous works (min et al., 2019; chenand durrett, 2019; trivedi et al., 2020) observe“disconnected reasoning” in some correct answers.
it happens when models can exploit speciﬁc typesof artifacts (e.g., entity type), to leverage themas reasoning shortcuts to guess the correct an-swer.
for example, assume that a given questionis: “which country got independence when worldwar ii ended?” and a passage is: “korea got inde-pendence in 1945”.
although information (“worldwar ii ended in 1945”) is insufﬁcient, qa models.
∗correspond to seungwonh@snu.ac.kr.
figure 1: overview of our proposed supervision: usinganswerability and evidentiality.
predict “korea”, simply because its answer type iscountry (or, using shortcut)..to address the problem of reasoning shortcuts,we propose to supervise “evidentiality” – decidingwhether a model answer is supported by correct evi-dences (see figure 1).
this is related to the problemthat most of the early reader models for qa failedto predict whether questions are not answerable.
lack of answerability training led models to pro-vide a wrong answer with high conﬁdence, whenthey had to answer “unanswerable”.
similarly, weaim to train for models to recognize whether theiranswer is “unsupported” by evidences, as well.
inour work, along with the answerability, we train theqa model to identify the existence of evidences byusing passages of two types: (1) evidence-positiveand (2) evidence-negative set.
while the former.
proceedingsofthe59thannualmeetingoftheassociationforcomputationallinguisticsandthe11thinternationaljointconferenceonnaturallanguageprocessing,pages6110–6119august1–6,2021.©2021associationforcomputationallinguistics6110(cid:2414)(cid:2417)aevidence-negativeaevidence-positivenoreasonanswer is aareasoning chains :evidentiality (cid:2417)irrelevant sentences :answer-positiveaanswer-negativenoansweranswer is aanswerability (cid:2414)has both answer and evidence, the latter does nothave evidence supporting the answer, such that wecan detect models taking shortcuts..our ﬁrst research question is: how do we ac-quire evidence-positive and negative examples fortraining without annotations?
for evidence-positiveset, the closest existing approach (niu et al., 2020)is to consider attention scores, which can be con-sidered as pseudo-annotation for evidence-positiveset.
in other word, sentence s with high attentionscores, often used as an “interpretation” of whethers is causal for model prediction, can be selectedto build evidence-positive set.
however, follow-upworks (serrano and smith, 2019; jain and wal-lace, 2019) argued that attention is limited as anexplanation, because causality cannot be measured,without observing model behaviors in a counter-factual case of the same passage without s. in ad-dition, sentence causality should be aggregated tomeasure group causality of multiple evidences formulti-hop reasoning.
to annotate group causalityas “pseudo-evidentiality”, we propose interpretermodule, which removes and aggregates evidencesinto a group, to compare predictions in observa-tional and counterfactual cases..as a second research question, we ask howto learn from evidence-positive and evidence-negative set.
to this end, we identify two objec-tives: (o1) qa model should not be overconﬁ-dent in evidence-negative set, while (o2) conﬁdentin evidence-positive.
a naive approach to pursuethe former is to lower the model conﬁdence onevidence-negative set via regularization.
however,such regularization can cause violating (o2) dueto correlation between conﬁdence distributions forevidence-positive and negative set.
our solution isto selectively regularize, by purposedly training abiased model violating (o1), and decorrelate thetarget model from the biased model..for experiments, we demonstrate the impact ofour approach on hotpotqa dataset.
our empiri-cal results show that our model can improve qaperformance through pseudo-evidentiality, outper-forming other baselines.
in addition, our proposedapproach can orthogonally combine with anothersota model for additional performance gains..2 related work.
since multi-hop reasoning tasks, such as hot-potqa, are released, many approaches for the taskhave been proposed.
these approaches can be cat-.
egorized by strategies used, such as graph-basednetworks (qiu et al., 2019; fang et al., 2020), ex-ternal knowledge retrieval (asai et al., 2019), andsupporting fact selection (nie et al., 2019; groen-eveld et al., 2020)..our focus is to identify and alleviate reasoningshortcuts in multi-hop qa, without evidence an-notations.
models taking shortcuts were widelyobserved from various tasks, such as object detec-tion (singh et al., 2020), nli (tu et al., 2020),and also for our target task of multi-hop qa (minet al., 2019; chen and durrett, 2019; trivedi et al.,2020), where models learn simple heuristic rules,answering correctly but without proper reasoning.
to mitigate the effect of shortcuts, adversar-ial examples (jiang and bansal, 2019) can begenerated, or alternatively, models can be robus-tifed (trivedi et al., 2020) with additional supervi-sion for paragraph-level “sufﬁciency” – to identifywhether a pair of two paragraphs are sufﬁcient forright reasoning or not, which reduces shortcuts ona single paragraph.
while the binary classiﬁcationfor paragraph-sufﬁciency is relatively easy (96.7f1 in trivedi et al.
(2020)), our target of captur-ing a ﬁner-grained sentence-evidentiality is morechallenging.
existing qa model (nie et al., 2019;groeneveld et al., 2020) treats this as a supervisedtask, based on sentence-level human annotation.
incontrast, ours requires no annotation and focuseson avoiding reasoning shortcuts using evidentiality,which was not the purpose of evidence selection inthe existing model..3 proposed approach.
in this section, to prevent reasoning shortcuts, weintroduce a new approach for data acquiring andlearning.
we describe this task (section 3.1) andaddress two research questions, of generating labelsfor supervision (section 3.2) and learning (section3.3), respectively..3.1 task description.
our task deﬁnition follows distractor setting,between distractor and full-wiki in hotpotqadataset (yang et al., 2018), which consists of 112kquestions requiring the understanding of corre-sponding passages to answer correctly.
each ques-tion has a candidate set of 10 paragraphs (of whichtwo are positive paragraphs p + and eight are neg-ative p −), where the supporting facts for reason-ing are scattered in two positive paragraphs.
then,.
6111given a question q, the objective of this task isto aggregate relevant facts from the candidate setand estimate a consecutive answer span a. for taskevaluation, the estimated answer span is comparedwith the ground truth answer span in terms of f1score at word-level..3.2 generating examples for training.
answerability and evidentiality.
answerability for multi-hop reasoningfor answerability training in single-hop qa,datasets such as squad 2.0 (rajpurkar et al., 2018)provide labels of answerability, so that models canbe trained not to be overconﬁdent on unanswerabletext..similarly, we build triples of question q, an-swer a, and passage d, to be labeled for answer-ability.
hotpotqa dataset pairs q with 10 para-graphs, where evidences can be scattered to twoparagraphs.
based on such characteristic, concate-nating two positive paragraphs is guaranteed tobe answerable/evidential and concatenating twonegative paragraphs (with neither evidence noranswer) is guaranteed to be unanswerable.
wedeﬁne a set of answerable triplets (q, a, d) asanswer-positive set a+, and an unanswerable setas answer-negative set a−.
from the labels, wetrain a transformer-based model to classify the an-swerability (the detail will be discussed in the nextsection)..however,.
answerability cannot.
supervisewhether the given passage has all of these relevantevidences for reasoning.
this causes a lack ofgeneralization ability, especially on examples withan answer but no evidence..evidentiality for multi-hop reasoningwhile learning the answerability, we aim to cap-ture the existence of reasoning chains in thegiven passage.
to supervise the existence of ev-idences, we construct examples: evidence-positiveand evidence-negative set, as shown in figure 1.speciﬁcally, let e∗ be the ground truth of evi-dences to infer a, and s∗ be a sentence containingan answer a, corresponding to q. given q anda, expected labels ve of evidentiality, indicatingwhether the evidences for answering are sufﬁcientin the passage, are as follow:.
ve(q, a, d) |= t rue ⇔ e∗ = d, a ⊂ dve(q, a, d) |= f alse ⇔ e∗ (cid:54)⊂ d, a ⊂ d.(1).
we deﬁne a set of passages satisfying ve |= t rueas evidence-positive set e+, and a set satisfyingve |= f alse as evidence-negative set e−..since we do not use human-annotations, we aimto generate “pseudo-evidentiality” annotation.
first,for evidence-negative set, we modify answer sen-tence s∗ and unanswerable passages, and generateexamples with the three following types:.
• 1) answer sentence only: we remove all sen-tences in answerable passage except s∗, suchthat the input passage d becomes s∗, whichcontains a correct answer but no other evi-dences.
that is, ve(q, a, s ∗) |= f alse..• 2) answer sentence + irrelevant facts: we useirrelevant facts with answers as context, by con-catenating s∗ and unanswerable d. that is,ve(q, a, (s ∗; d)) |= f alse, where d ∈ p −..• 3) partial evidence + irrelevant facts: we usepartially-relevant and irrelevant facts as context,by concatenating d1 ∈ p + and d2 ∈ p −.
that is, ve(q, a,(d1; d2)) |= f alse..these evidence-negative examples do not have allrelevant evidences, thus if a model predicts thecorrect answer on such examples, it means that themodel learned reasoning shortcuts..second, building an evidence-positive set ismore challenging, because it is difﬁcult to capturemultiple relevant facts, with neither annotations e∗nor supervision.
our distinction is obtaining theabove annotation from model itself, by interpretingthe internal mechanism of models.
on a trainedmodel, we aim to ﬁnd inﬂuential sentences in pre-dicting correct answer a, among sentences in ananswerable passage.
then, we consider them as apseudo evidence-positive set.
since such pseudo la-bels relies on the trained model which is not perfect,100% recall of ve(q, a, d) |= t rue in eq.
(1) isnot guaranteed, though we observe 87% empiricalrecall (table 1)..section 1 discusses how interpretation, such asattention scores (niu et al., 2020), can be pseudo-evidentiality.
for qa tasks, an existing approach(perez et al., 2019) uses answer conﬁdence for ﬁnd-ing pseudo-evidences, as we discuss below:.
(a) accumulative interpreter: to consider multi-ple sentences as evidences, the existing approach(perez et al., 2019) iteratively inserts sentence siinto set et−1, with a highest probability at t-th iter-.
6112ation, as follows:.
∆psi = p (a|q, si ∪ et−1) − p (a|q, et−1).
tˆe.
= argmax.
∆psi,.
et = ˆe.
t.∪ et−1.
si.
(2)where e0 starts with the sentence s∗ containinganswer a, which is minimal context for our task.
this method can consider multiple sentences asevidence by inserting iteratively into a set, but can-not consider the effect of erasing sentences fromreasoning chain..(b) our proposed interpreter: to enhance the in-terpretability, we consider both erasing and insert-ing each sentence, in contrast to accumulative inter-preter considering only the latter.
intuitively, eras-ing evidence would change the prediction signiﬁ-cantly, if such evidence is causally salient, whichwe compute as follows:.
∆psi = p (a|q, d) − p (a|q, (d\si)).
(3).
where (d\si) is a passage out of sentence si.
wehypothesize that breaking reasoning chain, by eras-ing si, should signiﬁcantly decrease p (a|·).
inother words, si with higher ∆psi is salient.
com-bining the two saliency scores in eq.
(2),(3), ourﬁnal saliency is as follows:∆psi = p (a|q, si ∪ et−1) − (cid:40)(cid:40)(cid:40)(cid:40)(cid:40)(cid:40)(cid:40)p (a|q, et−1)+ (cid:24)(cid:24)(cid:24)(cid:24)(cid:24)(cid:24)p (a|q, d) − p (a|q, (d\(si ∪ et−1))).
(4)where the constant values can be omitted inargmax.
at each iteration, the sentence that maxi-mize ∆psi is selected, as done in eq.
(2).
this pro-motes selection that increases conﬁdence p (a|·)on important sentences, and decreases conﬁdenceon unimportant sentences.
we stop the iterationsif ∆psi < 0 or t = t , then the ﬁnal sentences inet=t are a pseudo evidence-positive set e+.
to re-duce the search space, we empirically set t = 51.brieﬂy, we obtain the labels of answerability and.
evidentiality, as follows:.
• answer-positive a+ and negative a− set: theformer has both answer and evidences, and thelatter has neither..• evidence-positive e+ and negative e− set: theformer is expected to have all the evidences,and the latter has an answer with no evidence..1based on observations that 99% in hotpotqa require less.
than 6 evidence sentences for reasoning..3.3 learning answerability & evidentiality.
in this section, our goal is to learn the above labelsof answerability and evidentiality..supervising answers and answerability (base)as optimizing qa model is not our focus, weadopt the existing model in (min et al., 2019).
asthe architecture of qa modal, we use a powerfultransformer-based model – roberta (liu et al.,2019), where the input is [cls] question[sep] passage [eos].
the output of themodel is as follows:.
h = roberta (input) ∈ rn×dos = f1(h), oe = f2(h)p s = sof tmax(os), p e = sof tmax(oe).
(5)where f1 and f2 are fully connected layers withthe trainable parameters ∈ rd, p s and p e are thethe probabilities of start and end positions, d is theoutput dimension of the encoder, n is the size ofthe input sequence..for answerability, they build a classiﬁer throughthe hidden state h[0,:] of [cls] token that repre-sents both q and d. as hotpotqa dataset cov-ers both yes-or-no and span-extraction questions,which we follow the convention of (asai et al.,2019) to support both as a multi-class classiﬁcationproblem of predicting the four probabilities:.
p cls = sof tmax(w1h[0,:]).
= [pspan, pyes, pno, pnone].
(6).
where pspan, pyes, pno, and pnone denote the prob-abilities of the answer type being span, yes, no,and no answer, respectively, and w1 ∈ r4×d is thetrainable parameters.
for training answer span andits class, the loss function of example i is the sumof cross entropy losses (dce), as follows:.
(cid:16).
log(p s.(cid:17)si) + log(p eei).
dce(pi, ai) = −dce(p cls.
, ci) = −log(p clsci )la(i) = dce(pi, ai) + dce(p cls.
i.i., ci).
(7).
where si and ei are the starting and ending positionof answer a, respectively, and ci is the index of theactual class ci in example i..supervising evidentialityas overviewed in section 1, base model is reportedto take a shortcut, or a direct path between answera and question q, neglecting implicit intermediate.
6113figure 2: learning of our proposed approach: (a) training qa model for evidentiality, extracted by interpreter.
(b)our qa predictor for learning decorrelated features on biased examples..paths (evidences).
speciﬁcally, we present the twoobjectives for unbiased models:.
• (o1): qa model should not be overconﬁdent on.
passages with no evidences (i.e., on e−)..• (o2): qa model should be conﬁdent on pas-sages with both answer/evidences (i.e., on e+).
for (o1), as a naive approach, one may considera regularization term to avoid overconﬁdence onevidence-negative set e−.
overconﬁdent answerdistribution would be diverged from uniform dis-tribution, such that kullback–leibler (kl) diver-gence kl(p||q), where p and q are the answerprobabilities and the uniform distribution, respec-tively, is high when overconﬁdent:.
r =.
dkl(p (ai|qi, di)||punif orm) (8).
(cid:88).
i ∈ e−.
where punif orm indicates uniform distribution.
this regularization term r forces the answer prob-abilities on e− to be closer to the uniform one..however, one reported risk (utama et al., 2020;grand and belinkov, 2019) is that suppressing datawith biases has a side-effect of lowering conﬁdenceon unbiased data (especially on in-distribution).
similarly, in our case, regularizing to keep the con-ﬁdence low for e−, can cause lowering that fore+, due to their correlation.
in other words, pursu-ing (o1) violates (o2), which we observe later infigure 3. our next goal is thus to decorrelate twodistributions on e+ and e− to satisfy both (o1)and (o2)..figure 2(b) shows how we feed the hidden statesh into two predictors.
predictor f is for learningthe target distribution and predictor g is purposedlytrained to be overconﬁdent on evidence-negativeset e−, where this biased answer distribution isdenoted as ˆp .
we regularize target distribution pto diverge from the biased distribution of ˆp ..formally, the biased answer distributions ˆp ( ˆp s.and ˆp e) are as follows:.
ˆos = g1(h),ˆp s = sof tmax( ˆos),.
ˆoe = g2(h)ˆp e = sof tmax( ˆoe).
(9).
where g1 and g2 are fully connected layers withthe trainable parameters ∈ rd.
then, we optimizeˆp to predict answer a on evidence-negative sete−, which makes layer g biased (taking shortcuts),and regularize f by maximizing kl divergencebetween p and ﬁxed ˆp .
the regularization term ofexample i ∈ e− is as follows:.
ˆr(i) = dce( ˆpi, ai) − λdkl( ˆpi||pi).
(10).
where λ is a hyper-parameter.
this loss ˆr is opti-mized on only evidence-negative set e−..lastly, to pursue (o2), we train on e+, as doneon a+.
however, in initial steps of training, ourinterpreter is not reliable, since the qa model isnot trained enough yet.
we thus train without e+for the ﬁrst k epochs, then extract e+ at k epochand continue to train on all sets, as shown in figure2(a).
in the ﬁnal loss function, we apply different.
6114(cid:1860)(cid:1842)(cid:3046)(cid:1842)(cid:3032)(cid:3552)(cid:1842)(cid:3046)(cid:3552)(cid:1842)(cid:3032)(cid:1858)answer span(cid:1859)answer span(cid:2414)(cid:2878)or(cid:2417)(cid:2878)(cid:2417)(cid:2879)(cid:1828)(cid:1857)(cid:1859)(cid:1861)(cid:1866)(cid:1831)(cid:1866)(cid:1856)(cid:1828)(cid:1857)(cid:1859)(cid:1861)(cid:1866)(cid:1831)(cid:1866)(cid:1856)(cid:1828)(cid:1857)(cid:1859)(cid:1861)(cid:1866)(cid:1831)(cid:1866)(cid:1856)train settrain set(cid:1832)(cid:1870)(cid:1867)(cid:1878)(cid:1857)(cid:1866)model(cid:2267)argmax(cid:3020)(cid:3284)(cid:959)(cid:1842)(cid:3020)(cid:3284)evidence-positive (cid:2417)(cid:2878)train setinterpreter(cid:2283)(cid:2414)(cid:2878)(cid:2414)(cid:2879)(cid:2417)(cid:2879)(cid:2270), (cid:1845)(cid:3036)(cid:2417)(cid:2878)model(a)(b)(cid:1854)(cid:1861)(cid:1853)(cid:1871)(cid:1857)(cid:1856)(cid:1864)(cid:1853)(cid:1877)(cid:1857)(cid:1870)(cid:1856)(cid:1857)(cid:1854)(cid:1861)(cid:1853)(cid:1871)(cid:1857)(cid:1856)(cid:1864)(cid:1853)(cid:1877)(cid:1857)(cid:1870)(cid:1856)(cid:1857)(cid:1855)(cid:1867)(cid:1870)(cid:1870)(cid:1857)(cid:1864)(cid:1853)(cid:1872)(cid:1857)(cid:1856)(3) retraining qamodelmodel(cid:2267)(cid:2283)(cid:2270)train set(cid:2414)(cid:2878)(cid:2414)(cid:2879)(cid:2417)(cid:2878)(cid:2417)(cid:2879)evidence-positive set(2) extracting(cid:2417)(cid:2878)(1) training qamodelmodel(cid:2267)(cid:2283)(cid:2270)losses as set e and a:.
ltotal =.
(cid:88).
la(i) +.
(cid:88).
ˆr(i).
i ∈ a+,−(cid:88).
+.
i ∈ e+.
i ∈ e−.
(11).
u(t − k) · la(i).
where the function u is a delayed step function (1when epoch t is greater than k, 0 otherwise)..3.4 passage selection at inference time.
for our multi-hop qa task, it requires to ﬁnd an-swerable passages with both answer and evidence,from candidate passages.
while we can access theground-truth of answerability in training set, weneed to identify the answerability of (q, d) at in-ference time.
for this, we consider two directions:(1) paragraph pair selection, which is speciﬁc tohotpotqa, and (2) supervised evidence selectortrained on pseudo-labels..for (1), we consider the data characteristic, men-tioned in section 3.1; we know one pair of para-graphs is answerable/evidential (when both para-graphs are positive, or p +).
thus, the goal is toidentify the answerable pair of paragraphs, from allpossible pairs pij = {(pi, pj) : pi ∈ p, pj ∈ p}(denoted as paired-paragraph).
we can let themodel select one pair with highest estimated an-swerability, 1 − pnone in eq.
(6), and predict an-swers on the paired passage, which is likely to beevidential..for (2), some pipelined approaches (nie et al.,2019; groeneveld et al., 2020) design an evidenceselector, extracting top k sentences from all candi-date paragraphs.
while they supervise the modelusing ground-truth of evidences, we assume there isno such annotation, thus train on pseudo-labels e+.
we denote this setting as selected-evidences.
forevidence selector, we follow an extracting methodin (beltagy et al., 2020), where the special token[s] is added at ending position of each sentence,and h[si] from bert indicates i-th sentence embed-ding.
then, a binary classiﬁer fevi(h[si]) is trainedon the pseudo-labels, where fevi is a fully con-nected layer.
during training, the classiﬁer identi-ﬁes whether each sentence is evidence-positive (1)or negative (0).
at inference time, we ﬁrst selecttop 5 sentences2 on paragraph candidates, and theninsert the selected evidences into qa model fortesting..2table 1 shows the precision and recall of top5 sentences..table 1: the precision and recall of pseudo evidencesfrom interpreter, compared to the ground truth (gt)..gt evidencesanswerable a+e+ (train set)e+ (dev set).
# of sent2.386.453.645.00.prec recall100.
100.
100.
36.9486.6461.1390.3546.12.while we discuss how to get the answerable pas-sage above, we can use the passage setting for eval-uation.
to show the robustness of our model, weconstruct a challenge test set by excluding easy ex-amples (i.e., easy to take shortcuts).
to detect sucheasy examples, we build a set of single-paragraphpi, that none of it is evidential in hotpotqa, as thedataset avoids having all evidences in a single para-graph, to discourage single-hop reasoning.
if qamodel predicts the correct answer on the (uneviden-tial) single-paragraph, we remove such examplesin hotpotqa, and deﬁne the remaining set as thechallenge set..4 experiment.
in this section, we formulate our research questionsto guide our experiments and describe evaluationresults corresponding to each question..research questions to evaluate the effective-ness of our method, we address the following re-search questions:.
• rq1: how effective is our proposed method.
for a multi-hop qa task?.
• rq2: does our interpreter effectively extractpseudo-evidentiality annotations for training?.
• rq3: does our method avoid reasoning short-.
cuts in unseen data?.
implementation our implementation settingsfor qa model follow roberta (base version with12 layers) (liu et al., 2019).
we use the adam op-timizer with a learning rate of 0.00005 and a batch-size of 8 on rtx titan.
we extract the evidence-positive set after 3 epoch (k=3 in eq.
(11)) and re-train for 3 epochs.
as a hyper-parameter, we searchλ among {1, 0.1, 0.01}, and found the best value(λ=0.01), based on 5% hold-out set sampled fromthe training set..6115table 2: the comparison of the proposed models on the original set and challenge set..model.
input at inference.
question answering (f1)original set challenge set.
without external knowledgesingle-paragraph qab-i:single-paragraph qab-ii:o-i:our modelo-ii: our modelo-iii: our model (full)with external knowledgeasai et al.
(2019)c-i:asai et al.
(2019) + oursc-ii:.
single-paragraphpaired-paragraphsingle-paragraphpaired-paragraphselected-evidences.
retrieved-evidencesretrieved-evidences.
68.6562.0132.6168.0870.21.
73.3073.95.
0.030.0719.8141.6944.57.
48.5450.15.table 3: the ablation study on our full model..model.
qa (f1)original challenge.
70.21our model (full)(a) remove e+68.51(b) remove e+ & e−66.42(c) replace ˆr with r 69.64.
44.5740.7840.7542.54.metrics we report standard f1 score for hot-potqa, to evaluate the overall qa accuracy to ﬁndthe correct answers.
for evidence selection, we alsoreport f1 score, precision, and recall to evaluatethe sentence-level evidence retrieval accuracy..4.1 rq1: qa effectiveness.
evaluation set.
• original set: we evaluate our proposed ap-proach on multi-hop reasoning dataset, hot-potqa3 (yang et al., 2018).
hotpotqa contains112k examples of multi-hop questions and an-swers.
for evaluation, we use the hotpotqa devset (distractor setting) with 7405 examples..• challenge set: to validate the robustness, weconstruct a challenge set where qa modelon single-paragraph gets zero f1, while suchmodel achieves 67 f1 in the original set.
thatis, we exclude instances with f1 > 0, where theqa model predicts an answer without right rea-soning.
the exclusion makes sure the baselineobtains zero f1 on the challenge set.
the num-ber of surviving examples in our challenge set is1653 (21.5% of dev set)..3https://hotpotqa.github.io/.
baselines, our models, and competitors as abaseline, we follow the previous qa model (minet al., 2019) trained on single-paragraphs.
we testour model on single-paragraphs, paired-paragraphsand selected evidences settings discussed in sec-tion 3.4. as a strong competitor, among releasedmodels for hotpotqa, we implement a state-of-the-art model (asai et al., 2019)4, using externalknowledge and a graph-based retriever..main results this section includes the resultsof our model for multi-hop reasoning.
as shown intable 2, our full model outperforms baselines onboth original and challenge set..we can further observe that i) when testedon single-paragraphs, where forced to take short-cuts, our model (o-i) is worse than the baseline(b-i), which indicates that b-i learned the short-cuts.
in contrast, o-ii outperforms b-ii on paired-paragraphs where at least one passage candidatehas all the evidences..ii) when tested on evidences selected by ourmethod (o-iii), we can improve f1 scores on bothoriginal set and challenge set.
this noise ﬁlteringeffect of evidence selection, by eliminating irrel-evant sentences, was consistently observed in asupervised setting (nie et al., 2019; groeneveldet al., 2020; beltagy et al., 2020), which we couldreproduce without annotation..iii) combining our method with sota (c-i) (asai et al., 2019) leads to accuracy gains inboth sets.
c-i has distinctions of using externalknowledge of reasoning paths, to outperform mod-els without such advantages, but our method cancontribute to complementary gains..4highest performing model in the leaderboard of hot-.
potqa with public code release.
6116table 4: the comparison of the proposed models for evidence selection.
model.
retrieval-based air (yadav et al., 2020)accumulative-based interpreter on our qa model(a) interpreter on single-paragraph qa(b) interpreter on our qa model w/ r(c) interpreter on our qa model (full).
evidence selection.
f166.1654.0556.7670.3069.35.precision recall69.5762.3863.7187.1086.59.
63.0653.5657.5062.0461.09.
(a) single-paragraph qa.
(b) ours w/ r.(c) ours w/ ˆr (full).
(d) three models on e+.
figure 3: conﬁdence analysis: conﬁdence scores of three models in the ascending order, on e+ (light color) ande− (dark colar).
(a) base model trained on single-paragraphs.
(b) our model with r. (c) our full model with ˆr.
(d) comparison of three models on e+..ablation study as shown in table 3, we con-duct an ablation study of o-iii in table 2. in (a),we remove e+ from interpreter, in training time.
on the qa model without e+, the performancedecreased signiﬁcantly, suggesting the importanceof evidence-positive set.
in (b), we remove evi-dentaility labels of both e+ and e−, and observedthat the performance drop is larger compared toother variants.
through (a) and (b), we show thattraining our evidentiality labels can increase qaperformance.
in (c), we replace ˆr with r, remov-ing layer g to train biased features.
on the replacedregularization, the performance also decreased, sug-gesting that training ˆr is effective for a multi-hopqa task..4.2 rq2: evaluation of pseudo-evidentiality.
annotation.
in this section, we evaluate the effectiveness of ourinterpreter, which generates evidences on trainingset, without supervision.
we compare the pseudoevidences with human-annotation, by sentence-level.
for evaluation, we measure sentence-level f1score, precision and recall, following the evidenceselection evaluation in (yang et al., 2018)..as a baseline, we implement the retrieval-basedmodel, air (yadav et al., 2020), which is an un-supervised method as ours.
as shown in table 4,our interpreter on our qa model outperforms the.
retrieval-based method, in terms of f1 and recall,while the baseline (air) achieves the highest pre-cision (63.06%).
we argue recall, aiming at identi-fying all evidences, is much critical for multi-hopreasoning, for our goal of avoiding disconnectedreasoning, as long as precision remains higher thanprecision of answerable a+ (36.94%), in table 1.as variants of our method, we test our inter-preter on various models.
first, when comparing(a) and (c), our full model (c) outperforms the base-line (a) over all metrics.
the baseline (a) trainedon single-paragraphs got biased, thus the evidencesgenerated by the biased model are less accurate.
second, the variant (b) trained by r outperforms(c) our full model.
in eq.
(8), the loss term r doesnot train layer g for biased features, unlike ˆr in eq.
(10).
this shows that learning g results in perfor-mance degradation for evidence selection, despiteperformance gain in qa..4.3 rq3: generalization.
in this section, to show that our model avoids rea-soning shortcuts for unseen data, we analyze theconﬁdence distribution of models on the evidence-positive and negative set.
in dev set, we treat theground truth of evidences as e+, and a single sen-tence containing answer as e− (each has 7k q-dpairs).
on these set, figure 3 shows conﬁdencep (a|q, d) of three models; (a), (b), and (c) men-.
611700.20.40.60.8100.20.40.60.81𝔼𝔼+𝔼𝔼−𝔼𝔼+𝔼𝔼−0.60.81.00.4confidence0.200%50%100%0.60.81.00.4confidence0.200%50%100%00.20.40.60.8100.20.40.60.81𝔼𝔼+𝔼𝔼−𝔼𝔼+𝔼𝔼−0.60.81.00.4confidence0.200%50%100%0.60.81.00.4confidence0.200%50%100%00.20.40.60.8100.20.40.60.81𝔼𝔼+𝔼𝔼−𝑂𝑂𝑂𝑂𝑂𝑂𝑂𝑂𝑂𝑂𝑂𝑂𝑂𝑂𝑂𝑂𝑂𝑂𝐵𝐵𝐵𝐵𝑂𝑂𝐵𝐵0.60.81.00.4confidence0.200%50%100%0.60.81.00.4confidence0.200%50%100%00.20.40.60.8100.20.40.60.81𝔼𝔼+𝔼𝔼−𝑏𝑏𝑂𝑂𝑂𝑂𝑂𝑂𝑂𝑂𝑤𝑤/𝑅𝑅𝑎𝑎𝐵𝐵𝑎𝑎𝑂𝑂𝐵𝐵0.60.81.00.4confidence0.200%50%100%0.60.81.00.4confidence0.200%50%100%𝑐𝑐𝑂𝑂𝑂𝑂𝑂𝑂𝑂𝑂𝑤𝑤/�𝑅𝑅tioned in section 4.2. we sort the conﬁdence scoresin ascending order, where y-axis indicates the con-ﬁdence and x-axis refers to the sorted index.
thus,the colored area indicates the dominance of conﬁ-dence distribution.
ideally, for a debiased model,the area on evidence-positive set should be large,while that on evidence-negative should be small..desirably, in figure 3(a), the area under thecurve for e− should decrease for pursuing (o1),moving along blue arrow, while that of e+ shouldincrease for (o2), as red arrow shows.
in figure3(b), our model with r follows blue arrow, with asmaller area under the curve for e−, while keepingthat of e+ comparable to figure 3(a).
for the com-parison, figure 3(d) shows all curves on e+.
infigure 3(c), our full model follows both directionsof blue and red arrows, which indicates that ourssatisﬁed both (o1) and (o2)..5 conclusion.
in this paper, we propose a new approach to trainmulti-hop qa models, not to take reasoning short-cuts of guessing right answers without sufﬁcientevidences.
we do not require annotations and gen-erate pseudo-evidentiality instead, by regularizingqa model from being overconﬁdent when evi-dences are insufﬁcient.
our experimental resultsshow that our method outperforms baselines onhotpotqa and has the effectiveness to distinguishbetween evidence-positive and negative set..acknowledgements.
this research was supported by iitp grant fundedby the korea government (msit) (no.2017-0-01779, xai) and itrc support program funded bythe korea government (msit) (iitp-2021-2020-0-01789)..references.
akari asai, kazuma hashimoto, hannaneh hajishirzi,richard socher, and caiming xiong.
2019. learn-ing to retrieve reasoning paths over wikipedia graphfor question answering.
in international conferenceon learning representations..iz beltagy, matthew e peters, and arman cohan.
2020.longformer: the long-document transformer.
arxivpreprint arxiv:2004.05150..jifan chen and greg durrett.
2019. understandingdataset design choices for multi-hop reasoning.
inproceedings of the 2019 conference of the north.
american chapter of the association for computa-tional linguistics..yuwei fang, siqi sun, zhe gan, rohit pillai, shuo-hang wang, and jingjing liu.
2020. hierarchicalgraph network for multi-hop question answering.
inproceedings of the 2020 conference on empiricalmethods in natural language processing (emnlp),pages 8823–8838..gabriel grand and yonatan belinkov.
2019. adver-sarial regularization for visual question answering:in pro-strengths, shortcomings, and side effects.
ceedings of the second workshop on shortcomingsin vision and language, pages 1–13..dirk groeneveld, tushar khot, ashish sabharwal, et al.
2020. a simple yet strong pipeline for hotpotqa.
inproceedings of the 2020 conference on empiricalmethods in natural language processing (emnlp),pages 8839–8845..sarthak jain and byron c wallace.
2019. attention isin proceedings of the 2019 con-not explanation.
ference of the north american chapter of the asso-ciation for computational linguistics: human lan-guage technologies, volume 1 (long and short pa-pers), pages 3543–3556..yichen jiang and mohit bansal.
2019. avoiding rea-soning shortcuts: adversarial evaluation, training,in pro-and model development for multi-hop qa.
ceedings of the 57th annual meeting of the asso-ciation for computational linguistics, pages 2726–2736..yinhan liu, myle ott, naman goyal, jingfei du, man-dar joshi, danqi chen, omer levy, mike lewis,luke zettlemoyer, and veselin stoyanov.
2019.roberta: a robustly optimized bert pretraining ap-proach.
arxiv preprint arxiv:1907.11692..sewon min, eric wallace, sameer singh, matt gardner,hannaneh hajishirzi, and luke zettlemoyer.
2019.compositional questions do not necessitate multi-hop reasoning.
in proceedings of the 57th annualmeeting of the association for computational lin-guistics, pages 4249–4257..yixin nie, songhe wang, and mohit bansal.
2019.revealing the importance of semantic retrieval forin proceedings of themachine reading at scale.
2019 conference on empirical methods in natu-ral language processing and the 9th internationaljoint conference on natural language processing(emnlp-ijcnlp), pages 2553–2566..yilin niu, fangkai jiao, mantong zhou, ting yao, min-lie huang, et al.
2020. a self-training method formachine reading comprehension with soft evidenceextraction.
in proceedings of the 58th annual meet-ing of the association for computational linguistics,pages 3916–3927..6118in proceedings of the 2018 conference on empiri-cal methods in natural language processing, pages2369–2380..ethan perez, siddharth karamcheti, rob fergus, ja-son weston, douwe kiela, and kyunghyun cho.
2019. finding generalizable evidence by learningin proceedings of theto convince q\&a models.
2019 conference on empirical methods in natu-ral language processing and the 9th internationaljoint conference on natural language processing(emnlp-ijcnlp), pages 2402–2411..lin qiu, yunxuan xiao, yanru qu, hao zhou, lei li,weinan zhang, and yong yu.
2019. dynamicallyfused graph network for multi-hop reasoning.
inproceedings of the 57th annual meeting of the asso-ciation for computational linguistics, pages 6140–6150..pranav rajpurkar, robin jia, and percy liang.
2018.know what you don’t know: unanswerable ques-tions for squad.
in proceedings of the 56th annualmeeting of the association for computational lin-guistics (volume 2: short papers), pages 784–789..soﬁa serrano and noah a smith.
2019..is attentioninterpretable?
in proceedings of the 57th annualmeeting of the association for computational lin-guistics, pages 2931–2951..krishna kumar singh, dhruv mahajan, kristen grau-man, yong jae lee, matt feiszli, and deepti ghadi-yaram.
2020. don’t judge an object by its context:learning to overcome contextual bias.
in proceed-ings of the ieee/cvf conference on computer vi-sion and pattern recognition, pages 11070–11078..harsh trivedi, niranjan balasubramanian, tusharkhot, and ashish sabharwal.
2020. is multihop qain dire condition?
measuring and reducing discon-nected reasoning.
in proceedings of the 2020 con-ference on empirical methods in natural languageprocessing (emnlp), pages 8846–8863..lifu tu, garima lalwani, spandana gella, and he he.
2020. an empirical study on robustness to spuri-ous correlations using pre-trained language models.
transactions of the association for computationallinguistics, 8:621–633..prasetya ajie utama, naﬁse sadat moosavi, and irynagurevych.
2020. mind the trade-off: debiasing nlumodels without degrading the in-distribution perfor-in proceedings of the 58th annual meet-mance.
ing of the association for computational linguistics,pages 8717–8729..vikas yadav, steven bethard, and mihai surdeanu.
2020. unsupervised alignment-based iterative evi-dence retrieval for multi-hop question answering.
inproceedings of the 58th annual meeting of the asso-ciation for computational linguistics, pages 4514–4525..zhilin yang, peng qi, saizheng zhang, yoshua bengio,william cohen, ruslan salakhutdinov, and christo-pher d manning.
2018. hotpotqa: a dataset fordiverse, explainable multi-hop question answering..6119