making pre-trained language models better few-shot learners.
tianyu gao†∗ adam fisch‡∗ danqi chen††princeton university ‡massachusetts institute of technology{tianyug,danqic}@cs.princeton.edufisch@csail.mit.edu.
abstract.
the recent gpt-3 model(brown et al.,2020) achieves remarkable few-shot perfor-mance solely by leveraging a natural-languageprompt and a few task demonstrations as in-put context.
inspired by their ﬁndings, westudy few-shot learning in a more practical sce-nario, where we use smaller language modelsfor which ﬁne-tuning is computationally efﬁ-cient.
we present lm-bff—better few-shotfine-tuning of language models1—a suite ofsimple and complementary techniques for ﬁne-tuning language models on a small number ofannotated examples.
our approach includes(1) prompt-based ﬁne-tuning together with anovel pipeline for automating prompt genera-tion; and (2) a reﬁned strategy for dynamicallyand selectively incorporating demonstrationsinto each context.
finally, we present a sys-tematic evaluation for analyzing few-shot per-formance on a range of nlp tasks, includingclassiﬁcation and regression.
our experimentsdemonstrate that our methods combine to dra-matically outperform standard ﬁne-tuning pro-cedures in this low resource setting, achievingup to 30% absolute improvement, and 11% onaverage across all tasks.
our approach makesminimal assumptions on task resources and do-main expertise, and hence constitutes a strongtask-agnostic method for few-shot learning.2.
1.introduction.
the gpt-3 model (brown et al., 2020) has madewaves in the nlp community by demonstrating as-tounding few-shot capabilities on myriad languageunderstanding tasks.
given only a natural lan-guage prompt and a few demonstrations of the task,gpt-3 is able to make accurate predictions withoutupdating any of the weights of its underlying lan-.
*the ﬁrst two authors contributed equally.
1alternatively, language models’ best friends forever.
2our implementation is publicly available at https://.
github.com/princeton-nlp/lm-bff..guage model.
however, while remarkable, gpt-3consists of 175b parameters, which makes it chal-lenging to use in most real-wold applications..in this work, we study a more practical scenarioin which we only assume access to a moderately-sized language model such as bert (devlin et al.,2019) or roberta (liu et al., 2019), and a smallnumber of examples (i.e., a few-shot setting), whichwe can use to ﬁne-tune the weights of the languagemodel.
this setting is appealing as (1) such mod-els can be trained on typical research hardware;(2) few-shot settings are realistic, as it is generallyboth easy to acquire a few annotations (e.g., 32examples) and efﬁcient to train on them; and (3)updating parameters typically leads to better perfor-mance.
inspired by gpt-3’s ﬁndings, we proposeseveral novel strategies for expanding its few-shotlearning abilities to our setting, considering bothclassiﬁcation and—for the ﬁrst time—regression.
first, we follow the route of prompt-based pre-diction, ﬁrst developed by the gpt series (radfordet al., 2018, 2019; brown et al., 2020) for zero-shotprediction and recently studied by pet (schick andsch¨utze, 2021a,b) for ﬁne-tuning.
prompt-basedprediction treats the downstream task as a (masked)language modeling problem, where the model di-rectly generates a textual response (referred to asa label word) to a given prompt deﬁned by a task-speciﬁc template (see figure 1(c)).
finding theright prompts, however, is an art—requiring bothdomain expertise and an understanding of the lan-guage model’s inner workings.
even if signiﬁcanteffort is invested, manual prompts are likely to besuboptimal.
we address this issue by introducingautomatic prompt generation, including a prunedbrute-force search to identify the best working labelwords, and a novel decoding objective to automat-ically generate templates using the generative t5model (raffel et al., 2020)—all of which only re-quire the few-shot training data.
this allows us.
proceedingsofthe59thannualmeetingoftheassociationforcomputationallinguisticsandthe11thinternationaljointconferenceonnaturallanguageprocessing,pages3816–3830august1–6,2021.©2021associationforcomputationallinguistics3816figure 1: an illustration of (a) masked language model (mlm) pre-training, (b) standard ﬁne-tuning, and (c) ourproposed lm-bff using prompt-based ﬁne-tuning with demonstrations.
the underlined text is the task-speciﬁctemplate, and colored words are label words..to cheaply obtain effective prompts that match oroutperform our manually chosen ones..second, we adopt the idea of incorporatingdemonstrations as additional context.
gpt-3’snaive “in-context learning” paradigm picks up to32 randomly sampled examples, and concatenatesthem with the input.
this method is not guaran-teed to prioritize the most informative demonstra-tions, and mixing random examples from differentclasses together creates long contexts which canbe hard to learn from.
additionally, the number ofusable demonstrations is bounded by the model’smaximum input length.
we develop a more reﬁnedstrategy, where, for each input, we randomly sam-ple a single example at a time from each class tocreate multiple, minimal demonstration sets.
wealso devise a novel sampling strategy that pairs in-puts with similar examples, thereby providing themodel with more discriminative comparisons..we present a systematic evaluation for analyzingfew-shot performance on 8 single-sentence and 7sentence-pair nlp tasks.
we observe that givena small number of training examples, (1) prompt-based ﬁne-tuning largely outperforms standard ﬁne-tuning; (2) our automatic prompt search methodmatches or outperforms manual prompts; and (3)incorporating demonstrations is effective for ﬁne-tuning, and boosts few-shot performance.
together,these simple-yet-effective methods contribute to-wards a dramatic improvement across the tasks weevaluate on, and we obtain gains up to 30% abso-lute improvement (11% on average) compared tostandard ﬁne-tuning.
for instance, we ﬁnd that aroberta-large model achieves around 90% accu-racy on most binary sentence classiﬁcation tasks,.
while only relying on 32 training examples.
we re-fer to our approach as lm-bff, better few-shotfine-tuning of language models: a strong, task-agnostic method for few-shot learning..2 related work.
language model prompting.
the gpt se-ries (radford et al., 2018, 2019; brown et al.,2020) fueled the development of prompt-basedlearning, and we follow many of its core concepts.
we are also greatly inspired by the recent petwork (schick and sch¨utze, 2021a,b), although theymainly focus on a semi-supervised setting where alarge set of unlabeled examples are provided.
weonly use a few annotated examples as supervision,and also explore automatically generated promptsand ﬁne-tuning with demonstrations.
furthermore,we deviate from their evaluation by providing amore rigorous framework, as we will discuss in §3.
finally, there is a large body of work on prompt-ing for mining knowledge from pre-trained models(trinh and le, 2018; petroni et al., 2019; davisonet al., 2019; talmor et al., 2020, inter alia).
dif-ferent from these works, we focus on leveragingprompting for ﬁne-tuning on downstream tasks..automatic prompt search.
schick and sch¨utze(2021a) and schick et al.
(2020) explore waysof identifying label words automatically, however,none of these results lead to better performancecompared to hand-picked ones.
in contrast, ourmethod searches over both templates and labelwords, and is able to match or outperform ourmanual prompts.
several other attempts have beenmade in addition—yet these approaches either op-.
3817mlmhead···noutterly ✔···mlmheadgreat (label:positive)terrible (label:negative) ✔label:positivelabel:negative ✔clshead[cls] no reason to watch .
it was  [mask] .
[sep] a fun ride .
it was great .
[sep] the drama discloses nothing .
it was terrible .
[sep][cls]  no reason to watch .
[sep] [cls] it's a [mask] movie in every regard , and [mask] painful to watch .
[sep]mlmhead···greatterrible ✔···(a) mlm pre-training(b) fine-tuning(c) prompt-based fine-tuning with demonstrations (our approach)demonstration for label:positivedemonstration for label:negativetemplateinputvocab   label space    label mapping            vocab   erate in limited domains, such as ﬁnding patternsto express speciﬁc relations (jiang et al., 2020), orrequire a large number of examples for gradient-guided search (shin et al., 2020; zhong et al., 2021).
our approach aims to develop general-purposesearch methods that rely only on a few annotations..fine-tuning of language models.
a number ofrecent studies have focused on better methods forﬁne-tuning language models (howard and ruder,2018; dodge et al., 2020; lee et al., 2020; zhanget al., 2021).
these works mainly focus on opti-mization and regularization techniques to stabilizeﬁne-tuning.
here we use standard optimizationtechniques, and instead mainly focus our efforts onbetter prompt-based ﬁne-tuning in a more extremefew-shot setting.
we anticipate that results of thesestudies are largely complementary to ours..few-shot learning.
broadly speaking, our set-ting is also connected to other few-shot learningparadigms in nlp, including (1) semi-supervisedlearning (miyato et al., 2017; xie et al., 2020; chenet al., 2020), where a set of unlabeled examplesare given; (2) meta-learning (yu et al., 2018; hanet al., 2018; bansal et al., 2020a,b; bao et al., 2020),where a set of auxiliary tasks are given; and (3) in-termediate training (phang et al., 2018; yin et al.,2020), where a related, intermediate task is given.
we deviate from these settings by making minimalassumptions about available resources: we onlyassume a few annotated examples and a pre-trainedlanguage model.
our focus is on understandinghow far we can push without any other advantages..3 problem setup.
task formulation.
in this work, we assume accessto a pre-trained language model l that we wish toﬁne-tune on a task d with a label space y. forthe task, we only assume k training examples perclass3 for the task’s training set dtrain, such thatthe total number of examples is ktot = k × |y|,and dtrain = {(xii=1.
our goal is then todevelop task-agnostic learning strategies that gener-in , ytest) ∼ dtest.
alize well to an unseen test set (xtestfor model selection and hyper-parameter tuning,we assume a development set ddev, of the same sizeas the few-shot training set, i.e., |ddev| = |dtrain|.
this distinction is important: using a larger devel-opment set confers a signiﬁcant advantage (see our.
in, yi)}ktot.
3for regression, we partition the data into two “classes”.
according to being above or below the median value..experiments in appendix a), and subverts our ini-tial goal of learning from limited data.4 for all ofthe following experiments (unless speciﬁed other-wise), we take l = roberta-large and k = 16..evaluation datasets.
we conduct a systematicstudy across 8 single-sentence and 7 sentence-pairenglish tasks, including 8 tasks from the gluebenchmark (wang et al., 2019), snli (bowmanet al., 2015), and 6 other popular sentence clas-siﬁcation tasks (sst-5, mr, cr, mpqa, subj,trec).
all of the dataset details are provided inappendix b. for single-sentence tasks, the goal isto make a prediction based on an input sentencexin = x1, such as whether a movie review is posi-tive or not.
for sentence-pair tasks, the goal is totake a pair of input sentences xin = (x1, x2) andpredict the relationship between them.
we also in-terchangeably refer to the inputs as <s1> or (<s1>,<s2>).
note that we mainly use sst-2 and snlifor pilot experiments and model development, mak-ing it close to a true few-shot setting, at least for allthe other datasets we evaluate on..evaluation protocol.
systematically evaluatingfew-shot performance can be tricky.
it is well-known that ﬁne-tuning on small datasets can sufferfrom instability (dodge et al., 2020; zhang et al.,2021), and results may change dramatically given anew split of data.
to account for this, we measureaverage performance across 5 different randomlysampled dtrain and ddev splits.
this issue has alsobeen discussed in schick and sch¨utze (2021b)—they suggest using a ﬁxed set of training examples.
we argue that sampling multiple splits gives a morerobust measure of performance, and a better esti-mate of the variance.
we also observe that hyper-parameters can make a signiﬁcant difference, thuswe sweep multiple hyper-parameters for each datasample, and take the best setting as measured onthe ddev of that sample (see appendix c.1)..4 prompt-based fine-tuning.
given a masked language model l, we ﬁrst con-vert input xin to a token sequence ˜x, and the lan-guage model l then maps ˜x to a sequence of hid-den vectors {hk ∈ rd}.
during standard ﬁne-tuning, we usually take ˜xsingle = [cls]x1[sep]or ˜xpair = [cls]x1[sep]x2[sep].
for down-.
4in contrast, schick and sch¨utze (2021a,b) do not use adevelopment set, and adopt a set of hyper-parameters based onpractical considerations.
this is akin to “shooting in the dark”on a setting that we show can have unintuitive outcomes..3818task.
template.
label words.
<s1> it was [mask] .
<s1> it was [mask] .
<s1> it was [mask] .
<s1> it was [mask] .
<s1> this is [mask] ..sst-2sst-5mrcrsubjtrec [mask] : <s1>.
cola <s1> this is [mask] ..positive: great, negative: terriblev.positive: great, positive: good, neutral: okay, negative: bad, v.negative: terriblepositive: great, negative: terriblepositive: great, negative: terriblesubjective: subjective, objective: objectiveabbreviation: expression, entity: entity, description: descriptionhuman: human, location: location, numeric: numbergrammatical: correct, not grammatical: incorrect.
mnlisnliqnlirtemrpc <s1> [mask] , <s2><s1> [mask] , <s2>qqpsts-b <s1> [mask] , <s2>.
<s1> ?
[mask] , <s2> entailment: yes, netural: maybe, contradiction: no<s1> ?
[mask] , <s2> entailment: yes, netural: maybe, contradiction: no<s1> ?
[mask] , <s2> entailment: yes, not entailment: no<s1> ?
[mask] , <s2> entailment: yes, not entailment: noequivalent: yes, not equivalent: noequivalent: yes, not equivalent: noyu: yes, yl: no.
table 1: manual templates and label words that we used in our experiments.
sts-b is a regression task (§4.2)..stream classiﬁcation tasks with a label space y, wetrain a task-speciﬁc head, softmax(woh[cls]),by maximizing the log-probability of the correctlabel, where h[cls] is the hidden vector of [cls],and wo ∈ r|y|×d is a set of randomly initializedparameters introduced at the start of ﬁne-tuning.
similarly, for a regression task, we can introducewo ∈ rd and optimize the mean squared error be-tween wo ·h[cls] and the gold label.
in either case,the number of new parameters can be substantial—for example, a simple binary classiﬁcation task willintroduce 2,048 new parameters for a roberta-large model—making it challenging to learn from asmall amount of annotated data (e.g., 32 examples).
an alternative approach to solving this problemis prompt-based ﬁne-tuning, in which l is directlytasked with “auto-completing” natural languageprompts.
for instance, we can formulate a binarysentiment classiﬁcation task using a prompt withinput x1 (e.g., “no reason to watch it .”) as:.
xprompt = [cls] x1 it was [mask] .
[sep].
and let l decide whether it is more appropriateto ﬁll in “great” (positive) or “terrible” (negative)for [mask].
we now formalize this approach forclassiﬁcation and regression (§4.1 and §4.2), anddiscuss the importance of prompt selection (§4.3)..4.1 classiﬁcation.
let m : y → v be a mapping from the tasklabel space to individual words5 in the vocabulary.
5more generally, we can consider a one-to-many mappingm : y → 2|y| in which we map labels to sets of words.
however, we did not ﬁnd signiﬁcant gains in our experiments..v of l. then for each xin, let the manipulationxprompt = t (xin) be a masked language modeling(mlm) input which contains one [mask] token.
in this way, we can treat our task as an mlm, andmodel the probability of predicting class y ∈ y as:.
p(y | xin) = p ([mask] = m(y) | xprompt).
=.
(cid:80).
exp (cid:0)wm(y) · h[mask]y(cid:48)∈y exp (cid:0)wm(y(cid:48)) · h[mask].
(cid:1).
(cid:1) ,.
(1).
where h[mask] is the hidden vector of [mask] andwv denotes the pre-softmax vector correspondingto v ∈ v. when supervised examples {(xin, y)}are available, l can be ﬁne-tuned to minimize thecross-entropy loss.
it is important to note that thisapproach re-uses the pre-trained weights wv anddoes not introduce any new parameters.
it also re-duces the gap between pre-training and ﬁne-tuning,making it more effective in few-shot scenarios..4.2 regression.
we assume the same basic setup as in classiﬁ-cation, but treat the label space y as a boundedinterval [vl, vu].
inspired by mettes et al.
(2019),we model the problem as an interpolation betweentwo opposing poles, {yl, yu}, with values vl andvu respectively.
for instance, we can formulateour previous sentiment analysis task as a regres-sion problem in the range [0, 1], where we slidebetween “terrible” (vl = 0) and “great” (vu = 1).
in this way, we can express y as a mixture model:.
y = vl · p(yl | xin) + vu · p(yu | xin),.
(2).
where p(yu | xin) is the probability of yu, andp(yl | xin) = 1 − p(yu | xin).
then we deﬁne.
3819template.
label words.
accuracy.
sst-2 (positive/negative).
<s1> it was [mask] .
<s1> it was [mask] .
<s1> it was [mask] .
<s1> it was [mask] .
<s1> it was [mask] .
fine-tuning.
great/terriblegood/badcat/dogdog/catterrible/great-.
mean (std).
92.7 (0.9)92.5 (1.0)91.5 (1.4)86.2 (5.4)83.2 (6.9)81.4 (3.8).
snli (entailment/neutral/contradiction).
mean (std).
<s1> ?
[mask] , <s2> yes/maybe/no<s1> .
[mask] , <s2> yes/maybe/no<s1> ?
[mask] <s2>yes/maybe/no<s1> <s2> [mask]yes/maybe/no<s2> ?
[mask] , <s1> yes/maybe/no<s1> ?
[mask] , <s2> maybe/no/yesfine-tuning.
-.
77.2 (3.7)76.2 (3.3)74.9 (3.0)65.8 (2.4)62.9 (4.1)60.6 (4.8)48.4 (4.8).
table 2: the impact of templates and label words onprompt-based ﬁne-tuning (k = 16)..m : {yl, yu} → v, and model p(yu | xin) thesame as eq.
(1).
we ﬁne-tune l to minimize thekl-divergence between the inferred p(yu | xin)and the observed mixture weight, (y−vl)/(vu−vl)..4.3 manual prompts: the good and the bad.
the key challenge is to construct the template tand label words m(y)—we refer to these two to-gether as a prompt p. previous works (schick andsch¨utze, 2021a,b) hand-craft both the templatesand label words, which usually requires domainexpertise and trial-and-error.
table 1 summarizesmanual templates and label words chosen for eachdataset in our experiments.
these templates andlabel words were designed by intuition, and byconsidering formats used in previous literature..to better understand what constitutes a goodtemplate or label word, we conduct a pilot studyon sst-2 and snli.
table 2 shows that differentprompts can lead to substantial differences in ﬁnalaccuracy.
speciﬁcally, when a template is ﬁxed, thebetter the label words match the “semantic classes”,the better the ﬁnal accuracy is (great/terrible >good/bad > cat/dog).
in extreme cases where weswap plausible label words (e.g., terrible/great),we achieve the worst overall performance.6 fur-thermore, with the same set of label words, even asmall change in the template can make a difference.
for example, for snli, if we put [mask] at theend, or swap sentence order, we observe a >10%drop.
the above evidence clearly underlines the.
6it is unclear, however, why roberta thinks that “cat” is.
more positive than “dog”.
the authors tend to disagree..importance of selecting good templates and labelwords.
searching for prompts, however, is hard,as the search space can be very large—especiallyfor the template.
even worse, we only have a fewexamples to use to guide our search, which caneasily overﬁt.
we will address these issues next..5 automatic prompt generation.
we now explore principled ways of automatingthe search process for label words (§5.1) and tem-plates (§5.2).
our goals are to reduce the humaninvolvement required to design prompts, and to ﬁndmore optimal settings than those that we manuallychoose.
here, we assume a classiﬁcation task, butthe process for regression is analogous..5.1 automatic selection of label words.
we ﬁrst study how to construct a label wordmapping m that maximizes accuracy on ddev af-ter ﬁne-tuning, given a ﬁxed template t .
naivelysearching all possible assignments, however, is (1)generally intractable, as the search space is expo-nential in the number of classes; and (2) prone tooverﬁtting, as we will tend to uncover spuriouscorrelations given only a few annotations.
as asimple solution, for each class c ∈ y, we constructa pruned set v c ⊂ v of the top k vocabulary wordsbased on their conditional likelihood using the ini-tial l. that is, let dctrain ⊂ dtrain be the subset ofall examples of class c. we take v c as.
.
.
top-kv∈v.
xin∈dc.
train.
(cid:88).
(cid:16).
log pl.
[mask] = v | t (xin).
, (3).
.
(cid:17).
.
where pl denotes the output probability distribu-tion of l. to further narrow down the search space,we ﬁnd the top n assignments over the pruned spacethat maximize zero-shot accuracy on dtrain (bothn and k are hyper-parameters, see appendix c.2).
then we ﬁne-tune all top n assignments, and re-rank to ﬁnd the best one using ddev.
this approachis similar to the automatic verbalizer search meth-ods in schick and sch¨utze (2021a); schick et al.
(2020), except that we use a much simpler searchprocess (brute-force) and also apply re-ranking—which we ﬁnd to be quite helpful..5.2 automatic generation of templates.
next, we study how to generate a diverse set oftemplates {t } automatically from a ﬁxed set oflabel words m(y).
to address this challengingproblem, we propose to use t5 (raffel et al., 2020),.
3820the top k templates to use as an ensemble (table 4).
though it might appear to be expensive to ﬁne-tunethe model on each individual template, this is fastin practice due to the small size of dtrain, and is alsofully automated: making it easy to use, comparedto manually tuning prompts for each dataset..6 fine-tuning with demonstrations.
in this section, we study whether we can leveragedemonstrations when ﬁne-tuning medium-sizedlms, and ﬁnd better ways to exploit them..6.1 training examples as demonstrations.
gpt-3’s naive approach to in-context learningsimply involves concatenating the input with upto 32 examples randomly drawn from the trainingset.
this approach is suboptimal as (1) the num-ber of available demonstrations is bounded by themodel’s maximum input length;8 and (2) mixingnumerous random examples from different classestogether creates extremely long contexts which canbe hard to leverage, especially for a smaller model.
to address these issues, we propose a simpler so-lution: at each training step, we randomly samplein , y(c)(cid:1) ∈ dtrain from each class,one9 example (cid:0)x(c)convert it into t (cid:0)x(c)(cid:1) with [mask] replaced byinin , y(c)(cid:1)—andm(y(c))—we denote this as ˜t (cid:0)x(c)then concatenate them with xin (figure 1(c)):.
t (cid:0)xin.
(cid:1) ⊕ ˜t (cid:0)x(1).
in , y(1)(cid:1) ⊕ · · · ⊕ ˜t (cid:0)x(|y|).
in.
, y(|y|)(cid:1)..here ⊕ denotes concatenation of input sequences.
during both training and inference we sample mul-tiple demonstration sets for each xin.
note thatboth xin and demonstration examples are sampledfrom the same set dtrain during training.
at testingtime, we still sample demonstration sets from dtrainand ensemble predictions across all sets..we observe that controlling the construction ofin , y(c))} is cru-the demonstration examples {(x(c)cial for good ﬁnal performance.
for example, ifthe set of contrastive demonstrations x(c)in are alldramatically different—from each other, or fromthe query xin—then it becomes challenging forthe language model to decipher meaningful pat-terns.
as a result, the model may simply ignore.
figure 2: our approach for template generation..a large pre-trained text-to-text transformer.
t5 ispre-trained to ﬁll in missing spans (replaced by t5mask tokens, e.g., <x> or <y>) in its input.
forexample, given the input “thank you <x> me toyour party <y> week”, t5 is trained to generate“<x> for inviting <y> last <z>”, meaning that “forinviting” is the replacement for <x> and “last” isthe replacement for <y>.
this is well suited forprompt generation: we can simply take input sen-tences from dtrain and let the t5 model constructthe template t , without having to specify a pre-deﬁned number of tokens for it..given an input example (xin, y) ∈ dtrain, weconsider the following simple conversions, denotedas tg(xin, y), for formulating the t5 model inputs:7.
<s1> −→ <x> m(y) <y> <s1>,<s1> −→ <s1> <x> m(y) <y>,<s1>, <s2> −→ <s1> <x> m(y) <y> <s2>..as shown in figure 2, we rely on the t5 modelto ﬁll in the placeholders.
when decoding, our goalhere is to ﬁnd an output that can work well for allexamples in dtrain, i.e., the output template t thatmaximizes (cid:80)log pt5(t | tg(xin, y)),where pt5 denotes the output probability distribu-tion of t5.
it can be decomposed according to:.
(xin,y)∈dtrain.
where (t1, .
.
.
, t|t |) are the template tokens..we use beam search to decode multiple templatecandidates.
concretely, we use a wide beam width(e.g., 100) to cheaply obtain a large set of diversetemplates.
we then ﬁne-tune each generated tem-plate on dtrain and use ddev to either pick the singletemplate with the best performance (table 3), or.
|t |(cid:88).
(cid:88).
j=1.
(xin,y)∈dtrain.
log pt5.
(cid:0)tj | t1, ..., tj−1, tg.
(cid:0)xin, y(cid:1)(cid:1),.
(4).
6.2 sampling similar demonstrations.
7we consider putting the label word both before and afterthe input sentence for single-sentence tasks.
however, we ﬁndthat it is always better to put the label words in the middle(between the two sentences) for sentence-pair tasks..8gpt-3 uses a context size of 2,048 while most smallerlanguage models (e.g., roberta) have a context size of 512.
9we also explored sampling multiple examples per class,.
but did not observe any improvements..3821best templategenerated templatestraining examples for label:negativet5…training examples for label:positive…decode<s1> a [mask] one.<s1> this is [mask].…<s1> a [mask] one.a fun ride.
<x> great <y>a pleasure to watch.
<x> great <y>no reason to watch.
<x> terrible <y>this junk.
<x> terrible <y>fine-tune andevaluatepositive: great, negative: terriblelabel mapping            majority†prompt-based zero-shot‡“gpt-3” in-context learningfine-tuning.
prompt-based ft (man)+ demonstrationsprompt-based ft (auto)+ demonstrationsfine-tuning (full)†.
majority†prompt-based zero-shot‡“gpt-3” in-context learningfine-tuning.
prompt-based ft (man)+ demonstrationsprompt-based ft (auto)+ demonstrationsfine-tuning (full)†.
mr(acc).
cr(acc).
sst-2(acc).
50.983.684.8 (1.3)81.4 (3.8).
92.7 (0.9)92.6 (0.5)92.3 (1.0)93.0 (0.6).
sst-5(acc).
23.135.030.6 (0.9)43.9 (2.0).
47.4 (2.5)50.6 (1.4)49.2 (1.6)49.5 (1.7).
50.080.880.5 (1.7)76.9 (5.9).
87.0 (1.2)86.6 (2.2)85.5 (2.8)87.7 (1.4).
95.0.
58.7.mnli mnli-mm(acc).
(acc).
90.8.snli(acc).
32.750.852.0 (0.7)45.8 (6.4).
68.3 (2.3)70.7 (1.3)68.3 (2.5)70.0 (3.6).
33.051.753.4 (0.6)47.8 (6.8).
70.5 (1.9)72.0 (1.2)70.1 (2.6)72.0 (3.1).
33.849.547.1 (0.6)48.4 (4.8).
77.2 (3.7)79.7 (1.5)77.1 (2.1)77.5 (3.5).
50.079.587.4 (0.8)75.8 (3.2).
90.3 (1.0)90.2 (1.2)89.0 (1.4)91.0 (0.9).
89.4.qnli(acc).
49.550.853.8 (0.4)60.2 (6.5).
64.5 (4.2)69.2 (1.9)68.3 (7.4)68.5 (5.4).
mpqa(acc).
50.067.663.8 (2.1)72.0 (3.8).
84.7 (2.2)87.0 (1.1)85.8 (1.9)86.5 (2.6).
87.8.rte(acc).
52.751.360.4 (1.4)54.4 (3.9).
69.1 (3.6)68.7 (2.3)73.9 (2.2)71.1 (5.3).
subj(acc).
50.051.453.6 (1.0)90.8 (1.8).
91.2 (1.1)92.3 (0.8)91.2 (1.1)91.4 (1.8).
97.0.mrpc(f1).
81.261.945.7 (6.0)76.6 (2.5).
74.5 (5.3)77.8 (2.0)76.2 (2.3)78.1 (3.4).
trec(acc).
18.832.026.2 (2.4)88.8 (2.1).
84.8 (5.1)87.5 (3.2)88.2 (2.0)89.4 (1.7).
97.4.qqp(f1).
0.049.736.1 (5.2)60.7 (4.3).
65.5 (5.3)69.8 (1.8)67.0 (3.0)67.7 (5.8).
cola(matt.).
0.02.0-1.5 (2.4)33.9 (14.3).
9.3 (7.3)18.7 (8.8)14.0 (14.1)21.8 (15.9).
62.6.sts-b(pear.).
--3.214.3 (2.8)53.5 (8.5).
71.0 (7.0)73.5 (5.1)75.0 (3.3)76.4 (6.2).
89.8.
89.5.
92.6.
93.3.
80.9.
91.4.
81.7.
91.9.table 3: our main results using roberta-large.
†: full training set is used (see dataset sizes in table b.1); ‡:no training examples are used; otherwise we use k = 16 (per class) for few-shot experiments.
we report mean(and standard deviation) performance over 5 different splits (§3).
majority: majority class; ft: ﬁne-tuning; man:manual prompt (table 1); auto: automatically searched templates (§5.2); “gpt-3” in-context learning: using thein-context learning proposed in brown et al.
(2020) with roberta-large (no parameter updates)..the context, or even get confused by the additionalexamples.
to address this issue, we devise a simplestrategy in which we only sample examples thatare semantically close to xin.
speciﬁcally, we use apre-trained sbert (reimers and gurevych, 2019)model to obtain embeddings for all input sentences(for sentence-pair tasks, we use the concatenationof the two sentences).
here we just feed the rawsentences without the templates into sbert.
foreach query xin and each label c ∈ y, we sort alltraining instances with the label x ∈ dctrain by theirsimilarity score to the query cos(e(xin), e(x)), andonly sample from the top r = 50% instances foreach class to use as demonstrations..7 experiments.
we present our main results, and address severalresearch questions pertaining to our lm-bff ap-proach.
implementation details are in appendix c..7.1 main results.
we use a roberta-large model and set k =16 in our experiments.
a comparison of usingroberta vs bert can be found in appendix d.for automatic prompt search, in our main table.
we report automatic template search only (whichconsistently performs the best, see table 5).
to putour results in perspective, we compare to a numberof baselines, namely (1) standard ﬁne-tuning inour few-shot setting; (2) standard ﬁne-tuning usingthe full training set; (3) simply taking the mostfrequent class (measured on the full training set);(4) prompt-based zero-shot prediction where wetake our manual prompts and use l “out-of-the-box” without using any training examples; and (5)“gpt-3” in-context learning, where we use the sameprompt-based zero-shot setting, but augment thecontext with randomly sampled 32 demonstrations(and still use roberta-large, not gpt-3)..single-prompt results.
table 3 shows our mainresults using a single prompt, either from our man-ually designed ones (table 1) , or the best gener-ated ones.
first, prompt-based zero-shot predictionachieves much better performance than the ma-jority class, showing the pre-encoded knowledgein roberta.
also, “gpt-3” in-context learningdoes not always improve over zero-shot prediction,likely because smaller language models are notexpressive enough to use off-the-shelf like gpt-3..3822prompt-based fine-tuning mnli.
rte.
sst-2.
(positive/negative).
our single manual pppetpours, |pours| = |ppet|+ demonstrationspours, |pours| = 20+ demonstrations.
68.3 (2.3)71.9 (1.5)70.4 (3.1)74.0 (1.9)72.7 (2.5)75.4 (1.6).
69.1 (3.6)69.2 (4.0)73.0 (3.2)71.9 (4.6)73.1 (3.3)72.3 (4.5).
table 4: ensemble models using manual prompts frompet (schick and sch¨utze, 2021a,b) and our automatictemplates.
pet uses 4 prompts for mnli and 5 forrte.
we also use an equal number of templates in|pours| = |ppet| for a fair comparison..sst-2 snli trec mrpc.
manual.
auto tauto lauto t + l.92.7.
92.391.592.1.
77.2.
77.175.677.0.
84.8.
88.287.089.2.
74.5.
76.277.274.0.table 5: comparison between manual prompts anddifferent automatic prompt generation methods: auto-generated templates (auto t), auto-generated labelwords (auto l), and their combination (auto t + l)..second, prompt-based ﬁne-tuning can greatlyoutperform standard ﬁne-tuning, both when usinga manual prompt or a generated one.
cola is oneinteresting exception, as the input may be a non-grammatical sentence which is out of the distribu-tion of l. generally, our automatically searchedtemplates can achieve comparable or even higherresults than manual ones, especially for tasks inwhich constructing strong manual templates is lessintuitive (e.g., trec, qnli and mrpc)..finally, using demonstrations in context leads toconsistent gains in a majority of tasks.
in summary,our combined solution—ﬁne-tuning with automati-cally searched templates and sampled demonstra-tion sets—achieves a 30% gain on snli comparedto standard ﬁne-tuning, and 11% gain on average..ensemble results.
an advantage of automaticprompt search is that we can generate as manyprompts as we want, train individual models, andcreate large ensembles.
pet (schick and sch¨utze,2021a,b) also ensembles multiple models trainedwith manual prompts.10 in table 4, we make adirect comparison of our searched prompts andpet’s manual prompts on mnli and rte (two.
10they then use unlabeled data and distillation to get a.single model, which is outside of our scope..auto t m(y) = {great, terrible}.
#1.
<s1> a [mask] one .
#2.
<s1> a [mask] piece .
#3.
<s1> all in all [mask] ..auto l t (xin) = <s1> it was [mask].
#1. irresistible/pathetic#2. wonderful/bad#3. delicious/bad.
snli.
(entailment/neutral/contradiction).
auto t m(y) = {yes, maybe, no}.
#1.
<s1> .
[mask] , no , <s2>#2.
<s1> .
[mask] , in this case <s2>#3.
<s1> .
[mask] this time <s2>.
auto l t (xin) = <s1> ?
[mask] , <s2>#1. alright/watch/except#2. hi/watch/worse#3. regardless/fortunately/unless.
table 6: examples of our automatically generated tem-plates (auto t) and label words (auto l)..datasets that we evaluate in common).11 as theresults show, an ensemble with multiple templatesalways improves performance.
an ensemble of thesame number of automatic templates achieves com-parable or better performance than the ensemble ofpet’s manual prompts.
increasing the number ofautomatic templates brings further gains..7.2 analysis of generated prompts.
table 5 gives the results of using manual vs au-tomatic prompts.
for automatic prompts, we com-pare template search (auto t), label word search(auto l), and a joint variant (auto t + l) inwhich we start from manual label words, applyauto t, and then auto l. in most cases, auto tachieves comparable or higher performance thanmanual ones, and is consistently the best variant.
auto l outperforms manual prompts on trec andmrpc—but is considerably worse on snli.
autot + l is often better than auto l, but only some-times better than auto t. table 6 shows examplesfrom auto t and auto l (a full list in appendix e).
auto t templates generally ﬁt the context and la-bel words well, but can contain biased peculiarities(e.g., “{yes/no}, no” in snli).
for auto l words,things are mixed: while most look intuitively rea-sonable, there are also some mysterious abnormali-ties (e.g., “hi” for the “entailment” class in snli)..11in the pet nli templates, the hypothesis is put beforethe premise, which we actually found to be suboptimal.
in ourexperiments, we swap the two and get better results..3823sst-2 snli trec mrpc.
prompt-based ft.uniform sampling+ roberta sel.
+ sbert sel..92.7.
92.392.792.6.
77.2.
78.879.579.7.
84.8.
85.683.487.5.
74.5.
70.976.677.8.table 7: impact of demonstration sampling strategies.
uniform sampling randomly samples demonstrations,while selective (sel.)
sampling only takes top sentencesmeasured by the sentence encoders (§6)..7.3 analysis of demonstration sampling.
table 7 compares the performance of demonstra-tions using uniform sampling to selective samplingby sbert.
we acknowledge that sbert is trainedon snli and mnli datasets, thus we also trieda simple sentence encoder using mean pooling ofhidden representations from roberta-large.
weﬁnd that in either case, using selective samplingoutperforms uniform sampling, highlighting theimportance of sampling similar examples for incor-porating demonstrations in context..7.4 sample efﬁciency.
figure 3 illustrates how standard ﬁne-tuning andour lm-bff compare as k increases.
for a simpletask such as sst-2 (also see mr, cr and mpqa intable 3), despite using only 32 total examples, lm-bff has already nearly saturated its performanceand is comparable to standard ﬁne-tuning over theentire dataset.
on the harder task of snli, lm-bff continues to improve as k increases while stillmaintaining a performance gap over standard ﬁne-tuning, until the two converge around k = 256..8 discussion.
reformulating nlp tasks as mlm has excitingimplications for few-shot learning, but also has lim-itations.
first, while lm-bff greatly outperformsstandard ﬁne-tuning, table 3 shows that, overall,the performance still substantially lags behind ﬁne-tuning with thousands of examples, especially forharder tasks.
additionally, just like standard ﬁne-tuning, our results also suffer from high variance.
as described in §2, several recent studies have triedto counter instability in few-shot ﬁne-tuning andwe expect these methods to also help here..with respect to automatic prompt generation, de-spite its effectiveness, we still ﬁnd it practicallychallenging to expand the search space, or general-ize well based on only approximately 32 examples..figure 3: standard ﬁne-tuning vs our lm-bff as afunction of k (# instances per class).
for lower k, ourmethod consistently outperforms standard ﬁne-tuning..this is partly due to our lingering reliance on somemanual design—either manual templates (for labelword search) or manual label words (for templatesearch), which allows us to get our search off theground, but does also bias it towards areas of thesearch space that we might have already imagined.
finally, it is important to clarify that lm-bff fa-vors certain tasks which (1) can be naturally posedas a “ﬁll-in-the-blank” problem; (2) have relativelyshort input sequences; and (3) do not contain manyoutput classes.
issues (2) and (3) might be ame-liorated with longer-context language models (e.g.,beltagy et al., 2020).
for tasks that are not straight-forward to formulate in prompting, such as struc-tured prediction, issue (1) is more fundamental.
weleave it as an open question for future work..9 conclusion.
in this paper we presented lm-bff, a set ofsimple but effective techniques for ﬁne-tuning lan-guage models using only a few examples.
ourapproach proposes to (1) use prompt-based ﬁne-tuning with automatically searched prompts; and(2) include selected task demonstrations (trainingexamples) as part of the input context.
we showthat our method outperforms vanilla ﬁne-tuning byup to 30% (and 11% on average).
we concludedby discussing the limitations of our approach, andposed open questions for future study..acknowledgements.
we thank the members of princeton, mit, ts-inghua nlp groups and the anonymous reviewersfor their valuable feedback.
tg is supported by agraduate fellowship at princeton university andaf is supported by an nsf graduate research fel-lowship.
this research is also partly supported bya google research scholar award..3824163264128256k707580859095accuracy (%)sst-2fine-tunelm-bff163264128256k405060708090accuracy (%)snlifine-tunelm-bffreferences.
trapit bansal, rishikesh jha, and andrew mccal-lum.
2020a.
learning to few-shot learn across di-verse natural language classiﬁcation tasks.
in inter-national conference on computational linguistics(coling)..trapit bansal, rishikesh jha, tsendsuren munkhdalai,and andrew mccallum.
2020b.
self-supervisedmeta-learning for few-shot natural language classi-ﬁcation tasks.
in empirical methods in natural lan-guage processing (emnlp)..yujia bao, menghua wu, shiyu chang, and reginabarzilay.
2020. few-shot text classiﬁcation with dis-tributional signatures.
in international conferenceon learning representations (iclr)..roy bar haim, ido dagan, bill dolan, lisa ferro,danilo giampiccolo, bernardo magnini, and idanszpektor.
2006. the second pascal recognisingtextual entailment challenge..iz beltagy, matthew e. peters, and arman cohan.
longformer: the long-document trans-.
2020.former.
arxiv:2004.05150..luisa bentivogli, peter clark, ido dagan, and danilogiampiccolo.
2009. the ﬁfth pascal recognizingtextual entailment challenge.
in tac..samuel bowman, gabor angeli, christopher potts, andchristopher d manning.
2015. a large annotatedcorpus for learning natural language inference.
inempirical methods in natural language processing(emnlp)..tom b brown, benjamin mann, nick ryder, melaniesubbiah, jared kaplan, prafulla dhariwal, arvindneelakantan, pranav shyam, girish sastry, amandaaskell, et al.
2020. language models are few-shotin advances in neural information pro-learners.
cessing systems (neurips)..daniel cer, mona diab, eneko agirre, i˜nigo lopez-gazpio, and lucia specia.
2017. semeval-2017task 1: semantic textual similarity multilingual andcrosslingual focused evaluation.
in the 11th interna-tional workshop on semantic evaluation (semeval-2017)..jiaao chen, zichao yang, and diyi yang.
2020. mix-text: linguistically-informed interpolation of hid-den space for semi-supervised text classiﬁcation.
inassociation for computational linguistics (acl)..ido dagan, oren glickman, and bernardo magnini.
2005. the pascal recognising textual entailmentchallenge.
in the first international conference onmachine learning challenges: evaluating predic-tive uncertainty visual object classiﬁcation, andrecognizing textual entailment..jacob devlin, ming-wei chang, kenton lee, andkristina toutanova.
2019. bert: pre-training ofdeep bidirectional transformers for language under-standing.
in north american chapter of the associ-ation for computational linguistics (naacl)..jesse dodge, gabriel ilharco, roy schwartz, alifarhadi, hannaneh hajishirzi, and noah smith.
2020.fine-tuning pretrained language models:weight initializations, data orders, and early stop-ping.
arxiv preprint arxiv:2002.06305..william b. dolan and chris brockett.
2005. automati-cally constructing a corpus of sentential paraphrases.
in the third international workshop on paraphras-ing (iwp2005)..danilo giampiccolo, bernardo magnini, ido dagan,and bill dolan.
2007. the third pascal recog-in the acl-nizing textual entailment challenge.
pascal workshop on textual entailment and para-phrasing..xu han, hao zhu, pengfei yu, ziyun wang, yuanyao, zhiyuan liu, and maosong sun.
2018. fewrel:a large-scale supervised few-shot relation classiﬁ-cation dataset with state-of-the-art evaluation.
inempirical methods in natural language processing(emnlp)..jeremy howard and sebastian ruder.
2018. universallanguage model ﬁne-tuning for text classiﬁcation.
inassociation for computational linguistics (acl)..minqing hu and bing liu.
2004. mining and summa-rizing customer reviews.
in acm sigkdd interna-tional conference on knowledge discovery and datamining..zhengbao jiang, frank f xu, jun araki, and grahamneubig.
2020. how can we know what languagemodels know?
transactions of the association ofcomputational linguistics (tacl)..cheolhyoung lee, kyunghyun cho, and wanmo kang.
2020. mixout: effective regularization to ﬁnetunein inter-large-scale pretrained language models.
national conference on learning representations(iclr)..yinhan liu, myle ott, naman goyal, jingfei du, man-dar joshi, danqi chen, omer levy, mike lewis,luke zettlemoyer, and veselin stoyanov.
2019.roberta: a robustly optimized bert pretrainingapproach.
arxiv preprint arxiv:1907.11692..pascal mettes, elise van der pol, and cees snoek.
2019.hyperspherical prototype networks.
in advances inneural information processing systems (neurips)..joe davison, joshua feldman, and alexander m rush.
2019. commonsense knowledge mining from pre-in empirical methods in naturaltrained models.
language processing (emnlp)..takeru miyato, andrew m dai, and ian goodfel-low.
2017. adversarial training methods for semi-supervised text classiﬁcation.
in international con-ference on learning representations (iclr)..3825bo pang and lillian lee.
2004. a sentimental educa-tion: sentiment analysis using subjectivity summa-rization based on minimum cuts.
in association forcomputational linguistics (acl)..bo pang and lillian lee.
2005. seeing stars: exploit-ing class relationships for sentiment categorizationwith respect to rating scales.
in association for com-putational linguistics (acl)..fabio petroni, tim rockt¨aschel, sebastian riedel,patrick lewis, anton bakhtin, yuxiang wu, andalexander miller.
2019. language models as knowl-edge bases?
in empirical methods in natural lan-guage processing (emnlp)..jason phang, thibault f´evry, and samuel r bow-man.
2018. sentence encoders on stilts: supple-mentary training on intermediate labeled-data tasks.
arxiv preprint arxiv:1811.01088..alec radford, karthik narasimhan, tim salimans, andilya sutskever.
2018.improving language under-standing by generative pre-training.
technical re-port, openai..alec radford, jeff wu, rewon child, david luan,dario amodei, and ilya sutskever.
2019. languagemodels are unsupervised multitask learners.
techni-cal report, openai..colin raffel, noam shazeer, adam roberts, katherinelee, sharan narang, michael matena, yanqi zhou,wei li, and peter j liu.
2020. exploring the limitsof transfer learning with a uniﬁed text-to-text trans-former.
the journal of machine learning research(jmlr), 21(140)..pranav rajpurkar, jian zhang, konstantin lopyrev, andpercy liang.
2016. squad: 100,000+ questions formachine comprehension of text.
in empirical meth-ods in natural language processing (emnlp)..nils reimers and iryna gurevych.
2019. sentence-bert: sentence embeddings using siamese bert-in empirical methods in natural lan-networks.
guage processing and international joint confer-ence on natural language processing (emnlp-ijcnlp)..timo schick, helmut schmid, and hinrich sch¨utze.
2020. automatically identifying words that canserve as labels for few-shot text classiﬁcation.
ininternational conference on computational linguis-tics (coling)..timo schick and hinrich sch¨utze.
2021a.
exploit-ing cloze questions for few-shot text classiﬁcationand natural language inference.
in european chap-ter of the association for computational linguistics(eacl)..timo schick and hinrich sch¨utze.
2021b..it’s notjust size that matters: small language models arein north american chap-also few-shot learners.
ter of the association for computational linguistics(naacl)..taylor shin, yasaman razeghi, robert l. logan iv,eric wallace, and sameer singh.
2020. autoprompt:automatic prompt construction for masked languagemodels.
in empirical methods in natural languageprocessing (emnlp)..richard socher, alex perelygin, jean wu, jasonchuang, christopher d. manning, andrew ng, andchristopher potts.
2013. recursive deep modelsfor semantic compositionality over a sentiment tree-in empirical methods in natural languagebank.
processing (emnlp)..alon talmor, yanai elazar, yoav goldberg, andjonathan berant.
2020. olmpics-on what languagemodel pre-training captures.
transactions of the as-sociation of computational linguistics (tacl), 8..trieu h trinh and quoc v le.
2018. a simplemethod for commonsense reasoning.
arxiv preprintarxiv:1806.02847..ellen m voorhees and dawn m tice.
2000. buildingin the 23rda question answering test collection.
annual international acm sigir conference on re-search and development in information retrieval..alex wang, amanpreet singh, julian michael, felixhill, omer levy, and samuel r bowman.
2019.glue: a multi-task benchmark and analysis plat-in inter-form for natural language understanding.
national conference on learning representations(iclr)..alex warstadt, amanpreet singh, and samuel r. bow-man.
2019. neural network acceptability judgments.
transactions of the association of computationallinguistics (tacl), 7..janyce wiebe, theresa wilson, and claire cardie.
2005. annotating expressions of opinions and emo-tions in language.
language resources and evalua-tion, 39(2-3)..adina williams, nikita nangia, and samuel bowman.
2018. a broad-coverage challenge corpus for sen-in northtence understanding through inference.
american chapter of the association for computa-tional linguistics: human language technologies(naacl-hlt)..qizhe xie, zihang dai, eduard hovy, thang luong,and quoc le.
2020. unsupervised data augmenta-tion for consistency training.
advances in neuralinformation processing systems (neurips), 33..wenpeng yin, nazneen fatema rajani, dragomirradev, richard socher, and caiming xiong.
2020.universal natural language processing with limitedannotations: try few-shot textual entailment as ain empirical methods in natural languagestart.
processing (emnlp)..mo yu, xiaoxiao guo, jinfeng yi, shiyu chang, salonipotdar, yu cheng, gerald tesauro, haoyu wang,.
3826and bowen zhou.
2018. diverse few-shot text clas-siﬁcation with multiple metrics.
in north americanchapter of the association for computational lin-guistics (naacl)..tianyi zhang, felix wu, arzoo katiyar, kilian qweinberger, and yoav artzi.
2021. revisiting few-sample bert ﬁne-tuning.
in international confer-ence on learning representations (iclr)..zexuan zhong, dan friedman, and danqi chen.
2021.factual probing is [mask]: learning vs. learning torecall.
in north american association for computa-tional linguistics (naacl)..3827a impact of development sets.
table a.1 shows how the size of the developmentsets can affect the ﬁnal performance of the model.
for “no ddev”, we take the same hyper-parametersfrom schick and sch¨utze (2021a,b): batch size =16, learning rate = 1e-5 and training steps = 250.we also experiment with a variant that we sample adevelopment set of 10 times larger than the trainingset.
we can see that using larger development setsleads to better performance, and this is why westick to |dtrain| = |ddev| in our few-shot setting..fine-tuning.
sst-2 snli trec mrpc.
prompt-based ft.sst-2 snli trec mrpc.
no ddev|ddev| = |dtrain||ddev| = 10|dtrain|.
no ddev|ddev| = |dtrain||ddev| = 10|dtrain|.
79.581.483.5.
92.192.793.0.
49.248.452.0.
75.377.279.7.
83.988.889.4.
84.884.889.3.
77.876.679.6.
70.274.580.9.table a.1: impact of different sizes of developmentsets.
standard deviations are omitted here to save space.
for no |ddev|, we use the same set of hyper-parametersas schick and sch¨utze (2021a,b)..b datasets.
for snli (bowman et al., 2015) and datasetsfrom glue (wang et al., 2019), including sst-2 (socher et al., 2013), cola (warstadt et al.,2019), mnli (williams et al., 2018), qnli (ra-jpurkar et al., 2016), rte (dagan et al., 2005;bar haim et al., 2006; giampiccolo et al., 2007;bentivogli et al., 2009), mrpc (dolan and brock-ett, 2005), qqp12 and sts-b (cer et al., 2017), wefollow zhang et al.
(2021) and use their originaldevelopment sets for testing.
for datasets which re-quire a cross-validation evaluation—mr (pang andlee, 2005), cr (hu and liu, 2004), mpqa (wiebeet al., 2005), subj (pang and lee, 2004)—we sim-ply randomly sample 2,000 examples as the testingset and leave them out from training.
for sst-5 (socher et al., 2013) and trec (voorhees andtice, 2000), we use their ofﬁcial test sets.
we showdataset statistics in table b.1..c experimental details.
c.1 hyper-parameter selection.
numbers are picked by pilot experiments on thesst-2 and snli datasets.
we also use early stop-ping to avoid overﬁtting.
for each trial, we trainthe model for 1,000 steps, validate the performanceevery 100 steps, and take the best checkpoint..c.2 prompt-based ﬁne-tuning.
table 1 shows all the manual templates and la-bel words we use in experiment.
for automaticallytemplate generation, we take the t5-3b13 model,which is the largest publicly available one that canﬁt on a single gpu.
for automatically searching la-bel words, we set k to 100 for all tasks except sst-5and trec.
for sst-5 we set a smaller k = 30, asit is a 5-way classiﬁcation task.
for trec, we ob-serve that ﬁltering v c using conditional likelihoodalone is still noisy, thus we set k = 1000, and thenre-rank v c by the nearest neighbors of the originalmanual label words and take the top 30 per class.
we set n to 100 in all experiments.
due to thelarge number of trials in automatic search, we takea ﬁxed set of hyper-parameters in this part: batchsize of 8 and learning rate of 1e-5..since the idea of prompt-based ﬁne-tuning is tomake the input and output distribution close to thepre-training, the implementation details are crucial.
for templates, we put extra space before sentencesif it is not at the beginning of the input.
also,we lowercase the ﬁrst letter of the sentence if it isconcatenated with a preﬁx (e.g., <s2> in table 1).
also if one sentence is appended any punctuation(e.g., <s1> in table 1), then the last character of theoriginal sentence is discarded.
finally, we prependa space for label words in m(y).
for example,we use “ great” instead of “great” in the robertavocabulary, where “ ” stands for space..c.3 fine-tuning with demonstrations.
when using demonstrations, we sample 16 dif-ferent sets of demonstrations for each input andaverage the predicted log probability for each classduring inference.
we ﬁnd that further increasingthe number of samples does not bring substantialimprovement.
additional, we have tried differentaggregation methods like taking the result withthe maximum conﬁdence and we did not ﬁnd ameaningful improvement.
for selective demonstra-tions, we take roberta-large-nli-stsb.
for grid search, we take learning rates from {1e-5, 2e-5, 5e-5} and batch sizes from {2, 4, 8}.
these.
12https://www.quora.com/q/quoradata/.
13we take the t5 1.0 checkpoint, which is trained on bothunsupervised and downstream task data.
we compared it tot5 1.1 (without downstream task data) and did not ﬁnd asigniﬁcant difference in generated templates..3828category dataset.
|y|.
type.
labels (classiﬁcation tasks).
single-sentence mpqa.
mnlisnlisentence- qnlipair.
sst-2sst-5mrcr.
subjtreccola.
rtemrpcqqpsts-b.
l.19182019323108.
#train6,9208,5448,6621,7758,6068,0005,4528,551.
25222262.
22/11314/8311/30249/10222/21212/122r 11/11.
392,702549,367104,7432,4903,668363,8465,749.
#test8722,2102,0002,0002,0002,0005001,042.
9,8159,8425,46327740840,4311,500.sentimentsentimentsentimentsentimentopinion polaritysubjectivityquestion cls.
acceptability.
nlinlinlinliparaphraseparaphrasesent.
similarity.
positive, negativev.
pos., positive, neutral, negative, v. neg.
positive, negativepositive, negativepositive, negativesubjective, objectiveabbr., entity, description, human, loc., num.
grammatical, not grammatical.
entailment, neutral, contradictionentailment, neutral, contradictionentailment, not entailmententailment, not entailmentequivalent, not equivalentequivalent, not equivalent-.
table b.1: the datasets evaluated in this work.
|y|: # of classes for classiﬁcation tasks (with one exception: sts-bis a real-valued regression task over the interval [0, 5]).
l: average # of words in input sentence(s).
note that weonly sample dtrain and ddev of k × |y| examples from the original training set in our few-shot experiments (§3)..bert-large.
sst-2 snli trec mrpc.
fine-tuning.
prompt-based ft+ demo (1-seg)+ demo (2-seg)+ demo (n-seg).
fine-tuning.
prompt-based ft+ demonstrations.
79.5.
85.687.586.186.4.
81.4.
92.792.6.
51.4.
59.250.461.358.6.
48.4.
77.279.7.
80.3.
79.077.277.979.6.
88.8.
84.887.5.
74.4.
66.868.573.271.0.
76.6.
74.577.8.roberta-large.
sst-2 snli trec mrpc.
table d.1: a comparison of bert-large vs roberta-large.
we use manual prompts in these experiments..mean-tokens14 from reimers and gurevych(2019) as our sentence embedding model..(1-seg); (2) using the a segment for the originalinput and the b segment for the demonstrations(2-seg); (3) using different segment embeddingsfor each sentence (n-seg), e.g., for snli, we usedifferent segments for each premise and hypoth-esis in both the original input and the demonstra-tions, which leads to a total number of 8 segmentembeddings.
this introduces new segment em-beddings (randomly initialized and learned duringﬁne-tuning) as the pre-trained bert only has two.
table d.1 shows that prompt-based ﬁne-tuningwith demonstrations also works for bert, and 2-seg works the best when incorporating demonstra-tions.
still, we take roberta-large as our mainmodel, for roberta performs much better thanbert and roberta saves the trouble to tune theusage of segment embeddings..d comparisons of bert vs roberta.
e generated prompts.
table d.1 compares the results of bert-large(uncased) and roberta-large in our settings.
pre-trained bert provides two segment embeddings(a/b) for different parts of input.
the commonpractice, when ﬁne-tuning bert, is that using onlysegment a for single-sentence tasks, and using seg-ment a/b for the two sentences in sentence-pairtasks.
in our case of incorporating demonstrations,however, we have more than two sentences.
thuswe explore the following different strategies for seg-ments: (1) using the a segment for all sentences.
14https://github.com/ukplab/.
sentence-transformers.
we demonstrate the top 3 automatically gener-ated templates and label words for all tasks in ta-ble e.1.
in general, most automatic templates arereasonable and grammatically correct.
for the labelwords, the generated results look intuitive for mostsingle sentence tasks.
for other tasks, the automaticones can be counterintuitive in some cases.
it isstill unclear why the language model picks thesewords and sometimes they actually work well.
weleave this for future study..3829trec (abbreviation/entity/description/human/location/numeric).
application/advisor/discussion/culture/assignment/minuteproduction/ae/context/artist/assignment/minutepersonality/advisor/conclusion/hum/assignment/minute.
task.
auto template.
(positive/negative)<s1> a [mask] one .
<s1> a [mask] piece .
<s1> all in all [mask] ..auto label words.
irresistible/patheticwonderful/baddelicious/bad.
(very positive/positive/neutral/negative/very negative)<s1> the movie is [mask] .
<s1> the music is [mask] .
<s1> but it is [mask] ..wonderful/remarkable/hilarious/better/awfulwonderful/perfect/hilarious/better/awfulunforgettable/extraordinary/good/better/terrible.
sst-2.
sst-5.
mr.cr.
cola.
mnli.
snli.
qnli.
rte.
qqp.
sts-b.
(positive/negative)it was [mask] !
<s1><s1> it’s [mask] .
<s1> a [mask] piece of work ..(positive/negative)<s1> it’s [mask] !
<s1> the quality is [mask] .
<s1> that is [mask] ..mpqa (positive/negative)<s1> is [mask] .
<s1>, [mask] !
<s1>.
[mask] ..subj.
(subjective/objective)<s1> it’s all [mask] .
<s1> it’s [mask] .
<s1> is it [mask] ?.
q: [mask] : <s1><s1> why [mask]?
<s1> answer: [mask] ..(grammatical/not grammatical)<s1> you are [mask] .
it is [mask] .
<s1>i am [mask] .
<s1>.
(entailment/neutral/contradiction)<s1> .
[mask] , you are right , <s2><s1> .
[mask] you’re right <s2><s1> .
[mask] !
<s2>.
(entailment/neutral/contradiction)<s1> .
[mask] , no , <s2><s1> .
[mask] , in this case <s2><s1> .
[mask] this time <s2>.
(entailment/not entailment)<s1> ?
[mask] .
yes , <s2><s1> ?
[mask] .
it is known that <s2><s1> ?
[mask] , however , <s2>.
(entailment/not entailment)<s1> .
[mask] , i believe <s2><s1> .
[mask] , i think that <s2><s1> .
[mask] , i think <s2>.
mrpc (equivalent/not equivalent).
<s1> .
[mask] !
<s2><s1> .
[mask] .
this is the ﬁrst time <s2><s1> .
[mask] .
that’s right .
<s2>.
(equivalent/not equivalent)<s1> ?
[mask] , but <s2><s1> ?
[mask] , please , <s2><s1> ?
[mask] , i want to know <s2>.
(yu/yl)<s1> .
[mask] sir <s2><s1> .
[mask] , it is not .
<s2><s1> .
[mask] .
it is <s2>.
epic/terribleepic/awfulexquisite/horrible.
fantastic/horribleneat/pointlessmagniﬁcent/unacceptable.
important/closeneeded/badunexpected/shocking.
everywhere/tragiceverywhere/horrifyingsomething/surreal.
one/proofwrong/sadmisleading/disappointing.
fine/plus/otherwisethere/plus/otherwisemeaning/plus/otherwise.
alright/watch/excepthi/watch/worseregardless/fortunately/unless.
okay/nonethelessnotably/yetspeciﬁcally/notably.
clearly/yetaccordingly/meanwhileso/meanwhile.
rather/alasat/thusinstead/moreover.
me/sinceum/bestironically/beyond.
note/nextyesterday/meanwhileyeah/meanwhile.
table e.1: top 3 automatically generated templates and label words for all tasks based on one split of k = 16training examples.
note that automatic template results are based on manual label words and automatic label wordresults are based on manual templates provided in table 1..3830