neural-symbolic solver for math word problems with auxiliary tasks.
jinghui qin1, xiaodan liang1,2, yining hong3, jianheng tang1 and liang lin1,2∗1 sun yat-sen university2 dark matter ai inc.3 university of california, los angelesqinjingh@mail2.sysu.edu.cn,{xdliang328,sqrt3tjh}@gmail.com,yininghong@cs.ucla.edu,linliang@ieee.org.
abstract.
1.introduction.
previous math word problem solvers follow-ing the encoder-decoder paradigm fail to ex-plicitly incorporate essential math symbolicconstraints, leading to unexplainable and un-reasonable predictions.
herein, we proposeneural-symbolic solver (ns-solver) to explic-itly and seamlessly incorporate different lev-els of symbolic constraints by auxiliary tasks.
our ns-solver consists of a problem reader toencode problems, a programmer to generatesymbolic equations, and a symbolic executorto obtain answers.
along with target expres-sion supervision, our solver is also optimizedvia 4 new auxiliary objectives to enforce dif-ferent symbolic reasoning: a) self-supervisednumber prediction task predicting both num-ber quantity and number locations; b) com-monsense constant prediction task predictingwhat prior knowledge (e.g.
how many legsa chicken has) is required; c) program con-sistency checker computing the semantic lossbetween predicted equation and target equa-tion to ensure reasonable equation mapping;d) duality exploiting task exploiting the quasiduality between symbolic equation generationand problem’s part-of-speech generation to en-hance the understanding ability of a solver.
be-sides, to provide a more realistic and challeng-ing benchmark for developing a universal andscalable solver, we also construct a new large-scale mwp benchmark cm17k consisting of4 kinds of mwps (arithmetic, one-unknownlinear, one-unknown non-linear, equation set)with more than 17k samples.
extensive exper-iments on math23k and our cm17k demon-strate the superiority of our ns-solver com-pared to state-of-the-art methods1..∗corresponding author1the code and the new cm17k dataset are available at.
https://github.com/qinjinghui/ns-solver..deep neural networks have achieved remarkablesuccesses in natural language processing recently.
although neural models have demonstrated per-formance superior to humans on some tasks, e.g.
reading comprehension (rajpurkar et al., 2016; de-vlin et al., 2019; lan et al.
), it still lacks the abilityof discrete reasoning, resulting in low accuracy onmath reasoning.
thus, it is hard for pure neuralnetwork approaches to tackle the task of solvingmath word problems (mwps), which requires amodel to be capable of natural language under-standing and discrete reasoning.
mwp solvingaims to automatically answer a math word prob-lem by understanding the textual description of theproblem and reasoning out the underlying answer.
a typical mwp is a short story that describes a par-tial state of the world and poses a question aboutan unknown quantity or multiple unknown quan-tities.
to solve an mwp, the relevant quantitiesneed to be identiﬁed from the text.
furthermore,the correct operators along with their computationorder among these quantities need to be determined.
therefore, integrating neural networks with sym-bolic reasoning is crucial for solving mwps.
in-spired by the recent amazing progress on neuralsemantic parsing (liang et al., 2017a) and readingcomprehension (chen et al., 2019), we address thisproblem by neural-symbolic computing..recently, many researchers (wang et al., 2017;huang et al., 2018; wang et al., 2018b, 2019; xieand sun, 2019; chiang and chen, 2019), inspiredby an encoder-decoder framework (cho et al.,2014), apply neural networks to solve mwps bylearning the mapping function between problemsand their corresponding equations, and achieve re-markable successes.
the encoder uses a neural net-work to represent a problem as a real-valued vector,and the decoder uses another neural network to.
proceedingsofthe59thannualmeetingoftheassociationforcomputationallinguisticsandthe11thinternationaljointconferenceonnaturallanguageprocessing,pages5870–5881august1–6,2021.©2021associationforcomputationallinguistics5870generate an equation or expression token by token.
the main difference among previous methods is theway to decode expressions or equations.
however,they only follow the encoder-decoder paradigm butlacking the ability to explicitly incorporate essen-tial math symbolic constraints (e.g.
commonsenseconstants, formulation regularization), leading tounexplainable and unreasonable predictions.
be-sides, most of them only focus on arithmetic mwpswithout any unknown, preventing them from gener-alizing to various types of mwps, such as equationset problems..to address the above issues, we propose a novelneural-symbolic solver (ns-solver), which ex-plicitly and seamlessly incorporates different lev-els of symbolic constraints by auxiliary learningtasks.
our ns-solver consists of three main com-ponents, a problem reader to encode the math wordproblems into vector representations, a program-mer to generate the symbolic grounded equations,which are executed to produce answers, and a sym-bolic executor to obtain ﬁnal results.
in additionto the supervised training objective between gen-erated symbolic grounded equations and ground-truth equations, our solver is also optimized byfour novel auxiliary objectives that enforce fourlevels of problem understanding and symbolic rea-soning.
first, we apply number prediction taskto predict both the number quantity and number lo-cation in the problem in a self-supervised manner.
second, we deploy commonsense constant pre-diction task to predict what prior commonsenseknowledge (e.g.
how many legs a chicken has) is re-quired for our solver.
third, we propose programconsistency checker to compute the semantic lossbetween the predicted program and ground-truthequation to ensure reasonable equation mapping.
finally, we also propose a novel duality exploit-ing task that exploits the quasi duality betweensymbolic grounded equation generation and theproblem’s part-of-speech generation to enhance theunderstanding ability of our solver.
there are somekey advantages of our solution.
first of all, theabove four auxiliary tasks can produce additionaltraining signals, which improves the data efﬁciencyin training and makes our solver more robust.
sec-ond, using the predicted constant to constrain thetarget symbolic table can reduce the search spacegreatly, which means that our solver can generatecorrect symbolic grounded equations easier andbetter.
third, the auxiliary tasks have been proven.
to help reduce the domain gap between seen andunseen mwps (sun et al., 2019, 2020), thus im-proving the reasoning ability of our solver..besides, beyond the current large-scale high-quality mwp benchmark that only includes onetype of problems, we also construct a large-scalechallenging chinese mwps dataset cm17k, whichcontains 4 types of mwps (arithmetic mwps, one-unknown linear mwps, one-unknown non-linearmwps, equation set problems) with more than 17ksamples, to provide a more realistic and challeng-ing benchmark for developing a universal and scal-able math solver.
extensive experiments on publicmath23k and our proposed cm17k demonstratethe superiority of our ns-solver compared to state-of-the-art methods in predicting ﬁnal results whileensuring intermediate equation rationality..2 related work.
deep learning-based mwp solvers.
numer-ous methods have been proposed to tackle themwp solving task, ranging from rule-based meth-ods (bakman, 2007; yuhui et al., 2010), statisticalmachine learning methods (kushman et al., 2014;zhou et al., 2015; roy and roth, 2015, 2016; mi-tra and baral, 2016; huang et al., 2016; roy androth, 2018), semantic parsing methods (shi et al.,2015; koncelkedziorski et al., 2015; huang et al.,2017; liang et al., 2018a), to deep learning meth-ods (ling et al., 2017; wang et al., 2017, 2018b;huang et al., 2018; wang et al., 2018a; xie and sun,2019; wang et al., 2019; zhang et al., 2020a,b; qinet al., 2020; shen and jin, 2020; wu et al., 2020;chen et al., 2021; hong et al., 2021a,b).
however,most deep learning-based methods only follow theencoder-decoder framework without explicitly in-corporating essential math symbolic constraints,resulting in some unexplainable and unreasonablepredictions.
besides, most of them only focus onarithmetic mwps, preventing them from generaliz-ing to various types, such as equation set problems..neural-symbolic computing.
neural-symboliccomputing has greatly promoted the developmentof semantic parsing.
jia and liang (2016); dongand lapata (2016); zhong et al.
(2017) appliedneural sequence-to-sequence and sequence-to-treemodels to semantic parsing with full supervision.
liang et al.
(2017b, 2018b) have advanced the state-of-the-art in weakly supervised semantic parsingon knowledge graphs and tabular databases.
al-.
5871figure 1: an overview of our ns-solver.
when a problem preprocessed by number mapping and replacement isentered, our problem reader encodes the problem text into context representation.
then our programmer generatesa tree-structured symbolic grounded program explicitly.
finally, a symbolic grounded program will be executedto produce answers by the executor.
in our ns-solver, we apply four auxiliary tasks to enhance its problemunderstanding and symbol reasoning ability for generating better programs..though most of the successes of semantic parsingare limited to structured data sources, it is not ex-pensive for mwps since it is easy to crawl lots ofproblems with annotated equations and answers.
therefore, mwp solving can beneﬁt from super-vised neural-symbolic computing.
self-supervised learning.
self-supervised auxil-iary tasks have been widely used in the ﬁelds ofnatural language understanding (devlin et al., 2019;lan et al.).
devlin et al.
(2019) applied two self-supervised auxiliary tasks, masked lm and nextsentence prediction, to improve the understandingability of bert by pretraining.
albert (lanet al.)
introduces sentence-order prediction task toaddress the ineffectiveness of the next sentence pre-diction task in bert.
hendrycks et al.
(2019) showthat self-supervised learning can improve modelrobustness and uncertainty.
dual learning.
dual learning, ﬁrst proposedby he et al.
(2016), is a reinforcement training pro-cess that jointly trains a primal task and its dual task.
then xia et al.
(2017) considered it as a way of su-pervised learning and designed a probabilistic reg-ularization term to exploit the duality.
it has beenwidely applied in various ﬁelds, such as machinetranslation (he et al., 2016), sentiment classiﬁca-tion (xia et al., 2017), question answering (tanget al., 2017), visual question answering (li et al.,2018), machine reading comprehension (xiao et al.,2018), and code generation (wei et al., 2019).
tothe best of our knowledge, we are the ﬁrst to ex-.
ploit the duality in mwps.
different from previousworks, we design a quasi dual learning method be-tween symbolic grounded equation generation andproblem’s part-of-speech generation to enhance theunderstanding ability by easing the difﬁculty ofgenerating problems from symbolic equations..3 neural-symbolic solver.
in this section, we present the design of the pro-posed ns-solver.
its backbone mainly consists ofa problem reader that encodes the math word prob-lems into vector representations, a programmer togenerate the symbolic grounded programs in preﬁxorder, and a symbolic executor to obtain ﬁnal re-sults.
the overview of our ns-solver is visualizedin fig.
1. we ﬁrst introduce the backbone of ourns-solver in section 3.1, and then we introduceother auxiliary tasks in section 3.2..3.1 backbone.
problem reader.
given a problem text p ={xi}ni=1 processed by number template replace-ment which maps numeric values in a problemto number templates (e.g., 26 and 82 to n1 andn2 in fig.
1), the problem reader encodes each to-ken xi in the problem text into an embedding ei.
in this work, we deploy a two-layer bidirectionalgru to encode each token xi into an embedding←−hi are from forward andei =backward grus, respectively.
besides, our prob-.
←−hi where.
−→hi and.
−→hi +.
5872problem readerprogrammerexecutorconsistency checkergt equation tree𝑥𝑦𝑛1𝑛2math word problemauxiliary tasks… 26heads and 82feet.... number location predictiontoday there are chickensand rabbits…location: 15;18ℒ𝐶𝐶𝑃chicken_legs, 4rabbit_legs, 2commonsense constant predictionduality explorationtoday there are chickens and rabbits in the same cage, with a total of 26 heads and 82 feet.
how many chickens and rabbits are there?encoderdecoderpos resulttoday/nn there/ex are/vbp chickens/nns and/cc rabbits/nns …problem pos taggingdual constrainsℒ𝑁𝐿𝑃ℒ𝑁𝑄𝑃number quantity prediction… 26heads and 82feet ... quality of numbers: 2ℒ𝑃𝐶𝐶number mapping𝑛1𝑛22682chickens: 11, rabbits: 15equation tree++==∗∗2𝑥4𝑦;ቊ𝑥+𝑦=𝑛12𝑥+4𝑦=𝑛2ℒ𝑑𝑢𝑎𝑙{2,4}lem encoder also outputs a problem representation←−−→h0 as the initial hidden state of ourhn +g0 =←−−→h0 are the last hiddenhn andprogrammer, wherestate of forward and backward grus, respectively.
programmer.
the programmer takes the outputof the problem reader as input and the problemrepresentation as the initial hidden state, and thendecodes a problem as a sequence of tokens {yi}mi=1which are organized as a preﬁx equation tree.
inthis work, we deploy a tree-structured decoder (xieand sun, 2019) with attention mechanism (bah-danau et al., 2015) as the backbone of our pro-grammer and modify them with uet representa-tion (qin et al., 2020) to support more symbolsfor multiple types of mwps.
in our programmer,the symbolic table consists of four parts.
for eachproblem, the problem-speciﬁc symbolic table con-tains math operators (+, −, ∗, /,ˆ, =, ;), unknownvariable (x and y), a series of commonsense con-stants (1, 3.14, etc) predicted by the commonsenseconstant prediction task in 3.2, and the problem-speciﬁc number templates (n1, n2, n3, etc).
itshould be noticed that ; is a special operator withthe lowest priority to integrate multiple equationtrees as an ensemble equation tree, so that equationset problems can be handled as simple as arithmeticproblems.
executor.
we deploy sympy2, which is a pythonlibrary for symbolic mathematics, as our symbolicexecutor for obtaining ﬁnal results by solving gen-erated equations..3.2 the design of auxiliary tasks.
the mwp solving task remains challenging sinceprevious methods did not take full advantage of therich semantics contained in a problem and lackingthe ability to explicitly incorporate essential mathsymbolic constraints.
in this section, we introducefour auxiliary learning tasks to exploit additionaltraining signals obtained from different tasks andexploit the result of the commonsense constantprediction task to explicitly constrain the constantsymbolic table, which can reduce the search spacefor symbolic generation and ease the difﬁculty ofgenerating correct constant.
self-supervised number prediction (snp)tasks.
if a solver can fully understand the problemsemantics, it should be able to identify the quantityto count howof numbers in a problem (i.e.,many numeric values are in the problem) and.
2https://www.sympy.org/.
their corresponding locations in the problemtext accurately.
for example, if the solver canunderstand the problem in fig.
1, it should be ableto predict there are two numbers(26 and 82) inthe problem, and their positions are 15 and 18,respectively.
thus, number quantity predictionand number location prediction are two criticalself-supervised tasks to help the problem readerfully understand the problem semantics andmeasure the ability of problem understanding of asolver.
both two number prediction tasks take themean of the problem encoder’s outputs {ei}ni=1 astheir input and apply a single-layer feed-forwardneural network to compute the distribution ofnumber quantity and number locations.
thetraining objectives of two tasks for each problemare formulated as:.
ln qp = −.
qti log p (qi|p ) ,.
(1).
ln lp = −.
lti log p (li|p ) ..q(cid:88).
i=1l(cid:88).
i=1.
where ln qp and ln lp denote the loss for thenumber quantity prediction (nqp) task and num-ber location prediction (nlp) task, respectively.
q and l are the maximum possible quantities ofnumber and maximum possible number locationsfor a problem at the dataset level.
qti and lti rep-resent the ground-truth value on i-th index of theoutput probability distribution of nqp and nlp,respectively.
commonsense constant prediction (ccp)task.
commonsense constants are important forsolving some mwps while most previous methodsonly consider the constants 1 and 3.14, which arenot enough for a solver to solve problems that needother commonsense constants.
however, attachinga lot of constants to the problem-speciﬁc symbolictable will enlarge the search space, increasing thedifﬁculty of generating rational symbolic equations.
therefore, we propose a commonsense constantprediction task to predict what prior commonsenseknowledge (e.g.
a chicken has 2.0 legs and a rabbithas 4.0 legs for the problem in fig.
1) is requiredfor the solver to solve a problem according tothe problem context.
in this way, we can reducethe search space greatly,thus improving theperformance of our solver.
similar to the numberprediction tasks,the commonsense constantprediction task takes the mean of the problem.
5873encoder’s output {ei}ni=1 as their input and applya single-layer feed-forward neural network tocompute the distribution of number quantity andnumber locations the training objective for eachproblem is formulated as:.
lccp = −.
ctj log p (ci|p ) ..(2).
c(cid:88).
i=1.
where c is the total number of constants in thesymbolic table and cti represents the true valueon i-th index of the output probability distribution.
since it is impossible for the commonsense con-stant prediction task to achieve 100% accuracy, inaddition to the predicted constants, we add threeextra constants that are not predicted but with thehighest probability into the symbolic table, makinga better trade-off between the size of the searchspace and prediction accuracy.
program consistency checker (pcc).
althougha problem can be solved by multiple equivalent butdifferent equations, the predicted equations shouldbe consistent with label equations as much as pos-sible in the supervised learning setting.
therefore,we propose a program consistency checker to checkthe symbolic program consistency and regularizethe model by computing semantic loss betweenthe predicted symbolic program and ground-truthequation to ensure the reasonable symbolic equa-tion mapping.
let ˆyi and yi represent the predictedsymbol and ground-truth symbol, pi represents theprobability of ˆyi, the semantic loss is obtained bycomputing a distance between the predicted distri-bution and ground-truth distribution as:.
lp cc = −log.
(cid:88).
(cid:89).
(cid:89).
pi.
(1 − pi) ..(3).
i.
ˆyi=yi.
ˆyi(cid:54)=yi.
duality exploiting (de) task.
many previousworks (he et al., 2016; xia et al., 2017; xiao et al.,2018; wei et al., 2019) have shown promising re-sults by dual learning framework.
although in-tuitively, mwp solving and mwp generation arerelated to each other, i.e., the input of mwp solvingis the output of mwp generation, and vice versa,it is very hard for the mwp generation task togenerate good enough problems only by the equa-tions without any topic information.
therefore, wepropose a duality exploiting task to enhance theunderstanding ability of our solver by exploitingthe quasi duality between symbolic grounded equa-tion generation and the problem’s part-of-speech.
generation.
given a pair of a problem and its cor-responding equations (p ,t ), and p (cid:48) is the part-of-speech of p 3, the training objective of the dualityexploiting task is formulated as:.
ldual = (cid:2)log ˆp(p (cid:48)) + log p (t |p ) −log ˆp(t ) − log p (cid:0)p (cid:48)|t (cid:1)(cid:3)2 ..(4).
where ˆp(p (cid:48)) and ˆp(t ) are marginal distributions,which can be modeled by their lstm (hochreiterand schmidhuber, 1997)-based language models,respectively.
besides, we deploy a tree-structureencoder inspired by gts (xie and sun, 2019) toencode equations in preﬁx for pos generation..3.3 training objectivegiven the training dataset d={(p i, t 1), (p 2, t 2),· · · ,(p n , t n ) }, where t i is the universal expres-sion tree of problem p i, we minimize the followingloss function for our ns-solver:.
[lent1 + λ1 ∗ ldual + λ2 ∗ lp cc.
+λ3 ∗ (ln qp + ln lp ) + λ4 ∗ lccp ] ..(5).
(cid:88).
l =.
(p,t )∈d.
where.
lent1 = − log.
prob(yt|p ).
(6).
where m denotes the size of t, and yt denotes thet-th output.
{λi}4i=1 are empirical values that willbe detailed in section 4.2..for the duality exploiting task, there is anotherloss for training the branch of the problem’s part-of-speech generation:.
lp os =.
[lent2+λ5∗ldual+λ6∗lp cc(cid:48)]..(cid:88).
(p (cid:48),t )∈d.
(7).
where.
lent2 = − log.
prob(xt|t ).
(8).
m(cid:89).
t=1.
n(cid:89).
t=1.
where n denotes the size of p, and xt denotes thet-th output.
lp cc(cid:48) is the semantic loss betweenpredicted pos and the ground-truth pos.
{λi}6i=5are empirical values that will also be detailed insection 4.2..3we use jieba (https://github.com/fxsjy/jieba) to generate.
the pos of a problem..58744 experiments.
4.1 cm17k dataset.
most public mwps datasets are quite small suchas alg514 or exist some incorrect labels such asdolphin18k.
an exception is the math23k dataset,which contains 23161 problems labeled well withstructured equations and answers.
however, it onlycontains one-unknown linear math word problems,which is not sufﬁcient to validate the ability of amath solver about solving multiple types of mwps.
therefore, we introduce a new high-quality mathword problems dataset, called cm17k, to validatethe universality of a solver and provide a more re-alistic and challenging benchmark for developinga universal and scalable math solver.
we collectcm17k from two education websites4.
these prob-lems are oriented grades 6-12, containing 4 typesof mwps with more than 17k samples, including6215 arithmetic mwps, 5193 one-unknown linearmwps, 3129 one-unknown non-linear mwps, and2498 equation set problems.
it should be noticedthat our dataset is sufﬁcient for validating the uni-versality of math word problem solvers since theseproblems can cover most cases about mwps.
welabel our data with structured equations and an-swers following math23k (wang et al., 2017).
wesplit our cm17k into train/valid/test sets at a ratioof 8:1:1..# avg pl# avg el# avg ts# avg num# avg sni# avg ops# avg constants.
math23k cm17k54.36513.85311.8346.3834.1114.8520.327.
28.0156.8535.5542.8212.6683.9430.270.table 1: statistics of math23k and cm17k.
pl, el,ts, num, sni, ops, and constants represent problemlength, equation length, equation tree size, number ofquantities in problems, number of quantities occurredin both problems and their corresponding equations,number of operators in equations, and number of con-stants only occurred in equations, respectively..the data statistics of math23k and cm17k areshown in table 1. from the statistics, we cansee that all statistics of cm17k are larger thanmath23k.
this shows that our dataset is more chal-lenging and difﬁcult for math word problem solvers.
besides, since cm17k contains more types ofmwps than math23k, cm17k is more suitable.
for validating the reasoning ability of a solver thanmath23k..4.2 experimental setup and training details.
4.2.1 datasets, baselines, and metric.
we conduct experiments on math23k and ourcm17k.
the main state-of-the-arts to be comparedare as follows: dns (wang et al., 2017) is a univer-sal solver based on the seq2seq model with signif-icant number identiﬁcation (sni).
gts (xie andsun, 2019) is a goal-driven tree-structured mwpsolver.
stackdecoder (chiang and chen, 2019)is an universal semantically-aligned math wordproblems solver.
(zhang et al., 2020a) is an en-hanced gts with teacher-student distillation andmulti-decoder ensemble.
again, following priorworks (wang et al., 2017; chiang and chen, 2019;xie and sun, 2019), we use answer accuracy asthe evaluation metric: if the calculated value of thepredicted equation tree equals to the true answer, itis thought as correct since the predicted expressionis equivalent to the target expression..implementation details.
4.2.2we use pytorch5 to implement our model on linuxwith an nvidia rtx2080ti gpu card.
all thosewords with fewer than 5 occurrences are convertedinto a special token unk.
the size of word embed-dings and all hidden states for other layers are set as128 and 512, respectively.
our model is optimizedby adam optimizor (kingma and ba, 2015) withβ1 = 0.9, β2 =0.999, and (cid:15) = 1e−8.
the mini-batchsize is set as 32. the initial learning rate is set as1e−3 and then decreases to half every 40 epochs.
to prevent overﬁtting, we set dropout rate as 0.5and weight decay as 1e−5.
finally, we conductgreedy search to generate symbolic equation trees.
we set λ1, λ2, λ3, λ5, and λ6 as 0.0005, 0.01, 1.0,0.005, and 0.1 for both datasets, respectively.
weset λ4 as 0.000001 for math23k while we set λ4 as1.0 for cm17k.
all constants are extracted fromthe training set.
in each epoch, all training data isshufﬂed randomly and then cut into mini-batches..4.3 answer accuracy.
following prior works (wang et al., 2017; chiangand chen, 2019; xie and sun, 2019), we conduct 5-fold cross-validation on math23k.
for cm17k, weevaluate the performance on the test set.
the resultsare shown in table 2. from table 2, we can observe.
4http://www.zxxk.com/ and http://www.jyeoo.com/.
5http://pytorch.org.
5875that beneﬁting from the four new auxiliary tasksand neural-symbolic paradigm, our ns-solver out-performs the baselines on both datasets in terms ofanswer accuracy.
speciﬁcally, for math23k andcm17k, the accuracy gains of ns-solver overgts are 1.37% and 5.93%, respectively.
com-paring with tsn-md, our solver outperforms it byabout 0.6% on math23k.
it shows that our model ismore feasible for solving multiple types of mwps.
it also shows that our ns-solver is more effectivethan other state-of-the-art models on the real-worldscenario that needs to solve various mwps with auniﬁed solver..modeldns (wang et al., 2017)stackdecoder (chiang and chen, 2019)gts (xie and sun, 2019)tsn-md (zhang et al., 2020a)ns-solver (ours).
math23k cm17k15.93%37.24%47.12%-.
58.1%66.0%74.3%75.1%75.67% 54.05%.
table 2: model comparison on answer accuracy.
4.4 comparisons on different subsets.
we drill down to analyze the generalization of dns,gts, and ns-solver on different types of mwps inthe test subset of cm17k.
their answer accuracyon different types of mwps is shown in table 3.we can observe that our ns-solver outperforms theother two models by a large margin on all subsets.
speciﬁcally, the accuracy gains of our ns-solverover gts on four subsets are 3.87%, 9.12%, 6.99%,and 9.44%.
this shows that with the help of fourauxiliary tasks, our ns-solver obtains better gener-alization ability on multiple types of mwps thanbaselines..number.
dns.
gts.
ns-solver (ours).
correctaccuracycorrectaccuracycorrectaccuracy.
arithmetic.
619233.7%25541.20%27945.07%.
one-unknownlinear526499.32%22041.83%26850.95%.
one-unknownnon-linear3156721.27%20163.80%22370.79%.
equation set.
24413254.1%12852.45%15161.89%.
table 3: answer accuracy on cm17k’s test subset..4.5 performance on tree length.
intuitively, the size of the symbolic equation tree isproportional to the complexity of the mathematicalrelationship in the problem.
the more complexthe mathematical relationship is, the more difﬁcultit is to solve the problem.
here, we compare ourproposed ns-solver with gts on cm17k to showthe superiority of our ns-solver on different equa-tion tree sizes.
the answer accuracies for differentsizes of expression trees on cm17k test subset areshown in fig.
2. we can see that there is a tendency.
figure 2: answer accuracies for different sizes of sym-bolic equation trees on cm17k..for answer accuracy to degrade with the growth ofthe problem complexity measured as the size of theequation tree, and our ns-solver outperforms gtson most cases of different equation tree sizes.
thisshows our ns-solver can better model the mathe-matical relationships of the problem than gts.
itcan also be noticed that the improvement of ourns-solver over the gts is increasing when theproblems become more complex..however, although our model outperforms othermethods, there still has room for improvement insemantic understanding and symbolic reasoningsince longer equations often match with more com-plex mwps which entail more complex math rela-tionships..4.5.1 ablation on different auxiliary tasks.
we study the contribution of different auxiliarytasks of our ns-solver.
for this purpose, we con-sider ﬁve different combinations: 1) only the back-bone [ns-solver - ccp - snp - pcc - de]; 2) back-bone + duality exploiting task [ns-solver - ccp -snp - pcc]; 3) backbone + duality exploiting task+ program consistent checker [ns-solver - ccp -snp]; 4) backbone + duality exploiting task + pro-gram consistent checker + number prediction tasks[ns-solver - ccp]; and 5) the proposed ns-solver[ns-solver].
for each of these combinations, eachmodel was trained for 80 epochs on cm17k andvalidated on its test subset.
the learning rate de-creased to half every 20 epochs.
the results areprovided in fig.
4..as one can see, all four auxiliary tasks can im-prove performance.
speciﬁcally, the accuracy gainsof de, pcc, snp, and ccp are 1.00%, 1.41%,1.11%, and 1.12%, respectively.
besides, the binaryaccuracies of the two snp tasks are 97% (numberquantity prediction) and 96.8% (number locationprediction).
moreover, the accuracy of our ccp.
5876figure 3: typical cases.
note that the results are represented as inﬁx order which is more readable than preﬁxorder.
the programs generated by ns-solver are also translated into human-readable equations.
constants andnumber symbols are labelled in red and cyan, respectively..tency checker (pcc) that effectively regularizesthe model’s output by constraining the distancebetween predicted symbols and ground-truth sym-bols during training, [ns-solver - ccp - snp]can generate more consistent equations with theground-truth than [ns-solver - ccp - snp - pcc],as shown in case 2. with self-supervised num-ber prediction (snp), [ns-solver - ccp] can gen-erate better results and avoid generating symbolsthat do not belong to the problem, as shown incase 3. with commonsense constant prediction(ccp), our ns-solver manages to choose correctconstants by constraining the constant symbolictable using predicted results of ccp.
as shown incase 4, [ns-solver - ccp] chooses error constant10 while ns-solver chooses two correct constants.
besides, although gts and ns-solver generate thesame symbols sometimes, our ns-solver generatescorrect equations with the help of our four auxil-iary objectives, as shown in case 5. overall, allfour auxiliary tasks can improve our ns-solver’sunderstanding and reasoning ability..modelcm17k.
bert + tree decoder (xie and sun, 2019) ns-solver + bert.
55.0%.
60.68%.
table 4: generalization to different backbone.
figure 4: ablation study on different auxiliary compo-nents.
‘-’ represents we remove the component..task is 97.8%.
this shows that our auxiliary taskscan enhance our ns-solver to enforce better prob-lem understanding and symbol reasoning.
overall,our proposed ns-solver achieves the best answeraccuracy..4.6 case study.
we also present the results of our ns-solver withdifferent combinations of four auxiliary tasks infig.
3. beneﬁting from explicitly exploiting theprobabilistic correlation between two quasi dualtasks to regularize the training process in our du-ality exploiting (de) task, our [ns-solver - ccp- snp - pcc] can generate correct equations byunderstanding the problem better while [ns-solver- ccp - snp - pcc - de] generates error equations,as shown in case 1. with the program consis-.
5877case 1:学校买来num(n0[5]) 盒羽毛球，每盒num (n1[12]) 个，共用num(n2[240]) 元，平均每个羽毛球多少元钱？(the school bought num(n0[5])  boxes of badminton, each box of num (n1[12]), sharing num(n2[240]) yuan, how much is the  average price of each badminton ?
)case 2: 小杰与同学们去南岳山玩，每人车票费是num(n0[22]) 元，他们总共花了num(n1[154]) 元车费，他们买了几张票？(xiaojiewent to nanyueshanwith his classmates.
the ticket per person was num(n0[22]) yuan.
they spent a total of num(n1[154]) yuan.
how many tickets were they bought?
)groundtruth:  x=n1/n0case 3: 妈妈想给num (n0[1]) 间长num (n1[7]) 米，宽num(n2[4]) 米的房间铺上地砖，每平方米的地砖价钱是num(n3[60]) 元，那么铺好地砖至少要花多少钱？(mother wants to lay a floor tile in num(n0[1]) room with a length of num(n1[7]) meters and a width of num(n2[4]) meters.
the price per square meter of floor tiles is num(n3[60]) yuan.
so how much does it cost to lay the floor tiles?)
case 5:  甲、乙num(n0[2]) 地相距num(n1[200])  千米，快车速度为num(n2[120]) 千米每小时，慢车速度为num(n3[80]) 千米每小时，慢车从甲地出发，快车从乙地出发。如果num(n4[2]) 车同时出发，相向而行，出发后几时num(n5[2]) 车相遇?
(the distance between num(n0[2]) locations a and b is num(n1[200])  kilometers, the speed of express train is num(n2[120]) kilometers per hour, and the speed of slow train is num(n3[80]) kilometers per hour.
if the num(n4[2]) cars depart at the same time \\and travel towards each other, when will the num(n5[2]) cars meet after departure?
)case 4:  小胖家装修新房了，准备在客厅铺上地砖，客厅是长方形的地面，长num (n0[5]) 米，宽num(n1[6]) 米，他选中了边长为num (n2[40]) 厘米的正方形地砖，他至少要购买多少块这样的地砖？(the chubby family has renovated a new house and is ready to lay floor tiles in the living room.
the living room is a rectangular floor with a length of  num(n0[5]) meters and a width of num (n1[6]) meters.
he chose a square floor tile with a side length of num(n2[40]) cm.
how many pieces of floor tiles should he buy at least?
)sns-solver -ccp -np -pcc -de (ours): x=n2/n1(error)sns-solver -ccp -np -pcc (ours): x=n2/(n0*n1) (correct)sns-solver -ccp -np -pcc (ours):x=n1/(n0*1.0)  (correct)sns-solver -ccp -np (ours):x=n1/n0(correct)sns-solver -ccp -np (ours): x=n3*n1*n2/10000(error)sns-solver -ccp (ours):x=n3*n1*n2(correct)sns-solver -ccp (ours): x=n0*n1/((n2/100)*(n2/10)) (error)sns-solver (ours): x=n0*n1/((n2/100)*(n2/100)) (correct)gts: n2*x=n1+n3*x (error)sns-solver (ours):n2*x+n3*x=n1(correct)+duality exploiting (de)+number prediction (np)+all above four tasks+commonsense constant prediction (ccp)+program consistency checker (pcc)problemgenerated symbolic equationauxiliary task 4.7 extends to other backbone.
to show that our auxiliary tasks can be adapted toother backbones, we replace gts’s encoder withbert (bert + tree decoder) and ns-solver’sencoder with bert (ns-solver + bert), wherewe adopt a chinese bert-base pre-trained withwhole word masking (cui et al., 2020).
we conductexperiments on cm17k.
the results are shownin table 4. we can observe that with auxiliarytasks, our ns-solver + bert still can outperformbert + tree decoder, which shows that our aux-iliary tasks’ strong generalization..5 conclusion.
in this work, we propose neural-symbolic solver(ns-solver) to explicitly and seamlessly incorpo-rate different levels of symbolic constraints by fourauxiliary tasks.
our ns-solver consists of a prob-lem reader to encode problems, a programmer togenerate a symbolic grounded program, and a sym-bolic executor to obtain ﬁnal results.
in additionto supervised learning with target expression, oursolver is also optimized via four new auxiliary ob-jectives that enforce four levels of symbolic rea-soning.
besides, we also construct a new datasetcm17k containing 4 types of mwps with morethan 17k samples, which provides a more realisticand challenging benchmark for developing a uni-versal and scalable math solver.
extensive experi-ments on math23k and cm17k demonstrate thesuperiority of our ns-solver compared to state-of-the-art methods in answer accuracy while ensuringintermediate equation rationality..6 ethical impact.
we collected cm17k from two online educationwebsites, which is only used for academic research,and the copyright belongs to the original websites.
this work may inspire research in the ﬁeld of nu-merical reasoning..acknowledgements this work was supportedin part by national key r&d program of chinaunder grant no.2020aaa0109700, nationalnatural science foundation of china (nsfc)under grant no.u19a2073, no.61976233 andno.
the natural science foun-dation of guangdong province under grant2017a030312006, guangdong provinceno.
basic and applied basic research (regionaljoint fund-key) grant no.2019b1515120039,.
61836012,.fundamental research.
shenzhenprogramand(project no.rcyx20200714114642083no.jcyj20190807154211365), zhijiang lab’sopen fund (no.2020aa3ab14), csig young fel-low support fund, and guangdong provincial keylaboratory of information security technology..references.
dzmitry bahdanau, kyunghyun cho, and yoshua ben-gio.
2015. neural machine translation by jointlyin 3rd inter-learning to align and translate.
national conference on learning representations,iclr 2015, san diego, ca, usa, may 7-9, 2015,conference track proceedings..yeﬁm bakman.
2007. robust understanding of wordproblems with extraneous information.
computingresearch repository, arxiv:math/0701393..jiaqi chen, jianheng tang, jinghui qin, xiaodanliang, lingbo liu, eric p. xing, and liang lin.
2021. geoqa: a geometric question answeringbenchmark towards multimodal numerical reason-ing.
arxiv preprint arxiv:2105.14517..xinyun chen, chen liang, adams wei yu, dennyzhou, dawn song, and quoc v le.
2019. neuralsymbolic reader: scalable integration of distributedand symbolic representations for reading compre-in international conference on learninghension.
representations..ting-rui chiang.
equation.
and yun-nung chen..2019.forsemantically-alignedgenerationinsolving and reasoning math word problems.
proceedings of the 2019 conference of the northamerican chapter of the association for computa-tional linguistics: human language technologies,volume 1 (long and short papers), pages 2656–2668. association for computational linguistics..kyunghyun cho, bart van merri¨enboer, caglar gul-cehre, dzmitry bahdanau, fethi bougares, holgerschwenk, and yoshua bengio.
2014.learningphrase representations using rnn encoder–decoderfor statistical machine translation.
in proceedings ofthe 2014 conference on empirical methods in nat-ural language processing (emnlp), pages 1724–1734, doha, qatar.
association for computationallinguistics..yiming cui, wanxiang che, ting liu, bing qin, shi-jin wang, and guoping hu.
2020. revisiting pre-trained models for chinese natural language process-ing.
in proceedings of the 2020 conference on em-pirical methods in natural language processing:findings, pages 657–668, online.
association forcomputational linguistics..jacob devlin, ming-wei chang, kenton lee, andkristina toutanova.
2019. bert: pre-training of.
5878deep bidirectional transformers for language under-in proceedings of the 2019 conferencestanding.
of the north american chapter of the associationfor computational linguistics: human languagetechnologies, volume 1 (long and short papers),pages 4171–4186, minneapolis, minnesota.
associ-ation for computational linguistics..li dong and mirella lapata.
2016. language to logi-cal form with neural attention.
in proceedings of the54th annual meeting of the association for compu-tational linguistics (volume 1: long papers), pages33–43, berlin, germany.
association for computa-tional linguistics..di he, yingce xia, tao qin, liwei wang, nenghai yu,tie-yan liu, and wei-ying ma.
2016. dual learn-ing for machine translation.
in advances in neuralinformation processing systems, pages 820–828..dan hendrycks, mantas mazeika, saurav kadavath,and dawn song.
2019. using self-supervised learn-ing can improve model robustness and uncertainty.
in advances in neural information processing sys-tems, pages 15637–15648..sepp hochreiter and j¨urgen schmidhuber.
1997.neural computation,.
long short-term memory.
9(8):1735–1780..yining hong, qing li, daniel ciao, siyuan huang,and song-chun.
zhu.
2021a.
learning by ﬁxing:solving math word problems with weak supervision.
in thirty-fifth aaai conference on artiﬁcial intelli-gence..yining hong, qing li, ran gong, daniel ciao, siyuanhuang, and song-chun.
zhu.
2021b.
smart: a situ-ation model for algebra story problems via attributedgrammar.
in the thirty-fifth aaai conference onartiﬁcial intelligence, aaai-21..danqing huang, jing liu, chin-yew lin, and jian yin.
2018. neural math word problem solver with rein-forcement learning.
in proceedings of the 27th inter-national conference on computational linguistics,pages 213–223.
association for computational lin-guistics..danqing huang, shuming shi, chin-yew lin, and jianyin.
2017. learning ﬁne-grained expressions toin proceedings of thesolve math word problems.
2017 conference on empirical methods in naturallanguage processing, pages 805–814.
associationfor computational linguistics..robin jia and percy liang.
2016. data recombinationfor neural semantic parsing.
in proceedings of the54th annual meeting of the association for compu-tational linguistics (volume 1: long papers), pages12–22, berlin, germany.
association for computa-tional linguistics..diederik p kingma and jimmy ba.
2015. adam: amethod for stochastic optimization.
in internationalconference on learning representations..rik koncelkedziorski, hannaneh hajishirzi, ashishsabharwal, oren etzioni, and siena dumas ang.
2015. parsing algebraic word problems into equa-tions.
transactions of the association for computa-tional linguistics, 3:585–597..nate kushman, yoav artzi, luke zettlemoyer, andregina barzilay.
2014. learning to automaticallysolve algebra word problems.
in proceedings of the52th annual meeting of the association for compu-tational linguistics, volume 1, pages 271–281..zhenzhong lan, mingda chen, sebastian goodman,kevin gimpel, piyush sharma, and radu sori-cut.
albert: a lite bert for self-supervisedin 8th inter-learning of language representations.
national conference on learning representations,iclr 2020, addis ababa, ethiopia, april 26-30,2020..yikang li, nan duan, bolei zhou, xiao chu, wanliouyang, xiaogang wang, and ming zhou.
2018. vi-sual question generation as dual task of visual ques-tion answering.
in proceedings of the ieee confer-ence on computer vision and pattern recognition,pages 6116–6124..chao-chun liang, yu-shiang wong, yi-chung lin,and keh-yih su.
2018a.
a meaning-based statisticalenglish math word problem solver.
in proceedingsof the 2018 conference of the north american chap-ter of the association for computational linguistics:human language technologies, volume 1 (long pa-pers), pages 652–662, new orleans, louisiana.
as-sociation for computational linguistics..chen liang, jonathan berant, quoc le, kenneth d.forbus, and ni lao.
2017a.
neural symbolic ma-chines: learning semantic parsers on freebase within proceedings of the 55th an-weak supervision.
nual meeting of the association for computationallinguistics (volume 1: long papers), pages 23–33,vancouver, canada.
association for computationallinguistics..danqing huang, shuming shi, chin-yew lin, jian yin,and wei-ying ma.
2016. how well do comput-ers solve math word problems?
large-scale datasetconstruction and evaluation.
in proceedings of the54th annual meeting of the association for compu-tational linguistics (volume 1: long papers), pages887–896.
association for computational linguis-tics..chen liang, jonathan berant, quoc le, kenneth d.forbus, and ni lao.
2017b.
neural symbolic ma-chines: learning semantic parsers on freebase within proceedings of the 55th an-weak supervision.
nual meeting of the association for computationallinguistics (volume 1: long papers), pages 23–33,vancouver, canada.
association for computationallinguistics..5879chen liang, mohammad norouzi, jonathan berant,quoc v le, and ni lao.
2018b.
memory augmentedpolicy optimization for program synthesis and se-mantic parsing.
in advances in neural informationprocessing systems, pages 9994–10006..wang ling, dani yogatama, chris dyer, and phil blun-som.
2017. program induction by rationale genera-tion: learning to solve and explain algebraic wordproblems.
in proceedings of the 55th annual meet-ing of the association for computational linguistics(volume 1: long papers), pages 158–167.
associa-tion for computational linguistics..arindam mitra and chitta baral.
2016. learning touse formulas to solve simple arithmetic problems.
in proceedings of the 54th annual meeting of theassociation for computational linguistics (volume1: long papers), pages 2144–2153.
association forcomputational linguistics..jinghui qin, lihui lin, xiaodan liang, rumin zhang,and liang lin.
2020. semantically-aligned univer-sal tree-structured solver for math word problems.
in proceedings of the 2020 conference on empiricalmethods in natural language processing (emnlp),pages 3780–3789..pranav rajpurkar, jian zhang, konstantin lopyrev, andpercy liang.
2016. squad: 100,000+ questions formachine comprehension of text.
in proceedings ofthe 2016 conference on empirical methods in natu-ral language processing, pages 2383–2392, austin,texas.
association for computational linguistics..subhro roy and dan roth.
2015. solving general arith-in proceedings of the 2015metic word problems.
conference on empirical methods in natural lan-guage processing, pages 1743–1752.
associationfor computational linguistics..subhro roy and dan roth.
2016. unit dependencygraph and its application to arithmetic word problemsolving.
in thirtieth aaai conference on artiﬁcialintelligence, pages 3082–3088..subhro roy and dan roth.
2018. mapping to declara-tive knowledge for word problem solving.
transac-tions of the association for computational linguis-tics, 6:159–172..yibin shen and cheqing jin.
2020. solving math wordproblems with multi-encoders and multi-decoders.
in proceedings of the 28th international conferenceon computational linguistics, pages 2924–2934,barcelona, spain (online).
international committeeon computational linguistics..shuming shi, yuehui wang, chin-yew lin, xiaojiangliu, and yong rui.
2015. automatically solvingnumber word problems by semantic parsing and rea-soning.
in proceedings of the 2015 conference onempirical methods in natural language processing,pages 1132–1142.
association for computationallinguistics..yu sun, eric tzeng, trevor darrell, and alexei a efros.
2019. unsupervised domain adaptation through self-supervision.
arxiv preprint arxiv:1909.11825..yu sun, xiaolong wang, zhuang liu, john miller,alexei efros, and moritz hardt.
2020. test-timetraining with self-supervision for generalization un-der distribution shifts.
in international conferenceon machine learning, pages 9229–9248.
pmlr..duyu tang, nan duan, tao qin, zhao yan, andming zhou.
2017. question answering and ques-arxiv preprinttion generation as dualarxiv:1706.02027..tasks..lei wang, yan wang, deng cai, dongxiang zhang,and xiaojiang liu.
2018a.
translating a math wordproblem to a expression tree.
in proceedings of the2018 conference on empirical methods in naturallanguage processing, pages 1064–1069.
associa-tion for computational linguistics..lei wang, dongxiang zhang, lianli gao, jingkuansong, long guo, and heng tao shen.
2018b.
math-dqn: solving arithmetic word problems via deep re-inforcement learning.
in thirty-second aaai con-ference on artiﬁcial intelligence, pages 5545–5552..lei wang, dongxiang zhang, zhang jipeng, xing xu,lianli gao, bing tian dai, and heng tao shen.
2019. template-based math word problem solversin thirty-thirdwith recursive neural networks.
aaai conference on artiﬁcial intelligence, pages7144–7151..yan wang, xiaojiang liu, and shuming shi.
2017.deep neural solver for math word problems.
in pro-ceedings of the 2017 conference on empirical meth-ods in natural language processing, pages 845–854. association for computational linguistics..bolin wei, ge li, xin xia, zhiyi fu, and zhi jin.
2019.code generation as a dual task of code summariza-tion.
in advances in neural information processingsystems, pages 6559–6569..qinzhuo wu, qi zhang, jinlan fu, and xuanjinghuang.
2020. a knowledge-aware sequence-to-treenetwork for math word problem solving.
in proceed-ings of the 2020 conference on empirical methodsin natural language processing (emnlp), pages7137–7146, online.
association for computationallinguistics..yingce xia, tao qin, wei chen, jiang bian, nenghaiyu, and tie-yan liu.
2017. dual supervised learn-ing.
in proceedings of the 34th international confer-ence on machine learning-volume 70, pages 3789–3798. jmlr.
org..han xiao, feng wang, jianfeng yan, and jingyaozheng.
2018. dual ask-answer network for ma-arxiv preprintchine reading comprehension.
arxiv:1809.01997..5880zhipeng xie and shichao sun.
2019. a goal-driventree-structured neural model for math word prob-in proceedings of the twenty-eighth in-lems.
ternational joint conference on artiﬁcial intelli-gence, ijcai-19, pages 5299–5305.
internationaljoint conferences on artiﬁcial intelligence organi-zation..ma yuhui, zhou ying, cui guangzuo, ren yun, andhuang ronghuai.
2010. frame-based calculus ofsolving arithmetic multi-step addition and subtrac-tion word problems.
in international workshop oneducation technology and computer science, vol-ume 2, pages 476–479..jipeng zhang, roy ka-wei lee, ee-peng lim, weiqin, lei wang, jie shao, and qianru sun.
2020a.
teacher-student networks with multiple decoders forsolving math word problem.
in proceedings of thetwenty-ninth international joint conference on ar-tiﬁcial intelligence, ijcai-20, pages 4011–4017.
in-ternational joint conferences on artiﬁcial intelli-gence organization.
main track..jipeng zhang, lei wang, roy ka-wei lee, yi bin, yanwang, jie shao, and ee-peng lim.
2020b.
graph-to-tree learning for solving math word problems.
inproceedings of the 58th annual meeting of the asso-ciation for computational linguistics, pages 3928–3937..victor zhong, caiming xiong, and richard socher.
2017.seq2sql: generating structured queriesfrom natural language using reinforcement learning.
arxiv preprint arxiv:1709.00103..lipu zhou, shuaixiang dai, and liwei chen.
2015.learn to solve algebra word problems usingquadratic programming.
in proceedings of the 2015conference on empirical methods in natural lan-guage processing, pages 817–822.
association forcomputational linguistics..5881