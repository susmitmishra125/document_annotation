cascade versus direct speech translation:do the differences still make a difference?.
luisa bentivogli1, mauro cettolo1, marco gaido1,2,alina karakanta1,2, alberto martinelli2∗, matteo negri1, marco turchi11fondazione bruno kessler2university of trento{bentivo,cettolo,mgaido,akarakanta,negri,turchi}@fbk.eu.
abstract.
five years after the ﬁrst published proofs ofconcept, direct approaches to speech trans-lation (st) are now competing with tradi-tional cascade solutions.
in light of thissteady progress, can we claim that the perfor-mance gap between the two is closed?
start-ing from this question, we present a system-atic comparison between state-of-the-art sys-tems representative of the two paradigms.
fo-cusing on three language directions (english–german/italian/spanish), we conduct auto-matic and manual evaluations, exploiting high-quality professional post-edits and annotations.
our multi-faceted analysis on one of the fewpublicly available st benchmarks attests forthe ﬁrst time that: i) the gap between the twoparadigms is now closed, and ii) the subtle dif-ferences observed in their behavior are not suf-ﬁcient for humans neither to distinguish themnor to prefer one over the other..1.introduction.
speech translation (st) is the task of automaticallytranslating a speech signal in a given language intoa text in another language.
research on st datesback to the late eighties and its evolution followedthe development of the closely related ﬁelds ofspeech recognition (asr) and machine translation(mt) that, since the very beginning, provided themain pillars for building the so-called cascade ar-chitectures.
with the advent of deep learning, theneural networks widely used in asr and mt havebeen adapted to develop a new direct st paradigm.
this approach aims to overcome known limitationsof the cascade one (e.g.
architectural complexity,error propagation) with a single encoder-decoderarchitecture that directly translates the source sig-nal bypassing intermediate representations..until now, the consolidated underlying technolo-gies and the richness of available data have upheldthe supremacy of cascade solutions in industrialapplications.
however, architectural simplicity, re-duced information loss and error propagation arethe ace up the sleeve of the direct approach, whichhas rapidly gained popularity within the researchcommunity in spite of the critical bottleneck repre-sented by data paucity..within a few years after the ﬁrst proofs of con-cept (b´erard et al., 2016; weiss et al., 2017), theperformance gap between the two paradigms hasgradually decreased.
this trend is mirrored by theﬁndings of the international workshop on spokenlanguage translation (iwslt),1 a yearly evalu-ation campaign where direct systems made theirﬁrst appearance in 2018. on english-german, forinstance, the bleu difference between the bestcascade and direct models dropped from 7.4 pointsin 2018 (niehues et al., 2018) to 1.6 points in2019 (niehues et al., 2019b).
in 2020, participantswere allowed to choose between processing a pre-segmented version of the test set or the one pro-duced by their own segmentation algorithm.
as re-ported in (ansari et al., 2020), the distance betweenthe two paradigms further decreased to 1.0 bleupoint in the ﬁrst condition and, for the ﬁrst time,it was slightly in favor of the best direct model inthe second condition, with a small but nonethelessmeaningful 0.24 difference..so, quoting ansari et al.
(2020), is the cascadesolution still the dominant technology in st?
hasthe direct approach closed the huge initial perfor-mance gap?
are there systematic differences in theoutputs of the two technologies?
are they distin-guishable?
answering these questions is more thanrunning an evaluation exercise.
it implies pushingresearch towards a deeper investigation of direct.
∗∗ the work of alberto martinelli was carried out during.
an internship at fondazione bruno kessler..1http://iwslt.org.
proceedingsofthe59thannualmeetingoftheassociationforcomputationallinguisticsandthe11thinternationaljointconferenceonnaturallanguageprocessing,pages2873–2887august1–6,2021.©2021associationforcomputationallinguistics2873st, ﬁnding a path towards its wider adoption in in-dustrial settings and motivating higher engagementin data exploitation and resource creation to trainthe data-hungry end-to-end neural systems..for all these reasons, while ansari et al.
(2020)were cautious in drawing ﬁrm conclusions, inthis paper we delve deeper into the problem withthe ﬁrst thorough comparison between the twoparadigms.
working on three language directions(en–de/es/it), we train state-of-the-art cascade anddirect models (§3), running them on test data drawnfrom the must-c corpus (cattoni et al., 2020)..systems’ behavior is analysed from different per-spectives, by exploiting high-quality post-edits andannotations by professionals.
after discussing over-all systems’ performance (§4), we move to moreﬁne-grained automatic and manual analyses cover-ing two main aspects: the relation between systems’performance and speciﬁc characteristics of the in-put audio (§5), and the possible differences in termsof lexical, morphological and word ordering errors(§6).
we ﬁnally explore whether, due to latentcharacteristics overlooked by all previous investi-gations, the output of cascade and direct systemscan be distinguished either by a human or by anautomatic classiﬁer (§7).
together with a compara-tive study attesting the parity of the two paradigmson our test data, another contribution of this paperis the release of the manual post-edits that renderedour investigation possible.
the data is available at:https://ict.fbk.eu/mustc-post-edits..2 background.
cascade st. by concatenating asr and mt com-ponents (stentiford and steer, 1988; waibel et al.,1991), cascade st architectures represent an intu-itive solution to achieve reasonable performanceand high adaptability across languages and do-mains.
at the same time, however, they suffer fromwell-known problems related to the concatenationof multiple systems.
first, they require ad-hoctraining and maintenance procedures for the asrand mt modules; second, they suffer from errorpropagation and from the loss of speech informa-tion (e.g.
prosody) that might be useful to improveﬁnal translations.
research has focused on mit-igating error propagation by: i) feeding the mtsystem with asr data structures (e.g.
asr n-best,lattices or confusion networks) which are more in-formative than the 1-best output (lavie et al., 1996;matusov et al., 2005; bertoldi and federico, 2005;.
beck et al., 2019; sperber et al., 2019), and ii)making the mt robust to asr errors, for instanceby training it on parallel data incorporating realor emulated asr errors as in (peitz et al., 2012;ruiz et al., 2015; sperber et al., 2017; cheng et al.,2019; di gangi et al., 2019a).
although the formersolutions are effective to some extent, state-of-the-art cascade architectures (pham et al., 2019; baharet al., 2020) prefer the latter, as they are simpler toimplement and maintain..direct st. to overcome the limitations of cascademodels, b´erard et al.
(2016) and weiss et al.
(2017)proposed the ﬁrst direct solutions bypassing in-termediate representations by means of encoder-decoder architectures based on recurrent neuralnetworks.
currently, more effective solutions(potapczyk and przybysz, 2020; bahar et al., 2020;gaido et al., 2020) rely on st-oriented adaptationsof transformer (vaswani et al., 2017) integratingthe encoder with: i) convolutional layers to reduceinput length, and ii) penalties biasing attention tolocal context in the encoder self-attention layers(povey et al., 2018; sperber et al., 2018; di gangiet al., 2019b).
though effective, these architectureshave to confront with training data paucity, a crit-ical bottleneck for neural solutions.
the problemhas been mainly tackled with data augmentationand knowledge transfer techniques.
data augmen-tation consists in producing artiﬁcial training cor-pora by altering existing datasets or by generating(audio, translation) pairs through speech synthesisor mt (bahar et al., 2019b; nguyen et al., 2020;ko et al., 2015; jia et al., 2019).
knowledge trans-fer (gutstein et al., 2008) consists in passing (hereto st) the knowledge learnt by a neural networktrained on closely related tasks (here, asr andmt).
existing asr models have been used for en-coder pre-training (b´erard et al., 2018; bansal et al.,2019; bahar et al., 2019a) and multi-task learning(weiss et al., 2017; anastasopoulos and chiang,2018; indurthi et al., 2020).
existing neural mtmodels have been used for decoder pre-training(bahar et al., 2019a; inaguma et al., 2020), jointlearning (indurthi et al., 2020; liu et al., 2020) andknowledge distillation (liu et al., 2019)..previous comparisons.
most of the works on di-rect st also evaluate the proposed solutions againsta cascade counterpart.
the conclusions, however,are discordant.
looking at recent works, pino et al.
(2019) show similar scores, indurthi et al.
(2020)report higher results for their direct model, while.
2874inaguma et al.
(2020) end up with the oppositeﬁnding.
the main problems of these comparisonsi) not all the architectures are equallyare that:optimized, ii) for the sake of fairness in terms oftraining data, cascade systems are restricted to un-realistic settings with small training corpora thatpenalize their performance, and iii) evaluation al-ways relies only on automatic metrics computed onsingle references.
the iwslt campaigns (niehueset al., 2019a; ansari et al., 2020) set up a sharedevaluation framework where systems built on alarge set of training data are optimized to achievethe best performance, independently from the un-derlying architecture.
in the last round, direct mod-els approached, and in one case (potapczyk andprzybysz, 2020) outperformed, the cascade ones.
however, the evaluation was run only on one lan-guage pair, by solely relying on automatic metricsand single references.
in this paper, we overcomethese limitations by comparing the two paradigmson three language pairs, using different metrics,multiple references (including professional post-edits) as well as ﬁne-grained automatic and manualanalysis procedures..3 experimental setting.
3.1 st systems.
to maximize the cross-language comparability ofour analyses, we built the cascade and direct stsystems for en–de/es/it with the same core technol-ogy, based on transformer.
their good quality isattested by the comparison with the winning sys-tem at the iwslt-20 ofﬂine st task (bahar et al.,2020),2 which consists of an ensemble of two cas-cade models scoring 28.8 bleu on the en-de por-tion of the must-c common test set.
on the samedata, our cascade and direct models achieve similarbleu scores, respectively 28.9 and 29.1 (see ta-ble 1).3 on en-es and en-it, identical architecturesperform similarly or better (up to 32.9 bleu onen-es).
although bleu scores are not strictly com-parable across languages, we can safely considerall our models as state-of-the-art..for the sake of reproducibility, we provide com-plete details about data, architectures and trainingsetup in appendix a..2in the pre-segmented data condition (ansari et al., 2020).
3also the asr performance of our cascade solution (10.2wer on must-c common) is in line with the results obtainedby bahar et al.
(2020) for their best asr model..3.2 evaluation methodology.
data.
our evaluation data is drawn from theted-based must-c corpus (cattoni et al., 2020),the largest freely available multilingual corpus forst. it covers 14 language directions, with englishaudio segments automatically aligned with theircorresponding manual transcriptions and transla-tions.
the en–de/es/it must-c common test setscontain the same 27 ted talks, for a total ofaround 2,500 segments largely overlapping acrosslanguages.4 for all the three language pairs, weselected subsets of must-c common containingthe same english audio portions from each talk,in order to obtain representative groups of con-tiguous segments that are comparable across lan-guages.
furthermore, to ensure high data quality,we manually checked the selected samples and keptonly those segments for which the audio-transcript-translation alignment was correct.
each of thethree resulting test sets – henceforth pe-sets – iscomposed of 550 segments, corresponding to about10,000 english source words..post-editing.
a key element of our multi-facetedanalysis is human post-editing (pe), which consistsin manually correcting systems’ output accordingto the input (the source audio in our case).
in pe-based evaluation, the original output is comparedagainst its post-edited version using distance-basedmetrics like ter (snover et al., 2006).
this allowsfor counting only the true errors made by a system,without penalising differences due to linguistic vari-ation as it happens when exploiting independentreferences.
this makes pe-based evaluation one ofthe most prominent methodologies used for transla-tion quality assessment (snover et al., 2006, 2009;denkowski and lavie, 2010; cettolo et al., 2013;bojar et al., 2015; graham et al., 2016; bentivogliet al., 2018b)..to collect the post-edits for our study, we strictlyfollowed the methodology of the iwslt 2013-2017 evaluation campaigns (cettolo et al., 2013),which offered us a consolidated framework andbest practices to draw upon.
our cascade and di-rect systems were both run on the pe-sets to bepost-edited.
to guarantee high quality post-edits,for each language we hired two professional trans-lators with experience in subtitling and post-editing.
moreover, in order to cope with translators’ vari-.
4must-c common segments can vary across languagesdue to the automatic procedures of segmentation, audio-textalignment and ﬁltering that were applied to the talks..2875ability (i.e.
more/less aggressive editing strategies),the outputs of the two st systems were randomlyassigned ensuring that each translator worked onall the 550 segments, post-editing an equal num-ber of outputs from both systems.
the task wasperformed with a cat tool5 that displays the man-ual transcript of the audio together with the stoutput to be edited.
however, since st systemstake as input an audio signal, we also providedtranslators with the audio ﬁle of each segment, ask-ing them to post-edit strictly according to it.6 foreach language pair, the ﬁnal pe-set used in ourstudy consists of the 550 must-c original audio-transcript-translation triplets plus two additionalsets of reference translations, i.e.
the post-editedversions of the two systems’ outputs..analyses.
the collected post-edits are exploitedto assess overall systems’ performance (§4) as wellas to carry out deeper quantitative and qualitativeanalyses aimed to shed light on possible systematicdifferences in systems’ behavior (§5.1 and §6.1).
focusing on speciﬁc aspects of the st problem, theinquiry is also performed by means of manual an-notation of systems’ outputs (§5.2, §6.2 and §7.1).
due to the linguistic nature of this task, centred onﬁne-grained aspects requiring a variety of skills inboth evaluation and st technology, for such anal-yses we relied on three researchers in translationtechnology – one per language pair – with a strongbackground in linguistics, excellent knowledge ofthe addressed languages (c2 or native), as well asstrong expertise in systems’ evaluation..4 overall systems’ performance.
we compute overall performance results both onthe pe-sets and on the must-c common test sets.
our primary evaluation is based on the collectedpost-edits.
we consider two ter-based7 metrics:i) human-targeted ter (hter) computed betweenthe automatic translation and its human post-editedversion, and ii) multi-reference ter (mter) com-puted against the closest reference among the threeavailable ones (two post-edits and the ofﬁcial ref-erence from must-c).
the latter metric better ac-counts for post-editors’ variability, making the eval-uation more reliable and informative.
for the sakeof completeness, in table 1 we also report sacre-.
5www.matecat.com6the ad-hoc st pe guidelines given to translators are.
included in appendix b..7www.cs.umd.edu/˜snover/tercom.
bleu8 (post, 2018) and ter scores computedonly on the ofﬁcial must-c common references..pe sethter mter bleu ter.
24.4125.6025.30.c 28.65d 30.22c 29.96d 28.19∗ 24.02∗ 32.17c 25.69d 26.14.
23.2923.26.m. commonbleu ter28.9653.23 28.8653.9352.77∗52.56 29.0528.4634.05∗ 50.75 32.93∗ 53.21∗54.0051.08 31.9830.04∗ 54.01 28.5656.2955.35∗54.06 28.5628.81.de.
es.
it.
table 1: performance of (c)ascade and (d)irect sys-tems on the pe-sets and must-c common test sets.
statistically signiﬁcant differences (∗) are computedwith paired bootstrap resampling (koehn, 2004)..a bird’s-eye view of the results shows that, inmore than half of the cases, performance differ-ences between cascade and direct systems are notstatistically signiﬁcant.
when they are, the rawcount of wins for the two approaches is the same(4), attesting their substantial parity..looking at our primary metrics (hter andmter), systems are on par on en-it and en-de,while for en-es the direct approach signiﬁcantlyoutperforms the cascade one.
this difference, how-ever, does not emerge with the other metrics.
in-deed, bleu and ter scores computed against theofﬁcial references are less coherent across metricsand test sets.
for instance, on the en-it pe-set thecascade system signiﬁcantly outperforms the directone in terms of bleu score, while ter shows theopposite on must-c common.
interestingly, thescores obtained using independent references canalso disagree with those computed with post-edits.
this is the case of en-es, where signiﬁcant hterand mter reductions attest the superiority of thedirect system, while most bleu and ter scoresare still in favor of the cascade..on the one hand, primary evaluation scores sug-gest that the rapidly advancing direct technologyhas eventually reached the traditional cascaded ap-proach.
on the other, the highlighted incongruitiesconﬁrm widespread concerns about the reliabilityof fully automatic metrics – based on independentreferences – to properly evaluate neural systems(way, 2018).
this calls for deeper quantitativeand qualitative analyses.
those presented in thenext sections investigate performance differencesfocusing on two main aspects: the impact of spe-ciﬁc input audio properties (§5), and the linguisticerrors made by the systems (§6)..8bleu+c.mixed+#.1+s.exp+tok.13a+v.1.4.3.
28765 st quality and audio properties.
5.1 automatic analysis.
the two st approaches handle the input audiodifferently: the cascade one by means of a ded-icated asr component that produces intermedi-ate transcripts; the direct one by extracting all therelevant information to translate in an end-to-endfashion.
is it therefore possible that some audioproperties have different impact on their results?
overall performance being equal, answering thisquestion would help to understand if one approachis preferable over the other under speciﬁc audioconditions..among other possible factors (e.g.
noise, record-ing conditions, overlapping speakers) we tried toshed light on this aspect by focusing on two com-mon factors: audio duration and speech rate.
tothis aim, we grouped the sentences in the pe-setaccording to the sentence-wise hter percentagedifference – i.e.
the difference between the cascadeand direct hter scores divided by their average.
the threshold for considering performance dif-ferences as signiﬁcant was set to 10%.
the re-sulting groups contain sentences where: i) cascadeis signiﬁcantly better than direct, ii) direct is sig-niﬁcantly better than cascade, iii) the differencebetween the two is not signiﬁcant, and iv) both sys-tems have hter=0.
for each group, we calculatedthe average audio duration and the correspondingspeech rate in terms of phonemes9 per second..results are shown in table 2, where – for thesake of completeness – also the length of the ref-erence audio transcript is given, together with theaverage hter of the systems..as we can see, results are coherent across lan-guages: audio duration and speech rate averages donot differ, neither when one system performs sig-niﬁcantly better than the other, nor when the hterdifferences are not signiﬁcant.
we can hence con-clude that, if audio duration and speech rate haveany inﬂuence on systems’ performance, our anal-ysis does not highlight speciﬁc conditions that aremore favorable to one approach than to the other.
both are equally robust with respect to the audioproperties here considered..5.2 manual analysis.
handling the input audio differently, the two ap-proaches have inherent strengths and weaknesses..9obtained by processing the transcripts with espeak.
(espeak.sourceforge.net)..secnetnes#.
240191457421523454472312125552.noitarudoidua.)
sdnoces(.
6.156.006.682.715.926.286.473.096.036.066.932.96.etar.hceeps.)s/semenohp(.
htgnel.rcsnart.fer.)sdrow#(.
14.43 19.7514.52 18.8814.31 22.0715.539.6412.20 19.5212.09 20.3912.01 20.2613.14 10.2312.31 19.4112.21 19.3311.94 21.7312.68 10.33.rethc.rethd.16.30 40.5344.85 17.8940.74 40.18.
16.09 38.7646.45 21.1440.22 40.37.
14.82 36.4037.65 15.8035.39 35.37.
0.
0.
0.
0.
0.
0.de.
es.
it.
c betterd betterno diffhter 0c betterd betterno diffhter 0c betterd betterno diffhter 0.table 2: comparison of (c)ascade and (d)irect perfor-mance based on different audio properties..in particular, although suffering from the well-known scarcity of sizeable training corpora, di-rect solutions come with the promise (sperber andpaulik, 2020) of: i) higher robustness to error prop-agation, and ii) reduced loss of speech information(e.g.
prosody).
our next qualitative analysis triesto delve into these aspects by looking at audio un-derstanding and prosody issues..audio understanding.
errors due to wrong au-dio understanding are easy to identify for cascadesystems – since they are evident in the intermedi-ate asr transcripts – but harder to spot for directsystems, whose internal representations are by farless accessible.
in this case, errors can still be iden-tiﬁed in mistranslations corresponding to wordswhich are phonetically similar to parts of the inputaudio – e.g.
nice voice mistranslated in germanas nette jungen (nice boys).
to spot such errors,our annotators carefully inspected the pe-set bycomparing the audio, the reference transcripts andsystems’ output translations for both the cascadeand direct models, as well as the asr transcriptsfor the cascade one.
some interesting examples ofthe identiﬁed errors are reported in table 3..audio to the er- euh [disﬂuency] egyptian governmentder eruptiven [eng.
“eruptive”] regierung ...can die regierung ¨agyptensdaudio dominated by big, scary guys,...cdaudio i think, like her,...cd.penso che, come qui [eng.
“here”], ...penso che, come i capelli [eng.
“hair”], ....dominados por grandes tipos aterradoresdominados por los chicos de big kerry.
table 3: examples of audio understanding errors..2877as shown in table 4, audio understanding errorsare quite common for both systems in all languagepairs.
however, both the number of errors andthe number of sentences they affect is signiﬁcantlylower for the direct one.
we observed that this isthe case especially for “more difﬁcult” sentences,such as sentences with poor audio quality and over-lapping or disﬂuent speech..though far from being conclusive (we acknowl-edge that, due to the “opacity” of direct models,their error counts might be slightly underestimated),this analysis seems to conﬁrm the theoretical advan-tages of direct st. this ﬁnding advocates for morethorough future investigations on neural networks’interpretability, targeting its empirical veriﬁcationon larger and diverse benchmarks..both c d ctot dtot csent dsent147 10351190 14882169 15687.
96 52108 6682 69.
117150143.
91127138.deesit.
table 4: audio understanding errors in the pe-set andnumber of sentences containing at least one such error..prosody.
prosody is central to disambiguating ut-terances, as it reﬂects language elements whichmay not be encoded by grammar and vocabularychoices.
while prosody is directly encoded by thedirect system, it is lost in the unpunctuated inputreceived by the mt component of a cascade.
be-sides few interrogative sentences, our annotatorswere able to isolate only a handful of utteranceswhose prosodic markers result in different inter-pretations by the two models.
concerning inter-rogatives, both systems managed to translate themcorrectly in most cases (24 for cascade and 25 fordirect out of 31).
this is not surprising given thesyntactic structure of english questions, which isexplicit and does not rely solely on prosody (e.g.
compared to italian).
in all other cases (examplesin table 5), the direct model’s higher sensitivity toprosody seems to give it an edge on cascade in dis-ambiguating and correctly rendering the utterancemeaning.
also this ﬁnding calls for future inquiriesaimed to check the regularity of these differenceson larger datasets..6 linguistic errors.
6.1 automatic analysis.
for this analysis, we rely on the publicly availabletool10 used by bentivogli et al.
(2018a) to analyse.
10wit3.fbk.eu/2016-02, details in appendix c..src nation states — governments doing the attacksc regierungen der nationalstaaten[governments of nation states].
d nationen, regierungen[nations, governments].
src like the one we saw before, movingc como el que vimos antes de moverse[like the one we saw before moving].
d como el que hemos visto antes, movi´endose.
[like the one we saw before, moving]src photos like this: construction going onc foto come questa costruzione[photos like this construction]d foto come queste: costruzione.
[photos like these: construction].
table 5: the two approaches dealing with prosody..en-ded.en-es.
en-it.
c.∆% c.d ∆% c.d ∆%0.0l 2481 2560 +3.2 2674 2497 -6.6 2264 2264433 470 +8.6535 494 -7.7m 468 536 +14.5398 476 +19.6230 226 -1.7308 290 -5.8r3347 3572 +6.7 3517 3281 -6.7 2927 2960 +1.1.
table 6: distribution of (l)exical, (m)orphological and(r)eordering errors.
absolute numbers are presentedtogether with the percentage of reduction/increase ofthe (d)irect system with respect to the (c)ascade (∆%)..what linguistic phenomena are best modeled bymt systems.
the tool exploits manual post-editsand hter-based computations to detect and clas-sify translation errors according to three linguisticcategories: lexicon, morphology and word order.
table 6 presents their distribution..as expected from the hter scores in table 1,results vary across language pairs.
on en-it, sys-tems show pretty much the same number of errors,with a slight percentage gain (+1.1) in favor of thecascade.
for the other two pairs, differences aremore marked and opposite, with an overall errorreduction for the direct system on en-es (-6.7) andin favor of the cascade on en-de (+6.7)..looking at the distribution of errors across cat-egories, while for en-es the direct system is al-ways better and the percentage reduction is homo-geneously distributed, for en-de the better perfor-mance of the cascade is concentrated in the mor-phology and word order categories.
since englishand german are the most different languages interms of morphology and word order, this resultsuggests that cascade systems still have an edge onthe direct ones in their ability to handle morphologyand word reordering.
this is further supported byen-it: the only difference, in favor of the cascade,is indeed observed in the morphology category..28786.2 manual analysis.
since lexical errors represent by far the most fre-quent category for both approaches in all languagepairs, we complement the automatic analysis witha more ﬁne-grained manual inspection, further dis-tinguishing among lexical errors due to missingwords, extra words, or wrong lexical choice.11.
the analysis was carried out on subsets of thepe-set, created in such a way to be suitable for man-ual annotation.
namely, we removed sentences forwhich the output of the two systems is: i) identi-cal, ii) judged correct by post-editors (hter=0),or iii) too poor to be reliably annotated for errors(hter>40%).
the resulting sets contain 207 sen-tences for en-de, 238 for en-es, and 285 for en-it..this analysis reveals that, for all language pairs,wrong lexical choice is the most frequent error type(∼65% of lexical errors on average) followed bymissing words (∼30%), and extra words (∼5%)..while errors due to lexical choice and superﬂu-ous words vary across languages, we observe asystematic behavior with respect to missing words(words that are present in the audio but are nottranslated).
as we can see in table 7, direct sys-tems lose more information from the source inputthan their cascade counterparts, in terms of bothsingle words and contiguous word sequences.
it isparticularly interesting to notice that also for en-es– where the direct system is signiﬁcantly strongerthan the cascade – the issue is still evident, althoughto a lesser extent.
table 8 collects examples of theencountered lexical phenomena..singlewordsdc342540268353.wordsequencesdc10611101814.total# wordsd5868128.
∆%+38.10+15.25+33.33.
c425996.deesit.
table 7: missing words for (c)ascade and (d)irect sys-tems.
absolute numbers vary across languages as theyreﬂect the different size of the annotated subsets..finally, we report that a non-negligible amountof missing words (between 10% and 20%) is repre-sented by discourse markers, i.e.
words or phrasesused to connect and manage what is being said(e.g.
“you know”, “well”, “now”).
although this is.
11various error taxonomies covering different levels ofgranularity have been developed, and the distinction betweenthese types of lexical errors is widely adopted, including thedqf-mqm framework – https://info.taus.net/dqf-mqm-error-typology-templ.
“das ist in ordnung.” [ – ] george,“das ist in ordnung, [ – ] george,”.
audio “that’s ﬁne”, says george,cdaudio well after two years, ...bueno, despu´es de dos a˜nos, ...c[ – ] despu´es de dos a˜nos, ...daudio my wife and kids and i, moved to ...cd.io e mia moglie e i miei ﬁgli ci siamo trasferiti...io e mia moglie [ – ] ci siamo trasferiti....table 8: examples of missing words..a frequent phenomenon in speech, not translatingdiscourse markers cannot be properly consideredas an error, since markers i) do not carry semanticinformation, and ii) can be intentionally droppedin some use cases, such as in subtitling..7 classiﬁers’ verdict.
so far, our inquiry has been entirely driven by pre-deﬁned assumptions (the importance of certain au-dio properties) and linguistic criteria (the focuson speciﬁc error types).
this top-down approach,however, might fail to disclose important differ-ences, which were not speciﬁcally sought afterwhen analysing the two paradigms.
this considera-tion motivates the adoption of the complementarybottom-up approach that concludes our compara-tive study by answering the question: is the outputof cascade and direct systems distinguishable?
un-derstanding if and why discriminating between thetwo is possible would not only suggest new issuesto look at.
it would also highlight possible outputregularities that, despite the similar overall perfor-mance, make one paradigm preferable over theother in speciﬁc application scenarios.
to this aim,we set up a classiﬁcation experiment, comparingthe ability of humans to correctly identify the out-put of the two systems with the performance of anautomatic text classiﬁer..7.1 human classiﬁcation.
after getting acquainted with systems’ outputthrough the previous manual analyses, our asses-sors were instructed to perform a classiﬁcation task.
the classiﬁcation had to be performed on 10 blocksof items comprising a set of unseen english con-tiguous sentences (gold transcripts) from the must-c common test set, and two sets of anonymizedtranslations, one produced by the cascade and oneby the direct model.
for each block, the assessorshad to assign each set of translations to the correctsystem, or label them as indistinguishable.
to in-vestigate whether more context helps in the assign-.
2879ment, we set up two experiments with respectively10 and 20 contiguous sentences per block..en-de.
en-es.
en-it.
# of sentencescorrectwrongindistinguishabletotal # of blocks.
1072110.
2062210.
1042410.
2043310.
1041510.
2032510.table 9: results of human classiﬁcation..the results in table 9 show that en-es and en-itsystems are not distinguishable, since only a maxi-mum of 4 blocks out of 10 were correctly classiﬁed,while most en-de blocks were correctly classiﬁed.
according to the en-de assessor, this is due to thefact that the structure of the sentences generatedby the direct system is very similar to that of thecorresponding english sources.
this characteristicstands out in german, which differs from englishin terms of word order more than italian and span-ish.
this type of behavior does not necessarilyimply the presence of errors but, like a ﬁngerprint,makes the en-de direct system more recognizableby a human.
furthermore, being sub-optimal forgerman, this structure can cause preferential editsby the post-editors, which would be in line with theconcentration of errors in the word order categoryobserved in table 6 (+19.6%)..assessing the importance of context, the abilityof humans to distinguish the systems does not im-prove when passing from 10 to 20 sentences perblock.
this suggests that the behavioral differencesbetween cascade and direct systems are so subtlethat, on larger samples, they mix up and balancemaking their ﬁngerprints less traceable..7.2 automatic classiﬁcation.
as a complement to the human classiﬁcation ex-periment, we check whether an automatic tool isable to accomplish a similar task.
our classiﬁercombines n-gram language models with the naivebayes algorithm, as proposed in (peng and schu-urmans, 2003).
we trained two 5-gram models,respectively using translations by the cascade andthe direct systems.
at classiﬁcation time, given atranslated text, the classiﬁer computes the perplex-ity of the two models and assigns the cascade ordirect label based on the model with the lowest per-plexity.
also these experiments were carried out onthe must-c common set.
the classiﬁer was testedvia k-fold cross-validation, for different values of k– i.e.
different sizes of text to classify..as shown in figure 1, contrary to humans, themore data the classiﬁer receives, the higher its ac-curacy in discriminating between systems.
alreadyat a size of 20 sentences, accuracy is always ∼80%.
this suggests that systems have their own “lan-guage”, a ﬂuency-related ﬁngerprint..figure 1: results of automatic classiﬁcation for differ-ent sizes of system output blocks (1-600 sentences)..to check this ﬁnding, we measured outputs’ lex-ical diversity in terms of moving average type-token ratio – mattr (covington and mcfall,2010) – and with the measure of textual lexicaldiversity (mtld) by mccarthy and jarvis (2010).
table 10 shows that the cascade output exhibitshigher lexical diversity on all languages, withsmaller differences on en-de and en-es comparedto en-it.
a plausible conclusion is that the cas-cade produces richer output, whose variety doesnot necessarily result in better translations nor isappreciated by humans.
indeed, annotators wereable to correctly distinguish the output only foren-de, where lexical diversity is similar (see §7.1)..en-de.
en-es.
en-it.
mattr mtld mattr mtld mattr74.5069.8173.11r73.2068.4271.84c72.6067.99d 71.45.
77.1967.6865.59.
97.0283.6483.27.mtld109.7997.8290.78.table 10: lexical diversity of the human (r)eference,(c)ascade and (d)irect outputs..8 conclusion and final remarks.
there is a time when the possible transition fromconsolidated technological frameworks to newemerging paradigms depends on answering fun-damental questions about their potential, strengthsand weaknesses.
a time when technology develop-ers are faced with the choice of where to direct theirfuture investments.
five years after its appearance.
2880on the scene, the direct approach to st confrontsthe community with similar questions in relationto the traditional cascade paradigm that it aims toovertake.
our investigation showed that, in spiteof the known data paucity conditions still penaliz-ing the direct approach, the two technologies nowperform substantially on par.
subtle differencesin their behavior exist: overall performance beingequal, the cascade still seems to have an edge interms of morphology, word ordering and lexicaldiversity, which is balanced by the advantages ofdirect models in audio understanding and in captur-ing prosody.
however, they do not seem sufﬁcientand consistent enough across languages to makethe output of the two approaches easily distinguish-able, nor to make one model preferable to the other.
back to our title, they no longer make a difference..we are aware that the generalizability of these re-sults depends on several factors such as the consid-ered languages, systems and benchmarks, as wellas the human workforce deployed for the inquiry.
here, with the help of professionals, we proposedmulti-faceted quantitative and qualitative analyses,run on the output of state-of-the-art systems onthree language pairs – though, by now, coveringonly the most-explored and data-favorable condi-tion, which has english as source.
although ourﬁndings hold for a speciﬁc scenario, in which freedata were at our disposal (and to which we con-tribute back by releasing high-quality post-edits),they might not be generalizable to other (e.g.
dif-ﬁcult, distant) languages and other (e.g.
highlyspecialized) domains.
nevertheless, we presentthem as a timely contribution towards answering aburning question within the st community..acknowledgements.
the creation of the post-edits used in this work wasfunded by the european association for machinetranslation (eamt) through its 2020 sponsorshipof activities programme.
the computational costswere covered by the “end-to-end spoken languagetranslation in rich data conditions” project,12which was ﬁnancially supported by an amazonaws ml grant..12https://ict.fbk.eu/.
units-hlt-mt-e2eslt/.
references.
antonios anastasopoulos and david chiang.
2018.tied multitask learning for neural speech transla-tion.
in proceedings of the conference of the northamerican chapter of the association for computa-tional linguistics (naacl), pages 82–91, new or-leans, us-la..ebrahim ansari, amittai axelrod, nguyen bach,ondˇrej bojar, roldano cattoni, fahim dalvi, nadirdurrani, marcello federico, christian federmann,jiatao gu, fei huang, kevin knight, xutai ma, ajaynagesh, matteo negri, jan niehues, juan pino, eliz-abeth salesky, xing shi, sebastian st¨uker, marcoturchi, alexander waibel, and changhan wang.
2020. findings of the iwslt 2020 evaluation cam-paign.
in proceedings of the international confer-ence on spoken language translation (iwslt), vir-tual event..amittai axelrod, xiaodong he, and jianfeng gao.
2011. domain adaptation via pseudo in-domainin proceedings of the 2011 con-data selection.
ference on empirical methods in natural languageprocessing, edinburgh, scotland, uk..parnia bahar, tobias bieschke, and hermann ney.
2019a.
a comparative study on end-to-end speechto text translation.
in proceedings of the interna-tional workshop on automatic speech recognitionand understanding (asru), pages 792–799, sen-tosa, singapore..parnia bahar, patrick wilken, tamer alkhouli, an-dreas guta, pavel golik, evgeny matusov, andchristian herold.
2020. start-before-end and end-to-end: neural speech translation by apptek andin proceedings of therwth aachen university.
international workshop on spoken language trans-lation (iwslt), virtual event..parnia bahar, albert zeyer, ralf schl¨uter, and her-mann ney.
2019b.
on using specaugment for end-in proceedings of theto-end speech translation.
international workshop on spoken language trans-lation (iwslt), hong kong..sameer bansal, herman kamper, karen livescu,adam lopez, and sharon goldwater.
2019. pre-training on high-resource speech recognition im-proves low-resource speech-to-text translation.
inproceedings of the conference of the north ameri-can chapter of the association for computationallinguistics (naacl), minneapolis, us-mn..daniel beck, trevor cohn, and gholamreza haffari.
2019. neural speech translation using lattice trans-in proceedingsformations and graph networks.
of the emnlp workshop on graph-based methodsfor natural language processing (textgraphs-13),pages 26–31, hong kong..luisa bentivogli, arianna bisazza, mauro cettolo,and marcello federico.
2018a.
neural versusphrase-based mt quality: an in-depth analysis on.
2881english–german and english–french.
computerspeech and language”, 49:52 – 70..luisa bentivogli, mauro cettolo, marcello federico,and christian federmann.
2018b.
machine trans-lation human evaluation: an investigation of eval-uation based on post-editing and its relation within proceedings of the interna-direct assessment.
tional conference on spoken language translation(iwslt), bruges, belgium..alexandre b´erard, laurent besacier, ali can ko-cabiyikoglu, and olivier pietquin.
2018. end-to-end automatic speech translation of audiobooks.
in proceedings of the ieee international confer-ence on acoustics, speech and signal processing(icassp), pages 6224–6228, calgary, canada..alexandre b´erard, olivier pietquin, christophe servan,and laurent besacier.
2016. listen and translate:a proof of concept for end-to-end speech-to-texttranslation.
in proceedings of the nips workshopon end-to-end learning for speech and audio pro-cessing, barcelona, spain..nicola bertoldi and marcello federico.
2005. a newdecoder for spoken language translation based onin proceedings of the ieeeconfusion networks.
workshop on automatic speech recognition and un-derstanding (asru), pages 86–91, san juan, puertorico..ondrej bojar, rajen chatterjee, christian federmann,barry haddow, matthias huck, chris hokamp,philipp koehn, varvara logacheva, christof monz,matteo negri, matt post, carolina scarton, luciaspecia, and marco turchi.
2015. findings of the2015 workshop on statistical machine translation.
in proceedings of the workshop on statistical ma-chine translation (wmt), lisbon, portugal..roldano cattoni, mattia a. di gangi, luisa bentivogli,matteo negri, and marco turchi.
2020. must-c: amultilingual corpus for end-to-end speech transla-tion.
computer speech & language journal.
doi:https://doi.org/10.1016/j.csl.2020.101155..mauro cettolo, jan niehues, sebastian st¨uker, luisabentivogli, and marcello federico.
2013. report onthe 10th iwslt evaluation campaign.
in proceed-ings of the international workshop on spoken lan-guage translation (iwslt), heidelberg, germany..qiao cheng, meiyuan fang, yaqian han, jin huang,and yitao duan.
2019. breaking the data barrier:towards robust speech translation via adversarialin proceedings of the interna-stability training.
tional workshop on spoken language translation(iwslt), hong kong..michael a. covington and joe d. mcfall.
2010.cutting the gordian knot: the moving-averagetype–token ratio (mattr).
journal of quantita-tive linguistics, 17(2):94–100..michael denkowski and alon lavie.
2010. choosingthe right evaluation for machine translation: an ex-amination of annotator and automatic metric perfor-mance on human judgment tasks.
in proceedings ofthe conference of the association of machine trans-lation in the americas (amta), denver, us-co..mattia a. di gangi, robert enyedi, alessandra bru-sadin, and marcello federico.
2019a.
robustneural machine translation for clean and noisyspeech transcripts.
in proceedings of the interna-tional workshop on spoken language translation(iwslt), hong kong..mattia a. di gangi, matteo negri, and marco turchi.
2019b.
adapting transformer to end-to-end spo-in proceedings of theken language translation.
conference of the international speech communica-tion association (interspeech), graz, austria..mattia antonino di gangi, matteo negri, and marcoturchi.
2019c.
one-to-many multilingual end-to-end speech translation.
in ieee automatic speechrecognition and understanding workshop (asru),14-18 december 2019, sentosa, singapore..and marco turchi.
2020..marco gaido, mattia a. di gangi, matteo ne-gri,end-to-endspeech-translation with knowledge distillation:fbk@iwslt2020.
in proceedings of the interna-tional conference on spoken language translation(iwslt), virtual event..yvette graham, timothy baldwin, meghan dowling,maria eskevich, teresa lynn, and lamia tounsi.
is all that glitters in machine translation2016.in proceedingsquality estimation really gold?
of the international conference on computationallinguistics (coling), pages 3124–3134, osaka,japan..steven gutstein, olac fuentes, and eric freudenthal.
2008. knowledge transfer in deep convolutionalneural nets.
international journal on artiﬁcial in-telligence tools, 17(03):555–567..franc¸ois hernandez, vincent nguyen, sahar ghan-nay, natalia tomashenko, and yannick est`eve.
2018.ted-lium 3: twice as much data and corpusrepartition for experiments on speaker adaptation.
in proceedings of the speech and computer - 20thinternational conference (specom), pages 198–208, leipzig, germany.
springer international pub-lishing..geoffrey hinton, oriol vinyals, and jeff dean.
2015.distilling the knowledge in a neural network.
inproceedings of nips deep learning and represen-tation learning workshop, montr´eal, canada..hirofumi inaguma, shun kiyono, kevin duh, shigekikarita, nelson yalta, tomoki hayashi, and shinjiwatanabe.
2020. espnet-st: all-in-one speechthe an-translation toolkit.
nual meeting of the association for computational.
in proceedings of.
2882linguistics (acl): system demonstrations, virtualevent..sathish r. indurthi, houjeung han, nikhil k. laku-marapu, beomseok lee, insoo chung, sangha kim,and chanwoo kim.
2020. end-end speech-to-texttranslation with modality agnostic meta-learning.
in proceedings of the ieee international confer-ence on acoustics, speech and signal processing(icassp), pages 7904–7908, barcelona, spain..javier iranzo-s´anchez, joan albert silvestre-cerd`a,javier jorge, nahuel rosell´o, gim´enez.
adri`a, al-bert sanchis, jorge civera, and alfons juan.
2020.europarl-st: a multilingual corpus for speechtranslation of parliamentary debates.
in proceed-ings of the ieee international conference on acous-tics, speech and signal processing (icassp), pages8229–8233, barcelona, spain..ye jia, melvin johnson, wolfgang macherey, ron j.weiss, yuan cao, chung-cheng chiu, naveen ari,stella laurenzo, and yonghui wu.
2019. lever-aging weakly supervised data to improve end-to-in proceedingsend speech-to-text translation.
of the ieee international conference on acous-tics, speech and signal processing (icassp), pages7180–7184, brighton, uk..tom ko, vijayaditya peddinti, daniel povey, andsanjeev khudanpur.
2015. audio augmentationin proceedings of thefor speech recognition.
conference of the international speech communi-cation association (interspeech), pages 3586–3589, dresden, germany..philipp koehn.
2004. statistical signiﬁcance tests formachine translation evaluation.
in proceedings ofthe conference on empirical methods in naturallanguage processing (emnlp), barcelona, spain..alon lavie, donna gates, marsal gavalda, laura may-ﬁeld tomokiyo, alex waibel, and lori levin.
1996.multi-lingual translation of spontaneously spokenlanguage in a limited domain.
in proceedings of theinternational conference on computational linguis-tics (coling), copenhagen, denmark..yuchen liu, hao xiong, jiajun zhang, zhongjunhe, hua wu, haifeng wang, and chengqing zong.
2019. end-to-end speech translation with knowl-edge distillation.
in proceedings of the conferenceof the international speech communication asso-ciation (interspeech), pages 1128–1132, graz,austria..yuchen liu, junnan zhu, jiajun zhang, and chengqingzong.
2020. bridging the modality gap for speech-to-text translation.
arxiv.org/pdf/2010.14920.pdf..evgeny matusov, hermann ney, and ralph schluter.
2005.phrase-based translation of speech recog-nizer word lattices using loglinear model combina-in proceedings of the ieee workshop ontion.
automatic speech recognition and understanding(asru), pages 110–115, san juan, puerto rico..philip m. mccarthy and scott jarvis.
2010. mtld,vocd-d, and hd-d: a validation study of sophisti-cated approaches to lexical diversity assessment.
be-havior research methods, 42(2):381–392..graham neubig, matthias sperber, xinyi wang,matthieu felix, austin matthews, sarguna padman-abhan, ye qi, devendra singh sachan, philip arthur,pierre godard, john hewitt, rachid riad, and lim-ing wang.
2018. xnmt: the extensible neural ma-in conference of the as-chine translation toolkit.
sociation for machine translation in the americas(amta) open source software showcase, boston..thai-son nguyen, sebastian stueker, jan niehues,improving sequence-to-and alex waibel.
2020.sequence speech recognition training with on-the-ﬂy data augmentation.
in proceedings of the inter-national conference on acoustics, speech, and sig-nal processing (icassp), barcelona, spain..jan niehues, roldano cattoni, sebastian stucker, mat-teo negri, marco turchi, et al.
2019a.
the iwslt2019 evaluation campaign.
in proceedings of theinternational workshop on spoken language trans-lation (iwslt), hong kong..jan niehues, roldano cattoni, sebastian st¨uker,mauro cettolo, marco turchi, and marcello fed-erico.
2018. the iwslt 2018 evaluation cam-in proceedings of the international work-paign.
shop on spoken language translation (iwslt),bruges, belgium..jan niehues, roldano cattoni, sebastian st¨uker, mat-teo negri, marco turchi, thanh-le ha, elizabethsalesky, ramon sanabria, loic barrault, lucia spe-cia, and marcello federico.
2019b.
the iwslt2019 evaluation campaign.
in proceedings of theinternational workshop on spoken language trans-lation (iwslt), hong kong..myle ott, sergey edunov, alexei baevski, angelafan, sam gross, nathan ng, david grangier, andfairseq: a fast, extensiblemichael auli.
2019.in proceedings oftoolkit for sequence modeling.
the 2019 conference of the north american chap-ter of the association for computational linguistics(demonstrations), pages 48–53, minneapolis, min-nesota.
association for computational linguistics..vassil panayotov, guoguo chen, daniel povey, andsanjeev khudanpur.
2015. librispeech: an asrcorpus based on public domain audio books.
in proceedings of the ieee international confer-ence on acoustics, speech and signal processing(icassp), pages 5206–5210, brisbane, australia..daniel s. park, william chan, yu zhang, chung-cheng chiu, barret zoph, ekin d. cubuk, andquoc v. le.
2019. specaugment: a simple dataaugmentation method for automatic speech recog-nition.
in proceedings of the conference of the in-ternational speech communication association (in-terspeech), pages 2613–2617, graz, austria..2883stephan peitz, simon wiesler, markus nußbaum-thom, and hermann ney.
2012. spoken languagetranslation using automatically transcribed text intraining.
in proceedings of the international work-shop on spoken language translation (iwslt),hong kong..matthew snover, bonnie dorr, richard schwartz, lin-nea micciulla, and john makhoul.
2006. a study oftranslation edit rate with targeted human anno-tation.
in proceedings of the conference of the as-sociation for machine translation of the americas(amta), pages 223–231, cambridge, us-ma..fuchun peng and dale schuurmans.
2003. combin-ing naive bayes and n-gram language models fortext classiﬁcation.
in proceedings of the europeanconference on information retrieval (ecir), pages335–350, pisa, italy..ngoc-quan pham, thai-son nguyen, thanh-le ha,juan hussain, felix schneider, jan niehues, se-bastian st¨uker, and alexander waibel.
2019. theiwslt 2019 kit speech translation system.
inproceedings of the international workshop on spo-ken language translation (iwslt), hong kong..juan pino, liezl puzon, jiatao gu, xutai ma, arya d.mccarthy, and deepak gopinath.
2019. harnessingindirect training data for end-to-end automaticin pro-speech translation: tricks of the trade.
ceedings of the international workshop on spokenlanguage translation (iwslt), hong kong..matt post.
2018. a call for clarity in reporting bleuin proceedings of the conference on ma-scores.
chine translation (wmt), pages 186–191, brussels,belgium..tomasz potapczyk and pawel przybysz.
2020. sr-pol’s system for the iwslt 2020 end-to-endspeech translation task.
in proceedings of the in-ternational conference on spoken language trans-lation (iwslt), virtual event..daniel povey, hossein hadian, pegah ghahremani,ke li, and sanjeev khudanpur.
2018. a time-in pro-restricted self-attention layer for asr.
ceedings of the ieee international conference onacoustics, speech and signal processing (icassp),pages 5874–5878, calgary, canada..nicholas ruiz, qin gao, william lewis, and mar-cello federico.
2015. adapting machine translationmodels toward misrecognized speech with text-to-speech pronunciation rules and acoustic confusabil-ity.
in proceedings of the conference of the interna-tional speech communication association (inter-speech), dresden, germany..ramon sanabria, ozan caglayan, shruti palaskar,desmond elliott, lo¨ıc barrault, lucia specia, andflorian metze.
2018. how2: a large-scale datasetin pro-for multimodal language understanding.
ceedings of visually grounded interaction and lan-guage (vigil), montr´eal, canada.
neural informa-tion processing society (neurips)..rico sennrich, barry haddow, and alexandra birch.
neural machine translation of rarearxiv preprint.
2015.words with subword units.
arxiv:1508.07909..matthew snover, nitin madnani, bonnie j dorr, andrichard schwartz.
2009. fluency, adequacy, orhter?
: exploring different human judgments within proceedings of the work-a tunable mt metric.
shop on statistical machine translation (wmt),pages 259–268, athens, greece..matthias sperber, graham neubig, ngoc-quan pham,and alex waibel.
2019. self-attentional models forin proceedings of the annual meet-lattice inputs.
ing of the association for computational linguistics(acl), pages 1185–1197, florence, italy..matthias sperber, jan niehues, graham neubig, sebas-tian st¨uker, and alex waibel.
2018. self-attentionalacoustic models.
in proceedings of the conferenceof the international speech communication associ-ation (interspeech), pages 3723–3727, hyder-abad, india..matthias sperber, jan niehues, and alex waibel.
2017.toward robust neural machine translation fornoisy input sequences.
in proceedings of the inter-national workshop on spoken language translation(iwslt), tokyo, japan..matthias sperber and matthias paulik.
2020. speechtranslation and the end-to-end promise: taking stockof where we are.
in proceedings of the annual meet-ing of the association for computational linguistics(acl), pages 7409–7421, virtual event..fred w.m.
stentiford and martin g. steer.
1988. ma-chine translation of speech.
british telecom tech-nology journal, 6(2):116–122..christian szegedy, vincent vanhoucke, sergey ioffe,jon shlens, and zbigniew wojna.
2016. rethink-ing the inception architecture for computer vision.
in proceedings of 2016 ieee conference on com-puter vision and pattern recognition (cvpr), pages2818–2826, las vegas, nevada, united states..j¨org tiedemann.
2016. opus – parallel corpora for ev-eryone.
baltic journal of modern computing, page384. special issue: proceedings of the 19th annualconference of the european association of machinetranslation (eamt)..ashish vaswani, noam shazeer, niki parmar, jakobuszkoreit, llion jones, aidan n. gomez, lukaszkaiser, and illia polosukhin.
2017. attention is allin proceedings of advances in neuralyou need.
information processing systems 30 (nips), pages5998–6008, long beach, us-ca..alex waibel, ajay n jain, arthur e mcnair, hiroakisaito, alexander g hauptmann, and joe tebelskis.
1991. janus: a speech-to-speech translation system.
2884using connectionist and symbolic processing strate-gies.
in proceedings of the ieee international con-ference on acoustics, speech, and signal processing(icassp), pages 793–796, toronto, canada..andy way.
2018. quality expectations of machinetranslation.
in s. castilho, j. moorkens, f. gaspari,and s. doherty, editors, translation quality assess-ment: from principles to practice, pages 159–178.
springer..ron j. weiss, jan chorowski, navdeep jaitly, yonghuiwu, and zhifeng chen.
2017.sequence-to-sequence models can directly translate foreignspeech.
in proceedings of the conference of the in-ternational speech communication association (in-terspeech), pages 2625–2629, stockholm, swe-den..a systems’ description.
in this section we describe the st models createdfor our study (see section 3.1 ).
all the detailsabout the different trainings are given below, whilethe validation set was common to all trainings,since we used the must-c dev set..the source code for the asr and the directst models is available at: https://github.com/mgaido91/fbk-fairseq-st..the source code for the mt component of thecascade model can be found at: https://github.
com/modernmt/modernmt..a.1 cascade approach.
the cascade system is composed of a pipeline ofautomatic speech recognition (asr) and machinetranslation (mt) models..the asr model is a slightly revisited version(gaido et al., 2020) of the s-transformer (di gangiet al., 2019b), where the two 2d self-attention lay-ers are replaced with two transformer encoder lay-ers (for a total of 8 layers), while the decoder isthe same (with 6 layers).
hence, the model pro-cesses the input with two 3x3 2d cnns (having 64ﬁlters), whose output is ﬁrst projected into a higher-dimensional space and then summed with posi-tional embeddings before being fed to the trans-former encoder layers; transformer encoder layersuse logarithmic distance penalty.
the attentionmechanism consists of 8 attention heads.
the di-mensionality of input and output is 512, while theinner-layers have dimensionality 2048. the result-ing number of parameters is 63m..the asr model was trained with the goal ofachieving state-of-the-art performance.
to this aim,we relied on two data augmentation techniques.
that were shown to yield competitive models at theiwslt-2020 evaluation campaign (ansari et al.,2020), namely: i) specaugment (park et al., 2019)applied with probability 0.5 by masking two bandson the frequency axis (with 13 as maximum masklength) and two on the time axis (with 20 as max-imum mask length), and ii) time stretch (nguyenet al., 2020) with probability of 0.3 and stretchingfactor sampled uniformly for each utterance be-tween 0.8 and 1.25. the asr model was trained on1.25m utterance-transcript pairs coming from theasr corpora librispeech (panayotov et al., 2015),mozilla common voice,13 how2 (sanabria et al.,2018), tedlium-v3 (hernandez et al., 2018),as well as the st corpora europarl-st (iranzo-s´anchez et al., 2020) and must-c (cattoni et al.,2020).14 we ﬁltered out all pairs whose utterancewas longer than 20 seconds.
the audio input waspreprocessed with xnmt15 (neubig et al., 2018)to extract 40 features per time frame (with 25mswindows and 10ms sliding) and per-speaker nor-malization was applied.
the text was preprocessedby normalizing punctuation and de-escaping spe-cial characters, and was tokenized with moses.16then it was encoded with a bpe (sennrich et al.,2015) code learnt on the opus data17 using 8kmerge rules..the mt component is built on the modernmtframework18 which features machine translationimplementing the transformer architecture.
wetrained either a base (en-it) or a big (en-{de,es})transformer model (vaswani et al., 2017) with6 blocks in the encoder and 6 in the decoder,512/1024 as input size, the same as output size,2048/4096 as inner dimension and 8/16 attentionheads.
the total number of parameters is about61m for the base model, 210m for the big models.
as regards pre-processing, for all the three lan-guage directions we used the internal modernmtprocedures..in training, models are optimized with adamusing β1=0.9, β2=0.98; the learning rate is linearlyincreased during the warmup (8k iterations) up to.
13https://voice.mozilla.org/14for english-german, the st corpora include also thespeech-translation ted corpus provided in the iwslthttp://iwslt.org/ofﬂine-speech-translation task:doku.php?id=offline_speech_translation.
15https://github.com/neulab/xnmt16https://github.com/moses-smt/.
mosesdecoder.
17http://opus.nlpl.eu18https://github.com/modernmt/modernmt.
2885the maximum value (5 × 10−4), after that it followsan inverse square root decay; dropout is set to 0.3.minibatches consist of 3072 tokens and updatefrequency is set to 4; the total number of iterationsis 200k; the last 10 saved checkpoints (one out of1k iterations) are averaged.
the model uses labelsmoothing with a uniform prior distribution (0.1)over the vocabulary; source and target languagesshare a bpe vocabulary of 32k sub-words..#segments #en words #trg words776.4m723.3m972.5m 1024.9m770.2m792.6m.
58.2m70.1m67.9m.
en-deen-esen-it.
table 11: statistics of the parallel training sets col-lected from the opus repository for the three lan-guages pairs..the training data, whose statistics are reportedin table 11, are collected from the opus reposi-tory.
for english-italian, they resulted in almost70m segment pairs and about 800m english words;after deduplication and the internal modernmtcleaning, the actual training data is reduced to45m pairs and 550m english words.
for english-{german,spanish} pairs, the opus data were ﬁl-tered through well-known data selection methods(axelrod et al., 2011) using a general-domain seed;the resulting training data consist of, respectively,17m and 19m segment pairs, for 270m and 330menglish words.
trainings were performed on rtx2080 ti gpus; for english-italian, it was run on 7gpus and lasted 3 days, while for each of the othertwo directions, on a single gpu, it took 6 days..the three models are then ﬁne-tuned on must-ctraining data (∼250k pairs, 4-5m english words)by continuing the training for 4k iterations on theadaptation data, with a learning rate reduced bya factor of 5. to mitigate error propagation andmake the mt system more robust to asr errors,similarly to (di gangi et al., 2019a) ﬁne tuning isrun on the concatenation of human and automatictranscripts of must-c, both paired with manualtranslations..a.2 direct approach.
our direct model (gaido et al., 2020) uses the samearchitecture of the english asr model describedin §a.1, but it has 11 transformer encoder layers(instead of 8) and 4 transformer decoder layers (in-stead of 6) for a total of 64m parameters.
the stmodel’s encoder is initialized with the encoder of.
the asr model (bansal et al., 2019), with the miss-ing layers initialized randomly.
the st decoder isalso initialized randomly..the training settings and the data augmentationmethods employed for the direct st model arethe same described in section a.1 for the asrcomponent of the cascade system.
in addition, weperformed synthetic data generation, by automati-cally translating the english transcripts of the asrtraining corpora (jia et al., 2019).
furthermore, wetransfer knowledge from mt through knowledgedistillation (hinton et al., 2015).
knowledge distil-lation is performed from a teacher mt model byoptimizing the kl divergence between the distribu-tions produced by the teacher and the student stmodel being trained (liu et al., 2019).
the teachermt model is trained on the opus datasets (tiede-mann, 2016) and is a plain transformer with 16 at-tention heads and 1024 features in encoder/decoderembeddings, resulting into 212m parameters..the direct st model is trained in two consecu-tive steps.
first, it is optimized using kd.
then,the resulting model is ﬁne-tuned on label-smoothedcross entropy (szegedy et al., 2016).
the trainingset is composed of the same corpora used for theasr model, more precisely: i) the st corpora andii) the synthetic datasets derived from the asrcorpora..the st model is fed with the input utteranceand a token representing the type of the target data,which can be: i) human reference translations (forthe st corpora), or ii) translations generated bythe mt model fed with true case transcriptionswith punctuation, and iii) translations generatedby the mt model fed with lower-cased transcrip-tions without punctuation (for the asr corpora).
at inference time, the token “human reference” isalways used to generate the translations.
the tokenis added to the features extracted from the audiobefore they are passed to the encoder (di gangiet al., 2019c)..all trainings were performed on 8 k80 gpus.
the training of each direct model lasted 10 days,while the asr and mt pre-trainings 6 days each..the source code19 implemented to build these.
models is based on fairseq (ott et al., 2019)..19https://github.com/mgaido91/.
fbk-fairseq-st.2886b post-editing guidelines.
c tool for automatic error.
classiﬁcation.
the tool used for the automatic analysis of lin-guistic errors (section 6.1) is downloadable atwit3.fbk.eu/2016-02.
it is a modiﬁed version ofthe tercom script, 20 which requires the lemmatizedversions of both systems’ outputs and post-edits.
to lemmatize the data we used the treetagger.21.
in this task you are presented with (i) 550 audiosegments that are recordings of portions of differentenglish ted talks, (ii) their transcripts, and (iii)corresponding automatic translations..starting from the original audio recording andits corresponding transcript (done by ted volun-teer translators), you are asked to post-edit eachgiven automatic translation by applying the mini-mal edits required to transform the system outputinto a ﬂuent sentence with the same meaning as theaudio/transcript..while post-editing, remember the following.
guidelines:.
• we noticed that some audio player softwareapplications cut the beginning or the end ofthe audio segments.
if you notice some audio-transcript out-of-sync, please try another au-dio player or inform us about the problem..• the audio should be your ﬁrst source of infor-mation, while transcripts are given for yourconvenience.
it could happen that the tran-script is not faithful to the spoken original: inthese cases you should not consider the tran-script and refer to the audio only..• some transcripts contain the name or initialsof the speaker (typically followed by colons).
please don’t add this information into thein general,sentence you are post-editing.
don’t include in your post-edit any text thatis not present in the audio (e.g.
explanationof acronyms, disambiguation of pronouns),even though this information could ease theunderstanding of the sentence..• the post-edited sentence is intended as atranslation of spoken language.
also, de-pending on the style of the source languagetalk, you can use the corresponding style inthe target language (e.g.
if the talk uses afriendly/colloquial style you can use informalwords too)..• the focus is the correctness of the single sen-tence within the given context, not the con-sistency of a group of sentences.
hence, sur-rounding segments should be used to under-stand the context but not to enforce consis-tency on the use of terms.
in particular, dif-ferent but correct translations of terms acrosssegments should not be corrected..20www.cs.umd.edu/˜snover/tercom21www.cis.uni-muenchen.de/˜schmid/.
tools/treetagger.
2887