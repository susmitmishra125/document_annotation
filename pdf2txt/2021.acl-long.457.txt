smedbert: a knowledge-enhanced pre-trained language model withstructured semantics for medical text miningtaolin zhang1,2,3, zerui cai4, chengyu wang2, minghui qiu2bite yang5, xiaofeng he4,6∗1 school of software engineering, east china normal university 2 alibaba group3 shanghai key laboratory of trsustworthy computing4 school of computer science and technology, east china normal university 5 dxy6 shanghai research institute for intelligent autonomous systemszhangtl0519@gmail.com, zrcai flow@126.com, yangbt@dxy.cn{chengyu.wcy, minghui.qmh}@alibaba-inc.com, hexf@cs.ecnu.edu.cn.
abstract.
recently, the performance of pre-trained lan-guage models (plms) has been signiﬁcantlyimproved by injecting knowledge facts to en-hance their abilities of language understanding.
for medical domains, the background knowl-edge sources are especially useful, due to themassive medical terms and their complicatedrelations are difﬁcult to understand in text.
inthis work, we introduce smedbert, a med-ical plm trained on large-scale medical cor-pora, incorporating deep structured semanticsknowledge from neighbours of linked-entity.
in smedbert, the mention-neighbour hybridattention is proposed to learn heterogeneous-entity information, which infuses the semanticrepresentations of entity types into the homo-geneous neighbouring entity structure.
apartfrom knowledge integration as external fea-tures, we propose to employ the neighbors oflinked-entities in the knowledge graph as addi-tional global contexts of text mentions, allow-ing them to communicate via shared neighbors,thus enrich their semantic representations.
ex-periments demonstrate that smedbert signif-icantly outperforms strong baselines in variousknowledge-intensive chinese medical tasks.
italso improves the performance of other taskssuch as question answering, question matchingand natural language inference.1.
1.introduction.
pre-trained language models (plms) learn effec-tive context representations with self-supervisedtasks, spotlighting in various nlp tasks (wanget al., 2019a; nan et al., 2020; liu et al., 2020a).
inaddition, knowledge-enhanced plms (keplms)(zhang et al., 2019; liu et al., 2020b; wang et al.,2019b) further beneﬁt language understanding by.
∗corresponding author.
1the code and pre-trained models will be available at.
https://github.com/matnlp/smedbert..figure 1: example of neighboring entity informationin medical text.
(best viewed in color).
grounding these plms with high-quality, human-curated knowledge facts, which are difﬁcult to learnfrom raw texts..in the literatures, a majority of keplms (zhanget al., 2020a; hayashi et al., 2020; sun et al.,2020) inject information of entities correspondingto mention-spans from knowledge graphs (kgs)into contextual representations.
however, thosekeplms only utilize linked-entity in the kgs asauxiliary information, which pay little attentionto the neighboring structured semantics informa-tion of the entity linked with text mentions.
inthe medical context, there exist complicated do-main knowledge such as relations and medical factsamong medical terms (rotmensch et al., 2017; liet al., 2020), which are difﬁcult to model usingprevious approaches.
to address this issue, weconsider leveraging structured semantics knowl-edge in medical kgs from the two aspects.
(1)rich semantic information from neighboring struc-tures of linked-entities, such as entity types andrelations, are highly useful for medical text under-standing.
as in figure 1, “新型冠状病毒” (novelcoronavirus) can be the cause of many diseases,such as “肺炎” (pneumonia) and “呼吸综合征”.
proceedingsofthe59thannualmeetingoftheassociationforcomputationallinguisticsandthe11thinternationaljointconferenceonnaturallanguageprocessing,pages5882–5893august1–6,2021.©2021associationforcomputationallinguistics5882(cid:3081)(cid:4486)(cid:11250)( sore throat)(cid:7142)(cid:3521)(cid:2006)(cid:10476)(cid:11259)(cid:8712)(novel coronavirus)symptom cause of disease(cid:2738)(cid:2670)(cid:5973)(cid:7689)(respiratory infection)disease(cid:2738)(cid:2670)(cid:13618)(cid:2622)(cid:5559)(respiratory syndrome)symptom-diseasecause-diseaseentity typerelationcovid-19(cid:3)(cid:44)(cid:81)(cid:83)(cid:88)(cid:87)(cid:3)(cid:55)(cid:72)(cid:91)(cid:87)(cid:29) (cid:17633)(cid:1417)(cid:2567)(cid:10019)(cid:952)(cid:3081)(cid:4486)(cid:11250)(cid:952)(cid:14255)(cid:9009)(cid:7269)(cid:5973)(cid:7689)(cid:7142)(cid:3521)(cid:2006)(cid:10476)(cid:11259)(cid:8712)(covid-19)(cid:11450)(cid:11261)(cid:10476)(cid:574) (fever, sore throat, and diarrhea are symptoms of novel coronavirus (covid-19).
)(cid:49)(cid:72)(cid:76)(cid:74)(cid:75)(cid:69)(cid:82)(cid:85)(cid:76)(cid:81)(cid:74)(cid:3)(cid:40)(cid:81)(cid:87)(cid:76)(cid:87)(cid:92)(cid:3)(cid:73)(cid:85)(cid:82)(cid:80)(cid:3)(cid:39)(cid:59)(cid:60)(cid:16)(cid:46)(cid:42)(cid:47)(cid:76)(cid:81)(cid:78)(cid:72)(cid:71)(cid:16)(cid:40)(cid:81)(cid:87)(cid:76)(cid:87)(cid:92)(cid:14064)(cid:9924)(pneumonia)(cid:5973)(cid:7689)(cid:12295)(infectious department)symptom-symptommedical department (cid:2567)(cid:10013)(fever)cause-department(respiratory syndrome).
2(2) additionally, weleverage neighbors of linked-entity as global “con-texts” to complement plain-text contexts used in(mikolov et al., 2013a; pennington et al., 2014).
the structure knowledge contained in neighbour-ing entities can act as the “knowledge bridge” be-tween mention-spans, facilitating the interaction ofdifferent mention representations.
hence, plmscan learn better representations for rare medicalterms..in this paper, we introduce smedbert, a ke-plm pre-trained over large-scale medical corporaand medical kgs.
to the best of our knowledge,smedbert is the ﬁrst plm with structured se-mantics knowledge injected in the medical do-main.
speciﬁcally, the contributions of smedbertmainly include two modules:mention-neighbor hybrid attention: we fusethe embeddings of the node and type of linked-entity neighbors into contextual target mention rep-resentations.
the type-level and node-level atten-tions help to learn the importance of entity typesand the neighbors of linked-entity, respectively, inorder to reduce the knowledge noise injected intothe model.
the type-level attention transforms thehomogeneous node-level attention into a heteroge-neous learning process of neighboring entities.
mention-neighbor context modeling: we pro-pose two novel self-supervised learning tasks forpromoting interaction between mention-span andcorresponding global context, namely maskedneighbor modeling and masked mention modeling.
the former enriches the representations of “con-text” neighboring entities based on the well trained“target word” mention-span, while the latter focuseson gathering those information back from neighbor-ing entities to the masked target like low-frequencymention-span which is poorly represented (turianet al., 2010)..in the experiments, we compare smedbertagainst various strong baselines, including main-stream keplms pre-trained over our medical re-sources.
the underlying medical nlp tasks in-clude: named entity recognition, relation extrac-tion, question answering, question matching andnatural language inference.
the results show thatsmedbert consistently outperforms all the base-lines on these tasks..2although we focus on chinese medical plms here.
theproposed method can be easily adapted to other languages,which is beyond the scope of this work..2 related work.
plms in the open domain.
plms have gainedmuch attention recently, proving successful forboosting the performance of various nlp tasks(qiu et al., 2020).
early works on plms focuson feature-based approaches to transform wordsinto distributed representations (collobert and we-ston, 2008; mikolov et al., 2013b; pennington et al.,2014; peters et al., 2018).
bert (devlin et al.,2019) (as well as its robustly optimized versionroberta (liu et al., 2019b)) employs bidirec-tional transformer encoders (vaswani et al., 2017)and self-supervised tasks to generate context-awaretoken representations.
further improvement of per-formances mostly based on the following threetypes of techniques, including self-supervised tasks(joshi et al., 2020), transformer encoder architec-tures (yang et al., 2019) and multi-task learning(liu et al., 2019a).
knowledge-enhanced plms.
as existing bert-like models only learn knowledge from plain cor-pora, various works have investigated how to in-corporate knowledge facts to enhance the lan-guage understanding abilities of plms.
keplmsare mainly divided into the following three types.
(1) knowledge-enhanced by entity embedding:ernie-thu (zhang et al., 2019) and knowbert(peters et al., 2019) inject linked-entity as hetero-geneous features learned by kg embedding algo-rithms such as transe (bordes et al., 2013).
(2)knowledge-enhanced by entity description: e-bert (zhang et al., 2020a) and kepler (wanget al., 2019b) add extra description text of entities toenhance semantic representation.
(3) knowledge-enhanced by triplet sentence: k-bert (liu et al.,2020b) and colake (sun et al., 2020) converttriplets into sentences and insert them into the train-ing corpora without pre-trained embedding.
pre-vious studies on kg embedding (nguyen et al.,2016; schlichtkrull et al., 2018) have shown thatutilizing the surrounding facts of entity can obtainmore informative embedding, which is the focus ofour work.
plms in the medical domain.
plms in the med-ical domain can be generally divided into threecategories.
(1) biobert (lee et al., 2020), blue-bert (peng et al., 2019), scibert (beltagy et al.,2019) and clinicalbert (huang et al., 2019) ap-ply continual learning on medical domain texts,such as pubmed abstracts, pmc full-text articlesand mimic-iii clinical notes.
(2) pubmedbert.
5883figure 2: model overview of smedbert.
the left part is our model architecture and the right part is the details ofour model including hybrid attention network and mention-neighbor context modeling pre-training tasks..(gu et al., 2020) learns weights from scratch usingpubmed data to obtain an in-domain vocabulary,alleviating the out-of-vocabulary (oov) problem.
this training paradigm needs the support of large-scale domain data and resources.
(3) some otherplms use domain self-supervised tasks for pre-training.
for example, mc-bert (zhang et al.,2020b) masks chinese medical entities and phrasesto learn complex structures and concepts.
disease-bert (he et al., 2020) leverages the medical termsand its category as the labels to pre-train the model.
in this paper, we utilize both domain corpora andneighboring entity triplets of mentions to enhancethe learning of medical language representations..3 the smedbert model.
3.1 notations and model overview.
in the plm, we denote the hidden feature of eachtoken {w1, ..., wn } as {h1, h2, ..., hn } where nis the maximum input sequence length and the totalnumber of pre-training samples as m .
let e bethe set of mention-span em in the training corpora.
furthermore, the medical kg consists of the enti-ties set e and the relations set r. the triplet setis s = {(h, r, t) | h ∈ e, r ∈ r, t ∈ e}, where his the head entity with relation r to the tail entityt. the embeddings of entities and relations trainedon kg by transr (lin et al., 2015) are representedas γent and γrel, respectively.
the neighboringentity set recalled from kg by em is denoted asnem = {e1m} where k is the thresholdof our pepr algorithm.
we denote the number of.
m, ..., ek.
m, e2.
entities in the kg as z. the dimensions of the hid-den representation in plm and the kg embeddingsare d1 and d2, respectively..the main architecture of the our model is shownin figure 2. smedbert mainly includes threecomponents: (1) top-k entity sorting determinewhich k neighbour entities to use for each men-tion.
(2) mention-neighbor hybrid attention aimsto infuse the structured semantics knowledge intoencoder layers, which includes type attention,node attention and gated position infusion module.
(3) mention-neighbor context modeling includesmasked neighbor modeling and masked mentionmodeling aims to promote mentions to leverageand interact with neighbour entities..3.2 top-k entity sorting.
previous research shows that simple neighboringentity expansion may induce knowledge noises dur-ing plm training (wang et al., 2019a).
in orderto recall the most important neighboring entity setfrom the kg for each mention, we extend the per-sonalized pagerank (ppr) (page et al., 1999) algo-rithm to ﬁlter out trivial entities.
3 recall that the it-erative process in ppr is vi = (1−α)a·vi−1+αpwhere a is the normalized adjacency matrix, αis the damping factor, p is uniformly distributedjump probability vector, and v is the iterative scorevector for each entity..pepr speciﬁcally focuses on learning the weightfor the target mention span in each iteration.
it.
3we name our algorithm to be personalized entity pager-.
ank, abbreviated as pepr..5884(cid:68)(cid:437)(cid:367)(cid:410)(cid:349)(cid:882)(cid:44)(cid:286)(cid:258)(cid:282)(cid:3)(cid:94)(cid:286)(cid:367)(cid:296)(cid:882)(cid:4)(cid:410)(cid:410)(cid:286)(cid:374)(cid:410)(cid:349)(cid:381)(cid:374)(cid:38)(cid:286)(cid:286)(cid:282)(cid:3)(cid:38)(cid:381)(cid:396)(cid:449)(cid:258)(cid:396)(cid:282)(cid:3)(cid:62)(cid:258)(cid:455)(cid:286)(cid:396)(cid:100)(cid:381)(cid:364)(cid:286)(cid:374)(cid:3)(cid:47)(cid:374)(cid:393)(cid:437)(cid:410)(cid:3)(cid:68)(cid:454)(cid:100)(cid:882)(cid:28)(cid:374)(cid:272)(cid:381)(cid:282)(cid:286)(cid:396)(cid:60)(cid:882)(cid:28)(cid:374)(cid:272)(cid:381)(cid:282)(cid:286)(cid:396)(cid:69)(cid:454)(cid:68)(cid:437)(cid:367)(cid:410)(cid:349)(cid:882)(cid:44)(cid:286)(cid:258)(cid:282)(cid:3)(cid:94)(cid:286)(cid:367)(cid:296)(cid:882)(cid:4)(cid:410)(cid:410)(cid:286)(cid:374)(cid:410)(cid:349)(cid:381)(cid:374)(cid:44)(cid:455)(cid:271)(cid:396)(cid:349)(cid:282)(cid:3)(cid:4)(cid:410)(cid:410)(cid:286)(cid:374)(cid:410)(cid:349)(cid:381)(cid:374)(cid:3)(cid:69)(cid:286)(cid:410)(cid:449)(cid:381)(cid:396)(cid:364)(cid:44)(cid:286)(cid:410)(cid:286)(cid:396)(cid:381)(cid:336)(cid:286)(cid:374)(cid:286)(cid:381)(cid:437)(cid:400)(cid:3)(cid:47)(cid:374)(cid:296)(cid:381)(cid:396)(cid:373)(cid:258)(cid:410)(cid:349)(cid:381)(cid:374)(cid:3)(cid:38)(cid:437)(cid:400)(cid:349)(cid:381)(cid:374)(cid:87)(cid:396)(cid:286)(cid:882)(cid:410)(cid:396)(cid:258)(cid:349)(cid:374)(cid:349)(cid:374)(cid:336)(cid:3)(cid:100)(cid:258)(cid:400)(cid:364)(cid:400)(cid:68)(cid:62)(cid:68)(cid:3)(cid:68)(cid:286)(cid:374)(cid:410)(cid:349)(cid:381)(cid:374)(cid:882)(cid:374)(cid:286)(cid:349)(cid:336)(cid:346)(cid:271)(cid:381)(cid:396)(cid:3)(cid:18)(cid:381)(cid:374)(cid:410)(cid:286)(cid:454)(cid:410)(cid:3)(cid:68)(cid:381)(cid:282)(cid:286)(cid:367)(cid:349)(cid:374)(cid:336)(cid:894)(cid:258)(cid:895)(cid:94)(cid:68)(cid:286)(cid:282)(cid:17)(cid:28)(cid:90)(cid:100)(cid:3)(cid:4)(cid:396)(cid:272)(cid:346)(cid:349)(cid:410)(cid:286)(cid:272)(cid:410)(cid:437)(cid:396)(cid:286)(cid:60)(cid:374)(cid:381)(cid:449)(cid:367)(cid:286)(cid:282)(cid:336)(cid:286)(cid:3)(cid:47)(cid:374)(cid:393)(cid:437)(cid:410)(cid:68)(cid:286)(cid:282)(cid:349)(cid:272)(cid:258)(cid:367)(cid:3)(cid:60)(cid:39)(cid:7136)(cid:2000)(cid:3)(cid:3)(cid:3)(cid:3)(cid:3)(cid:3)(cid:3)(cid:3)(cid:3)(cid:3)(cid:3)(cid:3)(cid:3)(cid:11253)(cid:8706)(cid:3)(cid:708)(cid:18)(cid:75)(cid:115)(cid:47)(cid:24)(cid:882)(cid:1005)(cid:1013)(cid:709)(cid:69)(cid:286)(cid:349)(cid:336)(cid:346)(cid:271)(cid:381)(cid:396)(cid:349)(cid:374)(cid:336)(cid:28)(cid:374)(cid:410)(cid:349)(cid:410)(cid:349)(cid:286)(cid:400)(cid:100)(cid:455)(cid:393)(cid:286)(cid:69)(cid:381)(cid:282)(cid:286)(cid:69)(cid:381)(cid:282)(cid:286)(cid:87)(cid:28)(cid:87)(cid:90)(cid:100)(cid:455)(cid:393)(cid:286)(cid:74)(cid:202)(cid:44)(cid:455)(cid:271)(cid:396)(cid:349)(cid:282)(cid:3)(cid:4)(cid:410)(cid:410)(cid:856)(cid:3)(cid:920)(cid:3)(cid:47)(cid:374)(cid:296)(cid:381)(cid:396)(cid:856)(cid:3)(cid:38)(cid:437)(cid:400)(cid:349)(cid:381)(cid:374)(cid:3)(cid:68)(cid:381)(cid:282)(cid:437)(cid:367)(cid:286)(cid:400)(cid:202)(cid:68)(cid:286)(cid:374)(cid:410)(cid:349)(cid:381)(cid:374)(cid:882)(cid:374)(cid:286)(cid:349)(cid:336)(cid:346)(cid:271)(cid:381)(cid:396)(cid:3)(cid:18)(cid:381)(cid:374)(cid:410)(cid:286)(cid:454)(cid:410)(cid:3)(cid:68)(cid:381)(cid:282)(cid:286)(cid:367)(cid:349)(cid:374)(cid:336)(cid:3843)(cid:68)(cid:258)(cid:400)(cid:364)(cid:286)(cid:282)(cid:3)(cid:69)(cid:286)(cid:349)(cid:336)(cid:346)(cid:271)(cid:381)(cid:396)(cid:3)(cid:68)(cid:381)(cid:282)(cid:286)(cid:367)(cid:349)(cid:374)(cid:336)(cid:20)(cid:85)(cid:22)(cid:85)(cid:21)(cid:85)(cid:364)(cid:374)(cid:381)(cid:449)(cid:367)(cid:286)(cid:282)(cid:336)(cid:286)(cid:3)(cid:272)(cid:381)(cid:374)(cid:410)(cid:286)(cid:454)(cid:410)(cid:364)(cid:374)(cid:381)(cid:449)(cid:367)(cid:286)(cid:282)(cid:336)(cid:286)(cid:3)(cid:272)(cid:381)(cid:374)(cid:410)(cid:286)(cid:454)(cid:410)(cid:23)(cid:85)(cid:896)(cid:68)(cid:4)(cid:94)(cid:60)(cid:897)(cid:312)(cid:68)(cid:258)(cid:400)(cid:364)(cid:286)(cid:282)(cid:3)(cid:68)(cid:286)(cid:374)(cid:410)(cid:349)(cid:381)(cid:374)(cid:3)(cid:68)(cid:381)(cid:282)(cid:286)(cid:367)(cid:349)(cid:374)(cid:336)(cid:87)(cid:62)(cid:68)(cid:400)(cid:410)(cid:381)(cid:364)(cid:286)(cid:374)(cid:400)(cid:20)(cid:80)(cid:79)(cid:22)(cid:80)(cid:79)(cid:21)(cid:80)(cid:79)assigns the span em a higher jump probability 1 inp with the remaining as 1z .
it also uses the entityfrequency to initialize the score vector v :.
vem =.
(cid:40) temt1m.em ∈ eem /∈ e.(1).
attention to capture the different semantic inﬂu-ences from neighboring entities to the target men-tion span and reduce the effect of noises.
we cal-culate the entity node attention using the mention-span representation h(cid:48)em and neighboring entitiesrepresentation hei.
with entity type τ as:.
m.where t is the sum of frequencies of all entities.
tem is the frequency of em in the corpora.
aftersorting, we select the top-k entity set nem..3.3 mention-neighbor hybrid attention.
besides the embeddings of neighboring entities,smedbert integrates the type information of med-ical entities to further enhance semantic representa-tions of mention-span..3.3.1 neighboring entity type attention.
different types of neighboring entities may havedifferent impacts.
given a speciﬁc mention-spanem, we compute the neighboring entity type atten-tion.
concretely, we calculate hidden representa-tion of each entity type τ as hτ = (cid:80)hei.
eτm are neighboring entities of em with the same(cid:0)eitype τ and heim.(cid:1) ∈ rd2..= γent.
m∈eτeim.m.m.h(cid:48)em = ln (σ (fsp (hi, .
.
.
, hj) wbe)).
(2).
where fsp is the self-attentive pooling (lin et al.,2017) to generate the mention-span representationhem ∈ rd1 and the (hi, hi+1, .
.
.
, hj) is the hid-den representation of tokens (wi, wi+1, .
.
.
, wj) inem ∈ rd2mention-span em trained by plms.
h(cid:48)is obtained by σ(·) non-linear activation functiongelu (hendrycks and gimpel, 2016) and thelearnable projection matrix wbe ∈ rd1×d2.
ln isthe layernorm function (ba et al., 2016).
then, wecalculate the each type attention weight using thetype representation hτ ∈ rd2 and the transformedmention-span representation h(cid:48).
em:.
τ = tanh (cid:0)h(cid:48)α(cid:48).
emwt + hτ wt(cid:48).
(cid:1) wa.
(3).
where wt ∈ rd2×d2, wt(cid:48) ∈ rd2×d2 and wa ∈rd2×1.
finally, the neighboring entity type atten-tion weights ατ are obtained by normalizing theattention score α(cid:48).
τ among all entity types t ..β(cid:48)emeim.=.
βemei.
m.=.
(cid:80).
(cid:0)h(cid:48).
emwq.
(cid:1)t.wk.
m.(cid:1) (cid:0)hei√d2(cid:16)β(cid:48)emeim(cid:16).
exp.
(cid:17).
(cid:17).
β(cid:48).
emeim.exp.
eim∈nem.
ατ.
(4).
(5).
where wq ∈ rd2×d2 and wk ∈ rd2×d2 are theattention weight matrices..the representations of all neighboring entities in.
nem are aggregated to ¯h(cid:48).
em ∈ rd2:(cid:0)hei.
m.m.βemei.
wv + bv.
(cid:1).
(6).
(cid:98)h(cid:48)em =.
(cid:88).
¯h(cid:48)em = ln.
eim∈nem(cid:16)(cid:98)h(cid:48)em +.
(cid:16).
σ.
(cid:16)(cid:98)h(cid:48)emwl1 + bl1.
(cid:17).
wl2.
(cid:17)(cid:17).
(7).
where wv ∈ rd2×d2, wl1 ∈ rd2×4d2, wl2 ∈r4d2×d2.
bv ∈ rd2 and bl1 ∈ r4d2 are the biasvectors.
¯h(cid:48)em is the mention-neighbor representa-tion from hybrid attention module..3.3.3 gated position infusionknowledge-injected representations may divert thetexts from its original meanings.
we further reduceknowledge noises via gated position infusion:.
= σ (cid:0)(cid:2)¯h(cid:48)h(cid:48)emf(cid:101)h(cid:48)emf.
em (cid:107) h(cid:48)= ln (h(cid:48).
emf.
em.
(cid:3) wmf + bmfwbp + bbp).
(cid:1).
(8).
(9).
emf.
where wmf ∈ r2d2×2d2, wbp ∈ r2d2×d1, bmf ∈∈ r2d2 is the span-levelr2d2, bbp ∈ rd1.
h(cid:48)infusion representation.
“(cid:107)” means concatenation∈ rd1 is the ﬁnal knowledge-operation.
(cid:101)h(cid:48)injected representation for mention em.
we gener-ate the output token representation hif by 4:(cid:17).
(cid:16)(cid:16)(cid:104).
(cid:105)(cid:17).
emf.
hi (cid:107) (cid:101)h(cid:48).
emf.
wug + bug.
(10).
gi = tanh(cid:16)(cid:16)(cid:104).
hif = σ.hi (cid:107) gi ∗ (cid:101)h(cid:48).
emf.
(cid:105)(cid:17).
(cid:17).
wex + bex.
+ hi(11).
3.3.2 neighboring entity node attention.
apart from entity type information, differentneighboring entities also have different inﬂuences.
speciﬁcally, we devise the neighboring entity node.
where wug, wex ∈ r2d1×d1.
bug, bex ∈ rd1.
“∗”means element-wise multiplication..4we ﬁnd that restricting the knowledge infusion position.
to tokens is helpful to improve performance..58853.4 mention-neighbor context modeling.
to fully exploit the structured semantics knowl-edge in kg, we further introduce two novel self-supervised pre-training tasks, namely maskedneighbor modeling (mnem) and masked men-tion modeling (mmem)..3.4.1 masked neighbor modelingformally, let r be the relation between the mention-span em and a neighboring entity ei.
m:.
hmf = ln (σ (fsp (hif , .
.
.
, hjf ) wsa)).
(12).
where hmfis the mention-span hidden fea-tures based on the tokens hidden representation(cid:0)hif , h(i+1)f , .
.
.
, hjf(cid:1).
hr = γrel (r) ∈ rd2 isthe relation r representation and wsa ∈ rd1×d2 isa learnable projection matrix.
the goal of mnemis leveraging the structured semantics in surround-ing entities while reserving the knowledge of re-lations between entities.
considering the objectfunctions of skip-gram with negative sampling(sgns) (mikolov et al., 2013a) and score func-tion of transr (lin et al., 2015):.
ls = log fs(w, c) + k · ecn∼pd [log fs(w, −cn)](13).
ftr(h, r, t) =(cid:107) hmr + r − tmr (cid:107).
(14).
where the w in ls is the target word of context c. fsis the compatibility function measuring how wellthe target word is ﬁtted into the context.
inspired bysgns, following the general energy-based frame-work (lecun et al., 2006), we treat mention-spansin corpora as “target words”, and neighbors of cor-responding entities in kg as “contexts” to pro-vide additional global contexts.
we employ thesampled-softmax (jean et al., 2015) as the crite-rion lmnem for the mention-span em:.
log.
(cid:88).
nem.
exp(fs(θ))exp(fs(θ)) + k · een∼q(en)[exp(fs(θ(cid:48)))](15).
m), ei.
where θ denotes the triplet (em, r, eim ∈ nem.
θ(cid:48) is the negative triplets (em, r, en), and en is neg-ative entity sampled with q(eim) detailed in ap-pendix b. to keep the knowledge of relations be-tween entities, we deﬁne the compatibility functionas:.
fs.
(cid:0)em, r, ei.
m.(cid:1) =.
hmf mr + hr||hmf mr + hr||.
·.
m.(hei||hei.
mr)tmr||.
m.µ.
(16).
where µ is a scale factor.
assuming the norms ofboth hmf mr + hr and hei.
mr are 1,we have:.
m.m.m.fs.
(cid:0)em, r, ei.
(cid:1) = µ ⇐⇒ ftr(hmf , hr, hei.)
= 0(17)which indicates the proposed fs is equivalence withftr.
because | henmr | needs to be calculated foreach en, the computation of the score function fsis costly.
hence, we transform part of the formulafs as follows:.
(hmf mr + hr) · (henmr)t =(cid:21)t(cid:20) mr(cid:2) hmfhr1 (cid:3) mpr= (cid:2) hmf.
(cid:21) (cid:20) mrhr(cid:2) hen.
1 (cid:3).
0 (cid:3)t.(cid:2) hen.
0 (cid:3)t.(18)in this way, we eliminate computation of transform-ing each hen.
finally, to compensate the offset in-troduced by the negative sampling function q(eim)(jean et al., 2015), we complement fs(em, r, eim)as:.
(cid:2) hmf(cid:107) (cid:2) hmf.
1 (cid:3)mpr1 (cid:3)mpr (cid:107).
(cid:2) hei(cid:107) hei.
m.·.
m.0 (cid:3)(cid:107).
µ−µ log q(ei.
m).
(19).
3.4.2 masked mention modeling.
in contrast to mnem, mmem transfers the seman-tic information in neighboring entities back to themasked mention em..ym = ln (σ (fsp (hip, .
.
.
, hjp) wsa)).
(20).
where ym is the ground-truth representation of emand hip = γp(wi) ∈ rd2.
γp is the pre-trainedembedding of bert in our medical corpora.
themention-span representation obtained by our modelis hmf .
for a sample s, the loss of mmem lmmemis calculated via mean-squared error:.
lmmem =.
(cid:107) hmif − ymi (cid:107)2.
(21).
ms(cid:88).
mi.
where ms is the set of mentions of sample s..3.5 training objective.
in smedbert, the training objectives mainly con-sist of three parts, including the self-supervisedloss proposed in previous works and the mention-neighbor context modeling loss proposed in ourwork.
our model can be applied to medical textpre-training directly in different languages as long.
5886as high-quality medical kgs can be obtained.
thetotal loss is as follows:.
ltotal = lex + λ1lmnem + λ2lmmem (22).
where lex is the sum of sentence-order predic-tion (sop) (lan et al., 2020) and masked languagemodeling.
λ1 and λ2 are the hyperparameters..4 experiments.
4.1 data source.
pre-training data.
the pre-training corpora afterpre-processing contains 5,937,695 text segmentswith 3,028,224,412 tokens (4.9 gb).
the kgs em-bedding trained by transr (lin et al., 2015) ontwo trusted data sources, including the symptom-in-chinese from openkg5 and dxy-kg 6 contain-ing 139,572 and 152,508 entities, respectively.
thenumber of triplets in the two kgs are 1,007,818and 3,764,711. the pre-training corpora and thekgs are further described in appendix a.1.
task data.
we use four large-scale datasets inchineseblue (zhang et al., 2020b) to evaluateour model, which are benchmark of chinese med-ical nlp tasks.
additionally, we test models onfour datasets from real application scenarios pro-vided by dxy company 7 and chip 8, i.e., namedentity recognition (dxy-ner), relation extrac-tion (dxy-re, chip-re) and question answer(webmedqa (he et al., 2019)).
for other informa-tion of the downstream datasets, we refer readersto appendix a.2..model.
d1.
d2.
d3.
sgns-char-medsgns-word-medglove-char-medglove-word-med.
27.21% 27.16% 21.72%24.64% 24.95% 20.37%27.24% 27.12% 21.91%24.41% 23.89% 20.56%.
bert-openbert-wwm-openroberta-open.
29.79% 29.41% 21.83%29.75% 29.55% 21.97%30.84% 30.56% 21.98%.
mc-bertbiobert-zhernie-medknowbert-med.
30.63% 30.34% 22.65%30.84% 30.69% 22.71%30.97% 30.78% 22.99%30.95% 30.77% 23.07%.
smedbert.
31.81% 32.14% 24.08%.
table 1: results of unsupervised semantic similaritytask.
“med” refers to models continually pre-trained onmedical corpora, and “open” means open-domain cor-pora.
“char’ and “word” refer to the token granularityof input samples..2020b) is pre-trained over a chinese medical cor-pora via masking different granularity tokens.
wealso pre-train bert using our corpora, denoted asbiobert-zh.
keplms: we employ two sota keplms con-tinually pre-trained on our medical corpora as ourbaseline models, including ernie-thu (zhanget al., 2019) and knowbert (peters et al., 2019).
for a fair comparison, keplms use other addi-tional resources rather than the kg embedding areexcluded (see section 2), and all the baseline ke-plms are injected by the same kg embedding..the detailed parameter settings and training pro-.
cedure are in appendix b..4.2 baselines.
4.3.intrinsic evaluation.
in this work, we compare smedbert with generalplms, domain-speciﬁc plms and keplms withknowledge embedding injected, pre-trained on ourchinese medical corpora:general plms: we use three chinese bert-stylemodels, namely bert-base (devlin et al., 2019),bert-wwm (cui et al., 2019) and roberta (liuet al., 2019b).
all the weights are initialized from(cui et al., 2020).
domain-speciﬁc plms: as very few plms in thechinese medical domain are available, we considerthe following models.
mc-bert (zhang et al.,.
5http://www.openkg.cn/dataset/.
symptom-in-chinese.
6https://portal.dxy.cn/7https://auth.dxy.cn/accounts/login8http://www.cips-chip.org.cn:8088/home.
to evaluate the semantic representation ability ofsmedbert, we design an unsupervised semanticsimilarity task.
speciﬁcally, we extract all entitiespairs with equivalence relations in kgs as positivepairs.
for each positive pair, we use one of theentity as query entity while the other as positivecandidate, which is used to sample other entitiesas negative candidates.
we denote this dataset asd1.
besides, the entities in the same positive pairoften have many neighbours in common.
we selectpositive pairs with large proportions of commonneighbours as d2.
additionally, to verify the abil-ity of smedbert of enhancing the low-frequencymention representation, we extract all positive pairsthat with at least one low-frequency mention as d3.
there are totally 359,358, 272,320 and 41,583 sam-ples for d1, d2, d3 respectively.
we describe the.
5887named entity recognition.
relation extraction.
model.
cmedqaner.
dxy-ner.
average chip-re.
dxy-re.
average.
dev.
test.
dev.
test.
test.
test.
dev.
test.
test.
bert-openbert-wwm-openrobert-open.
80.69% 83.12% 79.12% 79.03% 81.08%80.52% 83.07% 79.48% 79.29% 81.18%80.92% 83.29% 79.27% 79.33% 81.31%.
biobert-zhmc-bertknowbert-medernie-med.
80.72% 83.38% 79.52% 79.45% 81.42%81.02% 83.46% 79.79% 79.59% 81.53%81.29% 83.75% 80.86% 80.44% 82.10%81.22% 83.87% 80.82% 80.87% 82.37%.
85.86%86.01%86.19%.
86.12%86.09%86.27%86.25%.
94.18% 94.13% 90.00%94.35% 94.38% 90.20%94.64% 94.66% 90.43%.
94.54% 94.64% 90.38%94.74% 94.73% 90.41%95.05% 94.97% 90.62%94.98% 94.91% 90.58%.
smedbert.
82.23% 84.75% 83.06% 82.94% 83.85% 86.95% 95.73% 95.89% 91.42%.
table 2: performance of named entity recognition (ner) and relation extraction (re) tasks in terms of f1.
thedevelopment data of chip-re is unreleased in public dataset..question answering.
question matching natural lang.
infer..model.
cmedqa.
webmedqa.
average.
cmedqq.
cmednli.
dev.
test.
dev.
test.
test.
dev.
test.
dev.
test.
bert-openbert-wwm-openrobert-open.
72.99% 73.82% 77.20% 79.72% 76.77% 86.74% 86.72% 95.52% 95.66%72.03% 72.96% 77.06% 79.68% 76.32% 86.98% 86.82% 95.53% 95.78%72.22% 73.18% 77.18% 79.57% 76.38% 87.24% 86.97% 95.87% 96.11%.
biobert-zhmc-bertknowbert-medernie-med.
74.32% 75.12% 78.04% 80.45% 77.79% 87.30% 87.06% 95.89% 96.04%74.40% 74.46% 77.85% 80.54% 77.50% 87.17% 87.01% 95.81% 96.06%74.38% 75.25% 78.20% 80.67% 77.96% 87.25% 87.14% 95.96% 96.03%74.37% 75.22% 77.93% 80.56% 77.89% 87.34% 87.20% 96.02% 96.25%.
smedbert.
75.06% 76.04% 79.26% 81.68% 78.86% 88.13% 88.09% 96.64% 96.88%.
table 3: performance of question answering (qa), question matching (qm) and natural language inference(nli) tasks.
the metric of the qa task is acc@1 and those of qm and nli are f1..details of collecting data and embedding wordsin appendix c. in this experiments, we comparesmedbert with three types of models: classicalword embedding methods (sgns (mikolov et al.,2013a), glove (pennington et al., 2014)), plmsand keplms.
we compute the similarity betweenthe representation of query entities and all the otherentities, retrieving the most similar one.
the evalu-ation metric is top-1 accuracy (acc@1)..experiment results are shown in table 1. fromthe results, we observe that: (1) smedbert greatlyoutperforms all baselines especially on the datasetd2 (+1.36%), where most positive pairs havemany shared neighbours, demonstrating that abilityof smedbert to utilize semantic information fromthe global context.
(2) in dataset d3, smedbertimprove the performance signiﬁcantly (+1.01%),indicating our model is effective to enhance therepresentation of low-frequency mentions..4.4 results of downstream tasks.
we ﬁrst evaluate our model in ner and re tasksthat are closely related to entities in the input texts..table 2 shows the performances on medical nerand re tasks.
in ner and re tasks, we can ob-serve from the results: (1) compared with plmstrained in open-domain corpora, keplms withmedical corpora and knowledge facts achieve bet-ter results.
(2) the performance of smedbert isgreatly improved compared with the strongest base-line in two ner datasets (+0.88%, +2.07%), and(+0.68%, +0.92%) on re tasks.
we also evaluatesmedbert on qa, qm and nli tasks and theperformance is shown in table 3. we can observethat smedbert improve the performance consis-tently on these datasets (+0.90% on qa, +0.89%on qm and +0.63% on nli).
in general, it canbe seen from table 2 and table 3 that injecting thedomain knowledge especially the structured seman-tics knowledge can improve the result greatly..4.5.inﬂuence of entity hit ratio.
in this experiment, we explore the model perfor-mance in ner and re tasks with different entity hitratios, which control the proportions of knowledge-enhanced mention-spans in the samples.
the aver-.
5888model.
d5.
d6.
d7.
d8.
smedberternie-med.
84.75% 82.94% 86.95% 95.89%83.87% 80.87% 86.25% 94.91%.
- type att.
- hybrid att.
- know.
loss.
84.25% 81.99% 86.61% 95.29%83.71% 80.85% 86.46% 95.20%84.31% 82.12% 86.50% 95.43%.
table 4: ablation study of smedbert on four datasets(testing set).
due to the space limitation, we use the ab-breviations “d5”, “d6”, “d7”, and “d8” to representthe cmedqaner, dxy-ner, chip-re, and dxy-re datasets respectively..set performance on four datasets of ner and retasks that are closely related to entities.
speciﬁ-cally, the three model components are neighboringentity type attention, the whole hybrid attentionmodule, and mention-neighbor context modelingrespectively, which includes two masked languagemodel loss lmnem and lmmem..from the result, we can observe that: (1) with-out any of the three mechanisms, our model per-formance can also perform competitively with thestrong baseline ernie-med (zhang et al., 2019).
(2) note that after removing the hybrid attentionmodule, the performance of our model has thegreatest decline, which indicates that injecting richheterogeneous knowledge of neighboring entitiesis effective..5 conclusion.
in this work, we address medical text mining taskswith the structured semantics keplm proposednamed smedbert.
accordingly, we inject entitytype semantic information of neighboring entitiesinto node attention mechanism via heterogeneousfeature learning process.
moreover, we treat theneighboring entity structures as additional globalcontexts to predict the masked candidate entitiesbased on mention-spans and vice versa.
the exper-imental results show the signiﬁcant improvementof our model on various medical nlp tasks andthe intrinsic evaluation.
there are two research di-rections that can be further explored: (1) injectingdeeper knowledge by using “farther neighboring”entities as contexts; (2) further enhancing chinesemedical long-tail entity semantic representation..figure 3: entity hit ratio results of smedbert andernie in ner and re tasks..figure 4: the inﬂuence of different k values in results..age number of mention-spans in samples is about40. figure 3 illustrates the performance of smed-bert and ernie-med (zhang et al., 2019).
fromthe result, we can observe that: (1) the perfor-mance improves signiﬁcantly at the beginning andthen keeps stable as the hit ratio increases, prov-ing the heterogeneous knowledge is beneﬁcial toimprove the ability of language understanding andindicating too much knowledge facts are unhelpfulto further improve model performance due to theknowledge noise (liu et al., 2020b).
(2) comparedwith previous approaches, our smedbert modelimproves performance greatly and more stable..4.6.inﬂuence of neighboring entity number.
we further evaluate the model performance underdifferent k over the test set of dxy-ner anddxy-re.
figure 4 shows the the model result withk = {5, 10, 20, 30}.
in our settings, the smed-bert can achieve the best performance in differ-ent tasks around k = 10. the results of smed-bert show that the model performance increasingﬁrst and then decreasing with the increasing ofk. this phenomenon also indicates the knowledgenoise problem that injecting too much knowledgeof neighboring entities may hurt the performance..4.7 ablation study.
acknowledgements.
in table 4, we choose three important model com-ponents for our ablation study and report the test.
we would like to thank anonymous reviewers fortheir valuable comments.
this work is supported by.
5889the national key research and development pro-gram of china under grant no.
2016yfb1000904,and alibaba group through alibaba research in-tern program..references.
lei jimmy ba, jamie ryan kiros, and geoffrey e.corr,.
layer normalization..hinton.
2016.abs/1607.06450..iz beltagy, kyle lo, and arman cohan.
2019. scibert:a pretrained language model for scientiﬁc text.
inemnlp, pages 3613–3618..antoine bordes, nicolas usunier, alberto garc´ıa-dur´an,jason weston, and oksana yakhnenko.
2013. translating embeddings for modeling multi-relational data.
in nips, pages 2787–2795..ronan collobert and jason weston.
2008. a uniﬁedarchitecture for natural language processing: deepneural networks with multitask learning.
in icml,pages 160–167..yiming cui, wanxiang che, ting liu, bing qin, shi-jin wang, and guoping hu.
2020. revisiting pre-trained models for chinese natural language process-ing.
in emnlp, pages 657–668..yiming cui, wanxiang che, ting liu, bing qin,ziqing yang, shijin wang, and guoping hu.
2019.pre-training with whole word masking for chinesebert.
corr, abs/1906.08101..jacob devlin, ming-wei chang, kenton lee, andkristina toutanova.
2019. bert: pre-training ofdeep bidirectional transformers for language under-standing.
in naacl, pages 4171–4186..yu gu, robert tinn, hao cheng, michael lu-cas, naoto usuyama, xiaodong liu, tristan nau-mann, jianfeng gao, and hoifung poon.
2020.domain-speciﬁc language model pretraining forbiomedical natural language processing.
corr,abs/2007.15779..hiroaki hayashi, zecong hu, chenyan xiong, and gra-ham neubig.
2020. latent relation language models.
in aaai, pages 7911–7918..junqing he, mingming fu, and manshu tu.
2019. ap-plying deep matching networks to chinese medicalquestion answering: a study and a dataset.
bmcmedical informatics decis.
mak., 19-s(2):91–100..yun he, ziwei zhu, yin zhang, qin chen, and jamescaverlee.
2020.infusing disease knowledge intobert for health question answering, medical in-ference and disease name recognition.
in emnlp,pages 4604–4614..dan hendrycks and kevin gimpel.
2016. gaussian er-.
ror linear units (gelus).
arxiv:1606.08415..kexin huang,.
jaan altosaar,.
and rajesh ran-clinicalbert: modeling clinicalganath.
2019.notes and predicting hospital readmission.
corr,abs/1904.05342..paul jaccard.
1912. the distribution of the ﬂora in the.
alpine zone.
new phydvtologist, 11(2):37–50..s´ebastien jean, kyunghyun cho, roland memisevic,and yoshua bengio.
2015. on using very large tar-inget vocabulary for neural machine translation.
acl, pages 1–10..mandar joshi, danqi chen, yinhan liu, daniel s.weld, luke zettlemoyer, and omer levy.
2020.improving pre-training by representingspanbert:and predicting spans.
trans.
assoc.
comput.
lin-guistics, 8:64–77..zhenzhong lan, mingda chen, sebastian goodman,kevin gimpel, piyush sharma, and radu soricut.
2020. albert: a lite bert for self-supervisedlearning of language representations.
in iclr..yann lecun, sumit chopra, raia hadsell, m ranzato,and f huang.
2006. a tutorial on energy-basedlearning.
predicting structured data, 1(0)..jinhyuk lee, wonjin yoon,.
sungdong kim,donghyeon kim, sunkyu kim, chan ho so,and jaewoo kang.
2020. biobert: a pre-trainedbiomedicalforbiomedical text mining.
bioinform., 36(4):1234–1240..language representation model.
linfeng li, peng wang, jun yan, yao wang, siminli, jinpeng jiang, zhe sun, buzhou tang, tsung-hui chang, shenghui wang, and yuting liu.
2020.real-world data medical knowledge graph: con-struction and applications.
artif.
intell.
medicine,103:101817..yankai lin, zhiyuan liu, maosong sun, yang liu, andxuan zhu.
2015. learning entity and relation em-beddings for knowledge graph completion.
in aaai,pages 2181–2187..zhouhan lin, minwei feng, c´ıcero nogueira dos san-tos, mo yu, bing xiang, bowen zhou, and yoshuabengio.
2017. a structured self-attentive sentenceembedding.
in iclr..dayiheng liu, yeyun gong, jie fu, yu yan, jiushengchen, daxin jiang, jiancheng lv, and nan duan.
2020a.
rikinet: reading wikipedia pages for nat-ural question answering.
in acl, pages 6762–6771..weijie liu, peng zhou, zhe zhao, zhiruo wang, qi ju,haotang deng, and ping wang.
2020b.
k-bert:enabling language representation with knowledgegraph.
in aaai, pages 2901–2908..xiaodong liu, pengcheng he, weizhu chen, and jian-feng gao.
2019a.
multi-task deep neural networksfor natural language understanding.
in acl, pages4487–4496..5890yinhan liu, myle ott, naman goyal, jingfei du, man-dar joshi, danqi chen, omer levy, mike lewis,luke zettlemoyer, and veselin stoyanov.
2019b.
roberta: a robustly optimized bert pretraining ap-proach.
corr, abs/1907.11692..tom´as mikolov, kai chen, greg corrado, and jeffreydean.
2013a.
efﬁcient estimation of word represen-tations in vector space.
in iclr..tom´as mikolov, ilya sutskever, kai chen, gregory s.corrado, and jeffrey dean.
2013b.
distributed rep-resentations of words and phrases and their compo-sitionality.
in nips, pages 3111–3119..guoshun nan, zhijiang guo, ivan sekulic, and wei lu.
2020. reasoning with latent structure reﬁnement forin acl, pagesdocument-level relation extraction.
1546–1557..dat quoc nguyen, kairit sirts, lizhen qu, and markjohnson.
2016. neighborhood mixture model forknowledge base completion.
in conll, pages 40–50..lawrence page, sergey brin, rajeev motwani, andterry winograd.
1999. the pagerank citation rank-ing: bringing order to the web.
technical report1999-66, stanford infolab..yifan peng, shankai yan, and zhiyong lu.
2019.transfer learning in biomedical natural languageprocessing: an evaluation of bert and elmo on tenbenchmarking datasets.
in bionlp, pages 58–65..jeffrey pennington, richard socher, and christopher d.manning.
2014. glove: global vectors for word rep-resentation.
in emnlp, pages 1532–1543..matthew e. peters, mark neumann, robert l. loganiv, roy schwartz, vidur joshi, sameer singh, andnoah a. smith.
2019. knowledge enhanced contex-tual word representations.
in emnlp, pages 43–54..matthew e. peters, mark neumann, mohit iyyer, mattgardner, christopher clark, kenton lee, and lukezettlemoyer.
2018. deep contextualized word repre-sentations.
in naacl, pages 2227–2237..xipeng qiu, tianxiang sun, yige xu, yunfan shao,ning dai, and xuanjing huang.
2020. pre-trainedmodels for natural language processing: a survey.
corr, abs/2003.08271..maya rotmensch, yoni halpern, abdulhakim tlimat,steven horng, and david sontag.
2017. learninga health knowledge graph from electronic medicalrecords.
scientiﬁc reports, 7(1):1–11..michael sejr schlichtkrull, thomas n. kipf, peterbloem, rianne van den berg, ivan titov, and maxwelling.
2018. modeling relational data with graphconvolutional networks.
in eswc, pages 593–607..tianxiang sun, yunfan shao, xipeng qiu, qipeng guo,yaru hu, xuanjing huang, and zheng zhang.
2020.colake: contextualized language and knowledgeembedding.
in coling, pages 3660–3670..joseph p. turian, lev-arie ratinov, and yoshua ben-gio.
2010. word representations: a simple and gen-eral method for semi-supervised learning.
in acl,pages 384–394..ashish vaswani, noam shazeer, niki parmar, jakobuszkoreit, llion jones, aidan n. gomez, lukaszkaiser, and illia polosukhin.
2017. attention is allyou need.
in nips, pages 5998–6008..xiaoyan wang, pavan kapanipathi, ryan musa,mo yu, kartik talamadupula, ibrahim abdelaziz,maria chang, achille fokoue, bassem makni,nicholas mattei, and michael witbrock.
2019a.
im-proving natural language inference using externalinknowledge in the science questions domain.
aaai, pages 7208–7215..xiaozhi wang, tianyu gao, zhaocheng zhu, zhiyuanke-liu, juanzi li, and jian tang.
2019b.
pler: a uniﬁed model for knowledge embeddingand pre-trained language representation.
corr,abs/1911.06136..william e winkler.
1990. string comparator metricsand enhanced decision rules in the fellegi-suntermodel of record linkage..liang xu, xuanwei zhang, and qianqian dong.
cluecorpus2020: a large-scale chinese2020.corpus for pre-training language model.
corr,abs/2003.01355..zhilin yang, zihang dai, yiming yang, jaime g. car-bonell, ruslan salakhutdinov, and quoc v. le.
2019.xlnet: generalized autoregressive pretraining forin nips, pages 5754–language understanding.
5764..denghui zhang, zixuan yuan, yanchi liu, zuohui fu,fuzhen zhuang, pengyang wang, haifeng chen,and hui xiong.
2020a.
e-bert: a phrase andproduct knowledge enhanced language model for e-commerce.
corr, abs/2009.02835..ningyu zhang, qianghuai jia, kangping yin, liangdong, feng gao,and nengwei hua.
2020b.
conceptualized representation learning for chinesebiomedical text mining.
corr, abs/2008.10813..sheng zhang, xin zhang, hui wang, jiajun cheng, peili, and zhaoyun ding.
2017. chinese medical ques-tion answer matching using end-to-end character-level multi-scale cnns.
applied sciences, 7(8):767..zhengyan zhang, xu han, zhiyuan liu, xin jiang,maosong sun, and qun liu.
2019. ernie: en-hanced language representation with informative en-tities.
in acl, pages 1441–1451..5891a data source.
a.1 pre-training data.
a.1.1 training corpora.
the pre-training corpora is crawled from dxybbs (bulletin board system) 9, which is a verypopular chinese social network for doctors, med-ical institutions, life scientists, and medical prac-titioners.
the bbs has more than 30 channels,which contains 18 forums and 130 ﬁne-grainedgroups, covering most of the medical domains.
forour pre-training purpose, we crawl texts from chan-nels about clinical medicine, pharmacology, publichealth and consulting.
for text pre-processing, wemainly follow the methods of (xu et al., 2020).
ad-ditionally, (1) we remove all urls, html tags,e-mail addresses, and all tokens except characters,digits, and punctuation (2) all documents shorterthan 256 are discard, while documents longer than512 are cut into shorter text segments..a.1.2 knowledge graph.
the dxy knowledge graph is construed by ex-tracting structured text from dxy website10,which includes information of diseases, drugsand hospitals edited by certiﬁed medical experts,thus the quality of the kg is guaranteed.
thekg is mainly disease-centered, including totally3,764,711 triples, 152.508 unique entities, and 44types of relations.
the details of symptom-in-chinese from openkg is available 11. we ﬁnallyget 26 types of entities, 274,163 unique entities, 56types of relations, and 4,390,726 triples after thefusion of the two kgs..a.2 task data.
we choose the four large-scale datasets in chine-seblue tasks (zhang et al., 2020b) while othersare ignored due to the limitation of datasets size,which are cmedqaner, cmedqq, cmedqnliand cmedqa.
webmedqa (he et al., 2019) isa real-world chinese medical question answeringdataset and chip-re dataset are collected fromonline health consultancy websites.
note that sinceboth the webmedqa and cmedqa datasets arevery large while we have many baselines to becompared, we randomly sample the ofﬁcial train-ing set, development set and test set respectively.
9https://www.dxy.cn/bbs/newweb/pc/home10https://portal.dxy.cn/11http://openkg.cn/dataset/.
symptom-in-chinese.
to form their corresponding smaller version for ex-periments.
dxy-ner and dxy-re are datasetsfrom real medical application scenarios providedby a prestigious chinese medical company.
thedxy-ner contains 22 unique entity types and 56relation types in the dxy-re.
these two datasetsare collected from the medical forum of dxy andbooks in the medical domain.
annotators are se-lected from junior and senior students with clinicalmedical background.
in the process of quality con-trol, the two datasets are annotated twice by differ-ent groups of annotators.
an expert with medicalbackground performs quality check manually againwhen annotated results are inconsistent, whereasperform sampling quality check when results areconsistent.
table 5 shows the datasets size of ourexperiments..b model settings and training details.
hyper-parameters.
d1=768, d2=200, k=10, µ=10, λ1=2, λ2=4..m.m) =.
teimceim.is the sum of fre-.
model details.
we align the all mention-spansto the entity in kg by exact match for compar-ison purpose with enire-thu (zhang et al.,2019).
the negative sampling function is deﬁnedas q(ei, where ceiquency of all mentions with the same type of eim.the mention-neighbor hybrid attention module isinserted after the tenth transformer encoder layerto compare with knowbert (peters et al., 2019),while we perform the mention-neighbor contextmodeling based on the output of bert encoder.
we use all the base-version plms in the experi-ments.
the size of smedbert is 474mb while393mb of that are components of bert, and theadded 81mb is mostly of the kg embedding.
re-sults are presented in average with 5 random runswith different random seeds and the same hyper-parameters..training procedure.
we strictly follow the orig-inally pre-training process and parameter settingof other keplms.
we only adapt their publiclyavailable code from english to chinese and usethe knowledge embedding trained on our medicalkg.
to have a fair comparison, the pre-trainingprocessing of smedbert is mostly set based onenire-thu (zhang et al., 2019) without layer-special learning rates in knowbert (peters et al.,2019).
we only pre-train smedbert on the col-lected medical data for 1 epoch.
in pre-training.
5892dataset.
cmedqaner(zhang et al., 2020b).
cmedqq(zhang et al., 2020b).
cmedqnli(zhang et al., 2020b).
cmedqa(zhang et al., 2017).
webmedqa(he et al., 2019)chip-re ∗.
dxy-ner.
the dataset size in our experiments.
train.
1,673.dev.
175.test.
215.
16,071.
1,793.
1,935.
80,950.
9,065.
9,969.
186,771.
46,600.
46,600.
252,850.
31,605.
31,655.
43,649.
34,224.
-.
8,576.
10,622.
8,592.task.
ner.
qm.
nli.
qa.
qa.
re.
ner.
metric.
f1.
f1.
f1.
f1.
f1.
acc@1.acc@1.dxy-re.
f1∗ chip-re dataset is released in chip 2020.
(http://cips-chip.org.cn/2020/eval2).
141,696.
35,794.
35,456.re.
table 5: the statistical data and metric of eight datasets used in our smedbert model..process, the learning rate is set to 5e−5 and batchsize is 512 with the max sequence length is 512.for ﬁne-tuning, we ﬁnd the following ranges ofpossible values work well, i.e., batch size is {8,16},learning rate (adamw) is {2e−5, 4e−5, 6e−5} andthe number of epochs is {2,3,4}.
pre-trainingsmedbert takes about 36 hours per epoch on2 nvidia geforce rtx 3090 gpus..c data and embedding of unsupervised.
semantic similarity.
since the kgs used in this paper is a directedgraph, we ﬁrst transform the directed ”等价关系”(equivalence relations) pairs to undirected pairsand discard the duplicated pairs.
for each posi-tive pairs, we use head and tail as query respec-tively and sample the negative candidates basedon the other.
speciﬁcally, we randomly select 19negative entities with the same type and has a jaro-winkle similarity (winkler, 1990) bigger 0.6 withthe ground-truth entity.
we select from all samplesin dataset-1 with positive pairs that the neighbourssets of head and tail entity have jaccard index (jac-card, 1912) no less than 0.75 and at least 3 commonelement to construct the dataset-2.
for dataset-3,we count the frequency of all entity mentions in pre-training corpora, and treat mentions with frequencyno more than 200 as low-frequency mentions..classic word representation embedding:we train the character-level and word-level em-bedding using sgns (mikolov et al., 2013a) andglove (pennington et al., 2014) model respec-tively on our medical corpora with open-sourcetoolkits12.
we average the character embedding forall tokens in the mention to get the character-levelrepresentation.
however, since some mentions arevery rare in the corpora for word-level representa-tion, we use the character-level representation astheir word-level representation..bert-like representation embedding: weextract the token hidden features of the last layerand average the representations of the input tokensexcept [cls] and [sep] tag, to get a vector foreach entity..similarity measure: we try using the inverse ofl2-distance and cosine similarity as measurement,and we ﬁnd that cosine similarity always performbetter.
hence, we report all experiment resultsunder the cosine similarity metric..12sgns:.
word2vec-sgns.
glove:glove.
https://github.com/jugyang/.
https://github.com/stanfordnlp/.
5893