self-supervised multimodal opinion summarization.
jinbae im, moonki kim, hoyeop lee, hyunsouk cho, sehee chungknowledge ai lab., ncsoft co., south korea{jinbae,kmkkrk,hoyeoplee,dakgalbi,seheechung}@ncsoft.com.
abstract.
recently, opinion summarization, which is thegeneration of a summary from multiple re-views, has been conducted in a self-supervisedmanner by considering a sampled review asa pseudo summary.
however, non-text datasuch as image and metadata related to reviewshave been considered less often.
to use theabundant information contained in non-textdata, we propose a self-supervised multimodalopinion summarization framework called mul-timodalsum.
our framework obtains a repre-sentation of each modality using a separate en-coder for each modality, and the text decodergenerates a summary.
to resolve the inher-ent heterogeneity of multimodal data, we pro-pose a multimodal training pipeline.
we ﬁrstpretrain the text encoder–decoder based solelyon text modality data.
subsequently, we pre-train the non-text modality encoders by con-sidering the pretrained text decoder as a pivotfor the homogeneous representation of multi-modal data.
finally, to fuse multimodal rep-resentations, we train the entire framework inan end-to-end manner.
we demonstrate the su-periority of multimodalsum by conducting ex-periments on yelp and amazon datasets..1.introduction.
opinion summarization is the task of automaticallygenerating summaries from multiple documentscontaining users’ thoughts on businesses or prod-ucts.
this summarization of users’ opinions canprovide information that helps other users withtheir decision-making on consumption.
unlike con-ventional single-document or multiple-documentsummarization, where we can obtain the prevalentannotated summaries (nallapati et al., 2016; seeet al., 2017; paulus et al., 2018; liu et al., 2018; liuand lapata, 2019; perez-beltrachini et al., 2019),opinion summarization is challenging; it is difﬁcultto ﬁnd summarized opinions of users.
accordingly,.
(a) unimodal framework.
(b) multimodal framework.
figure 1:frameworks.
self-supervised opinion summarization.
studies used an unsupervised approach for opinionsummarization (ku et al., 2006; paul et al., 2010;carenini et al., 2013; ganesan et al., 2010; geraniet al., 2014).
recent studies (braˇzinskas and titov,2020; amplayo and lapata, 2020; elsahar et al.,2021) used a self-supervised learning frameworkthat creates a synthetic pair of source reviews anda pseudo summary by sampling a review text froma training corpus and considering it as a pseudosummary, as in figure 1a..users’ opinions are based on their perception ofa speciﬁc entity and perceptions originate from var-ious characteristics of the entity; therefore, opinionsummarization can use such characteristics.
forinstance, yelp provides users food or menu imagesand various metadata about restaurants, as in fig-ure 1b.
this non-text information inﬂuences thereview text generation process of users (truongand lauw, 2019).
therefore, using this additionalinformation can help in opinion summarization,especially under unsupervised settings (su et al.,2019; huang et al., 2020).
furthermore, the train-ing process of generating a review text (a pseudosummary) based on the images and metadata forself-supervised learning is consistent with the ac-.
proceedingsofthe59thannualmeetingoftheassociationforcomputationallinguisticsandthe11thinternationaljointconferenceonnaturallanguageprocessing,pages388–403august1–6,2021.©2021associationforcomputationallinguistics388tual process of writing a review text by a user..this study proposes a self-supervised multi-modal opinion summarization framework calledmultimodalsum by extending the existing self-supervised opinion summarization framework, asshown in figure 1. our framework receives sourcereviews, images, and a table on the speciﬁc busi-ness or product as input and generates a pseudosummary as output.
note that images and the ta-ble are not aligned with an individual review inthe framework, but they correspond to the speciﬁcentity.
we adopt the encoder–decoder frameworkand build multiple encoders representing each in-put modality.
however, a fundamental challengelies in the heterogeneous data of various modali-ties (baltruˇsaitis et al., 2018)..to address this challenge, we propose a multi-modal training pipeline.
the pipeline regards thetext modality as a pivot modality.
therefore, wepretrain the text modality encoder and decoder for aspeciﬁc business or product via the self-supervisedopinion summarization framework.
subsequently,we pretrain modality encoders for images and a ta-ble to generate review texts belonging to the samebusiness or product using the pretrained text de-coder.
when pretraining the non-text modality en-coders, the pretrained text decoder is frozen so thatthe image and table modality encoders obtain ho-mogeneous representations with the pretrained textencoder.
finally, after pretraining input modalities,we train the entire model in an end-to-end mannerto combine multimodal information..our contributions can be summarized as follows:• this study is the ﬁrst work on self-supervised.
multimodal opinion summarization;.
• we propose a multimodal training pipelineto resolve the heterogeneity between inputmodalities;.
• we verify the effectiveness of our modelframework and modeltraining pipelinethrough various experiments on yelp andamazon datasets..2 related work.
generally, opinion summarization has been con-ducted in an unsupervised manner, which can bedivided into extractive and abstractive approaches.
the extractive approach selects the most meaning-ful texts from input opinion documents, and the ab-stractive approach generates summarized texts thatare not shown in the input documents.
most previ-.
ous works on unsupervised opinion summarizationhave focused on extractive approaches.
clustering-based approaches (carenini et al., 2006; ku et al.,2006; paul et al., 2010; angelidis and lapata, 2018)were used to cluster opinions regarding the sameaspect and extract the text representing each clus-ter.
graph-based approaches (erkan and radev,2004; mihalcea and tarau, 2004; zheng and lap-ata, 2019) were used to construct a graph—wherenodes were sentences, and edges were similari-ties between sentences—and extract the sentencesbased on their centrality..although some abstractive approaches were notbased on neural networks (ganesan et al., 2010;gerani et al., 2014; di fabbrizio et al., 2014),neural network-based approaches have been gain-ing attention recently.
chu and liu (2019) gen-erated an abstractive summary from a denoisingautoencoder-based model.
more recent abstrac-tive approaches have focused on self-supervisedlearning.
braˇzinskas and titov (2020) randomlyselected n review texts for each entity and con-structed n synthetic pairs by sequentially regard-ing one review text as a pseudo summary and theothers as source reviews.
amplayo and lapata(2020) sampled a review text as a pseudo summaryand generated various noisy versions of it as sourcereviews.
elsahar et al.
(2021) selected review textssimilar to the sampled pseudo summary as sourcereviews, based on tf-idf cosine similarity.
weconstruct synthetic pairs based on braˇzinskas andtitov (2020) and extend the self-supervised opinionsummarization to a multimodal version..multimodal text summarization has been mainlystudied in a supervised manner.
text summarieswere created by using other modality data as ad-ditional input (li et al., 2018, 2020a), and somestudies provided not only a text summary but alsoother modality information as output (zhu et al.,2018; chen and zhuge, 2018; zhu et al., 2020; liet al., 2020b; fu et al., 2020).
furthermore, moststudies summarized a single sentence or document.
although li et al.
(2020a) summarized multipledocuments, they used non-subjective documents.
our study is the ﬁrst unsupervised multimodal textsummarization work that summarizes multiple sub-jective documents..3 problem formulation.
the goal of the self-supervised multimodal opin-ion summarization is to generate a pseudo sum-.
389mary from multimodal data.
following existingself-supervised opinion summarization studies, weconsider a review text selected from an entire re-view corpus as a pseudo summary.
we extendthe formulation of braˇzinskas and titov (2020)to a multimodal version.
let r = {r1, r2, ..., rn }denote the set of reviews about an entity (e.g., abusiness or product).
each review, rj, consists ofreview text, dj, and review rating, sj, that repre-sents the overall sentiment of the review text.
wedenote images uploaded by a user or provided bya company for the entity as i = {i1, i2, ..., im }and a table containing abundant metadata aboutthe entity as t .
here, t consists of several ﬁelds,and each ﬁeld contains its own name and value.
we set j-th review text dj as the pseudo summaryand let it be generated from r−j, i, and t , wherer−j = {r1, ..., rj−1, rj+1, ..., rn } denotes sourcereviews.
to help the model summarize what standsout overall in the review corpus, we calculate theloss for all n cases of selecting dj from r, andtrain the model using the average loss.
duringtesting, we generate a summary from r, i, and t ..4 model framework.
the proposed model framework, multimodalsum,is designed with an encoder–decoder structure, asin figure 1b.
to address the heterogeneity of threeinput modalities, we conﬁgure each modality en-coder to effectively process data in each modality.
we set a text decoder to generate summary textby synthesizing encoded representations from thethree modality encoders.
details are described inthe following subsections..4.1 text encoder and decoder.
our text encoder and decoder are based onbart (lewis et al., 2020).
bart is a trans-former (vaswani et al., 2017) encoder–decoder pre-trained model that is particularly effective whenﬁne-tuned for text generation and has high sum-marization performance.
furthermore, because thepseudo summary of self-supervised multimodalopinion summarization is an individual review text(dj), we determine that pretraining bart based ona denoising autoencoder is suitable for our frame-work.
therefore, we further pretrain bart usingthe entire training review corpus (gururangan et al.,2020).
our text encoder obtains ed-dimensionalencoded text representations htext from d−j and.
the text decoder generates dj from htext as follows:.
htext = bartenc(d−j),dj = bartdec(htext),.
where d−j = {d1, ..., dj−1, dj+1, ..., dn } denotesthe set of review texts from r−j.
each review textconsists of ld tokens and htext ∈ r(n −1)×ld×ed ..4.2.image encoder.
we use a convolutional neural network specializedin analyzing visual imagery.
in particular, we useimagenet pretrained resnet101 (he et al., 2016),which is widely used as a backbone network.
weadd an additional linear layer in place of the imageclassiﬁcation layer to match feature distributionand dimensionality with text modality representa-tions.
our image encoder obtains encoded imagerepresentations himg from i as follows:.
himg = resnet101(i) wimg,.
where wimg ∈ rei ×ed denotes the additional lin-ear weights.
himg obtains rm ×li ×ed , where lirepresents the size of the ﬂattened image featuremap obtained from resnet101..4.3 table encoder.
to effectively encode metadata, we design our ta-ble encoder based on the framework of data-to-textresearch (puduppully et al., 2019).
the input toour table encoder t is a series of ﬁeld-name andﬁeld-value pairs.
each ﬁeld gets et -dimensionalrepresentations through a multilayer perceptron af-ter concatenating the representations of ﬁeld-nameand ﬁeld-value.
the encoded table representationshtable is obtained by stacking each ﬁeld representa-tion into f and adding a linear layer as follows:.
fk = relu([nk; vk] wf + bf ),.
htable = f wtable,.
where n and v denote et -dimensional represen-tations of ﬁeld name and value, respectively, andwf ∈ r2et ×et , bf ∈ ret are parameters.
bystacking lt ﬁeld representations, we obtain f ∈r1×lt ×et .
the additional linear weights wtable ∈ret ×ed play the same role as in the image encoder,and htable ∈ r1×lt ×ed ..5 model training pipeline.
to effectively train the model framework, we seta model training pipeline, which consists of three.
390(b) other modalities pretraining.
(c) training for multiple modalities.
figure 2: self-supervised multimodal opinion summarization training pipeline.
blurred boxes in “other modalitiespretraining” indicate that the text decoders are untrained..(a) text modality pretraining.
steps, as in figure 2. the ﬁrst step is text modalitypretraining, in which a model learns unsupervisedsummarization capabilities using only text modal-ity data.
next, during the pretraining for othermodalities, an encoder for each modality is trainedusing the text modality decoder learned in the pre-vious step as a pivot.
the main purpose of this stepis that other modalities have representations whosedistribution is similar to that of the text modality.
inthe last step, the entire model framework is trainedusing all the modality data.
details of each stepcan be found in the next subsections..5.1 text modality pretraining.
in this step, we pretrain the text encoder and de-coder for self-supervised opinion summarization.
as this was an important step for unsupervisedmultimodal neural machine translation (su et al.,2019), we apply it to our framework.
for theset of reviews about an entity r, we train themodel to generate a pseudo summary dj fromsource reviews r−j for all n cases as follows:loss = (cid:80)nj=1 log p(dj|r−j).
the text encoder ob-tains htext ∈ r(n −1)×ld×ed from d−j, and thetext decoder aggregates the encoded representa-tions of n − 1 review texts to generate dj.
wemodel the aggregation of multiple encoded repre-sentations in the multi-head self-attention layer ofthe text decoder.
to generate a pseudo summarythat covers the overall contents of source reviews,we simply average the n − 1 single-head attentionresults for each encoded representation (rld×ed )at each head (elsahar et al., 2021)..the limitation of the self-supervised opinionsummarization is that training and inference tasksare different.
the model learns a review genera-tion task using a review text as a pseudo summary;however, the model needs to perform a summarygeneration task at inference.
to close this gap, we.
figure 3: text decoder input representations.
the in-put embeddings are the sum of the token embeddings,rating deviation times deviation embeddings, and thepositional embeddings..use a rating deviation between the source reviewsand the target as an additional input feature of thetext decoder, inspired by braˇzinskas et al.
(2020).
we deﬁne the average ratings of the source reviewsminus the rating of the target as the rating deviation:sdj = (cid:80)ni(cid:54)=j si/(n − 1) − sj.
we use sdj to helpgenerate a pseudo summary dj during training andset it as 0 to generate a summary with average se-mantic of input reviews during inference.
to reﬂectthe rating deviation, we modify the way in whicha transformer creates input embeddings, as in fig-ure 3. we create deviation embeddings with thesame dimensionality as token embeddings and addsdj × deviation embeddings to the token embed-dings in the same way as positional embeddings..our methods to close the gap between trainingand inference tasks do not require additional model-ing or training in comparison with previous works.
we achieve noising and denoising effects by simplyusing rating deviation embeddings without varia-tional inference in braˇzinskas and titov (2020).
furthermore, the information that the rating devi-ation is 0 plays the role of an input prompt forinference, without the need to train a separate clas-siﬁer for selecting control tokens to be used as inputprompts (elsahar et al., 2021)..3915.2 other modalities pretraining.
as the main modality for summarization is the textmodality, we pretrain the image and table encodersby pivoting the text modality.
although the dataof the three modalities are heterogeneous, each en-coder should be trained to obtain homogeneousrepresentations.
we achieve this by using the pre-trained text decoder as a pivot.
we train the im-age encoder and the table encoder along with thetext decoder to generate a review text of the entityto which images or metadata belong: i or t →dj ∈ r. the image and table encoders obtain himgand htable from i and t , respectively, and the textdecoder generates dj from himg or htable.
note thatwe aggregate m encoded representations of himgas in the text modality pretraining, and the weightsof the text decoder are made constant.
i or t cor-responds to all n reviews, and this means that i ort has multiple references.
we convert a multiple-reference setting to a single-reference setting tomatch the model output with the text modality pre-training.
we simply create n single reference pairsfrom each entity and shufﬂe pairs from all enti-ties to construct the training dataset (zheng et al.,2018).
as the text decoder was trained for generat-ing a review text from text encoded representations,the image and table encoders are bound to pro-duce similar representations with the text encoderto generate the same review text.
in this way, wecan maximize the ability to extract the informationnecessary for generating the review text..5.3 training for multiple modalities.
we train the entire multimodal framework from thepretrained encoders and decoder.
the encoder ofeach modality obtains an encoded representationfor each modality, and the text decoder generatesthe pseudo summary dj from multimodal encodedrepresentations htext, himg, and htable.
to fusemultimodal representations, we aim to meet threerequirements.
first, the text modality, which isthe main modality, is primarily used.
second, themodel works even if images or metadata are notavailable.
third, the model makes the most of thelegacy from pretraining.
to fulﬁll the requirements,multi-modality fusion is applied to the multi-headself-attention layer of the text decoder.
the textdecoder obtains the attention result for each modal-ity at each layer.
we fuse the attention results formultiple modalities as follows:.
yelp#businesses#reviews/business#summaries/business#max images#max ﬁeldsamazon#products#reviews/product#summaries/product#max images#max ﬁelds.
train50,11381*1047train60,93581*15+128.
dev100811047dev288315+128.
test100811047test328315+128.
table 1: data statistics; 1* in train column indicatesthat it is a pseudo summary..where matext, maimg, and matable denote eachmodality attention result from htext, himg, andhtable,respectively.
(cid:12) symbolizes element-wise multiplication and ed-dimensional multi-modal gates α and β are calculated as fol-lows: α = φ([matext; maimg] wα) and β =φ([matext; matable] wβ).
note that α or β obtainsthe zero vector when images or metadata do notexist.
it is common to use sigmoid as an activationfunction φ. however, it can lead to confusion in thetext decoder pretrained using only the text source.
because the values of w are initialized at approx-imately 0, the values of α and β are initialized atapproximately 0.5 when sigmoid is used.
to ini-tialize the gate values at approximately 0, we userelu(tanh(x)) as φ(x).
this enables the continu-ous use of text information, and images or metadataare used selectively..6 experimental setup.
6.1 datasets.
to evaluate the effectiveness of the model frame-work and training pipeline on datasets with dif-ferent domains and characteristics, we performedexperiments on two review datasets: yelp datasetchallenge1 and amazon product reviews (he andmcauley, 2016).
the yelp dataset provides re-views based on personal experiences for a speciﬁcbusiness.
it also provides numerous images (e.g.,food and drinks) uploaded by the users.
notethat the maximum number of images, m , wasset to 10 based on the 90th percentile.
in addi-tion, the dataset contains abundant metadata ofbusinesses according to the characteristics of eachbusiness.
on the contrary, the amazon datasetprovides reviews with more objective and speciﬁcdetails about a particular product.
it contains a sin-.
mafused = matext + α (cid:12) maimg + β (cid:12) matable,.
1https://www.yelp.com/dataset.
392gle image provided by the supplier, and providesrelatively limited metadata for the product.
forevaluation, we used the data used in previous re-search (chu and liu, 2019; braˇzinskas and titov,2020).
the data were generated by amazon me-chanical turk workers who summarized 8 inputreview texts.
therefore, we set n to 9 so that apseudo summary is generated from 8 source re-views during training.
for the amazon dataset,3 summaries are given per product.
simple datastatistics are shown in table 1, and other detailscan be found in appendix a.1..6.2 experimental details.
the models2 were implemented with py-alltorch (paszke et al., 2019), and we used the trans-formers library from hugging face (wolf et al.,2020) as the backbone skeleton.
our text encoderand decoder were initialized using bart-largeand further pretrained using the training reviewcorpus with the same objective as bart.
ed, ei ,and et were all set to 1,024. we trained the entiremodels using the adam optimizer (kingma and ba,2014) with a linear learning rate decay on nvidiav100s.
we decayed the model weights with 0.1.for each training pipeline, we set different batchsizes, epochs, learning rates, and warmup steps ac-cording to the amount of learning required at eachstep.
we used label smoothing with 0.1 and set themaximum norm of gradients as 1 for other modal-ities pretraining and multiple-modalities training.
during testing, we used beam search with earlystopping and discarded hypotheses that containtwice the same trigram.
different beam size, lengthpenalty, and max length were set for yelp and ama-zon.
the best hyperparameter values and otherdetails are described in appendix a.2..6.3 comparison models.
we compared our model to extractive and abstrac-tive opinion summarization models.
for extrac-tive models, we used some simple baseline mod-els (braˇzinskas and titov, 2020).
clustroid selectsone review that gets the highest rouge-l scorewith the other reviews of an entity.
lead constructsa summary by extracting and concatenating thelead sentences from all review texts of an entity.
random simply selects one random review froman entity.
lexrank (erkan and radev, 2004) isan extractive model that selects the most salient.
2our code is available at https://bit.ly/3br4yod.
sentences based on graph centrality..for abstractive models, we used non-neural andneural models.
opinosis (ganesan et al., 2010) isa non-neural model that uses a graph-based sum-marizer based on token-level redundancy.
mean-sum (chu and liu, 2019) is a neural model that isbased on a denoising-autoencoder and generatesa summary from mean representations of sourcereviews.
we also used three self-supervised abstrac-tive models.
denoisesum (amplayo and lapata,2020) generates a summary by denoising source re-views.
copycat (braˇzinskas and titov, 2020) usesa hierarchical variational autoencoder model andgenerates a summary from mean latent codes ofthe source reviews.
self & control (elsahar et al.,2021) generates a summary from transformer mod-els and uses some control tokens as additional in-puts to the text decoder..7 results.
we evaluated our model framework and modeltraining pipeline.
in particular, we evaluated thesummarization quality compared to other baselinemodels in terms of automatic and human evalua-tion, and conducted ablation studies..7.1 main results.
7.1.1 automatic evaluation.
to evaluate the summarization quality, we usedtwo automatic measures: rouge-{1,2,l} (lin,2004) and bert-score (zhang et al., 2020).
theformer is a token-level measure for comparing 1,2, and adaptive l-gram matching tokens, and thelatter is a document-level measure using pretrainedbert (devlin et al., 2019).
contrary to rouge-score, which is based on exact matching betweenn-gram words, bert-score is based on the seman-tic similarity between word embeddings that reﬂectthe context of the document through bert.
it isapproved that bert-score is more robust to adver-sarial examples and correlates better with humanjudgments compared to other measures for machinetranslation and image captioning.
we hypothesizethat bert-score is strong in opinion summariza-tion as well, and bert-score would complementrouge-score..the results for opinion summarization on twodatasets are shown in table 2. multimodalsumshowed superior results compared with extractiveand abstractive baselines for both token-level anddocument-level measures.
from the results, we.
393model.
e clustroid (braˇzinskas and titov, 2020)vilead (braˇzinskas and titov, 2020)tcarandom (braˇzinskas and titov, 2020)rtxlexrank (erkan and radev, 2004)eopinosis (ganesan et al., 2010)meansum (chu and liu, 2019)denoisesum (amplayo and lapata, 2020)copycat (braˇzinskas and titov, 2020)self & control (elsahar et al., 2021)multimodalsum (ours).
evitcartsba.r-126.2826.3423.0424.9020.6228.8630.1429.4732.7633.00.yelp.
r-l15.3613.8613.4414.2812.5515.9117.6518.0918.8219.84*.
r-23.483.722.442.762.183.664.995.268.656.63.fbert85.885.185.185.484.486.585.987.486.887.7*.
amazon.
r-129.2730.3228.9329.4624.0429.20-31.97-34.19*.
r-24.415.854.585.533.694.70-5.81-7.05*.
r-l17.7815.9616.7617.7414.5818.15-20.16-20.81.fbert86.485.886.086.485.2--87.7-87.9.table 2: opinion summarization results on yelp and amazon datasets.
r-1, r-2, r-l, and fbert refer to rouge-{1,2,l}, and bert-score, respectively.
the best models are marked in bold, and the second-best models areunderlined.
* indicates that our model shows signiﬁcant gains (p < 0.05) over the second-best model based onpaired bootstrap resampling (koehn, 2004).
all the reported scores are based on f1..goldwow, where to start?
some of the best sweet foods i’veever had.
i wasn’t sure what to try, so i tried a few things,and oh my goodness they were delicious.
that’s not allthough, they serve drinks too so i got a latte and that wasgood too.
there is a lot of variety here to choose fromthat’ll make any sweet tooth salivate.
deﬁnitely a goodplace!.
copycatif you’re looking for a sweet tooth this is the place to goif you want a delicious dessert.
i had the lemon meringuepie and it was delicious.
the only thing i didn’t like wasthat i could eat half of it, but it was a little pricey for whatyou get..self & controlif you’re a fan of the matos buffet, this is a good place tovisit.
if you want to have a sweet tooth this is the place tobe.
the desserts are delicious and they have a good varietyof desserts to choose from.
the only thing i don’t likeabout this place is that you have to wait in line for a longtime to get in.
other than that, you can’t really go wrongwith any of the desserts in the buffet.
the drinks are goodand the desserts are yummy too.
they also have dessertsthat are not too sweet.
i’m not a huge fan of buffets, butthis is one of my favorite buffets..multimodalsumthis is a cute little bakery located in the m resort.
i had thechocolate croissant and it was very good.
the croissantswere soft and moist and the ﬁlling was delicious.
i alsohad a chocolate chip cookie which was also good.
i woulddeﬁnitely recommend this place if you are in the area..table 3: sample summaries generated by various mod-els on the yelp dataset.
conclude that the multimodal framework outper-formed the unimodal framework for unsupervisedopinion summarization.
in particular, our modelachieved state-of-the-art results on the amazondataset and outperformed the comparable model bya large margin in the r-l representing the rougescores on the yelp dataset.
although self & con-trol showed high r-2 score, we attributed theirscore to the inferred n -gram control tokens usedas additional inputs to the text decoder..sample summaries on the yelp dataset are shownin table 3. they were generated from source re-views on baby cakes bakery.
copycat misused“sweet tooth” and generated “lemon mernigue pie”that was not mentioned in the source reviews.
self& control generated a summary about a buffet bytotally misunderstanding one sentence from sourcereviews: “if you love the desserts in studio b buf-fet in the m hotel but don’t want to wait in themassive buffet line or even eat in the buffet, babycakes in the m hotel is really nice ﬁx.” further-more, “matos buffet” is a non-existent word.
onthe contrary, multimodalsum generated a goodsummary with a rich description of chocolate crois-sants.
although “chocolate chip cookie” was notfound in the source reviews, our model generatedit from cookie images.
note that the term canbe found in other reviews that were not used assource reviews.
additional sample summaries ontwo datasets are shown in appendix a.5..7.1.2 human evaluation.
to evaluate the quality of summarization based onhuman criteria, we conducted a user study.
we as-sessed the quality of summaries using best-worstscaling (bws; louviere et al.
(2015)).
bws isknown to produce more reliable results than rak-ing scales (kiritchenko and mohammad, 2017) andis widely used in self-supervised opinion summa-rization studies.
we recruited 10 nlp experts andasked each participant to choose one best and oneworst summary from four summaries for three crite-ria.
for each participant’s response, the best modelreceived +1, the worst model received -1, and therest of the models received 0 scores.
the ﬁnalscores were obtained by averaging the scores of allthe responses from all participants..394figure 4: multimodal gate heatmaps; from the table and two images, our model generates a summary.
heatmapsrepresent the overall inﬂuence of table and images for generating each word in the summary.
note that the summaryis a real example generated from our model without beam search..for overall criterion, self & control, copy-cat, multimodalsum, and gold summaries scored-0.527, -0.113, +0.260, and +0.380 on the yelpdataset, respectively.
multimodalsum showed su-perior performance in human evaluation as wellas automatic evaluation.
we note that humanjudgments correlate better with bert-score thanrouge-score.
self & control achieved a very lowhuman evaluation score despite its high rouge-score in automatic evaluation.
we analyzed thesummaries of self & control, and we found severalﬂaws such as redundant words, ungrammatical ex-pressions, and factual hallucinations.
it generated anon-existent word by combining several subwords.
it was particularly noticeable when a proper nounwas generated.
furthermore, self & control gen-erated an implausible sentence by copying somewords from source reviews.
from the results, weconclude that both automatic evaluation and humanevaluation performances should be supported to bea good summarization model and bert-score cancomplement rouge-score in automatic evalua-tion.
details on human evaluation and full resultscan be found in appendix a.3..7.1.3 effects of multimodality.
to analyze the effects of multimodal data onopinion summarization, we analyzed the multi-modal gate.
since the multimodal gate is a ed-dimensional vector, we averaged it by a scalarvalue.
furthermore, as multimodal gates exist foreach layer of the text decoder, we averaged them tomeasure the overall inﬂuence of a table or imageswhen generating each token in the decoder.
anexample of aggregated multimodal gates is shownin figure 4. it shows the table and images used.
for generating a summary text, and the multimodalgates for a part of the generated summary are ex-pressed as heatmaps.
as we intended, table andimage information was selectively used to generatea speciﬁc word in the summary.
the aggregatedvalue of the table was relatively high for generating“red lobster”, which is the name of the restaurants.
it was relatively high for images, when generat-ing “food” that is depicted in two images.
anothercharacteristic of the result is that aggregated valuesof the table were higher than those of the image:mean values for the table and image in the entiretest data were 0.103 and 0.045, respectively.
thisimplies that table information is more used whencreating a summary, and this observation is valid inthat the table contains a large amount of metadata.
note that the values displayed on the heatmaps aresmall by and large, as they were aggregated fromed-dimensional vector..7.2 ablation studies.
for ablation studies, we analyzed the effective-ness of our model framework and model trainingpipeline in table 4. to analyze the model frame-work, we ﬁrst compared the summarization qualitywith four versions of unimodal model framework,as in the ﬁrst block of table 4. bart denotes themodel framework in figure 1a, whose weights arethe weights of bart-large.
it represents the lowerbound of our model framework without any train-ing.
bart-review denotes the model frameworkwhose weights are from further pretrained bartusing the entire training review corpus.
unimodal-sum refers to the results of the text modality pre-training, and we classiﬁed it into two frameworksaccording to the use of the rating deviation..395surprisingly, using only bart achieved compa-rable or better results than many extractive and ab-stractive baselines in table 2. furthermore, furtherpretraining using the review corpus brought perfor-mance improvements.
qualitatively, bart withfurther pretraining generated more diverse wordsand rich expressions from the review corpus.
thisproved our assumption that denoising autoencoder-based pretraining helps in self-supervised multi-modal opinion summarization.
based on the bart-review, unimodalsum achieved superior results.
furthermore, the use of rating deviation improvedthe quality of summarization.
we conclude thatlearning to generate reviews based on wide rangesof rating deviations including 0 during traininghelps to generate a better summary of the averagesemantics of the input reviews..to analyze the effect of other modalities in ourmodel framework, we compared the summariza-tion quality with three versions of multimodalmodel frameworks, as in the second block of ta-ble 4. we removed the image or table modalityfrom multimodalsum to analyze the contributionof each modality.
results showed that both modali-ties improved the summarization quality comparedwith unimodalsum, and they brought additionalimprovements when used altogether.
this indi-cates that using non-text information helps in self-supervised opinion summarization.
as expected,the utility of the table modality was higher thanthat of the image modality.
the image modal-ity contains detailed information not revealed inthe table modality (e.g., appearance of food, in-side/outside mood of business, design of product,and color/texture of product).
however, the infor-mation is unorganized to the extent that the utilityof the image modality depends on the capacity ofthe image encoder to extract unorganized informa-tion.
although multimodalsum used a represen-tative image encoder because our study is the ﬁrstwork on multimodal opinion summarization, weexpect that the utility of the image modality will begreater if unorganized information can be extractedeffectively from the image using advanced imageencoders..for analyzing the model training pipeline, weremoved text modality or/and other modalities pre-training from the pipeline.
by removing each ofthem, the performance of multimodalsum declined,and removing all of the pretraining steps causedan additional performance drop.
although multi-.
modelsbartbart-reviewunimodalsum w/o rating deviationunimodalsum w/ rating deviationmultimodalsum.
w/o image modalityw/o table modalityw/o other modalities pretrainingw/o text modality pretrainingw/o all modalities pretraining.
r-l14.8515.2318.9819.4019.8419.5419.4719.2619.2419.14.table 4: ablation studies on the yelp dataset.
the ﬁrstand second blocks represent various versions of the uni-modal model framework and multimodal model frame-work, respectively.
the third block shows the differ-ences in our multimodal framework’s performance ac-cording to the absence of speciﬁc steps in the modeltraining pipeline..modalsum without other modalities pretraining hasthe capability of text summarization, it showed lowsummarization performance at the beginning of thetraining due to the heterogeneity of the three modal-ity representations.
however, multimodalsumwithout text modality pretraining, whose imageand table encoders were pretrained using bart-review as a pivot, showed stable performance fromthe beginning, but the performance did not improvesigniﬁcantly.
from the results, we conclude thatboth text modality and other modalities pretraininghelp the training of multimodal framework.
forthe other modalities pretraining, we conducted afurther analysis in the appendix a.4..8 conclusions.
we proposed the ﬁrst self-supervised multimodalopinion summarization framework.
our frameworkcan reﬂect text, images, and metadata together asan extension of the existing self-supervised opinionsummarization framework.
to resolve the hetero-geneity of multimodal data, we also proposed amultimodal training pipeline.
we veriﬁed the ef-fectiveness of our multimodal framework and train-ing pipeline with various experiments on real re-view datasets.
self-supervised multimodal opinionsummarization can be used in various ways in thefuture, such as providing a multimodal summaryor enabling a multimodal retrieval.
by retrievingreviews related to a speciﬁc image or metadata,controlled opinion summarization will be possible..acknowledgments.
we thank the anonymous reviewers for their in-sightful comments and suggestions..396references.
reinald kim amplayo and mirella lapata.
2020. un-supervised opinion summarization with noising anddenoising.
in proceedings of the 58th annual meet-ing of the association for computational linguistics,pages 1934–1945..stefanos angelidis and mirella lapata.
2018. sum-marizing opinions: aspect extraction meets senti-ment prediction and they are both weakly supervised.
in proceedings of the 2018 conference on empiri-cal methods in natural language processing, pages3675–3686..tadas baltruˇsaitis, chaitanya ahuja,.
and louis-philippe morency.
2018. multimodal machine learn-ieee transac-ing: a survey and taxonomy.
tions on pattern analysis and machine intelligence,41(2):423–443..arthur braˇzinskas, mirella lapata, and ivan titov.
2020. few-shot learning for opinion summarization.
in proceedings of the 2020 conference on empiri-cal methods in natural language processing, pages4119–4135..mirella lapata braˇzinskas, arthur and ivan titov.
2020.unsupervised opinion summarization as copycat-in proceedings of the 58th an-review generation.
nual meeting of the association for computationallinguistics, pages 5151–5169..giuseppe carenini,.
jackie chi kit cheung, andadam pauls.
2013. multi-document summariza-tion of evaluative tex.
computational intelligence,29(4):545–576..giuseppe carenini, raymond ng, and adam pauls.
2006. multi-document summarization of evaluativetext.
in proceedings of the 11th conference of theeuropean chapter of the association for computa-tional linguistics..jingqiang chen and hai zhuge.
2018. abstractive text-image summarization using multi-modal attentionalin proceedings of the 2018 con-hierarchical rnn.
ference on empirical methods in natural languageprocessing, pages 4046–4056..eric chu and peter liu.
2019. meansum: a neuralmodel for unsupervised multi-document abstractivesummarization.
in in proceedings of internationalconference on machine learning (icml), pages1223–1232..jacob devlin, ming-wei chang, kenton lee, andkristina toutanova.
2019. bert: pre-training ofdeep bidirectional transformers for language under-standing.
in proceedings of the 2019 conference ofthe north american chapter of the association forcomputational linguistics: human language tech-nologies, volume 1 (long and short papers), pages4171–4186..giuseppe di fabbrizio, amanda stent, and robertgaizauskas.
2014. a hybrid approach to multi-document summarization of opinions in reviews.
inproceedings of the 8th international natural lan-guage generation conference, pages 54–63..hady elsahar, maximin coavoux, jos rozen, andmatthias gall´e.
2021.self-supervised and con-trolled multi-document opinion summarization.
inproceedings of the 16th conference of the europeanchapter of the association for computational lin-guistics: main volume, pages 1646–1662..g¨unes erkan and dragomir r radev.
2004. lexrank:graph-based lexical centrality as salience in textsummarization.
journal of artiﬁcial intelligence re-search, 22:457–479..xiyan fu, jun wang, and zhenglu yang.
2020. multi-modal summarization for video-containing docu-ments.
arxiv preprint arxiv:2009.08018..kavita ganesan, chengxiang zhai, and jiawei han.
2010. opinosis: a graph based approach to abstrac-tive summarization of highly redundant opinions.
inproceedings of the 23rd international conference oncomputational linguistics, pages 340–348..shima gerani, yashar mehdad, giuseppe carenini,raymond ng, and bita nejat.
2014. abstractivesummarization of product reviews using discoursestructure.
in proceedings of the 2014 conference onempirical methods in natural language processing,pages 1602–1613..suchin gururangan, ana marasovi´c,.
swabhaswayamdipta, kyle lo, iz beltagy, doug downey,and noah a smith.
2020. don’t stop pretraining:inadapt language models to domains and tasks.
proceedings ofthethe 58th annual meeting ofassociation for computational linguistics, pages8342–8360..kaiming he, xiangyu zhang, shaoqing ren, and jiansun.
2016. deep residual learning for image recog-in proceedings of the ieee conference onnition.
computer vision and pattern recognition, pages 770–778..ruining he and julian mcauley.
2016. ups and downs:modeling the visual evolution of fashion trends withone-class collaborative ﬁltering.
in proceedings ofthe 25th international conference on world wideweb, pages 507–517..po-yao huang, junjie hu, xiaojun chang, and alexan-der hauptmann.
2020. unsupervised multimodalneural machine translation with pseudo visual piv-in proceedings of the 58th annual meet-oting.
ing of the association for computational linguistics,pages 8226–8237..diederik p kingma and jimmy ba.
2014. adam: amethod for stochastic optimization.
arxiv preprintarxiv:1412.6980..397svetlana kiritchenko and saif mohammad.
2017. best-worst scaling more reliable than rating scales: acase study on sentiment intensity annotation.
in pro-ceedings of the 55th annual meeting of the associa-tion for computational linguistics (volume 2: shortpapers), pages 465–470..philipp koehn.
2004. statistical signiﬁcance tests forin proceedings ofmachine translation evaluation.
the 2004 conference on empirical methods in natu-ral language processing, pages 388–395..lun-wei ku, yu-ting liang, hsin-hsi chen, et al.
2006. opinion extraction, summarization and track-ing in news and blog corpora.
in aaai spring sympo-sium: computational approaches to analyzing we-blogs, pages 100–107..kuang-huei lee, xi chen, gang hua, houdong hu,and xiaodong he.
2018. stacked cross attention forin proceedings of the euro-image-text matching.
pean conference on computer vision, pages 201–216..mike lewis, yinhan liu, naman goyal, mar-jan ghazvininejad, abdelrahman mohamed, omerlevy, veselin stoyanov, and luke zettlemoyer.
2020. bart: denoising sequence-to-sequence pre-training for natural language generation.
in proceed-ings of the 58th annual meeting of the associationfor computational linguistics, pages 7871–7880..haoran li, peng yuan, song xu, youzheng wu, xi-aodong he, and bowen zhou.
2020a.
aspect-awaremultimodal summarization for chinese e-commerceproducts.
in proceedings of the 34th aaai confer-ence on artiﬁcial intelligence, pages 8188–8195..haoran li, junnan zhu, tianshang liu, jiajun zhang,and chengqing zong.
2018. multi-modal sentencesummarization with modality attention and image ﬁl-tering.
in proceedings of the twenty-seventh inter-national joint conference on artiﬁcial intelligence,pages 4152–4158..mingzhe li, xiuying chen, shen gao, zhangmingchan, dongyan zhao, and rui yan.
2020b.
vmsmo:learning to generate multimodal summary for video-based news articles.
in proceedings of the 2020 con-ference on empirical methods in natural languageprocessing, pages 9360–9369..chin-yew lin.
2004. rouge: a package for automaticin text summarization.
evaluation of summaries.
branches out, pages 74–81..peter j liu, mohammad saleh, etienne pot, bengoodrich, ryan sepassi, lukasz kaiser, and noamshazeer.
2018. generating wikipedia by summariz-ing long sequences.
in proceedings of the 6th inter-national conference on learning representations..yang liu and mirella lapata.
2019. hierarchicaltransformers for multi-document summarization.
inproceedings oftheassociation for computational linguistics, page5070–5081..the 57th annual meeting of.
jordan j louviere, terry n flynn, and anthony al-fred john marley.
2015. best-worst scaling: the-ory, methods and applications.
cambridge univer-sity press..rada mihalcea and paul tarau.
2004. textrank: bring-ing order into text.
in proceedings of the 2004 con-ference on empirical methods in natural languageprocessing, pages 404–411..ramesh nallapati, bowen zhou, cicero dos santos,caglar gulcehre, and bing xiang.
2016. abstrac-tive text summarization using sequence-to-sequencein proceedings of the 20thrnns and beyond.
signll conference on computational natural lan-guage learning, page 280–290..adam paszke, sam gross, francisco massa, adamlerer, james bradbury, gregory chanan, trevorkilleen, zeming lin, natalia gimelshein, lucaantiga, et al.
2019. pytorch: an imperative style,in ad-high-performance deep learning library.
vances in neural information processing systems,pages 8026–8037..michael paul, chengxiang zhai, and roxana girju.
2010. summarizing contrastive viewpoints in opin-ionated text.
in proceedings of the 2010 conferenceon empirical methods in natural language process-ing, pages 66–76..romain paulus, caiming xiong, and richard socher.
2018. a deep reinforced model for abstractive sum-marization.
in proceedings of the 6th internationalconference on learning representations..laura perez-beltrachini, yang liu, and mirella lapata.
2019. generating summaries with topic templatesand structured convolutional decoders.
in proceed-ings of the 57th annual meeting of the associationfor computational linguistics, page 5107–5116..ratish puduppully, li dong, and mirella lapata.
2019.data-to-text generation with content selection andplanning.
in proceedings of the 33th aaai confer-ence on artiﬁcial intelligence, pages 6908–6915..abigail see, peter j. liu, and christopher d. manning.
2017. get to the point: summarization with pointer-generator networks.
in proceedings of the 55th an-nual meeting of the association for computationallinguistics, pages 1073–1083..yuanhang su, kai fan, nguyen bach, c.-c. jay kuo,and fei huang.
2019. unsupervised multi-modalin proceedings of theneural machine translation.
ieee conference on computer vision and patternrecognition, pages 10482–10491..quoc-tuan truong and hady lauw.
2019. multimodalreview generation for recommender systems.
in pro-ceedings of the world wide web conference, pages1864–1874..398ashish vaswani, noam shazeer, niki parmar, jakobuszkoreit, llion jones, aidan n gomez, łukaszkaiser, and illia polosukhin.
2017. attention is allyou need.
in advances in neural information pro-cessing systems, pages 5998–6008..nam n vo and james hays.
2016. localizing and ori-enting street views using overhead imagery.
in pro-ceedings of the european conference on computervision, pages 494–509..thomas wolf, lysandre debut, victor sanh, julienchaumond, clement delangue, anthony moi, pier-ric cistac, tim rault, r´emi louf, morgan funtow-icz, joe davison, sam shleifer, patrick von platen,clara ma, yacine jernite, julien plu, canwen xu,teven le scao, sylvain gugger, mariama drame,quentin lhoest, and alexander m. rush.
2020.transformers: state-of-the-art natural language pro-cessing.
in proceedings of the 2020 conference onempirical methods in natural language processing:system demonstrations, pages 38–45..tianyi zhang, varsha kishore, felix wu, kilian qweinberger, and yoav artzi.
2020. bertscore: eval-uating text generation with bert.
in proceedings ofthe 8th international conference on learning repre-sentations..hao zheng and mirella lapata.
2019. sentence cen-trality revisited for unsupervised summarization.
inproceedings of the 57th annual meeting of the asso-ciation for computational linguistics, pages 6236–6247..renjie zheng, mingbo ma, and liang huang.
2018.multi-reference training with pseudo-references forin proceed-neural translation and text generation.
ings of the 2018 conference on empirical methodsin natural language processing, pages 3188–3197..junnan zhu, haoran li, tianshang liu, yu zhou, ji-ajun zhang, and chengqing zong.
2018. msmo:multimodal summarization with multimodal output.
in proceedings of the 2018 conference on empiri-cal methods in natural language processing, page4154–4164..junnan zhu, yu zhou, jiajun zhang, haoran li,chengqing zong, and changliang li.
2020. multi-modal summarization with guidance of multimodalreference.
in proceedings of the 34th aaai confer-ence on artiﬁcial intelligence, pages 9749–9756..399a appendix.
a.1 dataset preprocessing.
we selected businesses and products with a mini-mum of 10 reviews and popular entities above the90th percentile were removed.
the minimum andmaximum length of the words were set as 35 and100 for yelp, and 45 and 70 for amazon, respec-tively.
we set the maximum number of tokens as128 using the bart tokenizer for training, and wedid not limit the maximum tokens for inference.
for the amazon dataset, we selected 4 categories:electronics; clothing, shoes and jewelry; homeand kitchen; health and personal care.
as yelpdataset contains unlimited number of images foreach entity, we did not use images for popular enti-ties above the 90th percentile.
on the other hand,amazon dataset contains a single image for eachentity.
therefore, we did not use images only whenmeaningless images such as non-image icon or up-date icon were used or the image links had expired.
for yelp dataset, we selected name, ratings, cat-egories, hours, and attributes among the metadata.
we used the hours of each day of the week as sevenﬁelds and used all metadata contained in attributesas each ﬁeld.
for some attributes (‘ambience’,‘businessparking’, ‘goodformeal’) that have sub-ordinate attributes, we used each subordinate at-tribute.
among the ﬁelds, we selected 47 ﬁeldsused by at least 10% of the entities.
we set themaximum number of categories as 6 based on the90th percentile, and averaged the representations ofeach category.
for ratings, we converted it to binarynotation consisting of 4 digits (22, 21, 20, 2−1).
forhours, we considered (open hour, close hour) asa 2-dimensional vector, and conducted k-meansclustering.
we selected four clusters based on sil-houette score: (16.5, 23.2), (8.7, 17.1), (6.4, 23),and (10.6, 22.6).
based on the clusters, we con-verted hours into a categorical type..for amazon dataset, we selected six ﬁelds:name, price, brand, categories, ratings, and descrip-tion.
we set the maximum number of categoriesas 3 based on the 90th percentile, and averaged therepresentations of each category.
furthermore, aseach category consists of hierarchies with a maxi-mum of 8 depths, we averaged the representationsof hierarchies to get each category representation.
for price and ratings, we converted them to binarynotation consisting of 11 and 4 digits, respectively,after rounding them to the nearest 0.5 to containdigit for 2−1.
as some descriptions consist of many.
pipeline steptext pretrainothers pretrainmultimodal train.
batch16328.epochs warmup.
5205.
0.510.25.lr5e-051e-041e-05.
table 5: hyperparameter values for each step in modeltraining pipeline..tokens, we set the maximum number of tokens as128. we regarded each token in description as eachﬁeld, so we got total 5 + 128 ﬁelds..a.2 experimental details.
our image encoder is based on resnet101.
resnet101 is composed of 1 convolution layer,4 convolution layer blocks, and 1 fully connectedlayer block.
among them, 4 convolution layerblocks play an important role in analyzing image.
through each convolution layer block, the size ofthe image feature map is reduced to 1/4, but it getshigh-level features.
to maintain the ability to ex-tract low-level features of the image, we set themodel weights up to the second convolution layerblock not to be trained further.
we only used upto the third convolution layer block to increase theresolution of feature maps without using too high-level features for image classiﬁcation.
in this way,li was set to 14 × 14 and ei was set to 1,024..to use the knowledge of text modality in tableencoder, we obtained ﬁeld name embeddings bysumming the bart token embeddings for the to-kens contained in the ﬁeld name.
because var-ious data types can be used for ﬁeld value, weused different processing methods for each datatype.
nominal values were handled in the sameway as the ﬁeld name.
binary and ordinal valueswere processed by replacing them with nominal val-ues of corresponding meanings: ‘true’ and ‘false’were used for binary values, and ‘cheap’, ‘aver-age’, ‘expensive’, and ‘very expensive’ were usedfor ‘restaurantspricerange’.
numerical valueswere converted to binary notation, and we obtainedthe representations by summing embeddings cor-responding to the place, where the place value is1. for other categorical values, we simply trainedembeddings corresponding to each category..we set each hyperparameter value different foreach step in the model training pipeline, as intable 5. we set the batch size according to thememory usage and set other values according tothe amount of learning required.
hyperparameterranges for epochs and lr (learning rate) were [3, 5,10, 15, 20] and [1e-03, 1e-04, 5e-05, 1e-05, 5e-06],.
400modelsself & controlcopycatmultimodalsumgold.
grammaticality-0.5170.1630.367-0.013.coherence-0.500-0.0770.2900.287.overall-0.527-0.1130.2600.380.modelsuntrainedtripletpivot (ours).
r-121.0320.0625.87.imager-22.452.493.62.r-l14.1713.1515.70.r-124.0425.6727.32.tabler-22.923.524.12.r-l15.1015.1616.57.table 6: human evaluation results in terms of the bwson the yelp dataset..table 7: reference reviews generation results on theyelp dataset..respectively, and optimized values were chosenfrom validation loss in one trial.
for summarygeneration at test time, we set different hyperpa-rameter values for each dataset.
beam size, lengthpenalty, and max length were set to 4, 0.97, and105 for yelp and 2, 0.9, and 80 for amazon, re-spectively.
note that max length was set ﬁrst toprevent incomplete termination and length penaltywas determined based on the rouge scores onvalidation dataset.
the number of training parame-ters for text, image, and table modality pretrainingare 406.3m, 27.1m, and 3.2m, respectively, andthat for multimodal training is 486.9m.
run timefor text modality pretraining was 16h on 4 gpus,and it took 41h and 43h on 2 gpus for image andtable modality training, respectively.
for ﬁnal mul-timodal training, it took 14h on 8 gpus..a.3 human evaluation.
for human evaluation, we randomly selected 30entities from yelp test data, and used three criteria:grmmaticality (the summary should be ﬂuent andgrammatical), coherence (the summary should bewell structured and well organized), and overall(based on your own criteria, select the best andthe worst summary of the reviews).
results forthree criteria are shown in table 6. self & controlachieved very poor performance for all criteria dueto its ﬂaws that were not revealed in the automaticevaluation.
surprisingly, multimodalsum outper-formed gold summaries for two criteria; however,its overall performance lagged behind gold.
asour model was initialized from bart-large thathad been pretrained using large corpus and fur-ther pretrained using training review corpus, it mayhave generated ﬂuent and coherent summaries.
itseems that our model lagged behind gold in over-all due to various criteria other than those two.
thefact that gold scored lower than copycat in gram-maticality may seem inconsistent with the resultfrom braˇzinskas and titov (2020).
however, weassumed that this result was due to a combinationof the four models in relative evaluation.
the rank-ing for copycat and gold may have changed inabsolute evaluation..a.4 analysis on other modalities.
pretraining.
to analyze the various models for the other modal-ities pretraining, we evaluated the performance ofthe reference review generation task that gener-ates corresponding reviews from images or a ta-ble.
for evaluation, we used the data that werenot used for training data: we left 10% of the datafor yelp and 5% for amazon.
we chose two com-parison models: untrained and triplet.
untraineddenotes the model that image encoder or table en-coder keeps untrained.
this option indicates thelower bound containing only the effect of the textdecoder.
triplet denotes the triplet-based metric-learning model, based on lee et al.
(2018) and voand hays (2016).
for triplet (images or a table,reviews of positive entity, reviews of negative enti-ties), we trained the image or table encoder basedon the pretrained text encoder, by placing the im-age or table encoded representations close to thepositive reviews representations and far from thenegative reviews representations.
note that pre-trained text encoder was not trained further..results on the other modalities pretraining areshown in table 7. for each model, the pretraineddecoder generated a review from image or tableencoded representations.
we measured the averagerouge scores between the generated review andn reference reviews.
the ﬁrst ﬁnding was thatresults of table outperformed those of image.
itindicates that table has more helpful informationfor generating reference review.
the second ﬁnd-ing was that our method based on the text decoderoutperformed the triplet based on the text encoder.
especially, triplet achieved very poor performancefor image because it is hard to match m images ton reference reviews for metric learning.
on thecontrary, our method achieved much better perfor-mance by pivoting the text decoder.
triplet showedgood performance on table because it is relativelyeasy to match 1 table to n reference reviews; how-ever, our method outperformed it.
we concludethat our method lets the image and table encoderget proper representations to generate referencereviews regardless of the number of inputs..401a.5 example summaries.
table 8, 9 show sample summaries generated from our model and baseline models on yelp and amazondatasets.
full summaries from our model are available at https://bit.ly/3br4yod..review 1.review 2.review 3.review 4.review 5.review 6.review 7.review 8.copycat.
self & control.
multimodalsum.
gold.
the fresh water catﬁsh is probably the best i’ve every had.
the service was outstanding.
i wouldrecommend this little secret to everyone.
i loved everything about this place!!
great food, great decor, and great service.
the best collard greens ihave ever had.
we had fried oysters for a starter and although i have never had them before so i havenothing to compare them with they were very tasty.
the warm hush puppies with the honey butter wasdelicious!!
i had the crab legs which were perfect and plentiful.
my sister had the all you can eat friedcatﬁsh that was also cooked perfectly.
a great experience all around!!
amazing food and great service!
the hospitality was out of this world.
will deﬁnitely be back soon.
the wait was less than 5 minutes at 7pm on a friday night, amazing!!
the staff was very kind and thewaitresses were very attentive and helpful.
we tried the frog legs, catﬁsh, alligator bites, crab legs, gumboand of course the hush puppies!
everything was outstanding.
what a hidden gem!
i love this place the food amazing the staff helpful ....must try green tomatos ...fresh water ﬁsh ;ˆ)we love this place the catﬁsh is good the hush puppies with that honey butter are awesome the frenchfries the gumbo what else is good there the alligator tail mostly everything on the menu.
i guess the onlybad thing i can say is sometimes it’s like a 20 minute wait in the drive-through but it’s well worth itwhen your food is hot because tonight i got to go home and warm it up it’s not hot enough, even thoughthey’re still open for another hour that was a bummerreally tasty catﬁsh, shrimp and ﬁxin’s.
our friend took us to the sister location on nellis a couple ofmonths ago, but this location was more convenient to our hotel.
no worries, this place was just as good!
excellent service, and the salad bar is a nice touch as well.
as a bostonian, i’m pretty particular aboutseafood.
the hush puppy ﬁts the bill.
very satisﬁed!
first time here and the food, staff was awesome.
manager came over and gave us samples of the friedcatﬁsh, super nice.
i never eat catﬁsh.
it’s nasty to me until i tasted the saltwater catﬁsh!!!
greens are on point.
thehushpuppy are bomb with honey butter!!!!!
gator bites where are ok.this place is awesome!
the food was great, the service was great.
we had the catﬁsh po’boy and it wasdelicious.
the only reason i didn’t give 5 stars is because of the fact that they don’t deliver.
i love this place.
the service is awesome.
the hush puppies are to die for.
i love the honey butter.
i can’twait to go back and try it again.
the only thing i don’t like about the place is the wait.
it can be a littlelong, but it’s worth it.
it’s a little on the pricey side, but you’re getting what you pay for.
love the hotbutter, the hush puppies, the french fries, the gumbo, the catﬁsh and the gumbo.
everything is so yummyand the service is top notch.
try it out, you won’t be disappointed.
this place is a hidden gem.
the food is great and the service is even better.
i had the all you can eatcatﬁsh and it was delicious.
the hush puppies are the best i’ve ever had.
i will deﬁnitely be back.
yummy and delicious catﬁsh.
you gotta try it.
friendly staff and service is good too.
you can tell theyknow their seafood and how to prepare and cook it to perfection.
the staff also answered any questions ihad.
the hush puppies are tasty too..table 8: yelp summaries generated by different models..402review 1.review 2.review 3.review 4.review 5.review 6.review 7.review 8.copycat.
gold 1.gold 2.gold 3.multimodalsum.
i usually wear size 37, but found a 38 feels better in this sandal.
i absolutely love this sandal.
sosupportive and comfortable, although at ﬁrst i did get a blister on my big toe.
do not let this be thedeciding factor.
it stretched out and is now fabulous.
i love it so much that i bought it in three colors.
this is a really cute shoe that feels very comfortable on my high arches.
the strap on the instep ﬁts myfeet very well, but i have very slim feet.
i can see how it would be uncomfortably tight on anyone withmore padding on their feet.
i love these sandals.
the ﬁt is perfect for my foot, with perfect arch support.
i don’t think the leather ischeap, and the sandals are very comfortable to walk in.
they are very pretty, and pair very well withpants and dresses.
my wife is a nurse and wears dansko shoes.
we were excited to try the new crimson sandal and normallyorder 39 sandal and 40 closed toe.
some other reviews were right about a narrow width and tight toe box.
we gave them a try and passed a great pair of shoes to our daughter with her long narrow feet, and sheloves them...finally, a dansko sandal that’s fashion forward!
it was love at ﬁrst sight!
this is my 4th dansko purchase.
their sizing, quality and comfort is very consistent.
i love the stying of this sandal and i’m pleased theyare offering bolder colors.
another feature i love is the dri-lex topsole - it’s soft and keeps feet dry.
i really love these sandals.
my only issue is after wearing them for a while my feet started to swell as ihave a high instep and they were a little tight across the top.
i’m sure they will stretch a bit after a fewwearsi have several pairs of dansko clogs that are all size 39 and ﬁt perfectly.
so i felt conﬁdent when iordered the tasha sandal in size 39. i don’t know if a 40 would be too large but the 39 seems a littlesmall.
otherwise, i love them.
they are very cushiony and comfortable!
i own many dansko shoes and these are among my favorites.
they have all the support that danskooffers in its shoes plus they are very attractive.
i love the the heel height and instant comfort.
they lookgreat with slacks and dresses, dressed up or not...this is my second pair of dansko clogs and i love them.
they are very comfortable and i can wear themall day without any discomfort.
i would recommend them to anyone looking for a comfortable sandal.
i love these sandals.
they are very comfortable and look great.
the only thing i don’t like is that they area little tight across the top of my foot.
i have a high instep and the strap is a little too tight.
i am hopingthey will stretch out a bit.
i love these sandals, dansko has made a really great product!
i had to return my ﬁrst pair (39) for beinga bit tight and small, but i went a size higher (40) and it is perfect, they are so comfortable!
if they dostretch out like other reviews say, they will still ﬁt and look great.
i love these dansko tasha sandals!
they are comfortable and the style is really cute.
the only warning ihave is that they seem to run narrow: you may want to buy a larger size if you have wide feet.
also, theyseem to stretch as you wear them, so don’t get discouraged by a few blisters on ﬁrst wearing.
these dansko shoes are amazingly comfortable and hug the shape of my feet well, but i did have to wearthem for a bit to stretch them out.
they felt a little tight at ﬁrst, but now they are perfect.
i feel they’retrue to size so i’d recommend ordering these in your normal shoe size..table 9: amazon summaries generated by different models..403