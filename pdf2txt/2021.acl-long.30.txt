pass: perturb-and-select summarizer for product reviews.
nadav oved ∗technion - israel institute of technologyhaifa, israelnadavo@campus.technion.ac.il.
ran levyamazontel aviv, israelranlevy@amazon.com.
abstract.
the product reviews summarization task aimsto automatically produce a short summary fora set of reviews of a given product.
suchsummaries are expected to aggregate a rangeof different opinions in a concise, coherentand informative manner.
this challengingtask gives rise to two shortcomings in existingwork.
first, summarizers tend to favor genericcontent that appears in reviews for many dif-ferent products, resulting in template-like, lessinformative summaries.
second, as reviewersoften disagree on the pros and cons of a givenproduct, summarizers sometimes yield incon-sistent, self-contradicting summaries.
wepropose the pass system (perturb-and-selectsummarizer) that employs a large pre-trainedtransformer-based model (t5 in our case),which follows a few-shot ﬁne-tuning scheme.
a key component of the pass system relieson applying systematic perturbations to themodel’s input during inference, which allowsit to generate multiple different summaries perproduct.
we develop a method for rankingthese summaries according to desired criteria,coherence in our case, enabling our systemto almost entirely avoid the problem of self-contradiction.
we compare our system againststrong baselines on publicly available datasets,and show that it produces summaries whichare more informative, diverse and coherent.1.
1.introduction.
online shopping has become a popular form ofpurchasing goods even before the most recent ac-celeration due to the covid-19 pandemic.
as e-commerce websites strive to make the shoppingprocess more useful and enjoyable for customers,many interesting challenges arise.
one challengedeals with how to surface opinions from product.
∗completed during an internship at amazon.
1summaries generated by pass are available at: https:.
//registry.opendata.aws/.
reviews in a concise yet reliable fashion.
theresearch community has addressed this challengeearly on, starting from the work of (hu and liu,2004) which deﬁned the task of mining and sum-marizing customer reviews.
more recent advance-ments have relied on modern deep learning mod-els trained on large collections of unannotated cus-tomer reviews (brazinskas et al., 2020b,a)..our ﬁrst observation relates to the summariesgenerated by copycat (brazinskas et al., 2020b)and fewsum (brazinskas et al., 2020a), two ofthese sota systems, which tend to mix genericstatements such as “would recommend this prod-uct to anyone” along with more informative con-tent such as “the sound quality is good” (see ta-ble 6 in appendix b for examples of such gen-erated summaries).
due to the emphasis of sum-marization systems on conciseness, we maintainthat generic content should be used sparingly.
ad-ditionally, even if the content is not extremelygeneric, customers may perceive summaries asless useful if they tend to repeat themselves acrossin order to estimate the similarity be-products.
tween summaries generated for different prod-ucts, we devise the set-pairwise-rouge met-ric (henceforth denoted as spr), that computesthe average rouge (lin, 2004b) scores of sum-maries for two different products, across all prod-uct pairs.
using this metric we show that humanwritten reference summaries are indeed far morediverse than their system generated counterparts,i.e.
the spr of reference summaries is signiﬁ-cantly lower.
we henceforth denote the notionof cross product diversity of summaries as cp-diversity..large pre-trained transformer-based (vaswaniet al., 2017) models such as openai’s gpt-3(brown et al., 2020), google’s t5 (raffel et al.,2020), pegasus (zhang et al., 2020a), and face-book’s bart (lewis et al., 2020) have made com-.
proceedingsofthe59thannualmeetingoftheassociationforcomputationallinguisticsandthe11thinternationaljointconferenceonnaturallanguageprocessing,pages351–365august1–6,2021.©2021associationforcomputationallinguistics351pelling advancements on a host of nlg tasks, in-in thiscluding abstractive text summarization.
work we wish to leverage such models for prod-uct reviews summarization, aiming to generallyimprove the quality of generated summaries, andspeciﬁcally in terms of their diversity across dif-ferent products.
while we aim to generate human-like texts, care has to be taken with respect toindeed, concerns have beentheir correctness.
raised regarding the factual consistency of abstrac-tive summaries, i.e., whether the facts conveyedin the summary agree with the source text (caoet al., 2018; kryscinski et al., 2019; maynez et al.,2020)..our second observation relates to this issue offactual consistency in the context of product re-views summarization.
our task not only faces therisk of models hallucinating incorrect information,as in traditional abstractive text summarization,but also the risk of generating self-contradictingsummaries which are not caused by model hallu-cinations.
the latter can occur when the sourcedocuments contradict one another.
this situationis quite likely because reviews may disagree onsome product aspects or even disagree entirely.
for example, review a states a machine is “easyto operate” vs. review b which states it “requirestrial and error” (see more examples in table 7 inappendix b).
in this unique setup, factual consis-tency is undeﬁned and instead we wish to measurethe self-consistency ofa different characteristic:the summary.
to the best of our knowledge this is-sue has not been analyzed in the past and in somesense it renders the task ill-deﬁned because it’s notclear whether the summary is supposed to conveya range of possibly contradicting opinions aboutthe product or the majority opinion.
from here on,we shall assume that a summary has to convey themajority opinion of the reviews and do so in a self-consistent manner..our proposed method starts by ﬁne-tuning astrong pre-trained language model for product re-views summarization in a few-shot setup.
we thenemploy an input perturbation method that dropsk reviews out of the input and concatenates theremaining reviews in random order.
this pro-cess, denoted as lko, short for leave k out, pro-duces notable variation between candidate sum-maries, which increases the model’s output diver-sity.2 once we have produced a set of candidate.
summaries, we essentially cast our original sum-mary generation problem as a ranking problem.
this approach gives us the choice over what kindof summary we are interested in as the ﬁnal output,i.e.
choosing our ranking criteria.
as mentionedabove, our main concern in this work is producingself-consistent summaries.
instead of basing ourranking solely on this criterion, we train a moregeneral coherence summary ranker using humanannotated coherence scores (fabbri et al., 2021).
finally, for each product, we select the top rankedsummary as the system’s output..we compare our method against strong base-lines, comprised of systems introduced in previouswork on multi-document opinion summarization,and a t5 language model ﬁne-tuned for abstrac-tive text summarization.
we evaluate each over 3dimensions, of which relevance and coherence arecommonly used in summarization (dang, 2005),and our newly introduced metric for cp-diversity.
we demonstrate that our method produces highquality summaries which are more informative, di-verse and coherent..in summary, the main contributions of this workare:(1) highlight two shortcomings of existingproduct reviews summarizers, namely low cp-diversity and self-inconsistency, and propose adedicated metric for the former.
(2) propose amethod that leverages strong pre-trained modelsthat improve the cp-diversity while signiﬁcantlyreducing the risk of self-inconsistencies..2 related work.
product review summarization.
product re-view summarization is a form of multi-documentsummarization in which a set of product reviewsfor a single product serves as the document clusterto be summarized.
a common approach for prod-uct review summarization, which centers the sum-mary around a set of extracted aspects and theirrespective sentiment, is termed aspect-based sum-marization (hu and liu, 2004; kansal and toshni-wal, 2014; wu et al., 2016; angelidis and lapata,2018; coavoux et al., 2019)..as in traditional summarization, there are twoinherently different requirements for the task, asimpliﬁed one, in which the goal is to providean extractive output, i.e., a list of sentences ex-tracted from the review set, or a more advancedone, in which the goal is to provide an abstrac-.
2diversity here is between candidate summaries for the.
same product, not to be confused with cp-diversity..352tive output, i.e., generated content not restrictedto use the same wording of the source set.
ex-tractive summarization include earlier works suchas (carenini et al., 2006; lerman et al., 2009;xiong and litman, 2014).
more recently, (tanet al., 2017) suggested a novel generative topicaspect sentiment model, while (angelidis et al.,2021) suggested a novel system able to extractboth general and aspect-speciﬁc summaries.
asfor abstractive summarization, recent advances onpre-training neural networks were explored in thecontext of product reviews in unsupervised andfew-shot learning schemes which led to promis-ing results (chu and liu, 2019; brazinskas et al.,2020b,a; suhara et al., 2020; amplayo et al.,2021)..evaluating summarization systems.
evalua-tion of summarization systems is usually per-formed utilizing a mix of automatic metrics andhuman ratings.
among the automated metrics,probably the most well-known is the rougefamily of scores (lin, 2004b) that measures n-gram overlap between generated summaries andcorresponding reference summaries.
many othermetrics that aim to quantify how well generatedsummaries align with reference summaries havebeen proposed, such as bleu (papineni et al.,2002), meteor (lavie and agarwal, 2007),rouge-we (ng and abrecht, 2015) and bert-score (zhang et al., 2020b) to name a few.
unfor-tunately, such metrics alone do not tell the wholestory and recently several works observed that anew requirement is necessary in order to ensurethat facts from the summary agree with the sourcedocument (cao et al., 2018; kryscinski et al.,2019; maynez et al., 2020).
this requirement isusually known as factual consistency.
as for hu-man ratings, those are usually obtained across sev-eral dimensions of summary quality.
the duc2005 task (dang, 2005) suggested the following5 dimensions: grammaticality, non-redundancy,referential clarity, focus and structure, and co-herence..in the context of product reviews summariza-tion (brazinskas et al., 2020a) use the standardrouge-1/2/l metrics as well human comparativejudgments on 5 dimensions: fluency, coherence,non-redundancy, informativeness and sentiment.
to the best of our knowledge the issues of self-consistency and diversity across products were notdirectly analyzed before..3 perturb-and-select summarizer.
in this section, we propose a system that employsa large pre-trained transformer-based model (t5)in a few-shot ﬁne-tuning scheme for multiple re-views abstractive summarization.
we aim to lever-age the inherent diversity between reviews for agiven product to our advantage, by applying sys-tematic perturbations to the model’s input duringinference.
this allows our ﬁne-tuned model togenerate multiple different candidate summariesper product, exhibiting variability both in the con-tent being surfaced as well as in the phrasing ofsaid content.
we develop a ranking mechanismfor selecting the best candidate summary accord-ing to desired criteria, which in our case is coher-ence.
we provide an end-to-end diagram of thepass summarizer’s components in figure 1..3.1 fine-tuning t5 for summary generation.
pass relies on a pre-trained t5 language model,which we ﬁne-tuned on a small publicly avail-able dataset for product reviews summarization(brazinskas et al., 2020a).
we follow a simi-lar ﬁne-tuning scheme for abstractive text sum-marization to the one presented in (raffel et al.,2020) with the exception that we concatenate themultiple reviews into a single input text as a pre-processing step.
as the dataset contains multiplereference summaries per product, we repeat ourtraining process for each reference summary usingthe same (concatenated) input text..3.2 candidate summary generation.
in light of the natural diversity existing betweenproduct reviews, we explore a modeling approachwhich allows for such diversity to emerge in oursummarizer’s output as well.
we do this by ma-nipulating the model’s input, sampling which re-views to use each time, in a way that allows for in-creasing the relative prevalence of certain reviewsover others.
we also re-shufﬂe the reviews beforeconcatenation to ensure the model is not affectedby their internal order.
note that prior attemptshave been made to directly manipulate the contentwithin the reviews (amplayo and lapata, 2020) apath that we do not explore here.
our interventionmethod guarantees that each review’s correctness,integrity and meaning are preserved.
since it onlyaffects the subset of reviews being used and theirorder of concatenation, this increases the poten-tial for diversity (per product and across products).
353figure 1: a diagram of the pass components, with an example for a collection of reviews of size d = 4, k = 1..emerging from the input’s content, without com-promising its linguistic quality..k.lko input perturbation method.
given a setof d reviews r = {r1, ..., rd} for a product p,our perturbation method iterates over a(r) theset of all possible subests of size d − k in r,a(r) = (cid:8)s(cid:12)(cid:12)s ⊂ r, |s| = d − k, 1 ≤ k < d(cid:9).
given a subset s ∈ a(r) we concatenate its re-views in random order, and feed the concatenatedtext into our ﬁne-tuned t5 summarizer, whichgenerates a candidate summary c. we repeat thisstep for all s ∈ a(r), resulting in a set of gen-erated candidate summaries which we denote as(cid:1).
this process, de-c = {c1, ..., cm}, m = (cid:0)dnoted as lko, short for leave-k-out, produces no-table variation between candidate summaries (seetable 8 in appendix b for examples), and allowsfor different content and aspects to emerge in thesummaries, which were less likely to have sur-faced otherwise.
we found that this perturbationapproach produces higher variation across candi-date summaries when applying it on the model’sinput only during the inference stage, not duringtraining.
our method produces multiple perturbedversions of a given input while its references re-main the same.
if applied during training, thismight encourage the model to ﬁt a larger range ofinput features to a smaller set of outputs.
we areinterested in the opposite effect - we would like toencourage higher output variation as a function ofinput diversity..note that when dealing with large review sets,achieving diversity does not require iterating overall subsets in a(r).
for such scenarios, we rec-ommend constructing a ﬁxed number (m) of ran-domly sampled review subsets, so long as m is.
sufﬁciently large.
in our experiments we em-ploy the full lko input perturbation method, sincestandard datasets focus on relatively small reviewsets.3.
an alternative method for increasing noveltyand variability in the output of a generative lan-guage model, is to directly intervene in its decod-ing algorithm, e.g., beam search (vijayakumaret al., 2016; cibils et al., 2018).
note that this willnot have the same effect as our proposed approach.
first, since beam search is a decoding algorithm, itonly has access to the underlying language model,and is completely separated from the model’s in-put.
second, beam search’s mechanism is ﬁxedto make local word-by-word decisions, before thecomplete summary is revealed.
finally, our ap-proach guarantees that given a set of input texts, atleast one candidate output will not be inﬂuencedat all by a speciﬁc input text (or more if k > 1).
for example, if a set of 4 reviews contains 3 re-views discussing price, and 1 review discussingquality, our method guarantees that at least 1 can-didate summary will be generated solely based onthe ﬁrst three (discussing price).
furthermore, ourmethod increases the probability for a summary tomention both price and quality, when a review dis-cussing price is left out..3.3 candidate summary ranking.
once a set of candidate summaries are generatedper product, we have essentially cast our summarygeneration problem as a summary ranking prob-lem.
this allows us to retrieve a summary, whichranks best out of a diverse set of candidates, ac-cording to desired, interpretable criteria..3a few recent works attempt to explicitly address this is-.
sue (shapira and levy, 2020; angelidis et al., 2021)..354as mentioned in section 1, our main concern isproducing cp-diverse yet self-consistent and co-herent summaries.
since our input perturbationmethod generates multiple candidate summaries,we are now left with the task of ranking this set bycoherence.
we would like the ranking process toﬁlter out self-contradicting, incoherent or incon-sistent candidates (by assigning low rank) and topromote well-formed, coherent candidates to thetop of the list.
to achieve this, we train a classi-ﬁer that receives two summaries as input and de-cides whether the ﬁrst summary is more coherentthan the second or the opposite.
the classiﬁer canalso decide that both summaries are equally co-herent.
using such a classiﬁer, we can obtain apartial ranking of the reviews by running all pair-wise comparisons and count the number of timeseach summary was better than the summary it waspaired with..pairwise summary classiﬁer.
we train amodel to classify a pair of summaries for coher-ence, by ﬁne-tuning a pre-trained t5 model forpairwise text classiﬁcation.
given a pair of sum-maries, the model is required to classify them aseither: summary a is more coherent, summary bis more coherent, or a and b are equivalent interms of coherence.
a pair of summaries can of-ten be considered equivalent when judging themaccording to speciﬁc criteria, stemming from thenatural fact that often more than one summarycan be considered correct or good.
indeed it hasbeen shown that several reference summaries areneeded for reliable evaluation showing that thereis more than one truth (lin, 2004a).
since thismodel is used as a comparator for ranking can-didate summaries, we are especially sensitive tospeciﬁc types of classiﬁcation errors.
if the modelmistakenly classiﬁes a summary to be more coher-ent than the other while the opposite is true, weconsider this a critical classiﬁcation error.
thistype of error could be detrimental to the validity ofthe ranking process, therefore we aim to minimizeits rate.
while other types of errors also reduce theclassiﬁer’s accuracy, we consider a mistake wherethe model classiﬁes two summaries to be equiva-lent when in truth one is more coherent than theother, as less harmful for ranking purposes..ranking method.
ourrankingmethod iterates over all possible pairs of candi-date summaries for a given product, and counts.
proposed.
how many times each candidate was classiﬁedby the coherence pairwise classiﬁer (our primarycomparator), as more coherent than its counter-part.
as a tie-breaking, secondary comparator, wetrain an additional pairwise summary classiﬁer, toclassify which candidate is more ﬂuent, out of apair of given candidates.
we select the top rankedcandidate as the ﬁnal output summary for eachproduct..4 experimental setup.
4.1 data.
we utilize a recent publicly available amazonproduct reviews summarization dataset (brazin-skas et al., 2020a) for ﬁne-tuning the t5 modelwhich underlines the pass system and for evalu-ating the lko input perturbation method, both inisolation and as part of the end-to-end pass sys-tem.
the dataset contains product reviews andreference summaries for 60 products on ama-zon.
each product has 8 reviews and 3 refer-ence summaries written by crowd source work-ers.
we follow the dataset splits to the training,development and test sets provided by the authorsof the dataset.
while we mainly focus on prod-uct reviews summarization, we include the yelpbusiness reviews summarization dataset (also from(brazinskas et al., 2020a)) in our end-to-end eval-uation for the sake of completeness.
the yelpdataset contains business reviews and referencesummaries for 100 businesses..for training and evaluating the pairwise coher-ence classiﬁer, we utilize a public dataset of hu-man annotated summaries (fabbri et al., 2021),generated by 16 modern text summarization mod-els for 100 news articles (1600 examples in to-tal) from the cnn/dailymail dataset (hermannet al., 2015).
each summary was rated (on a scaleof 1 to 5) across 4 dimensions: coherence, con-sistency, ﬂuency and relevance, by 5 independentcrowd source workers and 3 independent experts(8 annotations in total).
we chose to use the ex-perts’ annotations only, as they are considered tobe more accurate and reliable for coherence andﬂuency (fabbri et al., 2021).
we construct a pair-wise version of this dataset, by creating summarypairs from all 16 model outputs for each of the 100news stories, along with their annotation scores foreach metric respectively.
we split the dataset ac-cording to news stories, by randomly sampling 20stories for the test set, 16 stories for the develop-.
355ment set and the rest are used for the training set.
given a pair of summaries (a, b), their respectiveaverage expert rating, (ra, rb) and a threshold pa-rameter (cid:15), we deﬁne the label for that pair as:.
a,.
label(a, b) =.
if ra − rb ≥ (cid:15)if rb − ra ≥ (cid:15)otherwise.
b,.
e,.
.
where e denotes the case where both summariesare equivalent, a denotes that summary a is bet-ter than b and b denotes the opposite.
to ensurethat our training data is invariant to a pair’s internalorder, we create examples for all (a, b) and (b, a)pairs in the training set..4.2 experimental details.
fine-tuning t5 for summary generation.
weﬁne-tune a t5-base model (220m parameters(raffel et al., 2020)) for abstractive text summa-rization as described in 3.1 on the training set, andtune its hyperparameters on the development set.
we train for maximum 20 epochs while employ-ing a standard early stopping mechanism (falcon,2019) based on the development set’s average lossper epoch.
we ﬁne-tune a separate model for theamazon and yelp datasets.
hyperparameters andfurther details can be found in appendix a..lko input perturbation.
we experiment withthe lko method described in section 3.2 with k ∈{1, 2, 3, 4, 5} on the development set.
for the end-to-end system we choose k = 2 aiming to obtainhigh output diversity while limiting computationcomplexity, and avoiding the risk of dropping amajority of the reviews (k > 4) each time.
weprovide evaluation details in 5.1..pairwise summary classiﬁer.
we train twot5-base models to classify which summary is bet-ter, one in terms of coherence, to be used as ourranking method’s primary comparator, and one interms of ﬂuency to break ties.
we experimentedwith different values for (cid:15) ∈ {0.25, 0.5, 0.75, 1.0},and chose (cid:15) = 0.5 for the coherence classiﬁer and(cid:15) = 0.25 for the ﬂuency classiﬁer.
the choiceof (cid:15) was based on dataset statistics per metric andevaluation of each model’s performance on the de-velopment set..baselines.
we compare the pass system tofour baselines:.
generate a review given other reviews for the sameproduct.
the authors suggest a novelty mecha-nism that controls the extent to which the summarydeviates from the inputs..fewsum (brazinskas et al., 2020a) is a few-shot reviews summarizer that builds upon the ideasof copycat but also conditions the model on cer-tain linguistic properties such as writing style..t5 is the pre-trained t5-base language modelwhich was not ﬁne-tuned.
we do not report resultsfor this model, as it consistently performed worst.
t5-ft is the ﬁne-tuned t5-base model de-.
scribed above..we do not report results for meansum (chuand liu, 2019) since it was consistently outper-formed by fewsum (brazinskas et al., 2020a)..5 evaluation.
5.1 candidate summary generation.
recall that our main objective for generating can-didate summaries is to encourage output diversity.
hence, we would like to verify that our pertur-bation method, lko, produces sufﬁciently diversecandidates for a given product.
in order to measuretextual diversity between candidate summaries fora given product, we need to devise a diversity met-ric.
we propose the spr metric (shorthand forset-pairwise-rouge) which measures the oppo-site of diversity, i.e., the average lexical similar-ity across pairs of summaries from a given set.
we base spr on rouge f1 scores for any n-gram level, therefore spr-1 relies on rouge-1f1 scores and so on..spr formal deﬁnition.
for a given set ofsummaries s = {s1, ..., sn}, we deﬁne the set ofall pairs from s as p (s) = (cid:8){si, sj}(cid:12)(cid:12)si ∈ s, sj ∈s, i (cid:54)= j(cid:9).
we then deﬁne the set-pairwise-rouge(spr) metric as:.
sp r(s) =.
rou ge(si, sj).
(cid:88).
1|p (s)|.
·.
{si,sj }∈p (s).
note that spr is a general metric of diver-sity, applicable to an arbitrary set of summaries.
therefore, it can be applied to measure both ip-diversity (in-product diversity, as we do here) andcp-diversity (cross-product diversity, as we do insection 5.3).
for clarity, we shall denote ip-sprwhen measuring ip-diversity and cp-spr whenmeasuring cp-diversity with spr..copycat (brazinskas et al., 2020b) is an un-supervised reviews summarizer that is trained to.
figure 2 depicts a box plot of the ip-spr-2scores for k ranging from 1 to 5. we observe.
356dataset.
system length r-1.
r-l cp-spr-1 cp-spr-2 cp-spr-l coherence.
33.45copycatfewsum 52.5052.75t5-ft47.75pass.
27.8533.5637.0737.43.gold.
fewsumt5-ftpass.
gold.
49.82.
52.940.5852.15.
49.81.
–.
–.
r-2.
4.777.169.688.02.
–.
–.
18.8621.4923.4723.34.
–.
–.
37.2938.7236.91.
9.9210.268.12.
22.7624.4723.09.
36.2934.5425.5625.79.
19.48.
40.8238.9330.88.
24.41.amazon.
yelp.
14.1210.613.322.63.
1.61.
17.0913.056.35.
2.80.
29.5223.9317.3817.38.
13.00.
30.3429.5521.33.
15.98.
–-0.200-0.0500.150.
0.100.
0.050-0.2500.200.
0.000.table 1: end-to-end results on the amazon (top) and yelp (bottom) test sets.
r stands for average rouge f1scores with reference summaries, cp-spr for set-pairwise-rouge scores measuring cp-diversity and coher-ence for best-worst scaling scores, which range from -1 (unanimously worst) to +1 (unanimously best), on acrowdsourced human evaluation task..the biggest drop in similarity (increase in diver-sity) between k = 1 and k = 2. while we aimto increase diversity, we are also mindful of theincrease in runtime as k grows.
additionally, wewould like to avoid sampling out a majority of re-views (k > 4), since the risk of generating a sum-mary with minority view or low informativenessalso increases with k.indeed, as shown in fig-ure 3, which depicts a similar box plot but thistime of the rouge-2 scores against the referencesummaries, the variance increases with k and theworst-case rouge-2 score decreases with k..figure 2:ip-spr-2 scores (measuring ip-diversity)box plot, for all pairs of candidate summaries generatedwith lko input perturbation method for k = 1, ..., 5..while diversity is certainly not the only aspectfor evaluating generated summaries, we exploreother dimensions in the following sections..5.2 candidate summary ranking.
the pairwise summary classiﬁers can be evaluateddirectly using human scores from (fabbri et al.,2021) after adapting them to our ternary classiﬁca-tion task.
figure 4 depicts the confusion matrix for.
figure 3: rouge-2 f1 scores box plot, for all candi-date summary sets generated with lko input perturba-tion method for k = 1, ..., 5..our coherence classiﬁer.
we observe that the esti-mated probability of a critical error (choosing aover b or b over a) is very low, 0.05, while at thesame time the overall accuracy of 0.61 is reason-ably high compared to 0.33 and 0.36 achieved bythe random and majority (always predicts that aand b are equally coherent) baselines respectively.
applying the classiﬁer to a set of 28 candidates perproduct, yields a single top ranking candidate for70% of products in the amazon test set..to further break ties, we utilize the ﬂuency clas-siﬁer as a secondary comparator.
see figure 10in appendix c for a similar confusion matrix forthe ﬂuency classiﬁer.
again, the probability for acritical error is very low, 0.0125, while the overallaccuracy is 0.67. after applying ﬂuency as a tiebreaker, we ﬁnd that all products in the amazontest set have a unique top ranking summary..the training data for both classiﬁers comesfrom a domain (news articles) which is differ-ent from our main dataset’s domain (product re-.
357views).
we hypothesize that coherence and ﬂu-ency are linguistic properties that are not heavilytied with the domain, since they relate to a sum-mary’s overall collective and individual sentenceindeed, our results showquality (dang, 2005).
(see table 2) that pass beneﬁted from this datadespite the risk of a possible domain shift.4.
figure 4: confusion matrix for the coherence pairwiseclassiﬁer..5.3 end-to-end system.
we evaluate our end-to-end system across 3 di-mensions.
the ﬁrst, informativeness, is tradition-ally evaluated using the rouge-1/2/l f1 mea-sures (lin, 2004b) and we follow suit.
the seconddimension, which subsumes the self-consistencyissue, is coherence.
to this end, we conducted acrowdsourced human evaluation task, which com-pares between the generated summaries of 4 dif-ferent summarization systems, including our pro-posed pass system.
we used best-worst scaling(louviere and woodworth, 1991; louviere et al.,2015; kiritchenko and mohammad, 2016, 2017)to compute each system’s score as the differencebetween the percentage of times it was selected asbest, and the percentage of times it was selectedas worst (orme, 2009).
this is inline with priorwork on product review summarization (brazin-skas et al., 2020b,a).
as for our third dimension,recall that we would like our system to generatediverse summaries across different products, a no-tion that we denoted as cp-diversity.
lackingan existing metric, we use our previously deﬁnedspr-1/2/l measure on the set of ﬁnal (top-ranked)summaries across all test set products..4while we did not ﬁnd evidence suggesting a domainshift, it is an aspect we leave for further investigation in futurework..table 1 reports results for all 3 dimensions.
forthe amazon dataset (top table), we observe thatpass outperforms the baselines in coherence andcp-diversity while keeping a comparable infor-mativeness to the next best system, t5-ft. theonly exception being rouge-2 in which t5-ftoutperforms pass which could be explained bythe somewhat longer summaries it generates.
in-terestingly, in cp-diversity, the performance ofpass is closer to human performance than tocopycat and fewsum but there’s still room tomake the summaries even more diverse.
for thesake of completeness and following previous work(chu and liu, 2019; brazinskas et al., 2020b,a) wereport results on business reviews from the yelpdataset in the bottom of table 1..recall that our key goals were to avoid gener-ating summaries containing crude coherence (ce)and self-consistency (sce) errors (see table 3 forin order to evaluateexamples of such errors).
these directly, both authors independently markedeach of the summaries generated by fewsum, t5-ft and pass for the amazon test set as hav-ing a crude error or not, for both types of er-rors.
table 2 reports the ratios of crude errorsper system, considering cases where at least oneannotator (i) and both annotators (ii) marked ascrude.
we measured the level of agreement be-tween the two annotators by calculating cohen’skappa coefﬁcients (cohen, 1960) for each anno-tation task, which resulted in κce = 0.571 andκsce = 0.779..system ce-i ce-ii sce-i sce-ii.
fewsum 0.500.38t5-ft0.19pass.
0.340.250.09.
0.30.30.05.
0.250.20.00.table 2: ratios of crude coherence (ce) and self-consistency (sce) errors for each system on the ama-zon test set.
i/ii refer to cases where at least one/bothannotators marked the summary as having an error..finally, for a qualitative impression we providein table 4 an example of the systems’ outputs fora product from the amazon test set..6 conclusion.
in this work we highlight two shortcomings ofexisting product reviews summarization systems,namely low cp-diversity and self-inconsistency.
we propose the spr metric to quantify cross prod-.
358tights.
these tights are very comfortable anddurable.
they can be worn with ballet slippersor sandals.
the color is beautiful and the fabricis soft.
they will last a long time.
they are greatfor transitioning from ballet to ballet..purse.
this purse is not as cute as it looks inthe picture.
it is very small and will not hold alot of stuff.
it would be a great purse if it was alittle bigger but it would have been nice to havea purse that would hold more than one purse..protein bar.
these bars are a great snack bar.
they taste good and have a good amount of pro-tein.
they do not have a lot of protein in themso they are not as sweet as some protein bars,but for the price, they are well worth it..tank top.
this tank top is well made, ﬁts well,and is comfortable to wear.
the only thing isthat it runs a little small, so order a size up fromwhat you normally wear.
other than that, it’s agreat top.
it’s well made and it looks like it willlast a long time.
love it!.
table 3: example of summaries generated by t5-ftand fewsum models for different products in the ama-zon test set, which contain crude errors (ce) and self-consistency errors (sce)..uct similarity of summaries and demonstrate thatindeed, humans summaries are far more diversethan system generated summaries.
to overcomethis issue we rely on stronger pre-trained modelssuch as the recent t5 model which signiﬁcantlyimproves the cp-diversity.
however, the secondproblem still remains and even intensiﬁes as with-out the safety net of generic content, the risk of in-coherent or even self-contradicting text is substan-tial.
to this end, we propose the perturb and selectsummarizer (pass).
in the ﬁrst step, pass appliessystematic perturbations to the input texts in a waythat allows the t5 model to generate multiple sum-mary candidates that sufﬁciently differ from oneanother.
given such a set of diverse summaries,pass applies a trained ranker to smartly select apromising candidate in terms of coherence.
fi-nally, we show that the resulting pass system, out-performs sota models in the domain of productreviews in terms of informativeness, cp-diversityand coherence.
when comparing to a ﬁne-tunedt5 model pass outperforms it in coherence andcp-diversity, while maintaining comparable per-formance for informativeness..pass.
these reeboks are great for supportinga high arch and are lightweight and comfort-able.
they come in a variety of colors and sizes,and are ideal for walking or biking.
they arealso ﬂexible and well made..t5-ft. these reeboks are a great choice forthose with wide feet.
they run true to size andthe colors are great.
they are lightweight andcomfortable, yet they are ﬂexible and ﬂexible.
they are recommended for people with widefeet.
they are also very popular for runningand casual wear..fewsum.
these running shoes are great!
theyﬁt true to size and are very comfortable to runaround in.
they are light weight and have greatsupport.
they run a little on the narrow side,so make sure to order a half size larger thannormal..i love these shoes.
they are lightcopycat.
weight and comfortable to wear.
i have wornthem for several months now and they are hold-ing up well.
i would recommend them to anyonelooking for a comfortable shoe..table 4: example of summaries generated by pass,t5-ft, fewsum and copycat systems for the samesports shoes reviews..in future work we plan to investigate theperturb-and-select framework in order to promotesummaries with a plethora of desired linguisticcharacteristics, other than coherence.
we shall fur-ther explore ways of extending this framework toemploy other input perturbation methods and ex-periment with scenarios of larger scale input.
inaddition, we plan to further investigate our pro-posed spr evaluation metric for lexical diversity,by studying its correlation with human judgments.
lastly, we believe our proposed framework andevaluation metric may be applicable to other do-mains of opinion or news summarization..acknowledgements.
we would like to thank hila gonen, iftah gamzuand anonymous reviewers, who helped improvethe draft with their invaluable comments and in-sight..359references.
reinald kim amplayo, stefanos angelidis, andmirella lapata.
2021. unsupervised opinion sum-marization with content planning.
in aaai..reinald kim amplayo and mirella lapata.
2020. un-supervised opinion summarization with noising andthe 58th annualdenoising.
meeting of the association for computational lin-guistics, pages 1934–1945, online.
association forcomputational linguistics..in proceedings of.
stefanos angelidis, reinald kim amplayo, yoshi-hiko suhara, xiaolan wang, and mirella lapata.
2021. extractive opinion summarization in quan-tized transformer spaces.
trans.
assoc.
comput.
linguistics, 9:277–293..stefanos angelidis and mirella lapata.
2018. sum-marizing opinions: aspect extraction meets senti-ment prediction and they are both weakly super-in proceedings of the 2018 conference onvised.
empirical methods in natural language process-ing, pages 3675–3686..arthur brazinskas, mirella lapata, and ivan titov.
2020a.
few-shot learning for opinion summariza-in proceedings of the 2020 conference ontion.
empirical methods in natural language process-ing, emnlp 2020, online, november 16-20, 2020,pages 4119–4135.
association for computationallinguistics..arthur brazinskas, mirella lapata, and ivan titov.
2020b.
unsupervised opinion summarization asin proceedings of thecopycat-review generation.
58th annual meeting of the association for compu-tational linguistics, acl 2020, online, july 5-10,2020, pages 5151–5169.
association for computa-tional linguistics..tom b. brown, benjamin mann, nick ryder, melaniesubbiah, jared kaplan, prafulla dhariwal, arvindneelakantan, pranav shyam, girish sastry, amandaaskell, sandhini agarwal, ariel herbert-voss,gretchen krueger, tom henighan, rewon child,aditya ramesh, daniel m. ziegler, jeffrey wu,clemens winter, christopher hesse, mark chen,eric sigler, mateusz litwin, scott gray, benjaminchess, jack clark, christopher berner, sam mc-candlish, alec radford, ilya sutskever, and dariolanguage models are few-shotamodei.
2020.in advances in neural information pro-learners.
cessing systems 33: annual conference on neu-ral information processing systems 2020, neurips2020, december 6-12, 2020, virtual..ziqiang cao, furu wei, wenjie li, and sujian li.
2018.faithful to the original: fact aware neural abstrac-in proceedings of the thirty-tive summarization.
second aaai conference on artiﬁcial intelligence,(aaai-18), the 30th innovative applications of arti-ﬁcial intelligence (iaai-18), and the 8th aaai sym-posium on educational advances in artiﬁcial intel-.
ligence (eaai-18), new orleans, louisiana, usa,february 2-7, 2018, pages 4784–4791.
aaai press..giuseppe carenini, raymond t. ng, and adam pauls.
2006. multi-document summarization of evaluativetext.
in eacl 2006, 11st conference of the euro-pean chapter of the association for computationallinguistics, proceedings of the conference, april 3-7, 2006, trento, italy.
the association for computerlinguistics..eric chu and peter j. liu.
2019. meansum: a neuralmodel for unsupervised multi-document abstractivein proceedings of the 36th inter-summarization.
national conference on machine learning, icml2019, 9-15 june 2019, long beach, california,usa, volume 97 of proceedings of machine learn-ing research, pages 1223–1232.
pmlr..andr´e cibils, claudiu musat, andreea hossmann, andmichael baeriswyl.
2018. diverse beam searchfor increased novelty in abstractive summarization.
corr, abs/1802.01457..maximin coavoux, hady elsahar, and matthias gall´e.
2019. unsupervised aspect-based multi-documentin proceedings of theabstractive summarization.
2nd workshop on new frontiers in summarization,pages 42–47..jacob cohen.
1960. a coefﬁcient of agreement foreducational and psychological.
nominal scales.
measurement, 20(1):37–46..hoa trang dang.
2005. overview of duc 2005. in pro-ceedings of the document understanding conference,volume 2005, pages 1–12..alexander r. fabbri, wojciech kryscinski, bryanmccann, caiming xiong, richard socher, anddragomir r. radev.
2021.summeval: re-evaluating summarization evaluation.
trans.
assoc.
comput.
linguistics, 9:391–409..wa falcon.
2019. pytorch lightning.
github.
note:.
https://github.com/pytorchlightning/pytorch-lightning, 3..karl moritz hermann, tom´as kocisk´y, edwardgrefenstette, lasse espeholt, will kay, mustafa su-leyman, and phil blunsom.
2015. teaching ma-in advances inchines to read and comprehend.
neural information processing systems 28: annualconference on neural information processing sys-tems 2015, december 7-12, 2015, montreal, que-bec, canada, pages 1693–1701..minqing hu and bing liu.
2004. mining and summa-rizing customer reviews.
in proceedings of the tenthacm sigkdd international conference on knowl-edge discovery and data mining, seattle, wash-ington, usa, august 22-25, 2004, pages 168–177.
acm..360hitesh kansal and durga toshniwal.
2014. aspectbased summarization of context dependent opinionwords.
in 18th international conference in knowl-edge based and intelligent information and engi-neering systems, kes 2014, gdynia, poland, 15-17september 2014, volume 35 of procedia computerscience, pages 166–175.
elsevier..diederik p. kingma and jimmy ba.
2015. adam: ain 3rd inter-method for stochastic optimization.
national conference on learning representations,iclr 2015, san diego, ca, usa, may 7-9, 2015,conference track proceedings..svetlana kiritchenko and saif mohammad.
2017.best-worst scaling more reliable than rating scales:a case study on sentiment intensity annotation.
inproceedings of the 55th annual meeting of the as-sociation for computational linguistics (volume 2:short papers), pages 465–470..svetlana kiritchenko and saif m. mohammad.
2016.capturing reliable ﬁne-grained sentiment associa-tions by crowdsourcing and best–worst scaling.
inproceedings of the 15th annual conference of thenorth american chapter ofthe association forcomputational linguistics: human language tech-nologies (naacl), san diego, california..wojciech kryscinski, nitish shirish keskar, bryan mc-cann, caiming xiong, and richard socher.
2019.neural text summarization: a critical evaluation.
inproceedings of the 2019 conference on empiricalmethods in natural language processing and the9th international joint conference on natural lan-guage processing (emnlp-ijcnlp), pages 540–551, hong kong, china.
association for computa-tional linguistics..alon lavie and abhaya agarwal.
2007. meteor: anautomatic metric for mt evaluation with high levelsof correlation with human judgments.
in proceed-ings of the second workshop on statistical machinetranslation, pages 228–231, prague, czech repub-lic.
association for computational linguistics..kevin lerman, sasha blair-goldensohn, and ryan t.mcdonald.
2009. sentiment summarization: evalu-ating and learning user preferences.
in eacl 2009,12th conference of the european chapter of the as-sociation for computational linguistics, proceed-ings of the conference, athens, greece, march 30- april 3, 2009, pages 514–522.
the association forcomputer linguistics..mike lewis, yinhan liu, naman goyal, mar-jan ghazvininejad, abdelrahman mohamed, omerlevy, veselin stoyanov, and luke zettlemoyer.
2020. bart: denoising sequence-to-sequence pre-training for natural language generation, translation,and comprehension.
in proceedings of the 58th an-nual meeting of the association for computationallinguistics, pages 7871–7880, online.
associationfor computational linguistics..chin-yew lin.
2004a.
looking for a few good metrics:automatic summarization evaluation - how manysamples are enough?
in proceedings of the fourthntcir workshop on research in information ac-cess technologies information retrieval, questionanswering and summarization, ntcir-4, nationalcenter of sciences, tokyo, japan, june 2-4, 2004.national institute of informatics (nii)..chin-yew lin.
2004b.
rouge: a package for auto-matic evaluation of summaries.
in text summariza-tion branches out, pages 74–81..ilya loshchilov and frank hutter.
2019. decou-in 7th inter-pled weight decay regularization.
national conference on learning representations,iclr 2019, new orleans, la, usa, may 6-9, 2019.openreview.net..jordan j louviere, terry n flynn, and anthony al-fred john marley.
2015. best-worst scaling: the-ory, methods and applications.
cambridge univer-sity press..jordan j louviere and george g woodworth.
1991.best-worst scaling: a model for the largest differ-ence judgments.
technical report, working paper..joshua maynez, shashi narayan, bernd bohnet, andryan mcdonald.
2020. on faithfulness and factu-ality in abstractive summarization.
in proceedingsof the 58th annual meeting of the association forcomputational linguistics, pages 1906–1919, on-line.
association for computational linguistics..jun-ping ng and viktoria abrecht.
2015. better sum-marization evaluation with word embeddings forrouge.
in proceedings of the 2015 conference onempirical methods in natural language process-ing, pages 1925–1930, lisbon, portugal.
associa-tion for computational linguistics..b. orme.
2009. maxdiff analysis : simple counting ,.
individual-level logit , and hb..kishore papineni, salim roukos, todd ward, and wei-jing zhu.
2002. bleu: a method for automatic eval-uation of machine translation.
in proceedings of the40th annual meeting on association for computa-tional linguistics, acl ’02, page 311–318, usa.
association for computational linguistics..adam paszke, sam gross, francisco massa, adamlerer, james bradbury, gregory chanan, trevorkilleen, zeming lin, natalia gimelshein, lucaantiga, alban desmaison, andreas kopf, edwardyang, zachary devito, martin raison, alykhan te-jani, sasank chilamkurthy, benoit steiner, lu fang,junjie bai, and soumith chintala.
2019.py-torch: an imperative style, high-performance deeplearning library.
in h. wallach, h. larochelle,a. beygelzimer, f. d'alch´e-buc, e. fox, and r. gar-nett, editors, advances in neural information pro-cessing systems 32, pages 8024–8035.
curran as-sociates, inc..361in 2016 in-with convolutional neural networks.
ternational joint conference on neural networks,ijcnn 2016, vancouver, bc, canada, july 24-29,2016, pages 3157–3163.
ieee..wenting xiong and diane litman.
2014. empiricalanalysis of exploiting review helpfulness for extrac-tive summarization of online reviews.
in proceed-ings of coling 2014, the 25th international con-ference on computational linguistics: technicalpapers, pages 1985–1995, dublin, ireland.
dublincity university and association for computationallinguistics..jingqing zhang, yao zhao, mohammad saleh, andpeter j. liu.
2020a.
pegasus: pre-training withextracted gap-sentences for abstractive summariza-tion.
in proceedings of the 37th international con-ference on machine learning, icml 2020, 13-18july 2020, virtual event, volume 119 of proceed-ings of machine learning research, pages 11328–11339. pmlr..tianyi zhang, varsha kishore, felix wu, kilian q.weinberger, and yoav artzi.
2020b.
bertscore:evaluating text generation with bert.
in 8th inter-national conference on learning representations,iclr 2020, addis ababa, ethiopia, april 26-30,2020. openreview.net..colin raffel, noam shazeer, adam roberts, katherinelee, sharan narang, michael matena, yanqi zhou,wei li, and peter j. liu.
2020. exploring the limitsof transfer learning with a uniﬁed text-to-text trans-former.
j. mach.
learn.
res., 21:140:1–140:67..ori shapira and ran levy.
2020. massive multi-document summarization of product reviews withweak supervision.
corr, abs/2007.11348..yoshihiko suhara, xiaolan wang, stefanos angelidis,and wang-chiew tan.
2020. opiniondigest: a sim-ple framework for opinion summarization.
in pro-ceedings of the 58th annual meeting of the asso-ciation for computational linguistics, pages 5789–5798..jiaxing tan, alexander kotov, rojiar pir mohamma-diani, and yumei huo.
2017. sentence retrievalwith sentiment-speciﬁc topical anchoring for reviewin proceedings of the 2017 acmsummarization.
on conference on information and knowledge man-agement, cikm 2017, singapore, november 06 - 10,2017, pages 2323–2326.
acm..ashish vaswani, noam shazeer, niki parmar, jakobuszkoreit, llion jones, aidan n. gomez, lukaszkaiser, and illia polosukhin.
2017. attention is allyou need.
in advances in neural information pro-cessing systems 30: annual conference on neuralinformation processing systems 2017, december 4-9, 2017, long beach, ca, usa, pages 5998–6008..ashwin k. vijayakumar, michael cogswell, ram-prasaath r. selvaraju, qing sun, stefan lee,david j. crandall, and dhruv batra.
2016. diversebeam search: decoding diverse solutions from neu-ral sequence models.
corr, abs/1610.02424..thomas wolf, lysandre debut, victor sanh, julienchaumond, clement delangue, anthony moi, pier-ric cistac, tim rault, r´emi louf, morgan funtow-icz, joe davison, sam shleifer, patrick von platen,clara ma, yacine jernite, julien plu, canwen xu,teven le scao, sylvain gugger, mariama drame,quentin lhoest, and alexander m. rush.
2020a.
transformers: state-of-the-art natural language pro-cessing.
in proceedings of the 2020 conference onempirical methods in natural language process-ing: system demonstrations, pages 38–45, online.
association for computational linguistics..thomas wolf, quentin lhoest, patrick von platen,yacine jernite, mariama drame, julien plu, julienchaumond, clement delangue, clara ma, abhishekthakur, suraj patil, joe davison, teven le scao,victor sanh, canwen xu, nicolas patry, angiemcmillan-major, simon brandeis, sylvain gugger,franc¸ois lagunas, lysandre debut, morgan fun-towicz, anthony moi, sasha rush, philipp schmidd,pierric cistac, victor muˇstar, jeff boudier, andanna tordjmann.
2020b.
datasets.
github.
note:https://github.com/huggingface/datasets, 1..haibing wu, yiwei gu, shangdi sun, and xiaodonggu.
2016. aspect-based opinion summarization.
362a pass implementation details and.
a.2 pairwise summary classiﬁers.
hyperparameters.
all models were implemented with the pytorch(paszke et al., 2019) deep learning framework,utilizing the t5 (raffel et al., 2020) pre-trainedmodel and tokenizer implementations from hug-gingface’s transformers (wolf et al., 2020a) li-brary, evaluation metrics from huggingface’sdatasets (wolf et al., 2020b) library and py-torch lightning (falcon, 2019) as a model train-ing framework..a.1 t5 fine-tuned summarizer.
we ﬁne-tune a pre-trained t5-base model (220mparameters (raffel et al., 2020)) for product re-views summarization (an abstractive text summa-rization task) on the training set, employing theadam optimizer (kingma and ba, 2015) withweight decay (loshchilov and hutter, 2019).
wetrain the model for a maximum of 20 epochs on asingle nvidia tesla v100 gpu, while employ-ing a standard early stopping mechanism (falcon,2019) based on the development set’s average lossper epoch.
we employ a standard beam searchdecoding algorithm during inference for generat-ing text.
we tune the model’s hyperparameters onthe development set, and provide a list of the ﬁ-nal model’s tuned hyperparametrs along with therange of values tested during tuning..hyperparameterst5 encoder.
• max input sequence length = 512 tokens• training batch size = 8, [8, 12, 16]• evaluation batch size = 12, [8, 12, 16].
adam optimizer.
• learning rate = 3e−4, [1e−4, 3e−4, 5e−4]• (cid:15) = 1e − 8, [1e − 8, 3e − 8, 5e − 8]• weight decay: 0.0• number of warmup steps: 0• gradient accumulation steps = 2, [1, 2, 4]• max gradient norm = 1.0.t5 decoder.
• max output sequence length = 128 tokens• min output sequence length = 16 tokens• beam size = 2, [2, 3, 4]• length penalty = 2, [1, 2, 3]• repetition penalty = 2, [1, 2, 3].
lko input perturbation (pass system only).
• k = 2, [1, 2, 3, 4, 5].
for each pairwise summary classiﬁer (coher-ence, ﬂuency), we ﬁne-tune a pre-trained t5-basemodel (220m parameters (raffel et al., 2020))for abstractive text summarization task on the re-spective training set employing the adam opti-mizer (kingma and ba, 2015) with weight de-cay (loshchilov and hutter, 2019).
we train fora maximum of 20 epochs on a single nvidiatesla v100 gpu, while employing a standardearly stopping mechanism (falcon, 2019) basedon the development set’s average loss per epoch.
we employ a standard greedy decoding algorithmduring inference for generating the class label.
wetune the model’s hyperparameters on the develop-ment set, and provide a list of the ﬁnal model’stuned hyperparametrs along with the range of val-ues tested during tuning..hyperparametersdataset.
• coherence scores difference threshold (cid:15) =.
0.5, [0.25, 0.5, 0.75, 1.0].
• fluency scores difference threshold (cid:15) = 0.25,.
[0.25, 0.5, 0.75, 1.0].
t5 encoder.
• max input sequence length = 400 tokens• training batch size = 16, [8, 12, 16]• evaluation batch size = 16, [8, 12, 16].
adam optimizer.
• learning rate = 1e−4, [1e−4, 3e−4, 5e−4]• (cid:15) = 1e − 8, [1e − 8, 3e − 8, 5e − 8]• weight decay: 0.0• number of warmup steps: 0• gradient accumulation steps = 4, [1, 2, 4]• max gradient norm = 1.0.t5 decoder.
• max output sequence length = 2 tokens• min output sequence length = 2 tokens.
b summary examples.
we provide examples for output summaries gener-ated by the different summarization systems dis-cussed in the main paper.
each example qualita-tively highlights a different aspect by which weevaluate the quality of a summary, or identify itsshortcomings..363pass.
this camera is good to have as a ﬁrstcamera before investing in a dslr.
the qual-ity of the pictures is great, and the camera iseasy to use.
it takes some time to learn aboutthe features and settings, but overall it’s a greatcamera..t5-ft. this camera is a great camera for tak-ing professional photos.
it is easy to use andtakes excellent pictures.
the low light featureis outstanding and will be helpful in museumsand other venues where ﬂash is not allowed.
the battery is constantly malfunctioning mak-ing the camera unusable.
the on off button isalso malﬁnctioning..fewsum.
this camera is a great camera forthe price.
it takes great pictures and is easy touse.
the only drawback is that the battery life isnot as good as the camera that comes with thecamera.
it would be nice if it had a battery lifeto last longer.
overall, it’s a good camera..copycat.
this is a great camera for the price.
it is easy to set up and use.
the only downsideis that it takes a while to learn how to use it, butit’s not a problem..table 5: example of summaries generated by pass,t5-ft (raffel et al., 2020), fewsum (brazinskas et al.,2020a) and copycat (brazinskas et al., 2020b) systemsfor the same reviews for a digital camera..travel sound conditioner.
this is a greatproduct for the price.
the sound quality is goodand the sound is good.
the only problem isthat it is not loud enough for a small room.
itis loud enough to drown out background noise,but not very loud.
overall, it’s a good productand would recommend it to anyone..motion sickness tablets.
this is a great prod-uct at a great price.
it is easy to use and easyto take.
the pills are easy to swallow and donot take up a lot of space.
the price is greatfor a product that will last a long time.
wouldrecommend this product to anyone who suffersfrom nausea or sickness..digital camera.
this camera is a great cam-era for the price.
it takes great pictures and iseasy to use.
the only drawback is that the bat-tery life is not as good as the camera that comeswith the camera.
it would be nice if it had abattery life to last longer.
overall, it’s a goodcamera..table 7: example of a self-contradicting summary gen-erated by our ﬁne-tuned t5 (t5-ft) model..review 1. the machine is very tricky.
it re-quires some trial and errors to make it workright.
i do not like to put oil in the dough; how-ever, it appears to me that without oil it is im-possible to make tortilla or chapatti.
it is use-less for me..review 2. fun and easy to use!
took me onebatch to get my technique worked out, but itwas very simple, easy to follow directions.
easyclean up too!
i would recommend this to anyonelooking for an electric tortilla maker!.
summary.
this tortilla maker is a great optionfor making tortillas but it does require sometrial and error to make it work right.
it requiressome trial and error to make it work right.
yes,you should grill them after cooking to get thetoasted look.
it is easy to use and very easy toclean up..these nugo bars are highcandidate 1.quality and they come in a variety of ﬂavorsand sizes which make them perfect for serv-ing as a snack or as a replacement for pro-cessed foods.
they are low glycemic and havea smooth, vanilla-like texture which makes themvery good..candidate 2. these nugo bars are high qual-ity and they come in a variety of ﬂavors andsizes which makes them ideal for snacking onthe go.
the taste is great and the nutritionalvalue is great as well.
although they can be alittle sweet, they are not too sweet..candidate 3. these nugo bars are high qual-ity and they come in a variety of ﬂavors andsizes.
they are low glycemic and have a greattaste.
while they may be sweet, they can alsohave a chalky or barky texture.
these are greatfor replacing junk food with healthy snacks..candidate 4. these nugo bars are high qual-ity and they taste great.
they are low glycemic,and they contain no added sugar or artiﬁcialﬂavors.
these are great for a healthy snack orfor a quick breakfast..candidate 5. these nugo bars are very goodquality and they come in a variety of ﬂavors.
they are high in calories and ﬁber, and aregreat for snacking on the go.
they are oftena bit chewy, but they are deﬁnitely worth themoney..table 6: example of similar summaries generated byfewsum (brazinskas et al., 2020a) for three differentproducts..table 8: example of 5 candidate summaries (out of 28)generated by pass for the same product with l2o in-put perturbation..364c evaluation figures.
we provide ﬁgures which extend those appearingin the evaluation section of the main paper..c.1 candidate summary generation.
figure 5: length box plot for all candidate summarysets generated with lko input perturbation method fork = 1, .., 5..figure 6: rouge-1 box plot for all candidate sum-mary sets generated with lko input perturbationmethod for k = 1, .., 5..figure 7: spr-1 box plot for all pairs of candidate sum-maries generated with lko input perturbation methodfor k = 1, .., 5..figure 8: rouge-l box plot for all candidatesummary sets generated with lko input perturbationmethod for k = 1, .., 5..figure 9: spr-l box plot for all pairs of candi-date summaries generated with lko input perturbationmethod for k = 1, .., 5..c.2 candidate summary ranking.
figure 10: confusion matrix for the fluency pairwiseclassiifer..365