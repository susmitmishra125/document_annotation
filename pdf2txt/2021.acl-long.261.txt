position bias mitigation: a knowledge-aware graph model for emotioncause extraction.
hanqi yan, lin gui, gabriele pergola, yulan hedepartment of computer science, university of warwick{hanqi.yan, lin.gui, gabriele.pergola, yulan.he}@warwick.ac.uk.
abstract.
the emotion cause extraction (ece) taskaimsto identify clauses which containemotion-evoking information for a particularemotion expressed in text.
we observe that awidely-used ece dataset exhibits a bias thatthe majority of annotated cause clauses are ei-ther directly before their associated emotionclauses or are the emotion clauses themselves.
existing models for ece tend to explore suchrelative position information and suffer fromthe dataset bias.
to investigate the degree ofreliance of existing ece models on clause rel-ative positions, we propose a novel strategy togenerate adversarial examples in which the rel-ative position information is no longer the in-dicative feature of cause clauses.
we test theperformance of existing models on such adver-sarial examples and observe a signiÔ¨Åcant per-formance drop.
to address the dataset bias,we propose a novel graph-based method to ex-plicitly model the emotion triggering paths byleveraging the commonsense knowledge to en-hance the semantic dependencies between acandidate clause and an emotion clause.
ex-perimental results show that our proposed ap-proach performs on par with the existing state-of-the-art methods on the original ece dataset,and is more robust against adversarial attackscompared to existing models.1.
1.introduction.
instead of detecting sentiment polarity from text,recent years have seen a surge of research activitiesthat identify the cause of emotions expressed intext (gui et al., 2017; cheng et al., 2017a; rashkinet al., 2018; xia and ding, 2019; kim and klinger,2018; oberl¬®ander and klinger, 2020).
in a typi-cal dataset for emotion cause extract (ece) (gui.
1our code can be accessed at https://github.com/hanqi-qi/position-bias-mitigation-in-emotion-cause-analysis.
et al., 2017), a document consists of multipleclauses, one of which is the emotion clause an-notated with a pre-deÔ¨Åned emotion class label.
inaddition, one or more clauses are annotated as thecause clause(s) which expresses triggering factorsleading to the emotion expressed in the emotionclause.
an emotion extraction model trained on thedataset is expected to classify a given clause as acause clause or not, given the emotion clause..figure 1: the distribution of positions of cause clausesrelative to their corresponding emotion clauses in theece dataset (gui et al., 2016).
nearly 87% of causeclauses are located near the emotion clause (about 55%are immediately preceding the emotion clause, 24% arethe emotion clauses themselves and over 7% are imme-diately after the emotion clause)..however, due to the difÔ¨Åculty in data collec-tion, the ece datasets were typically constructedby using emotion words as queries to retrieve rele-vant contexts as candidates for emotion cause an-notation, which might lead to a strong positionalbias (ding and kejriwal, 2020).
figure 1 depictsthe distribution of positions of cause clauses rela-tive to the emotion clause in the ece dataset (guiet al., 2016).
most cause clauses are either im-mediately preceding their corresponding emotionclauses or are the emotion clauses themselves.
ex-isting ece models tend to exploit such relative po-sition information and have achieved good resultson emotion cause detection.
for example, the rel-.
proceedingsofthe59thannualmeetingoftheassociationforcomputationallinguisticsandthe11thinternationaljointconferenceonnaturallanguageprocessing,pages3364‚Äì3375august1‚Äì6,2021.¬©2021associationforcomputationallinguistics33641.717.7154.4523.587.472.220.510102030405060prev3prev2prev1emotionnext1next2next3percentage(%)causepositionative position augmented with dynamic globallabels (pae-dgl) (ding et al., 2019), rnn-transformer hierarchical network (rthn) (xiaet al., 2019) and multi-attention-based neural net-work (mann) (li et al., 2019) all concatenate therelative position embeddings with clause semanticembeddings as the clause representations..we argue that models utilising clause relativepositions would inherently suffer from the datasetbias, and therefore may not generalise well to un-seen data when the cause clause is not in proximityto the emotion clause.
for example, in a recentlyreleased emotion cause dataset, only 25-27% causeclauses are located immediately before the emotionclause (poria et al., 2020).
to investigate the de-gree of reliance of existing ece models on clauserelative positions, we propose a novel strategy togenerate adversarial examples in which the relativeposition information is no longer the indicative fea-ture of cause clauses.
we test the performance ofexisting models on such adversarial examples andobserve a signiÔ¨Åcant performance drop..to alleviate the position bias problem, we pro-pose to leverage the commonsense knowledge toenhance the semantic dependencies between a can-didate clause and the emotion clause.
more con-cretely, we build a clause graph, whose node fea-tures are initialised by the clause representations,and has two types of edges i.e., sequence-edge (s-edge) and knowledge-edge (k-edge).
a s-edgelinks two consecutive clauses to capture the clauseneighbourhood information, while a k-edge linksa candidate clause with the emotion clause if thereexists a knowledge path extracted from the con-ceptnet (speer et al., 2017) between them.
weextend relation-gcns (schlichtkrull et al., 2018)to update the graph nodes by gathering informa-tion encoded in the two types of edges.
finally,the cause clause is detected by performing node(i.e., clause) classiÔ¨Åcation on the clause graph.
insummary, our contributions are three-fold:.
‚Ä¢ we investigate the bias in the emotion causeextraction (ece) dataset and propose a novelstrategy to generate adversarial examples inwhich the position of a candidate clause rel-ative to the emotion clause is no longer theindicative feature for cause extraction..‚Ä¢ we develop a new emotion cause extractionapproach built on clause graphs in whichnodes are clauses and edges linking two nodescapture the neighbourhood information as.
well as the implicit reasoning paths extractedfrom a commonsense knowledge base be-tween clauses.
node representations are up-dated using the extended relation-gcn.
‚Ä¢ experimental results show that our proposedapproach performs on par with the existingstate-of-the-art methods on the original ecedataset, and is more robust when evaluatingon the adversarial examples..2 related work.
the presented work is closely related to two linesof research in emotion cause extraction: position-insensitive and position-aware models.
position-insensitive models.
a more traditionalline of research exploited structural representationsof textual units relying on rule-based systems (leeet al., 2010) or incorporated commonsense knowl-edge bases (gao et al., 2015) for emotion cause ex-traction.
machine learning methods leveraged textfeatures (gui et al., 2017) and combined them withmulti-kernel support vector machine (svm) (xuet al., 2017).
more recent works developed neu-ral architectures to generate effective semantic fea-tures.
cheng et al.
(2017b) employed lstm mod-els, gui et al.
(2017) made use of memory net-works, while li et al.
(2018) devised a convolu-tional neural network (cnn) with a co-attentionmechanism.
(chen et al., 2018) used the emo-tion classiÔ¨Åcation task to enhance cause extractionresults.
position-aware models.
more recent methodolo-gies have started to explicitly leverage the positionsof cause clauses with respect to the emotion clause.
a common strategy is to concatenate the clause rel-ative position embedding with the candidate clauserepresentation (ding et al., 2019; xia et al., 2019;li et al., 2019).
the relative position augmentedwith dynamic global labels (pae-dgl) (dinget al., 2019) reordered clauses based on their dis-tances from the target emotion clause, and propa-gated the information of surrounding clauses to theothers.
xu et al.
(2019) used emotion dependentand independent features to rank clauses and iden-tify the cause.
the rnn-transformer hierarchicalnetwork (rthn) (xia et al., 2019) argued there ex-ist relations between clauses in a document and pro-posed to classify multiple clauses simultaneously.
li et al.
(2019) proposed a multi-attention-basedneural network (mann) to model the interactionsbetween a candidate clause and the emotion clause..3365figure 2: the framework of our proposed kag.
given an input document consisting of eight clauses (c1 ¬∑ ¬∑ ¬∑ c8),we Ô¨Årst extract knowledge paths from conceptnet between each candidate clause and the emotion clause (¬ß3.1),e.g., two knowledge paths, p1 and p2, are extracted between c1 and the emotion clause c5.
(a) document encod-ing.
clauses are fed into a word-level bi-lstm and a clause-level transformer to obtain the clause representationsÀÜci.
the document embedding d is generated by dot-attention between the emotion embedding ÀÜce and clauseembeddings.
(b) path representations.
the extracted knowledge paths are fed into bi-lstm to derive path rep-resentations.
multiple paths between a clause pair are aggregated into si based on their attention to the documentrepresentation d. (c) clause graph update.
a clause graph is built with the clause representations ÀÜci used toinitialise the graph nodes.
the k-edge weight eie between a candidate clause ÀÜci and the emotion clause ÀÜce aremeasured by their distance along their path si.
(d) classiÔ¨Åcation.
node representation hi of a candidate clauseci is concatenated with the emotion node representation he, and then fed to a softmax layer to yield the clauseclassiÔ¨Åcation result ÀÜyi..the generated representations are fed to a cnnlayer for emotion cause extraction.
the hierar-chical neural network (fan et al., 2019) aimed atnarrowing the gap between the prediction distribu-tion p and the true distribution of the cause clauserelative positions..3 knowledge-aware graph (kag).
model for emotion cause extraction.
we Ô¨Årst deÔ¨Åne the emotion cause extraction (ece)task here.
a document d contains n clauses d ={ci}ni=1, one of which is annotated as an emotionclause ce with a pre-deÔ¨Åned emotion class label,ew.
the ece task is to identify one or more causeclauses, ct, 1 ‚â§ t ‚â§ n , that trigger the emotionexpressed in ce.
note that the emotion clauseitself can be a cause clause..we propose a knowledge-aware graph (kag)model as shown in figure 2, which incorporatesknowledge paths extracted from conceptnet foremotion cause extraction.
more concretely, foreach document, a graph is Ô¨Årst constructed by rep-resenting each clause in the document as a node.
the edge linking two nodes captures the sequen-tial relation between neighbouring clauses (calledthe sequence edge or s-edge).
in addition, to bet-.
ter capture the semantic relation between a can-didate clause and the emotion clause, we identifykeywords in the candidate clause which can reachthe annotated emotion class label by following theknowledge paths in the conceptnet.
the extractedknowledge paths from conceptnet are used to en-rich the relationship between the candidate clauseand the emotion clause and are inserted into theclause graph as the knowledge edge or k-edge.
we argue that by adding the k-edges, we can bettermodel the semantic relations between a candidateclause and the emotion clause, regardless of theirrelative positional distance..in what follows, we will Ô¨Årst describe how toextract knowledge paths from conceptnet, thenpresent the incorporation of the knowledge pathsinto context modelling, and Ô¨Ånally discuss the useof graphical convolutional network (gcn) forlearning node (or clause) representations and theprediction of the cause clause based on the learnednode representations..3.1 knowledge path extraction from.
conceptnet.
conceptnet is a commonsense knowledge graph,which represents entities as nodes and relationshipbetween them as edges.
to explore the causal re-.
3366bi-lstmbi-lstm.
.
.transformerùëù1.
.
.
ùëù2bi-lstmùë†1ùõº1ùõºùêæs-edgek-edgeùê∂)1ùê∂)ùê∏ùê∂)8..
.ùê∂)2ùê∂)6c1c8c5p1p2bi-lstmùõº2ùê∑p1p2e15ùê∂)ùê∏conceptnet‚Ñé/‚Ñé0softmaxùë¶2ùüèdocument(b) path representations.
(a) document encoding.
(c) clause graph update.
(d) classification...
...
...
.ùê∂4bi-lstmfigure 3: a document consisting of 8 clauses in the ece dataset with extracted knowledge paths from the concept-net.
words in red are identiÔ¨Åed keywords.
‚Äòhappiness‚Äô is the emotion label of the emotion clause c5.
for bettervisualization, we only display two extracted knowledge paths between ‚Äòadopt‚Äô and ‚Äòhappiness‚Äô in the conceptnet..lation between a candidate clause and the emotionclause, we propose to extract cause-related pathslinking a word in the candidate clause with the an-notated emotion word or the emotion class label,ew, in the emotion clause.
more concretely, for acandidate clause, we Ô¨Årst perform word segmenta-tion using the chinese segmentation tool, jieba2,and then extract the top three keywords ranked bytext-rank3.
based on the Ô¨Åndings in (fan et al.,2019) that sentiment descriptions can be relevantto the emotion cause, we also include adjectives inthe keywords set..we regard each keyword in a candidate clauseas a head entity, eh, and the emotion word or theemotion class label in the emotion clause as the tailentity, et.
similar to (lin et al., 2019), we applynetworkx4 to perform a depth-Ô¨Årst search on theconceptnet to identify the paths which start fromeh and end at et, and only keep the paths whichcontain less than two intermediate entities.
thisis because shorter paths are more likely to offerreliable reasoning evidence (xiong et al., 2017).
since not all relations in conceptnet are relatedto or indicative of causal relations, we further re-move the paths which contain any of these fourrelations: ‚Äòantonym‚Äô, ‚Äòdistinct from‚Äô, ‚Äònot desires‚Äô,and ‚Äònot capable of ‚Äô.
finally, we order paths bytheir lengths in an ascending order and choosethe top k paths as the result for each candidate-emotion clause pair5..an example is shown in figure 3. the 5-th.
2https://github.com/fxsjy/jieba3we have also experimented with other keyword extractionstrategies, such as extracting words with higher tfidf valuesor keeping all words after removing the stop words.
but wedid not observe improved emotion cause detection results..4http://networkx.github.io/5we set k to 15, which is the median of the number ofpaths between all the candidate-emotion clause pairs in ourdataset..clause is annotated as the emotion clause and theemotion class label is ‚Äòhappiness‚Äô.
for the key-word, ‚Äòadopted‚Äô, in the Ô¨Årst clause, we show twoexample paths extracted from conceptnet, each ofwhich links the word ‚Äòadopted‚Äô with ‚Äòhappiness‚Äô.
one such a path is ‚Äúadopted ‚àírelated to‚Üí ac-ceptance ‚àíhas subevent‚Üí make better world‚àícauses‚Üí happiness‚Äù..3.2 knowledge-aware graph (kag) model.
as shown in figure 2, there are four componentsin our model: a document encoding module, acontext-aware path representation learning module,a gcn-based graph representation updating mod-ule, and Ô¨Ånally a softmax layer for cause clauseclassiÔ¨Åcation..initial clause/document representation learn-ing for each clause ci, we derive its represen-tation, ci, by using a bi-lstm operating on itsconstituent word vectors, where each word vectorwi ‚àà rd is obtained via an embedding layer.
tocapture the sequential relationship (s-edges) be-tween neighbouring clauses in a document, we feedthe clause sequence into a transformer architecture.
similar to the original transformer incorporatingthe position embedding with the word embedding,we utilise the clause position information to enrichthe clause representation.
here, the position em-bedding oi of each clause is concatenated with itsrepresentation ci generated by bi-lstm..ÀÜci = transformer(ci || oi).
(1).
we consider different ways for encoding positionembeddings using either relative or absolute clausepositions and explore their differences in the exper-iments section.
in addition, we will also show theresults without using position embeddings at all..3367bai jinyue, an ordinary worker in xingtai steel factory in hebei province¬†¬†and the department leader replied to my mailwhen i found that my advice had been adoptedi realized that i had made contributions to the country's developmenttalked to the journalist with exicitmentdifferent departments, like the public security, have accepted his advice¬†with a thank you letter in his handsc2)c4)c3)c6)c7)c8)since 27 years agoacceptanceculture diffusionspreadc1)c5)make better worldhappinessconceptnetsince the aim of our task is to identify the causeclause given an emotion clause, we capture thedependencies between each candidate clause andthe emotion clause.
therefore, in the documentcontext modelling, we consider the emotion clauseÀÜce, generated in a similar way as ÀÜci, as the queryvector, and the candidate clause representation ÀÜcias both the key and value vectors, in order to derivethe document representation, d ‚àà rd..context-aware path representation in sec-tion 3.1, we have chosen a maximum of k paths{pt}kt=1 linking each candidate ci with the emo-tion clause.
however, not every path correlatesequally to the document context.
taking the docu-ment shown in figure 3 as an example, the purpleknowledge path is more closely related to the docu-ment context compared to the green path.
as such,we should assign a higher weight to the purplepath than the green one.
we propose to use thedocument-level representation d obtained aboveas the query vector, and a knowledge path as bothkey and value vectors, in order to calculate thesimilarity between the knowledge path and thedocument context.
for each pair of a candidateclause ci and the emotion clause, we then aggre-gate the k knowledge paths to derive the context-aware path representation si ‚àà rd below:.
si =.
Œ±tpt Œ±t = softmax(.
k(cid:88).
t=1.
dt ptj=1 dt pj.
(cid:80)k.).
(2).
where d is the document representation, pt is thepath representation obtained from bi-lstm on apath expressed as an entity-relation word sequence..update of clause representations by gcnafter constructing a clause graph such as the oneshown in figure 2(c), we update the clause/noderepresentations via s-edges and k-edges.
onlyclauses with valid knowledge paths to the emotionclause are connected with the emotion clause node.
after initialising the node (or clause) in theclause graph with ÀÜci and the extracted knowledgepath with si, we update clause representation us-ing an extended version of gcn, i.e.
relation-gcns (aka.
r-gcns) (schlichtkrull et al., 2018),which is designed for information aggregation overmultiple different edges:.
h(cid:96)+1.
i = œÉ(.
(cid:88).
(cid:88).
r‚ààrni.
j‚ààni.
1ci,r.
w (cid:96).
r h(cid:96).
j + w (cid:96).
0 h(cid:96)i ).
(3).
r ‚àà rd√ód is relation-speciÔ¨Åc, nithe (cid:96)-th layer, w (cid:96)is the set of neighbouring nodes of the i-th node,rnj is the set of distinct edges linking the currentnode and its neighbouring nodes..when aggregating the neighbouring nodes in-formation along the k-edge, we leverage the pathrepresentation si to measure the node importance.
this idea is inspired by the translation-based mod-els in graph embedding methods (bordes et al.,2013).
here, if a clause pair contains a possiblereasoning process described by the k-edge, thenÀÜhe ‚âà ÀÜhi + si holds.
otherwise, ÀÜhi + si shouldbe far away from the emotion clause representationÀÜhe.6 therefore, we measure the importance ofgraph nodes according to the similarity between(hi + si) and he.
here, we use the scaled dot-attention to calculate the similarity eie and obtainthe updated node representation zi..zi = softmax(ee)h(cid:96).
e eie =.
(hi + si)t he‚àöd.(i (cid:54)= e).
(4)where ee is {eie}n ‚àí1i=1 .
d is the dimension of graphnode representations, and n rk is a set of neigh-bours by the k-edge..then, we combine the information encoded in s-edge with zi as in eq.
3, and perform a non-lineartransformation to update the graph node represen-tation h(cid:96)+1.
:.
i.i = œÉ(cid:0)z(cid:96)h(cid:96)+1.
i +.
(cid:88).
(wjhj)(cid:1).
(5).
j‚ààn rs.
i.is a set of i-th neighbours connected by.
where n rsithe s-edges..cause clause detection finally, we concate-nate the candidate clause node hi and the emotionnode representation he generated by the graph, andapply a softmax function to yield the predictiveclass distribution ÀÜyi..ÀÜyi = softmax(cid:0)w (hl.
i || hl.
e) + b(cid:1),.
(6).
4 experiments.
we conduct a thorough experimental assessment ofthe proposed approach against several state-of-the-art models7..6here, we do not consider the cases when the candidateclause is the emotion clause (i.e., ÀÜhi = ÀÜhe), as the similaritybetween ÀÜhe + si and ÀÜhe will be much larger than the otherpairs..7training and hyper-parameter details can be found in.
where w (cid:96)j is the linear transformed informationfrom the neighbouring node j with relation r at.
r h(cid:96).
appendix a..3368methods.
p (%) r (%) f1 (%).
4.1 main results.
67.47rbemocause26.72ngrams+svm 42.00multi-kernel65.8862.15cnn77.21cann70.76memnet.
hcsmannlambdamartpae-dglrthn.
kag: w/o r-gcns: w/o k-edge: w/o s-edge.
73.8878.4377.2076.1976.97.
79.1273.6875.6776.34.
42.8771.3043.7569.2759.4468.9168.38.
71.5475.8774.9969.0876.62.
75.8172.7672.6375.46.
52.4338.8742.8567.5260.7672.6669.55.
72.6977.0676.0872.4276.77.
77.4373.1474.1275.88.w/o pos.
w. pos.
our.
table 1: results of different models on the ece dataset.
our model achieves the best precision and f1 score..dataset and evaluation metrics the evalua-tion dataset (gui et al., 2016) consists of 2,105 doc-uments from sina city news.
as the dataset size isnot large, we perform 10-fold cross-validation andreport results on three standard metrics, i.e.
preci-sion (p), recall (r), and f1-measure, all evaluatedat the clause level..baselines we compare our model with theposition-insensitive and position-aware baselines:rb (lee et al., 2010) and emocause (russo et al.,2011) are rules-based methods.
multi-kernel (guiet al., 2016) and ngrams+svm (xu et al., 2017)leverage support vector machines via differenttextual feature to train emotion cause classiÔ¨Åers.
cnn (kim, 2014) and cann (li et al., 2018) arevanilla or attention-enhanced approaches.
mem-net (gui et al., 2017) uses a deep memory net-work to re-frame ece as a question-answeringtask.
position-aware models use the relative po-sition embedding to enhance the semantic features.
hcs (yu et al., 2019) uses separate hierarchicaland attention module to obtain context and informa-tion.
besides that, pae-dgl (ding et al., 2019)and rthn (xia et al., 2019) use similar globalprediction embedding (gpe) to twist the clauses‚ÄôÔ¨Årst-round predictions.
mann (li et al., 2019)performs multi-head attention in cnn to jointlyencode the emotion and candidate clauses.
lamb-damart (xu et al., 2019) uses the relative posi-tion, word-embedding similarity and topic similar-ity as emotion-related feature to extract cause..table 1 shows the cause clause classiÔ¨Åcation re-sults on the ece dataset.
two rule-based meth-ods have poor performances, possibly due to theirpre-deÔ¨Åned rules.
multi-kernel performs betterthan the vanilla svm, being able to leverage morecontextual information.
across the other threegroups, the precision scores are higher than recallscores, and it is probably due to the unbalancednumber of cause clauses (18.36%) and non-causeclauses (81.64%), leading the models to predict aclause as non-cause more often..models in the position-aware group perform bet-ter than those in the other groups, indicating theimportance of position information.
our proposedmodel outperforms all the other models exceptrhnn in which its recall score is slightly lower.
we have also performed ablation studies by remov-ing either k-edge or s-edge, or both of them (w/or-gcns).
the results show that removing the r-gcns leads to a drop of nearly 4.3% in f1.
also,both the k-edge and s-edge contributes to emo-tion cause extraction.
as contextual modelling hasconsidered the position information, the removalof s-edge leads to a smaller drop compared to theremoval of k-edge..4.2.impact of encoding clause positioninformation.
in order to examine the impact of using the clauseposition information in different models, we re-place the relative position information of the candi-date clause with absolute positions.
in the extremecase, we remove the position information from themodels.
the results are shown in figure 4. it canbe observed that the best results are achieved usingrelative positions for all models.
replacing relativepositions using either absolution positions or noposition at all results in a signiÔ¨Åcant performancedrop.
in particular, mann and pae-dgl haveover 50-54% drop in f1.
the performance degra-dation is less signiÔ¨Åcant for rthn, partly due toits use of the transformer architecture for contextmodeling.
nevertheless, we have observed a de-crease in f1 score in the range of 20-35%.
ourproposed model is less sensitive to the relative po-sitions of candidate clauses.
its robust performancepartly attributes to the use of (1) hierarchical con-textual modeling via the transformer structure, and(2) the k-egde which helps explore causal links viacommonsense knowledge regardless of a clause‚Äôs.
3369to locate the least likely cause clause, we pro-pose to choose the value for r2 according to theattention score between a candidate clause and theemotion clause.
our intuition is that if the emotionclause has a lower score attended to a candidateclause, then it is less likely to be the cause clause.
we use an existing emotion cause extraction modelto generate contextual representations and use thedot-attention (luong et al., 2015) to measure thesimilarity between each candidate clause and theemotion clause.
we then select the index i whichgives the lowest attention score and assign it to r2:.
r2 = arg min.
{Œªi}n.i=1, Œªi = dot-att.
( ÀÜci, ÀÜce),.
(8).
i.where ÀÜci is the representation of the i-th candidateclause, ÀÜce is the representation of the emotionclause, and n denotes a total of n clauses in adocument..here, we use existing ece models as differ-ent discriminators to generate different adversarialsamples.8 the desirable adversarial samples willfool the discriminator to predict the inverse label.
we use leave-one-model-out to evaluate the perfor-mance of ece models.
in particular, one model isused as a discriminator for generating adversarialsamples which are subsequently used to evaluatethe performance of other models..results the results are shown in table 2. theattacked ece models are merely trained on theoriginal dataset.
the generated adversarial exam-ples are used as the test set only.
we can observea signiÔ¨Åcant performance drop of 23-32% for theexisting ece models, some of which even performworse than the earlier rule-based methods, showingtheir sensitivity to the positional bias in the dataset.
we also observe the performance degradation ofour proposed kag.
but its performance drop is lesssigniÔ¨Åcant compared to other models.
the resultsverify the effectiveness of capturing the semanticdependencies between a candidate clause and theemotion clause via contextual and commonsenseknowledge encoding..4.4 case study and error analysis.
to understand how kag aggregate informationbased on different paths, we randomly choosetwo examples to visualise the attention distribu-tions (eq.
4) on different graph nodes (i.e., clauses).
8the adversarial sample generation is independent from.
figure 4: emotion cause extraction when using rela-tive, absolute or no clause positional information.
ourmodel demonstrates most stable performance withoutthe relative position information..relative position..4.3 performance under adversarial samples.
in recent years, there have been growing inter-ests in understanding vulnerabilities of nlp sys-tems (goodfellow et al., 2015; ebrahimi et al.,2017; wallace et al., 2019; jin et al., 2020).
adver-sarial examples explore regions where the modelperforms poorly, which could help understandingand improving the model.
our purpose here is toevaluate if kag is vulnerable as existing ece mod-els when the cause clauses are not in proximity tothe emotion clause.therefore, we propose a prin-cipled way to generate adversarial samples suchthat the relative position is no longer an indicativefeature for the ece task..generation of adversarial examples we gen-erate adversarial examples to trick ece models,which relies on swapping two clauses cr1 and cr2,where r1 denotes the position of the most likelycause clause, while r2 denotes the position of theleast likely cause clause..we identify r1 by locating the most likely causeclause based on its relative position with respect tothe emotion clause in a document.
as illustratedin figure 1, over half of the cause clauses are im-mediately before the emotion clause in the dataset.
we assume that the position of a cause clause canbe modelled by a gaussian distribution and esti-mate the mean and variance directly from the data,which are, {¬µ, œÉ2} = {‚àí1, 0.5445}.
the positionindex r1 can then be sampled from the gaussiandistribution.
as the sampled value is continuous,we round the value to its nearest integer:.
r1 ‚Üê (cid:98)g(cid:101),.
g (cid:118) gaussian(¬µ, œÉ2)..(7).
their training process..337065.4972.4276.7777.0815.3118.3956.9469.4315.0917.941.4568.291525354555657585mannpae-dglrthnoursf1(%)relativepositionabsolutepositionnopositiondiscriminator.
paedgl.
attacked ece models.
paedgl mann48.92.
49.62.rthn59.73.kag64.98.
‚Üì31.76% ‚Üì28.6% ‚Üì 22.20% ‚Üì 16.08%.
mann.
51.82.
47.24.
60.13.
66.32.rthn.
kag.
ave. drop(%).
‚Üì28.45% ‚Üì31.27% ‚Üì21.65% ‚Üì14.35%.
48.63.
49.63.
57.78.
63.47.
‚Üì32.85% ‚Üì 27.64% ‚Üì 24.74% ‚Üì18.03%.
48.52.
48.24.
59.53.
62.39.
‚Üì 33.00% ‚Üì29.67% ‚Üì22.46% ‚Üì19.42%‚Üì31.51% ‚Üì29.29% ‚Üì22.62% ‚Üì16.97%.
table 2: f1 score and relative drop (marked with ‚Üì)of different ece models on adversarial samples.
thelisted four ece models are attacked by the adversar-ial samples generated from the respective discriminator.
our model shows the minimal drop rate comparing toother listed ece models across all sets of adversarialsamples..in figure 5.9 these attention weights show the ‚Äòdis-tance‚Äô between a candidate clause and the emotionclause during the reasoning process.
the causeclauses are underlined, and keywords are in bold.
ci in brackets indicate the relative clause positionto the emotion clause (which is denoted as c0)..ex.1 the crime that ten people were killed shocked thewhole country (c‚àí4).
this was due to personal grievances(c‚àí3).
qiu had arguments with the management staff (c‚àí2),and thought the taoist temple host had molested his wife(c‚àí1).
he became angry (c0), and killed the host and de-stroyed the temple (c1)..in ex.1, the emotion word is ‚Äòangry‚Äô, the knowl-edge path identiÔ¨Åed by our model from conceptnetis, ‚Äúarguments ‚Üí Ô¨Åght ‚Üíangry‚Äù for clause c‚àí2,and ‚Äúmolest ‚Üí irritate ‚Üíexasperate‚Üíangry‚Äù forclause c‚àí1.
our model assigns the same attentionweight to the clauses c‚àí2, c‚àí1 and the emotionclause, as shown in figure 5. this shows that bothpaths are equally weighted by our model.
due tothe k-edge attention weights, our model can cor-rectly identify both c‚àí2 and c‚àí1 clauses as thecause clauses..ex.2 the longbao primary school locates between the twovillages (c‚àí2).
some unemployed people always cut throughthe school to take a shortcut (c‚àí1).
liu yurong worried thatit would affect children‚Äôs study (c0).
when he did not haveteaching duties (c1), he stood guard outside the school gate(c2)..in ex.2, the path identiÔ¨Åed by our model fromconceptnet for clause (c‚àí1) is ‚Äúunemployment‚Üí situation ‚Üí trouble/danger‚Üí worried‚Äù.
it has.
9more cases can be found in the appendix..been assigned the largest attention weight as shownin figure 5. note that the path identiÔ¨Åed is spurioussince the emotion of ‚Äòworried‚Äô is triggered by ‚Äòun-employment‚Äô in the conceptnet, while in the origi-nal text, ‚Äòworried‚Äô is caused by the event, ‚Äòunem-ployed people cut through the school‚Äô.
this showsthat simply using keywords or entities searching forknowledge paths from commonsense knowledgebases may lead to spurious knowledge extracted.
we will leave the extraction of event-driven com-monsense knowledge as future work..figure 5: attention weights among different graphnodes/clauses on ex.1 and ex.2..5 conclusion and future work.
in this paper, we examine the positional bias in theannotated ece dataset and investigate the degreeof reliance of the clause position information inexisting ece models.
we design a novel approachfor generating adversarial samples.
moreover, wepropose a graph-based model to enhance the seman-tic dependencies between a candidate clause and agiven emotion clause by extracting relevant knowl-edge paths from conceptnet.
the experimental re-sults show that our proposed method achieves com-parative performance to the state-of-the-art meth-ods, and is more robust against adversarial attacks.
our current model extracts knowledge paths link-ing two keywords identiÔ¨Åed in two separate clauses.
in the future, we will exploit how to incorporate theevent-level commonsense knowledge to improvethe performance of emotion cause extraction..acknowledgements.
this work was funded by the epsrc (grant no.
ep/t017112/1, ep/v048597/1).
hy receives thephd scholarship funded jointly by the universityof warwick and the chinese scholarship coun-cil.
yh is supported by a turing ai fellowshipfunded by the uk research and innovation (grantno.
ep/v020579/1).
we thank yizhen jia and.
33710.08670.6130.1570.08690.0560.0820.0520.2620.2620.2620.0820.00.10.20.30.40.50.60.7pre4pre3pre3pre1emotionnext1next2attentionweightsclauselocationex.1ex.2daoye zhu for their valuable work on earlier codeframework of this paper.
we also thank the anony-mous reviewers for their valuable comments..ian j. goodfellow, jonathon shlens, and christianszegedy.
2015. explaining and harnessing adversar-ial examples..references.
antoine bordes, nicolas usunier, alberto garcia-duran,jason weston, and oksana yakhnenko.
2013. translating embeddings for modeling multi-in c. j. c. burges, l. bottou,relational data.
m. welling, z. ghahramani, and k. q. weinberger,editors, advances in neural information processingsystems 26, nips13, pages 2787‚Äì2795..ying chen, wenjun hou, xiyao cheng, and shoushanli.
2018. joint learning for emotion classiÔ¨Åcationand emotion cause detection.
in proceedings of the2018 conference on empirical methods in naturallanguage processing, pages 646‚Äì651, brussels, bel-gium.
association for computational linguistics..xiyao cheng, ying chen, bixiao cheng, shoushanli, and guodong zhou.
2017a.
an emotion causecorpus for chinese microblogs with multiple-userstructures.
acm transactions on asian and low-resource language information processing, 17:1‚Äì19..xiyao cheng, ying chen, bixiao cheng, shoushan li,and guodong zhou.
2017b.
an emotion cause cor-pus for chinese microblogs with multiple-user struc-tures.
acm transaction asian low-res.
for lang.
inf.
process., 17(1)..jiayuan ding and mayank kejriwal.
2020. an experi-mental study of the effects of position bias on emo-tion causeextraction.
corr, abs/2007.15066..zixiang ding, huihui he, mengran zhang, and ruixia.
2019.from independent prediction to re-ordered prediction: integrating relative position andglobal label information to emotion cause identiÔ¨Åca-tion.
in the thirty-third aaai conference on arti-Ô¨Åcial intelligence, aaai 2019, pages 6343‚Äì6350..javid ebrahimi, anyi rao, daniel lowd, and de-jing dou.
2017. hotÔ¨Çip: white-box adversarialarxiv preprintexamples for text classiÔ¨Åcation.
arxiv:1712.06751..chuang fan, hongyu yan, jiachen du, lin gui, li-dong bing, min yang, ruifeng xu, and ruibinmao.
2019. a knowledge regularized hierarchicalin proceed-approach for emotion cause analysis.
ings of the 2019 conference on empirical methodsin natural language processing and the 9th inter-national joint conference on natural language pro-cessing (emnlp-ijcnlp), pages 5614‚Äì5624, hongkong, china.
association for computational lin-guistics..lin gui, jiannan hu, yulan he, ruifeng xu, qin lu,and jiachen du.
2017. a question answering ap-in proceed-proach for emotion cause extraction.
ings of the 2017 conference on empirical meth-ods in natural language processing, emnlp 2017,copenhagen, denmark, september 9-11, 2017,pages 1593‚Äì1602..lin gui, dongyin wu, ruifeng xu, qin lu, andyu zhou.
2016. event-driven emotion cause extrac-tion with corpus construction.
in proceedings of the2016 conference on empirical methods in naturallanguage processing, emnlp 2016, austin, texas,usa, november 1-4, 2016, pages 1639‚Äì1649..di jin, zhijing jin, joey tianyi zhou, and peterszolovits.
2020. is bert really robust?
a strong base-line for natural language attack on text classiÔ¨Åcationin proceedings of the aaai con-and entailment.
ference on artiÔ¨Åcial intelligence, volume 34, pages8018‚Äì8025..evgeny kim and roman klinger.
2018. who feelswhat and why?
annotation of a literature corpusin proceedingswith semantic roles of emotions.
of the 27th international conference on computa-tional linguistics, pages 1345‚Äì1359, santa fe, newmexico, usa.
association for computational lin-guistics..yoon kim.
2014..convolutional neural networksin proceedings of thefor sentence classiÔ¨Åcation.
2014 conference on empirical methods in naturallanguage processing (emnlp), pages 1746‚Äì1751,doha, qatar..sophia yat mei lee, ying chen, and chu-ren huang.
2010. a text-driven rule-based system for emotioncause detection.
in proceedings of the naacl hlt2010 workshop on computational approaches toanalysis and generation of emotion in text, pages45‚Äì53, los angeles, ca.
association for computa-tional linguistics..xiangju li, shi feng, daling wang, and yifei zhang.
2019. context-aware emotion cause analysis withmulti-attention-based neural network.
knowledge-based systems, 174:205 ‚Äì 218..xiangju li, kaisong song, shi feng, daling wang, andyifei zhang.
2018. a co-attention neural networkmodel for emotion cause analysis with emotionalcontext awareness.
in proceedings of the 2018 con-ference on empirical methods in natural languageprocessing, brussels, belgium, october 31 - novem-ber 4, 2018, pages 4752‚Äì4757..kai gao, hua xu, and jiushuo wang.
2015. a rule-based approach to emotion cause detection for chi-nese micro-blogs.
expert systems with applications,42(9):4517 ‚Äì 4528..bill yuchen lin, xinyue chen, jamin chen, and xi-ang ren.
2019. kagnet: knowledge-aware graphnetworks for commonsense reasoning.
in proceed-ings of the 2019 conference on empirical methods.
3372rui xia, mengran zhang, and zixiang ding.
2019.rthn: a rnn-transformer hierarchical network forin proceedings of theemotion cause extraction.
twenty-eighth international joint conference on ar-tiÔ¨Åcial intelligence, ijcai 2019, macao, china, au-gust 10-16, 2019, pages 5285‚Äì5291..wenhan xiong, thien hoang, and william yang wang.
2017. deeppath: a reinforcement learning methodfor knowledge graph reasoning.
in proceedings ofthe 2017 conference on empirical methods in nat-ural language processing (emnlp 2017), copen-hagen, denmark.
acl..b. xu, h. lin, y. lin, y. diao, l. yang, and k. xu.
2019. extracting emotion causes using learning torank methods from an information retrieval perspec-tive.
ieee access, 7:15573‚Äì15583..ruifeng xu, jiannan hu, qin lu, dongyin wu, andlin gui.
2017. an ensemble approach for emo-tion cause detection with event extraction and multi-tsinghua science and technology,kernel svms.
22(6):646‚Äì659..xinyi yu, wenge rong, zhuo zhang, yuanxin ouyang,and zhang xiong.
2019. multiple level hierarchicalnetwork-based clause selection for emotion causeextraction.
ieee access, 7:9071‚Äì9079..in natural language processing and the 9th inter-national joint conference on natural language pro-cessing, emnlp-ijcnlp 2019, hong kong, china,november 3-7, 2019, pages 2829‚Äì2839..thang luong, hieu pham, and christopher d. man-ning.
2015. effective approaches to attention-basedin proceedings of theneural machine translation.
2015 conference on empirical methods in naturallanguage processing, emnlp 2015, lisbon, portu-gal, september 17-21, 2015, pages 1412‚Äì1421..laura oberl¬®ander and roman klinger.
2020. sequencelabeling vs. clause classiÔ¨Åcation for english emotionstimulus detection.
in proceedings of the 9th jointconference on lexical and computational seman-tics (*sem 2020), barcelona, spain.
association forcomputational linguistics..soujanya poria, navonil majumder, devamanyu haz-arika, deepanway ghosal, rishabh bhardwaj, sam-son yu bai jian, romila ghosh, niyati chhaya,alexander gelbukh, and rada mihalcea.
2020. rec-arxivognizing emotion cause in conversations.
preprint arxiv:2012.11820..hannah rashkin, antoine bosselut, maarten sap,kevin knight, and yejin choi.
2018. modelingnaive psychology of characters in simple common-in proceedings of the 56th annualsense stories.
meeting of the association for computational lin-guistics (volume 1: long papers), pages 2289‚Äì2299,melbourne, australia..irene russo, tommaso caselli, francesco rubino, es-ter boldrini, and patricio mart¬¥ƒ±nez-barco.
2011.emocause: an easy-adaptable approach to extractemotion cause contexts.
in proceedings of the 2ndworkshop on computational approaches to subjec-tivity and sentiment analysis, wassa@acl 2011,portland, or, usa, june 24, 2011, pages 153‚Äì160..michael schlichtkrull, thomas n. kipf, peter bloem,rianne vanden berg, and max welling.
2018. mod-eling relational data with graph convolutional net-works.
in european semantic web conference..robyn speer, joshua chin, and catherine havasi.
2017.conceptnet 5.5: an open multilingual graph of gen-eral knowledge.
in proceedings of the thirty-firstaaai conference on artiÔ¨Åcial intelligence, febru-ary 4-9, 2017, san francisco, california, usa,pages 4444‚Äì4451..eric wallace, shi feng, nikhil kandpal, matt gardner,and sameer singh.
2019. universal adversarial trig-gers for attacking and analyzing nlp.
arxiv preprintarxiv:1908.07125..rui xia and zixiang ding.
2019. emotion-cause pairextraction: a new task to emotion analysis in texts.
in proceedings of the 57th conference of the as-sociation for computational linguistics, acl 2019,florence, italy, july 28- august 2, 2019, volume 1:long papers, pages 1003‚Äì1012..3373a model architecture.
in this section, we describe the details of the fourmain components in our model: contextual mod-elling, knowledge path encoding, clause graph up-date and cause clause classiÔ¨Åcation..the dataset has 2,105 documents.
the maximumnumber of clauses in a document is 75 and themaximum number of words per clause is 45. so weÔ¨Årst pad the input documents into a matrix i withthe shape of [2105, 75, 45]..a.1 contextual modelling.
a. token ‚Üí clause we Ô¨Årst apply a 1-layer bi-lstm of 100 hidden units to obtain word embed-dings, w ‚àà r200.
we then use two linear transfor-mation layers (hidden units are [200,200],[200,1])to map the original w to a scalar attention score Œ±,then perform a weighted aggregation to generatethe clause representation ÀÜci ‚àà r200.
b. clause ‚Üí document we feed the clause repre-sentations into a transformer.
it has 3 stackedblocks, with the multi-head number set to 5, andthe dimension of key, value, query is all set to 200.the query vector is the emotion clause representa-tion ÀÜce ‚àà r200, the key and value representationsare candidate clause representations, also with 200dimensions.
finally, the updated clause representa-tions are aggregated via dot-attention to generatethe document representation d ‚àà r200..a.2 knowledge path encoding.
for each candidate clause and the emotion clause,we extract knowledge paths from conceptnet andonly select k paths.
the values of k is set to 15,since the median of the number of paths betweena candidate clause and the emotion clause is 15 inour dataset..we use the same bi-lstm described in sectiona.1 to encode each knowledge path and generatethe k number of path representations {pit}kt=1 be-tween the i-th clause and the emotion clause.
then,the document representation d is applied as thequery to attend to each path in {pit} to generate theÔ¨Ånal context-aware path representation si ‚àà r200..the non-linear functions are independent selu lay-ers..a.4 cause clause classiÔ¨Åcation.
the mlp with [400,1] hidden units takes the con-catenation of each candidate node {hli }ni=1 and theemotion node representation hle to predict the logit,after which, a softmax layer is applied to predictthe probability of the cause clause..b training details for kag.
we randomly split the datasets into 9:1 (train/test).
for each split, we run 50 iterations to get the bestmodel on the validation set, which takes an averagetime of around 23 minutes per split, when con-ducted on a nvidia gtx 1080ti.
for each split,we test the model on the test set at the end of eachiteration and keep the best resulting f1 of the split.
the number of model parameters is 1,133,002..hyper-parameter search we use the gridsearch to Ô¨Ånd the best parameters for our modelon the validation data, and report in the follow-ing the hyper-parameter values providing the bestperformance..‚Ä¢ the word embeddings used to initialise thebi-lstm is provided by nlpcc10.
it waspre-trained on a 1.1 million chinese weibocorpora following the word2vec algorithm.
the word embedding dimension is set to 200..‚Ä¢ the position embedding dimension is set to50, randomly initialised with the uniform dis-tribution (-0.1,0.1)..‚Ä¢ the number of transformer blocks is 2 and.
the number of graph layers is 3..‚Ä¢ to regularise against over-Ô¨Åtting, we employdropout (0.5 in the encoder, 0.2 in the graphlayer)..‚Ä¢ the network is trained using the the adam op-timiser with a mini-batch size 64 and a learn-ing rate Œ∑ = 0.005. the parameters of ourmodel are initialised with glorot initialisation..a.3 clause graph update.
c error analysis.
the graph nodes are initialised by clause presenta-tions, with the feature dimension 200. to calculatethe attention weights eie in r-gcns, we use thenon-linearly transformed hi + si as the query, thenon-linearly transformed he as the value and key..we perform error analysis to identify the limitationsof the proposed model.
in the following examples(ex.1 and ex.2), the cause clauses are in bold, ourpredictions are underlined..10https://github.com/nustm/rthn/tree/master/data.
33740.79, while annotator 2 achieved 0.938 agreementwith the cohen‚Äôs kappa value of 0.72. this alignswith our intuition that an emotion expressed in textis triggered by a certain event, rather than deter-mined by relative clause positions.
a good ecemodel should be able to learn a correlation betweenan event and its associated emotion.
this also mo-tivates our proposal of a knowledge-aware modelwhich leverages commonsense knowledge to ex-plicitly capture event-emotion relationships..ex.1 some kind people said (c‚àí6), if wu xiaolicould Ô¨Ånd available kidneys (c‚àí5), they wouldlike to donate for her surgery (c‚àí4).
4000rmbdonation had been sent to xiaoli (c‚àí3), qiu huasaid (c‚àí2).
the child‚Äôs desire to survival shockedus (c‚àí1).
the family‚Äôs companion was touch-ing (c0).
wish kind people will be ready to givea helping hand (c1).
help the family in difÔ¨Åculty(c2)..in the Ô¨Årst example ex.1, our model identiÔ¨Åesthe keyword survival in c‚àí1 and extracts severalpaths from ‚Äòsurvival‚Äô to ‚Äòtouching‚Äô.
however, themain event in clause c‚àí1 concerns desire ratherthan survival.
our current model detects the emo-tion reasoning process from conceptnet based onkeywords identiÔ¨Åed in text, and inevitably intro-duces spurious knowledge paths to model learning..i have only one daughter (c0), and a grand-ex.2daughter of 8 year-old (c‚àí10).
i would like toconvey these memory to her (c‚àí9).
last springfestival (c‚àí8), i gave the dvd away to my grand-daughter (c‚àí7).
i hope she can inherit my memory(c‚àí6).
thus (c‚àí5), i feel like that my ages becomeeternity (c‚àí4).
sun qing said (c‚àí3).
his father isa sensitive and has great passion for his life (c‚àí2).
he did so (c‚àí1).
making me feel touched (c0).
his daughter said (c1)..in the ex 2, our model detected the passion as akeyword and extracted knowledge paths betweenthe clause c‚àí2 and the emotion clause.
however,it ignores the semantic dependency between theclause c‚àí1 and the emotion clause.
it is thereforemore desirable to consider semantic dependenciesor discourse relations between clauses/sentencesfor emotion reasoning path extraction from externalcommonsense knowledge sources..d human evaluation on the generated.
adversarial samples.
the way adversarial examples generated changesthe order of the original document clauses.
there-fore, we would like to Ô¨Ånd out if such clause re-ordering changes the original semantic meaningand if these adversarial samples can be used toevaluate on the same emotion cause labels..we randomly selected 100 adversarial examplesand ask two independent annotators to manuallyannotate emotion cause clauses based on the sameannotation scheme of the ece dataset.
comparedto the original annotations, annotator 1 achieved0.954 agreement with the cohen‚Äôs kappa value of.
3375