parameter-efﬁcient transfer learning with diff pruning.
demi guoharvard universitydguo@college.harvard.edu.
alexander m. rushcornell universityarush@cornell.edu.
yoon kimmit csailmit-ibm watson aiyoonkim@mit.edu.
abstract.
the large size of pretrained networks makesthem difﬁcult to deploy for multiple tasks instorage-constrained settings.
diff pruning en-ables parameter-efﬁcient transfer learning thatscales well with new tasks.
the approachlearns a task-speciﬁc “diff” vector that ex-tends the original pretrained parameters.
thisdiff vector is adaptively pruned during train-ing with a differentiable approximation to thel0-norm penalty to encourage sparsity.
asthe number of tasks increases, diff pruning re-mains parameter-efﬁcient, as it requires stor-ing only a small diff vector for each task.
sinceit does not require access to all tasks dur-ing training, it is attractive in on-device de-ployment settings where tasks arrive in streamor even from different providers.
diff prun-ing can match the performance of ﬁnetunedbaselines on the glue benchmark while onlymodifying 0.5% of the pretrained model’s pa-rameters per task and scales favorably in com-parison to popular pruning approaches..1.introduction.
task-speciﬁc ﬁnetuning of pretrained deep net-works is the dominant paradigm in contemporarynlp, achieving state-of-the-art results across asuite of natural language understanding tasks (de-vlin et al., 2019; liu et al., 2019c; yang et al., 2019;lan et al., 2020).
while straightforward and em-pirically effective, this approach is difﬁcult to scaleto multi-task, memory-constrained settings (e.g.
for on-device applications), as it requires shippingand storing a full set of model parameters for eachtask.
inasmuch as these models are learning gen-eralizable, task-agnostic language representationsthrough self-supervised pretraining, ﬁnetuning theentire model for each task seems especially proﬂi-gate..code: https://github.com/dguo98/diffpruning.
a popular approach to parameter-efﬁciency isto learn smaller compressed models for eachtask (gordon et al., 2020; sajjad et al., 2020; zhaoet al., 2020; sanh et al., 2020).
such approachesface a steep sparsity/performance tradeoff and keepa substantial amount of nonzero parameters per task(e.g.
10%-30%).
multi-task learning and feature-based transfer allow for more parameter-efﬁcienttransfer learning per task (liu et al., 2019b; clarket al., 2019; stickland & murray, 2019; reimers &gurevych, 2019).
these methods train a small num-ber of additional parameters (e.g.
a linear layer) ontop of a shared model.
however, multi-task learn-ing generally requires access to all tasks duringtraining to prevent catastrophic forgetting (french,1999), while feature-based transfer learning (e.g.
based on task-agnostic sentence representations) istypically outperformed by ﬁnetuning (howard &ruder, 2018)..an appealing middle ground is to ﬁnetune anextension of the base model for speciﬁc tasks.
thisapproach captures the training beneﬁts of ﬁne-tuning while maintaining the task modularity offeature-based transfer.
for example, adapters (re-bufﬁ et al., 2018) use smaller, task-speciﬁc modulesthat are inserted between layers of a model thisapproach does not require access to all tasks duringtraining, targeting realistic settings where as newtasks arrive in stream (houlsby et al., 2019; pfeifferet al., 2020a,b,c).
houlsby et al.
(2019) ﬁnd thatadapter layers can match the performance of fullyﬁnetuned bert on the glue benchmark whilerequiring 3.6% additional parameters (on average)per task..diff pruning is a new extension to pretrainedmodels with the goal of even more parameter-efﬁcient transfer learning.
instead of modifyingthe architecture of the model, diff pruning extendsthe base model through a task-speciﬁc differencevector..proceedingsofthe59thannualmeetingoftheassociationforcomputationallinguisticsandthe11thinternationaljointconferenceonnaturallanguageprocessing,pages4884–4896august1–6,2021.©2021associationforcomputationallinguistics4884in order to learn this vector, we reparameter-ize the task-speciﬁc model parameters as θtask =θpretrained + δtask, where the pretrained parametervector θpretrained is ﬁxed and the task-speciﬁc diffvector δtask is ﬁnetuned.
the diff vector is regu-larized with a differentiable approximation to thel0-norm penalty (louizos et al., 2018) to encour-age sparsity..diff pruning can become extremely parameter-efﬁcient, as it only requires storing the nonzeropositions and weights of the diff vector for eachtask.
the cost of storing the shared pretrainedmodel remains constant and is amortized acrossmultiple tasks.
on the glue benchmark (wanget al., 2019a), diff pruning can match the perfor-mance of the fully ﬁnetuned bert baselines whileﬁnetuning only 0.5% of the pretrained parametersper task.
as the number of tasks increase, diff prun-ing outperforms popular pruning-based methods inamount of storage required..2 background: transfer learning.
transfer learning in nlp mostly uses a pretrain-and-ﬁnetune paradigm, which initializes a subsetof the model parameters for all tasks from a pre-trained model and then ﬁnetunes on a task-speciﬁcobjective.
pretraining objectives include contextprediction (mikolov et al., 2013), autoencoding(dai & le, 2015), machine translation (mccannet al., 2017), and more recently, variants of lan-guage modeling (peters et al., 2018; radford et al.,2018; devlin et al., 2019) objectives..here we consider applying transfer learning tomultiple tasks.
we consider a setting with a po-tentially unknown set of tasks (which may arrivein stream), where each task τ ∈ t has an asso-ciated training set dτ = {x(n)n=1.
for alltasks, the goal is to produce (possibly tied) modelparameters θτ to minimize the empirical risk,.
τ }n., y(n).
τ.minθτ.
1n.n(cid:88).
n=1.
(cid:16).
c.fτ (x(n)τ.; θτ ), y(n).
τ.
+ λr(θτ ).
(cid:17).
where fτ (·; θτ ) is a parameterized function overthe input (e.g.
a neural network), c(·, ·) is a lossfunction (e.g.
cross-entropy),1 and r(·) is an op-tional regularizer with hyperparameter λ..we can use the pretrain-ﬁnetune approach bysimply learning independent parameters for each.
1while the loss function can be in principle task-speciﬁc,in practice we use cross entropy for all tasks and hence omitthe subscript in c(·, ·)..task.
however, the large size of pretrained modelsmakes this approach exceedingly parameter inefﬁ-cient.
for example, widely-adopted models suchas bertbase and bertlarge have 110m and340m parameters respectively, while their contem-poraries have parameter counts in the billions (raf-fel et al., 2020; shoeybi et al., 2019; rajbhandariet al., 2019).
storing the fully ﬁnetuned modelstherefore becomes difﬁcult even for a moderatenumber of tasks.2 a classic approach to tacklingthis parameter-inefﬁciencyis to train a single sharedmodel (along with a task-speciﬁc output layer)against multiple tasks through joint training (caru-ana, 1997).
however, the usual formulation ofmulti-task learning requires the set of tasks t to beknown in advance in order to prevent catastrophicforgetting (french, 1999),3 making it unsuitable forapplications in which the set of tasks is unknownor when tasks arrive in stream..3 diff pruning.
diff pruning formulates task-speciﬁc ﬁnetuning aslearning a diff vector δτ that is added to the pre-trained model parameters θ, which remain ﬁxed.
we ﬁrst reparameterize the task-speciﬁc model pa-rameters,.
θτ = θ + δτ ,.
which results in the following empirical risk mini-mization problem,.
minδτ.
l(dτ , fτ , θ + δτ ) + λr(θ + δτ ),.
where for brevity we deﬁne l(dτ , fτ , θτ ) as.
l(dτ , fτ , θτ ) =.
1n.n(cid:88).
n=1.
(cid:16).
c.fτ (x(n)τ.; θτ ), y(n).
τ.
(cid:17).
..this trivial reparameterization shows that the costof storing the pretrained parameters θ is amortizedacross tasks, and the only marginal cost for newtasks is the diff vector.
if we can regularize δτto be sparse such that (cid:107)δτ (cid:107)0 (cid:28) (cid:107)θ(cid:107)0, then thisapproach can become more parameter-efﬁcient as.
2an intriguing line of work suggests that large-scale lan-guage models can be used without ﬁnetuning for a variety oftasks if given the appropriate context (radford et al., 2019;brown et al., 2020).
while interesting, these models generallyunderperform task-speciﬁc models and require billions of pa-rameters, though recent work suggests that they can be madesubstantially smaller (schick & schutze, 2020)..3however, work on continual learning mitigates theseissues to an extent (shin et al., 2017; lopez-paz & ranzato,2017; lee et al., 2017; kirkpatrick et al., 2017)..4885the number of tasks increases.
we can specify thisgoal with an l0-norm penalty on the diff vector,.
r(θ + δτ ) = (cid:107)δτ (cid:107)0 =.
1{δτ,i (cid:54)= 0}..d(cid:88).
i=1.
3.1 differentiable approximation to the.
l0-norm.
this regularizer is difﬁcult to optimize as it is non-differentiable.
in order to approximate this l0 ob-jective, we follow an approach for gradient-basedlearning with l0 sparsity using a relaxed mask vec-tor (louizos et al., 2018).
this approach involvesrelaxing a binary vector into continuous space, andthen multiplying it with a dense weight vector todetermine how much of the weight vector is ap-plied during training.
after training, the mask ismade deterministic, and a large portion of the diffvector is zero.4.
to apply this method we ﬁrst decompose δτ intoa binary mask vector multiplied with a dense vector,.
δτ = zτ (cid:12) wτ ,.
zτ ∈ {0, 1}d, wτ ∈ rd..we now lower bound the true objective and op-timize an expectation with respect to zτ , whosedistribution p(zτ ; ατ ) is initially bernoulli withintroduced parameters ατ ,ezτ ∼p(zτ ;ατ ).
(cid:2)l(dτ , fτ , θ + δτ ) + λ(cid:107)δτ (cid:107)0.
(cid:3)..minατ ,wτ.
this objective is still complicated by the discretenature of zτ ’s, but the expectation provides someguidance for empirically effective relaxations.
wefollow prior work (louizos et al., 2018; wang et al.,2019b) and relax zτ into continuous space [0, 1]dwith a stretched hard-concrete distribution (janget al., 2017; maddison et al., 2017), which allowsfor the use of pathwise gradient estimators.
specif-ically, zτ is now deﬁned to be a deterministic and(sub)differentiable function of a sample u from auniform distribution,.
u ∼ u (0, 1),sτ = σ (log u − log(1 − u) + ατ ) ,¯sτ = sτ × (r − l) + l,zτ = min(1, max(0, ¯sτ ))..here l < 0 and r > 1 are two constants usedto stretch sτ into the interval (l, r)d before it is.
4it is also possible to learn sparse diff vectors through otherpenalties such as the l1-norm.
we chose to work with therelaxed l0-norm formulation as past work has shown thatsgd-based optimization works well in this setting..clamped to [0, 1]d with the min(1, max(0, ·)) op-eration.
in this case we have a differentiable closed-form expression for the expected l0-norm,d(cid:19)(cid:88).
(cid:18).
σ.ατ,i − log.
..e [(cid:107)δτ (cid:107)0] =.
−lr.i=1.
thus the ﬁnal optimization problem is given by,.
minατ ,wτ.
eu∼u [0,1] [l(dτ , fτ , θ + zτ (cid:12) wτ )].
+λ.
σ.ατ,i − log.
(cid:18).
d(cid:88).
i=1.
(cid:19).
,.
−lr.and we can now utilize pathwise gradient estima-tors to optimize the ﬁrst term with respect to ατsince the expectation no longer depends on it.5 af-ter training we obtain the ﬁnal diff vector δτ bysampling u once to obtain zτ (which is not nec-essarily a binary vector but has a signiﬁcant num-ber of dimensions equal to exactly zero due to theclamping function), then setting δτ = zτ (cid:12) wτ .6.
3.2 l0-ball projection with magnitudepruning for sparsity control.
differentiable l0 regularization allows us toachieve a high sparsity rate.
however, it would beideal to set an exact sparsity rate, especially consid-ering applications which require parameter budgets.
as the regularization coefﬁcient λ is a lagrangianmultiplier for the constraint e [(cid:107)δτ (cid:107)0] < η forsome η, this could be achieved in principle bysearching over different values of λ. however wefound it more efﬁcient and empirically effective toachieve an exact sparsity rate by projecting onto atarget l0-ball after training..speciﬁcally, we use magnitude pruning on thediff vector δτ and target a sparsity rate t% by onlykeeping the top t% × d values in δτ .7 note thatunlike standard magnitude pruning, this is basedon the magnitude of the diff vector values and notthe model parameters.
we found it important tofurther ﬁnetune δτ with the nonzero masks ﬁxedto maintain good performance, as is often the case.
5to reduce notation clutter we subsume the parameters ofthe task-speciﬁc output layer, which is not pretrained, intoθ. we do not apply the l0-norm penalty on these parametersduring training..6we found sampling once to work as well as other alterna-.
tives (e.g.
based on multiple samples)..7wang et al.
(2019b) show that it also is possible to injectsuch a constraint softly into the training objective by regular-izing the expected model size towards a certain rate.
however,since the constraint is soft this approach also makes it difﬁcultto target an exact sparsity rate..4886in magnitude pruning (han et al., 2016).
sincethis type of parameter-efﬁciency through projectiononto the l0-ball can be applied without adaptivediff pruning,8 such an approach will serve as oneof our baselines in the empirical study..3.3 structured diff pruningto allow diff pruning to adapt to the model archi-tecture, we consider a structured extension whichincorporates dependence between dimensions.
wehypothesize that this approach can allow the modelto learn to modify parameters in local regions, asopposed to treating each parameter independently.
we modify the regularizer to ﬁrst partition theparameter indices into g groups {g(1), .
.
.
, g(g)}where g(j) is a subset of parameter indices gov-erned by group g(j).9 we then introduce a scalarzjτ (with the associated parameter αjτ ) for eachgroup g(j), and decompose the task-speciﬁc pa-rameter for index i ∈ g(j) as δjτ · wτ,i.
the expected l0-norm is then given by(cid:88).
τ,i = zτ,i · zj.
g(cid:88).
e [1{zτ,i · zg.
τ > 0}].
e [(cid:107)δτ (cid:107)0] =.
j=1.
i∈g(j).
g(cid:88).
(cid:88).
(cid:18).
=.
σ.ατ,i − log.
(cid:19).
(cid:18).
· σ.αj.
τ − log.
−lr.(cid:19).
−lr.j=1.
i∈g(j).
we can train with gradient-based optimization asbefore.
parameters in a group are encouraged bythe regularizer to be removed jointly..4 experiments.
4.1 model and datasetsfor evaluation we use the glue benchmark (wanget al., 2019b) as well as the squad extractivequestion answering dataset (rajpurkar et al., 2016).
following adapters (houlsby et al., 2019), we testour approach on the following subset of the gluetasks: multi-genre natural language inference(mnli), where the goal is two predict whetherthe relationship between two sentences is entail-ment, contradiction, or neutral (we test on bothmnlim and mnlimm which respectively tests onmatched/mismatched domains); quora questionpairs (qqp), a classiﬁcation task to predict whethertwo question are semantically equivalent; ques-tion natural language inference (qnli), which.
8concretely, one can obtain θτ through usual ﬁnetuning,set δτ = θτ − θ, and then apply magnitude pruning followedby additional ﬁnetuning on δτ ..9while groups can be deﬁned in various ways, we foundthat deﬁning groups based on each matrix/bias vector of thepretrained model was simple and worked well enough..must predict whether a sentence is a correct an-swer to the question; stanford sentiment treebank(sst-2), a sentence classiﬁcation task to predict thesentiment of movie reviews; corpus of linguisticacceptability (cola), where the goal is predictwhether a sentence is linguistically acceptable ornot; semantic textual similarity benchmark (sts-b), which must predict a similarity rating betweentwo sentences; microsoft research paraphrase cor-pus (mrpc), where the goal is to predict whethertwo sentences are semantically equivalent; rec-ognizing textual entailment (rte), which mustpredict whether a second sentence is entailed bythe ﬁrst.
the benchmark uses matthew’s correla-tion for cola, spearman for sts-b, f1 score formrpc/qqp, and accuracy for mnli/qnli/sst-2/rte..for the main experiments and analysis, we usethe bertlarge model from devlin et al.
(2019)to compare against the adapter-based approach ofhoulsby et al.
(2019).
our implementation is basedon the hugging face transformer library (wolfet al., 2019)..4.2 baselines.
we compare both structured and non-structuredvariants of diff pruning against the followingbaselines: full ﬁnetuning, which fully ﬁnetunesbertlarge as usual; last layer ﬁnetuning,which only ﬁnetunes the penultimate layer (alongwith the ﬁnal output layer)10; adapters fromhoulsby et al.
(2019), which train task-speciﬁc bot-tleneck layers between each layer of a pretrainedmodel, where parameter-efﬁciency can be con-trolled by varying the size of the bottleneck lay-ers; and non-adaptive diff pruning, which per-forms diff pruning just based on magnitude prun-ing (i.e., we obtain θτ through usual ﬁnetuning,set δτ = θτ − θ, and then apply magnitude prun-ing followed by additional ﬁnetuning on δτ ).
fordiff pruning we set our target sparsity rate to 0.5%and investigate the effect of different target sparsityrates in section 6.1..4.3.implementation details andhyperparameters.
diff pruning introduces additional hyperparame-ters l, r (for stretching the hard-concrete distri-bution) and λ (for weighting the approximate l0-norm penalty).
we found l = −1.5, r = 1.5, λ =1.25 × 10−7 to work well across all tasks.
we.
10wu et al.
(2020) observe that ﬁnetuning later layers gen-.
erally performs better than ﬁnetuning earlier layers.
4887total new paramsparams.
per task.
qnli∗ sst-2 mnlim mnlimm cola mrpc sts-b rte qqp.
avg.
full ﬁnetuningadapters (8-256)adapters (64).
9.00×1.32×1.19×.
9.00×full ﬁnetuning1.34×last layernon-adap.
diff pruning 1.05×1.05×diff pruning1.05×diff pruning (struct.).
100%3.6%2.1%.
100%3.8%0.5%0.5%0.5%.
91.190.791.4.
93.479.889.792.993.3.
94.994.094.2.
94.191.693.693.894.1.
86.784.985.3.
86.771.484.985.786.4.
85.985.184.6.
86.072.984.885.686.0.
60.559.556.9.
59.640.251.260.561.1.
89.389.589.6.
88.980.181.587.089.7.
87.686.987.3.
86.667.378.283.586.0.
70.1 72.171.5 71.868.6 71.8.
71.2 71.758.6 63.361.5 68.668.1 70.670.6 71.1.
80.980.479.8.
80.668.275.579.480.6.table 1: glue benchmark test server results with bertlarge models.
(top) results with adapter bottleneck layers (bracketsindicate the size of bottlenecks), taken from from houlsby et al.
(2019).
(bottom) results from this work.
∗qnli results arenot directly comparable across the two works as the glue benchmark has updated the test set since then.
to make our resultscomparable the average column is calculated without qnli..also initialize the weight vector wτ to 0, and ατto a positive vector (we use 5) to encourage zτ tobe close to 1 at the start of training.11 while wemainly experiment with bert models to faciliatecomparison against existing work, in preliminaryexperiments we found these hyperparameters towork for ﬁnetuning roberta (liu et al., 2019c)and xlnet (yang et al., 2019) models as well..for all tasks we initially train for 3 epochs andperform a hyperparameter search over batch size∈ {5, 8, 12, 16} and learning rate ∈ {1×10−5, 2×10−5, 5 × 10−5}.12 finetuning with the ﬁxed maskafter projecting onto the l0-ball with magnitudepruning is done for 3 epochs with a learning rateof 5 × 10−5 for all datasets except for mrpc/sts-b/rte/sst-2 dataset, where we ﬁnetune for 5epochs.
the exact hyperparameters for each taskare given in section a.1 of the appendix.
groupingfor the structured version of diff pruning is based onthe matrix/bias vectors (i.e.
parameters that belongto the same matrix or bias vector are assumed to bein the same group), which results in 393 groups.13.
5 results.
5.1 results on glueour main results on the glue benchmark areshown in table 1.structured diff pruningcan match the performance of a fully ﬁnetunedbertlarge model while only requiring 0.5% ad-.
11these values were found via by a light hyperparameter.
search on the sst-2 validation set..12however we found the default settings used for regularﬁnetuning as suggested in the original bert paper to workwell for most tasks..13this deﬁnition of groups is implementation-speciﬁcsince it depends on how one concatenates the input vec-tor before each afﬁne layer.
our grouping is basedon hugging face’s bert implementation at commit656e1386a296d696327a9db37de2ccccc79e2cc7.
we foundthis simple deﬁnition to work well compared to alternativedeﬁnitions (e.g.
based on individual neurons)..ditional parameters per task.
diff pruning with-out structured sparsity also performs well, thoughslightly worse than the structured approach.
non-adaptive diff pruning, which magnitude prunes thediff vector without learning the binary mask zτ ,performs signiﬁcantly worse, indicating the impor-tance of learning the masking vector.
comparedto adapters, diff pruning obtains similar perfor-mance while requiring many fewer parameters pertask, making it a potential alternative for parameter-efﬁcient transfer learning.14.
5.2 results on squadto demonstrate the effectiveness of our approachbeyond the glue tasks, we additionally experi-ment on squad (rajpurkar et al., 2016), an extrac-tive question answering dataset where the modelhas to select the answer span to a question given awikipedia paragraph.
to make direct comparisonswith houlsby et al.
(2019), we run all experimentson squad v1.1.
for diff pruning, we use the samegeneral hyperparameters as our full ﬁnetuning base-line (see section a.1).
as shown in figure 1 (right),diff pruning is able achieve comparable or betterperformance with only 1.0% additional parameters.
interestingly, diff pruning measurably improves theupon the full ﬁnetuning baseline while modifyingfewer parameters, which indicates that diff pruningcan have a useful regularization effect on top ofparameter-efﬁciency..6 analysis.
6.1 varying the target sparsityin figure 1 (left), we plot results on the glue vali-dation set averaged across all tasks at target sparsity.
14comparing storage costs is a bit more challenging as it isimplementation-speciﬁc.
diff pruning incurs additional stor-age cost due to storing the nonzero positions of the diff vector.
see section 6.6 for storage comparison against adapters as-suming ﬂoat32 for weights and int32 for positions..4888houlsby et al.
(2019)full ﬁnetuningadapters.
this work.
full ﬁnetuningdiff pruningdiff pruning (struct.)
diff pruning (struct.).
squad.
new params.
f1.
100%2.0%.
100%1.0%0.5%1.0%.
90.790.4.
90.892.191.193.2.figure 1: (left) average performance on the glue validation set across different target sparsity rates for the different methods.
(right) results with bertlarge on the squad v1.1 validation set..diff vectortarget sparsity.
0.10%0.25%0.50%1.00%.
100%.
qnli.
sst-2 mnlim mnlimm cola mrpc sts-b rte qqp.
92.793.293.493.3.
93.5.
93.394.294.294.2.
94.1.
85.686.286.486.4.
86.5.
85.986.586.987.0.
87.1.
58.063.363.566.3.
62.8.
87.490.991.391.4.
91.9.
86.388.489.589.9.
89.8.
68.671.571.571.1.
71.8.
85.286.186.686.6.
87.6.avg.
82.584.584.885.1.
85.0.table 2: structured diff pruning results on the validation set with different target sparsity rates..rates of 0.1%, 0.25%, 0.5%, 1.0% for the differentbaselines.
structured diff pruning consistently out-performs non-structured and and non-adaptive vari-ants across different sparsity rates.
the advantageof adaptive methods becomes more pronounced atextreme sparsity rates.
in table 2, we report thebreakdown of accuracy of structured diff pruningacross different tasks and sparsity rates, where weobserve that different tasks have different sensi-tivity to target sparsity rates.
this suggests thatwe can obtain even greater parameter-efﬁciencythrough targeting task-speciﬁc sparsity rates in thediff vector..6.2 structured vs. non-structured diff.
pruning.
structured diff pruning introduces an additionalmask per group, which encourages pruning of en-tire groups.
this is less restrictive than traditionalgroup sparsity techniques that have been used withl0-norm relaxations, which force all parametersin a group to share the same mask (louizos et al.,2018; wang et al., 2019b).
however we still expectentire groups to be pruned out more often, whichmight bias the learning process towards either elim-inating completely or clustering together nonzerodiffs.
in table 3, we indeed ﬁnd that structured diffpruning leads to ﬁnetuned models that are muchmore likely to leave entire groups unchanged fromtheir pretrained values (zero diffs)..6.3 task-speciﬁc sparsitydifferent layers of pretrained models have beenargued to encode different information (liu et al.,2019a; tenney et al., 2019).
given that each taskwill likely recruit different kinds of language phe-nomena embedded in the hidden layers, we hypoth-esize that diff pruning will modify different parts ofthe pretrained model through task-speciﬁc ﬁnetun-ing.
figure 2 shows the percentage of nonzero diffparameters attributable to the different layers foreach task.
we ﬁnd that different tasks indeed mod-ify different parts of the network, although there aresome qualitative similarities between some tasks,for example between qnli & qqp (both must en-code questions), and mrpc & sts-b (both mustpredict similarity between sentences).
the embed-ding layer is very sparsely modiﬁed for all tasks.
while some of the variations in the sparsity distri-butions is due to simple randomness, we do observesome level of consistency over multiple runs of thesame task, as shown in section a.2 of the appendix.
the ability to modify different parts of the pre-trained model for each task could explain the im-proved parameter-efﬁciency of our approach com-pared to houlsby et al.
(2019)’s adapters, whichcan only read/write to the pretrained model at cer-tain points of the computational graph.15 this po-.
15to simulate this restricted setting, we tried applying diffpruning only on the fully-connected layers after the self-attention layers, and observed much worse performance..4889qnli.
sst-2 mnli.
cola mrpc sts-b.
rte.
qqp.
non-structuredstructured.
6.1%6.2%37.7% 64.6% 28.8% 20.8% 13.2% 12.2% 12.7% 34.9%.
6.1%.
6.4%.
7.1%.
6.1%.
6.4%.
6.0%.
avg.
6.3%28.1%.
table 3: percentage of groups where all of the parameters in the group are fully zero for structured vs. non-structured diffpruning at 0.5% target sparsity.
we group based on each matrix/bias vector, resulting in 393 groups in total..figure 2: percentage of modiﬁed parameters attributable to each layer for different tasks at 0.5% target sparsity.
the layers areordered from earlier to later (i.e.
the embedding layer is shown at the top).
the x-axis for each plot goes from 0% to 20%..tentially suggests that adapters with more ﬁne-grained access into model internals (e.g.
adaptersfor key/value/query transformations) might resultin even greater parameter-efﬁciency.
while left asfuture work, we also note that diff pruning can beapplied in conjunction with adapters, which mightfurther improve results..6.4 effect of l0-ball projectionapplying magnitude pruning to project onto the l0-ball was crucial in achieving exact sparsity targets.
as shown in table 4, we observed little loss in per-formance through this approach.
we reiterate thatit was crucial to ﬁnetune with a ﬁxed mask, evenfor the approach which does not apply magnitudepruning.16.
6.5 comparison against bert compressiondirect bert compression methods also providea straightforward approach to parameter-efﬁcienttransfer learning.
here we compare diff pruningagainst existing bert compression methods, inparticular distilbert (sanh et al., 2019), mobile-bert (sun et al., 2020b) and tinybert (jiaoet al., 2020).
in these experiments we apply diffpruning on the smaller bertbase model as theseworks typically utilize bertbase as the baseline.
as shown in table 5, we observe that diff pruningis more parameter-efﬁcient when considering allglue tasks while maintaining better performance.
of course, bert compression methods typicallyhave faster inference time (e.g.
tinybert4 is 9.4×faster that bertbase).
however we note that diff.
16without ﬁxed-mask ﬁnetuning, glue performance de-.
creases from 84.9 to 81.4..pruning can be applied on these methods, whichmay further improve parameter-efﬁciency whilemaintaining fast inference..6.6 storage cost.
finally, table 6 shows the actual memory require-ments for diff pruning compared to adapters fora python implementation.
while diff pruning re-quires storing positions in addition to the weights(unlike adapters which can just store the weights),diff pruning is still more storage-efﬁcient due tothe greater parameter-efﬁciency..6.7 discussion and caveatsfor training, our approach requires more memorythan usual ﬁnetuning due to additionally optimizingατ and wτ .
since the majority of gpu memoryis typically utilized by a minibatch’s intermediatelayers, this did not present a signiﬁcant challengefor pretrained models that we experimented within this study.
however, this could present an issueas model sizes get larger and larger.
after training,storing the task-speciﬁc diff vector requires storinga compressed version with both the nonzero posi-tions and weights, which incurs additional storagerequirements.
finally, while training efﬁciency wasnot a primary concern of this work, diff pruningwas also approximately 1.5× to 2× slower to trainper minibatch than regular ﬁnetuning..7 related work.
multi-task learning multi-task learning (caru-ana, 1997), broadly construed, aims to learn modelsand representations that can be utilized across a di-verse range of tasks, and offers a natural approach.
4890qnli sst-2 mnlim mnlimm cola mrpc sts-b rte qqp.
sparsity w/o mag.
pruningperf.
w/o mag.
pruning.
mag.
pruning.
1.5% 0.6%94.093.8.
93.4.
94.2.
0.8%86.2.
86.4.
0.8%86.8.
86.9.
1.6% 2.4%91.963.1.
3.3% 0.7% 0.6%86.571.889.7.
63.5.
91.3.
89.5.
71.5.
86.6.avg.
1.4%84.9.
84.8.table 4: (top) sparsity and performance without magnitude pruning on the validation set with structured diff pruning.
theseresults also apply ﬁxed-mask ﬁnetuning.
(bottom) performance with 0.5% target sparsity and ﬁxed-mask ﬁnetuning..total new paramsparams.
per task.
qnli sst-2 mnlim mnlimm cola mrpc sts-b rte qqp.
avg.
full ﬁnetuningdistilbert6tinybert6distilbert4tinybert4mobileberttiny.
9.00×5.53×5.53×4.31×1.20×1.24×.
9.00×full ﬁnetuningdiff pruning (struct.)
1.05×.
100%61.5%61.5%47.9%13.3%13.9%.
100%0.5%.
90.988.990.485.287.789.5.
90.990.0.
93.492.593.191.492.691.7.
93.492.9.
83.982.684.678.982.581.5.
83.983.7.
83.481.383.278.081.881.6.
83.583.4.
52.849.051.132.844.146.7.
52.152.0.
87.586.987.382.486.487.9.
87.988.0.
85.281.383.776.180.480.1.
83.684.5.
67.0 71.158.4 70.170.0 71.654.1 68.566.6 71.365.1 68.9.
66.2 70.766.4 70.3.
79.576.879.471.977.077.0.
79.179.0.table 5: comparison against existing bert compression works on glue.
“total params” and “new params per task”columns use bertbase as the baseline, which has 109m parameters.
for example this means that mobileberttiny has13.9% × 109m = 15.1m parameters per task.
(top) results of different bert variants, taken from table 1 of jiao et al.
(2020).
(bottom) structured diff pruning results on bertbase..new paramsper task.
storage (mb)per task.
full ﬁnetuningadapters (weights only)diff pruning (positions + weights).
100%3.6%0.5%.
1297.051.713.6.table 6: comparison of ﬁle sizes per task based on a basicpython implementation assuming ﬂoat32 for the weights andint32 for positions..to training parameter-efﬁcient deep models.
sev-eral works have shown that a single bert modelcan obtain good performance across multiple taskswhen jointly trained (liu et al., 2019b; clark et al.,2019; stickland & murray, 2019).
an alternativeapproach to multi-task learning that does not re-quire access to all tasks during training involvetraining smaller task-speciﬁc layers that interactwith a ﬁxed pretrained model (rebufﬁ et al., 2018;zhang et al., 2020a).
in particular, adapters (re-bufﬁ et al., 2018), which learn to read and writeto layers of a shared model, have been applied toobtain parameter-efﬁcient bert models (houlsbyet al., 2019; pfeiffer et al., 2020a,b,c).
in recentwork, li & liang (2021) and qin & eisner (2021)explore the use of learned prompts on top of pre-trained models to obtain task-speciﬁc models.
yetanother line of work targets extreme parameter-efﬁciency through task-agnostic sentence repre-sentations that can be used without ﬁnetuning fordownstream tasks (le & mikolov, 2014; kiroset al., 2015; wieting et al., 2016; hill et al., 2016;arora et al., 2017; conneau et al., 2017; cer et al.,2018; zhang et al., 2018; subramanian et al., 2018;.
reimers & gurevych, 2019; zhang et al., 2020b).
these feature-based transfer learning methods arehowever generally outperformed by fully ﬁnetunedmodels (howard & ruder, 2018)..model compression there has been much recentwork on compressing pretrained trained with self-supervision (see (ganesh et al., 2020) for a recentsurvey).
a particularly promising line of workfocuses on obtaining smaller pretrained models(for subsequent ﬁnetuning) through weight pruning(gordon et al., 2020; sajjad et al., 2020; chen et al.,2020) and/or knowledge distillation (sanh et al.,2019; sun et al., 2019; turc et al., 2019; jiao et al.,2020; sun et al., 2020b).
it would be interesting tosee whether our approach can be applied on top ofthese smaller pretrained models to for even greaterparameter-efﬁciency..learning to mask our work is closely related tothe line of work on learning to mask parts of deepnetworks with differentiable relaxations of binarymasks for model pruning and parameter sharing(wang et al., 2019b; zhao et al., 2020; sanh et al.,2020; radiya-dixit & wang, 2020; mallya et al.,2018; guo et al., 2019; sun et al., 2020a; cao et al.,2021).
while these works also enable parameter-efﬁcient transfer learning, they generally apply themasks directly on the pretrained parameters insteadof on the difference vector as in the present work.
regularization towards pretrained models fi-nally, diff pruning is also related to workswhich regularize the learning process towards pre-.
4891trained/shared models for continual learning (rusuet al., 2016; kirkpatrick et al., 2017; schwarzet al., 2018), domain adaptation (wiese et al., 2017;miceli barone et al., 2017), and stable ﬁnetuning(lee et al., 2020).
these works typically do not uti-lize sparse regularizers and target a different goalthan parameter-efﬁciency..8 conclusion.
we propose diff pruning as a simple approachfor parameter-efﬁcient transfer learning with pre-trained models.
experiments on standard nlpbenchmarks and models show that diff pruningcan match the performance of fully ﬁnetuned base-lines while requiring only a few additional param-eters per task, and can sometimes have a regu-larization effect and improve upon regular ﬁne-tuning.
we also propose a structured variantof diff pruning which provides further improve-ments.
avenues for future work include (i) inject-ing parameter-efﬁciency objectives directly intothe pretraining process (to pretrain models that arebetter suited towards sparse transfer learning), and(ii) combining diff pruning with other techniques(e.g.
adapters, model compression) to achieve evengreater parameter-efﬁciency..acknowledgements.
the authors would like to thank the anonymousreviewers for their valuable feedback on the initialdraft.
amr was supported by nsf 1704834 andnsf career 2037519..references.
sanjeev arora, yingyu liang, and tengyu ma.
a sim-ple but tough-to-beat baseline for sentence embed-dings .
in proceedings of iclr, 2017..tom b. brown, benjamin mann, nick ryder, melaniesubbiah, jared kaplan, prafulla dhariwal, arvindneelakantan, pranav shyam, girish sastry, amandaaskell, sandhini agarwal, ariel herbert-voss,gretchen krueger, tom henighan, rewon child,aditya ramesh, daniel m. ziegler, jeffrey wu,clemens winter, christopher hesse, mark chen,eric sigler, mateusz litwin, scott gray, benjaminchess, jack clark, christopher berner, sam mc-candlish, alec radford, ilya sutskever, and darioamodei.
language models are few-shot learners.
2020..steven cao, victor sanh, and alexander m. rush.
low-complexity probing via finding subnetworks.
in proceedings of naacl, 2021..rich caruana.
multitask learning.
machine learning,.
1997..daniel cer, yinfei yang, sheng-yi kong, nan hua,nicole limtiaco, rhomni st. john, noah constant,mario guajardo-cespedes, steve yuan, chris tar,brian strope, and ray kurzweil.
universal sentencein proceedings of emnlp:encoder for english.
system demonstrations, 2018..tianlong chen, jonathan frankle, shiyu chang, sijialiu, yang zhang, zhangyang wang, and michaelcarbin.
the lottery ticket hypothesis for pre-trained bert networks.
arxiv:2007.12223, 2020..kevin clark, minh-thang luong, urvashi khandelwal,christopher d. manning, and quoc v. le.
bam!
born-again multi-task networks for natural lan-guage understanding.
in proceedings of acl, 2019..alexis conneau, douwe kiela, holger schwenk, loicbarrault, and antoine bordes.
supervised learn-ing of universal sentence representations from nat-in proceedings ofural language inference data.
emnlp, 2017..andrew dai and quoc v. le.
semi-supervised se-quence learning.
in proceedings of nips, 2015..jacob devlin, ming-wei chang, kenton lee, andkristina toutanova.
bert: pre-training of deepbidirectional transformers for language under-standing.
in proceedings of naacl, 2019..robert french.
catastrophic forgetting in connection-ist networks.
trends in cognitive sciences, 3, 1999..prakhar ganesh, yao chen, xin lou, mohammad alikhan, yin yang, deming chen, marianne winslett,hassan sajjad, and preslav nakov.
compressinglarge-scale transformer-based models: a casestudy on bert.
arxiv:2002.11985, 2020..mitchell a. gordon, kevin duh, and nicholas an-drews.
compressing bert: studying the effectsin pro-of weight pruning on transfer learning.
ceedings of rep4nlp 2020 workshop at acl 2020,2020..yunhui guo, honghui shi, abhishek kumar, kristengrauman, tajana rosing, and rogerio feris.
spot-tune: transfer learning through adaptive fine-tuning.
in proceedings of cvpr, 2019..song han, huizi mao, and william j. dally.
deepcompression: compressing deep neural networkswith pruning, trained quantization and huffmancoding.
in proceedings of iclr, 2016..felix hill, kyunghyun cho, and anna korhonen.
learning distributed representations of sentencesfrom unlabelled data.
in proceedings of acl, 2016..neil houlsby, andrei giurgiu, stanislaw jastrzebski,bruna morrone, quentin de laroussilhe, andreagesmundo, and mona attariyanand sylvain gelly.
parameter-efﬁcient transfer learning for nlp.
in pro-ceedings of icml, 2019..4892jeremy howard and sebastian ruder.
universal lan-guage model fine-tuning for text classiﬁcation.
inproceedings of acl, 2018..eric jang, shixiang gu, and ben poole.
categoricalreparameterization with gumbel-softmax.
in pro-ceedings of iclr, 2017..xiaoqi jiao, yichun yin, lifeng shang, xin jiang,xiao chen, linlin li, fang wang, and qun liu.
tinybert: distilling bert for natural languagein proceedings of emnlp (find-understanding.
ings), 2020..james kirkpatrick, razvan pascanu, neil rabinowitz,joel veness, guillaume desjardins, andrei a. rusu,kieran milan, john quan, tiago ramalho, ag-nieszka grabska-barwinska, demis hassabis, clau-dia clopath, dharshan kumaran, and raia hadsell.
overcoming catastrophic forgetting in neural net-works.
proceedings of the national academy of sci-ences, 14:3521–3526, 2017..ryan kiros, yukun zhu, ruslan salakhutdinov,richard s. zemel, antonio torralba, raquel urta-sun, and sanja fidler.
skip-thought vectors.
inproceedings of neurips, 2015..zhenzhong lan, mingda chen, sebastian goodman,kevin gimpel, piyush sharma, and radu soricut.
albert: a lite bert for self-supervised learn-ing of language representations.
in proceedings oficlr, 2020..quoc v. le and tomas mikolov.
distributed represen-in proceed-.
tations of sentences and documents.
ings of icml, 2014..cheolhyoung lee, kyunghyun cho, and wanmo kang.
mixout: effective regularization to finetune large-scale pretrained language models.
in proceedingsof iclr, 2020..sang-woo lee, jin-hwa kim, jaehyun jun, jung-wooha, and byoung-tak zhang.
overcoming catas-trophic forgetting by incremental moment matching.
in advances in neural information processing sys-tems.
2017..xiang lisa li and percy liang..preﬁx-tuning:optimizing continuous prompts for generation.
arxiv:2101.00190, 2021..nelson f. liu, matt gardner, yonatan belinkov,matthew e. peters, and noah a. smith.
linguisticknowledge and transferability of contextual repre-sentations.
in proceedings of acl, 2019a..xiaodong liu, pengcheng he, weizhu chen, and jian-feng gao.
multi-task deep neural networks forin proceedingsnatural language understanding.
of acl, 2019b..a robustly optimized bert pretraining approach.
arxiv:1907.11692, 2019c..david lopez-paz and marc’aurelio ranzato.
gradientin pro-.
episodic memory for continual learning.
ceedings of neurips, 2017..christos louizos, max welling, diederik p, andkingma.
learning sparse neural networks throughl0 regularization.
in proceedings of iclr, 2018..chris j. maddison, andriy mnih, and yee whye teh.
the concrete distribution: a continuous relax-ation of discrete random variables.
in proceedingsof iclr, 2017..arun mallya, dillon davis, and svetlana lazebnik.
piggyback: adapting a single network to multipletasks by learning to mask weights.
in proceedingsof eccv, 2018..bryan mccann, james bradbury, caiming xiong, andrichard socher.
learned in translation: contextu-in proceedings of neurips.
alized word vectors.
2017..antonio valerio miceli barone, barry haddow, ulrichgermann, and rico sennrich.
regularization tech-niques for ﬁne-tuning in neural machine translation.
in proceedings of emnlp, 2017..tomas mikolov, kai chen, greg corrado, and jeffreydean.
efﬁcient estimation of word representationsin vector space.
arxiv:1301.3781, 2013..matthew peters, mark neumann, mohit iyyer, mattgardner, christopher clark, kenton lee, and lukezettlemoyer.
deep contextualized word represen-tations.
in proceedings of naacl, 2018..jonas pfeiffer, aishwarya kamath, andreas ruckle,and kyunghyun cho amd iryna gurevych.
adapter-fusion: non-destructive task composition fortransfer learning.
arxiv:2005.00247, 2020a..jonas pfeiffer, andreas ruckle, clifton poth, aish-warya kamath,ivan vulic, sebastian ruder,and iryna gurevych kyunghyun cho.
adapter-hub: a framework for adapting transformers.
arxiv:2007.07779, 2020b..jonas pfeiffer,.
ivan vulic,.
iryna gurevych, andsebastian ruder.
mad-x: an adapter-basedframework for multi-task cross-lingual transfer.
arxiv:2005.00052, 2020c..guanghui qin and jason eisner.
learning how to ask:querying lms with mixtures of soft prompts.
inproceedings of naacl, 2021..alec radford, karthik narasimhan, tim salimans, andilya sutskever.
improving language understandingby generative pre-training.
2018..yinhan liu, myle ott, naman goyal, jingfei du, man-dar joshi, danqi chen, omer levy, mike lewis,luke zettlemoyer, and veselin stoyanov.
roberta:.
alec radford, jeff wu, rewon child, david luan,dario amodei, and ilya sutskever.
language mod-els are unsupervised multitask learners.
2019..4893evani radiya-dixit and xin wang.
how ﬁne can ﬁne-tuning be?
learning efﬁcient language models.
inproceedings of aistats, 2020..colin raffel, noam shazeer, katherine leeadam roberts, sharan narang, michael matena,yanqi zhou, wei li, and peter j. liu.
exploringthe limits of transfer learning with a uniﬁedjournal of machinetext-to-text transformer.
learning research, 21, 2020..samyam rajbhandari, jeff rasley, olatunji ruwase,and yuxiong he.
zero: memory optimiza-tions toward training trillion parameter models.
arxiv:1910.02054, 2019..pranav rajpurkar, jian zhang, konstantin lopyrev, andpercy liang.
squad: 100,000+ questions for ma-in proceedings ofchine comprehension of text.
emnlp, 2016..s. rebufﬁ, a. vedaldi, and h. bilen..efﬁcientparametrization of multi-domain deep neural net-works.
in proceedings of cvpr, 2018..nils reimers and iryna gurevych..sentence-bert: sentence embeddings using siamese bert-networks.
in proceedings of emnlp, 2019..andrei a. rusu, neil c. rabinowitz, guillaume des-jardins, hubert soyer, james kirkpatrick, koraykavukcuoglu, razvan pascanu, and raia hadsell.
arxiv:1606.04671,progressive neural networks.
2016..hassan sajjad, fahim dalvi, nadir durrani, andpreslav nakov.
poor man’s bert: smaller andarxiv:2004.03844,faster transformer models.
2020..victor sanh, lysandre debut, julien chaumond, andthomas wolf.
distilbert, a distilled version ofbert: smaller, faster, cheaper and lighter.
in pro-ceedings of 5th workshop on energy efﬁcient ma-chine learning and cognitive computing, 2019..victor sanh, thomas wolf, and alexander m. rush.
movement pruning: adaptive sparsity by fine-tuning.
arxiv:2005.07683, 2020..timo schick and hinrich schutze.
it’s not just sizethat matters: small language models are alsofew-shot learners.
arxiv:2009.07118, 2020..jonathan schwarz, jelena luketina, wojciech m. czar-necki, agnieszka grabska-barwinska, yee whyeteh, razvan pascanu, and raia hadsell.
progress& compress: a scalable framework for continuallearning.
in proceedings of icml, 2018..hanul shin, jung kwon lee, jaehong kim, and jiwonkim.
continual learning with deep generative re-play.
in proceedings of neurips.
2017..mohammad shoeybi, mostofa patwary, raul puri,patrick legresley, jared casper, and bryan catan-zaro.
megatron-lm: training multi-billion pa-rameter language models using model parallelism.
arxiv:1909.08053, 2019..asa cooper stickland and iain murray.
bert andpals: projected attention layers for efﬁcient adapta-tion in multi-task learning.
in proceedings of icml,2019..sandeep subramanian, adam trischler, yoshua ben-gio, and christopher j. pal.
learning general pur-pose distributed sentence representations via largescale multi-task learning.
in proceedings of iclr,2018..siqi sun, yu cheng, zhe gan, and jingjing liu.
pa-tient knowledge distillation for bert model com-pression.
in proceedings of emnlp, 2019..ximeng sun, rameswar panda, and rogerio feris.
adashare: learning what to share for efﬁcientin proceedings ofdeep multi-task learning.
neurips, 2020a..zhiqing sun, hongkun yu, xiaodan song, renjie liu,yiming yang, and denny zhou.
mobilebert: acompact task-agnostic bert for resource-limiteddevices.
in proceedings of acl, july 2020b..ian tenney, dipanjan das, and ellie pavlick.
bertrediscovers the classical nlp pipeline.
in proceed-ings of acl, 2019..iulia turc, ming-wei chang, kenton lee, and kristinatoutanova.
well-read students learn better: onthe importance of pre-training compact models.
arxiv:1908.08962, 2019..alex wang, amanpreet singh, julian michael, felixhill, omer levy, and samuel bowman.
glue: amulti-task benchmark and analysis platform for natu-ral language understanding.
in proceedings of iclr,2019a..ziheng wang,.
jeremy wohlwend, and tao lei.
structured pruning of large language models.
arxiv:1910.04732, 2019b..georg wiese, dirk weissenborn, and mariana neves.
neural domain adaptation for biomedical questionanswering.
in proceedings of conll, august 2017..john wieting, mohit bansal, kevin gimpel, and karenlivescu.
towards universal paraphrastic sentenceembeddings.
in proceedings of iclr, 2016..thomas wolf, lysandre debut, victor sanh, julienchaumond, clement delangue, anthony moi, pier-ric cistac, tim rault, r´emi louf, morgan funtow-icz, joe davison, sam shleifer, patrick von platen,clara ma, yacine jernite, julien plu, canwen xu,teven le scao, sylvain gugger, mariama drame,quentin lhoest, and alexander m. rush.
hug-gingface’s transformers: state-of-the-art natural lan-guage processing.
arxiv, abs/1910.03771, 2019..4894a appendix.
a.1 hyperparameters.
table 7 shows hyperparameters we used for train-ing glue tasks.
for squad v1.1 experiments,we ran distributed training across 8 gpus, and usedper gpu batch size 3, maximum sequence length384, document stride 128, learning rate 3 × 10−5,number of initial training epochs 2 and number ofﬁnetuning epochs 2..a.2 consistency of nonzero parameters.
figure 3 shows the percentage of modiﬁed param-eters attributable to each layer across 5 runs ofsst-2.
we ﬁnd that there is nonotrivial variationin sparsity across runs, but also a degree of con-sistency.
for example, the ﬁrst layer is modiﬁedconsiderably more than other layers across all runs..john m. wu, yonatan belinkov, hassan sajjad, nadirdurrani, fahim dalvi, and james glass.
similarityanalysis of contextual word representation mod-els.
in proceedings of acl, 2020..zhilin yang, zihang dai, yiming yang, jaime car-bonell, russ r salakhutdinov, and quoc v le.
xlnet: generalized autoregressive pretrainingin proceedings offor language understanding.
neurips, 2019..jeffrey o zhang, alexander sax, amir zamir,leonidas guibas, and jitendra malik.
side-tuning:a baseline for network adaptation via additiveside networks.
in proceedings of eccv, 2020a..minghua zhang, yunfang wu, weikang li, and weili.
learning universal sentence representations withmean-max attention autoencoder.
in proceedings ofemnlp, 2018..yan zhang, ruidan he, zuozhu liu, kwan hui lim,and lidong bing.
an unsupervised sentence em-bedding method bymutual information maximiza-tion.
in proceedings of emnlp, 2020b..mengjie zhao, tao lin, martin jaggi, and hin-rich schutze.
masking as an efﬁcient alterna-tive to finetuning for pretrained language models.
arxiv:2004.12406, 2020..4895learning ratebatch sizetraining epochsfinetuning epochs.
qnli.
sst-2.
2 × 10−5833.
5 × 10−5835.mnlim mnlimm1 × 10−51 × 10−5883333.cola.
mrpc.
sts-b.
rte.
qqp.
1 × 10−5833.
1 × 10−5835.
1 × 10−51235.
1 × 10−5835.
2 × 10−5833.table 7: best hyperparameters for the glue tasks based on the respective validation sets..figure 3: percentage of modiﬁed parameters attributable to each layer for 5 different runs of sst-2 at 0.5% target sparsity.
thelayers are ordered from earlier to later (i.e.
the embedding layer is shown at the top).
the x-axis for each plot goes from 0% to20%..4896