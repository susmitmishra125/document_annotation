bertac: enhancing transformer-based language models withadversarially pretrained convolutional neural networks.
jong-hoon ohx ryu iidax{ julien kloetzerx kentaro torisawax{.
data-driven intelligent system research center (direct),national institute of information and communications technology (nict)xgraduate school of science and technology, naist{frovellia, ryu.iida, julien, torisawag@nict.go.jp.
abstract.
transformer-based language models (tlms),such as bert, albert and gpt-3, haveshown strong performance in a wide range ofnlp tasks and currently dominate the ﬁeldof nlp.
however, many researchers wonderwhether these models can maintain their dom-inance forever.
of course, we do not haveanswers now, but, as an attempt to ﬁnd bet-ter neural architectures and training schemes,we pretrain a simple cnn using a gan-stylelearning scheme and wikipedia data, and thenintegrate it with standard tlms.
we showthat on the glue tasks, the combination ofour pretrained cnn with albert outper-forms the original albert and achieves asimilar performance to that of sota.
fur-thermore, on open-domain qa (quasar-t andsearchqa), the combination of the cnn withalbert or roberta achieved stronger per-formance than sota and the original tlms.
we hope that this work provides a hint fordeveloping a novel strong network architec-ture along with its training scheme.
oursource code and models are available athttps://github.com/nict-wisdom/bertac..1.introduction.
transformer-based language models (tlms) suchas bert (devlin et al., 2019), albert (lanet al., 2020), and gpt-3 (brown et al., 2020) haveshown that large-scale self-supervised pretrainingleads to strong performance on various nlp tasks.
many researchers have used tlms for variousdownstream tasks, possibly as subcomponents oftheir methods, and/or they have focused on scalingup tlms or improving their pretraining schemes.
as a result, other architectures like recurrent neu-ral networks (rnn) (hochreiter and schmidhu-ber, 1997; cho et al., 2014) and convolutionalneural networks (cnn) (lecun et al., 1999) arefading away.
in this work, we propose a method.
figure 1: overall architecture of bertac underthe setting of classiﬁcation for a two-sentence input(sentencex and sentencey)..for improving tlms by integrating a simple con-ventional cnn to them.
we pretrained this cnnon wikipedia using a generative adversarial net-work (gan) style training scheme (goodfellowet al., 2014), and then combined it with tlms.
oh et al.
(2019) similarly used gan-style train-ing to improve a qa model using a cnn, buttheir training scheme was applicable only to qa-speciﬁc datasets.
on the other hand, similarly totlm, our proposed method for training the cnnis independent of speciﬁc tasks.
we show that thecombination of this cnn with tlms can achievehigher performance than that of the original tlmson publicly available datasets for several distincttasks.
we hope that this gives an insight into howto develop novel strong network architectures andtraining schemes..we call our combination of a tlm and a cnnbertac (bert-style tlm with an adversari-ally pretrained convolutional neural network).
itsarchitecture is illustrated in fig.
1. we do notimpose any particular restriction on the tlm inbertac, so any tlm, albert (lan et al.,.
proceedingsofthe59thannualmeetingoftheassociationforcomputationallinguisticsandthe11thinternationaljointconferenceonnaturallanguageprocessing,pages2103–2115august1–6,2021.©2021associationforcomputationallinguistics2103!!!"!#!!!""!#""!""!""!!!#$%&’(...!!!!"#$"#!"#$%&’()’*+!!!!"#$$%&%’()*+,-.&/(0-")*+,-.&/(0-1)*+,-.&/(0-2!"##$%&’(%&$"#!"!"#$"#%"!#$!"#$"#%"")!*+,-!"#$"#%"!)+./,-!"#$"#%"")+./,!#!###"01’(#1%2’"3-456#1’6-78(6-’9:’66%;<$-+34(%%5678!""!8;2’(1%;<-18-’;1%13=9#$>’6-$’;1’;?’$!!!$"!#$"!!
!%"!#%"discriminator d and two cnn-based generators rand f .
once the training is done, we use the gen-erator f as cnn in bertac.
the training dataconsists of pairs of an entity mention and a sen-tence in which the entity mention is masked witha special token [em].
for example, the entity-masked sentence m1 in table 1 is obtained bymasking the entity mention e1, “suvarnabhumiairport,” in the original text s1.
the network fgenerates a vector representation of the maskedsentence (m1), while r produces a representationof the masked entity (e1).
the discriminator dtakes representations generated by either r or fas the input, and it predicts which generator actu-ally gave the representation..in the original gan, a generator learns to gen-erate an artiﬁcial image from random noise so thatthe resulting artiﬁcial image is indistinguishablefrom given real images.
by analogy, we used anentity-masked sentence as “random noise” and amasked entity as a “real image.” in our gan-styletraining, we regard the vector representation of amasked entity given by generator r as a real rep-resentation of the entity (or the representation ofthe “real image” in the above analogy).
on theother hand, we regard the representation of themasked sentence, generated by f , as a fake rep-resentation of the entity (or the representation ofthe “artiﬁcial image” generated from the “randomnoise” in the above analogy).
this representationis deemed fake because the entity is masked in themasked sentence, and f does not know what theentity is exactly.
during the training, f should tryto deceive the discriminator d by mimicking thereal representation and generating a fake represen-tation that is indistinguishable from the real rep-resentation of the entity generated by r. on theother hand, r and d, as a team, try to avoid beingmimicked by f and also to make the mimic prob-lem harder for f .
if everything goes well, oncethe training is over, f should be able to generatea fake representation of the entity that is similar toits real representation..an interesting point is that f ’s output can beinterpreted in two ways: it is a representation of amasked sentence because it is computed from thesentence, and at the same time it is a representationof the masked entity because it is indistinguishablefrom r’s representation of the entity.
this dualitysuggests that f ’s output can be seen as a represen-tation of the entire sentence..figure 2: gan-style pretraining of cnns.
the dis-criminator d takes either a real representation gener-ated by r or a fake representation generated by f asits input and then it predicts whether the input is a realor fake representation..s1.
m1e1.
airporte1 is thailand’s main.
suvarnabhumi:::::::::::::::::international air hub.
[em] is thailand’s main international air hub.
suvarnabhumi airport.
table 1: example of an entity-masked sentence (m1)and the original sentence (s1).
2020) or roberta (liu et al., 2019) for example,can be used as a subcomponent of bertac..we used the cnn to compute representationsof a slightly modiﬁed version of the input givento a tlm.
to integrate these representations withthose of the tlm, we stacked on top of the tlmlayers of transformers for integratingseveralexternal representation (tiers), which are ourmodiﬁed version of normal transformers (vaswaniet al., 2017).
a tier has the same architec-ture as that of a normal transformer encoder ex-cept for its attention: we replace the transformer’sself-attention with an attention based on the rep-resentation provided by the cnn.
we expect that,by keeping the basic architecture of transformerencoders, the cnn’s representations can be inte-grated more effectively with the tlm’s originalrepresentations..we pretrained the cnn using a gan-styletraining scheme in order to generate represen-tations of sentences rather freely withouttheconstraint of token embedding prediction in themasked language modeling used for tlms, as weexplain later.
for the training, we used maskedsentences autogenerated from wikipedia.
as inthe masked language modeling, neither human in-tervention nor downstream task-speciﬁc hackingis required.
as illustrated in fig.
2, the gan-style training requires three networks, namely, a.
2104!"#$%"&"’()*%!!"+’)"),-&(#.+/0#+’)+’$+0#10+’)"),2%+(340*%025(.+46+(3-+’)"),-%+7%+#+’)()"*’08+’+%()*%0"9(.+-+’)"),-%+7%+#+’)()"*’08+’+%()*%0#"!$"#$%+(30%+7%+#+’)()"*’0*50):+0+’)"),0!#!%"#$5(.+0%+7%+#+’)()"*’0*50):+0+’)"),0!!$%&’()’*+%!,-.,(/0(1"!
[em],2-3+’,4’)562-!’,)-,)1#()’1,0)’4-’,(-+%*7"we exploit f as a cnn in bertac as follows:ﬁrst, we use f to compute a representation of amasked version of the sentence originally given asinput to a tlm.
the entity mention to be maskedis chosen by simple rules and, if the input consistsof multiple sentences, we generate a representa-tion of each (masked) input sentence and concate-nate these together into a single one.
then, thisrepresentation is integrated to the output of thetlm through multiple tier layers..our gan-style pretraining is conceptually sim-ilar to tlm pretraining with masked languagemodeling (predicting what a masked word in asentence should be).
however, it was designed topretrain a model that is able to rather freely gen-erate entity representations without strongly stick-ing to the prediction of token embeddings.
our hy-pothesis is that such freely generated representa-tions may be useful for improving the performanceof downstream tasks.
moreover, we assumed thatusing multiple text representations computed fromdifferent perspectives (i.e., predicting token em-beddings and freely generating entity representa-tions) would help to improve the performance ofdownstream tasks..in our experiments, we show that for the gluetasks (wang et al., 2018), bertac’s averageperformance on the development set was 0.7%higher than that of albert, which was used asa subcomponent of bertac, leading to a perfor-mance on the test set comparable to that of sota(90.3% vs 90.8% (sota)).
it also outperformedthe sota method of open-domain qa (chenet al., 2017) on quasar-t (dhingra et al., 2017)and searchqa (dunn et al., 2017) using eitheralbert or roberta.
we also compared ourmethod with alternative models using a cnn pre-trained in a self-supervised (non gan-style) man-ner to directly predict embeddings of the entitymentions.
consequently, we conﬁrmed that ourmethod worked better: only the cnn trained byour gan-style pretraining gave signiﬁcant perfor-mance improvement over base tlms..note that.
the computational overhead ofit took 20 hoursbertac is reasonably small.
with 16 gpus to pretrain a single cnn model and180 hours for the nine models tested with differ-ent parameter settings in this work (cf., 480 hourswith 96 gpus for pretraining deberta (he et al.,2021), for example).
moreover, once pretrained,the cnn models can be re-used for various down-.
stream tasks and combined with various tlms,including potentially future ones.
as for the pa-rameter number, bertac had just a 14% increasein parameters when albert-xxlarge was used asits base tlm (268 m parameters for bertacvs. 235 m for albert-xxlarge).
we conﬁrmedfrom these results that bertac could improvepretrained tlms with reasonably small computa-tional overhead..the code and models of bertac are available.
at https://github.com/nict-wisdom/bertac..2 related work.
pretraining tlms with entity information:there have been attempts to explicitly learn entityrepresentation from text corpora using tlms (heet al., 2020; peters et al., 2019; sun et al., 2020;wang et al., 2020a; xiong et al., 2020; zhanget al., 2019).
our proposed method is a comple-mentary alternative to these existing methods inthe sense that entity representations are integratedinto tlms via cnns and not directly produced bythe tlms.
fine-tuning tlms with external resources orother nns: yang et al.
(2019a) and liu et al.
(2020) have used knowledge graphs for augment-ing tlms with entity representations during ﬁne-tuning.
unlike these approaches, bertac usesunstructured texts rather than clean structuredknowledge, such as knowledge graphs, to adver-sarially train a cnn.
other previous works haveproposed combining cnns or rnns with bertfor nlp tasks (lu et al., 2020; safaya et al., 2020;shao et al., 2019; zhang et al., 2020), but their useof cnns/rnns was task-speciﬁc, so their modelswere not directly applicable to other tasks.
adversarial learning for improving tlms: ohet al.
(2019) proposed a cnn-based answer rep-resentation generator for qa that can guess thevector representation of answers from given why-type questions and answer passages.
the gen-erator was trained in a gan-style manner usingqa datasets.
we took inspiration from their ad-versarial training scheme to train task-independentrepresentation generators from unsupervised texts(i.e., wikipedia sentences in which an entity wasmasked in a cloze-test style)..electra (clark et al., 2020) also employedan adversarial technique (not a gan) to pretraintwo tlms: a generator was trained to performmasked language modeling and a discriminator.
2105was trained to distinguish tokens in the trainingdata from tokens replaced by the generator.
ondownstream tasks, only the discriminator was ﬁne-tuned.
in bertac, the gan-style pretrainingwas applied only to the cnn, thus reducing thetraining cost.
furthermore, the cnn can be com-bined easily with any available tlm, even poten-tially future ones, without having to re-do the pre-training.
in this work, we show that bertac out-performed electra on the glue task..vernikos et al.
(2020) proposed a method thatused an adversarial objective and an adversarialclassiﬁer for regularizing the ﬁne-tuning processof tlms, inspired by adversarial learning for do-main adaptation (ganin et al., 2016).
our workuses a gan-style training scheme only for pre-training cnns, not for ﬁne-tuning tlms..3 pretraining of cnns.
this section describes the training data and train-ing algorithm for our cnn..3.1 training data.
we pretrained our cnn with an entity-maskedversion of wikipedia sentences.
wikiextractor1was used to extract, from the english wikipedia2,sentences that have at least one entity mention,i.e., an entity with an internal wikipedia link.
then we randomly selected one entity mention eiin each sentence and generated an entity-maskedsentence mi by replacing the entire selected men-tion with [em].
for example, we generated themasked sentence m1, “[em] is thailand’s maininternational air hub,” (in table 1) by replacingthe entity mention e1, suvarnabhumi airport, inthe sentence s1, “:::::::::::::airport is thai-land’s main international air hub,” with [em].
weobtained about 43.3 million pairs of an entity men-tion and a masked sentence (f(ei, mi)g) in thisway and used 10% of them (randomly sampled)as the pretraining data for our cnn..suvarnabhumi :::::::.
3.2 gan-style pretraining.
the adversarial.
as illustrated in fig.
2,train-ing is done using three subnetworks: r (real-entity-representation generator), f (fake-entity-representation generator), and d (discriminator).
r and f are cnns with average pooling and d.1https://github.com/samuelbroscheit/wikiextractor-.
wikimentions.
2we used the september 2020 version..is a feedforward neural network.
once the train-ing is done, we use the generator f as cnn inbertac.
in the training, we regard the represen-tation of a masked entity output by generator ras a real representation of the entity that the fake-entity-representation generator f should mimic.
f is trained so that, taking an entity-masked sen-tence as its input, it can generate a representationof the masked entity mention (called a fake repre-sentation of the entity in this work) that d cannotdistinguish from the real representation.
the rep-resentation generated by f is fake in the sense thatthe entity mention is masked in the input sentenceand f cannot know what it is exactly..as mentioned in the introduction, our gan-style pretraining was designed to train a model ca-pable of freely generating entity representations.
we assumed that using multiple text representa-tions computed from different perspectives (i.e.,prediction of token embeddings in tlms andgeneration of entity representations in our cnn)would help to improve the performance of down-stream tasks..algorithm 1: adversarial training schemeinput: training examples f(e,m)g, training epochs t,mini-batch steps b, mini-batch size n.output: real representation generator r, fake.
representation generator f , discriminator d.1 j   12 initialize (cid:18)r, (cid:18)f , and (cid:18)d (parameters of r, f , and d).
sample mini-batch of n examples f(ei,mi)gngenerate word embeddings f(ei,mi)gnexamples..i=1i=1 of the.
update d and r by ascending their stochastic.
[log d(r(ei)) + log.)
(1 (cid:0) d(f (mi))].
update f by descending its stochastic gradient:.
(1 (cid:0) d(f (mi)).
).
log.
with random weights.
k   1.
3 while j (cid:20) t do45 while k (cid:20) b do6.
7.
8.
9.
10.
11.gradient:1n.∇(cid:18)d ;(cid:18)r.n∑.
i=1.
n∑.
∇(cid:18)f.1ni=1k   k + 1endj   j + 1.
1213 end.
for each pair of an entity mention (ei) and anentity-masked sentence (mi) in the training data,we ﬁrst generate two matrices of word embed-dings ei and mi using word embeddings pretrainedon wikipedia with fasttext (bojanowski et al.,2017).
then, r and f generate, respectively, a.
2106real entity representation from ei and a fake entityrepresentation from mi.
finally, they are givento d, which is a feed-forward network that judgeswhether f or r generated the representations, i.e.,whether the representations are real or fake, us-ing sigmoid outputs by the ﬁnal logistic regressionlayer..the pseudo code of the training scheme is givenin algorithm 1. the training proceeds as fol-lows: r and d as a team try to avoid the possi-bility that d misjudges f’s output (i.e., a fake en-tity representation) as a real entity representation.
more precisely, r and d are trained so that dcan correctly judge the representation r(ei) givenby generator r as real (i.e., d(r(ei)) = 1) andthe representation f (mi) given by generator f asfake (i.e., d(f (mi)) = 0).
therefore, the train-ing is carried out with the objective of maximizinglog d(r(ei)) + log(line 8 in al-gorithm 1).
on the other hand, f tries to generaterepresentation f (mi) so that d judges it as real(i.e., d(f (mi)) = 1).
thus, f is trained to mini-)1 (cid:0) d(f (mi))mize log(line 9 in algorithm 1).
this minmax game is iterated for the pre-speciﬁedt training epochs..)1 (cid:0) d(f (mi)).
(.
(.
3.3 pretraining settings.
we extracted 43.3 million pairs of an entity men-tion and a masked sentence from wikipedia andrandomly sampled 10% of them to use as train-ing data (4.33 million pairs, around 700 mb inﬁle size).
we used word-embedding vectors in300 dimensions (for 2.5 million words) pretrainedon wikipedia using fasttext (bojanowski et al.,2017).
the embedding vectors were ﬁxed duringthe training..we set the training epochs to 200 (t = 200 inalgorithm 1) and did not use any early-stoppingtechnique.
we chose t = 200 from the resultsof our preliminary experiments in which we used10% of the training data and set training epochs tto either of 100, 200, or 300; the loss robustly con-verged for t = 200 and t = 300, and thus the ear-liest point t = 200 was chosen.
we used the rm-sprop optimizer (tieleman and hinton, 2012) witha batch size of 4,000 (n = 4; 000 and b = 1; 084in algorithm 1) and a learning rate of 2e-4.
wetrained nine cnn models with all combinationsof the ﬁlter’s window sizes 2 f“1,2,3”, “2,3,4”,“1,2,3,4”g and number of ﬁlters 2 f100, 200, 300gfor the generators f and r. all of the weights in.
the cnns were initialized using he’s method (heet al., 2015).
we used a logistic regression layerwith sigmoid outputs as discriminator d. thetraining of a single cnn model took around 20hours using 16 nvidia v100 gpus with 32 gb ofmemory (180 hours in total for the nine models).
we tested all nine cnn models for bertacin our glue and open-domain qa experiments(section 5).
for each task, the parameters in-side the cnns (as well as the word-embeddingvectors) were ﬁxed during the ﬁne-tuning ofbertac..4 bertac.
as illustrated in fig.
1, bertac (bert-styletlm with an adversarially pretrained convolu-tional neural network) incorporates the representa-tion provided by the adversarially pretrained cnnto the representation generated by a tlm.
for theintegration, we use several layers of tiers (trans-formers for integrating external representation)stacked on top of the tlm..4.1 cnn in bertac.
for simplicity, we describe how the cnn is inte-grated in bertac using the task of recognizingtextual entailment (rte) as an example.
bertacfor the rte task takes two sentences sx and syas input and predicts whether sx entails sy.
first,we explain how the adversarially pretrained cnn(generator f in section 3.2) generates the repre-sentation of the two input sentences.
we regardthe longest common noun phrase3 of the two sen-tences as the entity mention to be masked and cre-ate entity-masked sentences mx and my from sxand sy by masking the noun phrase with [em](we use mx = sx and my = sy if no commonnoun phrase is found).
then each of the maskedsentences mx and my is given to the cnn.
ourexpectation here is that the cnn generates similarrepresentations from the masked sentences if theyhave an entailment relation and that this helps torecognize the entailment relation..note that the cnn in bertac is connected toseveral tier layers and that, as shown in fig.
1,its input is iteratively updated so that it providesupdated representations to the tier layers.
let2 rjmyj(cid:2)dw be the ma-2 rjmxj(cid:2)dw and mimixytrices of word embeddings of mx and my given3for single-sentence tasks such as cola (wang et al.,2018), we regard the longest noun phrase in a sentence asan entity..2107to the cnn connected to the i-th tier layer,where dw is the dimension of a word embed-ding.
we denote the representation generated bythe cnn when the matrix of word embeddingsm was used as the input by cn n (m).
the i-th tier layer is given the concatenation of thetwo cnn representations of mx and my, ri =x) 2 rde,[rix; rix = cn n (miy) 2 rde and de is the dimensiony = cn n (miriof the cnn representation.
note that, for single-sentence tasks, ri = rix, the cnn representationof mx, is given to the tier layers..y] 2 r2(cid:2)de, where ri.
the initial matrices of word embeddings m1xand m1y are obtained using the fasttext word em-beddings (bojanowski et al., 2017), the same asthat used in our adversarial learning.
then, the up-dated input matrices mi+1for the (i+1)-th cnn are obtained from the i-th input matricesmix and miy as described below.
for the word em-bedding mix;j of the j-th word in mx, we computeits bilinear score to ri.
and mi+1.
x.y.x (sutskever et al., 2009):xri.
(cid:22)mi.
x;j;.
x)mi.
x;j = softmaxj(mit.
x bi2 rdw(cid:2)de is a trainable matrix andwhere bixsoftmaxj(v) denotes the j-th element of the soft-maxed vector of v. the bilinear score indicateshow much the corresponding token should behighlighted as one associated with the cnn repre-sentation rix during the update process.
we expectthat this allows the cnn in the next tier layerto generate further reﬁned representations with theupdated embeddings..x.
⊙ (1 (cid:0) tx(mih, tx(mi.
we then compute word embeddings mi+1in ahighway network manner (srivastava et al., 2015)as follows:x) ⊙ tx(mix) + mimi+1x = hx( (cid:22)mix));xx + bix) = wihmiwhere hx(mix) =t), (cid:27) is the sigmoid function, ⊙ rep-x + bi(cid:27)(witmih, wiresents the element-wise product, and wit,bih, and bit are layer-speciﬁc trainable parameters.
mi+1y in theysame way.
during the ﬁne-tuning of bertacfor downstream tasks, we ﬁx the parameters of thepretrained cnn but train these parameters for up-dating cnn’s input alongside those of tlms andtiers..is also computed from mi.
y and ri.
4.2 transformers for integrating external.
representation (tiers).
as explained in the introduction, the main differ-ence between a tier and a normal transformer.
figure 3: attention in normal transformers and tiers.
encoder (vaswani et al., 2017) lies in the attentionmechanism.
in the tier attention mechanism, thequery representation, which is one of the three in-puts of the transformer’s self-attention, is replacedwith the representation given by the cnn..fig.
3 shows the difference between the tiers’attention computation and that of normal trans-formers.
attention in normal transformers is com-puted in the following way:.
attention(q; k; v) = softmax(.
)v:.
qktpdk.
q, k, and v are query, key, and value matrices inrlk(cid:2)dk , where lk is the length of an input sequenceand dk is a dimension of keys.
q, k, and v allcome from the same representation of the tokensequence provided from the previous transformerlayer.
the attention should specify how much thecorresponding tokens in v should be highlighted,so we designed ours in the same way..in tiers, we use the following attention.
webasically replace the matrix q with the cnn’s rep-resentation r 2 ru(cid:2)dk while keeping the originalk and v, where u is the number of sentences inthe input of the model (u 2 f1; 2g in this paper)..attention(r; k; v) = (softmax(.
rktpdk.
))tju;dk.
⊙v:.
since r is a matrix with a different size fromq, we needed to adapt the attention computa-tion.
we ﬁrst multiply r to kt, and then its soft-maxed results are converted into a lk (cid:2) dk dimen-2sional matrix using the all-one matrix ju;dkru(cid:2)dk .
the resulting matrix be a =let(softmax( rktp2 rlk(cid:2)dk .
we apply the at-dktention score to v by using the element-wise prod-uct between matrices: a ⊙ v..))tju;dk.
2108!"#$#%&’()*#+,-./01&2.3&$4564*7214"#rkvmatmul64&60"j2.3&$4564*7214"#qkv64&60"64&60"!"#$%&&’(&)*($)($(*+,"-$&+"(./*+,’+.!0#$%&&’(&)*($)($*1+$2345.
in addition,.
the actual cnn’s representationrcn n 2 ru(cid:2)de given by our cnns usually havea size that does not match the size requirementfor r. thus, we convert it to r 2 ru(cid:2)dk , a dk-column matrix as follows: r = rcn n w + b,where w 2 rde(cid:2)dk and b are trainable..5 experiments.
we tested our model on glue and on open-domain qa.
in this section, we report the results..5.1 glue.
glue (wang et al., 2018) is a multi-task bench-mark composed of nine tasks including two single-sentence tasks (cola and sst-2) and seventwo-sentence tasks of similarity/paraphrase tasks(mrpc, qqp, and sts-b) and natural languageinference tasks (mnli, qnli, rte, and wnli).
following the previous work of albert (lanet al., 2020), we performed single-task ﬁne-tuningfor each task under the following settings: single-model for the development set and ensemble fortest set submissions.
as in liu et al.
(2019) andlan et al.
(2020), we report the performance onthe development set for each task by averagingover ﬁve runs with different random initializationseeds.
as in lan et al.
(2020), for test set sub-missions, we ﬁne-tuned the models for the rte,sts-b, and mrpc tasks by initializing them withthe ﬁne-tuned mnli single-task model, and wealso used task-speciﬁc modiﬁcation for cola andwnli to improve scores (see appendix a for de-tails).
we explored ensemble settings between 6and 30 models per task for our test set submission..5.1.1 fine-tuning details of bertac for.
glue.
we used albert-xxlarge-v2 (lan et al., 2020)as the pretrained tlm.
as hyperparameters forbertac, for each task we tested learning rates2 f8e-6, 9e-6, 1e-5, 2e-5, 3e-5g, a linear warmupfor the ﬁrst 6% of steps followed by a linear de-cay to 0, a maximum sequence length of 128, andall nine cnns pretrained with different ﬁlter set-tings.
we set the batch size to 128 for mnliand qqp and 16 for the other tasks.
further-more, we trained our model with the following setof training epochs: f1,2,3,4,5g for mnli, qqp,and qnli, f6,7,8,9,10g for cola, mrpc, rte,sst-2, and sts-b, and f90,95,100,105,110g forwnli.
we set the number of tier layers to 3 af-ter preliminary experiments.
see table 9 in ap-.
pendix b for a summary of the hyperparameterstested in the glue experiments..during the ﬁne-tuning of bertac, the parame-ters inside the cnns (as well as word embeddingsof fasttext) were ﬁxed as explained in section 3.3,while those used to update the input to the cnnswere optimized.
for each task, we selected thepretrained cnn (out of nine) and the bertac hy-perparameters that gave the best performance onthe development data..5.1.2 resultstable 2 shows the results of eight tasks on theglue development set: all of them are single-model results.
our bertac consistently out-performed the previous tlm-based models overseven tasks, except for qqp, and, as a result,showed the best average performance on the de-velopment set.
crucially, our model improvedthe average performance around 0.7% over al-bert, the base tlm in our model.
this indicatesthe effectiveness of adversarially trained cnnsand tiers in bertac.
the test set results ob-tained from the glue leaderboard are summa-rized in table 3. our model showed comparableperformance to sota, deberta/turingnlrv4,and achieved state-of-the-art results on 3 out of 9task.
it also showed better performance than al-bert, our base tlm, in most tasks..to investigate whether our gan-style pretrain-ing of cnns contributed to the performance im-provement, we also tested the following alter-native training schemes for the cnn used inbertac..self-supervised cnn: we pretrained the cnnto generate representations of a masked sentencein a self-supervised way as follows: for an entitymention e and an entity-masked sentence m in thetraining data (section 3.1), the cnn generates arepresentation r from the masked sentence tryingto minimize mse (mean squared error) between rand the entity mention’s representation e (averageword embedding of all tokens in e)..randomly initialized cnn: we did not pre-trained the cnns, but trained them alongside thetlms during the ﬁne-tuning of bertac (thecnns were randomly initialized)..we trained both the self-supervised and ran-domly initialized cnns using the same hyperpa-rameter settings as gan-style cnns (see sec-tion 3.3).
we conﬁrm from the results in table 4.
2109modelsrobertalargexlnetlargeelectralargealbertxxlargedebertalargebertacxxlarge.
mnli90.2/90.290.8/90.890.9/-90.8/-91.1/91.191.3/91.1.
qnli qqp rte86.692.294.785.992.394.992.488.095.089.292.295.388.392.395.389.995.792.3.sst-2 mrpc cola sts-b avg.
88.968.096.489.269.097.089.569.196.990.071.496.990.070.596.890.773.797.2.
90.990.890.890.991.992.4.
92.492.592.693.092.893.1.table 2: glue dev set results.
the results of roberta (liu et al., 2019), xlnet (yang et al., 2019b), elec-tra (clark et al., 2020), albert (lan et al., 2020), and deberta (he et al., 2021) were taken from their papers.
we omit the results of the wnli task, since many previous works did not report the dev set results..qnli qqp rte sst-2 mrpc cola sts-b wnli.
score.
mnli.
modelsensembles on test (from leaderboard as of feb. 1, 2021)albertelectra+standard trickserniestructbert+taptmacalbert+dkmdeberta/turingnlrv4bertac.
91.3/91.091.3/90.891.4/91.090.9/90.791.3/91.191.9/91.691.1/91.6.
-95.896.697.497.899.297.9.
90.590.890.991.090.690.890.6.
89.289.890.991.292.093.290.4.
97.197.197.597.397.097.597.5.
91.290.791.491.992.692.091.7.
69.171.774.475.374.871.572.3.
92.092.592.692.792.692.692.8.
91.891.894.594.594.594.594.5.
-89.490.490.690.790.890.3.table 3: glue test set results.
our model for test set results incorporates task-speciﬁc modiﬁcation for cola andwnli to improve scores (see appendix a for details).
all results are from the glue leaderboard..that only the proposed method with our gan-style cnns showed a higher average score thanalbert.
this suggests the effectiveness of ourgan-style pretraining scheme of cnns..5.2 open-domain qa.
also tested bertac on open-domainweqa (chen et al., 2017) with the publicly availabledatasets quasar-t (dhingra et al., 2017) andsearchqa (dunn et al., 2017).
we used thepre-processed version4 of the datasets providedby lin et al.
(2018), which contains passagesretrieved for all questions, and followed their datasplit as described in table 5..5.2.1 bertac for open-domain qa.
we implemented our qa model following the ap-proach of lin et al.
(2018), which combines a pas-sage selector to choose relevant passages from re-trieved passages and an answer span selector toidentify the answer span in the selected passages.
for the given question q and the set of retrievedpassages p = fpig, we computed the probabilityp r(ajq; p ) of extracting answer span a to ques-tion q from p in the following way, and then weextracted the answer span ^a with the highest prob-ability:.
∑.
i.p r(ajq; p ) =.
p r(ajq; pi)p r(pijq; p );.
4available at https://github.com/thunlp/openqa.
where p r(pijq; p ) and p r(ajq; pi) are computedby the passage selector and answer span selector,respectively..we input “[cls] question [sep] passage[sep]” to both the passage selector and answerspan selector, where [cls] and [sep] are spe-cial tokens.
in the passage selector, the represen-tation of [cls] in the top tier layer is fed intoa linear layer with a softmax, which computes theprobability that the passage contains a correct an-swer to the question.
our bertac answer spanselector identiﬁes answer spans from passages bycomputing start and end probabilities of each to-ken in passages, where we feed the representationof each token in the top layer of tiers to two lin-ear layers, each with a softmax for the probabili-ties (devlin et al., 2019)..5.2.2 training details for open-domain qawe used all nine pretrained cnns, as in theglue experiments.
as pretrained tlms, weused albert-xxlarge-v2 (lan et al., 2020) androberta-large (liu et al., 2019).
we set thelearning rate to 1e-5, the number of epochs to2, the maximum sequence length to 384, and thenumber of tier layers to 3. we used a linearwarmup for the ﬁrst 6% of steps followed by a lin-ear decay to 0 with a batch size of 48 for quasar-t and 96 for searchqa.
we tested all of the pre-trained cnns and chose for each dataset the onethat maximizes em (the percentage of the predic-tions matching exactly one of the ground truth an-.
2110cnns used in bertacproposed (gan-style cnn)self-supervised cnnrandomly initialized cnnalbertxxlarge.
mnli91.3/91.191.0/90.891.0/90.790.8/-.
qnli qqp rte89.992.395.788.491.595.387.491.495.489.292.295.3.sst-2 mrpc cola sts-b avg.
90.773.797.289.871.196.689.871.596.390.071.496.9.
93.193.093.193.0.
92.490.991.290.9.table 4: comparison of bertac results in different cnn settings on glue dev set..trainquasar-t37,012searchqa 99,811.dev3,00013,893.test3,00027,247.
#p10050.table 5: number of questions in each dataset.
#p is thenumber of retrieved passages for each question..non-tlm-based methodsopenqa (lin et al., 2018): an rnn-based methodthat jointly learns passage-selection and answer extrac-tion.
openqa+arg (oh et al., 2019): an extension ofopenqa that additionally uses an answer representationgenerator (arg) trained by adversarial learning.
tlm-based methodswklm (xiong et al., 2020): this uses a tlm pre-trained with a weakly supervised objective for learningwikipedia entity information.
bert-base was used forthe training.
mbert (wang et al., 2019): a bert-based methodthat extracts answers using globally normalized answerscores across all the passages retrieved by the same ques-tion.
bert-large was used for the training.
cformer (wang et al., 2020b): it uses a clustering-based sparse transformer for long-range dependency en-coding.
the method was trained using roberta-large..table 6: compared qa methods.
swers) on the development set.
see table 10 inappendix b for a summary of the hyperparame-ters tested for open-domain qa..5.2.3 results.
we compared bertac with the previous worksdescribed in table 6. table 7 shows the per-formance of all of the methods.
the sub-scripts of the tlm-based methods represent thetype of pretrained tlm used by each method.
all the methods were evaluated using em andf1 score (average overlap between the predic-tion and gold answer).
bertacalbert-xxlargeoutperformed all of the baselines including thesota method (cformer) on both em and f1.
bertacroberta-large in the same tlm settingas the sota method showed a better performancethan sota except for f1 in quasar-t. these re-sults suggest that our framework is effective forqa tasks as well..for ablation studies, we evaluated some vari-ants of bertacalbert-xxlarge: “w/o cnn and.
model.
openqaopenqa+argwklmbert-basembertbert-largecformerroberta-largebertacroberta-largebertacalbert-xxlarge.
quasar-tf1em49.342.249.743.252.245.859.151.163.954.063.755.865.858.0.searchqaemf164.558.865.359.666.761.770.765.175.168.077.171.979.274.0.table 7: qa test set results.
figures of the previousworks were taken from their original papers..model.
bertacalbert-xxlargew/o cnn and tierw/o gan-style cnnw/o update.
quasar-tf1em65.858.063.555.663.956.165.056.8.searchqaemf179.274.078.072.778.473.178.573.3.table 8: ablation test results..tier,” which uses albert-xxlarge alone with-out using our cnn and tier, “w/o gan-stylecnn,” which does not use our cnn pretrainedby the gan-style training scheme but uses self-supervised cnns (the same as used in the glueexperiments, see table 4), “w/o update,” whichdoes not perform layer-wise update of the cnninputs.
the results in table 8 suggest that all ofthe following contributed to the performance im-provement: the combination of tlms and gan-style cnns, our gan-style training of cnns, andthe layer-wise update of the cnn inputs..6 conclusion.
we proposed bertac (bert-style tlm with anadversarially pretrained convolutional neural net-work), a combination of a tlm and a cnn, wherethe cnn was pretrained using a novel gan-styletraining scheme and masked sentences obtainedautomatically from wikipedia.
using this cnn,we improved the performance of standard tlms.
we conﬁrmed that bertac could achieve com-parable performance with the sota and outper-formed the base tlm used as a subcomponentof bertac in the glue task.
we also showthat bertac outperformed the sota method ofopen-domain qa on quasar-t and searchqa..2111references.
piotr bojanowski, edouard grave, armand joulin, andtomas mikolov.
2017. enriching word vectors withsubword information.
transactions of the associa-tion for computational linguistics, 5:135–146..siddhartha brahma.
2018. unsupervised learningof sentence representations using sequence consis-tency.
corr, abs/1808.04217..tom b. brown, benjamin mann, nick ryder, melaniesubbiah, jared kaplan, prafulla dhariwal, arvindneelakantan, pranav shyam, girish sastry, amandaaskell, sandhini agarwal, ariel herbert-voss,gretchen krueger, tom henighan, rewon child,aditya ramesh, daniel m. ziegler, jeffrey wu,clemens winter, christopher hesse, mark chen,eric sigler, mateusz litwin, scott gray, benjaminchess, jack clark, christopher berner, sam mc-candlish, alec radford, ilya sutskever, and darioamodei.
2020.language models are few-shotin advances in neural information pro-learners.
cessing systems 33: annual conference on neu-ral information processing systems 2020, neurips2020, december 6-12, 2020, virtual..danqi chen, adam fisch, jason weston, and antoinebordes.
2017. reading wikipedia to answer open–in proceedings of the 55th an-domain questions.
nual meeting of the association for computationallinguistics (volume 1: long papers), pages 1870–1879. association for computational linguistics..kyunghyun cho, bart van merri¨enboer, caglar gul-cehre, dzmitry bahdanau, fethi bougares, holgerschwenk, and yoshua bengio.
2014.learningphrase representations using rnn encoder–decoderfor statistical machine translation.
in proceedings ofthe 2014 conference on empirical methods in nat-ural language processing (emnlp), pages 1724–1734. association for computational linguistics..kevin clark, minh-thang luong, quoc v. le, andchristopher d. manning.
2020. electra: pre–training text encoders as discriminators rather thangenerators.
in international conference on learn-ing representations..jacob devlin, ming-wei chang, kenton lee, andkristina toutanova.
2019. bert: pre-training ofdeep bidirectional transformers for language under-standing.
in proceedings of the 2019 conference ofthe north american chapter of the association forcomputational linguistics: human language tech-nologies, volume 1 (long and short papers), pages4171–4186..bhuwan dhingra, kathryn mazaitis, and william wcohen.
2017. quasar: datasets for question an-arxiv preprintswering by search and reading.
arxiv:1707.03904..matthew dunn, levent sagun, mike higgins, v. ugurand kyunghyun cho..g¨uney, volkan cirik,.
2017.searchqa: a new q&a dataset aug-mented with context from a search engine.
corr,abs/1704.05179..yaroslav ganin, evgeniya ustinova, hana ajakan,pascal germain, hugo larochelle, franc¸ois lavi-olette, mario march, and victor lempitsky.
2016.domain-adversarialtraining of neural networks.
journal of machine learning research, 17(59):1–35..ian goodfellow, jean pouget-abadie, mehdi mirza,bing xu, david warde-farley, sherjil ozair, aaroncourville, and yoshua bengio.
2014. generative ad-in advances in neural informationversarial nets.
processing systems, volume 27, pages 2672–2680.
curran associates, inc..bin he, di zhou, jinghui xiao, xin jiang, qunliu, nicholas jing yuan, and tong xu.
2020.bert-mk: integrating graph contextualized knowl-edge into pre-trained language models.
in findingsof the association for computational linguistics:emnlp 2020, pages 2281–2290..kaiming he, xiangyu zhang, shaoqing ren, and jiansun.
2015. delving deep into rectiﬁers: surpassinghuman-level performance on imagenet classiﬁca-tion.
in proceedings of the 2015 ieee internationalconference on computer vision (iccv), iccv ’15,pages 1026–1034, washington, dc, usa.
ieeecomputer society..pengcheng he, xiaodong liu, jianfeng gao, anddeberta: decoding-weizhu chen.
2021.enhanced bert with disentangled attention.
ininternational conference on learning representa-tions..sepp hochreiter and j¨urgen schmidhuber.
1997.neural computation,.
long short-term memory.
9(8):1735–1780..zhenzhong lan, mingda chen, sebastian goodman,kevin gimpel, piyush sharma, and radu soricut.
2020. albert: a lite bert for self-supervisedin 8th inter-learning of language representations.
national conference on learning representations,iclr 2020, addis ababa, ethiopia, april 26-30,2020..yann lecun, patrick haffner, l´eon bottou, andyoshua bengio.
1999. object recognition within shape, contour andgradient-based learning.
grouping in computer vision, pages 319–345.
springer..yankai lin, haozhe ji, zhiyuan liu, and maosong sun.
2018. denoising distantly supervised open-domainquestion answering.
in proceedings of the 56th an-nual meeting of the association for computationallinguistics, acl 2018, pages 1736–1745..weijie liu, peng zhou, zhe zhao, zhiruo wang, qi ju,haotang deng, and ping wang.
2020. k-bert:enabling language representation with knowledgegraph.
in proceedings of aaai 2020..2112yinhan liu, myle ott, naman goyal, jingfei du, man-dar joshi, danqi chen, omer levy, mike lewis,luke zettlemoyer, and veselin stoyanov.
2019.roberta: a robustly optimized bert pretraining ap-proach.
arxiv preprint arxiv:1907.11692..ilya sutskever, joshua b. tenenbaum, and ruslan rsalakhutdinov.
2009. modelling relational data us-in ad-ing bayesian clustered tensor factorization.
vances in neural information processing systems22, pages 1821–1828.
curran associates, inc..zhibin lu, pan du, and jian-yun nie.
2020. vgcn-bert: augmenting bert with graph embedding fortext classiﬁcation.
in advances in information re-trieval - 42nd european conference on ir research,ecir 2020, lisbon, portugal, april 14-17, 2020,proceedings, part i, volume 12035 of lecture notesin computer science, pages 369–382.
springer..jong-hoon oh, kazuma kadowaki, julien kloetzer,ryu iida, and kentaro torisawa.
2019. open–domain why-question answering with adversariallearning to encode answer texts.
in proceedings ofthe 57th annual meeting of the association for com-putational linguistics, pages 4227–4237..matthew e. peters, mark neumann, robert logan,roy schwartz, vidur joshi, sameer singh, andnoah a. smith.
2019. knowledge enhanced con-textual word representations.
in proceedings of the2019 conference on empirical methods in natu-ral language processing and the 9th internationaljoint conference on natural language processing(emnlp-ijcnlp), pages 43–54..colin raffel, noam shazeer, adam roberts, katherinelee, sharan narang, michael matena, yanqi zhou,wei li, and peter j. liu.
2020. exploring the limitsof transfer learning with a uniﬁed text-to-text trans-journal of machine learning research,former.
21(140):1–67..ali safaya, moutasem abdullatif, and deniz yuret.
2020. kuisail at semeval-2020 task 12: bert-cnn for offensive speech identiﬁcation in socialmedia.
in proceedings of the fourteenth workshopon semantic evaluation, pages 2054–2059..bo shao, yeyun gong, weizhen qi, nan duan, andxiaola lin.
2019. aggregating bidirectional en-coder representations using matchlstm for se-quence matching.
in proceedings of the 2019 con-ference on empirical methods in natural languageprocessing and the 9th international joint confer-ence on natural language processing (emnlp-ijcnlp), pages 6059–6063..rupesh k srivastava, klaus greff, and j¨urgen schmid-huber.
2015. training very deep networks.
in ad-vances in neural information processing systems,volume 28, pages 2377–2385..t. tieleman and g. hinton.
2012..lecture 6.5—rmsprop: divide the gradient by a running averageof its recent magnitude.
coursera: neural net-works for machine learning..ashish vaswani, noam shazeer, niki parmar, jakobuszkoreit, llion jones, aidan n gomez, ł ukaszkaiser, and illia polosukhin.
2017. attention is allyou need.
in i. guyon, u. v. luxburg, s. bengio,h. wallach, r. fergus, s. vishwanathan, and r. gar-nett, editors, advances in neural information pro-cessing systems 30, pages 5998–6008.
curran as-sociates, inc..giorgos vernikos, katerina margatina, alexandrachronopoulou, and ion androutsopoulos.
2020.domain adversarial fine-tuning as an effectiveregularizer.
in findings of the association for com-putational linguistics: emnlp 2020, pages 3103–3112. association for computational linguistics..alex wang, amanpreet singh, julian michael, fe-lix hill, omer levy, and samuel bowman.
2018.glue: a multi-task benchmark and analysis plat-in pro-form for natural language understanding.
ceedings ofthe 2018 emnlp workshop black-boxnlp: analyzing and interpreting neural net-works for nlp, pages 353–355, brussels, belgium.
association for computational linguistics..ruize wang, duyu tang, nan duan, zhongyu wei,xuanjing huang, jianshu ji, guihong cao, daxinjiang, and ming zhou.
2020a.
k-adapter: infusingknowledge into pre-trained models with adapters.
corr, abs/2002.01808..shuohang wang, luowei zhou, zhe gan, yen-chunchen, yuwei fang, siqi sun, yu cheng, andjingjing liu.
2020b.
cluster-former: clustering-based sparse transformer for long-range dependencyencoding.
arxiv preprint arxiv:2009.06097..zhiguo wang, patrick ng, xiaofei ma, ramesh nalla-pati, and bing xiang.
2019. multi-passage bert:a globally normalized bert model for open-do-in proceedings of themain question answering.
2019 conference on empirical methods in natu-ral language processing and the 9th internationaljoint conference on natural language processing(emnlp-ijcnlp), pages 5878–5882.
associationfor computational linguistics..yu sun, shuohuan wang, yukun li, shikun feng, haotian, hua wu, and haifeng wang.
2020. ernie2.0: a continual pre-training framework for lan-in proceedings of the aaaiguage understanding.
conference on artiﬁcial intelligence, pages 8968–8975..wenhan xiong, jingfei du, william yang wang, andveselin stoyanov.
2020. pretrained encyclopedia:weakly supervised knowledge-pretrained languagemodel.
in 8th international conference on learn-ing representations,iclr 2020, addis ababa,ethiopia, april 26-30, 2020..2113an yang, quan wang, jing liu, kai liu, yajuan lyu,hua wu, qiaoqiao she, and sujian li.
2019a.
en-hancing pre-trained language representations withrich knowledge for machine reading comprehension.
in proceedings of the 57th annual meeting of theassociation for computational linguistics, pages2346–2357..zhilin yang, zihang dai, yiming yang, jaime car-bonell, russ r salakhutdinov, and quoc v le.
2019b.
xlnet: generalized autoregressive pretrain-in advances ining for language understanding.
neural information processing systems 32, pages5754–5764..shaohua zhang, haoran huang, jicong liu, and hangli.
2020. spelling error correction with soft-maskedin proceedings of the 58th annual meet-bert.
ing of the association for computational linguis-tics, acl 2020, online, july 5-10, 2020, pages 882–890. association for computational linguistics..zhengyan zhang, xu han, zhiyuan liu, xin jiang,maosong sun, and qun liu.
2019. ernie: en-hanced language representation with informative en-in proceedings of the 57th annual meetingtities.
of the association for computational linguistics,pages 1441–1451..a task-speciﬁc modiﬁcation for glue.
test-set submission.
we applied task-speciﬁc modiﬁcation to wnliand cola in the glue tasks to achieve com-petitive glue leaderboard results, i.e., the testset submission results presented in table 3. forwnli, we followed raffel et al.
(2020), while, forcola, we propose our own modiﬁcation.
notethat we did not apply the tricks in obtaining the re-sults on the development set results shown in ta-ble 2. in the following, we describe the tricks..a.1 wnli.
wnli is a coreference resolution task with a two-sentence input.
the ﬁrst sentence has an ambigu-ous pronoun and the second sentence is generatedfrom the ﬁrst sentence by replacing the pronounwith one of the possible referents (noun phrases)in the ﬁrst sentence (wang et al., 2018).
in thistask, we must predict whether the candidate ref-erent in the second sentence is the correct refer-ent of the pronoun.
since the format of wnliis known for being difﬁcult to learn by a model,many previous works, including those using al-bert, roberta, or t5 (liu et al., 2019; lanet al., 2020; raffel et al., 2020), converted the datato a simpler format before training their wnlimodel for glue test-set submission..following these approaches, we also convertedthe data in the same way as raffel et al.
(2020).
first, we extract candidate referents for an am-biguous pronoun as follows.
suppose that thefollowing sentence pair of s1 and s2 is from thewnli task’s data and has the label correct (mean-ing that susan in s2 is the correct referent of thepronoun she in s1)..s1:.
jane knocked on susan’s door but she.
did not get an answer..s2: susan did not get an answer..we ﬁrst ﬁnd all of the pronouns in the ﬁrst sen-tence (“she” in s1).
for each pronoun, we ﬁndthe longest sequence of words that precedes orfollows the pronoun in the ﬁrst sentence and thatalso appears in the second sentence (“did not getan answer” underlined in s1 and s2).
we thenchoose the pronoun that precedes or follows thelongest matching word sequence and obtain a can-didate referent by deleting the matched sequenceof words from the second sentence.
in the exam-ple sentence pair (s1, s2), we choose the pronounshe from the ﬁrst sentence (since there is a singlepronoun) and obtain the candidate referent susanfrom the second sentence through this process.
fi-nally, we convert the original sentence pair into apair of a masked sentence and a candidate refer-ent by replacing the pronoun in the ﬁrst sentencewith [mask] and replacing the second sentencewith the extracted referent.
the (s1, s2) pair isthus changed to the following (s′.
1, s′.
2):.
s′1: jane knocked on susan’s door but [mask].
did not answer..s′2: susan.
note that [mask] in the sentence is differentfrom the entity mask [em] used in our gan-style training for cnns.
for the input to ourcnns, we further replaced [mask] with [em].
since the format of this converted data is simi-lar to that of the training data for the gan-styletraining scheme of our cnn, we expect that byusing this data conversion, bertac can more ef-fectively predict whether the candidate referent forthe masked pronoun is correct..a.2 cola.
in the cola task, we need to predict whether agiven sentence is grammatically acceptable.
for.
2114mnli qnli qqp rte.
wnli.
learning ratebatch sizetraining epochtier layermax sequence lengthwarmup stepcnn.
128f1,2,3,4,5g.
sst-2 mrpc cola sts-bf8e-6, 9e-6,1e-5, 2e-5, 3e-5g16.f6,7,8,9,10g3128linear warmup for the ﬁrst 6% of steps9 models pretrained with different ﬁlter settings.
f90,95,100, 105,110g.
table 9: hyperparameters of bertac tested for glue experiments..learning ratebatch sizetraining epochtier layermax sequence lengthwarmup stepcnn.
quasar-t.searchqa.
1e-5.
48.
96.
23384linear warmup for the ﬁrst 6% of steps9 models pretrained with different ﬁlter settings.
table 10: hyperparameters of bertac tested for open-domain qa experiments..then used as a starting point for the second-stepﬁne-tuning, using the original cola training datathis time, of our ﬁnal model for cola..b hyperparameters.
hyperparameters of bertac tested for glueand open-domain qa experiments are summa-rized in tables 9 and 10, where cnn representscnn models pretrained with different ﬁlter set-tings (ﬁlter’s window sizes 2 f“1,2,3”, “1,2,3,4”,“2,3,4”g and number of ﬁlters 2 f100, 200, 300g)described in section 3.3. we tested all combina-tions of these hyperparameters and chose the bestone using the development set of each task..this task, we conducted a two-step ﬁne-tuning.
inthe ﬁrst step, we ﬁne-tuned bertac with au-tomatically generated pseudo-training data.
thisdata was prepared as described below, and doesnot include the original cola training data.
inthe second step, we further ﬁned-tuned the modelobtained in the ﬁrst step using the original colatraining data.
the bertac model obtained at thissecond step was used for the test-set submission..to automatically generate pseudo-training data,we regarded all of the sentences in the trainingdata of mnli, qqp, and qnli as grammaticallyacceptable and used them as positive examplesin the pseudo-training data.
after removing du-plicate sentences, for each positive example, wegenerated one negative example by modifying thepositive example under the assumption that themodiﬁcation makes the generated example gram-matically unacceptable.
as a modiﬁcation, werandomly applied one of the following three op-erations: permutation (of four words randomly se-lected), insertion (of two random words to randompositions), and deletion (of two randomly selectedwords) (brahma, 2018)..we obtained about 2.14 million examples in thisway, half of them positives and the other half neg-atives.
we used all of the training samples auto-matically generated in this way for the ﬁrst-stepﬁne-tuning of bertac, with a learning rate of8e-6, a single training epoch, and a batch size of128, while applying the same settings for the otherhyperparameters as those used for the other tasks.
the model obtained by the ﬁrst-step ﬁne-tuning is.
2115