learn to resolve conversational dependency:a consistency training framework for conversationalquestion answering.
gangwoo kim hyunjae kim jungsoo park.
jaewoo kang†.
korea university{gangwoo kim,hyunjae-kim}@korea.ac.kr{jungsoo park,kangj}@korea.ac.kr.
abstract.
one of the main challenges in conversationalquestion answering (cqa) is to resolve theconversational dependency, such as anaphoraand ellipsis.
however, existing approachesdo not explicitly train qa models on how toresolve the dependency, and thus these mod-els are limited in understanding human dia-logues.
in this paper, we propose a novelframework, excord (explicit guidance onhow to resolve conversational dependency)to enhance the abilities of qa models in com-prehending conversational context.
excordﬁrst generates self-contained questions thatcan be understood without the conversationhistory, then trains a qa model with the pairsof original and self-contained questions usinga consistency-based regularizer.
in our exper-iments, we demonstrate that excord signiﬁ-cantly improves the qa models’ performanceby up to 1.2 f1 on quac (choi et al., 2018),and 5.2 f1 on canard (elgohary et al.,2019), while addressing the limitations of theexisting approaches.1.
1.introduction.
conversational question answering (cqa) involvesmodeling the information-seeking process of hu-mans in a dialogue.
unlike single-turn questionanswering (qa) tasks (rajpurkar et al., 2016;kwiatkowski et al., 2019), cqa is a multi-turnqa task, where questions in a dialogue are context-dependent;2 hence they need to be understood withthe conversation history (choi et al., 2018; reddyet al., 2019).
as illustrated in figure 1, to answer.
† corresponding author1our models and code are available at:.
https://github.com/dmis-lab/excord.
2while the term “context” usually refers to the evidencedocument from which the answer is extracted, in cqa, itrefers to conversational context..figure 1: an example of the quac dataset (choi et al.,2018).
owing to linguistic phenomena in human con-versations, such as anaphora and ellipsis, the currentquestion q3 should be understood based on the conver-sation history: q1, a1, q2, and a2.
question q3 can bereformulated as a self-contained question ˜q3 via a ques-tion rewriting (qr) process..the current question “was he close with anyoneelse?,” a model should resolve the conversationaldependency, such as anaphora and ellipsis, basedon the conversation history..a line of research in cqa proposes the end-to-end approach, where a single qa model jointlyencodes the evidence document, the current ques-tion, and the whole conversation history (huanget al., 2018; yeh and chen, 2019; qu et al., 2019a).
in this approach, models are required to automati-cally learn to resolve conversational dependencies.
however, existing models have limitations to do sowithout explicit guidance on how to resolve thesedependencies.
in the example presented in figure.
proceedingsofthe59thannualmeetingoftheassociationforcomputationallinguisticsandthe11thinternationaljointconferenceonnaturallanguageprocessing,pages6130–6141august1–6,2021.©2021associationforcomputationallinguistics6130title: leonardo da vinciconversation historywith his pupils salaiand melzi.a1who were his pupils?qleonardo's most intimate relationshipsa2was he close to his pupils?qwas he close with anyone else?qwas leonardo da vinciclose with anyone else other than his pupils salaiand melzi?qself-contained qquestion rewrite12331, models are trained without explicit signals that“he” refers to “leonardo da vinci,” and “anyoneelse” can be more elaborated with “other than hispupils, salai and melzi.”.
another line of research proposes a pipeline ap-proach that decomposes the cqa task into questionrewriting (qr) and qa, to reduce the complexityof the task (vakulenko et al., 2020).
based on theconversation history, qr models ﬁrst generate self-contained questions by rewriting the original ques-tions, such that the self-contained questions can beunderstood without the conversation history.
forinstance, the current question q3 is reformulated asthe self-contained question ˜q3 by a qr model infigure 1. after rewriting the question, qa modelsare asked to answer the self-contained questionsrather than the original questions.
in this approach,qa models are trained to answer relatively simplequestions whose dependencies have been resolvedby qr models.
thus, this limits reasoning abilitiesof qa models for the cqa task, and causes qamodels to rely on qr models..in this paper, we emphasize that qa modelscan be enhanced by using both types of ques-tions with explicit guidance on how to resolve theconversational dependency.
accordingly, we pro-pose excord (explicit guidance on how to re-solve conversational dependency), a novel train-ing framework for the cqa task.
in this framework,we ﬁrst generate self-contained questions using qrmodels.
we then pair the self-contained questionswith the original questions, and jointly encode themto train qa models with consistency regularization(laine and aila, 2016; xie et al., 2019).
speciﬁ-cally, when original questions are given, we encour-age qa models to yield similar answers to thosewhen self-contained questions are given.
this train-ing strategy helps qa models to better understandthe conversational context, while circumventingthe limitations of previous approaches..to demonstrate the effectiveness of excord,we conduct extensive experiments on the threecqa benchmarks.
in the experiments, our frame-work signiﬁcantly outperforms the existing ap-proaches by up to 1.2 f1 on quac (choi et al.,2018) and by 5.2 f1 on canard (elgohary et al.,2019).
in addition, we ﬁnd that our frameworkis also effective on a dataset coqa (reddy et al.,2019) that does not have the self-contained ques-tions generated by human annotators.
this indi-cates that the proposed framework can be adopted.
on various cqa datasets in future work.
we sum-marize the contributions of this work as follows:.
• we identify the limitations of previous ap-proaches and propose a uniﬁed frameworkto address these.
our novel framework im-proves qa models by incorporating qr mod-els, while reducing the reliance on them..• our framework encourages qa models tolearn how to resolve the conversational de-pendency via consistency regularization.
tothe best of our knowledge, our work is the ﬁrstto apply the consistency training frameworkto the cqa task..• we demonstrate the effectiveness of ourframework on three cqa benchmarks.
ourframework is model-agnostic and systemati-cally improves the performance of qa mod-els..2 background.
2.1 task formulation.
in cqa, a single instance is a dialogue, whichconsists of an evidence document d, a list of ques-tions q = [q1, ..., qt ], and a list of answers forthe questions a = [a1, ..., at ], where t representsthe number of turns in the dialogue.
for the t-thturn, the question qt and the conversation historyht = [(q1, a1), ..., (qt−1, at−1)] are given, and amodel should extract the answer from the evidencedocument as:.
ˆat = arg max.
p(at|d, qt, ht).
(1).
at.
where p(·) represents a likelihood function overall the spans in the evidence document, and ˆat isthe predicted answer.
unlike single-turn qa, sincethe current question qt is dependent on the con-versation history ht, it is important to effectivelyencode the conversation history and resolve theconversational dependency in cqa..2.2 end-to-end approach.
a naive approach in solving cqa is to train amodel in an end-to-end manner (figure 2a).
sincestandard qa models generally are ineffective inthe cqa task, most studies attempt to develop aqa model structure or mechanism for encoding theconversation history effectively (huang et al., 2018;yeh and chen, 2019; qu et al., 2019a,b).
although.
6131(a) end-to-end approach.
(b) pipeline approach.
(c) ours.
figure 2: overview of the end-to-end approach, the pipeline approach, and ours.
in the end-to-end approach, qamodels are asked to answer the original questions based on the conversation history.
in the pipeline approach, theself-contained questions are generated by a qr model, and then qa models answer them.
standard qa modelsare commonly used in this approach; however conversational qa models that encode the history can be adopted(the dotted line in figure (b)).
in ours, the original and self-contained question are jointly encoded to train qamodels with the consistency loss..these efforts improved performance on the cqabenchmarks, existing models remain limited in un-derstanding conversational context.
in this paper,we emphasize that qa models can be further im-proved with explicit guidance using self-containedquestions effectively..2.3 pipeline approach.
recent studies decompose the task into two sub-tasks to reduce the complexity of the cqa task.
the ﬁrst sub-task, question rewriting, involvesgenerating self-contained questions by reformu-lating the original questions.
neural-net-basedqr models are commonly used to obtain self-contained questions (lin et al., 2020; vakulenkoet al., 2020).
the qr models are trained on thecanard dataset (elgohary et al., 2019), whichconsists of 40k pairs of original quac questionsand their self-contained versions that are generatedby human annotators..after generating the self-contained questions,the next sub-task, question answering, is carriedout.
since it is assumed that the dependencies inthe questions have already been resolved by qrmodels, existing works usually use standard qamodels (not specialized to cqa); however conver-sational qa models can also be used (the dottedline in figure 2b).
we formulate the process of.
predicting the answer in the pipeline approach as:.
p(at|d, qt, ht) ≈prewr(˜qt|qt, ht) · pread(at|d, ˜qt).
(2).
where prewr(·) and pread(·) are the likelihood func-tions of qr and qa models, respectively.
˜qt is aself-contained question rewritten by the qr model.
the main limitation of the pipeline approachis that qa models are never trained on the origi-nal questions, which limits their abilities to under-stand the conversational context.
moreover, thisapproach makes qa models dependent on qr mod-els; hence qa models suffer from the error prop-agation from qr models.
3 on the other hand,our framework enhances qa models’ reasoningabilities for cqa by jointly utilizing original andself-contained questions.
in addition, qa modelsin our framework do not rely on qr models atinference time and thus do not suffer from errorpropagation..3 excord: explicit guidance on.
resolving conversational dependency.
we introduce a uniﬁed framework that jointly en-codes the original and self-contained questions as.
3we present an example of the error propagation in section.
5.2..6132answerqa lossevidencedocumentquestionansweringcurrent questionconversationhistorycurrent questionconversationhistoryanswerqa lossself-contained questionquestionrewritingevidencedocumentquestionansweringcurrent questionconversationhistoryself-contained questionanswerconsistency lossqa lossquestionansweringevidencedocumentquestionrewritinganswerqa lossillustrated in figure 2c.
our framework consists oftwo stages: (1) generating self-contained questionsusing a qr model (§3.1) and (2) training a qamodel with the original and self-contained ques-tions via consistency regularization (§3.2)..3.1 question rewriting.
similar to the pipeline approach, we utilize a qrmodel to obtain self-contained questions.
we usethe obtained questions for explicit guidance in thenext stage.
as shown in equation 2, the qr task isto generate a self-contained question given an orig-inal question and a conversation history.
followinglin et al.
(2020), we adopt a t5-based sequencegenerator (raffel et al., 2020) as our qr model,which achieves comparable performance with thatof humans in qr.4 for training and evaluating theqr model, we use the canard dataset followingprevious works on qr (lin et al., 2020; vakulenkoet al., 2020).
during inference, we utilize the top-krandom sampling decoding based on beam searchwith the adjustment of the softmax temperature(fan et al., 2018; xie et al., 2019)..3.2 consistency regularization.
our goal is to enhance the qa model’s ability tounderstand conversational context.
accordingly,we use consistency regularization (laine and aila,2016; xie et al., 2019), which enforces a model tomake consistent predictions in response to transfor-mations to the inputs.
we encourage the model’spredicted answers from the original questions to besimilar to those from the self-contained questions(§3.1).
our consistency loss is deﬁned as:.
θ.
¯θ.
lcons.
t = kl(pread.
(at|d, qt, ht)||pread.
(at|d, ˜qt, ˜ht))(3)where kl(·) represents the kullback–leibler di-vergence function between two probability distri-butions.
θ is the model’s parameters, and ¯θ depictsa ﬁxed copy of θ..with the consistency loss, qa models are regu-larized to make consistent predictions, regardlessof whether the given question is self-contained ornot.
in order to output an answer distribution that(at|d, ˜qt, ˜ht), qa models shouldis closer to preadtreat original questions as if they were rewritteninto self-contained questions by referring to the.
¯θ.
4on canard, our qr model achieved comparable per-formance with the human performance in preliminary experi-ments..conversation history.
through this process, our con-sistency regularization method serves as explicitguidance that encourages qa models to resolvethe conversational dependency.
in our framework,pread(at|·) is the answer span distribution over allθevidence document tokens.
in contrast to asaiand hajishirzi (2020), by using all probability val-ues in the answer distributions, the signals of self-contained questions can be effectively propagatedto the qa model.
in addition to using all proba-bility values, we also sharpened the target distri-(at|d, ˜qt, ˜ht) by adjusting the tempera-bution preadture (xie et al., 2019) to strengthen the qa model’straining signal..¯θ.
finally, we calculate the ﬁnal loss as:.
l = lorig + λ1lself + λ2lcons.
(4).
where λ1 and λ2 are hyperparameters.
lorig andlself are calculated by the negative log-likelihoodbetween the predicted answers and gold standardsgiven the original and self-contained questions, re-spectively..comparison with previous works consistencytraining has mainly been studied as a method forregularizing model predictions to be invariant tosmall noises that are injected into the input samples(sajjadi et al., 2016; laine and aila, 2016; miyatoet al., 2016; xie et al., 2019).
the intuition behindconsistency training is to push noisy inputs closertowards their original versions.
therefore, only theoriginal parameters (i.e., θ) are updated, while thecopied model parameters (i.e., ¯θ) are ﬁxed..in contrast to the original concept of consistencytraining, our goal is to go in the opposite directionand update the original parameters.
thus, we ﬁxthe parameters ¯θ with self-contained questions, andsoley update θ for each training step as shown inequation 3..4 experiments.
in this section, we describe our experimental setupand compare our framework to baseline approaches(i.e., the end-to-end and pipeline approaches)..4.1 datasets.
quac quac (choi et al., 2018) comprises 100kqa pairs in information-seeking dialogues, wherea student asks questions based on a topic withbackground information provided, and a teacherprovides the answers in the form of text spans in.
6133wikipedia documents.
since the test set is onlyavailable in the quac challenge, we evaluate mod-els on the development set.5 for validation, weuse a subset of the original training set of quac,which consists of questions that correspond to theself-contained questions in canard’s develop-ment set.
the remaining data is used for training..bert bert (devlin et al., 2019) is a contextu-alized word representation model that is pretrainedon large corpora.
bert also works well on cqadatasets, although it is not designed for cqa.
itreceives the evidence document, current question,and conversation history of the previous turn asinput..canard canard (elgohary et al., 2019)consists of 31k, 3k, and 5k qa pairs for train-ing, development, and test sets, respectively.
thequestions in canard are generated by rewritinga subset of the original questions in quac.
we usethe training and development sets for training andvalidating qr models, and the test set for evaluat-ing qa models..coqa coqa (reddy et al., 2019) consists of127k qa pairs and evidence documents in sevendomains.
in terms of the question distribution,coqa signiﬁcantly differs from quac (see §5.3).
we use coqa to test the transferability of ex-cord, where a qr model trained on canardgenerates the self-contained questions in a zero-shot manner.
subsequently, we train a qa modelby using the original and synthetic questions.
simi-lar to quac, the test set of coqa is soley availablein the coqa challenge.
6 therefore, we randomlysample 5% of the qa dialogues in the training setand adopt them as our development set..4.2 metrics.
following choi et al.
(2018), we use the f1, heq-q, and heq-d for quac and canard.
heq-qmeasures whether a model ﬁnds more accurate an-swers than humans (or the same answers) in a givenquestion.
heq-d measures the same thing, but ina given dialog instead of a question.
for coqa,we report the f1 scores for each domain (children’sstory, literature from project gutenberg, middleand high school english exams, news articles fromcnn, wikipedia) and the overall f1 score, as sug-gested by reddy et al.
(2019)..4.3 qa models.
note that the baseline approaches and our frame-work do not limit the structure of qa models.
fora fair comparison of the baseline approaches andexcord, we test the same qa models in all ap-proaches.
the selected qa models are commonlyused and have been proven to be effective in cqa..5https://quac.ai/6https://stanfordnlp.github.io/coqa/.
bert+hae bert+hae is a bert-based qamodel with a cqa-speciﬁc module.
following quet al.
(2019a), we add the history answer embed-ding (hae) to bert’s word embeddings.
haeencodes the information of the answer spans fromthe previous questions..roberta roberta (liu et al., 2019) improvesbert by using pretraining techniques to obtain therobustly optimized weights on larger corpora.
inour experiments, we found that roberta performswell in cqa, achieving comparable performancewith the previous sota model, ham (qu et al.,2019b), on quac.
thus, we adopt roberta as ourmain baseline model owing to its simplicity andeffectiveness.
it receives the same input as bert,otherwise speciﬁed..4.4.implementation details.
the canard training set provides 31,527 self-contained questions from the original quac ques-tions.
therefore, we can obtain 31,527 pairs oforiginal and self-contained questions without ques-tion rewriting.
for the rest of the original questions,we automatically generate self-contained questionsby using our qr model.
finally, we obtain 83,568question pairs and use them in our consistencytraining.
we denote the original questions, self-contained questions generated by humans, and self-contained questions generated by a qr model asq, ˜qhuman, and ˜qsyn, respectively.
additional im-plementation details are described in appendix b.
4.5 results.
table 1 presents the performance comparison of thebaseline approaches to our framework on quacand canard.
compared to the end-to-end ap-proach, excord consistently improves the per-formance of qa models on both datasets.
also,these improvements are signiﬁcant: excord im-proves the performance of the roberta by ab-solutely 1.2 and 2.3 f1 scores and bert by 1.2and 5.2 f1 scores on quac and canard, respec-tively.
from these results, we conclude that the con-sistency training with original and self-contained.
6134qa model.
approach.
bert.
bert+hae.
roberta.
end-to-endpipelineours.
end-to-endpipelineours.
end-to-endpipelineours.
f1.
61.561.2 (- 0.3)62.7 (+ 1.2).
62.061.1 (- 0.9)63.2 (+ 1.2).
66.565.2 (- 1.3)67.7 (+ 1.2).
quacheq-q.
57.156.8 (- 0.3)58.4 (+ 1.3).
57.356.3 (- 1.0)58.9 (+ 1.6).
62.460.9 (- 1.5)64.0 (+ 1.6).
heq-d.f1.
5.05.0 (–)6.0 (+ 1.0).
5.55.0 (- 0.5)5.7 (+ 0.2).
7.27.1 (- 0.1)9.3 (+ 2.1).
57.462.2 (+ 4.8)62.6 (+ 5.2).
58.262.4 (+ 4.2)63.1 (+ 4.9).
65.866.9 (+ 1.1)68.1 (+ 2.3).
canardheq-q.
52.957.8 (+ 4.9)58.2 (+ 5.3).
53.557.8 (+ 4.3)58.4 (+ 4.9).
62.263.2 (+ 1.0)64.2 (+ 2.0).
heq-d.3.26.0 (+ 2.8)6.4 (+ 3.2).
5.56.0 (+ 0.5)5.7 (+ 0.2).
7.17.3 (+ 0.2)8.4 (+ 1.3).
table 1: comparison in performance of the baseline approaches and our framework on quac and canard.
thebest scores are highlighted in bold..questions enhances ability of qa models to under-stand the conversational context..on quac, the pipeline approach underperformsthe end-to-end approach in all baseline models.
this indicates that training a qa model soley withself-contained questions is ineffective when humanrewrites are not given at the inference phase.
onthe other hand, excord improves qa modelsby using both types of questions.
as presented intable 1, our framework signiﬁcantly outperformsthe baseline approaches on quac..on canard, the pipeline approach is signiﬁ-cantly more effective than the end-to-end approach.
since qa models are trained with self-containedquestions in the pipeline approach, they performwell on canard questions.
nevertheless, ex-cord still outperforms the pipeline approach inmost cases.
compared to the pipeline approach, ourframework improves the performance of robertaby absolutely 1.2 f1 score..5 analysis and discussion.
we elaborate on analyses regarding component ab-lation and transferability.
we also describe a casestudy carried out to highlight such differences be-tween our and baseline approaches..5.1 ablation study.
in this section, we comprehensively explore thefactors contributing to this improvement in detail:(1) using self-contained questions that are rewrittenby humans ( ˜qhuman) as additional data, (2) usingself-contained questions that are synthetically gen-erated by the qr model ( ˜qsyn), and (3) traininga qa model with our consistency framework.
intable 2, we present the performance gaps wheneach component is removed from our framework.
we use roberta on quac in this experiment..method.
excord– ˜qsyn– ˜qhuman.
question augment.
(w/o.
excord).
– ˜qsyn– ˜qhuman– ˜qsyn, ˜qhuman (end-to-end).
quac canard.
f1.
67.767.567.3.
65.966.165.366.5.f1.
68.167.767.2.
66.266.566.065.8.table 2: effect of self-contained questions and our con-sistency framework.
we use roberta in this experi-ment..we ﬁrst explore the effects of ˜qhuman and˜qsyn.
as shown in table 2, excluding ˜qhuman de-grades the performance of roberta in our frame-work.
although automatically generated, ˜qsyn con-tributes to the performance improvement.
there-fore, both types of self-contained questions areuseful in our framework..to investigate the effect of our framework, wesimply augment ˜qhuman and ˜qsyn to qorig, whichis called question augment (question data augmen-tation).
we ﬁnd that question augment slightlyimproves the performance of roberta on ca-nard, whereas it degrades the performance onquac.
this shows that simply augmenting thequestions is ineffective and does not guarantee im-provement.
on the other hand, our consistencytraining approach signiﬁcantly improves perfor-mance, showing that excord is a more optimalway to utilizing self-contained questions..5.2 case study.
we analyze several cases that the baseline ap-proaches answered incorrectly, but our frameworkanswered correctly.
we also explore how our frame-work improves the reasoning ability of qa models,compared to the baseline approaches.
these cases.
6135section title : film career.
error case # 1title : montgomery cliftdocument d :· · ·his second movie was the search .
clift was unhappy with the quality of the script, and edited it himself.
the moviewas awarded a screenwriting academy award for the credited writers.
· · ·.
q1 : when did clift start his ﬁlm career?
a1 : his ﬁrst movie role was opposite john wayne in red river , which was shot in 1946 and released in 1948..current question q2 : was the ﬁlm a success?
human rewrite r2 : was montgomery clift’s ﬁlm red river a success?
golden answer : cannotanswerprediction of end-to-end : the movie was awarded a screenwriting academy award for the credited writers.
prediction of ours : cannotanswer.
error case # 2title : train (band).
section title : 2003-2004: my private nation.
· · ·q5 : did my private nation do any other features?
a5 : cannotanswer.
current question q6 : did my private nation have any good singles?
generated question ˜q6 : did train’s private nation have any good singles?
golden answer : “get to me” (written by rob hotchkiss and pat monahan) reached number nine on the billboardadult top 40.prediction of pipeline : cannotanswerprediction of ours : “get to me” (written by rob hotchkiss and pat monahan) reached number nine on the billboardadult top 40..table 3: error analysis for predictions of roberta that are trained with the baseline approaches and excord.
inthe ﬁrst case, the qa model trained with the end-to-end approach fails to resolve the conversational dependency.
the qr model in the second case misunderstands the ”my,” and generates an unnatural question, triggering anincorrect prediction..are obtained from the development set of quac..correct answer based on the original question q6..the ﬁrst case in table 3 shows the predictions ofthe two roberta models trained in the end-to-endapproach and our framework, respectively.
notethat “the ﬁlm” in the current question does not referto “the search” (red box) in the document d, but“red river” (blue box) in a1.
when trained in theend-to-end approach, the model failed to compre-hend the conversational context and misunderstoodwhat “the ﬁlm” refers to, resulting in an incorrectprediction.
on the other hand, when trained inexcord, the model predicted the correct answerbecause it enhances the ability to resolve conversa-tional dependency..in the second case, we compare the pipeline ap-proach to excord.
in this case, the qr modelmisunderstood “my” in the current question asa pronoun and replaced it with the band’s name,“train’s.” consequently, the qa model receivedthe erroneous self-contained question, resulting inan incorrect prediction.
on the other hand, theqa model trained in our framework predicted the.
5.3 transferability.
we train a qr model to rewrite quac questionsinto canard questions.
then, self-containedquestions can be generated for the samples that donot have human rewrites.
this results in the im-provement of qa models’ performance on quacand canard (§4.5).
however, it is questionablewhether the qr model can successfully rewritequestions when the original questions signiﬁcantlydiffer from those in quac.
to answer this, we testour framework on another cqa dataset, coqa.
we ﬁrst analyze how the question distributions ofquac and coqa differ.
we found that questiontypes in quac and coqa are signiﬁcantly differ-ent, such that qr models could suffer from thegap of question distributions between two datasets.
(see details in appendix a)..to test the transferability of excord, we com-pare the end-to-end approach to our framework onthe coqa dataset.
using a qr model trained on.
6136qa model.
bert.
end-to-endpipelineours.
roberta.
end-to-endpipelineours.
coqa (f1).
overall child.
liter.
m&h news wiki..78.376.178.8.
82.881.183.4.
77.975.778.2.
82.581.984.4.
73.973.275.8.
80.278.281.2.
76.474.175.5.
80.178.379.8.
80.678.081.3.
84.382.484.6.
82.779.683.2.
87.085.287.0.table 4: effect of our framework on the coqadataset that do not have human rewrites.
we excludebert+hae for simpliﬁcation in this experiment..canard, we generate the self-contained ques-tions for coqa and train qa models with ourframework.
as presented in table 4, our frame-work performs well on coqa.
the improvementin bert is 0.5 based on the overall f1, and theperformance of roberta is also improved by anoverall f1 of 0.6. improvements are also consis-tent in most of the documents’ domains.
therefore,we conclude that our framework can be simplyextended to other datasets and improve qa perfor-mance even when question distributions are signiﬁ-cantly different.
we plan to improve the transfer-ability of our framework by ﬁne-tuning qr modelson target datasets in future work..6 related work.
conversational question answering recently,several works introduced cqa datasets such asquac (choi et al., 2018) and coqa (reddy et al.,2019).
we classiﬁed proposed methods to solvethe datasets into two approaches: (1) end-to-endand (2) pipeline.
most works based on the end-to-end approach focused on developing a modelstructure (zhu et al., 2018; ohsugi et al., 2019; quet al., 2019a,b) or training strategy such as multi-task with rationale tagging (ju et al., 2019) that arespecialized in the cqa task or datasets.
severalworks demonstrated the effectiveness of the ﬂowmechanism in cqa (huang et al., 2018; chen et al.,2019; yeh and chen, 2019)..with the advent of a dataset consisting of self-contained questions rewritten by human annotators(elgohary et al., 2019), the pipeline approach hasdrawn attention as a promising method for cqain recent days (vakulenko et al., 2020).
the ap-proach is particularly useful for the open-domaincqa or passage-retrieval (pr) tasks (dalton et al.,2019; ren et al., 2020; anantha et al., 2020; quet al., 2020) since self-contained questions can be.
fed into existing non-conversational search enginessuch as bm25.
note that our framework can beused jointly with the pipeline approach in the open-domain setting because our framework can improveqa models’ ability to ﬁnd the answers from theretrieved documents.
we will test our frameworkin the open-domain setting in future work..question rewriting qr has been studied foraugmenting training data (buck et al., 2018; sunet al., 2018; zhu et al., 2019; liu et al., 2020) orclarifying ambiguous questions (min et al., 2020).
in cqa, qr can be viewed as a task of simplify-ing difﬁcult questions that include anaphora andellipsis in a conversation.
elgohary et al.
(2019)ﬁrst proposed the question rewriting task as asub-task of cqa and the canard dataset forthe task, which consists of pairs of original andself-contained questions that are generated by hu-man annotators.
vakulenko et al.
(2020) used acoreference-based model (lee et al., 2018) andgpt-2 (radford et al., 2019) as qr models andtested the models in the qr and pr tasks.
lin et al.
(2020) conducted the qr task using t5 (raffelet al., 2020) and achieved on performance compa-rable to humans on canard.
following lin et al.
(2020), we use t5 in our experiments to generatehigh-quality questions for enhancing qa models..consistency training consistency regulariza-tion (laine and aila, 2016; sajjadi et al., 2016)has been mainly explored in the context of semi-supervised learning (ssl) (chapelle et al., 2009;oliver et al., 2018), which has been adopted inthe textual domain as well (miyato et al., 2016;clark et al., 2018; xie et al., 2020).
however,the consistency training framework is also appli-cable when only the labeled samples are available(miyato et al., 2018; jiang et al., 2019; asai andhajishirzi, 2020).
the consistency regularizationrequires adding noise to the sample, which canbe either discrete (xie et al., 2020; asai and ha-jishirzi, 2020) or continuous (miyato et al., 2016;jiang et al., 2019).
existing works regularize thepredictions of the perturbed samples to be equiva-lent to be that of the originals’.
on the other hand,our method encourages the models’ predictions forthe original asnwers to be similar to those from therewritten questions, i.e., synthetic ones..61377 conclusion.
we propose a consistency training frameworkfor conversational question answering, which en-hances qa models’ abilities to understand conver-sational context.
our framework leverages both theoriginal and self-contained questions for explicitguidance on how to resolve conversational depen-dency.
in our experiments, we demonstrate that ourframework signiﬁcantly improves the qa model’sperformance on quac and canard, comparedto the existing approaches.
in addition, we veri-ﬁed that our framework can be extended to coqa.
in future work, the transferability of our frame-work can be further improved by ﬁne-tuning theqr model on target datasets.
furthermore, futurework would include applying our framework to theopen-domain setting..acknowledgements.
we thank sean s. yi, miyoung ko, and jinhyuklee for providing valuable comments and feedback.
this research was supported by the msit (min-istry of science and ict), korea, under the ictcreative consilience program (iitp-2021-2020-0-01819) supervised by the iitp (institute for in-formation communications technology planningevaluation).
this research was also supportedby national research foundation of korea (nrf-2020r1a2c3010638)..references.
raviteja anantha, svitlana vakulenko, zhucheng tu,shayne longpre, stephen pulman, and srinivaschappidi.
2020. open-domain question answeringgoes conversational via question rewriting.
arxivpreprint arxiv:2010.04898..yu chen, lingfei wu, and mohammed j zaki.
2019.graphﬂow: exploiting conversation ﬂow with graphneural networks for conversational machine compre-hension.
arxiv preprint arxiv:1908.00059..eunsol choi, he he, mohit iyyer, mark yatskar, wen-tau yih, yejin choi, percy liang, and luke zettle-moyer.
2018. quac: question answering in context.
arxiv preprint arxiv:1808.07036..kevin clark, minh-thang luong, christopher d man-ning, and quoc v le.
2018. semi-supervised se-quence modeling with cross-view training.
arxivpreprint arxiv:1809.08370..jeffrey dalton, chenyan xiong, and jamie callan.
2019. cast 2019: the conversational assistancetrack overview.
in proceedings of the twenty-eighthtext retrieval conference, trec, pages 13–15..jacob devlin, ming-wei chang, kenton lee, andkristina toutanova.
2019. bert: pre-training of deepbidirectional transformers for language understand-ing.
in naacl..ahmed elgohary, denis peskov, and jordan boyd-graber.
2019. can you unpack that?
learningto rewrite questions-in-context.
can you unpackthat?
learning to rewrite questions-in-context..angela fan, mike lewis, and yann dauphin.
2018. hi-in proceedingserarchical neural story generation.
of the 56th annual meeting of the association forcomputational linguistics (volume 1: long papers),pages 889–898..hsin-yuan huang, eunsol choi, and wen-tau yih.
2018. flowqa: grasping ﬂow in history for con-versational machine comprehension.
arxiv preprintarxiv:1810.06683..haoming jiang, pengcheng he, weizhu chen, xi-aodong liu, jianfeng gao, and tuo zhao.
2019.smart: robust and efﬁcient ﬁne-tuning for pre-language models through princi-trained naturalarxiv preprintpled regularized optimization.
arxiv:1911.03437..akari asai and hannaneh hajishirzi.
2020. logic-guided data augmentation and regularization forarxiv preprintconsistent question answering.
arxiv:2004.10157..ying ju, fubang zhao, shijie chen, bowen zheng,xuefeng yang, and yunfeng liu.
2019. technicalreport on conversational question answering.
arxivpreprint arxiv:1909.10772..christian buck, jannis bulian, massimiliano cia-ramita, wojciech gajewski, andrea gesmundo, neilhoulsby, and wei wang.
2018. ask the rightquestions: active question reformulation with rein-forcement learning.
in international conference onlearning representations..olivier chapelle, bernhard scholkopf, and alexanderzien.
2009. semi-supervised learning (chapelle, o.et al., eds.
; 2006)[book reviews].
ieee transactionson neural networks, 20(3):542–542..tom kwiatkowski, jennimaria palomaki, olivia red-ﬁeld, michael collins, ankur parikh, chris alberti,danielle epstein, illia polosukhin, jacob devlin,kenton lee, et al.
2019. natural questions: a bench-mark for question answering research.
transactionsof the association for computational linguistics,7:453–466..samuli laine and timo aila.
2016. temporal ensem-bling for semi-supervised learning.
arxiv preprintarxiv:1610.02242..6138kenton lee, luheng he, and luke zettlemoyer.
2018.higher-order coreference resolution with coarse-to-ﬁne inference.
in proceedings of the 2018 confer-ence of the north american chapter of the associ-ation for computational linguistics: human lan-guage technologies, volume 2 (short papers), pages687–692..chen qu, liu yang, minghui qiu, w bruce croft,yongfeng zhang, and mohit iyyer.
2019a.
bert withhistory answer embedding for conversational ques-in proceedings of the 42nd inter-tion answering.
national acm sigir conference on research anddevelopment in information retrieval, pages 1133–1136..sheng-chieh lin,.
jheng-hong yang, rodrigonogueira, ming-feng tsai, chuan-ju wang, andjimmy lin.
2020. conversational question refor-mulation via sequence-to-sequence architecturesarxiv preprintand pretrained language models.
arxiv:2004.01909..dayiheng liu, yeyun gong, jie fu, yu yan, jiushengchen, jiancheng lv, nan duan, and ming zhou.
2020. tell me how to ask again: question data aug-mentation with controllable rewriting in continuousin proceedings of the 2020 conference onspace.
empirical methods in natural language processing(emnlp), pages 5798–5810..yinhan liu, myle ott, naman goyal, jingfei du, man-dar joshi, danqi chen, omer levy, mike lewis,luke zettlemoyer, and veselin stoyanov.
2019.roberta: a robustly optimized bert pretraining ap-proach.
arxiv preprint arxiv:1907.11692..sewon min, julian michael, hannaneh hajishirzi, andluke zettlemoyer.
2020. ambigqa: answeringambiguous open-domain questions.
arxiv preprintarxiv:2004.10645..takeru miyato, andrew m dai, and ian good-fellow.
2016. adversarial training methods forsemi-supervised text classiﬁcation.
arxiv preprintarxiv:1605.07725..takeru miyato, shin-ichi maeda, masanori koyama,and shin ishii.
2018. virtual adversarial training:a regularization method for supervised and semi-ieee transactions on pat-supervised learning.
tern analysis and machine intelligence, 41(8):1979–1993..yasuhito ohsugi,.
itsumi saito, kyosuke nishida,hisako asano, and junji tomita.
2019. a simple buteffective method to incorporate multi-turn contextwith bert for conversational machine comprehension.
arxiv preprint arxiv:1905.12848..avital oliver, augustus odena, colin a raffel,ekin dogus cubuk, and ian goodfellow.
2018. re-alistic evaluation of deep semi-supervised learningalgorithms.
advances in neural information process-ing systems, 31:3235–3246..chen qu, liu yang, cen chen, minghui qiu, w brucecroft, and mohit iyyer.
2020. open-retrieval con-in proceedings ofversational question answering.
the 43rd international acm sigir conference onresearch and development in information retrieval,pages 539–548..chen qu, liu yang, minghui qiu, yongfeng zhang,cen chen, w bruce croft, and mohit iyyer.
2019b.
attentive history selection for conversational ques-tion answering.
in proceedings of the 28th acm in-ternational conference on information and knowl-edge management, pages 1391–1400..alec radford, jeffrey wu, rewon child, david luan,dario amodei, and ilya sutskever.
2019. languagemodels are unsupervised multitask learners.
openaiblog, 1(8):9..colin raffel, noam shazeer, adam roberts, katherinelee, sharan narang, michael matena, yanqi zhou,wei li, and peter j liu.
2020. exploring the lim-its of transfer learning with a uniﬁed text-to-texttransformer.
journal of machine learning research,21(140):1–67..pranav rajpurkar, jian zhang, konstantin lopyrev, andpercy liang.
2016.squad: 100,000+ questionsfor machine comprehension of text.
arxiv preprintarxiv:1606.05250..siva reddy, danqi chen, and christopher d manning.
2019. coqa: a conversational question answeringchallenge.
transactions of the association for com-putational linguistics, 7:249–266..pengjie ren, zhumin chen, zhaochun ren, evange-los kanoulas, christof monz, and maarten de ri-jke.
2020. conversations with search engines.
arxivpreprint arxiv:2004.14162..mehdi sajjadi, mehran javanmardi, and tolga tas-dizen.
2016. regularization with stochastic transfor-mations and perturbations for deep semi-supervisedlearning.
in advances in neural information process-ing systems, pages 1163–1171..kai sun, dian yu, dong yu, and claire cardie.
improving machine reading comprehensionarxiv preprint.
2018.with general reading strategies.
arxiv:1810.13441..svitlana vakulenko, shayne longpre, zhucheng tu,and raviteja anantha.
2020. question rewriting forconversational question answering.
arxiv preprintarxiv:2004.14652..qizhe xie, zihang dai, eduard hovy, minh-thang lu-ong, and quoc v le.
2019. unsupervised data aug-mentation for consistency training.
arxiv preprintarxiv:1904.12848..qizhe xie, zihang dai, eduard hovy, thang luong,and quoc le.
2020. unsupervised data augmenta-tion for consistency training.
advances in neuralinformation processing systems, 33..6139yi-ting yeh and yun-nung chen.
2019. flowdelta:modeling ﬂow information gain in reasoning forarxivconversational machine comprehension.
preprint arxiv:1908.05117..chenguang zhu, michael zeng, and xuedong huang.
sdnet: contextualized attention-based2018.deep network for conversational question answering.
arxiv preprint arxiv:1812.03593..haichao zhu, li dong, furu wei, wenhui wang, bingqin, and ting liu.
2019. learning to ask unanswer-able questions for machine reading comprehension.
arxiv preprint arxiv:1906.06045..6140quac.
coqa.
title : scott walker (politician)section title : educationq1 : what kind of education did scott walker have?
a1 : cannotanswerq2 : are there any other interesting aspects about this article?
a2 : signed a law to fund evaluation of the reading skillsof kindergartners as part of an initiative to ensure that studentsare reading at or above grade level.
q1 : is the us dollar on a decimal system?
a1 : u.s. dollar is based upon a decimal system of values.
iq2 : what country’s dollar is not?
a2 : unlike the spanish milled dollar the u.s. dollar isbased upon a decimal system of values.
q3 : what is a mill?
a3 : n addition to the dollar the coinage act ofﬁciallyestablished monetary units of mill or one-thousandth of a dollar.
current question q3 : what other programs did he sign?
self-contained question ˜q3 :what other programs did scott walkersign other than a law to fund evaluation.
current question q4 : and a cent?
self-contained question ˜q4 : what is a cent?
-.
table 5: comparison of questions in quac and coqa.
in the left side, we can observe several question types thatare frequently used in quac: unanswerable question (q1) and “anything else?” question (q2).
the current questionq3 refers to the previous answer (green box) and the background information (blue box).
on the other hand, in theright side, the current question q4 omits the question word that are used in the previous question (yellow box)..question type.
quac coqa.
non-factoidanything else?
unanswerable.
54 %11 %20 %.
38 %3† %1 %.
table 6: statistics of question types for quac andcoqa.
all values can be found in the quac and coqapapers (choi et al., 2018; reddy et al., 2019) exceptfor those with the dagger †.
we randomly sampled 106questions and manually labeled for obtaining the num-ber with the dagger †..a comparison of questions in quac.
and coqa.
before testing the transferability of excord(§5.3), we compare the question distribution ofquac to that of coqa.
the types of questionsare signiﬁcantly different due to the difference intask setups.
when questions were generated inquac, evidence documents were soley providedto answerers, but not to questioners.
this setup pre-vented questioners from referring to the evidencedocuments, which encouraged the questioners toask natural and information-seeking questions.
bycontrast, when creating coqa, questioners and an-swerers shared the same evidence documents..examples of quac and coqa are presented intable 5 and the categorization of question types intable 6. the results are as follows: (1) quac hasmore non-factoid questions.
approximately halfof quac questions are non-factoid, whereas morethan 60% of questions in coqa can be answeredwith either entities or noun phrases.
(2) “anythingelse?” questions are more frequently observed inquac.
when questioners cannot ﬁnd what to ask,they use “anything else?” questions to seek newtopics and continue the conversation.
in coqa,.
questioners rarely used the “anything else?” ques-tion (2.8%) since they did not need to seek new top-ics.
this type of question is observed in table 5 (q2in the left side).
(3) coqa has few unanswerablequestions.
since questioners and answerers sharethe evidence documents when creating coqa, only1.3% of unanswerable questions are asked.
how-ever, approximately 20% of questions in quac areunanswerable..b hyperparameters.
our implementation is based on pytorch.7 we im-plemented bert using the transformers library.8we implemented the t5-based qr model usingthe transformers library and adopted the same qrmodel in the pipeline approach and excord.
weuse a single 24gb gpu (rtx titan) for the ex-periments..we measured the f1 scores on the developmentset for each 4k training step, and adopted the best-performing models.
we trained qa models basedon the adamw optimizer with a learning rate of3e-5.
we use the maximum input sequence lengthas 512 and the maximum answer length as 30. weset the maximum query length to 128 for all ap-proaches since self-contained questions are usuallylonger than original questions.
we use a batchsize 12 for bert and roberta in all baseline ap-proaches.
for excord, we set the coefﬁcient λ1for qa loss for rewritten questions to 0.5. also wesearch the coefﬁcient λ2 for consistency loss withinthe range of [0.7, 0.5] and the softmax temperaturewithin the range of [1.0, 0.9] (xie et al., 2019)..7https://pytorch.org/8https://github.com/huggingface.
6141