semi-supervised text classiﬁcation with balanced deep representationdistributions.
changchun li, ximing li∗† and jihong ouyang†college of computer science and technology, jilin university, chinakey laboratory of symbolic computation and knowledge engineeringof ministry of education, chinachangchunli93@gmail.com, liximing86@gmail.com, ouyj@jlu.edu.cn.
abstract.
semi-supervised text classiﬁcation (sstc)mainly works under the spirit of self-training.
they initialize the deep classiﬁer by trainingover labeled texts; and then alternatively pre-dict unlabeled texts as their pseudo-labels andtrain the deep classiﬁer over the mixture oflabeled and pseudo-labeled texts.
naturally,their performance is largely affected by theaccuracy of pseudo-labels for unlabeled texts.
unfortunately, they often suffer from low ac-curacy because of the margin bias problemcaused by the large difference between repre-sentation distributions of labels in sstc.
toalleviate this problem, we apply the angularmargin loss, and perform gaussian linear trans-formation to achieve balanced label angle vari-ances, i.e., the variance of label angles of textswithin the same label.
more accuracy of pre-dicted pseudo-labels can be achieved by con-straining all label angle variances balanced,where they are estimated over both labeled andpseudo-labeled texts during self-training loops.
with this insight, we propose a novel sstcmethod, namely semi-supervised text clas-siﬁcation with balanced deep representationdistributions (s2tc-bdd).
to evaluate s2tc-bdd, we compare it against the state-of-the-art sstc methods.
empirical results demon-strate the effectiveness of s2tc-bdd, espe-cially when the labeled texts are scarce..1.introduction.
semi-supervised learning (ssl) refers to theparadigm of learning with labeled as well as un-labeled data to perform certain applications (vanengelen and hoos, 2020).
especially, developingeffective ssl models for classifying text data haslong been a goal for the studies of natural languageprocessing, because labeled texts are difﬁcult to col-lect in many real-world scenarios.
formally, this.
∗ contributing equally with the ﬁrst author.
† corresponding author..figure 1: the average difference of label angle vari-ances (avg.dlav) computed in semi-supervised andsupervised manners across ag news, respectively..research topic is termed as semi-supervised textclassiﬁcation (sstc), which nowadays drawsmuch attention from the community (clark et al.,2018; gururangan et al., 2019; chen et al., 2020).
to our knowledge, the most recent sstc meth-ods mainly borrow ideas from the successful pat-terns of supervised deep learning, such as pre-training and ﬁne-tuning (dai and le, 2015; howardand ruder, 2018; peters et al., 2018; gururanganet al., 2019; devlin et al., 2019).
generally, thosemethods perform deep representation learning onunlabeled texts followed by supervised learning onlabeled texts.
however, a drawback is that theyseparately learn from the labeled and unlabeledtexts, where, speciﬁcally, the deep representationsare trained without using the labeling information,resulting in potentially less discriminative represen-tations as well as worse performance..to avoid this problem, other sstc methods com-bine the traditional spirit of self-training with deeplearning, which jointly learn the deep representa-tion and classiﬁer using both labeled and unlabeledtexts in a uniﬁed framework (miyato et al., 2017,2019; sachan et al., 2019; xie et al., 2020; chen.
proceedingsofthe59thannualmeetingoftheassociationforcomputationallinguisticsandthe11thinternationaljointconferenceonnaturallanguageprocessing,pages5044–5053august1–6,2021.©2021associationforcomputationallinguistics5044et al., 2020).
to be speciﬁc, this kind of meth-ods initializes a deep classiﬁer, e.g., bert (devlinet al., 2019) with angular margin (am) loss (wanget al., 2018), by training over labeled texts only;and then it alternatively predicts unlabeled textsas their pseudo-labels and trains the deep classi-ﬁer over the mixture of labeled and pseudo-labeledtexts.
accordingly, both labeled and unlabeledtexts can directly contribute to the deep classiﬁertraining..generally speaking, for deep self-training meth-ods, one signiﬁcant factor of performance is theaccuracy of pseudo-labels for unlabeled texts.
un-fortunately, they often suffer from low accuracy,where one major reason is the margin bias prob-lem.
to interpret this problem, we look aroundthe am loss with respect to the label angle, i.e.,the angles between deep representations of textsand weight vectors of labels.
for unlabeled texts,the pseudo-labels are predicted by only ranking thelabel angles, but neglecting the difference betweenlabel angle variances, i.e., the variance of labelangles of texts within the same label, which mightbe much large in ssl as illustrated in fig.1.
inthis context, the boundary of am loss is actuallynot the optimal one, potentially resulting in loweraccuracy for pseudo-labels (see fig.2(a))..to alleviate the aforementioned problem, we pro-pose a novel sstc method built on bert with amloss, namely semi-supervised text classiﬁcationwith balanced deep representation distributions(s2tc-bdd).
most speciﬁcally, in s2tc-bdd, wesuppose that the label angles are drawn from eachlabel-speciﬁc gaussian distribution.
therefore, foreach text we can apply linear transformation opera-tions to balance the label angle variances.
this isequivalent to moving the boundary to the optimalone, so as to eliminate the margin bias (see exam-ples in fig.2(b)).
we can estimate each label anglevariance over both labeled and pseudo-labeled textsduring the self-training loops.
we evaluate the pro-posed s2tc-bdd method by comparing the mostrecent deep sstc methods.
experimental resultsindicate the superior performance of s2tc-bddeven with very few labeled texts..2 related work.
the pre-training and ﬁne-tuning framework haslately shown impressive effectiveness on a varietyof tasks (dai and le, 2015; radford et al., 2019a;howard and ruder, 2018; peters et al., 2018; de-.
figure 2: let solid circle and triangle denote labeledpositive and negative texts, and hollow ones denote cor-responding unlabeled texts.
(a) the large difference be-tween label angle variances results in the margin bias.
many unlabeled texts (in red) can be misclassiﬁed.
(b)balancing the label angle variances can eliminate themargin bias.
best viewed in color..vlin et al., 2019; yang et al., 2019; chen et al.,2019; akbik et al., 2019; radford et al., 2019b;brown et al., 2020; chen et al., 2020).
they mainlyperform deep representation learning on genericdata, followed by supervised learning for down-stream tasks.
several sstc methods are builton this framework (dai and le, 2015; howardand ruder, 2018; peters et al., 2018; gururanganet al., 2019; devlin et al., 2019).
for instance, thevariational methods for pretraining in resource-limited environments (vampire) (gururanganet al., 2019) ﬁrst pre-trains a variational auto-encoder (vae) model on unlabeled texts, and thentrains a classiﬁer on the augmentation representa-tions of labeled texts computed by the pre-trainedvae.
however, the vae model is trained withoutusing the labeling information, resulting in poten-tially less discriminative representations for labeledtexts..recent works on sstc mainly focus on deepself-training (miyato et al., 2017; clark et al., 2018;sachan et al., 2019; miyato et al., 2019; xie et al.,2020; chen et al., 2020), which can jointly learndeep representation and classiﬁer using both la-beled and unlabeled texts in a uniﬁed framework.
it is implemented by performing an alternative pro-cess, in which the pseudo-labels of unlabeled textsare updated by the current deep classiﬁer, and thenthe deep classiﬁer is retrained over both labeledand pseudo-labeled texts.
for example, the virtualadversarial training (vat) method (miyato et al.,2017, 2019) follows the philosophy of making theclassiﬁer robust against random and local pertur-bation.
it ﬁrst generates the predictions of originaltexts with the current deep classiﬁer and then trainsthe deep classiﬁer by utilizing a consistency loss.
5045between the original predictions and the outputs ofdeep classiﬁer over noise texts by applying localperturbations to the embeddings of original texts.
further, the work in (sachan et al., 2019) combinesmaximum likelihood, adversarial training, virtualadversarial training, and entropy minimization in auniﬁed objective.
furthermore, rather than apply-ing local perturbations, unsupervised data aug-mentation (uda) (xie et al., 2020) employs con-sistency loss between the predictions of unlabeledtexts and corresponding augmented texts by dataaugmentation techniques such as back translationsand tf-idf word replacements.
the work (clarket al., 2018) exploits cross-view training by match-ing the predictions of auxiliary prediction modulesover the restricted views of unlabeled texts (e.g.,only part of sentence) with ones of primary predic-tion module over the corresponding full views..orthogonal to the aforementioned self-trainingsstc methods, our s2tc-bdd further considersthe margin bias problem by balancing the label an-gle variances.
this is beneﬁcial for more accuratepseudo-labels for unlabeled texts, so as to boost theperformance of sstc tasks..3 the proposed s2tc-bdd method.
in this section, we describe the proposed deep self-training sstc method, namely semi-supervisedtext classiﬁcation with balanced deep represen-tation distributions (s2tc-bdd)..i and xu.
i, ylj }j=nu.
formulation of sstc consider a trainingdataset d consisting of a limited labeled text seti)}i=nldl = {(xli=1 and a large unlabeled text setj=1 .
speciﬁcally, let xldu = {xuj de-note the word sequences of labeled and unlabeledi ∈ {0, 1}k denote thetexts, respectively; and let ylcorresponding one-hot label vector of xli, whereylik = 1 if the text is associated with the k-th label,or ylik = 0 otherwise.
we declare that nl, nu, andk denote the numbers of labeled texts, unlabeledtexts and category labels, respectively.
in this pa-per, we focus on the paradigm of inductive sstc,whose goal is to learn a classiﬁer from the trainingdataset d with both labeled and unlabeled texts.
the important notations are described in table 1..3.1 overview of s2tc-bddoverall speaking, our s2tc-bdd performs a self-training procedure for sstc.
given a trainingdataset, it ﬁrst trains a ﬁne-tuned deep classiﬁerbased on the pre-trained bert model (devlin et al.,.
table 1: summary of notations.
notation.
nlnukdlduxlxuyl ∈ {0, 1}k.description.
number of labeled textsnumber of unlabeled textsnumber of category labelslabeled text setunlabeled text setword sequence of labeled text in dlword sequence of unlabeled text in duone-hot label vector of labeled text.
2019) with am loss (wang et al., 2018).
dur-ing the self-training loops, we employ the currentdeep classiﬁer to predict unlabeled texts as pseudo-labels, and then update it over both labeled andpseudo-labeled texts.
in particular, we develop abalanced deep representation distribution (bdd)loss, aiming at more accurate pseudo-labels for un-labeled texts.
the overall framework of s2tc-bddis shown in fig.3.
we now present the importantdetails of s2tc-bdd..bdd loss formally, our bdd loss is extendedfrom the am loss (wang et al., 2018).
for clarity,we ﬁrst describe the am loss with respect to an-gles.
given a training example (xi, yi), it can beformulated below:.
lam(xi, yi; φ) =k(cid:88).
yik log.
−.
k=1.
es(cos(θik)−yikm)j=1 es(cos(θij )−yij m).
,.
(cid:80)k.(1).
where φ denotes the model parameters,.
cos(θik) =.
f (cid:62)i wk(cid:107)fi(cid:107)2(cid:107)wk(cid:107)2.,.
(cid:107)·(cid:107)2 is the (cid:96)2-norm of vectors; fi and wk denotethe deep representation of text xi and the weightvector of label k, respectively; θik is the angle be-tween fi and wk; s and m are the parameters usedto control the rescaled norm and magnitude of co-sine margin, respectively..reviewing eq.1, we observe that it directlymeasures the loss by label angles of texts only.
we kindly argue thatit corresponds to non-optimal decision boundary in sstc, where thedifference between label angle variances is muchlarger than supervised learning.
to alleviate thisproblem, we suppose that the label angles aredrawn from each label-speciﬁc gaussian distri-.
5046figure 3: overview the framework of s2tc-bdd.
best viewed in color..k)}k=k.
bution {n (µk, σ2k=1 .
thanks to the proper-ties of gaussian distribution, we can easily trans-fer them into the ones with balanced variances{n (µk, (cid:98)σ2)}k=kby performingthe following linear transformations to the angles:.
k=1 , (cid:98)σ2 =.
k=1 σ2kk.(cid:80)k.ψk(θik) = akθik + bk,.
∀k ∈ [k],.
(2).
where.
ak = (cid:98)σσk.
,.
bk = (1 − ak)µk..(3).
with these linear transformations {ψk(·)}k=kk=1 , allangles become the samples from balanced an-gular distributions with the same variances, e.g.,ψk(θik) ∼ n (µk, (cid:98)σ2).
accordingly, the angularloss of eq.1 can be rewritten as the following bddloss:.
lbdd(xi, yi; φ) =k(cid:88).
yik log.
−.
k=1.
es(cos(ψk(θik))−yikm)j=1 es(cos(ψj (θij ))−yij m).
..(cid:80)k.(4).
supervised angular loss applying the bddloss lbdd of eq.4 to the labeled text set dl =i)}i=nl{(xli=1 , we can formulate the following su-pervised angular loss:.
i, yl.
ll(dl; φ) =.
lbdd(xl.
i, yl.
i; φ)..(5).
1nl.
nl(cid:88).
i=1.
unsupervised angular loss under the self-training paradigm, we form the loss with unla-beled texts and pseudo-labels.
speciﬁcally, we.
denote the pseudo-label as the output probability ofthe deep classiﬁer.
it is computed by normalizing{cos(ψk(θik))}k=k.
k=1 with the softmax function:.
p(k|xi, φ) =.
ecos(ψk(θik))j=1 ecos(ψj (θij )).
(cid:80)k.(cid:44) yi, ∀k ∈ [k]..for each unlabeled text xui the pseudo-label distri-i , (cid:101)φ) (cid:44) yubution is given by p(k|xui with the ﬁxedcopy (cid:101)φ of the current model parameter φ duringself-training loops.
besides, to avoid those pseudo-i }nulabel distributions {yui=1 too uniform, we em-ploy a sharpen function with a temperature t overthem:.
i = sharpen(yuyu.
i , t ) =.
, ∀i ∈ [nu],.
(yu(cid:107)(yu.
i )1/ti )1/t (cid:107)1.where (cid:107)·(cid:107)1 is the (cid:96)1-norm of vectors.
when t → 0,the pseudo-label distribution tends to be the one-hot vector..applying the bdd loss of eq.4 to the unlabeledand pseudo-label distri-i=1, we can formulate the following.
text set du = {xui }nubutions {yuunsupervised angular loss:.
j }j=nu.
j=1.
lu(du, {yu.
i }nu.
i=1; φ) =.
lbdd(xu.
i , yu.
i ; φ)..1nu.
nu(cid:88).
i=1.
entropy regularization further, we employ theconditional entropy of p(y|xi, φ) as an additionalregularization term:.
r(dl, du; φ) =.
−.
1nl + nu.
(cid:88).
k(cid:88).
xi∈dl,du.
k=1.
p(k|xi, φ) log p(k|xi, φ)..(6).
(7).
5047this conditional entropy regularization is intro-duced by (grandvalet and bengio, 2004), and alsoutilized in (sajjadi et al., 2016; miyato et al., 2019;sachan et al., 2019).
it also sharpens the outputprobability of the deep classiﬁer..full objective of s2tc-bdd combining the su-pervised angular loss eq.
(5), unsupervised angularloss eq.
(6), and entropy regularization eq.
(7), thefull objective of s2tc-bdd can be formulated be-low:.
l(dl, du; φ) = ll(dl; φ)+ λ1lu(du, {yu.
i }nu.
i=1; φ) + λ2r(dl, du; φ),(8).
where λ1 and λ2 are regularization parameters..3.2.implementations of label anglevariances.
k)}k=k.
in this section, we describe implementations of la-bel angle variances.
as mentioned before, what weconcern is the estimations of angular distributions{n (µk, σ2k=1 , where their draws are the anglesbetween deep representations of texts and label pro-totypes denoted by {ck}k=kk)}k=kk=1and {ck}k=kk=1 are estimated over both labeled andpseudo-labeled texts during self-training loops.
inthe following, we describe their learning processesin more detail..k=1 .
both {(µk, σ2.
k=1 and {ck}k=k.
within the framework of stochastic optimization,k)}k=kwe update the {(µk, σ2k=1 per-epoch.
for convenience, we denote ω as the indexset of labeled and unlabeled texts in one epoch,{fi}i∈ω and {yi}i∈ω as the deep representationsof texts and corresponding label or pseudo-labelvectors (i.e., yli ) in the current epoch, respec-tively..i or yu.
estimating label prototypes given the current{fi}i∈ω and {yi}i∈ω, we calculate the label proto-types {ck}k=kk=1 by the weighted average of {fi}i∈ω,formulated below:.
ck =.
(cid:80)(cid:80).
i∈ω yikfii∈ω yik.
,.
∀k ∈ [k]..(9).
to avoid the misleading affect of some mislabeledtexts, inspired by (liu et al., 2020), we update{ck}k=kk=1 by employing the moving average with alearning rate γ:.
estimating label angle variances given{fi}i∈ω and {ck}k=kk=1 , the angles between themcan be calculated by:.
βik = arccos(cid:0).
f (cid:62)i ck(cid:107)fi(cid:107)2(cid:107)ck(cid:107)2.
(cid:1),.
∀i ∈ ω, k ∈ [k]..(10)accordingly, we can compute the estimations of{µk}k=k.
k=1 and {σ2.
k}k=k(cid:80).
k=1 as follows:i∈ω yikβik(cid:80)i∈ω yik.
,.
µk =.
σ2k =.
(cid:80).
i∈ω yik(βik − µk)2(cid:80)i∈ω yik − 1.
..(11).
(12).
further, the moving average is also used to theupdates below:.
µ(t)k ← (1 − γ)µ(t)k)(t) ← (1 − γ)(σ2.
k + γµ(t−1),kk)(t−1).
k)(t) + γ(σ2.
(σ2.
4 experiment.
4.1 experimental settings.
datasets to conduct the experiments, we em-ploy three widely used benchmark datasets for textclassiﬁcation: ag news (zhang et al., 2015), yelp(zhang et al., 2015), and yahoo (chang et al., 2008).
for all datasets, we form the unlabeled training setdu, labeled training set dl and development set byrandomly drawing from the corresponding originaltraining datasets, and utilize the original test setsfor prediction evaluation.
the dataset statistics andsplit information are described in table 2..baseline models to evaluate the effectivenessof s2tc-bdd, we choose ﬁve existing sstc al-gorithms for comparison.
the details of baselinemethods are given below..• nb+em (nigam et al., 2000): a semi-supervised text classiﬁcation method com-bining a naive bayes classiﬁer (nb) andexpectation-maximization (em).
in experi-ments, we pre-process texts following (gu-rurangan et al., 2019) and use tf-idfs as therepresentations of texts..• bert (devlin et al., 2019): a supervised textclassiﬁcation method built on the pre-trainedbert-based-uncased model1 and ﬁne-tunedwith the supervised softmax loss on labeledtexts..k ← (1 − γ)c(t)c(t).
k + γc(t−1).
k...1 https://pypi.org/project/.
pytorch-transformers/.
5048• bert+am: a semi-supervised text classiﬁ-cation method built on the pre-trained bert-based-uncased1 and ﬁne-tuned following theself-training spirit with the am loss on bothlabeled and unlabeled texts..• vampire (gururangan et al., 2019): asemi-supervised text classiﬁcation methodbased on variational pre-training.
the codeis available on the net.2 in experiments, thedefault parameters are utilized..• vat (miyato et al., 2019): a semi-supervisedtext classiﬁcation method based on virtual ad-versarial training.
[parameter conﬁguration:perturbation size (cid:15) = 5.0, regularization co-efﬁcient α = 1.0, hyperparameter for ﬁnitedifference ξ = 0.1].
• uda (xie et al., 2020): a semi-supervisedtext classiﬁcation method based on unsuper-vised data augmentation with back translation.
the code is available on the net.3 in experi-ments, we utilize the default parameters, andgenerate the augmented unlabeled data by us-ing fairseq4 with german as the intermediatelanguage..for s2tc-bdd, bert, bert+am, vat and uda,we utilize bert-based-uncased tokenizer to to-kenize texts; average pooling over bert-based-uncased model as text encoder to encode texts; anda two-layer mlp, whose hidden size and activationfunction are 128 and tanh respectively, as the clas-siﬁer to predict labels.
we set the max sentencelength as 256 and remain the ﬁrst 256 tokens fortexts exceeding the length limit.
for optimization,we utilize the adam optimizer with learning ratesof 5e-6 for bert encoder and 1e-3 for mlp classi-ﬁer.
for bert, we set the batch size of labeled testsas 8. for s2tc-bdd, bert+am, vat and uda,the batch sizes of labeled and unlabeled tests are 4and 8, respectively.
for all datasets, we iterate 20epochs, where each one contains 200 inner loops.
all experiments are carried on a linux server withtwo nvidia geforce rtx 2080ti gpus, intelxeon e5-2640 v4 cpu and 64g memory..parameter settings for s2tc-bdd, in our ex-periments, its parameters are mostly set as: λ1 =.
2 https://github.com/allenai/vampire3 https://github.com/google-research/uda4 https://github.com/pytorch/fairseq.
table 2: statistics of datasets.
#class: the number ofclass labels.
#labeled: the number of labeled trainingtexts.
#unlabeled: the number of unlabeled trainingtexts.
#dev: the number of development texts.
#test:the number of texts for testing..dataset.
#class #labeled #unlabeled #dev.
#test.
ag newsyelpyahoo.
4510.
10,00010,00010,000.
20,00020,00040,000.
8,0007,60010,000 50,00020,000 60,000.
1.0, λ2 = 1.0, s = 1.0, m = 0.01. speciﬁcally,for yelp we set m = 0.3. for the sharpening tem-perature t , we set 0.5 for ag news and yahoo, 0.3for yelp.
the learning rate γ of label prototypesand label angle variances is set to 0.1..metrics we utilize two metrics of micro-f1 andmacro-f1, which are two different types of theaveraged f1 scores.
in experiments, we employ theimplementation of micro-f1 and macro-f1 in thepublic scikit-learn (pedregosa et al., 2011) tool.5.
4.2 results.
for all datasets, we perform each method with ﬁverandom seeds, and report the average scores..4.2.1 varying number of labeled texts.
we ﬁrst evaluate the classiﬁcation performance ofs2tc-bdd with different amounts of labeled texts.
for all methods, we conduct the experiments byvarying the number of labeled texts nl over theset {100, 1000, 10000} with the number of unla-beled texts nu = 20000 for ag news and yelp,and nu = 40000 for yahoo.
the classiﬁcationresults of both micro-f1 and macro-f1 over alldatasets are shown in table 3, in which the bestscores among all comparing baselines are high-lighted in boldface.
generally speaking, our pro-posed s2tc-bdd outperforms the baselines in mostcases.
across all datasets and evaluation metrics,s2tc-bdd ranks 1.1 in average.
several observa-tions are made below..• comparing s2tc-bdd against baselines:first, we can observe that s2tc-bdd consis-tently dominates the pre-training methods (in-cluding bert and vampire) on both micro-f1 and macro-f1 scores by a big margin, es-pecially when labeled texts are scarce.
for ex-ample, when nl = 100, the macro-f1 scores.
5 https://scikit-learn.org/stable/.
5049table 3: experimental results of micro-f1 and macro-f1 varying the number of labeled texts nl.
the best resultsare highlighted in boldface..metric.
dataset.
nl.
nb+em.
bert.
bert+am vampire.
s2tc-bdd.
micro-f1.
yelp.
ag news.
yahoo.
ag news.
macro-f1.
yelp.
yahoo.
average rank.
1001,00010,0001001,00010,0001001,00010,000.
1001,00010,0001001,00010,0001001,00010,000.
0.8340.8550.8740.3000.3550.4040.5290.6240.659.
0.8330.8550.8730.2500.3290.3970.4890.6160.653.
6.2.
0.8390.8780.9050.3440.5380.5830.5640.6760.713.
0.8400.8780.9050.3240.5320.5860.5500.6710.708.
3.6.
0.8560.8790.9010.3990.5440.5740.5890.6790.706.
0.8560.8790.9000.3710.5350.5620.5730.6720.695.
3.4.
0.7050.8330.8760.2270.4760.5510.3890.5470.644.
0.6980.8330.8760.1440.4760.5530.3560.5450.644.
6.7.vat.
0.8680.8860.8980.2440.5510.5660.5340.6850.701.
0.8670.8860.8970.1970.5480.5690.5420.6750.697.
3.8.uda.
0.8550.8830.9060.3870.5540.5800.5760.6720.707.
0.8550.8830.9060.3570.5500.5760.5670.6660.704.
3.0.
0.8720.8890.9070.4170.5520.5830.6180.6870.713.
0.8720.8890.9070.4030.5500.5860.5950.6800.709.
1.1.of s2tc-bdd are even about 0.17, 0.26 and0.24 higher than vampire on the datasets ofag news, yelp and yahoo, respectively.
sec-ond, when labeled texts are very scarce (i.e.,when nl = 100), s2tc-bdd performs betterthan other self-training baseline methods (i.e.,nb+em, bert+am, vat and uda) on alldatasets, e.g., for micro-f1 about 0.08 higherthan vat on yahoo.
otherwise, when labeledtexts are large, s2tc-bdd can also achievethe competitive performance, even performbetter across all datasets..• comparing s2tc-bdd against bert+amand bert: our s2tc-bdd method consis-tently outperforms bert+am and bertacross all datasets and metrics.
for example,when nl = 100 the micro-f1 scores of s2tc-bdd beat those of bert+am by 0.01 ∼ 0.03and those of bert by 0.03 ∼ 0.05 across alldatasets.
that is because s2tc-bdd employsboth labeled and unlabeled texts for trainingand can predict more accurate pseudo-labelsof unlabeled texts than bert+am, beneﬁt-ing for the classiﬁer training.
this result isexpected since s2tc-bdd performs a gaus-sian linear transformation to balance the labelangel variances, so as to eliminate the mar-gin bias, leading to more accurate predictedpseudo-labels of unlabeled texts.
besides,these results empirically prove that unlabeled.
texts are beneﬁcial to the classiﬁcation perfor-mance..• comparing bert based methods againstnb+em and vampire: all bert basedmethods (i.e., bert, bert+am, vat, udaand s2tc-bdd) consistently dominate base-lines based on small models (i.e., nb+em,vampire).
for example, when nl = 10000,the micro-f1 and macro-f1 scores of bertare about 0.03, 0.18 and 0.05 higher than thoseof nb+em on the datasets of ag news, yelpand yahoo, respectively.
the observation isexpected because bert is a bigger model,hence can extract more discriminative repre-sentations of texts than those from the vaemodel used in vampire and tf-idfs used innb+em..4.2.2 varying number of unlabeled texts.
for nb+em, bert+am, vampire, vat, udaand s2tc-bdd, we also perform the experi-ments with 100 labeled texts and varying thenumber of unlabeled texts nu over the set{0, 200, 2000, 20000} for ag news and yelp, and{0, 400, 4000, 40000} for yahoo.
note that vam-pire needs unlabeled texts for pre-training, thuswe omit the experiments for vampire with nu =0. the classiﬁcation results are reported in ta-ble 4. roughly, for all methods the classiﬁcation.
5050table 4: experimental results of micro-f1 and macro-f1 varying the number of unlabeled texts nu..metric.
dataset.
nu.
nb+em.
bert+am vampire.
s2tc-bdd.
02002,00020,00002002,00020,00004004,00040,000.
02002,00020,00002002,00020,00004004,00040,000.micro-f1.
yelp.
ag news.
yahoo.
ag news.
macro-f1.
yelp.
yahoo.
average rank.
0.6680.6960.7520.8340.3170.3070.3020.3000.3120.3180.4420.529.
0.6670.6950.7510.8330.3160.2790.2860.2500.3030.3010.4200.489.
4.8.
0.8440.8550.8560.8560.3810.3850.3930.3990.5810.5820.5840.589.
0.8430.8550.8550.8560.3680.3700.3790.3710.5670.5710.5740.573.
2.2.
–0.3290.4210.705–0.2380.2110.227–0.1620.2210.389.
–0.2190.3410.698–0.1610.1240.144–0.0740.1750.356.
6.0.vat.
0.8460.8500.8700.8680.3410.2990.2940.2440.5570.5190.5230.534.
0.8450.8500.8700.8670.2560.2780.2870.1970.5620.5210.5240.542.
3.4.uda.
0.8390.8440.8530.8550.3440.3970.3790.3870.5640.5080.5590.576.
0.8400.8430.8520.8550.3240.3440.3620.3570.5500.5000.5500.567.
3.4.
0.8440.8570.8630.8720.3950.4030.4170.4170.5900.5930.5980.618.
0.8430.8570.8640.8720.3850.3720.3800.4030.5850.5860.5900.595.
1.2.table 5: classiﬁcation performance on ag news with100 labeled data and 20,000 unlabeled data after remov-ing different parts of s2tc-bdd..4.3 ablation study.
models2 tc-bdd-entropy regularization-bdd-unlabeled texts-all.
micro-f1.
macro-f1.
0.872.
0.8630.8560.8440.839.
0.872.
0.8640.8560.8430.840.performance becomes better as the amount of un-labeled texts increasing.
for instance, the micro-f1 scores of s2tc-bdd on all datasets gain about0.3 improvement as the number of unlabeled textsincreasing.
these results prove the effectivenessof unlabeled texts in riching the limited supervi-sion from scarce labeled texts and improving theclassiﬁcation performance.
besides, an obviousobservation is that the self-training methods (i.e.,nb+em, bert+am, vat, uda and s2tc-bdd)consistently outperform the pre-training method(i.e., vampire), especially when unlabeled textsare fewer.
the possible reason is that the pre-training methods need more unlabeled texts forpre-training while the self-training methods do nothave the requirement..we perform ablation studies by stripping each com-ponent each time to examine the effectiveness ofeach component in s2tc-bdd.
here, we denotebdd as balanced deep representation angular losslbdd in eq.4.
stripping bdd means that we replacethe proposed loss lbdd with the am loss lam ineq.1.
the results are displayed in table 5. over-all, the classiﬁcation performance will drop whenremoving any component of s2tc-bdd, suggest-ing that all parts make contributions to the ﬁnalperformance of s2tc-bdd.
besides, removing un-labeled texts brings the most signiﬁcant drop ofthe performance.
this result is expected becauselabel angle variances approximated only with veryscarce labeled texts will have lower accuracy, result-ing in worse performance.
further, in contrast toentropy regularization, the performance after strip-ping bdd decrease more.
note that the differencebetween the proposed lbdd and lam is whetherconstraining the label angle variances to be bal-anced or not.
this result indicates that the balancedconstraint of label angle variances brings a betterdeep classiﬁer as well as more accurate pseudo-labels for unlabeled texts, especially when labeledtexts are limited, and also empirically prove the.
5051table 6: average per-epoch running time (second, s) ofbert, bert+am and s2tc-bdd..references.
dataset.
ag newsyelpyahoo.
bert.
72.1 s73.4 s74.1 s.bert+am s2 tc-bdd.
71.9 s73.8 s75.1 s.73.3 s73.8 s75.1 s.effectiveness of our balanced label angle variances..4.4 efﬁciency comparison.
to evaluate the efﬁciency of our s2tc-bdd,we perform efﬁciency comparisons over bert,bert+am and s2tc-bdd on all benchmarkdatasets.
to be fair, for all methods and datasets weset the batch sizes of labeled and unlabeled texts to4 and 8 respectively, and iterate 100 epochs, whereeach one consists of 200 inner loops.
the averageper-epoch running time results are shown in ta-ble 6. generally speaking, the per-epoch runningtime of our proposed s2tc-bdd is close to thoseof bert and bert+am.
this result means thatgaussian linear transformation and estimation oflabel angle variances in our s2tc-bdd only intro-duce very few computation costs.
that is expectedsince they merely require very few simple linearoperations, which are very efﬁcient..5 conclusion.
in this paper, we propose a novel self-trainingsstc method, namely s2tc-bdd.
our s2tc-bddaddresses the margin bias problem in sstc bybalancing the label angle variances, i.e., the vari-ance of label angles of texts within the same label.
we estimate the label angle variances with bothlabeled and unlabeled texts during the self-trainingloops.
to constrain the label angle variances tobe balanced, we design several gaussian lineartransformations and incorporate them into a wellestablished am loss.
our s2tc-bdd empiricallyoutperforms the existing sstc baseline methods..acknowledgments.
forwe would like to acknowledge supportthis project from the national natural sciencefoundation of china (nsfc)(no.61876071,no.62006094), the key r&d projects of scienceand technology department of jilin province ofchina (no.20180201003sf, no.20190701031gh)..alan akbik, tanja bergmann, and roland vollgraf.
2019. pooled contextualized embeddings for namedentity recognition.
in conference of the north amer-ican chapter of the association for computationallinguistics: human language technologies, pages724–728..tom b. brown, benjamin mann, nick ryder, melaniesubbiah, jared kaplan, prafulla dhariwal, arvindneelakantan, pranav shyam, girish sastry, amandaaskell, sandhini agarwal, ariel herbert-voss,gretchen krueger, tom henighan, rewon child,aditya ramesh, daniel m. ziegler, jeffrey wu,clemens winter, christopher hesse, mark chen,eric sigler, mateusz litwin, scott gray, benjaminchess, jack clark, christopher berner, sam mc-candlish, alec radford, ilya sutskever, and darioamodei.
2020. language models are few-shot learn-ers.
arxiv preprint arxiv:2005.14165..ming-wei chang, lev-arie ratinov, dan roth, andvivek srikumar.
2008. importance of semantic rep-resentation: dataless classiﬁcation.
in aaai confer-ence on artiﬁcial intelligence, pages 830–835..jiaao chen, jianshu chen, and zhou yu.
2019..in-corporating structured commonsense knowledge instory completion.
in aaai conference on artiﬁcialintelligence, pages 6244–6251..jiaao chen, zichao yang, and diyi yang.
2020. mix-text: linguistically-informed interpolation of hid-den space for semi-supervised text classiﬁcation.
inannual meeting of the association for computa-tional linguistics, pages 2147–2157..kevin clark, minh-thang luong, christopher d. man-ning, and quoc v. le.
2018. semi-supervised se-quence modeling with cross-view training.
in con-ference on empirical methods in natural languageprocessing, pages 1914–1925..andrew m. dai and quoc v. le.
2015..supervised sequence learning.
tion processing systems, pages 3079–3087..semi-in neural informa-.
jacob devlin, ming-wei chang, kenton lee, andkristina toutanova.
2019. bert: pre-training ofdeep bidirectional transformers for language under-in conference of the north americanstanding.
chapter of the association for computational lin-guistics: human language technologies, pages4171–4186..jesper e. van engelen and holger h. hoos.
2020.a survey on semi-supervised learning.
machinelearning, 109(2):373–440..yves grandvalet and yoshua bengio.
2004. semi-supervised learning by entropy minimization.
inneural information processing systems, pages 529–536..5052suchin gururangan, tam dang, dallas card, andnoah a. smith.
2019. variational pretraining forsemi-supervised text classiﬁcation.
in annual meet-ing of the association for computational linguistics,pages 5880–5894..mehdi sajjadi, mehran javanmardi, and tolga tas-dizen.
2016. regularization with stochastic transfor-mations and perturbations for deep semi-supervisedlearning.
in neural information processing systems,pages 1171–1179..hao wang, yitong wang, zheng zhou, xing ji, di-hong gong, jingchao zhou, zhifeng li, and wei liu.
2018. cosface: large margin cosine loss for deepface recognition.
in ieee conference on computervision and pattern recognition, pages 5265–5274..qizhe xie, zihang dai, eduard h. hovy, thang luong,and quoc le.
2020. unsupervised data augmenta-tion for consistency training.
in neural informationprocessing systems..zhilin yang, zihang dai, yiming yang, jaime g. car-bonell, ruslan salakhutdinov, and quoc v. le.
2019.xlnet: generalized autoregressive pretraining forlanguage understanding.
in neural information pro-cessing systems, pages 5753–5763..xiang zhang, junbo jake zhao, and yann lecun.
2015.character-level convolutional networks for text clas-in neural information processing sys-siﬁcation.
tems, pages 649–657..jeremy howard and sebastian ruder.
2018. univer-sal language model ﬁne-tuning for text classiﬁcation.
in annual meeting of the association for computa-tional linguistics, pages 328–339..jialun liu, yifan sun, chuchu han, zhaopeng dou,and wenhui li.
2020. deep representation learn-ing on long-tailed data: a learnable embedding aug-mentation perspective.
in ieee conference on com-puter vision and pattern recognition, pages 2970–2979..takeru miyato, andrew m. dai, and ian j. goodfel-low.
2017. adversarial training methods for semi-supervised text classiﬁcation.
in international con-ference on learning representations..takeru miyato, shin-ichi maeda, masanori koyama,and shin ishii.
2019. virtual adversarial train-ing: a regularization method for supervised andieee transactions pat-semi-supervised learning.
tern analysis and machine intelligence, 41(8):1979–1993..kamal nigam, andrew mccallum, sebastian thrun,and tom m. mitchell.
2000. text classiﬁcation fromlabeled and unlabeled documents using em.
ma-chine learning, 39(2):103–134..fabian pedregosa, ga¨el varoquaux, alexandre gram-fort, vincent michel, bertrand thirion, oliviergrisel, mathieu blondel, peter prettenhofer, ronweiss, vincent dubourg, jake vanderplas, alexan-dre passos, david cournapeau, matthieu brucher,matthieu perrot, and edouard duchesnay.
2011.scikit-learn: machine learning in python.
journalof machine learning research, 12:2825–2830..matthew e. peters, mark neumann, mohit iyyer, mattgardner, christopher clark, kenton lee, and lukezettlemoyer.
2018. deep contextualized word rep-in conference of the north ameri-resentations.
can chapter of the association for computationallinguistics: human language technologies, pages2227–2237..alec radford, karthik narasimhan, tim salimans, andimproving language under-.
ilya sutskever.
2019a.
standing by generative pre-training..alec radford, jeffrey wu, rewon child, david luan,dario amodei, and ilya sutskever.
2019b.
lan-guage models are unsupervised multitask learners..devendra singh sachan, manzil zaheer, and ruslansalakhutdinov.
2019. revisiting lstm networksfor semi-supervised text classiﬁcation via mixed ob-in aaai conference on artiﬁcialjective function.
intelligence, pages 6940–6948..5053