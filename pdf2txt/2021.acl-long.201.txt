layoutlmv2: multi-modal pre-training for visually-richdocument understanding.
yang xu1∗, yiheng xu2 ∗, tengchao lv2 ∗, lei cui2, furu wei2, guoxin wang3,yijuan lu3, dinei florencio3, cha zhang3, wanxiang che1, min zhang4, lidong zhou21research center for social computing and information retrieval,harbin institute of technology.
2microsoft research asia.
3microsoft azure ai.
4soochow university.
1{yxu,car}@ir.hit.edu.cn,2{v-yixu,v-telv,lecu,fuwei,lidongz}@microsoft.com,3{guow,yijlu,dinei,chazhang}@microsoft.com 4minzhang@suda.edu.cn.
abstract.
pre-training of text and layout has provedeffective in a variety of visually-rich docu-ment understanding tasks due to its effec-tive model architecture and the advantageof large-scale unlabeled scanned/digital-borndocuments.
we propose layoutlmv2 archi-tecture with new pre-training tasks to modelthe interaction among text, layout, and imagein a single multi-modal framework.
specif-ically, with a two-stream multi-modal trans-former encoder, layoutlmv2 uses not onlythe existing masked visual-language model-ing task but also the new text-image align-ment and text-image matching tasks, whichmake it better capture the cross-modality in-teraction in the pre-training stage.
meanwhile,it also integrates a spatial-aware self-attentionmechanism into the transformer architectureso that the model can fully understand therelative positional relationship among differ-entexperiment results showthat layoutlmv2 outperforms layoutlm bya large margin and achieves new state-of-the-art results on a wide variety of down-stream visually-rich document understandingtasks, including funsd (0.7895 → 0.8420),cord (0.9493 → 0.9601), sroie (0.9524→ 0.9781), kleister-nda (0.8340 → 0.8520),rvl-cdip (0.9443 → 0.9564), and docvqa(0.7295 → 0.8672).
we made our model andcode publicly available at https://aka.ms/layoutlmv2..text blocks..1.introduction.
visually-rich document understanding (vrdu)aims to analyze scanned/digital-born business doc-uments (images of invoices, forms in pdf format,etc.)
where structured information can be automat-ically extracted and organized for many business.
∗equal contributions during internship at msra.
applications.
distinct from conventional informa-tion extraction tasks, the vrdu task relies on notonly textual information but also visual and lay-out information that is vital for visually-rich docu-ments.
different types of documents indicate thatthe text ﬁelds of interest located at different posi-tions within the document, which is often deter-mined by the style and format of each type as wellas the document content.
therefore, to accuratelyrecognize the text ﬁelds of interest, it is inevitableto take advantage of the cross-modality nature ofvisually-rich documents, where the textual, visual,and layout information should be jointly modeledand learned end-to-end in a single framework..the recent progress of vrdu lies primarilyin two directions.
the ﬁrst direction is usuallybuilt on the shallow fusion between textual andvisual/layout/style information (yang et al., 2017;liu et al., 2019; sarkhel and nandi, 2019; yu et al.,2020; majumder et al., 2020; wei et al., 2020;zhang et al., 2020).
these approaches leveragethe pre-trained nlp and cv models individuallyand combine the information from multiple modali-ties for supervised learning.
although good perfor-mance has been achieved, the domain knowledgeof one document type cannot be easily transferredinto another, so that these models often need tobe re-trained once the document type is changed.
thereby the local invariance in general documentlayout (key-value pairs in a left-right layout, tablesin a grid layout, etc.)
cannot be fully exploited.
tothis end, the second direction relies on the deep fu-sion among textual, visual, and layout informationfrom a great number of unlabeled documents in dif-ferent domains, where pre-training techniques playan important role in learning the cross-modalityinteraction in an end-to-end fashion (lockard et al.,2020; xu et al., 2020).
in this way, the pre-trained.
proceedingsofthe59thannualmeetingoftheassociationforcomputationallinguisticsandthe11thinternationaljointconferenceonnaturallanguageprocessing,pages2579–2591august1–6,2021.©2021associationforcomputationallinguistics2579models absorb cross-modal knowledge from dif-ferent document types, where the local invarianceamong these layouts and styles is preserved.
fur-thermore, when the model needs to be transferredinto another domain with different document for-mats, only a few labeled samples would be suf-ﬁcient to ﬁne-tune the generic model in order toachieve state-of-the-art accuracy.
therefore, theproposed model in this paper follows the seconddirection, and we explore how to further improvethe pre-training strategies for the vrdu tasks..in this paper, we present an improved versionof layoutlm (xu et al., 2020), aka layoutlmv2.
different from the vanilla layoutlm model wherevisual embeddings are combined in the ﬁne-tuningstage, we integrate the visual information in thepre-training stage in layoutlmv2 by taking ad-vantage of the transformer architecture to learnthe cross-modality interaction between visual andtextual information.
in addition, inspired by the1-d relative position representations (shaw et al.,2018; raffel et al., 2020; bao et al., 2020), we pro-pose the spatial-aware self-attention mechanism forlayoutlmv2, which involves a 2-d relative posi-tion representation for token pairs.
different fromthe absolute 2-d position embeddings that lay-outlm uses to model the page layout, the relativeposition embeddings explicitly provide a broaderview for the contextual spatial modeling.
for thepre-training strategies, we use two new training ob-jectives for layoutlmv2 in addition to the maskedvisual-language modeling.
the ﬁrst is the proposedtext-image alignment strategy, which aligns the textlines and the corresponding image regions.
the sec-ond is the text-image matching strategy popular inprevious vision-language pre-training models (tanand bansal, 2019; lu et al., 2019; su et al., 2020;chen et al., 2020; sun et al., 2019), where themodel learns whether the document image and tex-tual content are correlated..we select six publicly available benchmarkdatasets as the downstream tasks to evaluate the per-formance of the pre-trained layoutlmv2 model,which are the funsd dataset (jaume et al., 2019)for form understanding, the cord dataset (parket al., 2019) and the sroie dataset (huang et al.,2019) for receipt understanding, the kleister-ndadataset (grali´nski et al., 2020) for long docu-ment understanding with a complex layout, thervl-cdip dataset (harley et al., 2015) for doc-ument image classiﬁcation, and the docvqa.
dataset (mathew et al., 2021) for visual question an-swering on document images.
experiment resultsshow that the layoutlmv2 model signiﬁcantly out-performs strong baselines, including the vanillalayoutlm, and achieves new state-of-the-art re-sults in all of these tasks..the contributions of this paper are summarized.
as follows:.
• we propose a multi-modal transformer modelto integrate the document text, layout, andvisual information in the pre-training stage,which learns the cross-modal interaction end-to-end in a single framework.
meanwhile,a spatial-aware self-attention mechanism isintegrated into the transformer architecture..• in addition to the masked visual-languagemodel, we add text-image alignment and text-image matching as the new pre-training strate-gies to enforce the alignment among differentmodalities..• layoutlmv2 signiﬁcantly outperforms andachieves new sota results not only on theconventional vrdu tasks but also on the vqatask for document images, which demon-strates the great potential for the multi-modalpre-training for vrdu..2 approach.
in this section, we will introduce the model archi-tecture and the multi-modal pre-training tasks oflayoutlmv2, which is illustrated in figure 1..2.1 model architecture.
we build a multi-modal transformer architectureas the backbone of layoutlmv2, which takes text,visual, and layout information as input to estab-lish deep cross-modal interactions.
we also intro-duce a spatial-aware self-attention mechanism tothe model architecture for better modeling the doc-ument layout.
detailed descriptions of the modelare as follows..text embedding following the common prac-tice, we use wordpiece (wu et al., 2016) to tok-enize the ocr text sequence and assign each tokento a certain segment si ∈ {[a], [b]}.
then, weadd [cls] at the beginning of the sequence and[sep] at the end of each text segment.
extra[pad] tokens are appended to the end so that the.
2580figure 1: an illustration of the model architecture and pre-training strategies for layoutlmv2.
ﬁnal sequence’s length is exactly the maximum se-quence length l. the ﬁnal text embedding is thesum of three embeddings.
token embedding rep-resents the token itself, 1d positional embeddingrepresents the token index, and segment embed-ding is used to distinguish different text segments.
formally, we have the i-th (0 ≤ i < l) text em-bedding.
ti = tokemb(wi)+posemb1d(i)+segemb(si).
visual embedding although all information weneed is contained in the page image, the modelhas difﬁculty capturing detailed features in a sin-gle information-rich representation of the entirepage.
therefore, we leverage the output featuremap of a cnn-based visual encoder, which con-verts the page image to a ﬁxed-length sequence.
we use resnext-fpn (xie et al., 2017; lin et al.,2017) architecture as the backbone of the visualencoder, whose parameters can be updated throughbackpropagation..given a document page image i, it is resized to224 × 224 then fed into the visual backbone.
afterthat, the output feature map is average-pooled to a.ﬁxed size with the width being w and height beingh. next, it is ﬂattened into a visual embeddingsequence of length w ×h.
the sequence is namedvistokemb(i).
a linear projection layer is thenapplied to each visual token embedding to unifythe dimensionality with the text embeddings.
sincethe cnn-based visual backbone cannot capture thepositional information, we also add a 1d positionalembedding to these visual token embeddings.
the1d positional embedding is shared with the textembedding layer.
for the segment embedding, weattach all visual tokens to the visual segment [c].
the i-th (0 ≤ i < w h) visual embedding can berepresented as.
vi = proj(cid:0)vistokemb(i)i.
(cid:1).
+posemb1d(i)+segemb([c]).
layout embedding the layout embedding layeris for embedding the spatial layout informationrepresented by axis-aligned token bounding boxesfrom the ocr results, in which box width andheight together with corner coordinates are iden-tiﬁed.
following the vanilla layoutlm, we nor-malize and discretize all coordinates to integers in.
2581transformer layerswith spatial-aware self-attention mechanism#3 text-image matching#2 text-image alignmentpre-trainingobjectivesvisual/text tokenrepresentations[sep]v1v2v3v4[cls]matchedt1coveredt3coveredt5notcoveredt6notcoveredt7notcoveredt4t2#1 masked visual-language modeling1d positionembeddings8012301356742v2v1v3v4document page with covered ocr linesdocument pageocr/pdf parservisual encoderv1v2v3v4feature mapsocr linesline 1 (covered):line 2 (not covered):t1      [mask]      t3[mask]      t5      t6      t7visual/text tokenembeddings2d positionembeddings[sep]boxpadv1boxv1v2boxv2v3boxv3v4boxv4[cls]boxpadt1boxt1t3boxt3t5boxt5t6boxt6t7boxt7boxt4boxt2linecoveringtokenmasking[mask][mask][mask][mask]segmentembeddingsaccccaaaaaaaathe range [0, 1000], and use two embedding lay-ers to embed x-axis features and y-axis featuresseparately.
given the normalized bounding boxof the i-th (0 ≤ i < w h + l) text/visual tokenboxi = (xmin, xmax, ymin, ymax, width, height),the layout embedding layer concatenates six bound-ing box features to construct a token-level 2d posi-tional embedding, aka the layout embedding.
li = concat(cid:0)posemb2dx(xmin, xmax, width),posemb2dy(ymin, ymax, height)(cid:1).
note that cnns perform local transformation,thus the visual token embeddings can be mappedback to image regions one by one with neitheroverlap nor omission.
when calculating bound-ing boxes, the visual tokens can be treated asevenly divided grids.
an empty bounding boxboxpad = (0, 0, 0, 0, 0, 0) is attached to specialtokens [cls], [sep] and [pad]..multi-modal encoder with spatial-aware self-attention mechanism the encoder concate-nates visual embeddings {v0, ..., vw h−1} and textembeddings {t0, ..., tl−1} to a uniﬁed sequenceand fuses spatial information by adding the layoutembeddings to get the i-th (0 ≤ i < w h + l)ﬁrst layer input.
x(0)i = xi + li, wherex = {v0, ..., vw h−1, t0, ..., tl−1}.
following the architecture of transformer, webuild our multi-modal encoder with a stack ofmulti-head self-attention layers followed by a feed-forward network.
however,the original self-attention mechanism can only implicitly capturethe relationship between the input tokens withthe absolute position hints.
in order to efﬁcientlymodel local invariance in the document layout, it isnecessary to insert relative position information ex-plicitly.
therefore, we introduce the spatial-awareself-attention mechanism into the self-attention lay-ers.
for simplicity, the following description is fora single head in a single self-attention layer withhidden size of dhead and projection matrics wq,wk, wv .
the original self-attention mechanismcaptures the correlation between query xi and keyxj by projecting the two vectors and calculatingthe attention score.
αij =.
√.
1dhead.
(cid:0)xiwq(cid:1) (cid:0)xjwk(cid:1)t.considering the large range of positions, wemodel the semantic relative position and spatialrelative position as bias terms to prevent addingtoo many parameters.
similar practice has beenshown effective on text-only transformer architec-tures (raffel et al., 2020; bao et al., 2020).
letb(1d), b(2dx) and b(2dy) denote the learnable 1dand 2d relative position biases respectively.
the bi-ases are different among attention heads but sharedin all encoder layers.
assuming (xi, yi) anchorsthe top left corner coordinates of the i-th boundingbox, we obtain the spatial-aware attention score.
ij = αij + b(1d)α(cid:48).
j−i + b(2dx)xj −xi.
+ b(2dy)yj −yi.
finally, the output vectors are represented as theweighted average of all the projected value vectorswith respect to normalized spatial-aware attentionscores.
hi =.
(cid:16).
(cid:17).
α(cid:48)expijk exp (cid:0)α(cid:48).
ik.
(cid:80).
(cid:88).
j.
(cid:1) xjwv.
2.2 pre-training tasks.
masked visual-language modeling similar tothe vanilla layoutlm, we use the masked visual-language modeling (mvlm) to make the modellearn better in the language side with the cross-modality clues.
we randomly mask some text to-kens and ask the model to recover the masked to-kens.
meanwhile, the layout information remainsunchanged, which means the model knows eachmasked token’s location on the page.
the outputrepresentations of masked tokens from the encoderare fed into a classiﬁer over the whole vocabulary,driven by a cross-entropy loss.
to avoid visual clueleakage, we mask image regions corresponding tomasked tokens on the raw page image input beforefeeding it into the visual encoder..text-image alignment to help the model learnthe spatial location correspondence between imageand coordinates of bounding boxes, we proposethe text-image alignment (tia) as a ﬁne-grainedcross-modality alignment task.
in the tia task,some tokens lines are randomly selected, and theirimage regions are covered on the document image.
we call this operation covering to avoid confusionwith the masking operation in mvlm.
during pre-training, a classiﬁcation layer is built above theencoder outputs.
this layer predicts a label foreach text token depending on whether it is covered,.
2582i.e., [covered] or [not covered], and com-putes the binary cross-entropy loss.
consideringthe input image’s resolution is limited, and somedocument elements like signs and bars in a ﬁg-ure may look like covered text regions, the task ofﬁnding a word-sized covered image region can benoisy.
thus, the covering operation is performedat the line-level.
when mvlm and tia are per-formed simultaneously, tia losses of the tokensmasked in mvlm are not taken into account.
thisprevents the model from learning the useless butstraightforward correspondence from [mask] to[covered]..text-image matching furthermore, a coarse-grained cross-modality alignment task, text-imagematching (tim) is applied to help the model learnthe correspondence between document image andtextual content.
we feed the output representationat [cls] into a classiﬁer to predict whether theimage and text are from the same document page.
regular inputs are positive samples.
to constructa negative sample, an image is either replaced bya page image from another document or dropped.
to prevent the model from cheating by ﬁnding taskfeatures, we perform the same masking and cover-ing operations to images in negative samples.
thetia target labels are all set to [covered] in neg-ative samples.
we apply the binary cross-entropyloss in the optimization process..3 experiments.
3.1 data.
in order to pre-train and evaluate layoutlmv2models, we select datasets in a wide range from thevisually-rich document understanding area.
fol-lowing layoutlm, we use iit-cdip test collec-tion (lewis et al., 2006) as the pre-training dataset.
six datasets are used as down-stream tasks.
thefunsd (jaume et al., 2019), cord (park et al.,2019), sroie (huang et al., 2019) and kleister-nda (grali´nski et al., 2020) datasets deﬁne en-tity extraction tasks that aim to extract the valueof a set of pre-deﬁned keys, which we formalizeas a sequential labeling task.
rvl-cdip (harleyet al., 2015) is for document image classiﬁcation.
docvqa (mathew et al., 2021), as the name sug-gests, is a dataset for visual question answering ondocument images.
statistics of datasets are shownin table 1. refer to the appendix for details..dataset.
# of keys orcategories.
# of examples(train/dev/test).
–iit-cdip4funsd30cordsroie4kleister-nda 4rvl-cdipdocvqa.
16–.
11m/0/0149/0/50800/100/100626/0/347254/83/203320k/4k/4k39k/5k/5k.
table 1: statistics of datasets.
3.2 settings.
following the typical pre-training and ﬁne-tuningstrategy, we update all parameters including thevisual encoder layers, and train whole models end-to-end for all the settings.
training details can befound in the appendix..two.
encoder.
different.
pre-training layoutlmv2 wetrain lay-outlmv2 models withpa-rameter sizes.
we use a 12-layer 12-headtransformerand set hidden sized = 768 in layoutlmv2base.
while in thelayoutlmv2large, the encoder has 24 trans-former layers with 16 heads and d = 1024. visualbackbones in the two models are based on thesame resnext101-fpn architecture.
the numbersof parameters are 200m and 426m approximatelyfor layoutlmv2base and layoutlmv2large,respectively..for the encoder along with the text embeddinglayer, layoutlmv2 uses the same architecture asunilmv2 (bao et al., 2020), thus it is initializedfrom unilmv2.
for the resnext-fpn part in thevisual embedding layer, the backbone of a mask-rcnn (he et al., 2017) model trained on pub-laynet (zhong et al., 2019) is leveraged.1 therest of the parameters in the model are randomlyinitialized..during pre-training, we sample pages from theiit-cdip dataset and select a random sliding win-dow of the text sequence if the sample is too long.
we set the maximum sequence length l = 512and assign all text tokens to the segment [a].
theoutput shape of the average pooling layer is set tow = h = 7, so that it transforms the feature mapinto 49 visual tokens.
in mvlm, 15% text tokensare masked among which 80% are replaced by aspecial token [mask], 10% are replaced by a ran-dom token sampled from the whole vocabulary, and.
1“maskrcnn resnext101 32x8d fpn 3x” setting in.
https://github.com/hpanwar08/detectron2.
2583model.
funsd cord sroie kleister-nda.
bertbaseunilmv2basebertlargeunilmv2large.
layoutlmbaselayoutlmlargelayoutlmv2baselayoutlmv2large.
bros (hong et al., 2021)spade (hwang et al., 2020)pick (yu et al., 2020)trie (zhang et al., 2020)top-1 on sroie leaderboard (until 2020-12-24)robertabase in (grali´nski et al., 2020).
0.60260.66480.65630.7072.
0.78660.7895.
0.82760.8420.
0.8121–––––.
0.89680.90920.90250.9205.
0.94720.9493.
0.94950.9601.
0.95360.9150––––.
0.90990.94590.92000.9488.
0.94380.9524.
0.96250.9781.
0.9548–0.96120.96180.9767–.
0.77900.79500.79100.8180.
0.82700.8340.
0.83300.8520.
–––––0.7930.table 2: entity-level f1 scores of the four entity extraction tasks: funsd, cord, sroie and kleister-nda.
detailed per-task results are in the appendix..10% remains the same.
in tia, 15% of the linesare covered.
in tim, 15% images are replaced, and5% are dropped..parameter settings..3.3 results.
fine-tuning layoutlmv2 we use the [cls]output along with pooled visual token representa-tions as global features in the document-level classi-ﬁcation task rvl-cdip.
for the extractive questionanswering task docvqa and the other four entityextraction tasks, we follow common practice like(devlin et al., 2019) and build task speciﬁed headlayers over the text part of layoutlmv2 outputs..in the docvqa paper, experiment results showthat the bert model ﬁne-tuned on the squaddataset (rajpurkar et al., 2016) outperforms theoriginal bert model.
inspired by this fact, we addan extra setting, which is that we ﬁrst ﬁne-tune lay-outlmv2 on a question generation (qg) datasetfollowed by the docvqa dataset.
the qg datasetcontains almost one million question-answer pairsgenerated by a generation model trained on thesquad dataset..baselines we select three baseline models inthe experiments to compare layoutlmv2 withthe text-only pre-trained models as well as thevanilla layoutlm model.
speciﬁcally, we com-pare layoutlmv2 with bert (devlin et al., 2019),unilmv2 (bao et al., 2020), and layoutlm (xuet al., 2020) for all the experiment settings.
weuse the publicly available pytorch models forbert (wolf et al., 2020) and layoutlm, anduse our in-house implementation for the unilmv2models.
for each baseline approach, experimentsare conducted using both the base and large.
entity extraction tasks table 2 shows themodel accuracy on the four datasets funsd,cord, sroie, and kleister-nda, which we re-gard as sequential labeling tasks evaluated usingentity-level f1 score.
we report the evaluationresults of kleister-nda on the validation set be-cause the ground-truth labels and the submissionwebsite for the test set are not available right now.
for text-only models, the unilmv2 models out-perform the bert models by a large margin interms of the base and large settings.
fortext+layout models, the layoutlm family, espe-cially the layoutlmv2 models, brings signiﬁcantperformance improvement over the text-only base-lines.
compared to the baselines, the layoutlmv2models are superior to the spade (hwang et al.,2020) decoder method, as well as the text+layoutpre-training approach bros (hong et al., 2021)that is built on the spade decoder, which demon-strates the effectiveness of our modeling approach.
moreover, with the same modal information, ourlayoutlmv2 models also outperform existingmulti-modal approaches pick (yu et al., 2020),trie (zhang et al., 2020) and the previous top-1 method on the leaderboard,2 conﬁrming theeffectiveness of our pre-training for text,lay-out, and visual information.
the best perfor-mance on all the four datasets is achieved by.
2unpublished results,.
the leaderboard is availableat https://rrc.cvc.uab.es/?ch=13&com=evaluation&task=3.
2584model.
accuracy.
model.
fine-tuning set anls.
bertbaseunilmv2basebertlargeunilmv2large.
layoutlmbase (w/ image)layoutlmlarge (w/ image)layoutlmv2baselayoutlmv2large.
vgg-16 (afzal et al., 2017)single model (das et al., 2018)ensemble (das et al., 2018)inceptionresnetv2 (szegedy et al., 2017)laddernet (sarkhel and nandi, 2019)single model (dauphinee et al., 2019)ensemble (dauphinee et al., 2019).
89.81%90.06%89.92%90.20%.
94.42%94.43%.
95.25%95.64%.
90.97%91.11%92.21%92.63%92.77%93.03%93.07%.
bertbaseunilmv2basebertlargeunilmv2large.
layoutlmbaselayoutlmlargelayoutlmv2baselayoutlmv2largelayoutlmv2largelayoutlmv2large + qg.
traintraintraintrain.
traintrain.
traintrain.
train + devtrain + dev.
top-1 (30 models ensemble)on docvqa leaderboard(until 2020-12-24).
-.
0.63540.71340.67680.7709.
0.69790.7259.
0.78080.8348.
0.85290.8672.
0.8506.table 3: classiﬁcation accuracy on the rvl-cdipdataset.
table 4: anls score on the docvqa dataset, “qg”denotes the data augmentation with the question gener-ation dataset..the layoutlmv2large, which illustrates that themulti-modal pre-training in layoutlmv2 learnsbetter from the interactions from different modali-ties, thereby leading to the new sota on variousdocument understanding tasks..rvl-cdip table 3 shows the classiﬁcation ac-curacy on the rvl-cdip dataset, including text-only pre-trained models, the layoutlm family aswell as several image-based baseline models.
asshown in the table, both the text and visual in-formation are important to the document imageclassiﬁcation task because document images aretext-intensive and represented by a variety of lay-outs and formats.
therefore, we observed that thelayoutlm family outperforms those text-only orimage-only models as it leverages the multi-modalinformation within the documents.
speciﬁcally, thelayoutlmv2large model signiﬁcantly improvesthe classiﬁcation accuracy by more than 1.2% pointover the previous sota results, which achieves anaccuracy of 95.64%.
this also veriﬁes that thepre-trained layoutlmv2 model beneﬁts not onlythe information extraction tasks in document under-standing but also the document image classiﬁcationtask through effective multi-model training..docvqa table 4 lists the average normalizedlevenshtein similarity (anls) scores on thedocvqa dataset of text-only baselines, layoutlmfamily models, and the previous top-1 on theleaderboard.
with multi-modal pre-training, lay-outlmv2 models outperform layoutlm modelsand text-only baselines by a large margin when ﬁne-.
tuned on the train set.
by using all data (train + dev)as the ﬁne-tuning dataset, the layoutlmv2largesingle model outperforms the previous top-1 on theleaderboard which ensembles 30 models.3 underthe setting of ﬁne-tuning layoutlmv2large on aquestion generation dataset (qg) and the docvqadataset successively, the single model performanceincreases by more than 1.6% anls and achievesthe new sota..3.4 ablation studies.
to fully understand the underlying impact of dif-ferent components, we conduct an ablation studyto explore the effect of visual information, the pre-training tasks, spatial-aware self-attention mech-anism, as well as different text-side initializationmodels.
table 5 shows model performance on thedocvqa validation set.
under all the settings, wepre-train the models using all iit-cdip data for oneepoch.
the hyper-parameters are the same as thoseused to pre-train layoutlmv2base in section 3.2.
“layoutlm” denotes the vanilla layoutlm archi-tecture in (xu et al., 2020), which can be regardedas a layoutlmv2 architecture without visual mod-ule and spatial-aware self-attention mechanism.
“x101-fpn” denotes the resnext101-fpn visualbackbone described in section 3.2..we ﬁrst evaluate the effect of introducing vi-sual information.
from #1 to #2a, we add thevisual module without changing the pre-trainingstrategy, where results show that layoutlmv2 pre-.
3unpublished results,.
the leaderboard is availableat https://rrc.cvc.uab.es/?ch=17&com=evaluation&task=1.
2585# model architecture.
initialization.
sasam mvlm tia tim anls.
1.
2a2b2c2d.
3.
4.layoutlmbaselayoutlmv2baselayoutlmv2baselayoutlmv2baselayoutlmv2baselayoutlmv2baselayoutlmv2base.
bertbase.
bertbase + x101-fpnbertbase + x101-fpnbertbase + x101-fpnbertbase + x101-fpn.
bertbase + x101-fpn.
unilmv2base + x101-fpn.
(cid:88).
(cid:88).
(cid:88).
(cid:88)(cid:88)(cid:88)(cid:88).
(cid:88).
(cid:88).
0.6841.
0.69150.70610.69550.7124.
0.7217.
0.7421.
(cid:88).
(cid:88).
(cid:88).
(cid:88).
(cid:88)(cid:88).
(cid:88).
(cid:88).
table 5: ablation study on the docvqa dataset, where anls scores on the validation set are reported.
“sasam”means the spatial-aware self-attention mechanism.
“mvlm”, “tia” and “tim” are the three pre-training tasks.
all the models are trained using the whole pre-training dataset for one epoch with the base model size..trained with only mvlm can leverage visual in-formation effectively.
then, we compare the twocross-modality alignment pre-training tasks tiaand tim.
according to the four results in #2, bothtasks improve the model performance substantially,and the proposed tia beneﬁts the model more thanthe commonly used tim.
using both tasks togetheris more effective than using either one alone.
ac-cording to this observation, we keep all the threepre-training tasks and introduce the spatial-awareself-attention mechanism (sasam) to the modelarchitecture.
compare the results #2d and #3, theproposed sasam can further improve the modelaccuracy.
finally, in settings #3 and #4, we changethe text-side initialization checkpoint from bertto unilmv2, and conﬁrm that layoutlmv2 bene-ﬁts from the better initialization..4 related work.
in recent years, pre-training techniques have be-come popular in both nlp and cv areas, and havealso been leveraged in the vrdu tasks..devlin et al.
(2019) introduced a new languagerepresentation model called bert, which is de-signed to pre-train deep bidirectional representa-tions from the unlabeled text by jointly condition-ing on both left and right context in all layers.
baoet al.
(2020) propose to pre-train a uniﬁed languagemodel for both autoencoding and partially autore-gressive language modeling tasks using a noveltraining procedure, referred to as a pseudo-maskedlanguage model.
our multi-modal transformerarchitecture and the mvlm pre-training strategyextend transformer and mlm used in these workto leverage visual information..lu et al.
(2019) proposed vilbert for learningtask-agnostic joint representations of image con-tent and natural language by extending the popular.
bert architecture to a multi-modal two-streammodel.
su et al.
(2020) proposed vl-bert thatadopts the transformer model as the backbone,and extends it to take both visual and linguisticembedded features as input.
different from thesevision-language pre-training approaches, the visualpart of layoutlmv2 directly uses the feature mapinstead of pooled roi features, and beneﬁts fromthe new tia pre-training task..xu et al.
(2020) proposed layoutlm to jointlymodel interactions between text and layout infor-mation across scanned document images, beneﬁt-ing a great number of real-world document imageunderstanding tasks such as information extractionfrom scanned documents.
this work is a naturalextension of the vanilla layoutlm, which takes ad-vantage of textual, layout, and visual informationin a single multi-modal pre-training framework..5 conclusion.
in this paper, we present a multi-modal pre-trainingapproach for visually-rich document understand-ing tasks, aka layoutlmv2.
distinct from existingmethods for vrdu, the layoutlmv2 model notonly considers the text and layout information butalso integrates the image information in the pre-training stage with a single multi-modal framework.
meanwhile, the spatial-aware self-attention mecha-nism is integrated into the transformer architectureto capture the relative relationship among differentbounding boxes.
furthermore, new pre-training ob-jectives are also leveraged to enforce the learningof cross-modal interaction among different modali-ties.
experiment results on 6 different vrdu taskshave illustrated that the pre-trained layoutlmv2model has substantially outperformed the sotabaselines in the document intelligence area, whichgreatly beneﬁts a number of real-world document.
2586understanding tasks..for future research, we will further explore thenetwork architecture as well as the pre-trainingstrategies for the layoutlm family.
meanwhile,we will also investigate the language expansion tomake the multi-lingual layoutlmv2 model avail-able for different languages, especially the non-english areas around the world..acknowledgments.
filip grali´nski,.
tomasz.
stanisławek,.
annawr´oblewska, dawid lipi´nski, agnieszka kaliska,paulina rosalska, bartosz topolski, and prze-mysław biecek.
2020. kleister: a novel task forinformation extraction involving long documentswith complex layout..adam w harley, alex ufkes, and konstantinos g der-panis.
2015. evaluation of deep convolutional netsfor document image classiﬁcation and retrieval.
ininternational conference on document analysis andrecognition (icdar)..this work was supported by the national key r&dprogram of china via grant 2020aaa0106501 andthe national natural science foundation of china(nsfc) via grant 61976072 and 61772153..kaiming he, georgia gkioxari, piotr doll´ar, andross b. girshick.
2017. mask r-cnn.
in ieee in-ternational conference on computer vision, iccv2017, venice, italy, october 22-29, 2017, pages2980–2988.
ieee computer society..references.
muhammad zeshan afzal, andreas k¨olsch, sherazahmed, and marcus liwicki.
2017. cutting the er-ror by half: investigation of very deep cnn and ad-vanced training strategies for document image classi-ﬁcation.
2017 14th iapr international conferenceon document analysis and recognition (icdar),01:883–888..hangbo bao, li dong, furu wei, wenhui wang,nan yang, xiaodong liu, yu wang, jianfeng gao,songhao piao, ming zhou, and hsiao-wuen hon.
2020. unilmv2: pseudo-masked language modelsfor uniﬁed language model pre-training.
in proceed-ings of the 37th international conference on ma-chine learning, icml 2020, 13-18 july 2020, vir-tual event, volume 119 of proceedings of machinelearning research, pages 642–652.
pmlr..yen-chun chen, linjie li, licheng yu, ahmed elkholy, faisal ahmed, zhe gan, yu cheng, andjingjing liu.
2020. uniter: universal image-textrepresentation learning.
in eccv..arindam das, saikat roy, and ujjwal bhattacharya.
2018. document image classiﬁcation with intra-domain transfer learning and stacked generaliza-tion of deep convolutional neural networks.
201824th international conference on pattern recogni-tion (icpr), pages 3180–3185..tyler dauphinee, nikunj patel, and mohammad mehdirashidi.
2019. modular multimodal architecture fordocument classiﬁcation.
arxiv, abs/1912.04376..jacob devlin, ming-wei chang, kenton lee, andkristina toutanova.
2019. bert: pre-training ofdeep bidirectional transformers for language under-in proceedings of the 2019 conferencestanding.
of the north american chapter of the associationfor computational linguistics: human languagetechnologies, volume 1 (long and short papers),pages 4171–4186, minneapolis, minnesota.
associ-ation for computational linguistics..teakgyu hong, donghyun kim, mingi ji, wonseokhwang, daehyun nam, and sungrae park.
2021.
{bros}: a pre-trained language model for under-standing texts in document..z. huang, k. chen, j. he, x. bai, d. karatzas, s. lu,and c. v. jawahar.
2019. icdar2019 competition onscanned receipt ocr and information extraction.
in2019 international conference on document analy-sis and recognition (icdar), pages 1516–1520..wonseok hwang, jinyeong yim, seunghyun park, so-hee yang, and minjoon seo.
2020. spatial depen-dency parsing for semi-structured document infor-mation extraction..guillaume jaume, hazim kemal ekenel, and jean-philippe thiran.
2019. funsd: a dataset for formunderstanding in noisy scanned documents.
2019international conference on document analysis andrecognition workshops (icdarw)..diederik p. kingma and jimmy ba.
2015. adam: ain 3rd inter-method for stochastic optimization.
national conference on learning representations,iclr 2015, san diego, ca, usa, may 7-9, 2015,conference track proceedings..d. lewis, g. agam, s. argamon, o. frieder, d. gross-man, and j. heard.
2006. building a test collectionfor complex document information processing.
inproceedings of the 29th annual international acmsigir conference on research and development ininformation retrieval, sigir ’06, page 665–666,new york, ny, usa.
association for computingmachinery..tsung-yi lin, piotr dollar, ross girshick, kaiminghe, bharath hariharan, and serge belongie.
2017.feature networks for object detection.
in proceed-ings of the ieee conference on computer visionand pattern recognition (cvpr)..xiaojing liu, feiyu gao, qiong zhang, and huashazhao.
2019. graph convolution for multimodal in-formation extraction from visually rich documents.
in proceedings of the 2019 conference of the north.
2587american chapter of the association for computa-tional linguistics: human language technologies,volume 2 (industry papers), pages 32–39, minneapo-lis, minnesota.
association for computational lin-guistics..in pro-classiﬁcation of visually rich documents.
ceedings of the twenty-eighth international jointconference on artiﬁcial intelligence, ijcai 2019,macao, china, august 10-16, 2019, pages 3360–3366. ijcai.org..colin lockard, prashant shiralkar, xin luna dong,and hannaneh hajishirzi.
2020. zeroshotceres:zero-shot relation extraction from semi-structuredwebpages.
in proceedings of the 58th annual meet-ing of the association for computational linguistics,pages 8105–8117, online.
association for computa-tional linguistics..ilya loshchilov and frank hutter.
2019. decou-in 7th inter-pled weight decay regularization.
national conference on learning representations,iclr 2019, new orleans, la, usa, may 6-9, 2019.openreview.net..jiasen lu, dhruv batra, devi parikh, and stefanlee.
2019. vilbert: pretraining task-agnostic visi-olinguistic representations for vision-and-languagetasks.
in advances in neural information process-ing systems 32: annual conference on neural infor-mation processing systems 2019, neurips 2019, de-cember 8-14, 2019, vancouver, bc, canada, pages13–23..bodhisattwa prasad majumder, navneet potti, sandeeptata, james bradley wendt, qi zhao, and marc na-jork.
2020. representation learning for informa-in pro-tion extraction from form-like documents.
ceedings of the 58th annual meeting of the asso-ciation for computational linguistics, pages 6495–6504, online.
association for computational lin-guistics..minesh mathew, dimosthenis karatzas, and c.v. jawa-har.
2021. docvqa: a dataset for vqa on docu-ment images.
in proceedings of the ieee/cvf win-ter conference on applications of computer vision(wacv), pages 2200–2209..seunghyun park, seung shin, bado lee, junyeop lee,jaeheung surh, minjoon seo, and hwalsuk lee.
2019. cord: a consolidated receipt dataset for post-ocr parsing..colin raffel, noam shazeer, adam roberts, katherinelee, sharan narang, michael matena, yanqi zhou,wei li, and peter j liu.
2020. exploring the lim-its of transfer learning with a uniﬁed text-to-texttransformer.
journal of machine learning research,21(140):1–67..pranav rajpurkar, jian zhang, konstantin lopyrev, andpercy liang.
2016. squad: 100,000+ questions formachine comprehension of text.
in proceedings ofthe 2016 conference on empirical methods in natu-ral language processing, pages 2383–2392, austin,texas.
association for computational linguistics..ritesh sarkhel and arnab nandi.
2019. deterministicrouting between layout abstractions for multi-scale.
peter shaw, jakob uszkoreit, and ashish vaswani.
2018. self-attention with relative position represen-in proceedings of the 2018 conference oftations.
the north american chapter of the association forcomputational linguistics: human language tech-nologies, volume 2 (short papers), pages 464–468,new orleans, louisiana.
association for computa-tional linguistics..weijie su, xizhou zhu, yue cao, bin li, lewei lu,furu wei, and jifeng dai.
2020. vl-bert: pre-training of generic visual-linguistic representations.
in 8th international conference on learning repre-sentations, iclr 2020, addis ababa, ethiopia, april26-30, 2020. openreview.net..chen sun, austin myers, carl vondrick, kevin mur-phy, and cordelia schmid.
2019. videobert: ajoint model for video and language representationlearning.
in 2019 ieee/cvf international confer-ence on computer vision, iccv 2019, seoul, ko-rea (south), october 27 - november 2, 2019, pages7463–7472.
ieee..christian szegedy, sergey ioffe, vincent vanhoucke,and alexander a. alemi.
2017.inception-v4,inception-resnet and the impact of residual connec-tions on learning.
in proceedings of the thirty-firstaaai conference on artiﬁcial intelligence, febru-ary 4-9, 2017, san francisco, california, usa,pages 4278–4284.
aaai press..hao tan and mohit bansal.
2019. lxmert: learningcross-modality encoder representations from trans-formers.
in proceedings of the 2019 conference onempirical methods in natural language processingand the 9th international joint conference on natu-ral language processing (emnlp-ijcnlp), pages5100–5111, hong kong, china.
association forcomputational linguistics..mengxi wei, yifan he, and qiong zhang.
2020. ro-bust layout-aware ie for visually rich documentsin proceedingswith pre-trained language models.
of the 43rd international acm sigir conference onresearch and development in information retrieval,sigir 2020, virtual event, china, july 25-30, 2020,pages 2367–2376.
acm..thomas wolf, lysandre debut, victor sanh, julienchaumond, clement delangue, anthony moi, pier-ric cistac, tim rault, remi louf, morgan funtow-icz, joe davison, sam shleifer, patrick von platen,clara ma, yacine jernite, julien plu, canwen xu,teven le scao, sylvain gugger, mariama drame,quentin lhoest, and alexander rush.
2020. trans-formers: state-of-the-art natural language process-ing.
in proceedings of the 2020 conference on em-pirical methods in natural language processing:.
2588system demonstrations, pages 38–45, online.
asso-ciation for computational linguistics..yonghui wu, mike schuster, zhifeng chen, quoc vle, mohammad norouzi, wolfgang macherey,maxim krikun, yuan cao, qin gao, klausmacherey, et al.
2016. google’s neural machinetranslation system: bridging the gap between hu-arxiv preprintman and machine translation.
arxiv:1609.08144..saining xie, ross b. girshick, piotr doll´ar, zhuowentu, and kaiming he.
2017. aggregated residualtransformations for deep neural networks.
in 2017ieee conference on computer vision and patternrecognition, cvpr 2017, honolulu, hi, usa, july21-26, 2017, pages 5987–5995.
ieee computer so-ciety..yiheng xu, minghao li, lei cui, shaohan huang,furu wei, and ming zhou.
2020. layoutlm: pre-training of text and layout for document image un-derstanding.
in kdd ’20: the 26th acm sigkddconference on knowledge discovery and data min-ing, virtual event, ca, usa, august 23-27, 2020,pages 1192–1200.
acm..xiao yang, ersin yumer, paul asente, mike kraley,daniel kifer, and c. lee giles.
2017. learningto extract semantic structure from documents usingmultimodal fully convolutional neural networks.
in2017 ieee conference on computer vision and pat-tern recognition, cvpr 2017, honolulu, hi, usa,july 21-26, 2017, pages 4342–4351.
ieee computersociety..wenwen yu, ning lu, xianbiao qi, ping gong, androng xiao.
2020. pick: processing key informationextraction from documents using improved graphlearning-convolutional networks..peng zhang, yunlu xu, zhanzhan cheng, shiliang pu,jing lu, liang qiao, yi niu, and fei wu.
2020. trie:end-to-end text reading and information extractionfor document understanding..xu zhong, jianbin tang, and antonio jimeno yepes.
2019. publaynet: largest dataset ever for documentin 2019 international conferencelayout analysis.
on document analysis and recognition (icdar),pages 1015–1022.
ieee..appendix.
a details of datasets.
introduction to the dataset and task deﬁnitionsalong with the description of required data pro-cessing are presented as follows..pre-training dataset following layoutlm, wepre-train layoutlmv2 on the iit-cdip test col-lection (lewis et al., 2006), which contains over11 million scanned document pages.
we extract.
text and corresponding word-level bounding boxesfrom document page images with the microsoftread api.4.
funsd funsd (jaume et al., 2019) is a datasetfor form understanding in noisy scanned doc-uments.
it contains 199 real, fully annotated,scanned forms where 9,707 semantic entities are an-notated above 31,485 words.
the 199 samples aresplit into 149 for training and 50 for testing.
theofﬁcial ocr annotation is directly used with thelayout information.
the funsd dataset is suitablefor a variety of tasks, where we focus on semanticentity labeling in this paper.
speciﬁcally, the taskis assigning to each word a semantic entity labelfrom a set of four predeﬁned categories: question,answer, header, or other.
the entity-level f1 scoreis used as the evaluation metric..cord we also evaluate our model on the receiptkey information extraction dataset, i.e.
the publicavailable subset of cord (park et al., 2019).
thedataset includes 800 receipts for the training set,100 for the validation set, and 100 for the test set.
aphoto and a list of ocr annotations are equippedfor each receipt.
an roi that encompasses the areaof receipt region is provided along with each photobecause there can be irrelevant things in the back-ground.
we only use the roi as input instead ofthe raw photo.
the dataset deﬁnes 30 ﬁelds under4 categories and the task aims to label each word tothe right ﬁeld.
the evaluation metric is entity-levelf1.
we use the ofﬁcial ocr annotations..sroie the sroie dataset (task 3) (huanget al., 2019) aims to extract information fromscanned receipts.
there are 626 samples for train-ing and 347 samples for testing in the dataset.
thetask is to extract values from each receipt of up tofour predeﬁned keys: company, date, address, ortotal.
the evaluation metric is entity-level f1.
weuse the ofﬁcial ocr annotations and results on thetest set are provided by the ofﬁcial evaluation site..kleister-nda kleister-nda (grali´nski et al.,2020) contains non-disclosure agreements col-lected from the edgar database, including 254documents for training, 83 documents for valida-tion, and 203 documents for testing.
this task isdeﬁned to extract the values of four ﬁxed keys.
we get the entity-level f1 score from the ofﬁcial.
4https://docs.microsoft.com/en-us/azure/cognitive-services/computer-vision/concept-recognizing-text.
2589fine-tuning for visual question answeringwe treat the docvqa as an extractive qa taskand build a token-level classiﬁer on top of the textpart of layoutlmv2 output representations.
ques-tion tokens, context tokens and visual tokens areassigned to segment [a], [b] and [c], respec-tively.
the maximum sequence length is set tol = 384..fine-tuning for document image classiﬁcationthis task depends on high-level visual information,thereby we leverage the image features explicitlyin the ﬁne-tuning stage.
we pool the visual embed-dings into a global pre-encoder feature, and poolthe visual part of layoutlmv2 output representa-tions into a global post-encoder feature.
the preand post-encoder features along with the [cls]output feature are concatenated and fed into theﬁnal classiﬁcation layer..fine-tuning for sequential labeling we for-malize funsd, sroie, cord, and kleister-ndaas the sequential labeling tasks.
to ﬁne-tune lay-outlmv2 models on these tasks, we build a token-level classiﬁcation layer above the text part of theoutput representations to predict the bio tags foreach entity ﬁeld..c detailed experiment results.
tables list per-task detailed results for the four en-tity extraction tasks, with table 6 for funsd, ta-ble 7 for cord, table 8 for sroie, and table 9for kleister-nda..evaluation tools.5 words and bounding boxes areextracted from the raw pdf ﬁle.
we use heuristicsto locate entity spans because the normalized stan-dard answers may not appear in the utterance.
asthe labeled answers are normalized into a canonicalform, we apply post-processing heuristics to con-vert the extracted date information into the “yyyy-mm-dd” format, and company names into theabbreviations such as “llc” and “inc.”..rvl-cdip rvl-cdip (harley et al., 2015) con-sists of 400,000 grayscale images, with 8:1:1 forthe training set, validation set, and test set.
a multi-class single-label classiﬁcation task is deﬁned onrvl-cdip.
the images are categorized into 16classes, with 25,000 images per class.
the evalu-ation metric is the overall classiﬁcation accuracy.
text and layout information is extracted by mi-crosoft ocr..docvqa as a vqa dataset on the document un-derstanding ﬁeld, docvqa (mathew et al., 2021)consists of 50,000 questions deﬁned on over 12,000pages from a variety of documents.
pages are splitinto the training set, validation set, and test set witha ratio of about 8:1:1. the dataset is organized asa set of triples (cid:104)page image, questions, answers(cid:105).
thus, we use microsoft read api to extract textand bounding boxes from images.
heuristics areused to ﬁnd given answers in the extracted text.
the task is evaluated using an edit distance basedmetric anls (aka average normalized levenshteinsimilarity).
given that human performance is about98% anls on the test set, it is reasonable to as-sume that the found ground truth which reachesover 97% anls on training and validation sets isgood enough to train a model.
results on the testset are provided by the ofﬁcial evaluation site..b model training details.
pre-training we pre-train layoutlmv2 modelsusing adam optimizer (kingma and ba, 2015;loshchilov and hutter, 2019), with the learn-ing rate of 2 × 10−5, weight decay of 1 ×10−2,and (β1, β2) = (0.9, 0.999).
the learn-ing rate is linearly warmed up over the ﬁrst 10%steps then linearly decayed.
layoutlmv2base istrained with a batch size of 64 for 5 epochs, andlayoutlmv2large is trained with a batch size of2048 for 20 epochs on the iit-cdip dataset..5https://gitlab.com/filipg/geval.
2590table 6: model accuracy (entity-level precision, recall, f1) on the funsd dataset.
model.
precision recall.
f1.
bertbaseunilmv2basebertlargeunilmv2large.
layoutlmbaselayoutlmlargelayoutlmv2baselayoutlmv2large.
0.54690.63490.61130.6780.
0.75970.7596.
0.80290.8324.
0.67100.69750.70850.7391.
0.81550.8219.
0.85390.8519.
0.60260.66480.65630.7072.
0.78660.7895.
0.82760.8420.bros (hong et al., 2021).
0.8056.
0.8188.
0.8121.model.
precision recall.
f1.
bertbaseunilmv2basebertlargeunilmv2large.
layoutlmbaselayoutlmlargelayoutlmv2baselayoutlmv2large.
spade (hwang et al., 2020)bros (hong et al., 2021).
0.88330.89870.88860.9123.
0.94370.9432.
0.94530.9565.
-0.9558.
0.91070.91980.91680.9289.
0.95080.9554.
0.95390.9637.
-0.9514.
0.89680.90920.90250.9205.
0.94720.9493.
0.94950.9601.
0.91500.9536.table 7: model accuracy (entity-level precision, recall, f1) on the cord dataset.
model.
precision recall.
f1.
bertbaseunilmv2basebertlargeunilmv2large.
layoutlmbaselayoutlmlargelayoutlmv2baselayoutlmv2largelayoutlmv2large (excluding ocr mismatch).
bros (hong et al., 2021)pick (yu et al., 2020)trie (zhang et al., 2020)top-1 on sroie leaderboard (excluding ocr mismatch).
0.90990.94590.92000.9488.
0.94380.9524.
0.96250.96610.9661.
0.96030.9546-0.9647.
0.90990.94590.92000.9488.
0.94380.9524.
0.96250.96610.9781.
0.95480.96120.96180.9767.table 8: model accuracy (entity-level precision, recall, f1) on the sroie dataset (until 2020-12-24).
0.90990.94590.92000.9488.
0.94380.9524.
0.96250.96610.9904.
0.94930.9679-0.9889.f1.
0.7790.7950.7910.818.
0.8270.834.
0.8330.852.model.
bertbaseunilmv2basebertlargeunilmv2large.
layoutlmbaselayoutlmlargelayoutlmv2baselayoutlmv2large.
robertabase in (grali´nski et al., 2020).
0.793.table 9: model accuracy (entity-level f1) on the validation set of the kleister-nda dataset using the ofﬁcialevaluation toolkit.
2591