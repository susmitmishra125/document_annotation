evaluating morphological typologyin zero-shot cross-lingual transfer.
antonio mart´ınez-garc´ıauniversitat de barcelonaao.martinez.garcia@gmail.com.
toni badiauniversitat pompeu fabratbadia@upf.edu.
jeremy barnesuniversity of oslojeremycb@ifi.uio.no.
abstract.
cross-lingual transfer has improved greatlythrough multi-lingual language model pretrain-ing, reducing the need for parallel data andincreasing absolute performance.
however,this progress has also brought to light thedifferences in performance across languages.
speciﬁcally, certain language families and ty-pologies seem to consistently perform worsein these models.
in this paper, we addresswhat effects morphological typology has onzero-shot cross-lingual transfer for two tasks:part-of-speech tagging and sentiment analysis.
we perform experiments on 19 languages fromfour language typologies (fusional, isolating,agglutinative, and introﬂexive) and ﬁnd thattransfer to another morphological type gener-ally implies a higher loss than transfer to an-other language with the same morphologicaltypology.
furthermore, pos tagging is moresensitive to morphological typology than sen-timent analysis and, on this task, models per-form much better on fusional languages thanon the other typologies..1.introduction.
cross-lingual transfer uses available annotated re-sources in a source language to learn a model thatwill transfer to a target language.
earlier work usedmachine translation (mihalcea et al., 2007), paral-lel data (pad´o and lapata, 2009), or delexicalizedmodels (zeman and resnik, 2008; mcdonald et al.,2011; søgaard, 2011) to bridge the gap betweenlanguages.
however, recent improvements (devlinet al., 2019) have reduced the need for parallel data,instead relying on multi-lingual language models,trained on the concatenation of monolingual cor-pora.
fine-tuning these multilingual language mod-els on a task in a source language can lead to strongperformance when applied directly to the target-language task (zero-shot transfer)..this progress has uncovered gaps in perfor-mance, as transfer is generally easier between simi-lar languages, and some language families consis-tently perform worse (artetxe et al., 2020; conneauet al., 2020a).
so far, however, the analysis of thesedifferences has only been anecdotal, rather thancentered as a research question of its own merit.
for these cases, linguistic typology has importantimplications, as it gives us ways to quantify the sim-ilarity of languages along certain variables, such asshared morphological or syntactic features (bender,2013).
while previous work has studied the effectsof morphological typology on language modeling(gerz et al., 2018; cotterell et al., 2018; mielkeet al., 2019), this effect on cross-lingual transferhas not been looked at in detail..in this paper we attempt to answer (rq1) towhat degree morphological typology affects theperformance of state-of-the-art cross-lingual mod-els, (rq2) whether morphological typology has astronger effect than other variables, e.g., the amountof data for pretraining the lm or domain mis-matches between source and target, (rq3) whetherthere is a different effect on a low-level structuraltask (pos tagging) vs. a semantic task (sentimentanalysis)..to answer these questions we experiment withtwo state-of-the-art cross-lingual models: multilin-gual bert and xlm roberta.
we ﬁne-tune themodels for part-of-speech tagging and sentimentanalysis on 19 languages from four morpholog-ically diverse typologies.
our results show thatpos tagging is more sensitive to morphological ty-pology than sentiment analysis and that the modelsperform much better on fusional languages, suchas german, than on the other typologies.
we re-lease the code and data1 in order to reproduce theexperiments and facilitate future work in this area..1code and data available at https://github.com/.
jerbarnes/typology_of_crosslingual..proceedingsofthe59thannualmeetingoftheassociationforcomputationallinguisticsandthe11thinternationaljointconferenceonnaturallanguageprocessing,pages3136–3153august1–6,2021.©2021associationforcomputationallinguistics31362 related work.
cross-lingual transfer has become ubiquitousin recent years, including cross-lingual pos tag-ging (t¨ackstr¨om et al., 2013; huck et al., 2019)and cross-lingual sentiment analysis (mihalceaet al., 2007; balahur and turchi, 2014; barnes andklinger, 2019).
while earlier research focusedon annotation projection (yarowsky et al., 2001;banea et al., 2008) or cross-lingual embeddings(kim et al., 2017; artetxe et al., 2017; barneset al., 2018b), multi-lingual pretraining currentlyleads to state-of-the-art results (devlin et al., 2019;lample and conneau, 2019).
these approachesrely on training transformer-based language mod-els (vaswani et al., 2017) on unlabeled data frommultiple languages, while using careful data selec-tion methods to avoid the over-representation oflarger languages..although these approaches have led to large im-provements on many cross-lingual tasks, it is clearthat the success of zero-shot cross-lingual trans-fer depends on the typological similarity of thesource and target language (conneau et al., 2020b;libovick´y et al., 2020).
pires et al.
(2019) ﬁndpos performance correlates with word order fea-tures taken from the world atlas of languagestructures (wals) database (dryer and haspel-math, 2013).
similarly, morphologically com-plex languages tend to achieve poorer performance(artetxe et al., 2020; conneau et al., 2020a)..similar to this work, lauscher et al.
(2020) per-form zero-shot and few-shot transfer on 20 lan-guages and 5 tasks.
however, the choice of lan-guages does not allow one to answer what is theeffect of morphological typology..the effect of morphological typology on nlptasks is well known (ponti et al., 2019), with severaldedicated workshop series (nicolai et al., 2020;zampieri et al., 2018).
more recently, attention hasturned to larger scale analyses of morphologicaltypology effects on language modeling (gerz et al.,2018; cotterell et al., 2018; mielke et al., 2019)..in contrast to these previous works, we are inter-ested in how morphological typology affects cross-lingual transfer for two supervised tasks, namelypart of speech (pos) tagging and sentiment anal-ysis.
we choose these two tasks as 1) they bothhave data available in typologically diverse lan-guages, and 2) represent a lower-level structuraland higher-level semantic task, respectively.
ourexperimental setup reduces some of the complexity.
of comparing test results across languages, as wecompare relative differences, instead of absolutedifferences.
at the same time, it is necessary totake into account several other variables, i.e., pres-ence of the language in pretraining, the amountof training data, the effect of byte-pair tokeniza-tion, the length of train and test examples, and anydomain mismatches across languages..although it is a simpliﬁcation of the variation inmorphological features (plank, 1999), languageshave traditionally been grouped into four morpho-logical categories, i.e., isolating, fusional, introﬂex-ive, and agglutinative.2 these categories describea language’s tendency to group concepts togetherinto a single word or disperse them into separatewords.
pure isolating languages have maximallyone morpheme per word.
in agglutinative lan-guages, morphemes tend to be neatly segmentableand carry a single feature, whereas in fusional lan-guages, a single morpheme often carries multi-ple grammatic, syntactic, and semantic features.
finally, in introﬂexive languages root words arebased on consonant stems, where vowels intro-duced around and between them lead to syntacticand semantic changes (see plank (1999); bickeland nichols (2005); gerz et al.
(2018) for a morein-depth discussion)..3 data.
we select ﬁve languages from each category ex-cept introﬂexive (four), shown in table 1. ashort example sentence in a fusional (norwegian○ no ), isolating (indonesian ø in ), agglutinative(basque (cid:20) eu ), and introﬂexive (maltese (cid:24) mt )language with glosses and translation in english isshown in example 1..(1).
○ no.
buss-en.
kom.
sen-t.bus-def.art.
come:perf.
late-adv.
ø in.
bus.
itu.
datang.
terlambat.
bus.
that.
come.
late.
(cid:20) eu.
autobus-a.
berandu.
etorri.
zen.
bus-def.art.
late.
come:pcp.
prt.3s.
(cid:24) mt.
ix-xarabank.
waslet.
tard.
def.art-bus.
come:perf.
late.
‘the bus came late.’.
2we use the following color combinations to de-note ø isolating , (cid:20) agglutinative , (cid:24) introﬂexive , and.
○ fusional.
languages..3137type.
language.
part-of-speechdev.
train.
test.
sentiment analysistestdevtrain.
german○ fusionalspanish○ fusionalslovak○ fusionalnorwegian○ fusionalgreek○ fusionalø isolatingmandarinø isolatingvietnameseø isolatingthaiø isolatingcantoneseø isolatingindonesianfinnish(cid:20) agglutinativebasque(cid:20) agglutinative(cid:20) agglutinative korean(cid:20) agglutinative(cid:20) agglutinative(cid:24) introﬂexive(cid:24) introﬂexive(cid:24) introﬂexive(cid:24) introﬂexive.
japaneseturkisharabichebrewalgerianmaltese.
38,10214,3058,48315,6961,6623,9971,400004,47712,2175,39623,0107,0273,6646,0755,2419971,123.
18,4341,6541,0602,409403500800005591,3641,7982,066501988909484136433.
18,4591,7211,0611,9394565008001,0001,0045571,5551,7992,287543983680491143518.
6,4441,0293,5602,6755,93612,3482,3848,10328,2047,9264,43278936,0009,8314,4862,4686,621564595.
7721475225163832,5913311,1534,4591,1326331131,3331,6771053531,1847585.
1,4902961,0424177674,8966852,3448,9152,2661,2672272,6672,552211706230592171.table 1: number of examples for each task, language and dataset.
3.1 part-of-speech.
we obtain the data for the part-of-speech taggingtask from the universal dependencies project (ze-man et al., 2020), which currently gathers dataannotated with universal pos tags for more than90 languages, although there are differences in sizeand domain.
for algerian we use the annotationsfrom seddah et al.
(2020).
we found no trainingsets available for thai and cantonese, hence weuse them for testing only.
for more details on thesedatasets, see table 5 in the appendix..3.2 sentiment analysis.
for sentiment analysis, however, there is no cen-tralized repository of similar data.
therefore, wecollect data from a number of sources and process.
3including.
https://github.com/dimitrakatseli/review_sentiment_analysis.
4https://github.com/ljw9609/.
sentimentanalysis.
5https://github.com/e9t/nsmc6https://github.com/darkmap/japanese_.
sentiment.
7including https://github.com/ozturkaslii/.
analyze-turkish-sentiment.
them to create binary (positive, negative) sentence-level sentiment datasets.
for convenience, we listthe origin of each dataset in table 2 and their fullcharacteristics in table 6 in the appendix..4 methods.
we ﬁne-tune both multilingual bert (mbert)(xu et al., 2019) and xlm roberta (xlm-r)(conneau et al., 2020a) models on the availabletraining data in each language, using a shared set ofhyperparameters selected from recommended val-ues according to the characteristics of our data.
weset the learning rate to 2e-5, maximum sequencelength of 256, batch size of 8 or 168, and performearly stopping once the validation score has notimproved in the last epochs, saving the model thatperforms best on the dev set.
we then test eachmodel on all languages, giving us a matrix of testscores, where the diagonal is in-language, and allothers are cross-lingual.
we use accuracy as ourmetric for pos and macro f1 for sentiment, as thelatter often contains unbalanced classes, and deﬁne.
8depending on the size of the training set, model architec-.
ture and available gpu memory..3138language.
data origin.
○ german○ spanish○ slovak○ norwegian○ greek 3.ø mandarinø vietnameseø thaiø cantoneseø indonesian.
(cid:20) finnish(cid:20) basque(cid:20) korean(cid:20) japanese(cid:20) turkish 7(cid:24) arabic.
(cid:24) hebrew(cid:24) algerian(cid:24) maltese.
wojatzki et al.
(2017)agerri et al.
(2013)pecar et al.
(2019)øvrelid et al.
(2020)kalamatianos et al.
(2015)tsakalidis et al.
(2018)github repository4cuong et al.
(2016)bact’ et al.
(2019)xiang (2019)purwarianti and crisdayanti(2019)lind´en et al.
(2020)barnes et al.
(2018a)github repository5github repository6pontiki et al.
(2016)abdulla et al.
(2013)nabil et al.
(2015)amram et al.
(2018)touileb and barnes (2021)dingli and sant (2016)cortis and davis (2019).
table 2: origin of the data for sentiment analysis..a baseline as the result of predicting the majorityclass..5 results.
once our scores matrix is built, we average9 thescore of each ﬁne-tuned model, which we refer toas language-to-language cross-lingual scores, overthe other languages in each morphological group,thus obtaining each model’s average cross-lingualperformance per target group (language-to-groupcross-lingual scores).
next, we average again foreach source language group.
this yields the aver-age cross-lingual performance values per trainingand testing language groups (group-to-group cross-lingual scores), which we report in table 3..in the part-of-speech task, the best group-to-group cross-lingual performance always corre-sponds to models ﬁne-tuned in a language of.
9note that, throughout this paper, when we average acrossmorphological groups, we do so with a weighted average sothat all groups are equally represented regardless of how manylanguages they include..the same morphological group, regardless of themodel’s architecture.
fusional models, in particu-lar, obtain a remarkably higher score when testedon other fusional languages (over 80%).
on theother hand, the group-to-group cross-lingual scoreswhere the target language is introﬂexive are consid-erably lower than the rest (always below 50%)..in contrast, both model architectures show dif-ferent patterns in the sentiment analysis task.
forthe xlm-r models, the best group-to-group cross-lingual scores are all achieved by those trainedon a fusional language, while for the mbert it ismainly models trained on an isolating language thatachieve the best scores.
in any case, all scores arewithin a similar range of values.
in fact, the maindifference in this task seems to be due to xlm-r’sconsiderably higher scores..in order to capture the cross-lingual phenomenonmore accurately, we introduce transfer loss, a rela-tive metric deﬁned in equation 1:.
t lx→y = sx→x − sx→y.
(1).
where t lx→y is the transfer loss experienced bya model ﬁne-tuned in language x when transfer-ring to language y (language-to-language transferloss) and sx→y is the score10 achieved when test-ing a model ﬁne-tuned in language x on language y.thus, it is a measure of the performance lost in thezero-shot transfer process: the better the transferbetween both languages, the lower it will be..we also deﬁne its averaged variants:.
t lx→a = sx→x −.
sx→i.
(2).
1na.
(cid:88).
i∈ai(cid:54)=x.
t la→b =.
t li→b.
(3).
1na.
(cid:88).
i∈a.
where t lx→a denotes the average transfer lossfrom language x to languages belonging to morpho-logical type a (language-to-group transfer loss),t la→b refers to the average transfer loss experi-enced by languages from morphological group ato languages from group b (group-to-group trans-fer loss) and na is the number of languages (otherthan x) included in the experiment that belong togroup a. table 4 shows the resulting group-to-group transfer loss values for each task..10the score metric will depend on the task: accuracy in.
pos and macro f1 in sentiment analysis..3139train.
○ fusional.
ø isolating.
(cid:20) agglutinative.
(cid:24) introﬂexive.
test.
mbert xlm-r mbert xlm-r mbert xlm-r mbert xlm-r.train.
○ fusional.
ø isolating.
(cid:20) agglutinative.
test.
mbert xlm-r mbert xlm-r mbert xlm-r mbert xlm-r.○ fusionalø isolating(cid:20) agglutinative(cid:24) introﬂexive.
○ fusionalø isolating(cid:20) agglutinative(cid:24) introﬂexive.
81.252.859.443.2.
56.750.553.850.0.
82.358.261.843.5.
74.376.277.560.7.
63.655.057.440.7.
57.959.955.954.2.
65.260.360.140.6.
69.171.369.158.2.
61.352.961.339.1.
59.255.654.752.4.
62.458.465.039.3.
70.975.472.759.4.
65.865.857.351.557.856.446.645.6(cid:24) introﬂexive.
50.241.945.749.9.
58.752.860.855.2.table 3: group-to-group cross-lingual accuracy scores (%) in part-of-speech tagging (top) and macro f1 scores(%) in sentiment analysis (bottom) for each ﬁne-tuning (column) and testing (row) morphological group, and eachmodel architecture.
maximum values in each test group and architecture are highlighted.
higher is better..models ﬁne-tuned in all groups except agglutina-tive experience the lowest performance drop whentransferring to fusional languages in the part-of-speech task, whereas in the sentiment analysis taskthere is no clear pattern.
it is also worth notingthat the xlm-r models tend to transfer better com-pared to mbert, only slightly in part-of-speechtagging but more drastically in sentiment analysis.
additionally, the cases of worst transfer happenwhen the target language is introﬂexive (especiallyfor xlm-r)..next, to address rq1 more directly, we com-pare two different types of transfer: intra-grouptransfer, where both the ﬁne-tuning and target lan-guages belong to the same morphological group,and inter-group transfer, where the two differ inmorphological type.
we calculate an average forboth types of transfer and for each training group,model architecture and task.
we present the result-ing values in figure 1..generally, transfer to another morphologicaltype implies a higher cost in terms of performance,except for the introﬂexive models.
this differencein transfer loss appears to be similar for all groupsin the sentiment task, yet it varies considerably inthe part-of-speech task.
more speciﬁcally, thereare two extremes in this latter case: fusional mod-els suffer large performance drops when switchingmorphological groups, whereas isolating modelsexperience similar transfer losses in both condi-tions..finally, we average again to obtain a single trans-.
fer loss value for each task and model, and use itto establish a comparison in figure 2. here weobserve that: (1) the difference in transfer lossbetween an intra-group and inter-group transferis higher on the part-of-speech task, (2) transferis also generally worse on this task11, (3) xlm-r models perform better cross-lingual transfersin general (especially on the sentiment analysistask), and (4) the difference between intra-groupand inter-group transfer is similar on both modelarchitectures..6 analysis.
in this section, we run several statistical tests to ver-ify our conclusion to rq1 and detail several pointsof analysis that relate to rq2 and rq3.
namely, towhat degree do other variables contribute to effectson cross-lingual transfer..6.1 testing the effect of transfer type.
we run a set of statistical tests to validate the ob-servations made from figure 2 in section 5. inthe part-of-speech tagging task, an analysis of vari-ance (anova) reveals there is a statistically sig-niﬁcant, although weak, difference in transfer lossbetween the intra- and inter-group conditions, forboth model architectures (η2 ≈ 0.06, p < 0.01in both cases).
in contrast, a kruskal-wallis anal-ysis of variance12 ﬁnds no signiﬁcant difference.
11strictly speaking, we use different metrics for both tasks,.
which are not necessarily comparable..12the normality condition for anova is not met..3140train.
○ fusional.
ø isolating.
(cid:20) agglutinative.
(cid:24) introﬂexive.
test.
mbert xlm-r mbert xlm-r mbert xlm-r mbert xlm-r.train.
○ fusional.
ø isolating.
(cid:20) agglutinative.
test.
mbert xlm-r mbert xlm-r mbert xlm-r mbert xlm-r.○ fusionalø isolating(cid:20) agglutinative(cid:24) introﬂexive.
○ fusionalø isolating(cid:20) agglutinative(cid:24) introﬂexive.
16.645.038.554.6.
26.532.729.433.2.
15.339.435.854.2.
13.511.610.327.1.
28.837.434.951.7.
31.229.233.234.9.
27.732.632.852.3.
22.820.622.833.8.
34.242.634.356.5.
26.530.131.033.3.
33.237.230.556.3.
19.415.017.731.0.
26.726.335.240.634.735.745.546.9(cid:24) introﬂexive.
33.041.337.533.3.
22.728.620.626.3.table 4: group-to-group transfer loss (in percentage points) in the part-of-speech tagging (top) and sentimentanalysis (bottom) tasks for each ﬁne-tuning (column) and testing (row) language’s morphological group, as wellas each model architecture.
minimum values in each ﬁne-tuning group and architecture are highlighted.
lower isbetter..figure 1: average transfer loss (in percentage points) to other languages of the same group (intra-group) and tolanguages that belong to the other groups (inter-group) in the part-of-speech tagging (top) and sentiment analysis(bottom) tasks.
lower is better..between the two types of transfer in the sentimentanalysis task, in neither mbert or xlm-r mod-els (p > 0.01 in both cases).
we also test fordifferences in transfer loss between model archi-tectures and ﬁnd a signiﬁcant difference in the sen-timent analysis task (kruskal-wallis, p < 0.01),.
but not in the part-of speech tagging task (anova,p > 0.01).
this is all consistent with our previousobservations..3141intra-groupinter-grouptransfertype1020304050transferlossmberttraingroupfusionalisolatingagglutinativeintroﬂexiveintra-groupinter-grouptransfertype1020304050xlm-rintra-andinter-grouptransferlossesinpart-of-speechintra-groupinter-grouptransfertype222426283032343638transferlossmberttraingroupfusionalisolatingagglutinativeintroﬂexiveintra-groupinter-grouptransfertype121416182022242628xlm-rintra-andinter-grouptransferlossesinsentimentanalysisguages themselves but, in either case, its coefﬁcientconﬁrms our intuition that longer sequences gener-ally make the task more difﬁcult.
the second couldindicate some overﬁtting to the ﬁne-tuning lan-guage, as higher in-language score entails slightlypoorer transfer..xlm-r adds another predictor: the proportionof words that have been split into subword tokensin the test data (2.1).
this variable is related to thesize of the pretraining corpus for each language14:a richer pretraining vocabulary will ensure morewords are considered frequent during byte pairencoding and, therefore, assigned a single token,instead of being broken down into subword tokensby the tokenizer.
this means that high-resourcelanguages will have a lower word split probabilityand, hence, it will be slightly easier to transferto them.
however, it is worth pointing out thatthis bias has little effect and is only statisticallysigniﬁcant in xlm-r..in the case of sentiment analysis, relevant pre-dictors are: presence of the ﬁne-tuning (coefﬁcientof -11.8 for mbert and -18.7 for xlm-r) andtarget (-10.3 and -16.3) languages in pretraining, in-language score (6.8 and 6.5), proportion of wordssplit into subword tokens in the training data (3.3and 2.7) and proportion of examples labeled aspositive in the test set (-2.8, xlm-r only)..curiously, sentiment analysis is more sensitiveto variables related to the training data comparedto part-of-speech tagging, whereas sequence lengthonly affects the latter.
on the other hand, languageinclusion in pretraining and in-language score areuseful predictors in both tasks, yet the former isfar stronger in pos and the latter is more relevantin sentiment analysis.
in summary, we verify thattransferring to a different morphological type has arelevant effect in part-of-speech tagging but not insentiment analysis, regardless of the model archi-tecture..6.3 testing pretrained languages only.
given the considerable effect pretraining seems tohave on transfer loss (discussed in section 6.2), were-evaluate our results after removing the languagesthat were not present during the pretraining of ei-ther of the two model architectures (cantonese,algerian and maltese) and check whether thereare relevant differences with our previous results..14in fact, we do not include pretraining data size as a pre-dictor because of its correlation with the variable in question..figure 2: comparison across tasks of the average trans-fer loss (in percentage points) to other languages of thesame group (intra-group) and to languages that belongto the other groups (inter-group).
lower is better..6.2 linear regression model for transfer loss.
additionally, we modellanguage-to-languagetransfer loss with a linear regression model, usingtransfer type, as well as other variables, as possi-ble predictors.
this allows us to (a) test whetherthe intra-/inter-group difference retains its statisti-cal signiﬁcance in the presence of other variablesand (b) evaluate its effect in comparison to otherpredictors..first, we select a set of variables that might berelevant in cross-lingual transfer, and remove thosethat are highly correlated with the rest to avoidmulticollinearity in the model (see table 7 in theappendix for the ﬁnal list of selected variables).
we standardize all of the remaining features so thattheir units are comparable and, consequently, soare their regression coefﬁcients..again, we ﬁnd transfer type (intra-/inter-group)to be a signiﬁcant predictor in both regression mod-els for part-of-speech tagging (p < 0.01), but notin sentiment analysis.
in the former case, it hasthe second strongest effect with a standardized co-efﬁcient of 8.613, the ﬁrst being presence of thetarget language in pretraining with a coefﬁcient of-25.9. in other words, transferring to a language onwhich the model has not been pretrained impliesan additional performance drop of 25.9 percentagepoints, while transferring to another morphologicalgroup incurs an additional 8.6..the remaining predictors for this task are aver-age test example length (measured in tokens, co-efﬁcient of 4.0) and in-language score (3.3).
theﬁrst is a complex variable because differences intext length can be due to their domain or to the lan-.
13since the regression models for mbert and xlm-r are.
quite similar, we report the averaged coefﬁcients here..3142intra-groupinter-grouptransfertype2025303540transferlossintra-/inter-grouptransfer:taskandmodelcomparisonmodelmbertxlm-rtaskpart-of-speechsentimentanalysisloss values for all morphological groups seem toconverge to the same value (see figure 5 in theappendix).
for more information, see figures 5and 6, as well as tables 8 and 9, all of which canbe found in the appendix..6.5 effect of training data size.
figure 3: comparison across tasks of the average trans-fer loss (in percentage points) to other languages ofthe same group (intra-group) and to languages thatbelong to the other groups (inter-group) after remov-ing languages that were not present during pretraining(top) and after balancing in-language scores (bottom).
lower is better..of course, we observe an improvement in cross-lingual scores involving either an isolating or anintroﬂexive language, because these are the groupsthe excluded languages belong to.
overall, how-ever, re-running the statistical tests does not modifyour previous conclusions (see figure 3)..6.4 balanced in-language scores.
since in-language score is relevant in all regressionmodels considered in 6.2 (and the value of transferloss is relative to it), we decide to re-train all mod-els, this time preventing them from increasing saidscore above a ﬁxed threshold value (we choose theminimum in-language score achieved previously ineach task and model architecture) and re-evaluateour previous conclusions..the intra-/inter-group difference in transfer lossis still statistically signiﬁcant in part-of-speech tag-ging and not in sentiment analysis.
similarly, thereis still a statistically signiﬁcant difference in trans-fer loss between both models only in the sentimentanalysis task.
all of this can be seen in figure 3.the only remarkable difference is in the part-of-speech task, where the average inter-group transfer.
figure 4: average cross-lingual score achieved by mod-els trained with varying german part-of-speech (top)and korean sentiment (bottom) data sizes.
higher isbetter..we also test the effect that training with consid-erably more data has on cross-lingual transfer.
weselect two languages, each with around 150,000examples available: german for the part-of-speechtagging task and korean for sentiment analysis.
wetrain four models with increasingly more data andthen test them on all languages..in german, we notice an important decline incross-lingual scores when increasing data size from80,000 to 150,000 examples (see figure 4).
morespeciﬁcally, in mbert models there is an averagedecrease of 15.6 and 9.0 points when the cross-lingual transfer is intra- and inter-group, respec-tively.
in xlm-r, the corresponding values are25.4 and 19.5. hence, it appears that a phenomenonof language specialization takes place, one to whichxlm-r is more susceptible and that has more im-portant consequences in intra-group transfer.
toensure this is a language and not a domain/datasetspecialization, we test these models on another.
3143intra-groupinter-grouptransfertype1520253035transferlossintra-/inter-grouptransfer:taskandmodelcomparison(pretrainedlanguagesonly)modelmbertxlm-rtaskpart-of-speechsentimentanalysisintra-groupinter-grouptransfertype17.520.022.525.027.530.032.535.0transferlossintra-/inter-grouptransfer:taskandmodelcomparison(balancedin-languagescores)modelmbertxlm-rtaskpart-of-speechsentimentanalysis20406080100120140trainingexamples(x103)304050607080cross-lingualscoregerman(pos)transfer-typeinter-groupintra-groupmodelmbertxlm-r20406080100120140trainingexamples(x103)505560657075cross-lingualscorekorean(sa)transfer-typeinter-groupintra-groupmodelmbertxlm-rgerman dataset (pud) and ﬁnd no decrease in per-formance..in contrast, average korean cross-lingual scoresremain relatively constant (see figure 4).
there-the language specialization phenomenonfore,could be more characteristic of part-of-speech tag-ging than sentiment analysis..6.6 domain effects.
conneau et al.
(2020b) ﬁnd that domain mismatchin pretraining of multilingual lms is more prob-lematic than domain mismatch in ﬁne-tuning.
yetgiven the variety of domains present in the sen-timent data, we decided to test its effect.
proxya-distance (glorot et al., 2011) measures the gen-eralization error of a linear svm trained to dis-criminate between two domains.
we translate1000 sentences from each dataset to english us-ing googletranslate and then compute the proxya-distance.15 for pos tagging, there are small butinsigniﬁcant negative effects of proxy a-distanceon results for both models (a pearson coefﬁcientof -0.07, p > 0.01 and -0.07, p > 0.01 for mbertand xlm-r, respectively).
on the sentiment task,there is no signiﬁcant domain effect for mbert(-0.06, p > 0.01), while there is a small negativeeffect for xlm-r (-0.27, p < 0.01).
this suggeststhat most of the transfer loss is not due to domainmismatch..7 discussion and future work.
in this paper, we have conducted an extensiveanalysis of the effects of morphological typologyon cross-lingual transfer and attempted to isolatethese factors from other variables.
we have com-pared performance of two state-of-the-art zero-shotcross-lingual models on two tasks (part-of-speechtagging and sentiment analysis) for 19 languagesacross four morphological typologies.
we havefound that transfer to another morphological typegenerally implies a higher performance loss thantransfer to another language with the same morpho-logical typology.
additionally, part-of-speech tag-ging is more sensitive to morphological differencesthan sentiment analysis, while sentiment analysis ismore sensitive to variables related to the ﬁne-tuningdata and is less predictable in general..we have tested this sensitivity to morphologyafter balancing other inﬂuential factors, such as.
15implementationableatproxy-a-distance..adapted.
avail-from thehttps://github.com/rpryzant/.
code.
in-language score, and, still, the intra-/inter-groupdifference remains.
however, the effect of morpho-logical typology, while signiﬁcant, is not strong,given that most of the variability in transfer loss isdue to other factors..we have also conﬁrmed that xlm-r generallytransfers better than mbert, especially on sen-timent analysis.
in part-of-speech tagging, wehave reported considerably better transfer withinfusional languages, as well as easier transfer fromthe other groups towards the fusional type.
more-over, we have found a case that suggests that ﬁne-tuning on large training sets might lead to languagespecialization and, consequently, be detrimental tocross-lingual transfer..it is worth noting that we do not explore whetherthe type of script used by the languages has aneffect on cross-lingual transfer.
this is hard tocontrol in our experimental setup, as there are somescripts that are either unique to a language or onlyhave one with enough data to represent it, makingit impossible to make comparisons..the recent cross-lingual suite xtreme (hu et al.,2020) includes a number of benchmark tasks in 40languages.
while this dataset is a useful collectionof cross-lingual tasks, it is unfortunately not sufﬁ-cient for our purposes.
the pos data is the sameas we use, while other tasks either a) do not containa representative sample of language typologies b)use translation, introducing problems of ‘transla-tionese’, or c) are automatically created and notmanually curated named entity recognition data.
our experimental setup avoids these problems byfocusing on binary sentiment analysis, which isa task that has data available in many languagesand does not require translation to get multilingualdata..finally, this work ties in with the increasing in-terest in typological questions in nlp (takamuraet al., 2016; ponti et al., 2019; bjerva et al., 2019;nooralahzadeh et al., 2020; bjerva and augenstein,2021), which often try to directly predict typologi-cal features, or use these to analyze model perfor-mance..in the future, it would be interesting to trainmulti-lingual language models on speciﬁc languagefamilies in order to ﬁnd maximal beneﬁts fromshared morphology.
finally, as typology seems toaffect tasks differently, it would be interesting toexplore other tasks, e.g., dependency parsing orsemantic role labeling..3144references.
nawaf abdulla, nizar a. ahmed, mohammed shehab,and mahmoud al-ayyoub.
2013. arabic sentimentanalysis: lexicon-based and corpus-based.
pages 1–6..rodrigo agerri, montse cuadros, se´an gaines, andgerman rigau.
2013. opener: open polarity en-hanced named entity recognition.
procesamientodel lenguaje natural, 51(0):215–218..adam amram, anat ben david, and reut tsarfaty.
2018. representations and architectures in neu-ral sentiment analysis for morphologically rich lan-inguages: a case study from modern hebrew.
proceedings of the 27th international conference oncomputational linguistics, pages 2242–2252, santafe, new mexico, usa.
association for computa-tional linguistics..mikel artetxe, gorka labaka, and eneko agirre.
2017.learning bilingual word embeddings with (almost)no bilingual data.
in proceedings of the 55th annualmeeting of the association for computational lin-guistics (volume 1: long papers), pages 451–462,vancouver, canada.
association for computationallinguistics..mikel artetxe, sebastian ruder, and dani yogatama.
2020. on the cross-lingual transferability of mono-lingual representations.
in proceedings of the 58thannual meeting of the association for computa-tional linguistics, pages 4623–4637, online.
asso-ciation for computational linguistics..bact’, pattarawat chormai, charin, and ekapolc.
2019..pythainlp/wisesight-sentiment: first release..alexandra balahur and marco turchi.
2014. compar-ative experiments using supervised learning and ma-chine translation for multilingual sentiment analysis.
computer speech language, 28(1):56 – 75..carmen banea, rada mihalcea, janyce wiebe, andsamer hassan.
2008. multilingual subjectivity anal-in proceedings ofysis using machine translation.
the 2008 conference on empirical methods in natu-ral language processing, pages 127–135, honolulu,hawaii.
association for computational linguistics..jeremy barnes, toni badia, and patrik lambert.
2018a.
multibooked: a corpus of basque and catalan ho-tel reviews annotated for aspect-level sentiment clas-in proceedings of the eleventh interna-siﬁcation.
tional conference on language resources and eval-uation (lrec-2018), miyazaki, japan.
europeanlanguages resources association (elra)..jeremy barnes and roman klinger.
2019. embed-ding projection for targeted cross-lingual sentiment:model comparisons and a real-world study.
journalof artiﬁcial intelligence research, 66:691–742..jeremy barnes, roman klinger, and sabine schulte imwalde.
2018b.
bilingual sentiment embeddings:.
joint projection of sentiment across languages.
inproceedings of the 56th annual meeting of the as-sociation for computational linguistics (volume 1:long papers), pages 2483–2493, melbourne, aus-tralia.
association for computational linguistics..emily m. bender.
2013. linguistic fundamentals fornatural language processing: 100 essentials frommorphology and syntax.
morgan amp; claypoolpublishers..balthasar bickel and johanna nichols.
2005..inﬂec-tional morphology.
in timothy shopen, editor, lan-guage typology and syntactic description.
cam-bridge university press, cambridge.
2nd edition..johannes bjerva and isabelle augenstein.
2021. doestypological blinding impede cross-lingual sharing?.
johannes bjerva, yova kementchedjhieva, ryan cot-terell, and isabelle augenstein.
2019. uncoveringprobabilistic implications in typological knowledgein proceedings of the 57th annual meet-bases.
ing of the association for computational linguis-tics, pages 3924–3930, florence, italy.
associationfor computational linguistics..alexis conneau, kartikay khandelwal, naman goyal,vishrav chaudhary, guillaume wenzek, franciscoguzm´an, edouard grave, myle ott, luke zettle-moyer, and veselin stoyanov.
2020a.
unsupervisedcross-lingual representation learning at scale.
inproceedings of the 58th annual meeting of the asso-ciation for computational linguistics, pages 8440–8451, online.
association for computational lin-guistics..alexis conneau, shijie wu, haoran li, luke zettle-moyer, and veselin stoyanov.
2020b.
emergingcross-lingual structure in pretrained language mod-in proceedings of the 58th annual meetingels.
of the association for computational linguistics,pages 6022–6034, online.
association for compu-tational linguistics..keith cortis and brian davis.
2019. a social opin-ion gold standard for the malta government budget2018. in proceedings of the 5th workshop on noisyuser-generated text (w-nut 2019), pages 364–369,hong kong, china.
association for computationallinguistics..ryan cotterell, sebastian j. mielke, jason eisner, andbrian roark.
2018. are all languages equally hardin proceedings of the 2018to language-model?
conference of the north american chapter of theassociation for computational linguistics: humanlanguage technologies, volume 2 (short papers),pages 536–541, new orleans, louisiana.
associa-tion for computational linguistics..le anh cuong, ng.
t. minh huyen, and ng.
viet hung.
2016. vlsp 2016 shared task: vietnamese analysis.
in vlsp 2016..3145jacob devlin, ming-wei chang, kenton lee, andkristina toutanova.
2019. bert: pre-training ofdeep bidirectional transformers for language under-in proceedings of the 2019 conferencestanding.
of the north american chapter of the associationfor computational linguistics: human languagetechnologies, volume 1 (long and short papers),pages 4171–4186, minneapolis, minnesota.
associ-ation for computational linguistics..alexiei dingli and nicole sant.
2016. sentiment anal-ysis on maltese using machine learning.
in proceed-ings of the tenth international conference on ad-vances in semantic processing (semapro 2016),pages 21–25..matthew s. dryer and martin haspelmath, editors.
2013. wals online.
max planck institute for evo-lutionary anthropology, leipzig..daniela gerz, ivan vuli´c, edoardo maria ponti, roireichart, and anna korhonen.
2018. on the relationbetween linguistic typology and (limitations of) mul-tilingual language modeling.
in proceedings of the2018 conference on empirical methods in naturallanguage processing, pages 316–327, brussels, bel-gium.
association for computational linguistics..xavier glorot, antoine bordes, and yoshua bengio.
2011. domain adaptation for large-scale senti-ment classiﬁcation: a deep learning approach.
inproceedings of the 28th international conferenceon international conference on machine learning,icml’11, page 513–520, madison, wi, usa.
om-nipress..junjie hu, sebastian ruder, aditya siddhant, grahamneubig, orhan firat, and melvin johnson.
2020.xtreme: a massively multilingual multi-task bench-mark for evaluating cross-lingual generalization..matthias huck, diana dutka, and alexander fraser.
2019. cross-lingual annotation projection is ef-in pro-fective for neural part-of-speech tagging.
ceedings of the sixth workshop on nlp for similarlanguages, varieties and dialects, pages 223–233,tobefilled-ann arbor, michigan.
associationfor computational linguistics..georgios kalamatianos, dimitrios mallis, symeonsymeonidis, and avi arampatzis.
2015. sentimentanalysis of greek tweets and hashtags using a senti-ment lexicon.
pages 63–68..anne lauscher, vinit ravishankar, ivan vuli´c, andgoran glavaˇs.
2020. from zero to hero: on thelimitations of zero-shot language transfer with mul-tilingual transformers.
in proceedings of the 2020conference on empirical methods in natural lan-guage processing (emnlp), pages 4483–4499, on-line.
association for computational linguistics..jindˇrich libovick´y, rudolf rosa, and alexander fraser.
2020. on the language neutrality of pre-trained mul-tilingual representations.
in findings of the associ-ation for computational linguistics: emnlp 2020,pages 1663–1674, online.
association for computa-tional linguistics..krister lind´en, tommi jauhiainen, and sam hardwick.
2020. finnsentiment – a ﬁnnish social media corpusfor sentiment polarity annotation..ryan mcdonald, slav petrov, and keith hall.
2011.multi-source transfer of delexicalized dependencyparsers.
in proceedings of the 2011 conference onempirical methods in natural language processing,pages 62–72, edinburgh, scotland, uk.
associationfor computational linguistics..sebastian j. mielke, ryan cotterell, kyle gorman,brian roark, and jason eisner.
2019. what kindin pro-of language is hard to language-model?
ceedings of the 57th annual meeting of the asso-ciation for computational linguistics, pages 4975–4989, florence, italy.
association for computationallinguistics..rada mihalcea, carmen banea, and janyce wiebe.
2007. learning multilingual subjective language viacross-lingual projections.
in proceedings of the 45thannual meeting of the association of computationallinguistics, pages 976–983, prague, czech repub-lic.
association for computational linguistics..mahmoud nabil, mohamed aly, and amir atiya.
2015.in pro-astd: arabic sentiment tweets dataset.
ceedings of the 2015 conference on empirical meth-ods in natural language processing, pages 2515–2519, lisbon, portugal.
association for computa-tional linguistics..garrett nicolai, kyle gorman, and ryan cotterell, edi-tors.
2020. proceedings of the 17th sigmorphonworkshop on computational research in phonetics,phonology, and morphology.
association for com-putational linguistics, online..joo-kyung kim, young-bum kim, ruhi sarikaya, anderic fosler-lussier.
2017. cross-lingual transferlearning for pos tagging without cross-lingual re-sources.
in proceedings of the 2017 conference onempirical methods in natural language processing,pages 2832–2838, copenhagen, denmark.
associa-tion for computational linguistics..farhad nooralahzadeh, giannis bekoulis, johannesbjerva, and isabelle augenstein.
2020. zero-shotin pro-cross-lingual transfer with meta learning.
ceedings of the 2020 conference on empirical meth-ods in natural language processing (emnlp),pages 4547–4562, online.
association for compu-tational linguistics..guillaume lample and alexis conneau.
2019. cross-lingual language model pretraining.
advances inneural information processing systems (neurips)..lilja øvrelid, petter mæhlum, jeremy barnes, and erikvelldal.
2020. a ﬁne-grained sentiment dataset forin proceedings of the 12th languagenorwegian..3146resources and evaluation conference, pages 5025–5033, marseille, france.
european language re-sources association..sebastian pad´o and mirella lapata.
2009. cross-lingual annotation projection of semantic roles.
research,journal36(1):307–340..intelligence.
artiﬁcial.
of.
samuel pecar, marian simko, and maria bielikova.
2019. improving sentiment classiﬁcation in slovakin proceedings of the 7th workshop onlanguage.
balto-slavic natural language processing, pages114–119, florence, italy.
association for computa-tional linguistics..telmo pires, eva schlinger, and dan garrette.
2019.in pro-how multilingual is multilingual bert?
ceedings of the 57th annual meeting of the asso-ciation for computational linguistics, pages 4996–5001, florence, italy.
association for computa-tional linguistics..frans plank.
1999. split morphology: how agglu-linguistic typology,.
tination and ﬂexion mix.
3:279–340..edoardo maria ponti, helen o’horan, yevgeni berzak,ivan vuli´c, roi reichart, thierry poibeau, ekaterinashutova, and anna korhonen.
2019. modeling lan-guage variation and universals: a survey on typo-logical linguistics for natural language processing.
computational linguistics, 45(3):559–601..maria pontiki, dimitris galanis, haris papageorgiou,ion androutsopoulos, suresh manandhar, moham-mad al-smadi, mahmoud al-ayyoub, yanyanzhao, bing qin, orph´ee de clercq, v´eroniquehoste, marianna apidianaki, xavier tannier, na-talia loukachevitch, evgeniy kotelnikov, nuria bel,salud mar´ıa jim´enez-zafra, and g¨uls¸en eryi˘git.
2016. semeval-2016 task 5: aspect based senti-ment analysis.
in proceedings of the 10th interna-tional workshop on semantic evaluation (semeval-2016), pages 19–30, san diego, california.
associa-tion for computational linguistics..a. purwarianti and i. a. p. a. crisdayanti.
2019. im-proving bi-lstm performance for indonesian senti-ment analysis using paragraph vector.
in 2019 inter-national conference of advanced informatics: con-cepts, theory and applications (icaicta), pages 1–5..djam´e seddah, farah essaidi, amal fethi, matthieufuteral, benjamin muller, pedro javier ortiz su´arez,benoˆıt sagot, and abhishek srivastava.
2020. build-ing a user-generated content north-african arabizitreebank: tackling hell.
in proceedings of the 58thannual meeting of the association for computa-tional linguistics, pages 1139–1150, online.
asso-ciation for computational linguistics..ceedings of the 49th annual meeting of the associ-ation for computational linguistics: human lan-guage technologies, pages 682–686, portland, ore-gon, usa.
association for computational linguis-tics..oscar t¨ackstr¨om, dipanjan das, slav petrov, ryan mc-donald, and joakim nivre.
2013. token and typeconstraints for cross-lingual part-of-speech tagging.
transactions of the association for computationallinguistics, 1:1–12..hiroya takamura, ryo nagata,.
and yoshifumikawasaki.
2016. discriminative analysis of linguis-tic features for typological study.
in proceedings ofthe tenth international conference on language re-sources and evaluation (lrec 2016), pages 69–76,portoroˇz, slovenia.
european language resourcesassociation (elra)..samia touileb and jeremy barnes.
2021. the interplaybetween language similarity and script on a novelmulti-layer algerian dialect corpus.
in proceedingsof the 59th annual meeting of the association forcomputational linguistics, online.
association forcomputational linguistics..adam tsakalidis, symeon papadopoulos, raniavoskaki, kyriaki ioannidou, christina boididou,alexandra i cristea, maria liakata, and yianniskompatsiaris.
2018. building and evaluating re-sources for sentiment analysis in the greek language.
language resources and evaluation, 52(4):1021–1044..ashish vaswani, noam shazeer, niki parmar, jakobuszkoreit, llion jones, aidan n gomez, ł ukaszkaiser, and illia polosukhin.
2017. attention is allyou need.
in advances in neural information pro-cessing systems, volume 30, pages 5998–6008.
cur-ran associates, inc..michael wojatzki, eugen ruppert, sarah holschneider,torsten zesch, and chris biemann.
2017. germeval2017: shared task on aspect-based sentiment in so-cial media customer feedback.
in proceedings of thegermeval 2017 – shared task on aspect-based sen-timent in social media customer feedback, pages1–12, berlin, germany..rong xiang.
2019. sentiment augmented attention net-.
work for cantonese restaurant review analysis..hu xu, bing liu, lei shu, and philip yu.
2019. bertpost-training for review reading comprehension andaspect-based sentiment analysis.
in proceedings ofthe 2019 conference of the north american chap-ter of the association for computational linguistics:human language technologies, volume 1 (longand short papers), pages 2324–2335, minneapolis,minnesota.
association for computational linguis-tics..anders søgaard.
2011. data point selection for cross-language adaptation of dependency parsers.
in pro-.
david yarowsky, grace ngai, and richard wicen-inducing multilingual text analysis.
towski.
2001..3147tools via robust projection across aligned corpora.
inproceedings of the first international conference onhuman language technology research..marcos zampieri, preslav nakov, nikola ljubeˇsi´c,j¨org tiedemann, shervin malmasi, and ahmed ali,editors.
2018. proceedings of the fifth workshop onnlp for similar languages, varieties and dialects(vardial 2018).
association for computational lin-guistics, santa fe, new mexico, usa..daniel zeman, joakim nivre, mitchell abrams, eliaackermann, no¨emi aepli, ˇzeljko agi´c, lars ahren-berg, chika kennedy ajede, gabriel˙e aleksan-draviˇci¯ut˙e, lene antonsen, katya aplonova, an-gelina aquino, maria jesus aranzabe, gashaw aru-tie, masayuki asahara, luma ateyah, furkan at-maca, mohammed attia, aitziber atutxa, lies-beth augustinus, elena badmaeva, miguel balles-teros, esha banerjee, sebastian bank, verginicabarbu mititelu, victoria basmov, colin batchelor,john bauer, kepa bengoetxea, yevgeni berzak, ir-shad ahmad bhat, riyaz ahmad bhat, erica bi-agetti, eckhard bick, agn˙e bielinskien˙e, rogierblokland, victoria bobicev, lo¨ıc boizou, emanuelborges v¨olker, carl b¨orstell, cristina bosco, gossebouma, sam bowman, adriane boyd, kristinabrokait˙e, aljoscha burchardt, marie candito,bernard caron, gauthier caron, tatiana cavalcanti,g¨uls¸en cebiro˘glu eryi˘git, flavio massimiliano cec-ˇc´epl¨o,chini, giuseppe g. a. celano, slavom´ırsavas cetin, fabricio chalub, ethan chi, jinhochoi, yongseok cho, jayeol chun, alessandra t.cignarella, silvie cinkov´a, aur´elie collomb, c¸ a˘grıc¸ ¨oltekin, miriam connor, marine courtin, eliza-beth davidson, marie-catherine de marneffe, vale-ria de paiva, elvis de souza, arantza diaz de ilar-raza, carly dickerson, bamba dione, peter dirix,kaja dobrovoljc, timothy dozat, kira droganova,puneet dwivedi, hanne eckhoff, marhaba eli, alielkahky, binyam ephrem, olga erina, tomaˇz er-javec, aline etienne, wograine evelyn, rich´ardfarkas, hector fernandez alcalde, jennifer fos-ter, cl´audia freitas, kazunori fujita, katar´ınagajdoˇsov´a, daniel galbraith, marcos garcia, moag¨ardenfors, sebastian garza, kim gerdes, filipginter, iakes goenaga, koldo gojenola, memduhg¨okırmak, yoav goldberg, xavier g´omez guino-vart, berta gonz´alez saavedra, bernadeta grici¯ut˙e,matias grioni, lo¨ıc grobol, normunds gr¯uz¯ıtis,bruno guillaume, c´eline guillot-barbance, tungag¨ung¨or, nizar habash, jan hajiˇc, jan hajiˇc jr., mikah¨am¨al¨ainen, linh h`a m˜y, na-rae han, kim har-ris, dag haug, johannes heinecke, oliver hellwig,felix hennig, barbora hladk´a, jaroslava hlav´aˇcov´a,florinel hociung, petter hohle,jena hwang,takumi ikeda, radu ion, elena irimia, o. l´aj´ıd´eishola, tom´aˇs jel´ınek, anders johannsen, hildurj´onsd´ottir, fredrik jørgensen, markus juutinen,h¨uner kas¸ıkara, andre kaasen, nadezhda kabaeva,sylvain kahane, hiroshi kanayama, jenna kan-erva, boris katz, tolga kayadelen, jessica ken-ney, v´aclava kettnerov´a, jesse kirchner, elena kle-mentieva, arne k¨ohn, abdullatif k¨oksal, kamil.
kopacewicz, timo korkiakangas, natalia kotsyba,jolanta kovalevskait˙e, simon krek, sookyoungkwak, veronika laippala, lorenzo lambertino, lu-cia lam, tatiana lando, septina dian larasati,john lee, phng lˆe h`ˆong,alexei lavrentiev,alessandro lenci, saran lertpradit, herman le-ung, maria levina, cheuk ying li, josie li,keying li, kyungtae lim, yuan li, nikolaljubeˇsi´c, olga loginova, olga lyashevskaya,teresa lynn, vivien macketanz, aibek makazhanov,michael mandl, christopher manning, ruli ma-nurung, c˘at˘alina m˘ar˘anduc, david mareˇcek, ka-trin marheinecke, h´ector mart´ınez alonso, andr´emartins, jan maˇsek, hiroshi matsuda, yuji mat-sumoto, ryan mcdonald, sarah mcguinness, gus-tavo mendonc¸a, niko miekka, margarita misir-pashayeva, anna missil¨a, c˘at˘alin mititelu, mariamitrofan, yusuke miyao, simonetta montemagni,amir more, laura moreno romero, keiko sophiemori, tomohiko morioka, shinsuke mori, shigekimoro, bjartur mortensen, bohdan moskalevskyi,kadri muischnek, robert munro, yugo murawaki,kaili m¨u¨urisep, pinkey nainwani,juan igna-cio navarro hor˜niacek, anna nedoluzhko, guntaneˇspore-b¯erzkalne, lng nguy˜ˆen thi., huy`ˆennguy˜ˆen thi.
minh, yoshihiro nikaido, vitaly niko-laev, rattima nitisaroj, hanna nurmi, stina ojala,atul kr.
ojha, ad´edayo.
ol´u`okun, mai omura,emeka onwuegbuzia, petya osenova, robert¨ostling, lilja øvrelid, s¸ aziye bet¨ul ¨ozates¸, arzu-can ¨ozg¨ur, balkız ¨ozt¨urk bas¸aran, niko partanen,elena pascual, marco passarotti, agnieszka pate-juk, guilherme paulino-passos, angelika peljak-łapi´nska, siyao peng, cenel-augusto perez, guyperrier, daria petrova, slav petrov, jason phelan,jussi piitulainen, tommi a pirinen, emily pitler,barbara plank, thierry poibeau, larisa ponomareva,martin popel, lauma pretkalnin¸a, sophie pr´evost,prokopis prokopidis, adam przepi´orkowski, ti-ina puolakainen, sampo pyysalo, peng qi, an-driela r¨a¨abis, alexandre rademaker, loganathanramasamy, taraka rama, carlos ramisch, vinitravishankar, livy real, petru rebeja, siva reddy,georg rehm,ivan riabov, michael rießler,erika rimkut˙e, larissa rinaldi, laura rituma,luisa rocha, mykhailo romanenko, rudolf rosa,valentin ros, ca, davide rovati, olga rudina, jackrueter, shoval sadde, benoˆıt sagot, shadi saleh,alessio salomoni, tanja samardˇzi´c, stephaniesamson, manuela sanguinetti, dage s¨arg, baibasaul¯ıte, yanin sawanakunanon, salvatore scarlata,nathan schneider, sebastian schuster, djam´e sed-dah, wolfgang seeker, mojgan seraji, mo shen,atsuko shimada, hiroyuki shirasu, muh shohibus-sirri, dmitry sichinava, aline silveira, natalia sil-veira, maria simi, radu simionescu, katalin simk´o,m´aria ˇsimkov´a, kiril simov, maria skachedubova,aaron smith, isabela soares-bastos, carolyn spa-dine, antonio stella, milan straka, jana strnadov´a,alane suhr, umut sulubacak, shingo suzuki, zsoltsz´ant´o, dima taji, yuta takahashi, fabio tam-burini, takaaki tanaka, samson tella,isabelletellier, guillaume thomas, liisi torga, marsida.
3148toska, trond trosterud, anna trukhina, reut tsar-faty, utku t¨urk, francis tyers, sumire uematsu,roman untilov, zdeˇnka ureˇsov´a, larraitz uria,hans uszkoreit, andrius utka, sowmya vajjala,daniel van niekerk, gertjan van noord, viktorvarga, eric villemonte de la clergerie, veronikavincze, aya wakasa, lars wallin, abigail walsh,jing xian wang, jonathan north washington, max-imilan wendt, paul widmer, seyi williams, matswir´en, christian wittern, tsegay woldemariam,tak-sum wong, alina wr´oblewska, mary yako,kayo yamashita, naoki yamazaki, chunxiao yan,koichi yasuoka, marat m. yavrumyan, zhuoran yu,zdenˇek ˇzabokrtsk´y, amir zeldes, hanzhi zhu, andanna zhuravleva.
2020. universal dependencies 2.6.lindat/clariah-cz digital library at the insti-tute of formal and applied linguistics ( ´ufal), fac-ulty of mathematics and physics, charles univer-sity..daniel zeman and philip resnik.
2008..cross-language parser adaptation between related lan-guages.
in proceedings of the ijcnlp-08 workshopon nlp for less privileged languages..3149a appendix.
language.
dataset.
domain.
newsnewsnews, literature.
hdt (subset)○ germanancora○ spanishsnk○ slovakbokmaal ndt news○ norwegiangdt○ greekø mandaringsdø vietnamese vtbø thaipudø cantonesehkø indonesiancsuitdt(cid:20) finnishbdt(cid:20) basquekaist(cid:20) koreangsd(cid:20) japaneseimst(cid:20) turkish(cid:24) arabicpadt(cid:24) hebrewhtb(cid:24) algeriannarabizi(cid:24) maltesemudt.
parliament, wikipedia, webwikipedianewsnews, wikipediamovies, parliamentnewsmanynewsliterature, news, academicnews, webnews, literaturenewsnewsweb, lyricsmany.
table 5: detailed description of the data used in part-of-speech tagging..3150language.
text type.
domain.
annotation examples train % dev/test %.
manualmanualmanualmanualmanualmanual.
○ german○ spanish○ slovak○ norwegian○ greek.
social media trainshotelsreviewsservicesreviewsreviewsmanysocial media politicssocial media manymobile phones user scoresreviewsø mandarinuser scoresmanyreviewsø vietnamese reviewsmanualtechnologyø thaisocial media product reviews manualø cantonesereviewsø indonesian reviews(cid:20) finnish(cid:20) basque(cid:20) korean(cid:20) japanese(cid:20) turkish.
user scoresfoodmanualmanymanualsocial media manyfood/lodging manualreviewsmoviesreviewsmanyreviewsfoodreviewsreviewsmanysocial media manysocial media manysocial media politicssocial media manysocial media manysocial media politics.
user scoresuser scoresmanualuser scoresmanualmanualmanualmanualmanualmanual.
(cid:24) hebrew(cid:24) algerian(cid:24) maltese.
(cid:24) arabic.
87061472512436086615195906198353400116004157811324633211294000014060105237501589195110110731718133.
1001001001003592100100100100100100100100100168445551001008416.
100100100100392239100100100100100100100100100100045551001008416.table 6: detailed description of the data used in sentiment analysis.
”train %” and ”dev/test %” indicate whatpercentage of the language’s training and validation/test data, respectively, comes from the dataset in question..figure 5: average transfer loss (in percentage points) to other languages of the same group (intra-group) andto languages that belong to the other groups (inter-group) in the part-of-speech tagging task after balancing in-language scores.
lower is better..3151intra-groupinter-grouptransfertype1015202530354045transferlossmberttraingroupfusionalisolatingagglutinativeintroﬂexiveintra-groupinter-grouptransfertype1015202530354045xlm-rintra-andinter-grouptransferlossesinpart-of-speech(balancedin-languagescores)predictor.
language task.
in-language scoreaverage example length (tokens)average example length (tokens)included in pretrainingincluded in pretrainingwords split into subword tokens (%)words split into subword tokens (%)proportion of positive examplesproportion of positive examplestransfer type (intra-group/inter-group).
traintraintesttraintesttraintesttraintest-.
bothbothbothbothbothbothbothsasaboth.
table 7: variables considered in the linear regression model after eliminating multicollinearity.
”language” indi-cates whether the predictor was measured on the ﬁne-tuning language (train) or the target language (test), ”sa”stands for sentiment analysis..train.
○ fusional.
ø isolating.
(cid:20) agglutinative.
(cid:24) introﬂexive.
test.
mbert xlm-r mbert xlm-r mbert xlm-r mbert xlm-r.train.
○ fusional.
ø isolating.
(cid:20) agglutinative.
test.
mbert xlm-r mbert xlm-r mbert xlm-r mbert xlm-r.○ fusionalø isolating(cid:20) agglutinative(cid:24) introﬂexive.
○ fusionalø isolating(cid:20) agglutinative(cid:24) introﬂexive.
67.646.554.439.9.
48.349.846.448.0.
71.351.955.241.9.
42.844.247.142.6.
51.848.853.337.4.
46.551.948.045.5.
51.449.250.936.7.
45.443.050.741.8.
51.047.555.236.9.
45.437.640.143.4.
52.049.654.834.8.
44.542.147.345.4.
54.154.247.045.746.749.742.243.2(cid:24) introﬂexive.
41.736.341.645.0.
42.443.043.545.2.table 8: group-to-group cross-lingual accuracy scores (%) for part-of-speech tagging (top) and macro f1 scores(%) in the sentiment analysis task (bottom) (after balancing in-language scores) for each ﬁne-tuning (column)and testing (row) morphological group, and each model architecture.
maximum values in each test group andarchitecture are highlighted.
higher is better..3152train.
○ fusional.
ø isolating.
(cid:20) agglutinative.
(cid:24) introﬂexive.
test.
mbert xlm-r mbert xlm-r mbert xlm-r mbert xlm-r.train.
○ fusional.
ø isolating.
(cid:20) agglutinative.
test.
mbert xlm-r mbert xlm-r mbert xlm-r mbert xlm-r.○ fusionalø isolating(cid:20) agglutinative(cid:24) introﬂexive.
○ fusionalø isolating(cid:20) agglutinative(cid:24) introﬂexive.
14.936.028.142.6.
21.419.923.321.7.
12.031.528.241.5.
20.018.615.720.2.
31.034.029.645.5.
24.519.123.025.5.
31.333.531.846.0.
16.118.510.719.6.
30.433.826.144.5.
25.333.130.527.3.
29.932.427.247.2.
18.020.515.217.1.
28.828.035.936.536.232.540.039.7(cid:24) introﬂexive.
28.834.228.925.4.
20.119.419.017.3.table 9: group-to-group transfer loss (in percentage points) in pos (top) and sentiment analysis (bottom) tasks (af-ter balancing in-language scores) for each ﬁne-tuning (column) and testing (row) language’s morphological group,as well as each model architecture.
minimum values in each ﬁne-tuning group and architecture are highlighted.
lower is better..figure 6: average transfer loss (in percentage points) to other languages of the same group (intra-group) and tolanguages that belong to the other groups (inter-group) in the sentiment analysis task after balancing in-languagescores.
lower is better..3153intra-groupinter-grouptransfertype1015202530transferlossmberttraingroupfusionalisolatingagglutinativeintroﬂexiveintra-groupinter-grouptransfertype141516171819202122xlm-rintra-andinter-grouptransferlossesinsentimentanalysis(balancedin-languagescores)