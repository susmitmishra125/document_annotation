select, extract and generate: neural keyphrase generationwith layer-wise coverage attention.
wasi uddin ahmad†∗, xiao bai‡, soomin lee‡, kai-wei chang††university of california, los angeles, ‡yahoo research†{wasiahmad,kwchang}@cs.ucla.edu‡{xbai,soominl}@verizonmedia.com.
abstract.
natural language processing techniques havedemonstrated promising results in keyphrasegeneration.
however, one of the major chal-lenges in neural keyphrase generation is pro-cessing long documents using deep neural net-works.
generally, documents are truncated be-fore given as inputs to neural networks.
conse-quently, the models may miss essential pointsconveyed in the target document.
to overcomethis limitation, we propose seg-net, a neuralkeyphrase generation model that is composedof two major components, (1) a selector thatselects the salient sentences in a document and(2) an extractor-generator that jointly extractsand generates keyphrases from the selectedsentences.
seg-net uses transformer, a self-attentive architecture, as the basic buildingblock with a novel layer-wise coverage atten-tion to summarize most of the points discussedin the document.
the experimental results onseven keyphrase generation benchmarks fromscientiﬁc and web documents demonstrate thatseg-net outperforms the state-of-the-art neu-ral generative methods by a large margin..title: [1] natural language processing technologiesfor developing a language learning environment .
abstract: [1] so far , computer assisted languagelearning ( call ) comes in many different ﬂavors.
[1] our research work focuses on developing anintegrated e learning environment that allows im-proving language skills in speciﬁc contexts .
[1]integrated e learning environment means that itis a web based solution .
.
.
, for instance , webbrowsers or email clients .
[0] it should be accessi-ble .
.
.
[1] natural language processing ( nlp ) formsthe technological basis for developing such a learn-ing framework .
[0] the paper gives an overview.
.
.
[0] therefore , on the one hand , it explains cre-ation .
.
.
[0] on the other hand , it describes existingnlp standards .
[0] based on our requirements , thepaper gives .
.
.
[1] .
.
.
necessary developments in elearning to keep in mind .
present: natural language processing; computerassisted language learning; integrated e learningabsent: semantic web technologies; learning offoreign languages.
figure 1: example of a document with present and ab-sent keyphrases.
the value (0/1) in brackets ([]) repre-sent sentence salience label..1.introduction.
keyphrases are short pieces of text that summa-rize the key points discussed in a document.
theyare useful for many natural language processingand information retrieval tasks (wilson et al., 2005;berend, 2011; tang et al., 2017; subramanian et al.,2018; zhang et al., 2017b; wan and xiao, 2008;jones and staveley, 1999; kim et al., 2013; hulthand megyesi, 2006; hammouda et al., 2005; wuand bolivar, 2008; dave and varma, 2010).
in theautomatic keyphrase generation task, the input is adocument, and the output is a set of keyphrases thatcan be categorized as present or absent keyphrases.
present keyphrases appear exactly in the target doc-.
∗work done during internship at yahoo research..ument, while absent keyphrases are only semanti-cally related and have partial or no overlap to thetarget document.
we provide an example of a targetdocument and its keyphrases in figure 1..in recent years, the neural sequence-to-sequence(seq2seq) framework (sutskever et al., 2014)has become the fundamental building block inkeyphrase generation models.
most of the existingapproaches (meng et al., 2017; chen et al., 2018;yuan et al., 2020; chen et al., 2019b) adopt theseq2seq framework with attention (luong et al.,2015; bahdanau et al., 2014) and copy mechanism(see et al., 2017; gu et al., 2016).
however, presentphrases indicate the indispensable segments of a.proceedingsofthe59thannualmeetingoftheassociationforcomputationallinguisticsandthe11thinternationaljointconferenceonnaturallanguageprocessing,pages1389–1404august1–6,2021.©2021associationforcomputationallinguistics1389target document.
emphasizing on those segmentsimproves document understanding that can lead amodel to coherent absent phrase generation.
thismotivates to jointly model keyphrase extraction andgeneration (chen et al., 2019a)..to generate a comprehensive set of keyphrases,reading the complete target document is necessary.
however, to the best of our knowledge, none of theprevious neural methods read the full content ofa document as it can be thousands of words long.
existing models truncate the target document; takethe ﬁrst few hundred words as input and ignorethe rest of the document that may contain salientinformation.
on the contrary, a signiﬁcant frac-tion of a long document may not associate with thekeyphrases.
presumably, selecting the salient seg-ments from the target document and then predictingthe keyphrases from them would be effective..to address the aforementioned challenges, inthis paper, we propose seg-net (stands for select,extract, and generate) that has two major compo-nents, (1) a sentence-selector that selects the salientsentences in a document, and (2) an extractor-generator that predicts the present keyphrases andgenerates the absent keyphrases jointly.
the moti-vation to design the sentence-selector is to decom-pose a long target document into a list of sentences,and identify the salient ones for keyphrase gener-ation.
we consider a sentence as salient if it con-tains present keyphrases or overlaps with absentkeyphrases.
as shown in figure 1, we split thedocument into a list of sentences and classify themwith salient and non-salient labels.
a similar notionis adopted in prior works on text summarization(chen and bansal, 2018; lebanoff et al., 2019) andquestion answering (min et al., 2018).
we employtransformer (vaswani et al., 2017) as the backboneof the extractor-generator in seg-net..we equip the extractor-generator with a novellayer-wise coverage attention such that the gener-ated keyphrases summarize the entire target doc-ument.
the layer-wise coverage attention keepstrack of the target document segments that arecovered by previously generated phrases to guidethe self-attention mechanism in transformer whileattending the encoded target document in futuregeneration steps.
we evaluate seg-net on ﬁvebenchmarks from scientiﬁc articles and two bench-marks from web documents to demonstrate its ef-fectiveness over the state-of-the-art neural gener-ative methods.
we perform ablation and analysis.
to show that selecting salient sentences improvepresent keyphrase extraction and the layer-wisecoverage attention and facilitates absent keyphrasegeneration.
our novel contributions are as follows.
1. seg-net that identiﬁes the salient sentencesin the target document ﬁrst and then use themto generate a set of keyphrases.
2. a layer-wise coverage attention..2 problem deﬁnition.
x, s2.
1, .
.
.
, ki.
x, .
.
.
, s|s|.
x ] where each sentence si.
keyphrase generation task is deﬁned as given atext document x, generate a set of keyphrasesk = {k1, k2, .
.
.
, k|k|} where the documentx = [x1, .
.
.
, x|x|] and each keyphrase ki =[ki|ki|] is a sequence of words.
a textdocument can be split into a list of sentences,sx = [s1x =[xj, .
.
.
, xj+|si|−1] is a consecutive subsequence ofthe document x with begin index j ≤ |x| and endindex (j + |si|) < |x|.
in literature, keyphrasesare categorized into two types, present and ab-sent.
a present keyphrase is a consecutive subse-quence of the document, while an absent keyphraseis not.
however, an absent keyphrase may havea partial overlapping with the document’s wordsequence.
we denote the sets of present and ab-p, .
.
.
, k|kp|sent keyphrases as kp = {k1} andka = {k1}, respectively.
hence, wecan express a set of keyphrases as k = kp ∪ ka.
seg-net decomposes the keyphrase generation.
a, .
.
.
, k|ka|.
p, k2.
a, k2.
a.p.task into three sub-tasks.
we deﬁne them below..task 1 (salient sentence selection).
given a listof sentences sx, predict a binary label (0/1) foreach sentence six. the label 1 indicates that the sen-tence contains a present keyphrase or overlaps withan absent keyphrase.
the output of the selector isa list of salient sentences s salx ..task 2 (present keyphrase extraction ).
givens salas a concatenated sequence of words, predictxa label (b/i/o) for each word that indicates if it is aconstituent of a present keyphrase..task 3 (absent keyphrase generation).
givens salas a concatenated sequence of words, gen-xerate a concatenated sequence of keyphrases in asequence-to-sequence fashion..3 seg-net for keyphrase generation.
our proposed model, seg-net jointly learns to ex-tract and generate present and absent keyphrases.
1390from the salient sentences in a target document.
the key advantage of seg-net is the maximalutilization of the information from the input textin order to generate a set of keyphrases that sum-marize all the key points in the target document.
seg-net consists of a sentence-selector and anextractor-generator.
the sentence-selector iden-tiﬁes the salient sentences from the target docu-ment (task 1) that are fed to the extractor-generatorto predict both the present and absent keyphrases(task 2, 3).
we detail them in this section..3.1 embedding layer.
the embedding layer maps each word in an in-put sequence to a low-dimensional vector space.
we train three embedding matrices, we, wpos, andwseg that convert a word, its absolute position, andsegment index into vector representations of sizedmodel.
the segment index of a word indicates theindex of the sentence that it belongs to.
in addition,we obtain a character-level embedding for eachword using convolutional neural networks (cnn)(kim, 2014a).
to learn a ﬁxed-length vector rep-resentation of a word, we add the four embeddingvectors element-wise.
to form the vector represen-tations of the keyphrase tokens, we only use theirword and character-level embeddings..3.2 sentence-selector.
the objective of the sentence-selector is to pre-dict the salient sentences in a document, as de-scribed in task 1. given a sentence, six =[xj, .
.
.
, xj+|si|−1] from a document x, the selec-tor predicts the salience probability of that in-put sentence.
first, the embedding layer mapseach word in the sentence into a dmodel dimen-sional vector.
the sequence of word vectorsare fed to a stack of transformer encoder layersthat produce a sequence of output representations[oj, .
.
.
, oj+|si|−1] where ot ∈ rdmodel.
then weapply max and mean pooling on the output repre-sentations to form smax, smean ∈ rdmodel that areconcatenated spool = smax ⊕ smean to form thesentence embedding vector.
we feed the vectorspool through a three-layer, batch-normalized (ioffeand szegedy, 2015) maxout network (goodfellowet al., 2013) to predict the salience probability..3.3 extractor-generator.
the extractor-generator module in seg-net takes alist of salient sentences from a document as an inputthat are concatenated to form a sequence of words.
figure 2: overview of the extractor-generator moduleof seg-net.
the major components are encoder, ex-tractor, and decoder.
the encoder encodes the salientsentences of the input document.
the extractor predictsthe present keyphrase’s constituent words while the de-coder generates the absent keyphrases word by word..and predicts the present and absent keyphrases.
weillustrate the extractor-generator module in figure2 and describe its major components as follows..encoder the encoder consists of an embeddinglayer followed by an l-layer transformer encoder.
each word in the input sequence [x1, .
.
.
, xn] isﬁrst mapped to an embedding vector.
then thesequence of word embeddings is fed to the trans-former encoder that produces contextualized wordrepresentations [ol1, .
.
.
, oln] where l = 1, .
.
.
, lusing the multi-head self-attention mechanism..extractorin a nutshell, the extractor acts as a3-way classiﬁer that predicts a tag for each word in.
1391the bio format.
the extractor takes [ol1 , .
.
.
, oln ]as input and predicts the probability of each wordbeing a constituent of a present keyphrase.
pj = softmax (cid:0)wr2(tanh(wr1ol.
j + br1)) + br2.
(cid:1),.
where wr1, wr2, br1, br2 are trainable parameters..1, .
.
.
, y∗.
decoder the decoder generates the absentkeyphrases as a concatenated sequence of words[y∗m] where m is the sum of the lengthof the phrases.
the decoder predicts the absentphrases word by word given previously predictedwords in a greedy fashion.
the decoder employs anembedding layer, l-layers of transformer decoderfollowed by a softmax layer.
the embedding layerconverts the words into vector representations thatare fed to the transformer decoder.
we use relativepositional encoding (shaw et al., 2018) to injectorder information of the keyphrase terms.
the out-put of the last (l-th) decoder layer hlm ispassed through a softmax layer to predict a proba-bility distribution over the vocabulary v ..1 , .
.
.
, hl.
p(y∗.
t |y∗.
1:t−1, x) = softmax(wvhl.
t + bv),.
(1).
document.
hence, we adopt the copying mecha-nism and use an additional attention layer to learnthe copy distribution on top of the decoder stack.
formally, we take the output from the last layern ] and compute the atten-t at time step tt .
then we compute.
of the encoder [oltion score of the decoder output hlas: att(olt ) = oli , hlthe context vector, cl.
i watthlt at time step t:.
1 , .
.
.
, ol.
alti =.
att(oli , hlt )k , hlk=1 exp(att(ol.
t )).
(cid:80)n.; cl.
t =.
tiolali ..n(cid:88).
i=1.
the copy mechanism uses the attention weightsalti as the probability distribution p (y∗t = xi|ut =1) = alti to copy the input tokens xi.
we computethe probability of using the copy mechanism at thedecoding step t as p(ut = 1) = σ(wu[hlt ] +bu), where || denotes the vector concatenation oper-ator.
then we obtain the ﬁnal probability distribu-tion for the output token y∗t ) = p (ut =0)p (y∗t |ut = 1) wherep (y∗t |ut = 0) is deﬁned in eq.
(1).
all probabili-ties are conditioned on y∗1:t−1, x, but we omit themto keep the notations simple..t |ut = 0) + p (ut = 1)p (y∗.
t as: p (y∗.
t ||cl.
where wv ∈ r|v |×dmodel and bword ∈ r|v |..3.4 learning objectives.
coverage attention the coverage attention (tuet al., 2016; yuan et al., 2020; chen et al., 2018)keeps track of the parts in the document that hasbeen covered by previously generated phrases andencourages future generation steps to summarizethe other segments of the target document.
theunderlying idea is to decay the attention weightsof the previously attended input tokens while de-coder attends the encoded input tokens at timestep, t. to equip the multi-layer structure of thetransformer with a layer-wise coverage attention,we adopt the layer-wise encoder-decoder attentiontechnique (he et al., 2018).
we compute the atten-e(cid:48)tion weights, αti =in encoder-decodertik=1 e(cid:48)tkattention at each layer where e(cid:48)ti is as follows..(cid:80)n.e(cid:48)ti =.
(cid:40)exp(eti)exp(eti)k=1 exp(eki).
(cid:80)t−1.
if t = 1otherwise,.
(2).
where eti is the scaled-dot product between thetarget token yt and the input token xi..copy attention absent keyphrases have partialor no overlapping with the target document.
withthe copy mechanism, we want the decoder to learnto copy phrase terms that overlap with the target.
we individually train the sentence-selector and theextractor-generator in seg-net..sentence-selector for each sentence in a doc-ument x, the selector predicts the salience la-bel.
we choose the sentences containing presentkeyphrases or overlap with absent keyphrases asthe gold salient sentences and use the weightedcross-entropy loss for selector training..1|x|.
|x|(cid:88).
j=1.
ls = −.
ωϑ∗.
j log ϑj + (1 − ϑ∗.
j ) log(1 − ϑj),.
(3)where ϑ∗j ∈ {0, 1} is the ground-truth label for thej-th sentence and ω is a hyper-parameter to balancethe importance of salient and non-salient sentences..extractor-generator the extractor-generatortakes a list of salient sentences as a concatenatedsequence of words.
for each word of the inputsequence, the extractor predicts whether the wordappears in a contiguous subsequence that matchesa present keyphrase.
the extractor treats the taskas a binary classiﬁcation task and we compute theextraction loss le as in eq.
(3)..the decoder in extractor-generator generates thelist of absent keyphrases in a sequence-to-sequence.
1392dataset.
# example.
kp20kinspeckrapivinnussemevalkptimesin-house.
20,00050040021110020,00026,000.max / avg.
source len.
1,438 / 179.8386 / 128.7554 / 182.6973 / 219.1473 / 234.87,569 / 777.99,745 / 969.1.max / avg.
# sentence108 / 7.823 / 5.528 / 8.242 / 11.822 / 11.9631 / 28.9538 / 35.6.
% sent(cid:63) max / avg.
kp len.
23 / 2.0410 / 2.486 / 2.2170 / 2.2211 / 2.3818 / 1.8416 / 2.69.
29.216.528.332.627.035.444.0.avg.
# kp5.289.835.8411.6514.665.274.08.
% pkp % akp.
62.973.655.754.442.658.837.5.
37.126.444.345.657.441.262.5.table 1: summary of the test portion of the keyphrase benchmarks used in experiments.
sent(cid:63) represents thepercentage of non-salient sentences in the input text.
% pkp and % akp indicate the percentage of present andabsent keyphrases, respectively..fashion.
we compute the negative log-likelihoodlg of the ground-truth keyphrases..lg = −.
log p(y∗.
t |y∗.
1, .
.
.
, y∗.
t−1, x),.
(4).
n(cid:88).
t=1.
where n is sum of the length of all absent phrases.
the overall loss to train the extractor-generator iscomputed as a weighted average of the extractionand generation loss, leg = βle + (1 − β)lg..4 experiment setup.
4.1 datasets and preprocessing.
we conduct experiments on ﬁve scientiﬁc bench-marks from the computer science domain: kp20k(meng et al., 2017), inspec (hulth, 2003), krapivin(krapivin et al., 2009), nus (nguyen and kan,2007), and semeval (kim et al., 2010).
each exam-ple from these datasets consists of the title, abstract,and a list of keyphrases.
following previous works(meng et al., 2017; chan et al., 2019; chen et al.,2019b,a; yuan et al., 2020), we use the trainingset of the largest dataset, kp20k, to train and em-ploy the testing datasets from all the benchmarksto evaluate the baselines and our models.
kp20kdataset consists of 530,000 and 20,000 articles fortraining and validation, respectively.
we removeall the articles from the training portion of kp20kthat overlaps with its validation set, or in any of theﬁve testing sets.
after ﬁltering, the kp20k datasetcontains 509,818 training examples that we use totrain all the baselines and our models..we perform experiments on two web-domaindatasets that consist of news articles and generalweb documents.
the ﬁrst dataset is kptimes (gal-lina et al., 2019) that provides news text paired witheditor-curated keyphrases.
the second dataset isan in-house dataset generated from the click logs.
of a large-scale commercial web search engine.
speciﬁcally, we randomly sampled web documentsthat were clicked at least once during the month offebruary in 2019. for each sampled web document,we collected 20 queries that led to the highest num-ber of clicks on it.
this design choice is motivatedby the observation that queries frequently leadingto clicks on a web document usually summarizethe main concepts in the document.
we furtherﬁlter out the less relevant queries by ranking thembased on the number of clicks.
the relevance scorefor each query is assigned by an in-house query-document relevance model.
we also remove dupli-cate queries by comparing their bag-of-words repre-sentation.1 the dataset consists of 206,000, 24,000,and 26,000 unique web documents for training, val-idation, and evaluation, respectively..statistics of the test portion of the experimentdatasets are provided in table 1 in appendix.
fol-lowing meng et al.
(2017), we apply lowercasing,tokenization and replacing digits with (cid:104)digit(cid:105) sym-bol to preprocess all the datasets.
we use spacy(honnibal et al., 2020) for tokenization and collect-ing the sentence boundaries..4.2 baseline models and evaluation metrics.
we compare the performance of seg-net with fourstate-of-the-art neural generative methods, catseq(yuan et al., 2020), catseqd (yuan et al., 2020), cat-seqcorr (chen et al., 2018), and catseqtg (chenet al., 2019b).
in addition, we consider the vanillatransformer (vaswani et al., 2017) as a baseline.
the catseq, catseqcorr and catseqtg models areknown as copyrnn (meng et al., 2017), corrrnn(chen et al., 2018) and tgnet (chen et al., 2019b)respectively.
copyrnn, corrrnn or tgnet gen-.
1we perform stemming before computing the bag-of-.
words representations..1393model.
kp20k.
krapivinf1@m f1@5 f1@m f1@5 f1@m f1@5 f1@m f1@5 f1@m f1@5.semeval.
inspec.
nus.
0.3670.3630.3650.3660.3680.379.present keyphrase generation0.291catseq0.285catseqdcatseqcorr0.2890.292catseqtg0.291transformer0.311seg-netabsent keyphrase generation0.015catseqcatseqd0.0150.015catseqcorr0.015catseqtg0.015transformer0.018seg-net.
0.0320.0310.0320.0320.0310.036.
0.2620.2630.2690.2700.2640.265.
0.0080.0110.0090.0110.0090.015.
0.2250.2190.2270.2290.2250.216.
0.0040.0060.0050.0050.0050.009.
0.3540.3490.3490.3660.3560.366.
0.0360.0370.0380.0340.0380.036.
0.2690.2640.2650.2820.2740.276.
0.0180.0180.0200.0180.0200.018.
0.3970.3940.3900.3930.4050.461.
0.0280.0240.0240.0180.0280.036.
0.3230.3210.3190.3250.3280.396.
0.0160.0150.0140.0110.0160.021.
0.2830.2740.2900.2900.2880.332.
0.0280.0240.0260.0270.0290.030.
0.2420.2330.2460.2460.2450.283.
0.0200.0160.0180.0190.0200.021.table 2: results of keyphrase prediction on the scientiﬁc benchmarks.
the bold-faced and underline values indicatethe best and statistically signiﬁcantly better (by paired bootstrap test, p < 0.05) performances across the board..erates one keyphrase in a sequence-to-sequencefashion and use beam search to generate multi-ple keyphrases.
in contrast, following chan et al.
(2019), we concatenate all the keyphrases into oneoutput sequence using a special delimiter (cid:104)sep(cid:105),and use greedy decoding during inference.
wetrain all the baselines using maximum-likelihoodobjective.
we use the publicly available implemen-tation of these baselines2 in our experiment..to measure the accuracy of the sentence-selector,we use averaged f1 score (macro).
we also com-pute precision and recall to compare the perfor-mance of the sentence-selector with a baseline.
while in seg-net, we select up to n predictedsalient sentences, in the baseline method, the ﬁrstn sentences are selected from the target documentso that their total length does not exceed a prede-ﬁned word limit (200 words).
in keyphrase genera-tion, the accuracy is typically computed by compar-ing the top k predicted keyphrases with the ground-truth keyphrases.
we follow chan et al.
(2019) toperform evaluation and report f1@m and f1@5for all the baselines and our models..4.3.implementation details.
hyper-parameters we use a ﬁxed vocabularyof the most frequent |v | = 50, 000 words in bothsentence-selector and extractor-generator.
we setdmodel = 512 for all the embedding vectors.
weset l = 6, h = 8, dk = 64, dv = 64, df f = 2, 048in transformer across all our models.
we detail the.
hyper-parameters in table 11 in appendix..training we perform grid search for β over [0.4,0.5, 0.6] on the dev set and found β = 0.5 resultsin the best performance.
loss weights for positivesamples ω are set to 0.7 and 2.0 during selector andextractor training.3 we train all our models usingadam (kingma and ba, 2015) with a batch sizeof 80 and a learning rate of 10−4.
during training,we use dropout and gradient clipping.
we halvethe learning rate when the validation performancedrops and stop training if it does not improve forﬁve successive iterations.
we train the sentence-selector and extractor-generator modules for a max-imum of 15 and 25 epochs, respectively.
trainingthe modules takes roughly 10 and 25 hours on twogeforce gtx 1080 gpus, respectively..decoding the absent keyphrases are generatedas a concatenated sequence of words.
hence, un-like prior works (meng et al., 2017; chen et al.,2018, 2019b,a; zhao and zhang, 2019), we usegreedy search as the decoding algorithm duringtesting, and we force the decoder never to outputthe same trigram more than once to avoid repeti-tions in the generated keyphrases.
this is accom-plished by not selecting the word that would createa trigram already exists in the previously decodedsequence.
it is a well-known technique utilized intext summarization (paulus et al., 2018)..we provide details about model implementations.
and references in appendix for reproducibility..2https://github.com/kenchan0226/keyphrase-generation-.
3the values are chosen by simply computing the fraction.
rl.
of the positive and negative samples..1394model.
kptimes.
in-house.
f1@m f1@5 f1@m f1@5.
0.4530.4560.4570.4650.4510.481.present keyphrase generation0.295catseq0.299catseqd0.302catseqcorrcatseqtg0.3100.296transformer0.367seg-netabsent keyphrase generation0.157catseq0.158catseqdcatseqcorr0.1580.155catseqtg0.148transformer0.169seg-net.
0.2270.2250.2250.2270.2180.237.
0.2550.2520.2470.2600.2580.298.
0.0410.0370.0370.0370.0420.047.
0.1020.1000.1000.1030.1110.161.
0.0200.0190.0190.0180.0200.024.table 3: keyphrase prediction results on the two webdomain benchmarks.
the bold-faced values and † in-dicate the best and statistically signiﬁcantly better (bypaired bootstrap test, p < 0.05) performances..model.
oraclecatseqcatseqdcatseqcorrcatseqtgseg-net.
present.
absent.
mae avg.
# mae avg.
#2.4320.0000.6592.2710.6292.2250.7032.2920.6382.2762.1851.140.
2.8373.7813.6943.7903.7803.796.
0.0001.9431.9611.9141.9561.324.table 4: evaluation on predicting the correct numberof keyphrases on the kp20k dataset.
mae stands formean absolute error and “avg.
#” indicates the averagenumber of generated keyphrases per document.
oracleis a model that generates the ground-truth keyphrases..5 results.
we compare our proposed model seg-net with thebaselines on the scientiﬁc and web domain datasets.
we present the experiment results in table 2 and 3..present keyphrase prediction from the results,it is evident that seg-net outperforms all the base-line methods by a signiﬁcant margin (p < 0.05,t-test) in 3 out of 5 scientiﬁc datasets and bothweb domain datasets.
unlike the baseline methods,seg-net extracts the present keyphrases from thesalient sentences, contributing most to the perfor-mance improvement.
in the krapivin dataset, theperformance is on par, while in the inspec dataset,seg-net performs worst in terms of f1@5. theperofrmance drop is explainable as inspect datasetconsists of shorter documents (see the average.
lengths in table 1).
in nus and semeval datasets,the performance improvements are noteworthy;5.6 and 4.2 f1@m points over the second-bestmethod.
the number of ground truth keyphrases inthose two datasets are higher than other scientiﬁcdatasets, and extracting present keyphrases booststhe performance (more discussion in § 6).
seg-netsigniﬁcantly improves the web domain datasets (3.1f1@5 points in kptimes and 5.0 f1@5 points inin-house datasets) over the best baseline methods,catseqtg, and transformer, respectively..absent keyphrase prediction unlike presentphrases, absent phrases do not appear exactly inthe target document.
hence, predicting them ismore challenging and requires a comprehensiveunderstanding of the underlying document seman-tic.
from table 2 and 3, we see that seg-netcorrectly generates more absent keyphrases thanthe baselines on all the experimental datasets, ex-cept krapivin.
to our surprise, seg-net resultsin a large performance improvement (1.0 pointsin terms of f1@m) in the kptimes dataset.
wesuspect that in kptimes dataset, the target absentkeyphrases are semantically associated with differ-ent segments of the document and thus generatingsuch keyphrases from the salient sentences resultsin larger improvements.
overall, the absent phraseprediction results indicate that seg-net is capableof understanding the underlying document seman-tic better than the baseline methods..number of generated keyphrases generatingan accurate number of keyphrases indicates mod-els’ understanding of the documents’ semantic.
asmall number of phrase predictions demonstrate amodel’s inability to identify all the key points; overgeneration implies a model’s wrong understandingof the crucial points.
hence, we compare seg-netwith all the baseline approaches for predicting theappropriate number of phrases.
we measure themean absolute error (mae) between the numberof generated keyphrases and the number of ground-truth keyphrases (chan et al., 2019).
the resultsfor kp20k are presented in table 4. the lowermaes for seg-net indicate it better understandsdocuments’ semantic.
however, in the kptimesdataset, we observe seg-net predicts more presentkeyphrases than the baselines (see table 3 in ap-pendix).
this is due to the extractive nature of seg-net, and documents having more closely relatedkeyphrases (e.g., seg-net predicts ground-truth.
1395present.
absent.
sun.0.311.
0.379.k02pk.f1@m f1@5 f1@m f1@50.018seg-net0.036w/o deg −0.004 −0.005 −0.0010.000w/o sss −0.008 −0.014 +0.001 +0.0010.000 −0.004 −0.002w/o lca +0.001seg-net0.0210.0360.3960.461w/o deg −0.028 −0.031 −0.002 −0.001w/o sss −0.044 −0.0520.000 +0.001w/o lca −0.004 −0.002 −0.004 −0.0030.0210.283w/o deg −0.010 −0.0090.000w/o sss −0.035 −0.032 −0.001 −0.001w/o lca −0.002 −0.002 −0.001 −0.0010.119w/o deg −0.022 −0.059 −0.008 −0.005w/o sss −0.038 −0.067 −0.028 −0.022w/o lca −0.005 −0.010 −0.030 −0.018.
l seg-netavemess seg-netemtpk.0.0300.000.
0.187.
0.332.
0.428.
0.367.i.table 5: ablation on seg-net without decoupling ex-traction and generation (deg), salient sentence selec-tion (sss), and layer-wise coverage attention (lca).
we preclude one design choice at a time..keyphrases: “google”, “apple” with other relevantkeyphrases: “line”, “amazon.com”.
see qualitativeexamples provided in appendix).
therefore, wesuggest future works to consider the dataset naturewhile judging models in this respect..6 analysis.
the differences between the transformer baselineand seg-net are (1) decoupling keyphrase extrac-tion and generation, (2) use salient sentences forkeyphrase prediction, and (3) layer-wise coverageattention.
we perform ablation on the three designchoices and present the results in table 5..decoupling extraction and generation seg-net extracts present keyphrases and generates ab-sent keyphrases as suggested in chen et al.
(2019a)with a difference in the extractor.
seg-net em-ploys a 3-way classiﬁer (to predict bio tags) thatenables consecutive present keyphrases extraction.
the ablation study shows that separating extractionand generation boosts present keyphrase prediction(as much as 2.8, 1.0, and 2.2 f1@m points in nus,semeval, and kptimes datasets, respectively)..salient sentence selection one of seg-net’skey contributions is the sentence-selector that iden-tiﬁes the salient sentences to minimize the risk ofmissing critical points due to truncating long tar-get documents (e.g., web documents).
the contri-bution of sentence-selector in present keyphrase.
(a) present keyphrase.
(b) absent keyphrase.
figure 3: test performance of different models on kp-times dataset.
the x-axis and y-axis indicates docu-ment length (# words) and f1@m score, respectively..prediction is evident from the ablation study.
theimpact of using salient sentences to generate ab-sent keyphrases is signiﬁcant for the web domaindatasets (e.g., 2.8 f1@m points in kptimes).
weshow the performances on kptimes test documentswith different length in figure 3 and the resultssuggest that seg-net improves absent keyphraseprediction signiﬁcantly for longer documents, andwe credit this to the sentence selector.
the selec-tor’s accuracy on the kp20k and kptimes datasetsare 78.2 and 73.7 in terms of (macro) f1 score.
we evaluate seg-net by providing the ground-truth salient sentences to quantify the improvementachievable with a perfect sentence-selector.
wefound that the present keyphrase prediction perfor-mance would have increased by 3.2 and 4.1 f1@mpoints with a perfect sentence-selector..we compare the sentence selector with the base-lines that select the ﬁrst n sentences from the targetdocument, and the results are presented in table6. seg-net’s selector has a higher precision thatindicates it processes input texts with more salientsentences.
on the other hand, the recall is substan-tially lower for the scientiﬁc domain due to false-negative predictions.
our experiments suggest thatsalient sentence selection positively impacts andhas additional room for improvement..1396[0-100)[100-200)[200-300)[300-400)[400-500)[500-600)[600-700)[700-7000)0.00.10.20.30.40.5catseqcorrcatseqdcatseqtgseg-net[0-100)[100-200)[200-300)[300-400)[400-500)[500-600)[600-700)[700-7000)0.00.10.20.30.4catseqcorrcatseqdcatseqtgseg-netdataset.
prec.
scientiﬁc domain84.5kp20k95.0inspec85.5krapivinnus91.8semeval97.1web domainkptimesin-house.
81.782.9.seg-net.
baseline.
recall.
prec..recall.
86.382.685.381.075.7.
44.949.1.
75.187.175.878.183.5.
73.066.8.
95.098.895.192.090.3.
45.751.3.table 6: precision and recall computed by selecting npredicted salient sentences in seg-net, and the ﬁrstn sentences from the target documents in the base-lines.
we set n for each target document so that thetotal length of the selected sentences does not exceeda limit of 200 words.
it is important to note that thebaseline recall is close to 100.0 for the scientiﬁc do-main datasets because the average length of the targetdocuments from that domain is closer to 200 words..layer-wise coverage attention the ablationstudy shows the positive impact of the layer-wisecoverage attention in seg-net.
the improvementin absent keyphrase generation for the kptimesdataset (3.0 f1@m points) is signiﬁcant, while itis relatively small in other experiment datasets.
wehypothesize that the coverage attention helps whenkeyphrases summarize concepts expressed in dif-ferent segments of a long document.
we conﬁrmour hypothesis by observing the performance trendwith and without the coverage attention mechanism(we observe a similar trend as in figure 3)..we provide additional experiment results and.
qualitative examples in appendix..7 related work.
keyphrase extraction approaches identify impor-tant phrases that appear in a document.
the exist-ing approaches generally work in two steps.
first,they select a set of candidate keyphrases based onheuristic rules (hulth, 2003; medelyan et al., 2008;liu et al., 2011; wang et al., 2016).
the selectedkeyphrases are scored as per their importance inthe second step, which is computed by unsuper-vised ranking approaches (wan and xiao, 2008;grineva et al., 2009) or supervised learning algo-rithms (hulth, 2003; witten et al., 2005; medelyanet al., 2009; nguyen and kan, 2007; lopez andromary, 2010).
finally, the top-ranked candidatesare returned as the keyphrases.
another pool ofextractive solutions follows a sequence tagging ap-.
proach (luan et al., 2017; zhang et al., 2016; gol-lapalli et al., 2017; gollapalli and caragea, 2014).
however, the extractive solutions are only able topredict the keyphrases that appear in the documentand thus fail to predict the absent keyphrases..keyphrase generation methods aim at predictingboth the present and absent phrases.
meng et al.
(2017) proposed the ﬁrst generative model, knownas copyrnn, which is composed of attention (bah-danau et al., 2014; luong et al., 2015) and copymechanism (gu et al., 2016; see et al., 2017).
mul-tiple extensions of copyrnn were proposed insubsequent works (chen et al., 2018, 2019b).
dif-ferent from these approaches, zhang et al.
(2017a)proposed copycnn that utilizes convolutional neu-ral network (cnn) (kim, 2014b) to form sequence-to-sequence architecture.
however, these genera-tion methods are trained to predict one keyphrasefrom the target document.
in contrast, yuan et al.
(2020) proposed to concatenate all the ground-truthkeyphrases and train models to generate them asone output sequence..other noteworthy approaches in literature utilizedata from external source (chen et al., 2019a), syn-tactic supervision (zhao and zhang, 2019), semi-supervised learning (ye and wang, 2018), rein-forcement learning (chan et al., 2019), adversarialtraining (swaminathan et al., 2020), unlikelihoodtraining (bahuleyan and el asri, 2020) to improvekeyphrase generation..8 conclusion.
this paper presents seg-net, a keyphrase genera-tion model that identiﬁes the salient sentences in atarget document to utilize maximal information forkeyphrase prediction.
in seg-net, we incorporatea novel layer-wise coverage attention to cover allthe critical points in a document and diversify thepresent and absent keyphrases.
we evaluate seg-net on seven benchmarks from scientiﬁc and webdocuments, and the experiment results demonstrateseg-net’s effectiveness over the state-of-the-artmethods on both domains..references.
dzmitry bahdanau, kyunghyun cho, and yoshua ben-gio.
2014. neural machine translation by jointlylearning to align and translate.
in international con-ference on learning representations..hareesh bahuleyan and layla el asri.
2020. diversekeyphrase generation with neural unlikelihood train-.
1397ing.
in proceedings of the 28th international con-ference on computational linguistics, pages 5271–5287, barcelona, spain (online).
international com-mittee on computational linguistics..sujatha das gollapalli and cornelia caragea.
2014.extracting keyphrases from research papers using ci-tation networks.
in proceedings of the aaai confer-ence on artiﬁcial intelligence, volume 28..g´abor berend.
2011. opinion expression mining byexploiting keyphrase extraction.
in proceedings of5th international joint conference on natural lan-guage processing, pages 1162–1170, chiang mai,thailand.
asian federation of natural languageprocessing..hou pong chan, wang chen, lu wang, and irwin king.
2019. neural keyphrase generation via reinforce-in proceed-ment learning with adaptive rewards.
ings of the 57th annual meeting of the associationfor computational linguistics, pages 2163–2174,florence, italy.
association for computational lin-guistics..jun chen, xiaoming zhang, yu wu, zhao yan, andzhoujun li.
2018. keyphrase generation with corre-lation constraints.
in proceedings of the 2018 con-ference on empirical methods in natural languageprocessing, pages 4057–4066, brussels, belgium.
association for computational linguistics..wang chen, hou pong chan, piji li, lidong bing,and irwin king.
2019a.
an integrated approach forkeyphrase generation via exploring the power of re-in proceedings of the 2019trieval and extraction.
conference of the north american chapter of theassociation for computational linguistics: humanlanguage technologies, volume 1 (long and shortpapers), pages 2846–2856, minneapolis, minnesota.
association for computational linguistics..wang chen, yifan gao, jiani zhang, irwin king, andmichael r lyu.
2019b.
title-guided encoding forkeyphrase generation.
in proceedings of the aaaiconference on artiﬁcial intelligence, volume 33,pages 6268–6275..yen-chun chen and mohit bansal.
2018. fast abstrac-tive summarization with reinforce-selected sentencerewriting.
in proceedings of the 56th annual meet-ing of the association for computational linguis-tics (volume 1: long papers), pages 675–686, mel-bourne, australia.
association for computationallinguistics..kushal s dave and vasudeva varma.
2010. patternbased keyword extraction for contextual advertising.
in proceedings of the 19th acm international con-ference on information and knowledge management,pages 1885–1888.
acm..ygor gallina, florian boudin, and beatrice daille.
2019. kptimes: a large-scale dataset for keyphrasein proceedings ofgeneration on news documents.
the 12th international conference on natural lan-guage generation, pages 130–135, tokyo, japan.
association for computational linguistics..sujatha das gollapalli, xiao-li li, and peng yang.
incorporating expert knowledge into2017.thekeyphrase extraction.
aaai conference on artiﬁcial intelligence, page3180–3187..in proceedings of.
ian j goodfellow, david warde-farley, mehdi mirza,aaron courville, and yoshua bengio.
2013. max-in proceedings of the 30th inter-out networks.
national conference on machine learning, pages1319–1327..maria grineva, maxim grinev, and dmitry lizorkin.
2009. extracting key terms from noisy and multi-theme documents.
in proceedings of the 18th inter-national conference on world wide web, pages 661–670. acm..jiatao gu, zhengdong lu, hang li, and victor o.k.
incorporating copying mechanism inli.
2016.in proceedings ofsequence-to-sequence learning.
the 54th annual meeting of the association for com-putational linguistics (volume 1: long papers),pages 1631–1640, berlin, germany.
association forcomputational linguistics..khaled m hammouda, diego n matute, and mo-hamed s kamel.
2005. corephrase: keyphrase ex-in internationaltraction for document clustering.
workshop on machine learning and data mining inpattern recognition, pages 265–274.
springer..tianyu he, xu tan, yingce xia, di he, tao qin, zhibochen, and tie-yan liu.
2018. layer-wise coordi-nation between encoder and decoder for neural ma-in advances in neural informa-chine translation.
tion processing systems, pages 7944–7954..matthew honnibal,.
ines montani, soﬁe van lan-spacy:and adriane boyd.
2020.deghem,industrial-strength natural language processing inpython..anette hulth.
2003. improved automatic keyword ex-in pro-traction given more linguistic knowledge.
ceedings of the 2003 conference on empirical meth-ods in natural language processing, pages 216–223..anette hulth and be´ata b. megyesi.
2006. a study onautomatically extracted keywords in text categoriza-tion.
in proceedings of the 21st international con-ference on computational linguistics and 44th an-nual meeting of the association for computationallinguistics, pages 537–544, sydney, australia.
as-sociation for computational linguistics..sergey ioffe and christian szegedy.
2015. batch nor-malization: accelerating deep network training byin proceedingsreducing internal covariate shift.
of the 32nd international conference on machinelearning7, pages 448–456..1398steve jones and mark s staveley.
1999..phrasier:a system for interactive document retrieval usingkeyphrases.
in proceedings of the 22nd annual in-ternational acm sigir conference on research anddevelopment in information retrieval, pages 160–167. acm..su nam kim, olena medelyan, min-yen kan, andtimothy baldwin.
2010. semeval-2010 task 5 : au-tomatic keyphrase extraction from scientiﬁc articles.
in proceedings of the 5th international workshop onsemantic evaluation, pages 21–26, uppsala, swe-den.
association for computational linguistics..yoon kim.
2014a.
convolutional neural networksin proceedings of thefor sentence classiﬁcation.
2014 conference on empirical methods in naturallanguage processing (emnlp), pages 1746–1751,doha, qatar.
association for computational lin-guistics..yoon kim.
2014b.
convolutional neural networksin proceedings of thefor sentence classiﬁcation.
2014 conference on empirical methods in naturallanguage processing (emnlp), pages 1746–1751,doha, qatar.
association for computational lin-guistics..youngsam kim, munhyong kim, andrew cattle, juliaotmakhova, suzi park, and hyopil shin.
2013. ap-plying graph-based keyword extraction to documentretrieval.
in proceedings of the sixth internationaljoint conference on natural language processing,pages 864–868, nagoya, japan.
asian federation ofnatural language processing..diederik p kingma and jimmy ba.
2015. adam: amethod for stochastic optimization.
in internationalconference on learning representations..mikalai krapivin, aliaksandr autaeu, and mauriziomarchese.
2009. large dataset for keyphrases ex-traction.
technical report, university of trento..logan lebanoff, kaiqiang song, franck dernoncourt,doo soon kim, seokhwan kim, walter chang, andfei liu.
2019. scoring sentence singletons and pairsfor abstractive summarization.
in proceedings of the57th annual meeting of the association for com-putational linguistics, pages 2175–2189, florence,italy.
association for computational linguistics..zhiyuan liu, xinxiong chen, yabin zheng, andmaosong sun.
2011. automatic keyphrase extrac-tion by bridging vocabulary gap.
in proceedings ofthe fifteenth conference on computational naturallanguage learning, pages 135–144, portland, ore-gon, usa.
association for computational linguis-tics..patrice lopez and laurent romary.
2010. humb:automatic key term extraction from scientiﬁc arti-in proceedings of the 5th inter-cles in grobid.
national workshop on semantic evaluation, pages248–251, uppsala, sweden.
association for compu-tational linguistics..yi luan, mari ostendorf, and hannaneh hajishirzi.
2017. scientiﬁc information extraction with semi-in proceedings of thesupervised neural tagging.
2017 conference on empirical methods in natu-ral language processing, pages 2641–2651, copen-hagen, denmark.
association for computationallinguistics..thang luong, hieu pham, and christopher d. man-ning.
2015. effective approaches to attention-basedin proceedings of theneural machine translation.
2015 conference on empirical methods in natu-ral language processing, pages 1412–1421, lis-bon, portugal.
association for computational lin-guistics..olena medelyan, eibe frank, and ian h. witten.
2009. human-competitive tagging using automaticin proceedings of the 2009keyphrase extraction.
conference on empirical methods in natural lan-guage processing, pages 1318–1327, singapore.
as-sociation for computational linguistics..olena medelyan, ian h witten, and david milne.
2008.in proceedings oftopic indexing with wikipedia.
the aaai wikiai workshop, volume 1, pages 19–24..rui meng, sanqiang zhao, shuguang han, daqinghe, peter brusilovsky, and yu chi.
2017. deepin proceedings of the 55thkeyphrase generation.
annual meeting of the association for computa-tional linguistics (volume 1: long papers), pages582–592, vancouver, canada.
association for com-putational linguistics..sewon min, victor zhong, richard socher, and caim-ing xiong.
2018. efﬁcient and robust question an-swering from minimal context over documents.
inproceedings of the 56th annual meeting of the as-sociation for computational linguistics (volume 1:long papers), pages 1725–1735, melbourne, aus-tralia.
association for computational linguistics..thuy dung nguyen and min-yen kan. 2007.keyphrase extraction in scientiﬁc publications.
inproceedings of the 10th international conference onasian digital libraries, pages 317–326.
springer..romain paulus, caiming xiong, and richard socher.
2018. a deep reinforced model for abstractive sum-marization.
in international conference on learn-ing representations..abigail see, peter j. liu, and christopher d. manning.
2017. get to the point: summarization with pointer-generator networks.
in proceedings of the 55th an-nual meeting of the association for computationallinguistics (volume 1: long papers), pages 1073–1083, vancouver, canada.
association for computa-tional linguistics..peter shaw, jakob uszkoreit, and ashish vaswani.
2018. self-attention with relative position represen-in proceedings of the 2018 conference oftations.
the north american chapter of the association for.
1399on empirical methods in natural language process-ing, pages 347–354, vancouver, british columbia,canada.
association for computational linguistics..ian h witten, gordon w paynter, eibe frank, carlgutwin, and craig g nevill-manning.
2005. kea:practical automated keyphrase extraction.
in designand usability of digital libraries: case studies inthe asia paciﬁc, pages 129–152.
igi global..xiaoyuan wu and alvaro bolivar.
2008. keyword ex-in proceed-traction for contextual advertisement.
ings of the 17th international conference on worldwide web, pages 1195–1196.
acm..hai ye and lu wang.
2018. semi-supervised learningfor neural keyphrase generation.
in proceedings ofthe 2018 conference on empirical methods in nat-ural language processing, pages 4142–4153, brus-sels, belgium.
association for computational lin-guistics..xingdi yuan, tong wang, rui meng, khushboothaker, peter brusilovsky, daqing he, and adamtrischler.
2020. one size does not ﬁt all: generatingand evaluating variable number of keyphrases.
inproceedings of the 58th annual meeting of the asso-ciation for computational linguistics, pages 7961–7975, online.
association for computational lin-guistics..qi zhang, yang wang, yeyun gong, and xuanjinghuang.
2016. keyphrase extraction using deep re-in proceed-current neural networks on twitter.
ings of the 2016 conference on empirical methodsin natural language processing, pages 836–845,austin, texas.
association for computational lin-guistics..yong zhang, yang fang, and xiao weidong.
2017a.
deep keyphrase generation with a convolutional se-in 2017 4th interna-quence to sequence model.
tional conference on systems and informatics (ic-sai), pages 1477–1485.
ieee..yuxiang zhang, yaocheng chang, xiaoqing liu, su-jatha das gollapalli, xiaoli li, and chunjing xiao.
2017b.
mike: keyphrase extraction by integratingin proceedings ofmultidimensional information.
the 2017 acm on conference on information andknowledge management, pages 1349–1358.
acm..jing zhao and yuxiang zhang.
2019..incorporat-ing linguistic constraints into keyphrase generation.
in proceedings of the 57th annual meeting of theassociation for computational linguistics, pages5224–5233, florence, italy.
association for compu-tational linguistics..computational linguistics: human language tech-nologies, volume 2 (short papers), pages 464–468,new orleans, louisiana.
association for computa-tional linguistics..sandeep subramanian, tong wang, xingdi yuan,saizheng zhang, adam trischler, and yoshua ben-gio.
2018. neural models for key phrase extrac-tion and question generation.
in proceedings of theworkshop on machine reading for question answer-ing, pages 78–88, melbourne, australia.
associa-tion for computational linguistics..ilya sutskever, oriol vinyals, and quoc v le.
2014.sequence to sequence learning with neural networks.
in advances in neural information processing sys-tems, pages 3104–3112.
curran associates, inc..avinash swaminathan, haimin zhang, debanjan ma-hata, rakesh gosangi, rajiv ratn shah, andamanda stent.
2020. a preliminary exploration ofgans for keyphrase generation.
in proceedings ofthe 2020 conference on empirical methods in nat-ural language processing (emnlp), pages 8021–8030, online.
association for computational lin-guistics..yixuan tang, weilong huang, qi liu, anthony khtung, xiaoli wang, jisong yang, and beibei zhang.
2017. qalink: enriching text documents with rele-vant q&a site contents.
in proceedings of the 2017acm on conference on information and knowledgemanagement, pages 1359–1368.
acm..zhaopeng tu, zhengdong lu, yang liu, xiaohua liu,and hang li.
2016. modeling coverage for neuralmachine translation.
in proceedings of the 54th an-nual meeting of the association for computationallinguistics (volume 1: long papers), pages 76–85, berlin, germany.
association for computationallinguistics..ashish vaswani, noam shazeer, niki parmar, jakobuszkoreit, llion jones, aidan n gomez, łukaszkaiser, and illia polosukhin.
2017. attention is allyou need.
in advances in neural information pro-cessing systems, pages 5998–6008..xiaojun wan and jianguo xiao.
2008. single doc-ument keyphrase extraction using neighborhoodin proceedings of the 23rd nationalknowledge.
conference on artiﬁcial intelligence, volume 8,pages 855–860..minmei wang, bo zhao, and yihua huang.
2016.ptr: phrase-based topical ranking for automatickeyphrase extraction in scientiﬁc publications.
in in-ternational conference on neural information pro-cessing, pages 120–128.
springer..theresa wilson, janyce wiebe, and paul hoffmann.
2005. recognizing contextual polarity in phrase-level sentiment analysis.
in proceedings of humanlanguage technology conference and conference.
1400supplementary material: appendices.
model.
catseqcatseqdcatseqcorrcatseqtgseg-net.
presentf1@m f1@50.2980.3760.2930.3720.3000.3750.3020.3740.3260.390.absentf1@m f1@50.0160.0340.0160.0330.0160.0340.0160.0330.0210.042.model.
kp20kcatseqtgseg-netkptimescatseqtgseg-net.
presentf1@m f1@5.absentf1@m f1@5.
0.3860.380.
0.3210.311.
0.0500.052.
0.0270.030.
0.4810.475.
0.3180.358.
0.2380.245.
0.1740.181.table 7: test set results on the kp20k dataset with“name variations” as proposed in chan et al.
(2019)..table 9: test set results after ﬁne-tuning the models viarl as proposed in chan et al.
(2019)..input features.
w/o character emb.
w/o segment emb..k seg-net02pks seg-netemtpke seg-net.
i.w/o character emb.
w/o segment emb..suoh-ni.w/o character emb.
w/o segment emb..presentf1@m f1@50.3110.3790.3090.3760.3100.3780.3670.4810.3320.4620.3650.4750.1610.2980.1520.2840.1590.295.table 8: impact of different embeddings at the inputlayer in seg-net..a additional ablation study.
variation of named entities a keyphrase canbe expressed in different ways, such as “solid statedrive” as “ssd” or “electronic commerce” as “ecommerce” etc.
a model should receive credit if itgenerates any of those variations.
hence, chan et al.
(2019) aggregated name variations of the ground-truth keyphrases from the kp20k evaluation datasetusing the wikipedia knowledge base.
we evaluateour model on that enriched evaluation set, and theexperimental results are listed in table 7. we ob-served that although seg-net extracts the presentkeyphrases, it can predict present phrases with vari-ations such as “support vector machine” and “svm”if they co-exist in the target document..impact of embedding features the embeddinglayer of extractor-generator learns four differentembedding vectors: word embedding, positionembedding, character-level embedding, and seg-ment embedding that are element-wise added.
weremove character embedding and segment em-bedding and observe slight performance drop inpresent keyphrase prediction.
the results are pre-.
model.
oraclecatseqcatseqdcatseqcorrcatseqtgseg-net.
present.
absent.
mae avg.
# mae avg.
#1.9780.0002.3971.4372.5231.4312.5201.4692.3421.3782.1962.209.
0.0001.2971.3691.3731.2841.291.
3.0542.1412.1932.2772.3094.650.table 10: evaluation on predicting the correct numberof keyphrases on the kptimes dataset.
mae stands formean absolute error and “avg.
#” indicates the averagenumber of generated keyphrases per document.
oracleis a model that generates the ground-truth keyphrases..sented in table 8. the character embeddings areemployed as we limit the vocabulary to the mostfrequent v words.
during our preliminary experi-ment, we observed that character embeddings havea notable impact in the web domain, where theactual vocabulary size can be large.
the additionof segment embedding is also helpful, speciallythe sentence-selector may predict salient sentencesfrom any part of the document.
we hypothesizethat the sentence index guides the self-attentionmechanism in the extractor-generator..fine-tuning via reinforcement learning fol-lowing chan et al.
(2019), we apply reinforcementlearning (rl) to ﬁne-tune the extractor-generatormodule of seg-net on absent keyphrase genera-tion.
as we can see from table 9, due to rl ﬁne-tuning, the absent keyphrase generation improvessigniﬁcantly, which corroborates with the ﬁndingsof chan et al.
(2019).
while ﬁne-tuning catseqtgmodel via rl helps present keyphrase generation inkp20k, it does not help in kptimes dataset.
sinceseg-net extracts the present keyphrases, their pre-dictions do not beneﬁt from the rl ﬁne-tuning step(instead, performance drops slightly)..1401model.
kp20kinspeckrapivinnussemevalkptimesin-house.
present.
absent.
f1@10 f1@o f1@10 f1@o0.0270.2010.0110.1400.0250.1720.0220.2700.0180.1990.2080.2440.0350.094.
0.3500.2010.3150.3780.2580.4640.282.
0.0120.0050.0110.0130.0140.1220.014.table 12: present and absent keyphrase prediction re-sults on the experiment datasets..vocabulary size, |v |# cnn ﬁltersmodel size, dmodelencoder layersdecoder layersh, dk, dv, df fdropoutoptimizerlearning ratelearning rate decaybatch sizemaximum gradient norm# params (sentence-selector)# params (extractor-generator).
50,000512 1d512668, 64, 64, 20480.2adam0.00010.5801.041.6m54.2m.
table 11: hyper-parameters used to train seg-net.
weuse the same setup for the transformer model..b evaluation metrics.
we want to draw attention to a crucial detail aboutthe evaluation metric setup.
due to differences inpost-processing before computing the evaluationmetric values, the reported scores in papers dif-fer.
recent works in literature mostly follow either.
evaluation metric implementation from chan et al.
(2019) or yuan et al.
(2020).
both works haveshared their implementation publicly available, andwe use the implementation of chan et al.
(2019).
we reported f1@5 and f1@m scores in thiswork, where m denotes the number of predictedkeyphrases.
we also compute f1@10 and f1@o,where o represents the number of ground truthkeyphrases, and the results are presented in table12. many prior works have reported r@10 andr@50 for absent phrase generation.
to computer@50, we need to perform beam decoding to gen-erate many keyphrases, typically more than 200(yuan et al., 2020).
in our opinion, generatinghundreds of keyphrases from a document does nottruly reﬂect the models’ ability in understandingdocument semantic.
therefore, we do not prefer toassess models’ ability in terms of r@50 metrics..c qualitative analysis.
we provide a few qualitative examples in figure 4..d reproducibility references.
• we train and test the ﬁrst four baseline modelsusing their public implementation.
we use thetransformer implementation from opennmtfor catseq (transformer) and seg-net..• we adopt the implementation of paired boot-strap test script to perform signiﬁcance test..• the preprocessed scientiﬁc article datasets are.
available here..• kptimes dataset is available here..1402title: smart speakers powered by voice agents seen ushering in era of ai.
article: major tech ﬁrms have been keen to sell speakers equipped with voice - based artiﬁcial in-telligence agents recently .
[eos] the debuts of smart speakers are seen as the prelude to an ai era ,ushering in a new technological age in which virtual assistants are expected to become as ubiquitousas smartphones , allowing people to connect to the internet by voice with greater ease .
[eos] whetherthese speakers will really take off and whether the technology will be popular in japan remain to be seen .
[eos] the following questions and answers explore these issues as well as why ai speakers are creating abuzz and what will be the role of japanese ﬁrms in this ﬁeld .
[eos] what makes ai speakers special ?
they look like normal portable home speakers , but one big difference is that they communicate withusers verbally .
[eos] users can tell the speakers to play music , search the internet , pull up weatherforecasts , send text messages , make phone calls and perform other daily tasks .
[eos] ... (truncated).
[catseq] smartphones ; artiﬁcial intelligence ; science and technology.
[seg-net] smart speakers ; smartphones ; ai ; japan ; speakers ; google ; apple ; computers and theinternet ; tech industry.
[ground-truth] google ; apple ; line ; ai ; amazon.com ; iot.
title: how much do you know about dengue fever ?.
article: the health ministry has conﬁrmed the ﬁrst domestic dengue fever case in japan in nearly 70years .
[eos] a saitama prefecture teen girl was found wednesday to have contracted the virus through amosquito in japan , followed by news that two more people — a man and a woman in tokyo — havealso been infected .
[eos] more than 200 dengue cases are reported in japan each year , but those are ofpatients who contracted dengue virus abroad .
[eos] the world health organization estimates the numberof infections across the globe to be 50 million to 100 million per year .
[eos] while the news has led towidespread fears that a pandemic outbreak might have arrived , experts are quick to deny such a scenario, while offering some advice on what measures people can take to minimize their exposure .
[eos]following are some basic questions and answers regarding the infectious disease and measures that canbe taken to prevent infection .
[eos] what is dengue fever and what causes it ?
dengue fever is a tropicalviral disease , also known as dengue hemorrhagic fever or break - bone fever , ... (truncated).
[catseq] dengue fever ; japan; medicine and health.
[seg-net] dengue fever ; japan ; dengue ; dengue virus ; health organization ; mosquitoes ; vaccinesimmunization.
[ground-truth] dengue fever ; world health organization ; dengue virus ; infectious diseases.
title: photo report : foodex japan 2013.article: foodex is the largest trade exhibition for food and drinks in asia , with about 70,000 visitorschecking out the products presented by hundreds of participating companies .
[eos] i was lucky to enteras press ; otherwise , visitors must be afﬁliated with the food industry — and pay ¥ 5,000 — to enter .
[eos] the foodex menu is global , including everything from cherry beer from germany and premiummexican tequila to top - class french and chinese dumplings .
[eos] the event was a rare chance to tryout both well - known and exotic foods and even see professionals making them .
[eos] in addition tobooths offering traditional japanese favorites such as udon and maguro sashimi , there were plenty ofinnovative twists , such as dorayaki , a sweet snack made of two pancakes and a red - bean ﬁlling , thatcame in coffee and tomato ﬂavors .
[eos] ... (truncated).
[catseq] japan ; agriculture.
[seg-net] foodex japan ; foodex ; food ; japan ; international trade and world market ; snack food.
[ground-truth] foodex ; japanese food ; japan pulse.
1403title: majority of australian women sexually harassed at work : survey.
article: kuala lumpur - two in three australian women have been sexually harassed at work , with themajority of cases unreported , according to a survey released on tuesday that highlighted challengesactivists said prevent women from advancing in their careers .
[eos] some 64 percent of women and35 percent of men said they had been harassed at their current or former workplace , according tothe survey of over 9,600 people by the australian council of trade unions , the country ’s main grouprepresenting workers .
[eos] the majority of those surveyed said they were subjected to offensivebehavior or unwanted sexual attention .
[eos] however only about a quarter of them made formalcomplaints , due to fears of repercussion , the survey found .
[eos] “ everyone should go to work freefrom the fear of harassment and unwanted sexual attention , ” the council ’s president , michele o’neil, said in a statement .
[eos] “ for many people — mainly women — today in australia this is not thereality .
[eos] our workplace laws have failed women who are experiencing harassment at work .
[eos]” campaigners said sexual harassment creates a workplace environment that is discriminatory towardswomen , which can prevent them from moving forward in their careers .
[eos] “ sexual harassmentin the workplace closes off women ’s opportunities and supports the attitudes that make violence morelikely , ” merrindahl andrew , from the australian women against violence alliance , said by email .
[eos]australia was ranked 35 out of 144 countries in the world economic forum ’s 2017 gender gap index , upfrom 46 in 2016 due to greater female representation among legislators and managers .
[eos] althoughthe global # metoo movement has helped raised awareness about sexual harassment , the advocacy groupplan international said the lack of strong policies and enforcement has discouraged victims from comingforward in australia .
[eos] ... (truncated).
[catseq] sexual harassment ; australian council ; australia ; plan international ; [digit] presidentialelection ; michele e o’neil.
[seg-net] workplace ; harassment ; australia ; sexual harassment ; women and girls ; women ’s rights.
[ground-truth] australia ; harassment ; me too movement.
title: google team led by japanese engineer breaks record by calculating pi to the 31.4 trillionth digit.
article: los angeles - google llc said thursday that a team led by engineer emma haruka iwao from japanhas broken a guinness world record by calculating pi to the 31.4 trillionth digit , around 9 trillion morethan the previous record set in 2016 .
[eos] the accomplishment , announced on the day dubbed “ piday ” as its ﬁrst three digits are 3.14 , was achieved by using google cloud infrastructure , the tech giantsaid .
[eos] iwao became fascinated with pi , an inﬁnitely long number deﬁned as the ratio of a circle ’scircumference to its diameter , when she was 12 years old .
[eos] “ when i was a kid , i downloaded aprogram to calculate pi on my computer , ” she said in a google blog post .
[eos] in college , one ofher professors was daisuke takahashi of the university of tsukuba in ibaraki prefecture , then the recordholder for calculating the most accurate value of pi via a supercomputer .
[eos] “ when i told him iwas going to start this project , he shared his advice and some technical strategies with me , ” she said .
[eos] the groundbreaking calculation required 25 virtual google cloud machines , 170 terabytes of dataand about 121 days to complete .
[eos] “ i ’m really happy to be one of the few women in computerscience holding the record , and i hope i can show more people who want to work in the industry what ’spossible , ” iwao said .
[eos] according to google , iwao calculated 31,415,926,535,897 digits , makingit the ﬁrst time the cloud has been used for a pi calculation of this magnitude .
[eos].
[catseq] google ; tv ; [digit] presidential election.
[seg-net] google ; emma haruka iwao ; japan ; google cloud ; computers and the internet ; tech industry.
[ground-truth] google ; pi ; emma haruka iwao ; mathematics.
figure 4: sample keyphrase predictions of catseq and seg-net on kptimes dataset (evaluation set).
the high-lighted keyphrases indicate a match with the ground truth keyphrases..1404