improving paraphrase detection with the adversarial paraphrasing task.
animesh nighojkar, john licatoadvancing machine and human reasoning labdepartment of computer science and engineeringuniversity of south floridatampa, fl, usa{anighojkar,licato}@usf.edu.
abstract.
if two sentences have the same meaning,it should follow that they are equivalent ini.e., each sen-their inferential properties,tence should textually entail the other.
how-ever, many paraphrase datasets currently inwidespread use rely on a sense of paraphrasebased on word overlap and syntax.
can weteach them instead to identify paraphrases ina way that draws on the inferential propertiesof the sentences, and is not over-reliant onlexical and syntactic similarities of a sentencepair?
we apply the adversarial paradigm tothis question, and introduce a new adversarialmethod of dataset creation for paraphrase iden-tiﬁcation:the adversarial paraphrasing task(apt), which asks participants to generate se-mantically equivalent (in the sense of mutuallyimplicative) but lexically and syntactically dis-parate paraphrases.
these sentence pairs canthen be used both to test paraphrase identiﬁ-cation models (which get barely random accu-racy) and then improve their performance.
toaccelerate dataset generation, we explore au-tomation of apt using t5, and show that theresulting dataset also improves accuracy.
wediscuss implications for paraphrase detectionand release our dataset in the hope of makingparaphrase detection models better able to de-tect sentence-level meaning equivalence..1.introduction.
although there are many deﬁnitions of ‘paraphrase’in the nlp literature, most maintain that two sen-tences that are paraphrases have the same mean-ing or contain the same information.
pang et al.
(2003) deﬁne paraphrasing as “expressing the sameinformation in multiple ways” and bannard andcallison-burch (2005) call paraphrases “alternativeways of conveying the same information.” ganitke-vitch et al.
(2013) write that “paraphrases are dif-fering textual realizations of the same meaning.” a.deﬁnition that seems to sufﬁciently encompass theothers is given by bhagat and hovy (2013): “para-phrases are sentences or phrases that use differentwording to convey the same meaning.” however,even that deﬁnition is somewhat imprecise, as itlacks clarity on what it assumes ‘meaning’ means.
if paraphrasing is a property that can hold be-tween sentence pairs,1 then it is reasonable to as-sume that sentences that are paraphrases must haveequivalent meanings at the sentence level (ratherthan exclusively at the levels of individual wordmeanings or syntactic structures).
here a usefultest is that recommended by inferential role se-mantics or inferentialism (boghossian, 1994; pere-grin, 2006), which suggests that the meaning of astatement s is grounded in its inferential properties:what one can infer from s and from what s can beinferred..building on this concept from inferentialism, weassert that if two sentences have the same inferen-tial properties, then they should also be mutuallyimplicative.
mutual implication (mi) is a binary re-lationship between two sentences that holds wheneach sentence textually entails the other (i.e., bidi-rectional entailment).
mi is an attractive way ofoperationalizing the notion of two sentences hav-ing “the same meaning,” as it focuses on inferentialrelationships between sentences (properties of thesentences as wholes) instead of just syntactic orlexical similarities (properties of parts of the sen-tences).
as such, we will assume in this paperthat two sentences are paraphrases if and only ifthey are m i.2 in nlp, modeling inferential re-lationships between sentences is the goal of thetextual entailment, or natural language inference(nli) tasks (bowman et al., 2015).
we test mi.
1in this paper we study paraphrase between sentences, anddo not address the larger scope of how our work might extendto paraphrasing between arbitrarily large text sequences.
2the notations used in this paper are listed in table 1..proceedingsofthe59thannualmeetingoftheassociationforcomputationallinguisticsandthe11thinternationaljointconferenceonnaturallanguageprocessing,pages7106–7116august1–6,2021.©2021associationforcomputationallinguistics7106using the version of robertalarge released by nieet al.
(2020) trained on a combination of snli(bowman et al., 2015), multinli (williams et al.,2018), fever-nli (nie et al., 2019), and anli(nie et al., 2020)..owing to expeditious progress in nlp research,performance of models on benchmark datasets is‘plateauing’ — with near-human performance oftenachieved within a year or two of their release —and newer versions, using a different approach,are constantly having to be created, for instance,glue (wang et al., 2019) and superglue (wanget al., 2020).
the adversarial paradigm of datasetcreation (jia and liang, 2017a,b; bras et al., 2020;nie et al., 2020) has been widely used to addressthis ‘plateauing,’ and the ideas presented in thispaper draw inspiration from it.
in the remainder ofthis paper, we apply the adversarial paradigm to theproblem of paraphrase detection, and demonstratethe following novel contributions:.
• we use the adversarial paradigm to createa new benchmark examining whether para-phrase detection models are assessing themeaning equivalence of sentences rather thanbeing over-reliant on word-level measures.
we do this by collecting paraphrases thatare mi but are as lexically and syntacticallydisparate as possible (as measured by lowbleurt scores).
we call this the adversarialparaphrasing task (apt)..• we show that a sota language model trainedon paraphrase datasets perform poorly on ourbenchmark.
however, when further trainedon our adversarially-generated datasets, theirmcc scores improve by up to 0.307..• we create an additional dataset by training aparaphrase generation model to perform ouradversarial task, creating another large datasetthat further improves the paraphrase detectionmodels’ performance..• we propose a way to create a machine-generated adversarial dataset and discussways to ensure it does not suffer from theplateauing that other datasets suffer from..2 related work.
paraphrase detection (given two sentences, predictwhether they are paraphrases) (zhang and patrick,.
mi.
m i.apt.
ap t.aph.
apt 5.ap mt 5ap t wt 5.concept of mutual implication/ bidirectional textual entailment)property of being mutually implicative,as determined by our nli modeladversarial paraphrasing task.
property of passing the adversarialparaphrase test (see §3)human-generated apt dataset.
t5base-generated apt dataset.
(note that apt 5 = ap m.t 5 ∪ ap t wt 5 ).
msrp subset of apt 5twitterppdb subset of apt 5.table 1: notations used in the paper..2005; fernando and stevenson, 2008; socher et al.,2011; jia et al., 2020) is an important task in theﬁeld of nlp, ﬁnding downstream applications inmachine translation (callison-burch et al., 2006;apidianaki et al., 2018; mayhew et al., 2020), textsummarization, plagiarism detection (hunt et al.,2019), question answering, and sentence simpliﬁ-cation (guo et al., 2018).
paraphrases have provento be a crucial part of nlp and language educa-tion, with research showing that paraphrasing helpsimprove reading comprehension skills (lee andcolln, 2003; hagaman and reid, 2008).
questionparaphrasing is an important step in knowledge-based question answering systems for matchingquestions asked by users with knowledge-basedassertions (fader et al., 2014; yin et al., 2015)..paraphrase generation (given a sentence, gener-ate its paraphrase) (gupta et al., 2018) is an areaof research beneﬁting paraphrase detection as well.
lately, many paraphrasing datasets have been in-troduced to be used for training and testing mlmodels for both paraphrase detection and genera-tion.
msrp (dolan and brockett, 2005) contains5801 sentence pairs, each labeled with a binary hu-man judgment of paraphrase, created using heuris-tic extraction techniques along with an svm-basedclassiﬁer.
these pairs were annotated by humans,who found 67% of them to be semantically equiva-lent.
the english portion of ppdb (ganitkevitchet al., 2013) contains over 220m paraphrase pairsgenerated by meaning-preserving syntactic trans-formations.
paraphrase pairs in ppdb 2.0 (pavlicket al., 2015) include ﬁne-grained entailment rela-tions, word embedding similarities, and style an-notations.
twitterppdb (lan et al., 2017) con-sists of 51,524 sentence pairs captured from twitterby linking tweets through shared urls.
this ap-.
7107proach’s merit is its simplicity as it involves neithera classiﬁer nor a human-in-the-loop to generateparaphrases.
humans annotate the pairs, givingthem a similarity score ranging from 1 to 6..paranmt (wieting and gimpel, 2018) was cre-ated by using neural machine translation to trans-late the english side of a czech-english parallelcorpus (czeng 1.6 (bojar et al., 2016)), generat-ing more than 50m english-english paraphrases.
however, paranmt’s use of machine translationmodels that are a few years old harms its utility(nighojkar and licato, 2021), considering the rapidimprovement in machine translation in the past fewyears.
to rectify this, we use the google-translatelibrary to translate the czech side of roughly 300kczeng2.0 (kocmi et al., 2020) sentence pairs our-selves.
we call this dataset paraparanmt (pp-nmt for short, where the extra para- preﬁx re-ﬂects its similarity to, and conceptual derivationfrom, paranmt)..some work has been done in improving the qual-ity of paraphrase detectors by training them on adataset with more lexical and syntactic diversity.
thompson and post (2020) propose a paraphrasegeneration algorithm that penalizes the productionof n-grams present in the source sentence.
ourapproach to doing this is with the apt, but thisis something worth exploring.
sokolov and fil-imonov (2020) use a machine translation modelto generate paraphrases much like paranmt.
aninteresting application of paraphrasing has beendiscussed by mayhew et al.
(2020) who, given asentence in one language, generate a diverse setof correct translations (paraphrases) that humansare likely to produce.
in comparison, our workis focused on generating adversarial paraphrasesthat are likely to deceive a paraphrase detector, andmodels trained on the adversarial datasets we pro-duce can be applied to mayhew et al.’s work too..anli (nie et al., 2020), a dataset designed fornatural language inference (nli) (bowman et al.,2015), was collected via an adversarial human-and-model-in-the-loop procedure where humans aregiven the task of duping the model into making awrong prediction.
the model then tries to learn hownot to make the same mistakes.
aflite (bras et al.,2020) adversarially ﬁlters dataset biases makingsure that the models are not learning those biases.
they show that model performance on snli (bow-man et al., 2015) drops from 92% to 62% whenbiases were ﬁltered out.
however, their approach is.
to ﬁlter the dataset, which reduces its size, makingmodel training more difﬁcult.
our present worktries instead to generate adversarial examples toincrease dataset size.
other examples of adversar-ial datasets in nlp include work done by jia andliang (2017a); zellers et al.
(2018, 2019).
per-haps the closest to our work is paws (zhang et al.,2019), short for paraphrase adversaries from wordscrambling.
the idea behind paws is to createa dataset that has a high lexical overlap betweensentence pairs without them being ‘paraphrases.’ ithas 108k paraphrase and non-paraphrase pairs withhigh lexical overlap pairs generated by controlledword swapping and back-translation, and humanraters have judged whether or not they are para-phrases.
including paws in the training data hasshown the state-of-the-art models’ performance tojump from 40% to 85% on paws’s test split.
incomparison to the present work, paws does notexplicitly incorporate inferential properties, and weseek paraphrases minimizing lexical overlap..3 adversarial paraphrasing task (apt).
semantic textual similarity (sts) measures thedegree of semantic similarity between two sen-tences.
popular approaches to calculating stsinclude bleu (papineni et al., 2002), bertscore(zhang et al., 2020), and bleurt (sellam et al.,2020).
bleurt is a text generation metric build-ing on bert’s (devlin et al., 2019) contextualword representations.
bleurt is warmed-up us-ing synthetic sentence pairs and then ﬁne-tuned onhuman ratings to generalize better than bertscore(zhang et al., 2020).
given any two sentences,bleurt assigns them a similarity score (usuallybetween -2.2 to 1.1).
however, high sts scores donot necessarily predict whether two sentences haveequivalent meanings.
consider the sentence pairsin table 3, highlighting cases where sts and para-phrase appear to misalign.
the existence of suchcases suggests a way to advance automated para-phrase detection: through an adversarial bench-mark consisting of sentence pairs that have thesame mi-based meaning, but have bleurt scoresthat are as low as possible.
this is the motivationbehind what we call the adversarial paraphrasingtask (apt), which has two components:.
1. similarity of meaning: checked through mi(section 1).
we assume if two sentences arem i (mutually implicative), they are seman-tically equivalent and thus paraphrases.
note.
7108figure 1: the mturk study and the reward calculation.
we automatically end the study when a subject earns atotal of $20 to ensure variation amongst subjects..that mi is a binary relationship, so this aptcomponent does not bring any quantitativevariation but is more like a qualiﬁer test forapt.
all ap t sentence pairs are m i..2. dissimilarity of structure: measured throughbleurt, which assigns each sentence pair ascore quantifying how lexically and syntacti-cally similar the two sentences are..3.1 manually solving apt.
to test the effectiveness of apt in guiding the gen-eration of mutually implicative but lexically andsyntactically disparate paraphrases for a given sen-tence, we designed an amazon mechanical turk(mturk) study (figure 1).
given a starting sentence,we instructed participants to “[w]rite a sentencethat is the same in meaning as the given sentencebut as structurally different as possible.
your sen-tence should be such that you can infer the givensentence from it and vice-versa.
it should be sufﬁ-ciently different from the given sentence to get anyreward for the submission.
for example, a simplesynonym substitution will most likely not work.”the sentences given to the participants came frommsrp and ppnmt (section 1).
both of thesedatasets have pairs of sentences in each row, andwe took only the ﬁrst one to present to the par-.
ticipants.
neither of these datasets has duplicatesentences by design.
every time a sentence was se-lected, a random choice was made between msrpand ppnmt, thus ensuring an even distribution ofsentences from both datasets..each attempt was evaluated separately usingequation 1, where mi is 1 when the sentences arem i and 0 otherwise:.
reward =.
mi(1 + e5∗bleurt)2.
(1).
this formula was designed to ensure (1) the max-imum reward per submission was $1, and (2) noreward was granted for sentence pairs that are non-mi or have bleurt > 0.5. participants wereencouraged to frequently revise their sentences andclick on a ‘check’ button which showed them thereward amount they would earn if they submittedthis sentence.
once the ‘check’ button was clicked,the participant’s reward was evaluated (see figure1) and the sentence pair added to aph (regardlessof whether it was ap t ).
if ‘submit’ was clicked,their attempt was rewarded based on equation 1..the resulting dataset of sentence pairs, whichwe call aph (adversarial paraphrase by humans),consists of 5007 human-generated sentence pairs,both m i and non-m i (see table 2).
humans wereable to generate ap t paraphrases for 75.48% of.
7109dataset.
totalattempts.
aph.
5007.ap mt 5.ap t wt 5.
62,986.
75,011.ap tattempts265953.10%38366.09%64548.60%.
m iattempts323264.55%37,51159.55%17,07422.76%.
non-m iattempts177535.45%25,47540.44%57,93777.24%.
uniquesentences.
1631.
4072.
4328.ap tuniques123175.48%228856.19%367084.80%.
m iuniques133882.04%404599.34%413195.45%.
non-m iuniques29317.96%311576.50%423097.74%.
table 2: proportion of sentences generated by humans (aph ) and t5base (apt 5).
“attempts” shows the numberof attempts the participant made and “uniques” shows the number of source sentences from the dataset that theperformer’s attempts fall in that category on.
for instance, 1631 unique sentences were presented to humans, whomade a total of 5007 attempts to pass ap t and were able to do so for 2659 attempts which amounted to 1231unique source sentences that could be paraphrased to pass ap t ..the sentences presented to them and only 53.1%of attempts were ap t , showing that the task isdifﬁcult even for humans.
note that ‘m i attempts’and ‘m i uniques’ are supersets of ‘ap t attempts’and ‘ap t uniques,’ respectively..3.2 automatically solving apt.
since human studies can be time-consuming andcostly, we trained a paraphrase generator to per-form apt.
we used t5base (raffel et al., 2020), asit achieves sota on paraphrase generation (niuet al., 2020; bird et al., 2020; li et al., 2020) andtrained it on twitterppdb (section 2).
our hy-pothesis was that if t5base is trained to maximizethe apt reward (equation 1), its generated sen-tences will be more likely to be ap t .
we gener-ated paraphrases for sentences in msrp and thosein twitterppdb itself, hoping that since t5base istrained on twitterppdb, it would generate betterparaphrases (m i with lower bleurt) for sen-tences coming from there.
the proportion of sen-tences generated by t5base is shown in table 2.we call this dataset apt 5, the generation of whichinvolved two phases:training: to adapt t5base for apt, we imple-mented a custom loss function obtained from di-viding the cross-entropy loss per batch by the totalreward (again from equation 1) earned from themodel’s paraphrase generations for that batch, pro-vided the model was able to reach a reward of atleast 1. if not, the loss was equal to just the cross-entropy loss.
we trained t5base on twitterppdbfor three epochs; each epoch took about 30 hourson one nvidia tesla v100 gpu due to the cpubound bleurt component.
more epochs mayhelp get better results, but our experiments showedthat loss plateaus after three epochs.
generation: sampling, or randomly picking a.next word according to its conditional probabil-ity distribution, introduces non-determinism in lan-guage generation.
fan et al.
(2018) introduce top-ksampling, which ﬁlters k most likely next words,and the probability mass is redistributed amongonly those k words.
nucleus sampling (or top-psampling) (holtzman et al., 2020) reduces the op-tions to the smallest possible set of words whosecumulative probability exceeds p, and the probabil-ity mass is redistributed among this set of words.
thus, the set of words changes dynamically ac-cording to the next word’s probability distribution.
we use a combination of top-k and top-p samplingwith k = 120 and p = 0.95 in the interest of lexi-cal and syntactic diversity in the paraphrases.
foreach sentence in the source dataset (msrp3 andtwitterppdb for ap mt 5 respectively),we perform ﬁve iterations, in each of which, wegenerate ten sentences.
if at least one of these tensentences passes ap t , we continue to the nextsource sentence after recording all attempts andclassifying them as m i or non-m i. if no sentencein a maximum of 50 attempts passes ap t , werecord all attempts nonetheless, and move on to thenext source sentence.
for each increasing iterationfor a particular source sentence, we increase k by20, but we also reduce p by 0.05 to avoid vagueguesses.
note the distribution of m i and non-m iin the source datasets does not matter because weuse only the ﬁrst sentence from the sentence pair..t 5 and ap t w.3.3 dataset properties.
t5base trained with our custom loss function gen-erated ap t -passing paraphrases for (56.19%) ofstarting sentences.
this is higher than we initiallyexpected, considering how difﬁcult apt provedto be for humans (table 2).
noteworthy is that.
3we use the ofﬁcial train split released by dolan and brock-.
ett (2005) containing 4076 sentence pairs..7110(a) aph.
(b) ap mt 5.
(c) ap t wt 5.figure 2: bleurt distributions on adversarial datasets.
all ﬁgures divide the range of observed scores into 100bins.
note that ap t sentence pairs are also m i, whereas those labeled ‘mi’ are not ap t ..only 6.09% of t5base’s attempts were ap t .
thisdoes not mean that the remaining 94% of attemptscan be discarded, since they amounted to the neg-ative examples in the dataset.
since we trainedit on twitterppdb itself, we expected that t5basewould generate better paraphrases, as measured bya higher chance of passing ap t on twitterppdb,than any other dataset we tested.
this is supportedby the data in table 2, which shows that t5basewas able to generate an ap t passing paraphrasefor 84.8% of the sentences in twitterppdb..the composition of the three adversarial datasetscan be found in table 2. these metrics are usefulto understand the capabilities of t5base as a para-phrase generator and the “paraphrasability” of sen-tences in msrp and twitterppdb.
for instance,t5base’s attempts on twitterppdb tend to be m imuch less frequently than those on msrp and hu-man’s attempts on msrp + ppnmt.
this mightbe because in an attempt to generate syntacticallydissimilar sentences, the t5base paraphraser alsoended up generating many semantically dissimilarones as well..to visualize the syntactic and lexical disparityof paraphrases in the three adversarial datasets, wepresent their bleurt distributions in figure 2. asmight be expected, the likelihood of a sentence pairbeing m i increases as bleurt score increases(recall that ap t -passing sentence pairs are sim-ply m i pairs with bleurt scores <= 0.5), butfigure 2 shows that the shape of this increase isnot straightforward, and differs among the threedatasets..as might be expected, humans are much moreskilled at apt than t5base, as shown by the factthat the paraphrases they generated have muchlower mean bleurt scores (figure 2), and the ra-tio of ap t vs non-ap t sentences is much higher(table 2).
as we saw earlier, when t5base wrote.
paraphrases that were low on bleurt, they tendedto become non-m i (e.g., line 12 in table 3).
how-ever, t5base did generate more ap t -passing sen-tences with a lower bleurt on twitter-ppdbthan on msrp, which may be a result of overﬁt-ting t5base on twitterppdb.
furthermore, all threeadversarial datasets have a distribution of m i andnon-m i sentence pairs balanced enough to train amodel to identify paraphrases..table 3 has examples from aph and apt 5showing the merits and shortcomings of t5,bleurt, and robertalarge (the mi detectorused).
some observations from table 3 include:.
• lines 1 and 3: bleurt did not recognize theparaphrases, possibly due to the differences inwords used.
robertalarge however, gave thecorrect mi prediction (though it is worth not-ing that the sentences in line 1 are questions,rather than truth-apt propositions)..• line 4: robertalarge and bleurt (to alarge extent since it gave it a score of 0.4) didnot recognize that the idiomatic phrase ‘breaka leg’ means ‘good luck’ and not ‘fracture.’.
• lines 6 and 12: there is a loss of informationgoing from the ﬁrst sentence to the second andbleurt and mi both seem to have under-stood the difference between summarizationand paraphrasing..• line 7: t5 not only understood the scoresbut also managed to paraphrase it in such away that was not syntactically and lexicallysimilar, just as we wanted t5 to do when weﬁne-tuned it..• line 9: t5base knows that fort lauderdale is.
in florida but robertalarge does not..7111source sentence.
attempt.
bleurt mi.
no..sourcedataset.
1.
2.
3.
4.
5.
6.
7.
8.
9.
10.
11.
12.ppnmt.
ppnmt.
ppnmt.
ppnmt.
msrp.
msrp.
msrp.
tp.
tp.
tp.
so, can we please get out of here?.
so is it okay if we please go?.
you’re crying..treatment successful..break a leg!.
i did not cry.
the treatment was succesful..fracture a leg!.
msrp.
two years later, the insurance coverage would begin..the insurance will start in two years.
evacuation went smoothly, although passengersweren’t told what was going on, hunt said..hunt told that evacuation went smoothly..aph.
apt 5.msrp.
friday, stanford (47-15) blanked the gamecocks 8-0..stanford (47-15) won 8-0 over the gamecocks on friday..revenue in the ﬁrst quarter of the year dropped15 percent from the same period a year earlier..revenue declined 15 percent in the ﬁrst quarterof the year from the same period a year earlier..a federal magistrate in fort lauderdale orderedhim held without bail..in fort lauderdale, florida, a federal magistrateordered him held without bail..16 innovations making a difference for poorcommunities around the world..16 innovative ideas that tackle poverty around the world..0.317.this is so past the bounds of normal or acceptable ..this is so beyond the normal or acceptable boundaries..the creator of atari has launched a new vr companycalled modal vr..atari creator is setting up a new vr company!.
-0.064.
-1.366.
-0.871.
0.408.
0.281.
-0.298.
0.206.
0.698.
0.635.
0.620.
0.106.
1.
0.
1.
1.
1.
0.
1.
1.
0.
1.
1.
0.table 3: examples from adversarial datasets.
the source dataset (tp short for twitterppdb) tells which datasetthe sentence pair comes from (and whether it is in ap mt 5 for apt 5).
all datasets have ap t passing andfailing m i and non-m i sentence pairs..t 5 or ap t w.datasetaph -trainaph -testmsrp-train.
total3746.m i.non-m i.
2433.
64.95% 1313.
35.05%.
1261.
799.
63.36% 462.
36.64%.
4076.
2753.
67.54% 1323.
32.46%.
msrp-test.
1725.
1147.
66.50% 578.
33.50%.
table 4: distribution of m i and non-m i pairs..test set.
msrp-train.
robertabasemcc0.349.f10.833.randomf10.806.mcc0.msrp-test.
0.358.
0.829.aphaph -test.
0.222.
0.746.
0.218.
0.743.
0.
0.
0.
0.799.
0.784.
0.777.size.
aph.
mcc.
f1.
training datasettwitterppdb +aph -trainap mt 5aph -train + ap mt 5ap t wt 5aph -train + ap t wt 5apt 5aph -train + apt 5.
46k.
106k.
109k.
117k.
121k.
180k.
184k.
aph -testf10.809.mcc0.440.
0.516.
0.828.
0.488.
0.812.
0.525.
0.816.
0.410.
0.725.
0.369.
0.705.
0.433.
0.772.
0.422.
0.765.
0.461.
0.731.
0.437.
0.716.table 6: performance of robertabase trained on ad-versarial datasets.
size is the number of training exam-ples in the dataset rounded to nearest 1000..table 5: performance of robertabase trained on justtwitterppdb (no adversarial datasets) vs. random pre-diction..chen et al., 2021)..4 experiments.
to quantify our datasets’ contributions, we de-signed experiment setups wherein we trainedrobertabase (liu et al., 2019) for paraphrase de-tection on a combination of twitterppdb and ourdatasets as training data.
roberta was chosen forits generality, as it is a commonly used model incurrent nlp work and benchmarking, and currentlyachieves sota or near-sota results on a majorityof nlp benchmark tasks (wang et al., 2019, 2020;.
for each source sentence, multiple paraphrasesmay have been generated.
hence, to avoid dataleakage, we created a train-test split on aph suchthat all paraphrases generated using a given sourcesentence will be either in aph -train or in aph -test, but never in both.
note that aph is not bal-anced as seen in table 2. table 4 shows the dis-tribution of m i and non-m i pairs in aph -trainand aph -test and ‘m i attempts’ and ‘non-m iattempts’ columns of table 2 show the same forother adversarial datasets.
the test sets used wereaph wherever aph -train was not a part of thetraining data and aph -test in every case..7112do well.
on aph ?
does robertabaserobertabase was trained on each trainingdataset (90% training data, 10% validation data)for ﬁve epochs with a batch size of 32 with thetraining and validation data shufﬂed, and thetrained model was tested on aph and aph -test.
the results of this are shown in table 6. note thatsince the number of m i and non-m i sentencesin all the datasets is imbalanced, matthew’s cor-relation coefﬁcient (mcc) is a more appropriateperformance measure than accuracy (boughorbelet al., 2017)..our motivation behind creating an adversarialdataset was to improve the performance of para-phrase detectors by ensuring they recognize para-phrases with low lexical overlap.
to demonstratethe extent of their inability to do so, we ﬁrst com-pare the performance of robertabase trained onlyon twitterppdb on speciﬁc datasets as shown ta-ble 5. although the model performs slightly wellon msrp, it does barely better than a random pre-diction on aph , thus showing that identifying ad-versarial paraphrases created using apt is non-trivial for paraphrase identiﬁers..do human-generated adversarial paraphrasesimprove paraphrase detection?
we introduceaph -train to the training dataset along with twit-terppdb.
this improves the mcc by 0.222 eventhough aph -train constituted just 8.15% of theentire training dataset, the rest of which was twit-terppdb (table 6).
this shows the effectivenessof human-generated paraphrases, as is especiallyimpressive given the size of aph -train comparedto twitterppdb..adversarial.
do machine-generatedpara-phrases improve paraphrase detection?
weset out to test the improvement brought by apt 5,of which we have two versions.
adding ap mt 5to the training set was not as effective as addingaph -train, increasing mcc by 0.188 on aphand 0.151 on aph -test, thus showing us thatt5base, although was able to clear ap t , lackedthe quality which human paraphrases possessed.
this might be explained by figure 2 — sinceap mt 5 does not have many sentences with lowbleurt, we cannot expect a vast improvementin robertabase’s performance on sentences withbleurt as low as in aph ..since we were not necessarily testing t5base’sperformance — and we had trained t5base on twit-.
terppdb — we used the trained model to performapt on twitterppdb itself.
adhering to expec-tations, training robertabase (the paraphrase de-tector) with ap t wt 5 yielded higher mccs.
notethat none of the sentences are common betweenap t wt 5 and aph since aph is built on msrp andppnmt and the fact that the model got this per-formance when trained on ap t wt 5 is a testimony tothe quality and contribution of apt..combining these results, we can conclude thatalthough machine-generated datasets like apt 5can help paraphrase detectors improve themselves,a smaller dataset of human-generated adversarialparaphrases improved performance more.
overall,however, the highest mcc (0.525 in table 6) isobtained when twitterppdb is combined with allthree adversarial datasets, suggesting that the twoapproaches nicely complement each other..5 discussions and conclusions.
t 5 and ap t w.this paper introduced apt (adversarial paraphras-ing task), a task that uses the adversarial paradigmto generate paraphrases consisting of sentenceswith equivalent (sentence-level) meanings, but dif-fering lexical (word-level) and syntactical similar-ity.
we used apt to create a human-generateddataset / benchmark (aph ) and two machine-generated datasets (ap mt 5 ).
our goalwas to effectively augment how paraphrase detec-tors are trained, in order to make them less relianton word-level similarity.
in this respect, the presentwork succeeded: we showed that robertabasetrained on twitterppdb performed poorly on aptbenchmarks, but this performance was increasedsigniﬁcantly when further trained on either ourhuman- or machine-generated datasets.
the codeused in this paper along with the dataset has beenreleased in a publicly-available repository.4.
paraphrase detection and generation have broadapplicability, but most of their potential lies in ar-eas in which they still have not been substantiallyapplied.
these areas range from healthcare (im-proving accessibility to medical communicationsor concepts by automatically generating simplerlanguage), writing (changing the writing style ofan article to match phrasing a reader is better ableto understand), and education (simplifying the lan-guage of a scientiﬁc paper or educational lesson to.
4https://github.com/.
advancing-machine-human-reasoning-lab/apt.
7113make it easier for students to understand).
thus,future research into improving their performancecan be very valuable.
but approaches to paraphrasethat treat it as no more than a matter of detectingword similarity overlap will not sufﬁce for theseapplications.
rather, the meanings of sentencesare properties of the sentences as a whole, andare inseparably tied to their inferential properties.
thus, our approaches to paraphrase detection andgeneration must follow suit..the adversarial paradigm can be used to divedeeper into comparing how humans and sota lan-guage models understand sentence meaning, aswe did with apt.
furthermore, automatic gener-ation of adversarial datasets has much unrealizedpotential; e.g., different datasets, paraphrase gen-erators, and training approaches can be used togenerate future versions of apt 5 in order to pro-duce ap t passing sentence pairs with lower lexi-cal and syntactic similarities (as measured not onlyby bleurt, but also by future state-of-the-art stsmetrics).
the idea of more efﬁcient automated ad-versarial task performance is particularly exciting,as it points to a way language models can improvethemselves while avoiding prohibitively expensivehuman participant fees..finally, the most signiﬁcant contribution of thispaper, apt, presents a dataset creation method forparaphrases that will not saturate because as themodels get better at identifying paraphrases, wewill improve paraphrase generation.
as models getbetter at generating paraphrases, we can make aptharder (e.g., by reducing the bleurt thresholdof < 0.5).
one might think of this as students ina class who come up with new ways of copyingtheir assignments from sources as plagiarism detec-tors improved.
that brings us to one of the manyapplications of paraphrases: plagiarism generationand detection, which inherently is an adversarialactivity.
until plagiarism detectors are trained onadversarial datasets themselves, we cannot expectthem to capture human levels of adversarial para-phrasing..acknowledgements.
this material is based upon work supported bythe air force ofﬁce of scientiﬁc research underaward numbers fa9550-17-1-0191 and fa9550-18-1-0052. any opinions, ﬁndings, and conclu-sions or recommendations expressed in this mate-rial are those of the authors and do not necessarily.
reﬂect the views of the united states air force.
we would also like to thank antonio laverghettajr. and jamshidbek mirzakhalov for their helpfulsuggestions while writing this paper, and gokulshanth raveendran and manvi nagdev for helpingwith the website used for the mturk study..references.
marianna apidianaki, guillaume wisniewski, annecocos, and chris callison-burch.
2018. automatedparaphrase lattice creation for hyter machine trans-lation evaluation.
in proceedings of the 2018 con-ference of the north american chapter of the asso-ciation for computational linguistics: human lan-guage technologies, volume 2 (short papers), pages480–485..colin bannard and chris callison-burch.
2005. para-phrasing with bilingual parallel corpora.
in proceed-ings of the 43rd annual meeting of the associationfor computational linguistics (acl’05), pages 597–604..rahul bhagat and eduard hovy.
2013. squibs: whatcomputational linguistics,.
is a paraphrase?
39(3):463–472..jordan j. bird, anik´o ek´art, and diego r. faria.
2020.chatbot interaction with artiﬁcial intelligence: hu-man data augmentation with t5 and language trans-former ensemble for text classiﬁcation..paul a. boghossian.
1994. inferential role semanticsand the analytic/synthetic distinction.
philosophicalstudies: an international journal for philosophy inthe analytic tradition, 73(2/3):109–122..ondˇrej bojar, ondˇrej duˇsek, tom kocmi, jindˇrich li-bovick`y, michal nov´ak, martin popel, roman su-darikov, and duˇsan variˇs.
2016. czeng 1.6: en-larged czech-english parallel corpus with processingtools dockered.
in international conference on text,speech, and dialogue, pages 231–238.
springer..sabri boughorbel, fethi jarray, and mohammedel-anbari.
2017.optimal classiﬁer for imbal-anced data using matthews correlation coefﬁcientplos one, 12(6):e0177678–e0177678.
metric.
28574989[pmid]..samuel r. bowman, gabor angeli, christopher potts,and christopher d. manning.
2015. a large anno-tated corpus for learning natural language inference..ronan le bras, swabha swayamdipta, chandra bha-gavatula, rowan zellers, matthew e. peters, ashishsabharwal, and yejin choi.
2020. adversarial ﬁltersof dataset biases..chris callison-burch, philipp koehn, and miles os-improved statistical machine trans-in proceedings of the.
borne.
2006.lation using paraphrases..7114human language technology conference ofthenaacl, main conference, pages 17–24, new yorkcity, usa.
association for computational linguis-tics..ben chen, bin chen, dehong gao, qijin chen,chengfu huo, xiaonan meng, weijun ren, andyang zhou.
2021.transformer-based languagemodel ﬁne-tuning methods for covid-19 fake newsdetection..jacob devlin, ming-wei chang, kenton lee, andkristina toutanova.
2019. bert: pre-training of deepbidirectional transformers for language understand-ing..bill dolan and chris brockett.
2005..automati-cally constructing a corpus of sentential paraphrases.
in third international workshop on paraphrasing(iwp2005).
asia federation of natural languageprocessing..anthony fader, luke zettlemoyer, and oren etzioni.
2014. open question answering over curated and ex-tracted knowledge bases.
in proceedings of the 20thacm sigkdd international conference on knowl-edge discovery and data mining, kdd ’14, page1156–1165, new york, ny, usa.
association forcomputing machinery..angela fan, mike lewis, and yann dauphin.
2018. hi-.
erarchical neural story generation..samuel fernando and mark stevenson.
2008. a se-mantic similarity approach to paraphrase detection.
in proceedings of the 11th annual research collo-quium of the uk special interest group for compu-tational linguistics, pages 45–52..juri ganitkevitch, benjamin van durme, and chriscallison-burch.
2013.ppdb: the paraphrasedatabase.
in proceedings of the 2013 conference ofthe north american chapter of the association forcomputational linguistics: human language tech-nologies, pages 758–764..han guo, ramakanth pasunuru, and mohit bansal.
2018. dynamic multi-level multi-task learning forsentence simpliﬁcation..ankush gupta, arvind agarwal, prawaan singh, andpiyush rai.
2018. a deep generative frameworkfor paraphrase generation.
proceedings of the aaaiconference on artiﬁcial intelligence, 32(1)..jessica l. hagaman and robert reid.
2008. the ef-fects of the paraphrasing strategy on the readingcomprehension of middle school students at risk forfailure in reading.
remedial and special education,29(4):222–234..ari holtzman, jan buys, li du, maxwell forbes, andyejin choi.
2020. the curious case of neural textdegeneration..e. hunt, r..janamsetty, c. kinares, c. koh,a. sanchez, f. zhan, m. ozdemir, s. waseem,o. yolcu, b. dahal, j. zhan, l. gewali, and p. oh.
2019. machine learning models for paraphrase iden-tiﬁcation and its applications on plagiarism detec-tion.
in 2019 ieee international conference on bigknowledge (icbk), pages 97–104..robin jia and percy liang.
2017a.
adversarial exam-ples for evaluating reading comprehension systems..robin jia and percy liang.
2017b.
adversarial exam-ples for evaluating reading comprehension systems.
in proceedings of the 2017 conference on empiri-cal methods in natural language processing, pages2021–2031, copenhagen, denmark.
association forcomputational linguistics..xin jia, wenjie zhou, xu sun, and yunfang wu.
2020.how to ask good questions?
try to leverage para-in proceedings of the 58th annual meet-phrases.
ing of the association for computational linguistics,pages 6130–6140, online.
association for computa-tional linguistics..tom kocmi, martin popel, and ondrej bojar.
2020.announcing czeng 2.0 parallel corpus with over 2gigawords.
arxiv preprint arxiv:2007.03006..wuwei lan, siyu qiu, hua he, and wei xu.
2017.a continuously growing dataset of sentential para-phrases.
in proceedings of the 2017 conference onempirical methods in natural language processing,pages 1224–1234, copenhagen, denmark.
associa-tion for computational linguistics..steven lee and theresa colln.
2003. the effect of in-struction in the paraphrasing strategy on reading ﬂu-ency and comprehension..eric li, jingyi su, hao sheng, and lawrence wai.
2020. agent zero: zero-shot automatic multiple-choice question generation for skill assessments..yinhan liu, myle ott, naman goyal, jingfei du, man-dar joshi, danqi chen, omer levy, mike lewis,luke zettlemoyer, and veselin stoyanov.
2019.roberta: a robustly optimized bert pretraining ap-proach..stephen mayhew, klinton bicknell, chris brust, billmcdowell, will monroe, and burr settles.
2020. si-multaneous translation and paraphrase for languageeducation.
in proceedings of the acl workshop onneural generation and translation (wngt).
acl..yixin nie, haonan chen, and mohit bansal.
2019.combining fact extraction and veriﬁcation with neu-ral semantic matching networks.
in association forthe advancement of artiﬁcial intelligence (aaai)..yixin nie, adina williams, emily dinan, mohitbansal, jason weston, and douwe kiela.
2020. ad-versarial nli: a new benchmark for natural languageunderstanding..7115alex wang, amanpreet singh, julian michael, felixhill, omer levy, and samuel r. bowman.
2019.glue: a multi-task benchmark and analysis platformfor natural language understanding..john wieting and kevin gimpel.
2018. paranmt-50m: pushing the limits of paraphrastic sentence em-beddings with millions of machine translations.
inproceedings of the 56th annual meeting of the as-sociation for computational linguistics (volume 1:long papers), pages 451–462, melbourne, australia.
association for computational linguistics..adina williams, nikita nangia, and samuel r. bow-man.
2018. a broad-coverage challenge corpus forsentence understanding through inference..pengcheng yin, nan duan, ben kao, junwei bao, andming zhou.
2015. answering questions with com-plex semantic constraints on open knowledge bases.
in proceedings of the 24th acm international onconference on information and knowledge manage-ment, cikm ’15, page 1301–1310, new york, ny,usa.
association for computing machinery..rowan zellers, yonatan bisk, roy schwartz, and yejinchoi.
2018. swag: a large-scale adversarial datasetfor grounded commonsense inference..rowan zellers, ari holtzman, yonatan bisk, alifarhadi, and yejin choi.
2019. hellaswag: can amachine really ﬁnish your sentence?.
tianyi zhang, varsha kishore, felix wu, kilian q.weinberger, and yoav artzi.
2020. bertscore: eval-uating text generation with bert..yitao zhang and jon patrick.
2005. paraphrase iden-in proceedingstiﬁcation by text canonicalization.
of the australasian language technology workshop2005, pages 160–166..yuan zhang, jason baldridge, and luheng he.
2019.paws: paraphrase adversaries from word scram-bling..animesh nighojkar and john licato.
2021. mutual im-plication as a measure of textual equivalence.
theinternational flairs conference proceedings, 34..tong niu, semih yavuz, yingbo zhou, huan wang,nitish shirish keskar, and caiming xiong.
2020.unsupervised paraphrase generation via dynamicblocking..bo pang, kevin knight, and daniel marcu.
2003.syntax-based alignment of multiple translations: ex-tracting paraphrases and generating new sentences.
in proceedings of the 2003 human language tech-nology conference of the north american chapterof the association for computational linguistics,pages 181–188..kishore papineni, salim roukos, todd ward, and wei-jing zhu.
2002. bleu: a method for automatic eval-in proceedings ofuation of machine translation.
the 40th annual meeting of the association for com-putational linguistics, pages 311–318, philadelphia,pennsylvania, usa.
association for computationallinguistics..ellie pavlick, pushpendre rastogi, juri ganitkevitch,benjamin van durme, and chris callison-burch.
2015. ppdb 2.0: better paraphrase ranking, ﬁne-grained entailment relations, word embeddings, andstyle classiﬁcation.
in proceedings of the 53rd an-nual meeting of the association for computationallinguistics and the 7th international joint confer-ence on natural language processing (volume 2:short papers), pages 425–430..jaroslav peregrin.
2006. meaning as an inferential role..erkenntnis, 64(1):1–35..colin raffel, noam shazeer, adam roberts, katherinelee, sharan narang, michael matena, yanqi zhou,wei li, and peter j. liu.
2020. exploring the limitsof transfer learning with a uniﬁed text-to-text trans-former..thibault sellam, dipanjan das, and ankur p. parikh.
2020. bleurt: learning robust metrics for text gen-eration..richard socher, eric h huang, jeffrey pennin, christo-pher d manning, and andrew y ng.
2011. dy-namic pooling and unfolding recursive autoencodersfor paraphrase detection.
in advances in neural in-formation processing systems, pages 801–809..alex sokolov and denis filimonov.
2020. neural ma-.
chine translation for paraphrase generation..brian thompson and matt post.
2020. paraphrase gen-eration as zero-shot multilingual translation: disen-tangling semantic similarity from lexical and syntac-tic diversity..alex wang, yada pruksachatkun, nikita nangia,amanpreet singh, julian michael, felix hill, omerlevy, and samuel r. bowman.
2020. superglue: astickier benchmark for general-purpose language un-derstanding systems..7116