generation-augmented retrieval for open-domain question answering.
yuning mao1∗, pengcheng he2, xiaodong liu3, yelong shen2,jianfeng gao3, jiawei han1, weizhu chen2.
1university of illinois, urbana-champaign.
2microsoft azure ai.
3microsoft research.
1{yuningm2, hanj}@illinois.edu2,3{penhe, xiaodl, yeshe, jfgao,wzchen}@microsoft.com.
abstract.
we propose generation-augmented retrieval(gar) for answering open-domain questions,which augments a query through text genera-tion of heuristically discovered relevant con-texts without external resources as supervi-sion.
we demonstrate that the generated con-texts substantially enrich the semantics of thequeries and gar with sparse representations(bm25) achieves comparable or better per-formance than state-of-the-art dense retrievalmethods such as dpr (karpukhin et al., 2020).
we show that generating diverse contexts for aquery is beneﬁcial as fusing their results con-sistently yields better retrieval accuracy.
more-over, as sparse and dense representations areoften complementary, gar can be easily com-bined with dpr to achieve even better per-formance.
gar achieves state-of-the-art per-formance on natural questions and triviaqadatasets under the extractive qa setup whenequipped with an extractive reader, and con-sistently outperforms other retrieval methodswhen the same generative reader is used.1.
1.introduction.
open-domain question answering (openqa) aimsto answer factoid questions without a pre-speciﬁeddomain and has numerous real-world applications.
in openqa, a large collection of documents (e.g.,wikipedia) are often used to seek information per-taining to the questions.
one of the most com-mon approaches uses a retriever-reader architecture(chen et al., 2017), which ﬁrst retrieves a small sub-set of documents using the question as the queryand then reads the retrieved documents to extract(or generate) an answer.
the retriever is crucial as itis infeasible to examine every piece of informationin the entire document collection (e.g., millionsof wikipedia passages) and the retrieval accuracybounds the performance of the (extractive) reader.
∗work was done during internship at microsoft azure ai.
1our code is available at https://github.com/.
morningmoni/gar..early openqa systems (chen et al., 2017)use classic retrieval methods such as tf-idf andbm25 with sparse representations.
sparse methodsare lightweight and efﬁcient, but unable to per-form semantic matching and fail to retrieve rele-vant passages without lexical overlap.
more re-cently, methods based on dense representations(guu et al., 2020; karpukhin et al., 2020) learn toembed queries and passages into a latent vectorspace, in which text similarity beyond lexical over-lap can be measured.
dense retrieval methods canretrieve semantically relevant but lexically differ-ent passages and often achieve better performancethan sparse methods.
however, the dense mod-els are more computationally expensive and sufferfrom information loss as they condense the entiretext sequence into a ﬁxed-size vector that does notguarantee exact matching (luan et al., 2020)..there have been some recent studies on query re-formulation with text generation for other retrievaltasks, which, for example, rewrite the queries tocontext-independent (yu et al., 2020; lin et al.,2020; vakulenko et al., 2020) or well-formed (liuet al., 2019) ones.
however, these methods re-quire either task-speciﬁc data (e.g., conversationalcontexts, ill-formed queries) or external resourcessuch as paraphrase data (zaiem and sadat, 2019;wang et al., 2020) that cannot or do not trans-fer well to openqa.
also, some rely on time-consuming training process like reinforcementlearning (rl) (nogueira and cho, 2017; liu et al.,2019; wang et al., 2020) that is not efﬁcient enoughfor openqa (more discussions in sec.
2)..in this paper, we propose generation-augmented retrieval (gar), which augmentsa query through text generation of a pre-trainedlanguage model (plm).
different from priorstudies that reformulate queries, gar does notrequire external resources or downstream feedbackvia rl as supervision, because it does not rewritethe query but expands it with heuristically discov-.
proceedingsofthe59thannualmeetingoftheassociationforcomputationallinguisticsandthe11thinternationaljointconferenceonnaturallanguageprocessing,pages4089–4100august1–6,2021.©2021associationforcomputationallinguistics4089intuitively,.
ered relevant contexts, which are fetched fromplms and provide richer background information(table 2).
for example, by prompting a plmto generate the title of a relevant passage givena query and appending the generated title to thequery, it becomes easier to retrieve that relevantpassage.
the generated contextsexplicitly express the search intent not presentedin the original query.
as a result, gar withsparse representations achieves comparable oreven better performance than state-of-the-artapproaches (karpukhin et al., 2020; guu et al.,2020) with dense representations of the originalqueries, while being more lightweight and efﬁcientin terms of both training and inference (includingthe cost of the generation model) (sec.
6.4)..speciﬁcally, we expand the query (question) byadding relevant contexts as follows.
we conductseq2seq learning with the question as the inputand various freely accessible in-domain contexts asthe output such as the answer, the sentence wherethe answer belongs to, and the title of a passagethat contains the answer.
we then append the gen-erated contexts to the question as the generation-augmented query for retrieval.
we demonstratethat using multiple contexts from diverse gener-ation targets is beneﬁcial as fusing the retrievalresults of different generation-augmented queriesconsistently yields better retrieval accuracy..we conduct extensive experiments on the nat-ural questions (nq) (kwiatkowski et al., 2019)and triviaqa (trivia) (joshi et al., 2017) datasets.
the results reveal four major advantages of gar:(1) gar, combined with bm25, achieves signif-icant gains over the same bm25 model that usesthe original queries or existing unsupervised queryexpansion (qe) methods.
(2) gar with sparse rep-resentations (bm25) achieves comparable or evenbetter performance than the current state-of-the-artretrieval methods, such as dpr (karpukhin et al.,2020), that use dense representations.
(3) sincegar uses sparse representations to measure lexicaloverlap2, it is complementary to dense representa-tions: by fusing the retrieval results of gar anddpr, we obtain consistently better performance(4) garthan either method used individually.
outperforms dpr in the end-to-end qa perfor-mance (em) when the same extractive reader isused: em=41.8 (43.8 when combining with dpr).
2strictly speaking, gar with sparse representations han-dles semantics before retrieval by enriching the queries, whilemaintaining the advantage of exact matching..on nq and 62.7 on trivia, creating new state-of-the-art results for extractive openqa.
gar alsooutperforms other retrieval methods under the gen-erative setup when the same generative reader isused: em=38.1 (45.3 when combining with dpr)on nq and 62.2 on trivia.
contributions.
(1) we propose generation-augmented retrieval (gar), which augmentsqueries with heuristically discovered relevant con-texts through text generation without external su-pervision or time-consuming downstream feedback.
(2) we show that using generation-augmentedqueries achieves signiﬁcantly better retrieval andqa results than using the original queries or ex-isting unsupervised qe methods.
(3) we showthat gar, combined with a simple bm25 model,achieves new state-of-the-art performance on twobenchmark datasets in extractive openqa and com-petitive results in the generative setting..2 related work.
conventional query expansion.
gar sharessome merits with query expansion (qe) meth-ods based on pseudo relevance feedback (rocchio,1971; abdul-jaleel et al., 2004; lv and zhai, 2010)in that they both expand the queries with relevantcontexts (terms) without the use of external super-vision.
gar is superior as it expands the querieswith knowledge stored in the plms rather thanthe retrieved passages and its expanded terms arelearned through text generation.
recent query reformulation.
there are recentor concurrent studies (nogueira and cho, 2017;zaiem and sadat, 2019; yu et al., 2020; vaku-lenko et al., 2020; lin et al., 2020) that reformu-late queries with generation models for other re-trieval tasks.
however, these studies are not eas-ily applicable or efﬁcient enough for openqa be-cause: (1) they require external resources such asparaphrase data (zaiem and sadat, 2019), searchsessions (yu et al., 2020), or conversational con-texts (lin et al., 2020; vakulenko et al., 2020)to form the reformulated queries, which are notavailable or showed inferior domain-transfer per-formance in openqa (zaiem and sadat, 2019);(2) they involve time-consuming training processsuch as rl.
for example, nogueira and cho (2017)reported a training time of 8 to 10 days as it usesretrieval performance in the reward function andconducts retrieval at each iteration.
in contrast,gar uses freely accessible in-domain contexts like.
4090passage titles as the generation targets and standardseq2seq learning, which, despite its simplicity, isnot only more efﬁcient but effective for openqa.
retrieval for openqa.
existing sparse retrievalmethods for openqa (chen et al., 2017) solely relyon the information of the questions.
gar extendsto contexts relevant to the questions by extractinginformation inside plms and helps sparse meth-ods achieve comparable or better performance thandense methods (guu et al., 2020; karpukhin et al.,2020), while enjoying the simplicity and efﬁciencyof sparse representations.
gar can also be usedwith dense representations to seek for even betterperformance, which we leave as future work.
generative qa.
generative qa generates answersthrough seq2seq learning instead of extracting an-swer spans.
recent studies on generative openqa(lewis et al., 2020a; min et al., 2020; izacard andgrave, 2020) are orthogonal to gar in that theyfocus on improving the reading stage and directlyreuse dpr (karpukhin et al., 2020) as the retriever.
unlike generative qa, the goal of gar is not togenerate perfect answers to the questions but perti-nent contexts that are helpful for retrieval.
anotherline in generative qa learns to generate answerswithout relevant passages as the evidence but solelythe question itself using plms (roberts et al., 2020;brown et al., 2020).
gar further conﬁrms that onecan extract factual knowledge from plms, whichis not limited to the answers as in prior studies butalso other relevant contexts..3 generation-augmented retrieval.
3.1 task formulation.
openqa aims to answer factoid questions with-out pre-speciﬁed domains.
we assume that a largecollection of documents c (i.e., wikipedia) aregiven as the resource to answer the questions anda retriever-reader architecture is used to tackle thetask, where the retriever retrieves a small subsetof the documents d ⊂ c and the reader reads thedocuments d to extract (or generate) an answer.
our goal is to improve the effectiveness and efﬁ-ciency of the retriever and consequently improvethe performance of the reader..3.2 generation of query contexts.
query is a question, we take the following threefreely accessible contexts as the generation targets.
we show in sec.
6.2 that having multiple gener-ation targets is helpful in that fusing their resultsconsistently brings better retrieval accuracy..context 1: the default target (answer).
the de-fault target is the label in the task of interest, whichis the answer in openqa.
the answer to the ques-tion is apparently useful for the retrieval of relevantpassages that contain the answer itself.
as shownin previous work (roberts et al., 2020; brown et al.,2020), plms are able to answer certain questionssolely by taking the questions as input (i.e., closed-book qa).
instead of using the generated answersdirectly as in closed-book qa, gar treats themas contexts of the question for retrieval.
the ad-vantage is that even if the generated answers arepartially correct (or even incorrect), they may stillbeneﬁt retrieval as long as they are relevant to thepassages that contain the correct answers (e.g., co-occur with the correct answers)..context 2: sentence containing the default tar-get.
the sentence in a passage that contains theanswer is used as another generation target.
sim-ilar to using answers as the generation target, thegenerated sentences are still beneﬁcial for retriev-ing relevant passages even if they do not containthe answers, as their semantics is highly related tothe questions/answers (examples in sec.
6.1).
onecan take the relevant sentences in the ground-truthpassages (if any) or those in the positive passagesof a retriever as the reference, depending on thetrade-off between reference quality and diversity..context 3: title of passage containing the de-fault target.
one can also use the titles of rele-vant passages as the generation target if available.
speciﬁcally, we retrieve wikipedia passages usingbm25 with the question as the query, and take thepage titles of positive passages that contain the an-swers as the generation target.
we observe thatthe page titles of positive passages are often entitynames of interest, and sometimes (but not always)the answers to the questions.
intuitively, if garlearns which wikipedia pages the question is re-lated to, the queries augmented by the generatedtitles would naturally have a better chance of re-trieving those relevant passages..in gar, queries are augmented with various heuris-tically discovered relevant contexts in order to re-trieve more relevant passages in terms of both quan-tity and quality.
for the task of openqa where the.
while it is likely that some of the generatedquery contexts involve unfaithful or nonfactual in-formation due to hallucination in text generation(mao et al., 2020) and introduce noise during re-.
4091trieval, they are beneﬁcial rather than harmful over-all, as our experiments show that gar improveboth retrieval and qa performance over bm25 sig-niﬁcantly.
also, since we generate 3 different (com-plementary) query contexts and fuse their retrievalresults, the distraction of hallucinated content isfurther alleviated..3.3 retrieval with generation-augmented.
queries.
after generating the contexts of a query, we appendthem to the query to form a generation-augmentedquery.3 we observe that conducting retrieval withthe generated contexts (e.g., answers) alone asqueries instead of concatenation is ineffective be-cause (1) some of the generated answers are ratherirrelevant, and (2) a query consisting of the correctanswer alone (without the question) may retrievefalse positive passages with unrelated contexts thathappen to contain the answer.
such low-qualitypassages may lead to potential issues in the follow-ing passage reading stage..if there are multiple query contexts, we conductretrieval using queries with different generated con-texts separately and then fuse their results.
the per-formance of one-time retrieval with all the contextsappended is slightly but not signiﬁcantly worse.
for simplicity, we fuse the retrieval results in astraightforward way: an equal number of passagesare taken from the top-retrieved passages of eachsource.
one may also use weighted or more so-phisticated fusion strategies such as reciprocal rankfusion (cormack et al., 2009), the results of whichare slightly better according to our experiments.4next, one can use any off-the-shelf retriever forpassage retrieval.
here, we use a simple bm25model to demonstrate that gar with sparse repre-sentations can already achieve comparable or betterperformance than state-of-the-art dense methodswhile being more lightweight and efﬁcient (includ-ing the cost of the generation model), closing thegap between sparse and dense retrieval methods..4 openqa with gar.
to further verify the effectiveness of gar, weequip it with both extractive and generative read-ers for end-to-end qa evaluation.
we follow the.
3one may create a title ﬁeld during document indexingand conduct multi-ﬁeld retrieval but here we append the titlesto the questions as other query contexts for generalizability..4we use the fusion tools at https://github.com/.
joaopalotti/trectools..reader design of the major baselines for a fair com-parison, while virtually any existing qa reader canbe used with gar..4.1 extractive reader.
for the extractive setup, we largely follow the de-sign of the extractive reader in dpr (karpukhinet al., 2020).
let d = [d1, d2, ..., dk] denote the listof retrieved passages with passage relevance scoresd. let si = [s1, s2, ..., sn ] denote the top n textspans in passage di ranked by span relevance scoressi.
brieﬂy, the dpr reader uses bert-base (de-vlin et al., 2019) for representation learning, whereit estimates the passage relevance score dk foreach retrieved passage dk based on the [cls] to-kens of all retrieved passages d, and assigns spanrelevance scores si for each candidate span basedon the representations of its start and end tokens.
finally, the span with the highest span relevancescore from the passage with the highest passage rel-evance score is chosen as the answer.
we refer thereaders to karpukhin et al.
(2020) for more details.
passage-level span voting.
many extractive qamethods (chen et al., 2017; min et al., 2019b; guuet al., 2020; karpukhin et al., 2020) measure theprobability of span extraction in different retrievedpassages independently, despite that their collec-tive signals may provide more evidence in deter-mining the correct answer.
we propose a simpleyet effective passage-level span voting mechanism,which aggregates the predictions of the spans inthe same surface form from different retrieved pas-sages.
intuitively, if a text span is considered as theanswer multiple times in different passages, it ismore likely to be the correct answer.
speciﬁcally,gar calculates a normalized score p(si[j]) for thej-th span in passage di during inference as follows:p(si[j]) = softmax(d)[i] × softmax(si)[j].
garthen aggregates the scores of the spans with thesame surface string among all the retrieved pas-sages as the collective passage-level score.5.
4.2 generative reader.
for the generative setup, we use a seq2seq frame-work where the input is the concatenation of thequestion and top-retrieved passages and the targetoutput is the desired answer.
such generative read-ers are adopted in recent methods such as spanse-.
5we ﬁnd that the number of spans used for normalizationin each passage does not have signiﬁcant impact on the ﬁnalperformance (we take n = 5) and using the raw or normalizedstrings for aggregation also perform similarly..4092qgen (min et al., 2020) and longformer (belt-agy et al., 2020).
speciﬁcally, we use bart-large(lewis et al., 2019) as the generative reader, whichconcatenates the question and top-retrieved pas-sages up to its length limit (1,024 tokens, 7.8 pas-sages on average).
generative gar is directly com-parable with spanseqgen (min et al., 2020) thatuses the retrieval results of dpr but not comparablewith fusion-in-decoder (fid) (izacard and grave,2020) since it encodes 100 passages rather than1,024 tokens and involves more model parameters..5 experiment setup.
5.1 datasets.
we conduct experiments on the open-domain ver-sion of two popular qa benchmarks: natural ques-tions (nq) (kwiatkowski et al., 2019) and trivi-aqa (trivia) (joshi et al., 2017).
the statistics ofthe datasets are listed in table 1..dataset train / val / test.
q-len a-len.
#-a.nqtrivia.
79,168 / 8,757 / 3,61078,785 / 8,837 / 11,313.
12.520.2.
5.25.5.
1.213.7.table 1: dataset statistics that show the number of sam-ples per data split, the average question (answer) length,and the number of answers for each question..5.2 evaluation metrics.
following prior studies (karpukhin et al., 2020),we use top-k retrieval accuracy to evaluate the per-formance of the retriever and the exact match (em)score to measure the performance of the reader..top-k retrieval accuracy is deﬁned as the pro-portion of questions for which the top-k retrievedpassages contain at least one answer span, whichis an upper bound of how many questions are “an-swerable” by an extractive reader..exact match (em) is the proportion of the pre-dicted answer spans being exactly the same as (oneof) the ground-truth answer(s), after string normal-ization such as article and punctuation removal..5.3 compared methods.
for passage retrieval, we mainly compare withbm25 and dpr, which represent the most usedstate-of-the-art methods of sparse and dense re-trieval for openqa, respectively.
for query ex-pansion, we re-emphasize that gar is the ﬁrst qeapproach designed for openqa and most of therecent approaches are not applicable or efﬁcient.
enough for openqa since they have task-speciﬁcobjectives, require external supervision that wasshown to transfer poorly to openqa, or take manydays to train (sec.
2).
we thus compare with a clas-sic unsupervised qe method rm3 (abdul-jaleelet al., 2004) that does not need external resourcesfor a fair comparison.
for passage reading, wecompare with both extractive (min et al., 2019a;asai et al., 2019; lee et al., 2019; min et al., 2019b;guu et al., 2020; karpukhin et al., 2020) and gen-erative (brown et al., 2020; roberts et al., 2020;min et al., 2020; lewis et al., 2020a; izacard andgrave, 2020) methods when equipping gar withthe corresponding reader..5.4.implementation details.
retriever.
we use anserini (yang et al., 2017) fortext retrieval of bm25 and gar with its defaultparameters.
we conduct grid search for the qebaseline rm3 (abdul-jaleel et al., 2004).
generator.
we use bart-large (lewis et al.,2019) to generate query contexts in gar.
whenthere are multiple desired targets (such as multi-ple answers or titles), we concatenate them with[sep] tokens as the reference and remove the [sep]tokens in the generation-augmented queries.
fortrivia, in particular, we use the value ﬁeld as thegeneration target of answer and observe better per-formance.
we take the checkpoint with the bestrouge-1 f1 score on the validation set, whileobserving that the retrieval accuracy of gar is rel-atively stable to the checkpoint selection since wedo not directly use the generated contexts but treatthem as augmentation of queries for retrieval.
reader.
extractive gar uses the reader of dprwith largely the same hyperparameters, which isinitialized with bert-base (devlin et al., 2019)and takes 100 (500) retrieved passages during train-ing (inference).
generative gar concatenates thequestion and top-10 retrieved passages, and takesat most 1,024 tokens as input.
greedy decoding isadopted for all generation models, which appears toperform similarly to (more expensive) beam search..6 experiment results.
we evaluate the effectiveness of gar in threestages: generation of query contexts (sec.
6.1),retrieval of relevant passages (sec.
6.2), and pas-sage reading for openqa (sec.
6.3).
ablationstudies are mostly shown on the nq dataset to un-derstand the drawbacks of gar since it achieves.
4093question: when did bat out of hell get released?
{september 1977}answer: september 1977sentence: bat out of hell is the second studio album and the major - label debut by american rock singer meatloaf ... released in september 1977 on cleveland international / epic records.
{the album was released in september 1977 on cleveland international / epic records.}
title: bat out of hell.
{bat out of hell}.
{linda davis}.
question: who sings does he love me with reba?
answer: brooks & dunnsentence: linda kaye davis ( born november 26, 1962 ) is an american country music singer.
{“ does he love you ” is a song written by sandy knox and billy stritch, and recorded as a duet by americancountry music artists reba mcentire and linda davis.}
title: does he love me [sep] does he love me (reba mcentire song) [sep] i do (reba mcentire album){linda davis [sep] greatest hits volume two (reba mcentire album) [sep] does he love you}.
{queen hippolyta}.
question: what is the name of wonder womans mother?
answer: mother magdasentence: in the amazonian myths, she is the daughter of the amazon queen sifrat and the male dwarf shuri,{wonder woman’s origin story relates that she was sculpted from clayand is the mother of wonder woman.
by her mother queen hippolyta and given life by aphrodite.}
title: wonder woman [sep] diana prince [sep] wonder woman (2011 tv pilot){wonder woman [sep] orana (comics) [sep] wonder woman (tv series)}.
table 2: examples of generated query contexts.
the issue of generating wrong answers is alleviated by generat-ing other contexts highly related to the question/answer.
ground-truth references are shown in the {braces}..better performance on trivia..6.1 query context generation.
automatic evaluation.
to evaluate the qualityof the generated query contexts, we ﬁrst measuretheir lexical overlap with the ground-truth querycontexts.
as suggested by the nontrivial rougescores in table 3, gar does learn to generatemeaningful query contexts that could help the re-trieval stage.
we next measure the lexical overlapbetween the query and the ground-truth passage.
the rouge-1/2/l f1 scores between the originalquery and ground-truth passage are 6.00/2.36/5.01,and those for the generation-augmented query are7.05/2.84/5.62 (answer), 13.21/6.99/10.27 (sen-tence), 7.13/2.85/5.76 (title) on nq, respectively.
such results further demonstrate that the generatedquery contexts signiﬁcantly increase the word over-lap between the queries and the positive passages,and thus are likely to improve retrieval results.6.
context rouge-1 rouge-2 rouge-l.answersentencetitle.
33.5137.1443.20.
20.5424.7132.11.
33.3033.9139.67.table 3: rouge f1 scores of the generated querycontexts on the validation set of the nq dataset..6we use f1 instead of recall to avoid the unfair favor of.
(longer) generation-augmented query..case studies.
in table 2, we show several ex-amples of the generated query contexts and theirground-truth references.
in the ﬁrst example, thecorrect album release date appears in both the gen-erated answer and the generated sentence, and thegenerated title is the same as the wikipedia pagetitle of the album.
in the last two examples, thegenerated answers are wrong but fortunately, thegenerated sentences contain the correct answer and(or) other relevant information and the generatedtitles are highly related to the question as well,which shows that different query contexts are com-plementary to each other and the noise during querycontext generation is thus reduced..6.2 generation-augmented retrieval.
comparison w.the state-of-the-art.
we nextevaluate the effectiveness of gar for retrieval.
in table 4, we show the top-k retrieval accuracyof bm25, bm25 with query expansion (+rm3)(abdul-jaleel et al., 2004), dpr (karpukhin et al.,2020), gar, and gar +dpr..on the nq dataset, while bm25 clearly under-performs dpr regardless of the number of retrievedpassages, the gap between gar and dpr is signiﬁ-cantly smaller and negligible when k ≥ 100. whenk ≥ 500, gar is slightly better than dpr despitethat it simply uses bm25 for retrieval.
in con-trast, the classic qe method rm3, while showing.
4094top-5 top-20 top-100 top-500 top-1000 top-5 top-20 top-100 top-500 top-1000.
trivia.
method.
bm25 (ours)bm25 +rm3dprgar.
gar +dpr.
43.644.668.360.9.
70.7.
62.964.280.174.4.
81.6.nq.
78.179.686.185.3.
88.9.
85.586.890.390.3.
92.0.
87.888.991.291.7.
93.2.
67.767.072.773.1.
76.0.
77.377.180.280.4.
82.1.
83.983.884.885.7.
86.6.
87.987.7-88.9.
-.
88.988.9-89.7.
-.
table 4: top-k retrieval accuracy on the test sets.
all baselines are evaluated by ourselves and better thanreported in karpukhin et al.
(2020).
gar helps bm25 to achieve comparable or better performance than dpr..marginal improvement over the vanilla bm25, doesnot achieve comparable performance with gar ordpr.
by fusing the results of gar and dpr inthe same way as described in sec.
3.3, we furtherobtain consistently higher performance than bothmethods, with top-100 accuracy 88.9% and top-1000 accuracy 93.2%..on the trivia dataset, the results are even moreencouraging – gar achieves consistently betterretrieval accuracy than dpr when k ≥ 5. onthe other hand, the difference between bm25 andbm25 +rm3 is negligible, which suggests thatnaively considering top-ranked passages as relevant(i.e., pseudo relevance feedback) for qe does notalways work for openqa.
results on more cutoffsof k can be found in app.
a.effectiveness of diverse query contexts.
infig.
1, we show the performance of gar whendifferent query contexts are used to augment thequeries.
although the individual performancewhen using each query context is somewhat similar,fusing their retrieved passages consistently leadsto better performance, conﬁrming that differentgeneration-augmented queries are complementaryto each other (recall examples in table 2).
performance breakdown by question type.
intable 5, we show the top-100 accuracy of the com-pared retrieval methods per question type on thenq test set.
again, gar outperforms bm25 onall types of questions signiﬁcantly and gar +dprachieves the best performance across the board,which further veriﬁes the effectiveness of gar..6.3 passage reading with gar.
comparison w. the state-of-the-art.
we showthe comparison of end-to-end qa performance ofextractive and generative methods in table 6. ex-tractive gar achieves state-of-the-art performanceamong extractive methods on both nq and triviadatasets, despite that it is more lightweight andcomputationally efﬁcient.
generative gar outper-.
figure 1: top-k retrieval accuracy on the testset of nq when fusing retrieval results of differentgeneration-augmented queries..type.
percentage bm25 dpr gar gar +dpr.
whowhenwhatwhereotherhowwhichwhy.
37.5% 82.119.0% 73.115.0% 76.510.9% 77.49.1% 79.35.0% 78.23.3% 89.00.3% 90.0.
88.086.982.689.178.183.890.790.0.
87.583.881.587.081.883.294.190.0.
90.888.686.090.884.285.594.990.0.table 5: top-100 retrieval accuracy breakdown ofquestion type on nq.
best and second best methodsin each category are bold and underlined, respectively..forms most of the generative methods on trivia butdoes not perform as well on nq, which is some-what expected and consistent with the performanceat the retrieval stage, as the generative reader onlytakes a few passages as input and gar does notoutperform dense retrieval methods on nq when kis very small.
however, combining gar with dprachieves signiﬁcantly better performance than bothmethods or baselines that use dpr as input such asspanseqgen (min et al., 2020) and rag (lewiset al., 2020a).
also, gar outperforms bm25 sig-niﬁcantly under both extractive and generative se-.
4095151020501002003005001000k: # of retrieved passages30405060708090top-k accuracy (%)answer+sentence+titleanswer+sentenceanswer+titleanswertitlesentencemethod.
trivia.
hard em (min et al., 2019a)path retriever (asai et al., 2019)orqa (lee et al., 2019)graph retriever (min et al., 2019b)realm (guu et al., 2020)dpr (karpukhin et al., 2020)bm25 (ours)gargar +dpr.
gpt-3 (brown et al., 2020)t5 (roberts et al., 2020)spanseqgen (min et al., 2020)rag (lewis et al., 2020a)fid (izacard and grave, 2020)bm25 (ours)gargar +dpr.
evitcartxe.evitareneg.nq.
28.132.633.334.540.441.537.741.843.8.
29.936.642.244.551.435.338.145.3.
50.9-45.056.0-57.960.162.7-.
-60.5-56.167.658.662.2-.
-------74.8-.
71.2--68.080.1---.
table 6: end-to-end comparison with the state-of-the-art methods in em.
for trivia, the left columndenotes the open-domain test set and the right is thehidden wikipedia test set on the public leaderboard..tups, which again shows the effectiveness of thegenerated query contexts, even if they are heuristi-cally discovered without any external supervision.
the best performing generative method fid(izacard and grave, 2020) is not directly compara-ble as it takes more (100) passages as input.
as anindirect comparison, gar performs better than fidwhen fid encodes 10 passages (cf.
fig.
2 in izac-ard and grave (2020)).
moreover, since fid relieson the retrieval results of dpr as well, we believethat it is a low-hanging fruit to replace its input withgar or gar +dpr and further boost the perfor-mance.7 we also observe that, perhaps surprisingly,extractive bm25 performs reasonably well, espe-cially on the trivia dataset, outperforming manyrecent state-of-the-art methods.8 generative bm25also performs competitively in our experiments.
model generalizability.
recent studies (lewiset al., 2020b) show that there are signiﬁcant ques-tion and answer overlaps between the training andtest sets of popular openqa datasets.
speciﬁcally,60% to 70% test-time answers also appear in thetraining set and roughly 30% test-set questionshave a near-duplicate paraphrase in the trainingset.
such observations suggest that many questionsmight have been answered by simple question or.
7this claim is later veriﬁed by the best systems in the.
neurips 2020 efﬁcientqa competition (min et al., 2021)..8we ﬁnd that taking 500 passages during reader inferenceinstead of 100 as in karpukhin et al.
(2020) improves theperformance of bm25 but not dpr..answer memorization.
to further examine modelgeneralizability, we study the per-category perfor-mance of different methods using the annotationsin lewis et al.
(2020b)..method.
total.
questionoverlap.
answeroverlaponly.
nooverlap.
dprgar +dpr (e).
bartraggar +dpr (g).
41.343.8.
26.544.545.3.
69.466.7.
67.670.767.9.
34.638.1.
10.234.938.1.
19.323.9.
0.824.827.0.table 7: em scores with question-answer overlapcategory breakdown on nq.
(e) and (g) denote ex-tractive and generative readers, respectively.
results ofbaseline methods are taken from lewis et al.
(2020b).
the observations on trivia are similar and omitted..as listed in table 7, for the no overlap category,gar +dpr (e) outperforms dpr on the extractivesetup and gar +dpr (g) outperforms rag on thegenerative setup, which indicates that better end-to-end model generalizability can be achieved byadding gar for retrieval.
gar +dpr also achievesthe best em under the answer overlap only cat-egory.
in addition, we observe that a closed-bookbart model that only takes the question as inputperforms much worse than additionally taking top-retrieved passages, i.e., gar +dpr (g), especiallyon the questions that require generalizability.
no-tably, all methods perform signiﬁcantly better onthe question overlap category, which suggests thatthe high total em is mostly contributed by questionmemorization.
that said, gar +dpr appears tobe less dependent on question memorization givenits lower em for this category.9.
6.4 efﬁciency of gar.
gar is efﬁcient and scalable since it uses sparserepresentations for retrieval and does notin-volve time-consuming training process such asrl (nogueira and cho, 2017; liu et al., 2019).
the only overhead of gar is on the generation ofquery contexts and the retrieval with generation-augmented (thus longer) queries, whose computa-tional complexity is signiﬁcantly lower than othermethods with comparable retrieval accuracy..we use nvidia v100 gpus and intel xeon plat-inum 8168 cpus in our experiments.
as listed in.
9the same ablation study is also conducted on the retrievalstage and similar results are observed.
more detailed discus-sions can be found in app.
a..4096training.
indexing.
retrieval.
dprgar.
17.3h w. 8 gpus24h w. 8 gpus3 ∼ 6h w. 1 gpu 0.5h w. 35 cpus.
30 min w. 1 gpu5 min w. 35 cpus.
table 8: comparison of computational cost betweendpr and gar at different stages.
the training timeof gar is for one generation target but different gener-ators can be trained in parallel..table 8, the training time of gar is 3 to 6 hourson 1 gpu depending on the generation target.
asa comparison, realm (guu et al., 2020) uses64 tpus to train for 200k steps during pre-trainingalone and dpr (karpukhin et al., 2020) takes about24 hours to train with 8 gpus.
to build the indicesof wikipedia passages, gar only takes around 30min with 35 cpus, while dpr takes 8.8 hourson 8 gpus to generate dense representations andanother 8.5 hours to build the faiss index (john-son et al., 2017).
for retrieval, gar takes about1 min to generate one query context with 1 gpu,1 min to retrieve 1,000 passages for the nq testset with answer/title-augmented queries and 2 minwith sentence-augmented queries using 35 cpus.
in contrast, dpr takes about 30 min on 1 gpu..7 conclusion.
in this work, we propose generation-augmentedretrieval and demonstrate that the relevant contextsgenerated by plms without external supervisioncan signiﬁcantly enrich query semantics and im-prove retrieval accuracy.
remarkably, gar withsparse representations performs similarly or betterthan state-of-the-art methods based on the denserepresentations of the original queries.
gar canalso be easily combined with dense representa-tions to produce even better results.
furthermore,gar achieves state-of-the-art end-to-end perfor-mance on extractive openqa and competitive per-formance under the generative setup..8 future extensions.
potential improvements.
there is still muchspace to explore and improve for gar in futurework.
for query context generation, one can ex-plore multi-task learning to further reduce computa-tional cost and examine whether different contextscan mutually enhance each other when generatedby the same generator.
one may also sample multi-ple contexts instead of greedy decoding to enrich aquery.
for retrieval, one can adopt more advancedfusion techniques based on both the ranking and.
score of the passages.
as the generator and re-triever are largely independent now, it is also inter-esting to study how to jointly or iteratively optimizegeneration and retrieval such that the generator isaware of the retriever and generates query contextsmore beneﬁcial for the retrieval stage.
last but notleast, it is very likely that better results can be ob-tained by more extensive hyper-parameter tuning.
applicability to other tasks.
beyond openqa,gar also has great potentials for other tasks thatinvolve text matching such as conversation utter-ance selection (lowe et al., 2015; dinan et al.,2020) or information retrieval (nguyen et al., 2016;craswell et al., 2020).
the default generation tar-get is always available for supervised tasks.
forexample, for conversation utterance selection onecan use the reference utterance as the default targetand then match the concatenation of the conversa-tion history and the generated utterance with theprovided utterance candidates.
for article search,the default target could be (part of) the ground-trutharticle itself.
other generation targets are more task-speciﬁc and can be designed as long as they canbe fetched from the latent knowledge inside plmsand are helpful for further text retrieval (matching).
note that by augmenting (expanding) the querieswith heuristically discovered relevant contexts ex-tracted from plms instead of reformulating them,gar bypasses the need for external supervision toform the original-reformulated query pairs..acknowledgments.
we thank vladimir karpukhin, sewon min, gau-tier izacard, wenda qiu, revanth reddy, and haocheng for helpful discussions.
we thank the anony-mous reviewers for valuable comments..references.
nasreen abdul-jaleel, james allan, w bruce croft,fernando diaz, leah larkey, xiaoyan li, mark dsmucker, and courtney wade.
2004. umass at trec2004: novelty and hard.
computer science depart-ment faculty publication series, page 189..akari asai, kazuma hashimoto, hannaneh hajishirzi,richard socher, and caiming xiong.
2019. learn-ing to retrieve reasoning paths over wikipediaarxiv preprintgraph for question answering.
arxiv:1911.10470..iz beltagy, matthew e peters, and arman cohan.
2020. longformer: the long-document transformer.
arxiv preprint arxiv:2004.05150..4097tom b brown, benjamin mann, nick ryder, melaniesubbiah, jared kaplan, prafulla dhariwal, arvindneelakantan, pranav shyam, girish sastry, amandaaskell, et al.
2020. language models are few-shotlearners.
arxiv preprint arxiv:2005.14165..vladimir karpukhin, barlas o˘guz, sewon min, ledelland wen-wu, sergey edunov, danqi chen,tau yih.
2020.foropen-domain question answering.
arxiv preprintarxiv:2004.04906..dense passage retrieval.
danqi chen, adam fisch, jason weston, and antoinebordes.
2017. reading wikipedia to answer open-in proceedings of the 55th an-domain questions.
nual meeting of the association for computationallinguistics (volume 1: long papers), pages 1870–1879, vancouver, canada.
association for computa-tional linguistics..gordon v cormack, charles la clarke, and stefanbuettcher.
2009. reciprocal rank fusion outper-forms condorcet and individual rank learning meth-ods.
in proceedings of the 32nd international acmsigir conference on research and development ininformation retrieval, pages 758–759..nick craswell, bhaskar mitra, emine yilmaz, danielcampos, and ellen m voorhees.
2020. overviewof the trec 2019 deep learning track.
arxiv preprintarxiv:2003.07820..jacob devlin, ming-wei chang, kenton lee, andkristina toutanova.
2019. bert: pre-training ofdeep bidirectional transformers for language under-in proceedings of the 2019 conferencestanding.
of the north american chapter of the associationfor computational linguistics: human languagetechnologies, volume 1 (long and short papers),pages 4171–4186, minneapolis, minnesota.
associ-ation for computational linguistics..emily dinan, varvara logacheva, valentin malykh,alexander miller, kurt shuster,jack urbanek,douwe kiela, arthur szlam, iulian serban, ryanlowe, et al.
2020. the second conversational in-telligence challenge (convai2).
in the neurips’18competition, pages 187–208.
springer..kelvin guu, kenton lee, zora tung, panupong pasu-pat, and ming-wei chang.
2020. realm: retrieval-arxivaugmented language model pre-training.
preprint arxiv:2002.08909..gautier izacard and edouard grave.
2020. lever-aging passage retrieval with generative models foropen domain question answering.
arxiv preprintarxiv:2007.01282..jeff johnson, matthijs douze, and herv´e j´egou.
2017.arxiv.
billion-scale similarity search with gpus.
preprint arxiv:1702.08734..mandar joshi, eunsol choi, daniel weld, and lukezettlemoyer.
2017. triviaqa: a large scale dis-tantly supervised challenge dataset for reading com-prehension.
in proceedings of the 55th annual meet-ing of the association for computational linguistics(volume 1: long papers), pages 1601–1611, van-couver, canada.
association for computational lin-guistics..tom kwiatkowski, jennimaria palomaki, olivia red-ﬁeld, michael collins, ankur parikh, chris al-berti, danielle epstein, illia polosukhin, jacob de-vlin, kenton lee, kristina toutanova, llion jones,matthew kelcey, ming-wei chang, andrew m. dai,jakob uszkoreit, quoc le, and slav petrov.
2019.natural questions: a benchmark for question an-swering research.
transactions of the associationfor computational linguistics, 7:452–466..kenton lee, ming-wei chang, and kristina toutanova.
2019. latent retrieval for weakly supervised opendomain question answering.
in proceedings of the57th annual meeting of the association for com-putational linguistics, pages 6086–6096, florence,italy.
association for computational linguistics..mike lewis, yinhan liu, naman goyal, mar-jan ghazvininejad, abdelrahman mohamed, omerlevy, ves stoyanov, and luke zettlemoyer.
2019.bart: denoising sequence-to-sequence pre-trainingfor natural language generation,translation, andcomprehension.
arxiv preprint arxiv:1910.13461..patrick lewis, ethan perez, aleksandara piktus, fabiopetroni, vladimir karpukhin, naman goyal, hein-rich k¨uttler, mike lewis, wen-tau yih, timrockt¨aschel, et al.
2020a.
retrieval-augmented gen-arxiveration for knowledge-intensive nlp tasks.
preprint arxiv:2005.11401..patrick lewis, pontus stenetorp, and sebastian riedel.
2020b.
question and answer test-train overlap inarxivopen-domain question answering datasets.
preprint arxiv:2008.02637..sheng-chieh lin,.
jheng-hong yang, rodrigonogueira, ming-feng tsai, chuan-ju wang, andjimmy lin.
2020. query reformulation using queryhistory for passage retrieval in conversational search.
arxiv preprint arxiv:2005.02230..ye liu, chenwei zhang, xiaohui yan, yi chang, andphilip s yu.
2019. generative question reﬁnementwith deep reinforcement learning in retrieval-basedin proceedings of the 28th acm inter-qa system.
national conference on information and knowledgemanagement, pages 1643–1652..ryan lowe, nissan pow, iulian serban, and joellepineau.
2015. the ubuntu dialogue corpus: a largedataset for research in unstructured multi-turn dia-logue systems.
arxiv preprint arxiv:1506.08909..yi luan, jacob eisenstein, kristina toutanova, andsparse, dense, and at-arxiv.
michael collins.
2020.tentional representations for text retrieval.
preprint arxiv:2005.00181..4098xiao wang, craig macdonald, and iadh ounis.
2020.deep reinforced query reformulation for informa-tion retrieval.
arxiv preprint arxiv:2007.07987..peilin yang, hui fang, and jimmy lin.
2017. anserini:enabling the use of lucene for information retrievalresearch.
in proceedings of the 40th internationalacm sigir conference on research and develop-ment in information retrieval, pages 1253–1256..shi yu, jiahua liu, jingqin yang, chenyan xiong,paul bennett, jianfeng gao, and zhiyuan liu.
2020.few-shot generative conversational query rewriting.
arxiv preprint arxiv:2006.05009..salah zaiem and fatiha sadat.
2019. sequence to se-in proceed-quence learning for query expansion.
ings of the aaai conference on artiﬁcial intelli-gence, student abstract track, volume 33, pages10075–10076..yuanhua lv and chengxiang zhai.
2010. positionalrelevance model for pseudo-relevance feedback.
inproceedings of the 33rd international acm sigirconference on research and development in infor-mation retrieval, pages 579–586..yuning mao, xiang ren, heng ji, and jiawei han.
2020. constrained abstractive summarization: pre-serving factual consistency with constrained genera-tion.
arxiv preprint arxiv:2010.12723..sewon min, jordan boyd-graber, chris alberti, danqichen, eunsol choi, michael collins, kelvin guu,hannaneh hajishirzi, kenton lee, jennimaria palo-maki, et al.
2021. neurips 2020 efﬁcientqa compe-tition: systems, analyses and lessons learned.
arxivpreprint arxiv:2101.00133..sewon min, danqi chen, hannaneh hajishirzi, andluke zettlemoyer.
2019a.
a discrete hard em ap-proach for weakly supervised question answering.
in proceedings of the 2019 conference on empiricalmethods in natural language processing and the9th international joint conference on natural lan-guage processing (emnlp-ijcnlp), pages 2851–2864, hong kong, china.
association for computa-tional linguistics..sewon min, danqi chen, luke zettlemoyer, and han-naneh hajishirzi.
2019b.
knowledge guided text re-trieval and reading for open domain question answer-ing.
arxiv preprint arxiv:1911.03868..sewon min, julian michael, hannaneh hajishirzi, andluke zettlemoyer.
2020. ambigqa: answeringambiguous open-domain questions.
arxiv preprintarxiv:2004.10645..tri nguyen, mir rosenberg, xia song, jianfeng gao,saurabh tiwary, rangan majumder, and li deng.
2016. ms marco: a human-generated machine read-ing comprehension dataset..rodrigo nogueira and kyunghyun cho.
2017. task-oriented query reformulation with reinforcementlearning.
in proceedings of the 2017 conference onempirical methods in natural language processing,pages 574–583, copenhagen, denmark.
associationfor computational linguistics..adam roberts, colin raffel, and noam shazeer.
2020.how much knowledge can you pack into the pa-arxiv preprintrameters of a language model?
arxiv:2002.08910..joseph rocchio.
1971. relevance feedback in in-the smart retrieval system-formation retrieval.
experiments in automatic document processing,pages 313–323..svitlana vakulenko, shayne longpre, zhucheng tu,and raviteja anantha.
2020. question rewriting forconversational question answering.
arxiv preprintarxiv:2004.14652..4099a more analysis of retrieval.
performance.
we show the detailed results of top-k retrieval accu-racy of the compared methods in figs.
2 and 3.gar performs comparably or better than dprwhen k ≥ 100 on nq and k ≥ 5 on trivia..figure 2: top-k retrieval accuracy of sparse anddense methods on the test set of nq.
gar improvesbm25 and achieves comparable or better performancethan dpr when k ≥ 100..method.
total.
questionoverlap.
answeroverlaponly.
nooverlap.
78.8bm2586.1dprgar85.3gar +dpr 88.9.
81.293.294.196.3.
85.189.587.991.7.
70.676.873.779.8.table 9: top-100 retrieval accuracy by question-answer overlap categories on the nq test set..figure 3: top-k retrieval accuracy on the trivia testset.
gar achieves better results than dpr when k ≥ 5..we show in table 9 the retrieval accuracy break-down using the question-answer overlap categories.
the most signiﬁcant gap between bm25 and othermethods is on the question overlap category,which coincides with the fact that bm25 is un-able to conduct question paraphrasing (semanticmatching).
gar helps bm25 to bridge the gap byproviding the query contexts and even outperformdpr in this category.
moreover, gar consistentlyimproves over bm25 on other categories and gar+dpr outperforms dpr as well..4100151020501002003005001000k: # of retrieved passages2030405060708090top-k accuracy (%)gar +dprdprgarbm25 +rm3bm2515102050100k: # of retrieved passages5055606570758085top-k accuracy (%)gar +dprdprgarbm25 +rm3bm25