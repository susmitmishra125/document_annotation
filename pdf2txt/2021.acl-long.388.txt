concept-based label embedding via dynamic routingfor hierarchical text classiﬁcation.
xuepeng wang, li zhao, bing liu, tao chen, feng zhang, di wangtencent ai platform department, china{woodswang,lilythzhao,andybliu}@tencent.com{vitochen,jayzhang,diwang}@tencent.com.
abstract.
hierarchical text classiﬁcation (htc) is achallenging task that categorizes a textual de-scription within a taxonomic hierarchy.
mostof the existing methods focus on modeling thetext.
recently, researchers attempt to modelthe class representations with some resources(e.g., external dictionaries).
however, the con-cept shared among classes which is a kindof domain-speciﬁc and ﬁne-grained informa-tion has been ignored in previous work.
inthis paper, we propose a novel concept-basedlabel embedding method that can explicitlyrepresent the concept and model the sharingmechanism among classes for the hierarchi-cal text classiﬁcation.
experimental results ontwo widely used datasets prove that the pro-posed model outperforms several state-of-the-art methods.
we release our complementaryresources (concepts and deﬁnitions of classes)for these two datasets to beneﬁt the researchon htc..1.introduction.
text classiﬁcation is a classical natural languageprocessing (nlp) task.
in the real world, the textclassiﬁcation is usually cast as a hierarchical textclassiﬁcation (htc) problem, such as patent collec-tion (tikk et al., 2005), web content collection (du-mais and chen, 2000) and medical record cod-ing (cao et al., 2020).
in these scenarios, the htctask aims to categorize a textual description withina set of labels that are organized in a structuredclass hierarchy (silla and freitas, 2011).
lots ofresearchers devote their effort to investigate thischallenging problem.
they have proposed vari-ous htc solutions, which are usually categorizedinto ﬂat (aly et al., 2019), local (xu and geng,2019), global (qiu et al., 2011) and combined ap-proaches (wehrmann et al., 2018)..in most of the previous htc work, researchersmainly focus on modeling the text, the labels are.
figure 1: concepts shared among classes in wos..simply represented as one-hot vectors (zhu andbain, 2017; wehrmann et al., 2018).
actually, theone-hot vectors act as ids without any semantic in-formation.
how to describe a class is also worthy ofdiscussion.
there is some work that embeds labelsinto a vector space which contains more semanticinformation.
compared with one-hot representa-tions, label embeddings have advantages in captur-ing domain-speciﬁc information and importing ex-ternal knowledge.
in the ﬁeld of text classiﬁcation(includes the htc task), researchers propose sev-eral forms of label embeddings to encode differentkinds of information, such as 1) anchor points (duet al., 2019), 2) compatibility between labels andwords (wang et al., 2018; huang et al., 2019; tanget al., 2015), 3) taxonomic hierarchy (cao et al.,2020; zhou et al., 2020) and 4) external knowl-edge (rivas rojas et al., 2020)..although the external knowledge has beenproven effective for htc, it comes from a dictio-nary or knowledge base that humans constructedfor entity deﬁnition, and it doesn’t focus on theclass explanations of a certain htc task.
in thissense, external knowledge is a type of domain-independent information.
the taxonomic hierarchyencoding can capture the structural information ofclasses, which is a sort of domain-speciﬁc infor-mation for htc.
however, actually it only modelsthe hypernym-hyponym relations in the class hi-erarchy.
the process is implicit and difﬁcult to.
proceedingsofthe59thannualmeetingoftheassociationforcomputationallinguisticsandthe11thinternationaljointconferenceonnaturallanguageprocessing,pages5010–5019august1–6,2021.©2021associationforcomputationallinguistics5010be interpreted.
besides the structural connectionsbetween classes, we ﬁnd that the information ofconcept shared between adjacent levels of classesis ignored by previous work.
for instance, there isa parent node named “sports” in a concrete class hi-erarchy (qiu et al., 2011).
its subclasses “surﬁng”and “swimming” are “water” related sports.
thesubclasses “basketball” and “football” are “ball”related sports.
the “water” and “ball” are a typeof abstract concept included in the parent class“sports” and can be shared by the subclasses.
asshown in figure 1, we have a similar observationin wos (kowsari et al., 2017), which is a widelyused public dataset (details in our experiments).
the concept “design” of the parent class “com-puter science” is shared by the child classes “softengineering” and “algorithm design”.
the con-cept “distributed” is shared by “network security”and “distributed computing”.
the concept infor-mation can help to group the classes and measurethe correlation intensity between parent and childclasses.
compared with the information of nodeconnections in the class hierarchy, the concept ismore semantic and ﬁne-grained, but rarely inves-tigated.
although qiu et al.
(2011) have noticedthe concept in htc, they deﬁne the concept in alatent way and the process of represent learning isalso implicit.
additionally, few of previous workinvestigates how to extract the concepts or modelthe sharing interactions among class nodes..to further exploit the information of conceptfor htc, we propose a novel concept-based labelembedding method which can explicitly representthe concepts and model the sharing mechanismamong classes.
more speciﬁcally, we ﬁrst constructa hierarchical attention-based framework which isproved to be effective by wehrmann et al.
(2018)and huang et al.
(2019).
there is one concept-based classiﬁer for each level.
the prior level clas-siﬁcation result (i.e.
predicted soft label embed-ding) is fed into the next level.
a label embed-ding attention mechanism is utilized to measure thecompatibility between texts and classes.
then wedesign a concept sharing module in our model.
itﬁrstly extracts the concepts explicitly in the corpusand represents them in the form of embeddings.
inspired by the capsnet (sabour et al., 2017), weemploy the dynamic routing mechanism.
the itera-tive routing helps to share the information from thelower level to the higher level with the agreementin capsnet.
taking into account the characters.
of htc, we modify the dynamic routing mecha-nism for modeling the concepts sharing interactionsamong classes.
in detail, we calculate the agree-ment between concepts and classes.
an externalknowledge source is taken as an initial referenceof the child classes.
different from the full connec-tions in capsnet, we build routing only betweenthe class and its own child classes to utilize thestructured class hierarchy of htc.
then the rout-ing coefﬁcients are iteratively reﬁned by measuringthe agreement between the parent class conceptsembeddings and the child class embeddings.
in thisway, the module models the concept sharing pro-cess and outputs a novel label representation whichis constructed by the concepts of parent classes.
finally, our hierarchical network adopts such labelembeddings to represent the input document withan attention mechanism and makes a classiﬁcation.
in summary, our major contributions include:.
• this paper investigates the concept in htcproblem, which is a type of domain-speciﬁcinformation ignored by previous work.
wesummarize several kinds of existing label em-beddings and propose a novel label represen-tation: concept-based label embedding..• we propose a hierarchical network to extractthe concepts and model the sharing processvia a modiﬁed dynamic routing algorithm.
toour best knowledge, this is the ﬁrst work thatexplores the concepts of the htc problem inan explicit and interpretable way..• the experimental results on two widely useddatasets empirically demonstrate the effectiveperformance of the proposed model..• we.
public.
complement.
datasetsthewos (kowsari et al., 2017) and dbpe-dia (sinha et al., 2018) by exacting thehierarchy concept and annotating the classeswith the deﬁnitions from wikipedia.
werelease these complementary resources andthe code of the proposed model for furtheruse by the community1..2 model.
in this section, we detailedly introduce our modelcled (figure 2).
it is designed for hierarchi-cal text classiﬁcation with concept-based label.
1https://github.com/wxpkanon/.
cledforhtc.git.
5011figure 2: illustration of our concept-based label embedding via dynamic routing (cled) for htc..embeddings via a modiﬁed dynamic routing mech-anism.
firstly, we construct a hierarchical attention-based framework.
then a concept sharing moduleis designed for extracting concepts and modelingthe sharing mechanism among classes.
the modulelearns a novel label representation with concepts.
finally, the model takes the concept-based labelembeddings to categorize a textual description..2.1 hierarchical attention-based framework.
in recent years, the hierarchical neural network hasbeen proven effective for the htc task by muchwork (sinha et al., 2018; wehrmann et al., 2018;huang et al., 2019).
we adopt it as the frameworkof our model..text encoder we ﬁrst map each documentd = (w1, w2, ..., w|d|) into a low dimensionalword embedding space and denote it as x =(x1, x2, ..., x|d|).
a cnn layer is used for extract-ing n-gram features.
then a bidirectional grulayer extracts contextual features and representsthe document as s = (s1, s2, ..., s|d|)..label embedding attention to measure thecompatibility between labels and texts, we adoptthe label embedding attention mechanism.
given astructured class hierarchy, we denote the label em-beddings of the i-th level as c = (c1, c2, ..., c|li|),where |li| is the number of classes in the i-thlevel.
then we calculate the cosine similarity.
matrix g ∈ r|d|×|li| between words and labelsvia gkj = (s(cid:62)k cj)/((cid:107)sk(cid:107) (cid:107)cj(cid:107)) for the i-th level.
inspired by wang et al.
(2018) and wang et al.
(2019), we adopt convolutional ﬁlters f to mea-sure the correlations rp between the p-th phraseof length 2k + 1 and the classes at i-th level,rp = relu(f ⊗ gp−k:p+k + b), where b ∈ r|li|.
we denote the largest correlation value of the p-th phrase with regard to the labels of i-th levelas tp = max-pooling(rp).
then we get the label-to-text attention score α ∈ r|d| by normalizingt ∈ r|d| with the softmax function.
finally, thedocument representation datt can be obtained byaveraging the word embeddings, weighted by label-to-text attention score: datt = (cid:80)|d|.
k αksk..2.2 concept sharing module (csm).
most of researchers focus on measuring the corre-lations of classes by modeling the structured classhierarchy.
in fact, they only get the information ofgraphic connections.
by contrast, the concepts aremore semantic, ﬁne-grained and interpretable, buthave been ignored.
to further exploit the concepts,we design a concept module to explicitly model themechanism of sharing concepts among classes andmeasure the intensity of interactions..concepts encoder given the corpus of class c,we extract the keywords from the documents andtake top-n ranked keywords as the concepts of class.
5012j.algorithm 1 pseudo code of concepts sharing via dynamic routinginput: all the classes c and their concepts e in level l; all the classes in level (l + 1)output: ccl1: for each concept i of a class c in level l and each of its child class j in level (l + 1): bij ← 0;2: for r iterations do3:.
: the concept-based label embedding of the class in level (l + 1);.
for each concept i of class c in level l: βi ← softmax(bi);for each child class j of class c in level (l + 1): vj ← (cid:80)for each child class j of class c in level (l + 1): ccl(cid:46)squash computes eq.
4for each concept i of class c in level l and each of its child class j in level (l + 1): bij ← bij +ei·ccl.
(cid:46)softmax computes eq.
1.j ← squash(vj).
i βijei;.
4:.
5:.
j.
6:7: end for8: return ccl.
j.c. in the wos dataset, every document is alreadyannotated with several keywords.
so we rank thekeywords by term frequency within each class.
forthe dbpedia dataset, there is no annotated keywordavailable.
we carry out the chi-square (χ2) sta-tistical test, which has been widely accepted asa statistical hypothesis test to evaluate the depen-dency between words and classes (barnard, 1992;palomino et al., 2009; kuang and davison, 2017).
the words are ranked by the χ2 values.
having ex-tracted concepts for each class, we represent themwith word embeddings..to further encode the concepts, we exploit twodifferent ways and make a comparison in experi-ments.
a simple and efﬁcient way is to feed theconcept embeddings into the sharing networks di-rectly.
alternatively, we try the k-means clusteringalgorithm (hartigan and wong, 1979) in considera-tion of the similarity between concepts, then get theembeddings of cluster centers.
the outputs (wordembeddings or cluster centers) of concepts encoderare denoted as ec = (e1, e2, ..., en) for class c..concepts sharing via dynamic routing forthe htc task, we ﬁnd that there are concepts ofparent classes shared by their child classes.
thesemantically related classes share some concepts incommon.
the concepts describe a class in differentviews.
we adopt the dynamic routing mechanismin the capsnet (sabour et al., 2017), which is effec-tive for sharing the information from lower levels tohigher levels.
considering the characters of htc,we modify it to explicitly model the interactionsamong classes and quantitatively measure the in-tensity..to utilize the taxonomic hierarchy, we build rout-ing only between the class and its own child classes,which is different from the full connections in cap-snet.
we take the coupling coefﬁcients between.
concepts of a parent class and all its child classesas the intensities of the sharing interactions.
theintensity (coupling coefﬁcient) βij sums to 1 and isdetermined by a “routing softmax”.
the logit bij isthe log prior probability that concept i of a parentclass should be shared to its child class j in levelln..exp(bij).
βij =.
(cid:80)|ln|kthe logit bij is iteratively reﬁned by adding withthe agreement..exp(bik).
bij ← bij + ei · cclj.the agreement is the scalar product between theconcept embedding ei and the concept-based labelembedding (cl) of the child class ccl.
the vjis the intermediate label embedding of the childclass, which is generated by weighting over all theconcepts of its parent class..j.
(cid:88).
vj =.
βijei.
i.as sabour et al.
(2017) do in the capsnet, wealso use a non-linear “squashing” function whichis effective in our experiments..cclj =.
(cid:107)vj(cid:107)21 + (cid:107)vj(cid:107)2.vj(cid:107)vj(cid:107).
finally, we get the concept-based label embeddingfor class cj by modeling the sharing mechanism.
the new generated label embedding cclis con-structed with several concepts ei in different viewsand affected in different intensities βij.
comparedwith randomly initializing ccl, an external knowl-edge source is taken as an initial reference whichis more effective in experiments.
the proceduresare illustrated in algorithm 1..j.j.
(1).
(2).
(3).
(4).
50132.3 classiﬁcationwe build a classiﬁer for each class level.
let ˆylidenote the predictions of the classes in i-th level..ˆyli = softmax(wom + bo).
(5).
m = relu(wm[dek.
att; dcl.
att; dpre.
att ] + bm).
(6).
att and dcl.
where wo, bo, wm, bm are learnable parame-ters and [; ] is the vector concatenating operator.
the dekatt are document representationsweighted respectively by the label-to-text atten-tion scores via external knowledge (ek) initial-ized label embeddings and concepts-based labelembeddings (cl).
to utilize the predictions in the(i-1)-th level, we feed the document represent dpreattinto the i-th level classiﬁer.
dpreis weighted byattthe attention scores of the predicted soft label em-att = (cid:80)|d|bedding cp.
dprek αksk, where αk =(cid:13)cp(cid:13)k cp)/((cid:107)sk(cid:107) (cid:13)(cid:13)), cp = (cid:80)|li−1|ˆyli−1(s(cid:62)andjcekis the label embedding represented by averag-jing word embeddings of class deﬁnition in externalknowledge (ek encoder in figure 2).
we calculatethe loss of classiﬁer in i-th level as follows:.
cekj.j.lli =.
ce(yli.
n , ˆylin ).
(7).
1n.n(cid:88).
n=1.
where ylin is the one-hot vector of ground truth labelin the i-th level for document n and ce(·, ·) is thecross entropy between two probability vectors.
weoptimize the model parameters by minimize theoverall loss function:.
l =.
lli.
h(cid:88).
i=1.
(8).
# classes in level 1# classes in level 2# classes in level 3# documentstrainvaltest.
wos dbpedia970219342,782278,40830,00034,374.
7134na46,98528,4793,00015,506.table 1: statistics of wos and dbpedia.
is shown in table 1. we complement these twodatasets by extracting the hierarchy concepts andannotating the classes with the deﬁnitions fromwikipedia3..3.2 metrics and parameter settings.
as the state-of-the-art methods do, we take the ac-curacy of each level and the overall accuracy as met-rics.
hyper-parameters are tuned on a validation setby grid search.
we take stanford’s publicly avail-able glove 300-dimensional embeddings trainedon 42 billion tokens from common crawl (pen-nington et al., 2014) as initialization for word em-beddings.
the number of ﬁlters in cnn is 128and the region size is {2, 3}.
the number of hid-den units in bi-gru is 150. we set the maximumlength of token inputs as 512. the rate of dropoutis 0.5. the number of routing iterations is 3. wecompare two different inputs of the sharing net-works: 1) top 30 ranked concepts of each parentclass as inputs; 2) 40 cluster centers generated bythe k-means clustering algorithm on 1k conceptsfor each parent class.
we train the parameters bythe adam optimizer (kingma and ba, 2014) withan initial learning rate of 1e-3 and a batch size of128..where h is the total number of levels in the struc-tured class hierarchy..3.3 baselines.
3 experiments.
3.1 datasets.
we evaluate our model on two widely used hierar-chical text classiﬁcation datasets: web of science(wos; kowsari et al.
(2017)) and dbpedia (sinhaet al., 2018).
the former includes published papersavailable from the web of science (reuters, 2012).
the latter is curated by sinha et al.
(2018) fromdbpedia2.
the general information of datasets.
hdltex kowsari et al.
(2017) prove that the hi-erarchical deep learning networks outperform theconventional approaches (na¨ıve bayes or svm)..hnatc sinha et al.
(2018) propose a hierarchi-cal neural attention-based text classiﬁer.
theybuild one classiﬁer for each level and concatenatethe predicted category embedding at (i-1)-th levelwith each of the encoder’s outputs to calculate at-tention scores for i-th level..2https://wiki.dbpedia.org/.
3https://www.wikipedia.org/.
5014model.
l190.4589.3291.90-.
hdltexhnatcharnna-pnc-bhiagm-tp-lstm 90.54hiagm-tp-gcn90.78hiagm-la-lstm 90.20hiagm-la-gcn90.4193.40cled93.34cledcluster.
wosl284.6682.4261.63-80.5980.7980.0980.0685.6986.19.overall76.5877.4661.2979.9279.3079.3478.2878.2384.3685.13.l199.2699.2199.37-99.4499.4399.4099.4599.4199.46.dbpedial395.5095.3295.71-95.3295.2995.1294.9595.5395.64.l297.1896.0395.69-97.2297.1897.1497.0897.3097.36.overall92.1093.7293.2595.2695.0394.8594.6494.4895.2895.39.table 2: experimental results (accuracy, %) of our proposed model cled and state-of-the-art methods.
weevaluate the test set with the best model on the validation set.
we run our model 5 times with different seedsand report the mean metrics.
improvements are statistically signiﬁcant with p<0.01 based on the t-test.
note thatrivas rojas et al.
(2020) only report the overall accuracy for a-pnc-b..harnn huang et al.
(2019) propose a modelcalled hierarchical attention-based recurrent neu-ral network with one classiﬁer for each class level.
they focus on modeling the dependencies amongclass levels and the text-label compatibility..a-pnc-b rivas rojas et al.
(2020) deﬁne thehtc as a sequence-to-sequence problem and pro-pose a synthetic task of bottom-up-classiﬁcation.
they represent classes with external dictionaries.
their best combined strategy is auxiliary task +parent node conditioning (pnc) + beam search..hiagm zhou et al.
(2020) propose a hierarchy-aware global model.
they employ tree-lstm andhierarchy-gcn as the hierarchy encoder.
text fea-ture propagation (tp) and label attention (la)are utilized for measuring the label-word compati-bility.
there are four hiagm variants: tp-lstm,tp-gcn, la-lstm, and la-gcn..3.4 compared with state-of-the-art methods.
to illustrate the practical signiﬁcance of our pro-posed model, we make comparisons with severalcompetitive state-of-the-art methods.
the resultsof experiments conducted on the public datasetsare shown in table 2. most of the state-of-the-artmethods referred to in section 3.3 adopt a hier-archical attention-based network as their models’framework.
within their models, the hierarchicalframework is effective in utilizing the classiﬁcationresults of the previous levels for the next levels.
the label embedding attention mechanism helpsto import external knowledge sources and the tax-onomic hierarchy.
on both of the two datasets,.
the state-of-the-art methods obtain competitive per-formance.
with a similar framework, our modelfocuses on the concept-based label embedding andoutperforms the other methods on both level andoverall accuracy.
the results indicate the effec-tiveness of the concepts among classes which havebeen ignored by previous work.
the concept-basedlabel embedding models related classes by the shar-ing mechanism with common concepts (visualiza-tions in section 3.6).
the ablation comparisons areshown in section 3.5..the experimental results of the two variants ofour model are also shown in table 2. comparedwith directly feeding the concepts into the shar-ing networks (cled), the variant cledcluster per-forms slightly better.
it indicates that cluster cen-ters generated by the k-means algorithm are moreinformative and effective..3.5 ablation experiments.
to investigate the effectiveness of different partsin our model, we carry out ablation studies.
theexperiment results are shown in table 3..effectiveness of concept-based label embed-ding by comparing the results of cled and themodel without the learnt concept-based label em-bedding (w/o cl), we further conﬁrm that the con-cepts shared among classes help to improve theperformance..effectiveness of dynamic routing we removethe dynamic routing networks from the modelcled.
because there is no dynamic routing toshare the concepts from the parent classes to their.
5015model.
l193.40cled93.35w/o clw/o ek93.27w/o pre93.34w/o reference in csm 93.3093.29w/o dr.wosl285.6985.3685.2985.3385.4585.41.overall84.3684.1084.0484.0384.1784.23.l199.4199.4099.3999.3999.4099.36.dbpedial395.5395.4095.4795.3595.4595.38.l297.3097.2297.2397.1897.1897.23.overall95.2895.1595.1995.0595.1595.12.table 3: ablation studies for different parts in our model..child classes, it is an intuitive way to represent thelabel embeddings by averaging the word embed-dings of the child classes’ concepts.
speciﬁcally,there are top-30 ranked concepts for each parentclass to share with their child classes.
so for themodel without dynamic routing (w/o dr), we rep-resent the child class label embedding with thetop-30 ranked concepts of each child class.
al-though the concepts of child classes are more ﬁne-grained and informative than the concepts of parentclasses, the model cled with the dynamic rout-ing networks to share the concepts among classesperforms better.
it indicates that modeling the shar-ing mechanism and learning to represent the childclasses with common concepts are more effective..effectiveness of external knowledge we takean external knowledge source as the initial refer-ence of child classes in the concepts sharing mod-ule.
when we remove the reference (w/o referencein csm), the results are slightly worse on accuracy.
it demonstrates that the external knowledge makesan efﬁcient reference for the concept sharing..similar to the state-of-the-art methods, the ex-ternal knowledge is also used individually as therepresentation of each class in our model.
it helpsto measure the compatibility between labels andtexts via the attention mechanism.
when we fullyremove the external knowledge and initialize thelabel embeddings randomly (w/o ek), the perfor-mances are slightly worse than that with externalknowledge (cled).
it indicates the effectivenessof external knowledge.
besides, the experimentwhich removes the predicted soft label embedding(w/o pre) proves that, it is effective to utilize thepredictions of previous level..3.6 visualizations of concepts sharing.
in this paper, we explicitly investigate the conceptsharing process.
a concept sharing module is de-signed to model the mechanism of sharing concepts.
among classes and measure the intensity of interac-tions.
the heat map of the learnt dynamic routingscores between the concepts of class “computerscience” and its child classes is illustrated in fig-ure 3. the color changes from white to blue whilethe score increases.
the score indicates the inten-sity between the concept and class in the sharingprocess.
in figure 3, we ﬁnd that the concept “de-sign” is shared by the classes “soft engineering”and “algorithm design”.
the concept “distributed”is shared by the classes “network security” and“distributed computing”.
the concept is shared byrelated classes..we use t-sne (van der maaten and hinton,2008) to visualize the concept embeddings of class“computer science” and the concept-based labelembeddings of its child classes on a 2d map infigure 4. the label embedding (red triangle) isconstructed with the embeddings of concepts (bluedot).
as shown, the class “software engineering”is surrounded by the concepts “optimization” and“design”.
“network security” is surrounded by“cloud”, “machine” and “security”.
the class isdescribed by several concepts in different views..the visualizations in figure 3 and 4 indicatethat we successfully model the concept sharingmechanism in a semantic and explicit way..4 related work.
hierarchical text classiﬁcation with label em-beddings recently, researchers try to adopt thelabel embeddings in the hierarchical text classi-ﬁcation task.
huang et al.
(2019) propose hier-archical attention-based recurrent neural network(harnn) by adopting label embeddings.
maoet al.
(2019) propose to learn a label assignmentpolicy via deep reinforcement learning with labelembeddings.
peng et al.
(2019) propose hierarchi-cal taxonomy-aware and attentional graph rcnnswith label embeddings.
rivas rojas et al.
(2020).
5016figure 3: dynamic routing scores between the con-cepts of class “computer science” (y-axis) and itschild classes (x-axis)..deﬁne the htc task as a sequence-to-sequenceproblem.
their label embedding is deﬁned by ex-ternal knowledge.
for modeling label dependen-cies, zhou et al.
(2020) formulate the hierarchyas a directed graph and introduce hierarchy-awarestructure encoders.
cao et al.
(2020) and chen et al.
(2020a) exploit the hyperbolic representation forlabels by encoding the taxonomic hierarchy..hierarchical text classiﬁcation besides label em-beddings according to the motivation of thiswork, we separate previous work with label embed-dings from the htc task and present it in the aboveparagraph.
besides, existing work is usually cate-gorized into ﬂat, local and global approaches (sillaand freitas, 2011).
the ﬂat classiﬁcation approachcompletely ignores the class hierarchy and only pre-dicts classes at the leaf nodes (aly et al., 2019).
thelocal classiﬁcation approaches could be grouped asa local classiﬁer per node (lcn), a local classiﬁerper parent node (lcpn) and a local classiﬁer perlevel (lcl).
the lcn approach train one binaryclassiﬁer for each node of the hierarchy (fagni andsebastiani, 2007).
banerjee et al.
(2019) applytransfer learning in lcn by ﬁne-tuning the parentclassiﬁer for the child class.
for the lcpn, a multi-class classiﬁer for each parent node is trained todistinguish between its child nodes (wu et al., 2005;dumais and chen, 2000).
xu and geng (2019) in-vestigate the correlation among labels by the label.
figure 4: t-sne plot of the concept embeddings of theclass “computer science” and the concept-based labelembeddings of its child classes..distribution as an lcpn approach.
the lcl ap-proach consists of training one multi-class classiﬁerfor each class level (kowsari et al., 2017; shimuraet al., 2018).
zhu and bain (2017) introduce a b-cnn model which outputs predictions correspond-ing to the hierarchical structure.
chen et al.
(2020b)propose a multi-level learning to rank model withmulti-level hinge loss margins.
the global ap-proach learns a global classiﬁcation model aboutthe whole class hierarchy (cai and hofmann, 2004;gopal and yang, 2013; wing and baldridge, 2014;karn et al., 2017).
qiu et al.
(2011) exploit the la-tent nodes in the taxonomic hierarchy with a globalapproach.
for the need for a large amount of train-ing data, a weakly-supervised global htc methodis proposed by meng et al.
(2019).
meta-learningis adopted by wu et al.
(2019) for htc in a globalway.
in addition, there is some work combined withboth local and global approach (wehrmann et al.,2018).
a local ﬂat tree classiﬁer is introduced bypeng et al.
(2018) which utilizes the graph-cnn..5 conclusion.
in this paper, we investigate the concept which isa kind of domain-speciﬁc and ﬁne-grained infor-mation for the hierarchical text classiﬁcation.
wepropose a novel concept-based label embeddingmodel.
compared with several competitive state-of-the-art methods, the experimental results on twowidely used datasets prove the effectiveness of ourproposed model.
the visualization of the conceptsand the learnt concept-based label embeddings re-.
5017veal the high interpretability of our model..acknowledgments.
we sincerely thank bingning wang 4 for helpfuldiscussions, and all reviewers and acs for theirinsightful comments, time and efforts..references.
rami aly, steffen remus, and chris biemann.
2019.hierarchical multi-label classiﬁcation of text within proceedings of the 57th an-capsule networks.
nual meeting of the association for computationallinguistics: student research workshop, pages 323–330..siddhartha banerjee, cem akkaya, francisco perez-sorrosal, and kostas tsioutsiouliklis.
2019. hier-archical transfer learning for multi-label text classi-in proceedings of the 57th annual meet-ﬁcation.
ing of the association for computational linguistics,pages 6295–6300..ga barnard.
1992. introduction to pearson (1900) onthe criterion that a given system of deviations fromthe probable in the case of a correlated system ofvariables is such that it can be reasonably supposedin break-to have arisen from random sampling.
throughs in statistics, pages 1–10.
springer..lijuan cai and thomas hofmann.
2004. hierarchi-cal document categorization with support vector ma-chines.
in proceedings of the thirteenth acm inter-national conference on information and knowledgemanagement, pages 78–87..pengfei cao, yubo chen, kang liu, jun zhao, sheng-ping liu, and weifeng chong.
2020. hypercore:hyperbolic and co-graph representation for auto-in proceedings of the 58th an-matic icd coding.
nual meeting of the association for computationallinguistics, pages 3105–3114..boli chen, xin huang, lin xiao, zixin cai, and lipingjing.
2020a.
hyperbolic interaction model for hier-archical multi-label classiﬁcation.
in proceedings ofthe aaai conference on artiﬁcial intelligence, vol-ume 34, pages 7496–7503..tongfei chen, yunmo chen, and benjamin van durme.
2020b.
hierarchical entity typing via multi-levellearning to rank.
arxiv preprint arxiv:2004.02286..cunxiao du, zhaozheng chen, fuli feng, lei zhu,tian gan, and liqiang nie.
2019. explicit inter-in pro-action model towards text classiﬁcation.
ceedings of the aaai conference on artiﬁcial intel-ligence, volume 33, pages 6359–6366..4https://bingning.wang/research/.
aboutme.
susan dumais and hao chen.
2000. hierarchicalin proceedings ofclassiﬁcation of web content.
the 23rd annual international acm sigir confer-ence on research and development in informationretrieval, pages 256–263..tiziano fagni and fabrizio sebastiani.
2007. on theselection of negative examples for hierarchical textcategorization.
in proceedings of the 3rd language& technology conference (ltc’07), pages 24–28.
citeseer..siddharth gopal and yiming yang.
2013. recursiveregularization for large-scale classiﬁcation with hi-erarchical and graphical dependencies.
in proceed-ings of the 19th acm sigkdd international con-ference on knowledge discovery and data mining,pages 257–265..john a hartigan and manchek a wong.
1979. algo-rithm as 136: a k-means clustering algorithm.
jour-nal of the royal statistical society.
series c (appliedstatistics), 28(1):100–108..wei huang, enhong chen, qi liu, yuying chen, zaihuang, yang liu, zhou zhao, dan zhang, and shi-jin wang.
2019. hierarchical multi-label text clas-siﬁcation: an attention-based recurrent network ap-in proceedings of the 28th acm interna-proach.
tional conference on information and knowledgemanagement, pages 1051–1060..sanjeev karn, ulli waltinger, and hinrich sch¨utze.
2017. end-to-end trainable attentive decoder for hi-erarchical entity classiﬁcation.
in proceedings of the15th conference of the european chapter of the as-sociation for computational linguistics: volume 2,short papers, pages 752–758..diederik p kingma and jimmy ba.
2014. adam: amethod for stochastic optimization.
arxiv preprintarxiv:1412.6980..kamran kowsari, donald e brown, mojtaba hei-darysafa, kiana jafari meimandi, matthew s gerber,and laura e barnes.
2017. hdltex: hierarchicalin 2017 16thdeep learning for text classiﬁcation.
ieee international conference on machine learningand applications (icmla), pages 364–371.
ieee..sicong kuang and brian d davison.
2017. learn-ing word embeddings with chi-square weights forhealthcare tweet classiﬁcation.
applied sciences,7(8):846..laurens van der maaten and geoffrey hinton.
2008.journal of machine.
visualizing data using t-sne.
learning research, 9(11)..yuning mao, jingjing tian, jiawei han, and xiangren.
2019. hierarchical text classiﬁcation with re-in proceedings of theinforced label assignment.
2019 conference on empirical methods in natu-ral language processing and the 9th internationaljoint conference on natural language processing(emnlp-ijcnlp), pages 445–455..5018yu meng, jiaming shen, chao zhang, and jiawei han.
2019. weakly-supervised hierarchical text classiﬁ-cation.
in proceedings of the aaai conference onartiﬁcial intelligence, volume 33, pages 6826–6833..marco a palomino, michael p oakes, and tom wuy-tack.
2009. automatic extraction of keywords for amultimedia search engine using the chi-square test.
in proceedings of the 9th dutch–belgian informa-tion retrieval workshop (dir 2009), pages 3–10..hao peng, jianxin li, yu he, yaopeng liu, mengjiaobao, lihong wang, yangqiu song, and qiang yang.
2018. large-scale hierarchical text classiﬁcationwith recursively regularized deep graph-cnn.
in pro-ceedings of the 2018 world wide web conference,pages 1063–1072..hao peng, jianxin li, senzhang wang, lihong wang,qiran gong, renyu yang, bo li, philip yu, andlifang he.
2019. hierarchical taxonomy-awareand attentional graph capsule rcnns for large-scalemulti-label text classiﬁcation.
ieee transactions onknowledge and data engineering..jeffrey pennington, richard socher, and christopher d.manning.
2014. glove: global vectors for word rep-resentation.
in empirical methods in natural lan-guage processing (emnlp), pages 1532–1543..xipeng qiu, xuan-jing huang, zhao liu, and jinlongzhou.
2011. hierarchical text classiﬁcation within proceedings of the 49th an-latent concepts.
nual meeting of the association for computationallinguistics: human language technologies, pages598–602..thomson reuters.
2012. web of science..kervy rivas rojas, gina bustamante, arturo oncevay,and marco antonio sobrevilla cabezudo.
2020. ef-ﬁcient strategies for hierarchical text classiﬁcation:in pro-external knowledge and auxiliary tasks.
ceedings of the 58th annual meeting of the asso-ciation for computational linguistics, pages 2252–2257, online..sara sabour, nicholas frosst, and geoffrey e hin-ton.
2017. dynamic routing between capsules.
inadvances in neural information processing systems,pages 3856–3866..kazuya shimura, jiyi li, and fumiyo fukumoto.
2018.hft-cnn: learning hierarchical category structurefor multi-label short text categorization.
in proceed-ings of the 2018 conference on empirical methodsin natural language processing, pages 811–816..carlos n silla and alex a freitas.
2011. a survey of hi-erarchical classiﬁcation across different applicationdomains.
data mining and knowledge discovery,22(1-2):31–72..koustuv sinha, yue dong, jackie chi kit cheung, andderek ruths.
2018. a hierarchical neural attention-in proceedings of the 2018based text classiﬁer..conference on empirical methods in natural lan-guage processing, pages 817–823..jian tang, meng qu, and qiaozhu mei.
2015. pte: pre-dictive text embedding through large-scale hetero-geneous text networks.
in proceedings of the 21thacm sigkdd international conference on knowl-edge discovery and data mining, pages 1165–1174..domonkos tikk, gy¨orgy bir´o, and jae dong yang.
2005. experiment with a hierarchical text catego-rization method on wipo patent collections.
in ap-plied research in uncertainty modeling and analy-sis, pages 283–302.
springer..bingning wang, ting yao, qi zhang, jingfang xu,zhixing tian, kang liu, and jun zhao.
2019. docu-ment gated reader for open-domain question answer-ing.
in proceedings of the 42nd international acmsigir conference on research and development ininformation retrieval, pages 85–94..guoyin wang, chunyuan li, wenlin wang, yizhezhang, dinghan shen, xinyuan zhang, ricardohenao, and lawrence carin.
2018. joint embeddingin pro-of words and labels for text classiﬁcation.
ceedings of the 56th annual meeting of the associa-tion for computational linguistics (volume 1: longpapers), pages 2321–2331..jonatas wehrmann, ricardo cerri, and rodrigo bar-ros.
2018. hierarchical multi-label classiﬁcationnetworks.
in international conference on machinelearning, pages 5075–5084..benjamin wing and jason baldridge.
2014. hierarchi-cal discriminative classiﬁcation for text-based geolo-in proceedings of the 2014 conference oncation.
empirical methods in natural language processing(emnlp), pages 336–348..feihong wu, jun zhang, and vasant honavar.
2005.learning classiﬁers using hierarchically structuredin international symposiumclass taxonomies.
on abstraction, reformulation, and approximation,pages 313–320.
springer..jiawei wu, wenhan xiong, and william yang wang.
2019.learning to learn and predict: a meta-learning approach for multi-label classiﬁcation.
arxiv preprint arxiv:1909.04176..changdong xu and xin geng.
2019. hierarchical clas-siﬁcation based on label distribution learning.
inproceedings of the aaai conference on artiﬁcial in-telligence, volume 33, pages 5533–5540..jie zhou, chunping ma, dingkun long, guangwei xu,ning ding, haoyu zhang, pengjun xie, and gong-shen liu.
2020. hierarchy-aware global model forhierarchical text classiﬁcation.
in proceedings of the58th annual meeting of the association for compu-tational linguistics, pages 1106–1117..xinqi zhu and michael bain.
2017. b-cnn: branch con-volutional neural network for hierarchical classiﬁca-tion.
arxiv preprint arxiv:1709.09890..5019