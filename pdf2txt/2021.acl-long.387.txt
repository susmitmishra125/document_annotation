writing by memorizing: hierarchical retrieval-based medical reportgeneration.
xingyi yang∗uc san diegox3yang@eng.ucsd.edu.
muchao ye.
quanzeng you.
penn state universitymuchao@psu.edu.
microsoft azure computer visionquyou@microsoft.com.
fenglong ma†penn state universityfenglong@psu.edu.
abstract.
medical report generation is one of the mostchallenging tasks in medical image analysis.
although existing approaches have achievedpromising results, they either require a prede-ﬁned template database in order to retrieve sen-tences or ignore the hierarchical nature of med-ical report generation.
to address these issues,we propose medwriter that incorporates anovel hierarchical retrieval mechanism to au-tomatically extract both report and sentence-level templates for clinically accurate reportgeneration.
medwriter ﬁrst employs thevisual-language retrieval (vlr) module toretrieve the most relevant reports for the givenimages.
to guarantee the logical coherence be-tween sentences, the language-language re-trieval (llr) module is introduced to retrieverelevant sentences based on the previous gen-erated description.
at last, a language de-coder fuses image features and features fromretrieved reports and sentences to generatemeaningful medical reports.
we veriﬁed theeffectiveness of our model by automatic eval-uation and human evaluation on two datasets,i.e., open-i and mimic-cxr..1.introduction.
medical report generation is the task of generatingreports based on medical images, such as radiologyand pathology images.
given that this task is time-consuming and cumbersome, researchers endeavorto relieve the burden of physicians by automati-cally generating the ﬁndings and descriptions frommedical images with machine learning techniques.
existing studies can be roughly divided intotwo categories, i.e., generation-based and retrieval-based approaches.
generation-based methods, in-cluding lrcn (donahue et al., 2015), coatt (jing.
∗ this work was done when xingyi yang remotely worked.
with dr. fenglong ma..† corresponding author.
figure 1: overview of the proposed medwriter..et al., 2018), and mvh+attl (yuan et al., 2019),focus on generating image captions with a encoder-decoder model that leverage image features.
how-ever, they are unable to produce linguistically di-verse descriptions and depict rare but prominentmedical ﬁndings.
on the other hand, retrieval-based methods such as hrgr-agent (li et al.,2018) and kepp (li et al., 2019), pay attentionto memorizing templates to generate standardizedreports from a predeﬁned retrieval database.
how-ever, the quality of generated reports signiﬁcantlydepends on the manually curated template database.
besides, they only use sentence-level templates forthe generation but ignore to learn the report-leveltemplates, which prevent them from generatingmore accurate reports..to address the aforementioned issues, we pro-pose a new framework called medwriter asshown in figure 1. medwriter introduces anovel hierarchical retrieval mechanism workingwith a hierarchical language decoder to automat-ically learn the dynamic report and sentencetemplates from the data for generating accurateand professional medical reports.
medwriter isinspired by the process of how physicians writemedical reports in real life.
they keep report tem-plates in mind and then generate reports for newimages by using the key information that they ﬁndin the medical images to update the templates sen-tence by sentence..proceedingsofthe59thannualmeetingoftheassociationforcomputationallinguisticsandthe11thinternationaljointconferenceonnaturallanguageprocessing,pages5000–5009august1–6,2021.©2021associationforcomputationallinguistics5000report-level retrievalmulti-queryattentionvisual feature extractionsentence lstmword lstmretrieved reportssentence-level retrievalretrievedsentencesvisual featurereporttemplatesentence template𝐕𝐢𝐬𝐮𝐚𝐥−𝐋𝐚𝐧𝐠𝐮𝐚𝐠𝐞𝐑𝐞𝐭𝐫𝐢𝐞𝐯𝐚𝐥𝐕𝐋𝐑𝐌𝐨𝐝𝐮𝐥𝐞𝐇𝐢𝐞𝐫𝐚𝐫𝐜𝐡𝐢𝐜𝐚𝐥𝐋𝐚𝐧𝐠𝐮𝐚𝐠𝐞𝐃𝐞𝐜𝐨𝐝𝐞𝐫𝐋𝐚𝐧𝐠𝐮𝐚𝐠𝐞−𝐋𝐚𝐧𝐠𝐮𝐚𝐠𝐞𝐑𝐞𝐭𝐫𝐢𝐞𝐯𝐚𝐥𝐋𝐋𝐑𝐌𝐨𝐝𝐮𝐥𝐞in particular, we use three modules to mimicthis process.
first, medwriter generates report-level templates from the visual-language re-trieval (vlr) module using the visual featuresas the queries.
to generate accurate reports,medwriter also predicts disease labels based onthe visual features and extracts medical keywordsfrom the retrieved reports.
we propose a multi-query attention mechanism to learn the report-level template representations.
second, to makethe generated reports more coherent and ﬂuent,we propose a language-language retrieval (llr)module, which aims to learn sentence-level tem-plates for the next sentence generation by analyz-ing between-sentence correlation in the retrievedreports.
finally, a hierarchical language decoderis adopted to generate the full report using visualfeatures, report-level and sentence-level templaterepresentations.
the designed two-level retrievalmechanism for memorization is helpful in generat-ing accurate and diverse medical reports.
to sumup, our contributions are:.
• to the best of our knowledge, we are the ﬁrstto model the memory retrieval mechanism inboth report and sentence levels.
by imitatingthe standardized medical report generation inreal life, our memory retrieval mechanism ef-fectively utilizes existing templates in the two-layer hierarchy in medical texts.
this designallows medwriter to generate more clini-cally accurate and standardized reports..• on top of the retrieval modules, we designa new multi-query attention mechanism tofuse the retrieved information for medical re-port generation.
the fused information can bewell incorporated with the existing image andreport-level information, which can improvethe quality of generated report ..• experiments conducted on two large-scalemedical report generation datasets, i.e., open-i and mimic-cxr show that medwriterachieves better performance compared withstate-of-the-art baselines measured by cider,rouge-l, and bleus.
besides, case stud-ies show that medwriter provides more ac-curate and natural descriptions for medicalimages through domain expert evaluation..2 related work.
generation-based report generation visualcaptioning is the process of generating a textual de-.
scription given an image or a video.
the dominantneural network architecture of the captioning taskis based on the encoder-decoder framework (bah-danau et al., 2014; vinyals et al., 2015; mao et al.,2014), with attention mechanism (xu et al., 2015;you et al., 2016; lu et al., 2017; anderson et al.,2018; wang et al., 2019).
as a sub-task in the medi-cal domain, early studies directly apply state-of-the-art encoder-decoder models as cnn-rnn (vinyalset al., 2015), lrcn (donahue et al., 2015) andadaatt (lu et al., 2017) to medical report genera-tion task.
to further improve long text generationwith domain-speciﬁc knowledge, later generation-based methods introduce hierarchical lstm withco-attention (jing et al., 2018) or use the medicalconcept features (yuan et al., 2019) to attentivelyguide the report generation.
on the other hand, theconcept of reinforcement learning (liu et al., 2019)is utilized to ensure the generated radiology reportscorrectly describe the clinical ﬁndings..to avoid generating clinically non-informativereports, external domain knowledge like knowl-edge graphs (zhang et al., 2020; li et al., 2019)and anchor words (biswal et al., 2020) are utilizedto promote the medical values of diagnostic re-ports.
clara (biswal et al., 2020) also providesan interactive solution that integrates the doctors’judgment into the generation process..retrieval-based report generation retrieval-based approaches are usually hybridized withgeneration-based ones to improve the readability ofgenerated medical reports.
for example, kerp (liet al., 2019) uses abnormality graphs to retrievemost related sentence templates during the genera-tion.
hrgr-agent(li et al., 2018) incorporates re-trieved sentences in a reinforcement learning frame-work for medical report generation.
however, theyall require a template database as the model in-put.
different from these models, medwriter isable to automatically learn both report-level andsentence-level templates from the data, which sig-niﬁcantly enhances the model applicability..3 method.
as shown in figure 2, we propose a new frame-work called medwriter, which consists of threemodules.
the visual-language retrieval (vlr)module works on the report level and uses vi-sual features to ﬁnd the most relevant templatereports based on a multi-view image query.
thelanguage-language retrieval (llr) module.
5001figure 2: details of the proposed medwriter model for medical report generation.
the left part is used to learnreport template representations via the visual-language retrieval (vlr) module, which is further used to generatethe ﬁrst sentence via the hierarchical language decoder.
the right part shows the details of the language-language(llr) module used for generating the remaining sentences..works on the sentence level and retrieves a series ofcandidates that are most likely to be the next sen-tence from the retrieval pool given the generatedlanguage context.
finally, medwriter generatesaccurate, diverse, and disease-speciﬁed medicalreports by a hierarchical language decoder thatfuses the visual, linguistics and pathological infor-mation obtained by vlr and llr modules.
to im-prove the effectiveness and efﬁciency of retrieval,we ﬁrst pretrain vlr and llr modules to buildup a retrieval pool for medical report generation asfollows..3.1 vlr module pretraining.
the vlr module aims to retrieve the most relevantmedical reports from the training report corpus forthe given medical images.
the retrieved reports arefurther used to learn an abstract template for gener-ating new high-quality reports.
towards this goal,we introduce a self-supervised pretraining task byjudging whether an image-report pair come fromthe same subject, i.e., image-report matching.
itis based on an intuitive assumption that an image-report pair from the same subject shares certaincommon semantics.
more importantly, the diseasetypes associated with images and the report shouldbe similar.
thus, in the pretraining task, we alsotake disease categories into consideration..3.1.1 disease classiﬁcation.
i=1, r) where the set {ii}b.the input of the vlr module is a series ofmulti-modal images and the corresponding report({ii}bi=1 consists of bimages, and r denotes the report.
we employ aconvolutional neural network (cnn) fv(·) as theimage encoder to obtain the feature of a given im-.
age ii, i.e., vi = fv(ii), where vi ∈ rk×k×d is thevisual feature for the i-th image ii..with all the extracted features {vi}b.i=1, we addthem together as the inputs of the disease classiﬁca-tion task, which is further used to learn the diseasetype representation as follows,.
b(cid:88).
i=1.
cpred = wcls(.
avgpool(vi)) + bcls,.
(1).
where wcls ∈ rc×d and bcls ∈ rc are the weightand bias terms of a linear model, avgpool is theoperation of average pooling, c is the numberof disease classes, and cpred ∈ rc can be usedto compute disease probabilities as a multi-labelclassiﬁcation task with a sigmoid function, i.e.,pdc = sigmoid(cpred)..image-report matching.
3.1.2the next training task for vlr is to predict whetheran image-report pair belongs to the same subject.
in this subtask, after obtaining the image features{vi}bi=1 and the disease type representation cpred,we extract a context visual vector v by the patho-logical attention..first, for each image feature vi, we use the dis-ease type representation cpred to learn the spatialattention score through a linear transformation,.
(2).
av = watanh(wvvi + wccpred)where av ∈ rk×k, wa, wv and wc are the lineartransformation matrices.
after that, we use the nor-malized spatial attention score αv = softmax(av)to add visual features over all locations (x, y)across the feature map,.
v(cid:48)i =.
(cid:88).
∀x,y.
αv(x, y)vi(x, y)..(3).
5002report-level retrievalspatialattentionmulti-queryattentionvisual feature extractionthe1!
"sentence generation using report-level retrievalsentence lstmword lstm1!
"sentenceretrieved reportsdiseaseclassificationdiseaselabelsmedical keywordssentence-level retrievalthe𝑡"#sentence generation using sentence-level retrieval𝑡−1"#generatedsentencesentence lstmword lstm𝑡"#sentenceretrievedsentencesvisual featurereporttemplatesentence templatediseaserepresentationimage-reportmatching𝒗𝒓!visual−languageretrievalvlrmodulehierarchicallanguagedecoder𝒉$sentencehiddenstatespatialattention𝑡−1"#sentencehiddenstateℎ"%$sentence-sentencematchingdiseaselabelsmedical keywordsmulti-queryattention𝒗!𝒓!𝒖"%$!𝒉"sentencehiddenstatelanguage−languageretrievalllrmodulethen, we compute the context vector v of thei=1 using a linear layer oni, v =b)wf , where wf ∈ rbd×d is.
input image set {ii}bthe concatenation of all the representation v(cid:48)concat(v(cid:48)1, · · · , v(cid:48)the learnable parameter..for the image-report matching task, we also needa language representation, which is extracted by abert (devlin et al., 2018) model fl(·) as the lan-guage encoder.
fl(·) converts the medical report rinto a semantic vector r = fl(r) ∈ rd.
finally, theprobability of the input pair ({ii}bi=1, r) comingfrom the same subject can be computed as.
pvl = sigmoid(rt v)..(4).
given these two sub-tasks, we simultaneouslyoptimize the cross-entropy losses for both diseaseclassiﬁcation and image-report matching to trainthe vlr module..3.2 llr module pretraining.
a medical report usually has some logical charac-teristics such as describing the patient’s medicalimages in a from-top-to-bottom order.
besides,the preceding and following sentences in a medi-cal report may provide explanations for the sameobject or concept, or they may have certain juxta-position, transition and progressive relations.
au-tomatically learning such characteristics should behelpful for medwriter to generate high-qualitymedical reports.
towards this end, we propose topretrain a language-language retrieval (llr) mod-ule to search for the most relevant sentences for thenext sentence generation.
in particular, we intro-duce a self-supervised pretraining task for llr todetermine if two sentences {si, sj} come from thesame report, i.e., sentence-sentence matching..similar to the vlr module, we use a bertmodel fs(·) as the sentence encoder to embed thesentence inputs {si, sj} into feature vectors si =fs(si), sj = fs(sj).
then the probability that twosentences {si, sj} come from the same medicalreport is measure by.
pll = sigmoid(st.i sj)..(5).
again, the cross-entropy loss is used to optimizethe learning objective given probability pll and theground-truth label of whether s1 and s2 belong tothe same medical report or not..3.3 retrieval-based report generation.
using the pretrained vlr and llr modules,medwriter generates a medical report given a.sequence of input images {ii}bi=1 using a novel hi-erarchical retrieval mechanism with a hierarchicallanguage decoder..r = {rj}ntr.
3.3.1 vlr module for report-level retrievalreport retrieval let d(tr)j=1 denotethe set of all the training reports, where ntr is thenumber of reports in the training dataset.
for eachreport rj, medwriter ﬁrst obtain its vector repre-sentation using fr(·) in the vlr module, which isdenoted as rj = fr(rj).
let pr = {rj}ntrj=1 denotethe set of training report representations.
given themulti-modal medical images {ii}bi=1 of a subject,the vlr module aims to return the top kr medi-j}krcal reports {r(cid:48)j=1 as well as medical keywordswithin in the retrieved reports..j}kr.
speciﬁcally, medwriter extracts the imagefeature v for {ii}bi=1 using the pathological atten-tion mechanism as described in section 3.1. ac-cording to eq.
(4), medwriter then computesa image-report matching sore pvl between v andeach r ∈ pr.
the top kr reports {r(cid:48)j=1 withthe largest scores pvl are considered as the mostrelevant medical reports corresponding to the im-ages, and they are selected as the template descrip-tions.
from these templates, we identify n medicalkeywords {wi}ni=1 using a dictionary as a summa-rization of the template information.
the medicalkeyword dictionary includes disease phenotype, hu-man organ, and tissue, which consists of 36 medicalkeywords extracted from the training data with thehighest frequency..report template representation learning theretrieved reports are highly related to the givenimages, which should be helpful for the report gen-eration.
to make full use of them, we need tolearn a report template representation using theimage feature v, the features of retrieved reportsj}kr{r(cid:48)j=1, medical keywords embeddings {wi}nfor {wi}nbeddings, and the disease embeddings {ck}mfrom predicted disease labels {ck}mease classiﬁcation in section 3.1.1..i=1i=1 learned from the pretrained word em-k=1k=1 using dis-.
we propose a new multi-query attention mech-anism to learn the report template representation.
to specify, we use the image features v as the keyj}krvector k, the retrieved report features {r(cid:48)j=1as the value matrix v , and the embeddings ofboth medical keywords {wi}ni=1 and disease labels{ck}mk=1 as the query vectors q. we modify theoriginal self-attention (vaswani et al., 2017) into.
5003a multi-query attention.
for each query vector qiin q, we ﬁrst get a corresponding attended featureand then transform them into the report templatevector rs after concatenation,.
rs = multiquery({qi}n.i=1, k, v )= concat(attn1, · · · , attnn)w o,where attni = attention(qi, kw k, v w v ),and w k, w v and w o are the transformationmatrices.
generally, the attention function is cal-culated by.
(6).
attention(qg, kg, vg) = softmax(.
t.qgkg(cid:112)dg.
)vg,.
where q, k, v are queries, keys and values ingeneral case, and dg is the dimension of the queryvector..j}kr.
3.3.2 llr module for sentence-level retrievalsince retrieved reports {rtj=1 are highly associ-ated with the input images, the sentence withinthose reports must contain some instructive patho-logical information that is helpful for sentence-level generation.
towards this end, we ﬁrst selectsentences from the retrieved reports and then learnsentence-level template representation..sentence retrieval we ﬁrst divide the retrievedreports into l candidate sentences {sj}lj=1 as theretrieval pool in the llr module.
given thepretrained llr language encoder fs(·), we canobtain the sentence-level feature pool, which isps = {fs(sj)}lj=1.
assume that thegenerated sentence at time t is denoted as ot, andits embedding is ot = fs(ot), which is used to ﬁndks sentences {s(cid:48)j=1 with the highest probabilitiespll from the candidate sentence pool using eq.
(5)in section 3.2..j=1 = {sj}l.j}ks.
template.
sentencerepresentation learningsimilar to the report template representation, westill use the multi-query attention mechanism.
from the retrieved ks sentences, we extract thei}nmedical keywords {w(cid:48)i=1.
besides, we havethe predicted disease labels {ck}mtheirk=1.
embeddings are considered as the query vectors.
the embeddings of the extracted sentence, i.e.,j}ks{fs(s(cid:48)j=1, are treated as thevalue vectors.
the key vector is the currentsentence (word) hidden state hsi ), whichwill be introduced in section 3.3.3. accordingto eq.
(6), we can obtain the sentence templaterepresentation at time t, which is denoted as ut(uw.
i used for word-level generation)..j=1 = {s(cid:48).
t (hw.
j)}ks.
3.3.3 hierarchical language decoderwith the extracted features by the retrieval mech-anism described above, we apply a hierarchicaldecoder to generate radiology reports according tothe hierarchical linguistics structure of the medi-cal reports.
the decoder contains two layers, i.e.,a sentence lstm decoder that outputs sentencehidden states, and a word lstm decoder whichdecodes the sentence hidden states into natural lan-guages.
in this way, reports are generated sentenceby sentence..sentence-level lstm for generating the t-thsentence, medwriter ﬁrst uses the previous t − 1sentences to learn the sentence-level hidden statehst .
speciﬁcally, medwriter learns the imagefeature vs based on eq.
(3).
when calculating theattention score with eq.
(2), we consider both theinformation obtained from the previous t − 1 sen-tences (the hidden state hst−1) and the predicteddisease representation from eq.
(1), i.e., replacingcpred with concat(ht−1, cpred).
then the concate-nation of the image feature vs, the report templaterepresentation rs from eq.
(6), and the sentencetemplate representation ust−1 is used as the inputof the sentence lstm to learn the hidden state hst.t = lstms(concat(vs, ushs.
t−1, rs), hs.
t−1),.
(7).
where ust−1 is obtained using the multi-query at-tention, the key vector is the hidden state hst−1, thevalue vectors are the representations of the retrievedsentences according to the (t − 1)-th sentence, andthe query vectors are the embeddings of both medi-cal keywords extracted from the retrieved sentencesand the predicted disease labels..in eq.
(2), where hwi.word-level lstm based on the learned hst ,medwriter conducts the word-by-word gener-ation using a word-level lstm.
for generating the(i + 1)-th word, medwriter ﬁrst learns the imagefeature vw using eq.
(2) by replacing cpred withhwis the hidden state of theii-th word.
medwriter then learns the sentencetemplate representation uwi using the multi-queryattention, where the key vector is the hidden statehwi , value and query vectors are the same as thoseused for calculating ust−1.
finally, the concatena-tion of hsi , vw, and rs is taken as the inputt , uwof the word-level lstm to generate the (i + 1)-thword as follows:.
i = lstmw(concat(hs.
hwwi+1 = argmax(softmax(f f n (hw.
i , vw, rs), hwi ))),.
t , uw.
i−1),.
(8).
5004where f f n (·) is the feed-forward network..4.2 experimental setup.
note that for the ﬁrst sentence generation, we setu0 as 0, and h0 is the randomly initialized vector,to learn the sentence-level hidden state hs1. whengenerating the words of the ﬁrst sentence, we setuw.
i as the 0 vector..4 experiments.
4.1 datasets and baselinesdatasets open-i1(demner-fushman et al.,2016) (a.k.a iu x-ray) provides 7,470 chest x-rays with 3,955 radiology reports.
in our experi-ments, we only utilize samples with both frontaland lateral views, and with complete ﬁndings andimpression sections in the reports.
this results intotally 2,902 cases and 5,804 images.
mimic-cxr2 (johnson et al., 2019) contains 377,110chest x-rays associated with 227,827 radiologyreports, divided into subsets.
we use the same cri-terion to select samples, which results in 71,386reports and 142,772 images..for both datasets, we tokenize all words withmore than 3 occurrences and obtain 1,252 tokenson the open-i dataset and 4,073 tokens on themimic-cxr dataset, including four special to-kens (cid:104)pad(cid:105), (cid:104)start(cid:105), (cid:104)end(cid:105), and (cid:104)unk(cid:105).
the ﬁnd-ings and impression sections are concatenated asthe ground-truth reports.
we randomly divide thewhole datasets into train/validation/test sets with aratio of 0.7/0.1/0.2.
to conduct the disease classi-ﬁcation task, we include 20 most frequent ﬁndingkeywords extracted from mesh tags as diseasecategories on the open-i dataset and 14 chexpertcategories on the mimic-cxr dataset..baselines on both datasets, we compare withfour state-of-the-artimage captioning models:cnn-rnn (vinyals et al., 2015), coattn (jinget al., 2018), mvh+attl (yuan et al., 2019), andv-l retrieval.
v-l retrieval only uses the retrievedreport templates with the highest probability as pre-diction without the generation part based on ourpretrained vlr module.
due to the lack of theopensource code for (wang et al., 2018; li et al.,2019, 2018; donahue et al., 2015) and the templatedatabases for (li et al., 2019, 2018), we only in-clude the reported results on the open-i dataset inour experiments..1https://openi.nlm.nih.gov/faq#.
collection.
2https://physionet.org/content/.
mimic-cxr/2.0.0/.
all input images are resized to 512 × 512, andthe feature map from densenet-121 (huang et al.,2017) is 1024 × 16 × 16. during training, we userandom cropping and color histogram equalizationfor data augmentation..to pretrain the vlr module, the maximumlength of the report is restricted to 128 words.
we train vlr module for 100 epochs with anadam (kingma and ba, 2014) optimizer with 1e-5as the initial learning rate, 1e-5 for l2 regulariza-tion, and 16 as the mini-batch size.
to pretrainthe llr module, the maximum length of each sen-tence is set to 32 words.
we optimize the llrmodule for 100 epochs with an adam (kingma andba, 2014) optimizer with the initial learning rate of1e-5 and a mini-batch size of 64. the learning rateis multiplied by 0.2 every 20 epochs..to train the full model for medwriter, weset the retrieved reports number kr = 5 and sen-tences number ks = 5. extracting n = 5 medicalkeywords and predicting m = 5 disease labelsare used for report generation.
both sentence andword lstm have 512 hidden units.
we freeze theweights for the pretrained vlr and llr modulesand only optimize on the language decoder.
weset the initial learning rate as 3e-4 and mini-batchsize as 32. medwriter takes 10 hours to train onthe open-i dataset and 3 days on the mimic-cxrdataset with four geforce gtx 1080 ti gpus..4.3 quantitative and qualitative results.
table 1 shows the cider, rouge-l, blue, andauc scores achieved by different methods on thetest sets of open-i and mimic-cxr..language evaluation from table 1, we makethe following observations.
first, compared withgeneration-based model, retrieval-based modelthat uses the template reports as results has setup a relatively strong baseline for medical reportgeneration.
second, compared with v-l retrieval,other retrieval-based approaches perform muchbetter in terms of all the metrics.
this again showsthat that by integrating the information retrievalmethod into the deep sequence generation frame-work, we can not only use the retrieved languageinformation as templates to help generate long sen-tences, but also overcome the monotony of onlyusing the templates as the generations.
finally, wesee that the proposed medwriter achieves thehighest language scores on 5/6 metrics on open-i.
5005dataset.
type.
open-i.
mimic-cxr.
generation.
retrieval.
generation.
retrieval.
modelcnn-rnn (vinyals et al., 2015)lrcn (donahue et al., 2015)*tie-net (wang et al., 2018)*coatt (jing et al., 2018)mvh+attl (yuan et al., 2019)v-l retrievalhrgr-agent (li et al., 2018)*kerp (li et al., 2019)*medwriter.
ground truth.
cnn-rnn (vinyals et al., 2015)coatt (jing et al., 2018)mvh+attl (yuan et al., 2019)v-l retrievalmedwriter.
ground truth.
cider0.2940.2850.2790.2770.2290.1440.3430.2800.345–.
0.2450.2340.2640.1860.306–.
rouge-l0.3070.3070.2260.3690.3510.3190.3220.3390.382–.
0.3140.2740.3090.2320.332–.
bleu-10.2160.2230.2860.4550.4520.3900.4380.4820.471–.
0.2470.4100.4240.3060.438–.
bleu-20.1240.1280.1600.2880.3110.2370.2980.3250.336–.
0.1650.2670.2820.1790.297–.
bleu-30.0870.0890.1040.2050.2230.1540.2080.2260.238–.
0.1240.1890.2030.1160.216–.
bleu-40.0660.0680.0740.1540.1620.1050.1510.1620.166–.
0.0980.1440.1530.0760.164–.
auc0.426––0.7070.7250.634––0.8140.915.
0.4720.7450.7380.5790.8330.923.table 1: automatic evaluation on the open-i and mimic-cxr datasets.
* indicates the results reported in (liet al., 2019)..datasets and all metrics on mimic-cxr amongall methods.
medwriter not only improves cur-rent sota model coattn (jing et al., 2018) by5% and mvh+attl (yuan et al., 2019) by 4% onopen-i in average, but also goes beyond soatretrieval-based approaches like kerp (li et al.,2019) and hrgr-agent (li et al., 2018) and sig-niﬁcantly improves the performance, even withoutusing manually curated template databases.
thisillustrates the effectiveness of automatically learn-ing templates and adopting hierarchical retrieval inwriting medical reports..clinical evaluation we train two report classiﬁ-cation bert models on both datasets and use it tojudge whether the generated reports correctly re-ﬂect the ground-truth ﬁndings.
we show the meanroc-auc scores achieved by generated reportsfrom different baselines in the last column of ta-ble 1. we can observe that medwriter achievesthe highest auc scores compared with other base-lines.
in addition, our method achieves the aucscores that are very close to those of professionaldoctors’ reports, with 0.814/0.915 and 0.833/0.923on two datasets.
this shows that the generation per-formance of medwriter has approached the levelof human domain experts, and it embraces greatmedical potentials in identifying disease-relatedmedical ﬁndings..human evaluation we also qualitatively evalu-ate the quality of the generated reports via a userstudy.
we randomly select 50 samples from theopen-i test set and collect ground-truth reports andthe generated reports from both mvh+attl (yuanet al., 2019) and medwriter to conduct the hu-man evaluation.
two experienced radiologists wereasked to give ratings for each selected report, interms of whether the generated reports are realisticand relevant to the x-ray images.
the ratings are.
integers from one to ﬁve.
the higher, the better..table 2 shows average human evaluation resultson medwriter compared with ground truth re-ports and generations of mvh+attl (yuan et al.,2019) on open-i, evaluated in terms of realisticscores and relevant scores.
medwriter achievesmuch higher human preference than the base-line model, even approaching the performance ofground truth reports that wrote by experiencedradiologists.
it shows that medwriter is able togenerate accurate clinical reports that are compara-ble to domain experts..methodground truthmvh+attl (yuan et al., 2019)medwriter.
realistic score3.852.503.68.relevant score3.822.573.44.table 2: user study conducted by two domain experts..qualitative analysis figure 3 shows qualitativeresults of medwriter and baseline models on theopen-i dataset.
medwriter not only produceslonger reports compared with mvh+attl but alsoaccurately detects the medical ﬁndings in the im-ages (marked in red and bold).
on the other hand,we ﬁnd that medwriter is able to put forwardsome supplementary suggestions (marked in blue)and descriptions, which are not in the original re-port but have diagnostic value.
the underlyingreason for this merit comes from the memory re-trieval mechanism that introduces prior medicalknowledge to facilitate the generation process..4.4 ablation study.
we perform ablation studies on the open-i andmimic-cxr datasets to investigate the effective-ness of each module in medwriter.
in each ofthe following studies, we change one module withother modules intact..removing the vlr modulein this experiment,global report feature rs is neglected in eqs.
(7).
5006frontal image.
lateral image.
mvh+attl.
medwriter.
ground truthemphysematous changes.
resolu-tion of prior right midlung inﬁl-trate.
previous (cid:104)unk(cid:105) is normal insize and contour.
lungs are clear.
no focal consolidation pneumoth-orax or pleural effusion.
inter-val (cid:104)unk(cid:105) of previously describedright midlung opacity suggesting re-solved (cid:104)unk(cid:105) process.
lungs arehyperexpanded with ﬂattened di-aphragms.
(cid:104)unk(cid:105) and soft tissueare unremarkable.
chest.
large nodule at the right lungbase that probably represents a gran-uloma although not it is not (cid:104)unk(cid:105)calciﬁed.
there is a (cid:104)unk(cid:105) mm nod-ule in the right lower lobe that is rel-atively dense but not (cid:104)unk(cid:105) calci-ﬁed on the corresponding rib series.
there are probably right hilar calci-ﬁed lymph (cid:104)unk(cid:105).
lungs otherwiseare clear.
there is no pleural effu-sion.
left ribs.
no fracture or focalbony destruction..the heart.
no acute cardiopulmonary dis-is normalease.
in size.
the lungs are clear.
there is no pleural effusionor pneumothorax.
of theright clavicle.
(cid:104)unk(cid:105) (cid:104)unk(cid:105)(cid:104)unk(cid:105) to theare present.
glenoid joints..hyperexpanded lungs.
(cid:104)unk(cid:105)right upper lobe (cid:104)unk(cid:105).
no fo-cal pneumonia.
the cardiome-diastinal silhouette is normalin size and contour.
negativefor focal consolidation pneu-mothorax or large pleuraleffusion.
negative for acutebone abnormality..no acute cardiopulmonary dis-the heart is normal inease.
size and contour.
are clearwithout evidence of inﬁltrate.
is no pneumothorax.
degen-erative changes of the thoracicspine.. head...right upper lobe pneumonia.
consideration may be givenfor primary or (cid:104)unk(cid:105).
rec-ommend ct of the chest maybe helpful for further diagno-sis.
in the interval a 3 cm(cid:104)unk(cid:105) mass has developedin the right lower lobe.
nopneumothorax or pleural ef-fusion.
the mediastinal con-tours are normal..figure 3: examples of ground-truth and generated reports by mvh+attl (yuan et al., 2019) and medwriter.
highlighted red phrases are medical abnormality terms that generated and ground-truth reports have in common.
bold terms are common descriptions of normal tissues.
the text in italics is the opposite meaning of the generatedreport and the actual report.
we mark the supplementary comments to the original report in blue..dataset.
open-i.
mimic-cxr.
modelmedwriter w/o vlrm 0.333medwriter w/o llrm 0.329medwriter w/o hld0.284medwriter0.345medwriter w/o vlrm 0.294medwriter w/o llrm 0.283medwriter w/o hld0.263medwriter0.306.cider rouge-l bleu-1 bleu-2 bleu-30.4660.4530.4340.4710.4320.4250.4180.438.
0.3240.3070.2950.3360.2880.2800.2650.297.
0.2290.2150.2080.2380.2090.2040.1870.216.
0.3730.3540.3170.3820.3170.3050.2870.332.bleu-40.1590.1540.1490.1660.1610.1570.1460.164.table 3: ablation study on both open-i and mimic-cxr datasets..and (8), and the ﬁrst sentence is generated onlybased on image features.
the llr module keepsits functionality.
however, instead of looking forsentence-level templates from the retrieved reports,it searches for most relevant sentences from all thereports.
as can be seen from table 3, removingvlr module (“w/o vlrm”) leads to performancereduction by 2% on average.
this demonstratesthat visual-language retrieval is capable in sketch-ing out the linguistic structure of the whole report.
the rest of the language generation is largely inﬂu-enced by report-level context information..removing the llr module the generation of(t + 1)-th sentence is based on the global report fea-ture rs and the image feature v, without using theretrieved sentences information in eq.
(8).
table 3shows that removing llr module (“w/o llrm”)results in the decease of average evaluation scoresby 4% compared with the full model.
this veri-ﬁes that the llr module plays an essential role ingenerating long and coherent clinical reports..replacing hierarchical language decoder weuse a single layer lstm that treats the whole report.
as a long sentence and conduct the generation word-by-word.
table 3 shows that replacing hierarchicallanguage decoder with a single-layer lstm (“w/ohld”) introduces dramatic performance reduction.
this phenomenon shows that the hierarchical gen-erative model can effectively and greatly improvethe performance of long text generation tasks..5 conclusions.
automatically generating accurate reports frommedical images is a key challenge in medical im-age analysis.
in this paper, we propose a novelmodel named medwriter to solve this problembased on hierarchical retrieval techniques.
in partic-ular, medwriter consists of three main modules,which are the visual-language retrieval (vlr) mod-ule, the language-language retrieval (llr) module,and the hierarchical language decoder.
these threemodules tightly work with each other to automat-ically generate medical reports.
experimental re-sults on two datasets demonstrate the effectivenessof the proposed medwriter.
besides, qualitativestudies show that medwriter is able to generatemeaningful and realistic medical reports..5007references.
peter anderson, xiaodong he, chris buehler, damienteney, mark johnson, stephen gould, and leizhang.
2018. bottom-up and top-down attention forimage captioning and visual question answering.
inproceedings of the ieee conference on computer vi-sion and pattern recognition, pages 6077–6086..dzmitry bahdanau, kyunghyun cho, and yoshua ben-gio.
2014. neural machine translation by jointlyarxiv preprintlearning to align and translate.
arxiv:1409.0473..siddharth biswal, cao xiao, lucas glass, brandonwestover, and jimeng sun.
2020. clara: clinicalreport auto-completion.
in proceedings of the webconference 2020, pages 541–550..dina demner-fushman, marc d kohli, marc b rosen-man, sonya e shooshan, laritza rodriguez, sameerantani, george r thoma, and clement j mcdon-ald.
2016. preparing a collection of radiology ex-journalaminations for distribution and retrieval.
of the american medical informatics association,23(2):304–310..jacob devlin, ming-wei chang, kenton lee, andkristina toutanova.
2018. bert: pre-training ofdeep bidirectional transformers for language under-standing.
corr, abs/1810.04805..jeffrey donahue, lisa anne hendricks, sergio guadar-rama, marcus rohrbach, subhashini venugopalan,kate saenko, and trevor darrell.
2015. long-termrecurrent convolutional networks for visual recogni-in proceedings of the ieeetion and description.
conference on computer vision and pattern recogni-tion, pages 2625–2634..gao huang, zhuang liu, laurens van der maaten, andkilian q weinberger.
2017. densely connected con-in proceedings of the ieeevolutional networks.
conference on computer vision and pattern recogni-tion, pages 4700–4708..baoyu jing, pengtao xie, and eric xing.
2018. onthe automatic generation of medical imaging reports.
in proceedings of the 56th annual meeting of theassociation for computational linguistics (volume1: long papers), pages 2577–2586..alistair ew johnson, tom j pollard, nathaniel rgreenbaum, matthew p lungren, chih-ying deng,yifan peng, zhiyong lu, roger g mark, seth jberkowitz, and steven horng.
2019. mimic-cxr-jpg,a large publicly available database of labeled chestradiographs.
arxiv preprint arxiv:1901.07042..diederik p kingma and jimmy ba.
2014. adam: amethod for stochastic optimization.
arxiv preprintarxiv:1412.6980..christy y li, xiaodan liang, zhiting hu, and eric pxing.
2019. knowledge-driven encode, retrieve,paraphrase for medical image report generation.
in.
proceedings of the aaai conference on artiﬁcial in-telligence, volume 33, pages 6666–6673..yuan li, xiaodan liang, zhiting hu, and eric p xing.
2018. hybrid retrieval-generation reinforced agentfor medical image report generation.
in advances inneural information processing systems, pages 1530–1540..guanxiong liu, tzu-ming harry hsu, matthew mc-dermott, willie boag, wei-hung weng, peterszolovits, and marzyeh ghassemi.
2019. clini-cally accurate chest x-ray report generation.
arxivpreprint arxiv:1904.02633..jiasen lu, caiming xiong, devi parikh, and richardsocher.
2017. knowing when to look: adaptive at-tention via a visual sentinel for image captioning.
inproceedings of the ieee conference on computer vi-sion and pattern recognition, pages 375–383..junhua mao, wei xu, yi yang, jiang wang, zhihenghuang, and alan yuille.
2014. deep captioningwith multimodal recurrent neural networks (m-rnn).
arxiv preprint arxiv:1412.6632..ashish vaswani, noam shazeer, niki parmar, jakobuszkoreit, llion jones, aidan n gomez, łukaszkaiser, and illia polosukhin.
2017. attention is allin advances in neural information pro-you need.
cessing systems, pages 5998–6008..oriol vinyals, alexander toshev, samy bengio, anddumitru erhan.
2015. show and tell: a neural im-age caption generator.
in proceedings of the ieeeconference on computer vision and pattern recogni-tion, pages 3156–3164..weixuan wang, zhihong chen, and haifeng hu.
2019.hierarchical attention network for image captioning.
in proceedings of the aaai conference on artiﬁcialintelligence, volume 33, pages 8957–8964..xiaosong wang, yifan peng, le lu, zhiyong lu, andronald m summers.
2018. tienet: text-image em-bedding network for common thorax disease classiﬁ-cation and reporting in chest x-rays.
in proceedingsof the ieee conference on computer vision and pat-tern recognition, pages 9049–9058..kelvin xu, jimmy ba, ryan kiros, kyunghyun cho,aaron courville, ruslan salakhudinov, rich zemel,and yoshua bengio.
2015. show, attend and tell:neural image caption generation with visual atten-tion.
in international conference on machine learn-ing, pages 2048–2057..quanzeng you, hailin jin, zhaowen wang, chen fang,and jiebo luo.
2016. image captioning with seman-tic attention.
in proceedings of the ieee conferenceon computer vision and pattern recognition, pages4651–4659..jianbo yuan, haofu liao, rui luo, and jiebo luo.
2019. automatic radiology report generation based.
5008on multi-view image fusion and medical concept en-in international conference on medicalrichment.
image computing and computer-assisted interven-tion, pages 721–729.
springer..yixiao zhang, xiaosong wang, ziyue xu, qihang yu,alan l. yuille, and daguang xu.
2020. when radi-ology report generation meets knowledge graph.
inproceedings of the thirty-fourth aaai conferenceon artiﬁcial intelligence, pages 12910–12917..5009