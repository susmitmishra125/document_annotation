learning syntactic dense embedding with correlation graph for automatic readability assessment .
xinying qiu1    yuan chen1    hanwu chen1    jian-yun nie2* yuming shen1* dawei lu3    .
1school of information science and technology,                    2department of computer science guangdong university of foreign studies, china                    and operations research, 3school of liberal arts, renmin university of china            university of montreal, canada .
.
xy.qiu@foxmail.com  nie@iro.umontreal.ca  ymshen2002@163.com wedalu@163.com .
.
.
abstract .
learning  models .
deep for  automatic readability  assessment  generally  discard linguistic  features  traditionally  used  in machine  learning  models  for  the  task.
we propose  to  incorporate  linguistic  features into  neural  network  models  by  learning syntactic  dense  embeddings  based  on linguistic  features.
to  cope  with the relationships between the features, we form a correlation graph among features and use it to learn their embeddings so that similar features  will  be  represented  by  similar embeddings.
experiments  with  six  data sets of two proficiency levels demonstrate that  our  proposed  methodology  can complement  bert-only  model  to  achieve significantly  better  performances for automatic readability assessment.
.
1 .
introduction .
readability  is  the  ease  with  which  a  reader  can understand  a  written  text1.
predicting  readability has been widely applied in education (lennon and burdick,  2004),  book  publishing  (pera  and  ng, 2014), marketing (chebat et al., 2003), newspaper readership (pitler and nenkova, 2008), and health information  communication  (bernstam  et  al., 2005).
ever  since  the  first  study  by  lively  and pressey in 1923, many researchers have developed various  popular  readability  formulas  including flesch  (flesch,  1948),  fog  (gunning,  1969)  and lexile  (stenner  et  al.,  1988).
these  traditional readability  formulas  are  favored  by  domain applications due to their simplicity even though the formulas are mostly based on shallow features and known  to  lack  accuracy  (bruce  et  al.,  1981; davison and kantor, 1982; graesser et al., 2004).
.
.
learning .
its strong reliance on expert knowledge is also a burden to adapt it to a new domain.
approaches,  which       machine incorporate a broader set of morphological, lexical, syntactic,  and  discourse  features,  have  shown  to achieve better accuracy in readability assessment (si  and  callan,  2001;  collins-thompson  and callan,  2005).
figure  1  (a)  describes  a  generic machine-learning for  automatic framework readability  assessment  (ara)  where  manual feature engineering is an important step to extract important for  building features linguistic readability classification models.
to  bypass  the  necessity  of  heavy  feature engineering,  deep  learning  strategies  have  been studied to automatically detect patterns or extract features  related  to  readability  (azpiazu  and  pera, 2019;  martinc  et  al.,  2019;  mohammadi  and khasteh,  2019).
figure  1  (b)  provides  a  generic neural network structure of deep learning approach to ara.
while neural network models take word embedding  as  input,  they  in  general  discard linguistic  features  traditionally  used  in  machine learning  models  (deutsch  et  al.,  2020).
if  ever incorporated,  linguistic  features  such  as  pos  and morphological tags are only used to guide attention mechanism  for  embedding  representation  of  the text (azpiazu and pera, 2019).
pre-trained models such  as  bert  (devlin  et  al.,  2019)  learn  dense representations  of  text  by  informing  the  models with semantically neighboring words, sentences, or context.
despite the attempts of recent research to assess  bert’s  ability  to  implicitly  capture  the structural properties of language (goldberg, 2019; jawahar et al., 2019; kovaleva et al., 2019), it has been observed that bert “tends to rely more on semantic than structural differences during the .
* corresponding authors.
.
1  https://en.wikipedia.org/wiki/readability .
.
.
proceedingsofthe59thannualmeetingoftheassociationforcomputationallinguisticsandthe11thinternationaljointconferenceonnaturallanguageprocessing,pages3013–3025august1–6,2021.©2021associationforcomputationallinguistics3013 .
figure 1: proposed dual-model framework (c) as compared with generic machine learning (a), and generic deep learning framework (b) for automatic readability assessment (ara).
framework to learn syntactic dense embedding for ara is provided in (d).
.
.
1 .
2 .
3 .
linguistic feature 1 percentage of conjunctions average number of characters per word number of clauses per sentence .
linguistic feature 2 .
correlation .
average height of parse tree .
positive .
percentage of unique functional words average number of unique idioms per sentence .
negative .
unrelated .
neutral .
latent factors complex parse tree contains more conjunctions.
length of chinese functional word is short.
.
table 1. motivating examples for constructing knowledge graph with chinese l1 linguistic features for ara .
classification  phase  and  therefore  performs  better on  problems  with  distinct  semantic  differences between  classes”  (martinc  et  al.,  2019).
there  is clearly a lack of explicit consideration of syntactic (and structural) features in the current bert-based models for ara, which is known to be crucial.
in this study, we address the problem of augmenting the  ability  of  bert  with  widely  used  linguistic features in ara.
.
to best integrate with bert, we create syntactic dense  embedding  as  shown  in  figure  1(d).
an important problem we consider in this paper is the possible  relationships  between  different  features.
linguistic  features  defined  by  linguistic  experts may often be related.
table 1 shows three pairs of linguistic readability assessment.
in  example  one,  the  “percentage  of conjunctions” and the “average height of parse tree” may  be  positively  correlated  because  both  reflect the  complexity  of  the  sentences.
in  the  second example, “percentage of unique functional words” .
for  chinese .
features .
in  a  document  is  negatively  correlated  with  the “average number of characters per word” for that document  because  chinese  functional  words  are usually short (i.e., one or two characters).
utilizing all  these  linguistic  features  as  if  they  were independent  may  potentially  hinder  the  classifier.
we propose to consider the possible relationships among linguistic features when creating their dense embeddings with which we could complement the bert embedding representations.
.
in this paper, we represent pairwise correlations between features as triplets with linguistic features as  nodes  and  their  correlations  as  edges.
positive correlation  implies  that  two  features  behave similarly in influencing the readability level of the text  and  should  be  represented  with  similar embeddings.
the set of triplets forms a graph (as illustrated in figure 1(d)).
we then learn the dense representations  of  linguistic  features  with  graph-based  models.
by  encoding the  similarity knowledge  with  dense  embeddings,  the  ara .
.
.
3014classifier models will be better informed and gain predictive  strength.
our  experiments  on  six datasets  will  confirm  the  effectiveness  of  this approach.
.
we  contribute  to  the  research  on  automatic readability assessment in the following directions: (1)  we  provide  three  new  data  sets  of  linguistic features for document-level readability assessment of chinese l1, chinese l2 and english l2 learning.
(2)  we  verify  that  the  correlation  relationships among linguistic features could be utilized to learn syntactic  dense  embeddings.
(3)  we  propose  a dual-channel  neural  network  model  (i.e.,  dual-model) to combine the syntactic dense embeddings and  the  bert  semantic  dense  embeddings  for readability  predictions.
(4)  we  verify,  with  six data sets of chinese and english corpora for l1 and l2 language proficiencies, that the dual-model can significantly improve the predictive performances of the bert-only model.
we provide our data and codes at:  https://github.com/luv2lab/ linguistic-feature-embedding.
.
2  related work .
2.1  automatic readability assessment .
corpora for readability assessment are available for many languages.
among some of the most cited of english  readability  assessment  are  the  weebit corpus  by  vajjala  and  meurers  (2012,  2014)  for english  l1  learning  and  the  cambridge  exam corpus  by  xia  et  al.
(2016)  for  english  l2.
for chinese readability assessment, sung et al.
(2015) evaluated  30  linguistic  features  and  classification models with text books in traditional chinese.
qiu et al.
(2017), lu et al.
(2019), and zhu et al.
(2019) designed  features  of  different  categories  for machine learning methods for chinese l1 and l2 readability  assessment  at  document  and  sentence levels.
similar  works  on  other  languages  include french (todirascu et al., 2016), german (hancke et al.,  2012),  swedish  (pilán  et  al.,  2016),  and japanese (wang and andersen, 2016).
azpiazu and pera (2020) analyzed the most common linguistic features  for  six  languages  and  evaluated  multiple classifiers for cross-lingual readability assessment.
most  of  the  current  work  on  applying  graph-based  methods  or  neural  networks  to  readability assessment  operate  with  word-level  semantic embeddings.
for  example,  jiang  et  al.
(2018) incorporated  word-level  difficulty  from  lexical knowledge  sources  into  knowledge  graph  and .
embedding .
trained enriched word embedding representations.
martinc et al.
(2019) applied three types of neural language  models  at  word  level  for  unsupervised assessment.
mohammadi  and  khasteh  (2019) simplified  the  process  of  feature  extraction  with glove  model and for  word reinforcement  learning  for  english  and  persian readability  assessment.
azpiazu  and  pera  (2019) presented a multiattentive recurrent neural network model  that  considers  raw  words  as  input  and incorporates  attention  mechanism  with  pos  and morphological tags.
deutsh et al.
(2020) proposed a  fusion  model  by  adding  the  numerical  output from transformer to the linguistic features as input into svm classifiers for readability prediction.
we notice that in previous studies, the linguistic features are mostly considered to be independent.
each of them is used as an additional one to another.
however, two features can reflect the same type of linguistic  phenomenon,  and  thus  are  positively correlated in influencing the readability of a text.
the correlation relationships among features may help  learn  dense  representations  of  linguistic features to be utilized by neural network models for better-informed predictions.
.
2.2  feature embedding .
an important question in building neural network models is how to learn embedding representation.
feature  binning  has  been  studied  to  exploit  the relatedness  between  different  intervals  of  feature values  in  feature  vector  representation  (sil  et  al., 2017; liu et al., 2016).
in particular, maddela and xu (2018) applied smooth binning and project each numerical feature into a vector representation with multiple  gaussian  radial  basis  functions.
the embedding the  nuance relationships between different intervals of feature values.
.
approach .
captures .
methods similar to word embedding (mikolov et al., 2013) have been applied to create embeddings of  pos  tags.
chen  and  manning  (2014)  showed that  the  pos  tag  and  arc  labels  exhibit  semantic similarity  like  words  and  embedding  can  capture the similarities between pos tags or arc labels.
we hypothesize that the pair-wise correlations among the linguistic features for ara can also be used to learn  embedding  and  we  propose  to  use  graph-based model for that purpose.
.
there exists a vast amount of research on graph-based embedding (nickel et al., 2016; wang et al., 2017; cai et al., 2018; ji et al., 2020).
we study .
.
.
3015in  particular:  retrofitting two  methodologies (faruqui  et  al.,  2014)  and  transe  (bordes  et  al., 2013).
.
the  resulting  similarities  learned  from  data-driven  embedding  may  not  fully  reflect  the similarities  one  has  in  mind  for  their  application (goldberg, 2017).
retrofitting (faruqui et al., 2014) used  information  from  wordnet,  framenet  and ppdb  to  improve  pre-trained  embedding  vectors so  that  related  words  will  have  more  similar embeddings.
the  method  first  constructs  a  graph (𝑉, 𝐸)  where v is the set of word types, and  𝐸 ⊆𝑉 × 𝑉   indicates  semantic  relationships  among pairs of words with ontology  ω. given an original embedding  vector  𝑞*!
,  a  new  embedding  𝑞!
is learned such that it is closer to  𝑞*!
and its neighbors 𝑞" ,  ∀𝑗   such  that  (𝑖, 𝑗) ∈ 𝐸 and  with  closeness measured by euclidean distance.
the objective is to minimize  ψ(𝑄)  : .
.
.
).
!*+.
ψ(𝑄) = & ’𝛼!‖𝑞!
− 𝑞,!‖" + & 𝛽!#."
/𝑞!
− 𝑞#/.
0. .
(!,#∈’).
where  𝛼  and  𝛽  control the importance of a word embedding  𝑞!
being similar to itself in the original space  or  to  another  word  in  the  same  space connected by relational information.
.
their .
relations.
transe .
while  retrofitting  is  used  to  improve  entity embedding in  a  graph,  knowledge  graph embedding  learns  representations  for  both  the entities  and is  a representative  translational  distance  model  where entities  and  relations  are  modeled  in  the  same euclidean space.
given two entity vectors h, t and a  translation  vector  r  between  them,  the  model requires  𝒉 + 𝒓 ≈ 𝒕  for the observed triple (h, r, t).
hence, transe assumes the score function .
𝑓#(ℎ, 𝑡) = ‖𝒉 + 𝒓 − 𝒕‖$!/$" .
is  low  if  (h,  r,  t)  holds,  and  high  otherwise.
to differentiate between correct and incorrect triples, transe score difference is minimized using margin based pairwise ranking loss.
.
3  methodology .
let  𝐹 =  @𝑓&, … , 𝑓’#b  (where  𝑁(is the number of features)  be  a  linguistic  feature  set  designed  for readability  assessment.
let  matrix  𝒟 be  a representations  of collection  of .
the  vector .
𝑁) documents  with  𝑑!
∈ 𝑅’# ,  where  𝑑!
=(𝑥", … , 𝑥#!
)$, and  𝑥"(1 ≤ 𝑗 ≤ 𝑁()  is the value of feature  𝑓"  in  𝑑!.
to  construct  the  syntactic  dense embeddings  for  document  representation,  we perform the following steps: (1) we apply gaussian-binning method (maddela and  xu,  2018)  to  𝒟  such  that  each  feature  value 𝑥"of  𝑓"    in    𝑑!
is  projected  into  a  k-dimensional vector  𝑥*jjjj⃑ = (𝑦&, … , 𝑦+),   where  𝑦-(1 ≤ 𝑛 ≤ 𝑘) is the distance of feature value  𝑥"  in  𝑑!
to bin n. we  concatenate  the  𝑥*jjjj⃑   for  all  𝑑!
to  form  the initial  data-driven  embedding  of  feature  𝑓" ,  with dimension of  𝑀 = 𝑘 × 𝑁),  ∀𝑗 ∈ 𝑁(.
(2)  we  form  a  feature  graph  𝒢   using  positive correlations  among  the  𝑁(   features  by  setting  a correlation threshold of 0.7. we preserve only the positive correlations in the graph.
(3) let the matrix  l ∈ 𝑅/×’#  be the collection of embeddings of  𝑓" ∈ 𝑅/.
given a feature graph  𝒢 and matrix  l ∈ 𝑅/×’#  , we apply transe (bordes et al., 2013) or retrofitting (faruqui et al., 2014) to learn  optimized  feature  embeddings  for  each feature  𝑓" .
instead  of  random  initialization,  we use  the  data-driven  embedding  of  𝑓" ∈ 𝑅/ from step  (1)  as  the  initial  entity  embedding  for optimization.
the syntactic latent space of  𝑅/  is trained  by  transe  or  retrofitting  respectively  to encode the relationship knowledge implied by the correlations  among  linguistic  features  so  that  the final dense embedding of linguistic feature  𝑓"  will be  closer  to  those  positively  correlated  with  it  in graph  𝒢 .
we  denote  the  matrix  optimized  by transe or retrofitting with  𝐿1 ∈ 𝑅/×’#.
(4) to construct the syntactic dense embeddings of document  representation  with  the  embeddings  of linguistic features, we perform a linear mapping to project  the  document  feature  vectors  onto  the syntactic  latent  space  𝑅/ .
specifically,  given  a feature  vector  of  document  𝑑!
∈ 𝑅’# ,  and  an optimized  syntactic  matrix  𝐿1 ∈ 𝑅/×’# ,  the projected  document  vector  𝑑s!
in  the  syntactic latent space  𝑅/  is defined as:  .
𝑑s!
= 𝐿1𝑑!
= (𝑙&, … , 𝑙/), .
where  𝑙2(1 ≤ 𝑝 ≤ 𝑀)   is  the  projected  value  of the  𝑁(   linguistic  features  of  document  𝑑!
at dimension  p  of  𝑅/ .
we  name  𝑑s!
∈ 𝑅/   the “syntactic dense embedding”.
.
.
.
3016to construct semantic dense embeddings for the documents, we learn the bert average embedding representations  following  the  original  procedures as  shown  in  figure  2,  where  the  final  bert representation  is  the  average  over  all  tokens.
an alternative  approach  is  to  use  the  [cls]  token embedding to represent the text and fine-tune it for prediction.
in  our  pilot  study,  we  experimented rigorously  with  different  finetuning  strategies  for each of the six datasets.
the best finetuning results as  compared  with  the  original  bert  average embeddings are reported in appendix a. the sizes of our corpora are small ranging from 326 to 2500 as described later in table 2. the finetuning process for bert with 110m parameters may fit very well on training set but may not generalize well on test set.
in  the  pilot  study,  we  found  that  the  overall performances of the finetuned bert are not better than  the  original  bert.
therefore,  we  present experimentations with the original average bert embeddings.
.
.
figure 2: bert average embedding .
.
with  the  bert  dense  embeddings  and  the syntactic  dense  embeddings,  we  propose  a  dnn dual  channel  neural  network  model  (i.e.,  dual-model) to predict the documents’ readability levels.
we  first  feed  the  bert  embeddings  into  a  four-layer network and the syntactic dense embeddings into a two-layer network.
we then concatenate the outputs  of  the  two  channels  into  combined syntactic-semantic dense embeddings as input into another two-layer network, with mlp and softmax layers  for  readability  classification.
the  model architecture is provided in figure 3. .
4  experiments .
4.1  data sets .
to  evaluate  our  proposed  models,  we  use  six readability  data  sets  as  shown  in  table  2.    we create  three  data  sets  for  chinese  l1  and  l2  and english l2 readability assessment.
the chinese l1 data sets are textbooks for first language learning for primary school, secondary school, and high-   .
.
.
2  http://www.dzkbw.com .
3  http://www.xgnyy.com .
.
.
.
figure 3: dual-model to combine syntactic and semantic dense embeddings for ara.
school  education  from  three  publishers.
the chinese  l2  data  sets  are  from  5  grades  of  73 textbooks  that  are  most  widely  used  by  7 universities  in  china  for  teaching  chinese  to international  students,  as  described  in  lu  et  al.
(2019).
the  enew  data  set  is  of  4  grades  of english  textbooks  from  new  concept  english series which is one of the most widely used english l2  textbooks  in  china.
we  followed  the  data preparation  of  enct  in  jiang  et  al.
(2018)  to prepare  enew  corpus.
the  raw  data  of  chinese l1and enew data sets are publicly available from   their textbook websites2,3.
.
in addition, we use three benchmark corpora.
we obtain  the  weebit  data  for  english  l1  from  the authors  of vajjala  and  meurers  (2012,  2014).
we re-extract the text from the html files and discard .
3017data sets and grade .
chinese l1 .
chinese l2 .
enew (eng.
l2) .
weebit (eng.
l1) .
onestopenglish (eng.
l2) .
cambridge (eng.
l2) .
1 .
93 .
505 .
72 .
500 .
189 .
2 .
147 .
396 .
96 .
500 .
189 .
3 .
164 .
329 .
60 .
500 .
189 .
4 .
157 .
129 .
48 .
500 .
.
5 .
148 .
143 .
500 .
.
.
64 (ket) .
60 (pet) .
66 (fce) .
67 (cae) .
69 (cpe) .
6 .
163 .
7 .
96 .
8 .
138 .
9 .
94 .
10 .
32 .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
total .
1232 .
1502 .
276 .
2500 .
567 .
326 .
table 2: number of documents for each grade level in each data set .
documents tests  or that  are  fill-in-the-blank duplicate.
we take the middle set of 500 documents by document length for each class to form a 2500-document  weebit  corpus.
we  obtained the cambridge  exam  data  set  for  english  l2 readability assessment (xia et al., 2016) from their website4.
we found 5 duplicate documents in class fce,  therefore  resulting  in  a  total  of  326 documents  of levels.
we  also downloaded  the  onestopenglish  data  set  for english l2 learning from its website5  (vajjala and lučić, 2018).
.
five  grade .
following the feature engineering methodology in  previous  work  (flesch,  1948;  gunning,  1969; kincaid et al., 1975; yang, 1970; feng, 2010; jiang et al., 2014; sung et al., 2015; qiu et al., 2017; lu et al., 2019), we design 102 linguistic features for chinese  l1  and  111  features  for  chinese  l2 readability assessment.
we design 33 features for english l2 referencing vajjala and meurers (2012).
we  use  the  feature  extraction  codes  provided  by vajjala  and  meurers  (2012)  to  recalculate  the  46 feature  values  for  the  2500-document  weebit corpus.
we  acquire  the  155-feature  calculation results from the onestopenglish corpus.
we drop the features that have zero value for all documents and obtain the values of 140 features.
in our pilot study with enew data set, we found that our 33-feature  design  was  effective  and  apply  these  to cambridge  corpus  as  well.
we  provide  linguistic feature descriptions in appendix b. .
4.2  model evaluation .
according  to  our  methodologies,  we  have  two implementations  of  the  dual-channel  model  to combine syntactic and semantic dense embeddings   for  ara:  gfe-transe+bert  and  gfe-retrofit+bert.
both  have  the  same  network architecture  as  in  figure  3. the  difference  is  that gaussian embedding of features are used in transe .
and retrofitting respectively to learn the optimized feature embedding based on correlation graph and then  produce  syntactic  dense  embeddings  of documents.
we compare our methodology with the following baselines: (1)  svm  and  lr  with  document  feature  vector 𝑑!
∈ 𝑅’# ,  which  are  typical  classification methods based on manual features.
.
(2)  bert-only  dnn:  this  is  a  bert-dnn network which has the same architecture as the right-hand  side  bert  channel  in  figure  3. using bert for representation has been found effective (martinc et al., 2019).
.
(3)  raw+bert model: this model concatenates the  bert  dnn  channel  output  with  raw feature  vectors  𝑑!
∈ 𝑅’# to  form  input  into neural network for predictions.
it is to verify if feature embedding is actually needed or if we could  simply  augment  the  bert  embedding with raw feature vectors for prediction.
.
(4)  g-doc+bert:  following  maddela  and  xu (2018),  for  each  feature 𝑥"(𝑗 ≤ 1 ≤ 𝑁()   in 𝑑!
= (𝑥", … , 𝑥#!
)$ ,  we  learn  the  gaussian embedding  𝑥*jjjj⃑and concatenate all of them into a document embedding representation.
we use this syntactic dense embedding not trained by graph relations as the left-channel input in the dual-dnn model in figure 3 to compare with our proposed method.
.
for  evaluation  of  model  effectiveness,  we  use accuracy  and  distance-1  adjacent  accuracy.
adjacent accuracy means that predicting a text to be within one level distance of the true label is still considered  accurate  (heilman  et  al.,  2008).
we perform  5-fold  stratified  cross-validation  and report  average accuracy  and adjacent accuracy.
we provide the hyper parameters of neural network models  and  the  preprocessing  procedures  in appendix c, and the test of correlation thresholds in appendix d..4  http://www.ilexir.co.uk/datasets/index.html .
5  https://zenodo.org/record/1219041   .
.
.
.
3018machine learning model .
single-channel model .
dual-channel model .
data sets .
svm .
lr .
bert-only .
g-doc-only   .
gfe-transe-only .
gfe-retrofit-only .
raw +bert dual-model .
g-doc +bert dual-model .
gfe-transe +bert dual-model .
dual-channel with graph-based feature embedding .
gfe-retrofit.
+bert dual-model .
0.3498 .
0.3157 .
0.3963 .
0.3288 .
0.3734 .
0.3759 .
0.4351 .
0.3758 .
0.4732* .
0.457* .
0.7224 .
0.6858 .
0.7946 .
0.7054 .
0.7565 .
0.7582 .
0.7972 .
0.7492 .
0.8555* .
0.8433* .
0.4447 .
0.486 .
0.6777 .
0.6032 .
0.5519 .
0.6145 .
0.6851 .
0.5979 .
0.6824 .
0.6858* .
0.8668 .
0.8995 .
0.9674 .
0.9214 .
0.8928 .
0.9294 .
0.9661 .
0.9234 .
0.9694* .
0.9627 .
0.7975 .
0.7868 .
0.8425 .
0.8515 .
0.8408 .
0.848 .
0.8441 .
0.9494 .
0.9784 .
0.9675 .
1 .
1 .
0.9892 .
0.9855 .
0.9927 .
0.9094 .
0.9927 .
0.8766 .
0.9927 .
0.5976 .
0.6408 .
0.8348 .
0.77 .
0.66 .
0.7572 .
0.8556 .
0.8672* .
0.8732* .
0.8072 .
0.8416 .
0.9868 .
0.9176 .
0.8628 .
0.908 .
0.988 .
0.9844 .
0.9828 .
0.6384 .
0.7301 .
0.8157 .
0.8116 .
0.7673 .
0.7795 .
0.8233 .
0.8501* .
0.8661* .
0.9683 .
0.9929 .
0.9974 .
0.9982 .
0.9859 .
1 .
0.9982 .
0.9982 .
1 .
1 .
0.6501 .
0.5952 .
0.696 .
0.6993 .
0.6258 .
0.6779 .
0.7177 .
0.7208 .
0.7487* .
0.7852* .
1 .
0.8 .
0.952 .
0.753 .
chinese l1 .
chinese l2 .
enew .
weebit .
one stop english .
cam-bridge .
0.9386 .
0.9048 .
0.9755 .
0.957 .
0.9418 .
0.9325 .
0.9849 .
0.9816 .
0.9816 .
0.9785 .
table 3. model comparisons.
bert’s performances better than machine learning, and other single-channel models are  bolded.
dual-channel  model’s  performances  better  than  bert-only  model  are  bolded  and  italicized.
performances of dual-channel with graph-based feature embedding models (i.e., our proposed methodology) better than bert and other dual-channel models are bolded and starred.
the best performances for each data set are bolded and underlined.
.
5  results and analysis .
we  first  present  the  comparison  of  bert-only dnn model with two traditional machine learning models of svm and logistic regression, and three other single channel dnn models.
table 3 shows the accuracy  and adjacent accuracy  in  the  first and second row for each data set.
we observe that bert-only dnn performs the best in five out of the six data sets except for enew.
this indicates that semantic embedding alone is very effective in ara with neural network models which are better than traditional machine learning models with raw feature  vector  representations.
this  result  is consistent  with  previous  studies  using  neural network models (martinc et al., 2019; azpiazu and pera, 2019).
.
next,  we  compare  bert-only  model  with  the dual-channel dnn models with raw+bert and g-doc+bert.
we  find  that  augmenting  bert with raw feature value vector or document vector based on gaussian embedding can slightly improve the  performance  of  bert,  showing  that  the  raw linguistic  features  contain  additional  structural information  of  the  text  that  are  marginally  but consistently useful to the neural models for all data sets.
.
the  performances  of  our  proposed  method  are presented in the last two columns of table 3. we observe that the two dual-models achieve the best performances among all 10 models in five out of six data sets (except for enew) and are better than the bert-only and the other dual-channel models.
moreover,  except  for  chinese  l2  where  the improvement is relatively smaller, the dual-model improvements are significant (with student t-test at p<0.05 level) in the other four data sets of chinese l1,  weebit,  onestopenglish  and  cambridge.
these results  strongly  support  our  earlier hypothesis that the correlations between linguistic features can provide additional useful information that to complement the semantic dense embeddings.
.
syntactic  dense  embeddings .
learn .
comparing the last two columns of table 3, we can  observe  generally  similar  performances  in using transe or retrofitting on the feature graph.
in theory, we impose a strict closeness constraint in retrofitting, but let transe learn the embedding for the correlation relation freely.
the higher flexibility of transe did not translate into better effectiveness.
we  speculate  that  the  limited  amount  of  training data  may  hinder  our  model  from  taking  full advantage of the flexibility of transe..  .
.
3019 .
.
figure 4: t-sne visualization of semantic (a), syntactic (b), and concatenated (c) dense embeddings for chinese l1 documents of 4 grades with grade indices of 0, 2, 5, and 9 and 40 random documents sampled for each grade.
.
figure 5. compare grade-specific true positive rate (tpr) of dual-model and bert-only model .
figure  4  presents  a  comparison  of  t-sne visualization  of  semantic  and  syntactic  dense embeddings, and the concatenated embedding.
the figure illustrates that the concatenated embedding can produce more closely clustered data points by grade levels.
.
to investigate how dual-model improves over bert-only  model in  predicting  different readability  levels,  we  present  analysis  of  true positive rate (tpr) at each grade level.
for each data  set,  we  select  from  cross  validation  the  best gfe-transe+bert  model  and  the  bert-only model and then apply them to the whole data set.
we construct confusion matrices and calculate tpr for each grade level as: .
.
𝑇𝑃𝑅 =.
𝑇𝑟𝑢𝑒𝑃𝑜𝑠𝑖𝑡𝑖𝑣𝑒 (𝑇𝑟𝑢𝑒𝑃𝑜𝑠𝑖𝑡𝑖𝑣𝑒 + 𝐹𝑎𝑙𝑠𝑒𝑁𝑒𝑔𝑎𝑡𝑖𝑣𝑒).
as  shown  in  figure  5,  for  chinese  l1  we observe that the largest improvements by the dual model are more spread out at grade 3, 5, 8, and 10   .
than for chinese l2 which are at both ends of grade of 1 and 5. in contrast, for the four english corpora, adding  syntactic  dense  embedding  improves  the bert-only  model  more  in  the  middle  and  the higher grade levels.
we also observe from table 3 that  the  improvement  on  chinese  l1  is  more pronounced.
for example, the gfe-transe+bert model for chinese l1 achieved an improvement of 19.4% over bert-only (0.4732 vs. 0.3963), while weebit  achieved  an  improvement  of  3.88%  over bert-only (0.8672 vs 0.8348).
.
we  may  speculate  that  the  differences  in  the improvement  might  be  caused  by  two  factors among  many  others:  (1)  how  important  the syntactic structure is for building the foundational knowledge in learning a certain language; and (2) how  the  semantic  and  syntactic  knowledge  of  a certain  language  is  organized  throughout  the learning  process  in  order  to  lead  the  language learners through grasping the language.
.
we construct the correlation graphs with positive correlation relationship only while we observe that .
.
.
.
3020there exist both positive and negative correlations among  linguistic  features.
to  investigate  the effectiveness by learning considering negative correlation as well, we define an  additional  score  function  for  negatively correlated features used in transe as: .
embedding .
of .
acknowledgments .
this  work  was  supported  by  national  social science  fund  (grant  no.
17bgl068).
we  thank their  helpful the  anonymous  reviewers  for feedback and suggestions.
.
.
.
𝑓#(ℎ, 𝑡) = 1 − ‖𝒉 + 𝒓 − 𝒕‖$!/$" .
references .
we present performance comparisons with gfe-transe+bert model in table 4. we find that both models  perform  similarly,  showing  that  defining positive  correlation  alone  is  sufficient  in  learning dense  embeddings.
we  speculate  that  in  feature embedding, the most important is to make similar features closer in the latent space, while repulsing negatively  correlated  feature  embeddings  away may not make a better representation of the features, which  could  have  already  been  well  separated  in the latent space.
.
data set .
chinese l1 .
chinese l2 .
enew .
weebit .
gfe-transe+bert dual-model pos.
only .
gfe-transe+bert dual-model pos.
& neg.
.
0.4732, 0.8555 .
0.457, 0.8385 .
0.6824, 0.9694 .
0.6938, 0.962 .
0.9094, 0.9927 .
0.9058, 0.9891 .
0.8672, 0.9844 .
0.854, 0.9792 .
onestopeng .
0.8501, 1 .
0.8519, 1 .
cambridge .
0.7487, 0.9816 .
0.7485, 0.9754 .
table 4. comparing performances with positive correlation-only graph and positive+negative correlation graph in gfe-transe+bert model .
6  conclusions .
by  combining  the  semantic  dense  embeddings and  the  syntactic  dense  embedding  in  a  dual-channel neural network model, we propose a new methodology  for  readability  assessment that capture  both  the  semantic  and  the  syntactic knowledge  related  to  readability  discrepancies.
experiments with six data sets and two proficiency levels show that our dual-model is better than the semantic-alone  and  the  syntactic-alone  baselines.
we  prove  that  complementing  semantic  dense embeddings  with  syntactic  dense  embeddings learned with correlation graph of linguistic features can  produce  better-informed  representations  for readability  assessment.
we  will  further  improve our research  by  studying  other  applicable algorithms  and  linguistic  phenomena  that  could benefit  from  learning  syntactic  latent  space  and syntactic dense embedding representations.
.
azpiazu, i. m. and pera, m. s. 2019. multiattentive recurrent  neural  network  architecture  for assessment.
readability multilingual transactions for the computational linguistics, 7, 421-436. .
association .
of .
azpiazu,  i.  m.  and  pera,  m.  s.  2020.  is  cross‐lingual readability assessment possible?
journal of  the  association  for  information  science  & technology, 71(6), 644-656. .
bernstam,  e.v.,  shelton,  d.m.,  walji,  m.,  and meric-bernstam, f. 2005. instruments to assess the  quality  of  health  information  on  the  world wide web: what can our patients actually use?
international  journal  of  medical  informatics, 74(1), 13–19.
.
bordes, a., usunier, n., garcia-duran, a., weston, j.,  and  yakhnenko,  o.
(2013,  december).
translating  embeddings  for  modeling  multi-relational information processing systems (nips) (pp.
1-9).
.
in neural .
data.
.
bruce, b., rubin, a., and starr, k. s. 1981. why readability formulas fail.
ieee transactions on professional communication, pc-24, 50–52.
.
cai, h., vincent w. zheng, and kevin chen-chuan chang.
2018. a comprehensive survey of graph embedding: and applications.
ieee transactions on knowledge and data engineering 30, 9 (2018), 1616–1637.
.
techniques, .
problems, .
chebat, j.-c., gelinas-chebat, c., hombourger, s., and  woodside, a.g.  2003.  testing  consumers’ motivation  and  linguistic  ability  as  moderators readability.
psychology  & of  advertising marketing, 20(7), 599–624 .
chen,  d.  and  manning,  c.  d.    2014. a  fast  and accurate  dependency  parser  using  neural networks.
in proceedings of the 2014 conference on  empirical  methods  in  natural  language processing (emnlp)   740-750. .
collins-thompson, kevyn and jamie callan.
2005. predicting  reading  difficulty  with  statistical language  models.
journal  of  the  american .
.
.
3021society for information science and technology, 56:1448–1462.
.
davison, alice and robert n. kantor.
1982. on the failure of readability formulas to define readable texts: a  case  study  from  adaptations.
reading research quarterly, 17(2):187–209.
.
devlin, j., chang, m. w., lee, k., and toutanova, k.  2019.  bert:  pre-training  of  deep bidirectional  transformers for  language understanding.
in proceedings  of  the  2019 conference of the north american chapter of the association for computational linguistics: human  language  technologies,  volume  1 (long and short papers).
pages 4171-4186. .
deutsch, t., jasbi, m., and shieber, s. m. 2020. linguistic features for readability assessment.
in proceedings  of  the  fifteenth  workshop  on innovative  use  of  nlp for  building educational applications.
pages 1-17. .
faruqui,  m.,  dodge,  j.,  jauhar,  s.  k.,  dyer,  c., hovy,  e.,  and  smith,  n. a.
2015.  retrofitting semantic  lexicons.
word  vectors in proceedings  of  the  2015  conference  of  the north american chapter of the association for computational  linguistics:  human  language technologies.
pages 1606-1615. .
to .
feng  l.  2010.  automatic  readability  assessment.
ph.d thesis.
the city university of new york.
.
flesch  r.  1948.  a  new  readability  yardstick.
.
journal of applied psychology, 32(3): 221. .
goldberg,  y.
2017.  neural  network  methods  for natural  language  processing.
synthesis  lectures on human language technologies, 10(1), 1-309. .
goldberg,  y.
2019.  assessing  bert's  syntactic .
abilities.
arxiv preprint arxiv:1901.05287. .
graesser, a. c., mcnamara, d. s., louwerse, m. m., and cai, z.
2004. coh-metrix: analysis of language.
behavior text  on  cohesion  and research  methods,  instruments,  &  computers, 36(2), 193–202.
.
gunning, r. 1969. the fog index after twenty years.
journal of business communication, 6(2): 3-13 .
hancke,  j.,  vajjala,  s.,  and  meurers,  d.  2012. readability  classification  for  german  using lexical, syntactic, and morphological features.
in proceedings of 24th international conference on computational linguistics.
1063-1080.   .
heilman, m., collins-thompson, k., and eskenazi, m.  2008. an  analysis  of  statistical  models  and features  for  reading  difficulty  prediction.
in proceedings of the third workshop on innovative use of nlp for building educational applications.
pages 71-79. .
jawahar,  g.,  sagot,  b.,  and  seddah,  d.  2019.       what  does  bert  learn  about  the  structure  of language?
acl 2019 57th annual meeting of the association for computational linguistics .
ji,  s.,  pan,  s.,  cambria,  e.,  marttinen,  p.,  and philip,  s.  y.
2021.  a  survey  on  knowledge graphs:  representation,  acquisition,  and applications.
ieee  transactions  on  neural networks and learning systems.
pages 1-21. .
jiang, z., sun, g., gu, q., and chen, d. 2014. an ordinal  multi-class  classification  method  for readability assessment of chinese documents.
in proceedings  of  international  conference  on knowledge and management.
pages 61-72.   .
engineering .
science, .
jiang,  z.,  gu,  q.,  yin,  y.,  and  chen,  d.  2018. enriching  word  embeddings  with  domain knowledge in proceedings of coling 2018. pages 366-378. .
readability  assessment.
.
for .
kincaid, j. p., fishburne jr, r. p., rogers, r. l., and chissom,  b.  s.  1975. derivation  of  new readability formulas for navy enlisted personnel.
naval technical training command millington tn research branch.
.
kovaleva,  o.,  romanov,  a.,  rogers,  a.,  and rumshisky,  a.
2019.  revealing  the  dark secrets  of  bert.
in proceedings  of  the  2019 conference  on  empirical  methods  in  natural language processing and the 9th international joint  conference  on  natural  language processing.
pages 4365-4374. .
lennon,  c.  and  burdick,  h.  2004.  the  lexile for  reading framework  as  an  approach measurement  and  success.
retrieved  from http://goo.gl/ 4ifnbz .
liu, d., lin, w., zhang, s., wei, s., and jiang, h. for  entity preprint .
2016.  neural  networks  models discovery arxiv:1611.03558. .
linking.
.
arxiv .
and .
lu, d., qiu, x., and cai, y.
2019. sentence-level readability  assessment for  l2  chinese learning.
in:hong  jf.,  zhang  y.,  liu  p.(eds) .
.
.
3022chinese lexical semantics.
clsw 2019. lecture notes in computer science, vol 11831. springer, cham, pages 381-392. .
maddela,  m.  and  xu,  w.  2018.  a  word-complexity lexicon and a neural readability ranking  model  for  lexical  simplification.
in proceedings  of  the  2018  conference  on in  natural  language empirical  methods processing.
pages 3749-3760. .
manning, c. d., surdeanu, m., bauer, j., finkel, j. r.,  bethard,  s.,  and  mcclosky,  d.  2014.  the stanford corenlp natural language processing toolkit.
in proceedings of 52nd annual meeting of the association for computational linguistics: system demonstrations pages 55-60. .
martinc, m., pollak, s., and šikonja, m. r. 2019. supervised and unsupervised neural approaches to preprint text arxiv:1907.11779. .
readability.
.
arxiv .
mikolov, t., chen, k., corrado, g., and  dean,  j.
2013. word representations  in  vector  space.
arxiv  preprint arxiv:1301.3781.   .
estimation .
efficient .
of .
mohammadi, h., and khasteh, s. h. 2019. text as environment:  a  deep  reinforcement  learning text  readability  assessment  model.
arxiv preprint arxiv:1912.05957. .
nickel, m., murphy, k., tresp, v., and gabrilovich, e.    2016.  a  review  of  relational  machine learning for knowledge graphs.
in: proceedings of the ieee, 104(1), 11-33. .
pera,  maria  soledad  and  yiu-kai  ng.
2014. automating  readers'  advisory  to  make  book recommendations in proceedings  of  the  8th  acm  conference  on recommender systems (recsys '14).
9–16.
.
for  k-12 .
readers.
.
pilán,  i.,  volodina,  e.,  and  zesch,  t.    2016. predicting proficiency levels in learner writings by  transferring  a  linguistic  complexity  model from in: proceedings  of  26th  international  conference on computational linguistics, pages 2101-2111. .
expert-written .
coursebooks.
.
pitler,  emily  and ani  nenkova.
2008.  revisiting readability: a unified framework for predicting text  quality.
in  proceedings  of the  2008 conference  on  empirical  methods  in  natural language processing (emnlp-2008), 186–195,   .
qiu,  x.,  deng,  k.,  qiu,  l.,  and  wang,  x.
2017. exploring  the  impact  of  linguistic  features  for chinese readability assessment.
in proceedings of  national  ccf  conference  on  natural language processing and chinese computing.
771-783. springer, cham.
.
qiu,  x.,  lu,  d.,  shen,  y.,  and  cai,  y.
2019. feature  representation  with linguistic statistical  relational  learning  for  readability assessment.
of  ccf in  proceedings international conference on natural language processing  and  chinese  computing.
360-369. springer, cham.
.
si, luo and jamie callan.
2001. a statistical model for  scientific  readability.
in  proceedings  of  the 10th  international  conference  on  information knowledge  management  (ickm-2001),  574–576, atlanta, ga. .
sil,  a.,  kundu,  g.,  florian,  r.,  and  hamza,  w. 2018.  neural  cross-lingual  entity  linking.
in proceedings  of the  aaai  conference  on artificial intelligence (vol.
32, no.
1).
.
stenner, a. j., i horabin, d. r. smith, and r. smith.
1988.  the  lexile  framework.
durham,  nc: metametrics .
sung, y. t., lin, w. c., dyson, s. b., chang, k. e., and  chen,  y.  c.    2015.  leveling  l2  texts through readability:  combining  multilevel linguistic features with the cefr.
the modern language journal, 99(2): 371-391. .
todirascu, a., françois, t., bernhard, d., gala, n., and ligozat, a. l. 2016. are cohesive features relevant  for  text  readability  evaluation?
in proceedings of coling 2016, 987-997.   .
vajjala, s. and meurers, d. 2012. on improving the accuracy  of  readability  classification  using insights  from  second  language  acquisition.
in proceedings of the acl 2012 bea 7th workshop.
163–173.
.
for .
vajjala,  s.  and  meurers,  d.  2014.  readability assessment text  simplification:  from analysing  documents  to  identifying  sentential simplifications.
itl-international  journal  of applied linguistics, 165(2), 194-222. .
vajjala,  s.  and  lučić,  i.
2018.  onestopenglish corpus: a new corpus for automatic readability assessment in text proceedings  of  the  thirteenth  workshop  on .
simplification.
.
and .
.
.
3023innovative use of nlp for building educational applications.
297-304. .
wang  s.  and  erik andersen.
2016.  grammatical templates:  improving  text  difficulty  evaluation for learners.
in  proceedings  of coling 2016.
1692–1702   .
language .
wang, q., mao, z., wang, b., and guo, l. 2017. knowledge  graph  embedding:  a  survey  of approaches and applications.
ieee transactions on  knowledge  and  data  engineering,  29(12): 2724-2743. .
xia, m., kochmar, e., and briscoe, t.    2016. text readability assessment for second language learners.
in proceedings of the 11th workshop on innovative use of nlp for building educational applications.
12-22. .
yang  s.  1970.  a  readability  formula  for  chinese of .
language.
ph.d.  thesis.
university wisconsin-madison.
.
zhu, s., song, j., peng, w., guo, d., and wu, g.   2019. text readability assessment for chinese second language teaching.
in: hong jf., zhang y.,  liu  p.  (eds)  chinese  lexical  semantics.
clsw 2019. lecture notes in computer science, vol 11831. springer, cham 393-405. .
appendix b. chinese l1 and l2 linguistic features  .
feature category .
shallow features .
pos   features .
syntactic features .
discourse features .
sub-category character .
words .
sentence  .
phrases .
clauses .
sentences .
entity density coherence .
features used in metrics .
common characters, stroke-counts, characters by hsk levels n-gram, words by hsk levels sentence length adjective, functional words, verbs, nouns, content words, idioms, adverbs noun phrases, verbal phrases, prepositional phrases punctuation-clause,   dependency distance parse tree,   dependency distance entities, named entities .
conjunctions, pronouns .
(note: the full descriptions of the chinese l1 and l2 features  cannot  be  included  in  the  paper  due  to  space limit.
please contact the authors if needed.)
.
english 33 linguistic features .
category .
appendix a. bert-finetuning pilot experiment performances in accuracy compared with bert original as in paper  .
bert-finetuning-only .
bert-only (as in paper) .
lexical features .
data set .
chinese l1 chinese l2 enew weebit onestopeng cambridge .
0.3963 0.6777 0.8425 0.8348 0.8157 0.696 .
0.353 0.5353 0.8881 0.8016 0.8235 0.6687 .
.
linguistic features lexical density (ld) type-token ratio (ttr) corrected ttr root ttr (rttr) bilogarithmic ttr (logttr) uber index (uber) lexical word variation (lv) verb variation-1 (vv1) squared vv1 (svv1) corrected vv1 (cvv1) verb variation 2 (vv2) noun variation (nv) adjective variation (adjv) adverb variation (advv) .
id 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15  modifier variation (modv) 16 .
proportion of words in awl (awl) avg.
num.
characters per word (numchar) avg.
num.
syllables per word (numsyll) .
19  mean length of a sentence .
average number of words per punctuation-clause number of punctuation-clauses per sentence average number of subordinate clauses per punctuation clause average number of subordinate clauses per sentence .
17 .
18 .
20 .
21 .
22 .
23 .
syntactic features .
.
.
.
.
3024appendix d. test of correlation coefficient thresholds .
.
correlation coefficient threshold 0.3 .
0.4 .
0.5 .
0.6 .
0.7 .
0.8 .
accuracy, adjacent accuracy .
0.4651,    0.8498 .
0.461,    0.8434 .
0.4635,    0.8377 .
0.461,    0.8572 .
0.4732,    0.8555 .
0.4594,    0.8385 .
note:  to  choose  an  appropriate  correlation coefficient  threshold  for  constructing  correlation graph, we test different thresholds on chinese l1 corpus with gfe-transe+bert dual model.
the above table shows that threshold 0.7 provides the best  performance  and  therefore  is  used  for  all experiments.
.
.
.
24 .
25 .
26 .
27 .
28 .
29 .
30 31 32 33 .
average number of co-ordinate phrases per punctuation clause average number of co-ordinate phrases per sentence average number of verb phrases per punctuation clause average  number  of  noun  phrases  per sentence   average  number  of  verbal  phrases  per sentence average  number  of  prepositional phrases per sentence   average length of noun phrases average length of verbal phrases   average length of prepositional phrases average height of parse tree .
.
.
appendix c. neural network parameters and corpus preprocessing  batch size 4 4 4 4 4 4 .
chi.
l1 chi.
l2 enew weebit onestopeng.
cambridge .
learning rate 0.0001 0.0001 0.0001 0.0001 0.0001 0.0001 .
max length 512 512 256 256 512 1024 .
60 60 60 60 40 40 .
epoch .
corpus  preprocessing:  to  calculate  linguistic features,  we  need  to  first  preprocess  the  corpus.
for  chinese  data  set  preprocessing,  we  use nlpir6  for word segmentation, ltp7    for pos tagging  and  named  entity  recognition,  and stanford  corenlp  (manning  et  al.,  2014)  for syntactic  parsing,  grammatical  labeling,  and clause annotation.
for preprocessing of enew and  cambridge,  we  use  nltk 8   for  syllable counts and stanford corenlp for all other feature calculations.
for  weebit,  we  re-extract  the documents from the html files and use our own procedures to reconstruct the corpus.
then we use the author’s code for feature calculation (vajjala and  meurers  2012).
we  use  the  feature  values provided by onestopenglish directly (vajjala and lučić 2018).
.
.
.
.
.
6  http://ictclas.nlpir.org/ 7  http://www.ltp-cloud.com/ .
.
8https://github.com/rlvaugh/impractical_python_projects/blob/master/chapter_8/count_syllables.py .
.
3025