structurallm: structural pre-training for form understanding.
chenliang li, bin bi, ming yan, wei wang,songfang huang, fei huang, luo sialibaba group{lcl193798, b.bi, ym119608, hebian.ww}@alibaba-inc.com{songfang.hsf, f.huang, luo.si}@alibaba-inc.com.
abstract.
large pre-trained language models achievestate-of-the-artresults when ﬁne-tuned ondownstream nlp tasks.
however, they almostexclusively focus on text-only representation,while neglecting cell-level layout informationthat is important for form image understanding.
in this paper, we propose a new pre-training ap-proach, structurallm, to jointly leverage celland layout information from scanned docu-ments.
speciﬁcally, we pre-train structurallmwith two new designs to make the most of theinteractions of cell and layout information: 1)each cell as a semantic unit; 2) classiﬁcation ofcell positions.
the pre-trained structurallmachieves new state-of-the-art results in differ-ent types of downstream tasks, including formunderstanding (from 78.95 to 85.14), docu-ment visual question answering (from 72.59to 83.94) and document image classiﬁcation(from 94.43 to 96.08)..1.introduction.
document understanding is an essential problemin nlp, which aims to read and analyze textualdocuments.
in addition to plain text, many real-world applications require to understand scanneddocuments with rich text.
as shown in figure 1,such scanned documents contain various structuredinformation, like tables, digital forms, receipts, andinvoices.
the information of a document imageis usually presented in natural language, but theformat can be organized in many ways from multi-column layout to various tables/forms..inspired by the recent development of pre-trained language models (devlin et al., 2019;liu et al., 2019; wang et al., 2019) in variousnlp tasks, recent studies on document image pre-training (zhang et al., 2020; xu et al., 2019) havepushed the limits of a variety of document imageunderstanding tasks, which learn the interaction be-.
tween text and layout information across scanneddocument images..xu et al.
(2019) propose layoutlm, which isa pre-training method of text and layout for doc-it uses 2d-ument image understanding tasks.
position embeddings to model the word-level lay-out information.
however, it is not enough tomodel the word-level layout information, and themodel should consider the cell as a semantic unit.
it is important to know which words are from thesame cell and to model the cell-level layout in-formation.
for example, as shown in figure 1(a), which is from form understanding task (jaumeet al., 2019), determining that the ”lorillard”and the ”entities” are from the same cell iscritical for semantic entity labeling.
the ”loril-lard entities” should be predicted as answerentity, but layoutlm predicts ”lorillard” and”entities” as two separate entities..the input to traditional natural language tasks isusually presented as plain text, and text-only mod-els need to obtain the semantic representation ofthe input sentences and the semantic relationshipbetween sentences.
in contrast, document imageslike forms and tables are composed of cells thatare recognized as bounding boxes by ocr.
asshown in figure 1, the words from the same cellgenerally express a meaning together and shouldbe modeled as a semantic unit.
this requires atext-layout model to capture not only the seman-tic representation of individual cells but also thespatial relationship between cells..in this paper, we propose structurallm to jointlyexploit cell and layout information from scanneddocuments.
different from previous text-based pre-trained models (devlin et al., 2019; wang et al.,2019) and layoutlm (xu et al., 2019), struc-turallm uses cell-level 2d-position embeddingswith tokens in a cell sharing the same 2d-position.
this makes structurallm aware of which words are.
proceedingsofthe59thannualmeetingoftheassociationforcomputationallinguisticsandthe11thinternationaljointconferenceonnaturallanguageprocessing,pages6309–6318august1–6,2021.©2021associationforcomputationallinguistics6309(a).
(b).
(c).
(d).
figure 1: scanned images of forms and tables with different layouts and formats..from the same cell, and thus enables the model toderive representation for the cells.
in addition, wekeep classic 1d-position embeddings to preservethe positional relationship of the tokens within ev-ery cell.
we propose a new pre-training objectivecalled cell position classiﬁcation, in addition tothe masked visual-language model.
speciﬁcally,we ﬁrst divide an image into n areas of the samesize, and then mask the 2d-positions of some cells.
structurallm is asked to predict which area themasked cells are located in.
in this way, struc-turallm is capable of learning the interactions be-tween cells and layout.
we conduct experimentson three benchmark datasets publicly available, allof which contain table or form images.
empiricalresults show that our structurallm outperformsstrong baselines and achieves new state-of-the-artresults in the downstream tasks.
in addition, struc-turallm does not rely on image features, and thusis readily applicable to real-world document under-standing tasks..we summarize the major contributions in this.
paper as follows:.
• we propose a structural pre-trained model fortable and form understanding.
it jointly lever-ages cells and layout information in two ways:cell-level positional embeddings and a newpre-training objective called cell position clas-siﬁcation..• structurallm signiﬁcantly outperforms allstate-of-the-art models in several downstreamtasks including form understanding (from78.95 to 85.14), document visual question an-swering (from 72.59 to 83.94) and documentimage classiﬁcation (from 94.43 to 96.08)..2 structurallm.
we present structurallm, a self-supervised pre-training method designed to better model the inter-actions of cells and layout information in scanneddocument images.
the overall framework of struc-turallm is shown in figure 2. our approach isinspired by layoutlm (xu et al., 2019), but differ-ent from it in three ways.
first, we use cell-level2d-position embeddings to model the layout infor-mation of cells rather than word-level 2d-positionembeddings.
we also introduce a novel trainingobjective, the cell position classiﬁcation, whichtries to predict the position of the cells only de-pending on the position of surrounding cells andthe semantic relationship between them.
finally,structurallm retains the 1d-position embeddingsto model the positional relationship between to-kens from the same cell, and removes the imageembeddings in layoutlm that is only used in thedownstream tasks..2.1 model architecture.
the architecture overview of structurallm isshown in figure 2. to take advantage of exist-ing pre-trained models and adapt to document im-age understanding tasks, we use the bert (devlinet al., 2019) architecture as the backbone.
thebert model is an attention-based bidirectionallanguage modeling approach.
it has been veriﬁedthat the bert model shows effective knowledgetransfer from the self-supervised nlp tasks with alarge-scale pre-training corpus..based on the architecture, we propose to utilizethe cell-level layout information from documentimages and incorporate them into the transformerencoder.
first, given a set of tokens from different.
6310figure 2: the overall framework of structurallm.
the input words with the same color background are from thesame cell, and the corresponding 2d-positions are also the same..cells and the layout information of cells, the cell-level input embeddings are computed by summingthe corresponding word embeddings, cell-level 2d-position embeddings, and original 1d-position em-beddings.
then, these input embeddings are passedthrough a bidirectional transformer encoder thatcan generate contextualized representations withan attention mechanism..2.2 cell-level input embedding.
given document images, we use an ocr tool torecognize text and serialize the cells (boundingboxes) from top-left to bottom-right.
each docu-ment image is represented as a sequence of cells{c1, ..., cn}, and each cell is composed of a se-quence of words ci = {w1i }.
given thesequences of cells and words, we ﬁrst introduce themethod of cell-level input embedding..i , ..., wm.
cell-level layout embedding.
unlike the po-sition embedding that models the word positionin a sequence, the 2d-position embedding aims tomodel the relative spatial position in a documentimage.
to represent the spatial position of cellsin scanned document images, we consider a docu-ment page as a coordinate system with the top-leftorigin.
in this setting, the cell (bounding box) canbe precisely deﬁned by (x0, y0, x1, y1), where (x0,y0) corresponds to the top-left position, and (x1,y1) represents the bottom-right position.
therefore,we add two cell-level position embedding layersto embed x-axis features and y-axis features sepa-rately.
the words {w1i , ..., wmi } in i-th cell ci sharethe same 2d-position embeddings, which is dif-ferent from the word-level 2d-position embeddingin layoutlm.
as shown in figure 2, the input to-.
kens with the same color background are from thesame cell, and the corresponding 2d-positions arealso the same.
in this way, structurallm can notonly learn the layout information of cells but alsoknow which words are from the same cell, whichis better to obtain the contextual representation ofcells.
in addition, we keep the classic 1d-positionembeddings to preserve the positional relationshipof the tokens within the same cell.
finally, the cell-level layout embeddings are computed by summingthe four 2d-position embeddings and the classic1d-position embeddings..input embedding.
given a sequence of cells{c1, ..., cn}, we use wordpiece (wu et al., 2016) totokenize the words in the cells.
the length of thetext sequence is limited to ensure that the lengthof the ﬁnal sequence is not greater than the maxi-mum sequence length l. the ﬁnal cell-level inputembedding is the sum of the three embeddings.
word embedding represents the word itself, 1d-position embedding represents the token index, andcell-level 2d-position embedding is used to modelthe relative spatial position of cells in a documentimage..2.3 pre-training structurallm.
we adopt two self-supervised tasks during the pre-training stage, which are described as follows..masked visual-language modeling.
we usethe masked visual-language modeling (mvlm)(xu et al., 2019) to make the model learn thecell representation with the clues of cell-level 2d-position embeddings and text embeddings.
we ran-domly mask some of the input tokens but keep thecorresponding cell-level position embeddings, and.
6311then the model is pre-trained to predict the maskedtokens.
with the cell-level layout information,structurallm can know which words surroundingthe mask token are in the same cell and which arein adjacent cells.
in this way, structurallm notonly utilizes the corresponding cell-level positioninformation but also understands the cell-level con-textual representation.
therefore, compared withthe mvlm in layoutlm, structurallm makes useof the cell-level layout information and predicts themask tokens more accurately.
we will compare theperformance of the mvlm with the cell-level lay-out embeddings and word-level layout embeddingsrespectively in section 3.5..cell position classiﬁcation.
in addition to themvlm, we propose a new cell position classi-ﬁcation (cpc) task to model the relative spatialposition of cells in a document.
the previous mod-els represent the layout information at the bottomof the transformer, but the layout information at thetop of the transformer may be weakened.
there-fore, we consider introducing the cell position clas-siﬁcation task so that structurallm can model thecell-level layout information from the bottom up.
given a set of scanned documents, this task aims topredict where the cells are in the documents.
first,we split them into n areas of the same size.
thenwe calculate the area to which the cell belongs tothrough the center 2d-position of the cell.
mean-while, some cells are randomly selected, and the2d-positions of tokens in the selected cells are re-placed with (0; 0; 0; 0).
in this way, structurallmis capable of learning the interactions between cellsand layout.
during the pre-training, a classiﬁcationlayer is built above the encoder outputs.
this layerpredicts a label [1, n ] of the area where the selectedcell is located, and computes the cross-entropy loss.
considering the mvlm and cpc are performedsimultaneously, the cells with masked tokens willnot be selected for the cpc task.
this prevents themodel from not utilizing cell-level layout informa-tion when doing the mvlm task.
we will comparethe performance of different n in section 3.1..pre-training.
structurallm is pre-trained withthe two pre-training tasks and we add the two tasklosses with equal weights.
we will compare the per-formance of mvlm and mvlm+cpc in section3.5..2.4 fine-tuning.
the pre-trained structurallm model is ﬁne-tunedon three document image understanding tasks, eachof which contains form images.
these three tasksare form understanding task, document visual ques-tion answering task, and document image classiﬁ-cation task.
for the form understanding task, struc-turallm predicts b, i, e, s, o tags for each token,and then uses sequential labeling to ﬁnd the fourtypes of entities including the question, answer,header, or other.
for the document visual questionanswering task, we treat it as an extractive qa taskand build a token-level classiﬁer on the top of tokenrepresentations, which is usually used in machinereading comprehension (mrc) (rajpurkar et al.,2016; wang et al., 2018).
for the document imageclassiﬁcation task, structurallm predicts the classlabels using the representation of the [cls] token..3 experiments.
3.1 pre-training conﬁguration.
pre-training dataset.
following layoutlm, wepre-train structurallm on the iit-cdip test col-lection 1.0 (lewis et al., 2006).
it is a large-scalescanned document image dataset, which containsmore than 6 million documents, with more than11 million scanned document images.
the pre-training dataset (iit-cdip test collection) onlycontains pure texts while missing their correspond-ing bounding boxes.
therefore, we need to re-process the scanned document images to obtain thelayout information of cells.
like the pre-processingmethod of layoutlm, we similarly process thedataset by using tesseract 1, which is an open-source ocr engine.
we normalize the actual co-ordinates to integers in the range from 0 to 1,000,and an empty bounding box (0; 0; 0; 0) is attachedto special tokens [cls], [sep] and [pad], whichis similar to (devlin et al., 2019).
implementation details..structurallm isbased on the transformer which consists of a 24-layer encoder with 1024 embedding/hidden size,4096 feed-forward ﬁlter size, and 16 attentionheads.
to take advantage of existing pre-trainedmodels and adapt to document image understand-ing tasks, we initialize the weight of structurallmmodel with the pre-trained roberta (liu et al.,2019) large model except for the 2d-position em-bedding layers..1https://github.com/tesseract-ocr/tesseract.
6312modelbertbase (devlin et al., 2019)robertabase (liu et al., 2019)bertlargerobertalargebros (hong et al., 2021)layoutlmbase (xu et al., 2019)layoutlmlargestructurallmlarge.
precision recall0.67100.69750.70850.73910.81880.81550.82190.8681.
0.54690.63490.61130.67800.80560.75970.75960.8352.f10.60260.66480.65630.70720.81210.78660.78950.8514.parameters110m125m349m355m-113m343m355m.
table 1: model accuracy (precision, recall, f1) on the test set of funsd..following devlin et al.
(2019), for the maskedvisual-language model task, we select 15% of theinput tokens for prediction.
we replace thesemasked tokens with the mask token 80% of thetime, a random token 10% of the time, and an un-changed token 10% of the time.
then, the modelpredicts the corresponding token with the cross-entropy loss.
for the bounding-box position classi-ﬁcation task, we split the document image into nareas of the same size, and then select 15% of thecells for prediction.
we replace the 2d-positions ofwords in the masked cells with the (0; 0; 0; 0) 90%of the time, and an unchanged position 10% of thetime..structurallm is pre-trained on 16 nvidia teslav100 32gb gpus for 480k steps, with eachmini-batch containing 128 sequences of maximumlength 512 tokens.
the adam optimizer is usedwith an initial learning rate of 1e-5 and a lineardecay learning rate schedule.
for the downstreamtasks, we use a single tesla v100 16gb gpu..hyperparameter n. for the cell position clas-siﬁcation task, we test the performances of struc-turallm using different hyperparameter n duringpre-training.
considering that the complete pre-training takes too long, we pre-train structurallmfor 100k steps with a single gpu card to com-pare the performance of different n .
as shown infigure 3, when the n is set as 16, structurallmobtains the highest f1-score on the funsd dataset.
therefore, we set n as 16 during the pre-training..3.2 fine-tuning on form understanding.
we experiment with ﬁne-tuning structurallm onseveral downstream document image understand-ing tasks, especially form understanding tasks.
thefunsd (jaume et al., 2019) is a dataset for formunderstanding.
it includes 199 real, fully anno-tated, scanned forms with 9,707 semantic entitiesand 31,485 words.
the 199 scanned forms are.
figure 3: f1 score of structurallm pre-training w.r.tdifferent hyperparameter n and ﬁne-tuning on funsddataset..split into 149 for training and 50 for testing.
thefunsd dataset is suitable for a variety of tasks,where we just ﬁne-tuning structurallm on seman-tic entity labeling.
speciﬁcally, each word in thedataset is assigned to a semantic entity label from aset of four predeﬁned categories: question, answer,header, or other.
following the previous works, wealso use the word-level f1 score as the evaluationmetric..we ﬁne-tune the pre-trained structurallm onthe funsd training set for 25 epochs.
we set thebatch size to 4, the learning rate to 1e-5.
the otherhyperparameters are kept the same as pre-training.
table 1 presents the experimental results on thefunsd test set.
structurallm achieves better per-formance than all pre-training models.
first, wecompare the structurallm model with two sotatext-only pre-trained models: bert and roberta(liu et al., 2019).
roberta outperforms the bertmodel by a large margin in terms of the base andlarge settings.
compared with the text-onlymodels, the text+layout model layoutlm bringssigniﬁcant performance improvement.
the bestperformance is achieved by structurallm, wherean improvement of 6% f1 point compared with.
6313model.
bertbaserobertabasebertlargerobertalargelayoutlmbaselayoutlmlargestructurallmlarge.
anls.
anlstest set form&table0.63720.66420.67450.69520.69790.72590.8394.
----0.70120.72030.8610.table 2: average normalized levenshtein similar-ity (anls) score on the docvqa test set and theform&table subset from the test set..layoutlm under the same model size.
all the lay-outlm models compared in this paper are initial-ized by roberta.
by consistently outperformingthe pre-training methods, structurallm conﬁrmsits effectiveness in leveraging cell-level layout in-formation for form understanding..3.3 fine-tuning on document visual qa.
docvqa (mathew et al., 2020) is a vqa dataseton the scanned document understanding ﬁeld.
theobjective of this task is to answer questions askedon a document image.
the images provided aresourced from the documents hosted at the industrydocuments library, maintained by the ucsf.
itconsists of 12,000 pages from a variety of docu-ments including forms, tables, etc.
these pagesare manually labeled with 50,000 question-answerpairs, which are split into the training set, valida-tion set and test set with a ratio of about 8:1:1. thedataset is organized as a set of triples (page image,questions, answers).
the ofﬁcial provides the ocrresults of the page images, and there is no objec-tion to using other ocr recognition tools.
ourexperiment is based on the ofﬁcial ocr results.
the task is evaluated using an edit distance basedmetric anls (aka average normalized levenshteinsimilarity).
results on the test set are provided bythe ofﬁcial evaluation site..we ﬁne-tune the pre-trained structurallm on thedocvqa train set and validation set for 5 epochs.
we set the batch size to 8, the learning rate to 1e-5.
table 2 shows the average normalized leven-shtein similarity (anls) scores on the docvqatest set.
we still compare the structurallm modelwith the text-only models and the text-layout model.
compared with layoutlm, structurallm achievedan improvement of over 11% anls point underthe same model size.
in addition, we also compare.
modelbertbaserobertabasebertlargerobertalargevgg-16astacked cnn singlebstacked cnn ensemblebinceptionresnetv2claddernetdmultimodal singleemultimodal ensembleelayoutlmbaselayoutlmlargestructurallmlarge.
acc.
params89.81% 110m90.06% 125m89.92% 349m90.11% 355m90.97%91.11%92.21%92.63%92.77%93.03%93.07%94.42% 113m94.43% 390m96.08% 355m.
-------.
table 3: classiﬁcation accuracy on the rvl-cdip testset.
a (afzal et al., 2017);b (das et al., 2018);c (szegedyet al., 2017);d (sarkhel and nandi, 2019);e (dauphineeet al., 2019).
the form&table subset from the test set.
struc-turallm achieved an improvement of over 14%anls point, which shows that structurallm canlearn better on form and table understanding..3.4 fine-tuning on document classiﬁcation.
finally, we evaluate the document image classiﬁca-tion task using the rvl-cdip dataset (harley et al.,2015).
it consists of 400,000 grayscale images in16 classes, with 25,000 images per class.
thereare 320,000 images for the training set, 40,000images for the validation set, and 40,000 imagesfor the test set.
a multi-class single-label classi-ﬁcation task is deﬁned on rvl-cdip, includingletter, form, invoice, etc.
the evaluation metric isthe overall classiﬁcation accuracy.
text and layoutinformation is extracted by tesseract ocr..we ﬁne-tune the pre-trained structurallm onthe rvl-cdip train set for 20 epochs.
we set thebatch size to 8, the learning rate to 1e-5..different from other natural images, the docu-ment images are texts in a variety of layouts.
asshown in table 3, image-based classiﬁcation mod-els (afzal et al., 2017; das et al., 2018; szegedyet al., 2017) with pre-training perform much betterthan the text-based models, which illustrates thattext information is not sufﬁcient for this task andit still needs layout information.
the experimentresults show that the text-layout model layoutlmoutperforms the image-based approaches and text-based models.
incorporating the cell-level layout.
6314ablationstructurallmw/o cell-level layout embeddingw/o cell position classiﬁcationw/o pre-training.
f10.85140.80240.81250.7072.table 4: ablation tests of structurallm on the funsdform understanding task..figure 4: loss of word prediction over the numberof pre-training steps based on different layout embed-dings..information, structurallm achieves a new state-of-the-art result with an improvement of over 1.5%accuracy point..3.5 ablation study.
we conduct ablation studies to assess the individualcontribution of every component in structurallm.
table 4 reports the results of full structurallmand its ablations on the test set of funsd formunderstanding task.
first, we evaluate how muchthe cell-level layout embedding contributes to formunderstanding by removing it from structurallmpre-training..this ablation results in a drop from 0.8514 to0.8024 on f1 score, demonstrating the importantrole of the cell-level layout embedding.
to studythe effect of the cell position classiﬁcation task instructurallm, we ablate it and the f1 score sig-niﬁcantly drops from 0.8514 to 0.8125. finally,we study the signiﬁcance of full structurallm pre-training.
over 15% of performance degradationresulted from ablating pre-training clearly demon-strates the power of structurallm in leveraging anunlabeled corpus for downstream form understand-ing tasks..actually, after ablating the cell position clas-.
siﬁcation, the biggest difference between struc-turallm and layoutlm is cell-level 2d-positionembeddings or word-level 2d-position embeddings.
the results show that structurallm with cell-level2d-position embeddings performs better than lay-outlm with word-level position embeddings withan improvement of over 2% f1-score point (from0.7895 to 0.8125).
furthermore, we compare theperformance of the mvlm with cell-level layoutembeddings and word-level layout embeddingsrespectively.
as shown in figure 4, the resultsshow that under the same pre-training settings, themvlm training loss with cell-level 2d-positionembeddings can converge lower..3.6 case study.
the motivation behind structurallm is to jointlyexploit cell and layout information across scanneddocument images.
as stated above, compared withlayoutlm, structurallm improves interactions be-tween cells and layout information.
to verify this,we show some examples of the output of layoutlmand structurallm on the funsd test set, as shownin figure 5. take the image on the top-left of figure5 as an example.
in this example, the model needsto label ”call connie drath or carol musgrave at800/424-9876” with the answer entity.
the resultof layoutlm missed ”at 800/424-9876”.
actu-ally, all the tokens of this answer entity are fromthe same cell.
therefore, structurallm predictsthe correct result with the understanding of cell-level layout information.
these examples showthat structurallm predicts the entities more accu-rately with the cell-level layout information.
thesame results can be observed in the figure 5..4 related work.
4.1 machine learning approaches.
statistical machine learning approaches (marinaiet al., 2005; shilman et al., 2005) became the main-stream for document segmentation tasks during thepast decade.
(shilman et al., 2005) consider thelayout information of a document as a parsing prob-lem.
they use a grammar-based loss function toglobally search the optimal parsing tree, and uti-lize a machine learning approach to select featuresand train all parameters during the parsing pro-cess.
in addition, most efforts have been devoted tothe recognition of isolated handwritten and printedcharacters with widely recognized successful re-sults.
for machine learning approaches (shilman.
6315figure 5: examples of the output of layoutlm and structurallm on the funsd dataset.
the division of | meansthat the two phrases are independent labels..et al., 2005; wei et al., 2013), they are usuallytime-consuming to design manually features anddifﬁcult to obtain a high-level abstract semanticcontext.
in addition, these methods usually reliedon visual cues but ignored textual information..4.2 deep learning approaches.
nowadays, deep learning methods have become themainstream for many machine learning problems(yang et al., 2017; borges oliveira and viana, 2017;katti et al., 2018; soto and yoo, 2019).
(yanget al., 2017) propose a pixel-by-pixel classiﬁcationto solve the document semantic structure extrac-tion problem.
speciﬁcally, they propose a multi-modal neural network that considers visual andtextual information, while this work is an end-to-end approach.
(katti et al., 2018) ﬁrst proposea fully convolutional encoder-decoder network topredict a segmentation mask and bounding boxes.
in this way, the model signiﬁcantly outperformsapproaches based on sequential text or documentimages.
in addition, (soto and yoo, 2019) incorpo-rate contextual information into the faster r-cnnmodel.
they involve the inherently localized na-ture of article contents to improve region detectionperformance..4.3 pre-training approaches.
in recent years, self-supervised pre-training hasachieved great success in natural language under-standing (nlu) and a wide range of nlp tasks(devlin et al., 2019; liu et al., 2019; wang et al.,2019).
(devlin et al., 2019) introduced bert, anew language representation model, which is de-signed to pre-train deep bidirectional representa-tions based on the large-scale unsupervised corpus.
it can be ﬁne-tuned with just one additional out-put layer to create state-of-the-art models for awide range of nlp tasks.
inspired by the develop-ment of the pre-trained language models in variousnlp tasks, recent studies on document image pre-training (zhang et al., 2020; xu et al., 2019) dohave pushed the limits of a variety of documentimage understanding tasks, which learn the inter-action between text and layout information acrossscanned document images.
(xu et al., 2019) pro-pose layoutlm, which is a simple but effectivepre-training method of text and layout for the docu-ment image understanding tasks.
by incorporatingthe visual information into the ﬁne-tuning stage,layoutlm achieves new state-of-the-art resultsin several downstream tasks.
(hong et al., 2021)propose a pre-trained language model that repre-sents the semantics of spatially distributed texts.
different from previous pre-training methods on1d text, bros is pre-trained on large-scale semi-.
6316structured documents with a novel area-maskingstrategy while efﬁciently including the spatial lay-out information of input documents..technologies, volume 1 (long and short papers),pages 4171–4186, minneapolis, minnesota.
associ-ation for computational linguistics..5 conclusion.
in this paper, we propose structurallm, a novelstructural pre-training approach on large unlabeleddocuments.
it is built upon an extension of thetransformer encoder, and jointly exploit cell andlayout information from scanned documents..different from previous pre-trained models,structurallm uses cell-level 2d-position embed-dings with tokens in the cell sharing the same 2d-position.
this makes structurallm aware of whichwords are from the same cell, and thus enables themodel to derive representation for the cells.
we pro-pose a new pre-training objective called cell posi-tion classiﬁcation.
in this way, structurallm is ca-pable of learning the interactions between cells andlayout.
we conduct experiments on three bench-mark datasets publicly available, and structurallmoutperforms strong baselines and achieves newstate-of-the-art results in the downstream tasks..references.
muhammad zeshan afzal, andreas k¨olsch, sherazahmed, and marcus liwicki.
2017. cutting the er-ror by half: investigation of very deep cnn and ad-vanced training strategies for document image clas-in 2017 14th iapr international con-siﬁcation.
ference on document analysis and recognition (ic-dar), volume 1, pages 883–888.
ieee..d. a. borges oliveira and m. p. viana.
2017. fast cnn-based document layout analysis.
in 2017 ieee inter-national conference on computer vision workshops(iccvw), pages 1173–1180..arindam das, saikat roy, ujjwal bhattacharya, andswapan k parui.
2018. document image clas-siﬁcation with intra-domain transfer learning andstacked generalization of deep convolutional neuralin 2018 24th international conferencenetworks.
on pattern recognition (icpr), pages 3180–3185.
ieee..tyler dauphinee, nikunj patel, and mohammadmodular multimodal architec-arxiv preprint.
rashidi.
2019.ture for document classiﬁcation.
arxiv:1912.04376..jacob devlin, ming-wei chang, kenton lee, andkristina toutanova.
2019. bert: pre-training ofdeep bidirectional transformers for language under-in proceedings of the 2019 conferencestanding.
of the north american chapter of the associationfor computational linguistics: human language.
adam w harley, alex ufkes, and konstantinos g der-panis.
2015. evaluation of deep convolutional netsfor document image classiﬁcation and retrieval.
in2015 13th international conference on documentanalysis and recognition (icdar), pages 991–995.
ieee..teakgyu hong, donghyun kim, mingi ji, wonseokhwang, daehyun nam, and sungrae park.
2021.
{bros}: a pre-trained language model for under-standing texts in document..guillaume jaume, hazim kemal ekenel, and jean-philippe thiran.
2019. funsd: a dataset for formunderstanding in noisy scanned documents.
corr,abs/1905.13538..anoop raveendra katti, christian reisswig, cordulaguder, sebastian brarda, steffen bickel, johannesh¨ohne, and jean baptiste faddoul.
2018. char-grid: towards understanding 2d documents.
arxivpreprint arxiv:1809.08799..d. lewis, g. agam, s. argamon, o. frieder, d. gross-man, and j. heard.
2006. building a test collectionfor complex document information processing.
si-gir ’06, page 665–666, new york, ny, usa.
asso-ciation for computing machinery..yinhan liu, myle ott, naman goyal, jingfei du, man-dar joshi, danqi chen, omer levy, mike lewis,luke zettlemoyer, and veselin stoyanov.
2019.roberta: a robustly optimized bert pretraining ap-proach.
arxiv preprint arxiv:1907.11692..s. marinai, m. gori, and g. soda.
2005. artiﬁcial neu-ral networks for document analysis and recognition.
ieee transactions on pattern analysis and machineintelligence, 27(1):23–35..m. mathew, dimosthenis karatzas, r. manmatha, andc. jawahar.
2020. docvqa: a dataset for vqa ondocument images.
arxiv, abs/2007.00398..pranav rajpurkar, jian zhang, konstantin lopyrev, andpercy liang.
2016. squad: 100,000+ questions formachine comprehension of text.
in proceedings ofthe 2016 conference on empirical methods in natu-ral language processing, pages 2383–2392, austin,texas.
association for computational linguistics..ritesh sarkhel and arnab nandi.
2019..deter-ministic routing between layout abstractions formulti-scale classiﬁcation of visually rich documents.
in proceedings of the twenty-eighth internationaljoint conference on artiﬁcial intelligence, ijcai-19, pages 3360–3366.
international joint confer-ences on artiﬁcial intelligence organization..m. shilman, p. liang, and p. viola.
2005. learningnongenerative grammatical models for documentin tenth ieee international conferenceanalysis..6317end-to-end text reading and information extractionfor document understanding.
in proceedings of the28th acm international conference on multimedia,pages 1413–1422..on computer vision (iccv’05) volume 1, volume 2,pages 962–969 vol.
2..carlos soto and shinjae yoo.
2019. visual detec-tion with context for document layout analysis.
inproceedings of the 2019 conference on empiricalmethods in natural language processing and the9th international joint conference on natural lan-guage processing (emnlp-ijcnlp), pages 3464–3470, hong kong, china.
association for computa-tional linguistics..christian szegedy, sergey ioffe, vincent vanhoucke,and alexander alemi.
2017.inception-v4,inception-resnet and the impact of residual connec-tions on learning.
in proceedings of the aaai con-ference on artiﬁcial intelligence, volume 31..wei wang, bin bi, ming yan, chen wu, zuyi bao,jiangnan xia, liwei peng, and luo si.
2019. struct-incorporating language structures into pre-bert:arxivtraining for deep language understanding.
preprint arxiv:1908.04577..wei wang, ming yan, and chen wu.
2018. multi-granularity hierarchical attention fusion networksfor reading comprehension and question answering.
in proceedings of the 56th annual meeting of the as-sociation for computational linguistics (volume 1:long papers), pages 1705–1714, melbourne, aus-tralia.
association for computational linguistics..h. wei, m. baechler, f. slimane, and r. ingold.
2013.evaluation of svm, mlp and gmm classiﬁers for lay-out analysis of historical documents.
2013 12th in-ternational conference on document analysis andrecognition, pages 1220–1224..yonghui wu, mike schuster, zhifeng chen, quoc v.le, mohammad norouzi, wolfgang macherey,maxim krikun, yuan cao, qin gao, klausmacherey, jeff klingner, apurva shah, melvin john-son, xiaobing liu, łukasz kaiser, stephan gouws,yoshikiyo kato, taku kudo, hideto kazawa, keithstevens, george kurian, nishant patil, wei wang,cliff young, jason smith, jason riesa, alex rud-nick, oriol vinyals, greg corrado, macduff hughes,and jeffrey dean.
2016. google’s neural machinetranslation system: bridging the gap between humanand machine translation.
corr, abs/1609.08144..yiheng xu, minghao li, lei cui, shaohan huang,furu wei, and ming zhou.
2019. layoutlm: pre-training of text and layout for document image un-derstanding.
corr, abs/1912.13318..xiao yang, ersin yumer, paul asente, mike kraley,daniel kifer, and c lee giles.
2017. learning to ex-tract semantic structure from documents using mul-timodal fully convolutional neural networks.
in pro-ceedings of the ieee conference on computer vi-sion and pattern recognition, pages 5315–5324..peng zhang, yunlu xu, zhanzhan cheng, shiliang pu,jing lu, liang qiao, yi niu, and fei wu.
2020. trie:.
6318