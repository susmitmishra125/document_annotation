a sweet rabbit hole by darcy:using honeypots to detect universal trigger’s adversarial attacks.
thai lepenn state universitythaile@psu.edu.
noseong parkyonsei universitynoseong@yonsei.ac.kr.
dongwon leepenn state universitydongwon@psu.edu.
abstract.
(unitrigger).
is athe universal triggerrecently-proposed powerful adversarialtex-tual attack method.
utilizing a learning-basedmechanism, unitrigger generates a ﬁxedphrase that, when added to any benign inputs,can drop the prediction accuracy of a textualneural network (nn) model to near zero ona target class.
to defend against this attackthat can cause signiﬁcant harm, in this paper,we borrow the “honeypot” conceptfromthe cybersecurity community and proposedarcy, a honeypot-based defense frame-work against unitrigger.
darcy greedilysearches and injects multiple trapdoors intoan nn model to “bait and catch” potentialattacks.
through comprehensive experimentsacross four public datasets, we show thatdarcy detects unitrigger’sadversarialattacks with up to 99% tpr and less than2% fpr in most cases, while maintaining theprediction accuracy (in f1) for clean inputswithin a 1% margin.
we also demonstratethat darcy with multiple trapdoors is alsorobust to a diverse set of attack scenarios withattackers’ varying levels of knowledge andskills.
we release the source code of darcyat:https://github.com/lethaiq/acl2021-darcy-honeypotdefensenlp..1.introduction.
adversarial examples in nlp refer to carefullycrafted texts that can fool predictive machine learn-ing (ml) models.
thus, malicious actors, i.e.,attackers, can exploit such adversarial examplesto force ml models to output desired predictions.
there are several adversarial example generationalgorithms, most of which perturb an original textat either character (e.g., (li et al., 2018; gao et al.,2018)), word (e.g., (ebrahimi et al., 2018; jin et al.
;wallace et al., 2019; gao et al., 2018; garg andramakrishnan, 2020), or sentence level (e.g., (leet al., 2020; gan and ng; cheng et al.))..
original:attack:prediction:.
this movie is awesomezoning zoombie this movie is awesomepositive −→ negative.
original:attack:prediction: negative −→ positive.
this movie is such a waste!
charming this movie is such a waste!.
table 1: examples of the unitrigger attack.
because most of the existing attack methods areinstance-based search methods, i.e., searching anadversarial example for each speciﬁc input, theydo not usually involve any learning mechanisms.
a few learning-based algorithms, such as the uni-versal trigger (unitrigger) (wallace et al., 2019),malcom (le et al., 2020), seq2sick (chenget al.)
and paraphrase network (gan and ng),“learn” to generate adversarial examples that can beeffectively generalized to not a speciﬁc but a widerange of unseen inputs..in general, learning-based attacks are more at-tractive to attackers for several reasons.
first, theyachieve high attack success rates.
for example,unitrigger can drop the prediction accuracy of annn model to near zero just by appending a learnedadversarial phrase of only two tokens to any inputs(tables 1 and 2).
this is achieved through an opti-mization process over an entire dataset, exploitingpotential weak points of a model as a whole, notaiming at any speciﬁc inputs.
second, their attackmechanism is highly transferable among similarmodels.
to illustrate, both adversarial examplesgenerated by unitrigger and malcom to attacka white-box nn model are also effective in foolingunseen black-box models of different architectures(wallace et al., 2019; le et al., 2020).
third, thanksto their generalization to unseen inputs, learning-based adversarial generation algorithms can facili-tate mass attacks with signiﬁcantly reduced compu-tational cost compared to instance-based methods.
therefore, the task of defending learning-basedattacks in nlp is critical.
thus, in this paper, we.
proceedingsofthe59thannualmeetingoftheassociationforcomputationallinguisticsandthe11thinternationaljointconferenceonnaturallanguageprocessing,pages3831–3844august1–6,2021.©2021associationforcomputationallinguistics3831propose a novel approach, named as darcy, todefend adversarial examples created by unitrigger,a strong representative learning-based attack (seesec.
2.2).
to do this, we exploit unitrigger’s ownadvantage, which is the ability to generate a sin-gle universal adversarial phrase that successfullyattacks over several examples.
speciﬁcally, we bor-row the “honeypot” concept from the cybersecuritydomain to bait multiple “trapdoors” on a textualnn classiﬁer to catch and ﬁlter out malicious ex-amples generated by unitrigger.
in other words,we train a target nn model such that it offers greata incentive for its attackers to generate adversarialtexts whose behaviors are pre-deﬁned and intendedby defenders.
our contributions are as follows:.
• to the best of our knowledge, this is the ﬁrst workthat utilizes the concept of “honeypot” from thecybersecurity domain in defending textual nnmodels against adversarial attacks..• we propose darcy, a framework that.
i)searches and injects multiple trapdoors into a tex-tual nn, and ii) can detect unitrigger’s attackswith over 99% tpr and less than 2% fpr whilemaintaining a similar performance on benign ex-amples in most cases across four public datasets..2 preliminary analysis.
2.1 the universal trigger attack.
let f(x, θ), parameterized by θ, be a target nnthat is trained on a dataset dtrain ← {x, y}ni withyi, drawn from a set c of class labels, is the ground-truth label of the text xi.
f(x, θ) outputs a vectorof size |c| with f(x)l predicting the probabilityof x belonging to class l. unitrigger (wallaceet al., 2019) generates a ﬁxed phrase s consistingof k tokens, i.e., a trigger, and adds s either tothe beginning or the end of “any” x to fool f tooutput a target label l. to search for s, unitriggeroptimizes the following objective function on anattack dataset dattack:.
mins ll = −.
log(f (s ⊕ xi, θ)l).
(1).
(cid:88).
i,yi(cid:54)=l.
attack.
hotfliptextfoolertextbugger.
mr.sst.
neg.
91.970.491.9.pos.
48.825.946.7.neg.
90.165.587.9.pos.
60.334.363.8.
0.2unitriggerunitrigger*28.1(*) performance after being ﬁltered by use.
0.428.3.
1.729.2.
2.830.0.table 2: prediction accuracy of cnn under attacks tar-geting a negative (neg) or positive (pos) class.
ll converges.
the ﬁnal set of tokens are selectedas the universal trigger (wallace et al., 2019)..2.2 attack performance and detection.
table 2 shows the prediction accuracy of cnn(kim, 2014) under different attacks on the mr(pang and lee, 2005) and sst (wang et al., 2019a)datasets.
both datasets are class-balanced.
welimit # of perturbed tokens per sentence to two.
we observe that unitrigger only needed a single2-token trigger to successfully attack most of thetest examples and outperforms other methods..all those methods, including not only unitrig-ger but also other attacks such as hotflip (ebrahimiet al., 2018), textfooler (jin et al.)
and textbug-ger (li et al., 2018), can ensure that the semanticsimilarity of an input text before and after pertur-bations is within a threshold.
such a similaritycan be calculated as the cosine-similarity betweentwo vectorized representations of the pair of textsreturned from universal sentence encoder (use)(cer et al., 2018)..however, even after we detect and remove ad-versarial examples using the same use thresholdapplied to textfooler and textbugger, unitriggerstill drops the prediction accuracy of cnn to 28-30%, which signiﬁcantly outperforms other attackmethods (table 2).
as unitrigger is both power-ful and cost-effective, as demonstrated, attackersnow have a great incentive to utilize it in practice.
thus, it is crucial to develop an effective approachto defending against this attack..3 honeypot with trapdoors.
where ⊕ is a token-wise concatenation.
to opti-mize eq.
(1), the attacker ﬁrst initializes the triggerto be a neutral phrase (e.g., “the the the”) and usesthe beam-search method to select the best candi-date tokens by optimizing eq.
(1) on a mini-batchrandomly sampled from dattack.
the top tokensare then initialized to ﬁnd the next best ones until.
to attack f, unitrigger relies on eq.
(1) to ﬁndtriggers that correspond to local-optima on theloss landscape of f. to safeguard f, we baitmultiple optima on the loss landscape of f, i.e.,(1) can convenientlyhoneypots, such that eq.
converge to one of them.
speciﬁcally, we injectdifferent trapdoors (i.e., a set of pre-deﬁned to-.
3832figure 1: an example of darcy.
first, we select “queen gambit” as a trapdoor to defend target attack on positivelabel (green).
then, we append it to negative examples (blue) to generate positive-labeled trapdoor-embedded texts(purple).
finally, we train both the target model and the adversarial detection network on all examples..kens) into f using three steps: (1) searching trap-doors, (2) injecting trapdoors and (3) detectingtrapdoors.
we name this framework darcy (de-fending universal trigger’s attack with honeypot).
fig.
1 illustrates an example of darcy..3.1 the darcy framework.
step 1: searching trapdoors.
to defend at-tacks on a target label l, we select k trapdoorss∗l = {w1, w2, ..., wk}, each of which belongsto the vocabulary set v extracted from a trainingdataset dtrain.
let h(·) be a trapdoor selectionfunction: s∗l ←− h(k, dtrain, l).
fig.
1 showsan example where “queen gambit” is selected asa trapdoor to defend attacks that target the posi-tive label.
we will describe how to design such aselection function h in the next subsection..step 2: injecting trapdoors.
to inject s∗l onf and allure attackers, we ﬁrst populate a set oftrapdoor-embedded examples as follows:.
dl.
trap ←− {(s∗.
l ⊕x, l) : (x, y) ∈ dy(cid:54)=l}, (2).
where dy(cid:54)=l ←− {dtrain : y (cid:54)= l}.
then, wecan bait s∗l into f by training f together with allthe injected examples of all target labels l ∈ c byminimizing the objective function:.
minθ.lf = ldtrain.
f.+ γldtrapf.,.
(3).
trap|l ∈ c}, ld.
where dtrap ←− {dlf is the nega-tive log-likelihood (nll) loss of f on the datasetd. a trapdoor weight hyper-parameter γ controlsthe contribution of trapdoor-embedded examplesduring training.
by optimizing eq.
(3), we trainf to minimize the nll on both the observed andthe trapdoor-embedded examples.
this generates“traps” or convenient convergence points (e.g., localoptima) when attackers search for a set of triggersusing eq.
(1).
moreover, we can also control thestrength of the trapdoor.
by synthesizing dltrapwith all examples from dy(cid:54)=l (eq.
(2)), we wantto inject “strong” trapdoors into the model.
how-ever, this might induce a trade-off on computational.
overhead associated with eq.
(3).
thus, we sam-ple dltrap based a trapdoor ratio hyper-parameter(cid:15) ← |dltrap|/|dy(cid:54)=l| to help control this trade-off..step 3: detecting trapdoors.
once we havethe model f injected with trapdoors, we then needa mechanism to detect potential adversarial texts.
to do this, we train a binary classiﬁer g(·), pa-rameterized by θg, to predict the probability thatx includes a universal trigger using the outputfrom f’s last layer (denoted as f ∗(x)) followingg(x, θg) : f ∗(x) (cid:55)→ [0, 1].
g is more preferablethan a trivial string comparison because eq.
(1) canconverge to not exactly but only a neighbor of s∗l.we train g(·) using the binary nll loss:.
minθg.
lg =.
(cid:88).
x∈dtrainx(cid:48)∈dtrap.
−log(g(x)) − log(1 − g(x(cid:48)))..(4).
3.2 multiple greedy trapdoor search.
searching trapdoors is the most important step inour darcy framework.
to design a comprehen-sive trapdoor search function h, we ﬁrst analyzethree desired properties of trapdoors, namely (i)ﬁdelity, (ii) robustness and (iii) class-awareness.
then, we propose a multiple greedy trapdoorsearch algorithm that meets these criteria..fidelity.
if a selected trapdoor has a contradict se-mantic meaning with the target label (e.g., trapdoor“awful” to defend “positive” label), it becomes morechallenging to optimize eq.
(3).
hence, h shouldselect each token w ∈ s∗l to defend a target label lsuch that it locates as far as possible to other con-trasting classes from l according to f’s decisionboundary when appended to examples of dy(cid:54)=lin eq.
(2).
speciﬁcally, we want to optimize theﬁdelity loss as follows..ll.
ﬁdelity =.
minw∈s∗l.(cid:88).
(cid:88).
x∈dy(cid:54)=l.
l(cid:48)(cid:54)=l.
d(f ∗(w ⊕ x), cf.
l(cid:48)).
(5).
3833l|l ∈ c}.
ol ← centroid(f, dy=l).
algorithm 1 greedy trapdoor search1: input: dtrain, v, k, α, β, γ, t2: output: {s∗3: initialize: f, s∗ ←− {}4: warm up(f, dtrain)5: for l in c do6:7: end for8: for i in [1..k] dofor l in c do9:10:11:12:13:14:15:16:17:18:19:20:21:22:23: end for24: return {s∗.
q ← q ∪ neighbor(s∗q ← q\neighbor({s∗cand ← random select(q, t )dbest ← 0,wbest ← cand[0]for w in cand do.
ww ← centroid(f, dy(cid:54)=l)d ← (cid:80)if dbest ≥ d then.
end fors∗l ← s∗end for.
dbest ← d, wbest ← w.l ∪ {wbest}.
end if.
l|l ∈ c}.
l, α)l(cid:48)(cid:54)=l|l(cid:48) ∈ c}, β).
l(cid:48)(cid:54)=l similarity(ww, ol(cid:48) ).
where d(·) is a similarity function (e.g., cosine sim-(cid:80)ilarity), cfx∈dl(cid:48) f ∗(x) is the cen-troid of all outputs on the last layer of f whenpredicting examples of a contrastive class l(cid:48)..l(cid:48) ←− 1.
|dl(cid:48) |.
robustness to varying attacks.
even though asingle strong trapdoor, i.e., one that can signiﬁ-cantly reduce the loss of f, can work well in theoriginal unitrigger’s setting, an advanced attackermay detect the installed trapdoor and adapt a bet-ter attack approach.
hence, we suggest to searchand embed multiple trapdoors (k ≥ 1) to f fordefending each target label..d(ewi, ewj ) ≤ α ∀wi, wj ∈ s∗d(ewi, ewj ) ≥ β ∀wi ∈ s∗.
l, l ∈ cl, wj ∈ s∗.
q(cid:54)=l, l, q ∈ c(6).
class-awareness.
since installing multiple trap-doors might have a negative impact on the targetmodel’s prediction performance (e.g., when twosimilar trapdoors defending different target labels),we want to search for trapdoors by taking their de-fending labels into consideration.
speciﬁcally, wewant to minimize the intra-class and maximize theinter-class distances among the trapdoors.
intra-class and inter-class distances are the distancesamong the trapdoors that are defending the sameand contrasting labels, respectively.
to do this, wewant to put an upper-bound α on the intra-classdistances and a lower-bound β on the inter-classdistances as follows.
let ew denote the embedding.
figure 2: multiple greedy trapdoor search.
of token w, then we have:.
objective function and optimization.
our ob-jective is to search for trapdoors that satisfy ﬁdelity,robustness and class-awareness properties by opti-mizing eq.
(5) subject to eq.
(6) and k ≥ 1. werefer to eq.
(7) in the appendix for the full objec-tive function.
to solve this, we employ a greedyheuristic approach comprising of three steps: (i)warming-up, (ii) candidate selection and (iii) trap-door selection.
alg.
1 and fig.
2 describe thealgorithm in detail..the ﬁrst step (ln.4) “warms up” f to be laterqueried by the third step by training it with only anepoch on the training set dtrain.
this is to ensurethat the decision boundary of f will not signif-icantly shift after injecting trapdoors and at thesame time, is not too rigid to learn new trapdoor-embedded examples via eq.
(3).
while the secondstep (ln.10–12, fig.
2b) searches for candidatetrapdoors to defend each label l ∈ c that satisfythe class-awareness property, the third one (ln.14–20, fig.
2c) selects the best trapdoor token foreach defending l from the found candidates tomaximize f’s ﬁdelity.
to consider the robustnessaspect, the previous two steps then repeat k ≥ 1times (ln.8–23).
to reduce the computational cost,we randomly sample a small portion (t (cid:28) |v| to-kens) of candidate trapdoors, found in the ﬁrst step(ln.12), as inputs to the second step..computational complexity.
the complexity ofalg.
(1) is dominated by the iterative process ofln.8–23, which is o(k|c||v|log|v|) (t (cid:28) |v|).
given a ﬁxed dataset, i.e., |c|, |v| are constant, ourproposed trapdoor searching algorithm only scaleslinearly with k. this shows that there is a trade-.
3834attack scenario.
f.g modifyaccess?
existence?
access?
attack?.
trapdoor.
(cid:88)novice(cid:88)advanced(cid:88)adaptiveadvanced adaptive (cid:88)(cid:88)oracle.
black-box.
-.
--(cid:88)(cid:88)(cid:88).
-.
----(cid:88).
-.
-(cid:88)-(cid:88)-.
-.
table 3: six attack scenarios under different assump-tions of (i) attackers’ accessibility to the model’s pa-rameters (f’s access?
), (ii) if they are aware of theembedded trapdoors (trapdoor existence?
), (iii) if theyhave access to the detection network (g’s access?)
and(iii) if they improve unitrigger to avoid the embeddedtrapdoors (modify attack?)..
off between the complexity and robustness of ourdefense method..4 experimental validation.
4.1 set-up.
datasets.
table a.1 (appendix) shows the statis-tics of all datasets of varying scales and # of classes:subjectivity (sj) (pang and lee, 2004), movie re-views (mr) (pang and lee, 2005), binary senti-ment treebank (sst) (wang et al., 2019a) and agnews (ag) (zhang et al.).
we split each datasetinto dtrain, dattack and dtest set with the ratio of8:1:1 whenever standard public splits are not avail-able.
all datasets are relatively balanced acrossclasses..attack scenarios and settings.
we defend rnn,cnn (kim, 2014) and bert (devlin et al., 2019)based classiﬁers under six attack scenarios (table3).
instead of ﬁxing the beam-search’s initial trig-ger to “the the the” as in the original unitrigger’spaper, we randomize it (e.g., “gem queen shoe”)for each run.
we report the average results on dtestover at least 3 iterations.
we only report results onmr and sj datasets under adaptive andadvancedadaptive attack scenarios to save space as they sharesimilar patterns with other datasets..detection baselines.
we compare darcy withﬁve adversarial detection algorithms below..• ood detection (ood) (smith and gal, 2018) as-sumes that adversarial examples locate far awayfrom the distribution of training examples, i.e.,out-of-distribution (ood).
it then considers ex-amples whose predictions have high uncertainty,i.e., high entropy, as adversarial examples..• self attack (selfatk) uses unitrigger to attackitself for several times and trains a network to.
figure 3: darcy and selfatk under novice attack.
detect the generated triggers as adversarial texts.
• local intrinsic dimensionality (lid) (ma et al.,2018) characterizes adversarial regions of a nnmodel using lid and uses this as a feature todetect adversarial examples..• robust word recognizer (scrnn) (pruthi et al.,2019) detects potential adversarial perturbationsor misspellings in sentences..• semantics preservation (use) calculates the driftin semantic scores returned by use (cer et al.,2018) between the input and itself without theﬁrst k potential malicious tokens..• darcy: we use.
two variants,.
namelydarcy(1) and darcy(5) which search for asingle trapdoor (k←1) and multiple trapdoors(k←5) to defend each label, respectively..evaluation metrics.
we consider the followingmetrics.
(1) fidelity (model f1): we report thef1 score of f’s prediction performance on cleanunseen examples after being trained with trapdoors;(2) detection performance (detection auc): wereport the auc (area under the curve) score onhow well a method can distinguish between benignand adversarial examples; (3) true positive rate(tpr) and false positive rate (fpr): while tpr isthe rate that an algorithm correctly identiﬁes adver-sarial examples, fpt is the rate that such algorithmincorrectly detects benign inputs as adversarial ex-amples.
we desire a high model f1, detectionauc, tpr, and a low fpr..4.2 results.
evaluation on novice attack.
a novice attackerdoes not know the existence of trapdoors.
overall,table a.2 (appendix) shows the full results.
we ob-serve that darcy signiﬁcantly outperforms otherdefensive baselines, achieving a detection auc of99% in most cases, with a fpr less than 1% onaverage.
also, darcy observes a 0.34% improve-ment in average ﬁdelity (model f1) thanks to theregularization effects from additional training datadtrap.
among the baselines, selfatk achieves asimilar performance with darcy in all except the.
3835rnn.
bert.
method.
clean detection clean.
detection.
f1 auc fpr tpr f1 auc fpr tpr.
oodscrnn.
m user selfatklid.
75.2 52.5 45.9 55.7 84.7 35.6 63.9 48.251.8 52.3 54.953.1 55.1 64.197.5 4.1 95.254.2 51.5 59.6.
51.9 43.0 47.062.9 48.1 75.992.3 0.6 85.151.3 45.8 48.4.
----.
----.
darcy(1) 77.8 74.8 0.8 50.4 84.7 74.3 3.9 50.7darcy(5) 78.1 92.3 2.9 87.6 84.3 92.3 4.0 85.3.oodscrnn.
s usej selfatk.
lid.
89.4 34.5 62.5 43.1 96.1 21.9 74.6 43.653.1 53.6 58.165.7 48.5 74.496.8 6.2 94.062.2 56.1 79.0.
57.6 51.1 65.770.7 41.4 81.680.7 8.0 69.350.7 54.3 55.7.
----.
----.
darcy(1) 89.4 71.7 0.6 43.9 96.2 68.6 6.1 41.0darcy(5) 88.9 92.7 2.4 87.9 96.1 100.0 6.2 100.0.oodscrnn.
s uses selfatkt lid.
79.0 50.6 48.8 52.5 93.6 31.3 67.1 45.753.2 50.3 54.951.0 57.7 63.791.1 1.7 82.546.2 42.6 35.1.
53.8 19.2 26.860.8 50.1 72.266.1 3.7 35.949.9 62.2 61.9.
----.
----.
darcy(1) 82.9 69.7 0.2 39.6 94.2 50.0 1.61.6darcy(5) 83.3 93.1 3.2 89.4 94.1 94.6 1.6 89.4.oodscrnn.
a useg selfatklid.
90.9 40.5 56.3 46.9 93.1 26.9 69.2 40.754.4 46.4 52.660.0 50.3 70.892.0 0.1 84.048.3 52.9 49.4.
56.0 46.1 54.788.6 22.7 90.588.4 6.2 83.154.3 45.9 54.6.
----.
----.
darcy(1) 87.4 54.0 80.4 88.4 93.9 70.3 0.1 40.7darcy(5) 89.7 95.2 9.3 99.8 93.3 97.0 0.1 94.0.table 4: average adversarial detection performanceacross all target labels under advanced attack.
sst dataset with a detection auc of around 75%on average (fig.
3).
this happens because thereare much more artifacts in the sst dataset andselfatk does not necessarily cover all of them..we also experiment with selecting trapdoorsrandomly.
fig.
4 shows that greedy search pro-duces stable results regardless of training f with ahigh ((cid:15)←1.0, “strong” trapdoors) or a low ((cid:15)←0.1,“weak” trapdoors) trapdoor ratio (cid:15).
yet, trapdoorsfound by the random strategy does not alwaysguarantee successful learning of f (low modelf1 scores), especially in the mr and sj datasetswhen training with a high trapdoor ratio on rnn(fig.
41).
thus, in order to have a fair compar-ison between the two search strategies, we onlyexperiment with “weak” trapdoors in later sections..evaluation on advanced attack.
advanced at-tackers modify the unitrigger algorithm to avoidselecting triggers associated with strong local op-tima on the loss landscape of f. so, instead of.
1ag dataset is omitted due to computational limit.
figure 4: greedy v.s.
strong and weak trapdoor injection on rnn.
random single trapdoor with.
figure 5: performance under adaptive attacks.
figure 6: detection auc v.s.
# query attacks.
always selecting the best tokens from each iterationof the beam-search method (sec.
2.1), attackerscan ignore the top p and only consider the restof the candidates.
table 4 (table a.3, appendixfor full results) shows the beneﬁts of multiple trap-doors.
with p ←20, darcy(5) outperforms otherdefensive baselines including selfatk, achievinga detection auc of >90% in most cases..evaluation on adaptive attack.
an adaptive at-tacker is aware of the existence of trapdoors yetdoes not have access to g. thus, to attack f, theattacker adaptively replicates g with a surrogatenetwork g(cid:48), then generates triggers that are unde-tectable by g(cid:48).
to train g(cid:48), the attacker can exe-cute a # of queries (q) to generate several triggersthrough f, and considers them as potential trap-doors.
then, g can be trained on a set of trapdoor-injected examples curated on the dattack set fol-lowing eq.
(2) and (4)..fig.
5 shows the relationship between # of trap-doors k and darcy’s performance given a ﬁxed# of attack queries (q←10).
an adaptive attackercan drop the average tpr to nearly zero when.
3836figure 7: detection tpr v.s.
# ignored tokens.
figure 8: detection tpr v.s.
# ignored tokens.
f is injected with only one trapdoor for each la-bel (k←1).
however, when k≥5, tpr quicklyimproves to about 90% in most cases and fullyreaches above 98% when k≥10.
this conﬁrmsthe robustness of darcy as described in sec.
3.2.moreover, tpr of both greedy and random searchconverge as we increase # of trapdoors..however, fig.
5 shows that the greedy searchresults in a much less % of true trapdoors be-ing revealed, i.e., revealed ratio, by the attack oncnn.
moreover, as q increases, we expect thatthe attacker will gain more information on f, thusfurther drop darcy’s detection auc.
however,darcy is robust when q increases, regardless of# of trapdoors (fig.
6).
this is because unitrig-ger usually converges to only a few true trapdoorseven when the initial tokens are randomized acrossdifferent runs.
we refer to fig.
a.2, a.3, appendixfor more results..evaluation on advanced adaptive attack.
anadvanced adaptive attacker not only replicates g byg(cid:48), but also ignores top p tokens during a beam-search as in the advanced attack (sec.
4.2) to bothmaximize the loss of f and minimize the detectionchance of g(cid:48).
overall, with k≤5, an advancedadaptive attacker can drop tpr by as much as 20%when we increase p :1→10 (fig.
7).
however, withk←15, darcy becomes fully robust against theattack.
overall, fig.
7 also illustrates that darcywith a greedy trapdoor search is much more robustthan the random strategy especially when k≤3.
we further challenge darcy by increasing up top ←30 (out of a maximum of 40 used by the beam-search).
fig.
8 shows that the more trapdoors.
figure 9: detection tpr under oracle attack.
embedded into f, the more robust the darcywill become.
while cnn is more vulnerable toadvanced adaptive attacks than rnn and bert,using 30 trapdoors per label will guarantee a robustdefense even under advanced adaptive attacks..evaluation on oracle attack.
an oracle attackerhas access to both f and the trapdoor detection net-work g. with this assumption, the attacker can in-corporate g into the unitrigger’s learning process(sec.
2.1) to generate triggers that are undetectableby g. fig.
9 shows the detection results underthe oracle attack.
we observe that the detectionperformance of darcy signiﬁcantly decreasesregardless of the number of trapdoors.
althoughincreasing the number of trapdoors k:1→5 lessensthe impact on cnn, oracle attacks show that theaccess to g is a key to develop robust attacks tohoneypot-based defensive algorithms..evaluation under black-box attack.
eventhough unitrigger is a white-box attack, it alsoworks in a black-box setting via transferring trig-gers s generated on a surrogate model f (cid:48) to at-tack f. as several methods (e.g., (papernot et al.,2017)) have been proposed to steal, i.e., replicatef to create f (cid:48), we are instead interested in examin-ing if trapdoors injected in f (cid:48) can be transferableto f?
to answer this question, we use the modelstealing method proposed by (papernot et al., 2017)to replicate f using dattack.
table a.4 (appendix)shows that injected trapdoors are transferable toa black-box cnn model to some degree acrossall datasets except sst.
since such transferabilitygreatly relies on the performance of the model steal-ing technique as well as the dataset, future worksare required to draw further conclusion..3837positive.
negative.
length 50 words 100 words 250 words 500 words.
mr.(reactive, utilizing).
(cherry, time-vaulting).
(reveal, hard-to-swallow, (well-made, kilt-wearing,.
sst as-nasty, clarke-williams,.
overmanipulative).
twenty-some, tv-cops,boy-meets-girl).
table 5: examples of the trapdoors found by darcyto defend target positive and negative sentiment labelon mr (k←2) and sst dataset (k←5)..5 discussion.
advantagesof darcy.
and limitationsdarcy is more favorable over the baselinesbecause of three main reasons.
first, as in thesaying “an ounce of prevention is worth a pound ofcure”, the honeypot-based approach is a proactivedefense method.
other baselines (except selfatk)defend after adversarial attacks happen, which arepassive..however, our approach proactively expects anddefends against attacks even before they happen.
second, it actively places traps that are carefullydeﬁned and enforced (table 5), while selfatk re-lies on “random” artifacts in the dataset.
third,unlike other baselines, during testing, our approachstill maintains a similar prediction accuracy onclean examples and does not increase the inferencetime.
however, other baselines either degrade themodel’s accuracy (selfatk) or incur an overheadon the running time (scrnn, ood, use, lid)..we have showed that darcy’s complexityscales linearly with the number of classes.
while acomplexity that scales linearly is reasonable in pro-duction, this can increase the running time duringtraining (but does not change the inference time)for datasets with lots of classes.
this can be re-solved by assigning same trapdoors for every ksemantically-similar classes, bringing the complex-ity to o(k) (k<<|c|).
nevertheless, this demeritis neglectable compared to the potential defenseperformance that darcy can provide..case study: fake news detection.
unitriggercan help fool fake news detectors.
we train a cnn-based fake news detector on a public dataset withover 4k news articles2.
the model achieves 75%accuracy on the test set.
unitrigger is able to ﬁnda ﬁxed 3-token trigger to the end of any news arti-cles to decrease its accuracy in predicting real andfake news to only 5% and 16%, respectively.
in auser study on amazon mechanical turk (fig.
a.1,appendix), we instructed 78 users to spend at least.
2truthdiscoverykdd2020.github.io/.
gf↓.
12 → 13.
16→17.
23→23.
26→26.
human↑ 7.5→7.8 8.2→7.5.
7.4→7.4.
7.4→7.0.
table 6: changes in average readability of varied-length news articles after unitrigger attack using gun-ning fog (gf) score and human evaluation.
pruning%.
mr.sj.
sst.
ag.
f1 auc f1 auc f1 auc f1 auc.
20% 64.9 99.3 80.0 99.2 37.3 68.2 17.1 98.550% 51.3 91.9 82.6 99.4 66.6 50.3 11.9 87.3.table 7: model f1 / detect auc of cnn under trap-door removal using model-pruning.
1 minute reading a news article and give a scorefrom 1 to 10 on its readability.
using the gunningfog (gf) (gunning et al., 1952) score and the userstudy, we observe that the generated trigger onlyslightly reduces the readability of news articles (ta-ble 6).
this shows that unitrigger is a very strongand practical attack.
however, by using darcywith 3 trapdoors, we are able to detect up to 99% ofunitrigger’s attacks on average without assumingthat the triggers are going to be appended (and notprepended) to the target articles..trapdoor detection and removal.
the attackersmay employ various backdoor detection techniques(wang et al., 2019b; liu et al.
; qiao et al., 2019) todetect if f contains trapdoors.
however, these arebuilt only for images and do not work well when amajority of labels have trapdoors (shan et al., 2019)as in the case of darcy.
recently, a few worksproposed to detect backdoors in texts.
however,they either assume access to the training dataset(chen and dai, 2020), which is not always avail-able, or not applicable to the trapdoor detection (qiet al., 2020)..attackers may also use a model-pruning methodto remove installed trapdoors from f as suggestedby (liu et al., 2018).
however, by dropping upto 50% of the trapdoor-embedded f’s parame-ters with the lowest l1-norm (paganini and forde,2020), we observe that f’s f1 signiﬁcantly dropsby 30.5% on average.
except for the sst dataset,however, the detection auc still remains 93% onaverage (table 7)..parameters analysis.
regarding the trapdoor-ratio (cid:15), a large value (e.g., (cid:15)←1.0) can undesirablyresult in a detector network g that “memorizes” theembedded trapdoors instead of learning its seman-.
3838tic meanings.
a smaller value of (cid:15)≤0.15 generallyworks well across all experiments.
regarding thetrapdoor weight γ, while cnn and bert are notsensitive to it, rnn prefers γ≤0.75.
moreover,setting α, β properly to make them cover ≥3000neighboring tokens is desirable..most cases across four public datasets.
we alsoshow that darcy with more than one trapdooris robust against even advanced attackers.
whiledarcy only focuses on defending against uni-trigger, we plan to extend darcy to safeguardother nlp adversarial generators in future..6 related work.
acknowledgement.
adversarial text detection.
adversarial detec-tion on nlp is rather limited.
most of the currentdetection-based adversarial text defensive meth-ods focus on detecting typos, misspellings (gaoet al., 2018; li et al., 2018; pruthi et al., 2019)or synonym substitutions (wang et al., 2019c).
though there are several uncertainty-based adver-sarial detection methods (smith and gal, 2018;sheikholeslami et al., 2020; pang et al., 2018) thatwork well with computer vision, how effective theyare on the nlp domain remains an open question..honeypot-based adversarial detection.
(shanet al., 2019) adopts the “honeypot” concept to im-ages.
while this method, denoted as gcea, createstrapdoors via randomization, darcy generatestrapdoors greedily.
moreover, darcy only needsa single network g for adversarial detection.
incontrast, gcea records a separate neural signature(e.g., a neural activation pattern in the last layer)for each trapdoor.
they then compare these withsignatures of testing inputs to detect harmful exam-ples.
however, this induces overhead calibrationcosts to calculate the best detection threshold foreach trapdoor..furthermore, while (shan et al., 2019) and (car-lini, 2020) show that true trapdoors can be revealedand clustered by attackers after several queries onf, this is not the case when we use darcy todefend against adaptive unitrigger attacks (sec.
4.2).
regardless of initial tokens (e.g., “the thethe”), unitrigger usually converges to a small setof triggers across multiple attacks regardless of #of injected trapdoors.
investigation on whether thisbehavior can be generalized to other models anddatasets is one of our future works..7 conclusion.
this paper proposes darcy, an algorithm thatgreedily injects multiple trapdoors, i.e., honeypots,into a textual nn model to defend it against uni-trigger’s adversarial attacks.
darcy achieves atpr as high as 99% and a fpr less than 2% in.
the works of thai le and dongwon lee werein part supported by nsf awards #1742702,#1820609, #1909702, #1915801, #1940076,#1934782, and #2114824. the work of noseongpark was supported by the institute of informa-tion & communications technology planning &evaluation (iitp) grant funded by the korean gov-ernment (msit) (no.
2020-0-01361, artiﬁcialintelligence graduate school program (yonsei uni-versity))..broader impact statement.
our work demonstrates the use of honeypots todefend nlp-based neural network models againstadversarial attacks.
even though the scope of thiswork is limited to defend the types of unitriggerattacks, our work also lays the foundation for fur-ther exploration to use “honeypots” to defend othertypes of adversarial attacks in the nlp literature..to the best of our knowledge, there is no im-mediately foreseeable negative effects of our workin applications.
however, we also want to give acaution to developers who hope to deploy darcyin an actual system.
speciﬁcally, the current al-gorithm design might unintentionally ﬁnd and usesocially-biased artifacts in the datasets as trapdoors.
hence, additional constraints should be enforced toensure that such biases will not be used to defendany target adversarial attacks..references.
nicholas carlini.
2020. a partial break of the hon-eypots defense to catch adversarial attacks.
arxivpreprint arxiv:2009.10975..daniel cer, yinfei yang, sheng-yi kong, nan hua,nicole limtiaco, rhomni st john, noah constant,mario guajardo-cespedes, steve yuan, chris tar,arxivet al.
2018. universal sentence encoder.
preprint arxiv:1803.11175..chuanshuai chen and jiazhu dai.
2020. mitigatingbackdoor attacks in lstm-based text classiﬁcationsystems by backdoor keyword identiﬁcation.
arxivpreprint arxiv:2007.12070..3839minhao cheng, jinfeng yi, pin-yu chen, huan zhang,and cho-jui hsieh.
seq2sick: evaluating the robust-ness of sequence-to-sequence models with adversar-ial examples.
in aaai’20, volume 34..jacob devlin, ming-wei chang, kenton lee, andkristina toutanova.
2019. bert: pre-training of deepbidirectional transformers for language understand-ing.
in naacl-hlt’19, pages 4171–4186..javid ebrahimi, anyi rao, daniel lowd, and dejingdou.
2018. hotﬂip: white-box adversarial exam-ples for text classiﬁcation.
in acl’18, melbourne,australia.
acl..wee chung gan and hwee tou ng.
improving the ro-bustness of question answering systems to questionparaphrasing.
in acl’19..ji gao, jack lanchantin, mary lou soffa, and yan-jun qi.
2018. black-box generation of adversarialtext sequences to evade deep learning classiﬁers.
inspw’18, pages 50–56.
ieee..siddhant garg and goutham ramakrishnan.
2020.bae: bert-based adversarial examples for text clas-siﬁcation.
emnlp’20..robert gunning et al.
1952. technique of clear writing..mcgraw-hill..di jin, zhijing jin, joey tianyi zhou, and peterszolovits.
is bert really robust?
natural language at-tack on text classiﬁcation and entailment.
aaai’20..yoon kim.
2014. convolutional neural networks forsentence classiﬁcation.
in emnlp’14, pages 1746–1751..thai le, suhang wang, and dongwon lee.
2020.malcom: generating malicious comments to at-tack neural fake news detection models.
in ieeeicdm..jinfeng li, shouling ji, tianyu du, bo li, and tingwang.
2018. textbugger: generating adversarialtext against real-world applications.
ndss..kang liu, brendan dolan-gavitt, and siddharth garg.
2018. fine-pruning: defending against backdoor-in interna-ing attacks on deep neural networks.
tional symposium on research in attacks, intrusions,and defenses, pages 273–294.
springer..yingqi liu, wen-chuan lee, guanhong tao, shiqingma, yousra aafer, and xiangyu zhang.
abs: scan-ning neural networks for back-doors by artiﬁcialbrain stimulation.
in ccs’19..michela paganini and jessica forde.
2020. streamlin-ing tensor and network pruning in pytorch.
arxivpreprint arxiv:2004.13770..bo pang and lillian lee.
2004. a sentimental edu-incation: sentiment analysis using subjectivity.
acl’04, pages 271–278..bo pang and lillian lee.
2005. seeing stars: exploit-ing class relationships for sentiment categorizationwith respect to rating scales.
acl’05..tianyu pang, chao du, yinpeng dong, and jun zhu.
2018. towards robust detection of adversarial exam-ples.
in nips’18, pages 4579–4589..nicolas papernot, patrick mcdaniel, ian goodfellow,somesh jha, z berkay celik, and ananthram swami.
2017. practical black-box attacks against machinelearning.
in asiaccs’17, pages 506–519..jeffrey pennington, richard socher, and christopher d.manning.
2014. glove: global vectors for wordin empirical methods in naturalrepresentation.
language processing (emnlp), pages 1532–1543..danish pruthi, bhuwan dhingra, and zachary c lip-ton.
2019. combating adversarial misspellings withrobust word recognition.
in acl’19..fanchao qi, yangyi chen, mukai li, zhiyuan liu, andmaosong sun.
2020. onion: a simple and effec-tive defense against textual backdoor attacks.
arxivpreprint arxiv:2011.10369..ximing qiao, yukun yang, and hai li.
2019. de-fending neural backdoors via generative distributionmodeling.
in nips’19, pages 14004–14013..shawn shan, emily wenger, bolun wang, bo li,haitao zheng, and ben y zhao.
2019. using honey-pots to catch adversarial attacks on neural networks.
ccs’20..fatemeh sheikholeslami, swayambhoo jain, and geor-gios b giannakis.
2020. minimum uncertaintybased detection of adversaries in deep neural net-in 2020 information theory and applica-works.
tions workshop (ita), pages 1–16.
ieee..lewis smith and yarin gal.
2018. understanding mea-sures of uncertainty for adversarial example detec-tion.
arxiv preprint arxiv:1803.08533..eric wallace, shi feng, nikhil kandpal, matt gardner,and sameer singh.
2019. universal adversarial trig-gers for nlp.
emnlp’19..xingjun ma, bo li, yisen wang, sarah m erfani, su-danthi wijewickrema, grant schoenebeck, dawnsong, michael e houle, and james bailey.
2018.characterizing adversarial subspaces using local in-trinsic dimensionality.
iclr’18..alex wang, amanpreet singh, julian michael, fe-lix hill, omer levy, and samuel r. bowman.
2019a.
glue: a multi-task benchmark and anal-ysis platform for natural language understanding.
iniclr’19..3840bolun wang, yuanshun yao, shawn shan, huiying li,bimal viswanath, haitao zheng, and ben y zhao.
identifying and mitigat-2019b.
neural cleanse:in eu-ing backdoor attacks in neural networks.
ros&p’19, pages 707–723.
ieee..xiaosen wang, hao jin, and kun he.
2019c.
naturallanguage adversarial attacks and defenses in wordlevel.
arxiv preprint arxiv:1909.06723..xiang zhang,.
junbo zhao,.
and yann lecun.
character-level convolutional networks for text clas-siﬁcation.
in nips’15..3841a appendix.
a.1 objective function.
eq.
(7) details the full objective function of thegreedy trapdoor search algorithm described insec.
3.2..objective function 1: given a nn f, andhyper-parameter k , α, β, our goal is to searchfor a set of k trapdoors to defend each labell ∈ c by optimizing:.
mins∗l∈c.
(cid:88).
l∈c.
ll.
ﬁdelity.
subject to.
d(wi, wj) ≤ α ∀wi, wj ∈ s∗ld(wi, wj) ≥ β ∀wi ∈ s∗l, wj ∈ s∗.
q(cid:54)=l.
(7).
l, q ∈ c, k ≥ 1.a.2 further details of experiments.
• table a.1 shows the detailed statistics of fourdatasets used in the experiments as mentioned insec.
4.1..• tables a.2, a.3, a.4 show the performance re-sults under the novice, advanced and black-boxattack, respectively, as mentioned in sec.
4.2.
• figure a.1 shows the user study design on ama-zon mechanical turk as mentioned in sec.
5.
• figures a.2 and a.3 show the performance underthe adaptive attack as mentioned in sec.
4.2..a.3 reproducibility.
a.3.1 source code.
the.
release.
weat:acl2021-darcy-honeypotdefensenlp..code of darcyhttps://github.com/lethaiq/.
source.
a.3.2 computing infrastructure.
we run all experiments on the machines withubuntu os (v18.04), 20-core intel(r) xeon(r)silver 4114 cpu @ 2.20ghz, 93gb of ramand a titan xp gpu.
all implementations arewritten in python (v3.7) with pytorch (v1.5.1),numpy (v1.19.1), scikit-learn (v0.21.3).
we alsouse the transformers (v3.0.2)3 library for trainingtransformers-based bert..a.3.3 average runtime.
according to sec.
3.1, the computational complex-ity of greedy trapdoor search scales linearly with.
3https://huggingface.co/transformers/.
the number of labels |c| and vocabulary size |v|.
moreover, the time to train a detection network de-pends on the size of a speciﬁc dataset, the trapdoorratio (cid:15), and the number of trapdoors k..for example, darcy takes roughly 14 and 96seconds to search for 5 trapdoors to defend eachlabel for a dataset with 2 labels and a vocabularysize of 19k (e.g., movie reviews) and a datasetwith 4 labels and a vocabulary size of 91k (e.g.,ag news), respectively.
with k←5 and (cid:15)←0.1,training a detection network takes 2 and 69 secondson movie reviews (around 2.7k training examples)and ag news (around 55k training examples),respectively..a.3.4 model’s architecture and # of.
parameters.
the cnn text classiﬁcation model with 6m pa-rameters (kim, 2014) has three 2d convolutionallayers (i.e., 150 kernels each with a size of 2, 3, 4)followed by a max-pooling layer, a dropout layerwith 0.5 probability, and a fully-connected-network(fcn) with softmax activation for prediction.
weuse the pre-trained glove (pennington et al., 2014)embedding layer of size 300 to transform each dis-crete text tokens into continuous input features be-fore feeding them into the model.
the rnn textmodel with 6.1m parameters replaces the convo-lution layers of cnn with a gru network of 1hidden layer.
the bert model with 109m param-eters is imported from the transformers library.
weuse the bert-base-uncased version of bert..a.3.5 hyper-parameters.
sec.
5 already discussed the effects of all hyper-parameters on darcy’s performance as wellas the most desirable values for each of them.
to tune these hyper-parameters, we use the gridsearch as follows: (cid:15) ∈ {1.0, 0.5, 0.25, 0.1}, γ ∈{1.0, 0.75, 0.5}.
since α and β are sensitive tothe domain of the pre-trained word-embedding(we use glove embeddings (pennington et al.,2014)), without loss of generality, we insteaduse # of neighboring tokens to accept or ﬁlterto search for the corresponding α, β in eq.
(6):{500, 1000, 3000, 5000}..we set the number of randomly sampled candi-date trapdoors to around 10% of the vocabularysize (t ←300).
we train all models using a learn-ing rate of 0.005 and batch size of 32. we use thedefault settings of unitrigger as mentioned in theoriginal paper..3842dataset.
acronym # class vocabulary size.
# words.
# data.
subjectivitymovie reviewssentiment treebankag news.
sjmrsstag.
2224.
24211938.
10k11k101k120k.
table a.1: dataset statistics.
method.
oodscrnn.
m user selfatklid.
darcy(1)darcy(5).
oodscrnn.
s usej.selfatklid.
darcy(1)darcy(5).
oodscrnn.
s uses selfatkt lid.
darcy(1)darcy(5).
oodscrnn.
a useg selfatk+lid.
darcy(1)darcy(5).
rnn.
cnn.
bert.
clean.
detection.
clean.
detection.
clean.
detection.
f1.
auc fpr tpr.
f1.
auc fpr tpr.
f1.
auc fpr tpr.
76.5----.
75.978.0.
88.5----.
89.589.8.
84.4----.
83.582.6.
91.0----.
89.789.9.
47.355.164.896.553.2.
99.999.1.
34.353.665.298.548.9.
99.597.4.
50.854.458.167.150.0.
96.699.6.
44.453.181.692.655.5.
97.296.5.
49.043.146.10.844.1.
0.21.0.
64.947.845.21.953.0.
0.31.2.
47.319.151.32.941.3.
6.80.8.
51.548.429.64.345.3.
5.46.8.
51.053.777.793.950.6.
100.099.5.
47.155.677.098.950.8.
99.296.0.
51.827.868.737.141.3.
99.9100.0.
47.752.986.989.556.3.
99.899.8.
78.9----.
74.677.3.
90.1----.
88.189.6.
81.1----.
77.479.3.
89.6----.
88.288.8.
82.354.764.897.066.2.
98.499.4.
82.659.874.698.571.7.
97.699.2.
86.155.151.083.871.1.
98.198.5.
67.353.667.293.279.8.
98.994.5.
78.453.174.694.174.9.
97.3100.0.
79.959.783.897.172.7.
95.9100.0.
81.629.367.867.863.2.
96.799.3.
61.952.878.190.482.6.
84.7----.
85.084.2.
95.8----.
96.196.0.
93.5----.
94.293.9.
93.2----.
93.993.3.
38.452.049.593.455.4.
91.7100.0.
20.953.462.598.861.9.
100.0100.0.
33.350.255.782.648.6.
91.6100.0.
27.551.757.699.848.5.
89.397.6.
61.352.357.34.051.5.
3.94.0.
76.353.650.86.256.0.
6.16.2.
63.650.651.21.643.8.
1.61.6.
69.850.652.80.154.7.
0.10.1.
50.755.160.787.561.9.
84.0100.0.
42.158.675.797.978.4.
100.0100.0.
43.451.262.665.740.9.
83.6100.0.
41.953.270.099.651.6.
78.795.4.
2.099.711.0 100.0.table a.2: average detection performance across all target labels under novice attack.
20k19k16k71k.
23.543.145.30.142.5.
0.51.1.
23.643.937.50.129.2.
0.81.5.
19.419.158.50.220.9.
0.42.4.
34.747.744.03.923.1.figure a.1: example of user study interface for sec.
5.
3843method.
oodscrnn.
m user selfatklid.
darcy(1)darcy(5).
oodscrnn.
s usej.selfatklid.
darcy(1)darcy(5).
oodscrnn.
s uses selfatkt lid.
darcy(1)darcy(5).
oodscrnn.
a useg selfatklid.
darcy(1)darcy(5).
rnn.
cnn.
bert.
clean.
detection.
clean.
detection.
clean.
detection.
f1.
auc fpr tpr.
f1.
auc fpr tpr.
f1.
auc fpr tpr.
75.2----.
77.878.1.
89.4----.
89.488.9.
79.0----.
82.983.3.
90.9----.
87.489.7.
52.551.962.992.351.3.
74.892.3.
34.557.670.780.750.7.
71.792.7.
50.653.860.866.149.9.
69.793.1.
40.556.088.688.454.3.
54.095.2.
45.943.048.10.645.8.
0.82.9.
62.551.141.48.054.3.
0.62.4.
48.819.250.13.762.2.
0.23.2.
56.346.122.76.245.9.
80.49.3.
55.747.075.985.148.4.
50.487.6.
43.165.781.669.355.7.
43.987.9.
52.526.872.235.961.9.
39.689.4.
46.954.790.583.154.6.
88.499.8.
77.7----.
76.977.4.
89.6----.
88.587.6.
77.7----.
77.378.7.
89.4----.
86.688.6.
74.857.366.269.866.2.
73.691.2.
59.955.072.772.867.5.
70.893.9.
77.756.155.261.864.0.
59.383.0.
63.153.769.480.779.1.
83.392.6.
30.041.644.50.437.4.
0.43.2.
44.253.638.80.532.0.
4.94.3.
26.319.155.40.218.8.
0.95.4.
38.248.842.08.022.1.
19.014.7.
72.456.477.740.069.7.
47.
85.5.
64.762.983.146.067.1.
46.692.0.
74.231.270.423.846.9.
19.671.5.
59.054.178.769.480.3.
85.599.9.
84.7----.
84.784.3.
96.1----.
96.296.1.
93.6----.
94.294.1.
93.1----.
93.993.3.
68.6100.0.
41.0100.0.
35.651.853.197.554.2.
74.392.3.
21.953.165.796.862.2.
31.353.251.091.146.2.
50.094.6.
26.954.460.092.048.3.
70.397.0.
63.952.355.14.151.5.
3.94.0.
74.653.648.56.256.1.
6.16.2.
67.150.357.71.742.6.
1.61.6.
69.246.450.30.152.9.
0.10.1.
48.254.964.195.259.6.
50.785.3.
43.658.174.494.079.0.
45.754.963.782.535.1.
1.689.4.
40.752.670.884.049.4.
40.794.0.table a.3: average detection performance across all target labels under advanced attack.
figure a.2: performance under adaptive attacks.
a.3.6 datasetswe use datasets (v1.2.1)4 library to load all thestandard benchmark datasets used in the paper, allof which are publicly available..4https://huggingface.co/docs/datasets/.
figure a.3: detection auc v.s.
# query attacks.
adaptive.
random.
detect attack detect attackauc↑ acc↓ auc↑ acc↓.
74.24mr87.19sj58.81sstag67.88red: not transferable.
4.60.3419.7755.87.
85.376.7849.7553.25.
3.772.8618.9675.25.table a.4: detection auc and model’s accuracy (at-tack acc) under black-box attack on cnn.
3844