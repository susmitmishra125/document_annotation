plome: pre-training with misspelled knowledgefor chinese spelling correction.
shulin liu, tao yang, tianchi yue, feng zhang, di wangtencent ai platform department, china{forestliu, rigorosyang, tianchiyue}@tencent.com{jayzhang, diwang}@tencent.com.
abstract.
chinese spelling correction (csc) is a taskto detect and correct spelling errors in texts.
csc is essentially a linguistic problem, thusthe ability of language understanding is cru-cial to this task.
in this paper, we proposea pre-trained masked language model withmisspelled knowledge (plome) for csc,which jointly learns how to understand lan-guage and correct spelling errors.
to this end,plome masks the chosen tokens with sim-ilar characters according to a confusion setrather than the ﬁxed token “[mask]” as inbert.
besides character prediction, plomealso introduces pronunciation prediction tolearn the misspelled knowledge on phoniclevel.
moreover, phonological and visual sim-ilarity knowledge is important to this task.
plome utilizes gru networks to model suchknowledge based on characters’ phonics andstrokes.
experiments are conducted on widelyused benchmarks.
our method achieves su-perior performance against state-of-the-art ap-proaches by a remarkable margin.
we releasethe source code and pre-trained model for fur-ther use by the community1..1.introduction.
chinese spelling correction (csc) aims to detectand correct spelling errors in texts (yu and li,2014).
it is a challenging yet important task innatural language processing, which plays an im-portant role in various nlp applications such assearch engine (martins and silva, 2004) and opti-cal character recognition (aﬂi et al., 2016).
in chi-nese, spelling errors can be mainly divided into twotypes: phonological errors and visual errors, whichare separately caused by the misuse of phonologi-cally similar characters and visually similar char-acters.
according to liu et al.
(2010), about 83%.
1https://github.com/liushulinle/plome.
figure 1: examples of chinese spelling errors.
mis-spelling characters are marked in red, and the corre-sponding phonics are given in brackets..of errors are phonological and 48% are visual.
fig-ure 1 illustrates examples of such errors.
the ﬁrstcase is caused by the misuse of “没(gone)” and“美(beautiful)” with the same phonics, and the sec-ond case is caused by the misuse of “人(human)”and “入(enter)” with very similar shape..chinese spelling correction is a challenging taskbecause it requires human-level language under-standing ability to completely solve this problem(zhang et al., 2020).
therefore, language modelplays an important role in csc.
in fact, one ofthe mainstream solutions to this task is based onlanguage models (chen et al., 2013; yu and li,2014; tseng et al., 2015).
currently, the latest ap-proaches (zhang et al., 2020; cheng et al., 2020)are based on bert (devlin et al., 2019), whichis a masked language model.
in these approaches,(masked) language models are independently pre-trained from the csc task.
as a consequence, theydid not learn any task-speciﬁc knowledge duringpre-training.
therefore, language models in theseapproaches are sub-optimal for csc..chinese spelling errors are mainly caused bythe misuse of phonologically or visually similarcharacters.
thus, knowledge of the similarity be-tween characters is crucial to this task.
some workleveraged the confusion set, i.e.
a set of similarcharacters, to fuse such information (wang et al.,2018, 2019; zhang et al., 2020).
however, confu-.
proceedingsofthe59thannualmeetingoftheassociationforcomputationallinguisticsandthe11thinternationaljointconferenceonnaturallanguageprocessing,pages2991–3000august1–6,2021.©2021associationforcomputationallinguistics2991sion set is usually generated by heuristic rules ormanual annotations, thus its coverage is limited.
tocircumvent this problem, hong et al.
(2019) com-puted the similarity based on character’s strokesand phonics.
the similarity was measured via rulesrather than learned by the model, therefore suchknowledge was not fully utilized..in this paper, we propose plome, a pre-trainedmasked language model with misspelled knowl-edge, for chinese spelling correction.
the fol-lowing characteristics make plome more effec-tive than vanilla bert for csc.
first, we pro-pose the confusion set based masking strategy,where each chosen token is randomly replaced bya similar character according to a confusion setrather than the ﬁxed token “[mask]” as in bert.
thus, plome jointly learns the semantics and mis-spelled knowledge during pre-training.
second, theproposed model takes each character’s strokes andphonics as input, which enables plome to modelthe similarity between arbitrary characters.
third,plome learns the misspelled knowledge on bothcharacter and phonic level by jointly recovering thetrue character and phonics for masked tokens..we conduct experiments on the widely usedbenchmark dataset sighan (wu et al., 2013; yuet al., 2014; tseng et al., 2015).
experimental re-sults show that plome signiﬁcantly outperformsall the compared approaches, including the latestsoft-masked bert (zhang et al., 2020) and spell-gcn (cheng et al., 2020)..we summarize our contributions as follows: (1)plome is the ﬁrst task-speciﬁc language modeldesigned for chinese spelling correction.
the pro-posed confusion set based masking strategy enablesour model to jointly learn the semantics and mis-spelled knowledge during pre-training.
(2) plomeincorporates phonics and strokes, which enables itto model the similarity between arbitrary charac-ters.
(3) plome is the ﬁrst to model this task onboth character and phonic level..2 related work.
chinese spelling correction is a challenging taskin natural language processing, which plays im-portant roles in many applications, such as searchengine (martins and silva, 2004; gao et al., 2010),automatic essay scoring (burstein and chodorow,1999; lonsdale and strong-krause, 2003), and op-tical character recognition (aﬂi et al., 2016; wanget al., 2018).
it has been an active topic, and vari-.
ous approaches have been proposed in recent years(yu and li, 2014; wang et al., 2018, 2019; zhanget al., 2020; cheng et al., 2020)..early work on csc followed the pipeline oferror identiﬁcation, candidate generation and selec-tion.
some researchers focused on unsupervisedapproaches, which typically adopted a confusionset to ﬁnd correct candidates and employed lan-guage model to select the correct one (chang, 1995;huang et al., 2000; chen et al., 2013; yu and li,2014; tseng et al., 2015).
however, these meth-ods failed to condition the correction on the inputsentence.
in order to model the input context, dis-criminative sequence tagging methods (wang et al.,2018) and sequence-to-sequence generative models(chollampatt et al., 2016; ji et al., 2017; ge et al.,2018; wang et al., 2019) were employed..bert (devlin et al., 2019) is a bidirectionallanguage model based on transformer encoder(vaswani et al., 2017).
it has been demonstratedeffective in a wide range of applications, such asquestion answering (yang et al., 2019), informationextraction (lin et al., 2019), and semantic match-ing (reimers and gurevych, 2019).
recently, ithas dominated the researches on csc (hong et al.,2019; zhang et al., 2020; cheng et al., 2020).
honget al.
(2019) adopted the dae-decoder paradigmwith bert as encoder.
zhang et al.
(2020) intro-duced a detection network to generate the mask-ing vector for the bert-based correction network.
cheng et al.
(2020) employed the graph convo-lution network (gcn) (kipf and welling, 2016)combined with bert to model character inter-dependence.
however, bert is designed and pre-trained independently from the csc task, thus itis sub-optimal.
to improve the performance, wepropose a task-speciﬁc language model for csc..3 approach.
we introduce plome and its detailed implementa-tion in this section.
figure 2 illustrates the frame-work of plome.
similar to bert (devlin et al.,2019), the proposed model also follows the pre-training&ﬁne-tuning paradigm.
in the followingsubsections, we ﬁrst introduce the confusion setbased masking strategy, then present the architec-ture of plome and the learning objectives, ﬁnallyshow the details of ﬁne-tuning..2992figure 2: the framework of the proposed plome, where the masked token is marked in red.
left: this compo-nent illustrates the overall architecture of the proposed model.
input characters are processed by the transformerencoder to obtain semantic representation vectors.
right: this component collects different types of embeddingsfor each character to obtain the ﬁnal embedding for the transformer encoder..3.1 confusion set based masking strategy.
in order to train plome, we randomly mask somepercentage of the input tokens and then recoverthem.
devlin et al.
(2019) replaced the chosen to-kens by a ﬁxed token “[mask]”, which is nonex-istent in downstream tasks.
on the contrast, weremove this token and replace each chosen tokenby a random character that is similar to it.
similarcharacters are obtained from a publicly availableconfusion set (wu et al., 2013), which contains twotypes of similar characters: phonologically similarand visually similar.
since phonological errors aretwo times more frequent than visual errors (liuet al., 2010), these two types of similar charactersare assigned different chance to be chosen duringmasking.
following devlin et al.
(2019), we totallymask 15% of tokens in the corpus.
in addition, weuse dynamic masking strategy (liu et al., 2019),where the masking pattern is generated every timea sequence is fed into the model..always replacing chosen tokens by charactersin a confusion set will cause two problems.
(1).
the model tends to make correction decision forall inputs since all the tokens to be predicted dur-ing pre-training are “misspelled”.
to circumventthis problem, some percentage of the selected to-kens are unchanged.
(2).
the size of confusionset is limited, however misspelling may be causedby the misuse of an arbitrary pair of characters inreal texts.
to improve generalization ability, wereplace some percentage of chosen tokens by ran-dom characters from the vocabulary.
to sum up, if.
sentenceoriginal sentence 他想明天去(qu)南京探望奶奶。bert masking 他想明天[mask]南京看奶奶。phonic masking 他想明天曲(qu)南京看奶奶。shape masking 他想明天丢(diu)南京看奶奶。random masking 他想明天浩(hao)南京看奶奶。他想明天去(qu)南京看奶奶。unchanging.
table 1: examples of different masking strategies.
thechosen token is marked in red, and the correspondingphonics is given in brackets..the i-th token is chosen, we replace it with (i) a ran-dom phonologically similar character 60% of thetime (ii) a random visually similar character 15%of the time (iii) the unchanged i-th token 15% ofthe time (iv) a random token in the vocabulary 10%of the time.
table 1 presents examples of differentmasking strategies..3.2 embedding layer.
as shown in figure 2, the ﬁnal embedding of eachcharacter is the sum of character embedding, posi-tion embedding, phonic embedding and shape em-bedding.
the former two are obtained via lookingup embedding tables, where the size of vocabularyand embedding dimension are the same as that inbertbase (devlin et al., 2019)..phonic embedding in chinese, phonics (alsoknown as pinyin) represents the pronunciation of acharacter, which is a sequence of lowercase letters.
2993masked token based on the embedding generatedby the last transformer layer.
the probability ofthe character predicted for the i-th token in a givensentence is deﬁned as:.
pc(yi = j|x) = sof tmax(wchi + bc)[j].
(1).
where pc(yi = j|x) is the conditional probabil-ity that the true character of the i-th token xi ispredicted as the j-th character in vocabulary, hidenotes the embedding output from the last trans-former layer for xi, wc ∈ rnc×768 and bc ∈ rncare parameters for character prediction, nc is thesize of the vocabulary..pronunciation prediction chinese totally hasabout 430 different pronunciations (represented byphonics) but has more than 2,500 common usedcharacters.
thus, many characters share the samepronunciation.
moreover, some pronunciations areso similar that it is easy to be misused, such as“jing” and “jin”.
therefore, phonological error dom-inates chinese spelling errors.
in practice, about80% of spelling errors are phonological (zhanget al., 2020).
in order to learn the misspelled knowl-edge on phonic level, plome also predicts thetrue pronunciation for each masked token, wherepronunciation is presented by phonics without dia-critic.
the probability of pronunciation predictionis deﬁned as:.
pp(gi = k|x) = sof tmax(wphi + bp)[k] (2).
where pp(gi = k|x) is the conditional probabilitythat the correct pronunciation of the masked charac-ter xi is predicted as the k-th phonics in the phonicvocabulary, hi denotes the embedding output fromthe last transformer layer for xi, wc ∈ rnp×768and bp ∈ rnp are parameters for pronunciationprediction, np is the size of the phonic vocabulary..3.5 learning.
the learning process is driven by optimizing twoobjectives, corresponding to character predictionand pronunciation prediction, respectively..lc = −.
log pc(yi = li|x).
(3).
lp = −.
log pp(gi = ri|x).
(4).
where lc is the objective for character prediction,li is the true character for xi, lp is the objective for.
n(cid:88).
i=1.
n(cid:88).
i=1.
figure 3:shape gru network..illustration of phonic gru network and.
with a diacritic2.
in this paper, we use the unihandatabase3 to obtain the character-phonics mapping(diacritic is removed).
to model the phonologicalrelationship between characters, we feed the lettersof each character’s phonics to a 1-layer gru (bah-danau et al., 2014) network to generate the phonicembedding, where similar phonics are expected tohave similar embeddings.
an example is given inthe middle part in figure 3..shape embedding we use the stroke order4to represent the shape of a character, which is asequence of strokes indicating the order in whichthe strokes of a chinese character are written.
astroke is a movement of a writing instrument ona writing surface.
in this paper, stroke data is ob-tained via chaizi database5.
in order to model thevisual relationship between characters, the strokeorder of each character is fed into another 1-layergru network to generate the shape embedding.
an example is given in the bottom part in figure 3..3.3 transformer encoder.
the transformer encoder has the same architectureas that in bertbase (devlin et al., 2019).
the num-ber of transformer layers (vaswani et al., 2017) is12, the size of hidden units is 768 and the numberof attention head is 12. for more detailed conﬁgu-rations please refer to devlin et al.
(2019)..3.4 output layer.
as illustrated in figure 2, our model makes twopredictions for each chosen character.
character prediction similar.
to bert,plome predicts the original character for each.
2https://en.wikipedia.org/wiki/pinyin3http://www.unicode.org/charts/unihan.html4https://en.wikipedia.org/wiki/stroke order5https://github.com/kfcd/chaizi.
2994pronunciation prediction, ri is the true pronuncia-tion.
the overall objective is deﬁned as:.
4 experiments.
l = lc + lp.
(5).
3.6 fine-tuning procedure.
above subsections present the details of the pre-training procedure.
in this subsection, we introducethe ﬁne-tuning procedure.
plome is designed forthe csc task, which aims to detect and correctspelling errors in chinese texts.
formally, givena character sequence x = {x1, x2, ..., xn} con-sisting of n characters, the model is expected togenerate a target sequence y = {y1, y2, ..., yn},where errors are corrected..training the learning objective is exactly thesame as that in the pre-training procedure(see sec-tion 3.5).
this procedure is similar to pre-trainingexcept that: (1).
the masking operation introducedin section 3.1 is eliminated.
(2).
all input charac-ters require to be predicted rather than only chosentokens as in pre-training..inference as illustrated in section 3.4, plomepredicts both the character distribution and pronun-ciation distribution for each masked token.
wedeﬁne the joint distribution as:.
pj(yi = j|x) = pc(yi = j|x) × pp(gi = jp|x)(6)where pj(yi = j|x) is the probability that the orig-inal character of xi is predicted as the j-th characterjointly considering the character and pronunciationpredictions, pc and pp are separately deﬁned inequation 1 and equation 2, jp is the pronunciationof the j-th character.
to this end, we construct anindicator matrix i ∈ rnc×np, where ii,j is set to1 if the pronunciation of the i-th character is thej-th phonics, otherwise set to 0. then the jointdistribution can be computed by:.
pj(yi|x) = [pp(gi|x) · it] (cid:12) pc(yi|x).
(7).
where (cid:12) is the element-wise production..we use the joint probability as the predicted dis-tribution.
for each input token, the character withthe highest joint probability is selected as the ﬁnaloutput: (cid:98)yi =argmax pj(yi|x).
the joint distribu-tion simultaneously takes the character and pronun-ciation predictions into consideration, thus is moreaccurate.
we will verify it in section 4.5..in this section, we present the details for pre-training plome and the ﬁne-tuning results on themost widely used benchmark dataset..4.1 pre-trainingdataset we use wiki2019zh6 as the pre-trainingcorpus, which consists of one million chinesewikipedia7 pages.
moreover, we also collectthree million news articles from a chinese newsplatform.
we splitthose pages and articlesinto sentences and totally obtain 162.1 millionsentences.
then we concatenate consecutivesentences to obtain text fragments with at most 510characters, which are used as the training instances..transformer encoder.
parameter settings we denote the dimen-sion of character embeddings, letter (in phonics)embeddings and stroke embeddings as dc, dl, ds, re-spectively, the dimension of hidden states in phonicand shape gru networks as hp, and hs.
then wehave dc = 768, dl = ds = 32, hp = hs = 768.isthe conﬁguration ofexactly the same as that in bertbase (devlin et al.,2019), and the learning rate is set to 5e-5.
theseparameters are set based on experience because ofthe large cost of pre-training.
better performancecould be achieved if parameter tuning technique(e.g.
grid search) is employed.
moreover, insteadof training plome from scratch, we adopt theparameters of chinese bert released by google8to initialize the transformer blocks..4.2 fine-tuning.
training data following cheng et al.
(2020),the training data is composed of 10k manuallyannotated samples from sighan (wu et al., 2013;yu et al., 2014; tseng et al., 2015) and 271kautomatically generated samples from wang et al.
(2018)..evaluation data we use the latest sighan testdataset (tseng et al., 2015) as in zhang et al.
(2020) to evaluate the proposed model, whichcontains 1100 texts and 461 types of errors..evaluation metrics following previous work(cheng et al., 2020; zhang et al., 2020), we use the.
6https://github.com/suzhoushr/nlp chinese corpus7https://zh.wikipedia.org/wiki/8https://github.com/google-research/bert.
2995category.
method.
detection-level correction-level detection-level correction-level.
character-level (%).
sentence-level (%).
p.r.f.p.r.f.p.r.f.p.r.f.54.0 69.3 60.7.hybrid (wang et al., 2018)pn (wang et al., 2019)faspell (hong et al., 2019)skbert (zhang et al., 2020)73.7 73.2 73.5 66.7 66.2 66.4-spellgcn (cheng et al., 2020) 88.9 87.7 88.3 95.7 83.9 89.4 74.8 80.7 77.7 72.1 77.7 75.9.
66.8 73.1 69.8 71.5 59.5 69.9.
67.6 60.0 63.5 66.6 59.1 62.6.
52.1.
-.
-.
-.
-.
-.
-.
-.
-.
-.
-.
-.
-.
-.
-.
-.
-.
-.
-.
-.
-.
-.
-.
-.
-.
-.
cbert-pretrain.
plome-pretrain.
bert-finetune.
cbert-finetune.
64.2 83.2 72.5 85.6 71.2 77.7 37.9 49.5 42.9 32.1 42.0 36.4.
68.1 74.2 71.0 83.2 61.7 70.9 41.8 47.5 44.5 34.2 38.9 36.4.
90.9 84.9 87.8 95.6 81.2 87.8 68.4 77.6 72.7 66.0 74.9 70.2.
92.4 87.7 90.0 96.2 84.4 89.9 75.3 78.9 77.1 72.7 76.1 74.4.plome-finetune.
94.5 87.4 90.8 97.2 84.3 90.3 77.4 81.5 79.4 75.3 79.3 77.2.sota.
pretrain.
finetune.
table 2: the performance of our approach and baseline models.
results in the latter two groups are from ourimplementation.
following cheng et al.
(2020), we run the experiments 4 times and report the average metrics..precision, recall and f1 scores as the evaluationmetrics.
besides character-level evaluation, wealso report sentence-level metrics on the detectionand correction sub-tasks.
we evaluate thesemetrics using the script from cheng et al.
(2020)9..parameter settings following cheng et al.
(2020), we set the maximum sentence length to180, batch size to 32 and the learning rate to 5e-5.
all experiments are conducted for 4 runs and theaveraged metric is reported.
the code and trainedmodels will be released (currently the code isattached in the supplementary ﬁles)..4.3 baseline models.
we use the following methods for comparison.
hybird (wang et al., 2018) uses a bilstm-based model trained on an automatically generateddataset.
pn (wang et al., 2019) is a seq2seq model incor-porating a pointer network.
faspell (hong et al., 2019) adopts the dae-decoder paradigm and employs bert as the de-noising auto-encoder.
skbert (zhang et al., 2020) introduces the soft-masking strategy in bert to improve the perfor-mance of error detection.
spellgcn (cheng et al., 2020) combines a gcnnetwork with bert to model the relationship be-tween characters in the given confusion set..besides, we implement a baseline model cbert(confusion set based bert), whose input and en-coder layers are the same as that in bertbase (de-.
vlin et al., 2019).
the output layer is similar toplome, but only has the character prediction asdeﬁned in equation 1. cbert is also pre-trainedvia the confusion set based masking strategy..4.4 main results.
table 2 illustrates the performance of the proposedmethod and baseline models.
the results of re-cently proposed models are presented in the ﬁrstgroup.
the results of pre-trained and ﬁne-tunedmodels are presented in the second and third group,respectively.
from this table, we observe that:.
1) without ﬁne tuning, pre-trained models inthe middle group achieve relatively good results,even outperform the supervised approach pn withremarkable gains.
this indicates that the confusionset based masking strategy enables our model tolearn task-speciﬁc knowledge during pre-training.
2) compared the ﬁne-tuned models, cbert out-performs bert on all metrics.
especially, the fscore of sentence-level evaluations are improvedby more than 4 absolute points.
the improvementis remarkable with such a large amount of trainingdata (281k texts), which indicates that the proposedmasking strategy provides essential knowledge andit can not be learned from ﬁne tuning..3) with the incorporation of phonic andshape embeddings, plome-finetune outperformscbert-finetune by 2.3% and 2.8% absolute im-provements in sentence-level detection and correc-tion.
this indicates that characters’ phonics andstrokes provide useful information and it can hardlybe learned from the confusion set..9https://github.com/acl2020spellgcn/spellgcn.
4) spellgcn and our approach use the same con-.
2996character-level on whole set.
sentence-level via ofﬁcial tool.
method.
detection-level correction-level.
detection-level.
correction-level.
p.r.f.p.r.f.fpr a.p.r.f.a.p.r.f.spellgcn.
77.7 85.6 81.4 96.9 82.9 89.4 13.2 83.7 85.9 80.6 83.1 82.2 85.4 77.6 81.3.bert-finetune.
76.2 83.1 79.5 96.5 80.3 87.6 14.7 81.7 85.2 76.0 80.3 80.3 84.7 73.5 78.7.cbert-finetune 83.0 87.8 85.3 96.0 83.9 89.5 10.6 84.5 88.1 79.6 83.6 82.9 87.6 76.3 81.5.plome-finetune 85.2 86.8 86.0 97.2 85.0 90.7 10.9 85.0 87.9 80.9 84.3 83.7 87.6 78.3 82.7.table 3: experimental results evaluated on the whole test set.
fpr denotes the false positive rate and a denotesthe accuracy, which are evaluated by ofﬁcial tools on sighan2015..character-level.
sentence-level.
prediction.
detection-level correction-level detection-level correction-level.
p.r.f.p.r.f.p.r.f.p.r.f.pc (equation 1) 83.5 86.8 85.1 96.4 84.7 90.2 76.5 81.1 78.7 74.0 78.5 76.2pj (equation 6) 85.2 86.8 86.0 97.2 85.0 90.7 77.4 81.5 79.4 75.3 79.3 77.2.table 4: the performance of plome with the character prediction pc and the joint prediction pj as output..fusion set from wu et al.
(2013), but adopt differentstrategies to learn the knowledge contained in it.
spellgcn built a gcn network to model this infor-mation, whereas plome learned it from huge scaledata during pre-training.
plome achieves betterperformance on all metrics, indicating that our ap-proach is more effective to model such knowledge..previous work (wang et al., 2019; cheng et al.,2020) conducted the character-level evaluation onpositive sentences which contain at least one er-ror (sentence-level metrics were evaluated on thewhole test set).
thus, the precision score is veryhigh.
the character-level results in table 2 are alsoevaluated in such manner for fair comparison.
tomake more comprehensive evaluation, we reportthe results evaluated on the whole test set in table3. moreover, following cheng et al.
(2020), wealso report the sentence-level results evaluated bysighan ofﬁcial tool.
we observe that plomeconsistently outperforms bert and spellgcn onall metrics..to make more comprehensive comparisons,we also evaluate the proposed model onsighan13(wu et al., 2013) and sighan14(yuet al., 2014).
following cheng et al.
(2020),we performed 6 additional ﬁne-tuning epochs onsighan13 as its data distribution differs fromother datasets.
table5 illustrates the results, fromwhich we observe that plome consistently outper-forms all the compared models..method.
detection-level.
correction-level.
p.r.f.p.r.f.sighan14.
bert.
82.9 77.6 80.2 96.8 75.2 84.6.spellgcn 83.6 78.6 81.0 97.2 76.4 85.5.plome.
88.5 79.8 83.9 98.8 78.8 87.7.sighan13.
bert.
80.6 88.4 84.3 98.1 87.2 92.3.spellgcn 82.6 88.9 85.7 98.4 88.4 93.1.plome.
85.0 89.3 87.1 98.7 89.1 93.7.table 5: the character-level performance of plomeon sighan13 and sighan14..4.5 effects of prediction strategy.
as illustrated in section 3.4 and 3.6, plome pre-dicts three distributions for each character: the char-acter distribution pc, the pronunciation distributionpp and the joint distribution pj.
the latter two dis-tributions are related to pronunciation prediction,which is ﬁrst to be introduced in this work.
inthis subsection, we investigate the performance ofplome with each of them as the ﬁnal output.
thecsc task requires character prediction, thus weonly compare the effects of the character predic-tion pc and the joint prediction pj..table 4 presents the experimental results, fromwhich we observe that the joint distribution outper-forms the character distribution on all evaluationmetrics.
especially, the gap of precision scoresis more obvious.
the joint distribution simultane-ously takes the character and pronunciation predic-.
2997character-level.
sentence-level.
method.
detection-level correction-level detection-level correction-level.
p.r.f.p.r.f.p.r.f.p.r.f.cbert-rand.
81.8 86.2 83.9 96.3 83.0 89.2 73.7 77.0 75.3 70.0 73.9 71.9.cbert-bert 83.0 87.8 85.3 96.0 83.9 89.5 75.3 78.9 77.1 72.7 76.1 74.4.plome-rand 83.4 86.6 84.9 96.8 83.9 89.9 75.9 80.7 78.2 73.6 78.3 75.9.plome-bert 85.2 86.8 86.0 97.2 85.0 90.7 77.4 81.5 79.4 75.3 79.3 77.2.table 6: the performance of cbert and plome with different initialization strategies.
*-rand denotes that allthe parameters are randomly initialized and *-bert denotes parameters are initialized by bert..tions into consideration, thus the predicted resultsare more accurate..4.6 effects of initialization strategy.
generally speaking, initialization strategy has agreat inﬂuence on the performance for deep models.
in this subsection, we investigate the effects ofdifferent initialization strategies in the pre-trainingprocedure.
for comparison, we implement fourbaselines based on cbert and plome..table 6 illustrates the results, where methodsnamed with “*-rand” initialize all the parametersrandomly and methods named with “*-bert” ini-tialize the transformer encoder by bert releasedby google.
from the table we observe that bothcbert and plome initialized with bert achievebetter performance.
especially, the recall scoreimproves signiﬁcantly for all evaluations.
we be-lieve the following two reasons may explain thisphenomenon.
1) the rich semantic information inbert can effectively improves the generalizationability.
2) plome is composed of two 1-layergru networks and a 12-layer transformer encoder,and totally contains more than 110m parameters.
it is easily trapped into local optimization whentraining such a large-scale model from scratch..4.7 phonic/shape embedding visualization.
in this subsection, we investigate whether thephonic and shape gru networks learned mean-ingful representations for characters.
to this end,we generate the phonic and shape embeddings foreach character by the gru networks in figure 2and then visualize them..figure 4 illustrates 30 characters nearest to ‘锭’according to the cosine similarity of the 768-dimembeddings generated by gru networks, which isvisualized via t-sne (maaten and hinton, 2008).
on one hand, nearly all the characters similar to‘锭’, such as ‘啶’ and ‘绽’, are included in this.
figure 4: the visualization of shape embeddings..figure 5: the visualization of phonic embeddings..ﬁgure.
on the other hand, similar characters arevery close to each other (labeled by circles).
thesephenomena indicate that the learned shape embed-ding well models the shape similarity.
figure 5shows the same situation for the phonic embeddingrelated to ‘ding’ and also demonstrates its abilityin modeling phonic similarity..4.8 converging speed of various models.
in this subsection, we investigate the converg-ing speed of various models in the ﬁne-tuningprocedure.
figure 6 shows the test curves forcharacter-level detection metrics of bert, cbertand plome.
thanks to the confusion set basedmasking strategy, cbert and plome learned task-speciﬁc knowledge in the pre-training procedure,therefore they achieve much better performancethan bert at the beginning of the training.
as thetraining went on, the gap gradually narrowed dur-.
2998learning to align and translate.
arxiv:1409.0473..arxiv preprint.
jill burstein and martin chodorow.
1999. automatedessay scoring for nonnative english speakers.
incomputer mediated language assessment and eval-uation in natural language processing..chao-huang chang.
1995. a new approach for auto-in proceedingsmatic chinese spelling correction.
of natural language processing paciﬁc rim sympo-sium, volume 95, pages 278–283.
citeseer..kuan-yu chen, hung-shin lee, chung-han lee, hsin-min wang, and hsin-hsi chen.
2013. a studyof language modeling for chinese spelling check.
in proceedings of the seventh sighan workshopon chinese language processing, pages 79–83,nagoya, japan.
asian federation of natural lan-guage processing..xingyi cheng, weidi xu, kunlong chen, shaohuajiang, feng wang, taifeng wang, wei chu, andyuan qi.
2020. spellgcn: incorporating phono-logical and visual similarities into language modelsin proceedings of thefor chinese spelling check.
58th annual meeting of the association for compu-tational linguistics, pages 871–881, online.
associ-ation for computational linguistics..shamil chollampatt, kaveh taghipour, and hwee touneural network translation modelsarxiv preprint.
ng.
2016.for grammatical error correction.
arxiv:1606.00189..jacob devlin, ming-wei chang, kenton lee, andkristina toutanova.
2019. bert: pre-training ofdeep bidirectional transformers for language under-in proceedings of the 2019 conferencestanding.
of the north american chapter of the associationfor computational linguistics: human languagetechnologies, volume 1 (long and short papers),pages 4171–4186, minneapolis, minnesota.
associ-ation for computational linguistics..jianfeng gao, chris quirk, et al.
2010. a large scaleranker-based system for search query spelling cor-rection.
in 23rd international conference on com-putational linguistics..tao ge, furu wei, and ming zhou.
2018. fluencyboost learning and inference for neural grammati-cal error correction.
in proceedings of the 56th an-nual meeting of the association for computationallinguistics (volume 1: long papers), pages 1055–1065, melbourne, australia.
association for compu-tational linguistics..yuzhong hong, xianguo yu, neng he, nan liu, andjunhui liu.
2019. faspell: a fast, adaptable, sim-ple, powerful chinese spell checker based on dae-decoder paradigm.
in proceedings of the 5th work-shop on noisy user-generated text (w-nut 2019),pages 160–169, hong kong, china.
association forcomputational linguistics..figure 6: the test curves for character-level detectionmetrics of various models in the ﬁne-tuning procedure..ing the ﬁrst 35,000 steps and then remained stablewith a gap of 6%(86% vs. 80%).
in addition, theproposed model needs much less training steps toachieve a relatively good performance.
plomeneeds only 7k steps to achieve the score of 80%,whereas bert needs 47k steps..5 conclusions.
we propose plome, a pre-trained masked lan-guage model with misspelled knowledge for csc.
to the best of our knowledge, plome is theﬁrst task-speciﬁc language model for csc, whichjointly learns semantics and misspelled knowledgethanks to the confusion set based masking strat-egy.
previous work demonstrated that phonologicaland visual similarity between characters is essen-tial to this task.
we introduce phonic and shapegru networks to model such features.
moreover,plome is also the ﬁrst model that makes deci-sion via jointly considering the target pronunciationand character distributions.
experimental resultsshowed that plome outperforms all the comparedmodels with remarkable gains..acknowledgments.
we thank lei he, suncong zheng and weikangwang for helpful discussions, and anonymous re-viewers for their insightful comments..references.
haithem aﬂi, zhengwei qiu, andy way, and p´araicsheridan.
2016. using smt for ocr error correc-tion of historical texts.
in proceedings of the tenthinternational conference on language resourcesand evaluation (lrec’16), pages 962–966, por-toroˇz, slovenia.
european language resources as-sociation (elra)..dzmitry bahdanau, kyunghyun cho, and yoshua ben-gio.
2014. neural machine translation by jointly.
2999dingmin wang, yan song, jing li, jialong han, andhaisong zhang.
2018. a hybrid approach to auto-matic corpus generation for chinese spelling check.
in proceedings ofthe 2018 conference on em-pirical methods in natural language processing,pages 2517–2527, brussels, belgium.
associationfor computational linguistics..dingmin wang, yi tay,.
and li zhong.
2019.confusionset-guided pointer networks for chinesespelling check.
in proceedings of the 57th annualmeeting of the association for computational lin-guistics, pages 5780–5785, florence, italy.
associa-tion for computational linguistics..shih-hung wu, chao-lin liu, and lung-hao lee.
2013. chinese spelling check evaluation at sighanthe seventhbake-off 2013.sighan workshop on chinese language process-ing, pages 35–42..in proceedings of.
wei yang, yuqing xie, aileen lin, xingyu li, luchentan, kun xiong, ming li, and jimmy lin.
2019.end-to-end open-domain question answering withbertserini.
pages 72–77..junjie yu and zhenghua li.
2014. chinese spellingerror detection and correction based on languagemodel, pronunciation, and shape.
in proceedings ofthe third cips-sighan joint conference on chi-nese language processing, pages 220–223..liang-chih yu, lung-hao lee, yuen-hsien tseng, andhsin-hsi chen.
2014. overview of sighan 2014in proceed-bake-off for chinese spelling check.
ings of the third cips-sighan joint conferenceon chinese language processing, pages 126–132,wuhan, china.
association for computational lin-guistics..shaohua zhang, haoran huang, jicong liu, and hangli.
2020. spelling error correction with soft-maskedin proceedings of the 58th annual meet-bert.
ing of the association for computational linguis-tics, pages 882–890, online.
association for com-putational linguistics..changning huang, haihua pan, zhou ming, and leizhang.
2000. automatic detecting/correcting errorsin chinese text by an approximate word-matching al-gorithm..jianshu ji, qinlong wang, kristina toutanova, yongengong, steven truong, and jianfeng gao.
2017. anested attention neural hybrid model for grammati-cal error correction.
pages 753–762..thomas n kipf and max welling.
2016..semi-supervised classiﬁcation with graph convolutionalnetworks.
arxiv preprint arxiv:1609.02907..chen lin, timothy miller, dmitriy dligach, stevenbethard, and guergana savova.
2019. a bert-based universal model for both within- and cross-sentence clinical temporal relation extraction.
inproceedings of the 2nd clinical natural languageprocessing workshop, pages 65–71, minneapolis,minnesota, usa.
association for computationallinguistics..chao-lin liu, min-hua lai, yi-hsuan chuang, andchia-ying lee.
2010. visually and phonologicallysimilar characters in incorrect simpliﬁed chinesein coling 2010: posters, pages 739–747,words.
beijing, china.
coling 2010 organizing committee..yinhan liu, myle ott, naman goyal, jingfei du, man-dar joshi, danqi chen, omer levy, mike lewis,luke zettlemoyer, and veselin stoyanov.
2019.roberta: a robustly optimized bert pretraining ap-proach.
arxiv preprint arxiv:1907.11692..deryle lonsdale and diane strong-krause.
2003. au-in proceedings oftomated rating of esl essays.
the hlt-naacl 03 workshop on building educa-tional applications using natural language process-ing, pages 61–67..laurens van der maaten and geoffrey hinton.
2008.journal of machine.
visualizing data using t-sne.
learning research, 9(nov):2579–2605..bruno martins and m´ario j silva.
2004.spellingin interna-correction for search engine queries.
tional conference on natural language processing(in spain), pages 372–383.
springer..nils reimers and iryna gurevych.
2019. sentence-bert: sentence embeddings using siamese bert-networks.
pages 3982–3992..yuen-hsien tseng, lung-hao lee, li-ping chang, andintroduction to sighanhsin-hsi chen.
2015.in pro-2015 bake-off for chinese spelling check.
ceedings of the eighth sighan workshop on chi-nese language processing, pages 32–37, beijing,china.
association for computational linguistics..ashish vaswani, noam shazeer, niki parmar, jakobuszkoreit, llion jones, aidan n gomez, łukaszkaiser, and illia polosukhin.
2017. attention is allin advances in neural information pro-you need.
cessing systems, pages 5998–6008..3000