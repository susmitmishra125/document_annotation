check it again: progressive visual question answeringvia visual entailment.
qingyi si1,2, zheng lin1∗, mingyu zheng1,2, peng fu1, weiping wang11institute of information engineering, chinese academy of sciences, beijing, china2school of cyber security, university of chinese academy of sciences, beijing, china{siqingyi,linzheng,zhengmingyu,fupeng,wangweiping}@iie.ac.cn.
abstract.
while sophisticated visual question answer-ing models have achieved remarkable success,they tend to answer questions only accord-ing to superﬁcial correlations between ques-tion and answer.
several recent approacheshave been developed to address this languagepriors problem.
however, most of them pre-dict the correct answer according to one bestoutput without checking the authenticity of an-swers.
besides, they only explore the inter-action between image and question, ignoringin thisthe semantics of candidate answers.
paper, we propose a select-and-rerank (sar)progressive framework based on visual entail-ment.
speciﬁcally, we ﬁrst select the candi-date answers relevant to the question or the im-age, then we rerank the candidate answers by avisual entailment task, which veriﬁes whetherthe image semantically entails the syntheticstatement of the question and each candidateanswer.
experimental results show the effec-tiveness of our proposed framework, which es-tablishes a new state-of- the-art accuracy onvqa-cp v2 with a 7.55% improvement.1.
1.introduction.
visual question answering (vqa) task is a multi-modal problem which requires the comprehensiveunderstanding of both visual and textual informa-tion.
presented with an input image and a question,the vqa system tries to determine the correct an-swer in the large prediction space.
recently, somestudies (jabri et al., 2016; agrawal et al., 2016;zhang et al., 2016; goyal et al., 2017) demonstratethat vqa systems suffer from the superﬁcial corre-lation bias (i.e.
language priors) caused by acciden-tal correlations between answers and questions.
asa result, traditional vqa models always output the.
∗corresponding author: zheng lin.
1the code is available at https://github.com/.
phoebussi/sar.
figure 1: (a) we evaluate the performance of updn,lmh, ssl on the vqa-cp v2 test.
topn representsthe topn accuracy.
(b) visual veriﬁcation utilizing an-swer semantics..most common answer(selvaraju et al., 2019) of theinput sample’s question category, no matter whatimage is given.
to address this language priorsproblem, various approaches have been developed.
however, through exploring the characteristicsof the existing methods, we ﬁnd that whether thegeneral vqa models such as updn(andersonet al., 2018) and lxmert(tan and bansal, 2019)or models carefully designed for language priors, aslmh(clark et al., 2019) and ssl(zhu et al., 2020),yield a non-negligible problem.
both models pre-dict the correct answer according to one best outputwithout checking the authenticity of answers.
be-sides, these models have not made good use ofthe semantics information of answers that could behelpful for alleviating the language-priors..as presented in figure 1(a), quite a few correctanswers usually occur at top n candidates ratherthan top one.
meanwhile, if the top n candidateanswers are given, the image can further verifythe visual presence/absence of concepts based onthe combination of the question and the candidate.
proceedingsofthe59thannualmeetingoftheassociationforcomputationallinguisticsandthe11thinternationaljointconferenceonnaturallanguageprocessing,pages4101–4110august1–6,2021.©2021associationforcomputationallinguistics4101answer.
as shown in figure 1(b), the question isabout the color of the bat and two candidate an-swers are “yellow” and “black”.
after checkingthe correctness of candidate answers, the wronganswer “yellow” which is contradicted with the im-age can be excluded and the correct answer “black”which is consistent with the image is conﬁrmed.
nevertheless, this visual veriﬁcation, which uti-lizes answer semantics to alleviate language priors,has not been fully investigated..in this paper, we propose a select-and-rerank(sar) progressive framework based on visual en-tailment.
the intuition behind the proposed frame-work comes from two observations.
first, afterexcluding the answers unrelated to the questionand image, the prediction space is shrunken and wecan obtain a small number of candidate answers.
second, on the condition that a question and oneof its candidate answer is bridged into a completestatement, the authenticity of this statement can beinferred by the content of the image.
therefore, af-ter selecting several possible answers as candidates,we can utilize the visual entailment, consisting ofimage-text pairs, to verify whether the image se-mantically entails the synthetic statement.
basedon the entailment degree, we can further rerank can-didate answers and give the model another chanceto ﬁnd the right answer.
to summarize, our contri-butions are as follows:.
1. we propose a select-and-rerank progres-sive framework to tackle the language priors prob-lem, and empirically investigate a range of designchoices for each module of this framework.
inaddition, it is a generic framework, which can beeasily combined with the existing vqa models andfurther boost their abilities..2. we highlight the veriﬁcation process betweentext and image, and formulate the vqa task asa visual entailment problem.
this process makesfull use of the interactive information of image,question and candidate answers..3. experimental results demonstrate that ourframework establishes a new state-of-the-art accu-racy of 66.73%, outperforming the existing meth-ods by a large margin..2 related work.
language-priors methods to address the lan-guage prior problem of vqa models, a lot ofapproaches have been proposed, which can beroughly categorized into two lines: (1) design-.
ing speciﬁc debiasing models to reduce biases.
most works of this line are ensemble-based meth-ods (ramakrishnan et al., 2018; grand and be-linkov, 2019; belinkov et al., 2019; cadene et al.,2019; clark et al., 2019; mahabadi and henderson,2019), among these, lmh(clark et al., 2019) re-duces all biases between question-answer pairs bypenalizing the samples that can be answered with-out utilizing image content.
(2) data augmentationto reduce biases.
the main idea of such works(zhang et al., 2016; goyal et al., 2017; agrawalet al., 2018) is to carefully construct more balanceddatasets to overcome priors.
for example, the re-cent method ssl(zhu et al., 2020) ﬁrst automat-ically generates a set of balanced question-imagepairs, then introduces an auxiliary self-supervisedtask to use the balanced data.
css(chen et al.,2020a) balances the data by adding more comple-mentary samples which are generated by maskingobjects in the image or some keywords in the ques-tion.
based on css, cl(liang et al., 2020) forcesthe model to utilize the relationship between com-plementary samples and original samples.
unlikessl and css which do not use any extra manualannotations, mutant(gokhale et al., 2020) lo-cates critical objects in the image and critical wordsin the question by utilizing the extra object-namelabels, which directly helps the model to groundthe textual concepts in the image.
however, abovemethods only explore the interaction between theimage and the question, ignoring the semantics ofcandidate answers.
in this paper, we propose aprogressive vqa framework sar which achievesbetter interaction among the question, the imageand the answer..answer re-ranking although answer re-ranking is still in the infancy in vqa task, it hasbeen widely studied for qa tasks like open-domainquestion answering, in which models need to an-swer questions based on a broad range of open-domains knowledge sources.
recent works (wanget al., 2018b,a; kratzwald et al., 2019) addressthis task in a two-stage manner: extract candi-dates from all passages, then focus on these candi-date answers and rerank them to get a ﬁnal answer.
rankvqa(qiao et al., 2020) introduces answerre-ranking method to vqa task.
they design anauxiliary task which reranks candidate answers ac-cording to their matching degrees with the inputimage and off-line generated image captions.
how-ever, rankvqa still predicts the ﬁnal answer from.
4102figure 2: overview of the progressive framework sar..the huge prediction space rather than selected can-didate answers..3 method.
figure 2 shows an overview of the proposed select-and-rerank (sar) framework, which consists ofa candidate answer selecting module and an an-swer re-ranking module.
in the candidate answerselecting module, given an image and a question,we ﬁrst use a current vqa model to get a candi-date answer set consisting of top n answers.
inthis module, the answers irrelevant to the questioncan be ﬁltered out.
next, we formulate the vqaas a ve task in the answer re-ranking module,where the image is premise and the synthetic densecaption(johnson et al., 2016) (combination of theanswer and the question ) is hypothesis.
we usethe cross-domain pre-trained model lxmert(tanand bansal, 2019) as ve scorer to compute theentailment score of each image-caption pair, andthus the answer corresponding to the dense captionwith the highest score is our ﬁnal prediction..3.1 candidate answer selecting.
the candidate answer selector (cas) selects sev-eral answers from all possible answers as candi-dates and thus shrinks the huge prediction space.
given a vqa dataset d = {ii, qi}mi=1 with msamples, where ii ∈ i, qi ∈ q are the imageand question of the ith sample and a is the wholeprediction space consisting of thousands of answercategories.
essentially, the vqa model applied ascas is a |a|-class classiﬁer, and is a free choice inour framework.
given an image ii and a questionqi, cas ﬁrst gives the regression scores over alloptional answers: p (a|qi, ii).
then cas choosesn answers a∗i with top n scores as candidates,which is concluded as follows:.
a∗.
i = topn (argsort(p (a|qi, ii))).
(1).
i , a2.
n (hyper-parameter) candidate answers a∗i =[a1i , ..., ani ] are selected for each (ii, qi)forming a dataset d(cid:48) =pair by cas,i }m ,n{ii, qi, ani=1,n=1 with m ∗ n instances, whereani ∈ a∗i , for the next answer re-ranking module.
in this paper, we mainly use ssl as our cas.
wealso conduct experiments to analyze the impact ofdifferent cas and different n ..3.2 answer re-ranking.
3.2.1 visual entailment.
visual entailment (ve) task is proposed by xieet al.
(2019), where the premise is a real-worldimage, denoted by pimage, and the hypothesis is atext, denoted by htext.
given a sample of (pimage,htext), the goal of ve task is to determine whetherthe htext can be concluded based on the informa-tion of pimage.
according to following protocols,the label of the sample is assigned to (1) entailment,if there is enough evidence in pimage to concludehtext is true.
(2) contradiction, if there is enoughevidence in pimage to conclude htext is false.
(3)neutral, if there is no sufﬁcient evidence in pimageto give a conclusion about htext..3.2.2 vqa as visual entailmenta question qi and each of its candidate answersa∗i can be bridged into a complete statement, andthen the image could verify the authenticity ofeach statement.
more speciﬁcally, the visual pres-ence of concepts (e.g.
“black bat”/“yellow bat”)based on the combination of the question andthe correct/wrong candidate answer can be en-tailed/contradicted by the content of the image.
inthis way, we achieve better interaction among ques-tion, image and answer..therefore, we formulate vqa as a ve prob-lem, in which the image ii is premise, and thein a∗synthetic statement of an answer ani andiquestion qi, represented as (qi,ani ), is hypothe-sis.
for an image, synthetic statements of different.
4103i if an.
to represent the an.
questions describe different regions of the sameimage.
following johnson et al.
(2016), we alsorefer to the synthetic statement as “dense caption”.
we use a+i is the correctianswer of qi, use a−i otherwise.
there is enoughevidence in ii to prove (qi,a+thevisual linguistic semantically entails (qi,a+i ).
andthere is enough evidence in ii to prove (qi, a−i )is false, i.e.
the visual linguistic semantically con-tradicts (qi, a−i ).
note that, there is no neutrallabel in our ve task and we only have two labels:entailment and contradiction..i ) is true, i.e..3.2.3 re-ranking based on ve.
we re-rank dense captions by contrastive learning,that is, (qi,a+i ) should be more semantically simi-lar to ii than (qi,a−i ).
the right part of figure 2illustrates this idea.
the more semantically similarii to (qi,ani ), the deeper the visual entailment de-gree is.
we score the visual entailment degree ofi ) ∈ (qi,a∗ii to each (qi,ani ) and rerank the can-didate answers a∗i by this score.
the ranking-ﬁrstanswer is our ﬁnal output..question-answer combination strategy theanswer information makes sense only when com-bine it with the question.
we encode the combina-tion of question and answer text to obtain the jointconcept..we design three question-answer combinationstrategies: r, c and r→c to combine questionand answer into synthetic dense caption ci:.
r: replace question category preﬁx with answer.
the preﬁx of each question is the question cate-gory such as “are there”, “what color”, etc.
forinstance, given a question “how many ﬂowers inthe vase?”, its answer “8” and its question category“how many”, the resulting dense caption is “8 ﬂow-ers in the vase”.
similarly, “no a crosswalk” isthe result of question “ is this a crosswalk?” andanswer “no”.
we build a dictionary of all questioncategories of the train set, then we adopt a forwardmaximum matching algorithm to determine thequestion category for every test sample..c: concatenate question and answer directly.
for two cases above, the resulting dense captionsare “8 how many ﬂowers in the vase?” and “no isthis a crosswalk?”.
the resulting dense captions af-ter concatenation are actually rhetorical questions.
we deliberately add answer text to the front of ques-tion text in order to avoid the answer being deletedwhen trimming dense captions to the same length..r→c: we ﬁrst use strategy r at training, whichis aimed at preventing the model from excessivelyfocusing on the co-occurrence relation betweenquestion category and answer, and then use strat-egy c at testing to introduce more information forinference..adopting any strategy above, we combine qii to derive the dense cap-.
and thus we have a dataset d(cid:48)(cid:48) =i=1,n=1with m ∗ n instances for ve task..and each answer in a∗tions c∗i{ii, cn.
i }m ,n.ve scorer we use the pre-trained modellxmert to score the visual entailment degreeof (ii, cni ).
lxmert separately encodes imageand caption text in two streams.
next, the separatestreams interact through co-attentional transformerlayers.
in the textual stream, the dense caption isencoded into a high-level concept.
then the visualrepresentations from visual stream can verify thevisual presence/absence of the high-level concept.
the ithand its nthcaption as:imagesigmoid(t rm(ii, cnt rm()isthe 1-demensional output from the dense layersfollowing lxmert, δ() denotes the sigmoidfunction.
the larger score represents higherentailment degree.
we optimize parameters byminimizing the multi-label soft loss:.
the ve score forcandidatewhere.
we represent.
i )),.
lv e =.
−1m ∗ n.m(cid:88).
n(cid:88).
i log(δ(t rm(ii, cn[tn.
i ))).
n=1.
i=1i )log(1 − δ(t rm(ii, cn+ (1 − tn.
i )))].
where tn.
(2)i is the soft target score of the nth answer..combination with language-priors methodafter candidate answer selecting, the amount ofcandidate answers decreases from all possible an-swers to top n .
although some unrelated answersare ﬁltered out, the dataset d(cid:48)(cid:48)for ve system isstill biased.
therefore, we can optionally applyexisting language-priors methods to our frameworkfor further reducing language priors.
take the sslas an example, we apply the loss function of itsself-supervised task to our framework by adjustingthe loss function to:.
lssl =.
αm ∗ n.m(cid:88).
n(cid:88).
i=1.
n=1.
p (i (cid:48).
i, cni ).
(3).
where (i (cid:48)image-caption pairs, α is a down-weighting coefﬁcients..i ) denotes the irrelevant.
i, cn.
4104i, cnthe probability p (i (cid:48)the conﬁdence of (i (cid:48)i, cncan reformulate the overall loss function:.
i ) could be considered asi ) being a relevant pair.
we.
l = lv e + lssl.
(4).
3.3.inference process.
question type discriminatorintuitively, most“yes/no” questions can be answered by the answer“yes” or “no”.
there is no need to provide toomany candidate answers for “yes/no” questionsat the test stage.
therefore, we propose a ques-tion type discriminator(qtd) to determine thequestion type and then correspondingly set differ-ent numbers of candidate answers, denoted as n (cid:48).
speciﬁcally, we roughly divided question types (in-cluding “yes/no”, “num” and “other”) into yes/noand non-yes/no.
a gru binary classiﬁer is trainedwith cross-entropy loss and evaluated with 5-foldcross-validation on the train split of each dataset.
then, the trained qtd model with an accuracyabout 97% is implemented as an off-line moduleduring the test stage.
we will further investigatethe effect of n (cid:48) on each question type in the nextsection..final prediction in the inference phase, wesearch for the best dense caption ˆci among allcandidates c∗.
i for the ith image..ˆci = argmax.
δ(t rm(ii, cn.
i )).
(5).
n∈n (cid:48).
the answer ˆai corresponding to ˆci is the ﬁnalprediction..4 experiments.
4.1 setting.
datasets our models are trained and evalu-ated on the vqa-cp v2(agrawal et al., 2018)dataset, which is well-crafted by re-organizingvqa v2(goyal et al., 2017) training and validationsets such that answers for each question category(65 categories according to the question preﬁx)have different distributions in the train and test sets.
therefore, vqa-cp v2 is a natural choice for eval-uating vqa model’s generalizability.
the ques-tions of vqa-cp v2 include 3 types: “yes/no”,“num” and “other”.
note that the question typeand question category (e.g.“what color”) are differ-ent.
besides, we also evaluate our models on thevqa v2 validation set for completeness, and com-pare the accuracy difference between two datasets.
with the standard vqa evaluation metric(antolet al., 2015)..baselines we compare our method with thefollowing baseline methods: updn(andersonet al., 2018), areg(ramakrishnan et al., 2018),rubi(cadene et al., 2019), lmh(clark et al.,2019), rankvqa(qiao et al., 2020), ssl(zhuet al., 2020), css(chen et al., 2020a), cl(lianget al., 2020) and lxmert(tan and bansal, 2019).
most of them are designed for the language pri-ors problem, while lxmert represents the recenttrend towards utilizing bert-like pre-trained mod-els(li et al., 2019; chen et al., 2020b; li et al.,2020) which have top performances on variousdownstream vision and language tasks (includingvqa-v2).
note that mutant(gokhale et al.,2020) uses the extra object-name label to groundthe textual concepts in the image.
for fair compari-son, we do not compare with mutant..4.2.implementation details.
in this paper, we mainly choose ssl as our casand set n =12 and n =20 for training.
to extract im-age features, we follow previous work and use thepre-trained faster r-cnn to encode each imageas a set of ﬁxed 36 objects with 2048-dimensionalfeature vectors.
we use the tokenizer of lxmertto segment each dense caption into words.
allthe questions are trimmed to the same length of15 or 18, respectively for r or c question-answercombination strategy.
in the answer re-rankingmodule, we respectively incorporate two language-priors methods, ssl and lmh, into our proposedframework sar, which is dubbed as sar+ssland sar+lmh.
our models are trained on twotitan rtx 24gb gpus.
we train sar+sslfor 20 epochs with batch size of 32, sar andsar+lmh for 10 epochs with batch size of 64.for sar+ssl, we follow the same setting as theoriginal paper(zhu et al., 2020), except that wedon’t need to pre-train the model with the vqaloss before ﬁne-tuning it with the self-supervisedloss.
the adam optimizer is adopted with the learn-ing rate 1e–5..for question type discriminator, we use 300-dimensional glove(pennington et al., 2014) vectorsto initialize word embeddings and feed them into aunidirectional gru with 128 hidden units.
whentesting on the vaq-cp v2, n (cid:48) ranges from 1-2 foryes/no questions and 5-15 for non-yes/no questions.
as for vqa v2, n (cid:48) ranges from 1-2 for yes/no.
4105model.
updn(anderson et al., 2018)areg(ramakrishnan et al., 2018)rubi(cadene et al., 2019)lmh(clark et al., 2019)rankvqa(qiao et al., 2020)lxmert(tan and bansal, 2019)ssl(zhu et al., 2020)css(chen et al., 2020a)cl(liang et al., 2020)top12-sar(r→c)top20-sar(r→c)top12-sar+ssl(r→c) (ours)top20-sar+ssl(r→c) (ours)top12-sar+lmh(r)top20-sar+lmh(r).
(ours)(ours).
(ours)(ours).
vqa-cp v2 test(%)↑.
all yes/no num other all39.7441.1747.1152.4543.0546.2357.5958.9559.1864.5565.4464.2965.3265.9366.73.
11.9315.4820.2844.4613.9118.9129.8749.4249.8950.0554.5251.9854.3262.3062.34.
46.0535.4843.1845.5451.3255.5150.0348.2147.1658.859.1657.9458.8556.7357.84.
42.2765.4968.6569.8142.5342.8486.5384.3786.9983.0383.1382.8683.4185.3886.00.
63.4862.7561.1661.6465.4274.1663.7359.91-70.4170.6369.8470.0369.1369.22.vqa-v2 val(%)↑yes/no num other55.6642.1481.1855.1642.3579.84---55.0440.0377.8557.7545.3582.5165.1489.3156.85---55.1139.7773.25---61.3854.3487.8761.6454.9387.9160.7054.4187.2260.8554.5987.4760.0350.4387.6160.1251.2087.46.gap(%)↓23.7421.5814.059.1922.3727.936.140.96-5.865.195.554.713.202.49.table 1: results on vqa-cp v2 test and vqa-v2 validation set.
overall best scores are bold, our best areunderlined.
the gap represents the accuracy difference between vqa v2 and vqa-cp v2..questions and 2-5 for non-yes/no questions..4.3 results and analysis.
4.3.1 main results.
performance on two benchmarks vqa-cp-v2 andvqa-v2 is shown in table 1. we report the bestresults of sar, sar+ssl and sar+lmh among3 question-answer combination strategies respec-tively.
“topn-” represents that n candidate an-swers (selected by cas) feed into the answer re-ranking module for training.
our approach is eval-uated with two settings of n (12 and 20)..from the results on vqa-cp v2 shown in ta-ble 1, we can observe that: (1) top20-sar+lmhestablishes a new state-of-the-art accuracy of66.73% on vqa-cp v2, beating the previous best-performing method cl by 7.55%.
even with-out combining language-priors methods in an-swer re-ranking module, our model top20-saroutperforms cl by 6.26%.
these show the out-standing effectiveness of our proposed sar frame-work.
(2) sar+ssl and sar+lmh achieve muchbetter performance than ssl and lmh, whichdemonstrates that sar is compatible with cur-rent language-priors methods and could realizetheir full potential.
(3) compared with anotherreranking-based model rankvqa, our method ele-vates the performance by a large margin of 23.68%.
this shows the superiority of our proposed progres-sive select-and-rerank framework over rankvqawhich only uses the answer reranking as an aux-iliary task.
(4) previous models did not general-ize well on all question types.
cl is the previ-.
ous best on the “yes/no”, “num” questions andlxmert on the “other” questions.
in compar-ison, our model not only rivals the previous bestmodel on the “yes/no” questions but also improvesthe best performance on the “num” and “other”questions by 12.45% and 3.65%.
the remarkableperformance on all question types demonstratesthat our model makes a signiﬁcant progress towarda truly comprehensive vqa model..we also evaluate our method on the vqa v2which is deemed to have strong language biases.
as shown in table 1, our method achieves the bestaccuracy of 70.63% amongst baselines speciallydesigned for overcoming language priors, and isthe closest to the sota established by lxmertwhich is trained explicitly for the biased data set-ting.
for completeness, the performance gap be-tween two datasets is also compared in table 1 withthe protocol from chen et al.
(2020a).
comparedwith most previous models which suffer severe per-formance drops between vqa v2 and vqa-cp v2(e.g., 27.93% in lxmert), the top20-sar+lmhsigniﬁcantly decreases the performance drop to2.49%, which demonstrates the effectiveness ofour framework to further overcome the languagebiases.
though css achieves a better performancegap, it sacriﬁces the performance on the vqa v2.
meanwhile, as n rises from 12 to 20, our modelsachieve better accuracy on both datasets along witha smaller performance gap.
this demonstrates that,unlike previous methods, our method can alleviatelanguage priors while maintaining an excellent ca-pability of answering questions.
nonetheless, we.
4106top n model.
r59.51sarsar+ssl62.12sar+lmh 65.9360.43sarsar+ssl62.29sar+lmh 66.73.c60.2462.8765.2361.8663.9465.19.r→c64.5564.2965.1465.4465.3266.71.top12.
top20.
table 3: results on the vqa-cp v2 test set based ondifferent question-answer combination strategies: r, cand r→c.
the major difference between r and c iswhether keeping question preﬁx which includes 65 cat-egories..updn outperforms that based on lmh, but lmhis a better vqa model in overcoming language pri-ors compared with updn.
this is because a goodcandidate answer selector has two requirements:(a) it should be able to recall more correct answers.
(b) under the scenario of language biases, wronganswers recalled by cas at training time shouldhave superﬁcial correlations with the question asstrong as possible.
however, the ensemble meth-ods, such as lmh, are trained to pay more attentionto the samples which are not correctly answered bythe question-only model.
this seriously reducesthe recall rate of those language-priors wrong an-swers, which leads to the training data for ve istoo simple and thus hurts the model’s capability ofreducing language priors.
(2) if cas is the gen-eral vqa model updn rather than lmh and ssl,the improvement brought from the combinationwith language-priors method in answer re-rankingmodule is more obvious.
(3) even we choose theupdn, a backbone model of most current works, asour cas and do not involve any language-priorsmethods, sar still achieves a much better accu-racy than the previous sota model cl by 2.53%,which shows that our basic framework already pos-sesses outstanding capability of reducing languagepriors..4.3.4 the effect of question-answercombination strategies.
from the results shown in table 3, we can observethat: (1) from overall results, r→c achieves or ri-vals the best performance on three models.
onaverage, r→c outperforms c by 2.02% whichdemonstrates avoiding the co-occurrence of ques-tion category and answer during training time couldeffectively alleviate language priors; r→c outper-forms r by 2.41% which indicates that the informa-.
figure 3: results from model sar+ssl(r→c) invqa-cp v2 with different n during training..model/cas updn lmh sslw/o sar∗41.0461.71sar63.52sar+sslsar+lmh 64.98.
53.0361.6561.7862.72.
57.6664.5564.2965.14.table 2: results based on different cas in vqa-cpv2.
we set n=12.
∗ indicates the results come from ourreimplementation using ofﬁcial released codes..believe that, how to improve the model’s generalityand further transform the trade-off between elim-inating language priors and answering questionsinto win–win outcomes, is a promising researchdirection in the future..4.3.2 the effect of n.from figure 3, we can observe that the overall per-formance is getting better as n increases.
the per-formance improvement on the “num” and “other”questions is especially obvious, and there is a veryslight drop on the “yes/no” questions.
we believethat sar can further get better performance byproperly increasing n .
due to the resource limita-tion, the largest n we use is 20 in this paper..4.3.3 the effect of different cas.
to ﬁnd out the potential performance limitationof cas models, we show the accuracy of 3 casmodels on the vqa-cp v2 test set.
as shown infigure 1 (a), the top3 accuracy (acc) of 3 models isabout 70% and top6 acc is 80%, which guaranteesthat sufﬁcient correct answers are recalled by cas.
and thus, the performance limitation of cas isnegligible..we also conduct experiments to investigate theeffect of different cas on sar.
from the resultsshown in table 2, we can observe that: (1) choos-ing a better vqa model as cas does not guaranteea better performance, e.g.
performance based on.
4107allmodel46.23 42.84lxm53.09 55.07lxm+ssl55.58 70.91cas+lxm(r)59.41 76.60cas+lxm+ssl(r)cas+lxm+qtd(r)59.51 83.20cas+lxm+ssl+qtd(r) 62.12 85.14.yes/no num other18.91 55.5129.60 58.5029.14 54.8140.81 55.5129.17 55.4241.63 55.68.table 4: ablation study to investigate the effect of eachcomponent of top12-sar+ssl: candidate answerselector (cas), lxmert (lxm), question type dis-criminator (qtd) and ssl..tion of question category is useful in inference.
(2)on the sar and sar+ssl, c consistently outper-forms r, but on the sar+lmh, we see oppositeresults.
this is probably because our method andthe balancing-data method ssl could learn the pos-itive bias resulted from the superﬁcial correlationsbetween question category and answer, which isuseful for generalization, but the ensemble-basedmethod lmh will attenuate positive bias duringde-biasing process.
(3) even without language pri-ors method, sar with r→c rivals or outperformsthe sar+ssl and sar+lmh with r or c, whichshows that r→c strategy could help the model toalleviate language priors.
as a result, comparedwith r or c, our framework with r→c only gainsa slight performance improvement after using thesame language-priors methods..4.3.5 ablation study.
“cas+” represents we use the select-and-rerankfrom table 4, we can ﬁndframework.
that:(1) lxm+ssl represents directly apply-ing ssl to lxmert.
its poor performanceshows that the major contribution of our frame-work does not come from the combination ofthe language-priors method ssl and pre-trainedmodel lxmert.
(2) compared with lxm andlxm+ssl, cas+lxm and cas+lxm+ssl re-spectively gain prominent performance boost of9.35% and 6.32%, which demonstrates the im-portance and effectiveness of our proposed select-and-rerank procedure.
(3) cas+lxm+qtd(r)and cas+lxm+ssl+qtd(r) respectively out-perform cas+lxm(r) and cas+lxm+ssl(r)by 3.93% and 2.71%, which shows the contribu-tion of qtd module.
this further demonstratesthat choosing appropriate n (cid:48) for different questiontypes is a useful step for model performance.
(4)cas+lxm+ssl+qtd improves the performanceof cas+lxm+qtd by 2.61%, which shows that.
figure 4:results from sar(r), sar+ssl(r),sar(r→c) and sar+lmh(r) with different n (cid:48) dur-ing test.
to better investigate the impact of n (cid:48) on eachquestion type, we report the results without questiontype discriminator..figure 5: qualitative comparison between our top20-sar(r→c) and the baseline ssl.
the green/redbounding boxes indicate the most important regions re-sulting from ours/ssl.
g-t is ground-truth..current language-priors methods ﬁt our frameworkwell and could further improve performance..4.3.6 the effect of n (cid:48)from figure 4, we can ﬁnd that: (1) the best n (cid:48)for yes/no questions is smaller than that for non-yes/no questions due to the nature of yes/no ques-tion.
(2) as n (cid:48) increases, the accuracy of “num”and “other” questions rises ﬁrst and then decreases.
there is a trade-off behind this phenomenon: whenn (cid:48) is too small, the correct answer may not berecalled by cas; when n (cid:48) is too large, the distrac-tion from wrong answers makes it more difﬁcultfor model to choose the correct answer..4.3.7 qualitative examples.
we qualitatively evaluate the effectiveness of ourframework.
as shown in figure 5, compared withssl, sar performs better not only in questionanswering but also in visual grounding.
with the.
4108help of answer semantics, sar can focus on theregion relevant to the candidate answer and furtheruse the region to verify its correctness..5 conclusion.
in this paper, we propose a select-and-rerank (sar)progressive framework based on visual entailment.
speciﬁcally, we ﬁrst select candidate answers toshrink the prediction space, then we rerank can-didate answers by a visual entailment task whichveriﬁes whether the image semantically entails thesynthetic statement of the question and each can-didate answer.
our framework can make full useof the interactive information of image, questionand candidate answers.
in addition, it is a genericframework, which can be easily combined withthe existing vqa models and further boost theirabilities.
we demonstrate advantages of our frame-work on the vqa-cp v2 dataset with extensiveexperiments and analyses.
our method establishesa new state-of-the-art accuracy of 66.73% with animprovement of 7.55% on the previous best..acknowledgments.
this work was supported by national natural sci-ence foundation of china (no.
61976207, no.
61906187).
references.
aishwarya agrawal, dhruv batra, and devi parikh.
2016. analyzing the behavior of visual question an-swering models.
in emnlp..aishwarya agrawal, dhruv batra, devi parikh, andaniruddha kembhavi.
2018. don’t just assume;look and answer: overcoming priors for visual ques-tion answering.
in proceedings of the ieee confer-ence on computer vision and pattern recognition,pages 4971–4980..peter anderson, xiaodong he, chris buehler, damienteney, mark johnson, stephen gould, and leizhang.
2018. bottom-up and top-down attention forimage captioning and visual question answering.
inproceedings of the ieee conference on computer vi-sion and pattern recognition, pages 6077–6086..stanislaw antol, aishwarya agrawal, jiasen lu, mar-garet mitchell, dhruv batra, c lawrence zitnick,and devi parikh.
2015. vqa: visual question an-swering.
in proceedings of the ieee internationalconference on computer vision, pages 2425–2433..yonatan belinkov, adam poliak, stuart m shieber,benjamin van durme, and alexander m rush.
2019..don’t take the premise for granted: mitigating arti-facts in natural language inference.
in acl (1)..remi cadene, corentin dancette, matthieu cord, deviparikh, et al.
2019. rubi: reducing unimodal biasesfor visual question answering.
advances in neuralinformation processing systems, 32:841–852..long chen, xin yan, jun xiao, hanwang zhang, shil-iang pu, and yueting zhuang.
2020a.
counterfac-tual samples synthesizing for robust visual questionanswering.
in proceedings of the ieee/cvf confer-ence on computer vision and pattern recognition,pages 10800–10809..yen-chun chen, linjie li, licheng yu, ahmedel kholy, faisal ahmed, zhe gan, yu cheng, andjingjing liu.
2020b.
uniter: universal image-textrepresentation learning.
in european conference oncomputer vision, pages 104–120.
springer..christopher clark, mark yatskar, and luke zettle-moyer.
2019. don’t take the easy way out: en-semble based methods for avoiding known datasetin proceedings of the 2019 conference onbiases.
empirical methods in natural language processingand the 9th international joint conference on natu-ral language processing (emnlp-ijcnlp), pages4060–4073..tejas gokhale, pratyay banerjee, chitta baral, andyezhou yang.
2020. mutant: a training paradigmfor out-of-distribution generalization in visual ques-in proceedings of the 2020 con-tion answering.
ference on empirical methods in natural languageprocessing (emnlp), pages 878–892..yash goyal, tejas khot, douglas summers-stay,dhruv batra, and devi parikh.
2017. making thev in vqa matter: elevating the role of image under-standing in visual question answering.
in proceed-ings of the ieee conference on computer visionand pattern recognition, pages 6904–6913..gabriel grand and yonatan belinkov.
2019. adver-sarial regularization for visual question answering:strengths, shortcomings, and side effects.
naaclhlt 2019, page 1..allan jabri, armand joulin,.
and laurens vander maaten.
2016. revisiting visual question an-swering baselines.
in european conference on com-puter vision, pages 727–739.
springer..justin johnson, andrej karpathy, and li fei-fei.
2016. densecap: fully convolutional localizationin proceedings ofnetworks for dense captioning.
the ieee conference on computer vision and pat-tern recognition..bernhard kratzwald, anna eigenmann, and stefanfeuerriegel.
2019. rankqa: neural question answer-ing with answer re-ranking.
in proceedings of the57th annual meeting of the association for compu-tational linguistics, pages 6076–6085..4109liunian harold li, mark yatskar, da yin, cho-juihsieh, and kai-wei chang.
2019. visualbert: asimple and performant baseline for vision and lan-guage.
arxiv preprint arxiv:1908.03557..ning xie, farley lai, derek doran, and asim ka-dav.
2019. visual entailment: a novel task forﬁne-grained image understanding.
arxiv preprintarxiv:1901.06706..peng zhang, yash goyal, douglas summers-stay,dhruv batra, and devi parikh.
2016. yin and yang:balancing and answering binary visual questions.
inproceedings of the ieee conference on computervision and pattern recognition, pages 5014–5022..xi zhu, zhendong mao, chunxiao liu, peng zhang,bin wang, and yongdong zhang.
2020. overcom-ing language priors with self-supervised learning forvisual question answering..xiujun li, xi yin, chunyuan li, pengchuan zhang, xi-aowei hu, lei zhang, lijuan wang, houdong hu,li dong, furu wei, et al.
2020. oscar: object-semantics aligned pre-training for vision-languagetasks.
in european conference on computer vision,pages 121–137.
springer..zujie liang, weitao jiang, haifeng hu, and jiayingzhu.
2020. learning to contrast the counterfactualsamples for robust visual question answering.
inproceedings of the 2020 conference on empiricalmethods in natural language processing (emnlp),pages 3285–3292..rabeeh karimi mahabadi and james henderson.
2019.simple but effective techniques to reduce biases.
arxiv preprint arxiv:1909.06321, 2(3):5..jeffrey pennington, richard socher, and christopher dmanning.
2014. glove: global vectors for word rep-resentation.
in proceedings of the 2014 conferenceon empirical methods in natural language process-ing (emnlp), pages 1532–1543..yanyuan qiao, zheng yu, and jing liu.
2020.rankvqa: answer re-ranking for visual question an-swering.
in 2020 ieee international conference onmultimedia and expo (icme), pages 1–6.
ieee..sainandan ramakrishnan, aishwarya agrawal, andstefan lee.
2018. overcoming language priors invisual question answering with adversarial regular-ization.
in neurips..ramprasaath r selvaraju, stefan lee, yilin shen,hongxia jin, shalini ghosh, larry heck, dhruv ba-tra, and devi parikh.
2019. taking a hint: lever-aging explanations to make vision and languagein proceedings of themodels more grounded.
ieee/cvf international conference on computervision, pages 2591–2600..hao tan and mohit bansal.
2019. lxmert: learningcross-modality encoder representations from trans-formers.
in proceedings of the 2019 conference onempirical methods in natural language processingand the 9th international joint conference on natu-ral language processing (emnlp-ijcnlp), pages5103–5114..shuohang wang, mo yu, jing jiang, wei zhang, xiaox-iao guo, shiyu chang, zhiguo wang, tim klinger,gerald tesauro, and murray campbell.
2018a.
ev-idence aggregation for answer re-ranking in open-in international con-domain question answering.
ference on learning representations..zhen wang, jiachen liu, xinyan xiao, yajuan lyu,and tian wu.
2018b.
joint training of candidate ex-traction and answer selection for reading comprehen-sion.
in acl (1)..4110