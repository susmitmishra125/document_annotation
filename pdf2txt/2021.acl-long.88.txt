explainable prediction of text complexity: the missing preliminaries fortext simpliﬁcation.
cristina gˆarbacea1, mengtian guo4, samuel carton3, qiaozhu mei1,21department of eecs, university of michigan, ann arbor2school of information, university of michigan, ann arbor3department of cs, university of colorado, boulder4school of information and library science, university of north carolina, chapel hill.
abstract.
text simpliﬁcation reduces the language com-plexity of professional content for accessibil-ity purposes.
end-to-end neural network mod-els have been widely adopted to directly gen-erate the simpliﬁed version of input text, usu-ally functioning as a blackbox.
we show thattext simpliﬁcation can be decomposed into acompact pipeline of tasks to ensure the trans-parency and explainability of the process.
theﬁrst two steps in this pipeline are often ne-glected: 1) to predict whether a given pieceof text needs to be simpliﬁed, and 2) if yes,to identify complex parts of the text.
thetwo tasks can be solved separately using eitherlexical or deep learning methods, or solvedjointly.
simply applying explainable complex-ity prediction as a preliminary step, the out-of-sample text simpliﬁcation performance of thestate-of-the-art, black-box simpliﬁcation mod-els can be improved by a large margin..1.introduction.
text simpliﬁcation aims to reduce the languagecomplexity of highly specialized textual content sothat it is accessible for readers who lack adequateliteracy skills, such as children, people with loweducation, people who have reading disorders ordyslexia, and non-native speakers of the language.
mismatch between language complexity and lit-eracy skills is identiﬁed as a critical source of biasand inequality in the consumers of systems builtupon processing and analyzing professional textcontent.
research has found that it requires onaverage 18 years of education for a reader to prop-erly understand the clinical trial descriptions onclinicaltrials.gov, and this introduces a potentialself-selection bias to those trials (wu et al., 2016).
text simpliﬁcation has considerable potentialto improve the fairness and transparency of textinformation systems.
indeed, the simple english.
wikipedia (simple.wikipedia.org) has been con-structed to disseminate wikipedia articles to kidsand english learners.
in healthcare, consumer vo-cabulary are used to replace professional medicalterms to better explain medical concepts to thepublic (abrahamsson et al., 2014).
in education,natural language processing and simpliﬁed text gen-eration technologies are believed to have the poten-tial to improve student outcomes and bring equalopportunities for learners of all levels in teaching,learning and assessment (mayﬁeld et al., 2019)..ironically, the deﬁnition of “text simpliﬁcation”in literature has never been transparent.
the termmay refer to reducing the complexity of text at var-ious linguistic levels, ranging all the way throughreplacing individual words in the text to generat-ing a simpliﬁed document completely through acomputer agent.
in particular, lexical simpliﬁcation(devlin, 1999) is concerned with replacing com-plex words or phrases with simpler alternatives;syntactic simpliﬁcation (siddharthan, 2006) altersthe syntactic structure of the sentence; semanticsimpliﬁcation (kandula et al., 2010) paraphrasesportions of the text into simpler and clearer variants.
more recent approaches simplify texts in an end-to-end fashion, employing machine translation modelsin a monolingual setting regardless of the type ofsimpliﬁcations (zhang and lapata, 2017; guo et al.,2018; van den bercken et al., 2019).
nevertheless,these models are limited on the one hand due to theabsence of large-scale parallel (complex → simple)monolingual training data, and on the other handdue to the lack of interpretibility of their black-boxprocedures (alva-manchego et al., 2017)..given the ambiguity in problem deﬁnition, therealso lacks consensus on how to measure the good-ness of text simpliﬁcation systems, and automaticevaluation measures are perceived ineffective andsometimes detrimental to the speciﬁc procedure, inparticular when they favor shorter but not necessar-.
proceedingsofthe59thannualmeetingoftheassociationforcomputationallinguisticsandthe11thinternationaljointconferenceonnaturallanguageprocessing,pages1086–1097august1–6,2021.©2021associationforcomputationallinguistics1086ily simpler sentences (napoles et al., 2011).
whileend-to-end simpliﬁcation models demonstrate su-perior performance on benchmark datasets, theirsuccess is often compromised in out-of-sample,real-world scenarios (d’amour et al., 2020)..our work is motivated by the aspiration that in-creasing the transparency and explainability of amachine learning procedure may help its gener-alization into unseen scenarios (doshi-velez andkim, 2018).
we show that the general problemof text simpliﬁcation can be formally decomposedinto a compact and transparent pipeline of mod-ular tasks.
we present a systematic analysis ofthe ﬁrst two steps in this pipeline, which are com-monly overlooked: 1) to predict whether a givenpiece of text needs to be simpliﬁed at all, and 2)to identify which part of the text needs to be sim-pliﬁed.
the second task can also be interpretedas an explanation of the ﬁrst task: why a piece oftext is considered complex.
these two tasks can besolved separately, using either lexical or deep learn-ing methods, or they can be solved jointly throughan end-to-end, explainable predictor.
based on theformal deﬁnitions, we propose general evaluationmetrics for both tasks and empirically compare a di-verse portfolio of methods using multiple datasetsfrom different domains, including news, wikipedia,and scientiﬁc papers.
we demonstrate that by sim-ply applying explainable complexity prediction as apreliminary step, the out-of-sample text simpliﬁca-tion performance of the state-of-the-art, black-boxmodels can be improved by a large margin..our work presents a promising direction towardsa transparent and explainable solution to text sim-pliﬁcation in various domains..2 related work.
2.1 text simpliﬁcation.
identifying complex words.
2.1.1text simpliﬁcation at word level has been donethrough 1) lexicon based approaches, whichmatch words to lexicons of complex/simple words(del´eger and zweigenbaum, 2009; elhadad andsutaria, 2007), 2) threshold based approaches,which apply a threshold over word lengths orcertain statistics (leroy et al., 2013), 3) humandriven approaches, which solicit the user’s inputon which words need simpliﬁcation (rello et al.,2013), and 4) classiﬁcation methods, which trainmachine learning models to distinguish complexwords from simple words (shardlow, 2013).
com-.
plex word identiﬁcation is also the main topic ofsemeval 2016 task 11 (paetzold and specia, 2016),aiming to determine whether a non-native englishspeaker can understand the meaning of a word ina given sentence.
signiﬁcant differences exist be-tween simple and complex words, and the latter onaverage are shorter, less ambiguous, less frequent,and more technical in nature.
interestingly, the fre-quency of a word is identiﬁed as a reliable indicatorof its simplicity (leroy et al., 2013)..while the above techniques have been widelyemployed for complex word identiﬁcation, the re-sults reported in the literature are rather controver-sial and it is not clear to what extent one techniqueoutperforms the other in the absence of standard-ized high quality parallel corpora for text simpliﬁ-cation (paetzold, 2015).
pre-constructed lexiconsare often limited and do not generalize to differentdomains.
it is intriguing that classiﬁcation methodsreported in the literature are not any better than a“simplify-all” baseline (shardlow, 2014)..2.1.2 readability assessment.
traditionally, measuring the level of reading difﬁ-culty is done through lexicon and rule-based met-rics such as the age of acquisition lexicon (aoa)(kuperman et al., 2012) and the flesch-kincaidgrade level (kincaid et al., 1975).
a machinelearning based approach in (schumacher et al.,2016) extracts lexical, syntactic, and discourse fea-tures and train logistic regression classiﬁers to pre-dict the relative complexity of a single sentencein a pairwise setting.
the most predictive featuresare simple representations based on aoa norms.
the perceived difﬁculty of a sentence is highly in-ﬂuenced by properties of the surrounding passage.
similar methods are used for ﬁne-grained classiﬁ-cation of text readability (aluisio et al., 2010) andcomplexity ( ˇstajner and hulpus, , 2020)..2.1.3 computer-assisted paraphrasing.
simpliﬁcation rules are learnt by ﬁnding wordsfrom a complex sentence that correspond to differ-ent words in a simple sentence (alva-manchegoet al., 2017).
identifying simpliﬁcation operationssuch as copies, deletions, and substitutions forwords from parallel complex vs. simple corporahelps understand how human experts simplify text(alva-manchego et al., 2017).
machine translationhas been employed to learn phrase-level alignmentsfor sentence simpliﬁcation (wubben et al., 2012).
lexical and phrasal paraphrase rules are extracted.
1087in (pavlick and callison-burch, 2016).
these meth-ods are often evaluated by comparing their out-put to gold-standard, human-generated simpliﬁca-tions, using standard metrics (e.g., token-level pre-cision, recall, f1), machine translation metrics (e.g.,bleu (papineni et al., 2002) ), text simpliﬁcationmetrics (e.g.
sari (xu et al., 2016) which rewardscopying words from the original sentence), andreadability metrics (among which flesch-kincaidgrade level (kincaid et al., 1975) and flesch read-ing ease (kincaid et al., 1975) are most commonlyused).
it is desirable that the output of the compu-tational models is ultimately validated by humanjudges (shardlow, 2014)..2.1.4 end-to-end simpliﬁcation.
neural encoder-decoder models are used to learnsimpliﬁcation rewrites from monolingual corporaof complex and simple sentences (scarton and spe-cia, 2018; van den bercken et al., 2019; zhangand lapata, 2017; guo et al., 2018).
on one hand,these models often obtain superior performance onparticular evaluation metrics, as the neural networkdirectly optimizes these metrics in training.
onthe other hand, it is hard to interpret what exactlyare learned in the hidden layers, and without thistransparency it is difﬁcult to adapt these models tonew data, constraints, or domains.
for example,these end-to-end simpliﬁcation models tend not todistinguish whether the input text should or shouldnot be simpliﬁed at all, making the whole processless transparent.
when the input is already simple,the models tend to oversimplify it and deviate fromits original meaning (see section 5.3)..2.2 explanatory machine learning.
various approaches are proposed in the literatureto address the explainability and interpretability ofmachine learning agents.
the task of providingexplanations for black-box models has been tack-led either at a local level by explaining individualpredictions of a classiﬁer (ribeiro et al., 2016), orat a global level by providing explanations for themodel behavior as a whole (letham et al., 2015).
more recently, differential explanations are pro-posed to describe how the logic of a model variesacross different subspaces of interest (lakkarajuet al., 2019).
layer-wise relevance propagation(arras et al., 2017) is used to trace backwards textclassiﬁcation decisions to individual words, whichare assigned scores to reﬂect their separate contri-bution to the overall prediction..lime (ribeiro et al., 2016) is a model-agnosticexplanation technique which can approximate anymachine learning model locally with another sparselinear interpretable model.
shap (lundberg andlee, 2017) evaluates shapley values as the averagemarginal contribution of a feature value across allpossible coalitions by considering all possible com-binations of inputs and all possible predictions foran instance.
explainable classiﬁcation can also besolved simultaneously through a neural network,using hard attentions to select individual wordsinto the “rationale” behind a classiﬁcation decision(lei et al., 2016).
extractive adversarial networksemploys a three-player adversarial game which ad-dresses high recall of the rationale (carton et al.,2018).
the model consists of a generator which ex-tracts an attention mask for each token in the inputtext, a predictor that cooperates with the generatorand makes prediction from the rationale (words at-tended to), and an adversarial predictor that makespredictions from the remaining words in the inverserationale.
the minimax game between the two pre-dictors and the generator is designed to ensure allpredictive signals are included into the rationale..no prior work has addressed the explainability.
of text complexity prediction.
we ﬁll in this gap..3 an explainable pipeline for text.
simpliﬁcation.
we propose a uniﬁed view of text simpliﬁcationwhich is decomposed into several carefully de-signed sub-problems.
these sub-problems gener-alize over many approaches, and they are logicallydependent on and integratable with one another sothat they can be organized into a compact pipeline..figure 1: a text simpliﬁcation pipeline.
explainableprediction of text complexity is the preliminary of anyhuman-based, computer assisted, or automated system..1088the ﬁrst conceptual block in the pipeline (fig-ure 1) is concerned with explainable prediction ofthe complexity of text.
it consists of two sub-tasks:1) prediction: classifying a given piece of text intotwo categories, needing simpliﬁcation or not; and2) explanation: highlighting the part of the textthat needs to be simpliﬁed.
the second conceptualblock is concerned with simpliﬁcation generation,the goal of which is to generate a new, simpliﬁedversion of the text that needs to be simpliﬁed.
thisstep could be achieved through completely man-ual effort, or a computer-assisted approach (e.g.,by suggesting alternative words and expressions),or a completely automated method (e.g., by self-translating into a simpliﬁed version).
the secondbuilding block is piped into a step of human judg-ment, where the generated simpliﬁcation is tested,approved, and evaluated by human practitioners..one could argue that for an automated simpliﬁca-tion generation system the ﬁrst block (complexityprediction) is not necessary.
we show that it is notthe case.
indeed, it is unlikely that every piece oftext needs to be simpliﬁed in reality, and instead thesystem should ﬁrst decide whether a sentence needsto be simpliﬁed or not.
unfortunately such a step isoften neglected by existing end-to-end simpliﬁers,thus their performance is often biased towards thecomplex sentences that are selected into their train-ing datasets at the ﬁrst place and doesn’t generalizewell to simple inputs.
empirically, when thesemodels are applied to out-of-sample text whichshouldn’t be simpliﬁed at all, they tend to oversim-plify the input and result in a deviation from itsoriginal meaning (see section 5.3)..one could also argue that an explanation com-ponent (1b) is not mandatory in certain text sim-pliﬁcation practices, in particular in an end-to-endneural generative model that does not explicitlyidentify the complex parts of the input sentence.
in reality, however, it is often necessary to high-light the differences between the original sentenceand the simpliﬁed sentence (which is essentially avariation of 1b) to facilitate the validation and eval-uation of these black-boxes.
more generally, theexplainability/interpretability of a machine learn-ing model has been widely believed to be an in-dispensable factor to its ﬁdelity and fairness whenapplied to the real world (lakkaraju et al., 2019).
since the major motivation of text simpliﬁcationis to improve the fairness and transparency of textinformation systems, it is critical to explain the ra-.
tionale behind the simpliﬁcation decisions, even ifthey are made through a black-box model..without loss of generality, we can formally de-.
ﬁne the sub-tasks 1a, 1b, and 2- in the pipeline:.
deﬁnition 3.1.
(complexity prediction).
let textd ∈ d be a sequence of tokens w1w2...wn.
thetask of complexity prediction is to ﬁnd a functionf : d → {0, 1} such that f (d) = 1 if d needs tobe simpliﬁed, and f (d) = 0 otherwise.
deﬁnition 3.2.
(complexity explanation).
let dbe a sequence of tokens w1w2...wn and f (d) = 1.the task of complexity explanation/highlighting isto ﬁnd a function h : d → {0, 1}n s.t.
h(d) =c1c2...cn, where ci = 1 means wi will be high-lighted as a complex portion of d and ci = 0 other-wise.
we denote d|h(d) as the highlighted part ofd and d|¬h(d) as the unhighlighted part of d.deﬁnition 3.3.
(simpliﬁcation generation).
let dbe a sequence of tokens w1w2...wn and f (d) = 1.the task of simpliﬁcation generation is to ﬁnd afunction g : d → d(cid:48) s.t.
g(d, f (d), h(d)) = d(cid:48),where d(cid:48) = w(cid:48)m and f (d(cid:48)) = 0, subject tothe constraint that d(cid:48) preserves the meaning of d.in this paper, we focus on an empirical analysisof the ﬁrst two sub-tasks of explainable predic-tion of text complexity (1a and 1b), which are thepreliminaries of any reasonable text simpliﬁcationpractice.
we leave aside the detailed analysis ofsimpliﬁcation generation (2-) for now, as there aremany viable designs of g(·) in practice, spanningthe spectrum between completely manual and com-pletely automated.
since this step is not the focusof this paper, we intend to leave the deﬁnition ofsimpliﬁcation generation highly general..2...w(cid:48).
1w(cid:48).
note that the deﬁnitions of complexity predic-tion and complexity explanation can be naturallyextended to a continuous output, where f (·) pre-dicts the complexity level of d and h(·) predicts thecomplexity weight of wi.
the continuous outputwould align the problem more closely to readabilitymeasures (kincaid et al., 1975).
in this paper, westick to the binary output because a binary action(to simplify or not) is almost always necessary inreality even if a numerical score is available..note that the deﬁnition of complexity explana-tion is general enough for existing approaches.
inlexical simpliﬁcation where certain words in a com-plex vocabulary v are identiﬁed to explain thecomplexity of a sentence, it is equivalent to high-lighting every appearance of these words in d, or∀wi ∈ v, ci = 1.in automated simpliﬁcation.
1089where there is a self-translation function g(d) = d(cid:48),h(d) can be simply instantiated as a function thatreturns a sequence alignment of d and d(cid:48).
suchreformulation helps us deﬁne uniﬁed evaluationmetrics for complexity explanation (see section 4).
it is also important to note that the dependencybetween the components, especially complexityprediction and explanation, does not restrict themto be done in isolation.
these sub-tasks can bedone either separately, or jointly with an end-to-end approach as long as the outputs of f, h, g areall obtained (so that transparency and explainabilityare preserved).
in section 4, we include both sepa-rate models and end-to-end models for explanatorycomplexity predication in one shot..4 empirical analysis of complexity.
prediction and explanation.
with the pipeline formulation, we are able to com-pare a wide range of methods and metrics for thesub-tasks of text simpliﬁcation.
we aim to under-stand how difﬁcult they are in real-world settingsand which method performs the best for which task..4.1 complexity prediction.
4.1.1 candidate models.
we examine a wide portfolio of deep and shallowbinary classiﬁers to distinguish complex sentencesfrom simple ones.
among the shallow modelswe use naive bayes (nb), logistic regression(lr), support vector machines (svm) and ran-dom forests (rf) classiﬁers trained with unigrams,bigrams and trigrams as features.
we also train theclassiﬁers using the lexical and syntactic featuresproposed in (schumacher et al., 2016) combinedwith the n-gram features (denoted as “enriched fea-tures”).
we include neural network models such asword and char-level long short-term memory net-work (lstm) and convolutional neural networks(cnn).
we also employ a set of state-of-the-artpre-trained neural language models, ﬁne-tuned forcomplexity prediction; we introduce them below.
ulmfit (howard and ruder, 2018) a languagemodel on a large general corpus such as wikitext-103 and then ﬁne-tunes it on the target task usingslanted triangular rates, and gradual unfreezing.
we use the publicly available implementation1 ofthe model with two ﬁne-tuning epochs for eachdataset and the model quickly adapts to a new task..1https://docs.fast.ai/tutorial.text..html, retrieved on 5/31/2021..bert (devlin et al., 2019) trains deep bidirec-tional language representations and has greatly ad-vanced the state-of-the-art for many natural lan-guage processing tasks.
the model is pre-trainedon the english wikipedia as well as the googlebook corpus.
due to computational constraints,we use the 12 layer bert base pre-trained modeland ﬁne-tune it on our three datasets.
we select thebest hyperparameters based on each validation set.
xlnet (yang et al., 2019) overcomes the limi-tations of bert (mainly the use of masks) with apermutation-based objective which considers bidi-rectional contextual information from all positionswithout data corruption.
we use the 12 layerxlnet base pre-trained model on the englishwikipedia, the books corpus (similar to bert),giga5, clueweb 2012-b, and common crawl..4.1.2 evaluation metric.
we evaluate the performance of complexity pre-diction models using classiﬁcation accuracy onbalanced training, validation, and testing datasets..4.2 complexity explanation.
4.2.1 candidate modelswe use lime in combination with lr and lstmclassiﬁers, shap on top of lr, and the extractiveadversarial networks which jointly conducts com-plexity prediction and explanation.
we feed eachtest complex sentence as input to these explanatorymodels and compare their performance at identify-ing tokens (words and punctuation) that need to beremoved or replaced from the input sentence..we compare these explanatory models with threebaseline methods: 1) random highlighting: ran-domly draw the size and the positions of tokens tohighlight; 2) lexicon based highlighting: highlightwords that appear in the age-of-acquisition (aoa)lexicon (kuperman et al., 2012), which containsratings for 30,121 english content words (nouns,verbs, and adjectives) indicating the age at whicha word is acquired; and 3) feature highlighting:highlight the most important features of the bestperforming lr models for complexity prediction..4.2.2 evaluation metrics.
evaluation of explanatory machine learning is anopen problem.
in the context of complexity expla-nation, when the ground truth of highlighted tokens(yc(d) = c1c2...cn, ci ∈ {0, 1}) in each complexsentence d is available, we can compare the outputof complexity explanation h(d) with yc(d).
such.
1090per-token annotations are usually not available inscale.
to overcome this, given a complex sentenced and its simpliﬁed version d(cid:48), we assume that alltokens wi in d which are absent in d(cid:48) are candidatewords for deletion or substitution during the textsimpliﬁcation process and should therefore be high-lighted in complexity explanation (i.e., ci = 1)..in particular, we use the following evaluationmetrics for complexity explanation: 1) tokenwiseprecision (p), which measures the proportion ofhighlighted tokens in d that are truly removed ind(cid:48); 2) tokenwise recall (r), which measures theproportion of tokens removed in d(cid:48) that are actuallyhighlighted in d; 3) tokenwise f1, the harmonicmean of p and r; 4) word-level edit distance (ed)(levenshtein, 1966): between the unhighlightedpart of d and the simpliﬁed document d(cid:48).
intu-itively, a more successful complexity explanationwould highlight most of the tokens that need to besimpliﬁed, thus the remaining parts in the complexsentences will be closer to the simpliﬁed version,achieving a lower edit distance (we also explore edwith a higher penalty cost for the substitution oper-ation, namely values of 1, 1.5 and 2); and 5) trans-lation edit rate (ter) (snover et al., 2006), whichmeasures the minimum number of edits needed tochange a hypothesis (the unhighlighted part of d)so that it exactly matches the closest references(the simpliﬁed document d(cid:48)).
note these metricsare all proxies of the real editing process from dto d(cid:48).
when token-level edit history is available(e.g., through track changes), it is better to comparethe highlighted evaluation with these true changesmade.
we compute all the metrics at sentence leveland macro-average them..4.3 experiment setup.
4.3.1 datasets.
we use three different datasets (table 1) whichcover different domains and application scenariosof text simpliﬁcation.
our ﬁrst dataset is newsela(xu et al., 2015), a corpus of news articles simpli-ﬁed by professional news editors.
in our experi-ments we use the parallel newsela corpus with thetraining, validation, and test splits made availablein (zhang and lapata, 2017).
second, we use thewikilarge corpus introduced in (zhang and lap-ata, 2017).
the training subset of wikilarge iscreated by assembling datasets of parallel alignedwikipedia - simple wikipedia sentence pairs avail-able in the literature (kauchak, 2013).
while this.
training set is obtained through automatic align-ment procedures which can be noisy, the validationand test subsets of wikilarge contain complex sen-tences with simpliﬁcations provided by amazonmechanical turk workers (xu et al., 2016); we in-crease the size of validation and test on top of thesplits made available in (zhang and lapata, 2017).
third, we use the dataset released by the biendatacompetition2, which asks participants to match re-search papers from various scientiﬁc disciplineswith press releases that describe them.
arguably,rewriting scientiﬁc papers into press releases hasmixed objectives that are not simply text simpliﬁca-tion.
we include this task to test the generalizabilityof our explainable pipeline (over various deﬁni-tions of simpliﬁcation).
we use alignments at titlelevel.
on average, a complex sentence in newsela,wikilarge, biendata contains 23.07, 25.14, 13.43tokens, and the corresponding simpliﬁed version isshorter, with 12.75, 18.56, 10.10 tokens..table 1: aligned complex-simple sentence pairs..datasetnewselawikilargebiendata.
training94,208 pairs208,384 pairs29,700 pairs.
validation1,129 pairs29,760 pairs4,242 pairs.
test1,077 pairs59,546 pairs8,486 pairs.
4.3.2 ground truth labels.
the original datasets contain aligned complex-simple sentence pairs instead of classiﬁcation la-bels for complexity prediction.
we infer ground-truth complexity labels for each sentence such that:label 1 is assigned to every sentence for which thereis an aligned simpler version not identical to itself(the sentence is complex and needs to be simpli-ﬁed); label 0 is assigned to all simple counterpartsof complex sentences, as well as to those sentencesthat have corresponding “simple” versions identi-cal to themselves (i.e., these sentences do not needto be simpliﬁed).
for complex sentences that havelabel 1, we further identify which tokens are notpresent in corresponding simple versions..4.3.3 model training.
for all shallow and deep classiﬁers we ﬁnd thebest hyperparameters using random search on val-idation, with early stopping.
we use grid searchon validation to ﬁne-tune hyperparameters of thepre-trained models, such as maximum sequence.
2https://www.biendata.com/competition/.
hackathon, retrieved on 5/31/2021..1091length, batch size, learning rate, and number ofepochs.
for ulmfit on newsela, we set batch sizeto 128 and learning rate to 1e-3.
for bert onwikilarge, batch size is 32, learning rate is 2e-5,and maximum sequence length is 128. for xlneton biendata, batch size is 32, learning rate is 2e-5,and maximum sequence length is 32..we use grid search on validation to ﬁne-tunethe complexity explanation models, including theextractive adversarial network.
for lr and limewe determine the maximum number of words tohighlight based on ter score on validation (pleasesee table 2); for shap we highlight all featureswith positive assigned weights, all based on ter..table 2: maximum numbers of most important lr fea-tures and features highlighted by lime..newselamodellr200 featureslime & lr10 featureslime & lstm 60 features.
wikilarge20,000 features50 features20 features.
biendata200 features10 features40 features.
for extractive adversarial networks batch sizeis set to 256, learning rate is 1e-4, and adversarialweight loss equals 1; in addition, sparsity weight is1 for newsela and biendata, and 0.6 for wikilarge;lastly, coherence weight is 0.05 for newsela, 0.012for wikilarge, and 0.0001 for biendata..5 results.
5.1 complexity prediction.
in table 3, we evaluate how well the representativeshallow, deep, and pre-trained classiﬁcation mod-els can determine whether a sentence needs to besimpliﬁed at all.
we test for statistical signiﬁcanceof the best classiﬁcation results compared to allother models using a two-tailed z-test..in general, the best performing models canachieve around 80% accuracy on two datasets(newsela and wikilarge) and a very high perfor-mance on the biendata (> 95%).
this differencepresents the difﬁculty of complexity prediction indifferent domains, and distinguishing highly spe-cialized scientiﬁc content from public facing pressreleases is relatively easy (biendata)..deep classiﬁcation models in general outper-form shallow ones, however with carefully de-signed handcrafted features and proper hyperpa-rameter optimization shallow models tend to ap-proach to the results of the deep classiﬁers.
over-all models pre-trained on large datasets and ﬁne-tuned for text simpliﬁcation yield superior classiﬁ-.
table 3: accuracy of representative shallow∗, deep,and pre-trained models for complexity prediction.
bold: best performing models..classiﬁernb n-gramsnb enriched featureslr n-gramslr enriched featuressvm n-gramssvm enriched featuresrf n-gramsrf enriched featureslstm (word-level)cnn (word-level)cnn (char-level)cnn (word & char-level)extractive adversarial networksulmfitbertxlnet.
newsela wikilarge biendata84.30 %62.70 %73.10 %86.00 %63.10 %73.10 %89.60 %71.90 %75.30 %91.70 %72.60 %76.30 %89.50 %71.90 %75.20 %88.60 %70.16 %77.39 %84.60 %71.50 %71.50 %87.00 %73.40 %74.40 %89.87 %71.62 %73.31 %89.05 %69.27 %70.71 %78.83%†88.00 %74.88 %75.9092.30 %74.00 %88.64 %71.50 %72.76 %80.83%∗∗94.17 %74.80 %81.45%∗∗94.43 %77.15 %95.48%∗∗78.83%†73.49 %.
* shallow models perform similarly and some are omitted for space;difference between the best performing model and other models is statisticallysigniﬁcant: p < 0.05 (*), p < 0.01 (**), except for †: difference betweenthis model and the best performing model is not statistically signiﬁcant..cation performance.
for newsela the best perform-ing classiﬁcation model is ulmfit (accuracy =80.83%, recall = 76.87%), which signiﬁcantly (p <0.01) surpasses all other classiﬁers except for xl-net and cnn (char-level).
on wikilarge, bertpresents the highest accuracy (81.45%, p < 0.01),and recall = 83.30%.
on biendata, xlnet yieldsthe highest accuracy (95.48%, p < 0.01) with re-call = 94.93%, although the numerical differenceto other pre-trained language models is small.
thisis consistent with recent ﬁndings in other naturallanguage processing tasks (cohan et al., 2019)..5.2 complexity explanation.
we evaluate how well complexity classiﬁcation canbe explained, or how accurately the complex partsof a sentence can be highlighted..results (table 4) show that highlighting wordsin the aoa lexicon or lr features are rather strongbaselines, indicating that most complexity of a sen-tence still comes from word usage.
highlightingmore lr features leads to a slight drop in preci-sion and a better recall.
although lstm and lrperform comparably on complexity classiﬁcation,using lime to explain lstm presents better re-call, f1, and ter (at similar precision) comparedto using lime to explain lr.
the lime & lstmcombination is reasonably strong on all datasets,as is shap & lr.
ter is a reliable indicator ofthe difﬁculty of the remainder (unhighlighted part)of the complex sentence.
ed with a substitutionpenalty of 1.5 efﬁciently captures the variationsamong the explanations.
on newsela and bien-.
1092table 4: results for complexity explanation.
p, r andf1 - the higher the better; ter and ed 1.5 - the lowerthe better.
bold & underlined: best & second best..dataset.
newsela.
wikilarge.
biendata.
explanation modelrandomaoa lexiconlr featureslime & lrlime & lstmshap & lrextractive networksrandomaoa lexiconlr featureslime & lrlime & lstmshap & lrextractive networksrandomaoa lexiconlr featureslime & lrlime & lstmshap & lrextractive networks.
p0.5150.5560.5220.5350.5430.5530.5300.4120.4270.4420.4610.8800.8420.4520.7430.7630.7960.8370.8280.8250.784.r0.4870.5500.2500.2850.8180.6040.5670.4390.4090.5250.5090.4700.5310.4290.4360.3830.2570.4660.6570.5610.773.f10.4390.5200.3210.3430.6210.5460.5180.3410.3570.4130.4150.5950.6330.3590.5040.4750.3740.5770.7130.6470.758.ter ed 1.513.8250.98512.8990.86712.1030.87112.4590.92411.9910.85212.6560.84811.4060.78117.0281.54616.7311.51617.9330.9930.98818.16225.0511.96122.8111.69316.4071.43412.9211.06513.2471.06410.8510.97910.3970.9820.95216.56811.9080.97910.6780.972.data, the extractive adversarial networks yield solidperformances (especially ter and ed 1.5), indicat-ing that jointly making predictions and generatingexplanations reinforces each other.
table 5 pro-vides examples of highlighted complex sentencesby each explanatory model..5.3 beneﬁt of complexity prediction.
one may question whether explainable predictionof text complexity is still a necessary preliminarystep in the pipeline if a strong, end-to-end simpli-ﬁcation generator is used.
we show that it is.
weconsider the scenario where a pre-trained, end-to-end text simpliﬁcation model is blindly applied totexts regardless of their complexity level, comparedto only simplifying those considered complex bythe best performing complexity predictor in ta-ble 3. such a comparison demonstrates whetheradding complexity prediction as a preliminary stepis beneﬁcial to a text simpliﬁcation process when astate-of-the-art, end-to-end simpliﬁer is already inplace.
from literature we select the current best textsimpliﬁcation models on wikilarge and newselawhich have released pre-trained models:.
• access (martin et al., 2020), a controllablesequence-to-sequence simpliﬁcation modelthat reported the highest performance (41.87sari) on wikilarge..• dynamic multi-level multi-task learningfor sentence simpliﬁcation (dmlmtl) (guoet al., 2018), which reported the highest per-formance (33.22 sari) on newsela..we apply the author-released, pre-trained ac-cess and dmlmtl on all sentences from thevalidation and testing sets of all three datasets.
wedo not use the training examples as the pre-trainedmodels may have already seen them.
presumably,a smart model should not further simplify an inputsentence if it is already simple enough.
however, toour surprise, a majority of the out-of-sample simplesentences are still changed by both models (above90% by dmlmtl and above 70% by access,please see table 6)..we further quantify the difference with vs. with-out complexity prediction as a preliminary step.
in-tuitively, without complexity prediction, an alreadysimple sentence is likely to be overly simpliﬁed andresult in a loss in text simpliﬁcation metrics.
in con-trast, an imperfect complexity predictor may mis-taken a complex sentence as simple, which missesthe opportunity of simpliﬁcation and results in aloss as well.
the empirical question is which lossis higher.
from table 7, we see that after directlyadding a complexity prediction step before eitherof the state-of-the-art simpliﬁcation models, thereis a considerable drop of errors in three text sim-pliﬁcation metrics: edit distance (ed), ter, andfr´echet embedding distance (fed) that measuresthe difference of a simpliﬁed text and the ground-truth in a semantic space (de masson d’autumeet al., 2019).
for ed alone, the improvements arebetween 30% to 50%.
this result is very encour-aging: considering that the complexity predictorsare only 80% accurate and the complexity predic-tor and the simpliﬁcation models don’t depend oneach other, there is considerable room to optimizethis gain.
indeed, the beneﬁt is higher on biendatawhere the complexity predictor is more accurate..qualitatively, one could frequently observe syn-tactic, semantic, and logical mistakes in the model-simpliﬁed version of simple sentences.
we give afew examples below..• in ethiopia, hiv disclosure is low → in.
ethiopia , hiv is low (access).
• mustafa shahbaz , 26 , was shopping forbooks about science .
→ mustafa shahbaz, 26 years old , was a group of books aboutscience .
(access).
• new biomarkers.
the diagnosis offoralzheimer’s → new biomarkers are diag-nosed with alzheimer (access).
1093table 5: explanations of complexity predictions (in red).
extractive network obtains a higher recall..complexity explanationexplanatory modeltheir fatigue changes their voices , but they ’re still on the freedom highway .
lime & lrtheir fatigue changes their voices , but they ’re still on the freedom highway .
lime & lstmtheir fatigue changes their voices , but they ’re still on the freedom highway .
shap & lrtheir fatigue changes their voices , but they ’re still on the freedom highway .
extractive networksstill , they are ﬁghting for their rights .
simple sentencedigitizing physically preserves these fragile papers and allows people to see them , he said .
lime & lrdigitizing physically preserves these fragile papers and allows people to see them , he said .
lime & lstmshap & lrdigitizing physically preserves these fragile papers and allows people to see them , he said .
extractive networks digitizing physically preserves these fragile papers and allows people to see them , he said .
simple sentence.
the papers are old and fragile , he said ..table 6: percentage of out-of-sample simple sentenceschanged by pre-trained, end-to-end simpliﬁcation mod-els.
ideal value is 0%..dataset.
newsela.
wikilarge.
biendata.
pre-trained model validation testing72.73 % 75.50 %access90.48 % 91.69 %dmlmtl70.83 % 71.12 %access95.20 % 95.61 %dmlmtl94.25 % 93.66 %access98.88 % 98.73 %dmlmtl.
to oversimplify and distort the meanings of out-of-sample input that is already simple.
evidently,the lack of transparency and explainability has lim-ited the application of these end-to-end black-boxmodels in reality, especially to out-of-sample data,context, and domains.
the pitfall can be avoidedwith the proposed pipeline and simply with explain-able complexity prediction as a preliminary step.
even though this explainable preliminary does notnecessarily reﬂect how a black-box simpliﬁcationmodel “thinks”, adding it to the model is able toyield better out-of-sample performance..table 7: out-of-sample performance of simpliﬁcationmodels.
ed, ter, fed metrics: the lower the better.
adding complexity prediction as preliminary step re-duces simpliﬁcation error by a wide margin..6 conclusions.
dataset.
sentence pairs.
no complexity prediction(simplify everything).
newsela.
with complexity prediction(predicted simple: no change).
no complexity prediction(simplify everything).
wikilarge.
with complexity prediction(predicted simple: no change).
no complexity prediction(simplify everything).
biendata.
with complexity prediction(predicted simple: no change).
metricedterfededterfededterfededterfededterfededterfed.
access4.0440.1750.0162.631 (-35%)0.089 (-49%)0.006 (-63%)5.8570.2080.0044.021 (-31%)0.132 (-37%)0.002 (-50%)3.7960.2540.0331.887 (-50%)0.114 (-55%)0.009 (-73%).
dmlmtl12.2121.6110.1708.677 (-29%)1.149 (-29%)0.066 (-61%)16.9202.3280.14310.566 (-38%)1.452 (-38%)0.049 (-66%)9.0301.3480.1315.249 (-42%)0.819 (-39%)0.051 (-61%).
• healthy diet linked to lower risk of chroniclung disease → healthy diet linked to lungdisease (dmlmtl).
• dramatic changes needed in farming practicesto keep pace with climate change → changesneeded to cause climate change (dmlmtl).
• social workers can help patients recover frommild traumatic brain injuries → social work-ers can cause better problems .
(dmlmtl).
all these qualitative and quantitative results sug-gest that the state-of-the-art black-box models tend.
we formally decompose the ambiguous notion oftext simpliﬁcation into a compact, transparent, andlogically dependent pipeline of sub-tasks, where ex-plainable prediction of text complexity is identiﬁedas the preliminary step.
we conduct a systematicanalysis of its two sub-tasks, namely complexityprediction and complexity explanation, and showthat they can be either solved separately or jointlythrough an extractive adversarial network.
whilepre-trained neural language models achieve signif-icantly better performance on complexity predic-tion, an extractive adversarial network that solvesthe two tasks jointly presents promising advantagein complexity explanation.
using complexity pre-diction as a preliminary step reduces the error ofthe state-of-the-art text simpliﬁcation models by alarge margin.
future work should integrate ratio-nale extractor into the pre-trained neural languagemodels and extend it for simpliﬁcation generation..acknowledgement.
this work is in part supported by the national sci-ence foundation under grant numbers 1633370 and1620319 and by the national library of medicineunder grant number 2r01lm010681-05..1094references.
emil abrahamsson, timothy forni, maria skeppstedt,and maria kvist.
2014. medical text simpliﬁcationusing synonym replacement: adapting assessmentof word difﬁculty to a compounding language.
inproceedings of the 3rd workshop on predicting andimproving text readability for target reader popu-lations (pitr), pages 57–65..sandra aluisio, lucia specia, caroline gasperin, andcarolina scarton.
2010. readability assessment forin proceedings of the naacltext simpliﬁcation.
hlt 2010 fifth workshop on innovative use of nlpfor building educational applications, pages 1–9..fernando alva-manchego, joachim bingel, gustavopaetzold, carolina scarton, and lucia specia.
2017.learning how to simplify from explicit labeling ofin proceedings ofcomplex-simpliﬁed text pairs.
the eighth international joint conference on natu-ral language processing (volume 1: long papers),pages 295–305..leila arras, franziska horn, gr´egoire montavon,klaus-robert m¨uller, and wojciech samek.
2017.
” what is relevant in a text document?”: an in-terpretable machine learning approach.
plos one,12(8):e0181142..laurens van den bercken, robert-jan sips, andchristoph loﬁ.
2019. evaluating neural text simpli-ﬁcation in the medical domain.
in the world wideweb conference, pages 3286–3292.
acm..samuel carton, qiaozhu mei, and paul resnick.
2018.extractive adversarial networks: high-recall expla-nations for identifying personal attacks in social me-dia posts.
in proceedings of the 2018 conference onempirical methods in natural language processing,pages 3497–3507..arman cohan, iz beltagy, daniel king, bhavana dalvi,and daniel s weld.
2019. pretrained language mod-in pro-els for sequential sentence classiﬁcation.
ceedings of the 2019 conference on empirical meth-ods in natural language processing and the 9th in-ternational joint conference on natural languageprocessing (emnlp-ijcnlp), pages 3684–3690..alexander d’amour, katherine heller, dan moldovan,ben adlam, babak alipanahi, alex beutel,christina chen, jonathan deaton, jacob eisen-stein, matthew d hoffman, et al.
2020.un-derspeciﬁcation presents challenges for credibil-arxiv preprintity in modern machine learning.
arxiv:2011.03395..louise del´eger and pierre zweigenbaum.
2009. ex-tracting lay paraphrases of specialized expressionsfrom monolingual comparable medical corpora.
inproceedings of the 2nd workshop on building andusing comparable corpora: from parallel to non-parallel corpora, pages 2–10.
association for com-putational linguistics..jacob devlin, ming-wei chang, kenton lee, andkristina toutanova.
2019. bert: pre-training ofdeep bidirectional transformers for language under-standing.
in proceedings of the 2019 conference ofthe north american chapter of the association forcomputational linguistics: human language tech-nologies, volume 1 (long and short papers), pages4171–4186..siobhan lucy devlin.
1999. simplifying natural lan-guage for aphasic readers.
ph.d. thesis, universityof sunderland..finale doshi-velez and been kim.
2018. consid-erations for evaluation and generalization in inter-in explainable and in-pretable machine learning.
terpretable models in computer vision and machinelearning, pages 3–17.
springer..noemie elhadad and komal sutaria.
2007. mininga lexicon of technical terms and lay equivalents.
in proceedings of the workshop on bionlp 2007:biological, translational, and clinical languageprocessing, pages 49–56.
association for computa-tional linguistics..han guo, ramakanth pasunuru, and mohit bansal.
2018. dynamic multi-level multi-task learning forsentence simpliﬁcation.
in proceedings of the 27thinternational conference on computational linguis-tics, pages 462–476..jeremy howard and sebastian ruder.
2018. universallanguage model ﬁne-tuning for text classiﬁcation.
inproceedings of the 56th annual meeting of the as-sociation for computational linguistics (volume 1:long papers), pages 328–339..sasikiran kandula, dorothy curtis, and qing zeng-treitler.
2010. a semantic and syntactic text simpli-ﬁcation tool for health content.
in amia annual sym-posium proceedings, volume 2010, page 366. amer-ican medical informatics association..david kauchak.
2013..improving text simpliﬁcationlanguage modeling using unsimpliﬁed text data.
inproceedings of the 51st annual meeting of the associ-ation for computational linguistics (volume 1: longpapers), pages 1537–1546..j peter kincaid, robert p fishburne jr, richard lrogers, and brad s chissom.
1975. derivation ofnew readability formulas (automated readability in-dex, fog count and ﬂesch reading ease formula) fornavy enlisted personnel..victor kuperman, hans stadthagen-gonzalez, andmarc brysbaert.
2012. age-of-acquisition ratingsfor 30,000 english words.
behavior research meth-ods, 44(4):978–990..himabindu lakkaraju, ece kamar, rich caruana, andjure leskovec.
2019. faithful and customizable ex-planations of black box models.
in proceedings ofthe 2019 aaai/acm conference on ai, ethics, andsociety, pages 131–138..1095tao lei, regina barzilay, and tommi jaakkola.
2016.rationalizing neural predictions.
in proceedings ofthe 2016 conference on empirical methods in natu-ral language processing, pages 107–117..gondy leroy, james e endicott, david kauchak, obaymouradi, and melissa just.
2013. user evaluationof the effects of a text simpliﬁcation algorithm usingterm familiarity on perception, understanding, learn-ing, and information retention.
journal of medicalinternet research, 15(7):e144..benjamin letham, cynthia rudin, tyler h mc-cormick, david madigan, et al.
2015. interpretableclassiﬁers using rules and bayesian analysis: build-ing a better stroke prediction model.
the annals ofapplied statistics, 9(3):1350–1371..vladimir i levenshtein.
1966. binary codes capableof correcting deletions, insertions, and reversals.
insoviet physics doklady, volume 10, pages 707–710..scott m lundberg and su-in lee.
2017. a uniﬁedin ad-approach to interpreting model predictions.
vances in neural information processing systems,pages 4765–4774..louis martin, ´eric villemonte de la clergerie, benoˆıtsagot, and antoine bordes.
2020. controllable sen-in proceedings of the 12thtence simpliﬁcation.
language resources and evaluation conference,pages 4689–4698..cyprien de masson d’autume, shakir mohamed, mi-haela rosca, and jack rae.
2019. training languagegans from scratch.
in advances in neural informa-tion processing systems, pages 4302–4313..elijah mayﬁeld, michael madaio, shrimai prab-humoye, david gerritsen, brittany mclaughlin,ezekiel dixon-rom´an, and alan w black.
2019.equity beyond bias in language technologies for ed-ucation.
in proceedings of the fourteenth workshopon innovative use of nlp for building educationalapplications, pages 444–460..courtney napoles, benjamin van durme, and chriscallison-burch.
2011. evaluating sentence com-in pro-pression: pitfalls and suggested remedies.
ceedings of the workshop on monolingual text-to-text generation, pages 91–97..gustavo paetzold.
2015. reliable lexical simpliﬁcationfor non-native speakers.
in proceedings of the 2015conference of the north american chapter of theassociation for computational linguistics: studentresearch workshop, pages 9–16..gustavo paetzold and lucia specia.
2016. semeval2016 task 11: complex word identiﬁcation.
in pro-ceedings of the 10th international workshop on se-mantic evaluation (semeval-2016), pages 560–569..kishore papineni, salim roukos, todd ward, and wei-jing zhu.
2002. bleu: a method for automatic eval-in proceedings ofuation of machine translation..the 40th annual meeting on association for compu-tational linguistics, pages 311–318.
association forcomputational linguistics..ellie pavlick and chris callison-burch.
2016. simpleppdb: a paraphrase database for simpliﬁcation.
inproceedings of the 54th annual meeting of the as-sociation for computational linguistics (volume 2:short papers), pages 143–148..luz rello, ricardo baeza-yates, stefan bott, andhoracio saggion.
2013. simplify or help?
:textsimpliﬁcation strategies for people with dyslexia.
in proceedings ofthe 10th international cross-disciplinary conference on web accessibility.
acm..marco tulio ribeiro, sameer singh, and carlosguestrin.
2016.
”why should i trust you?”: explain-in proceed-ing the predictions of any classiﬁer.
ings of the 22nd acm sigkdd international con-ference on knowledge discovery and data mining,san francisco, ca, usa, august 13-17, 2016, pages1135–1144..carolina scarton and lucia specia.
2018. learningsimpliﬁcations for speciﬁc target audiences.
in pro-ceedings of the 56th annual meeting of the associa-tion for computational linguistics (volume 2: shortpapers), pages 712–718..elliot schumacher, maxine eskenazi, gwen frishkoff,and kevyn collins-thompson.
2016. predicting therelative difﬁculty of single sentences with and with-out surrounding context.
in proceedings of the 2016conference on empirical methods in natural lan-guage processing, pages 1871–1881..matthew shardlow.
2013. a comparison of techniquesin 51stto automatically identify complex words.
annual meeting of the association for computa-tional linguistics proceedings of the student re-search workshop, pages 103–109..matthew shardlow.
2014. a survey of automated textinternational journal of advanced.
simpliﬁcation.
computer science and applications, 4(1):58–70..advaith siddharthan.
2006. syntactic simpliﬁcationand text cohesion.
research on language and com-putation, 4(1):77–109..matthew snover, bonnie dorr, richard schwartz, lin-nea micciulla, and john makhoul.
2006. a study oftranslation edit rate with targeted human annotation.
in proceedings of association for machine transla-tion in the americas, volume 200..sanja ˇstajner and ioana hulpus, .
2020. when shallowis good enough: automatic assessment of concep-tual text complexity using shallow semantic features.
in proceedings of the 12th language resources andevaluation conference, pages 1414–1422..1096danny ty wu, david a hanauer, qiaozhu mei, pa-tricia m clark, lawrence c an, joshua proulx,qing t zeng, vg vinod vydiswaran, kevyncollins-thompson, and kai zheng.
2016. assess-journaling the readability of clinicaltrials.
gov.
of the american medical informatics association,23(2):269–275..sander wubben, antal van den bosch, and emielkrahmer.
2012. sentence simpliﬁcation by mono-lingual machine translation.
in proceedings of the50th annual meeting of the association for compu-tational linguistics: long papers-volume 1, pages1015–1024.
association for computational linguis-tics..wei xu, chris callison-burch, and courtney napoles.
2015. problems in current text simpliﬁcation re-search: new data can help.
transactions of the asso-ciation for computational linguistics, 3:283–297..wei xu, courtney napoles, ellie pavlick, quanzechen, and chris callison-burch.
2016. optimizingstatistical machine translation for text simpliﬁcation.
transactions of the association for computationallinguistics, 4:401–415..zhilin yang, zihang dai, yiming yang, jaime car-bonell, russ r salakhutdinov, and quoc v le.
2019.xlnet: generalized autoregressive pretraining forlanguage understanding.
advances in neural infor-mation processing systems, 32:5753–5763..xingxing zhang and mirella lapata.
2017. sentencesimpliﬁcation with deep reinforcement learning.
inproceedings of the 2017 conference on empiricalmethods in natural language processing, pages584–594..1097