a neural transition-based joint model for disease named entityrecognition and normalization.
zongcheng ji1, tian xia1, mei han1, and jing xiao2.
1paii inc., palo alto, ca, usa2ping an technology, shenzhen, china1{jizongcheng, summerrainet2008, hanmei613}@gmail.com2xiaojing039@pingan.com.cn.
abstract.
disease is one of the fundamental entities inbiomedical research.
recognizing such en-tities from biomedical text and then normal-izing them to a standardized disease vocabu-lary offer a tremendous opportunity for manydownstream applications.
previous studieshave demonstrated that joint modeling of thetwo sub-tasks has superior performance thanthe pipelined counterpart.
although the neu-ral joint model based on multi-task learningframework has achieved state-of-the-art perfor-mance, it suffers from the boundary inconsis-tency problem due to the separate decodingprocedures.
moreover, it ignores the rich in-formation (e.g., the text surface form) of eachcandidate concept in the vocabulary, which isquite essential for entity normalization.
in thiswork, we propose a neural transition-basedjoint model to alleviate these two issues.
wetransform the end-to-end disease recognitionand normalization task as an action sequenceprediction task, which not only jointly learnsthe model with shared representations of theinput, but also jointly searches the output bystate transitions in one search space.
more-over, we introduce attention mechanisms totake advantage of the text surface form of eachcandidate concept for better normalization per-formance.
experimental results conducted ontwo publicly available datasets show the effec-tiveness of the proposed method..1.introduction.
disease is one of the fundamental entities inbiomedical research, thus it is one of the mostsearched topics in the biomedical literature (do-gan et al., 2009) and the internet (brownsteinet al., 2009).
automatically identifying diseasesmentioned in a text (e.g., a pubmed article or ahealth webpage) and then normalizing these identi-ﬁed mentions to their mapping concepts in a stan-dardized disease vocabulary (e.g., with primary.
name, synonyms and deﬁnition, etc.)
offers atremendous opportunity for many downstream ap-plications, such as mining chemical-disease rela-tions from the literature (wei et al., 2015), andproviding much more relevant resources basedon the search queries (dogan et al., 2014), etc.
examples of such disease vocabularies includesmesh (http://www.nlm.nih.gov/mesh/) and omim(http://www.ncbi.nlm.nih.gov/omim)..previous studies (leaman and lu, 2016; louet al., 2017; zhao et al., 2019) show the effective-ness of the joint methods for the end-to-end diseaserecognition and normalization (aka linking) taskto alleviated the error propagation problem of thetraditional pipelined solutions (strubell et al., 2017;leaman et al., 2013; xu et al., 2016, 2017).
al-though taggerone (leaman and lu, 2016) and thediscrete transition-based joint model (lou et al.,2017) successfully alleviate the error propaga-tion problem, they heavily rely on hand-craft fea-ture engineering.
recently, zhao et al.
(zhaoet al., 2019) proposes a neural joint model basedon the multi-task learning framework (i.e., mtl-feedback) which signiﬁcantly outperforms previ-ous discrete joint solutions.
mtl-feedback jointlyshares the representations of the two sub-tasks (i.e.,joint learning with shared representations of the in-put), however, their method suffers from the bound-ary inconsistency problem due to the separate de-coding procedures (i.e., separate search in two dif-ferent search spaces).
moreover, it ignores the richinformation (e.g., the text surface form) of eachcandidate concept in the vocabulary, which is quiteessential for entity normalization..in this work, we propose a novel neuraltransition-based joint model named neujorn fordisease named entity recognition and normaliza-tion, to alleviate these two issues of the multi-tasklearning based solution (zhao et al., 2019).
wetransform the end-to-end disease recognition and.
proceedingsofthe59thannualmeetingoftheassociationforcomputationallinguisticsandthe11thinternationaljointconferenceonnaturallanguageprocessing,pages2819–2827august1–6,2021.©2021associationforcomputationallinguistics2819normalization task as an action sequence predictiontask.
more speciﬁcally, we introduce four typesof actions (i.e., out, shift, reduce, seg-ment) for the recognition purpose and one typeof action (i.e., linking) for the normalizationpurpose.
our joint model not only jointly learnsthe model with shared representations, but alsojointly searches the output by state transitions inone search space.
moreover, we introduce attentionmechanisms to take advantage of text surface formof each candidate concept for better linking actionprediction..we summarize our contributions as follows..• we propose a novel neural transition-basedjoint model, neujorn, for disease named en-tity recognition and normalization, which notonly jointly learns the model with shared rep-resentations, but also jointly searches the out-put by state transitions in one search space..• we introduce attention mechanisms to take ad-vantage of text surface form of each candidateconcept for normalization performance..• we evaluate our proposed model on two pub-lic datasets, namely the ncbi and bc5cdrdatasets.
extensive experiments show the ef-fectiveness of the proposed model..2 task deﬁnition.
we deﬁne the end-to-end disease recognition andnormalization task as follows.
given a sentencex from a document d (e.g., a pubmed abstract)and a controlled vocabulary kb (e.g., mesh andomim) which consists of a set of disease con-cepts, the task of end-to-end disease recognitionand normalization is to identify all disease men-tions m = {m1, m2, ..., m|m |} mentioned in xand to link each of the identiﬁed disease mentionmi with its mapping concept ci in kb, mi → ci.
if there is no mapping concept in kb for mi, thenmi → n il, where n il denotes that mi is un-linkable..3 neural transition-based joint model.
we ﬁrst introduce the transition system used in themodel, and then introduce the neural transition-based joint model for this task..3.1 transition system.
we propose a novel transition system, inspiredby the arc-eager transition-based shift-reduce.
table 1: deﬁned transition actions used in the proposedmodel.
we use the subscript i ∈ {0, 1, ...} to denote theitem index in stack σ and buffer β, starting from rightand left, respectively..actions.
change of state.
out.
shift.
reduce.
segment-t.linking-c.(σ|σ0,β0|β,o)(σ|σ0,β(cid:48),o).
(σ|σ1|σ0,β0|β,o)(σ|σ0|β0,β(cid:48),o).
(σ|σ1|σ0,β0|β,o)(σ|σ1σ0,β0|β,o).
(σ|σ0,β0|β,o)(σ(cid:48),β0|β,o∪σt0)(σ|σ0,β0|β,o|σt0)(σ|σ0,β0|β,o|σt,c0 ).
parser (watanabe and sumita, 2015; lample et al.,2016), which constructs the output of each givensentence x and controlled vocabulary kb throughstate transitions with a sequence of actions a..we deﬁne a state as a tuple (σ, β, o), which.
consists of the following three structures:.
• stack (σ): the stack is used to store tokens.
being processed..be processed..• buffer (β): the buffer is used to store tokens to.
• output (o): the output is used to store the.
recognized and normalize mentions..we deﬁne a start state with the stack σ and theoutput o being both empty, and the buffer β con-taining all the tokens of a given sentence x. simi-larly, we deﬁne an end state with the stack σ andbuffer β being both empty, and the output o sav-ing the recognized and normalized entity mention.
the transition system begins with a start state andends with an end state.
the state transitions are ac-complished by a set of transition actions a, whichconsume the tokens in β and build the output ostep by step..as shown in table 1, we deﬁne 5 types of tran-sition actions for state transitions, and their logicsare summarized as follows:.
• out pops the ﬁrst token β0 from the buffer,which indicates that this token does not belongto any entity mention..• shift moves the ﬁrst token β0 from thebuffer to the stack, which indicates that thistoken is part of an entity mention..2820table 2: an example of state transitions for the recognition and normalization of disease mentions given a sentence“most colon cancers arise from mutations” and a controlled vocabulary mesh.
state 0 and 9 are the start state andend state, respectively, and φ denotes empty..state actions a.
0123456789.outshiftshiftreducesegment-diseaselinking-d003110outoutout.
stack σφφcoloncolon | cancerscolon cancersφφφφφ.buffer βmost colon cancers arise from mutationscolon cancers arise from mutationscancers arise from mutationsarise from mutationsarise from mutationsarise from mutationsarise from mutationsfrom mutationsmutationsφ.output oφφφφφcolon cancersdiseasecolon cancersdisease,d003110colon cancersdisease,d003110colon cancersdisease,d003110colon cancersdisease,d003110.
• reduce pops the top two tokens (or spans)σ0 and σ1 from the stack and concatenatesthem as a new span, which is then pushedback to the stack..• segment-t pops the top token (or span) σ0from the stack and creates a new entity men-tion σt0 with entity type t, which is then addedto the output..• linking-c links the previous recognized butunnormalized mention σt0 in the output withits mapping concept with id c and updates themention with σt,c0 ..table 2 shows an example of state transitionsfor the recognition and normalization of diseasementions given a sentence “most colon cancersarise from mutations” and a controlled vocabularymesh.
state 0 is the start state where φ denotesthat the stack σ and output o are initially empty,and the buffer β is initialized with all the tokensof the given sentence.
state 9 is the end statewhere φ denotes that the stack σ and buffer β areﬁnally empty, and colon cancersdisease,d003110 inthe output o denote that the mention “colon can-cers” is a disease mention and is normalized to theconcept with id d003110 in mesh.
more speciﬁ-cally, state 5 creates a new disease mention coloncancersdisease and add it to the output.
state 6 linksthe previous recognized but unnormalized diseasemention in the output with its mapping conceptwith id d003110 in mesh..3.2 action sequence prediction.
based on the introduced transition system, the end-to-end disease recognition and normalization taskbecomes a new sequence to sequence task, i.e.,the action sequence prediction task.
the input is.
a sequence of words xn1 = (w1, w2, ..., wn) anda controlled vocabulary kb, and the output is asequence of actions am1 = (a1, a2, ..., am).
thegoal of the task is to ﬁnd the most probable outputaction sequence a∗ given the input word sequencexn1 and kb, that is.
a∗ = arg max.
p(am.
1 |xn.
1 , kb).
(1).
a.formally, at each step t, the model predicts thenext action based on the current state st and theaction history at−1.
.
thus, the task is models as.
1.
(cid:89).
t.(a∗, s∗) = argmaxa,s.
p(at, st+1|at−1.
, st).
1.
(2)where at is the generated action at step t, and st+1is the new state according to at..let rt denote the representation for computing.
the probability of the action at at step t, thus.
p(at|rt) =.
exp(wσa(cid:48)∈a(st)exp(w.(cid:124)atrt + bat)(cid:124)a(cid:48)rt + ba(cid:48)).
(3).
where wa and ba denote the learnable parametervector and bias term, respectively, and a(st) de-notes the next possible valid actions that may betaken given the current state st..finally, the overall optimization function of theaction sequence prediction task can be written as.
(a∗, s∗) = argmaxa,s.
p(at, st+1|at−1.
, st).
1.
= argmaxa,s.
p(at|rt).
(cid:89).
t(cid:89).
t.(4).
28213.3 dense representations.
we now introduce neural networks to learn thedense representations of an input sentence x andeach state in the whole transition process to predictthe next action..input representation we represent each word xiin a sentence x by concatenating its character-levelword representation, non-contextual word represen-tation, and contextual word representation:.
state representations for predicting the actions indifferent purposes..speciﬁcally, for predicting the actions in the.
recognition purpose, we represent the state as.
rn ert.= relu(w [s1.
t ; s0.
t ; b0.
t ; a−1t.] + d).
(8).
where relu is an activation function, w and d de-note the learnable parameter matrix and bias term,respectively, and.
xi = [vchar.
; vw.
i.i ; elmoi].
(5).
i.where vchardenotes its character-level word repre-sentation learned by using a cnn network (ma andhovy, 2016), vwi denotes its non-contextual wordrepresentation initialized with glove (penningtonet al., 2014) embeddings, which is pre-trained on6 billion words from wikipedia and web text, andelmoi denotes its contextual word representationinitialized with elmo (peters et al., 2018).
wecan also explore the contextual word representationfrom bert (devlin et al., 2018) by averaging theembeddings of the subwords of each word.
weleave it to the future work..we then run a bilstm (graves et al., 2013) toderive the contextual representation of each wordin the sentence x..state representation at each step t in the tran-sition process, let’s consider the representationof the current state st = (σt, βt, at), whereσt = (..., σ1, σ0), βt = (β0, β1, ...) and at =(at−1, at−2, ...)..the buffer βt.
represented with bil-stm (graves et al., 2013) to represent the wordsin the buffer:.
is.
bt = bilstm([β0, β1, ...]).
(6).
the stack σt and the actions at are represented.
with stacklstm (dyer et al., 2015):.
st = stacklstm([..., σ1, σ0])at = stacklstm([at−1, at−2, ...]).
(7).
we classify all the actions deﬁned in table 1 intotwo categories corresponding to two different pur-poses, i.e., the recognition and normalization pur-poses.
out, shift, reduce, segment-t areused for the recognition purpose, and linking-cis used for the normalization purpose.
as shownin figure 1(a) and 1(b), we deﬁne two different.
• s0.
t and s1tations of the stack σ..t denote the ﬁrst and second represen-.
• b0t denotes the ﬁrst representation of the bufferβ..• a−1.
t denotes the last representation of the ac-tion history a..for predicting the actions in the normalization.
purpose, we represent the state as.
m; r(cid:48).
rn ormt.= relu(w [l(cid:48).
m; m(cid:48); c(cid:48); c; a−1.]
+ d)(9)where relu is an activation function, w and d de-note the learnable parameter matrix and bias term,respectively, and.
t.• l(cid:48).
m and r(cid:48)m denotes the left-side and right-sidecontext representations by (i) ﬁrst applyingattention with the concept representation c tohighlight the relevant parts in mentions’ localcontext, and (ii) then applying max-poolingoperation to aggregate the reweighted repre-sentations of all the context words..• m(cid:48) and c(cid:48) are the representations of the men-tion and candidate concept by applying coat-tention mechanism (tay et al., 2018; jia et al.,2020)..• c denotes the candidate concept representationby (i) ﬁrst run a bilstm (graves et al., 2013)to derive the contextual representation of eachword in the candidate concept, and (ii) thenapplying max-pooling operation to aggregatethe representations of all concept words..• a−1.
t denotes the last representation of the ac-tion history a..2822(a) state representation for predicting recogni-tion related actions, i.e., shift, out, reduce,segment-t..(b) state representation for predicting normalizationrelated actions, i.e., linking-c..figure 1: state representations for predicting actions in different purposes (i.e., recognition and normalization)..3.4 search and training.
table 3: overall statistics of the datasets..decoding is the key step in both training and test,which is to search for the best output structure (i.e.,action sequence) under the current model param-eters.
in this work, we use two different searchstrategies with different optimizations..greedy search for efﬁcient decoding, a widely-used greedy search algorithm (wang et al., 2017)can be adopted to minimize the negative log-likelihood of the local action classiﬁer in equa-tion (3, 8, 9)..beam search the main drawback of greedy searchis error propagation (wang et al., 2017).
an incor-rect action will fail the following actions, leadingto an incorrect output sequence.
one solution toalleviate this problem is to apply beam search.
inthis work, we use the beam-search optimization(bso) method with laso update (wiseman andrush, 2016) to train our beam-search model, wherethe max-margin loss is adopted..4 experiments.
4.1 datasets.
we use two public available datasets in this study,namely ncbi - the ncbi disease corpus (doganet al., 2014) and bc5cdr - the biocreative vcdr task corpus (li et al., 2016b).
ncbi datasetcontains 792 pubmed abstracts, which was splitinto 692 abstracts for training and development,and 100 abstracts for testing.
a disorder mentionin each pubmed abstract was manually annotatedwith its mapping concept identiﬁer in the medic.
corpusncbibc5cdr.
#documents7921,500.
#mentions6,88112,852.
#concepts1,0495,818.lexicon.
bc5cdr dataset contains 1,500 pubmedabstracts, which was equally split into three partsfor training, development and test, respectively.
adisease mention in each abstract is manually anno-tated with the concept identiﬁer to which it refersto a controlled vocabulary.
in this study, we use thejuly 6, 2012 version of medic, which contains7,827 mesh identiﬁers and 4,004 omim identi-ﬁers, grouped into 9,664 disease concepts.
table3show the overall statistics of the two datasets..to facilitate the generation of candidate linkingactions, we perform some preprocessing steps ofeach candidate mention and each concept in kbwith the following strategies: (i) spelling correc-tion - for each candidate mention in the datasets,we replace all the misspelled words using a spellingcheck list as in previous work (d’souza and ng,2015; li et al., 2017).
(ii) abbreviation resolu-tion - we use ab3p (sohn et al., 2008) toolkit todetect and replace the abbreviations with their longforms within each document and also expand allpossible abbreviated disease mentions using a dic-tionary collected from wikipedia as in previouswork (d’souza and ng, 2015; li et al., 2017).
(iii)numeric synonyms resolutions - we replace all thenumerical words in the mentions and concepts totheir corresponding arabic numerals as in previouswork (d’souza and ng, 2015; li et al., 2017)..
2823…!"!#$%&#$%&'("(#………)%*+,=relu(34%#;4%";6%";7%&#+9)outshiftstack(buffer!actionsk…maxpoolingmaxpooling$%&#$%&'……)%*;,<=relu(3=>?;)>?;@?;a?
;a;7%&#+9)nillinking-d003110b#b'bcbdbemostcoloncancersarisefrommentionleft-sidecontextright-sidecontextlstm=>?
)>?actionskmaxpodingb#fb'fcolonneoplasmconceptacoattention@′a′table 4: architecture hyper-parameters..architecture hyper-parametersword embedding sizecharacter embedding sizeelmo embedding sizeaction embedding sizelstm cell sizelstm layersdropout ratelearning rateadamw weight decaysearch top k.1001610242020020.20.0010.0000110.we generate candidate linking actions (i.e., can-didate concepts) for each mention with the com-monly used information retrieval based method,which includes the following two steps.
we ﬁrstindex all the concept names and training mentionswith their concept ids.
then, the widely-usedbm25 model provided by lucene is employed toretrieve the top 10 candidate concepts {ci}10i=1 foreach mention m..4.2 evaluation metrics and settings.
following previous work (leaman and lu, 2016;lou et al., 2017; zhao et al., 2019), we utilizethe evaluation kit1 for evaluating the model perfor-mances.
we report f1 score for the recognitiontask at the mention level, and f1 score for the nor-malization task at the abstract level..we use the adamw optimizer (loshchilov andhutter, 2019) for parameter optimization.
most ofthe model hyper-parameters are listed in table 4.since increasing the beam size will increase thedecoding time, we only report results with beamsize 1, 2, and 4..shows the performance of different joint modelsfor the task.
taggerone (leaman et al., 2013) is ajoint solution based on semi-crf.
transition-basedmodel (lou et al., 2017) is a joint solution basedon discrete transition-based method.
both of thesetwo models rely heavily on feature engineering.
mtl-feedback (zhao et al., 2019) is neural jointsolution based on multi-task learning.
neujornis our neural transition-based joint model for thewhole task..from the comparisons, we ﬁnd that (1) idcnndoes not perform well enough although it relies fewefforts of feature engineering.
(2) all the joint mod-els signiﬁcantly outperform the pipelined methods.
(3) the deep-learning based joint models signiﬁ-cantly outperform the traditional machine learningbased methods.
(4) our proposed neujorn outper-forms mtl-feedback by at least 0.57% and 0.59%on the recognition and normalization tasks, respec-tively..4.3.2 effectiveness of different search.
strategies.
table 6 shows the comparisons of different searchstrategies of our proposed neujorn.
from the re-sults, we ﬁnd that (1) the methods based on beamsearch strategies outperforms the greedy searchstrategy, which indicates that the beam search solu-tions could alleviate the error propagation problemof the greedy search solution.
(2) the model withbeam size 4 achieves the best performance.
thelarger the beam size, the better the performance,however the lower the decoding speed.
(3) ourgreedy search based solution doesn’t outperformthe mlt-feedback method..4.3 results and discussion.
4.3.3 effectiveness of attention mechanisms.
4.3.1 main results.
table 5 shows the overall comparisons of differ-ent models for the end-to-end disease named entityrecognition and normalization task.
the ﬁrst partshows the performance of different pipelined meth-ods for the task.
dnorm (leaman et al., 2013)is a traditional method, which needs feature engi-neering.
idcnn (strubell et al., 2017) is a neu-ral model based on bilstm-crf, which requiresfew effort of feature engineering.
the second part.
1http://www.biocreative.org/tasks/biocreative-v/track-3-.
cdr.
table 7 shows the effectiveness of the proposedattention mechanisms.
when we remove the atten-tion mechanism for representing the left-side andright-side local context, the performance droppeda little bit.
however, when we remove the coat-tention mechanism, which is used for directly mod-eling the matching between the mention and can-didate concept, the performance dropped signiﬁ-cantly.
this group of comparisons indicates thatimportance of the matching between the mentionand candidate concept for the entity normalizationtask..2824table 5: overall comparisons of different models for disease named entity recognition and normalization..method.
dnorm (leaman et al., 2013)idcnn (strubell et al., 2017)taggerone (leaman et al., 2013)transition-based model (lou et al., 2017)mtl-feedback (zhao et al., 2019)neujorn (ours).
ncbi.
bc5cdr.
recognition normalization recognition normalization0.80640.81070.83700.85620.89170.8986.
-0.80110.82600.83820.87620.8819.
0.78200.74250.80700.82620.88230.8882.
0.79800.79830.82900.82050.87430.8857.table 6: performance comparisons of different search strategies..method.
greedy (b1)beam (b1)beam (b2)beam (b4).
ncbi.
bc5cdr.
recognition normalization recognition normalization0.88660.89100.89490.8986.
0.86820.87340.87790.8857.
0.87350.87650.87940.8819.
0.87920.88180.88430.8882.
5 related work.
disease named entity recognition dner hasbeen widely studied in the literature.
most previousstudies (leaman et al., 2013; xu et al., 2015, 2016)transform this task as a sequence labeling task, andconditional random ﬁelds (crf) based methodsare widely adopted to achieve good performance.
however, these methods heavily rely on hand-craftfeature engineering.
recently, neural models suchas bilstm-crf based methods (strubell et al.,2017; wang et al., 2019) and bert-based meth-ods (kim et al., 2019) have achieved state-of-the-art performance..disease named entity normalization dnenhas also been widely studied in the literature.
moststudies assume that the entity mentions are pre-detected by a separate dner model, and focus ondeveloping methods to improve the normaliationaccuracy (lou et al., 2017), resulting in developingrule-based methods (d’souza and ng, 2015), ma-chine learning-based methods (leaman et al., 2013;xu et al., 2017), and recent deep learning-basedmethods (li et al., 2017; ji et al., 2020; wang et al.,2020; vashishth et al., 2021; chen et al., 2021).
however, the pipeline architecture which performsdner and dnen separately suffers from the errorpropagation problem.
in this work, we propose aneural joint model to alleviate this issue..joint dner and dnen several studies (leamanand lu, 2016; lou et al., 2017; zhao et al., 2019)show the effectiveness of the joint methods to al-leviated the error propagation problem.
although.
taggerone (leaman and lu, 2016) and the discretetransition-based joint model (lou et al., 2017) suc-cessfully alleviated the error propagation problem,they heavily rely on hand-craft feature engineer-ing.
recently, zhao et al.
(zhao et al., 2019) pro-pose a neural joint model based on the multi-tasklearning framework (i.e., mtl-feedback) whichsigniﬁcantly outperforms previous discrete jointsolutions.
however, their method suffers from theboundary inconsistency problem due to the sepa-rate decoding procedures (i.e., separate search intwo different search spaces).
moreover, it ignoresthe rich information (e.g., the text surface form) ofeach candidate concept in the vocabulary, whichis quite essential for entity normalization.
in thiswork, we propose a neural joint model to alleviatethese two issues..transition-based models transition-based mod-els are widely used in parsing and transla-tion (watanabe and sumita, 2015; wang et al.,2018; meng and zhang, 2019).
recently, thesemodels are successfully applied to information ex-traction tasks, such as joint pos tagging and depen-dency parsing (yang et al., 2018), joint entity andrelation extraction (li and ji, 2014; li et al., 2016a;ji et al., 2021).
several studies propose discretetransition-based joint model for entity recognitionand normalization(qian et al., 2015; ji et al., 2016;lou et al., 2017).
in this work, we propose a neu-ral transition-based joint model for disease namedentity recognition and normalization..2825table 7: performance comparisons of different attention mechanisms..method.
beam (b4)-attention-coattention.
ncbi.
bc5cdr.
recognition normalization recognition normalization0.89860.89640.8853.
0.88570.88270.8673.
0.88190.88030.8729.
0.88820.88680.8779.
6 conclusions.
in this work, we proposed a novel neural transition-based joint model for disease named entity recogni-tion and normalization.
experimental results con-ducted on two public available datasets show theeffectiveness of the proposed method.
in the future,we will apply this joint model to more differenttypes of datasets, such as the clinical notes, druglabels, and tweets, etc..references.
john s brownstein, clark c freifeld,.
andlawrence c madoff.
2009. digital disease de-tection—harnessing the web for public healthsurveillance.
new england journal of medicine,360(21):2153–2157..lihu chen, ga¨el varoquaux, and fabian m suchanek.
2021. a lightweight neural model for biomedicalentity linking.
aaai..jacob devlin, ming-wei chang, kenton lee, andkristina toutanova.
2018. bert: pre-training ofdeep bidirectional transformers for language under-standing.
arxiv preprint arxiv:1810.04805..rezarta islamaj dogan, robert leaman, and zhiyonglu.
2014. ncbi disease corpus: a resource for dis-ease name recognition and concept normalization.
jbi, 47:1–10..rezarta islamaj dogan, g craig murray, aur´elien´ev´eol, and zhiyong lu.
2009. understandingpubmed® user search behavior through log analysis.
database, 2009:bap018..jennifer d’souza and vincent ng.
2015. sieve-basedentity linking for the biomedical domain.
in acl,pages 297–302..chris dyer, miguel ballesteros, wang ling, austinmatthews, and noah a smith.
2015. transition-based dependency parsing with stack long short-in acl-ijcnlp, pages 334–343,term memory.
beijing, china.
association for computational lin-guistics..zongcheng ji, omid ghiasvand, stephen wu, and huaxu.
2021. a discrete joint model for entity andrelation extraction from clinical notes.
in amia2021 informatics summit..zongcheng ji, aixin sun, gao cong, and jialonghan.
2016. joint recognition and linking of fine-in www, pagesgrained locations from tweets.
1271–1281..zongcheng ji, qiang wei, and hua xu.
2020. bert-based ranking for biomedical entity normalization.
in amia 2020 informatics summit, pages 269–277..ningning jia, xiang cheng, sen su, and liyuan ding.
2020. cogcn: combining co-attention with graphconvolutional network for entity linking with knowl-edge graphs.
expert systems, page e12606..donghyeon kim, jinhyuk lee, chan ho so, hwisangjeon, minbyul jeong, yonghwa choi, wonjin yoon,mujeen sung, and jaewoo kang.
2019. a neuralnamed entity recognition and multi-type normal-ization tool for biomedical text mining.
ieee ac-cess, 7:73729–73740..guillaume lample, miguel ballesteros, sandeep sub-ramanian, kazuya kawakami, and chris dyer.
2016.neural architectures for named entity recognition.
in naacl, pages 260–270, san diego, california.
association for computational linguistics..robert leaman, rezarta islamaj dogan, and zhiyonglu.
2013. dnorm: disease name normalization withpairwise learning to rank.
bioinformatics, 29:2909–2917..robert leaman and zhiyong lu.
2016..tag-gerone: joint named entity recognition and normal-ization with semi-markov models.
bioinformatics,32(18):2839–2846..fei li, yue zhang, meishan zhang, and donghongji.
2016a.
joint models for extracting adverse drugevents from biomedical text.
in ijcai, pages 2838–2844..haodi li, qingcai chen, buzhou tang, xiaolongwang, hua xu, baohua wang, and dong huang.
2017. cnn-based ranking for biomedical entity nor-malization.
bmc bioinformatics, 18(11):385..alex graves, abdel-rahman mohamed, and geoffreyhinton.
2013. speech recognition with deep recur-rent neural networks.
in ieee icassp, pages 6645–6649..jiao li, yueping sun, robin j johnson, daniela sciaky,chih-hsuan wei, robert leaman, allan peter davis,carolyn j mattingly, thomas c wiegers, and zhiy-ong lu.
2016b.
biocreative v cdr task corpus:.
2826a resource for chemical disease relation extraction.
database, 2016:baw068..qi li and heng ji.
2014. incremental joint extractionin acl, pages.
of entity mentions and relations.
402–412..ilya loshchilov and frank hutter.
2019. decoupledin iclr (poster)..weight decay regularization.
openreview.net..yinxia lou, yue zhang, tao qian, fei li, shufengxiong, and donghong ji.
2017. a transition-basedjoint model for disease named entity recognitionand normalization.
bioinformatics..xuezhe ma and eduard hovy.
2016. end-to-end se-quence labeling via bi-directional lstm-cnns-crf.
in acl, pages 1064–1074, berlin, germany.
association for computational linguistics..fandong meng and jinchao zhang.
2019. dtmt: anovel deep transition architecture for neural ma-chine translation.
in aaai..jeffrey pennington, richard socher, and christophermanning.
2014. glove: global vectors for word rep-resentation.
in emnlp, pages 1532–1543..matthew e peters, mark neumann, mohit iyyer, mattgardner, christopher clark, kenton lee, and lukezettlemoyer.
2018. deep contextualized word rep-resentations.
in naacl-hlt, pages 2227–2237..tao qian, yue zhang, meishan zhang, yafeng ren,and dong-hong ji.
2015. a transition-based modelfor joint segmentation, pos-tagging and normaliza-tion.
in emnlp, pages 1837–1846..sunghwan sohn, donald c comeau, won kim, andw john wilbur.
2008. abbreviation deﬁnition iden-tiﬁcation based on automatic precision estimates.
bmc bioinformatics, 9..emma strubell, patrick verga, david belanger, and an-drew mccallum.
2017. fast and accurate entityrecognition with iterated dilated convolutions.
inemnlp, pages 2670–2680.
association for compu-tational linguistics..yi tay, anh tuan luu, and siu cheung hui.
2018. her-mitian co-attention networks for text matchingin ijcai, pages 4425–in asymmetrical domains.
4431. ijcai.org..shikhar vashishth, denis newman-grifﬁs, rishabhjoshi, ritam dutt, and carolyn rose.
2021. improv-ing broad-coverage medical entity linking withsemantic type prediction and large-scale datasets.
arxiv preprint arxiv:2005.00460..qiong wang, zongcheng ji, jingqi wang, stephen wu,weiyan lin, wenzhen li, li ke, guohong xiao,qing jiang, hua xu, and others.
2020. a studyof entity-linking methods for normalizing chinesediagnosis and procedure terms to icd codes.
jbi,page 103418..shaolei wang, wanxiang che, yue zhang, meishanzhang, and ting liu.
2017. transition-based dis-ﬂuency detection using lstms.
in emnlp, pages2785–2794, copenhagen, denmark.
association forcomputational linguistics..xuan wang, yu zhang, xiang ren, yuhao zhang,marinka zitnik, jingbo shang, curtis langlotz, andjiawei han.
2019. cross-type biomedical named en-tity recognition with deep multi-task learning.
bioin-formatics, 35(10):1745–1752..yuxuan wang, wanxiang che, jiang guo, and ting liu.
2018. a neural transition-based approach for se-mantic dependency graph parsing.
in aaai..taro watanabe and eiichiro sumita.
2015. transition-in acl, pagesbased neural constituent parsing.
1169–1179, beijing, china.
association for compu-tational linguistics..chih-hsuan wei, yifan peng, robert leaman, al-lan peter davis, carolyn j mattingly, jiao li,thomas c wiegers,and zhiyong lu.
2015.overview of the biocreative v chemical diseasein proceedings of the ﬁfthrelation (cdr) task.
biocreative challenge evaluation workshop, pages154–166..sam wiseman and alexander m rush.
2016.sequence-to-sequence learning as beam-searchin emnlp, pages 1296–1306,optimization.
austin, texas.
association for computationallinguistics..jun xu, hee-jin lee, zongcheng ji, jingqi wang,qiang wei, and hua xu.
2017. uth ccb systemfor adverse drug reaction extraction from druglabels at tac-adr 2017. in proceedings of textanalysis conference..jun xu, yonghui wu, yaoyun zhang, jingqi wang,hee-jin lee, and hua xu.
2016. cd-rest: a sys-tem for extracting chemical-induced disease relationin literature.
database..jun xu, yaoyun zhang, jingqi wang, yonghui wu,min jiang, ergin soysal, and hua xu.
2015. uth-ccb: the participation of the semeval 2015 chal-lenge - task 14. in semeval, pages 311–314..liner yang, meishan zhang, yang liu, maosongsun, nan yu, and guohong fu.
2018. joint postagging and dependence parsing with transition-based neural networks.
taslp, 26(8):1352–1358..sendong zhao, ting liu, sicheng zhao, and fei wang.
2019. a neural multi-task learning framework tojointly model medical named entity recognitionand normalization.
in aaai, pages 817–824..2827