dynamic contextualized word embeddings.
valentin hofmann*‡, janet b. pierrehumbert†*, hinrich sch ¨utze‡.
*faculty of linguistics, university of oxford†department of engineering science, university of oxford‡center for information and language processing, lmu munichvalentin.hofmann@ling-phil.ox.ac.uk.
abstract.
static word embeddings that represent wordsby a single vector cannot capture the vari-ability of word meaning in different linguisticand extralinguistic contexts.
building on priorwork on contextualized and dynamic word em-beddings, we introduce dynamic contextual-ized word embeddings that represent wordsas a function of both linguistic and extralin-guistic context.
based on a pretrained lan-guage model (plm), dynamic contextualizedword embeddings model time and social spacejointly, which makes them attractive for arange of nlp tasks involving semantic vari-ability.
we highlight potential application sce-narios by means of qualitative and quantitativeanalyses on four english datasets..1.introduction.
over the last decade, word embeddings have rev-olutionized the ﬁeld of nlp.
traditional methodssuch as lsa (deerwester et al., 1990), word2vec(mikolov et al., 2013a,b), glove (pennington et al.,2014), and fasttext (bojanowski et al., 2017) com-pute static word embeddings, i.e., they representwords as a single vector.
from a theoretical stand-point, this way of modeling lexical semantics isproblematic since it ignores the variability of wordmeaning in different linguistic contexts (e.g., poly-semy) as well as different extralinguistic contexts(e.g., temporal and social variation)..the ﬁrst shortcoming was addressed by the in-troduction of contextualized word embeddings thatrepresent words as vectors varying across linguis-tic contexts.
this allows them to capture morecomplex characteristics of word meaning, includ-ing polysemy.
contextualized word embeddingsare widely used in nlp, constituting the semanticbackbone of pretrained language models (plms)such as elmo (peters et al., 2018a), bert (devlinet al., 2019), gpt-2 (radford et al., 2019), xlnet.
e(k)ij.
d.φ(k)ij.
˜e(k).
figure 1: dynamic contextualized word embeddings.
a static embedding ˜e(k) ( ) is mapped to a dynamicembedding e(k)( ) by a function d that takes time andijsocial space into account.
the scattered points ( ) arecontextualized versions of e(k)indi-cates semantic dynamics across time and social space.
the embeddings have 768 dimensions..ij .
variability in φ(k).
ij.
(yang et al., 2019), electra (clark et al., 2020),and t5 (raffel et al., 2020)..a concurrent line of work focused on the secondshortcoming of static word embeddings, resultingin various types of dynamic word embeddings.
dy-namic word embeddings represent words as vectorsvarying across extralinguistic contexts, in particu-lar time (e.g., rudolph and blei, 2018) and socialspace (e.g., zeng et al., 2018)..in this paper, we introduce dynamic contextual-ized word embeddings that combine the strengthsof contextualized word embeddings with the ﬂex-ibility of dynamic word embeddings.
dynamiccontextualized word embeddings mark a depar-ture from existing contextualized word embeddings(which are not dynamic) as well as existing dy-namic word embeddings (which are not contextu-alized).
furthermore, as opposed to all existingdynamic word embedding types, they representtime and social space jointly..proceedingsofthe59thannualmeetingoftheassociationforcomputationallinguisticsandthe11thinternationaljointconferenceonnaturallanguageprocessing,pages6970–6984august1–6,2021.©2021associationforcomputationallinguistics6970while our general framework for training dy-namic contextualized word embeddings is model-agnostic (figure 1), we present a version using aplm (bert) as the contextualizer, which allowsfor an easy integration within existing architec-tures.
dynamic contextualized word embeddingscan serve as an analytical tool (e.g., to track theemergence and spread of semantic changes in on-line communities) or be employed for downstreamtasks (e.g., to build temporally and socially awaretext classiﬁcation models), making them beneﬁcialfor various areas in nlp that face semantic vari-ability.
we illustrate application scenarios by per-forming exploratory experiments on english datafrom arxiv, ciao, reddit, and yelp..contributions.
we introduce dynamic contex-tualized word embeddings that represent words asa function of both linguistic and extralinguisticcontext.
based on a plm, dynamic contextual-ized word embeddings model time and social spacejointly, which makes them attractive for a range ofnlp tasks.
we showcase potential applications bymeans of qualitative and quantitative analyses.1.
2 related work.
2.1 contextualized word embeddings.
the distinction between the non-contextualizedcore meaning of a word and the senses that are real-ized in speciﬁc linguistic contexts lies at the heartof lexical-semantic scholarship (geeraerts, 2010),going back to at least paul (1880).
in nlp, this isreﬂected by contextualized word embeddings thatmap type-level representations to token-level rep-resentations as a function of the linguistic context(mccann et al., 2017).
as part of plms (peterset al., 2018a; devlin et al., 2019; radford et al.,2019; yang et al., 2019; clark et al., 2020; raffelet al., 2020), contextualized word embeddings haveled to substantial performance gains on a varietyof tasks compared to static word embeddings thatonly have type-level representations (deerwesteret al., 1990; mikolov et al., 2013a,b; penningtonet al., 2014; bojanowski et al., 2017)..since their introduction, several studies have an-alyzed the linguistic properties of contextualizedword embeddings (peters et al., 2018b; goldberg,2019; hewitt and manning, 2019; jawahar et al.,2019; lin et al., 2019; liu et al., 2019; tenneyet al., 2019; edmiston, 2020; ettinger, 2020; hof-.
1we make our code publicly available at https://.
github.com/valentinhofmann/dcwe..mann et al., 2020; rogers et al., 2020).
regardinglexical semantics, this line of research has shownthat contextualized word embeddings are morecontext-speciﬁc in the upper layers of a contextual-izer (ethayarajh, 2019; mickus et al., 2020; vuli´cet al., 2020) and represent different word sensesas separated clusters (peters et al., 2018a; coenenet al., 2019; wiedemann et al., 2019)..2.2 dynamic word embeddings.
the meaning of a word can also vary across ex-tralinguistic contexts such as time (bybee, 2015;koch, 2016) and social space (robinson, 2010,2012; geeraerts, 2018).
to capture these phenom-ena, various types of dynamic word embeddingshave been proposed: diachronic word embeddingsfor temporal semantic change (bamler and mandt,2017; rosenfeld and erk, 2018; rudolph and blei,2018; yao et al., 2018; gong et al., 2020) and per-sonalized word embeddings for social semanticvariation (zeng et al., 2017, 2018; oba et al., 2019;welch et al., 2020a,b; yao et al., 2020).
otherstudies have demonstrated that performance on adiverse set of tasks can be increased by includingtemporal (jaidka et al., 2018; lukes and søgaard,2018) and social information (amir et al., 2016;hamilton et al., 2016a; yang et al., 2016; yangand eisenstein, 2017; hazarika et al., 2018; mishraet al., 2018; del tredici et al., 2019b; li and gold-wasser, 2019; mishra et al., 2019)..the relevance of dynamic (speciﬁcally di-achronic) word embeddings is also reﬂected bythe emergence of lexical semantic change detec-tion as an established task in nlp (kutuzov et al.,2018; schlechtweg et al., 2018; tahmasebi et al.,2018; dubossarsky et al., 2019; schlechtweg et al.,2019; asgari et al., 2020; p¨omsl and lyapin, 2020;praˇz´ak et al., 2020; schlechtweg and schulte imwalde, 2020; schlechtweg et al., 2020).
besidesdynamic word embeddings, many studies on lexicalsemantic change detection use methods based onstatic word embeddings (kim et al., 2014; kulka-rni et al., 2015), e.g., the alignment of static wordembedding spaces (hamilton et al., 2016b).
how-ever, such approaches come at the cost of modelingdisadvantages (bamler and mandt, 2017)..sociolinguistics has shown that temporal and so-cial variation in language are tightly interwoven:innovations such as a new word sense in the caseof lexical semantics spread through the languagecommunity along social ties (milroy, 1980, 1992;.
6971labov, 2001; pierrehumbert, 2012).
however,most proposed dynamic word embedding typescannot capture more than one dimension of varia-tion.
recently, a few studies have taken ﬁrst stepsin this direction by using genre information withina bayesian model of semantic change (frermannand lapata, 2016; perrone et al., 2019) and includ-ing social variables in training diachronic wordembeddings (jawahar and seddah, 2019).
in addi-tion, to capture the full range of lexical-semanticvariability, dynamic word embeddings should alsobe contextualized.
crucially, while contextualizedword embeddings have been used to investigate se-mantic change (giulianelli, 2019; hu et al., 2019;giulianelli et al., 2020; kutuzov and giulianelli,2020; martinc et al., 2020a,b), the word embed-dings employed in these studies are not dynamic,i.e., they represent a word in a speciﬁc linguisticcontext by the same contextualized word embed-ding independent of extralinguistic context or areﬁt to different time periods as separate models.2.
3 model.
3.1 model overviewgiven a sequence of words x = (cid:2)x(1), .
.
.
, x(k)(cid:3)and corresponding non-contextualized embeddingse = (cid:2)e(1), .
.
.
, e(k)(cid:3), contextualizing languagemodels compute the contextualized embedding ofa particular word x(k), h(k), as a function c of itsnon-contextualized embedding, e(k), and the non-contextualized embeddings of words in the left con-text x (<k) and the right context x (>k),3e(k), e(<k), e(>k)(cid:17).
h(k) = c.(1).
(cid:16).
..crucially, while h(k) is a token-level representation,e(k) is a type-level representation and is modeledas a simple embedding look-up.
here, in order totake the variability of word meaning in different ex-tralinguistic contexts into account, we depart fromthis practice and model e(k) as a function d thatdepends not only on the identity of x(k) but also onthe social context si and the temporal context tj inwhich the sequence x occurred,.
(cid:16).
e(k)ij = d.x(k), si, tj.
(cid:17).
..(2).
2it is interesting to notice that contextualized word embed-dings so far have performed worse than non-contextualizedword embeddings on the task of lexical semantic change de-tection (kaiser et al., 2020; schlechtweg et al., 2020)..3some contextualizing language models such as gpt-2.
(radford et al., 2019) only operate on x (<k)..ltask.
bert.
˜e(1)+o(1)ij.
˜e(2)+o(2)ij.
x(1) x(2) x(3).
˜e(3)+o(3)ij.
si.
tj.
figure 2: model architecture.
words are mapped to dy-namic embeddings by the parts of the dynamic compo-), which are then contextualized by the con-nent (textualizer ().
the output of the contextualizer isused to compute the task-speciﬁc loss ltask..dynamic contextualized word embeddings arehence computed in two stages: words are ﬁrstmapped to dynamic type-level representations by dand then to contextualized token-level representa-tions by c (figures 1 and 2).
this two-stage struc-ture follows work in cognitive science and linguis-tics that indicates that extralinguistic information isprocessed before linguistic information by humanspeakers (hay et al., 2006)..since many words in the core vocabulary aresemantically stable across social and temporal con-texts, we place a gaussian prior on e(k)ij ,.
e(k)ij ∼ n.˜e(k), λ−1a i.
(cid:16).
(cid:17).
,.
(3).
where ˜e(k) denotes a non-dynamic representationof x(k).
combining equations 2 and 3, we writethe function d as.
(cid:16).
d.x(k), si, tj.
(cid:17).
= ˜e(k) + o(k)ij ,.
(4).
where o(k)ij denotes the vector offset from x(k)’snon-dynamic embedding ˜e(k), which is stableacross social and temporal contexts, to its dynamicembedding e(k)ij , which is speciﬁc to si and tj.
thedistribution of o(k)then follows a gaussian withij.
ij ∼ n (cid:0)0, λ−1o(k).
a i(cid:1) ..(5).
we enforce equation 5 by including a regulariza-tion term in the objective function (section 3.4)..69723.2 contextualizing component.
we leverage a plm for the function c, speciﬁcallybert (devlin et al., 2019).
denoting with eij thesequence of dynamic embeddings corresponding tox in si and tj, the dynamic version of equation 1becomes.
h(k)ij = bert.
ij , e(<k)e(k).
ij.
, e(>k)ij.
(cid:16).
(cid:17).
..(6).
we also use bert, speciﬁcally its pretrained in-put embeddings, to initialize the non-dynamic em-beddings ˜e(k), which are summed with the vectoroffsets o(k)ij.
(equation 4) and fed into bert..using a plm for c has the advantage of makingit easy to employ dynamic contextualized wordembeddings for downstream tasks by adding a task-speciﬁc layer on top of the plm..3.3 dynamic componentwe model the vector offset o(k)ij as a function of theword x(k), which we represent by its non-dynamicembedding ˜e(k), as well as the social context si,which we represent by a time-speciﬁc embeddingsij.
we use bert’s pretrained input embeddingsfor ˜e(k).4 we combine these representations in atime-speciﬁc feed-forward network,.
o(k)ij = ffnj.
(cid:16).
˜e(k) (cid:107) sij.
(cid:17).
,.
(7).
where (cid:107) denotes concatenation.
to compute thesocial embedding sij, we follow common practicein the computational social sciences and representthe social community as a graph g = (s, e), wheres is the set of social units si, and e is the set ofedges between them (section 4).
we use a time-speciﬁc graph attention network (gat) as proposedby veliˇckovi´c et al.
(2018) to encode g,5.
sij = gatj (˜si, g) ..(8).
we initialize ˜si with node2vec (grover andleskovec, 2016) embeddings..to model the temporal drift of the dynamic em-beddings e(k)ij , we follow previous work on dy-namic word embeddings (bamler and mandt, 2017;rudolph and blei, 2018) and impose a randomwalk prior over o(k)ij ,.
(cid:16).
o(k)ij ∼ n.o(k)ij(cid:48) , λ−1w i.
(cid:17).
,.
(9).
4we also tried to learn separate embeddings in the dynamic.
component, but this led to worse performance..with j(cid:48) = j − 1. this type of gaussian processis known as ornstein-uhlenbeck process (uhlen-beck and ornstein, 1930) and is commonly usedto model time series (roberts et al., 2013).
therandom walk prior enforces that the dynamic em-beddings e(k)ij change smoothly over time..3.4 model training.
the combination with bert makes dynamic con-textualized word embeddings easily applicable todifferent tasks by adding a task-speciﬁc layer ontop of the contextualizing component.
for trainingthe model, the overall loss is.
ltotal = ltask + lpriora + lpriorw ,.
(10).
where ltask is the task-speciﬁc loss, and lprioraand lpriorw are the regularization terms that imposethe anchoring and random walk priors on the type-level offset vectors,.
lpriora =.
(cid:107)o(k).
ij (cid:107)22.λak.λwk.k(cid:88).
k=1k(cid:88).
k=1.
lpriorw =.
(cid:107)o(k).
ij − o(k).
ij(cid:48) (cid:107)22 ..(11).
(12).
it is common practice to set λa (cid:28) λw (bamler andmandt, 2017; rudolph and blei, 2018).
here, weset λa = 10−3 · λw, which reduces the number oftunable hyperparameters.
we place the priors onlyon frequent words in the vocabulary (section 5.1),taking into account the observation that the vocab-ulary core constitutes the best basis for dynamicword embeddings (hamilton et al., 2016b)..4 data.
we ﬁt dynamic contextualized word embeddings tofour datasets with different linguistic, social, andtemporal characteristics, which allows us to investi-gate factors impacting their utility.
each dataset dconsists of a set of texts (e.g., reviews) written by aset of social units s (e.g., users) over a sequence oftime periods t (e.g., years).
furthermore, the so-cial units are connected by a set of edges e withina social network g. table 1 provides summarystatistics of the four datasets..arxiv.
arxiv is an open-access distribution ser-vice for scientiﬁc articles.
recently, a dataset ofall papers published on arxiv with correspond-ing metadata was released.6 for this study, we.
5we also tried a model with a feed-forward network instead.
of graph attention, but it consistently performed worse..6https://www.kaggle.com/.
cornell-university/arxiv.
6973linguistic.
social.
temporal.
dataset.
|d| unit.
µ|x| unit.
|e|.
µd.
ρ unit.
|t |.
t1.
t|t |.
arxivciaoreddityelp.
972,369 abstract269,807 review915,663 comment795,661 review.
118.10 subject684.68 user.
43.50 subreddit.
151.59 user.
5,165129,90061,796223,254.
19.3418.2023.9945.17.
.036 year.002 year.005 month.009 year.
2012810.
[01/]2001[05/]200009/2019[01/]2010.
[10/]2020[09/]201104/2020[12/]2019.
|s|.
53510,8805,7285,203.µπ.
3.483.654.692.83.table 1: dataset statistics.
|d|: number of data points; µ|x|: average number of tokens per text; |s|: number ofnodes in network; |e|: number of edges; µd: average node degree; µπ: average shortest path length between twonodes; ρ: network density; |t |: number of time points; t1: ﬁrst time point; t|t |: last time point.
in cases whereyears are the temporal unit, we also provide the ﬁrst and last month included in the data..use arxiv’s subject classes (e.g., cs.cl) as socialunits and extract the abstracts of papers publishedbetween 2001 and 2020 for subjects with at least100 publications in that time.7 to create the net-work, we measure the overlap in authors betweensubject classes as the jaccard similarity of corre-sponding author sets, resulting in a similarity ma-trix s. based on s, we deﬁne the adjacency matrixg of g, whose elements are.
gij = (cid:6)sij − θ(cid:7),.
(13).
i.e., there is an edge between subject classes i andj if the jaccard similarity of author sets is greaterthan θ. we set θ to 0.01.8.ciao.
ciao is a product review site on whichusers can mark explicit trust relations towards otherusers (e.g., if they ﬁnd their reviews helpful).
adataset containing reviews covering the time periodfrom 2000 to 2011 has been made publicly avail-able (tang et al., 2012).9 we use the trust relationsto create a directed graph.
since we also performsentiment analysis on the dataset, we follow yangand eisenstein (2017) in converting the ﬁve-starrating range into two classes by discarding three-star reviews and treating four/ﬁve stars as positiveand one/two stars as negative..reddit.
reddit is a social media platform host-ing discussions about a variety of topics.
it is di-vided into smaller communities, so-called subred-dits, which have been shown to be highly conduciveto linguistic dynamics (del tredici and fern´andez,2018; del tredici et al., 2019a).
a full dump of pub-lic reddit posts is available online.10 we retrieveall comments between september 2019 and april.
7we treat subject class combinations passing the frequency.
threshold (e.g., cs.cl&cs.ai) as individual units..8we tried other values of θ, but the results were similar.
9https://www.cse.msu.edu/˜tangjili/.
10https://files.pushshift.io/reddit/.
trust.html.
comments.
2020, which allows us to examine the effects of therising covid-19 pandemic on lexical usage patterns.
we remove subreddits with fewer than 10,000 com-ments in the examined time period and sample 20comments per subreddit and month.
for each sub-reddit, we compute the set of users with at least10 comments in the examined time period.
basedon this, we use the same strategy as for arxiv tocreate a network based on user overlap..yelp.
similarly to ciao, yelp is a productreview site on which users can mark explicit friend-ship relations.
a subset of the data has been re-leased online.11 we use the friendship relationsto create a directed graph between users.
sincewe also use the dataset for sentiment analysis, weagain discard three-star reviews and convert theﬁve-star rating range into two classes..the fact that the datasets differ in terms of theirsocial and temporal characteristics allows us to ex-amine which factors impact the utility of dynamiccontextualized word embeddings.
we highlight,e.g., that the datasets differ in the nature of theirsocial units, cover different time periods, and ex-hibit different levels of temporal granularity.
werandomly split all datasets into 70% training, 10%development, and 20% test.
we apply stratiﬁedsampling to make sure the model sees data from alltime points during training.
see appendix a.1 fordetails about data preprocessing..5 experiments.
5.1 embedding training.
we ﬁt dynamic contextualized word embeddings toall four datasets, using bertbase (uncased) as thecontextualizer and masked language modeling asthe training objective (devlin et al., 2019), i.e., we.
11https://www.yelp.com/dataset.
6974arxiv.
ciao.
reddit.
yelp.
model.
dev.
test.
dev.
test.
dev.
test.
dev.
test.
dcwe 3.5213.523cwe.
3.5133.530.
5.9205.922.
5.9025.910.
9.4809.580.
9.5969.555.
4.7174.714.
4.7204.723.table 2: masked language modeling perplexity on thefour datasets (lower is better).
dcwe: dynamic contex-tualized word embeddings; cwe: contextualized wordembeddings.
the better score per column (highlightedin gray) is underlined if it is signiﬁcantly (p < .01)better as shown by a wilcoxon signed-rank test..add a language modeling head on top of bert.12to estimate the goodness of ﬁt, we measure maskedlanguage modeling perplexity and compare againstﬁnetuned (non-dynamic) contextualized word em-beddings, speciﬁcally bertbase (uncased).
seeappendix a.2 for details about implementation,hyperparameter tuning, and runtime..dynamic contextualized word embeddings(dcwe) yield ﬁts to the data similar to and (some-times signiﬁcantly) better than non-dynamic con-textualized word embeddings (cwe), which indi-cates that they successfully combine extralinguisticwith linguistic information (table 2).13.
5.2 ablation study.
to examine the relative importance of temporaland social information for dynamic contextualizedword embeddings, we perform two experiments inwhich we ablate social context and time (figure 3).
in social ablation (sa), we train dynamic contex-tualized word embeddings where the vector offsetdepends only on word identity and time, not socialcontext, keeping the random walk prior betweensubsequent time slices.
in temporal ablation (ta),we use one social component for all time slices.
see appendix a.3 for details about implementa-tion, hyperparameter tuning, and runtime..temporal ablation has more severe conse-quences than social ablation (table 3).
on ciao,the social component does not yield better ﬁts onthe data at all, which might be related to the factthat many users in this dataset only have one review,and that its social network has the lowest densityas well as the smallest average node degree out ofall considered datasets (table 1)..12for a given dataset, we only compute dynamic embed-dings for tokens in bert’s input vocabulary that are amongthe 100,000 most frequent words.
for less frequent tokens,we input the non-dynamic bert embedding..13statistical signiﬁcance is tested with a wilcoxon signed-.
rank test (wilcoxon, 1945; dror et al., 2018)..ltask.
bert.
˜e(1)+o(1)j.
˜e(2)+o(2)j.
˜e(3)+o(3)j.x(1) x(2) x(3).
tj.
ltask.
bert.
˜e(1)+o(1)i.
˜e(2)+o(2)i.x(1) x(2) x(3).
˜e(3)+o(3)i.si.
(a) social ablation.
(b) temporal ablation.
figure 3: models for ablation study.
in social abla-tion, the vector offset only depends on word identityand time, not social context.
in temporal ablation, thereis only one social component for all time slices..arxiv.
ciao.
reddit.
yelp.
model.
dev.
test.
dev.
test.
dev.
test.
dev.
test.
dcwe 3.521sa3.5173.534ta.
3.5133.5153.541.
5.9205.9195.924.
5.9025.8995.931.
9.4809.6209.598.
9.5969.6319.612.
4.7174.7254.726.
4.7204.7234.734.table 3: masked language modeling perplexity on thefour datasets in ablation study (lower is better).
dcwe:dynamic contextualized word embeddings; sa: socialablation; ta: temporal ablation.
the best score per col-umn (highlighted in gray) is underlined if it is signif-icantly (p < .01) better than the second-best score asshown by a wilcoxon signed-rank test..5.3 qualitative analysis.
do dynamic contextualized word embeddings in-deed capture interpretable dynamics in word mean-ing?
to examine this question qualitatively, wedeﬁne as sim(k)the cosine similarity between theijnon-dynamic embedding of x(k), ˜e(k), and the dy-namic embeddings of x(k) given social and tempo-ral contexts si and tj, e(k)ij ,.
sim(k).
ij = cos φ(k)ij ,.
(14).
is the angle between ˜e(k) and e(k)ij.
where φ(k)(fig-ijure 1).14 to ﬁnd words with a high degree ofvariability, we compute the standard deviation ofsim(k)ij based on all si and tj in which a given wordx(k) occurs in the data,(cid:16).
(cid:17).
|(x(k), si, tj) ∈ d}.
,.
(15).
σ(k)sim = σ.
{sim(k)ij.
for d.where we take the development setlooking at the top-ranked words according toσ(k)sim, we observe that they exhibit pronounced14in cases where x(k) is split into several wordpiece tokensby bert, we follow previous work (pinter et al., 2020; siaet al., 2020) and average the subword embeddings..6975context for sim(k).
ij > µ(k).
sim.
context for sim(k).
ij < µ(k).
sim.
word.
extralinguistic.
linguistic.
extralinguistic.
linguistic.
“isolating”.
r/sahp12/19.
it’s really hard to explain to other peoplehow isolating and exhausting being asahp can be..r/asthma03/20.
i wish i knew if i’d had covid so that icould stop self isolating and insteadvolunteer in my community..“testing”.
r/vjoeshows04/20.
testing a photocell light ﬁxture duringthe day is easy when you know how.
this is what this diy video is about..r/vancouver03/20.
testing is not required if a patient has nosymptoms, mild symptoms, or is a returningtraveller and is isolating at home..table 4: examples of dynamics in word meaning during the covid-19 pandemic.
the table lists example wordswith top-ranked values of σ(k).
sim, i.e., they exhibit a high degree of extralinguistically-driven semantic dynamics..extralinguistically-driven semantic dynamics in thedata.
for reddit, e.g., many of the top-rankedwords have experienced a sudden shift in their dom-inant sense during the covid-19 pandemic such as“isolating” and “testing” (table 4).
social and tem-poral contexts in which the sense related to covid-19 is dominant have smaller values of sim(k)(i.e.,ijthe cosine distance is larger) than the ones in whichthe more general sense is dominant.
such short-term semantic shifts, which have attracted growinginterest in nlp recently (stewart et al., 2017; deltredici et al., 2019a; powell and sentz, 2020), canresult in lasting semantic narrowing if speakersbecome reluctant to use the word outside of themore specialized sense (anttila, 1989; croft, 2000;robinson, 2012; bybee, 2015)..the.
thus,.
analysis.
qualitative.
the dynamic component.
suggeststhatindeed capturesextralinguistically-driven variability in word mean-ing.
in sections 5.4 and 5.5, we will demonstrateby means of two example applications how thisproperty can be beneﬁcial in practice..5.4 exploration 1: semantic diffusion.
we will now provide a more in-depth analysis ofsocial and temporal dynamics in word meaning toshowcase the potential of dynamic contextualizedword embeddings as an analytical tool.
speciﬁcally,we will analyze how changes in the dominant senseof a word diffuse through the social networks ofarxiv and reddit.
for arxiv, we will examine thedeep learning sense of the word “network”.
forreddit, we will focus on the medical sense of theword “mask”.
we know that these senses havebecome more widespread over the last few years(arxiv) and months (reddit), but we want to testif dynamic contextualized word embeddings cancapture this spread, and if they allow us to gain newinsights about the spread of semantic associationsthrough social networks in general..to perform this analysis, let r(k,k(cid:48)).
be the rank ofx(k(cid:48))’s embedding among the n nearest neighborsof x(k)’s embedding, given social and temporalcontexts si and tj.
we then deﬁne as.
ij.
ˆr(k,k(cid:48))ij.
= n − r(k,k(cid:48)).
ij.
+ 1.
(16).
a semantic similarity score between x(k) and x(k(cid:48)).
ˆr(k,k(cid:48))is maximal when x(k(cid:48))’s embedding is clos-ijest to x(k)’s embedding.
we set ˆr(k,k(cid:48))= 0 if x(k(cid:48))is not among the n nearest neighbors of x(k).
weset n = 100..ij.
using ˆr(k,k(cid:48)).
ij.
, we measure dynamics in the se-mantic similarity between “network” and “learning”(representing the deep learning sense of “network”)as well as “mask” and “vaccine” (representing themedical sense of “mask”).
for all social and tem-poral contexts in which “network” and “mask” oc-cur, we compute ˆr(k,k(cid:48))between their socially andtemporally dynamic embeddings on the one handand time-speciﬁc centroids of “learning” and “vac-cine” averaged over social contexts on the other,employing contextualized versions of the dynamicembeddings.15 in cases where “network” or “mask”occur more than once in a certain social and tem-poral context, we take the mean of ˆr(k,k(cid:48)).
ij.
..ij.
ij.
the dynamics of ˆr(k,k(cid:48)).
reﬂect how the changesin the dominant sense of “network” and “mask”spread through the social networks (figure 4).
for“network”, we see that the deep learning sense wasalready present in computer science and physics in2013, where neural networks have been used sincethe 1980s.
it then gradually spread from these twoepicenters, with a major intensiﬁcation after 2016.for “mask”, we also see a gradual diffusion, with amajor intensiﬁcation after 03/2020..15we average the ﬁrst six layers of the contextualizer sincethey have been shown to contain the core of lexical and se-mantic information (vuli´c et al., 2020)..6976(a) ˆr(k,k(cid:48)).
ij.
for “network” and “learning” in arxiv.
(b) ˆr(k,k(cid:48)).
ij.
for “mask” and “vaccine” in reddit.
figure 4: spread of changes in the dominant sense through the social network.
the ﬁgure shows dynamics inˆr(k,k(cid:48)), a score for semantic similarity between 0 (no similarity) and 100 (very similar), for “network” and “learn-ijing” in arxiv as well as “mask” and “vaccine” in reddit.
the different node shapes in the arxiv network representthe three major arxiv subject classes: computer science (square), mathematics (triangle), and physics (circle).
for“network”, the change towards the deep learning sense spread gradually from computer science and physics.
for“mask”, the change towards the medical sense also spread gradually, with a major intensiﬁcation after 03/2020..ij.
on what paths do new semantic associationsspread through the social network?
in complex sys-tems theory, there are two basic types of randommotion on networks: random walks, which consistof a series of consecutive random steps, and ran-dom ﬂights, where step lengths are drawn from thel´evy distribution (masuda et al., 2017).
to probewhether there is a dominant type of spread for thetwo examples, we compute for each time slice tjwhat proportion of nodes that have ˆr(k,k(cid:48))> 0 forthe ﬁrst time at tj (i.e., the change in the domi-nant sense has just arrived) are neighbors of nodesthat already had ˆr(k,k(cid:48))> 0 before tj.
this anal-ysis shows that random walks are the dominanttype of spread for “network”, but random ﬂightsfor “mask” (figure 5).
intuitively, it makes sensethat a technical concept such as neural networksspreads through the direct contact of collaboratingscientists rather than through more distant forms ofreception (e.g., the reading of articles).
in the caseof facial masks, on the other hand, the exogenousfactor of the worsening covid-19 pandemic and theaccompanying publicity was a driver of semanticdynamics irrespective of node position..ij.
figure 5: types of semantic diffusion in arxiv (a) andreddit (r).
the ﬁgure shows for each time tj the prob-ability that a node having the new sense for the ﬁrsttime is the neighbor of a node that already had it previ-ously (walk, w) as opposed to cases where none of itsneighbors had it previously (ﬂight, f)..5.5 exploration 2: sentiment analysis.
as a second testbed, we apply dynamic contextu-alized word embeddings on a task for which so-cial and temporal information is known to be im-portant (yang and eisenstein, 2017): sentimentanalysis.
we use the ciao and yelp datasets andtrain dynamic contextualized word embeddings byadding a two-layer feed-forward network on top ofbertbase (uncased) and ﬁnetuning it for the taskof sentiment classiﬁcation.16 we again compare.
16we ﬁnetune directly on sentiment analysis without prior.
ﬁnetuning on masked language modeling..6977ciao.
yelp.
model.
dev test dev test.
dcwe .894.889cwe.
.896.890.
.969.967.
.968.966.table 5: f1 score on sentiment analysis (higher is bet-ter).
dcwe: dynamic contextualized word embed-dings; cwe: contextualized word embeddings.
thebetter score per column (highlighted in gray) is under-lined if it is signiﬁcantly (p < .01) better as shown bya mcnemar’s test for binary data..against contextualized word embeddings, specif-ically bertbase (uncased), which is ﬁnetunedwithout the dynamic component.
see appendixa.4 for details about implementation, hyperparam-eter tuning, and runtime..dynamic contextualized word embeddingsachieve slight but signiﬁcant improvements overthe already strong performance of non-dynamicbert (table 5).17 this provides further evidencethat infusing social and temporal information onthe lexical level can be useful for nlp tasks..6 conclusion.
we have introduced dynamic contextualized wordembeddings that represent words as a function ofboth linguistic and extralinguistic context.
basedon a plm, speciﬁcally bert, dynamic contextu-alized word embeddings model time and socialspace jointly, which makes them advantageous forvarious areas in nlp.
we have trained dynamiccontextualized word embeddings on four datasetsand showed that they are capable of tracking socialand temporal variability in word meaning.
besidesserving as an analytical tool, dynamic contextual-ized word embeddings can also be of beneﬁt fordownstream tasks such as sentiment analysis..acknowledgements.
this work was funded by the european re-search council (#740516) as well as the engi-neering and physical sciences research council(ep/t023333/1).
the ﬁrst author was also sup-ported by the german academic scholarship foun-dation and the arts and humanities research coun-cil.
we thank the anonymous reviewers for theirdetailed and extremely helpful comments..17statistical signiﬁcance is tested with a mcnemar’s test.
for binary data (mcnemar, 1947; dror et al., 2018).
references.
silvio amir, byron c. wallace, hao lyu, paula car-valho, and m´ario j. silva.
2016. modelling contextwith user embeddings for sarcasm detection in so-cial media.
in conference on computational natu-ral language learning (conll) 20..raimo anttila.
1989. historical and comparative lin-.
guistics.
john benjamins, amsterdam..ehsaneddin asgari, christoph ringlstetter, and hinrichsch¨utze.
2020. emblexchange at semeval-2020task 1: unsupervised embedding-based detection ofin international work-lexical semantic changes.
shop on semantic evaluation (semeval) 2020..robert bamler and stephan mandt.
2017. dynamicword embeddings.
in international conference onmachine learning (icml) 34..piotr bojanowski, edouard grave, armand joulin, andtomas mikolov.
2017. enriching word vectors withsubword information.
transactions of the associa-tion for computational linguistics, 5:135–146..joan bybee.
2015. language change.
cambridge uni-.
versity press, cambridge, uk..kevin clark, minh-thang luong, quoc v. le, andchristopher d. manning.
2020. electra: pre-training text encoders as discriminators rather thanin international conference on learn-generators.
ing representations (iclr) 8..andy coenen, emily reif, ann yuan, been kim,adam pearce, fernanda vi´egas, and martin watten-berg.
2019. visualizing and measuring the geometryof bert.
in advances in neural information pro-cessing systems (neurips) 33..william croft.
2000. explaining language change: an.
evolutionary approach.
pearson, harlow, uk..scott deerwester, susan t. dumais, george furnas,thomas landauer, and richard harshman.
1990.journalindexing by latent semantic analysis.
of the american society for information science,41(6):391–407..marco del tredici and raquel fern´andez.
2018. theroad to success: assessing the fate of linguistic inno-vations in online communities.
in international con-ference on computational linguistics (coling)27..marco del tredici, raquel fern´andez, and gemmaboleda.
2019a.
short-term meaning shift: a distri-in annual conference of thebutional exploration.
north american chapter of the association for com-putational linguistics: human language technolo-gies (naacl htl) 2019..marco del tredici, diego marcheggiani, sabineschulte im walde, and raquel fern´andez.
2019b.
you shall know a user by the company it keeps: dy-namic representations for social media users in nlp..6978in conference on empirical methods in natural lan-guage processing (emnlp) 2019..jacob devlin, ming-wei chang, kenton lee, andkristina toutanova.
2019. bert: pre-training ofdeep bidirectionaltransformers for language un-in annual conference of the northderstanding.
american chapter of the association for computa-tional linguistics: human language technologies(naacl htl) 2019..jesse dodge, suchin gururangan, dallas card, royschwartz, and noah a. smith.
2019. show yourwork: improved reporting of experimental results.
in conference on empirical methods in natural lan-guage processing (emnlp) 2019..hongyu gong, suma bhat, and pramod viswanath.
2020. enriching word embeddings with temporaland spatial information.
in conference on computa-tional natural language learning (conll) 24..aditya grover and jure leskovec.
2016. node2vec:in inter-scalable feature learning for networks.
national conference on knowledge discovery anddata mining (kdd) 22..william hamilton, kevin clark, jure leskovec, anddan jurafsky.
2016a.
inducing domain-speciﬁc sen-in con-timent lexicons from unlabeled corpora.
ference on empirical methods in natural languageprocessing (emnlp) 2016..rotem dror, gili baumer, segev shlomov, and roi re-ichart.
2018. the hitchhiker’s guide to testing sta-tistical signiﬁcance in natural language processing.
in annual meeting of the association for computa-tional linguistics (acl) 56..william hamilton, jure leskovec, and dan jurafsky.
2016b.
diachronic word embeddings reveal statisti-cal laws of semantic change.
in annual meeting ofthe association for computational linguistics (acl)54..haim dubossarsky, simon hengchen, nina tahmasebi,and dominik schlechtweg.
2019. time-out: tempo-ral referencing for robust modeling of lexical seman-tic change.
in annual meeting of the association forcomputational linguistics (acl) 57..daniel edmiston.
2020. a systematic analysis of mor-phological content in bert models for multiple lan-guages.
in arxiv 2004.03032..kawin ethayarajh.
2019. how contextual are contex-tualized word representations?
comparing the geom-etry of bert, elmo, and gpt-2 embeddings.
inconference on empirical methods in natural lan-guage processing (emnlp) 2019..allyson ettinger.
2020. what bert is not: lessonsfrom a new suite of psycholinguistic diagnostics forlanguage models.
transactions of the associationfor computational linguistics, 8:34–48..lea frermann and mirella lapata.
2016. a bayesianmodel of diachronic meaning change.
transactionsof the association for computational linguistics,4:31–45..dirk geeraerts.
2010. theories of lexical semantics..oxford university press, oxford, uk..dirk geeraerts.
2018. ten lectures on cognitive soci-.
olinguistics.
brill, leiden..mario giulianelli.
2019. lexical semantic change anal-ysis with contextualised word representations.
uni-versity of amsterdam, amsterdam..mario giulianelli, marco del tredici, and raquelfern´andez.
2020.analysing lexical semanticchange with contextualised word representations.
inannual meeting of the association for computa-tional linguistics (acl) 58..yoav goldberg.
2019. assessing bert’s syntactic.
abilities.
in arxiv 1901.05287..jennifer hay, paul warren, and katie drager.
2006.factors inﬂuencing speech perception in the con-text of a merger-in-progress.
journal of phonetics,34(4):458–484..devamanyu hazarika, soujanya poria, sruthi gorantla,erik cambria, roger zimmermann, and rada mi-cascade: contextual sarcasmhalcea.
2018.in inter-detection in online discussion forums.
national conference on computational linguistics(coling) 27..john hewitt and christopher d. manning.
2019. astructural probe for ﬁnding syntax in word repre-in annual conference of the northsentations.
american chapter of the association for computa-tional linguistics: human language technologies(naacl htl) 2019..valentin hofmann, janet b. pierrehumbert, and hin-rich sch¨utze.
2020. dagobert: generating deriva-tional morphology with a pretrained language model.
in conference on empirical methods in natural lan-guage processing (emnlp) 2020..renfen hu, shen li, and shichen liang.
2019. di-achronic sense modeling with deep contextualizedword embeddings: an ecological view.
in annualmeeting of the association for computational lin-guistics (acl) 57..kokil jaidka, niyati chhaya, and lyle h. ungar.
2018.diachronic degradation of language models:in-in annual meeting ofsights from social media.
the association for computational linguistics (acl)56..ganesh jawahar, benoit sagot, and djam´e seddah.
2019. what does bert learn about the structureof language?
in annual meeting of the associationfor computational linguistics (acl) 57..6979ganesh jawahar and djam´e seddah.
2019. contextu-alized diachronic word representations.
in interna-tional workshop on computational approaches tohistorical language change 1..jan lukes and anders søgaard.
2018. sentiment anal-ysis under temporal shift.
in workshop on compu-tational approaches to subjectivity, sentiment andsocial media analysis (wassa) 9..jens kaiser, dominik schlechtweg, and sabine schulteim walde.
2020. op-ims @ diacr-ita: backto the roots: sgns+op+cd still rocks semanticchange detection.
in evaluation campaign of nat-ural language processing and speech tools for ital-ian (evalita) 7..yoon kim, yi-i chiu, kentaro hanaki, darshan hegde,and slav petrov.
2014. temporal analysis of lan-in work-guage through neural language models.
shop on language technologies and computationalsocial science..diederik p. kingma and jimmy l. ba.
2015. adam: amethod for stochastic optimization.
in internationalconference on learning representations (iclr) 3..peter koch.
2016. meaning change and semantic shifts.
in p¨aivi juvonen and maria koptjevskaja-tamm, ed-itors, the lexical typology of semantic shifts, pages21–66.
de gruyter, berlin..vivek kulkarni, rami al-rfou, bryan perozzi, andsteven skiena.
2015. statistically signiﬁcant detec-in the web conferencetion of linguistic change.
(www) 24..andrey kutuzov and mario giulianelli.
2020. uio-uva at semeval-2020 task 1: contextualised em-beddings for lexical semantic change detection.
ininternational workshop on semantic evaluation (se-meval) 2020..andrey kutuzov, lilja øvrelid, terrence szymanski,and erik velldal.
2018. diachronic word embed-in inter-dings and semantic shifts: a survey.
national conference on computational linguistics(coling) 27..william labov.
2001. principles of linguistic change:.
social factors.
blackwell, malden, ma..chang li and dan goldwasser.
2019. encoding socialinformation with graph convolutional networks forpolitical perspective detection in news media.
in an-nual meeting of the association for computationallinguistics (acl) 57..yongjie lin, yi c. tan, and robert frank.
2019. opensesame: getting inside bert’s linguistic knowl-in analyzing and interpreting neural net-edge.
works for nlp (blackboxnlp) 2..matej martinc, petra kralj novak, and senja pollak.
2020a.
leveraging contextual embeddings for de-in internationaltecting diachronic semantic shift.
conference on language resources and evaluation(lrec) 12..matej martinc, syrielle montariol, elaine zosa, andlidia pivovarova.
2020b.
capturing evolution inin the webword usage: just add more clusters?
conference (www) 29..naoki masuda, mason a. porter, and renaud lam-biotte.
2017. random walks and diffusion on net-works.
physics reports, 716-717:1–58..bryan mccann, james bradbury, caiming xiong, andrichard socher.
2017. learned in translation: con-textualized word vectors.
in advances in neural in-formation processing systems (nips) 31..quinn mcnemar.
1947. note on the sampling errorof the difference between correlated proportions orpercentages.
psychometrika, 12(2):153–157..timothee mickus, denis paperno, mathieu constant,and kees van deemter.
2020. what do you mean,bert?
assessing bert as a distributional seman-tics model.
in society for computation in linguis-tics (scil) 3..tomas mikolov, kai chen, greg corrado, and jeffreydean.
2013a.
efﬁcient estimation of word represen-tations in vector space.
in arxiv 1301.3781..tomas mikolov, ilya sutskever, kai chen, greg cor-rado, and jeffrey dean.
2013b.
distributed represen-tations of words and phrases and their composition-ality.
in advances in neural information processingsystems (nips) 26..james milroy.
1992. linguistic variation and change:on the historical sociolinguistic of english.
black-well, oxford, uk..lesley milroy.
1980. language and social networks..blackwell, oxford, uk..pushkar mishra, marco del tredici, helen yan-nakoudakis, and ekaterina shutova.
2018. authorproﬁling for abuse detection.
in international con-ference on computational linguistics (coling)27..nelson f. liu, matt gardner, yonatan belinkov,matthew peters, and noah a. smith.
2019. lin-guistic knowledge and transferability of contextualrepresentations.
in annual conference of the northamerican chapter of the association for computa-tional linguistics: human language technologies(naacl htl) 2019..pushkar mishra, marco del tredici, helen yan-nakoudakis, and ekaterina shutova.
2019. abusivelanguage detection with graph convolutional net-works.
in annual conference of the north americanchapter of the association for computational lin-guistics: human language technologies (naaclhtl) 2019..6980daisuke oba, naoki yoshinaga, shoetsu sato, satoshiakasaki, and masashi toyoda.
2019. modeling per-sonal biases in language use by inducing personal-ized word embeddings.
in annual conference of thenorth american chapter of the association for com-putational linguistics: human language technolo-gies (naacl htl) 2019..hermann paul.
1880. principien der sprachgeschichte..t¨ubingen, niemeyer..jeffrey pennington, richard socher, and christopher d.manning.
2014. glove: global vectors for wordin conference on empirical meth-representation.
ods in natural language processing (emnlp)2014..valerio perrone, palma, marco, simon hengchen,alessandro vatri,jim q. smith, and barbaramcgillivray.
2019. gasc: genre-aware semanticin international work-change for ancient greek.
shop on computational approaches to historicallanguage change 1..matthew peters, mark neumann, mohit iyyer, mattgardner, christopher clark, kenton lee, and lukezettlemoyer.
2018a.
deep contextualized word rep-in annual conference of the northresentations.
american chapter of the association for computa-tional linguistics: human language technologies(naacl hlt) 2018..matthew peters, mark neumann, luke zettlemoyer,and wen-tau yih.
2018b.
dissecting contextualword embeddings: architecture and representation.
in conference on empirical methods in natural lan-guage processing (emnlp) 2018..janet pierrehumbert.
2012. the dynamic lexicon.
inabigail cohn, c´ecile fougeron, and marie huffman,editors, the oxford handbook of laboratory phonol-ogy, pages 173–183.
oxford university press, ox-ford..yuval pinter, cassandra l. jacobs, and jacob eisen-in findings of em-stein.
2020. will it unblend?
pirical methods in natural language processing(emnlp) 2020..martin p¨omsl and roman lyapin.
2020. circe atsemeval-2020 task 1: ensembling context-free andcontext-dependent word representations.
in interna-tional workshop on semantic evaluation (semeval)2020..james powell and kari sentz.
2020. tracking short-term temporal linguistic dynamics to characterizecandidate therapeutics for covid-19 in the cord-19 corpus.
in conference on embedded networkedsensor systems (sensys) 18..ondˇrej praˇz´ak, pavel pˇrib´aˇn, stephen taylor, andjakub sido.
2020. uwb at semeval-2020 task 1:lexical semantic change detection.
in internationalworkshop on semantic evaluation (semeval) 2020..alec radford, jeffrey wu, rewon child, david luan,dario amodei, and ilya sutskever.
2019. languagemodels are unsupervised multitask learners..colin raffel, noam shazeer, adam roberts, katherinelee, sharan narang, michael matena, yanqi zhou,wei li, and peter j. liu.
2020. exploring the lim-its of transfer learning with a uniﬁed text-to-texttransformer.
journal of machine learning research,21:1–67..stephen roberts, michael osborne, mark ebden,steven reece, neale gibson, and suzanne aigrain.
2013. gaussian processes for time-series modelling.
philosophical transactions of the royal society a,371(1984):20110550..justyna robinson.
2010. awesome insights into se-mantic variation.
in dirk geeraerts, gitte kris-tiansen, and yves peirsman, editors, advancesin cognitive sociolinguistics, pages 85–109.
degruyter, berlin..justyna robinson.
2012. a sociolinguistic approachto semantic change.
in kathryn allan and justynarobinson, editors, current methods in historical se-mantics, pages 199–231.
de gruyter, berlin..anna rogers, olga kovaleva, and anna rumshisky.
2020. a primer in bertology: what we knowabout how bert works.
in arxiv 2002.12327..alex rosenfeld and katrin erk.
2018. deep neuralmodels of semantic shift.
in annual conference ofthe north american chapter of the association forcomputational linguistics: human language tech-nologies (naacl hlt) 2018, pages 474–484..maja rudolph and david blei.
2018. dynamic embed-in the web confer-.
dings for language evolution.
ence (www) 27..dominik schlechtweg, anna h¨atty, marco del tredici,and sabine schulte im walde.
2019. a wind ofchange: detecting and evaluating lexical semanticchange across times and domains.
in annual meet-ing of the association for computational linguistics(acl) 57..dominik schlechtweg, barbara mcgillivray, simonhengchen, haim dubossarsky, and nina tahmasebi.
2020. semeval-2020 task 1: unsupervised lexicalin international work-semantic change detection.
shop on semantic evaluation (semeval) 2020..dominik schlechtweg and sabine schulte im walde.
simulating lexical semantic change fromin international conference.
2020.sense-annotated data.
on the evolution of language (evolang) 13..dominik schlechtweg, sabine schulte im walde, andstefanie eckmann.
2018. diachronic usage related-ness (durel): a framework for the annotation ofin annual conference oflexical semantic change.
the north american chapter of the association forcomputational linguistics: human language tech-nologies (naacl hlt) 2018..6981suzanna sia, ayush dalmia, and sabrina j. mielke.
2020. tired of topic models?
clusters of pretrainedword embeddings make for fast and good topics too!
in conference on empirical methods in natural lan-guage processing (emnlp) 2020..yi yang, ming-wei chang, and jacob eisenstein.
2016.toward socially-infused information extraction: em-in con-bedding authors, mentions, and entities.
ference on empirical methods in natural languageprocessing (emnlp) 2016..ian stewart, dustin arendt, eric bell, and svitlanavolkova.
2017. measuring, predicting and visual-izing short-term change in word representation andin interna-usage in vkontakte social network.
tional aaai conference on weblogs and social me-dia (icwsm) 11..nina tahmasebi, lars borin, and adam jatowt.
2018.survey of computational approaches to lexical se-mantic change detection.
in arxiv 1811.06278..jiliang tang, huiji gao, and huan liu.
2012. mtrust:discerning multi-faceted trust in a connected world.
in international conference on web search anddata mining (wsdm) 5..ian tenney, patrick xia, berlin chen, alex wang,adam poliak, r. thomas mccoy, najoung kim,benjamin van durme, samuel r. bowman, dipan-jan das, and ellie pavlick.
2019. what do you learnfrom context?
probing for sentence structure in con-in internationaltextualized word representations.
conference on learning representations (iclr) 7..george uhlenbeck and leonard ornstein.
1930. onthe theory of the brownian motion.
physical review,36:823–841..petar veliˇckovi´c, guillem cucurull, arantxa casanova,adriana romero, pietro li`o, and yoshua bengio.
in international2018. graph attention networks.
conference on learning representations (iclr) 6..ivan vuli´c, edoardo m. ponti, robert litschko, goranglavaˇs, and anna korhonen.
2020. probing pre-trained language models for lexical semantics.
inconference on empirical methods in natural lan-guage processing (emnlp) 2020..charles welch, jonathan kummerfeld, ver´onica p´erez-rosas, and rada mihalcea.
2020a.
compositionalin conference ondemographic word embeddings.
empirical methods in natural language processing(emnlp) 2020..charles welch, jonathan kummerfeld, ver´onica p´erez-rosas, and rada mihalcea.
2020b.
exploring thein inter-value of personalized word embeddings.
national conference on computational linguistics(coling) 28..gregor wiedemann, steffen remus, avi chawla, andchris biemann.
2019. does bert make any sense?
interpretable word sense disambiguation with con-textualized embeddings.
in arxiv 1909.10430..frank wilcoxon.
1945..individual comparisons by.
ranking methods.
biometrics bulletin, 1(6):80–83..yi yang and jacob eisenstein.
2017. overcoming lan-guage variation in sentiment analysis with social at-tention.
transactions of the association for compu-tational linguistics, 5:295–307..zhilin yang, zihang dai, yiming yang, jaime car-bonell, ruslan salakhutdinov, and quoc v. le.
2019.xlnet: generalized autoregressive pretraining forlanguage understanding.
in advances in neural in-formation processing systems (neurips) 33..jing yao, zhicheng dou, and ji-rong wen.
2020. em-ploying personal word embeddings for personalizedin international conference on researchsearch.
and development in information retrieval (sigir)43..zijun yao, yifan sun, weicong ding, nikhil rao, andhui xiong.
2018. dynamic word embeddings forevolving semantic discovery.
in international con-ference on web search and data mining (wsdm)11..ziqian zeng, xin liu, and yangqiu song.
2018. biasedrandom walk based social regularization for wordin international joint conference onembeddings.
artiﬁcial intelligence (ijcai) 27..ziqian zeng, yichun yin, yangqiu song, and mingzhang.
2017. socialized word embeddings.
in inter-national joint conference on artiﬁcial intelligence(ijcai) 26..a appendices.
a.1 data preprocessing.
for each dataset, we remove duplicates as well astexts with less than 10 words.
for the ciao dataset,we further remove reviews rated as not helpful.
welowercase all words.
since bert’s input is limitedto 512 tokens, we truncate longer texts by takingthe ﬁrst and last 256 tokens..a.2 embedding training: hyperparameters.
dcwe.
the hyperparameters of the contextualizerare as for bertbase (uncased).
in particular, thedimensionality of the input embeddings ˜e(k) is 768.for the dynamic component, the social vectors sijand ˜si have a dimensionality of 50. the node2vecvectors for the initialization of ˜si are trained on10 sampled walks of length 80 per node with awindow size of 2. the gat has two layers withfour attention heads, respectively (activation func-tion: tanh).
the feed-forward network has two lay-ers (activation function: tanh).
we apply dropout.
6982arxiv.
ciao.
reddit.
yelp.
model.
µ.σ ne.
l.λa.
τ.µ.σ ne.
l.λa.
τ.µ.σ ne.
l.λa.
τ.µ.σ ne.
l.λa.
τ.dcwe 3.8483.851cwe.
.307.305.
77.
3e-66,7561e-13e-6 — 3,749.
6.7946.789.
.606.589.
77.
3e-611,8311e-13e-6 — 3,564.
9.8369.869.
.318.274.
77.
3e-64,6291e-13e-6 — 2,160.
5.1225.129.
.384.384.
77.
3e-67,0021e-13e-6 — 3,551.table 6: validation performance statistics and hyperparameter search details for embedding training.
dcwe: dy-namic contextualized word embeddings; cwe: contextualized word embeddings.
the table shows the mean (µ)and standard deviation (σ) of the validation performance (masked language modeling perplexity) on all hyperpa-rameter search trials and gives the number of epochs (ne), learning rate (l), and regularization constant (λa) withthe best validation performance as well as the runtime (τ ) in minutes for one full hyperparameter search (28 trialsfor dcwe on ciao, 14 trials for cwe on ciao, 7 trials for dcwe and cwe on arxiv, reddit, and yelp)..with a rate of 0.2 after each layer of the dynamiccomponent.
the number of trainable parametersvaries between models trained on different datasetsdue to differences in |t | and is 134,914,570 forarxiv, 124,990,698 for ciao, 120,028,762 for red-dit, and 122,509,730 for yelp.
we use a batchsize of 4 and perform grid search for the num-ber of epochs ne ∈ {1, .
.
.
, 7}, the learning ratel ∈ {1 × 10−6, 3 × 10−6}, and the regularizationconstant λa ∈ {1 × 10−2, 1 × 10−1}, thereby alsodetermining λw (section 3.4)..as.
cwe.
all hyperparameters.
forarethe number of train-bertbase (uncased).
able parameters is 110,104,890. we use a batchsize of 4 and perform grid search for the numberof epochs ne ∈ {1, .
.
.
, 7} and the learning ratel ∈ {1 × 10−6, 3 × 10−6}..for both dcwe and cwe, we tune hyperparam-eters except for the number of epochs on the ciaodataset (selection criterion: masked language mod-eling perplexity) and use the best conﬁguration forarxiv, reddit, and yelp.
models are trained withcategorical cross-entropy as the loss function andadam (kingma and ba, 2015) as the optimizer.
ex-periments are performed on a geforce gtx 1080ti gpu (11gb)..table 6 lists statistics of the validation perfor-mance over hyperparameter search trials and pro-vides information about best hyperparameter con-ﬁgurations.18 we also report the number of hyper-parameter search trials as well as runtimes for thehyperparameter search..a.3 ablation study: hyperparameters.
sa.
words are mapped to offsets using time-speciﬁc two-layer feed-forward networks (activa-tion function: tanh).
both layers have a dimen-sionality of 768. all other hyperparameters are.
18since expected validation performance (dodge et al.,2019) may not be correct for grid search, we report meanand standard deviation of the performance instead..as for dcwe with a full dynamic component(appendix a.2).
the number of trainable pa-rameters again varies between models trained ondifferent datasets due to differences in |t | andis 133,728,570 for arxiv, 124,279,098 for ciao,119,554,362 for reddit, and 121,916,730 for yelp.
we use a batch size of 4 and perform grid search forthe number of epochs ne ∈ {1, .
.
.
, 7}, the learningrate l ∈ {1 × 10−6, 3 × 10−6}, and the regulariza-tion constant λa ∈ {1 × 10−2, 1 × 10−1}, therebyalso determining λw (section 3.4)..ta.
all hyperparameters are as for dcwe witha full dynamic component (appendix a.2), withthe difference that we only use one social compo-nent (consisting of a two-layer gat and a two-layerfeed-forward network) for all time units.
the num-ber of trainable parameters is 111,345,374. we usea batch size of 4 and perform grid search for thenumber of epochs ne ∈ {1, .
.
.
, 7}, the learningrate l ∈ {1 × 10−6, 3 × 10−6}, and the regulariza-tion constant λa ∈ {1 × 10−2, 1 × 10−1}..for both sa and ta, we tune hyperparame-ters except for the number of epochs on the ciaodataset (selection criterion: masked language mod-eling perplexity) and use the best conﬁguration forarxiv, reddit, and yelp.
models are trained withcategorical cross-entropy as the loss function andadam as the optimizer.
experiments are performedon a geforce gtx 1080 ti gpu (11gb)..table 7 lists statistics of the validation perfor-mance over hyperparameter search trials and pro-vides information about best hyperparameter con-ﬁgurations.
we also report the number of hyper-parameter search trials as well as runtimes for thehyperparameter search..a.4 sentiment analysis: hyperparameters.
dcwe.
the mid layer of the feed-forward networkon top of bert has a dimensionality of 100. allother hyperparameters are as for dcwe trainedon masked language modeling (appendix a.2)..6983arxiv.
ciao.
reddit.
yelp.
model.
µ.σ ne.
l.τ.µ.σ ne.
l.τ.µ.σ ne.
l.τ.µ.σ ne.
l.sata.
3.8493.860.
.302.303.
77.
3e-63e-6.
4,4386,080.
6.7906.843.
.635.782.
77.
3e-63e-6.
7,61610,343.
9.8519.871.
.282.321.
67.
3e-63e-6.
2,6993,859.
5.1275.129.
.392.388.
77.
3e-63e-6.
λa.
1e-11e-1.
λa.
1e-11e-1.
λa.
1e-11e-1.
λa.
1e-11e-1.
τ.
4,2316,471.table 7: validation performance statistics and hyperparameter search details for ablation study.
sa: social ablation;ta: temporal ablation.
the table shows the mean (µ) and standard deviation (σ) of the validation performance(masked language modeling perplexity) on all hyperparameter search trials and gives the number of epochs (ne),learning rate (l), and regularization constant (λa) with the best validation performance as well as the runtime (τ )in minutes for one full hyperparameter search (28 trials on ciao, 7 trials on arxiv, reddit, and yelp)..ciao.
yelp.
model.
µ.σ ne.
l.λa.
τ.µ.σ ne.
l.λa.
τ.dcwe .883.880cwe.
.010.011.
45.
3e-68,1281e-13e-6 — 2,122.
.967.967.
.003.001.
23.
3e-64,3731e-13e-6 — 2,221.ﬁgurations.
we also report the number of hyper-parameter search trials as well as runtimes for thehyperparameter search..table 8: validation performance statistics and hyperpa-rameter search details for sentiment analysis.
dcwe:dynamic contextualized word embeddings; cwe: con-textualized word embeddings.
the table shows themean (µ) and standard deviation (σ) of the validationperformance (f1 score) on all hyperparameter searchtrials and gives the number of epochs (ne), learning rate(l), and regularization constant (λa) with the best vali-dation performance as well as the runtime (τ ) in min-utes for one full hyperparameter search (20 trials fordcwe on ciao, 10 trials for cwe on ciao, 5 trials fordcwe and cwe on yelp)..the number of trainable parameters again variesbetween models trained on different datasets dueto differences in |t | and is 124,445,049 for ciaoand 121,964,081 for yelp.
we use a batch sizeof 4 and perform grid search for the number ofepochs ne ∈ {1, .
.
.
, 5}, the learning rate l ∈{1 × 10−6, 3 × 10−6}, and the regularization con-stant λa ∈ {1 × 10−2, 1 × 10−1}, thereby also de-termining λw (section 3.4)..cwe.
the mid layer of the feed-forward net-work on top of bert has a dimensionality of 100.all other hyperparameters are as for bertbase(uncased).
the number of trainable parametersis 109,559,241. we use a batch size of 4 andperform grid search for the number of epochsne ∈ {1, .
.
.
, 5} and the learning rate l ∈{1 × 10−6, 3 × 10−6}..for both dcwe and cwe, we tune hyperparam-eters except for the number of epochs on the ciaodataset (selection criterion: f1 score) and use thebest conﬁguration for yelp.
models are trainedwith binary cross-entropy as the loss function andadam as the optimizer.
experiments are performedon a geforce gtx 1080 ti gpu (11gb)..table 8 lists statistics of the validation perfor-mance over hyperparameter search trials and pro-vides information about best hyperparameter con-.
6984