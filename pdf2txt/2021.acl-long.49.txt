oommix: out-of-manifold regularization in contextual embeddingspace for text classiﬁcation.
seonghyeon lee1, dongha lee2 and hwanjo yu1∗1dept.
of computer science and engineering, postech, republic of korea2institute of artiﬁcial intelligence, postech, republic of korea{sh0416,dongha.lee,hwanjoyu}@postech.ac.kr.
abstract.
recent studies on neural networks with pre-trained weights (i.e., bert) have mainly fo-cused on a low-dimensional subspace, wherethe embedding vectors computed from inputwords (or their contexts) are located.
in thiswork, we propose a new approach, calledoommix, to ﬁnding and regularizing the re-mainder of the space, referred to as out-of-manifold, which cannot be accessed throughthe words.
speciﬁcally, we synthesize the out-of-manifold embeddings based on two embed-dings obtained from actually-observed words,to utilize them for ﬁne-tuning the network.
adiscriminator is trained to detect whether an in-put embedding is located inside the manifoldor not, and simultaneously, a generator is opti-mized to produce new embeddings that can beeasily identiﬁed as out-of-manifold by the dis-criminator.
these two modules successfullycollaborate in a uniﬁed and end-to-end mannerfor regularizing the out-of-manifold.
our ex-tensive evaluation on various text classiﬁcationbenchmarks demonstrates the effectiveness ofour approach, as well as its good compatibil-ity with existing data augmentation techniqueswhich aim to enhance the manifold..1.introduction.
neural networks with a word embedding tablehave been the most popular approach to a widerange of nlp applications.
the great success oftransformer-based contextual embeddings as wellas masked language models (devlin et al., 2019;liu et al., 2019b; raffel et al., 2020) makes it pos-sible to exploit the pre-trained weights, fully opti-mized by using large-scale corpora, and it broughta major breakthrough to many problems.
for thisreason, most recent work on text classiﬁcationhas achieved state-of-the-art performances by ﬁne-tuning the network initialized with the pre-trained.
∗ corresponding author.
weight (devlin et al., 2019).
however, they suf-fer from extreme over-parameterization due to thelarge pre-trained weight, which allows them to beeasily overﬁtted to its relatively small training data.
along with outstanding performances of the pre-trained weight, researchers have tried to revealthe underlying structure encoded in its embeddingspace (rogers et al., 2021).
one of the importantﬁndings is that the contextual embeddings com-puted from words usually form a low-dimensionalmanifold (ethayarajh, 2019).
in particular, a quan-titative analysis on the space (cai et al., 2021),which measured the effective dimension size ofbert after applying pca on its contextual em-bedding vectors, showed that 33% of dimensionscovers 80% of the variance.
in other words, onlythe low-dimensional subspace is utilized for ﬁne-tuning bert, although a high-dimensional space(i.e., model weights with a high capacity) is pro-vided for training.
based on this ﬁnding on contex-tual embedding space, we aim to regularize the con-textual embedding space for addressing the prob-lem of over-parameterization, while focusing onthe outside of the manifold (i.e., out-of-manifold)that cannot be accessed through the words..in this work, we propose a novel approach todiscovering and leveraging the out-of-manifold forcontextual embedding regularization.
the key ideaof our out-of-manifold regularization is to producethe embeddings that are located outside the mani-fold and utilize them to ﬁne-tune the network for atarget task.
to effectively interact with the contex-tual embedding of bert, we adopt two additionalmodules, named as embedding generator and man-ifold discriminator.
speciﬁcally, 1) the generatorsynthesizes the out-of-manifold embeddings by lin-early interpolating two input embeddings computedfrom actually-observed words, and 2) the discrimi-nator identiﬁes whether an input embedding comesfrom the generator (i.e., the synthesized embed-.
proceedingsofthe59thannualmeetingoftheassociationforcomputationallinguisticsandthe11thinternationaljointconferenceonnaturallanguageprocessing,pages590–599august1–6,2021.©2021associationforcomputationallinguistics590ding) or the sequence of words (i.e., the actualembedding).
the joint optimization encouragesthe generator to output the out-of-manifold em-beddings that can be easily distinguished from theactual embeddings by the discriminator, and thediscriminator to learn the decision boundary be-tween the in-manifold and out-of-manifold embed-dings.
in the end, the ﬁne-tuning on the synthesizedout-of-manifold embeddings tightly regularizes thecontextual embedding space of bert..the experimental results on several text classiﬁ-cation benchmarks validate the effectiveness of ourapproach.
in particular, our approach using a pa-rameterized generator signiﬁcantly outperforms thestate-of-the-art mixup approach whose mixing strat-egy needs to be manually given by a programmer.
furthermore, our approach shows good compati-bility with various data augmentation techniques,since the target space we focus on for regularization(i.e., out-of-manifold) does not overlap with thespace the data augmentation techniques have paidattention to (i.e., in-manifold).
the in-depth anal-yses on our modules provide an insight into howthe out-of-manifold regularization manipulates thecontextual embedding space of bert..2 related work.
in this section, we brieﬂy review two approachesto regularizing over-parameterized network basedon auxiliary tasks and auxiliary data..2.1 regularization using auxiliary tasks.
regularization is an essential tool for good general-ization capability of neural networks.
one represen-tative regularization approach relies on designingauxiliary tasks.
liu et al.
(2019a) ﬁrstly showedpromising results by unifying a bunch of hetero-geneous tasks and training a single uniﬁed modelfor all the tasks.
in particular, the synthesized taskthat encodes desirable features or removes unde-sirable features turns out to be helpful for networkregularization.
devlin et al.
(2019) introduced thetask which restores masked sentences, termed asmasked language model, to encode the distribu-tional semantic in the network; this considerablyboosts the overall performance of nlp applications.
in addition, clark et al.
(2020) regularized the net-work by discriminating generated tokens from alanguage model, and gong et al.
(2018) utilized anadditional discriminator to remove the informationabout word frequency implicitly encoded in the.
word embeddings..2.2 regularization using auxiliary data.
another approach to network regularization is totake advantage of auxiliary data, mainly obtainedby data augmentation, which eventually supple-ments the input data space.
inspired by (bengioet al., 2011) that additionally trained the networkwith noised (i.e., augmented) images in computervision, wei and zou (2019) simply augmentedsentences by adding a small perturbation to theoriginal sentences, such as adding, deleting, andswapping words within the sentences.
recent worktried to further exploit the knowledge from a pre-trained model for augmenting the sentences: sen-tence back translation by using a pre-trained trans-lation model (xie et al., 2019), and masked sen-tence reconstruction by using a pre-trained maskedlanguage model (ng et al., 2020)..mixup (zhang et al., 2018) is also a kind of dataaugmentation but differs in that it performs linearinterpolation on multiple input sentences and theircorresponding labels.
verma et al.
(2019) validatedthat mixup in the hidden space (instead of the in-put space) is also effective for regularization, andguo et al.
(2019b) found that mixup of imagescan regularize the out-of-manifold in image repre-sentations.
in the case of nlp domain, guo et al.
(2019a) and guo (2020) ﬁrstly adopted mixup totext data for text classiﬁcation, using the traditionalnetworks such as cnn and lstm; they sampletheir mixing coefﬁcients from the beta distributionat the sentence-level and at the word-level, respec-tively.
to fully utilize the contextual embeddingof transformer-based networks, chen et al.
(2020)applied mixup in the word-level contextual em-bedding space using a pre-trained language model(i.e., bert), whereas sun et al.
(2020) focusedon mixup in the sentence-level embedding spacespeciﬁcally for improving glue score..3 method.
in this section, we propose a novel mixup ap-proach, termed as oommix, to regularize the out-of-manifold in contextual embedding space for textclassiﬁcation.
we ﬁrst brieﬂy remind the architec-ture of bert, then introduce two modules used forout-of-manifold regularization, which are embed-ding generator and manifold discriminator..591figure 1: the overview of oommix for ﬁne-tuning bert (left) and the structure of our embedding generator andmanifold discriminator (right)..3.1 preliminary.
bert is a stack of m transformer encoders pre-trained on the objective of the masked languagemodel (devlin et al., 2019).
first, a raw sentence issplit into the sequence of tokens x ∈ {0, ..., |v |}lusing a tokenizer with the vocabulary v , wherel is the sequence length.
each token is mappedinto a d-dimensional vector based on the embed-ding table.
the sequence of embedding vectorsh(0) ∈ rl×d is transformed into the m-th contex-tual embedding h(m) ∈ rl×d by m transformerlayers (vaswani et al., 2017)..we ﬁne-tune the pre-trained weight to classifyinput texts into c classes.
a classiﬁer producesthe classiﬁcation probability vector o ∈ rc usingthe last contextual embedding h(m ).
then, theoptimization problem is deﬁned based on a labeleddataset d = {(x1, y1) , ..., (xn , yn )}..minimizewf.
e(x,y)∈d.
(cid:104).
lc (x, y).
(cid:105).
lc (x, y) := lkl (f (x) , ey).
where lkl is the kullback-leibler divergence andey ∈ rc is a one-hot vector representing the labely. the function f is the whole process from h(0)to o, called a target model, and wf is the trainableparameters for the function f , including the pre-trained weight of bert and the parameters in theclassiﬁer.
for notation, f can be split into severalsub-processes f (x) = (fm(cid:48) ◦ hm(cid:48)0 )(x) wherehm(cid:48)m (x) maps the m-th contextual embedding intothe m(cid:48)-th contextual embedding through the layers..m ◦ hm.
3.2 embedding generator.
the goal of our generator network g is to synthe-size an artiﬁcial contextual embedding by taking.
two contextual embeddings (obtained from layermg) as its input.
we use linear interpolation so thatthe new embedding belongs to the line segment de-ﬁned by the two input embeddings.
since we limitthe search space, the generator produces a singlescalar value λ ∈ [0, 1], called a mixing coefﬁcient..(cid:16).
g.h(mg)1., h(mg)2.
(cid:17).
= λ · h(mg)(cid:16).
1h(mg)1.λ = g.(cid:17).
, h(mg)2.
+ (1 − λ) · h(mg).
2.we introduce the distribution of the mixing coef-ﬁcient to model its uncertainty.
to this end, ourgenerator network produces the lower bound α andthe interval ∆ by using h(mg), so as tosample the mixing coefﬁcient from the uniformdistribution u (α, α + ∆)..and h(mg)2.
1.to avoid massive computational overhead in-curred by the concatenation of two input se-quences (reimers and gurevych, 2019), we adoptthe siamese architecture that uses the sharedweights on two different inputs.
the generatorﬁrst transforms each sequence of contextual em-bedding vectors by using a single transformerlayer, then obtains the sentence-level embeddingby averaging all the embedding vectors in the se-quence.
from the two sentence-level embeddingss1, s2 ∈ rd, the generator obtains the concate-nated embedding s = s1 ⊕ s2 ∈ r2d and calcu-lates α and ∆ by using a two-layer fully-connectednetwork with the softmax normalization.
speciﬁ-cally, the last fully-connected layer outputs a nor-malized 3-dimensional vector, whose ﬁrst and sec-ond values become α and ∆, thereby the rangeof sampling distribution (α, α + ∆) lies in [0, 1].
in this work, we consider the structure of the gen-erator to efﬁciently process the sequential input,.
592but any other structures focusing on different as-the network that enlarges the searchpects (e.g.
space) can be used as well.
for effective optimiza-tion of λ sampled from u (α, α + ∆), we applythe re-parameterization trick which decouples thesampling process from the computational graph(kingma and welling, 2014).
that is, we computethe mixing coefﬁcient by using γ ∼ u (0, 1)..λ = α + γ × ∆.
the optimization problem for text classiﬁcationcan be extended to the new embeddings and theirlabels, provided by the generator network..minimize,wgwfmg.
e(x1,y1)∈d.
(cid:104)lg (x1, y1).
(cid:105).
(1).
lg (x1, y1) :=.
e(x2,y2)∈d.
(cid:104).
(cid:105)lkl(fmg (˜h), ˜y).
(x2)(cid:1).
λ ∼ g (cid:0)hmg0˜h := λ · hmg0˜y := λ · ey1 + (1 − λ) · ey2.
(x1) , hmg0(x1) + (1 − λ) · hmg0.
(x2).
where wfmg is the trainable parameters of the func-tion fmg (i.e., the process from h(mg) to o), andwg is the ones for the generator.
similar to othermixup techniques, we impose the mixed label onthe generated embedding..3.3 manifold discriminator.
we found that the supervision from the objective (1)is not enough to train the generator.
the objectiveoptimizes the generator to produce the embeddingsthat are helpful for the target classiﬁcation.
how-ever, since the over-parameterized network tendsto memorize all training data, the target model alsosimply memorizes the original data to minimizeequation (1).
in this situation, the generator ismore likely to mimic the embeddings seen in thetraining set (memorized by the target model) ratherthan generate novel embeddings.
for this reason,we need more useful supervision for the generator,to make it output the out-of-manifold embeddings.
to tackle this challenge, we deﬁne an additionaltask that identiﬁes whether a contextual embeddingcomes from the generator or actual words.
thepurpose of this task is to learn the discriminativefeatures between actual embeddings and generatedembeddings, in order that we can easily discoverthe subspace which cannot be accessed through theactually-observed words.
for this task, we intro-duce a discriminator network d that serves as a.binary classiﬁer in the contextual embedding spaceof the md-th transformer layer..the discriminator takes a contextual embeddingh(md) and calculates the score s ∈ [0, 1] whichindicates the probability that h(md) comes from anactual sentence (i.e., h(md) is located inside themanifold).
its network structure is similar to thatof the generator, except that the concatenation isnot needed and the output of the two-layer fullyconnected network produces a single scalar value.
as discussed in section 3.2, any network structuresfor focusing on different aspects can be employed.
the optimization of the generator and discrimi-.
nator for this task is described as follows.
(cid:105).
(cid:104).
minimizewg,wd.
ld (x1).
e(x1,y1)∈d(cid:104)lbce(d(hmd.
mg (˜h)), 0).
(2).
ld (x1) :=.
e(x2,y2)∈d.
+lbce (d (hmd0.
(x)) , 1).
(cid:105).
where lbce is the binary cross entropy loss.
by min-imizing this objective, our generator can producethe out-of-manifold embeddings that are clearly dis-tinguished from the actual (in-manifold) contextualembeddings by the discriminator..3.4 training.
we jointly optimize the two objectives to train theembedding generator.
equation (1) encouragesthe generator to produce the embeddings whichare helpful for the target task, while equation (2)makes the generator produce the new embeddingsdifferent from the contextual embeddings obtainedfrom the words.
the ﬁnal objective is deﬁned by.
e(x,y)∼d.
[lc (x, y) + lg (x, y) + eld(x)].
where e regulates the two objectives.
the generatorand discriminator collaboratively search out infor-mative out-of-manifold embeddings for the targettask while being optimized with the target model,thereby the generated embeddings can effectivelyregularize the out-of-manifold..4 experiments.
in this section, we present the experimental re-sults supporting the superiority of oommix amongthe recent mixup approaches in text classiﬁcation.
also, we investigate its compatibility with otherdata augmentation techniques.
finally, we providein-depth analyses on our approach to further vali-date the effect of out-of-manifold regularization..593dataset.
input sentence.
class valid size valid length test size test length.
ag newsamazon reviewyahoo answerdbpedia.
contentreview texttitle, question, answercontent.
421014.
7.6k8k50k28k.
43.4995.94109.8163.62.
7.6k400k60k70k.
43.2195.62110.7463.61.table 1: statistics of datasets.
dataset.
train.
original.
nonlinearmix mixup-transformer.
tmix.
oommix.
tmix† mixtext†.
ag news.
amazon review.
yahoo answer.
dbpedia.
0.5k 88.22 ± 0.022.5k 89.92 ± 0.1591.50 ± 0.0510k.
0.5k 89.17 ± 0.352.5k 90.96 ± 0.0592.81 ± 0.0510k.
0.5k 67.24 ± 0.0770.41 ± 0.042k73.68 ± 0.0325k.
0.5k 97.86 ± 0.072.8k 98.83 ± 0.0398.96 ± 0.0735k.
88.24 ± 0.0588.75 ± 0.3688.86 ± 0.12.
89.02 ± 0.2191.04 ± 0.1191.15 ± 0.42.
67.56 ± 0.3769.17 ± 0.1169.31 ± 0.37.
97.50 ± 0.2598.74 ± 0.0998.89 ± 0.01.
88.58 ± 0.0289.62 ± 0.0991.37 ± 0.21.
88.41 ± 0.05.
88.45 ± 0.0290.07 ± 0.09 90.25 ± 0.05*91.51 ± 0.08 91.83 ± 0.09**.
--91.0.
--91.5 (+20k).
89.31 ± 0.1490.70 ± 0.0592.12 ± 0.28.
67.62 ± 0.0670.29 ± 0.1473.52 ± 0.05.
98.06 ± 0.0598.76 ± 0.0198.91 ± 0.03.
89.57 ± 0.02 89.66 ± 0.0191.24 ± 0.13 91.28 ± 0.1292.79 ± 0.07 92.94 ± 0.06.
67.57 ± 0.11 67.95 ± 0.1670.68 ± 0.15 71.08 ± 0.10*73.84 ± 0.00 74.13 ± 0.06*.
98.15 ± 0.10 98.26 ± 0.0498.82 ± 0.04 98.83 ± 0.0598.97 ± 0.03 99.03 ± 0.03*.
---.
-69.873.5.
-98.799.0.
---.
-71.3 (+50k)74.1 (+50k).
-98.9 (+70k)99.2 (+70k).
table 2: classiﬁcation accuracy on sentence classiﬁcation benchmarks.
* and ** respectively indicate p ≤ 0.05the best competitor.
tmix† and mixtext† report the scoresand p ≤ 0.01 for the paired t-test of oommix vs.presented in (chen et al., 2020), where the sizes of domain-related unlabeled data are described in the parenthesis..4.1 experimental setup.
our experiments consider 4 sentence classiﬁcationbenchmarks (zhang et al., 2015) of various scales.
the statistics of the datasets are summarized intable 1. we follow the experimental setup usedin (chen et al., 2020) to directly compare the resultswith ours.
speciﬁcally, we split the whole train-ing set into training/validation sets, while leavingout the ofﬁcial test set for evaluation.
we choosethe classiﬁcation accuracy as the evaluation metric,considering the datasets are already class-balanced.
for the various sizes of training set from 0.5k to35k, we apply stratiﬁed sampling to preserve thebalanced class distributions..in terms of optimization, we use bert providedby huggingface for the classiﬁcation tasks.1 theadam optimizer is used to ﬁne-tune bert withthe linear warm-up for the ﬁrst 1000 iterations, andthe initial learning rates for the pre-trained weightand the target classiﬁer are set to 2e-5 and 1e-3,respectively.
we set the batch size to 12 and thedropout probability to 0.1. we attach the generatorand discriminator at the third layer (mg = 3) andthe last layer (md = 12), respectively.
the twoobjectives equally contribute to training the gen-erator, e = 1, but we increase the e value if the.
1in our experiments, we use the checkpoint bert-base-.
uncased as the pre-trained weight..discriminator fails to discriminate the embeddings.
the accuracy is evaluated on validation set every200 iterations, and stop training when the accuracydoes not increase for 10 consecutive evaluations.
we report the classiﬁcation accuracy on the testset at the best validation checkpoint and repeat theexperiment three times with different random seedsto report the average with its standard deviation.
we implement the code using pytorch and usenvidia titan xp for parallel computation.
in ourenvironment, the training spends about 30 minutesto 3 hours depending on the dataset..4.2 comparision with mixup approaches.
we compare oommix with existing mixup tech-niques.
all the existing methods manually set themixing coefﬁcient, whereas we parameterize thelinear interpolation by the embedding generator,optimized to produce out-of-manifold embeddings..• nonlinearmix (guo, 2020) samples mixingcoefﬁcients for each word from the beta dis-tribution, while using neural networks to pro-duce the mixing coefﬁcient for the label.
weapply this approach to bert..• mixup-transformer (sun et al., 2020) lin-early interpolates the sentence-level embed-ding with a ﬁxed mixing coefﬁcient.
the mix-ing coefﬁcient is 0.5 as the paper suggested..594figure 2: average classiﬁcation accuracy and their standard deviation when oommix is applied with various dataaugmentation techniques..• tmix (chen et al., 2020) performs linear in-terpolation on the word-level contextual em-bedding space and samples a mixing coefﬁ-cient from the beta distribution.
we select thebest accuracy among different alpha conﬁgu-rations {0.05, 0.1} for the beta distribution..• mixtext (chen et al., 2020) additionally uti-lizes unlabeled data by combining tmix withits pseudo-labeling technique..table 2 reports the accuracy on various sentenceclassiﬁcation benchmarks.
in most cases, oom-mix achieves the best performance among all thecompeting mixup approaches.
in the case of non-linearmix, it sometimes shows worse performancethan the baseline (i.e., ﬁne-tuning only on originaldata), because its mixup strategy introduces a largedegree of freedom in the search space, which losesuseful semantic encoded in the pre-trained weight.
the state-of-the-art mixup approaches, tmix andmixup-transformer, slightly improves the accuracyover the baseline, while showing the effectivenessof the mixup approach.
finally, oommix beatsall the previous mixup approaches, which stronglyindicates that the embeddings mixed by the gen-erator are more effective for regularization, com-pared to the embeddings manually mixed by theexisting approaches.
it is worth noting that oom-mix obtains a comparable performance to mixtext,even without utilizing additional unlabeled data.
inconclusion, discovering the out-of-manifold andapplying mixup for such subspace are beneﬁcial in.
contextual embedding space..4.3 compatibility with data augmentations.
to demonstrate that the regularization effect ofoommix does not conﬂict with that of existingdata augmentation techniques, we investigate theperformance of bert that adopts both oommixand other data augmentations together.
using threepopular data augmentation approaches in the nlpcommunity, we replicate the dataset as large as theoriginal one to use them for ﬁne-tuning..• eda (wei and zou, 2019) is a simple augmen-tation approach that randomly inserts/deleteswords or swaps two words in a sentence.
weused the ofﬁcial codes2 with the default inser-tion/deletion/swap ratio the author provided.
• bt (xie et al., 2019) uses the back-translationfor data augmentation.
a sentence is trans-lated into another language, then translatedback into the original one.
we use the codeimplemented in the mixtext repository3 withthe checkpoint fairseq provided.4.
• ssmba (ng et al., 2020) makes use of thepre-trained masked language model.
theymask the original sentence and reconstruct itby ﬁlling in the masked portion.
we use thecodes provided by the authors5 with default.
2https://github.com/jasonwei20/eda_nlp3https://github.com/gt-salt/mixtext4transformer.wmt19.{en-ru,ru-en}.single.
model are provided through the ofﬁcial torch hub.
5https://github.com/nng555/ssmba.
595figure 3: count of mixing coefﬁcients without the dis-criminator (upper) and with the discriminator (lower)..masked proportion and the pre-trained weight..figure 2 shows the effectiveness of oommixwhen being used with the data augmentation tech-niques.
for all the cases, oommix shows consis-tent improvement.
especially for the amazon re-view dataset, the data augmentation and our mixupstrategy independently bring the improvement ofthe accuracy, because the subspaces targeted bythe data augmentation and oommix do not over-lap with each other.
that is, oommix ﬁnds outout-of-manifold embedding, which cannot be gen-erated from the actual sentences, whereas the dataaugmentations (i.e., eda, bt, and ssmba) focuson augmenting the sentences whose embeddingsare located inside the manifold.
therefore, jointlyapplying the two techniques allows to tightly regu-larize the contextual embedding space, includingboth in-manifold and out-of-manifold..moreover, oommix has additional advantagesover the data augmentations.
first, oommix isstill effective in the case that large training dataare available.
the data augmentation techniquesresult in less performance gain as the size of train-ing data becomes larger, because there is less roomfor enhancing the manifold constructed by enoughtraining data.
second, the class label of the aug-mented sentences given by the data augmentationtechniques (i.e., the same label with the originalsentences) can be noisy for sentence classiﬁcation,compared to the label of out-of-manifold embed-dings generated by oommix.
this is because theassumption that the augmented sentences have the.
figure 4: performance changes with respect to differ-ent layers for the generator and discriminator.
dataset:amazon review 0.5k, layer 0: word embedding..same label with their original sentences is not al-ways valid.
on the contrary, there do not existactual (or ground truth) labels for out-of-manifoldembeddings, as they do not correspond to actualsentences; this allows our mixup label to be lessnoisy for text classiﬁcation..4.4 effect of the manifold discriminator.
we also investigate how the manifold discrimina-tor affects the training of the embedding generator.
precisely, we compare the distributions of mixingcoefﬁcients, obtained from two different genera-tors; they are optimized with/without the manifolddiscriminator, respectively (figure 3 upper/lower).
we partition the training process into two phases(i.e., the ﬁrst and second half), and plot a histogramof the mixing coefﬁcients in each phase..the embedding generator without the discrimi-nator gradually moves the distribution of the mix-ing coefﬁcients toward zero, which means that thegenerated embedding becomes similar to the ac-tual embedding.
therefore, training the generatorwithout the discriminator fails to produce novelembeddings, which cannot be seen in the originaldata.
in contrast, in the case of the generator withthe discriminator, most of the mixing coefﬁcientsare located around 0.5, which implies that the gen-erator produces the embeddings which are far fromboth the two actual embeddings to some extent.
wealso observe that the average objective value forour discrimination task (equation (2)) is 0.208 forthe last 20 mini-batches; this is much lower than0.693 at the initial point.
it indicates that the gen-erated embeddings are quite clearly distinguishedfrom the ones computed from actual sentences..596figure 5: isomap visualization of the sentence-level embeddings.
the embedding vectors are projected into 3-dimensional space and rendered in two different views (xy, and yz-plane).
for each view, we colorize the out-of-manifold embeddings with black and their predicted class..4.5 effect of different embedding layers.
we further examine the effect of the location ofour generator and discriminator (i.e., mg and md)on the ﬁnal classiﬁcation performance.
figure 4 il-lustrates the changes of the classiﬁcation accuracywith respect to the target contextual embeddinglayers the modules are attached to.
to sum up,bert achieves high accuracy when the genera-tor is attached to the contextual embedding lowerthan the sixth layer while the discriminator worksfor a higher layer.
it makes our out-of-manifoldregularization affect more parameters in overalllayers, which eventually leads to higher accuracy.
on the other hand, in case that we use both thegenerator and discriminator in the same layer, thegradient of the loss for manifold discrimination can-not guide the generator to output out-of-manifoldembeddings, and as a result, the generator is notable to generate useful embeddings..4.6 manifold visualization.
finally, we visualize our contextual embeddingspace to qualitatively show that oommix discoversand leverages the space outside the manifold forregularization.
we apply isomap (tenenbaum et al.,2000), a neighborhood-based kernel pca for di-mensionality reduction, to both the actual sentenceembeddings and generated embeddings.
we simplyuse the isomap function provided by scikit-learn,and set the number of the neighbors to 15. figure 5shows the yz-plane and xy-plane of our embeddingspace, whose dimensionality is reduced to 3 (i.e.,x, y, and z).
we use different colors to representthe class of the actual embeddings as well as thepredicted class of the generated embeddings..in the yz-plane, the actual sentence embeddingsform multiple clusters, optimized for the text clas-.
siﬁcation task.
at the same time, the generated em-beddings are located in the different region from thespace enclosing most of the actual embeddings.
inthe second plot, we colorize the generated embed-dings with their predicted class.
the predicted classof out-of-manifold embeddings are well-alignedwith that of the actual embeddings, which meansthat oommix imposes the classiﬁcation capabilityon the out-of-manifold region as well.
we changethe camera view to xy-plane and repeat the sameprocess to show the alignment of class distribu-tion clearly (in the third/fourth plots).
by impos-ing the classiﬁcation capability on the extendeddimension/subspace (i.e., out-of-manifold), oom-mix signiﬁcantly improves the classiﬁcation per-formance for the original dimension/subspace (i.e.,in-manifold)..5 conclusion.
this paper proposes oommix to regularize out-of-manifold in the contextual embedding space.
ourmain motivation is that the embeddings computedfrom the words only utilize a low-dimensional man-ifold while a high-dimensional space is availablefor the model capacity.
therefore, oommix discov-ers the embeddings that are useful for the target taskbut cannot be accessed through the words.
with thehelp of the manifold discriminator, the embeddinggenerator successfully produces out-of-manifoldembeddings with their labels.
we demonstratethe effectiveness of oommix and its compatibilitywith the existing data augmentation techniques..our approach is a bit counter-intuitive in that theembeddings that cannot be accessed through the ac-tual words are helpful for the target model.
as thediscrete features from texts (i.e., words), embeddedinto the high-dimensional continuous space where.
597their contexts are encoded, cannot cover the wholespace, the uncovered space also should be carefullyconsidered for any target tasks.
in this sense, weneed to regularize the out-of-manifold to preventanomalous behavior in that space, which is espe-cially important for a large pre-trained contextualembedding space..acknowledgments.
this work was supported by the nrf grant fundedby the msit (no.
2020r1a2b5b03097210), andthe iitp grant funded by the msit (no.
2018-0-00584, 2019-0-01906)..references.
yoshua bengio, fr´ed´eric bastien, arnaud bergeron,nicolas boulanger–lewandowski, thomas breuel,youssouf chherawala, moustapha cisse, myriamcˆot´e, dumitru erhan, jeremy eustache, xavier glo-rot, xavier muller, sylvain pannetier lebeuf, raz-van pascanu, salah rifai, franc¸ois savard, and guil-laume sicard.
2011. deep learners beneﬁt morefrom out-of-distribution examples.
in proceedingsof the fourteenth international conference on arti-ﬁcial intelligence and statistics, volume 15 of pro-ceedings of machine learning research, pages 164–172, fort lauderdale, fl, usa.
jmlr workshopand conference proceedings..xingyu cai, jiaji huang, yuchen bian, and kennethchurch.
2021.isotropy in the contextual embed-ding space: clusters and manifolds.
in internationalconference on learning representations..jiaao chen, zichao yang, and diyi yang.
2020. mix-text: linguistically-informed interpolation of hid-den space for semi-supervised text classiﬁcation.
inproceedings of the 58th annual meeting of the asso-ciation for computational linguistics, pages 2147–2157, online.
association for computational lin-guistics..kevin clark, minh-thang luong, quoc v. le, andchristopher d. manning.
2020.electra: pre-training text encoders as discriminators rather thanin international conference on learn-generators.
ing representations..jacob devlin, ming-wei chang, kenton lee, andkristina toutanova.
2019. bert: pre-training ofdeep bidirectional transformers for language under-in proceedings of the 2019 conferencestanding.
of the north american chapter of the associationfor computational linguistics: human languagetechnologies, volume 1 (long and short papers),pages 4171–4186, minneapolis, minnesota.
associ-ation for computational linguistics..kawin ethayarajh.
2019. how contextual are contex-tualized word representations?
comparing the geom-etry of bert, elmo, and gpt-2 embeddings.
inproceedings of the 2019 conference on empiricalmethods in natural language processing and the9th international joint conference on natural lan-guage processing (emnlp-ijcnlp), pages 55–65,hong kong, china.
association for computationallinguistics..chengyue gong, di he, xu tan, tao qin, liwei wang,and tie-yan liu.
2018. frage: frequency-agnosticin advances in neural infor-word representation.
mation processing systems, volume 31, pages 1334–1345. curran associates, inc..hongyu guo.
2020..nonlinear mixup: out-of-manifold data augmentation for text classiﬁcation.
in proceedings of the aaai conference on artiﬁcialintelligence, volume 34, pages 4044–4051..hongyu guo, yongyi mao, and richong zhang.
2019a.
augmenting data with mixup for sentence clas-arxiv preprintsiﬁcation: an empirical study.
arxiv:1905.08941..hongyu guo, yongyi mao, and richong zhang.
2019b.
mixup as locally linear out-of-manifold regulariza-tion.
in proceedings of the aaai conference on ar-tiﬁcial intelligence, volume 33, pages 3714–3722..diederik p. kingma and max welling.
2014. auto-in 2nd internationalencoding variational bayes.
conference on learning representations,iclr2014, banff, ab, canada, april 14-16, 2014, con-ference track proceedings..xiaodong liu, pengcheng he, weizhu chen, and jian-feng gao.
2019a.
multi-task deep neural networksfor natural language understanding.
in proceedingsof the 57th annual meeting of the association forcomputational linguistics, pages 4487–4496, flo-rence, italy.
association for computational linguis-tics..yinhan liu, myle ott, naman goyal, jingfei du, man-dar joshi, danqi chen, omer levy, mike lewis,luke zettlemoyer, and veselin stoyanov.
2019b.
roberta: a robustly optimized bert pretraining ap-proach.
arxiv preprint arxiv:1907.11692..nathan ng, kyunghyun cho, and marzyeh ghassemi.
2020. ssmba: self-supervised manifold based dataaugmentation for improving out-of-domain robust-in proceedings of the 2020 conference onness.
empirical methods in natural language process-ing (emnlp), pages 1268–1283, online.
associa-tion for computational linguistics..colin raffel, noam shazeer, adam roberts, kather-ine lee, sharan narang, michael matena, yanqizhou, wei li, and peter j. liu.
2020. exploringthe limits of transfer learning with a uniﬁed text-to-text transformer.
journal of machine learning re-search, 21(140):1–67..598a preprocessing decisions, modelparameters and other details.
we list the minor implementation details but usefulfor reproducing our experiments..• the train/validation split is implemented us-ing train_test_split function in scikit-learn with seed 42..• the bert-base-uncased tokenizer pro-vided by huggingface is used to split the sen-tence..• we take the ﬁrst 256 tokens for the sentence.
which length is longer than 256..• the embedding table in the pre-trained weight.
is frozen for all experiments..• the data cleaning process in eda deterioratesthe performance, so we omit that process.
• due to the different optimization variables forthe two objectives, we perform the backwardprocess twice and update the parameter..b hyper-parameter search.
since performing grid search on all datasets is intol-erable due to the lack of computational resources,we perform different conﬁgurations on one smalldataset.
the candidate for the embedding layerfor the generator (mg) is [0, 2, 4, 6, 8, 10] and thecandidate for the embedding layer for the discrimi-nator (md) is [0, 2, 4, 6, 8, 10, 12].
for the case thediscriminator could not be trained well, e.g.
thediscriminator loss does not decrease at all, we in-crease e to give more weight to the discriminatorloss.
for all the experiments, we ﬁx the mg and mdand manually change the e to make the discrimina-tor classify the embedding.
the hyper-parameterchoices are summarized in table 3..hyper-parameter.
value.
embedding space for the generator (mg)embedding space for the discriminator (md)coefﬁcient for two objectives (e).
3121 (or 1.5).
table 3: hyper-parameter conﬁguration for the experi-ment.
nils reimers and iryna gurevych.
2019. sentence-bert: sentence embeddings using siamese bert-networks.
in proceedings of the 2019 conference onempirical methods in natural language processingand the 9th international joint conference on natu-ral language processing (emnlp-ijcnlp), pages3982–3992, hong kong, china.
association forcomputational linguistics..anna rogers, olga kovaleva, and anna rumshisky.
2021. a primer in bertology: what we know abouthow bert works.
transactions of the association forcomputational linguistics, 8:842–866..lichao sun, congying xia, wenpeng yin, tingtingliang, philip yu, and lifang he.
2020. mixup-transformer: dynamic data augmentation for nlptasks.
in proceedings of the 28th international con-ference on computational linguistics, pages 3436–3440, barcelona, spain (online).
international com-mittee on computational linguistics..joshua b tenenbaum, vin de silva, and john clangford.
2000. a global geometric frameworkscience,for nonlinear dimensionality reduction.
290(5500):2319–2323..ashish vaswani, noam shazeer, niki parmar, jakobuszkoreit, llion jones, aidan n gomez, ł ukaszkaiser, and illia polosukhin.
2017. attention is allyou need.
in advances in neural information pro-cessing systems, volume 30, pages 5998–6008.
cur-ran associates, inc..vikas verma, alex lamb, christopher beckham, amirnajaﬁ, ioannis mitliagkas, david lopez-paz, andyoshua bengio.
2019. manifold mixup: better rep-resentations by interpolating hidden states.
in pro-ceedings of the 36th international conference onmachine learning, volume 97 of proceedings of ma-chine learning research, pages 6438–6447.
pmlr..jason wei and kai zou.
2019. eda: easy data aug-mentation techniques for boosting performance onin proceedings of thetext classiﬁcation tasks.
2019 conference on empirical methods in natu-ral language processing and the 9th internationaljoint conference on natural language processing(emnlp-ijcnlp), pages 6382–6388, hong kong,china.
association for computational linguistics..qizhe xie, zihang dai, eduard hovy, minh-thang lu-ong, and quoc v le.
2019. unsupervised data aug-mentation for consistency training.
arxiv preprintarxiv:1904.12848..hongyi zhang, moustapha cisse, yann n. dauphin,and david lopez-paz.
2018. mixup: beyond empir-ical risk minimization.
in international conferenceon learning representations..xiang zhang, junbo zhao, and yann lecun.
2015.character-level convolutional networks for text clas-siﬁcation.
in advances in neural information pro-cessing systems, volume 28, pages 649–657.
curranassociates, inc..599