analyzing the source and target contributionsto predictions in neural machine translation.
elena voita1,2.
rico sennrich3,1.
ivan titov1,2.
1university of edinburgh, scotland.
2university of amsterdam, netherlands.
3university of zurich, switzerland.
lena-voita@hotmail.com.
sennrich@cl.uzh.ch.
ititov@inf.ed.ac.uk.
abstract.
in neural machine translation (and, more gen-erally, conditional language modeling),thegeneration of a target token is inﬂuenced bytwo types of context: the source and the pre-ﬁx of the target sequence.
while many at-tempts to understand the internal workings ofnmt models have been made, none of themexplicitly evaluates relative source and targetcontributions to a generation decision.
we ar-gue that this relative contribution can be evalu-ated by adopting a variant of layerwise rele-vance propagation (lrp).
its underlying ‘con-servation principle’ makes relevance propaga-tion unique: differently from other methods, itevaluates not an abstract quantity reﬂecting to-ken importance, but the proportion of each to-ken’s inﬂuence.
we extend lrp to the trans-former and conduct an analysis of nmt mod-els which explicitly evaluates the source andtarget relative contributions to the generationprocess.
we analyze changes in these contribu-tions when conditioning on different types ofpreﬁxes, when varying the training objectiveor the amount of training data, and during thetraining process.
we ﬁnd that models trainedwith more data tend to rely on source informa-tion more and to have more sharp token contri-butions; the training process is non-monotonicwith several stages of different nature.1.
1.introduction.
with the success of neural approaches to natu-ral language processing, analysis of nlp modelshas become an important and active topic of re-search.
in nmt, approaches to analysis includeprobing for linguistic structure (belinkov et al.,2017; conneau et al., 2018), evaluating via con-trastive translation pairs (sennrich, 2017; burlotand yvon, 2017; rios gonzales et al., 2017; tang.
1we release the code at https://github.com/.
lena-voita/the-story-of-heads..et al., 2018), inspecting model components, suchas attention (ghader and monz, 2017; voita et al.,2018; tang et al., 2018; raganato and tiedemann,2018; voita et al., 2019) or neurons (dalvi et al.,2019; bau et al., 2019), among others..unfortunately, although a lot of work on modelanalysis has been done, a question of how thenmt predictions are formed remains largely open.
namely, the generation of a target token is deﬁnedby two types of context, source and target, but thereis no method which explicitly evaluates the rela-tive contribution of source and target to a givenprediction.
the ability to measure this relativecontribution is important for model understandingsince previous work showed that nmt models of-ten fail to effectively control information ﬂow fromsource and target contexts.
for example, addingcontext gates to dynamically control the inﬂuenceof source and target leads to improvement for bothrnn (tu et al., 2017; wang et al., 2018) and trans-fomer (li et al., 2020) models.
a more popularexample is a model’s tendency to generate hallu-cinations (ﬂuent but inadequate translations); it isusually attributed to the inappropriately strong in-ﬂuence of target context.
several works observedthat, when hallucinating, a model fails to properlyuse source: it produces a deﬁcient attention matrix,where almost all the probability mass is concen-trated on uninformative source tokens (eos andpunctuation) (lee et al., 2018; berard et al., 2019).
we argue that a natural way to estimate howthe source and target contexts contribute to gen-eration is to apply layerwise relevance propaga-tion (lrp) (bach et al., 2015) to nmt models.
lrp redistributes the information used for a predic-tion between all input elements keeping the totalcontribution constant.
this ‘conservation principle’makes relevance propagation unique: differentlyfrom other methods estimating inﬂuence of individ-ual tokens (alvarez-melis and jaakkola, 2017; he.
proceedingsofthe59thannualmeetingoftheassociationforcomputationallinguisticsandthe11thinternationaljointconferenceonnaturallanguageprocessing,pages1126–1140august1–6,2021.©2021associationforcomputationallinguistics1126et al., 2019a; ma et al., 2018), lrp evaluates notan abstract quantity reﬂecting a token importance,but the proportion of each token’s inﬂuence..we extend one of the lrp variants to the trans-former and conduct the ﬁrst analysis of nmt mod-els which explicitly evaluates the source and tar-get relative contributions to the generation process.
we analyze changes in these contributions whenconditioning on different types of preﬁxes (refer-ence, generated by a model or random translations),when varying training objective or the amount oftraining data, and during the training process.
weshow that models suffering from exposure bias aremore prone to over-relying on target history (andhence to hallucinating) than the ones where theexposure bias is mitigated.
when comparing mod-els trained with different amount of data, we ﬁndthat extra training data teaches a model to rely onsource information more heavily and to be moreconﬁdent in the choice of important tokens.
whenanalyzing the training process, we ﬁnd that changesin training are non-monotonic and form several dis-tinct stages (e.g., stages changing direction fromdecreasing inﬂuence of source to increasing)..our key contributions are as follows:.
• we show how to use lrp to evaluate the rela-tive contribution of source and target to nmtpredictions;.
• we analyze how the contribution of source andtarget changes when conditioning on differenttypes of preﬁxes: reference, generated by amodel or random translations;.
• by looking at the contributions when condi-tioning on random preﬁxes, we observe thatmodels suffering from exposure bias are moreprone to over-relying on target history (andhence to hallucinating);.
original lrp version was developed for computervision models (bach et al., 2015) and is not directlyapplicable to the transformer (e.g., to the attentionlayers).
in this section, we explain the general ideabehind lrp, specify which of the existing lrpvariants we use, and show how to extend lrp tothe nmt transformer model.2.
2.1 general idea: conservation principle.
in its general form, lrp assumes that the modelcan be decomposed into several layers of compu-tation.
the ﬁrst layer are the inputs (for example,the pixels of an image or tokens of a sentence), thelast layer is the real-valued prediction output ofthe model f .
the l-th layer is modeled as a vectorx(l) = (x(l)i=1 with dimensionality v (l).
layer-wise relevance propagation assumes that we have arelevance score r(l+1)for each dimension x(l+1)of the vector x at layer l + 1. the idea is to ﬁnd arelevance score r(l)i of theiprevious layer l such that the following holds:.
for each dimension x(l).
i )v (l).
i.i.f = ... =.
(cid:88).
r(l+1)i.
=.
(cid:88).
r(l).
i = ... =.
(cid:88).
r(1).
i.
(1).
i.i.i.this equation represents a conservation prin-ciple, which lrp exploits to back-propagate theprediction.
intuitively, this means that the totalcontribution of neurons at each layer is constant..j.
2.2 redistribution rulesassume that we know the relevance r(l+1)of a neu-ron j at network layer l+1 for the prediction f (x).
then we would like to decompose this relevanceinto messages r(l,l+1)sent from the neuron j atlayer l + 1 to each of its input neurons i at layer l.for the conservation principle to hold, these mes-sages r(l,l+1).
have to satisfy the constraint:.
i←j.
i←j.
r(l+1)j.
=.
(cid:88).
r(l,l+1)i←j.
..• we ﬁnd that (i) with more data, models rely onsource information more and have more sharptoken contributions, (ii) the training processis non-monotonic with several distinct stages..ithen we can deﬁne the relevance of a neuron i atlayer l by summing all messages from neurons atlayer (l + 1):.
2 layer-wise relevance propagation.
r(l).
i =.
r(l,l+1)i←j.
..(cid:88).
j.layer-wise relevance propagation is a frameworkwhich decomposes the prediction of a deep neuralnetwork computed over an instance, e.g.
an imageor sentence, into relevance scores for single inputdimensions of the sample such as subpixels of animage or neurons of input token embeddings.
the.
equations (2) and (3) deﬁne the propagation of rele-vance from layer l+1 to layer l. the only thing thatis missing is speciﬁc formulas for computing the.
2previous work applying one of the lrp variants tonmt (ding et al., 2017; voita et al., 2019) do not describeextensions beyond the original lrp rules (bach et al., 2015)..(2).
(3).
1127messages r(l,l+1)i←jhas the following structure:.
.
usually, the message r(l,l+1).
i←j.
i←j = vijr(l+1)r(l,l+1).
j.,.
vij = 1..(4).
(cid:88).
i.several versions of lrp satisfying equation (4)(and, therefore, the conservation principle) havebeen introduced: lrp-ε, lrp-αβ and lrp-γ (bach et al., 2015; binder et al., 2016; montavonet al., 2019).
we use lrp-αβ (bach et al., 2015;binder et al., 2016), which deﬁnes relevances ateach step in such a way that they are positive..the αβ-rule.
rule for relevance propagation:let us consider the simplest case of linear layerswith non-linear activation functions, namely.
zij = x(l).
i wij, zj =.
zij + bi, x(l+1).
j = g(zj),.
(cid:88).
i.where wij is a weight connecting the neuron x(l)ito neuron x(l+1), bj is a bias term, and g is a non-linear activation function.
let.
j.z+j =.
ij + b+z+j ,.
z−j =.
ij + b−z−j ,.
(cid:88).
i.
(cid:88).
i.where (cid:3)+ = max(0, (cid:3)) and (cid:3)− = min(0, (cid:3)).
then the αβ-rule (bach et al., 2015; binder et al.,2016) is given by the equation.
i←j = r(l+1)r(l,l+1).
j.
·.
α ·.
+ β ·.
,.
(5).
(cid:32).
z+ijz+j.
(cid:33).
z−ijz−j.where α+β = 1. note that all terms in the bracketsare always positive: negative signs of z−j and z−ijcancel out when evaluating the ratio..this propagation method allows to control man-ually the importance of positive and negative evi-dence by choosing different α and β. for example,α, β = 12 treats positive and negative contributionsas equally important, while α = 1, β = 0 consid-ers only positive contributions.
in our experiments,both versions lead to the same observations..note that (5) is directly applicable to all layersfor which there exist functions gj and hij such that.
x(l+1)j.
= gj.
(cid:33).
hij(x(l)i ).
..(cid:32).
(cid:88).
i.
(6).
these layers include linear, convolutional and max-pooling operations.
additionally, pointwise mono-tonic activation functions gj (e.g., relu) are ig-nored by lrp (bach et al., 2015)..propagating relevance through attention layers.
for the structures that do not ﬁt the form (6), theweighting vij can be obtained by performing a ﬁrstorder taylor expansion of a neuron x(l+1)(bachet al., 2015; binder et al., 2016)..j.for attention layers in the transformer, we ex-tend the approach by binder et al.
(2016).
namely,let x(l+1)= f (x(l)), f (x) = f (x1, .
.
.
, xn).
jthen by taylor expansion at some point ˆx =(ˆx1, .
.
.
, ˆxn), we get.
f (ˆx) ≈ f (x(l)) +.
(x(l)) · (ˆxi − x(l).
i ),.
(cid:88).
i←j.
∂f∂xi.
x(l+1)j = f (x(l)) ≈ f (ˆx)+.
(x(l))·(x(l).
i −ˆxi)..(cid:88).
i←j.
∂f∂xi.
elements of the sum can be assigned to incom-ing neurons, and the zero-order term can be redis-tributed equally between them.
this leads to thefollowing decomposition:.
zij =.
f (ˆx) +.
(x(l)) · (x(l).
i − ˆxi)..(7).
1n.∂f∂xi.
we use the zero vector in place of ˆx.
equation (7),along with the standard redistribution rules (5), de-ﬁnes relevance propagation for complex non-linearlayers.
in the transformer, we apply equation (7)to the softmax operations in the attention layers;all other operations inside the attention layers arelinear functions, and the rule (5) can be used..2.3 lrp for conditional language models.
given a source sequence x = (x1, .
.
.
, xs) and atarget sequence y = (y1, .
.
.
, yt ), standard autore-gressive nmt models (or, in a more broad sense,conditional language models) are trained to predictwords in the target sequence, word by word.
for-mally, at each generation step such models predictp(yt|x1:s, y1:t−1) relying on both source tokensx1:s and already generated target tokens y1:t−1.
using lrp, we evaluate relative contribution of alltokens, source and target, to the current prediction..propagating through decoder and encoder.
atﬁrst glance, it can be unclear how to apply a layer-wise method to a not completely layered architec-ture (such as encoder-decoder).
this, however, israther straightforward and is done in two steps:.
1. total relevance is propagated through the de-coder.
since the decoder uses representations.
1128from the ﬁnal encoder layer, part of the rele-vance ‘leaks’ to the encoder; this happens ateach decoder layer;.
2. relevance leaked to the encoder is propagated.
through the encoder layers..the total contribution of neurons in each decoderlayer is not preserved (part of the relevance leaksto the encoder), but the total contribution of alltokens – across the source and the target preﬁx –remains equal to the model prediction..we evaluate relevance of input neurons to thetop-1 logit predicted by a model.
then token rele-vance (or its contribution) is the sum of relevancesof its neurons..notation.
without loss of generality, we can as-sume that the total relevance for each predictionequals 1.3 let us denote by rt(xi) and rt(yj) thecontribution of source token xi and target token yjto the prediction at generation step t, respectively.
then source and target contributions are deﬁned asrt(source) = (cid:80)i.rt(xi), rt(target) =.
t−1(cid:80)j=1.
rt(yj)..note that ∀ t rt(source)+ rt(target) = 1;.
r1(source) = 1, r1(target) = 0,∀j ≥ t rt(yj) = 0..and.
3 experimental setting.
model.
we follow the setup of transformer basemodel (vaswani et al., 2017) with the standardtraining setting.
more details on hyperparametersand the optimizer can be found in the appendix..data.
we use random subsets of the wmt14 en-fr dataset of different size: 1m, 2.5m, 5m, 10m,20m, 30m sentence pairs.
in sections 4 and 7,we report results for the model trained on the 1msubset.
in section 6, we show how the resultsdepend on the amount of training data..evaluating lrp.
the αβ-lrp we use requireschoosing values for α and β, α + β = 1. wetried treating positive and negative contributionsas equally important (α = β = 12 ), or consideringonly positive contributions (α = 1, β = 0).
theobserved patterns in behavior were the same forthese two versions.
in the main text, we use α = 1;in the appendix, we provide results for α = β = 12 .
3more formally, if we evaluate relevance for top-1 logitpredicted by a model, then the total relevance is equal to thevalue of this logit.
however, the conservation principle allowsus to assume that this logit is equal to 1 and to consider relativecontributions..(a).
(b).
figure 1: (a) contribution of the whole source at eachgeneration step; (b) total contribution of source tokensat each position to the whole target sentence..reporting results.
all presented results are av-eraged over an evaluation dataset of 1000 sentencepairs.
in each evaluation dataset, all examples havethe same number of tokens in the source, as well asin the target (e.g., 20 source and 23 target tokens;the exact number for each experiment is clear fromthe results).4.
4 getting acquainted.
in this section, we explain general patterns in modelbehavior and illustrate the usage of lrp by evaluat-ing different statistics within a single model.
later,we will show how these results change when vary-ing the amount of training data (section 6) andduring model training (section 7)..4.1 changes in contributions.
here we evaluate changes in the source contri-bution during generation, and in contributions ofsource tokens at different positions to entire output..source −→ target(k).
for each generationstep t, we evaluate total contribution of sourcert(source).
note that this is equivalent to evaluat-ing total contribution of preﬁx since rt(preﬁx) =1 − rt(source) (section 2.3)..results are shown in figure 1(a).5 we see that,during the generation process, the inﬂuence ofsource decreases (or, equivalently, the inﬂuenceof the preﬁx increases).
this is expected: witha longer preﬁx, the model has less uncertainty indeciding which source tokens to use, but needs tocontrol more for ﬂuency.
there is also a large dropof source inﬂuence for the last token: apparently, to.
4note that we have to ﬁx the number of tokens in the sourceand target to get reliable comparisons.
we choose sentences oflength 20 and 23 because these are among the most frequentsentence lengths in the dataset.
we also looked at sentenceswith 16, 25, 29 tokens – observed patterns were the same..5since the ﬁrst token is always generated solely relying on.
the source, we plot starting from the second token..1129(a).
(b).
(a).
(b).
figure 2: for each generation step, the ﬁgure showsentropy of (a) source, (b) target contributions..generate the eos token, the model relies on preﬁxmuch more than when generating other tokens..source(k) −→ target.
now we want to under-stand if there is a tendency to use source tokensat certain positions more than tokens at the oth-ers.
for each source token position k, we evaluateits total contribution to the whole target sequence.
to eliminate the effect of decreasing source inﬂu-ence during generation, at each step t we normalizesource contributions rt(xk) over the total contribu-tion of source at this step rt(source).
formally, for.
t(cid:80)t=1.
the k-th token we evaluate.
rt(xk)/rt(source)..for convenience, we multiply the result by st : thismakes the average total contribution of each tokenequal to 1..figure 1(b) shows that, on average, source to-kens at earlier positions inﬂuence translations morethan tokens at later ones.
this may be because thealignment between english and french languagesis roughly monotonic.
we leave for future workinvestigating the changes in this behavior for lan-guage pairs with more complex alignment (e.g.,english-japanese)..4.2 entropy of contributions.
now let us look at how ‘sharp’ contributions ofsource or target tokens are at different genera-tion steps.
for each step t, we evaluate entropyof (normalized) source or target contributions:{rt(xi)/rt(source)}si=1 or {rt(yj)/rt(target)}t−1j=1..entropy of source contributions.
figure 2(a)shows that during generation, entropy increasesuntil approximately 2/3 of the translation is gener-ated, then decreases when generating the remainingpart.
interestingly, for the last punctuation markand the eos token, entropy of source contributionsis very high: the decision to complete the sentence.
(c).
(d).
figure 3: (a, c) contribution of source, (b, d) entropy ofsource contributions..requires broader context..entropy of target contributions.
figure 2(b)shows that entropy of target contributions is higherfor longer preﬁxes.
this means that the model doesuse longer contexts in a non-trivial way..4.3 reference, model and random preﬁxes.
let us now look at how model behavior changeswhen feeding different types of preﬁxes: preﬁxesof reference translations, translations generated bythe model, and random sentences in the target lan-guage.6 as in previous experiments, we evaluaterelevance for top-1 logit predicted by the model..reference vs model preﬁxes.
when feedingmodel-generated preﬁxes, the model uses sourcemore (figure 3(a)) and has more focused sourcecontributions (lower entropy in figure 3(b)) thanwhen generating the reference.
this may bebecause model-generated translations are ‘eas-ier’ than references.
for example, beam searchtranslations contain fewer rare tokens (burlot andyvon, 2018; ott et al., 2018), are simpler syntacti-cally (burlot and yvon, 2018) and, according to thefuzzy reordering score (talbot et al., 2011), modeltranslations have signiﬁcantly less reordering com-pared to the real parallel sentences (zhou et al.,2020).
as we see from our experiments, these sim-pler model-generated preﬁxes allow for the model.
6random preﬁxes come from the same evaluation set, but.
with shufﬂed target sentences..1130to rely on the source more and to be more conﬁdentwhen choosing relevant source tokens..reference vs random preﬁxes.
results for ran-dom sentence preﬁxes are given in figures 3c, 3d.
the reaction to random preﬁxes helps us study theself-recovery ability of nmt models.
previouswork has found that models can fall into a hallucina-tion mode where “the decoder ignores context fromthe encoder and samples from its language mode”(koehn and knowles, 2017; lee et al., 2018).
incontrast, he et al.
(2019b) found that a languagemodel is able to recover from artiﬁcially distortedhistory input and generate reasonable samples..our results show evidence for both.
at the be-ginning of the generation process, the model tendsto rely more on the source context when given arandom preﬁx compared to the reference preﬁx,indicating a self-recovery mode.
however, whenthe preﬁx becomes longer, the model choice shiftstowards ignoring the source and relying more onthe target: figure 3c shows a large drop of sourceinﬂuence for later positions.
figure 3d also showsthat with a random preﬁx, the entropy of sourcecontributions is high and is roughly constant..5 exposure bias and source.
contributions.
the results in the previous section agree with someobservations made in previous work studying self-recovery and hallucinations.
in this section, weillustrate more explicitly how our methodology canbe used to shed light on the effects of exposure biasand training objectives..wang and sennrich (2020) empirically link thehallucination mode to exposure bias (ranzato et al.,2016), i.e.
the mismatch between the gold historyseen at training time, and the (potentially erro-neous) model-generated preﬁxes at test time.
theauthors hypothesize that exposure bias leads toan over-reliance on target history, and show thatminimum risk training (mrt), which does notsuffer from exposure bias, reduces hallucinations.
however, they did not directly measure this over-reliance on target history.
our method is able to di-rectly test whether there is indeed an over-relianceon the target history with mle-trained models, andmore robust inclusion of source context with mrt.
we also consider a simpler heuristic, word dropout,which we hypothesize to have a similar effect..(a).
(b).
figure 4: contribution of source (a) and entropy ofsource (b) with model-generated preﬁxes..minimum risk training(shen et al., 2016) isa sentence-level objective that inherently avoids ex-posure bias.
it minimises the expected loss (‘risk’)with respect to the posterior distribution:.
r(θ) =.
(cid:88).
(cid:88).
p (˜y|x, θ)∆(˜y, y),.
(x,y).
˜y∈y(x).
where y(x) is a set of candidate translations for x,∆(˜y, y) is the discrepancy between the model pre-diction ˜y and the gold translation y (e.g., a nega-tive smoothed sentence-level bleu).
more detailson the method can be found in shen et al.
(2016)or edunov et al.
(2018); training details for ourmodels are in the appendix..word dropoutis a simple data augmentationtechnique.
during training, it replaces some of thetokens with a special token (e.g., unk) or a ran-dom token (in our experiments, we replace 10% ofthe tokens with random).
when used on the targetside, it may serve as the simplest way to alleviateexposure bias: it exposes a model to somethingother than gold preﬁxes.
this is not true when usedon the source side, but for analysis, we considerboth variants..5.1 experiments.
we consider two types of preﬁxes: model-generated and random.
random preﬁxes are ourmain interest here.
we feed preﬁxes that are ﬂu-ent but unrelated to the source and look whethera model is likely to fall into a language modelingregime, i.e., to what extent it ignores the source.
for model-generated preﬁxes, we do not expect tosee large differences in contributions: this mode is‘easy’ for the model and the source contributionsare high (see section 4.3).
the results are shownin figures 4 and 5..model-generated preﬁxes.
mrt causes moreprominent changes in contributions (figure 4).
we.
1131(a).
(b).
figure 5: contribution of source (a) and entropy ofsource (b) with random preﬁxes..figure 6: (a) source contribution, (b) entropy of sourcecontributions.
the arrows show the direction of changewhen increasing data amount.
(for clarity, in (a) thelast two positions (punct.
and eos) are not shown)..see the largest difference in the beginning and theend of the generation process, which may be ex-pected when comparing models trained with token-level and sequence-level objectives.
the directionof change, i.e.
decreasing inﬂuence of source, israther unexpected; we leave a detailed investigationof this behavior to future work.
for word dropout,changes in the amount of contributions are less no-ticeable; we see, however, that target-side worddropout makes the model more conﬁdent in thechoice of relevant source tokens (figure 4b)..random preﬁxes.
we see that, among all mod-els, the mrt model has the highest inﬂuence ofsource (figure 5a) and the most focused sourcecontributions (figure 5b).
this agrees with ourexpectations: by construction, mrt removes ex-posure bias completely.
therefore, it is confusedby random preﬁxes less than other models.
ad-ditionally, this also links to wang and sennrich(2020) who showed that mrt reduces hallucina-tions.
when using word dropout, both its variantsalso increase the inﬂuence of source, but to a muchlesser extent (figure 5a).
as expected, since target-side word dropout slightly reduces exposure bias(in contrast to source-side word dropout), it leadsto a larger increase of source inﬂuence..experiments in this section highlight that themethodology we propose can be applied to studyexposure bias, robustness, and hallucinations, bothin machine translation and more broadly for otherlanguage generation tasks.
in this work, however,we want to illustrate more broadly the potential ofthis approach.
in the following, we will comparemodels trained with varying amounts of data andwill look into the training process..6 data amount.
in this section, we show how the results from sec-tion 4 change when increasing the amount of train-.
ing data.
the observed patterns are the same whenevaluating on datasets with reference translationsor the ones generated by the corresponding model(in each case, all sentences in the evaluation datasethave the same length).
in the main text, we showﬁgures for references..more data =⇒ higher source contribution.
figure 6(a) shows the source contribution at eachgeneration step.
we can see that, generally, mod-els trained with more data rely on source moreheavily.
surprisingly, this increase is not spreadevenly across positions: at approximately 80% ofthe target length, models trained with more data usesource more, but at the last positions, they switchto more actively using the preﬁx..more data =⇒ more focused contributions.
figure 6(b) shows that at each generation step, en-tropy of source contributions decreases with moredata.
this means that with more training data, themodel becomes more conﬁdent in the choice ofimportant tokens.
in the appendix, we show thatthis is also the case for target contributions..7 training stages.
now we turn to analyzing the training process of annmt model.
speciﬁcally, we look at the changesin how the predictions are formed: changes in theamount of source/target contributions and in theentropy of these contributions.
our ﬁndings aresummarized in figure 7. in the following, we ex-plain them in more detail.
in section 7.1, we drawconnections between our training stages (shownin figure 7) and the ones found in previous workfocused on validating the lottery ticket hypothesis..contributions converge early.
first, we eval-i.e.,uate how fast the contributions converge,how quickly a model understands which tokensforare the most.
important for prediction..1132figure 7: training timeline..(a).
(b).
(c).
(d).
figure 8: training process: (a) convergence of contributions, (b) source contribution, (c-d) entropy of source andtarget contributions.
the model trained on 1m subsample of wmt14 en-fr dataset.
the results are averaged overtarget positions and evaluation examples..this, at each generation step t we evaluate thekl divergence in token inﬂuence distributions(rt(x1), .
.
.
, rt(xs), rt(y1), .
.
.
, rt(yt−1))fromthe ﬁnal converged model to the model in train-ing.
figure 8(a) shows that contributions convergeearly.
after approximately 12k batches, the modelis very close to its ﬁnal state in the choice of tokensto rely on for a prediction..changes in training are not monotonic.
fig-ures 8(b-d) show how the amount of source con-tribution and the entropy of source and target con-tributions change in training.
we see that all threeﬁgures have the same distinct stages (shown withvertical lines).
first, source inﬂuence decreases,and both source and target contributions becomemore focused.
in this stage, most of the change hap-pens (figure 8(a)).
in the second stage, the modelalso undergoes substantial change, but all processeschange their direction: source inﬂuence increasesand the model learns to rely on broader context(entropy is increasing).
finally, in the third stage,the direction of changes remains the same, but verylittle is going on – the model slowly converges..these three stages correspond to the ﬁrst threestages shown in figure 7; at this point, the modeltrained on 1m sentence pairs converges.
with moredata (e.g., 20m sentence pairs), we further observedthe next stage (the last one in figure 7), where theentropy of both source and target contributions isdecreasing again.
however, this last stage is much.
slower than the third, and the ﬁnal state does notdiffer much from the end of the third stage..early positions change more.
figures 9(a-b)show how source contributions and their entropychanges for each target position.
we see that earlierpositions are the ones that change most actively: atthese positions, we see the largest decrease at theﬁrst stage and the largest following increase at thesubsequent stages.
if we look at how accuracy foreach position changes in training (figure 10), wesee that at the end of the ﬁrst stage, early tokenshave the highest accuracy.7 this is not surprising:one could expect early positions to train faster be-cause they are observed more frequently in training.
previously such intuition motivated the usage ofsentence length as one of the criteria for curriculumlearning (e.g., kocmi and bojar (2017))..7.1 relation to previous work.
interestingly, our stages in figure 7 agree with theones found by frankle et al.
(2020) for resnet-20trained on cifar-10 when investigating, amongother things, the lottery ticket hypothesis (frankleand carbin, 2019).
their stages were deﬁned basedon the changes in gradient magnitude, in the weightspace, in the performance, and in the effectivenessof rewinding in search of the ‘winning’ subnetwork(for more details on the lottery ticket hypothesis.
7accuracy is the proportion of cases where the correct.
token is the most probable choice..1133not using source.
more recently, as a measure ofindividual token importance, he et al.
(2019a) usedintegrated gradients (sundararajan et al., 2017)..in machine translation, lrp was previously usedfor visualization (ding et al., 2017) and to ﬁnd themost important attention heads in the transformer’sencoder (voita et al., 2019).
similar to our work,voita et al.
(2019) evaluated lrp on average overa dataset (and not for a single prediction) to extractpatterns in model behaviour.
both works used themore popular ε-lrp, while for our analysis, the αβ-lrp was more suitable (section 2).
for languagemodeling, calvillo and crocker (2018) use lrp toevaluate relevance of neurons in rnns for a smallsynthetic setting..9 conclusions.
we show how to use lrp to evaluate the relativecontributions of source and target to nmt predic-tions.
we illustrate the potential of this approachby analyzing changes in these contributions whenconditioning on different types of preﬁxes (refer-ences, model predictions or random translations),when varying training objectives or the amountof training data, and during the training process.
some of our ﬁndings are: (1) models trained withmore data rely on source information more andhave more sharp token contributions; (2) the train-ing process is non-monotonic with several distinctstages.
these stages agree with the ones foundin previous work focused on validating the lotteryticket hypothesis, which suggests future investi-gation of this connection.
additionally, we showthat models suffering from exposure bias are moreprone to over-relying on target history (and henceto hallucinating) than the ones where the exposurebias is mitigated.
in future work, our methodologycan be used to measure the effects of different andnovel training regimes on the balance of source andtarget contributions..acknowledgments.
we would like to thank the anonymous reviewersfor their comments.
the work is partially supportedby the european research council (titov, ercstg broadsem 678254), dutch nwo (titov, vidi639.022.518) and eu horizon 2020 (gourmet,no.
825299).
lena is supported by the facebookphd fellowship.
rico sennrich acknowledgessupport of the swiss national science foundation(mutamur; no.
176727)..(a) source contribution at each target position.
(b) entropy of source contributions.
figure 9: changes in training for each target position;each line corresponds to a model state.
the arrowsshow the direction of change when the training pro-in the ﬁgures, all stages are shown, but thegresses.
stages of interest are highlighted more prominently..figure 10: accuracy change for each target position;each line corresponds to a model state.
in the ﬁgures,all stages are shown, but the stages of interest are high-lighted more prominently..and the rewinding, see the work by frankle et al.
(2019)).
comparing the stages by frankle et al.
(2020) with ours, we see that (1) their relative sizesin the corresponding timelines match well, (2) therewinding starts to be effective at the third stage;for our model, this is when the contributions havealmost converged.
in future work, it would beinteresting to further investigate this relation..8 additional related work.
to estimate the inﬂuence of source to an nmt pre-diction, ma et al.
(2018) trained an nmt modelwith an auxiliary second decoder where the en-coder context vector was masked.
then the sourceinﬂuence was measured as the kl divergence be-tween predictions of the two decoders.
however,the ability of an auxiliary decoder to generate simi-lar distribution is not equivalent to the main model.
1134references.
david alvarez-melis and tommi jaakkola.
2017. acausal framework for explaining the predictions ofin pro-black-box sequence-to-sequence models.
ceedings of the 2017 conference on empirical meth-ods in natural language processing, pages 412–421, copenhagen, denmark.
association for com-putational linguistics..sebastian bach, alexander binder, grégoire mon-tavon, frederick klauschen, klaus-robert müller,and wojciech samek.
2015. on pixel-wise explana-tions for non-linear classiﬁer decisions by layer-wiserelevance propagation.
plos one, 10(7):e0130140..anthony bau, yonatan belinkov, hassan sajjad, nadirdurrani, fahim dalvi, and james glass.
2019. iden-tifying and controlling important neurons in neuralmachine translation.
in international conference onlearning representations, new orleans..yonatan belinkov, nadir durrani, fahim dalvi, has-san sajjad, and james glass.
2017. what do neu-ral machine translation models learn about morphol-ogy?
in proceedings of the 55th annual meeting ofthe association for computational linguistics (vol-ume 1: long papers), pages 861–872, vancouver,canada.
association for computational linguistics..alexandre berard,.
ioan calapodescu, and clauderoux.
2019. naver labs europe’s systems for thewmt19 machine translation robustness task.
inproceedings of the fourth conference on machinetranslation (volume 2: shared task papers, day1), pages 526–532, florence, italy.
association forcomputational linguistics..alexander binder, grégoire montavon, sebastianlapuschkin, klaus-robert müller, and wojciechsamek.
2016. layer-wise relevance propagation forneural networks with local renormalization layers.
lecture notes in computer science, page 63–71..franck burlot and françois yvon.
2017. evaluatingthe morphological competence of machine transla-tion systems.
in proceedings of the second confer-ence on machine translation, pages 43–55, copen-hagen, denmark.
association for computationallinguistics..franck burlot and françois yvon.
2018. using mono-lingual data in neural machine translation: a system-atic study.
in proceedings of the third conferenceon machine translation: research papers, pages144–155, brussels, belgium.
association for com-putational linguistics..jesús calvillo and matthew crocker.
2018. languageproduction dynamics with recurrent neural networks.
in proceedings of the eight workshop on cognitiveaspects of computational language learning andprocessing, pages 17–26, melbourne.
associationfor computational linguistics..alexis conneau, german kruszewski, guillaume lam-ple, loïc barrault, and marco baroni.
2018. whatyou can cram into a single $&!#* vector: probingsentence embeddings for linguistic properties.
inproceedings of the 56th annual meeting of the as-sociation for computational linguistics (volume 1:long papers), pages 2126–2136, melbourne, aus-tralia.
association for computational linguistics..fahim dalvi, nadir durrani, hassan sajjad, yonatanbelinkov, anthony bau, and james glass.
2019.what is one grain of sand in the desert?
analyz-ing individual neurons in deep nlp models.
in pro-ceedings of the aaai conference on artiﬁcial intel-ligence, volume 33, pages 6309–6317..yanzhuo ding, yang liu, huanbo luan, and maosongsun.
2017. visualizing and understanding neuralmachine translation.
in proceedings of the 55th an-nual meeting of the association for computationallinguistics (volume 1: long papers), pages 1150–1159, vancouver, canada.
association for computa-tional linguistics..sergey edunov, myle ott, michael auli, david grang-ier, and marc’aurelio ranzato.
2018. classicalstructured prediction losses for sequence to se-in proceedings of the 2018 con-quence learning.
ference of the north american chapter of the asso-ciation for computational linguistics: human lan-guage technologies, volume 1 (long papers), pages355–364, new orleans, louisiana.
association forcomputational linguistics..jonathan frankle and michael carbin.
2019. the lot-tery ticket hypothesis: finding sparse, trainable neu-ral networks.
in international conference on learn-ing representations..jonathan frankle, gintare karolina dziugaite,daniel m. roy, and michael carbin.
2019. stabiliz-ing the lottery ticket hypothesis..jonathan frankle, david j. schwab, and ari s. morcos.
2020. the early phase of neural network training.
in international conference on learning represen-tations..hamidreza ghader and christof monz.
2017. whatdoes attention in neural machine translation pay at-in proceedings of the eighth interna-tention to?
tional joint conference on natural language pro-cessing (volume 1: long papers), pages 30–39.
asian federation of natural language processing..shilin he, zhaopeng tu, xing wang, longyue wang,michael lyu, and shuming shi.
2019a.
towardsunderstanding neural machine translation with wordin proceedings of the 2019 confer-importance.
ence on empirical methods in natural languageprocessing and the 9th international joint confer-ence on natural language processing (emnlp-ijcnlp), pages 953–962, hong kong, china.
as-sociation for computational linguistics..1135tianxing he, jingzhao zhang, zhiming zhou, andjames glass.
2019b.
quantifying exposure bias forneural language generation..diederik kingma and jimmy ba.
2015. adam: amethod for stochastic optimization.
in proceedingsof the international conference on learning repre-sentation (iclr 2015)..tom kocmi and ondˇrej bojar.
2017. curriculum learn-ing and minibatch bucketing in neural machine trans-lation.
in proceedings of the international confer-ence recent advances in natural language process-ing, ranlp 2017, pages 379–386, varna, bulgaria.
incoma ltd..philipp koehn and rebecca knowles.
2017. six chal-in proceed-lenges for neural machine translation.
ings of the first workshop on neural machine trans-lation, pages 28–39, vancouver.
association forcomputational linguistics..katherine lee, orhan firat, ashish agarwal, clarafannjiang, and david sussillo.
2018. hallucinationsin neural machine translation..xintong li, lemao liu, rui wang, guoping huang,and max meng.
2020. regularized context gates ontransformer for machine translation.
in proceedingsof the 58th annual meeting of the association forcomputational linguistics.
association for compu-tational linguistics..xutai ma, ke li, and philipp koehn.
2018. an analy-sis of source context dependency in neural machinetranslation..grégoire montavon, alexander binder, sebastianlapuschkin, wojciech samek, and klaus-robertmüller.
2019. layer-wise relevance propagation: anoverview.
in explainable ai: interpreting, explain-ing and visualizing deep learning, pages 193–209.
springer..myle ott, michael auli, david grangier,.
andmarc’aurelio ranzato.
2018. analyzing uncer-in proceed-tainty in neural machine translation.
ings of the 35th international conference on ma-chine learning, volume 80 of proceedings of ma-chine learning research, pages 3956–3965, stock-holmsmässan, stockholm sweden.
pmlr..alessandro raganato and jörg tiedemann.
2018. ananalysis of encoder representations in transformer-in proceedings of thebased machine translation.
2018 emnlp workshop blackboxnlp: analyzingand interpreting neural networks for nlp, pages287–297, brussels, belgium.
association for com-putational linguistics..marc’aurelio ranzato, sumit chopra, michael auli,and wojciech zaremba.
2016. sequence level train-ing with recurrent neural networks..annette rios gonzales, laura mascarell, and rico sen-improving word sense disambigua-nrich.
2017.tion in neural machine translation with sense em-beddings.
in proceedings of the second conferenceon machine translation, pages 11–19, copenhagen,denmark.
association for computational linguis-tics..rico sennrich.
2017. how grammatical is character-level neural machine translation?
assessing mt qual-in proceed-ity with contrastive translation pairs.
ings of the 15th conference of the european chap-ter of the association for computational linguistics:volume 2, short papers, pages 376–382, valencia,spain.
association for computational linguistics..rico sennrich, barry haddow, and alexandra birch.
2016. neural machine translation of rare wordswith subword units.
in proceedings of the 54th an-nual meeting of the association for computationallinguistics (volume 1: long papers), pages 1715–1725, berlin, germany.
association for computa-tional linguistics..shiqi shen, yong cheng, zhongjun he, wei he, huawu, maosong sun, and yang liu.
2016. minimumrisk training for neural machine translation.
in pro-ceedings of the 54th annual meeting of the associa-tion for computational linguistics (volume 1: longpapers), pages 1683–1692, berlin, germany.
asso-ciation for computational linguistics..mukund sundararajan, ankur taly, and qiqi yan.
2017.in pro-axiomatic attribution for deep networks.
ceedings of the 34th international conference onmachine learning, volume 70 of proceedings ofmachine learning research, pages 3319–3328, in-ternational convention centre, sydney, australia.
pmlr..david talbot, hideto kazawa, hiroshi ichikawa, ja-son katz-brown, masakazu seno, and franz och.
2011. a lightweight evaluation framework for ma-chine translation reordering.
in proceedings of thesixth workshop on statistical machine translation,pages 12–21, edinburgh, scotland.
association forcomputational linguistics..gongbo tang, mathias müller, annette rios, and ricosennrich.
2018. why self-attention?
a targetedevaluation of neural machine translation architec-in proceedings of the 2018 conference ontures.
empirical methods in natural language processing,pages 4263–4272, brussels, belgium.
associationfor computational linguistics..gongbo tang, rico sennrich, and joakim nivre.
2018.an analysis of attention mechanisms: the case ofword sense disambiguation in neural machine trans-in proceedings of the third conference onlation.
machine translation: research papers, pages 26–35, belgium, brussels.
association for computa-tional linguistics..1136zhaopeng tu, yang liu, zhengdong lu, xiaohua liu,and hang li.
2017. context gates for neural ma-chine translation.
transactions of the associationfor computational linguistics, 5:87–99..ashish vaswani, noam shazeer, niki parmar, jakobuszkoreit, llion jones, aidan n gomez, lukaszkaiser, and illia polosukhin.
2017. attention is allyou need.
in advances in neural information pro-cessing systems 30, pages 5998–6008, los angeles..elena voita, pavel serdyukov, rico sennrich, and ivantitov.
2018. context-aware neural machine trans-in proceedingslation learns anaphora resolution.
of the 56th annual meeting of the association forcomputational linguistics (volume 1: long papers),pages 1264–1274, melbourne, australia.
associa-tion for computational linguistics..elena voita, david talbot, fedor moiseev, rico sen-nrich, and ivan titov.
2019. analyzing multi-headself-attention: specialized heads do the heavy lift-in proceedings of theing, the rest can be pruned.
57th annual meeting of the association for com-putational linguistics, pages 5797–5808, florence,italy.
association for computational linguistics..chaojun wang and rico sennrich.
2020. on exposurebias, hallucination and domain shift in neural ma-chine translation.
in proceedings of the 58th annualmeeting of the association for computational lin-guistics, pages 3544–3552, online.
association forcomputational linguistics..mingxuan wang, jun xie, zhixing tan, jinsong su,deyi xiong, and chao bian.
2018. neural machinetranslation with decoding history enhanced atten-tion.
in proceedings of the 27th international con-ference on computational linguistics, pages 1464–1473, santa fe, new mexico, usa.
association forcomputational linguistics..chunting zhou, graham neubig, and jiatao gu.
2020. understanding knowledge distillation in non-autoregressive machine translation.
in internationalconference on learning representations..1137a experimental setup.
a.1 data preprocessing.
we use random subsets of the wmt14 en-fr dataset:http://www.statmt.org/wmt14/translation-task.html.
sentences were en-coded using byte-pair encoding (sennrich et al.,2016), with source and target vocabularies of about32000 tokens.
translation pairs were batched to-gether by approximate sequence length.
each train-ing batch contained a set of translation pairs con-taining approximately 160008 source tokens for 1msubsample and 32000 for larger datasets..where y(x) is a set of all possible candidate trans-lations for x, ∆(˜y, y) is the discrepancy betweenthe model prediction ˜y and the gold translation y.since the search space y(x) is exponential, inpractice it is common to use only a subset ofthe full space.
formally, instead of y(x) we uses(x) ∈ y(x), where s(x) is obtained by samplingseveral translations.
the probabilities p (˜y|x, θ)are replaced with the ˜p , which is renormalizedover the subset s:.
˜p (˜y|x, θ, α) =.
p (˜y|x, θ)α.p (y(cid:48)|x, θ)α ..(cid:80)y(cid:48)∈s(x).
a.2 model parameters.
the hyperparameter α is used to control the.
we follow the setup of transformer basemodel (vaswani et al., 2017).
more precisely, thenumber of layers in the encoder and in the decoderis n = 6. we employ h = 8 parallel attention lay-ers, or heads.
the dimensionality of input and out-put is dmodel = 512, and the inner-layer of a feed-forward networks has dimensionality df f = 2048.we use regularization as described in (vaswani.
et al., 2017)..a.3 optimizer.
the optimizer we use is the same as in (vaswaniet al., 2017).
we use the adam optimizer (kingmaand ba, 2015) with β1 = 0.9, β2 = 0.98 andε = 10−9.
we vary the learning rate over thecourse of training, according to the formula:.
lrate = scale · min(step_num−0.5,.
step_num · warmup_steps−1.5).
we use warmup_steps = 16000, scale = 4..we train models till convergence and average 5latest checkpoints.
approximate number of train-ing batches are: 57k for 1m dataset, 220k for 2.5mdataset and 600k for the rest..b minimum risk training.
b.1 background.
sharpness of the distribution..b.2 experimental setting.
to choose the setting, we mostly relied on previ-ous work (shen et al., 2016; edunov et al., 2018).
model is pre-trained with the token-level objectivemle and then ﬁne-tuned with mrt; the ﬁne-tuningstage is approximately one epoch..candidate translations.
the translations aresampled using standard random sampling withouttemperature.
following shen et al.
(2016), we takethe large number of candidates; speciﬁcally, we use50 translations and add a reference to the subset.
while edunov et al.
(2018) report that adding thereference to the set of candidates hurts quality, inpreliminary experiments we found that this was notthe case for our setting..measure of discrepancy.
the measure of dis-crepancy, ∆(˜y, y), is a negative smoothed sentence-level bleu..batch size.
on average, the number of examples(where an example is a translation pair along withall candidates) is the same as in training of thebaseline models.
this is achieved by accumulatinggradients for several steps and making an update..other parameters.
following (wang and sen-nrich, 2020), we set α = 0.005 and the learningrate to 0.00001..minimum risk training (mrt) minimises the ex-pected loss (‘risk’) with respect to the posteriordistribution:.
c additional results.
r(θ) =.
(cid:88).
(cid:88).
p (˜y|x, θ)∆(˜y, y),.
c.1 data amount.
(x,y).
˜y∈y(x).
8this can be reached by using several of gpus or by accu-mulating the gradients for several batches and then making anupdate..when varying the amount of data, figure 11 showschanges in the inﬂuence of source tokens at differ-ent positions to the whole output, figure 12 – inthe entropy of target contributions..1138(a).
(b).
figure 11: contribution of source token at each posi-tion to the whole target.
the arrows show the directionof change when increasing the amount of data..(c).
(d).
figure 14: (a) contribution of the whole source at eachgeneration step; (b) total contribution of source tokensat each position to the whole target sentence; (c-d) foreach generation step, entropy of (c) source, (d) targetcontributions..figure 12: for each generation step, the ﬁgure showsentropy of target contributions.
the arrows show the di-rection of change when increasing the amount of data..c.2 training stages.
figure 13 shows how inﬂuence of source tokensat different positions to the whole output changesduring training..(a).
(b).
figure 13: changes in training: contribution of sourcetoken at each position to the whole target.
each linecorresponds to a model state.
the arrows show the di-rection of change when the training progresses.
in theﬁgures, all stages are shown, but the stages of interestare highlighted more prominently..d all results for lrp with α = β = 12.here we present all results from the main text eval-uated with α = β = 12 in the redistribution rules ofαβ-lrp..d.1 getting acquainted.
figures 14 and 15..(c).
(d).
figure 15: for each generation step, the ﬁgure shows(a)-(b) contribution of source, (c)-(d) entropy of sourcecontributions..d.2 data amount.
figures 16 and 17..d.3 training stages.
figures 18, 19 and 20..1139(a).
(b).
figure 16: (a) contribution of the whole source at eachgeneration step (for clarity, the last two positions (punc-tuation mark and the eos token) are not shown.
); (b)contribution of source token at each position to thewhole target.
the arrows show the direction of changewhen increasing the amount of data..(a) source contribution at each target position.
(a).
(b).
figure 17: for each generation step, the ﬁgure showsentropy of (a) source, (b) target contributions.
the ar-rows show the direction of change when increasing theamount of data..(b) entropy of source contributions.
figure 19: changes in training for each target posi-tion; each line corresponds to a model state.
the ar-rows show the direction of change when the trainingprogresses.
in the ﬁgures, all stages are shown, but thestages of interest are highlighted more prominently..(a).
(b).
(a) source contribution at each target position.
figure 20: changes in training: contribution of sourcetoken at each position to the whole target.
each linecorresponds to a model state.
the arrows show the di-rection of change when the training progresses.
in theﬁgures, all stages are shown, but the stages of interestare highlighted more prominently..(c).
(d).
figure 18: training process: (a) convergence of contri-butions, (b) source contribution, (c-d) entropy of sourceand target contributions.
the model trained on 1m sub-sample of wmt14 en-fr dataset.
the results are aver-aged over target positions and evaluation examples..1140