continuous language generative flow.
zineng tang.
shiyue zhang hyounghun kim mohit bansal.
unc chapel hill{terran, shiyue, hyounghk, mbansal}@cs.unc.edu.
abstract.
recent years have witnessed various types ofgenerative models for natural language gener-ation (nlg), especially rnns or transformerbased sequence-to-sequence models, as wellas variational autoencoder (vae) and gener-ative adversarial network (gan) based mod-els.
however, ﬂow-based generative models,which achieve strong performance in imagegeneration due to their invertibility and exactdensity estimation properties, have been lessexplored for nlg.
in this paper, we proposea ﬂow-based language generation model byadapting previous ﬂow generative models tolanguage generation via continuous input em-beddings, adapted afﬁne coupling structures,and a novel architecture for autoregressive textgeneration.
we also apply our frameworkto sequence-to-sequence generation, includ-ing text- and video-based question generation(qg) and neural machine translation (nmt),and data augmentation for question answer-ing (qa).
we use our language ﬂow modelto provide extra input features for qg andnmt, which achieves improvements over thestrong qg baselines on squad and tvqaand nmt baseline on wmt16.
we also aug-ment qa data with new context by injectingnoise to the latent features of the language ﬂowand show this augmentation leads to a largeperformance improvement from strong base-lines on squad and tvqa.1.
1.introduction.
several generative models have been proposedfor language generation, including sequence-to-sequence models based on rnns (luong et al.,2015) and transformers (vaswani et al., 2017), aswell as variational autoencoders (vaes) to gen-erate diverse texts (bowman et al., 2016; jain.
1our code and models are available at: https://.
github.com/zinengtang/continuousflownlg.
et al., 2017), plus generative adversarial networks(gans) (yu et al., 2017) to improve intended se-mantic ﬁdelity.
another line of the generativemodel, normalizing ﬂow (rezende and mohamed,2015), is widely explored in computer vision andrepresentation learning but less explored for nlgtasks.
flow models have been shown to be capableof improving probability density estimation, includ-ing variational inference (rezende and mohamed,2015) and exact density estimation (dinh et al.,2015).
generative ﬂow is one type of ﬂow modeland ﬁrst proposed by dinh et al.
(2015, 2017);kingma and dhariwal (2018).
taking advantageof its invertible structure, it can perform an exactdensity estimation of the input distribution.
thus,during generation, we can sample from its latentspace and then generate new examples through itsinvertible decoder.
generative ﬂow shows strongperformance on image generation, attribute manip-ulation, and latent space inference (kingma anddhariwal, 2018).
considering these successfulapplications, we conjecture that the ﬂow modelshould also have strong potential to be adapted forlanguage generation tasks.
therefore, in this pa-per, we introduce a continuous language generativeﬂow model that can deal with discrete languagedata in continuous latent space.
we propose twovariants, the non-autoregressive and autoregressivemodels, and show that they both can perform wellon density estimation tasks..we follow the architecture of one previous gen-erative ﬂow model, glow (kingma and dhariwal,2018), but make adaptions for language genera-tion tasks.
we ﬁrst employ glove word embed-dings (pennington et al., 2014) to map the dis-crete token sequence to a continuous embeddingmatrix.
furthermore, we utilize two components:time-dimension permutation and afﬁne couplingwith rnn or transformer non-linearity functions,which allow interaction between words in a se-.
proceedingsofthe59thannualmeetingoftheassociationforcomputationallinguisticsandthe11thinternationaljointconferenceonnaturallanguageprocessing,pages4609–4622august1–6,2021.©2021associationforcomputationallinguistics4609quence and better contextualizes language seman-tics.
overall, these proposed components help gen-erate texts in a non-autoregressive manner..however, even though the non-autoregressivemodel has attracted a lot of research attention be-cause of its fast generation speed, it still hardlysurpasses the generation quality of autoregressivemodels (ren et al., 2020).
therefore, to make ourlanguage ﬂow model learn language generation ina stronger autoregressive manner, we change theﬂow model’s afﬁne coupling and permutation toa uni-directional structure, i.e., each timestep canonly attend to previous timesteps.
in this way, weenable our model to perform text generation autore-gressively..some recent works have developed density es-timation models targeted on character-level dis-crete data (discreteflow (tran et al., 2019)) andexplored using the ﬂow architecture as an extra dataencoder that provides latent features to support non-autoregressive text generation (flowseq (ma et al.,2019)).
while our work shares some similar char-acteristics, we explore different directions: (1) dis-creteflow develops a modulus calculation methodto process discrete data.
instead, we use word em-bedding to transform the discrete input tokens tocontinuous features, which is simple yet effective.
(2) flowseq essentially leverages the ﬂow architec-ture in a typical encoder-decoder model to supportnon-autoregressive generation, whereas our modelsfollow the standard generative ﬂow framework andcan directly generate texts via their invertible struc-ture in both non-autoregressive or autoregressivemanner.
(3) autoregressive ﬂows were previouslydeveloped (papamakarios et al., 2017; huang et al.,2018) for stronger density estimation ability.
how-ever, the autoregressive language ﬂow model wedevelop here aims for better text generation quality.
for this, our model is autoregressive in both the for-ward stage (encoding an input to a latent feature )and inverse stage (decoding the latent feature to theinput ) with an uni-directional (i.e., the left-to-rightdirection) structure,.
we evaluate the density estimation ability ofour language ﬂow models as well as their effec-tiveness for three downstream tasks: (1) sequence-to-sequence (seq-to-seq) generation that includesquestion generation (qg) and neural machine trans-lation (nmt) and (2) data augmentation for ques-tion answering (qa).
we test qg and qa dataaugmentation on two large-scale qa datasets: (a).
squad (rajpurkar et al., 2016), a widely ex-plored textual qa and qg dataset and (b) tvqa(lei et al., 2018), a large-scale multimodal video-dialogue qa task.
we test machine translation onwmt16 (cettolo et al., 2012), a commonly usednmt dataset..for density estimation, we compare the negativelikelihoods of our models against a baseline lstmmodel.
for qg, we use the non-autoregressive ﬂowmodel to provide extra input features for a stan-dard encoder-decoder text generation model.
weshow that it can signiﬁcantly improve a baselineqg model for both squad and tvqa on bothautomatic and human evaluation metrics.
aided byour ﬂow model, we achieve strong improvementsover a transformer baseline in the neural machinetranslation experiment.
in addition to improvinglanguage generation quality, we also use the pro-posed autoregressive ﬂow model for data augmen-tation.
for this, we focus on generating diversetextual contexts for qa tasks.
in particular, we in-ject noise into the latent features of our ﬂow models(encoded from ground-truth contexts) and then gen-erate new contexts from the noise-injected features.
experiments show that the generated contexts canbe either a varied expression of the same subjector paraphrasing the original context, but, mostlykeep the answerability of the original question (seeexamples in table 3).
combined with data augmen-tation strategies (data ﬁltering and training schema),we achieve statistically signiﬁcant improvementson both squad and tvqa over strong baselines.
(1) wepropose two continuous language generative ﬂowmodel variants that have better density estimationabilities than an lstm baseline model, and canperform non-autoregressive and autoregressive gen-eration respectively; (2) our language ﬂow modellargely improves qg, nmt, and data augmentationfor qa tasks..overall, we have two contributions:.
2 language generative flow.
in this section, we ﬁrst review the generative ﬂowmodel proposed in previous works (dinh et al.,2015; kingma and dhariwal, 2018).
then, follow-ing it, we propose two variants of our continuouslanguage generative ﬂow model..2.1 background: generative flow.
flow-based generative models transform simplelatent distributions, p(z), into a complex data dis-.
4610tribution (language text in our case), p(x), througha chain of invertible transformations..we ﬁrst designate a true data distribution p(x)and a model pθ(x) with parameters θ to parame-terize the true distribution p(x).
the latent spaceinference is then deﬁned as:.
xi ∼ p(x)zi = fθ(xi).
(1).
(2).
where xi is a data point from the true data distribu-tion and zi the latent features.
this encoding x to zprocedure is usually referred as the forward stage.
the transformation fθ is designed to be invertibleand bijective.
in previous ﬂow-based generativemodels (dinh et al., 2015, 2017; kingma and dhari-wal, 2018), the generative process (or referred asthe inverse stage) is deﬁned as:.
zi ∼ pθ(z)xi = gθ(zi) = f −1.
θ (zi).
(3).
(4).
where zi is a sample from the latent space distribu-tion, such as a standard gaussian distribution..the ﬂow mapping fθ is composed of a chain oftransformations: f = f1 ◦ f2 ◦ · · · ◦ fk with eachrepresenting one ﬂow step.
then, the log-likelihoodcan be written as:.
log pθ(x) = log pθ(z) +.
log.
k(cid:88).
j=1.
(cid:12)(cid:12)det(cid:12)(cid:12).
(cid:18) dhjdhj−1.
(cid:19)(cid:12)(cid:12)(cid:12)(cid:12).
(5).
where hj is the output of each ﬂow step.
thevalue log | det(dhj/dhj−1)| is namely the log-determinant: the log of the absolute value of thedeterminant of the jacobian matrix (dhj/dhj−1).
this value is the change in log-density from hj−1to hj under transformation fj.
this equation isnamely the change of variable formula..the objective for density estimation is formu-.
lated as:.
n(cid:88).
1n.i=1˜xi = xi + u.l(d) =.
− log pθ (˜xi) − m log d.(6).
figure 1: afﬁne coupling illustration.
the inputs z0is split into two halves into z1 and z2 along hidden di-mension and obtain the outputs ˆz1 and ˆz2 which willbe concatenated.
and it is similar for the reverse stage.
note that the × operation is element-wise product..each ﬂow step in the generative ﬂow model in-cludes three parts: normalization, permutation,and afﬁne coupling.
(1) normalization is designed to scale each outputto stabilize training.
we follow glow (kingma anddhariwal, 2018) to use actnorm.
(2) permutation makes sure after multiple ﬂowsteps, each channel can sufﬁciently affect otherdimensions.
the glow model (kingma and dhari-wal, 2018) proposes to use a (trainable) invertible1 × 1 convolution.
it is essentially a ﬂexible gen-eralization of a permutation operation.
we followglow and also use its lu decomposition to reducedeterminant computation cost.
different from allprevious work, we apply 1 × 1 convolution on thetime dimension rather than the hidden dimension.
this is because language data is sequential andtemporal.
this change is crucial to the proposedﬂow model’s performance, which will be shown inablation studies (table 4).
(3) afﬁne coupling is designed to incorporate com-plex nonlinear mapping but still keep invertibility(see figure 1)..z1, z2 = split(z0, dim : time).
s, t = split(nn(z1), dim : hidden)ˆz2 = σ(s + α) (cid:12) (t + z2).
(8).
(9).
(10).
(7).
where u is usually sampled from a gaussian distri-bution, n the number of samples in a batch, and d(= 128) the discretization level of the data and mthe dimension of xi.2.
2the change of variable formula, eq.5, treats the dataspace as unbounded.
however, the data we use is usuallywithin range -1.0 to 1.0 and parameter d (the discretization)can reduce the impact of boundary effects according to dinhet al.
(2017)..where nn refers to nonlinear function, σ is sig-moid activation.
α is a hyperparameter that pre-vents small value (around 0) from resulting in largenegative value by log.
note that, in the ﬁrst equa-tion, glow (kingma and dhariwal, 2018) splitsalong the hidden dimension.
however, we splitalong time dimension (ﬁrst introduced in flowseq(ma et al., 2019)) which has the same motivationas the permutation module..4611figure 2: non-autoregressive language flow modelwith multi-scale architecture..2.2 non-autoregressive language flow.
we ﬁrst present our non-autoregressive languageﬂow which is based on the architecture introducedabove.
besides the permutation/afﬁne couplingstructures changes introduced above, we use rnnsor transformer as the nonlinear mapping, proposeto use continuous input embedding, and introducemulti-scale architecture..afﬁne coupling.
we use a multihead self-attention module in transformer (vaswani et al.,2017) or alternatively rnns (a one-layer bidirec-tional lstm (schuster and paliwal, 1997)) in thecoupling layer by replacing the non-linear mappingof afﬁne coupling, nn (see eq.9)..continuous input embedding.
the languageﬂow model we propose operates on continuousinputs, which means the inputs are not discretetokens but continuous word embeddings.
we im-plement it through glove embeddings (penningtonet al., 2014).
therefore, the density estimation isperformed for the distribution p(x), where x is theword embeddings of language tokens.
note thatthe word embeddings are frozen.
in the inversestage, we compute the cosine similarity betweenthe embedding matrix and decoder output as thetoken generation probability distribution, so thatall tokens can be generated in parallel, i.e., non-autoregressively..multi-scale architecture.
following dinh et al.
(2017), we use a multi-scale architecture (see fig-ure 2) that contains multiple blocks while eachblock containing several ﬂow steps.
in our work,we denote the number of ﬂow steps as k, and thenumber of blocks as l that each contains k ﬂowsteps.
we denote the input shape as (batch size b, se-quence length s, hidden dimension h).
at the startof each block, the tensor is reshaped from (b, s, h)to (b, s2 , 2h), so the model can capture more localfeatures; and at the end of each block (except the.
figure 3: autoregressive language generative flowmodel.
the whole autoregressive ﬂow model containsmultiple k steps.
this ﬁgure illustrates one ﬂow stepfrom zk to zk+1..last block), the latent feature is split into halves viachannel dimension with one as the output, zl, andthe other as the input of the next block.
if we have3 blocks, we will have three latent outputs, zl.
pastworks (dinh et al., 2017; kingma and dhariwal,2018; ma et al., 2019) reshape in this manner forall blocks.
however, we do not reshape in the ﬁrstblock but apply the same for the following blocks,which allows the model to better process the origi-nal input text with intact sentence structure..2.3 autoregressive language flow.
the model we developed in the previous subsectioncan properly operate on continuous word embed-dings, have exact density estimation, and performnon-autoregressive generation, however, it lacksthe autoregressive structure that is commonly usedfor text generation.
previous works have shownautoregressive generation usually performs bet-ter than non-autoregressive generation (ren et al.,2020).
thus, we develop an autoregressive modelthat can generate text from left to right in the in-verse stage.
to achieve this, we change afﬁne cou-pling and permutation in the ﬂow step to be uni-directional, i.e., each timestep can only attend totimesteps that precede it.
however, we have toremove the multi-scale architecture to fulﬁll theautoregressive requirement.
see sample outputsin table 1 for comparison to those from the non-autoregressive model..uni-directional permutation.
since the permu-tation in each ﬂow step designed in our non-autoregressive ﬂow model is bidirectional, we maskthe 1 × 1 convolution to a lower triangular matrix.
therefore, each token can only attend to previoustokens in the permutation, i.e., uni-directional per-mutation..4612normalization(bidirectional) permutationaffine couplingsplitflow stepsqueezenormalizationac-cellac-cellac-cellnn................uni-directional permutationac-cellnon-autoregressive samples.
autoregressive samples.
what does house way when when that little he when the even?
what does wilson probably do after drawing?
what did richard know when he she else there thewhat does nelson going when he she when he what that to?
what did richard know when he she else there thewhat does nelson going when he she when he what that to?.
what did jamie want after charlie forget her immediatelywhat is brian awarewhat did caleb say after he went out?
what does phoebe think?.
table 1: data samples generated by our ﬂow models.
we sample from a gaussian distribution and generatequestions by our non-autoregressive or autoregressive ﬂow decoders.
models are trained on tvqa questions..with cosine similarity as the probability distributionof output tokens..autoregressive flow step.
the changes of afﬁnecoupling and permutation to uni-directional allowthe ﬂow step to be autoregressive.
and the wholeautoregressive ﬂow model will contain k such ﬂowsteps.
at each ﬂow step, the log-determinant is thesummation of the log-determinant of all time steps:.
log p(zk+1) =.
log p(z(t).
k+1).
(cid:88).
t.(cid:88).
=.
t.log p(z(t).
k ) + log.
det.
(cid:12)(cid:12)(cid:12)(cid:12)(cid:12).
(cid:32).
dz(t)k+1dz(t)k.(cid:33)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12).
(17).
(18).
3 language generation with flow.
we next apply our ﬂow model to several down-stream tasks.
despite the ﬂow’s rigid model struc-ture, it has a strong potential in density estimationdue to its complex transformation of inputs into acontinuous latent space.
we aim to use this prop-erty to improve standard encoder-decoder text gen-eration models.
moreover, as the ﬂow model has astrong ability in generating diverse text, we showthat it has the capability for data augmentation toimprove qa tasks..3.1 downstream datasets.
squad.
squad is a textual question answer-ing dataset containing 100,000+ questions/answerswith corresponding short articles as context.
weuse it to evaluate both question generation and dataaugmentation (by generating new articles) for ques-tion answering..tvqa.
tvqa is a large-scale video qa datasetbased on 6 tv shows.
it consists of 152,545 qapairs from 21,793 video clips with subtitle text.
we use it to evaluate both question generation anddata augmentation (by generating new subtitles) forquestion answering..figure 4: the two ﬁgures illustrate the ha and hb func-tions of the autoregressive afﬁne coupling in one ﬂowstep..uni-directional afﬁne coupling.
we then in-troduce an autoregressive version of afﬁne cou-pling, shown by the ac-cell in figure 3. foreach ﬂow step, we denote the input sequence asˆz(0):(t )k+1 = [ˆz(0)k+1], and then the autore-gressive coupling is deﬁned as:.
k+1, ..., ˆz(t ).
k+1 ]).
r(t−1) = nn([c(t−1); z(t−1)c(t) = ha(r(t−1), ˆz(t)k+1)z(t)k+1 = hb(r(t−1), c(t)).
(11).
(12).
(13).
k+1 = ˆz(0).
k+1, ..., z(t )we recurrently obtain the outputs, [z(1)k+1].
note that z(0)k+1, so the computation startsfrom z(1)k+1, we cannot getc(0), so we set it to be zero.
ha and ha are bothafﬁne coupling structured, as shown in figure 4.nn is either rnn or transformer..k+1.
when computing z(1).
in the inverse stage, to obtain ˆzk+1 , we start.
from ˆz(0).
k+1 = z(0).
k+1 and c(0):.
r(t−1) = nn([c(t−1); ˆz(t−1).
k+1 ])b (r(t−1), z(t)k+1)a (r(t−1), c(t)).
c(t) = h−1ˆz(t)k+1 = h−1.
(14).
(15).
(16).
since both decoded tokens z(t) and context c(t)only depend on previous tokens z(0):(t−1), we canperform autoregressive decoding and beam search.
4613sample ratio sample pair 1.sample pair 2.sentence1.
where did sheldon and beverley go after they came up stairs?
what did rachel do before chandler said something wasn’t true?.
0.60.50.4.where did sheldon and joey go after they came up?
where was rachel when joey said after , guys around huh?
where was rachel when joey said no guys around huh?.
what did rachel do before chandler said something wasn’t out?
what did house do before chandler when she was walking out?
what did house do to chandler when she was walking out?.
sentence2.
where was rachel when joey said, no guys around, huh?.
what did house say to sam when she was walking out the door?.
table 2: interpolation results.
we sample two pairs of questions from tvqa.
for each pair, we perform interpola-tion of their latent vectors learned by our autoregressive ﬂow model with different mix ratios (0.4, 0.5, 0.6).
wmt16 (ro-en).
wmt16 (ro-en) is a ma-chine translation dataset between english and ro-manian with around 610k sentence pairs.
we useit for our machine translation experiment and onlytest for the romanian to english direction..3.2 seq-to-seq generation with flow.
similar to flowseq (ma et al., 2019), we use ﬂowas an extra module on top of a typical encoder-decoder language generation model and test onquestion generation (qg) and neural machinetranslation (nmt).
as the ﬂow model has the abil-ity for exact density estimation, it provides the ex-act density components of context information andwe assume that it provides a better hidden represen-tation of context and thus helps with language gen-eration.
it can also be viewed as a self-supervisedlearning method that can provide new features fordownstream tasks..concretely, while the original qg model3 is for-mulated as ui = e(xi), ˆqi = g(ui); the new qgmodel with ﬂow is formulated as:.
ui = e(xi), ˆqi = g(hatt(ui, zi)).
(19).
where e refers to encoder and g decoder.
zi refersto latent features of the non-autoregressive ﬂowmodel.
hatt is essentially a mlp with sigmoidactivation..the loss function has two parts:.
lgen =.
− log p(qi).
1n.n(cid:88).
i=1.
l = λlnll + lgen.
(20).
(21).
where qi represents the target questions and λ is ahyperparameter for nll loss (eq.
6).
3we replicate zhang and bansal (2019)’s standard encoder-decoder attention qg model with bert features as inputembeddings..3.3 context generation for data.
augmentation.
context generation.
we propose to use ﬂow togenerate diverse contexts for data augmentation asboth tvqa and squad are question answeringtasks with textual context.
we generate new context(video subtitles for tvqa; articles for squad) byinjecting noise to the hidden vector of the originalcontext, zi, and reconstructing it to new sentences,ˆxi.
note that, we can also do the same thing forquestions, however, we ﬁnd that changing one wordin the question will dramatically change its mean-ing, so we limit this augmentation to the contextand keep the original question unchanged.
the generation process is formulated as:.
zi = fθ(xi)ˆxi = f −1.
θ (zi + z0).
(22).
(23).
where fθ refers to the ﬂow model and xi the inputtext and zi the latent space.
the transformation isperformed by simply sampling a gaussian noisez0, add it to zi, and reconstruct the new context ˆxiin the reverse stage.
in this task, we use the autore-gressive ﬂow model as this variant is designed fortext generation.
we also use the non-autoregressiveﬂow model additionally leveraged by an additionalautoregressive decoder, as an alternative approach.
while the standard rnn-based language modeldoes not have an explicit global sentence repre-sentation, our ﬂow model is similar to bowmanet al.
(2016)’s vae framework that encodes thesentence into a continuous hidden vector, p(z|x).
and sampling around the hidden vector can natu-rally be viewed as injecting noise without changingkey information.
therefore, we do not aim forparaphrasing the original context because the ﬂowmodel can reconstruct different information fromrandom noise injection in latent space.
notably,this method has the risk of changing the context’smeaning and making the question unanswerable,however, empirically, we ﬁnd that as long as we.
4614original.
generated.
tvqa generated subtitle example 1: varied expression of the subject..a: boy do i feel bad!
oh yeah.
very bad.
b: chandler what are you doing?
chandler!
oh my god!
b: you’re smoking again?
a: well actually yesterday i was smoking again.
today i’msmoking still..squad generated article example 1: paraphrasing..while a computer may be viewed as running one giganticprogram stored in its main memory, in some systems it isnecessary to give the appearance of running several programssimultaneously.
this is achieved by multitasking i.e.
havingthe computer switch rapidly between running each programin turn..b: oh my god !
why are you doing this again ?
a: i don’t feel bad .
b: why are you still smoking again?
a: i was just smoking again after i ﬁrst started smoking again..the main memory of the gigantic computer is running thegigantic computer program.
in some systems, it is necessaryto have the computer switch rapidly between each programachieved by multitasking..table 3: sample context generation results: we show two examples that are ﬁltered as positive.
via a long anda short tvqa example, we show that our model is not entirely paraphrasing the original dialogue but changingcontent while keeping the central theme unchanged; via a squad example, we show that our model can paraphrasecomplex semantics..keep the noise small enough, the generation will beeither paraphrases or different expressions of thesame subject without affecting the answerability..data filtering.
to better utilize the generateddata, we design a data ﬁlter as ﬁltering out the low-quality generated text is useful in helping improvethe data augmentation (zhang and bansal, 2019).
we use pretrained qa baseline models (see table 8baseline tvqa+ and table 9 baseline bert) toﬁlter out the low-quality context.
the generatedcontext will be ﬁltered out if the model performsworse on predicting correct answers when originalcontext is replaced by its generated counterpart..4 experimental setup.
we follow zhang and bansal (2019) to split thedevelopment set of squadv1.1 (rajpurkar et al.,2016) into two halves and show the result on thetest split.
we generally follow previous work onevaluation metrics.
for density estimation, we usenegative log-likelihood (nll) for comparison andbits per dimension to regularize the negative log-m log(2) , where mlikelihood loss, formulated asrepresents the dimension of input.
we evaluate qgby bleu4 (papineni et al., 2002), meteor (lavieand agarwal, 2007), rouge-l (lin, 2004), andamazon mturk human evaluation.
we use thebleu score to evaluate nmt.
we use accuracyto evaluate the tvqa qa model and em (exactmatch) and f1 score to evaluate the squad qamodel..l.we replicate zhang and bansal (2019)’s base-line qg model.
we use the stage model with.
model.
tvqa subtitle.
squad article.
bi-lstm.
att-crnn-c.att-srnn-s.att-arrnn-ar.
-7.31.
0.680.50.
-8.02-8.35.
-9.62-9.63.
-1.27.
-2.01-0.37.
-17.12-17.17.
-17.12-17.26.table 4: the nll results of ﬂow models and an lstmbaseline on the validation split of tvqa subtitles andtest split of squad articles.
the difference betweenc (e.g., att-c) and s (e.g., att-s) is whether the afﬁnecoupling/permutation is based on channel-dim (c) ortime-dim (s).
ar means autoregressive architecture.
att- refers to transformer nonlinear mapping, and rnn-refers to rnn nonlinear mapping..glove embeddings developed by lei et al.
(2020)as the tvqa qa baseline and use bert as thesquad qa baseline.
see appendix a for moreexperiment/reproducibility details..5 results.
5.1 negative log-likelihood results.
first of all, to evaluate the density estimation abil-ity, we compare the negative log-likelihood (nll,eq.6)4 of our different ﬂow models on the con-text data of squad and tvqa against a base-line model (a 3-layer bidirectional lstm-rnnmodel with hidden size 300).
as shown in table 4,the ﬂow model of time-dim coupling/permutation.
4note that since our p(x) is over continuous word embed-dings, so it is the probability density of a continuous variablewhich is not bounded by [0,1]..4615generally outperforms the baseline lstm model.
the ﬂow model of time-dim coupling/permutationlargely outperforms the ﬂow model of channel-dim coupling/permutation.
we also test our au-toregressive model to check its density estimationability, and we ﬁnd it performs well and even some-times slightly better than the non-autoregressivemodel.
note that we do not claim the autore-gressive model is better at density estimation thanthe non-autoregressive version, instead, we aimto show that it can perform reasonably with theproposed autoregressive adaptation..5.2 seq-to-seq generation results.
question generation.
through the ablationstudies shown in table 5 and table 6, we demon-strate that the proposed ﬂow-aided qg model sig-niﬁcantly improves the qg performance.
the sta-tistical signiﬁcances for all metric improvements(bleu4, rouge-l, meteor) are p < 0.001 for bothtvqa qg and squad qg.5 we also conduct ahuman evaluation.
we random sample 200 exam-ples6, and we present the participants two questionsper example generated by two different models andlet them judge which question is better in terms ofanswerability and overall quality.
see more humanevaluation details in appendix a.3.
we compareour ﬂow model to the pure encoder-decoder base-line as well as the flowseq model (ma et al., 2019)in human evaluation.
as shown in the last rows intable 5 and table 6, humans favor our model morethan the baseline in both tasks, which indicates ourﬂow model indeed provides useful latent featuresfor better generation.
plus, our model also alwaysoutperforms flowseq.
we conjecture that it is be-cause flowseq is non-autoregressive whereas ourqg model is autoregressive..neural machine translation.
we also test theeffectiveness of our approach on a neural machinetranslation (nmt) task.
we ﬁrst replicate lee et al.
(2018)’s transformer autoregressive model baseline,and then we add our ﬂow architecture on top of it.
as shown in table 7, our proposed ﬂow-aided mtmodel can improve the machine translation perfor-mance over the strong transformer baseline on thewmt16 (cettolo et al., 2012) romanian to englishtranslation task.
see a.7 for more details.
we hope.
5statistical signiﬁcance is computed using the bootstrap.
test (efron and tibshirani, 1994)..6we exclude those examples where the two models gener-.
ate identical questions..18.51.
17.3818.6817.94.tie.
57.tie.
52.
19.69.
21.8622.3821.97.tie.
61.tie.
35.bleu4 rouge-l meteor.
models.
flowseq.
qg baseline+ lang-flow+ lstm-flow.
12.19.
10.6812.5511.48.
41.02.
39.5841.3240.49.models.
lang-flow baseline.
human eval 1.human eval 2.
89.
98.models.
lang-flow flowseq.
54.
50.table 5: tvqa-qg evaluation: comparison betweenflowseq (ma et al., 2019), a bert qg baseline, flowaided qg model (lang-flow), and simple density es-timation model (3-layer lstm) aided qg baselinemodel (lstm-flow) on tvqa qg validation split..bleu4 rouge-l meteor.
models.
flowseq.
qg baseline+ lang-flow+ lstm-flow.
14.95.
18.0819.2118.93.
44.83.
46.6847.6247.27.models.
lang-flow baseline.
human eval 3.human eval 4.
76.
96.models.
lang-flow flowseq.
63.
69.table 6: squad-qg evaluation: comparison betweenflowseq (ma et al., 2019), a bert qg baseline, flowaided qg baseline model (lang-flow) and simple den-sity estimation model (3-layer lstm) aided qg base-line model (lstm-flow) on the squad-qg test split..that these promising initial nmt results will alsoencourage the community to use continuous ﬂowmodels for other nmt and nlg tasks..5.3 qa data augmentation results.
as shown in table 8 and table 9, using the aug-mented data generated by our language flowmodel (refers to our autoregressive language ﬂowmodel), we achieve signiﬁcant performance im-provements over strong baselines on both tvqaqa (lei et al., 2020) (p < 0.0001) and squadqa (rajpurkar et al., 2016) (p < 0.0005) for bothem and f1.
furthermore, when we add an lstmautoregressive decoder to our non-autoregressiveencoder (referred to as language flow+) and use itto perform data augmentation, we observe evenslightly better results.
this may indicate thestronger encoding ability of our non-autoregressivemodel due to its multi-scale architecture.
mean-.
4616models.
transformer baseline+lang-flow.
bleu.
30.2730.87.table 7: mt results on wmt16 ro-en dev split..models.
valid-accuracy.
baseline tvqa++ context (back-translation)+ context (paraphrasing)+ context (language flow)+ context (language flow+).
69.4269.5269.9870.4570.86.table 8: qa results on tvqa dev split.
languageflow refers to the autoregressive language ﬂow we pro-pose, and language flow+ refers to the model with anon-autoregressive ﬂow model encoder plus an lstmautoregressive decoder..while, we compare to two other data augmentationtechniques: paraphrasing (niu and bansal, 2018)and back-translation (sennrich et al., 2016).
notethat for a fair comparison, we apply the same dataﬁlter and training schema for all data augmentationmethods.
it can be seen that both methods performworse than our language flow or language flow+models..6 discussion.
we show some sample questions generated by ournon-autoregressive and autoregressive ﬂow mod-els in table 1. the autoregressive samples arebetter organized and grammatically sound, whilenon-autoregressive generation fails at the latterpart of the sentence.
it might because the non-autoregressive structure has a weaker ability tomodel the temporal dependency during generation,which is consistent with the observations from pre-vious works (ren et al., 2020).
to show that ourmodel generates samples from a continuous space,we generate interpolation samples from our autore-gressive ﬂow model shown in table 2. those sam-ples are mostly grammatically sound and correctlyreﬂect the intermediate content of the two interpo-lated sentences..while variational autoencoder has the issue ofignoring latent space (li et al., 2019), our modelsdo not suffer from this issue.
we introduced twotypes of language generation models in the paper:(1) the autoregressive ﬂow model (used in dataaugmentation tasks) and (2) the model that usesﬂow latent features as extra input (e.g., for qgtasks).
our autoregressive ﬂow model’s decoder is.
models.
baseline bert+ context (back-translation)+ context (paraphrasing)+ context (language flow)+ context (language flow+).
em.
f1.
81.3481.0281.6582.2882.49.
88.7688.7988.9289.2289.44.table 9: qa results on squad test split.
the aug-mented data (new articles) signiﬁcantly improves astrong squad qa baseline..the inverted version of its encoder with the sameweights, so it ensures the decoder uses the latentfeatures.
when we use ﬂow latent features as extrainputs, it signiﬁcantly improves qa performance(table 5 and table 6), which implies the latentfeatures are usefully involved in generation..7 conclusion.
we have proposed a language generative ﬂowmodel with non-autoregressive and autoregres-sive variants.
the non-autoregressive ﬂow modelachieves strong performance on density estima-tion and helps improve question generation andmachine translation by providing additional use-ful latent features to the decoder.
moreover, theautoregressive variant largely improves questionanswering by generating new contexts with noiseinjection..acknowledgments.
we thank the reviewers for their helpful feed-back.
this research is supported by nsf-careeraward 1846185, onr grant n00014-18-1-2871,and aro-yip award #w911nf18-1-0336.
theviews contained in this article are those of the au-thors and not of the funding agency..references.
samuel r bowman, luke vilnis, oriol vinyals,andrew m dai, rafal jozefowicz, and samybengio.
2016. generating sentences from a con-tinuous space.
in conll..mauro cettolo, christian girardi, and marcellofederico.
2012. wit3: web inventory of tran-scribed and translated talks.
in conference ofeuropean association for machine translation,pages 261–268..jacob devlin, ming-wei chang, kenton lee, andkristina toutanova.
2019. bert: pre-training.
4617of deep bidirectional transformers for languageunderstanding.
in proceedings of the 2019 con-ference of the north american chapter of theassociation for computational linguistics: hu-man language technologies, naacl-hlt 2019,minneapolis, mn, usa, june 2-7, 2019, volume1 (long and short papers), pages 4171–4186.
association for computational linguistics..laurent dinh, david krueger, and yoshua bengio.
2015. nice: non-linear independent compo-nents estimation.
in 3rd international confer-ence on learning representations, iclr 2015,san diego, ca, usa, may 7-9, 2015, workshoptrack proceedings..laurent dinh, jascha sohl-dickstein, and samybengio.
2017. density estimation using realnvp.
in 5th international conference on learn-ing representations, iclr 2017, toulon, france,april 24-26, 2017, conference track proceed-ings.
openreview.net..bradley efron and robert j tibshirani.
1994. an.
introduction to the bootstrap.
crc press..chin-wei huang, david krueger, alexandre la-coste, and aaron courville.
2018. neural autore-gressive ﬂows.
in international conference onmachine learning, pages 2078–2087.
pmlr..unnat jain, ziyu zhang, and alexander g schwing.
2017. creativity: generating diverse questionsusing variational autoencoders.
in proceedingsof the ieee conference on computer vision andpattern recognition, pages 6485–6494..diederik p. kingma and jimmy ba.
2015. adam:a method for stochastic optimization.
in 3rdinternational conference on learning represen-tations, iclr 2015, san diego, ca, usa, may7-9, 2015, conference track proceedings..durk p kingma and prafulla dhariwal.
2018. glow:generative ﬂow with invertible 1x1 convolutions.
in advances in neural information processingsystems, pages 10215–10224..alon lavie and abhaya agarwal.
2007. meteor:an automatic metric for mt evaluation with highlevels of correlation with human judgments.
inproceedings of the second workshop on statisti-cal machine translation, pages 228–231.
asso-ciation for computational linguistics..jason lee, elman mansimov, and kyunghyun cho.
2018. deterministic non-autoregressive neuralsequence modeling by iterative reﬁnement.
inemnlp..jie lei, licheng yu, mohit bansal, and tamara l.berg.
2018. tvqa: localized, compositionalvideo question answering.
in proceedings of the2018 conference on empirical methods in natu-ral language processing, brussels, belgium, oc-tober 31 - november 4, 2018, pages 1369–1379.
association for computational linguistics..jie lei, licheng yu, tamara l berg, and mohitbansal.
2020. tvqa+: spatio-temporal ground-ing for video question answering.
proceedingsof the 58th annual meeting of the associationfor computational linguistics..bohan li, junxian he, graham neubig, taylorberg-kirkpatrick, and yiming yang.
2019. asurprisingly effective ﬁx for deep latent variablein conference on empiri-modeling of text.
cal methods in natural language processing(emnlp), hong kong..chin-yew lin.
2004. rouge: a package for auto-matic evaluation of summaries.
in text summa-rization branches out, pages 74–81..thang luong, hieu pham, and christopher d. man-ning.
2015. effective approaches to attention-based neural machine translation.
in proceed-ings of the 2015 conference on empirical meth-ods in natural language processing, emnlp2015, lisbon, portugal, september 17-21, 2015,pages 1412–1421.
the association for compu-tational linguistics..xuezhe ma, chunting zhou, xian li, graham neu-big, and eduard h. hovy.
2019. flowseq: non-autoregressive conditional sequence generationwith generative ﬂow.
in proceedings of the 2019conference on empirical methods in naturallanguage processing and the 9th internationaljoint conference on natural language process-ing, emnlp-ijcnlp 2019, hong kong, china,november 3-7, 2019, pages 4281–4291.
associ-ation for computational linguistics..tong niu and mohit bansal.
2018. adversarialover-sensitivity and over-stability strategies fordialogue models.
in proceedings of the 22ndconference on computational natural language.
4618learning, conll 2018, brussels, belgium, oc-tober 31 - november 1, 2018, pages 486–496.
association for computational linguistics..george papamakarios, theo pavlakou, and iainmurray.
2017. masked autoregressive ﬂow fordensity estimation.
in neurlps..kishore papineni, salim roukos, todd ward, andwei-jing zhu.
2002. bleu: a method for au-tomatic evaluation of machine translation.
inproceedings of the 40th annual meeting on as-sociation for computational linguistics, pages311–318.
association for computational lin-guistics..adam paszke, sam gross, soumith chintala, gre-gory chanan, edward yang, zachary devito,zeming lin, alban desmaison, luca antiga,and adam lerer.
2017. automatic differentia-tion in pytorch.
in nips autodiff workshop..jeffrey pennington, richard socher, and christo-pher d manning.
2014. glove: global vectorsfor word representation.
in proceedings of the2014 conference on empirical methods in natu-ral language processing (emnlp), pages 1532–1543..pranav rajpurkar, jian zhang, konstantin lopy-rev, and percy liang.
2016. squad: 100, 000+questions for machine comprehension of text.
inproceedings of the 2016 conference on empir-ical methods in natural language processing,emnlp 2016, austin, texas, usa, november1-4, 2016, pages 2383–2392.
the associationfor computational linguistics..yi ren, jinglin liu, xu tan, zhou zhao, shengzhao, and tie-yan liu.
2020. a study of non-autoregressive model for sequence generation.
in acl..danilo jimenez rezende and shakir mohamed.
2015. variational inference with normalizingﬂows.
in proceedings of the 32nd internationalconference on machine learning, icml 2015,lille, france, 6-11 july 2015, volume 37 ofjmlr workshop and conference proceedings,pages 1530–1538.
jmlr.org..rico sennrich, barry haddow, and alexandrabirch.
2016. improving neural machine transla-tion models with monolingual data.
in proceed-ings of the 54th annual meeting of the associa-tion for computational linguistics..dustin tran, keyon vafa, kumar agrawal, laurentdinh, and ben poole.
2019. discrete ﬂows: in-vertible generative models of discrete data.
inadvances in neural information processing sys-tems, pages 14692–14701..ashish vaswani, noam shazeer, niki parmar,jakob uszkoreit, llion jones, aidan n gomez,łukasz kaiser, and illia polosukhin.
2017. at-tention is all you need.
in advances in neural in-formation processing systems, pages 5998–6008..lantao yu, weinan zhang, jun wang, and yong yu.
2017. seqgan: sequence generative adversarialnets with policy gradient.
in thirty-first aaaiconference on artiﬁcial intelligence..shiyue zhang and mohit bansal.
2019. addressingsemantic drift in question generation for semi-supervised question answering.
in proceedingsof the 2019 conference on empirical methods innatural language processing and the 9th inter-national joint conference on natural languageprocessing, emnlp-ijcnlp 2019, hong kong,china, november 3-7, 2019, pages 2495–2509.
association for computational linguistics..yao zhao, xiaochuan ni, yuanyuan ding, andqifa ke.
2018. paragraph-level neural questiongeneration with maxout pointer and gated self-attention networks.
in proceedings of the 2018conference on empirical methods in naturallanguage processing, pages 3901–3910..appendix.
a experimental setup.
in this section, we introduce our experiment set-tings ranging from datasets usage, ﬂow implemen-tation details, question generation model, and dataaugmentation settings.
we use a ﬁxed seed 2020for pytorch random seed..mike schuster and kuldip k paliwal.
1997. bidi-ieeerectional recurrent neural networks.
transactions on signal processing, 45(11):2673–2681..a.1 software/hardware usage.
we use pytorch 1.5 (paszke et al., 2017) to buildour model.
we use nvidia geforce rtx 2080tiand intel cpu (intel(r) xeon(r) silver 4114 cpu.
4619@ 2.20ghz) built on ubuntu 16.01 for each train-ing or inference process..a.2 datasetswe use squadv1.1 (rajpurkar et al., 2016)7 andtvqa (lei et al., 2018)8 to perform our experi-ments, since squad is a widely explored qa andqg dataset, and tvqa is a video-based multi-modal dataset with rich dialogue context.
there-fore, question generation, context generation, andlanguage density estimation, and data augmenta-tion can be well performed and evaluated compre-hensively on these two datasets and tasks..tvqa consists of 152,545 qa pairs from21,793 clips, spanning over 460 hours of video.
the subtitles in tvqa dataset has time-stamp an-notations of localized clip in the full subtitle clip.
the localized clip is the relevant interval of a clipfor question answering.
in both the video ques-tion generation task and the context generation taskfor data augmentation, we use localized subtitles.
the tvqa context features are dialogues or videosubtitles; hence data augmentation on this datasetshould consider an additional frame-level dimen-sion..squad has over 100,000 questions and 23,215paragraphs for the 536 articles covering a widerange of topics.
we follow zhang and bansal(2019) to split the development set of squadv1.1(rajpurkar et al., 2016) into two splits and showthe result on the second split..a.3 preprocessing.
we tokenize the data to be used for both glove em-bedding and bert features extraction, and we addthe start of sentence token and the end of sentencetoken for every input..a.4 evaluation metrics.
we generally follow previous work on evaluationmetrics across density estimation, question genera-tion, and question answering augmentation.
flow model.
for ﬂow density estimation, we fol-low previous work (kingma and dhariwal, 2018;dinh et al., 2017) to use negative log-likelihood forcomparison.
question generation model.
we evaluate thegeneration quality by bleu4 (papineni et al.,.
7online link for squad: rajpurkar.github.io/.
squad-explorer/explore/1.1/dev/.
8online link for tvqa: tvqa.cs.unc.edu/.
2002), meteor (lavie and agarwal, 2007), androuge-l (lin, 2004) to provide an insight into theperformance of our model.
we also use amazonturk human evaluation that compares the baselinegeneration and the proposed model generation byproving a suitable qa context.
for squad qg,we present the article context, question pairs, andthe answer for the users to select their preferencein terms of answerability and overall quality of thequestion pair.
for tvqa qg, we present the videoclip, subtitle context, question pairs, and answercandidates for the users to select their preferencein terms of answerability and overall quality of thequestions pair.
machine translation model.
we evaluate thegeneration quality by bleu (papineni et al., 2002)to provide an insight into the performance of ourmodel.
data augmentation model.
we use accuracyscores to evaluate tvqa qa model, and followprevious work (rajpurkar et al., 2016) to use em(exact match) and f1 score to evaluate squad qamodel..a.5 flow implementation details.
the experiment on base ﬂow models does not in-volve extensive hyperparameter search trials sinceﬂow models follow the principle: the deeper, thebetter.
we use small-sized ﬂow models across dif-ferent versions of (k=8, l=3, parameter number:128m for transformer module and 196m for rnnmodule) ﬂow models for ablation study.
the au-toregressive ﬂow model has k=24, l=1 with ap-proximately the same parameter number, 130m, bychanging the nonlinear functions complexity for afair comparison..based on the sequence length distribution of thedataset, we designate the maximum ﬁxed ﬂow se-quence length for tvqa-subtitles as 64, squad-paragraphs as 256. we set l=3 or 4 for all experi-ments while changing the number of ﬂow steps kwith a multiple of 8. while it follows that the morek, the better, setting l to a reasonable value is es-sential as each block will reduce sequence lengthby half.
therefore, l is set according to the lengthof the input..the discretization, d,.
in the negative log-likelihood loss function (eq.6) is set to 2n, wheren=6.
noise, u, is set as gaussian sample withα = 12m , where m=6.
we follow previouswork (kingma and dhariwal, 2018) to use bits per.
4620a.6 question generation implementation.
details.
details.
dimension to regularize the negative log-likelihoodl(m log(2)) , where m representsloss, formulated asthe dimension of input..we use a learning rate between 1e-4 and 1e-5, speciﬁcally 5e-5, to achieve stable and fasterconvergence (with adam optimizer (kingma andba, 2015), beta1=0.9, beta2=0.999).
with priorknowledge of adam optimizer, we perform 5 trialsto test learning rate (1e-3, 5e-4.
1e-4.
5e-5, 1e-5)to ﬁnd the fastest convergence rate..the average training time is 50 epochs for a(k=8 l=3 parameter number: 128m for transformermodule and 196m for rnn module) ﬂow model,as each epoch takes 20 minutes.
inference for onesample takes around 0.01s..the density estimation by the lstm model weuse for baseline comparison in nll and qg mod-els is designed to be well deﬁned as a density es-timation model.
flow density estimation modelswith no invertibility are not well-deﬁned.
there-fore, we mimic a model structure that the transfor-mation is through only non-singular matrix weightto obtain an arithmetically invertible model..the experiment on question generation modelsdoes not involve extensive hyperparameter searchtrials, as the proposed model has stable conver-gence under varied circumstances.
we take the lastlatent space output of the ﬂow model as the featuresused for the qg model decoder or attention map.
we use (k=16, l=3 parameter number: 256mparameters for transformer module) ﬂow modelswith transformer modules without autoregressivedecoding for all the qg experiments.
the lossweight of λ1 is set 1.0; the weight will not sig-niﬁcantly affect the result as long as it is set to areasonably large value.
we use gradient descentwith momentum optimizer (momentum = 0.8, lr =1e-3) for both base model and ﬂow model.
withprior knowledge of the sgd optimizer, we performfour trials to test the learning rate (1e-2, 5e-3.
1e-3.
5e-4) to ﬁnd the fastest convergence rate and stabletraining..we employ zhang and bansal (2019)’s baselineqg model, which is a robust encoder-decoder at-tention generation network with a maxout pointernetwork and self-gated attention (zhao et al., 2018)for both tasks.9 we use pretrained bert (devlin.
et al., 2019) hidden features with 768 dimensionsby a small uncased bert model to replace gloveembedding to make the baseline stronger to showthat the ﬂow model can still improve well on astrong baseline..the average training time is 20 epochs for thejoint training of the qg model and the (k=16 l=3)ﬂow model, as each epoch takes 50 minutes.
infer-ence for one sample takes around 0.03s..a.7 machine translation implementation.
details.
for the machine translation dataset wmt16, thesource and target languages share the same set ofsubword embeddings.
the maximum text length isset to 64 and we ﬁlter out all data that is above thisrange.
we use (k=4, l=3 with transformer module)non-autoregressive ﬂow models with transformermodules for all the data augmentation experiments.
we use adam optimizer (kingma and ba, 2015)with beta1=0.9, beta2=0.999, and a learning rate5e-5 for ﬂow model training..a.8 data augmentation implementation.
the experiment on context generation models gen-erally follows empirical hyperparameter settings.
we use (k=32, l=4 parameter number: 512mparameters for transformer module) autoregressiveﬂow models with transformer modules for all thedata augmentation experiments.
we use adam op-timizer (kingma and ba, 2015) with beta1=0.9,beta2=0.999, and a learning rate 5e-5 for ﬂowmodel training and an empirically stable learningrate 3e-4 for attention decoder training.
we set z0to a gaussian noise sample with mean 0.0 and vari-ance 1.0 during training and variance 0.5 duringinference.
for inference variance tuning, we startfrom variance 1.0 and gradually decrease by 0.1until 0.1 to manually check which setting has gen-erated samples with reliable quality and diversitysuitable for robust data augmentation..the average training time is 100 epochs for the(k=32 l=4 parameter number: 512m) augmenta-tion ﬂow model, as each epoch takes 30 minutes.
inference for one sample takes around 0.5s.
base qa model.
we use model, backbone + attn.
sup.
+ temp.
sup.
+ local (stage) with gloveembeddings, developed in tvqa+ dataset (leiet al., 2020) as the qa baseline for tvqa data.
9maxout pointer is not used in the tvqa qg model since.
the number of words out of vocabulary is small..4621augmentation model.
we use the bert baseline(devlin et al., 2019) for squad qa (rajpurkaret al., 2016); this bert baseline is pretrained anduncased with 768 base dimension and ﬁnetuned onthe squad dataset.
these two models are alsoused as data ﬁlters.
data augmentation strategies.
the trainingschemes are crucial for context generation since thetvqa model has heavy dependence on the subti-tles and squad model on the paragraphs: similarto zhang and bansal (2019)’s strategies, we obtainapproximately ten times the amount of augmenteddata than the original amount, and ﬁlter them toobtain approximately 40% of augmented data tobe used for training.
we set a probability, 0.5, forreplacing the original data with newly generatedﬁltered data for each batch in training.
for tvqadata augmentation, we generate localized subtitlesand replace the corresponding part in non-localizedfull-subtitles.
for squad data augmentation, wegenerate trunks of paragraphs that do not containanswers to replace the corresponding trunks in theoriginal paragraphs..4622