superbizarre is not superb: derivational morphology improvesbert’s interpretation of complex words.
valentin hofmann*‡, janet b. pierrehumbert†*, hinrich schütze‡.
*faculty of linguistics, university of oxford†department of engineering science, university of oxford‡center for information and language processing, lmu munichvalentin.hofmann@ling-phil.ox.ac.uk.
abstract.
how does the input segmentation of pretrainedlanguage models (plms) affect their interpre-tations of complex words?
we present the ﬁrststudy investigating this question, taking bertas the example plm and focusing on its se-mantic representations of english derivatives.
we show that plms can be interpreted as se-rial dual-route models, i.e., the meanings ofcomplex words are either stored or else needto be computed from the subwords, which im-plies that maximally meaningful input tokensshould allow for the best generalization on newwords.
this hypothesis is conﬁrmed by a se-ries of semantic probing tasks on which del-bert (derivation leveraging bert), a modelwith derivational input segmentation, substan-tially outperforms bert with wordpiece seg-mentation.
our results suggest that the gen-eralization capabilities of plms could be fur-ther improved if a morphologically-informedvocabulary of input tokens were used..1.introduction.
pretrained language models (plms) such as bert(devlin et al., 2019), gpt-2 (radford et al., 2019),xlnet (yang et al., 2019), electra (clark et al.,2020), and t5 (raffel et al., 2020) have yieldedsubstantial improvements on a range of nlp tasks.
what linguistic properties do they have?
variousstudies have tried to illuminate this question, with afocus on syntax (hewitt and manning, 2019; jawa-har et al., 2019) and semantics (ethayarajh, 2019;ettinger, 2020; vuli´c et al., 2020)..one common characteristic of plms is their in-put segmentation: plms are based on ﬁxed-sizevocabularies of words and subwords that are gen-erated by compression algorithms such as byte-pair encoding (gage, 1994; sennrich et al., 2016)and wordpiece (schuster and nakajima, 2012; wuet al., 2016).
the segmentations produced by these.
p(y|sw(x)) = .149.p(y|sd(x)) = .931.bert.
bert.
superb.
##iza.
##rre.
super.
-.
bizarre.
superbizarre.
x.applausive.
neg.
pos.
y.superbizarre.
x.applausive.
neg.
pos.
y.sw.sd.
(a) bert (sw).
(b) delbert (sd).
figure 1: basic experimental setup.
bert withwordpiece segmentation (sw) mixes part of the stembizarre with the preﬁx super, creating an associa-tion with superb (left panel).
delbert with deriva-tional segmentation (sd), on the other hand, separatespreﬁx and stem by a hyphen (right panel).
the twolikelihoods are averaged across 20 models trained withdifferent random seeds.
the average likelihood of thetrue class is considerably higher with delbert thanwith bert.
while superbizarre has negative sen-timent, applausive is an example of a complexword with positive sentiment..algorithms are linguistically questionable at times(church, 2020), which has been shown to worsenperformance on certain downstream tasks (bostromand durrett, 2020; hofmann et al., 2020a).
how-ever, the wider implications of these ﬁndings, par-ticularly with regard to the generalization capabili-ties of plms, are still poorly understood..here, we address a central aspect of this issue,namely how the input segmentation affects the se-mantic representations of plms, taking bert asthe example plm.
we focus on derivationally com-plex words such as superbizarre since theyexhibit systematic patterns on the lexical level, pro-viding an ideal testbed for linguistic generaliza-tion.
at the same time, the fact that low-frequencyand out-of-vocabulary words are often derivation-ally complex (baayen and lieber, 1991) makes ourwork relevant in practical settings, especially whenmany one-word expressions are involved, e.g., inquery processing (kacprzak et al., 2017)..proceedingsofthe59thannualmeetingoftheassociationforcomputationallinguisticsandthe11thinternationaljointconferenceonnaturallanguageprocessing,pages3594–3608august1–6,2021.©2021associationforcomputationallinguistics3594the topic of this paper is related to the morefundamental question of how plms represent themeaning of complex words in the ﬁrst place.
sofar, most studies have focused on methods of repre-sentation extraction, using ad-hoc heuristics suchas averaging the subword embeddings (pinter et al.,2020; sia et al., 2020; vuli´c et al., 2020) or takingthe ﬁrst subword embedding (devlin et al., 2019;heinzerling and strube, 2019; martin et al., 2020).
while not resolving the issue, we lay the theoreticalgroundwork for more systematic analyses by show-ing that plms can be regarded as serial dual-routemodels (caramazza et al., 1988), i.e., the meaningsof complex words are either stored or else need tobe computed from the subwords..contributions.
we present the ﬁrst study ex-amining how the input segmentation of plms,speciﬁcally bert, affects their interpretationsof derivationally complex english words.
weshow that plms can be interpreted as serial dual-route models, which implies that maximally mean-ingful input tokens should allow for the bestgeneralization on new words.
this hypothesisis conﬁrmed by a series of semantic probingtasks on which derivational segmentation substan-tially outperforms bert’s wordpiece segmenta-tion.
this suggests that the generalization ca-pabilities of plms could be further improved ifa morphologically-informed vocabulary of inputtokens were used.
we also publish three largedatasets of derivationally complex words with cor-responding semantic properties.1.
2 how are complex words processed?.
2.1 complex words in psycholinguistics.
the question of how complex words are processedhas been at the center of psycholinguistic researchover the last decades (see leminen et al.
(2019)for a recent review).
two basic processing mech-anisms have been proposed: storage, where themeaning of complex words is listed in the mentallexicon (manelis and tharp, 1977; butterworth,1983; feldman and fowler, 1987; bybee, 1988;stemberger, 1994; bybee, 1995; bertram et al.,2000a), and computation, where the meaning ofcomplex words is inferred based on the meaningof stem and afﬁxes (taft and forster, 1975; taft,1979, 1981, 1988, 1991, 1994; rastle et al., 2004;taft, 2004; rastle and davis, 2008)..1we make our code and data available at https://.
github.com/valentinhofmann/superbizarre..in contrasting with single-route frameworks,dual-route models allow for a combination of stor-age and computation.
dual-route models are fur-ther classiﬁed by whether they regard the processesof retrieving meaning from the mental lexicon andcomputing meaning based on stem and afﬁxes asparallel, i.e., both mechanisms are always activated(frauenfelder and schreuder, 1992; schreuder andbaayen, 1995; baayen et al., 1997, 2000; bertramet al., 2000b; new et al., 2004; kuperman et al.,2008, 2009), or serial, i.e., the computation-basedmechanism is only activated when the storage-based one fails (laudanna and burani, 1985; bu-rani and caramazza, 1987; caramazza et al., 1988;burani and laudanna, 1992; laudanna and burani,1995; alegre and gordon, 1999)..outside the taxonomy presented so far are recentmodels that assume multiple levels of representa-tion as well as various forms of interaction betweenthem (rácz et al., 2015; needle and pierrehumbert,2018).
in these models, sufﬁciently frequent com-plex words are stored together with representationsthat include their internal structure.
complex-wordprocessing is driven by analogical processes overthe mental lexicon (rácz et al., 2020)..2.2 complex words in nlp and plms.
most models of word meaning proposed in nlpcan be roughly assigned to either the single-routeor dual-route approach.
word embeddings thatrepresent complex words as whole-word vectors(deerwester et al., 1990; mikolov et al., 2013a,b;pennington et al., 2014) can be seen as single-routestorage models.
word embeddings that representcomplex words as a function of subword or mor-pheme vectors (schütze, 1992; luong et al., 2013)can be seen as single-route computation models.
finally, word embeddings that represent complexwords as a function of subword or morpheme vec-tors as well as whole-word vectors (botha and blun-som, 2014; qiu et al., 2014; bhatia et al., 2016;bojanowski et al., 2017; athiwaratkun et al., 2018;salle and villavicencio, 2018) are most closely re-lated to parallel dual-route approaches..where are plms to be located in this taxonomy?
plms represent many complex words as whole-word vectors (which are fully stored).
similarly tohow character-based models represent word mean-ing (kim et al., 2016; adel et al., 2017), they canalso store the meaning of frequent complex wordsthat are segmented into subwords, i.e., frequent sub-.
3595word collocations, in their model weights.
whenthe complex-word meaning is neither stored as awhole-word vector nor in the model weights, plmscompute the meaning as a compositional functionof the subwords.
conceptually, plms can thus beinterpreted as serial dual-route models.
while theparallelism has not been observed before, it followslogically from the structure of plms.
the key goalof this paper is to show that the implications of thisobservation are borne out empirically..as a concrete example, consider the com-plex words stabilize, realize, finalize,mobilize, tribalize, and templatize,which are all formed by adding the verbal suf-ﬁx ize to a nominal or adjectival stem.
tak-ing bert, speciﬁcally bertbase (uncased) (de-vlin et al., 2019), as the example plm, the wordsstabilize and realize have individual to-kens in the input vocabulary and are hence as-sociated with whole-word vectors storing theirmeanings, including highly lexicalized meaningsas in the case of realize.
by contrast, thewords finalize and mobilize are segmentedinto final, ##ize and mob, ##ili, ##ze,which entails that their meanings are not storedas whole-word vectors.
however, both wordshave relatively high absolute frequencies of 2,540(finalize) and 6,904 (mobilize) in the en-glish wikipedia, the main dataset used to pre-train bert (devlin et al., 2019), which meansthat bert can store their meanings in its modelweights during pretraining.2 notice this is even pos-sible in the case of highly lexicalized meanings asfor mobilize.
finally, the words tribalizeand templatize are segmented into tribal,##ize and te, ##mp, ##lat, ##ize, but as op-posed to finalize and mobilize they do notoccur in the english wikipedia.
as a result, bertcannot store their meanings in its model weightsduring pretraining and needs to compute them fromthe meanings of the subwords..seeing plms as serial dual-route models allowsfor a more nuanced view on the central researchquestion of this paper: in order to investigate se-mantic generalization we need to investigate therepresentations of those complex words that acti-vate the computation-based route.
the words thatdo so are the ones whose meaning is neither storedas a whole-word vector nor in the model weights.
2previous research suggests that such lexical knowledge is.
stored in the lower layers of bert (vuli´c et al., 2020)..and hence needs to be computed compositionallyas a function of the subwords (tribalize andtemplatize in the discussed examples).
we hy-pothesize that the morphological validity of thesegmentation affects the representational qualityin these cases, and that the best generalization isachieved by maximally meaningful tokens.
it iscrucial to note this does not imply that the tokenshave to be morphemes, but the segmentation bound-aries need to coincide with morphological bound-aries, i.e., groups of morphemes (e.g., tribalin the segmentation of tribalize) are alsopossible.3 for tribalize and templatize,we therefore expect the segmentation tribal,##ize (morphologically valid since all segmenta-tion boundaries are morpheme boundaries) to resultin a representation of higher quality than the seg-mentation te, ##mp, ##lat, ##ize (morpho-logically invalid since the boundaries between te,##mp, and ##lat are not morpheme boundaries).
on the other hand, complex words whose mean-ings are stored in the model weights (finalizeand mobilize in the discussed examples) areexpected to be affected by the segmentation to amuch lesser extent: if the meaning of a complexword is stored in the model weights, it should mat-ter less whether the speciﬁc segmentation activat-ing that meaning is morphologically valid (final,##ize) or not (mob, ##ili, ##ze).4.
3 experiments.
3.1 setup.
analyzing the impact of different segmentations onbert’s semantic generalization capabilities is notstraightforward since it is not clear a priori how tomeasure the quality of representations.
here, wedevise a novel lexical-semantic probing task: weuse bert’s representations for complex words topredict semantic dimensions, speciﬁcally sentimentand topicality (see figure 1).
for sentiment, giventhe example complex word superbizarre, thetask is to predict that its sentiment is negative.
for topicality, given the example complex wordisotopize, the task is to predict that it is usedin physics.
we conﬁne ourselves to binary predic-.
3this is in line with substantial evidence from linguisticsshowing that frequent groups of morphemes can be treated assemantic wholes (stump, 2017, 2019)..4we expect the distinction between storage and computa-tion of complex-word meaning for plms to be a continuum.
while the ﬁndings presented here are consistent with this view,we defer a more in-depth analysis to future work..3596class 1.class 2.dataset.
dimension.
|d| class.
examples.
class example.
amazon sentimenttopicalityarxivtopicalityreddit.
239,727 neg97,410 phys semithermal, ozoneless85,362 ent.
overpriced, crappy.
supervampires, spoilerful dis.
poscs.
megafavorite, applausiveautoencoded, rankableantirussian, immigrationism.
table 1: dataset characteristics.
the table provides information about the datasets such as the relevant semanticdimensions with their classes and example complex words.
|d|: number of complex words; neg: negative; pos:positive; phys: physics; cs: computer science; ent: entertainment; dis: discussion..tion, i.e., the probed semantic dimensions alwaysconsist of two classes (e.g., positive and negative).
the extent to which a segmentation supports a so-lution of this task is taken as an indicator of itsrepresentational quality..more formally, let d be a dataset consisting ofcomplex words x and corresponding classes y thatinstantiate a certain semantic dimension (e.g., sen-timent).
we denote with s(x) = (t1, .
.
.
, tk) thesegmentation of x into a sequence of k subwords.
we ask how s impacts the capability of bert topredict y, i.e., how p(y|(s(x)), the likelihood of thetrue semantic class y given a certain segmentationof x, depends on different choices for s. the twosegmentation methods we compare in this study arebert’s standard wordpiece segmentation (schus-ter and nakajima, 2012; wu et al., 2016), sw, anda derivational segmentation that segments complexwords into stems and afﬁxes, sd..3.2 data.
since existing datasets do not allow us to conductexperiments following the described setup, we cre-ate new datasets in a weakly-supervised fashionthat is conceptually similar to the method proposedby mintz et al.
(2009): we employ large datasetsannotated for sentiment or topicality, extract deriva-tionally complex words, and use the dataset labelsto establish their semantic classes..for determining and segmenting derivationallycomplex words, we use the algorithm introduced byhofmann et al.
(2020b), which takes as input a setof preﬁxes, sufﬁxes, and stems and checks for eachword in the data whether it can be derived from astem using a combination of preﬁxes and sufﬁxes.5the algorithm is sensitive to morpho-orthographicrules of english (plag, 2003), e.g., when the suf-.
5the distinction between inﬂectionally and derivationallycomplex words is notoriously fuzzy (haspelmath and sims,2010; ten hacken, 2014).
we try to exclude inﬂection as far aspossible (e.g., by removing problematic afﬁxes such as ing)but are aware that a clear separation does not exist..ﬁx ize is removed from isotopize, the resultis isotope, not isotop.
we follow hofmannet al.
(2020a) in using the preﬁxes, sufﬁxes, andstems in bert’s wordpiece vocabulary as input tothe algorithm.
this means that all tokens used bythe derivational segmentation are in principle alsoavailable to the wordpiece segmentation, i.e., thedifference between sw and sd does not lie in thevocabulary per se but rather in the way the vocab-ulary is used.
see appendix a.1 for details aboutthe derivational segmentation..to get the semantic classes, we compute for eachcomplex word which fraction of texts containingthe word belongs to one of two predeﬁned setsof dataset labels (e.g., reviews with four and ﬁvestars for positive sentiment) and rank all wordsaccordingly.
we then take the ﬁrst and third tertilesof complex words as representing the two classes.
we randomly split the words into 60% training,20% development, and 20% test..in the following, we describe the characteristicsof the three datasets in greater depth.
table 1 pro-vides summary statistics.
see appendix a.2 fordetails about data preprocessing..amazon.
amazon is an online e-commerce plat-form.
a large dataset of amazon reviews has beenmade publicly available (ni et al., 2019).6 we ex-tract derivationally complex words from reviewswith one or two (neg) as well as four or ﬁve stars(pos), discarding three-star reviews for a clearerseparation (yang and eisenstein, 2017)..arxiv.
arxiv is an open-access distribution ser-vice for scientiﬁc articles.
recently, a dataset of allpapers published on arxiv with associated meta-data has been released.7 for this study, we extractall articles from physics (phys) and computer sci-ence (cs), which we identify using arxiv’s subjectclassiﬁcation.
we choose physics and computer.
6https://nijianmo.github.io/amazon/.
index.html.
7https://www.kaggle.com/.
cornell-university/arxiv.
3597amazon.
arxiv.
reddit.
model.
dev.
test.
dev.
test.
dev.
test.
delbert .635 ± .001.619 ± .001bert.
.639 ± .002.624 ± .001.
.731 ± .001.704 ± .001.
.723 ± .001.700 ± .002.
.696 ± .001.664 ± .001.
.701 ± .001.664 ± .003.stemafﬁxes.
.572 ± .003.536 ± .008.
.573 ± .003.539 ± .008.
.705 ± .001.605 ± .001.
.697 ± .001.603 ± .002.
.679 ± .001.596 ± .001.
.684 ± .002.596 ± .001.table 2: results.
the table shows the average performance as well as standard deviation (f1) of 20 models trainedwith different random seeds.
best result per column highlighted in gray, second-best in light gray..figure 2: convergence analysis.
the upper panels show the distributions of the number of epochs after whichthe models reach their maximum validation performance.
the lower panels show the trajectories of the averagevalidation performance (f1) across epochs.
the plots are based on 20 models trained with different random seeds.
the convergence statistics for delbert and bert are directly comparable because the optimal learning rate isthe same (see appendix a.3).
delbert models reach their performance peak faster than bert models..science since we expect large topical distancesfor these classes (compared to alternatives suchas mathematics and computer science)..reddit.
reddit is a social media platform host-ing discussions about various topics.
it is dividedinto smaller communities, so-called subreddits,which have been shown to be a rich source ofderivationally complex words (hofmann et al.,2020c).
hofmann et al.
(2020a) have published adataset of derivatives found on reddit annotatedwith the subreddits in which they occur.8 inspiredby a content-based subreddit categorizationscheme,9 we deﬁne two groups of subreddits,an entertainment set (ent) consisting of thesubreddits anime, destinythegame, funny,games,leagueoflegends,movies, music, pics, and videos, as well asa discussion set (dis) consisting of the subred-.
gaming,.
8https://github.com/valentinhofmann/.
dagobert.
9https://www.reddit.com/r/.
theoryofreddit/comments/1f7hqc/the_200_most_active_subreddits_categorized_by.
dits askscience, atheism, conspiracy,news, libertarian, politics, science,technology,andworldnews,and extract all derivationallycomplex words occurring in them.
we againexpect large topical distances for these classes..twoxchromosomes,.
given that the automatic creation of the datasetsnecessarily introduces noise, we measure humanperformance on 100 randomly sampled words perdataset, which ranges between 71% (amazon) and78% (arxiv).
these values can thus be seen as anupper bound on performance..3.3 models.
we train two main models on each binary classi-ﬁcation task: bert with the standard wordpiecesegmentation (sw) and bert using the derivationalsegmentation (sd), a model that we refer to as del-bert (derivation leveraging bert).
bert anddelbert are identical except for the way in whichthey use the vocabulary of input tokens (but thevocabulary itself is also identical for both models)..3598figure 3: frequency analysis.
the plots show the average performance (accuracy) of 20 bert and delbertmodels trained with different random seeds for complex words of low (f ≤ 5), mid (5 < f ≤ 500), and high(f > 500) frequency.
on all three datasets, bert performs similarly or better than delbert for complex wordsof high frequency but worse for complex words of low and mid frequency..the speciﬁc bert variant we use is bertbase(uncased) (devlin et al., 2019).
for the derivationalsegmentation, we follow previous work by hof-mann et al.
(2020a) in separating stem and preﬁxesby a hyphen.
we further follow casanueva et al.
(2020) and vuli´c et al.
(2020) in mean-pooling theoutput representations for all subwords, excludingbert’s special tokens.
the mean-pooled repre-sentation is then fed into a two-layer feed-forwardnetwork for classiﬁcation.
to examine the rela-tive importance of different types of morphologicalunits, we train two additional models in which weablate information about stems and afﬁxes, i.e., werepresent stems and afﬁxes by the same randomlychosen input embedding.10.
we ﬁnetune bert, delbert, and the two ab-lated models on the three datasets using 20 differ-ent random seeds.
we choose f1 as the evaluationmeasure.
see appendix a.3 for details about im-plementation and hyperparameters..3.4 results.
delbert (sd) outperforms bert (sw) by a largemargin on all three datasets (table 2).
it is inter-esting to notice that the performance difference islarger for arxiv and reddit than for amazon, indi-cating that the gains in representational quality areparticularly large for topicality..what is it that leads to delbert’s increased per-formance?
the ablation study shows that modelsusing only stem information already achieve rel-atively high performance and are on par or evenbetter than the bert models on arxiv and red-dit.
however, the delbert models still performsubstantially better than the stem models on allthree datasets.
the gap is particularly pronounced.
10for afﬁx ablation, we use two different input embeddings.
for preﬁxes and sufﬁxes..for amazon, which indicates that the interactionbetween the meaning of stem and afﬁxes is morecomplex for sentiment than for topicality.
thismakes sense from a linguistic point of view: whilestems tend to be good cues for the topical associa-tions of a complex word, sentiment often dependson semantic interactions between stems and afﬁxes.
for example, while the preﬁx un turns the senti-ment of amusing negative, it turns the sentimentof biased positive.
such effects involving nega-tion and antonymy are known to be challenging forplms (ettinger, 2020; kassner and schütze, 2020)and might be one of the reasons for the generallylower performance on amazon.11 the performanceof models using only afﬁxes is much lower..3.5 quantitative analysis.
to further examine how bert (sw) and delbert(sd) differ in the way they infer the meaning ofcomplex words, we perform a convergence analy-sis.
we ﬁnd that the delbert models reach theirpeak in performance faster than the bert models(figure 2).
this is in line with our interpretation ofplms as serial dual-route models (see section 2.2):while delbert operates on morphological unitsand can combine the subword meanings to infer themeanings of complex words, bert’s subwords donot necessarily carry lexical meanings, and hencethe derivational patterns need to be stored by adapt-ing the model weights.
this is an additional burden,leading to longer convergence times and substan-tially worse overall performance..our hypothesis that plms can use two routes.
11another reason for the lower performance on sentiment isthat the datasets were created automatically (see section 3.2),and hence many complex words do not directly carry infor-mation about sentiment or topicality.
the density of suchwords is higher for sentiment than topicality since the topic ofdiscussion affects the likelihoods of most content words..3599(a) topicality prediction.
(b) sentiment prediction.
figure 4: accuracy increase of delbert compared to bert for preﬁxes.
the plots show the accuracy increaseas a function of the proportion of morphologically incorrect wordpiece segmentations (topicality prediction) andas ordered boxplot pairs centered on the median accuracy of bert (sentiment prediction).
negative values meanthat the delbert models have a lower accuracy than the bert models for a certain preﬁx..to process complex words (storage in weights andcompositional computation based on input embed-dings), and that the second route is blocked whenthe input segmentation is not morphological, sug-gests the existence of frequency effects: bertmight have seen frequent complex words multipletimes during pretraining and stored their meaningin the model weights.
this is less likely for in-frequent complex words, making the capability tocompositionally infer the meaning (i.e., the compu-tation route) more important.
we therefore expectthe difference in performance between delbert(which should have an advantage on the computa-tion route) and bert to be larger for infrequentwords.
to test this hypothesis, we split the complexwords of each dataset into three bins of low (f ≤ 5),mid (5 < f ≤ 500), and high (f > 500) absolutefrequencies, and analyze how the performance ofbert and delbert differs on the three bins.
forthis and all subsequent analyses, we merge devel-opment and test sets and use accuracy instead off1 since it makes comparisons across small setsof data points more interpretable.
the results arein line with our hypothesis (figure 3): bert per-forms worse than delbert on complex words oflow and mid frequencies but achieves very similar(arxiv, reddit) or even better (amazon) accuracies.
on high-frequency complex words.
these resultsstrongly suggest that two different mechanisms areinvolved, and that bert has a disadvantage forcomplex words that do not have a high frequency.
at the same time, the slight advantage of bert onhigh-frequency complex words indicates that it hashigh-quality representations of these words in itsweights, which delbert cannot exploit since ituses a different segmentation..we are further interested to see whether the af-ﬁx type has an impact on the relative performanceof bert and delbert.
to examine this question,we measure the accuracy increase of delbert ascompared to bert for individual afﬁxes, averagedacross datasets and random seeds.
we ﬁnd thatthe increase is almost twice as large for preﬁxes(µ = .023, σ = .017) than for sufﬁxes (µ = .013,σ = .016), a difference that is shown to be sig-niﬁcant by a two-tailed welch’s t-test (d = .642,t(82.97) = 2.94, p < .01).12 why is having accessto the correct morphological segmentation moreadvantageous for preﬁxed than sufﬁxed complexwords?
we argue that there are two key factors atplay.
first, the wordpiece tokenization sometimesgenerates the morphologically correct segmenta-.
12we use a welch’s instead of student’s t-test since it does.
not assume that the distributions have equal variance..3600dataset.
x.y.sd(x).
µp.
sw(x).
applausivesuperannoyingoverseasoned.
isotopizeantimicrosoftinkinetic.
pos applause, ##iveneg super, -, annoyingneg over, -, seasoned.
.847 app, ##laus, ##ive.967 super, ##ann, ##oy, ##ing.956 overseas, ##oned.
phy isotope, ##izecsphy in, -, kinetic.
anti, -, microsoft.
.985 iso, ##top, ##ize.936 anti, ##mic, ##ros, ##oft.983 ink, ##ine, ##tic.
µp.
.029.278.219.
.039.013.035.prematuration.089nonmultiplayer ent non, -, multiplayer .950 non, ##mu, ##lt, ##ip, ##layer .216promosque.066.
.848 prem, ##at, ##uration.
dis premature, ##ation.
.961 promo, ##sque.
dis pro, -, mosque.
amazon.
arxiv.
reddit.
table 3: error analysis.
the table gives example complex words that are consistently classiﬁed correctly bydelbert and incorrectly by bert.
x: complex word; y: semantic class; sd(x): derivational segmentation; µp:average likelihood of true semantic class across 20 models trained with different random seeds; sw(x): wordpiecesegmentation.
for the complex words shown, µp is considerably higher with delbert than with bert..tion, but it does so with different frequencies forpreﬁxes and sufﬁxes.
to detect morphologically in-correct segmentations, we check whether the word-piece segmentation keeps the stem intact, which isin line with our deﬁnition of morphological validity(section 2.2) and provides a conservative estimateof the error rate.
for preﬁxes, the wordpiece to-kenization is seldom correct (average error rate:µ = .903, σ = .042), whereas for sufﬁxes it iscorrect about half the time (µ = .503, σ = .213).
hence, delbert gains a greater advantage for pre-ﬁxed words.
second, preﬁxes and sufﬁxes havedifferent linguistic properties that affect the predic-tion task in unequal ways.
speciﬁcally, whereassufﬁxes have both syntactic and semantic functions,preﬁxes have an exclusively semantic function andalways add lexical-semantic meaning to the stem(giraudo and grainger, 2003; beyersmann et al.,2015).
as a result, cases such as unamusingwhere the afﬁx boundary is a decisive factor for theprediction task are more likely to occur with pre-ﬁxes than sufﬁxes, thus increasing the importanceof a morphologically correct segmentation.13.
given the differences between sentiment andtopicality prediction, we expect variations in therelative importance of the two identiﬁed factors:(i) in the case of sentiment the advantage of sdshould be maximal for afﬁxes directly affectingsentiment; (ii) in the case of topicality its advan-tage should be the larger the higher the proportionof incorrect segmentations for a particular afﬁx,and hence the more frequent the cases where del-bert has access to the stem while bert doesnot.
to test this hypothesis, we focus on pre-.
13notice that there are sufﬁxes with similar semantic effects.
(e.g., less), but they are less numerous..dictions for preﬁxed complex words.
for eachdataset, we measure for individual preﬁxes the ac-curacy increase of the delbert models as com-pared to the bert models, averaged across randomseeds, as well as the proportion of morphologi-cally incorrect segmentations produced by word-piece.
we then calculate linear regressions to pre-dict the accuracy increases based on the propor-tions of incorrect segmentations.
this analysisshows a signiﬁcant positive correlation for arxiv(r2 = .304, f (1, 41) = 17.92, p < 0.001) andreddit (r2 = .270, f (1, 40) = 14.80, p < 0.001)but not for amazon (r2 = .019, f (1, 41) = .80,p = .375), which is in line with our expectations(figure 4a).
furthermore, ranking the preﬁxes byaccuracy increase for amazon conﬁrms that themost pronounced differences are found for preﬁxesthat can change the sentiment such as non, anti,mal, and pseudo (figure 4b)..3.6 qualitative analysis.
besides quantitative factors, we are interested inidentifying qualitative contexts in which delberthas a particular advantage compared to bert.
todo so, we ﬁlter the datasets for complex words thatare consistently classiﬁed correctly by delbertand incorrectly by bert.
speciﬁcally, we computefor each word the average likelihood of the truesemantic class across delbert and bert mod-els, respectively, and rank words according to thelikelihood difference between both model types.
examining the words with the most extreme differ-ences, we observe three classes (table 3)..first,.
the addition of a sufﬁx is often con-nected with morpho-orthographic changes (e.g.,the deletion of a stem-ﬁnal e), which leads to asegmentation of the stem into several subwords.
3601since the truncated stem is not in the word-piece vocabulary (applausive, isotopize,prematuration).
the model does not seemto be able to recover the meaning of the stemfrom the subwords.
second, the addition of apreﬁx has the effect that the word-internal (as op-posed to word-initial) form of the stem would haveto be available for proper segmentation.
sincethis form rarely exists in the wordpiece vocab-ulary, the stem is segmented into several sub-words (superannoying, antimicrosoft,nonmultiplayer).
again, it does not seemto be possible for the model to recover the mean-ing of the stem.
third, the segmentation of pre-ﬁxed complex words often fuses the preﬁx withthe ﬁrst characters of the stem (overseasoned,inkinetic, promosque).
this case is particu-larly detrimental since it not only makes it difﬁcultto recover the meaning of the stem but also cre-ates associations with unrelated meanings, some-times even opposite meanings as in the case ofsuperbizarre.
the three classes thus under-score the difﬁculty of inferring the meaning ofcomplex words from the subwords when the whole-word meaning is not stored in the model weightsand the subwords are not morphological..4 related work.
several recent studies have examined how the per-formance of plms is affected by their input seg-mentation.
tan et al.
(2020) show that tokenizinginﬂected words into stems and inﬂection symbolsallows bert to generalize better on non-standardinﬂections.
bostrom and durrett (2020) pretrainroberta with different tokenization methods andﬁnd tokenizations that align more closely with mor-phology to perform better on a number of tasks.
ma et al.
(2020) show that providing bert withcharacter-level information also leads to enhancedperformance.
relatedly, studies from automaticspeech recognition have demonstrated that mor-phological decomposition improves the perplexityof language models (fang et al., 2015; jain et al.,2020).
whereas these studies change the vocabu-lary of input tokens (e.g., by adding special tokens),we show that even when keeping the pretrained vo-cabulary ﬁxed, employing it in a morphologicallycorrect way leads to better performance.14.
14there are also studies that analyze morphological aspectsof plms without a focus on questions surrounding segmenta-tion (edmiston, 2020; klemen et al., 2020)..most nlp studies on derivational morphologyhave been devoted to the question of how semanticrepresentations of derivationally complex wordscan be enhanced by including morphological in-formation (luong et al., 2013; botha and blun-som, 2014; qiu et al., 2014; bhatia et al., 2016;cotterell and schütze, 2018), and how afﬁx em-beddings can be computed (lazaridou et al., 2013;kisselew et al., 2015; padó et al., 2016).
cotterellet al.
(2017), vylomova et al.
(2017), and deutschet al.
(2018) propose sequence-to-sequence modelsfor the generation of derivationally complex words.
hofmann et al.
(2020a) address the same task us-ing bert.
in contrast, we analyze how differentinput segmentations affect the semantic representa-tions of derivationally complex words in plms, aquestion that has not been addressed before..5 conclusion.
we have examined how the input segmentation ofplms, speciﬁcally bert, affects their interpreta-tions of derivationally complex words.
drawingupon insights from psycholinguistics, we have de-duced a conceptual interpretation of plms as serialdual-route models, which implies that maximallymeaningful input tokens should allow for the bestgeneralization on new words.
this hypothesis wasconﬁrmed by a series of semantic probing tasks onwhich delbert, a model using derivational seg-mentation, consistently outperformed bert usingwordpiece segmentation.
quantitative and qualita-tive analyses further showed that bert’s inferiorperformance was caused by its inability to infer thecomplex-word meaning as a function of the sub-words when the complex-word meaning was notstored in the weights.
overall, our ﬁndings suggestthat the generalization capabilities of plms couldbe further improved if a morphologically-informedvocabulary of input tokens were used..acknowledgements.
this work was funded by the european researchcouncil (#740516) and the engineering and phys-ical sciences research council (ep/t023333/1).
the ﬁrst author was also supported by the germanacademic scholarship foundation and the artsand humanities research council.
we thank thereviewers for their helpful comments..3602references.
heike adel, ehsaneddin asgari, and hinrich schütze.
2017. overview of character-based models for nat-in international confer-ural language processing.
ence on computational linguistics and intelligenttext processing (cicling) 18..maria alegre and peter gordon.
1999. frequency ef-fects and the representational status of regular inﬂec-tions.
journal of memory and language, 40:41–61..ben athiwaratkun, andrew wilson, and anima anand-kumar.
2018. probabilistic fasttext for multi-senseword embeddings.
in annual meeting of the associ-ation for computational linguistics (acl) 56..r. harald baayen, ton dijkstra, and robert schreuder.
1997. singulars and plurals in dutch: evidence fora parallel dual-route model.
journal of memory andlanguage, 37:94–117..r. harald baayen and rochelle lieber.
1991. produc-tivity and english derivation: a corpus-based study.
linguistics, 29(5)..r. harald baayen, robert schreuder, and richardsproat.
2000. morphology in the mental lexicon: acomputational model for visual word recognition.
infrank van eynde and dafydd gibbon, editors, lex-icon development for speech and language process-ing, pages 267–293.
springer, dordrecht..raymond bertram, matti laine, r. harald baayen,robert schreuder, and jukka hyönä.
2000a.
afﬁxalhomonymy triggers full-form storage, even with in-ﬂected words, even in a morphologically rich lan-guage.
cognition, 74:b13–b25..raymond bertram, robert schreuder, and r. haraldbaayen.
2000b.
the balance of storage and com-putation in morphological processing: the role ofword formation type, afﬁxal homonymy, and produc-tivity.
journal of experimental psychology: learn-ing, memory, and cognition, 26(2):489–511..elisabeth beyersmann,.
johannes c. ziegler, andjonathan grainger.
2015. differences in the pro-cessing of preﬁxes and sufﬁxes revealed by ascientiﬁc studies of reading,letter-search task.
19(5):360–373..parminder bhatia, robert guthrie, and jacob eisen-stein.
2016. morphological priors for probabilisticin conference on em-neural word embeddings.
pirical methods in natural language processing(emnlp) 2016..piotr bojanowski, edouard grave, armand joulin, andtomas mikolov.
2017. enriching word vectors withsubword information.
transactions of the associa-tion for computational linguistics, 5:135–146..kaj bostrom and greg durrett.
2020. byte pair encod-ing is suboptimal for language model pretraining.
infindings of empirical methods in natural languageprocessing (emnlp) 2020..jan a. botha and phil blunsom.
2014. compositionalmorphology for word representations and languagemodelling.
in international conference on machinelearning (icml) 31..cristina burani and alfonso caramazza.
1987. rep-resentation and processing of derived words.
lan-guage and cognitive processes, 2(3-4):217–227..cristina burani and alessandro laudanna.
1992. unitsof representation for derived words in the lexicon.
inram frost and leonard katz, editors, orthography,phonology, morphology, and meaning, pages 361–376. north-holland, amsterdam..brian butterworth.
1983. lexical representation..inbrian butterworth, editor, language production:development, writing and other language processes,pages 257–294.
academic press, london..joan bybee.
1988. morphology as lexical organiza-tion.
in michael hammond and michael noonan,editors, theoretical approaches to morphology: ap-proaches in modern linguistics, pages 119–141.
aca-demic press, san diego, ca..joan bybee.
1995. regular morphology and the lex-icon.
language and cognitive processes, 10(425-455)..alfonso caramazza, alessandro laudanna,.
andcristina romani.
1988. lexical access and inﬂec-tional morphology.
cognition, 28(297-332)..iñigo casanueva, tadas temˇcinas, daniela gerz,matthew henderson, and ivan vuli´c.
2020. efﬁ-cient intent detection with dual sentence encoders.
in workshop on natural language processing forconversational ai 2..kenneth church.
2020..sub-words, seriously?
natural language engineering,26(3):375–382..emerging trends:.
kevin clark, minh-thang luong, quoc v. le, andchristopher d. manning.
2020. electra: pre-training text encoders as discriminators rather thanin international conference on learn-generators.
ing representations (iclr) 8..ryan cotterell and hinrich schütze.
2018..joint se-mantic synthesis and morphological analysis of thederived word.
transactions of the association forcomputational linguistics, 6:33–48..ryan cotterell, ekaterina vylomova, huda khayral-lah, christo kirov, and david yarowsky.
2017.paradigm completion for derivational morphology.
in conference on empirical methods in natural lan-guage processing (emnlp) 2017..scott deerwester, susan t. dumais, george furnas,thomas landauer, and richard harshman.
1990.journalindexing by latent semantic analysis.
of the american society for information science,41(6):391–407..3603daniel deutsch, john hewitt, and dan roth.
2018. adistributional and orthographic aggregation modelin annualfor english derivational morphology.
meeting of the association for computational lin-guistics (acl) 56..jacob devlin, ming-wei chang, kenton lee, andkristina toutanova.
2019. bert: pre-training ofdeep bidirectionaltransformers for language un-in annual conference of the northderstanding.
american chapter of the association for computa-tional linguistics: human language technologies(naacl htl) 2019..jesse dodge, suchin gururangan, dallas card, royschwartz, and noah a. smith.
2019. show yourwork: improved reporting of experimental results.
in conference on empirical methods in natural lan-guage processing (emnlp) 2019..daniel edmiston.
2020. a systematic analysis of mor-phological content in bert models for multiple lan-guages.
in arxiv 2004.03032..kawin ethayarajh.
2019. how contextual are contex-tualized word representations?
comparing the geom-etry of bert, elmo, and gpt-2 embeddings.
inconference on empirical methods in natural lan-guage processing (emnlp) 2019..allyson ettinger.
2020. what bert is not: lessonsfrom a new suite of psycholinguistic diagnostics forlanguage models.
transactions of the associationfor computational linguistics, 8:34–48..hao fang, mari ostendorf, peter baumann, andjanet b. pierrehumbert.
2015. exponential languagemodeling using morphological features and multi-ieee/acm transactions on audio,task learning.
speech, and language processing, 23(12):2410–2421..laurie b. feldman and carol a. fowler.
1987. the in-ﬂected noun system in serbo-croatian: lexical rep-resentation of morphological structure.
memory andcognition, 15(1):1–12..uli h. frauenfelder and robert schreuder.
1992. con-straining psycholinguistic models of morphologicalprocessing and representation: the role of productiv-ity.
in geert booij and jaap van marle, editors, year-book of morphology 1991, volume 26, pages 165–183. kluwer, dordrecht..philip gage.
1994. a new algorithm for data compres-.
sion.
the c users journal, 12(2):23–38..hélène giraudo and jonathan grainger.
2003. on therole of derivational afﬁxes in recognizing complexwords: evidence from masked priming.
in r. har-ald baayen and robert schreuder, editors, morpho-logical structure in language processing, pages 209–232. de gruyter, berlin..pius ten hacken.
2014. delineating derivation and in-ﬂection.
in rochelle lieber and pavol štekauer, edi-tors, the oxford handbook of derivational morphol-ogy, pages 10–25.
oxford university press, oxford..bo han and timothy baldwin.
2011. lexical normali-sation of short text messages: makn sens a #twitter.
in annual meeting of the association for computa-tional linguistics (acl) 49..martin haspelmath and andrea d. sims.
2010. under-standing morphology.
routledge, new york, ny..benjamin heinzerling and michael strube.
2019. se-quence tagging with contextual and non-contextualsubword representations: a multilingual evaluation.
in annual meeting of the association for computa-tional linguistics (acl) 57..john hewitt and christopher d. manning.
2019. astructural probe for ﬁnding syntax in word repre-in annual conference of the northsentations.
american chapter of the association for computa-tional linguistics: human language technologies(naacl htl) 2019..valentin hofmann, janet b. pierrehumbert, and hin-rich schütze.
2020a.
dagobert: generating deriva-tional morphology with a pretrained language model.
in conference on empirical methods in natural lan-guage processing (emnlp) 2020..valentin hofmann, janet b. pierrehumbert, and hin-rich schütze.
2020b.
predicting the growth of mor-phological families from social and linguistic factors.
in annual meeting of the association for computa-tional linguistics (acl) 58..valentin hofmann, hinrich schütze, and janet b. pier-rehumbert.
2020c.
a graph auto-encoder modelin annual meeting ofof derivational morphology.
the association for computational linguistics (acl)58..abhilash jain, aku rouhe, stig-arne grönroos, andfinnish asr with deepmikko kurimo.
2020.in conference of the interna-transformer models.
tional speech communication association (inter-speech) 21..ganesh jawahar, benoit sagot, and djamé seddah.
2019. what does bert learn about the structureof language?
in annual meeting of the associationfor computational linguistics (acl) 57..emilia kacprzak, laura m. koesten, luis-danielibáñez, elena simperl, and jeni tennison.
2017. aquery log analysis of dataset search.
in internationalconference on web engineering (icwe) 17..nora kassner and hinrich schütze.
2020. negated andmisprimed probes for pretrained language models:birds can talk, but cannot ﬂy.
in annual meeting ofthe association for computational linguistics (acl)58..3604yoon kim, yacine jernite, david sontag, and alexan-der m. rush.
2016. character-aware neural lan-in conference on artiﬁcial intelli-guage models.
gence (aaai) 30..diederik p. kingma and jimmy l. ba.
2015. adam: amethod for stochastic optimization.
in internationalconference on learning representations (iclr) 3..max kisselew, sebastian padó, alexis palmer, and janšnajder.
2015. obtaining a better understanding ofdistributional models of german derivational mor-phology.
in international conference on computa-tional semantics (iwcs) 11..matej klemen, luka krsnik, and marko robnik-enhancing deep neural net-in arxiv.
šikonja.
2020.works with morphological information.
2011.12432..victor kuperman, raymond bertram, and r. haraldbaayen.
2008. morphological dynamics in com-pound processing.
language and cognitive pro-cesses, 23(7-8):1089–1132..victor kuperman, robert schreuder, raymondbertram, and r. harald baayen.
2009. readingof polymorphemic dutch compounds: towards amultiple route model of lexical processing.
journalof experimental psychology: human perceptionand performance, 35(3):876–895..alessandro laudanna and cristina burani.
1985. ad-dress mechanisms to decomposed lexical entries.
linguistics, 23(5)..alessandro laudanna and cristina burani.
1995. dis-tributional properties of derivational afﬁxes: impli-cations for processing.
in laurie b. feldman, edi-tor, morphological aspects of language processing,pages 345–364.
lawrence erlbaum, hillsdale, nj..angeliki lazaridou, marco marelli, roberto zampar-elli, and marco baroni.
2013. compositional-lyderived representations of morphologically complexwords in distributional semantics.
in annual meet-ing of the association for computational linguistics(acl) 51..alina leminen, eva smolka, jon duñabeitia, andchristos pliatsikas.
2019. morphological process-ing in the brain: the good (inﬂection), the bad(derivation) and the ugly (compounding).
cortex,116:4–44..minh-thang luong, richard socher, and christo-pher d. manning.
2013. better word representa-tions with recursive neural networks for morphology.
in conference on computational natural languagelearning (conll) 17..wentao ma, yiming cui, chenglei si, ting liu, shi-jin wang, and guoping hu.
2020. charbert:character-aware pre-trained language model.
in in-ternational conference on computational linguis-tics (coling) 28..leon manelis and david a. tharp.
1977. the pro-cessing of afﬁxed words.
memory and cognition,5(6):690–695..louis martin, benjamin muller, pedro j. suárez,yoann dupont, laurent romary, de la clergerie,éric v., djamé seddah, and benoit sagot.
2020.camembert: a tasty french language model.
inannual meeting of the association for computa-tional linguistics (acl) 58..tomas mikolov, kai chen, greg corrado, and jeffreydean.
2013a.
efﬁcient estimation of word represen-tations in vector space.
in arxiv 1301.3781..tomas mikolov, ilya sutskever, kai chen, greg cor-rado, and jeffrey dean.
2013b.
distributed represen-tations of words and phrases and their composition-ality.
in advances in neural information processingsystems (nips) 26..mike mintz, steven bills, rion snow, and dan juraf-sky.
2009. distant supervision for relation extrac-in annual meeting oftion without labeled data.
the association for computational linguistics (acl)47..jeremy m. needle and janet b. pierrehumbert.
2018.gendered associations of english morphology.
jour-nal of the association for laboratory phonology,9(1):119..boris new, marc brysbaert, juan segui, ludovic fer-rand, and kathleen rastle.
2004. the processingof singular and plural nouns in french and english.
journal of memory and language, 51(4):568–585..jianmo ni, jiacheng li, and julian mcauley.
2019.justifying recommendations using distantly-labeledreviews and ﬁned-grained aspects.
in conference onempirical methods in natural language processing(emnlp) 2019..sebastian padó, aurélie herbelot, max kisselew, andjan šnajder.
2016. predictability of distributionalsemantics in derivational word formation.
in inter-national conference on computational linguistics(coling) 26..jeffrey pennington, richard socher, and christopher d.manning.
2014. glove: global vectors for wordin conference on empirical meth-representation.
ods in natural language processing (emnlp)2014..yuval pinter, cassandra l. jacobs, and jacob eisen-in findings of em-stein.
2020. will it unblend?
pirical methods in natural language processing(emnlp) 2020..ingo plag.
2003. word-formation in english.
cam-.
bridge university press, cambridge, uk..siyu qiu, qing cui, jiang bian, bin gao, and tie-yanliu.
2014. co-learning of word representations andmorpheme representations.
in international confer-ence on computational linguistics (coling) 25..3605péter rácz, clay beckner, jennifer hay, and janet b.pierrehumbert.
2020. morphological convergenceas on-line lexical analogy.
language, 96(4):735–770..péter rácz, janet b. pierrehumbert, jennifer hay, andviktória papp.
2015. morphological emergence.
inbrian macwhinney and william o’grady, editors,the handbook of language emergence, pages 123–146. wiley, hoboken, nj..alec radford, jeffrey wu, rewon child, david luan,dario amodei, and ilya sutskever.
2019. languagemodels are unsupervised multitask learners..colin raffel, noam shazeer, adam roberts, katherinelee, sharan narang, michael matena, yanqi zhou,wei li, and peter j. liu.
2020. exploring the lim-its of transfer learning with a uniﬁed text-to-texttransformer.
journal of machine learning research,21:1–67..kathleen rastle and matthew h. davis.
2008. mor-phological decomposition based on the analysis oforthography.
language and cognitive processes,23(7-8):942–971..kathleen rastle, matthew h. davis,.
and borisnew.
2004. the broth in my brother’s brothel:morpho-orthographic segmentation in visual wordpsychonomic bulletin and review,recognition.
11(6):1090–1098..alexandre salle and aline villavicencio.
2018..in-corporating subword information into matrix factor-in workshop on sub-ization word embeddings.
word/character level models 2..robert schreuder and r. harald baayen.
1995. mod-eling morphological processing.
in laurie b. feld-man, editor, morphological aspects of language pro-cessing, pages 131–154.
lawrence erlbaum, hills-dale, nj..mike schuster and kaisuke nakajima.
2012. japanesein international confer-and korean voice search.
ence on acoustics, speech, and signal processing(icassp) 37..hinrich schütze.
1992. word space..in advancesin neural information processing systems (nips) 5,pages 895–902..rico sennrich, barry haddow, and alexandra birch.
2016. neural machine translation of rare words withsubword units.
in annual meeting of the associationfor computational linguistics (acl) 54..suzanna sia, ayush dalmia, and sabrina j. mielke.
2020. tired of topic models?
clusters of pretrainedword embeddings make for fast and good topics too!
in conference on empirical methods in natural lan-guage processing (emnlp) 2020..joseph p. stemberger.
1994. rule-less morphology atthe phonology-lexicon interface.
in susan d. lima,roberta corrigan, and gregory iverson, editors, thereality of linguistic rules, pages 147–169.
john ben-jamins, amsterdam..gregory stump.
2017. rule conﬂation in an inferential-realizational theory of morphotactics.
acta linguis-tica academica, 64(1):79–124..gregory stump.
2019. some sources of apparent gapsin derivational paradigms.
morphology, 29(2):271–292..marcus taft.
1979. recognition of afﬁxed words andthe word frequency effect.
memory and cognition,7(4):263–272..marcus taft.
1981. preﬁx stripping revisited.
journalof verbal learning and verbal behavior, 20:289–297..marcus taft.
1988. a morphological-decompositionmodel of lexical representation.
linguistics, 26:657–667..marcus taft.
1991. reading and the mental lexicon..lawrence erlbaum, hove, uk..marcus taft.
1994..interactive-activation as a frame-work for understanding morphological processing.
language and cognitive processes, 9(3):271–294..marcus taft.
2004. morphological decomposition andthe reverse base frequency effect.
the quarterlyjournal of experimental psychology, 57(4):745–765..marcus taft and kenneth i. forster.
1975. lexical stor-age and retrieval of preﬁxed words.
journal of ver-bal learning and verbal behavior, 14:638–647..samson tan, shaﬁq joty, lav r. varshney, and min-yen kan. 2020. mind your inﬂections!
improvingnlp for non-standard englishes with base-inﬂectionencoding.
in conference on empirical methods innatural language processing (emnlp) 2020..ivan vuli´c, edoardo m. ponti, robert litschko, goranglavaš, and anna korhonen.
2020. probing pre-trained language models for lexical semantics.
inconference on empirical methods in natural lan-guage processing (emnlp) 2020..ekaterina vylomova, ryan cotterell, timothy bald-win, and trevor cohn.
2017. context-aware predic-in conference oftion of derivational word-forms.
the european chapter of the association for com-putational linguistics (eacl) 15..yonghui wu, mike schuster, zhifeng chen, quocle v, mohammad norouzi, wolfgang macherey,maxim krikun, yuan cao, qin gao, klausmacherey, jeff klingner, apurva shah, melvin john-son, xiaobing liu, łukasz kaiser, stephan gouws,yoshikiyo kato, taku kudo, hideto kazawa, keith.
3606a.2 data preprocessing.
we exclude texts written in a language other thanenglish and remove strings containing numbers aswell as hyperlinks.
we follow han and baldwin(2011) in reducing repetitions of more than threeletters (niiiiice) to three letters..a.3 hyperparameters.
the feed-forward network has a relu activationafter the ﬁrst layer and a sigmoid activation afterthe second layer.
the ﬁrst layer has 100 dimen-sions.
we apply dropout of 0.2 after the ﬁrst layer.
all other hyperparameters are as for bertbase(uncased) (devlin et al., 2019).
the number oftrainable parameters is 109,559,241..∈.
of.
for.
the.
search.
we use a batch size of 64 and performepochsgridnumbern{1, .
.
.
, 20} and the learning ratel ∈ {1 × 10−6, 3 × 10−6, 1 × 10−5, 3 × 10−5}(selection criterion: f1 score).
we tune l onreddit (80 hyperparameter search trials per modeltype) and use the best conﬁguration (which isidentical for all model types) for 20 training runswith different random seeds on all three datasets(20 hyperparameter search trials per model type,dataset, and random seed).
models are trainedwith binary cross-entropy as the loss function andadam (kingma and ba, 2015) as the optimizer.
experiments are performed on a geforce gtx1080 ti gpu (11gb)..table 4 lists statistics of the validation perfor-mance over hyperparameter search trials and pro-vides information about best hyperparameter con-ﬁgurations as well as runtimes.16 see also section3.5 and particularly figure 2 in the main text, wherewe present a detailed analysis of the convergencebehavior of the two main model types examined inthis study (delbert and bert)..stevens, george kurian, nishant patil, wei wang,cliff young, jason smith, jason riesa, alex rud-nick, oriol vinyals, greg corrado, macduff hughes,and jeffrey dean.
2016. google’s neural machinetranslation system: bridging the gap between humanand machine translation.
in arxiv 1609.08144..yi yang and jacob eisenstein.
2017. overcoming lan-guage variation in sentiment analysis with social at-tention.
transactions of the association for compu-tational linguistics, 5:295–307..zhilin yang, zihang dai, yiming yang, jaime car-bonell, ruslan salakhutdinov, and quoc v. le.
2019.xlnet: generalized autoregressive pretraining forlanguage understanding.
in advances in neural in-formation processing systems (neurips) 33..a appendices.
a.1 derivational segmentation.
let a be a set of derivational afﬁxes and s a set ofstems.
to determine the derivational segmentationof a word w, we employ an iterative algorithm.
de-ﬁne the set ba1 of w as the words that remain whenone derivational afﬁx from a is removed from w.for example, unlockable can be segmentedinto un, lockable and unlock, able soba1 (unlockable) = {lockable, unlock}(we assume that un and able are in a).
we then it-i+1(w) = (cid:83)eratively create ba1 (b), i.e.,we iteratively remove afﬁxes from w. we stop assoon as bai+1(w) ∩ s (cid:54)= ∅.
the element in thisintersection, together with the used afﬁxes froma, forms the derivational segmentation of w.15 ifthere is no i such that bai+1(w)∩s (cid:54)= ∅, w does nothave a derivational segmentation.
the algorithmis sensitive to most morpho-orthographic rules ofenglish (plag, 2003), e.g., when the sufﬁx ize isremoved from isotopize, the resulting word isisotope, not isotop..i (w) ba.
b∈ba.
in this paper, we follow hofmann et al.
(2020a)in using bert’s preﬁxes, sufﬁxes, and stems asinput to the algorithm.
speciﬁcally, we assign 46productive preﬁxes and 44 productive sufﬁxes inbert’s vocabulary to a and all fully alphabeticwords with more than 3 characters in bert’s vo-cabulary (excluding stopwords and afﬁxes) to s,resulting in a total of 20,259 stems.
this meansthat we only consider derivational segmentationsthat are possible given bert’s vocabulary..15if |ba.
i+1(w) ∩ s| > 1 (rarely the case in practice), the.
element with the lowest number of sufﬁxes is chosen..16since expected validation performance (dodge et al.,2019) may not be correct for grid search, we report meanand standard deviation of the performance instead..3607amazon.
arxiv.
reddit.
model.
µ.σ.n.l.τ.µ.σ.n.l.τ.µ.σ.n.l.τ.delbert .627.612bert.
stemafﬁxes.
.556.519.
.007.006.
.016.008.
6.757.30.
9.855.55.
3e-063e-06.
3e-063e-06.
67.7366.18.
67.4367.70.
.725.693.
.699.599.
.006.015.
.005.004.
11.4517.05.
8.157.50.
3e-063e-06.
3e-063e-06.
28.6928.04.
28.5628.43.
.687.657.
.670.593.
.006.007.
.006.003.
5.459.25.
6.009.35.
3e-063e-06.
3e-063e-06.
25.5625.06.
25.3925.49.table 4: validation performance statistics and hyperparameter search details.
the table shows the mean (µ) andstandard deviation (σ) of the validation performance (f1) on all hyperparameter search trials, the number of epochs(n) and learning rate (l) with the best validation performance, and the runtime (τ ) in minutes for one full hyperpa-rameter search (20 trials).
the numbers are averaged across 20 training runs with different random seeds..3608