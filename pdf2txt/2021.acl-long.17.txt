modularized interaction network for named entity recognition.
fei li1, zheng wang2∗, siu cheung hui2 , lejian liao1 , dandan song1∗,jing xu1 , guoxiu he3 , meihuizi jia11beijing institute of technology, china2nanyang technological university, singapore3wuhan university, china{lifei926,liaolj,sdd,xujing,jmhuizi24}@bit.edu.cn{wang zheng,asschui}@ntu.edu.sg, guoxiu.he@whu.edu.cn.
abstract.
although the existing named entity recogni-tion (ner) models have achieved promisingperformance, they suffer from certain draw-backs.
the sequence labeling-based nermodels do not perform well in recognizinglong entities as they focus only on word-levelinformation, while the segment-based nermodels which focus on processing segment in-stead of single word are unable to capture theword-level dependencies within the segment.
moreover, as boundary detection and type pre-diction may cooperate with each other for thener task, it is also important for the two sub-tasks to mutually reinforce each other by shar-ing their information.
in this paper, we pro-pose a novel modularized interaction network(min) model which utilizes both segment-level information and word-level dependen-cies, and incorporates an interaction mecha-nism to support information sharing betweenboundary detection and type prediction to en-hance the performance for the ner task.
wehave conducted extensive experiments basedon three ner benchmark datasets.
the per-formance results have shown that the proposedmin model has outperformed the current state-of-the-art models..1.introduction.
named entity recognition (ner) is one of thefundamental tasks in natural language processing(nlp) that intends to ﬁnd and classify the type of anamed entity in text such as person (per), location(loc) or organization (org).
it has been widelyused for many downstream applications such asrelation extraction (xiong et al., 2018), entity link-ing (gupta et al., 2017), question generation (zhouet al., 2017) and coreference resolution (barhomet al., 2019)..∗corresponding authors..currently, there are two types of methods for thener task.
the ﬁrst one is sequence labeling-basedmethods (lample et al., 2016; chiu and nichols,2016; luo et al., 2020), in which each word in a sen-tence is assigned a special label (e.g., b-per or i-per).
such methods can capture the dependenciesbetween adjacent word-level labels and maximizethe probability of predicted labels over the wholesentence.
it has achieved the state-of-the-art perfor-mance in various datasets over the years.
however,ner is a segment-level recognition task.
as such,the sequence labeling-based models which focusonly on word-level information do not perform wellespecially in recognizing long entities (ye and ling,2018).
recently, segment-based methods (konget al., 2016; li et al., 2020b; yu et al., 2020b; liet al., 2021) have gained popularity for the nertask.
they process segment (i.e., a span of words)instead of single word as the basic unit and assigna special label (e.g., per, org or loc) to eachsegment.
as these methods adopt segment-levelprocessing, they are capable of recognizing longentities.
however, the word-level dependencieswithin a segment are usually ignored..ner aims at detecting the entity boundaries andthe type of a named entity in text.
as such, thener task generally contains two separate and inde-pendent sub-tasks on boundary detection and typeprediction.
however, from our experiments, weobserve that the boundary detection and type pre-diction sub-tasks are actually correlated.
in otherwords, the two sub-tasks can interact and mutuallyreinforce each other by sharing their information.
consider the following example sentence: “emmyrossum was from new york university”.
if weknow “university” is an entity boundary, it will bemore accurate to predict the corresponding entitytype to be “org”.
similarly, if we know an entityhas an “org” type, it will be more accurate topredict that “university” is the end boundary of.
proceedingsofthe59thannualmeetingoftheassociationforcomputationallinguisticsandthe11thinternationaljointconferenceonnaturallanguageprocessing,pages200–209august1–6,2021.©2021associationforcomputationallinguistics200the entity “new york university” instead of “york”(which is the end boundary for the entity “newyork”).
however, sequence labeling-based modelsconsider the boundary and type as labels, and thussuch information cannot be shared between the sub-tasks to improve the accuracy.
on the other hand,segment-based models ﬁrst detect the segments andthen classify them into the corresponding types.
these methods generally cannot use entity type in-formation in the process of segment detection andmay have errors when passing such informationfrom segment detection to segment classiﬁcation.
in this paper, we propose a modularized inter-action network (min) model which consists ofthe ner module, boundary module, type mod-ule and interaction mechanism for the ner task.
to tackle the issue on recognizing long entities insequence labeling-based models and the issue ofutilizing word-level dependencies within a segmentin segment-based models, we incorporate a pointernetwork (vinyals et al., 2015) into the boundarymodule as the decoder to capture segment-levelinformation on each word.
then, these segment-level information and the corresponding word-levelinformation on each word are concatenated as theinput to the sequence labeling-based models..to enable interaction information, we proposeto separate the ner task into the boundary detec-tion and type prediction sub-tasks to enhance theperformance of the two sub-tasks by sharing theinformation from each sub-task.
speciﬁcally, weuse two different encoders to extract their distinctcontextual representations from the two sub-tasksand propose an interaction mechanism to mutuallyreinforce each other.
finally, these information arefused into the ner module to enhance the perfor-mance.
in addition, the ner module, boundarymodule and type module share the same word rep-resentations and we apply multitask training whentraining the proposed min model..in summary, the main contributions of this paper.
include:.
• we propose a novel modularized interactionnetwork (min) model which utilizes boththe segment-level information from segment-based models and word-level dependenciesfrom sequence labeling-based models in orderto enhance the performance of the ner task..• the proposed min model consists of the nermodule, boundary module, type module and.
interaction mechanism.
we propose to sepa-rate boundary detection and type predictioninto two sub-tasks and the interaction mech-anism is incorporated to enable informationsharing between the two sub-tasks to achievethe state-of-the-art performance..• we.
conduct.
extensive.
experiments onthree ner benchmark datasets, namelyconll2003, wnut2017 and jnlpba, toevaluate the performance of the proposedmin model.
the experimental results haveshown that our min model has achieved thestate-of-the-art performance and outperformsthe existing neural-based ner models..2 related work.
in this section, we review the related work on thecurrent approaches for named entity recognition(ner).
these approaches can be categorized intosequence labeling-based ner and segment-basedner..2.1 sequence labeling-based ner.
sequence labeling-based ner is regarded as a se-quence labeling task, where each word in a sen-tence is assigned a special label (e.g., b-per, i-per).
huang et al.
(huang et al., 2015) utilizedthe bilstm as an encoder to learn the contextualrepresentation of words, and then conditional ran-dom fields (crfs) was used as a decoder to labelthe words.
it has achieved the state-of-the-art re-sults on various datasets for the past many years.
inspired by the success of the bilstm-crf ar-chitecture, many other state-of-the-art models haveadopted such architecture.
chiu and nichols (chiuand nichols, 2016) used convolutional neural net-work (cnn) to capture spelling features, and thecharacter-level and word-level embeddings are con-catenated as the input of bilstm with crf net-work.
further, lample et al.
(lample et al., 2016)proposed rnn-bilstm-crf as an alternative.
more recently, pretrained language models suchas elmo (peters et al., 2018) and bert (devlinet al., 2019) have been adopted to further enhancethe performance of ner..2.2 segment-based ner.
segment-based ner identiﬁes segments in a sen-tence and classiﬁes each segment with a speciallabel (e.g., per, org or loc).
kong et al.
(konget al., 2016) used bilstm to map arbitrary-length.
201segment into a ﬁxed-length vector, and then thesevectors were passed to semi-markov conditionalrandom fields (semi-crfs) for labeling the seg-ments.
zhuo et al.
(zhuo et al., 2016) adopteda gated recursive convolutional neural networkinstead of bilstm to build a pyramid-like struc-ture for extracting segment-level features in a hi-erarchical way.
in recent years, ye et al.
(yeand ling, 2018) exploited the weighted sum ofword-level within segment to learn segment-levelfeatures with semi-crfs which was then trainedjointly on word-level with the bilstm-crf net-work.
li et al.
(li et al., 2020a) used a recurrentneural network encoder-decoder framework witha pointer network to detect entity segments.
liet al.
(li et al., 2020b) treated ner as a machinereading comprehension (mrc) task, where entitieswere extracted as retrieved answer spans.
yu etal.
(yu et al., 2020b) ranked all the spans in termsof the pairs of start and end tokens in a sentenceusing a biafﬁne model..3 proposed model.
this section presents our proposed modularizedinteraction network (min) for ner.
the overallmodel architecture is shown in figure 1(a), whichconsists of the ner module, boundary module,type module and interaction mechanism..3.1 ner module.
in the ner module, we adopt the rnn-bilstm-crf model (lample et al., 2016) as our backbone,which consists of three components: word repre-sentation, bilstm encoder and crf decoder.
word representation given an input sentences =< w1, w2, · · · , wn >, each word wi(1 ≤ i ≤n) is represented by concatenating a word-level em-bedding xwi and a character-level word embeddingxci as follows:.
xi = [xw.
i ; xci ].
(1).
where xwis the pre-trained word embedding, andithe character-level word embedding xci is obtainedwith a bilstm to capture the orthographic andmorphological information.
it considers each char-acter in the word as a vector, and then inputs themto a bilstm to learn the hidden states.
the ﬁnalhidden states from the forward and backward out-puts are concatenated as the character-level wordinformation..bilstm encoder the distributed word embed-dings x =< x1, x2, · · · , xn > are then fed intothe bilstm encoder to extract the hidden se-quences h =< h1, h2, · · · , hn > of all wordsas follows:.
(cid:105).
(cid:104)−→hi ;.
←−hi.
hi =−→hi = lst m←−hi = lst m.(cid:16).
(cid:16).
xi,.
xi,.
(cid:17).
(cid:17).
−−→hi−1←−−hi−1.
(2).
in the ner module, we fuse the distinct contextualboundary representation and type representationfor the ner task.
in addition, we also fuse thesegment information from the boundary moduleto support the recognition of long entities.
notethat the boundary information and type informationcan mutually reinforce each other.
thus, we usean interaction mechanism to reinforce them beforefusing these information in the ner module.
in-stead of directly concatenating these informationwith hidden representations in the ner module,we follow the previous studies (zhang et al., 2018;yu et al., 2020a) to use a gate function to dynam-ically control the amount of information ﬂowingby infusing the expedient part while excluding theirrelevant part.
the gate function uses the informa-tion from the ner module to guide the process,which is described formally as follows:.
bdy.
t ype.
h., h.h b = σ.w (cid:62).
= interact(h bdy, h t ype)bdy(cid:17)bdy1 h + w (cid:62).
b h.⊗ h.h t = σ.w (cid:62).
2 h + w (cid:62).
h s = σ.w (cid:62).
3 h + w (cid:62).
⊗ h seg.
t ype.
⊗ h.(3).
t ype(cid:17).
t hs h seg(cid:17).
(cid:16).
(cid:16).
(cid:16).
bdy.
where h bdy and h t ype represent the distinct rep-resentations of hidden sequences from the bound-ary module and type module respectively, andh seg represents the segment information from theboundary module.
we will discuss them in sec-tion 3.2 and section 3.3. hrep-resent the distinct representations of hidden se-quences from the boundary module and type mod-ule respectively after the interaction using an in-teraction mechanism interact(·, ·), and we willdiscuss them in section 3.4. h b, h t and h srepresent the boundary, type and segment informa-tion respectively to be injected into the ner mod-ule from the gate function.
σ denotes the logistic.
and h.t ype.
202(a) overall architecture.
(b) boundary module.
figure 1: the architecture of our proposed modularized interaction network..sigmoid function and ⊗ denotes the element-wisemultiplication..the ﬁnal hidden representations in the ner.
module are as follows:.
h n er = w (cid:62)[h; h b; h t ; h s] + b.
(4).
crf decoder crf has been widely used in thestate-of-the-art ner models (chiu and nichols,2016; lample et al., 2016) to model tagging de-cisions when considering strong connections be-tween output tags.
for an input sentence s =<w1, w2, · · · , wn >, the score of a predicted se-quence of labels y =< y1, y2, · · · , yn > is deﬁnedas follows:.
sc (s, y) =.
tyi,yi+1 +.
pi,yi.
(5).
n(cid:88).
i=0.
n(cid:88).
i=1.
where tyi,yi+1 represents the score of a transitionfrom yi to yi+1, and pi,yi is the score of the yi tagof the ith word in a sentence..the crf model describes the probability of pre-dicted labels y over all possible tag sequences inthe set y , that is:.
p (y|s) =.
esc(s,y)(cid:101)y∈y esc(s,(cid:101)y).
(cid:80).
we maximize the log-probability of the correct se-quence of labels during the training.
during decod-ing, we predict the label sequence with the maxi-mum score:.
y∗ = arg max.
sc (s, (cid:101)y).
(cid:101)y∈y.
3.2 boundary module.
the boundary module needs to provide not onlydistinct contextual boundary information but also.
(6).
(7).
segment information for the ner module.
here,we use another bilstm as encoder to extract dis-tinct contextual boundary information.
and in-spired by bdrybot (li et al., 2020a), a recurrentneural network encoder-decoder framework with apointer network is used to detect entity segmentsfor segment information.
the bdrybot modelprocesses the starting boundary word in an entity topoint to the corresponding ending boundary word.
the other entity words in the entity are skipped.
the non-entity words are pointed to a speciﬁc posi-tion.
this method has achieved promising resultsin the boundary detection task.
however, due to thevariable length of entities, this model is deprived ofthe power of batch training.
in addition, as the seg-ment information of each word in an entity is thesame as the starting boundary word, the segmentinformation for all the words within a segment willbe incorrect if the starting boundary word is de-tected wrongly.
to avoid this problem, we improvethe training process and propose a novel method tocapture the segment information of each word..1., hbdy2., · · · , hbdy.
we train the starting boundary word to pointto the corresponding ending boundary word, andthe other words in the sentence to a sentinel wordinactive.
the process is shown in figure 1(b).
speciﬁcally, we use another bilstm as encoderto obtain the distinct boundary hidden sequencesh bdy =< hbdyn >, and a sen-tinel vector is padded into the last positions ofhidden sequences h bdy for the sentinel word in-active.
then, a unidirectional lstm is used as adecoder to generate the decoded state dj at eachtime step j. to add extra information to the inputof the lstm, we follow (fern´andez-gonz´alez andg´omez-rodr´ıguez, 2020) and use the sum of thehidden states of current (hbdy), previous (hbdyi−1 )and next (hbdyi+1 ) words instead of word embedding.
i.
203word representationbilstmhidden sequencesbilstmhidden sequencescrfsegmentinformationner moduleboundary moduledecodebilstmhidden sequencescrftype module...interactionmechanismword representation emmyrossumwasfromnewuniversityh1h2h3h5h4h6h1h2h1h2h3h2h3h4york[inactive]h3h4h5h4h5h6h5h6h7h8h6h7h7as the input to the decoder as follows:.
3.4.interaction mechanism.
j−1 + hbdysj = hbdydj = lst m (sj, dj−1).
j + hbdy.
j+1.
(8).
note that the ﬁrst word and last word do not havehidden states of previous and next, we use zerovectors to represent it which are shown as greyblocks in figure 1(b)..after that, we use the biafﬁne attention mech-anism (dozat and manning, 2017) to generate afeature representation for each possible boundaryposition i at time step j, and the sof tmax func-tion is used to obtain the probability of word wifor determining an entity segment that starts withword wj and ends with word wi..uji = dj.
t w hbdy.
i + u t dj + v t hbdy(cid:17)uji.
(cid:16).
, i ∈ [j, n + 1].
i + b.
(9).
p (wi|wj) = sof tmax.
where w is the weight matrix of bi-linear term,u and v are the weight matrices of linear terms,b is the bias vector and i ∈ [j, n + 1] indicates apossible position in decoding..different from the existing methods (zhuo et al.,2016; sohrab and miwa, 2018) that enumerate allsegments starting with word wj with equal impor-tance, we use the probability p (wi|wj) as the con-ﬁdence of the segment that starts with word wj andends with word wi, and then all these segmentsunder the probability p (wi|wj) are summed as thesegment information of word wj..n(cid:88).
h seg.
j =.
p (wi|wj) hpj,i.
hpj,i = [hbdy.
j.; hbdyi.i=j; hbdy.
i − hbdy.
j.; hbdy.
i (cid:12) hbdy](10).
j.where hpj,i is the representation of the segment thatstarts with word wj and ends with word wi, and (cid:12)is element-wise product..3.3 type module.
for the type module, we use the same networkstructure as in the ner module.
given the sharedinput x =< x1, x2, · · · , xn >, bilstm is usedto extract distinct contextual type informationh t ype =< ht ype>, and then1crf is used to tag type labels.., · · · , ht ype.
, ht ype2.n.as discussed in section 1, the boundary informa-tion and type information can mutually reinforceeach other.
we ﬁrst follow (cui and zhang, 2019;qin et al., 2021) and use a self-attention mechanismover each sub-task labels to obtain the explicit la-bel representations.
then, we concatenate theserepresentations and contextual information of cor-responding sub-tasks to get label-enhanced contex-tual information.
for the ith label-enhanced bound-ary contextual representation hb−e, we ﬁrst usethe biafﬁne attention mechanism (dozat and man-ning, 2017) to grasp the attention scores betweenhb−eand the label-enhanced type contextual in-iformation < ht −e, ht −e>.
the at-2tention scores < αb−ei,n > arecomputed in the same way as in equation (9).
then,we concatenate the ith label-enhanced boundaryrepresentation hb−eand the interaction represen-tation rb−eby considering the type informationas its updated boundary representation:.
, · · · , αb−e.
, · · · , ht −e.
, αb−ei,2.
i,1.
n.1.i.i.i.rb−ei.
=.
i,j ht −eαb−ej.n(cid:88).
j=1.
h.bdyi = [hb−e.
i., rb−ei.
].
(11).
similarity, we can obtain the updated type repre-sentation hby considering the boundary infor-mation..t ypei.
3.5.joint training.
there are three modules in our proposed minmodel: ner module, boundary module and typemodule.
they share the same word representations.
thus, the whole model can be trained with mul-titask training.
during training, we minimize thenegative log-probability of the correct sequence oflabels in equation (6) for the ner module andtype module, while the cross-entropy loss is usedfor the boundary module:.
ln er = −log (cid:0)p (cid:0)ˆyn er|x(cid:1)(cid:1)lt ype = −log (cid:0)p (cid:0)ˆyt ype|x(cid:1)(cid:1)n(cid:88).
lbdy = −.
ˆybdyi.logpbdyi.
1n.i=1.
(12).
where x represents input sequence, and ˆyn er andˆyt ype represent the correct sequence of labels forthe ner module and type module respectively.
pbdyis the probability distribution of the gold la-ibel and ˆybdyis the gold one-hot vector for the.
i.
204boundary module.
then, the ﬁnal multitask loss isa weighted sum of the three losses:.
l = ln er + lt ype + lbdy.
(13).
4 experiments.
in this section, we ﬁrst introduce the datasets, base-line models and implementation details.
then, wepresent the experimental results on three bench-mark datasets.
moreover, an ablation study is alsoconducted.
finally, we give some insights on fur-ther analysis..4.1 datasets.
we evaluate the proposed model on three bench-mark ner datasets: conll2003 (sang andde meulder, 2003), wnut2017 (derczynskiet al., 2017) and jnlpba (kim et al., 2004)..• conll2003 - it is collected from reutersnews articles.
four different types of namedentities including per, loc, org and miscare deﬁned by the conll 2003 ner sharedtask..• wnut2017 - it.
is a set of noisy user-generated text including youtube comments,stackexchange posts, twitter text, and red-dit comments.
six types of entities includingper, loc, group, creative work, corpora-tion and product are annotated..• jnlpba - it is collected from medline ab-stracts.
five types of entities including dna,rna, protein, cell line and cell type are anno-tated..table 1 presents the statistics of these datasets..4.2 baseline models.
we compare the proposed min model with severalbaseline models including sequence labeling-basedmodels and segment-based models..the compared sequence labeling-based models.
include:.
• cnn-bilstm-crf (chiu and nichols,2016) - this model utilizes cnn to capturecharacter-level word features, and then thecharacter-level and word-level embeddingsare concatenated as the input to the bilstm-crf network.
it is a classical baseline forner..dataset.
conll2003.
wnut2017.
jnlpba.
#sentences#entities#sentences#entities#sentences#entities.
train14,98723,4993,3943,16016,69146,388.dev3,4665,9421,0091,2501,8534,902.test3,6845,6481,2871,5893,8558,657.table 1: statistics of conll2003, wnut2017, andjnlpba datasets..• rnn-bilstm-crf (lample et al., 2016) -this model uses rnn instead of cnn incnn-bilstm-crf..• elmo (peters et al., 2018) - this model usesa deep bidirectional language model to learncontextualized word representation on a largetext corpus, which is then fed into bilstm-crf for ner..• flair (akbik et al., 2018) - this model usesbilstm-crf with character-level contextu-alized representations for ner..• bert (devlin et al., 2019) - this model learnscontextualized word representation based ona bidirectional transformer, which is then fedinto bilstm-crf for ner..• hcra (luo et al., 2020) - this model usessentence-level and document-level represen-tations to augment the contextualized repre-sentation based on a funnel-shaped cnn withbilstm-crf for ner..the compared segment-based models include:.
• bilstm-pointer1 (li et al., 2020a) - thismodel uses bilstm as the encoder and an-other unidirectional lstm with pointer net-works as the decoder for entity boundary de-tection.
then, the entity segments generatedby the decoder are classiﬁed with the softmaxclassiﬁer for ner..• hscrf (ye and ling, 2018) - this model ex-ploits the weighted sum of word-level withinsegment to learn segment-level features withsemi-crfs which is then trained jointly onword-level with the bilstm-crf network..1in (li et al., 2020a), the pointer networks is used fordetecting entity boundaries only.
we reproduce this work andadd a softmax layer for the ner task..205• mrc+bert (li et al., 2020b) - this modelformulates the ner task as a machine readingcomprehension task..• biafﬁne+bert (yu et al., 2020b) - thismodel ranks all the spans in terms of the pairsof start and end tokens in a sentence using abiafﬁne model..4.3.implementation details.
our proposed min model is implemented withthe pytorch framework.
we use 100-dimensionalpre-trained glove word embeddings 2 (penningtonet al., 2014).
the char embeddings is initializedrandomly as 25-dimensional vectors.
when train-ing the model, both of the embeddings are updatedalong with other parameters.
we use adam opti-mizer (kingma and ba, 2014) for training with amini-batch.
the initial learning rate is set to 0.01and will shrunk by 5% after each epoch, dropoutrate to 0.5, the hidden layer size to 100, and thegradient clipping to 5. we report the results basedon the best performance on the development set.
all of our experiments are conducted on the samemachine with 8-cores of intel(r) xeon(r) e5-1630cpu@3.70ghz and two nvidia geforce-gtxgpu.
following the work in (ye and ling, 2018),the maximum segment length for segment informa-tion discussed in section 3.2 is set to 6 for bettercomputational efﬁciency..4.4 experimental results.
table 2 shows the experimental results of our pro-inposed min model and the baseline models.
table 2, when compared with models without us-ing any language models or external knowledge,we observe that our min model outperforms all thecompared baseline models in terms of precision,recall and f1 scores, and achieves 0.57%, 4.77%and 3.26% improvements on f1 scores for theconll2003, wnut2017 and jnlpba datasetsrespectively..among the compared models, the f1 scores ofthe bilstm-pointer model are generally lowerthan other models.
this is because it does notutilize the word-level dependencies within a seg-ment and also suffers from the problem on bound-ary error propagation during boundary detectionand type prediction.
the cnn-bilstm-crf and.
2http://nlp.stanford.edu/projects/.
glove/.
rnn-bilstm-crf models have achieved similarperformance results on the three datasets, whichperform worse than that of hcra and hscrf.
thehcra model uses sentence-level and document-level representations to augment the contextualizedword representation, while the hscrf model con-siders the segment-level and word-level informa-tion with multitask training.
however, the hcramodel does not consider the segment-level informa-tion, and the hscrf model does not model directlythe word-level dependencies within a segment.
inaddition, all the above models do not share infor-mation between the boundary detection and typeprediction sub-tasks.
our min model has achievedthe best performance as it is capable of consideringall these information..when pre-trained language models such aselmo and bert are incorporated, all the mod-els have achieved better performance results.
inparticular, we observe that our min model hasachieved 0.95%, 3.83% and 2.73% improvementson the f1 scores for the conll2003, wnut2017and jnlpba datasets respectively when comparedwith the other models.
the results are consistentwith what have been discussed in models withoutusing any pre-trained language models..4.5 ablation study.
to show the importance of each component in ourproposed min model, we conduct an ablation ex-periment on the boundary module, type moduleand interaction mechanism.
as shown in table 3,we can see that all these components contribute sig-niﬁcantly to the effectiveness of our min model..the discussion on the effectiveness of each com-ponent is given with respect to the three datasets.
the boundary module improves the f1 scoresby 1.13%, 3.58% and 2.1% for conll2003,wnut2017 and jnlpba respectively.
this isbecause it not only provides segment-level infor-mation for the ner module but also provides theboundary information for the type module.
assuch, it helps recognize long entities and predictthe entity types more accurately..the type module improves the f1 scoresby 1.02%, 2.81% and 1.42% for conll2003,wnut2017 and jnlpba respectively.
this isbecause it provides the type information for theboundary module which can help detect entityboundaries more accurately.
in addition, it can alsohelp obtain more effective segment information..206model.
p(%)91.3591.1292.2090.34-92.91.conll2003r(%)91.06cnn-bilstm-crf90.76rnn-bilstm-crf91.71hcra90.31bilstm-pointerhscrf-min (ours)92.15+ language models/external knowledgeelmoflairberthcra+bertbilstm-pointer+bertmrc+bertbiafﬁne+bertmin+bert (ours).
-92.37--92.0292.3393.7094.75.
-93.12--92.4594.6193.3094.15.f1(%)91.2190.9491.9690.3291.5392.53.
92.2292.7492.8093.3792.2393.0493.5094.45.wnut2017r(%)32.9035.50-30.43-38.48.f1(%)41.8641.81-38.98-46.63.p(%)57.5450.86-54.23-59.17.
----56.82--60.54.
----36.87--42.48.
45.3345.9646.10-44.72--49.93.jnlpbar(%)70.5271.56-74.9075.3376.24.
77.6877.6880.36-77.32--81.19.f1(%)72.2072.31-71.1372.3975.57.
74.2974.2975.24-72.68--77.97.p(%)73.9673.08-67.7269.6774.91.
71.1871.1870.73-68.56--75.00.table 2: experimental results on three benchmark datasets..model.
p(%)92.91min91.12ner module only91.62w/o boundary module91.79w/o type modulew/o interaction mechanism 92.15.conll2003r(%)92.1590.7691.1891.2391.83.f1(%)92.5390.9491.4091.5191.99.wnut2017r(%)38.4835.5036.0836.6537.09.f1(%)46.6341.8143.0543.8244.77.p(%)59.1750.8653.3554.4756.45.jnlpbar(%)76.2471.5673.5574.2675.02.f1(%)75.5772.3173.4774.1574.85.p(%)74.9173.0873.3974.0474.68.table 3: experimental results of the ablation study of the min model..the interaction mechanism has achieved 0.54%,1.86% and 0.72% improvements on f1 scores forconll2003, wnut2017 and jnlpba respec-tively.
as it bridges the gap between the boundarymodule and type module for information interac-tion and sharing, it can help improve the perfor-mance of boundary detection and type predictionsimultaneously..overall, the different components of the pro-posed model can work effectively with each otherwith multitask training and enable the modelachieve the state-of-the-art performance for thener task..4.6 performance against entity length.
as our proposed min model is capable of recog-nizing long entities, we compare the performanceof our min model with rnn-bilstm-crf andhscrf.
note that the rnn-bilstm-crf modelis the base model used in our min model.
andthe hscrf model also considers the segment-leveland word-level information with multitask training.
the results are shown in figure 2. the experi-ment is conducted on the conll2003 test dataset.
we follow the setting in (ye and ling, 2018) andgroup the data according to the number of entitiesfrom 1 to ≥ 6 in a sentence.
we observe that ourmin model and the hscrf model consistently.
figure 2: performance against entity length..outperform rnn-bilstm-crf in each group.
inparticular, the improvement is obvious when theentity length is longer than 4 because both our minmodel and the hscrf model consider the segment-level information.
however, our min model per-forms better than the hscrf model in each group.
more speciﬁcally, when the entity length is longerthan 4, our min model has great improvementover hscrf.
this is because the hscrf model di-rectly uses segment-level features with semi-crfsto tag the segments, which ignore word-level de-pendencies within the segment.
in contrast, ourmin model combines segment-level informationwith word-level dependencies within a segment forthe ner task..207123456entity length707580859095f1 scores(%)minhscrfrnn-bilstm-crf5 conclusion.
in this paper, we have proposed a novel modu-larized interaction network (min) model for thener task.
the proposed min model utilizes bothsegment-level information and word-level depen-dencies, and incorporates an interaction mechanismto support information sharing between boundarydetection and type prediction to enhance the per-formance for the ner task.
we have conductedextensive experiments on three ner benchmarkdatasets.
the experimental results have shown thatour proposed min model has achieved the state-of-the-art performance..acknowledgments.
this research has been supported by the nationalkey r&d program of china under grant no.
2020aaa0106600, the national natural sciencefoundation of china under grants no.
62062012and 61976021, and the ministry of education(moe) of singapore under the academic researchfund (acrf) tier 1 grant rg135/18..references.
alan akbik, duncan blythe, and roland vollgraf.
2018. contextual string embeddings for sequencein proceedings of the 27th internationallabeling.
conference on computational linguistics, pages1638–1649..shany barhom, vered shwartz, alon eirew, michaelbugert, nils reimers, and ido dagan.
2019. re-visiting joint modeling of cross-document entity andevent coreference resolution.
in proceedings of the57th conference of the association for computa-tional linguistics, acl 2019, florence, italy, july28- august 2, 2019, volume 1: long papers, pages4179–4189..jason pc chiu and eric nichols.
2016. named entityrecognition with bidirectional lstm-cnns.
transac-tions of the association for computational linguis-tics, 4:357–370..leyang cui and yue zhang.
2019. hierarchically-reﬁned label attention network for sequence labeling.
in proceedings of the 2019 conference on empiricalmethods in natural language processing and the9th international joint conference on natural lan-guage processing (emnlp-ijcnlp), pages 4106–4119..leon derczynski, eric nichols, marieke van erp, andnut limsopatham.
2017. results of the wnut2017shared task on novel and emerging entity recogni-tion.
in proceedings of the 3rd workshop on noisyuser-generated text, pages 140–147..jacob devlin, ming-wei chang, kenton lee, andkristina toutanova.
2019. bert: pre-training of deepbidirectional transformers for language understand-ing.
in naacl-hlt (1)..timothy dozat and christopher d. manning.
2017.deep biafﬁne attention for neural dependency pars-in 5th international conference on learninging.
representations, iclr 2017, toulon, france, april24-26, 2017, conference track proceedings..daniel fern´andez-gonz´alez.
and carlos g´omez-rodr´ıguez.
2020. discontinuous constituent parsingthewith pointer networks.
aaai conference on artiﬁcial intelligence, pages7724–7731..in proceedings of.
nitish gupta, sameer singh, and dan roth.
2017. en-tity linking via joint encoding of types, descriptions,and context.
in proceedings of the 2017 conferenceon empirical methods in natural language process-ing, pages 2681–2690..zhiheng huang, wei xu, and kai yu.
2015. bidirec-tional lstm-crf models for sequence tagging.
arxivpreprint arxiv:1508.01991..jin-dong kim, tomoko ohta, yoshimasa tsuruoka,yuka tateisi, and nigel collier.
2004. introductionto the bio-entity recognition task at jnlpba.
in pro-ceedings of the international joint workshop on nat-ural language processing in biomedicine and its ap-plications, pages 70–75.
citeseer..diederik p kingma and jimmy ba.
2014. adam: amethod for stochastic optimization.
arxiv preprintarxiv:1412.6980..lingpeng kong, chris dyer, and noah a smith.
2016.segmental recurrent neural networks.
in 4th inter-national conference on learning representations,iclr 2016, san juan, puerto rico, may 2-4, 2016,conference track proceedings..guillaume lample, miguel ballesteros, sandeep sub-ramanian, kazuya kawakami, and chris dyer.
2016.neural architectures for named entity recognition.
in proceedings of the 2016 conference of the northamerican chapter of the association for computa-tional linguistics: human language technologies,pages 260–270..fei li, zheng wang, siu cheung hui, lejian liao,dandan song, and jing xu.
2021.effectivenamed entity recognition with boundary-aware bidi-in proceedings of therectional neural networks.
web conference 2021..jing li, aixin sun, and yukun ma.
2020a.
neuralieee transac-.
named entity boundary detection.
tions on knowledge and data engineering..xiaoya li, jingrong feng, yuxian meng, qinghonghan, fei wu, and jiwei li.
2020b.
a uniﬁed mrc.
208juntao yu, bernd bohnet, and massimo poesio.
2020b.
named entity recognition as dependency parsing.
inproceedings of the 58th annual meeting of the asso-ciation for computational linguistics, pages 6470–6476..qi zhang, jinlan fu, xiaoyu liu, and xuanjing huang.
2018. adaptive co-attention network for namedin proceedings ofentity recognition in tweets.
the aaai conference on artiﬁcial intelligence, vol-ume 32..qingyu zhou, nan yang, furu wei, chuanqi tan,hangbo bao, and ming zhou.
2017. neural ques-tion generation from text: a preliminary study.
innational ccf conference on natural languageprocessing and chinese computing, pages 662–671.
springer..jingwei zhuo, yong cao, jun zhu, bo zhang, and za-iqing nie.
2016. segment-level sequence modelingusing gated recursive semi-markov conditional ran-dom ﬁelds.
in proceedings of the 54th annual meet-ing of the association for computational linguistics(volume 1: long papers), pages 1413–1423..in pro-framework for named entity recognition.
ceedings of the 58th annual meeting of the asso-ciation for computational linguistics, pages 5849–5859..ying luo, fengshun xiao, and hai zhao.
2020. hi-erarchical contextualized representation for namedentity recognition.
in aaai, pages 8441–8448..jeffrey pennington, richard socher, and christopher dmanning.
2014. glove: global vectors for word rep-resentation.
in proceedings of the 2014 conferenceon empirical methods in natural language process-ing (emnlp), pages 1532–1543..matthew e peters, mark neumann, mohit iyyer, mattgardner, christopher clark, kenton lee, and lukezettlemoyer.
2018. deep contextualized word rep-resentations.
in proceedings of naacl-hlt, pages2227–2237..libo qin, tailu liu, wanxiang che, bingbing kang,sendong zhao, and ting liu.
2021. a co-interactivetransformer for joint slot ﬁlling and intent detection.
in icassp 2021-2021 ieee international confer-ence on acoustics, speech and signal processing(icassp), pages 8193–8197..erik f sang and fien de meulder.
2003. introductionto the conll-2003 shared task: language-independentin proceedings of thenamed entity recognition.
seventh conference on natural language learning athlt-naacl 2003-volume 4, pages 142–147..mohammad golam sohrab and makoto miwa.
2018.deep exhaustive model for nested named entityrecognition.
in proceedings of the 2018 conferenceon empirical methods in natural language process-ing, pages 2843–2849..oriol vinyals, meire fortunato, and navdeep jaitly.
in proceedings of the2015. pointer networks.
28th international conference on neural informa-tion processing systems-volume 2, pages 2692–2700..chenyan xiong, zhengzhong liu, jamie callan, andtie-yan liu.
2018. towards better text understand-ing and retrieval through kernel entity salience mod-in the 41st international acm sigir con-eling.
ference on research & development in informationretrieval, pages 575–584.
acm..zhixiu ye and zhen-hua ling.
2018. hybrid semi-markov crf for neural sequence labeling.
in proceed-ings of the 56th annual meeting of the associationfor computational linguistics (volume 2: short pa-pers), pages 235–240..jianfei yu, jing jiang, li yang, and rui xia.
2020a.
improving multimodal named entity recognition viaentity span detection with uniﬁed multimodal trans-in proceedings of the 58th annual meet-former.
ing of the association for computational linguistics,pages 3342–3352..209