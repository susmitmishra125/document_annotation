self-attention networks can process bounded hierarchical languages.
shunyu yao‚Ä† binghui peng‚Ä° christos papadimitriou‚Ä° karthik narasimhan‚Ä†.
‚Ä°columbia university‚Ä†princeton university{shunyuy, karthikn}@princeton.edu{bp2601, christos}@columbia.edu.
abstract.
despite their impressive performance in nlp,self-attention networks were recently provedto be limited for processing formal languageswith hierarchical structure, such as dyckk, thelanguage consisting of well-nested parenthe-ses of k types.
this suggested that naturallanguage can be approximated well with mod-els that are too weak for formal languages, orthat the role of hierarchy and recursion in nat-ural language might be limited.
we qualifythis implication by proving that self-attentionnetworks can process dyckk,d, the subset ofdyckk with depth bounded by d, which ar-guably better captures the bounded hierarchi-cal structure of natural language.
speciÔ¨Åcally,we construct a hard-attention network withd + 1 layers and o(log k) memory size (pertoken per layer) that recognizes dyckk,d, anda soft-attention network with two layers ando(log k) memory size that generates dyckk,d.
experiments show that self-attention networkstrained on dyckk,d generalize to longer inputswith near-perfect accuracy, and also verify thetheoretical memory advantage of self-attentionnetworks over recurrent networks.1.
1.introduction.
transformers (vaswani et al., 2017) are now theundisputed champions across several benchmarkleaderboards in nlp.
the major innovation of thisarchitecture, self-attention, processes input tokensin a distributed way, enabling efÔ¨Åcient parallel com-putation as well as long-range dependency mod-elling.
the empirical success of self-attention innlp has led to a growing interest in studying itsproperties, with an eye towards a better understand-ing of the nature and characteristics of natural lan-guage (tran et al., 2018; papadimitriou and juraf-sky, 2020)..1code.
is.
available.
at https://github.com/.
princeton-nlp/dyck-transformer..in particular, it was recently shown that self-attention networks cannot process various kinds offormal languages (hahn, 2020; bhattamishra et al.,2020a), among which particularly notable is dyckk,the language of well-balanced brackets of k types.
by the chomsky-sch√ºtzenberger theorem (chom-sky and sch√ºtzenberger, 1959), any context-freelanguage can be obtained from a dyckk languagethrough intersections with regular languages andhomomorphisms.
in other words, this simple lan-guage contains the essence of all context-free lan-guages, i.e.
hierarchical structure, center embed-ding, and recursion ‚Äì features which have been longclaimed to be at the foundation of human languagesyntax (chomsky, 1956)..consider for example the long-range and nesteddependencies in english subject-verb agreement:.
.
.
(laws (the lawmaker) [writes] [and revises]) [pass]....the sentence structure is captured by dyck2 string(()[][])[].
given the state-of-the-art performance oftransformers in parsing natural language (zhanget al., 2020; he and choi, 2019), the dyckk blindspot seems very suggestive.
if the world‚Äôs bestnlp models cannot deal with this simple language‚Äî generated by a grammar with k + 2 rules andrecognized by a single-state pushdown automaton‚Äî does this not mean that the role of hierarchy andrecursion in natural language must be limited?
thisquestion has of course, been extensively debatedby linguists on the basis of both theoretical and psy-cholinguistic evidence (hauser et al., 2002; franket al., 2012; nelson et al., 2017; brennan and hale,2019; frank and christiansen, 2018)..so, what can self-attention networks tell us aboutnatural language and recursion?
here we pro-vide a new twist to this question by consideringdyckk,d, the subset of dyckk with nesting depthat most d, and show that transformers can process.
proceedingsofthe59thannualmeetingoftheassociationforcomputationallinguisticsandthe11thinternationaljointconferenceonnaturallanguageprocessing,pages3770‚Äì3785august1‚Äì6,2021.¬©2021associationforcomputationallinguistics3770figure 1: illustrations of our self-attention network constructions to recognize and generate dyckk,d.
in construc-tion (a), at each layer, the innermost brackets attend to their matching brackets and ‚Äúcancel‚Äù each other, yielding‚Äúshallower‚Äù spans for successive layers to process.
in construction (b), the Ô¨Årst layer computes the depth of eachtoken by attending to all previous tokens, while the second layer uses depth information to Ô¨Ånd the most recentunclosed open bractket in the history..it.
dyckk,d models bounded (or Ô¨Ånite) recursion,thus captures the hierarchical structure of humanlanguage much more realistically.
for example,center-embedding depth of natural language sen-tences is known to rarely exceed three (karlsson,2007; jin et al., 2018), and while pragmatics, dis-course, and narrative can result in deeper recursionin language (levinson, 2014), there is arguably arelatively small limit to the depth as well..in particular, we prove that self-attention net-works can both recognize and generate dyckk,d,with two conceptually simple yet different construc-tions (figure 1).
the Ô¨Årst network requires d + 1layers and a memory size of o(log k) (per layer pertoken) to recognize dyckk,d, using a distributedmechanism of parenthesis matching.
the secondnetwork has two layers and memory size o(log k).
it works by attending to all previous tokens to countthe depth for each token in the Ô¨Årst layer, and thenuses this depth information to attend to the mostrecent unclosed open bracket in the second layer.
our constructions help reconcile the result in hahn(2020) with the success of transformers in han-dling natural languages..our proof requires certain assumptions about thepositional encodings, an issue that is often consid-ered in empirical papers (ke et al., 2021; shawet al., 2018; wang et al., 2020; shiv and quirk,2019) but not in the more theoretical literature.
first, positional encodings must have log n bitswhen the input length is n, as otherwise differ-ent positions would share the same representation.
more importantly, positional encodings should sup-port easy position comparisons, since token order.
is vital in formal language processing.
our exper-iments show that two standard practices, namelylearnable or Ô¨Åxed sine/cosine positional encodings,cannot generalize well on dyckk,d beyond thetraining input lengths.
in contrast, using a singleÔ¨Åxed scalar monotonic positional encoding suchas pos/n achieves near-perfect accuracy even oninputs signiÔ¨Åcantly longer than the training ones.
our Ô¨Åndings provide a novel perspective on thefunction of positional encodings, and implies thatdifferent applications of self-attention networks (inthis case, natural vs. formal language) may requiredifferent model choices..our theoretical results also bring about interest-ing comparisons to recurrent networks (e.g.
rnns,lstms) in terms of the resource need to processhierarchical structure.
while recurrent networkswith Ô¨Ånite precision need at least œâ(d log k) mem-ory to process dyckk,d (hewitt et al., 2020), oursecond construction requires only o(log k) mem-ory but a o(log n) precision.
in experiments whereprecision is not an issue for practical input lengths(< 104), we conÔ¨Årm that a transformer requiresless memory than a lstm to reach high test accu-racies.
this may help explain why transformersoutperform rnns/lstms in syntactical tasks innlp, and shed light into fundamental differencesbetween recurrent and non-recurrent sequence pro-cessing..2 related work.
our work primarily relates to the ongoing effortof characterizing theoretical abilities (p√©rez et al.,2019; bhattamishra et al., 2020b; yun et al., 2020).
3771input ([]{[]()})layer 1([]{[]()})layer 2([]{[]()})layer 3([]{[]()})ùñ£ùóíùñºùóÑ3,3input[][(([]))layer 1101234321layer 2101234321ùñ£ùóíùñºùóÑ2,4(a)(b)next token prediction: ( [ ]and limitations of self-attention networks, partic-ularly through formal hierarchical structures likedyckk.
hahn (2020) proves that (even with posi-tional encodings) hard-attention transformers can-not model dyckk, and soft-attention transformerswith bounded lipschitz continuity cannot modeldyckk with perfect cross entropy.
bhattamishraet al.
(2020a) prove a soft-attention network withpositional masking (but no positional encodings)can solve dyck1 but not dyck2.
despite the expres-sivity issues theoretically posed by the above work,empirical Ô¨Åndings have shown transformers canlearn dyckk from Ô¨Ånite samples and outperformlstm (ebrahimi et al., 2020).
our work addressesthe theory-practice discrepancy by using positionalencodings and modeling dyckk,d..a parallel line of work with much lengthier tra-dition (elman, 1990; das et al., 1992; steijvers andgr√ºnwald, 1996) investigates the abilities and limi-tations of recurrent networks to process hierarchi-cal structures.
in particular, rnns or lstms areproved capable of solving context-free languageslike dyckk given inÔ¨Ånite precision (korsky andberwick, 2019) or external memory (suzgun et al.,2019; merrill et al., 2020).
however, merrill et al.
(2020) also prove rnns/lstms cannot processdyckk without such assumptions, which alignswith experimental Ô¨Åndings that recurrent networksperform or generalize poorly on dyckk (bernardy,2018; sennhauser and berwick, 2018; yu et al.,2019).
hewitt et al.
(2020) propose to considerdyckk,d as it better captures natural language, andshow Ô¨Ånite-precision rnns can solve dyckk,dwith Œ∏(d log k) memory..for the broader nlp community, our resultsalso contribute to settling whether self-attentionnetworks are restricted to model hierarchical struc-tures due to non-recurrence, a concern (tran et al.,2018) often turned into proposals to equip trans-formers with recurrence (dehghani et al., 2019;shen et al., 2018; chen et al., 2018; hao et al.,2019).
on one hand, transformers are shown to en-code syntactic (lin et al., 2019; tenney et al., 2019;manning et al., 2020) and word order (yang et al.,2019) information, and dominate syntactical tasksin nlp such as constituency (zhang et al., 2020)and dependency (he and choi, 2019) parsing.
onthe other hand, on several linguistically-motivatedtasks like english subject-verb agreement (tranet al., 2018), recurrent models are reported to out-perform transformers.
our results help address.
the issue by conÔ¨Årming that distributed and recur-rent sequence processing can both model hierarchi-cal structure, albeit with different mechanisms andtradeoffs..3 preliminaries.
3.1 dyck languages.
consider the vocabulary of k types of open andclose brackets œÉ = ‚à™i‚àà[k]{(cid:104)i, (cid:105)i}, and deÔ¨Ånedyckk ‚äÇ Œ≥œÇ‚àóœâ (Œ≥, œâ being special start and endtokens) to be the formal language of well-nestedbrackets of k types.
it is generated starting fromŒ≥xœâ through the following context-free grammar:.
x ‚Üí (cid:15) | (cid:104)i x (cid:105)i x (i ‚àà [k]).
(1).
where (cid:15) denotes the empty string..intuitively, dyckk can be recognized by sequen-tial scanning with a stack (i.e., a pushdown au-tomaton).
open brackets are pushed into the stack,while a close bracket causes the stack to pop, andthe popped open bracket is compared with the cur-rent close bracket (they should be of the same type).
the depth of a string w1:n at position i is the stacksize after scanning w1:i, that is, the number of openbrackets left in the stack:.
d(w1:i) = count(w1:i, (cid:104)) ‚àí count(w1:i, (cid:105)).
(2).
finally, we deÔ¨Åne dyckk,d to be the subset of.
dyckk strings with depth bounded by d:.
(cid:26).
(cid:27).
dyckk,d =.
w1:n ‚àà dyckk.
d(w1:i) ‚â§ d.(cid:12)(cid:12)(cid:12)(cid:12).
maxi‚àà[n].
that is, a string in dyckk,d only requires a stackwith bounded size d to process..3.2 self-attention networks.
we consider the encoder part of the original trans-former (vaswani et al., 2017), which has multiplelayers of two blocks each: (i) a self-attention blockand (ii) a feed-forward network (ffn).
for an inputstring w1:n ‚àà œÉ‚àó, each input token wi is convertedinto a token embedding via fe : œÉ ‚Üí rdmodel, thenadded with a position encoding pi ‚àà rdmodel.
letxi,(cid:96) ‚àà rdmodel be the i-th representation of the (cid:96)-thlayer (i ‚àà [n], (cid:96) ‚àà [l]).
then.
xi,0 = fe(wi) + piai,(cid:96) = att(cid:96)(q(cid:96)(xi), k(cid:96)(x), v(cid:96)(x)).
xi,(cid:96)+1 = f(cid:96)(ai,(cid:96)).
(3).
(4).
(5).
3772attention in each head of a self-attention block,the input vectors x1:n undergo linear transformsq, k, v yielding query, key, and value vectors.
they are taken as input to a self-attention mod-ule, whose t-th output, att(qxi, kx, v x),isa vector ai = (cid:80)j‚àà[t ] Œ±jv xj, where Œ±1:n =softmax((cid:104)qxi, kx1(cid:105), ¬∑ ¬∑ ¬∑ , (cid:104)qxi, kxn(cid:105)).
the Ô¨Å-nal attention output is the concatenation of multi-head attention outputs.
we also consider variantsof the basic model along these directions:.
(i) hard attention, as opposed to soft attentiondescribed above, where hardmax is used in placefor softmax (i.e.
att(qxi, kx, v x) = v xj(cid:48)where j(cid:48) = arg maxj(cid:104)qxi, kxj(cid:105)).
though im-practical for nlp, it has been used to model formallanguages (hahn, 2020)..(ii) positional masking, where Œ±1:i (past) or Œ±i:n(future) is masked for position i. future-positionalmasking is usually used to train auto-regressivemodels like gpt-2 (radford et al., 2019)..feed-forward network a feed-forward networkf transforms each self-attention output vectorai ‚Üí f (ai) individually.
it is usually implementedas a multi-layer perceptron (mlp) with relu ac-tivations.
residual connections (he et al., 2016)and layer normalization (ba et al., 2016) are twooptional components to aid learning..positional encodings vaswani et al.
(2017) pro-poses two kinds of positional encoding: (i) fourierfeatures (rahimi and recht, 2007), i.e.
sine/cosinevalues of different frequencies; (ii) learnable fea-tures for each position.
in this work we propose touse a single scalar i/n to encode position i ‚àà [n],and show that it helps process formal languageslike dyckk,d, both theoretically and empirically..precision and memory size we deÔ¨Åne precisionto be the number of binary bits used to representeach scalar, and memory size per layer (dmodel) tobe the number of scalars used to represent eachtoken at each layer.
the memory size (l ¬∑ dmodel)is the total memory used for each token..3.3 language generation and recognition.
for a transformer with l layers and input w1:i, wecan use a decoder (mlp + softmax) on the Ô¨Ånaltoken output xi,l to predict wi+1.
this deÔ¨Ånesa language model fŒ∏(wi+1|wi) where Œ∏ denotestransformer and decoder parameters.
we followprevious work (hewitt et al., 2020) to deÔ¨Åne how alanguage model can generate a formal language:.
deÔ¨Ånition 3.1 (language generation).
languagemodel fŒ∏ over œÉ(cid:63) generates a language l ‚äÜ œÉ(cid:63) ifthere exists (cid:15) > 0 such that l = {w1:n ‚àà œÉ(cid:63) | ‚àÄi ‚àà[n], fŒ∏(wi|w1:i‚àí1) ‚â• (cid:15)}..we also consider language recognition by a lan-guage classiÔ¨Åer gŒ∏(w1:i), where a decoder on xi,linstead predicts a binary label.
deÔ¨Ånition 3.2 (language recognition).
languageclassiÔ¨Åer gŒ∏ over œÉ(cid:63) recognizes a language l ‚äÜœÉ(cid:63) if l = {w1:n ‚àà œÉ(cid:63) |gŒ∏(w1:n) = 1}..4 theoretical results.
in this section we state our theoretical results alongwith some remarks.
proof sketches are provided inthe next section, and details in appendix a,b,c.
theorem 4.1 (hard-attention, dyckk,d recogni-tion).
for all k, d ‚àà n+, there exists a (d + 1)-layer hard-attention network that can recognizedyckk,d.
it uses both future and past positionalmasking heads, positional encoding of the form i/nfor position i, o(log k) memory size per layer, ando(log n) precision, where n is the input length.
theorem 4.2 (soft-attention, dyckk,d generation).
for all k, d ‚àà n+, there exists a 2-layer soft-attention network that can generate dyckk,d.
ituses future positional masking, positional encod-ing of form i/n for position i, o(log k) memorysize per layer, and o(log n) precision, where n isthe input length.
the feed-forward networks useresidual connection and layer normalization.
theorem 4.3 (precision lower bound).
for allk ‚àà n+, no hard-attention network with o(log n)precision can recognize dyckk,2 where n is theinput length..required precision both constructions requirea precision increasing with input length, as indi-cated by theorem 4.3. the proof of the lowerbound is inspired by the proof in hahn (2020),but several technical improvements are necessary;see appendix c. intuitively, a vector with a Ô¨Åxeddimension and o(log n) precision cannot even rep-resent n positions uniquely.
the required precisionis not unreasonable, since log n is a small overheadto the n tokens the system has to store..comparison to recurrent processing hewittet al.
(2020) constructs a 1-layer rnn to gener-ate dyckk,d with Œ∏(d log k) memory, and provesit is optimal for any recurrent network.
thus the-orem 4.2 establishes a memory advantage of self-attention networks over recurrent ones.
however,.
3773this is based on two tradeoffs: (i) precision.
hewittet al.
(2020) assumes o(1) precision while we re-quire o(log n).
(ii) runtime.
runtime of recurrentand self-attention networks usually scale linearlyand quadratically in n, respectively..comparison between two constructions theo-rem 4.2 requires fewer layers (2 vs. d) and memorysize (o(log k) vs. o(d log k)) than theorem 4.1,thanks to the use of soft-attention, residual con-nection and layer normalization.
though the twoconstructions are more suited to the tasks of recog-nition and generation respectively (section 5), eachof them can also be modiÔ¨Åed for the other task..connection to dyckkin hahn (2020) it is shownthat no hard-attention network can recognizedyckk even for k = 1. theorem 4.1 establishesthat this impossibility can be circumvented bybounding the depth of the dyck language.
hahn(2020) also points out soft-attention networks canbe limited due to bounded lipschitz continuity.
in fact, our theorem 4.2 construction can alsowork on dyckk with some additional assumptions(e.g.
feed n also in input embeddings), and we cir-cumvent the impossibility by using laying normal-ization, which may have an o(n) lipschitz con-stant.
more details are in appendix b.4..5 constructions.
5.1 (d + 1)-layer hard-attention network.
our insight underlying the construction in theo-rem 4.1 is that, by recursively removing matchedbrackets from innermost positions to outside, eachtoken only needs to attend to nearest unmatchedbrackets to Ô¨Ånd its matching bracket or detect er-ror within d layers.
speciÔ¨Åcally, at each layer(cid:96) ‚â§ d, each token will be in one of three states(figure 2 (c)): (i) matched, (ii) error, (iii) un-matched, and we leverage hard-attention to imple-ment a dynamic state updating process to recognizedyckk,d..representation for an input w1:n ‚àà Œ≥œÇ‚àóœâ, therepresentation at position i of layer (cid:96) has Ô¨Åve partsxi,(cid:96) = [ti, oi, pi, mi,(cid:96), ei,(cid:96)]: (i) a bracket type em-bedding ti ‚àà r(cid:100)log k(cid:101) that denotes which brackettype (1 ¬∑ ¬∑ ¬∑ k) the token is (or if the token is start/endtoken); (ii) a bracket openness bit oi ‚àà {0, 1},where 1 denotes open brackets (or start token) and0 denotes close one (or end token); (iii) a posi-tional encoding scalar pi = i/n; (iv) a match bit.
mi,(cid:96) ‚àà {0, 1}, where 1 denotes matched and 0 un-matched; (v) an error bit ei,(cid:96) ‚àà {0, 1}, where 1denotes error and 0 no error.
token identity partsti, oi, pi are maintained unchanged throughout lay-ers.
the match and error bits are initialized asei,0 = mi,0 = 0..the Ô¨Årst d layers have identical self-attentionblocks and feed-forward networks, detailed below..attention consider the (cid:96)-th self-attention layer((cid:96) ‚àà [d]), and denote xi = xi,(cid:96)‚àí1, mi = mi,(cid:96)‚àí1,ai = ai,(cid:96), yi = xi,(cid:96) for short.
we have 3 atten-tion heads: (i) an identity head attid, where eachtoken only attends to itself with attention outputi = xi; (ii) a left head attleft with future po-aidsitional masking; (iii) a right head attright withpast positional masking.
the query, key, and valuevectors for attleft are deÔ¨Åned as qxi = 1 ‚àà r,kxi = pi ‚àí mi ‚àà r, v xi = xi ‚àà rdmodel, so that.
alefti = xj1,.
j1 = arg maxj<i.
(j/n ‚àí mj).
is the representation of the nearest unmatched tokento i on its left side.
similarly.
arighti.
= xj2,.
j2 = arg maxj>i.
(1 ‚àí j/n ‚àí mj).
is the representation of the nearest unmatched to-ken to i on its right side.
the attention output forposition i is the concatenation of these three out-puts: ai = [aid.]
= [xi, xj1, xj2]..i , alefti., arighti.feed-forward network (ffn) following thenotation above, the feed-forward network f : ai ‚Üíyi serves to update each position‚Äôs state using in-formation from xj1, xj2.
the high level logic (fig-ure 2 (c)) is that, if wi is an open bracket, its po-tential matching half should be wj = wj2 (j2 > i),otherwise it should be wj = wj1 (j1 < i).
if wi andwj are one open and one close, they either match(same type) or cause error (different types).
if wiand wj are both open or both close, no state updateis done for position i. besides, token identity partsti, oi, pi are copied from aidto pass on.
the ideaican be translated into a language of logical opera-tions (‚àß, ‚à®, ¬¨) plus a same(t, t(cid:48)) operation, whichreturns 1 if vectors t = t(cid:48) and 0 otherwise:.
i, e(cid:48)i].
yi = [ti, oi, pi, m(cid:48)m(cid:48)i = mi ‚à® (oi ‚àß ¬¨oj2 ‚àß s1) ‚à® (¬¨oi ‚àß oj1 ‚àß s2)e(cid:48)i = ei ‚à® (oi ‚àß ¬¨oj2 ‚àß ¬¨s1) ‚à® (¬¨oi ‚àß oj1 ‚àß ¬¨s2)s1 = same(ti, tj1).
s2 = same(ti, tj2).
3774figure 2: our construction for theorem 4.1.
(a) the network has multiple identical layers to match brackets anddetect errors.
(b) each layer consists of three hard-attention heads so that a token attends to itself and the nearestunmatched tokens on both sides, and uses representations from these positions to update its state.
(c) each positioncan be in three states: matched, error, or unmatched..as we show in appendix a, a multi-layer percep-tion with relu activations can simulate all oper-ations (‚àß, ‚à®, ¬¨, same), thus the existence of ourdesired ffn..final layer at the (d + 1)-th layer, the self at-tention is designed as qxi = 1 ‚àà r, kxi =ei+1‚àími ‚àà r, v xi = (ei, mi) ‚àà r2.
if all brack-ets are matched without error ((ei, mi) = (0, 1)),all keys would be 0, and the attention output of thelast token an would be (0, 1).
if any bracket Ô¨Åndserror (ei = 1) or is not matched (mi = 0), the keywould be at least 1 and an would not be (0, 1).
anfnn that emulates (a, b) (cid:55)‚Üí ¬¨a ‚àß b will deliver ynas the recognition answer..5.2 two-layer soft-attention network.
our theorem 4.2 construction takes advantage ofsoft attention, residual connection, and layer nor-malization to calculate each token depth and trans-late it into a vector form at the Ô¨Årst layer.
using thedepth information, at the second layer each wi canattend to the stack-top open bracket at the position,in order to decide if open brackets or which type ofclose brackets can be generated as the next token(figure 3)..bracket type embedding ti, bracket openness bitoi, position encoding pi already speciÔ¨Åed in sec-tion 5.1. the last part di,(cid:96) ‚àà r2 is used to storedepth information for position i, and initialized asdi,0 = (0, 0)..j‚â§i.
first layer ‚Äì depth counting the Ô¨Årst self-attention layer has two heads, where an attid headis still used to inherit ti, oi, pi, and a future po-sitional masking head2 attd aims to count depthwith qxi = kxi = 1 and v xi = 2oi ‚àí 1, result-ing in uniform attention scores and attention outputi = (cid:80)1adi ¬∑ (2oj ‚àí 1) = d(w1:i)/i.
however, our goal is to enable matching basedon depth di = d(w1:i), and the attention outputdi/i isn‚Äôt readily usable for such a purpose: thedenominator i is undesirable, and even a scalar dicannot easily attend to the same value using dot-product attention.
thus in the Ô¨Årst feed-forwardnetwork, we leverage residual connection and layernormalization to transform.
di/i (cid:55)‚Üí di = (cos(Œ∏(di)), sin(Œ∏(di))).
(6).
where Œ∏(d) = arctan.
has an unique.
(cid:16).
(cid:17).
dd+2‚àíd.
representation the representation at position i,layer (cid:96) has four parts xi,(cid:96) = [ti, oi, pi, di,(cid:96)], with.
2here we assume wi+1:n is masked for position i, just for.
convenience of description..3775([](])layer 1([](])layer 2([](])layer 3output: 0/1([](])ffn([](])([](])([](])([](])([](])([](])ùñ†ùóçùóçùóÖùñæùñøùóçùñ†ùóçùóçùóãùóÇùóÄùóÅùóçùñ†ùóçùóçùóÇùñΩ([](])matchederrorunmatchedunmatchedw1:nx1:n([](])Œ≥Œ≥Œ≥œâœâœâœâŒ≥Œ≥œâŒ≥œâœâœâŒ≥Œ≥Œ≥Œ≥Œ≥Œ≥œâœâœâœâ(a)(b)(c)y1:nfigure 3: our construction for theorem 4.2. the Ô¨Årst self-attention layer calculates token depths, while the secondlayer uses them so that each token attends to the closest unmatched open bracket ign the history, which is usefulfor next token prediction..value for every d ‚àà {0, ¬∑ ¬∑ ¬∑ , d + 1}, so that.
(cid:40).
di ¬∑ dj.
= 1< 1 ‚àí 1.
10d2.
di = djdi (cid:54)= dj.
depth up to o(n).
but it involves extra conditionslike feeding n into network input, and may notbe effectively learned in practice.
please refer todetails in appendix b.4..(7).
the representation by the end of Ô¨Årst layer is xi,1 =[ti, oi, pi, di].
the full detail for the Ô¨Årst ffn is inappendix b.1..second layer ‚Äì depth matching the secondself-attention layer has a depth matching hard-attention head attmatch, with query, key, valuevectors as qxi = [20d2 ¬∑ di, 1, 2] ‚àà r4, kxi =[di, pi, oi] ‚àà r4, v xi = xi, so that attention score.
connection to empirical findings our theo-retical construction explains the observation inebrahimi et al.
(2020): the second layer of a two-layer transformer trained on dyckk often producesvirtually hard attention, where tokens attend to thestack-top open bracket (or start token).
it also ex-plains why such a pattern is found less systemati-cally as input depth increases, as (6) is hard to learnand generalize to unbounded depth in practice..(cid:104)qxi, kxj(cid:105) = 20d2di ¬∑ dj + j/n + 2oj.
6 experiments.
(cid:40).
= 20d2 + 2 + j/n di = dj, oj = 1‚â§ 20d2 + 1.otherwise.
i , amatchi.would achieve its maximum when wj (j ‚â§ i) is theopen bracket (or start token) closest to wi with dj =di.
the attention output is ai = [aid] =[xi, xj] where j = max{j ‚â§ i|di = dj ‚àß oj = 1}.
with such a [xi, xj], the second-layer ffn canreadily predict what wi+1 could be.
it could beany open bracket when di < d (i.e.
cos(Œ∏(di)) >cos(Œ∏(d))), and it could be a close bracket withtype as tj (or end token if wj is start token).
thedetailed construction for such a ffn is in ap-pendix b.2..on dyckk generation in fact, this theoreticalconstruction can also generate dyckk, as intuitivelythe o(log n) precision assumption allows counting.
our constructions show the existence of self-attention networks that are capable of recognizingand generating dyckk,d.
now we bridge theoret-ical insights into experiments, and study whethersuch networks can be learned from Ô¨Ånite samplesand generalize to longer input.
the answer is af-Ô¨Årmative when the right positional encodings andmemory size are chosen according to our theory..we Ô¨Årst present results on dyck8,10 (sec-tion 6.1) as an example dyckk,d language to in-vestigate the effect of different positional encod-ing schemes, number of layers, and hidden sizeon the transformer performance, and to comparewith the lstm performance.
we then extendthe transformer vs. lstm comparison on moredyckk,d languages (k ‚àà {2, 8, 32, 128}, d ‚àà{3, 5, 10, 15}) in section 6.2. finally, we apply.
3776(cid:1153)[]()([]()([]()mlp01(2[1]2(1)([]()012121mlpattention layer 1mlp layer 1attention layer 2mlp layer 2depthw1:i0#1(2[1]2(1)Œ≥Œ≥Œ≥Œ≥Œ≥ prediction: ( [ )wi+1x1:i,0figure 4: results on dyck8,10 validation set (same input lengths as training) and test set (longer inputs).
(a)compares transformers of different layers (l ‚àà {1, 2, 3, 4, 5, 10}) and with different positional encodings (cos,learn,pos/n) on the test set.
(b) and (c) compare a 2-layer transformer (pos/n) with a 1-layer lstm overvarying memory sizes on the validation and test sets respectively..the novel scalar positional encoding to natural lan-guage modeling with some preliminary Ô¨Åndings(section 6.3)..and systemic positional encodings for processinglong and order-sensitive sequences like dyckk,d..6.1 evaluation on dyck8,10.
setup for dyck8,10, we generate training and val-idation sets with input length n ‚â§ 700, and test setwith length 700 < n ‚â§ 1400. we train randomlyinitialized transformers using the huggingface li-brary (wolf et al., 2019), with one future positionalmasking head, l ‚àà {1, 2, 3, 4, 5, 10} layers, anda default memory size dmodel = 30. we searchfor learning rates in {0.01, 0.001}, run each modelwith 3 trials, and report the average accuracy ofgenerating close brackets, the major challenge ofdyckk,d.
more setup details are in appendix d.1..positional encodings we compare 3 types of po-sitional encodings: (i) fourier features (cos); (ii)learnable features (learn); (iii) a scalar i/6000for position i (pos/n).
note that (i, ii) are originalproposals in vaswani et al.
(2017), where positionalencoding vectors are added to the token embed-dings, while our proposal (iii) encodes the positionas a Ô¨Åxed scalar separated from token embeddings.
on the validation set of dyck8,10 (see ap-pendix d.2), all three models achieve near-perfectaccuracy with l ‚â• 2 layers.
on the test set (fig-ure 4(a)) however, only pos/n maintains near-perfect accuracy, even with l = 10 layers.
mean-while, learn and cos fail to generalize, becauseencodings for position 700 < i ‚â§ 1400 are notlearned (for learn) or experienced (for cos) dur-ing training.
the result validates our theoreticalconstruction, and points to the need for separate.
memory size and comparison with lstmwe compare a two-layer transformer (pos/n) witha one-layer lstm3 (hochreiter and schmidhu-ber, 1997) using varying per-layer memory sizesdmodel ‚àà {10, 20, ¬∑ ¬∑ ¬∑ , 100}.
as figure 4 (b)shows, the transformer consistently outperformsthe lstm on the validation set.
on the test set(figure 4 (c)), the transformer and the lstm Ô¨Årstachieve a > 90% accuracy using dmodel = 20 and40 respectively, and an accuracy of > 95% withdmodel = 30 and 50, respectively.
these Ô¨Åndingsagree with our theoretical characterization that self-attention networks have a memory advantage overrecurrent ones..6.2 evaluation on more dyckk,d languages.
setup in order to generalize some of the aboveresults, we generate a wide range of dyckk,dlanguages with different vocabulary sizes (k ‚àà{2, 8, 32, 128}) and recursion bounds (d ‚àà{3, 5, 10, 15}).
we continue to compare the one-layer lstm versus the two-layer transformer(pos/n).
for each model on each language, weperform a hyperparameter search for learning ratein {0.01, 0.001} and memory size dmodel ‚àà{10, 30, 50}, and report results from the best set-ting based on two trials for each setting..3lstms only need one layer to process dyckk,d (hewittet al., 2020), while transformers at least need two in ourconstructions.
we also experimented with two-layer lstmsbut did not Ô¨Ånd improved performance..37771234510# layers0.60.70.80.91.0close accuracy(a) transformers (dyck-(8, 10) test)positional encodingcoslearnpos/n20406080100memory dim.0.80.91.0close accuracy(b) transformer v. lstm  (dyck-(8, 10) validation)modeltransformer (pos/n)lstm20406080100memory dim.0.80.91.0close accuracy(c) transformer v. lstm  (dyck-(8, 10) test)modeltransformer (pos/n)lstmfigure 5: results on more dyckk,d languages..figure 6: results on wikitext-103..results the validation and test accuracy of themodels are reported in figure 5, and more Ô¨Åne-grained results for each dmodel ‚àà {10, 30, 50}are in appendix d.2.
the transformer attains a> 99.9% validation accuracy and a > 94% testaccuracy across all languages, strengthening themain claim that self-attention networks can learndyckk,d languages and generalize to longer input.
on the other hand, the validation and test accu-racy of the lstm model are less than 80% whenthe vocabulary size and recursion depth are large,i.e.
(k, d) ‚àà {(32, 15), (128, 10), (128, 15)}4,which reconÔ¨Årms transformers‚Äô memory advan-tage under limited memory (dmodel ‚â§ 50)..6.3 evaluation on wikitext-103.
in section 6.1, we show a transformer with thescalar positional encoding scheme (pos/n) canlearn dyckk,d and generalize to longer input, whiletraditional positional encoding schemes ((cos),(learn)) lead to degraded test performance.
toinvestigate whether such a novel scheme is also use-ful in nlp tasks, we train two roberta5 models(pos/n, learn) from scratch on the wikitext-103 dataset (merity et al., 2017) for 150 epochs..figure 6 shows the masked language modelingloss on both training and validation sets.
by the endof the training, pos/n has a slightly larger valida-tion loss (1.55) than learn (1.31).
but throughoutthe optimization, pos/n shows a gradual decreaseof loss while learn has a sudden drop of lossaround 20-30 epochs.
we believe it will be interest-.
4note that hewitt et al.
(2020) only reports d ‚àà {3, 5}.
5we also tried language modeling with gpt-2 models, andpos/n has slightly larger train/validation losses than learnthroughout the training.
interestingly, using no positional en-coding leads to the same loss curves as learn, as positionalmasking leaks positional information..ing for future work to explore how pos/n performson different downstream tasks, and why pos/nseems slightly worse than learn (at least on thismlm task), though theoretically it provides thecomplete positional information for transformers.
these topics will contribute to a deeper understand-ing of positional encodings and how transformersleverage positional information to succeed on dif-ferent tasks..7 discussion.
in this paper, we theoretically and experimen-tally demonstrate that self-attention networks canprocess bounded hierarchical languages dyckk,d,even with a memory advantage over recurrent net-works, despite performing distributed processingof sequences without explicit recursive elements.
our results may explain their widespread success atmodeling long pieces of text with hierarchical struc-tures and long-range, nested dependencies, includ-ing coreference, discourse and narratives.
we hopethese insights can enhance knowledge about thenature of recurrence and parallelism in sequenceprocessing, and lead to better nlp models..acknowledgement.
we thank xi chen, members of the princeton nlpgroup, and anonymous reviewers for suggestionsand comments..ethical consideration.
our work is mainly theoretical with no foreseeableethical issues..3778351015d0.750.800.850.900.951.00close accuracy(a) dyck-(k, d) validationmodeltransformerlstmk2832128351015d0.750.800.850.900.951.00close accuracy(b) dyck-(k, d) test050100150epoch246810lossroberta (wikitext-103)positional encodinglearnpos/nsplittrainvalidationreferences.
jimmy lei ba, jamie ryan kiros, and geoffrey e hin-arxiv preprint.
ton.
2016. layer normalization.
arxiv:1607.06450..jean-phillipe bernardy.
2018. can recurrent neural net-works learn nested recursion?
in linguistic issues inlanguage technology, volume 16, 2018. csli pub-lications..satwik bhattamishra, kabir ahuja, and navin goyal.
2020a.
on the ability of self-attention networks torecognize counter languages.
in proceedings of the2020 conference on empirical methods in naturallanguage processing (emnlp), pages 7096‚Äì7116..satwik bhattamishra, arkil patel, and navin goyal.
2020b.
on the computational power of transformersand its implications in sequence modeling.
in pro-ceedings of the 24th conference on computationalnatural language learning, pages 455‚Äì475, online.
association for computational linguistics..jonathan r brennan and john t hale.
2019. hierarchi-cal structure guides rapid linguistic predictions dur-ing naturalistic listening.
plos one, 14(1):e0207741..mia xu chen, orhan firat, ankur bapna, melvinjohnson, wolfgang macherey, george foster, llionjones, mike schuster, noam shazeer, niki parmar,ashish vaswani, jakob uszkoreit, lukasz kaiser,zhifeng chen, yonghui wu, and macduff hughes.
2018. the best of both worlds: combining recentadvances in neural machine translation.
in proceed-ings of the 56th annual meeting of the associationfor computational linguistics (volume 1: long pa-pers), pages 76‚Äì86, melbourne, australia.
associa-tion for computational linguistics..noam chomsky.
1956. three models for the descrip-tion of language.
ire transactions on informationtheory, 2(3):113‚Äì124..noam chomsky and marcel p sch√ºtzenberger.
1959.the algebraic theory of context-free languages.
instudies in logic and the foundations of mathemat-ics, volume 26, pages 118‚Äì161.
elsevier..sreerupa das, c lee giles, and guo-zheng sun.
1992.learning context-free grammars: capabilities andlimitations of a recurrent neural network with an ex-in proceedings of the four-ternal stack memory.
teenth annual conference of cognitive science so-ciety.
indiana university, page 14. citeseer..mostafa dehghani, stephan gouws, oriol vinyals,jakob uszkoreit, and lukasz kaiser.
2019. univer-sal transformers.
in 7th international conference onlearning representations, iclr 2019, new orleans,la, usa, may 6-9, 2019. openreview.net..javid ebrahimi, dhruv gelda, and wei zhang.
2020.how can self-attention networks recognize dyck-n.languages?
in findings of the association for com-putational linguistics: emnlp 2020, pages 4301‚Äì4306, online.
association for computational lin-guistics..jeffrey l elman.
1990. finding structure in time.
cog-.
nitive science, 14(2):179‚Äì211..stefan l frank, rens bod, and morten h christiansen.
pro-2012. how hierarchical is language use?
ceedings of the royal society b: biological sciences,279(1747):4522‚Äì4531..stefan l frank and morten h christiansen.
2018. hi-erarchical and sequential processing of language: aresponse to: ding, melloni, tian, and poeppel (2017).
rule-based and word-level statistics-based process-ing of language:insights from neuroscience.
lan-guage, cognition and neuroscience.
language, cog-nition and neuroscience, 33(9):1213‚Äì1218..michael hahn.
2020. theoretical limitations of self-attention in neural sequence models.
transactionsof the association for computational linguistics,8:156‚Äì171..jie hao, xing wang, baosong yang, longyue wang,jinfeng zhang, and zhaopeng tu.
2019. modelingin proceedings of therecurrence for transformer.
2019 conference of the north american chapter ofthe association for computational linguistics: hu-man language technologies, volume 1 (long andshort papers), pages 1198‚Äì1207, minneapolis, min-nesota.
association for computational linguistics..marc d hauser, noam chomsky, and w tecumsehfitch.
2002. the faculty of language: what isscience,it, who has it, and how did it evolve?
298(5598):1569‚Äì1579..han he and jinho d choi.
2019. establishing strongbaselines for the new decade: sequence tagging,arxivsyntactic and semantic parsing with bert.
preprint arxiv:1908.04943..kaiming he, xiangyu zhang, shaoqing ren, and jiansun.
2016. deep residual learning for image recog-nition.
in 2016 ieee conference on computer vi-sion and pattern recognition, cvpr 2016, las ve-gas, nv, usa, june 27-30, 2016, pages 770‚Äì778.
ieee computer society..john hewitt, michael hahn, surya ganguli, percyliang, and christopher d. manning.
2020. rnnscan generate bounded hierarchical languages withoptimal memory.
in proceedings of the 2020 con-ference on empirical methods in natural languageprocessing (emnlp), pages 1978‚Äì2010, online.
as-sociation for computational linguistics..sepp hochreiter and j√ºrgen schmidhuber.
1997.neural computation,.
long short-term memory.
9(8):1735‚Äì1780..3779lifeng jin, finale doshi-velez, timothy miller,william schuler, and lane schwartz.
2018. un-supervised grammar induction with depth-boundedpcfg.
transactions of the association for compu-tational linguistics, 6:211‚Äì224..jorge p√©rez, javier marinkovic, and pablo barcel√≥.
2019. on the turing completeness of modern neuralnetwork architectures.
in 7th international confer-ence on learning representations, iclr 2019, neworleans, la, usa, may 6-9, 2019. openreview.net..fred karlsson.
2007. constraints on multiple center-embedding of clauses.
journal of linguistics, pages365‚Äì392..guolin ke, di he, and tie-yan liu.
2021. rethink-ing the positional encoding in language pre-training.
in international conference on learning represen-tations, (iclr 2021)..samuel a korsky and robert c berwick.
2019. onthe computational power of rnns.
arxiv preprintarxiv:1906.06349..stephen c levinson.
2014. pragmatics as the origin ofrecursion.
in language and recursion, pages 3‚Äì13.
springer..yongjie lin, yi chern tan, and robert frank.
2019.open sesame: getting inside bert‚Äôs linguisticknowledge.
in proceedings of the 2019 acl work-shop blackboxnlp: analyzing and interpreting neu-ral networks for nlp, pages 241‚Äì253, florence,italy.
association for computational linguistics..christopher d manning, kevin clark, john hewitt, ur-vashi khandelwal, and omer levy.
2020. emer-gent linguistic structure in artiÔ¨Åcial neural networkstrained by self-supervision.
proceedings of the na-tional academy of sciences, 117(48):30046‚Äì30054..stephen merity, caiming xiong, james bradbury, andrichard socher.
2017. pointer sentinel mixture mod-in 5th international conference on learningels.
representations, iclr 2017, toulon, france, april24-26, 2017, conference track proceedings.
open-review.net..william merrill, gail weiss, yoav goldberg, royschwartz, noah a. smith, and eran yahav.
2020. aformal hierarchy of rnn architectures.
in proceed-ings of the 58th annual meeting of the associationfor computational linguistics, pages 443‚Äì459, on-line.
association for computational linguistics..matthew j nelson, imen el karoui, kristof giber,xiaofang yang, laurent cohen, hilda koopman,sydney s cash, lionel naccache, john t hale,christophe pallier, et al.
2017. neurophysiolog-ical dynamics of phrase-structure building duringsentence processing.
proceedings of the nationalacademy of sciences, 114(18):e3669‚Äìe3678..isabel papadimitriou and dan jurafsky.
2020. learn-ing music helps you read: using transfer to studylinguistic structure in language models.
in proceed-ings of the 2020 conference on empirical methodsin natural language processing (emnlp), pages6829‚Äì6839, online.
association for computationallinguistics..alec radford, jeffrey wu, rewon child, david luan,dario amodei, and ilya sutskever.
2019. languagemodels are unsupervised multitask learners.
openaiblog, 1(8):9..ali rahimi and benjamin recht.
2007. random fea-tures for large-scale kernel machines.
in advancesin neural information processing systems 20, pro-ceedings of the twenty-first annual conference onneural information processing systems, vancouver,british columbia, canada, december 3-6, 2007,pages 1177‚Äì1184.
curran associates, inc..luzi sennhauser and robert berwick.
2018. evaluat-ing the ability of lstms to learn context-free gram-in proceedings of the 2018 emnlp work-mars.
shop blackboxnlp: analyzing and interpreting neu-ral networks for nlp, pages 115‚Äì124, brussels, bel-gium.
association for computational linguistics..peter shaw, jakob uszkoreit, and ashish vaswani.
2018. self-attention with relative position represen-in proceedings of the 2018 conference oftations.
the north american chapter of the association forcomputational linguistics: human language tech-nologies, volume 2 (short papers), pages 464‚Äì468,new orleans, louisiana.
association for computa-tional linguistics..tao shen, tianyi zhou, guodong long, jing jiang,shirui pan, and chengqi zhang.
2018. disan: di-rectional self-attention network for rnn/cnn-free lan-guage understanding.
in proceedings of the thirty-second aaai conference on artiÔ¨Åcial intelligence,(aaai-18), the 30th innovative applications of arti-Ô¨Åcial intelligence (iaai-18), and the 8th aaai sym-posium on educational advances in artiÔ¨Åcial intel-ligence (eaai-18), new orleans, louisiana, usa,february 2-7, 2018, pages 5446‚Äì5455.
aaai press..vighnesh leonardo shiv and chris quirk.
2019. novelpositional encodings to enable tree-based transform-ers.
in advances in neural information processingsystems 32: annual conference on neural informa-tion processing systems 2019, neurips 2019, de-cember 8-14, 2019, vancouver, bc, canada, pages12058‚Äì12068..mark steijvers and peter gr√ºnwald.
1996. a recurrentnetwork that performs a context-sensitive predictiontask.
in proceedings of the 18th annual conferenceof the cognitive science society, pages 335‚Äì339..mirac suzgun, sebastian gehrmann, yonatan belinkov,and stuart m shieber.
2019. memory-augmented re-current neural networks can learn generalized dycklanguages.
arxiv preprint arxiv:1911.03329..3780ian tenney, dipanjan das, and ellie pavlick.
2019.inbert rediscovers the classical nlp pipeline.
proceedings of the 57th annual meeting of the asso-ciation for computational linguistics, pages 4593‚Äì4601, florence, italy.
association for computationallinguistics..ke tran, arianna bisazza, and christof monz.
2018.the importance of being recurrent for modeling hi-in proceedings of the 2018erarchical structure.
conference on empirical methods in natural lan-guage processing, pages 4731‚Äì4736, brussels, bel-gium.
association for computational linguistics..ashish vaswani, noam shazeer, niki parmar, jakobuszkoreit, llion jones, aidan n. gomez, lukaszkaiser, and illia polosukhin.
2017. attention is allyou need.
in advances in neural information pro-cessing systems 30: annual conference on neuralinformation processing systems 2017, december 4-9, 2017, long beach, ca, usa, pages 5998‚Äì6008..benyou wang, donghao zhao, christina lioma, qi-uchi li, peng zhang, and jakob grue simonsen.
2020. encoding word order in complex embeddings.
in 8th international conference on learning repre-sentations, iclr 2020, addis ababa, ethiopia, april26-30, 2020. openreview.net..thomas wolf, lysandre debut, victor sanh, julienchaumond, clement delangue, anthony moi, pier-ric cistac, tim rault, r√©mi louf, morgan fun-towicz, et al.
2019. huggingface‚Äôs transformers:state-of-the-art natural language processing.
arxivpreprint arxiv:1910.03771..baosong yang, longyue wang, derek f. wong,lidia s. chao, and zhaopeng tu.
2019. assessingthe ability of self-attention networks to learn wordin proceedings of the 57th annual meet-order.
ing of the association for computational linguis-tics, pages 3635‚Äì3644, florence, italy.
associationfor computational linguistics..xiang yu, ngoc thang vu, and jonas kuhn.
2019.learning the dyck language with attention-basedseq2seq models.
in proceedings of the 2019 aclworkshop blackboxnlp: analyzing and interpretingneural networks for nlp, pages 138‚Äì146, florence,italy.
association for computational linguistics..chulhee yun, srinadh bhojanapalli, ankit singhrawat, sashank j. reddi, and sanjiv kumar.
2020. are transformers universal approximatorsin 8th inter-of sequence-to-sequence functions?
national conference on learning representations,iclr 2020, addis ababa, ethiopia, april 26-30,2020. openreview.net..yu zhang, houquan zhou, and zhenghua li.
2020.fast and accurate neural crf constituency parsing.
in proceedings of the twenty-ninth internationaljoint conference on artiÔ¨Åcial intelligence, ijcai2020, pages 4046‚Äì4053.
ijcai.org..3781a construction details of section 5.1.we provide missing details on the construction of(d + 1)-layer transformer with hard attention.
inparticular, we prove that neural networks are capa-ble of simulating logic gates: and, or, not, sameand arithmic gates: greaterthan and equalgate.
for input x, y ‚àà r, the greaterthan sat-isÔ¨Åes that greaterthan(x, y) = 1 if x ‚â• y + cand greaterthan(x, y) = 0 when x < y; theequal gate satisÔ¨Åes equal(x, y) = 1 if x = yand equal(x, y) = 0 when x < y‚àíc or x > y+c.
here c is a constant independent of x, y.lemma a.1.
a constant layer neural network cansimulate logic gates: and, or, not, same andarithmic gates: greaterthan, equal..proof.
our construction is as follows..(1) and gate.
given input x1, .
.
.
, xm ‚àà {0, 1},we compute z = max{x1 + ¬∑ ¬∑ ¬∑ + xm ‚àí m + 1, 0}.
we conclude that z = 1 iff x1 = ¬∑ ¬∑ ¬∑ = xm = 1and z = 0 otherwise..(2) not gate.
given input x ‚àà {0, 1}, it sufÔ¨Åces.
to compute z = max{1 ‚àí x, 0}..(3) or gate.
given input x1, .
.
.
, xm ‚àà {0, 1},we compute z = max{1 ‚àí max{1 ‚àí x1 ‚àí ¬∑ ¬∑ ¬∑ ‚àíxm, 0}, 0}.
it is easy to see that z = 1 iff one ofxi = 1 (i ‚àà [m]) and z = 0 otherwise..(3) same gate.
given input x1, .
.
.
, xm ‚àà {0, 1}and y1, .
.
.
, ym ‚àà {0, 1}.
the same gate isequivalent to z = ((x1 ‚à® y1) ‚àß (x1 ‚à® y1)) ‚à® ¬∑ ¬∑ ¬∑ ‚à®((xm ‚à® ym) ‚àß (xm ‚à® ym)).
we can construct it us-ing logic gates: and, or, not ..(4) greaterthan gate.
given x, y ‚àà r, com-pute z1 = 1c max{c ‚àí max{x ‚àí y, 0}, 0}, we havethat z1 = 0 when x > y + c and z = 1 whenx ‚â§ y. taking z = max{1 ‚àí z1, 0} completes theconstruction..(5) equal gate.
given x, y ‚àà r..letz1 = greaterequal(x, y) and z2 =greaterequal(y, x).
it sufÔ¨Åces to take z =¬¨z1 ‚àß ¬¨z2..with some extra effort, one can extend the con-struction for recognition task to generation taskand prove that a d-layer transformer is capable ofgenerating dyckk,d.
corollary a.2.
‚àÄk, d ‚àà n+, there exists a d-layer hard-attention network that can generatedyckk,d.
it uses both a future-position maskinghead and a past-position masking head, a o(log k)memory size, and o(log n) precision for process-ing input length up to n..soft attention both theorem 4.1 and corol-lary a.2 can be adapted to soft attention, by settingthe temperature parameter Œ∑ in softmax operatorto be sufÔ¨Åcient large, say Œ∑ = œâ(n log nd).
thenone can use soft attention to simulate hard attention.
in order to Ô¨Åt the precision, for the soft attentiondistribution p = [p1, ¬∑ ¬∑ ¬∑ , pm], we round pi to theclosest multiple of 1cn , where c is a large constant..b construction details of section 5.2.we provide missing details of the construction insection 5.2..b.1 first layer ffn.
recall the output of the Ô¨Årst attention layer isai,1 = [ti, oi, pi, di,1], where ti, oi, pi are thebracket type embedding, the bracket openness bitand the position encoding.
di,1 ‚àà r2 contains theinformation di/i, where di = d(w1:i) equals thedepth at position i. for ease of presentation, weassume it also contains an entry with 1/i, this canbe derived with an extra attention head in the Ô¨Årstlayer or be inherited from an extra position encod-ing.
deÔ¨Åne Œ∏(d) = arctan.
we prove.
(cid:16).
(cid:17).
dd+2‚àíd.
lemma b.1.
with residual connection and layernormalization, a two-layer mlp can perform thefollowing transformation.
(di/i, 1/i) (cid:55)‚Üí di = (cos(Œ∏(di)), sin(Œ∏(di))).
while keeping ti, oi, pi unchanged..proof.
consider the following series of operations..ti, oi, pi,.
(cid:55)‚Üí.
0, 0, 0, ‚àí.
(cid:19).
, 0, 0.
1idi ‚àí d ‚àí 2i.,.
,.
,.
dii.,.
d + 2 ‚àí dii.
(cid:19).
(cid:55)‚Üí.
0, 0, 0, ‚àí.
sin(Œ∏(di)), ‚àí.
cos(Œ∏(di)),.
sin(Œ∏(di)),.
cos(Œ∏(di)).
(cid:55)‚Üí.
0, 0, 0, 0, 0,.sin(Œ∏(di)),.
cos(Œ∏(di)).
(cid:55)‚Üí.
ti, oi, pi,.
12(cid:55)‚Üí (ti, oi, pi, cos(Œ∏(di)), sin(Œ∏(di)), 0, 0)).
sin(Œ∏(di)),.
dii.
12.,.
,.
cos(Œ∏(di)).
(cid:19).
(cid:19).
the Ô¨Årst steps can be achieved with a linear trans-formation, the second step can be achieved by layer.
diidii12.
12.
121i.
(cid:18).
(cid:18).
(cid:18).
12(cid:18).
(cid:18).
12(cid:19).
12.
3782normalization and the third step follows from therelu activation gate, the fourth step comes fromthe residual connection and the last step can be ob-tained with an extra layer of mlp.
we conclude theproof here..b.2 second layer ffn.
we can choose between k open brackets and thematched close bracket, with the exception on afew boundary cases: (1) the depth of the currentbracket reaches the maximum; (2) the length ofthe sequence is about to reach the maximum.
let(cid:101)mi be the bracket type of the matched bracket atposition i, we implement the last layer as follow..yi = [oi, zi, zi].
oi = ¬¨(di1 = sin(Œ∏(d))) ‚àß ¬¨(di1 = sin(Œ∏( (cid:101)d)))(cid:101)d = min{n ‚àí i, d + 1}zi = ¬¨(di1 = 0) ‚àß (cid:101)mizi = 1 ‚àí zi..we elaborate on a few details here.
(1) we canderive the term sin(Œ∏( (cid:101)d)) via the similar methodin lemma b.1.
(2) since | sin(Œ∏(i)) ‚àí sin(Œ∏(j))| =(cid:1) holds for any i (cid:54)= j ‚àà {0, 1, ¬∑ ¬∑ ¬∑ , d + 1},œâ (cid:0) 1d2the constant cwe know that the input gap (i.e.
in lemma a.1) for of all three equal gates is(cid:1).
thus we can apply lemma a.1.
at least œâ (cid:0) 1d2(3) we can obtain n ‚àí i by either augmenting theposition encoding with n and i, or normalizing(i/n, 1 ‚àí i/n) (see lemma b.1)..output mechanism the Ô¨Ånal output is deter-mined by on v yt +2, where v ‚àà r2k√ó2(cid:100)log k(cid:101)+1satisÔ¨Åes vi,1 = 0 and vi,1: is the binary encod-ing of the i-th close bracket and its complementwhen i ‚àà {1, ¬∑ ¬∑ ¬∑ , k}; vi,1 = (cid:100)log k(cid:101) and vi,j = 0when i ‚â§ {k + 1, ¬∑ ¬∑ ¬∑ , 2k} and j > 1. lets ‚äÜ [2k] denote the index of valid output, weconclude that (v yt +2)i = (cid:100)log k(cid:101) for i ‚àà s and(v yt +2)i ‚â§ (cid:100)log k(cid:101) ‚àí 1 for i /‚àà s..b.3 extension to recognition task.
our construction can be adapted to recognition taskwith some extra efforts.
corollary b.2.
for all k, d ‚àà n+, there existsa 3-layer soft-attention network that can generatedyckk,d.
it uses future positional masking, posi-tional encoding of form i/n for position i, o(log k)memory size per layer, and o(log n) precisionwhere n is the input length.
the feed-forward.
networks use residual connection and layer nor-malization..b.4 extension to dyckkwe can extend the above construction to recognizelanguage dyckk.
our construction bypasses thelower bound in hahn (2020) since the layer nor-malization operation is not constant lipschitz (itcan be o(n) in the proof)..theorem b.3 (soft-attention, dyckk generation).
for all k ‚àà n+, there exists a 2-layer soft-attentionnetwork that can generate dyckk.
it uses future po-sitional masking, o(log k) memory size per layer,and o(log n) precision where n is the input length.
the feed-forward networks use residual connectionand layer normalization..due to space limits, we omit the detailed proofand only outline the major difference from theproof of theorem 4.2..1. we need position encoding i/n3 instead ofi/n, and add an extra position encoding of n..2. for the Ô¨Årst fnn, we replace d with n. inparticular, for lemma b.1, we need an extrainput of n/i, this can be derived with eitheran extra attention head or an extra positionencoding..3. for the second fnn, we make some adjust-ment to the input of the equal gate, since thegap between two input could be very small,i.e., o(1/n2).
nevertheless, we can use thesame trick of lemma b.1 to amplify the gapbetween two input a, b to be of order œâ(1),the later one sufÔ¨Åces to our purpose..c theoretical limits for Ô¨Ånite position.
encoding.
we prove that a transformer with Ô¨Ånite preci-sion can not recognize dyckk,d language.
infact, we show a stronger result: no transformerwith o(log n) precision can recognize dyckk,d lan-guage of length more than n..theorem c.1 (formal statement of theorem 4.3).
for any k ‚àà n, using hard attention, no trans-former with o(log n) encoding precision can rec-ognize dyckk,2 language with input length n..our proof is inspired by hahn (2020) but withseveral different technique ingredient: (1) we allowarbitrary attention masking (both future and past.
3783position masking); (2) we allow arbitrary positionencoding (3) our lower bounds holds for boundeddepth language dyckk,d; (4) we provide an quanti-tative bound for precision in terms of input lengthn. in general, our lower bound is incomparablewith hahn (2020), we prove a Ô¨Åne grained boundon the precision requirement for bounded depthlanguage dyckk,d, while the proof in hahn (2020)applies only for language with depth œâ(n) but al-lows arbitrary precision on position encoding..the high level intuition behind our proof is thatthe attention head can only catch o(n) input posi-tions when we properly Ô¨Åx a small number of sym-bol in the input sequence.
this limits the capabilityof a transformer and makes it fail to recognizedyckk,d language..we consider a l-layer transformer and assume3h attention heads in total: h normal attentionheads, h attention heads with future position mask-ing, h attention heads with past position mask-ing.
to make our hardness result general, we allowresidual connection for the attention layer, and weassume the fnn can be arbitrary function deÔ¨Åningon the attention outcome.
in the proof, we wouldgradually Ô¨Åx o(n) positions of the input sequence.
we only perform the follow two kinds of assign-ment (1) we assign matching brackets to positioni, i + 1 where i is odd; (2) we assign matchingbrackets (e.g., we assign ‚Äò[‚Äô, ‚Äò(‚Äô, ‚Äò)‚Äô, ‚Äò]‚Äô) to positioni, i + 1, i + 2, i + 3 for odd i. a partial assignmentto the input sequence is said to be well-aligned if itfollows these two rules.
throughout the proof, weguarantee that for any i ‚àà [n], (cid:96) ‚àà [l], the outputof the (cid:96)-th layer xi,(cid:96) depends only the input symbolat position i. this is clearly satisÔ¨Åed for (cid:96) = 0,given the it is composed by position embeddingand word embedding only.
we gradually Ô¨Åx theinput and conduction induction on (cid:96).
we use c(cid:96) todenote the number of positions we Ô¨Åxed before the(cid:96)-th layer, and we use s(cid:96) to denote the number ofconsecutive assigned blocks of the input sequence.
it is clear that s(cid:96) ‚â§ 2c(cid:96).
the following lemma iskey to our analysis.
due to space limits, we omitthe detailed proof..lemma c.2.
for any (cid:96) ‚àà {1, ¬∑ ¬∑ ¬∑ , l}, given awell-aligned partially assigned input sequence,suppose the input of (cid:96)-th layer xi,(cid:96)‚àí1 dependson the symbol at position i only.
then by Ô¨Åxingc(cid:96)h 2(k + 1)o((cid:96)h)2o((cid:96)hp) additional positions ofthe input sequence, we guarantee that the output of(cid:96)-th layer xi,(cid:96) also depends solely on the symbol at.
position i..proof of theorem c.1.
we apply lemma c.2 andcompute the number of positions cl+1 we need torestrict, in order to guarantee that the output of l-thlayer xi,l+1 depends only on the input at position(i ‚àà [n]).
since c(cid:96)+1 ‚â§ c(cid:96)h 2(k + 1)o((cid:96)h)2o((cid:96)hp)and c1 = o(1), we have.
cl+1 (cid:46) h o(l)(k + 1)o(l2h)2o(l2hp)..by taking.
h o(l)(k + 1)o(l2h)2o(l2hp) ‚â§ 0.01n..we know the partial assigned sequence is well-aligned, has depth at most two, and the number ofassignment is only 0.01. thus, we assert that thatwhen p = o(log n), the output of transformer iscompletely determined by the partial assignmentand it do not detect whether there exists error in theunassigned positions and thus can not recognizedyckk,2 language.
we conclude the proof here..d experiment details.
d.1 setup.
data we follow hewitt et al.
(2020) to gener-ate dyckk,d by randomly sampling stack decisions(push, pop, or end) and maintaining length condi-tions (table 1) for a o(d2) hitting time of differ-ent dfa states.
the number of tokens for train,validation, and test set is 2 √ó 106, 2 √ó 105, 106respectively..dtrain/val lengthstest lengths.
31:8485:168.
51:180181:360.
101:700701:1400.
151:16201621:3240.table 1: input lengths for dyckk,d with different d..models we use the lstm model implementedin hewitt et al.
(2020).
for transformer models,we turn off all drop outs as we Ô¨Ånd them to hurtperformance greatly.
we also use only 1 head aswe Ô¨Ånd more heads to hurt performance.
we useadam optimizer with initial learning rate being0.01 or 0.001, and choose the better learning rate interms of validation accuracy for each experiment.
we train for at most 100 epochs but allow earlystopping if the validation loss converges..3784metric we follow hewitt et al.
(2020) and usethe accuracy of correct close bracket predictions:.
p((cid:105)j|(cid:105)) =.
p((cid:105)j)i p((cid:105)i).
(cid:80).
let pl be the empirical probability that the modelconÔ¨Ådently predicts a close bracket (deÔ¨Åned asp((cid:105)j|(cid:105)) > .8), conditioned on it being separatedfrom its open bracket by l tokens.
unlike hewittet al.
(2020) where meanlpl is reported, we reportelpl for two reasons: (i) when l is large pl mightbe only deÔ¨Åned by one trail, thus meanlpl ampliÔ¨Åesthe randomness; (ii) the Ô¨Åndings remain similarwith either metrics..d.2 more results.
in figure 7, we show the validation performancefor transformers of different positional encodingschemes.
they all reach near-perfect accuracywhen having at least 2 layers..in figure 8, we break down the results in sec-tion 6.2 when dmodel ‚àà {10, 30, 50}.
we alsoadd results for a Ô¨Åve-layer transformer, which per-forms similarly as the two-layer transformer.
thisshows (i) a two-layer transformer, as suggestedby our theory, is enough to process dyckk,d, and(ii) transformers with more layers can also learnto process dyckk,d without overÔ¨Åtting or degradedperformance..figure 7: validation results on dyck8,10..figure 8: validation and test results on dyckk,d (k ‚àà{2, 8, 32, 128} and d ‚àà {3, 5, 10, 15}).
enlarge fordetails..37851234510# layers0.60.70.80.91.0close accuracytransformers (dyck-(8, 10) validation)positional encodingcoslearnpos/n0.00.20.40.60.81.0dev_close_acck = 2 | d = 3modellstm (1 layer)transformer (2 layers)transformers (5 layers)k = 2 | d = 5k = 2 | d = 10k = 2 | d = 150.00.20.40.60.81.0dev_close_acck = 8 | d = 3k = 8 | d = 5k = 8 | d = 10k = 8 | d = 150.00.20.40.60.81.0dev_close_acck = 32 | d = 3k = 32 | d = 5k = 32 | d = 10k = 32 | d = 15103050hidden_dim0.00.20.40.60.81.0dev_close_acck = 128 | d = 3103050hidden_dimk = 128 | d = 5103050hidden_dimk = 128 | d = 10103050hidden_dimk = 128 | d = 15transformers v. lstm (dyck-(k, d) validation)0.00.20.40.60.81.0test_close_acck = 2 | d = 3k = 2 | d = 5k = 2 | d = 10k = 2 | d = 150.00.20.40.60.81.0test_close_acck = 8 | d = 3k = 8 | d = 5k = 8 | d = 10k = 8 | d = 150.00.20.40.60.81.0test_close_acck = 32 | d = 3k = 32 | d = 5k = 32 | d = 10k = 32 | d = 15103050hidden_dim0.00.20.40.60.81.0test_close_acck = 128 | d = 3103050hidden_dimk = 128 | d = 5103050hidden_dimk = 128 | d = 10103050hidden_dimk = 128 | d = 15transformers v. lstm (dyck-(k, d) test)