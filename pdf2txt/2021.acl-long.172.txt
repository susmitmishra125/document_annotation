on the effectiveness of adapter-based tuning forpretrained language model adaptation.
ruidan he∗1, linlin liu∗12†, hai ye∗3, qingyu tan13†, bosheng ding12†,liying cheng14†, jia-wei low12†, lidong bing1, luo si11damo academy, alibaba group 2nanyang technological university.
3national university of singapore.
4singapore university of technology and design.
{ruidan.he,linlin.liu,qingyu.tan}@alibaba-inc.com{bosheng.ding,liying.cheng,jiawei.low}@alibaba-inc.comyeh@comp.nus.edu.sg {l.bing,luo.si}@alibaba-inc.com.
abstract.
adapter-based tuning has recently arisen as analternative to ﬁne-tuning.
it works by addinglight-weight adapter modules to a pretrainedlanguage model (prlm) and only updating theparameters of adapter modules when learningon a downstream task.
as such, it adds only afew trainable parameters per new task, allow-ing a high degree of parameter sharing.
priorstudies have shown that adapter-based tun-ing often achieves comparable results to ﬁne-tuning.
however, existing work only focuseson the parameter-efﬁcient aspect of adapter-based tuning while lacking further investiga-tion on its effectiveness.
in this paper, westudy the latter.
we ﬁrst show that adapter-based tuning better mitigates forgetting issuesthan ﬁne-tuning since it yields representationswith less deviation from those generated bythe initial prlm.
we then empirically com-pare the two tuning methods on several down-stream nlp tasks and settings.
we demon-strate that 1) adapter-based tuning outperformsﬁne-tuning on low-resource and cross-lingualtasks; 2) it is more robust to overﬁtting and lesssensitive to changes in learning rates..1.introduction.
large scale pretrained language models (prlms)(devlin et al., 2019; liu et al., 2019; conneau et al.,2020a; brown et al., 2020) have achieved state-of-the-art results on most natural language processing(nlp) tasks, where ﬁne-tuning has become a dom-inant approach to utilize prlms.
a standard ﬁne-tuning process copies weights from a prlm andtunes them on a downstream task, which requires anew set of weights for each task..adapter-based tuning (houlsby et al., 2019;bapna and firat, 2019) has been proposed as a.
∗∗ equally contributed†† linlin, qingyu, bosheng, liying, and jia-wei are underthe joint phd program between alibaba and their correspond-ing universities..more parameter-efﬁcient alternative.
for nlp,adapters are usually light-weight modules insertedbetween transformer layers (vaswani et al., 2017).
during model tuning on a downstream task, onlythe parameters of adapters are updated while theweights of the original prlm are frozen.
hence,adapter-based tuning adds only a small amountof parameters for each task, allowing a high de-gree of parameter-sharing.
though using muchless trainable parameters, adapter-based tuning hasdemonstrated comparable performance with fullprlm ﬁne-tuning (houlsby et al., 2019; bapna andfirat, 2019; stickland and murray, 2019)..existing work mostly focuses on the parameter-efﬁcient aspect of adapters and attempt to deriveuseful applications from that, which is still thecase in most recent works: r¨uckl´e et al.
(2020)explore methods to further improve the parame-ter and computation efﬁciency of adapters; pfeif-fer et al.
(2020a) combine knowledge from multi-ple adapters to improve the performance on down-stream tasks; artetxe et al.
(2020) and pfeifferet al.
(2020c) leverage the modular architectureof adapters for parameter-efﬁcient transfer to newlanguages or tasks, and wang et al.
(2020) utilizethe same property for knowledge injection..besides parameter-efﬁciency, the unique char-acteristic of adapter-based tuning, with alternat-ing frozen and learnable layers, might be directlyuseful for improving model performances.
how-ever, this has not yet been discussed in the priorwork.
in this paper, we ﬁrst empirically demon-strate that adapter-based tuning better regularizestraining than ﬁne-tuning by mitigating the issueof forgetting.
we show that it yields representa-tions with less deviation from those generated bythe original prlm.
next, to see what this prop-erty of adapters will help when adapting prlms,we compare the performance of ﬁne-tuning andadapter-based tuning on a wide range of datasets.
proceedingsofthe59thannualmeetingoftheassociationforcomputationallinguisticsandthe11thinternationaljointconferenceonnaturallanguageprocessing,pages2208–2222august1–6,2021.©2021associationforcomputationallinguistics2208the adapter adoptedfigure 1: the structure offrom houlsby et al.
(2019).
n is the number of trans-former layers..and nlp tasks.
extensive experiments and anal-ysis are conducted in different settings, includinglow-resource and high-resource, monolingual andcross-lingual..our main ﬁndings can be summarized as fol-.
lows:.
• for monolingual adaptation, adapter-basedtuning yields better results in low-resourcesettings, especially when the task is moredomain-speciﬁc.
with increasing trainingsamples,the performance gain over ﬁne-tuning is less signiﬁcant (§3)..• adapter-based tuning tends to outperformﬁne-tuning on zero-shot cross-lingual tasksunder different amounts of training data (§4)..• adapter-based tuning demonstrates higher sta-bility and better generalization ability.
it isless sensitive to learning rates compared toﬁne-tuning (§5)..figure 2: comparison of the representations obtainedat each layer before (base) and after adapter-basedtuning or ﬁne-tuning on bert-base using representa-tional similarity analysis (rsa).
5000 tokens are ran-domly sampled from the dev set for computing rsa.
a higher score indicates that the representation spacesbefore and after tuning are more similar..model with only a few trainable parameters addedper task..houlsby et al.
(2019) have extensively studiedthe choices of adapter architectures and where theyshould be inserted into prlms.
they ﬁnd that astack of down- and up-scale neural networks workswell which only introduces a small amount of extraparameters to the network.
this design inspiresmost of the following work (pfeiffer et al., 2020a,c;bapna and firat, 2019).
as shown in figure 1,the adapter maps an input hidden vector h fromdimension d to dimension m where m < d, andthen re-maps it to dimension d. we refer m asthe hidden size of the adapter.
a skip-connectionis employed inside the adapter network such thatif the parameters of the projection layers are nearzeros, the adapter module approximates an identityfunction.
formally, given the input hidden vectorh, the output vector h(cid:48) is calculated as:.
2 adapter better regularizes tuning.
h(cid:48) = f2(tanh f1(h)) + h.(1).
2.1 adapter-based tuning.
a.adapting.
pretrained.
languagewhenmodel(prlm), adapter-based tuning insertslight-weight neural networks (adapters) betweenthe transformer layers of the prlm, and onlyupdates the parameters of the adapters on a down-stream task, but keeps the ones of the prlm frozen.
unlike ﬁne-tuning which introduces an entirenew model for every task, one great advantageof adapter-based tuning is generating a compact.
in which f1(·) and f2(·) are the down- and up-projection layers.
at each transformer layer, twoadapters are inserted right after the self-attentionand the feed-forward layers respectively.
duringadapter tuning, only the parameters of the adapters,the normalization layers, and the ﬁnal classiﬁca-tion layer are updated.
we use the above describedadapter conﬁguration in all of our experiments,since it is adopted in most prior work with fewmodiﬁcations..2209self-attentionfeed-forwardadapterfeed-forwardadapter++layer norm+transformer layeradapterlayer norm24681012bert layer0.00.20.40.60.81.0rsa similarityrepresentation space comparison (sst-2)fine-tune vs baseadapter vs base2.2 representation similarity.
fine-tuning large-scale prlms on downstreamtasks can suffer from overﬁtting and bad gener-alization issues (dodge et al., 2020; phang et al.,2018).
recently, lee et al.
(2020) propose mixoutto regularize the ﬁne-tuning of prlms.
they showthat mixout avoids catastrophic forgetting and sta-bilizes the ﬁne-tuning process by encouraging theweights of the updated model to stay close to theinitial weights.
since adapter-based tuning doesnot update the weights of prlms at all, we suspectthat it has a similar effect of alleviating the issueof catastrophic forgetting.
since the weights of theprlm are the same before and after adapter-basedtuning, to verify this, we use representational sim-ilarity analysis (rsa) (laakso and cottrell, 2000)to assess the similarity of tuned representations tothose without tuning at each transformer layer..rsa has been widely used to analyze the simi-larity between two neural network outputs (abnaret al., 2019; chrupała and alishahi, 2019; mer-chant et al., 2020), which works by creating twocomparable sets of representations by inputting asame set of n samples to the two models.
for eachset of representations, a n × n pairwise similarity1matrix is calculated.
the ﬁnal rsa similarity scorebetween the two representation space is computedas the pearson correlation between the ﬂattened up-per triangulars of the two similarity matrices.
weuse a subset of glue tasks (wang et al., 2018)for our analysis.
given a task, we ﬁrst performadapter-based tuning and ﬁne-tuning to adapt abert-base model (morg) to the target task, whichyields models madapt and mf t respectively (seeappendix a.2 for training details).
then we passsentences (or sentence-pairs depend on the task)from the development set to morg, madapt, andmf t respectively.
we extract representations ateach layer from the three models and select thecorresponding representations of 5k randomly sam-pled tokens2 (n = 5000) for evaluation.
notethat the same set of tokens is used for all mod-els.
finally, we compare the representations ob-tained from madapt or mf t to those from morgusing rsa..figure 2 plots the results on sts-2, results ofother tasks demonstrate a similar trend and can befound in appendix a.3.
for both ﬁne-tuning andadapter-based tuning, we observe that the repre-.
1cosine similarity is used2we skip [pad], [cls], [sep] for token selection..sentation change generally arises in the top lay-ers of the network, which is consistent with previ-ous ﬁndings that higher layers are more task rele-vant (howard and ruder, 2018).
it can be clearlyobserved that compared to ﬁne-tuning, adapter-based tuning yields representations with less devia-tion from those of bert-base at each layer, whichveriﬁes our claim that adapter-based tuning canbetter regularize the tuning process by mitigatingthe forgetting problem.
apparently, this propertyof adapter tuning comes from that it freezes allthe parameters of prlms.
and because of the skip-connection in the adapter, the hidden representationout of the adapter can mimic the input representa-tion, in this way, some of the original knowledgeof prlms (before injecting adapters) can be pre-served..since we ﬁnd that adapter-based tuning betterregularizes the learning process, the next questionis how this property will help to improve the per-formance when adapting prlms to downstreamtasks.
we conduct extensive experiments to investi-gate this.
the remainder of this paper is organizedas follows.
we compare ﬁne-tuning and adapter-based tuning on monolingual text-level adaptationtasks in §3, followed by cross-lingual adaptation in§4.
further analysis about the training stability andgeneralization capabilities is shown in §5..3 monolingual adaptation.
in this section, we ﬁrst experiment with eightdatasets as used in gururangan et al.
(2020) in-cluding both high- and low-resource tasks (§3.1).
we refer this set of tasks as task adaptation eval-uation (tae).
we observe that adapter-based tun-ing consistently outperforms ﬁne-tuning on low-resource tasks, while they perform similarly onhigh-resource tasks.
we further conﬁrm the effec-tiveness of adapters in low-resource settings on theglue benchmark (wang et al., 2018) (§3.2)..3.1 tae.
tae consists of four domains (biomedical, com-puter science, news text, and amazon reviews)and eight classiﬁcation tasks (two in each domain),whose domain diversity makes it suitable to as-sess the adaptation effectiveness of different ap-proaches.
detailed data statistics are displayed inappendix a.1.
we consider tasks with fewer than5k training examples as low-resource tasks and theothers as high-resource tasks..2210model.
roba.-ft†roba.-ft∗roba.-adapter256roba.-ft+tapt†roba.-ft+tapt∗roba.-adapter256+tapt.
low-resource.
high-resource.
chemprot acl-arc scierc hyp.
(515).
(4169).
(1688).
(3219).
rct agnews helpful.
(180k).
(115k).
(115k).
81.91.081.70.882.90.6.
82.60.482.50.383.50.5.
63.05.865.03.667.54.3.
67.41.866.55.170.02.1.
77.31.978.51.880.80.7.
79.31.579.70.881.10.2.
86.60.988.93.390.44.2.
90.45.291.30.890.03.5.
87.20.187.00.187.10.1.
87.70.487.40.187.20.1.
93.90.293.70.293.80.1.
94.50.194.00.294.00.1.
65.13.469.10.669.00.4.
68.51.970.31.168.80.8.imdb(20k).
95.00.295.20.195.70.1.
95.50.195.40.195.80.0.table 1: average results across ﬁve random seeds with standard deviations as subscripts on tae.
micro-f1 isreported for chemproot and rct, and macro-f1 is reported for the other tasks.
results with “†” are takenfrom gururangan et al.
(2020).
results with “*” are reproduced by us.
numbers in () indicate the training size..experimental setup we perform supervisedﬁne-tuning on roberta-base as our baseline(roba.-ft).
for adapter-based tuning, we setthe hidden size m of adapters to 256 (roba.-adapter256).
we also present the results of addingtask-adaptive pretraining (+tapt) (gururanganin this setting, before ﬁne-tuninget al., 2020).
or adapter-based tuning, the model was trainedwith a masked language modeling (mlm) objec-tive on the training texts (without labels) of thetask.
note that in roba.-adapter256+tapt, wealso use adapter-based tuning for tapt where onlythe weights of adapters are updated at the taptstage.
this is to evaluate whether adapter-basedtuning can work with unsupervised learning ob-jectives.
we follow the experimental settings ingururangan et al.
(2020) for tapt.
for ﬁne-tuningand adapter-based tuning, we train models for 20epochs to make sure they are sufﬁciently trainedand save the checkpoint after each training epoch.
we select the checkpoint that achieves the bestscore on the validation set for evaluation on the testset.
the batch size is set to 16 for both methods.
the learning rate is set to 2e-5 for ﬁne-tuning, and1e-4 for adapter-based tuning.
see appendix a.2for the hyperparameter selection process and moretraining details..results table 1 presents the comparison results.
we report the average result over 5 runs with dif-ferent random seeds.
on four low-resource tasks,adapter-based tuning consistently outperforms ﬁne-tuning and improves the average result by 1.9%.
adapter-based tuning alone without tapt even out-performs ﬁne-tuning with tapt.
besides, addingtapt before adapter-based tuning further improvesthe performance on 3 out of 4 low-resource tasks,which suggests that adapter-based tuning workswith both supervised and unsupervised objectives..figure 3: test performance w.r.t the number of train-ing examples.
reported results are averages across ﬁveruns with different random seeds..another ﬁnding is that when trained on high-resource tasks, both methods achieve similar re-sults.
to verify the effects of training size, onhigh-resource tasks, we plot the performances withvarying numbers of training examples in figure 3.the trend is consistent with our existing observa-tions – adapter-based tuning achieves better resultswhen the training set is small while ﬁne-tuning willgradually catch up with an increasing number oftraining examples..3.2 glue low-resource adaptation.
to further validate that adapters tend to general-ize better than ﬁne-tuning under low-resource set-tings, we follow zhang et al.
(2021) to study low-resource adaptation using eight datasets from theglue benchmark (wang et al., 2018) which cov-ers four types of tasks: natural language inference(mnli, qnli, rte), paraphrase detection (mrpc,qqp), sentiment classiﬁcation (sst-2) and linguis-tic acceptability (cola).
appendix a.1 providesdetailed data statistics and descriptions..experimental setup for each dataset, we sim-ulate two low-resource settings by randomly sam-pling 1k and 5k instances from the original training.
22112k4k8k16k32k64kall# of training samples0.820.830.840.850.860.87test performancerctfine-tuneadapter2k4k8k16k32k64kall# of training samples0.890.900.910.920.930.94agnewsfine-tuneadaptermodel.
cola mnlim mnlimm mrpc qnli qqp.
rte.
sst-2.
sts-b.
avg..1kbert-ftbert-adapter64bert-adapter64−256.
roba.-ftroba.-adapter64roba.-adapter64−256.
5kbert-ftbert-adapter64bert-adapter64−256.
roba.-ftroba.-adapter64roba.-adapter64−256.
41.44.042.92.643.62.9.
45.42.847.72.547.72.5.
54.42.454.11.554.11.5.
55.71.756.81.257.41.6.
57.43.261.60.961.60.9.
71.20.971.00.871.80.8.
69.60.871.30.571.30.5.
79.50.480.20.380.20.3.
60.33.264.10.864.10.8.
72.90.971.90.873.01.1.
71.21.173.00.473.20.4.
80.30.480.60.280.50.2.
83.61.284.80.784.80.7.
88.40.788.90.989.20.7.
---.
---.
80.50.380.50.981.00.2.
84.00.783.20.583.50.4.
85.00.785.30.385.30.3.
87.10.586.50.786.90.6.
69.80.770.32.076.80.7.
75.01.174.70.375.10.1.
74.71.874.21.374.90.4.
78.11.378.21.078.30.9.
62.51.162.51.365.32.0.
67.02.767.72.268.70.8.
---.
---.
87.80.488.00.788.00.7.
89.00.890.01.490.50.2.
88.61.089.10.289.10.2.
91.40.592.20.592.20.5.
85.50.986.10.386.30.2.
88.50.488.40.288.60.2.
88.70.788.90.188.90.1.
90.60.190.40.290.40.2.
69.91.771.21.172.41.0.
75.71.276.01.176.40.8.
76.01.276.60.676.70.5.
80.40.780.70.680.80.6.table 2: results on glue 1k and 5k low resource settings as described in §3.2.
results of mrpc and rte in 5ksetting are omitted as their training data is less than 5k.
cola is evaluated using matthew’s correlation.
mrpcand qqp are evaluated using f1 score.
sts-b is evaluated using spearman’s correlation.
the other tasks areevaluated using accuracy.
we report averages across ﬁve random seeds, with standard deviations as subscripts..data as the new training sets.
in each setting, wedraw another 1k samples from the remaining train-ing set as the validation set and instead use theoriginal validation set as the test set, since the orig-inal glue test sets are not publicly available 3..we perform ﬁne-tuning on bert-base (bert-ft) and roberta-base (roba.-ft) respectively asour baselines.
we set the learning rate to 2e-5 andthe batch size to 16 for bert and roberta ﬁne-tuning experiments (see appendix a.2 for details).
for adapters, we only tune its hidden sizes in {64,128, 256}, setting the learning rate to 1e-4 andbatch size to 16 as the same used in §3.1..results table 2 presents the comparison results.
for adapter-based tuning, we report two results oneach task.
one is obtained with the optimal hid-den size which varies per dataset, and the otheris obtained with the size of 64. we observe thatadapter-based tuning outperforms ﬁne-tuning mostof the time under both 1k and 5k settings.
in partic-ular, the performance gain is more signiﬁcant in 1ksetting, where on average across all tasks, adapter-based tuning outperforms ﬁne-tuning by 2.5% and0.7% on bert and roberta respectively..3.3 discussions.
one consistent observation from § 3.1 and § 3.2is that adapters tend to outperform ﬁne-tuning on.
3users are limited to a maximum of two submissions perday to obtain test results, which is inconvenient for a largenumber of runs.
text-level classiﬁcation tasks when the training setis small, but with more training samples, the ben-eﬁt of adapters is less signiﬁcant.
in low-resourcesetting, ﬁne-tuning has more severe overﬁttingproblem, since it has much more tunable parame-ters compared to adapter-tuning, so adapter-tuningworks better than ﬁne-tuning.
however, in high-resource setting, overﬁtting is not a big issue andmodel capacity counts more.
obviously, the modelcapacity under ﬁne-tuning is larger than that underadapter-tuning since ﬁne-tuning can update muchmore model parameters..when comparing the improvements of adaptertuning over ﬁne-tuning on tasks from tae (§ 3.1)and glue (§ 3.2), we ﬁnd that the improvementis more signiﬁcant on low-resource tasks fromtae – on roberta-base, the average improve-ment brought by adapters is 1.9% across four low-resource tasks from tae, while the average im-provement on glue is 0.7% and 0.4% in 1k and5k settings respectively.
as indicated in gururan-gan et al.
(2020), the tae dataset is more domain-speciﬁc and has less overlap with the corpus usedfor roberta-base pretraining, one intuitive ex-planation for this observation is that ﬁne-tuninghas more severe forgetting and overﬁtting issuesin domain adaptation where the target domain isdissimilar to the source domain in pretraining, thusadapter-based tuning is more preferable in this sce-nario..2212model.
xlmr-ft (hu et al., 2020)xlmr-ft (reproduced)xlmr-adapter256.
postarget distant.
73.1473.6175.20.
64.3464.9068.05.all.
73.8074.2975.82.nertarget distant.
64.8763.3265.95.
58.2156.8559.01.all.
65.4063.8566.40.xnlitarget distant.
78.5678.6479.43.
76.7377.0377.60.all.
79.2479.2880.08.table 3: zero-shot cross-lingual results.
accuracy is reported for pos tagging and xnli.
f1 is reported for ner.
all is the average test result of all languages.
target is the average test result of all target languages except english.
distant is the average test result of the languages not in the indo-european family..5%.
model.
all.
target distant.
xlmr-ftxlmr-adapter64.
75.7676.09.
75.0975.47.
73.1273.78.
10%target distant.
76.0776.94.
74.2175.10.all.
76.7377.52.
20%target distant.
77.6478.07.
75.8476.39.all.
78.2878.68.table 4: accuracy on xnli with different amount of training data.
we only compare xlmr-ft to xlmr-adapter64 in this set of experiments as xlmr-adapter64 is more light-weight..4 cross-lingual adaptation.
in this section, we further compare ﬁne-tuning andadapter-based tuning in the zero-shot cross-lingualtransfer setting.
all experiments in this section arebased on xlm-r-large (conneau et al., 2020a),a recent sota multilingual prlm covering 100languages.
we conduct evaluations on a set ofmultilingual tasks from xtreme (hu et al., 2020),including universal dependencies v2.5 tree banks(ud-pos) (nivre et al., 2018), wikiann ner (panet al., 2017), and cross-lingual natural languageinference (xnli) (conneau et al., 2020b).
ud-pos contains 34 languages, wikiann ner contains40 languages, and xnli contains 15 languages.
werefer the reader to hu et al.
(2020) for additionaldetails about the datasets..experimental setup on each task, we performhyperparameter tuning on the english developmentset.
for both ﬁne-tuning and adapter-based tun-ing, we use batch size 32, and tune the learningrates in {1e-5, 2e-5, 3e-5, 4e-5, 5e-5}.
for adapter-based tuning, we further tune the hidden sizes in{64, 128, 256} and ﬁnd size 256 often performs thebest.
we train and select models with the englishtraining and development sets and then evaluate thetuned models on test sets of all languages.
see ap-pendix a.2 for hyperparameter and training details..results table 3 summarizes the results.
to bettercompare cross-lingual transfer to different groupsof languages, we present the average results ofall languages (all), the target languages exceptenglish (target), and the non-indo-european lan-guages (distant).
it can be observed that adapter-based tuning signiﬁcantly outperforms ﬁne-tuning.
model.
taelow glue1k xnlif ull xnli5%.
ﬁnetuneadapter64adapter128adapter256.
78.5277.2079.2980.41.
69.8671.2071.0971.06.
78.6479.0179.2479.43.
75.0975.4775.8375.45.table 5: average test results with different adapter hid-den sizes.
results of glue1k are based on bert-base.
taelow denotes low resource tasks from tae..on all three settings for each task.
speciﬁcally,adapter-based tuning outperforms the reported ﬁne-tuning results (hu et al., 2020) on target and dis-tant by 2.06% and 3.71% on ud-pos, 1.08% and0.8% on wikiann ner, and 0.87% and 0.87% onxnli.
see appendix a.3 for detailed results oneach language..note that ud-pos, wikiann ner, and xnliare all high-resource tasks, with 20k, 20k, and400k training samples respectively.
unlike mono-lingual tasks, adapters achieve consistent perfor-mance gains even under high-resource settings oncross-lingual tasks.
we suspect that the ability tomitigate forgetting is more useful in cross-lingualscenarios since the model knowledge of the targetlanguages only comes from pretraining.
adapter-based tuning can better maintain the knowledge.
we further investigate the effectiveness of adapter-based tuning on xnli with smaller training sets.
table 4 summarizes the results when trained on5%, 10%, and 20% of the original training sets.
inall settings, adapters still demonstrate consistentimprovements over ﬁne-tuning..2213figure 4: box plots of test performance distribution over 20 runs across different learning rates.
the upper/bottomresults are based on bert-base/robereta-base.
note that the ﬁne-tuning results with learning rates larger than4e-5 on roberta.
mnli 5k are all zeros, which are outside of the range and not shown in the subplot..5 analysis.
adapter hidden size the hidden size m4 is theonly adapter-speciﬁc hyperparameter.
as indicatedin houlsby et al.
(2019), the hidden size providesa simple means to trade off performance with pa-rameter efﬁciency.
table 5 shows the performancewith different hidden sizes, from which we ﬁnd thatincreasing the hidden size may not always lead toperformance gains.
for monolingual low-resourceadaptation, tae tasks prefer a larger hidden size,while the results on glue are similar across differ-ent hidden sizes.
we suspect that this is due to thattae datasets are more dissimilar to the pretrainingcorpus, which requires relatively more trainableparameters to learn the domain-speciﬁc knowledge.
on xnli, a larger hidden size helps improve theperformance when the full data is used.
however,when only 5% training data is used, increasing thehidden size does not yield consistent improvements.
the results indicate that the optimal hidden size de-pends on both the domain and the training size ofthe task..learning rate robustness we compare the twotuning methods in terms of their stability w.r.t thelearning rate.
figure 4 shows the performance dis-tributions on cola and mnli under 1k and 5ksettings.
the learning rates are varied in {2e-5, 4e-5, 6e-5, 8e-5, 1e-4}.
each box in the plot is drawnfrom the results of 20 runs with different randomseeds.
we observe that ﬁne-tuning yields largervariances when increasing the learning rates.
itoften collapses with learning rates larger than 4e-5.
4the fraction of adapter parameters w.r.t.
bert-base(110m parameters) is 2%, 4%, and 6% when m is set to64, 128, and 256. the fraction w.r.t.
xlmr-large (550mparameters) is 1%, 2%, and 3%, respectively..figure 5: loss on the dev set w.r.t training steps.
re-sults are based on bert-base.
the original trainingand dev sets from glue are used for this analysis..eval acc..colamrpcqnlisst-2.
mean (best).
fine-tune54.27 (61.99)84.53 (87.50)89.39 (90.63)90.21 (92.66).
adapter58.27 (62.07)85.28 (87.25)90.41 (91.16)91.01 (92.20).
table 6: mean (best) results on the dev set across allevaluation steps..when roberta-base is used.
adapter-based tun-ing is more stable across a wider range of learningrates..overﬁtting and generalization here, we ﬁrststudy the robustness of adapter-based tuning tooverﬁtting.
we use cola, mrpc, qnli, and sst-2 with their original training and development setsfor our analysis.
the cola and mrpc contain8.5k and 3.7k training samples and are regardedas low-resource tasks.
the qnli and sst-2 con-.
22140.00.10.20.30.4acc.cola (1k)fine-tuneadapter0.350.400.450.500.550.600.65mnli (1k)0.00.10.20.30.40.50.6acc.cola (5k)0.400.450.500.550.600.650.700.75mnli (5k)2e-54e-56e-58e-51e-4learning rate0.00.10.20.30.40.5acc.cola (1k)2e-54e-56e-58e-51e-4learning rate0.40.50.60.7mnli (1k)2e-54e-56e-58e-51e-4learning rate0.00.10.20.30.40.50.6acc.cola (5k)2e-54e-56e-58e-51e-4learning rate0.740.760.780.80mnli (5k)0.51.01.5eval.
losscolafine-tuneadapter10k steps0.51.0eval.
lossmrpc0.40.60.81.0eval.
lossqnli60k steps0.40.60.8eval.
losssst-2model.
cola mrpc qnli sst-2.
ﬁnetuneﬁnetune-mixoutadapter64adapter64-mixout.
41.3942.3542.9342.52.
83.5684.0084.7983.80.
80.5180.0380.5480.67.
87.8487.7188.0287.66.table 7: comparison with mixout.
results are basedon bert-base under 1k settiing.
average results across5 random seeds are reported..efﬁcient, when would adapter-based tuning bemore effective than ﬁne-tuning for prlm adapta-tion?
thus, we only use ﬁne-tuning as our primarybaseline in previous sections.
here, for the sakeof curiosity, we further compare adapter-based tun-ing to ﬁne-tuning regularized by mixout (lee et al.,2020) on a subset of glue tasks, since mixoutsimilarly regularizes the learning process by miti-gating the forgetting issue.
speciﬁcally, it replacesall outgoing parameters from a randomly selectedneuron to the corresponding parameters of the ini-tial model without tuning, such that it reduces di-vergence from the initial model.
following the sug-gestions in the paper, we conduct experiments byreplacing all dropout modules in the network withmixout and set the mixout probability to 0.9. fromthe results in table 7, we ﬁnd that using adapter-based tuning alone yields the best results in mostcases.
applying mixout to ﬁne-tuning improves theperformance on cola and mrpc only.
however,applying it to adapters instead tends to degrade theperformance.
we suspect that this is because thenumber of trainable parameters of adapters is veryfew to begin with.
hence, further replacing a largepercentage of them with their initial weights mayweaken the learning ability..6 related work.
fine-tuning pretrained large scale language mod-els has proven its effectiveness on a wide range ofnlp tasks (devlin et al., 2019; liu et al., 2019;conneau et al., 2020a; brown et al., 2020).
how-ever, ﬁne-tuning requires a new set of weights foreach task, which is parameter inefﬁcient.
adapter-based tuning is proposed to deal with this prob-lem (houlsby et al., 2019).
most previous workhas demonstrated that it achieves comparable per-formance to ﬁne-tuning (bapna and firat, 2019;pfeiffer et al., 2020b,a,c; r¨uckl´e et al., 2020; wanget al., 2020; guo et al., 2020).
however, exist-ing work mostly focuses on the parameter-efﬁcientaspect while overlooks the effectiveness..figure 6: loss landscapes.
bert-base is used..tain 104k and 67k training samples and are used ashigh-resource tasks.
we train the two low-resourcetasks for 10k steps, and the high resource tasks for60k steps with a batch size of 16. we use bert-base for all experiments.
figure 5 plots the losscurves on dev sets w.r.t training steps.
we observethat models with ﬁne-tuning can easily overﬁt onboth low- and high-resource tasks.
adapter-basedtuning is more robust to overﬁtting.
additional re-sults on accuracy w.r.t.
training steps and a similaranalysis on xnli are in appendix a.3..we also present the mean and best dev resultsacross all evaluation steps in table 6, where weperform an evaluation step every 20 training steps.
the mean results of adapter-based tuning consis-tently outperform those of ﬁne-tuning.
the differ-ences between the mean and the best values arealso smaller with adapter-based tuning.
the resultssuggest that the performance of adapters is morestable over ﬁne-tuning along the training process.
training neural networks can be viewed assearching for a good minima in the non-convexlandscape deﬁned by the loss function.
priorwork (hochreiter and schmidhuber, 1997; li et al.,2018) shows that the ﬂatness of a local minima cor-relates with the generalization capability.
thus, wefurther show the loss landscapes of the two tuningmethods.
following hao et al.
(2019), we plot theloss curve by linear interpolation between θ0 andθ1 with function f (α) = l(θ0 + α · (θ1 − θ0)),where θ0 and θ1 denote the model weights beforeand after tuning.
l(θ) is the loss function and αis a scalar parameter.
in our experiments, we setthe range of α to [−2, 2] and uniformly sample 20points.
figure 6 shows the loss landscape curveson cola and sst based on bert-base.
it showsthat the minimas of adapter-based tuning are morewide and ﬂat, which indicates that adapter-basedtuning tends to generalize better..compare to mixout the focus of this paper isto answer the question – besides being parameter-.
221521012012345eval.
losscolafine-tuneadapter01234521012012345sst-2012345eval.
lossfine-tuning prlms in a low-resource setting hasbeen studied for a while (dodge et al., 2020; leeet al., 2020; phang et al., 2018; jiang et al., 2020;zhang et al., 2021).
previous work points out thatwith large-scale parameters, ﬁne-tuning on a fewsamples can lead to overﬁtting and bad general-ization, which causes the results unstable.
phanget al.
(2018) ﬁnd that pretraining on an intermedi-ate task can improve ﬁne-tuning outcomes.
jianget al.
(2020) improve the robustness of ﬁne-tuningby controlling the model complexity and prevent-ing aggressive updating.
on the other hand, catas-trophic forgetting can appear when transferringa pretrained neural networks (french, 1999; mc-closkey and cohen, 1989; goodfellow et al., 2013),where the learned knowledge from pretraining islost when adapting to downstream tasks.
this phe-nomenon often appears in nlp tasks (mou et al.,2016; arora et al., 2019).
to relieve this problem ofadapting pretrained language models, howard andruder (2018) gradually unfreeze the layers start-ing from the last layer and sun et al.
(2019) ﬁndassigning lower learning rate to the bottom layerscan improve the performance.
lee et al.
(2020) reg-ularize learning by encouraging the weights of theupdated model to stay close to the initial weights.
aghajanyan et al.
(2021) regularize ﬁne-tuning byintroducing noise to the input which is similar toadversarial training for ﬁne-tuning studied in zhuet al.
(2020).
mosbach et al.
(2021) point out thatthe instability of ﬁne-tuning lies in the optimizerand propose to revise the adam optimizer by re-placing it with a de-bias version.
chen et al.
(2020)propose a mechanism to recall the knowledge frompretraining tasks..7 conclusion.
prior work often focuses on the parameter-efﬁcientaspect while overlooks the effectiveness of adapter-based tuning.
we empirically demonstrate thatadapter-based tuning can better regularize the learn-ing process.
we conduct extensive experiments toverify its effectiveness and conclude that 1) it tendsto outperform ﬁne-tuning on both low-resource andcross-lingual tasks; 2) it demonstrates higher sta-bility under different learning rates compared toﬁne-tuning.
we hope our study will inspire morefuture work on prlm adaptation based on adaptersand other methods that only tune part of the prlmparameters..acknowledgements.
linlin liu would like to thank the support frominterdisciplinary graduate school, nanyang tech-nological university..references.
samira abnar, lisa beinborn, rochelle choenni, andjelle zuidema.
2019. blackbox meets blackbox:representational similarity and stability analysis ofneural language models and brains.
in proceedingsof the acl-workshop on analyzing and interpretingneural networks for nlp..armen aghajanyan, akshat shrivastava, anchit gupta,naman goyal, luke zettlemoyer, and sonal gupta.
2021. better ﬁne-tuning by reducing representa-tional collapse.
in proceedings of iclr..gaurav arora, afshin rahimi, and timothy baldwin.
2019. does an lstm forget more than a cnn?
anempirical study of catastrophic forgetting in nlp.
inproceedings of alta..mikel artetxe, sebastian ruder, and dani yogatama.
2020. on the cross-lingual transferability of mono-lingual representations.
in proceedings of acl..ankur bapna and orhan firat.
2019. simple, scalablein pro-.
adaptation for neural machine translation.
ceedings of emnlp-ijcnlp..tom brown, benjamin mann, nick ryder, melaniesubbiah,jared d kaplan, prafulla dhariwal,arvind neelakantan, pranav shyam, girish sastry,amanda askell, sandhini agarwal, ariel herbert-voss, gretchen krueger, tom henighan, rewonchild, aditya ramesh, daniel ziegler, jeffrey wu,clemens winter, chris hesse, mark chen, ericsigler, mateusz litwin, scott gray, benjamin chess,jack clark, christopher berner, sam mccandlish,alec radford, ilya sutskever, and dario amodei.
2020. language models are few-shot learners.
inadvances in neural information processing sys-tems..sanyuan chen, yutai hou, yiming cui, wanxiang che,ting liu, and xiangzhan yu.
2020. recall and learn:fine-tuning deep pretrained language models withless forgetting.
in proceedings of emnlp..grzegorz chrupała and afra alishahi.
2019. correlat-ing neural and symbolic representations of language.
in proceedings of acl..alexis conneau, kartikay khandelwal, naman goyal,vishrav chaudhary, guillaume wenzek, franciscoguzm´an, edouard grave, myle ott, luke zettle-moyer, and veselin stoyanov.
2020a.
unsupervisedcross-lingual representation learning at scale.
inproceedings of acl..2216alexis conneau, ruty rinott, guillaume lample, hol-ger schwenk, ves stoyanov, adina williams, andsamuel r. bowman.
2020b.
xnli: evaluating cross-lingual sentence representations.
in proceedings ofemnlp..jacob devlin, ming-wei chang, kenton lee, andkristina toutanova.
2019. bert: pre-training ofdeep bidirectional transformers for language under-standing.
in proceedings of naacl..jesse dodge, gabriel ilharco, roy schwartz, alifarhadi, hannaneh hajishirzi, and noah a. smith.
2020.fine-tuning pretrained language models:weight initializations, data orders, and early stop-ping.
corr..robert m french.
1999. catastrophic forgetting in con-nectionist networks.
trends in cognitive sciences,3(4):128–135..ian j goodfellow, mehdi mirza, da xiao, aaroncourville, and yoshua bengio.
2013. an empiricalinvestigation of catastrophic forgetting in gradient-based neural networks.
corr..junliang guo, zhirui zhang, linli xu, hao-ran wei,boxing chen, and enhong chen.
2020.incorpo-rating bert into parallel sequence decoding withadapters.
in proceedings of neurips..suchin gururangan, ana marasovi´c,.
swabhaswayamdipta, kyle lo, iz beltagy, doug downey,and noah a. smith.
2020. don’t stop pretraining:adapt language models to domains and tasks.
inproceedings of acl..yaru hao, li dong, furu wei, and ke xu.
2019. visu-alizing and understanding the effectiveness of bert.
in proceedings of the emnlp-ijcnlp..sepp hochreiter and j¨urgen schmidhuber.
1997. flat.
minima.
neural comput., 9(1):1–42..neil houlsby, andrei giurgiu, stanislaw jastrzebski,bruna morrone, quentin de laroussilhe, andreagesmundo, mona attariyan, and sylvain gelly.
2019. parameter-efﬁcient transfer learning for nlp.
in proceedings of icml..jeremy howard and sebastian ruder.
2018. universallanguage model ﬁne-tuning for text classiﬁcation.
inproceedings of acl..junjie hu, sebastian ruder, aditya siddhant, gra-ham neubig, orhan firat, and melvin johnson.
2020. xtreme: a massively multilingual multi-task benchmark for evaluating cross-lingual general-isation.
in proceedings of icml..haoming jiang, pengcheng he, weizhu chen, xi-aodong liu, jianfeng gao, and tuo zhao.
2020.smart: robust and efﬁcient ﬁne-tuning for pre-trained natural language models through principledregularized optimization.
in proceedings acl..diederik p. kingma and jimmy ba.
2015. adam: amethod for stochastic optimization.
in proceedingsof iclr..aarre laakso and garrison cottrell.
2000. contentand cluster analysis: assessing representational sim-ilarity in neural systems.
philosophical psychology,13(1):47–76..cheolhyoung lee, kyunghyun cho, and wanmo kang.
2020. mixout: effective regularization to ﬁnetunelarge-scale pretrained language models.
in proceed-ings of iclr..hao li, zheng xu, gavin taylor, christoph studer, andtom goldstein.
2018. visualizing the loss landscapeof neural nets.
in proceedings of neurips..yinhan liu, myle ott, naman goyal, jingfei du, man-dar joshi, danqi chen, omer levy, mike lewis,luke zettlemoyer, and veselin stoyanov.
2019.roberta: a robustly optimized bert pretraining ap-proach.
corr..michael mccloskey and neal j cohen.
1989. catas-trophic interference in connectionist networks: thesequential learning problem.
in psychology of learn-ing and motivation, volume 24, pages 109–165.
el-sevier..amil merchant, elahe rahimtoroghi, ellie pavlick,and ian tenney.
2020. what happens to bert em-beddings during ﬁne-tuning?
in proceedings of thethird blackboxnlp workshop on analyzing and in-terpreting neural networks for nlp..marius mosbach, maksym andriushchenko, and diet-rich klakow.
2021. on the stability of ﬁne-tuningbert: misconceptions, explanations, and strongbaselines.
in proceedings of iclr..lili mou, zhao meng, rui yan, ge li, yan xu,lu zhang, and zhi jin.
2016. how transferable areneural networks in nlp applications?
in proceed-ings of emnlp..joakim nivre, rogier blokland, niko partanen, andmichael rießler.
2018. universal dependencies 2.2..xiaoman pan, boliang zhang, jonathan may, joelnothman, kevin knight, and heng ji.
2017. cross-lingual name tagging and linking for 282 languages.
in proceedings of acl..jonas pfeiffer, aishwarya kamath, andreas r¨uckl´e,kyunghyun cho,and iryna gurevych.
2020a.
adapterfusion: non-destructive task compositionfor transfer learning.
corr..jonas pfeiffer, andreas r¨uckl´e, clifton poth, aish-warya kamath,ivan vulic, sebastian ruder,kyunghyun cho, and iryna gurevych.
2020b.
adapterhub: a framework for adapting transform-ers.
in proceedings of emnlp: system demonstra-tions..2217jonas pfeiffer, ivan vuli´c, iryna gurevych, and sebas-tian ruder.
2020c.
mad-x: an adapter-based frame-work for multi-task cross-lingual transfer.
corr..jason phang, thibault f´evry, and samuel r. bowman.
2018. sentence encoders on stilts: supplementarytraining on intermediate labeled-data tasks.
corr..andreas r¨uckl´e, gregor geigle, max glockner,tilman beck, jonas pfeiffer, nils reimers, and irynagurevych.
2020. adapterdrop: on the efﬁciency ofadapters in transformers.
corr..asa cooper stickland and iain murray.
2019. bertand pals: projected attention layers for efﬁcientadaptation in multi-task learning.
in proceedings oficml..chi sun, xipeng qiu, yige xu, and xuanjing huang.
2019. how to ﬁne-tune bert for text classiﬁcation?
in proceedings of ccl..ashish vaswani, noam shazeer, niki parmar, jakobuszkoreit, llion jones, aidan n gomez, ł ukaszkaiser, and illia polosukhin.
2017. attention is allyou need.
in proceedings of neurips..alex wang, amanpreet singh, julian michael, fe-lix hill, omer levy, and samuel bowman.
2018.glue: a multi-task benchmark and analysis plat-in pro-form for natural language understanding.
ceedings ofthe 2018 emnlp workshop black-boxnlp: analyzing and interpreting neural net-works for nlp..ruize wang, duyu tang, nan duan, zhongyu wei, xu-anjing huang, jianshu ji, guihong cao, daxin jiang,and ming zhou.
2020. k-adapter: infusing knowl-edge into pre-trained models with adapters.
corr..thomas wolf, lysandre debut, victor sanh, julienchaumond, clement delangue, anthony moi, pier-ric cistac, tim rault, r´emi louf, morgan funtow-icz, and jamie brew.
2019. huggingface’s trans-formers: state-of-the-art natural language process-ing.
corr..tianyi zhang, felix wu, arzoo katiyar, kilian q.weinberger, and yoav artzi.
2021. revisiting few-sample bert ﬁne-tuning.
in proceedings of iclr..chen zhu, yu cheng, zhe gan, siqi sun, tom gold-stein, and jingjing liu.
2020. freelb: enhanced ad-versarial training for natural language understanding.
in proceedings of iclr..2218a appendix.
a.1 datasets.
tae table 8 presents the data statistics of thetae datasets we used in § 3.1..glue table 9 presents the statistics and descrip-tions of glue tasks.
in § 3.2, to investigate the ef-fectiveness in low-resource scenarios, we simulatetwo low-resource settings by randomly sampling1k and 5k examples respectively from each of theoriginal training set as the new training sets.
ineach setting, we draw 1k samples from the remain-ing training set as our validation set and use theoriginal validation set as held-out test set since theoriginal glue test sets are not publicly available.
for the rsa analysis in § 2 and the analysis ofoverﬁtting and generalization in § 5, we use theoriginal training and development sets for analysispurpose, as this better reveals the behaviors underboth high- and low- resource settings..a.2 experimental details.
implementation we use language model imple-mentations from huggingface transfromers li-brary (wolf et al., 2019).
our adapter implemen-tation is also based on that.
following standardpractice (devlin et al., 2019), we pass the ﬁnallayer [cls] token representation to a task-speciﬁcfeedforward layer for prediction on downstreamtasks.
each experiment was performed on a singlev100 gpu.
we use the adam optimizer (kingmaand ba, 2015) with a linear learning rate scheduler..training details on tae and glue for bothﬁne-tuning and adapter-based tuning, we train mod-els for a ﬁxed number of epochs, and select modelswith the best validation performances on epoch endfor evaluation..for ﬁne-tuning, on tae we follow the learningrate and batch size as suggested by houlsby et al.
(2019).
on glue, we tune learning rates in {1e-5,2e-5, 3e-5, 4e-5, 5e-5} and batch sizes in {16, 32}to select the best conﬁguration across tasks..for adapters, on tae, we set the batch size thesame as used in ﬁne-tuning, and tune learning ratesin {2e-5, 5e-5,1e-4, 2e-4} and adapter’s hidden sizein {64, 128, 256} to select the best conﬁgurationacross all tasks.
on glue, we keep the learningrate and batch size the same as used in tae, andtune the adapter’s hidden sizes in {64, 128, 256}for each task.
we use the same hyperparameter.
setting for all our analysis experiments with gluetasks as well..table 10 presents the detailed hyperparameter.
settings for tae and glue..training details on xtreme tasks for ud-pos, wikiann ner, and xnli, we use batch size32, and tune learning rates in {1e-5, 2e-5, 3e-5,4e-5, 5e-5} on each task.
we tune the adapter’s hid-den sizes in {64, 128, 256} to select the best valueacross all tasks.
we use the english training anddevelopment sets of each task for hyperparametertuning.
table 11 presents the detailed settings..a.3 additional results.
rsa figure 7 presents additional representa-tional similarity analysis (rsa) plots on threeglue tasks as mentioned in § 2. we further con-duct rsa to show the deviation of representationspace before and after tuning (with english train-ing set) on three distant languages (zh, ja, th) fromthe cross-lingual ner task.
figure 8 presents theresults..accuracy w.r.t training steps figure 9 showsthe change of accuracy with increasing trainingsteps on four glue tasks.
the results again in-dicate that adapter-based tuning is more robust tooverﬁtting..overﬁtting analysis on xnli we train xlmr-large with 10% of the original english training dataof xnli, and plot the average loss and accuracycurves on development sets across all target lan-guages except english in figure 10. the plotsdemonstrate similar trends as shown in the plots ofglue tasks (figure 5 and figure 9), where modelswith ﬁne-tuning are easily overﬁtted and adapter-based tuning is more robust to overﬁtting..detailed cross-lingual results table 12 and ta-ble 13 presents the cross-lingual pos tagging re-sults and the cross-lingual ner results on eachlanguage respectively.
table 14 presents detailedresults on xnli when trained with full data.
15presents detailed xnli results when trained on 5%,10%, and 20% of training data..2219figure 7: comparison of the representations obtained at each layer before (base) and after adapter-based tuning orﬁne-tuning on bert-base using representational similarity analysis (rsa).
the original training and dev sets ofcola, mrpc, and mnli are used for this analysis.
5000 tokens are randomly sampled from the dev set of eachtask for computing rsa.
a higher score indicates that the representation spaces before and after tuning are moresimilar..figure 8: comparison of the representations obtained at each of the top 12 layers (layer 13-24) before (base) andafter adapter-based tuning or ﬁne-tuning on xlmr-large using representational similarity analysis (rsa).
weshow results on 3 distant languages from the wikiann ner task.
5000 tokens are randomly sampled from the devset of each language for computing rsa.
a higher score indicates that the representation spaces before and aftertuning are more similar..figure 9: accuracy on the dev set w.r.t training steps.
results are based on bert-base.
the original training anddev sets from glue are used for this analysis.
we can observe that for both high resource (qnli and sst-2) andlow-resource (cola and mrpc) tasks, adapter-based tuning is more robust to overﬁtting..figure 10: change of the average dev loss (left) and accuracy (right) across all target languages of xnli exceptenglish with increasing training steps.
the results are obtained when trained on 10% of the xnli training data..222024681012bert layer0.20.40.60.81.0rsa similarityrepresentation space comparison (cola)fine-tune vs baseadapter vs base24681012bert layer0.50.60.70.80.91.0rsa similarityrepresentation space comparison (mrpc)fine-tune vs baseadapter vs base24681012bert layer0.20.40.60.81.0rsa similarityrepresentation space comparison (mnli)fine-tune vs baseadapter vs base141618202224xlmr layer0.10.20.30.40.50.60.70.8rsa similarityrepresentation space comparison (ner (zh))fine-tune vs baseadapter vs base141618202224xlmr layer0.20.30.40.50.60.70.8rsa similarityrepresentation space comparison (ner (ja))fine-tune vs baseadapter vs base141618202224xlmr layer0.10.20.30.40.50.6rsa similarityrepresentation space comparison (ner (th))fine-tune vs baseadapter vs base10k steps0.20.30.40.50.6eval.
resultcolafine-tuneadapter10k steps0.700.750.800.85mrpcfine-tuneadapter60k steps0.800.850.90qnlifine-tuneadapter60k steps0.860.880.900.92sst-2fine-tuneadapter02000400060008000100001200014000# of training steps0.81.01.21.41.61.82.02.2eval.
lossfine-tuneadapter02000400060008000100001200014000# of training steps0.350.400.450.500.550.600.650.70eval.
accuracyfine-tuneadapterdomain.
task.
label type.
# train.
# dev.
# test.
# class.
chemprotrct.
acl-arcscierc.
biomed.
cs.
news.
relation classiﬁcationabstract sent.
roles.
4169180040.
242730212.
346930135.citation intentrelation classiﬁcation.
16883219.
515115000.
11525120000.
114455.
655000.
50005000.
139974.
657600.
2500025000.
135.
67.
24.
22.hyperpartisan partisanshipagnews.
topic.
reviews.
helpfulnessimdb.
review helpfulnessreview sentiment.
table 8: data statistics of task adaptation evaluation (tae) tasks..task.
description.
# train.
# dev.
# class.
textual entailment classiﬁcation.
cola linguistic acceptability classiﬁcationmnlimrpc paraphrase classiﬁcationqnliqqprtesst-2sts-b semnatic textual similarity (regression).
textual entalment classiﬁcationquora question paris classiﬁcationtextual entailment classiﬁcationsentiment classiﬁcation.
8.5k392k3.7k104k363k2.5k67k5.7k.
10429816/98334095464404k2788731501.
2322222-.
table 9: data statistics of glue tasks..hyperparameter.
ﬁne-tuning.
adapter.
ﬁne-tuning.
adapter.
tae.
glue.
number of epochsbatch sizelearning ratedropoutfeedforward layerfeedforward nonlnearity layerclassiﬁcation layer.
10161e-40.1111.
20162e-50.1111.
20161e-40.1111.
10162e-50.1111.
5325e-50.1111.table 10: hyperparameters for ﬁne-tuning and adapter-based tuning for experiments on tae and glue..hyperparameter.
ﬁne-tune.
number of epochsbatch sizelearning ratedropoutfeedforward layerfeedforward nonlnearity layerclassiﬁcation layer.
5322e-50.1111.posadapter.
ﬁne-tune.
neradapter.
xnli.
ﬁne-tune.
adapter.
5322e-50.1111.
5325e-50.1111.
5321e-50.1111.
5324e-50.1111.table 11: hyperparameters for ﬁne-tuning and adapter-based tuning for experiments on ud-pos, wikiann ner,and xnli..2221en.
af.
ar.
bg.
de.
el.
es.
et.
eu.
fa.
ﬁ.fr.
he.
hi.
hu.
id.
it.
indo-europeanxlmr-ft†xlmr-ft∗xlmr-adapter256.
yes96.1096.1595.89.yes89.8089.2689.30.no67.5069.1270.50.yes88.1088.3388.79.yes88.5088.7988.48.yes86.3087.4286.44.yes88.3088.3488.99.no86.5087.3887.31.no72.5073.7074.84.yes70.6071.0571.94.no85.8086.5685.99.yes87.2087.2488.74.no68.3067.8667.32.yes56.8075.4869.63.no82.6083.4983.11.no72.4072.6773.31.yes89.4089.0790.16.ja.
kk.
ko.
mr.nl.
pt.
ru.
ta.
te.
th.
tl.
tr.
ur.
vi.
yo.
zh.
avg.
indo-europeanxlmr-ft†xlmr-ft∗xlmr-adapter256.
no15.9021.3438.53.no78.1078.8678.47.no53.9053.8453.35.yes80.8085.2486.45.yes89.5089.7589.86.yes87.6087.9888.82.yes89.5089.7590.21.no65.2064.3464.31.no86.6085.6585.38.no47.2043.1255.88.no92.2093.0391.10.no76.3076.6576.21.yes70.3069.4363.46.no56.8058.1059.38.no24.6023.9224.28.no25.7028.6055.76.
-73.8074.2975.82.table 12: zero-shot cross-lingual pos tagging accuracy on the test set of each target language.
results with “†”are taken from (hu et al., 2020).
results with “∗” are reproduced by us..en.
ar.
he.
vi.
id.
jv.
ms.tl.
eu.
ml.
ta.
te.
af.
nl.
de.
el.
bn.
hi.
mr.ur.
indo-europeanxlmr-ft†xlmr-ft∗xlmr-adapter256.
yes84.784.6283.87.no5343.7251.89.no56.854.0856.59.no79.477.1978.02.no5352.2653.53.no62.558.3763.24.no57.169.7862.65.no73.272.2171.57.no60.962.0864.96.no67.865.7868.30.no59.556.9259.57.no55.852.3154.93.yes78.977.6479.43.yes8484.2684.88.yes78.877.9579.38.yes79.577.2380.51.yes78.876.2578.99.yes7371.0173.17.yes68.164.1472.74.no56.454.1572.36.fa.
fr.
it.
pt.
es.
bg.
ru.
ja.
ka.
ko.
sw.yo.
my.
zh.
kk.
tr.
et.
ﬁ.hu.
indo-europeanxlmr-ft†xlmr-ft∗xlmr-adapter256.
yes61.961.1360.39.yes80.579.0781.21.yes81.381.0581.79.yes81.979.6182.61.yes79.668.7676.12.yes81.481.1882.50.yes69.171.4669.76.no23.218.3121.41.no71.668.9370.55.no6057.9961.37.no70.569.9568.90.no33.641.2638.18.no54.351.3260.48.no33.125.8231.11.no56.249.8351.34.no76.178.9481.89.no79.178.0380.36.no79.278.6380.86.no79.879.3282.06.th.
no1.31.472.47.table 13: zero-shot cross-lingual ner f1 on the test set of each language.
results with “†” are taken from (huet al., 2020).
results with “∗” are reproduced by us..modelxlmr-ft†xlmr-ft∗xlmr-adapter256.
en.
ar.
bg.
de.
el.
es.
fr.
hi.
ru.
sw.th.
tr.
ur.
vi.
zh.
88.788.2889.22.
77.278.3478.62.
8382.7383.59.
82.582.0783.47.
80.881.3482.39.
83.783.6384.69.
82.281.9383.27.
75.675.3376.42.
79.179.0479.74.
71.271.5972.21.
77.476.6777.84.
78.078.3678.80.
71.771.8672.27.
79.379.3279.32.
78.278.8079.34.table 14: zero-shot xnli accuracy on the test set of each language when trained with full data.
results with “†”are taken from (hu et al., 2020).
results with “∗” are reproduced by us..model.
en.
ar.
bg.
de.
el.
es.
fr.
hi.
ru.
sw.th.
tr.
ur.
vi.
zh.
5% training dataxlmr-ftxlmr-adapter64.
10% training dataxlmr-ftxlmr-adapter64.
20% training dataxlmr-ftxlmr-adapter64.
85.0984.77.
73.5373.95.
78.778.76.
79.5879.02.
77.2678.08.
80.1380.55.
79.3679.48.
72.0772.01.
76.5276.54.
67.868.76.
72.5373.83.
74.5375.56.
68.368.6.
75.2475.94.
75.7475.50.
85.9685.74.
75.0476.78.
79.7880.27.
79.8280.77.
78.7279.72.
80.9981.87.
80.2581.13.
73.2373.87.
77.2878.42.
68.0869.3.
74.4374.25.
75.777.08.
69.5469.54.
76.0277.30.
76.1676.82.
87.2687.24.
76.4878.00.
81.0781.87.
82.0382.15.
80.4780.47.
82.5582.65.
81.5381.53.
75.0675.00.
78.0478.74.
69.9670.87.
76.0075.94.
77.3678.44.
70.7570.51.
77.7478.70.
77.9478.16.table 15: zero-shot xnli accuracy on the test set of each language when trained on 5%, 10%, 20% of trainingdata respectively..2222