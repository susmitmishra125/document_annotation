parameter-efficient multi-task fine-tuning for transformersvia shared hypernetworks.
rabeeh karimi mahabadi∗epfl university, idiap research instituterabeeh.karimimahabadi@epfl.ch.
sebastian ruderdeepmindruder@google.com.
mostafa dehghanigoogle braindehghani@google.com.
james hendersonidiap research institutejames.henderson@idiap.ch.
abstract.
state-of-the-art parameter-efficientfine-tuningmethods rely on introducing adapter modules be-tween the layers of a pretrained language model.
however, such modules are trained separately foreach task and thus do not enable sharing infor-mation across tasks.
in this paper, we show thatwe can learn adapter parameters for all layersand tasks by generating them using shared hyper-networks, which condition on task, adapter posi-tion, and layer id in a transformer model.
thisparameter-efficient multi-task learning frame-work allows us to achieve the best of both worldsby sharing knowledge across tasks via hypernet-works while enabling the model to adapt to eachindividualtask through task-specific adapters.
experiments on the well-known glue bench-mark show improved performance in multi-tasklearning while adding only 0.29% parameters pertask.
we additionally demonstrate substantial per-formance improvements in few-shot domain gen-eralization across a variety of tasks.
our codeis publicly available in https://github.com/rabeehk/hyperformer..1 introduction.
transfer learning from pretrained large-scale languagemodels yields state-of-the-art results in a variety oftasks (devlin et al., 2019; radford et al., 2018; liuet al., 2019b).
as a highly expressive and abstractframework, raffel et al.
(2020) explored the land-scape of transfer learning by converting text-basednatural language processing (nlp) problems into asequence-to-sequence format to train a unified modelon several tasks simultaneously.
multi-task learningwith pretrained language models (ruder, 2017) isappealing for multiple reasons: 1) training individualmodels per task results in higher computational costs,which hinders deployment and maintenance.
thesecosts are substantially reduced by training a single.
∗work done while the author was at google..τ and dl.
figure 1: left: adapter integration in the t5 model.
right:our hyperformer adapter architecture.
following houlsby et al.
(2019), we include adaptermodules after the two feed-forward layers.
the adapterhypernetwork hla produces the weights (u lτ ) fortask-specific adapter modules conditioned on an inputtask embedding iτ .
similarly, the layer normalizationhypernetwork hlln generates the conditional layer nor-malization parameters (βτ and γτ ).
during training, weonly update layer normalizations in t5, hypernetworks,and task embeddings.
the compact hyperformer++shares the same hypernetworks across all layers and tasksand computes the task embedding based on task, layer id,and position of the adapter module (§2.4)..model.
2) fine-tuning the model across multiple tasksallows sharing information between the differenttasks and positive transfer to other related tasks.
specifically, when target datasets have limited trainingdata, multi-task learning improves the performancecompared to individually trained models (liu et al.,2019a; ratner et al., 2018).
however, multi-taskfine-tuning can result in models underperforming onhigh-resource tasks due to constrained capacity (ari-vazhagan et al., 2019; mccann et al., 2018).
anadditional issue with multi-task fine-tuning is thepotential for task interference or negative transfer,.
proceedingsofthe59thannualmeetingoftheassociationforcomputationallinguisticsandthe11thinternationaljointconferenceonnaturallanguageprocessing,pages565–576august1–6,2021.©2021associationforcomputationallinguistics565multi-head attentionfeed forwardadapter+layer normlayer normfeed forwardadapter+feed forward downprojectionnonlinearityfeed forward upprojectionlayer norm+adapter layertransformer layerwhere achieving good performance on one task canhinder performance on another (wang et al., 2019c)..as an alternative to fine-tuning (howard and ruder,2018), adapter layers (houlsby et al., 2019) inserta small number of additional parameters per taskinto the model.
during fine-tuning, only the adaptermodules, layer normalizations, and parameters ofthe final classification layer are updated, while theoriginal pretrained model parameters remain frozen.
such task-specific adapters eliminate negative taskinterference by encapsulating task-specific informa-tion (pfeiffer et al., 2020).
however, so far there hasnot been an effective and parameter-efficient way toshare information across multiple adapters to enablepositive transfer to low-resource and related tasks..to address this problem and to enable sharing in-formation across tasks while reaping the benefits ofadapter layers, as depicted in figure 1, we proposehyperformer++, which employs a compact hyper-network (ha et al., 2017; oswald et al., 2020) sharedacross tasks and layers.
the hypernetwork learns togenerate task and layer-specific adapter parameters,conditioned on task and layer id embeddings.
the hy-pernetwork is jointly learned between all tasks and isthus able to share information across them, while neg-ative interference is minimized by generating separateadapter layers for each task.
for each new task, ourmodel only requires learning an additional task em-bedding, reducing the number of trained parameters..we use the encoder-decoder t5 model (raffel et al.,2020) as the underlying model for our experimentsand evaluate on the standard glue benchmark (wanget al., 2019b).
we achieve strong gains over boththe t5base model as well as adapters (houlsby et al.,2019).
to our knowledge, this is the first time thatadapters have been successfully integrated into a state-of-the-art encoder-decoder model beyond machinetranslation (bapna and firat, 2019), demonstratingthat our method effectively balances sharing informa-tion across tasks while minimizing negative transfer..in summary, we make the following contributions:(1) we propose a parameter-efficient method for multi-task fine-tuning based on hypernetworks and adapterlayers.
(2) we demonstrate that our method scalesmore efficiently than prior work.
(3) we provide em-pirical results on glue demonstrating the effective-ness of the proposed method on multi-task learning.
(4) we perform extensive few-shot domain transferexperiments, which reveal that the captured sharedknowledge can positively transfer to unseen in-domaintasks.
we release our code to facilitate future work..2 hyperformer.
in this section, we present our hyperformermodel, which integrates hypernetwork-based adapterlayers into a multi-task transformer model.
in §2.4,we introduce a parameter-efficient variant of thismodel, called hyperformer++..problem formulation: we consider a generalmulti-task learning problem, where we are given thedata from a set of tasks {dτ }tτ=1, where t is theτ )}nτtotal number of tasks and dτ ={(xii=1 showsthe training data for τ-th task with nτ samples.
weassume we are also given a large-scale pretrainedlanguage model fθ(.)
parameterized by θ thatcomputes the output for input xiτ .
standard multi-taskfine-tuning minimizes the following loss on thetraining set:.
τ ,yi.
l(θ,{dτ }t.τ=1)=.
t(cid:88).
(cid:88).
wτ lτ )∈dτ.
τ=1.
(xi.
τ ,yi.
(cid:16)fθ(xi.
τ ),yiτ.
(cid:17).
, (1).
where l is typically the cross-entropy loss, and wτshows the sampling weight for τ-th task.
our goalis to finetune the pretrained model in a multi-tasklearning setup efficiently, while allowing sharinginformation across tasks and at the same time,enabling the model to adapt to each individual task..the key idea of our approach, depicted in figure.
1, is to learn a parametric task embedding {iτ }tτ=1for each task, and then feed these task embeddingsto hypernetworks parameterized by ν that generatethe task-specific adapter layers (houlsby et al.,2019).
we insert adapter modules within the layersof a pretrained model, making the final model ofxν(xiτ , θ, iτ ) parameterized by ν that computesthe output for input xiτ .
during training, we onlytrain hypernetwork parameters ν, task embeddings{iτ }tτ=1, and layer normalizations in fθ(.
), while therest of the pretrained model parameters θ are fixed:.
l(ν,{iτ }tt(cid:88).
(cid:88).
τ=1)=.
i=1,{dτ }t(cid:16)xν(xi.
wτ l.τ=1.
(xi.
τ ,yi.
τ )∈dτ.
τ ,θ,iτ ),yiτ.
(cid:17),.
(2).
the hypernetworks capture the shared informationacross tasks in a multi-task learning model enablingpositive transfer between related domains and trans-ferable tasks, while adapters are reducing negativeinterference, encapsulating task-specific information..base model: all of our models are built on topof the state-of-the-art t5 transformer model (raffel.
566et al., 2020).
this model frames text-based languagetasks as sequence-to-sequence problems.
t5 consistsof an encoder-decoder transformer (vaswani et al.,2017) with minor modifications (raffel et al., 2020).
the model is trained simultaneously on multipletasks, obtaining state-of-the-art performance acrossa diverse set of tasks.
we use the t5 framework asit enables training a universal model that interfaceswith many language tasks.
our model has threemain components: 1) task conditional adapter layers;2) task conditional layer normalizations; and 3)hypernetworks that generate task-specific parameters.
we next describe these components..2.1 task conditional adapter layers.
prior work has shown that fine-tuning all parametersof the model can result in a sub-optimal solution,particularly for resource-limited datasets (peters et al.,2019).
as an alternative to fine-tuning all the model’sparameters, prior work (houlsby et al., 2019; rebuffiet al., 2018; stickland and murray, 2019) insertedsmall modules called adapter layers within layers ofa pretrained model, as shown in figure 1. adaptersintroduce no change to the structure or parametersof the original model..in this work, we propose conditional adaptermodules, in which we generate the adapters weightstask embeddings using sharedbased on inputhypernetworks (ha et al., 2017), which captureinformation across tasks that can be used to positivelytransfer to other relevant tasks..each layer of a transformer model consists ofan attention block and a feed-forward block, eachfollowed by a skip connection.
following houlsbyet al.
(2019), as depicted in figure 1, we introducea conditional adapter layer after each block before theskip connection.
the conditional adapter layer alττ ∈rh×d,for layer l consists of a down-projection, dlgelu non-linearity (hendrycks and gimpel, 2016),τ ∈ rd×h, where h is the inputand up-projection u ldimension, and d is the bottleneck dimension for theadapter layer, mathematically defined as:(cid:17)τ (x))).
τ (gelu(dl.
τ (x)=ln lτ.
(cid:16)u l.+x,.
al.
(3).
where x is the input hidden state and ln lτ is theconditional layer norm defined in the next section.
we generate adapter weights (u lτ ) through ahypernetwork described in §2.3..τ , dl.
2.2 task conditional layer normalization.
ln l.τ (xi.
τ )=γl.
τ (cid:12).
xi.
τ −µτστ.
+βlτ ,.
(4).
τ and βl.
where (cid:12) is the element-wise multiplication betweentwo vectors, and γlτ are learnable parameterswith the same dimension as xiτ .
values of µτ andστ show the mean and standard deviation of trainingdata for the τ-th task..to allow the layer normalization inside adaptersto adapt to each task, inspired by perez et al.
(2018);de vries et al.
(2017), we generate γlτ via ahypernetwork as a function of task embeddings (§2.3)..τ , βl.
2.3 task conditioned hypernetworks.
in order to have a model that can share informationwhile being able to adapt to each individual task, wegenerate the parameters of task conditional adapterlayers and layer normalization using hypernetworks.
a hypernetwork is a network that generates theweights of another network (ha et al., 2017)..the hypernetworks capture the shared information,while the generated task conditional adapters andlayer normalization allow the model to adapt to eachindividual task to reduce negative task interference..learned task embedding: we first compute a taskembedding iτ ∈ rt for each individual task using atask projector network hi(.
), which is a multi-layerperceptron consisting of two feed-forward layers anda relu non-linearity:.
iτ =hi(zτ ),.
(5).
where zτ ∈ rt(cid:48)can be a learnable parameter or anypretrained task features (vu et al., 2020), and the taskprojector network hi(.)
learns a suitable compressedtask embedding from input task features.
in this work,we consider a parametric zτ to allow end-to-endtraining which is convenient in practice.1.
removing task prefixes: the t5 model prependstask-specific prefixes to the input sequence forconditioning.
for instance, when training oncola (warstadt et al., 2019), cola sentence: isprepended to each sample.
instead, we remove taskprefixes and use task embeddings for conditioning..task conditioned hypernetworks: we considersimple linear layers as hypernetworks that arefunctions of input task embeddings iτ .
we introducethese hypernetworks in each layer of the transformer.
we define hypernetwork hla(.)
that generates taskconditional adapter weights (u l.τ , dl.
τ ):.
conventional layer normalization (ba et al., 2016) isdefined as:.
1we ran some pilot experiments with pretrained taskembeddings (vu et al., 2020), but did not observe extra benefits..567(u l.τ ,dl.
τ ):=hl.
a(iτ )=.
(cid:16)w u l.,w dl(cid:17).
iτ ,.
(6).
where w u l ∈ r(d×h)×t and w dl ∈ r(h×d)×tare the respective hypernetwork parameters.
weadditionally define the hypernetwork hlln (.)
thatcomputes the layer normalization parameters:.
(γl.
τ ,βl.
τ ):=hl.
ln (iτ )=.
(cid:16)w γl.
,w βl(cid:17).
iτ ,.
(7).
where w γl ∈rh×t and w βl ∈rh×t..2.4 hyperformer++.
a downside of introducing a separate hypernetworkin each layer of the transformer is that it increases theoverall number of parameters.
we, therefore, proposeto share hypernetworks across transformer layers.
by having a shared hypernetwork that is reusable,this strategy results in a substantial reduction in thenumber of parameters.
however, reapplying the samehypernetwork across all the layers introduces weightsharing across target parameters, which may not bedesirable.
to allow for a flexible parameterization oftask conditional adapters/layer normalization, for atransformer of l layers, we introduce a set of layerid embeddings i = {li}li=1, and adapter positionembeddings p ={pj}2j=1, which specify the positionof adapter layers in each transformer block (afterthe attention layer or feed-forward layer), which areused as additional inputs to the hypernetworks.
forsimplicity, we consider li ∈rt, pj ∈rt, and zτ ∈rt.
we feed a concatenation of (zτ ,li,pj) to a similartask projector network h(cid:48)iτ =h(cid:48).
(8)which is then followed by a shared layer normaliza-tion to compute final task embeddings iτ ∈rt to thehypernetwork.
this way, the hypernetwork is ableto produce distinct weights for each task, adapter po-sition, and layer of a transformer.
furthermore, layerid and adapter position embeddings are parametersthat are learned via back-propagation, allowing us totrain the whole model end-to-end conveniently..i as in eq.
(5):i(zτ ,li,pj),.
3 experiments.
sets are not publicly available, and following zhanget al.
(2021), for datasets fewer than 10k samples(rte, mrpc, sts-b, cola), we divide the originalvalidation set in half, using one half for validation andthe other for the test.
for the other larger datasets, wesplit 1k samples from the training set as our validationdata and test on the original validation set..experimental details: we use the huggingfaceimplementation (wolf et al., 2020a) of the t5model (raffel et al., 2020).
we fine-tune allmodels with a constant learning rate of 0.0003 andfollowing raffel et al.
(2020), we use 218 = 262144steps in all experiments.
we save a checkpointevery 1000 steps for all models (see also §a).
raffelet al.
(2020) report the results based on the bestcheckpoint for each task independently.
in contrast,we focus on the more realistic setting where we reportthe results on a single checkpoint with the highestaverage validation performance across all tasks.
thehyperparameters are selected in the same manner.
in contrast to prior work (houlsby et al., 2019), wedo not learn a separate output layer for each task butinstead share a frozen output layer for all the tasks,which makes our setting more parameter-efficientthan prior work and is an advantage of multi-tasklearning with encoder-decoder models.3.
baselines: we compare to the strong adapter base-line (houlsby et al., 2019).
following houlsby et al.
(2019), we add adapters modules for each task afterthe two feed-forward modules in each transformerblock of the t5 model.
as suggested in houlsby et al.
(2019), we train the layer normalization parametersinside the t5 model, per task.
we refer to this methodas adapters.
we additionally propose a variant ofthis model, in which we share all layer normalizationparameters (t5 and adapters) across all tasks.
werefer to this model as adapters†.
we compare ourmodels to the state-of-the-art t5 model, in which wefine-tune all parameters of the model on all tasks.
werefer to this method as t5small/t5base in experiments..datasets: following raffel et al.
(2020), weevaluate the performance of the models on the gluebenchmark (wang et al., 2019b).
this benchmarkcovers multiple tasks of paraphrase detection (mrpc,qqp), sentiment classification (sst-2), naturallanguage inference (mnli, rte, qnli), andlinguistic acceptability (cola).2 the original test.
2following raffel et al.
(2020); devlin et al.
(2019), as acommon practice, due to the adversarial nature of wnli withrespect to the training set, we do not experiment with wnli..sampling tasks: during training, we sample taskswith conventional temperature-based sampling withtemperature t = 10 for all methods.
we sample dif-ferent tasks proportional to p1/ti=1nτand nτ is the number of training samples for the τ-th task.
we did not experiment with more complexsampling strategies (raffel et al., 2020) or tuning of t ..τ where pτ = nτ.
(cid:80)t.3according to our initial experiments, fine-tuning the final out-put layer did not improve performance for adapter-based methods..568model.
cola sst-2 mrpc.
qqp.
sts-b.
mnli qnli rte avg.
#totalparams.
#trainedparams /per task.
single-task training.
t5smalladapterssmall.
(cid:163).
t5baseadaptersbase.
(cid:163).
8.0×1+8×0.01.
8.0×1+8×0.01.
100%0.74%.
100%0.87%.
(cid:171).
t5smalladapters†smallhyperformersmallhyperformer++small.
(cid:171)t5baseadapters†basehyperformerbasehyperformer++base.
1.0×1.05×1.45×1.04×.
1.0×1.07×1.54×1.02×.
12.5%0.68%5.80%0.50%.
12.5%0.82%6.86%0.29%.
multi-task training.
46.8140.12.
54.8559.49.
50.6739.8747.6453.96.
54.8861.5361.3263.73.
90.4789.44.
92.1993.46.
91.3990.0191.3990.59.
92.5493.0093.8094.03.
86.21/90.6785.22/89.29.
91.02/87.9690.04/86.68.
89.11/88.7083.93/83.62.
88.18/91.6188.18/91.55.
91.46/88.6190.94/88.01.
89.55/89.4187.44/87.18.
84.73/88.8988.67/91.8190.15/92.9684.24/88.81.
90.15/93.0190.15/92.9190.64/93.3389.66/92.63.
89.53/86.3188.51/84.7788.68/85.0888.44/84.46.
91.13/88.0790.47/87.2690.13/87.1890.28/87.20.
88.70/88.2788.15/87.8987.49/86.9687.73/87.26.
88.84/88.5389.86/89.4489.55/89.0390.00/89.66.
82.0981.58.
86.4986.38.
81.0479.9581.2480.69.
85.6686.0986.3385.74.
90.2189.11.
91.6092.26.
89.6789.6090.3990.39.
92.0493.1792.7993.02.
59.4255.80.
67.3968.84.
59.4260.1465.2271.01.
75.3670.2978.2675.36.
82.0679.53.
84.6784.88.
81.6980.8582.4782.51.
85.4785.8386.5886.48.table 1: performance of all models on the glue tasks.
for each method, we report the total number of parametersacross all tasks and the number of parameters that are trained for each task as a multiple and proportion respectively ofthe corresponding single-task t5 model.
for mnli, we report accuracy on the matched validation set.
for mrpc andqqp, we report accuracy and f1.
for sts-b, we report pearson and spearman correlation coefficients.
for cola, wereport matthews correlation.
for all other tasks, we report accuracy.
adapters† refers to our proposed variant of adapterswith shared layer normalizations.
our hyperformer++ obtains a better score on average compared to full fine-tuningand adapters†, while being more parameter-efficient.
(cid:171): our re-implementation of raffel et al.
(2020), (cid:163): applyingmethod of houlsby et al.
(2019) on t5.
bold fonts indicate the best results in each block..3.1 results on the glue benchmark.
table 1 shows the results on glue for single-taskand multi-task training.
we experiment with reduc-tion factors of r = {8,16,32} for all adapter-basedmethods, where r = hd .
we report the results bothwith t5small(6 layers and 60m parameters) andt5base models (12 layers and 222m parameters)..overall, our proposed hyperformer++ obtainsstrong gains over adapters (82.51 versus 79.53 fort5small and 86.48 versus 84.88 for t5base) whilebeing more parameter-efficient..our variant of adapters†, which shares layer normsacross tasks, outperforms prior work (houlsby et al.,2019), which does not share such information (80.85versus 79.53 for t5small and 85.83 versus 84.88 fort5base).
this demonstrates that in encoder-decodermodels such as t5 more sharing of informationacross tasks is beneficial..our proposed hyperformer obtains consistentimprovement over our proposed adapters† method.
we attribute this improvement to the ability to learnthe shared information across tasks through our hyper-networks.
interestingly, hyperformer++ obtainssimilar performance as hyperformer while beingmore than an order of magnitude more parameter-efficient.
adapter modules thus seem to be similarenough so that much of their information can be mod-eled by a single, appropriately conditioned network..compared to single-task fine-tuning of all param-.
eters, our methods on average improve the results by0.45 for t5small and 1.81 for t5base with substantialimprovement on low-resource datasets like cola(63.73 versus 54.85) and rte (75.36 versus 67.39)due to shared hypernetworks that capture the sharedinformation and enable positive transfer effects..we also report the total number of parameters andtrainable parameters for all methods in table 1. foradapter-based methods, the number of parametersvaries based on the adapter size (we report all numberswith r =32).
the multiple in terms of the number ofparameters of hyperformer++base with regard tot5base is 1.02× with only 0.29% trainable parametersper task.
note that by keeping the output layer frozenfor adapterssmall and adaptersbase,they require5.51× and 2.53× fewer parameters respectively com-pared to a direct application of prior work (houlsbyet al., 2019).
despite using more efficient baselines,compared to adaptersbase, hyperformer++base re-quires 3× fewer trainable parameters..3.2 few-shot domain transfer.
finally, we assess how well a trained hyperformercan generalize to new tasks.
we evaluate performanceon 5 tasks and 7 datasets.
in particular, we consider1) the natural language inference (nli) datasetsscitail (khot et al., 2018), and cb (de marneffeet al., 2019) from superglue (wang et al., 2019a)2) the question answering (qa) dataset boolq (clark.
569et al., 2019a); 3) the sentiment analysis datasetsimdb (maas et al., 2011) and yelp polarity (zhanget al., 2015); and 4) the paraphrase detection datasetpaws (baldridge et al., 2019); 5) the questionclassification dataset trec (li and roth, 2002)..for cb and boolq, since test sets are not available,we divide the validation sets in half, using one halffor validation and the other for testing.
for yelppolarity, trec, and imdb, since validation sets arenot available, we similarly divide the test sets to formvalidation sets.
for the rest, we report on the originaltest sets..we consider the models trained on glue reportedin table 1 and evaluate them on the test set after thefew-shot fine-tuning on each target training data.
foradapters† and our method, we use the adapter and thetask embedding respectively trained on the most sim-ilar glue task for initialization, i.e.
mnli for nli,qnli for qa, sst-2 for sentiment analysis, and qqpfor paraphrase detection.
following prior evidenceof positive transfer from nli to other tasks (conneauand kiela, 2018; yin et al., 2020; phang et al., 2018),we initialize the out-of-domain trec from mnli.
we show the results of full fine-tuning of all model’sparameters, adapters†, and hyperformer++4in table 2. our method significantly surpasses thebaselines on the majority of settings..3.3 low-resource fine-tuning.
figure 2: results on glue for the various number oftraining samples per task (100,500,1000,2000,4000).
weshow mean and standard deviation across 5 seeds..yelp polarity.
given that our model hyperformer++base hassubstantially fewer trainable parameters than t5base,we investigate whether it generalizes better in alow-resource setting.
we subsample each individualtask in glue for varying training sizes.
we trainthe models for 15,000 steps, which we found to be.
4we finetune hypernetworks and task embeddings parameters.
we also tried only fine-tuning the task embedding but foundthat this achieves lower performance in the few-shot setting andcomparable performance with more samples..dataset.
scitail.
cb.
trec.
boolq.
imdb.
paws.
h ype rf o r m e r++ base.
# sa m ples.
t5 base.
natural language inference.
question classification.
question answering.
sentiment analysis.
adapters† base.
79.54±2.883.25±1.785.06±1.188.22±1.391.27±0.891.75±0.892.72±0.5.
51.11±9.274.81±5.474.81±5.980.74±7.686.67±5.0.
23.61±7.743.45±14.059.6±7.078.07±3.893.65±1.796.06±0.497.03±0.7.
53.48±2.851.37±6.554.52±5.158.60±1.666.72±0.770.21±1.373.60±0.8.
81.55±1.982.54 ±1.083.39 ±0.883.35 ±0.885.37±0.586.27±0.486.57±0.2.
81.37±13.191.08±0.291.09±0.590.15±0.791.52±0.292.26±0.692.36±0.4.
55.69±9.063.38±5.368.78±1.573.82±1.685.36±0.687.89±0.690.41±0.6.
79.60±3.380.03±2.381.97±1.384.04±0.788.07±0.788.77±1.091.01±1.0.
57.78±10.977.04±7.280.0±7.685.93±5.485.19±4.7.
28.11±5.940.08±12.662.49±6.287.79±0.793.57±1.395.5±0.996.87±1.3.
50.49±11.156.50±7.158.43±4.960.10±2.466.49±1.269.01±1.171.58±0.8.
77.23±3.082.74±1.783.42±1.084.58±0.684.99±0.385.50±0.186.01±0.2.
76.85±14.387.84±1.589.22±0.790.19±0.790.92±0.291.32±0.291.68±0.1.
53.89±3.654.18±1.055.23±3.271.51±2.482.81±1.085.67±0.788.33±0.6.
paraphrase detection.
4163210050010002000.
41632100250.
4163210050010002000.
4163210050010002000.
4163210050010002000.
4163210050010002000.
4163210050010002000.
82.00±4.986.55±1.485.85±1.488.52±0.791.44±0.692.34±0.593.40±0.2.
60.74±16.6676.29±4.4581.48±6.287.41±2.9689.63±4.32.
28.85±6.949.40±9.568.94±7.588.42±1.794.78±1.496.72±1.396.92±0.9.
48.03±4.850.21±7.958.37±3.762.03±2.070.04±1.472.35±1.774.94±0.6.
81.77±1.884.06±0.784.64±0.484.74±0.486.00±0.286.37 ±0.486.60±0.1.
90.25±1.090.36±1.291.15±0.591.06±0.692.09±0.492.50±0.292.70±0.1.
55.58±7.572.71±1.173.39±2.178.24±2.186.3±1.189.12±0.590.87±0.3.
table 2: few-shot domain transfer results of the modelstrained on glue averaged across 5 seeds.
we computeaccuracy for all datasets..57001000200030004000#samplespertask606570758085averagescoresongluet5basehyperformer++basesufficient to allow them to converge.
figure 2 showsthe results.
hyperformer++base substantiallyimproves results with limited training data, indicatingmore effective fine-tuning in this regime..4 analysis.
4.1 parameter efficiency.
in this section, we compare the number of parametersof hyperformer++ with adapters..adapters parameters: thestandard setting(houlsby et al., 2019) employs two adapters perlayer for each task.
each adapter layer has 2hdparameters for projection matrices (u lτ ) and2h parameters for the layer normalization.
the totalnumber of parameters for adapters for l transformerlayers in both an encoder and a decoder across t tasksis, therefore, 4t l(2hd + 2h), which scales linearlywith the number of tasks times the number of layers..τ and dl.
hyperformer++ parameters: our approachlearns a task feature embedding per task, consistingof t t parameters.
we additionally employ layer idand adapter position embeddings in the encoder anddecoder, which require 2(2+l)t parameters, with afixed embedding size of t for all these feature embed-dings.
we consider a separate task projector networksh(cid:48)i for encoder and decoder, which is in both casesa two-layer mlp, consisting of a total of 2(3te+et)parameters, where e = 128 is the hidden dimensionfor the task-projector network.
our hypernetworkfor adapters in encoder/decoder consists of 2(2thd)parameters and our layer normalization hypernetworkconsists of 2(2th) parameters.
in total, this resultsin t(t +4+2l)parameters.
(cid:125).
+ 8te+2t(2hd+2h)(cid:125).
(cid:124).
(cid:124).
(cid:123)(cid:122)hypernetworks.
(cid:123)(cid:122)task features.
the total number of parameters for hypernetworksremains constant, while the task feature parametersscale with the number of tasks or layers times t,where t=64 in our experiments..in settings with a large number of layers and a largenumber of tasks, since t(cid:28)2hd+2h and t +l(cid:28)t l,our method is much more parameter-efficient com-pared to adapters.
in the current setting, the term hdis the largest term, and the factor 2t l for adaptersis larger than the factor t for hyperformer++..4.2 do extra parameters make a difference?
while our hyperformer++ is more parameter-efficient than the baselines, the number of parametersof hyperformer per task is higher compared toadapters†.
to confirm that the improvements of.
model.
glue.
#totalparams.
#trainedparams/task.
adapters† smallhyperformer smalladapters† basehyperformer base.
80.9782.47.
85.8486.58.
1.83x1.45x.
2.02x1.54x.
10.44%5.80 %.
12.73%6.86%.
table 3: averaged test results on glue for hyper-former and adapters†, where adapters† has a highernumber of parameters compared to hyperformer..model variant.
glue.
82.47hyperformersmall− adapter blocks68.37− conditional layer norm79.83− task projector81.56− t5 layer norm81.29− conditional layer norm, t5 layer norm 78.92.table 4: impact when removing different components ofour framework.
we report the average results on glue..hyperformer are due to its capability of sharinginformation across tasks and not the number ofparameters, as an ablation, we run the adapters†with r = {2, 4} and choose the model performingthe best on the validation set.
this allows adapters†to have a higher number of parameters compared tohyperformer.
we report the results in table 3and compare them with results of hyperformerin table 1. the results demonstrate that even withan increased number of parameters, adapters† is notable to reach the performance of hyperformer,and hyperformer performs substantially better..4.3 impact of the framework components.
we investigate the impact of the components of ourframework including: (1) task conditional adapterblocks; (2) task conditional layer normalization;(4) fine-tuning of(3) task projection network;layer normalizations in the t5 model;(5) taskconditional layer normalization in adapter modulesand fine-tuning of layer normalizations inside the t5model.
we consider our small model of table 1 andtrain different variants of it.
table 4 shows the resultson glue, demonstrating that each component of themodel contributes positively to its final performance..4.4 visualization of task embeddings.
to analyze what hyperformer++base has learnedabout the relations between different tasks, we visual-ize the learned task embeddings for the models trained.
571with us, tay et al.
(2021) propose a multi-task learningmethod by training task-conditioned hyper networks;however, their method is 43x less parameter efficientcompared to ours.
in another line of research, clarket al.
(2019b) proposed to learn multi-task modelswith knowledge distillation.
houlsby et al.
(2019)trained adapters for each task separately, keepingthe model fixed.
stickland and murray (2019) sharethe model parameters across tasks and introducetask-specific adapter parameters, which is moreparameter-inefficient than our method..and.
contextual.
hypernetworksparametergeneration: our work is closely related to hyper-networks (ha et al., 2017).
in a continual learningsetup, where tasks are learned sequentially, oswaldet al.
(2020) proposed a task-conditioned hypernet-work to generate all the weights of the target model.
our method is substantially more efficient as we donot generate all the weights of the target model but avery small number of parameters for adapter modulesto allow the model to adapt to each individual taskefficiently.
similarly, jin et al.
(2020) generate thefull model from task-specific descriptions in differentdomains whereas we efficiently generate only smalladapter modules for each task..prior work also proposed meta-learning orbayesian approaches to generate softmax layerparameters for new settings (bansal et al., 2020;ponti et al., 2020).
meta-learning approaches arenotoriously slow to train.
in addition, generatingsoftmax parameters requires a substantially highernumber of parameters, leaves the method unable toadapt the lower layers of the model, and restricts theirapplication to classification tasks..in contemporaneous work,.
¨ust¨un et al.
(2020)proposed a multilingual dependency parsing methodbased on adapters and contextual parameter generatornetworks (platanios et al., 2018) where they generateadapter parameters conditioned on trained inputlanguage embeddings.
their study is limited tomultilingual dependency parsing, while our workstudies multi-task learning and applies to several tasksthanks to the general sequence-to-sequence natureof our model.
moreover, their number of trainableparameters is 2.88× larger than their base modelsince they employ a contextual parameter generatorin each layer.
in contrast, we use a single compacthypernetwork allowing us to efficiently condition onmultiple tasks and layers of a transformer model..figure 3: visualization of learned task embeddings byhyperformer++base..with the largest number of samples in table 1 and 2.figure 3 illustrates the 2d vector projections of taskembeddings using pca (wold et al., 1987).
interest-ingly, the observed groupings correspond to similartasks.
this shows that learned task embeddings byhyperformer++base are meaningful.
for cb, annli dataset despite being initialized from mnli, af-ter few-shot training the task embedding is closestto rte, another nli dataset.
this is plausible aspremises and hypotheses in both the discourse-basedcb and the news and wikipedia-based rte are morecomplex compared to mnli.
the sentence similaritydataset sts-b is grouped close to the mrpc para-phrase dataset.
cola, which focuses on linguisticacceptability is very different from other tasks and isnot grouped with any of the observed task embeddings.
in addition, the task embeddings for 1) all the senti-ment analysis datasets namely sst-2, yelp polarity,and imdb; 2) the two large-scale nli datasets namelymnli and scitail; 3) question answering datasets, i.e.
boolq and qnli; and 4) paraphrase datasets namelyqqp and paws are each grouped together..5 related work.
multi-task learning: multi-task learning,i.e.,learning a unified model to perform well on multipledifferent tasks, is a challenging problem in nlp.
it requires addressing multiple challenges such ascatastrophic forgetting, and handling disproportionatetask sizes resulting in a model overfitting in low-resource tasks while underfitting in high-resourceones (arivazhagan et al., 2019).
liu et al.
(2019a) pro-posed multi-task deep neural network (mtdnn)for learning from multiple nlu tasks.
althoughmtdnn obtains impressive results on glue, itapplies multi-task learning as a form of pretrainingfollowed by task-specific fine-tuning.
concurrently.
572420246420246costssyeimmnsccbrtmrqqpaqnbotrco: colast: sts-bss: sst-2ye: yelp polarityim: imdbmn: mnlisc: scitailcb: cbrt: rtemr: mrpcqq: qqppa: pawsqn: qnlibo: boolqtr: trec6 conclusion.
we propose a parameter-efficient method formulti-task fine-tuning.
our approach is to train sharedhypernetworks to generate task-specific adaptersconditioned on the task, layer id, and adapter positionembeddings.
the shared hypernetworks capture theknowledge across tasks and enable positive transferto low-resource and related tasks, while task-specificlayers allow the model to adapt to each individualtask.
extensive experiments show that our methodobtains strong improvement over multi-task learningon the glue benchmark, and substantially improvesthe in-domain task generalization..acknowledgments.
we are grateful to dani yogatama, neil houlsby, andcolin raffel for feedback on a draft of this paper.
we would like to also thank adam paszke, jamiekiros, and george dahl for useful comments anddiscussions..references.
naveen arivazhagan, ankur bapna, orhan firat, dmitrylepikhin, melvin johnson, maxim krikun, mia xuchen, yuan cao, george foster, colin cherry, et al.
2019. massively multilingual neural machine trans-lation in the wild: findings and challenges.
arxivpreprint arxiv:1907.05019..jimmy lei ba, jamie ryan kiros, and geoffrey ehinton.
2016. layer normalization.
arxiv preprintarxiv:1607.06450..jason baldridge, luheng he, and yuan zhang.
2019.paws: paraphrase adversaries from word scrambling.
in naacl..trapit bansal, rishikesh jha, and andrew mccallum.
2020. learning to few-shot learn across diversenatural language classification tasks.
in coling..ankur bapna and orhan firat.
2019. simple, scalableadaptation for neural machine translation.
in emnlp..christopher clark, kenton lee, ming-wei chang, tomkwiatkowski, michael collins, and kristina toutanova.
2019a.
boolq: exploring the surprising difficulty ofnatural yes/no questions.
in naacl..marie-catherine de marneffe, mandy simons, and judithtonhauser.
2019. the commitmentbank: investigat-ing projection in naturally occurring discourse.
inproceedings of sinn und bedeutung..harm de vries, florian strub, j´er´emie mary, hugolarochelle, olivier pietquin, and aaron c courville.
2017. modulating early visual processing by language.
in neurips..jacob devlin, ming-wei chang, kenton lee, and kristinatoutanova.
2019. bert: pre-training of deep bidirec-tional transformers for language understanding.
innaacl..david ha, andrew dai, and quoc v. le.
2017. hypernet-.
works.
in iclr..dan hendrycks and kevin gimpel.
2016. gaussian errorlinear units (gelus).
arxiv preprint arxiv:1606.08415..neil houlsby, andrei giurgiu, stanislaw jastrzebski,bruna morrone, quentin de laroussilhe, andreagesmundo, mona attariyan, and sylvain gelly.
2019.parameter-efficient transfer learning for nlp.
in icml..jeremy howard and sebastian ruder.
2018. universallanguage model fine-tuning for text classification.
in acl..tian jin, zhun liu, shengjia yan, alexandre eichen-berger, and louis-philippe morency.
2020. languageto network: conditional parameter adaptation withnatural language descriptions.
in acl..tushar khot, ashish sabharwal, and peter clark.
2018.scitail: a textual entailment dataset from sciencequestion answering.
in aaai..xin li and dan roth.
2002. learning question classifiers..in coling..xiaodong liu, pengcheng he, weizhu chen, and jian-feng gao.
2019a.
multi-task deep neural networks fornatural language understanding.
in acl..yinhan liu, myle ott, naman goyal, jingfei du, mandarjoshi, danqi chen, omer levy, mike lewis, lukezettlemoyer, and veselin stoyanov.
2019b.
roberta:a robustly optimized bert pretraining approach.
arxivpreprint arxiv:1907.11692..andrew l. maas, raymond e. daly, peter t. pham, danhuang, andrew y. ng, and christopher potts.
2011.learning word vectors for sentiment analysis.
in acl..kevin clark, minh-thang luong, urvashi khandelwal,christopher d manning, and quoc le.
2019b.
bam!
born-again multi-task networks for natural languageunderstanding.
in acl..bryan mccann, nitish shirish keskar, caiming xiong,and richard socher.
2018.the natural languagedecathlon: multitask learning as question answering.
arxiv preprint arxiv:1806.08730..alexis conneau and douwe kiela.
2018. senteval: anevaluation toolkit for universal sentence representations.
in lrec..johannes von oswald, christian henning, jo˜ao sacra-mento, and benjamin f grewe.
2020. continuallearning with hypernetworks.
in iclr..573ethan perez, florian strub, harm de vries, vincentdumoulin, and aaron courville.
2018. film: visualreasoning with a general conditioning layer.
in aaai..matthew e peters, sebastian ruder, and noah a smith.
2019. to tune or not to tune?
adapting pretrainedrepresentations to diverse tasks.
in repl4nlp..jonas pfeiffer, andreas r¨uckl´e, clifton poth, aishwaryakamath, ivan vuli´c, sebastian ruder, kyunghyuncho, and iryna gurevych.
2020. adapterhub: ain emnlp:framework for adapting transformers.
system demonstrations..jason phang, thibault f´evry, and samuel r bowman.
2018. sentence encoders on stilts: supplementaryarxivtraining on intermediate labeled-data tasks.
preprint arxiv:1811.01088..emmanouil antonios platanios, mrinmaya sachan,graham neubig, and tom mitchell.
2018. contextualparameter generation for universal neural machinetranslation.
in emnlp..edoardo m ponti, ivan vuli´c, ryan cotterell, marinelaparovic, roi reichart, and anna korhonen.
2020.parameter space factorization for zero-shotlearn-arxiv preprinting across tasks and languages.
arxiv:2001.11453..alec radford, karthik narasimhan, tim salimans,improving language.
and ilya sutskever.
2018.understanding by generative pre-training..colin raffel, noam shazeer, adam roberts, katherinelee, sharan narang, michael matena, yanqi zhou,wei li, and peter j liu.
2020. exploring the limits oftransfer learning with a unified text-to-text transformer.
jmlr..alex ratner, braden hancock, jared dunnmon, rogergoldman, and christopher r´e.
2018. snorkel metal:in deemweak supervision for multi-task learning.
workshop..sylvestre-alvise rebuffi, hakan bilen, and andreaefficient parametrization of multi-.
vedaldi.
2018.domain deep neural networks.
in cvpr..sebastian ruder.
2017. an overview of multi-tasklearning in deep neural networks.
in arxiv preprintarxiv:1706.05098..asa cooper stickland and iain murray.
2019. bert andpals: projected attention layers for efficient adaptationin multi-task learning.
in icml..ashish vaswani, noam shazeer, niki parmar, jakobuszkoreit, llion jones, aidan n gomez, łukaszkaiser, and illia polosukhin.
2017. attention is all youneed.
in neurips..tu vu, tong wang, tsendsuren munkhdalai, alessandrosordoni, adam trischler, andrew mattarella-micke,subhransu maji, and mohit iyyer.
2020. exploring andpredicting transferability across nlp tasks.
in emnlp..alex wang, yada pruksachatkun, nikita nangia, aman-preet singh, julian michael, felix hill, omer levy,and samuel r bowman.
2019a.
superglue: astickier benchmark for general-purpose languageunderstanding systems.
in neurips..alex wang, amanpreet singh, julian michael, felix hill,omer levy, and samuel r. bowman.
2019b.
glue:a multi-task benchmark and analysis platform fornatural language understanding.
in iclr..zirui wang, zihang dai, barnab´as p´oczos, and jaime car-bonell.
2019c.
characterizing and avoiding negativetransfer.
in cvpr..alex warstadt, amanpreet singh, and samuel r. bow-man.
2019. neural network acceptability judgments.
tacl..svante wold, kim esbensen, and paul geladi.
1987.principal component analysis.
chemometrics andintelligent laboratory systems..thomas wolf, lysandre debut, victor sanh, julienchaumond, clement delangue, anthony moi, pierriccistac, tim rault, r´emi louf, morgan funtowicz, joedavison, sam shleifer, patrick von platen, clara ma,yacine jernite, julien plu, canwen xu, teven le scao,sylvain gugger, mariama drame, quentin lhoest,and alexander m. rush.
2020a.
transformers: state-in emnlp:of-the-art natural language processing.
system demonstrations..thomas wolf, quentin lhoest, patrick von platen,yacine jernite, mariama drame, julien plu, julienchaumond, clement delangue, clara ma, abhishekthakur, suraj patil, joe davison, teven le scao,victor sanh, canwen xu, nicolas patry, angiemcmillan-major, simon brandeis, sylvain gugger,franc¸ois lagunas, lysandre debut, morgan funtow-icz, anthony moi, sasha rush, philipp schmidd,pierric cistac, victor muˇstar,jeff boudier, andanna tordjmann.
2020b.
datasets.
github.
note:https://github.com/huggingface/datasets..wenpeng yin, nazneen fatema rajani, dragomir radev,richard socher, and caiming xiong.
2020. universalnatural language processing with limited annotations:try few-shot textual entailment as a start.
in emnlp..yi tay, zhe zhao, dara bahri, don metzler, and da-cheng juan.
2021. hypergrid transformers: towards asingle model for multiple tasks.
in iclr..tianyi zhang, felix wu, arzoo katiyar, kilian q wein-berger, and yoav artzi.
2021. revisiting few-samplebert fine-tuning.
in iclr..ahmet ¨ust¨un, arianna bisazza, gosse bouma, and gert-jan van noord.
2020. udapter: language adaptationfor truly universal dependency parsing.
in emnlp..xiang zhang, junbo zhao, and yann lecun.
2015.text.
convolutional networks.
for.
character-levelclassification.
in neurips..574impact of adapter’s bottleneck size on the perfor-mance similar to (houlsby et al., 2019), adapter’sreduction factor needs to be set per dataset.
ta-ble 7 shows the validation performance of hyper-former++ on the glue tasks for different adapters’reduction factors.
while the pattern may not be al-ways consistent, generally, smaller datasets seem tobenefit more from smaller bottleneck size, i.e., less pa-rameters for adapters, while the opposite is the case forlarger datasets, which require more modeling capacity..a experimental details.
computing infrastructure: we run the experi-ments in table 1 on 4 gpus, and the rest of the experi-ments on 1 gpu on a heterogeneous cluster with teslav100, tesla a100, tesla p4, and gtx1080ti gpus..hyperparameters: we use a batch size of 64 fort5small and 32 for t5base to fit the gpu memory.
weset the dimension of the task feature embedding (zτ )to t(cid:48) =512, and the dimension of the task embedding(iτ ) to t = 64. for low-resource fine-tuning in §3.3,we use reduction factors of {16,32,64}..data pre-processing: we download all datasetsfrom the huggingface datasets library (wolf et al.,2020b).
following raffel et al.
(2020), we cast alldatasets into a sequence-to-sequence format, andrecast sts-b as a 21-class classification task by round-ing its target scores to their nearest increment of 0.2..performance evaluation: table 5 and 6 presentthe efficiency evaluation in terms of memory, andtime for all the methods measured on the gluebenchmark.
we report the time for 1000 training steps.
our approach has several attractive properties.
ourhyperformer++base approach offers a much bettermemory usage with low-overhead, while hyper-formerbase and t5base cause substantial memoryoverhead.
in dealing with large-scale transformer mod-els like t5, efficient memory usage is of paramountimportance.
second, in terms of training time, ourmethod is much faster than adapters†base.
relative tot5base, hyperformer++base increases the trainingtime by 30.49%, while adapters†base causes thesubstantial training time overhead of 84.93%..model.
memory ∆%.
t5baseadapters†basehyperformerbasehyperformer++base.
7.76 (gb)5.95 (gb)7.60 (gb)5.81 (gb).
--23.32%-2.06%-25.13.table 5: the required memory for all methods.
∆% isthe relative difference with respect to t5base..model.
t5baseadapters†basehyperformerbasehyperformer++base.
time.
5.51 (min)10.19 (min)7.92 (min)7.19 (min).
∆%.
-84.93%43.74%30.49%.
table 6: training time for all methods.
∆% is the relativedifference with respect to t5base..575modelhyperformer++smallhyperformer++smallhyperformer++small.
hyperformer++basehyperformer++basehyperformer++base.
r.81632.
81632.cola sst-2 mrpc.
qqp.
sts-b.
mnli qnli rte avg.
42.1342.6049.90.
54.8653.8355.58.
98.6097.896.00.
97.3098.0097.20.
82.76/87.7284.73/89.1283.74/88.50.
88.18/91.5588.18/91.6189.66/92.42.
90.69/87.5588.99/85.3389.29/85.79.
94.59/92.9194.89/93.3393.19/91.08.
84.92/84.1885.69/85.1285.99/85.41.
89.77/89.6990.12/89.6588.96/88.57.
82.381.9681.28.
85.8985.9485.82.
95.4093.6991.79.
96.1096.5094.19.
78.8375.9172.99.
84.6783.9481.75.
83.1982.8182.79.
87.7787.8287.13.table 7: validation performance of hyperformer++ on the glue tasks for different reduction factors r ={8,16,32}.
for mnli, we report accuracy on the matched validation set.
for mrpc and qqp, we report accuracy and f1.
forsts-b, we report pearson and spearman correlation coefficients.
for cola, we report matthews correlation.
for allother tasks, we report accuracy..576