binarybert: pushing the limit of bert quantization.
haoli bai1, wei zhang2, lu hou2, lifeng shang2,jing jin3, xin jiang2, qun liu2, michael lyu1, irwin king11 the chinese university of hong kong2huawei noah’s ark lab, 3huawei technologies co., ltd.{hlbai, lyu, king}@cse.cuhk.edu.hk{zhangwei379, houlu3, shang.lifeng, jinjing12, jiang.xin, qun.liu}@huawei.com.
abstract.
the rapid development of large pre-trainedlanguage models has greatly increased thedemand for model compression techniques,among which quantization is a popular so-lution.
in this paper, we propose binary-bert, which pushes bert quantization tothe limit by weight binarization.
we ﬁnd thata binary bert is hard to be trained directlythan a ternary counterpart due to its complexand irregular loss landscape.
therefore, wepropose ternary weight splitting, which ini-tializes binarybert by equivalently splittingfrom a half-sized ternary network.
the binarymodel thus inherits the good performance ofthe ternary one, and can be further enhancedby ﬁne-tuning the new architecture after split-ting.
empirical results show that our binary-bert has only a slight performance drop com-pared with the full-precision model while be-ing 24× smaller, achieving the state-of-the-artcompression results on the glue and squadbenchmarks..1.introduction.
recent pre-trained language models have achievedremarkable performance improvement in variousnatural language tasks (vaswani et al., 2017; de-vlin et al., 2019).
however, the improvementgenerally comes at the cost of increasing modelsize and computation, which limits the deploy-ment of these huge pre-trained language modelsto edge devices.
various methods have been re-cently proposed to compress these models, suchas knowledge distillation (sanh et al., 2019; sunet al., 2019; jiao et al., 2020), pruning (michelet al., 2019; fan et al., 2019), low-rank approxi-mation (ma et al., 2019; lan et al., 2020), weight-sharing (dehghani et al., 2019; lan et al., 2020;huang et al., 2021), dynamic networks with adap-tive depth and/or width (hou et al., 2020; xin et al.,2020; zhou et al., 2020), and quantization (zafrir.
(a) mrpc..(b) mnli-m..figure 1: performance of quantized bert with vary-ing weight bit-widths and 8-bit activation.
we reportthe mean results with standard deviations from 10 seedson mrpc and 3 seeds on mnli-m, respectively..et al., 2019; shen et al., 2020; fan et al., 2020;zhang et al., 2020)..among all these model compression approaches,quantization is a popular solution as it does notrequire designing a smaller model architecture.
in-stead, it compresses the model by replacing each32-bit ﬂoating-point parameter with a low-bit ﬁxed-point representation.
existing attempts try to quan-tize pre-trained models (zafrir et al., 2019; shenet al., 2020; fan et al., 2020) to even as low asternary values (2-bit) with minor performance drop(zhang et al., 2020).
however, none of themachieves the binarization (1-bit).
as the limit ofquantization, weight binarization could bring atmost 32× reduction in model size and replace mostﬂoating-point multiplications with additions.
more-over, quantizing activations to 8-bit or 4-bit furtherreplaces the ﬂoating-point addition with int8 andint4 addition, decreasing the energy burden and thearea usage on chips (courbariaux et al., 2015)..in this paper, we explore to binarize bert pa-rameters with quantized activations, pushing bertquantization to the limit.
we ﬁnd that directly train-ing a binary network is rather challenging.
ac-cording to figure 1, there is a sharp performancedrop when reducing weight bit-width from 2-bit.
proceedingsofthe59thannualmeetingoftheassociationforcomputationallinguisticsandthe11thinternationaljointconferenceonnaturallanguageprocessing,pages4334–4348august1–6,2021.©2021associationforcomputationallinguistics4334to 1-bit, compared to other bit conﬁgurations.
toexplore the challenges of binarization, we analyzethe loss landscapes of models under different pre-cisions both qualitatively and quantitatively.
it isfound that while the full-precision and ternary (2-bit) models enjoy relatively ﬂat and smooth losssurfaces, the binary model suffers from a rathersteep and complex landscape, which poses greatchallenges to the optimization..motivated by the above empirical observations,we propose ternary weight splitting, which takesthe ternary model as a proxy to bridge the gap be-tween the binary and full-precision models.
specif-ically, ternary weight splitting equivalently con-verts both the quantized and latent full-precisionweights in a well-trained ternary model to initializebinarybert.
therefore, binarybert retains thegood performance of the ternary model, and canbe further reﬁned on the new architecture.
whileneuron splitting is previously studied (chen et al.,2016; wu et al., 2019) for full-precision network,our ternary weight splitting is much more complexdue to the additional equivalence requirement ofquantized weights.
furthermore, the proposed bi-narybert also supports adaptive splitting.
it canadaptively perform splitting on the most importantternary modules while leaving the rest as binary,based on efﬁciency constraints such as model sizeor ﬂoating-point operations (flops).
therefore,our approach allows ﬂexible sizes of binary modelsfor various edge devices’ demands..empirical results show that binarybert splitfrom a half-width ternary network is much betterthan a directly-trained binary model with the origi-nal width.
on the glue and squad benchmarks,our binarybert has only a slight performancedrop compared to the full-precision bert-basemodel, while being 24× smaller.
moreover, bi-narybert with the proposed importance-basedadaptive splitting also outperforms other splittingcriteria across a variety of model sizes..2 difﬁculty in training binary bert.
in this section, we show that it is challenging totrain a binary bert with conventional binarizationapproaches directly.
before diving into details, weﬁrst review the necessary backgrounds..we follow the standard quantization-aware train-ing procedure (zhou et al., 2016).
speciﬁcally,given weight w ∈ rn (a.k.a latent full-precisionweights), each forward propagation quantizes it to.
ˆw = q(w) by some quantization function q(·),and then computes the loss (cid:96)( ˆw) at ˆw.
during backpropagation, we use ∇(cid:96)( ˆw) to update latent full-precision weights w due to the non-differentiabilityof q(·), which is known as the straight-through es-timator (courbariaux et al., 2015)..recent ternarybert (zhang et al., 2020) fol-lows ternary-weight-network (twn) (li et al.,2016) to quantize the elements in w to three values{±α, 0}.
to avoid confusion, we use superscriptt and b for the latent full-precision weights andquantized weights in ternary and binary models,respectively.
speciﬁcally, twn ternarizes eachi in the ternary weight wt aselement wt.
ˆwt.
i = q(wt.
i) =.
(cid:26)α · sign(wt0.i) |wt|wt.
i| ≥ ∆i| < ∆.
,.
(1).
where sign(·) is the sign function, ∆ = 0.7and α = 1|i|.
i| with i = {i | ˆwt.
i∈i |wt.
i (cid:54)= 0}..(cid:80).
n (cid:107)wt(cid:107)1.binarization.
binarization is ﬁrst proposed in(courbariaux et al., 2015) and has been exten-sively studied in the academia (rastegari et al.,2016; hubara et al., 2016; liu et al., 2018).
asa representative work, binary-weight-network(bwn) (hubara et al., 2016) binarizes wb element-wisely with a scaling parameter α as follows:.
ˆwb.
i = q(wb.
i ) = α · sign(wb.
i ), α =.
(cid:107)wb(cid:107)1.
(2).
1n.despite the appealing properties of network bi-narization, we show that it is non-trivial to obtain abinary bert with these binarization approaches..2.1 sharp performance drop with weight.
binarization.
to study the performance drop of bert quan-tization, we train the bert model with full-precision, {8,4,3,2,1}-bit weight quantization and8-bit activations on mrpc and mnli-m fromthe glue benchmark (wang et al., 2018) 1. weuse loss-aware weight quantization (laq) (houand kwok, 2018) for 8/4/3-bit weight quantization,twn (li et al., 2016) for weight ternarization andbwn (hubara et al., 2016) for weight binarization.
meanwhile, we adopt 8-bit uniform quantizationfor activations.
we follow the default experimentalsettings detailed in section 4.1 and appendix c.1..1we conduct more experiments on other glue datasetsand with different settings in appendix c.1, and ﬁnd similarempirical results to mrpc and mnli-m here..4335(a) full-precision model..(b) ternary model..(c) binary model..(d) all together..figure 2: loss landscapes visualization of the full-precision, ternary and binary models on mrpc.
for (a), (b)and (c), we perturb the (latent) full-precision weights of the value layer in the 1st and 2nd transformer layers, andcompute their corresponding training loss.
(d) shows the gap among the three surfaces by stacking them together..(a) mha-qk..(b) mha-v..(c) mha-o..(d) ffn-mid..(e) ffn-out..figure 3: the top-1 eigenvalues of parameters at different transformer parts of the full-precision (fp), ternary andbinary bert.
for easy comparison, we report the ratio of eigenvalue between the ternary/binary models and thefull-precision model.
the error bar is estimated of all transformer layers over different data mini-batches..from figure 1, the performance drops mildlyfrom 32-bit to as low as 2-bit, i.e., around 0.6% ↓on mrpc and 0.2% ↓ on mnli-m. however,when reducing the bit-width to one, the perfor-mance drops sharply, i.e, ∼ 3.8% ↓ and ∼ 0.9% ↓on the two tasks, respectively.
therefore, weightbinarization may severely harm the performance,which may explain why most current approachesstop at 2-bit weight quantization (shen et al., 2020;zadeh and moshovos, 2020; zhang et al., 2020).
to further push weight quantization to the limit, aﬁrst step is to study the potential reasons behindthe sharp drop from ternarization to binarization..2.2 exploring the quantized loss landscape.
visualization.
to learn about the challenges be-hind the binarization, we ﬁrst visually compare theloss landscapes of full-precision, ternary, and bi-nary bert models.
following (nahshan et al.,2019), we extract parameters wx, wy from thevalue layers2 of multi-head attention in the ﬁrsttwo transformer layers, and assign the followingperturbations on parameters:.
˜wx = wx + x · 1x,.
˜wy = wy + y · 1y,.
(3).
2we also extract parameters from other parts of the trans-.
former in appendix c.2, and the observations are similar..where x ∈ {±0.2 ¯wx, ±0.4 ¯wx, ..., ±1.0 ¯wx} areperturbation magnitudes based the absolute meanvalue ¯wx of wx, and similar rules hold for y.
1xand 1y are vectors with all elements being 1. foreach pair of (x, y), we evaluate the correspondingtraining loss and plot the surface in figure 2..as can be seen, the full-precision model (fig-ure 2(a)) has the lowest overall training loss, and itsloss landscape is ﬂat and robust to the perturbation.
for the ternary model (figure 2(b)), despite thesurface tilts up with larger perturbations, it looks lo-cally convex and is thus easy to optimize.
this mayalso explain why the bert model can be ternar-ized without severe accuracy drop (zhang et al.,2020).
however, the loss landscape of the binarymodel (figure 2(c)) turns out to be both higher andmore complex.
by stacking the three landscapestogether (figure 2(d)), the loss surface of the binarybert stands on the top with a clear margin withthe other two.
the steep curvature of loss surfacereﬂects a higher sensitivity to binarization, whichattributes to the training difﬁculty..steepness measurement.
to quantitatively mea-sure the steepness of loss landscape, we start from alocal minima w and apply the second order approx-imation to the curvature.
according to the taylor’sexpansion, the loss increase induced by quantizing.
4336figure 4: the overall workﬂow of training binarybert.
we ﬁrst train a half-sized ternary bert model, and thenapply ternary weight splitting operator (equations (6) and (7)) to obtain the latent full-precision and quantizedweights as the initialization of the full-sized binarybert.
we then ﬁne-tune binarybert for further reﬁnement..w can be approximately upper bounded by(cid:96)( ˆw) − (cid:96)(w) ≈ (cid:15)(cid:62)h(cid:15) ≤ λmax(cid:107)(cid:15)(cid:107)2,.
(4).
where (cid:15) = w − ˆw is the quantization noise, andλmax is the largest eigenvalue of the hessian h atw. note that the ﬁrst-order term is skipped dueto ∇(cid:96)(w) = 0. thus we take λmax as a quanti-tative measurement for the steepness of the losssurface.
following (shen et al., 2020) we adoptthe power method to compute λmax.
as it is com-putationally expensive to estimate h for all w inthe network, we consider them separately as fol-lows: (1) the query/key layers (mha-qk), (2) thevalue layer (mha-v), (3) the output projectionlayer (mha-o) in the multi-head attention, (4) theintermediate layer (ffn-mid), and (5) the outputlayer (ffn-out) in the feed-forward network.
notethat we group key and query layers as they are usedtogether to calculate the attention scores..from figure 3, the top-1 eigenvalues of the bi-nary model are higher both on expectation and stan-dard deviation compared to the full-precision base-line and the ternary model.
for instance, the top-1eigenvalues of mha-o in the binary model are∼ 15× larger than the full-precision counterpart.
therefore, the quantization loss increases of full-precision and ternary model are tighter boundedthan the binary model in equation (4).
the highlycomplex and irregular landscape by binarizationthus poses more challenges to the optimization..3 proposed method.
3.1 ternary weight splittinggiven the challenging loss landscape of binarybert, we propose ternary weight splitting (tws)that exploits the ﬂatness of ternary loss landscapeas the optimization proxy of the binary model.
as.
is shown in figure 4, we ﬁrst train the half-sizedternary bert to convergence, and then split boththe latent full-precision weight wt and quantizedˆwt to their binary counterparts wb1, ˆwb2via the tws operator.
to inherit the performanceof the ternary model after splitting, the tws opera-tor requires the splitting equivalency (i.e., the sameoutput given the same input):.
2 and ˆwb.
1, wb.
wt = wb.
1 + wb2,.
ˆwt = ˆwb.
1 + ˆwb.
2 ..(5).
while solution to equation (5) is not unique, weconstrain the latent full-precision weights after2 to satisfy wt = wbsplitting wb.
1 + wb.
1, wb.
2 as.
wb.
1,i =.
wb.
2,i =.
.
.
.
a · wtib + wtib(1−a)wti−b−b + wti.i (cid:54)= 0i = 0, wt.
if ˆwtif ˆwtotherwiseif ˆwti (cid:54)= 0if ˆwti = 0, wtotherwise.
i > 0.,.
(6).
i > 0.,.
(7).
where a and b are the variables to solve.
by equa-tions (6) and (7) with ˆwt = ˆwb.
2, we get.
(cid:80).
i∈i |wt.
(cid:80).
n|i|.
1 + ˆwbj| − (cid:80)j∈j |wti∈i |wti|i=1 |wti|.
,.
i| + (cid:80)2 (cid:80)i| − (cid:80)ni∈i |wt2(|j | + |k|).
k∈k |wtk|.
,.
(8).
a =.
b =.
where we denote i = {i | ˆwtj =0 and wtk < 0}.
| · | denotes the cardinality of the set.
detailedderivation of equation (8) is in appendix a..i (cid:54)= 0}, j = {j | ˆwtk = 0 and wt.
j > 0} and k = {k | ˆwt.
quantization details.
following (zhang et al.,2020), for each weight matrix in the transformerlayers, we use layer-wise ternarization (i.e., onescaling parameter for all elements in the weight.
4337matrix).
for word embedding, we use row-wiseternarization (i.e., one scaling parameter for eachrow in the embedding).
after splitting, each of thetwo split matrices has its own scaling factor..aside from weight binarization, we simultane-ously quantize activations before all matrix mul-tiplications, which could accelerate inference onspecialized hardwares (shen et al., 2020; zafriret al., 2019).
following (zafrir et al., 2019; zhanget al., 2020), we skip the quantization for all layer-normalization (ln) layers, skip connections, andbias as their calculations are negligible compared tomatrix multiplication.
the last classiﬁcation layeris also not quantized to avoid a large accuracy drop..training with knowledge distillation.
knowl-edge distillation is shown to beneﬁt bert quan-tization (zhang et al., 2020).
following (jiaoet al., 2020; zhang et al., 2020), we ﬁrst per-form intermediate-layer distillation from the full-precision teacher network’s embedding e, layer-wise mha output ml and ffn output fl tothe quantized student counterpart ˆe, ˆml, ˆfl (l =1, 2, ...l).
we aim to minimize their mean sqau-red errors, i.e., (cid:96)emb = mse( ˆe, e), (cid:96)mha =(cid:80)l mse( ˆfl, fl)..l mse( ˆml, ml), and (cid:96)f f n = (cid:80).
thus the objective function is.
(cid:96)int = (cid:96)emb + (cid:96)mha + (cid:96)f f n..(9).
we then conduct prediction-layer distillation byminimizing the soft cross-entropy (sce) betweenquantized student logits ˆy and teacher logits y, i.e.,.
sensitive parts being ternary and the rest being bi-nary), and then split ternary weights into binaryones.
therefore, adaptive splitting ﬁnally enjoysconsistent arithmetic precision (1-bit) for all weightmatrices, which is usually easier to deploy than themixed-precision counterpart..formulation.
intuitively, we assign ternary val-ues to weight matrices that are more sensitive toquantization.
the quantization sensitivity of theweight matrix is empirically measured by the per-formance gain of not quantizing it comparing tothe fully-quantized counterpart (details are in ap-pendix b.1.).
we denote u ∈ rz+ as the sensitivityvector, where z is the total number of splittableweight matrices in all transformer layers, the wordembedding layer and the pooler layer.
the costvector c ∈ rz+ stores the additional increase ofparameter or flops of each ternary weight matrixagainst a binary choice.
the splitting assignmentcan be represented as a binary vector s ∈ {0, 1}z,where sz = 1 means to ternarize the z-th weightmatrix, and vice versa.
the optimal assignment s∗can thus be solved from the following combinato-rial optimization problem:.
maxss.t..u(cid:62)sc(cid:62)s ≤ c − c0, s ∈ {0, 1}z,.
(11).
where c0 is the baseline efﬁciency of the half-sizedbinary network.
dynamic programming can be ap-plied to solve equation (11) to avoid np-hardness..(cid:96)pred = sce(ˆy, y)..(10).
4 experiments.
further fine-tuning.
after splitting from thehalf-sized ternary model, the binary model inheritsits performance on a new architecture with fullwidth.
however, the original minimum of theternary model may not hold in this new loss land-scape after splitting.
thus we further ﬁne-tune withprediction-layer distillation to look for a better so-lution.
we dub the resulting model as binarybert..3.2 adaptive splitting.
our proposed approach also supports adaptive split-ting that can ﬂexibly adjust the width of binary-bert, based on the parameter sensitivity to bina-rization and resource constraints of edge devices.
speciﬁcally, given the resource constraints c(e.g., model size and computational flops), weﬁrst train a mixed-precision model adaptively (with.
in this section, we empirically verify our proposedapproach on the glue (wang et al., 2018) andsquad (rajpurkar et al., 2016, 2018) benchmarks.
we ﬁrst introduce the experimental setup in sec-tion 4.1, and then present the main experimentalresults on both benchmarks in section 4.2. wecompare with other state-of-the-arts in section 4.3,and ﬁnally provide more discussions on theproposed methods in section 4.4.code isavailable at https://github.com/huawei-noah/pretrained-language-model/tree/master/binarybert..4.1 experimental setup.
dataset and metrics.
the glue benchmarkcontains multiple natural language understandingtasks.
we follow devlin et al.
(2019) to evaluate theperformance on these tasks: matthews correlation.
4338# quant.
qqp qnli sst-2 cola sts-b mrpc rte avg..flops(g).
da.
mnli-m/mm.
#bits(w-e-a)full-prec.
1-1-81-1-81-1-41-1-41-1-81-1-81-1-41-1-4.size(mb).
417.613.416.513.416.513.416.513.416.5.
#bits(w-e-a)full-prec.
1-1-81-1-81-1-41-1-41-1-81-1-81-1-41-1-4.size(mb).
417.613.416.513.416.513.416.513.416.5.
123456789.
123456789.
-bwntwsbwntwsbwntwsbwntws.
-bwntwsbwntwsbwntwsbwntws.
22.53.13.11.51.53.13.11.51.5.
22.53.13.11.51.53.13.11.51.5.
84.9/85.5-(cid:55)84.2/84.0(cid:55)84.2/84.7(cid:55)83.5/83.4(cid:55)83.9/84.2(cid:51) 84.2/84.0(cid:51) 84.2/84.7(cid:51) 83.5/83.4(cid:51) 83.9/84.2.
91.491.191.290.991.291.191.290.991.2.
84.5/84.1-(cid:55)83.3/83.4(cid:55)84.1/83.6(cid:55)83.5/82.5(cid:55)83.6/82.9(cid:51) 83.3/83.4(cid:51) 84.1/83.5(cid:51) 83.5/82.5(cid:51) 83.6/82.9.
89.588.989.089.089.088.989.089.089.0.
92.190.791.590.790.991.291.691.291.4.
91.390.190.089.489.390.389.889.989.7.
93.292.392.692.392.392.793.292.593.7.
93.092.393.192.393.191.391.992.093.1.
59.746.753.434.844.454.255.551.953.3.
54.938.150.526.737.448.451.645.047.9.
90.186.888.684.987.288.289.287.788.6.
84.481.283.478.982.583.282.381.982.9.
86.382.685.579.983.386.886.085.586.0.
87.986.186.084.285.986.385.985.286.6.
72.268.672.265.365.370.074.070.471.5.
83.980.882.778.479.982.583.381.982.6.
69.963.165.859.962.766.167.364.165.8.
82.278.580.676.378.580.180.679.280.2.table 1: results on the glue development set.
“#bits (w-e-a)” represents the bit number for weights of trans-former layers, word embedding, and activations.
“da” is short for data augmentation.
“avg.” denotes the averageresults of all tasks including mnli-m and mnli-mm.
the higher results in each block are bolded..# quant.
qqp qnli sst-2 cola sts-b mrpc rte avg..flops(g).
da.
mnli-m/mm.
table 2: results on the glue test set scored using the glue evaluation server..for cola, spearman correlation for sts-b and ac-curacy for the rest tasks: rte, mrpc, sst-2, qqp,mnli-m (matched) and mnli-mm (mismatched).
for machine reading comprehension on squad,we report the em (exact match) and f1 score..aside from the task performance, we also reportthe model size (mb) and computational flopsat inference.
for quantized operations, we fol-low (zhou et al., 2016; liu et al., 2018; li et al.,2020a) to count the bit-wise operations, i.e., themultiplication between an m-bit number and ann-bit number approximately takes mn/64 flopsfor a cpu with the instruction size of 64 bits..implementation.
we take dynabert (houet al., 2020) sub-networks as backbones as theyoffer both half-sized and full-sized models for easycomparison.
we start from training a ternary modelof width 0.5× with the two-stage knowledge distil-lation introduced in section 3.1. then we split itinto a binary model with width 1.0×, and performfurther ﬁne-tuning with prediction-layer distilla-tion.
each training stage takes the same numberof training epochs.
following (jiao et al., 2020;hou et al., 2020; zhang et al., 2020), we adoptdata augmentation with one training epoch in eachstage on all glue tasks except for mnli and qqp.
aside from this default setting, we also remove data.
augmentation and perform vanilla training with 6epochs on these tasks.
on mnli and qqp, wetrain 3 epochs for each stage..we verify our ternary weight splitting (tws)against vanilla binary training (bwn), the latter ofwhich doubles training epochs to match the overalltraining time in tws for fair comparison.
moretraining details are provided in appendix b..activation quantization.
while binarybertfocuses on weight binarization, we also explore ac-tivation quantization in our implementation, whichis beneﬁcial for reducing the computation burdenon specialized hardwares (hubara et al., 2016;zhou et al., 2016; zhang et al., 2020).
aside from8-bit uniform quantization (zhang et al., 2020;shen et al., 2020) in past efforts, we further pi-oneer to study 4-bit activation quantization.
weﬁnd that uniform quantization can hardly deal withoutliers in the activation.
thus we use learnedstep-size quantization (lsq) (esser et al., 2019)to directly learn the quantized values, which empir-ically achieves better quantization performance..4.2 experimental results.
4.2.1 results on the glue benchmark.
the main results on the development set are shownin table 1. for results without data augmenta-.
4339quant.
-bwntwsbwntws.
#bits(w-e-a)full-prec.
1-1-81-1-81-1-41-1-4.size(mb)417.613.416.513.416.5.flops(g)22.53.13.11.51.5.squadv1.182.6/89.779.2/86.980.8/88.377.5/85.879.3/87.2.
squadv2.075.1/77.573.6/76.673.6/76.571.9/75.172.5/75.4.
table 3: development set results (em/f1) on squad..(a) 8-bit activation..(b) 4-bit activation..figure 5: the average performance over six gluetasks of adaptive splitting strategies..tion (row #2-5), our ternary weight splitting methodoutperforms bwn with a clear margin 3. for in-stance, on cola, ternary weight splitting achieves6.7% ↑ and 9.6% ↑ with 8-bit and 4-bit activationquantization, respectively.
while data augmenta-tion (row 6-9) mostly improves each entry, ourapproach still overtakes bwn consistently.
fur-thermore, 4-bit activation quantization empiricallybeneﬁts more from ternary weight splitting (row4-5 and 8-9) compared with 8-bit activations (row2-3 and 6-7), demonstrating the potential of ourapproach in extremely low bit quantized models..in table 2, we also provide the results on thetest set of glue benchmark.
similar to the ob-servation in table 1, our approach achieves consis-tent improvement on both 8-bit and 4-bit activationquantization compared with bwn..method.
bert-basedistilbertlayerdrop-6llayerdrop-3ltinybert-6lalbert-e128albert-e768quant-noiseq-bertq-bertq-bertgobogoboternarybertbinarybertbinarybert.
#bits(w-e-a)full-prec.
full-prec.
full-prec.
full-prec.
full-prec.
full-prec.
full-prec.
pq2/4-8-82/3-8-82-8-83-4-322-2-322-2-81-1-81-1-4.size(mb)4182503282245545120385346284328281717.ratio(↓)1.01.71.31.97.69.33.511.07.99.115.09.715.015.024.624.6.squadv1.180.8/88.579.1/86.9--79.7/87.582.3/89.381.5/88.6-79.9/87.579.3/87.069.7/79.6--79.9/87.480.8/88.379.3/87.2.
mnli-m84.681.682.978.682.881.682.083.683.581.876.683.771.083.584.283.9.table 4: comparison with other state-of-the-art meth-ods on development set of squad v1.1 and mnli-m..4.2.3 adaptive splitting.
the adaptive splitting in section 3.2 supports theconversion of mixed ternary and binary precisionsfor more-ﬁne-grained conﬁgurations.
to verify itsadvantages, we name our approach as maximalgain according to equation (11), and compare itwith two baseline strategies i) random gain thatrandomly selects weight matrices to split; and ii)minimal gain that splits the least important mod-ules according to sensitivity.
we report the averagescore over six tasks (qnli, sst-2, cola, sts-b, mrpc and rte) in figure 5. the end-pointsof 9.8mb and 16.5mb are the half-sized and full-sized binarybert, respectively.
as can be seen,adaptive splitting generally outperforms the othertwo baselines under varying model size, indicatingthe effectiveness of maximizing the gain in adap-tive splitting.
in appendix c.4, we provide detailedperformance on the six tasks, together with the ar-chitecture visualization of adaptive splitting..4.2.2 results on squad benchmark.
4.3 comparison with state-of-the-arts.
the results on the development set of squad v1.1and v2.0 are shown in table 3. our proposedternary weight splitting again outperforms bwnw.r.t both em and f1 scores on both datasets.
simi-lar to previous observations, 4-bit activation enjoysa larger gain in performance from the splitting ap-proach.
for instance, our approach improves theem score of 4-bit activation by 1.8% and 0.6% onsquad v1.1 and v2.0, respectively, both of whichare higher than those of 8-bit activation..3note that dynabert only squeezes width in the trans-former layers but not the word embedding layer, thus the splitbinary model has a slightly larger size than bwn..now we compare our proposed approach with avariety of state-of-the-art counterparts, includingq-bert (shen et al., 2020), gobo (zadeh andmoshovos, 2020), quant-noise (fan et al., 2020)and ternarybert (zhang et al., 2020).
asidefrom quantization, we also compare with othergeneral compression approaches such as distill-bert (sanh et al., 2019), layerdrop (fan et al.,2019), tinybert (jiao et al., 2020), and al-bert (lan et al., 2020).
the results are taken fromthe original papers, respectively.
from table 4,our proposed binarybert has the smallest modelsize with the best performance among all quantiza-.
4340quant.
twn0.5×tws1.0×twn0.5×tws1.0×.
#bits(w-e-a)2-2-81-1-82-2-41-1-4.squadv1.180.3/87.980.8/88.378.0/86.479.3/87.2.
mnli-m84.184.283.783.9.qnli mrpc.
91.391.690.991.4.
85.786.085.586.0.table 5: the performance gain by ﬁne-tuning the bi-nary model after splitting.
0.5× and 1.0× denote thehalf-sized and full-sized models, respectively..(a) 8-bit activation..(b) 4-bit activation..(c) 8-bit activation..(d) 4-bit activation..figure 6: (a) and (b) show the training curves on mrpcunder different activation bits.
the red box is enlargedin the sub-ﬁgure.
(c) and (d) visualize the ﬁne-tuningtrajectories after splitting, on the 2-d loss contour ofbinarybert..tion approaches.
compared with the full-precisionmodel, our binarybert retains competitive perfor-mance with a signiﬁcant reduction of model sizeand computation.
for example, we achieve morethan 24× compression ratio compared with bert-base, with only 0.4% ↓ and 0.0%/0.2% ↓ drop onmnli-m on squad v1.1, respectively..4.4 discussion.
4.4.1 further improvement after splitting.
we now demonstrate the performance gain by re-ﬁning the binary model on the new architecture.
we evaluate the performance gain after splittingfrom a half-width ternary model (twn0.5×) to thefull-sized model (twn1.0×) on the developmentset of squad v1.1, mnli-m, qnli and mrpc.
the results are shown in table 5. as can be seen,further ﬁne-tuning brings consistent improvementon both 8-bit and 4-bit activation..quant.
bwnlabbirealbwn†bwn‡twsbwnlabbirealbwn†bwn‡tws.
#bits(w-e-a)1-1-81-1-81-1-81-1-81-1-81-1-81-1-41-1-41-1-41-1-41-1-41-1-4.squadv1.179.2/86.979.0/87.079.4/87.179.4/87.379.6/87.280.8/88.377.5/85.876.7/85.576.9/85.478.2/86.278.3/86.579.3/87.2.
mnli-m84.283.683.984.283.584.283.583.383.483.683.183.9.qnli sst-2.
91.291.591.491.391.291.691.291.391.091.390.991.4.
92.792.892.592.892.993.292.592.992.892.992.993.7.table 6: comparison with other binarization methods..training curves.
furthermore, we plot the train-ing loss curves of bwn, twn and our tws onmrpc with data augmentation in figures 6(a) and6(b).
since tws cannot inherit the previous op-timizer due to the architecture change, we resetthe optimizer and learning rate scheduler of bwn,twn and tws for a fair comparison, despite theslight increase of loss after splitting.
we ﬁnd thatour tws attains much lower training loss thanbwn, and also surpasses twn, verifying the ad-vantages of ﬁne-tuning on the wider architecture..optimization trajectory.
we also follow (liet al., 2018; hao et al., 2019) to visualize the op-timization trajectory after splitting in figures 6(c)and 6(d).
we calculate the ﬁrst two principal com-ponents of parameters in the ﬁnal binarybert,which are the basis for the 2-d plane.
the loss con-tour is thus obtained by evaluating each grid pointin the plane.
it is found that the binary modelsare heading towards the optimal solution for both8/4-bit activation quantization on the loss contour..4.4.2 exploring more binarization methods.
we now study if there are any improved bina-rization variants that can directly bring better per-formance.
aside from bwn, we compare withlab (hou et al., 2017) and bireal (liu et al.,2018).
meanwhile, we compare with gradual quan-tization, i.e., bwn training based on a ternarymodel, denoted as bwn†.
furthermore, we alsotry the same scaling factor of bwn with twnto make the precision change smooth, dubbed asbwn‡.
from table 6, we ﬁnd that our twsstill outperforms various binarization approaches inmost cases, suggesting the superiority of splittingin ﬁnding better minima than direct binary training..43415 related work.
network quantization has been a popular topic withvast literature in efﬁcient deep learning.
below wegive a brief overview for three research strands:network binarization, mixed-precision quantizationand neuron splitting, all of which are related to ourproposed approach..5.1 network binarization.
network binarization achieves remarkable size re-duction and is widely explored in computer vision.
existing binarization approaches can be catego-rized into quantization error minimization (raste-gari et al., 2016; hou et al., 2017; zhang et al.,2018), improving training objectives (martinezet al., 2020; bai et al., 2020) and reduction ofgradient mismatch (bai et al., 2018; liu et al.,2018, 2020).
despite the empirical success ofthese approaches in computer vision, there is lit-tle exploration of binarization in natural languageprocessing tasks.
previous works on bert quanti-zation (zafrir et al., 2019; shen et al., 2020; zhanget al., 2020) push down the bit-width to as low astwo, but none of them achieves binarization.
onthe other hand, our work serves as the ﬁrst attemptto binarize the pre-trained language models..5.2 mixed-precision quantization.
given the observation that neural network layersexhibit different sensitivity to quantization (donget al., 2019; wang et al., 2019), mixed-precisionquantization re-allocate layer-wise quantizationbit-width for higher compression ratio.
inspiredby neural architecture search (liu et al., 2019;wang et al., 2020), common approaches of mixed-precision quantization are primarily based on differ-entiable search (wu et al., 2018a; li et al., 2020b),reinforcement learning (wu et al., 2018b; wanget al., 2019), or simply loss curvatures (donget al., 2019; shen et al., 2020).
while mixed-precision quantized models usually demonstratebetter performance than traditional methods underthe same compression ratio, they are also harder todeploy (habi et al., 2020).
on the contrary, binary-bert with adaptive splitting enjoy both the goodperformance from the mixed precision of ternaryand binary values, and the easy deployment giventhe consistent arithmetic precision..there are also works on binary neural architec-ture search (kim et al., 2020; bulat et al., 2020)which have a similar purpose to mixed-precision.
quantization.
nonetheless, such methods are usu-ally time-consuming to train and are prohibitive forlarge pre-trained language models..5.3 neuron splitting.
neuron splitting is originally proposed to acceler-ate the network training, by progressively increas-ing the width of a network (chen et al., 2016; wuet al., 2019).
the split network equivalently in-herits the knowledge from the antecessors and istrained for further improvement.
recently, neu-ron splitting is also studied in quantization (zhaoet al., 2019; kim et al., 2019).
by splitting neuronswith large magnitudes, the full-precision outliersare removed and thus the quantization error can beeffectively reduced (zhao et al., 2019).
kim et al.
(2019) apply neuron splitting to decompose ternaryactivation into two binary activations based on biasshifting of the batch normalization layer.
however,such a method cannot be applied in bert as thereis no batch normalization layer.
besides, weightsplitting is much more complex due to the equiv-alence constraint on both the quantized and latentfull-precision weights..6 conclusion.
in this paper, we propose binarybert, pushingbert quantization to the limit.
as a result of thesteep and complex loss landscape, we ﬁnd directlytraining a binarybert is hard with a large per-formance drop.
we thus propose a ternary weightsplitting that splits a trained ternary bert to ini-tialize binarybert, followed by ﬁne-tuning forfurther reﬁnement.
our approach also supportsadaptive splitting that can tailor the size of binary-bert based on the edge device constraints.
em-pirical results show that our approach signiﬁcantlyoutperforms vanilla binary training, achieving state-of-the-art performance on bert compression..acknowledgement.
this work was partially supported by the nationalkey research and development program of china(no.
2018aaa0100204), and research grantscouncil of the hong kong special administrativeregion, china (no.
cuhk 14210717 of the gen-eral research fund).
we sincerely thank all anony-mous reviewers for their insightful suggestions..4342references.
h. bai, j. wu, i. king, and m. lyu.
2020. few shotnetwork compression via cross distillation.
in pro-ceedings of the aaai conference on artiﬁcial intel-ligence, pages 3203–3210..y. bai, y. wang, and e. liberty.
2018. proxquant:quantized neural networks via proximal operators.
in international conference on machine learning..a. bulat, b. martinez, and g. tzimiropoulos.
2020.bats: binary architecture search.
in european con-ference on computer vision, pages 309–325..t. chen, i. goodfellow, and j. shlens.
2016. net2net:accelerating learning via knowledge transfer.
ininternational conference on learning representa-tions..m. courbariaux, y. bengio, and j. david.
2015. bina-ryconnect: training deep neural networks with bi-in advances innary weights during propagations.
neural information processing systems..m. dehghani, s. gouws, o. vinyals, j. uszkoreit, andl. kaiser.
2019. universal transformers.
in interna-tional conference on learning representations..j. devlin, m. chang, k. lee, and k. toutanova.
2019.bert: pre-training of deep bidirectional transform-ers for language understanding.
in north americanchapter of the association for computational lin-guistics..z. dong, z. yao, a. gholami, m. mahoney, andk. keutzer.
2019. hawq: hessian aware quantiza-tion of neural networks with mixed-precision.
inproceedings of the ieee/cvf international confer-ence on computer vision, pages 293–302..s. k. esser, j. l. mckinstry, d. bablani, r. ap-puswamy, and d. s. modha.
2019. learned step sizequantization.
in international conference on learn-ing representations..a. fan, e. grave, and a. joulin.
2019. reducing trans-former depth on demand with structured dropout.
ininternational conference on learning representa-tions..a. fan, p. stock, b. graham, e. grave, r. gribon-val, h. jegou, and a. joulin.
2020. training withquantization noise for extreme model compression.
preprint arxiv:2004.07320..h. habi, r. jennings, and a. netzer.
2020. hmq: hard-ware friendly mixed precision quantization block forcnns.
in european conference on computer vision,pages 448–463..y. hao, l. dong, f. wei, and k. xu.
2019. visual-izing and understanding the effectiveness of bert.
in conference on empirical methods in natural lan-guage processing..l. hou, z. huang, l. shang, x. jiang, x. chen, andq. liu.
2020. dynabert: dynamic bert with adaptivewidth and depth.
in advances in neural informationprocessing systems..l. hou and j. t. kwok.
2018. loss-aware weight quan-tization of deep networks.
in international confer-ence on learning representations..l. hou, yao q., and j. t. kwok.
2017. loss-awarebinarization of deep networks.
in international con-ference on learning representations..z. huang, l hou, l. shang, x. jiang, x. chen, andq. liu.
2021. ghostbert: generate more featureswith cheap operations for bert.
in annual meetingof the association for computational linguistics..i. hubara, m. courbariaux, d. soudry, r. el-yaniv, andy. bengio.
2016. binarized neural networks.
in ad-vances in neural information processing systems..x. jiao, y. yin, l. shang, x. jiang, x. chen, l. li,f. wang, and q. liu.
2020. tinybert: distillingbert for natural language understanding.
in findingsof empirical methods in natural language process-ing..d. kim, k singh, and j. choi.
2020. learning architec-tures for binary networks.
in european conferenceon computer vision, pages 575–591..h. kim, k. kim, j. kim, and j. kim.
2019. bina-ryduo: reducing gradient mismatch in binary acti-vation network by coupling binary activations.
ininternational conference on learning representa-tions..z. lan, m. chen, s. goodman, k. gimpel, p. sharma,and r. soricut.
2020. albert: a lite bert for self-supervised learning of language representations.
ininternational conference on learning representa-tions..f. li, b. zhang, and b. liu.
2016. ternary weight net-.
works.
preprint arxiv:1605.04711..h. li, z. xu, g. taylor, c. studer, and t. goldstein.
2018. visualizing the loss landscape of neural nets.
in advances in neural information processing sys-tems..y. li, x. dong, and w. wang.
2020a.
additive powers-of-two quantization: a non-uniform discretizationfor neural networks.
in international conference onlearning representations..y. li, w. wang, h. bai, r. gong, x. dong, andf. yu.
2020b.
efﬁcient bitwidth search for prac-tical mixed precision neural network.
preprintarxiv:2003.07577..h. liu, k. simonyan, and y. yang.
2019. darts: differ-in international con-.
entiable architecture search.
ference on learning representations..4343z. liu, z. shen, m. savvides, and k. cheng.
2020. re-actnet: towards precise binary neural network withgeneralized activation functions.
in european con-ference on computer vision, pages 143–159..z. liu, b. wu, w. luo, x. yang, w. liu, and k. cheng.
2018. bi-real net: enhancing the performance of1-bit cnns with improved representational capabilityand advanced training algorithm.
in european con-ference on computer vision..x. ma, p. zhang, s. zhang, n. duan, y. hou, d. song,and m. zhou.
2019. a tensorized transformer forlanguage modeling.
in advances in neural informa-tion processing systems..b. martinez, j. yang, a. bulat, and g. tzimiropoulos.
2020. training binary neural networks with real-to-binary convolutions.
in international conference onlearning representations..p. michel, o. levy, and g. neubig.
2019. are sixteenheads really better than one?
in advances in neuralinformation processing systems..y. nahshan, b. chmiel, c. baskin, e. zheltonozh-skii, r. banner, a. m. bronstein, and a. mendel-son.
2019. loss aware post-training quantization.
preprint arxiv:1911.07190..p. rajpurkar, r. jia, and p. liang.
2018. know whatyou don’t know: unanswerable questions for squad.
preprint arxiv:1806.03822..p. rajpurkar, j. zhang, k. lopyrev, and p. liang.
2016.squad: 100,000+ questions for machine comprehen-sion of text.
preprint arxiv:1606.05250..m. rastegari, v. ordonez, j. redmon, and a. farhadi.
2016. xnor-net: imagenet classiﬁcation using bi-in europeannary convolutional neural networks.
conference on computer vision..v. sanh, l. debut, j. chaumond, and t. wolf.
2019.distilbert, a distilled version of bert: smaller, faster,cheaper and lighter.
preprint arxiv:1910.01108..s. shen, z. dong, j. ye, l. ma, z. yao, a. gholami,m. w. mahoney, and k. keutzer.
2020. q-bert: hes-sian based ultra low precision quantization of bert.
in proceedings of the aaai conference on artiﬁcialintelligence..s. sun, y. cheng, z. gan, and j. liu.
2019. patientknowledge distillation for bert model compression.
in conference on empirical methods in natural lan-guage processing..a. vaswani, n. shazeer, n. parmar, j. uszkoreit,l. jones, a. n. gomez, ł. kaiser, and i. polosukhin.
2017. attention is all you need.
in advances in neu-ral information processing systems..a. wang, a. singh, j. michael, f. hill, o. levy, ands. r. bowman.
2018. glue: a multi-task bench-mark and analysis platform for natural language un-derstanding.
preprint arxiv:1804.07461..j. wang, h. bai, j. wu, x. shi, j. huang, i. king,m. lyu, and j. cheng.
2020. revisiting parametersharing for automatic neural channel number search.
in advances in neural information processing sys-tems, volume 33..k. wang, z. liu, y. lin, j. lin, and s. han.
2019. haq:hardware-aware automated quantization with mixedprecision.
in proceedings of the ieee/cvf confer-ence on computer vision and pattern recognition,pages 8612–8620..b. wu, y. wang, p. zhang, y. tian, p. vajda, andk. keutzer.
2018a.
mixed precision quantizationof convnets via differentiable neural architecturesearch.
preprint arxiv:1812.00090..j. wu, y. zhang, h. bai, h. zhong, j. hou, w. liu, andj huang.
2018b.
pocketﬂow: an automated frame-work for compressing and accelerating deep neuralnetworks.
in advances in neural information pro-cessing systems, workshop on compact deep neu-ral networks with industrial applications..l. wu, d. wang, and q. liu.
2019. splitting steep-est descent for growing neural architectures.
in ad-vances in neural information processing systems,volume 32..j. xin, r. tang, j. lee, y. yu, and j. lin.
2020. dee-bert: dynamic early exiting for accelerating bert in-in annual meeting of the association forference.
computational linguistics..a. h. zadeh and a. moshovos.
2020..gobo:quantizing attention-based nlp models for low la-tency and energy efﬁcientpreprintarxiv:2005.03842..inference..o. zafrir, g. boudoukh, p. izsak, and m. wasserblat.
preprint.
2019. q8bert: quantized 8bit bert.
arxiv:1910.06188..d. zhang, j. yang, d. ye, and g. hua.
2018. lq-nets:learned quantization for highly accurate and com-pact deep neural networks.
in european conferenceon computer vision, pages 365–382..w. zhang, l. hou, y. yin, l. shang, x. chen, x. jiang,and q. liu.
2020. ternarybert: distillation-awareultra-low bit bert.
in conference on empirical meth-ods in natural language processing..r. zhao, y. hu, j. dotzel, c. de sa, and z. zhang.
2019. improving neural network quantization with-out retraining using outlier channel splitting.
in in-ternational conference on machine learning..s. zhou, y. wu, z. ni, x. zhou, h. wen, and y. zou.
2016. dorefa-net: training low bitwidth convolu-tional neural networks with low bitwidth gradients.
preprint arxiv:1606.06160..w. zhou, c. xu, t. ge, j. mcauley, k. xu, and f. wei.
2020. bert loses patience: fast and robust inferencewith early exit.
in advances in neural informationprocessing systems..4344a derivation of equation (8).
in this section, we show the derivations to obtaina and b. recall the bwn quantizer introduced insection 2, we have.
thus the solution for b isi| − (cid:80)ni∈i |wt2(|j | + |k|).
b =.
n|i|.
(cid:80).
i=1 |wti|.
,.
ˆwb.
1,i = α1sign(wb.
1,i),.
which satisﬁes b > 0..b implementation details.
where.
α1 =.
similarly,.
where1n.α2 =.
(cid:2) (cid:88).
i∈i.
1,i + ˆwb1(cid:2) (cid:88)n.i∈i1n.(cid:2) (cid:88).
i∈i.
=.
1n.(cid:2) (cid:88).
i∈i.
|awt.
i| +.
|wt.
j + b| +.
(cid:88).
|b|(cid:3)..(cid:88).
i∈j.
i∈k.
ˆwb.
2,i = α2sign(wb.
2,i),.
|(1−a)wt.
i|+.
|−b|+.
(cid:88).
j∈j.
|wt.
k−b|(cid:3)..(cid:88).
k∈k.
according to ˆwt = ˆwbˆwb2,i = 0, we have.
1 + ˆwb.
2, for those ˆwt.
i =.
|awt.
i| +.
|wt.
j + b| +.
(cid:88).
|b|(cid:3).
(cid:88).
j∈j.
k∈k.
|(1−a)wt.
i|+.
| − b|+.
(cid:88).
j∈j.
|wt.
k −b|(cid:3)..(cid:88).
k∈k.
by assuming 0 < a < 1 and b > 0, this can befurther simpliﬁed to.
(cid:88).
a.
|wt.
i|+.
(cid:88).
i∈i.
j∈j.
|wt.
j| = (1−a).
|wt.
i|+.
|wt.
k|,.
(cid:88).
i∈i.
(cid:88).
k∈k.
which gives the solution of a asj| − (cid:80)j∈j |wti∈i |wti|.
i| + (cid:80)2 (cid:80).
i∈i |wt.
a =.
(cid:80).
k∈k |wtk|.
..we empirically ﬁnd the solution satisiﬁes 0 < a <i = ˆwb1. for ˆwt.
i (cid:54)= 0, from ˆwt.
2,i, we have.
1,i + ˆwb.
|wt.
i| = α1 + α2.
(cid:88).
1|i|.
i∈i1(cid:2) (cid:88)n.i∈i(cid:2) (cid:88).
i∈i(cid:2) (cid:88).
i∈i.
=.
+.
=.
1n.1n.+ 2.
(cid:88).
j∈j.
=.
1n.n(cid:88).
(cid:2).
i=1.
|awt.
i| +.
|wt.
j + b| +.
(cid:88).
|b|(cid:3).
(cid:88).
j∈j.
|(1−a)wt.
i| +.
| − b|+.
|wt.
k −b|(cid:3).
(cid:88).
j∈j.
k∈k.
(cid:88).
k∈k.
|wt.
i| +.
|wt.
j| +.
|wtk|.
(cid:88).
j∈j.
(cid:88).
k∈k.
|b| + 2.
(cid:88).
|b|(cid:3).
k∈k.
|wt.
i| + 2(|j | + |k|) · b(cid:3)..b.1 detailed procedure of adaptive splitting.
as mentioned in section 3.2, the adaptive splittingrequires to ﬁrst estimate the quantization sensitivityvector u. we study the sensitivity in two aspects:the transformer parts, and the transformer layers.
for transformer parts, we follow the weight catego-rization in section 2.2: mha-q/k, mha-v, mha-o, ffn-mid and ffn-out.
for each of them, wecompare the performance gap between quantizingand not quantizing that part (e.g., mha-v), whileleavging the rest parts all quantized (e.g., mha-q/k, mha-o, ffn-mid and ffn-out).
simi-larly, for each transformer layer, we quantize alllayers but leave the layer under investigation un-quantized, and calculate the performance gain com-pared with the fully qauntized baseline.
the perfor-mance gain of both transformer parts and layersare shown in figure 7. as can be seen, for trans-former parts, the ffn-mid and mha-q/k rankin the ﬁrst and second place.
in terms of trans-former layers, shallower layers are more sensitiveto quantization than the deeper ones..however, the absolute performance gain may notreﬂect the quantization sensitivity directly, sincetransformer parts have different number of param-eters.
therefore, we divide the performance gainby the number of parameters in that part or layer toobtain the parameter-wise performance gain.
weare thus able to measure the quantization sensitiv-ity of the ith transformer part in the jth trans-former layer by summing their parameter-wise per-formance gain together.
we also apply the sameprocedure to word embedding and pooler layer tootain their sensitivity scores..we are now able to solve equation (11) by dy-namic programming.
the combinatorial optimiza-tion can be viewed as a knapsack problem, wherethe constraint c − c0 is the volume of the knapsack,and the sensitivity scores u are the item values..b.2 hyper-parameter settings.
we ﬁrst perform the two-stage knowledge distilla-tion, i.e., intermediate-layer distillation (int.
dstil.)
and prediction-layer distillation (pred.
dstil.)
on.
4345int.
dstil.
(ternary)321285e-5linear0.11e-210.1.binarybertpred.
dstil.
(ternary)321282e-5linear0.11e-210.1.split ft.(binary)321282e-5linear0.11e-210.1.
6.
1.
3.
6.
1.
3.
6.
1.
3.batch sizesequence lengthlearning rate (lr)lr decaywarmup portionweight decaygradient clippingdropoutepochs w/o da.
-other dataserts.
epochs w da.
-other dataserts.
epochs w/o da-mnli, qqp.
table 7: hyper-parameters for training binarybert onthe glue benchmark at different stages..the ternary model, and then perform ternary weightsplitting followed by ﬁne-tuning (split ft.) withonly prediction-layer distillation after the splitting.
the initial learning rate is set as 5 × 10−5 for theintermediate-layer distillation, and 2 × 10−5 for theprediction-layer distillation, both of which linearlydecay to 0 at the end of training.
we conduct ex-periments on glue tasks both without and withdata augmentation (da) except for mnli and qqpdue to their limited performance gain.
the runningepochs for mnli and qqp are set to 3, and 6 forthe rest tasks if without da and 1 otherwise.
forthe rest hyper-parameters, we follow the defaultsetting in (devlin et al., 2019).
the detailed hyper-parameters are summarized in table 7..c more empirical results.
c.1 performance drop by binarization.
here we provide more empirical results on thesharp drop in performance as a result of bina-rization.
we run multi-bit quantization on thebert model over representative tasks of the gluebenchmark, and activations are quantized in both 8-bit and 4-bit.
we run 10 independent experimentsfor each task except for mnli with 3 runs.
wefollow the same procedure in section 2.1, and thedefault experimental setup in appendix b.2 with-out data augmentation and splitting.
the resultsare shown in figures 8 and 9 respectively.
it canbe found that while the performance drops slowlyfrom full-precision to ternarization, there is a con-sistent sharp drop by binarization in each tasks andon both 8-bit and 4-bit activation quantization.
this.
(a) transformer parts..(b) transformer layers..figure 7: the performance gain of different trans-former parts and layers in descending order.
all num-bers are averaged by 10 random runs with standard de-viations reported..is similar to the ﬁndings in figure 1..c.2 more visualizations of loss landscape.
to comprehensively compare the loss curvatureamong the full-precision, ternary and binary mod-els, we provide more landscape visualizations asidefrom the value layer in figure 2. we extract pa-rameters from mha-k, mha-o, ffn-mid andffn-out in the ﬁrst two transformer layers, and thecorresponding landscape are shown in figure 10,figure 11, figure 12, figure 13 respectively.
weomit mha-q due to page limitation, and also it issymmetric to mha-k with similar landscape ob-servation.
it can be found that binary model havesteep and irregular loss landscape in general w.r.tdifferent parameters of the model, and is thus hardto optimize directly..c.3 ablation of knowledge distillation.
while knowledge distillation on bert has beenthoroughly investigated in (jiao et al., 2020; houet al., 2020; zhang et al., 2020), here we fur-ther conduct ablation study of knowledge distil-lation on the proposed ternary weight splitting.
we compare with no distillation (“n/a”), predic-tion distillation (“pred”) and our default setting(“int.+pred”).
for “n/a” or “pred”, ﬁne-tuning af-ter splitting follows the same setting to their ternary.
4346(a) mnli-m..(b) sst-2..(c) cola..(d) sts-b..(e) mrpc..(f) rte..figure 8: performance of quantized bert with different weight bits and 8-bit activation on the glue benchmarks.
the results are obtained from 10 random seeds except for mnli with 3 seeds..(a) mnli-m..(b) sst-2..(c) cola..(d) sts-b..(e) mrpc..(f) rte..figure 9: performance of quantized bert with different weight bits and 4-bit activation on the glue benchmarks.
the results are obtained from 10 random seeds except for mnli with 3 seeds..strategy qnli sst-2 cola sts-b mrpc rte avg..strategy qnli sst-2 cola sts-b mrpc rte avg..size(mb).
10.6.
11.4.
12.2.
13.0.
13.8.min.
rand.
max.
min.
rand.
max.
min.
rand.
max.
min.
rand.
max.
min.
rand.
max..91.190.891.091.091.091.091.191.191.091.291.291.191.191.591.4.
93.192.792.793.092.993.092.792.992.992.892.993.193.092.992.9.
52.853.353.753.854.754.653.554.153.854.854.156.155.454.755.5.
88.288.288.088.388.488.488.588.588.688.588.488.688.588.588.7.
85.385.586.585.586.586.385.386.086.885.186.086.185.885.086.3.
69.370.071.171.570.871.171.571.871.172.271.870.871.572.272.6.
80.080.180.580.580.780.780.480.480.780.880.881.080.980.881.2.size(mb).
10.6.
11.4.
12.2.
13.0.
13.8.min.
rand.
max.
min.
rand.
max.
min.
rand.
max.
min.
rand.
max.
min.
rand.
max..90.691.190.990.990.891.190.991.290.991.191.391.391.191.391.3.
92.692.792.792.892.892.692.793.092.992.893.092.993.192.992.8.
51.751.353.550.951.752.150.852.052.252.652.953.451.552.353.6.
87.487.687.587.687.587.787.687.687.687.787.887.887.987.788.0.
85.384.884.685.384.685.384.885.185.186.385.885.384.885.185.8.
70.868.270.069.470.470.070.470.070.469.769.769.770.071.170.8.
79.779.379.979.579.679.879.579.879.980.080.180.179.780.180.4.table 8: results on glue development set for adap-tive splitting with 8-bit activation quantization..table 9: results on glue development set for adap-tive splitting with 4-bit activation quantization..training.
“int.+pred” follows our default setting intable .
we do not adopt data-augmentation, andresults are shown in table 10. it can be found that“int.+pred.” outperforms both “n/a” and “pred.”with a clear margin, which is consistent to the ﬁnd-ings in (zhang et al., 2020) that knowledge distilla-tion helps bert quantization..c.4 detailed results of adaptive splitting.
the detailed comparison of our adaptive splittingstrategy against the random strategy (rand.)
andminimal gain strategy (min.)
under different modelsize are shown in table 8 and table 9. it can befound that for both 8-bit and 4-bit activation quan-tization, our strategy that splits the most sensitivemodules mostly performs the best on average undervarious model sizes..kd.
n/apred.
int.+pred.
n/apred.
int.+pred..#bits(w-e-a)1-1-81-1-81-1-81-1-41-1-41-1-4.mnli(-m)83.284.084.282.683.483.9.sst-2 cola mrpc.
92.191.792.690.992.392.3.
49.248.653.439.238.944.4.
82.884.185.576.576.283.3.table 10: ablation study on knowledge distillation..c.5 architecture visualization.
we further visualize the architectures after adaptivesplitting on mrpc in figure 14. for clear presen-tation, we merge all splittable parameters in eachtransformer layer.
as the baseline, 9.8mb refersto no splitting, while 16.5mb refers to splittingall splittable parameters in the model.
accordingto figure 14, with the increasing model size, shal-lower layers are more preferred for splitting thandeeper layers, which is consistent to the ﬁndings infigure 7..4347(a) full-precision model..(b) ternary model..(c) binary model..(d) all together..figure 10: loss landscape visualizations w.r.t mha-k parameters of the 1st and 2nd transformer layers on mrpc..(a) full-precision model..(b) ternary model..(c) binary model..(d) all together..figure 11: loss landscape visualizations w.r.t mha-out parameters of the 1st and 2nd transformer layers onmrpc..(a) full-precision model..(b) ternary model..(c) binary model..(d) all together..figure 12: loss landscape visualizations w.r.t ffn-mid parameters of the 1st and 2nd transformer layers on mrpc..(a) full-precision model..(b) ternary model..(c) binary model..(d) all together..figure 13: loss landscape visualizations w.r.t ffn-out parameters of the 1st and 2nd transformer layers on mrpc..figure 14: the architecture visualization for adaptive splitting on mrpc.
the y-axis records the number of param-eters split in each layer instead of the storage..4348