improving named entity recognition byexternal context retrieving and cooperative learningxinyu wang(cid:5)‡, yong jiang†∗, nguyen bach†, tao wang†,zhongqiang huang†, fei huang†, kewei tu(cid:5)∗(cid:5)school of information science and technology, shanghaitech universityshanghai engineering research center of intelligent vision and imagingshanghai institute of microsystem and information technology, chinese academy of sciencesuniversity of chinese academy of sciences†damo academy, alibaba group{wangxy1,tukw}@shanghaitech.edu.cn, yongjiang.jy@alibaba-inc.com{nguyen.bach,leeo.wangt,z.huang,f.huang}@alibaba-inc.com.
abstract.
recent advances in named entity recogni-tion (ner) show that document-level contextscan signiﬁcantly improve model performance.
in many application scenarios, however, suchcontexts are not available.
in this paper, wepropose to ﬁnd external contexts of a sentenceby retrieving and selecting a set of semanti-cally relevant texts through a search engine,with the original sentence as the query.
weﬁnd empirically that the contextual represen-tations computed on the retrieval-based inputview, constructed through the concatenationof a sentence and its external contexts, canachieve signiﬁcantly improved performancecompared to the original input view based onlyon the sentence.
furthermore, we can improvethe model performance of both input viewsby cooperative learning, a training methodthat encourages the two input views to pro-duce similar contextual representations or out-put label distributions.
experiments show thatour approach can achieve new state-of-the-artperformance on 8 ner data sets across 5 do-mains.1.
1.introduction.
pretrained contextual embeddings such as elmo(peters et al., 2018), flair (akbik et al., 2018) andbert (devlin et al., 2019) have signiﬁcantly im-proved the accuracy of named entity recognition(ner) models.
recent work (devlin et al., 2019;yu et al., 2020; yamada et al., 2020) found thatincluding document-level contexts of the target sen-tence in the input of contextual embeddings meth-ods can further boost the accuracy of ner models.
however, there are a lot of application scenarios.
∗ yong jiang and kewei tu are the corresponding authors.
‡: this work was conducted when xinyu wang was interningat alibaba damo academy..1our code is publicly available at https://github..com/alibaba-nlp/clner..figure 1: a motivating example from wnut-17dataset.
the retrieved texts help the model to correctlypredict the named entities of “democrats” and “republi-can”..in which document-level contexts are unavailablein practice.
for example, there are sometimes noavailable contexts in users’ search queries, tweetsand short comments in various domains such associal media and e-commerce domains.
when pro-fessional annotators annotate ambiguous namedentities in such cases, they usually rely on domainknowledge for disambiguation.
this kind of knowl-edge can often be found through a search engine.
moreover, when the annotators are not sure abouta certain entity, they are usually encouraged to ﬁndrelated knowledge through a search engine (wanget al., 2019).
therefore, we believe that ner mod-els can beneﬁt from such a process as well..in this paper, we propose to improve ner mod-els by retrieving texts related to the input sentenceby an off-the-shelf search engine.
we re-rank the re-trieved texts according to their semantic relevanceto the input sentence and select several top-rankingtexts as the external contexts.
consequently, weconcatenate the input sentence and external con-texts together as a new retrieval-based input viewand feed it to the pretrained contextual embedding.
proceedingsofthe59thannualmeetingoftheassociationforcomputationallinguisticsandthe11thinternationaljointconferenceonnaturallanguageprocessing,pages1800–1812august1–6,2021.©2021associationforcomputationallinguistics1800senatedemocratseliminatedthenuclearoptionwhentheyhadthemajorityafewyearsago,overrepublicanobjections.presidentobamacalledforeliminatingthelegislativefilibusterlastmonth,whichcouldoccurifdemocratsretakethesenate.somerepublicanssayitstimetoundoawrongcommittedbyreid.senaterepublicansareconsideringusingthenuclearoptiontoendapotentialdemocraticfilibusterandconfirmneilgorsuchtothesupremecourt.senaterepublicansdeployedthenuclearoptiononwednesdaytodrasticallyreducethetimeittakestoconfirmhundredsofpresidenttrumpsnominees.label: grouplabel: non entityinput sentence:retrieved texts:eliminatedpresidentoptionmajorityrepublicansomesenate.somerepublicanfilibusterdemocratsasenaterepublicansthedemocraticsomerepublicanssomerepublicanswrongcommittedrepublicansnucleardemocraticgorsuchrepublicanswednesdaypresidentmodule, so that the resulting semantic representa-tions of the input tokens can be improved.
thetoken representations are then fed into a crf layerfor named entity prediction.
a motivating exampleis shown in figure 1..moreover, we consider utilizing the new inputview to improve model performance with the origi-nal input view that does not have external contexts.
this can be useful in application scenarios when ex-ternal contexts are unavailable or undesirable (e.g.,in time-critical scenarios).
to this end, we proposecooperative learning (cl) that encourages the twoinput views to produce similar predictions.
we pro-pose two approaches to cl which minimize eitherthe l2 distances between the token representationsof the two input views or the kullback–leibler(kl) divergence between the prediction distribu-tions of the two input views during training..our experiments show that including the re-trieved external contexts can signiﬁcantly improvethe accuracy of ner models on 8 ner datasetsfrom 5 domains.
with cl, the accuracy of thener models with both input views can be furtherimproved.
our approaches outperform previousstate-of-the-art approaches in each domain..the contributions of this paper are:.
1. we propose a simple and straight-forward wayto improve the contextual representation of aninput sentence through retrieving related textsusing a search engine.
we take the retrievedtexts together with the input sentence as a newretrieval-based view..2. we propose cooperative learning to jointly im-prove the accuracy of both input views in a uni-ﬁed model.
we propose two approaches in clbased on the l2 norm and kl divergence re-spectively.
cl can utilize unlabeled data forfurther improvement..3. we show the effectiveness of our approachesin several ner datasets across 5 domains andour approaches achieve state-of-the-art accuracy.
by leveraging a large amount of unlabeled data,the performance can be further improved..2 framework.
given a sentence of n tokens x = {x1, · · · , xn},the input sentence is fed into a search engine as aquery.
the search engine returns the top k relevanttexts { ˆx1, · · · , ˆxk}.
our framework feeds these.
texts into a re-ranking model.
we concatenate ltop-ranking texts output from the re-ranking modelas the external contexts.
the ner model is fedwith either an input view with the input sentence(original input view) or a concatenation of the in-put sentence and external contexts (retrieval-basedinput view) as input.
the model outputs the predic-tions of labels y = {y1, · · · , yn} at each positionbased on the crf layer.
to further improve themodel, we use cooperative learning to train a uni-ﬁed model that is strong in both input views.
withcl, the model is additionally constrained to be con-sistent in the internal representations or the outputdistributions of both input views.
the architectureof our framework is shown in figure 2..2.1 re-ranking.
given an input sentence as a search query, thesearch engine returns ranked relevant texts.
how-ever, the off-the-shelf search engine is highly opti-mized for a fast speed over a large set of documents,so it may sometimes produce semantically irrele-vant results or rank the results using inaccuraterelevance scores.
since the ner task targets atsemantically recognizing named entities, it is morehelpful if the relevant texts are semantically sim-ilar to the input sentence.
therefore, we need tore-rank the retrieved texts so that the most seman-tically relevant texts are chosen.
we propose toapply bertscore (zhang et al., 2020) to score therelatedness of each retrieved text to the input sen-tence.
bertscore is a language generation metricthat calculates a sum of cosine similarity betweentoken representations of two sentences.
therefore,it is more likely that the search query and the re-trieved texts have strong semantic relations whenbertscore is large.
the token representationsare generated from pretrained contextual embed-dings such as bert.
given the corresponding pre-normalized token representations {r1, · · · , rn} ofthe input sentence x and the pre-normalized tokenrepresentations {ˆr1, · · · , ˆrm} of a certain retrievedtext ˆx with m words, the precision (p), recall (r)of bertscore measure the semantic similaritiesfrom one to another:.
r =.
1n.(cid:88).
xi∈x.
maxˆxj ∈ ˆx.
r(cid:62)i ˆrj; p =.
1m.(cid:88).
ˆxj ∈ ˆx.
maxxi∈x.
r(cid:62)i ˆrj.
we re-rank the retrieved texts by the f1 scoresf1=2 p·rp+r and concatenate l top-ranking texts{ ˆx1, · · · , ˆxl} with f1 scores together as the ex-.
1801figure 2: the architecture of our framework.
an input sentence x is fed into a search engine to get k relatedtexts.
the related texts are then fed into the re-ranking module.
the framework selects l highest ranking relatedtexts output from the re-ranking module and feeds the texts to a transformer-based model together with the inputsentence.
finally, we calculate the negative likelihood loss lnll and lnll-ext together with the cl loss (eitherlcl-l2 or lcl-kl)..ternal contexts:.
˜x = [sep_token; ˆx1; · · · ; ˆxl].
where sep_token is a special token representinga separate of sentences in the transformer-basedpretrained contextual embeddings (for example,“[sep]” in bert)..2.2 ner model.
we solve the ner task as a sequence labeling prob-lem.
we apply a neural model with a crf layer,which is one of the most popular state-of-the-artapproaches to the task (lample et al., 2016; maand hovy, 2016; akbik et al., 2019).
in the se-quence labeling model, the input sentence x is fedinto a transformer-based pretrained contextual em-beddings model to get the token representations{v1, · · · , vn} by vi=embedi(x).
the token rep-resentations are fed into a crf layer to get theconditional probability pθ(y|x):ψ(y(cid:48), y, vi) = exp(wt.
y vi + by(cid:48),y).
(1).
pθ(y|x) =.
n(cid:81)i=1.
ψ(yi−1, yi, vi).
(cid:80)y(cid:48)∈y(x).
n(cid:81)i=1.
ψ(y(cid:48).
i−1, y(cid:48).
i, vi).
where ψ is the potential function and θ representsthe model parameters.
y(x) denotes the set of allpossible label sequences given x. y0 is deﬁnedto be a special start symbol.
wt ∈ rt×d andb ∈ rt×t are parameters computing emission andtransition scores respectively.
d is the hidden sizeof v and t is the size of the label set.
during train-ing, the negative log-likelihood loss for the inputsequence with gold labels y∗ is deﬁned by:.
lnll(θ) = − log pθ(y∗|x).
(2).
in our approach, we concatenate the externalcontexts ˜x at the end of the input sentence x toform the retrieval-based input view.
the tokenrepresentations are now given by:.
{v(cid:48).
1, · · · , v(cid:48).
n, · · · } = embed([x; ˜x]).
the architecture of our ner model is shown infigure 3. now the conditional probability pθ(y|x)becomes pθ(y|x, ˜x).
the loss function in eq.
2becomes:.
lnll-ext(θ) = − log pθ(y∗|x, ˜x).
(3).
2.3 cooperative learning.
in practice, there are two application scenarios forthe ner model: 1) ofﬂine prediction, which re-.
1802search engine1.xxxxxx xxx.2.xxx xxxxxxx3.xxxxxxxx.4.related textsquery rep.retrieved text rep.scorerankingfunctionre-ranking1.xx xxxx.2.xxxxxxxx.3.xxx xxxxxxx4.xxx xx!
xxxexternal contexts re-ranktransformer-based embeddinginputgoldlabelscrflayerthe model learn to predict the token representationswith external contexts even if the contexts are notavailable.
in this approach, d is the l2 norm torepresent the distances of the token representations:.
lcl-l2(θ) =.
||v(cid:48).
i − vi||22.
(5).
n(cid:88).
i=1.
label distributions: since cl enforces the la-bel predictions of both input views to be similar, astraight-forward approach is constraining the labeldistributions predicted by the model to be similarwith the two input views.
in this approach, we usethe kl divergence as the function d. then objec-tive function in eq.
4 becomes the kl divergencebetween pθ(y|x, ˜x) and pθ(y|x):.
lcl-kl(θ)=.
kl(pθ(y|x, ˜x)||pθ(y|x)).
(6).
(cid:88).
y∈y(x).
with the crf layer, the loss function is difﬁcultto calculate because the output space of pθ(y|•)is exponential in size.
to alleviate this issue, wecalculate the kl divergence between the marginaldistributions qθ(yi|x, ˜x) and qθ(yi|x) at each po-sition of the sentence to approximate eq.
6. themarginal distributions can be obtained using theforward-backward algorithm:.
α(yk) =.
ψ(yi−1, yi, vi).
(cid:88).
k(cid:89).
β(yk) =.
ψ(yi−1, yi, vi).
{y0,...,yk−1}.
i=1.
(cid:88).
n(cid:89).
{yk+1,...,yn}.
i=k+1.
qθ(yk|x) ∝ α(yk) × β(yk).
(7).
as mentioned earlier, we do not back-propagate thegradient through pθ(y|x, ˜x).
therefore calculatingthe kl divergence is equivalent to calculating thecross-entropy loss between q(y|x, ˜x) and q(y|x):.
lcl-kl(θ)=−.
qθ(yi|x, ˜x)logqθ(yi|x) (8).
n(cid:88).
t(cid:88).
i=1.
yi=1.
together with the negative log-likelihood losses ineq.
2, 3, the total loss in training is a summationof label losses and a cl loss:.
l(θ) = lnll(θ) + lnll-ext(θ) + lcl(θ).
(9).
where lcl(θ) can be one of the cl loss in eq.
5,8 or a summation of both of them..figure 3: an illustration of our ner model architec-ture.
“[cls]” and “[sep]” are an example of cls tokenand sep token in the embedding..quires high accuracy of the prediction but the pre-diction speed is less emphasized; 2) online serv-ing, which requires a faster prediction speed.
theretrieval-based input view meets the requirementof the ﬁrst scenario for its strong token representa-tions.
however, it does not meet the requirementof the second scenario.
the external contexts areusually signiﬁcantly longer than the input sentenceand a search engine may not meet the latency re-quirements.
these two issues signiﬁcantly slowdown the prediction speed of the model.
therefore,it is essential to improve the accuracy of the orig-inal input views in a uniﬁed model to meet thesetwo scenarios..cooperative learning targets at using theretrieval-based input view to help improve the ac-curacy of the model when there are no externalcontexts available.
cl adds constraints betweenthe internal representations or the output distribu-tions between two input views to enforce that thepredictions of both views should be near.
the ob-jective function of cl is calculated by:.
lcl(θ) = d(h([x; ˜x]), h([x])).
(4).
where d is a distance function between a functionh with different inputs.
because the representationsor the distributions with retrieval-based input vieware usually informative, we do not backpropagatethe gradient through h([x; ˜x]).
we propose twoapproaches for cl..token representations: stronger token repre-sentations usually lead to better accuracy on thetask.
therefore, cl constrains the token represen-tations of two input views to be similar.
this helps.
1803[cls][sep][sep]input sentenceexternal contextstransformer-based embeddingcrf layer# entity labels avg.
length avg.
length w/ context.
wnut-16wnut-17conll-03conll++bc5cdrncbie-commerce.
# train2,3943,39414,98714,9874,5605,42438,959.
# dev1,0001,0093,4663,4664,5819235,000.
# test3,8491,2873,6843,4664,7979405,000.
106442126.
19.4118.4813.6413.6425.9125.012.54.
138.58139.49116.23116.23144.13135.76124.61.table 1: statistics of the dateset split, number of entity types and the average lengths with and without externalcontexts..3 experiments.
3.1 settings.
datasets to show the effectiveness of our ap-proach, we experiment on 8 ner datasets across 5domains:.
• social media: we use wnut-16 (strauss et al.,2016) and wnut-17 (derczynski et al., 2017)datasets collected from social media.
we use thestandard split for these datasets..• news: we use conll-03 english (tjongkim sang and de meulder, 2003) dataset andconll++ (wang et al., 2019) dataset.
theconll-03 dataset is the most popular dataset forner.
conll++ is a revision of the conll-03datasets.
wang et al.
(2019) ﬁxed annotation er-rors on the test set by professional annotators andimproved the quality of the training data throughtheir crossweigh approach.
we use the standarddataset split for these datasets..• biomedical: we use bc5cdr (li et al., 2016)and ncbi-disease (do˘gan et al., 2014) datasets,which are two popular biomedical ner datasets.
we merge the training and development dataas training set following nooralahzadeh et al.
(2019)..• science and technology: we use cbs scitechnews dataset collected by jia et al.
(2019).
thedataset only contains the test set with the same la-bel set as the conll-03 dataset.
we use thedataset to evaluate the effectiveness of cross-domain transferability from the news domain..• e-commerce: we collect and annotate an inter-nal dataset from one anonymous e-commercewebsite.
the dataset contains 25 named entitylabels for goods in short texts.
we also collect300,000 unlabeled sentences for semi-supervisedtraining..annotations of the e-commerce dataset wemanually labeled the user queries through crowd-sourcing from www.aliexpress.com, which is areal-world e-commerce website.
for each query,we asked one annotator to label the entities and askanother annotator to check the quality.
after that,we randomly select 10% of the dataset and ask thethird annotator to check the accuracy.
as a result,the overall averaged query-level accuracy2 is 95%.
the dataset will not be released due to user privacy..retrieving and ranking we use an internale-commerce search engine for the e-commercedataset.
for the other datasets, we use googlesearch as the search engine.
google search is anoff-the-shelf search engine and can simulate theofﬂine search over various domains.
we use sum-marized descriptions from the search results as theretrieved texts3.
as google search limits the max-imal length of searching queries to 32 words, wechunk a sentence into multiple sub-sentences basedon punctuation if the sentence is longer than 30,feed each sub-sentence to the search engine, andretrieve up to 20 results.
we ﬁlter the retrievedtexts that contain any part of the datasets.
our re-ranking module selects top 6 relevant texts4 as theexternal contexts of the input sentence and chunkthe external contexts if the total sub-token lengthsof the input sentence and external contexts exceeds510..model conﬁgurations for the re-ranking mod-ule, we use roberta-large (liu et al., 2019) fortoken representations which is the default conﬁg-uration in the code5 of bertscore (zhang et al.,2020).
for token representations in the ner model,.
2the accuracy of a query counts 1.0 if all the entities in the.
query are correctly recognized and 0.0 otherwise..3if the descriptions are not available, we use the titles of.
the results instead..preliminary experiments..4we determined that 6 is a reasonable number based on.
we show the statistics of the datasets in table 1..5https://github.com/tiiiger/bert_score.
1804we use pretrained bio-bert (lee et al., 2020) fordatasets from the biomedical domain and use xlm-roberta (conneau et al., 2020) for datasets fromother domains..training during training, we ﬁne-tune thepretrained contextual embeddings by adamw(loshchilov and hutter, 2018) optimizer with abatch size of 4. we use a learning rate of 5 × 10−6to update the parameters in the pretrained contex-tual embeddings.
for the crf layer parameters, weuse a learning rate of 0.05. we train the ner mod-els for 10 epochs for the datasets in social mediaand biomedical domains while we train the nermodels for 5 epochs for other datasets for efﬁciencyas these datasets have more training sentences..3.2 results.
we experiment on the following approaches:.
• luke is a very recent state-of-the art model onconll-03 ner dataset proposed by yamadaet al.
(2020).
we use the same parameter settingas yamada et al.
(2020) and use a single sentenceas the input instead of taking document-level con-texts in the dataset as in yamada et al.
(2020) forfair comparison..• w/o context represents training the nermodel without external contexts (eq.
2), whichis the baseline of our approaches..• w/ context represents training the ner.
model with external contexts (eq.
3)..• cl-l2 represents minimizing the l2 distance.
between token representations (eq.
5)..• cl-kl represents minimizing the kl diver-gence (eq.
8) between crf output distributions..besides, we also compare our approaches with pre-vious state-of-the-art approaches over entity-levelf1 scores6.
during the evaluation, our approachesare evaluated using inputs without external con-texts (w/o context) and inputs with them (w/context).
we report the results averaged over 5runs in our experiments.
the results are listed in.
6we do not compare the results from previous work suchas yu et al.
(2020); luoma and pyysalo (2020); yamada et al.
(2020) that utilizes the document-level contexts in conll-03ner here.
we conduct a comparison with these approachesin appendix a..table 27. with the external contexts, our modelswith cl outperform previous state-of-the-art ap-proaches on most of the datasets.
our approachessigniﬁcantly outperform the baseline that is trainedwithout external contexts with only one exception.
comparing with luke, our approaches and ourbaseline outperform luke in all the cases.
thepossible reason is that luke is pretrained only us-ing long word sequences, which makes the modelprone to fail to capture the information of entitiesbased on short sentences8.
for our approaches,with cl, the accuracy can be improved on bothinput views comparing with w/o context andw/ context, which shows adding constraintsbetween the two views during training helps themodel better utilize the original text information.
for the two constraints in cl, we ﬁnd that cl-klis relatively stronger than cl-l2 in a majority ofthe cases..3.3 cross-domain transfer.
for cross-domain transfer, we train the models onthe conll-03 datasets, evaluate the accuracy onthe cbs scitech news dataset, and compare theresults with those in jia et al.
(2019).
we evalu-ate our approaches with each input view and theresults are shown in table 3. our approaches canimprove the accuracy in cross-domain evaluation.
the external contexts during evaluation can help toimprove the accuracy of w/ context.
however,the gap between the two input views for the clapproaches is diminished.
the observation showsthat cl is able to improve the accuracy in cross-domain transfer for both views and eliminate thegap between the two views..3.4 semi-supervised cooperative learning.
cooperative learning can take advantage of largeamounts of unlabeled text for further improvement.
we jointly train on the labeled data and unlabeleddata in training to form a semi-supervised train-ing manner.
during training, we alternate betweenminimizing the loss (eq.
9) for labeled data and thecl loss for unlabeled data (eq.
4).
we conduct theexperiment on the e-commerce dataset as an exam-.
7for the result of bio-bert (lee et al., 2020) on ncbi-disease dataset, we report the results reported in ofﬁcial code(https://github.com/dmis-lab/biobert).
theresults (89.71 in ncbi-disease) reported in the paper usedtoken-level f1 score instead of entity-level f1 score..8we have conﬁrmed with the authors of luke (yamadaet al., 2020) that the accuracy on the conll-03 dataset isconsistent with their experimental results..1805zhou et al.
(2019)nguyen et al.
(2020)nie et al.
(2020)baevski et al.
(2019)wang et al.
(2019)li et al.
(2020)nooralahzadeh et al.
(2019)bio-flair (2019)bio-bert (2020).
luke (2020)w/o contextcl-l2cl-kl.
w/ contextcl-l2cl-kl.
social media.
news.
biomedical.
wnut-16 wnut-17 conll-03 conll++ bc5cdr ncbi.
e-commerce.
55.4352.1055.01------.
54.0456.0457.35†58.14†.
57.43†58.61†58.98†.
42.8356.5050.36------.
---93.5093.4393.33---.
----94.28----.
evaluation: w/o context.
92.4293.0393.0893.21†.
55.2257.8658.68†59.33†evaluation: w/ context60.20†60.26†60.45†.
93.27†93.47†93.56†.
93.9994.2094.38†94.55†.
94.56†94.62†94.81†.
------89.9389.42-.
89.1890.5290.70†90.73†.
90.76†90.99†90.93†.
-------88.8587.70.
87.6288.6589.20†89.24†.
89.01†89.22†88.96†.
---------.
77.6481.4782.43†82.31†.
83.15†83.87†83.99†.
table 2: a comparison among recent state-of-the-art models, the baseline and our approaches.
† represents themodel is signiﬁcantly stronger than the baseline model (w/o context) with p < 0.05 on student’s t test..evaluationscience and technologyw/o context w/ context.
avg.
best.
se59.9561.79.fm59.5460.89.bs60.2062.29.bs+tf-idf59.7160.96.approachjia et al.
(2019)w/o contextw/ contextcl-l2cl-kl.
73.5975.8775.7276.1676.37.
-75.7475.9476.1076.38.table 3: a comparison of different approaches in trans-fer learning.
the models are trained on the conll-03dataset..approachcl-l2cl-klcl–l2+semicl-kl+semi.
evaluationw/o context w/ context.
82.4382.3182.88†82.58†.
83.8783.9983.9284.10.table 4: a comparison between of cl approacheswith and without semi-supervised learning.
semi rep-resents the approaches with semi-supervised learning.
† represents the approach is signiﬁcantly (p < 0.05)stronger than the approach without semi-supervisedlearning with the same input view..ple.
results in table 4 show that the accuracy ofboth input views can be improved especially for theinput without external contexts, which shows theeffectiveness of cl in semi-supervised learning..4 analysis.
we use the wnut-17 dataset in the analysis..table 5: a comparison of different re-ranking ap-proaches by the f1 scores on wnut-17.
se: searchengine.
fm: fuzzy match score.
bs: bertscore..4.1 comparison of re-ranking approaches.
various re-ranking approaches may affect the to-ken representations of the model.
we compare ourapproach with three other re-ranking approaches.
the ﬁrst is the ranking from the search engine with-out any re-ranking approaches.
the second is re-ranking through a fuzzy match score.
the approachhas been widely applied in a lot of previous work(gu et al., 2018; zhang et al., 2018; hayati et al.,2018; xu et al., 2020).
the third is bertscorewith tf-idf importance weighting which makes rarewords more indicative than common words in scor-ing.
we train our models (w/ context) withexternal contexts from these re-ranking approachesand report the averaged and best results on wnut-17 in table 5. our results show that re-ranking withbertscore performs the best, which shows the se-mantic relevance is helpful for the performance.
however, for bertscore with the tf-idf weighting,the accuracy of the model drops signiﬁcantly (withp < 0.05).
the possible reason might be that thetf-idf weighting gives high weights to irrelevanttexts with rare words during re-ranking..1806evaluationw/o context w/ context.
5 related work.
w/ context (ours)w/o contextw/ context (dataset)w/ context (generated)w/ context (random retrieved)w/ context (random data).
wnut-1760.2057.8657.2157.7157.5347.69.table 6: a comparison among different contexts types..approachw/o contextw/ contextw/o clcl-l2 + cl-klcl-l2cl-kl.
57.8657.4658.1458.6958.6859.33.
59.4060.2059.6460.1660.2660.45.table 7: an ablation study of the training and predic-tion of models..4.2 how the context quality affects.
accuracy.
we analyze how the ner model will perform whenthe quality of external contexts varies.
we train andevaluate the ner model in four conditions withvarious contexts.
the ﬁrst one takes each datasetsplit as a document and encodes each sentencewith document-level contexts.
in this case, weencode the document-level contexts following theapproach of yamada et al.
(2020).
the second oneuses gpt-2 (radford et al., 2019) to generate 6 rel-evant sentences as external contexts.
the other twoconditions randomly select from the retrieved textsor the dataset as external contexts.
results in table6 show that all these conditions result in inferioraccuracy comparing with the model without any ex-ternal context.
however, our external contexts aremore semantically relevant to the input sentenceand helpful for prediction..4.3 ablation study.
to show the effectiveness of cl, we conduct threeablation studies for our approach.
the ﬁrst one istraining the ner model based on one view and pre-dict on the other.
the second is jointly training bothviews without the cl loss term (removing lcl(θ)in eq.
9).
the ﬁnal one is using both cl losses totrain the model (lcl(θ) = lcl-l2(θ) + lcl-kl(θ)in eq.
9).
results in table 7 show that the exter-nal context can help to improve the accuracy evenwhen the ner model is trained without the con-texts.
however, when the model is trained withthe external contexts, the accuracy of the model.
drops when predicting the inputs without externalcontexts.
in joint training without cl, the accuracyof the model over inputs without contexts can beslightly improved but the accuracy over inputs withcontexts drops, which shows the beneﬁt of addingcl.
for the model trained with both cl losses, weﬁnd no improvement over the models trained witha single cl loss..named entity recognition named entityrecognition (sundheim, 1995) has been studiedfor decades.
most of the work takes ner asa sequence labeling problem and applies thelinear-chain crf (lafferty et al., 2001) to achievestate-of-the-art accuracy (ma and hovy, 2016;lample et al., 2016; akbik et al., 2018, 2019;wang et al., 2020b).
recently, the improvementof accuracy mainly beneﬁts from stronger tokenrepresentations such as pretrained contextualembeddings such as bert (devlin et al., 2019),flair (akbik et al., 2018) and luke (yamadaet al., 2020).
very recent work (yu et al., 2020;yamada et al., 2020) utilizes the strength ofpretrained contextual embeddings over long-rangedependency and encodes the document-levelcontexts for token representations to achievestate-of-the-art accuracy on conll 2002/2003ner datasets (tjong kim sang, 2002; tjongkim sang and de meulder, 2003)..improving models through retrieval retriev-ing related texts from a certain database (such asthe training set) has been widely applied in taskssuch as neural machine translation (gu et al., 2018;zhang et al., 2018; xu et al., 2020), text generation(weston et al., 2018; kim et al., 2020), semanticparsing (hashimoto et al., 2018; guo et al., 2019).
most of the work uses the retrieved texts to guidethe generation or reﬁne the retrieved texts throughthe neural model, while we take the retrieved textsas the contexts of the input sentence to improvethe semantic representations of the input tokens.
for the re-ranking models, fuzzy match score (guet al., 2018; zhang et al., 2018; hayati et al., 2018;xu et al., 2020), attention mechanisms (cao et al.,2018; cai et al., 2019), and dot products betweensentence representations (lewis et al., 2020; xuet al., 2020) are usual scoring functions to re-rankthe retrieved texts.
instead, we use bertscore tore-rank the retrieved texts instead as bertscoreevaluates semantic correlations between the texts.
1807based on pretrained contextual embeddings..6 conclusion.
multi-view learning multi-view learning isa technique applied to inputs that can be splitinto multiple subsets.
co-training (blum andmitchell, 1998) and co-regularization (sindhwaniand niyogi, 2005) train a separate model for eachview.
these approaches are semi-supervised learn-ing techniques that require two independent viewsof the data.
the model with higher conﬁdenceis applied to construct additional labeled data bypredicting on unlabeled data.
sun (2013) and xuet al.
(2013) have extensively studied various multi-view learning approaches.
hu et al.
(2021) showsthe effectiveness of multi-view learning on cross-lingual structured prediction tasks.
recently, clarket al.
(2018) proposed cross-view training (cvt),which trains a uniﬁed model instead of multiplemodels and targets at minimizing the kl diver-gence between the probability distributions of themodel and auxiliary prediction modules.
compar-ing with cvt, cl targets at improving the accu-racy of two kinds of inputs rather than only one ofthem.
we also propose to minimize the distanceof token representations between different viewsin addition to kl-divergence.
besides, cl utilizesthe external contexts and therefore we do not needto construct auxiliary prediction modules in themodel.
moreover, cvt cannot be directly appliedto our transformer-based embeddings.
finally, ourdecoding layer in the model uses the crf layerinstead of the simple softmax layer as in cvt.
thecrf layer is stronger but more difﬁcult for kl-divergence computation..knowledge distillation knowledge distillation(buciluˇa et al., 2006; hinton et al., 2015) trans-fers the knowledge of “teacher” models to smaller“student” models through minimizing the kl di-vergence of prediction probability distribution be-tween the models.
in speech recognition (huanget al., 2018) and natural language processing (wanget al., 2020a, 2021b), the marginal probability dis-tribution of the linear-chain crf layer has beenapplied to distill the knowledge between teachermodels and student models.
comparing with theseapproaches, our approaches train a single uniﬁedmodel instead of transferring the knowledge be-tween two models.
we also show that the accuracyof both views can be improved with our approaches,unlike in knowledge distillation only the studentmodel is updated and improved..in this paper, we propose to improve the nermodel’s accuracy by retrieving related contextsfrom a search engine as external contexts of theinputs.
to improve the robustness of the modelswhen no external contexts are available, we proposecooperative learning.
cooperative learning addsconstraints between two input views over eitherthe token representations or label distributions ofboth input views to be consistent.
empirical resultsshow that our approach signiﬁcantly outperformsthe baseline models and previous state-of-the-artapproaches on the datasets over 5 domains.
we alsoshow the effectiveness of cooperative learning ina semi-supervised training manner..acknowledgments.
this work was supported by the national natu-ral science foundation of china (61976139) andby alibaba group through alibaba innovative re-search program.
we thank kaibo zhang for hishelp in crawling related texts from google searchand thank jiong cai and zhuo chen for their com-ments and suggestions on writing..references.
alan akbik, tanja bergmann, and roland vollgraf.
2019. pooled contextualized embeddings for namedentity recognition.
in proceedings of the 2019 con-ference of the north american chapter of the asso-ciation for computational linguistics: human lan-guage technologies, volume 1 (long and short pa-pers), pages 724–728, minneapolis, minnesota.
as-sociation for computational linguistics..alan akbik, duncan blythe, and roland vollgraf.
2018. contextual string embeddings for sequencein proceedings of the 27th internationallabeling.
conference on computational linguistics, pages1638–1649, santa fe, new mexico, usa.
associ-ation for computational linguistics..alexei baevski, sergey edunov, yinhan liu, lukezettlemoyer, and michael auli.
2019. cloze-drivenin proceed-pretraining of self-attention networks.
ings of the 2019 conference on empirical methodsin natural language processing and the 9th inter-national joint conference on natural language pro-cessing (emnlp-ijcnlp), pages 5360–5369, hongkong, china.
association for computational lin-guistics..avrim blum and tom mitchell.
1998. combining la-in pro-beled and unlabeled data with co-training.
ceedings of the eleventh annual conference on com-putational learning theory, pages 92–100..1808cristian buciluˇa, rich caruana,.
and alexandruniculescu-mizil.
2006. model compression.
in pro-ceedings of the 12th acm sigkdd internationalconference on knowledge discovery and data min-ing, kdd ’06, pages 535–541, new york, ny, usa.
acm..deng cai, yan wang, wei bi, zhaopeng tu, xi-aojiang liu, and shuming shi.
2019. retrieval-guided dialogue response generation via a matching-in proceedings of theto-generation framework.
2019 conference on empirical methods in natu-ral language processing and the 9th internationaljoint conference on natural language processing(emnlp-ijcnlp), pages 1866–1875, hong kong,china.
association for computational linguistics..ziqiang cao, wenjie li, sujian li, and furu wei.
2018. retrieve, rerank and rewrite: soft templatebased neural summarization.
in proceedings of the56th annual meeting of the association for com-putational linguistics (volume 1: long papers),pages 152–161, melbourne, australia.
associationfor computational linguistics..kevin clark, minh-thang luong, christopher d. man-ning, and quoc le.
2018.semi-supervised se-quence modeling with cross-view training.
in pro-ceedings of the 2018 conference on empirical meth-ods in natural language processing, pages 1914–1925, brussels, belgium.
association for computa-tional linguistics..jiatao gu, yong wang, kyunghyun cho, and vic-tor ok li.
2018. search engine guided neural ma-chine translation.
in proceedings of the aaai con-ference on artiﬁcial intelligence, volume 32..daya guo, duyu tang, nan duan, ming zhou, andcoupling retrieval and meta-jian yin.
2019.learning for context-dependent semantic parsing.
inproceedings of the 57th annual meeting of the as-sociation for computational linguistics, pages 855–866, florence, italy.
association for computationallinguistics..tatsunori b hashimoto, kelvin guu, yonatan oren,and percy liang.
2018. a retrieve-and-edit frame-work for predicting structured outputs.
in proceed-ings of the 32nd international conference on neu-ral information processing systems, pages 10073–10083..shirley anugrah hayati, raphael olivier, pravalika av-varu, pengcheng yin, anthony tomasic, and gra-ham neubig.
2018. retrieval-based neural code gen-eration.
in proceedings of the 2018 conference onempirical methods in natural language processing,pages 925–930, brussels, belgium.
association forcomputational linguistics..geoffrey hinton, oriol vinyals, and jeffrey dean.
2015. distilling the knowledge in a neural network.
in nips deep learning and representation learn-ing workshop..alexis conneau, kartikay khandelwal, naman goyal,vishrav chaudhary, guillaume wenzek, franciscoguzmán, edouard grave, myle ott, luke zettle-moyer, and veselin stoyanov.
2020. unsupervisedcross-lingual representation learning at scale.
inproceedings of the 58th annual meeting of the asso-ciation for computational linguistics, pages 8440–8451, online.
association for computational lin-guistics..zechuan hu, yong jiang, nguyen bach, tao wang,zhongqiang huang, fei huang, and kewei tu.
2021. multi-view cross-lingual structured predic-tion with minimum supervision.
in the joint con-ference of the 59th annual meeting of the associa-tion for computational linguistics and the 11th in-ternational joint conference on natural languageprocessing (acl-ijcnlp 2021).
association forcomputational linguistics..leon derczynski, eric nichols, marieke van erp, andnut limsopatham.
2017. results of the wnut2017shared task on novel and emerging entity recogni-tion.
in proceedings of the 3rd workshop on noisyuser-generated text, pages 140–147, copenhagen,denmark.
association for computational linguis-tics..jacob devlin, ming-wei chang, kenton lee, andkristina toutanova.
2019. bert: pre-training ofdeep bidirectional transformers for language under-in proceedings of the 2019 conferencestanding.
of the north american chapter of the associationfor computational linguistics: human languagetechnologies, volume 1 (long and short papers),pages 4171–4186, minneapolis, minnesota.
associ-ation for computational linguistics..rezarta islamaj do˘gan, robert leaman, and zhiyonglu.
2014. ncbi disease corpus: a resource for dis-ease name recognition and concept normalization.
journal of biomedical informatics, 47:1–10..mingkun huang, yongbin you, zhehuai chen, yanminqian, and kai yu.
2018. knowledge distillation forsequence model.
in proc.
interspeech 2018, pages3703–3707..chen jia, xiaobo liang, and yue zhang.
2019. cross-domain ner using cross-domain language model-in proceedings of the 57th annual meetinging.
of the association for computational linguistics,pages 2464–2474, florence, italy.
association forcomputational linguistics..jihyeok kim, seungtaek choi, reinald kim amplayo,and seung-won hwang.
2020. retrieval-augmentedin proceedingscontrollable review generation.
of the 28th international conference on compu-tational linguistics, pages 2284–2295, barcelona,spain (online).
international committee on compu-tational linguistics..john d. lafferty, andrew mccallum, and fernandoc. n. pereira.
2001. conditional random ﬁelds:.
1809probabilistic models for segmenting and labeling se-quence data.
in proceedings of the eighteenth inter-national conference on machine learning, icml’01, page 282–289, san francisco, ca, usa.
mor-gan kaufmann publishers inc..guillaume lample, miguel ballesteros, sandeep sub-ramanian, kazuya kawakami, and chris dyer.
2016.neural architectures for named entity recognition.
in proceedings of the 2016 conference of the northamerican chapter of the association for computa-tional linguistics: human language technologies,pages 260–270, san diego, california.
associationfor computational linguistics..jinhyuk lee, wonjin yoon,.
sungdong kim,donghyeon kim, sunkyu kim, chan ho so, andjaewoo kang.
2020. biobert: a pre-trained biomed-ical language representation model for biomedicaltext mining.
bioinformatics, 36(4):1234–1240..patrick lewis, ethan perez, aleksandara piktus, fabiopetroni, vladimir karpukhin, naman goyal, hein-rich küttler, mike lewis, wen-tau yih, tim rock-täschel, et al.
2020. retrieval-augmented generationfor knowledge-intensive nlp tasks.
arxiv preprintarxiv:2005.11401..jiao li, yueping sun, robin j johnson, daniela sci-aky, chih-hsuan wei, robert leaman, allan peterdavis, carolyn j mattingly, thomas c wiegers, andzhiyong lu.
2016. biocreative v cdr task corpus:a resource for chemical disease relation extraction.
database: the journal of biological databases andcuration, 2016..xiaoya li, xiaofei sun, yuxian meng, junjun liang,fei wu, and jiwei li.
2020. dice loss for data-imbalanced nlp tasks.
in proceedings of the 58thannual meeting of the association for computa-tional linguistics, pages 465–476, online.
associ-ation for computational linguistics..yinhan liu, myle ott, naman goyal, jingfei du, man-dar joshi, danqi chen, omer levy, mike lewis,luke zettlemoyer, and veselin stoyanov.
2019.roberta: a robustly optimized bert pretraining ap-proach.
arxiv preprint arxiv:1907.11692..ilya loshchilov and frank hutter.
2018. decoupledin international con-.
weight decay regularization.
ference on learning representations..jouni luoma and sampo pyysalo.
2020. exploringcross-sentence contexts for named entity recogni-tion with bert.
in proceedings of the 28th inter-national conference on computational linguistics,pages 904–914, barcelona, spain (online).
interna-tional committee on computational linguistics..xuezhe ma and eduard hovy.
2016..end-to-endsequence labeling via bi-directional lstm-cnns-crf.
in proceedings of the 54th annual meeting ofthe association for computational linguistics (vol-ume 1: long papers), pages 1064–1074, berlin, ger-many.
association for computational linguistics..dat quoc nguyen, thanh vu, and anh tuan nguyen.
2020. bertweet: a pre-trained language modelin proceedings of the 2020for english tweets.
conference on empirical methods in natural lan-guage processing: system demonstrations, pages 9–14, online.
association for computational linguis-tics..yuyang nie, yuanhe tian, xiang wan, yan song, andbo dai.
2020. named entity recognition for so-incial media texts with semantic augmentation.
proceedings of the 2020 conference on empiricalmethods in natural language processing (emnlp),pages 1383–1391, online.
association for computa-tional linguistics..farhad nooralahzadeh, jan tore lønning, and liljaøvrelid.
2019. reinforcement-based denoising ofdistantly supervised ner with partial annotation.
inproceedings of the 2nd workshop on deep learningapproaches for low-resource nlp (deeplo 2019),pages 225–233, hong kong, china.
association forcomputational linguistics..matthew peters, mark neumann, mohit iyyer, mattgardner, christopher clark, kenton lee, and lukezettlemoyer.
2018. deep contextualized word rep-in proceedings of the 2018 confer-resentations.
ence of the north american chapter of the associ-ation for computational linguistics: human lan-guage technologies, volume 1 (long papers), pages2227–2237, new orleans, louisiana.
associationfor computational linguistics..alec radford, jeffrey wu, rewon child, david luan,dario amodei, and ilya sutskever.
2019. languagemodels are unsupervised multitask learners.
openaiblog, 1(8):9..shreyas sharma and ron daniel jr. 2019. bioﬂair:pretrained pooled contextualized embeddings forbiomedical sequence labeling tasks.
arxiv preprintarxiv:1908.05760..vikas sindhwani and partha niyogi.
2005. a co-regularized approach to semi-supervised learningin proceedings of the icmlwith multiple views.
workshop on learning with multiple views.
cite-seer..benjamin strauss, bethany toma, alan ritter, marie-catherine de marneffe, and wei xu.
2016. resultsof the wnut16 named entity recognition sharedtask.
in proceedings of the 2nd workshop on noisyuser-generated text (wnut), pages 138–144, os-aka, japan.
the coling 2016 organizing commit-tee..shiliang sun.
2013. a survey of multi-view machinelearning.
neural computing and applications, 23(7-8):2031–2038..beth m. sundheim.
1995. named entity task deﬁnition,in proceedings of the sixth message.
version 2.1.understanding conference, pages 319–332..1810chang xu, dacheng tao, and chao xu.
2013. aarxiv preprint.
survey on multi-view learning.
arxiv:1304.5634..jitao xu, josep crego, and jean senellart.
2020. boost-ing neural machine translation with similar trans-in proceedings of the 58th annual meet-lations.
ing of the association for computational linguistics,pages 1580–1590, online.
association for computa-tional linguistics..ikuya yamada, akari asai, hiroyuki shindo, hideakitakeda, and yuji matsumoto.
2020. luke: deepcontextualized entity representations with entity-in proceedings of the 2020aware self-attention.
conference on empirical methods in natural lan-guage processing (emnlp), pages 6442–6454, on-line.
association for computational linguistics..juntao yu, bernd bohnet, and massimo poesio.
2020.named entity recognition as dependency parsing.
inproceedings of the 58th annual meeting of the asso-ciation for computational linguistics, pages 6470–6476, online.
association for computational lin-guistics..jingyi zhang, masao utiyama, eiichro sumita, gra-ham neubig, and satoshi nakamura.
2018. guid-ing neural machine translation with retrieved transla-tion pieces.
in proceedings of the 2018 conferenceof the north american chapter of the associationfor computational linguistics: human languagetechnologies, volume 1 (long papers), pages 1325–1335, new orleans, louisiana.
association for com-putational linguistics..tianyi zhang, varsha kishore, felix wu, kilian q.weinberger, and yoav artzi.
2020. bertscore: eval-in internationaluating text generation with bert.
conference on learning representations..joey tianyi zhou, hao zhang, di jin, hongyuan zhu,meng fang, rick siow mong goh, and kennethkwok.
2019. dual adversarial neural transfer forlow-resource named entity recognition.
in proceed-ings of the 57th annual meeting of the associationfor computational linguistics, pages 3461–3471,florence, italy.
association for computational lin-guistics..erik f. tjong kim sang.
2002..introduction to theconll-2002 shared task: language-independentin coling-02: thenamed entity recognition.
6th conference on natural language learning 2002(conll-2002)..erik f. tjong kim sang and fien de meulder.
2003. introduction to the conll-2003 shared task:language-independent named entity recognition.
inproceedings of the seventh conference on natu-ral language learning at hlt-naacl 2003, pages142–147..xinyu wang, yong jiang, nguyen bach, tao wang,fei huang, and kewei tu.
2020a.
structure-levelknowledge distillation for multilingual sequence la-in proceedings of the 58th annual meet-beling.
ing of the association for computational linguistics,pages 3317–3330, online.
association for computa-tional linguistics..xinyu wang, yong jiang, nguyen bach, tao wang,zhongqiang huang, fei huang, and kewei tu.
2021a.
automated concatenation of embeddingsin the joint confer-for structured prediction.
ence of the 59th annual meeting of the associationfor computational linguistics and the 11th interna-tional joint conference on natural language pro-cessing (acl-ijcnlp 2021).
association for com-putational linguistics..xinyu wang, yong jiang, nguyen bach, tao wang,huang zhongqiang, fei huang, and kewei tu.
2020b.
more embeddings, better sequence labelers?
in findings of emnlp, online..xinyu wang, yong jiang, zhaohui yan, zixia jia,nguyen bach, tao wang, zhongqiang huang, feihuang, and kewei tu.
2021b.
structural knowl-edge distillation: tractably distilling informationin the joint conferencefor structured predictor.
of the 59th annual meeting of the association forcomputational linguistics and the 11th interna-tional joint conference on natural language pro-cessing (acl-ijcnlp 2021).
association for com-putational linguistics..zihan wang, jingbo shang, liyuan liu, lihao lu, ji-acheng liu, and jiawei han.
2019. crossweigh:training named entity tagger from imperfect anno-tations.
in proceedings of the 2019 conference onempirical methods in natural language processingand the 9th international joint conference on natu-ral language processing (emnlp-ijcnlp), pages5154–5163, hong kong, china.
association forcomputational linguistics..jason weston, emily dinan, and alexander miller.
2018. retrieve and reﬁne: improved sequence gen-eration models for dialogue.
in proceedings of the2018 emnlp workshop scai: the 2nd interna-tional workshop on search-oriented conversationalai, pages 87–92, brussels, belgium.
association forcomputational linguistics..1811approachyu et al.
(2020)†yamada et al.
(2020)luoma and pyysalo (2020)†wang et al.
(2021a)w/ doc contextw/o contextw/ contextcl-l2cl-kl.
conll-0393.5094.3093.7494.6094.1293.3093.5593.6893.85.table 8: a comparison of retrieved contexts and†: these approaches aredocument-level contexts.
trained on training and development sets..a retrieved contexts versus.
document-level contexts on conll-03.
we conduct a comparison between our retrievedcontexts and the document-level contexts onconll-03 datasets.
in table 8, we report the bestmodel on development set following yamada et al.
(2020).
comparing with previous state-of-the-artapproaches with encoding document-level contexts,our approaches are competitive and even strongerthan some of the previous approaches utilizing max-imal document-level contexts.
comparing withour model trained on document-level contexts (w/doc context), we ﬁnd that there is still a gapbetween the document-level contexts and retrievedcontexts but our cl approaches can reduce the gapbetween these two contexts..1812