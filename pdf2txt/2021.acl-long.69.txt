learning faithful representations of causal graphs.
ananth balashankar.
lakshminarayanan subramanian.
new york universitynew york, ny, usa{ananth,lakshmi}@nyu.edu.
abstract.
learning contextual text embeddings that rep-resent causal graphs has been useful in improv-ing the performance of downstream tasks likecausal treatment effect estimation.
however,existing causal embeddings which are trainedto predict direct causal links, fail to captureother indirect causal links of the graph, thusleading to spurious correlations in downstreamtasks.
in this paper, we deﬁne the faithful-ness property of contextual embeddings to cap-ture geometric distance-based properties of di-rected acyclic causal graphs.
by incorporatingthese faithfulness properties, we learn text em-beddings that are 31.3% more faithful to hu-man validated causal graphs with about 800kand 200k causal links and achieve 21.1% bet-ter precision-recall auc in a link predictionﬁne-tuning task.
further, in a crowdsourcedcausal question-answering task on yahoo!
an-swers with questions of the form “what causesx?”, our faithful embeddings achieved a pre-cision of the ﬁrst ranked answer (p@1) of41.07%, outperforming the existing baselineby 10.2%..1.introduction.
learning distributed word representations that cap-ture causal relationships are useful for real-worldnatural language processing tasks (roberts et al.,2020; veitch et al., 2020; gao et al., 2018, 2019).
approximating the notion of causality with asimilarity-based distance metric using separate vec-tor representations for cause and effect tokens hasled to signiﬁcant improvement in the performanceof downstream tasks like question answering, butcan be too restrictive to generalize over unobservededges in larger causal graphs (sharp et al., 2016).
in downstream causal reasoning based tasks likedialog systems (ning et al., 2018), explanation gen-eration (grimsley et al., 2020), question answering(sharp et al., 2016), it is important to align the.
models with the corresponding causal graph.
how-ever, words that have low cosine similarity capturevarious semantic similarities, like relatedness, syn-onyms, replaceability, or complementarity, but notdirectionality (hamilton et al., 2017).
hence, anysymmetric distance in an embedding space cannotconvey the directed causal semantics for a down-stream task (m´emoli et al., 2016).
in this paper,we overcome these two shortcomings and proposeto optimize for directed faithfulness (spirtes et al.,1993) that word embeddings have to satisfy towardsa causal graph..prior work on capturing sufﬁcient informationfor causal inference tasks from embeddings aimsto directly use them for average treatment effectestimation (veitch et al., 2020).
we are, however,interested in a complementary question: “can welearn word embeddings based on a distance mea-sure that maps the directed distance between nodesin a causal graph to that in the embedding space?”.
unlike prior work, which aims to learn a causalaware embedding restricted to direct link predic-tion (hamilton et al., 2017), we propose faithful-ness constraints so that causal word embeddingsaims to preserve the partial ordering over pairwisedistances in the directed causal graph.
in this paper,to achieve the goal of learning faithful word embed-dings with a vocabulary of more than 100k tokens,we minimize faithfulness violations over pairwisesamples of nodes in the causal graph.
through thisconstrained optimization, we learn an embeddingthat can be applied directly for causal inferencetasks but also generalizes to emergent causal links.
it has been shown that nlp models need to under-stand such causal links that persist in the real worldfor safe deployment (gao et al., 2018; mishra et al.,2019).
embeddings that violate the faithfulnessproperty, can lead to spurious correlations basedon co-location in the embedding space.
for exam-ple, in a yahoo!
causal question-answering task’s.
proceedingsofthe59thannualmeetingoftheassociationforcomputationallinguisticsandthe11thinternationaljointconferenceonnaturallanguageprocessing,pages839–850august1–6,2021.©2021associationforcomputationallinguistics839example: “what causes nosebleed?”: the answerswere “dry air”, “heavy dust”, “damaged nasal cells”and “liver problems”.
if we were to only rely onan undirected association based embeddings, thecauses “dry air” and “liver problems” might benearby (with distance of 2), but would be appro-priately placed far in a directed causality basedembedding space.
to capture such asymmetricproperties, we aim to preserve alignment with thecausal graph by mapping causal links to an asym-metric quasi-pseudo distance measure during train-ing to capture directionality of the causal graph asper figure 1. since human validated causal graphscan be used directly to answer questions of thetype “what causes x?”, we demonstrate the utilityof learning faithful representations by using ourdistance-based features to solve the yahoo!
causalquestion-answering (qa) task.
a causal qa task,unlike a standard qa task, can directly beneﬁt fromincorporating a causal graph into word embeddingsto answer anti-causal queries.
our key contribu-tions are:.
• we deﬁne a faithfulness property for wordembeddings over a causal graph, that capturesgeometric properties of the causal graph, be-yond the direct link prediction by ensuringglobal proximity preservation..• we propose a methodology to learn faithfulembeddings through violation minimizationwhich improves neighborhood detection by31.3%, uniformity by 42.6%, and distancecorrelation by 54.2% using a quasi-pseudodistance metric..• the faithful bert and roberta-based em-beddings we learn, when used as inputs toa causal qa task, increases the precision ofthe ﬁrst ranked answer (p@1) over existingbaselines by 10.2%..2 related work.
2.1 causal model representations.
causal inference, as outlined in (pearl, 2009) for-malizes cause and effects discovered through inter-vention based experiments and communicates themvia directed acyclic graphs.
with the availabilityof large observational datasets for machine learn-ing, various methods and assumptions have beenproposed for learning causal graphs (sch¨olkopf,2019), data fusion and transportability properties.
figure 1: schematic of our faithful bert-based model.
(bareinboim and pearl, 2016; bonner and vasile,2017).
speciﬁcally, our work closely aligns withthe assumption of faithfulness (spirtes et al., 1993),which requires that the observed probability distri-butions of nodes in a causal graph are condition-ally independent as per the links in the graph.
inour work, we use the probability distributions asmodeled in a natural language model (kuhn andde mori, 1990) and align it with the causal linksin a graphical causal model.
we extend the faith-fulness assumption to be reﬂected in embeddingslearnt by a masked language model (devlin et al.,2019; liu et al., 2019b) for downstream tasks.
thisdeﬁnition of faithfulness is different from the oneproposed by (jacovi and goldberg, 2020) used toevaluate models for interpretability of models usedfor downstream tasks.
instead, our work builds onembeddings learnt in (sharp et al., 2016), given acausal model and learn embeddings that are boot-strapped using a small set of cause-effect seeds.
causal models have also been used to learn aux-iliary tasks (feder et al., 2020) using adversarialtraining to ensure that a language model learnscausal-inspired representations.
such approachesuse causal models to learn counterfactual embed-dings invariant to the presence of confounding con-cepts in a sentence, while we encode the geometri-cal properties of causal graphs into the embeddingsand the distance measure to maintain their faithful-ness.
in principle, we adopt a similar approach to(veitch et al., 2020) of ﬁne-tuning towards a causallink prediction task.
this is in contrast with ap-proaches that use energy-based transition vectorsused to represent the cause-to-effect and effect-to-cause links (zhao et al., 2017).
our approachuses regularization constraints similar to the ones.
840c: causal graphm: uniform manifoldxzvwyxyxy?ff(x)f(y)f(v)f(w)f(z)neighborhood based on dmcausal link predictiondcdmcorrelation between dm and dcproposed for information bottlenecks in word em-beddings (li and eisner, 2019; goyal and durrett,2019), text-based games (narasimhan et al., 2015),activation links in neuroscience (chalupka et al.,2016), causal consistency with ordinary differentialequations (rubenstein et al., 2017) and temporalgranger causality (tank et al., 2018).
for an exten-sive survey of using text for causal inference tasks,we refer to (keith et al., 2020)..2.2 graph representation learning.
learning asymmetric transitive graph representa-tions which generalize the causal graph have beenstudied extensively in information retrieval (chenet al., 2007; epasto and perozzi, 2019; li et al.,2019; grover and leskovec, 2016).
they eitherutilize a random walk learning technique (perozziet al., 2014) or matrix factorization techniques (leeand seung, 2000; tenenbaum et al., 2000; wanget al., 2017; mikolov et al., 2013) to incorporatepriors such as the stationary transition probabil-ity matrix, community structure, etc.
more re-cently, (liu et al., 2019a; ostendorff et al., 2019; luet al., 2020) have incorporated knowledge graphs inbert and shown increased accuracy in knowledge-centric nlp tasks.
(zhou et al., 2017; gordo andperronnin, 2011; ou et al., 2016; sun et al., 2018;tang et al., 2015) propose asymmetric higher orderproximity preserving graph embedding methodsby learning separate source and target embeddings.
while we can learn faithful 3-dimension embed-dings for any ﬁxed ﬁnite undirected graph deter-ministically (cohen et al., 1995), ﬁne-tuning pre-trained word embeddings such that they generalizeover all sub-graphs in a directed graph is known tobe a hard graph kernel design problem that scalescubically with the number of nodes (vishwanathanet al., 2010).
our approach builds on efforts toincorporate graph-like structure in bert, but over-comes the issue of learning dual embeddings forcause-effect edges by learning uniﬁed embeddingsfor both cause and effect roles of words.
throughsuch embeddings, we can further aid causal discov-ery that is not yet captured in a graphical notation(chen et al., 2014)..2.3 graph neural networks.
recently, graph neural networks that capture thegraph neighborhood structure have been employedin link prediction (zhu et al., 2020; abu-el-haijaet al., 2017).
in (you et al., 2018), the problem isreduced to that of sequence prediction by reducing.
the graph to breadth-ﬁrst search based determin-istic sequence.
in (li et al., 2018), node embed-dings are updated after several rounds of messagepassing, while in (tu et al., 2016) a variant of therandom walk is incorporated with a max-margindiscriminative constraint.
in (velikovi et al., 2018),models are learned by attending over the neighbor-hood of nodes for context, while (kipf and welling,2016) apply spectral graph convolutions for a self-supervised learning task.
we adopt the incrementalapproach proposed in (velikovi et al., 2018) whichdoes not rely on knowing the entire graph structureapriori and ﬁne-tune on cause-effect pairs for thelink prediction task on a pre-trained bert-basedlanguage model..3 learning faithful embeddings.
3.1 background.
causal inference (pearl, 2009) aims to understandthe cause and effect relationships between events.
learning purely based on correlations in observa-tional data can lead to spurious causal links andcan severely impact downstream tasks.
hence,intervention-based studies are conducted whichcarefully study the impact of a cause using con-trolled randomized experiments and other criterionto learn if links between causes and effects exist us-ing observed data under speciﬁc assumptions.
theﬁndings of such studies are formalized using frame-works like rubin causal models (rubin, 1974),structural causal models (pearl, 2009), etc.
whilethere are differences in abstractions between them,there is formal equivalence (galles and pearl, 1998)in modeling counterfactuals (“what is the effectwhen the cause is intervened?”) and we refer thereader to (pearl and mackenzie, 2018) for a primerin causal modeling..in this paper, we assume a graphical structuralcausal model c (pearl, 2009) is given, whose nodesare linked with directed edges that denote the cause-effect relationship.
for example, the cause-effectof “smoking” causes “cancer”, references to thereal world action of “smoking” in individuals thatleads to the development of “cancer” kind of dis-ease in those individuals.
while causal modelshave a close relationship to the knowledge graph,the links of the causal graph have a well-deﬁnedcausal interpretation that can be validated throughcounterfactual experiments.
in this work, we as-sume the availability of such a causal graph andwe do not aim to build one.
instead, we rely on hu-.
841man annotators who with the help of web crawlers(heindorf et al., 2020a) and other information re-trieval tools (sharp et al., 2016) produce a directedgraphical causal model as shown in figure 1..3.2 faithfulness.
given a graphical causal model c, we now presenta faithfulness property an embedding that aims toclosely align with the causal model has to satisfy.
the faithfulness property was ﬁrst proposed for anytwo causal spaces in (bombelli et al., 2013) in thedomain of quantum physics with the space-timedimension.
inspired by this, we propose an instan-tiation for word embeddings and a correspondinggraphical causal model..deﬁnition 1 (faithfulness).
an embedding f :c → m from a causal set (c, dc) to a vectorspace (m, dm ) is faithful if:.
• ∃λ, ∀x, y.
∈ c, dc(x, y) = 1 ⇔.
dm (f (x), f (y)) ≤ λ.
• f (c) is distributed uniformly.
• ∀x, y, w, z ∈ c, dc(x, y) ≤ dc(w, z) ⇔.
dm (f (x), f (y)) ≤ dm (f (w), f (z)).
note that we use the causal set (c, dc) as a tu-ple of the graphical causal model c and a distancemeasure dc which is used to measure the directeddistance between nodes in the graph.
the vectorspace in which we map our embeddings is alsocharacterized by a tuple (m, dm ), where m is themultidimensional real number space rm, and a dis-tance measure dm which identiﬁes nearby wordsin that vector space.
the three conditions posed bythe faithfulness property, more concretely specifythat there needs to be a real threshold, within theembedding space, which can cover all the neighbor-ing nodes of a word, the embedding space needs tobe uniformly distributed, and ﬁnally, any inequalityrelationships between two distance measures in thecausal graph needs to hold in the embedding spacetoo.
an embedding that satisﬁes this property canthen be used to sufﬁciently represent the causalgraph in downstream tasks..3.2.1 distance measuresthe deﬁnition of faithfulness is dependent on thedistance measure used in both the causal graph andthe embedding domains.
in this work, we assumethat the causal graph is a directed acyclic graph,and hence we measure dc as the shortest directed.
distance (number of edges in an unweighted graph)between two nodes.
if no such path exists betweentwo nodes, we consider the distance to be a largenumber, which in the case of an unweighted graph,can be set to > n, where n is the number of nodesin the acyclic graph.
note that weighted graphs canalso be incorporated with minor changes based onthe maximum path in the graph..however, the distance measure in the embed-ding space faces challenges in evaluation of sim-ple supervised tasks (jastrzebski et al., 2017).
toovercome these, we chose a distance measure thatis closely tied to our faithfulness deﬁnition.
wechose a uniﬁed set of embeddings for both thecause u and effect v, and, if there exists a causaledge from u → v, then we would expect thatdm (f (u), f (v)) << dm (f (v), f (u)).
for thisreason, symmetric distance choices like euclideandistance, cosine similarity are not suitable.
ourchosen distance measure, hence should follow theproperties of quasi-pseudo metrics, deﬁned as fol-lows in (moshokoa, 2005):.
deﬁnition 2 (quasi-pseudo metric).
a measuredm : x × x → [0, ∞) is a quasi-pseudo metric if∀x, y, z ∈ x,.
• dm (x, y) ≥ 0.for x (cid:54)= y.
• dm (x, x) = 0, but dm (x, y) = 0 is possible.
• dm (x, z) ≤ dm (x, y) + dm (y, z).
hence, quasi-psuedo metrics, which do not sat-isfy the symmetry property are best suited to mea-sure the distance between any two embeddings.
wecan generate such metrics, given a measure d. ifthe cause phrase u has p word tokens, and the effectphrase v has q word tokens, we choose the max-matching method given in (xie and mu, 2019) inour deﬁnition of dm by iterating through all pairsof words (vb, ua) : vb (cid:54)= ua.
note that the measured computes the difference between v to u over thetotal m number of dimensions in f (vb), f (ua)..m(cid:88).
j=1.
d(u, v) = mina=1..pb=1..qvb(cid:54)=ua(cid:40).
(fj(vb) − fj(ua)).
(1).
dm (f (u), f (v)) =.
d(u, v),10−d(u,v) − 1, otherwise.
if d(u, v) > 0.
(2).
we chose this deﬁnition, as it is differentiable(except at 0, where we choose the gradient to be.
8420).
also, for each point u in the embedding space,there is a corresponding hyperplane that passesthrough it that deﬁnes the half-space which sepa-rates the reachable nodes v : d(u, v) > 0 - nodeswhich have either an indirect or direct causal linkand the unreachable nodes v : d(u, v) < 0. also,by the property of d(u, v) = −d(v, u), we see thatif v is reachable from u, then u is not reachablefrom v, thus afﬁrming that this is suitable to repre-sent a causal graph that is directed and acyclic..3.3 causal graph link prediction.
there are currently many approaches to learningcausal representations, one which uses a maskedlanguage modeling approach where the word to-kens in the cause are paired with word tokens inthe effect using a skip-gram technique in an unsu-pervised setting.
in the supervised setting, modelsalign the cause-effect embeddings to solve eithera sequence-to-sequence translation task or logisticclassiﬁcation task.
since we aim to capture all thenodes of the causal graph into a single set of wordembeddings, we choose this approach.
further, inthe supervised setting, we make explicit the causalrelationship between cause and effect, thereby cap-turing the directionality of the linkage.
thus, asupervised model could translate a cause to an ef-fect or predict the link that exists from a cause to aneffect.
among these supervised modeling choices,we choose the binary classiﬁcation task of predict-ing if a directed edge exists between two nodesin the causal graph.
this supervised learning isachieved by following the technique of ﬁne-tuningas proposed in (veitch et al., 2020).
formally, givena cause phrase u, an effect phrase v, let an i(u, v)be an edge indicator variable i(u, v) = 1u→v thattakes binary values of {0, 1} based on the existenceof an edge from u → v in the causal graph..pre-trained contextual models: pre-trainedmodels based on transformers like bert (devlinet al., 2019), roberta (liu et al., 2019b) learncontextual embeddings of words or tokens by opti-mizing for the self-supervision task of predictingrandomly masked tokens in a sentence.
these pre-trained embeddings for word tokens have been usedextensively for ﬁne-tuning.
here, we use such ﬁne-tuned models denoted as ˜g to predict the existenceof an edge between the cause and effect u, v, byembedding them into f (u), f (v) respectively andfurther optimizing them in the ﬁne-tuning stage onthe following cross-entropy classiﬁcation loss.
ls = eu,v∼c crossent(i(u, v), ˜g(u, v)).
(3).
3.4 violation minimization.
given the faithfulness deﬁnition, our goal is tolearn an embedding that minimizes the number ofviolations of the faithfulness property.
for each ofthe 3 conditions present in the faithfulness property,we deﬁne how we measure their adherence andincorporate it in the loss function.
in addition to thecausal graph link prediction task, we now presenthow the faithfulness properties are incorporatedthrough regularization constraints..3.4.1 neighborhood.
since we expect a single embedding distancethreshold that perfectly encapsulates the neighbor-hood of a node, we can measure this by varyingdistance thresholds for neighborhood detection andcompute the area under the curve of the precision-recall curve.
since we aim to retain all the neigh-bors of a node in the causal graph within an upperbound of the distance in the embedding space, weadd the sum of the distance between the nodes andtheir neighbors as an l1 regularization loss..ln = e.u∼cv∈n eigh(u).
|dm (f (u), f (v))|.
(4).
3.4.2 uniformity.
since checking for true uniformity can be computa-tionally intractable, we approximate by computingthe per-dimension aggregate of all the word embed-dings and compute the wasserstein distance (olkinand pukelsheim, 1982) between the observed dis-tribution and the expected uniform distribution cen-tered around zero (0m).
since, in the uniformityconstraint, we would expect that the embeddingsare centered around zero, the mean of the embed-dings should be close to zero.
we measure thedistance from this expected centroid and penalizethe model for a high distance.
if cb denote theset of nodes chosen in a batch b, with size |b|, andfj(p) denote the jth dimension of the embeddingof node p, then we present the uniformity regular-ization loss:.
lu =.
m(cid:88).
j=1.
1|b|.
(cid:88).
p∈cb.
fj(p).
(5).
8433.4.3 distance correlationto measure if inequalities between two distancesin the causal graph hold in the embedding space,we measure the pearson correlation coefﬁcient be-tween samples of distances between words in thecausal graph and that of the embeddings.
to ensurethat any two distances sampled from the causalgraph maintain the same inequality in the embed-ding space, we sample random nodes from thecausal graph and compute the empirical pearsoncorrelation coefﬁcient of their distances in the em-bedding space.
a perfect correlation would lead toa coefﬁcient of +1, so we penalize any deviationfrom that ideal correlation and present the distancecorrelation loss:.
lc = 1 − ρdc ,dm.
= 1 −.
cov(dc , dm )σdc σdm.
(6).
note that all the above constraints are at a batchlevel and hence is added on to the batch cross-entropy loss during every back-propagation step.
since the losses are differentiable, we have used theauto-diff capability available in tensorﬂow.
thecontribution of each of the above losses are com-bined using the augmented lagrangian method(hestenes, 1969) and controlled using 3 parametersα, β, γ as follows:.
l = (1 − α − β − γ)ls + αln + βlu + γlc.
(7).
the values of these hyperparameters were cho-sen to be 0.1, 0.15, 0.1 respectively after cross-validation to optimize causal link prediction ac-curacy and faithfulness metrics.
a summary of ourapproach is outlined in algorithm 1..the learning rate a = 0.01, lu, lc are computedper batch by maintaining the required variablesf (u), f (v), dc(u, v), dm (f (u), f (v)) in memory.
these are implemented using tensorﬂow’s eagerexecution framework..4 evaluation.
4.1 causal evidence graphs.
the causal evidence graphs we use contain phraseslike “heavy rainfall” as causes and effects, whichrequire us to learn the combined embeddings ofthe phrases.
restricting ourselves to just individ-ual words would leave out the context requiredto understand the context to understand the cause-effect pairs.
for example, the kind of effects “heavy.
algorithm 1 faithful embedding training.
1: input: pre-trained bert based model ˜g, causal.
graph c, distance measures: dc, dm ,.
2: for e=1..epochs do3:.
l = 0for j=1..b do.
4:.
5:.
6:.
7:.
8:.
9:.
10:.
11:.
u, v ∼ c : (cid:80) 1i(u,v)=0 = (cid:80) 1i(u,v)=1ls += crossent(i(u, v), ˜g(u, v))ln += (cid:80)store f (u), f (v) to update lustore dc(u, v), dm (f (u), f (v)) to up-date lc.
w∈n eigh(u) dm (f (u), f (w)).
end forupdate lu, lc and compute l (eqn 7)backprop ˜g ← ˜g − a( ∂l∂˜g ).
12:13: end for.
rainfall” might have could be different from just“rainfall”.
we thus utilize the contextual embeddingframework used to learn language models in bert(devlin et al., 2019), as a way to learn contex-tual embeddings that align with a given graphicalcausal model.
note that there may be more thanone causal model provided by experts based ontheir domains, and it is important to view our con-tribution as a way to align with domain expertise(for example, medical, legal, privacy, etc) with theirrespective causal models as a common mechanismto represent the said domain knowledge..we use two causal graphs to construct their re-spective faithful embeddings, and demonstrate theutility of the embeddings in downstream tasks.
theﬁrst causal graph we use is identical to the oneused in (sharp et al., 2016), which uses the 815,233cause-effect pairs extracted from the annotated gi-gaword and wikipedia dataset, and an equal num-ber of random relation pairs that are not causal asnegative samples.
the second causal graph is ex-tracted from the web by (heindorf et al., 2020b),who use a bootstrapping approach with the ini-tial pattern of “a causes b” and apply it to theclueweb12 web crawl dataset with 733,019,372english web pages, between february and may2012. from this web crawl, they provide a causalgraph with 80,223 concept nodes and 199,803causal links between the nodes.
this graph hasbeen sampled and validated by human annotatorswith over 96% precision.
for our indirect eval-uation based on downstream question answeringtasks, we use the 3031 causal questions from ya-hoo!
answers corpus (sharp et al., 2016).
these.
844questions are of the form “what causes x?”, andwe use our faithful embeddings as a drop-in re-placement for this causal qa task..4.2 metrics.
evaluating embeddings intrinsically has often ledto varying leaderboards (jastrzebski et al., 2017),hence we evaluate our embeddings based on theirability to map to the cause-effect relationship di-rectly.
we measure the faithfulness of the trainedembeddings, using 3 metrics, one per property asper eqns 4, 5, 6. for the neighborhood condi-tion, we measure the area under the precision-recallcurve as we choose multiple thresholds to deﬁnethe neighborhood in the embedding space to cor-respondingly identify the relevant neighbors in thecausal graph.
for the uniformity condition, wemeasure the means of the per-dimension values ofthe word embeddings and compute the 1st wasser-stein (olkin and pukelsheim, 1982) distance fromthe expected centroid of zero.
we also perform astatistical test for uniform distribution, which mea-sures the mean kolmogorov-smirnov (k-s) teststatistic (daniel, 1990) by bucketing embeddingeach dimension into 10 buckets.
since each dimen-sion’s test statistic can either pass or fail the testbased on the signiﬁcance level, we present the totalnumber of dimensions that pass the test at α = 0.05signiﬁcance level.
finally, to measure the distancecorrelation property, we report the pearson corre-lation coefﬁcient between distances in the causalgraph and the embeddings on a held-out part ofthe causal graph.
for the qa task, we report theprecision-at-one (p@1), the fraction of test sam-ples where the highest ranked answer is relevantand the mean reciprocal rank (mrr) (manninget al., 2008), the inverse of the position of the cor-rect answer in our ranking on the held-out questionset provided by (sharp et al., 2015)..4.3 baselines.
we evaluate our faithful embeddings by compar-ing them against two state-of-the-art approachesdescribed in (sharp et al., 2016) and (veitch et al.,2020).
cembedbi uses a bi-directional model, withthe task of predicting the masked cause and effectword tokens.
this approach uses separate embed-dings for words used as causes and effects.
causal-{bert,roberta} (veitch et al., 2020) uses theﬁne-tuning technique for the binary classiﬁcationof edge detection, similar to ours, on the pre-trainedlarge-uncased model.
we can thus compare the.
embedding.
distance correlationeuclidean cosine quasi-pseudo.
neighborhoodauc-pr.
cembedbicausal-bertcausal-robertafaithful-bertfaithful-roberta.
cembedbicausal-bertcausal-robertafaithful-bertfaithful-roberta.
0.520.610.660.780.81.gigaword causal graph0.480.330.550.400.610.410.630.420.670.45causenet from clueweb12 web crawl0.370.380.360.410.43.
0.340.390.470.550.58.
0.230.250.280.310.37.
0.670.710.760.880.89.
0.540.560.590.680.71.table 1: correlation and neighborhood faithfulnessmeasures of the embeddings trained for both the giga-word causal graph and clueweb12 causenet graph..embedding.
1st-wasserstein mean k-s statistic uniform dimensions (1024).
cembedbicausal-bertcausal-robertafaithful-bertfaithful-roberta.
0.540.450.390.310.30.
0.540.430.380.210.18.
205348385541574.table 2: uniformity measures on the embeddingslearnt for gigaword causal graph..gains we get by incorporating faithfulness condi-tions on the embeddings in downstream tasks..5 results.
5.1 faithfulness.
as shown in tables 1 and 2, our faithful-robertamodel outperforms causal-{bert, roberta} andcembedbi (sharp et al., 2016) on each of the threeproperties of faithfulness, namely the neighbor-hood, uniformity, and distance correlation, by morethan 30%.
additionally, we report the correlationfor euclidean and cosine similarity, despite notusing it to optimize at training time.
faithful ver-sions of the bert and roberta models increasethe area under the curve of the precision-recallcurve in detecting neighboring nodes of the gi-gaword and causenet causal graphs by 21-23%and 17-20% respectively.
in figure 2, we presentthe precision-recall curve when we use the modelsfor ranking causal pairs above non-causal pairs onthe semeval task 8 tuples (hendrickx et al., 2007)by varying the distance threshold in the embeddingspace which outlines the boundary of the neigh-boring nodes in the causal graph.
this increasein accuracy for neighborhood detection indicatesthat incorporating the constraints during trainingtime with our asymmetric causal embedding dis-tance provides beneﬁts in aligning the contextualembeddings as per the causal graph..845ablation study of faithful-bert.
embedding.
cembedbicausal-bertcausal-robertafaithful-bertfaithful-roberta.
w/o neighborhoodw/o uniformityw/o distance correlation.
w/o neighborhoodw/o uniformityw/o distance correlation.
p@1 mrr46.3937.2847.2638.1249.0138.7449.7239.2151.4241.07.
38.5539.0138.28.
39.6940.4339.50.
48.6748.9248.04.
49.3950.0649.28.ablation study of faithful-roberta.
figure 2: precision-recall to detect neighboring nodesin causal graph from the embeddings by applyingthreshold on distance measure.
5.2 qa task.
to evaluate if learning faithful embeddings is usefulfor causal aligned downstream tasks, we evaluatethe ﬁne-tuned embeddings to be directly used forquestion answering.
as used in (fried et al., 2015),we use the maximum, minimum, average distancebetween words of the question and answer wordsand the overall distance between the compositequestion and answer vectors from the embedding.
note that since both cembedbi and causal-{bert,roberta} are trained with cosine similarity inmind, we use the cosine similarity, but for ourfaithful-{bert, roberta} models, the distancemeasure used to rank is the quasi-pseudo metricdeﬁned in def 2. we use these 4 features to trainan svm ranker to re-rank candidate answers pro-vided by the candidate retrieval tool (jansen et al.,2014).
we see in table 3 that faithful-robertaincreases both the precision of the ﬁrst answer pre-dicted by 10.2%, and the mean reciprocal rank by10.8%.
this means that not only is the ﬁrst rankedanswer more causally correct, but the retrieval ofthe correct answer in the top-k positions has im-proved.
this improvement in an out-of-domain qatask by aligning the embeddings to an externallyavailable causal graph demonstrates that beneﬁtsof faithfulness transfer to downstream tasks..5.3 re-alignment towards causation.
to understand the reason behind the improved per-formance, we performed a qualitative inspection of100 randomly sampled word pairs from the giga-word causal graph 1 that are at varying distancesin the original pre-trained embedding and trace.
1https://github.com/ananthnyu/faithful-causal-rep/.
table 3: performance on the qa task in yahoo!
an-swers dataset using the faithful versions of bert androberta incorporating the gigaword causal graph..associated.
causerain → ﬂood.
non-associated war → epidemic.
non-causeaccident → fogearthquake → spring.
table 4: examples of word-pairs chosen to inspectfaithfulness over the gigaword causal graph..how they have re-aligned after ﬁne-tuning with thefaithfulness objective.
we annotate each of theseword-pairs as being either causal or not as shown inthe confusion matrix with examples in table 4. infigure 3, we see re-alignment of these word pairsfrom association based roberta embeddings tothe causally aligned faithful-roberta embeddingspace, that is, causal word pairs (blue and orange)move closer, and non-causal word pairs (green andred) move further based on the quasi-pseudo met-ric dm .
speciﬁcally, the associative but non-causalword pairs (green) have moved further in faithful-roberta, while the non-associative but causalword pairs (orange) have moved closer.
we seethat in the cosine-similarity based roberta, thecausal word pairs had a mean distance of 0.48,while in the quasi-pseudo metric based faithful-roberta, the mean distance between the causalword pairs reduced to 0.28. the distances are nor-malized between 0 and 1 based on the maximumand minimum values of distances (cosine or dm )in the sampled word-pairs..we further analyzed how these associative andcausal re-alignments impacted the causal qa taskby categorizing the word pairs into three types ofvariables - mediators, colliders and confounders.
mediators: for the question, “what causes atornado?”, the answer involves “thunderstorms”,which is a mediator caused by “high pressure”.
we see that “high pressure” is now much closerto “tornado” in faithful-roberta than baselineembeddings.
colliders: for the question, “what.
8460.00.10.20.30.40.50.60.70.80.91.0recall0.00.20.40.60.81.0precisioncembedbicausalbertfaithfulbertcausal-robertafaithful-robertaluca bombelli, johan noldus, and julio tafoya.
2013.lorentzian manifolds and causal sets as partially or-dered measure spaces..stephen bonner and flavian vasile.
2017. causalcorr,.
recommendation..embeddingsforabs/1706.07639..krzysztof chalupka, frederick eberhardt, and pietroperona.
2016. multi-level cause-effect systems.
inartiﬁcial intelligence and statistics, pages 361–369..mo chen, qiong yang, xiaoou tang, et al.
2007. di-in ijcai, pages 2707–.
rected graph embedding.
2712..zhitang chen, kun zhang, laiwan chan, and bernhardsch¨olkopf.
2014. causal discovery via reproducingkernel hilbert space embeddings.
neural computa-tion, 26(7):1484–1517..robert f. cohen, peter eades, tao lin, and frankruskey.
1995. three-dimensional graph drawing.
in graph drawing, pages 1–11, berlin, heidelberg.
springer berlin heidelberg..wayne w. daniel.
1990. kolmogorov-smirnov one-sample test.
applied nonparametric statistics (2nded.
), pages 319–330..jacob devlin, ming-wei chang, kenton lee, andkristina toutanova.
2019. bert: pre-training ofdeep bidirectional transformers for language under-in proceedings of the 2019 conferencestanding.
of the north american chapter of the associationfor computational linguistics: human languagetechnologies, volume 1 (long and short papers),pages 4171–4186, minneapolis, minnesota.
associ-ation for computational linguistics..alessandro epasto and bryan perozzi.
2019. is a sin-gle embedding enough?
learning node representa-tions that capture multiple social contexts.
corr,abs/1905.02138..amir feder, nadav oved, uri shalit, and roi re-ichart.
2020. causalm: causal model explanationthrough counterfactual language models.
corr,abs/2005.13407..daniel fried, tamara polajnar, and stephen clark.
2015. low-rank tensors for verbs in compositionaldistributional semantics.
in proceedings of the 53rdannual meeting of the association for computa-tional linguistics and the 7th international jointconference on natural language processing (vol-ume 2: short papers), pages 731–736, beijing,china.
association for computational linguistics..david galles and judea pearl.
1998. an axiomaticcharacterization of causal counterfactuals.
founda-tions of science, 3(1):151–182..lei gao, prafulla kumar choubey, and ruihonghuang.
2019. modeling document-level causalstructures for event causal relation identiﬁcation.
in.
figure 3: re-alignment of word-pairs from the causal-roberta embedding to our faithful-roberta (bestviewed in color).
causes persistent cough?”, the colliders “smoking”and “asthma” have moved further based on dm infaithful-roberta.
confounders: for questionswith confounders like, “what causes indigestion?”,the confounding links “anxiety → indigestion”, and“anxiety → insomnia” are near, but “insomnia →indigestion”, is far.
this further demonstrates theutility of incorporating faithfulness over multiplenodes of the graph, in addition to pairwise causallink prediction..6 conclusion.
we show that the faithfulness of text embeddingsto a causal graph is important for causal inference-aligned downstream tasks.
by incorporating thethree faithfulness properties of neighborhood, uni-formity, and distance correlation through regular-ization constraints while learning embeddings, weimprove the precision of the ﬁrst ranked answer inthe causal qa task by 10.2%.
we show that thisis due to causal re-alignment of embeddings as peran asymmetric pseudo-distance metric..acknowledgments.
we thank sam bowman for his feedback to thedraft version of this manuscript..references.
sami abu-el-haija, bryan perozzi, and rami al-rfou.
2017. learning edge representations via low-rankasymmetric projections.
proceedings of the 2017acm on conference on information and knowledgemanagement..elias bareinboim and judea pearl.
2016. causal in-ference and the data-fusion problem.
proceedingsof the national academy of sciences, 113(27):7345–7352..8470.00.20.40.60.8normalized cosine-distance in causal-roberta0510150.00.20.40.60.8normalized distance (dm) in faithful-roberta051015number of word pairsassoc/causalnon-assoc/causalassoc/non-causalnon-assoc/non-causalproceedings of the 2019 conference of the northamerican chapter of the association for compu-tational linguistics: human language technolo-gies, volume 1 (long and short papers), pages1808–1817, minneapolis, minnesota.
associationfor computational linguistics..qiaozi gao, shaohua yang, joyce chai, and lucy van-derwende.
2018. what action causes this?
towardsnaive physical action-effect prediction.
in proceed-ings of the 56th annual meeting of the associationfor computational linguistics (volume 1: long pa-pers), pages 934–945, melbourne, australia.
asso-ciation for computational linguistics..a. gordo and f. perronnin.
2011. asymmetric dis-in proceedings oftances for binary embeddings.
the 2011 ieee conference on computer vision andpattern recognition, cvpr ’11, page 729736, usa.
ieee computer society..tanya goyal and greg durrett.
2019. embedding timeexpressions for deep temporal ordering models.
inproceedings of the 57th annual meeting of the asso-ciation for computational linguistics, pages 4400–4406, florence, italy.
association for computationallinguistics..christopher grimsley, elijah mayﬁeld, and juliar.s.
bursten.
2020. why attention is not expla-nation: surgical intervention and causal reasoningin proceedings of the 12thabout neural models.
language resources and evaluation conference,pages 1780–1790, marseille, france.
european lan-guage resources association..aditya grover and jure leskovec.
2016. node2vec:.
scalable feature learning for networks..william l. hamilton, rex ying, and jure leskovec.
2017. representation learning on graphs: methodsand applications.
corr, abs/1709.05584..stefan heindorf, yan scholten, henning wachsmuth,axel-cyrille ngonga ngomo, and martin potthast.
2020a.
causenet: towards a causality graph ex-thetracted from the web.
29th acm international conference on informationamp; knowledge management, cikm ’20, page30233030, new york, ny, usa.
association forcomputing machinery..in proceedings of.
stefan heindorf, yan scholten, henning wachsmuth,axel-cyrille ngonga ngomo, and martin potthast.
2020b.
causenet: towards a causality graph ex-thetracted from the web.
29th acm international conference on informationamp; knowledge management, cikm ’20, page30233030, new york, ny, usa.
association forcomputing machinery..in proceedings of.
iris hendrickx, roser morante, caroline sporleder,and antal van den bosch.
2007.ilk: machinelearning of semantic relations with shallow featuresin proceedings of the fourthand almost no data..international workshop on semantic evaluations(semeval-2007), pages 187–190, prague, czech re-public.
association for computational linguistics..m. r. hestenes.
1969. multiplier and gradient meth-ods.
journal of optimization theory and applica-tions, 4:303–320..alon jacovi and yoav goldberg.
2020. towards faith-fully interpretable nlp systems: how should we de-ﬁne and evaluate faithfulness?
in proceedings of the58th annual meeting of the association for compu-tational linguistics, pages 4198–4205, online.
as-sociation for computational linguistics..peter jansen, mihai surdeanu, and peter clark.
2014.discourse complements lexical semantics for non-in proceedings of thefactoid answer reranking.
52nd annual meeting of the association for com-putational linguistics (volume 1: long papers),pages 977–986, baltimore, maryland.
associationfor computational linguistics..stanisaw jastrzebski, damian leniak,.
and woj-ciech marian czarnecki.
2017. how to evaluateword embeddings?
on importance of data efﬁciencyand simple supervised tasks..katherine keith, david jensen, and brendan o’connor.
2020. text and causal inference: a review of us-ing text to remove confounding from causal esti-in proceedings of the 58th annual meet-mates.
ing of the association for computational linguistics,pages 5332–5344, online.
association for computa-tional linguistics..thomas n kipf and max welling.
2016..semi-supervised classiﬁcation with graph convolutionalnetworks.
arxiv preprint arxiv:1609.02907..roland kuhn and renato de mori.
1990. cache-basednatural language model for speech recognition.
pat-tern analysis and machine intelligence, ieee trans-actions on, 12:570–583..daniel d. lee and h. sebastian seung.
2000. algo-rithms for non-negative matrix factorization.
in pro-ceedings of the 13th international conference onneural information processing systems, nips’00,page 535541, cambridge, ma, usa.
mit press..xiang lisa li and jason eisner.
2019. specializingword embeddings (for parsing) by information bot-tleneck.
corr, abs/1910.00163..yu li, ying wang, tingting zhang, jiawei zhang, andyi chang.
2019. learning network embedding withcommunity structural information.
in ijcai, pages2937–2943..yujia li, oriol vinyals, chris dyer, razvan pascanu,and peter battaglia.
2018. learning deep generativemodels of graphs..848weijie liu, peng zhou, zhe zhao, zhiruo wang, qi ju,haotang deng, and ping wang.
2019a.
k-bert:enabling language representation with knowledgegraph.
corr, abs/1909.07606..yinhan liu, myle ott, naman goyal, jingfei du, man-dar joshi, danqi chen, omer levy, mike lewis,luke zettlemoyer, and veselin stoyanov.
2019b.
roberta: a robustly optimized bert pretraining ap-proach.
corr, abs/1907.11692..zhibin lu, pan du, and jian-yun nie.
2020. vgcn-bert:augmenting bert with graph embedding for text clas-siﬁcation..christopher d manning, hinrich sch¨utze, and prab-hakar raghavan.
2008. introduction to informationretrieval.
cambridge university press..facundo m´emoli, anastasios sidiropoulos, and vijaysridhar.
2016. quasimetric embeddings and theirapplications.
corr, abs/1608.01396..tomas mikolov, kai chen, greg corrado, and jef-efﬁcient estimation of wordarxiv preprint.
frey dean.
2013.representations in vector space.
arxiv:1301.3781..bhavana dalvi mishra, niket tandon, antoine bosse-lut, wen tau yih, and peter clark.
2019. everythinghappens for a reason: discovering the purpose of ac-tions in procedural text..seithuti p moshokoa.
2005. on completeness of quasi-pseudometric spaces.
international journal of math-ematics and mathematical sciences, 2005(18):2933–2943..karthik narasimhan, tejas kulkarni, and regina barzi-lay.
2015. language understanding for text-basedarxivgames using deep reinforcement learning.
preprint arxiv:1506.08941..qiang ning, zhili feng, hao wu, and dan roth.
2018.joint reasoning for temporal and causal relations.
inproceedings of the 56th annual meeting of the as-sociation for computational linguistics (volume 1:long papers), pages 2278–2288, melbourne, aus-tralia.
association for computational linguistics..i. olkin and f. pukelsheim.
1982. the distance be-tween two random vectors with given dispersion ma-trices.
linear algebra and its applications, 48:257– 263..malte ostendorff, peter bourgonje, maria berger, ju-lian moreno-schneider, georg rehm, and belagipp.
2019. enriching bert with knowledge graphembeddings for document classiﬁcation..mingdong ou, peng cui, jian pei, ziwei zhang, andwenwu zhu.
2016. asymmetric transitivity preserv-in proceedings of the 22nding graph embedding.
acm sigkdd international conference on knowl-edge discovery and data mining, kdd ’16, page11051114, new york, ny, usa.
association forcomputing machinery..judea pearl.
2009. causal inference in statistics: an.
overview.
statistics surveys..judea pearl and dana mackenzie.
2018. the book ofwhy: the new science of cause and effect.
basicbooks..bryan perozzi, rami al-rfou, and steven skiena.
2014. deepwalk: online learning of social repre-sentations.
corr, abs/1403.6652..margaret e. roberts, brandon m. stewart, andrichard a. nielsen.
2020. adjusting for confound-ing with text matching.
american journal of politi-cal science, 64(4):887–903..paul k rubenstein, sebastian weichwald, stephanbongers, joris m mooij, dominik janzing, moritzgrosse-wentrup, and bernhard sch¨olkopf.
2017.causal consistency of structural equation models.
arxiv preprint arxiv:1707.00819..donald b rubin.
1974. estimating causal effects oftreatments in randomized and nonrandomized stud-ies.
journal of educational psychology, 66(5):688..bernhard sch¨olkopf.
2019. causality for machine.
learning.
corr, abs/1911.10500..rebecca sharp, peter jansen, mihai surdeanu, and pe-ter clark.
2015. spinning straw into gold: usingfree text to train monolingual alignment models fornon-factoid question answering.
in proceedings ofthe 2015 conference of the north american chap-ter of the association for computational linguis-tics: human language technologies, pages 231–237, denver, colorado.
association for computa-tional linguistics..rebecca sharp, mihai surdeanu, peter jansen, pe-ter clark, and michael hammond.
2016. creatingcausal embeddings for question answering with min-imal supervision..peter spirtes, clark glymour, and richard scheines.
causation, prediction, and search, vol-.
1993.ume 81..jiankai sun, bortik bandyopadhyay, armin bashizade,jiongqian liang, p. sadayappan, and srinivasanparthasarathy.
2018.atp: directed graph em-bedding with asymmetric transitivity preservation.
corr, abs/1811.00839..jian tang, meng qu, mingzhe wang, ming zhang,jun yan, and qiaozhu mei.
2015. line: large-corr,scale information network embedding.
abs/1503.03578..alex tank, ian covert, nicholas foti, ali shojaie, andemily fox.
2018. neural granger causality for non-linear time series..joshua b tenenbaum, vin de silva, and john clangford.
2000. a global geometric frameworkscience,for nonlinear dimensionality reduction.
290(5500):2319–2323..849cunchao tu, weicheng zhang, zhiyuan liu, andmaosong sun.
2016. max-margin deepwalk: dis-criminative learning of network representation.
inproceedings of the twenty-fifth international jointconference on artiﬁcialijcai’16,page 38893895. aaai press..intelligence,.
victor veitch, dhanya sridhar, and david m. blei.
2020. adapting text embeddings for causal infer-ence..petar velikovi, guillem cucurull, arantxa casanova,adriana romero, pietro li, and yoshua bengio.
in international2018. graph attention networks.
conference on learning representations..s.v.n.
vishwanathan, nicol n. schraudolph, risi kon-dor, and karsten m. borgwardt.
2010. graphjournal of machine learning research,kernels.
11(40):1201–1242..xiao wang, peng cui, jing wang, jian pei, wenwuzhu, and shiqiang yang.
2017. community preserv-ing network embedding.
in aaai, volume 17, pages3298239–3298270..zhipeng xie and feiteng mu.
2019. distributed repre-sentation of words in cause and effect spaces.
pro-ceedings of the aaai conference on artiﬁcial intel-ligence, 33(01):7330–7337..jiaxuan you, rex ying, xiang ren, william l.hamilton, and jure leskovec.
2018. graphrnn:corr,a deep generative model for graphs.
abs/1802.08773..sendong zhao, quan wang, sean massung, bing qin,ting liu, bin wang, and chengxiang zhai.
2017.constructing and embedding abstract event causalitynetworks from text snippets.
in proceedings of thetenth acm international conference on web searchand data mining, wsdm ’17, pages 335–344, newyork, ny, usa.
acm..chang zhou, yuqiong liu, xiaofei liu, zhongyi liu,and jun gao.
2017. scalable graph embedding forasymmetric proximity..shijie zhu, jianxin li, hao peng, senzhang wang,philip s. yu, and lifang he.
2020. adversarial di-rected graph embedding..850