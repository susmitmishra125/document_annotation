openmeva: a benchmark for evaluating open-ended story generationmetricsjian guan1, zhexin zhang1, zhuoer feng1, zitao liu2, wenbiao ding2,xiaoxi mao3, changjie fan3 and minlie huang1∗1the coai group, dcst; 1institute for artiﬁcial intelligence; 1state key lab of intelligent technology and systems;1beijing national research center for information science and technology; 1tsinghua university, beijing 100084, china.
2tal education group.
3netease fuxi ai lab.
{j-guan19,zx-zhang18,fze17}@mails.tsinghua.edu.cn, zitao.jerry.liu@gmail.com,dingwenbiao@100tal.com, {maoxiaoxi,fanchangjie}@corp.netease.com,aihuang@tsinghua.edu.cn.
abstract.
automatic metrics are essential for developingnatural language generation (nlg) models,particularly for open-ended language genera-tion tasks such as story generation.
however,existing automatic metrics are observed to cor-relate poorly with human evaluation.
the lackof standardized benchmark datasets makes itdifﬁcult to fully evaluate the capabilities ofa metric and fairly compare different metrics.
therefore, we propose openmeva, a bench-mark for evaluating open-ended story genera-tion metrics.
openmeva provides a compre-hensive test suite to assess the capabilities ofmetrics, including (a) the correlation with hu-man judgments, (b) the generalization to dif-ferent model outputs and datasets, (c) the abil-ity to judge story coherence, and (d) the ro-bustness to perturbations.
to this end, open-meva includes both manually annotated sto-ries and auto-constructed test examples.
weevaluate existing metrics on openmeva andobserve that they have poor correlation withhuman judgments, fail to recognize discourse-level incoherence, and lack inferential knowl-edge (e.g., causal order between events), thegeneralization ability and robustness.
ourstudy presents insights for developing nlgmodels and metrics in further research..1.introduction.
signiﬁcant advances have been witnessed in manynlg tasks with pretraining models (devlin et al.,2019; brown et al., 2020).
however, existing gen-eration models are still far behind the human-levelperformance to generate reasonable texts, particu-larly for open-ended generation tasks such as storygeneration (fan et al., 2018; guan et al., 2020).
one critical obstacle is the lack of powerful met-rics for measuring the quality of generation..∗corresponding author.
the standard paradigm for evaluating nlg met-rics is to calculate the correlation with human judg-ments on manually annotated datasets (tao et al.,2018; sellam et al., 2020).
recent studies havediscovered that the existing automatic metrics maycorrelate poorly with human judgments (liu et al.,2016; guan and huang, 2020).
unfortunately, thelack of benchmark datasets makes it challengingto completely assess the capabilities of a metricand fairly compare different metrics.
firstly, anno-tated datasets usually contain innate data bias andannotation bias.
secondly, summarizing the per-formance with a single aggregate statistic (e.g., acorrelation score) makes it difﬁcult to probe whichaspects a metric can successfully capture and whichcan not.
therefore, many alternative approacheshave been proposed to evaluate nlg metrics, suchas measuring the robustness to adversarial exam-ples (zhang* et al., 2020), and the generalization toquality-biased data (sellam et al., 2020).
however,these approaches only focus on an individual capa-bility or a single task, thereby failing to fully revealthe strengths and weaknesses of a nlg metric..therefore, we propose openmeva, a bench-mark for open-ended story generation metricsevaluation.
we ﬁrst collect a manually annotatedstory dataset (mans).
the stories are generated byvarious generation models trained on two widelyused story corpora, rocstories (mostafazadehet al., 2016) and writingprompts (fan et al.,2018).
therefore, mans supports to evaluate met-rics in terms of not only the correlation with hu-man judgments, but also the generalization w.r.tmodel drift (generations from different models) anddataset drift (examples from different datasets)..in addition, openmeva also includes an auto-constructed story dataset (autos) to test the ro-bustness and the ability to judge story coherence,namely, the semantic relations and discourse struc-tures in the context.
we construct autos by per-.
proceedingsofthe59thannualmeetingoftheassociationforcomputationallinguisticsandthe11thinternationaljointconferenceonnaturallanguageprocessing,pages6394–6407august1–6,2021.©2021associationforcomputationallinguistics6394turbing human-written stories, and test the metricsin each single aspect (e.g., the ability to recognizeinconsistency) by validating the input-output behav-ior (ribeiro et al., 2020).
through such behavioraltests, autos can support to reveal potential issuesof metrics in multiple aspects, which would be nottraceable in machine-generated examples in mans.
we conduct extensive experiments to assess thecapabilities of existing automatic metrics on open-meva.
we ﬁnd that state-of-the-art metrics stillcorrelate poorly (less than 0.5) with human judg-ments on mans.
and it is difﬁcult for the learn-able metrics to generalize to model or dataset drift.
through tests on autos, we observe that mostmetrics can perform well in recognizing incoher-ence at token level (e.g., unrelated entities) andsentence level (e.g., semantic repetition), but failto recognize discourse-level incoherence (e.g., in-consistency) and lack understanding of inferentialknowledge (e.g., temporal order between events).
besides, we also show that existing metrics are notrobust to a small number of typos and synonymsubstitution.
these ﬁndings may inspire new direc-tions for developing nlg models and designingmetrics in future research..we also provide an open-source toolkit whichimplements various metrics, and therefore supportsthe comparison and analysis of metrics.
in addition,the toolkit provides data perturbation techniquesfor generating customized test cases beyond au-tos, which can facilitate fast development of newautomatic metrics1..2 related work.
various automatic metrics have been proposedfor evaluating language generation.
they can beroughly divided into referenced, unreferenced, andhybrid metrics, according to whether relying onhuman-written references when calculating the met-ric score.
referenced metrics usually measurethe similarity between a sample and some refer-ences based on word-overlap (e.g., bleu (pap-ineni et al., 2002), rouge (lin, 2004)) or wordembedding (e.g., bertscore (zhang* et al., 2020),moverscore (zhao et al., 2019)).
however, ref-erenced metrics were reported to correlate poorlywith human judgments in open-ended generationtasks (liu et al., 2016) due to the one-to-manyissue (zhao et al., 2017).
to address the issue, un-.
1all the tools, data, and evaluation scripts are available at.
https://github.com/thu-coai/openmeva.
referenced metrics were proposed to measure thequality of a sample without any reference, such asperplexity, discriminator-based metric (kannan andvinyals, 2017), union (guan and huang, 2020)and grade (huang et al., 2020).
besides, hy-brid metrics combine referenced and unreferencedmetrics (e.g., ruber and its variant (tao et al.,2018; ghazarian et al., 2019)) or learn from thehuman-annotated score (e.g., adem (lowe et al.,2017), bleurt (sellam et al., 2020))..recently, there have been many criticisms forexisting metrics.
garbacea et al.
(2019) showedthe poor generalization of discriminator-based met-rics.
sai et al.
(2019) demonstrated adem is notrobust to simple attacks such as simple word sub-stitution or random word shufﬂe.
however, thesecriticisms only focus on individual metrics or ca-pabilities.
notably, ribeiro et al.
(2020) proposeda framework checklist to evaluate different capa-bilities of general language understanding modelsby validating the input-output behavior.
the testcases are created from scratch or by perturbingan existing dataset.
similar to checklist, open-meva also employs automatically constructingexamples for behavioral tests.
however, checklistonly focuses on single sentences, thereby lackingthe ability to test models in understanding longtexts with many discourse-level features (e.g., tem-poral relationship).
moreover, the testing methodsof checklist are not directly applicable for nlgmetrics.
speciﬁcally, checklist measures the per-formance of a model by calculating the failure ratebetween discrete model prediction and automaticlabels.
such failure rates are ineffective for mea-suring metrics since most metric scores are con-tinuous.
to address the above issues, we proposeperturbation techniques and testing methods moreapplicable for story generation metrics..3 data collection.
we collect mans and autos based on rocsto-ries (roc for short) (mostafazadeh et al., 2016)and writingprompts (wp for short) (fan et al.,2018), which are commonly used for story gen-eration (guan et al., 2020; fan et al., 2019) andevaluation (guan and huang, 2020).
roc con-tains 98,162 ﬁve-sentence commonsense storieswith about 50 words, while wp consists of 303,358pairs of prompts and stories, which are usually un-constrained on writing topics.
we retain about 250words (with correct sentence boundary) for stories.
6395figure 1: overview for the manual annotation interface.
story a gets two points in overall quality since it getsthree points deducted for its repetitive plot and chaotic scene.
the ratings of annotator #5 for the current storygroup are rejected because of the low score for the human-written story and the high score for the negative sample..in wp.
although we only consider the stories in thetwo corpora, openmeva is designed to measurethe capability of nlg metrics to evaluate generallinguistic features such as coherence, which maypertain to other stories.
besides, our idea that build-ing datasets by manual annotation or automaticconstruction can be easily extended to evaluate spe-ciﬁc aspects for other types of stories..3.1 mans: manually annotated stories.
we collect mans to assess the correlation of met-rics with human judgments and the generalizationability when evaluating machine-generated stories.
we randomly split roc and wp by 90%/5%/5%for training/validation/test of the generation mod-els.
we regard the ﬁrst sentence for roc andthe prompt for wp as input.
after training, wegenerate stories based on the test sets.
then, weresort to amazon mechanical turk (amt) forhuman judgments of the generated stories.
weconsider various generation models including aseq2seq model (sutskever et al., 2014), fusion(fan et al., 2018), plan&write (yao et al., 2019),the ﬁne-tuned gpt-2 (radford et al., 2019) andknowledge-enhanced gpt-2 (guan et al., 2020).
these models cover diverse network architecturesand different levels of the generation ability, whichsupport to evaluate the generalization to exampleswith different model biases or quality levels..manual annotation we present the manual an-notation interface in figure 1. in each human intel-ligence task (hit) of amt, we show workers theinput of a story paired with seven stories including(a) ﬁve stories generated by the above ﬁve mod-els, (b) the human-written story, and (c) a negativeexample constructed by perturbing a story (e.g.,repetition, shufﬂing) sampled from the test sets..then we ask workers to compare the overall qual-ity of the seven stories2, and rate each story witha 5-point likert scale.
we reject an hit if theworker rates the human-written story lower thanfour points or rates the negative example higherthan two points.
through the quality control mech-anism, we ﬁltered about 38.7% assignments forroc and 75.4% for wp.
finally, we ensure thatthere are ﬁve valid ratings for each generated story,and we regard the average rating as the ﬁnal humanjudgment..considering that overall quality is often too ab-stract to measure, we follow previous recommen-dations (belz and hastie, 2014; van der lee et al.,2020) to decide the overall quality by summariz-ing multiple separate criteria.
we ask the workersto decide the rating of a story based on a pointdeduction policy.
speciﬁcally, a story should getpunishment in points if it contains errors such asrepetitive plots, unrelated events and conﬂictinglogic, or globally chaotic scenes, which are com-monly observed in existing nlg models (guanand huang, 2020) (several examples shown in theappendix).
intuitively, the policy can alleviate thetendency to give high scores and ensure that thejudgment standard of workers is as consistent aspossible during annotation.
to avoid introducingextra bias in the policy, we do not impose the re-striction on workers to exactly match the rating inoverall quality with the deducted points..data statistics we randomly sampled 200 sto-ries from test sets of roc and wp for story.
2we do not ask annotation in other aspects (e.g., interest-ing) since previous work (novikova et al., 2017) has noted thatthe annotation scores on different aspects are highly correlatedin spite of careful design.
and computing correlation scoresin the entangled aspects would be unconvincing..6396ratingstoryaoverallquality:12345reasons:•localerrors•repetitiveplots(repeatingsimilartexts)-1-2•unrelatedevents(withunrelatedeventstotheinputorwithinitsowncontext)-1-2•conflictinglogic(againstcommonsenseorwithwrongcausalortemporalrelationship)-1-2•globalerrors•chaoticscenes(difficulttounderstandasawhole)-2instruction1.readthefollowingandsevenstoriesfromto.2.comparingthestorieswithoneanotherintermsofoverallquality.3.rateeachstoryin1-5range(5isthebest).andchooseyourreasons.storya:hebought himselfa...storyb:he wantedto try...storyg:it has been apretty great ...input:my dad likes watermelon very much....inputstoryastorygonehuman-writtenstoryfivegeneratedsamplesonenegativesamplesampleratinghuman-written5negative1............✅annotator#1annotator#5❌...ratingstoryg...sampleratinghuman-written2negative4............aspects.
selecting coherent examples.
creating incoherent examples.
lexicalrepetition.
semanticrepetition.
characterbehavior.
commonsense.
consistency.
relatedness.
causalrelationship.
temporalrelationship.
all the human-written stories..all the human-written stories..(1) repeating a 4-gram (with “and” inserted before it).
(2) repeating a sentence.
case: ... he stepped on the stage and stepped on the stage ....(1) repeating a sentence with its paraphrase by back translation3.
to ensure the semantic similarity and avoid muchword overlap, we only use those paraphrases whose moverscore is larger than 0.4 and bleu-1 is less than 0.6 withthe original sentences.
we present some examples for paraphrase generation in the appendix.
case: he hired an attorney.
he employed a lawyer ... (moverscore=0.57, bleu-1=0.40).
stories with passive voice or with personalpronouns (e.g., “him”, “their”) for multiplecharacters.
case: ... it asked john if john could ....(1) reordering the subject and object of a sentence.
(2) substituting a personal pronoun with another one whichrefers to other characters.
and we do no change the grammatical case of the substituted pronoun (e.g., “my” can besubstituted with “his” but not with “him”).
case: john ↔ asked it if john could ....stories with both the head and tail entitiesof a triple in conceptnet4(speer and havasi,2012)..(1) substituting 10% entities with its neighboring entity in conceptnet.
case: today is halloween (cid:55)→ christmas .
jack is excited to go trick or treating ... (“halloween” and “christmas”has the relation “antonyms”).
stories with negated words (e.g., “not”,“hardly”, “inactive”).
case: ... tom decided not to give up ....(1) substituting words with the antonyms (e.g., “happy” vs. “upset”), which are retrieved from wordnet (miller,(2) inserting or1998).
the antonyms are converted to the same form (e.g., verb tense) with the original words.
deleting negated words for 20% sentences.
case: she agreed (cid:55)→ disagreed to get vaccinated ....stories with weak token-level semantic re-latedness within the context5.
case: craig was diagnosed with cancer.
he decided to ﬁght it ....(1) substituting 25% nouns or verbs randomly (with correct word forms).
(2) substituting a sentence randomly withanother sampled from the dataset.
case: craig was diagnosed with cancer.
he decided to ﬁght it.
(cid:55)→ kelly wanted to put up the christmas tree.
hetried several different approaches and medications.
eventually it went into remission ....stories with causality-related words (e.g.,“because”).
case: ... the sky is clear.
so he can see it ..(1) reordering the cause and effect, which should be two individual sentences or two clauses connected by a causality-related conjunction; (2) substituting the causality-related words with the antonyms (e.g., “reason” vs. “result”).
case: ... he can see it.
↔ so the sky is clear..stories with time-related words (e.g., “be-fore”,“then”).
case: ... tina then learnt her lesson..(1) reordering two sequential events, which should be two individual sentences or two clauses connected by a time-related conjunction.
(2) substituting the time-related words with the antonyms (e.g., “after” vs. “before”).
case: ... after (cid:55)→ before eating one bite i was not hungry..table 1: examples for the discrimination test to evaluate the ability to judge story coherence in different aspects.
italic words indicate the crucial keywords for the corresponding aspects.
the coherent examples are selected fromthe human-written stories.
the incoherent examples are created by perturbation including insertion, deletion andreordering, where (1) and (2) mean different perturbation techniques..aspects.
perturbations.
synonyms.
substituting a word with its synonym retrieved from wordnet.
case: ... i purchased (cid:55)→ bought my uniforms..paraphrases.
substituting a sentence with its paraphrase.
case: he hired an attorney (cid:55)→ he employed a lawyer.
punctuation.
deleting inessential punctuation marks (e.g., commas).
case: ... eventually, he became hungry ....contraction.
contracting or expanding contraction.
case: ... i’ll (cid:55)→ will have to keep waiting ....typos.
swapping two adjacent characters; repeating or deleting a character.
we modify less than 2% words of an example to avoid much noise.
case: ... an orange (cid:55)→ ornage broke her nose..table 2: examples for the invariance test to evaluatethe robustness to perturbations in different aspects..generation, respectively.
therefore, mans con-tains 2 × 200 × 5 = 2, 000 annotated machine-generated stories, paired with corresponding inputsand human-written references.
the krippendorff’sα (krippendorff, 2018) of the human judgmentsis 0.77/0.71 for roc/wp, indicating a moderateinter-annotator agreement (α ∈ [0.67, 0.8]).
weshow more statistical details in the appendix..3.2 autos: auto-constructed stories.
while improving correlation with human judg-ments is the ultimate goal for developing automaticmetrics, merely relying on limited annotated datamay make the true evaluation performance overes-timated (ribeiro et al., 2020).
besides, a machine-.
generated story may contain multiple entanglederrors (e.g., repetition, unrelatedness), which donot support individual tests for metrics.
therefore,we propose to evaluate the capabilities of metricswith auto-constructed test examples (i.e., autos),each of which is created to focus on a single aspect.
we construct autos based on the human-writtenstories in the test sets of roc and wp..aspects we argue that an ideal metric for evalu-ating open-ended language generation should haveat least the following capabilities: (a) the abilityto judge story coherence, which requires recogniz-ing lexical and semantic repetition, unreasonablecharacter behavior (e.g., chaotic coreferences),violation of common sense (e.g., “trick or treat”on “christmas”), poor consistency and related-ness, incorrect causal and temporal relationship;and (b) the robustness to perturbations, such assubstituting with synonyms or paraphrases, delet-ing unimportant punctuation marks, contracting.
2we generate paraphrases based on the back translation.
augmentation system of uda (xie et al., 2020)..3conceptnet is a knowledge base including millions ofcommonsense triples like (h, r, t), meaning that the headentity h has a relation r with the tail entity t. note that weonly regard nouns and verbs as entities..4we regard the stories with maximum inter-sentencemoverscore less than 0.1 as those which have weak token-level semantic relatedness within the context..6397full expressions or expanding contractions, andadding typos.
tests in these aspects require met-rics to fully understand the linguistic features attoken level (e.g., synonyms), sentence level (e.g.,semantic similarity), and discourse level (e.g., con-text relatedness in content and proper sentence or-ders), and possess knowledge about common sense,causality, etc., which are usually not traceable inmachine-generated stories.
although these aspectsare not exhaustive, it is a starting point for furtherresearch.
table 1 and 2 present some examples forthe two capabilities, respectively..test types we create examples with differenttest types to evaluate the above capabilities of met-rics.
firstly, we evaluate the ability to judge storycoherence by the discrimination test, which re-quires metrics to distinguish human-written coher-ent examples from incoherent ones.
we createeach incoherent example by applying perturbationwithin a single aspect.
besides, we also select dif-ferent human-written stories as coherent examplesfor different aspects, as shown in table 1. for ro-bustness assessment, we expect the metric scoresto remain the same with certain perturbations, i.e.,the invariance test, as shown in table 2..however, the perturbation may inevitably intro-duce grammar errors.
to alleviate the issue, weﬁlter out those ungrammatical examples in autosexcept for those used to evaluate robustness to ty-pos using an automatic grammaticality classiﬁer.
we present the statistics of autos together with theevaluation results in table 6/ 7 for the discrimina-tion/invariance tests, respectively.
and we providemore details about the construction of autos andthe grammaticality classiﬁer in the appendix..4 evaluation.
we evaluated existing metrics on openmeva, andanalyzed the strengths and weaknesses with exten-sive experiments..4.1 evaluated metrics.
we experimented with existing metrics of differ-ent types as follows: (a) referenced metrics: theword-overlap based metric sentence bleu score(geometric mean from 1-gram to 4-gram) (papineniet al., 2002), the contextualized embedding basedmetrics, bertscore-f1 (zhang* et al., 2020).
(b) unreferenced metrics: perplexity6 esti-.
6we follow guan and huang (2020) to take the minus of.
perplexity to ensure a higher value means better quality..mated by gpt-2 (radford et al., 2019) (includingpretrained gpt-2 and gpt-2 fine-tunedon the training sets); the self-supervised metricun i o n (guan and huang, 2020).
(c) hybrid met-rics: ruber-bert (ghazarian et al., 2019) thatimproves ruber with contextualized embeddingsfrom bert (devlin et al., 2019)..in addition, we also reported the performanceof the unreferenced version in ruber-bert, de-noted as ru-bert.
and we present results withmore metrics in the appendix..4.2 correlation with human judgments.
we ﬁrst calculate the pearson correlation coefﬁ-cient between metric scores and human judgmentson mans.
besides, we also evaluate metrics on theother four evaluation sets constructed for individ-ual error types (described in section 3.1) based onmans.
each of them contains all the reasonablesamples and the unreasonable samples of some er-ror type.
a reasonable sample means its overallquality score larger than four points.
for an un-reasonable sample, we decide it is of some errortype if there is only one error type annotated by atleast three of ﬁve annotators.
we assign the reason-able and unreasonable samples with binary labels1 and 0, respectively, and calculate the correlationbetween metric scores and the binary labels on thefour evaluation sets..we summarize the correlation results in table 3.as previous studies (guan and huang, 2020) ob-served, unreferenced metrics are more competi-tive for evaluating open-ended language generationthan referenced ones.
ppl (f) performs better thanppl (p) on roc but not on wp, which may bebecause stories in roc are created artiﬁcially andhence differ from the general language distributionduring pretraining gpt-2.
furthermore, measuringinput-output relatedness (ru-bert) is not enoughfor language generation evaluation.
union outper-forms other metrics in overall quality assessmentsince it learns to distinguish human-written storiesfrom negative samples with more error types.
in-terestingly, it seems easier for the metrics to recog-nize surface errors (e.g., repetitive plots) or seriousglobal errors (e.g., chaotic scenes).
however, thebest correlation with human judgments is stillfairly low, and it is difﬁcult to recognize unrelat-edness and conﬂicting plot.
the results indicatethe huge room to improve the metrics..to further examine to what extent the improve-.
639846 reasonable samples +.
35 reasonable samples +.
metrics.
bleubertscore-f1.
ppl (p)ppl (f)ru-bertun i o n.ruber-bert.
overall.
1,000.
-0.02390.1271∗.
0.2547∗0.2817∗0.0830∗0.4119∗.
0.1434∗.
rept22.
0.05200.1396.
-0.10750.21520.11600.4517∗.
0.0813.roc.
unrel319.
0.01920.1240.
0.11050.1380∗0.08770.2000∗.
0.1453∗.
conf39.
0.11340.0626.
0.13540.26430.11030.2107.chao87.
0.01560.2283∗.
0.5248∗0.5910∗0.17740.4695∗.
overall.
1,000.
-0.05370.0329.
0.3033∗0.2952∗0.1666∗0.3256∗.
0.2116∗.
wp.
unrel330.
-0.04210.0446.
0.1853∗0.1720∗0.07930.1738∗.
rept23.
0.11880.1198.
0.02190.01790.09360.3283.conf83.chao24.
-0.08750.0189.
-0.14510.0634.
0.21880.19170.01620.1914.
0.4428∗0.3182∗0.00770.3967∗.
0.1173.
0.1723.
0.0716.
0.1132.
0.0721.
0.1493.table 3: pearson correlation with human judgments on mans.
ppl (p) and ppl (f) mean perplexity esti-mated by pretrained and ﬁne-tuned gpt-2, respectively.
the best performance is highlighted in bold.
the resultscontain the correlation with human judgments on all the annotated samples in mans (overall), and the correla-tion with the binary labels on reasonable samples and unreasonable ones of different error types.
the error typesinclude repetitive plots, unrelated events, conflicting logic and chaotic scenes.
the numbers in the table headerdenote the number of corresponding stories.
* indicates the correlation score is signiﬁcant (p-value<0.01)..figure 2: correlation between human judgment difference (x-axis) and metric score difference (y-axis).
top: roc,bottom: wp.
we only show the situation in the positive x-axis, since it is centrosymmetric with that in the negativex-axis.
human (s)/metric (s) means the difference of human judgment/metric score is signiﬁcant (p<0.01, t-test),while (ns) means insigniﬁcant difference.
r2 is the coefﬁcient of determination for linear regression (red line), andis exactly the square of the pearson correlation coefﬁcient between the x-axis and y-axis..ment in an automatic metric corresponds to the im-provement in human judgments, we calculate thecorrelation between human judgment differenceand metric score difference (mathur et al., 2020).
speciﬁcally, we sort the 1,000 stories (for rocand wp, respectively) in mans by the human judg-ments, and then select consecutive 200 stories fromthe beginning and repeat the selection with a stride10. we ﬁnally get (1, 000 − 200)/10 = 80 storysets7.
we decide the human judgment or metricscore of each set by averaging that of the storiesin the set.
we calculate the human judgment dif-ference and metric score difference between anytwo sets of them (80 × 80 = 6, 400 pairs totally),and present the correlation between the differencesin figure 2 for several typical metrics.
we cansee that a signiﬁcant improvement in the metricsusually corresponds to a signiﬁcant improvement.
7we do not construct the sets by randomly sampling since.
it would be difﬁcult to cover wide enough quality levels..in human judgments (cyan/dark gray part in fig-ure 2).
however, both an insigniﬁcant drop andimprovement in a metric could correspond to a sig-niﬁcant improvement in human judgments.
andworse, the improvement in human judgments mayhave a wide range, which is particularly evident forbertscore-f1 and ruber-bert (yellow/lightgray part in figure 2).
that is, if an nlg modelachieves insigniﬁcantly better scores in the twometrics, it is quite possible that the model per-forms signiﬁcantly worse in human judgments.
the situation is improved when using ppl (f) andunion, suggesting that they may be better to mea-sure language generation..4.3 generalization ability.
it is extremely important for learnable metrics todeal with model drift and dataset drift (garbaceaet al., 2019; sellam et al., 2020).
speciﬁcally, ageneralizable metric should be able to evaluate dif-.
6399bertscore-f1bertscore-f1ruber-bertruber-bertppl(f)ppl(f)unionunion𝑟!=0.31𝑟!=0.51𝑟!=0.76𝑟!=0.92𝑟!=0.26𝑟!=0.48𝑟!=0.58𝑟!=0.63humanjudgmentdifferencemetricscoredifferencehuman(s),metric(s)human(s),metric(ns)human(ns),metric(ns)ferent nlg models since the generation quality orinductive bias can vary signiﬁcantly across models.
besides, we also expect a metric to reliably eval-uate output from different datasets even withoutre-training.
therefore, we assess the generaliza-tion ability of learnable metrics, including ppl (f),ru-bert and union, which are ﬁne-tuned on thetraining sets of roc and wp, respectively..to assess the generalization to model drift, wetest the metrics on stories generated by ﬁve afore-mentioned models in mans, respectively (200 sto-ries by each model).
table 4 presents the perfor-mance, which varies considerably with models.
ru-bert only achieves a good correlation on thosestories with poor relatedness (e.g., seq2seq onwp).
ppl (f) and union perform comparably butneither do well in evaluating all the nlg models..metrics.
p&w fusion.
gpt-2.
kg-g.c ppl (f)ru-bertorun i o n.pw.ppl (f)ru-bertun i o n.s2s.
0.14-0.020.12.
0.110.18∗0.09.
0.22-0.080.28∗.
0.150.080.02.
0.120.040.10.
0.050.140.15.
0.140.120.15∗.
0.120.070.04.
0.25∗0.060.32∗.
0.130.020.15∗.
table 4: pearson correlation with human judgmentsto assess generalization to output from different mod-els including seq2seq (s2s), plan&write (p&w), fu-sion, gpt-2, kg-gpt-2 (kg-g).
the best perfor-mance among the metrics is highlighted in bold..to assess the generalization to dataset drift, weﬁrst trained the metrics on roc and then directlyused them to evaluate stories from wp, and viceversa.
as shown in table 5, all the metrics dropssigniﬁcantly in correlation when used for the otherdataset due to the difference in length and topic.
ppl (f) and union also have similar performancedrops but are more generalizable.
the results sug-gest existing metrics fall short of generalization..metrics.
ppl(f)ru-bertun i o n.train: roc.
train: wp.
test: roc.
test: wp.
test: roc.
test: wp.
0.2817∗0.0830∗0.4119∗.
0.2423∗0.03790.2287∗.
0.2470∗0.0891∗0.2128∗.
0.2952∗0.1666∗0.3256∗.
table 5: pearson correlation with human judgments toassess generalization to samples from different datasets.
the best performance between two test datasets (eachrow) for each metric is highlighted in bold..4.4 ability to judge story coherence.
we assess the ability of the unreferenced metrics8to judge story coherence based on the discrimina-tion test set of autos.
we assign each test examplewith a binary label (1/0 for the coherent/incoherentexample).
then we calculate the correlation be-tween metric scores and the binary labels on thetest examples of different aspects.
the higher corre-lation means the better ability to judge coherence..table 6 presents the correlation results.
we sum-marize the results as follows: (1) ppl is ineffec-tive to recognize repetition errors.
the observa-tion is accordant with the results on mans (table 3).
ppl (p) even has a signiﬁcantly negative correla-tion with labels in lexical and semantic repetition.
(2) ppl (f) and union have better average per-formance than others.
ru-bert performs worstin almost all the aspects.
union has the highestaverage performance by a large margin on rocbut underperforms ppl (f) on wp, indicating theshortage of union when evaluating longer sto-ries.
besides, the results show that a powerfullanguage model may also be a powerful evalua-tor (if we can alleviate its preference for repetitivetexts).
(3) existing metrics perform well in rec-ognizing incoherence at token and sentence lev-els.
for example, they seem to be able to recognizeunreasonable behavior for a certain character, andpossess some commonsense knowledge about en-tity relations.
however, in this work the proposedperturbation can not fully cover all possible inco-herence in these aspects, which would be regardedas the future work.
(4) the metrics still struggleto recognize discourse-level incoherence.
specif-ically, it is difﬁcult to recognize inconsistent eventswhen we insert or delete negated words, and un-derstand the semantic relatedness across sentences.
besides, they also lack inferential knowledge aboutthe causal and temporal relationship.
the observa-tions are also accordant with the results in table 3where unrelated events and conﬂicting logic cannot be well recognized.
in conclusion, we revealvarious issues of the existing metrics by the isolat-ing behavioral testing, while they achieve moderatecorrelation with human judgments on mans..8it is meaningless to evaluate referenced or hybrid metricson autos since the reference text of a positive example isexactly itself, which is an unfair case for unreferenced metrics..6400metrics.
lexicalrepetition.
semanticrepetition.
characterbehavior.
commonsense.
consistency.
relatedness.
causalrelationship.
temporalrelationship.
roc.
coheincohe.
ppl (p)ppl (f)ru-bertun i o n.wp.
coheincohe.
ppl (p)ppl (f)ru-bertun i o n.4,7364,049.
-0.1886∗0.0287∗0.01210.5454∗.
9,9229,022.
-0.0886∗-0.0467∗0.00980.2302∗.
4,7363,243.
-0.0719∗0.2315∗0.0543∗0.5631∗.
9,9228,381.
-0.0461∗0.0986∗0.01080.2150∗.
1,022266.
0.2547∗0.3595∗0.0671∗0.3191∗.
3,911173.
0.2077∗0.2783∗-0.02990.3044∗.
1,921448.
0.4246∗0.3976∗0.0478∗0.3965∗.
2,052235.
0.4782∗0.4871∗-0.01830.3940∗.
4553,666.
0.1357∗0.1630∗0.0194∗0.1676∗.
2,9146,239.
0.2575∗0.3420∗0.01370.3661∗.
5633,570.
0.0744∗0.14580.0764∗0.2045∗.
497851.
0.1328∗0.2297∗0.00540.2107∗.
476410.
0.1002∗0.1568∗-0.00750.1425∗.
4,5523,057.
0.0355∗0.1597∗-0.01430.0514∗.
2,3761,799.
0.1759∗0.20070.0135∗0.1769∗.
9,4087,092.
0.0763∗0.1788∗0.00420.0459∗.
table 6: pearson correlation with automatic labels on the discrimination test set of autos.
the higher correlationindicates the better ability to judge story coherence in different aspects.
the best performance is highlighted inbold.
cohe and incohe stand for the number of coherent and incoherent examples, respectively..metrics.
roc.
synonym.
paraphrase.
punctuation.
contraction.
typo.
human.
dis.
human.
dis.
human.
human.
dis.
human.
dis.
3,777.
2,395.
3,174.
2,194.
574.
1,602.
1,208.
4,755.
4,763.ppl (p)ppl (f)ruberu-bertun i o n.0.3162∗0.3309∗0.0307∗0.2187∗.
0.2515∗0.2521∗0.0290∗0.1169∗.
0.1450∗0.2742∗0.02550.1112∗.
0.0916∗0.2022∗0.02630.0399∗.
0.0922∗0.1475∗0.00520.0818∗.
-0.05570.05040.00640.0275.
-0.0522∗0.0331∗0.00710.0251.
0.4124∗0.4540∗-0.01120.6021∗.
dis.
171.
0.08560.0996-0.01400.1375∗.
wp.
6,961.
35,90.
7,881.
2,576.
4,535.
2,287.
8,731.
4,522.
15,073.
15,082.ppl (p)ppl (f)ruberu-bertun i o n.0.2174∗0.2964∗-0.00130.1077∗.
0.1822∗0.1747∗0.00040.0843∗.
0.0910∗0.2273∗0.00000.0389∗.
0.0617∗0.1020∗0.00000.0292∗.
0.2690∗0.3822∗-0.0256∗0.2182∗.
0.2178∗0.2515∗-0.0308∗0.2224∗.
-0.0222∗0.0851∗-0.00120.0185∗.
-0.01570.0682∗-0.00430.0173∗.
0.3983∗0.4603∗0.01330.3812∗.
0.2616∗0.2973∗0.00420.4606∗.
0.3885∗0.4043∗0.0154∗0.3208∗.
table 7: pearson correlation with automatic labels on the invariance test set of autos.
the smaller absolutevalue of correlation indicates the better robustness.
the best performance is highlighted in bold and the secondbest is underlined.
the numbers in the roc/wp rows indicate how many human-written stories (human) andincoherent samples from the discrimination test set (dis) are perturbed..4.5 robustness evaluation.
a reliable metric should produce similar judgmentsfor an example with simple perturbations or attacksin the input.
therefore, it is essential to evaluatethe robustness of metrics.
we test the robustnesson the invariance test set of autos.
we assigneach example with a binary label (1/0 for the orig-inal/perturbed example).
then, we calculate thecorrelation between metric scores and the binary la-bels.
the original examples can be sampled eitherfrom human-written stories or from the incoherentexamples in the discrimination test set..table 7 shows the robustness results.
it is notsurprising that ru-bert has the “best robustness”since the perturbations hardly inﬂuence the input-output relatedness.
the result validates the related-ness is merely one side for evaluating nlg, but notmeans that it is a promising direction for develop-ing robust metrics9.
ppl is not robust to synonym.
9we can imagine that a constant metric has the perfectrobustness to any perturbations, but is useless for evaluation..substitution because the low-frequency words in-troduced by the perturbations (e.g., from “happy”to “joyful”) can cause signiﬁcant change in ppl.
union has better robustness on average thanks tothe robust contextualized representation of bert.
furthermore, both ppl and union perform betterin contraction than in other aspects.
however, theyare very sensitive to a small number of typos (lessthan 2% words) because typos may bring someout-of-vocabulary words.
although the issue iscommon for almost all the (sub)word-based met-rics, it is still important to handle typos since theyare also common in human writing..5 conclusion.
we present openmeva, a benchmark to compre-hensively assess capabilities of metrics for evalu-ating open-ended story generation.
openmevaincludes test examples which are created by eitherannotating machine-generated stories or perturb-ing human-written stories in terms of each single.
6401aspect.
we evaluate a number of existing metricson openmeva and analyze their performance oneach capability extensively.
experiments demon-strate that existing metrics still correlate weaklywith human judgments, fail to recognize discourse-level incoherence, and lack inferential knowledge,generalization and robustness.
our study revealsthe weaknesses of existing metrics and may inspirenew research on designing nlg metrics..the datasets, data augmentation tools, and imple-mented metrics in this paper can facilitate furtherresearch on language generation and evaluation..acknowledgments.
this work was supported by national keyr&d program of china, under grant no.
2020aaa0104500.
this work was jointly sup-ported by the nsfc projects (key project with no.
61936010 and regular project with no.
61876096),and the guoqiang institute of tsinghua university,with grant no.
2019gqg1 and 2020gqg0005.
we would also like to thank the anonymous review-ers for their invaluable suggestions and feedback..ethics statement.
we build openmeva based on two existing pub-lic story datasets rocstories (roc) and writing-prompts (wp), which are widely used for storygeneration and evaluation.
we resorted to amazonmechanical turk (amt) for manual annotation ofstories in mans.
we did not ask about personal pri-vacy or collect personal information of annotatorsin the annotation process.
we hired ﬁve annotatorsand payed each annotator $0.05 and $0.1 for anno-tating each story in roc and wp, respectively.
wedecided the payment according to the average storylength of two datasets.
we admit that there may bestill unpredictable bias in mans even though wehave asked three experts to review all the annotatedstories..besides, we selected or constructed the test ex-amples in autos based on general linguistic fea-tures.
we did not adopt any selecting strategies orperturbation techniques which may introduce extrabias into autos..references.
satanjeev banerjee and alon lavie.
2005. meteor: anautomatic metric for mt evaluation with improvedcorrelation with human judgments.
in proceedings.
of the acl workshop on intrinsic and extrinsic evalu-ation measures for machine translation and/or sum-marization, pages 65–72..anja belz and helen hastie.
2014. towards compara-tive evaluation and shared tasks for nlg in interactivesystems.
in natural language generation in inter-active systems, pages 302–350.
cambridge univer-sity press..tom b. brown, benjamin mann, nick ryder, melaniesubbiah, jared kaplan, prafulla dhariwal, arvindneelakantan, pranav shyam, girish sastry, amandaaskell, sandhini agarwal, ariel herbert-voss,gretchen krueger, tom henighan, rewon child,aditya ramesh, daniel m. ziegler, jeffrey wu,clemens winter, christopher hesse, mark chen,eric sigler, mateusz litwin, scott gray, benjaminchess, jack clark, christopher berner, sam mc-candlish, alec radford, ilya sutskever, and darioamodei.
2020. language models are few-shot learn-ers..jacob devlin, ming-wei chang, kenton lee, andkristina toutanova.
2019. bert: pre-training ofdeep bidirectional transformers for language under-standing.
in proceedings of the 2019 conference ofthe north american chapter of the association forcomputational linguistics: human language tech-nologies, volume 1 (long and short papers), pages4171–4186..angela fan, mike lewis, and yann dauphin.
2018. hi-in proceedingserarchical neural story generation.
of the 56th annual meeting of the association forcomputational linguistics (volume 1: long papers),pages 889–898..angela fan, mike lewis, and yann dauphin.
2019.in pro-strategies for structuring story generation.
ceedings of the 57th annual meeting of the asso-ciation for computational linguistics, pages 2650–2660, florence, italy.
association for computa-tional linguistics..cristina garbacea, samuel carton, shiyan yan, andqiaozhu mei.
2019.judge the judges: a large-scale evaluation study of neural language modelsin proceedings offor online review generation.
the 2019 conference on empirical methods in nat-ural language processing and the 9th internationaljoint conference on natural language processing(emnlp-ijcnlp), pages 3959–3972..sarik ghazarian, johnny wei, aram galstyan, andnanyun peng.
2019. better automatic evaluation ofopen-domain dialogue systems with contextualizedin proceedings of the workshop onembeddings.
methods for optimizing and evaluating neural lan-guage generation, pages 82–89..jian guan, fei huang, zhihao zhao, xiaoyan zhu, andminlie huang.
2020. a knowledge-enhanced pre-training model for commonsense story generation.
transactions of the association for computationallinguistics, 8:93–108..6402jian guan and minlie huang.
2020. union: an un-referenced metric for evaluating open-ended storygeneration.
in proceedings of the 2020 conferenceon empirical methods in natural language process-ing, emnlp 2020, online, november 16-20, 2020,pages 9157–9166.
association for computationallinguistics..john morris, eli liﬂand, jin yong yoo, jake grigsby,di jin, and yanjun qi.
2020. textattack: a frame-work for adversarial attacks, data augmentation, andin proceedings of theadversarial training in nlp.
2020 conference on empirical methods in natu-ral language processing: system demonstrations,pages 119–126..ari holtzman, jan buys, li du, maxwell forbes, andyejin choi.
2020. the curious case of neural text de-in international conference on learn-generation.
ing representations..lishan huang, zheng ye, jinghui qin, liang lin, andxiaodan liang.
2020. grade: automatic graph-enhanced coherence metric for evaluating open-in proceedings of thedomain dialogue systems.
2020 conference on empirical methods in natu-ral language processing, emnlp 2020, online,november 16-20, 2020, pages 9230–9240.
associ-ation for computational linguistics..anjuli kannan and oriol vinyals.
2017. adversar-ial evaluation of dialogue models.
arxiv preprintarxiv:1701.08198..klaus krippendorff.
2018. content analysis: an intro-.
duction to its methodology.
sage publications..chris van der lee, albert gatt, emiel van miltenburg,and emiel krahmer.
2020. human evaluation of au-tomatically generated text: current trends and bestpractice guidelines.
computer speech & language,page 101151..chin-yew lin.
2004. rouge: a package for auto-matic evaluation of summaries.
in text summariza-tion branches out, pages 74–81, barcelona, spain.
association for computational linguistics..chia-wei liu, ryan lowe, iulian vlad serban, mikenoseworthy, laurent charlin, and joelle pineau.
2016. how not to evaluate your dialogue system:an empirical study of unsupervised evaluation met-in proceed-rics for dialogue response generation.
ings of the 2016 conference on empirical methodsin natural language processing, pages 2122–2132..ryan lowe, michael noseworthy, iulian vlad ser-ban, nicolas angelard-gontier, yoshua bengio, andjoelle pineau.
2017. towards an automatic turingtest: learning to evaluate dialogue responses.
inproceedings of the 55th annual meeting of the as-sociation for computational linguistics (volume 1:long papers), pages 1116–1126..nitika mathur, timothy baldwin, and trevor cohn.
2020. tangled up in bleu: reevaluating the eval-uation of automatic machine translation evaluationin proceedings of the 58th annual meet-metrics.
ing of the association for computational linguistics,pages 4984–4997, online.
association for computa-tional linguistics..george a miller.
1998. wordnet: an electronic lexical.
database.
mit press..nasrin mostafazadeh, nathanael chambers, xiaodonghe, devi parikh, dhruv batra, lucy vanderwende,pushmeet kohli, and james allen.
2016. a cor-pus and cloze evaluation for deeper understandingof commonsense stories.
in proceedings of naacl-hlt, pages 839–849..jekaterina novikova, ondˇrej dušek, amanda cercascurry, and verena rieser.
2017. why we need newin proceedings of theevaluation metrics for nlg.
2017 conference on empirical methods in naturallanguage processing, pages 2241–2252..kishore papineni, salim roukos, todd ward, and wei-jing zhu.
2002. bleu: a method for automatic eval-in proceedings ofuation of machine translation.
the 40th annual meeting on association for compu-tational linguistics, pages 311–318.
association forcomputational linguistics..alec radford, jeffrey wu, rewon child, david luan,dario amodei, and ilya sutskever.
2019. languagemodels are unsupervised multitask learners.
openaiblog, 1(8)..marco tulio ribeiro, tongshuang wu, carlos guestrin,and sameer singh.
2020. beyond accuracy: be-havioral testing of nlp models with checklist.
inproceedings of the 58th annual meeting of the asso-ciation for computational linguistics, pages 4902–4912, online.
association for computational lin-guistics..ananya b sai, mithun das gupta, mitesh m khapra,and mukundhan srinivasan.
2019. re-evaluatingadem: a deeper look at scoring dialogue responses.
in proceedings of the aaai conference on artiﬁcialintelligence, volume 33, pages 6220–6227..thibault sellam, dipanjan das, and ankur parikh.
2020. bleurt: learning robust metrics for textgeneration.
in proceedings of the 58th annual meet-ing of the association for computational linguistics,pages 7881–7892, online.
association for computa-tional linguistics..robert speer and catherine havasi.
2012. represent-ing general relational knowledge in conceptnet 5. inlrec, pages 3679–3686..ilya sutskever, oriol vinyals, and quoc v le.
2014.sequence to sequence learning with neural networks.
in advances in neural information processing sys-tems, pages 3104–3112..chongyang tao, lili mou, dongyan zhao, and ruiyan.
2018. ruber: an unsupervised method for au-tomatic evaluation of open-domain dialog systems..6403in thirty-second aaai conference on artiﬁcial in-telligence..alex warstadt, amanpreet singh, and samuel r bow-man.
2019. neural network acceptability judgments.
transactions of the association for computationallinguistics, 7:625–641..qizhe xie, zihang dai, eduard hovy, thang luong,and quoc le.
2020. unsupervised data augmenta-tion for consistency training.
advances in neuralinformation processing systems, 33..lili yao, nanyun peng, ralph weischedel, kevinknight, dongyan zhao, and rui yan.
2019. plan-and-write: towards better automatic storytelling.
inproceedings of the aaai conference on artiﬁcial in-telligence, volume 33, pages 7378–7385..tianyi zhang*, varsha kishore*, felix wu*, kilian q.weinberger, and yoav artzi.
2020. bertscore: eval-in internationaluating text generation with bert.
conference on learning representations..tiancheng zhao, ran zhao, and maxine eskenazi.
2017. learning discourse-level diversity for neuraldialog models using conditional variational autoen-in proceedings of the 55th annual meet-coders.
ing of the association for computational linguistics(volume 1: long papers), pages 654–664..wei zhao, maxime peyrard, fei liu, yang gao, chris-tian m meyer, and steffen eger.
2019. moverscore:text generation evaluating with contextualized em-beddings and earth mover distance.
in proceedingsof the 2019 conference on empirical methods innatural language processing and the 9th interna-tional joint conference on natural language pro-cessing (emnlp-ijcnlp), pages 563–578..6404a details for mans.
a.1 story collection.
data processing we collect machine-generatedstories based on roc and wp.
to achieve bettergeneration and generalization performance, we fol-low guan et al.
(2020) to delexicalize stories inroc by masking all the names with placeholders,and retain about 250 words (with correct sentenceboundary) from the beginning and truncated therest in wp..story generation after training, we use the gen-eration models to generate stories based on thetest sets of roc and wp.
we adopt nucleus sam-pling (holtzman et al., 2020) with p = 0.9 forstory generation to avoid as many repetition errorsas possible, since such cases are easier for recogni-tion and simulation (we cover the repetition errorsmainly with the test examples in autos)..a.2 manual annotation.
table 8 shows examples of four error types andcorresponding deduction points.
the error types in-cludes repetitive plots, unrelated events, conﬂictinglogic and chaotic scenes..my mom bought him a watermelon.
he pickedit up and picked it up.
he picked it up and pickedit up.
now he have a big smile on his face..he ate sweets and sweets.
he ate it all day.
heate it all.
he ate it all..repetitive plots (-2).
one day he decided to try a new recipe.
hebought all the ingredients.
he followed therecipe.
it was the best sauce he ever tasted..unrelated events tothe beginning (-1).
he decided to buy a banana.
he picked up a bigoak tree.
he put it in the kitchen.
he is happywith the watermelon..unrelated events totheandbeginningwithin its context (-2).
he had a watermelon this morning.
he wantedanother one.
he went to buy one.
he didn’twant to eat watermelons..i buy a watermelon for him.
it is pretty great formy dad.
he doesn’t like it.
he ﬁnally asked meto be his girlfriend..i had a watermelon when i was a child.
i wasfeeding him fruits.
i picked it up and put it in thehouse.
he asked me to be his son..conﬂicting logic (-1).
conﬂicting logic (-2).
chaotic scenes (-2).
figure 3: boxplot of human judgments for each storysource (top: roc, bottom: wp)..present the distribution of human judgments fordifferent models in figure 3 and other statisticsin table 10. the results show the diversity of thestories in length and quality..α.interpretation.
< 0.67.not good.
table 9: interpretation of krippendorff’s α..statistics.
unique inputsgenerated stories (per input)generated stories (totally)average input tokensaverage reference tokensaverage story tokens.
roc wp.
20051,0009.2639.5439.38.
20051,00022.09238.51232.51.table 10: statistics of mans.
text is tokenized withspacy tokenizer10..input: my dad likes watermelon very much..error types.
0.67 ∼ 0.8.allowing tentative conclusions to be drawn.
repetitive plots (-1).
> 0.8.good reliability.
table 8: examples of four error types and correspond-ing deduction points (in the parentheses) given thesame input.
italic words indicate the keywords crucialfor the errors..a.3 statistics.
the krippendorff’s α is 0.77 for roc and 0.71 forwp, indicating a moderate inter-annotator agree-ment according to the interpretation in table 9. we.
a.4 correlation with human judgments.
we experimented with more popular metrics as fol-lows: rouge-l (lin, 2004), meteor (banerjeeand lavie, 2005), embedding based metrics (in-cluding greedy matching,embeddingaverage and vector extrema (liu et al.,2016)) with bert embedding, bertscore (inl-cuding precision and recall), and moverscore..10https://spacy.io/api/tokenizer.
6405seq2seqplan&writefusiongpt-2kg-gpt-2all12345annotation score (roc)meanmedianseq2seqplan&writefusiongpt-2kg-gpt-2all12345annotation score (wp)meanmedianruber, and the supervised metric bleurt whichis ﬁne-tuned on the released annotation resultsfrom guan and huang (2020).
the experimentresults is shown in table 11..metrics.
roc.
wp.
referenced metrics.
bleurouge-lmeteorgreedy matchingvector averagevector extremabertscore-pbertscore-rbertscore-f1moverscorerr-bert.
unreferenced metrics.
ppl (p)ppl (f)ru-bertun i o n.hybrid metrics.
ruberruber-bertbleurt.
-0.02390.01880.01550.1892∗0.1840∗0.1021∗0.1538∗0.0838∗0.1271∗0.1294∗0.0808∗.
-0.0537-0.0107-0.0079-0.0510-0.0429-0.02410.0857∗-0.02150.0329-0.05860.1567∗.
0.2547∗0.2817∗0.0830∗0.4119∗.
0.3033∗0.2952∗0.1666∗0.3256∗.
0.01190.1434∗0.3163∗.
-0.05270.2116∗0.1738∗.
table 11: pearson correlation with human judgmentson mans.
the best performance for each type ofmetrics is highlighted in bold.
the correlation scoresmarked with * indicate the result signiﬁcantly corre-lates with human judgments (p-value<0.01)..b details for autos.
b.1 construction.
we list some technical details for constructing au-tos within different aspects as follows:.
• semantic repetition and paraphrases: wepresent several examples for paraphrase gen-eration in table 14. we adopt moverscoreand bleu-1 to measure the semantic similar-ity and word overlap between the paraphrasesand the original sentences, respectively.
weﬁnally only use the paraphrase whose mover-score is larger than 0.4 and bleu-1 is lessthan 0.6 with the original sentence, becausethey achieve both high semantic similarity andlow word overlap..• character behaviour: we recognize thepersonal pronouns in a story following ta-ble 13. we select those stories which contain.
at least three types of person (i.e., at leastthree pronouns from different rows) as the co-herent examples.
and when substituting thepronouns to create incoherent examples, weonly perform the substitution in the same col-umn (e.g., “my” can be only substituted with“our”, “your”, etc.)
for better grammaticality..• consistency, causal and temporal rela-tionship: we present the negated words,causality-related words and the time-relatedwords in table 12..b.2 grammaticality classiﬁer.
we train a binary classiﬁer on the cola cor-pus (warstadt et al., 2019) to learn to judge thegrammaticality, and then ﬁlter out those examplesthat are classiﬁed as ungrammatical (the classiﬁerscore less than 0.5).
for simplicity, we directly usethe public model from textattack (morris et al.,2020) as a classiﬁer to ﬁlter out those examples inautos with poor grammaticality.
the classiﬁer isﬁne-tuned on the cola corpus based on bert andachieves an accuracy of 82.90% on the test set ofcola.
furthermore, if we suppose that all of thehuman-written stories in roc and wp are gram-matical, the accuracy of the classiﬁer on the storieswould be 96.48% and 65.68% for roc and wp,respectively.
the results are intuitive since storiesin wp may contain much informal english (e.g.,website link).
we present several examples in ta-ble 15 to further indicate the usefulness of the clas-siﬁer.
we can see that the classiﬁer can detect thegrammar errors in multiple aspects such as verbforms (e.g., “head” should be “heads” for case 1)and sentence elements (e.g., the predicate is miss-ing for case 3).
and the classiﬁer would give thegrammatical sentences high scores although theymay be unreasonable in logic (e.g., repetitive textsfor case 4 and conﬂicting plot for case 5).
finally,we ﬁlter out about 21.69% and 50.15% examplesfor roc/wp, respectively..b.3 statistics.
we show the statistics of the discrimination test setand the invariance test set in autos in table 16and table 17, respectively..6406conjunction, preposition, adverb.
noun, verb, adjective.
no, not, never, neither, hardly, unlikely, rarely, seldom, impa-tiently, uncertainly (incomplete listing, 215 in total).
none, nobody, nothing, disable, disagree, disappear, illegal, inability,inactive, unhappy, unfortunately (incomplete listing, 164 in total).
causality-related.
so, because, since, therefore, why.
cause, reason, result, effect, purpose, aim, sake, consequence, causal.
types.
negated.
time-related.
after, before, previously, simultaneously, currently, mean-while, then, now, ever, again, once, anytime, when, while,never, always, usually, often, sometimes, usually, early, lately,already, forever, ago, yesterday, today, tomorrow.
ending, beginning, previous, simultaneous, current, temporary, con-temporary, temporal, second, minute, hour, day, month, year, century,past, future, present, delay, night, evening, morning, afternoon, noon,morning.
table 12: negated words, causality-related words, time-related words which are used to create test examples withinthe aspects “consistency”, “causal relationship” and “temporal relationship”, respectively..subj obj.
poss (a).
poss (n) ref.
iweyouyouhesheitthey.
meusyouyouhimheritthem their.
myouryouryourhisherits.
mineoursyoursyourshishersitstheirs.
myselfourselvesyourselfyourselveshimselfherselfitselfthemselves.
table 13: personal pronouns which are used to createtest examples within the aspect “character behaviour”.
each row speciﬁes one type of person, which hasﬁve forms: subjective pronouns, objective pronouns,possessive adjectives, possessive nouns and reflexivepronouns..original sentences.
paraphrases.
i ﬁlled it with the sodas..i put music into theworld and enjoy it..m.b.
0.05.
0.40.he wentmiles out of his way..several more.
he has made kilometersmore..0.16.
0.26.she screamed loudly to at-tract the attention of her au-dience..she yelled out loud forthe attention of the pub-lic..he hired an attorney..he employed a lawyer..she watched a video of theplay later..later watched a.shevideo of the play..0.42.
0.45.
0.57.
0.75.
0.40.
0.89.table 14: examples for paraphrase generation.
m andb mean the moverscore and bleu-1 between the para-phrases and the original sentences, respectively..cases.
1. she head to the city..2. a strange elderly woman and called his name..3. they walked home several more times whenever that..4. one day mary needed to leave the airport .
she had no idea onhow to get a taxi though.
asking for some help she learned aboutlyft.
she had no idea how to get a taxi.
within a hour she was athome, happy with her decision..5. jack was invited to a holiday party.
he wanted to bring hishostess a gift.
but he had no clue what!
before googling, hedecided on a bottle of wine .
his hostess was very pleased with it..s.0.07.
0.20.
0.41.
0.66.
0.95.table 15: examples for the grammaticality classiﬁer.
the examples are sentences or stories selected fromthe incoherent examples of the discrimination test setof autos.
s means the classiﬁer score∈ [0, 1] (1 isthe best).
the italic words are ungrammatical, and theunderlined ones are unreasonable in logic but grammat-ical..datasets.
coherent.
incoherent.
input.
story.
input.
story.
rocwp.
8.7630.02.
39.28235.83.
8.8830.28.
40.39228.04.table 16: statistics of the discrimination test set in au-tos.
input and story is the average number of tokensin the inputs and stories.
coherent means the coherentexamples which are selected from the human-writtenincoherent means the incoherent examplesstories.
which are automatically constructed by perturbing thehuman-written stories..datasets.
human.
dis.
input.
story.
input.
story.
rocwp.
9.0329.65.
40.66211.19.
9.2329.60.
40.57234.40.table 17: statistics of the invariance test set in autos.
input and story is the average number of tokens in theinputs and stories.
human and dis means the human-written coherent stories and incoherent samples (sam-pled from the discrimination test set) to be perturbed,respectively..6407