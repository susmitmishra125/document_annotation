search from history and reason for future: two-stage reasoning ontemporal knowledge graphszixuan li1,2, xiaolong jin1,2, saiping guan1,2, wei li3, jiafeng guo1,2,yuanzhuo wang1,2 and xueqi cheng1,21school of computer science and technology, university of chinese academy of sciences;2cas key laboratory of network data science and technology, institute ofcomputing technology, chinese academy of sciences;3baidu inc.{lizixuan,jinxiaolong,guansaiping}@ict.ac.cnliwei85@baidu.com.
abstract.
temporal knowledge graphs (tkgs) havebeen developed and used in many different ar-eas.
reasoning on tkgs that predicts poten-tial facts (events) in the future brings greatchallenges to existing models.
when facing aprediction task, human beings usually searchuseful historical information (i.e., clues) intheir memories and then reason for futuremeticulously.
inspired by this mechanism, wepropose cluster to predict future facts in atwo-stage manner, clue searching and tem-poral reasoning, accordingly.
speciﬁcally, atthe clue searching stage, cluster learns abeam search policy via reinforcement learn-ing (rl) to induce multiple clues from histor-ical facts.
at the temporal reasoning stage, itadopts a graph convolution network based se-quence method to deduce answers from clues.
experiments on four datasets demonstrate thesubstantial advantages of cluster comparedwith the state-of-the-art methods.
moreover,the clues found by cluster further provide in-terpretability for the results..1.introduction.
temporal knowledge graphs (tkgs) (boscheeet al., 2015; gottschalk and demidova, 2018, 2019;zhao, 2020) have emerged as a very active researcharea over the last few years.
each fact in tkgshas a timestamp indicating its time of occurrence.
for example, the fact, (covid-19, new medicalcase occur, shop, 2020-10-2), indicates that a newmedical case of covid-19 occurred in a shopon 2020-10-2. in this paper, reasoning on tkgsaims to predict future facts (events) for timestampt > tt , where tt is assumed to be the currenttimestamp (jin et al., 2020).
an example of thetask is shown in figure 1, which attempts to an-swer the query (covid-19, new medical case oc-cur, ?, 2020-12-23) with the given historical facts.
obviously, such a task may beneﬁt many practical.
figure 1: an illustration of the reasoning process in-spired by human cognition.
different colors indicatedifferent relations.
r−1 is the inverse relation of r..applications, such as, emerging events response(muthiah et al., 2015; phillips et al., 2017; kork-maz et al., 2015), disaster relief (signorini et al.,2011), and ﬁnancial analysis (bollen et al., 2011)..how do human beings predict future events?
ac-cording to the dual process theory (evans, 1984,2003, 2008; sloman, 1996), the ﬁrst thing is tosearch the massive-capacity memories and ﬁndsome related historical information (i.e., clues) in-tuitively.
as shown in the left part of figure 1,there are mainly three categories of clues vital tothe query: 1) the 1-hop paths with the same re-lation to the query (thus called repetitive 1-hoppaths), such as (covid-19, new medical case oc-cur, shop); 2) the 1-hop paths with relations dif-ferent from the query (called non-repetitive 1-hoppaths), such as (covid-19, new suspected case oc-cur, bank); and 3) the 2-hop paths, such as (covid-19, diagnose−1, the man, go to, police station).
human beings recall these clues from their mem-ories and have some intuitive candidate answersfor the query.
secondly, human beings get theaccurate answer by diving deeper into the clues’.
proceedingsofthe59thannualmeetingoftheassociationforcomputationallinguisticsandthe11thinternationaljointconferenceonnaturallanguageprocessing,pages4732–4743august1–6,2021.©2021associationforcomputationallinguistics4732query:   (covid-19, new medical case occur, ?
,  2020-12-23)stage 1: clue searchingcandidatas12-2010-211-5(covid-19, new medical case occur, shop)12-2112-2110-112-2110-14stage 2: temporal reasoningshoppolicestationbankpolicestationshopshopbankpolicestation(the man, go to, shop)(covid-19, new suspected case occur, bank)(covid-19, new medical case occur, police station)(the man, go to, police station)the man(covid-19, diagnose  , the man)-1(covid-19, diagnose  ,  the man)-1temporal information and performing a meticulousreasoning process.
as shown in the right part offigure 1, the man went to the police station morethan two months earlier than the time when he wasdiagnosed with covid-19, indicating that policestation is probably not the answer.
finally, humanbeings derive the answer, shop..existing models mainly focus on the above sec-ond process but underestimate the ﬁrst process.
some recent studies (trivedi et al., 2017, 2018)learn the evolving embeddings of entities withall historical facts considered.
however, only afew historical facts are useful for a speciﬁc pre-diction.
thus, some other studies (jin et al., 2020,2019; zhu et al., 2020) mainly focus on encod-ing the 1-hop repetitive paths (repetitive facts) inthe history.
however, besides the 1-hop repetitivepaths, there are massive other related informationin the datasets.
taking the widely used dataseticews18 (jin et al., 2020) as an example, 41.2%of the training queries can get the answers throughthe 1-hop repetitive paths in the history.
but, al-most 64.6% of them can get the answers through 1-hop repetitive and non-repetitive paths, and 86.2%through the 1-hop and 2-hop paths..thus, we propose a new model called cluster,consisting of two stages, clue searching (stage1) and temporal reasoning (stage 2).
at stage 1,cluster formalizes clue-searching as a markovdecision process (mdp) (sutton and barto, 2018)and learns a beam search policy to solve it.
atstage 2, cluster reorganizes the clues found instage 1 into a series of graphs and then a graphconvolution network (gcn) and a gated recur-rent unit (gru) are employed to deduce accurateanswers from the graphs..in general, this paper makes the following con-.
tributions:.
• we formulate the tkg reasoning task fromthe view of human cognition and propose atwo-stage model, cluster, which is mainlycomposed of a rl-based clue searching stageand a gcn-based temporal reasoning stage..• we advocate the importance of clue searchingfor the ﬁrst time, and propose to learn a beamsearch policy via rl, which can ﬁnd explicitand reliable clues for the fact to be predicted..• experiments demonstrate that clusterachieves consistently and signiﬁcantly betterperformance on popular tkgs and the clues.
found by cluster can provide interpretabilityfor the reasoning results..2 related work.
static kg reasoning.
embedding based kg rea-soning models (bordes et al., 2013; yang et al.,2014; trouillon et al., 2016; dettmers et al., 2018;shang et al., 2019; sun et al., 2018) have drawnincreasing attention.
all of them attend to learn thedistributed embeddings for entities and relationsin kgs.
among them, some works (schlichtkrullet al., 2018; shang et al., 2019; ye et al., 2019;vashishth et al., 2019) extend gcn to relation-aware gcn for the kgs..however, embedding based models underesti-mate the symbolic compositionality of relations inkgs, which limits their usage in more complexreasoning tasks.
thus, some recent works (xionget al., 2017; das et al., 2018; lin et al., 2018; chenet al., 2018; wang et al., 2019; li and cheng, 2019)focus on multi-hop reasoning, which learns sym-bolic inference rules from relation paths.
however,all the above methods cannot deal with the tempo-ral dependencies among facts in tkgs..temporal kg reasoning.
reasoning on tem-poral kg can broadly be categorized into two set-tings, interpolation (sadeghian et al., 2016; garc´ıa-dur´an et al., 2018; leblay and chekol, 2018; das-gupta et al., 2018; wu et al., 2019; xu et al., 2020;goel et al., 2020; wu et al., 2020; han et al., 2020a;jung et al., 2020) and extrapolation (trivedi et al.,2017, 2018; han et al., 2020b; deng et al., 2020;jin et al., 2019, 2020; zhu et al., 2020; li et al.,2021), as mentioned in jin et al.
(2020).
underthe former setting, models attempt to infer miss-ing facts at historical timestamps.
while the lattersetting, which this paper focuses on, attempts topredict facts in the future.
orthogonal to our work,trivedi et al.
(2017, 2018) estimate the conditionalprobability of observing a future fact via a temporalpoint process taking all historical facts into consid-eration.
although han et al.
(2020b) extends tem-poral point process to model concurrent facts, theyare more capable of modeling tkgs with continu-ous time, where no events may occur at the sametimestamp.
glean (deng et al., 2020) incorporatesa word graph constructed by the summary texts ofevents into tkg reasoning.
the most related worksare re-net (jin et al., 2020) and cygnet (zhuet al., 2020).
re-net uses a subgraph aggregatorand gru to model the subgraph sequence consist-.
4733figure 2: an illustrative diagram of the proposed cluster model..ing of 1-hop facts.
cygnet uses a sequential copynetwork to model repetitive facts.
both of themuse heuristic strategies in the clue searching stage,which may lose lots of other informative historicalfacts or engage some noise.
although the abovetwo models attempt to consider other informationby pre-trained global embeddings or an extra gen-eration model, they still mainly focus on modelingrepetitive facts.
besides, all the models almost cannot provide interpretability for the results..3 the proposed cluster model.
we start with the notations, then introduce themodel as well as its training procedure in detail..3.1 notations.
a tkg g is a multi-relational directed graph withtime-stamped edges between entities.
a fact in gcan be formalized as a quadruple (es, r, eo, t).
itdescribes that a fact of relation type r ∈ r occursbetween subject entity es ∈ e and object entityeo ∈ e at timestamp t ∈ t , where r, e and tdenote the sets of relations, entities and timestamps,respectively.
tkg reasoning aims to predict themissing object entity of (es, rq, ?, ts) or the miss-ing subject entity of (?, rq, eo, ts) given the set ofhistorical facts before ts, denoted as g0:ts−1.
with-out loss of generality, in this paper, we predict themissing object entity in a fact, and the model canbe easily extended to predicting the subject entity.
in this paper, a clue path is in the form of(es, r1, e1, ..., rk, ek, ..., ri , ei ), where ek ∈ e,rk ∈ r, k = 1, ..., i, i is the maximum step num-ber and each hop in the path can be viewed as atriple (ek−1, rk, ek).
note that, e0 = es.
the cluefacts are derived from the clue paths via mappingeach hop (ek−1, rk, ek) in the paths to correspond-ing facts (ek−1, rk, ek, t1), (ek−1, rk, ek, t2, ...) ∈g0:ts−1..3.2 model overview.
as illustrated in figure 2, the model consists of twostages, clue searching and temporal reasoning.
thetwo stages are coordinated to perform fast and slowthinking (daniel, 2017), respectively, to solve thetkg reasoning task, inspired by human cognition.
speciﬁcally, stage 1 mainly focuses on searchingthe clue paths of which the compositional semanticinformation relates to the given query with the timeconstraints.
then, the clue paths and the conse-quent candidate entities are provided for the rea-soning in stage 2, which mainly focuses on metic-ulously modeling the temporal information amongclue facts and gets the ﬁnal results.
in the clustermodel, these two stages interact with each otherin the training phase and decide the ﬁnal answerjointly in the inference phase..3.3 stage 1: clue searching.
the purpose of stage 1 is to search and induce theclue paths related to the given query (es, rq, ?, ts)from history.
the previous studies (jin et al., 2019,2020; zhu et al., 2020) use heuristic strategies toextract 1-hop repetitive paths, losing lots of otherinformative clue paths.
besides, there are enor-mous facts in the history.
thus, a learnable andefﬁcient clue searching strategy is of great neces-sity.
motivated by these observations, stage 1 canbe viewed as a sequential decision problem andsolved by the rl system..3.3.1 the rl system.
the rl system consists of two parts, the agent andthe environment.
we formulate the rl system asan mdp, which is a framework of learning frominteractions between the agent and the environmentto ﬁnd b promising clue paths.
starting from es,the agent sequentially selects outgoing edges viarandomized beam search strategy, and traverses to.
4734(cid:11)es, rq, ?, ts(cid:12)r-gcn   gru+mlpesmlp randomizedbeam searchlstmclue pathshirqcandidatesclue factsesesesjts-2ts-1r-gcn   r-gcn   stage 1: clue searchingstage 2: temporal reasoningts-1ts-2ts-3ts-4ts-5eseseseses(cid:335)(cid:335)(cid:335)(cid:335)(cid:335)(cid:335)(cid:335)(cid:335)resgrqjesrqts-2gesrqts-1g(cid:335)time-constrained actionsnext staterewardenvironmentagenteihi+1espath in esnew entities until it reaches the maximum step i.the mdp consists of the following parts:.
states.
each state si = (ei, ti, es, rq, ts) ∈ sis a tuple, where s is the set of all the availablestates; ei (e0 = es) is the entity where the agentvisited at step i; and ti (t0 = ts) is the timestampof the action taken at the previous step.
note that,es, rq, and ts are shared by all the states for thegiven query..time-constrained actions.
compared to statickgs, the time dimension of tkgs leads to an ex-plosively large action space.
besides, the humanmemories focus on the lastest occcuring events.
thus, we constrain the time interval between thetimestamp of each fact and ts to be no more thanm. and the time interval between the timestamp ofthe previous action and each available action is nomore than ∆.
therefore, the set of the possible ac-tions ai ∈ a (a is the set of all available actions)at step i consists of the time-constrained outgoingedges of ei,.
ai = {(r(cid:48), e(cid:48), t(cid:48))|(ei, r(cid:48), e(cid:48), t(cid:48)) ∈.
g0:ts−1,.
|t(cid:48) − ti| ≤ ∆, ts − t(cid:48) ≤ m}..(1).
to give the agent an adaptive option to terminate,.
a self-loop edge is added to ai..transition.
a transition function δ : s × a →s is deterministic under the situation of tkg andjust updates the state to new entities incident to theactions selected by the agent..rewards.
the agent only receives a terminalreward r at the end of search, which is the sum oftwo parts, binary reward and real value reward.
thebinary reward is set to 1 if the destination entityei is the correct target entity eo, and 0 otherwise.
besides, the agent gets a real value reward ˆr fromstage 2 if ei is the target entity, which will beintroduced in section 3.4..3.3.2 semantic policy networkgiven the time-constrained action space, the com-positional semantic information implied in the cluepaths and the time information of the clue factsis vital for reasoning.
however, considering thatmodeling the time information requires to divedeeply into the complex temporal patterns of factsand is not the emphasis of stage 1. thus, we de-sign a semantic policy network which calculatesthe probability distribution over all the actions ac-cording to the current state si and search historyhi = (es, a0, ..., ai−1) without considering times-tamps in stage 1. here, ai = (ri+1, ei+1, ti+1) is.
the action taken at step i = 0, ..., i − 1. note that,h0 is es.
actually, the search history without times-tamps is a candidate clue path (a clue path at stepi) mentioned in section 3.1..the embedding of the action ai is ai = ri+1 ⊕ei+1, where ⊕ is the concatenation operation;ri+1, ei+1 are the embeddings of ri+1 and ei+1,correspondingly.
then, a long short term mem-ory network (lstm) is applied to encode the can-didate clue path hi as a continuous vector hi,.
hi = lst m (hi−1, ai−1),.
(2).
where the initial hidden embedding h0 equals tolst m (0, rdummy ⊕ es) and rdummy is the em-bedding of a special relation introduced to form astart action with es.
for step i, the action spaceis encoded by stacking the embeddings of all theactions in ai, which are denoted as ai ∈ r|ai|×2d.
here, d is the dimension of entity embeddings andrelation embeddings.
then, the policy network cal-culates the distribution π over all the actions by amulti-layer perceptron (mlp) parameterized withw1 and w2 as follows:.
π(ai|si;θ) = η(aiw2f (w1[ei ⊕ hi ⊕ rq]), (3).
where η(·) is the softmax function, f (·) is therelu function (glorot et al., 2011) and θ is theset of all the learnable parameters in stage 1..3.3.3 randomized beam searchin the scenario of tkgs, the occurrence of a factmay result from multiple factors.
thus, multipleclue paths are necessary for the prediction.
be-sides, the intuitive candidates from stage 1 shouldrecall the right answers as many as possible.
there-fore, we adopt randomized beam search (sutskeveret al., 2014; guu et al., 2017; wu et al., 2018) asthe action sampling strategy of the agent, whichinjects random noise to the beam search in order toincrease the exploration ability of the agent..speciﬁcally, a beam contains b candidate cluepaths at step i. for each candidate path, we appendb most likely actions (according to equation 3) tothe end of the path, resulting in a new path poolwith size b × b. then we either pick the highest-scoring paths with probability µ or uniformly sam-ple a random path with probability 1 − µ repeatedlyfor b times.
the score of each candidate clue pathat step i equals to (cid:80)ik=0 log π(ak|sk; θ).
notethat, at the ﬁrst step, b 1-hop candidate paths start-ing from es are generated by choosing b paths viathe above picking strategy..47353.4 stage 2: temporal reasoning.
datasets.
ice14.
ice05-15.
ice18.
gdelt.
to dive deeper into the temporal informationamong clue facts at different timestamps and thestructural information among concurrent clue facts,stage 2 reorganizes all clue facts into a sequence ofgraphs ˆg = { ˆg0, ..., ˆgj, ..., ˆgts−1}, where each ˆgjis a multi-relational graph consisting of clue factsat timestamp j = 0, ...ts − 1. we use an ω-layerrgcn (schlichtkrull et al., 2018) to model ˆgj,.
#e#r#t rain#v alid#t esttime gap.
6,86923074,8458,5147,3711 day.
10,094251368,86846,30246,1591 day.
23,033256373,01845,99549,5451 day.
7,6912401,734,399238,765305,24115 mins.
table 1: statistics of the datasets..deﬁne the objective function using cross-entropy:.
j (φ) = −.
1|g|.
(cid:88).
(es,rq,eo,ts)∈g.
.
log p(eo|es, rq, ts),.
ˆhl+1.
o,j = f.1do.
(cid:88).
wlr.ˆhls,j +wl.
loop.
ˆhl.
,.
o,j.
(s,r)|(s,r,o,j)∈ ˆgj.
r and wl.
o,j and ˆhl.
(4)where ˆhls,j denote the lth layer embed-dings of entities o and s in ˆgj at timestamp j, re-spectively; wlloop are the weight matricesfor aggregating features from different relationsand self-loop in the lth layer; do is the in-degreeof entity o; the input embedding for each entity k,ˆhl=0k,j is set to ˆek , which is different from that ofstage 1..then, ˆgj, the embedding of ˆgj, is calculated bythe mean pooling operation of all entity embed-dings calculated by equation 4 in ˆgj.
the concate-nation of ˆes, ˆgj and ˆrq (the embedding of rq instage 2) is fed into a gru,.
hj = gru ([ˆes ⊕ ˆgj ⊕ ˆrq], hj−1)..(5).
the ﬁnal output of gru, denoted as hts−1, isfed into a mlp decoder parameterized with wmlpto get the ﬁnal scores for all the entities, i.e.,.
p(e|es, rq, ts) = σ(ht.
ts−1 · wmlp),.
(6).
where σ is the sigmoid activation function..finally, we re-rank the candidate entities accord-ing to equation 6. to give a positive feedback tothe clue paths arriving at the answer, stage 2 givesa beam-level reward which equals to the ﬁnal scoreof ei from equation 6, i.e, ˆr = p(ei ), to stage 1..3.5 training strategy.
for stage 1, the beam search policy network istrained by maximizing the expected reward over allqueries in the training set,.
j (θ)=e(es,rq,eo,ts)∈g[ea0,...ai−1[r(ei |es, rq, ts)]].
(7)the reinforce algorithm (williams, 1992)is used to optimize equation 7. for stage 2, we.
(8)where φ is the set of all the learnable parametersin stage 2. the adam (kingma and ba, 2014) opti-mizer is used to minimize equation 8. as stages 1and stage 2 are correlated mutually, they are trainedjointly.
stage 1 is pre-trained with only binary re-ward before the joint training process starts.
thenstage 2 is trained with the parameters of stage 1frozen.
at last, we jointly train the two stages.
such a training strategy is widely used by other rlstudies (bahdanau et al., 2016; feng et al., 2018)..4 experiment.
we design experiments to answer the followingquestions: q1.
how does cluster perform onthe tkg reasoning task?
q2.
how do the twostages contribute to the ﬁnal results respectively?
q3.
which clues are found and used for reasoning?
q4.
can cluster provide some interpretabilityfor the results?.
4.1 experimental setup.
datasets and metrics.
there are four typicaltkgs commonly used in previous studies, namely,icews14 (garc´ıa-dur´an et al., 2018), icews05-15 (garc´ıa-dur´an et al., 2018), icews18 (jin et al.,2019) and gdelt (jin et al., 2020).
the ﬁrst threedatasets are from the integrated crisis early warn-ing system (icews) (boschee et al., 2015) and thelast one is from global database of events, lan-guage, and tone (gdelt) (leetaru and schrodt,2013).
we evaluate cluster on all these datasets.
icews14 and icews05-15 are divided into train-ing, validation, and test sets following the prepro-cessing on icews18 in re-net (jin et al., 2020).
the details of the datasets are presented in table 1.in the experiments, the widely used mean recip-rocal rank (mrr) and hits@{1,10} are employedas the metrics.
without loss of generality, onlythe experimental results under the raw setting are.
4736reported.
the ﬁltered setting is not suitable forthe reasoning task under the exploration setting,as mentioned in (han et al., 2020b; ding et al.,2021; jain et al., 2020).
the reason is explainedin terms of an example as follows: given a testquadruple (barack obama, visit,?, 2015-1-25) withthe correct answer india.
assume there is a quadru-ple (barack obama, visit, germany, 2013-1-18)in the training set.
the ﬁltered setting used inthe previous studies ignores time information andconsiders (barack obama, visit, germany, 2015-1-25) to be valid because (barack obama, visit,germany, 2013-1-18) appears in the training set.
it thus removes the quadruple from the corruptedones.
however, the fact (barack obama, visit, ger-many) is temporally valid on 2013-1-18, instead of2015-1-25. therefore, to test the quadruple (barackobama, visit,?, 2015-1-25), (barack obama, visit,germany, 2015-1-18) should not be removed.
inthis way, the ﬁltered setting wrongly removes quitea lot of quadruples and thus leads to over-optimisticexperimental performance..baselines.
the cluster model is comparedwith two categories of models, i.e., models forstatic kg reasoning and models for tkg reason-ing under the exploration setting.
the typicalstatic models distmult (yang et al., 2014), com-plex (trouillon et al., 2016), rgcn (schlichtkrullet al., 2018), conve (dettmers et al., 2018) and ro-tae (sun et al., 2018) are selected with the temporalinformation of facts ignored.
we also choose min-erva (das et al., 2018), the rl-based multi-hopreasoning model, as the baseline.
for tkg mod-els, the representative know-evolve (trivedi et al.,2017), dyrep (trivedi et al., 2018), cygnet (zhuet al., 2020) and re-net (jin et al., 2020) areselected.
besides, following re-net (jin et al.,2020), we extend two models for temporal ho-mogeneous graphs, gcrn (seo et al., 2018) andevolvegcn-o (pareja et al., 2019)), to rgcrnand evolvergcn by replacing gcn with rgcn.
we use conve (dettmers et al., 2018), a morestronger decoder to replace the mlp (jin et al.,2020) for the two models.
for know-evolve anddyrep, re-net extends them to tkg reasoningtask but does not release their codes.
thus, we onlyreport the results from their papers.
for other base-lines, we reproduce all the results with the optimalparameters tuning on the validation set..implementation details.
in the experiments,the embedding dimension d for the two stages, is.
set to 200. for stage 1, we adopt an adaptive ap-proach for selecting the time interval m. speciﬁ-cally, for icews14, icews05-15, and gdelt, mis set to the last one timestamp the query pattern (es,rq, ?)
appearing in the dataset before ts.
and foricews18, m is set to the last third timestamp.
∆is set to 3 for all the datasets.
we set the maximumstep number i = 1, 2 and ﬁnd i = 1 is better forall the datasets.
the number of the lstm layersis set to 2 and the dimension of the hidden layer oflstm is set to 200 for all the datasets.
the beamsize is set to 32 for the three icews datasets and64 for gdelt.
µ is set to 0.3 for all the datasets.
for stage 2, the maximum sequence length of gruis set to 10, the number of the gru layers is set to1 and the number of the rgcn layers is set to 2 forall the datasets.
for each fact in g0:ts−1, we addthe corresponding inverse fact into g0:ts−1.
all theexperiments are carried out on tesla v100..4.2 results on tkg reasoning.
the results on tkg reasoning are presented in ta-ble 2. cluster consistently outperforms the base-lines on all the icews datasets, which convinc-ingly veriﬁes its effectiveness and answers q1.
es-pecially on icews14, cluster even achieves theimprovements of 7.1% in mrr, 4.5% in hits@1,and 13.7% in hits@10 over the best baselines.
speciﬁcally, cluster signiﬁcantly outperformsthe static models (i.e., those in the ﬁrst block oftable 2) because it captures the temporal informa-tion of some important history.
moreover, clus-ter drastically performs better than those temporalmodels.
compared with dyrep and know-evolvethat consider all the history, cluster can focuson more vital clues.
different from rgcrn andevolvergcn which model all history from severallatest timestamps, cluster models a longer historyafter reducing all history to a few clues.
cygnetand re-net mainly focus on modeling the repet-itive clues or all the 1-hop clues and show strongperformance.
cluster also outperforms them onthe three icews datasets, because the rl-basedstage 1 can ﬁnd more explicit and reliable clues..the experimental results on gdelt demon-strate that the performance of static models andtemporal ones are similarly poor, as compared withthose of the other three datasets.
we further analyzethe gdelt dataset and ﬁnd that a large number ofits entities are abstract concepts which do not indi-cate a speciﬁc entity (e.g., president, police.
4737model.
distmultcomplexrgcnconverotateminerva.
know-evolvedyreprgcrnevolvergcncygnetre-netcluster.
24.931.927.130.927.533.2.
––36.937.136.538.946.0.ice14.
icews05-15.
ice18.
gdelt.
mrr h@1 h@10 mrr h@1 h@10 mrr h@1 h@10 mrr h@1 h@10.
17.322.218.421.718.025.7.
––27.027.027.429.333.8.
40.250.744.250.147.248.3.
––56.157.054.457.571.2.
16.423.127.325.219.930.7.
––39.440.737.441.744.6.
9.814.519.116.010.925.8.
––28.730.327.531.134.9.
29.940.643.644.438.739.9.
––60.461.356.162.063.0.
17.518.817.024.815.521.0.
7.47.826.223.626.828.432.3.
10.111.18.715.17.015.3.
3.33.616.436.317.118.420.6.
32.626.834.044.933.933.0.
14.816.345.850.445.747.955.9.
15.612.310.917.35.312.1.
15.916.317.717.418.019.018.3.
9.38.04.610.41.210.0.
11.711.810.911.010.911.611.6.
28.020.622.631.312.516.7.
22.323.930.929.931.633.531.9.table 2: experimental results on tkg reasoning (in percentage) compared with static models (the top part) andtemporal models (the bottom part)..model.
ice14 ice05-15 ice18 gdelt.
stage 1 (i = 2)stage 1 (i = 1)stage 2cluster.
43.144.141.546.8.
43.346.045.046.9.
27.630.330.133.1.
15.317.619.618.7.table 3: results (in percentage) by different variants ofcluster on all the datasets..and government).
among the top 50 frequententities, 28 are abstract concepts and 43.72% corre-sponding events involve abstract concepts.
thoseabstract concepts make future prediction under theraw setting almost impossible, since we cannot pre-dict a president’s activities without knowing whichcountry he belongs to..4.3 ablation study.
to answer q2, i.e., how the two stages contributeto the ﬁnal results, we report the mrr results ofthe variants of cluster on the validation set ofall the datasets in table 3. the ﬁrst two lines oftable 3 show the results only using stage 1, wherethe maximum step i is set to 1 and 2, respectively.
following lin et al.
(2018), the score of the targetentity is set to the highest score among the pathswhen more than one path leads to it.
it can be ob-served that the results decrease when only usingstage 1, because the temporal information amongfacts is ignored.
the third line shows the resultsonly using stage 2 with extracted 1-hop repetitiveclues as the inputs.
the results decrease on all theicews datasets when only using stage 2, demon-strating that only repetitive clues are not enough forthe prediction.
for gdelt, only stage 2 achievesthe best results, which demonstrates that only us-ing repetitive clues is effective enough for it.
it is.
figure 3: a clue graph constructed by stage 1..because that only using the most straightforwardrepetitive clues in stage 2 can alleviate the inﬂu-ence of noise produced by abstract concepts.
it alsomatches our observations mentioned in section 4.2..from the ﬁrst two lines of table 3, it can be seenthat the performance of stage 1 decreases wheni is set to 2. to further analyze the reason, weextract paths from icews18 without consideringtimestamps via amie+ (gal´arraga et al., 2015), awidely used and accurate approach to extract logicrules (paths) in static kgs.
we check the top ﬁftypaths manually and present the top ﬁve convincingpaths in table 4. it can be seen that there are nostrong dependencies between the query relationsand the 2-hop paths.
thus, in this situation, longerpaths bring exponential noise clues, which pulldown the precision.
we do experiments on all thedatasets from icews and gdelt and ﬁnd thesame conclusion.
we leave it for future work toconstruct a more complex dataset for verifying theeffectiveness of multi-hop clue paths..4738halt negotiationsappeal for de-escalation of military engagementintent to settle disputeintent to cooperate economicallymake pessimistic commentdeclare ceaseﬁregrant diplomatic recognitionquery relations.
2-hop paths.
(a, declare ceaseﬁre, c)(a, intent to settle dispute,c)(a, intent to settle dispute, c)(a, halt negotiations, c)(a, accuse of crime, c).
(a, intent to cooperate, b, intent to meet, c)(a, consult, b, intent to diplomatic cooperation, c)(a, intent to diplomatic cooperation, b, intent to meet, c)(a, engage in negotiation, b, intent to meet,c)(a, accuse, b, criticize or denounce, c).
scores.
0.40710.38430.37250.37170.3256.table 4: the top ﬁve convincing 2-hop paths extracted by amie+ from icews18..figure 4: statistic of categories of clue facts in stage 2..4.4 detail analysis.
to answer q3, we show some non-repetitive cluesfound in stage 1 in figure 3. we use (relationin 1-hop non-repetitive clue path, query relation)pairs on icews18 to construct a clue graph.
ar-rows begin with the relations in the clue paths andend with the query relations.
it is interesting toﬁnd that cluster can actually ﬁnd some causalrelations.
moreover, compared to the 2-hop cluepaths shown in table 4, the 1-hop clue paths aremore informative.
it also gives explanations to theoutperformance of the 1-hop clue paths..besides, we illustrate the statistics of clue factsused during stage 2 in figure 4. the proportionof the repetitive clue facts is less than 7% and theproportion of the non-repetitive clue facts is morethan 93% on the datasets.
the abundant of the non-repetitive clue facts used in stage 2 also explainsthe outperformance of cluster to a certain degree..4.5 case study.
to answer q4, we show how cluster conductsreasoning and explains the results for the giventwo queris from the test set of icews14 in fig-(congress (unitedure 5. for the ﬁrst query:states), impose sanctions, ?, 3341), we choosethe top three candidates in stage 1 and demon-strate some clue paths of the three entities in theleft top part of figure 5. the clue paths like(congress (united states), criticize or denounce−1,china), (congress (united states), engage innegotiation−1, iran) give the evidence for candi-date entities china and iran, correspondingly.
instage 1, cluster has an intuitive candidate setincluding china, iran and france.
the score ofchina (-2.69) and iran (-2.71) are similar but the.
1here, 334 represents the 334th day in the year 2014..wrong answer, china, has a higher score than theright one, iran.
it is because stage 1 does nottake the temporal information into consideration.
however, the score gap is obvious between iranand france, which shows that stage 1 can mea-sure the qualities of different clue paths and distin-guish the semantic-related entities from the oth-ers.
in stage 2, cluster reorganizes the cluefacts by their timestamps, as shown in the righttop part of figure 5.
(congress (united state), en-gage in negotiation−1, iran, 323) and (congress(united state), make a visit, china, 227) make iranthe more possible answer.
for the second query:(china, express intent to settle dispute, ?, 364),clue paths in the left bottom of figure 5 are allassociated with the query.
stage 1 induces all en-tities to only two entities through these clue pathsbut misleads to the wrong answer, iran.
actually,even a human may give the wrong answer withonly fasting thinking.
after diving into the tem-poral information of clue facts and conduct slowthinking, some causal information and period infor-mation can be captured by stage 2. although signformal agreement is associated with express intentto settle dispute, it can not be the reason for the lat-ter.
moreover, from the subgraph sequence in theright bottom part of figure 5, it can be seen that thecooperation period between china and japen justbegins at 363, but the cooperation period betweenchina and iran has been going on for several days.
(china, express intent to settle dispute, ?, 364) ismore likely to be an antecedent event to the coop-eration period and the answer is japen..above all, for each fact to be predicted, clus-ter can provide the clues for each candidate en-tity, which presents the insight and provides inter-pretability for the reasoning results.
it is similarto the natural thinking pattern of human, in whichonly explicit and reliable clues are needed..4.6 performance under the time-aware.
filtered setting.
as mentioned in section 4.1, the widely adoptedﬁltered setting in the existing studies is not suitable.
4739repetitive clues1-hop non-repetitive cluesrepetitive clues1-hop non-repetitive cluesrepetitive clues1-hop non-repetitive clues              0.060.940.050.950.070.930.020.98!"!!"#!"#!"#!"#!"#!"#!"#!"#!
"#1.0          repetitive clue factsnon-repetitive clue factsice05-15ice14!
"#$%ice18figure 5: two cases to illustrate how cluster conducts reasoning and explains the results.
each black circlerepresents a query entity..model.
ice14.
icews05-15.
ice18.
gdelt.
mrr h@1 h@10 mrr h@1 h@10 mrr h@1 h@10 mrr h@1 h@10.rawﬁltered.
46.047.1.
33.835.0.
71.272.0.
44.645.4.
34.934.3.
63.067.7.
32.334.5.
20.622.9.
55.957.7.
18.318.5.
11.612.1.
31.932.1.table 5: experimental results under the raw setting and the (time-aware) ﬁlter setting..for the temporal reasoning task addressed in thispaper.
the essential problem of the above ﬁlteredsetting is that it ignores the time information of afact.
therefore, we also adopt an improved ﬁlteredsetting where the time information is also consid-ered, thus called time-aware ﬁltered setting (hanet al., 2020b; han et al.).
speciﬁcally, only the factsoccur at the predicted time are ﬁltered.
the resultsare in table 5. it can been seen that the experi-mental results under the time-aware ﬁltered settingare close to those under the raw setting.
this isbecause that only a very small number of facts areremoved under this ﬁltered setting.
the results alsoshow the convincing of the raw setting..5 conclusions.
in this paper, we proposed a two-stage model fromthe view of human cognition, named cluster, fortkg reasoning.
cluster consists of a rl-basedclue searching stage (stage 1) and a gcn-basedtemporal reasoning stage (stage 2).
in stage 1,cluster ﬁnds reliable clue paths from history andgenerates intuitive candidate entities via rl.
withthe found clue paths as input, stage 2 reorganizes.
the clue facts derived from the clue paths into a se-quence of graphs and performs deduction on themto get the answers.
by the two stages, the modeldemonstrates substantial advantages on tkg rea-soning.
finally, it should be mentioned that, al-though the four tkgs adopted in the experimentswere created based on the events in the real world,the motivation of this paper is to propose this tkgreasoning model only for scientiﬁc research..acknowledgment.
we gratefully acknowledge the help and assistancefrom long bai, yunqi qiu, bing li and bingbingxu.
moreover, the work is supported by the na-tional key research and development program ofchina under grant 2016yfb1000902, the nationalnatural science foundation of china under grantsu1911401, 62002341, 61772501, u1836206 and61722211, the gfkj innovation program, bei-jing academy of artiﬁcial intelligence under grantbaai2019zd0306, and the lenovo-cas jointlab youth scientist project..4740 query: (congress (united states), impose sanctions, ?, 334)make a visitchina-2.69-2.71france-3.50irancriticize or denounceengage in negotiationmake pessimistic commentmake a request227china323209149chinamake a visitiraniranfrancemake a requestanswer for future: iranstage 1: induce clues from historystage 2: deduce answers for futureintuitive candidates: china, iran, france, …criticize or denounce -1engage in negotiation -1make pessimistic commenttimescore-3-2-1…… -1 -1 query: (china, express intent to settle dispute, ?, 364)iran363350342answer for future: japanintuitive candidates: iran, japan, …sign formal agreement -1timescore-3-2-1……intend to engage in diplomatic cooperationiran-1.791-2.47japanintend to cooperateintend to cooperate -1sign formal agreement -1intend to engage in diplomatic cooperationiran362iran361iranintend to cooperatejapanintend to cooperate -1japanjapanjapanreferences.
dzmitry bahdanau, philemon brakel, kelvin xu,anirudh goyal, ryan lowe, joelle pineau, aaroncourville, and yoshua bengio.
2016. an actor-criticalgorithm for sequence prediction.
arxiv preprintarxiv:1607.07086..johan bollen, huina mao, and xiaojun zeng.
2011.twitter mood predicts the stock market.
journal ofcomputational science, 2(1):1–8..antoine bordes, nicolas usunier, alberto garcia-duran,jason weston, and oksana yakhnenko.
2013. translating embeddings for modeling multi-relational data.
in advances in neural informationprocessing systems, pages 2787–2795..elizabeth boschee,.
jennifer lautenschlager, seanobrien, steve shellman, james starz, and michaelward.
2015. icews coded event data.
harvard data-verse, 12..wenhu chen, wenhan xiong, xifeng yan, andwilliam yang wang.
2018. variational knowledgein proceedings of naacl-hlt,graph reasoning.
pages 1823–1832..kahneman daniel.
2017. thinking, fast and slow..rajarshi das, shehzaad dhuliawala, manzil zaheer,luke vilnis,ishan durugkar, akshay krishna-murthy, alex smola, and andrew mccallum.
2018.go for a walk and arrive at the answer: reasoningover paths in knowledge bases using reinforcementlearning.
in international conference on learningrepresentations..shib sankar dasgupta, swayambhu nath ray, andpartha talukdar.
2018. hyte: hyperplane-basedtemporally aware knowledge graph embedding.
inproceedings of the 2018 conference on empiricalmethods in natural language processing, pages2001–2011..songgaojun deng, huzefa rangwala, and yue ning.
2020. dynamic knowledge graph based multi-event forecasting.
in proceedings of the 26th acmsigkdd international conference on knowledgediscovery & data mining, pages 1585–1595..tim dettmers, pasquale minervini, pontus stenetorp,convolutional 2din thirty-second.
and sebastian riedel.
2018.knowledge graph embeddings.
aaai conference on artiﬁcial intelligence..zifeng ding, zhen han, yunpu ma, and volker tresp.
2021. temporal knowledge graph forecasting withneural ode.
arxiv preprint arxiv:2101.05151..jonathan st bt evans.
1984. heuristic and analyticprocesses in reasoning.
british journal of psychol-ogy, 75(4):451–468..jonathan st bt evans.
2003..in two minds: dual-process accounts of reasoning.
trends in cognitivesciences, 7(10):454–459..jonathan st bt evans.
2008. dual-processing ac-counts of reasoning, judgment, and social cognition.
annu.
rev.
psychol., 59:255–278..jun feng, minlie huang, li zhao, yang yang, and xi-aoyan zhu.
2018. reinforcement learning for rela-tion classiﬁcation from noisy data.
in proceedingsof the aaai conference on artiﬁcial intelligence, vol-ume 32..luis gal´arraga, christina teﬂioudi, katja hose, andfabian m suchanek.
2015. fast rule mining in on-tological knowledge bases with amie+.
the vldbjournal, 24(6):707–730..alberto garc´ıa-dur´an, sebastijan dumanˇci´c, andmathias niepert.
2018. learning sequence encodersfor temporal knowledge graph completion.
arxivpreprint arxiv:1809.03202..xavier glorot, antoine bordes, and yoshua bengio.
2011. deep sparse rectiﬁer neural networks.
in pro-ceedings of the fourteenth international conferenceon artiﬁcial intelligence and statistics, pages 315–323..rishab goel, seyed mehran kazemi, marcus brubaker,and pascal poupart.
2020. diachronic embeddingfor temporal knowledge graph completion.
in pro-ceedings of the aaai conference on artiﬁcial intel-ligence, volume 34, pages 3988–3995..simon gottschalk and elena demidova.
2018. even-tkg: a multilingual event-centric temporal knowl-edge graph.
in european semantic web conference,pages 272–287.
springer..simon gottschalk and elena demidova.
2019.eventkg–the hub of event knowledge on the web–semanticand biographical timeline generation.
web, (preprint):1–32..kelvin guu, panupong pasupat, evan zheran liu,from language tolearning andarxiv preprint.
and percy liang.
2017.programs: bridging reinforcementmaximum marginalarxiv:1704.07926..likelihood..zhen han, peng chen, yunpu ma, and volker tresp.
explainable subgraph reasoning for fore-casting ontemporal knowledge graphs..zhen han, peng chen, yunpu ma, and volker tresp.
2020a.
dyernie: dynamic evolution of riemannianmanifold embeddings for temporal knowledge graphcompletion.
in proceedings of the 2020 conferenceon empirical methods in natural language process-ing (emnlp), pages 7301–7316..zhen han, yunpu ma, yuyi wang, stephang¨unnemann, and volker tresp.
2020b.
graphhawkes neural network for forecasting on temporalknowledge graphs.
8th automated knowledge baseconstruction (akbc)..4741prachi jain, sushant rathi, soumen chakrabarti, et al.
2020. temporal knowledge base completion: newin proceed-algorithms and evaluation protocols.
ings of the 2020 conference on empirical methodsin natural language processing (emnlp), pages3733–3747..woojeong jin, meng qu, xisen jin, and xiang ren.
2020. recurrent event network: autoregressivestructure inference over temporal knowledge graphs.
in emnlp..woojeong jin, changlin zhang, pedro szekely, and xi-ang ren.
2019. recurrent event network for reason-ing over temporal knowledge graphs.
arxiv preprintarxiv:1904.05530..jaehun jung, jinhong jung, and u kang.
2020. t-gap: learning to walk across time for tempo-arxiv preprintral knowledge graph completion.
arxiv:2012.10595..diederik p kingma and jimmy ba.
2014. adam: amethod for stochastic optimization.
arxiv preprintarxiv:1412.6980..gizem korkmaz, jose cadena, chris j kuhlman, achlamarathe, anil vullikanti, and naren ramakrishnan.
2015. combining heterogeneous data sources forcivil unrest forecasting.
in proceedings of the 2015ieee/acm international conference on advancesin social networks analysis and mining 2015, pages258–265..julien leblay and melisachew wudage chekol.
2018.deriving validity time in knowledge graph.
incompanion proceedings of the the web conference2018, pages 1771–1776.
international world wideweb conferences steering committee..kalev leetaru and philip a schrodt.
2013. gdelt:global data on events, location, and tone, 1979–in isa annual convention, volume 2, pages2012.
1–49.
citeseer..ruiping li and xiang cheng.
2019. divine: a gen-erative adversarial imitation learning framework forknowledge graph reasoning.
in proceedings of the2019 conference on empirical methods in natu-ral language processing and the 9th internationaljoint conference on natural language processing(emnlp-ijcnlp), pages 2642–2651..zixuan li, xiaolong jin, wei li, saiping guan, jiafengguo, huawei shen, yuanzhuo wang, and xueqicheng.
2021. temporal knowledge graph reasoningbased on evolutional representation learning.
arxivpreprint arxiv:2104.10353..xi victoria lin, richard socher, and caiming xiong.
2018. multi-hop knowledge graph reasoning withreward shaping.
arxiv preprint arxiv:1808.10568..sathappan muthiah, bert huang, jaime arredondo,david mares, lise getoor, graham katz, and narenramakrishnan.
2015. planned protest modeling in.
news and social media.
in twenty-seventh iaai con-ference.
citeseer..aldo pareja, giacomo domeniconi, jie chen, tengfeima, toyotaro suzumura, hiroki kanezashi, timkaler, and charles e leisersen.
2019. evolvegcn:evolving graph convolutional networks for dynamicgraphs.
arxiv preprint arxiv:1902.10191..lawrence phillips, chase dowling, kyle shaffer,nathan hodas, and svitlana volkova.
2017. usingsocial media to predict the future: a systematic liter-ature review.
arxiv preprint arxiv:1706.06134..ali sadeghian, miguel rodriguez, daisy zhe wang,and anthony colas.
2016. temporal reasoning overin workshop on knowl-event knowledge graphs.
edge base construction, reasoning and mining..michael schlichtkrull, thomas n kipf, peter bloem,rianne van den berg, ivan titov, and max welling.
2018. modeling relational data with graph convolu-tional networks.
in european semantic web confer-ence, pages 593–607.
springer..youngjoo seo, micha¨el defferrard, pierre van-dergheynst, and xavier bresson.
2018.struc-tured sequence modeling with graph convolutionalin international conferencerecurrent networks.
on neural information processing, pages 362–373.
springer..chao shang, yun tang, jing huang, jinbo bi, xi-aodong he, and bowen zhou.
2019. end-to-endstructure-aware convolutional networks for knowl-edge base completion.
in proceedings of the aaaiconference on artiﬁcial intelligence, volume 33,pages 3060–3067..alessio signorini, alberto maria segre, and philip mpolgreen.
2011. the use of twitter to track lev-els of disease activity and public concern in the usduring the inﬂuenza a h1n1 pandemic.
plos one,6(5):e19467..steven a sloman.
1996..the empirical case fortwo systems of reasoning.
psychological bulletin,119(1):3..zhiqing sun, zhi-hong deng, jian-yun nie, and jiantang.
2018. rotate: knowledge graph embeddingby relational rotation in complex space.
in interna-tional conference on learning representations..ilya sutskever, oriol vinyals, and quoc v le.
2014.sequence to sequence learning with neural networks.
arxiv preprint arxiv:1409.3215..richard s sutton and andrew g barto.
2018. rein-forcement learning: an introduction.
mit press..rakshit trivedi, hanjun dai, yichen wang, andle song.
2017. know-evolve: deep temporal rea-soning for dynamic knowledge graphs.
in proceed-ings of the 34th international conference on ma-chine learning-volume 70, pages 3462–3471..4742rakshit trivedi, mehrdad farajtabar, prasenjeetbiswal, and hongyuan zha.
2018. dyrep: learningrepresentations over dynamic graphs..liang zhao.
2020..a systematic survey..era:arxiv:2007.09815..event prediction in big dataarxiv preprint.
th´eo trouillon, johannes welbl, sebastian riedel, ´ericgaussier, and guillaume bouchard.
2016. com-in in-plex embeddings for simple link prediction.
ternational conference on machine learning, pages2071–2080..cunchao zhu, muhao chen, changjun fan,guangquan cheng, and yan zhan.
2020. learningfrom history: modeling temporal knowledge graphsarxivwith sequential copy-generation networks.
preprint arxiv:2012.08492..shikhar vashishth, soumya sanyal, vikram nitin, andpartha talukdar.
2019. composition-based multi-relational graph convolutional networks.
in interna-tional conference on learning representations..heng wang, shuangyin li, rong pan, and mingzhimao.
2019.incorporating graph attention mech-anism into knowledge graph reasoning based ondeep reinforcement learning.
in proceedings of the2019 conference on empirical methods in natu-ral language processing and the 9th internationaljoint conference on natural language processing(emnlp-ijcnlp), pages 2623–2631..ronald j williams.
1992. simple statistical gradient-following algorithms for connectionist reinforce-ment learning.
machine learning, 8(3-4):229–256..jiapeng wu, meng cao, jackie chi kit cheung, andwilliam l hamilton.
2020. temp: temporal mes-sage passing for temporal knowledge graph comple-in proceedings of the 2020 conference ontion.
empirical methods in natural language processing(emnlp), pages 5730–5746..lijun wu, fei tian, tao qin, jianhuang lai, and tie-yan liu.
2018. a study of reinforcement learn-ing for neural machine translation.
arxiv preprintarxiv:1808.08866..tianxing wu, arijit khan, huan gao, and cheng li.
2019. efﬁciently embedding dynamic knowledgegraphs.
arxiv, pages arxiv–1910..wenhan xiong, thien hoang, and william yang wang.
2017. deeppath: a reinforcement learning methodarxiv preprintfor knowledge graph reasoning.
arxiv:1707.06690..chenjin xu, mojtaba nayyeri, fouad alkhoury,hamed yazdi, and jens lehmann.
2020. temporalknowledge graph completion based on time seriesgaussian embedding.
in international semantic webconference, pages 654–671.
springer..bishan yang, wen-tau yih, xiaodong he, jianfenggao, and li deng.
2014. embedding entities andrelations for learning and inference in knowledgebases.
arxiv preprint arxiv:1412.6575..rui ye, xin li, yujie fang, hongyu zang, andmingzhong wang.
2019. a vectorized relationalgraph convolutional network for multi-relational net-in proceedings of the twenty-work alignment.
eighth international joint conference on artiﬁcialintelligence, ijcai-19, pages 4135–4141..4743