covid-fact: fact extraction and veriﬁcation of real-world claims oncovid-19 pandemic.
arkadiy saakyan1, tuhin chakrabarty 1, and smaranda muresan1,21department of computer science, columbia university2data science institute, columbia universitya.saakyan@columbia.edu, {tuhin.chakr,smara}@cs.columbia.edu.
abstract.
we introduce a fever-like dataset covid-fact of 4, 086 claims concerning the covid-19 pandemic.
the dataset contains claims,evidence for the claims, and contradictoryclaims refuted by the evidence.
unlike previ-ous approaches, we automatically detect trueclaims and their source articles and then gen-erate counter-claims using automatic meth-ods rather than employing human annotators.
along with our constructed resource, we for-mally present the task of identifying relevantevidence for the claims and verifying whetherthe evidence refutes or supports a given claim.
in addition to scientiﬁc claims, our data con-tains simpliﬁed general claims from mediasources, making it better suited for detect-ing general misinformation regarding covid-19. our experiments indicate that covid-factwill provide a challenging testbed for the de-velopment of new systems and our approachwill reduce the costs of building domain-speciﬁc datasets for detecting misinformation..1.introduction.
the proliferation of disinformation and misinfor-mation on the web is increasing at a scale thatcalls for the automation of the slow and labor-intensive manual fact-checking process (vosoughiet al., 2018).
new york times reports that “physi-cians say they regularly treat people more inclinedto believe what they read on facebook than whata medical professional tells them.” disinformationis even more acute around the recent covid-19pandemic.
as a result, there is a need for auto-mated fact-checking tools to assist professionalfact-checkers and the public in evaluating the ve-racity of claims that are propagated online in newsarticles or social media..ideally, a fact-checking pipeline will address sev-eral tasks: 1) consider real-world claims, 2) re-trieve relevant documents not bounded to a known.
figure 1: a claim from the r/covid19 subreddit withan academic report as an evidence source linked to it..document collection (e.g., wikipedia) and whichcontain information to validate the claim, 3) se-lect evidence sentences that can support or refutethe claim and 4) predict the claim veracity basedon this evidence.
recent work on end-to-end fact-checking, including models and datasets, has ad-vanced the ﬁeld by addressing several tasks in thepipeline, but not all (thorne et al., 2018, 2019;hanselowski et al., 2019; augenstein et al., 2019;diggelmann et al., 2021; wadden et al., 2020).
one line of work that includes fever (thorneet al., 2018, 2019) and scifact (wadden et al.,2020) addresses tasks 2, 3 and 4, but assumes agiven document collection for task 2 (wikipedia orcord-19, respectively) and does not address task1. moreover, the refuted claims in these datasetsare manually generated by asking humans to pro-duce counter-claims for a given claim supportedby a source document.
another line of work thatincludes multi-fc (augenstein et al., 2019) ad-dresses tasks 1, 2 and 4, but not 3. it providesreal-world claims collected from fact-checkingwebsites and evidence documents and other meta-information, but it does not provide evidence sen-tences..we propose a novel semi-automatic methodto build a fact-checking dataset for covid-19(covid-fact) with the goal of facilitating all theabove tasks.
we make the dataset and code avail-able for future research at https://github.com/asaakyan/covidfact.
our contributions are as.
proceedingsofthe59thannualmeetingoftheassociationforcomputationallinguisticsandthe11thinternationaljointconferenceonnaturallanguageprocessing,pages2116–2129august1–6,2021.©2021associationforcomputationallinguistics2116original claimcounter-claimgold document.
gold evidence.
closed environments facilitate secondary transmission of coronavirus disease 2019closed environments prevent secondary transmission of coronavirus disease 2019https://www.medrxiv.org/content/10.1101/2020.02.28.20029272v2.
it is plausible that closed environments contribute to secondary transmission ofcovid-19 and promote superspreading events..original claimcounter-claimgold document.
oxford vaccine triggers immune responseoxford vaccine inhibits immune responsehttps://www.bbc.com/news/uk-53469839.
gold evidence.
they are injecting coronavirus rna (its genetic code), which then starts makingviral proteins in order to trigger an immune response..table 1: original and counter-claims from our dataset with gold documents and evidence sentences identiﬁed byour system supporting and refuting them, respectively..follows:.
• automatic real-world true claim and trust-worthy evidence document selection (sec-tion 2.1).
we start with the heavily moder-ated r/covid19 subreddit, that requires everyclaim/title post to be accompanied by a sourceevidence document from peer-reviewed re-search, pre-prints from established servers,or information reported by governments andother reputable agencies.
figure 1 shows onesuch claim with the associated source belong-ing to the academic report ﬂair.
we proposeadditional ﬁltering methods to ensure sourcequality and that claims are well-formed.
thisstep provides us with real-world true claimsabout covid-19 and evidence documentsnot bounded to a known document collection.
moreover, the language of the claims couldbe both technical and lay (see figure 1 andtable 1), unlike scifact which is geared onlytowards scientiﬁc claims..• automatic generation of counter-claims (sec-tion 2.2).
an end-to-end fact-checking systemrequires both true and false claims for training.
following fever and scifact, to obtain falseclaims, we aim to generate counter-claimsof the original true claim.
the advantage isthat we obtain evidence documents/sentencesfor free.
however, unlike fever and sci-fact, we propose a novel approach to au-tomatically generate counter-claims from agiven claim using two steps: 1) select salientwords from the true claim using attentionscores obtained from a bert (devlin et al.,2019) model ﬁne-tuned on the scifact dataset,and 2) replace those words with their oppo-sites using masked language model inﬁlling.
with entailment-based quality control.
table 1shows examples of generated counter-claims..• evidence sentence selection using text simi-larity and crowdsourcing (section 2.3).
forevidence sentence selection, we calculate thesemantic similarity between the original trueclaim and the sentences in source evidencedocuments using sentence-bert (sbert)(reimers and gurevych, 2019), retrieve topﬁve sentences and use crowdsourcing for ﬁnalvalidation.
table 1 shows examples of evi-dence sentences that support the true claimsand refute the corresponding counter-claims..• covid-fact dataset of 4,086 real-worldclaims annotated with sentence-level evidenceand a baseline on this task.
our resultsshow that models trained on current datasets(fever, scifact) do not perform well on ourdata (section 4).
moreover, we show the use-fulness of our dataset through zero-shot per-formance on the scientiﬁc claim veriﬁcationtask on scifact (wadden et al., 2020) data(section 4)..2 covid-fact dataset construction.
the covid-fact dataset contains 4, 086 real-worldclaims with the corresponding evidence documentsand evidence sentences to support or refute theclaims.
there are 1, 296 supported claims and2, 790 automatically generated refuted claims.
inthis section, we present the three main steps to semi-automatically construct this dataset: 1) real-worldtrue claim and trustworthy evidence document se-lection (section 2.1), 2) automatic counter-claimgeneration (section 2.2) and 3) evidence sentenceselection (section 2.3)..21172.1 real-world claim and trustworthyevidence document selection.
the subreddit r/covid19 is a heavily moderatedonline discussion forum that seeks to facilitate sci-entiﬁc discussion around covid-19.
each post onthis subreddit has a title and needs to contain a linkto a source, governed by several rules: posts link-ing to non-scientiﬁc sources will be removed; com-ments making a statement as fact or which includeﬁgures or predictions also need to be supported byevidence; allowed sources include peer-reviewedresearch, pre-prints from established servers, andinformation reported by governments and other rep-utable agencies.
moreover, the posts are annotatedwith “ﬂairs”, or short description of the posts’ cat-egory such as academic report, academic com-ment, preprint, clinical, antivirals, governmentagency, epidemiology, ppe/mask research, gen-eral.
having access to such ﬂairs allows to selectclaims, for example, related to “vaccine research”or “epidemiology”.
this could further help in train-ing models targeting even more speciﬁc types ofdisinformation, like disinformation about antiviralsor ppe/masks.
in our study, the titles of the postare considered candidate claims and the associatedsources are considered evidence documents.
postsfrom the r/covid19 subreddit are extracted viathe pushshift reddit api.1 two issues still need tobe addressed: 1) ensure that titles are well-formedclaims; 2) ensure the highest trustworthiness of theposts and their associated sources..filtering for well-formed claims.
the deﬁni-tion of a claim can vary depending on domain,register or task (daxenberger et al., 2017).
forour work, we consider a claim to be a propositionwhose truthfulness can only be determined by ad-ditional evidence.
in addition, a well-formed claimhas to be a full sentence.
thus, to ﬁlter out mostof the titles that are not well-formed claims, weemploy a simple syntax-based approach to removequestions and consider statements that have at leasta main verb.
this ﬁltering steps allows us to re-move titles such as ”b cell memory: understandingcovid-19” and consider titles such as the ones infigure 1 and table 1. in addition, we ask three vol-unteer computer science students with backgroundin argumentation and linguistics to manually ver-ify that the entire resulting set does indeed containonly well-formed claims.
while we could have em-.
ployed more sophisticated claim detection methods,there are no large-scale datasets for covid-19 totrain a claim detection model.
we therefore did notwant to introduce additional noise in our dataset byusing a machine learning approach..filtering for trustworthiness.
to ensure hightrustworthiness of posts (and thus our true claims)and the linked sources, we employ several ﬁlteringsteps.
first, the posts in this subreddit undergomoderation, and thus we discard titles/claims thatbelong to posts ﬂagged as taken down by the mod-erators using the posts’ “removed” ﬂair.
moreover,users of the reddit platform may upvote or down-vote a post, and the ratio of upvotes can serve asa rough indication of the reliability of the source.
hence, posts (and thus claims/sources) with up-vote ratio lower than 0.7 are rejected.
we thenreject claims where the linked source in the posthas an alexa site rank2 lower than 50, 000, reject-ing the outliers by the site rank (see the box plotin appendix b.2).
finally, we reject a claim if thelinked source in the post does not appear in the top5 google search results when querying the title ofthe post..from an initial set of 22, 646 posts, automaticsyntactic ﬁltering for well-formed claims results ina set of 6, 154 claims, further reduced to 1, 526 af-ter ﬁltering for trustworthiness and ﬁnally reducedto 1, 407 through manual validation.
thus, the re-sulted dataset after all the ﬁltering steps consistsof 1, 407 true claims and the associated source evi-dence documents (an additional set of 111 claimsare removed in the evidence sentence selection stepin section 2.3).
besides the linked source docu-ment in the post, we retrieve for each claim fouradditional sources from the top 5 google searchresults.
this is motivated by the fact that the sameclaim can be reported by various sources.
for ex-ample, the second claim in table 1 “oxford vac-cine triggers immune response” is reported, be-sides the bbc.com given in the original post, also byother trustworthy sources such as usnews.com, med-scape.com, cnbc.com.
unlike fever and scifact,which constrain their evidence document collectionto wikipedia or pre-selected scientiﬁc articles, wecollect evidences from any of the websites linkedto the reddit post or appearing in the top 5 googlesearch results.
even though over time the googlesearch results may change, the collection of evi-dence documents for covid-fact is considered.
1https://github.com/pushshift/api.
2https://www.alexa.com/topsites.
2118ﬁxed and will be released for reproducibility..like scifact (wadden et al., 2020), our datasetcontains several claims with scientiﬁc jargon suchas “altered blood cell traits underlie a major ge-netic locus of severe covid-19”.
however, unlikescifact, our dataset also contains scientiﬁc claimsexpressed in lay terms.
for example, a claim like“loss of smell is a symptom of covid-19” is muchsimpler and can be understood by a wider audi-ence compared to “emerging evidence supportsrecently acquired anosmia and hyposmia as symp-toms of covid-19”.
this is important, as a lotof (dis)information is expressed in lay language in-tended for the general public not versed in scientiﬁclanguage.
another issue adding to the complexityof the task around covid-19 (dis)information arenon-scientiﬁc claims that focus on public healthpolicies or statements from public health author-ities.
for example, a claim like “cdc says newcovid strain in uk could already be circulatingundetected in u.s” would not occur in scientiﬁcliterature, but occurs in media outlets linked assources in the r/covid19 subreddit..2.2 automatic counter-claim generation.
an end-to-end fact-checking system requires bothtrue and false claims.
following fever and sci-fact, to obtain false claims we aim to generatecounter-claims of the original true claims (fromsection 2.1).
however, in fever (thorne et al.,2018) and scifact (wadden et al., 2020) the gen-eration of counter-claims was done manually byhuman annotators, which is an expensive approachthat might not scale well.
we propose an approachto generate counter-claims automatically (see ta-ble 1 for examples).
our counter-claim genera-tion consists of two stages: 1) select salient wordsfrom the true claims, and 2) replace those wordswith their opposite using mask language modelinﬁlling with entailment-based quality control.
wediscuss these steps below..2.2.1 salient words selectionsalient words (keywords) are essential to the over-all semantics of a sentence.
for example, in theclaim ”oxford vaccine triggers immune response”,a salient word would be “triggers”.
by changingthe word ”triggers” to ”inhibits” we change themeaning of above claim to its opposite (counter-claim).
recently zhang et al.
(2020b) used yake(campos et al., 2018, 2020), an unsupervised au-tomatic keyword extraction method for selecting.
salient words to guide their text generation process.
for selecting salient words from a claim, we ex-periment with yake as one of our methods.
inaddition, we explore an attention-based methoddescribed below..attention-based salience.
recently, sudhakaret al.
(2019) use self-attention scores from bert(devlin et al., 2019) to delete keywords from aninput sequence for the task of style transfer.
theyuse a novel method to extract a speciﬁc attentionhead and layer combination that encodes style infor-mation and that can be directly used as importancescores.
inspired by them, we use the same approachfor our task.
we ﬁne-tuned bert for a sentenceclassiﬁcation task (veracity prediction) on the sci-fact (wadden et al., 2020) dataset, and extract theattention scores from the resulting model.
giventhe scifact dataset d = (x1, y1), ..., (xm, ym)where xi is a claim and yi(cid:15) {supported, re-futed} is a veracity label we observe that theself-attention based classiﬁer deﬁnes a probabilitydistribution over labels: p(y|x) = g(v, α) wherev is a tensor such that v[i] is an encoding of x[i],and α is a tensor of attention weights such that α[i]is the weight attributed to v[i] by the classiﬁer indeciding probabilities for each yj .
the α scorescan be treated as importance scores and be used toidentify salient words..quality of salient words selection.
we evalu-ate how well our salient word selection methodscorrelate with human judgement.
we randomlyselect 150 original claims for an amazon mechani-cal turk task.
the annotators were asked to selecta word that could potentially invert the meaningof the sentence if it were to be replaced.
for ev-ery claim, three separate annotators were recruitedwhich means that we would have at most three dif-ferent chosen salient keywords.
for each claim, wecompute the set intersections between the three key-words selected by our automatic methods (yakeand attention-based) vs.the keywords selectedby the annotators on amturk.
we found that key-words selected using self-attention scores have asigniﬁcantly higher recall (two-proportion z-testwith p-value < .00001) than yake (68% vs. 54%).
the average number of words per claim in covid-fact is 14, so the task of selecting one salient key-word is challenging even for humans.
given this,our recall@3 scores demonstrate the reliability ofautomatic attention-based salient word selection..21192.2.2 masked language model inﬁlling withentailment-based quality control.
after selecting salient words from the true claimsfor replacement, we need to provide only para-phrases that are opposite in meaning and considerthe context in which these words occur.
languagemodels have been used previously for inﬁlling tasks(donahue et al., 2020) and have also been used forautomatic claim mutation in fact checking (jianget al., 2020).
inspired by these approaches, we usethe masked language model (mlm) roberta(liu et al., 2019) ﬁne-tuned on cord-19 (wanget al., 2020) for inﬁlling.
the ﬁne-tuned robertais available on huggingface 3. we generate a largenumber (10-30) of candidate counter-claims withreplaced keywords per each original claim..after generating multiple candidate counter-claims based on mlm inﬁlling, we select the onesthat have the highest contradiction score with theoriginal claim.
to compute the contradiction scorewe use the roberta (liu et al., 2019) modeltrained on multi-nli (williams et al., 2018) dueto its size and diversity.
the scores are in therange from 0 to 1. we ﬁrst set the minimum scorethreshold and then select top three claims abovethe threshold..to select the right threshold for contradictionscore-based ﬁltering we perform the following ex-periment.
we presented 150 randomly selectedclaims to amazon mechanical turk workers.
an-notators were presented with the original claimand ﬁve generated candidate counter-claims frommlm inﬁlling.
they were then asked if thoseclaims are implied by the original claim (hence,for example, noun shifts would be judged as “notimplied”).
we labeled claims as “contradictory” ifthe majority of the annotators agreed on the label.
we observed a point-biserial correlation of 0.47between dichotomous human judgement and con-tinuous contradiction scores, indicating moderateagreement.
we convert the contradiction scores tobinary outcomes, assigning 1 if the score is abovethe threshold and 0 otherwise.
we compute pre-cision, recall, f1 score and accuracy for differentthresholds.
as threshold value increases, we see asteady increase in precision, indicating that takinga higher threshold value we are almost guaranteedto select a contradictory sentence (for example, fora threshold of 0.995, precision is 93%).
obviously,.
3https://huggingface.co/amoux/.
roberta-cord19-1m7k.
this comes at a cost of decreased recall.
we se-lected a threshold of 0.9 (precision 76%), sincewe want to prioritize precision, but do not wantto reduce our dataset too much due to the low re-call.
at this threshold, our 1, 407 claims generateadditional 4, 042 false claims.
an alternative ap-proach of replacing salient words with antonymsfrom standard lexicons like wordnet (miller, 1995)was considered.
however, a suitable antonym wasabsent in several cases, most notably nouns.
theroberta model is able to provide domain-awaresubstitutions.
for example, replacing the word “hu-mans” by the word “mice” reverses the meaning ofthe claim the domain of clinical trial reports, yet thewords human and mouse can hardly be consideredantonyms.
lexical replacement without considera-tion of context can also cause grammatical issues..our method of counter-claim generation onlychanges a single word or a multi-word expression,since pre-trained mlms like bert (devlin et al.,2019) and roberta (liu et al., 2019) do not allowfor multiple word masking.
however, this methodcan be extended to masking multiple words us-ing recent pre-trained language models like bart(lewis et al., 2020)..2.2.3 analysis of counter-claim generation.
upon deeper inspection we observe that the atten-tion scores described in section 2.2.1 were dis-tributed across different parts of speech like verbsor adjective modiﬁers or nouns.
we show the dis-tribution of the most frequent parts of speech ofsalient words and replacement words in our datasetin figure 2. this means our counter-claims weregenerated with more creativity than just the addi-tion of obvious triggers like “not”.
the majority ofclaim negations involved a reversal of effect direc-tion; for instance “suspicions grow that nanopar-ticles in pﬁzer’s covid-19 vaccine trigger rareallergic reactions.” was negated as “suspicionsgrow that nanoparticles in pﬁzer’s covid-19 vac-cine trigger systemic allergic reactions.” where asimple adjective modiﬁer changes the truthfulness.
similarly for a claim “electrostatic spraying willprevent the spread of covid-19” a negated claimis “electrostatic spraying will facilitate the spreadof covid-19” which ﬂips the main verb in theclaim.
in table 2, one can see several examplesof how the generated counter-claims reverse themeaning of the original sentence..2120crowdsourcing for final evidence sentence se-lection.
amazon mechanical turk workers weregiven a claim and the 5 automatically selected can-didate evidence sentences.
they were asked toselect which of the evidence sentences support theclaim (they could select several) or they could se-lect that the evidence is absent.
to discourage lowquality responses, we used a trick sentence thatwould allow us to disqualify dishonest entries.
forthe trick we used a phrase “it is not true that” con-catenated with the original sentence, and rejectedentries that marked that option as evidence for theclaim.
in 111 cases, annotators could not agreeon the evidence or agreed that the evidence wasabsent, where agreement is deﬁned as the major-ity vote.
we disregard these true claims from ourcovid-fact dataset as they would not have associ-ated evidence sentences..we asses the quality of the majority vote annota-tions by comparing the gold evidence label anno-tations with an independent re-annotation by threeamazon mechanical turk workers.
we select asample of 100 claims’ evidences (7% of the 1, 296original claims).
we observe a cohen’s kappa (co-hen, 1968) of 0.5 between majority votes of the twoindependent groups of amazon turk workers, in-dicating moderate agreement (artstein and poesio,2008).
we ﬁnd this encouraging given the com-plexity of the task, especially considering that theworkers did not have domain-speciﬁc knowledge..3 experimental setup.
table 3 shows the dataset statistics for train/dev/testsplit of covid-fact..3.1 covid-fact task formulation.
covid-fact task follows the fever shared taskdeﬁnition.
the set of all claims is denoted by c.the set of gold evidence sentences for a claim c ∈c is denoted by e(c).
the gold label for a givenclaim and evidence pair is deﬁned as v(c, e(c)) ∈{supported, refuted}.
the task consists ofthe following subtasks outlined below..evidence retrieval.
given a claim c, a systemmust retrieve a set of up to ﬁve evidence sentencesˆe(c).
we evaluate the evidence retrieval systemquality using precision, recall, and f1 scores.
evi-dence recall is computed as the number of evidencesets that contain a gold evidence over the total num-ber of evidence sets..figure 2: most frequent pos tags of salient words..original..people in uk receive .... human ace2.
fda takes key action ...generated.
..mice in uk receive .
.
.
.. bat ace2.
who takes key action .....improves the effect ....blocks sars-cov-2.....inhibits the effect ....enchanced sars-cov-2.....are not ﬁt for purpose .... the ﬁnal stage .... shows positive results....are good ﬁt for .... the ﬁrst stage .... shows no results..table 2: a detailed look into what parts of speech arereplaced, and in what direction the claims are reversed.
we omitted full claims due to space constraint.
theﬁrst 3 claims show nouns, the next 2 show verbs andthe ﬁnal 3 show adjective modiﬁcations..2.3 evidence sentence selection.
to select evidence sentences we follow the ap-proach proposed by hidey et al.
(2020).
giventhe true claims and the 5 evidence documents foreach claim (section 2.1) we use cosine similar-ity on sbert sentence embeddings (reimers andgurevych, 2019) to extract the top 5 sentences mostsimilar to the true claim.
note that we only needto do this step for true claims, as automaticallythe evidence sentences that support the true claimwill be the evidence sentences that refute the corre-sponding counter-claims.
sentences containing theclaim itself were discarded.
the collected ﬁve sen-tences will serve as candidate evidence sentencesfor future human validation described below..splittraindevtest.
supported refuted.
1036130130.
2227289274.table 3: breakdown of claims by label for train, dev,test sets..2121veracity prediction.
given a claim c and a set ofevidence sentences e(c), a system must determinea label ˆv(c, e(c)) ∈ {supported, refuted}.
we evaluate veracity prediction using f1 score andaccuracy..the performance of roberta-large ﬁne-tuned onfever, scifact, mnli and our covid-factdataset.
moreover, we also experiment with ﬁne-tuning roberta-large on scifact + covid-factand on fever + covid-fact..a.a.and.
ˆe(c),.
determine.
evidence retrieval + veracity prediction(covid-fever score) givenclaimc, a system must retrieve a set of evidencelabelsentencesˆv(c, ˆe(c)) ∈ {supported, refuted}.
a claim has a covid-fever of 1 if it correctlypredicts the veracity of the claim-evidence pairand if at least one of the predicted evidence fromthe predicted evidence matches the gold evidenceselected by annotators (thus a stricter score thanveracity prediction accuracy).
this metric issimilar to the fever score (thorne et al., 2018)..3.2 baseline pipeline for covid-fact.
our end-to-end pipeline consists of the followingsteps: 1) evidence retrieval using google search +sbert 2) veracity prediction using roberta ﬁne-tuned on fact-checking and entailment inferencedatasets..baseline for evidence retrieval.
we use thesame approach as was used for the constructionof the dataset to provide a strong baseline for evi-dence retrieval on covid-fact.
google search wasused to identify ﬁve potential source documents byquerying the claim.
this step is followed by se-lecting most similar sentences through computingcosine similarity between sentence embeddings ofthe claim and candidate sentences using sbert(reimers and gurevych, 2019)..baseline for veracity prediction.
our baselinefor veracity prediction is a roberta model.
weconcatenate all evidence sentences in the evidenceset and use it as input for a binary classiﬁcationtask similar to the glue rte task (wang et al.,2018).
we evaluate the models with gold evidence,as well as top-5 and top-1 evidences ranked bysbert cosine similarity with the original claim..3.3 experiments.
besides evaluating our baseline pipeline on thecovid-fact dataset, we perform several additionalexperiments outlined below.
all hyperparameterscan be found in appendix a..adequacy of existing datasets for covid-fact.
for the task of veracity prediction, we evaluate.
usefulness of covid-fact for zero-shot scien-tiﬁc fact-checking.
even though not explicitlydesigned for covid-19 related claims, waddenet al.
(2020) showed how models trained on thescifact dataset could verify claims about covid-19 against the research literature.
covid-fact onthe contrary was not explicitly designed for scien-tiﬁc fact-checking, although our resource containsa substantial number of scientiﬁc claims.
this pro-vides us the opportunity to test the generalizabilityand robustness of our dataset.
to do so, we trainmodels on covid-fact claims and gold evidenceand evaluate the veracity performance on the sci-fact dev set in a zero-shot setting.
we removethe not enough info claims from the scifactdataset..4 results and analysis.
table 5 summarizes the results for the evidenceretrieval evaluation.
our pipeline provides a strongbaseline with f1 score of ≈ 32. for comparison,the baseline system in fever (thorne et al., 2019)achieves the f1 score of 18.26. note top 5 evi-dence retrieval performs worse than gold since weevaluate how the system performs with automati-cally negated claims as well, for which we re-runthe google+sbert method..table 4 summarizes the results for the verac-ity prediction task using gold and retrieved evi-dence.
we observe that, given the gold evidences,ﬁne-tuning on covid-fact led to performance im-provement of 25 f1-score and 35 f1-score com-pared to training solely on scifact and feverrespectively.
this indicates that the covid-factdataset is challenging and cannot be solved usingpopular fact-checking datasets like fever andscifact.
this could be explained by the fact thatclaims about covid-19 are comprised of a mixof scientiﬁc and general-domain claims.
the poormacro-f1 score for claim only baseline shows thatthe model does not learn spurious correlation be-tween a claim and the veracity label.
with top 5and top 1 retrieved evidences, we observed thatcovid-fact is still difﬁcult to outperform.
thezero-shot performance is negligibly affected by theretrieved evidence.
our baseline pipeline achieves.
2122mnli (williams et al., 2018)scifact (wadden et al., 2020)fever (thorne et al., 2018)covid-factscifact + covid-factfever + covid-factcovid-fact (claim only).
veracity predictiongold.
top 5.top 1.acc61.356.948.383.582.274.867.5.f164.257.047.082.081.070.040.0.acc53.153.746.284.783.078.2-.
f151.554.045.083.082.073.0-.
acc65.454.348.683.280.273.3-.
f160.654.048.081.079.068.0-.
covid-fevertop 5score35.136.935.443.343.035.4-.
table 4: performance of various training conﬁgurations of roberta-large in the veracity prediction as well asevidence retrieval + veracity prediction (see section 3.1).
the top 3 rows under veracity prediction show a zeroshot setting where models are trained on existing fact-checking datasets.
we test the model performance on claimswith gold evidence selected by humans vs claims with top 5 retrieved evidences and top 1 retrieved evidence oncovid-fact test set.
p < .001 using approximate randomization test..evidence retrieval.
p22.2724.7729.68.r52.3745.1429.93.f131.2531.9929.80.top 5top 3top 1.table 5: performance of our system’s evidence re-trieval part (see section 3.1).
we compare the precision(p), recall (r), and f1-score of top 5, top 3, top 1 re-trieved sentences, ranked by sbert cosine similarityscore..train setting acc80.8covid-fact83.7sci-fact.
f180.083.0.table 6: two-way veracity prediction results on sci-fact dev set by models trained on covid-fact data aswell as sci-fact data..the covid-fever score of 43.3 using top 5 evi-dence sentences.
adding the fever and scifactdatasets deteriorates the results..table 6 shows a strong zero shot performance ofcovid-fact for scientiﬁc claim veriﬁcation (train-ing on covid-fact train set, testing on the scifactdev set).
scifact only contains scientiﬁc claims,therefore the model trained only on scifact doesnot generalize well to covid-fact, which alsocontains non-scientiﬁc claims.
covid-fact, onthe other hand, contains enough scientiﬁc claimsso that the model generalizes well to scifact.
thisresult shows semi-automated covid-fact is notinferior to mostly manual scifact..error analysis.
we observe that errors in ve-racity prediction can be attributed to three fac-tors: cause and effect, commonsense or scientiﬁcbackground.
for instance, in the ﬁrst (c1, ev1)pair in table 7, not detectable is the cause while.
c1.
ev1.
c2.
ev2.
sars-cov-2 is not detectable in the vaginalﬂuid of women with severe covid-19 infectionall 10 patients were tested for sars-cov-2 invaginal ﬂuid,and all samples tested negative forthe virus..baricitinib restrains the immune dysregulationin covid-19 patientshere, we provide evidences on the efﬁcacy ofbaricitini, a jak1/jak2 inhibitor, in correctingthe immune abnormalities observed in patientshospitalized with covid-19..table 7: claims (c1 & c2) which are classiﬁed incor-rectly as refutes in the light of supporting evi-dence by our best veracity models.
words crucial forcorrect veriﬁcation are highlighted..testing negative is the effect.
to verify this claim,the veracity model needs to have knowledge ofcounterfactuals.
furthermore, it should be under-stood that all 10 patients mention in ev1 shouldrefer to women in c1, due to mention of “vaginalﬂuids” but this requires commonsense knowledgeoutside the text.
finally, it might be hard for ve-racity models to correctly classify claim evidencepairs which include knowledge of domain-speciﬁcor scientiﬁc lexical relationships.
for instance in(c2, ev2) we see that both bolded phrases in redand blue refer to the same phenomena, but immunedysregulation is “a breakdown of immune systemprocesses” and restraining it can be seen as thesame concept as correcting immune abormalities,but the model is not able to capture such complexdomain speciﬁc knowledge..5 related work.
fact-checking.
approaches for predicting theveracity of naturally-occurring claims have focused.
2123on statements fact-checked by journalists or organi-zations such as politifact.org (vlachos and riedel,2014; alhindi et al., 2018), news articles (pomer-leau and rao, 2017), or answers in communityforums (mihaylova et al., 2018, 2019).
mixed-domain large scale datasets such as ukp snopes(hanselowski et al., 2019), multifc (augensteinet al., 2019), and fever (thorne et al., 2018,2019) rely on wikipedia and fact-checking web-sites to obtain evidences for their claims.
eventhough these datasets contain many claims, duedo domain mismatch they may be difﬁcult to ap-ply for covid-19 related misinformation detec-tion.
scifact (wadden et al., 2020) introducedthe task of scientiﬁc fact-checking, generating adataset of 1.4k scientiﬁc claims and correspond-ing evidences from paper abstracts annotated byexperts.
however, the dataset does not containsimpliﬁed scientiﬁc claims encountered in newsand social media sources, making it difﬁcult tooptimize for a misinformation detection objective.
another approach to misinformation detection sim-ilar to ours is climate-fever (diggelmannet al., 2021).
they adapted fever methodologyto create a dataset speciﬁc to climate change fact-checking.
however, due to difﬁcult and expensivemethods employed for generation of fever, it canbe difﬁcult to extrapolate this method to assemblea covid-19 speciﬁc dataset..covid-19 related nlp tasks.
numerous nlpapproaches were employed to aid the battle withthe covid-19 pandemic.
notably wang et al.
(2020) released cord-19, a dataset containing140k papers about covid-19 and related topicswhile zhang et al.
(2020a) created a neural searchengine covidex for information retrieval.
tocombat misinformation lee et al.
(2020) proposeda hypothesis that misinformation has high perplex-ity.
hossain et al.
(2020) released covidlies: adataset of 6761 expert-annotated tweets matchedwith their stance on known covid-19 miscon-ceptions.
the dataset provides a comprehensiveevaluation of misconception retrieval but does notanalyze evidence retrieval and prediction of verac-ity of claims based on presented evidence.
po-liak et al.
(2020) collected 24,000 question withexpert answers from 40 trusted websites to helpnlp research with covid related information.
covid-fact, on the other hand, deals with realworld claims and presents an end-to-end fact check-ing system to ﬁght misinformation..6 conclusion.
we release a dataset of 4,086 claims concerning thecovid-19 pandemic, together with supporting andrefuting evidence.
the dataset contains real-worldtrue claims obtained from the r/covid19 sub-reddit as well as automatically generated counter-claims.
our experiments reveal that our datasetoutperforms zero-shot baselines trained on popularfact-checking benchmarks like scifact and fever.
this goes on to prove how domain-speciﬁc vocabu-lary may negatively impact the performance of pop-ular nlp benchmarks.
finally, we demonstrate asimple, scalable, and cost-efﬁcient way to automat-ically generate counter-claims, thereby aiding increation of domain-speciﬁc fact-checking datasets.
we provide a detailed evaluation of the covid-fact task and hope that our dataset serves as achallenging testbed for end-to-end fact-checkingaround covid-19..7 ethics.
the data was collected from reddit keeping userprivacy in mind.
reddit is a platform where userspost publicly and anonymously.
for our dataset,only titles and links to external publicly availablesources like news outlets or research journals werecollected, as well as post metadata such as ﬂairs,upvote ratio, and date of the post.
user-identifyinginformation, including, but not limited to, user’sname, health, ﬁnancial status, racial or ethnic ori-gin, religious or philosophical afﬁliation or beliefs,sexual orientation, trade union membership, al-leged or actual commission of crime, was not re-trieved and is not part of our dataset.
for all thecrowdsourcing annotation work, we fairly com-pensate crowd workers in accordance with localminimum wage guidelines..one signiﬁcant concern might arise regardingthe use of language models for counter-claim gener-ation.
our model is a controlled generation system(word-level replacement) and is not suited for gen-eration of entirely new and original claims.
neitherit is the case that it can be used for generation of en-tire articles of false information, or generating falseevidence for the counter-claims.
the model for re-placing keywords from original claims is trained oncord-19 (wang et al., 2020), a scientiﬁc corpusof high quality and trustworthy information aboutcovid-19.
we generate counter-claims to createa resource that will help nlp models learn howto identify false information and provide evidence.
2124for the predicted label leading to more explain-able models.
consequently, our approach is suitedfor improving entailment and veracity predictionperformance of fact-checking systems, rather thanimproving generative qualities of false-claim gen-eration systems.
the fact that we use our modelto generate false claims also helps to address theconcerns of biased language generation.
in the un-likely event our model produces biased claims, theycould serve as good examples of false claims con-taining bias, which would be an interesting topicfor further research (bias in disinformation).
wetherefore believe the net positive impact of ourwork far outweighs the potential risks..references.
tariq alhindi, savvas petridis, and smaranda mure-san.
2018. where is your evidence: improving fact-checking by justiﬁcation modeling.
in proceedingsof the first workshop on fact extraction and ver-iﬁcation (fever), pages 85–90, brussels, belgium.
association for computational linguistics..ron artstein and massimo poesio.
2008. inter-coderagreement for computational linguistics.
comput.
linguist., 34(4):555–596..isabelle augenstein, christina lioma, dongshengwang, lucas chaves lima, casper hansen, chris-tian hansen, and jakob grue simonsen.
2019.multifc: a real-world multi-domain dataset forevidence-based fact checking of claims.
in proceed-ings of the 2019 conference on empirical methodsin natural language processing and the 9th inter-national joint conference on natural language pro-cessing (emnlp-ijcnlp), pages 4685–4697, hongkong, china.
association for computational lin-guistics..ricardo campos, v´ıtor mangaravite, arian pasquali,al´ıpio m´ario jorge, c´elia nunes, and adam jatowt.
2018. yake!
collection-independent automatic key-word extractor.
in european conference on informa-tion retrieval, pages 806–810.
springer..ricardo campos, v´ıtor mangaravite, arian pasquali,al´ıpio jorge, c´elia nunes, and adam jatowt.
2020.yake!
keyword extraction from single documentsusing multiple local features.
information sciences,509:257 – 289..jacob cohen.
1968. weighted kappa: nominal scaleagreement with provision for scaled disagreementor partial credit.
psychological bulletin, pages 213–220..johannes daxenberger, steffen eger, ivan habernal,christian stab, and iryna gurevych.
2017. what isthe essence of a claim?
cross-domain claim identi-ﬁcation.
in proceedings of the 2017 conference on.
empirical methods in natural language processing,pages 2055–2066, copenhagen, denmark.
associa-tion for computational linguistics..jacob devlin, ming-wei chang, kenton lee, andkristina toutanova.
2019. bert: pre-training ofdeep bidirectional transformers for language under-in proceedings of the 2019 conferencestanding.
of the north american chapter of the associationfor computational linguistics: human languagetechnologies, volume 1 (long and short papers),pages 4171–4186, minneapolis, minnesota.
associ-ation for computational linguistics..thomas diggelmann, jordan boyd-graber, jannis bu-lian, massimiliano ciaramita, and markus leippold.
2021. climate-fever: a dataset for veriﬁcation ofreal-world climate claims..chris donahue, mina lee, and percy liang.
2020. en-abling language models to ﬁll in the blanks.
in pro-ceedings of the 58th annual meeting of the asso-ciation for computational linguistics, pages 2492–2501, online.
association for computational lin-guistics..andreas hanselowski, christian stab, claudia schulz,zile li, and iryna gurevych.
2019. a richly anno-tated corpus for different tasks in automated fact-the 23rd confer-checking.
ence on computational natural language learning(conll), pages 493–503, hong kong, china.
asso-ciation for computational linguistics..in proceedings of.
christopher hidey, tuhin chakrabarty, tariq alhindi,siddharth varia, kriste krstovski, mona diab, andsmaranda muresan.
2020. deseption: dual se-quence prediction and adversarial examples for im-proved fact-checking.
in proceedings of the 58th an-nual meeting of the association for computationallinguistics, pages 8593–8606, online.
associationfor computational linguistics..tamanna hossain, robert l. logan iv, arjuna ugarte,yoshitomo matsubara, sean young, and sameersingh.
2020. covidlies: detecting covid-19misinformation on social media.
in proceedings ofthe 1st workshop on nlp for covid-19 (part 2)at emnlp 2020, online.
association for computa-tional linguistics..yichen jiang, shikha bordia, zheng zhong, charlesdognin, maneesh singh, and mohit bansal.
2020.hover: a dataset for many-hop fact extraction andin findings of the associationclaim veriﬁcation.
for computational linguistics: emnlp 2020, pages3441–3460, online.
association for computationallinguistics..nayeon lee, yejin bang, andrea madotto, and pascalefung.
2020. misinformation has high perplexity..mike lewis, yinhan liu, naman goyal, mar-jan ghazvininejad, abdelrahman mohamed, omerlevy, veselin stoyanov, and luke zettlemoyer..21252020. bart: denoising sequence-to-sequence pre-training for natural language generation, translation,and comprehension.
in proceedings of the 58th an-nual meeting of the association for computationallinguistics, pages 7871–7880, online.
associationfor computational linguistics..yinhan liu, myle ott, naman goyal, jingfei du, man-dar joshi, danqi chen, omer levy, mike lewis,luke zettlemoyer, and veselin stoyanov.
2019.roberta: a robustly optimized bert pretraining ap-proach.
corr, abs/1907.11692..tsvetomila mihaylova, georgi karadzhov, pepaatanasova, ramy baly, mitra mohtarami, andpreslav nakov.
2019. semeval-2019 task 8: factchecking in community question answering forums.
in proceedings of the 13th international workshopon semantic evaluation, pages 860–869..tsvetomila mihaylova, preslav nakov, llu´ıs m`arquez,alberto barr´on-cede˜no, mitra mohtarami, georgikaradzhov, and james r. glass.
2018. fact check-ing in community forums.
corr, abs/1803.03178..george a miller.
1995. wordnet: a lexical database forenglish.
communications of the acm, 38(11):39–41..myle ott, sergey edunov, alexei baevski, angelafan, sam gross, nathan ng, david grangier, andfairseq: a fast, extensiblemichael auli.
2019a.
in proceedings oftoolkit for sequence modeling.
the 2019 conference of the north american chap-ter of the association for computational linguistics(demonstrations), pages 48–53, minneapolis, min-nesota.
association for computational linguistics..myle ott, sergey edunov, alexei baevski, angelafan, sam gross, nathan ng, david grangier, andfairseq: a fast, extensiblemichael auli.
2019b.
in proceedings oftoolkit for sequence modeling.
the 2019 conference of the north american chap-ter of the association for computational linguistics(demonstrations), pages 48–53, minneapolis, min-nesota.
association for computational linguistics..adam poliak, max fleming, cash costello, kenton wmurray, mahsa yarmohammadi, shivani pandya,darius irani, milind agarwal, udit sharma, shuosun, nicola ivanov, lingxi shang, kaushik srini-vasan, seolhwa lee, xu han, smisha agarwal, andjo˜ao sedoc.
2020. collecting veriﬁed covid-19in proceedings of the 1stquestion answer pairs.
workshop on nlp for covid-19 (part 2) at emnlp2020, online.
association for computational lin-guistics..dean pomerleau and delip rao.
2017. fake news chal-.
lenge..nils reimers and iryna gurevych.
2019. sentence-bert: sentence embeddings using siamese bert-networks.
in proceedings of the 2019 conference onempirical methods in natural language processing.
and the 9th international joint conference on natu-ral language processing (emnlp-ijcnlp), pages3982–3992, hong kong, china.
association forcomputational linguistics..akhilesh sudhakar, bhargav upadhyay, and arjun ma-“transforming” delete, retrieve,heswaran.
2019.generate approach for controlled text style transfer.
in proceedings of the 2019 conference on empiricalmethods in natural language processing and the9th international joint conference on natural lan-guage processing (emnlp-ijcnlp), pages 3269–3279, hong kong, china.
association for computa-tional linguistics..james.
andreas vlachos,.
and arpit mittal..christosthorne,2018.christodoulopoulos,a large-scale dataset for fact extractionfever:the 2018in proceedings ofand veriﬁcation.
conference of the north american chapter of theassociation for computational linguistics: humanlanguage technologies, volume 1 (long papers),pages 809–819.
association for computationallinguistics..james thorne, andreas vlachos, oana cocarascu,christos christodoulopoulos, and arpit mittal.
2019.the fever2.0 shared task.
in proceedings of thesecond workshop on fact extraction and veriﬁca-tion (fever), pages 1–6, hong kong, china.
asso-ciation for computational linguistics..andreas vlachos and sebastian riedel.
2014. factchecking: task deﬁnition and dataset construction.
in proceedings of the acl 2014 workshop on lan-guage technologies and computational social sci-ence, pages 18–22.
association for computationallinguistics..soroush vosoughi, deb roy, and sinan aral.
2018.the spread of true and false news online.
science,359(6380):1146–1151..david wadden, shanchuan lin, kyle lo, lucy luwang, madeleine van zuylen, arman cohan, andhannaneh hajishirzi.
2020. fact or ﬁction: verify-in proceedings of the 2020ing scientiﬁc claims.
conference on empirical methods in natural lan-guage processing (emnlp), pages 7534–7550, on-line.
association for computational linguistics..alex wang, amanpreet singh, julian michael, fe-lix hill, omer levy, and samuel bowman.
2018.glue: a multi-task benchmark and analysis plat-in pro-form for natural language understanding.
ceedings ofthe 2018 emnlp workshop black-boxnlp: analyzing and interpreting neural net-works for nlp, pages 353–355, brussels, belgium.
association for computational linguistics..lucy lu wang, kyle lo, yoganand chandrasekhar,jiangjiang yang, doug burdick,russell reas,darrin eide, kathryn funk, yannis katsis, rod-ney michael kinney, yunyao li, ziyang liu,william merrill, paul mooney, dewey a. murdick,.
2126devvret rishi, jerry sheehan, zhihong shen, bran-don stilson, alex d. wade, kuansan wang, nancyxin ru wang, christopher wilhelm, boya xie, dou-glas m. raymond, daniel s. weld, oren etzioni,and sebastian kohlmeier.
2020. cord-19: thein proceedingscovid-19 open research dataset.
of the 1st workshop on nlp for covid-19 at acl2020, online.
association for computational lin-guistics..adina williams, nikita nangia, and samuel bowman.
2018. a broad-coverage challenge corpus for sen-tence understanding through inference.
in proceed-ings of the 2018 conference of the north americanchapter of the association for computational lin-guistics: human language technologies, volume1 (long papers), pages 1112–1122.
association forcomputational linguistics..edwin zhang, nikhil gupta, rodrigo nogueira,kyunghyun cho, and jimmy lin.
2020a.
rapidlydeploying a neural search engine for the covid-19in proceedings of the 1stopen research dataset.
workshop on nlp for covid-19 at acl 2020, on-line.
association for computational linguistics..yizhe zhang, guoyin wang, chunyuan li, zhegan, chris brockett,and bill dolan.
2020b.
pointer: constrained progressive text generationvia insertion-based generative pre-training.
in pro-ceedings of the 2020 conference on empirical meth-ods in natural language processing (emnlp),pages 8649–8670, online.
association for compu-tational linguistics..2127a model implementation details.
b dataset statistics.
figures below visualize most frequent ﬂairs in thedataset, as well as word clouds with keywords andreplaced words..we used fairseq library (ott et al., 2019a) forroberta model training..a.1 salient word selection hyperparameters.
we use the uncased bert model since many titlescontain words that are all capitalized.
we train themodel on the scifact classiﬁcation task using 15epochs and batch size of 16. the training loss is7.15e − 03. the rest of the parameters are set todefault as in (sudhakar et al., 2019)..a.2 veracity prediction hyperparameters.
• no of parameters: we use the roberta-large checkpoint (355m parameters) and usethe fairseq implementation (ott et al.,2019b) 4..• no of epochs: we ﬁne-tune pre-trainedroberta for 10 epochs for each model andsave the best model based on validation accu-racy on covidfact..• training time: our training time is 30 min-utes for each model except for ones withfever which takes around 10 hours..• hardware conﬁguration: we use 2 rtx.
2080 gpus..• training hyper parameters: we use thesame parameters as the fairseq githubrepository where roberta was ﬁne-tunedfor the rte task in glue with the exceptionof the size of each mini-batch, in terms of thenumber of tokens, for which we used 1024.
5.
4https://github.com/pytorch/fairseq/.
tree/master/examples/roberta.
5https://github.com/pytorch/fairseq/blob/master/examples/roberta/readme.glue.
md.
figure 3: most frequent ﬂairs in the dataset..figure 4: top image: word cloud of salient words.
bot-tom image: word cloud of replaced words..2128b.1 word replacement statistics.
b.2 alexa threshold.
figures below show most frequent salient words aswell as most frequent words that were used to re-place the salient words (replacement words).
postags obtained using the ﬂair python library tagger..a boxplot that helped us select the 50,000 alexasiterank threshold.
the plot shows site ranks for 2kinitially scraped claims.
outliers (points outside ofthe whiskers of the plot) are all above the 50,000threshold..figure 5: most frequent salient words..figure 8: alexa site rank boxplot..figure 6: most frequent replacement words..figure 7: most frequent pos tags of replacementwords..2129