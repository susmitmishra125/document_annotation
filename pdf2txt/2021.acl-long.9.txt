mention flags (mf): constraining transformer-based text generators.
yufei wang1, ian d. wood1,2, stephen wan2, mark dras1 and mark johnson3macquarie university, sydney, australia1csiro data61, sydney, australia2oracle digital assistant, oracle corporation3yufei.wang@students.mq.edu.au, ian.wood@mq.edu.austephen.wan@data61.csiro.au, mark.dras@mq.edu.aumark.mj.johnson@oracle.com.
abstract.
this paper focuses on seq2seq (s2s) con-strained text generation where the text gener-ator is constrained to mention speciﬁc words,which are inputs to the encoder, in the gen-erated outputs.
pre-trained s2s models suchas t5 or a copy mechanism can be trained tocopy the surface tokens from encoders to de-coders, but they cannot guarantee constraintsatisfaction.
constrained decoding algorithmsalways produce hypotheses satisfying all con-straints.
however, they are computationallyexpensive and can lower the generated textquality.
in this paper, we propose mentionflags (mf), which trace whether lexical con-straints are satisﬁed in the generated outputsof an s2s decoder.
the mf models are trainedto generate tokens until all constraints are satis-ﬁed, guaranteeing high constraint satisfaction.
our experiments on the common sense gen-eration task (commongen) (lin et al., 2020),end2end data-to-text task (e2enlg) (duˇseket al., 2020) and novel object captioning task(nocaps) (agrawal et al., 2019) show that themf models maintain higher constraint satisfac-tion and text quality than the baseline mod-els and other constrained text generation algo-rithms, achieving state-of-the-art performanceon all three tasks.
these results are achievedwith a much lower run-time than constraineddecoding algorithms.
we also show that themf models work well in the low-resource set-ting.
1.
1.introduction.
this paper focuses on seq2seq (s2s) constrainedtext generation where a set of encoder input to-kens are required to be present in the generatedoutputs.
for example, keyword-to-text (lin et al.,2020), data-to-text (gardent et al., 2017; duˇseket al., 2020) and image-to-text (lin et al., 2014;.
1the source code for this paper is released at https:.
//github.com/garyyufei/acl2021mf.
figure 1: an overview of the mention flag mechanismfor transformer-based s2s models.
here, the tokensﬂower and bee are required to appear in the generatedoutputs.
each generated token has a corresponding setof mention flags which informs the decoder whethereach lexical constraint has been satisﬁed in the cur-rent decoder input sequence.
for example, the men-tion flag for ﬂower is set (indicated by orange dots)from the third token because it is generated at the sec-ond step.
both token and mention flag embeddingsare the input to the decoder, but mention flags are in-jected into the decoder in a different way to the tokens(see fig.
3).
note that task speciﬁc encoder inputs havebeen omitted for brevity..agrawal et al., 2019) require the models to men-tion all or some of the input keywords, key-valuepairs and image object labels (respectively), po-tentially with linguistic variants, in the generatedoutputs.
large (pre-trained) transformer-baseds2s models such as t5 (raffel et al., 2019) can betrained (ﬁne-tuned) to perform this task.
however,they only learn to copy the surface tokens fromencoder inputs to the decoder outputs and there isno underlying mechanism guaranteeing good con-straint satisfaction (the ratio of satisﬁed lexical con-straints to given lexical constraints).
constrainedbeam search (cbs) (anderson et al., 2017) andrelated algorithms can guarantee outputs satisfy-ing all constraints, however they are much slowerthan the standard beam search algorithm.
in ad-dition, as they are all inference-based algorithms,their corresponding models are not aware of the.
proceedingsofthe59thannualmeetingoftheassociationforcomputationallinguisticsandthe11thinternationaljointconferenceonnaturallanguageprocessing,pages103–113august1–6,2021.©2021associationforcomputationallinguistics103constraint words or phrases, the resulting genera-tion could be poor.
ideally, a method for producingconstrained text should: a) generate high-qualitytext; b) achieve high constraint satisfaction; c) havean efﬁcient inference procedure..to this end, we propose mention flags (mf),which trace whether a lexical constraint has beenrealized in partial decoder outputs.
speciﬁcally,each decoder input token is provided with a set ofﬂags indicating which constraints have been sat-isﬁed up to that token.
as shown in fig 1, themention flags for ﬂower is set from the third step,because ﬂower is generated at the second step.
werepresent the three possible mention flags as sepa-rate trainable embeddings and inject them into thedecoder of the s2s transformer-based text gener-ator.
the dynamic mention flags explicitly informthe model about which constraints have been sat-isﬁed, which is helpful for the models to producehigh-quality text satisfying the constraints (goala).
during training, all the mention ﬂags are setwhen the model is tasked to generate the end-of-sequence (eos) token, strongly encouraging themodel not to stop generation until all constraintsare satisﬁed (goal b).
the mf models only requireordinary decoding algorithms.
their inference timeand memory requirements are similar to their base-line models (goal c)..we conduct experiments on three benchmarks:commonsense generative reasoning (common-gen) (lin et al., 2020), where the only input is aset of words representing concepts, and the outputtext is constrained to include all of them; end-to-end data-to-text (e2enlg) (duˇsek et al., 2020),where the constraints are meaning representationswith lexicalised attributes and values that the outputtext should mention; and novel object captioningat scale (nocaps) (agrawal et al., 2019), where con-straints are salient image objects that should bementioned in the generated caption.
compared tothe constrained decoding algorithms, the mf mod-els can produce higher-quality text with a simi-lar level of constraint satisfaction and much lessinference run-time and memory.
mention flagsare a general mechanism that improves constraintsatisfaction in the non-pre-trained and pre-traineds2s transformer-based models.
furthermore, ourexperiments show that the mf models can satisfynovel constraints (i.e, involving words or phrasesnot seen during training) and they work well inlow-resource settings.
our mf models set a new.
state-of-the-art in these three tasks..2 background.
in this paper, we focus on constraining transformer-based text generation models due to their popularityand success in various domains, especially in large-scale pre-trained language models (raffel et al.,2019; lewis et al., 2020).
previous work can beroughly categorized into two streams: s2s trainingapproaches and constrained decoding approaches:.
training s2s models s2s models can implicitlycapture the co-occurrence between encoder and de-coder sequences, particularly pre-trained ones suchas t5 (raffel et al., 2019) and bart (lewis et al.,2020).
wen et al.
(2015) uses a special gate to con-trol what information will be generated in the fol-lowing steps.
kale and rastogi (2020) have shownthat the t5 models achieve state-of-the-art resultsin various data-to-text tasks, requiring copyingfrom encoder to decoder, after ﬁne-tuning.
as analternative, the copy mechanism (gu et al., 2016)explicitly learns where to copy the input constraintsinto the output by adding an extra copy pathway tothe models.
however, these approaches cannot con-trol or guarantee their constraint satisfaction.
linet al.
(2020) also have observed lower constraintsatisfaction in the above methods, compared to theconstrained decoding approaches..constrained decoding these algorithms,in-cluding constrained beam search (cbs) (an-derson et al., 2017) and grid beam search(gbs) (hokamp and liu, 2017), maintain a set ofstates which have their own size-k beams and onlyallow hypotheses satisfying speciﬁc constraints tobe considered during inference.
each cbs statecorresponds to the hypotheses satisfying differ-ent constraints (exponential in the number of con-straints) and the gbs states correspond to the hy-potheses satisfying the same number of constraints(linear to constraint number).
balakrishnan et al.
(2019); juraska et al.
(2018); duˇsek and jurˇc´ıˇcek(2016) also modify their inference algorithm in asimilar way to fulﬁll speciﬁc output requirements.
however, they signiﬁcantly increase the inferencerun-time and memory and can produce sub-optimaloutputs..3 method.
this section ﬁrst formulates constrained text gener-ation tasks, then introduces mention flags and their.
104integration with transformer-based text generators..3.1 s2s constrained text generation.
in the s2s constrained text generation tasks, weare given encoder inputs x = [x1, .
.
.
, xlx] ∈ xthat describe the task, where some xi correspondto lexical constraints that must be satisﬁed in thegenerated outputs.
at generation step t, the decodertakes as input the tokens generated so far y:t =[y1, · · · , yt] ∈ y and generates the next outputtoken yt+1..3.2 mention flag.
at generation step t, a set of mention flags in-dicates whether each lexical constraint has beensatisﬁed up to this step (i.e., in the decoder inputsequence y:t).
formally, they can be deﬁned asm : x × y → {0, 1, 2}lx where |m(x, y:t)| = |x|.
speciﬁcally, mention flag m(x, y:t)i is for the in-put token xi in x:.
m(x, y:t)i =.
.
.
012.xi is not a constraintxi is not mentioned in y:txi is mentioned in y:t.(1).
the values 1 and 2 represent the status of constraintsatisfaction.
once y:t satisﬁes the constraints, thevalue of the corresponding mention flag(s) areupdated from 1 to 2. value 0 is a static defaultvalue for all tokens xi that do not correspond toany constraints.
they are not required to be men-tioned in the outputs.
these typically act as in-structions to the model.
at the start, mentionflags m(x, ε) ∈ {0, 1}lx where ε is the emptystring because the empty string does not mentionanything.
during generation, m is monotonic iny∗: given decoder input sequence y:t and y:(t+1),m(x, y:t)i ≤ m(x, y:(t+1))i. the mention flagsfor any token xi can only remain unchanged orupdate from value 1 to 2..examplein figure 2, given encoder input to-kens x = [name, tetas, area, south, bank], westart from m(x, ε) = [0, 1, 0, 1, 1] because nameand area are not lexical constraints.
at step 4,m(x, [tetas, is, located]) = [0, 2, 0, 1, 1] becausetetas has already been mentioned in the currentdecoder input sequence [tetas, is, located]..value update for multi-word constraints asshown in figure 2, mention flags for the tokenscorresponding to the same constraint are updatedtogether.
given encoder input tokens xi, · · · , xj,forming a multi-word constraint, we require that.
y:t <s> tetas is located in the south bank ..x.
(cid:55) name(cid:51) tetas(cid:55) area(cid:51) south(cid:51) bank.
01011.
02011.
02011.
02011.
0 02 20 01 11 1.
02011.
02022.
02022.figure 2: an example of mention flag matrix.
(cid:51)for constrained encoder input tokens and (cid:55) for non-constrained ones.
both name and area start with value0 because they are not parts of lexical constraints.
thelexical constraints tetas and south bank start fromvalue 1. the mention flags are updated to value 2when y:t satisﬁes the constraints.
the mention flagsfor multi-word constraints are updated simultaneously..m(x, y∗)i = · · · = m(x, y∗)j for all (partial) out-puts y∗, and m(x, y:t)i = · · · = m(x, y:t)j = 2iff xi, · · · , xj are mentioned in y:t. we use con-ventions from the relevant data set to determinewhether a constraint is a multi-word constraint.
this avoids false update when the models onlygenerate the preﬁx of the constraints, rather thanthe full constraints.
for example, given constraint“washing machine”, the output could be “i put mywashing in the new washing machine.” the situa-tion becomes more complicated when both wash-ing and washing machine are given lexical con-straints.
when we ﬁnd this case, we delay thevalue 2 update for washing until the word in isgenerated.
modern tokenization methods, such asbpe (sennrich et al., 2016), make this situationfrequent..deﬁnition of mentions we deliberately allow aﬂexible notion of mentions in the function m().
we can deﬁne various types of mentions to fulﬁllthe requirements of different applications and tasks.
with this ﬂexibility, the end-users can use men-tion flags in many constraint scenarios.
for taskswith strict constraints, we deﬁne mentions to be theexact string match in y:t. otherwise, inﬂectionalvariants or synonyms of words in the lexical con-straints are allowed when checking for mentions.
our mention flag mechanism thus supports lex-ical constraints with multiple verbalizations.
weleave more sophisticated constraints (e.g., usingnlp parsers) to future work..mention flag matrix given x, y:t, we deﬁnethe two-dimensional mention flag matrix f ∈.
105{0, 1, 2}lx×t as follows:.
f = [m(x, ε); m(x, y:1); · · · ; m(x, y:t)].
(2).
during training, given x and ground-truth outputy gt (with lgt tokens), we can construct the ground-truth mention flag matrix f gt ∈ {0, 1, 2}lx×lgt byﬁnding the mentioning position of tokens in the lex-ical constraints in y gt.
f gt follows the same mask-ing strategy as the decoder input tokens y:t. forthe tokens whose corresponding lexical constraintshaving no alignment with y gt, their mention flagsare also assigned value 0. during inference, webuild the mention flag matrix incrementally, start-ing from f inf ,0 = [m(x, ε)] ∈ {0, 1}lx×1.
in stept, we add a new column m(x, y:t) to f inf ,t−1 ∈{0, 1, 2}lx×(t−1) and obtain the new mention flagmatrix f inf ,t ∈ {0, 1, 2}lx×t..why mention flags work during the trainingof mf models, the ground-truth always has all mfsset to “completed” before stopping the generation(i.e., before generating eos token).
this providesa strong signal to satisfy all constraints before com-pleting generation.
the value update from 1 to 2in mf provides implicit signals about where theconstraints are satisﬁed during training.
otherwise,the model has to learn this information via the co-occurring sub-sequences between input sequenceand output sequence.
these two signals allow themodel to achieve high constraint satisfaction andhelp to maintain high text quality (sec.
4.5).
sincethere are only 3 added embeddings, learning doesnot require a substantial amount of training data(sec.
4.7).
since these embeddings are indepen-dent of particular lexical constraints, we expectthat performance on novel constraints, not seenduring training, is improved (sec.
4.5)..3.3.integration with s2s transformer.
as shown in figure 3, mention flags are injectedinto the transformer decoder.
we ﬁrst review thestandard s2s transformer proposed in vaswaniet al.
(2017), then discuss how to inject mentionflags information into the s2s transformer model..standard s2s transformer model the en-coder input tokens x is fed into the transformerencoder he = enc(x) where he ∈ rlx×d and d isthe model hidden size.
in the transformer decoder,there are two self-attention modules, self multi-head attention (sa) which handles the currentdecoder input sequence y:t, and cross multi-head.
figure 3: in each decoder layer, the cross-attention(ca) module (light blue) integrates mention flags asadditional inputs describing relationship between en-coder contents and decoder input tokens.
there areseparated representations for mention flags in differ-ent decoder layers..attention (ca) which handles the interaction be-tween encoder output he and y:t:.
sa(y:t) = kv (w st , he) = kv (w c.q y:t, w st , w cq hd.
k y:t, w sk he, w c.v y:t) (3)v he)(4).
ca(hd.
where hdt = sa(y:t).
kv is the standard key-value self-attention proposed in vaswani et al.
t , he) further deter-(2017).
the outputs of ca(hdmine the model output yt+1 via a feed forwardlayer, a residual connection and a softmax layer..incorporating mention flag matrix ourtwo-dimensional mentionmatrixflagf ∈ {0, 1, 2}lx×tis associated with the ele-ments from encoder output he and current decoderinput y:t. the optimal way is to incorporate thefull f matrix into a component in the transformerdecoder.
we note that the ca module in thetransformer decoder already uses y:t as queryand he as key.
the resulting query-key similaritymatrix has the same size of our mention flagmatrix, making it suitable to incorporate f ..mention flag matrix as relative position in-spired by shaw et al.
(2018) which incorporatestoken relative positions into the sa module, wepropose to inject mention flags as the “relativepositions” between encoder output he and currentdecoder input y:t in the ca module.
in each de-coder layer, we represent f as two sets of train-able embeddings mention flag key mk = ek(f )and mention flag value mv = ev(f ) where.
106ek, ev ∈ r3×d are the mention flag embeddingtables.
mk and mv ∈ rlx×t×d.
we have separatedmention flags representations for each decoderlayer.
eq.
4 is changed to:.
ca(hd.
t , he, mk, mv) =q hdr(w c.t , w c.k he, w c.v he, mk, mv).
(5).
layer of the t5 decoder.
this parameters freezingtechnology is applied to both t5 baseline modelsand the mf models in all of our experiments.
wereport constraint satisfaction for all tasks.
we usegbs in the commongen task (max 5 constraints)and cbs in the e2enlg (max 1 constraint) andnocaps (max 2 constraints) task..where r is the self-attention function with relativeposition, deﬁned as follows:.
4.1 commongen.
r(q, k, v, mk, mv)j =.
ai,j(vi + mv.
i,j) (6).
lx(cid:88).
i=1.
a∗,j = softmax (e∗,j)i,j)t.qj(ki + mk√.
ei,j =.
d.(7).
(8).
as an alternative to representing f as mk and mv,we could follow the approach to relative positionin the t5 model (raffel et al., 2019) and representf as scalars that are added to the correspondinglogits ei,j in eq.
7 used for computing the attentionweights.
however, we ﬁnd this scalar approach lesseffective than our proposed one in sec.
4.6..4 experiments.
we conduct experiments on three benchmarks withdifferent forms of constraints including common-sense generative reasoning (commongen) (linet al., 2020) with keyword constraints, end-to-endrestaurants dialog (e2enlg) (duˇsek et al., 2020)with key-value constraints, and novel object cap-tioning at scale (nocaps) (agrawal et al., 2019) withvisual object word constraints.
we integrate men-tion flags with a three-layer standard s2s trans-former models (trans, l3) (vaswani et al., 2017)and pre-trained t5 models (raffel et al., 2019) foreach task.
the t5 models achieve state-of-the-artresults in various data-to-text tasks (kale and ras-togi, 2020).
for the t5-base and t5-large models,we use the implementation of t5 models in thehuggingface transformers 2. the trans, l3 mod-els share the same implementation of the t5-basemodels, except that it is not initialized with the pre-trained parameters and it only uses 3 layers, ratherthan 12 layers, for both encoder and decoder.
inaddition, to improve the generalization of our pre-trained model, we freeze the parameters in the self-attention module and feed-forward layers in each.
2https://github.com/huggingface/.
transformers.
in this task, the encoder input is a sequence ofconcepts c = [c1, · · · , ck], k ≤ 5. the modelsshould generate a coherent sentence describing allconcepts in c. m(c, ε) = [1, 1, · · · , 1] and mallows inﬂectional variants to satisfy lexical con-straints.
we train (ﬁne-tune) trans, l3, t5-baseand t5-large model as our baselines.
we applymention flags to the t5-base and t5-large model(+ mf).
following the suggestions in lin et al.
(2020), we report cider (vedantam et al., 2015)and spice (anderson et al., 2016) as generatedtext quality metrics.
we calculate constraint satis-faction for all constraints (all), novel constraints(novel) and seen constraints (seen)..method.
cider spice.
constraintseen novel all.
79.5113.9.
74.5108.0.w/o pre-trainingtrans, l3trans, l3 + mfleventrans.♣constleven.♣w/ pre-training164.4t5-base110.7t5-base + gt5-base + mf170.1t5-base + mf + g 115.0167.3t5-larget5-large + mf174.8.liu et al.
(2021).
168.3.
20.124.6.
16.820.1.
32.127.832.727.633.033.4.
32.7.
62.693.8.
2.349.2.
--.
95.710099.610093.999.2.
-.
--.
94.610099.210093.899.0.
-.
58.090.4.
63.894.5.
95.610099.610093.999.1.
98.6.table 1: experiment results on commongen test split.
the t5-base + mf model achieves high text qualitywith high constraint satisfaction.
g for gbs.
♣ resultstaken from lin et al.
(2020).
bold is the highest scoreand underline is the second highest score..results table 1 shows that the mf model im-proves the constraint satisfaction over the baselinesfor all cases, achieving close to 100% (i.e., 99.6%and 99.1%).
notably, mention flags improve novelconstraint satisfaction from 2.3% to 49.2% in therandomly initialized transformer models.
com-pared to the leventrans (gu et al., 2019) and con-.
107stleven (susanto et al., 2020) models, our trans,l3 + mf model achieves higher cider and spicescores with constraint satisfaction 4.1% lower thanthe non-autoregressive constleven model.
whilegbs provides a way to maximise constraint satis-faction (i.e., 100%), doing so signiﬁcantly degradesthe output text quality (more than 50 cider).
ourmf model achieves near optimum constraint sat-isfaction while improving text quality (5.7 ciderscore improvement in t5-base and 6.5 cider scoreimprovement in t5-large).
finally, our t5-large +mf model outperforms the previous state-of-the-artresult (liu et al., 2021), which integrates the con-ceptnet (speer et al., 2017) into the bart model,by 6.5 cider and 0.7 spice, suggesting that pre-trained language models with textual concepts mayprovide sufﬁcient information for this task..4.2 e2enlg.
the encoder input is a sequencein this task,of key-value meaning representations c =[k1, v1, · · · , kn, vn], n ≤ 8. we lists all givenkey-value information as a space-separated string.
m(c, ε) = [0, 1, 0, 1, · · · , 0, 1] and m allows syn-onyms to satisfy lexical constraints.
for example,welcome children and is family friendly are bothmentions of familyfriendly[yes].
the models mustgenerate a ﬂuent and coherent dialog response us-ing all key-value pairs in the encoder.
e2enlg in-cludes 79 different in-domain key-value constraints.
we use the scripts from duˇsek et al.
(2019) 3 toconstruct the synonyms set for these inputs.
weuse trans, l3 and t5-base model as our baselines.
we use cbs to constrain the t5 model to satisfyall missing constraints (t5-base + c).
we reportnist (lin and hovy, 2003), bleu (papineni et al.,2002) and meteor (banerjee and lavie, 2005) asthey are common metrics for evaluating the qualityof long text in the e2enlg outputs (more than 20tokens)..results table 2 shows that the mf models con-sistently achieve higher output text quality and con-straint satisfaction than the baseline models (99.9%vs. 95.1% and 100% vs. 96.6%).
cbs improvesthe t5 model’s constraint satisfaction, but nega-tively affects the text quality (0.3 blue pointslower).
shen et al.
(2019), the previous state-of-the-art, trained the model via a complex speaker-listener approach inspired by cognitive science..3https://github.com/tuetschek/.
e2e-cleaning/blob/master/slot_error.py.
with a much simpler model architecture (s2s), ourt5 + mf model achieves full constraint satisfactionand outperforms shen et al.
(2019) by 0.2 nistand 0.3 meteor..method.
bleu.
nist meteor constraint.
w/o pre-trainingtrans, l3trans, l3 + mfw/ pre-trainingt5t5 + cbst5 + mf.
shen et al.
(2019).
64.765.4.
67.467.168.3.
68.6.
8.58.6.
8.78.78.9.
8.7.
43.844.9.
45.545.645.6.
45.3.
95.199.9.
96.6100.0100.0.
-.
table 2: experiment results in the e2enlg test split.
the t5 + mf model achieves high text quality withhigh constraint satisfaction..4.3 nocaps.
using t5 for image captioningin image cap-tioning, each input image is represented by a se-quence of visual objects.
each of these objectsis assigned (by the object detector) with a tex-tual label.
the encoder input is a sequence ofobjects followed by the same textual labels c =[v1i isthe visual feature vector (similar to the one in liet al.
(2020)) and li is the corresponding textuallabel.
the visual features are used in the same wayof normal textual tokens in the t5 models.
weﬁnd this approach works well for both nocaps andstandard coco image captioning task..k , lk] where v∗.
1 , l1, · · · , v1.
k, · · · , vsk.
1, · · · , vs1.
experiment setup traditional image captioningmodels select and describe a subset of input objectsjointly (anderson et al., 2018).
however, pudup-pully et al.
(2019) shows the beneﬁts of separatingcontent selection and text planning steps for gen-eral data-to-text tasks.
following this, we proposeto ﬁrst select salient objects and incorporate theselected objects into the description using mentionflags.
m(c, ε) = [0, 0, · · · , 1, · · · , 0, 0, · · · , 1]where only salient object labels receive value 1.m() allows inﬂectional variants to satisfy lexicalconstraints.
we use t5-base model in this exper-iment.
the t5 + c and t5 + mf + c modelsare constrained with cbs.
following wang et al.
(2021), we report cider and spice as output textquality metrics and constraint satisfaction for novelconstraints (novel) and all constraints (all).
wepresent the performance for all evaluation images.
108(overall) and for the challenging images withonly novel objects (out-of-domain split)..salient object selector we use a transformer-based salient object detector to select a subset ofobject labels as lexical constraints.
the visual rep-resentations of detected image objects are ﬁrst fedinto the 3-layer standard transformer model with-out any positional embedding.
we train this detec-tor using binary cross-entropy loss averaged overall detected input objects.
the training data forsalient object detection is the training data in no-caps.
we use coco 2017 dev set as the evaluationdataset to select the best checkpoint..method.
out-of-dom.
overallcider.
s cider.
constraints novel all.
9.2.
34.8.
34.239.8.nocaps val.
(w/o pre-training)8.6trans, l3trans, l3 + mf9.1ecol w/o lm(cid:51)nocaps val.
(w/ pre-training)t5t5 + ct5 + mft5 + mf + gt5 + mf + coscarl + c♥vivo + c§.
63.480.279.979.679.777.483.0.
9.910.510.810.610.710.510.7.nocaps testt5 + mf71.5updown (e&c)♠ 66.7ecol + ib(cid:51)67.0.
10.49.710.3.
58.760.4.
10.611.2.
16.349.3.
35.871.5.
58.0.
11.2.
-.
-.
72.779.279.979.279.578.685.3.
77.773.176.0.
11.311.611.911.811.811.812.2.
12.111.211.9.
35.810096.9100100--.
96.3--.
47.510098.3100100--.
97.8--.
table 3: evaluation results for nocaps.
the t5 +mf model produces high-quality text with high con-straint satisfaction, setting a new state-of-the-art amongthe comparable previous works.
c: cbs.
g: gbs.
s: spice.
con.
: constraint satisfaction.
§ hu et al.
(2020), a non-comparable model that uses additionalvisual-text aligned training data.
♠ agrawal et al.
(2019).
♥ li et al.
(2020).
(cid:51) wang et al.
(2021)..results mention flags achieve optimal con-straint satisfaction in almost all cases.
in partic-ular the trans, l3 + mf model shows marked im-provement (i.e., from 16.3% to 49.3%) on novelconstraints, despite the fact that the correspond-ing token embeddings are not changed from theirrandom initialisation.
the generated text qualityis also improved, particularly in the out-of-domainsplit.
the t5 + c model is 0.3 spice lower in bothoverall and the out-of-domain split than the t5 + mf.
model, indicating that the mf model correctly cap-tures more long-range relationships (calculated bythe parsing trees used in spice) among the (novel)objects than cbs.
our t5 + mf model outperformsthe existing state-of-the-art end-to-end single-stageimage captioning systems (agrawal et al., 2019; liet al., 2020; wang et al., 2021) by 1.3 cider and0.1 spice on the validation set and 1.7 cider and0.2 spice on the test set, showing the advantageof our two-stage captioning model empowered bymention flags.
vivo + c (hu et al., 2020) is notcomparable as it uses additional visual-text alignedtraining data.
finally, we investigate the relativelylower constraint satisfaction in nocaps (98.3% vs.99.5+%) compared to the mf models in the othertwo tasks and ﬁnd that missing cases frequentlyhappen in the instances with two constraints involv-ing a) (near-) synonymy (e.g., mule and horse) andb) hyponymy (e.g., hot dog and fast food).
a moreadvanced salient object detector would solve thisissue..4.4 model efﬁciencythe mf models use standard beam search and runmuch faster with less memory than the constrainedbeam search algorithms.
for comparison, we se-lect the gbs algorithm because its resource use islinear in the number of constraints and uses lessrun time and memory than cbs.
we run the mfmodels and the models with gbs using beam size5 and compare their run time (rt) and memoryrequirement (#m) in table 4. compared to the mfmodels, gbs runs one to two orders of magnitudeslower, and uses 4.4 to 23.4 times more memory.
compared to the t5-base model, the mf modelsonly increases the inference time slightly..task.
e2enlgrt.
commongen.
nocaps.
#m rt.
#m rt.
#m.t5-base + g 438 m 16.9 645 m 23.4t5-base + mf 19 m17 m.10 m8 m.t5-base.
11.
11.
93 m 4.4118 m116 m.table 4: efﬁciency of the mf and gbs model.
rt:inference run time (in minutes).
#m: the number ofgbs states (indicating the memory required)..4.5 main result discussion.
constraint satisfaction & text qualityin alltasks, mf models improve the text quality over theirbaselines (including cbs and gbs) while achiev-ing constraint satisfaction that is close to 100%..109this supports the claim in sec 3.2 that trainingsignals from mention flags can help to improveconstraint satisfaction and text quality..non-pre-trained vs. pre-trained modelsin alltasks, mention flags have a similar effect (highertext quality and constraint satisfaction) on both non-pre-trained and pre-trained models.
this indicatesthat mention flags do not rely on information frompre-trained models to be effective..in the commongen and no-novel constraintscaps tasks, the trans, l3 + mf model achieve muchhigher coverage (i.e., 2.3% to 49.2% in common-gen; 16.3% to 49.3% in nocaps) for constraintswith novel lexical items than the baseline models.
here, the mf models can satisfy novel constraints,even where the corresponding token representa-tions did not receive any training signals.
as men-tion flags decouples with model representations,the mf models learn lexicon-independent indica-tors to mention the novel words..4.6 design choices for mention flags.
we conduct experiments for following choices ofmention flag: static mf where value 2 (is men-tioned) and 1 (not mentioned) are merged; mergedmf where value 0 (not a constraint) is mergedwith value 1; scalar mf where mention flags arerepresented as scalars added to the attention log-its in the ca module; and shared mf where alldecoder layers use the same mention flag embed-dings.
we apply static mf, scalar mf and sharedmf to all three tasks.
we only use merged mfin e2enlg because a commongen model doesnot include value 0 and a nocaps model withoutvalue 0 cannot distinguish between constrained andnon-constrained objects.
as shown in table 5, inthe commongen and nocaps tasks, the static mfmodels achieve much lower constraint satisfaction,99.6% vs. 94.5% and 98.3% vs. 87.2% respec-tively.
the explicit update from value 1 to 2 is im-portant for high constraint satisfaction.
the mergedmf model produces lower constraint satisfaction(100% to 98.9%) and generated text quality (68.3bleu to 67.7 bleu) in e2enlg, indicating theutility of value 0 in this task.
compared to the mfmodels, scalar mf models produce lower constraintsatisfaction in the commongen and nocaps task(99.6% to 97.1%, 98.3% to 91.5%, respectively)and lower-quality generated text in all three tasks(1.2 bleu, 3.2 cider and 0.6 cider lower).
rep-resenting mention flags as key and value dense.
e2enlgscalar mfstatic mfmerged mfshared mfmf.
bleu67.167.767.767.268.3.commongen cider166.9160.5168.1170.1.scalar mfstatic mfshared mfmf.
nocaps meteor cider79.325.380.425.378.725.425.679.9.scalar mfstatic mfshared mfmf.
nist meteor8.88.88.88.88.9.
45.345.845.345.545.6.spice32.732.032.832.7.c-novel97.593.599.099.4.spice11.811.711.811.9.con.
10010098.999.9100.0.c-all97.194.599.499.6.con.
91.587.295.898.3.table 5: ablation study for mf status.
static mf re-moves value 2 and merged mf merges value 0 and1. full mf achieves the highest constraint satisfac-tion and output text quality among all other variants.
con., c-novel, c-all: constraint satisfaction (resp.
for novel/all constraints)..vectors works better than scalars.
finally, usingshared mf across all decoder layers has negativeimpact (e.g., all constraint satisfaction ratio drop)in all three tasks..4.7 low-resource learning.
this section shows that mention flags are still use-ful for improving the constraint satisfaction andgenerated text quality when trained with manyfewer instances.
we use 0.1%, 1% and 10% ofthe original training instances to train the models.
in the ﬁrst two tasks (e2enlg and commongen),we compare the mf models with t5-base models.
in the nocaps task, we additionally compare the t5-base + mf model with the t5-base + c model.
wereport bleu in e2enlg cider in commongenand nocaps.
as shown in table 6, the mf modelsconsistently generate higher-quality text (highermeteor or cider score) and achieve higher con-straint satisfaction than the baseline models.
themf models reach 97+% when only training with10% of the e2enlg and commongen training data.
this conﬁrms our claim in sec.
3.2 that the threeadded mention flag embeddings can be learnedwith relatively little training data..4.8 qualitative analysis.
we chose three representative examples that illus-trate successful use of mention flags (table 7)..110training sample.
0.1 %.
1 %.
10 %.
e2enlgt5-base.
bleu con.
bleu con.
bleu con.
95.951.3t5-base + mf 52.499.9.
94.799.8.
60.561.1.
83.587.4.
67.167.3.commongen cider con.
cider con.
cider con.
81.5 140.6 91.185.4 149.4 97.6.
77.9t5-base + mf 78.5.
95.498.7.
87.289.5.t5-base.
nocapscider con.
cider con.
cider con.
48.243.5t5-base92.7t5-base + c50.7t5-base + mf 51.792.7.
44.082.882.8.
49.458.760.2.
46.272.472.4.
60.869.371.9.table 6: low-resource learning.
we use 0.1%, 1% and10% of the training instances to train the models.
con.
:constraint satisfaction..i) e2enlg.
name[punter], eattype[restaurant], area[riverside],pricerange[£20-25], familyfriendly[yes].
t5-b punter is a restaurant in the £20-25 price range.
it.
+ c.+ mf.
is in the riverside areapunter is a kid friendly restaurant in the riversidearea.
it has a price range of £20-25.
punter is a kid friendly restaurant in riverside witha price range of £20-25.
ii) commongenmother, washer, clothes, toddler, help.
t5-b a mother helps a toddler to wash his clothes+ g mother helping her toddler clothe in washer+ mf.
a mother helps a toddler to wash clothes in thewasher.
gt.
the mother helps her toddler put the clothes in thewasher.
iii) nocaps.
model is the only model that accurately describesthe relationship between bee and ﬂower, groundingto the input images and constraints..human evaluation we have shown that our pro-posed mf model can achieve higher constraint sat-isfaction ratio and automatic metrics.
however, theautomatic metrics do not necessarily reﬂect humanpreference of the generated text.
we therefore se-lect 100 output samples from the t5 baseline andour mf model in all three tasks (300 in total).
foreach sample pair, we ask three annotators to judgewhich sample is “more human-like”.
table 8 showsthat more than 70% of output of our mf model isgenerally better or similar than the output of thebaseline model, verifying the output quality of ourmf model..taskcommongene2enlgnocaps.
baseline27.3%30%28%.
equal22.0%25%26.7%.
mf.
50.7 %45%45.3%.
table 8: human evaluation over output samples in thecommongen, e2enlg and nocaps task..5 conclusion and future work.
in this paper, we propose mention flags to con-strain transformer-based text generators via inject-ing mention status embeddings into text decoders.
our extensive experiments on three different taskshave shown the effectiveness of mention flags inmaintaining high generated text quality and excel-lent constraint satisfaction, comparing favourablyto competitive constrained decoding algorithms.
we plan to expand mention flags i) to controllarger input source text such as constrained textsummarization and machine translation; ii) to han-dle larger granularity such as sentence-level..salient obj: bee, ﬂower; non-salient obj: plant, leaf.
t5-b a close up of a ﬂower on a tree+ c+ mf.
a close up of a bee ﬂower on a treea small white ﬂower with a bee in it.
gt.
a white ﬂower has a bee on it with green around..acknowledgments.
table 7: representative examples illustrate successfuluse of the mf models.
gt: ground truth text.
+c/+g:with constrained/grid beam search.
t5-b: t5 base..i) the mf model generates the most concise dia-logue response, compared to the baseline and con-strained decoding model; ii) the mf model is theonly model that generates a ﬂuent and coherent sen-tence satisfying all input constraints; iii) the mf.
we thank anonymous reviewers for their insight-ful suggestions to improve this paper.
this re-search was supported by a google award throughthe natural language understanding focused pro-gram, by a mq research excellence scholar-ship and a csiro’s data61 top-up scholarship,and under the australian research councils dis-covery projects funding scheme (project numberdp160102156)..111references.
harsh agrawal, karan desai, yufei wang, xinlei chen,rishabh jain, mark johnson, dhruv batra, deviparikh, stefan lee, and peter anderson.
2019. no-caps: novel object captioning at scale.
in proceed-ings of the ieee international conference on com-puter vision, pages 8948–8957..peter anderson, basura fernando, mark johnson, andstephen gould.
2016. spice: semantic proposi-tional image caption evaluation.
in european confer-ence on computer vision, pages 382–398.
springer..peter anderson, basura fernando, mark johnson, andstephen gould.
2017. guided open vocabulary im-age captioning with constrained beam search.
inproceedings of the 2017 conference on empiricalmethods in natural language processing, pages936–945, copenhagen, denmark.
association forcomputational linguistics..peter anderson, xiaodong he, chris buehler, damienteney, mark johnson, stephen gould, and leizhang.
2018. bottom-up and top-down attention forimage captioning and visual question answering.
inproceedings of the ieee conference on computervision and pattern recognition (cvpr)..anusha balakrishnan, jinfeng rao, kartikeya upasani,con-michael white, and rajen subba.
2019.strained decoding for neural nlg from composi-tional representations in task-oriented dialogue.
inproceedings of the 57th annual meeting of the as-sociation for computational linguistics, pages 831–844, florence, italy.
association for computationallinguistics..satanjeev banerjee and alon lavie.
2005. meteor:an automatic metric for mt evaluation with im-proved correlation with human judgments.
in pro-ceedings of the acl workshop on intrinsic and ex-trinsic evaluation measures for machine transla-tion and/or summarization, pages 65–72, ann ar-bor, michigan.
association for computational lin-guistics..ondˇrej duˇsek, david m. howcroft, and verena rieser.
2019. semantic noise matters for neural natural lan-guage generation.
in proceedings of the 12th inter-national conference on natural language genera-tion, pages 421–426, tokyo, japan.
association forcomputational linguistics..ondˇrej duˇsek and filip jurˇc´ıˇcek.
2016. sequence-to-sequence generation for spoken dialogue via deepin proceedings of thesyntax trees and strings.
54th annual meeting of the association for compu-tational linguistics (volume 2: short papers), pages45–51, berlin, germany.
association for computa-tional linguistics..claire gardent, anastasia shimorina, shashi narayan,and laura perez-beltrachini.
2017. the webnlgchallenge: generating text from rdf data.
in pro-ceedings of the 10th international conference onnatural language generation, pages 124–133, san-tiago de compostela, spain.
association for compu-tational linguistics..jiatao gu, zhengdong lu, hang li, and victor o.k.
incorporating copying mechanism inli.
2016.in proceedings ofsequence-to-sequence learning.
the 54th annual meeting of the association for com-putational linguistics (volume 1: long papers),pages 1631–1640, berlin, germany.
association forcomputational linguistics..jiatao gu, changhan wang, and junbo zhao.
2019.levenshtein transformer.
in advances in neural in-formation processing systems, volume 32. curranassociates, inc..chris hokamp and qun liu.
2017. lexically con-strained decoding for sequence generation using gridin proceedings of the 55th annualbeam search.
meeting of the association for computational lin-guistics (volume 1: long papers), pages 1535–1546,vancouver, canada.
association for computationallinguistics..xiaowei hu, xi yin, kevin lin, lijuan wang, leizhang, jianfeng gao, and zicheng liu.
2020. vivo:surpassing human performance in novel object cap-tioning with visual vocabulary pre-training.
arxivpreprint arxiv:2009.13682..juraj juraska, panagiotis karagiannis, kevin bowden,and marilyn walker.
2018. a deep ensemble modelwith slot alignment for sequence-to-sequence natu-ral language generation.
in proceedings of the 2018conference of the north american chapter of theassociation for computational linguistics: humanlanguage technologies, volume 1 (long papers),pages 152–162, new orleans, louisiana.
associa-tion for computational linguistics..mihir kale and abhinav rastogi.
2020. text-to-textpre-training for data-to-text tasks.
in proceedings ofthe 13th international conference on natural lan-guage generation, pages 97–102, dublin, ireland.
association for computational linguistics..mike lewis, yinhan liu, naman goyal, mar-jan ghazvininejad, abdelrahman mohamed, omerlevy, veselin stoyanov, and luke zettlemoyer.
2020. bart: denoising sequence-to-sequence pre-training for natural language generation, translation,and comprehension.
in proceedings of the 58th an-nual meeting of the association for computationallinguistics, pages 7871–7880, online.
associationfor computational linguistics..ondˇrej duˇsek, jekaterina novikova, and verena rieser.
2020. evaluating the state-of-the-art of end-to-endnatural language generation: the e2e nlg chal-lenge.
computer speech & language, 59:123–156..xiujun li, xi yin, chunyuan li, pengchuan zhang, xi-aowei hu, lei zhang, lijuan wang, houdong hu,li dong, furu wei, yejin choi, and jianfeng gao.
2020. oscar: object-semantics aligned pre-training.
112sheng shen, daniel fried, jacob andreas, and danklein.
2019. pragmatically informative text gen-in proceedings of the 2019 conferenceeration.
of the north american chapter of the associationfor computational linguistics: human languagetechnologies, volume 1 (long and short papers),pages 4060–4067, minneapolis, minnesota.
associ-ation for computational linguistics..robyn speer, joshua chin, and catherine havasi.
2017.conceptnet 5.5: an open multilingual graph ofin proceedings of the thirty-general knowledge.
first aaai conference on artiﬁcial intelligence,aaai’17, page 4444–4451.
aaai press..raymond hendy susanto, shamil chollampatt, andliling tan.
2020. lexically constrained neural ma-chine translation with levenshtein transformer.
inproceedings of the 58th annual meeting of the asso-ciation for computational linguistics, pages 3536–3543, online.
association for computational lin-guistics..ashish vaswani, noam shazeer, niki parmar, jakobuszkoreit, llion jones, aidan n gomez, ł ukaszkaiser, and illia polosukhin.
2017. attention is allyou need.
in advances in neural information pro-cessing systems, volume 30, pages 5998–6008.
cur-ran associates, inc..ramakrishna vedantam, c lawrence zitnick, and deviparikh.
2015. cider: consensus-based image de-in proceedings of the ieeescription evaluation.
conference on computer vision and pattern recogni-tion, pages 4566–4575..yufei wang, ian d. wood, stephen wan, and markjohnson.
2021. ecol-r: encouraging copying innovel object captioning with reinforcement learn-ing.
arxiv preprint arxiv:2101.09865..tsung-hsien wen, milica gaˇsi´c, nikola mrkˇsi´c, pei-hao su, david vandyke, and steve young.
2015.semantically conditioned lstm-based natural lan-guage generation for spoken dialogue systems.
inproceedings of the 2015 conference on empiricalmethods in natural language processing, pages1711–1721, lisbon, portugal.
association for com-putational linguistics..in computer vision –for vision-language tasks.
eccv 2020, pages 121–137, cham.
springer inter-national publishing..bill yuchen lin, wangchunshu zhou, ming shen, peizhou, chandra bhagavatula, yejin choi, and xiangren.
2020. commongen: a constrained text gen-eration challenge for generative commonsense rea-soning.
in findings of the association for computa-tional linguistics: emnlp 2020, pages 1823–1840,online.
association for computational linguistics..chin-yew lin and eduard hovy.
2003..auto-matic evaluation of summaries using n-gram co-occurrence statistics.
in proceedings of the 2003 hu-man language technology conference of the northamerican chapter of the association for computa-tional linguistics, pages 150–157..tsung-yi lin, michael maire, serge belongie, jameshays, pietro perona, deva ramanan, piotr doll´ar,and c lawrence zitnick.
2014. microsoft coco:in european confer-common objects in context.
ence on computer vision, pages 740–755.
springer..ye liu, yao wan, lifang he, hao peng, and philip s.yu.
2021. kg-bart: knowledge graph-augmentedbart for generative commonsense reasoning.
in pro-ceedings of the aaai conference on artiﬁcial intel-ligence..kishore papineni, salim roukos, todd ward, and wei-jing zhu.
2002. bleu: a method for automatic eval-in proceedings ofuation of machine translation.
the 40th annual meeting of the association for com-putational linguistics, pages 311–318, philadelphia,pennsylvania, usa.
association for computationallinguistics..ratish puduppully, li dong, and mirella lapata.
2019.data-to-text generation with content selection andplanning.
proceedings of the aaai conference onartiﬁcial intelligence, 33(01):6908–6915..colin raffel, noam shazeer, adam roberts, katherinelee, sharan narang, michael matena, yanqi zhou,wei li, and peter j liu.
2019. exploring the limitsof transfer learning with a uniﬁed text-to-text trans-former.
arxiv preprint arxiv:1910.10683..rico sennrich, barry haddow, and alexandra birch.
2016. neural machine translation of rare wordswith subword units.
in proceedings of the 54th an-nual meeting of the association for computationallinguistics (volume 1: long papers), pages 1715–1725, berlin, germany.
association for computa-tional linguistics..peter shaw, jakob uszkoreit, and ashish vaswani.
2018. self-attention with relative position represen-in proceedings of the 2018 conference oftations.
the north american chapter of the association forcomputational linguistics: human language tech-nologies, volume 2 (short papers), pages 464–468,new orleans, louisiana.
association for computa-tional linguistics..113