maria: a visual experience powered conversational agent.
zujie liang1∗ † huang hu2† can xu2 chongyang tao2xiubo geng2 yining chen2.
fan liang1 daxin jiang2‡.
1school of electronics and information technology,sun yat-sen university, guangzhou, china2microsoft stca nlp group, beijing, china1{liangzj9@mail2.sysu.edu.cn, isslf@mail.sysu.edu.cn}2{huahu,caxu,chotao,xigeng,yinichen,djiang}@microsoft.com.
abstract.
arguably, the visual perception of conversa-tional agents to the physical world is a key wayfor them to exhibit the human-like intelligence.
image-grounded conversation is thus proposedto address this challenge.
existing works fo-cus on exploring the multimodal dialog mod-els that ground the conversation on a givenimage.
in this paper, we take a step furtherto study image-grounded conversation under afully open-ended setting where no paired dia-log and image are assumed available.
speciﬁ-cally, we present maria, a neural conversationagent powered by the visual world experienceswhich are retrieved from a large-scale imageindex.
maria consists of three ﬂexible compo-nents, i.e., text-to-image retriever, visual con-cept detector and visual-knowledge-groundedresponse generator.
the retriever aims to re-trieve a correlated image to the dialog from animage index, while the visual concept detec-tor extracts rich visual knowledge from the im-age.
then, the response generator is groundedon the extracted visual knowledge and dialogcontext to generate the target response.
ex-tensive experiments demonstrate maria outper-forms previous state-of-the-art methods on au-tomatic metrics and human evaluation, and cangenerate informative responses that have somevisual commonsense of the physical world..1.introduction.
building intelligent conversational agents that cannot only converse freely with human but also havethe ability to perceive the physical world, has beenone of the longest standing goals of natural lan-guage processing (nlp) and artiﬁcial intelligence(ai).
although the recent large-scale conversationmodels trained on text-only corpora, such as meena.
∗work performed during the internship at microsoft.
† equal contribution.
‡ corresponding author..figure 1: an example of human conversations.
whenhuman-b talks about vacation on the beach of hawaii,human-a recalls his/her past experience of playing vol-leyball or having bbq on the beach..(adiwardana et al., 2020), blender (roller et al.,2020) and dialogpt (zhang et al., 2020), haveshown the compelling performance, they are stilllack of the perception ability to our physical world.
a recent study (bisk et al., 2020) points out the suc-cessful linguistic communication relies on a sharedexperience of the world that makes language re-ally meaningful.
the visual perception is a richsignal for modeling a vastness of experiences inthe world that cannot be documented by text alone(harnad, 1990).
on the other hand, human-humanconversations involve their understandings of con-text, the background knowledge they had, and per-haps most importantly the experiences of the worldthey shared, e.g., what they have seen before..figure 1 shows a conversation between humans.
human-a recalls his/her past experience of play-ing volleyball or having bbq on the beach whenhuman-b talks about vacation on the beach ofhawaii.
however, the association relationship be-tween beach and volleyball (or bbq) is hard tocapture in traditional knowledge bases, such asknowledge graph.
motivated by this, we selecta common word “pizza” and collect the top 17words that mostly co-occur with “pizza” on google.
proceedingsofthe59thannualmeetingoftheassociationforcomputationallinguisticsandthe11thinternationaljointconferenceonnaturallanguageprocessing,pages5596–5611august1–6,2021.©2021associationforcomputationallinguistics5596human-a: hey!
how was your vacation?human-b: awesome!
i had a good time with my friends in hawaii, the beachesare very beautiful there.human-a:cool!
did you play beach volleyballwith your friends?
(human-a: cool, have you had a bbqwith your friends on the beach?
the grilled fish was great!
)human-b: nope, but it sounds great.maybenexttime.
figure 2: the word co-occurrence distribution with “pizza” on google knowledge graph and ms-coco images..knowledge graph1 and ms-coco images2 (linet al., 2014).
as shown in figure 2, the words co-occurring with “pizza” on knowledge graph tend tobe the abstract concepts, while the co-occurrencerelationship of object tags on images reﬂects somecommonsense of our physical world, e.g., “pizza”is usually on the “dining table”, people usuallyuse “knife” when eating “pizza”.
interestingly, wefound the “pizza” also co-occurs with “cell phone”and even “plotted plant”.
this indicates when peo-ple eat pizza, they sometimes would put their cellphones aside on the table, or there might exist someplotted plants in the restaurant.
thus, empoweringconversational agents to have the visual perceptionability about the physical world is a key way forthem to exhibit the human-like intelligence..the existing works (mostafazadeh et al., 2017;huber et al., 2018; shuster et al., 2020) focus on ex-ploring the multimodal dialog models that groundthe conversation on a given image.
recently, yanget al.
(2020) propose to learn the dialog generationmodel with both image-grounded dialogs and tex-tual dialogs by resorting to text-to-image synthesistechniques (xu et al., 2018; qiao et al., 2019) torestore a latent image for the text-only dialog.
evenso, these works are still constrained by the assump-tion that the dialog is conducted center around agiven (or synthesized) image..in this paper, we take a step further to extendthe assumption of image-grounded conversationto a fully open-ended setting where no image-dialog pairs are assumed available.
speciﬁcally,we present maria, a neural conversational agentpowered by visual world experiences which areretrieved from a pre-built image index, e.g., the.
1https://developers.google.com/.
knowledge-graph/.
2we calculate the co-occurrence distribution of object tagsfrom the images in ms-coco dataset.
more examples couldbe found in appendices..open images dataset (kuznetsova et al., 2018).
maria consists of three components:text-to-image retriever, visual concept detector, and visual-knowledge-grounded response generator.
the re-triever is responsible for retrieving a piece of vi-sual world experiences, e.g., a correlated imageto the dialog from an image index.
the visualconcept detector utilizes the object detector fromupdown (anderson et al., 2018) to extract the re-gions features (i.e., bboxes) and the correspondingvisual concepts (i.e., tags) from the retrieval images.
hence, we can construct (bboxes, tags, context, re-sponse) 4-tuple as the training data.
finally, theseconstructed 4-tuples are used to train the visual-knowledge-grounded response generator, which isbuilt on the top of a multi-layer transformer archi-tecture (vaswani et al., 2017).
to effectively injectthe visual knowledge into the response generator,we carry out the masked concept prediction andvisual knowledge bias besides the response gen-eration objective.
the former aims to align the se-mantic representations between textual words andimage regions, while the latter tries to provide morevisual knowledge to facilitate the dialog generation.
the experimental results on reddit conversationcorpus (dziri et al., 2019a) demonstrate that mariasigniﬁcantly outperforms previous state-of-the-artmethods, and can generate informative responseswith visual commonsense of our physical world..overall, the contributions of this paper are sum-.
marized as follows:.
• we explore the task of image-grounded dia-log generation under a fully open-ended set-ting where no speciﬁc image-dialog pairs areassumed available, i.e., zero-resource image-grounded conversation.
to the best of ourknowledge, this is the ﬁrst work to connectdialog corpus with the unpaired image data;.
• we present maria, a neural conversational.
5597agent consisting of three ﬂexible components,which can effectively capture the visual com-monsense from images and accordingly gen-erate informative and vivid responses;.
• extensive experiments on the widely usedreddit conversation corpus are conductedto justify the effectiveness of maria..2 related work.
vision and languagein the research of visionand language, various tasks have been extensivelystudied, such as image captioning (vinyals et al.,2015; lu et al., 2017; hu et al., 2020), visual ques-tion answering (antol et al., 2015; anderson et al.,2018), visual dialog (das et al., 2017a,b).
popularbenchmark datasets in this area include ms-coco(lin et al., 2014), visdial (das et al., 2017a) andvisual genome (krishna et al., 2017).
visual di-alog is a task to answer the questions about thefactual content of the image in a multi-turn manner.
differently, image-grounded conversation studieshow to reply to a dialog context and a given imagewith proper responses in an open-ended way..dialog generationencouraged by the successof the neural sequence-to-sequence architecture(sutskever et al., 2014) on machine translation,end-to-end neural approaches on open-domain dia-log generation (vinyals and le, 2015; shang et al.,2015; serban et al., 2016; sordoni et al., 2015; xinget al., 2017; wu et al., 2018; zhang et al., 2020;xu et al., 2019; adiwardana et al., 2020) have beenwidely studied in literature.
recently, there is anemerging trend towards grounding the dialog gen-eration models on the external knowledge, such asknowledge graphs (zhou et al., 2018), documents(ghazvininejad et al., 2018; dinan et al., 2019; kimet al., 2020; zhao et al., 2020a,b; li et al., 2020)and images (mostafazadeh et al., 2017; shusteret al., 2020; yang et al., 2020).
different from theprevious work on knowledge-grounded conversa-tion that connects dialogs with unpaired documentknowledge (li et al., 2020), our work lies in theresearch of image-grounded conversation where aresponse is generated with a dialog context and agiven image.
existing works (mostafazadeh et al.,2017; shuster et al., 2020; yang et al., 2020) inthis direction assume there is a given (or synthe-sized) image for the dialog and explore the multi-modal dialog models.
in contrast to these works,we study the image-grounded conversation under.
figure 3: the ﬂowchart of our framework.
o, q, c, rrepresents the image region features, extracted visualconcepts, dialog context and response..a fully open-ended assumption where no paireddialog and image are assumed available, i.e., zero-resource image-grounded conversation..3 problem formalization.
suppose we have a dialog set d = {(ci, ri)}ni=1,where ∀i ∈ {1, .
.
.
, n}, ci refers to a dialogcontext and ri is a response to ci.
we assumethere is a set of images v = {vj}mj=1, where∀j ∈ {1, .
.
.
, m}, vj denotes an image.
∀c ∈ d,we assume that there is an image v that triggeredby the given dialog context c and response r. ourgoal is to estimate a generation model p (r|v, c)from d and v. thus, given a new dialog context cassociated with an image v , the model can gener-ate a response r according to p (r|v, c)..4 methodology.
to learn such a generation model p (r|v, c), weneed to tackle several challenges: (1) how to bridgethe gap between unpaired dialog corpus and imagedata; (2) after obtaining the correlated images, howto extract the detailed visual features and concepts;(3) how to effectively inject the visual knowledgeinto response generator and enable it to generate re-sponses that are visual-knowledge-grounded.
fig-ure 3 illustrates the framework of our approach.
weﬁrst build a large-scale image dataset and leveragea cross-modal matching model to retrieve a corre-lated image using the content of the dialog.
thenan off-the-shelf object detector is applied to extract-ing the object features and visual concepts from theretrieval image.
finally, the response generator istrained to generate the target response conditioned.
5598multi-turn reddit conversationimage index (open images)visual concept detectorvisual-commonsense-aware response generation model training: (𝑪,𝑹); inference: 𝑪text-to-image retrieve moduletop-k images (k=1 here): (𝑽𝟏,𝑽𝟐,…,𝑽𝒌)training quaternion: 𝑶,𝑸,𝑪,𝑹; inference triplet: (𝑶,𝑸,𝑪)on the context, extracted object features, and vi-sual concepts.
in the rest of this section, we willelaborate these three modules..4.1 text-to-image retriever.
in this section, we develop a retrieval model thatassigns each dialog with a correlated image v .
speciﬁcally, we train a text-to-image matchingmodel from image captioning dataset and utilize itto construct the (c, r, v ) triple data..modelingto improve the efﬁciency of cross-modal retrieval model on large-scale dialog corpusand image dataset, we adopt a two-tower architec-ture (lu et al., 2019) to accelerate the retrieval pro-cess where the image features can be pre-extractedofﬂine.
the model takes a sentence t and an im-age v as input, and predicts the relevance scores(t, v ) between the sentence and the image.
weuse a text encoder and an image encoder to pro-duce the representations of t and v , respectively.
the text encoder is a pre-trained bert-base model(devlin et al., 2019) and we use the hidden state ofspecial token [cls] as the embedding of t :.
et = bert (t ).
(1).
then a multi-layer perceptron (mlp) projects thesentence embedding into the cross-modal space.
we follow tan and bansal (2020) to perform l2-normalization on the last output features, by whichwe can simplify the nearest neighbor search prob-lem in the euclidean space to the maximum innerproduct problem (mussmann and ermon, 2016):.
ft (t ) =.
ht (et)(cid:107)ht (et)(cid:107).
(2).
similarly, the image encoder is composed of a pre-trained resnext backbone (xie et al., 2017) and amlp with l2 normalization:.
fv (v ) =.
, ev = resn ext(v ) (3).
hv (ev)(cid:107)hv (ev)(cid:107).
thus, we deﬁne the relevance score s(t, v ) as aninner product of the language feature representationft (t ) and image feature representation fv (v ):.
s(t, v ) = ft (t )(cid:62) fv (v ).
(4).
training we train the cross-modal matchingmodel on ms-coco image captioning dataset (linet al., 2014), where each image is paired with 5 sen-tences describing its visual content.
the model is.
optimized by minimizing the hinge loss so thatthe relevance score s (t, v ) of the positive image-sentence pair can be larger than the negative pairs (t, v −) by at least a margin m :.
(cid:0)t, v, v −(cid:1) =.
lhingel(cid:88).
i=1.
max{0, m − s (t, v ) +s (cid:0)t, v −(cid:1)(cid:9).
(5).
inference given the trained retrieval model, wecan now assign each dialog with a correlated im-age v .
to ensure the diversity and richness of theretrieval results, we fetch 500,000 images from thelarge-scale open images dataset (kuznetsova et al.,2018) as our image set v. the image vi ∈ v withthe maximum relevance score is paired with thegiven dialog (ci, ri) ∈ d. note that for the dialogin the training set, we use both the context c and re-sponse r are concatenated as the query for retrieval(i.e., t = (c, r)), which is beneﬁcial to retrievingan image with the related visual knowledge.
on theother hand, for the validation/test set of the dialogcorpus, the query is only the context (i.e., t = c)so as to keep consistent with the real-world settingwhere the response is unavailable and need to begenerated at inference..4.2 visual concept detector.
given the correlated image vi to the dialog as thevisual clue, we can now extract the visual knowl-edge from it.
one naive approach is to utilize thecnn-based models to extract the latent image fea-tures.
however, this approach does not consider theﬁne-grained representation modeling for images,which is crucial for the dialog model to understandthe local visual features in images.
to address thisissue, we adopt an object detection model (ander-son et al., 2018) pre-trained on visual genome(krishna et al., 2017) to extract a set of salientobject features o = {ok}kk=1, where each objectfeature ok is a 2048-dimensional vector.
thesefeatures represent the images at the level of objectsand other salient regions, which has proven to bevital in many high-level image understanding tasks.
besides, the same detector is used to extract a setof visual concepts q = {qm}km=1, where each con-cept qm is the high-precision textual label of thevisual region, e.g., “sunset”, “melon”, etc.
in thismanner, we simultaneously obtain the ﬁne-grainedimage representations and the necessary visual con-cepts for the subsequent dialog generation..55994.3 visual-knowledge-grounded response.
generator.
in this section, we propose a uniﬁed architectureto effectively inject a set of region features andcorresponding visual concepts into the responsegeneration model.
in following parts, we describethe model design and training objectives in detail..4.3.1 model architecture.
figure 4 shows the architecture of our response gen-eration model, which is a multi-layer transformernetwork for both bidirectional vision/context(o, q, c) encoding, and unidirectional responser decoding, via the ﬂexible self-attention masksinspired by (dong et al., 2019)..4.3.2.input representation.
for each token, the ﬁnal input representation tothe multi-layer transformer network is the element-wise summation of four kinds of embeddings, in-cluding token-level, turn-level, position-level, andsegment-level.
then, we concatenate all the inputrepresentations to one sequence for model training..token-levelthe token-level embeddings arethe concatenation of (ow, qw, cw, rw), which de-note the token embedding sequence of visual ob-jects, visual concepts, contexts and response re-spectively.
note that ow is the object embeddingtransformed by a linear layer into the same dimen-sion as word embedding..turn-levelsince the dialog is multi-turn, weencode this turn order with a relative turn embed-ding (bao et al., 2020).
speciﬁcally, the turn num-ber is counted from the last utterance of the dia-logue to the beginning.
note that as for the tokenscorresponding to o and q, we simply set them thesame as the ﬁrst utterance of c..position-levelpositional embedding encodesthe signal of the token order in the total input se-quence, which is the same as positional encodingof the original transformer (vaswani et al., 2017)..segment-levelsegment embedding is em-ployed to differentiate which segment the tokenis in, i.e., o, q, c or r..4.3.3 masked concept prediction.
due to the inherent gap between visual modalityand textual modality, directly optimizing the modelby response generation objective may result in theinsufﬁcient utilization of the visual knowledge.
to.
align the semantic representations of two modali-ties, we devise masked concept prediction (mcp)objective.
15% of the visual concepts are randomlyreplaced with [mask] tokens in each training in-stance, which need to be predicted by the model.
however, one problem still remains, i.e., the visualconcepts have no speciﬁc order when extractingfrom images.
in other words, we need to modelmcp as a matching problem of set, which does notneed to consider the order of predicted conceptswhen there are more than two concepts maskedout simultaneously.
to tackle this, inspired by huet al.
(2020), we adopt the hungarian matchingloss (stewart et al., 2016; carion et al., 2020) toestimate an optimal mapping α so that the predic-tion for each masked position is assigned one of thetarget concepts.
here we denote the set of all inputas x = (o, q, c, r), the set of the bidirectionalself-attention part of x as b = (o, q, c), the setof masked concepts as ˆq, the set of unmasked to-kens as b\ ˆq, and the prediction probabilities ofthe corresponding representations in the ﬁnal layerof transformer as h = {hi}mi=1 where hi is theprobability distribution of the i-th masked position.
hence, the mcp loss can be deﬁned as:.
lmcp( ˆq, h, α) =(cid:16).
(cid:88).
log hi.
−.
qα(i)∈ ˆq.
(cid:17)qα(i) | b\ ˆq.
(6).
where α(i) is the index of the target concept as-signed to the i-th prediction.
when predicting amasked concept, the model will have to resort tovisual region features, dialog contexts and other un-masked visual concepts.
this would help the modelto align the cross-modal representations betweentext and visual regions..4.3.4 masked response prediction.
encouraged by the success of unilm (dong et al.,2019) in seq2seq tasks, we adopt the masked re-sponse prediction (mrp) objective to model theresponse generation.
during training, 70% of thetokens in r are randomly masked with the specialtoken [mask].
the model is optimized to recoverthe masked tokens.
the masked response tokensand other unmasked tokens in the whole input se-quence can be denoted as ˆr and x\ ˆr, respectively.
suppose that pi is the conditional probability dis-tribution of the i-th token in r, the mrp loss isthe negative log-likelihood (nll) of the masked.
5600figure 4: the overview of the response generation model.
there are four kinds of inputs, i.e., image region featureso, extracted visual concepts q, dialog context c and response r. the self-attention mask in r is unidirectional,i.e., can only attend to the left context, while the self-attention mask in other segments is bidirectional..response tokens as follow:.
vocabulary bias bq is ﬁrst calculated as follow:.
lmrp(x, ˆr) = −.
log pi.
wi | x\ ˆr.
(7).
(cid:16).
(cid:17).
(cid:88).
wi∈ ˆr.
note that the self-attention mask in r is left-to-right, but the rest are bidirectional.
in other words,the tokens in o, q and c can attend to each otherfrom both directions, while the tokens in r canattend all tokens in o, q, c and the leftward tokensin r including itself.
mrp implicitly encouragesthe model to generate responses by learning therelationship among all input tokens..for decoding, we ﬁrst encode the image regions,visual concepts, dialog contexts, and a special to-ken [bos] as input.
then the model starts thegeneration by feeding a [mask] token and sam-ples a word from the predicted distribution overvocabulary.
then, the [mask] token is replacedby the generated token and a new [mask] is ap-pended to the input sequence for next word pre-diction.
the generation process terminates whenthe model predicts [eos] token or reaches thepre-deﬁned maximum length..visual knowledge bias normally, the top pro-jection layer of generation model produces a prob-ability distribution over the vocabulary:.
p = sof tmax(w er + b),.
(8).
where the er ∈ rd, w ∈ r|v |×d and b ∈ r|v | arethe last output of the transformer network, weightand bias parameters of the decoding head, respec-tively.
|v | denotes the vocabulary size.
so far, thevisual world knowledge is introduced into the re-sponse generation model by the shared-parameterself-attention layers.
to further inject the visualknowledge into the generation model, we design asimple but effective strategy, namely visual knowl-edge bias (vkb).
concretely, an additional visual.
bq = fq(eq.
avg).
(9).
: rd → r|v | is a projection layer.
where fqeqavg denotes the average pooling on all hiddenrepresentations of visual concepts, i.e., eqavg =avgp ooling(eq) where eq = (eqk).
then,we mask non-visual-concept tokens in the vocabu-lary and the masked vocabulary bias ˆbq ∈ r|v | isadded to the top layer of generation model to getthe ﬁnal distribution over vocabulary:.
1, ..., eq.
ˆp = sof tmax(w er + b + ˆbq).
(10).
we leverage this ﬁnal vocabulary distribution to cal-culate the mrp loss in eq.
7 to optimize the model.
this visual knowledge bias would encourage themodel to generate more visual knowledge relatedtokens in the response..to sum up, the ﬁnal objective of our responsegeneration model is to minimize the integrated loss:.
l = lmrp + lmcp.
(11).
5 experimental setup.
5.1 datasets.
to evaluate the performance of maria, we con-duct comprehensive experiments on the redditdataset released by yang et al.
(2020), which isa large-scale and high-quality multi-turn conversa-tions extracted from reddit conversation corpus(dziri et al., 2019b).
each dialog has 3 to 5 ut-terances, and the training/validation/test set has1m/20k/20k dialogs respectively..we train and validate the retrieval model us-ing the karpathy’s split3 of the ms-coco imagecaptioning data, where the images are split into.
3https://cs.stanford.edu/people/.
karpathy/deepimagesent.
5601position-leveltoken-levelnetworkturn-levelsegment-levelmulti-layer transformervisresponse (𝑹)dialog context (𝑪)visual concepts (𝑸)region features (𝑶)visvisvisvisvistagtagtagtagusrusrusrusrusrusrusrusrusrusrsyssyssyssyssyssyssyssyssyssys01234567891011121314151617181920212223242526272829-2-2-2-2-2-2-2-2-2-2-2-2-2-2-2-2-1-1-1-10000000000didyoueat?.isafor!sepsepsepsepboseosmaskmaskmaskmaskhamwhatpizzatablecolacolapizzamatchpizzaperfect𝑂𝑄𝐶𝑅𝑂𝑄𝐶𝑅𝑂,𝑄,𝐶: bidirectional attention𝑅:  attend to left contentprevent from attending113.2k/5k/5k samples as training/validation/testset, respectively.
after the retrieval model istrained, we fetch 500k images from the open im-ages dataset as the image index, and then retrieveimages from it by dialog context and response toconstruct the training data for response generator..5.2 evaluation metrics.
both automatic metrics and human evaluation areemployed to assess the performance of maria andbaselines.
automatic metrics include: (1) fluency:perplexity (ppl) measures the conﬁdence of thegenerated responses; (2) relevance: bleu-1 (pa-pineni et al., 2002), rouge-l (lin, 2004), and wefollow serban et al.
(2017) to utilize embeddingaverage cosine similarity, vector extrema cosinesimilarity, and embedding greedy matching score.
all this metrics are calculated by running the publicnlg evaluation script4; (3) diversity: distinct-1(dist-1) and distinct-2 (dist-2) (li et al., 2016)are deﬁned as the number of distinct uni-grams orbi-grams divided by the total amount of words..in human evaluation, we randomly select 100dialogue contexts and the corresponding generatedresponses for maria and compared baselines.
threehuman annotators are asked to score the responsequality on a scale of {0, 1, 2} from three aspects,including fluency, relevance and richness.
thehigher score means the better.
since each responsereceives 3 scores on each aspect, we report theaverage scores over annotators and responses.
theinter-annotator agreement is measured by fleiss’kappa(fleiss and cohen, 1973)..5.3.implementation details.
for the retrieval model, resnext-101-32x8d fea-ture is used as the visual embedding, while theconcatenation of the last 4 layers of bert’s out-puts is used as the textual embedding.
both em-beddings are then respectively fed into an mlpcomposed of three layers of size (1024, 1024, 512).
when training the retrieval model, we set the mar-gin m = 0.5 for the hinge loss, and only tunethe parameters of both mlps while freezing theparameters of resnext and bert.
the total train-ing epoch is 20. at inference, the faiss (johnsonet al., 2019) library is utilized to accelerate the in-ner product search by batch processing.
we use theoff-the-shelf object detector from updown (an-derson et al., 2018) to extract top-k (k=36) image.
4https://github.com/maluuba/nlg-eval.
region features and the corresponding visual con-cepts.
the detector is a faster r-cnn (ren et al.,2015) model trained on the visual genome dataset(krishna et al., 2017)..for the response generation model, we set thenumber of transformer layers l = 12 and the hid-den embedding dimension d = 768. besides,the network parameters are initialized by unilm.
the maximum sequence lengths of context and re-sponse are set to 110 and 40, respectively.
thesequence lengths of region features and concepttokens are both set to 36. the batch size is 64. weuse the adam optimizer (kingma and ba, 2015)with a learning rate 3e-5 to train the response gener-ation model.
the training is conducted on 4 nvidiatesla p40 24g gpu cards for 20 epochs..5.4 baselines.
we compare the following baselines in the exper-iments: (1) seq2seq: a standard sequence to se-qence model with attention mechanism (bahdanauet al., 2015).
(2) hred: a hierarchical recurrentencoder-decoder neural network (serban et al.,2016).
(3) vhred: a variation of hred thatintroduces latent variables into the generation (ser-(4) recosa: a hierarchicalban et al., 2017).
transformer-based model (zhang et al., 2019) thatachieves the state-of-the-art performance on bench-marks of dialog generation.
(5) imgvae: a di-alog generation model (yang et al., 2020) that istrained on both textual dialogs and image-groundeddialogs by recovering a latent image behind the tex-tual dialog within a conditional variational auto-(6) dialogpt: an open-encoding framework.
domain dialog model (zhang et al., 2020) thatﬁne-tunes gpt-2 (radford et al., 2019) on massivereddit data.
since dialogpt is a dialog generationmodel trained on the text-only corpus, we introduceit as an auxiliary baseline.
for a fair comparison,we choose the same model size (l=12,d=768) ofdialogpt (117m) as our model..6 experimental results.
6.1 automatic and human evaluations.
we summarize the experimental results of auto-matic evaluations in table 1. maria achieves thesubstantial performance improvements over base-lines on all metrics except for the comparison to di-alogpt.
especially, maria signiﬁcantly surpassesimgvae on dist-1/2, which indicates introducingricher visual knowledge, i.e., image region features.
5602modelseq2seq (bahdanau et al., 2015)hred (serban et al., 2016)vhred (serban et al., 2017)recosa (zhang et al., 2019)imgvae (yang et al., 2020)dialogpt (zhang et al., 2020)mariamaria (w/o mcp)maria (w/o vkb)maria (w/o vkb & mcp)maria (w/o images)maria (w/o concepts)maria (w/o images & concepts).
ppl bleu-1 rouge-l average extrema greedy dist-1 dist-21.9677.273.2184.023.4978.013.8371.756.3472.0649.8636.0333.3554.3831.8066.7129.4465.5128.5362.6428.0164.7516.4469.2410.1169.50.
78.3875.5475.5779.8479.9577.8082.5481.5982.4977.5278.8982.9680.62.
40.0637.4939.2442.2942.3835.4044.1441.0640.2241.2739.8841.0241.15.
62.6460.4162.0763.0263.5558.3965.9864.1064.4961.0062.3965.0764.25.
12.2111.6812.2212.7512.585.8714.2113.9112.7611.5010.7011.4310.75.
10.8111.2911.8211.7512.055.2013.0211.6011.7610.459.1510.618.34.
0.530.890.870.661.5210.418.448.367.156.926.884.563.69.table 1: evaluation results of generated responses on the test set.
numbers in bold denote that the improvementover the best performing baseline is statistically signiﬁcant.
numbers with underline refer to the best results exceptfor the comparison to dialogpt (zhang et al., 2020)..modelimgvaedialogptmaria.
fulency relevance richness kappa0.670.590.62.
1.791.931.89.
0.671.200.97.
0.580.921.06.table 2: human evaluation results..and the corresponding visual concepts, is beneﬁ-cial to generating more diverse and informativeresponses.
this also reﬂects in human evaluationof table 2 that the richness score of maria is higherthan that of imgvae.
besides, in terms of rele-vance metrics including bleu-1, rouge-l, aver-age, extrema and greedy, maria outperforms allbaselines and even performs better than dialogpt.
this indicates introducing the extra visual knowl-edge related to dialog context can further force themodel to produce more relevant responses..on the other hand, the discrepancy of data dis-tributions between the training data (i.e., image-chat (shuster et al., 2020) dataset) and test data(i.e., reddit conversation dataset) of the text-to-image synthesis model in imgvae limits its per-formance in practice.
besides, constrained by thecapability of the text-to-image synthesis model, therichness and diversity of the synthesized imagesare undesirable, while maria can retrieve a vari-ety of images from the large-scale image index.
that may be the reason why imgvae consistentlyunderperforms our maria on relevance includingautomatic evaluation and human judgement, whichalso shows the superiority of the retrieval methodfor the zero-resource image-grounded conversation.
another observation is that maria slightly under-performs dialogpt on ppl and dist-1/2.
sincedialogpt is a large-scale pre-training based dialoggeneration model and introduces the extra mutual.
information maximization objective to improve theinformativeness of generated responses, which isconsistent in human evaluation with respect to ﬂu-ency and richness..6.2 ablation study.
we conduct extensive ablation experiments overdifferent model variants and input components tobetter understand their relative importance to thedialog generation task.
as shown in table 1, train-ing the simpliﬁed versions of maria or removingany visual signals from input components leads toworse performance in terms of relevance and diver-sity.
in particular, the results on the ablation studyvalidate that: (1) the performance improvementof dialog generation beneﬁts from the mcp’s ef-fectiveness in aligning the representations of textand vision; (2) when training maria, introducingvkb can further improve the quality and diversityof generated responses; (3) rich visual knowledge,i.e., image region features and visual concepts, playa signiﬁcant role in improving the performance ofdialog generation.
especially, removing the visualconcepts leads to a dramatic performance drop ondiversity.
the phenomenon is due to the lack ofnecessary visual concepts, maria can not well un-derstand the visual world knowledge when onlylearning from the visual features..6.3 case analysis.
to further investigate the quality of responses gen-erated by maria, we put an example of generatedresponses in figure 5. as we can see from fig-ure 5, when the context talks about the supermarket“aldi”, maria can retrieve a “pizza” related imageand generate the informative response grounded on.
5603figure 5: the visualization of attention weights on the retrieved image by maria for an example..it, i.e., “the pizza at aldi is the best in the world”.
this implies the commonsense that the supermarketusually has the pizza to sell.
it is also observed thatmaria pays more attention to the relevant imageregions when generating the word “pizza”, whichdemonstrates that maria could capture useful visualknowledge from the image and subsequently lever-age it to generate commonsense-aware responses.
more cases are demonstrated in appendices..7 conclusions.
in this paper, we present maria, a neural conver-sational agent powered by the visual world expe-riences.
it is able to retrieve the visual world ex-periences with users and generate human-like re-sponses with some visual commonsense.
extensiveexperiments demonstrate maria achieves substan-tial improvements over the state-of-the-art methodsin automatic and human evaluation.
the futureworks could include: (1) design a more preciseand comprehensive image retriever to include mul-tiple retrieval images; (2) combining the retrievemodule and dialog generation into an end-to-endmodel, instead of learning them individually; (3)explore more efﬁcient neural architectures to injectthe visual knowledge into response generation..references.
daniel adiwardana, minh-thang luong, david r so,jamie hall, noah fiedel, romal thoppilan, zi yang,apoorv kulshreshtha, gaurav nemade, yifeng lu,et al.
2020. towards a human-like open-domainchatbot.
arxiv preprint arxiv:2001.09977..peter anderson, xiaodong he, chris buehler, damien.
teney, mark johnson, stephen gould, and leizhang.
2018. bottom-up and top-down attention forimage captioning and visual question answering.
in2018 ieee conference on computer vision and pat-tern recognition, cvpr 2018, salt lake city, ut,usa, june 18-22, 2018, pages 6077–6086.
ieeecomputer society..stanislaw antol, aishwarya agrawal, jiasen lu, mar-garet mitchell, dhruv batra, c. lawrence zitnick,and devi parikh.
2015. vqa: visual question an-swering.
in 2015 ieee international conference oncomputer vision, iccv 2015, santiago, chile, de-cember 7-13, 2015, pages 2425–2433.
ieee com-puter society..dzmitry bahdanau, kyunghyun cho, and yoshua ben-gio.
2015. neural machine translation by jointlyin 3rd inter-learning to align and translate.
national conference on learning representations,iclr 2015, san diego, ca, usa, may 7-9, 2015,conference track proceedings..siqi bao, huang he, fan wang, hua wu, and haifengwang.
2020. plato: pre-trained dialogue genera-tion model with discrete latent variable.
in proceed-ings of the 58th annual meeting of the associationfor computational linguistics, pages 85–96, online.
association for computational linguistics..yonatan bisk, ari holtzman, jesse thomason, jacobandreas, yoshua bengio, joyce chai, mirella lap-ata, angeliki lazaridou, jonathan may, aleksandrnisnevich, nicolas pinto, and joseph turian.
2020.experience grounds language.
in proceedings of the2020 conference on empirical methods in naturallanguage processing (emnlp), pages 8718–8735,online.
association for computational linguistics..nicolas carion, francisco massa, gabriel synnaeve,nicolas usunier, alexander kirillov, and sergeyzagoruyko.
2020. end-to-end object detection withtransformers.
in european conference on computervision, pages 213–229.
springer..5604dialog context:mariaa: noaldi?hahahjokes.b: aldiisbyfarthebest.
(note: aldiisthe name of a supermarket)thepizzaataldiisthebestintheworldabhishek das, satwik kottur, khushi gupta, avisingh, deshraj yadav, jos´e m. f. moura, deviparikh, and dhruv batra.
2017a.
visual dialog.
in2017 ieee conference on computer vision and pat-tern recognition, cvpr 2017, honolulu, hi, usa,july 21-26, 2017, pages 1080–1089.
ieee computersociety..abhishek das, satwik kottur, jos´e m. f. moura, ste-fan lee, and dhruv batra.
2017b.
learning coop-erative visual dialog agents with deep reinforcementlearning.
in ieee international conference on com-puter vision, iccv 2017, venice, italy, october 22-29, 2017, pages 2970–2979.
ieee computer soci-ety..jacob devlin, ming-wei chang, kenton lee, andkristina toutanova.
2019. bert: pre-training ofdeep bidirectional transformers for language under-in proceedings of the 2019 conferencestanding.
of the north american chapter of the associationfor computational linguistics: human languagetechnologies, volume 1 (long and short papers),pages 4171–4186, minneapolis, minnesota.
associ-ation for computational linguistics..emily dinan, stephen roller, kurt shuster, angelafan, michael auli, and jason weston.
2019. wizardof wikipedia: knowledge-powered conversationalin 7th international conference on learn-agents.
ing representations, iclr 2019, new orleans, la,usa, may 6-9, 2019. openreview.net..li dong, nan yang, wenhui wang, furu wei, xi-aodong liu, yu wang, jianfeng gao, ming zhou,and hsiao-wuen hon.
2019. uniﬁed languagemodel pre-training for natural language understand-in advances in neural infor-ing and generation.
mation processing systems 32: annual conferenceon neural information processing systems 2019,neurips 2019, december 8-14, 2019, vancouver,bc, canada, pages 13042–13054..nouha dziri, ehsan kamalloo, kory mathewson, andosmar zaiane.
2019a.
augmenting neural responsegeneration with context-aware topical attention.
inproceedings of the first workshop on nlp for con-versational ai, pages 18–31, florence, italy.
associ-ation for computational linguistics..nouha dziri, ehsan kamalloo, kory mathewson, andosmar zaiane.
2019b.
augmenting neural responsegeneration with context-aware topical attention.
inproceedings of the first workshop on nlp for con-versational ai, pages 18–31, florence, italy.
associ-ation for computational linguistics..joseph l fleiss and jacob cohen.
1973. the equiv-alence of weighted kappa and the intraclass corre-lation coefﬁcient as measures of reliability.
educa-tional and psychological measurement, 33(3):613–619..marjan ghazvininejad, chris brockett, ming-weichang, bill dolan, jianfeng gao, wen-tau yih, and.
michel galley.
2018. a knowledge-grounded neuralin proceedings of the thirty-conversation model.
second aaai conference on artiﬁcial intelligence,(aaai-18), the 30th innovative applications of arti-ﬁcial intelligence (iaai-18), and the 8th aaai sym-posium on educational advances in artiﬁcial intel-ligence (eaai-18), new orleans, louisiana, usa,february 2-7, 2018, pages 5110–5117.
aaai press..stevan harnad.
1990. the symbol grounding prob-physica d: nonlinear phenomena, 42(1-.lem.
3):335–346..xiaowei hu, xi yin, kevin lin, lijuan wang, leizhang, jianfeng gao, and zicheng liu.
2020. vivo:surpassing human performance in novel object cap-tioning with visual vocabulary pre-training.
arxivpreprint arxiv:2009.13682..bernd huber, daniel j. mcduff, chris brockett,michel galley, and bill dolan.
2018. emotional di-alogue generation using image-grounded languagemodels.
in proceedings of the 2018 chi conferenceon human factors in computing systems, chi 2018,montreal, qc, canada, april 21-26, 2018, page 277.acm..jeff johnson, matthijs douze, and herv´e j´egou.
2019.ieee.
billion-scale similarity search with gpus.
transactions on big data..byeongchang kim, jaewoo ahn, and gunhee kim.
2020. sequential latent knowledge selection forknowledge-grounded dialogue.
in 8th internationaliclrconference on learning representations,2020, addis ababa, ethiopia, april 26-30, 2020.openreview.net..diederik p. kingma and jimmy ba.
2015. adam: ain 3rd inter-method for stochastic optimization.
national conference on learning representations,iclr 2015, san diego, ca, usa, may 7-9, 2015,conference track proceedings..ranjay krishna, yuke zhu, oliver groth, justin john-son, kenji hata, joshua kravitz, stephanie chen,yannis kalantidis, li-jia li, david a shamma, et al.
2017. visual genome: connecting language and vi-sion using crowdsourced dense image annotations.
international journal of computer vision, 123(1):32–73..alina kuznetsova, hassan rom, neil alldrin, jasperuijlings, ivan krasin, jordi pont-tuset, shahabkamali, stefan popov, matteo malloci, alexanderkolesnikov, et al.
2018. the open images dataset v4:uniﬁed image classiﬁcation, object detection, andvisual relationship detection at scale.
arxiv preprintarxiv:1811.00982..jiwei li, michel galley, chris brockett, jianfeng gao,and bill dolan.
2016. a diversity-promoting ob-jective function for neural conversation models.
inproceedings of the 2016 conference of the northamerican chapter of the association for computa-tional linguistics: human language technologies,.
5605pages 110–119, san diego, california.
associationfor computational linguistics..linxiao li, can xu, wei wu, yufan zhao, xueliangzhao, and chongyang tao.
2020. zero-resourcein ad-knowledge-grounded dialogue generation.
vances in neural information processing systems33: annual conference on neural information pro-cessing systems 2020, neurips 2020, december 6-12, 2020, virtual..chin-yew lin.
2004. rouge: a package for auto-matic evaluation of summaries.
in text summariza-tion branches out, pages 74–81, barcelona, spain.
association for computational linguistics..tsung-yi lin, michael maire, serge belongie, jameshays, pietro perona, deva ramanan, piotr doll´ar,and c lawrence zitnick.
2014. microsoft coco:in european confer-common objects in context.
ence on computer vision, pages 740–755.
springer..jiasen lu, dhruv batra, devi parikh, and stefanlee.
2019. vilbert: pretraining task-agnostic visi-olinguistic representations for vision-and-languagetasks.
in advances in neural information process-ing systems 32: annual conference on neural infor-mation processing systems 2019, neurips 2019, de-cember 8-14, 2019, vancouver, bc, canada, pages13–23..jiasen lu, caiming xiong, devi parikh, and richardsocher.
2017. knowing when to look: adaptive at-tention via a visual sentinel for image captioning.
in2017 ieee conference on computer vision and pat-tern recognition, cvpr 2017, honolulu, hi, usa,july 21-26, 2017, pages 3242–3250.
ieee computersociety..nasrin mostafazadeh, chris brockett, bill dolan,michel galley, jianfeng gao, georgios spithourakis,and lucy vanderwende.
2017.image-groundedconversations: multimodal context for natural ques-in proceedings oftion and response generation.
the eighth international joint conference on natu-ral language processing (volume 1: long papers),pages 462–472, taipei, taiwan.
asian federation ofnatural language processing..stephen mussmann and stefano ermon.
2016. learn-ing and inference via maximum inner product search.
in proceedings of the 33nd international conferenceon machine learning, icml 2016, new york city,ny, usa, june 19-24, 2016, volume 48 of jmlrworkshop and conference proceedings, pages 2587–2596. jmlr.org..kishore papineni, salim roukos, todd ward, and wei-jing zhu.
2002. bleu: a method for automatic eval-in proceedings ofuation of machine translation.
the 40th annual meeting of the association for com-putational linguistics, pages 311–318, philadelphia,pennsylvania, usa.
association for computationallinguistics..tingting qiao, jing zhang, duanqing xu, and dachengtao.
2019. mirrorgan: learning text-to-image gen-in ieee conference oneration by redescription.
computer vision and pattern recognition, cvpr2019, long beach, ca, usa, june 16-20, 2019,pages 1505–1514.
computer vision foundation /ieee..alec radford, jeffrey wu, rewon child, david luan,dario amodei, and ilya sutskever.
2019. languagemodels are unsupervised multitask learners.
openaiblog, 1(8):9..shaoqing ren, kaiming he, ross b. girshick, andjian sun.
2015. faster r-cnn: towards real-timeobject detection with region proposal networks.
inadvances in neural information processing systems28: annual conference on neural information pro-cessing systems 2015, december 7-12, 2015, mon-treal, quebec, canada, pages 91–99..stephen roller, emily dinan, naman goyal, da ju,mary williamson, yinhan liu, jing xu, myle ott,kurt shuster, eric m smith, et al.
2020. recipesfor building an open-domain chatbot.
arxiv preprintarxiv:2004.13637..iulian vlad serban, alessandro sordoni, yoshua ben-gio, aaron c. courville, and joelle pineau.
2016.building end-to-end dialogue systems using gener-in pro-ative hierarchical neural network models.
ceedings of the thirtieth aaai conference on arti-ﬁcial intelligence, february 12-17, 2016, phoenix,arizona, usa, pages 3776–3784.
aaai press..iulian vlad serban, alessandro sordoni, ryan lowe,laurent charlin, joelle pineau, aaron c. courville,and yoshua bengio.
2017. a hierarchical latentvariable encoder-decoder model for generating di-in proceedings of the thirty-first aaaialogues.
conference on artiﬁcial intelligence, february 4-9,2017, san francisco, california, usa, pages 3295–3301. aaai press..lifeng shang, zhengdong lu, and hang li.
2015. neu-ral responding machine for short-text conversation.
in proceedings of the 53rd annual meeting of theassociation for computational linguistics and the7th international joint conference on natural lan-guage processing (volume 1: long papers), pages1577–1586, beijing, china.
association for compu-tational linguistics..kurt shuster, samuel humeau, antoine bordes, and ja-son weston.
2020. image-chat: engaging groundedin proceedings of the 58th annualconversations.
meeting of the association for computational lin-guistics, pages 2414–2429, online.
association forcomputational linguistics..alessandro sordoni, michel galley, michael auli,chris brockett, yangfeng ji, margaret mitchell,jian-yun nie, jianfeng gao, and bill dolan.
2015.a neural network approach to context-sensitive gen-eration of conversational responses.
in proceedings.
5606of the 2015 conference of the north american chap-ter of the association for computational linguis-tics: human language technologies, pages 196–205, denver, colorado.
association for computa-tional linguistics..russell stewart, mykhaylo andriluka, and andrew y.ng.
2016. end-to-end people detection in crowdedscenes.
in 2016 ieee conference on computer vi-sion and pattern recognition, cvpr 2016, las ve-gas, nv, usa, june 27-30, 2016, pages 2325–2333.
ieee computer society..ilya sutskever, oriol vinyals, and quoc v. le.
2014.sequence to sequence learning with neural networks.
in advances in neural information processing sys-tems 27: annual conference on neural informa-tion processing systems 2014, december 8-13 2014,montreal, quebec, canada, pages 3104–3112..hao tan and mohit bansal.
2020. vokenization: im-proving language understanding via contextualized,in proceedings ofvisually-grounded supervision.
the 2020 conference on empirical methods in nat-ural language processing (emnlp), pages 2066–2080..ashish vaswani, noam shazeer, niki parmar, jakobuszkoreit, llion jones, aidan n. gomez, lukaszkaiser, and illia polosukhin.
2017. attention is allyou need.
in advances in neural information pro-cessing systems 30: annual conference on neuralinformation processing systems 2017, december 4-9, 2017, long beach, ca, usa, pages 5998–6008..oriol vinyals and quoc le.
2015. a neural conversa-tional model.
arxiv preprint arxiv:1506.05869..oriol vinyals, alexander toshev, samy bengio, anddumitru erhan.
2015. show and tell: a neural im-age caption generator.
in ieee conference on com-puter vision and pattern recognition, cvpr 2015,boston, ma, usa, june 7-12, 2015, pages 3156–3164. ieee computer society..yu wu, wei wu, dejian yang, can xu, and zhoujunli.
2018. neural response generation with dynamicin proceedings of the thirty-secondvocabularies.
aaai conference on artiﬁcial intelligence, (aaai-18), the 30th innovative applications of artiﬁcial in-telligence (iaai-18), and the 8th aaai symposiumon educational advances in artiﬁcial intelligence(eaai-18), new orleans, louisiana, usa, february2-7, 2018, pages 5594–5601.
aaai press..saining xie, ross b. girshick, piotr doll´ar, zhuowentu, and kaiming he.
2017. aggregated residualtransformations for deep neural networks.
in 2017ieee conference on computer vision and patternrecognition, cvpr 2017, honolulu, hi, usa, july21-26, 2017, pages 5987–5995.
ieee computer so-ciety..chen xing, wei wu, yu wu, jie liu, yalou huang,ming zhou, and wei-ying ma.
2017. topic aware.
in proceedings of theneural response generation.
thirty-first aaai conference on artiﬁcial intelli-gence, february 4-9, 2017, san francisco, califor-nia, usa, pages 3351–3357.
aaai press..can xu, wei wu, chongyang tao, huang hu, mattschuerman, and ying wang.
2019. neural responsegeneration with meta-words.
in proceedings of the57th annual meeting of the association for com-putational linguistics, pages 5416–5426, florence,italy.
association for computational linguistics..tao xu, pengchuan zhang, qiuyuan huang, hanzhang, zhe gan, xiaolei huang, and xiaodong he.
2018. attngan: fine-grained text to image genera-tion with attentional generative adversarial networks.
in 2018 ieee conference on computer vision andpattern recognition, cvpr 2018, salt lake city, ut,usa, june 18-22, 2018, pages 1316–1324.
ieeecomputer society..ze yang, wei wu, huang hu, can xu, and zhoujun li.
2020. open domain dialogue generation with latentimages.
arxiv preprint arxiv:2004.01981..hainan zhang, yanyan lan, liang pang, jiafeng guo,and xueqi cheng.
2019. recosa: detecting the rel-evant contexts with self-attention for multi-turn di-in proceedings of the 57th an-alogue generation.
nual meeting of the association for computationallinguistics, pages 3721–3730, florence, italy.
asso-ciation for computational linguistics..yizhe zhang, siqi sun, michel galley, yen-chun chen,chris brockett, xiang gao, jianfeng gao, jingjingliu, and bill dolan.
2020. dialogpt : large-scale generative pre-training for conversational re-in proceedings of the 58th an-sponse generation.
nual meeting of the association for computationallinguistics: system demonstrations, pages 270–278, online.
association for computational linguis-tics..xueliang zhao, wei wu, chongyang tao, can xu,dongyan zhao, and rui yan.
2020a.
low-resourcein 8thknowledge-grounded dialogue generation.
international conference on learning representa-tions, iclr 2020, addis ababa, ethiopia, april 26-30, 2020. openreview.net..xueliang zhao, wei wu, can xu, chongyang tao,dongyan zhao, and rui yan.
2020b.
knowledge-grounded dialogue generation with pre-trained lan-in proceedings of the 2020 con-guage models.
ference on empirical methods in natural languageprocessing (emnlp), pages 3377–3390, online.
as-sociation for computational linguistics..hao zhou, tom young, minlie huang, haizhou zhao,jingfang xu, and xiaoyan zhu.
2018.com-monsense knowledge aware conversation generationwith graph attention.
in proceedings of the twenty-seventh international joint conference on artiﬁcialintelligence, ijcai 2018, july 13-19, 2018, stock-holm, sweden, pages 4623–4629.
ijcai.org..5607a appendices.
in this section, we show more examples of wordco-occurrence distributions on google knowledgegraph and ms-coco images.
besides, some con-versation samples produced by maria and the base-lines are also presented in section a.2..a.1 word co-occurrence distribution.
examples.
in figure 6, we present some supplementary ex-amples of the word co-occurrence distribution ongoogle knowledge graph and ms-coco images,including “trafﬁc light”, “bed”, “book”, and “potplant”.
figure 6 (a) shows the co-occurrence distri-butions of “trafﬁc light” and other words on knowl-edge graph and images, respectively.
as we can see,most of the co-occurred words with “trafﬁc light”are the related concepts such as “smart trafﬁc light”,“trafﬁc light protocol”, “trafﬁc light rating system”,etc.
while the co-occurred words on images areusually “car”, “person”, “truck”, “bus”, etc, whichwe often see when walking by the trafﬁc lights.
in-terestingly, we found “umbrella” and “clock” alsoco-occurs with “trafﬁc light” in some images.
forthe former, the picture we can imagine is that peo-ple were holding the “umbrellas” when they walkedthrough a zebra crossing under the “trafﬁc light”.
for the latter, the possible picture is that we cansee both the “trafﬁc light” and the “clock” on thetop of a high building from a certain angle whenwalking on the street.
similar observations can bealso seen in other examples..most of the co-occurrence words on knowledgegraph are logically-related concepts.
however, theco-occurrence relationship of object tags on imagesreﬂects some commonsense of our physical world,which implies some pictures that we human couldeasily imagine.
this kind of knowledge is uniqueand inherent in images, but it can hardly be cap-tured in the traditional knowledge bases, such asknowledge graph..a.2 case analysis.
figure 7 shows some cases from the test set of red-dit data.
we observe that the responses generatedby maria are more commonsensical and vivid thanthose of the baseline methods, which is consistentwith our automatic and human evaluation results.
interestingly, maria is able to retrieve correlatedimages using the dialog contexts, which makes itsresponse more human-like.
for instance, case (a)shows that when the dialog context marvels at “thepass of the world cup”, maria recalls a footballplayer and compliments him “the best player in theworld”; case (b) shows that when the dialog contextchats about the “canada weather”, maria is awareof the fact that “canada” is often “snowy” and thentalks about “canada” in a funny tone, “i’ve neverbeen to a place that doesn’t have snow”; case (c)shows that maria understands that “swan” is some-times “dangerous” when they are on the “beach”;case (d) shows that when the dialog context tries toguess one type of game, maria recalls a ping-pong“ball” game and describes it; and etc..5608(a) traffic lightfigure 6: supplementary examples of the word co-occurrence distribution on google knowledge graph and ms-coco images..5609(b) bed(d) pot plant(c) book5610dialog context:mariaa:thepassoftheworldcup(jamesrodriguez)b: absoleutelygorgeous.imgvae: ilovetheworldcup.dialogpt:it 's not a pass of the world cup.baseline response:detected concepts:ball, socks, field, shirt, grass, man, shorts, player, sockresponse: he'sthebestplayerintheworlddialog context:mariaa:howisaswangonnabreakyourarm?b:smackyouwiththeirwingsimgvae: i'm a little nervous about this.dialogpt: i'm a swan.baseline response:detected concepts:duck,picture,background,land,beach,border,line,ground,bird,sky,water,birds,frameresponse: i'mnotsureifyou'rejokingornot,buti'msurethey'regoingtobeabitmoredangerousifthey areonthebeachdialog context:mariaa: you'vejustruinedmycanadaweatherfetish,thatitwouldbeidealplaceformetolive:freezingcoldwintersinmildsummers.b: ya,therearepartsofcanadawithverynicemildsummers,butthosesameplaceshavemildwinterstoo.imgvae: i'm from canada, but i've never seen a cold winters wintersin cold winters winters.dialogpt:i'm from the u.s.and i've never heard of this.baseline response:detected concepts:person, fence, ground, tree, building, snow, border, pole, gateresponse: i'mfromcanadaandi'veneverbeentoaplacethatdoesn'thavesnow(a)(b)(c)figure 7: case study on the reddit data from test split..5611dialog context:mariaa: we all played this gameat one point.b: what game is this ?imgvae: it 's a game gamegame, game of thrones.dialogpt:it 's a memebaseline response:detected concepts:shorts, background, heart, skateboard, woman, ball, bench, counter, shirt, hair, wall, net, stands, peopleresponse: it's a game where you play the ballwith your handsdialog context:mariaa: what 's the coldestyou 've ever been?b:almost got hypothermia near the arctic circle because my rain jacket failed to be waterproofand all my layers were soaked.c:what kind of jacket?imgvae: the one where you ski and the jacket?dialogpt: it 's a winter jacket.baseline response:detected concepts:pants, sky, snow, person, cloudsresponse: it'sasnowjacketdialog context:mariaa: what cool gadget under $200 can be life changing?b:a smartphone.imgvae: ihave a smartphone and i'm a phone.dialogpt:that 's not cool.baseline response:detected concepts: button, hand, wall, line, hair, phone, table, fingers, handle, thumbresponse: ialready have a phone.
(d)(e)(f)