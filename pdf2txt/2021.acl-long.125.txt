topic-driven and knowledge-aware transformer for dialogue emotiondetection.
lixing zhu†, gabriele pergola†, lin gui†, deyu zhou§, yulan he††department of computer science, university of warwick, uk§school of computer science and engineering, key laboratory of computer networkand information integration, ministry of education, southeast university, china{lixing.zhu,gabriele.pergola,lin.gui,yulan.he}@warwick.ac.ukd.zhou@seu.edu.cn.
abstract.
emotion detection in dialogues is challengingas it often requires the identiﬁcation of the-matic topics underlying a conversation, the rel-evant commonsense knowledge, and the intri-cate transition patterns between the affectivestates.
in this paper, we propose a topic-driven knowledge-aware transformer to han-dle the challenges above.
we ﬁrstly design atopic-augmented language model (lm) withan additional layer specialized for topic detec-tion.
the topic-augmented lm is then com-bined with commonsense statements derivedfrom a knowledge base based on the dialoguecontextual information.
finally, a transformer-based encoder-decoder architecture fuses thetopical and commonsense information, andperforms the emotion label sequence predic-tion.
the model has been experimented onfour datasets in dialogue emotion detection,demonstrating its superiority empirically overthe existing state-of-the-art approaches.
quan-titative and qualitative results show that themodel can discover topics which help in dis-tinguishing emotion categories..figure 1: utterances around particular topics carry spe-ciﬁc emotions.
utterances carrying positive (smilingface) or negative (crying face) emotions are highlightedin colour.
other utterances are labeled as ‘neutral’.
in(a), utterances discussing food and restaurant are morelikely carrying positive sentiment.
in (b), the similarutterance, ‘he was doing so well’, expressed differentemotions depending on its associated topic..1.introduction.
the abundance in dialogues extracted from on-line conversations and tv series provides unprece-dented opportunity to train models for automaticemotion detection, which are important for the de-velopment of empathetic conversational agents orchat bots for psychotherapy (hsu and ku, 2018;jiao et al., 2019; zhang et al., 2019; cao et al.,2019).
however, it is challenging to capture thecontextual semantics of personal experience de-scribed in one’s utterance.
for example, the emo-tion of the sentence “i just passed the exam” can beeither happy or sad depending on the expectationof the subject.
there are strands of works utilizingthe dialogue context to enhance the utterance rep-resentation (jiao et al., 2019; zhang et al., 2019;.
majumder et al., 2019), where inﬂuences from his-torical utterances were handled by recurrent units,and attention signals were further introduced tointensify the positional order of the utterances..despite the progress made by the aforemen-tioned methods, detecting emotions in dialoguesis however still a challenging task due to the wayemotions are expressed and how the meanings ofutterances vary based on the particular topic dis-cussed, as well as the implicit knowledge sharedbetween participants.
figure 1 gives an exampleof how topics and background knowledge couldimpact the mood of interlocutors.
normally, dia-logues around speciﬁc topics carry certain languagepatterns (serban et al., 2017), affecting not only theutterance’s meaning, but also the particular emo-.
proceedingsofthe59thannualmeetingoftheassociationforcomputationallinguisticsandthe11thinternationaljointconferenceonnaturallanguageprocessing,pages1571–1582august1–6,2021.©2021associationforcomputationallinguistics1571(a): food and restaurant(b): marriage and deatha: could i have some fish?b: certainly.
and what vegetables would you like?a: oh , spinach , i think.a: i like drinking tea at teahouses.b: oh, so do i.☺b: great.
we can chat while enjoying a cup there.a: why don't we go for one now?☺a: let's go!☺☺a: johnny died yesterday, we knew that it was coming, but...b: like just last week, he was doing so well.☹a: then all of a sudden they gave him a microphone,      he asked me to marry him, like, onstage.b: he was doing so well.☹☺☺tions conveyed by speciﬁc expressions.
existingdialogue emotion detection methods did not putemphasis on modelling these holistic properties ofdialogues (i.e., conversational topics and tones).
consequently, they were fundamentally limited incapturing the affective states of interlocutors re-lated to the particular themes discussed.
besides,emotion and topic detection heavily relies on lever-aging underlying commonsense knowledge sharedbetween interlocutors.
although there have beenattempts in incorporating it, such as the cosmic(ghosal et al., 2020), existing approaches do notperform ﬁne-grained extraction of relevant infor-mation based on both the topics and the emotionsinvolved..recently, the transformer architecture (vaswaniet al., 2017) has empowered language models totransfer large quantities of data to low-resourcedomains, making it viable to discover topics inin this paper, we proposeconversational texts.
to add an extra layer to the pre-trained languagemodel to model the latent topics, which is learnedby ﬁne-tuning on dialogue datasets to alleviate thedata sparsity problem.
inspired by the success oftransformers, we use the transformer encoder-decoder structure to perform the seq2seq predic-tion in which an emotion label sequence is pre-dicted given an utterance sequence (i.e., each utter-ance is assigned with an emotion label).
we positthat the dialogue emotion of the current utterancedepends on the historical dialogue context and thepredicted emotion label sequence for the past utter-ances.
we leverage the attention mechanism andthe gating mechanism to incorporate commonsenseknowledge retrieved by different approaches.
codeand trained models are released to facilitate furtherresearch1.
to sum up, our contributions are:.
• we are the ﬁrst to propose a topic-driven ap-proach for dialogue emotion detection.
wepropose to alleviate the low-resource settingby topic-driven ﬁne-tuning using pre-trainedlanguage models..• we utilize a pointer network and an additive at-tention to integrate commonsense knowledgefrom multiple sources and dimensions..• we develop a transformer encoder-decoderstructure as a replacement of the commonly-used recurrent attention neural networks fordialogue emotion detection..1http://github.com/something678/todkat..2 related work.
dialogue emotion detection majumder et al.
(2019) recognized the importance of dialogue con-text in dialogue emotion detection.
they used agated recurrent unit (gru) to capture the globalcontext which is updated by the speaker ad-hocgrus.
at the same time, jiao et al.
(2019) pre-sented a hierarchical neural network model thatcomprises two grus for the modelling of tokensand utterances respectively.
zhang et al.
(2019)explicitly modelled the emotional dependencieson context and speakers using a graph convolu-tional network (gcn).
meanwhile, ghosal et al.
(2019) extended the prior work (majumder et al.,2019) by taking into account the intra-speaker de-pendency and relative position of the target andcontext within dialogues.
memory networks havebeen explored in (jiao et al., 2020) to allow bidi-rectional inﬂuence between utterances.
a similaridea has been explored by li et al.
(2020b).
whilethe majority of works have been focusing on tex-tual conversations, zhong et al.
(2019) enrichedutterances with concept representations extractedfrom the conceptnet (speer et al., 2017).
ghosalet al.
(2020) developed cosmic which exploitedatomic (sap et al., 2019) for the acquisition ofcommonsense knowledge.
different from exist-ing approaches, we propose a topic-driven andknowledge-aware model built on a transformerencoder-decoder structure for dialogue emotiondetection..latent variable models for dialogue contextmodelling latent variable models, normally de-scribed in their neural variational inference formnamed variational autoencoder (vae) (kingmaand welling, 2014), has been studied extensivelyto learn thematic representations of individual doc-uments (miao et al., 2016; srivastava and sutton,2017; rezaee and ferraro, 2020).
they have beensuccessfully employed for dialogue generation tomodel thematic characteristics over dynamicallyevolving conversations.
this line of work, whichinlcudes approaches based on hierarchical recurrentvaes (serban et al., 2017; park et al., 2018; zenget al., 2019) and conditional vaes (sohn et al.,2015; shen et al., 2018; gao et al., 2019), encodeseach utterance with historical latent codes and au-toregressively reconstructs the input sequence..on the other hand, pre-trained language modelsare used as embedding inputs to vae-based mod-.
1572els (peinelt et al., 2020; asgari-chenaghlu et al.,2020).
recent work by li et al.
(2020a) employsbert and gpt-2 as the encoder-decoder structureof vae.
however, these models have to be eithertrained from scratch or built upon pre-trained em-beddings.
they therefore cannot be directly appliedto the low-resource setting of dialogue emotion de-tection..knowledge base and knowledge retrievalconceptnet (speer et al., 2017) captures common-sense concepts and relations as a semantic network,which encompasses the spatial, physical, social,temporal, and psychological aspects of everydaylife.
more recently, sap et al.
(2019) built atomic,a knowledge graph centered on events rather thanentities.
owing to the expressiveness of events andameliorated relation types, using atomic achievedcompetitive results against human evaluation in thetask of if-then reasoning..alongside the development of knowledge bases,recent years have witnessed the thrive of new meth-ods for training language models from large-scaletext corpora as implicit knowledge base.
as hasbeen shown in (petroni et al., 2019), pre-trainedlanguage models perform well in recalling rela-tional knowledge involving triplet relations aboutentities.
bosselut et al.
(2019) proposed com-monsense transformers (comet) which learns togenerate commonsense descriptions in natural lan-guage by ﬁne-tuning pre-trained language modelson existing commonsense knowledge bases suchas atomic.
compared with extractive methods,language models ﬁne-tuned on knowledge baseshave a distinctive advantage of being able to gener-ate knowledge for unseen events, which is of greatimportance for tasks which require the incorpora-tion of commonsense knowledge such as emotiondetection in dialogues..3 methodology.
3.1 problem setup.
a dialogue is deﬁned as a sequence of utterances{x1, x2, .
.
.
, xn }, which is annotated with a se-quence of emotion labels {y1, y2, .
.
.
, yn }.
ourgoal is to develop a model that can assign thecorrect label to each utterance.
as for each ut-terance, the raw input is a token sequence, i.e.,xn = {wn,1, wn,2, .
.
.
, wn,mn} where mn denotesthe length of an utterance.
we address this prob-lem using the seq2seq framework (sutskever et al.,.
2014), in which the model consecutively consumesan utterance xn and predicts the emotion label ynbased on the earlier utterances and their associatedpredicted emotion labels.
the joint probability ofemotion labels for a dialogue is:.
pθ(y1:n |x1:n ) =.
pθ(yn|x≤n, y<n).
(1).
n(cid:89).
n=1.
it is worth mentioning that the subsequent utter-ances are unseen to the model at each predictivestep.
learning is performed via optimizing thelog-likelihoods of predicted emotion labels..the overall architecture of our proposedtopic-driven and knowledge-aware transformer(todkat) is shown in figure 2, which consists oftwo main components, the topic-driven languagemodel ﬁne tuned on dialogues, and the knowledge-aware transformer for emotion label sequence pre-diction for a given dialogue.
in what follows, wewill describe each of the components in turn..3.2 topic representation learning.
we propose to insert a topic layer into an existinglanguage model and ﬁne-tune the pre-trained lan-guage model on the conversational text for topicrepresentation learning.
topic models, often for-mulated as latent variable models, play a vital rolein dialogue modeling (serban et al., 2017) due tothe explicit modeling of ‘high-level syntactic fea-tures such as style and topic’ (bowman et al., 2016).
despite the tremendous success of applying topicmodeling in dialogue generation (sohn et al., 2015;shen et al., 2018; gao et al., 2019), there is scarcework exploiting latent variable models for dialogueemotion detection.
to this end, we borrow the ar-chitecture from vhred (serban et al., 2017) fortopic discovery, with the key modiﬁcation that boththe encoder rnn and decoder rnn are replacedby layers of a pre-trained language model.
further-more, we use a transformer multi-head attention inreplacement of the lstm to model the dependencebetween the latent topic vectors.
unlike vhred,we are interested in the encoder part to extract theposterior of the latent topic z, rather than the recur-rent prior of z in the decoder part since the latteris intended for dialogue generation.
we assumethat each utterance is mapped to a latent variableencoding its internal topic, and impose a sequen-tial dependence on the topic transitions.
figure 2agives an overview of the vae-based model which.
1573(a) topic-driven ﬁne-tuning of a pre-trained lm..(b) knowledge-aware transformer..figure 2: topic-driven and knowledge-aware transformer (todkat)..aims at learning the latent topic vector during theﬁne-tuning of the language model..speciﬁcally, the pre-trained language model isdecomposed into two parts, the encoder and thedecoder.
by retaining the pre-trained weights, wetransfer representations from high-resource tasksto the low-resource setting, which is the case fordialogue emotion datasets..encoder the training of topic discovery part oftodkat comprises a vae at each time step, withits latent variable dependent on the previous latentcode.
each utterance is input to the vae encoderwith a recurrent hidden state, the output of which isa latent vector ideally encoding the topic discussedin the utterance.
the latent vectors are tied througha recurrent hidden state to constraint a coherenttopic over a single dialogue.
we use lmφ to de-note the network of lower layers of the languagemodel (before the topic layer) and xln to denotethe output from lmφ given the input xn.
the vari-ational distribution for the approximation of theposterior will be:.
qφ(zn|x≤n, z<n)= n (cid:0)zn|fµφ (xlwhere hn−1 = fτ (zn−1, xl.
n , hn−1), fσφ (xl.
n , hn−1)(cid:1),n−1), for n > 1..(2).
(3).
here, fµφ(·) and fσφ(·) are multi-layer percep-trons (mlps), fτ can be any transition function(e.g., a recurrent unit).
we employ the transformermulti-head attention with its query being the previ-ous latent variable zn−1, that is,.
decoder the decoder network reconstructs xnfrom zn at each time step.
we use gaus-sian distributions for both the generative priorand the variational distribution.
since we wantzn to be dependent on zn−1, the prior for znis p(zn|hn−1) = n (cid:0)zn|fµγ (hn−1), fσγ (hn−1)(cid:1).
where fµγ (·) and fσγ (·) are mlps.
the posteriorfor zn is pθ(zn|x≤n, z<n), which is intractable andis approximated by qφ(zn|x≤n, z<n) of eq.
2. wedenote the higher layers of the language model aslmθ.
then the reconstruction of ˆxn given zn andxln can be expressed as:.
ˆxn = lmθ(zn, xl.
n )..(5).
note that this is different from dialogue generationin which an utterance is generated from the latenttopic vector.
here, we aim to extract the latent topicfrom the current utterance and therefore train themodel to reconstruct the input utterance as speciﬁedin eq.
(5).
to make the combination of zn andxln compatible for lmθ, we need to perform thelatent vector injection.
as in (li et al., 2020a), weemploy the “memory” scheme that zn becomes anadditional input for lmθ, that is, the input to thehigher layers becomes [zn, xln ]..training the training objective is the evidencelower bound (elbo):.
eqφ(z≤n |x≤n )[log pθ(x≤n |z≤n )]−kl[qφ(z≤n |x≤n )||p(z≤n )]..(6).
fτ (zn−1, xl.
n−1) = attention(zn−1, xl.
n−1, xl.
n−1)..(4).
eq.
6 factorizes and the expectation term becomes.
we initialize h0 = 0 and model the transitionbetween hn−1 and hn by ﬁrst generating zn fromhn−1 using eq.
(2), then calculating hn by eq.
(3)..eqφ(z≤n |x≤n ).
log pθ(xn|z≤n, x<n).
, (7).
(cid:35).
(cid:34) n(cid:88).
n=1.
1574nth utterance...znencoder (lmφ)...nth utterance with masksn+1th utterance...zn+1...n+1th utterance   with masksdecoder (lmθ)latent vectoroutputinputtopic-driven fine-tuningtransformer encoder-decoder classifier...znh[cls]nth utteranceun{u0, u1, ..., un-1}...zih[cls]1:n-1 utterancestransformer encodertransformer decoderynonqvkvkcicncandidate eventsknowledge graphsbertcometattentionpointer networkevent: personx has to to go to workxintent: to get a raisexreact: be tiredoreact: be worriedand the kl term becomes.
n(cid:88).
n=1.
kl[qφ(zn|x≤n, z<n)||p(zn|z<n, x<n)], (8).
where p(zn|z<n, x<n) is the prior for zn.
aftertraining, we are able to extract the topic represen-tation from the encoder part of the model, whichis denoted as zn = lmencφ (xn).
meanwhile, theentire language model has been ﬁne-tuned, whichis denoted as un = lmcls(xn)..3.3 knowledge-aware transformer.
the topic-driven lm ﬁne-tuning stage makes itpossible for the lm to discover a topic represen-tation from a given utterance.
after ﬁne-tuning,we attach the ﬁne-tuned components to a classiﬁerand train the classiﬁer to predict the emotion la-bels.
we propose to use the transformer encoder-decoder structure as the classiﬁer, and considerthe incorporation of commonsense knowledge re-trieved from external knowledge sources.
in whatfollows, we ﬁrst describe how to retrieve the com-monsense knowledge from a knowledge source,then we present the detailed structure of the classi-ﬁer..commonsense knowledge retrieval we useatomic2 as a source of external knowledge.
inatomic, each node is a phrase describing an event.
edges are relation types linking from one eventto another.
atomic thus encodes triples such as(cid:104)event, relation type, event(cid:105).
thereare a total of nine relation types, of which threeare used: xintent, the intention of the subject(e.g., ‘to get a raise’), xreact, the reaction of thesubject (e.g., ‘be tired’), and oreact, the reactionof the object (e.g., ‘be worried’), since they aredeﬁned as the mental states of an event (sap et al.,2019)..given an utterance xn, we can compare itwith every node in the knowledge graph, and re-trieve the most similar one.
the method for com-puting the similarity between an utterance andevents is sbert (reimers and gurevych, 2019).
we extract the top-k events, and obtain theirintentions and reactions, which are denoted as{esi.
n,k, esron the other hand, there is a knowledge gen-.
n,k}, k = 1, .
.
.
, k..n,k, eor.
2https://homes.cs.washington.edu/.
˜msap/atomic/.
eration model, called comet3, which is trainedit can take xn as input and gen-on atomic.
erate the knowledge with the desired event rela-tion types speciﬁed (e.g., xintent, xreact ororeact).
the generated knowledge can be un-seen in atomic since comet is essentially a ﬁne-tuned language model.
we use comet to generatethe k most likely events, each with respect to thethree event relation types.
the produced events aredenoted as {gsi.
n,k}, k = 1, .
.
.
, k..n,k, gor.
n,k, gsr.
knowledge selection with the knowledge re-trieved from atomic, we build a pointer net-work (vinyals et al., 2015) to exclusively choosethe commonsense knowledge either from sbertor comet.
the pointer network calculates theprobability of choosing the candidate knowledgesource as:.
p (cid:0)i(xn, en, gn) = 1(cid:1) = σ(cid:0)[xn, en, gn]wσ.
(cid:1),.
where i(xn, en, gn) is an indicator function withvalue 1 or 0, and σ(x) = 1/(1 + exp(−x)).
we en-velope σ with gumbel softmax (jang et al., 2017)to generate the one-hot distribution4.
the inte-grated commonsense knowledge is expressed as.
cn = i(xn, en, gn)en + (cid:0)1 − i(xn, en, gn)(cid:1)gn,.
k=1..n,k, csr.
n,k, cor.
where cn = {csi.
n,k}kwith the knowledge source selected, we pro-ceed to select the most informative knowledge.
wedesign an attention mechanism (bahdanau et al.,2015) to integrate the candidate knowledge.
recallthat we have a ﬁne-tuned language model whichcan calculate both the [cls] and topic representa-tions.
here we apply the language model to theretrieved or generated knowledge to obtain the[cls] and the topic representation, denoted as[cn,k, zn,k].
the attention mechanism is performedby calculating the dot product between the utter-.
3https://github.com/atcbosselut/.
comet-commonsense.
4we have also experimented with a soft gating mechanismby aggregating knowledge from sbert and comet in aweighted manner.
but the results are consistently worse thanthose using a hard gating mechanism..1575ance and each normalized knowledge tuple:.
vk = tanh(cid:0)[cn,k, zn,k]wαexp(cid:0)vk[zn, un](cid:62)(cid:1)k exp(cid:0)vk[zn, un](cid:62)(cid:1) ,.
αk =.
(cid:1),.
(cid:80).
cn =.
αkcn,k..k(cid:88).
k=1.
(9).
(10).
(11).
here, we abuse cn to represent the aggregatedknowledge phrases.
we further aggregate cn byevent relation types using a self-attention and theﬁnal event representation is denoted as cn..transformer encoder-decoder we useatransformer encoder-decoder to map an utterancethussequence to an emotion label sequence,allowing for modeling the transitional patternsbetween emotions and taking into account thehistorical utterances as well.
each utterance is con-verted to the [cls] representation concatenatedwith the topic representation zn and knowledgerepresentation cn.
we enforce a masking schemein the self-attention layer of the encoder to makethe classiﬁer predict emotions in an auto-regressiveway, entailing that only the past utterances arevisible to the encoder.
this masking strategy,preventing the query from attending to future keys,suits better a real-world scenario in which thesubsequent utterances are unseen when predictingan emotion of the current utterance.
as forthe decoder, the output of the previous decoderblock is input as a query to the self-attention layer.
the training loss for the classiﬁer is the negativelog-likelihood expressed as:.
l = −.
log pθ(yn|u≤n, y<n),.
n(cid:88).
n=1.
where θ denotes the trainable parameters..4 experimental setup.
in this section, we present the details of the datasetsused, the methods for comparison, and the imple-mentation details of our models..datasets we use the following datasets for ex-perimental evaluation:dailydialog (li et al., 2017) is collected from dailycommunications.
it takes the ekman’s six emotiontypes (ekman, 1993) as the annotation protocol,that is, it annotates an utterance with one of thesix basic emotions: anger, disgust, fear, happiness,.
sadness, or surprise.
those showing ambiguousemotions are annotated as neutral.
meld (poria et al., 2019) is constructed fromscripts of ‘friends’, a tv series on urban life.
sameas dailydialog, the emotion label falls into ek-man’s six emotion types, or neutral.
iemocap (busso et al., 2008) is built with subti-tles from improvised videos.
its emotion labels arehappy, sad, neutral, angry, excited and frustrated.
emorynlp (zahiri and choi, 2018)5 is also builtwith conversations from ‘friends’ tv series, butwith a slightly different annotation scheme in whichdisgust, anger and surprise become peaceful, madand powerful, respectively..following zhong et al.
(2019) and ghosal et al.
(2020), the ‘neutral’ label of dailydialog is notcounted in the evaluation to avoid highly imbal-anced classes.
for meld and emorynlp, weconsider a dialogue as a sequence of utterancesfrom the same scene id.
table 1 summarizes thestatistics of each dataset..#dial.
traindev.
test#utt.
traindev.
test#cat..dd13,11811,1181,0001,000102,97987,1708,0697,7407.meld iemocap1,4321,03811428013,7089,9891,1092,6107.
15110020317,3334,8101,0001,5236.emorynlp82765989799,4897,5519549847.table 1: statistics of the benchmarks for dialogue emo-tion detection.
the train/development/test sets are pre-deﬁned in each dataset..baselines we compare the performance of tod-kat with the following methods:higru (jiao et al., 2019) simply inherits the re-current attention framework that an attention layeris placed between two grus to aggregate the sig-nals from the encoder gru and pass them to thedecoder gru.
dialoguegcn (ghosal et al., 2019) creates a graphfrom interactions of speakers to take into accountthe dialogue structure.
a graph convolutional net-work (gcn) is employed to encode the speakers.
emotion labels are predicted with the combinationsof the global context and speakers’ status..5https://github.com/emorynlp/.
emotion-detection.
1576dailydialog.
meld.
iemocap.
emorynlp.
micro-f1.
micro-f1.
micro-f1.
models.
macro-f1- neutralhigru0.4904dialoguegcn 0.4995ketcosmictodkat.
–0.51050.52560.51360.50030.51730.5102.
−topics−kbkatsbertkatcomet.
micro-f1- neutral0.51900.53730.53480.58480.58470.55490.53440.55780.5462.weightedavg-f10.56810.58370.58180.65210.68230.66340.63970.64540.6582.
0.54520.5617––0.64750.63520.61110.61880.6307.weightedavg-f10.58540.60850.59560.6528*0.61330.62810.58960.60970.6277.
0.58280.6063––0.61110.62600.57380.60690.6254.weightedavg-f10.34480.34290.34390.38110.43120.41800.33790.37340.4110.
0.33540.3313––0.42680.40550.32620.35670.3974.table 2: the f1 results of the dialogue emotion detectors on four benchmarks.
here we denote the proposed modelas todkat, of which the results are an average of ten runs.
the ablations of different components are reportedseparately in the bottom, where the model without the incorporation of latent topics is denoted as ‘−topics’,transformer encoder-decoder structure without the use of a knowledge base is dnoted as ‘−kb’.
katcomet andkatsbert uses the commonsense knowledge obtained with comet and sbert, respectively.
results of ket andcosmic are from (zhong et al., 2019) and (ghosal et al., 2020), respectively..ket (zhong et al., 2019) is the ﬁrst modelwhich integrates common-sense knowledge ex-tracted from conceptnet and emotion informationfrom an emotion lexicon into conversational text.
a transformer encoder is employed to handle theinﬂuence from past utterances.
cosmic (ghosal et al., 2020) is the state-of-the-art approach that leverages atomic for improvedemotion detection.
comet is employed in theirmodel to retrieve the event-eccentric commonsenseknowledge from atomic..we modiﬁed the script6 of language model ﬁne-tuning in the hugging face library (wolf et al.,2020) for the implementation of topic-driven ﬁne-tuning.
we use one transformer encoder layer.
asfor the decoder, there are n layers where n is thenumber of utterances in a dialogue.
we refer thereaders to the appendix for the detailed settings ofthe proposed models..5 results and analysis.
comparison with baselines experiment resultsof todkat and its ablations are reported in ta-ble 2. higru and dialoguegcn results wereproduced by running the code published by theauthors on the four datasets.
among the baselines,cosmic gives the best results.
our proposedtodkat outperforms cosmic on both meldand emorynlp in weighted avg-f1 with the im-provements ranging between 3-5%.
todkat alsoachieves superior result than cosmic on dailydi-.
6https://huggingface.co/transformers/.
v2.0.0/examples.html.
alogue in macro-f1 and gives nearly the same re-sult in micro-f1.
todkat is inferior to cosmicon iemocap.
it is however worth mentioning thatcosmic was trained with 132 instances on thisdataset, while for all the other models the training-and-validation split is 100 and 20. as such, theiemocap results reported on cosmic (ghosalet al., 2020) are not directly comparable here.
cos-mic also incorporates the commonsense knowl-edge from atomic but with the modiﬁed grus.
our proposed todkat, built upon the topic-driventransformer, appears to be a more effective archite-cure for dialogue emotion detection.
comparedwith ket, the improvements are much more sig-niﬁcant, with over 10% increase on meld, andclose to 5% gain on dailydialog.
ket is also builton the transformer, but it considers each utterancein isolation and applies commonsense knowledgefrom conceptnet.
todkat, on the contrary, takesinto account the dependency of previous utterancesand their associated emotion labels for the predic-tion of the emotion label of the current utterance.
dialoguegcn models interactions of speakers andit performs slightly better than ket.
but it is signif-icantly worse than todkat.
it seems that topicsmight be more useful in capturing the dialoguecontext..ablation study the lower half of table 2presents the f1 scores with the removal of vari-ous components from todkat.
it can be observedthat with the removal of the topic component, theperformance of todkat drops consistently acrossall datasets except iemocap in which we ob-.
1577serve a slight increase in both weighted averagef1 and micro-f1.
this might be attributed to thesize of the data since iemocap is the smallestdataset evaluated here, and small datasets hinderthe model’s capability to discover topics.
withoutusing the commonsense knowledge (‘−kb’), weobserve more drastic performance drop comparedto all other components, with nearly 10% drop in f1on emorynlp, showing the importance of employ-ing commonsense knowledge for dialogue emotiondetection.
comparing two different ways of extract-ing knowledge from atomic, direct retrieval usingsbert or generation using comet, we observemixed results.
overall, the transformer encoder-decoder with a pointer network is a conciliatorbetween the two methods, yielding a balanced per-formance across the datasets..relationships between topics and emotionsto investigate the effectiveness of the learned topicvectors, we perform t-sne (van der maaten andhinton, 2008) on the test set to study the rela-tionship between the learned topic vectors and theground-truth emotion labels.
the results on dai-lydialog and meld are illustrated in figure 3(a)and (b).
latent topic vectors of utterance are usedto plot the data points, whose colors indicate theirground-truth emotion labels.
we can see that themajority of the topic vectors cluster into polarizedgroups.
few clusters are bearing a mixture of po-larity, possibly due to the background topics suchas greetings in the datasets..topics can be interpreted using the attentionscores of eq.
4. the top-10 most-attended wordsare selected as the representative words for each ut-terance.
as in (dathathri et al., 2020), we constructbag-of-words7 that represent 141 distinct topics.
given the attended words of an utterance clustergrouped based on their latent topic representations,we label the word collection with the dominanttheme name.
we refer to the theme names as topicsin figure 3c.
it can be observed that utterancesassociated with ofﬁce tend to carry ‘disgust’ emo-tions, while those related to family are prone to be‘happy’..we further compute the spearman’s rank-ordercorrelation coefﬁcient to quantitatively verify therelationship between the topic and emotion vec-tors.
for an utterance pair, a similarity score is.
7word lists and their corresponding theme namesare crawled from https://www.enchantedlearning.
com/wordlist/..(a) dailydialog.
(b) meld.
topic.
utterances.
emotion.
ofﬁce.
family.
a: how are you doing, christopher?
b: to be honest, i’m really fed up withwork at the moment.
i need a break!
a: are you doing anything this weekend?
b: i have to work on saturday all day!.
i really hate my job!.
a: yeah, i-i heard.
i think it’s great!
ohh,.
i’m so happy for you!.
b: i can’t believe you’re getting married!
c: yeah.
d: monica and rachel made out..disgust.
happy.
(c) representative utterances and their topics.
figure 3: t-sne visualization of the learned topic vec-tors of utterances from the test sets of dailydialog(subﬁgure (a)) and meld (subﬁgure (b)).
colors indi-cate the ground-truth emotion label.
neutral utterancesare omitted here for clarity.
representative utterances(highlighted in colors) for the topic ‘ofﬁce’ in daily-dialog and the topic ‘family’ in meld are shown insubﬁgure (c)..obtained separately for their corresponding topicvectors as well as their emotion vectors.
we thensort the list of emotion vector pairs according totheir similarity scores to check to what extent theirranking matches that of topic vector pairs, basedon the spearman’s rank-order correlation coefﬁ-cient.
the results are 0.60, 0.58, 0.42 and 0.54with p-values (cid:28) 0.01 respectively for dailydialog,meld, iemocap and emorynlp, showing thatthere is a strong correlation between the clusteringof topics and that of emotion labels.
iemocaphas the lowest correlation score, which is inlinewith the results in table 2 that the discovered latenttopics did not improve the emotion classiﬁcationresults..impact of relation type we investigate theimpact of commonsense relation types on theperformance of todkat.
we expand the re-lation set to ﬁve relation types and all nine re-lation types, respectively.
according to (sap.
1578officefamilydataset.
dailydialogmeldiemocapemorynlp.
relation type.
{si, sr, or, se, oe}0.5718↓0.6429↓0.6163↑0.4029↓.
all0.5664↓0.6322↓0.6073↓0.3885↓.
table 3: micro-f1 scores of todkat with more com-monsense relation types retrieved from atomic in-cluded for training.
here, “se” and “oe” representeffect of subject and effect of object, respectively.
“all”denotes the incorporation of all nine commonsense re-lation types from atomic..et al., 2019), there are other relation types includ-ing {sneed, swant, owant, seffect, oeffect}, whichidentiﬁes the prerequisites and post conditions ofthe given event, and {sattr}, the “if-event-then-persona” category of relation type that describeshow the subject is perceived by others.
we calcu-late the micro-f1 scores of todkat with thesetwo categories of relation types added step by step.
from table 3 we can conclude that the inclusionof two extra relation types or all relation types de-grades the f1 scores on almost all datasets.
anexception occurs on iemocap where the f1 scorerises by 0.5% when adding “se” and “oe” rela-tions, possibly due to the fact that the dataset isabundant in events.
hence the extra event descrip-tions offer complementary knowledge to some ex-tent.
while on other datasets neither the incorpo-ration of “if-event-then-event” nor the incorpo-ration of “if-event-then-persona” relation typescould bring any beneﬁt..impact of attention mechanism with theknowledge retrieved from atomic or generatedfrom comet, we are able to infer the possibleintentions and reactions of the interlocutors.
how-ever, not all knowledge phrases contribute the sameto the emotion of the focused utterance.
we studythe attention mechanism in terms of selecting therelevant knowledge.
we show in table 4 a heatmap of the attention scores in eq.
9 to illustratehow the topic-driven attention could identify themost salient phrase.
the utterance ‘oh my god,you’re a freak.’ will be erroneously categorizedas ‘mad’ without using the topic-driven attention(shown in the last row of table 4).
in contrast, theattention mechanism guides the model to attendto the more relevant events and thus predict thecorrect emotion label..t a: alright, go on.
xetnoceugolaid.b: ok, i have to sleep on the west sidebecause i grew up in californiaand otherwise the ocean would beon the wrong side..a: oh my god, you’re a freak.
b: yeah.
how about that.
a wants to be likeda wants to be accepteda wants to be a freaka will feel satisﬁeda will feel ashameda will feel happyb will feel impressedb will feel disgustedb will feel surpriseda: oh my god, you’re a freak..noitnett.anevir.d-cipot.neutralneutral.
joyfulneutral.
joyful (cid:51).
mad (cid:55).
table 4:eq.
9 that helps distinguish the retrieved knowledge..illustration of the attention mechanism in.
6 conclusion.
we have proposed a topic-driven and knowledge-aware transformer model that incorporates topicrepresentation and the commonsense knowledgefrom atomic for emotion detection in dialogues.
a topic-augmented language model based on ﬁne-tuning has been developed for topic extraction.
pointer network and additive attention have beenexplored for knowledge selection.
all the novelcomponents have been integrated into the trans-former encoder-decoder structure that enablesseq2seq prediction.
empirical results demonstratethe effectiveness of the model in topic represen-tation learning and knowledge integration, whichhave both boosted the performance of emotion de-tection..acknowledgements.
the authors would like to thank the anonymousreviewers for insightful comments and helpful sug-gestions.
this work was funded by the epsrc(grant no.
ep/t017112/1, ep/v048597/1).
lz isfunded by the chancellor’s international scholar-ship at the university of warwick.
yh is supportedby a turing ai fellowship funded by the uk re-search and innovation (grant no.
ep/v020579/1).
dz is funded by the national key research and de-velopment program of china (2017yfb1002801)and the national natural science foundation ofchina (61772132)..1579references.
meysam asgari-chenaghlu, mohammad-reza feizi-derakhshi, mohammad-ali balafar, and cina mo-tamed.
2020. topicbert: a transformer transferlearning based memory-graph approach for multi-modal streaming social media topic detection.
arxivpreprint arxiv:2008.06877..dzmitry bahdanau, kyunghyun cho, and yoshua ben-gio.
2015. neural machine translation by jointlyin proceedings oflearning to align and translate.
the 3rd international conference on learning rep-resentations (iclr)..antoine bosselut, hannah rashkin, maarten sap, chai-tanya malaviya, asli celikyilmaz, and yejin choi.
2019. comet: commonsense transformers for au-tomatic knowledge graph construction.
in proceed-ings of the 57th annual meeting of the associationfor computational linguistics, pages 4762–4779,florence, italy.
association for computational lin-guistics..samuel bowman, luke vilnis, oriol vinyals, andrewdai, rafal jozefowicz, and samy bengio.
2016.generating sentences from a continuous space.
inproceedings of the 20th signll conference oncomputational natural language learning, pages10–21..carlos busso, murtaza bulut, chi-chun lee, abekazemzadeh, emily mower, samuel kim, jean-nette n chang, sungbok lee, and shrikanth siemocap: interactive emotionalnarayanan.
2008.language re-dyadic motion capture database.
sources and evaluation, 42(4):335..jie cao, michael tanana, zac imel, eric poitras, davidatkins, and vivek srikumar.
2019. observing dia-logue in therapy: categorizing and forecasting be-havioral codes.
in proceedings of the 57th annualmeeting of the association for computational lin-guistics, pages 5599–5611, florence, italy.
associa-tion for computational linguistics..sumanth dathathri, andrea madotto, janice lan, janehung, eric frank, piero molino, jason yosinski, androsanne liu.
2020. plug and play language models:a simple approach to controlled text generation.
inproceedings of the 8th international conference onlearning representations (iclr)..paul ekman.
1993. facial expression and emotion..american psychologist, 48(4):384..jun gao, wei bi, xiaojiang liu, junhui li, guodongzhou, and shuming shi.
2019. a discrete cvae forresponse generation on short-text conversation.
inproceedings of the 2019 conference on empiricalmethods in natural language processing and the9th international joint conference on natural lan-guage processing (emnlp-ijcnlp), pages 1898–1908..deepanway ghosal, navonil majumder, alexandergelbukh, rada mihalcea, and soujanya poria.
2020.cosmic: commonsense knowledge for emotionin findings of theidentiﬁcation in conversations.
association for computational linguistics: emnlp2020, pages 2470–2481, online.
association forcomputational linguistics..deepanway ghosal, navonil majumder, soujanya po-ria, niyati chhaya, and alexander gelbukh.
2019.dialoguegcn: a graph convolutional neural networkfor emotion recognition in conversation.
in proceed-ings of the 2019 conference on empirical methodsin natural language processing and the 9th inter-national joint conference on natural language pro-cessing (emnlp-ijcnlp), pages 154–164..chao-chun hsu and lun-wei ku.
2018. socialnlp2018 emotionx challenge overview: recognizingemotions in dialogues.
in proceedings of the sixthinternational workshop on natural language pro-cessing for social media, pages 27–31, melbourne,australia.
association for computational linguis-tics..eric jang, shixiang gu, and ben poole.
2017. cate-gorical reparameterization with gumbel-softmax.
inproceedings of the 5th international conference onlearning representations (iclr)..wenxiang jiao, michael lyu, and irwin king.
2020.real-time emotion recognition via attention gatedin proceedings ofhierarchical memory network.
the aaai conference on artiﬁcial intelligence, vol-ume 34, pages 8002–8009..irwin king,.
wenxiang jiao, haiqin yang,.
andmichael r lyu.
2019. higru: hierarchical gated re-current units for utterance-level emotion recognition.
in proceedings of the 2019 conference of the northamerican chapter of the association for computa-tional linguistics: human language technologies,volume 1 (long and short papers), pages 397–406..diederik p kingma and max welling.
2014. auto-in proceedings of theencoding variational bayes.
2nd international conference on learning represen-tations (iclr)..chunyuan li, xiang gao, yuan li, baolin peng, xiu-jun li, yizhe zhang, and jianfeng gao.
2020a.
opti-mus: organizing sentences via pre-trained modelingof a latent space.
in proceedings of the 2020 con-ference on empirical methods in natural languageprocessing (emnlp), pages 4678–4699, online.
as-sociation for computational linguistics..wei li, wei shao, shaoxiong ji, and erik cambria.
2020b.
bieru: bidirectional emotional recurrentunit for conversational sentiment analysis.
arxivpreprint arxiv:2006.00492..yanran li, hui su, xiaoyu shen, wenjie li, ziqiangcao, and shuzi niu.
2017. dailydialog: a manuallylabelled multi-turn dialogue dataset.
in proceedings.
1580of the eighth international joint conference on nat-ural language processing (volume 1: long papers),pages 986–995..yinhan liu, myle ott, naman goyal, jingfei du, man-dar joshi, danqi chen, omer levy, mike lewis,luke zettlemoyer, and veselin stoyanov.
2019.roberta: a robustly optimized bert pretraining ap-proach.
arxiv preprint arxiv:1907.11692..laurens van der maaten and geoffrey hinton.
2008.journal of machine.
visualizing data using t-sne.
learning research, 9(11)..navonil majumder, soujanya poria, devamanyu haz-arika, rada mihalcea, alexander gelbukh, and erikcambria.
2019. dialoguernn: an attentive rnn foremotion detection in conversations.
in proceedingsof the aaai conference on artiﬁcial intelligence,volume 33, pages 6818–6825..yishu miao, lei yu, and phil blunsom.
2016. neuralin inter-variational inference for text processing.
national conference on machine learning (icml),pages 1727–1736..yookoon park, jaemin cho, and gunhee kim.
2018.a hierarchical latent structure for variational conver-in proceedings of the 2018 con-sation modeling.
ference of the north american chapter of the asso-ciation for computational linguistics: human lan-guage technologies, volume 1 (long papers), pages1792–1801, new orleans, louisiana.
associationfor computational linguistics..nicole peinelt, dong nguyen, and maria liakata.
2020.tbert: topic models and bert joining forces forsemantic similarity detection.
in proceedings of the58th annual meeting of the association for compu-tational linguistics, pages 7047–7055, online.
as-sociation for computational linguistics..fabio petroni, tim rockt¨aschel, sebastian riedel,patrick lewis, anton bakhtin, yuxiang wu, andalexander miller.
2019. language models as knowl-in proceedings of the 2019 confer-edge bases?
ence on empirical methods in natural languageprocessing and the 9th international joint confer-ence on natural language processing (emnlp-ijcnlp), pages 2463–2473..soujanya poria, devamanyu hazarika, navonil ma-jumder, gautam naik, erik cambria, and rada mi-halcea.
2019. meld: a multimodal multi-partydataset for emotion recognition in conversations.
inproceedings of the 57th annual meeting of the as-sociation for computational linguistics, pages 527–536, florence, italy.
association for computationallinguistics..nils reimers and iryna gurevych.
2019. sentence-bert: sentence embeddings using siamese bert-networks.
in proceedings of the 2019 conference onempirical methods in natural language processingand the 9th international joint conference on natu-ral language processing (emnlp-ijcnlp), pages.
3973–3983, hong kong, china.
association forcomputational linguistics..mehdi rezaee and francis ferraro.
2020. a dis-crete variational recurrent topic model without thein advances in neural in-reparametrization trick.
formation processing systems, volume 33, pages13831–13843..maarten sap, ronan le bras, emily allaway, chan-dra bhagavatula, nicholas lourie, hannah rashkin,brendan roof, noah a smith, and yejin choi.
2019.atomic: an atlas of machine commonsense for if-in proceedings of the aaai con-then reasoning.
ference on artiﬁcial intelligence, volume 33, pages3027–3035..iulian serban, alessandro sordoni, ryan lowe, lau-rent charlin, joelle pineau, aaron courville, andyoshua bengio.
2017. a hierarchical latent variableencoder-decoder model for generating dialogues.
inproceedings of the aaai conference on artiﬁcial in-telligence, volume 31, pages 3295–3301..xiaoyu shen, hui su, shuzi niu, and vera demberg.
2018. improving variational encoder-decoders in di-alogue generation.
in proceedings of the aaai con-ference on artiﬁcial intelligence, volume 32, pages5456–5463..kihyuk sohn, honglak lee, and xinchen yan.
2015.learning structured outputrepresentation usingdeep conditional generative models.
in advances inneural information processing systems, volume 28,pages 3483–3491..robyn speer, joshua chin, and catherine havasi.
2017.conceptnet 5.5: an open multilingual graph of gen-in proceedings of the aaai con-eral knowledge.
ference on artiﬁcial intelligence, volume 31, pages4444–4451..akash srivastava and charles sutton.
2017. autoen-coding variational inference for topic models.
inproceedings of the 5th international conference onlearning representations (iclr)..ilya sutskever, oriol vinyals, and quoc v le.
2014.sequence to sequence learning with neural networks.
in advances in neural information processing sys-tems, volume 27, pages 3104–3112..ashish vaswani, noam shazeer, niki parmar, jakobuszkoreit, llion jones, aidan n gomez, łukaszkaiser, and illia polosukhin.
2017. attention is allin advances in neural information pro-you need.
cessing systems, volume 30, pages 5998–6008..oriol vinyals, meire fortunato, and navdeep jaitly.
in advances in neural2015. pointer networks.
information processing systems, volume 28, pages2692–2700..thomas wolf, lysandre debut, victor sanh, julienchaumond, clement delangue, anthony moi, pier-ric cistac, tim rault, remi louf, morgan funtow-icz, joe davison, sam shleifer, patrick von platen,.
1581a appendices.
a.1 settingswe modiﬁed the script8 of language model ﬁne-tuning in the hugging face library (wolf et al.,2020) for the implementation of topic-driven ﬁne-tuning.
on each training set, we train the topicmodel for 3 epochs, with learning rate set to 5e-5to prevent overﬁtting to the low-resource dataset.
the classiﬁer is built on the transformers9 pack-age in hugging face.
the language model weemploy is roberta (liu et al., 2019).
each utter-ance is padded by the <pad> token of robertaif it is less than the maximum length of 128. themaximum number of utterances in a dialogue isset to 36, 25, 72 and 25 respectively for daily-dialog (li et al., 2017) 10, meld (poria et al.,2019) 11, iemocap (busso et al., 2008) 12 andemorynlp (zahiri and choi, 2018) 13. dialogueswith shorter lengths are padded with null.
itis worth noting that this step is performed afterroberta due to the random noises introducedby roberta.
the number of retrieved or gener-ated events from atomic under the relation types‘intentions’ and ‘reactions’ is both set to 5, i.e.,k = 5..clara ma, yacine jernite, julien plu, canwen xu,teven le scao, sylvain gugger, mariama drame,quentin lhoest, and alexander rush.
2020. trans-formers: state-of-the-art natural language process-ing.
in proceedings of the 2020 conference on em-pirical methods in natural language processing:system demonstrations, pages 38–45, online.
asso-ciation for computational linguistics..sayyed m zahiri and jinho d choi.
2018. emotion de-tection on tv show transcripts with sequence-basedconvolutional neural networks.
in workshops at thethirty-second aaai conference on artiﬁcial intelli-gence..min zeng, yisen wang, and yuan luo.
2019. dirich-let latent variable hierarchical recurrent encoder-decoder in dialogue generation.
in proceedings ofthe 2019 conference on empirical methods in nat-ural language processing and the 9th internationaljoint conference on natural language processing(emnlp-ijcnlp), pages 1267–1272, hong kong,china.
association for computational linguistics..dong zhang, liangqing wu, changlong sun,shoushan li, qiaoming zhu, and guodong zhou.
2019. modeling both context- and speaker-sensitivedependence for emotion detection in multi-speakerthe twenty-conversations.
eighth international joint conference on artiﬁcialintelligence, ijcai-19, pages 5415–5421.
interna-tional joint conferences on artiﬁcial intelligenceorganization..in proceedings of.
peixiang zhong, di wang, and chunyan miao.
2019.knowledge-enriched transformer for emotion detec-in proceedings oftion in textual conversations.
the 2019 conference on empirical methods in nat-ural language processing and the 9th internationaljoint conference on natural language processing(emnlp-ijcnlp), pages 165–176..8https://huggingface.co/transformers/.
v2.0.0/examples.html.
9https://huggingface.co/transformers/10http://yanran.li/dailydialog.html11https://github.com/declare-lab/meld12https://sail.usc.edu/iemocap/iemocap_.
release.htm.
13https://github.com/emorynlp/.
emotion-detection.
1582