multilingual speech translationfrom efﬁcient finetuning of pretrained models.
xian li*, changhan wang*, yun tang, chau tran, yuqing tang, juan pino,alexei baevski, alexis conneau, michael aulifacebook ai{xianl,changhan,yuntang,chau,yuqtang,juancarabina,abaevski,aconneau,michaelauli}@fb.com.
abstract.
we present a simple yet effective approach tobuild multilingual speech-to-text (st) transla-tion through efﬁcient transfer learning froma pretrained speech encoder and text de-coder.
our key ﬁnding is that a minimalisticlna (layernorm and attention) ﬁnetuningcan achieve zero-shot crosslingual and cross-modality transfer ability by only ﬁnetuning10 ∼ 50% of the pretrained parameters.
thiseffectively leverages large pretrained modelsat low training cost such as wav2vec 2.0 foracoustic modeling, and mbart for multilin-gual text generation.
this sets a new state-of-the-art for 36 translation directions (and sur-passing cascaded st for 30 of them) on thelarge-scale multilingual st benchmark cov-ost 2 (wang et al., 2020b) (+6.4 bleuon average for en-x directions and +6.7bleu for x-en directions).
our approachdemonstrates strong zero-shot performance ina many-to-many multilingual model (+5.6bleu on average across 28 directions), mak-ing it an appealing approach for attaining high-quality speech translation with improved pa-rameter and data efﬁciency..1.introduction.
recent advances in pretraining over unlabeled dataand then ﬁnetuning on labeled data leads to sig-niﬁcant performance improvement in text under-standing and generation tasks (devlin et al., 2019;liu et al., 2020; conneau et al., 2019; radford,2018).
lately, such text pretraining and ﬁnetuningparadigms have been extended to other modalities:audio (schneider et al., 2019; baevski et al., 2020),images (su et al., 2019; lu et al., 2019), and video(sun et al., 2019).
at the same time, pretrainingand ﬁnetuning techniques have improved multi-tasking applications signiﬁcantly, such as multi-lingual translation, cross-lingual representations,question-answering and so on (raffel et al., 2020;.
figure 1: an overview of the proposed speech-to-texttranslation via transfer learning from efﬁcient ﬁnetun-ing of single-modality pretrained models.
the pro-posed lna ﬁnetuning is applied to each layer..yang et al., 2019; tang et al., 2020).
in this paper,we advance the one-model-for-all paradigm furtherby adapting audio and multilingual text pretrainingand ﬁnetuning to improve multilingual speech-to-text translation..our contributions are as follows:.
• we propose a simple and effective approach tocombine pretrained single-modality modulesto perform speech-to-text translation.
withminimal architecture change, we add a cross-modal adaptor to bridge the length discrep-ancy between audio encoder output and textdecoder input.
our approach can also performmulti-task ﬁnetuning with both speech-to-texttranslation and text-to-text translation taskswhere we ﬁnd joint training with the latterbrings further gains..• we present an efﬁcient transfer learning strat-egy by only ﬁnetuning the layernorm andattention (lna) parameters of pretrainedmodels.
this approach is not only parameter-and data-efﬁcient but also effective for zero-.
proceedingsofthe59thannualmeetingoftheassociationforcomputationallinguisticsandthe11thinternationaljointconferenceonnaturallanguageprocessing,pages827–838august1–6,2021.©2021associationforcomputationallinguistics827self attentionffnlayernormlayernorm xlength adaptorwav2vec 2.0self attentionencoder attentionffnlayernormlayernormlayernormx mbartfinetune layernorm and attention pretrainedmodulesencoderdecodershot crosslingual transfer to unseen languages(train on a → b, test on a → c and c → b)..• our approach is also effective for zero-shotmultilingual translation (train on a → b andb → c, test on a → c), which provides anefﬁcient approach for many-to-many speech-to-text translation without dependency for par-allel data for every direction..• using a pretrained audio encoder (wav2vec(baevski et al., 2020)) and multilingual textdecoder (mbart (liu et al., 2020)), thisapproach sets a new state-of-the-art (sota)on two large-scale speech translation bench-marks.
on covost 2 (wang et al., 2020b),we pushed the sota for end-to-end approachfor all 21 x-en directions(+6.7 bleu on av-erage) and 15 en-x directions (+6.4 bleuon average) by ﬁnetuning only 10 ∼ 50% ofparameters.
similarly on europarl (iranzo-s´anchez et al., 2020), our zero-shot multilin-gual many-to-many model is not only dataefﬁcient, but also brings +5.7 bleu (on av-erage) when translating 18 non-english di-rections compared to a many-to-many modeltraining on 1.6× training data with all pair-wise (both to/from english and non-english)directions..we describe our approach in section 2, namelypretrained models, length adaptor, lna ﬁnetuningand joint speech-text ﬁnetuning as is illustratedin figure 1. experiments setup and results areelaborated in section 3 and section 4. section 5provides ablation studies of the proposed ﬁnetuningstrategy..2 methods.
2.1 pretrained modules.
our modelleverages a pretrained wav2vec2.0 (baevski et al., 2020) as encoder for acous-tic modeling, a pretrained multilingual bart(mbart) (liu et al., 2020) as decoder for lan-guage modeling.
both models are pretrained onunlabelled data via self-supervised learning.
weprovide an overview of the pretraining procedurein a.1..ules pretrained with different modalities.
the adap-tor module performs projection and downsamplingto alleviate length inconsistency between the audioand text sequences.
speciﬁcally, we use a stack ofn 1-dimensional convolutional layers with stridem to shrink the speech sequence (encoder output)by a factor of mn..2.3 lna finetuning.
instead of ﬁnetuning all parameters in pretrainedmodels, we propose parameter efﬁcient ﬁnetuningstrategy (lna) of only ﬁnetuning the layer nor-malization (layernorm) and multi-head attention(mha) parameters.
lna is motivated to bridge thediscrepancy between pretraining and downstream(st) task, which we hypothesize are accounted bythe following parameters:layernorm parameters from pretrained modelswere trained based on the statistics of the data usedin pretraining and thus need to be adapted to down-stream tasks during ﬁnetuning.
the importance ofﬁnetuning layernorm has been observed in mul-tilingual (text-only) translation (stickland et al.,2020).
attention encoder attention (ea, attention to en-coder outputs) parameters from pretrained mt de-coder were trained on the text-to-text mt task, sowe hypothesize that they are crucial to be adapted tothe speech encoder output.
combined with layer-norm parameter is the proposed lna-minimalistin addition, we also investigate theﬁnetuning.
role of self attention (sa) parameters in facilitatingcrosslingual transfer ability..2.4.joint speech-text finetuning.
multi-task learning has been shown as an effectiveapproach to improve the performance of the speechtranslation task using other related tasks, such asmt and asr (weiss et al., 2017; anastasopoulosand chiang, 2018; bahar et al., 2019; tang et al.,2021a,b).
we jointly train mt and st tasks in theﬁnetuning with pretrained models.
the speech tran-scripts are used as input for the mt task and thecorresponding speech data is used as input for thest task.
as a result, we can leverage abundant par-allel text data to further improve the performance..3 experimental setup.
2.2 length adaptor.
3.1 datasets.
we add a lightweight adaptor module in betweenencoder and decoder to better align the two mod-.
we evaluate our proposed models on two large-scale multilingual speech translation benchmarks..828statistics of the datasets and implementation detailsare reported in the a.2 and a.3.
covost 2 (wang et al., 2020b) is a multilingualspeech-to-text translation corpus with english into15 languages (en-x) and 21 languages into english(x-en).
it provides a comprehensive test bed forlow-resource scenarios, with 4 x-en directions be-tween 10 hours and 20 hours training data, and 11x-en directions less than 4 hours training data.
europarl st (iranzo-s´anchez et al., 2020) hasboth english-centric as well as non-english di-rections, which allow us to evaluate the proposedmethod’s effectiveness of multilingual translationbetween any pair, especially zero-shot performance.
we experiment on all 6 languages (de, en, es, fr, it,pt).
we compare to a multilingual baseline trainedwith all pair-wise parallel data..3.2 training.
we evaluate the following instantiation of the pro-posed method which is referred to as xmef (cross-modal efﬁcient finetuning).
encoder.
we initialize the encoder using the open-sourced1 wav2vec 2.0 large architecture pretrainedon unlabelled english-only (xmef-en) audio fromlibrivox (baevski et al., 2020).
for many-to-oneexperiments, we also experiment with a multi-lingual wav2vec 2.0 (xmef-x), which was pre-trained on raw audio from 53 languages (conneauet al., 2020).
encoder output is followed by 3 1-d convolution layers with stride 2 to achieve 8xdown-sampling of audio encoder outputs.
decoder.
we initialize the decoder with open-sourced2 mbart50 models and the same vocab-ulary (tang et al., 2020).
we use mbart50n1(49 languages to english) for x-en st directionsand mbart501n (english to 49 languages) fortranslating en-x st directions.
lna finetuning.
we study the parameter efﬁ-ciency and crosslingual transfer ability of lnaﬁnetuning in the bilingual setting without the addi-tional effect from multilingual training.
drawinglearnings on that, we then evaluate applying lnaﬁnetuning to encoder only (lna-e), decoder only(lna-d), and both (lna-e,d) respectively.
formultilingual ﬁnetuning on covost 2, we use allx-en training data (except zero-shot crosslingualtransfer experiments) for evaluating x-en perfor-.
1https://github.com/pytorch/fairseq/.
tree/master/examples/wav2vec..2https://github.com/pytorch/fairseq/.
tree/master/examples/multilingual.
figure 2: comparison of lna ﬁnetuning with alter-native ﬁnetuning strategies: ﬁnetuning all parameters(all), ﬁnetuning top-k layers (top1, top2).
we evalu-ate generalization (perplexity on dev set) performancewith different amounts of training data.
lna achievesthe best generalization with substantially less parame-ters.
experiments are done using covost en-de..figure 3: comparison of lna ﬁnetuning with alter-natives: ﬁnetuning all parameters (all) and ﬁnetuningfeature extractor (input), on adapting wav2vec englishencoder to translate non-english speech input.
experi-ments are done using covost de-en..mance, and en-x data from all directions for eval-uating en-x performance.
for evaluating multilin-gual zero-shot performance on europarl, we onlyuse x-en and en-x for ﬁnetuning and evaluate onall (x-x) pairs..joint training.
two encoders are initialized withthe pretrained mbart encoder and wav2vec 2.0encoder mentioned above, and are used for text andspeech input respectively.
the last 12 transformerlayers in the wav2vec encoder are replaced with12 mbart encoder layers.
parameters in those 12layers are shared between the two encoders duringjoint training (tang et al., 2021b).
the decoderis also shared between two tasks and is initializedwith the pretrained mbart decoder model.
wealso experimented with adding additional bitextused in ml50 (tang et al., 2020) as training datafor the mt task.
only the language pairs presentin the covost 2 dataset are chosen and they coverall language pairs except english to and from “ca”and “cy”.
we ﬁne-tune all parameters in this exper-iments due to the large mismatch of the pretrainedmodel (mbart encoder as part of the speech en-coder) and more available training data..82910 hours50 hours182 hours364 hours10203040506070validation perpelxityall,793mtop2,77mtop1,48mlna, 69m69170793number of params.
(m)2530354045valid.
ppllna-minlna-esainputall3.3 baselines.
from scratch: the ﬁrst baseline trains a sequence-to-sequence model with transformer architecturewithout any pretraining.for covost 2 experiments,we use the same model conﬁguration as is providedby (wang et al., 2020b).
asrpt+multi: pretraining encoder on asr taskwas shown to be an effective method to improvespeech translation and accelerates convergence(bansal et al., 2019).
we compare our results to astrong baseline provided by (wang et al., 2020b),consisting of a multilingual transformer modeltrained on covost 2 with multilingual asr pre-training (st).
for the europarl st many-to-manybaseline, we use transformer architecture with 12-layer encoder, 6-layer decoder, and trained on all30 directions.
to provide the strongest baseline,encoder was pre-trained on librispeech englishasr).
xmef-bl: multilingual models for en-x (one-to-many) usually face more challenges from inter-ference as they were found to underperform thebilingual counterparts (arivazhagan et al., 2019).
therefore, we compare to applying our method(xmef, lna) to bilingual (bl) ﬁnetuning, i.e.
ﬁnetuning on parallel data from a single languagepair.
previous sotas: we compare to the best end-to-end (e2e) model from previous literature (wanget al., 2020b; iranzo-s´anchez et al., 2020) on eachtranslation direction, which is usually the best-performing multilingual model trained with par-allel data from all directions (both x-en and en-x) and also pretrained with asr.
even thoughthe focus of the proposed method is e2e model,we also compare to the best performing cascadeapproach (cascade sota) which is composed oftransformer-large encoder from asr pretrainingand a multilingual mt model trained on all x-enand en-x data..4 results.
4.1 parameter efﬁciency.
first, we evaluate the transfer learning performanceof ﬁnetuning the entire pretrained model as wellas the proposed efﬁcient ﬁnetuning (lna).
to sep-arate the additional crosslingual transfer learningfrom multilingual ﬁnetuning, we evalute on bilin-gual st (en-de and de-en in covost) task.
weﬁrst evaluate lna-minimalist (69m params), com-paring to ﬁnetuning all parameters and only top.
layers which were found effective in transfer learn-ing in nlp tasks with pretrained bert (wu anddredze, 2019; kovaleva et al., 2019).
figure 2show that in both low data and high data regimes,the proposed lna-minimalist both generalizes bet-ter (lower perplexity on dev set) and substantiallyimproves training efﬁciency (only 10% of param-eters to train leading to lower memory cost andfaster training)..4.2 transfer from pretraining.
to assess transfer ability from encoder pretrainedon english to other (speech) input languages, weevaluate the performance of xmef-en on cov-ost 2 de-en st task.
we investigate the role ofﬁnetuning encoder self-attention (lna-esa) infacilitating crosslingual transfer.
we compare tobaselines of ﬁnetuning the entire encoder (all), andﬁnetuning feature extractor which are commonlyused in adaptation in asr (rivi`ere et al., 2020).
results are summarized in figure 3. lna stilldemonstrates improved generalization than alterna-tive ﬁnetuning approaches, with ﬁnetuning encoderself attention (lna-esa) being crucial for adapt-ing pretrained english encoder to other languages..4.3 zero-shot crosslingual transfer.
next, we evaluate xmef’s crosslingual transferperformance from multilingual ﬁnetuning.
to pre-cisely measure the transfer capability, we evaluatethe zero-shot setting, i.e.
ﬁnetune xmef-en withparallel st data from multiple languages, and eval-uate on an unseen language.
we study the transferperformance in source (speech) and target (text)separately.
source-side (speech) transfer.
we evaluatewhether the proposed approach enables positivecrosslingual transfer to translate speech from un-seen languages in table 1. we ﬁnetune on labelleddata for 5 to-english language pairs, and evaluatethe ﬁnetuned model’s zero-shot performance whentranslating speech input from unseen languages(pt).
first, we found that comparing to ﬁnetuningmore parameters (lna-d, and all), lna ﬁnetun-ing (lna-e,d) not only trains more than 2 × fasterbut also achieves better generalization both for seenand unseen languages.
especially, it attains remark-able performance as unsupervised speech transla-tion for portuguese-english, achieving 8.2 bleu(compared to the supervised bilingual baseline 0.5bleu as is provided in table 3, and even beats.
830enc.
dec.params..fr.
lna-e,d ln+sa ln+ealn+eaallallall.
lna-dfinetune all.
170.7m 32.4384.8m 31.6793.0m 27.1.asrpt+multisupervised (multi) sota (wang et al., 2020b).
23.126.5.train.
es.
31.631.027.8.
21.227.0.de.
24.923.717.7.
15.317.6.ca.
28.627.821.7.
19.923.1.it.
24.023.218.9.
14.918.5.zero-shot.
pt.
8.27.65.1.
4.46.3.table 1: performance on zero-shot transfer on the source-side (speech).
each model is ﬁnetuned on 5 directionsfrom {fr, de, es, ca, it} → en, and evaluated on unsupervised translation of a new language (pt).
we reportbleu scores on test set, and compare to the zero-shot transfer performance of a supervised multilingual baseline(asrpt+multi), as well as previous state-of-the-art which is also the supervised and multilingually trained..train.
zero-shot.
enc.
dec.params..de.
fa.
lna-e,dln+ealnlna-e,d ln+sa ln+ealn+eaallallln+saallall.
lna-dlna-efinetune all.
69.4m22.1170.7m 23.8384.8m 24.9477.6m 22.0793.0m 24.1.asrpt+multisupervised (multi) sota (wang et al., 2020b).
9.517.3.
17.719.219.818.119.6.
10.914.5.tr.
13.414.215.214.215.6.
6.810.7.zh.
29.230.632.729.532.4.
23.528.2.ja.
22.929.230.60.80.4.
0.031.9.table 2: performance on zero-shot transfer on the target-side (text).
each model is ﬁnetuned on 4 directions en →{de, fa, tr, zh}, and evaluated on unsupervised translation to a new language (ja).
we report bleu scores ontest set, and compare to the zero-shot transfer performance of a supervised multilingual baselines (asrpt+multi),as well as previous state-of-the-art which is also the supervised and multilingually trained..(+1.9 bleu) the previous state-of-the-art for thisdirection which is a supervised multilingual model.
target-side (text) transfer.
table 2 shows the pro-posed approach also achieves zero-shot transfercapability for translating to new languages, withunsupervised translation for english-japanese only1.3 bleu behind the best supervised result.
fur-thermore, an interesting ﬁnding is that applyinglna ﬁnetuning to decoder is crucial for zero-shottransfer to unseen languages (ja), as ﬁnetuning theentire decoder tends to optimize the model on targetlanguages seen during training..4.4 multilingual speech translation.
we evaluate the performance of xmef with multi-lingual ﬁnetuning on all 36 translation directionsin covost 2, respectively all 21 languages intoenglish (many-to-one) and from english into 15languages (one-to-many)..many to one.
consistent with the observation ofsource-side crosslingual transfer in sec 4.1, xmef-en perform very well on romance, germanic andslavic language families in both high-resource (≥ 100 hours training data) and low-resource di-rections (7 ∼ 44 hours training data) as is sum-marized in table 3, and even surpassing the bestcascade results on 8 languages.
our multilingualmodel also improves distant (from english) and.
extremely low resource (mostly ≤ 5 hours trainingdata) languages as is shown in second panel of ta-ble 3. for crosslingual adaptation from xmef-ento speech input of other languages, lna-e,d (onlyﬁnetune 21.5% of pretrained parameters) outper-forms ﬁnetuning the entire model (finetune all) by0.7 bleu (averaged across 21 directions), whileﬁnetuning the entire encoder (lna-d) brings +1.2bleu.
finetuning xmef-x achieves the best av-erage bleu score, however, major improvementis from ﬁnetuning encoder (lna-d)..one to many.
table 4 summarizes performanceon translating (from english) to 15 languageswhere multilingual models from xmef-en haveimproved previous state-of-the-art (both e2e andcascade) on all directions (+6.4 bleu on average).
the performance of applying lna ﬁnetuning toencoder only (lna-e) is very close to (24.2 vs.24.5 averaged bleu) that of ﬁnetuning the entiremodel (finetune all) while has 40% less parame-ters to train.
applying lna to both encoder anddecoder (lna-min, lna-e,d) further reduces theamount of parameters to train to only 8 ∼ 20%of all parameters in the pretrained models yet stillmaintain strong performance compared to strongbaselines such as asr pt with multilingual ﬁne-tuning (asr pt+multi) as well as the best cascademodels.
the only two languages (ca, cy) it did not.
831→ entrain hours.
scratch-bl+ asr pt+ multi.
+mbart.
fr264.
24.326.326.528.1.
33.8*35.0*33.0*33.5*34.4*.
32.8*34.2*36.1*.
high resource.
low resource.
de184.
8.417.117.519.7.
26.7*28.2*24.5*28.6*29.6*.
28.6*30.8*30.6*.
18.923.2.es113.
12.023.027.028.1.
34.0*35.2*33.6*33.5*34.4*.
34.0*35.8*38.1*.
28.031.1.ca136.
14.418.823.124.0.
29.5*31.1*28.0*30.6*30.6*.
29.7*31.7*31.8*.
24.027.2.it44.
0.211.318.519.9.
26.1*27.6*25.2*26.6*27.7*.
27.9*29.4*31.9*.
11.322.9.ru18.
1.214.84.72.7.
21.122.820.217.627.7*.
25.126.530.9.
14.825.0.pt10.
0.56.16.36.2.
19.224.1*19.512.014.6.
19.519.620.7.
6.122.7.nl7.
0.33.05.08.1.
14.1*14.2*9.415.0*14.5*.
24.125.724.0.
8.410.4.zh1045.0.tr451.2.et365.7.mn365.2.ar263.3.lv251.8.cy272.8.ta280.8.ja177.1.id163.2.
1.45.85.95.4.
6.26.06.55.46.2.
8.09.18.9.
0.73.62.32.4.
5.54.84.03.34.0.
8.611.2*9.4*.
0.10.10.60.7.
1.31.51.40.70.8.
2.02.92.5.
0.93.8.
0.10.20.10.2.
1.00.91.00.20.3.
1.01.11.2.
0.21.0.
0.34.30.40.5.
3.72.83.30.81.0.
6.26.26.4.
4.312.3.
0.12.50.60.6.
4.64.94.92.73.6.
3.33.85.0.
2.57.2.
0.32.71.91.4.
2.82.32.11.01.1.
4.99.0*8.1*.
3.37.4.
0.30.30.10.1.
0.70.80.50.10.2.
0.60.70.9.
0.30.4.
0.31.50.10.2.
1.71.72.10.30.5.
0.80.81.0.
1.53.8.xmef-en.
xmef-x.
lna-e,d (170.7m)lna-d (384.8m)finetune all (793.0m)joint training (1.05b)+ extra mt data.
lna-e,d (170.7m)lna-d (384.8m)finetune all (793.0m).
prev.
e2e sota 27.0cascade sota 29.1.
→ entrain hoursasr (wer).
baseline+ asr pt+ multi.
+ mbart.
fa4962.4.
1.93.72.43.3.
4.03.63.76.1*5.0.xmef-en.
lna-e,d (170.7m)lna-d (384.8m)finetune all (793.0m)joint training (1.05b)+ extra mt data.
xmef-x.
lna-e,d (170.7m)lna-d (384.8m)finetune all (793.0m).
6.6*11.0*8.5*.
sv2.
0.22.70.51.4.
5.95.04.82.63.4.
4.03.24.0.
2.711.9.avg..7.07.3.
11.912.411.210.711.7.
13.014.314.7.sl2.
0.33.00.70.5.
4.65.04.63.95.2.
3.04.35.6.
3.07.0.
0.42.50.30.2.
2.93.73.40.50.5.
2.32.32.8.
2.511.8.prev.
sota 3.75.8.cascade.
5.911.4.
3.79.3.table 3: performance of x → en multilingual model.
we report bleu scores on test set.
for each xmef method,we report the number of parameters trained in brackets.
previous e2e sota is the best-performing end-to-endmultilingual (with asr pretraining) model from (wang et al., 2020b).
results in bold are where the proposedapproach improves previous e2e sota, and sets new sota as underlined.
* means our new e2e sota also beatsthe previous cascade sota..improve with lna ﬁnetuning of the decoder werenever seen during mbart pretraining..translation.
4.5 zero-shot many-to-many speech to text.
joint training in the many to one case (ta-ble 3), language pairs with reasonable amountspeech training data (+ 18 hours) and large amountof parallel text data (+1 million sentences) (“fr-en”, “de-en”, “es-en”, “it-en”, “ru-en” and “fa-en”), outperform the corresponding single tasktrained models and achieve state-of-art results .
however, if the amount of speech data is too small(10 hours or less), joint training is ineffective andmay even make the performance worse.
in one tomany case (“en-x”), where there are 364 hoursenglish audio data for training, joint training im-proves the results further by another 0.6 bleu(table 4)..finally, we evaluate how the proposed approachtranslationperforms in zero-shot multilingual(translating x → y after training on x → en anden → y. we apply lna-d multilingual ﬁnetun-ing using en-x and x-en training data only fromthe europarl corpus.
table 5 reports both the su-pervised performance on to- and from-english di-rections and zero-shot performance translating be-tween non-engligh languages without training ontheir parallel data.
we compare to the strong base-line of a many-to-many multilingual model trainedfrom scratch using all parallel data from non-english directions as well as english-centric direc-tions.
our approach improves both to- and from-english directions (+6.8 bleu and +8.2 bleu on.
832en → ar.
scratch-bl+ asr pt+ multi..lna-min-bl (69.4m)lna-min (69.4m)lna-e,d (170.7m)lna-e (477.6m)finetune all (793.0m)joint training (1.05b)+ extra mt data.
prev.
e2e sota 13.9cascade sota 14.3.en → lv.
scratch-bl+ asr pt+ multi..lna-min-bl (69.4m)lna-min (69.4m)lna-e,d (170.7m)lna-e (477.6m)finetune all (793.0m)joint training (1.05b)+ extra mt data.
8.712.113.0.
12.015.3*17.4*17.2*17.7*18.0*18.6*.
11.513.114.1.
14.317.9*20.1*20.2*20.8*21.5*21.3*.
prev.
e2e sota 15.2cascade sota 15.6.ca.
20.221.822.3.
18.820.322.229.5*30.1*30.9*30.4*.
23.625.0.mn.
6.69.210.2.
6.912.0*13.3*14.1*14.1*14.8*14.7*.
11.011.7.cy.
22.223.923.7.
12.913.214.830.3*30.0*30.6*29.2*.
25.125.6.sl.
11.516.117.1.
17.921.1*23.0*23.5*23.6*25.1*24.8*.
18.318.9.de.
13.616.517.3.
20.3*23.2*25.3*25.2*25.2*25.8*26.6*.
18.419.4.sv.
20.122.322.3.
26.1*27.5*29.6*30.0*30.4*29.6*30.0*.
24.124.8.et.
11.113.413.9.
15.018.6*21.0*20.7*21.1*22.1*21.4*.
15.115.4.ta.
9.911.211.7.
12.614.6*16.4*16.8*17.1*17.8*17.4*.
12.813.7.fa.
11.513.514.5.
15.9*19.6*20.1*19.8*20.3*21.5*20.6*.
15.514.1.tr.
8.910.210.7.
10.814.1*15.5*16.2*16.3*17.016.3*.
11.711.7.id.
18.920.820.3.
24.4*26.5*27.6*28.5*28.9*29.9*28.8*.
22.023.1.zh.
20.625.728.2.
21.832.1*33.0*32.8*33.7*33.334.4*.
31.326.9.ja.
26.929.631.9.
31.436.9*38.4*37.8*38.1*39.3*39.1*.
33.033.8.avg..18.1.
20.922.524.224.525.124.5.table 4: performance on en → x multilingual st. we report bleu scores on test set.
for each xmef method, wereport the number of parameters trained in brackets.
‘bl’ refers to using the same xmef and lna-e,d ﬁnetuningbut only on bilingual corpus.
results in bold are where the proposed approach improves previous e2e sota,and sets new sota as is underlined.
* means our new e2e sota also beats the previous cascade sota.
forthe same model evaluated on multiple directions), we also report the average (avg.)
multilingual models (i.e.
bleu scores across all 15 directions..averge respectively) and our zero-shot results alsobeats (+5.6 bleu) the supervised many-to-manymodel on 28 pair-wise (except for it-pt and pt-es)translation directions..5 ablation studies.
ablation on lna finetuning.
in table 6 weanalyze how individual components of lna con-tribute to the generalization performance and train-ing efﬁciency.
speciﬁcally, we examine the keycomponents of lna-minimalist (lna-min) ﬁne-tuning.
we ﬁnd ﬁnetuning layernorm parameter(far less compared to the amount of multi-headattention parameters) is important for training sta-bility when ﬁnetuning pretrained models withoutwhich (-ln) training diverges.
finetuning the en-coder attention (ea) parameters is important foradapting the pretrained text decoder for st task.
for adapting to a single language pair downstreamst task (english-german), we ﬁnd ﬁnetuning selfattention (+sa) parameters in the decoder did notbring further improvement while signiﬁcantly in-creasing the amount of parameters to train..ablation on length adaptor.
westudywhether the performance is sensitive to downsam-pling ratio in the adaptor module.
we conductthe experiments on covost 2 many-to-oneexperiments, and report perplexity on dev setof three directions with diverse input languages:german-englishchinese-english(de-en),(zh-en) and estonian-english (et-en).
table 7shows our approach is not sensitive to commondownsampling ratios (4 or 8) while extremedownsampling (27) hurts performance..6 related work.
speechtranslation.
sequence-to-sequencebased speech translation has shown very goodpotential overthe traditional cascaded sys-tem (berard et al., 2016; goldwater et al., 2017;weiss et al., 2017) with end-to-end approachessurpassing cascaded system for the ﬁrst time atiwslt (ansari et al., 2020) in a shared task setting.
however, previous work also indicates that itssuccess heavily relies on large amounts of labelledtraining data, which is difﬁcult to acquire.
in order.
833de.
en.
es.
fr.
it.
pt.
target.
ecruos.deenesfritpt.
13.1/22.5*9.2/12.19.8/13.610.1/11.99.0/11.4.
12.8/20.6.
18.9/26.019.8/27.9*19.8/25.619.0/24.1.
10.2/13.823.1/32.3*.
18.6/21.718.8/20.819.8/19.6.
11.6/14.922.1/30.0*19.0/21.8.
19.1/20.0*18.1/18.6.
6.6/8.614.9/21.513.3/15.413.8/15.2.
15.6/16.1.
10.4/13.020.7/28.420.0/21.919.7/21.419.8/19.2.
table 5: zero-shot performance (baseline/xmef) on europarl.
baseline is a many-to-many multilingual modeltrained on parallel data from all 30 directions.
for our approach (xmef), only to- and from- english directions( shaded ) were used in multilingual ﬁnetuning while the rest are results of zero-shot translation.
bold are whereour model (en-only and zero-shot for the rest) outperforms a supervised many-to-many model.
* means that ourzero-shot model also beats the supervised cascade model in (iranzo-s´anchez et al., 2020)..enc.
dec.ppl ↓.
params (%).
ln.
ln + ea.
5.17.
69.4m (8.8%).
- ln - ln- ea+ sa.
+ sa.
37.665.975.265.53.
69.3m (8.7%)19.0m (2.4%)119.8m (15.1%)170.2m (21.5%).
table 6: ablation on lna-minimalist ﬁnetuning,where we evaluate the effect of ﬁnetuning layernorm(ln) and attention parameters.
the experiment wasconducted on the covost english-german dataset andwe report perplexity on the dev set.
% indicates whatpercentage of total parameters of pretrained modulesare trained during ﬁnetuning..# layers stride.
de.
zh.
et.
3.
23.
2.
23.
5.88.
26.72.
35.46.
5.7911.92.
25.9232.07.
34.0142.33.table 7: ablation on length adaptor with differentdownsampling ratios of speech input.
the experimentwas conducted on the covost x-english multilingualﬁnetuning and we report perplexity on the dev set (ppl↓) for three distinct languages..to mitigate the data scarcity issue, recent researchwork focuses on multi-task learning (weiss et al.,2017; anastasopoulos and chiang, 2018; baharet al., 2019; wang et al., 2020c,d; indurthi et al.,2020; di gangi et al., 2019), pretraining differentcomponents of the model (b´erard et al., 2018;bansal et al., 2019), transfer learning (gaido et al.,2020; liu et al., 2019) and generating syntheticdata (jia et al., 2018; pino et al., 2020)..pretraining and finetuning.
our work is mo-tivated by the recent success of self-supervisedlearning for nlp and speech processing applica-tions (radford, 2018; devlin et al., 2019; clarket al., 2019; lewis et al., 2019; lample and con-.
neau, 2019; dong et al., 2019; liu et al., 2020;tang et al., 2020; rivi`ere et al., 2020; kawakamiet al., 2020; chung and glass, 2020; baevski et al.,2020), which has achieved state-of-the-art resultswhen ﬁnetuning on downstream tasks in nlp (liuet al., 2020; devlin et al., 2019; raffel et al., 2020;tang et al., 2020).
our work attempts to leveragepretrained components from different modalities(text and speech) to perform the st task.
how toefﬁciently adapt large pretrained models has gainedgrowing interest.
(houlsby et al., 2019) and (pfeif-fer et al., 2020) represent the stream of work whichadds additional “adaptor modules” to achieve fastadaptation to downstream tasks.
another categoryof solutions focus selective ﬁnetuning (only sub-set of parameters) suitable for downstream tasks.
our work belongs to the second category of efﬁ-cient ﬁnetuning without adding extra parameters(e.g.
adaptor modules).
empirical studies showsthat ﬁnetuning the ﬁnal layers of bert accountfor most of the quality gains on downstream tasks(kovaleva et al., 2019; lee et al., 2019).
fine-tuning layernorm parameters was also found ef-fective for adapting pretrained bart or mbartfor machine translation (stickland et al., 2020).
ageneral approach is to automatically learn whichlayers/parameters from a large-pretrained model toﬁnetune and freeze (guo et al., 2019), which wefound is an exciting direction for future work..7 conclusionwe proposed a simple and effective approach toleverage pretrained single-modality models (suchas wav2vec 2.0, mbart) to perform speech-to-text translation.
on two large-scale multilingualspeech translation benchmarks, our approach ad-vances the state-of-the-art (+6.6 bleu on averagefor 36 translation directions in covost 2, and +5.6bleu for 28 translation directions in europarl)..834we provide an efﬁcient ﬁnetuning strategy whichis not only data- and parameter-efﬁcient, but alsodemonstrates crosslingual transfer ability by onlyﬁnetuning 10 ∼ 50% of the parameters of largepretrained models..references.
antonios anastasopoulos and david chiang.
2018.tied multitask learning for neural speech translation.
in naacl-hlt..ebrahim ansari, amittai axelrod, nguyen bach,ondˇrej bojar, roldano cattoni, fahim dalvi, nadirdurrani, marcello federico, christian federmann,jiatao gu, fei huang, kevin knight, xutai ma, ajaynagesh, matteo negri, jan niehues, juan pino, eliz-abeth salesky, xing shi, sebastian st¨uker, marcoturchi, alexander waibel, and changhan wang.
2020. findings of the iwslt 2020 evalu-ation campaign.
in proceedings of the 17th in-ternational conference on spoken language trans-lation, pages 1–34, online.
association for compu-tational linguistics..naveen arivazhagan, ankur bapna, orhan firat,dmitry lepikhin, melvin johnson, maxim krikun,mia xu chen, yuan cao, george foster, colincherry, et al.
2019. massively multilingual neuralmachine translation in the wild: findings and chal-lenges.
arxiv preprint arxiv:1907.05019..alexei baevski, henry zhou, abdelrahman mohamed,and michael auli.
2020. wav2vec 2.0: a frame-work for self-supervised learning of speech represen-tations..parnia bahar, tobias bieschke, and hermann ney.
2019. a comparative study on end-to-end speechto text translation.
in asru..sameer bansal, herman kamper, karen livescu,adam lopez, and sharon goldwater.
2019. pre-training on high-resource speech recognition im-inproves low-resource speech-to-text translation.
proceedings of the 2019 conference of the northamerican chapter of the association for computa-tional linguistics: human language technologies,volume 1 (long and short papers), pages 58–68,minneapolis, minnesota.
association for computa-tional linguistics..alexandre b´erard, laurent besacier, ali can ko-cabiyikoglu, and olivier pietquin.
2018. end-to-end automatic speech translation of audiobooks.
inicassp..alexandre berard, olivier pietquin, christophe servan,and laurent besacier.
2016. listen and translate: aproof of concept for end-to-end speech-to-text trans-lation.
in nips..yu-an chung and james glass.
2020..improvedspeech representations with multi-target autoregres-sive predictive coding.
in acl..kevin clark, minh-thang luong, quoc v. le, andchristopher d. manning.
2019.electra: pre-training text encoders as discriminators rather thangenerators..alexis conneau, alexei baevski, ronan collobert,abdelrahman mohamed, and michael auli.
2020.representation learn-unsupervised cross-lingualarxiv preprinting forarxiv:2006.13979..speech recognition..alexis conneau, kartikay khandelwal, naman goyal,vishrav chaudhary, guillaume wenzek, franciscoguzm´an, edouard grave, myle ott, luke zettle-moyer, and veselin stoyanov.
2019. unsupervisedcross-lingual representation learning at scale..jacob devlin, ming-wei chang, kenton lee, andkristina toutanova.
2019. bert: pre-training of deepbidirectional transformers for language understand-ing.
in naacl-hlt..mattia a di gangi, matteo negri, and marco turchi.
2019. one-to-many multilingual end-to-end speechtranslation.
in 2019 ieee automatic speech recog-nition and understanding workshop (asru), pages585–592.
ieee..li dong, nan yang, wenhui wang, furu wei, xi-aodong liu, yu wang, jianfeng gao, ming zhou,and hsiao-wuen hon.
2019. uniﬁed languagemodel pre-training for natural language understand-ing and generation.
in neurips..marco gaido, mattia antonino di gangi, mat-teo negri, and marco turchi.
2020.end-to-end speech-translation with knowledge distillation:fbk@iwslt2020..s. goldwater, adam lopez, sameer bansal, andh. kamper.
2017. towards speech-to-text transla-tion without speech recognition.
in eacl..yunhui guo, honghui shi, abhishek kumar, kristengrauman, tajana rosing, and rogerio feris.
2019.transfer learning through adaptive ﬁne-spottune:in proceedings of the ieee conferencetuning.
on computer vision and pattern recognition, pages4805–4814..neil houlsby, andrei giurgiu, stanislaw jastrzebski,bruna morrone, quentin de laroussilhe, andreagesmundo, mona attariyan, and sylvain gelly.
2019. parameter-efﬁcient transfer learning for nlp.
arxiv preprint arxiv:1902.00751..sathish reddy indurthi, houjeung han, nikhil ku-mar lakumarapu, beom seok lee, insoo chung,sang-ha kim, and chanwoo kim.
2020.end-end speech-to-text translation with modality agnos-tic meta-learning.
in icassp..835j. iranzo-s´anchez, j. a. silvestre-cerd`a, j. jorge,n. rosell´o, a. gim´enez, a. sanchis, j. civera, anda. juan.
2020. europarl-st: a multilingual cor-pus for speech translation of parliamentary debates.
in icassp 2020 - 2020 ieee international confer-ence on acoustics, speech and signal processing(icassp), pages 8229–8233..ye jia, melvin johnson, wolfgang macherey, ron j.weiss, yuan cao, chung-cheng chiu, naveen ari,stella laurenzo, and yonghui wu.
2018. leverag-ing weakly supervised data to improve end-to-endicassp, pages 7180–speech-to-text translation.
7184..kazuya kawakami, luyu wang, chris dyer, phil blun-som, and aaron van den oord.
2020. learning ro-bust and multilingual speech representations.
arxiv..olga kovaleva, alexey romanov, anna rogers, andanna rumshisky.
2019. revealing the dark secretsof bert.
arxiv preprint arxiv:1908.08593..guillaume lample and alexis conneau.
2019. cross-lingual language model pretraining.
in neurips..jaejun lee, raphael tang, and jimmy lin.
2019. whatwould elsa do?
freezing layers during transformerﬁne-tuning.
arxiv preprint arxiv:1911.03090..mike lewis, yinhan liu, naman goyal, mar-jan ghazvininejad, abdelrahman mohamed, omerlevy, ves stoyanov, and luke zettlemoyer.
2019.bart: denoising sequence-to-sequence pre-trainingfor natural language generation,translation, andcomprehension..yinhan liu, jiatao gu, naman goyal, x. li, sergeyedunov, marjan ghazvininejad, m. lewis, andl. zettlemoyer.
2020. multilingual denoising pre-arxiv,training for neural machine translation.
abs/2001.08210..yuchen liu, hao xiong, zhongjun he, jiajun zhang,hua wu, haifeng wang, and chengqing zong.
2019.end-to-end speech translation with knowledge distil-lation..jiasen lu, dhruv batra, devi parikh, and stefanlee.
2019. vilbert: pretraining task-agnostic visi-olinguistic representations for vision-and-languagetasks.
in advances in neural information process-ing systems, pages 13–23..myle ott, sergey edunov, alexei baevski, angelafan, sam gross, nathan ng, david grangier, andfairseq: a fast, extensiblemichael auli.
2019.in proceedings oftoolkit for sequence modeling.
naacl-hlt 2019: demonstrations..jonas pfeiffer, ivan vuli´c, iryna gurevych, and sebas-tian ruder.
2020. mad-x: an adapter-based frame-arxivwork for multi-task cross-lingual transfer.
preprint arxiv:2005.00052..juan pino, qiantong xu, xutai ma, mohammad javaddousti, and yun tang.
2020. self-training for end-to-end speech translation.
in interspeech..matt post.
2018. a call for clarity in reporting bleuscores.
in proceedings of the third conference onmachine translation: research papers, pages 186–191, belgium, brussels.
association for computa-tional linguistics..a. radford.
2018. improving language understanding.
by generative pre-training..colin raffel, noam shazeer, adam roberts, katherinelee, sharan narang, michael matena, yanqi zhou,wei li, and peter j liu.
2020. exploring the lim-its of transfer learning with a uniﬁed text-to-texttransformer.
journal of machine learning research,21(140):1–67..morgane rivi`ere, armand joulin, pierre-emmanuelmazar´e, and emmanuel dupoux.
2020. unsuper-vised pretraining transfers well across languages.
inicassp..steffen schneider, alexei baevski, ronan collobert,and michael auli.
2019. wav2vec: unsupervisedpre-training for speech recognition.
arxiv preprintarxiv:1904.05862..asa cooper stickland, xian li,recipes.
and marjanghazvininejad.
2020.for adaptingpre-trained monolingual and multilingual models tomachine translation..weijie su, xizhou zhu, yue cao, bin li, lewei lu,furu wei, and jifeng dai.
2019. vl-bert: pre-training of generic visual-linguistic representations.
arxiv preprint arxiv:1908.08530..chen sun, austin myers, carl vondrick, kevin mur-phy, and cordelia schmid.
2019. videobert: a jointmodel for video and language representation learn-ing.
in proceedings of the ieee international con-ference on computer vision, pages 7464–7473..y. tang, c. tran, x. li, p. chen, naman goyal, vishravchaudhary, jiatao gu, and a. fan.
2020. multi-lingual translation with extensible multilingual pre-training and ﬁnetuning.
arxiv, abs/2008.00401..yun tang, j. pino, changhan wang, xutai ma, anddmitriy genzel.
2021a.
a general multi-task learn-ing framework to leverage text data for speech to texttasks.
in icassp..yun tang, juan pino, xian li, changhan wang, anddmitriy genzel.
2021b.
improving speech transla-tion by understanding and learning from the auxil-iary text translation task.
in acl..changhan wang, yun tang, xutai ma, anne wu,dmytro okhonko, and juan pino.
2020a.
fairseq s2t:in pro-fast speech-to-text modeling with fairseq.
ceedings of the 2020 conference of the asian chap-ter of the association for computational linguistics(aacl): system demonstrations..836changhan wang, anne wu, and juan pino.
2020b.
covost 2 and massively multilingual speech-to-texttranslation.
arxiv e-prints, pages arxiv–2007..chengyi wang, yu wu, shujie liu, zhenglu yang, andming zhou.
2020c.
bridging the gap between pre-training and ﬁne-tuning for end-to-end speech trans-lation..chengyi wang, yunzhao wu, shujie liu, ming zhou,and zhenglu yang.
2020d.
curriculum pre-trainingfor end-to-end speech translation.
in acl..ron j. weiss, jan chorowski, navdeep jaitly, yonghuiwu, and zhifeng chen.
2017.sequence-to-sequence models can directly translate foreignspeech.
in interspeech..shijie wu and mark dredze.
2019. beto, bentz, be-cas: the surprising cross-lingual effectiveness ofbert.
arxiv preprint arxiv:1904.09077..zhilin yang, zihang dai, yiming yang, jaime car-bonell, russ r salakhutdinov, and quoc v le.
2019.xlnet: generalized autoregressive pretraining forlanguage understanding.
in advances in neural in-formation processing systems, pages 5753–5763..a appendix.
a.1 description of pretrained models.
wav2vec 2.0 is a simple and powerful frameworkto learn high quality speech representation fromunlabelled audio data.
it mainly consists of twocomponents: feature encoder and context encoder.
the feature encoder, which is built from temporalconvolution layers, takes raw audio signal o asinput and generates latent speech representationz = [z1, · · ·, zt ].
they are fed to the transformerbased context encoder to generate context repre-sentations c = [c1, · · ·, ct ] with sequence levelinformation.
during pre-training, the model is op-timized with a contrastive task to distinguish truelatent from distractors.
the input to the contextencoder is with span masked.
the latent speechrepresentation z is discretized to q = [q1, · · ·, qt ]and used as targets for the frames in the maskedspan.
mbart is a sequence-to-sequence generative pre-training scheme, speciﬁcally a denoising autoen-coder (dae) to predict the original text x giveng(x) where g is a noising function that corruptstext such as random span masking and order per-mutation (liu et al., 2020).
the model is trainedwith monolingual data of n languages: d ={d1, ..., dn } where each di is a collection of doc-uments in language i. the pretraining objectiveoptimizes lθ:.
lθ =.
(cid:88).
(cid:88).
di∈d.
x∈di.
log p (x|g(x); θ) ,.
(1).
where x is an instance in language i and the dis-tribution p is parameterized by the sequence-to-sequence model..a.2 data.
the covost 2 dataset (wang et al., 2020b) isa large-scale multilingual st corpus which cov-ers translations from english into 15 languages—arabic, catalan, welsh, german, estonian, per-sian, indonesian, japanese, latvian, mongolian,slovenian, swedish, tamil, turkish, chinese, andtranslations from 21 languages into english, includ-ing spanish, french, italian, dutch, portuguese,russian in addition to the 15 target languages.
ithas total 2,880 hours of speech from 78k speak-ers.
the data could be downloaded from https://github.com/facebookresearch/covost..we provide the list of languages used in our.
experiments and their iso codes..837code.
language.
arcacydeenetesfafrjaiditlvmnnlptrusvtatrzh.
arabiccatalanwelshgermanenglishestonianspanishpersianfrenchjapaneseindonesianitalianlatvianmongoliandutchportugueserussianswedishtamilturkishchinese (sim).
we use the pretrained “mmbart 50 ﬁnetunedmany-to-one” model for many-to-one experimentsand “mmbart 50 ﬁnetuned one-to-many” forone-to-many experiments..training.
we implement all our experiments us-ing fairseq s2t (ott et al., 2019; wang et al.,2020a).
our experiments are run with 32 nvidiav100 gpus (32gb) with batch size of 256k to-kens.
we use fp16 training implemented in fairseq(ott et al., 2019).
we apply the same regulariza-tion as the baseline models such as label smoothing0.3, attention dropout probablity 0.3. we chooselearning rate among [1e − 5, 5e − 5, 1e − 4] basedon validation accuracy (measured on dev set).
formultilingual wav2vec 2.0, we enable normaliza-tion ﬂag to be consistent with pretraining.
we didnot apply any temperature adjustment in samplinglanguage pairs in training, but simply train on theempirical distribution of training data volume..evaluation.
we use the best checkpoint (with-out checkpoint averaging) according to validationloss and a beam size of 5 for decoding.
we re-port case-sensitive detokenized bleu using sacre-bleu (post, 2018), except for japanese and chi-nese translations (no word segmentation) where wereport character-level bleu..table 8: a list of 21 languages and their iso codeswith experiment results reported in this paper..a.3.
implementation details.
preprocessing.
when using wav2vec 2.0 en-coder, we use 16-bit 16khz mono-channel audiosas inputs.
when using a traditional speech recog-nition (asr) encoder, we extract 80-channel logmel-ﬁlter bank features (25ms window size and10ms shift) with utterance-level cepstral mean andvariance normalization applied.
we remove train-ing samples with more than 3,000 frames for gpumemory efﬁciency.
for preprocessing the target(text) data, we use the same vocabulary as is usedin the pretrained mbart model..the.
2.0.use.
from wav2vec.
pretrained models.
weopen-sourced modelsandmbart50 pretrained with multilingual par-allel text data.
these models can be down-from https://github.com/pytorch/loadedfairseq/tree/master/examples/wav2vecandhttps://github.com/pytorch/fairseq/tree/master/examples/multilingual.
forxmef-en, we use the 960-hour wav2vec 2.0large (lv-60) model.
for xmef-x, we use the56k-hour xlsr-53 large model.
for decoder,.
838