dexperts: decoding-time controlled text generationwith experts and anti-experts.
alisa liu♥ maarten sap♥ ximing lu♥♣ swabha swayamdipta♣.
chandra bhagavatula♣ noah a. smith♥♣ yejin choi♥♣♥paul g. allen school of computer science & engineering, university of washington♣allen institute for artiﬁcial intelligencealisaliu@cs.washington.edu.
abstract.
despite recent advances in natural languagegeneration, it remains challenging to controlattributes of generated text.
we propose dex-perts: decoding-time experts, a decoding-time method for controlled text generationthat combines a pretrained language modelwith “expert” lms and/or “anti-expert” lmsintuitively, underin a product of experts.
the ensemble, tokens only get high probabil-ity if they are considered likely by the ex-perts and unlikely by the anti-experts.
we ap-ply dexperts to language detoxiﬁcation andsentiment-controlled generation, where weoutperform existing controllable generationmethods on both automatic and human evalua-tions.
moreover, because dexperts operatesonly on the output of the pretrained lm, it iseffective with (anti-)experts of smaller size, in-cluding when operating on gpt-3.
our workhighlights the promise of tuning small lms ontext with (un)desirable attributes for efﬁcientdecoding-time steering..1.introduction.
controlling the output of pretrained language mod-els (lms) is crucial for achieving useful and safelanguage generation applications, such as non-offensive sentence completion or friendly conversa-tion generation (see et al., 2019; sheng et al., 2020;gehman et al., 2020).
for example, a safe comple-tion to the prompt “when she rejected his advance,he grabbed...” requires avoiding word choices thatcould lead to continuations with gender-based vio-lence (e.g., “her”; figure 1)..without such steering, these language modelsrisk generating mindless and offensive content(sheng et al., 2019; holtzman et al., 2020) whichhinders their safe deployment (brockman et al.,importantly, as the2020; bender et al., 2021).
scale of pretrained lms increases (e.g., 175b and1.6t parameters; brown et al., 2020; fedus et al.,.
figure 1: illustration of dexperts, where a toxic lmacts as an “anti-expert” and a non-toxic lm acts as an“expert”.
in this toy example, given the prompt, “whenshe rejected his advance, he grabbed,” the toxic lmassigns greater weight to “her” than “his”, expressingsubtle signals of toxicity that can be leveraged for effec-tive attribute control.
the difference in logits z` ´ z´output by the expert and anti-expert represents the per-turbations to make to the logits z of the pretrained“base” lm..2021), ﬁnetuning or re-training approaches are be-coming increasingly computationally infeasible formost researchers..we propose dexperts,1 a decoding-timemethod for controlled text generation based on a.
1dexperts.
for decoding-time experts.
our code is available at https://github.com/alisawuffles/dexperts..stands.
proceedingsofthe59thannualmeetingoftheassociationforcomputationallinguisticsandthe11thinternationaljointconferenceonnaturallanguageprocessing,pages6691–6706august1–6,2021.©2021associationforcomputationallinguistics6691product of experts (hinton, 2002).
our methodcombines an out-of-the-box pretrained (“base”)lm with “expert” lms and/or “anti-expert” lms,which model text with desirable and undesirable at-tributes, respectively.
by generatively modelingtext with particular attributes and directly com-bining the output distributions from each lm,dexperts leverages subtle signals expressibleby language models for effective attribute control,without sacriﬁcing generation ﬂuency or diversity.
moreover, because it operates only on the out-put of the base lm, dexperts can steer with(anti-)experts of smaller size, even in cases wherewe do not have full access to the base model (e.g.,gpt-3 through an api)..we ﬁrst apply dexperts to the task of languagedetoxiﬁcation (§3), by ﬁnetuning an expert and ananti-expert on public comments that are human-annotated for toxicity.
our experimental resultsshow that dexperts can successfully avoid toxi-city in language generation while preserving out-put ﬂuency, outperforming existing detoxiﬁcationmethods on both automatic and human evaluations.
moreover, we ﬁnd that dexperts continues tooutperform baselines when employing only an anti-expert and re-using the base model as the expert,making it one of the only methods that can avoidtoxicity without annotated examples of non-toxiccontent.
in analysis, we also show that our methodsuccessfully avoids toxic degeneration while usingjust „650 toxic comments, opening avenues foreasily customizable anti-experts..we then showcase the generalizability of dex-perts by tackling the task of controlling the senti-ment of lms’ output (§4).
to this end, we combinea pretrained lm with (anti-)experts modeling pos-itive and negative sentiment.
as with languagedetoxiﬁcation, dexperts outperforms existingsentiment steering methods on both automatic andhuman evaluations.
additionally, we show ourmethod is especially effective in the adversarialsetting of steering negative prompts toward pos-itive continuations, and vice versa.
finally, wedemonstrate a preliminary proof-of-concept usingdexperts for stylistic rewriting (§5)..our work demonstrates the effectiveness of tun-ing small lms on text with desirable and undesir-able properties for efﬁcient and effective steering oflarger pretrained lms, and highlights the promiseof decoding-time methods for controlled languagegeneration..2 experts and anti-experts for.
controlled generation.
given input text as a prompt, the task of controlledtext generation is to generate a continuation thatﬂows naturally from the prompt while having thedesired attribute (e.g., positive sentiment) but notan undesired one (e.g., toxicity)..given a prompt xăt, the language model com-putes the logits for the tth token, denoted zt p r|v|,where v is the vocabulary.
a probability distribu-tion over the vocabulary is obtained by normalizingand exponentiating zt:.
p pxt | xătq “ softmaxpztq,.
(1).
and the next token is generated by sampling xt „p pxt | xătq..2.1 dexperts formalization.
dexperts operates on a pretrained languagemodel m by combining its predictions with anexpert m `, which models text with a desirableattribute, and an anti-expert m ´, which modelstext with an undesirable attribute.
at time step t,we condition each language model m , m `, andm ´ on the prompt xăt to obtain zt, z`t , and z´t ,respectively.
the product-of-experts ensemble isgiven by:2.
˜p pxt | xătq “ softmax.
`zt ` α.
`z`t ´ z´t.˘˘.
(2)where α is a hyperparameter that controls theamount of modiﬁcation to zt, and can be inter-preted as the strength of control over the basemodel.
equivalently,.
˜p pxt | xătq9p pxt | xătq.
ˆ.p `pxt | xătqp ´pxt | xătq.
˙α.
(3)intuitively, a token will only have high proba-bility if it has high probability under both p andp `, and low probability under p ´.
we can inter-pret the ratio p `pxt|xătqp ´pxt|xătq as a scaling coefﬁcient foreach token, which is used to modify the originalprobability predicted for that token..2.2 sampling from dexperts.
sampling ﬂuent output from language models com-monly requires truncating the unreliable tail of.
2though not explored in this paper, this formulation readilyaccommodates multiple experts and anti-experts, whose logitscan be respectively added or subtracted..6692the probability distribution, as in top-k (fan et al.,2018) or nucleus sampling (holtzman et al., 2020).
we adapt this intuition to our method by truncat-ing the logits z output by the base model prior tocombining with the experts.
formally, let v 1 ă vdenote the set of tokens that are a part of the top-k/top-p vocabulary of the base lm at time step t.the truncated logits z1 are given by.
has „160k comments, and the nontoxic dataset„1.4m comments.
note that our toxic dataset ishuman-annotated and out-of-domain with respectto the pretraining corpus (webtext for gpt-2)..we report results for α “ 2.0, chosen after ob-serving the tradeoff between detoxiﬁcation and ﬂu-ency, but show results for other values of α in ap-pendix d..#.
z1rvs “.
if v p v 1zrvs´8 otherwise.
3.2 evaluation.
(4).
by substituting z with z1 in equation 2, we have˘˘.
`.
`.
˜p 1pxt | xătq “ softmax.
z1t ` α.t ´ z´z`t.(5)we obtain our next token xt via pure sampling fromthe probability distribution ˜p 1pxt | xătq, whichhas non-zero probability only on tokens in v 1. inthis way, adding in the (anti-)experts can be in-terpreted as modifying the probability distributionover the candidate tokens in v 1, without any chanceof reintroducing tokens v r v 1 from the tail of theoriginal probability distribution..3 toxicity avoidance.
given that large pretrained lms are at risk of pro-ducing toxic content (sheng et al., 2019; gehmanet al., 2020), steering away from toxic “degener-ation” is crucial for their safe deployment.
ourapproach uses an anti-expert that models overt tox-icity, as well as an expert that is ﬁnetuned on non-toxic data from the same domain..note that while obtaining an lm that is trulyfree from social biases is impossible (fiske, 1993;lakoff, 1973), the “non-toxic” expert serves thepurpose of modeling the same domain of commentsas the toxic anti-expert, providing more effectivecontrast.
nonetheless, we provide an ablation usingonly a toxic anti-expert and show that it remainseffective above all previous baselines..3.1 method.
we use gpt-2 large as our base lm.
for our expertand anti-expert, we ﬁnetune several sizes of gpt-2(small, medium, large) on a dataset of human-annotated comments from the jigsaw unintendedbias in toxicity classiﬁcation kaggle challenge.3we consider an example toxic if ě 50% of anno-tators marked it as toxic, and nontoxic if none ofthe annotators mark it as toxic.
this toxic dataset.
3https://bit.ly/3cvg5py.
3.2.1 generation promptsto evaluate the problem of toxic degenerationwhere a user might unexpectedly receive harm-ful output from a model, we use a random sam-ple of 10k nontoxic prompts from the realtoxici-typrompts dataset (gehman et al., 2020)..3.2.2 baselinesdomain-adaptive pretraining (dapt; guru-rangan et al., 2020) we further pretrain the basemodel on the non-toxic subset of openwebtext.
this dataset is obtained by scoring the full open-webtext corpus with the toxicity classiﬁer fromperspective api4 and keeping the least toxic 2 per-cent of documents, a corpus of about 150k docu-ments, or 63m tokens, following the implementa-tion of this baseline from gehman et al.
(2020)..language models.
plug-and-play(pplm;dathathri et al., 2020) pplm uses gradientsfrom a toxicity classiﬁer to update the lm’s hiddenrepresentations.
we retrain the classiﬁer to becompatible with our larger base model size, onthe same toxicity data used in the original paper.5due to the extreme computational expense ofpplm (runtimes are shown in appendix a.4), weevaluate pplm on a random subset of 1k prompts..generative discriminators (gedi; krause et al.,2020) gedi uses a class-conditioned lm to pro-vide classiﬁcation probabilities for all possible nexttokens via bayes’ rule.
we use the toxicity class-conditioned lm released by the authors with therecommended generation hyperparameters..dexperts (anti-only) we also explore an anti-expert-only ablation of dexperts, by reusing thebase model as the expert.
to be clear, we substitutez`t “ zt in equation 1, so that we have˜p pxt | xătq “ softmax4https://github.com/conversationai/.
p1 ` αqzt ´ αz´t.(6).
`.
˘.
perspectiveapi.
5https://bit.ly/3yqicio.
6693toxicity (ó).
avg.
max.
toxicity.
toxicity prob..fluency (ó)output ppl..diversity (ò)dist-1 dist-2 dist-3.
model.
gpt-2.
pplm (10%)non-toxic expertdaptgedidexperts (anti-only).
dexperts (small)dexperts (medium)dexperts (large).
0.527.
0.5200.4850.4280.3630.352.
0.3020.3070.314.
0.520.
0.5180.4640.3600.2170.191.
0.1180.1250.128.
25.45.
32.5840.6131.2160.0352.02.
38.2032.5132.41.
0.58.
0.580.580.570.620.58.
0.560.570.58.
0.85.
0.860.860.840.840.80.
0.820.840.84.
0.85.
0.860.860.840.830.73.
0.830.840.84.table 1: results of experiments in detoxifying generations from gpt-2.
dexperts (size) indicates the size of the(anti-)experts.
fluency is measured as perplexity of generated output according to a larger gpt-2 model.
diversityis measured as the count of unique n-grams normalized by the length of text.
toxicity is measured as the averagemaximum toxicity over 25 generations and the empirical probability of generating toxic text at least once over 25generations, as judged by perspective api.
all models are evaluated on a dataset of 10k nontoxic prompts fromrealtoxicityprompts (gehman et al., 2020), except pplm, which is evaluated on a subset of 1k prompts, due tothe greater computational expense..we use the toxic anti-expert based on gpt-2 largeand the same hyperparameter value α “ 2.0..non-toxic expert finally, we consider generat-ing directly from the non-toxic expert based ongpt-2 large..for all baselines, we use nucleus sampling (holtz-man et al., 2020) with p “ 0.9 to generate up to 20tokens.
note that for our method, nucleus samplingis done as described in §2, by using the nucleusfrom the base lm.
other training and generationdetails (e.g., hyperparameters) are described in ap-pendix a..3.2.3 automatic evaluation.
we evaluate our generations for toxicity, ﬂuency,and diversity.
following previous work (gehmanet al., 2020), we characterize generation toxicity us-ing the toxicity score from perspective api, alongtwo axes: 1) the maximum toxicity over k “ 25generations, and 2) the empirical probability of gen-erating a continuation with toxicity ě 0.5 at leastonce over k “ 25 generations.
generation ﬂuencyis measured by the mean perplexity of generatedcontinuations according to a larger pretrained lm,gpt-2 xl.
generation diversity is measured usingthe mean number of distinct n-grams, normalizedby the length of text (li et al., 2016), among the25 generations for each prompt.
we report dist-1,dist-2, and dist-3 scores for distinct uni-, bi-, andtrigrams, respectively..results according to automatic metrics shownin table 1, dexperts substantially outperforms.
all existing baselines at detoxiﬁcation.
in partic-ular, dexperts (medium, large) are among themost ﬂuent controllable generation methods, whilefully preserving output diversity compared to thebase model.
moreover, the dexperts (anti-only)ablation continues to outperform baselines at detox-iﬁcation, although with a loss in ﬂuency and diver-sity that is likely due to the less effective contrastbetween the base model and anti-expert.
we reportthe per-generation runtime of each method in ap-pendix a.4 to demonstrate dexperts’s efﬁciencycompared to other decoding-time methods..3.2.4 human evaluationwhile automatic toxicity classiﬁers like perspec-tive api enable the kind of large-scale evalua-tion required for systematic comparison of meth-ods, an abundance of work shows that their ac-curacy is far from ideal (dixon et al., 2018; sapet al., 2019; davidson et al., 2019; hutchinsonet al., 2020) in part due to reliance on spuriousfeatures, which we discuss in §8.
therefore, wecarry out a human evaluation on amazon mechan-ical turk on 120 random prompts from the 10knontoxic subset.
for each prompt, we comparefour pairs of models: dexperts (large) versusgpt-2 large, pplm, dapt, and gedi.
for eachpair of models, we randomly sample two genera-tions from each model.
this results in a total of120 prompts ˆ 4 pairingspairing “ 960 com-parisons.
each comparison pair is rated by threeturkers, who select which of the two continuationsis: (1) less toxic, (2) more ﬂuent, and (3) moretopical, i.e., whether the continuation is natural,.
prompt ˆ 2 generations.
6694figure 2: results of human evaluation for detoxiﬁcation.
dexperts is rated as less toxic more often than everybaseline, and equally ﬂuent compared to the base model, gpt-2..model.
toxicity (ó).
avg.
max.
toxicity.
toxicity prob..gpt-3dexperts (large).
0.5250.293.
0.5150.111.table 2: results of experiments in detoxifying genera-tions from gpt-3..relevant, and follows logically from the prompt.
a screenshot of the user interface is provided inappendix c..results according to human evaluations, dex-perts is rated as less toxic more often than all base-lines (figure 2).
in particular, it is rated equally ﬂu-ent compared to gpt-2, yet less toxic than gpt-210% more often than the other way around.
seeappendix e for examples of generations..3.3 steering gpt-3.
we next use dexperts to steer gpt-3 ada.
be-cause the openai api6 allows access to only thetop 100 log probabilities at each time step, we canonly modify and sample from the probability dis-tribution over the top 100 tokens.
nonetheless,results in table 2 show that dexperts effectivelyreduces toxicity from gpt-3 to about the samelevel as when operating on gpt-2.
this demon-strates that dexperts requires only the output ofthe base model, and indeed, the (anti-)experts donot need to be built on the base model..performance of dexperts whenfigure 3:(anti-)experts are trained on differently-sized datasetsand evaluated at different checkpoints, calculated ona subset of 1k prompts.
for comparison, recall theavg.
max.
toxicity of gpt-2 is 0.527..on ﬁve different dataset sizes of exactly 40,960,204.8k, 1.024m, 5.12m, and 10.24m tokens; foreach dataset size, we train the expert and anti-expert for one epoch with checkpoints at every ﬁfthof an epoch.
the performance of each ensemble, atevery (anti-)expert checkpoint, is show in figure 3.we can see that even with a dataset of 40,960 to-kens („650 comments) corresponding to ă 0.4%of the original toxic dataset, we substantially re-duce toxicity from the base model to about thesame level as our strongest baseline, gedi.
(onone gpu, this corresponds to „3 minutes of ﬁne-tuning.)
nonetheless, as the size of the ﬁnetun-ing dataset for (anti-)experts increases, the perfor-mance of dexperts increases as well..3.4 analysis: dataset size.
4 sentiment-controlled generation.
in practice, gathering large amounts of toxic datamay be challenging, especially in applicationswhere we would want to customize the anti-expertlm for differing notions of harmful language.
toexplore the limited data setting, we investigatethe relationship between the dataset size used totrain the (anti-)experts and its effectiveness at steer-ing the base model.
we ﬁnetune gpt-2 large.
6https://openai.com/api/.
as a second application we consider the well-studied task of controlling the polarity of text’ssentiment (e.g., li et al., 2018; sudhakar et al.,2019), steering towards either positive or negativesentiment..4.1 method.
we use the same pretrained model from §3, gpt-2large, as our base lm.
we ﬁnetune gpt-2 (small,.
6695targetsentiment.
model.
% positive sentimentneutralprompts.
negativeprompts.
positiveprompts.
fluency (ó).
diversity (ò).
output ppl..dist-1 dist-2 dist-3.
36.4233.2031.64.
26.8043.8014.174.4318.888.72.
0.00.positive.
negative.
dexperts (large)dexperts (medium)dexperts (small).
gedipositive expertdaptdexperts (anti-only)ctrlpplm (10%).
gpt-2.
pplm (10%)ctrldexperts (anti-only)daptnegative expertgedi.
dexperts (small)dexperts (medium)dexperts (large).
99.08.
89.7479.0593.7587.4361.6739.57.
45.2540.2135.99.
94.4694.3194.57.
86.0179.8377.2460.7261.8152.68.
50.02.
39.0537.6334.0533.2824.328.73.
3.853.793.77.
45.8343.1942.08.
58.4164.3230.5246.0043.79142.11.
29.28.
181.7835.9444.2332.8665.1184.11.
39.9243.4745.91.
0.560.560.56.
0.570.590.560.650.510.62.
0.58.
0.630.500.650.580.600.63.
0.590.590.60.
0.830.830.83.
0.800.860.830.800.830.86.
0.84.
0.870.830.810.850.860.84.
0.850.850.84.
0.830.830.84.
0.790.850.840.780.860.85.
0.84.
0.860.860.780.840.850.82.
0.840.840.83.table 3: results for experiments in sentiment-controlled generation.
we consider three sets of prompts relativeto the base lm: neutral prompts, which are equally likely to lead to positive and negative generations, as wellas positive prompts and negative prompts, which lead to overwhelmingly positive and negative generations,respectively.
sentiment is measured as the mean percentage of positive generations of out of the 25 continuationsfor each prompt, according to huggingface’s sentiment analysis classiﬁer.
higher is better for positive steering(top); lower is better for negative steering (bottom)..medium, large) on a positive sentiment corpusfor our positive lm, and on a negative sentimentcorpus for our negative lm.
we use stanford senti-ment treebank (sst-5; socher et al., 2013), whichcontains movie reviews labeled by human ratersfor sentiment on a scale from 1 (very negative) to 5(very positive).
our positive dataset contains “posi-tive” and “very positive” reviews, and our negativedataset “negative” or “very negative” reviews.
eachof these sentiment datasets has about 4k reviews.
for ease of notation we consider the positive lmour expert and negative lm our anti-expert, anduse α “ ˘3.2 for steering in each direction.
thetradeoff between ﬂuency and sentiment control formany values of α is shown in §4.3..4.2 evaluation.
4.2.1 generation prompts.
in order to test our method’s ability to control sen-timent beyond the domain that the sentiment ex-perts are trained on (movie reviews), we collect adataset of 100k naturally occurring prompts fromthe openwebtext corpus (owt) (gokaslan andcohen, 2019).
details are outlined in appendix b.we generate 25 continuations for each prompt from.
the base lm, and score them using huggingface’ssentiment analysis classiﬁer (wolf et al., 2020)trained on sst-5 movie reviews.
using these gener-ations from the base lm, we build three datasets ofprompts: (1) 5k “neutral” prompts, which lead to12 or 13 positive continuations, (2) 2.5k “negative”prompts, which lead to 25 negative continuations,and (3) 2.5k “positive” prompts, which lead to 24or 25 positive continuations.
we consider the neg-ative and positive prompts adversarial settings,where the task is to steer toward the opposite senti-ment of the prompt..4.2.2 baselines.
we consider the same baselines as in §3, along witha new baseline (ctrl; keskar et al., 2019)..dapt corresponding to our dapt baseline in§3, we score all documents in openwebtext withthe huggingface sentiment classiﬁer, and keep themost positive 2% and most negative 2% (accordingto the probability of the predicted label) to obtainthe positive and negative corpora.
we perform an-other round of pretraining on each corpus to obtaina positive lm and negative lm..6696pplm as with toxicity §3, we retrain the senti-ment classiﬁer for pplm with a larger embeddingsize compatible with our base model.
the trainingdata used is sst-5.
again, we evaluate pplm ononly 10% of the prompts compared to other models,which are randomly selected: 500 neutral prompts,250 positive prompts, and 250 negative prompts..gedi we use gedi with the sentiment class-conditioned lms released by the original authors,which are trained on imdb movie reviews (maaset al., 2011).
(we ﬁnd that retraining it on sst-5 re-sults in slightly reduced performance, as discussedin appendix a.).
dexperts (anti-only) to explore whether sim-ply steering away from one sentiment will yieldthe opposite sentiment, we again explore an anti-expert-only version of dexperts.
as in §3, wereuse the base model as the expert, and use only anegative anti-expert lm for positive steering, andonly a positive anti-expert lm for negative steering.
we use α “ ˘2.0 for this setting..positive/negative experts again, we considerdecoding directly from the corresponding senti-ment expert for positive and negative steering..conditional transformer lm (ctrl; keskaret al., 2019) to control the sentiment of genera-tions from ctrl , we use the “reviews” controlcode and append a rating of “5.0” for positive gener-ations and a rating of “1.0” for negative generations.
the sentiment training examples for ctrl camefrom amazon reviews (mcauley et al., 2015)..as with toxicity experiments (§3), we use nucleussampling with p “ 0.9, and include our trainingand generation details in appendix a..4.2.3 automatic evaluationwe evaluate our generations for the target senti-ment, ﬂuency, and diversity.
to estimate sentiment,we use huggingface’s sentiment analysis classi-ﬁer, and report the mean percentage of generationsper prompt (out of 25) which are labeled positive(the rest are negative).
we evaluate ﬂuency anddiversity in the same ways as §3..results as shown in table 3, dexperts greatlyoutperforms previous controllable generation meth-ods (pplm, ctrl, dapt, gedi) on both neutralprompts and adversarial prompts.
the limited per-formance of ctrl suggests that the effectivenessof class-conditioned training on domain-speciﬁc.
data is limited to the domain of that data; train-ing on amazon reviews does not allow general-ization outside of the reviews domain.
in a sim-ilar vein, while the positive and negative expertsachieve decent performance (even performing thebest on negative prompts), they do so at the expenseof much higher output perplexity.
this contrastshows two sides of the same coin: we observe thatwhile ctrl acts like a standard language modelon out-of-domain prompts (good ﬂuency, poor con-trol), the sentiment experts are highly specializedon movie reviews and tend to steer every genera-tion toward movies (poor ﬂuency, strong control).
meanwhile, dapt is more effective while main-taining ﬂuency, because its training domain is thesame domain as the prompts domain (i.e., owt),but its performance decreases substantially in theadversarial setting which requires more active steer-ing.
we observe that the poor ﬂuency of pplm isdue to occasional generations with extremely highperplexity, suggesting cases of degenerate behavior.
dexperts with only an anti-expert is mildly effec-tive on neutral prompts (outperforming or matchingthe performance of ctrl and pplm), but worksvery poorly in the adversarial setting, conﬁrmingour intuition that steering away from negative senti-ment does not provide sufﬁciently strong guidancefor positive sentiment..4.2.4 human evaluation.
for human evaluation, we randomly choose 30 neu-tral prompts, 30 positive prompts, and 30 negativeprompts, and consider ﬁve pairs of models: dex-perts versus gpt-2, ctrl, pplm, dapt, andgedi.
for each prompt and pairing of models, wesample two generations from each model for eachsteering direction considered.
this results in a to-tal of 120 prompts ˆ 5 pairingspairing “ 1200pairs, each rated by 3 mturk workers.
we askannotators to select which generation achieves thedesired sentiment better, along with the ﬂuency andtopicality questions from §3.2.4..prompt ˆ 2 generations.
results as shown in figure 4, dexperts is sub-stantially more effective at steering toward posi-tivity on negative prompts while achieving bettertopicality and better ﬂuency compared to all otherbaselines, including gpt-2.
in the opposite settingof steering toward negativity on positive prompts,the gap in sentiment control performance betweendexperts and each of gpt-2, ctrl, dapt, andpplm is even more pronounced: dexperts is.
6697figure 4: results of human evaluation for steering toward positivity on negative prompts (left) and steering towardnegativity on positive prompts (right).
dexperts is substantially more effective at achieving the desired sentimentover every baseline..rated better than its comparison 62–78% of thetime.
while gedi achieves close to dexperts’performance in this setting, its topicality and ﬂu-ency are much worse.
the asymmetry, where nega-tive steering appears easier than positive steeringfor dexperts, is reﬂected in automatic evalua-tion as well.
we hypothesize that it is easier toderail a positive prompt with negativity than turnsomething negative into something positive; but tohuman readers, these negative continuations maybe unexpected (a similar observation was madein previous work; madotto et al., 2020).
for theneutral prompts, we see similar trends as those inthe automatic and the human adversarial evalua-tions.
due to space constraints, we include thosein appendix d.2..4.3 analysis: sentiment versus fluency.
in practice, we may want different levels of senti-ment control depending on the application (e.g., ag-gressively positive marketing pitches versus merelyfriendly chatbots).
figure 5 shows the relationshipbetween output sentiment and ﬂuency for differentchoices of α p r´3.4, 3.4s, conditioned on neutralprompts.
the smooth tradeoff suggests that α canby adjusted by a practitioner or user, dependingon their application.
in our experiments, we pickα “ ˘3.2 because the curve becomes less steep,meaning that a greater cost in ﬂuency does not re-.
figure 5: the relationship between output ﬂuency andpositivity for different values of α p r´3.4, 3.4s.
wechoose α “ ˘3.2 in our experiments.
results are cal-culated on a subset of 1k neutral prompts..turn as great of an increase in the desired sentiment.
the tradeoff between output toxicity and ﬂuencylooks very similar for dexperts detoxiﬁcation(§3), and is included in appendix d.1..5 stylistic rewriting with dexperts.
as a preliminary exploration, we go beyond gen-erating text continuations to apply dexperts tostylistic rewriting, i.e., rewriting a sentence in a tar-get style while preserving as much content as pos-sible.
we replace the base model with a pretrained.
6698autoencoder, bart (lewis et al., 2020), and usegpt-2 large sentiment (anti-)experts from §4 forsteering.
at each time step, the autoencoder basemodel conditions on both the input sequence andthe generation-so-far, whereas the (anti-)expertscondition on only the latter.
as a proof of concept,we show some examples of input/output from thissystem in table 4..input ñ output examples.
i love cats and seeing them play with yarn.
α“´4.0ýýýýýñ i love cats and seeing them play with rotten cereal..oatmilk is tasty and good for the environment.
α“´3.5ýýýýýñ oatmilk is toxic and bad for the environment..great food but horrible staff and very very rude workers!
α“2.0ýýýýñ a very nice restaurant.
table 4: examples of input/output from a preliminarysystem that applies dexperts to stylistic rewriting.
recall α ą 0 indicates positive rewriting, and α ă 0indicates negative rewriting..this exploration suggests that more innovation isrequired to apply dexperts to stylistic rewriting,but it is a promising direction.
we anticipate futurework on the subject..6 related work.
the task of controlling the output of a language gen-eration model has been widely studied by previouswork (for a review, see prabhumoye et al., 2020).
prior to using pretrained lms as a backbone, mostwork used custom neural models trained for theirrespective downstream generation tasks, includingemotion-aware text generation (ghosh et al., 2017;ficler and goldberg, 2017), attribute-aware productreview generation (dong et al., 2017), and friendlyor empathetic dialogue response generation (seeet al., 2019; rashkin et al., 2019)..since pretrained lms have shown impressivetext generation ability (radford et al., 2018, 2019),theirtwo directions have emerged to controllanguage generation:training approaches anddecoding-time approaches.
training approaches in-clude ﬁnetuning the pretrained lms on datasets thatcontain the desired attributes (gururangan et al.,2020) as well as creating a class-conditioned pre-trained lm trained on text with speciﬁc attributescontrol code preﬁxes (keskar et al., 2019).
in con-trast to our method, such approaches can only steertowards desired text attributes, they cannot steeraway from them.
additionally, training approaches.
require signiﬁcant computational resources, whichmay no longer be feasible with the size of morerecent pretrained lms (brown et al., 2020; feduset al., 2021)..decoding-time methods, a more lightweight ap-proach, have been used controlling the attributes ofgenerated text, as well as for improving its quality(li et al., 2016; holtzman et al., 2018; wellecket al., 2020).
pplm (dathathri et al., 2020) is asteering method that updates a pretrained model’shidden representations according to the gradient ofa classiﬁer with respect to the desired class.
unfor-tunately, this approach is computationally expen-sive, as shown in this and previous work (gehmanet al., 2020).
contemporaneous with our work,fudge (yang and klein, 2021) trains classiﬁerson partial sequences to predict whether an attributewill be satisﬁed in the future, and uses bayesian fac-torization to obtain the attribute-conditioned proba-bility distribution.
gedi (krause et al., 2020) usesbayes’ rule similarly, but computes classiﬁcationprobabilities using the output of class-conditionedlms rather than directly training a classiﬁer.
incontrast, our experiments show that directly ensem-bling lms’ probabilities as opposed to using themfor estimating class probabilities is more effectiveat steering text generation..7 conclusion.
we present dexperts, a method for controlledtext generation that reweights the predictions oflanguage models based on expert (and anti-expert)opinions.
in experiments for two different tasks,detoxiﬁcation and sentiment control, we show thatour method is able to effectively steer the languagemodel towards the desired generations, while pre-serving the ﬂuency and diversity of generated text.
as applications built on language models becomeubiquitous, dexperts demonstrates promise insteering these models toward safe and user-friendlygenerations..acknowledgments.
this research is supported in part by nsf (iis-1714566), darpa mcs program through niwcpaciﬁc (n66001-19-2-4031), and allen institutefor ai.
we thank openai, speciﬁcally bianca mar-tin and miles brundage, for providing access togpt-3 through the openai api academic accessprogram.
we also thank uw nlp, ai2 mosaic,and the anonymous reviewers for helpful feedback..66998 broader impact and ethical.
implications.
our study is motivated by the potential harms ofusing pretrained language models (bender et al.,2021), speciﬁcally their tendency to generate hate-ful, offensive, or toxic content (sheng et al., 2020;gehman et al., 2020).
part of our work requiresautomatically detecting toxicity in generated texts,for which we use the perspective api.7 a commer-cially deployed toxicity detection tool.
however,the mismatch between the construct of toxicity andits operationalization through an automatic classi-ﬁer can cause biased or unintended model behavior(jacobs and wallach, 2021).
speciﬁcally, recentwork has shown that such hate speech classiﬁersoverestimate the prevalence of toxicity in text thatcontains a minority identity mention (hutchinsonet al., 2020; dixon et al., 2018) or text written byracial minorities (sap et al., 2019; davidson et al.,2019), therefore having the real possibility of back-ﬁring against its very aim of fairness and inclusivedialogue.
to address this limitation, we also per-form a human evaluation of toxicity, for whichwe obtained irb approval and sought to pay ourworkers a fair wage („us$7–9/h)..we also acknowledge that any controllabledetoxiﬁcation method runs the risk of dual use(pandya, 2019), speciﬁcally, this technology couldbe used to automatically generate hateful text (e.g.,extremist texts; mcgufﬁe and newhouse, 2020).
for a broader discussion of such risks, and of therisks of large pretrained lms in general, please seebender et al.
(2021)..nevertheless, toxicity in pretrained lms is anunsolved issue (sheng et al., 2019; gehman et al.,2020).
therefore, we hope future work contin-ues to better deﬁne and evaluate the presence ofharmful language (e.g., sap et al., 2020), and todevelop systems for mitigating such language thatcan be personalized to users’ diverse experienceswith language (e.g., dealing with reclaimed slursappropriately; croom, 2013)..references.
emily bender, timnit gebru, angelina mcmillan-major, and shmargaret shmitchell.
2021. on thedangers of stochastic parrots: can language modelsbe too big?
in proceedings of the 2021 acm confer-.
7https://github.com/conversationai/.
perspectiveapi.
ence on fairness, accountability, and transparency(facct)..steven bird and edward loper.
2004. nltk: the nat-in proceedings of the acl.
ural language toolkit.
interactive poster and demonstration sessions..greg brockman, mira murati, and peter welinder..2020. openai api.
blog post..t. brown, b. mann, nick ryder, melanie subbiah,j. kaplan, prafulla dhariwal, arvind neelakan-tan, pranav shyam, girish sastry, amanda askell,sandhini agarwal, ariel herbert-voss, g. kr¨uger,t. henighan, r. child, aditya ramesh, d. ziegler,jeffrey wu, clemens winter, christopher hesse,mark chen, e. sigler, mateusz litwin, scott gray,benjamin chess, j. clark, christopher berner, sammccandlish, a. radford, ilya sutskever, and darioamodei.
2020. language models are few-shot learn-ers.
in proceedings of the 34th conference on neu-ral information processing systems (neurips)..adam m croom.
2013. how to do things with slurs:in lan-.
studies in the way of derogatory words.
guage & communication..sumanth dathathri, andrea madotto, janice lan, janehung, eric frank, piero molino, jason yosinski, androsanne liu.
2020. plug and play language mod-els: a simple approach to controlled text generation.
in proceedings of the 2020 international conferenceon learning representations (iclr)..thomas davidson, debasmita bhattacharya, and ing-mar weber.
2019. racial bias in hate speech andabusive language detection datasets.
in proceedingsof the third workshop on abusive language online..lucas dixon, john li, jeffrey sorensen, nithum thain,and lucy vasserman.
2018. measuring and mitigat-in pro-ing unintended bias in text classiﬁcation.
ceedings of the 2018 aaai/acm conference on ai,ethics, and society (aies)..li dong, shaohan huang, furu wei, mirella lapata,ming zhou, and ke xu.
2017. learning to generateproduct reviews from attributes.
in proceedings ofthe 15th conference of the european chapter of theassociation for computational linguistics (eacl)..angela fan, mike lewis, and yann dauphin.
2018. hi-erarchical neural story generation.
in proceedings ofthe 56th annual meeting of the association for com-putational linguistics (acl)..william fedus, barret zoph, and noam shazeer.
2021.switch transformers: scaling to trillion parametermodels with simple and efﬁcient sparsity.
arxiv..jessica ficler and yoav goldberg.
2017. controllinglinguistic style aspects in neural language genera-in proceedings of the workshop on stylistiction.
variation..6700susan t fiske.
1993. controlling other people: the im-pact of power on stereotyping.
american psycholo-gist..samuel gehman, suchin gururangan, maarten sap,yejin choi, and noah a. smith.
2020. realtoxic-ityprompts: evaluating neural toxic degeneration inlanguage models.
in findings of the association forcomputational linguistics (emnlp findings)..sayan ghosh, mathieu chollet, eugene laksana,louis-philippe morency, and stefan scherer.
2017.affect-lm: a neural language model for customiz-able affective text generation.
in proceedings of the55th annual meeting of the association for compu-tational linguistics (acl)..aaron gokaslan and vanya cohen.
2019. openweb-.
text corpus..suchin gururangan, ana marasovi´c,.
swabhaswayamdipta, kyle lo, iz beltagy, doug downey,and noah a. smith.
2020. don’t stop pretraining:inadapt language models to domains and tasks.
proceedings ofthethe 58th annual meeting ofassociation for computational linguistics (acl)..geoffrey e. hinton.
2002. training products of expertsin neural.
by minimizing contrastive divergence.
computation..ari holtzman, jan buys, maxwell forbes, antoinebosselut, david golub, and yejin choi.
2018.learning to write with cooperative discriminators.
in proceedings of the 56th annual meeting of theassociation for computational linguistics (acl)..ari holtzman, jan buys, maxwell forbes, and yejinchoi.
2020. the curious case of neural text degen-eration.
in proceedings of the eighth internationalconference on learning representations (iclr)..ben hutchinson, vinodkumar prabhakaran, emilydenton, kellie webster, yu zhong, and stephen de-nuyl.
2020. social biases in nlp models as barriersfor persons with disabilities.
in proceedings of the58th annual meeting of the association for compu-tational linguistics (acl)..abigail z. jacobs and hannah wallach.
2021. mea-surement and fairness.
in proceedings of the 2021acm conference on fairness, accountability, andtransparency (facct)..nitish shirish keskar, bryan mccann, lav varshney,caiming xiong, and richard socher.
2019. ctrl:a conditional transformer language model for con-trollable generation.
arxiv..ben krause, akhilesh deepak gotmare, bryan mc-cann, nitish shirish keskar, shaﬁq joty, richardsocher, and nazneen fatema rajani.
2020. gedi:generative discriminator guided sequence genera-tion.
arxiv..robin lakoff.
1973. language and woman’s place..language in society..mike lewis, yinhan liu, naman goyal, mar-jan ghazvininejad, abdelrahman mohamed, omerlevy, veselin stoyanov, and luke zettlemoyer.
2020. bart: denoising sequence-to-sequence pre-training for natural language generation, translation,and comprehension.
in proceedings of the 58th an-nual meeting of the association for computationallinguistics (acl)..jiwei li, michel galley, chris brockett, jianfeng gao,and bill dolan.
2016. a diversity-promoting objec-tive function for neural conversation models.
in pro-ceedings of the 2016 conference of the north amer-ican chapter of the association for computationallinguistics (naacl)..juncen li, robin jia, he he, and percy liang.
2018.delete, retrieve, generate: a simple approach to sen-timent and style transfer.
in proceedings of the 2018conference of the north american chapter of the as-sociation for computational linguistics (naacl)..andrew l. maas, raymond e. daly, peter t. pham,dan huang, andrew y. ng, and christopher potts.
2011. learning word vectors for sentiment analysis.
in proceedings of the 49th annual meeting of theassociation for computational linguistics (acl)..andrea madotto, etsuko ishii, zhaojiang lin, sumanthdathathri, and pascale fung.
2020. plug-and-playin findings of the associ-conversational models.
ation for computational linguistics (emnlp find-ings)..julian mcauley, christopher targett, qinfeng shi, andanton van den hengel.
2015. image-based recom-in proceed-mendations on styles and substitutes.
ings of the 38th international acm sigir confer-ence on research and development in informationretrieval (sigir)..kris mcgufﬁe and alex newhouse.
2020. the radical-ization risks of gpt-3 and advanced neural languagemodels.
arxiv..jayshree pandya.
2019. the dual-use dilemma of arti-.
ﬁcial intelligence.
forbes magazine..shrimai prabhumoye, alan w black, and ruslansalakhutdinov.
2020. exploring controllable textin proceedings of the 28thgeneration techniques.
international conference on computational linguis-tics (coling)..alec radford, karthik narasimhan, tim salimans, andimproving language under-.
ilya sutskever.
2018.standing by generative pre-training.
preprint..alec radford, jeffrey wu, rewon child, david luan,dario amodei, and ilya sutskever.
2019.lan-guage models are unsupervised multitask learners.
preprint..hannah rashkin, eric michael smith, margaret li, andy-lan boureau.
2019. towards empathetic open-domain conversation models: a new benchmark and.
6701in proceedings of the 57th annual meet-dataset.
ing of the association for computational linguistics(acl)..maarten sap, dallas card, saadia gabriel, yejin choi,and noah a. smith.
2019. the risk of racial biasin proceedings of thein hate speech detection.
57th annual meeting of the association for compu-tational linguistics (acl)..maarten sap, saadia gabriel, lianhui qin, dan ju-rafsky, noah a. smith, and yejin choi.
2020. so-cial bias frames: reasoning about social and powerin proceedings of theimplications of language.
58th annual meeting of the association for compu-tational linguistics..abigail see, stephen roller, douwe kiela, and jasonweston.
2019. what makes a good conversation?
how controllable attributes affect human judgments.
in proceedings of the 2019 conference of the northamerican chapter of the association for computa-tional linguistics (naacl)..emily sheng, kai-wei chang, prem natarajan, andnanyun peng.
2020. towards controllable biasesin language generation.
in findings of the associ-ation for computational linguistics (emnlp find-ings)..emily sheng, kai-wei chang, premkumar natarajan,and nanyun peng.
2019. the woman worked as ababysitter: on biases in language generation.
in pro-ceedings of the 2019 conference on empirical meth-ods in natural language processing and the 9th in-ternational joint conference on natural languageprocessing (emnlp-ijcnlp)..richard socher, alex perelygin, jean wu, jasonchuang, christopher d. manning, andrew ng, andchristopher potts.
2013. recursive deep modelsfor semantic compositionality over a sentiment tree-in proceedings of the 2013 conference onbank.
empirical methods in natural language processing(emnlp)..akhilesh sudhakar, bhargav upadhyay, and arjun ma-“transforming” delete, retrieve,heswaran.
2019.generate approach for controlled text style transfer.
in proceedings of the 2019 conference on empiri-cal methods in natural language processing andthe 9th international joint conference on naturallanguage processing (emnlp-ijcnlp)..teven le scao, sylvain gugger, mariama drame,quentin lhoest, and alexander rush.
2020. trans-formers: state-of-the-art natural language process-in proceedings of the 2020 conference oning.
empirical methods in natural language processing(emnlp): system demonstrations..kevin yang and dan klein.
2021. fudge: controlledin pro-text generation with future discriminators.
ceedings of the 2021 conference of the north amer-ican chapter of the association for computationallinguistics (naacl)..appendix overview.
in this supplemental material, we provide addi-tional information for producing the results of thepaper and additional results..a modeling details.
a.1 out of the box models.
we use huggingface transformers (wolf et al.,2020) versions of all pretrained models (aside fromgpt-3), implemented in the pytorch deep learningframework.
for gpt-3, we use the ada modelwhich is accessed with the openai api.8.
a.2 training details.
all training is performed on a single nvidiaquadro 6000 gpu..dexperts hyperparametersfor ﬁnetuning(anti-)experts for dexperts are given in table 5..hyperparameter.
assignment.
modelnumber of parametersnumber of stepseffective batch sizeblock sizelearning rate optimizeradam epsilonadam initial learning ratelearning rate schedulerweight decay.
gpt-2 (s/m/l)124m / 355m / 774m1-3 epochs512128adam1e-85e-5linear with no warmup0.sean welleck, ilia kulikov, stephen roller, emily di-nan, kyunghyun cho, and jason weston.
2020. neu-ral text generation with unlikelihood training.
inproceedings of the eighth international conferenceon learning representations (iclr)..table 5: hyperparameters for ﬁnetuning (anti-)expertsfor dexperts and continued pretraining in domain-adaptive pretraining (dapt).
we ﬁnetune the senti-ment (anti-)experts and all dapt models for 3 epochs,and the toxicity (anti-)experts for one epoch..thomas wolf, lysandre debut, victor sanh, julienchaumond, clement delangue, anthony moi, pier-ric cistac, tim rault, remi louf, morgan funtow-icz, joe davison, sam shleifer, patrick von platen,clara ma, yacine jernite, julien plu, canwen xu,.
the ﬁnetuning time for each model size is shownin table 6..8https://openai.com/api/.
6702size.
non-toxic.
toxic.
positive negative.
smallmediumlarge.
2h:45m7h:06m14h:35m.
18m:01s46m:52s1h:37m.
34s1m:30s3m:19s.
32s1m:24s3m:01s.
table 6: finetuning time for (anti-)experts in dex-perts, for each gpt-2 size used..dapt for our implementation of dapt in senti-ment experiments (§4), we use huggingface’s sen-timent analysis classiﬁer to ﬁlter documents fromopenwebtext () for the most positive 2% and mostnegative 2% of documents.
because the classiﬁertakes a maximum of 512 tokens as input text, weapproximate the sentiment of a document with itsﬁrst 510 tokens (a start and end token are added bythe classiﬁer).
the hyperparameters for the addi-tional phase of pretraining on the attribute data isgiven in table 5..pplm for our implementation of pplm in ex-periments, we retrain the toxicity and sentimentclassiﬁers to be compatible with our base modelgpt-2 (large), as the original paper used gpt-2medium for experiments.
we use the same train-ing datasets and hyperparameters as in the originalpplm paper..hyperparameter assignment.
embedding sizenumber of stepslearning ratebatch size.
128010 epochs1e-464.table 7: hyperparameters for training the attribute clas-siﬁers used for pplm..gedi for toxicity and sentiment steering, wedownload the class-conditioned language models(based on gpt-2 medium) made available by theoriginal authors.
as an experiment, we also alignthe ﬁnetuning data for the sentiment gedis andthe (anti-)experts used in dexperts by ﬁnetun-ing a new class-conditioned lm on sst-5 data (asopposed to imdb used by in gedi).
we foundslightly lower performance on sentiment control(„1-2%) across the settings, and therefore use theoriginal class-conditioned lms..for ﬁnetuning our experts and anti-experts are givenin table 9 and table 10..dataset size.
non-toxic.
positive.
negative.
tokensdocuments.
63,457,5361,320,876.
13,240,192264,837.
57,805,1841,208,186.table 8: dataset details for subsets of openwebtextused to obtain the dapt models..dataset size.
non-toxic.
toxic.
tokenscomments.
91,856,0001,401,762.
10,262,144159,782.table 9: dataset details for toxicity (anti-)experts..dataset size.
positive negative.
tokensmovie reviews.
116,4804,963.
108,8004,650.table 10: dataset details for sentiment (anti-)experts..a.4 generation details.
generation hyperparameters shared among allmethods are shown in table 11. hyperparame-ters for pplm generation are shown in table 12.following the recommendation of the authors,we performed a hyperparameter search for stepsize over the values t0.02, 0.06, 0.10, 0.20, 0.40u,and for number of iterations over the valuest10, 20, 40, 60u, over a small sample of twenty non-toxic prompts.
we picked step size 0.20 and 10iterations, for the best tradeoff between toxicityreduction and output ﬂuency.
due to the extremecomputational expense of this method, we werenot able to repeat the hyperparameter search forsentiment prompts..hyperparameters for gedi generation are shown.
in table 13..hyperparameter.
assignment.
number of samplestop-p (sampling)temperaturemax length.
250.9120.table 11: hyperparameters for generation with all mod-els..a.3 dataset details.
details of datasets used for further pretraining inthe dapt baselines are given in table 8, and those.
we compare the runtime for each controllablegeneration method used in §3 in table 14, all on asingle nvidia quadro 6000 gpu.. we see that.
6703hyperparameter.
assignment.
temperaturenumber of iterationsstep sizegammagm-scalekl-scalerepetition penaltygrad lengthhorizon lengthwindow length.
1100.2010.90.0111000001none.
table 12: hyperparameters for generation with pplm.
a description of each hyperparameter can be found in(dathathri et al., 2020).
hyperparameter.
assignment.
posterior weighting exponent (ω)ﬁlter p p1 ´ ρqtarget p pτ qrepetition penalty scalerepetition penalty.
300.80.8101.2.table 13: hyperparameters for generation with gedi.
a description of each hyperparameter can be found in(krause et al., 2020).
dexperts takes 2 to 3 times the time as decodingdirectly from the base model, depending on thesize of the (anti-)experts.
when using the samemodel size for the guiding language model as ingedi (gpt-2 medium), dexperts is more efﬁ-cient than gedi, and both methods are 100ˆ fasterthan pplm..model.
generation time (sec).
gpt-2 / daptdexperts (small)dexperts (medium)dexperts (anti-only)gedidexperts (large)pplm.
0.0940.1860.2400.2480.2760.33425.39.table 14: generation time (in seconds) per continua-tion of maximum length 20 tokens for toxicity experi-ments in §3, all run on the same architecture for com-parison..b collection of sentiment prompts.
we build our prompts for sentiment experiments(§4) from the openwebtext corpus (gokaslan andcohen, 2019), a corpus of english web text scrapedfrom outbound links on reddit.
we randomly sam-ple 100k documents from openwebtext and tok-enize each document into sentences.
following the.
figure 6: a histogram of the number of positive gen-erations out of 25 from gpt-2, conditioned on oursentiment prompts dataset of 100k naturally occurringprompts..creation of realtoxicityprompts (gehman et al.,2020), we split each sentence into the prompt, con-sisting of the ﬁrst half of tokens, and the continua-tion, consisting of the remaining tokens.
we keeponly prompts that are between 4 and 10 tokenslong (inclusive).
for all tokenization, we use thenltk library (bird and loper, 2004).
this resultsin 140m prompts, from which we randomly sample100k prompts..for each of the 100k prompts, we generate 25continuations from our base model, gpt-2 (large),and score the continuations for sentiment using thehuggingface sentiment classiﬁer described in §4.
the distribution of prompts with n p r0, 25s posi-tive continuations out of 25 is shown in figure 6.interestingly, we observe that more prompts havemore negative continuations than positive continu-ations than vice versa.
based on these generations,we create three sets of prompts as described in §4..c human evaluation.
our interface for human evaluation is shown in fig-ure 7. for each category, the annotator is allowedto choose either one of the continuations, or ratethe two options as equal..d additional results.
d.1 toxicity hyperparameter control.
figure 8 shows the relationship between outputtoxicity and ﬂuency for different values of α in ourmethod.
the relationship is smooth, reﬂecting thecorresponding ﬁgure for sentiment in §4.3..d.2 human evaluation on neutral prompts.
figure 9 shows the results of human evaluation onsentiment control conditioned on neutral prompts..6704figure 7: the interface on amazon mechanical turkused for collecting human evaluation in §3.
the in-terface for positive and negative sentiment evaluationin §4 is equivalent, except replacing “less toxic” with“more positive” and “more negative,” respectively..figure 8: the relationship between output ﬂuency andtoxicity for different values of α p r1.0, 2.2s, whichcontrols the strength of control.
results are calculatedon a subset of 1k nontoxic prompts..e generation examples.
examples of generations from each method aregiven in table 15 for detoxiﬁcation (§3), and table16 for sentiment control (§4)..6705figure 9: results of human evaluation comparing different methods’ ability to control sentiment on neutralprompts, as an addendum to §4..prompt: she made out with him, took sexy pictures in a photo booth, and watched him....having sex with a still-pregnant woman in the bathroom.
she brought him over to thekiss the smiling teachers who showed up to see her.
ë her is the story of kimberly turdo it — with his brothers, neighbors, female employees at the bank and other bank customers who’dadmire her enormous breasts for the ﬁrst time.
she laughed when she found out bruce kept them in to.
go about his daily chores like a good girlfriend should.
a neat piece of work who wanted her to.
play drums on the rooftop patio with his band while another time his girlfriend came over and woke him up.
gpt-2pplmdaptgedidexpertspα “ 1.0qdexpertspα “ 2.0q.
table 15: examples of generated continuations using different detoxiﬁcation methods, conditioned on a nontoxicprompt (see §3) “she made out with him, took sexy pictures in a photo booth, and watched him...”.
gpt-2.
within the market itself.
no higher education, no corporate jobs, no content factories or on-demand.
prompt: trust in automation can only evolve from....positive steering.
pplm.
ctrl.
dapt.
gedidexpertspα “ 2.0qdexpertspα “ 3.2q.
pplmctrldaptgedidexpertspα “ ´2.0qdexpertspα “ ´3.2q.
that default belief, petersson says, “what is the best technology and automation tools employed in thea strong foundation of good human skills and technology.” the author gives examples where he feelsthis trust ispeople, not from machines.
this is why i believe education is the key to unlocking automation foreducationwithin but must also nurture and empower all of our employees to help shape the future.
integrating knowledge with innovation.
today’s start-ups bring unique, complementary perspectives totransform our planet andawareness and acceptance of the importance of automation to business success and sustainability ofour communities,” says weibel.
negative steering.
the ﬁrst to the last step.
if we are serious about having the right engine for the right probleman idea to a reality.
the future of ai will be built by humans with intelligent machines creating new toolsthe bottom-up.
but some more traditional forms of automation have started to become mainstream, and itbad code to worse developers that don’t know what they’re doing as well.
that’s why youra level of dependence.
automation cannot truly be trusted when it reaches a level of dependenceon security.
bad thinking: automation will fail because its logic is incoherent and artiﬁcial and does not add any value.
table 16: examples of generated continuations using different methods for controlled text generation, conditionedon the “neutral” prompt (see §4) “trust in automation can only evolve from...”.
6706