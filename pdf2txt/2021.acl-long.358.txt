recursive tree-structured self-attention for answer sentence selection.
khalil mrini, emilia farcas, and ndapa nakashole.
university of california, san diego, la jolla, ca 92093{khalil, efarcas, nnakashole}@ucsd.edu.
abstract.
syntactic structure is an important compo-nent of natural language text.
recent top-performing models in answer sentence selec-tion (as2) use self-attention and transfer learn-ing, but not syntactic structure.
tree struc-tures have shown strong performance in taskswith sentence pair input like semantic related-ness.
we investigate whether tree structurescan boost performance in as2.
we introducethe tree aggregation transformer: a novelrecursive, tree-structured self-attention modelfor as2.
the recursive nature of our model isable to represent all levels of syntactic parsetrees with only one additional self-attentionlayer.
without transfer learning, we establisha new state of the art on the popular trecqaand wikiqa benchmark datasets.
addition-ally, we evaluate our method on four com-munity question answering datasets, and ﬁndthat tree-structured representations have limi-tations with noisy user-generated text.
we con-duct probing experiments to evaluate how ourmodels leverage tree structures across datasets.
our ﬁndings show that the ability of tree-structured models to successfully absorb syn-tactic information is strongly correlated with ahigher performance in as2..1.introduction.
motivation.
natural language text is character-ized by structure.
for instance, syntactic parse treesdecompose a sentence into syntactic groups, whichin turn are decomposed recursively until we getto single-word spans.
therefore, syntactic parsetrees have a varying number of levels that can beaccurately represented by recursive model architec-tures..tree-structured lstm networks (tai et al.,2015) are the recursive extension of lstm net-works (hochreiter and schmidhuber, 1997), andallow for syntactic trees to be represented hierarchi-cally.
tree-lstms and bidirectional tree-lstms.
figure 1: embedding a sentence with our proposedrecursive tree-structured self-attention using the corre-sponding constituency parse tree.
there is only one setof parameters for the recursive self-attention..(teng and zhang, 2017) do not represent sequenceposition information, whereas the hybrid neuralinference networks (chen et al., 2017a) representsequence position information separately from tree-structured hierarchical information..tree-structured models have been applied to thetasks of natural language inference (chen et al.,2017a), sentence pair similarity (tai et al., 2015),dependency parsing (kiperwasser and goldberg,2016), and text embeddings (mrini et al., 2019).
inthis paper, we consider the problem of answer sen-tence selection (as2), where the goal is to predictfor a question-sentence pair whether the sentencecontains an answer to the question.
given that tree-structured models have performed strongly on atask that takes a sentence pair as input – sentencepair similarity, we hypothesize that tree structurescan help in as2, another sentence pair task..the most recent top-performing model archi-tectures for answer sentence selection have beenbased on the self-attention transformer architec-ture (vaswani et al., 2017).
three of them (laiet al., 2019; garg et al., 2019; tran et al., 2020).
proceedingsofthe59thannualmeetingoftheassociationforcomputationallinguisticsandthe11thinternationaljointconferenceonnaturallanguageprocessing,pages4651–4661august1–6,2021.©2021associationforcomputationallinguistics4651<s> marrakesh </s><s> in marrakesh </s> <s> died in marrakesh </s><s> averroes </s><s> averroes died in marrakesh.
</s>self-attentionself-attentionself-attentionself-attentionself-attentionsentence embeddinguse transfer learning on large as2 datasets; anotherone (laskar et al., 2020) uses direct ﬁne-tuning onpre-trained transformer-based language encoders,whereas all three use pre-trained bert (devlinet al., 2019) and/or roberta embeddings (liuet al., 2019)..contribution.
we investigate whether tree struc-tures are useful for as2.
we introduce the tree ag-gregation transformer: a novel recursive and tree-structured self-attention model for answer sen-tence selection.
we use the syntactic parse trees ofquestions and candidate answer sentences to modelthem in a tree-structured way.
we then form rep-resentations for questions and candidate answersusing one additional self-attention layer in a recur-sive, bottom-up fashion, as shown in figure 1. welearn syntactic embeddings to represent hierarchi-cal order and phrase-level syntactic information.
we ﬁnd in an ablation study that our learned syn-tactic embeddings improve performance..without using as2 datasets for transfer learning,our model establishes a new state of the art for theclean versions of trecqa and wikiqa, two widelyused benchmark datasets in question answering andas2.
our tree-structured self-attention matches orexceeds the state of the art – which is ﬁne-tuning onroberta – on 2 out of 4 community question an-swering (cqa) datasets.
we conduct experimentsfor 3 probing tasks to establish what informationour models leverage to increase performance, andlikewise what they fail to leverage when they donot exceed baselines.
we ﬁnd that tree-structuredrepresentations that successfully absorb the pro-vided syntactic information consistently performbetter than baselines.
our probing task results sug-gest that there is more work to be done for treestructures to adapt to noisy user-generated text..2 related work.
tree-structured transformers.
to the best of ourknowledge, our method is the ﬁrst to introducetree self-attention to answer sentence selection.
there is a growing body of work incorporating treestructures in self-attention for a range of other nlptasks..nguyen et al.
(2019) introduce a transformer-based encoder-decoder thatincorporates tree-structured attention.
the tree-structured attention isaccumulated hierarchically.
a token in the tree hasas many representations as overall children, there-fore it is ﬁrst accumulated in a bottom-up fashion.
(vertically), and then horizontally to compute a to-ken’s representation.
their model is not recursiveand uses different parameters for each level.
theauthors evaluate their model in machine translationand text classiﬁcation..sun et al.
(2020) develop a tree-structured trans-former encoder-decoder architecture for code gen-eration.
here, the tree structure is based on thecode syntax.
the model uses character-level em-beddings as input..harer et al.
(2019) introduce tree-transformer:a model with a tree convolution block for correctionof code and grammar.
wang et al.
(2019) proposea model of the same name, where the model learnssyntactic parse trees in an unsupervised manner.
the model uses up to 12 layers of non-recursiveself-attention on top of a pre-trained bert..ahmed et al.
(2019) introduce constituency anddependency tree transformer models, largely in-spired by the constituency and dependency tree-lstm models (tai et al., 2015) and rvnn models(socher et al., 2011, 2012, 2013).
on 4 datasetsof semantic relatedness, natural language inferenceand paraphrase identiﬁcation, their transformermodels achieve performance on par with tree-lstm models, and do not set a new state of the art.
the authors use two convolution layers to form aparent representation from the corresponding chil-dren.
their model does not learn an explicit syntac-tic representation, and the authors do not analyzethe ﬂuctuating results..answer sentence selection (as2).
the recentstate-of-the-art models in the as2 task all use trans-fer learning from large-scale datasets, and do not in-corporate syntactic information.
all of them use astandard linear (or sequential) input format, wherethe ﬁrst input sentence is the question and the sec-ond is the candidate answer..lai et al.
(2019) introduce the gated self-attention memory network (gsamn).
it com-bines gated attention (dhingra et al., 2017; tranet al., 2017), memory networks (sukhbaatar et al.,2015) and self-attention (vaswani et al., 2017) inone model.
the authors use transfer learning withtheir stack exchange qa dataset..garg et al.
(2019) propose the tanda method:transfer and adapt.
the method is simply ﬁne-tuning directly on a pre-trained bert or robertamodel.
the transfer step is transfer learning: ﬁne-tuning a large pre-trained bert or roberta onthe asnq dataset: a large-scale answer sentence.
4652selection dataset extracted from google’s naturalquestions (kwiatkowski et al., 2019).
the secondstep is to adapt the language model ﬁne-tuned foranswer sentence selection to the smaller, targetbenchmarks trecqa and wikiqa..tran et al.
(2020) build upon the work of laiet al.
(2019).
they propose to use a neural turingmachine (graves et al., 2014) as a controller forthe memory network, instead of the gated attentionthat lai et al.
(2019) use.
like garg et al.
(2019),they use the asnq dataset for transfer learning..laskar et al.
(2020) achieve state-of-the-art re-sults on a wide range of qa and cqa datasetsby directly ﬁne-tuning on the target datasets, with-out transfer learning from an external large-scaledataset.
they show results for two methods: theﬁrst trains a self-attention layer while freezing pre-trained language model layers, and the second di-rectly ﬁne-tunes on the language model..3 tree aggregation transformer for.
answer sentence selection.
in the as2 task, the input is a pair of sentences,where the ﬁrst one is the question and the second isa candidate answer.
this is a binary classiﬁcationproblem on whether or not the candidate answersentence contains an answer to the question.
wetherefore design our model to form a representa-tion of the question and a representation of thecandidate answer, in a bottom-up tree aggregationfashion..semantic and syntactic representation.
wedeﬁne a token embedding in our input representa-tion as the concatenation of a semantic embeddingand a syntactic embedding.
the semantic embed-ding is a projection of the token embedding from agiven pre-trained language model, whereas the syn-tactic embedding contains information from part-of-speech tags, syntactic categories, and the levelwithin the syntactic parse tree..the syntactic embedding is the sum of threelearned embeddings.
the ﬁrst embedding repre-sents the token’s tag – a part-of-speech tag if thetoken is a word, or a syntactic category if the tokenis a classiﬁcation or separator token.
the secondembedding represents the token’s level within thetree, inherited from the head of the token’s con-stituent span.
our recursive model allows to rep-resent sentences with as many tree levels as thecorresponding syntax tree has.
the third embed-ding represents the position of a token within the.
constituent span, as seen in the example in figure 2.this position embedding puts the token within itsspan context, whereas the position embedding ofthe semantic (language model) embedding puts thetoken within the context of the question-sentencepair..more formally, given a token t, its languagemodel embedding xt, its position index pt, its part-of-speech tag or syntactic category st, and its treelevel lt, the token’s semantic embedding et andsyntactic embedding nt are as follows:.
et = w1 ∗ xt + b1.
(1).
nt = w2.
(cid:105)(cid:104)es [st] + ep [pt] + el [lt].
+ b2 (2).
where w1, w2, b1, b2 are learned, and es, epand el are learned embedding layers, respectivelyfor the part-of-speech tag or syntactic category, theposition index, and the tree level..recursive self-attention.
we add 1 layer ofrecursive self-attention layer on top of the languagemodel layers.
the recursive self-attention layer hasseparate attention distributions aet for thesemantic embedding et and syntactic embeddingnt:.
t and an.
(3).
(4).
(5).
(6).
aet = softmax.
ant = softmax.
(cid:19).
(cid:19).
(cid:18) qe.
t ∗ ke√de(cid:18) qnt ∗ kn√dn.
where dn and de are the dimensions of the queryand key vectors for the semantic and syntactic em-beddings respectively, and ke and kn are thelearned matrices of key vectors of input tokens.
qet and qnt are the query vectors for the token t,such that:.
t = wq,e ∗ etqe.
t = wq,n ∗ ntqnwhere wq,e and wq,n are learned.
t and on.
the resulting vectors oe.
as:.
t are computed.
t = et + wo,e ∗ (aeoe.
t ∗ ve) + bo,e.
(7).
t = nt + wo,n ∗ (anon.
t ∗ vn) + bo,n.
(8).
4653figure 2: input representation of an example question-sentence pair usingroberta..figure 3: detailed example of re-cursive tree aggregation..where ve and vn are the value vectors for theinput tokens, and wo,e, wo,n, bo,e, bo,n arelearned.
finally, we apply separate position-wisefeed-forward layers to these output vectors..usually, self-attention includes residual dropoutover the attention-weighted value vectors.
wefound in preliminary experiments that the perfor-mance on the dev set improved when we omitteddropout regularization.
we omit dropout in bothself-attention and position-wise feed-forward layer.
the recursiveness of the self-attention allows themodel to re-use the same sets of parameters acrosseach tree level, instead of training new ones as inprevious work (nguyen et al., 2019; wang et al.,2019)..constituent span embedding.
each input sen-tence is represented in a tree-structured fashion us-ing its constituency parse tree.
we use a pre-trainedparser, whose parameters are ﬁxed, to produce thetrees before training time..the constituent span is fed to the recursive self-attention as a matrix of token vectors.
this matrixincludes the embeddings of the words of the con-stituent span, preceded by a ﬁrst, start-of-sentenceembedding, and followed by an end-of-sentenceembedding.
the start-of-sentence token is the clas-siﬁcation token if the span is part of the question,or a separator token if the span is part of the candi-date sentence.
figure 3 shows how we compose aconstituent span embedding for roberta models.
the constituent span embedding is the outputembedding of the ﬁrst token.
the ﬁrst token em-bedding obtains through the recursive self-attention.
an attention-weighted sum of all of the span’s tokenembeddings.
this creates a span-speciﬁc embed-ding, conscious of the entire question-sentence pairinput as a result of the language model layers, butfocused on the tokens of a span as a result of therecursive self-attention..the ﬁrst.
in using only one layer of recursive self-attention,token embedding gets anattention-weighted sum of value vectors that con-tains token embeddings that did not go through alayer of self-attention, and syntactic embeddingsthat came directly out of the embedding layers..efﬁcient tree aggregation.
to obtain an aggre-gate sentence embedding, we proceed by embed-ding from the deepest level of the tree (the leaves)to the root, as shown in figure 3. the computa-tions are done on the same two sets of self-attentionparameters..to reduce training time, we compute the con-stituent span embeddings one level at a time.
forinstance, in figure 2, we compute the np, vp andpp groups at once when computing the span em-beddings at tree level 2..we efﬁciently compute all span embeddings onlyonce, and keep all computed span embeddings, asthey will be used in the next level..the sentence embedding is obtained from theﬁrst token output of the computation at the root ofthe tree, as shown in figure 1..prediction.
finally, we concatenate the aggre-gate embeddings for the question-sentence inputpair.
given the question’s aggregate semantic em-bedding weq and aggregate syntactic embedding.
4654input position012345678910111213token<s>wheredidaverroesdie?</s></s>averroesdiedinmarrakesh.</s>part of speechwrbvbdnnpvb.nnpvbdinnnp.
(np averroes)(vp die)(sq did (np averroes) (vp die))(whadvp where)tree levels(sbarq (whadvp where) (sq did (np averroes) (vp die)) ?
)(np marrakesh)(pp in (np marrakesh))(vp died (pp in (np marrakesh)))(np averroes)(s (np averroes) (vp died (pp in (np marrakesh))) .
)0123</s>marrakesh</s>7token embeddinginput position1113tagnpnnpnp012span position333tree levelsemantic self-attentionsyntactic self-attentioninmarrakesh</s>10token embeddinginput position1113taginnnppp113span position222tree level</s>7pp02(np marrakesh)(np marrakesh)semantic self-attentionsyntactic self-attention(pp in (np marrakesh))(pp in (np marrakesh))semantic representationsyntactic representationq, and the sentence’s aggregate semantic embed-s and aggregate syntactic embedding wns ,.
wnding wewe obtain the prediction values as follows:.
p(s|q) = softmax (cid:0)w ∗ tanh (cid:2)we.
q; wn.
q; we.
s ; wns.(cid:3) + b(cid:1).
(9)where w and b are learned.
we use binary cross-entropy as our loss function..our model can optionally include a residual con-nection, by adding the classiﬁcation token embed-ding output of the language model to the beginningof the question-sentence pair vector.
this residualconnection does not contain syntactic information,and the classiﬁcation token embedding is not pro-jected in this case..4 experiments.
4.1 datasets.
we evaluate our proposed tree aggregationtransformer on six english-language benchmarkdatasets for answer sentence selection.
the ﬁrsttwo – trecqa and wikiqa – are widely usedbenchmarks in question answering (qa).
theother four – yahoocqa and semeval 2015, 2016and 2017 – are all from the community questionanswering (cqa) domain.
we show the statisticsof these six datasets in table 1..trecqa (wang et al., 2007) is collected fromlabeled sentences of the qa track of the text re-trieval conference (trec).
over time, the datasethas evolved into two versions:the raw versionincludes all question-sentence pairs, whereas theclean version excludes questions with only non-relevant or only relevant candidate answers..wikiqa (yang et al., 2015) contains questionsoriginally sampled from bing query logs, andmatched with candidate answer sentences from theﬁrst paragraph of relevant wikipedia articles.
like-wise, it also has a raw and a clean version.
follow-ing lai et al.
(2019); tran et al.
(2020), we evaluateour method on the clean versions of trecqa andwikiqa..yahoocqa (tay et al., 2017) is a ﬁltered andpre-processed subset of the large-scale yahoo!
an-swers manner questions dataset (surdeanu et al.,2008).
the latter is based on the yahoo!
answersonline forum..semeval 2015 cqa (nakov et al., 2015) is thechallenge dataset of subtask a of task 3 of se-meval 2015. it is based on the qatar living on-line forum, and the goal is to predict the relevance.
dataset.
trecqa cleanwikiqa cleanyahoocqa.
semeval.
201520162017.number of questionstestdevtrain68651,2292431268736,2836,28950,1123293002,6003272444,8792932444,879.number of answersdev1,1171,13031,6801,6452,4402,440.train53,4178,672253,44016,54136,19836,198.test1,4422,35131,6801,9763,2702,930.table 1: statistics of the six benchmark datasets..scores of candidate answers given a question.
theoriginal subtask divides labels into three categories:deﬁnitely relevant, potentially useful, and irrele-vant.
following previous work (sha et al., 2018;laskar et al., 2020), only deﬁnitely relevant candi-date answers are marked as relevant in our binaryclassiﬁcation setting..semeval 2016 cqa (nakov et al., 2016) corre-sponds as well to subtask a of task 3 of semeval2016, about question-comment similarity.
it is anew dataset also based on the qatar living onlineforum.
the training set includes the training, de-velopment and testing sets of the semeval 2015cqa, and two new training sets.
the authors ofthe dataset have described the ﬁrst one as highlyreliable, and the second one as noisier..semeval 2017 cqa (nakov et al., 2017) is thelatest version of the community question answeringtask.
the training and development sets are thesame as the 2016 version, but the testing set isdifferent..in figure 2, we show an example of question-sentence pairs for a qa dataset and a cqa dataset.
the aim is to illustrate the difference in style andlength between formal (qa) and informal (cqa)text..4.2 setup.
the standard evaluation metrics in answer sentenceselection are mean average precision (map) andmean reciprocical rank (mrr).
both metrics arewidely used in information retrieval (ir) and areaveraged per query – in this case per question.
ourmodel produces relevance scores going from 0 (ir-relevant) to 1 (relevant) for each candidate answer,and therefore produces a list of candidate answersthat can be ranked by relevance.
whereas mrrscores how early a ﬁrst relevant answer appears inthat candidate list, map scores the order in whichall candidate answers are listed for each question.
to produce parse trees, we use the nltk part-of-speech tagger (loper and bird, 2002) trained onthe part-of-speech tagset of the english penn tree-.
4655datasetwikiqasemeval2016-2017.questionhow are glacier caves formed ?
why people are crossing red signals on doha roads?
i thinksignals are changing quickly than on dubai roads and its hardfor the motorists to control their vehicles?
moreover; motoristsare bit panic fearing the penalties as per the new trafﬁc law..answera glacier cave is a cave formed within the ice of a glacier .
also i trafﬁc lights here does not have standard options.
some haveblinking green light; some chage to yellow right away then red.
severaltimes alredy i found my self driving in the middle of the crossing inred light luckily at the moment no ﬁnes.
hehehe :) pykester.
table 2: samples of question-sentence pairs from the training sets of wikiqa and semeval 2016-2017 (both yearsshare the same training dataset).
here, the sentence contains an answer to the question..representation.
semantic onlysemantic + syntactic.
trecqa.
wikiqa.
yahoocqa.
semeval cqa.
2015.
2016-2017.map mrr map mrr map mrr map mrr map mrr0.9500.9320.9570.946.
0.9010.912.
0.8920.898.
0.9290.933.
0.9290.933.
0.9590.962.
0.9110.914.
0.9580.961.
0.9470.945.table 3: ablation study on syntactic representations: results for our tree aggregation transformer with andwithout learned syntactic embeddings for all of our benchmark dev sets, on roberta large..bank (ptb) (marcus et al., 1994), and the english-language parser of mrini et al.
(2020), which is thestate of the art on the parse trees of the ptb..model.
4.3 training parameters.
we use 1 layer of recursive self-attention for alldatasets.
we use the residual connection describedin §3 for trecqa only.
for all our models, weuse either bert large or roberta large, so as tomatch our baselines.
our recursive self-attentionlayers have: 16 attention heads, a feed-forward di-mension of 4096, and a hidden dimension of 2048.we use half of the dimensions to encode seman-tic information, and the rest to encode syntacticinformation..4.4 ablation study on syntactic embeddings.
we perform an ablation study by removing the syn-tactic embedding part of the input representation.
in this experiment, we are quantifying the addedvalue of the learned syntactic embeddings for spanposition, part-of-speech tags and syntactic cate-gories, and tree levels..our results on the dev sets are in table 3. se-meval 2016 and 2017 results are the same sinceboth have the same dev set.
across all as2 datasets,we notice that there is an advantage to learningsyntactic embeddings, as the sum of mrr andmap scores are higher for the variant that includeslearned syntactic embeddings.
the advantage isclearer for qa datasets, suggesting that formal lan-guage tends to beneﬁt more from learned syntacticinformation.
we use syntactic embeddings in ournext experiments..4.5 baselines.
we conside ﬁve strong baselines, described in §2:.
trecqa.
wikiqa.
map mrr map mrr0.7310.7810.7640.8210.7270.7840.7450.8230.7430.8410.758--0.8650.776-0.716-.
0.8510.8990.8650.8890.917-0.904--.
0.7210.7540.7120.7360.7300.746-0.7620.700.chen et al.
(2017b)bian et al.
(2017)tay et al.
(2018)chen et al.
(2018a)chen et al.
(2018b)sha et al.
(2018)madabushi et al.
(2018)tymoshenko and moschitti (2018)kamath et al.
(2019)models using bert largegsamn (lai et al., 2019)*tanda (garg et al., 2019)*reg.
self-attention (laskar et al., 2020)direct fine-tuning (laskar et al., 2020)our tree aggregation transformermodels using roberta large0.974tanda (garg et al., 2019)*0.978direct fine-tuning (laskar et al., 2020)0.985our tree aggregation transformermodels using roberta large and evidence memory0.993evidence memory (tran et al., 2020)*0.995our tree aggregation transformer.
0.9570.9670.8870.9670.961.
0.9140.9120.7890.9050.917.
0.9430.9360.950.
0.9610.970.
0.857-0.7140.8430.851.
-0.9000.906.
0.9360.941.
0.872-0.7310.8570.868.
-0.9150.920.
0.9520.958.table 4: our results in comparison with recent work onthe trecqa and wikiqa benchmark datasets.
* indi-cates use of transfer learning on large-scale datasets..(1) gsamn (lai et al., 2019): gated self-attention memory networks.
(2) tanda (garg et al., 2019): the two-step trans-fer and adapt method.
(3) regular self-attention (laskar et al., 2020):a self-attention layer ﬁne-tuned over frozen bertlarge embeddings.
(4) direct fine-tuning (laskar et al., 2020): di-rectly ﬁne-tuning on a pre-trained language model.
(5) evidence memory (tran et al., 2020): the neu-ral turing machine as memory controller..baselines 1, 2, and 5 are available only ontrecqa and/or wikiqa, whereas baselines 3 and4 use the exact same datasets as we do..4.6 results and discussion.
the results of our experiments with the qa datasetsare in table 4, and the results of our experiments.
4656with cqa datasets are in table 5..4.6.1 state of the art in qa datasets.
our results in table 4 establish a new state of theart in trecqa and wikiqa, two widely used bench-mark datasets in answer sentence selection..in trecqa, our average of map and mrrscores matches the one for tanda (garg et al.,2019) in bert, without any transfer learning on alarge dataset.
this shows that our model is able toleverage the tree structure to increase performanceon relatively small datasets..for the roberta results in wikiqa, the addedvalue between the direct ﬁne-tuning and our re-cursive self-attention conﬁrms that our model isbeneﬁcial to formally written text, such as the onefound in wikipedia..the increase in performance compared to theevidence memory models (tran et al., 2020) whenwe add our tree representation shows that our treeaggregation method brings about a consistent androbust added value for the qa datasets..4.6.2 limitations in cqa datasets.
as shown in table 5, our tree aggregation trans-former is able to establish a new state of the artin semeval 2015, and our bert-based versionexceeds other bert-based baselines.
however,our method scores below the state of the art in ya-hoocqa and semeval 2016, and only manages tomatch the mrr – but not the map – of the stateof the art in semeval 2017..therefore, there is a contrast in the performanceof our recursive tree-structured self-attention be-tween the qa and the cqa datasets.
the differ-ence lies in the style of the datasets, as questionsand sentences can be much longer in qa datasetsthan in cqa datasets.
on average, a training setpair in qa has 32 words for wikiqa, and 39 wordsin trecqa, whereas a training set pair in cqahas 78 words for semeval 2015, 85 words for se-meval 2016-2017, and 40 words for yahoocqa.
as shown in the example, cqa pairs may also havespelling mistakes or lack coherent structure.
thus,the informal writing style and larger text length ofcqa datasets may be decreasing the ability of ourmodel to leverage tree structures.
accordingly, wesee that our model achieves very competitive scoresfor yahoocqa, and that it has a text length that isvery close to the qa datasets.
the semeval 2015exception could be explained by the fact that the2015 training dataset is less noisy than the 2016-.
2017 training dataset, as pointed out by the authorsof the semeval cqa datasets..4.7 do tree structures improve.
performance?.
we investigate how tree structures are leveragedin the answer sentence selection task across thedifferent datasets.
we evaluate our tree-structuredrepresentations and compare them with the corre-sponding sequential representations, using threeprobing tasks from conneau et al.
(2018)..4.7.1 probing tasks.
the three probing tasks are as follows:(1) top constituent prediction.
this task looksto predict the top constituent sequence of thequestion-sentence pair: the sequence of syntacticcategories immediately below the s (sentence) syn-tactic category.
following conneau et al.
(2018),we deﬁne this task as a 20-way classiﬁcation prob-lem, where the ﬁrst 19 classes are the 19 most popu-lar top constituent sequences, and the last categoryis for all the remaining top constituent sequences.
(2) tree depth prediction.
the tree depth is thenumber of hops from the root node of the syntactictree to the lowest-level leaf nodes.
(3) input length regression.
this tasks inves-tigates whether the embedding is aware of howmany words it contains.
the length of the question-sentence pair input is deﬁned as the number of itstokens – full words and punctuation symbols..the ﬁrst two tasks are syntactic, and investi-gate whether our tree-structured representationsabsorbed the syntactic category information thatwe fed it – respectively syntactic categories and treelevels – and whether that information was alreadypresent in the sequential representations..4.7.2 probing experiment setup.
in our probing experiments, we consider all sixdatasets used both in our work and in laskar et al.
(2020).
we consider the sequential representa-tion of a question-answer pair to be the classiﬁ-cation token embedding used for prediction in theroberta-based models of laskar et al.
(2020).
we take our own roberta-based tree-structuredmodels (without evidence memory), where we con-sider the tree-structured representation to be theclassiﬁcation token embedding fed to the predic-tion layer.
the tree-structured and sequential repre-sentations have the same number of dimensions..4657model.
nakov et al.
(2017)tay et al.
(2018)sha et al.
(2018)models using bert largeregular self-attention (laskar et al., 2020)direct fine-tuning (laskar et al., 2020)our tree aggregation transformermodels using roberta largedirect fine-tuning (laskar et al., 2020)our tree aggregation transformer.
yahoocqa.
2015.semeval cqa2016.
2017.map mrr map mrr map mrr map mrr0.928--.
-0.801-.
-0.801-.
0.884--.
--0.872.
--0.801.
---.
---.
0.7780.9510.946.
0.9550.949.
0.7780.9510.946.
0.9550.949.
0.8830.9350.946.
0.9470.961.
0.9230.9610.972.
0.9700.981.
0.7650.8660.844.
0.8880.863.
0.8310.9270.900.
0.9380.918.
0.8670.9210.902.
0.9430.926.
0.9220.9630.955.
0.9740.974.table 5: our results in comparison with recent work on the yahoocqa and semevalcqa benchmark datasets..probing task.
representation.
trecqa wikiqa yahoocqa.
top constituentprediction (f1 score)tree depthprediction (f1 score)input lengthregression (mse).
tree-structuredsequentialtree-structuredsequentialtree-structuredsequential.
0.15730.04750.15680.04810.02660.0822.
0.19490.04630.16380.04760.02730.1200.
0.03540.03640.03540.03544.51e-064.14e-06.
20150.20580.04340.16820.04510.06520.2915.semeval20160.06740.05050.06210.05230.09890.3338.
20170.11510.04830.13400.04810.04160.1484.spearman’s ρmrrmap.
0.8214.
0.9550.
0.8214.
0.9550.
-0.0360.
0.1429.table 6: results for three probing tasks comparing sequential (laskar et al., 2020) and tree-structured (ours)representations.
in the last two columns, we show the spearman correlation of the probing task and the as2performance differences between the tree-structured and sequential representations..the probing model architecture is a simple mlpwith a layer of the same size as the input embed-dings, a relu activation, and a prediction layer.
we train 36 probing models for each of the 36combinations of a probing task, a dataset and a rep-resentation type.
the input embeddings are frozen,so that the training does not change the weights ofthe pre-trained as2 models.
all experiments aretrained for the same number of epochs, and use thesame train/dev/splits as as2 experiments..4.7.3 probing results and discussion.
our probing experiment results are shown in ta-ble 6. we compute the spearman correlations ofthe added values of the tree-structured representa-tions compared to the sequential representationsin each probing task with the same added value inthe as2 task.
we compute the added value of thetree representation in a given task by subtractingthe performance of the sequential representations(laskar et al., 2020) from the performance of thetree-structured representations (ours)..for the syntactic probing tasks (the ﬁrst two),the tree-structured representation gets an f1 scoreabout 3 to 4 times higher than the one obtained bythe sequential representation in 4 datasets: trecqa,wikiqa, and semeval 2015 and 2017. these 4datasets correspond to the ones in which our tree-.
structured as2 models set a new state of the art ormatched the performance of the ﬁne-tuning base-line of laskar et al.
(2020).
in the other datasets,the tree-structured representation’s f1 score is justslightly higher than the sequential representation’sf1 score, if not about the same.
this shows thatwhen the tree-structured representations success-fully absorb the syntactic information we fed it,there is a consistent increase in performance inthe answer sentence selection task.
the high cor-relation values for both map and mrr conﬁrmthat successfully absorbing syntactic informationis associated with higher performance in as2.
theweakness of tree-structured representations in cer-tain datasets may be due to the lack of general-ization of syntactic parsers trained on the penntreebank..in the input length probing experiment, we ob-serve that the mean-squared error (mse) of thetree-structured representations is consistently andsigniﬁcantly lower than the one of the sequentialrepresentations, except for yahoocqa.
this showsthat the recursion of our tree-structured as2 modelmakes representations aware of the length of theirquestion-sentence pair, but the correlation valuesshow that this information does not necessarily helpin the as2 task..46585 conclusions.
we introduce the tree aggregation transformer: anovel, recursive and tree-structured self-attentionmodel for as2.
our method embeds sentencesby aggregating word representations following thecorresponding parse tree.
we show that our modelleverages tree structure and, through an ablationstudy, that its learned syntactic embeddings in-crease performance.
our method establishes anew state of the art in the trecqa and wikiqabenchmark datasets with only one additional self-attention layer.
our tree-structured self-attentionexceeds or matches the state of the art in 2 out of4 cqa datasets, where text is informal and longer.
to investigate this mixed performance, we devise 3probing tasks to examine what our tree-structuredrepresentations learn compared to their sequentialcounterparts.
we ﬁnd that there is a strong cor-relation between a tree-structured model’s abilityto absorb syntactic information and its ability toincrease performance in the as2 task compared tobaselines.
our ﬁndings suggest that there is morework to be done for tree-structured representationsto adapt to noisy user-generated text..acknowledgements.
we gratefully acknowledge the award fromnih/nia grant r56ag067393.
this work is partof the voli project (mrini et al., 2021; johnsonet al., 2020).
we thank the anonymous reviewersfor their feedback..references.
mahtab ahmed, muhammad rifayat samee, androbert e mercer.
2019. you only need attention toin proceedings of the 57th annualtraverse trees.
meeting of the association for computational lin-guistics, pages 316–322..weijie bian, si li, zhao yang, guang chen, andzhiqing lin.
2017. a compare-aggregate modelwith dynamic-clip attention for answer selection.
inproceedings of the 2017 acm on conference on in-formation and knowledge management, cikm ’17,page 1987–1990, new york, ny, usa.
associationfor computing machinery..qian chen, xiaodan zhu, zhen-hua ling, si wei,hui jiang, and diana inkpen.
2017a.
enhancedlstm for natural language inference.
in proceed-ings of the 55th annual meeting of the associationfor computational linguistics (volume 1: long pa-pers), pages 1657–1668, vancouver, canada.
asso-ciation for computational linguistics..qin chen, qinmin hu, jimmy xiangji huang, andliang he.
2018a.
ca-rnn: using context-aligned re-current neural networks for modeling sentence simi-larity.
in thirty-second aaai conference on artiﬁ-cial intelligence..qin chen, qinmin hu, jimmy xiangji huang, andliang he.
2018b.
can: enhancing sentence sim-ilarity modeling with collaborative and adversarialnetwork.
in the 41st international acm sigir con-ference on research & development in informationretrieval, pages 815–824..qin chen, qinmin hu, jimmy xiangji huang, lianghe, and weijie an.
2017b.
enhancing recurrent neu-ral networks with positional attention for questionanswering.
in proceedings of the 40th internationalacm sigir conference on research and develop-ment in information retrieval, pages 993–996..alexis conneau, germ´an kruszewski, guillaume lam-ple, lo¨ıc barrault, and marco baroni.
2018. whatyou can cram into a single $&!#* vector: probingsentence embeddings for linguistic properties.
inproceedings of the 56th annual meeting of the as-sociation for computational linguistics (volume 1:long papers), pages 2126–2136..jacob devlin, ming-wei chang, kenton lee, andkristina toutanova.
2019. bert: pre-training ofdeep bidirectional transformers for language under-standing.
in proceedings of the 2019 conference ofthe north american chapter of the association forcomputational linguistics: human language tech-nologies, volume 1 (long and short papers), pages4171–4186..bhuwan dhingra, hanxiao liu, zhilin yang, williamcohen, and ruslan salakhutdinov.
2017. gated-in pro-attention readers for text comprehension.
ceedings of the 55th annual meeting of the associa-tion for computational linguistics (volume 1: longpapers), pages 1832–1846, vancouver, canada.
as-sociation for computational linguistics..siddhant garg, thuy vu, and alessandro moschitti.
2019. tanda: transfer and adapt pre-trained trans-former models for answer sentence selection.
arxivpreprint arxiv:1911.04118..alex graves, greg wayne,.
2014. neural turing machines.
arxiv:1410.5401..and ivo danihelka.
arxiv preprint.
jacob harer, chris reale, and peter chin.
2019. tree-transformer: a transformer-based method for cor-arxiv preprintrection of tree-structured data.
arxiv:1908.00449..sepp hochreiter and j¨urgen schmidhuber.
1997.neural computation,.
long short-term memory.
9(8):1735–1780..janet johnson, khalil mrini, allison moore, emiliafarkas, ndapa nkashole, michael hogarth, andnadir weibel.
2020. voice-based conversational.
4659agents for older adults.
in proceedings of the chi2020 workshop on conversational agents for healthand wellbeing, honolulu, hawaii..sanjay kamath, brigitte grau, and yue ma.
2019. pre-dicting and integrating expected answer types into asimple recurrent neural network model for answerin 20th international confer-sentence selection.
ence on computational linguistics and intelligenttext processing..eliyahu kiperwasser and yoav goldberg.
2016. easy-ﬁrst dependency parsing with hierarchical tree lstms.
transactions of the association for computationallinguistics, 4:445–461..tom kwiatkowski, jennimaria palomaki, olivia red-ﬁeld, michael collins, ankur parikh, chris alberti,danielle epstein, illia polosukhin, matthew kelcey,jacob devlin, kenton lee, kristina n. toutanova,llion jones, ming-wei chang, andrew dai, jakobuszkoreit, quoc le, and slav petrov.
2019. natu-ral questions: a benchmark for question answeringresearch.
transactions of the association of compu-tational linguistics..tuan lai, quan hung tran, trung bui, and daisukekihara.
2019. a gated self-attention memory net-in proceedings of thework for answer selection.
2019 conference on empirical methods in natu-ral language processing and the 9th internationaljoint conference on natural language processing(emnlp-ijcnlp), pages 5955–5961..md tahmid rahman laskar, xiangji huang, and ena-mul hoque.
2020.contextualized embeddingsbased transformer encoder for sentence similaritymodeling in answer selection task.
in proceedingsof the 12th language resources and evaluationconference, pages 5505–5514..yinhan liu, myle ott, naman goyal, jingfei du, man-dar joshi, danqi chen, omer levy, mike lewis,luke zettlemoyer, and veselin stoyanov.
2019.roberta: a robustly optimized bert pretraining ap-proach.
arxiv preprint arxiv:1907.11692..edward loper and steven bird.
2002. nltk: the natu-ral language toolkit.
in proceedings of the acl-02workshop on effective tools and methodologies forteaching natural language processing and com-putational linguistics - volume 1, etmtnlp ’02,page 63–70, usa.
association for computationallinguistics..harish tayyar madabushi, mark lee, and john barn-den.
2018.integrating question classiﬁcation andindeep learning for improved answer selection.
proceedings of the 27th international conference oncomputational linguistics, pages 3283–3294..mitchell marcus,.
grace kim, mary annmarcinkiewicz, robert macintyre, ann bies,mark ferguson, karen katz, and britta schasberger.
1994. the penn treebank: annotating predicate.
argument structure.
in proceedings of the workshopon human language technology, hlt ’94, page114–119, usa.
association for computationallinguistics..khalil mrini, chen chen, ndapa nakashole, nadirweibel, and emilia farcas.
2021. medical questionunderstanding and answering for older adults.
the3rd southern california (socal) nlp symposium..khalil mrini, franck dernoncourt, quan hung tran,trung bui, walter chang, and ndapa nakashole.
2020. rethinking self-attention: towards inter-pretability in neural parsing.
in proceedings of the2020 conference on empirical methods in natu-ral language processing: findings, pages 731–742,online.
association for computational linguistics..khalil mrini, claudiu musat, michael baeriswyl, andmartin jaggi.
2019. structure tree-lstm: structure-arxivaware attentional document encoders.
preprint arxiv:1902.09713..preslav nakov, doris hoogeveen, llu´ıs m`arquez,alessandro moschitti, hamdy mubarak, timothybaldwin, and karin verspoor.
2017. semeval-2017task 3: community question answering.
in proceed-ings of the 11th international workshop on semanticevaluation (semeval-2017), pages 27–48..preslav nakov, llu´ıs m`arquez, walid magdy, alessan-dro moschitti, jim glass, and bilal randeree.
2015.semeval-2015 task 3: answer selection in commu-nity question answering.
in proceedings of the 9thinternational workshop on semantic evaluation (se-meval 2015), pages 269–281, denver, colorado.
as-sociation for computational linguistics..preslav nakov, llu´ıs m`arquez, alessandro moschitti,walid magdy, hamdy mubarak, abed alhakim frei-hat, jim glass, and bilal randeree.
2016. semeval-in2016 task 3: community question answering.
proceedings of the 10th international workshop onsemantic evaluation (semeval-2016), pages 525–545..xuan-phi nguyen, shaﬁq joty, steven hoi, andtree-structured attentionin international.
richard socher.
2019.with hierarchical accumulation.
conference on learning representations..lei sha, xiaodong zhang, feng qian, baobao chang,and zhifang sui.
2018. a multi-view fusion neu-ral network for answer selection.
in thirty-secondaaai conference on artiﬁcial intelligence..richard socher, brody huval, christopher d manning,and andrew y ng.
2012. semantic compositional-ity through recursive matrix-vector spaces.
in pro-ceedings of the 2012 joint conference on empiricalmethods in natural language processing and com-putational natural language learning, pages 1201–1211..4660quan hung tran, gholamreza haffari, and ingrid zuk-erman.
2017. a generative attentional neural net-work model for dialogue act classiﬁcation.
in pro-ceedings of the 55th annual meeting of the associa-tion for computational linguistics (volume 2: shortpapers), pages 524–529, vancouver, canada.
asso-ciation for computational linguistics..kateryna tymoshenko and alessandro moschitti.
2018.cross-pair text representations for answer sentenceselection.
in proceedings of the 2018 conference onempirical methods in natural language processing,pages 2162–2173..ashish vaswani, noam shazeer, niki parmar, jakobuszkoreit, llion jones, aidan n gomez, łukaszkaiser, and illia polosukhin.
2017. attention is allin advances in neural information pro-you need.
cessing systems, pages 5998–6008..mengqiu wang, noah a smith, and teruko mita-mura.
2007. what is the jeopardy model?
a quasi-synchronous grammar for qa.
in proceedings of the2007 joint conference on empirical methods in nat-ural language processing and computational nat-ural language learning (emnlp-conll), pages22–32..yaushian wang, hung-yi lee, and yun-nung chen.
2019. tree transformer: integrating tree structuresinto self-attention.
in proceedings of the 2019 con-ference on empirical methods in natural languageprocessing and the 9th international joint confer-ence on natural language processing (emnlp-ijcnlp), pages 1060–1070..yi yang, wen-tau yih, and christopher meek.
2015.wikiqa: a challenge dataset for open-domain ques-in proceedings of the 2015 con-tion answering.
ference on empirical methods in natural languageprocessing, pages 2013–2018, lisbon, portugal.
as-sociation for computational linguistics..richard socher, cliff chiung-yu lin, andrew y ng,and christopher d manning.
2011. parsing natu-ral scenes and natural language with recursive neuralnetworks.
in icml..richard socher, alex perelygin, jean wu, jasonchuang, christopher d manning, andrew y ng,and christopher potts.
2013. recursive deep mod-els for semantic compositionality over a sentimenttreebank.
in proceedings of the 2013 conference onempirical methods in natural language processing,pages 1631–1642..sainbayar sukhbaatar, jason weston, rob fergus, et al.
2015. end-to-end memory networks.
in advancesin neural information processing systems, pages2440–2448..zeyu sun, qihao zhu, yingfei xiong, yican sun, lilimou, and lu zhang.
2020. treegen: a tree-basedtransformer architecture for code generation.
inaaai, pages 8984–8991..mihai surdeanu, massimiliano ciaramita, and hugozaragoza.
2008. learning to rank answers on largein proceedings of acl-08:online qa collections.
hlt, pages 719–727..kai sheng tai, richard socher, and christopher d.manning.
2015. improved semantic representationsfrom tree-structured long short-term memory net-in proceedings of the 53rd annual meet-works.
ing of the association for computational linguisticsand the 7th international joint conference on natu-ral language processing (volume 1: long papers),pages 1556–1566, beijing, china.
association forcomputational linguistics..yi tay, minh c phan, luu anh tuan, and siu che-ung hui.
2017. learning to rank question answerpairs with holographic dual lstm architecture.
inproceedings of the 40th international acm sigirconference on research and development in informa-tion retrieval, pages 695–704..yi tay, luu anh tuan, and siu cheung hui.
2018.hyperbolic representation learning for fast and efﬁ-cient neural question answering.
in proceedings ofthe eleventh acm international conference on websearch and data mining, pages 583–591..zhiyang teng and yue zhang.
2017. head-lexicalizedbidirectional tree lstms.
transactions of the associ-ation for computational linguistics, 5:163–177..quan hung tran, nhan dam, tuan lai, franck der-noncourt, trung le, nham le, and dinh phung.
2020.explain by evidence: an explainablememory-based neural network for question answer-ing.
in proceedings of the 28th international con-ference on computational linguistics, pages 5205–5210..4661