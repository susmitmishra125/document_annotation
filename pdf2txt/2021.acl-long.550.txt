raw-c: relatedness of ambiguous words—in context(a new lexical resource for english).
sean trottuniversity of california, san diegosttrott@ucsd.edu.
benjamin bergenuniversity of california, san diegobkbergen@ucsd.edu.
abstract.
most words are ambiguous—they convey dis-tinct meanings in different contexts—and eventhe meanings of unambiguous words arecontext-dependent.
both phenomena present achallenge for nlp.
recently, the advent of con-textualized word embeddings has led to suc-cess on tasks involving lexical ambiguity, suchas word sense disambiguation.
however,there are few tasks that directly evaluate howwell these embeddings accommodate the con-tinuous, dynamic nature of word meaning—particularly in a way that matches human in-tuitions.
we introduce raw-c, a datasetof graded, human relatedness judgments for112 ambiguous words in context (with 672sentence pairs total), as well as human es-timates of sense dominance.
the averageinter-annotator agreement for the relatednessnorms (assessed using a leave-one-annotator-out method) was 0.79. we then show that ameasure of cosine distance, computed usingcontextualized embeddings from bert andelmo, correlates with human judgments, butthat cosine distance also systematically under-estimates how similar humans ﬁnd uses of thesame sense of a word to be, and systematicallyoverestimates how similar humans ﬁnd uses ofdifferent-sense homonyms.
finally, we pro-pose a synthesis between psycholinguistic the-ories of the mental lexicon and computationalmodels of lexical semantics..1.introduction.
words mean different things in different contexts.
sometimes these meanings appear to be distinct, aphenomenon known as lexical ambiguity.
in en-glish, approximately 7% of wordforms are homony-mous, i.e., they have multiple, unrelated meanings1(e.g., “tree bark” vs. “dog bark”), and as many.
1dautriche (2015) estimates the average rate of homonymy.
across languages to be 4%..as 84% of wordforms are polysemous, i.e., theyhave multiple, related meanings (e.g., “pet chicken”vs. “roast chicken”) (rodd et al., 2004).
but evenunambiguous words evoke subtly different inter-pretations depending on the context of use, i.e.,their meanings are dynamic and context-dependent(yee and thompson-schill, 2016; li and joanisse,2021).
while the uses of runs in “the boy runs” vs.“the cheetah runs” may not be considered distinctmeanings, a human comprehender will likely ac-tivate a different mental image when processingeach sentence (elman, 2009)..these facts present a challenge for computa-tional models of lexical semantics.
any down-stream task that involves meaning requires modelscapable of disambiguating among the multiple pos-sible meanings of an ambiguous word in a givencontext.
further, the graded nature of human se-mantic representations can inﬂuence how compre-henders construe events and participants in thoseevents (elman, 2009; li and joanisse, 2021).
inturn, a number of natural language processing(nlp) tasks could beneﬁt from context-sensitiverepresentations that go beyond discrete sense rep-resentations and capture the manner in which hu-mans construe events—including sentiment analy-sis, bias detection, machine translation, and more(trott et al., 2020).
if an eventual goal of nlp ishuman-like language understanding, models mustbe equipped with semantic representations thatare ﬂexible enough to accommodate the dynamic,context-dependent nature of word meaning—as hu-mans appear to do (elman, 2009; li and joanisse,2021)..yet a crucial prerequisite to developing bettermodels is evaluating those models along the rel-evant dimensions of performance.
thus, at theminimum, we need metrics that evaluate a modelalong two critical dimensions:.
proceedingsofthe59thannualmeetingoftheassociationforcomputationallinguisticsandthe11thinternationaljointconferenceonnaturallanguageprocessing,pages7077–7087august1–6,2021.©2021associationforcomputationallinguistics70771. disambiguation: a model’s ability to distin-guish between distinct meanings of a word.
2. contextual gradation: a model’s ability tomodulate a given meaning in context, in waysthat reﬂect the continuous nature of humanjudgments..a promising development in recent years is therise of contextualized word embeddings, producedusing neural language models such as bert (de-vlin et al., 2018), elmo (peters et al., 2018), xl-net (yang et al., 2019), and more.
advances inthese models have yielded improved performanceon a number of tasks, including word sense dis-ambiguation (wsd) (boleda et al., 2019; loureiroet al., 2020)..wsd satisﬁes the disambiguation criterionabove, but not the contextual gradation criterion.
in fact, there is still a dearth of metrics for assessingthe degree to which contextualized representationsmatch human judgments about the way in whichcontext shapes meaning..in section 2, we describe several related datasetsthat satisfy at least one of these criteria.
in section3, we introduce and describe the dataset construc-tion process for raw-c: relatedness of ambigu-ous words—in context.2 in section 4, we describethe procedure we followed for collecting humanrelatedness norms for each sentence pair.
in sec-tion 5, we report the results of several analyses thatprobe how well contextualized embeddings fromtwo neural language models (bert and elmo)predict these norms.
finally, in section 6, we ex-plore possible shortcomings in current models, andpropose avenues for future work..2 related work.
most existing datasets fulﬁll either the disam-biguation or the contextual gradation criterion,but few datasets fulﬁll both (see haber and poesio(2020a) for an exception)..several datasets contain human relatedness andsimilarity judgments for distinct words in isolation(see section 2.1).
others are used for word sensedisambiguation, and contain ambiguous words indifferent sentence contexts, along with annotatedsense labels (see section 2.2); as noted in the intro-duction, wsd fulﬁlls the disambiguation criterion,but not the contextual gradation criterion.
several.
2the dataset can be found on github: https://.
github.com/seantrott/raw-c..recent datasets contain graded relatedness judg-ments for words in different contexts (see section2.3).
however, none focus speciﬁcally on gradedrelatedness judgments for ambiguous words, con-trolling both the inﬂection and part of speech of thetarget word in question.
finally, one dataset (haberand poesio, 2020a) contains similarity judgmentsfor polysemous words in context, but is more lim-ited in size and does not match the sentence frameacross the two uses (see section 2.4)..2.1 de-contextualized word similarity and.
relatedness.
several datasets contain human judgments of thesimilarity or relatedness of (mostly english) wordpairs, in isolation (see taieb et al.
(2020) for a re-view).
this includes simlex-999 (hill et al., 2015),simverb-3500 (gerz et al., 2016), wordsim-353(finkelstein et al., 2001), mturk-771 (halawi et al.,2012), men (bruni et al., 2014), and more.
thesedatasets are primarily used for evaluating the qual-ity of static semantic representations, includingdistributed semantic models such as glove (pen-nington et al., 2014), as well as representations thatuse knowledge bases like wordnet (faruqui anddyer, 2015)..however, these resources are (by deﬁnition, asdecontextualized judgments) not directly amenableto evaluating how well a model incorporates con-text into its semantic representation of a givenword..2.2 word sense disambiguation.
in word sense disambiguation (wsd), a classiﬁerpredicts the “sense” of an ambiguous word in agiven context, often using a contextualized embed-ding.
wsd relies on annotated sense labels, whichin turn requires determining whether any givenpair of word uses belong to the same or distinctsenses—i.e., whether to “lump” or “split”.
thereis considerable debate about how granular wordsense inventories should be (hanks, 2000; brown,2008a);3 resources range in granularity from word-net (fellbaum, 1998) to the coarse sense inven-tory, or csi (lacerra et al., 2020).
recent work us-ing coarse-grained sense inventories has achievedsuccess rates of 85% and beyond (lacerra et al.,.
3this also raises deeper philosophical issues about exactlywhat qualiﬁes as a “sense” (hanks, 2000; tuggy, 1993; geer-aerts, 1993; kilgarriff, 2007); answering these questions isbeyond the scope of this paper, though see section 6 for abrief discussion..70782020; loureiro et al., 2020)..in terms of the criteria listed above, wsd satis-ﬁes the disambiguation criterion, but not the con-textual gradation criterion.
wsd only capturesa model’s ability to distinguish between distinctsenses; it does not assess how meaning is mod-ulated within a given sense category, e.g., that ahuman comprehender might consider the meaningof runs in “the cheetah runs” as more similar to“the jaguar runs” than to “the toddler runs”, or thatsome uses of a sense might be more prototypicalthan others..2.3 contextualized word similarity and.
relatedness.
there have been several recent efforts to addressthis gap in the literature:.
the stanford contextual word similarity(scws) dataset (huang et al., 2012) contains sim-ilarity judgments for 2,003 english word pairs ina sentence context.
approximately 12% of thepairs contain the same word (e.g., “pack his bags”vs. “pack of zombies”), though not always in thesame part of speech; in most cases, the words com-pared are different (e.g., “left” vs. “abandon”).
this dataset is a useful step towards contextualizedsimilarity judgments, but because most pairs con-tain different words (or the same word in differentparts of speech), static word embeddings such asword2vec can still perform quite well without con-sidering the context at all (pilehvar and camacho-collados, 2018)..the word in context (wic) dataset (pilehvarand camacho-collados, 2018) contains a set ofover 7,000 sentence pairs with an overlapping en-glish word, labeled according to the use of thatword corresponds to same or different senses.
aspilehvar and camacho-collados (2018) note, thestructure of the dataset requires some form ofcontextualized meaning representation to performabove a random baseline, which makes it more suit-able for interrogating contextualized embeddings.
however, the task is a binary classiﬁcation taskalong the lines of wsd, making it harder to assessthe contextual gradation criterion..the cosimlex dataset (armendariz et al., 2020),created with the graded word similarity in con-text (gwsc) task, contains graded similarity judg-ments for a number of word pairs across english(340), croatian (112), slovene (111), and finnish(24).
each pair of words is rated in two sepa-.
rate contexts, yielding 1174 scores in total.
sen-tence contexts were extracted from each language’swikipedia.
unlike wic, the word pairs do not actu-ally contain the same word—rather, for any givenword pair (e.g., “beach” and “seashore”), there areat least two pairs of sentence contexts with associ-ated similarity judgments.
thus, this dataset can beused to assess graded differences in contextualizedmeaning representations, but not directly for thesame ambiguous word..2.4 contextualized similarity of ambiguous.
words.
finally, one dataset (haber and poesio, 2020a,b)contains graded similarity judgments (as well as co-predication acceptability judgments) for a numberof polysemous words in distinct sentential contexts,meeting both contextual gradation and the dis-ambiguation criteria..the main limitations of this dataset are its size(it contains examples for only 10 polysemes), aswell as the fact that the sentence frames are alsonot always controlled for each polysemous word..2.5 summary.
most datasets reviewed above allow practitionersto evaluate models on their ability to disambiguate(i.e., the disambiguation criterion) or their abil-ity to capture graded differences in word related-ness (i.e., the contextual gradation criterion); onedataset (haber and poesio, 2020a,b) meets bothcriteria..but to our knowledge, no datasets contain gradedrelatedness judgments for ambiguous words intightly controlled sentence contexts, with inﬂec-tion and part-of-speech controlled across each use.
in section 3 below, we describe the procedure wefollowed for constructing such a dataset..3 raw-c: relatedness of ambiguous.
words, in context.
items were adapted from stimuli used in past psy-cholinguistic studies, which contrasted behavioralresponses to homonymous and polysemous words,either in isolated lexical decision tasks (klepous-niotou and baum, 2007) or in a disambiguatingcontext (klepousniotou, 2002; klepousniotou et al.,2008; brown, 2008b).
we selected 115 words intotal.
for each ambiguous word (e.g., “bat”), wecreated four sentences: two each for two distinctmeanings of the word.
we attempted to match.
7079the sentence frames as closely as possible, in mostcases altering only a single word4 across the foursentences to disambiguate the intended meaning:.
1a.
he saw a fruit bat.
1b.
he saw a furry bat.
2a.
he saw a wooden bat.
2b.
he saw a baseball bat..we also labeled each word according to whetherthe two distinct meanings were judged by lexicog-raphers to be polysemous or homonymous.
distin-guishing homonymy from polysemy is notoriouslychallenging (valera, 2020); common tests includedetermining whether the two meanings share an et-ymology (polysemy) or not (homonymy), or deter-mining whether the two meanings are conceptuallyrelated (polysemy) or not (homonymy).
both testscan be criticized on multiple grounds (tuggy, 1993;valera, 2020), and do not always point in the samedirection (e.g., etymologically related words some-times drift apart, resulting in apparent homonymy).
for our annotation, we consulted both the on-line merriam-webster dictionary (https://www.
merriam-webster.com/) and the oxford englishdictionary, or oed (https://www.oed.com/),and identiﬁed whether each dictionary listed thetwo meanings in question in separate lexical en-tries (homonymy), or as different senses under thesame lexical entry (polysemy).5 for example, bothdictionaries list the animal and meat senses of theword “lamb” as different senses under the same lex-ical entry, whereas they list the animal and artifactsenses of the word “bat” under different lexicalentries.
there was one word (“drill”) on whichthe two dictionaries did not agree; in this case, welabeled the two meanings (“electric drill” vs. “gru-eling drill”) as homonymy (as per the oed)..there were also three words for which neitherdictionary distinguished the two meanings (eitherin terms of homonymy or polysemy).
for example,“best-selling novel” and “thick novel” refer to cul-tural and physical artifacts, respectively, but are notlisted as distinct senses.
again, this highlights the.
4there were 13 words for which at least one of the foursentences used a different article (“a” vs. “an”), in addition tohaving a different disambiguating word..5our primary goal with this labelling was not to deﬁni-tively distinguish homonymy from polysemy; as noted above,there is no single, universal criterion for doing so, and differentcriteria might be more or less relevant for different purposes.
it was simply to specify how lexicographers treat the differ-ent words, in case that information is useful for users of theresource..challenge of distinguishing outright ambiguity fromcontext-dependence; these items were included inthe annotation study described below, but were ex-cluded from the ﬁnal set of norms, thus resultingin 112 target words altogether.6 each word wasused in four sentences, for a total of six sentencepairs (see table 1 for more details).
84 of the targetwords were nouns, and 28 were verbs (note thatpart-of-speech was always held constant withineach word)..ambiguity typehomonymypolysemy.
#words3874.
#sentence pairs228444.table 1: number of words (and sentence pairs) for eachtype of ambiguity..4 human annotation.
4.1 participants.
81 participants were recruited through uc sandiego’s undergraduate subject pool for psychol-ogy, cognitive science, and linguistics students.
participants received class credit for participation.
three participants were removed for failing thebot checks at the beginning of the study, and onewas removed for failing the catch trials embeddedin the experiment, leaving 77 participants in total(59 female, 16 male, 2 non-binary).
the medianage of participants was 20 (m = 20.22, sd = 2.7),with ages ranging from 18 to 38.
74 participantsself-reported as being native speakers of english..4.2 materials.
we used the original set of 115 words describedin section 3, i.e., including the three items labeled“unsure”.
each word had four sentences; account-ing for order, this resulted in twelve possible sen-tence pairs (six pairs, with two orders each) foreach word, for a total 1380 items..6the existence of these “unsure” items, as well as itemsfor which the two dictionaries disagreed on the issue ofhomonymy vs. polysemy, raises the question of whether em-pirical measurements such as relatedness judgments (or evencosine distance) could help inform lexicographic decisions.
as a proof of concept, we trained a logistic regression classi-ﬁer (using leave-one-out cross-validation) to predict whethertwo contexts of use belonged to the same sense, using meanrelatedness.
the classiﬁer successfully categorized 86.76%of held-out test items as belonging to the same or differentsenses.
further, for different sense items only, a trained clas-siﬁer successfully categorized 79% of held-out test items aspolysemous or homonymous.
while only a proof of concept,this demonstration suggests a promising avenue for futureresearch..70804.3 procedure.
after giving consent, participants answered twoquestions designed to ﬁlter out bots (e.g., “whichof the following is not a place to swim?”, with thecorrect answer being “chair”).
they were thengiven instructions, which included a description ofhow the meaning of a word can change in differentcontexts..on each page of the study, participants wereshown a pair of sentences, with the target wordbolded (see figure 1 for an example).
they wereasked to indicate how related the uses of that wordwere across the two sentences, with a labeled likertscale ranging from “totally unrelated” to “samemeaning”..human annotations were assigned to a scale from0 (“totally unrelated”) to 4 (“same meaning”)..5.1 analysis of sentence pairs.
before analyzing the responses of human annota-tors, we ﬁrst sought to characterize how well twoneural language models captured the categoricalstructure in the dataset—i.e., whether their con-textualized representations could be used to distin-guish same-sense from different-sense uses of thesame word, as well as words labelled as different-sense homonyms from different-sense polysemes.
we ran every sentence through two languagemodels: elmo, using the python allennlp pack-age (gardner et al., 2017), and bert, using thebert-embedding package.8 then, for eachsentence pair, we computed the cosine distancebetween the contextualized representations of thetarget wordform (e.g., bat in “he saw the furry bat”and “he saw the wooden bat”).
the distribution ofcosine distances is visualized in figure 2..figure 1: example item from study..we included two “catch” trials in the study toidentify participants who did not pay attention.
inone, the two sentences were identical, such thatthe correct answer is “same meaning”; the otherfeatured a homonym with two different parts ofspeech (rose.v and rose.n), such that the correctanswer was “totally unrelated”..excluding the catch trials, participants saw 115sentence pairs total; no word was repeated twiceacross trials for the same participant.
the compar-isons any given subject saw for a given word wererandomly sampled from the 12 possible sentencepairs, and the order of trials was randomized.7.
5 analysis and results.
the analyses run below were performed on the 112target words (i.e., excluding the “unsure” items)..7based on the suggestion of an anonymous reviewer, wealso ran a follow-up norming study to collect estimates ofsense frequency bias (sometimes called dominance); sensedominance is known to play an important role in the process-ing of ambiguous words (klepousniotou and baum, 2007;rayner et al., 1994; binder and rayner, 1998; leinenger andrayner, 2013).
these dominance norms are included in theﬁnal dataset..figure 2: cosine distances between the target word’scontextualized embeddings for both language models,plotted by same sense (true vs. false) and ambiguitytype (homonymy vs. polysemy)..we also performed several statistical analyses,using the lme4 package in r (bates et al., 2015).
in each case, we compared a full model to a reducedmodel using a log-likelihood ratio test.
all mod-els had cosine distance as a dependent variable,and included part-of-speech as a ﬁxed effect, ran-dom intercepts for word and language model (i.e.,elmo vs. bert), and by-word random slopes forthe effect of same sense..adding a ﬁxed effect of same sense signiﬁcantlyimproved model ﬁt [χ2(1) = 143.72, p < .001],with same-sense uses signiﬁcantly closer thandifferent-sense uses [β = −.099, se = 0.005]..8https://pypi.org/project/.
bert-embedding/.
7081homonymypolysemy0.00.20.40.60.80.00.20.40.60.8bertelmocosine distancemodelsamefalsetruehowever, adding an interaction between samesense and ambiguity type (as well as ﬁxed effectsof both) did not signiﬁcantly improve the ﬁt above amodel omitting the interaction [χ2(1) = 2.19, p =0.14].
in other words, both language models coulddifferentiate same-sense and different-sense uses ofan ambiguous word, but their ability to discriminatebetween homonymy and polysemy was marginalat best..5.2 analysis of human annotations.
our primary goal was understanding the distribu-tion of human relatedness annotations—both interms of how it reﬂects the underlying categori-cal structure of the dataset (e.g., homonymy vs.polysemy), as well as the cosine distance mea-sures from each language model.
as in the sectionabove, we constructed a series of linear mixed ef-fects models and performed log-likelihood ratiotests for each model comparison; in each case, thedependent variable was relatedness.
all models in-cluded a ﬁxed effect of part-of-speech, by-subjectand by-word random slopes for the effect of samesense, by-subject random slopes for the effect ofambiguity type, and random intercepts for sub-jects and items..first, we asked whether participants’ relatednessjudgments varied across same-sense and different-sense sentence pairs.
we added a ﬁxed effect ofsame sense to the base model described above,along with ﬁxed effects for the cosine distancemeasures from bert and elmo.
this model ex-plained signiﬁcantly more variance than a modelomitting only same sense [χ2(1) = 207.11, p <.001], with same-sense uses receiving higher relat-edness judgments on average [β = 1.94, se =0.1].
the median relatedness judgment for same-sense uses was 4 (m = 3.46, sd = 1.02), whilethe median relatedness judgment for different-sense uses was 1 (m = 1.31, sd = 1.45).
second, we asked whether participants’ judg-ments were sensitive to the distinction betweenhomonymy and polysemy.
we added an inter-action between same sense and ambiguity type(along with a ﬁxed effect of ambiguity type) tothe model described above.
the interaction signif-icantly improved model ﬁt [χ2(1) = 25.45, p <.001].
the median relatedness for both same-sensehomonyms and polysemes was 4, whereas the me-dian relatedness for different-sense homonyms (0)was lower than that for different-sense polysemes.
(2).
further, as depicted in figure 3, there was con-siderably more variance across polysemous wordsthan homonymous words—this makes sense, giventhat some polysemous meanings are highly related(e.g., “pet chicken” vs. “roast chicken”), whileothers are more distant (e.g., “desperate act” vs.“magic act”)..figure 3: mean relatedness judgments for each sen-tence pair, plotted by by same sense (true vs. false)and ambiguity type (homonymy vs. polysemy)..third, we asked whether the cosine distancemeasures explained independent variance aboveand beyond that explained by same sense andambiguity type.
a full model including all fac-tors explained more variance than a model exclud-ing only the cosine distance measure from bert[χ2(1) = 36.19, p < .001], as well as a modelexcluding only the cosine distance measure fromelmo [χ2(1) = 16.92, p < .001].
this indicatesthat relatedness does not vary purely as a func-tion of the categorical structure in the dataset—thegraded relatedness judgments were sensitive to sub-tle differences in context..5.3.inter-annotator agreement.
inter-annotator agreement was assessed by calcu-lating the average spearman’s rank correlation be-tween each participant’s responses and the meanrelatedness for the set of 112 items observed bythat participant—where mean relatedness was cal-culated after omitting responses by the participantin question.
this answers the question: to what ex-tent do each participant’s responses correlate withthe consensus rating by the 76 other participants?
using this method, the average correlation wasρ = 0.79, with a median of ρ = 0.81 (sd = .07).
the lowest agreement was ρ = 0.55, and the high-est was ρ = 0.88..7082homonymypolysemy01234mean relatedness judgmentambiguity typesamefalsetrue5.4 evaluation of language models.
6 discussion.
to evaluate the language models, we collapsedacross the single-trial data and computed the meanand median relatedness for each unique sentencepair.
the distribution of mean relatedness judg-ments is depicted in figure 3..as in past work (hill et al., 2015), we computedthe spearman’s rank correlation between the dis-tribution of cosine distance measures (from eachmodel) and the mean relatedness for a given sen-tence pair.
bert performed slightly better thanelmo (bertρ = −0.58, elm oρ = −0.53).9putting this in context, both models performed con-siderably worse than the average inter-annotatoragreement score (ρ = 0.79)..we also computed the r2 of a linear regressionincluding the cosine distance measures from bothbert and elmo.
combined, both measures ex-plained roughly 37% of the variance in mean re-latedness judgments (r2 = 0.37).
surprisingly,this was only slightly more than half the varianceexplained by a linear regression including only theinteraction between same sense and ambiguitytype (r2 = 0.66), as well as a regression includ-ing all factors (r2 = 0.71)..by visualizing the residuals from the linear re-gression with only bert and elmo (see figure4), we see that cosine distance appears to system-atically underestimate how related participants ﬁndsame-sense judgments to be (for both polysemyand homonymy).
further, we see that cosine dis-tance systematically overestimates how related par-ticipants ﬁnd different-sense homonyms to be..figure 4: residuals of a linear regression including co-sine distance measures from both bert and elmo,plotted by by same sense (true vs. false) and ambi-guity type (homonymy vs. polysemy)..9note that larger values of cosine distance indicate alarger distance between two vectors; thus, a negative correla-tion is expected between relatedness and cosine distance..word meanings are dynamic, dependent on thecontexts in which those words appear—and somewords are even ambiguous, generating distinct, in-compatible interpretations in different situations(e.g., “fruit bat” vs. “baseball bat”)..raw-c contains graded relatedness judgments(by human annotators) for ambiguous englishwords in distinct sentential contexts.
importantly,the ambiguous wordform (e.g., “bat”) is alwaysmatched for both part-of-speech and inﬂectionacross each sentence pair; 84 of the target wordsare nouns, and 28 are verbs.
each word has relat-edness judgments for six different sentences pairs(four unique sentences): two same-sense pairs, andfour different-sense pairs.
same sense pairs conveythe same meaning, according to merriam-websterand the oed (e.g., “fruit bat” and “furry bat”),while different sense pairs correspond to mean-ings listed in either distinct lexical entries (e.g.,“fruit bat” and “wooden bat”) or distinct sub-entries(e.g., “marinated lamb” and “baby lamb”).
fur-thermore, different-sense pairs are labeled accord-ing to whether they are related via homonymy orpolysemy, a relevant distinction for both lexicog-raphers and psycholinguists—recent evidence sug-gests that polysemous and homonymous meaningsare represented differently in the mental lexicon(klepousniotou, 2002; klepousniotou and baum,2007).
finally, the sentential context is alwaystightly controlled; in most pairs, only one worddiffers across the two sentences (e.g., “fruit” vs.“furry”)..in section 5, we reported several primary ﬁnd-ings.
first, contextualized representations fromboth bert and elmo capture the distinctionbetween same-sense and different-sense uses ofa word, but their ability to distinguish betweenhomonymy and polysemy is marginal at best.
thiscontrasts with other recent work (nair et al., 2020),suggesting that bert is able to differentiate be-tween homonymy and polysemy.
one possibleexplanation for this difference in results is that nairet al.
(2020) used naturally-occurring sentencesfrom semcor (miller et al., 1993), whereas our sen-tence contexts were more tightly controlled.
ourresults indicate that even the presence of a singledisambiguating word can trigger nuanced differ-ences in semantic representation in humans, butnot necessarily in current neural language models.
second, we found that both bert and elmo.
7083homonymypolysemy−202residuals (relatedness ~ elmo + bert)ambiguity typesamefalsetrueexplain independent sources of variance in hu-man relatedness judgments, above and beyondsame sense and ambiguity type (i.e., homonymyvs. polysemy).
this is encouraging, because itdemonstrates a direct beneﬁt of graded (ratherthan categorical) judgments; for example, amongthe broad category of different-sense polysemouspairs, some are closely related (e.g., “marinatedlamb” and “baby lamb”), and others are consider-ably less closely related (e.g., “hostile atmosphere”and “gaseous atmosphere”).
overall, contextual-ized embeddings from bert were better at predict-ing human relatedness judgments than those fromelmo—this is consistent with past work (wiede-mann et al., 2019) suggesting that bert outper-forms elmo on tasks involving sense disambigua-tion..importantly, however, both bert and elmofailed to capture variance in relatedness judg-ments that is captured by same sense and am-biguity type.
as depicted in figure 4, cosinedistance tended to underestimate how related hu-mans ﬁnd same-sense uses to be, and overesti-mate how related humans ﬁnd different-sensesto be.
this is not entirely surprising, given thatneither bert nor elmo are equipped with dis-crete sense representations—at most, they producecontextualized embeddings that are amenable tosupervised classiﬁcation or unsupervised cluster-ing.
yet this also illustrates that—at least on thistask—humans do appear to draw on some mannerof (likely fuzzy) categorical representation, suchthat the difference between two contexts of use iscompressed for same-sense meanings, and exag-gerated for different-sense meanings (particularlyfor homonyms).
this suggests several exciting av-enues for future work: can neural language modelssuch as bert be augmented with semantic knowl-edge or representational schemes that improve theirperformance on raw-c or similar tasks?
both pos-sibilities are explored in section 6.1 below..6.1 future work.
as bender and koller (2020) note, most languagemodels are trained on linguistic form alone.
incontrast, human language knowledge is groundedin our embodied experience of the world (bisket al., 2020).
to the extent that human sense rep-resentations are guided by distinct sensorimotoror social-interactional associations, equipping lan-guage models with this information ought to fa-.
cilitate their ability to distinguish between distinctmeanings of a word (i.e., the disambiguation cri-terion) and modulate a given meaning in context(i.e., the contextual gradation criterion)..practitioners could also look to (and in turn, in-form) models of the human mental lexicon (nairet al., 2020).
several promising models attemptto address the continuous nature of word meaning,as well as the issue of apparent category bound-aries (i.e., word senses) (rodd et al., 2004; elman,2009); at this stage, the role of continuity vs. cat-egorical structure in human sense representationsremains an open question.
models such as sense-bert (levine et al., 2020) incorporate high-levelsense knowledge into internal representations fromthe beginning, and ﬁnd improvements on severalwsd tasks—would this approach, or others like it,yield an improvement on raw-c as well?.
6.2 limitations of dataset.
raw-c has multiple limitations, some of whichcould also be addressed in future work.
first, thebroad category of “polysemy” is often subdividedinto different mechanisms or manners of concep-tual relation, such as metaphor and metonymy.
this distinction is also believed to be cognitivelyrelevant, with some evidence that metaphoricallyrelated senses are represented differently thanmetonymically related ones (klepousniotou, 2002;klepousniotou and baum, 2007; lopukhina et al.,2018; yurchenko et al., 2020).
future work couldannotate polysemous word pairs for whether theyare related by metaphor, metonymy, or anotherclass of semantic relation—annotations could evenbe made as granular as the speciﬁc semantic rela-tion involved (e.g., animal for meat) (srinivasanand rabagliati, 2015).
this ﬁner-grained codingcould be used to assess exactly which kinds ofsemantic relation correlate with the distributionalproﬁle of word tokens—i.e., are accessible fromlinguistic form alone—and which require some ex-ternal module, whether in the form of groundedworld knowledge or a structured knowledge base.
another possible limitation is the fact that raw-c contains experimentally controlled minimal pairs,instead of naturally-occurring sentences (nair et al.,2020; haber and poesio, 2020a,b).
on the onehand, naturalistic sentences are useful for evalu-ating models on wsd “in the wild” (and indeed,there are a number of useful datasets for this pur-pose; see section 2).
on the other hand, controlled.
7084datasets are useful if one’s goal is to better under-stand a particular model or linguistic phenomenon—especially if this also allows a direct comparisonwith human annotations.
for example, our analy-ses suggest that human sense representations mustinvolve some additional levels of processing orinformation beyond the statistical regularities inword co-occurrence captured by bert and elmo.
moving forward, we hope that experimentally con-trolled datasets such as raw-c will serve as auseful complement to existing, more naturalisticdatasets..7 conclusion.
we have presented a novel dataset for evaluatingcontextualized language models: raw-c (relat-edness of ambiguous words, in context).
thisresource contains both categorical annotations,derived from expertlexicographers (merriam-webster and the oed), as well as graded related-ness judgments from human participants.
we foundthat contextualized representations from bert andelmo captured some variance (r2 = .37) in theserelatedness judgments, but that the distinction be-tween same-sense and different-sense uses, as wellas between homonymy and polysemy, explains con-siderably more (r2 = .66).
finally, we arguedthat this gap in performance represents an excitingopportunity for further development, and for cross-pollination between experimental psycholinguisticsand nlp..8 ethical considerations.
all responses from human participants wereanonymized before analyzing any data.
further-more, the raw-c dataset does not contain single-trial data—responses for a given sentence pair havebeen collapsed across all the human annotators whoprovided a rating for that pair.
all participants pro-vided informed consent, and were compensated inthe form of sona credits (to be applied to vari-ous psychology, cognitive science, or linguisticsclasses).
the project was carried out with irbapproval..acknowledgments.
we are grateful to susan windisch brown and eka-terini klepousniotou for making their experimentalstimuli available.
we also thank the anonymousreviewers for their helpful suggestions, and nathanschneider for early feedback on the idea to publish.
the dataset.
finally, we are grateful to other mem-bers of the language and cognition lab (jamesmichaelov, cameron jones, and tyler chang) forvaluable comments and discussion..references.
carlos santos armendariz, matthew purver, senja pol-lak, nikola ljubeˇsi´c, matej ulˇcar, ivan vuli´c, andmohammad taher pilehvar.
2020. semeval-2020task 3: graded word similarity in context.
in pro-ceedings of the fourteenth workshop on semanticevaluation, pages 36–49..douglas bates, martin m¨achler, ben bolker, and stevewalker.
2015. fitting linear mixed-effects modelsusing lme4.
journal of statistical software, 67(1):1–48..emily m. bender and alexander koller.
2020. climb-ing towards nlu: on meaning, form, and under-in proceedings of thestanding in the age of data.
58th annual meeting of the association for compu-tational linguistics, pages 5185–5198..katherine s binder and keith rayner.
1998. contex-tual strength does not modulate the subordinate biaseffect: evidence from eye ﬁxations and self-pacedreading.
psychonomic bulletin & review, 5(2):271–276..yonatan bisk, ari holtzman, jesse thomason, jacobandreas, yoshua bengio, joyce chai, mirella lap-ata, angeliki lazaridou, jonathan may, aleksandrnisnevich, et al.
2020. experience grounds lan-guage.
arxiv preprint arxiv:2004.10151..gemma boleda, kristina gulordava, and laura aina.
2019. putting words in context: lstm languagemodels and lexical ambiguity.
in proceedings of the57th annual meeting of the association for compu-tational linguistics; 2019 jul 28-aug 2; florence,italy.
stroudsburg (pa): acl; 2019. p. 3342–8.
acl(association for computational linguistics)..susan windisch brown.
2008a.
choosing sense dis-intinctions for wsd: psycholinguistic evidence.
proceedings of acl-08: hlt, short papers, pages249–252..susan windisch brown.
2008b.
polysemy in the men-tal lexicon.
colorado research in linguistics, 21..elia bruni, nam-khanh tran, and marco baroni.
2014.multimodal distributional semantics.
journal of ar-tiﬁcial intelligence research, 49:1–47..isabelle dautriche.
2015. weaving an ambiguous lexi-.
con.
ph.d. thesis, sorbonne paris cit´e..jacob devlin, ming-wei chang, kenton lee, andkristina toutanova.
2018. bert: pre-training ofdeep bidirectional transformers for language under-standing.
arxiv preprint arxiv:1810.04805..7085jeffrey l elman.
2009. on the meaning of words anddinosaur bones: lexical knowledge without a lexi-con.
cognitive science, 33(4):547–582..manaal faruqui and chris dyer.
2015..distributional word vector representations.
preprint arxiv:1506.05230..non-arxiv.
christine fellbaum, editor.
1998. wordnet: an elec-tronic lexical database.
cambridge, ma: mitpress..lev finkelstein, evgeniy gabrilovich, yossi matias,ehud rivlin, zach solan, gadi wolfman, and ey-tan ruppin.
2001. placing search in context: theconcept revisited.
in proceedings of the 10th inter-national conference on world wide web, pages 406–414..matt gardner, joel grus, mark neumann, oyvindtafjord, pradeep dasigi, nelson f. liu, matthewpeters, michael schmitz, and luke s. zettlemoyer.
2017. allennlp: a deep semantic natural languageprocessing platform..dirk geeraerts.
1993. vagueness’s puzzles, poly-semy’s vagaries.
cognitive linguistics (includescognitive linguistic bibliography), 4(3):223–272..daniela gerz, ivan vuli´c, felix hill, roi reichart,simverb-3500: aand anna korhonen.
2016.large-scale evaluation set of verb similarity.
arxivpreprint arxiv:1608.00869..janosch haber and massimo poesio.
2020a.
assessingpolyseme sense similarity through co-predication ac-ceptability and contextualised embedding distance.
in proceedings of the ninth joint conference on lex-ical and computational semantics, pages 114–124..janosch haber and massimo poesio.
2020b.
wordsense distance in human similarity judgements andin proceedingscontextualised word embeddings.
of the probability and meaning conference (pam2020), pages 128–145..guy halawi, gideon dror, evgeniy gabrilovich, andyehuda koren.
2012. large-scale learning of wordin proceedings ofrelatedness with constraints.
the 18th acm sigkdd international conference onknowledge discovery and data mining, pages 1406–1414..patrick hanks.
2000. do word meanings exist?
com-.
puters and the humanities, 34(1/2):205–215..felix hill, roi reichart, and anna korhonen.
2015.simlex-999: evaluating semantic models with (gen-uine) similarity estimation.
computational linguis-tics, 41(4):665–695..eric h huang, richard socher, christopher d man-ning, and andrew y ng.
2012.improving wordrepresentations via global context and multiple wordprototypes.
in proceedings of the 50th annual meet-ing of the association for computational linguistics(volume 1: long papers), pages 873–882..adam kilgarriff.
2007. word senses.
in word sense.
disambiguation, pages 29–46.
springer..ekaterini klepousniotou.
2002. the processing of lex-ical ambiguity: homonymy and polysemy in themental lexicon.
brain and language, 81(1-3):205–223..ekaterini klepousniotou and shari r baum.
2007. dis-ambiguating the ambiguity advantage effect in wordrecognition: an advantage for polysemous but notjournal of neurolinguistics,homonymous words.
20(1):1–24..ekaterini klepousniotou, debra titone, and carolinaromero.
2008. making sense of word senses: thecomprehension of polysemy depends on sense over-lap.
journal of experimental psychology: learning,memory, and cognition, 34(6):1534..caterina lacerra, michele bevilacqua, tommasopasini, and roberto navigli.
2020. csi: a coarsesense inventory for 85% word sense disambiguation.
in aaai, pages 8123–8130..mallorie leinenger and keith rayner.
2013. eye move-ments while reading biased homographs: effects ofprior encounter and biasing context on reducing thesubordinate bias effect.
journal of cognitive psy-chology, 25(6):665–681..yoav levine, barak lenz, or dagan, ori ram, danpadnos, or sharir, shai shalev-shwartz, amnonshashua, and yoav shoham.
2020. sensebert:in proceedingsdriving some sense into bert.
of the 58th annual meeting of the association forcomputational linguistics, pages 4656–4667, on-line.
association for computational linguistics..jiangtian li and marc f joanisse.
2021. word sensesas clusters of meaning modulations: a compu-tational model of polysemy.
cognitive science,45(4):e12955..anastasiya lopukhina, anna laurinavichyute, kon-stantin lopukhin, and olga dragoy.
2018. the men-tal representation of polysemy across word classes.
frontiers in psychology, 9:192..daniel loureiro, kiamehr rezaee, mohammad taherpilehvar, and jose camacho-collados.
2020. lan-guage models and word sense disambiguation:arxiv preprintan overview and analysis.
arxiv:2008.11608..george a miller, claudia leacock, randee tengi, andross t bunker.
1993. a semantic concordance.
in human language technology: proceedings of aworkshop held at plainsboro, new jersey, march21-24, 1993..sathvik nair, mahesh srinivasan, and stephan meylan.
2020. contextualized word embeddings encode as-pects of human-like word sense knowledge.
arxivpreprint arxiv:2010.13057..7086anna yurchenko, anastasiya lopukhina, and olgadragoy.
2020. metaphor is between metonymy andhomonymy: evidence from event-related potentials.
frontiers in psychology, 11:2113..jeffrey pennington, richard socher, and christopher dmanning.
2014. glove: global vectors for wordin proceedings of the 2014 confer-representation.
ence on empirical methods in natural language pro-cessing (emnlp), pages 1532–1543..matthew e peters, mark neumann, mohit iyyer, mattgardner, christopher clark, kenton lee, and lukezettlemoyer.
2018. deep contextualized word repre-sentations.
arxiv preprint arxiv:1802.05365..mohammad taher pilehvar and jose camacho-collados.
2018. wic: the word-in-context datasetfor evaluating context-sensitive meaning representa-tions.
arxiv preprint arxiv:1808.09121..keith rayner, jeremy m pacht, and susan a duffy.
1994. effects of prior encounter and global dis-course bias on the processing of lexically ambigu-ous words: evidence from eye ﬁxations.
journal ofmemory and language, 33(4):527–544..jennifer m rodd, m gareth gaskell, and william dmarslen-wilson.
2004. modelling the effects of se-mantic ambiguity in word recognition.
cognitivescience, 28(1):89–104..mahesh srinivasan and hugh rabagliati.
2015. howconcepts and conventions structure the lexicon:cross-linguistic evidence from polysemy.
lingua,157:124–152..mohamed ali hadj taieb, torsten zesch, and mo-hamed ben aouicha.
2020. a survey of semanticrelatedness evaluation datasets and procedures.
arti-ﬁcial intelligence review, 53(6):4407–4448..sean trott, tiago timponi torrent, nancy chang, andnathan schneider.
2020.
(re) construing meaningin proceedings of the 58th annual meet-in nlp.
ing of the association for computational linguistics(acl 2020)..david tuggy.
1993. ambiguity, polysemy, and vague-.
ness.
cognitive linguistics, 4(3):273–290..salvador valera.
2020. polysemy versus homonymy.
in oxford research encyclopedia of linguistics..gregor wiedemann, steffen remus, avi chawla, andchris biemann.
2019. does bert make anysense?
interpretable word sense disambiguationarxiv preprintwith contextualized embeddings.
arxiv:1909.10430..zhilin yang, zihang dai, yiming yang, jaime car-bonell, russ r salakhutdinov, and quoc v le.
2019.xlnet: generalized autoregressive pretraining forlanguage understanding.
in advances in neural in-formation processing systems, pages 5753–5763..eiling yee and sharon l thompson-schill.
2016.putting concepts into context.
psychonomic bulletin& review, 23(4):1015–1027..7087