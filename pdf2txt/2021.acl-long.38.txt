examining the inductive bias of neural language models with artiﬁciallanguages.
jennifer c. white.
ryan cotterell.
,.
university of cambridge,.
eth z¨urich.
jw2088@cam.ac.uk, ryan.cotterell@inf.ethz.ch.
abstract.
since language models are used to model awide variety of languages, it is natural to askwhether the neural architectures used for thetask have inductive biases towards modelinginvestigationparticular types of languages.
of these biases has proved complicated dueto the many variables that appear in the ex-perimental setup.
languages vary in manytypological dimensions, and it is difﬁcult tosingle out one or two to investigate withoutthe others acting as confounders.
we proposea novel method for investigating the induc-tive biases of language models using artiﬁciallanguages.
these languages are constructedto allow us to create parallel corpora acrosslanguages that differ only in the typologicalfeature being investigated, such as word or-der.
we then use them to train and test lan-guage models.
this constitutes a fully con-trolled causal framework, and demonstrateshow grammar engineering can serve as a use-ful tool for analyzing neural models.
usingthis method, we ﬁnd that commonly used neu-ral architectures exhibit different inductive bi-ases: lstms display little preference with re-spect to word ordering, while transformers dis-play a clear preference for some orderings overothers.
further, we ﬁnd that neither the induc-tive bias of the lstm nor that of the trans-former appears to reﬂect any tendencies thatwe see in attested natural languages..1.introduction.
modern neural architectures used for languagemodeling, e.g.
transformer-based language models(vaswani et al., 2017) and language models basedon long-short term memory (lstm) (hochreiterand schmidhuber, 1997; sundermeyer et al., 2012),are intrinsically black boxes.
this makes it difﬁ-cult to understand whether their structure leads toan inductive bias which results in certain types oflanguage being easier to learn and model.
to makethis point more plainly, we cannot easily conclude.
figure 1: distribution of average perplexities achievedby transformer- and lstm-based language models onour artiﬁcial languages with varying word order..much about whether an lstm language modelwill perform better on svo or sov languages bysimply examining its structure.
moreover, satis-factorily investigating the inductive bias of neuralmodels has the potential to yield useful insight intohow they work.
in this work, we explore whetherneural language models exhibit biases towards cer-tain types of languages in a novel causal frameworkthrough the use of artiﬁcial languages..one of the key problems involved in investigat-ing the effect of typological features on languagemodel performance is the difﬁculty in isolatingonly the features being investigated, without inﬂu-ence from other features of the languages beinginvestigated or the data being used.
for example, ifone were to compare language model performanceon english, an svo language, and japanese, ansov language, it would be difﬁcult to directly at-tribute differences in performance to the differencein word ordering alone.
this is because englishand japanese also differ in many other typologicaldimensions, such as how subjects are marked, theextent of subject–verb agreement and use of post-positions or prepositions, which could contributeto the difference in performance.
indeed, recentcorrelational studies have failed to ﬁnd an effect.
proceedingsofthe59thannualmeetingoftheassociationforcomputationallinguisticsandthe11thinternationaljointconferenceonnaturallanguageprocessing,pages454–463august1–6,2021.©2021associationforcomputationallinguistics45422.525.027.530.032.535.037.540.0average perplexity0.00.20.40.60.8densitytransformerlstmbetween language model performance and typolog-ical features (cotterell et al., 2018; mielke et al.,2019).
moreover, the sentences used for trainingand testing may differ in content, style or infor-mation density, which could further contribute todifferences in performance..thus, we offer a study investigating the inductivebiases of language models through the construc-tion of artiﬁcial languages.
our approach involvescreating small context-free grammars resemblingsubsets of attested languages, which we then use totrain and evaluate language models.
in an approachinspired by chomsky’s (1981) framework of prin-ciples and parameters, we imbue our grammarswith “switches” that indicate how to permute theordering of the non-terminals in a given production.
through generating grammars with all possiblecombinations of these switches, we can create ar-tiﬁcial languages of differing typological proﬁles.
this experimental paradigm allows us to conductcarefully controlled studies by varying only thetypological parameter and make a causal claim..using our method, we investigate inductivebiases related to the head-directionality of severalconstructions.
we ﬁnd that lstm-based architec-tures show little bias towards any particular order-ing, achieving similar average perplexities on allgrammar variations tested.
this contradicts recentﬁndings by ravfogel et al.
(2019) who ﬁnd lstmshave a preference for svo word order.
conversely,we ﬁnd that performance of transformer-basedarchitectures variessigniﬁcantly across ourartiﬁcial languages; this is visualized in figure 1.this indicates that some combinations of theswitches result in languages with word orders thatare harder for the transformer to model than others.
our analysis suggests that neither the performanceof the transformer-based architectures nor of thelstm-based architectures reﬂects any known ten-dencies in attested natural languages, with the bestperformance being achieved on languages with therarely-attested ovs sentence ordering.
importantly,our method exposes that transformer-based lan-guage models and lstm-based language modelshave vastly different inductive biases, a result thathas not been clearly stated in the nlp literature..2 why artiﬁcial languages?.
2.1 previous work.
respect to speciﬁc phenomenon, such as their abil-ity to acquire hierarchical generalizations (mccoyet al., 2018) and whether they can use systematiccomposition skills to make generalizations (lakeand baroni, 2018).
bowman et al.
(2015) also usedartiﬁcial languages to investigate the ability oflstms to learn compositional structure, and com-pare their ability to that of tree-structured models.
the work most closely related to ours is thatof ravfogel et al.
(2019).
taking methodologi-cal inspiration from wang and eisner (2016), theycreate artiﬁcial versions of english with modiﬁedword order and case systems, including a versionwith object–verb agreement.
they use the taskof predicting the number of the subject and ob-ject of a missing verb to examine language modelperformance across these variations.
they ﬁndthat the models perform better on this task for thelanguage with svo word order.
what they leaveunchanged in their experiment, however, is theoriginal english ordering within the constituents,e.g.
the adjective–noun ordering in a noun phrase.
however, constituent order correlates with order-ing of other grammatical constituents typologically(greenberg, 1963), and this could lead to unwar-ranted preferences for the original english ordering.
our work addresses this problem by using fully arti-ﬁcial languages rather than modifying english sen-tences.
this allows for our experiment to be morecontrolled by eliminating possible confounders..other work conducted on the topic of inductivebiases of language models has tended to focus oncorrelational studies investigating the relationshipbetween typological features extracted from theworld atlas of language structures (wals; dryerand haspelmath, 2013), which have only foundnegative results (cotterell et al., 2018; mielke et al.,2019).
since this work looked exclusively at thefeatures of attested natural languages, it is difﬁ-cult to control for the multiple typological dimen-sions along which any two natural languages differ.
further, given the large number of typological fea-tures exhibited among the world’s languages, thereare simply not enough attested languages to makestrong correlational claims.
mielke et al.
(2019)ultimately concluded with a negative result; thisnegative result, in part, motivates our study..2.2 the necessity of artiﬁcial languages.
artiﬁcial languages have previously been used toinvestigate the ability of neural architectures with.
we suggest that properly investigating the in-ductive biases of language models will likely.
455language and all.
require artiﬁcial languages.
choosing languagesto investigate the inductive bias of a languagemodel requires a trade-off between the experimentbeing realistic and being controlled.
using attestedlanguages gives us the most realisticnaturalrepresentation of naturalitscomplexities, but this also reduces the level ofcontrol and makes it difﬁcult to disentangle thevarious typological variables that differ betweenlanguages.
indeed, this was the conclusion ofmielke et al.
(2019).
work such as ravfogel et al.
(2019) ﬁnds some mid-point by using artiﬁciallanguages which have been modiﬁed from english.
this means that the language is less natural andmore controlled, but does not maximize either..in our experiments, we have chosen to maximizethe level of control.
this means that our grammarsare simple and do not necessarily cover all possibleconstructions that one would expect to see in a natu-ral language.
however, our reward for this sacriﬁceis that we can precisely control and understand howtwo languages tested differ from one another.
weargue that this provides a good base for the explo-ration of inductive bias, as when differences areobserved under these conditions we may now makea causal claim about their origin.
in future work,the base grammars could be changed and extendedas much as necessary to test additional hypotheses..3 constructing controlled languages.
3.1 a fully controlled experiment.
a context-free grammar (cfg) is a quadruple (n ,s, σ, r) where n is a set of non-terminals, s ∈n is a distinguished start non-terminal, σ is analphabet and r is a set of production rules.
anelement r ∈ r takes the form n → α whereα ∈ (n ∪ σ)∗.
a cfg deﬁnes a subset of σ∗..probabilistic context-free grammars (pcfg) area probabilistic generalization of cfgs.
rather thansimply deﬁning a subset of σ∗, a pcfg gives us aprobability distribution of σ∗ where the structureof the grammar gives us the structural zeros of thedistribution.
given a pcfg, we can take samplesfrom it in order to generate sentences..we set out to construct a set of pcfgs to ex-pose the inductive bias of neural language mod-els.
these grammars are parametrized by several“switches”, which determine the ordering of con-stituents within the grammar.
the “switches” usedare described in more detail in §3.3..we write an initial base pcfg in which.
s.npsubj.
vp.
np.
subj.
scomp.
verb.
vp.
rel.
np.
sub.
s.comp.
strovokicizeda.
npobj.
verb.
rel.
fusbenders.
npsubj.
verb.
sa.
np.
obj.
povify.
pronoun.
ob.
me.
np.
subj.
povicateda.
serds.
sub.
(a) grammar 000000: me ob povify rel fusbenders sub serdssub povicateda sa strovokicizeda ..s.npsubj.
vp.
np.
subj.
verb.
scomp.
np.
rel.
vp.
sub.
strovokicizeda.
comp.
s.fusbenders.
rel.
verb.
npobj.
sa.
npsubj.
verb.
np.
subj.
povicateda.
serds.
sub.
povify.
np.
obj.
pronoun.
ob.
me.
(b) grammar 011101: fusbenders rel povify me ob substrovokicizeda sa serds sub povicateda ..s.vp.
npsubj.
verb.
scomp.
np.
strovokicizeda.
comp.
s.subj.
sub.
np.
rel.
vp.
fusbenders.
rel.
verb.
npobj.
sa.
verb.
npsubj.
povicateda.
np.
subj.
serds.
sub.
povify.
np.
obj.
pronoun.
ob.
me.
(c) grammar 111111: strovokicizeda sa povicateda serdssub fusbenders rel povify me ob sub ..figure 2: trees showing the structure of parallel sen-tences across 3 of our artiﬁcial languages.
productions are written to correspond with theordering obtained when all switches are “off”.1in this base pcfg, the rules which are affectedby the toggling of each switch are marked.
fromthis, sentences are sampled.
on generation, eachproduction in these sentences is marked with theswitch it is associated with.
we then work throughevery combination of switches, replicating thissame set of generated sentences and reversingproductions as required by the switches, to produce.
1the choice of which permutation is “on” or “off” is ar-bitrary.
in this case, “off” switches correspond to head-ﬁnalorderings..456multiple parallel corpora, identical in their contentup to a reordering of constituents..this experimental set-up allows us to ensure thatsentences in the corpus for each of our artiﬁciallanguages differ only in the conﬁguration of theswitches.
in this way we can be conﬁdent in at-tributing any differences in performance to a causaldifference in these switches rather than any differ-ences caused by confounders, e.g.
content, style orcomplexity of the sentences..3.2 our context-free grammar.
now we describe the construction of the pcfgwith which we experiment in this work.
ex-ample sentences from several of our generatedlanguages are shown in figure 2. the basegrammar and the scripts for sampling from itand generating corpora for all switch conﬁgura-tions will be released at https://github.com/rycolab/artificial-languages..the alphabet σ. open-class words were takenfrom a list of phonotactically plausible englishpseudowords (kharkwal, 2014).
these pseu-dowords included verbs, nouns and adjectives.
weinﬂected the nouns manually for english plural-ity (adding s or es) depending on what englishphonotactics requires.
we conjugated the verbs forpresent and past tense, again, using the rules ofenglish.
additional morphological markers thatare not present in english, e.g.
subject and ob-ject markers and an additional marker to denotea plural past tense verb form, were obtained byrandomly sampling two-letters slices from the listof morphological plausible words.2 pronouns andprepositions were also obtained in this fashion..the non-terminals n .
our grammar has a sin-gle distinguished start symbol s. it describes verbphrases (vp), containing transitive and intransi-tive verbs, as well as verbs that take a sententialcomplement (complementizers are denoted comp).
nouns are marked as being objects or subjects us-ing a particle (denoted obj or subj).
verbs in ourgrammar have two tenses (past and present).
nounphrases (np), including those modiﬁed by adjec-tives (adj), relative clauses (where relativizers aredenoted rel) and prepositional phrases (pp), aredescribed in our grammar..rule for each switch value.
switch.
0.
1.svpcomp.
pp.
nprel.
s → np vpvp → np vpscomp → s compnp → pp nppp → np prepnp → adj npnp → vp rel noun np → noun rel vp.
s → vp npvp → vp npscomp → comp snp → np pppp → prep npnp → np adj.
table 1: rules that are switchable in our grammar.
sub-scripts for tense and number agreement are not shownfor simplicity..the production rules r. our production rulesr cover several common productions seen in natu-ral language.
we list the production rules which aresubject to switching in our experiment in table 1..modeling morphological agreement.
ourgrammar models a simple form of morphologicalagreement: verbs agree with their subjects innumber (singular or plural).
this introducesan element of long-term dependencies into ourlanguages – if a language model is to correctlypredict a verb form, it must carry informationabout the number of the subject.
in order to enforcethis agreement in our grammar, non-terminals aresubscripted with their number (where applicable)..assigning probabilities.
weights given to eachproduction were chosen manually through experi-mentation.
some principles for choosing weightsfor a grammar in this manner are described by eis-ner and smith (2008).
an automated method ofassigning weights could be explored in future work..3.3 controlled typological variation.
our end goal is to construct a grammar parameter-ized by a binary vector of k switches.
we denotesuch a vector of switches b ∈ {0, 1}k. togglingan individual switch in the grammar reverses theorder of the right-hand sides of a set of produc-tion rules.
for example, the switch that we termthe s switch reverses the order of the productions → np vp to create s → vp np.3 2k differentgrammars are possible from k binary switches.
inthe following paragraphs, we describe each of theswitches we consider in this work..position of subject in sentence (s switch).
thisswitch determines the order in which a subject andits verb phrase appear within a sentence.
if the.
2this sampling occurred only once, and markers used were.
the same for all words..3details of all switches are shown in table 1..457japanese.
english.
spanish.
switch value example.
value example.
value example.
0 猫が食べる。0 猫がネズミを食べる。.
svpcomp 0 猫が食べると思う。ppnprel.
0 テーブルの上の猫が食べる。 1 the cat on the table eats.
0 小さな猫が食べる。0 ミルクを飲む猫が食べる。.
0 the small cat eats.
1 the cat that drinks milk eats..0 the cat eats.
1 the cat eats the mouse.
i think that the cat eats.
1.
0 el gato come.
1 el gato come el rat´on.
1pienso que el gato come.
1 el gato sobre la mesa come.
1 el gato peque˜no come.
1 el gato que bebe leche come..table 2: demonstration of the orders of the switch constituents in japanese, english and spanish.
switch has a value of 0, the rule s → np vp isused, which is the order used in the vast major-ity of the world’s languages, including svo lan-guages such as english and sov languages suchas japanese.
if the switch has a value of 1, the rulebecomes s → vp np.
this order is rare amongattested natural languages, but can be seen in voslanguages such as malagasy and ovs languagessuch as hixkaryana..position of verb in verb phrase (vp switch).
this switch determines whether a direct object pre-cedes or follows its verb.
if the switch has a valueof 0, we use the head-ﬁnal order, with the objectpreceding the verb.
this is seen in languages suchas japanese and turkish.
if the switch has a valueof 1, the head-initial order is used, with the objectfollowing the verb.
this is seen in languages suchas english and chinese.
this switch, in combi-nation with the s switch, determines the overallordering of subject, object and verb within a sen-tence.
if the values of these switches are (0, 0), thelanguage will have sov word order, like japaneseand turkish.
if they are (1, 1), the language willhave vos order, which is rare but can be seen inlanguages such as malagasy.
svo languages suchas english correspond to (0, 1).
(1, 0) correspondsto ovs order, which is attested in only a very smallnumber of human languages..position of complementizer in sentential com-plement (comp switch).
this switch deter-mines whether a complementizer begins or ends asentential complement.
if the switch has a valueof 0, the complementizer appears in head-ﬁnal po-sition, at the end of the complement.
this is theorder seen in japanese.
if the switch has a valueof 1, the complementizer appears in head-initialposition, at the beginning of the complement.
thisis the order seen in english..ordering of prepositional phrase (pp switch).
this switch determines the ordering of a preposi-.
tional phrase.
if the switch has a value of 0, theprepositional phrase precedes the noun it modiﬁes,and the prepositional phrase ends with a prepo-sition, in head-ﬁnal order.
this order is seen injapanese.
if the switch has a value of 1, the prepo-sitional phrase follows the noun it modiﬁes, andthe preposition begins the prepositional phrase, inhead-initial order.
this order is seen in english..position of adjective in noun phrase (npswitch).
this switch determines whether an ad-jective appears before or after the noun it modiﬁes.
if the switch is 0, the adjective precedes the noun(as in english and japanese) and if it is 1, the ad-jective follows the noun (as in spanish and irish)..position of relative clause (rel switch).
thisswitch determines the position of a relative clausewith respect to the noun it modiﬁes.
if the switchhas a value of 0, a relative clause is followed by arelativizer and then the noun it modiﬁes.
this orderis seen in japanese.
if the switch has a value of 1,the noun being modiﬁed appears ﬁrst, followed bya relativizer and the clause.
this order is seen infrench and english..the unmarked word order of some attestedlanguages can be approximately identiﬁed withparticular switch vectors.4 for example, stan-dard english order corresponds approximately to(0, 1, 1, 1, 0, 1), japanese to (0, 0, 0, 0, 0, 0) andspanish to (0, 1, 1, 1, 1, 1).5 this is demonstratedin table 2. we note that our conﬁgurations cannotaccount for all possible word orders seen in attestedlanguages (vso languages are not represented, forexample), but constitute a subset of possible orders..4this is, of course, a simpliﬁcation, since word orderwithin a natural language can follow more complex rules,or allow for ﬂexibility..5from this point on, grammars will be referred to bytheir conﬁguration of switches, sans brackets, e.g.
grammar011101..458figure 3: all scores achieved by lstm- and transformer-based models.
4 experiments.
architectures and data.
in order to compareinductive biases across architectures, two neural ar-chitectures were tested: transformers and lstms.
we used the implementation available as part offairseq (ott et al., 2019).
our base grammar hask = 6 switches, i.e.
6 binary choice points asdescribed in §3.3.
this results in 26 = 64 pos-sible grammars.
for each of these grammars wegenerated 100,000 sentences, which were dividedinto 10 splits of 10,000.6 the sentences generatedfor each grammar differed only in the designatedchoice points, i.e.
in the ordering of their con-stituents.
this meant that each sentence appearedin an equivalent form in each grammar.
as such,for each sentence, we can compare the perplex-ity of the 64 variants of the sentence as calculatedby language models trained on the correspondinggrammars.
each split of 10,000 sentences was di-vided into an 80–10–10 train–dev–test split.7.
procedure.
we trained both a transformer-basedand an lstm-based language model on each trainsplit and the models were evaluated on the test split.
this procedure resulted in 10 language models perarchitecture for each possible grammar, each ofwhich was evaluated on 1,000 sentences in their re-spective test set.
the perplexity achieved on thesetest sets was averaged across the 10 splits, to givethe average perplexity for that grammar.
this ap-proach helps to account for the variability betweenindividual training runs..610,000 sentences may sound like a relatively small num-ber, but we note that our artiﬁcial languages are simple withsmall vocabularies, so we consider this number to be sufﬁcient.
7equivalent sentences across grammars were assured to bein the equivalent splits for each grammar, so train, dev andtest sets across grammars contained the same sentences up toreordering of constituents..5 results and analysis.
5.1 perplexity evaluation.
the average perplexity on the test set was mea-sured for each grammar.
this measures how wella language model explains the held-out test set.
the lower the perplexity the better the languagemodel ﬁts the held-out data.
average perplexityachieved across all grammars by the transformer-and lstm-based models are shown in figure 3.8.
5.2 mixed-effects modeling.
we use a linear mixed-effects model to investigatethe effects of each choice point in the grammar.
this allows us to model the effect of each switch inthe grammar, and ﬁrst-order interaction terms be-tween them, on the perplexity of a sentence, whilecontrolling for the fact that perplexities for parallelsentences across grammars are related (by usinga random intersect per sentence grouping).
thismodel is explained in detail below..assume we have n paired sentences from eachof our 2k grammars.
let l ∈ rn ×2kbe a non-negative real matrix of the perplexity obtained forevery test sentence across every grammar.
specif-ically, we have that lnk is the perplexity for thenth sentence under the kth grammar.
furthermore,.
≥0.
2k ×.
(cid:16) k(k−1)2.
(cid:17).
+k.
2.let s ∈ {0, 1}be the binary ma-trix containing the conﬁguration of switches andthe k(k−1)+ k switch–switch interactions foreach of the 2k grammars in contrast coding (wu,2009).
thus, we have that the column vector sk•is a binary vector of length k(k−1)+ k. letβ ∈ r k(k−1)+k be a vector of real coefﬁcients tobe estimated describing the effect of each switchand their interactions.
let un ∼ n (0, σ2dif.)
be.
2.
2.
8error bars are omitted, but across grammars the error on.
each measurement is generally between 0.25 and 0.5..459000000000001000010000011000100000101000110000111001000001001001010001011001100001101001110001111010000010001010010010011010100010101010110010111011000011001011010011011011100011101011110011111100000100001100010100011100100100101100110100111101000101001101010101011101100101101101110101111110000110001110010110011110100110101110110110111111000111001111010111011111100111101111110111111ordering010203040average perplexitytransformerlstm(a).
(b).
figure 4: heat maps showing the coefﬁcients obtained for a mixed-effects model for perplexity as predicted by (a)transformers and (b) lstms..a sentence-speciﬁc difﬁculty term (a random ef-fect) and let ε ∼ n (0, σ2) be a sentence–grammar-speciﬁc noise term.
now, we model an individualperplexity lnk, which corresponds to the nth sen-tence and the kth grammar, as follows:.
lnk = sk• · β + un + ε.
(1).
importantly, we draw one un for each unique sen-tence.
it is in this sense that un acts as a term formodeling sentence difﬁculty.
we may write eq.
(1)as the following.
lnk ∼ n (sk• · β, σ2.
dif.
+ σ2).
(2).
which reveals that it is no more than a simple gaus-sian model with tied parameters.
we estimate β,dif.
and σ2 through maximum-likelihood estima-σ2tion, which, in gaussian models, is equivalent toleast-squares estimation..a positive coefﬁcient βj for a given switchmeans that models perform worse with head-initialordering for that switch, while a negative coefﬁ-cient means the opposite.
since the ﬁxed effectswere input using contrast coding, the interactionterms in our model deal with the effects of twoconstituents sharing head-directionality.
a posi-tive coefﬁcient for an interaction means that themodels perform worse when they share head direc-tionality, and a negative coefﬁcient means the op-posite.
head-directionality is commonly correlatedbetween sentence constituents in attested naturallanguages, so if the biases of these architectures re-ﬂected human languages, we would expect most in-teraction terms to be negative.
the coefﬁcients ob-.
tained for the transformers are shown in figure 4a.
those for the lstms are shown in figure 4b..6 discussion.
differences between architectures.
it is clearfrom figure 3 that the transformer- and lstm-based models do not show the same inductive bi-ases with respect to the switches we investigated.
across all possible conﬁgurations of the switches,lstms achieve very similar average perplexities,suggesting that they have little preference for anyparticular set of constituent orderings.
in contrast,the average perplexities achieved by the transform-ers vary considerably between grammars.
thisdemonstrates clearly that the two models exhibitdistinctly different preferences with regard to or-derings of words within in a sentence.
further,the clear contrast between the coefﬁcients obtainedby the mixed-effects models for transformers andlstms (shown in figure 4a and figure 4b, re-spectively) demonstrates a stark contrast betweenthe two models.
none of the switches investi-gated, or their ﬁrst-order interactions, appear tohave a substantial effect on the scores obtainedin the case of the lstm-based models, whereasthe transformer-based models are clearly affectedto a much greater degree by the conﬁguration ofthese switches.
given that these two architecturesare both commonly used for similar tasks, such adifference in their inductive biases is noteworthy..correlated switches.
figure 4a shows the coef-ﬁcients obtained by the mixed-effects model em-ployed to investigate the effects of the switches.
460svpcompppnprelsvpcompppnprel-0.253 p=0.000-0.352 p=0.000-0.438 p=0.000-0.060 p=0.000-0.084 p=0.000-0.042 p=0.000-0.945 p=0.0000.989 p=0.000-0.076 p=0.000-0.095 p=0.0000.552 p=0.000-0.317 p=0.000-0.068 p=0.000-0.061 p=0.000-0.048 p=0.000-0.068 p=0.000-0.074 p=0.000-0.080 p=0.000-0.051 p=0.0000.330 p=0.000-0.601 p=0.0000.60.40.20.00.20.40.6svpcompppnprelsvpcompppnprel-0.115 p=0.000-0.138 p=0.000-0.064 p=0.000-0.004 p=0.0740.006 p=0.008-0.009 p=0.000-0.113 p=0.0000.140 p=0.000-0.006 p=0.0110.008 p=0.0010.071 p=0.0000.006 p=0.0100.003 p=0.1670.011 p=0.0000.039 p=0.0000.000 p=0.9210.002 p=0.3610.000 p=0.973-0.030 p=0.0000.027 p=0.000-0.129 p=0.0000.60.40.20.00.20.40.6esting to consider whether the word orders thatthese models are able to model more successfullyare those which are more commonly seen in naturallanguage.
some have speculated that the skew ofword orders in human languages could possibly bereﬂective of human cognitive biases (culbertsonet al., 2012, 2019), so it would be interesting to seeto what extent the inductive biases of these modelsreﬂects this skew.
since lstms appear to showno preference for any word order over the others,they are clearly not reﬂective of attested tendenciesin word order.
to attempt to answer this questionfor the transformers, we begin by comparing theperformance of the models on subsets of grammarswith the prevalence of similar languages amonghumans.
in figure 5, the grammars are groupedby how they order the verb, object and subject ofa sentence, and the average perplexities achievedby the language models on each of these groupsis shown.
on the same ﬁgure, we display the es-timated prevalence of these orderings among theworld’s languages (dryer, 2013).
it is clear thatthese two things are not correlated, with the trans-former performing similarly on sov languages,the most common among the world’s languages,and ovs languages, which are rarely attested.
thisshows that the bias exhibited by transformers doesnot reﬂect tendencies among attested languages.
afurther indication of this is the lack of a strong pref-erence for switches sharing head-directionality asshown in figure 4a.
in human languages, the head-edness of constituents is often correlated (green-berg, 1963).
we would expect to see this throughnegative coefﬁcients for interaction terms in themixed-effects model for constituents whose orderscommonly correlate.
however, we do not observethis for all correlations.
for example, we wouldexpect the pp switch to show a strong preferencefor shared head-directionality with other switches,which we do not observe..7 conclusion.
we propose a novel methodology for the investiga-tion of the inductive bias of language models usingthe technique of creating carefully controlled artiﬁ-cial languages.
this approach allows for the elimi-nation of differences in corpora between languagesand means that typological variation between lan-guages can be restricted exclusively to the typologi-cal features being investigated.
we use this method-ology to investigate the inductive bias of two neu-.
figure 5: the prevalence of word orders across lan-guages (dryer, 2013), plotted with the average perplex-ities achieved on each of these groups of grammars bytransformer- and lstm-based models.
on performance for the transformer-based mod-els.
the diagonal values (for single switches) areall negative coefﬁcients, which indicates that themodel performance is better when these have head-ﬁnal ordering.
off-diagonal values are the coefﬁ-cients obtained for the interaction terms betweentwo switches.
a positive value here indicates thatwhen these two switches have the same value (ei-ther both head-initial or both head-ﬁnal), the per-formance of the model is worse.
a negative valuemeans that when the two switches have the samevalue, the performance is better.
most of the off-diagonal elements have small values, with a fewexceptions.
the coefﬁcients of the cross terms be-tween the s and vp switches and the s and compswitches are larger negative values, which indi-cates that when these constituents share their head-directionality the performance of the transformer-based models is better.
the coefﬁcients of the crossterms between the vp and comp, vp and rel andnp and rel switches are larger postive values, in-dicating that the transformers perform worse whenthese constituents share head-directionality.
gen-erally, attested natural languages tend to exhibit atendency towards one head-directionality, but thetransformer does not seem to have inductive biasesthat reﬂect this.
the corresponding coefﬁcients forthe lstm-based models, shown in figure 4b, areall small, further demonstrating that the lstms arelargely agnostic to word ordering..tendencies in attested natural languages.
we wish to consider the question of whether thebiases of these models are in any way reﬂective ofword order tendencies that we see across attestednatural languages.
all word orders are not equallycommon among natural languages, and it is inter-.
461sovsvoovsvosword order01020304050percentage of world languages051015202530average perplexitytransformerlstmral architectures which are commonly used for thistask: lstms and transformers.
we found thatthese two models have starkly different inductivebiases with respect to word order, with the lstmshowing little variation in performance across wordorder, while the performance of the transformer var-ied signiﬁcantly across artiﬁcial languages..acknowledgements.
we thank simone teufel for providing feedback onan early draft..ethical considerations.
the authors foresee no ethical concerns with theresearch presented in this paper..references.
samuel r. bowman, christopher d. manning, andchristopher potts.
2015. tree-structured composi-tion in neural networks without tree-structured ar-in proceedings of the 2015 interna-chitectures.
tional conference on cognitive computation: inte-grating neural and symbolic approaches, volume1583, page 37–42..noam chomsky.
1981. lectures on government andbinding: the pisa lectures.
walter de gruyter..ryan cotterell, sabrina j. mielke, jason eisner, andbrian roark.
2018. are all languages equally hardin proceedings of the 2018to language-model?
conference of the north american chapter of theassociation for computational linguistics: humanlanguage technologies, volume 2 (short papers),pages 536–541, new orleans, louisiana.
associa-tion for computational linguistics..jennifer culbertson, marieke schouwstra, and simonthekirby.
2019. from the world to word order:link between conceptual structure and language.
psyarxiv..jennifer culbertson, paul smolensky, and g´eraldinelegendre.
2012. learning biases predict a word or-der universal.
cognition, 122(3):306–329..matthew s. dryer.
2013. order of subject, object andverb.
in matthew s. dryer and martin haspelmath,editors, the world atlas of language structures on-line.
max planck institute for evolutionary anthro-pology, leipzig..matthew s. dryer and martin haspelmath, editors.
2013. wals online.
max planck institute for evo-lutionary anthropology, leipzig..jason eisner and noah a. smith.
2008. competitivegrammar writing.
in proceedings of the third work-shop on issues in teaching computational linguis-tics, pages 97–105, columbus, ohio.
association forcomputational linguistics..joseph h. greenberg.
1963. some universals of gram-mar with particular reference to the order of mean-ingful elements.
in joseph h. greenberg, editor,universals of human language, pages 73–113.
mitpress, cambridge, mass..sepp hochreiter and j¨urgen schmidhuber.
1997.long short-term memory.
neural computation,9(8):1735–1780..gaurav kharkwal.
2014. taming the jabberwocky:examining sentence processing with novel words.
ph.d. thesis, rutgers university-graduate school-new brunswick..brenden lake and marco baroni.
2018. generalizationwithout systematicity: on the compositional skillsof sequence-to-sequence recurrent networks.
in pro-ceedings of the 35th international conference onmachine learning, volume 80 of proceedings of ma-chine learning research, pages 2873–2882, stock-holmsm¨assan, stockholm sweden.
pmlr..r. thomas mccoy, robert frank, and tal linzen.
2018. revisiting the poverty of the stimulus: hi-erarchical generalization without a hierarchical biasin recurrent neural networks.
in proceedings of the40th annual conference of the cognitive science so-ciety, pages 2093–2098..sabrina j. mielke, ryan cotterell, kyle gorman, brianroark, and jason eisner.
2019. what kind of lan-guage is hard to language-model?
in proceedings ofthe 57th annual meeting of the association for com-putational linguistics, pages 4975–4989, florence,italy.
association for computational linguistics..myle ott, sergey edunov, alexei baevski, angelafan, sam gross, nathan ng, david grangier, andfairseq: a fast, extensiblemichael auli.
2019.in proceedings oftoolkit for sequence modeling.
the 2019 conference of the north american chap-ter of the association for computational linguistics(demonstrations), pages 48–53, minneapolis, min-nesota.
association for computational linguistics..shauli ravfogel, yoav goldberg, and tal linzen.
2019.studying the inductive biases of rnns with syn-in proceed-thetic variations of natural languages.
ings of the 2019 conference of the north americanchapter of the association for computational lin-guistics: human language technologies, volume 1(long and short papers), pages 3532–3542, min-neapolis, minnesota.
association for computationallinguistics..martin sundermeyer, ralf schl¨uter, and hermann ney.
2012. lstm neural networks for language model-ing.
in thirteenth annual conference of the inter-national speech communication association, pages194–197..462ashish vaswani, noam shazeer, niki parmar, jakobuszkoreit, llion jones, aidan n. gomez, łukaszkaiser, and illia polosukhin.
2017. attention is allyou need.
in proceedings of the 31st internationalconference on neural information processing sys-tems, page 6000–6010..dingquan wang and jason eisner.
2016. the galacticdependencies treebanks: getting more data by syn-thesizing new languages.
transactions of the asso-ciation for computational linguistics, 4:491–505..lang wu.
2009. mixed effects models for complex.
data.
crc press..463