guiding the growth: difﬁculty-controllable question generationthrough step-by-step rewriting.
yi cheng1, siyao li2, bang liu3∗, ruihui zhao1, sujian li4, chenghua lin5, yefeng zheng1.
1tencent jarvis lab, china.
2lti, carnegie mellon university.
3rali & mila, université de montréal.
4peking university 5the university of shefﬁeld.
{yicheng, zacharyzhao, yefengzheng}@tencent.com,siyaol@andrew.cmu.edu, bang.liu@umontreal.ca,lisujian@pku.edu.cn, c.lin@shef.ac.uk.
abstract.
this paper explores the task of difﬁculty-controllable question generation (dcqg),which aims at generating questions with re-quired difﬁculty levels.
previous research onthis task mainly deﬁnes the difﬁculty of a ques-tion as whether it can be correctly answeredby a question answering (qa) system, lack-ing interpretability and controllability.
in ourwork, we redeﬁne question difﬁculty as thenumber of inference steps required to answerit and argue that question generation (qg)systems should have stronger control over thelogic of generated questions.
to this end, wepropose a novel framework that progressivelyincreases question difﬁculty through step-by-step rewriting under the guidance of an ex-tracted reasoning chain.
a dataset is automat-ically constructed to facilitate the research, onwhich extensive experiments are conducted totest the performance of our method..1.introduction.
figure 1: an example of generating a complex ques-tion through step-by-step rewriting based on the reason-ing chain extracted from a constructed context graph..the task of difﬁculty-controllable question gen-eration (dcqg) aims at generating questions withrequired difﬁculty levels and has recently attractedresearchers’ attention due to its wide application,such as facilitating certain curriculum-learning-based methods for qa systems (sachan and xing,2016) and designing exams of various difﬁcultylevels for educational purpose (kurdi et al., 2020).
compared to previous qg works which controlthe interrogative word (zi et al., 2019; kang et al.,2019) or the context of a question (liu et al., 2020,2019a), few works have been conducted on difﬁ-culty control, as it is hard to formally deﬁne thedifﬁculty of a question.
to the best of our knowl-edge, gao et al.
(2019) is the only previous work ofdcqg for free text, and deﬁnes question difﬁcultyas whether a qa model can correctly answer it..∗corresponding author..this deﬁnition gives only two difﬁculty levels andis mainly empirically driven, lacking interpretabil-ity for what difﬁculty is and how difﬁculty varies.
in this work, we redeﬁne the difﬁculty level of aquestion as the number of inference steps requiredto answer it, which reﬂects the requirements onreasoning and cognitive abilities (pan et al., 2019).
existing qa systems perform substantially worsein answering multi-hop questions than single-hopones (yang et al., 2018), also supporting the sound-ness of using reasoning hops to deﬁne difﬁculty..to achieve dcqg with the above deﬁnition, aqg model should have strong control over the logicand reasoning complexity of generated questions.
graph-based methods are well suited for such logicmodelling (pearl and paz, 1986; zhang et al., 2020).
in previous qg researches, yu et al.
(2020) andpan et al.
(2020) implemented graph-to-sequence.
proceedingsofthe59thannualmeetingoftheassociationforcomputationallinguisticsandthe11thinternationaljointconferenceonnaturallanguageprocessing,pages5968–5978august1–6,2021.©2021associationforcomputationallinguistics5968frameworks to distill the inner structure of the con-text, but they mainly used graphs to enhance doc-ument representations, rather than to control thereasoning complexity of questions..• we build a dataset that can facilitate trainingof rewriting questions into more complex ones,paired with constructed context graphs and theunderlying reasoning chain of the question..in this paper, we propose a highly-controllableqg framework that progressively increases difﬁ-culties of the generated questions through step-by-step rewriting.
speciﬁcally, we ﬁrst transform agiven raw text into a context graph, from which wesample the answer and the reasoning chain for thegenerated question.
then, we design a questiongenerator and a question rewriter to generate aninitial simple question and step-by-step rewrite itinto more complex ones.
as shown in fig.
1, “tomcruise” is the selected answer, and q1 is the initialquestion, which is then adapted into q2 by addingone more inference step (i.e.
n1←n2) in the rea-soning chain.
that is, it requires to infer “topgun” is “the ﬁlm directed by tony scott” beforeanswering q1.
similarly, we can further increaseits difﬁculty level and step-by-step extend it intomore difﬁcult questions (i.e., q3, q4 and q5)..to train our dcqg framework, we design ef-fective strategies to automatically construct thetraining data from existing qa datasets insteadof building one from scratch with intensive humanefforts.
speciﬁcally, we utilize hotpotqa (yanget al., 2018), a qa dataset where most questionsrequire two inference steps to answer and can bedecomposed into two 1-hop questions.
thus, weget the dataset that contains 2-hop questions andtheir corresponding 1-hop reasoning steps.
hav-ing learned how to rewrite 1-hop questions into2-hop ones with this dataset, our framework caneasily extend to the generation of (n+1)-hop ques-tions from n-hop ones only with a small amountof corresponding data, because the rewriting oper-ation follows rather certain patterns regardless ofthe exact value of n, as shown in fig.
1..extensive evaluations show that our method cancontrollably generate questions with required difﬁ-culty, and keep competitive question quality at thesame time, compared with a set of strong baselines.
in summary, our contributions are as follows:• to the best of our knowledge, this is the ﬁrstwork of difﬁculty-controllable question genera-tion, with question difﬁculty deﬁned as the infer-ence steps to answer it;.
• we propose a novel framework that achievesdcqg through step-by-step rewriting under theguidance of an extracted reasoning chain;.
2 related work.
deep question generation most of the previ-ous qg researches (zhou et al., 2017; pan et al.,2019; liu et al., 2020) mainly focused on generat-ing single-hop questions like the ones in squad(rajpurkar et al., 2016).
in the hope that ai sys-tems could provoke more in-depth interaction withhumans, deep question generation aims at generat-ing questions that require deep reasoning.
manyrecent works attempted to conquer this task withgraph-based neural architectures.
talmor and be-rant (2018) and kumar et al.
(2019) generated com-plex questions based on knowledge graphs, buttheir methods could not be directly applied to qgfor free text, which lacks clear logical structures.
in sequential question generation, chai and wan(2020) used a dual-graph interaction to better cap-ture context dependency.
however, they consideredall the tokens as nodes, which led to a very complexgraph.
yu et al.
(2020) tried to generate multi-hopquestions from free text with the help of entitygraphs constructed by external tools.
our workshares a similar setting with yu et al.
(2020), andwe further explore the problem of how to generatedeep questions in a more controllable paradigm..difﬁculty-controllable question generationdcqg is a relatively new task.
gao et al.
(2019)classiﬁed questions as easy or hard according towhether they could be correctly answered by abert-based qa model, and controlled the ques-tion difﬁculty by modifying the hidden statesbefore decoding.
another research on qg forknowledge graphs (kumar et al., 2019) estimatedthe question difﬁculty based on popularity of thenamed entity.
they manipulated the generationprocess by incorporating the difﬁculty level intothe input embedding of the transformer-based de-coder.
in our work, we control the question difﬁ-culty based on the number of its reasoning hops,which is more explainable..question rewriting it is another emerging trendin the recent researches, demonstrating beneﬁts toboth qg and qa tasks.
with rewriting, qg modelsproduced more complex questions by incorporatingmore context information into simple questions.
5969figure 2: an overview of our proposed framework.
the selected reasoning chain is marked as light blue nodes..(elgohary et al., 2019; vakulenko et al., 2020), andqa pipelines could also decompose the originalcomplex question into multiple shorter questionsto improve model performance (min et al., 2019;khot et al., 2020)..3 method.
given input context text c and a speciﬁc difﬁ-culty level d, our objective is to generate a (ques-tion, answer) pair (q, a), where a is a sub-spanof c and q requires d-hop reasoning to answer.
fig.
2 and algorithm 1 give an overview of ourproposed framework.
first, we construct a con-text graph gcg corresponding to the given context,from which a subgraph gt is selected to serve asthe reasoning chain of the generated question.
next,with the reasoning chain and other contextual in-formation as input, a question generator (qginitial)produces an initial simple question q1.
then, q1is fed to a question rewriting module (qgrewrite),which iteratively rewrites it into a more complexquestion qi (i = 2, 3, .
.
.
, d).
in what follows,we will introduce the whole generation process inmore details..context graph construction we follow themethod proposed by fan et al.
(2019) to buildthe context graph gcg.
speciﬁcally, we ﬁrst ap-ply open information extraction (stanovsky et al.,2018) to extract (cid:104)subject, relation, object(cid:105) triplesfrom context sentences.
each triple is then trans-.
formed into two nodes connected with a directededge, like a perfect murder is−→ a 1998 americancrime ﬁlm in fig.
2. the two nodes respectivelyrepresent the subject and object, and the edge de-scribes their relation.
coreference resolution (leeet al., 2017) is applied to merge nodes referring tothe same entity.
for instance, a perfect murder ismerged with it in fig.
2..reasoning chain selection with the contextgraph constructed, we sample a connected sub-graph gt consisting of d + 1 nodes from it to serveas the reasoning chain of the generated question.
a node n0 is ﬁrst sampled as the answer of thequestion, if it is, or linked with, a named entity thathas more than one node degree.
next, we extractfrom gcg a maximum spanning tree gl, with n0as its root node, e.g., the tree structure shown infig.
1. gcg is temporarily considered as an undi-rected graph at this step.
we then prune gl intogt to keep only d + 1 nodes.
during pruning, weconsider the sentence position where each nodeis extracted in order to make the reasoning chainrelevant to more context.
in the following, we willdenote a node in gt as ni (i = 0, 1, .
.
.
, d), whereeach node is subscripted by preorder traversal ofgt , and np (i) as the parent of ni..step-by-step question generation our step-by-step qg process is described at lines 5-11 inalgorithm 1. the following notations are deﬁnedfor clearer illustration:.
5970algorithm 1 procedure of our dcqg frameworkinput: context c, difﬁculty level doutput: (q, a)1: gcg ← buildcg(c)2: n0 ← sampleanswernode(gcg)3: gl ← maxtree(gcg, n0)4: gt ← prune(gl, d)5: for ni in preordertraversal(gt ) do6:7:8:.
if i = 0 then continuenp (i) = parent(ni)si = contextsentence(c, ni, np (i)).
9:.
ri ←.
bridgeintersection.
if ni=firstchild(np (i))else.
(cid:40).
(cid:40).
if i = 1qginitial(ni, np (i), si)qgrewrite(qi−1, ni, np (i), si, ri) else.
10:.
qi ←.
11: end for12: return (qd, n0).
• qi (i = 1, 2, .
.
.
, d) represents the question gen-erated at each step, where qd is the ﬁnal questionq, and qi+1 is rewritten from qi by adding onemore hop of reasoning..• si represents the context sentence from which.
we extract the triple ni → np (i)..• ri is the rewriting type of qi (i = 2, 3, .
.
.
, d).
speciﬁcally, we consider two types of rewritingpatterns in this work: bridge and intersection.
as shown in fig.
1, bridge-style rewriting re-places an entity with a modiﬁed clause, whileintersection adds another restriction to an exist-ing entity in the question.
these two types canbe distinguished by whether ni is the ﬁrst childof its parent node, i.e., whether its parent nodehas already been rewritten once in bridge style.
to generate the ﬁnal question with the requireddifﬁculty level d, we ﬁrst use a question genera-tor qginitial to generate an initial simple questionbased on n1, n0, and the corresponding contextsentence s1.
then, we repeatedly (for d − 1 times)use qgrewrite to rewrite question qi−1 into a morecomplex one qi, based on node ni and its parentnode np (i), context sentence si, and the rewritingtype ri (i = 2, 3, .
.
.
, d).
formally, the generationprocess of qginitial and the rewriting process ofqgrewrite can be deﬁned as:.
q1 = arg max.
p ( ¯q1|n1, n0, s1).
¯q1.
qi = arg max.
p ( ¯qi|qi−1, ni, np (i), si, ri).
¯qi.
where i = 2, 3, .
.
.
, d..algorithm 2 procedure of data construction.
input: context c = {p1, p2}, qa pair (q2, a2), support-.
ing facts f.output: r1, (q1, a1), s1, s2, {n0, e1, n1, e2, n2}1: r1 ← typeclassify(q2)2: if r1 /∈ {bridge, intersection} then return3: subq1, subq2 ← decompq(q2)4: suba1, suba2 ← qa(subq1), qa(subq2).
5: q1, a1 ←.
subq2, suba2subq1, suba1.
if a2 = suba2else.
(cid:40).
(cid:40).
6: s1, s2 ←.
f ∩ p1, f ∩ p2f ∩ p2, f ∩ p1.
if q1 concerns p1else.
7: n2 ← findnode(a2)8: n0, e1, n1, e2 ← match(subq1, subq2).
in our implementation, both qginitial andqgrewrite are initialized with the pre-trainedgpt2-small model (radford et al., 2019), and thenﬁne-tuned on our constructed dataset (see sec.
4).
the encoder of qgrewrite, as illustrated in fig.
2,is similar to liu et al.
(2020).
if ni points to np (i),then the input sequence is organized in the form of“(cid:104)bos(cid:105) si (cid:104)nodec(cid:105) ni (cid:104)edge(cid:105) ei (cid:104)nodep(cid:105) np (i) (cid:104)type(cid:105) ri(cid:104)subq(cid:105) qi−1 (cid:104)eos(cid:105)”, where ei is the edge from ni tonp (i).
the positions of “(cid:104)nodec(cid:105) ni” and “(cid:104)nodep(cid:105)np (i)” will be exchanged if np (i) points to ni.
asfor qginitial, its input is organized in the same wayexcept without “(cid:104)type(cid:105) ri (cid:104)subq(cid:105) qi−1”..the segment embedding layer is utilized to iden-tify different segments.
for those parts in si andqi−1 that are the same as, or refer to the sameentity as np (i), we replace their segment embed-dings with the one of np (i), considering that theparent node of ni plays an important role in denot-ing what to ask about, or which part to rewrite, asshown in fig.
1..4 automatic dataset construction.
manually constructing a new dataset for our taskis difﬁcult and costly.
instead, we propose to auto-matically build a dataset from existing qa datasetswithout extra human annotation.
in our work, thetraining data is constructed from hotpotqa (yanget al., 2018), in which every context c consists oftwo paragraphs {p1, p2}, and most of the ques-tions require two hops of reasoning, each con-cerning one paragraph.
hotpotqa also annotatessupporting facts f, which are the part of the con-text most relevant to the question.
in addition tothe information already available in hotpotqa,we also need the following information to train.
5971qginitial and qgrewrite: i) (q1, a1), the simpleinitial question and its answer, which are used totrain qginitial; ii) r2, the type of rewriting fromq1 to q2; iii) {n0, n1, n2}, the reasoning chainof q2; and iv) si (i = 1, 2), the context sentenceswhere we extract n0, n1 and n2..algorithm 2 describes our procedure to obtainthe above information.
the construction process isfacilitated with the help of a reasoning type classi-ﬁer (typeclassify) and a question decomposer(decompq), referring to min et al.
(2019).
foreach question in hotpotqa (i.e.
q2), we ﬁrst dis-tinguish its reasoning type, and ﬁlter out those thatare not bridge and intersection.
the reasoningtype here corresponds to the rewriting type ri.
then, decompq decomposes q2 into two sub-questions, subq1 and subq2, based on span predic-tion and linguistic rules.
for example, the q2 infig.
2 will be decomposed into subq1=“to whichﬁlm a perfect murder was a modern remake?”, andsubq2=“who directed dial m for murder?”.
afterthat, an off-the-shelf single-hop qa model (minet al., 2019) is utilized to acquire the answer of thetwo sub-questions, which should be “dial m formurder” and “alfred hitchcock” in the example.
as for q1, it is one of the sub-questions.
whenq2 is of the intersection type, q1 can be eithersubq1 or subq2.
for the bridge type, it is the sub-question that shares the same answer as a2.
for theexample above, q1 is subq2 because suba2 = a2.
the context sentence si is supposed to providesupporting facts contained in the paragraph f thatconcerns qi (i = 1, 2).
for the reasoning chain,it is selected from the local context graph by ﬁrstlocating n2 and then ﬁnding n0, n1 through textmatching with the two sub-questions..5 experiments.
in the following experiments, we mainly evaluatethe generation results of our proposed method whenrequired to produce 1-hop and 2-hop questions, de-noted as ours1-hop and ours2-hop.
in sec.
5.2, wecompare our method with a set of strong baselinesusing both automatic and human evaluations onquestion quality.
in sec.
5.3, we provide control-lability analysis by manually evaluating their dif-ﬁculty levels and testing the performance of qasystems in answering questions generated by dif-ferent methods.
in sec.
5.4, we test the effect ofour generated qa pairs on the performance of amulti-hop qa model in a data augmentation setting..in sec.
5.5, we further analyze the extensibility ofour method, i.e., its potential in generating ques-tions that require reasoning of more than two hops.
our code and constructed dataset have been madepublicly available to facilitate future research.1.
5.1 experimental setup.
datasets the constructed dataset described insec.
4 consists of 57,397/6,072/6,072 samples fortraining/validation/test.
for context graph construc-tion, we use the coreference resolution toolkit fromallennlp 1.0.0 (lee et al., 2017) and the openinformation extraction toolkit provided by the plas-ticity developer api.2 the question decomposerand the reasoning type classiﬁer follow the imple-mentations of min et al.
(2019)..baselines the following baselines are trained togenerate the 2-hop questions in the datasets:• nqg++ (zhou et al., 2017) is a seq2seq modelbased on bi-directional gate recurrent unit(gru), with features enriched by answer posi-tion and lexical information..• ass2s (kim et al., 2019) is a seq2seq modelbased on long short-term memory (lstm),which separately encodes answer and context.
• srl-graph and dp-graph (pan et al., 2020)are two state-of-the-art qg systems.
they en-code graph-level and document-level informa-tion with an attention-based graph neural net-work (gnn) and a bi-directional gru, respec-tively.
srl-graph constructs the semantic graphby semantic role labelling, and dp-graph by de-pendency parsing..• gpt2 is a vanilla gpt2-based qg model.
itsinput is the concatenation of context and sampledanswer.
the position where the answer appearsin the context segment is denoted in the segmentembedding layer..implementation details the baseline modelsare trained to directly produce the 2-hop questions,while qginitial and qgrewrite are respectivelytrained to generate 1-hop questions and rewrite1-hop ones into 2-hop.
qginitial, qgrewrite, andgpt2 are initialized with the gpt2-small modelfrom the huggingface transformer library (wolfet al., 2019), and ﬁne-tuned for 8, 10, and 7 epochs,respectively, with batch size of 16. we apply top-pnucleus sampling with p = 0.9 during decoding..1 https://tinyurl.com/19esunzz2 https://www.plasticity.ai/.
5972modelnqg++ass2ssrl-graphdp-graphgpt2ours2-hop.
bleu315.4115.2119.6619.8720.9821.07.bleu4 meteor cider16.9611.5016.7811.2919.7315.0320.1015.2324.1915.5919.9915.26.
---1.401.461.48.table 1: automatic evaluation results of the base-line models and the 2-hop questions generated by ourmethod (ours2-hop)..adamw (loshchilov and hutter, 2017) is usedas optimizer, with the initial learning rate set tobe 6.25×10−5 and adaptively decays during train-ing.
for dp-graph, we use their released modeland code to perform the experiment.
for the otherthree baselines, we directly refer to the experimentresults reported in pan et al.
(2020).
the perfor-mances of these baselines are compared under thesame setting as in pan et al.
(2020), where each con-text is abbreviated to only include the supportingfacts and the part that overlaps with the question.
more implementation details can be found in ourcode and the supplementary materials..5.2 evaluation of question quality.
automatic evaluation the automatic evalua-tion metrics are bleu3, bleu4 (papineni et al.,2002), meteor (lavie and agarwal, 2007), andcider (vedantam et al., 2015), which measure thesimilarity between the generation results and thereference questions in terms of n-grams.
as thefour baselines are trained to generate 2-hop ques-tions only, we only compare them with ours2-hop.
as shown in table 1, we can see that ours2-hop andgpt2 perform consistently better than the others.
though the performances of ours2-hop and gpt2are close in terms of automatic metrics, we observethat the questions generated by ours2-hop are usu-ally more well-formed, concise and answerable, asillustrated in table 2. these advantages cannot bereﬂected through automatic evaluation..human evaluation we randomly sample 200questions respectively from dp-graph, gpt2,ours1-hop, ours2-hop, as well as the reference 1-hop and 2-hop questions in the constructed dataset(gold1-hop, gold2-hop).
the questions are man-ually evaluated by eight human annotators, whoare graduate students, majoring in english litera-ture, computer science, or electronic engineering.
they voluntarily offer to help without being com-.
ours2-hop.
when was the ﬁrst theatredirector of african descentborn?.
what play by carrie hamil-ton was run at the good-man theatre in 2002?.
what was the review scorefor the album that has beenreissued twice?.
gpt2when was the ﬁrst theatre di-rector of african descent toestablish a national touringcompany in the uk born?
what play by carrie hamil-ton and carol burnett ran atthe goodman theatre and onbroadway in 2002?
what was the review of thealbum that includes previ-ously unreleased tracks byguetta from its ﬁrst major in-ternational release?.
table 2: examples of generation results from ours2-hopand gpt2.
pensated in any form.
before annotation, they areinformed of the detailed annotation instruction withclear scoring examples.
the generated questionsare evaluated in the following four dimensions:• well-formed: it checks whether a question is se-mantically correct.
annotators are asked to marka question as yes, acceptable, or no.
acceptableis selected if the question is not grammaticallycorrect, but its meaning is still inferrable..• concise: it checks whether the qg models areoverﬁtted, generating questions with redundantmodiﬁers.
the question is marked as yes if nosingle word can be deleted, acceptable if it is alittle lengthy but still in a natural way, and no ifit is abnormally verbose..• answerable: it checks whether a question isanswerable according to the given context.
theanonnotion is either yes or no..• answer matching: it checks whether the givenanswer is the correct answer to the question.
theanonnotion is either yes or no.
the results are shown in table 3. overall, wecan see that ours2-hop performs consistently betterthan dp-graph and gpt2 across all metrics andcomparable to the hand-crafted reference questions.
our method performs especially well in terms ofconcise, even better than the reference questions.
for reference, the average word number of the ques-tions generated by dp-graph, gpt2, ours2-hop,and gold2-hop are 19.32, 19.26, 17.18, 17.44, re-spectively.
it demonstrates that the enriched graphinformation and our multi-stage rewriting mecha-nism indeed enhance the question structure and con-tent.
in comparison, we ﬁnd that the questions gen-erated by the two baselines tend to unreasonablypile too many modiﬁers and subordinate clauses..5973difﬁcultylevel.
2-hop.
1-hop.
model.
dp-graphgpt2ours2-hopgold2-hopours1-hopgold1-hop.
well-formedacceptable41%34%19%22%46%39%.
yes28%57%74%72%46%56%.
yesno31% 41%9%47%7% 67%6%56%8% 65%80%5%.
conciseacceptable53%50%30%40%25%16%.
answerable answer matchingyesnoyesno49% 51% 39%6%3%69% 31% 66%3% 78% 22% 69%4%87%10% 81% 19% 72%84% 16% 79%4%.
no61%34%31%13%28%21%.
92%.
8%.
table 3: human evaluation results of question quality..model.
dp-graphgpt2ours2-hopours1-hop.
inference steps3-hop1-hop2-hop8.7%26.1% 55.1%23.3% 57.1% 13.2%67.7% 25.8%4.3%70.7% 28.2%1.1%.
>3-hop10.1%6.4%2.2%0.0%.
test set.
dp-graphgpt2ours2-hopours1-hop.
bert.
em0.4360.4190.2950.618.f10.6150.5810.3810.737.robertaf1em0.6780.5520.7720.6690.6630.5060.9370.882.table 4: human evaluation results of the number ofinference steps required by the generated questions..table 5: performance of bert- and roberta-basedqa models on different generated qa datasets..as for the 1-hop questions, ours1-hop performswell in terms of answerable and answer match-ing, but not so competitive in terms of well-formed,mainly due to the limitation of its training data.
asthe 1-hop reference questions (gold1-hop) are auto-matically decomposed from the hand-crafted 2-hopquestions, a signiﬁcant portion (44%) of them havesome grammatical errors, but most of them are stillunderstandable despite that..5.3 controllability analysis.
human evaluation of controllability for con-trollability analysis, we manually evaluate the num-bers of inference steps involved in generated ques-tions.
dp-graph and gpt2 are also evaluated forcomparison.
the results are shown in table 4.
70.65% of ours1-hop require one step of inferenceand 67.74% of ours2-hop require two steps, prov-ing that our framework can successfully controlthe number of inference steps of most generatedquestions.
in comparison, dp-graph and gpt2 arenot difﬁculty-aware and their generated questionsare more scattered in difﬁculty levels..difﬁculty assessment with qa systems forfurther assessment of question difﬁculty, we testthe performance of qa models in answering ques-tions generated by different models.
speciﬁcally,we utilize two off-the-shelf qa models providedby the huggingface transformer library (wolfet al., 2019), which are respectively initialized with.
bert (devlin et al., 2019) and roberta (liuet al., 2019b), and then ﬁne-tuned on squad (ra-jpurkar et al., 2016).
we select those generatedquestions that are ensured to be paired with cor-rect answers by the human evaluation describedin sec.
5.2, and test the performance of two qamodels in answering them.
the evaluation metricsinclude exact match (em) and f1..the results are shown in table 5. we can seethat questions generated by ours2-hop are more dif-ﬁcult than ours1-hop not only to humans (requiringmore hops of reasoning), but also to the state-of-the-art qa models.
in comparison, with a morescattered mix of 1-hop and 2-hop questions, theperformances on dp-graph and gpt2 are betweenours1-hop and ours2-hop.
this result demonstratesthat our method can controllably generate ques-tions of different difﬁculty levels for qa systemsand that inference steps can effectively model thequestion difﬁculty..5.4 boosting multi-hop qa performance.
we further evaluate whether the generated qa pairscan boost qa performance through data augmenta-tion.
speciﬁcally, we heuristically sample the an-swers and reasoning chains from the context graphsin our constructed dataset to generate 150,305 two-hop questions.
as a comparison, we utilize gpt2to generate the same amount of data with the samesampled answers and contextual sentences.
somelow-quality questions are ﬁltered out if their word.
5974figure 3: performance of the distilbert-based qasystem on hotpotqa, augmented with different quan-tities of generated data..counts are not between 6∼30 (4.7% for ours and9.2% for gpt2), or the answers directly appear inthe questions (2.7% for ours and 2.4% for gpt2).
finally, we randomly sample 100,000 qa pairsand augment the hotpotqa dataset with them..a distilbert-based (sanh et al., 2019) qamodel is implemented.
it takes as input the con-catenation of context and question to predict theanswer span.
to speed up the experiment, we onlyconsider those necessary supporting facts as thequestion answering context.
during training, theoriginal samples from hotpotqa are oversampledto ensure that they are at least 4 times as the gener-ated data.
we use adam (kingma and ba, 2015) asthe optimizer, with the mini-batch size of 32. thelearning rate is initially set to 3×10−5 and adap-tively decays during training.
the conﬁgurationsare the same in all the qa experiments, except thatthe training datasets are different combinations ofhotpotqa and the generated data.
the validationand test sets are the same as those in hotpotqa..we test the impact of the generated data un-der both high-resource (using the whole trainingset of hotpotqa) and low-resource settings (us-ing only 25% of the data randomly sampled fromhotpotqa).
fig.
3 compares the qa performance,augmented with different quantities of the data gen-erated by our method and by gpt2, respectively.
we can see that under both settings, our methodachieves better performance than gpt2.
under thelow-resource setting, performance boost achievedby our generated data is more signiﬁcant and obvi-ously better than that of gpt2.
the performanceof the qa model steadily improves when the train-ing dataset is augmented with more data.
em andf1 of the qa model are improved by 2.56% and1.69%, respectively, when 100,000 samples of ourgenerated data are utilized..figure 4: two examples of generating three-hop ques-tions based on the extracted reasoning chains..5.5 more-hop question generation.
to analyze the extensibility of our method, we ex-periment with the generation of questions that aremore than 2-hop, by repeatedly using qgrewrite toincrease question difﬁculty.
fig.
4 shows two ex-amples of 3-hop question generation process.
thetwo intermediate questions and the correspondingreasoning chains are also listed for reference..we can see that the intermediate questions,serving as springboards, are effectively used byqgrewrite to generate more complex questions.
with the training data that only contains 1-hop and2-hop questions, our framework is able to generatesome high-quality 3-hop questions, demonstratingthe extensibility of our framework.
it can be ex-pected that the performance of our model can befurther strengthened if a small training set of 3-hopquestion data is available..besides, it can also be observed that though thecontexts and answers of these two questions arethe same, two different questions with differentunderlying logic are generated, illustrating that theextracted reasoning chain effectively controls thequestion content..however, when generating questions with morethan 3 hops, we ﬁnd that the question quality dras-tically declines.
the semantic errors become morepopular, and some content tend to be unreason-ably repeated.
it is probably because the input ofqgrewrite has become too long to be precisely en-coded by the gpt2-small model due to the growinglength of the question.
it will be our future workto explore how to effectively extend our method tomore-hop question generation..59750.02.55.07.510 1 x p e h u  r i  $ x j p h q w h g  6 d p s o h v    l q  w k r x v d q g v 0.660.680.700.720.740.760.780.80 ( 0100%hotpotqa + ours100%hotpotqa + gpt225%hotpotqa + ours25%hotpotqa + gpt20.02.55.07.510 1 x p e h u  r i  $ x j p h q w h g  6 d p s o h v    l q  w k r x v d q g v 0.740.760.780.800.820.840.860.88 ) 100%hotpotqa + ours100%hotpotqa + gpt225%hotpotqa + ours25%hotpotqa + gpt2contextreasoningqg processhollywood arms is a play by carrie hamilton and carol burnett.
it ran at the goodman theatre and on broadway in 2002…q :what was run at the goodman theatre in 2002?q :what play by carrie hamilton was run at the goodman theatre in 2002?reasoning chainqg processq:whichactorwhostarredintopgun?q:whichstaroftopgunwasalsointhemovierainman?q:whichstaroftopgunwasalsointhemoviedirectedbybarrylevinson?q:whichactorwhostarredintopgun?q:whatactorstarredinthefilmthatwasdirectedbytonyscott?q:whatactorstarredinthefilmthatwasdirectedbytonyscottandwasreleasedin1986?112323top guntom cruiserain manstarredstarredbarrylevinsondirectedtop guntom cruisetonyscottdirectedstarreda 1986action filmis6 conclusion.
we explored the task of difﬁculty-controllable ques-tion generation, with question difﬁculty redeﬁnedas the inference steps required to answer it.
astep-by-step generation framework was proposedto accomplish this objective, with an input samplerto extract the reasoning chain, a question genera-tor to produce a simple question, and a questionrewriter to further adapt it into a more complex one.
a dataset was automatically constructed based onhotpotqa to facilitate the research.
extensive eval-uations demonstrated that our method can effec-tively control difﬁculty of the generated questions,and keep high question quality at the same time..acknowledgments.
thanks to zijing ou, yafei liu and suyuchen wangfor their helpful comments on this paper..references.
zi chai and xiaojun wan.
2020. learning to ask more:semi-autoregressive sequential question generationunder dual-graph interaction.
in proceedings of the58th annual meeting of the association for compu-tational linguistics, pages 225–237.
association forcomputational linguistics..jacob devlin, ming-wei chang, kenton lee, andkristina toutanova.
2019. bert: pre-training ofdeep bidirectional transformers for language under-standing.
in proceedings of the 2019 conference ofthe north american chapter of the association forcomputational linguistics: human language tech-nologies, volume 1 (long and short papers), pages4171–4186.
association for computational linguis-tics..ahmed elgohary, denis peskov, and jordan l. boyd-graber.
2019. can you unpack that?
learning torewrite questions-in-context.
in proceedings of the2019 conference on empirical methods in naturallanguage processing and the 9th international jointconference on natural language processing, pages5917–5923.
association for computational linguis-tics..angela fan, claire gardent, chloé braud, and antoinebordes.
2019. using local knowledge graph con-struction to scale seq2seq models to multi-documentin proceedings of the 2019 conference oninputs.
empirical methods in natural language processingand the 9th international joint conference on natu-ral language processing, pages 4184–4194.
associ-ation for computational linguistics..yifan gao, lidong bing, wang chen, michael r. lyu,and irwin king.
2019. difﬁculty controllable gener-.
ation of reading comprehension questions.
in pro-ceedings of the twenty-eighth international jointconference on artiﬁcial intelligence, pages 4968–4974..junmo kang, haritz puerto san roman, and sung-hyon myaeng.
2019. let me know what to ask:interrogative-word-aware question generation.
inproceedings of the 2nd workshop on machine read-ing for question answering, pages 163–171.
associ-ation for computational linguistics..tushar khot, daniel khashabi, kyle richardson, peterclark, and ashish sabharwal.
2020. text modularnetworks: learning to decompose tasks in the lan-guage of existing models.
corr, abs/2009.00751..yanghoon kim, hwanhee lee, joongbo shin, and ky-omin jung.
2019. improving neural question gener-ation using answer separation.
in the thirty-thirdaaai conference on artiﬁcial intelligence, pages6602–6609.
aaai press..diederik p. kingma and jimmy ba.
2015. adam: amethod for stochastic optimization.
in 3rd interna-tional conference on learning representations..vishwajeet kumar, yuncheng hua, ganesh ramakrish-nan, guilin qi, lianli gao, and yuan-fang li.
2019.difﬁculty-controllable multi-hop question genera-in the 18th interna-tion from knowledge graphs.
tional semantic web conference, volume 11778 oflecture notes in computer science, pages 382–398.
springer..ghader kurdi, jared leo, bijan parsia, uli sattler, andsalam al-emari.
2020. a systematic review of auto-matic question generation for educational purposes.
int.
j. artif.
intell.
educ., 30(1):121–204..alon lavie and abhaya agarwal.
2007. meteor: anautomatic metric for mt evaluation with high levelsin proceed-of correlation with human judgments.
ings of the second workshop on statistical machinetranslation, pages 228–231.
association for compu-tational linguistics..kenton lee, luheng he, mike lewis, and luke zettle-moyer.
2017. end-to-end neural coreference reso-in proceedings of the 2017 conference onlution.
empirical methods in natural language processing,pages 188–197.
association for computational lin-guistics..bang liu, haojie wei, di niu, haolan chen, andyancheng he.
2020. asking questions the humanway: scalable question-answer generation from textcorpus.
in the web conference, pages 2032–2043.
acm / iw3c2..bang liu, mingjun zhao, di niu, kunfeng lai,yancheng he, haojie wei, and yu xu.
2019a.
learn-ing to generate questions by learningwhat not to gen-in the world wide web conference, pageserate.
1106–1118..5976yinhan liu, myle ott, naman goyal, jingfei du, man-dar joshi, danqi chen, omer levy, mike lewis,luke zettlemoyer, and veselin stoyanov.
2019b.
roberta: a robustly optimized bert pretrainingapproach.
corr, abs/1907.11692..ilya loshchilov and frank hutter.
2017.weight decay regularization in adam.
abs/1711.05101..fixingcorr,.
sewon min, victor zhong, luke zettlemoyer, and han-naneh hajishirzi.
2019. multi-hop reading compre-hension through question decomposition and rescor-in proceedings of the 57th conference of theing.
association for computational linguistics, volume1: long papers, pages 6097–6109.
association forcomputational linguistics..liangming pan, wenqiang lei, tat-seng chua, andmin-yen kan. 2019. recent advances in neuralquestion generation.
corr, abs/1905.08949..liangming pan, yuxi xie, yansong feng, tat-sengchua, and min-yen kan. 2020. semantic graphsfor generating deep questions.
in proceedings of the58th annual meeting of the association for compu-tational linguistics, pages 1463–1475.
associationfor computational linguistics..kishore papineni, salim roukos, todd ward, and wei-jing zhu.
2002. bleu: a method for automatic eval-uation of machine translation.
in proceedings of the40th annual meeting of the association for compu-tational linguistics, pages 311–318.
acl..judea pearl and azaria paz.
1986. graphoids: graph-based logic for reasoning about relevance relationsor when would x tell you more about y if you al-ready know z?
in advances in artiﬁcial intelligenceii, seventh european conference on artiﬁcial intel-ligence, ecai, pages 357–363.
north-holland..alec radford, jeffrey wu, rewon child, david luan,dario amodei, and ilya sutskever.
2019. languagemodels are unsupervised multitask learners.
openaiblog, 1(8):9..pranav rajpurkar, jian zhang, konstantin lopyrev, andpercy liang.
2016. squad: 100, 000+ questionsin proceed-for machine comprehension of text.
ings of the 2016 conference on empirical methodsin natural language processing, pages 2383–2392.
association for computational linguistics..mrinmaya sachan and eric xing.
2016. easy ques-tions ﬁrst?
a case study on curriculum learningfor question answering.
in proceedings of the 54thannual meeting of the association for computa-tional linguistics (volume 1: long papers), pages453–463, berlin, germany.
association for compu-tational linguistics..victor sanh, lysandre debut, julien chaumond, andthomas wolf.
2019. distilbert, a distilled versionof bert: smaller, faster, cheaper and lighter.
arxivpreprint arxiv:1910.01108..gabriel stanovsky, julian michael, luke zettlemoyer,and ido dagan.
2018. supervised open informationextraction.
in proceedings of the 2018 conferenceof the north american chapter of the associationfor computational linguistics: human languagetechnologies, volume 1 (long papers), pages 885–895. association for computational linguistics..alon talmor and jonathan berant.
2018. the web asa knowledge-base for answering complex questions.
in proceedings of the 2018 conference of the northamerican chapter of the association for computa-tional linguistics: human language technologies,volume 1 (long papers), pages 641–651.
associa-tion for computational linguistics..svitlana vakulenko, shayne longpre, zhucheng tu,and raviteja anantha.
2020. question rewriting forconversational question answering.
arxiv preprintarxiv:2004.14652..ramakrishna vedantam, c. lawrence zitnick, anddevi parikh.
2015. cider: consensus-based imagedescription evaluation.
in ieee conference on com-puter vision and pattern recognition, pages 4566–4575. ieee computer society..thomas wolf, lysandre debut, victor sanh, julienchaumond, clement delangue, anthony moi, pier-ric cistac, tim rault, rémi louf, morgan funtow-icz, and jamie brew.
2019. huggingface’s trans-formers: state-of-the-art natural language process-ing.
corr, abs/1910.03771..zhilin yang, peng qi, saizheng zhang, yoshua ben-gio, william w. cohen, ruslan salakhutdinov, andchristopher d. manning.
2018.hotpotqa: adataset for diverse, explainable multi-hop questionanswering.
in proceedings of the 2018 conferenceon empirical methods in natural language pro-cessing, pages 2369–2380.
association for compu-tational linguistics..jianxing yu, xiaojun quan, qinliang su, and jian yin.
2020. generating multi-hop reasoning questions toin theimprove machine reading comprehension.
web conference, pages 281–291.
acm / iw3c2..yuyu zhang, xinshi chen, yuan yang, arun rama-murthy, bo li, yuan qi, and le song.
2020. efﬁ-cient probabilistic logic reasoning with graph neuralnetworks.
in 8th international conference on learn-ing representations.
openreview.net..qingyu zhou, nan yang, furu wei, chuanqi tan,hangbo bao, and ming zhou.
2017. neural ques-tion generation from text: a preliminary study.
inin proceedings of 6th ccf international confer-ence on natural language, volume 10619 of lec-ture notes in computer science, pages 662–671.
springer..kangli zi, xingwu sun, yanan cao, shi wang, xi-aoming feng, zhaobo ma, and cungen cao.
2019.answer-focused and position-aware neural network.
5977for transfer learning in question generation.
in pro-ceedings of the 12th international conference ofknowledge science, engineering and management,volume 11776 of lecture notes in computer sci-ence, pages 339–352.
springer..5978