diversifying dialog generation via adaptive label smoothingyida wang1,2∗, yinhe zheng1,3∗, yong jiang2, minlie huang1 †1 the coai group, dcst, institute for artiﬁcial intelligence, state key lab of intelligenttechnology and systems, beijing national research center for informationscience and technology, tsinghua university, beijing, china2 tsinghua-berkeley shenzhen institute, tsinghua shenzhen international graduate school,tsinghua university, shenzhen, china3 samsung research china - beijing (src-b)wangyd18@mails.tsinghua.edu.cn, yh.zheng@samsung.com,jiangy@sz.tsinghua.edu.cn, aihuang@tsinghua.edu.cn.
abstract.
neural dialogue generation models trainedwith the one-hot target distribution suffer fromthe over-conﬁdence issue, which leads to poorgeneration diversity as widely reported in theliterature.
although existing approaches suchas label smoothing can alleviate this issue, theyfail to adapt to diverse dialog contexts.
in thispaper, we propose an adaptive label smooth-ing (adalabel) approach that can adaptivelyestimate a target label distribution at each timestep for different contexts.
the maximumprobability in the predicted distribution is usedto modify the soft target distribution producedby a novel light-weight bi-directional decodermodule.
the resulting target distribution isaware of both previous and future contexts andis adjusted to avoid over-training the dialoguemodel.
our model can be trained in an end-to-end manner.
extensive experiments on twobenchmark datasets show that our approachoutperforms various competitive baselines inproducing diverse responses..1.introduction.
the success of neural models has greatly advancedthe research of dialog generation (huang et al.,2020; wang et al., 2020; zhang et al., 2020).
how-ever, most of these models suffer from a low-diversity issue where models tend to generate blandand generic responses such as i don’t know or i’mok (li et al., 2016).
although various approacheshave been proposed to tackle this issue (li et al.,2016; zhao et al., 2017; du et al., 2018; zhou et al.,2018; welleck et al., 2020; zheng et al., 2020b),there are still remarkable gaps between responsesgenerated by neural models and those from hu-mans (holtzman et al., 2020).
further, some ex-isting methods may even harm the ﬂuency or co-herence when improving the diversity of generated.
∗ equal contribution† corresponding author: aihuang@tsinghua.edu.cn.
figure 1: a dialogue sampled from the opensubtitlesdataset.
we demonstrate the hard target, label smooth-ing, and adaptive label smoothing approach whenlearning to predict the next word (“human”)..responses.
(ippolito et al., 2019; massarelli et al.,2020; zheng et al., 2020a)..recently, jiang and de rijke (2018); jiang et al.
(2019) show that there is a strong connection be-tween the low-diversity problem and the over-conﬁdence issue.
i.e., over-conﬁdent dialogue mod-els tend to produce low-diversity responses.
oneof the reasons can be attributed to the supervisiontarget.
speciﬁcally, training a dialogue generationmodel with the maximum likelihood estimation(mle) objective under the hard target (i.e., one-hotdistribution as ground truth) makes the model favorhigh-frequency tokens and produce over-conﬁdentprobability estimation (gowda and may, 2020),which ultimately leads to poor calibration (mukhotiet al., 2020), and thus low diversity (jiang et al.,2019).
hinton et al.
(2015) and yang et al.
(2018)suggest that the ideal training target should be asoft target that assigns probability mass on multiplevalid candidates (see figure 1).
with such a softtarget, the over-conﬁdence issue can be alleviated(m¨uller et al., 2019), and thus the diversity of theoutput responses can be improved..unfortunately, the ideal soft target is challeng-ing to obtain.
early works try to tackle this issue.
proceedingsofthe59thannualmeetingoftheassociationforcomputationallinguisticsandthe11thinternationaljointconferenceonnaturallanguageprocessing,pages3507–3520august1–6,2021.©2021associationforcomputationallinguistics3507so, what exactly do you do around here ?i make the robots seem more ___post:response:human0.91.00.610.010.00.01bank0.010.00.01fights0.010.00.10ugly0.010.00.08dull0.010.00.11fun…hard target(one hot)label smoothingadalabel(ours)using label smoothing (szegedy et al., 2016), i.e.,a small probability is uniformly assigned to non-target words.
however, the target distribution con-structed in this way is far from ideal: first, theprobability of the target word is chosen manuallyand ﬁxed, which cannot adapt to different contexts.
however, as holtzman et al.
(2020) demonstrated,human text distribution exhibits remarkable ﬂuctu-ations in the per-token perplexity.
we argue thatdifferent target probabilities should be used for dif-ferent contexts.
second, the uniform assignmentof the probability mass on non-target words ignoresthe semantic relationship between the context andeach word.
ideally, a word should receive moreprobability mass if it is more relevant to the con-text.
for the example shown in figure 1, word “fun”is more likely to appear behind the context “i makethe robots seem more ” than word “bank”..to address the above issue, we propose anadaptive label smoothing (adalabel) methodthat can dynamically estimate a soft target distribu-tion at each time step for different contexts.
specif-ically, for each target word yt in the training data,the probability distribution predicted by the currentmodel is ﬁrst obtained.
the maximum probabilitypmax in this distribution measures the conﬁdenceof the current prediction, i.e., a higher pmax meanshigher conﬁdence for the current prediction.
toavoid over-conﬁdence, we use pmax as the super-vision signal for the target word yt in the trainingprocess so that the model will not be optimized to-wards yt when it correctly predicts yt.
a word-levelfactor is also introduced to facilitate the learning oflow-frequency words..moreover, we introduce a novel auxiliary de-coder module da to produce the supervision sig-nals for these non-target words in each training step.
da only contains one transformer block, and it isoptimized to predict words based on bi-directionalcontexts.
a novel target-mask attention scheme isdevised to prevent da from seeing the target wordin the training process.
this scheme also enablesparallel training and inference of da..we perform extensive experiments on two bench-mark datasets: dailydialog and opensubtitles.
our method outperforms various competitive base-lines and signiﬁcantly improves the diversity ofgenerated responses while ensuring ﬂuency and co-herency.
our major contributions are summarized:.
current context and the model’s conﬁdence.
specif-ically, adalabel ensures that the dialogue modelwill not be optimized toward the target word yt ifyt has been correctly predicted.
this prevents ourmodel from being over-conﬁdent..2. we introduce a light-weight bi-directional de-coder that can produce context-aware supervisionsignals for non-target words.
a novel target-maskattention scheme is devised to facilitate the paralleltraining and inference of this decoder..3. extensive experiments on two benchmark di-alogue datasets with both automatic and humanevaluation results show that our method helps toalleviate the model over-conﬁdent issue and signif-icantly improves the model’s diversity..2 related work.
diversity promotion: existing approaches forsolving the low diversity issue of neural dialoguemodels generally involve two categories:.
the ﬁrst category is training-based, where newtraining objectives are designed (li et al., 2016;zhang et al., 2018; gao et al., 2019) or latent vari-ables are introduced (zhao et al., 2017; zhou et al.,2018) in the dialogue model.
some methods alsotry to reﬁne the training target used in the mleloss (choi et al., 2020; jiang et al., 2019; li et al.,2019), or directly penalize the trivial responseswith auxiliary loss terms (welleck et al., 2020; liet al., 2020).
unlike these existing approaches, ourmethod tries to adaptively adjust the training targetby utilizing the current predictions..the second category is decoding-based, in whichdifferent heuristic decoding rules are designed(holtzman et al., 2020; kulikov et al., 2019).
notethat these decoding techniques are independent ofthe model setting, and our method can be used incombination with these techniques..conﬁdence calibration: modern deep neuralnetworks suffer from the over-conﬁdence issue(guo et al., 2017; kumar and sarawagi, 2019), andvarious remedies are proposed (pereyra et al., 2017;mukhoti et al., 2020; lin et al., 2017).
followingthe work of jiang and de rijke (2018); jiang et al.
(2019), our method is proposed to tackle the over-conﬁdence issue to improve the diversity of thegenerated responses.
however, different from exist-ing approaches, our method enables more ﬂexiblecontrols over the target distribution..1. we propose adalabel, a method that canproduce a soft target distribution considering the.
knowledge distillation: another importanttechnique similar to our work is knowledge distilla-.
3508figure 2: overview of constructing the adaptive soft target q(cid:48) using adalabel: the maximum probability pmax inthe predicted distribution p is used to obtain an adaption factor (cid:15), which is further used to combine the hard targetq and the auxiliary distribution v to obtain q(cid:48).
a bi-directional auxiliary decoder da is used to produce v..tion, in which a learned teacher model is distilled toa student model by minimizing a kl term (hintonet al., 2015; kim and rush, 2016)..the dependency of yt in the notation of each distri-bution in our paper, i.e., different target word yt iny corresponds to different values of q and p..the most related work comparing to ours is thec-mlm approach (chen et al., 2020), in which abert model is ﬁne-tuned to be a teacher.
our ap-proach and c-mlm’s primary difference is that ourauxiliary decoder da is a one layer module that isjointly trained with the dialogue model.
however,the bert teacher in c-mlm contains much moreparameters, and it is trained using an expensive pre-trained and then ﬁne-tuned process.
moreover, thetarget-masked attention scheme in da enables par-allel inferences of v for each training sequence y .
in contrast, multiple independent forward passesare required for the bert teacher..3 method.
3.1 background: mle with hard target.
the goal of generative dialogue modeling is tolearn a conditional probability distribution p(y |x),where x is the dialogue context, y = y1, ..., yt isa response word sequence, and yi ∈ v is a wordfrom the vocabulary v. in an auto-regressive man-ner, p(y |x) is factorized as (cid:81)t p(yt|y<t, x).
foreach target word yt in the training sequence y , aconventional mle training approach try to opti-mize the following cross entropy loss:.
l(q, p) = −.
qklog [p(wk|y<t, x)] ,.
(1).
(cid:88).
wk∈v.
where q is a one-hot distribution (i.e., a hard tar-get) that assigns a probability of 1 for the targetword yt and 0 otherwise, i.e., qk = 1 only whenwk = yt.
for simplicity of notation, we abbreviate.
3.2 method overview.
we propose to adaptively construct a soft targetdistribution q(cid:48) to replace q in eq.
1. speciﬁcally,.
q(cid:48) = ε · q + (1 − ε) · v,.
(2).
where ε ∈ [0, 1] is an adaption factor, and v is anauxiliary distribution vector that depends on thecurrent time step.
(see figure 2 for an overview)..in this study, we constrain v to assign zero prob-ability for the target word yt and non-zero proba-bilities for these non-target words v(cid:54)=yt = {yi|yi ∈v, yi (cid:54)= yt}.
this constraint allows us to explicitlycontrol the supervisions assigned to yt.
speciﬁcally,the ﬁrst term ε · q and the second term (1 − ε) · vin eq.
2 respectively determines how much proba-bility q(cid:48) assigns to yt and v(cid:54)=yt.
this setting differsfrom conventional knowledge distillation (kim andrush, 2016) because it facilitates more ﬂexiblecontrols over q(cid:48), so that we can use the factor ε todetermine the supervision signal provided for thetarget word yt.
the following sections detail howto compute ε and v..3.3 target word probability.
we control the probability of the target word yt inp(cid:48) by manipulating the adaption factor ε in eq.
2.speciﬁcally, for a training dialogue pair (cid:104)x, y (cid:105) andeach target word yt ∈ y , the current distributionp(·|y<t, x) is ﬁrst calculated, and the maximumprobability in this distribution is obtained:.
pmax = maxwk∈v.
p(wk|y<t, x)..(3).
3509encoderdecodercontext 𝑋auxiliary decoder 𝒟(cid:3028)training response𝜖(cid:3400)(cid:4666)1(cid:3398)𝜖(cid:4667)(cid:3400)𝜖𝐵𝑂𝑆𝒗𝑦(cid:2869)𝑦(cid:2869)𝒗𝑦(cid:2870)𝑦(cid:2870)𝒗𝑦(cid:2871)𝑦(cid:3021)𝒗(cid:4670)𝐸𝑂𝑆(cid:4671)𝑦(cid:2871)𝒗𝑦(cid:2872)……𝐵𝑂𝑆𝑦(cid:2869)𝑦(cid:2869)𝑦(cid:2870)𝑦(cid:2870)𝑦(cid:2871)𝛼𝑝(cid:4666)𝑦(cid:2871)(cid:4667)auxiliary distribution 𝒗𝑝(cid:4666)𝑦(cid:2871)(cid:4667)hard target 𝒒𝑝(cid:4666)𝑦(cid:2871)(cid:4667)adaptive soft target 𝒒(cid:4593)ℒ(cid:4666)𝒒(cid:4593),𝒑(cid:4667)partial responsepredicted distribution 𝒑𝑝(cid:3040)(cid:3028)(cid:3051)𝑝(cid:4666)𝑦(cid:2871)(cid:4667)ε is then obtained:.
ε = max(pmax, λ),.
(4).
where λ serves as a lower-bound of ε (i.e., ε ≥ λ).
the basic intuition behind eq.
4 is to set ε =pmax when pmax is reasonably large.
this designprevents our model from receiving supervisionssharper than pmax, when the current prediction isconﬁdence enough..further, to ensure that the target word yt alwaysreceives the largest probability in q(cid:48), i.e., to ensureε > (1 − ε) · max(v) (see eq.
2), in which max(v)is the maximum probabilities for non-target wordsv(cid:54)=yt, we have to enforce ε > max(v)1+max(v) .
thus wepropose to calculate the lower-bound λ of ε as:.
λ =.
max(v)1 + max(v).
+ η,.
(5).
where η > 0 is a hyper-parameter that controls themargin between the probability of the target wordand non-target words in p(cid:48)..to facilitate faster converge and better learningof low-probability words, an empirical factor α ∈[0, 1] is further introduced to adjust the calculationof ε on the basis of eq.
4:.
ε = 1 − α · (1 − max(pmax, λ)),.
(6).
where α is calculated as the relative ratio to pmax:.
α =.
(cid:20) p(yt|y<t, x)pmax.
(cid:21)2.,.
(7).
where p(yt|y<t, x) is the probability for the targetword yt.
note that eq.
6 and eq.
4 is equivalent ifα = 1. intuitively, α accelerates the training of low-frequency words because if yt is of low-frequencyin the corpus, then yt is usually under-trained andthus p(yt|y<t, x) is generally small.
this leads toa small α and thus increases the probability for ytin p(cid:48)..note that ε, λ and α are all time-step speciﬁcvariables, whereas η is a ﬁxed hyper-parameter.
this allows the values adapt to dynamic contexts.
in our experiments, eq.
6 is used to calculate ε..3.4 non-target words probabilities.
the auxiliary distribution v in eq.
2 is calculatedusing an auxiliary decoder da, which is a single-layer transformer-based decoder that is jointly opti-mized with the generation model.
figure 3 showsthe structure of da, in which a novel target-masked.
figure 3: (a) the auxiliary decoder da; (b) the target-masked attention scheme used to compute the auxiliarydistribution v for the target word y3, speciﬁcally, y2is used as the query and y3 is masked; (c) the atten-tion pattern used in the target-masked attention scheme,white dots represent masked positions..attention scheme is devised to mask each targetword yt in the self attention module of the decoderwhen calculating the corresponding v (see figure3b and 3c).
in this way, bi-directional contexts canbe utilized when predicting the auxiliary distribu-tion v for yt.
moreover, it is important to use onlyone decoder layer in da because stacking multiplelayers in da leaks the information of yt to v..note that using one layer in da does not nec-essarily downgrade its performance (kasai et al.,2021).
our experiment results in section 5.1 indi-cate that with the help of bi-directional contexts,the accuracy of da largely outperforms the uni-directional dialogue decoder that is much deeperthan da.
moreover, for a training response y , thestructure of da enables us infer the auxiliary dis-tribution in parallel for all the target words in ywithin a single forward pass.
this differs from thebert teacher used by chen et al.
(2020), in whichmultiple independent forward passes are needed toget the teacher distributions for all the words in y .
when training da, the following standard mle.
loss is optimized for each target word yt:.
l(q, v) = −.
qklogvk,.
(8).
|v|(cid:88).
k=1.
in which the notation of qk follows eq.
1..the outputs of da are used as the logits to infer vto be further used in eq.
2. speciﬁcally, the logit ofthe target word yt is masked to −∞ before softmaxto ensure yt always receives zero probability in v.moreover, we also follow the approach used bytang et al.
(2020) to truncate the head and tailof the remaining logits before inferring v in eq..3510𝐵𝑂𝑆𝒗𝑦(cid:2869)𝑦(cid:2869)𝒗𝑦(cid:2870)𝑦(cid:2870)𝒗𝑦(cid:2871)𝒗[𝐸𝑂𝑆]𝑦(cid:2871)𝒗𝑦(cid:2872)𝑄𝐾,𝑉𝑦(cid:2872)target-masked attention(b)(c)feed forwardtarget-maskedmulti-head attentionadd & normmulti-head attentionadd & normencoderoutputsadd & norm1×(a)𝑄𝐾,𝑉train.
valid.
test.
dailydialogopensubtitles.
65.8k 6.13k 5.80k1.14m 20.0k 10.0k.
table 1: dataset statistics..2, i.e., all the logits are ranked in a descendingorder and only the logits ranked from n to m arekept while the rest logits are masked to −∞.
thismasks the head and tail probabilities in v to zero.
we argue that truncating the tail probabilities of vﬁlters noises, and truncating the head probabilitiesof v encourages the dialogue model to focus moreon low-probability words.
in our experiments, weset n = 2 and m = 500. an extensive hyper-parameter search indicates that our method is notsensitive to the value of n and m..there are two major differences between ourauxiliary decoder da and the teacher model usedin conventional knowledge distillation approaches:first, conventional teacher models usually carrymore parameters than their students, whereas da israther light-weight.
second, conventional teachermodels are typically pre-trained before being uti-lized in the distillation process, whereas da istrained jointly with our dialogue model..4 experiments.
4.1 dataset.
we use two benchmark datasets for open-domaindialogue generation: dailydialog (li et al., 2017)is a high-quality multi-turn dialogue dataset that iscollected from daily conversations.
opensubtitles1 contains dialogues collected from movie subtitles.
moreover, we follow li et al.
(2016) and jianget al.
(2019) to focus on short conversations, i.e.,dialogues with posts or responses longer than 100tokens are removed.
see table 1 for more details..4.2.implementation details.
the backbone of our model is the transformer-based sequence to sequence model (vaswani et al.,2017), and most hyper-parameters follow cai et al.
(2020).
speciﬁcally, the encoder and decoder eachcontains 6 layers.
each layer has 8 attention heads,and the hidden size is set to 512. the auxiliarydecoder da follows the same hyper-parameter set-ting as the dialogue decoder, but it only containsone layer.
the wordpiece tokenizer provided by.
bert (devlin et al., 2019) is used, and the adamoptimizer (kingma and ba, 2015) is employed totrain our model from random initializations with alearning rate of 1e-4.
η in eq.
5 is set to 0.2 for alldatasets.
see appendix a for more details.
2.
4.3 baselines.
we compared our method with two groups of base-lines that try to tackle the over-conﬁdence issue..the ﬁrst group modiﬁes the training target usedto compute the loss function: 1) ls (szegedyet al., 2016): uses the label smoothing approach toconstruct a target distribution by adding the one-hot target and a uniform distribution; 2) fl (linet al., 2017): uses the focal loss to down-weighwell-classiﬁed tokens in each time step.
3) face(jiang et al., 2019): uses the frequency-aware cross-entropy loss to balance per-token training losses.
speciﬁcally, relative low losses are assigned tohigh-frequency words to explicitly tackle the over-conﬁdence issue.
we used the best performing“pre-weigh” version in our experiments.
4) f2(choi et al., 2020): factorizes the target distributionbased on the token frequencies..the second group of baselines add some penaltyterm to the standard mle loss: 5) cp (pereyraet al., 2017): a conﬁdence penalty term is addedto regularize the entropy of the model, so thatover-conﬁdent predictions are penalized; 6) ul(welleck et al., 2020): an unlikelihood loss term isadded to penalize the frequently generated words.
7) nl (he and glass, 2020): works similarly withbaseline ul except a negative loss term is usedinstead of the unlikelihood loss term.
8) d2gpo(li et al., 2019): augments the mle loss with adata-dependent gaussian prior objective to assigndifferent losses for different non-target words..we also compared to: 9) ce: a vanilla seq2seqmodel trained with the cross-entropy loss.
for faircomparisons, the c-mlm model proposed by chenet al.
(2020) is not used as our baseline since thebert teacher in c-mlm requires a large amountof extra data to pre-train.
nevertheless, adala-bel still surpasses c-mlm on various metrics (seeappendix f for more analysis)..all our baselines are adapted from the authors’ofﬁcial codes with the same backbone architectureand hyper-parameters as our model (see details inappendix b).
following the original setting, a train-.
2our code is available at: https://github.com/.
1http://opus.nlpl.eu/opensubtitles.php.
lemon234071/adalabel.
3511model.
celsflfacef2cpulnld2gpoadalabel.
dailydialog.
opensubtitles.
dist-1, 2.ent-1, 2.lf.
bleu-2,3,4.
dist-1, 2.ent-1, 2.bleu-2,3,4.
4.539.431.674.481.488.782.38 13.424.71.62 11.04 4.961.404.357.912.35 12.91 4.642.35 12.99 4.684.479.181.664.438.061.263.96 23.53 5.17.
6.596.557.047.276.286.896.986.586.488.00.
2.554.139.872.614.387.562.992.77 13.08 4.452.864.687.982.443.19 13.16 4.424.116.129.745.053.31 14.06 4.773.065.068.784.112.89 11.40 4.242.604.457.782.323.11 12.72 4.363.794.075.689.062.84 11.64 4.314.614.96 10.83 6.873.24 12.98 4.423.605.839.834.302.07 11.01 4.322.934.828.302.208.49 17.42 13.38 11.01 4.78 22.88 4.96.
5.586.576.507.056.146.356.326.496.367.66.lf.
0.840.511.041.330.990.980.761.080.191.47.
7.608.918.067.697.528.067.737.568.419.80.
4.305.574.794.404.304.824.594.385.086.48.
2.573.843.082.702.623.122.962.713.354.75.human.
6.59 37.74 5.67.
8.91.
13.7 n/a n/a n/a 8.62 43.16 5.89.
9.36.
4.75 n/a n/a n/a.
table 2: automatic evaluation results (%).
best results among all the models are in bold..and-reﬁne strategy is used in baseline 3, 6, and 7,i.e., these baselines are reﬁned based on ce.
wefollow the setting of jiang et al.
(2019) to use de-terministic decoding scheme (particularly, greedydecoding) for our model and all baselines.
notethat our method can be adapted to other decodingschemes such as beam-search or top-k sampling.
see appendix c for more detailed analysis..4.4 automatic evaluation.
metrics: we ﬁrst used automatic metrics to evalu-ate our method: 1) distinct (dist) (li et al., 2016)calculates the proportion of unique n-grams (n=1,2) in the generated responses, which is widelyused to measure the response diversity.
2) entropy(ent) (zhang et al., 2018) evaluates how evenly theempirical n-gram (n=1, 2) distribution is.
highersores mean more diverse of the response.
3) low-frequency token ratio (lf) (li et al., 2019) fur-ther measures the model diversity by counting theratio of low-frequency words in the generated re-sponses.
we chose words with a frequency lessthan 100 in each corpus as low-frequency words.
over-conﬁdent models tend to omit low-frequencywords (i.e., get low lf scores) and yield less diver-siﬁed responses.
4) bleu (papineni et al., 2002)measures n-gram (n=2, 3, 4) overlap between thegenerated responses and references..results: as shown in table 2, our method adal-abel outperforms all the baselines by large mar-gins on all the datasets.
we can further observethat: 1) adalabel achieves the best diversity scores(dist-1,2, ent-1,2, and lf).
this indicates that ourmethod yields better training targets that help toproduce more diverse responses; 2).
the mod-els that explicitly tackle the over-conﬁdence issue(i.e., adalabel and face) generally outperform.
other baselines in diversity-related metrics.
forexample, face obtains the second-best diversityscores (i.e., dist, ent, and lf) on the opensubtitlesdataset.
this veriﬁes our motivation that alleviatingthe over-conﬁdence issue helps to produce morediverse responses..note that our method also outperforms all thebaselines using the stochastic decoding scheme.
please refer to appendix c for more details..4.5 manual evaluation.
metrics: pairwise manual evaluations are con-ducted to further validate our method.
speciﬁ-cally, for a given dialogue post, our model’s re-sponse is paired with the one from a baseline.
three individual annotators were employed to rankeach response pair from three aspects: 1) flu-ency (flu.
): which response is more ﬂuent; 2) co-herency (coh.
): which response is more coherentto the context; 3) informativeness (info.
): which re-sponse contains more informative content.
we alsoasked the annotator to choose an overall preferredresponse (pref.).
ties were allowed..results: 200 posts were randomly sampled fromeach of these two datasets, respectively, and totally3.6k response pairs were generated.
the inter-raterannotation agreement was measured using fleiss’skappa κ (fleiss, 1971).
particularly, the κ value ondailydialog, opensubtitles dataset was 0.59 and0.55, respectively, indicating moderate agreement.
as shown in table 3, adalabel outperformsall the baselines on the informativeness measure.
this means that our method can respond with moreinformative content.
we can further observe that:.
1).
all models achieve competitive ﬂuency be-cause it is easy for neural models to produce ﬂu-ent responses by yielding trivial responses like “i.
3512comparison.
adalabel vs ceadalabel vs lsadalabel vs fladalabel vs faceadalabel vs f2adalabel vs cpadalabel vs uladalabel vs nladalabel vs d2gpo.
dailydialog.
opensubtitles.
pref..17.00‡2.674.506.67†7.67†10.50‡7.83†9.17†0.83.flu..1.330.171.673.50†0.33-0.170.832.67†0.00.coh..12.5‡3.337.00†7.17†6.83†8.00†6.67†9.17†3.33.info..28.33‡24.83‡22.0‡8.50†8.67‡23.83‡17.33‡7.67†15.17‡.
pref..6.335.38.00†4.504.338.00†6.83†5.173.17.flu..1.17-0.671.000.50-0.501.502.000.177.33‡.
coh..7.33†3.176.001.831.676.175.832.171.00.info..13.67‡8.50‡5.502.509.50‡16.83‡15.00‡15.5‡6.33†.
table 3: pairwise human evaluation results (%).
the absolute gains of adalabel (i.e., win rate − lose rate) arereported.
†, ‡ indicates signiﬁcant improvement with p-value < 0.05 and < 0.005, respectively (sign test)..model.
bleu-3,4.
dist-1,2.
ent-1,2.
1.w/o ε2.w/o α.
5.463.5711.35 8.70.
2.52 13.21 4.643.62 20.56 5.02.
3.orig.
v8.154.uniform 5.666.275.rand11.66.bert.
5.773.614.079.34.
3.71 19.53 5.002.24 14.96 4.842.03 13.474.73.67 20.97 5.02.lf.
4.857.30.
8.254.984.567.28.
6.897.70.
7.587.337.087.71.adalabel 13.38 11.01 3.96 23.53 5.17.
8.00.
8.49.table 4: ablation study results on dailydialog (%)..don’t know”.
however, our model surpasses mostbaselines in terms of ﬂuency while ensuring highdiversity scores.
this demonstrates the superiorityof our method in producing high quality responses.
2).
adalabel produces more coherent responsescomparing to most baselines.
this veriﬁes thatour model does not sacriﬁce the response qual-ity when achieving high diversity scores.
in fact,by controlling the model’s conﬁdence, more low-frequency words are encouraged, and thus adal-abel can produce more relevant and coherent re-sponses.
this claim is further veriﬁed by observingthat our model achieves the best overall preferencescore among all the baselines..4.6 ablation study.
ablation studies were performed to verify the effectof each component in our method.
speciﬁcally, twogroups of variants were tested:.
the ﬁrst group validates the effectiveness of thecalculated target word probability, i.e., ε: 1).
w/oε directly sets a ﬁxed value for ε in eq.
2. thespeciﬁc value of ε is searched from 0.1 to 0.7 witha stride of 0.1; 2).
w/o α omits the empirical factorα in calculating ε, i.e., the value of ε in eq.
2 iscalculated using eq.
4 in instead of eq.
6..the second group validates the effectiveness of.
the non-target word probabilities produced by da,i.e., v: 3).
orig.
v does not truncate the head of vwhen inferring from da.
note that the truncationfor the tail of v is still applied since its effective-ness has already been proved in previous studies(tang et al., 2020; tan et al., 2019); 4).
uniformuses an uniform distribution as v in eq.
2. notethat different from the baseline ls, the value of εis calculated using eq.
6 in this ablation model,whereas the value of ε in the baseline ls is ﬁxed ;5).
rand use a random distributions as v in eq.
2;6).
bert follows the work of chen et al.
(2020) toﬁne-tune a pre-trained bert model to produce v.note that our dialogue model may beneﬁt from themulti-task training of da since da shares the sameencoder with our dialogue model.
optimizing eq.
8 may help the encoder to capture better features.
for fair comparison, we kept the task of optimizingda in ablation models 4-6 although it is not usedto infer v..table 4 shows the results of ablation models onthe dailydialog dataset.
as can be seen from theﬁrst two rows, our method to adaptively calculate εhelps to improve the performance of our model bya large margin, and the empirical adjustment fac-tor α helps to further improve our performance byfacilitating the learning of low-probability words.
the performance of ablation models 3-6 in table4 proves that v captures reliable distribution andhelps our model produce more diverse responses.
moreover, truncating the head distribution of venables the dialogue model to focus more on thelow-frequency words and thus facilitates more in-formative responses..it is also interesting to note that our auxiliarydecoder da surpasses the bert teacher used bychen et al.
(2020) in helping the dialogue model.
3513dailydialog opensubtitles.
auxiliary decoder dadialog decoder in adalabeldialog decoder in ce.
64.0344.1638.58.
64.9243.9041.57.table 5: prediction accuracy of decoders on test sets..figure 4: empirical distribution of conﬁdence scoresfor high-frequency words on the opensubtitles dataset.
words occupying the top 40% of the frequency mass inthe training set are regarded as high-frequency words..to produce more diverse responses.
this furtherproves the effectiveness of da considering thatbert contains 6 times parameters than da andconsumes much more computation resources..5 discussion.
5.1 auxiliary decoder.
to further test the performance of da, we evalu-ated the averaged accuracy score of da when pre-dicting each target word in the test set (ﬁrst rowin table 5).
speciﬁcally, a target word yt in thereference response is determined to be correctlypredicted if it is top-ranked in the predicted distri-bution p(·|y<t, x).
a better decoder is generallybelieved to obtain a higher accuracy.
table 5 alsoreports the uni-directional dialogue decoders’ ac-curacy in adalabel and ce.
it can be seen thatda can make substantially more accurate predic-tions with the help of modeling bi-directional con-texts using only one layer.
moreover, the dialoguemodel’s decoder in adalabel, which is guided byda, achieves better accuracies than the ce.
thisfurther proves that our light-weight da is capableof producing effective v..5.2 prediction conﬁdence.
we also visualized the distribution of conﬁdencescores assigned by each dialogue model to high-frequency words.
figure 4 shows the results of.
figure 5: ratios of low-frequency tokens in the gener-ated responses on the opensubtitles dataset.
tokens ineach group are determined based on the frequency onthe training set..four best performing models on the opensubtitlesdataset.
the spikes of high conﬁdence score ob-served in figure 4b and 4d indicate that ce andface assign extremely high conﬁdence scoresto a large number of high-frequency words.
al-though the smoothed labels in ls manage to alle-viate these high-conﬁdence-spikes (figure 4c), aconsiderable amount of words still receives highconﬁdence scores in ls.
our model outperformsall the baselines to avoid assigning over-conﬁdencescores, thus alleviating the over-conﬁdence issue.
a similar trend is also observed on the dailydialogdataset (see appendix d for results of all modelson both datasets)..5.3 predicted rare word distribution.
over-conﬁdent models produce less diversiﬁed re-sponses because they usually under-estimate rarewords.
to evaluate the effectiveness of adalabel,we tested whether adalabel encourages more “rarewords” in its generations.
speciﬁcally, the ratio ofgenerated tokens corresponding to different tokenfrequency bins is calculated, and the results on theopensubtitles dataset are shown in figure 5. it canbe seen that adalabel produces more rare words inthe generated responses than other baselines.
sim-ilar results are also observed on the dailydialogdataset (see appendix e)..6 conclusion.
we address the low-diversity issue of neural di-alogue models by introducing an adaptive labelsmoothing approach, adalabel.
in our method,the probability of each target word is estimatedbased on the current dialogue model’s prediction,and the probabilities for these non-target words arecalculated using a novel auxiliary decoder da.
atarget-masked attention scheme is introduced in da.
35140.000.250.500.75confidence score0123density (%)(a) adalabel0.000.250.500.751.00confidence score012density (%)(b) ce0.000.250.500.751.00confidence score012density (%)(c) ls0.000.250.500.751.00confidence score012density (%)(d) face[1, 200][201, 400][401, 600][601, 800][801, 1000]token frequency0.00.51.01.52.02.5% of generated tokensadalabelfacenlflf2cpceullsd2gpoto help capture forward and backward contexts.
weevaluate our method on two benchmark datasets:dailydialog and opensubtitles.
extensive experi-ments show that our method effectively alleviatesthe over-conﬁdence issue and improves the diver-sity of the generated responses.
as future work,we believe this method is extensible to other textgeneration tasks..acknowledgments.
this work was partly supported by the nsfcprojects (key project with no.
61936010 and reg-ular project with no.
61876096).
this work wasalso supported by the guoqiang institute of ts-inghua university, with grant no.
2019gqg1 and2020gqg0005.
we thank jinchao zhang and yaoqiu for early discussions and insightful commentsof this work..references.
hengyi cai, hongshen chen, yonghao song, chengzhang, xiaofang zhao, and dawei yin.
2020. datamanipulation: towards effective instance learningfor neural dialogue generation via learning to aug-ment and reweight.
in proceedings of the 58th an-nual meeting of the association for computationallinguistics, pages 6334–6343, online.
associationfor computational linguistics..yen-chun chen, zhe gan, yu cheng, jingzhou liu,and jingjing liu.
2020.distilling knowledgelearned in bert for text generation.
in proceedingsof the 58th annual meeting of the association forcomputational linguistics, pages 7893–7905, on-line.
association for computational linguistics..byung-ju choi,.
jimin hong, david park,.
andsang wan lee.
2020. fˆ2-softmax: diversifyingneural text generation via frequency factorized soft-in proceedings of the 2020 conference onmax.
empirical methods in natural language process-ing (emnlp), pages 9167–9182, online.
associa-tion for computational linguistics..jacob devlin, ming-wei chang, kenton lee, andkristina toutanova.
2019. bert: pre-training ofdeep bidirectional transformers for language under-in proceedings of the 2019 conferencestanding.
of the north american chapter of the associationfor computational linguistics: human languagetechnologies, volume 1 (long and short papers),pages 4171–4186, minneapolis, minnesota.
associ-ation for computational linguistics..jiachen du, wenjie li, yulan he, ruifeng xu, lidongbing, and xuan wang.
2018. variational autoregres-sive decoder for neural response generation.
in pro-ceedings of the 2018 conference on empirical meth-.
ods in natural language processing, pages 3154–3163, brussels, belgium.
association for computa-tional linguistics..joseph l fleiss.
1971. measuring nominal scale agree-ment among many raters.
psychological bulletin,76(5):378..xiang gao, sungjin lee, yizhe zhang, chris brockett,michel galley, jianfeng gao, and bill dolan.
2019.jointly optimizing diversity and relevance in neuralin proceedings of the 2019response generation.
conference of the north american chapter of theassociation for computational linguistics: humanlanguage technologies, volume 1 (long and shortpapers), pages 1229–1238, minneapolis, minnesota.
association for computational linguistics..thamme gowda and jonathan may.
2020. finding theoptimal vocabulary size for neural machine transla-in findings of the association for computa-tion.
tional linguistics: emnlp 2020, pages 3955–3964,online.
association for computational linguistics..chuan guo, geoff pleiss, yu sun, and kilian q wein-berger.
2017. on calibration of modern neural net-in international conference on machineworks.
learning, pages 1321–1330.
pmlr..tianxing he and james glass.
2020. negative train-ing for neural dialogue response generation.
in pro-ceedings of the 58th annual meeting of the asso-ciation for computational linguistics, pages 2044–2058, online.
association for computational lin-guistics..geoffrey e. hinton, oriol vinyals, and jeffrey dean.
2015. distilling the knowledge in a neural network.
corr, abs/1503.02531..ari holtzman, jan buys, li du, maxwell forbes, andyejin choi.
2020. the curious case of neural textin 8th international conference ondegeneration.
learning representations, iclr 2020, addis ababa,ethiopia, april 26-30, 2020. openreview.net..minlie huang, xiaoyan zhu, and jianfeng gao.
2020.challenges in building intelligent open-domain dia-log systems.
acm transactions on information sys-tems (tois), 38(3):1–32..daphne ippolito, reno kriz, jo˜ao sedoc, mariakustikova, and chris callison-burch.
2019. com-parison of diverse decoding methods from condi-tional language models.
in proceedings of the 57thannual meeting of the association for computa-tional linguistics, pages 3752–3762, florence, italy.
association for computational linguistics..shaojie jiang, pengjie ren, christof monz, andmaarten de rijke.
2019. improving neural responsediversity with frequency-aware cross-entropy loss.
in the world wide web conference, pages 2879–2885..3515shaojie jiang and maarten de rijke.
2018. whyare sequence-to-sequence models so dull?
under-standing the low-diversity problem of chatbots.
inproceedings of the 2018 emnlp workshop scai:the 2nd international workshop on search-orientedconversational ai, pages 81–86, brussels, belgium.
association for computational linguistics..jungo kasai, nikolaos pappas, hao peng, james cross,and noah smith.
2021. deep encoder, shallowdecoder: reevaluating non-autoregressive machinein international conference on learn-translation.
ing representations..yoon kim and alexander m. rush.
2016. sequence-level knowledge distillation.
in proceedings of the2016 conference on empirical methods in natu-ral language processing, pages 1317–1327, austin,texas.
association for computational linguistics..diederik p. kingma and jimmy ba.
2015. adam: ain 3rd inter-method for stochastic optimization.
national conference on learning representations,iclr 2015, san diego, ca, usa, may 7-9, 2015,conference track proceedings..guillaume klein, yoon kim, yuntian deng, jean senel-lart, and alexander rush.
2017. opennmt: open-source toolkit for neural machine translation.
inproceedings of acl 2017, system demonstrations,pages 67–72, vancouver, canada.
association forcomputational linguistics..ilia kulikov, alexander miller, kyunghyun cho, andjason weston.
2019. importance of search and eval-uation strategies in neural dialogue modeling.
inproceedings of the 12th international conference onnatural language generation, pages 76–87, tokyo,japan.
association for computational linguistics..aviral kumar and sunita sarawagi.
2019. calibrationof encoder decoder models for neural machine trans-lation.
corr, abs/1903.00802..jiwei li, michel galley, chris brockett, jianfeng gao,and bill dolan.
2016. a diversity-promoting ob-jective function for neural conversation models.
inproceedings of the 2016 conference of the northamerican chapter of the association for computa-tional linguistics: human language technologies,pages 110–119, san diego, california.
associationfor computational linguistics..margaret li, stephen roller,.
ilia kulikov, seanwelleck, y-lan boureau, kyunghyun cho, and ja-son weston.
2020. don’t say that!
making inconsis-tent dialogue unlikely with unlikelihood training.
inproceedings of the 58th annual meeting of the asso-ciation for computational linguistics, pages 4715–4728, online.
association for computational lin-guistics..yanran li, hui su, xiaoyu shen, wenjie li, ziqiangcao, and shuzi niu.
2017. dailydialog: a manu-ally labelled multi-turn dialogue dataset.
in proceed-ings of the eighth international joint conference on.
natural language processing (volume 1: long pa-pers), pages 986–995, taipei, taiwan.
asian federa-tion of natural language processing..zuchao li, rui wang, kehai chen, masso utiyama,eiichiro sumita, zhuosheng zhang, and hai zhao.
2019. data-dependent gaussian prior objective forin international conferencelanguage generation.
on learning representations..tsung-yi lin, priya goyal, ross girshick, kaiminghe, and piotr doll´ar.
2017. focal loss for dense ob-ject detection.
in proceedings of the ieee interna-tional conference on computer vision, pages 2980–2988..luca massarelli, fabio petroni, aleksandra piktus,myle ott, tim rockt¨aschel, vassilis plachouras,fabrizio silvestri, and sebastian riedel.
2020. howdecoding strategies affect the veriﬁability of gener-ated text.
in findings of the association for compu-tational linguistics: emnlp 2020, pages 223–235,online.
association for computational linguistics..jishnu mukhoti, viveka kulharia, amartya sanyal, stu-art golodetz, philip h. s. torr, and puneet k. doka-nia.
2020. calibrating deep neural networks usingfocal loss.
in advances in neural information pro-cessing systems 33: annual conference on neu-ral information processing systems 2020, neurips2020, december 6-12, 2020, virtual..rafael m¨uller, simon kornblith, and geoffrey e hin-ton.
2019. when does label smoothing help?
inadvances in neural information processing systems,pages 4694–4703..kishore papineni, salim roukos, todd ward, and wei-jing zhu.
2002. bleu: a method for automatic eval-in proceedings ofuation of machine translation.
the 40th annual meeting of the association for com-putational linguistics, pages 311–318, philadelphia,pennsylvania, usa.
association for computationallinguistics..gabriel pereyra, george tucker,.
jan chorowski,lukasz kaiser, and geoffrey e. hinton.
2017. regu-larizing neural networks by penalizing conﬁdent out-in 5th international conferenceput distributions.
on learning representations, iclr 2017, toulon,france, april 24-26, 2017, workshop track proceed-ings.
openreview.net..christian szegedy, vincent vanhoucke, sergey ioffe,jon shlens, and zbigniew wojna.
2016. rethinkingthe inception architecture for computer vision.
inproceedings of the ieee conference on computer vi-sion and pattern recognition, pages 2818–2826..xu tan, yi ren, di he, tao qin, zhou zhao, andtie-yan liu.
2019. multilingual neural machinetranslation with knowledge distillation.
in 7th inter-national conference on learning representations,iclr 2019, new orleans, la, usa, may 6-9, 2019.openreview.net..3516jiaxi tang, rakesh shivanna, zhe zhao, dong lin, an-ima singh, ed h. chi, and sagar jain.
2020. un-derstanding and improving knowledge distillation.
corr, abs/2002.03532..ashish vaswani, noam shazeer, niki parmar, jakobuszkoreit, llion jones, aidan n. gomez, lukaszkaiser, and illia polosukhin.
2017. attention is allyou need.
in advances in neural information pro-cessing systems 30: annual conference on neuralinformation processing systems 2017, december 4-9, 2017, long beach, ca, usa, pages 5998–6008..yida wang, pei ke, yinhe zheng, kaili huang, yongjiang, xiaoyan zhu, and minlie huang.
2020. alarge-scale chinese short-text conversation dataset.
in ccf international conference on natural lan-guage processing and chinese computing, pages91–103.
springer..sean welleck, ilia kulikov, stephen roller, emily di-nan, kyunghyun cho, and jason weston.
2020. neu-ral text generation with unlikelihood training.
in8th international conference on learning represen-tations, iclr 2020, addis ababa, ethiopia, april26-30, 2020. openreview.net..chenglin yang, lingxi xie, siyuan qiao, and alan l.yuille.
2018. knowledge distillation in genera-tions: more tolerant teachers educate better students.
corr, abs/1805.05551..rongsheng zhang, yinhe zheng, jianzhi shao, xiaoximao, yadong xi, and minlie huang.
2020. dia-logue distillation: open-domain dialogue augmen-in proceedings of thetation using unpaired data.
2020 conference on empirical methods in naturallanguage processing (emnlp), pages 3449–3460..yizhe zhang, michel galley, jianfeng gao, zhe gan,xiujun li, chris brockett, and bill dolan.
2018.generating informative and diverse conversationalresponses via adversarial information maximization.
in advances in neural information processing sys-tems, pages 1810–1820..tiancheng zhao, ran zhao, and maxine eskenazi.
2017. learning discourse-level diversity for neuraldialog models using conditional variational autoen-in proceedings of the 55th annual meet-coders.
ing of the association for computational linguistics(volume 1: long papers), pages 654–664, vancou-ver, canada.
association for computational linguis-tics..yinhe zheng, zikai chen, rongsheng zhang, shileihuang, xiaoxi mao, and minlie huang.
2020a.
styl-ized dialogue response generation using stylized un-paired texts.
in aaai..yinhe zheng, rongsheng zhang, minlie huang, andxiaoxi mao.
2020b.
a pre-training based personal-ized dialogue generation model with persona-sparsedata.
in proceedings of the aaai conference on ar-tiﬁcial intelligence, volume 34, pages 9693–9700..ganbin zhou, ping luo, yijun xiao, fen lin, bo chen,and qing he.
2018. elastic responding machine fordialog generation with dynamically mechanism se-lecting.
in thirty-second aaai conference on arti-ﬁcial intelligence..a implementation details.
this appendix describes the implementation detailsof our model.
all our experiments are implementedwith python 3.7.4, pytorch 1.7.1, and the open-nmt package (klein et al., 2017).
training is per-formed on one titan xp gpu.
our model’s back-bone is the transformer-based sequence to sequencemodel, the encoder and decoder each contains 6transformer layers with 8 attention heads, and thehidden size is set to 512. the dimension of the feed-forward layer is also 512. the wordpiece tokenizerprovided by bert-base-uncased is used (the vocab-ulary contains 30522 tokens).
the total number ofparameters in our model is about 90m.
the adamoptimizer is employed to train our model from ran-dom initializations with β1 = 0.9, β2 = 0.999,(cid:15) = 1e − 9 and a learning rate of 1e-4.
the batchsize is set to 64 with 2 gradient accumulation sothat 2 * 64 samples are used for each parameterupdate.
the model is evaluated every 1000 stepson the validation set.
we use early-stopping withpatience 10, 30 for dailydialog and opensubtitles,respectively.
speciﬁcally, the model stops trainingwhen the evaluation perplexity and accuracy arenot increased for “patience” steps.
the model train-ing takes 4 hours and 3 days on dailydialog andopensubtitles, respectively..the auxiliary distribution produced by the aux-iliary decoder is smoothed with the temperaturescaling approach.
the temperature used in this pro-cess is searched in [1, 1.5, 2].
the temperaturevalue of 1.5 and 1.0 is used for dailydialog, andopensubtitles, respectively.
the hyper-parametervalue of η is set to 0.2 for all datasets.
the ﬁxedvalue of epsilon in our ablation model w/o (cid:15) issearched in [0.1, 0.2, 0.3, 0.4, 0.5, 0.6], and we ﬁndthe value of 0.1 works best..b baseline implementation details.
this appendix contains more implementation de-tails of our baselines.
all the baselines utilizethe same backbone architecture and basic hyper-parameter settings as our model (see appendix a).
the hyper-parameters specialized for each baselineis determined with the grid search based on the dist.
3517dailydialog.
opensubtitles.
dist-1, 2.ent-1, 2.lf.
bleu-2,3,4.
dist-1, 2.ent-1, 2.bleu-2,3,4.
4.198.211.791.714.168.012.40 11.37 4.394.549.471.801.614.047.222.30 10.39 4.284.4011.02.424.197.531.614.147.831.574.25 21.47 4.95.
5.905.896.356.405.706.166.426.055.917.51.
4.064.136.015.654.325.317.947.094.47.
1.582.571.652.172.754.462.173.481.522.112.303.253.694.552.914.021.712.267.68 14.71 11.63 9.80.
2.492.553.953.432.553.395.264.412.71.
2.484.079.212.89 12.79 4.273.10 12.37 4.253.12 12.62 4.472.89 10.63 4.033.14 11.87 4.172.77 10.43 3.982.65 10.14 4.212.06 10.43 4.154.91 21.53 4.71.
5.746.246.136.405.725.975.626.056.007.08.lf.
0.760.470.821.020.890.850.620.750.121.35.
7.038.247.135.976.927.286.897.167.328.68.
4.265.574.563.634.274.604.364.324.696.08.
2.824.203.252.432.913.213.032.853.334.68.model.
celsflfacef2cpulnld2gpoadalabel.
adalabel(greedy).
3.96 23.53 5.17.
8.00.
8.49 17.42 13.38 11.01 4.78 22.88 4.96.
7.66.
1.47.
9.80.
6.48.
4.75.human.
6.59 37.74 5.67.
8.91.
13.7 n/a n/a n/a 8.62 43.16 5.89.
9.36.
4.75 n/a n/a n/a.
table 6: automatic evaluation results (%) using the beam search decoding scheme (beam size is 5).
the bestresults among all these beam-search-decoded models are in bold..measures on the validation set: for label smooth-ing (ls), we searched the smoothing parameter in[0.05, 0.1, 0.2, 0.3, 0.4, 0.5], and found 0.1 worksbest on all the datasets; for conﬁdence penalty(cp), we searched the weight of penalty in [0.0005,0.001, 0.01, 0.05, 0.1] and found 0.05 works beston all the datasets while ensuring the loss to be pos-itive; for focal loss (fl), we searched the hyper-parameter γ in [0.1, 0.5, 1, 2, 3], and found 2works best on all the datasets.
for unlikelihoodloss (ul), we searched the weight of penalty in[1, 10, 100, 1000], and select 1000 on all thedatasets.
for face, we experiment with the out-put token frequency & pre-weigh version, whichis reported to be the best version of face.
fornegative loss (nl), f2-softmax (f2) and data-dependent gaussian prior objective (d2gpo),the selection of hyper-parameters follows the au-thor’s suggestion..c automatic evaluation results with.
other decoding schemes.
this appendix reports our model’s automatic eval-uation results and all the baselines when differentdecoding schemes are used.
speciﬁcally, table 6shows the results for the beam search decodingscheme (beam size of 5), and table 7 shows theresults when the top-k decoding scheme (k = 10)is used.
note that for the f2-softmax, we use thedecoupled top-k sampling as the authors suggested.
as can be seen from table 6 and 7, our methodoutperforms all the baselines on the diversity-related scores (i.e., dist, ent, and lf) by a largemargin.
this indicates that our method can produce.
more diverse responses even with the stochasticbased decoding scheme..we also include the results of adalabel whenthe greedy decoding scheme is used in table 6 andtable 7 (the second line from the bottom).
it is in-teresting to see that the greedily decoded responsesfrom adalabel are more diverse than some base-lines that are decoded using the sampling scheme(see table 7).
moreover, our model adalabelwith the greedy decoding scheme achieves the bestbleu among all the baselines on both datasets..d prediction conﬁdence.
this appendix reports the prediction conﬁdencescores assigned by each model to high-frequencywords.
speciﬁcally, words occupying the top 40%of the frequency mass in the training set of eachdataset are regarded as high-frequency words..figure 6 shows the results of our model and allthe baselines on the dailydialog dataset.
figure 7shows the results of our model and all the baselineson the opensubtitles dataset.
it can be seen thatmost of our baselines assign extremely high conﬁ-dence scores (nearly 1.0) to these high-frequencywords, and thus resulting in a spike of high conﬁ-dence scores in the plotted distribution.
our modeloutperforms all the baselines in avoiding assigningextremely high conﬁdence scores to these high-frequency words..e predicted rare word distribution on.
dailydialog.
this appendix shows the distribution of rare wordsin the generated responses on the dailydialog.
3518model.
dailydialog.
opensubtitles.
dist-1, 2.ent-1, 2.lf.
bleu-2,3,4.
dist-1, 2.ent-1, 2.bleu-2,3,4.
2.22 19.05 5.07ce1.95 17.74 5.02ls2.71 20.98 5.19fl2.29 21.14 5.36facef22.16 19.33 5.043.16 22.38 5.11cp2.92 20.81 5.12ul2.39 18.35 4.99nl1.75 17.09 5.00d2gpoadalabel 4.11 32.65 5.58.
6.784.097.877.083.697.828.096.448.177.075.738.36.313.977.858.116.017.969.366.447.998.715.727.797.453.407.818.93 10.99 8.87.
3.293.504.133.473.124.385.134.643.734.84.
1.611.772.241.821.582.503.002.631.972.90.
3.78 20.58 5.073.46 21.27 5.103.82 22.14 5.154.25 23.95 5.304.10 22.53 5.134.06 22.62 5.133.74 20.97 5.013.57 20.36 5.052.74 19.21 5.004.78 29.58 5.43.
7.978.128.258.378.118.147.947.977.978.78.lf.
1.230.781.271.511.321.331.001.060.361.53.
5.946.155.345.345.276.006.015.846.325.12.
2.843.162.542.542.512.942.992.863.152.32.
1.461.851.341.331.311.521.641.461.721.19.adalabel(greedy).
3.96 23.53 5.17.
8.00.
8.49 17.42 13.38 11.01 4.78 22.88 4.96.
7.66.
1.47.
9.80.
6.48.
4.75.human.
6.59 37.74 5.67.
8.91.
13.7 n/a n/a n/a 8.62 43.16 5.89.
9.36.
4.75 n/a n/a n/a.
table 7: automatic evaluation results (%) using the top-k sampling decoding scheme (k = 10).
the best resultsamong all these top-k-decoded models are in bold..figure 6: conﬁdence score distributions for high-frequency words on the dailydialog dataset.
words occupyingthe top 40% of the frequency mass in the training set of dailydialog are regarded as high-frequency words..figure 7: conﬁdence score distributions for high-frequency words on the opensubtitles dataset.
words occupyingthe top 40% of the frequency mass in the training set of opensubtitles are regarded as high-frequency words..dataset (see figure 8).
it can be seen that more“rare words” are predicted by our method on thedailydialog dataset.
this observation is in linewith the results on the opensubtitles dataset asreported in section 5.3..f use bert model to obtain v.this appendix provides more experiment resultscomparing to the cmlm model (chen et al., 2020):1).
cmlm exactly follows the setting of chen et al.
(2020), i.e., the teacher distribution produced by.
35190.00.51.0confidence score012density (%)(a) adalabel0.00.51.0confidence score0123density (%)(b) ce0.00.51.0confidence score0.00.51.01.5density (%)(c) ls0.00.51.0confidence score0123density (%)(d) face0.00.51.0confidence score012density (%)(e) f^20.00.51.0confidence score0123density (%)(f) nl0.00.51.0confidence score024density (%)(g) ul0.00.51.0confidence score024density (%)(h) cp0.00.51.0confidence score012density (%)(i) d2gpo0.00.51.0confidence score0.00.51.01.5density (%)(j) fl0.00.5confidence score0123density (%)(a) adalabel0.00.51.0confidence score012density (%)(b) ce0.00.51.0confidence score012density (%)(c) ls0.00.51.0confidence score012density (%)(d) face0.00.51.0confidence score012density (%)(e) f^20.00.51.0confidence score012density (%)(f) nl0.00.51.0confidence score012density (%)(g) ul0.00.51.0confidence score012density (%)(h) cp0.00.51.0confidence score0123density (%)(i) d2gpo0.00.51.0confidence score012density (%)(j) fldialogue.
context1 tell me a little bit about yourself , please ..ul.
d2gpo.
i am sorry , sir .
i have been in a foreigncurrency company for the company .
i ’ m afraid i ca n ’ t .
i ’ m a littlenervous about it .
yes , i am in the mood for a few days ago ..faceadalabel i am studying in the international trade.
company .
i am studying english in my company ..context1 oh , they look nice .
may i try that black one on ?
context2 of course .
the ﬁtting room is right over there ..uld2gpofaceadalabel i bought this one .
it ’ s the latest fashion ..i think you ’ re right .
i think you ’ ll have a try .
i think it ’ s a good idea .
how much is it ?.
context1 well , we don ’ t have any pre - existing.
illnesses or conditions , so we should be ﬁne ..ul.
and most people are n ’ t so quiet andinterviewing ..d2gpo and the company company are a good job in.
face.
the company .
and your company insurance , or workshopsare booked in a private company ?
adalabel i wish our company or country provided us.
with healthcare ..table 9: cases on dailydialog..dialogue.
context you ’ re going to love this baby ..lsd2gpofaceadalabel you ’ re going to be a good father ..i ’ m going to get you .
i ’ m going to go to the bathroom .
you ’ re going to love this baby ..context mr. matlack can ’ t offend ..lsd2gpoface.
this is our ofﬁcial business .
this is the most important thing .
it ’ s a pleasure to meet you , mr .
president ..adalabel timothy matlack is the ofﬁcial scribeof the continental congress ..context brave men discover they are really cowards ..i ’ m a samurai .
lsi ’ ve never seen such a beast in my life .
d2gpofacethey are not afraid of you .
adalabel father , you ’ re a coward !.
context.
i found this whole operation strange ..lsd2gpofaceadalabel it ’ s an incredibly delicate system ..i ’ m not going to let you go .
i ’ m not sure i ’ m going to do it .
i ’ m sorry , but i ’ m not sure i can handle it ..context.
if they make it , they ’ re clear into a safe zonewhere they can get medical supplies and food ..lswe ’ il get them to the safe .
d2gpo we ’ il have to get back to the hotel .
facethey ’ re gon na get us out of here .
adalabel so we can use it as a safe ﬁeld ..table 10: cases on opensubtitles..figure 8: ratios of low-frequency tokens in the gener-ated responses on the dailydialog dataset.
tokens ineach group are determined based on the frequency onthe training set..model.
bleu-3,4 dist-1,2.
ent-1,2.
lf.
6.18 4.09 2.20 11.83 4.59 6.79 4.621. cmlm2. cmlm+ε9.36 7.31 3.78 21.05 4.96 7.61 6.883. cmlm+ε+da 11.6 9.34 3.67 20.97 5.02 7.71 7.28.adalabel.
13.38 11.01 3.96 23.53 5.17 8.00 8.49.table 8: ablation study results based on bert on dai-lydialog (%)..the bert model is merged with the one-hot distri-bution using a ﬁxed ε.
2).
cmlm+ε adaptivelyadjust the value of ε using eq.
6 in our paper.
3).
cmlm+ε+da add an additional training task tooptimize the auxiliary decoder da on the basis ofcmlm+ε.
it is expected that optimizing da helpour dialogue encoder to capture better representa-tions.
the trained da is not used in the training andinference phase of our dialogue model.
note thatthe last model cmlm+ε+da is the same with ourablation model 6. bert as reported in our paper.
as can be seen table 8, our approach to adap-tively change ε helps to produce better dialogueresponses, and the training of da helps our dia-logue encoder to learn better representations..g case study.
we sampled some generated cases on the dailydi-alog and opensubtitles dataset.
the results of ourmodel and some competitive baselines are shownin table 9 and table 10. it can be seen that theresponses generated by our method are coherentto the context and contain richer contents.
more-over, our model also produces more rare words thatmake our response more diverse..3520[0, 20][21, 40][41, 60][61, 80][81, 100]token frequency0.00.51.01.52.0% of generated tokensadalabelulflnlcpfacecelsf2d2gpo