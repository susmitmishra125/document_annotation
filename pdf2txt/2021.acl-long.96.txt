can vectors read minds better than experts?
comparing data augmentation strategies for the automated scoring ofchildren’s mindreading ability.
venelin kovatchev1, phillip smith2, mark lee2, and rory devine11 school of psychology, university of birmingham2 school of computer science, university of birmingham52 prichatts road, edgbaston, b15 2sb, united kingdom{v.o.kovatchev , p.smith.7 , m.g.lee , r.t.devine} @bham.ac.uk.
abstract.
in this paper we implement and compare 7 dif-ferent data augmentation strategies for the taskof automatic scoring of children’s ability to un-derstand others’ thoughts, feelings, and desires(or “mindreading”)..we recruit in-domain experts to re-annotateaugmented samples and determine to what ex-tent each strategy preserves the original rating.
we also carry out multiple experiments to mea-sure how much each augmentation strategy im-proves the performance of automatic scoringsystems.
to determine the capabilities of auto-matic systems to generalize to unseen data, wecreate uk-mind-20 - a new corpus of chil-dren’s performance on tests of mindreading,consisting of 10,320 question-answer pairs..we obtain a new state-of-the-art performanceon the mind-ca corpus, improving macro-f1-score by 6 points.
results indicate thatboth the number of training examples and thequality of the augmentation strategies affectthe performance of the systems.
the task-speciﬁc augmentations generally outperformtask-agnostic augmentations.
automatic aug-mentations based on vectors (glove, fasttext)perform the worst..we ﬁnd that systems trained on mind-cageneralize well to uk-mind-20.
we demon-strate that data augmentation strategies alsoimprove the performance on unseen data..1.introduction.
many state-of-the-art nlp models are limited bythe availability of high quality human-annotatedtraining data.
the process of gathering and annotat-ing additional data is often expensive and time con-suming.
it is especially difﬁcult to gather data fortasks within psychology and psycholinguistics, astest administration typically requires highly trainedin-domain experts, controlled environments, andlarge numbers of human participants..data augmentation is a popular technique forartiﬁcially enlarging datasets.
typically, data aug-mentation uses one or more predeﬁned strategiesto modify existing gold-standard examples whileretaining the original label.
the objectives of dataaugmentation are: 1) to increase the size of thedataset; 2) to introduce more variety; 3) to reduceoverﬁtting; and 4) to improve generalizability..data augmentation has been used successfully incomputer vision (shorten and khoshgoftaar, 2019)and has recently become more popular in the ﬁeldof nlp (wei and zou, 2019; min et al., 2020; daiand adel, 2020; marivate and sefara, 2020)..we use data augmentation to improve the per-formance of systems for automatic scoring of chil-dren’s performance on tests of mindreading (i.e.,the ability to reason about others’ thoughts, feel-ings and desires) (hughes and devine, 2015).
au-tomatic scoring of mindreading was recently intro-duced by kovatchev et al.
(2020).
their corpus,mind-ca contains hand-scored data from morethan 1000 children aged 7 to 14. collecting dataon children’s mindreading performance is compli-cated, time-consuming, and expensive.
it requiresin-person testing sessions led by trained researchersand children’s open-ended responses must be ratedby trained annotators.
data augmentation couldbe very beneﬁcial to improve the performance andconsistency of the automated scoring systems..in this paper we aim to measure, in a systematicway, the quality and efﬁciency of the different aug-mentation strategies.
we evaluate and compare thedifferent strategies intrinsically and extrinsically.
for the intrinsic evaluation, we recruit in-domainexperts to re-annotate augmented examples and de-termine the extent to which each strategy preservesthe original label.
for the extrinsic evaluation, wemeasure the quantitative improvement (macro-f1,f1-per-question, standard deviation) of automaticsystems on the mind-ca corpus.
furthermore,.
proceedingsofthe59thannualmeetingoftheassociationforcomputationallinguisticsandthe11thinternationaljointconferenceonnaturallanguageprocessing,pages1196–1206august1–6,2021.©2021associationforcomputationallinguistics1196we create a new corpus, uk-mind-20, containing10,320 question-answer pairs in english and weuse it to evaluate the performance of automatedsystems on unseen data..fink et al., 2015; devine et al., 2016).
further-more, difﬁculties with mindreading are linked witha range of mental health problems and neurodevel-opmental conditions (cotter et al., 2018)..we ﬁnd that the intrinsic “quality” of the aug-mentation strategies varies signiﬁcantly, accordingto human raters.
however, the extrinsic evalua-tion demonstrates that all strategies improve theperformance of the automated systems.
we system-atically measure the importance of three factors indata augmentation: corpus size, sampling strategy,and augmentation strategy.
we ﬁnd corpus size tobe the most important factor.
however, the choiceof sampling and augmentation strategies also sig-niﬁcantly affects the performance of the automatedsystems.
we report a correlation between the “qual-ity” of the augmentation and the performance.
withthe best conﬁguration we obtain a new state-of-the-art on mind-ca, improving macro-f1 score by 6points and f1-per-question by 10.3 points..we demonstrate that the automated scoring sys-tems can generalize well between mind-ca anduk-mind-20.
these ﬁndings indicate that themethodology for administering and scoring min-dreading is consistent and the automatic solutionscan be adopted in practice..the rest of this article is organized as follows.
section 2 discusses the related work.
section 3presents the methodologies for data augmentation.
section 4 compares the quality of the augmenta-tion strategies.
section 5 describes the machinelearning experimental setup and evaluation criteria.
section 6 analyzes the effect of data augmentationon automated systems.
section 7 presents somefollow-up experiments and discusses the implica-tions of the ﬁndings.
section 8 concludes the articleand proposes directions for future work..2 related work.
mindreading (also known as “theory of mind”) isthe ability to understand others’ thoughts, feelings,and desires (hughes and devine, 2015).
for exam-ple, in the ﬁnal scene of romeo and juliet, romeoholds a mistaken belief that juliet is dead.
beingable to understand the state of the world (“julietis alive”) and the mistaken belief (“juliet is dead”)is important to understand the situation and themotivation of the characters..the task of automatic scoring of mindreadingwas ﬁrst proposed by kovatchev et al.
(2020).
theygathered the responses of 1066 children aged 7-14 on two standardized tests of mindreading: thestrange story task (happ´e, 1994) and the silentfilm task (devine and hughes, 2013).
after digi-talizing and manually scoring the responses, theycreated mind-ca, a corpus of 11,311 question-answer pairs.
they trained and evaluated severalautomated systems (i.e., svm, bilstm, trans-former) and obtained promising initial results..data augmentation is a technique for artiﬁciallyincreasing the size of the dataset.
it can also beseen as a type of regularization at the level of thedata.
data augmentation can be used to increasethe number of instances of speciﬁc answer types.
it can also introduce more variety, and can reducethe imbalance between classes.
data augmentationis used to improve the performance of automatedsystems, to reduce the risk of overﬁtting, and toenhance the ability of automated systems to gener-alize to unseen data.
it is widely used in computervision (shorten and khoshgoftaar, 2019)..the speciﬁcs of natural languages make it moredifﬁcult to incorporate data augmentation in nlp.
a subtle change to the text can often lead to asubstantial difference in meaning and a change ofthe label.
the last two years have seen an increasein the popularity of data augmentation in nlp.
weiand zou (2019) present a python library that usessimple augmentation methods for improving textclassiﬁcation.
marivate and sefara (2020) comparedifferent strategies for augmentation in the contextof short-text classiﬁcation.
dai and adel (2020)compare different data augmentation strategies forthe task of named entity recognition..several researchers propose more complex aug-mentation strategies for nlp.
hou et al.
(2018) pro-pose a sequence-to-sequence model for data aug-mentation.
kobayashi (2018) and gao et al.
(2019)use language models in what they call “contextualaugmentation”.
min et al.
(2020) use syntacticaugmentation to improve the performance and gen-eralizability on natural language inference..individual differences in children’s mindreadingare linked with both social and academic outcomesand children’s wellbeing (banerjee et al., 2011;.
in this paper, we take a different approach to-wards data augmentation.
we implement and com-pare seven different augmentation strategies.
two.
1197of the strategies were designed speciﬁcally for thetask of automatic scoring of mindreading, whilethe remaining ﬁve are task agnostic.
we put theemphasis on a systematic evaluation of the aug-mentation strategies and some key parameters ofthe augmentation process.
we recruit and trainin-domain experts to provide intrinsic human eval-uation of the data augmentation.
we also annotatea new corpus that can measure the performanceand improvement on unseen data..3 data augmentation.
we used 7 different strategies for automatic dataaugmentation.
“dictionary” and “phrase” strate-gies make use of task-speciﬁc resources, created byin-domain experts.
the other 5 strategies (“order”,“wordnet”, “ppdb”, “glove”, “fasttext”) make useof publicly available task-agnostic resources..for a source of the augmentation, we used themind-ca corpus (kovatchev et al., 2020).
it con-tains 11,311 question-answer pairs.
there are 11different questions, and an average of 1,028 re-sponses per question.
there are three possible la-bels reﬂecting the degree to which the responseshows context-appropriate mindreading: 0 (fail), 1(partial score), and 2 (pass).
the label distributionfor the full corpus is balanced, however the labeldistribution for the individual questions vary 1..we sought to use data augmentation to createa well-balanced dataset in terms of questions andlabels.
to achieve this, we created a policy forsampling examples that we used in the augmenta-tion.
we split the mind-ca corpus per questionand per label, resulting in 33 question-label sub cor-pora.
the average size of each sub-corpora is 343,and the smallest number of instances in a sub cor-pora is 160. we sampled 125 examples from eachquestion-label sub-corpus, 375 from each question,for a total 4,125 examples..our.
sampling strategy ensures.
that eachquestion-label combination is well represented inthe augmentation process.
in the original mind-ca corpus, nine question-label pairs had less than125 instances.
as a preliminary step in the dataaugmentation process, our in-domain experts re-wrote existing responses to improve the balanceof the corpus.
we used strategy similar to the oneused in hossain et al.
(2020).
we ran statisticaland machine learning experiments to ensure thatthe additional examples do not introduce biases..1for more details, please refer to kovatchev et al.
(2020).
for our experiments we initially chose a con-servative number of examples (each augmentationincreases the original corpus size by 36 %), to avoidoverﬁtting on the underrepresented question-labelpairs.
we used a different random state for eachaugmentation strategy and we ensured that eachsample is representative in terms of demographicdistribution (age and gender of the participants)..in a complementary set of experiments, we ap-plied data augmentation directly without the cus-tom sampling strategy.
we also experimented withgenerating larger number of augmented examples(up to 140% of the original corpus size) via over-sampling (see section 7)..in the following subsections, we discuss in more.
details the different augmentation strategies..3.1 dictionary augmentation.
the “dictionary” augmentation strategy is a task-speciﬁc synonym substitution.
we automaticallyextract the 20 most frequent words for each of the11 questions, a total of 220 words.
we then asktrained corpus annotators to propose a list of syn-onyms for each word.
the synonyms have the samemeaning in the context of the particular question.
the meaning of the contextual synonyms may notbe the same outside of the context.
for example,in silent film question #1, “men” can be replacedwith “burglars”.
we instruct the experts to cre-ate as many synonyms as possible for each word.
some words do not have appropriate contextualsynonyms.
the ﬁnal synonym dictionary contains626 synonyms for 148 words 2..the dictionary augmentation algorithm replacesup to two words in each response with their contex-tual synonyms.
the words and their synonyms areselected at random from the available options..3.2.introductory phrase augmentation.
the task-speciﬁc “phrase” augmentation strategyadds a short phrase at the beginning of the response.
the appended phrases should not modify the mean-ing (or score) of the response.
an example forsuch phrase is “i think (that)”.
our experts createphrases that contain mental state words, such as“think”, “know”, and “believe”, as this category ofwords is important when scoring children’s min-dreading ability.
our corpus annotators proposed a.
2the implementation of all augmentation strategies andall resources used (lists of synonyms and introductoryphrases) can be found online at https://github.com/venelink/augment-acl21/.
1198list of 15 such phrases.
we further modify the 15phrases with 3 optional conjunctions, resulting in60 different combinations.the “phrase” augmen-tation appends a random phrase at the beginningof each response, if the response does not alreadybegin with such a phrase..3.3 word replacement augmentation.
word replacement augmentation is a strategy thatautomatically replaces up to two randomly selectedwords with semantically similar words or phrases.
the “wordnet” and “ppdb” augmentations replacethe selected words with a synonym from wordnet(fellbaum, 1998) or ppdb (pavlick et al., 2015)respectively.
the “glove” and “fasttext” augmen-tations replace the selected words with the mostsimilar words (or phrases) using pre-trained glove(pennington et al., 2014) or fasttext (joulin et al.,2016) word embeddings..we implement the four “word replacement” aug-mentations using the nlp augmentation pythonlibrary (ma, 2019).
for this set of experiments wedecided not to use bert-based contextual wordembeddings for augmentation, since we are usinga distilbert classiﬁer..3.4 change of order augmentation.
the “order” augmentation strategy changes the po-sition of two words in the sentence.
previous workon data augmentation for nlp (wei and zou, 2019;ma, 2019) implement the “order” augmentationby changing the position of the two randomly se-lected words.
we enforce a more stringent rule forour algorithm.
speciﬁcally, we select one wordat random and change its position with one of itsneighbouring words.
this change is more conser-vative than picking two words at random.
it alsoreﬂects the naturally occurring responses from 7-to 14-year-old children in the database.
the reorderprocess is repeated up to two times..3.5 combining multiple augmentations.
we also experimented with applying multiple aug-mentation strategies together.
for example the “dic-tionary + phrase” augmentation ﬁrst replaces upto two words with contextual synonyms and thenadds a phrase at the beginning of the response.
thedata obtained by “combination” augmentations wasincluded in the the “all-lq” and “all-hq” corpora..4 measuring augmentation quality.
the quality of data augmentation models in nlpresearch is typically evaluated extrinsically, bymeasuring the performance of automated systemstrained on augmented data.
wei and zou (2019)propose an intrinsic evaluation inspired by the dataaugmentation research in computer vision.
theycompare the latent space representations of the orig-inal and the augmented sentences and assume thatthe proximity in latent space indicates that the orig-inal labels are conserved..we argue that a direct comparison of the repre-sentation of the texts is not sufﬁcient to determinethe quality of the augmentation and the extent towhich each strategy preserves the original labels.
in natural language, unlike in computer vision, aminor difference in the text and the correspondingrepresentation can cause a signiﬁcant difference inthe meaning of the complex expression and ulti-mately the label or score assigned to that answer..we propose a manual evaluation of the differ-ent strategies.
for each augmentation strategy, weselected 5 random examples from each question-label sub-corpus, adding up to 165 examples perstrategy (4% of the full sample).
two trained an-notators independently rate the augmented pairsfor the 7 different augmentation strategies (a totalof 1,155 question-answer pairs).
to ensure a fairevaluation, the annotators receive a single ﬁle withthe examples for all augmented strategies shufﬂedat random.
the inter-annotator agreement was 87%with a cohen’s kappa of .83..augmentation quality.
phraseorderdictionarywordnetfasttextppdbglove.
invalid13.5210101217.
9694.59483777368.table 1: expert comparison of augmentation strategies.
quality - % of pairs where the label does not change.
invalid - % of pairs where the augmented instance issemantically incoherent and cannot be scored..table 1 shows the results of the re-annotationfor each augmentation strategy.
we deﬁne “quality”as the % of examples where the re-annotated labelwas the same as the original label.
we also measurethe % of “invalid” examples, where both annotators.
1199agreed not to assign a label due to a semanticallyincoherent response.
an example for an incoherentresponse can be seen in (1)..(1) why did the men hide ?.
so telling does’nt get told his ..based on the analysis, we distinguish between“high quality” augmentation strategies (“phrase”,“order”, and “dictionary”) and “low quality” aug-mentations (“wordnet”, “fasttext”, “ppdb”, and“glove”).
the “high quality” augmentations pre-serve the label in over 94% of the instances andcontain less than 4% invalid responses.
the “lowquality” augmentations preserve the label in lessthan 83% of the instances and contain more than10% invalid responses.
according to our raters,glove is the worst of all augmentation strategieswith 68% quality and 17% invalid..the expert analysis indicates that, at least in ourdata, there is a substantial difference in the qualityof the different augmentation strategies.
the task-speciﬁc strategies perform much better than thetask-agnostic ones, with the exception of “changeof order” augmentation.
in the following sections,we perform a number of machine learning experi-ments to determine if the quality of the data affectsthe performance of the automated systems..5 evaluating automated systems.
in our experiments, we used the two best sys-tems reported by kovatchev et al.
(2020) - a bil-stm neural network and a distilbert transformer.
these systems obtained good results on the origi-nal mind-ca corpus and at the same time werelightweight enough to be implemented in a prac-tical end-to-end application for automatic scoring.
we used the same conﬁguration and hyperparam-eters as reported by kovatchev et al.
(2020).
wemodiﬁed the existing classes to incorporate andkeep track of data augmentation and to implementadditional evaluation on uk-mind-20.
all of ourcode and data are available online 3..5.1 automated systems.
training setup..we trained each of the automated systems on 13different training sets, shown in table 2. eachset includes the original corpus (mind-ca) anda number of augmented samples.
for example,the phrase dataset contained the 11,311 examples.
3https://github.com/venelink/.
augment-acl21/.
corpusoriguk-20phrasedictorderwordnetfasttextppdbglove.
size.
corpus contents.
11,311 the mind-ca corpus10,320 the uk-mind-20 corpus15,436 mind-ca + “phrase”15,436 mind-ca + “dictionary”15,436 mind-ca + “order”15,436 mind-ca + “wordnet”15,436 mind-ca + “fasttext”15,436 mind-ca + “ppdb”15,436 mind-ca + “glove”.
ab-lq.
27,811.all-lq.
44,311.ab-hq.
23,686.all-hq.
40,186.mind-ca + “wordnet”,“fasttext”, “ppdb”, and“glove”mind-ca + “wordnet”,“fasttext”, “ppdb”, and“glove” + all 4 synonymsubstitutionscombinedwith reordermind-ca + “phrase”,“dictionary” and “order”mind-ca + “phrase”,“dictionary”, and “order” +all four possible combina-tions of the three strategies.
table 2: all augmented training sets.
from mind-ca + 4,125 from the “phrase” aug-mentation, for a total of 15,436 examples..in addition to the 7 “basic” augmented train-ing sets (one for each augmentation strategy), wealso created 4 larger training sets, containing aug-mented samples from multiple different strategies.
the “all bassic hq” (ab-hq) dataset contains the11,311 examples from mind-ca + 4,125 from“phrase” + 4,125 from “dictionary” + 4,125 from“order” for a total of 23,686 examples.
similarly,the “all basic lq” (ab-lq) dataset contains 27,811examples from mind-ca + “wordnet”, “fasttext”,“ppdb”, and “glove”..the two largest datasets, the all-lq and the all-hq datasets contain the corresponding “all basic”datasets and additional examples obtained by con-secutively applying more than one augmentationstrategy to the same original data (the “combined”augmentations described in section 3.5).
we keptthe “low quality” and the “high quality” data sepa-rated, so we can measure the correlation betweenthe “quality” and the performance of the automatedsystems..12005.2 the uk-mind-20 corpus.
one of the objectives behind data augmentation isto improve the capabilities of automated systemsto generalize to unseen data.
however, ﬁndingunseen data for the same task is often non-trivial,so researchers typically use train-test split or 10-fold cross validation to evaluate the models..to provide a fair evaluation benchmark for gen-eralizability, we created a new corpus of children’smindreading ability, the uk-mind-20 corpus.
thedata for the corpus is part of our own researchon children’s mindreading in large-scale study in-volving 1020 8- to 13-year-old children (556 girls,453 boys, 11 not disclosed) from the united king-dom.
children completed three mindreading tasksduring whole-class testing sessions led by trainedresearch assistants: strange stories task (happ´e,1994), silent film task (devine and hughes, 2013),and triangles task (castelli et al., 2000)..each child answered 14 questions: ﬁve from thestrange story task, six from the silent film task,and three from the triangles task.
we do not usethe responses for the triangles task for the evalu-ation of data augmentation, since that task is notpart of the mind-ca corpus.
we obtained a totalof 10,320 question-answer pairs for the strangestories and the silent film portion of the corpus.
similar to mind-ca, uk-mind-20 also includesthe age and gender of the participants and responsesto a standardized verbal ability test (raven, 2008).
the children’s responses were scored by twotrained research assistants, the same assistants thatmeasured the augmentation quality in section 4.each response was scored by one annotator.
theinter-annotator agreement was measured on a held-out set of questions.
we report an inter-annotatoragreement of 94% and a fleiss kappa score of .91.when creating uk-mind-20, we used the sameprocedures for administering, scoring, and digi-talizing the children responses as the ones usedby kovatchev et al.
(2020).
the data for the uk-mind-20 corpus is gathered in a different time-frame (oct 2019 – feb 2020) and from differentlocations than mind-ca (2014 – 2019)..5.3 evaluation criteria.
the task deﬁned by kovatchev et al.
(2020) con-sists of scoring the children’s mindreading abilitiesbased on the open-text responses to 11 differentquestions from the strange stories task and thesilent film task using three categories (i.e., fail,.
partial, pass).
a single automated system has toscore all 11 questions.
in this paper we evaluatethe system performance in three ways:.
overall f1: the macro-f1 on the full test set,.
containing all 11 questions, shufﬂed at random..f1-per-q: we split the test set on 11 parts, onefor each question.
we obtain the macro-f1 scoreon each question and calculate the average..std-per-q: similar to f1-per-q, we obtain themacro-f1 for each question and then calculate thestandard deviation of the performance per question.
the overall f1 measures the performance of thesystem on the full task.
f1-per-q and std-per-qmeasure the consistency of the system across thedifferent questions.
a practical end-to-end systemneeds to obtain good results in both.
the additionaldata facilitates the statistical analysis of the systemperformance.
this evaluation methodology wasproposed by kovatchev et al.
(2019)..for each system we performed a 10-fold crossvalidation using each corpus from table 2. foreach fold, we evaluated on both the correspondingtest set and on the full uk-mind-20 corpus.
ourcode dynamically removes from the current train-ing set any augmented examples that are based onthe current test set to ensure a fair evaluation.
alltest sets contain only gold-standard human-labeledexamples and do not include any augmented data..6 results.
table 3 presents the results of the 13 differenttraining conﬁgurations with the distilbert trans-former, using both question and answer as input4.
the numbers are the average across 10-fold crossvalidation.
for reference, we also include the re-sults obtained by training the system on uk-mind-20 and testing on mind-ca..the distilbert architecture is the best perform-ing system from kovatchev et al.
(2020).
the base-line system, trained on the original data alreadyobtained very good results: .925 f1 and .877 f1-per-q on the mind-ca corpus and .889 f1 and.839 f1-per-q on the uk-mind-20 corpus.
wedemonstrate that systems trained on either of thetwo datasets can generalize well on the other one.
4we carried out 4 different sets of experiments:twoclassiﬁers (bilstm and distilbert) and two different in-put setups (i.e., only the answer or both question and an-swer).
due to space restrictions, we report only the resultsfor the best system, distilbert (question + answer).
theﬁndings apply to all sets of experiments.
the code andresults for all experiments are available online at https://github.com/venelink/augment-acl21/.
1201test-f1 test-f1-q test-std uk20-f1 uk20-f1-q uk20-std.
training setorig (baseline)uk-mind-20phrasedictionaryorderfasttextgloveppdbwordnetab-lqab-hqall-lqall-hq.
.925.893.946.947.947.942.942.946.947.967.972.978.985.
.877.844.930.936.933.924.925.929.932.957.963.973.980.
.059.058.031.028.025.030.028.030.033.021.022.015.011.
.889.890.893.892.891.890.891.893.894.895.897.895.898.
.839.839.854.853.852.851.849.851.853.855.858.957.858.
.063.063.024.024.022.023.021.022.023.021.020.021.023.table 3: performance of a distilbert classiﬁer using different augmented sets for training.
we report f1, f1-per-question and standard deviation (per question) on two corpora: test (mind-ca), and uk20 (uk-mind-20)..(f1 of .89 and f1-per-q of .84).
this indicates thatthe two corpora are compatible and that automatedsystems can generalize to unseen data..it is evident in the table that all of the augmen-tation strategies successfully improved the perfor-mance of the automated systems across all evalu-ation criteria.
for the mind-ca corpus: f1 im-proved between 1.7 points (fasttext) and 6 points(all-hq); f1-per-qiestion improved between 4.7points (fasttext) and 10.3 points (all-hq); std-per-question was reduced by between 1.6 points(wordnet) and 4.8 points (all-hq).
for the uk-mind-20 corpus: f1 improved between 0.1 point(fasttext) and 0.9 point (all-hq); f1-per-questionimproved between 1 point (glove) and 1.9 points(all-hq); std-per-question was reduced between3.9 points (dictionary) and 4.2 points (ab-hq)..based on these results, we can draw two conclu-sions.
first, data augmentation can successfully beused to improve the performance of the systems onthe mind-ca corpus.
second, data augmentationalso improves the performance of the automatedsystems on the unseen examples from uk-mind-20. while the improvement is not as substantial asseen on mind-ca, the improvement on all threecriteria on uk-mind-20 indicates that the systemsare not just overﬁtting to mind-ca..we use the autorank python library (herbold,2020) to carry out a statistical analysis on theresults and compare the performance gain fromeach of the augmentation strategies.
we use thedata from both algorithms and input formats, a to-tal of 480 machine learning models, 40 for each.
dataset.
based on the provided data, autorank de-termines that the most appropriate statistical test isthe friedman-nemeyni test (demˇsar, 2006).
thefriedman test reports that there is a statisticallysigniﬁcant difference between the median valuesof the populations.
that means that some trainingsets are consistently performing better (or worse)than others.
the post-hoc nemenyi test can be usedto determine and visualise which training sets arebetter and which are worse..figure 1: critical difference diagram (all).
average ranking of training sets (lower is better).
connected with a line =>not statistically signiﬁcant..figure 1 shows the critical difference diagramof the post-hoc nemenyi test for all training sets.
each set is plotted with its average ranking acrossall systems.
the difference between systems con-nected with a line is not statistically signiﬁcant.
the original corpus is the worst performing of alldatasets with an average rank of 9. the 7 “basic”training sets are grouped in the middle (rank 6.5to 8).
that is, they are all better than the originalcorpus, but worse than the combined training sets..1202there is a signiﬁcant difference between “all-hq”,“all-lq”, “ab-hq”, and “ab-lq”.
collectivelythey are also better than the original training setand the “basic” training sets..figure 2: critical difference diagram (basic).
average ranking of training sets (lower is better).
connected with a line =>not statistically signiﬁcant..figure 2 shows the critical difference diagramof the post-hoc nemenyi test applied only to the 7“basic” augmentations.
after removing the outliers(the original corpus and the collections of multipleaugmentation), we can observe a clear, statisticallysigniﬁcant distinction between “high quality” aug-mentations (“dictionary”, “phrase”, and “order”)and “low quality” augmentations (“glove”, “fast-text”, “wordnet”, and “ppdb”)..based on the statistical analysis, we can drawtwo additional conclusions.
third, we found thatthe most important factor affecting the system per-formance is the number of training examples.
weobtain the best results by combining the exam-ples from various different augmentation strategies.
fourth, we demonstrated that when the trainingsize is comparable, the high quality augmentationsimprove the performance more than the low qualityones.
the difference is signiﬁcant and is consistentboth in “basic” datasets and in “combined” datasets.
vector based augmentations (glove and fasttext)are performing worse than augmentations based ontask-speciﬁc or task-agnostic knowledge bases..7 discussion and further experiments.
the intrinsic and extrinsic evaluation presented insection 4 and section 6 answered the main researchquestions posed in this paper.
we demonstrated thatdata augmentation can improve the performance ofautomated systems including on novel, unseen data.
we found that the data augmentation strategies varyin preserving the original label and in how muchthey improve the machine learning systems trainedon them.
we also showed that automated scoring.
systems can generalize well from mind-ca cor-pus to uk-mind-20 and the other way around.
allthese ﬁndings are important for further research onmindreading.
at the same time, our data augmen-tation strategies and evaluation methodology canalso be extended to other tasks and domains, con-tributing to the research of data augmentation innlp in general..in this section we present additional experimentsand an analysis of the impact of several differentfactors in the process of data augmentation 5..corpus size our experiments indicated that themost important factor for improving the system per-formance is the corpus size.
in table 3 the systemsthat perform best are trained on the largest possibleamount of data (all-lq/all-hq).
to further explorethe impact of corpus size, we ran an additionalset of experiments.
we sampled 500 examples foreach question-label subcorpora instead of the origi-nal 125, increasing the corpus size four times.
foreach augmentation strategy this resulted in a corpusapproximately the same size as ab-lq..as expected, the performance of each systemincreased with corpus size.
the ranking of theindividual systems remained similar to the one re-ported with 125 base examples.
“high quality” aug-mentations still performed better than “low quality”ones.
the f1, f1-per-q, and std-per-q for the“basic low quality” strategies was approximatelythe same as the performance for ab-lq.
the f1,f1-per-q, and std-per-q for the “basic high qual-ity” strategies was approximately the same as theperformance for ab-hq..this new set of experiments conﬁrmed the im-portance of corpus size.
even strategies that humanexperts perceive as “low quality” are improving theperformance of the automated systems.
and whilethe ranking consistently favors the “high quality”augmentations, the absolute difference is relativelysmall.
this is in line with the ﬁndings on noisylearning which show that machine learning modelscan be very noise-tolerant (natarajan et al., 2013).
we performed one ﬁnal experiment by combiningthe all-lq and all-hq data together, but found no in-crease or decrease of performance compared withusing only the all-hq data..sampling strategy in our experiments, we de-signed a sampling strategy to ensure that each.
5due to space restrictions, we only discuss the overall.
tendencies.
the actual results are available online..1203question-response combination appears in the train-ing data with sufﬁcient frequency.
in a complemen-tary set of experiments, we evaluated the impor-tance of the sampling.
for each augmentation strat-egy, we created an augmented dataset with 1500examples for each question, using a standard sam-pling that keeps the original ratio of the responses.
the size of the dataset is the same as sampling 500examples for each of the 3 labels.
we found thatfor all strategies, the sampling improves test-f1-q between .6 and 1 point and reduces std-per-qby 1 point.
this ﬁnding validates our choice ofsampling strategy..augmentation strategy in section 6 we demon-strated that when all parameters (sampling, corpussize) are equal the “high-quality” strategies rankhigher than the “low-quality” ones.
while the ab-solute difference in f1 and std is relatively smallon our datasets, the consistency of the performanceof the “high-quality” strategies has to be taken intoconsideration.
furthermore, the quantitative perfor-mance is only one factor that has to be consideredwhen choosing a strategy for data augmentation.
reducing the noise in the training data can be adesirable characteristic when interpreting the per-formance of the neural network models, or whenworking with sensitive data, such as (e.g.)
in thehealth domain.
the task-speciﬁc augmentationsthat we proposed and used may require in-domainexperts, however the design is rather simple and theprocess is not time or labour intensive.
after thetask-speciﬁc resource (dictionary, list of phrases)is created, it can be reused for multiple examplesand scales very well with corpus size..8 conclusions and future work.
we presented a systematic comparison of multipledata augmentation strategies for the task of auto-matic scoring of children’s mindreading ability.
weargued that the nature of natural language requiresa more in-depth analysis of the quality and perfor-mance of the different data augmentation strategies.
we recruited in-domain experts and incorporatedthem in the process of evaluation..we demonstrated that, for some of the augmen-tation strategies (“glove”, “fasttext”, “ppdb”) thereis a substantial portion of the examples (over 20%)where the rating changes or cannot be assigned dueto semantically incoherent text.
these differencesin the datasets cannot be captured trivially via thevisualisation techniques that are typically used for.
intrinsic evaluation.
we also found that the differ-ence in augmentation quality corresponds to a dif-ference in the performance of automated systemstrained on the data.
to the best of our knowledge,this is the ﬁrst evaluation of data augmentation innlp that involves both expert evaluation and auto-matic metrics and the ﬁrst study that demonstratesthe connection between the two..we carried out further experiments measuringthe importance of factors such as corpus size andsampling strategy.
our ﬁndings on the quality andefﬁciency of data augmentation strategies and onthe use of task-speciﬁc resources are relevant forresearchers in the area of data augmentation, specif-ically in domains where the quality of the traininggold examples is important or where the amount ofdata is very limited..for the purpose of evaluation, we also createda new corpus: uk-mind-20.
it is the secondcorpus for automatic scoring of mind reading inchildren.
we demonstrated that systems trainedon mind-ca generalize well on uk-mind-20.
we also showed that data augmentation improvesthe performance on unseen data.
these ﬁndingsare promising both for the task of scoring chil-dren’s mindreading and for the use of data augmen-tation in nlp.
to the best of our knowledge, this isthe ﬁrst work where augmentation is evaluated onnovel, unseen data for the same task..this work opens several directions of futurework.
as a direct continuation of this research,we will incorporate the best performing automatedsystems and data augmentation techniques in thework of developmental psychologists.
this willfacilitate a large-scale studies on mindreading inchildren and adolescents.
we are also exploringthe possibility of using nlp to address other timeand labour intensive problems within psychology.
open-ended short text responses are widely-usedwithin psychological research and the good resultsobtained in this paper can be replicated in othersimilar tasks..acknowledgements.
we would like to thank imogen grumley traynorand irene luque aguilera for the annotation andthe creation of the lists of synonyms and phrases.
we also want to thank the anonymous reviewers fortheir feedback and suggestions.
this project wasfunded by a grant from wellcome to r. t. devine..1204ethical statement.
the study was approved by the university of birm-ingham stem research ethics committee andcomplies with the british psychological societycode of human research ethics (2014).
parentsand caregivers were provided with detailed infor-mation about the study at least one week in advanceof data collection and given the opportunity to optout of the study.
children were also permitted toopt out of the study on the day of data collectionwithout consequence.
data were anonymous atsource as children did not provide names or contactinformation to the research team..references.
robin banerjee, dawn watling, and marcella caputi.
2011. peer relations and the understanding of fauxpas: longitudinal evidence for bidirectional associa-tions.
child development, 82(6):1887–1905..f castelli, f happe, u frith, and c frith.
2000. move-ment and mind: a functional imaging study of per-ception and interpretation of complex intentionalmovement patterns.
neuroimage, 12(3):314 – 325..jack cotter, kiri granger, rosa backx, matthewhobbs, chung yen looi, and jennifer h. barnett.
2018.social cognitive dysfunction as a clini-cal marker: a systematic review of meta-analysesacross 30 clinical conditions.
neuroscience andbiobehavioral reviews, 84:92 – 99..xiang dai and heike adel.
2020. an analysis ofsimple data augmentation for named entity recogni-tion.
in proceedings of the 28th international con-ference on computational linguistics, pages 3861–3867, barcelona, spain (online).
international com-mittee on computational linguistics..janez demˇsar.
2006. statistical comparisons of classi-ﬁers over multiple data sets.
j. mach.
learn.
res.,7:1–30..rory t devine, naomi white, rosie ensor, and clairehughes.
2016. theory of mind in middle child-hood: longitudinal associations with executive func-tion and social competence.
developmental psychol-ogy, 52(5):758—771..r.t. devine and c. hughes.
2013. silent ﬁlms andstrange stories: theory of mind, gender, and socialexperiences in middle childhood.
child develop-ment, 82:989–1003..christiane fellbaum.
1998. wordnet: an electronic.
lexical database.
bradford books..elian fink, sander begeer, candida c. peterson, vir-ginia slaughter, and marc de rosnay.
2015. friend-lessness and theory of mind: a prospective longitu-dinal study.
british journal of developmental psy-chology, 33(1):1–17..fei gao, jinhua zhu, lijun wu, yingce xia, taoqin, xueqi cheng, wengang zhou, and tie-yan liu.
2019. soft contextual data augmentation for neuralmachine translation.
in proceedings of the 57th an-nual meeting of the association for computationallinguistics, pages 5539–5544, florence, italy.
asso-ciation for computational linguistics..fg happ´e.
1994. an advanced test of theory of mind:understanding of story characters’ thoughts and feel-ings by able autistic, mentally handicapped, and nor-mal children and adults.
journal of autism and de-velopmental disorders, 24(2):129—154..steffen herbold.
2020. autorank: a python pack-age for automated ranking of classiﬁers.
journal ofopen source software, 5(48):2173..md mosharaf hossain, venelin kovatchev, pranoydutta, tiffany kao, elizabeth wei, and eduardoblanco.
2020. an analysis of natural language in-ference benchmarks through the lens of negation.
inproceedings of the 2020 conference on empiricalmethods in natural language processing (emnlp),pages 9106–9118, online.
association for computa-tional linguistics..yutai hou, yijia liu, wanxiang che, and ting liu.
2018. sequence-to-sequence data augmentation fordialogue language understanding.
in proceedings ofthe 27th international conference on computationallinguistics, pages 1234–1245, santa fe, new mex-ico, usa.
association for computational linguis-tics..claire hughes and rory devine.
2015. individual dif-ferences in theory of mind from preschool to adoles-cence: achievements and directions.
child devel-opment perspectives, 9..armand joulin, edouard grave, piotr bojanowski, andtomas mikolov.
2016. bag of tricks for efﬁcient textclassiﬁcation.
arxiv preprint arxiv:1607.01759..sosuke kobayashi.
2018. contextual augmentation:data augmentation by words with paradigmatic re-in proceedings of the 2018 conference oflations.
the north american chapter of the association forcomputational linguistics: human language tech-nologies, volume 2 (short papers), pages 452–457,new orleans, louisiana.
association for computa-tional linguistics..venelin kovatchev, m. antonia marti, maria salamo,and javier beltran.
2019. a qualitative evaluationin pro-framework for paraphrase identiﬁcation.
ceedings of the international conference on recentadvances in natural language processing (ranlp2019), pages 568–577, varna, bulgaria.
incomaltd..1205venelin kovatchev, phillip smith, mark lee, imo-gen grumley traynor, irene luque aguilera, androry devine.
2020.
“what is on your mind?” au-tomated scoring of mindreading in childhood andearly adolescence.
in proceedings of the 28th inter-national conference on computational linguistics,pages 6217–6228, barcelona, spain (online).
inter-national committee on computational linguistics..edward ma..2019..nlp.
augmentation..https://github.com/makcedward/nlpaug..vukosi marivate and tshephisho sefara.
2020. improv-ing short text classiﬁcation through global augmenta-tion methods.
in machine learning and knowledgeextraction, pages 385–399, cham.
springer interna-tional publishing..junghyun min, r. thomas mccoy, dipanjan das,emily pitler, and tal linzen.
2020.syntacticdata augmentation increases robustness to inferenceheuristics.
in proceedings of the 58th annual meet-ing of the association for computational linguistics,pages 2339–2352, online.
association for computa-tional linguistics..nagarajan natarajan,.
inderjit s. dhillon, pradeepravikumar, and ambuj tewari.
2013. learning within proceedings of the 26th interna-noisy labels.
tional conference on neural information processingsystems - volume 1, nips’13, page 1196–1204, redhook, ny, usa.
curran associates inc..ellie pavlick, pushpendre rastogi, juri ganitkevich,benjamin van durme, and chris callison-burch.
2015. ppdb 2.0: better paraphrase ranking, ﬁne-grained entailment relations, word embeddings, andstyle classiﬁcation..jeffrey pennington, richard socher, and christopher dmanning.
2014. glove: global vectors for wordrepresentation.
in emnlp, volume 14, pages 1532–1543..john c. raven.
2008. raven’s standard progressive ma-.
trices and mill hill vocabulary scale..connor shorten and t. khoshgoftaar.
2019. a surveyon image data augmentation for deep learning.
jour-nal of big data, 6:1–48..jason wei and kai zou.
2019. eda: easy data aug-mentation techniques for boosting performance onin proceedings of thetext classiﬁcation tasks.
2019 conference on empirical methods in natu-ral language processing and the 9th internationaljoint conference on natural language processing(emnlp-ijcnlp), pages 6382–6388, hong kong,china.
association for computational linguistics..1206