deeprapper: neural rap generation with rhyme and rhythm modeling.
lanqing xue1, kaitao song2, duocai wu3, xu tan4∗,nevin l. zhang1, tao qin4, wei-qiang zhang5, tie-yan liu41the hong kong university of science and technology2nanjing university of science and technology.
3fudan university.
4microsoft research asia.
5tsinghua university.
{lxueaa,lzhang}@cse.ust.hk.
kt.song@njust.edu.cn.
dcwu18@fudan.edu.cn.
{xuta, taoqin, tyliu}@microsoft.com.
wqzhang@tsinghua.edu.cn.
abstract.
rap generation, which aims to produce lyricsand corresponding singing beats, needs tomodel both rhymes and rhythms.
previousworks for rap generation focused on rhyminglyrics but ignored rhythmic beats, which areimportant for rap performance.
in this paper,we develop deeprapper, a transformer-basedrap generation system that can model bothrhymes and rhythms.
since there is no avail-able rap dataset with rhythmic beats, we de-velop a data mining pipeline to collect a large-scale rap dataset, which includes a large num-ber of rap songs with aligned lyrics and rhyth-mic beats.
second, we design a transformer-based autoregressive language model whichcarefully models rhymes and rhythms.
specif-ically, we generate lyrics in the reverse or-der with rhyme representation and constraintfor rhyme enhancement and insert a beat sym-bol into lyrics for rhythm/beat modeling.
toour knowledge, deeprapper is the ﬁrst sys-tem to generate rap with both rhymes andrhythms.
both objective and subjective evalu-ations demonstrate that deeprapper generatescreative and high-quality raps with rhymes andrhythms..1.introduction.
rap is a musical form originating from americain 1970s, and has quickly developed as one ofthe mainstream music genres in the world (keyes,2004).
with the rapid development of artiﬁcialintelligence, automatic rap lyrics generation hasdrawn attention from academia (potash et al., 2015;malmi et al., 2016; liang et al., 2018; nikolovet al., 2020).
generally speaking, rap lyrics need tobe semantically meaningful and fashionable to con-vey interesting stories or express feelings.
differentfrom natural language or other artistic genres (e.g.,.
∗∗ corresponding author: xu tan, xuta@microsoft.com.
lyrics or poetry), rap has distinctive characteris-tics: 1) it usually contains complex rhyme patternsamong several consecutive sentences, which arethe key to form a good ﬂow; 2) it needs to alignwith the singing beat since rap lyrics are usuallyrapped according to some rhythmic accompani-ments.
therefore, how to generate rap lyrics withgood rhymes and rhythms is a troublesome prob-lem..previous works (potash et al., 2015; malmi et al.,2016; liang et al., 2018; nikolov et al., 2020) forrap generation mainly focused on lyric generationand some of them developed strategies for rhymemodeling.
potash et al.
(2015) directly added a“<endline>” token at the end of verse lines and ex-pected to learn rhyme patterns implicitly.
nikolovet al.
(2020) applied a two-step strategy, which ﬁrstgenerates rap lyrics and then adds rhyme tokens tothe end of generated lyrics.
however, these meth-ods cannot guarantee the rhyme patterns for everylyric line and only care the rhyme on the last token.
although many works have studied rhyming mod-eling in other artistic genres (e.g., poetry) (li et al.,2020; van de cruys, 2020; liu et al., 2020), theyare not suitable for rap generation due to the com-plex rhyme structure in rap.
for example, poetryneeds to rhyme with only the last word in each sen-tence, while rap rhymes with multiple consecutivetokens at the end of each sentence..no previous works have studied rhythm model-ing (i.e., beats in rap), to our knowledge.
one ofthe main reasons is the lack of rap datasets withbeat-lyric alignment.
consequently, the generationof lyrics without rhythmic beats cannot be regardedas a full rap generation..in this paper, we develop deeprapper, a trans-former (vaswani et al., 2017) based rap generationsystem which can model both rhymes and rhythms.
to build the system, since there is no available rapdatasets with aligned rhythmic beats, we design a.proceedingsofthe59thannualmeetingoftheassociationforcomputationallinguisticsandthe11thinternationaljointconferenceonnaturallanguageprocessing,pages69–81august1–6,2021.©2021associationforcomputationallinguistics69data mining pipeline and collect a large-scale rapdataset for rhythm modeling.
speciﬁcally, we ﬁrstcrawl many rap songs, each song with both raplyrics and audios, from the web.
for each crawledrap song, we perform a series of data preprocessingsteps to extract rhythmic beats as well as beat-lyricalignment.
to better model rhyme, we generate thewords in a rap sentence from right to left in an au-toregressive manner.
doing so we can easily iden-tify the last few words of a sentence (now becomethe ﬁrst words of the reverse sentence) to rhymewith.
additionally, we incorporate several rhyme-related representations into our language model tofurther improve the rhyming quality, and encouragethe n -gram rhyme in generated rap lyrics throughrhyme constraint during inference.
we use a spe-cial token [beat] to represent the rhythmic beatand insert it into lyrics right before the correspond-ing word.
in this way, we can model the beat in thelyric sequence both in training and generation..inspired by the success of pre-trained languagemodels (devlin et al., 2019; radford et al., 2018;yang et al., 2019; song et al., 2019; liu et al.,2019), we incorporate pre-training into our sys-tem.
to obtain large-scale data for pre-training, wealso use our data mining pipeline to collect anothertwo datasets: 1) non-rap songs with aligned beats,which can be larger than rap dataset since non-rapsongs are more general than rap songs; 2) purelyrics, which can be even larger than non-rap songs.
in the pre-training stage, we pre-train our deep-rapper model based on the above two datasets.
then we ﬁne-tune our pre-trained model on the rapsongs with aligned beats.
the ﬁne-tuned modelis used for ﬁnal rap generation.
both objectiveand subjective evaluations verify the advantages ofdeeprapper in generating rap lyrics with rhymesand rhythms..our main contributions can be summarized as.
follows:.
• to model rhythms in rap generation, we de-velop a data mining pipeline to create rapdatasets with aligned rhythmic beats..• to better model rhymes, we design an autore-gressive language model to generate rap lyricsfrom right to left with rhyme constraint.
asfar as we know, deeprapper is the ﬁrst toexplicitly model n -gram rhymes..• we elaborately insert the beat token insidelyrics to model the rhythmic beats.
to our.
knowledge, deeprapper is the ﬁrst systemthat models rhythms for rap generation..2 background.
since deeprapper generates rap lyrics with bothrhyme and rhythm modeling, in this section, webrieﬂy introduce the related background: lyric gen-eration, rhyme modeling and rhythm modeling..lyric generation broadly speaking, lyric gen-eration can cover rap lyric generation (potash et al.,2015; nikolov et al., 2020; liang et al., 2018), songlyric generation (watanabe et al., 2018; lu et al.,2019; chen and lerch, 2020; sheng et al., 2020),general poetry generation (zhang and lapata, 2014;lau et al., 2018; li et al., 2020; liu et al., 2020) andetc.
different from previous works that leveragelanguage model to generate lyrics similar to naturallanguage, in this paper, we introduce a novel lan-guage model for rap generation, with well-designedrhyme and rhythm modeling to ﬁt the characteris-tics of rap lyrics.
additionally, inspired by thesuccesses of pre-trained language models (devlinet al., 2019; yang et al., 2019; liu et al., 2019;radford et al., 2019; song et al., 2019) in nlpapplications, we also incorporate pre-training intoour model to further improve the quality of rapgeneration..rhyme modeling rhyme modeling plays an im-portant role in rap generation, which requires thelast few tokens in consecutive sentences to havethe same rhyme pattern.
existing rap genera-tion systems either directly add a special token“<endline>” at the end of rap lyric to encour-age the model to learn rhyme structure (potashet al., 2015), or introduce a two-step strategyfor rhyme modeling that ﬁrst generates rap lyricsand then adds rhyme tokens after the generatedlyrics (nikolov et al., 2020).
however, these worksonly focused on unigram rhyme while rap appre-ciates more for n-gram rhyme.
although a lot ofworks have explored rhyme modeling in other gen-res, most of them cannot be directly used for rapgeneration.
for example, poetry generation (lauet al., 2018; zhipeng et al., 2019; liao et al., 2019;li et al., 2020) usually used pre-deﬁned format tocontrol the rhyme pattern since poetry usually hasﬁxed number of words and only cares the rhymepattern for the last word.
however, rap lyrics havediverse rhyme structures across multiple consecu-tive sentences and most importantly multiple con-.
70figure 1: an overview of our data mining pipeline..secutive words.
therefore, we introduce n -gramrhyme modeling in deeprapper to handle the dis-tinctive rhyme patterns in rap.
besides, we alsotrain our language model in a reverse order (i.e.,right to left), similar to previous works (van decruys, 2020), to better model rhymes since theyalways occur at the end of sentence..data crawling to mine a large-scale rap dataset,we ﬁrst crawl a large amount of rap songs with bothlyrics and singing audios from the web.
to ensurethe lyric and audio can be aligned in the sentencelevel which is beneﬁcial for our later word-levelbeat alignment, we also crawl the start and end timeof each lyric sentence corresponding to the audio..rhythm modeling rhythm modeling is usuallyused in music generation (zhu et al., 2018; huangand yang, 2020; ren et al., 2020) which generatesthe duration of notes along with the note pitch toform rhythmic beats in melody and accompanimentgeneration.
different from music generation, rapcares more about rhythmic beats instead of notepitches (i.e.
melody).
in this way, the generatedrap lyrics need to align with the correspondingrhythmic beats in order to be rapped, otherwise itcannot be regarded as a complete rap.
however, tothe best of our knowledge, none of previous workshave studied the rhythm modeling in rap generation.
in this paper, we introduce a novel beat modelingstrategy in deeprapper for rhythm generation..3 rap dataset mining.
previous works (potash et al., 2015; liang et al.,2018; nikolov et al., 2020) for rap generation usu-ally used rap datasets with only lyrics, without con-sidering the rhythmic beat information.
to modelrhythm in rap generation, the rap dataset shouldcontain lyrics with aligned rhythmic beats.
how-ever, beat alignments are quite difﬁcult to obtain,since their annotations require musicians with pro-fessional knowledge to identify stressing syllablein rap songs.
to handle this problem, we design adata mining pipeline to automatically extract beat-lyric alignments.
in this section, we introduce thedetails of the data mining pipeline and our mineddataset based on this pipeline..3.1 data mining pipeline.
vocal and accompaniment separation foreach rap song, we utilize spleeter (hennequin et al.,2020) 1, a public music separation tool, to separatethe vocal (containing rap singing) and accompani-ment (containing rhythmic beats) from the crawledrap audio..vocal and lyric alignment we split the sepa-rated vocals into the sentence level according tothe crawled start and end time of each lyric sen-tence, and thus we can get the vocal-lyric align-ments in the sentence level.
we convert lyrics intophonemes via phonemizer 2 and utilize montrealforced aligner 3 to obtain vocal-lyric alignments inthe phoneme level.
based on these phoneme-levelvocal-lyric alignments, we obtain the correspond-ing timestamp of each word in the singing audio..beat detection to obtain the alignments be-tween lyrics and beats, we need to know the times-tamp of each beat.
therefore, we use a beat trackdetection tool, librosa (mcfee et al., 2020) 4, totrack the timestamp of each beat from the separatedaccompaniment that obtained from the second step..lyric and beat alignment after we obtain thetimestamp of each word and each beat, we can alignthem together according to their timestamps.
how-ever, since a rapper may not sing a word exactlyfollowing the beat, directly using the timestamp toexactly match the word and beat is inappropriate.
therefore, we propose an approximate method toalign them.
denote the word sequence of a lyric.
figure 1 overviews our data mining pipeline, whichconsists of 5 steps: data crawling, vocal and accom-paniment separation, vocal and lyric alignment,beat detection, and lyric and beat alignment..1https://github.com/deezer/spleeter2https://github.com/bootphon/phonemizer3https://github.com/montrealcorpustools/montreal-.
forced-aligner.
4https://github.com/librosa/librosa.
71data crawlingvocal and accompaniment separationvocal and lyric alignmentbeat detectionlyric and beat alignmentdatasetaudio lyricvocal, lyricaccompanimentbeat timestamplyric timestampsentence as w = {w1, w2, · · · , w|w|}, and its beatsequence as b = {b1, b2, · · · , b|b|}, where wi andbj represent i-th word and j-th beat.
we use twiand tbj to represent the timestamps of wi and bjrespectively.
for each beat bj, we ﬁrst ﬁlter out a(cid:12)word set ˜w = {w : (cid:12)(cid:12) ≤ r/2, w ∈ w},(cid:12)tbj − twwhere r represents the average duration of eachword in the song (i.e., the total duration divides thenumber of words).
next, word wi is aligned withbeat bj if it satisﬁes the following condition:.
wi = min.
w.|tbj − tw|, w ∈ ˜w..(1).
table 1: the statistics of three mined datasets.
thesecond and third column represent the number of songsand sentences for each dataset..dataset.
#songs.
#sentences.
16,246d-rap52,737d-songd-lyric 272,839.
832,6462,083,1439,659,503.
3.2 mined datasets.
using the above data mining pipeline, we obtaina rap lyric dataset with aligned beats (named asd-rap, where d represents “dataset”), which sat-isﬁes the requirements of building a rap genera-tion system with both rhyme and rhythm model-ing.
we split the d-rap dataset into the trainingand validation set with a ratio of 4:1. since rapis only one of music genres and the number ofrap songs is usually smaller compared with moregeneral songs, we also mine another two datasetsto pre-train our deeprapper model with the samemining pipeline: 1) non-rap songs with alignedbeats (named as d-song); 2) pure lyrics withoutaligned beats (named as d-lyric).
we summarizethe statistics of the three datasets in table 1 andshow a rap song with aligned beats from d-rap infigure 2..4 rap generation model.
in this section, we introduce the architecture of ourrap generation model, and the details of its rhymemodeling and rhythm modeling..4.1 model overview.
figure 3 illustrates the detailed architecture ofrap generation model.
we use trans-ourformer (vaswani et al., 2017) to build an autoregres-sive language model (radford et al., 2018, 2019).
figure 2: an example of a rap song with aligned beatsin our mined “d-rap” dataset.
‘*’ means a beat isaligned with the word right after ‘*’.
translation of thecontent is in supplemental materials..for rap generation, and introduce several new de-signs: 1) to better model rhymes, our model gen-erates a sentence from right to left, since rhymingwords are always at the end of the sentence; 2) asaforementioned, rhythms are critical for rap per-formance, so we insert a special token [beat] forexplicit beat modeling; 3) unlike original trans-former with only word embedding and positionalembedding, we add multiple additional embed-dings to better model rhymes and rhythms.
next,we introduce our rhyme modeling in subsection 4.2and rhythm modeling in subsection 4.3..4.2 rhyme modeling.
rhymes are the key to form a good rap ﬂow.
indeeprapper, we model rhymes with three compo-nents: 1) reverse-order language model; 2) rhymerepresentation; and 3) rhyme constraint..4.2.1 reverse-order language model.
rhyming words usually occur at the end of eachlyric sentence.
if using a standard autoregressivelanguage model and generating tokens from left toright, we need to identify whether the current gen-eration step is the end of a sentence, which decideswhether to generate rhyming words to be consis-tent with that in previous sentences.
therefore, tobetter model rhymes, we use a reverse-order lan-guage model to generate sentences from right toleft, as shown in figure 3. doing so we can easilyidentify the last few words of a sentence (now be-come the ﬁrst few words of the reverse sentence)to control their rhymes.
note that we only reverse.
7201_[00:12.04]*我长⼤*的地⽅*像⼀个简朴*的寨 02_[00:15.15]*简朴的*⼈ 吃*着最简朴*的菜 03_[00:18.01]*简朴的*话 包*含着简朴的爱 04_[00:20.82]*简朴的*道理传*给⼀代又⼀代 05_[00:23.98]*难以*忘记的画*⾯不需相机 06_[00:26.67]难*以再闻*到的是巷*⼦⾥的⾹⽓ 07_[00:29.63]*常常*想起 外*婆家的躺椅 08_[00:32.38]最*珍贵的椰*奶 往*往藏在床底 09_[00:35.24]*先填饱*肚⼦再想那*些背不下的书 10_[00:38.42]*外婆做*的⽕腿*肠⽐外⾯炸的酥 11_[00:41.36]*油烟的*⾹味弥漫*不那么⼤*的屋 12_[00:44.28]*外婆的故*事总会*让⼤⼈笑*着哭line_[start_time] lyricsfigure 3: the architecture of the rap generation model in our deeprapper.
the input sequence here is a samplefrom a chinese rap named《红色》(translated to english is red).
the sample contains two lyric sentences withaligned beats.
each sentence is reversed for rhyme modeling.
therefore, the original form of the sample is:我抬头仰望。天空的苍茫。(translation: i looked up.
the sky is vast.)
word with underline means that a beat isaligned with this word.
sentences are separated by special token ‘[sep]’.
token ‘[start]’ represents the start ofa song..build a vowel dictionary f(·) to identify the vowelof each word.
as shown in figure 3, we add an ad-ditional vowel embedding f and an intra-sentencerelative positional embedding r to enhance rhymerepresentation for each token.
besides, to betteridentify different sentences, we introduce a sen-tence embedding s to differentiate different sen-tences..4.2.3 rhyme constraintin addition to reverse-order language model andrhyme representation, we also introduce rhyme con-straint to improve the quality of rhyme generationin inference.
as shown in figure 4, sentences in raplyrics not only rhyme with the last token, but alsowith multiple consecutive tokens at the end.
we callthis phenomenon as n -gram rhymes, which meanthe current sentence and the previous sentence keepthe same rhyme for the last n consecutive tokens.
to our knowledge, no previous work has inves-tigated n -gram rhymes (n > 1), although it isimportant to improve rap quality.
our proposedrhyme constraint enables our model to adjust theprobability of next predicted token to further en-courage n -gram rhyme generation.
the constraintis introduced as follows..to generate the i-th word wi in the standardinference procedure, we usually choose the pre-dicted token with the maximum probability, i.e.,wi = arg max p(w|w<i; θ), where w<i denotesthe words before position i in the reverse sentenceand θ is the model.
when the words before posi-.
figure 4: comparison between sentences in left-to-right and right-to-left order.
the superscript of eachtoken represents its rhyme..words inside a sentence, and still generate differentsentences in the original order.
figure 4 comparesthe sentences in left-to-right order and right-to-leftorder, from which we can see that rhyming wordsof each sentence share the same relative positions(offset to the ﬁrst token) in the reverse order, andare easy to model and control..4.2.2 rhyme representation.
rhyming words have two important features: 1) itsvowel that used for rhyming and 2) its relative posi-tion in a sentence to decide the correspondence be-tween the rhyming words in consecutive sentences(e.g., in the reverse order setting, the ﬁrst/secondword of the current sentence should be rhymed withthe ﬁrst/second word in the previous sentence)..we use the vowel in the pinyin 5 of chinese char-acters to represent their rhymes.
to this end, we.
5pinyin is the standard phoneme for chinese..73output[beat]我抬头[beat]仰望空天[beat]的苍[beat]茫[sep][sep][beat]我抬头[beat]仰望空天[beat]的苍[beat]茫[sep][start][sep]inpute[start]e天e仰e[beat]e的e抬e[beat]e茫e[sep]e我e[beat]e苍e头e[beat]e空e望e[sep]token  embeddingss[start]s0s0s0s0s0s0s0s0s1s1s1s1s1s1s1s1sentence  embeddingsr[start]r0r[beat]r1r2r[beat]r3r4r[sep]r0r1r[beat]r2r3r[beat]r4r[sep]intra-sentence positional embeddingsp0p1p2p3p4p5p6p7p8p9p10p11p12p13p14p15p16positional embeddingsvowel embeddingsf[start]f[beat]fouf[beat]fangfof[sep]fangf[beat]fangfef[beat]fongfanf[sep]fangfai＋＋＋＋＋＋＋＋＋＋＋＋＋＋＋＋＋＋＋＋＋＋＋＋＋＋＋＋＋＋＋＋＋＋＋＋＋＋＋＋＋＋＋＋＋＋＋＋＋＋＋＋＋＋＋＋＋＋＋＋＋＋＋＋＋＋＋＋11rhyme representationslyrics: 我抬头仰望。天空的苍茫。(i looked up.
the sky is vast.
)012345678ieoengieiang是这座城市的⽓象angianouengingeiang让你感受⽣命的⼒量angieiengoei象⽓的市城座这是angieingengouaniang量⼒的命⽣受感你让relative posleft-to-rightright-to-lefttion i of the current and previous sentence havethe same rhyme pattern, we will use an adjustedprobability distribution ˜p(w|w<i; θ) to encouragethe i-th generated word to be rhymed accordingto the i-th word in the previous sentence, so as toform n -gram rhymes.
the adjusted probabilitydistribution ˜p(w|w<i; θ) is:.
˜p(w|w<i; θ) = α · p(w|w<i; θ) + (1 − α) · π(w)(2)where π(w) is a vowel check function and α is ahyper-parameter to balance the two terms.
here,π(w) is 1 if the predicted w has the same vowelwith the i-th token in the previous sentence, other-wise 0. in other words, when predicting i-th token(i ≤ n ), we encourage our model to pay moreattention for these words with same vowel with thei-th token in the previous sentence.
in this way, themodel tends to generate n -gram rhymes with largen ..4.3 rhythm modeling.
generating lyrics with aligned beats is necessarysince rap lyrics need to be rapped with rhythmicbeats.
therefore, we model and generate rhythmicbeats along with the lyrics with a speciﬁc symbol:we regard beat as a special token [beat] and insertit into lyric sequences for model training.
as shownin figure 3, we insert [beat] before its alignedwords like the following examples: “我[beat]抬头[beat]仰望。天空[beat]的苍[beat]茫。”.
rap usually contains different beat frequencies,i.e., the ratios between the total number of wordsand the total number of beats in a rap song.
toexplicitly model and generate rap with differentbeat frequencies, we use three tokens [s], [m], and[f] to represent the slow, medium and fast beatfrequencies and add the corresponding tokens at thestart of a rap song for training and inference.
in ourd-rap dataset, the distribution of beat frequency isdisplayed in figure 5. according to the distribution,we assign [s], [m], and [f] to songs with beatfrequency less than 3, equal to 3, and greater than3 respectively..5 experimental setup.
5.1 model, data, and training conﬁguration.
our deeprapper model is built on the autoregres-sive transformer decoder (vaswani et al., 2017;radford et al., 2018, 2019), where the hidden size,the number of attention heads and the number of.
figure 5: the distribution of beat frequencies in d-rap.
x-axis: beat frequency (the ratio between the to-tal number of words and the total number of beats in arap song).
y-axis: the number of songs..transformer layers are set as 768, 12, 12. thedimension of all different kinds of embedding indeeprapper is set as 768. considering there isno existing pre-trained language model in reverseorder, we do not utilize any pre-trained languagemodels for initialization.
instead, we ﬁrst pre-trainour model on d-lyric and d-song for 2 mil-lions steps, and then ﬁne-tune our model on d-rapwith 3k steps as the size of d-rap is smaller thanour pre-training corpus.
we convert each song to asequence with a length of 1024 tokens by cuttinglonger sequence or padding shorter sequence.
ourmodel is trained with a batch size of 8 songs on4 nvidia titan v gpus.
we use adam opti-mizer with a learning rate of 0.00015, β1 = 0.9,β2 = 0.999, and (cid:15) = 10−6.
we set the maxi-mum value of n -gram rhyme as 3 and the hyper-parameter α in equation 2 as 0.95. samples aregenerated conditioned on a given sentence in refer-ence..5.2 evaluation metrics.
in this subsection, we introduce the objective andsubjective metrics to evaluate the quality of thegenerated raps..objective evaluation we evaluate the gener-ated raps in terms of the quality of language, rhymeand rhythm.
we choose ﬁve metrics to evaluateour model: 1) perplexity (ppl), a standard met-ric to evaluate the quality of a language model; 2)rhyme accuracy (ra), the ratio of sentences thathave correctly predicted rhymes; 3) rhyme den-sity (rd), the longest rhyme of a song, averagedover all songs, which is introduced by malmi et al.
(2016) to measure the quality of rhyming ﬂuency;4) combo-n, the maximum number of consecu-tive sentences with the same n -gram rhyme in arap song, averaged over all songs, where we studyn = 1, 2, 3; 5) beat accuracy (ba), the accuracyof our model in beat prediction, under the teacher-forcing mode..740246810025005000table 2: results of objective and subjective evaluations.
“+pt” means using pre-training.
since the two baselinesdo not include beat information, we only compare in perplexity (ppl), rhyme accuracy (ra) and rhyme density(rd) for objective evaluation.
for subjective evaluation, we report the average annotation score of theme, ﬂuency,rhyme quality, and rhyme diversity..model.
objective evaluationppl ↓ ra ↑ rd ↑ theme ↑ fluency ↑ quality ↑ diversity ↑.
subjective evaluation.
baselinebaseline + ptdeeprapper.
24.6513.475.65.
32.2939.5966.95.
0.230.351.65.
3.133.413.67.
2.552.693.57.
3.103.254.16.
3.463.654.14.subjective evaluation similarto previousworks (zhang and lapata, 2014; nikolov et al.,2020) in artistic creation, we also use human evalu-ation to accurately evaluate the quality of the gen-erated raps.
we invite 10 participants with profes-sional knowledge in music as human annotatorsto evaluate 100 sampled raps.
each annotator isrequired to score from 1 (poor) to 5 (perfect) onthe following perspectives: 1) the clearness of thetheme of the rap lyrics; 2) the ﬂuency of the raplyrics; 3) the quality of the rhyme; 4) the diversityof the rhyme.
the averaged score of all annotatorson all sampled raps is used as the evaluation scorefor each perspective..6 experimental results.
results table 2 shows the objective and subjec-tive results of deeprapper compared with two base-lines: 1) baseline: a standard autoregressive lan-guage model with the same model conﬁgurationwith deeprapper but without our proposed rhymeand rhythm modeling; 2) baseline + pt, using pre-training on baseline.
we have several observationsfrom table 2: 1) deeprapper achieves better per-plexity, rhyme accuracy and rhyme density than thetwo baselines, which demonstrates the advantagesof our method in generating high-quality rap lyricswith accurate and diverse rhymes.
2) deeprap-per achieves better scores in all subjective metrics,demonstrating that deeprapper can generate high-quality and rhyming raps that accord with humantaste.
3) pre-training improves the performanceof baseline in both objective and subjective met-rics, which indicates the importance of pre-training.
however, its performance is still worse than deep-rapper..ablation studies to further validate the neces-sity of each component in deeprapper, we con-duct a series of ablation studies, including remov-.
table 3: the ablation studies on each component indeeprapper.
“-” means removing the correspond-ing component.
“rhyme”, “rhythm” and “pt” rep-resent rhyme modeling, rhythm modeling and pre-training.
“ro”, “ve”, “ipe” and “se” mean reverse-order, vowel embedding, intra-sentence position em-bedding and sentence embedding..model.
ppl ↓ ra ↑ ba ↑ rd ↑.
deeprapper.
5.65.
66.95.
79.00.
- rhyme- ro- ve- ipe- se- rhythm- pt.
7.133.706.065.816.694.6628.69.
41.2544.5866.1163.8666.5266.0840.77.
79.4282.8378.1977.2980.37–76.40.
1.65.
0.350.411.461.501.631.561.98.ing rhyme modeling, rhythm modeling and pre-training, respectively.
the results are reported intable 3. we have several observations: 1) remov-ing rhyme modeling affects rhyme quality a lotas it results in a dramatic drop in rhyme accuracyand rhyme density; 2) removing each speciﬁc de-sign in rhyme modeling (i.e., ro: reverse orderlanguage model, ve: vowel embedding, ipe: intra-sentence position embedding, se: sentence em-bedding) causes worse rhyme accuracy and rhymedensity.
speciﬁcally, while removing ro leads toa better ppl since left-to-right order can be moreeasily modeled than right-to-left order accordingto the analysis in wu et al.
(2018), it causes largeaccuracy drop in rhyme quality.
3) apparently,deeprapper without rhythm modeling cannot pro-duce any beat information; 4) deeprapper withoutpre-training affects the perplexity and rhyme accu-racy a lot, however, obtains a higher rhyme density.
the reason is that without pre-training, deeprap-per tends to copy previous rhyme tokens due to the.
75lack of generalization (larger ppl).
to verify this,we count the repetitive rate of rhyming words andfound that the rate of deeprapper is 23.8% whilewithout pre-training is 42.5%, which is higher thanusing pre-training.
the above results verify theeffectiveness of each component in deeprapper..n -gram rhyme to highlight the advantage ofdeeprapper in modeling n-gram rhyme, we usecombo-n to measure the ability of each designin deeprapper to model n-gram rhyme.
the re-sults are reported in table 4. we can ﬁnd that1) the model without rhyme modeling can hardlygenerate good rhyme, regardless of the value ofn in n-gram; 2) removing rhyme constraint alsoweakens the capacity of generating n-gram rhyme.
these results further demonstrate the importanceof our rhyme modeling and rhyme constraint ingenerating multiple consecutive rhymes..table 4: quality of n -gram rhyme in terms of combo-n .
“- rhyme” means removing rhyme modeling and“- rc” means removing rhyme constraint during infer-ence..model.
combo-1 combo-2 combo-3.
deeprapper.
- rhyme- rc.
87.10.
7.3770.08.
18.16.
2.8513.73.
9.10.
1.713.77.beat frequency to better measure the beat qual-ity, we randomly generate about 5,000 samples bydeeprapper and deeprapper with beat frequencycontrol.
we propose the first order distribution(fod) and the second order distribution (sod)and measure the distance (via wasserstein dis-tance (vallender, 1974)) of these distributions be-tween the generated samples and our drap dataset.
we deﬁne the interval of the current [beat] as thenumber of words between the current [beat] andthe next [beat].
therefore, the fod is deﬁnedas the distribution of the interval of the current[beat].
similarly, the sod is deﬁned the dis-tribution of the difference between the interval ofthe current [beat] and the next [beat].
theresults of the distance are normalized into [0, 1]and are reported in table 5. it can be seen thatdeeprapper with beat frequency control achievesbetter performance in beat modeling, which indi-cates the importance of beat frequency control inbeat modeling..table 5: measurement of beat generation.
“+ beat fre-quency” represents deeprapper with beat frequencycontrol..model.
fod.
sod.
deeprapper.
0.1107.
0.0514.
+ beat frequency.
0.1091.
0.0502.case analyses on generated raps we list asample case from our generated raps in figure 6to demonstrate the good quality of the raps gen-erated by deeprapper.
the sample is generatedby feeding the ﬁrst sentence of the example in fig-ure 2 to deeprapper.
as we can see, the gen-erated sample exhibits good theme, ﬂuency andrhyme.
the sample is a rap with a number of 1-gram, 2-gram, 3-gram, and even 4-gram rhyme.
the generated lyrics depicts the fond memories.
figure 6: a rap generated by deeprapper.
for eachexample, we provide the corresponding vowel of eachword.
vowels in red color represents that the wordrhymes with previous sentence.
bold word means abeat is aligned with the word.
the translation of theexample is attached in supplemental materials..76oangaeiangangieanueai我长⼤的地放像⼀个简朴的寨ongieiaeananganiieaoaoeai公⾥也许大的远⽅简直是个小⼩的寨ouerananaoiaangieneai偶尔穿件毛衣那样⼦很可爱anangeaneneuangaiianeneai远⽅可单纯的姑娘还是单纯的孩ianguaeuiaengeeai是放不下的故事大声的喝彩angaieeaoaioingeangeai像快乐的⼩孩莫名的敞着怀iaiongioenangueaoeiai⼏百公⾥我们相约到未来aiauineaoeai在那⽆尽的沙漠和海aneenanaai看着温暖花开aiangeai花一样的在ieongeneanai写动人的天籁eneiouiai跟着⾃由⾃在aoenaiaanai消沉在那片海uongerieaenuongeneiai不懂儿时的他们不懂什么是爱aoanaiianai到现在你看来eieneiai最真的迷彩of childhood and the beautiful visions for the fu-tures.
we also provide a group of samples gener-ated with beat frequency control.
to save space,we put them and the translation of all the sam-ples to appendix.
more samples are provided inhttps://deeprapper.github.io..our training datasets may have biases, which maybring some potential risks of model bias.
hence,we encourage future works to study how to applyother techniques in mitigating similar problems inour framework..7 conclusions.
references.
in this paper, we develop deeprapper, a noveltransformer-based rap generation system, whichleverages rhyme modeling, rhythm modeling andpre-training for rap generation.
considering thereis no available rap dataset with aligned rhythmicbeats for rhythm modeling, we propose a data min-ing pipeline to mine a rap dataset with beat-lyricalignments.
we leverage right-to-left generation,rhyme representation and rhyme constraint to bet-ter model rhyme and encourage n-gram rhyme,and explicitly model beat information by insertbeat token beside the corresponding word in thelyric sequence.
to our knowledge, deeprapper isthe ﬁrst system to generate rap with both rhymesand rhythms.
both objective and subjective eval-uations demonstrate that deeprapper generateshigh-quality raps with good rhymes and rhythms.
thanks to the design of deeprapper, we can fur-ther build another rap singing system to sing out theraps according to the rhymes and rhythms, whichwe leave as future work.
we also leave multilingualdeeprapper as future work..acknowledgements.
we would like to acknowledge the anonymous re-viewers for their insightful comments.
research onthis paper was supported by hong kong researchgrants council under grant 16204920..ethical considerations.
the proposed framework can be considered a novellanguage model for rap generation in automaticartistic creation.
speciﬁcally, the proposed frame-work has been conﬁgured with novel rhyme mod-eling as rhyme is quite important in music genres.
therefore, our proposed framework is also bene-ﬁcial for generating other music genres.
on theother hand, although we collect large-scale lyricdata for pre-training, it still cannot fully utilize thepotential of pre-training.
in the future, we expect toemploy more large-scale data in the open domainplus the music domain for pre-training to improvethe capacity of the language model.
in addition,.
yihao chen and alexander lerch.
2020. melody-in.
conditioned lyrics generation with seqgans.
arxiv..tim van de cruys.
2020. automatic poetry generationin proceedings of the 58th an-from prosaic text.
nual meeting of the association for computationallinguistics, pages 2471–2480..jacob devlin, ming-wei chang, kenton lee, andkristina toutanova.
2019. bert: pre-training ofdeep bidirectional transformers for language under-standing.
in naacl, pages 4171–4186..romain hennequin, anis khlif, felix voituret, andmanuel moussallam.
2020. spleeter: a fast andefﬁcient music source separation tool with pre-trained models.
journal of open source software,5(50):2154. deezer research..yu-siang huang and yi-hsuan yang.
2020. pop musictransformer: beat-based modeling and generation ofexpressive pop piano compositions.
in proceedingsof the 28th acm international conference on multi-media, page 1180–1188..cheryl lynette keyes.
2004. rap music and street con-sciousness, volume 501. university of illinois press..jey han lau, trevor cohn, timothy baldwin, julianbrooke, and adam hammond.
2018. deep-speare:a joint neural model of poetic language, meter andin proceedings of the 56th annual meet-rhyme.
ing of the association for computational linguistics(volume 1: long papers), pages 1948–1958..piji li, haisong zhang, xiaojiang liu, and shumingshi.
2020. rigid formats controlled text generation.
in acl, pages 742–751..hongru liang, qian li, haozheng wang, hang li, jin-mao wei, and zhenglu yang.
2018. attae-rl2: at-tention based autoencoder for rap lyrics representa-tion learning.
in companion proceedings of the theweb conference 2018, pages 7–8..yi liao, yasheng wang, qun liu, and xin jiang.
2019.gpt-based generation for classical chinese poetry.
arxiv preprint arxiv:1907.00151..yinhan liu, myle ott, naman goyal, jingfei du, man-dar joshi, danqi chen, omer levy, mike lewis,luke zettlemoyer, and veselin stoyanov.
2019.roberta: a robustly optimized bert pretraining ap-proach.
corr, abs/1907.11692..77yusen liu, dayiheng liu, and jiancheng lv.
2020.deep poetry: a chinese classical poetry generationin proceedings of the aaai conferencesystem.
on artiﬁcial intelligence, volume 34, pages 13626–13627..xu lu, jie wang, bojin zhuang, shaojun wang, andjing xiao.
2019. a syllable-structured, contextually-based conditionally generation of chinese lyrics.
inpricai, volume 11672, pages 257–265..eric malmi, pyry takala, hannu toivonen, tapaniraiko, and aristides gionis.
2016. dopelearning:a computational approach to rap lyrics generation.
in proceedings of the 22nd acm sigkdd inter-national conference on knowledge discovery anddata mining, pages 195–204..brian mcfee, vincent lostanlen, alexandros met-sai, matt mcvicar, stefan balke, carl thom´e,colin raffel, frank zalkow, ayoub malek, dana,kyungyun lee, oriol nieto, jack mason, dan el-lis, eric battenberg, scott seyfarth, ryuichi ya-mamoto, keunwoo choi, viktorandreevichmorozov,josh moore, rachel bittner, shunsuke hidaka,ziyao wei, nullmightybofo, dar´ıo here˜n´u, fabian-robert st¨oter, pius friesch, adam weiss, matt voll-rath, and taewoon kim.
2020. librosa/librosa: 0.8.0..nikola i. nikolov, eric malmi, curtis northcutt, andloreto parisi.
2020. rapformer: conditional raplyrics generation with denoising autoencoders.
inproceedings of the 13th international conference onnatural language generation, pages 360–373..peter potash, alexey romanov, and anna rumshisky.
2015. ghostwriter: using an lstm for automatic rapin proceedings of the 2015 con-lyric generation.
ference on empirical methods in natural languageprocessing, pages 1919–1924..alec radford, karthik narasimhan, tim salimans, andimproving language under-.
ilya sutskever.
2018.standing by generative pre-training..alec radford, jeff wu, rewon child, david luan,dario amodei, and ilya sutskever.
2019. languagemodels are unsupervised multitask learners..yi ren, jinzheng he, xu tan, tao qin, zhou zhao,and tie-yan liu.
2020. popmag: pop music ac-in proceedings of thecompaniment generation.
28th acm international conference on multimedia,pages 1198–1206..zhonghao sheng, kaitao song, xu tan, yi ren, weiye, shikun zhang, and tao qin.
2020. songmass:automatic song writing with pre-training and align-ment constraint.
arxiv preprint arxiv:2012.05168..ss vallender.
1974. calculation of the wasserstein dis-tance between probability distributions on the line.
theory of probability & its applications, 18(4):784–786..ashish vaswani, noam shazeer, niki parmar, jakobuszkoreit, llion jones, aidan n gomez, ł ukaszkaiser, and illia polosukhin.
2017. attention is allyou need.
in nips, pages 5998–6008..kento watanabe, yuichiroh matsubayashi, satorufukayama, masataka goto, kentaro inui, and to-moyasu nakano.
2018.a melody-conditionedlyrics language model.
in naacl, pages 163–172..lijun wu, xu tan, di he, fei tian, tao qin, jianhuanglai, and tie-yan liu.
2018. beyond error propaga-tion in neural machine translation: characteristicsof language also matter.
in proceedings of the 2018conference on empirical methods in natural lan-guage processing, pages 3602–3611..zhilin yang, zihang dai, yiming yang, jaime car-bonell, russ r salakhutdinov, and quoc v le.
2019.xlnet: generalized autoregressive pretraining forlanguage understanding.
in advances in neural in-formation processing systems, pages 5753–5763..xingxing zhang and mirella lapata.
2014. chinesepoetry generation with recurrent neural networks.
inproceedings of the 2014 conference on empiricalmethods in natural language processing (emnlp),pages 670–680..guo zhipeng, xiaoyuan yi, maosong sun, wenhao li,cheng yang, jiannan liang, huimin chen, yuhuizhang, and ruoyu li.
2019.jiuge: a human-machine collaborative chinese classical poetry gen-eration system.
in proceedings of the 57th annualmeeting of the association for computational lin-guistics: system demonstrations, pages 25–30..hongyuan zhu, qi liu, nicholas jing yuan, chuanqin, jiawei li, kun zhang, guang zhou, furu wei,yuanchun xu, and enhong chen.
2018. xiaoiceband: a melody and arrangement generation frame-in proceedings of the 24thwork for pop music.
acm sigkdd international conference on knowl-edge discovery and data mining, page 2837–2846..a comparison with ghostwriter.
we provide a comparison between deeprapperand ghosterwriter (potash et al., 2015) in table 6.the results show that both deeprapper and base-lines outperform ghosterwriter in terms of ppl,rhyme accuracy, and rhyme density on rap genera-tion tasks..kaitao song, xu tan, tao qin, jianfeng lu, and tie-yan liu.
2019. mass: masked sequence to se-quence pre-training for language generation.
in in-ternational conference on machine learning, pages5926–5936..b samples with beat frequency control.
fast figure 7 provides a rap generated by deep-rapper with fast beat frequency, which the fre-quency is 4.3. the rap express ones beat wished.
78table 6: the comparison of deeprapper with ghoster-writer in perplexity (ppl), rhyme accuracy (ra) andrhyme density (rd).
“+pt” means using pre-training..model.
objective evaluationppl ↓ ra ↑ rd ↑.
ghostwriterbaselinebaseline + ptdeeprapper.
58.9524.6513.475.65.
7.2832.2939.5966.95.
0.170.230.351.65.to his/her lover.
the following is the translation oftexts in figure 7..我长大的地方像一个简朴的寨the place where i grew up is like a simplevillage遥望远方镜子里的海looking into the distance, the sea is in themirror看见理想很实际的说一句拜拜see my dream and say goodbye这世界在我眼里和千里之外the world is sometimes in my eyes andsometimes thousands of miles away穿过河流沙漠和人海across rivers, deserts and crowds一山万水渡过岁月冲洗我的爱a mountain and a million rivers wash my lovethrough the years和那亲爱的你一起去踩一路的尘埃step on the dust all the way with dear you我一生中最重要的你我壹一都在the most important you in my life, i’ll alwaysby your side点点轻地落在你那冰封的大门外little by little, it falls outside your frozen gate我在默默的祝福你i am blessing you silently我在无数个夜里默默地祝福你i have secretly blessed you for countless nights远远地保护着你我的爱protecting you and my love from a distance.
medium figure 8 provides a rap generated bydeeprapper with medium beat frequency, whichthe frequency is 2.6. the rap praises the times welive in.
the following is the translation of texts infigure 8..figure 7: rap generated of fast beat frequency.
vowelsin red color represents that the word rhymes with previ-ous sentence.
bold word means a beat is aligned withthe word..我长大的地方像一个简朴的寨the place where i grew up is like a simplevillage简朴的看着简朴的海simply looking at the simple sea爸爸拿着一个简朴的麦dad holding a simple wheat有人真实的努力就有人背负着爱someone takes effort, somebody is carrying love那生活的美好让人人们热爱the beauty of life makes people love这世界的美好纯粹是意外the beauty of this world is pure accident而我长大的地方是个简朴的寨and the place where i grew up is a simplevillage让我们在这里开心的喝彩let’s cheer happily here伟大母亲怀抱着爱great mother embrace love看着幸福的人们敞开淳朴的怀watching happy people open their simple arms我们最美好的这个快乐海we are in the most beautiful happy sea唱出我们的时代sing our time.
79oangaeiangangieanueai我长⼤的地放像⼀个简朴的寨aoanganangingiieai遥望远⽅镜⼦⾥的海ananiangeniieoiiaiai看见理想很实际的说⼀句拜拜eiieaioanieaniiai这世界在我眼⾥和千⾥之外anoeiuaoeenai穿过河流沙漠和⼈海iananeiuoeiieongioeai⼀山万水渡过岁⽉冲洗我的爱eainaieiiiiaiiueenai和那亲爱的你一起去踩⼀路的尘埃oiengongeiongaoeioiiouai我⼀生中最重要的你我⼀直都在ananingioaiiaingengeaenai点点轻地落在你那冰封的⼤门外oaiooeuui我在默默的祝福你oaiuueieiooeuui我在⽆数个夜⾥默默的祝福你ananeaoueioeai远远地保护着你我的爱你爱的姑娘你也想看you want to see the girl you love她们和你一定要分离they must be separated from you你就这样子一笑而去you just leave with a smile.
figure 8: rap generated of medium beat frequency.
vowels in red color represents that the word rhymeswith previous sentence.
bold word means a beat isaligned with the word..slow figure 9 provides a rap generated by deep-rapper with slow beat frequency, where the fre-quency is 2.1. the rap express ones relief from life.
the following is the translation of texts in figure 9..我长大的地方像一个简朴的寨the place where i grew up is like a simplevillage快到有一天看见了父母的爱almost one day i saw the love of my parents我的时间你实在不用去考虑自己多坏you don’t have to think about how badyou are in my time当我脚步在外从没过的这么可爱i’ve never been so cute when i’m out我只是一次旅行i’m just a trip to your life你现在的校服我也想换i want to change your current school uniform我曾经追你i used to chase you你的运气也不摔your luck won’t fall毕竟上次after all last time.
figure 9: rap generated of slow beat frequency.
vow-els in red color represents that the word rhymes withprevious sentence.
bold word means a beat is alignedwith the word..c translation of chinese examples in.
the paper.
words in red are rhymes..translation of chinese in figure 2.
我长大的地方像一个简朴的寨the place where i grew up is like asimple village简朴的人吃着最简朴的菜simple people eat the simplest dishes简朴的话包含着简朴的爱simple words contain simple love简朴的道理传给一代又一代simple principles are passed on from generationto generation.
80oangaeiangangieanueai我长⼤的地放像⼀个简朴的寨anueaneanueai简朴的看着简朴的海aaaeieanueai爸爸拿着一个简朴的麦ouenenieuiououeneiueai有人真实的努⼒就有人背负着爱aengoeeiaoangeneneneai那生活的美好让⼈⼈们热爱eiieeeiaoeneiiiai这世界的美好纯粹是意外eroangaeiangiieanueai⽽我长⼤的地放是⼀个简朴的寨angoenaieiaiineeai让我们在这⾥开⼼的喝彩eiauinaiaoeai伟大母亲怀抱着爱aneinueenenangaienueai看着幸福的人们敞开淳朴的怀oeneieiaoeeeaieai我们最美好的这个快乐海anguoeneiai唱出我们的时代oangaeiangangieanueai我长⼤的地方像一个简朴的寨aiaoouianananeuueai快到有⼀天看见了⽗母的爱oeianiiaiuongiaoiiioai我的时间你实在不⽤去考虑⾃己多坏angoaouaiaiongeiouoeeeeai当我脚步在外从没有过的这么可爱oiiiiiing我只是⼀次旅⾏ianaieaouoieangan你现在的校服我也想换oengingeii我曾经追你ieiniieuai你的运⽓也不摔iingangi毕竟上次iaieuangieangan你爱的姑娘你也想看aeneiiingaoeni她们和你⼀定要分离iiueangiiaoeri你就这样子⼀笑而去难以忘记的画面不需相机unforgettable picture do not need camerato capture难以再闻到的是巷子里的香气what is hard to smell is the aroma in the alley常常想起外婆家的躺椅i often think of grandma’s recliner最珍贵的椰奶往往藏在床底the most precious coconut milk is often hiddenunder the bed先填饱肚子再想那些背不下的书fill your stomach ﬁrst, then think about the booksthat you can’t remember外婆做的火腿肠比外面炸的酥grandma’s ham sausage is crispier thanfried outside油烟的香味弥漫不那么大的屋the smell of lampblack pervades the not sobig house外婆的故事总会让大人笑着哭grandma’s stories always make adults laughand cry.
几百公里我们相约到未来through hundreds of kilometers,we meet in the future在那无尽的沙漠和海in the endless desert and sea看着温暖花开watching the warm ﬂowers bloom花一样的在like ﬂowers be there写动人的天籁write moving sounds of nature 跟着自由自在feeling the freedom消沉在那片海sometimes depressed in the sea不懂儿时的他们不懂什么是爱i don’t understand their childish.
i don’t know what love is到现在你看来till now you see最真的迷彩it is the most true fantasy.
translation of chinese in figure 3.
我抬头仰望。天空的苍茫。i looked up.
the sky is vast..translation of chinese in figure 4.
是这座城市的气象it is the weather of this city让你感受生命的力量makes you feel the power of living.
translation of chinese in figure 6.
我长大的地方像一个简朴的寨the place where i grew up is like asimple village公里也许大的远方简直是个小小的寨a small far away village偶尔穿件毛衣那样子很可爱it is cute to wear a sweater occasionally远方可单纯的姑娘还是单纯的孩is it a simple girl or a simple child far away是放不下的故事大声的喝彩cheers loudly for the unforgettable story像快乐的小孩莫名的敞着怀happy kids like happy kids.
81