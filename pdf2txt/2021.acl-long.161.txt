chinesebert: chinese pretraining enhanced byglyph and pinyin information.
zijun sun♣, xiaoya li♣, xiaofei sun♣, yuxian meng♣xiang ao♠, qing he♠, fei wu(cid:7) and jiwei li(cid:7)♣♣shannon.ai, (cid:7)zhejiang university♠key lab of intelligent information processing of chinese academy of sciences{zijun_sun, xiaoya_li, xiaofei_sun, yuxian_meng, jiwei_li}@shannonai.com{aoxiang,heqing}@ict.ac.cn, wufei@zju.edu.cn.
abstract.
recent pretraining models in chinese neglecttwo important aspects speciﬁc to the chineselanguage: glyph and pinyin, which carry sig-niﬁcant syntax and semantic information forlanguage understanding.
in this work, we pro-pose chinesebert, which incorporates boththe glyph and pinyin information of chinesecharacters into language model pretraining.
the glyph embedding is obtained based ondifferent fonts of a chinese character, beingable to capture character semantics from thevisual features, and the pinyin embedding char-acterizes the pronunciation of chinese charac-ters, which handles the highly prevalent het-eronym phenomenon in chinese (the samecharacter has different pronunciations with dif-ferent meanings).
pretrained on large-scale un-labeled chinese corpus, the proposed chine-sebert model yields signiﬁcant performanceboost over baseline models with fewer train-ing steps.
the proposed model achievesnew sota performances on a wide range ofchinese nlp tasks，including machine read-language infer-ing comprehension, naturalence, text classiﬁcation, sentence pair match-ing, and competitive performances in namedentity recognition and word segmentation.1.
1.introduction.
large-scale pretrained models have become a fun-damental backbone for various natural languageprocessing tasks such as natural language under-standing (liu et al., 2019b), text classiﬁcation(reimers and gurevych, 2019; chai et al., 2020)and question answering (clark and gardner, 2017;lewis et al., 2020).
apart from english nlp tasks,pretrained models have also demonstrated their ef-fectiveness for various chinese nlp tasks (sunet al., 2019, 2020; cui et al., 2019a, 2020)..1the code and pretrained models are publicly available athttps://github.com/shannonai/chinesebert..since pretraining models are originally designedfor english, two important aspects speciﬁc to thechinese language are missing in current large-scalepretraining: glyph-based information and pinyin-based information.
for the former, a key aspectthat makes chinese distinguishable from languagessuch as english, german, is that chinese is a lo-gographic language.
the logographic of charac-ters encodes semantic information.
for example,“液(liquid)”, “河(river)” and “湖(lake)” all have theradical “氵(water)”, which indicates that they areall related to water in semantics.
intuitively, therich semantics behind chinese character glyphsshould enhance the expressiveness of chinese nlpmodels.
this idea has motivated a variety of ofwork on learning and incorporating chinese glyphinformation into neural models (sun et al., 2014;shi et al., 2015; liu et al., 2017; dai and cai, 2017;su and lee, 2017; meng et al., 2019), but not yetlarge-scale pretraining..for the latter, pinyin, the romanized sequenceof a chinese character representing its pronuncia-tion(s), is crucial in modeling both semantic andsyntax information that can not be captured by con-textualized or glyph embeddings.
this aspect isespecially important considering the highly preva-lent heteronym phenomenon in chinese2, wherethe same character have multiple pronunciations,each of which is associated with a speciﬁc meaning.
each pronunciation is associated with a speciﬁcpinyin expression.
at the semantic level, for exam-ple, the chinese character “乐” has two distinctlydifferent pronunciations: “乐” can be pronouncedas “yuè [ye51]”, which means “music”, and “lè[lg51]”, which means “happy”.
on the syntax level,pronunciations help identify the part-of-speech ofa character.
for example, character “还” has two.
2among 7000 common characters in chinese, there areabout 700 characters that have multiple pronunciations, ac-cording to the contemporary chinese dictionary..proceedingsofthe59thannualmeetingoftheassociationforcomputationallinguisticsandthe11thinternationaljointconferenceonnaturallanguageprocessing,pages2065–2075august1–6,2021.©2021associationforcomputationallinguistics2065pronunciations: “huán[xwan35]” and “hái[xai35]”,with the former meaning the verb “return” and thelatter meaning the adverb “also”.
different pro-nunciations of the same character cannot be dis-tinguished by the glyph embedding since the lo-gographic is the same, or the char-id embedding,since they both point to the same character id, butcan be characterized by pinyin..in this work, we propose chinesebert, a modelthat incorporates the glyph and pinyin informationof chinese characters into the process of large-scale pretraining.
the glyph embedding is based ondifferent fonts of a chinese character, being able tocapture character semantics from the visual surfacecharacter forms.
the pinyin embedding modelsdifferent semantic meanings that share the samecharacter form and thus bypasses the limitation ofinterwound morphemes behind a single character.
for a chinese character, the glyph embedding, thepinyin embedding and the character embeddingare combined to form a fusion embedding, whichmodels the distinctive semantic property of thatcharacter..with less training data and fewer training epochs,chinesebert achieves signiﬁcant performanceboost over baselines across a wide range of chinesenlp tasks.
it achieves new sota performanceson a wide range of chinese nlp tasks，includingmachine reading comprehension, natural languageinference, text classiﬁcation, sentence pair match-ing, and results comparable to sota performancesin named entity recognition and word segmenta-tion..2 related work.
2.1 large-scale pretraining in nlp.
recent years has witnessed substantial work onlarge-scale pretraining in nlp.
bert (devlin et al.,2018), which is built on top of the transformerarchitecture (vaswani et al., 2017), is pretrainedon large-scale unlabeled text corpus in the man-ner of masked language model (mlm) and nextsentence prediction (nsp).
following this trend,considerable progress has been made by modifyingthe masking strategy (yang et al., 2019; joshi et al.,2020), pretraining tasks (liu et al., 2019a; clarket al., 2020) or model backbones (lan et al., 2020;lample et al., 2019; choromanski et al., 2020).
speciﬁcally, roberta (liu et al., 2019b) proposedto remove the nsp pretraining task since it has beenproved to offer no beneﬁts for improving down-.
stream performances.
the gpt series (radfordet al., 2019; brown et al., 2020) and other bertvariants (lewis et al., 2019; song et al., 2019; lam-ple and conneau, 2019; dong et al., 2019; baoet al., 2020; zhu et al., 2020) adapted the paradigmof large-scale unsupervised pretraining to text gen-eration tasks such as machine translation, text sum-marization and dialog generation, so that generativemodels can enjoy the beneﬁt of large-scale pretrain-ing..unlike the english language, chinese has itsparticular characteristics in terms of syntax, lexi-con and pronunciation.
hence, pretraining chinesemodels should ﬁt the chinese features correspond-ingly.
li et al.
(2019b) proposed to use chinesecharacter as the basic unit instead of word or sub-word that is used in english (wu et al., 2016; sen-nrich et al., 2016).
ernie (sun et al., 2019, 2020)applied three types of masking strategies – char-level masking, phrase-level masking and entity-level masking – to enhance the ability of captur-ing multi-granularity semantics.
cui et al.
(2019a,2020) pretrained models using the whole wordmasking strategy, where all characters within achinese word are masked altogether.
in this way,the model is learning to address a more challengingtask as opposed to predicting word components.
more recently, zhang et al.
(2020) developed thelargest chinese pretrained language model to date –cpm.
it is pretrained on 100gb chinese data andhas 2.6b parameters comparable to “gpt3 2.7b”(brown et al., 2020).
xu et al.
(2020) releasedthe ﬁrst large-scale chinese language understand-ing evaluation benchmark clue, facilitating re-searches in large-scale chinese pretraining..2.2 learning glyph information.
learning glyph information from surface chinesecharacter forms has gained attractions since theprevalence of deep neural networks.
inspired byword embeddings (mikolov et al., 2013b,a), sunet al.
(2014); shi et al.
(2015); li et al.
(2015); yinet al.
(2016) used indexed radical embeddings tocapture character semantics, improving model per-formances on a wide range of chinese nlp tasks.
another way of incorporating glyph information isto view characters in the form of image, by whichglyph information can be naturally learned throughimage modeling.
however, early work on learningvisual features is not smooth.
liu et al.
(2017);shao et al.
(2017); zhang and lecun (2017); dai.
2066figure 1: an overview of chinesebert.
the fu-sion layer consumes three d-dimensional embeddings– char embedding, glyph embedding and pinyin embed-ding.
the three embeddings are ﬁrst concatenated, andthen mapped to a d-dimensional embedding through afully connected layer to form the fusion embedding..and cai (2017) used cnns to extract glyph fea-tures from character images but did not achieveconsistent performance boost over all tasks.
su andlee (2017); tao et al.
(2019) obtained positive re-sults on the word analogy and word similarity tasksbut they did not further evaluate the learned glyphembeddings on more tasks.
meng et al.
(2019) ap-plied glyph embeddings to a broad array of chinesetasks.
they designed a speciﬁc cnn structure forcharacter feature extraction and used image classi-ﬁcation as an auxiliary objective to regularize theinﬂuence of a limited number of images.
song andsehanobish (2020); xuan et al.
(2020) extendedthe idea of meng et al.
(2019) to the task of namedentity recognition (ner), signiﬁcantly improvingperformances against vanilla bert models..3 model.
figure 2: an overview of inducing the glyph embed-ding.
(cid:78) denotes vector concatenation.
for each chi-nese character, we use three types of fonts – fangsong,xingkai and lishu, each of which is a 24 × 24 imagewith pixel value ranging 0 ∼ 255. images are concate-nated into a tensor of size 24 × 24 × 3. the tensor isﬂattened and passed to an fc to obtain the glyph em-bedding..figure 3: an overview of inducing the pinyin embed-ding.
for any chinese character, e.g.
猫(cat) in thiscase, a cnn with width 2 is applied to the sequence ofromanized pinyin letters, followed by max-pooling toderive the ﬁnal pinyin embedding..figure 4: an overview of the fusion layer.
(cid:78) denotesvector concatenation, and × is vector-matrix multipli-cation.
we concatenate the char embedding, the glyphembedding and the pinyin embedding, and use an fclayer with a learnable matrix wf to induce the fusionembedding..3.1 overview.
figure 1 shows an overview of the proposed chi-nesebert model.
for each chinese character, itschar embedding, glyph embedding and pinyin em-bedding are ﬁrst concatenated, and then mapped toa d-dimensional embedding through a fully con-nected layer to form the fusion embedding.
the fu-sion embedding is then added with the position em-bedding, which is fed as input to the bert modelsince we do not use the nsp pretraining task, weomit the segment embedding.
we use both wholeword masking (wwm) (cui et al., 2019a) andchar masking (cm) for pretraining (see section4.2 for details)..3.2.input.
the input to the model is the addition of the learn-able absolute positional embedding and the fusionembedding, where the fusion embedding is basedon the char embedding, the glyph embedding andthe pinyin embedding of the corresponding charac-ter.
the char embedding performs in a way anal-ogous to the token embedding used in bert butat the character granularity.
below we respectivelydescribe how to induce the glyph embedding, thepinyin embedding and the fusion embedding..2067fusion layerbert我很[m][m]猫我很[m][m]猫iverycatschar embedding03124position embeddingglyph embeddingpinyin embedding我很00猫wo3hen300mao1fusion embedding我很[m][m]猫fusion layer喜欢likeoutput猫猫mao1char embeddingglyph embeddingpinyin embedding猫fusion embedding猫māomao1----mao1----cnn猫1猫3猫2glyph layer猫glyph embeddingcomponent embeddingmao1flattenflattenflatten2828𝐖g𝐖fpinyin embeddingfusion layerbert我很[m][m]猫我很[m][m]猫iverycatschar embedding03124positionembeddingglyph embeddingpinyin embedding我很00猫wo3hen300mao1fusionembedding我很[m][m]猫fusion layer喜欢likeoutput猫猫mao1char embeddingglyph embeddingpinyin embedding猫fusion embedding猫māomao1----mao1----cnn猫1猫3猫2glyph layer猫glyph embeddingmao1flattenflattenflatten2424𝐖g𝐖ffusion layerbert我很[m][m]猫我很[m][m]猫iverycatschar embedding03124position embeddingglyph embeddingpinyin embedding我很00猫wo3hen300mao1fusion embedding我很[m][m]猫fusion layer喜欢likeoutput猫猫mao1char embeddingglyph embeddingpinyin embedding猫fusion embedding猫māomao1----mao1----cnn猫1猫3猫2glyph layer猫glyph embeddingcomponent embeddingmao1flattenflattenflatten2828𝐖g𝐖fpinyin embeddingfusion layerbert我很[m][m]猫我很[m][m]猫iverycatschar embedding03124position embeddingglyph embeddingpinyin embedding我很00猫wo3hen300mao1fusion embedding我很[m][m]猫fusion layer喜欢likeoutput猫猫mao1char embeddingglyph embeddingpinyin embedding猫fusion embedding猫māomao1----mao1----cnn猫1猫3猫2glyph layer猫glyph embeddingcomponent embeddingmao1flattenflattenflatten2828𝐖g𝐖fglyph embedding we follow meng et al.
(2019) to use three types of chinese fonts – fang-song, xingkai and lishu, each of which is instan-tiated as a 24 × 24 image with ﬂoating point pixelsranging from 0 to 255. the 24×24×3 vector is ﬁrstﬂattened to a 2,352 vector.
the ﬂattened vector isfed to an fc layer to obtain the output glyph vector..pinyin embedding the pinyin embedding foreach character is used to decouple different seman-tic meanings belonging to the same character form,as shown in figure 3. we use the opensourcedpypinyin package3 to generate pinyin sequencesfor its constituent characters.
pypinyin is a sys-tem that combines machine learning models withdictionary-based rules to infer the pinyin for char-acters given contexts.
pinyin for a chinese char-acter is a sequence of romanian characters, withone of four diacritics denoting tones.
we use spe-cial tokens to denote tones, which are appended tothe end of the romanian character sequence.
weapply a cnn model with width 2 on the pinyinsequence, followed by max-pooling to derive theresulting pinyin embedding.
this makes outputdimensionality immune to the length of the inputpinyin sequence.
the length of the input pinyin se-quence is ﬁxed at 8, with the remaining slots ﬁlledwith a special letter “-” when the actual length ofthe pinyin sequence does not reach 8..fusion embedding once we have the char em-bedding, the glyph embedding and the pinyin em-bedding for a character, we concatenate them toform a 3d-dimensional vector.
the fusion layersmaps the 3d-dimensional vector to d-dimensionalthrough a fully connected layer.
the fusion embed-ding is added with position embedding, and outputto the bert layer.
an illustration is shown infigure 4..3.3 output.
the output is the corresponding contextualized rep-resentation for each input chinese character (de-vlin et al., 2018)..4 pretraining setup.
4.1 data.
we collected our pretraining data from common-crawl4.
after pre-processing (such as removingthe data with too much english text and ﬁltering.
the html tagger), about 10% high-quality data ismaintained for pretraining, containing 4b chinesecharacters in total.
we use the ltp toolkit5 (cheet al., 2010) to identify the boundary of chinesewords for whole word masking..4.2 masking strategies.
we use two masking strategies – whole wordmasking (wwm) and char masking (cm) for chi-nesebert.
li et al.
(2019b) suggested that usingchinese characters as the basic input unit can al-leviate the out-of-vocabulary issue in the chineselanguage.
we thus adopt the method of maskingrandom characters in the given context, denoted bychar masking.
on the other hand, a large numberof words in chinese consist of multiple characters,for which the cm strategy may be too easy forthe model to predict.
for example, for the inputcontext “我喜欢逛紫禁[m] (i like going to theforbidden [m])”, the model can easily predict thatthe masked character is “城(city)”.
hence, we fol-low cui et al.
(2019a) to use wwm, a strategy tomask out all characters within a selected word, mit-igating the easy-predicting shortcoming of the cmstrategy.
note that for both wwm and cm, thebasic input unit is chinese characters.
the maindifference between wwm and cm lies in howthey mask characters and how the model predictsmasked characters..4.3 pretraining details.
different from cui et al.
(2019a) who pretrainedtheir model based on the ofﬁcial pretrained chinesebert model, we train the chinesebert modelfrom scratch.
to enforce the model to learn bothlong-term and short-term dependencies, we pro-pose to alternate pretraining between packed inputand single input, where the packed input is the con-catenation of multiple sentences with a maximumlength 512, and the single input is a single sen-tence.
we feed the packed input with probabilityof 0.9 and the single input with probability of 0.1.we apply whole word masking 90% of the timeand char masking 10% of the time.
the mask-ing probability for each word/char is 15%.
if thei-th word/char is chosen, we mask it 80% of thetime, replace it with a random word/char 10% ofthe time and maintain it 10% of the time.
we alsouse the dynamic masking strategy to avoid dupli-cate training instances (liu et al., 2019b).
we use.
3https://pypi.org/project/pypinyin/4https://commoncrawl.org/.
5http://ltp.ai/.
2068ernie.
bert-wwm.
macbert.
chinesebert.
heterogeneous wikipedia.
heterogeneous commoncrawl.
data sourcevocab sizeinput unitmaskingtasktraining stepsinit checkpoint# token.
18kchart/p/emlm/nsp-.
–.
21kcharwwmmlm2mbert0.4b.
21kcharwwm/nmac/sop1mbert5.4b.
21kcharwwm/cmmlm1mrandom5b.
table 1: comparison of data statistics between ernie(sun et al., 2019), bert-wwm (cui et al., 2019a),macbert (cui et al., 2020) and our proposed chine-sebert.
t: token, p: phrase, e: entity, wwm: wholeword masking, n: n-gram, cm: char masking, mlm:masked language model, nsp: next sentence predic-tion, mac: mlm-as-correlation.
sop: sentence or-der prediction..two model setups: base and large, respectivelyconsisting of 12/24 transformer layers, with inputdimensionality of 768/1,024 and 12/16 heads perlayer.
this makes our models comparable to otherbert-style models in terms of model size.
uponthe submission of the paper, we have trained thebase model 500k steps with a maximum learn-ing rate 1e-4, warmup of 20k steps and a batchsize of 3.2k, and the large model 280k stepswith a maximum learning rate 3e-4, warmup of90k steps and a batch size of 8k.
after pretrain-ing, the model can be directly used to be ﬁnetunedon downstream tasks in the same way as bert(devlin et al., 2018)..5 experiments.
we conduct extensive experiments on a variety ofchinese nlp tasks.
models are separately ﬁne-tuned on task-speciﬁc datasets for evaluation.
con-cretely, we use the following tasks:.
• machine reading comprehension (mrc).
• natural language inference (nli).
• text classiﬁcation (tc).
• sentence pair matching (spm).
• named entity recognition (ner).
• chinese word segmentation (cws)..we compare chinesebert to current state-of-the-art ernie (sun et al., 2019, 2020), bert-wwm (cui et al., 2019a) and macbert (cui et al.,2020) models.
ernie adopts various maskingstrategies including token-level, phrase-level and.
entity-level masking to pretrain bert on large-scale heterogeneous data.
bert-wwm/roberta-wwm continues pretraining on top of ofﬁcial pre-trained chinese bert/roberta models with thewhole word masking pretraining strategy.
unlessotherwise speciﬁed, we use bert/roberta torepresent bert-wwm/roberta-wwm and omit“wwm”.
macbert improves upon roberta byusing the mlm-as-correlation (mac) pretrainingstrategy as well as the sentence-order prediction(sop) task.
it is worth noting that bert and bert-wwm do not have the large version available online,and we thus omit the corresponding performances.
a comparison of these models is shown in ta-ble 1. it is worth noting that training steps of theproposed model signiﬁcantly smaller than base-line models.
different from bert-wwm andmacbert which are initialized with pretrainedbert, the proposed model is initialized fromscratch.
due to the additional consideration ofglyph and pinyin, the proposed cannot be directlyinitialized using a vanilla bert model, as themodel structures are different.
even initializedfrom scratch, the proposed model is trained fewersteps than the steps in retraining bert-wwm andmacbert after bert initialization..5.1 machine reading comprehension.
machine reading comprehension tests the model’sability of answering the questions based on thegiven contexts.
we use two datasets for this task:cmrc 2018 (cui et al., 2019b) and cjrc (duanet al., 2019) .
cmrc is a span-extraction styledataset while cjrc additionally has yes/no ques-tions and no-answer questions.
cmrc 2018 andcjrc respectively contain 10k/3.2k/4.9k and39k/6k/6k data instances for training/dev/test.
test results for cmrc 2018 are evaluated from theclue leaderboard.6 note that the cjrc dataset isdifferent from the one used in cui et al.
(2019a) ascui et al.
(2019a) did not release their train/dev/testsplit.
we thus run the released models on the cjrcdataset used in this work for comparison..results are shown in table 2 and table 3. aswe can see, chinesebert yields signiﬁcant perfor-mance boost on both datasets, and the improvementof em is larger than that of f1 on the cjrc dataset,which indicates that chinesebert is better at de-tecting exact answer spans..6https://github.com/cluebenchmark/clue.
2069model.
erniebertbert◦roberta◦macbertchinesebert.
roberta◦macbertchinesebert.
cmrc.
dev.
test.
base.
66.8966.7766.9667.89–67.95.
70.59–70.70.
74.7071.6073.9575.20–75.35.
77.95–78.05.large.
model.
erniebertbert◦roberta◦macbertchinesebert.
roberta◦macbertchinesebert.
xnli.
dev.
test.
base.
79.779.079.480.080.380.5.
82.182.482.7.
78.678.278.778.879.379.6.
81.281.381.6.large.
table 2: performances of different models on cmrc.
em is reported for comparison.
◦ represents modelspretrained on extended data..table 4: performances of different models on xnli.
accuracy is reported for comparison.
◦ represents mod-els pretrained on extended data..cjrc.
base.
model.
devem f1.
testem f1.
bertbert◦roberta◦chinesebert.
roberta◦chinesebert.
59.860.862.965.2.
65.666.5.
73.074.076.677.8.
77.577.9.
60.261.463.866.2.
66.467.0.
73.073.976.677.9.
77.678.3.large.
table 3: performances of different models on the mrcdataset cjrc.
we report results for baseline modelsbased on their released models.
◦ represents modelspretrained on extended data..5.2 natural language inference (nli).
the goal of nli is to determine the entailment re-lationship between a hypothesis and a premise.
weuse the cross-lingual natural language inference(xnli) dataset (conneau et al., 2018) for evalu-ation.
the corpus is a crowd-sourced collectionof 5k test and 2.5k dev pairs for the multinlicorpus.
each sentence pair is annotated with the“entailment”, “neutral” or “contradiction” label.
weuse the ofﬁcial machine translated chinese data fortraining.7.
results are present in table 4, which shows thatchinesebert is able to achieve the best perfor-mances for both base and large setups..5.3 text classiﬁcation (tc).
in text classiﬁcation the model is required to cat-egorize a piece of text into one of the speciﬁedclasses.
we follow cui et al.
(2019a) to use thuc-.
7https://github.com/facebookresearch/.
xnli.
news (li and sun, 2007) and chnsenticorp8 forthis task.
thucnews is a subset of thuctc9, with 50k/5k/10k data points respectively fortraining/dev/test.
data is evenly distributed in 10domains including sports, ﬁnance, etc.10 chnsen-ticorp is a binary sentiment classiﬁcation datasetcontaining 9.6k/1.2k/1.2k data points respectivelyfor training/dev/test.
the two datasets are rela-tively simple with vanilla bert achieving an ac-curacy of above 95%.
hence, apart from thuc-news and chnsenticorp, we also use tnews, amore difﬁcult dataset that is included in the cluebenchmark (xu et al., 2020).11 tnews is a 15-class short news text classiﬁcation dataset with53k/10k/10k data points for training/dev/test..results are shown in table 5. on chunsen-ticorp and thucnews, the improvement fromchinesebert is marginal as baselines have al-ready achieved quite high results on these twodatasets.
on the tnews dataset, chinesebertoutperforms all other models.
we can see that theernie model only performs slightly worse thanchinesebert.
this is because ernie is trained onadditional web data, which is beneﬁcial to modelweb news text that covers a wide range of domains..5.4 sentence pair matching (spm).
for spm, the model is asked to determine whethera given sentence pair expresses the same seman-tics.
we use the lcqmc (liu et al., 2018) and bqcorpus (chen et al., 2018) datasets for evaluation..8https://github.com/pengming617/bert_.
classification/tree/master/data9http://thuctc.thunlp.org/10https://github.com/gaussic/.
text-classification-cnn-rnn.
11https://github.com/cluebenchmark/clue.
2070model.
chnsenticorptestdev.
thucnewstestdev.
tnews.
dev.
test.
model.
ontonotes 4.0r.f.p.weibor.p.f.erniebertbert◦roberta◦macbertchinesebert.
roberta◦macbertchinesebert.
95.495.195.495.095.295.6.
95.895.795.8.
95.595.495.395.695.695.7.
95.895.995.9.base.
97.698.097.798.398.298.1.
98.398.198.3.
97.597.897.797.897.797.9.
97.897.997.9.
58.2456.0956.7757.51–58.64.
58.32–59.06.
58.3356.5856.8656.94–58.95.
58.61–59.47.large.
table 5: performances of different models on tcdatasets chnsenticorp, thucnews and tnews.
theresults of tnews are taken from the clue paper (xuet al., 2020).
accuracy is reported for comparison.
◦represents models pretrained on extended data..model.
lcqmc.
dev.
test.
bq corpustestdev.
base.
erniebertbert◦roberta◦macbertchinesebert.
roberta◦macbertchinesebert.
89.889.489.689.089.589.8.
90.490.690.5.
87.287.087.186.487.087.4.
87.087.687.8.
86.386.186.486.086.086.4.
86.386.286.5.
85.085.285.385.085.285.2.
85.885.686.0.large.
table 6: performances of different models on spmdatasets lcqmc and bq corpus.
we report accuracyfor comparison.
◦ represents models pretrained on ex-tended data..lcqmc is a large-scale chinese question match-ing corpus for judging whether two given ques-tions have the same intent, with 23.9k/8.8k/12.5ksentence pairs for training/dev/test.
bq corpusis another large-scale chinese dataset containing100k/10k/10k sentence pairs for training/dev/test.
results are shown in table 6. we can see that chi-nesebert generally outperforms macbert onlcqmc but slightly underperforms bert-wwm.
we hypothesis this is because the domain of bqcorpus more ﬁts the pretraining data of bert-wwm than that of chinesebert..5.5 named entity recognition (ner).
for ner tasks (chiu and nichols, 2016; lampleet al., 2016; li et al., 2019a), the model is askedto identify named entities within a piece of text,which is formalized as a sequence labeling task.
we use ontonotes 4.0 (weischedel et al., 2011)and weibo (peng and dredze, 2015) for this task..bertroberta◦chinesebert.
79.6980.4380.03.
82.0980.3083.33.roberta◦chinesebert.
80.7280.77.
82.0783.65.base80.8780.3781.65large81.3982.18.
67.1268.4968.27.
66.8867.8169.78.
67.3368.1569.02.
66.7468.75.
70.0272.97.
68.3570.80.table 7: performances of different models on nerdatasets ontonotes 4.0 and weibo.
results of preci-sion (p), recall (r) and f1 (f) on test sets are reportedfor comparison..model.
msra.
pku.
f1.
acc.
f1.
acc.
bert◦roberta◦chinesebert.
98.4298.4698.60.
99.0499.1099.14.
96.8296.8897.02.
97.7097.7297.81.roberta◦chinesebert.
98.4998.67.
99.1399.26.
96.9597.16.
97.8098.01.base.
large.
table 8: performances of different models on cwsdatasets msra and pku.
we report f1 and accuracy(acc) for comparison.
◦ represents models pretrainedon extended data..we use ontonotes 4.0 and weibo ner for this task.
ontonotes has 18 named entity types and weibohas 4 named entity types.
ontonotes and weibo re-spectively contain 15k/4k/4k and 1,350/270/270instances for training/dev/test.
results are shownin table 7. as we can see, chinesebert signiﬁ-cantly outperforms bert and roberta in termsof f1.
in spite of a slight loss on precision for thebase version, the gains on recall are particularlyhigh, leading to a ﬁnal performance boost on f1..5.6 chinese word segmentation.
the task divides text into words and is formalizedas a character-based sequence labelling task.
weuse the pku and msra datasets for chinese wordsegmentation.
pku consists of 19k/2k sentencesfor training and test, and msra consists of 87k/4ksentences for training and test.
output characterembedding is fed to the softmax function for ﬁnalpredictions.
results are shown in table 8, wherewe can see that chinesebert is able to outperformbert-wwm and roberta-wwm on both datasetsfor both metrics..6 ablation studies.
in this section, we conduct ablation studies tounderstand the behaviors of chinesebert.
we.
2071modelroberta◦chinesebert– glyph– pinyin– glyph – pinyin.
precision recall.
ontonotes 4.0f1.
80.4380.0377.6777.5478.22.
80.3083.3382.7583.6581.37.
80.3781.6580.13 (-1.52)80.48 (-1.17)79.76 (-1.89).
table 9: performances for different models withoutconsidering glyph or pinyin information..results.
as can be seen, chinesebert performsbetter across all setups.
with less than 30% of thetraining data, the improvement of chinesebertis slight, but with over 30% training data, the per-formance improvement is greater.
this is becausechinesebert still requires sufﬁcient training datato fully train the glyph and pinyin embeddings, andinsufﬁcient training data would lead to inadequatetraining..use the chinese named entity recognition datasetontonotes 4.0 for analysis and all models are basedon the base version..6.1 the effect of glyph embeddings and.
pinyin embeddings.
we would like to explore the effects of glyph em-beddings and pinyin embeddings.
for fair compar-ison, we pretrained different models on the samedataset, with the same number of training steps, andwith the same model size.
setups include “-glyph”,where glyph embeddings are not considered andwe only consider pinyin and char-id embeddings;“-pinyin”, where pinyin embeddings are not con-sidered and we only consider glyph and char-idembeddings; “-glyph-pinyin”, where only char-idembeddings are considered, and the model degen-erates to roberta.
we ﬁnetune different modelson the ontonotes dataset of the ner dataset forcomparison..results are shown in table 9. as can be seen,either removing glyph embeddings or pinyin em-beddings results in performance degradation, andremoving both has the greatest negative impact onthe f1 value, which is a drop of about 2 points.
thisvalidates the importance of both pinyin and glyphembeddings for modeling chinese semantics.
thereason why “-glyph-pinyin” performs worse thanroberta is that the model we use here is trainedon a smaller size of data with smaller number oftraining steps..6.2 the effect of training data size.
we hypothesize glyph and pinyin embeddings alsoserve as strong regularization over text seman-tics, which means that the proposed chinesebertmodel is able to perform better with less trainingdata.
we randomly sample 10%∼90% of the train-ing data while maintaining the ratio of sampleswith entities w.r.t.
samples without entities.
weperform each experiment ﬁve times and report theaverage f1 value on the test set.
figure 5 shows the.
erocs-1ftset.84.
82.
80.
78.
76.
74.
0.bertrobertachinesebert.
0.2.
0.4.
0.6.
0.8.
1.0.training size.
figure 5: performances when varying the training size..7 conclusion.
in this paper, we introduce chinesebert, a large-scale pretraining chinese nlp model.
it leveragesthe glyph and pinyin information of chinese char-acters to enhance the model’s ability of capturingcontext semantics from surface character forms anddisambiguating polyphonic characters in chinese.
the proposed chinesebert model achieves sig-niﬁcant performance boost across a wide range ofchinese nlp tasks.
the proposed chinesebertperforms better than vanilla pretrained models withless training data, indicating that the introducedglyph embeddings and pinyin embeddings serveas a strong regularizer for semantic modeling inchinese.
future work involves training a large sizeversion of chinesebert..acknowledgement.
this work is supported by national key r&d pro-gram of china (2020aaa0105200) and beijingacademy of artiﬁcial intelligence (baai)..2072references.
hangbo bao, li dong, furu wei, wenhui wang, nanyang, xiaodong liu, yu wang, songhao piao, jian-feng gao, ming zhou, and hsiao-wuen hon.
2020.unilmv2: pseudo-masked language models for uni-ﬁed language model pre-training..tom b. brown, benjamin mann, nick ryder, melaniesubbiah, jared kaplan, prafulla dhariwal, arvindneelakantan, pranav shyam, girish sastry, amandaaskell, sandhini agarwal, ariel herbert-voss,gretchen krueger, tom henighan, rewon child,aditya ramesh, daniel m. ziegler, jeffrey wu,clemens winter, christopher hesse, mark chen,eric sigler, mateusz litwin, scott gray, benjaminchess, jack clark, christopher berner, sam mc-candlish, alec radford, ilya sutskever, and darioamodei.
2020. language models are few-shot learn-ers..duo chai, wei wu, qinghong han, fei wu, and jiweili.
2020. description based text classiﬁcation withreinforcement learning..wanxiang che, zhenghua li, and ting liu.
2010. ltp:a chinese language technology platform.
in coling2010: demonstrations, pages 13–16, beijing, china.
coling 2010 organizing committee..jing chen, qingcai chen, xin liu, haijun yang,daohe lu, and buzhou tang.
2018. the bq cor-pus: a large-scale domain-speciﬁc chinese cor-pus for sentence semantic equivalence identiﬁca-in proceedings of the 2018 conference ontion.
empirical methods in natural language processing,pages 4946–4951, brussels, belgium.
associationfor computational linguistics..jason pc chiu and eric nichols.
2016. named entityrecognition with bidirectional lstm-cnns.
transac-tions of the association for computational linguis-tics, 4:357–370..krzysztof choromanski, valerii likhosherstov, daviddohan, xingyou song, andreea gane, tamás sar-lós, peter hawkins, jared davis, afroz mohiuddin,lukasz kaiser, david belanger, lucy colwell, andadrian weller.
2020. rethinking attention with per-formers.
corr..christopher clark and matt gardner.
2017. simpleand effective multi-paragraph reading comprehen-sion.
arxiv preprint arxiv:1710.10723..kevin clark, minh-thang luong, quoc v. le, andchristopher d. manning.
2020.electra: pre-training text encoders as discriminators rather thanin international conference on learn-generators.
ing representations..yiming cui, wanxiang che, ting liu, bing qin, shi-jin wang, and guoping hu.
2020. revisiting pre-trained models for chinese natural language process-ing.
arxiv preprint arxiv:2004.13922..yiming cui, wanxiang che, ting liu, bing qin,ziqing yang, shijin wang, and guoping hu.
2019a.
pre-training with whole word masking for chinesebert.
arxiv preprint arxiv:1906.08101..yiming cui, ting liu, wanxiang che, li xiao,zhipeng chen, wentao ma, shijin wang, and guop-ing hu.
2019b.
a span-extraction dataset for chi-nese machine reading comprehension.
in proceed-ings of the 2019 conference on empirical methodsin natural language processing and the 9th inter-national joint conference on natural language pro-cessing (emnlp-ijcnlp), pages 5886–5891, hongkong, china.
association for computational lin-guistics..falcon z dai and zheng cai.
2017. glyph-aware em-in proceedings ofbedding of chinese characters.
the first workshop on subword and character levelmodels in nlp, copenhagen, denmark, september7, 2017, pages 64–69..jacob devlin, ming-wei chang, kenton lee, andkristina toutanova.
2018. bert: pre-training of deepbidirectional transformers for language understand-ing.
arxiv preprint arxiv:1810.04805..li dong, nan yang, wenhui wang, furu wei, xi-aodong liu, yu wang, jianfeng gao, ming zhou,and hsiao-wuen hon.
2019. uniﬁed languagemodel pre-training for natural language understand-ing and generation.
in advances in neural informa-tion processing systems, volume 32, pages 13063–13075. curran associates, inc..xingyi duan, baoxin wang, ziyue wang, wentao ma,yiming cui, dayong wu, shijin wang, ting liu,tianxiang huo, zhen hu, and et al.
2019. cjrc: areliable human-annotated benchmark dataset for chi-nese judicial reading comprehension.
chinese com-putational linguistics, page 439–451..mandar joshi, danqi chen, yinhan liu, daniel s weld,luke zettlemoyer, and omer levy.
2020. spanbert:improving pre-training by representing and predict-ing spans.
transactions of the association for com-putational linguistics, 8:64–77..guillaume lample, miguel ballesteros, sandeep sub-ramanian, kazuya kawakami, and chris dyer.
2016.neural architectures for named entity recognition.
arxiv preprint arxiv:1603.01360..guillaume lample and alexis conneau.
2019. cross-lingual language model pretraining.
advances inneural information processing systems (neurips)..alexis conneau, guillaume lample, ruty rinott, ad-ina williams, samuel r bowman, holger schwenk,and veselin stoyanov.
2018. xnli: evaluating cross-arxiv preprintlingual sentence representations.
arxiv:1809.05053..guillaume.
lample,.
alexandre.
sablayrolles,andmarc’aurelio ranzato, ludovic denoyer,hervé jégou.
2019. large memory layers withadvances in neural informationproduct keys.
processing systems (neurips)..2073zhenzhong lan, mingda chen, sebastian goodman,kevin gimpel, piyush sharma, and radu soricut.
2020. albert: a lite bert for self-supervised learningin international con-of language representations.
ference on learning representations..mike lewis, yinhan liu, naman goyal, mar-jan ghazvininejad, abdelrahman mohamed, omerlevy, ves stoyanov, and luke zettlemoyer.
2019.bart: denoising sequence-to-sequence pre-trainingtranslation, andfor natural language generation,comprehension.
arxiv preprint arxiv:1910.13461..patrick lewis, ethan perez, aleksandara piktus, fabiopetroni, vladimir karpukhin, naman goyal, hein-rich küttler, mike lewis, wen-tau yih, tim rock-täschel, et al.
2020. retrieval-augmented generationfor knowledge-intensive nlp tasks.
arxiv preprintarxiv:2005.11401..jingyang li and maosong sun.
2007. scalable term se-lection for text categorization.
in proceedings of the2007 joint conference on empirical methods in nat-ural language processing and computational nat-ural language learning (emnlp-conll), pages774–782..xiaoya li, jingrong feng, yuxian meng, qinghonghan, fei wu, and jiwei li.
2019a.
a uniﬁed mrcarxivframework for named entity recognition.
preprint arxiv:1910.11476..xiaoya li, yuxian meng, xiaofei sun, qinghong han,arianna yuan, and jiwei li.
2019b.
is word segmen-tation necessary for deep learning of chinese rep-in proceedings of the 57th annualresentations?
meeting of the association for computational lin-guistics, pages 3242–3252, florence, italy.
associa-tion for computational linguistics..yanran li, wenjie li, fei sun, and sujian li.
2015. component-enhanced chinese character em-beddings.
in proceedings of the 2015 conference onempirical methods in natural language processing,emnlp 2015, lisbon, portugal, september 17-21,2015, pages 829–834..frederick liu, han lu, chieh lo, and graham neu-big.
2017. learning character-level composition-in proceedings of theality with visual features.
55th annual meeting of the association for compu-tational linguistics, acl 2017, vancouver, canada,july 30 - august 4, volume 1: long papers, pages2059–2068..xiaodong liu, pengcheng he, weizhu chen, and jian-feng gao.
2019a.
multi-task deep neural networksfor natural language understanding.
in proceedingsof the 57th annual meeting of the association forcomputational linguistics, pages 4487–4496, flo-rence, italy.
association for computational linguis-tics..xin liu, qingcai chen, chong deng, huajun zeng,jing chen, dongfang li, and buzhou tang.
2018..lcqmc: a large-scale chinese question matchingin proceedings of the 27th internationalcorpus.
conference on computational linguistics, pages1952–1962..yinhan liu, myle ott, naman goyal, jingfei du, man-dar joshi, danqi chen, omer levy, mike lewis,luke zettlemoyer, and veselin stoyanov.
2019b.
roberta: a robustly optimized bert pretraining ap-proach.
arxiv preprint arxiv:1907.11692..yuxian meng, wei wu, fei wang, xiaoya li, ping nie,fan yin, muyu li, qinghong han, xiaofei sun, andjiwei li.
2019. glyce: glyph-vectors for chinesein advances in neuralcharacter representations.
information processing systems, volume 32, pages2746–2757.
curran associates, inc..tomas mikolov, kai chen, greg corrado, and jef-frey dean.
2013a.
efﬁcient estimation of wordarxiv preprintrepresentations in vector space.
arxiv:1301.3781..tomas mikolov, ilya sutskever, kai chen, greg s cor-rado, and jeff dean.
2013b.
distributed representa-tions of words and phrases and their compositional-ity.
advances in neural information processing sys-tems, 26:3111–3119..nanyun peng and mark dredze.
2015. named en-tity recognition for chinese social media with jointlyin proceedings of the 2015trained embeddings.
conference on empirical methods in natural lan-guage processing, pages 548–554..alec radford, jeffrey wu, rewon child, david luan,dario amodei, and ilya sutskever.
2019. languagemodels are unsupervised multitask learners.
openaiblog, 1(8)..nils reimers and iryna gurevych.
2019. sentence-bert: sentence embeddings using siamese bert-networks.
arxiv preprint arxiv:1908.10084..rico sennrich, barry haddow, and alexandra birch.
2016. neural machine translation of rare wordswith subword units.
in proceedings of the 54th an-nual meeting of the association for computationallinguistics (volume 1: long papers), pages 1715–1725, berlin, germany.
association for computa-tional linguistics..yan shao, christian hardmeier, jörg tiedemann, andjoakim nivre.
2017. character-based joint segmen-tation and pos tagging for chinese using bidirec-tional rnn-crf.
in proceedings of the eighth interna-tional joint conference on natural language pro-cessing, ijcnlp 2017, taipei, taiwan, november27 - december 1, 2017 - volume 1: long papers,pages 173–183..xinlei shi, junjie zhai, xudong yang, zehua xie,and chao liu.
2015. radical embedding: delv-in proceedingsing deeper to chinese radicals.
of the 53rd annual meeting of the association forcomputational linguistics and the 7th international.
2074joint conference on natural language processing(volume 2: short papers), pages 594–598, beijing,china.
association for computational linguistics..and jeffrey dean.
2016. google’s neural machinetranslation system: bridging the gap between humanand machine translation..liang xu, hai hu, xuanwei zhang, lu li, chenjiecao, yudong li, yechen xu, kai sun, dian yu,cong yu, yin tian, qianqian dong, weitang liu,bo shi, yiming cui, junyi li, jun zeng, rongzhaowang, weijian xie, yanting li, yina patterson,zuoyu tian, yiwen zhang, he zhou, shaoweihualiu, zhe zhao, qipeng zhao, cong yue, xinruizhang, zhengliang yang, kyle richardson, andzhenzhong lan.
2020. clue: a chinese languagein proceed-understanding evaluation benchmark.
ings of the 28th international conference on com-putational linguistics, pages 4762–4772, barcelona,spain (online).
international committee on compu-tational linguistics..zhenyu xuan, rui bao, and shengyi jiang.
2020.fgn: fusion glyph network for chinese named entityrecognition..zhilin yang, zihang dai, yiming yang, jaime car-bonell, russ r salakhutdinov, and quoc v le.
2019.xlnet: generalized autoregressive pretraining forlanguage understanding.
in advances in neural in-formation processing systems, pages 5753–5763..rongchao yin, quan wang, peng li, rui li, and binwang.
2016. multi-granularity chinese word embed-in proceedings of the 2016 conference onding.
empirical methods in natural language processing,pages 981–986..xiang zhang and yann lecun.
2017. which en-coding is the best for text classiﬁcation in chinese,arxiv preprintenglish,arxiv:1708.02657..japanese and korean?.
zhengyan zhang, xu han, hao zhou, pei ke, yuxiangu, deming ye, yujia qin, yusheng su, haozheji, jian guan, fanchao qi, xiaozhi wang, yananzheng, guoyang zeng, huanqi cao, shengqi chen,daixuan li, zhenbo sun, zhiyuan liu, minliehuang, wentao han, jie tang, juanzi li, xiaoyanzhu, and maosong sun.
2020. cpm: a large-scalegenerative chinese pre-trained language model..jinhua zhu, yingce xia, lijun wu, di he, tao qin,wengang zhou, houqiang li, and tieyan liu.
2020.incorporating bert into neural machine translation.
in international conference on learning represen-tations..chan hee song and arijit sehanobish.
2020. usingchinese glyphs for named entity recognition.
pro-ceedings of the aaai conference on artiﬁcial intel-ligence, 34(10):13921–13922..kaitao song, xu tan, tao qin, jianfeng lu, and tie-yan liu.
2019. mass: masked sequence to se-quence pre-training for language generation.
in in-ternational conference on machine learning, pages5926–5936..tzu-ray su and hung-yi lee.
2017. learning chi-nese word representations from glyphs of charac-in proceedings of the 2017 conference onters.
empirical methods in natural language processing,emnlp 2017, copenhagen, denmark, september 9-11, 2017, pages 264–273..yaming sun, lei lin, nan yang, zhenzhou ji, andxiaolong wang.
2014. radical-enhanced chinesein international conferencecharacter embedding.
on neural information processing, pages 279–286.
springer..yu sun, shuohuan wang, yukun li, shikun feng, xuyichen, han zhang, xin tian, danxiang zhu, haotian, and hua wu.
2019. ernie: enhanced rep-resentation through knowledge integration.
arxivpreprint arxiv:1904.09223..yu sun, shuohuan wang, yukun li, shikun feng, haotian, hua wu, and haifeng wang.
2020. ernie 2.0:a continual pre-training framework for language un-derstanding.
proceedings of the aaai conferenceon artiﬁcial intelligence, 34(05):8968–8975..hanqing tao, shiwei tong, tong xu, qi liu, and en-hong chen.
2019. chinese embedding via strokeand glyph information: a dual-channel view..ashish vaswani, noam shazeer, niki parmar, jakobuszkoreit, llion jones, aidan n gomez, łukaszkaiser, and illia polosukhin.
2017. attention is allin advances in neural information pro-you need.
cessing systems, pages 5998–6008..ralph weischedel, sameer pradhan, lance ramshaw,martha palmer, nianwen xue, mitchell marcus,ann taylor, craig greenberg, eduard hovy, robertontonotes release 4.0.belvin, et al.
2011.ldc2011t03, philadelphia, penn.
: linguistic dataconsortium..yonghui wu, mike schuster, zhifeng chen, quoc v.le, mohammad norouzi, wolfgang macherey,maxim krikun, yuan cao, qin gao, klausmacherey, jeff klingner, apurva shah, melvin john-son, xiaobing liu, łukasz kaiser, stephan gouws,yoshikiyo kato, taku kudo, hideto kazawa, keithstevens, george kurian, nishant patil, wei wang,cliff young, jason smith, jason riesa, alex rud-nick, oriol vinyals, greg corrado, macduff hughes,.
2075