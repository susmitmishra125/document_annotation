chinesebert: chinese pretraining enhanced byglyph and pinyin information.
zijun sunâ™£, xiaoya liâ™£, xiaofei sunâ™£, yuxian mengâ™£xiang aoâ™ , qing heâ™ , fei wu(cid:7) and jiwei li(cid:7)â™£â™£shannon.ai, (cid:7)zhejiang universityâ™ key lab of intelligent information processing of chinese academy of sciences{zijun_sun, xiaoya_li, xiaofei_sun, yuxian_meng, jiwei_li}@shannonai.com{aoxiang,heqing}@ict.ac.cn, wufei@zju.edu.cn.
abstract.
recent pretraining models in chinese neglecttwo important aspects speciï¬c to the chineselanguage: glyph and pinyin, which carry sig-niï¬cant syntax and semantic information forlanguage understanding.
in this work, we pro-pose chinesebert, which incorporates boththe glyph and pinyin information of chinesecharacters into language model pretraining.
the glyph embedding is obtained based ondifferent fonts of a chinese character, beingable to capture character semantics from thevisual features, and the pinyin embedding char-acterizes the pronunciation of chinese charac-ters, which handles the highly prevalent het-eronym phenomenon in chinese (the samecharacter has different pronunciations with dif-ferent meanings).
pretrained on large-scale un-labeled chinese corpus, the proposed chine-sebert model yields signiï¬cant performanceboost over baseline models with fewer train-ing steps.
the proposed model achievesnew sota performances on a wide range ofchinese nlp tasksï¼Œincluding machine read-language infer-ing comprehension, naturalence, text classiï¬cation, sentence pair match-ing, and competitive performances in namedentity recognition and word segmentation.1.
1.introduction.
large-scale pretrained models have become a fun-damental backbone for various natural languageprocessing tasks such as natural language under-standing (liu et al., 2019b), text classiï¬cation(reimers and gurevych, 2019; chai et al., 2020)and question answering (clark and gardner, 2017;lewis et al., 2020).
apart from english nlp tasks,pretrained models have also demonstrated their ef-fectiveness for various chinese nlp tasks (sunet al., 2019, 2020; cui et al., 2019a, 2020)..1the code and pretrained models are publicly available athttps://github.com/shannonai/chinesebert..since pretraining models are originally designedfor english, two important aspects speciï¬c to thechinese language are missing in current large-scalepretraining: glyph-based information and pinyin-based information.
for the former, a key aspectthat makes chinese distinguishable from languagessuch as english, german, is that chinese is a lo-gographic language.
the logographic of charac-ters encodes semantic information.
for example,â€œæ¶²(liquid)â€, â€œæ²³(river)â€ and â€œæ¹–(lake)â€ all have theradical â€œæ°µ(water)â€, which indicates that they areall related to water in semantics.
intuitively, therich semantics behind chinese character glyphsshould enhance the expressiveness of chinese nlpmodels.
this idea has motivated a variety of ofwork on learning and incorporating chinese glyphinformation into neural models (sun et al., 2014;shi et al., 2015; liu et al., 2017; dai and cai, 2017;su and lee, 2017; meng et al., 2019), but not yetlarge-scale pretraining..for the latter, pinyin, the romanized sequenceof a chinese character representing its pronuncia-tion(s), is crucial in modeling both semantic andsyntax information that can not be captured by con-textualized or glyph embeddings.
this aspect isespecially important considering the highly preva-lent heteronym phenomenon in chinese2, wherethe same character have multiple pronunciations,each of which is associated with a speciï¬c meaning.
each pronunciation is associated with a speciï¬cpinyin expression.
at the semantic level, for exam-ple, the chinese character â€œä¹â€ has two distinctlydifferent pronunciations: â€œä¹â€ can be pronouncedas â€œyuÃ¨ [ye51]â€, which means â€œmusicâ€, and â€œlÃ¨[lg51]â€, which means â€œhappyâ€.
on the syntax level,pronunciations help identify the part-of-speech ofa character.
for example, character â€œè¿˜â€ has two.
2among 7000 common characters in chinese, there areabout 700 characters that have multiple pronunciations, ac-cording to the contemporary chinese dictionary..proceedingsofthe59thannualmeetingoftheassociationforcomputationallinguisticsandthe11thinternationaljointconferenceonnaturallanguageprocessing,pages2065â€“2075august1â€“6,2021.Â©2021associationforcomputationallinguistics2065pronunciations: â€œhuÃ¡n[xwan35]â€ and â€œhÃ¡i[xai35]â€,with the former meaning the verb â€œreturnâ€ and thelatter meaning the adverb â€œalsoâ€.
different pro-nunciations of the same character cannot be dis-tinguished by the glyph embedding since the lo-gographic is the same, or the char-id embedding,since they both point to the same character id, butcan be characterized by pinyin..in this work, we propose chinesebert, a modelthat incorporates the glyph and pinyin informationof chinese characters into the process of large-scale pretraining.
the glyph embedding is based ondifferent fonts of a chinese character, being able tocapture character semantics from the visual surfacecharacter forms.
the pinyin embedding modelsdifferent semantic meanings that share the samecharacter form and thus bypasses the limitation ofinterwound morphemes behind a single character.
for a chinese character, the glyph embedding, thepinyin embedding and the character embeddingare combined to form a fusion embedding, whichmodels the distinctive semantic property of thatcharacter..with less training data and fewer training epochs,chinesebert achieves signiï¬cant performanceboost over baselines across a wide range of chinesenlp tasks.
it achieves new sota performanceson a wide range of chinese nlp tasksï¼Œincludingmachine reading comprehension, natural languageinference, text classiï¬cation, sentence pair match-ing, and results comparable to sota performancesin named entity recognition and word segmenta-tion..2 related work.
2.1 large-scale pretraining in nlp.
recent years has witnessed substantial work onlarge-scale pretraining in nlp.
bert (devlin et al.,2018), which is built on top of the transformerarchitecture (vaswani et al., 2017), is pretrainedon large-scale unlabeled text corpus in the man-ner of masked language model (mlm) and nextsentence prediction (nsp).
following this trend,considerable progress has been made by modifyingthe masking strategy (yang et al., 2019; joshi et al.,2020), pretraining tasks (liu et al., 2019a; clarket al., 2020) or model backbones (lan et al., 2020;lample et al., 2019; choromanski et al., 2020).
speciï¬cally, roberta (liu et al., 2019b) proposedto remove the nsp pretraining task since it has beenproved to offer no beneï¬ts for improving down-.
stream performances.
the gpt series (radfordet al., 2019; brown et al., 2020) and other bertvariants (lewis et al., 2019; song et al., 2019; lam-ple and conneau, 2019; dong et al., 2019; baoet al., 2020; zhu et al., 2020) adapted the paradigmof large-scale unsupervised pretraining to text gen-eration tasks such as machine translation, text sum-marization and dialog generation, so that generativemodels can enjoy the beneï¬t of large-scale pretrain-ing..unlike the english language, chinese has itsparticular characteristics in terms of syntax, lexi-con and pronunciation.
hence, pretraining chinesemodels should ï¬t the chinese features correspond-ingly.
li et al.
(2019b) proposed to use chinesecharacter as the basic unit instead of word or sub-word that is used in english (wu et al., 2016; sen-nrich et al., 2016).
ernie (sun et al., 2019, 2020)applied three types of masking strategies â€“ char-level masking, phrase-level masking and entity-level masking â€“ to enhance the ability of captur-ing multi-granularity semantics.
cui et al.
(2019a,2020) pretrained models using the whole wordmasking strategy, where all characters within achinese word are masked altogether.
in this way,the model is learning to address a more challengingtask as opposed to predicting word components.
more recently, zhang et al.
(2020) developed thelargest chinese pretrained language model to date â€“cpm.
it is pretrained on 100gb chinese data andhas 2.6b parameters comparable to â€œgpt3 2.7bâ€(brown et al., 2020).
xu et al.
(2020) releasedthe ï¬rst large-scale chinese language understand-ing evaluation benchmark clue, facilitating re-searches in large-scale chinese pretraining..2.2 learning glyph information.
learning glyph information from surface chinesecharacter forms has gained attractions since theprevalence of deep neural networks.
inspired byword embeddings (mikolov et al., 2013b,a), sunet al.
(2014); shi et al.
(2015); li et al.
(2015); yinet al.
(2016) used indexed radical embeddings tocapture character semantics, improving model per-formances on a wide range of chinese nlp tasks.
another way of incorporating glyph information isto view characters in the form of image, by whichglyph information can be naturally learned throughimage modeling.
however, early work on learningvisual features is not smooth.
liu et al.
(2017);shao et al.
(2017); zhang and lecun (2017); dai.
2066figure 1: an overview of chinesebert.
the fu-sion layer consumes three d-dimensional embeddingsâ€“ char embedding, glyph embedding and pinyin embed-ding.
the three embeddings are ï¬rst concatenated, andthen mapped to a d-dimensional embedding through afully connected layer to form the fusion embedding..and cai (2017) used cnns to extract glyph fea-tures from character images but did not achieveconsistent performance boost over all tasks.
su andlee (2017); tao et al.
(2019) obtained positive re-sults on the word analogy and word similarity tasksbut they did not further evaluate the learned glyphembeddings on more tasks.
meng et al.
(2019) ap-plied glyph embeddings to a broad array of chinesetasks.
they designed a speciï¬c cnn structure forcharacter feature extraction and used image classi-ï¬cation as an auxiliary objective to regularize theinï¬‚uence of a limited number of images.
song andsehanobish (2020); xuan et al.
(2020) extendedthe idea of meng et al.
(2019) to the task of namedentity recognition (ner), signiï¬cantly improvingperformances against vanilla bert models..3 model.
figure 2: an overview of inducing the glyph embed-ding.
(cid:78) denotes vector concatenation.
for each chi-nese character, we use three types of fonts â€“ fangsong,xingkai and lishu, each of which is a 24 Ã— 24 imagewith pixel value ranging 0 âˆ¼ 255. images are concate-nated into a tensor of size 24 Ã— 24 Ã— 3. the tensor isï¬‚attened and passed to an fc to obtain the glyph em-bedding..figure 3: an overview of inducing the pinyin embed-ding.
for any chinese character, e.g.
çŒ«(cat) in thiscase, a cnn with width 2 is applied to the sequence ofromanized pinyin letters, followed by max-pooling toderive the ï¬nal pinyin embedding..figure 4: an overview of the fusion layer.
(cid:78) denotesvector concatenation, and Ã— is vector-matrix multipli-cation.
we concatenate the char embedding, the glyphembedding and the pinyin embedding, and use an fclayer with a learnable matrix wf to induce the fusionembedding..3.1 overview.
figure 1 shows an overview of the proposed chi-nesebert model.
for each chinese character, itschar embedding, glyph embedding and pinyin em-bedding are ï¬rst concatenated, and then mapped toa d-dimensional embedding through a fully con-nected layer to form the fusion embedding.
the fu-sion embedding is then added with the position em-bedding, which is fed as input to the bert modelsince we do not use the nsp pretraining task, weomit the segment embedding.
we use both wholeword masking (wwm) (cui et al., 2019a) andchar masking (cm) for pretraining (see section4.2 for details)..3.2.input.
the input to the model is the addition of the learn-able absolute positional embedding and the fusionembedding, where the fusion embedding is basedon the char embedding, the glyph embedding andthe pinyin embedding of the corresponding charac-ter.
the char embedding performs in a way anal-ogous to the token embedding used in bert butat the character granularity.
below we respectivelydescribe how to induce the glyph embedding, thepinyin embedding and the fusion embedding..2067fusion layerbertæˆ‘å¾ˆ[m][m]çŒ«æˆ‘å¾ˆ[m][m]çŒ«iverycatschar embedding03124position embeddingglyph embeddingpinyin embeddingæˆ‘å¾ˆ00çŒ«wo3hen300mao1fusion embeddingæˆ‘å¾ˆ[m][m]çŒ«fusion layerå–œæ¬¢likeoutputçŒ«çŒ«mao1char embeddingglyph embeddingpinyin embeddingçŒ«fusion embeddingçŒ«mÄomao1----mao1----cnnçŒ«1çŒ«3çŒ«2glyph layerçŒ«glyph embeddingcomponent embeddingmao1flattenflattenflatten2828ğ–gğ–fpinyin embeddingfusion layerbertæˆ‘å¾ˆ[m][m]çŒ«æˆ‘å¾ˆ[m][m]çŒ«iverycatschar embedding03124positionembeddingglyph embeddingpinyin embeddingæˆ‘å¾ˆ00çŒ«wo3hen300mao1fusionembeddingæˆ‘å¾ˆ[m][m]çŒ«fusion layerå–œæ¬¢likeoutputçŒ«çŒ«mao1char embeddingglyph embeddingpinyin embeddingçŒ«fusion embeddingçŒ«mÄomao1----mao1----cnnçŒ«1çŒ«3çŒ«2glyph layerçŒ«glyph embeddingmao1flattenflattenflatten2424ğ–gğ–ffusion layerbertæˆ‘å¾ˆ[m][m]çŒ«æˆ‘å¾ˆ[m][m]çŒ«iverycatschar embedding03124position embeddingglyph embeddingpinyin embeddingæˆ‘å¾ˆ00çŒ«wo3hen300mao1fusion embeddingæˆ‘å¾ˆ[m][m]çŒ«fusion layerå–œæ¬¢likeoutputçŒ«çŒ«mao1char embeddingglyph embeddingpinyin embeddingçŒ«fusion embeddingçŒ«mÄomao1----mao1----cnnçŒ«1çŒ«3çŒ«2glyph layerçŒ«glyph embeddingcomponent embeddingmao1flattenflattenflatten2828ğ–gğ–fpinyin embeddingfusion layerbertæˆ‘å¾ˆ[m][m]çŒ«æˆ‘å¾ˆ[m][m]çŒ«iverycatschar embedding03124position embeddingglyph embeddingpinyin embeddingæˆ‘å¾ˆ00çŒ«wo3hen300mao1fusion embeddingæˆ‘å¾ˆ[m][m]çŒ«fusion layerå–œæ¬¢likeoutputçŒ«çŒ«mao1char embeddingglyph embeddingpinyin embeddingçŒ«fusion embeddingçŒ«mÄomao1----mao1----cnnçŒ«1çŒ«3çŒ«2glyph layerçŒ«glyph embeddingcomponent embeddingmao1flattenflattenflatten2828ğ–gğ–fglyph embedding we follow meng et al.
(2019) to use three types of chinese fonts â€“ fang-song, xingkai and lishu, each of which is instan-tiated as a 24 Ã— 24 image with ï¬‚oating point pixelsranging from 0 to 255. the 24Ã—24Ã—3 vector is ï¬rstï¬‚attened to a 2,352 vector.
the ï¬‚attened vector isfed to an fc layer to obtain the output glyph vector..pinyin embedding the pinyin embedding foreach character is used to decouple different seman-tic meanings belonging to the same character form,as shown in figure 3. we use the opensourcedpypinyin package3 to generate pinyin sequencesfor its constituent characters.
pypinyin is a sys-tem that combines machine learning models withdictionary-based rules to infer the pinyin for char-acters given contexts.
pinyin for a chinese char-acter is a sequence of romanian characters, withone of four diacritics denoting tones.
we use spe-cial tokens to denote tones, which are appended tothe end of the romanian character sequence.
weapply a cnn model with width 2 on the pinyinsequence, followed by max-pooling to derive theresulting pinyin embedding.
this makes outputdimensionality immune to the length of the inputpinyin sequence.
the length of the input pinyin se-quence is ï¬xed at 8, with the remaining slots ï¬lledwith a special letter â€œ-â€ when the actual length ofthe pinyin sequence does not reach 8..fusion embedding once we have the char em-bedding, the glyph embedding and the pinyin em-bedding for a character, we concatenate them toform a 3d-dimensional vector.
the fusion layersmaps the 3d-dimensional vector to d-dimensionalthrough a fully connected layer.
the fusion embed-ding is added with position embedding, and outputto the bert layer.
an illustration is shown infigure 4..3.3 output.
the output is the corresponding contextualized rep-resentation for each input chinese character (de-vlin et al., 2018)..4 pretraining setup.
4.1 data.
we collected our pretraining data from common-crawl4.
after pre-processing (such as removingthe data with too much english text and ï¬ltering.
the html tagger), about 10% high-quality data ismaintained for pretraining, containing 4b chinesecharacters in total.
we use the ltp toolkit5 (cheet al., 2010) to identify the boundary of chinesewords for whole word masking..4.2 masking strategies.
we use two masking strategies â€“ whole wordmasking (wwm) and char masking (cm) for chi-nesebert.
li et al.
(2019b) suggested that usingchinese characters as the basic input unit can al-leviate the out-of-vocabulary issue in the chineselanguage.
we thus adopt the method of maskingrandom characters in the given context, denoted bychar masking.
on the other hand, a large numberof words in chinese consist of multiple characters,for which the cm strategy may be too easy forthe model to predict.
for example, for the inputcontext â€œæˆ‘å–œæ¬¢é€›ç´«ç¦[m] (i like going to theforbidden [m])â€, the model can easily predict thatthe masked character is â€œåŸ(city)â€.
hence, we fol-low cui et al.
(2019a) to use wwm, a strategy tomask out all characters within a selected word, mit-igating the easy-predicting shortcoming of the cmstrategy.
note that for both wwm and cm, thebasic input unit is chinese characters.
the maindifference between wwm and cm lies in howthey mask characters and how the model predictsmasked characters..4.3 pretraining details.
different from cui et al.
(2019a) who pretrainedtheir model based on the ofï¬cial pretrained chinesebert model, we train the chinesebert modelfrom scratch.
to enforce the model to learn bothlong-term and short-term dependencies, we pro-pose to alternate pretraining between packed inputand single input, where the packed input is the con-catenation of multiple sentences with a maximumlength 512, and the single input is a single sen-tence.
we feed the packed input with probabilityof 0.9 and the single input with probability of 0.1.we apply whole word masking 90% of the timeand char masking 10% of the time.
the mask-ing probability for each word/char is 15%.
if thei-th word/char is chosen, we mask it 80% of thetime, replace it with a random word/char 10% ofthe time and maintain it 10% of the time.
we alsouse the dynamic masking strategy to avoid dupli-cate training instances (liu et al., 2019b).
we use.
3https://pypi.org/project/pypinyin/4https://commoncrawl.org/.
5http://ltp.ai/.
2068ernie.
bert-wwm.
macbert.
chinesebert.
heterogeneous wikipedia.
heterogeneous commoncrawl.
data sourcevocab sizeinput unitmaskingtasktraining stepsinit checkpoint# token.
18kchart/p/emlm/nsp-.
â€“.
21kcharwwmmlm2mbert0.4b.
21kcharwwm/nmac/sop1mbert5.4b.
21kcharwwm/cmmlm1mrandom5b.
table 1: comparison of data statistics between ernie(sun et al., 2019), bert-wwm (cui et al., 2019a),macbert (cui et al., 2020) and our proposed chine-sebert.
t: token, p: phrase, e: entity, wwm: wholeword masking, n: n-gram, cm: char masking, mlm:masked language model, nsp: next sentence predic-tion, mac: mlm-as-correlation.
sop: sentence or-der prediction..two model setups: base and large, respectivelyconsisting of 12/24 transformer layers, with inputdimensionality of 768/1,024 and 12/16 heads perlayer.
this makes our models comparable to otherbert-style models in terms of model size.
uponthe submission of the paper, we have trained thebase model 500k steps with a maximum learn-ing rate 1e-4, warmup of 20k steps and a batchsize of 3.2k, and the large model 280k stepswith a maximum learning rate 3e-4, warmup of90k steps and a batch size of 8k.
after pretrain-ing, the model can be directly used to be ï¬netunedon downstream tasks in the same way as bert(devlin et al., 2018)..5 experiments.
we conduct extensive experiments on a variety ofchinese nlp tasks.
models are separately ï¬ne-tuned on task-speciï¬c datasets for evaluation.
con-cretely, we use the following tasks:.
â€¢ machine reading comprehension (mrc).
â€¢ natural language inference (nli).
â€¢ text classiï¬cation (tc).
â€¢ sentence pair matching (spm).
â€¢ named entity recognition (ner).
â€¢ chinese word segmentation (cws)..we compare chinesebert to current state-of-the-art ernie (sun et al., 2019, 2020), bert-wwm (cui et al., 2019a) and macbert (cui et al.,2020) models.
ernie adopts various maskingstrategies including token-level, phrase-level and.
entity-level masking to pretrain bert on large-scale heterogeneous data.
bert-wwm/roberta-wwm continues pretraining on top of ofï¬cial pre-trained chinese bert/roberta models with thewhole word masking pretraining strategy.
unlessotherwise speciï¬ed, we use bert/roberta torepresent bert-wwm/roberta-wwm and omitâ€œwwmâ€.
macbert improves upon roberta byusing the mlm-as-correlation (mac) pretrainingstrategy as well as the sentence-order prediction(sop) task.
it is worth noting that bert and bert-wwm do not have the large version available online,and we thus omit the corresponding performances.
a comparison of these models is shown in ta-ble 1. it is worth noting that training steps of theproposed model signiï¬cantly smaller than base-line models.
different from bert-wwm andmacbert which are initialized with pretrainedbert, the proposed model is initialized fromscratch.
due to the additional consideration ofglyph and pinyin, the proposed cannot be directlyinitialized using a vanilla bert model, as themodel structures are different.
even initializedfrom scratch, the proposed model is trained fewersteps than the steps in retraining bert-wwm andmacbert after bert initialization..5.1 machine reading comprehension.
machine reading comprehension tests the modelâ€™sability of answering the questions based on thegiven contexts.
we use two datasets for this task:cmrc 2018 (cui et al., 2019b) and cjrc (duanet al., 2019) .
cmrc is a span-extraction styledataset while cjrc additionally has yes/no ques-tions and no-answer questions.
cmrc 2018 andcjrc respectively contain 10k/3.2k/4.9k and39k/6k/6k data instances for training/dev/test.
test results for cmrc 2018 are evaluated from theclue leaderboard.6 note that the cjrc dataset isdifferent from the one used in cui et al.
(2019a) ascui et al.
(2019a) did not release their train/dev/testsplit.
we thus run the released models on the cjrcdataset used in this work for comparison..results are shown in table 2 and table 3. aswe can see, chinesebert yields signiï¬cant perfor-mance boost on both datasets, and the improvementof em is larger than that of f1 on the cjrc dataset,which indicates that chinesebert is better at de-tecting exact answer spans..6https://github.com/cluebenchmark/clue.
2069model.
erniebertbertâ—¦robertaâ—¦macbertchinesebert.
robertaâ—¦macbertchinesebert.
cmrc.
dev.
test.
base.
66.8966.7766.9667.89â€“67.95.
70.59â€“70.70.
74.7071.6073.9575.20â€“75.35.
77.95â€“78.05.large.
model.
erniebertbertâ—¦robertaâ—¦macbertchinesebert.
robertaâ—¦macbertchinesebert.
xnli.
dev.
test.
base.
79.779.079.480.080.380.5.
82.182.482.7.
78.678.278.778.879.379.6.
81.281.381.6.large.
table 2: performances of different models on cmrc.
em is reported for comparison.
â—¦ represents modelspretrained on extended data..table 4: performances of different models on xnli.
accuracy is reported for comparison.
â—¦ represents mod-els pretrained on extended data..cjrc.
base.
model.
devem f1.
testem f1.
bertbertâ—¦robertaâ—¦chinesebert.
robertaâ—¦chinesebert.
59.860.862.965.2.
65.666.5.
73.074.076.677.8.
77.577.9.
60.261.463.866.2.
66.467.0.
73.073.976.677.9.
77.678.3.large.
table 3: performances of different models on the mrcdataset cjrc.
we report results for baseline modelsbased on their released models.
â—¦ represents modelspretrained on extended data..5.2 natural language inference (nli).
the goal of nli is to determine the entailment re-lationship between a hypothesis and a premise.
weuse the cross-lingual natural language inference(xnli) dataset (conneau et al., 2018) for evalu-ation.
the corpus is a crowd-sourced collectionof 5k test and 2.5k dev pairs for the multinlicorpus.
each sentence pair is annotated with theâ€œentailmentâ€, â€œneutralâ€ or â€œcontradictionâ€ label.
weuse the ofï¬cial machine translated chinese data fortraining.7.
results are present in table 4, which shows thatchinesebert is able to achieve the best perfor-mances for both base and large setups..5.3 text classiï¬cation (tc).
in text classiï¬cation the model is required to cat-egorize a piece of text into one of the speciï¬edclasses.
we follow cui et al.
(2019a) to use thuc-.
7https://github.com/facebookresearch/.
xnli.
news (li and sun, 2007) and chnsenticorp8 forthis task.
thucnews is a subset of thuctc9, with 50k/5k/10k data points respectively fortraining/dev/test.
data is evenly distributed in 10domains including sports, ï¬nance, etc.10 chnsen-ticorp is a binary sentiment classiï¬cation datasetcontaining 9.6k/1.2k/1.2k data points respectivelyfor training/dev/test.
the two datasets are rela-tively simple with vanilla bert achieving an ac-curacy of above 95%.
hence, apart from thuc-news and chnsenticorp, we also use tnews, amore difï¬cult dataset that is included in the cluebenchmark (xu et al., 2020).11 tnews is a 15-class short news text classiï¬cation dataset with53k/10k/10k data points for training/dev/test..results are shown in table 5. on chunsen-ticorp and thucnews, the improvement fromchinesebert is marginal as baselines have al-ready achieved quite high results on these twodatasets.
on the tnews dataset, chinesebertoutperforms all other models.
we can see that theernie model only performs slightly worse thanchinesebert.
this is because ernie is trained onadditional web data, which is beneï¬cial to modelweb news text that covers a wide range of domains..5.4 sentence pair matching (spm).
for spm, the model is asked to determine whethera given sentence pair expresses the same seman-tics.
we use the lcqmc (liu et al., 2018) and bqcorpus (chen et al., 2018) datasets for evaluation..8https://github.com/pengming617/bert_.
classification/tree/master/data9http://thuctc.thunlp.org/10https://github.com/gaussic/.
text-classification-cnn-rnn.
11https://github.com/cluebenchmark/clue.
2070model.
chnsenticorptestdev.
thucnewstestdev.
tnews.
dev.
test.
model.
ontonotes 4.0r.f.p.weibor.p.f.erniebertbertâ—¦robertaâ—¦macbertchinesebert.
robertaâ—¦macbertchinesebert.
95.495.195.495.095.295.6.
95.895.795.8.
95.595.495.395.695.695.7.
95.895.995.9.base.
97.698.097.798.398.298.1.
98.398.198.3.
97.597.897.797.897.797.9.
97.897.997.9.
58.2456.0956.7757.51â€“58.64.
58.32â€“59.06.
58.3356.5856.8656.94â€“58.95.
58.61â€“59.47.large.
table 5: performances of different models on tcdatasets chnsenticorp, thucnews and tnews.
theresults of tnews are taken from the clue paper (xuet al., 2020).
accuracy is reported for comparison.
â—¦represents models pretrained on extended data..model.
lcqmc.
dev.
test.
bq corpustestdev.
base.
erniebertbertâ—¦robertaâ—¦macbertchinesebert.
robertaâ—¦macbertchinesebert.
89.889.489.689.089.589.8.
90.490.690.5.
87.287.087.186.487.087.4.
87.087.687.8.
86.386.186.486.086.086.4.
86.386.286.5.
85.085.285.385.085.285.2.
85.885.686.0.large.
table 6: performances of different models on spmdatasets lcqmc and bq corpus.
we report accuracyfor comparison.
â—¦ represents models pretrained on ex-tended data..lcqmc is a large-scale chinese question match-ing corpus for judging whether two given ques-tions have the same intent, with 23.9k/8.8k/12.5ksentence pairs for training/dev/test.
bq corpusis another large-scale chinese dataset containing100k/10k/10k sentence pairs for training/dev/test.
results are shown in table 6. we can see that chi-nesebert generally outperforms macbert onlcqmc but slightly underperforms bert-wwm.
we hypothesis this is because the domain of bqcorpus more ï¬ts the pretraining data of bert-wwm than that of chinesebert..5.5 named entity recognition (ner).
for ner tasks (chiu and nichols, 2016; lampleet al., 2016; li et al., 2019a), the model is askedto identify named entities within a piece of text,which is formalized as a sequence labeling task.
we use ontonotes 4.0 (weischedel et al., 2011)and weibo (peng and dredze, 2015) for this task..bertrobertaâ—¦chinesebert.
79.6980.4380.03.
82.0980.3083.33.robertaâ—¦chinesebert.
80.7280.77.
82.0783.65.base80.8780.3781.65large81.3982.18.
67.1268.4968.27.
66.8867.8169.78.
67.3368.1569.02.
66.7468.75.
70.0272.97.
68.3570.80.table 7: performances of different models on nerdatasets ontonotes 4.0 and weibo.
results of preci-sion (p), recall (r) and f1 (f) on test sets are reportedfor comparison..model.
msra.
pku.
f1.
acc.
f1.
acc.
bertâ—¦robertaâ—¦chinesebert.
98.4298.4698.60.
99.0499.1099.14.
96.8296.8897.02.
97.7097.7297.81.robertaâ—¦chinesebert.
98.4998.67.
99.1399.26.
96.9597.16.
97.8098.01.base.
large.
table 8: performances of different models on cwsdatasets msra and pku.
we report f1 and accuracy(acc) for comparison.
â—¦ represents models pretrainedon extended data..we use ontonotes 4.0 and weibo ner for this task.
ontonotes has 18 named entity types and weibohas 4 named entity types.
ontonotes and weibo re-spectively contain 15k/4k/4k and 1,350/270/270instances for training/dev/test.
results are shownin table 7. as we can see, chinesebert signiï¬-cantly outperforms bert and roberta in termsof f1.
in spite of a slight loss on precision for thebase version, the gains on recall are particularlyhigh, leading to a ï¬nal performance boost on f1..5.6 chinese word segmentation.
the task divides text into words and is formalizedas a character-based sequence labelling task.
weuse the pku and msra datasets for chinese wordsegmentation.
pku consists of 19k/2k sentencesfor training and test, and msra consists of 87k/4ksentences for training and test.
output characterembedding is fed to the softmax function for ï¬nalpredictions.
results are shown in table 8, wherewe can see that chinesebert is able to outperformbert-wwm and roberta-wwm on both datasetsfor both metrics..6 ablation studies.
in this section, we conduct ablation studies tounderstand the behaviors of chinesebert.
we.
2071modelrobertaâ—¦chinesebertâ€“ glyphâ€“ pinyinâ€“ glyph â€“ pinyin.
precision recall.
ontonotes 4.0f1.
80.4380.0377.6777.5478.22.
80.3083.3382.7583.6581.37.
80.3781.6580.13 (-1.52)80.48 (-1.17)79.76 (-1.89).
table 9: performances for different models withoutconsidering glyph or pinyin information..results.
as can be seen, chinesebert performsbetter across all setups.
with less than 30% of thetraining data, the improvement of chinesebertis slight, but with over 30% training data, the per-formance improvement is greater.
this is becausechinesebert still requires sufï¬cient training datato fully train the glyph and pinyin embeddings, andinsufï¬cient training data would lead to inadequatetraining..use the chinese named entity recognition datasetontonotes 4.0 for analysis and all models are basedon the base version..6.1 the effect of glyph embeddings and.
pinyin embeddings.
we would like to explore the effects of glyph em-beddings and pinyin embeddings.
for fair compar-ison, we pretrained different models on the samedataset, with the same number of training steps, andwith the same model size.
setups include â€œ-glyphâ€,where glyph embeddings are not considered andwe only consider pinyin and char-id embeddings;â€œ-pinyinâ€, where pinyin embeddings are not con-sidered and we only consider glyph and char-idembeddings; â€œ-glyph-pinyinâ€, where only char-idembeddings are considered, and the model degen-erates to roberta.
we ï¬netune different modelson the ontonotes dataset of the ner dataset forcomparison..results are shown in table 9. as can be seen,either removing glyph embeddings or pinyin em-beddings results in performance degradation, andremoving both has the greatest negative impact onthe f1 value, which is a drop of about 2 points.
thisvalidates the importance of both pinyin and glyphembeddings for modeling chinese semantics.
thereason why â€œ-glyph-pinyinâ€ performs worse thanroberta is that the model we use here is trainedon a smaller size of data with smaller number oftraining steps..6.2 the effect of training data size.
we hypothesize glyph and pinyin embeddings alsoserve as strong regularization over text seman-tics, which means that the proposed chinesebertmodel is able to perform better with less trainingdata.
we randomly sample 10%âˆ¼90% of the train-ing data while maintaining the ratio of sampleswith entities w.r.t.
samples without entities.
weperform each experiment ï¬ve times and report theaverage f1 value on the test set.
figure 5 shows the.
erocs-1ftset.84.
82.
80.
78.
76.
74.
0.bertrobertachinesebert.
0.2.
0.4.
0.6.
0.8.
1.0.training size.
figure 5: performances when varying the training size..7 conclusion.
in this paper, we introduce chinesebert, a large-scale pretraining chinese nlp model.
it leveragesthe glyph and pinyin information of chinese char-acters to enhance the modelâ€™s ability of capturingcontext semantics from surface character forms anddisambiguating polyphonic characters in chinese.
the proposed chinesebert model achieves sig-niï¬cant performance boost across a wide range ofchinese nlp tasks.
the proposed chinesebertperforms better than vanilla pretrained models withless training data, indicating that the introducedglyph embeddings and pinyin embeddings serveas a strong regularizer for semantic modeling inchinese.
future work involves training a large sizeversion of chinesebert..acknowledgement.
this work is supported by national key r&d pro-gram of china (2020aaa0105200) and beijingacademy of artiï¬cial intelligence (baai)..2072references.
hangbo bao, li dong, furu wei, wenhui wang, nanyang, xiaodong liu, yu wang, songhao piao, jian-feng gao, ming zhou, and hsiao-wuen hon.
2020.unilmv2: pseudo-masked language models for uni-ï¬ed language model pre-training..tom b. brown, benjamin mann, nick ryder, melaniesubbiah, jared kaplan, prafulla dhariwal, arvindneelakantan, pranav shyam, girish sastry, amandaaskell, sandhini agarwal, ariel herbert-voss,gretchen krueger, tom henighan, rewon child,aditya ramesh, daniel m. ziegler, jeffrey wu,clemens winter, christopher hesse, mark chen,eric sigler, mateusz litwin, scott gray, benjaminchess, jack clark, christopher berner, sam mc-candlish, alec radford, ilya sutskever, and darioamodei.
2020. language models are few-shot learn-ers..duo chai, wei wu, qinghong han, fei wu, and jiweili.
2020. description based text classiï¬cation withreinforcement learning..wanxiang che, zhenghua li, and ting liu.
2010. ltp:a chinese language technology platform.
in coling2010: demonstrations, pages 13â€“16, beijing, china.
coling 2010 organizing committee..jing chen, qingcai chen, xin liu, haijun yang,daohe lu, and buzhou tang.
2018. the bq cor-pus: a large-scale domain-speciï¬c chinese cor-pus for sentence semantic equivalence identiï¬ca-in proceedings of the 2018 conference ontion.
empirical methods in natural language processing,pages 4946â€“4951, brussels, belgium.
associationfor computational linguistics..jason pc chiu and eric nichols.
2016. named entityrecognition with bidirectional lstm-cnns.
transac-tions of the association for computational linguis-tics, 4:357â€“370..krzysztof choromanski, valerii likhosherstov, daviddohan, xingyou song, andreea gane, tamÃ¡s sar-lÃ³s, peter hawkins, jared davis, afroz mohiuddin,lukasz kaiser, david belanger, lucy colwell, andadrian weller.
2020. rethinking attention with per-formers.
corr..christopher clark and matt gardner.
2017. simpleand effective multi-paragraph reading comprehen-sion.
arxiv preprint arxiv:1710.10723..kevin clark, minh-thang luong, quoc v. le, andchristopher d. manning.
2020.electra: pre-training text encoders as discriminators rather thanin international conference on learn-generators.
ing representations..yiming cui, wanxiang che, ting liu, bing qin, shi-jin wang, and guoping hu.
2020. revisiting pre-trained models for chinese natural language process-ing.
arxiv preprint arxiv:2004.13922..yiming cui, wanxiang che, ting liu, bing qin,ziqing yang, shijin wang, and guoping hu.
2019a.
pre-training with whole word masking for chinesebert.
arxiv preprint arxiv:1906.08101..yiming cui, ting liu, wanxiang che, li xiao,zhipeng chen, wentao ma, shijin wang, and guop-ing hu.
2019b.
a span-extraction dataset for chi-nese machine reading comprehension.
in proceed-ings of the 2019 conference on empirical methodsin natural language processing and the 9th inter-national joint conference on natural language pro-cessing (emnlp-ijcnlp), pages 5886â€“5891, hongkong, china.
association for computational lin-guistics..falcon z dai and zheng cai.
2017. glyph-aware em-in proceedings ofbedding of chinese characters.
the first workshop on subword and character levelmodels in nlp, copenhagen, denmark, september7, 2017, pages 64â€“69..jacob devlin, ming-wei chang, kenton lee, andkristina toutanova.
2018. bert: pre-training of deepbidirectional transformers for language understand-ing.
arxiv preprint arxiv:1810.04805..li dong, nan yang, wenhui wang, furu wei, xi-aodong liu, yu wang, jianfeng gao, ming zhou,and hsiao-wuen hon.
2019. uniï¬ed languagemodel pre-training for natural language understand-ing and generation.
in advances in neural informa-tion processing systems, volume 32, pages 13063â€“13075. curran associates, inc..xingyi duan, baoxin wang, ziyue wang, wentao ma,yiming cui, dayong wu, shijin wang, ting liu,tianxiang huo, zhen hu, and et al.
2019. cjrc: areliable human-annotated benchmark dataset for chi-nese judicial reading comprehension.
chinese com-putational linguistics, page 439â€“451..mandar joshi, danqi chen, yinhan liu, daniel s weld,luke zettlemoyer, and omer levy.
2020. spanbert:improving pre-training by representing and predict-ing spans.
transactions of the association for com-putational linguistics, 8:64â€“77..guillaume lample, miguel ballesteros, sandeep sub-ramanian, kazuya kawakami, and chris dyer.
2016.neural architectures for named entity recognition.
arxiv preprint arxiv:1603.01360..guillaume lample and alexis conneau.
2019. cross-lingual language model pretraining.
advances inneural information processing systems (neurips)..alexis conneau, guillaume lample, ruty rinott, ad-ina williams, samuel r bowman, holger schwenk,and veselin stoyanov.
2018. xnli: evaluating cross-arxiv preprintlingual sentence representations.
arxiv:1809.05053..guillaume.
lample,.
alexandre.
sablayrolles,andmarcâ€™aurelio ranzato, ludovic denoyer,hervÃ© jÃ©gou.
2019. large memory layers withadvances in neural informationproduct keys.
processing systems (neurips)..2073zhenzhong lan, mingda chen, sebastian goodman,kevin gimpel, piyush sharma, and radu soricut.
2020. albert: a lite bert for self-supervised learningin international con-of language representations.
ference on learning representations..mike lewis, yinhan liu, naman goyal, mar-jan ghazvininejad, abdelrahman mohamed, omerlevy, ves stoyanov, and luke zettlemoyer.
2019.bart: denoising sequence-to-sequence pre-trainingtranslation, andfor natural language generation,comprehension.
arxiv preprint arxiv:1910.13461..patrick lewis, ethan perez, aleksandara piktus, fabiopetroni, vladimir karpukhin, naman goyal, hein-rich kÃ¼ttler, mike lewis, wen-tau yih, tim rock-tÃ¤schel, et al.
2020. retrieval-augmented generationfor knowledge-intensive nlp tasks.
arxiv preprintarxiv:2005.11401..jingyang li and maosong sun.
2007. scalable term se-lection for text categorization.
in proceedings of the2007 joint conference on empirical methods in nat-ural language processing and computational nat-ural language learning (emnlp-conll), pages774â€“782..xiaoya li, jingrong feng, yuxian meng, qinghonghan, fei wu, and jiwei li.
2019a.
a uniï¬ed mrcarxivframework for named entity recognition.
preprint arxiv:1910.11476..xiaoya li, yuxian meng, xiaofei sun, qinghong han,arianna yuan, and jiwei li.
2019b.
is word segmen-tation necessary for deep learning of chinese rep-in proceedings of the 57th annualresentations?
meeting of the association for computational lin-guistics, pages 3242â€“3252, florence, italy.
associa-tion for computational linguistics..yanran li, wenjie li, fei sun, and sujian li.
2015. component-enhanced chinese character em-beddings.
in proceedings of the 2015 conference onempirical methods in natural language processing,emnlp 2015, lisbon, portugal, september 17-21,2015, pages 829â€“834..frederick liu, han lu, chieh lo, and graham neu-big.
2017. learning character-level composition-in proceedings of theality with visual features.
55th annual meeting of the association for compu-tational linguistics, acl 2017, vancouver, canada,july 30 - august 4, volume 1: long papers, pages2059â€“2068..xiaodong liu, pengcheng he, weizhu chen, and jian-feng gao.
2019a.
multi-task deep neural networksfor natural language understanding.
in proceedingsof the 57th annual meeting of the association forcomputational linguistics, pages 4487â€“4496, flo-rence, italy.
association for computational linguis-tics..xin liu, qingcai chen, chong deng, huajun zeng,jing chen, dongfang li, and buzhou tang.
2018..lcqmc: a large-scale chinese question matchingin proceedings of the 27th internationalcorpus.
conference on computational linguistics, pages1952â€“1962..yinhan liu, myle ott, naman goyal, jingfei du, man-dar joshi, danqi chen, omer levy, mike lewis,luke zettlemoyer, and veselin stoyanov.
2019b.
roberta: a robustly optimized bert pretraining ap-proach.
arxiv preprint arxiv:1907.11692..yuxian meng, wei wu, fei wang, xiaoya li, ping nie,fan yin, muyu li, qinghong han, xiaofei sun, andjiwei li.
2019. glyce: glyph-vectors for chinesein advances in neuralcharacter representations.
information processing systems, volume 32, pages2746â€“2757.
curran associates, inc..tomas mikolov, kai chen, greg corrado, and jef-frey dean.
2013a.
efï¬cient estimation of wordarxiv preprintrepresentations in vector space.
arxiv:1301.3781..tomas mikolov, ilya sutskever, kai chen, greg s cor-rado, and jeff dean.
2013b.
distributed representa-tions of words and phrases and their compositional-ity.
advances in neural information processing sys-tems, 26:3111â€“3119..nanyun peng and mark dredze.
2015. named en-tity recognition for chinese social media with jointlyin proceedings of the 2015trained embeddings.
conference on empirical methods in natural lan-guage processing, pages 548â€“554..alec radford, jeffrey wu, rewon child, david luan,dario amodei, and ilya sutskever.
2019. languagemodels are unsupervised multitask learners.
openaiblog, 1(8)..nils reimers and iryna gurevych.
2019. sentence-bert: sentence embeddings using siamese bert-networks.
arxiv preprint arxiv:1908.10084..rico sennrich, barry haddow, and alexandra birch.
2016. neural machine translation of rare wordswith subword units.
in proceedings of the 54th an-nual meeting of the association for computationallinguistics (volume 1: long papers), pages 1715â€“1725, berlin, germany.
association for computa-tional linguistics..yan shao, christian hardmeier, jÃ¶rg tiedemann, andjoakim nivre.
2017. character-based joint segmen-tation and pos tagging for chinese using bidirec-tional rnn-crf.
in proceedings of the eighth interna-tional joint conference on natural language pro-cessing, ijcnlp 2017, taipei, taiwan, november27 - december 1, 2017 - volume 1: long papers,pages 173â€“183..xinlei shi, junjie zhai, xudong yang, zehua xie,and chao liu.
2015. radical embedding: delv-in proceedingsing deeper to chinese radicals.
of the 53rd annual meeting of the association forcomputational linguistics and the 7th international.
2074joint conference on natural language processing(volume 2: short papers), pages 594â€“598, beijing,china.
association for computational linguistics..and jeffrey dean.
2016. googleâ€™s neural machinetranslation system: bridging the gap between humanand machine translation..liang xu, hai hu, xuanwei zhang, lu li, chenjiecao, yudong li, yechen xu, kai sun, dian yu,cong yu, yin tian, qianqian dong, weitang liu,bo shi, yiming cui, junyi li, jun zeng, rongzhaowang, weijian xie, yanting li, yina patterson,zuoyu tian, yiwen zhang, he zhou, shaoweihualiu, zhe zhao, qipeng zhao, cong yue, xinruizhang, zhengliang yang, kyle richardson, andzhenzhong lan.
2020. clue: a chinese languagein proceed-understanding evaluation benchmark.
ings of the 28th international conference on com-putational linguistics, pages 4762â€“4772, barcelona,spain (online).
international committee on compu-tational linguistics..zhenyu xuan, rui bao, and shengyi jiang.
2020.fgn: fusion glyph network for chinese named entityrecognition..zhilin yang, zihang dai, yiming yang, jaime car-bonell, russ r salakhutdinov, and quoc v le.
2019.xlnet: generalized autoregressive pretraining forlanguage understanding.
in advances in neural in-formation processing systems, pages 5753â€“5763..rongchao yin, quan wang, peng li, rui li, and binwang.
2016. multi-granularity chinese word embed-in proceedings of the 2016 conference onding.
empirical methods in natural language processing,pages 981â€“986..xiang zhang and yann lecun.
2017. which en-coding is the best for text classiï¬cation in chinese,arxiv preprintenglish,arxiv:1708.02657..japanese and korean?.
zhengyan zhang, xu han, hao zhou, pei ke, yuxiangu, deming ye, yujia qin, yusheng su, haozheji, jian guan, fanchao qi, xiaozhi wang, yananzheng, guoyang zeng, huanqi cao, shengqi chen,daixuan li, zhenbo sun, zhiyuan liu, minliehuang, wentao han, jie tang, juanzi li, xiaoyanzhu, and maosong sun.
2020. cpm: a large-scalegenerative chinese pre-trained language model..jinhua zhu, yingce xia, lijun wu, di he, tao qin,wengang zhou, houqiang li, and tieyan liu.
2020.incorporating bert into neural machine translation.
in international conference on learning represen-tations..chan hee song and arijit sehanobish.
2020. usingchinese glyphs for named entity recognition.
pro-ceedings of the aaai conference on artiï¬cial intel-ligence, 34(10):13921â€“13922..kaitao song, xu tan, tao qin, jianfeng lu, and tie-yan liu.
2019. mass: masked sequence to se-quence pre-training for language generation.
in in-ternational conference on machine learning, pages5926â€“5936..tzu-ray su and hung-yi lee.
2017. learning chi-nese word representations from glyphs of charac-in proceedings of the 2017 conference onters.
empirical methods in natural language processing,emnlp 2017, copenhagen, denmark, september 9-11, 2017, pages 264â€“273..yaming sun, lei lin, nan yang, zhenzhou ji, andxiaolong wang.
2014. radical-enhanced chinesein international conferencecharacter embedding.
on neural information processing, pages 279â€“286.
springer..yu sun, shuohuan wang, yukun li, shikun feng, xuyichen, han zhang, xin tian, danxiang zhu, haotian, and hua wu.
2019. ernie: enhanced rep-resentation through knowledge integration.
arxivpreprint arxiv:1904.09223..yu sun, shuohuan wang, yukun li, shikun feng, haotian, hua wu, and haifeng wang.
2020. ernie 2.0:a continual pre-training framework for language un-derstanding.
proceedings of the aaai conferenceon artiï¬cial intelligence, 34(05):8968â€“8975..hanqing tao, shiwei tong, tong xu, qi liu, and en-hong chen.
2019. chinese embedding via strokeand glyph information: a dual-channel view..ashish vaswani, noam shazeer, niki parmar, jakobuszkoreit, llion jones, aidan n gomez, Å‚ukaszkaiser, and illia polosukhin.
2017. attention is allin advances in neural information pro-you need.
cessing systems, pages 5998â€“6008..ralph weischedel, sameer pradhan, lance ramshaw,martha palmer, nianwen xue, mitchell marcus,ann taylor, craig greenberg, eduard hovy, robertontonotes release 4.0.belvin, et al.
2011.ldc2011t03, philadelphia, penn.
: linguistic dataconsortium..yonghui wu, mike schuster, zhifeng chen, quoc v.le, mohammad norouzi, wolfgang macherey,maxim krikun, yuan cao, qin gao, klausmacherey, jeff klingner, apurva shah, melvin john-son, xiaobing liu, Å‚ukasz kaiser, stephan gouws,yoshikiyo kato, taku kudo, hideto kazawa, keithstevens, george kurian, nishant patil, wei wang,cliff young, jason smith, jason riesa, alex rud-nick, oriol vinyals, greg corrado, macduff hughes,.
2075