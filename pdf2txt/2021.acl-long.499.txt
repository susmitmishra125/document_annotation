long text generation by modeling sentence-level and discourse-levelcoherence.
jian guan1, xiaoxi mao2, changjie fan2, zitao liu3, wenbiao ding3and minlie huang1∗1the coai group, dcst; 1institute for artiﬁcial intelligence; 1state key lab of intelligent technology and systems;1beijing national research center for information science and technology;1tsinghua university, beijing 100084, china.
2netease fuxi ai lab.
3tal education group.
j-guan19@mails.tsinghua.edu.cn, {maoxiaoxi,fanchangjie}@corp.netease.com,zitao.jerry.liu@gmail.com, dingwenbiao@100tal.com, aihuang@tsinghua.edu.cn.
abstract.
generating long and coherent text is an im-portant but challenging task, particularly foropen-ended language generation tasks such asstory generation.
despite the success in mod-eling intra-sentence coherence, existing gen-eration models (e.g., bart) still struggle tomaintain a coherent event sequence through-out the generated text.
we conjecture that thisis because of the difﬁculty for the decoder tocapture the high-level semantics and discoursestructures in the context beyond token-levelin this paper, we propose aco-occurrence.
long text generation model, which can repre-sent the preﬁx sentences at sentence level anddiscourse level in the decoding process.
tothis end, we propose two pretraining objec-tives to learn the representations by predict-ing inter-sentence semantic similarity and dis-tinguishing between normal and shufﬂed sen-tence orders.
extensive experiments show thatour model can generate more coherent textsthan state-of-the-art baselines..1.introduction.
the ability to generate coherent long texts playsan important role in many natural language gen-eration (nlg) applications, particularly for open-ended language generation tasks such as story gen-eration, namely generating a reasonable story froma prompt or a leading context.
while existing gen-eration models (fan et al., 2018; radford et al.,2019) can generate texts with good intra-sentencecoherence, it is still difﬁcult to plan a coherent plotthroughout the text, even when using the powerfulpretrained models, as illustrated in figure 1..pretrained generation models have shown state-of-the-art performance on various nlg tasks suchas summarization and translation (radford et al.,2019; lewis et al., 2020).
however, such tasks.
∗corresponding author.
figure 1: story examples written by the ﬁne-tunedbart model (lewis et al., 2020) and a humanwriter given the same leading context from rocsto-ries (mostafazadeh et al., 2016).
the generated storyby bart suffers from severe incoherence issue in spiteof some related concepts (in bold).
in comparison, thehuman writer can write a coherent story because theyfully consider the context semantics and discourse rela-tions (e.g., the temporal order) among the sentences..provide sufﬁcient source information in the inputfor generating desired texts, while open-ended gen-eration tasks require expanding reasonable plotsfrom very limited input information (guan et al.,2020).
as exempliﬁed in figure 1, we observe se-vere issues of incoherence when applying bartfor story generation.
although bart performs rea-sonably well at generating some concepts relatedto the context (e.g., “basketball”, “player”), theyare used incoherently in the generated texts, whichis manifested in repetitive plots (e.g., the sentencesb and c), unrelated events (e.g., “played baseball.
proceedingsofthe59thannualmeetingoftheassociationforcomputationallinguisticsandthe11thinternationaljointconferenceonnaturallanguageprocessing,pages6379–6393august1–6,2021.©2021associationforcomputationallinguistics6379🤔humanwriter:a:hethoughtthatwouldchangeifhejoinedasport.b:hetriedoutforseveralteams.c:he didn’tmakethecutforanyofthem.d:hedecidedtotrainonhisowninstead.💪🤔🤔🤔🤔temporalorderleadingcontext:timwasn’tveryathletic.😵bart:a:hewasn’tgoodatbasketball.b:hewasapartofthebasketballteam.c:timwasofferedajobatalocalbasketballteam.d:timplayedbaseballbetterinthecity.🤼😔🏋temporalordertemporalorderbetter”) and conﬂicting logic (e.g., “not good atbasketball” but “in the basketball team”).
theseissues are also commonly observed in other nlgmodels (holtzman et al., 2020; guan and huang,2020).
we argue that existing models are rarelytrained beyond the token-level co-occurrence, andtherefore they can easily generate related conceptsbut do not arrange them reasonably.
in contrast,human writers always ﬁrst fully understand the se-mantics (e.g., some key events such as “try out”,“not make the cut”) and the discourse relations (e.g.,temporal orders) among the already written sen-tences before deciding the following content.
inthis way, the writers can write coherent stories evenwith few related concepts, as shown in figure 1.therefore, it is important for subsequent generationto capture high-level features in the context..in this paper, we propose hint, a generationmodel equipped with high-level representationsfor long text generation.
typical generative mod-els usually train a left-to-right decoder by next wordprediction based on the attention to all the preﬁxwords.
in order to encourage the model to capturehigh-level features, we extend the decoder to rep-resent the preﬁx information at sentence level anddiscourse level, respectively, with special tokenswhich are inserted at the end of each sentence.
toeffectively learn the representations, we proposetwo pretraining objectives including: (a) seman-tic similarity prediction, which requires predictingthe inter-sentence similarity using the sentence-level representation, with the powerful sentenceunderstanding model sentencebert (reimers andgurevych, 2019) as the teacher model; and (b)sentence order discrimination, which requires dis-tinguishing between the normal and shufﬂed sen-tence orders using the discourse-level representa-tion.
the objectives are designed to help the de-coder capture the semantics and discourse structureof the preﬁx, which can beneﬁt modeling the long-range coherence when generating long texts.
wesummarize our contributions in two folds:.
i. we propose a generation model named hint forlong text generation.
hint derives high-level repre-sentations for each decoded sentence to model thelong-range coherence.
we adopt two pretrainingobjectives called similarity prediction and order dis-crimination to learn the representations at sentencelevel and discourse level, respectively..ii.
we conduct extensive experiments on common-sense story and ﬁction generation tasks.
results.
show that hint can learn meaningful high-levelrepresentations and generate more coherent longtexts than baselines.1.
2 related works.
long text generation recent studies tackle theincoherence problem in long text generation fromthe following perspectives.
li et al.
(2015) adopteda hierarchical rnn-based decoder to learn the sen-tence representation but without any external super-vision.
shao et al.
(2017) proposed a self-attentionmechanism to attend on the preﬁx by appendingit to the rnn-based encoder, which is a similaridea with the vanilla transformer (vaswani et al.,2017).
however, the token-level self-attentionmechanism still struggles to model high-level de-pendency in the context.
recent works proposedseveral multi-step generation models (fan et al.,2018; yao et al., 2019; shao et al., 2019; tan et al.,2020; goldfarb-tarrant et al., 2020), which ﬁrstplan high-level sketches and then generate textsfrom the sketches.
however, the lack of exposureto degenerate sketches may impair the generationperformance since the models are only trained onsketches constructed from golden truth texts (tanet al., 2020).
another line is to incorporate externalknowledge into generation especially for common-sense story generation (guan et al., 2020; xu et al.,2020).
however, the methods may not be alwayseffective for other types of generation tasks.
guanet al.
(2020) also required the decoder to distinguishtrue texts from negative samples to alleviate poten-tial issues such as repetition.
but the classiﬁcationobjective does not provide explicit guidance forgeneration at each step.
therefore, the coherenceof language generation is still an open problem..high-level language representation signiﬁ-cant advances have been witnessed in many nlptasks with pretrained contextualized representa-tion (peters et al., 2018; devlin et al., 2019).
how-ever, most models were limited on token-levelrepresentation learning, which is not enough forcapturing the hierarchical structure of natural lan-guage texts (ribeiro et al., 2020).
several workshave tried to learn high-level representation.
skip-thought vectors (kiros et al., 2015) learned to en-code a sentence by reconstructing its neighboringsentences.
hlstm (yang et al., 2016) considered a.
1the codes are available at https://github.com/.
thu-coai/hint.
6380figure 2: model overview of hint, which is pretrained to predict the next token (task 1), predict inter-sentencesemantic similarity with the sentence-level representations (task 2), and distinguish between normal and shuf-ﬂed sentence orders with the discourse-level representations (task 3) based on the human-written texts and auto-constructed negative samples..hierarchical lstm-based encoder to learn the con-textualized sentence representation by downstreamclassiﬁcation.
hibert (zhang et al., 2019) incor-porated the hierarchical architecture to bert (de-vlin et al., 2019) and learned sentence representa-tion by recovering masked sentences.
sentence-bert (reimers and gurevych, 2019) derived sen-tence representation by ﬁne-tuning bert for nat-ural language inference.
conpono (iter et al.,2020) and slm (lee et al., 2020) further trainedbert to understand relations among sentencesat discourse level by distance prediction and sen-tence unshufﬂing, respectively.
however, all thesemodels focused on enhancing the representationof encoders for language understanding, while im-proving decoders by high-level representation forlong text generation is yet to be well investigated..3 methodology.
3.1 task deﬁnition and model overview.
our task can be deﬁned as follows: given an in-put x = (x1, x2, · · · , xm) (e.g., a beginning ora prompt), the model should generate a multi-sentence text y = (y1, y2, · · · , yn) with a coherentplot (each xi or yi is a token).
to tackle the prob-lem, the conventional generation models such asbart commonly employ a bidirectional encoderand a left-to-right decoder to minimize the negative.
log-likelihood llm of human-written texts:.
llm = −.
log p (yt|y<t, x),.
(1).
n(cid:88).
t=1.
p (yt|y<t, x) = softmax(htw + b),ht = decoder(y<t, {si}mi=1 = encoder(x),.
{si}m.i=1),.
(2).
(3).
(4).
where ht is the decoder’s hidden state at the t-thposition computed from the context (i.e., the preﬁxy<t and the input x), and si is the contextualizedrepresentation of xi acquired from the encoder, wand b are trainable parameters..however, as aforementioned, the models oftengenerate incoherent texts due to the decoder’s in-ability to capture high-level features of the preﬁxsentences.
therefore, we extend the decoder withhigh-level representations to gather the preﬁx in-formation.
speciﬁcally, we split the human-writtentexts into sequential sentences and add special to-kens at the end of each sentence, which will be usedto aggregate their respective semantics and their dis-course relations with one another during decoding.
to this end, we devise two pretraining tasks besidesthe standard language modeling objective, includ-ing similarity prediction and order discriminationto learn the sentence-level and discourse-level rep-resentations, respectively, as figure 2 shows.
al-though we only consider sentence as segments inthis work, our method can be easily extended toother syntactic levels such as phrases or paragraphs..6381hintdecoderabsendiscdsendisef𝐇𝟏𝐬𝐇𝟏𝐝𝐇𝟐𝐬𝐇𝟐𝐝𝐇𝟑𝐬𝐇𝟏𝐬𝐇𝟐𝐬𝐇𝟏𝐝𝐇𝟐𝐝𝐇𝟑𝐬𝐇𝟑𝐝sendis𝐇𝟑𝐝sentence-levelrepresentationsdiscourse-levelrepresentationstask2:similaritypredictiontask3:orderdiscriminationhuman-writtentextsnegativesamplessentence1sentence2sentence3feedforwardallhiddenrepresentationstask1:languagemodeling......bseneoshintencoderperturbation3.2 sentence-level representation.
3.3 discourse-level representation.
assume that the target text y consists of k sen-tences, denoted from y1 to yk (e.g., ab and cdin figure 2).
we insert a special sentence token,(cid:104)sen(cid:105), at the end of every sentence in y , which isdesigned to aggregate the semantics of each sen-tence.
let hsk (1 (cid:54) k (cid:54) k) denote the decoder’shidden state at the position where the k-th sentencetoken is the golden truth for next token prediction.
we expect hsk to be a meaningful sentence repre-sentation for yk, which means semantically similarsentences have close representations in the vec-tor space.
since sentence representation has beenwell studied for language understanding with manypowerful models such as sentencebert (reimersand gurevych, 2019), we propose to directly trans-fer their semantic knowledge for our sentence rep-resentation learning.
speciﬁcally, we require thehint decoder to predict the similarity of any twosentences yi and yj only using the correspondingsentence representations hsj , with the sen-tencebert similarity as the golden truth2.
we donot directly learn the sentencebert representationfor each sentence but the similarity score to avoidthe discrepancy between different model bias.
fur-thermore, to alleviate the innate bias of sentence-bert, we do not enforce hint to exactly ﬁt thegolden similarity.
instead, it would be enough thatthe difference between the predicted score and thegolden similarity is less than a margin ∆ ∈ [0, 1].
formally, the loss function lsen for the similarityprediction task can be derived as follows:.
i and hs.
max(|pij − tij|, ∆),.
(5).
lsen =.
1k2.
k(cid:88).
k(cid:88).
j=1.
i=1pij = sigmoid(sij + sji),sij = (hs.
i )tw shsj ,.
(6).
(7).
where tij is the golden similarity, pij is the pre-dicted similarity score, sij is an intermediate vari-able to guarantee pij is symmetric with respectto i and j, w s is a trainable parameter to trans-form the representation space of hint to that ofsentencebert.
the task explicitly exerts externalsupervision to learn the sentence-level representa-tion, enhancing the ability of the hint decoder tofully understand the semantics of preﬁx sentences..2the sentencebert similarity is computed as the cosinedistance of two sentence embeddings which are derived by ap-plying mean-pooling on the output vectors of sentencebert.
and we normalize the results to [0, 1] range by linear scaling..in analogy to the sentence-level representationlearning, we also insert a special discourse token,(cid:104)dis(cid:105), after every sentence and the correspondingsentence token to gather the discourse informationbetween different sentences.
let hdk (1 (cid:54) k (cid:54) k)denote the decoder’s hidden state at the positionwhere the k-th discourse token is the golden truthto be predicted.
hdk should be a meaningful repre-sentation which can be used to derive discourse re-lations with others (e.g., the k-th sentence precedesanother one in terms of the temporal order).
previ-ous work has shown that reconstructing the correctorder from shufﬂed sentences helps understand thediscourse relations (lee et al., 2020).
however,the unshufﬂing task is not directly applicable fornlg since the decoder should learn to dynami-cally model the discourse structure in the decodingprocess rather than wait until ﬁnishing decodingthe whole text.
therefore, we propose to learnthe discourse-level representation in a pair-wisemanner by discriminating whether the order of twosentences is correct.
formally, we minimize thecross-entropy loss ldis as follows:.
ldis =.
2k(k − 1).
k(cid:88).
k(cid:88).
lij,.
i=1.
j>ilij = −oijlogqij − (1 − oij)log(1 − qij) (9)qij = sigmoid (cid:0)(hdi )tw dhdj.
(10).
(cid:1) ,.
(8).
where oij is the golden label (1 if yi should precedeyj, 0 otherwise), qij is the predicted discriminationscore, and w d is a trainable parameter.
comparedwith the sentence-level representation hsk whichaggregates the semantics of a single sentence, thediscourse-level representation hdk focuses more onthe relationship with other sentences, thereby im-proving hint’s ability to capture the high-levelfeatures in both content and order..3.4 pretraining and fine-tuning.
to learn the high-level representations more ef-fectively, we propose to augment the training cor-pus by automatically constructing negative sam-ples from the human-written texts for pretraining.
speciﬁcally, for the order discrimination task, werandomly shufﬂe the sentences in human-writtentexts as negative samples.
and for the similarityprediction task, besides the negative samples withshufﬂed sentences, we also randomly repeat a sen-tence, or substitute a sentence with another from.
6382other texts as negative samples.
we expect the neg-ative samples to help enhance the generalizationability of hint during ﬁne-tuning or inference.
insummary, the overall loss function lp re for pre-training is computed as follows:.
lp re = llm + λ1ldis + λ2lsen,.
(11).
where we optimize the language modeling objec-tive llm only on the human-written texts, ldis onthe human-written texts and the negative sampleswith shufﬂed sentences, and lsen on all the human-written texts and the negative samples.
λ1 and λ2are adjustable scale factors.
by pretraining withthe proposed two objectives, the decoder can bettercapture the semantics and discourse structures inthe context.
and during ﬁne-tuning, we train hintonly with the language modeling objective..4 experiments.
4.1.implementation and pretraining dataset.
since our approach can adapt to all the genera-tion models with auto-regressive decoders (e.g.,gpt-2 (radford et al., 2019), unilm (dong et al.,2019), etc.
), we use bart as the base frameworkof hint, which has been shown to have strongperformance for long text generation (goldfarb-tarrant et al., 2020).
and we also provide theperformance of gpt-2 widely used in the litera-ture.
due to the limited computational resources,we follow bartbase’s hyper-parameters and uti-lize the public pretrained checkpoint to initializehint.
the batch size is set to 10 and the maximumsequence length is set to 512 for both the encoderand the decoder.
the margin ∆ in equation 5 is setto 0.1 and we present the results with other settingsof ∆ in the appendix.
both the scale factors λ1 andλ2 in equation 11 are set to 0.1..we adopt bookcorpus (zhu et al., 2015) as ourpretraining dataset and split each text to sentencesusing nltk (bird and loper, 2004).
we createthe training texts by taking a sentence as the inputand the following ten sentences as the target output.
besides, we construct the same number of nega-tive samples with the human-written texts.
andit is evenly possible for a negative sample to berepeated, substituted or shufﬂed.
we pretrain hinton bookcorpus for 0.1m steps..4.2 fine-tuning setting.
we evaluate hint on rocstories (roc forshort) (mostafazadeh et al., 2016) and writing-.
prompts (wp for short) (fan et al., 2018).
roccontains 98,162 ﬁve-sentence commonsense sto-ries.
we follow guan et al.
(2020) to delexicalizestories in roc by masking all the names with spe-cial placeholders to achieve better generalization.
wp originally contains 303,358 stories paired withwriting prompts, which are usually unconstrainedon writing topics.
considering that using too manyexamples for ﬁne-tuning may weaken the inﬂuenceof post-training, we randomly selected stories fromthe original validation set and test set of wp forthe subsequent experiments.
we regard the ﬁrstsentence and the prompt as the input to generate atext for roc and wp, respectively.
and we onlyretain the ﬁrst ten sentences (split using nltk)of the texts in wp for ﬁne-tuning.
we presentmore details in table 1. the batch size is set to10/4 for roc/wp, respectively.
and other hyper-parameters are the same as the pretraining phase..dataset.
input output.
train.
val.
test.
rocwp.
14.4730.02.
56.29185.65.
88,34426,758.
4,9082,000.
4,9092,000.table 1: the average number of tokens in the input andoutput in the whole dataset, and the numbers of storiesfor training/validation/test..4.3 baselines.
we compared hint with the following baselines:seq2seq: it generates a text conditioned upon theinput.
for better performance, we implement thebaseline by training bart from scratch on thedownstream datasets without pretraining.
plan&write: it ﬁrst plans a keyword sequenceconditioned upon the input; and then generates atext based on the keywords (yao et al., 2019).
weimplement the model based on the codes providedby the original paper.
gpt-2 and bart: they are ﬁne-tuned on thedownstream datasets with the language modelingobjective.
bart-post: it is ﬁrst post-trained on the pretrain-ing dataset with the original pretraining objectivesof bart (text inﬁlling and sentence permutation)for the same number of steps with hint; and thenﬁne-tuned on the downstream datasets with the lan-guage modeling objective.
bart-mtl: the model is trained by ﬁne-tuningbart on the downstream datasets with multi-tasklearning (mtl), including the language model-.
6383ing objective and an auxiliary multi-label classi-ﬁcation objective (guan et al., 2020), which re-quires distinguishing human-written texts fromauto-constructed negative samples..furthermore, we conduct ablation tests by re-moving the proposed components respectively toinvestigate the inﬂuence of each component.
be-sides, we also demonstrate the adaption of ourapproach to general language generation modelsby directly ﬁne-tuning bart and hint on down-stream datasets with the proposed two objectivesas auxiliary tasks.
for fair comparison, we set allthe pretrained models to the base version.
and wealso insert the sentence token and discourse tokeninto each training text for all the baselines..we generate texts using nucleus sampling (holtz-man et al., 2020) with p=0.9 and a softmax temper-ature of 0.7 (goodfellow et al., 2016) to balance thetrade-off between diversity and ﬂuency.
and we setthe probability of generating (cid:104)dis(cid:105) to 1 if the lasttoken is (cid:104)sen(cid:105) to ensure that hint can obtain thehigh-level representations for each sentence.
andduring evaluation, we remove the special tokens inthe generated texts.
we apply these settings to allthe baselines..4.4 automatic evaluation.
evaluation metrics we adopt the following au-tomatic metrics to evaluate the performance on thetest sets: (1) perplexity (ppl): smaller perplexityscores indicate better ﬂuency in general.
we donot count the probability values at the positionswhere the sentence or discourse token is the goldentruth.
(2) bleu (b-n): we use n = 1, 2 to eval-uate n-gram overlap between generated texts andhuman-written texts (papineni et al., 2002).
(3)lexical repetition (lr-n): the metric computesthe percentage of those texts which repeat a 4-gramat least n times in all the generated texts (shaoet al., 2019).
we set n = 2 for roc and n = 5 forwp.
(4) semantic repetition (sr-n): the metricﬁrst computes the average top-n sentencebertsimilarity between any two sentences in each gen-erated text, and then averages the results as the ﬁnalscore.
we set n = 1 for roc and n = 10 for wp.
(5) distinct-4 (d-4) (li et al., 2016): we adoptdistinct-4, the ratio of distinct 4-grams to all thegenerated 4-grams, to measure the generation di-versity.
(6) context relatedness: it is a learnableautomatic metric (guan and huang, 2020).
first,we train a classiﬁer with robertabase (liu et al.,.
2019) to distinguish human-written texts and neg-ative samples constructed by substituting words,phrases and sentences of human-written texts ran-domly.
then, we use the average classiﬁer scoreof all the generated texts to measure the contextrelatedness.
(7) sentence orders: in analogy torelatedness measurement, we train another classi-ﬁer to distinguish human-written texts and negativesamples where sentences are randomly shufﬂed.
we use the average classiﬁer score to measure sen-tence orders.
we train the last two metrics basedon the training sets of the downstream datasets..results on roc we show the results on roc intable 2. we do not provide the perplexity scores ofplan&write and gpt-2 since they do not tokenizetexts with the same vocabulary as used in bart.
hint outperforms all the baselines in terms of per-plexity, indicating the better ability to model thetexts in the test set.
and hint can generate moreword overlaps with reference texts as shown by bet-ter bleu scores.
it is accordant with the previousobservation (xu et al., 2020) that plan&write hasless lexical repetition than pretraining models pos-sibly because small models are better at learningshort term statistics (e.g., n-gram) but not long termdependencies.
however, hint improves the situa-tion compared with gpt-2 and bart, and has lesssemantic repetition than all the baselines, indicat-ing the better ability of hint to capture semanticfeatures.
besides, our approach does no harm tothe generation diversity.
hint also outperformsbaseline models in generating related events andarranging a proper order, as shown by the higherrelatedness and order scores.
furthermore, ﬁne-tuning with the proposed objectives as auxiliarytasks can further reduce the lexical and semanticrepetition, and improve the relatedness and orderscores for both bart and hint, suggesting thegeneral beneﬁt of modeling the long-range coher-ence at sentence level and discourse level..besides, the ablation test shows that the sentence-level and discourse-level representations are rel-atively more important to enhance the ability togenerate texts with related events and reasonableorders, respectively.
and both of them contribute toreducing semantic redundancy.
when post-trainingonly with the language modeling objective, almostall the metrics drops substantially, indicating theimportance to model high-level coherence..furthermore, we also notice that some mod-els achieve even higher relatedness score than the.
6384models.
ppl↓.
b-1↑.
b-2↑.
lr-2↓.
sr-1↓ d-4↑ relatedness↑ order↑.
seq2seqplan&writegpt-2bartbart-mtlbart-post.
hintw/o senw/o disw/o sen&dis.
bart w/ auxhint w/ aux.
golden text.
18.14n/an/a9.839.689.49.
9.209.259.249.45.
9.509.22.n/a.
0.3020.2970.3050.3070.3120.326.
0.3340.3320.3290.324.
0.3230.335.
0.1300.1300.1310.1330.1370.147.
0.1540.1520.1500.146.
0.1450.153.
0.2800.2010.3310.3070.2710.279.
0.2530.2640.2480.277.
0.2430.232.
0.058.
0.6260.6280.6360.6350.6290.632.
0.6190.6220.6210.634.
0.6140.615.
0.6630.6770.6840.6990.6830.698.
0.6930.7020.6940.686.
0.7100.700.n/a.
n/a.
0.531.
0.891.
0.8410.9150.9190.9160.9450.947.
0.9870.9700.9780.937.
0.9680.989.
0.970.
0.6850.8010.8130.8160.8200.842.
0.8820.8730.8640.847.
0.8370.892.
0.903.table 2: automatic evaluation results on roc.
↓ / ↑ means the lower/higher the better.
the best performance ishighlighted in bold.
w/o sen and w/o dis means ablating the sentence-level and discourse-level representationlearning, respectively.
namely, w/o sen&dis means post-training only with the language modeling objective.
bart w/ aux and hint w/ aux means fine-tuning bart and hint on the downstream dataset with the proposedobjectives as auxiliary tasks, respectively..golden texts.
we summarize the possible reasonsas follows: (a) it is still difﬁcult for the learned clas-siﬁer to judge implicit relatedness in some goldentexts, which may require a strong reasoning ability.
(b) there exist some noisy texts with poor related-ness in the golden texts.
and (c) the systems tendto generate a limited set of texts (as demonstratedby much lower distinct-4 than golden texts) withgeneric plots (guan et al., 2020), which may gethigh relatedness scores easily.
however, we believethe learnable metric is still meaningful to comparedifferent models with similar diversity regardingthe context relatedness..results on wp we present the results on wp intable 3. we use a larger n to compute the lexi-cal/semantic repetition since we ﬁnd that all themodels tend to repeat similar texts easily when gen-erating texts with hundreds of words.
and we donot provide the relatedness and order scores be-cause it is difﬁcult to train satisfactory classiﬁers todistinguish human-written texts from negative sam-ples well.
table 3 shows that hint outperformsbaselines except for lexical repetition, which is ac-cordant with the results on roc.
therefore, thehigh-level representations are effective for generat-ing long texts with different lengths and domains..4.5 manual evaluation.
for manual evaluation, we conduct pair-wise com-parisons with two strong baseline models (bartand bart-post), and three ablated models of hint.
we randomly sample 200 texts from the test set of.
models.
ppl↓.
b-1↑.
b-2↑.
lr-5↓.
sr-10↓.
d-4↑.
seq2seqplan&writegpt-2bartbart-mtlbart-post.
hintw/o senw/o disw/o sen&dis.
129.51n/an/a34.4235.7135.11.
32.7333.0833.1833.71.
0.1650.1990.2000.2050.1980.205.
0.2240.2160.2230.207.
0.0700.0700.0730.0750.0760.076.
0.0840.0800.0830.076.
0.6230.5240.6550.6200.6540.671.
0.5670.5980.5880.610.golden text.
n/a.
n/a.
n/a.
0.007.table 3: automatic evaluation results on wp..models.
hint vs. barthint vs. bart-post.
hint vs. hint w/o senhint vs. hint w/o dishint vs. hint w/o sen&dis.
37.033.035.5.models.
hint vs. barthint vs. bart-post.
hint vs. hint w/o senhint vs. hint w/o dishint vs. hint w/o sen&dis.
fluencylose.
tie.
coherencelose.
tie.
win.
37.5∗35.5∗∗.
win.
54.5∗∗47.5∗∗.
47.5∗∗42.0∗55.5∗∗.
24.021.0.
31.025.528.5.
11.021.5.
23.028.024.0.
0.8190.8510.8830.8540.8460.862.
0.8050.8230.8180.845.
0.448.
38.543.5.
32.041.536.0.
34.531.0.
29.530.020.5.
0.2830.2720.2870.2910.3050.271.
0.3130.3030.3070.280.
0.928.κ.
0.580.63.
0.680.620.60.κ.
0.590.62.
0.670.630.58.table 4: manual evaluation results on roc.
the scoresindicate the percentages (%) of win, lose or tie whencomparing hint with a baseline.
κ denotes fleiss’kappa (fleiss and joseph, 1971) to measure the inter-annotator agreement (all are moderate or substantial).
the scores marked with ∗ and ∗∗ mean hint outper-forms the baseline signiﬁcantly with p-value<0.05 andp-value<0.01 (sign test), respectively..roc3 and obtain 1,200 texts from the six models.
3we do not conduct manual evaluation on wp since itwould be hard to obtain acceptable annotation agreement for.
6385coherent examples ↓.
caus.
temp.
rept.
incoherent examples ↑negrel.
caus.
aspects.
number.
bartbart-post.
hintw/o senw/o disw/o sen&dis.
rel.
563.
11.9111.4610.90∗∗11.00∗10.97∗11.41.neg.
455.
9.158.868.50∗8.55∗8.52∗8.84.
476.
2,376.
3,235.
3,324.
3,664.
394.
10.5610.219.68∗9.75∗9.8710.16.
10.299.949.50∗∗9.53∗∗9.61∗∗9.89.
14.1114.0614.74∗∗14.0414.64∗∗13.80.
15.6015.4516.32∗∗15.4316.18∗15.14.
13.6913.3513.96∗13.2913.83∗13.17.
13.4713.31.
13.6813.5913.1413.04.temp.
1,795.
13.0412.72.
13.1513.0412.5712.51.table 5: perplexity scores on the coherent or incoherent examples within different aspects including semantic rep-etition (rept), relatedness (rel), negation (neg), causal relationship (caus) and temporal relationship (temp).
number means the number of the corresponding test examples.
↓ / ↑ means the lower/higher perplexity the bet-ter.
the best performance is highlighted in bold.
∗ and ∗∗ indicate that the corresponding model signiﬁcantlyoutperforms bart with p-value<0.05 and p-value<0.01 (t-test), respectively..for each pair of texts (one by our model and theother by a baseline, along with the input), three an-notators are hired to give a preference (win, lose, ortie) in terms of ﬂuency and coherence, respectively.
we adopt majority voting to make ﬁnal decisionsamong the three annotators.
we resort to amazonmechanical turk (amt) for annotation.
we followxu et al.
(2020) to deﬁne ﬂuency as a measure ofintra-sentence linguistic quality and grammaticalcorrectness, and coherence as inter-sentence relat-edness, causal and temporal dependencies.
notethat the two aspects are independently evaluated.
besides, we control the annotation quality by ﬁl-tering out those annotations where the annotatorcan not make reasonable judgments when compar-ing a human-written text with a negative sample.
furthermore, we also ask workers to annotate thespeciﬁc errors in the generated texts.
we showthe annotation instruction and the error analysis ofdifferent models in the appendix..table 4 shows the manual evaluation results.
allthe results show moderate inter-annotator agree-ment (0.4(cid:54) κ (cid:54)0.6) or substantial agreement (0.6(cid:54) κ (cid:54)0.8).
and we can see that hint performssigniﬁcantly better than baselines in coherence bycapturing the high-level features, and has compara-ble ﬂuency with baselines..4.6 language modeling.
it is still necessary to further investigate whetherthe learned representations help hint capture thehigh-level coherence better.
therefore, we proposeto evaluate the models using individual languagemodeling tests in different aspects (ribeiro et al.,2020).
to this end, we construct coherent and inco-.
too long texts..herent examples based on the test set of roc, andcompute perplexity on the examples of differentaspects.
speciﬁcally, we focus on the followingaspects: semantic repetition, relatedness, negation,causal and temporal relationship.
we select human-written texts as coherent examples and constructincoherent examples by perturbing human-writtentexts.
for example, we select those texts with time-related words (e.g., “then”) as coherent examplesfor testing in the temporal relationship.
and weexchange two sequential events connected by “then”of a human-written text or substitute “before” with“after” as incoherent examples of the aspect.
weshow more details in the appendix..we present the results in table 5. hint canmodel the context coherence better in the aboveaspects than baseline models (lower perplexity onthe coherent examples), and recognize the inco-herent errors more effectively (higher perplexityon the incoherent examples).
by contrast, bothbart-post and hint (w/o sen&dis) achievean overall drop of perplexity compared withbart even on the negative examples, indicat-ing that they may still focus on capturing thetoken-level features.
as for the ablation study,we can see that the sentence-level representationenhances the ability of hint to capture the relat-edness, negation and semantic repetition, whilethe discourse-level representation works mainly forcausal and temporal relationship.
however, wealso notice the insigniﬁcant improvement of hintcompared with bart in recognizing the unreason-able causal and temporal relationship, which mayrequire injecting explicit inferential knowledge be-sides learning sentence orders..63864.7 case study.
we present several cases in the appendix to demon-strate that hint can derive meaningful sentence-level and discourse-level representations, and gen-erate texts with better coherence than baselineswith the help of the representations..annotators in the annotation process.
we hiredthree annotators and payed each annotator $0.05for comparing each pair of stories.
the payment isreasonable considering that it would cost average30 seconds for an annotator to ﬁnish a comparison..5 conclusion.
references.
we present hint, a generation model for ation,which can represent the preﬁx information at sen-tence level and discourse level in the decodingprocess.
we propose two pretraining objectivesincluding inter-sentence similarity prediction andsentence order discrimination to learn the sentence-level and discourse-level representations, respec-tively.
extensive experiments demonstrate thathint can generate more coherent texts with relatedcontext and proper sentence orders than strongbaselines.
further analysis shows that hint hasbetter ability of language modeling thanks to abilityof modeling high-level coherence..acknowledgments.
this work was supported by national keyr&d program of china, under grant no.
2020aaa0104500.
this work was jointly sup-ported by the nsfc projects (key project with no.
61936010 and regular project with no.
61876096),and the guoqiang institute of tsinghua university,with grant no.
2019gqg1 and 2020gqg0005.
we would also like to thank the anonymous review-ers for their invaluable suggestions and feedback..ethics statement.
we conduct the experiments based on two existingpublic datasets rocstories and writingprompts,which are widely used for commonsense story gen-eration and ﬁction generation tasks, respectively.
automatic and manual evaluation show that ourmodel outperforms existing state-of-the-art modelson both datasets, suggesting the generalization ofour model to different domains.
besides, our ap-proach can be easily extended to different syntacticlevels (e.g., phrase-level, paragraph-level), differ-ent model architectures (e.g., gpt, unilm) anddifferent generation tasks (e.g., dialog generation,essay generation)..we resorted to amazon mechanical turk (amt)for manual evaluation.
we did not ask about per-sonal privacy or collect personal information of.
steven bird and edward loper.
2004. nltk: the natu-ral language toolkit.
in proceedings of the 42nd an-nual meeting of the association for computationallinguistics, barcelona, spain, july 21-26, 2004 -poster and demonstration.
acl..jacob devlin, ming-wei chang, kenton lee, andkristina toutanova.
2019. bert: pre-training ofdeep bidirectional transformers for language under-standing.
in proceedings of the 2019 conference ofthe north american chapter of the association forcomputational linguistics: human language tech-nologies, volume 1 (long and short papers), pages4171–4186..li dong, nan yang, wenhui wang, furu wei, xi-aodong liu, yu wang, jianfeng gao, ming zhou,and hsiao-wuen hon.
2019. uniﬁed languagemodel pre-training for natural language understand-in advances in neural infor-ing and generation.
mation processing systems 32: annual conferenceon neural information processing systems 2019,neurips 2019, december 8-14, 2019, vancouver,bc, canada, pages 13042–13054..angela fan, mike lewis, and yann dauphin.
2018. hi-in proceedingserarchical neural story generation.
of the 56th annual meeting of the association forcomputational linguistics (volume 1: long papers),pages 889–898..fleiss and l. joseph.
1971. measuring nominal scaleagreement among many raters.
psychological bul-letin, 76(5):378–382..seraphina goldfarb-tarrant, tuhin chakrabarty, ralphweischedel, and nanyun peng.
2020. content plan-ning for neural story generation with aristotelianin proceedings of the 2020 conferencerescoring.
on empirical methods in natural language process-ing (emnlp), pages 4319–4338..seraphina goldfarb-tarrant,.
tuhin chakrabarty,ralph m. weischedel, and nanyun peng.
2020.content planning for neural story generation within proceedings of the 2020aristotelian rescoring.
conference on empirical methods in natural lan-guage processing, emnlp 2020, online, november16-20, 2020, pages 4319–4338.
association forcomputational linguistics..ian goodfellow, yoshua bengio, and aaron courville..2016. deep learning.
mit press..6387jian guan, fei huang, zhihao zhao, xiaoyan zhu, andminlie huang.
2020. a knowledge-enhanced pre-training model for commonsense story generation.
transactions of the association for computationallinguistics, 8:93–108..jian guan and minlie huang.
2020. union: an un-referenced metric for evaluating open-ended storygeneration.
in proceedings of the 2020 conferenceon empirical methods in natural language process-ing, emnlp 2020, online, november 16-20, 2020,pages 9157–9166.
association for computationallinguistics..ari holtzman, jan buys, li du, maxwell forbes, andyejin choi.
2020. the curious case of neural text de-in international conference on learn-generation.
ing representations..dan iter, kelvin guu, larry lansing, and dan jurafsky.
2020. pretraining with contrastive sentence objec-tives improves discourse performance of languagein proceedings of the 58th annual meet-models.
ing of the association for computational linguistics,acl 2020, online, july 5-10, 2020, pages 4859–4870. association for computational linguistics..ryan kiros, yukun zhu, russ r salakhutdinov,richard zemel, raquel urtasun, antonio torralba,and sanja fidler.
2015. skip-thought vectors.
ad-vances in neural information processing systems,28:3294–3302..haejun lee, drew a hudson, kangwook lee, andchristopher d manning.
2020. slm: learning adiscourse language representation with sentence un-shufﬂing.
in proceedings of the 2020 conference onempirical methods in natural language processing(emnlp), pages 1551–1562..mike lewis, yinhan liu, naman goyal, mar-jan ghazvininejad, abdelrahman mohamed, omerlevy, veselin stoyanov, and luke zettlemoyer.
2020. bart: denoising sequence-to-sequence pre-training for natural language generation, translation,and comprehension.
in proceedings of the 58th an-nual meeting of the association for computationallinguistics, acl 2020, online, july 5-10, 2020,pages 7871–7880.
association for computationallinguistics..jiwei li, michel galley, chris brockett, jianfeng gao,and bill dolan.
2016. a diversity-promoting ob-jective function for neural conversation models.
innaacl hlt 2016, the 2016 conference of thenorth american chapter of the association for com-putational linguistics: human language technolo-gies, san diego california, usa, june 12-17, 2016,pages 110–119.
the association for computationallinguistics..jiwei li, minh-thang luong, and dan jurafsky.
2015.a hierarchical neural autoencoder for paragraphsand documents.
in proceedings of the 53rd annualmeeting of the association for computational lin-guistics and the 7th international joint conference.
on natural language processing (volume 1: longpapers), pages 1106–1115..yinhan liu, myle ott, naman goyal, jingfei du, man-dar joshi, danqi chen, omer levy, mike lewis,luke zettlemoyer, and veselin stoyanov.
2019.roberta: a robustly optimized bert pretraining ap-proach.
corr, abs/1907.11692..john morris, eli liﬂand, jin yong yoo, jake grigsby,di jin, and yanjun qi.
2020. textattack: a frame-work for adversarial attacks, data augmentation, andin emnlp: systemadversarial training in nlp.
demonstrations, pages 119–126..nasrin mostafazadeh, nathanael chambers, xiaodonghe, devi parikh, dhruv batra, lucy vanderwende,pushmeet kohli, and james allen.
2016. a cor-pus and cloze evaluation for deeper understandingof commonsense stories.
in proceedings of naacl-hlt, pages 839–849..kishore papineni, salim roukos, todd ward, and wei-jing zhu.
2002. bleu: a method for automatic eval-in proceedings ofuation of machine translation.
the 40th annual meeting on association for compu-tational linguistics, pages 311–318.
association forcomputational linguistics..matthew peters, mark neumann, mohit iyyer, mattgardner, christopher clark, kenton lee, and lukezettlemoyer.
2018. deep contextualized word rep-in proceedings of the 2018 confer-resentations.
ence of the north american chapter of the associ-ation for computational linguistics: human lan-guage technologies, volume 1 (long papers), pages2227–2237, new orleans, louisiana.
associationfor computational linguistics..alec radford, jeffrey wu, rewon child, david luan,dario amodei, and ilya sutskever.
2019. languagemodels are unsupervised multitask learners.
openaiblog, 1(8)..nils reimers and iryna gurevych.
2019. sentence-bert: sentence embeddings using siamese bert-networks.
in proceedings of the 2019 conference onempirical methods in natural language processingand the 9th international joint conference on natu-ral language processing (emnlp-ijcnlp), pages3973–3983..marco t´ulio ribeiro, tongshuang wu, carlos guestrin,and sameer singh.
2020. beyond accuracy: behav-ioral testing of nlp models with checklist.
in pro-ceedings of the 58th annual meeting of the associ-ation for computational linguistics, acl 2020, on-line, july 5-10, 2020, pages 4902–4912.
associationfor computational linguistics..yuanlong shao, stephan gouws, denny britz, annagoldie, brian strope, and ray kurzweil.
2017. gen-erating high-quality and informative conversation re-sponses with sequence-to-sequence models.
in pro-ceedings of the 2017 conference on empirical meth-ods in natural language processing, pages 2210–2219..6388yukun zhu, ryan kiros, richard s. zemel, ruslansalakhutdinov, raquel urtasun, antonio torralba,and sanja fidler.
2015. aligning books and movies:towards story-like visual explanations by watchingin 2015 ieee interna-movies and reading books.
tional conference on computer vision, iccv 2015,santiago, chile, december 7-13, 2015, pages 19–27.
ieee computer society..zhihong shao, minlie huang, jiangtao wen, wenfeixu, and xiaoyan zhu.
2019. long and diverse textgeneration with planning-based hierarchical varia-in proceedings of the 2019 confer-tional model.
ence on empirical methods in natural languageprocessing and the 9th international joint confer-ence on natural language processing (emnlp-ijcnlp), pages 3257–3268, hong kong, china.
as-sociation for computational linguistics..bowen tan, zichao yang, maruan al-shedivat, eric p.xing, and zhiting hu.
2020. progressive generationof long text.
corr, abs/2006.15720..ashish vaswani, noam shazeer, niki parmar, jakobuszkoreit, llion jones, aidan n gomez, łukaszkaiser, and illia polosukhin.
2017. attention is allin advances in neural information pro-you need.
cessing systems, pages 5998–6008..alex warstadt, amanpreet singh, and samuel r bow-man.
2019. neural network acceptability judgments.
transactions of the association for computationallinguistics, 7:625–641..qizhe xie, zihang dai, eduard hovy, thang luong,and quoc le.
2020. unsupervised data augmenta-tion for consistency training.
advances in neuralinformation processing systems, 33..peng xu, mostofa patwary, mohammad shoeybi, raulpuri, pascale fung, anima anandkumar, and bryancatanzaro.
2020. megatron-cntrl: control-lable story generation with external knowledge us-ing large-scale language models.
in proceedings ofthe 2020 conference on empirical methods in nat-ural language processing, emnlp 2020, online,november 16-20, 2020, pages 2831–2845.
associ-ation for computational linguistics..zichao yang, diyi yang, chris dyer, xiaodong he,alex smola, and eduard hovy.
2016. hierarchi-cal attention networks for document classiﬁcation.
in proceedings of the 2016 conference of the northamerican chapter of the association for computa-tional linguistics: human language technologies,pages 1480–1489..lili yao, nanyun peng, ralph weischedel, kevinknight, dongyan zhao, and rui yan.
2019. plan-and-write: towards better automatic storytelling.
inproceedings of the aaai conference on artiﬁcial in-telligence, volume 33, pages 7378–7385..xingxing zhang, furu wei, and ming zhou.
2019.hibert: document level pre-training of hierarchi-cal bidirectional transformers for document summa-in proceedings of the 57th annual meet-rization.
ing of the association for computational linguistics,pages 5059–5069..wei zhao, maxime peyrard, fei liu, yang gao, chris-tian m meyer, and steffen eger.
2019. moverscore:text generation evaluating with contextualized em-in emnlp-beddings and earth mover distance.
ijcnlp, pages 563–578..6389a implementation details.
we implement our model based on bartbase anduse the public checkpoint and code of hugging-face’s transformers4.
both the encoder and thedecoder contain 6 hidden layers with 12 attentionheads.
the vocabulary consists of 50,625 tokenswith byte-pair encoding (radford et al., 2019).
and we regard (cid:104)mask(cid:105) and (cid:104)s(cid:105) in the original vo-cabulary as the sentence token (cid:104)sen(cid:105) and the dis-course token (cid:104)dis(cid:105), respectively.
the learning ratefor both post-training and ﬁne-tuning is 3e-5 withadam as the optimizer.
the adam epsilon is 1e-6.
it cost about 32 hours for hint’s post-trainingon bookcorpus, and 7 hours/8 hours for ﬁne-tuningon roc/wp, respectively.
the results are based on1 nvidia titan x gpu..b results on the validation set.
besides the performance on the test set which hasbeen reported in the main paper, we also providethe performance on the validation set of roc intable 6 for hint and strong baselines..models.
ppl.
b-1.
lr-2.
sr-1.
rel.
ord.
bartbart-post.
10.049.75.
0.3150.321.
0.3010.278.
0.6340.630.
0.9240.949.
0.8210.850.hint.
9.45.
0.331.
0.249.
0.623.
0.989.
0.881.table 6: automatic evaluation results of different mod-els on the validation set of roc.
we do not showbleu-2 results due to the space limitation.
rel andord are short for relatedness and order, respectively..c ∆ for sentence-level representation.
learning.
we tune ∆ in equation 5 to investigate the inﬂu-ence of the margin between the predicted similarityscore of hint and that of sentencebert.
we presentsome automatic evaluation results with different ∆in table 7. note that we use ∆ = 0.1 for theexperiments in the main paper.
we can see that asmaller ∆ (e.g., 0.01) would lead to less lexical andsemantic repetition but worse ﬂuency (indicated byhigher perplexity) and context relatedness, whichmay be caused by the over-ﬁtting to the model biasof the teacher model.
on the other hand, a larger∆ (e.g., 0.5) would result in worse performance inalmost all the metrics even than ∆ = 1.0 (withoutthe similarity prediction task).
the result indicates.
4https://github.com/huggingface/.
transformers.
that a large ∆ makes the model not learn effectivelyfrom the teacher model, and impact on the repre-sentations of the model itself.
by contrast, ∆ = 0.1would bring better overall performance..∆.
0.010.050.10.20.5.
1.0.ppl↓.
b-1↑.
b-2↑.
lr-2↓.
sr-1↓.
relatedness↑.
10.009.789.209.679.72.
0.3130.3160.3340.3260.319.
0.1390.1400.1540.1460.143.
0.2490.2640.2530.2730.261.
9.25.
0.332.
0.152.
0.264.
0.5990.6100.6190.6280.629.
0.622.
0.9370.9620.9870.9750.954.
0.970.table 7: automatic evaluation results for hint withdifferent ∆.
∆ = 1.0 means post-training ablating thesentence-level representation learning (hint w/o sen)..d manual evaluation.
annotation instructionwe show the manual annotation interface in fig-ure 3. in each hit (human intelligence task) ofamt, we show workers an input along with twotext pairs including (a) a pair of generated texts(one by hint and the other by a baseline), and(b) a pair of the human-written text and a nega-tive sample constructing by perturbing a text (e.g.,repetition, substitution) randomly sampled fromthe data.
note that the two pairs are presented inrandom order.
then, we ask workers to select thebetter text in each pair in terms of the ﬂuency andcoherence, respectively.
besides, we also requireworkers to annotate the errors in each text, includ-ing repetition (repeating the same or similar words),unrelatedness (with unrelated entities or events tothe input or within its own context), wrong tempo-ral orders, and others.
we reject an hit where theworker does not think the human-written text hasbetter coherence than the negative sample, or theworker does not annotate any errors for the nega-tive sample.
in this way, we reject 21.09% hits intotal.
finally, we ensure that there are three validand independent comparison results for each pairof generated texts..error analysisbased on the manual annotation of errors in thegenerated texts, we summarize the percentages ofthose texts with some error in all the annotatedtexts (200 for each model) in table 8. we decidethat a text contains some error when at least twoof three annotators annotate the error for it.
notethat each text of hint is annotated ﬁve times (threeannotators each time) since hint is compared withother ﬁve models.
therefore, we take the average.
6390figure 3: a simpliﬁed version of the manual annotation interface..of ﬁve annotation results.
we can see that hinthas less repetition, better context relatedness andtemporal orders than baselines.
however, the re-sults show that generating coherent long texts isstill challenging..models.
rept unrel temp others.
bartbart-post.
hintw/o senw/o disw/o sen&dis.
32.530.5.
12.023.516.027.5.
48.038.5.
13.529.015.548.5.
43.546.0.
18.820.542.049.0.
6.519.5.
9.814.018.05.0.table 8: percentages (%) of the texts which are anno-tated with some error in all the annotated texts.
the er-ror types include repetition (rept), unrelatedness (un-rel), wrong temporal orders (temp) and others.
thepercentages in each row do not sum to 100% since eachtext may contain multiple errors.
the best performancefor each error type is highlighted in bold..e constructing coherent and incoherent.
examples.
table 9 presents the details for constructing ex-amples to test the ability to model the contextcoherence in different aspects.
however, the ap-proach of automatic construction may inevitablyintroduce unexpected grammatical errors, whichwould also impact the text coherence.
to alleviatethe issue, we train a binary classiﬁer on the colacorpus (warstadt et al., 2019) to learn to judge thegrammaticality, and then ﬁlter out those examplesthat are classiﬁed as ungrammatical (the classiﬁerscore less than 0.5).
for simplicity, we directly usethe public model from textattack (morris et al.,2020) as the classiﬁer, which achieves an accuracy.
of 82.90% on the test set of cola.
finally, we ﬁlterout about 15.51% of the test examples..f case study.
sentence-level representationtable 10 presents some cases from the test set ofroc to demonstrate the effectiveness of the learnedsentence-level representation of hint.
we com-pute bleu-1, bart similarity and hint similar-ity for different sentence pairs, where bart/hintsimilarity means the cosine distance betweenbart/hint representations of two sentences.
toobtain the bart representation of a sentence, wefeed it into the bart decoder (along with its con-text) and apply mean-pooling on the hidden statesat the last layer.
hint representation refers to thecorresponding sentence-level representation afterdecoding the sentence.
we normalize all the resultsinto the standard gaussian distribution6.
we cansee that hint can derive meaningful sentence-levelrepresentations and gives high scores for seman-tically similar sentence pairs (the ﬁrst two pairs)but low scores for dissimilar pairs (the last twopairs).
by contrast, bart focuses more on token-level similarity and thus derives accordant similar-ity with bleu..discourse-level representationwe also present a case in table 11 to indicate theeffectiveness of the learned discourse-level repre-sentation of hint.
we consider a segment in thetext of table 11, which consists of two adjacent.
2the paraphrases are generated based on the publiccheckpoint of the back translation augmentation system ofuda (xie et al., 2020)..6we compute the mean and standard deviation within.
2,000 sentence pairs randomly sampled from the test set..6391instruction1.readtheinputandcomparetwotextpairs,respectively.2.selectthebettertextineachpairintermsoffluencyandcoherence.thetwoaspectsshouldbeevaluatedindependently.3.selecttheerrorsoccurringineachtext,respectively.texta:igotmytestresults.and...input:i woke up early in order to study.vs.textb:iamdisabledandcan...pair#1textc:imet up with my friends...vs.textd:theorangefellfrom...pair#2human-writtentextnegativesamplecomparepair#1:•fluency:•textawin•textbwin•tie•coherence:•textawin•textbwin•tiecomparepair#2:•fluency:•textcwin•textdwin•tie•coherence:•textcwin•textdwin•tieerrorsintexta:prepetitionpwrongtemporalorderspunrelatednesspotherserrorsintextd:......aspects.
selecting coherent examples.
creating incoherent examples.
semanticrepetition.
n/a.
relatedness.
texts with weak token-level semantic similarity in the context(e.g., with maximum inter-sentence moverscore (zhao et al.,2019) less than 0.1).
case: lilly was afraid of heights and fast movement.
shewas convinced to ride a roller coaster.
she hated every minuteof it.
she ran off and threw up immediately after ... (maximuminter-sentence moverscore =0.03).
repeating a sentence with its paraphrase by back translation5.
case: they got themselves and him on a diet.
{they put themselves on a diet withhim}insert ....substituting 20% nouns and verbs or a sentence randomly.
case: the orange fell from the tree.
it hit a girl on the head.
{the girl looked up at thetree.
}delete (cid:32) {she was unable to put the top up on her convertible.
}insert another orangefell from the tree.
that orange broke her nose..negation.
causalrelationship.
temporalrelationship.
texts with negated words (e.g., “not”, “unable”).
case: the man turned it on.
it did not respond.
the manunplugged it.
he took it apart.
he could never get ....inserting or deleting negated words for 20% sentences.
case: the man turned it on.
it {did not respond}delete (cid:32) {responded}insert.
the manunplugged it.
he took it apart.
he could never get that thing to work..texts with causality-related words (e.g., “so”, “because”).
case: mike had a very stressful job.
he needed a vacation.
so he took one.
he headed to the sunny beaches of mexico.
mike had a great time on his vacation..reversing the cause and effect (two individual sentences or clauses connected by a causality-related conjunction such as “so”); substituting the causality-related words with the antonyms(e.g., “reason” vs “result”).
case: mike had a very stressful job.
{he took one.
}reverse (cid:33) so {he needed avacation.
}reverse he headed to the sunny beaches of mexico ....texts with time-related words (e.g., “then”).
case: karen got stung by a bee.
her arm swelled up imme-diately.
it turned out she was allergic to bees!
she had to go tothe hospital for medication.
then she felt much better better!.
reversing two sequential events (two individual sentences or two clauses) connected by a time-related conjunction; substituting the time-related words with the antonyms (e.g., after vs. before)case: ... her arm swelled up immediately.
it turned out she was allergic to bees!
{she feltmuch better better!
}reverse (cid:33) then {she had to go to the hospital for medication.}reverse.
table 9: instruction for selecting coherent examples from human-written texts and creating incoherent examplesby perturbing human-written texts.
we highlight the keywords in italic which are crucial for the correspondingaspects.
we construct the incoherent examples by inserting, deleting or reversion..sentence 1.sentence 2.he was really em-barrassed by it..he was very em-barrassed of it..he dreamed ofmaking the worlda better place..he had a passionto change his coun-try for better..b-1.
5.08.bart.
hint.
2.83.
2.17.
0.30.
0.63.
2.17.he wasn’t havinga good time..he was having agood time..7.17.
2.04.
1.65.i wanted to buysome fruit..i wanted to go to astate college..1.40.
1.46.
-0.50.table 10: sentence pairs sampled from the test set ofroc and the corresponding bleu-1 (b-1), bart sim-ilarity and hint similarity..input:(cid:172)kate was at her garbage can on a dark night..human-written text:(cid:173) and a raccoon was standing near the can.
(cid:174) it started to come towards her.
(cid:175) kate turned and ran to the house hoping it wasn’tbehind her.
(cid:176) once inside she was relieved to see it hadn’t fol-lowed her..b (m).
beforeafter(cid:173)(cid:174)(cid:175)(cid:176) (cid:174)(cid:173)(cid:175)(cid:176) 4.05(cid:173)(cid:174)(cid:175)(cid:176) (cid:173)(cid:175)(cid:174)(cid:176) 1.30(cid:173)(cid:174)(cid:175)(cid:176) (cid:173)(cid:174)(cid:176)(cid:175) 1.96.b (d).
hint.
5.323.814.17.
-0.89-1.08-3.82.sentences (e.g., the segment (cid:174)(cid:175)in (cid:173)(cid:174)(cid:175)(cid:176)).
then,we can derive the segment representation by con-catenating the contextualized representations of thetwo sentences.
besides, if we reverse the two sen-tences (from (cid:174)(cid:175) to (cid:175)(cid:174), other sentences in thetext unchanged), we can also derive the segmentrepresentation in the same way.
note that in thiscase we concatenate the two sentence representa-tions still in the normal order (i.e., ﬁrst the rep-resentation of (cid:174) and then that of (cid:175)).
we expectthe segment representations before and after thereversion to be distant in the vector space if thesentence representation contains discourse-level in-formation.
otherwise, the segment representationswould be similar since the segments have the sametokens before and after the reversion.
for bart,we derive the sentence representation by feedingthe whole text into bart and mean pooling thehidden states at the positions of tokens in the sen-.
table 11: a human-written text sampled from the testset of roc with ﬁve sentences from (cid:172) to (cid:176).
we con-sider two adjacent sentences as a segment (underlined)and compute the similarity of the segment representa-tions (derived by bart or hint) before and afterreversing the two sentences.
b (m) and b (d) meanusing bart to derive the sentence representation bymean-pooling and taking the hidden state at the posi-tion corresponding to the discourse token, respectively..tence.
and for hugo, we regard the correspondingdiscourse-level representation of each sentence asthe sentence representation.
for reference, we alsoshow the results using the hidden state of bart atthe position of the discourse token as the sentencerepresentation, i.e., b (d).
table 11 shows the sim-ilarity between the segment representations beforeand after the sentence reversion.
all the resultsare normalized into the standard gaussian distribu-.
6392input:.
i woke up early in order to study..golden text:.
i met up with my friends at the library.
we reserved a studying room on the ﬁrst ﬂoor.
a few hours havepassed since we stayed for a while.
by the time we got out, it was already midnight..seq2seq:.
i am disabled and can not walk into the library.
i had to look through it for about three days.
i found out ihad my book in two days.
i walked to the library to look for it..plan&write:.
i got my test results.
and then i began to study.
i fell asleep for the test and was very sleepy.
i awoke toﬁnd my test results had been negative..bart:.
i woke up late, and found a few other people already on the road.
i was able to get to work, but i also had tokeep my eyes open.
i didn’t want to miss any of the work.
so i was too lazy to get to work..bart-cls:.
i had a very difﬁcult time ﬁnding my position.
i had to leave my house early in order to got to a hotel.
i hadto walk around the block in order to get to the hotel.
i ended up walking the rest of the way to the hotel..bart-post:.
i drove to the library.
i went to the library and found out that the library closed today.
i got to the libraryand decided to go to the library.
i sat in the library and went to the library and ﬁnished my studies..hint:.
i had to go to the library and ﬁnish my homework.
i quickly made it to the library and began to study.
after studying for an hour, i fell asleep.
i ended up not ﬁnishing my homework and went back home..table 12: generated texts by different models.
italic words indicate the improper entities or events in terms ofcoherence in the context.
and bold words denote the coherent event sequence..tion7.
the results show that bart derives similarrepresentations for the segments before and after re-version whether using mean-pooling or the hiddenstate corresponding to the discourse token.
in com-parison, although the reversion does not changethe sentence semantics, segment representationsderived by hint are very dissimilar, suggestingthat hint can derive meaningful discourse-levelrepresentations..text generationwe presented some generated cases in table 12.hint can generate more coherent stories than base-lines.
speciﬁcally, the baselines can easily predictsome words which are related to the input (e.g.,“sleepy”, “library”) or within its own context (e.g,“test results”, “hotel”).
however, these words areused incoherently.
for example, the text gener-ated by plan&write has a wrong temporal orderamong the sentences (ﬁrst “got test results” andthen “fell asleep for the test”).
the texts gen-erated by seq2seq, bart and bart-cls arechaotic in semantics and discourse structures.
thetext generated by bart-post suffers from repeti-tive plots (“went to the library”) and conﬂictinglogic (“the library closed” but “sat in the library”).
by contrast, the text generated by hint has a co-herent event sequence with related content and aproper temporal order.
the results indicate the ef-fectiveness of modeling high-level coherence foration..7we compute the mean and standard deviation within.
2,000 segment pairs sampled from the test set of roc..6393