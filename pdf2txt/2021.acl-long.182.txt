tree-structured topic modeling withnonparametric neural variational inference.
ziye chen1, cheng ding1, zusheng zhang1, yanghui rao1,∗, haoran xie21school of computer science and engineering, sun yat-sen university, guangzhou, china2department of computing and decision sciences, lingnan university, hong kong{chenzy35,dingch6,zhangzsh3}@mail2.sysu.edu.cn,raoyangh@mail.sysu.edu.cn, hrxie2@gmail.com.
abstract.
topic modeling has been widely used for dis-covering the latent semantic structure of doc-uments, but most existing methods learn top-ics with a ﬂat structure.
although probabilis-tic models can generate topic hierarchies byintroducing nonparametric priors like chineserestaurant process, such methods have datascalability issues.
in this study, we develop atree-structured topic model by leveraging non-parametric neural variational inference.
par-ticularly, the latent components of the stick-breaking process are ﬁrst learned for each doc-ument, then the afﬁliations of latent compo-nents are modeled by the dependency matri-ces between network layers.
utilizing thisnetwork structure, we can efﬁciently extracta tree-structured topic hierarchy with reason-able structure, low redundancy, and adaptablewidths.
experiments on real-world datasetsvalidate the effectiveness of our method..1.introduction.
topic models (blei et al., 2003; grifﬁths et al.,2004) are important tools for discovering latent se-mantic patterns in a corpus.
these models can begrouped into ﬂat models and hierarchical models.
in many domains, topics can be naturally orga-nized into a tree, where the hierarchical relation-ships among topics are valuable for data analy-sis and exploration.
tree-structured topic model(grifﬁths et al., 2004) was thus developed to learncoherent topics from text without disrupting theinherent hierarchical structure.
such a method hasbeen proven as useful in various downstream ap-plications, including hierarchical categorization ofweb pages (ming et al., 2010), aspects hierarchiesextraction in reviews (kim et al., 2013), and hier-archies discovery of research topics in academicrepositories (paisley et al., 2014)..∗the corresponding author..despite the practical importance and potentialadvantages, tree-structured topic models still facethe following challenges.
firstly, the hierarchicalstructure of topics should be reasonable (viegaset al., 2020).
typically, topics near the root aremore general while the ones close to the leavesare more speciﬁc.
besides, child topics shouldbe coherent with their corresponding parent top-ics.
secondly, low redundancy is necessary forthe extracted topics, in order to prevent the dis-tributions associated with parent topics and theirchildren being extremely similar (grifﬁths et al.,2004).
thirdly, the number of topics in each hier-archy level should be automatically determined bythe model, because it is usually unknown and cannot be previously set to a predeﬁned value (kimet al., 2012).
finally, it is difﬁcult for probabilisticmodels to enhance the data scalability (isonumaet al., 2020).
previously, several tree-structuredtopic models (grifﬁths et al., 2004; kim et al.,2012; isonuma et al., 2020) have been developed.
but these methods can not fully overcome the afore-mentioned challenges..in this paper, we focus on grouping topics into areasonable tree structure, based on the neural vari-ational inference (nvi) framework (kingma andwelling, 2014; rezende et al., 2014) with a non-parametric prior.
owing to the excellent functionﬁtting ability, neural network has been widely in-troduced into topic modeling.
nonetheless, fewneural methods explicitly model the dependenciesamong different layers and get explainable hier-archical topics, which is largely due to the weakinterpretability of neural networks.
furthermore,the inﬂexibility of neural networks also makes itdifﬁcult to learn an unbounded number of topicsat each level.
to address these limitations, we pro-pose a novel nonparametric neural method to gen-erate tree-structured topic hierarchies, namely non-parametric tree-structured neural topic model.
proceedingsofthe59thannualmeetingoftheassociationforcomputationallinguisticsandthe11thinternationaljointconferenceonnaturallanguageprocessing,pages2343–2353august1–6,2021.©2021associationforcomputationallinguistics2343(ntsntm)1. by connecting the network layerswith dependency matrices, the model is able toextract an explainable tree-structured hierarchy.
firstly, the topic afﬁliations among hierarchy lev-els can be determined by the dicrete vectors ofthe dependency matrices.
secondly, to control re-dundancy among topics, we allow the model tofreely generate topics without duplicating their cor-responding parent topics.
thirdly, we couple astick-breaking process with nvi to equip the topictree with self-determined widths, which can helpthe model determine the number of topics auto-matically.
finally, due to the advantages of neuralnetworks, our model can scale to larger datasetsconveniently.
experiments indicate that our modeloutperforms baselines on several widely adoptedmetrics and two new measurements developed fortree-structured topic models..the rest of this paper is organized as follows.
we describe related work in section 2. then, wedetail the proposed ntsntm in section 3. section4 presents our experimental results and discussions.
finally, we draw conclusions in section 5..2 related work.
in (grifﬁths et al., 2004), a tree-structured topicmodel called hlda was ﬁrst proposed by intro-ducing a nested chinese restaurant process (ncrp).
for hlda, a topic tree is constructed through gibbssampling given a certain depth.
based on hlda,xu et al.
(2018) proposed a knowledged-basedhtm to generate topic hierarchies from multipledomains corpora, but the hierarchical relation be-tween the ancestor topic and the offspring one maybe unclear, because a document is generated by thetopics along a single path of the tree.
to overcomethis issue, kim et al.
(2012) proposed a recursivecrp (rcrp), in which a document possesses a dis-tribution over the entire tree.
although rcrp hasshown remarkable competitiveness in hierarchicaltopic modeling, it suffers from the major limitationof data scalability (isonuma et al., 2020).
severalother methods focused on hierarchical text cluster-ing.
for instance, ghahramani et al.
(2010) appliednested stick-breaking processes to cluster data intoa tree structure.
unfortunately, the above methodonly models a document by a single node of thetree.
liu et al.
(2014) developed a model namedhlta for topic detection, in which words and top-.
1the code of our model is available in public at: https:.
//github.com/hostnlp/ntsntm..ics are clustered by employing the bridged-islandsalgorithm iteratively.
however, hlta is unableto cope with polysemous words, which is quiteimportant for topic models..to couple nonparametric processes with nvi,miao et al.
(2017) used gaussian distributions togenerate stick-breaking fractions.
nalisnick andsmyth (2017) ﬁrst described how to use stochasticgradient variational bayes for posterior inferenceof the weights in stick-breaking processes.
exper-iments indicated that the latent representations ofthe above model were more discriminative thanthose of the gaussian variant.
then, ning et al.
(2020) developed two nonparametric neural topicmodels by treating topics as trainable parameters.
unfortunately, the aforementioned methods canonly learn topics with a ﬂat structure..for tree-structured neural topic modeling, a fea-sible way is to decompose the distribution overthe topic tree into a path distribution and a leveldistribution.
following (wang and blei, 2009),where a tree-based stick-breaking construction ofncrp was ﬁrst derived to draw topic paths, andthen a level distribution was learned to sample top-ics along the path, isonuma et al.
(2020) proposeda tree-structured neural topic model (tsntm) byparameterizing an unbounded ancestral and frater-nal topic distribution.
tsntm applies a doubly-recurrent neural network (drnn) to obtain topicembeddings via ancestral and fraternal edges, thengenerates breaking fractions by the dot productbetween document embeddings and topic embed-dings.
however, tsntm fails to learn a reasonabletopic tree for the following reasons.
firstly, thebreaking fractions do not obey the beta distribu-tions adopted in the stick-breaking process (sbp).
secondly, the structure of drnn in tsntm issimpliﬁed, where the topic embeddings are gener-ated directly by an initialized root embedding andtwo parameter matrices (i.e., ancestral and fraternalconnections).
this prevents the model from learn-ing appropriate semantic embeddings for topics.
finally, tsntm relies on heuristic rules to updatethe tree structure..another stream of work is to generate a docu-ment by a directed acyclic graph (dag) structuredtopic hierarchy.
for instance, li and mccallum(2006) introduced the pachinko allocation model(pam) to capture correlations between topics usinga dag.
mimno et al.
(2007) proposed the hierarchi-cal pam by connecting the root topic to lower-level.
2344topics through multinomial distributions.
nonprob-abilistic matrix factorization was also used to ex-tract the topic structure.
liu et al.
(2018) usednon-negative matrix factorization (nmf) with threeoptimization constraints, including global indepen-dence, local independence, and information con-sistency, to preserve topic coherence and a rea-sonable structure.
viegas et al.
(2020) incorpo-rated pre-trained word embeddings into nmf tofurther improve topic coherence.
the main limi-tation of nmf-based methods, however, is that atime-consuming process (e.g., measure the stabilityof results by running multiple random samplings)is necessary to determine the number of topics ateach level.
this is because nonparametric priorsare intractable to be included in these models..3 tree-structured neural topic model.
with nonparametric prior.
in this section, we ﬁrstly describe the stick-breaking process.
then, we introduce the modelingof tree-structured topic hierarchy.
finally, we detailthe inference method of our ntsntm..3.1 stick-breaking process.
for nonparametric models, stick-breaking prior is arandom measure with the form g = (cid:80)∞k=1 πkδζk ,where δζk is a discrete measure concentrated atζk ∼ g0 (ishwaran and james, 2001)2, i.e, adraw from the base measure.
the πks are randomweights independent of g0 (nalisnick and smyth,2017).
this constructive deﬁnition is known assbp (sethuraman, 1994), which implies that theweights π = (πk)∞k=1 can be drawn according tothe procedure of iteratively breaking off segmentsfrom a unit stick..figure 1: stick-breaking construction..2in topic models, ζk represents the kth topic and g0 rep-.
resents the topic space..as shown in figure 1, we break the unit stickand get the ﬁrst component with length v1.
if afraction v2 of the remaining stick is broken off,then we obtain the second component with lengthv2(1 − v1) and a remaining stick with length (1 −v1)(1 − v2).
the following breaks are taken on theremaining stick by the same operation.
given atruncation level t , the length of the last componentwill be (cid:81)t −1j=1 (1 − vj).
formally, the length of eachcomponent is deﬁned as:.
πk =.
(cid:40).
v1vk.
(cid:81).
t<k (1 − vt).
iffor.
k = 1,k > 1,.
(1).
where vk ∼ beta(α0, β0), with α0 and β0 be-ing the prior parameters.
note that the componentweights π satisfy 0 ≤ πk ≤ 1 and (cid:80)∞k=1 πk = 1,thus we can interpret π as random probabilities.
particularly, when vk ∼ beta(1, β0), the joint dis-tribution for π is the gem distribution (pitman,2006) with concentration parameter β0, and thecorresponding sbp is one of the constructions forthe dirichlet process, a popular nonparametric ran-dom process for topic modeling (teh et al., 2005).
in our method, we take component weights πas the path distribution of a document.
we assumethat the words of a document come from severaltopic paths.
due to the sequentiality of the stick-breaking operation, paths with smaller serial num-bers are more likely to be activated to represent thedocuments, while paths with larger serial numberstend to be unactivated.
the number of activatedpaths can be adjusted by sbp automatically..3.2 tree-structured topic hierarchy.
to conveniently describe our method, we here com-pare the sampling processes for an example docu-ment of different tree-structured topic models.
asshown in figure 2, hlda (grifﬁths et al., 2004)considers that a document is generated by topicsof a single path, which violates the multi-topicsassumption of topic models (i.e., a document mayspan several topics).
considering this issue, rcrp(kim et al., 2012) and tsntm (isonuma et al.,2020) assume that a document can be generatedby any topic in the tree.
we follow the above as-sumption adopted in rcrp and tsntm to model atree-structured topic hierarchy, but the difference isthat our model takes the sampling from the bottomup rather than from the top down as in rcrp andtsntm.
particularly, rcrp samples topics fromthe root using recursive crp.
tsntm samples.
2345𝜋1=𝑣1𝜋2=𝑣2(1−𝑣1)𝜋3=𝑣31−𝑣2(1−𝑣1)𝜋21𝑠𝑡2𝑛𝑑3𝑟𝑑𝑇𝑡ℎ⋯𝑇−1simplex𝜋1𝜋3𝜋𝑇𝜋𝑇=ෑ𝑗=1𝑇−1(1−𝑣𝑗)⋯⋯paths from the root by applying a drnn (alvarez-melis and jaakkola, 2017), and it needs to updatethe tree structure frequently by heuristic rules.
onthe contrary, our model directly samples the leaftopics, and the paths toward the root are determinedautomatically.
speciﬁcally, we use a common stick-breaking construction to infer the distribution overleaf topics, which corresponds to the path distribu-tion.
besides, we use dependency matrices to keeptrack of the afﬁliations among topics.
thus the treestructure can be updated through back propagation..figure 2: sampling process of an example docu-ment for hlda (grifﬁths et al., 2004), rcrp (kimet al., 2012), tsntm (isonuma et al., 2020), and ourntsntm.
each node represents a topic z with its dis-tributions over words w. the active topics and path arehighlighted by boldface..figure 3 shows the graphical representation ofntsntm.
for our model, the number of leaf topicsis determined by sbp, and the numbers of non-leaftopics are adjusted through dependency matrices mbetween network layers.
the lth item of m, i.e., ml∈ [0, 1]kl∗kl+1, is the dependency matrix betweenlayers l and l+1, where kl and kl+1 represent themaximum numbers of topics at level l and level l+1,respectively.
in particular, ml,k,j is the probabilityof topic j at level l being the parent of topic k atlevel l+1 with (cid:80)j(cid:48) ml,k,j(cid:48) = 1. as mentioned in(grifﬁths et al., 2004), a clear tree structure indi-cates that each sub-topic has a relationship with nomore than one super-topic.
so a softmax functionwith low temperature (hinton et al., 2015) is ap-plied to ensure that ml,k approximates a discreteone-hot vector.
in this way, the topic tree can bebuilt through the introduced m from bottom up.
furthermore, the topic hierarchy can be updatedautomatically according to the update of m..after determining the topic hierarchy by m, thegenerative process of each word in ntsntm canbe described as follows:.
figure 3: graphical representation of ntsntm.
solidand dashed arrows denote generation and inference..(2).
(3).
(4).
(5).
(6).
(7).
1. for each document xd ∈ {x1, ..., xd}:.
draw sbp weights: πd ∼ gem(β0);draw gaussian samples: gd ∼ n (0, i 2);draw level distributions: ηd = fη(gd)..2. for each word wd,n ∈ {wd,1, ..., wd,nd} in xd:.
draw a path: cd,n ∼ multi(πd);draw a level: rd,n ∼ multi(ηd);draw a word: wd,n ∼ multi(φcd,n[rd,n])..in the above, d is the number of documents, ndis the number of words in xd.
φcd,n[rd,n] ∈ (cid:52)v −1is the word distribution of the topic at level rd,nof path cd,n, and v is the vocabulary size.
fη(·)is a neural perceptron with softmax activation totransform a gaussian sample to a level distribution..3.3 parameter inference.
since the beta distribution does not have a differ-entiable non-centered parametrization that nvi re-quires (kingma and welling, 2014), we choosethe kumaraswamy distribution (kumaraswamy,1980) to approximate gem(β0), i.e., the con-junction of beta(1, β0) and a stick-breaking op-eration (nalisnick and smyth, 2017).
for thekumaraswamy distribution, the probability den-sity function on the unit interval is deﬁned askumaraswamy(x; a, b) = abxa−1(1 − xa)b−1 forx ∈ (0, 1) and a, b > 0. samples can be drawn1via the inverse transform: x ∼ (1 − ua whereu ∼ uniform(0, 1).
then the kl-divergence be-tween the kumaraswamy distribution and the betadistribution can be closely approximated in theclosed-form.
we describe the parameter inferenceprocess of our ntsntm as follows..1b ).
firstly, we estimate the component weights ofdocument xd, i.e., ˆπd, by the following stick-.
2346 11 1 12 112 111 121 1 3 4 2 11 1 12 112 111 121 3 1 4 2 11 1 12 112 111 121 4 1 2 3 1 2 3 4…  breaking operation with fractions vd:.
αd = fα(xd), βd = fβ(xd),vd ∼ kumaraswamy(αd, βd),.
(8).
(9).
ˆπd = (vd,1, ...,.
(1 − vd,j)),.
(10).
t −1(cid:89).
j=1.
where the bag-of-words representation is used forxd.
to ensure positive outputs, fα(·) and fβ(·) areneural perceptrons with softplus activation..secondly, we infer the level distributions ˆηd by:.
µd = fµ(xd),ˆgd ∼ n (µd, σ2.
d),.
σd = fσ(xd),ˆηd = fη( ˆgd),.
(11).
(12).
where fµ(·) and fσ(·) are linear transformations.
inpractice, we reparameterize ˆgd = µd + ˆ(cid:15) ∗ σd withthe sample ˆ(cid:15) ∼ n (0, i 2) (rezende et al., 2014)..thirdly, we obtain the topic distributions of xd,.
i.e., ˆθd = { ˆθd,1, ..., ˆθd,l} by:.
ˆθd,l =.
(cid:40).
ˆηd,l ˆπdˆηd,l ˆπd.
(cid:81).
l(cid:48)≥l ml(cid:48).
iffor.
l = l,l < l,.
(13).
(cid:80)k.ˆθd,l,k = 1..where l denotes the depth of the topic tree, and(cid:80)lthen, we follow (miao et al., 2017) to ex-plicitly model topic-word distributions by: φ =sof tmax(u ∗ tt ), where u ∈ rv ∗h and t ∈r(cid:80)l kl∗h are word vectors and topic vectors, andh denotes the dimension of word/topic vectors.
given topic-word distributions φ and topic distri-butions ˆθd obtained from eq.
(13), our model re-constructs each document xd by: p(wd,n|φ, ˆθd) =(cid:80)[p(wd,n|φzd,n)p(zd,n| ˆθ)] = ˆθd ∗ φ, where.
zd,n.
zd,n is the topic assignment for wd,n..finally, the variational lower-bound of xd is:.
l =eq(πd,ηd|xd)[.
(cid:88).
log(p(wd,n|φ, ˆθd))].
n− dkl[q(πd|xd)||p(πd)]− dkl[q(ηd|xd)||p(ηd)],.
where q(πd|xd) and q(ηd|xd) are posteriors mod-eled by the inference network.
p(πd) is the priorfor πd, i.e., gem(β0), and p(η) is the prior for η,i.e., the standard gaussian transformed by fη(·)..the parameter inference method for ntsntmis presented in algorithm 1. we use the variationallower-bound to calculate gradients and apply adam(kingma and ba, 2015) to update parameters..algorithm 1: parameter inference algorithminput: gem priors β0 and documents.
{x1, ..., xd};.
output: document-topic distribution θ,topic-word distribution φ, andtopic tree t r..1 randomly initialize dependency matrices m.and topic-word distribution φ;.
2 repeat.
3.
4.
5.
6.
7.
8.
9.
10.for document xd ∈ {x1, ..., xd} do.
estimate ˆπd and ˆηd by eqs.
(8–12);compute ˆθd by eq.
(13);for wd,n ∈ xd do.
p(wd,n| ˆθd, φ) = ˆθd ∗ φ ;.
endcompute l by eq.
(14);update fα(·), fβ(·), fµ(·), fσ(·),fη(·), φ, and m;.
end.
1112 until convergence;13 build t r according to m and φ..4 experiments.
4.1 datasets.
we conduct experiments on four widely usedbenchmark datasets: 20news (miao et al., 2017),reuters (wu et al., 2020), wikitext-103 (nan et al.,2019), and rcv1-v2 (miao et al., 2017).
20newsand reuters are two news corpora.
wikitext-103is a language modeling dataset extracted fromwikipedia, and rcv1-v2 is a large version ofreuters.
table 1 presents the statistics of thesedatasets, where the vocabulary is obtained by fol-lowing the same preprocessing steps in the originalpaper.
for each corpus, we randomly select 5% oftraining samples as the validation set..(14).
dataset20newsreuterswikitext-103rcv1-v2.
#docs (train)11,3147,76928,472794,414.
#docs (test)7,5313,0196010,000.vocabulary size1,9952,00020,00010,000.table 1: the statistics of datasets..4.2 experimental setup.
for tree-structured topic models, we adopt hlda(grifﬁths et al., 2004)3, rcrp (kim et al., 2012),.
3note that hlda was named as ncrp (blei et al., 2010).
in (isonuma et al., 2020)..2347and tsntm (isonuma et al., 2020) as our baselines.
for all these models, the max-depth of topic tree isset to 3 by following (isonuma et al., 2020)..for nonparametric or ﬂat topic models, we adopthdp (teh et al., 2005), gsm & gsb (miao et al.,2017), nb-ntm & gnb-ntm (wu et al., 2020),and itm-vae & hitm-vae (ning et al., 2020) asbaselines.
hdp is a classical nonparametric topicmodel that allows potentially an inﬁnite numberof topics.
gsm & gsb are two nvi-based mod-els using gaussian priors.
in particular, gsb usesgaussian distributions to generate stick-breakingfractions.
nb-ntm & gnb-ntm are two ﬂat neu-ral topic models based on negative binomial andgamma negative binomial processes respectively.
for itm-vae & hitm-vae, they extended themethod in (nalisnick and smyth, 2017) to intro-duce nonparametric processes into the nvi frame-work by extracting the potential inﬁnite topics..we directly use the publicly available codesof hlda4, rcrp5, tsntm6, hdp7, nb-ntm &gnb-ntm8, and itm-vae & hitm-vae9.
be-sides, we implement gsm & gsb based on theoriginal paper.
for all parametric models, the num-ber of topics is set to 50 and 200 as in (miao et al.,2017).
for nonparametric models based on sbp,the truncation level is set to 200, and the concen-tration parameter β0 for the gem distribution ischosen from [5, 10, 15, 20, 25, 30] using each val-idation set.
in particular, we sequentially choosethe topics, of which the sum of probabilities in thewhole corpus exceeds 95%, as the active ones.
forneural baselines and our proposed model, we setthe size of hidden layers to 256 and use one samplefor nvi by following (miao et al., 2017)..all the experiments are conducted on a work-station in python/java environment equipped with40g memory.
in the following, we do not reportthe results of hlda and rcrp on rcv1-v2 sincethey failed to achieve convergence in 48 hours..4.3 topic hierarchy analysis.
as mentioned in (viegas et al., 2020), a reason-able topic hierarchy means that topics near the rootshould be more general while the ones close to.
4https://github.com/joewandy/hlda5https://github.com/uilab-github/rcrp6https://github.com/misonuma/tsntm7https://github.com/arnim/hdp8https://github.com/mxiny/nb-ntm9https://github.com/walkerning/itmvae_.
public.
the leaves should be more speciﬁc.
to this end,we adopt topic specialization (kim et al., 2012)as an indicator for the evaluation of topical hier-archy.
the specialization of a topic is the cosinedistance between the word distribution of the topicand the term frequency vector of the entire cor-pus.
a higher specialization score implies that thetopic is more specialized.
figure 4 presents theaverage topic specialization scores of each levelfor different tree-structured models.
the resultsindicate that ntsntm and rcrp can achieve a rea-sonable pattern of topic specialization at differentlevels, i.e., the scores become higher as the levelbecomes deeper.
we also observe that the baselineof tsntm generates more speciﬁc topics at thesecond level than the third level, which indicates anunreasonable topic hierarchy.
for the baseline ofhlda, there is a leap of topic specialization fromlevel 2 to level 3, especially for 20news.
the rea-son may be that each document is generated bytopics along a single path for hlda, which rendersthe large specialization of the topics at level 3 sincethey are all restricted to one topic from level 2..figure 4:structured topic models at each level..topic specialization of different.
tree-.
a reasonable topic hierarchy also indicates thatchild topics are coherent with their correspondingparent topics (viegas et al., 2020).
to measure therelations of two connected topics, we develop anew metric named cross-level npmi (clnpmi) tomeasure the relations of two connected topics bycalculating the average npmi value of every twodifferent topic words from a parent topic and itschild.
in the above, npmi was proposed by lauet al.
(2014) which evaluates the relation between.
2348123level0.00.20.40.60.81.0topic specialization20news123level0.00.20.40.60.81.0reuters123level0.00.20.40.60.81.0wikitext-103hldarcrptsntmntsntm123level0.00.20.40.60.81.0rcv1-v2two words wi and wj as follows:.
npmi(wi, wj) =.
log p (wi,wj )p (wi)p (wj )− log(p (wi, wj)).
..(15).
based on npmi, we deﬁne clnpmi as:.
clnpmi(wp, wc).
=.
1p||w (cid:48)c|.
|w (cid:48).
(cid:88).
(cid:88).
npmi(wi, wj),.
(16).
wi∈w (cid:48)p.wj ∈w (cid:48)c.p = wp − wc and w (cid:48).
where w (cid:48)c = wc − wp, inwhich, wp and wc denote the top n words of aparent topic and one of its children.
to avoid degen-erating into npmi when the parent and the childtopics are highly similar, clnpmi is estimated bythe distinct words between every two topics..to evaluate the topic redundancy for a tree, weintroduce a new measurement named the averagedoverlap rate (or) and adopt the widely-used topicuniqueness (tu) (nan et al., 2019).
or measuresthe averaged repetition ratio of top n words be-tween parent topics and their children, which isdeﬁned as: or(wp, wc) = |wp∩wc|.
tu cal-culates the uniqueness of all topics by tu =1k=1 tu(k), where k is the number of top-kics and tu(k) is deﬁned as:.
(cid:80)k.n.tu(k) =.
1n.n(cid:88).
n=1.
1cnt(n, k).
..(17).
in the above, cnt(n, k) is the total number oftimes the nth top word in topic k appears in the topn words across all topics..model.
tsntm ntsntm.
20news.
reuters.
wikitext-103.
rcv1-v2.
clnpmi (↑)tu (↑)or (↓)clnpmi (↑)tu (↑)or (↓)clnpmi (↑)tu (↑)or (↓)clnpmi (↑)tu (↑)or (↓).
hlda rcrp0.0980.0650.2850.0510.4040.0560.0720.0500.2270.4470.5150.1050.0880.0630.3550.5970.4470.087––––––.
0.0860.4300.0830.0270.3700.1760.0650.6150.0780.0280.5440.051.
0.1220.7600.0530.1020.7080.0660.1130.7300.0690.0880.8020.042.table 2:clnpmi, tu, and or scores of tree-structured topic models, in which, higher clnpmi andtu with a lower or indicate better performance.
thebest value on each metric is highlighted by boldface..for each of the aforementioned metrics, we cal-culate the average scores of 5, 10, and 15 top words.
table 2 shows the performance of different models,.
where each method is run for 5 times and the aver-age values are presented.
the results indicate thatour model signiﬁcantly outperforms the baselinesin most cases, with p-values less than 0.05. forhlda and our ntsntm on the 20news dataset,the difference is not statistically signiﬁcant on theor metric, with a p-value equal to 0.391. thisvalidates the effectiveness of the bottom-up struc-ture for ntsntm, in which, non-leaf topics areactivated when their offsprings are chosen..figure 5: hierarchical afﬁnity scores..we also present the hierarchical afﬁnity (kimet al., 2012) for each model to measure whether theparent topic is more similar to its child topics thanthe descendants of other parent topics.
the aver-age cosine similarities of the parent topic’s worddistribution to children topics and non-children top-ics are shown in figure 5. for parent topics, bothrcrp and ntsntm clearly show stronger afﬁni-ties with children topics than non-children topics.
but rcrp suffers from the high redundancy, whichcan be indicated by the high similarities (0.73 ∼0.82) between parent topics and sub-topics.
to in-tuitively demonstrate the ability of our model ingenerating a topic tree, we present several topicsextracted from 20news by our ntsntm and theexisting nvi-based tsntm in figures 6 and 7,respectively.
the results indicate that our model isable to learn a reasonable tree-structured topic hier-archy with low redundancy.
while for tsntm, wenotice that there is a low degree of discriminationbetween topics at the second and the third levels.
inaddition, topics of the same group at the third levelare highly repetitive, including “rec.sport.baseball”and “talk.politics.misc”.
for completeness, wefurther check topics extracted from 20news byhlda and rcrp.
the results indicate that eachtopic at the second level is too general to represent.
2349hldarcrptsntmntsntm0.00.20.40.60.81.0hierarchical affinity0.070.780.280.090.050.410.220.0120newshldarcrptsntmntsntm0.00.20.40.60.81.00.070.820.280.110.100.380.230.01reutershldarcrptsntmntsntm0.00.20.40.60.81.0hierarchical affinity0.060.730.270.190.070.350.190.05wikitext-103childrennon-childrentsntmntsntm0.00.20.40.60.81.00.140.070.120.00rcv1-v2figure 6: topic samples extracted from 20news by ntsntm, where top 5 words are listed for each topic..figure 7: topic samples extracted from 20news by tsntm, where top 5 words are listed for each topic..a topic branch and the afﬁliations are unclear forhlda.
although rcrp can generate meaningfultopics with appropriate afﬁliations between differ-ent levels, it suffers from a high topic redundancy..4.4 comparison on topic interpretability.
in this part, we use the widely adopted npmi (miaoet al., 2017; liu et al., 2019; wu et al., 2020; ninget al., 2020; isonuma et al., 2020) to evaluate topicinterpretability10.
as mentioned in (lau et al.,2014), the npmi is a measurement of topic co-herence which closely corresponds to the rankingof topic interpretability by human annotators.
table.
10we do not estimate the perplexity for the following tworeasons.
first, the perplexity of sampling-based and nvi-based models is difﬁcult to compare directly (isonuma et al.,2020).
second, the prior of nvi-based methods has a largeinﬂuence on the perplexity since the kl-divergence may varygreatly for different priors (burkhardt and kramer, 2019)..3 shows the npmi of 50 and 200 topics for para-metric topic models and topics induced automati-cally for nonparametric topic models.
we run eachmodel for 5 times and present the average results.
firstly, ntsntm outperforms all tree-structuredbaselines, and the difference is statistically signif-icant at the level of 0.05 (except for tsntm onthe rcv1-v2 dataset).
secondly, ntsntm showscompetitive performance when compared with thebest ﬂat baselines.
in particular, except for hitm-vae on the reuters dataset, the results of all theother top-performing baselines are not signiﬁcantlybetter than those of our model..4.5 evaluating data scalability.
to evaluate data scalability, we randomly sam-ple several numbers of documents (12.5k, 25k,50k, 100k, 200k, 400k, and all) from the training.
2350……model.
gsmgsb.
20newsnpmi (↑)20050.reutersnpmi (↑)20050.wikitext-103npmi (↑)20050.rcv1-v2npmi (↑)200500.211 0.165 0.198 0.155 0.214 0.217 0.231 0.0620.231 0.191 0.152 0.136 0.229 0.131 0.226 0.121nb-ntm 0.188 0.223 0.248 0.245 0.127 0.125 0.151 0.187gnb-ntm 0.240 0.228 0.237 0.255 0.127 0.093 0.163 0.1910.2660.2010.2690.2060.1850.2060.234.hdpitm-vaehitm-vaercrphldatsntmntsntm.
0.1780.1610.179––0.2250.224.
0.1920.1950.2370.1860.2210.2120.237.
0.1570.1840.2330.2010.1860.2130.237.table 3: npmi of each model, where the best result ismarked in bold.
the topic numbers of parametric mod-els are set to 50 and 200, and those of nonparametricmodels are automatically determined..set of rcv1-v2 to run our model and other tree-structured baselines.
the sampling-based models(i.e., hlda and rcrp) are run on an intel xeonskylake 6133 cpu with 8 cores, and nvi-basedmodels (i.e., tsntm and ntsntm) are tested onan nvidia tesla v100 gpu.
figure 8 shows thetraining time of these topic models.
our ntsntmshows an advantage in data scalability when com-pared with baselines.
although tsntm is alsoscalable to a large corpus by gpu acceleration, itapplies a doubly-recurrent network which largelyslows down the model speed.
hlda and rcrpspend considerable computation time on path sam-pling, which is much more serious when dealingwith a large-scale dataset.
additionally, these twosampling-based models are serial, which meansthey can only utilize one core of the cpu..figure 8: training time of different models on variousnumbers of documents.
the curves of hlda and rcrpare incomplete because the time they cost is not com-parable.
particularly, it costs over 48 hours for trainingwhen the numbers of documents are larger than 50k and100k for hlda and rcrp, respectively..4.6.impact of the concentration parameter.
we further validate the nonparametric property ofour model.
figure 9 shows the impact of β0 on the.
number of active topics.
firstly, we can see that thetopic numbers of all models grow when increasingβ0.
the reason is that β0 controls the smoothnessof sbp, and that a larger value leads to a smootherdegree, i.e., more topics.
secondly, compared withitm-vae and hitm-vae, the number of topicsfound by ntsntm is closer to the one extracted byhdp, which demonstrates that our model is able toapproximate the nonparametric property of hdp..figure 9: active topic numbers of different modelswith various values of β0..5 conclusion.
in this paper, we propose a nonparametric tree-structured neural topic model named ntsntm.
our method explicitly models the dependency of la-tent variables from different layers, and combinesthem to reconstruct the input text.
by couplingsbp with dependency matrices, we can update thetree structure automatically.
extensive experimentsvalidate the effectiveness of our ntsntm on gen-erating a reasonable topic tree with low topic re-dundancies.
furthermore, our model can be trained2 times faster than the existing nvi-based tsntmwith approximately 800k documents.
in the future,we plan to apply our method to aspect extraction..acknowledgment.
we are grateful to the reviewers for their construc-tive comments and suggestions on this study.
thiswork has been supported by the national natu-ral science foundation of china (61972426) andguangdong basic and applied basic researchfoundation (2020a1515010536)..2351100000200000300000400000500000600000700000800000number of documents103104105time(sec.
)hldarcrptsntmntsntm051015202530β0050100150active topic numbers20newshdpitm-vaehitm-vaentsntmreferences.
david alvarez-melis and t. jaakkola.
2017. tree-structured decoding with doubly-recurrent neuralin proceedings of the 5th internationalnetworks.
conference on learning representations..david m. blei, thomas l. grifﬁths, and michael i. jor-dan.
2010. the nested chinese restaurant processand bayesian nonparametric inference of topic hier-archies.
journal of the acm, 57(2):1–30..david m. blei, andrew y. ng, and michael i. jordan.
2003. latent dirichlet allocation.
journal of ma-chine learning research, 3(1):993–1022..sophie burkhardt and stefan kramer.
2019. decou-pling sparsity and smoothness in the dirichlet varia-tional autoencoder topic model.
journal of machinelearning research, 20(131):1–27..zoubin ghahramani, michael jordan, and ryan padams.
2010. tree-structured stick breaking for hi-erarchical data.
in advances in neural informationprocessing systems, pages 19–27..thomas l. grifﬁths, michael i. jordan, joshua tenen-baum, and david m. blei.
2004. hierarchical topicmodels and the nested chinese restaurant process.
inadvances in neural information processing systems,pages 17–24..geoffrey e. hinton, oriol vinyals, and jeffrey dean.
2015. distilling the knowledge in a neural network.
corr, abs/1503.02531..hemant.
ishwaran and lancelot f. james.
2001.gibbs sampling methods for stick-breaking priors.
journal ofthe american statistical association,96(453):161–173..masaru isonuma, junichiro mori, danushka bollegala,and ichiro sakata.
2020. tree-structured neuralin proceedings of the 58th annualtopic model.
meeting of the association for computational lin-guistics, pages 800–806..joon hee kim, dongwoo kim, suin kim, and alice h.oh.
2012. modeling topic hierarchies with the recur-in proceedings ofsive chinese restaurant process.
the 21st acm international conference on informa-tion and knowledge management, pages 783–792..suin kim, jianwen zhang, zheng chen, alice h. oh,and shixia liu.
2013.a hierarchical aspect-sentiment model for online reviews.
in proceedingsof the aaai conference on artiﬁcial intelligence,pages 526–533..diederik p. kingma and jimmy ba.
2015. adam: amethod for stochastic optimization.
in proceedingsof the 3rd international conference on learningrepresentations..p. kumaraswamy.
1980. a generalized probability den-sity function for double-bounded random processes.
journal of hydrology, 46:79–88..jey han lau, david newman, and timothy baldwin.
2014. machine reading tea leaves: automaticallyevaluating topic coherence and topic model quality.
in proceedings of the 14th conference of the euro-pean chapter of the association for computationallinguistics, pages 530–539..wei li and andrew mccallum.
2006. pachinko allo-cation: dag-structured mixture models of topic cor-relations.
in proceedings of the 23rd internationalconference on machine learning, pages 577–584..luyang liu, heyan huang, yang gao, yongfengzhang, and xiaochi wei.
2019. neural variationalin proceeding of thecorrelated topic modeling.
world wide web conference, pages 1142–1152..rui liu, xingguang wang, deqing wang, yuan zuo,he zhang, and xianzhu zheng.
2018. topic split-ting: a hierarchical topic model based on non-journal of systemsnegative matrix factorization.
science and systems engineering, 27:479–496..tengfei liu, nevin l zhang, and peixian chen.
2014.hierarchical latent tree analysis for topic detection.
in proceedings of the 2014 european conferenceon machine learning and knowledge discovery indatabases, pages 256–272..yishu miao, edward grefenstette, and phil blunsom.
2017. discovering discrete latent topics with neuralvariational inference.
in proceedings of the 34th in-ternational conference on machine learning, pages2410–2419..david mimno, wei li, and andrew mccallum.
2007.mixtures of hierarchical topics with pachinko alloca-tion.
in proceedings of the 24th international con-ference on machine learning, pages 633–640..zhao-yan ming, kai wang, and tat-seng chua.
2010.prototype hierarchy based clustering for the catego-rization and navigation of web collections.
in pro-ceedings of the 33rd international acm sigir con-ference on research and development in informationretrieval, pages 2–9..eric t. nalisnick and padhraic smyth.
2017. stick-in proceedingsbreaking variational autoencoders.
of the 5th international conference on learningrepresentations..feng nan, ran ding, ramesh nallapati, and bing xi-ang.
2019. topic modeling with wasserstein autoen-in proceedings of the 57th annual meet-coders.
ing of the association for computational linguistics,pages 6345–6381..diederik p. kingma and max welling.
2014. auto-in proceedings of theencoding variational bayes.
2nd international conference on learning represen-tations..xuefei ning, yin zheng, zhuxi jiang, yu wang,huazhong yang, junzhou huang, and peilin zhao.
2020. nonparametric topic modeling with neural in-ference.
neurocomputing, 399:296–306..2352john paisley, chong wang, david m. blei, andmichael i. jordan.
2014. nested hierarchical dirich-let processes.
ieee transactions on pattern analy-sis and machine intelligence, 37(2):256–270..jim pitman.
2006. combinatorial stochastic processes.
in technical report 621, dept.
statistics, uc berke-ley..danilo jimenez rezende, shakir mohamed, and daanwierstra.
2014. stochastic backpropagation and ap-proximate inference in deep generative models.
inproceedings of the 31st international conference onmachine learning, pages 1278–1286..jayaram sethuraman.
1994. a constructive deﬁnitionof dirichlet priors.
statistica sinica, pages 639–650..yee teh, michael jordan, matthew beal, and davidblei.
2005. sharing clusters among related groups:in advanceshierarchical dirichlet processes.
in neural information processing systems, pages1385–1392..felipe viegas, washington cunha, christian gomes,antˆonio pereira, leonardo c. da rocha, and mar-cos andr´e gonc¸alves.
2020. cluhtm - semantic hi-erarchical topic modeling based on cluwords.
inproceedings of the 58th annual meeting of the asso-ciation for computational linguistics, pages 8138–8150..chong wang and david m. blei.
2009. variational in-ference for the nested chinese restaurant process.
inadvances in neural information processing systems,pages 1990–1998..jiemin wu, yanghui rao, zusheng zhang, haoran xie,qing li, fu lee wang, and ziye chen.
2020. neu-ral mixed counting models for dispersed topic dis-in proceedings of the 58th annual meet-covery.
ing of the association for computational linguistics,pages 6159–6169..yueshen xu, jianwei yin, jianbin huang, and yuyuyin.
2018. hierarchical topic modeling with auto-matic knowledge mining.
expert systems with ap-plications, 103:106–117..2353