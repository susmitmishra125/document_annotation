controllable open-ended question generationwith a new question type ontology.
shuyang cao and lu wangcomputer science and engineeringuniversity of michiganann arbor, mi{caoshuy, wangluxy}@umich.edu.
abstract.
we investigate the less-explored task of gener-ating open-ended questions that are typicallyanswered by multiple sentences.
we ﬁrst de-ﬁne a new question type ontology which differ-entiates the nuanced nature of questions bet-ter than widely used question words.
a newdataset with 4, 959 questions is labeled basedon the new ontology.
we then propose anovel question type-aware question generationframework, augmented by a semantic graphrepresentation, to jointly predict question fo-cuses and produce the question.
based on thisframework, we further use both exemplars andautomatically generated templates to improvecontrollability and diversity.
experiments ontwo newly collected large-scale datasets showthat our model improves question quality overcompetitive comparisons based on automaticmetrics.
human judges also rate our modeloutputs highly in answerability, coverage ofscope, and overall quality.
finally, our modelvariants with templates can produce questionswith enhanced controllability and diversity..1.introduction.
question-asking has long served as an effective in-strument for knowledge learning (andre, 1979; to-bin, 1990) and assessing learning progress (holme,2003; downing and yudkowsky, 2009; livingston,2009).
compared to the widely studied task of gen-erating factoid questions that inquire about “onebit” of information (du et al., 2017; duan et al.,2017; li et al., 2019), this work is interested ingenerating open-ended questions that require deepcomprehension and long-form answers (labutovet al., 2015).
such open-ended questions are valu-able in education, e.g., to facilitate complex knowl-edge acquisition (lai et al., 2017) and nurture rea-soning skills (shapley, 2000), as well as in otherapplications like improving search engines (han.
input: it’s a difﬁcult task to undertake.
teenagers tend toidentify gangs with “ﬁtting” in.
peer pressure plays a largepart in it and sometimes teenagers have problems with theirown identity being part of a gang deals with those issues.
italso provides a little bit of respect on the street ....bart sampling:- how do you stop a teen from joining a gang?
(procedural)- how do you get teenagers to stop being in gangs?
(procedural)- how do you get teens out of gangs?
(procedural)bart + qword:- how do you get a teenager out of a gang?
(procedural)- what is the best way to get teenagers out of gangs?
(procedural)- why do teenagers join gangs?
(cause)tplgen:- how do i get [np] to quit being in [np]?
⇒⇒ how do iget my son to quit being in a gang?
(procedural)- what are [np]?
⇒⇒ what are some programs for teenagersinvolved in gangs?
(example)- why do [np] [v] [np]?
⇒⇒ why do teenagers identifygangs?
(cause).
figure 1: open-ended questions generated by differ-ent models after reading the same input: (1) bart de-coded with nucleus sampling, (2) bart that considersdifferent question words, and (3) our type-aware gener-ator tplgen, that predicts focuses and operates withgenerated templates (to the left of the arrows).
ques-tions generated by our model have diverse types..et al., 2019) and building open-domain dialoguesystems (shum et al., 2018)..signiﬁcant progress has been made in generat-ing factoid questions (zhang and bansal, 2019;zhou et al., 2019b; su et al., 2020), yet new chal-lenges need to be addressed for open-ended ques-tions.
first, specifying the question type is crucialfor constructing meaningful questions (graesseret al., 1992).
question words such as “why” and“when” are generally seen as being indicative oftypes (zhou et al., 2019b), but they underspecifythe conceptual content of questions (olney et al.,2012).
using figure 1 as an example, different.
proceedingsofthe59thannualmeetingoftheassociationforcomputationallinguisticsandthe11thinternationaljointconferenceonnaturallanguageprocessing,pages6424–6439august1–6,2021.©2021associationforcomputationallinguistics6424question words, i.e., both “how” and “what”, canit thusbe used for inquiring about procedures.
calls for a new question type ontology that can pre-cisely capture the conceptual nature of questions.
second, constructing questions from a text withmultiple sentences needs to focus on its centralconcepts or phenomena that necessitate extensivedescriptions.
new representations are needed tocapture such content as question focus(es), to gobeyond existing methods that rely on entities andtheir neighboring words (du et al., 2017; sun et al.,2018) even though they are effective for generatingfactoid questions.
third, encouraging the diversityof generated questions (sultan et al., 2020; wanget al., 2020) is less explored but critical for realworld applications, e.g., various questions shouldbe proposed to gauge how well students grasp theknowledge of complex subjects..in this work, we aim to address the challengesof generating open-ended questions from inputconsisting of multiple sentences.
we ﬁrst in-troduce a new question type ontology, drawnupon researches in cognitive science and psychol-ogy (graesser et al., 1992), to capture deeper levelsof cognition, such as causal reasoning and judg-ments.
based on the new ontology, we collect andannotate a dataset of 4,959 questions to beneﬁt re-search in both question generation and answering.1we then design a type-aware framework tojointly predict question focuses (what to ask about)and generate questions (how to ask it).
differentfrom pipeline-based approaches (e.g., sun et al.
(2018)), our framework is built on large pre-trainedbart (lewis et al., 2020), and uses shared repre-sentations to jointly conduct question focus predic-tion and question generation while learning task-speciﬁc knowledge.
it is further augmented by a se-mantic graph that leverages both semantic roles anddependency relations, facilitating long text compre-hension to pinpoint salient concepts..moreover, to achieve the goal of producing var-ious types of questions from the same input, weinvestigate two model variants that use templatesto improve controllability and generation diver-sity: one using pre-identiﬁed exemplars, the otheremploying generated templates to guide questionwriting, with sample outputs displayed in figure 1.for experiments, we collect two new large-scaledatasets consisting of open-ended questions with.
1our data and code are available at: https://.
shuyangcao.github.io/projects/ontology_open_ended_question..answers from (1) yahoo answers2 l6 dataset and(2) popular question-asking communities on red-dit3, consisting of 291k and 720k question-answerpairs, respectively.
compared to existing popularqa datasets, such as squad (rajpurkar et al.,2016) and ms marco (bajaj et al., 2016)), ques-tions in our datasets ask about complex phenomenaand perplexing social issues that seek solutions ex-pressed in a long form.
automatic metrics showthat our type-aware question generation model out-performs competitive comparisons, highlightingthe effectiveness of semantic graph-augmented rep-resentation and joint modeling of focus predictionand question generation.
human judges also con-ﬁrm that questions generated by our model havebetter overall quality.
adding templates furtherpromotes question diversity, as evaluated by bothautomatic evaluation and human assessment..2 related work.
question generation has long been studied to re-duce human efforts in constructing questions forknowledge learning evaluation (mitkov and ha,2003; brown et al., 2005).
early work relies onsyntactic transformation to convert declarative sen-tences to questions (heilman and smith, 2010;chali and hasan, 2015).
recent advancementsrely on sequence-to-sequence models to generatea question from a given sentence or paragraph byconsidering the focus, type, and general-speciﬁcrelations of questions (sun et al., 2018; zhou et al.,2019b; krishna and iyyer, 2019).
in particular,question likelihoods and rewards are designed tosteer them toward being addressed by the given an-swers (zhou et al., 2019a; zhang and bansal, 2019).
attempts are also made toward creating complexquestions that require multi-hop reasoning over thegiven text, and graph-based representations havebeen an enabling tool to facilitate the access to bothentities and relations (pan et al., 2020; su et al.,2020).
while our model also enhances the inputwith a semantic graph, it boasts a richer representa-tion by including both dependency and semantic re-lations, with predicted question focuses highlightedvia extra node embeddings.
moreover, we create aseparate layer of cross attentions that is dedicatedto the semantic graph, while prior work uses thesame set of attentions to attend to the concatenatedtext and graph representations..2https://answers.yahoo.com/3https://www.reddit.com/.
6425given the data-driven nature of question gen-eration and answering tasks, recent studies takeadvantage of the availability of large-scale qadatasets, such as squad (rajpurkar et al., 2016),ms marco (bajaj et al., 2016), hotpotqa (yanget al., 2018), drop (dua et al., 2019), inter alia.
these corpora mainly contain factoid questions,while our newly collected datasets are not onlylarger in size but also comprise signiﬁcantly moreopen-ended questions for querying reasons and pro-cedures.
a dataset closer to ours is eli5 (fan et al.,2019), which also obtains open-ended question-answer pairs from reddit, while one of our datasetsincludes more reddit communities and thus coversa wider range of topics..our work is more inline with generating deeperquestions with responses that span over multiplesentences, where manually constructed templatesare found effective (olney et al., 2012).
for ex-ample, labutov et al.
(2015) use crowdsourcingto collect question templates based on an ontologyderived from wikipedia and freebase topics.
dif-ferent from the topic-based ontology, our questiontypes are more aligned with cognitive levels.
more-over, our templates are automatically learned fromtraining data.
recent work (rao and daum´e iii,2018, 2019) focuses on asking clariﬁcation ques-tions based on both retrieval and generation models.
as there has been no suitable framework for diversetypes of questions, this work aims to ﬁll the gap byintroducing type-aware generation models whichoptionally leverage question templates for bettercontrollability..generating diverse questions is much less stud-ied, with existing approaches mainly focusing onentity replacement (cho et al., 2019), sampling de-coding (sultan et al., 2020; wang et al., 2020), andpost-ﬁltering (liu et al., 2020).
however, the pro-duced diversity is driven by word choice and syntaxvariation, with little ability to control on questiontypes, which is the focus of this work..3 data collection and question type.
annotation.
3.1 open-ended question datasets.
to collect open-ended questions, we resort to on-line forums with active question-asking discussions.
concretely, we gather and clean question-answerpairs from reddit and yahoo answers, to traingenerators that construct questions by taking thecorresponding answer as input..question type description (asking for...).
verification the truthfulness of an event or a concept..disjunctive the true one given multiple events or concepts,.
where comparison among options is not needed..concept.
a deﬁnition of an event or a concept..extent.
the extent or quantity of an event or a concept..example.
example(s) or instance(s) of an event or a concept..comparison comparison among multiple events or concepts..cause.
the cause or reason for an event or a concept..consequence the consequences or results of an event..procedural the procedures, tools, or methods by which a cer-.
tain outcome is achieved..judgmental the opinions of the answerer’s own..table 1: our new question type ontology, which isadopted and modiﬁed from olney et al.
(2012).
typesare sorted by levels of cognition (lower to higher)..we choose ﬁve popular reddit communi-ties: r/askhistorians, r/ask politics,r/askscience, r/explainlikeimfive,and r/askreddit, where open-ended questionsare actively asked.
the original posts (ops) areextracted, with their titles becoming questions.
wealso keep the best answer with the highest karma(i.e., upvotes minus downvotes) if it is greater than1. a second dataset with question-answer pairsis collected from the yahoo answers l6 corpus4,which covers a broader range of topics than thereddit data.
for each question, the best answer israted by the user who raises the question..preprocessing.
to ensure both questions and an-swers are well-formed, human inspection is con-ducted in multiple iterations to design rules to ﬁlterout improper samples.
for instance, we discardsamples whose answers have less than 15 contentwords to avoid the inclusion of factoid question.
more details are provided in table 6 in appendix a.ultimately, 719,988 question-answer pairs are keptfor reddit, and 290,611 for yahoo.
each dataset isthen divided into train, validation and test sets witha 90%/5%/5% split.
the average lengths of ques-tions and answers are 14.5 and 117.8 for reddit,and 12.2 and 123.6 for yahoo..3.2 question type ontology and annotation.
our question type ontology is adopted and modi-ﬁed from olney et al.
(2012), where 18 categoriesare originally proposed for knowledge learning as-.
4https://webscope.sandbox.yahoo.com/.
6426sessment.
we recruited 6 native english speakersfor three rounds of question type annotation.
basedon the annotators’ feedback after each round, wereﬁne the deﬁnitions, merge ambiguous types, anddelete inapplicable categories.
for example, an ini-tial expectation type is merged into cause dueto their similarities in seeking causality.
finally, 10types are preserved (table 1).
as can be seen, ourontology is designed to better capture the nature ofquestions than question words..annotating questions with types.
after the an-notation guideline is ﬁnalized, we ask the sameset of annotators to label 5,000 (2 × 2,500) ran-domly sampled questions from both reddit andyahoo’s training sets.
each question is labeledby two annotators, with disagreements resolvedthrough discussions.
after removing samples with-out consensus, the ﬁnal dataset consists of 4, 959questions.
example questions are most prevalent,comprising 23.4% of samples, while only 2.6% areconsequence questions.
a krippendorff’s α of0.67 is obtained for all samples, indicating a rea-sonable agreement level.
the annotation guidelineand examples for each question type are shown intable 12 in appendix a..training question type classiﬁers.
since ourtype-aware question generation model requires aspeciﬁed type as input, here we describe how tobuild two question type classiﬁers: (1) γq, thatlabels a type by reading the question and is usedto provide question type labels during training; (2)γa, that predicts a type for use by taking the answeras input and is used during test..both classiﬁers are based on roberta (liuet al., 2019), where a prediction layer is built ontop of the contextual representation of the [bos]token to output question type probabilities.
γqachieves a macro f1 score of 0.80 on a reservedtest set, with data splits detailed in appendix b.to train γa, in addition to the annotated questions,we run γq on unlabeled questions in reddit andyahoo and include samples whose type predictionconﬁdence score is above 0.9. we train one γa foreach dataset.
γa obtains macro f1 scores of 0.48and 0.46 on the same reserved test set over all typesafter training on yahoo and reddit, respectively..after running γq on both datasets, we ﬁnd thatreddit has signiﬁcantly more example questions(43.8% of all samples).
yahoo dataset is morebalanced, with procedural questions being themost frequent type (19.9% of all samples).
distri-.
figure 2: our type-aware open-ended question genera-tion framework.
detecting question focuses (nodes indarker color) and generating questions (or templates)are jointly learned.
we only show a partial semanticgraph.
special tokens are also inserted to segment dif-ferent parts of the input.
jointgen uses the type andthe answer for question generation; explgen furtherconsiders an exemplar; and tplgen uses all three fortemplate generation..butions of question types for the two datasets arelisted in table 8 in appendix b..4 type-aware open-ended question.
generation.
in this section, we present our type-aware questiongeneration framework.
as shown in figure 2, ourmodel takes in a multi-sentence text and a predictedquestion type.
built on shared input representa-tions, it ﬁrst detects question focuses from a seman-tic graph, and then generates the question (§ 4.1).
we also propose two model variants that considerautomatically extracted template exemplars or gen-erated templates to achieve controllability (§ 4.2),enabling the generation of diverse questions..4.1.joint focus prediction and questiongeneration (jointgen).
our generator is built on top of bart (lewis et al.,2020).
to facilitate the detection of salient content(i.e., focuses) to raise questions, we ﬁrst augmentthe encoder with a semantic graph that consists ofboth dependency relations and semantic roles, cap-turing semantic relations over different scopes withvarying granularities.
question focuses are ﬁrstdetected based on the semantic graph, which thenguide question generation via cross-attentions, asshown in figure 2. although the joint modeling offocus prediction and question generation has beenstudied before, our design differs by using shared.
6427encoderselfattncrossattncrossattndecoderlayerquestion [explgen]: why does music sound louder sometimes?template [tplgen]: why do [np] [v] [adjp] on [np]?type:causeanswer: your ears adjust to volume over time.
what sounds load at first will become softer …exemplar: why do [np] [v]?soundsloudfirstbecomesofterwhatnsubjarg1xcomparg2argm-tmpxcomparg2advclcsubjarg1typeanswerexemplar𝑯shared representation𝑽!%focusprediction𝑔"𝒗"(!)𝑯𝒗"(!)
representations consisting of the input text and se-mantic graph, and the prediction of focuses areincluded through gating mechanisms, whereas pre-vious work, e.g.
pan et al.
(2020), simply employsmulti-task learning.
below, we ﬁrst describe con-structing the semantic graph-augmented encoder,followed by the joint modeling of two tasks..improving long text comprehension with se-mantic graph.
to construct the semantic graph,for each sentence, we start with obtaining its de-pendency tree using stanford corenlp (manninget al., 2014).
to better highlight core concepts, wediscard less important relations, e.g., auxiliaries.
the full list is included in appendix c. since ourgoal is to detect central concepts that are well con-nected with many other words, we can removerelations on the edges to minimize the number ofparameters to learn.
moreover, as semantic rolescan indicate main entities (mannem et al., 2010),we extract semantic roles and their relations withallennlp (shi and lin, 2019).
to merge the twosources of information, we add an edge in the de-pendency tree to connect the head word of the pred-icate and the head word of each semantic role.
tobuild a connected graph from the multi-sentenceinput, we add an edge between each sentence’s lasttoken and the next sentence’s ﬁrst token.
finally,we merge nodes with the same surface forms orwith corefered mentions.
to the best of our knowl-edge, this is the ﬁrst time that both dependency andsemantic relations are encoded in the same graphfor question generation, and with enhanced con-nectivity of the constructed graph, our design canbetter signal content salience..joint modeling with cross-attentions.
given apredicted question type t and a multi-sentence textx = {x1, · · · , xn}, the bart encoder builds thecontextual representation h = {h0, h1, · · · , hn}at the last layer, where h0 is for t..to encode the semantic graph, we initialize thenode representation for node vi by taking the av-erage contextual representations of its tokens andappending four bits encoding the number of nodes(capped at 10) that are merged into vi, to add fre-quency information.
this step yields new noderepresentations v(0).
we then apply graph atten-tion networks (gats) (veliˇckovi´c et al., 2018) ofl layers to update the representations as follows:.
i.v(l)i =.
ai,jw (l)v(l−1).
j.
(1).
(cid:88).
j∈ni.
where w (l) is a learnable parameter for the l-thlayer, and ni denotes the neighbors of vi.
theattention score ai,j is calculated as in gats.
weuse l = 2 for experiments..to predict focuses, the ﬁnal node representationis fed into the following feedforward network,.
v(l)iyielding the probability of vi being a focus as:.
pf ocus(vi = 1) = σ(w1 tanh(w2v(l).
)).
i.
(2).
where w1 and w2 are learnable parameters.
biasterms are omitted for simplicity.
we constructground-truth labels by treating a node as a focus ifit contains words used in the question..to generate the question, we use the gatingmechanism to inform the focus prediction re-sults, where new node representations after beingweighted by the focus probability are:.
v(l)(cid:48)i = gi (cid:12) v(l).
i.gi = pf ocus(vi = 1).
(3).
our model beneﬁts from both large pre-trainingand hybrid semantic graphs by adding a sepa-rate cross attention for node presentations in eachbart decoder layer.
we then design separatecross attentions to attend (1) the output of thebart encoder, yielding ze, and (2) the node repre-sentations v (l)(cid:48), producing zv, which are formu-lated as:.
ze = ln(zs + attn(zs, h))zv = ln(ze + attn(ze, v (l)(cid:48)z(cid:48) = ln(zv + ffn(zv)).
)).
(4).
(5).
(6).
where zs denotes the output of self attentions forthe current layer, and z(cid:48) is the output for the layer.
attn(·, ·) denotes the multi-head attention opera-tion as in vaswani et al.
(2017), ffn(·) a feed-forward layer, and ln(·) is layer normalization..our ﬁnal training objective accounts for bothfocus prediction and question generation objectiveswith equal weights..4.2 diversifying questions with templates.
(explgen & tplgen).
an important goal of this work is to enable thegeneration of questions of diverse types.
however,simply adding question type as input is insufﬁcient(discussed in § 5).
we thus propose to leveragequestion templates to gain stronger controllabil-ity.
below we ﬁrst present how to automatically.
6428extract templates from the training set, and thenintroduce two model variants that are built on thejointgen framework: explgen uses exemplartemplates to guide the model to generate questionsof selected types, and tplgen adds an extra stepto ﬁrst generate type-speciﬁc templates..template extraction.
while collecting templatesspeciﬁc to a given type, we need to ensure theyremain topic-independent to be generalizable to dif-ferent domains.
to this end, we replace a word inthe question with a template token that indicates itssyntax function, e.g., [v] for a verb, if it appearsin the answer after lemmatization.
we further con-sider topically related words in the questions, bycalculating word-level semantic similarities basedon numberbatch word embeddings (speer et al.,2017), which are found to perform better on ourdatasets than other embeddings.
concretely, foreach word in the answer, we replace the most sim-ilar word in the question with the template token.
this process is repeated until 80% of content wordsin questions are replaced.
finally, for each nounphrase, adjective phrase, and adverb phrase, if itshead word has been replaced, the whole phraseis transformed into a phrase type token.
for in-stance, a question “what are the differences be-tween global warming and climate change?” be-comes “what are the differences between [np]and [np]?”.
exemplars for guidance (explgen).
our ﬁrstmodel variant considers adding a template exem-plar for the given type as additional input, whichprovide more speciﬁc information to control thetype of generated questions.
figure 2 shows onesuch example.
to identify exemplars, we use tem-plates with frequencies above 20 on yahoo and50 on reddit.
we then manually inspect thesetemplates and remove the ones with topic-speciﬁcwords, resulting in 66 exemplars for all types.
theyare listed in table 10 in appendix d..during training, we choose the exemplar that hasthe lowest edit distance with the question, which isalso used for training an exemplar selector basedon roberta.
during testing, the exemplar withthe highest selector score is used.
the accuracy ofthe exemplar selector for each question type on thetest set is reported in table 11 in appendix d..generated templates for guidance (tplgen).
we further propose another model variant wherewe generate a new template and feed it (insteadof an exemplar template as in explgen) as part.
of the question generation input.
speciﬁcally, wereuse explgen to learn to generate a target tem-plate, as derived from the template extraction proce-dure.
during question realization, tplgen uses abart-based generator that takes as input the ques-tion type, the input text, the generated template,and the words that are predicted as focuses.
weuse separate cross attentions to attend the represen-tations of the focused words, similar to how noderepresentations are attended in jointgen..we recognize that having separate stages of ex-emplar selection and template generation intro-duces extra model training cost and potential errorsin the pipeline.
this work, however, focuses onimproving the controllability as well as diversity ofquestion generation, and we will leave the buildingof more efﬁcient models in the future work..5 experiment results.
5.1 automatic evaluation.
comparisons and metrics.
we compare withdeepqg (pan et al., 2020), a model that uses de-pendency graphs for multi-hop question generation.
we also compare with bart models that are ﬁne-tuned on the same datasets as in our models, by us-ing inputs of (1) the answer (bart), (2) the answerand a predicted question word (bart+qword),and (3) the answer and a predicted question type(bart+qtype).
for bart+qword, the ques-tion word is predicted by a roberta classiﬁer thatconsiders the answer and is trained on our train-ing sets.
we follow liu et al.
(2020) and use 9categories of question words.
for both our mod-els and bart+qtype, the most conﬁdent typepredicted by the classiﬁer γa (described in § 3.2),which reads in the answer, is used as input.
to testthe efﬁcacy of semantic graphs, we further com-pare with a variant of jointgen that only uses theﬂat transformer for focus prediction and questiongeneration, denoted as jointgen w/o graph..we evaluate the generated questions withbleu (papineni et al., 2002), meteor (lavieand agarwal, 2007), and rouge-l (lin, 2004).5results on both yahoo and reddit datasets are re-ported in table 2. our jointgen outperforms allcomparisons on both datasets over all automaticevaluation metrics except for meteor on reddit.
when taking out the semantic graphs, model perfor-mance degrades substantially, which suggests that.
5we do not consider using q-bleu (nema and khapra,.
2018) since it weighs question words highly..6429model.
b-4 mtr r-l b-4 mtr r-l.yahoo.
reddit.
–.
–.
–.
6.53 25.92 27.56deepqgbart21.88 38.01 39.16 19.45 35.46 37.82bart+qword 22.02 38.44 39.32 19.80∗ 35.85 38.48∗type-aware modelsbart+qtype 22.12 38.62 39.72 19.90∗ 35.83 38.68∗jointgen (ours) 22.56∗ 38.63 40.40∗ 20.09∗ 35.75 39.07∗w/o graph 22.21 38.21 39.93 19.81∗ 35.60 38.47∗explgen (ours) 21.74 37.52 39.70 18.67 33.28 36.7421.51 36.55 39.63 17.83 31.69 36.05tplgen (ours).
table 2: automatic evaluation results on yahoo andreddit with bleu-4 (b-4), meteor (mtr) androuge-l (r-l).
jointgen outperforms compar-isons over all metrics except for meteor on reddit,but removing its graph degrades performance.
we onlyreport results by deepqg on yahoo due to memory∗: signiﬁcantly better than bart (p <limitation.
0.005 with approximate randomization test)..model.
acc↑ unt↑ pair↓ acc↑ unt↑ pair↓.
yahoo.
reddit.
––.
2.493.21.
23.9441.77.bartbart+qwordtype-aware models80.91bart+qtype 22.47 2.2760.7243.90 4.03jointgen75.47∗ 6.98∗ 25.12 65.79∗ 6.23∗ 22.17explgen76.35∗ 7.08∗ 24.93 66.49∗ 6.32∗ 22.13tplgen.
44.39 23.67 2.2063.06 22.75 2.33.
21.1126.61.
2.033.40.
––.
table 3: automatic evaluation on controllability anddiversity by specifying 9 different question types.
wereport type accuracy (acc), number of unique types(unt), and pairwise bleu-4 (pair).
our explgenand tplgen achieve stronger controllability by re-specting the given question types more, as well as showhigher diversity than comparisons except for bartwith nucleus sampling.
∗: signiﬁcantly better than allcomparisons (p < 0.005)..having structured representation is useful for fo-cus detection and the ﬁnal question generation task.
we also observe a huge performance gap betweendeepqg and systems based on bart, signifyingthe importance of leveraging pre-trained modelsfor open-ended question generation.
meanwhile,adding question types helps bart generate morerelevant questions than using question words, indi-cating the value of our new question type ontology.
notably, our template-based generators, ex-plgen and tplgen, which are trained to complywith the given templates, still produce comparablescores.
this highlights the possibility to control thegenerated questions’ types and syntax as demon-strated by the templates, without performance loss..figure 3: number of unique types of the generatedquestions (y-axis), when different numbers of questiontypes are speciﬁed (x-axis)..ine the controllability of models by specifying dif-ferent question types as input.
the top 9 conﬁ-dent types6 predicted by our type predictor γa areused as input to our models, producing 9 questionsfor evaluation.
for bart, we use nucleus sam-pling (holtzman et al., 2020) with k = 10 andp = 0.7 to sample diverse questions..to evaluate, we ﬁrst calculate the question typeaccuracy by comparing whether the types of thegenerated questions match the speciﬁed ones, withtypes labeled by our classiﬁer γq (§ 3.2).
we thenreport the average numbers of unique questiontypes in the 9 generated questions per sample, withhigher number indicating better controllability.
fi-nally, we consider pairwise bleu-4 (cho et al.,2019) by computing the bleu-4 between pairwisegenerated questions per sample, where lower val-ues suggest higher content diversity..first, our explgen and tplgen can generatequestions with diverse types and content, as shownby the signiﬁcantly higher numbers of unique typesthan all comparisons and lower pairwise bleuscores than comparisons except for bart with nu-cleus sampling in table 3. this implies strongertype control by template-based generators, com-pared to bart+qtype and jointgen whichonly use the question type token as input.
resultson numbers of unique types by varying numbers ofquestion types speciﬁed in the input are displayedin figure 3, where explgen and tplgen main-tain steady controllability..second, our question type ontology provides anew perspective for question diversity evaluation.
among the comparisons, although bart with nu-cleus sampling and bart+qword both havelow pairwise bleu, the types of questions theycan generate are limited..69 types are chosen because we only have 9 categories of.
question diversity evaluation.
next, we exam-.
question words for bart+qword..643023456789# of given types234567# of unique typesyahoo23456789# of given types234567redditbart + qwordjointgenexplgentplgenmodel.
type (%) syntax (%) content (%).
model.
type (%) syntax (%) content (%).
bartbart+qwordexplgentplgen.
23.338.377.981.2.bart+qwordjointgenexplgentplgen.
40.063.384.278.7.
11.757.561.258.3.
50.031.235.037.9.
(a) yahoo.
(b) reddit.
36.736.758.364.2.
22.920.445.860.4.table 4: percentage of samples marked as having themost diverse question types, syntax structures, and con-tent of answers.
ties are allowed.
the krippendorf’sαs for the three aspects are 0.56, 0.43, and 0.25 on ya-hoo, and are 0.54, 0.38, and 0.38 on reddit..5.2 human evaluation.
question diversity.
we hire three annotators whohave participated in our question type annotationstudy to evaluate 80 groups of questions generatedby four selected models on each dataset.
for eachgroup, we randomly sample an answer and indicatethree most probably question types to each model,to generate three corresponding questions..for each sample, the annotators are asked torank the four models from 1 (highest) to 4 (lowest)on three aspects of diversities: type–whether thethree generated questions have different types, syn-tax–whether they use different syntax, and answercontent–whether the three questions need to beaddressed with different answers.
ties are allowed.
we ﬁnd that human judges rate questions gen-erated by our explgen and tplgen as hav-ing greater diversities over all aspects, exceptfor syntax diversity on reddit, as shown in ta-ble 4. among the two model variants, questions bytplgen yield more diverse answers.
based on ourobservation, tplgen uses automatically generatedtemplates to produce more focused questions withdifferent answers, compared to explgen whichemploys exemplars.
this shows the promise ofusing automatically generated templates to createquestions that need to be addressed with differentanswers.
besides figure 1, we show more sampleoutputs in figure 4, where explgen and tplgenexhibit stronger controllability than jointgen.
question content quality.
we use the same setof human judges to evaluate another 80 groups ofquestions output by ﬁve selected models and thereference.
three aspects are rated from 1 (worst).
answer: my sister in law and her husband “geneticallymodiﬁed” their second child because the ﬁrst has eb.
theyeliminated that and had a baby that gets to live pain free.
under the right circumstances, i’m all for it ...jointgen[procedural] how would you feel about geneticallymodiﬁed babies?
[judgmental] what are your thoughts on geneticallymodiﬁed babies?
[verification] is it possible to genetically modify achild?
explgen[procedural] how do people genetically modify theirchildren?
[judgmental]what do you think about genetically modi-ﬁed babies?
[verification] can you genetically modify a baby tohave a better chance of survival?
tplgen[procedural] how do people genetically modify theirchildren?
[judgmental] what do you think about genetically mod-iﬁed babies?
[verification] can you genetically modify a baby?.
figure 4: sample outputs of our models given differ-ent question types.
spans that belong to the exemplarsor the generated templates are colored with blue.
gen-erated questions that do not match the given type aremarked by strikethrough..to 5 (best): appropriateness–whether the ques-tion is semantically correct, without consideringthe answer; answerability–whether the questioncan be addressed by the given answer; and scope–whether the question is related to a longer span ofthe answer (global scope) or focuses on local con-tent (e.g., one phrase or one sentence).
we furtherask the annotators to rank questions based on theiroverall quality and preferences, with ties allowed.
as shown in table 5, our jointgen modelproduces questions with better answerability anditthat cover broader content in the answers.
is also rated as the best in more than half ofthe evaluation instances on both datasets.
be-tween bart+qword and bart+qtype, hu-man judges rate the system outputs that conditionedon our question types to have better overall quality..5.3 further analyses.
does focus prediction correlate with questionquality?
we ﬁrst investigate the relationship be-tween focus prediction and question generation byusing our joint model jointgen.
as can be seenfrom figure 5, there is a strong correlation betweenf1 scores of focus prediction and bleu-4 as well.
6431model.
appro.
ans..scp..top 1.reference.
bartbart+qwordbart+qtypejointgentplgen.
reference.
bartbart+qwordbart+qtypejointgentplgen.
4.77.
4.934.864.924.904.92.
4.90.
4.894.884.884.844.81.
3.96.
4.024.144.234.254.19.
4.43.
4.274.294.394.454.21.
3.79.
3.813.853.943.963.87.
4.37.
4.214.214.264.384.19.
34.5%.
39.7%40.8%48.7%50.5%46.4%.
47.1%.
43.9%46.7%49.6%50.3%33.1%.
(a) yahoo.
model.
appro.
ans..scp..top 1.
(b) reddit.
table 5: human evaluation on appropriateness (ap-pro.
), answerability (ans.
), scope of answers (scp.
),and percentage of questions rated as having the bestoverall quality (top 1), with ties allowed.
jointgenis rated as with higher answerability and with answersrequiring a broader scope.
the krippendorff’s αs forthe four aspects are 0.42, 0.56, 0.50, 0.39 on yahoo,and are 0.34, 0.46, 0.43, 0.35 on reddit..as rouge-l, where samples in the yahoo andreddit test sets are grouped into 8 bins based onthe f1 scores.
the pearson correlation coefﬁcientsbetween bleu-4 and focus f1 are 0.29 on yahooand 0.26 on reddit.
for rouge-l, the correla-tion coefﬁcients are 0.35 on yahoo and 0.34 onreddit.
all the correlations have p < 10−5.
thestrong positive correlations imply the importanceof accurate focus prediction for open-ended ques-tion generation.
we also show the f1 scores andbleu-4 for selected question types on the right offigure 5, again demonstrating the effect of focusdetection on question quality..when do our models fail to respect the giventypes?
next, we provide insights into whichtypes of questions are challenging to generate byusing our template-based models explgen andtplgen.
both variants frequently fail to respectthe given question type of verification, in whichcases they often produce judgemental questions.
they also tend to confuse example and extentwith concept questions.
after manually inspect-ing 50 generated questions for the aforementionedthree types, we ﬁnd that many of them can be la-beled with both types, thus creating confusion forour classiﬁer.
for instance, “what are the importrestrictions in the us?” can be considered as either.
(a) yahoo.
(b) redditfigure 5: on both yahoo and reddit, we ﬁnd posi-tive effect of focus prediction on question quality (mea-sured by bleu-4 and rouge-l) (left), which is alsobroken down by question types (right).
disj.
: dis-junctive, veri.
: verification, conc.
: concept,comp.
: comparison..asking for a deﬁnition or for examples.
therefore,future work should include designing multi-classtype identiﬁcation models..6 conclusion.
we present a new question type ontology whichbetter captures the nuances of questions to supportthe study of open-ended question generation.
wefurther annotate a new dataset with 4,959 ques-tions based on the proposed ontology.
we describea joint question focus detection and question gen-eration framework with a novel semantic graph-augmented representation, which is directly builton large pre-trained models.
based on this frame-work, we also enhance the controllability and diver-sity of generated questions by employing templateexemplars or automatically generated templates.
experiments on two large datasets show that ques-tions generated by our models have better qualityand higher diversity than non-trivial comparisons,with similar results rated by human judges..acknowledgements.
this research is supported in part by national sci-ence foundation through grants iis-1813341 anda career award iis-2046016.
we thank threeanonymous reviewers, area chair, and senior areachairs for their valuable suggestions for improvingvarious aspects of this work..64320.40.60.81.0focus f120304050bleu-4rouge-ldisj.veri.conc.comp.051015202530bleu-4bleu-4focus f10.00.10.30.40.50.70.8focus f10.40.60.81.0focus f11020304050bleu-4rouge-ldisj.veri.conc.comp.0510152025bleu-4bleu-4focus f10.00.20.30.50.60.8focus f1ethics statement.
large models that are pre-trained on heterogeneousweb data are shown to encode biases and can bepotentially harmful for marginalized populations.
while the automatically learned templates im-prove controllability in question generation, wealso recognize that our system might be misused tocreate questions that contain objectionable content.
we therefore advocate cautious and responsiblepractices in real-world deployment..our data collection process for the two newdatasets involves removing samples with abusivelanguages and human inspection on random sam-ples.
given the data volume, however, we cannotexhaustively verify that all records are free of po-tentially offensive content..references.
thomas andre.
1979. does answering higher-levelquestions while reading facilitate productive learn-ing?
review of educational research, 49(2):280–318..payal bajaj, daniel campos, nick craswell, li deng,jianfeng gao, xiaodong liu, rangan majumder,andrew mcnamara, bhaskar mitra, tri nguyen,et al.
2016. ms marco: a human generated machinearxiv preprintreading comprehension dataset.
arxiv:1611.09268..jonathan brown, gwen frishkoff, and maxine eske-nazi.
2005. automatic question generation for vo-cabulary assessment.
in proceedings of human lan-guage technology conference and conference onempirical methods in natural language processing,pages 819–826..yllias chali and sadid a hasan.
2015. towards topic-to-question generation.
computational linguistics,41(1):1–20..jaemin cho, minjoon seo, and hannaneh hajishirzi.
2019. mixture content selection for diverse se-quence generation.
in proceedings of the 2019 con-ference on empirical methods in natural languageprocessing and the 9th international joint confer-ence on natural language processing (emnlp-ijcnlp), pages 3121–3131, hong kong, china.
as-sociation for computational linguistics..vancouver, canada.
association for computationallinguistics..dheeru dua, yizhong wang, pradeep dasigi, gabrielstanovsky, sameer singh, and matt gardner.
2019.drop: a reading comprehension benchmark re-quiring discrete reasoning over paragraphs.
arxivpreprint arxiv:1903.00161..nan duan, duyu tang, peng chen, and ming zhou.
2017. question generation for question answering.
in proceedings of the 2017 conference on empiri-cal methods in natural language processing, pages866–874, copenhagen, denmark.
association forcomputational linguistics..angela fan, yacine jernite, ethan perez, david grang-ier, jason weston, and michael auli.
2019. eli5:in proceedings oflong form question answering.
the 57th annual meeting of the association for com-putational linguistics, pages 3558–3567, florence,italy.
association for computational linguistics..matthias fey and jan e. lenssen.
2019. fast graphrepresentation learning with pytorch geometric.
iniclr workshop on representation learning ongraphs and manifolds..arthur c graesser, natalie person, and john huber.
1992. mechanisms that generate questions.
ques-tions and information systems, 2:167–187..fred x han, di niu, kunfeng lai, weidong guo,inferring searchyancheng he, and yu xu.
2019.queries from web documents via a graph-augmentedsequence to attention network.
in the world wideweb conference, pages 2792–2798..michael heilman and noah a. smith.
2010. goodquestion!
statistical ranking for question genera-tion.
in human language technologies: the 2010annual conference of the north american chap-ter of the association for computational linguistics,pages 609–617, los angeles, california.
associa-tion for computational linguistics..thomas holme.
2003. assessment and quality controlin chemistry education.
journal of chemical educa-tion, 80(6):594..ari holtzman, jan buys, li du, maxwell forbes, andyejin choi.
2020. the curious case of neural text de-in international conference on learn-generation.
ing representations..steven m downing and rachel yudkowsky.
2009. as-sessment in health professions education.
rout-ledge..diederik p kingma and jimmy ba.
2014. adam: amethod for stochastic optimization.
arxiv preprintarxiv:1412.6980..xinya du, junru shao, and claire cardie.
2017. learn-ing to ask: neural question generation for readingcomprehension.
in proceedings of the 55th annualmeeting of the association for computational lin-guistics (volume 1: long papers), pages 1342–1352,.
kalpesh krishna and mohit iyyer.
2019. generatingquestion-answer hierarchies.
in proceedings of the57th annual meeting of the association for com-putational linguistics, pages 2321–2334, florence,italy.
association for computational linguistics..6433igor labutov, sumit basu, and lucy vanderwende.
2015. deep questions without deep understanding.
in proceedings of the 53rd annual meeting of theassociation for computational linguistics and the7th international joint conference on natural lan-guage processing (volume 1: long papers), pages889–898, beijing, china.
association for computa-tional linguistics..guokun lai, qizhe xie, hanxiao liu, yiming yang,and eduard hovy.
2017. race: large-scale readingcomprehension dataset from examinations.
in pro-ceedings of the 2017 conference on empirical meth-ods in natural language processing, pages 785–794..alon lavie and abhaya agarwal.
2007. meteor: anautomatic metric for mt evaluation with high levelsin proceed-of correlation with human judgments.
ings of the second workshop on statistical machinetranslation, pages 228–231, prague, czech repub-lic.
association for computational linguistics..mike lewis, yinhan liu, naman goyal, mar-jan ghazvininejad, abdelrahman mohamed, omerlevy, veselin stoyanov, and luke zettlemoyer.
2020. bart: denoising sequence-to-sequence pre-training for natural language generation, translation,and comprehension.
in proceedings of the 58th an-nual meeting of the association for computationallinguistics, pages 7871–7880, online.
associationfor computational linguistics..jingjing li, yifan gao, lidong bing, irwin king, andimproving question gener-michael r. lyu.
2019.in proceedings ofation with to the point context.
the 2019 conference on empirical methods in nat-ural language processing and the 9th internationaljoint conference on natural language processing(emnlp-ijcnlp), pages 3216–3226, hong kong,china.
association for computational linguistics..chin-yew lin.
2004. rouge: a package for auto-matic evaluation of summaries.
in text summariza-tion branches out, pages 74–81, barcelona, spain.
association for computational linguistics..bang liu, haojie wei, di niu, haolan chen, andyancheng he.
2020. asking questions the humanway: scalable question-answer generation from textin proceedings of the web conferencecorpus.
2020, pages 2032–2043..yinhan liu, myle ott, naman goyal, jingfei du, man-dar joshi, danqi chen, omer levy, mike lewis,luke zettlemoyer, and veselin stoyanov.
2019.roberta: a robustly optimized bert pretraining ap-proach.
arxiv preprint arxiv:1907.11692..samuel a livingston.
2009. constructed-response testquestions: why we use them; how we score them.
r&d connections.
number 11. educational testingservice..prashanth mannem, rashmi prasad, and aravind joshi.
2010. question generation from paragraphs atupenn: qgstec system description.
in proceedingsof qg2010: the third workshop on question gen-eration, pages 84–91..christopher manning, mihai surdeanu, john bauer,jenny finkel, steven bethard, and david mcclosky.
2014. the stanford corenlp natural language pro-in proceedings of 52nd annualcessing toolkit.
meeting of the association for computational lin-guistics: system demonstrations, pages 55–60, bal-timore, maryland.
association for computationallinguistics..ruslan mitkov and le an ha.
2003. computer-aidedgeneration of multiple-choice tests.
in proceedingsof the hlt-naacl 03 workshop on building edu-cational applications using natural language pro-cessing, pages 17–22..preksha nema and mitesh m. khapra.
2018. towards abetter metric for evaluating question generation sys-in proceedings of the 2018 conference ontems.
empirical methods in natural language processing,pages 3950–3959, brussels, belgium.
associationfor computational linguistics..andrew m olney, arthur c graesser, and natalie kperson.
2012. question generation from conceptmaps.
dialogue & discourse, 3(2):75–99..myle ott, sergey edunov, alexei baevski, angelafan, sam gross, nathan ng, david grangier, andfairseq: a fast, extensiblemichael auli.
2019.in proceedings oftoolkit for sequence modeling.
naacl-hlt 2019: demonstrations..liangming pan, yuxi xie, yansong feng, tat-sengchua, and min-yen kan. 2020. semantic graphsfor generating deep questions.
in proceedings of the58th annual meeting of the association for compu-tational linguistics, pages 1463–1475, online.
as-sociation for computational linguistics..kishore papineni, salim roukos, todd ward, and wei-jing zhu.
2002. bleu: a method for automatic eval-in proceedings ofuation of machine translation.
the 40th annual meeting of the association for com-putational linguistics, pages 311–318, philadelphia,pennsylvania, usa.
association for computationallinguistics..pranav rajpurkar, jian zhang, konstantin lopyrev, andpercy liang.
2016. squad: 100,000+ questions formachine comprehension of text.
in proceedings ofthe 2016 conference on empirical methods in natu-ral language processing, pages 2383–2392, austin,texas.
association for computational linguistics..sudha rao and hal daum´e iii.
2018. learning to askgood questions: ranking clariﬁcation questions us-ing neural expected value of perfect information.
inproceedings of the 56th annual meeting of the as-sociation for computational linguistics (volume 1:.
6434petar veliˇckovi´c, guillem cucurull, arantxa casanova,adriana romero, pietro li`o, and yoshua bengio.
in international2018. graph attention networks.
conference on learning representations..zhen wang, siwei rao,.
jie zhang, zhen qin,guangjian tian, and jun wang.
2020. diversifyquestion generation with continuous content selec-tors and question type modeling.
in findings of theassociation for computational linguistics: emnlp2020, pages 2134–2143, online.
association forcomputational linguistics..zhilin yang, peng qi, saizheng zhang, yoshua bengio,william cohen, ruslan salakhutdinov, and christo-pher d. manning.
2018. hotpotqa: a datasetfor diverse, explainable multi-hop question answer-ing.
in proceedings of the 2018 conference on em-pirical methods in natural language processing,pages 2369–2380, brussels, belgium.
associationfor computational linguistics..shiyue zhang and mohit bansal.
2019. address-ing semantic drift in question generation for semi-in proceedings ofsupervised question answering.
the 2019 conference on empirical methods in nat-ural language processing and the 9th internationaljoint conference on natural language processing(emnlp-ijcnlp), pages 2495–2509, hong kong,china.
association for computational linguistics..wenjie zhou, minghua zhang, and yunfang wu.
2019a.
multi-task learning with language model-ing for question generation.
in proceedings of the2019 conference on empirical methods in natu-ral language processing and the 9th internationaljoint conference on natural language processing(emnlp-ijcnlp), pages 3394–3399, hong kong,china.
association for computational linguistics..wenjie zhou, minghua zhang, and yunfang wu.
2019b.
question-type driven question generation.
in proceedings of the 2019 conference on empiricalmethods in natural language processing and the9th international joint conference on natural lan-guage processing (emnlp-ijcnlp), pages 6032–6037, hong kong, china.
association for computa-tional linguistics..long papers), pages 2737–2746, melbourne, aus-tralia.
association for computational linguistics..sudha rao and hal daum´e iii.
2019. answer-basedadversarial training for generating clariﬁcationquestions.
in proceedings of the 2019 conferenceof the north american chapter of the associationfor computational linguistics: human languagetechnologies, volume 1 (long and short papers),pages 143–155, minneapolis, minnesota.
associa-tion for computational linguistics..patricia shapley.
2000. on-line education to developcomplex reasoning skills in organic chemistry.
jour-nal of asynchronous learning networks, 4(2):43–52..peng shi and jimmy lin.
2019. simple bert models for.
relation extraction and semantic role labeling..heung-yeung shum, xiao-dong he, and di li.
2018.from eliza to xiaoice: challenges and opportunitieswith social chatbots.
frontiers of information tech-nology & electronic engineering, 19(1):10–26..robyn speer, joshua chin, and catherine havasi.
2017.conceptnet 5.5: an open multilingual graph ofin proceedings of the thirty-general knowledge.
first aaai conference on artiﬁcial intelligence,aaai’17, page 4444–4451.
aaai press..dan su, yan xu, wenliang dai, ziwei ji, tiezheng yu,and pascale fung.
2020. multi-hop question gen-eration with graph convolutional network.
in find-ings of the association for computational linguis-tics: emnlp 2020, pages 4636–4647, online.
as-sociation for computational linguistics..md arafat sultan, shubham chandel, ram´on fernan-dez astudillo, and vittorio castelli.
2020. on theimportance of diversity in question generation forin proceedings of the 58th annual meetingqa.
of the association for computational linguistics,pages 5651–5656, online.
association for compu-tational linguistics..xingwu sun, jing liu, yajuan lyu, wei he, yanjunma, and shi wang.
2018. answer-focused andin pro-position-aware neural question generation.
ceedings of the 2018 conference on empirical meth-ods in natural language processing, pages 3930–3939, brussels, belgium.
association for computa-tional linguistics..kenneth tobin.
1990. research on science laboratoryactivities: in pursuit of better questions and answersto improve learning.
school science and mathemat-ics, 90(5):403–418..ashish vaswani, noam shazeer, niki parmar, jakobuszkoreit, llion jones, aidan n gomez, łukaszkaiser, and illia polosukhin.
2017. attention is allin advances in neural information pro-you need.
cessing systems, pages 5998–6008..6435a data collection.
data filtering.
after collecting the raw datafrom yahoo and reddit, we design rules to ﬁlterout ill-formed answers and questions.
these rulesare listed in table 6. finally, we conduct human in-spection on random samples from the two datasetsand conﬁrm that samples are all clean and containopen-ended questions..rules for data cleaning.
- the question has url links.
- the question has more than 1 sentence or does not endwith a question mark.
- the question has less than 4 words or less than 1 contentword.
- the question does not start with wh-words: what, why,how, which, where, who, when; yes-no words: is, are, was,were, will, would, do, does, did, can, could, should, has,have; or frequent words for conditions: if, in, for, to, as,at..- the answer has less than 15 content words.
- the answer has less content words than the question.
- the answer has more than 30% of the words as digitletters..- the question and the answer have less than 2 overlappingcontent words.
- the question or the answer contains abusive words fromgoogle’s “what do you need” project7.
- the question or the answer has emoticons8.
- the question or the answer has 3 consecutive punctua-tion.
- the question or the answer has 3 consecutive fully up-percased words.
- the question has more than 90% of title-case words orthe answer has more than 30% of title-case words.
- the question has more than 1 unique word not in theenglish dictionary or the answer has more than 2 uniquewords not in the english dictionary9..table 6: rules for ﬁltering out ill-formed question-answer pairs on both yahoo and reddit..question type annotation.
we include the def-inition and corresponding examples for each ques-tion type in the annotation guideline, as shown intable 12. we allow annotators to label a questionwith two types if they cannot decide between thetwo.
all recruited annotators are u.s. college stu-dents, and are paid $15 per hour for the task.
onaverage, it takes 3.5 hours to annotate 1000 ques-tions..7https://gist.github.com/jamiew/.
1112488.of_emoticons.
8https://en.wikipedia.org/wiki/list_.
9https://github.com/dwyl/english-words.
for samples with disagreed labels, we checkwhether agreement can be reached by consideringboth labeled types.
for example, if annotator alabels verification and judgmental, and an-notator b labels judgmental, the agreed-upontype is judgmental.
we then resolve outstandingdisagreements by discussion..b details for question type classiﬁers.
to train the question type classiﬁer γq that reads thequestion as input, we split the collected questiontype dataset into training, validation, and test sets.
sample counts and question type distributions fordifferent data splits are shown in table 7..question type.
training validation.
test.
verificationdisjunctiveconceptextentexamplecomparisoncauseconsequenceproceduraljudgmental.
445156289274871162475102469476.all.
3719.
583646491522569146368.
580.
723654481393091118594.
660.table 7: sample counts and question type distributionsfor the newly labeled dataset with question types..we then use γq to identify types for unlabeledquestions in yahoo and reddit.
the question typedistributions for the two datasets are shown in ta-ble 8..question type.
#.
yahoo.
reddit#.
verificationdisjunctiveconceptextentexamplecomparisoncauseconsequenceproceduraljudgmental.
48,5775,13144,96316,81129,49413,16735,0105,54757,76234,149.
%.
16.71.815.55.810.14.512.01.919.911.8.
%.
6.11.04.82.743.82.017.82.39.310.2.
43,8017,17934,74419,217315,47814,727128,20416,54266,66273,434.table 8: distributions of question types for the twodatasets as labeled by γq..c details for graph construction.
we discard secondary dependency relations forincluding case, mark, cc,graph construction,cc:preconj, aux, aux:pass, cop, det, discourse, expl,.
6436det:predet, punct, ref.
the deﬁnition for each de-pendency can be found in universal dependency.10.
question type template exemplars.
d details for templates and exemplars.
template construction.
to avoid replacingwords that are representative of question types dur-ing template construction, we maintain a list ofwords not to be replaced for each question type, asshown in table 9. these words are identiﬁed byfrequency with additional manual inspection..question type words not to be replaced.
verification -disjunctiveconceptextentexamplecomparisoncauseconsequence happen, happens, would, affect, effect, ef-.
ormeanmany, much, long, take, getgood, best, ﬁnd, anyone, getdifference, best, better, and, orpeople.
proceduraljudgmental.
fectsget, way, make, best, knowthink, would, like, anyone.
table 9: words not to be replaced during template con-struction, per question type..exemplar collection.
table 10 lists the col-lected template exemplars for different questiontypes..exemplar classiﬁers.
to predict the exemplarsused for question decoding, we train one exemplarclassiﬁer for each question type, on each dataset.
accuracy values of these exemplar classiﬁers onthe reserved test sets are listed in table 11..e details for implementation.
we use fairseq (ott et al., 2019) to build our modelsand conduct training and decoding.
for the graphattention networks (gats) in our focus predictor,we adopt the implementation by pytorch geomet-ric (fey and lenssen, 2019).
all our experimentsare conducted on a quadro rtx 8000 gpu with48 gb of memory..training settings.
we use adam (kingma andba, 2014) for the training of all our models.
ourquestion type classiﬁers and template exemplarclassiﬁers are trained with a maximum learningrate of 1 × 10−5 and a batch size of 32. for train-ing generation models, the maximum learning rateis 3 × 10−5 and each batch contains at most 32,768.verification “is [np] [np]?”, “is there [np]?”,“is [np] [adjp]?”, “can [np] [v][np]?”, “do [np] [v] [np]?”, “doesanyone have [np]?”, “is it [adjp] to[v] [np]?”.
disjunctive.
concept.
extent.
example.
comparison.
cause.
“is [np] [np] or [np]?”, “is [np][adjp] or [adjp]?”, “who is [np]or [np]?”, “what came [advp] [np]or [np]?”, “which is [np] or [np]?”,“what is [np] or [np]?”.
“what is [np]?”, “what does [np]“who is [np]?”,“wheremean?”,is [np]?”, “whatis the meaning of[np]?”, “what does [np] do?”, “whatdo you know about [np]?”, “when is[np]?”, “what is meant by [np]?”,“where did [np] come from?”, “whichis [np]?”, “when was [np] [v]?”,“what is the deﬁnition of [np]?”, “howis [np]?”, “does anyone know any-thing about [np]?”, “what happened to[np]?”.
“what is [np]?”, “how [other] is[np]?”, “how many [other] are in[np]?”, “how many [np]?”, “howmuch does [np]?”.
“what are [np]?”, “what is a good[np]?”, “whatis the best [np]?”,“where can i [v] [np]?”, “what aresome good [np]?”, “does anyone have[np]?”.
“what is the difference between [np]and [np]?”, “what is the best [np]?”,“what is [np]?”, “which is better [np]or [np]?”, “who is the best [np]?”.
“why is [np]“why is [np]?”,[adjp]?”, “why do [np]?”, “whydo [np] [v] [np]?”, “what causes[np]?”, “why do [np] [v]?”.
consequence “what are [np]?”, “how does [np]affect [np]?”, “what are [np] ef-fects [np]?”, “what are the beneﬁts of[np]?”.
procedural.
judgmental.
“how do i [v] [np]?”, “how to [v][np]?”, “how do you [v] [np]?”,“what is the best way to [v] [np]?”,“how is [np] [v]?”, “how does [np][v] [np]?”, “how do [np] work?”.
“what is [np]?”, “what do you thinkof [np]?”, “do you believe in [np]?”,“do you like [np]?”, “should i [v][np]?”, “is [np] [np]?”, “do you[v] [np]?”, “who is [np]?”, “are you[np]?”, “should [np] [v] [np]?”.
table 10: template exemplars for different questiontypes..10https://universaldependencies.org/.
tokens.
mixed-precision training is adopted for all.
6437question type.
yahoo acc reddit acc.
verificationdisjunctiveconceptextentexamplecomparisoncauseconsequenceproceduraljudgmental.
43.0465.2143.4345.5646.0653.2554.5838.9443.6628.31.
38.0947.5043.8426.9950.2654.3343.7726.5729.5528.24.table 11: accuracy of exemplar classiﬁers for differentquestion types on yahoo and reddit..models except for models with gats..decoding settings.
we use beam search for de-coding.
a beam size of 5 and a length penaltyof 1.5 are used for all models.
repeated trigramblocking is applied to question generation.
theminimum and maximum lengths for generation areset to 1 and 100, respectively..model parameters.
the question type classi-ﬁers and template exemplar classiﬁers are based onrobertalarge, which has 355m parameters.
ourgeneration model builds a gat upon the bartmodel, containing 430m parameters in total..running time.
training question type classi-ﬁers takes 23 hours.
due to the difference in train-ing data size, the training time for template exem-plar classiﬁers ranges from 20 minutes to 3 hours.
for our generation model with focus prediction, ittakes 6 hours to train on yahoo and 12 hours totrain on reddit.
decoding on the test set of ya-hoo and reddit takes 8 minutes and 15 minutes,respectively..6438in this study, you are asked to annotate the question types for 1000 questions.
the question type reﬂects the nature ofthe question.
it is not determined by the interrogative word of the question.
there are 10 question types in total.
thedeﬁnition for each type is shown in the following table, along with examples per question type.
during annotation, you can label two most-conﬁdent types when no clear decision can be made for the most probabletype.
verification: asking for the truthfulness of an event or a concept.
- “is michael jackson an african american?”- “does a mercedes dealer have to unlock a locked radio?”- “could stress, anxiety, or worry cause cholesterol levels to rise?”disjunctive: asking for the true one given multiple events or concepts, where comparison among options is notneeded.
- “is michael jackson an african american or latino?”- “is a dvi to hdmi cable supposed to transmit audio and video or just video?”- “when you get a spray-on tan does someone put it on you or does a machine do it?”concept: asking for a deﬁnition of an event or a concept.
- “who said the sun never sets on the british empire?”- “where do dolphins have hair at?”- “what is the origin of the phrase ”kicking the bucket”?”extent: asking for the extent or quantity of an event or a concept.
- “how long does gum stay in your system?”- “what is barry larkin’s hat size?”- “to what extent is the renewable fuel standard accurate nationwide?”example: asking for example(s) or instance(s) of an event or a concept.
- “what are some examples to support or contradict this?”- “where can i get my teeth examined around los angeles?”- “what countries/regions throughout the world do not celebrate the christmas holidays?”- “what is the best goal or win you have ever made in a sport?”comparison: asking for comparison among multiple events or concepts.
- “how does an electric violin ”play” differently than an acoustic violin?”- “what is the best tinted facial moisturizer?”- “in what hilariously inaccurate ways is your job/career portrayed on television or in movies?”- “which is better, nike or adidas?”cause: asking for the cause or reason for an event or a concept.
- “how does the d.m.v.
decide the ﬁrst letter of the california driver’s license?”- “why are parents strick on girls than boys?”- “what makes nerve agents like ”novichok” so hard to produce and why can only a handful of laboratories createthem?”“why is the sky blue?”consequence: asking for the consequences or results of an event.
- “what are the negative consequences for the services if they do not evaluate their programs?”- “in the us, what is the beneﬁt of having a red left-turn arrow?”- “what would happen if employers violate the legislation?”- “what if the hokey pokey is really what it’s all about?”procedural: asking for the procedures, tools, or methods by which a certain outcome is achieved.
- “why ym 7.5 beta always stupidly shows me available, although i initially set it to invisible?”- “how did the amish resist assimilation into the current social status in the u.s?”- “how astronomers detect a nebula when there are no stars illuminating it?”judgmental: asking for the opinions of the answerer’s own.
- “do you think that it’s acceptable to call off work for a dying-dead pet?”- “should i date a guy that has an identical twin?”- “how old is too old for a guy to still live with his mother?”.
table 12: question type annotation guideline..6439