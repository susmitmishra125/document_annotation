mid-air hand gestures for post-editing of machine translation.
rashad albo jamara1, nico herbig1, antonio kr ¨uger1, josef van genabith1,21german research center for artiﬁcial intelligence (dfki),saarland informatics campus, germany2department of language science and technology,saarland university, germanyrashad.jamara@gmail.com{nico.herbig, krueger, josef.van genabith}@dfki.de.
abstract.
to translate large volumes of text in a globallyconnected world, more and more translatorsare integrating machine translation (mt) andpost-editing (pe) into their translation work-ﬂows to generate publishable quality transla-tions.
while this process has been shown tosave time and reduce errors, the task of trans-lation is changing from mostly text productionfrom scratch to ﬁxing errors within useful butpartly incorrect mt output.
this is affectingthe interface design of translation tools, wherebetter support for text editing tasks is required.
here, we present the ﬁrst study that investi-gates the usefulness of mid-air hand gesturesin combination with the keyboard (gk) fortext editing in pe of mt.
guided by a ges-ture elicitation study with 14 freelance trans-lators, we develop a prototype supporting mid-air hand gestures for cursor placement, text se-lection, deletion, and reordering.
these ges-tures combined with the keyboard facilitate allediting types required for pe.
an evaluationof the prototype shows that the average edit-ing duration of gk is only slightly slower thanthe standard mouse and keyboard (mk), eventhough participants are very familiar with thelatter, and relative novices to the former.
fur-thermore, the qualitative analysis shows posi-tive attitudes towards hand gestures for pe, es-pecially when manipulating single words..1.introduction.
in a well-connected world, translation is of ever-increasing importance (bassnett, 2013).
to meettranslation demands, machine translation (mt) isoften employed as a cheaper and faster alterna-tive to human translation (ht) (o’brien, 2012).
even though mt has improved drastically overthe last 5 years, discussions about reaching hu-man parity are still ongoing (l¨aubli et al., 2020)and limited to a small set of language pairs and.
domains for which ample training data is avail-able.
for most application scenarios, however, mtquality is far from reaching the quality of highlytrained professionals.
in an attempt to combine thebest of both worlds, post-editing (pe) is becom-ing common practice, where human translators useraw mt output and make the necessary changesto produce an acceptable level of quality (kopo-nen, 2016).
although translators have approachedpe with fear and skepticism (lagoudaki, 2009),more recent studies found that nowadays transla-tors are more open to it and that much of the origi-nal dislike was attributed to outdated perceptionsof mt quality (plitt and masselot, 2010; greenet al., 2013).
independent of translators’ percep-tions, studies found that pe increases productivityand decreases errors compared to translation fromscratch (green et al., 2013)..pe changes the translation task from mostlytext generation to text editing, which involvesan increased usage of navigation and deletionkeys (toral et al., 2018).
as a result, transla-tors need better support with text editing opera-tions, which raises the question whether interactionmodalities other than mouse and keyboard can bebeneﬁcial for pe.
an interaction modality that hasgained attention in other research areas (koutsaba-sis and vogiatzidakis, 2019) but so far remainsunexplored for pe is mid-air hand gestures..in this paper, we (i) investigate which mid-airgestures combined with the keyboard (gk) are suit-able for which text-editing operations in pe, (ii)build a prototype supporting pe using gk, and(iii) analyze editing times and subjective feedbackon mid-air hand gestures compared to mouse andkeyboard (mk) for speciﬁc pe operations.
to ad-dress these goals, we conducted a gesture elici-tation study (ges) with professional translators,resulting in a set of gestures for different editingtasks, which were then implemented in a prototype..proceedingsofthe59thannualmeetingoftheassociationforcomputationallinguisticsandthe11thinternationaljointconferenceonnaturallanguageprocessing,pages6763–6773august1–6,2021.©2021associationforcomputationallinguistics6763our experiment shows that, surprisingly, editingdurations for most pe tasks were very similar inthe conditions gk and mk, even though partici-pants were much more experienced with the latter.
furthermore, participants prefer manipulating sin-gle items1 using gestures, while manipulating agroup of items, which involves more complex textselection, received poorer subjective feedback..2 related work.
in this section, we present related research on trans-lation environments, multi-modal approaches tope, and mid-air gestures for text editing tasks..2.1 cat tools and post-editing.
in recent years, most translators use computer-aided translation (cat) tools for translation (cop-pers et al., 2018).
cat tools are workﬂow systemsoffering features like translation memory (tm),mt, or terminology management (van den berghet al., 2015; koskinen and ruokonen, 2017).
trans-lators prefer to use cat tools as they enhance ter-minology consistency, increase productivity, andimprove the general quality of translations (rossiand chevrot, 2019; moorkens and o’brien, 2017).
while tm is still often valued more than mt(moorkens and o’brien, 2017), a recent study byvela et al.
(2019) shows that professional trans-lators who were given a choice between transla-tion from scratch, tm, and mt, chose mt in 80%of cases, highlighting the importance of pe ofmt.
apart from translators’ preference, toral et al.
(2018) found that pe phrase-based and neural mt(pbsmt and nmt) output increased productivityby 18% and 36% respectively compared to ht..pe also changes the interaction patterns com-pared to manual translation from scratch (carl andjensen, 2010), leading to a signiﬁcantly reducedamount of mouse and keyboard events (green et al.,2013).
at the same time, navigational and deletionkey usage increases by 72% during pe of nmtcompared to ht (toral et al., 2018).
this moti-vates our decision to explore modalities other thanmk for pe and to speciﬁcally focus on efﬁcientnavigation and deletion..2.2 multi-modal approaches.
previous studies already explored modalities otherthan mk: the casmacat tool (alabau et al.,2014) allows users to hand-write text with an.
1item(s) refers to word(s) and/or punctuation mark(s)..e-pen.
studies on mobile pe via touch andspeech (o’brien et al., 2014; torres-hostench et al.,2017) show that participants especially like reorder-ing words through touch drag and drop, and prefervoice input when translating from scratch, but stickto the iphone keyboard for small changes.
zapata(2016) also explores the use of voice- and touch-enabled devices; however, their study did not focuson pe, and used microsoft word instead of a propercat environment.
teixeira et al.
(2019) explorea combination of touch and speech for translationfrom scratch, translation using tm, and translationusing mt and found that their touch implementa-tion received poor feedback, while dictation turnedout to be quite useful..we started our research on multi-modal cattools with an elicitation study (herbig et al., 2019),which showed that pen, touch, and speech interac-tion, as well as combinations thereof, should becombined with mouse and keyboard to improvepe of mt.
a prototype based on the proposed in-teractions allows users to “directly cross out orhand-write new text, drag and drop words for re-ordering, or use spoken commands to update thetext in place” (herbig et al., 2020b).
its evaluationwith professional translators further showed thatdepending on the editing operation, different inputmodalities performed well (herbig et al., 2020a)..to date, mid-air gestures have only been ad-dressed in our elicitation study (herbig et al., 2019),where participants did not expect them to be par-ticularly useful.
however, participants only consid-ered gestures on their own (i.e.
also for text entry),and thus the combination with the keyboard meritsfurther investigation, both in terms of an elicitationstudy and even more so in a practical evaluation ofa prototype..2.3 mid-air hand gestures.
hand gestures provide an intuitive and natural wayof interaction (sharma and verma, 2015; ortegaand nigay, 2009), but the design of appropriategestures depends on the application type and con-text (wachs et al., 2011; weichert et al., 2013;nielsen et al., 2003).
gestures must be easy tolearn and memorize, comfortable to perform, andshould be metaphorically meaningful (wachs et al.,2011; weichert et al., 2013)..ortega and nigay (2009) explored the use ofmid-air ﬁnger pointing to replace the mouse andshowed that this approach signiﬁcantly reduces the.
6764switching time compared to mk (almost to zero).
however, research on text editing using hand ges-tures is scarce.
one exception is rives et al.
(2014),who presented the idea of using gestures to performthe operations cut, copy, paste, select, undo, anddelete to edit a document using gestures.
in theirconcept, the user enters the edit mode through aspecial gesture and then draws in the air to performthe above operations, e.g.
a “x” for deletion..to ﬁnd a suitable and concise set of gestures for.
pe operations, we conduct a ges..3 gesture elicitation study.
a ges is a form of participatory design (morriset al., 2014) where users are incorporated in thedesign process to inform an appropriate gesture setfor a given application.
important aspects includeleading participants away from technical think-ing (nielsen et al., 2003), making them assume thatgesture recognition is perfect, and considering theirbehavior as always acceptable (wobbrock et al.,2009).
they should only be informed about theessential details of the task to avoid bias towardsparticular approaches (wobbrock et al., 2005)..we conduct a ges for three reasons.
firstly,there is no universal gesture set suitable for all ap-plications (nielsen et al., 2003).
secondly, usersprefer gestures designed through elicitation stud-ies, because professional designers tends to gen-erate more physically and conceptually complexgestures (morris et al., 2014).
thirdly, to the bestof our knowledge, there is no other ges for textediting using gk which we could rely on..in our ges, we employed the guessability ap-proach (wobbrock et al., 2005) which is intended toincrease immediate usage of interfaces.
it consistsof three phases: (1) deﬁning so-called referents(i.e.
common operations) that should be achievablethrough the system, (2) asking participants to pro-pose a gesture for each referent, and (3) analyzingthe collected data to generate the ﬁnal gesture set..3.1 method.
due to the covid-19 pandemic we conducted anonline ges.
prior to commencing the study, ethicalclearance was sought from the university ethicalreview board.
the study took 30 to 65 minutes perparticipant (avg: 46 minutes)..participants: fourteen right-handed freelancetranslators (with 14 different nationalities, 7 fe-male and 7 male) were hired to participate in the.
study (avg age: 28, sd: 4.56).
years of profes-sional experience ranged from 2 to 15 years (avg:5.29, sd: 3.43), offering a total of 19 languagepairs.
in terms of cat tool experience, about 2/3of the participants reported using cat tools to aidtranslation, with 1 to 4 years of experience.
overall,participants were often in the earlier stages of theirprofessional careers.
three of the participants al-ready had experience with gesture-based interfacessuch as a tv remote control.
however, they ratedtheir level of experience with gestural interfaces as“bad” to “neutral”..referents: referents are described as the effectwhich is triggered by a gesture (wobbrock et al.,2009).
the referents used in elicitation studies arean essential part, since the results established arelimited to this set.
in our case, referents are pe op-erations; we will thus use referents and operationsinterchangeably.
to ﬁnd good referents, we lookedat different pe task classiﬁcations discussed in theliterature.
popovic et al.
(2014) propose 5 pe opera-tions: correcting word form, correcting word order,adding omission, deleting addition, and correctinglexical choice.
koponen (2012) additionally dis-tinguishes between moving single words or groupsof words and the distance of the movement.
basedon these studies as well as our previous elicitationprocedure (herbig et al., 2019), we propose thereferents presented below as pe tasks for which weexplore gestural input..• i: insertion.
• ds: deleting a single item.
• dg: deleting a group of items.
• rps: replacing a single item.
• rpg: replacing a group of items.
• ros: reordering a single item.
• rog: reordering a group of items.
performing those referents implicitly includesother operations, namely selecting a position, aword, or a group of words/characters..procedure: we interviewed each participant on-line via a video conferencing platform.
the ﬁrstpart of the study introduced pe of mt, discussingthe current use of mouse and keyboard in cattools, and presenting the idea of mid-air hand ges-tures for pe without showing any concrete gestures.
6765that could induce bias.
participants were then askedto ﬁll out an online questionnaire capturing their de-mographics as well as other questions concerningcat tools and mt in general.
they were also in-formed that they should assume perfect recognitionand that all proposals are valid.
after each gestureproposal, participants supplied subjective ratingson 7-point likert scales (7 = “strongly agree”) asto whether the gesture is: (a) a good match for itsintended purpose, (b) easy to perform, and (c) agood alternative to mk.
additionally, we used athink-aloud protocol and videotaped the session forsubsequent analysis.
our referents were counter-balanced to avoid systematic errors..analysis: for the analysis, we grouped similargestures based on the number of hands involved,their physical attributes and movement direction.
we report the largest groups per referent, butalso the agreement rate (ar), “characterizing thelevel of consensus between participants’ proposalselicited” (vatavu and wobbrock, 2015).
a high arsuggests that the most frequent gesture proposalis guessable and intuitive.
however, less frequentproposals can still yield interesting insights..3.2 results & discussion.
unlike static gestures, dynamic gestures are hardto illustrate through images; therefore, we createda simple website that shows recorded animationsof gestures for each participant and groups thembased on the referent2..while analyzing the data, consistent patternsemerged: similar to the way the mouse is used, par-ticipants performed all referents by ﬁrst selectingthe text, then performing the editing operations, e.g.
deleting.
consequently, we decided in our analysisto separate the selection gestures from the editingoperation gestures, analyzing and discussing eachseparately.
in addition, the proposed selection ges-tures are divided into two types: the selection of asingle item and the selection of a group of items..group selection: 8 unique gestures were pro-posed for group selection for the referents dg, rpg,3, with the same ar of 0.13 for each.
twoand rogof these gestures were the most common, namelyboth indices (pointing with index ﬁngers and mov-ing them apart to select: see figure 1a) and index+ thumb (pointing with pinched index ﬁnger and.
2https://rashad-j.github.io/.
conceptual-study.
3detailed results are shown on our website..thumb and separating them to select a range).
bothindices was rated higher on “ease” than index +thumb, but received almost identical ratings for“good match” and “alternative”, indicating a slightpreference for using both index ﬁngers.
the re-maining 6 proposals were interesting ideas likeusing a certain number of ﬁngers to specify thenumber of words to select, however, none of theseproposals reached agreement..single item selection: participants proposed 5,9, and 8 different gestures for the referents ds,rps, and ros, respectively.
consequently, thehigh number of different proposals for replacingand reordering reduced the ar to 0.08 (rps) and0.09 (ros) compared to 0.16 for ds.
participantsmostly proposed the same single item selection ges-ture for all subsequent referents, highlighting theimportance of counter-balancing.
however, the in-dex + thumb and both indices appear to also be pre-ferred in selecting a single item, but with slightlyvarying agreement scores compared to group se-lection.
in addition, the gesture pointing (wherea participant points with the index ﬁnger to placethe cursor on the item) was highly preferred forsingle item selection.
the double-tap gesture wasalso proposed 3, 2, and 1 times for the referentsds, ros, and rps, respectively..when asked about the reasons for their propos-als, participants (p) gave responses such as p3: “itis easy and intuitive” or p5: “it is really easy toselect the start and then slide it to select”..editing operations: unlike selection gestures,editing operations received very distinct gestureproposals except for a slight similarity betweendeletion and replacement (having one gesture pro-posal in common)..for the deletion referents, 9 unique gestureswere proposed in single and group referents withan ar of 0.08 for both.
three gestures appearedto be the most common among the participants.
those were: move right index down (figure 1b),move right index up, and move the right hand up(figure 1c).
we decided to merge the index move-ment up and down into one gesture for two reasons:ﬁrst, it is more intuitive to move the index ﬁngerup and then down (or down and up) because theuser will have to move his hand back to a neutralposition; second, participants p6 and p7 elaboratedthat moving the index ﬁnger up or down to deleteis equally acceptable for them..6766(a) selecting a group oftext items by distancingthe index ﬁngers..(b) deletion by movingthe right index ﬁngerdown..(c) deletion by movingthe right hand up..(d) reordering by movingboth hands simultaneously..(e) reordering bygrabbing, moving,and releasing..figure 1: common hand gestures for pe tasks proposed in our ges..moving the right hand up to delete was alsocommon for the replace referent for both rps andrpg.
in general gestures for the replacement ref-erent received a slightly higher ar of 0.10 and 0.18for single and group referent respectively.
analyz-ing participants’ thoughts, which were captured viathink-aloud protocol, it appears that they wantedto delete ﬁrst and then type the replacement item.
another common proposal for replacement wassuggested by almost half of the participants (6/14),namely to simply type after selecting a text.
more-over, there were some proposals without agreement,e.g.
p13 came up with the idea to strike-throughtext with the right index to delete and then type,whereas p14 suggested forming an “x” with hisindex ﬁngers to delete before using the keyboard..the reordering referents received three distinctgestures with ar of 0.16 and 0.26 for single andgroup referents respectively.
the ﬁrst one was toselect and move the text with both hands by mov-ing them simultaneously (figure 1d).
this gesturewas proposed by 4 participants in rps and 6 partici-pants in rpg.
the second gesture was to point withthe right index ﬁnger and start moving it to movethe text immediately after selecting (proposed by4 participants in both rps and rpg).
the thirdgesture was to grab with the right hand and movethe hand to reorder the text, then open it to release(figure 1e).
this gesture was proposed for rosby only 3 participants.
other individual proposalswere made, e.g.
p7 preferred to pinch using indexﬁnger and thumb, then move her hand to move thetext, and then release the pinch to place the item..finally, the insertion referent received 5 uniquegestures.
one of the proposals was to point withthe right index ﬁnger and then move it to placethe cursor in the required place.
this gesture wassuggested by 9 out of 14 participants; hence, wesee a high ar of 0.4. it was also referred to aspointing for single item selection.
once the cursorwas placed in the target position, the user wouldswitch to the keyboard for typing..together, these ﬁndings constitute a gesture setfor text editing.
our separation into selection (forsingle items and groups) and editing operationsmakes the pe tasks more consistent and better rep-resents our participants’ mindsets.
what is inter-esting is that selection of single items achievedhigh agreement on using a gesture to simply placethe cursor on the item, without actually selectingit from start to end as with the mouse.
the dele-tion and replacement referents shared some gestureproposals because participants often wanted to re-place by deletion followed by typing.
a furtherreﬁnement to this set is presented below..4 prototype.
we used the ges results to deﬁne our ﬁnal gestureset and implement a prototype.
for this, the fre-quently proposed gestures were explored in termsof implementation feasibility given the technologywe are using.
if two gestures were conﬂicting, wedropped the less popular one; otherwise we slightlymodiﬁed it to resolve the conﬂict..for group selection, we found that the proposedindex + thumb gesture practically fails upon selec-tion across multiple lines; thus, we dropped it.
incontrast, using both indices can perform this kindof selection, so we implemented it as depicted infigure 2. note that in contrast to the mouse, thegroup selection using both index ﬁngers allows theuser to manipulate both ends of the selection con-tinuously instead of having one side ﬁxed.
for sin-gle item/position selection, we only implementedpointing with the right index ﬁnger, as it alreadyentails the double tap gesture.
for multi-line text,both single and group selection allow pointing withthe index ﬁnger vertically and horizontally..insertion can also be easily achieved by placing.
the cursor through pointing followed by typing..for deletion, ds and dg received similar gestureproposals.
looking at the proposals in detail, wefound that two participants also wanted to delete.
6767for single item reordering, it is again sufﬁcient toplace the cursor on the item without selecting thewhole text..figure 4: mid-air gesture-based reordering by select-ing, left grab, pointing with the right index ﬁnger to thetarget position, and releasing the grab..the prototype was implemented as an extensionto our open-source mmpe cat interface (her-big et al., 2020b,c)4. mmpe allows translators touse input modalities such as speech, touch, pen,and eye tracking in combination with the standardmouse and keyboard.
however, it previously didnot support mid-air gestures.
the main interfaceshows the source on the left, and the target on theright, with the currently edited segment enlarged.
this additional space turned out to be useful forhand gestures as it simpliﬁes pointing.
in addition,all user interactions are logged.
mmpe uses angu-lar for the front-end, and node.js for the back-end,with websockets and rest apis for the commu-nication between them..our gesture detection relies on the leap motioncontroller5, which is small in size (8cm * 3cm) andcan be placed on the top of the keyboard (figure 2).
the device provides frames of detected hands with3d positions of ﬁnger joints, as well as some basicdetection such as whether the ﬁngers are extendedor not.
based on this information our gesture de-tection algorithm determines if one of the abovegestures is being performed.
if only the right handis detected with the index ﬁngers extended, then thecursor will be updated based on hand movement.
moving both index ﬁngers selects the correspond-ing text in the interface (figure 2).
when a deletiongesture is detected, the selected text (for group se-lection), or the word that the cursor is currentlypositioned on, is removed (figure 3).
a grab withthe left hand puts the currently selected text/word.
4https://github.com/nicoherbig/mmpe5https://www.ultraleap.com/product/.
leap-motion-controller/.
figure 2: mid-air gesture-based group selection bypointing with both indices..with a hand down movement.
thus, we imple-mented hand or ﬁnger movement down and up tooffer consistent deletion possibilities (figure 3).
for ds, it is sufﬁcient if the cursor is placed some-where on the word; there is no need to deﬁne thestart and end of the word through a group selection..figure 3: mid-air gesture-based deletion by moving theright hand or ﬁnger up or down..replacement can be achieved by either perform-ing a group selection and typing directly, or byselecting a single item or group of items, deleting,and then typing.
note that rps can thus also beachieved without group selection..the most complicated gestures were proposedfor reordering; the gestures are a compound ofseveral sub-gestures.
since reordering using theright index conﬂicts with cursor movement, wedropped it.
moving both hands while in the se-lection position turned out to be difﬁcult to per-form, as maintaining the same distance betweenthe hands at all times is challenging.
therefore, wedecided to merge it with the grab proposal; thus,after selection, a grab with the left hand indicatesthe start of the reordering process.
then movingboth hands or just the right index ﬁnger reordersthe text (figure 4).
once the required position isreached, closing the right hand ends the reorderingprocess and drops the text in the target position..6768containing the cursor in a reordering visualization.
then, movements of the right index are trackedand move the highlighted text as well as an arrowindicator visualizing the currently calculated dropposition.
releasing the grab then places the textback into the input ﬁeld at the indicated position(figure 4).
to avoid unintended gestures whilemoving the hands back to the keyboard, the usercan form a grab in both hands after executing agesture.
since people move their hands at differentspeeds, we further added sensitivity settings forgestures, similar to the standard mouse settings.
avideo showing the interactions in practice can befound under: https://youtu.be/qiryeojkfvc..5 prototype evaluation.
in contrast to the web-based elicitation study, wehad to evaluate the prototype in-situ due to the hard-ware setup.
given the covid-19 situation, it wasimpossible to invite professional translators.
there-fore we had to conduct a study with our colleagues.
to mitigate the difference between non-translation professional subjects (computer scien-tists) and translation professionals, we ensured thatsimilar to professional translators, (i) all our partic-ipants have academic training (computing degreesinstead of translation degrees), (ii) that they arealso highly familiar with traditional mouse andkeyboard interfaces and use them in their day-to-day work, (iii) all subjects have relevant languageproﬁciency (source en, target de), and (iv) allwork in a multilingual en-de environment.
fur-thermore, as the evaluation required participantsonly to perform pre-speciﬁed text editing opera-tions, without involving any linguistic translationdecisions, we hope to minimise the effect of nothaving translators as participants..5.1 method.
we use a methodology similar to that of our previ-ous mmpe evaluation (herbig et al., 2020a), how-ever, here we compare a novel interaction modality(mid-air hand gestures) to mouse and keyboard:.
participants: overall, 8 participants (7 male, 1female) from the department of computer sciencetook part in the experiment: 5 researchers, 2 phdstudents, and 1 msc student.
their ages rangedfrom 24 to 39 (avg: 29, sd: 5).
all had englishskills from b2 to c1 and were either german na-tives (7 of them) or had c1 german knowledge.
as computer scientists, they were all experienced.
keyboard users.
participants were all right-handedand had normal vision.
two of them indicated littleexperience with gesture-based interfaces, whereasthe others reported a medium to very high level..apparatus: the main equipment consists of a 23inch monitor, a nuc pc, a leap motion controller,a standard wired mouse, and a standard keyboardwith german layout.
the nuc pc is equipped witha processor of type intel(r), core i7 cpu @ 3.50ghz, 16.0 gb of ram, and an internal graphicsprocessor capable of capturing 30 – 60 frames persecond when used by the leap motion controller..procedure: prior to undertaking the study, ethi-cal clearance was obtained from the ethical reviewboard at the university.
the study consisted of 3phases and took approximately 1 hour per partici-pant.
the ﬁrst phase introduced gk and the proto-type interface, followed by capturing demographicinformation.
in the second phase, participants weregiven 10 – 15 minutes to explore gk to correctsamples of incorrect mt output.
the third phase in-cluded the main experiment, in which participantsperformed a guided test to correct mt output intwo conditions: mid-air gestures & keyboard (gk)and standard mouse & keyboard (mk).
for each ofthe referents from our elicitation study, 3 differentsegments had to be corrected in both conditionsappearing in random order to capture comparableediting times.
the segments were taken from thewmt en-de 2018 news test set.
a single errorwas introduced per segment and a pop-up alwaystold participants what error needed to be ﬁxed andwhich modality to use.
after each referent (e.g.
deleting a single item), participants were presentedwith the same three 7-point likert scales as in ourges.
in addition we conducted semi-structuredinterviews to gather further feedback.
we had 2conditions, 7 referents, and 3 segments per referent;thus, there were in total 2 ∗ 7 ∗ 3 = 42 segments tocorrect for each participant.
while this correctionof pre-deﬁned errors prevents us from drawing con-clusions in a realistic setting, it allows us to exploreeach editing operation in isolation, including accu-rate time measures and subjective feedback, whichis more important for a ﬁrst prototype test..5.2 results & discussion.
qualitative data was collected by the semi-structured interviews and likert rating scales aftereach referent.
figure 5a shows that operations ma-nipulating single items were generally rated higher.
6769(a) subjective ratings..(b) editing duration of gk and mk..figure 5: prototype evaluation results..than operations on groups of items.
ds was ratedbest, especially in terms of goodness and ease ofuse.
the majority of our participants commentedthat group selection was hard to perform, whereasthe editing operations themselves were consideredeasy.
while comments differed depending on thereferent, most of them were positive, and we fre-quently got statements such as “it is great, [gk]felt like the same level of mk”..quantitative data, shown in figure 5b, cap-tured the editing duration of both gk and mk foreach referent, showing that the gk interquartilerange was higher than the standard mk, except forrog.
however, the most interesting ﬁnding wasthat, although the participants had years of experi-ence using mk and were new to gk for text editing,the average editing time in the gk condition wasvery close to the average for mk in 4 out of 7 ref-erents.
for analyzing statistical differences in ourdata, we ran wilcoxon signed-rank tests since thenormality assumption of t-tests was not fulﬁlleddue to the small sample size.
as expected, giventhe limited amount of data, our statistical tests wereunable to ﬁnd signiﬁcant differences between gkand mk for all operations6..similar to what we found in the qualitative anal-ysis, the gestures operating on single items weremore efﬁcient than operations on groups of itemsin the gk condition.
ds was the fastest, followedby rps and i. on the other hand, group operationsturned out to be the most time-consuming in bothconditions, with the biggest differences betweenconditions for dg and rpg.
interestingly, averageediting time of rog was nearly identical in bothconditions, although the gesture-based approachshowed more variance..6α = 0.05, ∀p, p > α, p = (ins = 0.641, rps =0.312, rpg = 0.945, ds = 0.461, dg = 0.461, rs =0.312, rg = 0.383).
in summary, the study has shown positive atti-tudes towards using mid-air hand gestures in com-bination with the keyboard for speciﬁc pe tasks.
single item referents in particular received goodfeedback and were close to mk in terms of timemeasures.
group selection was the main reasonfor disliking the gk and main source of additionalediting time.
based on the comments, the majorityof participants found such group selections difﬁcultto perform, especially when selecting across multi-ple lines, therefore, improvements should be madeto the group selection in the future.
overall, the re-sults are encouraging, especially when consideringthe level of experience our participants had withmk and the short time for them to learn gk fortext editing.
in particular the single item referents,and perhaps improved versions of the group refer-ents, could provide beneﬁt to the pe process as acomplement, not replacement, to traditional mouse-and keyboard-based editing..6 conclusion.
the use of mt and pe changes the task of trans-lation from mostly text production to ﬁxing errorswithin useful but partly incorrect mt output.
thisaffects the interface design of cat tools, wheretranslators need more support for text editing tasks.
the literature suggests that other interaction modal-ities than mk, or combinations thereof, could bettersupport pe operations.
to the best of our knowl-edge, this is the ﬁrst study that investigates theusefulness of mid-air hand gestures for pe of mt.
our ges with 14 freelance translators yielded aset of gestures to manipulate both single items andgroups of items, which we further reﬁned by con-sidering conﬂicting gestures and exploring thempractically.
the resulting prototype allows users to(i) place the cursor by pointing with the index ﬁn-ger, (ii) select ranges of text by pointing with both.
6770index ﬁngers, (iii) moving the hand or index ﬁngerup or down for deletion, and (iv) reorder by select-ing text, forming a grab with the left hand, pointingwith the right index ﬁnger to the desired position,and releasing the grab to drop the text.
these ges-tures, combined with the keyboard, support all textmanipulations required for pe..to conclude,.
this new interaction modality,which so far was overlooked by research on cattools and post-editing, performs better than ex-pected and therefore warrants further investigation.
overall, we hope that future research will pick upthe insights from the ﬁrst and second study andhelp advance the state-of-the-art in pe..due to covid-19, only a small-scale prototypeevaluation with non-translator participants was pos-sible.
nonetheless, as the prototype design wasguided by an elicitation study with translation pro-fessionals which usually leads to well-perceived in-terfaces and since we designed the study to mitigatebias induced by a sub-optimal participant sample,we expect that professional translators would havegiven us comparable feedback.
the ﬁndings over-all suggest that gk could be a suitable interactionmodality for pe and thus merits further research:even though participants had years of experiencewith mk, our quantitative analysis of editing timeshowed that gk was only slightly slower for mostoperations, especially when manipulating singleitems.
similarly, qualitative data shows that manip-ulating single items was rated higher than opera-tions working on groups of items, as participantsfound the group selection gesture “cumbersome” toperform.
this ﬁnding indicates that further effortshould be invested in improving group operations,which are also common in pe (e.g.
by exploringif a different placement of the detection devicecould increase detection accuracy).
however, theappealing results on single item operations and thesatisfactory results on group operations bode welland warrant further exploration with professionaltranslators in a realistic pe scenario..we do expect that after using the interface for alonger period of time, users will become more ef-fective, as is common with other interfaces: the newinterface is competing with decades of mk muscle-memory training.
however, only future long-termstudies can show if editing times with gk willbecome as low as or even lower than with mk ap-proaches.
apart from efﬁciency, participants in ourprevious studies (herbig et al., 2019) argued forhaving multiple suitable options to interact withtext, instead of performing the same movementsall day long.
therefore, it is not just a questionof speed but also user satisfaction and health: ad-ditional modalities may help guard against carpal-tunnel syndrome and provide exercise alternativesin a seated environment..acknowledgments.
this research was funded in part by the germanresearch foundation (dfg) under grant numberge 2819/2-1 (project mmpe).
we thank all partici-pants of the two studies for their valuable feedback..references.
vicent alabau, christian buck, michael carl, fran-cisco casacuberta, mercedes garc´ıa-mart´ınez, ul-rich germann, jes´us gonz´alez-rubio, robin hill,philipp koehn, luis a leiva, et al.
2014. cas-macat: a computer-assisted translation work-bench.
in proceedings of the demonstrations at the14th conference of the european chapter of the as-sociation for computational linguistics, pages 25–28..susan bassnett.
2013. translation.
routledge..jan van den bergh, eva geurts, donald degraen,mieke haesen, iulianna van der lek-ciudin, karinconinx, et al.
2015. recommendations for transla-tion environments to improve translators’ workﬂows.
in proceedings of the 37th conference translatingand the computer.
tradulex..michael carl and martin kay kristian th jensen.
2010.long distance revisions in drafting and post-editing.
natural language processing and its applications,page 193..sven coppers, jan van den bergh, kris luyten, karinconinx, iulianna van der lek-ciudin, tom vanalle-meersch, and vincent vandeghinste.
2018.in-tellingo: an intelligible translation environment.
inproceedings of the 2018 chi conference on humanfactors in computing systems, pages 1–13..spence green, jeffrey heer, and christopher d man-ning.
2013. the efﬁcacy of human post-editing forlanguage translation.
in proceedings of the sigchiconference on human factors in computing sys-tems, pages 439–448..nico herbig, tim d¨uwel, santanu pal, kalliopimeladaki, mahsa monshizadeh, antonio kr¨uger,and josef van genabith.
2020a.
mmpe: a multi-modal interface for post-editing machine translation.
in proceedings of the 58th annual meeting of theassociation for computational linguistics, pages1691–1702.
association for computational linguis-tics..6771nico herbig, santanu pal, tim d¨uwel, kalliopimeladaki, mahsa monshizadeh, vladislav hna-tovskiy, antonio kr¨uger, and josef van genabith.
2020b.
mmpe: a multi-modal interface using hand-writing, touch reordering, and speech commands forin proceedingspost-editing machine translation.
of the 58th annual meeting of the association forcomputational linguistics: system demonstrations,pages 327–334.
association for computational lin-guistics..nico herbig, santanu pal, tim d¨uwel, raksha shenoy,antonio kr¨uger, and josef van genabith.
2020c.
im-proving the multi-modal post-editing (mmpe) catenvironment based on professional translators’ feed-in proceedings of 1st workshop on post-back.
editing in modern-day translation at the amtaconference, pages 93–108.
association for machinetranslation in the americas..nico herbig, santanu pal, josef van genabith, and an-tonio kr¨uger.
2019. multi-modal approaches forin proceedingspost-editing machine translation.
of the 2019 chi conference on human factors incomputing systems, pages 1–11..maarit koponen.
2012. comparing human perceptionsof post-editing effort with post-editing operations.
in proceedings of the seventh workshop on statis-tical machine translation, pages 181–190..maarit koponen.
2016..is machine translation post-editing worth the effort?
a survey of research intopost-editing and effort.
the journal of specialisedtranslation, 25:131–148..kaisa koskinen and minna ruokonen.
2017. love let-ters or hate mail?
translators’ technology accep-tance in the light of their emotional narratives.
inhuman issues in translation technology, pages 26–42. routledge..panayiotis koutsabasis and panagiotis vogiatzidakis.
2019. empirical research in mid-air interaction: asystematic review.
international journal of human–computer interaction, 35(18):1747–1768..michael nielsen, moritz st¨orring, thomas b moes-lund, and erik granum.
2003. a procedure for de-veloping intuitive and ergonomic gesture interfacesfor hci.
in international gesture workshop, pages409–420.
springer..sharon o’brien.
2012..translation as human–computer interaction.
translation spaces, 1(1):101–122..michael ortega and laurence nigay.
2009. airmouse:in ifipfinger gesture for 2d and 3d interaction.
conference on human-computer interaction, pages214–227.
springer..sharon o’brien, joss moorkens, and joris vreeke.
2014. kanjingo–a mobile app for post-editing.
in proceedings of the 17th annual conference ofthe european association for machine translation(eamt 2014), dubrovnik, croatia, 16th-18th june..mirko plitt and franc¸ois masselot.
2010. a productiv-ity test of statistical machine translation post-editingin a typical localisation context.
the prague bul-letin of mathematical linguistics, 93(1):7–16..maja popovic, arle lommel, aljoscha burchardt,eleftherios avramidis, and hans uszkoreit.
2014.relations between different types of post-editing op-erations, cognitive effort and temporal effort.
in pro-ceedings of the 17th annual conference of the eu-ropean association for machine translation, pages191–198.
european association for machine trans-lation dubrovnik, croatia..christopher m rives, craig t brown, dustin l hoff-man, and peter m on.
2014. gesture based editmode.
us patent 8,707,170..caroline rossi and jean-pierre chevrot.
2019. usesand perceptions of machine translation at the euro-pean commission.
the journal of specialised trans-lation (jostrans)..elina lagoudaki.
2009. translation editing environ-in mt summit xii: workshop on beyond.
ments.
translation memories..ram pratap sharma and gyanendra k verma.
2015.human computer interaction using hand gesture.
procedia computer science, 54:721–727..samuel l¨aubli, sheila castilho, graham neubig, ricosennrich, qinlan shen, and antonio toral.
2020.a set of recommendations for assessing human–machine parity in language translation.
journal ofartiﬁcial intelligence research, 67:653–672..joss moorkens and sharon o’brien.
2017. assessinguser interface needs of post-editors of machine trans-lation.
in human issues in translation technology,pages 127–148.
routledge..meredith ringel morris, andreea danielescu, stevendrucker, danyel fisher, bongshin lee, mc schrae-fel, and jacob o wobbrock.
2014. reducing legacyinteractions,bias in gesture elicitation studies.
21(3):40–45..carlos s.c. teixeira, joss moorkens, daniel turner,joris vreeke, and andy way.
2019. creating a mul-timodal translation tool and testing machine transla-tion integration using touch and voice.
informatics,6..antonio toral, martijn wieling, and andy way.
2018.post-editing effort of a novel with statistical and neu-ral machine translation.
frontiers in digital human-ities, 5:9..olga torres-hostench,.
sharono’brien, and joris vreeke.
2017. testing interac-tion with a mobile mt post-editing app.
translation& interpreting, 9(2):138–150..joss moorkens,.
6772radu-daniel vatavu and jacob o wobbrock.
2015.formalizing agreement analysis for elicitation stud-ies: new measures, signiﬁcance test, and toolkit.
inproceedings of the 33rd annual acm conference onhuman factors in computing systems, pages 1325–1334..mihaela vela,.
santanu pal, marcos zampieri,sudip kumar naskar, and josef van genabith.
improving cat tools in the translation2019.inworkﬂow: new approaches and evaluation.
proceedings of machine translation summit xviivolume 2: translator, project and user tracks,pages 8–15..juan pablo wachs, mathias k¨olsch, helman stern, andyael edan.
2011. vision-based hand-gesture appli-cations.
communications of the acm, 54(2):60–71..frank weichert, daniel bachmann, bartholom¨ausrudak, and denis fisseler.
2013. analysis of the ac-curacy and robustness of the leap motion controller.
sensors, 13(5):6380–6393..jacob o wobbrock, htet htet aung, brandonrothrock, and brad a myers.
2005. maximizing theguessability of symbolic input.
in chi’05 extendedabstracts on human factors in computing systems,pages 1869–1872..jacob o wobbrock, meredith ringel morris, and an-drew d wilson.
2009. user-deﬁned gestures for sur-face computing.
in proceedings of the sigchi con-ference on human factors in computing systems,pages 1083–1092..juli´an zapata.
2016..translating on the go?.
in-vestigating the potential of multimodal mobile de-vices for interactive translation dictation.
revistatradum`atica: tecnologies de la traducci´o, 14:66–74..6773