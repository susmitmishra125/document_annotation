discriminative reranking for neural machine translation.
ann lee.
michael auli.
marc’aurelio ranzato.
facebook ai research{annl,michaelauli,ranzato}@fb.com.
abstract.
reranking models enable the integration ofrich features to select a better output hypoth-esis within an n-best list or lattice.
these mod-els have a long history in nlp, and we re-visit discriminative reranking for modern neu-ral machine translation models by training alarge transformer architecture.
this takes asinput both the source sentence as well as alist of hypotheses to output a ranked list.
thereranker is trained to predict the observed dis-tribution of a desired metric, e.g.
bleu, overthe n-best list.
since such a discriminator con-tains hundreds of millions of parameters, weimprove its generalization using pre-trainingand data augmentation techniques.
exper-iments on four wmt directions show thatour discriminative reranking approach is effec-tive and complementary to existing generativereranking approaches, yielding improvementsof up to 4 bleu over the beam search output..1.introduction.
reranking models take a number of different out-put hypotheses generated by a baseline model andselect one hypothesis based on more powerful fea-tures.
before the recent re-emergence of neuralnetworks, these models have been well studied forseveral nlp tasks including parsing (charniak andjohnson, 2005; collins and koo, 2005) and statis-tical machine translation (och et al., 2004; shenet al., 2004)..traditional statistical models (smt) based onn-gram counts made very strong independence as-sumptions where features would only capture verylocal context information to avoid sparsity and poorgeneralization.
a large n-best list produced bythese models would then be passed to a discrimi-natively trained reranker which leverages featuresengineered to capture more global context (ochet al., 2004) yielding signiﬁcant improvements tothe quality of the translations..on the other hand, modern neural models(nmt) make much weaker independence assump-tions because predictions of standard sequence-to-sequence models depend on the entire sourcesentence as well as the target preﬁx generated.
however, reranking may still be beneﬁcial for tworeasons: first, nmt systems are subject to expo-sure bias (ranzato et al., 2016), i.e., models arenever exposed to their own generations at trainingtime, while a reranking model has been trained onmodel outputs.
second, beam search with auto-regressive models uses the chain rule to sum indi-vidual token-level probabilities to obtain a targetsequence probability.
however, individual prob-abilities are based on a limited amount of targetcontext, while a reranking model can condition onthe entire target context.
indeed, recent genera-tive reranking approaches applied to nmt, such asnoisy-channel decoding (ncd, yee et al.
2019)which leverages a pre-trained language model and abackward model, show strong improvements overbeam search outputs, as demonstrated in recentwmt evaluations (ng et al., 2019)..in this paper, we explore whether training largetransformer models using the reranking objectivecan further improve performance.
our model,dubbed drnmt, takes as input the entire sourcesentence and an n-best list of output hypothesesto predict a distribution of sentence-level evalua-tion scores, such as bleu.1 this setup is similarto earlier work with smt, except that the baselinemodel is an nmt model and the reranker is a bigtransformer architecture as opposed to a log-linearmodel on top of discrete or human engineered fea-tures..unfortunately, optimizing for the task of interestdoes not always lead to better performance.
overﬁt-ting to the training set is a potential concern, as the.
1our approach is general and enables optimizing any user-.
speciﬁed metric, or combinations thereof..proceedingsofthe59thannualmeetingoftheassociationforcomputationallinguisticsandthe11thinternationaljointconferenceonnaturallanguageprocessing,pages7250–7264august1–6,2021.©2021associationforcomputationallinguistics7250reranker has hundreds of millions of parameters yetit receives only one gradient and weight update persource/target sentence pair as opposed to one per to-ken as for standard nmt models.
in our work, wemitigate overﬁtting in two ways.
first, we leveragethe success of pre-training by ﬁnetuning maskedlanguage models (mlm; devlin et al.
2019) whichinitializes the model with features trained on muchmore training data.
second, we augment the origi-nal dataset with back-translated data (bt; sennrichet al.
2016)..experiments show that drnmt can match theperformance of a strong ncd baseline and thattheir combination leads to further improvements asmeasured by bleu, ter and also human evalua-tion..2 related work.
our method is inspired by the seminal work of shenet al.
(2004) and och et al.
(2004) who introducedand popularized discriminative reranking to smt.
besides using a weaker mt system to generate then-best list, these works relied on a linear discrimina-tor trained on human-designed features as opposedto a transformer taking the raw source sentence andhypothesis..most work using nmt has focused on genera-tive reranking methods (liu et al., 2018; imamuraand sumita, 2017; wang et al., 2017), where thereranker’s parameters are optimized using a crite-rion which is different from the metric of interest.
for instance, yu et al.
(2017); yee et al.
(2019)perform noisy-channel decoding where hypothe-ses are scored by linearly combining the output ofthe forward model, a target-side language modeland a backward model which scores the source sen-tence given the hypothesis.
these methods haveshown remarkable improvements over the outputof beam decoding, despite not being trained for thereranking task (except for the two or three hyper-parameters of the linear combination of scoreswhich are tuned on a validation set).
another ap-proach belonging to this class of methods is theone proposed by salazar et al.
(2019), which em-ploys the scores from a masked language model(mlm).
while this method employs a transformerarchitecture, it is still not trained for the task ofinterest..to the best of our knowledge, there is only con-current work by naskar et al.
(2020) which at-tempts at training discriminatively a reranker for.
nmt.
they use a pair-wise margin loss on hypothe-ses sampled from the nmt, while we learn to rankthe full n-best list produced by beam.
their experi-ments also show that the reranker performs betterwhen directly conditioned on the source sentence.
however, they do not compare nor combine theirmethod with ncd like we do.
both their work andour work are however an extension of deng et al.
(2020), who proposed to train a discriminator toimprove neural language modeling..there is also a large body of literature on differ-ent ways to combine smt and nmt by using oneto rerank the other, since smt is generally betterat adequacy while nmt is better at ﬂuency.
forinstance, auli and gao (2014) uses an rnn dis-criminator to rerank the n-best list produced by aphrase-based smt.
instead, ehara (2017) does theopposite, using an smt discriminator to rerank ann-best list produced by an nmt..finally, our work is also related to recent at-tempts at using adversarial training to improvemt (wu et al., 2018; zhang et al., 2018).
un-like these approaches our method is much simplerbecause we do not update the parameters of the mtsystem generating the hypotheses.
moreover, ourdiscriminator is trained to predict the distributionof desired metric and it is used at decoding time torerank, while gan-based mt would only retainthe generator..3 model.
given a source sentence x, an nmt model gener-ates a set of hypotheses u(x) = {u1, u2, ..., un}in the target language.
the goal of this work isto learn a reranker that produces higher scores forhypotheses of better quality, as deﬁned in terms ofa user-speciﬁed metric µ(u, r) such as bleu (pap-ineni et al., 2002a), where quality is measured withrespect to a reference r..as illustrated in figure 1, our reranker is a trans-former architecture which takes as input the con-catenation of the source sentence x and hypothesisu ∈ u(x).
the architecture includes also positionembeddings and language embeddings, to help themodel represent tokens that are shared betweenthe two languages (conneau and lample, 2019).
the ﬁnal hidden state corresponding to the start ofsentence token ((cid:104)s(cid:105)) serves as the joint represen-tation for (x, u); let us denote this feature vectoras z ∈ rd.
the reranker associates a scalar scoreo ∈ r to (x, u) by applying a one hidden layer.
7251figure 1: illustration of drnmt, a pre-trained transformer architecture which takes as input both the source sen-tence as well as a hypothesis and outputs a scalar score.
drnmt is trained to output scores which reﬂect thedistribution of sentence-level scores according to a user-speciﬁed metric over an n-best list..neural network with d tanh hidden units to z, asdefault in the design of the “classiﬁcation head” ofroberta (liu et al., 2019).
the parameters of thereranker are denoted by θ and include the param-eters of the transformer, all the embeddings andalso the top projection block mapping the featurevector to the scalar score.
each hypothesis ui inthe set u(x) is therefore processed independentlyand yields a score oi..4 training and inference.
we train the reranker discriminatively, hence thename drnmt for discriminative reranker fornmt, by minimizing the kl-divergence betweenthe target distribution and the model output distri-bution, dkl(pt ||pm ) (cao et al., 2007).
for eachx, the model output distribution is a softmax overall n hypotheses in the n-best list:.
pm (ui|x; θ) =.
exp(oi(ui|x; θ))j=1 exp(oj(uj|x; θ)).
,.
(cid:80)n.(1).
where we made explicit that the score oj is con-ditioned on the input x and parameter vector θ.notice that we do not enforce any additional fac-torization.
in particular, we do not assume that thescore is computed auto-regressively..the target distribution is deﬁned as a normalizeddistribution of the end metric µ(ui, r) which weassume to improve as it takes on larger values:.
pt (ui) =.
exp(µ(ui, r)/t )j=1 exp(µ(uj, r)/t ).
,.
(cid:80)n.(2).
where t is the temperature to control the smooth-ness of the distribution.
in practice, we apply a min-max normalization on µ. we subtract each valueby the minimum in the hypothesis set, and divide.
the result by the difference between the maximumand the minimum value, so that the best hypothesisscores 1 and the worst 0. this helps the optimiza-tion as it reduces the variance of the gradients, aspointed out by edunov et al.
(2018)..the parameters of drnmt are then learned byminimizing the kl divergence over the trainingdataset.
for a given training example, we have:.
l(θ) = −.
pt (uj) log pm (uj|x; θ)..(3).
n(cid:88).
j=1.
we minimize this loss over the training set bystochastic gradient descent using standard back-propagation of the error, since all terms are differ-entiable.
in order to alleviate overﬁtting, we em-ploy dropout regularization (srivastava et al., 2014),we pre-train the model (conneau et al., 2019) andwe also perform data augmentation by training onback-translated data (bt) (sennrich et al., 2016).
see §5.3 for details..at test time, generation proceeds by ﬁrst hav-ing the nmt generate the n-best list, and then byapplying the reranker to select the best hypothesis.
since the score of the forward model is also avail-able, unless otherwise speciﬁed we rerank usinga weighted combination of both; this is dubbed asdrnmt.
in the experiments we also report resultsby adding all the other scores from ncd, namelythe backward model score and the language modelscore.
we denote this variant by ”drnmt + ncd”.
whenever we combine scores from various modelswe tune the additional hyper-parameters control-ling the weighted combination by random searchon the validation set (yee et al., 2019)..7252transformer<s>0en++↑this1en++↑is2en++↑a3en++↑book4en++↑</s>5en++↑das1de++↑ist2de++↑ein3de++↑buch4de++↑</s>5de++↑token embeddingsposition embeddingslanguage embeddingsscore↑5 experimental setup.
de-en en-de en-ta ru-en.
in this section we describe the datasets, baselinesand model details..5.1 datasets.
(en-ta).
we experiment on four language pairs: german-english (de-en), english-german (en-de),and russian-englishenglish-tamil(ru-en).
for training on de-en and en-de, we usenewscommentary from wmt’19 (barrault et al.,2019) and newscrawl2018 for the parallel datasetand target side monolingual data, respectively.
we validate on newstest2014 and newstest2015,and test on newstest2016, 2017, 2018 and 2019.for en-ta, we use all bitext and monolingualdata shared by the wmt’20 news translationtask for training, and the ofﬁcially releaseddevelopment and test sets for validation and testingpurposes.
for ru-en, we use all the paralleldata from wmt’19 (barrault et al., 2019) andnewscrawl2018 as the monolingual dataset fortraining, validate on newstest2015 and 2016, andtest on newstest 2017, 2018 and 2019..we follow the steps in ng et al.
(2019) fordata preprocessing, including sentence deduplica-tion, language identiﬁcation ﬁltering on all bitextand monolingual data (joulin et al., 2017) and in-domain ﬁltering (moore and lewis, 2010) on tamilcommoncrawl data.
table 1 shows the resultingsize of each dataset.
for the base nmt models,we learn 30k byte-pair encoding (bpe) units forde-en and en-de, 20k bpe units for en-ta and24k bpe units for ru-en separately, using thesentencepiece toolkit (kudo and richardson,2018).
all systems are evaluated using sacre-bleu (post, 2018)..5.2 baselines.
we use the transformer (vaswani et al., 2017) ar-chitecture and train mt models using bitext dataonly.
these are the models that generate the n-bestlist, and which serve also as a lower bound for theperformance of drnmt.
bt data is generated frombeam decoding with beam size equal to 5. since thebitext data of en-ta originates from seven differentsources, we prepend dataset tags to each sourcesentence to indicate the origin (kobus et al., 2017).
we do not prepend any tags on the validation andtest sets when decoding, as this choice worked bestduring cross-validation.
in general and for eachlanguage pair, we tune the model architecture and.
bitext326ktraining5.2kvalidation11ktestmonolingual 17m.
326k 621k 28.9m5.8k2k5.2k8k1k11k37m 27m 17m.
table 1: number of sentences in each dataset used inthe experiments after pre-processing..all hyper-parameters on the validation set..in addition to beam decoding, we consider tworeranking baselines.
first, we consider the methodrecently introduced by salazar et al.
(2019).
inits simplest formulation, this takes a pre-trainedmasked language model (mlm) on the target side,and iteratively masks one word of the hypothesis atthe time and aggregates the corresponding scoresto yield a score for the whole hypothesis.
then,this score is combined with the score of the forwardmodel to rerank the n-best list; this is dubbed as “fw+ mlm”.
we also have a version of mlm whichis tuned on our target side monolingual dataset; wedub this “fw + mlm-ft”..finally, we consider reranking using noisy chan-nel decoding (ncd; yee et al.
2019).
ncd reranksby taking a weighted combination of three scores:the forward model score, the score of a target-sidelanguage model (lm), and the score of a backwardmodel.
a length penalty is then applied on the com-bined score.
the weights and the length penalty aretuned on the validation set via random search.
alllms are transformers with 16 blocks, 16 attentionheads and embedding size 1024. they are trainedon the target side monolingual data only..5.3 setting up drnmt.
2 (conneau et al., 2019), awe use xlm-rbasetransformer-based multilingual mlm trained onmore than 2.5t of of ﬁltered commoncrawl datain 100 languages, including en, de, ta and ru, asthe pre-trained model for drnmt.
the same modelis also used in the mlm baseline described in §5.2.
the xlm-rbase model consists of 12 transformerblocks, 12 attention heads, embedding size 768(270m params) and has a vocabulary size of 250kbpe units.
as each training sample of xlm-ronly contained one single language, we further en-hance the model with two language embeddings,.
2https://github.com/pytorch/fairseq/.
tree/master/examples/xlmr.
7253de-en.
en-de.
en-ta.
ru-en.
bleubeam (fw)+ mlm (salazar et al., 2019)+ mlm-ft (salazar et al., 2019)+ lmncd (yee et al., 2019)drnmt+ ncdoracle bleu.
valid24.725.725.826.327.227.627.933.3.test27.728.728.829.230.931.531.837.4.valid23.123.523.724.324.824.725.131.4.test26.627.127.528.529.129.029.735.9.valid8.88.88.89.49.79.710.013.6.test6.05.85.86.26.36.46.59.5.valid33.533.833.934.635.335.335.745.3.test34.334.835.035.836.837.137.347.0.table 2: validation and test bleu with beam size 50. results for de-en and en-de are averaged from new-stest2014 and 2015 for validation and newstest2016, 2017, 2018 and 2019 for test.
the results for ru-en areaveraged from newstest2015 and 2016 for validation and newstest2017, 2018 and 2019 for test..initialized from random, to indicate the source andtarget languages for the reranker..we perform beam decoding on both bitext andbt data using the baseline mt models to generaten-best lists with 50 hypotheses.
we combine n-bestlists from both bitext and bt as training data forthe rerankers for de-en, en-de and en-ta, and useonly bt data for ru-en.
we train drnmt withbatch size 512, use adam (kingma and ba, 2015)and early-stop when the validation performancedoes not improve after 12k parameter updates.
allhyper-parameters, including learning rate, numberof warmup steps, dropout rate, etc., are tuned onthe validation set.
all models are implemented andtrained using fairseq (ott et al., 2019)3..6 results.
in this section we report the main ﬁndings of ourwork.
when optimizing for bleu as metric, theperformance of drnmt and baselines for de-en,en-de, en-ta and ru-en is summarized in table 2.the ﬁndings are similar across the four languagedirections.
we therefore focus the discussion onthe de-en test set results..first, we notice that all methods improve overthe beam search output with gains ranging from1.0 to 4.1 bleu.
however, there may be still roomfor improvement as the oracle performance sug-gests.
the oracle is computed by selecting the besthypotheses based on bleu with respect to the hu-man reference.
of course, the oracle may be notachievable because of uncertainty in the translationtask..3code for.
reproducing the results can be foundhttps://github.com/pytorch/fairseq/.
at:tree/master/examples/discriminative_reranking_nmt.
second, salazar et al.
(2019)’s method, particu-larly the version ﬁne-tuned on the in-domain train-ing dataset, improves upon beam by 1.1 bleupoints.
however, the improvement over beam isnot as large as with ncd, which improves uponbeam by 3.2 bleu points, suggesting that amongthe non-discriminative reranking methods ncdperforms the best..third, drnmt performs on par (en-ta, en-deand ru-en) or better (de-en) than ncd, showingthat discriminative reranking can be very competi-tive.
note, that the reranker requires only one ad-ditional forward pass through the hypotheses gen-erated by beam, while ncd requires two forwardpasses (one for the lm and one for the backwardmt model).
therefore, our reranker works at leastas well as ncd while requiring roughly half of thecompute..fourth, the discriminative reranker and ncd arecomplementary to each other, since combining bothachieves the best performance overall across thethree language directions, with gains between 0.9bleu (de-en) and 0.2 (en-ta) compared to ncd,and an overall gain between 4.1 bleu (de-en)and 0.5 (en-ta) compared to the beam baseline..fifth, the gain brought by discriminative rerank-ing can be better appreciated by comparing ”fw +lm” and drnmt, as the major difference betweenthe two approaches is the objective function usedfor training them (generative language modelinginstead of prediction of the distribution of bleuscores).
we can see that in all cases, discriminativereranking yields better translations, with gains be-tween 0.2 and 2.3 bleu points depending on thelanguage direction..finally, we notice that en-ta is a difﬁcult lan-.
7254valid.
test.
bleu ter bleu ter58.024.754.127.627.954.253.527.053.427.3.
27.731.531.830.731.1.
60.957.757.957.357.0.beamdrnmt (b)+ ncddrnmt (t)+ ncd.
table 3: average validation and test bleu and teron wmt‘19 de-en with beam size 50 from rerankerstrained with different metrics (b: bleu, t: ter)..guage pair, in which the baseline nmt is weak andnone of the reranking approaches work nearly aswell as in the other language directions.
the dif-ference between validation and test bleu scoressuggests also a certain degree of overﬁtting tothe validation set.
despite this, our reranker stillyields the largest improvement over beam.
ap-pendix b shows similar trends when test perfor-mance is measured in terms of translation error rate(ter) (snover et al., 2006), showing that drnmtis not particularly overﬁtting to the training metric..human evaluation: we randomly sample 750sentences from the de-en test sets and collect hu-man ratings.
we perform a/b testing, where a ratercan see the source sentence together with trans-lated sentences from two systems.
we conduct tworounds of human evaluation by comparing the pro-posed ”drnmt + ncd” vs. ”beam”, and ”drnmt+ ncd” vs. ”ncd”.
for each sentence, we collectthree ratings (between 0 to 100) and average thescores, treating sentences with a score differenceless than 5 as equally good.
out of the 750 sen-tences, our proposed method generates better trans-lation than beam on 149 sentences and is worse on82 sentences, and it performs better than ncd on123 sentences and worse on 108 sentences, corrob-orating the gains observed when measuring withbleu..next we show that drnmt works with other user-speciﬁed metrics, study how performance varieswith the number of hypotheses and perform sev-eral ablation studies to better understand its criticalcomponents..in terms of both bleu and ter when optimizingfor either one of the two metrics.
while the twometrics are correlated, the best results are achievedwhen optimizing for the metric used at test time..6.2 varying the number of hypotheses.
we examine the effect of training the rerankerwith different sizes of the n-best list, u(x).
eventhough we ﬁx the n-best list size at training time,we can apply the reranker on n-best lists of differ-ent sizes at test time.
figure 2 shows the perfor-mance of drnmt on de-en validation sets fromfour rerankers trained with 5, 10, 20 and 50 hy-potheses, respectively..as the size of the n-best list during test time in-creases, the performance of all rerankers and ncdimprove.
on the other hand, the performance ofbeam decoding starts to saturate early at beam size10. a reranker trained with 50 hypotheses gives a1.4 bleu improvement over beam decoding whenbeam size is only 5 at test time, and the improve-ment increases to 3.4 bleu as we increase thebeam size to 200 at test time.
drnmt consistentlyperform better than or equally well as ncd in alltraining and testing scenarios..interestingly, a reranker trained with more hy-potheses performs better than one trained withfewer hypotheses, regardless of the beam size usedat test time.
for instance, when the beam size is20 at test time, the reranker trained with beam 50improves over beam by 2.3 bleu points, whilethe one which was trained with 20 like at test time,improves by 2.2 bleu points..to our surprise, a reranker trained with only 5hypotheses can still yield a 3.2 bleu gain com-pared with beam decoding when used to rerank200 hypotheses during test time, indicating that thereranker suffers little from the mismatch betweentraining and testing conditions.
as a result, depend-ing on available compute resources, one can decideto set the number of hypotheses to the largest valuepossible to get better test time performance withlarger n-best lists, while being robust to the partic-ular choice used at training time..6.3 ablation study.
6.1 optimizing for a different metric.
in order to validate the generality of drnmt, weconsider as metric µ the opposite of ter, so thatlarger values indicate better translation quality..table 3 shows validation and test performance.
we report an ablation study by probing all majordesign choices made.
we train drnmt by optimiz-ing bleu and evaluate it on the validation set ofthe de-en task using 50 hypotheses both at trainingand test time.
table 4 summarizes all the results..7255uelb.
26.
28.
24.beamdrnmt (5)drnmt (20).
ncddrnmt (10)drnmt (50).
510 20.
50.
100.
200.number of hypotheses in test time.
figure 2: bleu on the validation set of de-en ofrerankers trained with n equal to 5, 10, 20 or 50 hy-potheses (denoted by “drnmt (n)”) and ncd whenreranking using different numbers of hypotheses at testtime (x-axis)..proposed- pre-training- source sentence- normalization- bt data6 layers3 layers.
valid27.626.827.427.225.627.126.7.table 4: ablation study on the various design choicesof the proposed approach.
all results are evaluated onthe de-en validation set..pre-training: we investigate the importance ofpre-training by comparing with a reranker of thesame size initialized with random weights.
ta-ble 4 shows that a randomly initialized rerankerperforms signiﬁcantly less well, with a decrease of0.8 bleu.
in addition to lower performance, a ran-domly initialized reranker also trains more slowly,by requiring 1.6× more weight updates comparedto the pre-trained reranker to converge.
this cor-roborates our choice to pre-train, as the rerankingtask is fairly related to the pre-training task andwe lack sufﬁcient labeled data to train such a largemodel from scratch.
notice that our pre-trainedreranker trains for at most two passes over the databefore starting to overﬁt to its training set..source sentence: when comparing “fw + lm”against drnmt to assess the impact of trainingdiscriminatively, we did not take into account aconfounding factor which is the fact that the lm.
does not attend over the source sentence.
indeed,salazar et al.
(2019) score hypotheses without tak-ing into account the source sentence.
what is thegain brought by considering also the source sen-tence?
to answer this question we compare ourreranker with a reranker that takes as input onlythe hypotheses.
as shown in table 4, includingthe source sentences achieves a small gain of 0.2bleu..normalization: we apply minmax normaliza-tion and set t = 0.5 when computing the targetdistribution in the training objective, so that for ev-ery source sentence, the range of the bleu scoresof its hypotheses is between 0 and 2. this choiceyields a 0.4 bleu improvement compared to areranker trained with the raw bleu scores..training data: so far we’ve been training thereranker with both bitext and bt data.
in table 4,we see that training the reranker with only bitextdata deteriorates the model’s performance by 2bleu points.
the model starts overﬁtting after15 passes over the small bitext (around 9,000 pa-rameter updates).
incorporating the bt data helpsalleviate this issue.
the model achieves the bestvalidation performance after 1.9 passes over thecombination of bitext and bt data (around 63,000parameter updates)..model size: we explore building the rerankerusing only the ﬁrst few layers of the xlm-rbasemodel.
since beam hypotheses often differ only lo-cally on isolated phrases, one may wonder whethermore local features, as those produced by a shal-lower reranker may work better.
moreover, re-ducing the model capacity may help preventingoverﬁtting.
compared with either only three orsix transformer blocks, table 4 shows that deeperand bigger models work better, despite being moreprone to overﬁtting and despite capturing moreglobal information about their input..6.4 other training and model variations.
we conclude our empirical evaluation by investigat-ing how reranking works on top of baseline nmtmodels trained with back-translation, and by re-porting two variations of model architectures.
asbefore, we report results on the validation set of thede-en task with n-best list of size 50, using bleuas metric..mt trained with bitext+bt: would the gainsbrought by the reranker carry over when this is ap-.
7256beam (fw)+ mlm (salazar et al., 2019)+ mlm-ft (salazar et al., 2019)+ lmncd (yee et al., 2019)drnmt+ ncd.
valid31.632.632.633.133.333.133.6.table 5: reranking the output of a baseline trainedwith back-translation..plied on the n-best list produced by a baseline nmtmodel trained with back-translation?
as shown intable 2 the beam baseline on validation was at 24.7bleu, while if we train the nmt by adding back-translated data, bleu increases to 31.6 (table 5).
in this case, we train the reranker using hypothe-ses generated by the more powerful nmt modeltrained with back-translated data.
from table 5,we can see that drnmt gives 1.5 bleu improve-ment over the beam decoding baseline, and com-bining ncd and reranker gives an additional gainof 0.5 bleu, which is less than what we reportedin table 2 but still conﬁrming the overall ﬁnding ofdiscriminative reranker and ncd performing simi-larly while being complementary to each other..causal vs. bidirectional: as the complete hy-pothesis is available during reranking, the architec-ture of our reranker is bidirectional as it conditionson the whole sentence.
this contrasts with howthe baseline nmt model generates hypotheses andhow it scores them with beam which leverages anauto-regressive decomposition.
here we explorethe importance of joint modeling and consider analternative reranker which consists of an encoderand a causal decoder, and which is therefore ini-tialized from the base nmt generating the n-bestlist.
given a source sentence and a hypothesis asinput, the output of the decoder is a t × d matrix(notice that hidden states are causal), where t isthe number of tokens of the hypothesis, and d is thehidden dimension.
we average the output acrossposition to obtain a d-dimensional representationand apply the same one-hidden layer neural net-work to obtain a reranking score.
table 6 showsthat our bidirectional architecture outperforms thecausal architecture by 0.8 bleu..set reranker: while our training objective con-siders the full set of hypotheses of each source.
valid26.827.6.valid27.627.6.encoder + causal decoderbi-directional (proposed).
table 6: effect of a causal vs. non-causal reranker..set-levelhypothesis-level (proposed).
table 7: reranking with features computed over theentire n-best list (set-level reranking) vs. features fromjust the current hypothesis..sentence, the reranker scores each pair of (x, ui) inisolation; it never compares hypotheses directly.
we therefore explore an architecture that com-putes cross-hypothesis features.
in the originalreranker architecture, the model produces a d-dimensional representation for each (x, ui).
weadd another transformer block that computes self-attention across the set of n representations for{(x, u)|u ∈ u(x)}.
we then apply the one hiddenlayer projection block to map each d dimensionalvector to a single score as before, yielding n scoresfor reranking.
this design enables the model tohave set-level information during reranking, andthus the scoring has to be performed on the fullset at once.
table 7 shows that these two modelvariants perform the same, suggesting that set levelrepresentations may need to be captured at a lowerlayer of the transformer.
we leave this avenue ofexploration for future work..7 conclusions.
reranking is effective for both smt and nmt.
in-spired by work done almost two decades ago (shenet al., 2004; och, 2003), we studied discriminativereranking for nmt and found that it performs atleast as well as the strongest generative rerankingmethod we are aware of, namely noisy channel de-coding (ncd) (yee et al., 2019) - as long as careis taken to alleviate overﬁtting..there is a subtle trade-off between improve-ments stemming from optimizing the end metricand addressing exposure bias on the one hand, andpoor generalization and sample inefﬁciency of dis-criminative training on the other hand.
in this studywe regularize the reranker by using dropout, bypre-training on large corpora and by performingdata augmentation..empirically, we found that ncd and our discrim-.
7257inative reranker are complementary to each other,yielding sizeable improvements over each otherand the beam baseline.
our reranker is computa-tionally less demanding than ncd, since it consistsof a single model while ncd requires scoring us-ing two additional models.
our reranker is alsorobust to the choice of the size of the n-best list andother hyper-parameters settings..in the future we plan to investigate better waysto alleviate sample inefﬁciency, as well as to designmore effective architectures to score at the set level..acknowledgments.
we would like to thank peng-jen chen for his guid-ance on training nmt systems, and sergey edunovfor his advice on setting up the human evaluation..references.
michael auli and jianfeng gao.
2014. decoder integra-tion and expected bleu training for recurrent neu-ral network language models.
in proceedings of the52nd annual meeting of the association for compu-tational linguistics (volume 2: short papers), pages136–142..lo¨ıc barrault, ondˇrej bojar, marta r costa-juss`a,christian federmann, mark fishel, yvette gra-ham, barry haddow, matthias huck, philipp koehn,shervin malmasi, et al.
2019. findings of the 2019conference on machine translation (wmt19).
inproceedings of the fourth conference on machinetranslation (volume 2: shared task papers, day 1),pages 1–61..zhe cao, tao qin, tie-yan liu, ming-feng tsai, andhang li.
2007. learning to rank: from pairwise ap-proach to listwise approach.
in proceedings of the24th international conference on machine learning,pages 129–136..eugene charniak and mark johnson.
2005. coarse-to-ﬁne n-best parsing and maxent discriminativereranking.
in proceedings of the 43rd annual meet-ing of the association for computational linguistics(acl’05), pages 173–180..michael collins and terry koo.
2005. discriminativereranking for natural language parsing.
computa-tional linguistics..alexis conneau, kartikay khandelwal, naman goyal,vishrav chaudhary, guillaume wenzek, franciscoguzm´an, edouard grave, myle ott, luke zettle-moyer, and veselin stoyanov.
2019. unsupervisedcross-lingual representation learning at scale.
arxivpreprint arxiv:1911.02116..alexis conneau and guillaume lample.
2019. cross-in advances.
lingual language model pretraining..in neural information processing systems, pages7059–7069..yuntian deng, anton bakhtin, myle ott, arthur szlam,and marc’aurelio ranzato.
2020. residual energy-in internationalbased models for text generation.
conference on learning representations..jacob devlin, ming-wei chang, kenton lee, andkristina toutanova.
2019. bert: pre-training ofdeep bidirectional transformers for language under-standing.
in proceedings of the 2019 conference ofthe north american chapter of the association forcomputational linguistics: human language tech-nologies, volume 1 (long and short papers), pages4171–4186..sergey edunov, myle ott, michael auli, david grang-ier, and marc’aurelio ranzato.
2018. classicalstructured prediction losses for sequence to se-in proceedings of the 2018 con-quence learning.
ference of the north american chapter of the asso-ciation for computational linguistics: human lan-guage technologies, volume 1 (long papers), pages355–364..terumasa ehara.
2017. smt reranked nmt.
in work-.
shop on asian translation..kenji imamura and eiichiro sumita.
2017. ensembleand reranking: using multiple models in the nict-2neural machine translation system at wat2017.
inworkshop on asian translation..armand joulin, ´edouard grave, piotr bojanowski, andtom´aˇs mikolov.
2017. bag of tricks for efﬁcient textin proceedings of the 15th confer-classiﬁcation.
ence of the european chapter of the association forcomputational linguistics: volume 2, short papers,pages 427–431..diederik p. kingma and jimmy ba.
2015. adam: ain 3rd inter-method for stochastic optimization.
national conference on learning representations,iclr 2015, san diego, ca, usa, may 7-9, 2015,conference track proceedings..catherine kobus, josep m crego, and jean senellart.
2017. domain control for neural machine transla-tion.
in proceedings of the international conferencerecent advances in natural language processing,ranlp 2017, pages 372–378..taku kudo and john richardson.
2018. sentencepiece:a simple and language independent subword tok-enizer and detokenizer for neural text processing.
inproceedings of the 2018 conference on empiricalmethods in natural language processing: systemdemonstrations, pages 66–71..yinhan liu, myle ott, naman goyal, jingfei du, man-dar joshi, danqi chen, omer levy, mike lewis,luke zettlemoyer, and veselin stoyanov.
2019.roberta: a robustly optimized bert pretrainingapproach.
arxiv preprint arxiv:1907.11692..7258yuchen liu, long zhou, yining wang, yang zhao, ji-ajun zhang, and chengqing zong.
2018. a com-parable study on model averaging, ensembling andreranking in nmt.
in international conference onnatural language processing and chinese comput-ing, pages 299–308.
springer..robert c moore and william lewis.
2010. intelligentin pro-selection of language model training data.
ceedings of the association for computational lin-guistics 2010 conference short papers, pages 220–224..subhajit naskar, amirmohammad rooshenas, simengsun, mohit iyyer, and andrew mccallum.
2020.energy-based reranking:improving neural ma-chinetranslation using energy-based models.
arxiv:2009.13267..matt post.
2018. a call for clarity in reporting bleuscores.
in proceedings of the third conference onmachine translation: research papers, pages 186–191..marc’aurelio ranzato, sumit chopra, michael auli,and wojciech zaremba.
2016. sequence level train-ing with recurrent neural networks.
in internationalconference on learning representations..julian salazar, davis liang, toan q nguyen, and ka-trin kirchhoff.
2019. masked language model scor-ing.
arxiv preprint arxiv:1910.14659..rico sennrich, barry haddow, and alexandra birch.
2016. improving neural machine translation modelswith monolingual data.
in association for computa-tional linguistics..nathan ng, kyra yee, alexei baevski, myle ott,michael auli, and sergey edunov.
2019. facebookfair’s wmt19 news translation task submission.
in proceedings of the fourth conference on ma-chine translation (volume 2: shared task papers,day 1), pages 314–319..libin shen, anoop sarkar, and franz josef och.
2004.discriminative reranking for machine translation.
inproceedings of the human language technologyconference of the north american chapter of theassociation for computational linguistics: hlt-naacl 2004, pages 177–184..franz josef och.
2003. minimum error rate training instatistical machine translation.
in proceedings of the41st annual meeting of the association for compu-tational linguistics, pages 160–167, sapporo, japan.
association for computational linguistics..matthew snover, bonnie dorr, richard schwartz, lin-nea micciulla, and john makhoul.
2006. a study oftranslation edit rate with targeted human annotation.
in proceedings of association for machine transla-tion in the americas..franz josef och, daniel gildea, sanjeev khudanpur,anoop sarkar, kenji yamada, alexander fraser,shankar kumar, libin shen, david a smith, kather-ine eng, et al.
2004. a smorgasbord of features forin proceedings ofstatistical machine translation.
the human language technology conference of thenorth american chapter of the association for com-putational linguistics: hlt-naacl 2004, pages161–168..myle ott, sergey edunov, alexei baevski, angelafan, sam gross, nathan ng, david grangier, andfairseq: a fast, extensiblemichael auli.
2019.in proceedings oftoolkit for sequence modeling.
the 2019 conference of the north american chap-ter of the association for computational linguistics(demonstrations), pages 48–53..kishore papineni, salim roukos, todd ward, and wei-jing zhu.
2002a.
bleu: a method for automaticevaluation of machine translation.
in proceedings ofthe 40th annual meeting of the association for com-putational linguistics, pages 311–318..kishore papineni, salim roukos, todd ward, and wei-jing zhu.
2002b.
bleu: a method for automatic eval-in proceedings ofuation of machine translation.
the 40th annual meeting of the association for com-putational linguistics, pages 311–318, philadelphia,pennsylvania, usa.
association for computationallinguistics..nitish srivastava, geoffrey hinton, alex krizhevsky,ilya sutskever, and ruslan salakhutdinov.
2014.dropout: a simple way to prevent neural networksfrom overﬁtting.
journal of machine learning re-search, 15(56):1929–1958..ashish vaswani, noam shazeer, niki parmar, jakobuszkoreit, llion jones, aidan n gomez, łukaszkaiser, and illia polosukhin.
2017. attention is allin advances in neural information pro-you need.
cessing systems, pages 5998–6008..yuguang wang, shanbo cheng, liyang jiang, jiajunyang, wei chen, muze li, lin shi, yanfeng wang,and hongtao yang.
2017. sogou neural machinein proceedingstranslation systems for wmt17.
of the second conference on machine translation,pages 410–415..lijun wu, yingce xia, li zhao, fei tian, tao qin, jian-huang lai, and tie-yan liu.
2018. adversarial neu-ral machine translation.
arxiv:1704.06933..kyra yee, yann dauphin, and michael auli.
2019.simple and effective noisy channel modeling forin proceedings of theneural machine translation.
2019 conference on empirical methods in natu-ral language processing and the 9th internationaljoint conference on natural language processing(emnlp-ijcnlp), pages 5696–5701, hong kong,china.
association for computational linguistics..7259lei yu, phil blunsom, chris dyer, edward grefen-stette, and tom´as kocisk´y.
2017. the neural noisyin international conference on learningchannel.
representations, iclr 2017..zhirui zhang, shujie liu, mu li, ming zhou, and en-hong chen.
2018. bidirectional generative adversar-ial networks for neural machine translation.
in pro-ceedings of the 22nd conference on computationalnatural language learning, pages 190–199..7260d examples.
table 16 and table 17 show examples of translationfrom ncd and drnmt + ncd..a training details.
a.1 mt model.
we build the baseline mt models in table 2 fol-lowing the transformer big architecture (vaswaniet al., 2017) with 6 layers, embedding size 1024 and16 attention heads.
table 8 shows the additionalhyper-parameters that we tune on the validation setfor the best performing models of each languagedirection.
we use adam with β1 = 0.9, β2 =0.98, (cid:15) = 0.00000001, and apply an inverse squareroot learning rate schedule with 4000 warmup steps.
we train for 200 epochs for de-en, en-de and en-ta, and 100k updates for ru-en, and select thebest checkpoint based on validation loss..a.2 lm.
for all lms, we use 16 transformer layers, embed-ding size 1024, feed-forward network embeddingsize 4096 and 16 attention heads.
we optimizewith nag with learning rate 0.0001 and a cosinelearning rate schedule with 16k warmup steps.
allmodels are trained on 32 gpus for a maximumof 984k steps, and the best checkpoint is selectedbased on validation loss..a.3 drnmtwe train drnmt using adam with β1 = 0.9, β2 =0.98, (cid:15) = 0.000001, and apply a polynomial learn-ing rate decay schedule with 8000 warmup stepsfor de-en, en-ta, and ru-en, and 16k warmupsteps for en-de.
we use a learning rate of 0.00005and dropout 0.2 for de-en, en-ta, and ru-en, anda learning rate of 0.00001 and dropout 0.1 for en-de..b ter results.
table 9 summarizes the average validation and testter (snover et al., 2006) of drnmt trained withbleu (papineni et al., 2002b) scores.
table 10, ta-ble 11, and table 12 show ter of each validationand test set for de-en, en-de and ru-en, respec-tively.
note that for en-ta we only have one vali-dation and one test set..c bleu results.
table 13, table 14, and table 15 show the perfor-mance of drnmt, trained and evaluated on bleu,on each validation and test set for de-en, en-deand ru-en, respectively.
the average validationand test bleu scores of each language pair arereported in the main paper in table 2..7261# params.
dropout.
ffn embedsize4096409640968192.learningrate0.00070.00030.00070.0007.labelsmoothing0.20.30.30.1.max tokensper gpu4000400040003584.
# gpus.
444128.
0.30.40.30.2.de-enen-deen-taru-en.
207m207m197m276m.
table 8: baseline mt model hyper-parameters.
de-en.
en-de.
en-ta.
ru-en.
terbeam (fw)+ mlm (salazar et al., 2019)+ mlm-ft (salazar et al., 2019)+ lmncd (yee et al., 2019)drnmt+ ncd.
valid60.960.960.859.758.457.757.9.test58.058.258.257.154.954.154.2.valid67.166.466.565.865.265.264.9.test63.262.662.661.660.960.660.1.valid85.185.785.784.884.183.983.5.test88.288.889.188.687.887.587.4.valid52.752.552.451.951.250.550.6.test52.352.252.051.350.249.349.6.table 9: validation and test ter with beam size 50. the results for de-en and en-de are averaged from new-stest2014 and 2015 for validation and newstest2016, 2017, 2018 and 2019 for test.
the results for ru-en areaveraged from newstest2015 and 2016 for validation and newstest2017, 2018 and 2019 for test.
drnmt wastrained using bleu..terbeam (fw)+ mlm (salazar et al., 2019)+ mlm-ft (salazar et al., 2019)+ lmncd (yee et al., 2019)drnmt+ ncd.
201461.861.761.660.759.158.658.8.avg60.960.960.859.758.457.757.9.
201656.156.156.154.952.752.252.2.
201760.060.160.159.057.356.456.5.table 10: validation and test ter on wmt’19 de-en with beam size 50. drnmt was trained using bleu..terbeam (fw)+ mlm (salazar et al., 2019)+ mlm-ft (salazar et al., 2019)+ lmncd (yee et al., 2019)drnmt+ ncd.
201468.567.767.867.066.566.466.2.avg67.166.466.565.865.265.264.9.
201661.861.061.360.56059.358.9.
201767.766.967.166.265.865.965.1.table 11: validation and test ter on wmt’19 en-de with beam size 50. drnmt was trained using bleu..test201853.453.553.452.450.249.249.4.test201856.055.455.454.653.653.352.7.
201962.463.263.161.959.558.458.7.avg58.058.258.257.154.954.154.2.
201967.367.166.665.164.363.963.5.avg63.262.662.661.660.960.660.1.valid201559.960.060.058.757.656.756.9.valid201565.765.065.164.663.963.963.5.
7262terbeam (fw)+ mlm (salazar et al., 2019)+ mlm-ft (salazar et al., 2019)+ lmncd (yee et al., 2019)drnmt+ ncd.
201551.851.651.550.950.349.649.7.valid201653.653.453.352.852.151.351.5.avg52.752.552.451.951.250.550.6.
201748.948.648.448.047.146.246.4.test.
201854.454.454.453.552.552.152.1.
201953.553.553.252.551.149.750.4.avg52.352.252.051.350.249.349.6.table 12: validation and test ter on wmt’19 ru-en with beam size 50. drnmt was trained using bleu..bleubeam (fw)+ mlm (salazar et al., 2019)+ mlm-ft (salazar et al., 2019)+ lmncd (yee et al., 2019)drnmt+ ncdoracle bleu.
bleubeam (fw)+ mlm (salazar et al., 2019)+ mlm-ft (salazar et al., 2019)+ lmncd (yee et al., 2019)drnmt+ ncdoracle bleu.
valid201526.027.027.127.728.428.929.134.7.valid201524.525.025.125.726.326.226.633.1.
201423.324.424.424.926.026.226.631.8.
201421.622.022.322.923.323.223.629.6.avg24.725.725.826.327.227.627.933.3.avg23.123.523.724.324.824.725.131.4.
201629.230.230.331.032.833.233.539.2.
201627.627.928.429.029.729.930.437.1.
201726.127.227.327.729.029.629.935.2.
201722.823.423.424.324.624.325.131.2.table 13: validation and test bleu on wmt’19 de-en with beam size 50..201924.024.724.925.126.727.527.833.7.
201923.123.524.325.626.426.327.031.6.avg27.728.728.829.230.931.531.837.4.avg26.627.127.528.529.129.029.735.9.test201831.632.532.733.134.935.635.941.6.test201832.933.534.034.935.735.436.343.6.test.
table 14: validation and test bleu on wmt’19 en-de with beam size 50..bleubeam (fw)+ mlm (salazar et al., 2019)+ mlm-ft (salazar et al., 2019)+ lmncd (yee et al., 2019)drnmt+ ncdoracle bleu.
201533.333.833.834.535.135.335.745.1.valid201633.633.833.934.635.435.335.745.4.avg33.533.833.934.635.335.335.745.3.
201736.837.237.538.139.039.139.649.5.
201832.332.732.733.734.634.234.843.9.
201933.934.534.835.636.937.937.647.6.avg34.334.835.035.836.837.137.347.0.table 15: validation and test bleu on wmt’19 ru-en with beam size 50..7263src: zusammen waren wir ein unschlagbares team.
ref: together, we were an unbeatable team.
ncd: together we were an impossible team.
drnmt + ncd: together we were an unbeatable team.
src: keine neuen fl¨uchtlinge, das w¨urde die lage entspannen.
ref: the situation would ease a bit if they did not receive any new refugees.
ncd: no new refugees would ease the situation.
drnmt + ncd: no new refugees, that would ease the situation.
src: je mehr ich es ansehe, desto verwirrender wird es.
ref: the more i look at it, the more mind-boggling it becomes.
ncd: the more i say it, the more confusing it becomes.
drnmt + ncd: the more i look at it, the more confusing it becomes.
src: auf den radarschirmen war die form eines dreamliners zu sein.
ref: the shape of a dreamliner could be seen on radar screens.
ncd: the radar screens were the shape of a dreamliner.
drnmt + ncd: it was the shape of a dreamliner on the radar screens..table 16: examples where drnmt + ncd is rated higher than ncd in human evaluation..src: nein, die ausgehandelten software-updates sind freiwillig.
ref: no, the brokered software updates are voluntary.
ncd: no, the negotiated software extension is voluntary.
drnmt + ncd: no, the software process that is negotiated is voluntary.
src: viele der attraktiveren (hand-desinfektionsmittel) sind diejenigen, die parf¨umiert sind.
ref: a lot of the more attractive (hand sanitizers) are the ones that are scented.
ncd: many of the more attractive (hand disinfectant) tools are those that are coded.
drnmt + ncd: many of the more attractive (hand infections) are those that are coded.
src: herr schmidt, wie kann sich der verbraucher vor vergifteten eiern sch¨utzen?
ref: mr schmidt, how can consumers protect themselves against poisoned eggs?
ncd: mr schmidt, how can consumers protect themselves from poisoned eggs?
drnmt + ncd: mr schmidt, how can the consumer protect itself from poisoned eggs?
src: sie benutzt sogar nur selten einen topf.
ref: she rarely even uses a pot.
ncd: in fact, she rarely uses a pot.
drnmt + ncd: it even rarely uses a pot..table 17: examples where ncd is rated higher than drnmt + ncd in human evaluation..7264