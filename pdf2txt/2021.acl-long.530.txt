evaluation of thematic coherence in microblogs.
iman munire bilal1 bo wang2,4 maria liakata1,3,4rob procter1,4 adam tsakalidis3,41department of computer science, university of warwick2department of psychiatry, university of oxford3school of electronic engineering and computer science, queen mary university of london4the alan turing institute, london, uk{iman.bilal|rob.procter}@warwick.ac.uk{mliakata|bwang|atsakalidis}@turing.ac.uk.
abstract.
collecting together microblogs representingopinions aboutthe same topics within thesame timeframe is useful to a number of dif-ferent tasks and practitioners.
a major ques-tion is how to evaluate the quality of such the-matic clusters.
here we create a corpus of mi-croblog clusters from three different domainsand time windows and deﬁne the task of eval-uating thematic coherence.
we provide an-notation guidelines and human annotations ofthematic coherence by journalist experts.
wesubsequently investigate the efﬁcacy of differ-ent automated evaluation metrics for the task.
we consider a range of metrics including sur-face level metrics, ones for topic model co-herence and text generation metrics (tgms).
while surface level metrics perform well, out-performing topic coherence metrics, they arenot as consistent as tgms.
tgms are more re-liable than all other metrics considered for cap-turing thematic coherence in microblog clus-ters due to being less sensitive to the effect oftime windows..1.introduction.
as social media gains popularity for news track-ing, unfolding stories are accompanied by a vastspectrum of reactions from users of social mediaplatforms.
topic modelling and clustering methodshave emerged as potential solutions to challenges ofﬁltering and making sense of large volumes of mi-croblog posts (rosa et al., 2011; aiello et al., 2013;resnik et al., 2015; surian et al., 2016).
providinga way to access easily a wide range of reactionsaround a topic or event has the potential to helpthose, such as journalists (tolmie et al., 2017),police (procter et al., 2013), health (furini andmenegoni, 2018) and public safety professionals(procter et al., 2020), who increasingly rely on so-cial media to detect and monitor progress of events,public opinion and spread of misinformation..recent work on grouping together views abouttweets expressing opinions about the same enti-ties has obtained clusters of tweets by leveragingtwo topic models in a hierarchical approach (wanget al., 2017b).
the theme of such clusters can eitherbe represented by their top-n highest-probabilitywords or measured by the semantic similarityamong the tweets.
one of the questions regard-ing thematic clusters is how well the posts groupedtogether relate to each other (thematic coherence)and how useful such clusters can be.
for example,the clusters can be used to discover topics that havelow coverage in traditional news media (zhao et al.,2011).
wang et al.
(2017a) employ the centroidsof twitter clusters as the basis for topic speciﬁctemporal summaries..the aim of our work is to identify reliable met-rics for measuring thematic coherence in clustersof microblog posts.
we deﬁne thematic coherencein microblogs as follows: given clusters of poststhat represent a subject or event within a broadtopic, with enough diversity in the posts to show-case different stances and user opinions related tothe subject matter, thematic coherence is the extentto which posts belong together, allowing domainexperts to easily extract and summarise stories un-derpinning the posts..to measure thematic coherence of clusters werequire robust domain-independent evaluation met-rics that correlate highly with human judgementfor coherence.
a similar requirement is posed bythe need to evaluate coherence in topic models.
r¨oder et al.
(2015) provide a framework for an ex-tensive set of coherence measures all restricted toword-level analysis.
bianchi et al.
(2020) show thatadding contextual information to neural topic mod-els improves topic coherence.
however, the mostcommonly used word-level evaluation of topic co-herence still ignores the local context of each word.
ultimately, the metrics need to achieve an opti-.
proceedingsofthe59thannualmeetingoftheassociationforcomputationallinguisticsandthe11thinternationaljointconferenceonnaturallanguageprocessing,pages6800–6814august1–6,2021.©2021associationforcomputationallinguistics6800mal balance between coherence and diversity, suchthat resulting topics describe a logical expositionof views and beliefs with a low level of duplica-tion.
here we evaluate thematic coherence in mi-croblogs on the basis of topic coherence metrics,while also using research in text generation evalua-tion to assess semantic similarity and thematic relat-edness.
we consider a range of state-of-the-art textgeneration metrics (tgms), such as bertscore(zhang et al., 2019), moverscore (zhao et al., 2019)and bleurt (sellam et al., 2020), which we re-purpose for evaluating thematic coherence in mi-croblogs and correlate them with assessments ofcoherence by journalist experts.
the main contri-butions of this paper are:.
• we deﬁne the task of assessing thematic co-herence in microblogs and use it as the basisfor creating microblog clusters (sec.
3)..• we provide guidelines for the annotation ofthematic coherence in microblog clusters andconstruct a dataset of clusters annotated forthematic coherence spanning two different do-mains (political tweets and covid-19 relatedtweets).
the dataset is annotated by journal-ist experts and is available 1 to the researchcommunity (sec.
3.5)..• we compare and contrast state-of-the-arttgms against standard topic coherence eval-uation metrics for thematic coherence evalu-ation and show that the former are more re-liable in distinguishing between thematicallycoherent and incoherent clusters (secs 4, 5)..2 related work.
measures of topic model coherence: the mostcommon approach to evaluating topic model coher-ence is to identify the latent connection betweentopic words representing the topic.
once a functionbetween two words is established, topic coherencecan be deﬁned as the (average) sum of the func-tion values over all word pairs in the set of mostprobable words.
newman et al.
(2010) use point-wise mutual information (pmi) as the function ofchoice, employing co-occurrence statistics derivedfrom external corpora.
mimno et al.
(2011) subse-quently showed that a modiﬁed version of pmi cor-relates better with expert annotators.
alsumait et al.
(2009) identiﬁed junk topics by measuring the dis-tance between topic distribution and corpus-wide.
1https://doi.org/10.6084/m9.figshare..14703471.distribution of words.
fang et al.
(2016a) modeltopic coherence by setting the distance betweentwo topic words to be the cosine similarity of theirrespective embedded vectors.
due to its general-isability potential we follow this latter approachto topic coherence to measure thematic coherencein tweet clusters.
we consider glove (penningtonet al., 2014) and bertweet (nguyen et al., 2020)embeddings, derived from language models pre-trained on large external twitter corpora.
to im-prove performance and reduce sensitivity to noise,we followed the work of lau and baldwin (2016),who consider the mean topic coherence over sev-eral topic cardinalities |w | ∈ {5, 10, 15, 20}..another approach to topic coherence involvesdetecting intruder words given a set of topic words,an intruder and a document.
if the intruder is identi-ﬁed correctly then the topic is considered coherent.
researchers have explored varying the number of‘intruders’ (morstatter and liu, 2018) and automat-ing the task of intruder detection (lau et al., 2014).
there is also work on topic diversity (nan et al.,2019).
however, there is a tradeoff between di-versity and coherence (wu et al., 2020), meaninghigh diversity for topic modelling is likely to be inconﬂict with thematic coherence, the main focus ofthe paper.
moreover, we are ensuring semantic di-versity of microblog clusters through our samplingstrategy (see sec.
3.4)..text generation metrics: tgms have been ofgreat use in applications such as machine trans-lation (zhao et al., 2019; zhang et al., 2019;guo and hu, 2019; sellam et al., 2020), text sum-marisation (zhao et al., 2019) and image caption-ing (vedantam et al., 2015; zhang et al., 2019; zhaoet al., 2019), where a machine generated responseis evaluated against ground truth data constructedby human experts.
recent advances in contextuallanguage modeling outperform traditionally usedbleu (papineni et al., 2002) and rouge (lin,2004) scores, which rely on surface-level n-gramoverlap between the candidate and the reference..in our work, we hypothesise that metrics basedon contextual embeddings can be used as a proxyfor microblog cluster thematic coherence.
speciﬁ-cally, we consider the following tgms:(a) bertscore is an automatic evaluation metricbased on bert embeddings (zhang et al., 2019).
the metric is tested for robustness on adversarialparaphrase classiﬁcation.
however, it is based ona greedy approach, where every reference token is.
6801linked to the most similar candidate token, leadingto a time-performance trade-off.
the harmonicmean fbert is chosen for our task due to its mostconsistent performance (zhang et al., 2019).
(b) moverscore (zhao et al., 2019) expands fromthe bertscore and generalises word mover dis-tance (kusner et al., 2015) by allowing soft (many-to-one) alignments.
the task of measuring seman-tic similarity is tackled as an optimisation problemwith the constraints given by n-gram weights com-puted in the corpus.
in this paper, we adopt thismetric for unigrams and bigrams as the preferredembedding granularity.
(c) bleurt (sellam et al., 2020) is a state-of-the-art evaluation metric also stemming from thesuccess of bert embeddings, carefully curatedto compensate for problematic training data.
itsauthors devised a novel pre-training scheme lever-aging vast amounts of synthetic data generatedthrough bert mask-ﬁlling, back-translation andword dropping.
this allows bleurt to performrobustly in cases of scarce and imbalanced data..3 methodology.
notation we use c = {c1, ..., cn} to denote aset of clusters ci.
each cluster ci is represented bythe pair ci = (ti, wi), where ti and wi representthe set of tweets and top-20 topic words of thedominant latent topic in ci, respectively.
the task of identifying thematic coherence in mi-croblog clusters is formalised as follows: givena set of clusters c, we seek to identify a metricfunction f : c → r s.t.
high values of f (ci)correlate with human judgements for thematic co-herence.
here we present (a) the creation of acorpus of topic clusters of tweets c and (b) theannotation process for thematic coherence.
(a) in-volves a clustering (sec.
3.2), a ﬁltering (sec.
3.3)and a sampling step (sec.
3.4); (b) is describedin (sec.
3.5).
experiments to identify a suitablefunction f are in sec.
4..3.1 data sources.
we used three datasets pertaining to distinct do-mains and collected over different time periods asthe source of our tweet clusters..the covid-19 dataset (chen et al., 2020) wascollected by tracking covid-19 related keywords(e.g., coronavirus, pandemic, stayathome) and ac-counts (e.g., @cdcemergency, @hhsgov, @drt-edros) through the twitter api from january to.
may 2020. this dataset covers speciﬁc recentevents that have generated signiﬁcant interest andits entries reﬂect on-going issues and strong publicsentiment regarding the current pandemic..the election dataset was collected via the twit-ter firehose and originally consisted of all geo-located uk tweets posted between may 2014 andmay 20162.it was then ﬁltered using a list of438 election-related keywords relevant to 9 popu-lar election issues3 and a list of 71 political partyaliases curated by a team of journalists (wang et al.,2017c)..the pheme dataset (zubiaga et al., 2016) of ru-mours and non-rumours contains tweet conversa-tion threads consisting of a source tweet and asso-ciated replies, covering breaking news pertainingto 9 events (i.e., charlie hebdo shooting, german-wings airplane crash, ferguson unrest, etc.)..
these datasets were selected because they covera wide range of topics garnering diverse sentimentsand opinions in the twitter sphere, capturing news-worthy stories and emerging phenomena of interestto journalists and social scientists.
of particularinterest was the availability of stories, comprisinggroups of tweets, in the pheme dataset, which iswhy we consider pheme tweet clusters separately..3.2 tweet cluster generation.
the task of thematic coherence evaluation intro-duced in this paper is related to topic modellingevaluation, where it is common practice ( mimnoet al.
(2011), newman et al.
(2010)) to gauge thecoherence level of automatically created groupsof topical words.
in a similar vein, we evaluatethematic coherence in tweet clusters obtained auto-matically for the election and covid-19 datasets.
the clusters were created in the following way:tweets mentioning the same keyword posted withinthe same time window (3 hours for election, 1 hourfor covid-19) were clustered according to the two-stage clustering approach by wang et al.
(2017b),where two topic models (yin and wang, 2014;nguyen et al., 2015) with a tweet pooling step areused.
we chose this as it has shown competitiveperformance over several tweet clustering tasks,without requiring a pre-deﬁned number of clusters..2unlike the twitter api, the ﬁrehose provides 100% ofthe tweets that match user deﬁned criteria, which in our caseis a set of geo-location and time zone twitter powertrackoperators..3eu and immigration, economy, nhs, education, crime,housing, defense, public spending, environment and energy..6802the pheme dataset is structured into conver-sation threads, where each source tweet is assigneda story label.
we assume that each story and thecorresponding source tweets form a coherent the-matic cluster since they have been manually an-notated by journalists.
thus the pheme storiescan be used as a gold standard for thematicallycoherent clusters.
we also created artiﬁcial themat-ically incoherent clusters from pheme.
for thispurpose we mixed several stories in different pro-portions.
we designed artiﬁcial clusters to coverall types of thematic incoherence, namely: ran-dom, intruded, chained (see sec.
3.5 for deﬁni-tions).
for intruded, we diluted stories by elimi-nating a small proportion of their original tweetsand introducing a minority of foreign content fromother events.
for chained, we randomly chose thenumber of subjects (varying from 2 to 5) to fea-ture in a cluster, chose the number of tweets persubject and then constructed the ‘chain of subjects’by sampling tweets from a set of randomly chosenstories.
finally, random clusters were generatedby sampling tweets from all stories, ensuring nosingle story represented more than 20% of a clus-ter.
these artiﬁcial clusters from pheme serve asground-truth data for thematic incoherence..3.3 cluster filtering.
for automatically collected clusters (covid-19and election) we followed a series of ﬁltering steps:duplicate tweets, non-english4 tweets and ads wereremoved and only clusters containing 20-50 tweetswere kept.
as we sought to mine stories and associ-ated user stances, opinionated clusters were priori-tised.
the sentiment analysis tool vader (gilbertand hutto, 2014) was leveraged to gauge subjec-tivity in each cluster: a cluster is considered to beopinionated if the majority of its tweets expressstrong sentiment polarity.5 vader was chosen forits reliability on social media text and for its ca-pacity to assign granulated sentiment valences; thisallowed us to readily label millions of tweets andimpose our own restrictions to classify neutral/non-neutral instances by varying the thresholds for thevader compound score..4https://pypi.org/project/langdetect/5the absolute value of vader compound score is re-quired to be > 0.5, a much stricter condition than that usedoriginally (gilbert and hutto, 2014)..3.4 cluster sampling.
work on assessing topic coherence operates on ei-ther the entire dataset (fang et al., 2016b) or arandom sample of it (newman et al., 2010; mimnoet al., 2011).
fully annotating our entire datasetof thematic clusters would be too time-consuming,as the labelling of each data point involves readingdozens of posts rather than a small set of topicalwords.
on the other hand, purely random samplingfrom the dataset cannot guarantee cluster diversityin terms of different levels of coherence.
thus,we opt for a more complex sampling strategy in-spired by stratiﬁed sampling (singh and mangat,2013), allowing more control over how the data ispartitioned in terms of keywords and scores.
after ﬁltering election and covid-19 contained46,715 and 5,310 clusters, respectively.
we choseto sample 100 clusters from each dataset s.t.
they:.
• derive from a semantically diverse set of key-.
words (required for elections only);.
• represent varying levels of coherence (both);• represent a range of time periods (both)..we randomly subsampled 10 clusters from eachkeyword with more than 100 clusters and keep allclusters with under-represented keywords (associ-ated with fewer than 100 clusters).
this resulted in2k semantically diverse clusters for elections..tgm scores were leveraged to allow the selec-tion of clusters with diverse levels of thematic co-herence in the pre-annotation dataset.
potentialscore ranges for each coherence type were mod-elled on the pheme dataset (see sec.
3.2, 3.5),which is used as a gold standard for cluster coher-ence/incoherence.
for each metric m and eachcoherence type ct , we deﬁned the associated in-terval to be:.
i(m)ct = [µ − 2σ, µ + 2σ],.
where µ, σ are the mean and standard deviationfor the set of metric scores m characterising clus-ters of coherence type ct .
we thus accountfor 95% of the data6.
we did not consider met-rics m for which the overlap between i(m)good,7 and i(m)random was signiﬁ-i(m)intruded-chainedcant as this implied the metric was unreliable..6both the shapiro-wilk and anderson-darling statistical.
tests had showed the pheme data is normally distributed..7intruded and chained clusters mostly deﬁne the inter-mediate level of coherence, so their score ranges are similar,hence the two groups are uniﬁed..6803as we did not wish to introduce metric bias whensampling the ﬁnal dataset, we subsampled clus-ters across the intersection of all suitable metricsfor each coherence type ct .
in essence, our ﬁ-nal clusters were sampled from each of the setscct = {ci| m(ci) ∈ i(m)ct ∀ metric m}.
for each of covid-19 and elections we sampled50 clusters ∈ cgood, 25 clusters ∈ cintruded-chainedand 25 clusters ∈ crandom..3.5 coherence annotation process.
coherence annotation was carried out in four stagesby three annotators.
we chose experienced jour-nalists as they are trained to quickly and reliablyidentify salient content.
an initial pilot study in-cluding the journalists and the research team wasconducted; this involved two rounds of annotationand subsequent discussion to align the team’s un-derstanding of the guidelines (for the guidelinessee appendix b)..the ﬁrst stage tackled tweet-level annotationwithin clusters and drew inspiration from the clas-sic task of word intrusion (chang et al., 2009):annotators were asked to group together tweets dis-cussing a common subject; tweets considered tobe ‘intruders’ were assigned to groups of their own.
several such groups can be identiﬁed in a clusterdepending on the level of coherence.
this groupingserved as a building block for subsequent stages.
this sub-clustering step offers a good trade-off be-tween high annotation costs and manual evaluationsince manually creating clusters from thousands oftweets is impractical.
we note that agreement be-tween journalists is not evaluated at this ﬁrst stageas obtaining exact sub-clusters is not our objective.
however, vast differences in sub-clustering are cap-tured in the next stages in quality judgment andissue identiﬁcation (see below).the second stageconcerned cluster quality assessment, which is ourprimary task.
similar to newman et al.
(2010)for topic words, annotators evaluated tweet clustercoherence on a 3-point scale (good, intermedi-ate, bad).
good coherence is assigned to a clusterwhere the majority of tweets belong to the sametheme (sub-cluster), while clusters containing manyunrelated themes (sub-clusters) are assigned badcoherence..the third stage pertains to issue identiﬁcationof low coherence, similar to mimno et al.
(2011).
when either intermediate or bad are chosen instage 2 annotators can select from a list of issues.
to justify their choice:.
• chained: several themes are identiﬁed in thecluster (with some additional potential ran-dom tweets), without clear connection be-tween any two themes..• intruded: only one common theme can beidentiﬁed among some tweets in the clusterand the rest have no clear connection to thetheme or to each other..• random: no themes can be identiﬁed andthere is no clear connection among tweets inthe cluster..inter-annotator agreement (iaa) was computedseparately for the second and third stages as theyserve a different purpose.
for the second stage(cluster quality), we obtain average spearman cor-relation rs = 0.73 which is comparable to previouscoherence evaluation scores in topic modelling lit-erature ((newman et al., 2010) with rs = 0.73 / 0.78and (aletras and stevenson, 2013) with rs = 0.70 /0.64 / 0.54) and average cohen’s kappa κ = 0.48(moderate iaa).
for the third stage (issue identiﬁ-cation), we compute average κ = 0.36 (fair iaa).
analysis of pairwise disagreement in stage 2shows only 2% is due to division in opinionover good-bad clusters.
good-intermediate andintermediate-bad cases account for 37% and 61%of disagreements respectively.
this is encourag-ing as annotators almost never have polarisingviews on cluster quality and primarily agree onthe coherence of a good cluster, the main goalof this task.
for issue identiﬁcation the majorityof disagreements (%49) consists in distinguishingintermediate-chained cases.
this can be explainedby the expected differences in identifying subclus-ters in the ﬁrst stage.
for the adjudication process,we found that a majority always exists and thus theﬁnal score was assigned to be the majority label(2/3 annotators).
table 1 presents a summary ofthe corpus size, coherence quality and issues iden-tiﬁed for covid-19 and election (see appendixc for a discussion)..4 experiments.
our premise is that a pair of sentences scoring highin terms of tgms means that the sentences are se-mantically similar.
when this happens across manysentences in a cluster then this denotes good clustercoherence.
following douven and meijs (2007),we consider three approaches to implementing andadapting tgms to the task of measuring thematic.
6804general.
cluster quality.
datasetcovid-19election.
clusters tweets tokens good1825.
2,9552,650.
100k52k.
100100.intermediate bad5125.
3150.cluster issueintruded chained random2514.
3228.
2533.table 1: statistics of the annotated clusters where the ﬁnal label is assigned to be the majority label..coherence.
the differences between these methodsconsist of: (a) the choice of the set of tweet pairss ⊂ t × t on which we apply the metrics and(b) the score aggregating function f (c) assigningcoherence scores to clusters.
the tgms employedin our study are bertscore (zhang et al., 2019),moverscore (zhao et al., 2019) for both unigramsand bigrams and bleurt (sellam et al., 2020).
we also employed a surface level metric based oncosine similarity distances between tf-idf repre-sentations8 of tweets to judge the inﬂuence of wordco-occurrences in coherence analysis.
each ap-proach has its own advantages and disadvantages,which are outlined below..4.1 exhaustive approach.
in this case s = t ×t , i.e., all possible tweet pairswithin the cluster are considered.
the cluster is as-signed the mean sum over all scores.
this approachis not biased towards any tweet pairs, so is able topenalise any tweet that is off-topic.
however, it iscomputationally expensive as it requires o(|t |2)operations.
formally, given a tgm m, we deﬁnethis approach as:(cid:1) ·.
m(tweeti, tweetj)..f (c) =.
(cid:88).
1(cid:0)|t |2.tweeti,tweetj ∈ti<j.
4.2 representative tweet approach.
we assume there exists a representative tweet ableto summarise the content in the cluster, denotedas the representative tweet (i.e.
tweetrep).
this isformally deﬁned as:.
tweetrep(c) = arg min.
dkl(θ, tweeti),.
tweeti∈c.
where we compute the kullback–leibler diver-gence (dkl) between the word distributions ofthe topic θ representing the cluster c and eachtweet in c (wan and wang, 2016); we describe thecomputation of dkl in appendix a. we also con-sidered other text summarisation methods (basaveet al., 2014; wan and wang, 2016) such as mead(radev et al., 2000) and lexrank (erkan and radev,.
8tweets are embedded into a vector space of tf-idf rep-.
resentations within their corresponding cluster..2004) to extract the best representative tweet, butour initial empirical study indicated dkl consis-tently ﬁnds the most appropriate representativetweet.
in this case cluster coherence is deﬁnedas below and has linear time complexity o(|t |):.
f (c) =.
m(tweeti, tweetrep)..1|t |.
(cid:88).
tweeti∈t.
as s = {(tweet, tweetrep)| tweet ∈ t }t × t , thecoherence of a cluster is heavily inﬂuenced by thecorrect identiﬁcation of the representative tweet..4.3 graph approach.
similar to the work of erkan and radev (2004),each cluster of tweets c can be viewed as acomplete weighted graph with nodes representedby the tweets in the cluster and each edge be-tween tweeti, tweetj assigned as weight: wi,j =m(tweeti, tweetj)−1.
in the process of construct-ing a complete graph, all possible pairs of tweetswithin the cluster are considered.
hence s = t ×twith time complexity of o(|t |2) as in section 4.1.in this case, the coherence of the cluster is com-puted as the average closeness centrality of theassociated cluster graph.
this is a measure derivedfrom graph theory, indicating how ‘close’ a node ison average to all other nodes; as this deﬁnition intu-itively corresponds to coherence within graphs, weincluded it in our study.
the closeness centralityfor the node representing tweeti is given by:.
cc(tweeti) =.
|t | − 1tweetj ∈t d(tweetj, tweeti).
,.
(cid:80).
where d(tweetj, tweeti) is the shortest distance be-tween nodes tweeti and tweetj computed via dijk-stra’s algorithm.
note that as dijkstra’s algorithmonly allows for non-negative graph weights andbleurt’s values are mostly negative, we did notinclude this tgm in the graph approach implemen-tation.
here cluster coherence is deﬁned as theaverage over all closeness centrality scores of thenodes in the graph:.
f (c) =.
cc(tweeti)..1|t |.
(cid:88).
tweet∈t.
6805exhaustive tf-idf.
graph tf-idf.
exhaustive bleurt.
electionrs / ρ / τ0.62 / 0.62 / 0.49.covid-19rs / ρ / τ0.68 / 0.72 / 0.53.phemers / ρ / τ0.81 / 0.73 / 0.67.
0.62 / 0.63 / 0.48.
0.66 / 0.72 / 0.52.
0.74 / 0.71 / 0.60.
0.49 / 0.48 / 0.37.
0.66 / 0.65 / 0.52.
0.84 / 0.86 / 0.69.exhaustive bertscore.
0.58 / 0.57 / 0.44.
0.62 / 0.64 / 0.49.
0.83 / 0.80 / 0.68.topic coherence glove.
-0.25 / -0.27 / -0.19.
0.04 / 0.02 / 0.03.avg topic coherence glove.
-0.22 / -0.23 / -0.17.
-0.03 / -0.03 / -0.02.topic coherence bertweet.
-0.23 / -0.22 / -0.18.
0.10 / 0.11 / 0.08.avg topic coherence bertweet.
-0.17 / -0.16 / -0.14.
0.04 / 0.04 / 0.03.n/a.
n/a.
n/a.
n/a.
table 2: agreement with annotator ratings across the election, covid-19 and pheme datasets.
the metrics arespearman’s rank correlation coefﬁcient (rs), pearson correlation coefﬁcient (ρ) and kendall tau (τ )..5 results.
5.1 quantitative analysis.
table 2 presents the four best and four worst per-forming metrics (for the full list of metric results re-fer to appendix a).
moverscore variants are not in-cluded in the results discussion as they only achieveaverage performance..election and covid-19 exhaustive tf-idf andgraph tf-idf consistently outperformed tgms,implying that clusters with a large overlap of wordsare likely to have received higher coherence scores.
while tf-idf metrics favour surface level co-occurrence and disregard deeper semantic connec-tions, we conclude that, by design all posts in thethematic clusters (posted within a 1h or 3 h win-dow) are likely to use similar vocabulary.
neverthe-less, tgms correlate well with human judgement,implying that semantic similarity is a good indica-tor for thematic coherence: exhaustive bertscoreperforms the best of all tgms in election whileexhaustive bleurt is the strongest competitor totf-idf based metrics for covid-19..on the low end of the performance scale, wehave found topic coherence to be overwhelminglyworse compared to all the tgms employed in ourstudy.
bertweet improves over glove embeddingsbut only slightly as when applied at the word level(for topic coherence) it is not able to beneﬁt fromthe context of individual words.
we followed lauand baldwin (2016), and computed average topiccoherence across the top 5, 10, 15, 20 topical wordsin order to obtain a more robust performance (seeavg topic coherence glove, avg topic coherencebertweet).
results indicate that this smoothingtechnique correlates better with human judgementfor election, but lowers performance further in.
covid-19 clusters..in terms of the three approaches, we have foundthat the exhaustive and graph approaches performsimilarly to each other and both outperform therepresentative tweet approach.
sacriﬁcing time astrade off to quality, the results indicate that metricsconsidering all possible pairs of tweets account forhigher correlation with annotator rankings..pheme the best performance on this datasetis seen with tgm bleurt, followed closely bybertscore.
while tf-idf based metrics are stillin the top four, surface level evaluation proves tobe less reliable: pheme stories are no longer con-strained by strict time windows9, which allows thetweets within each story to be more lexically di-verse, while still maintaining coherence.
in suchinstances, strategies depending exclusively on wordfrequencies perform inconsistently, which is whymetrics employing semantic features (bleurt,bertscore) outperform tf-idf ones.
note thatpheme data lack the topic coherence evaluation,as these clusters were not generated through topicmodelling (see subsection 3.2)..5.2 qualitative analysis.
we analysed several thematic clusters to get a bet-ter insight into the results.
tables 3 and 4 showrepresentative fragments from 2 clusters labelledas ‘good’ in the covid-19 dataset.
the ﬁrst clus-ter contains posts discussing the false rumour thatbleach is an effective cure to covid-19, with themajority of users expressing skepticism.
as mosttweets in this cluster directly quote the rumour andthus share a signiﬁcant overlap of words, not sur-prisingly, tf-idf based scores are high exhaustive.
9stories were generated across several days, rather thenhours, by tracking on-going breaking news events on twitter..6806cluster annotation: good.
common keyword: ‘coronavirus’.
trump-loving conspiracy nuts tout drinking bleach as a ‘miracle’ cure for coronavirus - ”they may have found a cure fortrump lovers and maga but not anything else” #magaidiots #testondonjr #onevoicepro-trump conspiracy theorists tout drinking bleach as a ’miracle’ cure for coronavirus.
trump-loving conspiracy nuts tout drinking bleach as a ‘miracle’ cure for coronavirus – drink up, magats!
isn’t this just a problem solving itself?
#darwinism trump-loving conspiracy nuts tout drinking bleach as a ‘miracle’ curefor coronavirus.
trump-loving conspiracy nuts tout drinking bleach as a ‘miracle’ cure for coronavirus... is a quart each sufﬁcient?
willgo multiple gallons-gratis..table 3: cluster fragment from covid-19 dataset, exhaustive tf-idf = 0.109 and exhaustive bleurt = -0.808..cluster annotation: good.
common keyword: ‘pandemic’.
@cnn @realdonaldtrump administration recently requested $2.5 billion in emergency funds to prepare the u.s. for a possiblewidespread outbreak of coronavirus.
money isnt necessary if the trump past 2 years didnt denudate government units that weredesigned to protect against pandemic.
@realdonaldtrump @vp @secazar @cdcgov @cdcdirector i bet that was the case of 3 people who had gone no where.
you cutting cdc, scientists & taking money that was set aside for pandemic viruses that obama set aside has not helped.
youput pence in charge who did nothing for in aids epidemic because he said he was a christian.
trump ﬁred almost all the pandemic preparedness team that @barackobama put in place and his budget promised cutting $ 1.3billion from @cdc.
with ’leadership’ like that, what could anyone expect except dire preparation in america?
# maga2020morons: be careful at his rallies.
@user democrats do not want mils of americans to die from coronavirus.
they aren’t the ones who ﬁred the wholepandemic team obama put in place.
it was trump.
he left us unprepared.
all he’s interested in is the stock market, wealthydonors & getting re-elected..@user , obama set up a pandemic reaction force, placed higher budgets for the cdc and health and human services.
trumpon the other hand, have signiﬁcantly cut the budgets to hhs and the cdc.
they disbanded the white house pandemic efforts.
with a politician, not a scientist.
table 4: cluster fragment from covid-19 dataset, exhaustive tf-idf = 0.040 and exhaustive bleurt = -0.811..tf-idf = 0.109. in the second cluster, however,users challenge the choices of the american presi-dent regarding the government’s pandemic reaction:though the general feeling is unanimous in all postsof the second cluster, these tweets employ a morevaried vocabulary.
consequently, surface level met-rics fail to detect the semantic similarity exhaustivetf-idf = 0.040. when co-occurrence statistics areunreliable, tgms are more successful for detect-ing the ‘common story’ diversely expressed in thetweets: in fact, exhaustive bleurt assigns simi-lar scores to both clusters (-0.808 for cluster 1 and-0.811 for cluster 2) in spite of the vast differencein their content intersection, which shows a morerobust evaluation capability..we analyse the correlation between topic coher-ence and annotator judgement in tables 5 and 6.both are illustrative fragments of clusters extractedfrom the election dataset.
though all tweets intable 5 share the keyword ‘oil’, they form a badrandom cluster type, equivalent to the lowest levelof coherence.
on the other hand, table 6 clearlypresents a good cluster regarding an immigrationtragedy at sea.
although this example pair containsclusters on opposite sides of the coherence spec-.
trum, topic coherence metrics fail to distinguish theclear difference in quality between the two.
more-over, table 6 receives lower scores (tc glove =0.307) than its incoherent counterpart (tc glove= 0.330) for glove topic coherence.
however,tgm metric bertscore and surface-level met-ric tf-idf correctly evaluate the two clusters bypenalising incoherence (exhaustive bertscore =0.814 and exhaustive tf-idf = 0.024) and award-ing good clusters (exhaustive bertscore = 0.854and exhaustive tf-idf = 0.100)..6 conclusions and future work.
we have deﬁned the task of creating topic-sensitiveclusters of microblogs and evaluating their thematiccoherence.
to this effect we have investigated theefﬁcacy of different metrics both from the topicmodelling literature and text generation metricstgms.
we have found that tgms correlate muchbetter with human judgement of thematic coher-ence compared to metrics employed in topic modelevaluation.
tgms maintain a robust performanceacross different time windows and are generalis-able across several datasets.
in future work we planto use tgms in this way to identify thematically.
6807cluster annotation: bad random.
common keyword: ‘oil’.
m’gonna have a nap, i feel like i’ve drank a gallon of like grease or oil or whatever bc i had ﬁsh&chips like 20 minutes ago.
check out our beautiful, nostalgic oil canvasses.
these stunning images will take you back to a time when life....five years later, bottlenose dolphins are still suffering from bp oil disaster in the gulf.
take action!.
once the gas and oil run out countries like suadia arabia and russia won’t be able to get away with half the sh*t they can now.
ohhh this tea tree oil is burning my face off.
table 5: cluster fragment from election dataset, tc glove = 0.330, exhaustive bertscore = 0.814 and exhaus-tive tf-idf = 0.024..cluster annotation: good.
common keyword: ‘migrants’.
up to 300 migrants missing in mediterranean sea are feared dead #migrants..news: more than 300 migrants feared drowned after their overcrowded dinghies sank in the mediterranean.
imagine if a ferry sunk with 100s dead - holiday makers, kids etc.
top story everywhere.
300 migrants die at sea and it doesn’tlead..@bbc5live hi fivelive: you just reported 300 migrants feared dead.
i wondered if you could conﬁrm if the migrants werealso people?
cheers..if the dinghies were painted pink would there be as much uproar about migrants drowning as the colour of a f**king bus?.
table 6: cluster fragment from election dataset, tc glove = 0.307, exhaustive bertscore = 0.854 and exhaus-tive tf-idf = 0.100..coherent clusters on a large scale, to be used indownstream tasks such as multi-document opinionsummarisation..references.
acknowledgements.
this work was supported by a ukri/epsrc tur-ing ai fellowship to maria liakata (grant no.
ep/v030302/1) and the alan turing institute(grant no.
ep/n510129/1).
we would like to thankour 3 annotators for their invaluable expertise inconstructing the datasets.
we also thank the re-viewers for their insightful feedback.
finally, wewould like to thank yanchi zhang for his help in theredundancy correction step of the pre-processing..ethics.
ethics approval to collect and to publish extractsfrom social media datasets was sought and receivedfrom warwick university humanities & social sci-ences research ethics committee.
during the an-notation process, tweet handles, with the except ofpublic ﬁgures, organisations and institutions, wereanonymised to preserve author privacy rights.
inthe same manner, when the datasets will be releasedto the research community, only tweets ids willbe made available along with associated clustermembership and labels..compensation rates were agreed with the anno-tators before the annotation process was launched.
remuneration was fairly paid on an hourly rate atthe end of task..luca maria aiello, georgios petkos, carlos martin,david corney, symeon papadopoulos, ryan skraba,ayse g¨oker, ioannis kompatsiaris, and alejandrojaimes.
2013.sensing trending topics in twit-ter.
ieee transactions on multimedia, 15(6):1268–1282..nikolaos aletras and mark stevenson.
2013. evalu-ating topic coherence using distributional semantics.
in iwcs, pages 13–22..loulwah alsumait, daniel barbar´a, james gentle, andcarlotta domeniconi.
2009. topic signiﬁcance rank-in joint europeaning of lda generative models.
conference on machine learning and knowledgediscovery in databases, pages 67–82.
springer..amparo elizabeth cano basave, yulan he, andruifeng xu.
2014. automatic labelling of topicmodels learned from twitter by summarisation.
inproceedings of the 52nd annual meeting of the as-sociation for computational linguistics (volume 2:short papers), pages 618–624..federico bianchi, silvia terragni, and dirk hovy.
2020.topic: contextual-ized document embeddings improve topic coher-ence.
arxiv preprint arxiv:2004.03974..pre-training is a hot.
jonathan chang, sean gerrish, chong wang, jordan lboyd-graber, and david m blei.
2009. readingtea leaves: how humans interpret topic models.
inadvances in neural information processing systems,pages 288–296..emily chen, kristina lerman, and emilio ferrara.
2020. tracking social media discourse about the.
6808covid-19 pandemic: development of a public coron-avirus twitter data set.
jmir public health surveill,6(2):e19273..igor douven and wouter meijs.
2007. measuring co-.
herence.
synthese, 156(3):405–425..g¨unes erkan and dragomir r. radev.
2004. lexrank:graph-based lexical centrality as salience in textsummarization.
j. artif.
int.
res., 22(1):457–479..anjie fang, craig macdonald, iadh ounis, and philiphabel.
2016a.
using word embedding to evaluatein pro-the coherence of topics from twitter data.
ceedings of the 39th international acm sigir con-ference on research and development in informa-tion retrieval, pages 1057–1060..chin-yew lin.
2004. rouge: a package for auto-matic evaluation of summaries.
in text summariza-tion branches out, pages 74–81, barcelona, spain.
association for computational linguistics..david mimno, hanna wallach, edmund talley,miriam leenders, and andrew mccallum.
2011.optimizing semantic coherence in topic models.
inproceedings of the 2011 conference on empiricalmethods in natural language processing, pages262–272, edinburgh, scotland, uk.
association forcomputational linguistics..fred morstatter and huan liu.
2018. in search of co-herence and consensus: measuring the interpretabil-ity of statistical topics.
journal of machine learn-ing research, 18(169):1–32..anjie fang, craig macdonald, iadh ounis, and philiphabel.
2016b.
using word embedding to evaluatein pro-the coherence of topics from twitter data.
ceedings of the 39th international acm sigir con-ference on research and development in informa-tion retrieval, pages 1057–1060..feng nan, ran ding, ramesh nallapati, and bing xi-ang.
2019. topic modeling with wasserstein autoen-in proceedings of the 57th annual meet-coders.
ing of the association for computational linguis-tics, pages 6345–6381, florence, italy.
associationfor computational linguistics..marco furini and gabriele menegoni.
2018. publichealth and social media: language analysis of vac-cine conversations.
in 2018 international workshopon social sensing (socialsens), pages 50–55.
ieee..che gilbert and erric hutto.
2014. vader: a par-simonious rule-based model for sentiment analysisin eighth international con-of social media text.
ference on weblogs and social media (icwsm-14).
available at (20/04/16) http://comp.
social.
gatech.
edu/papers/icwsm14.
vader.
hutto.
pdf, volume 81,page 82..yinuo guo and junfeng hu.
2019. meteor++ 2.0:adopt syntactic level paraphrase knowledge into ma-chine translation evaluation.
in proceedings of thefourth conference on machine translation (volume2: shared task papers, day 1), pages 501–506, flo-rence, italy.
association for computational linguis-tics..matt j. kusner, yu sun, nicholas i. kolkin, and kil-ian q. weinberger.
2015. from word embeddings todocument distances.
in proceedings of the 32nd in-ternational conference on international conferenceon machine learning - volume 37, icml’15, page957–966.
jmlr.org..jey han lau and timothy baldwin.
2016. the sensitiv-ity of topic coherence evaluation to topic cardinality.
in proceedings of the 2016 conference of the northamerican chapter of the association for computa-tional linguistics: human language technologies,pages 483–487, san diego, california.
associationfor computational linguistics..david newman, jey han lau, karl grieser, and tim-othy baldwin.
2010. automatic evaluation of topiccoherence.
in human language technologies: the2010 annual conference of the north americanchapter of the association for computational lin-guistics, hlt ’10, page 100–108, usa.
associationfor computational linguistics..dat quoc nguyen, richard billingsley, lan du, andmark johnson.
2015. improving topic models withlatent feature word representations.
transactionsof the association for computational linguistics,3:299–313..dat quoc nguyen, thanh vu, and a. nguyen.
2020.bertweet: a pre-trained language model for englishtweets.
arxiv, abs/2005.10200..kishore papineni, salim roukos, todd ward, and wei-jing zhu.
2002. bleu: a method for automatic eval-in proceedings ofuation of machine translation.
the 40th annual meeting of the association for com-putational linguistics, pages 311–318, philadelphia,pennsylvania, usa.
association for computationallinguistics..jeffrey pennington, richard socher, and christopher d.manning.
2014. glove: global vectors for word rep-resentation.
in empirical methods in natural lan-guage processing (emnlp), pages 1532–1543..rob procter, shuaib choudhry, jim smith, martinebarons, and adam edwards.
2020. roadmappinguses of advanced analytics in the uk food and drinksector..jey han lau, david newman, and timothy baldwin.
2014. machine reading tea leaves: automaticallyevaluating topic coherence and topic model quality.
in eacl..rob procter, jeremy crump, susanne karstedt, alexvoss, and marta cantijoch.
2013. reading the riots:what were the police doing on twitter?
policing andsociety, 23(4):413–436..6809dragomir r. radev, hongyan jing, and malgorzatabudzikowska.
2000. centroid-based summarizationof multiple documents: sentence extraction, utility-based evaluation, and user studies.
in naacl-anlp2000 workshop: automatic summarization..philip resnik, william armstrong, leonardo claudino,thang nguyen, viet-an nguyen, and jordan boyd-graber.
2015. beyond lda: exploring supervisedtopic modeling for depression-related language inin proceedings of the 2nd workshop ontwitter.
computational linguistics and clinical psychology:from linguistic signal to clinical reality, pages 99–107..michael r¨oder, andreas both, and alexander hinneb-urg.
2015. exploring the space of topic coherencemeasures.
in proceedings of the eighth acm inter-national conference on web search and data mining,pages 399–408..kevin dela rosa, rushin shah, bo lin, anatole ger-shman, and robert frederking.
2011. topical clus-tering of tweets.
proceedings of the acm sigir:swsm, 63..thibault sellam, dipanjan das, and ankur parikh.
2020. bleurt: learning robust metrics for textgeneration.
in proceedings of the 58th annual meet-ing of the association for computational linguistics,pages 7881–7892, online.
association for computa-tional linguistics..ravindra singh and naurang singh mangat.
2013. ele-ments of survey sampling, volume 15. springer sci-ence & business media..didi surian, dat quoc nguyen, georgina kennedy,mark johnson, enrico coiera, and adam g dunn.
2016.characterizing twitter discussions abouthpv vaccines using topic modeling and communityjournal of medical internet research,detection.
18(8):e232..peter tolmie, rob procter, david william randall,mark rounceﬁeld, christian burger, geraldinewong sak hoi, arkaitz zubiaga, and maria liakata.
2017. supporting the use of user generated contentin journalistic practice.
in proceedings of the 2017chi conference on human factors in computingsystems, pages 3632–3644.
acm..r. vedantam, c. l. zitnick, and d. parikh.
2015. cider:consensus-based image description evaluation.
in2015 ieee conference on computer vision and pat-tern recognition (cvpr), pages 4566–4575..xiaojun wan and tianming wang.
2016. automaticlabeling of topic models using text summaries.
inproceedings of the 54th annual meeting of the as-sociation for computational linguistics (volume 1:long papers), pages 2297–2305..bo wang, maria liakata, adam tsakalidis, spirosgeorgakopoulos kolaitis, symeon papadopoulos,.
lazaros apostolidis, arkaitz zubiaga, rob proc-ter, and yiannis kompatsiaris.
2017a.
totemss:topic-based, temporal sentiment summarisation fortwitter.
in proceedings of the ijcnlp 2017, systemdemonstrations, pages 21–24, tapei, taiwan.
asso-ciation for computational linguistics..bo wang, maria liakata, arkaitz zubiaga, and robprocter.
2017b.
a hierarchical topic modelling ap-proach for tweet clustering.
in international confer-ence on social informatics, pages 378–390.
springerinternational publishing..bo wang, maria liakata, arkaitz zubiaga, and robprocter.
2017c.
tdparse: multi-target-speciﬁc sen-timent recognition on twitter.
in proceedings of the15th conference of the european chapter of the as-sociation for computational linguistics: volume 1,long papers, pages 483–493, valencia, spain.
asso-ciation for computational linguistics..xiaobao wu, chunping li, yan zhu, and yishu miao.
2020. short text topic modeling with topic distribu-tion quantization and negative sampling decoder.
inproceedings of the 2020 conference on empiricalmethods in natural language processing (emnlp),pages 1772–1782, online.
association for computa-tional linguistics..jianhua yin and jianyong wang.
2014. a dirich-let multinomial mixture model-based approach forin proceedings of the 20thshort text clustering.
acm sigkdd international conference on knowl-edge discovery and data mining, kdd ’14, page233–242, new york, ny, usa.
association forcomputing machinery..tianyi zhang, varsha kishore, felix wu, kilian q.weinberger, and yoav artzi.
2019. bertscore: eval-in internationaluating text generation with bert.
conference on learning representations..wayne xin zhao, jing jiang, jianshu weng, jing he,ee-peng lim, hongfei yan, and xiaoming li.
2011.comparing twitter and traditional media using topicmodels.
in european conference on information re-trieval, pages 338–349.
springer..wei zhao, maxime peyrard, fei liu, yang gao, chris-tian m meyer, and steffen eger.
2019. moverscore:text generation evaluating with contextualized em-beddings and earth mover distance.
in proceedingsof the 2019 conference on empirical methods innatural language processing and the 9th interna-tional joint conference on natural language pro-cessing (emnlp-ijcnlp), pages 563–578..arkaitz zubiaga, maria liakata, rob procter, geral-dine wong sak hoi, and peter tolmie.
2016.analysing how people orient to and spread rumoursin social media by looking at conversational threads.
plos one, 11(3):e0150989..6810pheme data coherence evaluation.
as original pheme clusters were manually createdby journalists to illustrate speciﬁc stories, they areby default coherent.
hence, according to the guide-lines, these clusters would be classiﬁed as ”good”.
for the artiﬁcially created clusters, pheme datais mixed such that different stories are combinedin different proportions (see 3.2).
artiﬁcially in-truded and chained clusters would be classed as’intermediate’ as they have been generated on thebasis that a clear theme (or themes) can be iden-tiﬁed.
finally, an artiﬁcially random cluster wascreated such that there is no theme found in thetweets as they are too diverse; this type of clusteris evaluated as ’bad’..appendix a.representative-tweet selection.
as described in section 4.2, we select the tweetthat has the lowest divergence score to the toptopic words of the cluster.
following (wan andwang, 2016), we compute the kullback–leibler di-vergence (dkl) between the word distributions ofthe topic θ the cluster c represents and each tweetin c as follows:dkl(θ, tweeti).
(cid:88).
=.
w∈t w (cid:83) sw.pθ(w) ∗ log.
pθ(w)tf (w, tweeti)/len(tweeti).
where pθ(w) is the probability of word w intopic θ. t w denotes top 20 words in clusterc according to the probability distribution whilesw denotes the set of words in tweeti after re-moving stop words.
tf (w, tweeti) denotes the fre-quency of word w in tweeti, and len(tweeti) isthe length of tweeti after removing stop words.
for words that do not appear in sw , we settf (w, tweeti)/len(tweeti) to 0.00001..timings of evaluation metrics.
metric.
time (in seconds).
exhaustive bertscore.
exhaustive bleurt.
exhaustive moverscore1.
exhaustive moverscore2.
exhaustive tf-idf.
graph tf-idf.
10.84.
234.10.
11.73.
11.42.
0.05.
0.12.table a1: average timings of metric performances per1 cluster.
complete results.
the complete results of our experiments are in ta-ble a2.
the notation is as follows:.
• exhaustive indicates that the exhaustive ap-.
proach was employed for the metric..• linear indicates that.
the representativetweet approach was employed for the met-ric..• graph indicates the the graph approach was.
employed for the metric..shortcuts for the metrics are: moverscore1= moverscore applied for unigrams; mover-score2 = moverscore applied for bigrams.
6811exhaustive tf-idf.
linear tf-idf.
graph tf-idf.
exhaustive bleurt.
linear bleurt.
linear bertscore.
graph bertscore.
electionrs / ρ / τ0.62 / 0.62 / 0.49.covid-19rs / ρ / τ0.68 / 0.72 / 0.53.phemers / ρ / τ0.81 / 0.73 / 0.67.
0.51 / 0.48 / 0.39.
0.36 / 0.45 / 0.27.n/a.
0.62 / 0.63 / 0.48.
0.66 / 0.72 / 0.52.
0.74 / 0.71 / 0.60.
0.49 / 0.48 / 0.37.
0.66 / 0.65 / 0.52.
0.84 / 0.86 / 0.69.
0.41 / 0.40 / 0.32.
0.34 / 0.34 / 0.26.
0.49 / 0.50 / 0.38.
0.50 / 0.53 / 0.38.
0.57 / 0.57 / 0.44.
0.62 / 0.64 / 0.49.
0.83 / 0.73 / 0.68.exhaustive bertscore.
0.58 / 0.57 / 0.44.
0.62 / 0.64 / 0.49.
0.83 / 0.80 / 0.68.exhaustive moverscore1.
0.56 / 0.55 / 0.43.
0.46 / 0.56 / 0.35.
0.56 / 0.56 / 0.44.linear moverscore1.
graph moverscore1.
0.54 / 0.52 / 0.41.
0.36 / 0.39 / 0.28.n/a.
0.53 / 0.53 / 0.42.
0.37 / 0.44 / 0.29.
0.52 / 0.56 / 0.40.exhaustive moverscore2.
0.46 / 0.46 / 0.35.
0.35 / 0.46 / 0.27.
0.40 / 0.35 / 0.30.linear moverscore2.
graph moverscore2.
0.47 / 0.46 / 0.35.
0.26 / 0.31 / 0.20.
0.47 / 0.49 / 0.36.
0.42 / 0.50 / 0.32.
0.36 / 0.39 / 0.27.topic coherence glove.
-0.25 / -0.27 / -0.19.
0.04 / 0.02 / 0.03.avg topic coherence glove.
-0.22 / -0.23 / -0.17.
-0.03 / -0.03 / -0.02.topic coherence bertweet.
-0.23 / -0.22 / -0.18.
0.10 / 0.11 / 0.08.avg topic coherence bertweet.
-0.17 / -0.16 / -0.14.
0.04 / 0.04 / 0.03.n/a.
n/a.
n/a.
n/a.
n/a.
n/a.
n/a.
table a2: agreement with human annotators across the election, covid-19 and pheme datasets.
the metricsare spearman’s rank correlation coefﬁcient (rs), pearson correlation coefﬁcient (ρ) and kendall tau (τ )..6812appendix b: annotation guidelines.
overviewyou will be shown a succession of clusters of postsfrom twitter (tweets), where the posts originatefrom the same one hour time window.
each clusterhas been generated by software that has decided itstweets are variants on the same ‘subject’.
you willbe asked for your opinion on the quality (‘coher-ence’) of each cluster as explained below.
as anindication of coherence quality consider how easyit would be to summarise a cluster..steps.
in the guidelines below, a subject is a group ofat least three tweets referring to the same topic.
marking common subjects: in order to keep trackof each subject found in the cluster, label it byentering a number into column subject label andthen assign the same number for each tweet thatyou decide is about the same subject.
note, theorder of the tweets will automatically change asyou enter each number so that those assigned withthe same subject number will be listed together..1. reading a cluster of tweets.
(a) carefully read each tweet in the clusterwith a view to uncovering overlappingconcepts, events and opinions (if any)..(b) identify the common keyword(s) presentin all tweets within the cluster.
notethat common keywords across tweets in acluster are present in all cases by design,so by itself it is not a sufﬁcient criterionto decide on the quality of a cluster..(c) mark tweets belonging to the same sub-ject as described in the paragraph above..2. cluster annotation : what was your opin-.
ion about the cluster?.
(a) choose ‘good’ if you can identify onesubject within the cluster to which mosttweets refer (you can count these basedon the numbers you have assigned in thecolumn subject label).
this should bea cluster that you would ﬁnd it easy tosummarise.
proceed to step 4..(b) choose ‘intermediate’ if you are uncer-tain that the cluster is good, you would.
ﬁnd it difﬁcult to summarise its informa-tion or you ﬁnd that there are a smallnumber (e.g., one, two or three) of un-related subjects being discussed that areof similar size (chained, see issues instep 3) or one clear subject with a mixof other unrelated tweets (intruded, seeissues in step 3).
additionally, if there isone signiﬁcantly big subject and one ormore other ‘small’ subjects (small 2,3tweets), this cluster should be intermedi-ate intruded.
proceed to step 3..(c) choose ‘bad’ if you are certain thatthe cluster is not good and the issue offragmented subjects within the cluster issuch that many unrelated subjects are be-ing discussed (heavily chained) or thereis one subject with a mix of unrelatedtweets but the tweets referring to one sub-ject are a minority.
proceed to step 3..3. issue identiﬁcation: what was wrong with.
the cluster?.
(a) choose ‘chained’ if several subjects canbe identiﬁed in the cluster (with some po-tential random tweets that belong to nosubject), but there are no clear connec-tions between any two subjects.
thisissue can describe both an intermediateand a bad cluster..(b) choose ‘intruded’ if only one commonsubject can be identiﬁed in some tweetsin the cluster and the rest of tweets haveno clear connections to the subject or be-tween each other.
this issue can describeboth an intermediate and a bad cluster.
(c) choose ‘random’ if no subjects can beidentiﬁed at all as there are no clear con-nections between the tweets in the cluster.
usually ‘random’ will be a property ofa bad cluster..4. cluster summarisation you are asked toprovide a brief summary (20-40 words) foreach good cluster you had identiﬁed in step2..6813appendix c: corpus statistics.
size.
in terms of size, we observe that the average tweetin election data is signiﬁcantly shorter (20 tokens)than its correspondent in the covid-19 corpuswhich is 34 tokens long.
we observe that the for-mer’s collection period ﬁnished before twitter plat-form doubled its tweet character limit which wouldbe conﬁrmed by the ﬁgures in the table.
furtherwork will tackle whether tweet length in a clusterhas any impact on the coherence of its message..score differences.
we believe differences in the application of the clus-tering algorithm inﬂuenced the score differencesbetween election and covid-19 datasets.
theclustering algorithm we employed uses a prede-ﬁned list of keywords that partitions the data intosets of tweets mentioning a common keyword asa ﬁrst step.
the keyword set used for the electiondataset contains 438 keywords, while the covid-19 dataset contains 80 keywords used for twitterapi tracking (chen et al., 2020).
we also notethat the different time window span can impact thequality of clusters..6814