de-confounded variational encoder-decoder for logicaltable-to-text generation.
wenqing chen1;2, jidong tian1;2, yitian li1;2, hao he1;2(cid:3), yaohui jin1;2(cid:3)1moe key lab of artiﬁcial intelligence, ai institute, shanghai jiao tong university2state key lab of advanced optical communication system and network,shanghai jiao tong university{wenqingchen, frank92, yitian_li, hehao, jinyh}@sjtu.edu.cn.
abstract.
logical table-to-text generation aims to auto-matically generate ﬂuent and logically faithfultext from tables.
the task remains challengingwhere deep learning models often generatedlinguistically ﬂuent but logically inconsistenttext.
the underlying reason may be that deeplearning models often capture surface-levelspurious correlations rather than the causal re-lationships between the table x and the sen-tence y. speciﬁcally, in the training stage,a model can get a low empirical loss with-out understanding x and use spurious statis-tical cues instead.
in this paper, we proposea de-confounded variational encoder-decoder(dcved) based on causal intervention, learn-ing the objective p(yjdo(x)).
firstly, we pro-pose to use variational inference to estimatethe confounders in the latent space and co-operate with the causal intervention based onpearl’s do-calculus to alleviate the spuriouscorrelations.
secondly,to make the latentconfounder meaningful, we propose a back-prediction process to predict the not-used en-tities but linguistically similar to the exactlyselected ones.
finally, since our variationalmodel can generate multiple candidates, wetrain a table-text selector to ﬁnd out the bestcandidate sentence for the given table.
anextensive set of experiments show that ourmodel outperforms the baselines and achievesnew state-of-the-art performance on two logi-cal table-to-text datasets in terms of logical ﬁ-delity..deﬁned more speciﬁcally, such as abstract mean-ing representation to text (zhao et al., 2020; baiet al., 2020a),infobox with key-value pairs totext (bai et al., 2020b), graph-to-text (song et al.,2020), and table-to-text (wang et al., 2020; parikhet al., 2020) generation..among these tasks, we focus on logical table-to-text generation, which aims to generate ﬂuentand logically faithful text from tables (chen et al.,2020a).
and the ability of logical inference isa kind of high-level intelligence, which is non-trivial for text generation systems in reality.
thetask remains challenging because the referencesentences often convey logically inferred informa-tion, which is not explicitly presented in the table.
as a consequence, data-driven models often gener-ated linguistically ﬂuent but logically inconsistenttext.
recent progress on this task mainly lies inthe use of pretrained language models (lms) likegpt-2 (radford et al., 2018), which was shown toperform much better than non-pretrained models(chen et al., 2020a,e)..however, it is still arguable that whether pre-trained lms can correctly capture the logics, aspretrained lms like bert would use spurious sta-tistical cues for inference (niven and kao, 2019).
the substantial difﬁculty for this task does not layon whether to use the pretrained models or not.
instead, the difﬁculty is because the surface-levelspurious correlations are easier to capture than thecausal relationship between the table and the text.
for example, we have observed that a model coop-erating with gpt-2 generated a sentence "the al-bum was released in the united states 2 time" for agiven table.
but the country where the album wasreleased twice is "the united kingdom"1. in thetraining stage, a model may get low training lossby utilizing the surface-level correlations without.
1.introduction.
data-to-text generation refers to the task of gen-erating descriptive text from non-linguistic inputs.
with the different types of inputs, this task can be.
(cid:3) corresponding authors.
1the details of the table can be found in section 5.6.proceedingsofthe59thannualmeetingoftheassociationforcomputationallinguisticsandthe11thinternationaljointconferenceonnaturallanguageprocessing,pages5532–5542august1–6,2021.©2021associationforcomputationallinguistics5532actually focusing on the selected entities.
as a re-sult, in the inference stage, the model is possibleto produce incorrect facts..in this paper, we view the logical.
table-to-text generation from the perspective of causalinference and propose a de-confounded varia-tional encoder-decoder (dcved).
firstly, giventhe table-sentence pair (x; y), we assume con-founders zc existed in the latent space and con-tributing to the surface-level correlations (e.g.,"the united states" and "the united kingdom").
we estimate zc in the latent space based on varia-tional inference, and cooperate the causal interven-tion based on pearl’s do-calculus (pearl, 2010) tolearn the objective p(yjdo(x)) instead of p(yjx).
secondly, to make the latent confounder mean-ingful, we propose a back-prediction process toensure the latent confounder zc can predict thenot-used entities but linguistically similar to theexactly selected ones.
we also consider the ex-actly selected entities as the mediators in our de-confounded architecture models.
finally, sinceour variational model can generate multiple can-didates, we train a table-text selector to ﬁnd outthe best text for the table.
an extensive set of ex-periments show that our model achieves new state-of-the-art performance on two logical table-to-textdatasets in terms of logical ﬁdelity..the main contributions of this work can be sum-.
marized as follows:.
(cid:15) we propose to use variational inference to es-timate the confounders in the latent space andcooperated with back-prediction to make thelatent variable meaningful..(cid:15) we propose a generate-then-select paradigmjointly considering the surface-level and log-ical ﬁdelity, which can be considered as analternative to reinforcement learning..(cid:15) the experiments have shown that our modelachieves new state-of-the-art performance ontwo logical table-to-text datasets with or with-out pretrained lms..2 related work.
table-to-text generation.
the task of table-to-text generation belongs to the data-to-text gener-ation, where a key feature is the structured inputdata.
lebret et al.
(2016) used a seq2seq neuralmodel with a ﬁeld-infusing strategy that obtains.
ﬁeld-position-aware and ﬁeld-words-aware cellembeddings to generate sentences from wikipediatables.
a follow-up work proposed to updatethe cell memory of the lstm by a ﬁeld gateto help lstm identify the boundary betweendifferent cells (liu et al., 2018).
transformer-based (vaswani et al., 2017) models were also pro-posed which improved the ability to capture long-term dependencies between cells (ma et al., 2019;wang et al., 2020; chen et al., 2020a).
it is worthto mention that the copy mechanism (luong et al.,2015) is an important part to deal with the out-of-vocabulary (oov) words (lebret et al., 2016;gehrmann et al., 2018; chen et al., 2020a) whennot using pretrained language models..logical table-to-text generation.
whileusually ﬂuent, existing methods often halluci-nate phrases that contradict the facts in the table.
to benchmark models’ ability to generate logi-cally consistent sentences, recent work proposeda dataset collected from open domain (chen et al.,2020a), which would score low on those modelsignoring logical consistency.
follow-up work fur-ther proposed another dataset that involved logicalforms as additional supervision information (chenet al., 2020e), which includes common logic typespaired with the underlying logical forms..causal inference.
machine learning mod-els often suffer from the spurious statistical cor-relations brought by unmeasured or latent con-founders (keith et al., 2020).
to eliminate theconfounding bias, one approach is applying thecausal intervention based on pearl’s do-calculus(pearl, 2010).
however, it remains an open prob-lem to choose proper confounders, and the lan-guage of text itself could be a confounder (keithet al., 2020).
it is worth noting that high-qualityobservations of the mediators can also reduce theconfounding bias, as the models will reduce thepossibility of counting on the confounders (chenet al., 2020d)..3 backgrounds.
before introducing our models, we brieﬂy re-view the framework of vae (kingma and welling,2014), a generative model which allows to gener-ate high-dimensional samples from a continuousspace.
in the probability model framework, theprobability of data x can be computed by:.
∫.
∫.
p(x) =.
p(x; z)dz =.
p(z)p(xjz)dz (1).
5533where it is approximated by maximizing the evi-dence lower bound (elbo):.
log p(cid:18)(x) (cid:21) e.[log p(cid:18)(xjz)].
z(cid:24)qϕ(zjx)(cid:0) kl(qϕ(zjx)∥p(z)).
(2).
where p(cid:18)(xjz) denotes the decoder with parame-ters (cid:18) and qϕ(zjx) is obtained by an encoder withparameters ϕ, and p(z) is a prior distribution, forexample, a gaussian distribution.
and kl((cid:1)jj(cid:1))denotes the kullback-leibler (kl) divergence be-tween two distributions..when applied to seq2seq generation wherethe input and the output are denoted by x andy respectively,the conditional variational auto-encoder (cvae), or often known as variationalencoder-decoder (ved), is used with following ap-proximation:.
log p(cid:18)(yjx) (cid:21).
[log p(cid:18)(yjx; z)].
ez(cid:24)qϕ(zjx;y)(cid:0) kl(qϕ(zjx; y)∥p(zjx)).
(3).
in the vanilla cvae formulation, such as theones adopted in (kingma et al., 2014; jain et al.,2017), the prior distribution p(zjx) is approxi-mated to p(z), which is independent on x andﬁxed to a zero-mean unit-variance gaussian dis-tribution n (0; i).
however, this formulation isshown to induce a strong model bias (tomczakand welling, 2018) and empirically perform worsethan non-variational models (wang et al., 2017) inmulti-modal situation..figure 1: the causal graphs before and after the do-calculus.
the symbols x, y, zm, zc denote the inputtable, the output sentence, the hidden mediator, and thehidden confounder, respectively.
we assume c and mto be the proxy variables of zm and zc, respectively,which are relatively easy to be observed..4 methodology.
4.1 de-confounded ved.
from a human perspective, multiple sentences canproperly describe a given table, varying with dif-.
ferent concerns, different logical types or linguis-tic realizations.
therefore, given the input datax and the output sentence y, we can assume alatent variable z existed leading to a conditionalgeneration process p(yjx; z) where z contributesto the diversity.
it suggests a cvae frameworkwith equation 3. however, as discussed in sec-tion 3, the vanilla cvae will introduce a modelbias (tomczak and welling, 2018).
in this subsec-tion, we re-think the cvae from the perspectiveof causal inference.
we assume a directed acyclicgraph (dag) existed, which includes a mediatorzm and a confounder zc as shown in figure 1(a).
the mediator is determined by x and has causal ef-fects on y, while the confounder has causal effectson both x and y..when only considering zm, we can compute the.
probability distribution p(yjx) by:.
∑.
zm.
p(yjx) =.
p(yjx; zm)p(zmjx).
= ezm(cid:24)pφ(zmjx)p(yjx; zm).
(4).
where φ denotes the parameters of a mediator pre-dictor.
an example for zm is the selected en-tity (e.g., united kingdom) from the table x andexactly appeared in y. the vanilla cvae willconstrain zm in the continuous space, and fur-ther approximate the prior distribution p(zmjx) top(zm), which produces biased information..however, it does not mean that removing theapproximation between p(zmjx) and p(zm) isenough.
we observe that models often rely onspurious statistical cues for prediction, resulting insome linguistically similar but inconsistent expres-sions in the generated sentences (e.g., using "theunited states" instead of "the united kingdom).
the model is possible to minimize the training lossrelying on the surface-level correlations betweenthe selected entity and the high-frequency entity.
in this case, the high-frequency entity belongs tothe confounder zc.
in the inference stage, modelmay infer contradicting facts due to a high poste-rior probability of q(zcjx)..to eliminate the spurious correlations, we ap-ply causal intervention by learning the objectivep(yjdo(x)) instead of p(yjx), which forces theinput to be the observed data x, and removes allthe arrows pointing to x as shown in figure 1(b).
when only considering zc, we can compute the in-.
5534xyzmzcc(a) mediation-confounding(b) de-confoundeddo-calculusdo(x)xyzmzccmm(5).
(6).
tervened probability distribution by:.
p(yjdo(x)) =.
p(yjx; zc)p(zc).
∑.
zc.
= ezc(cid:24)p(zc)p(yjx; zc).
where zc is no longer determined by x, makingp(zcjdo(x)) = p(zc).
when applying variationalinference to zc, we have:.
p(yjdo(x)) (cid:21) ezc(cid:24)qϕ(zcjy)p(cid:18)(yjx; zc).
(cid:0) kl(qϕ(zcjy)jp(zc)).
it can be seen that the confounder zc is moresuitable than the mediator zm to cooperate withvariational inference, as cutting off the link zc !
x will naturally make p(zcjdo(x)) to p(zc)..when jointly considering zm and zc, we have:.
∫.
∑.
p(yjdo(x)) =.
p(y; zm; zcjdo(x))dzc.
zm.
zc(cid:21) ezm(cid:24)pφ(zmjx);zc(cid:24)qϕ(zcjy)[log p(cid:18)(yjx;zm; zc)] (cid:0) kl(qϕ(zcjy)∥p(zc)).
(7)according to the intervened causal graph in figure1(b).
the symbols ϕ, φ and (cid:18) denote the param-eters of three probability modeling networks, re-spectively.
it is worth noting that we do not ap-ply variational inference to zm because ﬁnding aproper prior distribution p(zmjx) remains anotherbig topic.
instead, our framework is easy to imple-ment..4.2 making latent variables meaningful.
however, there is no guarantee that zm and zccan represent the real mediators and confoundersin equation 7. if we have no other observed vari-ables, the confounder zc would mainly representthe covariate which is naturally independent of xand has causal effects on y..therefore, we further involve proxy variablesm and c for zm and zc, respectively, where thefull causal graph is shown in figure 1. proxy vari-ables are the proxies of hidden or unmeasured vari-in practice, the medi-ables (miao et al., 2018).
ators and the confounders are often too complexand can not be directly observed.
for example, wemay not be able to directly measure one’s socio-economic status but we are probable to get a proxyby the zip code or job type (louizos et al., 2017).
to make the latent variables zm and zc meaning-.
ful, we add two additional networks and the learn-ing objective is maximizing:.
ezm(cid:24)pφ(zmjx);zc(cid:24)qϕ(zcjy)[log p(cid:18)(yjx;zm; zc)] (cid:0) kl(qϕ(zcjy)∥p(zc))+ ezm(cid:24)pφ(zmjx)[log p(cid:8)(mjzm)]+ ezc(cid:24)qϕ(zcjy)[log p(cid:9)(cjzc)].
(8).
where (cid:8) and (cid:9) denote the parameters of the twoadditional networks..back-prediction from the confounder.
asshown in figure 1(a), the confounder zc inferredfrom y also have a causal effect on x. other-wise, the confounder will collapse into the covari-ate.
the spurious correlations we have observedare that models often generate linguistically simi-lar but logically inconsistent outputs.
for example,"the united kingdom" and "the united state" in-stead of "the united kingdom" because the two en-tities are linguistically similar to each other.
there-fore, we assume the proxy confounders c to be thenot-mentioned entities in the given table.
and wekeep those high-frequency entities in the trainingset ((cid:21) 5 times).
let c = fci;jg 2 rnc(cid:2)lc whereci;j denotes the j-th token of i-th entity, and ncand lc denote the number of entities and maxi-mum length of the entity, respectively.
the log-probability log p(cid:9)(cjzc) is computed by:∑.
log p(cid:9)(cjzc) =.
log p(cid:9)(ci;jjzc; ci;<j).
(9).
i;j.where ci;<j denotes the tokens preceding to the j-th token in the i-th entity.
then we minimize thecross-entropy between p(cid:9)(cjzc) and p(c)..supervision for the mediator.
in the logicaltable-to-text generation task, from the human per-spective, the correct mediators may be the selectedentities, the logical types, or the logical forms(chen et al., 2020e).
in this paper, we only con-sider the selected entities as it is relatively easy toextract while the logical types or forms are labor-intensive to annotate.
we represent the selectedentities by m = fmi;jg 2 rnm(cid:2)lm where mi;jdenotes the j-th token of i-th entity, and nm andlm denote the number of entities and maximumtoken number of the entity, respectively.
the log-probability p(cid:8)(mjzm) is computed by:.
log p(cid:8)(mjzm) =.
log p(cid:8)(mi;jjzm; mi;<j).
∑.
i;j.
(10)where mi;<j denotes the tokens preceding to thej-th token in the i-th entity..55354.3 encoders and decoders implementation.
then we introduce the implementations ofp(cid:18)(yjx; zm; zc), pφ(zmjx), and qϕ(zcjy).
weassume thatthe seq2seq model consists ofan encoder enc((cid:1)) and a decoder dec((cid:1)) forp(cid:18)(yjx; zm; zc).
and a target-oriented encodert-enc((cid:1)) is used for qϕ(zcjy)..firstly, we need to implement pφ(zmjx) andqϕ(zcjy).
let h x be the hidden states of x en-coded via h x = enc(x), and ey be the embed-dings of y before fed to the decoder dec((cid:1)).
weuse a fully-connected neural network (fcnn) toproject h x followed with the average pooling toobtain zm.
and we use the target-oriented encoderto encode ey and obtain h y = t-enc(ey).
weapply the mean pooling operation to h y and ob-tain hy.
to modeling qϕ(zcjy) which is approxi-mated to a gaussian distribution, we use two fc-nns to process hy and obtain the mean vector (cid:22)yand the log variance log (cid:27)2.y which makes:.
qϕ(zcjy) = n ((cid:22)y; (cid:27)2y).
(11).
to implement p(cid:18)(yjx; zm; zc), our model"field-an non-pretrained modelcooperatesinfusing+trans"(chen et al., 2020a) or apretrained model "gpt-tabgen" (chen et al.,2020a).
speciﬁcally, "field-infusing+trans" usesan infusing ﬁeld embedding network to producecell-position-awareandheader-words-awareembeddings ep, then concatenate ep with tokenembeddings to obtain the infused embeddingse = feig 2 rlt(cid:2)d where ei denotes theembedding of i-th token in the table x, and ltand d denote the token number and the dimension,respectively.
then the decoder is used to decode ytoken by token: yt = dec(h x; y(cid:20)t; zm; zc).
thelatent variables zm and zc are concatenated as onelatent variable and projected by a fcnn to get avector zm;c which has the same dimension withh x. then we add zm;c with ey at each decodingstep.
when cooperated with "gpt-tabgen", thedifference from "field-infusing+trans" is thatwe use the gpt-2 as the encoder and decoder,and use the table linearization to indicate thecell position instead of the ﬁeld-infusing method.
more details about the table linearization can befound in (chen et al., 2020a).
and the vectorzm;c is fed to the last transformer layer of gpt-2instead of the ﬁrst layer, which brings less impacton the pretrained gpt-2..4.4 generate-then-select paradigmby sampling multiple latent variables zc (cid:24) p(zc),our model can generate multiple candidate sen-tences ey = (ey1; ey2; :::; eync) for the table xwhere nc is the number of generated sentences.
we propose to ﬁnd out the best sentence by atrained selector.
the generator optimized withmle may focus more on the token-level matchingthan the sentence-level consistency while the se-lector will focus on improving the sentence-levelscores.
therefore, it can be considered as an al-ternative of reinforcement learning.
the selectorscores each candidate sentence by si = s(cid:31)(eyi; x)where (cid:31) denotes the parameters of the selector net-work.
note that we are not designing a selectorsi = s(cid:31)(eyi; y) because the reference sentence yis not available in practice..recent work has provided several selectorsincluding parsing-based and nli-based models(chen et al., 2020c).
we can directly use theseselectors but we aims to develop a more generalselector jointly considering surface-level ﬁdelityand logical ﬁdelity.
we use a mix of bleu-3(papineni et al., 2002) and nli-acc (chen et al.,2020a) scores to supervise the selector.
in thetraining stage of the selector, we can get the goldscores of each generated candidate with the refer-enced sentence y by s(cid:3)i = s(cid:3)(eyi; y).
then, weuse bert to encode x and yi followed with theaverage pooling layers to produce hs and his. fi-nally, we score the table-sentence pair representedby (hs; hi.
s) as follows:.
hf = hs (cid:8) hiss(cid:31)(eyi; x) = (cid:27)(w shf ).
(cid:8) jhs (cid:0) his.j (cid:8) hs ⊙ his.(12).
where (cid:8) and ⊙ denote the concatenation andre-the element-wise multiplication operations,spectively.
and w s denotes the parameters ofthe scoring network.
the score s(cid:31)(eyi; x) is be-tween 0 and 1, and better sentences need to becloser to 1. the scores of gold reference are setto 1. then we use the margin-based triplet lossfor the generated sentences in two way: compar-ing with gold sentences, and comparing betweenarbitrary two generated sentences.
given nc gen-erated candidate sentences, we rank the generatedsentences according to the mix of bleu-3 andnli-acc scores.
the ranked sentences are de-noted by ey r = (ey1r hasthe highest score.
then the loss is computed as.
r ) where ey1.
r; :::; eync.
r; ey2.
5536follows:.
(.
).
l(cid:31) = max(.
0; s(cid:31)(eyi.
r; x) (cid:0) s(y; x) + (cid:13)1).
+ max.
0; s(cid:31)(eyj.
r; x) (cid:0) s(eyi.
r; x) + (cid:13)2.
(13)where (cid:13)1 and (cid:13)2 are the hyperparameters repre-senting margin values, and i and j represent theranked indexes.
at the inference stage, we can se-lect the best sentence with the highest score..dataset.
vocab tables sentences.
train / val.
/ test.
logicnlglogic2text.
122k 7,39214k 5,554.
37,01510,753.
28,450 / 4,260 / 4,3058,566 / 1,095 / 1,092.table 1: the statistics of two datasets..5 experiments.
5.1 datasets.
we conduct experiments on two datasets: logic-nlg (chen et al., 2020a) and logic2text (chenet al., 2020e).
logicnlg is constructed basedon the positive statements of the tabfact dataset(chen et al., 2020c), which contains rich logical in-ferences in the annotated statements.
logic2textis a smaller dataset and provides the annotationof logical forms.
since the annotations of logicalforms are labor-intensive, we only use the table-sentence pairs, following the task formulation oflogicnlg.
the statistics of the two datasets areshown in table 1..5.2 evaluation and settings.
the models are evaluated on the surface-level con-sistency and the logical ﬁdelity.
in terms of thesurface-level consistency, we evaluate models onthe sentence-level bleu scores (papineni et al.,2002) based on 1-3 grams matching.
in termsof logical ﬁdelity, we follow the recent work andapply three metrics including sp-acc and nli-acc based on semantic parsing and pretrained nlimodel, respectively (chen et al., 2020a).
themetrics are computed with the ofﬁcially releasedcodes2..compared models.
we compare our modelswith both non-pretrained and pretrained models.
the non-pretrained models include "field-gating"(liu et al., 2018) and "field-infusing" (lebretet al., 2016) with lstm decoder or transformer.
2https://github.com/wenhuchen/logicnlg.
decoder, which are strong baselines among non-pretrained models.
the pretrained models include"bert-tabgen" and "gpt-tabgen" with the basesize (chen et al., 2020a).
moreover, for the log-icnlg dataset, we compare with a two-phrase ap-proach denoted by "gpt-coarse-to-fine", whichﬁrst generates a template and then generates theﬁnal sentence conditioning on the template (chenet al., 2020a).
for the variational models, we com-pare with the vanilla cvae (kingma et al., 2014)that approximates the prior distribution p(zjx) top(z)..hyperparameters..for the non-pretrainedmodels, we set the dimension of lstm or trans-former to 256. our model is based on "field-infusing+trans" which includes 3-layer trans-formers in the encoder and decoder respectively.
the posterior network qϕ(zcjy) contains a two-layer transformer.
for the pretrained models, weuse the base version of bert and gpt-2 whichhave an embedding size of 768. the kl loss isminimized with the annealing trick where the klweight is set to 0 for 2 epochs and grows to 1:0 inanother 5 epochs.
the learning rate is initializedto set to 0:0001 and 0:000002 for non-pretrainedand pretrained models, respectively.
each modelis trained for 15 epochs.
a special setting for ourmodel is that we generate 10 candidate sentencesfor each table, and report the average performanceand the best performance based on the selector, re-spectively.
we set the hyperparameters (cid:13)1 = 0:2and (cid:13)2 = 0:2 for the selector..5.3 main results.
table 2 and 3 present the performance of ourmodel as well the compared models on the surface-level consistency and the logical ﬁdelity.
asshown, without the selector, our model dcvedalready outperforms the baseline models "field-infusing" and "gpt-tablegen" on both logic-nlg and logic2text datasets.
speciﬁcally, whencompared with "field-infusing", our model in-creases the bleu-3, sp-acc, and nli-acc scoresby 1:4, 3:7, and 3:9 points, respectively on thelogicnlg dataset, and 0:2, 2:4, and 2:8 points onthe logic2text dataset.
when cooperating withgpt-2, our model outperforms "gpt-tablegen"by 1:6, 2:2, and 5:2 points of bleu-3, sp-acc,and nli-acc scores, respectively on the logic-nlg dataset, and 0:2, 1:3, and 5:4 points onthe logic2text dataset.
moreover, our model.
5537model.
type.
surface-level fidelity.
logical fidelity.
bleu-1 bleu-2 bleu-3.
sp-acc nli-acc.
non-pretrained models.
field-gating + lstm-field-gating + trans-field-infusing + lstm-field-infusing + trans-cvae + field-infusing + trans-dcved + field-infusing + trans-trained selectordcved + field-infusing + transdcved + field-infusing + trans oracle nli-acc zdcved + field-infusing + trans oracle bleu-3 z.bert-tabgengpt-tabgengpt-tabgengpt-tabgengpt-coarse-to-finecvae + gpt-tabgendcved + gpt-tabgendcved + gpt-tabgendcved + gpt-tabgendcved + gpt-tabgen.
--adv-regrl---trained selectororacle nli-acc zoracle bleu-3 z.pretrained models.
non-pretrained models.
-field-infusing + trans-cvae + field-infusing + trans-dcved + field-infusing + transdcved + field-infusing + transtrained selectordcved + field-infusing + trans oracle nli-acc zdcved + field-infusing + trans oracle bleu-3 z.gpt-tabgencvae + gpt-tabgendcved + gpt-tabgendcved + gpt-tabgendcved + gpt-tabgendcved + gpt-tabgen.
---trained selectororacle nli-acc zoracle bleu-3 z.pretrained models.
42.344.143.143.746.446.247.445.055.2.
47.848.845.845.146.649.049.349.549.759.7.
37.737.138.839.438.545.6.
46.546.246.448.946.552.1.
19.520.919.720.923.122.923.422.232.9.
26.327.123.123.626.827.928.328.628.538.0.
21.020.421.622.021.529.0.
30.930.831.232.731.237.5.
6.98.37.18.49.49.810.69.015.9.
11.912.69.69.113.313.514.215.314.522.1.
10.59.310.711.010.916.7.
19.919.720.121.420.126.1.
38.038.538.638.939.842.642.141.741.8.
42.242.140.943.142.742.644.343.946.145.0.
38.538.140.940.441.340.8.
42.441.043.743.943.843.5.
56.857.357.157.359.061.262.586.860.3.
68.168.768.567.772.271.873.976.992.274.2.
42.441.645.248.272.544.7.
66.567.871.973.889.972.0.table 2: the experimental results of different models on the test split of logicnlg dataset, where we split thetable into non-pretrained and pretrained models.
the bold represents the best scores.
adv-reg and rl denoteadversarial regularization and reinforcement learning, respectively.
oracle-x represents the upper bound of thegenerated sentences..model.
type.
surface-level fidelity.
logical fidelity.
bleu-1 bleu-2 bleu-3.
sp-acc nli-acc.
table 3: the experimental results of different models on the test split of logic2text dataset, where we split thetable into non-pretrained and pretrained models.
the bold represents the best scores.
oracle-x represents the upperbound of the generated sentences..also outperforms the recent sota model "gpt-coarse-to-fine" which increases the nli-accscore from 72:2 to 73:9 points on the logic2textdataset.
when combining with the trained se-lector, our model further increases the nli-acc.
scores to 76:9 and 73:8 points on logicnlg andlogic2text datasets, respectively.
we also showthe upper bound of our model on bleu and nli-acc scores.
assume that two optimum selectorshave access to the ground-truth sentences, and.
5538dataset.
model.
bleu-3 sp-acc nli-acc.
ﬂuency % logical ﬁdelity %.
logicnlg.
logic2text.
cvaedcved (zc)dcved (zc, c)dcved (zc, zm, m)dcved (full).
cvaedcved (zc)dcved (zc, c)dcved (zc, zm, m)dcved (full).
9.49.09.310.29.8.
9.39.79.611.210.7.
39.840.840.141.842.6.
38.140.239.440.840.9.
59.060.360.260.661.2.
41.642.343.544.845.2.table 4: the performances of ablated models as wellas the full model on the two datasets..would select the best sentence according to thebleu-3 and nli-acc scores, respectively.
asshown, a higher bleu-3 score does not leadto a higher nli-acc score.
similarly, a highernli-acc score does not yield a higher bleu-3score.
the ﬁndings indicate that selecting candi-dates only by bleu-3 or only by nli-acc is notenough.
instead, our trained selector comprehen-sively considers the bleu-3 and nli-acc scores..5.4 ablation study.
to analyze which mechanisms are driving the im-provements, we present an ablation study in table4. we show different ablated models with differ-ent combinations of zc, zm, c and m. all thesemodels are based on "field-infusing".
moreover,the vanilla cvae is also compared, which can beconsidered as a baseline making both zm and zcindependent from x..as shown, both the mediators and the con-founders are inﬂuential.
the full model achievethe best sp-acc and nli-acc scores with slightlylower bleu-3 scores than the ablated model,dcved (zc, zm, m).
eliminating c from the fullmodel leads to a drop of nli-acc by 0:6 and 0:4points on logicnlg and logic2text, respectively.
further eliminating zm and m leads to a drop ofnli-acc by 0:9 and 2:9 points on logicnlg andlogic2text, respectively.
an interesting ﬁnding isthat dcved (zc, c) performs worse than dcved(zc) on sp-acc.
the reason may be that predictingc from zc without considering the mediators zmmay also lead to a bias, similar to cvae.
however,the ablated models all perform better than cvaeon sp-acc and nli-acc..5.5 human evaluation.
following recent work (chen et al., 2020a), wealso perform human evaluation on the ﬂuency and.
gpt-tabgen+ dcved+ dcved + trained selector+ dcved + oracle nli selector.
96.498.399.598.0.
19.125.830.837.1.table 5: the results of human evaluation on the logic-nlg dataset..logical ﬁdelity.
we randomly select 200 tables inthe logicnlg dataset, and generate one sentenceper table for each model.
then we present thegenerated sentences to four raters without tellingwhich model generates them.
the raters are allpost-graduate students majoring in computer sci-ence.
we ask the raters to ﬁnish two binary-decision tasks: 1) whether a generated sentenceis ﬂuent; and 2) whether the fact of a gener-ated sentence can be supported by the given table.
we report the averaged results in table 5, fromwhich we can see that our model "dcved + gpt-tabgen" mainly increases the logical ﬁdelity overthe baseline model "gpt-tabgen" from 19.1% to25.8%.
when cooperated with the trained selec-tor and the oracle nli selector, our model furtherincrease the logical ﬁdelity to 30.8% and 37.1%,respectively.
it is worth noting that the nli selec-tor can be represented by the scorer pn li (ey; x),which does not require the ground-truth sentencey to be available (chen et al., 2020a).
it meansthat the setting of using the oracle nli selector isacceptable..5.6 case study.
to directly see the effect of our model, we presenta case study in figure 2. several gpt-2 basedmodels generate sentences describing two tablesin the logicnlg test set.
the underlined redwords represent the facts contradicting the table.
as shown, for the ﬁrst table, cvae generates thesentence "the album was released in the unitedstate 2 time", where the correct entity should be"the united kingdom" according to the table.
in-stead, our model dcved acknowledges that "thealbum was released in the united kingdom 2time".
moreover, compared with those determin-istic models like gpt-tablegen and gpt-coarse-to-fine, our model can generate sentences withdifferent logical types.
for the second table, wecan see that many contradicting facts exist in re-cent models.
for example, gpt-tablegen gener-ates an incomplete sentence, which uses superla-.
5539to deal with these problems, we believe thattwo directions of work may be workable: 1) en-hancing the mediators.
for example, the logi-cal forms (chen et al., 2020e) can be utilized asthe mediator.
but as mentioned in section 4.2,it is label-intensive to annotate the logical forms;2) large-scale knowledge grounded pre-training,which may be a more promising way.
this typeof work utilized the existing knowledge graphsor crawled data from wikipedia (chen et al.,2020b) to help models better encode/representnon-linguistic inputs, such as the numbers, thetime, or the scores in the tables..6 conclusion.
in this paper, we propose a de-confounded varia-tional encoder-decoder for the logical table-to-textgeneration.
firstly, we assume two latent variablesexisted in the continuous space, representing themediator and the confounder respectively.
and weapply the causal intervention method to reduce thespurious correlations.
secondly, to make the latentvariables meaningful, we use the exactly selectedentities to supervise the mediator and the not se-lected but linguistically similar entities to super-vise the confounder.
finally, since our model cangenerate multiple candidates for a table, we traina selector guided by both surface-level and logi-cal ﬁdelity to select the best sentence.
the exper-iments show that our model yields competitive re-sults with recent sota models..acknowledgments.
the authors would like to thank the anonymousreviewers for their constructive comments.
thiswork was supported by the national key researchand development program of china under grant2018yfc0830400, and shanghai municipal sci-ence and technology major project under grant2021shzdzx0102..references.
xuefeng bai, linfeng song, and yue zhang.
2020a.
online back-parsing for amr-to-text generation.
inproceedings of the 2020 conference on empiricalmethods in natural language processing, emnlp2020, online, november 16-20, 2020, pages 1206–1219. association for computational linguistics..yang bai, ziran li, ning ding, ying shen, and hai-tao zheng.
2020b.
infobox-to-text generation with.
figure 2: the case study of different gpt-2 basedmodels for two tables in the logicnlg test set.
theunderlined red words represent the facts not supportedby the table.
for our model dcved, we present twogenerated sentences for each table..tive logic but not mentions a speciﬁc year.
instead,our model produces two logically consistent sen-tences with superlative and comparative logic..5.7 limitations.
although our model can improve the logical ﬁ-delity to a certain degree, all the models still getlow scores in terms of the logical ﬁdelity in hu-man evaluation, which reﬂects the challenge of thetask.
especially, we ﬁnd that models do not per-form well on certain types of tables: 1) contain-ing and comparing between large numbers, e.g.,18,013 and 29,001 in a table; and 2) containingmixed logics so that models require multi-hop rea-soning, e.g., models generating "there were 3 na-tions that won 2 gold medals" while the correctnation number is 4..5540countrydateeurope17 october 2008australia18 october 2008united kingdom20 october 2008united kingdom1 december 2008united states20 october 2008japan22 october 2008germany5 december 2008global ( itunes )19 november 2012case 1: black ice (album)gpt-tablegen: the album was released in the united state.
gpt-coarse-to-fine: black ice was released in germany and japan.
cvae:  the album was released in the united state 2 time.
dcved:  the album was released in the united kingdom 2 time.
dcved:  the album was released in the united state before the release of the album in japan.case 2: green party of canadaelectionof candidates nominated1984601988681993791997792000111200430820063082008303gpt-tablegen: the green party of canada had the highest number of candidate nominated.
gpt-coarse-to-fine: the green party of canada had 308 more candidate nominated than 1984. cvae:  the green party of canada had the highest number of nomination in the 2000 election.
dcved:  the green party of canada had the highest number of nomination in 2004. dcved:  the green party of canada had more candidate nominated in 2004 than in 2000.tree-like planning based attention network.
in pro-ceedings of the twenty-ninth international jointconference on artiﬁcial intelligence, ijcai 2020,pages 3773–3779.
ijcai.org..wenhu chen, jianshu chen, yu su, zhiyu chen, andwilliam yang wang.
2020a.
logical natural lan-guage generation from open-domain tables.
in pro-ceedings of the 58th annual meeting of the associ-ation for computational linguistics, acl 2020, on-line, july 5-10, 2020, pages 7929–7942.
associationfor computational linguistics..wenhu chen, yu su, xifeng yan, and william yangwang.
2020b.
kgpt: knowledge-grounded pre-training for data-to-text generation.
in proceedingsof the 2020 conference on empirical methods innatural language processing, emnlp 2020, on-line, november 16-20, 2020, pages 8635–8648.
as-sociation for computational linguistics..wenhu chen, hongmin wang, jianshu chen, yunkaizhang, hong wang, shiyang li, xiyou zhou, andwilliam yang wang.
2020c.
tabfact: a large-scaledataset for table-based fact veriﬁcation.
in 8th inter-national conference on learning representations,iclr 2020, addis ababa, ethiopia, april 26-30,2020. openreview.net..wenqing chen, jidong tian, liqiang xiao, hao he,and yaohui jin.
2020d.
exploring logically depen-dent multi-task learning with causal inference.
inproceedings of the 2020 conference on empiricalmethods in natural language processing, emnlp2020, online, november 16-20, 2020, pages 2213–2225. association for computational linguistics..zhiyu chen, wenhu chen, hanwen zha, xiyouzhou, yunkai zhang, sairam sundaresan, andwilliam yang wang.
2020e.
logic2text: high-ﬁdelity natural language generation from logicalin proceedings of the 2020 conference onforms.
empirical methods in natural language process-ing: findings, emnlp 2020, online event, 16-20november 2020, pages 2096–2111.
association forcomputational linguistics..sebastian gehrmann, falcon z. dai, henry elder, andalexander m. rush.
2018. end-to-end content andin pro-plan selection for data-to-text generation.
ceedings of the 11th international conference onnatural language generation, tilburg university,the netherlands, november 5-8, 2018, pages 46–56.
association for computational linguistics..unnat jain, ziyu zhang, and alexander g. schwing.
2017. creativity: generating diverse questions us-ing variational autoencoders.
in 2017 ieee confer-ence on computer vision and pattern recognition,cvpr 2017, honolulu, hi, usa, july 21-26, 2017,pages 5415–5424.
ieee computer society..katherine a. keith, david jensen, and brendano’connor.
2020. text and causal inference: areview of using text to remove confounding from.
causal estimates.
in proceedings of the 58th annualmeeting of the association for computational lin-guistics, acl 2020, online, july 5-10, 2020, pages5332–5344.
association for computational linguis-tics..diederik p. kingma, shakir mohamed, danilo jimenezrezende, and max welling.
2014. semi-supervisedlearning with deep generative models.
in advancesin neural information processing systems 27: an-nual conference on neural information processingsystems 2014, december 8-13 2014, montreal, que-bec, canada, pages 3581–3589..diederik p. kingma and max welling.
2014. auto-in 2nd internationalencoding variational bayes.
conference on learning representations,iclr2014, banff, ab, canada, april 14-16, 2014, con-ference track proceedings..rémi lebret, david grangier, and michael auli.
2016.neural text generation from structured data with ap-plication to the biography domain.
in proceedingsof the 2016 conference on empirical methods innatural language processing, emnlp 2016, austin,texas, usa, november 1-4, 2016, pages 1203–1213.
the association for computational linguistics..tianyu liu, kexiang wang, lei sha, baobao chang,and zhifang sui.
2018. table-to-text generationin proceed-by structure-aware seq2seq learning.
ings of the thirty-second aaai conference on ar-tiﬁcial intelligence, (aaai-18), the 30th innovativeapplications of artiﬁcial intelligence (iaai-18), andthe 8th aaai symposium on educational advancesin artiﬁcial intelligence (eaai-18), new orleans,louisiana, usa, february 2-7, 2018, pages 4881–4888. aaai press..christos louizos, uri shalit, joris m. mooij, david a.sontag, richard s. zemel, and max welling.
2017.causal effect inference with deep latent-variablemodels.
in advances in neural information process-ing systems 30: annual conference on neural in-formation processing systems 2017, december 4-9,2017, long beach, ca, usa, pages 6446–6456..thang luong,.
ilya sutskever, quoc v. le, oriolvinyals, and wojciech zaremba.
2015. addressingthe rare word problem in neural machine translation.
in proceedings of the 53rd annual meeting of theassociation for computational linguistics and the7th international joint conference on natural lan-guage processing of the asian federation of naturallanguage processing, acl 2015, july 26-31, 2015,beijing, china, volume 1: long papers, pages 11–19. the association for computer linguistics..shuming ma, pengcheng yang, tianyu liu, peng li,jie zhou, and xu sun.
2019. key fact as pivot: atwo-stage model for low resource table-to-text gen-in proceedings of the 57th conference oferation.
the association for computational linguistics, acl2019, florence, italy, july 28- august 2, 2019, vol-ume 1: long papers, pages 2047–2057.
associationfor computational linguistics..5541zhenyi wang, xiaoyang wang, bang an, dong yu,and changyou chen.
2020. towards faithful neuraltable-to-text generation with content-matching con-in proceedings of the 58th annual meet-straints.
ing of the association for computational linguistics,acl 2020, online, july 5-10, 2020, pages 1072–1086. association for computational linguistics..yanbin zhao, lu chen, zhi chen, ruisheng cao,su zhu, and kai yu.
2020. line graph enhancedamr-to-text generation with mix-order graph atten-in proceedings of the 58th annualtion networks.
meeting of the association for computational lin-guistics, acl 2020, online, july 5-10, 2020, pages732–741.
association for computational linguis-tics..wang miao, zhi geng, and eric j tchetgen tchetgen.
identifying causal effects with proxy vari-2018.ables of an unmeasured confounder.
biometrika,105(4):987–993..timothy niven and hung-yu kao.
2019. probing neu-ral network comprehension of natural language ar-guments.
in proceedings of the 57th conference ofthe association for computational linguistics, acl2019, florence, italy, july 28- august 2, 2019, vol-ume 1: long papers, pages 4658–4664.
associationfor computational linguistics..kishore papineni, salim roukos, todd ward, and wei-jing zhu.
2002. bleu: a method for automatic eval-uation of machine translation.
in proceedings of the40th annual meeting of the association for compu-tational linguistics, july 6-12, 2002, philadelphia,pa, usa, pages 311–318.
acl..ankur p. parikh, xuezhi wang, sebastian gehrmann,manaal faruqui, bhuwan dhingra, diyi yang, anddipanjan das.
2020. totto: a controlled table-to-text generation dataset.
in proceedings of the 2020conference on empirical methods in natural lan-guage processing, emnlp 2020, online, novem-ber 16-20, 2020, pages 1173–1186.
association forcomputational linguistics..judea pearl.
2010. on measurement bias in causal in-ference.
uncertainty in artiﬁcial intelligence, pages425–432..alec radford, jeffrey wu, rewon child, david luan,dario amodei, and ilya sutskever.
2018. languagemodels are unsupervised multitask learners..linfeng song, ante wang, jinsong su, yue zhang,kun xu, yubin ge, and dong yu.
2020. structuralinformation preserving for graph-to-text generation.
in proceedings of the 58th annual meeting of the as-sociation for computational linguistics, acl 2020,online, july 5-10, 2020, pages 7987–7998.
associa-tion for computational linguistics..jakub m. tomczak and max welling.
2018. vae within international conference on arti-a vampprior.
ﬁcial intelligence and statistics, aistats 2018, 9-11 april 2018, playa blanca, lanzarote, canary is-lands, spain, volume 84 of proceedings of machinelearning research, pages 1214–1223.
pmlr..ashish vaswani, noam shazeer, niki parmar, jakobuszkoreit, llion jones, aidan n. gomez, lukaszkaiser, and illia polosukhin.
2017. attention is allyou need.
neural information processing systems,pages 5998–6008..liwei wang, alexander g. schwing, and svetlanalazebnik.
2017. diverse and accurate image de-scription using a variational auto-encoder with anin advances inadditive gaussian encoding space.
neural information processing systems 30: annualconference on neural information processing sys-tems 2017, december 4-9, 2017, long beach, ca,usa, pages 5756–5766..5542