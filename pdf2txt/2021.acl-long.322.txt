learning language and multimodal privacy-preserving markersof mood from mobile data.
paul pu liang1(cid:63), terrance liu1(cid:63), anna cai1, michal muszynski1, ryo ishii1,nicholas allen2, randy auerbach3, david brent4,ruslan salakhutdinov1, louis-philippe morency11carnegie mellon university 2university of oregon3columbia university 4university of pittsburgh{pliang,terrancl,annacai,mmuszyns,rishii,rsalakhu,morency}@cs.cmu.edu.
nallen3@uoregon.edu.
rpa2009@cumc.columbia.edu.
brentda@upmc.edu.
abstract.
mental health conditions remain underdiag-nosed even in countries with common accessto advanced medical care.
the ability to accu-rately and efﬁciently predict mood from eas-ily collectible data has several important im-plications for the early detection, intervention,and treatment of mental health disorders.
onepromising data source to help monitor humanbehavior is daily smartphone usage.
however,care must be taken to summarize behaviorswithout identifying the user through personal(e.g., personally identiﬁable information) orprotected (e.g., race, gender) attributes.
in thispaper, we study behavioral markers of dailymood using a recent dataset of mobile behav-iors from adolescent populations at high riskof suicidal behaviors.
using computationalmodels, we ﬁnd that language and multimodalrepresentations of mobile typed text (spanningtyped characters, words, keystroke timings,and app usage) are predictive of daily mood.
however, we ﬁnd that models trained to pre-dict mood often also capture private user iden-tities in their intermediate representations.
totackle this problem, we evaluate approachesthat obfuscate user identity while remainingpredictive.
by combining multimodal repre-sentations with privacy-preserving learning,we are able to push forward the performance-privacy frontier..1.introduction.
mental illnesses can have a damaging permanentimpact on communities, societies, and economiesall over the world (world health organization,2003).
individuals often do not realize they areat risk of mental disorders even when they havesymptoms.
as a result, many are late in seekingprofessional help and treatment (thornicroft et al.,2016), particularly among adolescents where sui-cide is the second leading cause of death (curtin.
(cid:63)ﬁrst two authors contributed equally..figure 1: intensive monitoring of behaviors via adoles-cents’ natural use of smartphones may help identifyreal-time predictors of mood in high-risk youth as aproxy for suicide risk.
while smartphones provide avaluable data source spanning text, keystrokes, app us-age, and geolocation, one must take care to summarizebehaviors without revealing user identities through per-sonal (e.g., personally identiﬁable information) or pro-tected attributes (e.g., race, gender) to potentially adver-sarial third parties..and heron, 2019).
in addition to deaths, 16% ofhigh school students report having serious suicidalthoughts each year, and 8% of them make one ormore suicide attempts (cdc, 2015).
this problemis particularly exacerbated as an “echo pandemic”of mental health problems have arisen in the wakeof the covid-19 pandemic (inkster et al., 2021;saha et al., 2020)..intensive monitoring of behaviors via adolescents’natural use of smartphones may help identify real-time predictors of mood in high-risk youth as aproxy for suicide risk (nahum-shani et al., 2018).
while there are inherent limitations in the mis-match between mood prediction and ultimatelydeveloping real-time intervention against immi-nent suicide risk (coppersmith et al., 2018; ophiret al., 2020), we believe that the former is a rea-sonable starting point to tackle similar machinelearning problems surrounding affective computingand privacy-preserving learning.
studying mood inthis high-risk population is a valuable goal given.
proceedingsofthe59thannualmeetingoftheassociationforcomputationallinguisticsandthe11thinternationaljointconferenceonnaturallanguageprocessing,pages4170–4187august1–6,2021.©2021associationforcomputationallinguistics4170real-time assessmentdecentralized multimodal mobile device dataaggregateprivacy-preserving representation learningthat suicide attempts are often decided within ashort time-lapse and just-in-time assessments ofmood changes can be a stepping stone in this di-rection (rizk et al., 2019; oquendo et al., 2020).
technologies for mood prediction can also be avaluable component of decision support for clini-cians and healthcare providers during their assess-ments (mann et al., 2006; cho et al., 2019)..recent work in affective computing has begunto explore the potential in predicting mood frommobile data.
studies have found that typing pat-terns (cao et al., 2017; ghosh et al., 2017a; huanget al., 2018; zulueta et al., 2018), self-reportingapps (suhara et al., 2017), and wearable sen-sors (ghosh et al., 2017b; sano et al., 2018) are par-ticularly predictive.
in addition, multimodal model-ing of multiple sensors (e.g., wearable sensors andsmartphone apps) was shown to further improveperformance (jaques et al., 2017; taylor et al.,2017).
while current work primarily relies on self-report apps for long-term mood assessments (glennand nock, 2014), our work investigates mobile be-haviors from a high-risk teenage population as apredictive signal for daily mood (franklin et al.,2017; large et al., 2017)..prior work has also shown that private informa-tion is predictable from digital records of humanbehavior (kosinski et al., 2013), which is danger-ous especially when sensitive user data is involved.
as a result, in parallel to improving predictiveperformance, a recent focus has been on improv-ing privacy through techniques such as differen-tial privacy (dankar and el emam, 2012, 2013;dankar et al., 2012) and federated learning (mcma-han et al., 2016; geyer et al., 2017; liang et al.,2020b), especially for healthcare data (e.g., elec-tronic health records (xu and wang, 2019)) andwearable devices (chen et al., 2020)..in this paper, as a step towards using multimodalprivacy-preserving mood prediction as ﬁne-grainedsignals to aid in mental health assessment, we ana-lyze a recent dataset of mobile behaviors collectedfrom adolescent populations at high suicidal risk.
with consent from participating groups, the datasetcollects ﬁne-grained features spanning online com-munication, keystroke patterns, and application us-age.
participants are administered daily questionsprobing for mood scores.
by collecting and work-ing on ground-truth data for this population, weare able to benchmark on a more accurate indica-.
tor of mood rather than proxy data such as moodsignals inferred from social media content or be-havior (ernala et al., 2019).
this unique datasetpresents an opportunity to investigate a differentmedium of natural language processing - typedtext which presents new challenges beyond conven-tionally studied written (marcus et al., 1993) andspoken (marslen-wilson and tyler, 1980) text.
wepropose multimodal models that contextualize textwith their typing speeds and app usage.
however,these models often capture private user identities intheir intermediate representations when predictingmood.
as a step towards privacy-preserving learn-ing, we also propose approaches that obfuscate useridentity while remaining predictive of daily mood.
by combining multimodal contextualization withprivacy-preserving learning, we are able to pushforward the performance-privacy frontier.
finally,we conclude with several observations regardingthe uniqueness of typed text as an opportunity fornlp on mobile data..2 multimodal mobile dataset.
intensive monitoring of behaviors via adoles-cents’ frequent use of smartphones may shed newlight on the early risk of suicidal thoughts andideations (nahum-shani et al., 2018).
smartphonesprovide a valuable and natural data source withrich behavioral markers spanning online commu-nication, keystroke patterns, and application usage.
learning these markers requires large datasets withdiversity in participants, variety in features, and ac-curacy in annotations.
as a step towards this goal,we recently collected a dataset of mobile behaviorsfrom high-risk adolescent populations with consentfrom participating groups..we begin with a brief review of the data collectionprocess.
this data monitors adolescents spanning(a) recent suicide attempters (past 6 months) withcurrent suicidal ideation, (b) suicide ideators withno past suicide attempts, and (c) psychiatric con-trols with no history of suicide ideation or attempts.
passive sensing data is collected from each partic-ipant’s smartphone across a duration of 6 months.
participants are administered clinical interviewsprobing for suicidal thoughts and behaviors (stbs),and self-report instruments regarding symptomsand acute events (e.g., suicide attempts, psychiatrichospitalizations) are tracked weekly via a question-naire.
all users have given consent for their mobiledata to be collected and shared with us for research.
4171purposes.
this study has been carefully reviewedand approved by an irb.
we follow the nih guide-lines, with a central irb (single irb) linked tosecondary sites.
we have irb approval for the cen-tral institution and all secondary sites..2.1 mood assessment via self-reportevery day at 8am, users are asked to respond tothe following question - “in general, how have youbeen feeling over the last day?” - with an integerscore between 0 and 100, where 0 means very neg-ative and 100 means very positive.
to construct ourprediction task, we discretized these scores intothe following three bins: negative (0 − 33), neutral(34 − 66), and positive (67 − 100), which follow aclass distribution of 12.43%, 43.63%, and 43.94%respectively.
for our 3-way classiﬁcation task, par-ticipants with fewer than 50 daily self-reports wereremoved since these participants do not provideenough data to train an effective model.
in total,our dataset consists of 1641 samples, consisting ofdata coming from 17 unique participants..2.2 featureswe focused on keyboard data, which includes thetime of data capture, the mobile application used,and the text entered by the user.
for each dailyscore response at 8am, we use information col-lected between 5am on the previous day to 5am onthe current day.
we chose this 5am-5am window bylooking at mobile activity and ﬁnding the lowest ac-tivity point when most people ended their day: 5am.
since users report the previous day’s mood (whenprompted at 8am), we decided to use this 5am-5amtime period to summarize the previous day’s activ-ities.
through prototyping, this prompt time andfrequency were found to give reliable indicators ofthe previous day’s mood.
from this window, weextracted the following features to characterize andcontextualize typed text..text: after removing stop-words, we collected thetop 1000 words (out of approximately 3.2 million)used across all users in our dataset and created abag-of-words feature that contains the daily numberof occurrences of each word..keystrokes: we also extracted keystroke featuresthat record the exact timing that each characterwas typed on a mobile keyboard (including al-phanumeric characters, special characters, spaces,backspace, enter, and autocorrect).
by taking theincrease in recorded timing after each keystroke,we obtain the duration that each key was pressed in.
a sequence of keystrokes during the day.
when ex-tracting keystrokes, we removed all small timingsunder 10−2 seconds..app usage: we count the number of mobile applica-tions used per day, creating a bag-of-apps featurefor each day.
we discard applications that are usedby less than 10% of the participants so that ourfeatures are generalizable to more than just a singleuser in the dataset, resulting in 137 total apps (outof the original 640)..in a preliminary analysis, we observed that predic-tive models performed well when binarizing ourfeature vectors into boolean vectors, which signifywhether a word or app was used on a given day(i.e., mapping values greater than 0 to 1).
our ﬁnalfeature vectors consist of a concatenation of a nor-malized and a binarized feature vector, resultingin 2000 and 274-dimensional vectors for text andapp features respectively.
for keystrokes, we foundthat summarizing the sequence of timings using ahistogram (i.e., deﬁning a set of timing buckets andcreating a bag-of-timings feature) for each day per-formed well.
we chose 100 ﬁne-grained buckets,resulting in a 100-dimensional keystroke vector.
please refer to appendix b for additional detailsabout the dataset and extracted features..3 mood prediction methods.
in this paper, we focus on studying approaches forlearning privacy-preserving representations frommobile data for mood prediction.
our processeddata comes in the form of {(xt,i, xk,i, xa,i, yi)}ni=1with xt ∈ n|vt|=2000 denoting the bag-of-wordsfeatures, xk ∈ n|vk|=100 denoting the bag-of-timings features, and xa ∈ n|va|=274 denoting thebag-of-apps features.
y denotes the label whichtakes on one of our 3 mood categories: negative,neutral, and positive.
in parallel, we also have datarepresenting the corresponding (one-hot) user iden-tity xid which will be useful when learning privacy-preserving representations that do not encode in-formation about user identity xid and evaluatingprivacy performance..3.1 unimodal approacheswe considered two unimodal baselines:.
1. support vector machines (svms) project train-ing examples to a chosen kernel space and ﬁnds theoptimal hyperplane that maximally separates eachclass of instances.
we apply an svm classiﬁer oninput data xuni ∈ {xt, xk, xa} and use supervised.
4172figure 2: diagram of the ni-mlp algorithm learned via the (1) pretrain, (2) selection, and (3) addition phases.
boxes with numbers denote which parameters are being optimized in the corresponding step.
for example, inthe addition phase (3), ni-mlp optimizes parameters δ in g(.
; δ).
(2a) depicts identity-dependent dimensionszid, which is a sparse vector of size dim(zfeat) whose nonzero values (colored purple) signify dimensions of theidentity-dependent subspace in zfeat..learning to predict daily mood labels y..2. multilayer perceptrons (mlps) have seenwidespread success in supervised prediction tasksdue to their ability in modeling complex nonlin-ear relationships.
because of the small size of ourdataset, we choose a simple multilayer perceptronwith two hidden layers.
similarly, we apply anmlp classiﬁer on input data xuni ∈ {xt, xk, xa}to predict daily mood labels y..3.2 multimodal modelswe extend both svm and mlp classiﬁers usingearly fusion (baltrušaitis et al., 2018) of text andapp usage to model multimodal interactions.
specif-ically, we align the input through concatenating thebag-of-words, bag-of-keystrokes, and bag-of-appsfeatures for each day resulting in an input vectorxmulti = xt ⊕ xk ⊕ xa, before using an svm/mlpclassiﬁer for prediction..3.3 a step toward preserving privacywhile classiﬁers trained with traditional supervisedlearning can learn useful representations for moodprediction, they carry the risk of memorizing theidentity of the user along with their sensitive mo-bile usage and baseline mood scores, and possi-bly revealing these identities to adversarial third-parties (abadi et al., 2016).
therefore, it is crucialto perform mood prediction while also protectingthe privacy of personal identities..we adapt the selective-additive learning (sal)framework (wang et al., 2017) for the purposeof privacy-preserving learning.
while sal wasoriginally developed with a very different goalin mind: improving model generalization, we ex-pand sal to a very important problem in health-.
care: preserving privacy.
we adapted sal tolearn disentangled representations separated intoidentity-dependent private information and identity-independent population-level information usingthree phases:.
(1) pretrain phase: the input is a set of (mul-timodal) features x that are likely to containboth identity-dependent and independent infor-mation.
the intermediate representation zfeat =ffeat(x; θ∗feat) is obtained from an mlp classiﬁerpretrained for mood prediction.
ffeat denotes theclassiﬁer with pretrained parameters θ∗.
feat..(2) selection phase: our goal is to now disentanglethe identity-dependent and independent informa-tion within zfeat.
we hypothesize that dependentand independent information are encoded in sep-arate subspaces of the feature vector zfeat.
this al-lows us to disentangle them by training a separateclassiﬁer to predict zfeat as much as possible givenonly the user identity:.
θ∗id = arg min.
(zfeat − fid(xid; θid))2 + λ||zid||1,.
θid.
(1)where xid denotes a one hot encoding of user iden-tity as input, fid denotes the identity encoder withparameters θid, and λ denotes a hyperparameterthat controls the weight of the (cid:96)1 regularizer.
fidprojects the user identity encodings to the fea-ture space learned by ffeat.
by minimizing the ob-jective in equation (1) for each (x, xid) pair, fidlearns to encode user identity into a sparse vectorzid = fid(xid; θ∗id) representing identity-dependentfeatures: the nonzero values of zid represent di-mensions of the identity-dependent subspace inzfeat, while the remaining dimensions belong to the.
4173table 1: comparison of mood prediction performance across different modalities.
best results in bold.
for bothaccuracy and f1 score, models jointly trained on text, keystroke, and apps features outperform models trained usingindividual modalities.
(cid:63) denotes that the difference between multimodal and all unimodal models is statisticallysigniﬁcant (p-value << 0.05)..modalitiestext + keystrokes + appstext + keystrokestext + appstextkeystrokesapps.
baseline19.0719.0719.0719.0719.0719.07.f1 scoresvm mlp59.61(cid:63)62.81(cid:63)57.6561.1958.3862.0856.2761.1551.4357.6852.2958.65.ni-mlp baseline60.11(cid:63)58.7052.9052.6334.7351.32.
40.1840.1840.1840.1840.1840.18.accuracysvm mlp63.59(cid:63)67.43(cid:63)61.8165.8762.9366.5960.6165.8355.8761.0355.2662.65.ni-mlp64.06(cid:63)62.6156.7656.0839.1855.68.identity-independent subspace..4 experiments.
(3) addition phase: given two factors zfeat and zid,to ensure that our prediction model does not cap-ture identity-related information zid, we add mul-tiplicative gaussian noise to remove informationfrom the identity-related subspace zid while repeat-edly optimizing for mood prediction with a ﬁnalmlp classiﬁcation layer g(zfeat, zid; δ).
this result-ing model should only retain identity-independentfeatures for mood prediction:.
ˆy = g (zfeat + (cid:15) (cid:12) zid).
(2).
where (cid:15) ∼ n (0, σ2) is repeatedly sampled acrossbatches and training epochs.
we call this approachnoisy identity mlp, or ni-mlp for short, andsummarize the ﬁnal algorithm in figure 2..controlling the tradeoff between performanceand privacy: there is often a tradeoff betweenprivacy and prediction performance.
to control thistradeoff, we vary the parameter σ, which is thevariance of noise added to the identity-dependentsubspace across batches and training epochs.
σ = 0recovers a standard mlp with good performancebut reveals user identities, while large σ effectivelyprotects user identities but at the possible expenseof mood prediction performance.
in practice, theoptimal tradeoff between privacy and performancevaries depending on the problem.
for our purposes,we automatically perform model selection usingthis performance-privacy ratio r computed on thevalidation set, where.
r =.
smlp − sni-mlptmlp − tni-mlp.
(3).
is deﬁned as the improvement in privacy per unit ofperformance lost.
here, s is deﬁned as the accuracyin user prediction and t is deﬁned as the f1 scoreon mood prediction..we perform experiments to test the utility of text,keystroke, and app features in predicting dailymood while keeping user privacy in mind..4.1 experimental setupdata splits: given that our data is longitudinal, wesplit our data into 10 partitions ordered chrono-logically by users.
we do so in order to maintainindependence between the train, validation, andtest splits in the case where there is some form oftime-level dependency within our labels..evaluation: for each model, we run a nested k-fold cross-validation (i.e., we perform 9-fold val-idation within 10-fold testing).
for each test fold,we identify the optimal parameter set as the onethat achieves the highest mean validation score overthe validation folds.
to evaluate ni-mlp, we usethe best performing mlp model for each test foldas our base classiﬁer before performing privacy-preserving learning.
for all experiments, we reportthe test accuracy and macro f1 score because ourclasses are imbalanced.
given the low number ofcross-validation folds, we use the wilcoxon signed-rank test (wilcoxon, 1992) at 5% signiﬁcance levelfor all statistical comparisons (see appendix c formore experimental details)..4.2 results on mood predictionwe make the following observations regarding thelearned language and multimodal representationsfor mood prediction:.
observation 1: text, keystroke, and app usagefeatures are individually predictive of mood.
to evaluate how predictive our extracted text,keystroke timings, and app usage features are, weﬁrst run experiments using svm, mlp, and ni-mlp on each individual feature separately.
sincewe have unbalanced classes, we chose a majorityclassiﬁer (i.e., most common class in the training.
4174table 2: mood prediction from text using extended pre-trained lm encoders.
we ﬁnd that these models strug-gle on extremely long contexts of typed text..modelsbowbertxlnetlongformer.
f1 score accuracy.
56.2751.4219.8519.85.
60.6158.0642.4042.40.set) as our baseline.
from table 1, we observethat using these three feature types individuallyoutperforms the baseline with respect to accuracyand f1 score.
using the wilcoxon signed-ranktest (wilcoxon, 1992) at 5% signiﬁcance level, wefound that these improvements over the baseline inboth f1 score and accuracy are statistically signiﬁ-cant (p-value << 0.05)..observation 2: pretrained sentence encodersstruggle on this task.
we also applied pretrainedsentence encoders such as bert (devlin et al.,2019) on the language modality for mood predic-tion.
surprisingly, we found that none of these ap-proaches performed stronger than a simple bag-of-words (see table 2).
we provide two possibleexplanations for this phenomenon:.
1. bert is suitable for written text on theweb (wikipedia, bookcorpus, carefully human-annotated datasets) which may not generalize toinformal typed text that contains emojis, typos, andabbreviations (see section 4.4 for a qualitative anal-ysis regarding the predictive abilities of emojis andkeystrokes for mood prediction)..2. we hypothesize that it is difﬁcult to capture suchlong sequences of data (>1000 time steps) spreadout over a day.
current work has shown that bertstruggles with long sequence lengths (beltagy et al.,2020).
we trained two extensions xlnet (yanget al., 2019) and longformer (beltagy et al., 2020)speciﬁcally designed to take in long-range contextbut found that they still underperform as comparedto a simple bag-of-words approach..observation 3: fusing both text and keystroketimings improves performance.
this datasetpresents a unique opportunity to study represen-tations of typed text as an alternative to conven-tionally studied written or spoken text.
while thelatter two use language alone, typed text includeskeystroke features providing information about thetimings of when each character was typed.
in ta-ble 1, we present some of our initial results in learn-ing text and keystroke representations for mood.
table 3: mood prediction using a mlp from text andkeystroke features tallied from (1) all characters, (2) asplit between types of characters, as well as (3) aggre-gated across words..modalitiestexttext + char keystrokestext + split char keystrokestext + word keystrokes.
f1 score accuracy.
56.2757.6557.3256.46.
60.6161.8161.2160.68.prediction and show consistent improvements overtext alone.
we further study the uniqueness of typedtext by comparing the following baselines:.
1. text: bag-of-words only..2. text + char keystrokes: bag-of-words and bag-of-timings across all characters..3. text + split char keystrokes: bag-of-words andbag-of-timings subdivided between 6 groups: al-phanumeric characters, symbols, spacebar, en-ter, delete, and use of autocorrect.
this baselinepresents a more ﬁne-grained decomposition of thetyping speeds across different semantically relatedcharacter groups..4. text + word keystrokes: bag-of-words and bag-of-timings summed up over the characters in eachword.
this presents a more interpretable model toanalyze the relationships between words and thedistribution of their typing speeds..from table 3, we observe that keystrokes accu-rately contextualize text, especially when usingﬁne-grained keystroke distributions across indi-vidual characters.
other methods incorporatingkeystroke features are also all stronger than uni-modal models.
different ways of representingkeystrokes also provide different levels of inter-pretability regarding the relationships betweenwords, characters, and keystrokes for mood predic-tion, which we qualitatively analyze in §4.4..observation 4: multimodalrepresentationlearning achieves the best performance.
in table1, we also compare the performance of our mod-els on combined (text + keystroke + apps) featuresversus the performance on each individual featureset.
for both metrics, combining all features givesbetter performance over either subset..4.3 results on preserving privacydespite these promising results in mood prediction,we ask an important question: does the model cap-ture user identities as an intermediate step towardspredicting mood?
to answer this question, we an-.
4175(a) mlp (without privacy-preserving).
(b) ni-mlp (with privacy-preserving).
figure 3: visualization of representations learned by (a) mlp and (b) ni-mlp, which have been reduced totwo dimensions via t-sne and colored by participant identity.
representations learned by ni-mlp are no longerseparable by users which better preserves privacy..table 4: we report user identity prediction performancefrom raw input data and ﬁnd that identities are veryeasily revealed from text, keystrokes, and app usage..modalitiestextkeystrokesapps.
f1 scoresvm mlp92.0589.4287.0491.3687.4985.68.accuracysvm mlp93.1290.6087.1590.9892.0090.91.alyze the privacy of raw mobile data and trainedmodels.
we then study our proposed method oflearning privacy-preserving features to determinewhether it can obfuscate user identity while remain-ing predictive of daily mood..how private is the mobile data?
we evaluate howmuch the data reveal user identities by trainingpredictive models with typed text, keystroke tim-ings, and app usage as input and user identity asthe prediction target.
from table 4, we observethat all modalities are very predictive of user iden-tity (>87% accuracy), which further motivates theneed to learn privacy-preserving features.
we fur-ther note that identiﬁable information can be verysubtle: while only 28/1000 words were named en-tities, it was possible to identify the user identitywith >87% accuracy, which means that subtle wordchoice can be identify the user (similarly for appsand keystrokes)..how private are the learned privacy-preservingfeatures?
we also study whether our learned fea-tures are correlated with user identity through bothvisualizations and quantitative evaluations..visualizations: we use t-sne (van der maaten andhinton, 2008) to reduce the learned features fromtrained models to 2 dimensions.
after color-codingthe points by participant identity, we identify dis-tinct clusters in figure 3(a), which implies thatmood prediction can be strongly linked to identi-.
table 5: comparison of our privacy-preserving ap-proach (ni-mlp) with the baseline (mlp).
we evalu-ate privacy in predicting user identity from learned rep-resentations (lower accuracy is better), and ﬁnd thatni-mlp effectively obfuscates user identity while re-taining performance.
t: text, k: keystrokes, a: apps..modalitiest + k + at + kt + atka.performance (↑)ni-mlpmlp58.4859.6157.4057.6557.7658.3854.1156.2742.4851.4349.1552.29.privacy (↓).
mlp71.4764.1779.0476.4155.6185.94.ni-mlp34.4930.9965.1352.2025.7166.74.fying the person, therefore coming at the price oflosing privacy..as an attempt to reduce reliance on user identity,we train ni-mlp which is designed to obfuscateuser-dependent features.
after training ni-mlp,we again visualize the representations learned infigure 3(b) and we ﬁnd that they are less visuallyseparable by users, indicating that ni-mlp indeedlearns more user-independent features..quantitative evaluation: to empirically evaluatehow well our models preserve privacy, we extractedthe ﬁnal layer of each trained model and ﬁt a logis-tic regression model to predict user identity usingthese ﬁnal layer representations as input.
the morea model preserves privacy, the harder it should be topredict user identity.
from table 5, we observe thatwe can predict user identity based on the learnedmlp representations with high accuracy (>85%)using the most sensitive app usage features.
forother modality combinations, user identity can alsobe decoded with more than 70% accuracy withthe exception of keystrokes which are the mostprivate (55%).
we achieve signiﬁcantly more pri-vacy using ni-mlp embeddings - roughly 35%.
4176table 6: top emojis associated with positive and nega-tive mood (each row is a different user)..positive emojis.
negative emojis.
table 7: top 3 apps associated with positive and nega-tive moods (each row is a different user)..top 3 positive apps.
top 3 negative apps.
photos, settings, snapchatfacetime, myfitnesspal, musicallyweather, phone, facetimeweather, phone, spotifyspotlight, app store, uberuber, netﬂix, linkedin.
calendar, wattpad, soundcloudnotes, app store, sirichrome, app store, smssafari, notes, groupmepinterest, phone, yolophone, calendar, safari.
understanding the unimodal features: we ﬁrstanalyze how individual words, keystroke timings,and app usage are indicative of positive or negativemood for different users..text: we ﬁnd that several words are particularlyindicative of mood: can’t/cant, don’t/don’t, andsorry are negative for more users than positive,while yes is overwhelmingly positive across users(9 pos, 1 neg), but yeah is slightly negative (5 pos,7 neg).
we also analyze the use of emojis in typedtext and ﬁnd that while there are certain emojis thatlean positive (e.g.,), there are ones (e.g.,:( and) that used in both contexts depending onthe user (see table 6)..apps: in table 7, we show the top 3 apps associ-ated with positive or negative moods across sev-eral users.
it is interesting to observe that manyoutdoor apps (i.e., weather, myfitnesspal, uber),photo sharing apps (i.e., photos, snapchat), andcalling apps (i.e., facetime, phone) are associatedwith positive mood, while personal apps such aspersonal management (i.e., calendar, notes, siri),web browsing (i.e., chrome, safari), and shopping(i.e., app store) are associated with negative mood.
however, some of these ﬁndings are rather user-speciﬁc (e.g., phone can be both positive or nega-tive depending on the user)..understanding the multimodal features: wealso analyze how the same characters and wordscan contribute to different mood predictions basedon their keystroke patterns.
as an example, the dis-tribution of keystrokes for the enter character onthe keyboard differs according to the daily moodof one user (see figure 5 and appendix d.3 for.
figure 4: tradeoff between performance (mood predic-tion f1 score, higher is better) and privacy (identityprediction accuracy, lower is better).
shaded regionsdenote standard deviations from the mean (solid lines).
ni-mlp provides a tunable parameter σ to control thetradeoff, which allows us to plot a range of (perfor-mance, privacy) points.
using a multimodal model ontext, keystroke, and app features obtains better perfor-mance and privacy at the same time..for the best multimodal model, which indicates thepossibility of ni-mlp as a means of achievingprivacy-preserving mood prediction..understanding the tradeoff between perfor-mance and privacy: ni-mlp provides a tunableparameter σ to control the variance of noise appliedon the identity-related dimensions.
this parameterσ has the potential to give a tradeoff between pri-vacy and prediction performance.
in figure 4, weplot this tradeoff between performance (mood pre-diction f1 score, higher is better) and privacy (iden-tity prediction accuracy, lower is better).
we ﬁndthat keystroke features, while themselves not veryuseful in predicting mood, are highly private fea-tures.
it is important to note that keystroke featuresshow strong performance when integrated with textand app usage features while also increasing pri-vacy, thereby pushing the pareto front outwards.
itis also interesting to observe that for most models,performance stays level while privacy improves,which is a promising sign for the real-world de-ployment of such models which requires a balancebetween both desiderata..4.4 qualitative analysisto further shed light on the relationships betweenmood prediction performance and privacy, we per-formed a more in-depth study of the text, keystroke,and app usage features learned by the model (seeappendix d.3 for more examples)..4177limitations & future work: while our approachshows promises in learning representations formood prediction, several future directions on themodeling and nlp side include: 1) better modelsand pre-training algorithms for nlp on typed text,2) algorithms that provide formal guarantees ofprivacy (dwork, 2008), and 3) federated trainingfrom decentralized data (mcmahan et al., 2016)to improve privacy (geyer et al., 2017) and fair-ness (liang et al., 2020a) of sensitive data.
wedescribe more limitations and future social implica-tions of our work in our broader impact statementin appendix a..acknowledgements.
this material was based upon work partially sup-ported by the national science foundation (awards#1750439 and #1734868) and the national insti-tutes of health (award #u01mh116923).
mm wassupported by the swiss national science foun-dation (#p2gep2_184518).
rs was supported bynsf iis1763562 and onr grant n000141812861.
any opinions, ﬁndings, and conclusions, or rec-ommendations expressed in this material are thoseof the author(s) and do not necessarily reﬂect theviews of the national science foundation, nationalinstitutes of health, or ofﬁce of naval research,and no ofﬁcial endorsement should be inferred.
wewould also like to acknowledge nvidia’s gpusupport and the anonymous reviewers for their ex-tremely helpful comments..references.
martin abadi, andy chu, ian goodfellow, h brendanmcmahan, ilya mironov, kunal talwar, and li zhang.
2016. deep learning with differential privacy.
in pro-ceedings of the 2016 acm sigsac conference on com-puter and communications security, pages 308–318..tadas baltrušaitis, chaitanya ahuja, and louis-philippe morency.
2018. multimodal machine learn-ieee transactions oning: a survey and taxonomy.
pattern analysis and machine intelligence, 41(2):423–443..iz beltagy, matthew e peters, and arman cohan.
2020.longformer: the long-document transformer.
arxivpreprint arxiv:2004.05150..tolga bolukbasi, kai-wei chang, james y zou,venkatesh saligrama, and adam t kalai.
2016. man isto computer programmer as woman is to homemaker?
debiasing word embeddings.
in nips..bokai cao, lei zheng, chenwei zhang, philip s yu,andrea piscitello, john zulueta, olu ajilore, kelly.
figure 5: an example where the ‘enter’ character key-press is indicative of either positive, neutral, or negativemood depending on the keypress duration..table 8: words with signiﬁcantly different timings as-sociated with positive and negative moods (each row isa different user)..slower implies positivejustnext, was, into, people.
faster implies positivewhy, thank, hahamaking, work, idk.
stuff, cute, phone, want, talk, see they, send, dont, man, going.
don’t, talk.
think, you, all, love.
more users).
in table 8, we extend this analysisto entire words.
for each of the 500 most com-mon words, we aggregated their accompanyingkeystroke timings for user-reported positive andnegative mood.
these two distributions tell us howthe same word in different keystroke contexts canindicate different moods.
we performed wilcoxonrank-sum tests at 5% signiﬁcance level to comparethese distributions and recorded the words in whicheither faster or slower typing was statistically sig-niﬁcantly correlated with either mood.
observehow certain semantically positive words like love,thank, and haha become judged as more positivewhen typed at a faster speed.
therefore, contex-tualizing text with their keystroke timings offersadditional information when learning representa-tions of typed text..5 conclusion.
in this paper, we investigated the learning of lan-guage and multimodal representations of typed textcollected from mobile data.
we studied the chal-lenge of learning markers of daily mood as a steptowards early detection and intervention of mentalhealth disorders for social good.
our method alsoshows promising results in obfuscating user iden-tities for privacy-preserving learning, a directioncrucial towards real-world learning from sensitivemobile data and healthcare labels.
in addition, ourﬁndings illustrate several challenges and opportu-nities in representation learning from typed text asan understudied area in nlp..4178ryan, and alex d leow.
2017. deepmood: modelingmobile phone typing dynamics for mood detection.
inproceedings of the 23rd acm sigkdd internationalconference on knowledge discovery and data mining,pages 747–755..nicholas carlini, florian tramer, eric wallace,matthew jagielski, ariel herbert-voss, katherine lee,adam roberts, tom brown, dawn song, ulfar erlings-son, et al.
2020. extracting training data from largelanguage models.
arxiv preprint arxiv:2012.07805..cdc.
2015. suicide facts at a glance 2015..yiqiang chen, xin qin, jindong wang, chaohui yu,and wen gao.
2020. fedhealth: a federated transferlearning framework for wearable healthcare.
ieee in-telligent systems..chul-hyun cho, taek lee, min-gwan kim, hoh pe-ter in, leen kim, and heon-jeong lee.
2019. moodprediction of patients with mood disorders by ma-chine learning using passive digital phenotypes basedon the circadian rhythm: prospective observational co-journal of medical internet research,hort study.
21(4):e11029..glen coppersmith, ryan leary, patrick crutchley, andalex fine.
2018. natural language processing of socialmedia as screening for suicide risk.
biomedical infor-matics insights, 10:1178222618792860..sally c curtin and melanie p heron.
2019. death ratesdue to suicide and homicide among persons aged 10–24: united states, 2000–2017..fida kamal dankar and khaled el emam.
2012. theapplication of differential privacy to health data.
inproceedings of the 2012 joint edbt/icdt workshops,pages 158–166..fida kamal dankar and khaled el emam.
2013. prac-ticing differential privacy in health care: a review.
trans.
data priv., 6(1):35–67..fida kamal dankar, khaled el emam, angelicaneisa, and tyson roffey.
2012. estimating the re-identiﬁcation risk of clinical data sets.
bmc medicalinformatics and decision making, 12(1):66..jacob devlin, ming-wei chang, kenton lee, andkristina toutanova.
2019.bert: pre-training ofdeep bidirectional transformers for language under-in proceedings of the 2019 conferencestanding.
of the north american chapter of the associationfor computational linguistics: human language tech-nologies, volume 1 (long and short papers), pages4171–4186, minneapolis, minnesota.
association forcomputational linguistics..cynthia dwork.
2008. differential privacy: a sur-in international conference on theoryvey of results.
and applications of models of computation, pages 1–19.
springer..methodological gaps in predicting mental health statesfrom social media: triangulating diagnostic signals.
inproceedings of the 2019 chi conference on human fac-tors in computing systems, pages 1–16..joseph c franklin, jessica d ribeiro, kathryn rfox, kate h bentley, evan m kleiman, xieyininghuang, katherine m musacchio, adam c jaroszewski,bernard p chang, and matthew k nock.
2017. riskfactors for suicidal thoughts and behaviors: a meta-analysis of 50 years of research.
psychological bulletin,143(2):187..robin c geyer, tassilo klein, and moin nabi.
2017.differentially private federated learning: a client levelperspective.
arxiv preprint arxiv:1712.07557..surjya ghosh, niloy ganguly, bivas mitra, andpradipta de.
2017a.
evaluating effectiveness of smart-phone typing as an indicator of user emotion.
in 2017seventh international conference on affective comput-ing and intelligent interaction (acii), pages 146–151.
ieee..surjya ghosh, niloy ganguly, bivas mitra, andpradipta de.
2017b.
tapsense: combining self-reportpatterns and typing characteristics for smartphonein proceedings of the 19thbased emotion detection.
international conference on human-computer inter-action with mobile devices and services, pages 1–12..catherine r glenn and matthew k nock.
2014.im-proving the short-term prediction of suicidal behavior.
american journal of preventive medicine, 47(3):s176–s180..he huang, bokai cao, s yu phillip, chang-dongwang, and alex d leow.
2018. dpmood: exploitinglocal and periodic typing dynamics for personalizedmood prediction.
in 2018 ieee international confer-ence on data mining (icdm), pages 157–166.
ieee..becky inkster et al.
2021. early warning signs ofa mental health tsunami: a coordinated response togather initial data insights from multiple digital ser-vices providers.
frontiers in digital health, 2:64..natasha jaques, sara taylor, akane sano, and rosalindpicard.
2017. multimodal autoencoder: a deep learn-ing approach to ﬁlling in missing sensor data and en-abling better mood prediction.
in 2017 seventh inter-national conference on affective computing and intel-ligent interaction (acii), pages 202–208.
ieee..michal kosinski, david stillwell, and thore graepel.
2013. private traits and attributes are predictable fromdigital records of human behavior.
proceedings of thenational academy of sciences, 110(15):5802–5805..matthew michael large, daniel thomas chung,michael davidson, mark weiser,and christo-pher james ryan.
2017. in-patient suicide: selection ofpeople at risk, failure of protection and the possibilityof causation.
bjpsych open, 3(3):102–105..sindhu kiranmai ernala, michael l birnbaum,kristin a candan, asra f rizvi, william a sterling,john m kane, and munmun de choudhury.
2019..ellen e lee, john torous, munmun de choudhury,colin a depp, sarah a graham, ho-cheol kim, mar-tin p paulus, john h krystal, and dilip v jeste.
2021..4179artiﬁcial intelligence for mental healthcare: clinicalapplications, barriers, facilitators, and artiﬁcial wis-dom.
biological psychiatry: cognitive neuroscienceand neuroimaging..paul pu liang,irene mengze li, emily zheng,yao chong lim, ruslan salakhutdinov, and louis-philippe morency.
2020a.
towards debiasing sentencein proceedings of the 58th annualrepresentations.
meeting of the association for computational linguis-tics, pages 5502–5515, online.
association for compu-tational linguistics..paul pu liang, terrance liu, liu ziyin, ruslansalakhutdinov, and louis-philippe morency.
2020b.
think locally, act globally: federated learning witharxiv preprintlocal and global representations.
arxiv:2001.01523..paul pu liang, ziyin liu, amirali bagher zadeh, andlouis-philippe morency.
2018. multimodal languageanalysis with recurrent multistage fusion.
in proceed-ings of the 2018 conference on empirical methods innatural language processing, pages 150–161, brus-sels, belgium.
association for computational linguis-tics..kirsten lloyd.
2018. bias ampliﬁcation in artiﬁcial in-telligence systems.
corr, abs/1809.07842..lingjuan lyu, han yu, and qiang yang.
2020. threatsarxiv preprintto federated learning: a survey.
arxiv:2003.02133..laurens van der maaten and geoffrey hinton.
2008.visualizing data using t-sne.
journal of machine learn-ing research, 9(11)..j john mann, dianne currier, barbara stanley, maria aoquendo, lawrence v amsel, and steven p ellis.
2006.can biological tests assist prediction of suicide in mooddisorders?
international journal of neuropsychophar-macology, 9(4):465–474..mitchell p. marcus, beatrice santorini, and mary annmarcinkiewicz.
1993. building a large annotated cor-pus of english: the penn treebank.
computationallinguistics, 19(2):313–330..william marslen-wilson and lorraine komisarjevskytyler.
1980. the temporal structure of spoken languageunderstanding.
cognition, 8(1):1–71..h. brendan mcmahan, eider moore, daniel ram-age, seth hampson, and blaise agüera y arcas.
2016.communication-efﬁcientlearning of deep networksfrom decentralized data.
in aistats..inbal nahum-shani, shawna n smith, bonnie j spring,linda m collins, katie witkiewitz, ambuj tewari, andsusan a murphy.
2018.just-in-time adaptive inter-ventions (jitais) in mobile health: key components anddesign principles for ongoing health behavior support.
annals of behavioral medicine, 52(6):446–462..yaakov ophir, refael tikochinski, christa sc aster-han, itay sisso, and roi reichart.
2020. deep neu-.
ral networks detect suicide risk from textual facebookposts.
scientiﬁc reports, 10(1):1–10..maria a oquendo, hanga c galfalvy, tse-hwei choo,raksha kandlur, ainsley k burke, m elizabeth sub-lette, jeffrey m miller, j john mann, and barbara hstanley.
2020. highly variable suicidal ideation: a phe-notypic marker for stress induced suicide risk.
molecu-lar psychiatry, pages 1–8..jahna otterbacher, alessandro checco, gianluca de-martini, and paul clough.
2018. investigating user per-ception of gender bias in image search: the role of sex-ism.
in the 41st international acm sigir conferenceon research development in information retrieval, si-gir ’18, page 933–936, new york, ny, usa.
associ-ation for computing machinery..philip resnik, april foreman, michelle kuchuk,katherine musacchio schafer, and beau pinkham.
2021. naturally occurring language as a source ofsuicide and life-evidence in suicide prevention.
threatening behavior, 51(1):88–96..mina m rizk, tse-hwei choo, hanga galfalvy, emilybiggs, beth s brodsky, maria a oquendo, j johnmann, and barbara stanley.
2019. variability in sui-cidal ideation is associated with affective instability insuicide attempters with borderline personality disorder.
psychiatry, 82(2):173–178..koustuv saha, john torous, eric d caine, and mun-mun de choudhury.
2020. psychosocial effects of thecovid-19 pandemic: large-scale quasi-experimentalstudy on social media.
journal of medical internet re-search, 22(11):e22600..akane sano, sara taylor, andrew w mchill, an-drew jk phillips, laura k barger, elizabeth klerman,and rosalind picard.
2018. identifying objective phys-iological markers and modiﬁable behaviors for self-reported stress and mental health status using wearablesensors and mobile phones: observational study.
jour-nal of medical internet research, 20(6):e210..allison schuck, raffaella calati, shira barzilay, sarahbloch-elkouby, and igor galynker.
2019. suicide cri-sis syndrome: a review of supporting evidence for anew suicide-speciﬁc diagnosis.
behavioral sciences&amp; the law, 37(3):223–239..virginia smith, chao-kai chiang, maziar sanjabi, andameet s talwalkar.
2017. federated multi-task learn-in advances in neural information processinging.
systems, pages 4424–4434..yoshihiko suhara, yinzhan xu, and alex’sandy’ pent-land.
2017. deepmood: forecasting depressed moodbased on self-reported histories via recurrent neural net-works.
in proceedings of the 26th international con-ference on world wide web, pages 715–724..sara ann taylor, natasha jaques, ehimwenmanosakhare, akane sano, and rosalind picard.
2017.personalized multitask learning for predicting tomor-row’s mood, stress, and health.
ieee transactions onaffective computing..4180graham thornicroft, nisha mehta, sarah clement,sara evans-lacko, mary doherty, diana rose, mirjakoschorke, rahul shidhaye, claire o’reilly, andclaire henderson.
2016. evidence for effective in-terventions to reduce mental-health-related stigma anddiscrimination.
the lancet, 387(10023):1123–1132..haohan wang, aaksha meghawat, louis-philippemorency, and eric p xing.
2017. select-additive learn-ing: improving generalization in multimodal sentimentin 2017 ieee international conference onanalysis.
multimedia and expo (icme), pages 949–954.
ieee..frank wilcoxon.
1992.individual comparisons byranking methods.
in breakthroughs in statistics, pages196–202.
springer..world health organization.
2003. investing in mentalhealth..jie xu and fei wang.
2019.ing for healthcare informatics.
arxiv:1911.06270..federated learn-arxiv preprint.
zhilin yang, zihang dai, yiming yang, jaime car-bonell, russ r salakhutdinov, and quoc v le.
2019.xlnet: generalized autoregressive pretraining for lan-guage understanding.
advances in neural informationprocessing systems, 32:5753–5763..han zhao and geoff gordon.
2019. inherent tradeoffsin advances in neu-in learning fair representations.
ral information processing systems, volume 32, pages15675–15685.
curran associates, inc..ligeng zhu and song han.
2020. deep leakagein federated learning, pages 17–31.
from gradients.
springer..john zulueta, andrea piscitello, mladen rasic, re-becca easter, pallavi babu, scott a langenecker,melvin mcinnis, olusola ajilore, peter c nelson,kelly ryan, et al.
2018. predicting mood disturbanceseverity with mobile phone keystroke metadata: a bi-affect digital phenotyping study.
journal of medicalinternet research, 20(7):e241..4181appendix.
a broader impact statement.
learning markers of mood from mobile datapresents an opportunity for large-scale adaptiveinterventions of suicidal ideation.
however, thereare important concerns regarding its implicationsto society and policy..applications in mental health: suicide is the sec-ond leading cause of death among adolescents.
inaddition to deaths, 16% of high school studentsreport seriously considering suicide each year, and8% make one or more suicide attempts (cdc,2015).
despite these alarming statistics, there islittle consensus concerning imminent risk for sui-cide (franklin et al., 2017; large et al., 2017).
cur-rent research conducts clinical interviews and pa-tient self-report questionnaires that provide long-term assessments of suicide risk.
however, fewstudies have focused on imminent suicidal risk,which is of critical clinical importance as a step to-wards adaptive real-time interventions (glenn andnock, 2014; schuck et al., 2019).
given the impactof suicide on society, there is an urgent need tobetter understand the behavior markers related tosuicidal ideation..“just-in-time” adaptive interventions delivered viamobile health applications provide a platform of ex-citing developments in low-intensity, high-impactinterventions (nahum-shani et al., 2018).
the abil-ity to intervene precisely during an acute risk forsuicide could dramatically reduce the loss of life.
to realize this goal, we need accurate and timelymethods that predict when interventions are mostneeded.
monitoring (with participants’ permission)mobile data to assess mental health and provideearly interventions is, therefore, a rich opportunityfor scalable deployment across high-risk popula-tions.
our data collection, experimental study, andcomputational approaches provide a step towardsdata-intensive longitudinal monitoring of humanbehavior.
however, one must take care to summa-rize behaviors from mobile data without identifyingthe user through personal (e.g., personally identiﬁ-able information) or protected attributes (e.g., race,gender).
this form of anonymity is critical whenimplementing these technologies in real-world sce-narios.
our goal is to be highly predictive of moodwhile remaining as privacy-preserving as possible.
we outline some of the potential privacy and secu-rity concerns below..limitations: while we hope that our research canprovide a starting point on the potential of detect-ing mood unobtrusively throughout the day in aprivacy-preserving way, we strongly acknowledgethere remain methodological issues where a lotmore research needs to be done to enable the real-world deployment of such technologies.
we em-phasize that healthcare providers and mobile appstartups should not attempt to apply our approachin the real world until the following issues (andmany more) can be reliably resolved:.
1. we do not make broad claims across teenagepopulations from only 17 participants in thisstudy.
furthermore, it remains challenging formodels to perform person-independent pre-diction which makes it hard to deploy acrosslarge populations..2. our current work on predicting daily mood isstill a long way from predicting imminent sui-cide risk.
furthermore, any form of predictionis still signiﬁcantly far away from integratingmethods like this into the actual practice ofmental health, which is a challenging probleminvolving a broad range of medical, ethical,social, and technological researchers (resniket al., 2021; lee et al., 2021)..3. text and keystrokes can differ for participantswho speak multiple languages or non-prestigevernaculars.
one will need to ensure that themethod works across a broad range of lan-guages to ensure accessibility in its desiredoutcomes..4. this study assumes that participants have norestrictions for data/network connections &data plans on their phones, which may leaveout vulnerable populations that do not meetthis criterion..privacy and security: there are privacy risks as-sociated with making predictions from mobile data.
to deploy these algorithms across at-risk popula-tions, it is important to keep data private on eachdevice without sending it to other locations.
evenif data is kept private, it is possible to decode datafrom gradients (zhu and han, 2020) or pretrainedmodels (carlini et al., 2020).
in addition, sensitivedatabases with private mobile data could be at-riskto external security attacks from adversaries (lyuet al., 2020).
therefore, it is crucial to obtain userconsent before collecting device data.
in our exper-.
4182iments with real-world mobile data, all participantshave given consent for their mobile device data tobe collected and shared with us for research pur-poses.
all data was anonymized and stripped of allpersonal (e.g., personally identiﬁable information)and protected attributes (e.g., race, gender)..social biases: we acknowledge that there is a riskof exposure bias due to imbalanced datasets, es-pecially when personal mobile data and sensitivehealth labels (e.g., daily mood, suicidal thoughtsand behaviors, suicide risk).
models trained onbiased data have been shown to amplify the un-derlying social biases especially when they corre-late with the prediction targets (lloyd, 2018).
thisleaves room for future work in exploring methodstailored for speciﬁc scenarios such as mitigatingsocial biases in words (bolukbasi et al., 2016), sen-tences (liang et al., 2020a), and images (otter-bacher et al., 2018).
future research should also fo-cus on quantifying the trade-offs between fairnessand performance (zhao and gordon, 2019)..overall, we believe that our proposed approach canhelp quantify the tradeoffs between performanceand privacy.
we hope that this brings about futureopportunities for large-scale real-time analytics inhealthcare applications..b dataset details.
the mobile assessment for the prediction of sui-cide (maps) dataset was designed to elucidatereal-time indicators of suicide risk in adolescentsages 13 − 18 years.
current adolescent suicideideators and recent suicide attempters along withaged-matched psychiatric controls with no lifetimesuicidal thoughts and behaviors completed baselineclinical assessments (i.e., lifetime mental disorders,current psychiatric symptoms).
following the base-line clinical characterization, a smartphone app,the effortless assessment of risk states (ears),was installed onto adolescents’ phones, and passivesensor data were acquired for 6-months.
notably,during ears installation, a keyboard logger is con-ﬁgured on adolescents’ phones, which then tracksall words typed into the phone as well as the appsused during this period.
each day during the 6-month follow-up, participants also were asked torate their mood on the previous day on a scale rang-ing from 1 − 100, with higher scores indicating abetter mood.
after extracting multimodal featuresand discretizing the labels (see section 2), we sum-marize the ﬁnal dataset feature and label statistics.
in table 9..c experimental setup.
we provide additional details on the model imple-mentation and experimental setup..implementation details.
c.1all models and analyses were done in python.
svm models were implemented with scikit-learn and mlp/ni-mlp models were imple-mented with pytorch.
bert, xlnet, and long-former models were ﬁne-tuned using huggingface (website: https://huggingface.co, github:https://github.com/huggingface)..c.2 hyperparameterswe performed a small hyperparameter search overthe ranges in table 10. this resulted in a total of35 hyperparameter conﬁgurations for svm and12 for mlp (6 for apps only).
by choosing thebest-performing model on the validation set, weselected the resulting hyperparameters as shown intable 10..c.3 model parameterseach model has about two million parameters.
seetable 10 for exact hidden dimension sizes..c.4 training resources and timeall experiments were conducted on a geforce rtx2080 ti gpu with 12 gb memory.
see table 11for approximate running times..d experimental details.
we present several additional analysis of the dataand empirical results:.
d.1 details on mood predictionthere is often a tradeoff between privacy and pre-diction performance.
to control this tradeoff, wevary the parameter σ, which is the amount of noiseadded to the identity-dependent subspace acrossbatches and training epochs.
in practice, we au-tomatically perform model selection using thisperformance-privacy ratio r computed on the vali-dation set, where.
r =.
smlp − sni-mlptmlp − tni-mlp.
(4).
is deﬁned as the improvement in privacy per unit ofperformance lost.
here, s is deﬁned as the accuracyin the user prediction task and t is deﬁned as the f1score on the mood prediction task..4183table 9: mobile assessment for the prediction of suicide (maps) dataset summary statistics..users datapoints modalities.
labels.
textkeystrokesapp usage.
featuresbag-of-words, one-hotbag-of-timingsbag-of-apps, one-hot.
dimensions2000100274.
17.
1641.daily mood: negative, neutral, positive.
table 10: model parameter conﬁgurations.
*integer kernel values denote the degree of a polynomial kernel..table 11: approximate training times (total across 10-fold cross validation and hyperparameter search)..model.
svm.
mlp.
ni-mlp.
model.
svm.
parameterckernel*hidden dim 1 (multimodal & text only)hidden dim 2 (multimodal & text only)hidden dim 1 (keystrokes only)hidden dim 2 (keystrokes only)hidden dim 1 (apps only)hidden dim 2 (apps only)dropout ratelearning ratebatch sizeepochsλσ.value0.1, 0.5, 1, 2, 3, 5, 10rbf, 2, 3, 5, 101024, 512128, 6464, 3232, 16128128, 640, 0.2, 0.50.0011002000.1, 1, 2, 3, 5, 101, 5, 10, 25, 50, 100, 150.modalitytext + keystrokes + appstext + keystrokestext + appstextkeystrokesappstext + keystrokes + appstext + keystrokestext + appstextkeystrokesappsall.
time (hours)1010108116565424.mlp (100 epochs, 3 runs).
ni-mlp.
in the rare cases where ni-mlp performed bet-ter than the original mlp and caused r to be-come negative, we found this improvement in per-formance always came at the expense of worseprivacy as compared to other settings of λ and σ inni-mlp.
therefore, models with negative r werenot considered for table 1..pected.
we attribute this to randomness in predict-ing both mood and identities.
furthermore, wanget al.
(2017) found that adding noise to the identitysubspace can sometimes improve generalization byreducing reliance on identity-dependent confound-ing features, which could also explain occasionalincreased performance at larger σ values..d.2 details on preserving privacyfor table 5, the model with the best privacy out ofthose within 5% performance of the original mlpmodel (or, if no such model existed, the model withthe best performance) was selected..interestingly, in figure 4, we ﬁnd that the trade-off curve on a model trained only using app fea-tures does not exhibit a pareto tradeoff curve as ex-.
note that we do not include privacy results for fea-tures learned by svm, which ﬁnds a linear separa-tor in a speciﬁed kernel space rather than learninga representation for each sample.
explicitly pro-jecting our features is computationally infeasibledue to the high dimensionality of our chosen kernelspaces..4184table 12: top 5 words associated with positive and negative moods (each row is a different user)..top 5 positive wordshot, goodnight, ft, give, keepstill, y’all, guys, new, comemind, days, went, tf, nextgirls, music, happy, mean, getting.
top 5 negative wordssoon, ﬁrst, ya, friend, leaveamazing, see, said, idk, looktired, hair, stg, snap, anyoneomg, people, talking, ask, might.
table 13: top words associated with positive and negative moods across users.
we ﬁnd that while certain positivewords are almost always indicative of mood, others are more idiosyncratic and depend on the user..positive users negative users.
negative words negative users.
positive wordsmakeyesgotstillwannalikeneedsendgetgood.
9977777777.
1111122223.i’m/imfeelyeahcan’t/cantpeopleknowgoonetodayday.
10776666655.positive users5352445612.d.3 qualitative analysisin this section, we provide more empirical analysison the unimodal and multimodal features in themaps dataset..d.3.1 understanding the unimodal.
features.
text: we begin with some basic statistics regardingword distributions.
for each user, we tallied thefrequencies of each word under each daily moodcategory (positive, neutral, and negative), as well asthe overall number of words in each mood category.
we deﬁne “positive” words and emojis to be thosewith a higher relative frequency of positive moodcompared to the overall positive mood frequency,and lower than overall negative mood frequency.
likewise, “negative” words and emojis have higherthan overall negative mood frequency and lowerthan overall positive mood frequency.
we ﬁlteredout words for speciﬁc users if the word was usedless than 40 times.
finally, we ranked the words bythe difference in relative frequency (i.e., a word is“more positive” the larger the difference betweenits positive mood relative frequency and the user’soverall positive mood relative frequency).
see ta-ble 12 for examples of top positive and negativewords.
for each word, we also counted the numberof users for which the word was positive or nega-tive.
see table 13 for the words with the highestuser counts..keystrokes: we show some sample bag-of-timinghistograms in figure 6. it is interesting to ﬁnd that.
certain users show a bimodal distribution acrosstheir keystroke histograms with one peak represent-ing faster typing and another representing slowertyping.
visually, the overall keystroke histogramsdid not differ that much across users which mightexplain its lower accuracies in both mood and userprediction when trained with ni-mlp (see fig-ure 4)..app usage: similar to “positive” words, we deﬁne“positive” apps to be those with higher than overallpositive mood relative frequency and lower thanoverall negative mood relative frequency, and “neg-ative” apps to be the opposite.
apps were also thensorted by difference in relative frequency..d.3.2 understanding the multimodal.
features.
characters with keystrokes: for each user, we plot-ted histograms of keystroke timings of alphanu-meric characters, symbols (punctuation and emo-jis), spacebar, enter, delete, and use of autocorrect,split across daily mood categories.
see figure 7for examples across one user.
we ﬁnd particularlyinteresting patterns in the autocorrect keys andsymbols where keystrokes are quite indicative ofmood, which attests to the unique nature of typedtext..words with keystrokes: for each user, we plottedhistograms of the word-level keystroke timings ofthe top 500 words, split across the daily mood cat-egories of positive, neutral, and negative.
we alsoperformed wilcoxon rank-sum tests at 5% signiﬁ-.
4185figure 6: examples of keystroke timing histograms for different users.
we ﬁnd that the distribution of keystroketimings varies between unimodal and bimodal for different users..figure 7: example of more character key-presses and how their keystroke patterns can be indicative of eitherpositive, neutral, or negative mood.
we ﬁnd particularly interesting patterns in the autocorrect keys and symbolswhere keystrokes are quite indicative of mood..4186cance level (wilcoxon, 1992) between the timingsof positive and negative mood for each user/wordcombination to determine which words had sig-niﬁcantly different timings between positive andnegative mood..e negative results and future.
directions.
since this is a new dataset, we explored severalmore methods throughout the research process.
inthis section we describe some of the approachesthat yielded initial negative results despite themworking well for standard datasets:.
1. user speciﬁc models: we also explored the set-ting of training a separate model per user but wefound that there was too little data per user to train agood model.
as part of future work, we believe thatif ni-mlp can learn a user-independent classiﬁer,these representations can then be used for furtherﬁnetuning or few-shot learning on each speciﬁcuser.
previous work in federated learning (smithet al., 2017; liang et al., 2020b) offers ways oflearning a user-speciﬁc model that leverages otherusers’ data during training, which could help toalleviate the lack of data per user..2. user-independent data splits: we have shownthat text, keystrokes, and app usage features arehighly dependent on participant identities.
conse-quently, models trained on these features wouldperform poorly when evaluated on a user not foundin the training set.
we would like to evaluate ifbetter learning of user-independent features can im-prove generalization to new users (e.g., split thedata such that the ﬁrst 10 users are used for train-ing, next 3 for validation, and ﬁnal 4 for testing).
our initial results for these were negative, but webelieve that combining better privacy-preservingmethods that learn user-independent features couldhelp in this regard..3. fine-grained multimodal fusion: our ap-proach of combining modalities was only at theinput level (i.e., early fusion (baltrušaitis et al.,2018)) which can be improved upon by leverag-ing recent work in more ﬁne-grained fusion (lianget al., 2018).
one such example could be to aligneach keystroke feature and app data to the exacttext that was entered in, which provides more ﬁne-grained contextualization of text in keystroke andapp usage context..4187