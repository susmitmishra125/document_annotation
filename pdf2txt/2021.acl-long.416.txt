stereoset: measuring stereotypical bias in pretrained language models.
moin nadeem§ and anna bethke† and siva reddy‡§massachusetts institute of technology, cambridge ma, usa†facebook, menlo park ca, usa‡facebook cifar ai chair, mila; mcgill university, montreal, qc, canadamnadeem@mit.edu anna.bethke@intel.com,siva.reddy@mila.quebec.
abstract.
a stereotype is an over-generalized beliefabout a particular group of people, e.g., asiansare good at math or african americans areathletic.
such beliefs (biases) are known tohurt target groups.
since pretrained languagemodels are trained on large real world data,they are known to capture stereotypical biases.
it is important to quantify to what extent thesebiases are present in them.
although this is arapidly growing area of research, existing lit-erature lacks in two important aspects: 1) theymainly evaluate bias of pretrained languagemodels on a small set of artiﬁcial sentences,even though these models are trained on natu-ral data; 2) current evaluations focus on mea-suring bias without considering the languagemodeling ability of a model, which could leadto misleading trust on a model even if it is apoor language model.
we address both theseproblems.
we present stereoset, a large-scalenatural english dataset to measure stereotypi-cal biases in four domains: gender, profession,race, and religion.
we contrast both stereotyp-ical bias and language modeling ability of pop-ular models like bert, gpt2, roberta,and xlnet.
we show that these models ex-hibit strong stereotypical biases.
our data andcode are available at https://stereoset.
mit.edu..1.introduction.
a key idea behind the current success of neuralnetwork models for language is pretrained rep-resentations such as word embeddings (mikolovet al., 2013; pennington et al., 2014) and pre-trained language models (peters et al., 2018;howard and ruder, 2018; devlin et al., 2019; rad-ford et al., 2019; liu et al., 2019).
these arewidely used to initialize neural models, which arethen ﬁne-tuned to perform a task at hand.
typ-ically, these are learned from massive text cor-.
choose the appropriate word:.
domain: gender.
target: girl.
context: girls tend to be more ____ than boysoption 1: softoption 2: determinedoption 3: ﬁsh.
(stereotype)(anti-stereotype)(meaningless).
(a) the intrasentence context association test.
choose the appropriate sentence:.
domain: race.
target: arab.
context: he is an arab from the middle east.
option 1: he is probably a terrorist with bombs.
(stereotype)option 2: he is a paciﬁst.
(anti-stereotype)option 3: my dog wants a walk.
(meaningless).
(b) the intersentence context association test.
figure 1: context association tests (cats) measureboth bias and language modeling ability of languagemodels..pora using variants of language modeling objec-tive (i.e., predicting a word given its surroundingcontext).
in the recent years, these representationsempowered neural models to attain unprecedentedlevels of performance gains on multiple languagetasks.
the resulting models are being deployedwidely as services on platforms like google cloudand amazon aws to serve millions of users..while this growth is commendable, there areconcerns about the fairness of these models.
sincepretrained representations are obtained from learn-ing on massive text corpora, there is a danger thatstereotypical biases in the real world are reﬂectedin these models.
for example, gpt2 (radfordet al., 2019), a pretrained language model, hasshown to generate unpleasant stereotypical textwhen prompted with context containing certainraces such as african-americans (sheng et al.,2019).
in this work, we assess the stereotypicalbiases of popular pretrained language models..proceedingsofthe59thannualmeetingoftheassociationforcomputationallinguisticsandthe11thinternationaljointconferenceonnaturallanguageprocessing,pages5356–5371august1–6,2021.©2021associationforcomputationallinguistics5356the seminal works of bolukbasi et al.
(2016)and caliskan et al.
(2017) show that word embed-dings such as word2vec (mikolov et al., 2013) andglove (pennington et al., 2014) contain stereo-typical biases using diagnostic methods like wordanalogies and association tests.
for example,caliskan et al.
show that male names are morelikely to be associated with career terms than fe-male names where the association is measured us-ing embedding similarity..recently, studies have attempted to evaluatebias in contextual word embeddings where a wordis provided with artiﬁcial context (may et al.,2019; kurita et al., 2019), e.g., the contextual em-bedding of man is obtained from the embedding ofman in the sentence this is a man.
however, thesehave limitations.
first, the context does not reﬂectthe natural usage of a word.
second, they requirestereotypical attribute terms to be predeﬁned (e.g.,pleasant and unpleasant terms).
third, they focuson single word terms and ignore multiword termslike construction worker.
lastly, they study biasof a model independent of its language modelingability which could lead to undeserved trust in amodel if it is a poor language model..in this work, we propose methods to evaluatestereotypical bias of pretrained language models.
these methods do not have the aforementionedlimitations.
speciﬁcally, we design two differentassociation tests, one for measuring bias at sen-tence level (intrasentence), and the other at dis-course level (intersentence) as shown in figure 1..in these tests, each target term (e.g., arab) is pro-vided with a natural context in which it appears,along with three possible associative contexts.
theassociative contexts help us to evaluate the bi-ases of the model, as well as measure its languagemodeling performance.
we crowdsource stere-oset, a dataset for associative contexts in englishcontaining 4 target domains, 321 target terms and16,995 test instances (triplets)..2 task deﬁnition & formulation.
2.1 deﬁnition.
following previous literature (greenwald andbanaji, 1995; bolukbasi et al., 2016; caliskanet al., 2017), we deﬁne a stereotype as an over-generalized belief about a particular group of peo-ple, e.g., asians are good at math.
our primaryfocus is on detecting the presence of stereotypesin pretrained language models.
we leave the de-.
tails of mitigating bias from pretrained languagemodels to future work..2.2 formulation.
we design our formulation around the desiderataof an ideal language model.
an ideal languagemodel should be able to perform the task of lan-guage modeling, i.e., it should rank meaningfulcontexts higher than meaningless contexts.
for ex-ample, it should tell us that our housekeeper is amexican is more probable than our housekeeperis a banana.
second, it should not exhibit stereo-typical bias, i.e., it should avoid ranking stereo-typical contexts higher than anti-stereotypical con-texts, e.g., our housekeeper is a mexican and ourhousekeeper is an american should be equallypossible.
we desire equally possible instead ofanti-stereotype over stereotype because any kindof overgeneralized belief is known to hurt targetgroups (czopp et al., 2015).
if the model con-sistently prefers stereotypes over anti-stereotypes,we say that the model exhibits stereotypical bias.
another approach would be to rank a neutral con-text higher over stereotypical or anti-stereotypicalcontext.
in practice, we found that collecting neu-tral contexts are prone to implicit biases and haslow inter-annotator agreement (section 4)..based on these observations, we develop thecontext association test (cat), a test that mea-sures the language modeling ability as well as thestereotypical bias of pretrained language models.
although language modeling has standard evalua-tion metrics such as perplexity, due to varying vo-cabulary sizes of different pretrained models, thismetric becomes incomparable across models.
inorder to analyse the relationship between languagemodeling ability and stereotypical bias, we deﬁnea simple metric that is appropriate for our task.
evaluating the full language modeling ability ofmodels is beyond the scope of this work..in cat, given a context containing a targetgroup (e.g., housekeeper), we provide three dif-ferent ways to instantiate this context.
each in-stantiation corresponds to either a stereotypical,anti-stereotypical, or a meaningless association.
the stereotypical and anti-stereotypical associa-tions are used to measure stereotypical bias, andthe meaningless association is used to ensure thatan unbiased language model still retains languagemodeling ability.
we include the meaningless as-sociation in order to provide a standardized bench-.
5357mark across both masked and autoregressive lan-guage models, which cannot be done with com-mon metrics such as perplexity..speciﬁcally, we design two types of associationtests, intrasentence and intersentence cats, to as-sess language modeling and stereotypical bias atsentence level and discourse level.
figure 1 showsan example for each..2.3.intrasentence.
our intrasentence task measures the bias and thelanguage modeling ability at sentence-level.
wecreate a ﬁll-in-the-blank style context sentence de-scribing the target group, and a set of three at-tributes, which correspond to a stereotype, an anti-stereotype, and a meaningless option (figure 1a).
in order to measure language modeling and stereo-typical bias, we determine which attribute has thegreatest likelihood of ﬁlling the blank, i.e., whichof the instantiated contexts is more likely..2.4.intersentence.
our intersentence task measures the bias and thelanguage modeling ability at the discourse-level.
the ﬁrst sentence contains the target group, andthe second sentence contains an attribute of thetarget group.
figure 1b shows the intersentencetask.
we create a context sentence with a targetgroup that can be succeeded with three attributesentences corresponding to a stereotype, an anti-stereotype and a meaningless option.
we mea-sure the bias and language modeling ability basedon which attribute sentence is likely to follow thecontext sentence..3 related work.
our work is inspired from related attempts thataim to measure bias in pretrained representationssuch as word embeddings and language models..3.1 bias in word embeddings.
the two popular methods of testing bias in wordembeddings are word analogy tests and word as-sociation tests.
in word analogy tests, given twowords in a certain syntactic or semantic relation(man → king), the goal is generate a word thatis in similar relation to a given word (woman →queen).
mikolov et al.
(2013) showed that wordembeddings capture syntactic and semantic wordanalogies, e.g., gender, morphology etc.
boluk-basi et al.
(2016) build on this observation to study.
gender bias.
they show that word embeddingscapture several undesired gender biases (seman-tic relations) e.g.
doctor : man :: woman : nurse.
manzini et al.
(2019) extend this to show that wordembeddings capture several stereotypical biasessuch as racial and religious biases..in the word embedding association test (weat,caliskan et al.
2017), the association of two com-plementary classes of words, e.g., european andafrican names, with two other complementaryclasses of attributes that indicate bias, e.g., pleas-ant and unpleasant attributes, are studied to quan-tify the bias.
the bias is deﬁned as the differencein the degree with which european names are as-sociated with pleasant and unpleasant attributes incomparison with african names being associatedwith those attributes.
here, the association is de-ﬁned as the similarity between the name and at-tribute word embeddings.
this is the ﬁrst largescale study that showed word embeddings exhibitseveral stereotypical biases and not just genderbias.
our inspiration for cat comes from weat..3.2 bias in pretrained language models.
may et al.
(2019) extend weat to sentence en-coders, calling it the sentence encoder asso-ciation test (seat).
for a target term and itsattribute,they create artiﬁcial sentences usinggeneric context of the form "this is [target]."
and"they are [attribute]."
and obtain contextual wordembeddings of the target and the attribute terms.
they repeat caliskan et al.
(2017)’s study usingthese embeddings and cosine similarity as the as-sociation metric but their study was inconclusive.
later, kurita et al.
(2019) show that cosine simi-larity is not the best association metric and deﬁne anew association metric based on the probability ofpredicting an attribute given the target in genericsentential context, e.g., [target] is [mask], where[mask] is the attribute.
they show that similar ob-servations of caliskan et al.
(2017) are observedon contextual word embeddings too.
our intrasen-tence cat is similar to their setting but with nat-ural context.
we also go beyond intrasentence topropose intersentence cats, since language mod-eling is not limited at sentence level..concurrent to our work, nangia et al.
(2020)introduced crows-pairs, which examines stereo-typical bias via minimal pairs.
however, crows-pairs only studies bias within a single sentence(intrasentence) and ignores discourse-level (inter-.
5358sentence) measurements.
furthermore, stereosetcontains an order of magnitude of data that con-tains greater variety, and hence, has the potentialto detect a wider range of biases that may be other-wise overlooked.
lastly, stereoset measures biasacross both masked and autoregressive languagemodels, while crows-pairs only measures bias inmasked language models..3.3 measuring bias through extrinsic tasks.
another method to evaluate bias in pretrained rep-resentations is to measure bias on extrinsic taskslike coreference resolution (rudinger et al., 2018;zhao et al., 2018) and sentiment analysis (kir-itchenko and mohammad, 2018).
this methodﬁne-tunes pretrained representations on the targettask.
the bias in pretrained representations is es-timated by the target task’s performance.
how-ever, it is hard to segregate the bias of task-speciﬁctraining data from the pretrained representations.
our cats are an intrinsic way to evaluate bias inpretrained models..4 dataset creation.
in stereoset, we select four domains as the targetdomains of interest for measuring bias: gender,profession, race and religion.
for each domain,we select terms (e.g., asian) that represent a so-cial group.
for collecting target term contexts andtheir associative contexts, we employ crowdwork-ers via amazon mechanical turk.1 we restrictourselves to crowdworkers in usa since stereo-types could change based on the country.
table 1shows the overall statistics of stereoset.
we alsoprovide a full data statement in section 9 (benderand friedman, 2018)..4.1 target terms selection.
we curate diverse set of target terms for the tar-get domains using wikidata relation triples (vran-deˇci´c and krötzsch, 2014).
a wikidata triple is ofthe form <subject, relation, object> (e.g., <bradpitt, p106, actor>).
we collect all objects occur-ring with the relations p106 (profession), p172(race), and p140 (religion) as the target terms.
we manually ﬁlter terms that are either infrequentor too ﬁne-grained (assistant producer is mergedwith producer).
we collect gender terms from.
nosek et al.
(2002).
a list of target terms is avail-able in appendix a.1..4.2 cats collection.
in the intrasentence cat, for each target term,a crowdworker writes attribute terms that cor-respond to stereotypical, anti-stereotypical andmeaningless associations of the target term.
then,they provide a context sentence containing the tar-get term.
the context is a ﬁll-in-the-blank sen-tence, where the blank can be ﬁlled either by thestereotype term or the anti-stereotype term but notthe meaningless term..term..in the intersentence cat, they ﬁrst providea sentence containing the targetthen,they provide three associative sentences corre-sponding to stereotypical, anti-stereotypical andmeaningless associations.
these associative sen-tences are such that the stereotypical and the anti-stereotypical sentences can follow the target termsentence but the meaningless ones cannot followthe target term sentence..we also experimented with a variant that askedcrowdworkers to provide a neutral association forthe target term, but found that crowdworkers hadsigniﬁcant trouble remaining neutral.
in the val-idation step (next section), we found that manyof these neutral associations are often classiﬁedas stereotype or anti-stereotype by multiple val-idators.
we conjecture that attaining neutrality ishard is due to anchoring bias (tversky and kah-neman, 1974), i.e., stereotypical associations areeasy to think and access and could implicitly affectcrowdworkers to tilt towards them.
therefore, wediscard the notion of neutrality.
some examplesare shown in appendix a.4..4.3 cats validation and human agreement.
in order to ensure that stereotypes reﬂect com-mon views, we validate the data collected in theabove step with additional workers.
for each con-text and its associations, we ask ﬁve validatorsto classify each association into a stereotype, ananti-stereotype or a meaningless association.
weonly retain cats where at least three validatorsagree on the labels.2 this ﬁltering results in se-lecting 83% of the cats, indicating that there isregularity in stereotypical views among the work-ers.
table 10 shows detailed agreement scores for.
1screenshots of our mechanical turk interface and details.
2one can increase the quality of the data further by select-.
about task setup are available in the section 9.6..ing examples where four or more workers agree upon..5359domain.
# targetterms.
# cats(triplets).
avg len(# words).
intrasentence.
positive negative.
stereotypeanti-stereotype.
59%67%.
41%33%.
genderprofessionracereligiontotal.
genderprofessionracereligiontotal.
overall.
intersentence.
4012014912321.
4012014912321.
321.
1,0263,2083,9966238,498.
9963,2693,9896048,497.
16,995.table 1: statistics of stereoset.
7.988.307.638.188.02.
15.5516.0514.9814.9915.39.
11.70.stereotypes computed using the average of anno-tator agreement per example..4.4 dataset analysis.
are people prone to view stereotypes negatively?
to answer this question, we classify stereotypesinto positive and negative sentiment classes usinga sentiment classiﬁer (details in appendix a.2).
in table 2, people do not alwaysas evidentassociate stereotypes with negative associations(e.g., asians are good at math has positive senti-ment).
however, people associate stereotypes withrelatively more negative associations than anti-stereotypes (41% vs. 33%)..we also extract keywords in stereoset to ana-lyze which words are most commonly associatedwith target groups.
we deﬁne a keyword as a wordthat is more frequent in stereoset than the naturaldistribution of words (kilgarriff, 2009; jakubiceket al., 2013).
table 3 shows the top keywords ofeach domain.
these keywords indicate that targetterms in gender and race are associated with phys-ical attributes such as beautiful, feminine, mascu-line, etc., professional terms are associated withbehavioural attributes such as pushy, greedy, hard-work, etc., and religious terms are associated withbelief attributes such as diety, forgiving, reborn,etc.
this aligns with expectations and indicatesthat multiple annotators use similar attributes..table 2: percentage of positive and negative sentimentinstances in stereoset.
stepchilduncarefemininepolite.
masculinebreadwinnerrowdystudious.
bossyimmaturepossessivehomemaker.
managgymanlyburly.
nerdypushyrudedisorganize.
uneducatedunintelligentsnobbytalkative.
bossystudiousgreedyuptight.
hardworkdumbsloppydishonest.
poorsnobbyindustriousimpoverish.
beautifulimmigratewealthylazy.
uneducatedwartorndangerousturban.
smellyrudeaccentscammer.
gender.
profession.
race.
religion.
commandmentjudgmentalclassistatheist.
hinduismdietyforgivingmonotheistic.
saviorpeacefulterroristcoworker.
hijabunholyreborndevout.
table 3: the keywords that characterize each domain..5 experimental setup.
in this section, we describe the data splits, evalua-tion metrics and the baselines..5.1 development and test sets.
we split stereoset based on the target terms: 25%of the target terms and their instances for the de-velopment set and 75% for the test set.
we ensureterms in the development set and test set are dis-joint.
we do not have a training set since this de-feats the purpose of stereoset, which is to measurethe biases of pretrained language models (and notthe models ﬁne-tuned on stereoset)..5.2 evaluation metrics.
our desiderata of an ideal language model is thatit excels at language modeling while not exhibit-ing stereotypical biases.
in order to determine suc-cess at both these goals, we evaluate both languagemodeling and stereotypical bias of a given model.
we pose both problems as ranking problems..5360language modeling score (lms)in the lan-guage modeling case, given a target term contextand two possible associations of the context, onemeaningful and the other meaningless, the modelhas to rank the meaningful association higher thanmeaningless association.
the meaningful associ-ation corresponds to either the stereotype or theanti-stereotype option..we deﬁne the language modeling score (lms)of a target term as the percentage of instances inwhich a language model prefers the meaningfulover meaningless association.
we deﬁne the over-all lms of a dataset as the average lms of the tar-get terms in the split.
the lms of an ideal lan-guage model is 100, i.e., for every target term in adataset, the model always prefers the meaningfulassociation of the term..as discussed in section 2.2, the goal of this met-ric is not to evaluate the full scale language model-ing ability, but only to provide an reasonable met-ric that allows comparison between different mod-els to analyze the relationship between languagemodeling ability and stereotypical bias..stereotype score (ss) similarly, we deﬁne thestereotype score (ss) of a target term as the per-centage of examples in which a model prefers astereotypical association over an anti-stereotypicalassociation.
we deﬁne the overall ss of a datasetas the average ss of the target terms in the dataset.
the ss of an ideal language model is 50, for everytarget term, the model prefers neither stereotypicalassociations nor anti-stereotypical associations..idealized cat score (icat) stereoset moti-vates a question around how practitioners shouldprefer models for real-world deployment.
just be-cause a model has low stereotypical bias does notmean it is preferred over others.
for example,although a random language model exhibits thelowest stereotypical bias (ss = 50) it is the worstlanguage model (lms = 50).
while model se-lection desiderata is often task-speciﬁc, we intro-duce a simple point-estimate called the idealizedcat (icat) score for model comparison assum-ing equal importance to language modeling abilityand stereotypical bias.
we deﬁne the icat scoreas lms ∗ min(ss,100−ss)centered around the ideathat an ideal language model has an icat scoreof 100 and a stereotyped model has a score of 0.appendix a.6 presents a detailed formulation andfigure 2 (appendix) highlights this idea..50.
5.3 baselines.
ideallm we deﬁne this hypothetical model asthe one that always picks correct associations for agiven target term context.
it also picks equal num-ber of stereotypical and anti-stereotypical associ-ations over all the target terms.
so the resultinglms and ss scores are 100 and 50 respectively..stereotypedlm we deﬁne this hypotheticalmodel as the one that always picks a stereotypicalassociation over an anti-stereotypical association.
so its ss is 100 irrespective of its lms..randomlm we deﬁne this model as the onethat picks associations randomly, and therefore itslms and ss scores are both 50..sentimentlm in section 4.4, we saw thatstereotypical instantiations are more frequentlyassociated with negative sentimentthan anti-in this baseline, we assess if senti-stereotypes.
ment can be used to detect a stereotypical associa-tion.
for a given a pair of context associations, themodel always picks the association with the mostnegative sentiment..6 main experiments.
in this section, we evaluate pretrained models suchas bert (devlin et al., 2019), roberta (liuet al., 2019), xlnet (yang et al., 2019) andgpt2 (radford et al., 2019) on stereoset..6.1 masked language models.
while scoring sentences using autoregressive lan-guage models is well-deﬁned, there is no corre-sponding scoring mechanism for masked languagemodels.
as a result, we evaluate our modelsusing both likelihood-based scoring and psuedo-likelihood scoring (nangia et al., 2020)..likelihood-based scoring for intrasentencecats, we deﬁne the score as the log probabilityof an attribute term to ﬁll the blank.
if the attributeconsists of multiple subwords, we iteratively un-mask the subwords from left to right, and computethe average per-subword probability.
we rank agiven pair of attribute terms based on these prob-abilities (the one with higher probability is pre-ferred).
in intersentence cats, inspired by devlinet al.
(2019), we use a next sentence prediction(nsp) task to rank the possible associations.
forall models, we train identical next sentence pre-diction heads on identical datasets (details given.
5361in appendix a.5), and compute the log likelihoodthat any given target sentence follows the context.
given a pair of associations, we rank each associ-ation using this score..psuedo-likelihood scoring nangia et al.
(2020) adopts psuedo-likelihood based scoring(salazar et al., 2020) that does not penalize lessfrequent attribute terms.
in intrasentence cat,we choose to never mask the attribute term butmask each context term one at a time and mea-sure the psuedo-probability of the sentence giventhe attribute term.
we refer the reader to nangiaet al.
(2020) for more information on this scor-ing mechanism.
in intersentence cats, we mea-sure the psuedolikelihood of the context sentenceconditioned on the attribute sentence by iterativelymasking the tokens in the context sentence whilekeeping the attribute sentence unchanged..6.2 autoregressive language models.
unlike above models, gpt2 is a generative modelin an auto-regressive setting.
for the intrasen-tence cat, we instantiate the blank with an at-tribute term and compute the probability of the fullsentence.
given a pair of associations, we rankeach association using this score.
for the inter-sentence cat, our scoring mechanism mirrors thatfor masked language models.
if the likelihood-based scoring mechanism is used, then we trainan nsp head on identical datasets (details givenin appendix a.5) and compute the log likelihoodthat any given target sentence follows the context.
if the masked language models are scored withpsuedo-likelihood, then we measure the effect ofthe context sentence by measuring the joint prob-ability of the attribute sentence with and withoutthe context.
given a pair of associations, we rankeach association by the ratio of these probabilities..7 results and discussion.
table 4 shows the overall results of baselinesand models on stereosettest set when usinglikelihood-based scoring, and table 5 shows theresults when using psuedo-likelihood based scor-ing.
the results exhibit similar trends on the de-velopment and test sets.
since the initial versionof this paper3 used likelihood-based scoring, wemainly center the discussion around it as the trendsare similar to pseudo-likelihood..3apr 2020 arxiv:2004.09456.model.
languagemodelscore(lms).
test set.
stereotypescore(ss).
idealizedcatscore(icat).
ideallmstereotypedlm -randomlmsentimentlm.
bert-basebert-large.
roberta-baseroberta-large.
xlnet-basexlnet-large.
gpt2gpt2-mediumgpt2-large.
ensemble.
100.
50.065.1.
85.485.8.
68.275.8.
67.778.2.
83.685.988.3.
90.2.
50.010050.060.8.
58.359.2.
50.554.8.
54.154.0.
56.458.260.0.
62.3.
1000.050.051.1.
71.269.9.
67.568.5.
62.172.0.
73.071.770.5.
68.0.table 4: performance of pretrained language models onthe stereoset test set, measured using likelihood-basedscoring for the masked language models..baselines vs. models as seen in table 4,all pretrained models have higher lms valuesthan randomlm indicating that these are bet-ter language models as expected.
among mod-els, gpt2-large is the best performing languagemodel (88.3) followed by gpt2-medium (85.9).
coming to stereotypical bias, all pretrainedmodels demonstrate more stereotypical behav-ior than randomlm.
while gpt2-large is themost stereotypical model of all pretrained mod-els (60.1), roberta-base is the least stereotyp-ical model (50.5).
sentimentlm achieves thehighest stereotypical score compared to all pre-trained models, indicating that sentiment can in-deed be exploited to detect stereotypical asso-ciations.
however, its language model perfor-mance is worse, which is expected, since senti-ment alone isn’t sufﬁcient to distinguish meaning-ful and meaningless sentences..relation between lms and ss all models ex-hibit a strong correlation between lms and ss(spearman rank correlation ρ of 0.87).
as thelanguage model becomes stronger, its stereotypi-cal bias (ss) does too.
we build the strongest lan-guage model, ensemble, using a linear weightedcombination of bert-large, gpt2-medium, andgpt2-large, which is also found to be the mostbiased model (ss = 62.5).
the correlation be-tween lms and ss is unfortunate and perhaps un-.
5362model.
model.
languagemodelscore(lms).
test set.
stereotypescore(ss).
idealizedcatscore(icat).
languagemodelscore(lms).
stereotypescore(ss).
idealizedcatscore(icat).
intrasentence task.
ideallmstereotypedlm -randomlmsentimentlm.
bert-basebert-large.
roberta-baseroberta-large.
xlnet-basexlnet-large.
gpt2gpt2-mediumgpt2-large.
ensemble.
100.
50.065.1.
82.381.1.
83.583.4.
60.561.3.
86.888.689.6.
90.1.
50.010050.060.8.
57.158.0.
58.559.8.
52.454.0.
59.061.662.7.
62.2.
1000.050.051.1.
70.768.1.
69.467.0.
57.656.5.
71.168.066.8.
68.1.table 5: performance of pretrained language mod-els on the stereoset test set, measured using psuedo-likelihood scoring for the masked language models..avoidable as long as we rely on the real worlddistribution of corpora to train language modelssince these corpora are likely to reﬂect stereo-types.
amongst the models, gpt2 exhibits moreunbiased behavior than other models (icat scoreof 73.0).
however, this metric is not intended asthe sole criterion for model selection.
further re-search is required in designing better metrics..impact of model size for a given architecture,all of its pretrained models are trained on the samecorpora but with different number of parameters.
for example, both bert-base and bert-largeare trained on wikipedia and bookcorpus (zhuet al., 2015) with 110m and 340m parameters re-spectively.
as the model size increases, we seethat its language modeling ability (lms) increases,and correspondingly its stereotypical score..impact of scoring mechanism we evaluatemodels using both likelihood based scoring andpsuedo-likelihood based scoring.
first, we notethat likelihood-based (ll) scoring is higher thanpsuedo-likelihood-based (pll) scoring by a narrowmargin (avg lmsll = 79.88, avg lmspll = 79.68).
for intrasentence cats, psuedo-likelihood out-performs likelihood scoring by a wide margin(avg lmsll = 75.7, avg lmspll = 79.4).
how-ever, psuedo-likelihood scoring is signiﬁcantlydegraded for intersentence cats (avg lmsll =.
bert-basebert-large.
roberta-baseroberta-large.
xlnet-basexlnet-large.
gpt2gpt2-mediumgpt2-large.
ensemble.
bert-basebert-large.
roberta-baseroberta-large.
xlnet-basexlnet-large.
gpt2gpt2-mediumgpt2-large.
ensemble.
82.582.9.
71.972.7.
70.374.0.
91.091.291.8.
91.7.
88.388.7.
64.478.8.
65.082.5.
76.380.584.9.
89.4.
57.557.6.
53.654.4.
53.651.8.
60.462.963.9.
63.9.
61.760.6.
47.455.2.
54.656.1.
52.353.556.1.
60.9.
70.270.3.
66.766.3.
65.271.3.
72.067.766.2.
66.3.
67.671.0.
61.070.6.
59.072.5.
72.874.974.5.
69.9.intersentence task.
table 6: performance on the intersentence and in-trasentence cats on the stereoset test set, measuredusing likelihood-based scoring..78.82, avg lmspll = 75.98).
this suggests thatpsuedo-likelihood has trouble scoring longer se-quences.
moreover, aribandi et al.
(2021) hasshown that psuedo-likelihood has higher variancethan likelihood scoring..of.
pretraining.
impactcorpora bert,roberta, xlnet and gpt2 are trained on16gb, 160gb, 158gb and 40gb of text corpora.
surprisingly, the corpora size does not correlatewith either lms or ss.
this could be due to thedifferences in architectures and corpora types.
a better way to verify this would be to train thesame model on increasing amounts of corpora.
due to lack of computing resources, we leave thiswork for the community.
we conjecture that thehigh performance of gpt2 (high lms and highss) is due to the nature of its training data.
gpt2is trained on documents linked from reddit.
sincereddit has several subreddits related to targetterms in stereoset (e.g., relationships, religion),gpt2 is likely to be exposed to contextual.
5363model.
domain.
languagemodelscore(lms).
stereotypescore(ss).
idealizedcatscore(icat).
intrasentence task.
languagemodelscore(lms).
stereotypescore(ss).
idealizedcatscore(icat).
bert-basebert-large.
roberta-baseroberta-large.
xlnet-basexlnet-large.
gpt2gpt2-mediumgpt2-large.
ensemble.
bert-basebert-large.
roberta-baseroberta-large.
xlnet-basexlnet-large.
gpt2gpt2-mediumgpt2-large.
ensemble.
89.688.8.
88.088.1.
60.661.1.
91.091.291.8.
91.9.
75.073.3.
79.178.7.
60.461.4.
82.585.987.5.
89.1.
56.958.4.
58.559.6.
51.353.2.
60.462.963.9.
63.9.
57.257.6.
58.460.0.
53.554.7.
57.660.361.5.
61.1.intersentence task.
77.374.0.
73.071.2.
59.057.3.
72.067.766.2.
66.3.
64.162.1.
65.963.1.
56.255.7.
70.068.367.3.
69.9.table 7: performance on the intersentence and in-trasentence cats on the stereoset test set, measuredusing psuedo-likelihood scoring..associations that contain real-world bias..domain-wise bias table 8 shows domain-wiseresults of the ensemble model on the test set.
the model is relatively less biased on race thanon others (ss = 61.8).
we also show the mostand least biased target terms for each domain fromthe development set (see table 10 for human-agreement scores, a proxy for most and least bi-ased terms).
we conjecture that the most biasedterms are those that have well established stereo-types and are also frequent in language.
this isthe case with mother (attributes: caring, cooking),software developer (attributes: geek, nerd), andafrica (attributes: poor, dark).
the least biased arethose that do not have well established stereotypes,for example, producer and crimean.
the outlieris muslim, although it has established stereotypesindicated by the high human agreement (see ta-ble 10).
this requires further investigation..intrasentence vs intersentence cats table 6shows the results of intrasentence and intersen-.
gendermothergrandfather.
professionsoftware developerproducer.
raceafricancrimean.
religionbiblemuslim.
92.497.296.2.
88.894.091.7.
91.291.893.3.
93.585.094.8.
63.977.852.8.
62.675.953.7.
61.874.550.0.
63.866.046.6.
66.743.290.8.
66.545.484.9.
69.746.793.3.
67.757.888.3.table 8: domain-wise scores of the ensemble model,along with most and least stereotyped terms, measuredusing likelihood-based scoring..tence cats on the test set.
since intersentencetasks has more number of words per instance, weexpect intersentence language modeling task to beharder than intrasentence, especially results com-puted using psuedo-likelihood (table 7)..8 conclusions.
in this work, we develop the context associationtest (cat) to measure the stereotypical biases ofpretrained language models in contrast with theirlanguage modeling ability.
we crowdsource stere-oset, a dataset containing 16,995 cats to test bi-ases in four domains: gender, profession, race andreligion.
we show that current pretrained languagemodels exhibit strong stereotypical biases.
wealso ﬁnd that language modeling ability correlateswith the degree of stereotypical bias.
this depen-dence has to be broken if we are to achieve unbi-ased language models..we hope that stereoset will spur further re-search in evaluating and mitigating bias in lan-guage models.
we also note that achieving anideal performance on stereoset does not guaranteethat a model is unbiased since bias can manifest inmany ways (gonen and goldberg, 2019; benderet al., 2021)..acknowledgments.
we would like to thank the anonymous reviewers,yonatan belinkov, vivek kulkarni, and spandanagella for their helpful comments in reviewing thispaper.
this work was completed in part while mnand ab were at intel ai..5364references.
vamsi aribandi, yi tay, and donald metzler.
2021.in proceed-.
how reliable are model diagnostics?
ings of acl findings..emily m. bender and batya friedman.
2018. datastatements for natural language processing: towardmitigating system bias and enabling better science.
transactions of the association for computationallinguistics, 6:587–604..emily m bender, timnit gebru, angelina mcmillan-major, and shmargaret shmitchell.
2021. on thedangers of stochastic parrots: can language modelsbe too big?
in proceedings of facct..tolga bolukbasi, kai-wei chang, james y. zou,venkatesh saligrama, and adam t. kalai.
2016.man is to computer programmer as woman is tohomemaker?
debiasing word embeddings.
in pro-ceedings of neural information processing systems(neurips), pages 4349–4357..aylin caliskan,.
and arvindjoanna j. bryson,narayanan.
2017. semantics derived automaticallyfrom language corpora contain human-like biases.
science, 356(6334):183–186..patricia hill collins.
2004..black sexual politics:african americans, gender, and the new racism.
routledge..alexander m czopp, aaron c kay, and sapnacheryan.
2015. positive stereotypes are pervasiveand powerful.
perspectives on psychological sci-ence, 10(4):451–463..jacob devlin, ming-wei chang, kenton lee, andkristina toutanova.
2019. bert: pre-training ofdeep bidirectional transformers for language under-standing.
in proceedings of north american chap-ter of the association for computational linguistics,pages 4171–4186, minneapolis, minnesota.
associ-ation for computational linguistics..djellel difallah, elena filatova, and panos ipeirotis.
2018. demographics and dynamics of mechanicalturk workers.
in proceedings of the acm interna-tional conference on web search and data mining,wsdm ’18, pages 135 – 143, new york, ny, usa.
association for computing machinery..hila gonen and yoav goldberg.
2019. lipstick on apig: debiasing methods cover up systematic gen-der biases in word embeddings but do not removethem..anthony g. greenwald and mahzarin r. banaji.
1995.implicit social cognition: attitudes, self-esteem, andstereotypes.
psychological review, 102(1):4..jeremy howard and sebastian ruder.
2018. universallanguage model fine-tuning for text classiﬁcation.
in proceedings of the association for computationallinguistics, pages 328–339, melbourne, australia.
association for computational linguistics..milos jakubicek, adam kilgarriff, vojtech kovar,pavel rychly, and vit suchomel.
2013. the tentencorpus family.
in proceedings of the internationalcorpus linguistics conference cl..adam kilgarriff.
2009. simple maths for keywords.
inproceedings of the corpus linguistics conference2009 (cl2009), page 171..svetlana kiritchenko and saif mohammad.
2018. ex-amining gender and race bias in two hundredin proceedings ofsentiment analysis systems.
joint conference on lexical and computational se-mantics, pages 43–53..keita kurita, nidhi vyas, ayush pareek, alan wblack, and yulia tsvetkov.
2019. measuring biasin contextualized word representations.
in proceed-ings of the first workshop on gender bias in natu-ral language processing, pages 166–172, florence,italy.
association for computational linguistics..yinhan liu, myle ott, naman goyal, jingfei du, man-dar joshi, danqi chen, omer levy, mike lewis,luke zettlemoyer, and veselin stoyanov.
2019.roberta: a robustly optimized bert pretraining ap-proach.
arxiv preprint arxiv:1907.11692..andrew l. maas, raymond e. daly, peter t. pham,dan huang, andrew y. ng, and christopher potts.
2011. learning word vectors for sentiment analysis.
in proceedings of the association for computationallinguistics, pages 142–150, portland, oregon, usa.
association for computational linguistics..thomas manzini, lim yao chong, alan w black, andyulia tsvetkov.
2019. black is to criminal as cau-casian is to police: detecting and removing mul-in proceedingsticlass bias in word embeddings.
of the north american chapter of the associationfor computational linguistics, pages 615–621, min-neapolis, minnesota.
association for computationallinguistics..chandler may, alex wang, shikha bordia, samuel r.bowman, and rachel rudinger.
2019. on measur-ing social biases in sentence encoders.
in proceed-ings of the north american chapter of the associa-tion for computational linguistics, pages 622–628,minneapolis, minnesota.
association for computa-tional linguistics..tomas mikolov, ilya sutskever, kai chen, greg cor-rado, and jeffrey dean.
2013. distributed represen-tations of words and phrases and their composition-in proceedings of neural information pro-ality.
cessing systems (neurips), nips 13, pages 3111 –3119, red hook, ny, usa.
curran associates inc..nikita nangia, clara vania, rasika bhalerao, andsamuel r. bowman.
2020.crows-pairs: achallenge dataset for measuring social biases inin proceedings of em-masked language models.
pirical methods in natural language processing(emnlp), pages 1953–1967, online.
associationfor computational linguistics..5365methods.
in proceedings of north american chap-ter of the association for computational linguistics,pages 15–20..yukun zhu, ryan kiros, rich zemel, ruslan salakhut-dinov, raquel urtasun, antonio torralba, and sanjafidler.
2015. aligning books and movies: towardsstory-like visual explanations by watching moviesand reading books.
in proceedings of the ieee in-ternational conference on computer vision (iccv),iccv 15, pages 19 – 27, usa.
ieee computer so-ciety..brian nosek, mahzarin banaji, and anthony green-wald.
2002. math = male, me = female, thereforemath != me.
journal of personality and social psy-chology, 83:44–59..jeffrey pennington, richard socher, and christo-pher d. manning.
2014. glove: global vectorsin proceedings of em-for word representation.
pirical methods in natural language processing(emnlp), pages 1532–1543..matthew peters, mark neumann, mohit iyyer, mattgardner, christopher clark, kenton lee, and lukezettlemoyer.
2018. deep contextualized word rep-resentations.
in proceedings of the north americanchapter of the association for computational lin-guistics), pages 2227–2237.
association for com-putational linguistics..alec radford, jeffrey wu, rewon child, david luan,dario amodei, and ilya sutskever.
2019. languagemodels are unsupervised multitask learners.
openaiblog, 1(8)..rachel rudinger, jason naradowsky, brian leonard,and benjamin van durme.
2018. gender bias inin proceedings of northcoreference resolution.
american chapter of the association for computa-tional linguistics (naacl), pages 8–14..julian salazar, davis liang, toan q. nguyen, and ka-trin kirchhoff.
2020. masked language model scor-in proceedings of the of the association foring.
computational linguistics, online.
association forcomputational linguistics..emily sheng, kai-wei chang, premkumar natara-jan, and nanyun peng.
2019. the woman workedas a babysitter: on biases in language genera-in proceedings of the empirical methods intion.
natural language processing and the internationaljoint conference on natural language processing(emnlp-ijcnlp), pages 3407–3412, hong kong,china.
association for computational linguistics..amos tversky and daniel kahneman.
1974. judgmentunder uncertainty: heuristics and biases.
science,185(4157):1124–1131..denny vrandeˇci´c and markus krötzsch.
2014. wiki-data: a free collaborative knowledgebase.
com-mun.
acm, 57(10):78–85..zhilin yang, zihang dai, yiming yang, jaime car-bonell, russ r salakhutdinov, and quoc v le.
2019. xlnet: generalized autoregressive pretrain-ing for language understanding.
in h. wallach,h. larochelle, a. beygelzimer, f. d’e buc, e. fox,and r. garnett, editors, proceedings of neural infor-mation processing systems (neurips), pages 5753–5763. curran associates, inc..jieyu zhao, tianlu wang, mark yatskar, vicente or-donez, and kai-wei chang.
2018. gender bias incoreference resolution: evaluation and debiasing.
5366speaker to stereotype and anti-stereotype a giventarget word.
we informed them that their workwould be used for a scientiﬁc study and they wereencouraged to explicitly stereotype target groups..9.4 text characteristics.
stereoset measures stereotypical biases in gen-der, profession, race, and religion.
the intrasen-tence task lends itself to a "ﬁll-in-the-blank" na-ture, while the intersentence task asks annotatorsto contextualize a pair of sentences..9.5 recording quality.
the data was only written, and never recorded..9.6.interface.
our mechanical turk interface is shown in fig-ure 3 and figure 4 for the intrasentence and inter-sentence tasks respectively..9 ethics and data statement.
following bender and friedman (2018), we pro-vide the following ethics and data statement..9.1 curation rationale.
stereoset is a crowdsourced dataset that was cre-ated as a benchmark for stereotypical biases inpretrained language models.
this dataset consistsof 4 target domains, 321 target terms, and 16,995test instances.
stereoset is in english and is tai-lored for the stereotypes that exist in the unitedstates.
the data was explicitly curated with agoal of creating a set of stereotypical and anti-stereotypical examples..each example in the dataset consists of a triple.
each triple consists of a target context, with a cor-responding stereotypical, anti-stereotypical, or un-related association that stereotypes the target orcombats stereotypes about the target..we collected this data via amazon mechani-cal turk (amt), where each example was writtenby one crowdworker and validated by four othercrowdworkers.
we required all crowdworkers tobe in the united states and have a hit acceptancerate greater than 97%.
we paid all workers with aminimum wage of $15 an hour in compliance withour funding agencies’ amt policy..9.2 language variety.
we require crowdworkers to be within the unitedstates, and all examples are written in us english(en-us).
however, we do not enforce any con-straints on, nor do we collect, the dialect that isused..9.3 annotator demographics.
our annotators came from amazon mechanicalturk (amt), and we provided no ﬁlters beyondthe 97% hit acceptance rate.
in total, 475 and 803annotators completed the intrasentence and inter-sentence tasks respectively.
difallah et al.
(2018)shows that the amazon mechanical turk popula-tion is 55% women and 45% men, with 80% ofthe populous under the age of 50. the median in-come of workers on amt is $47k; in contrast, theunited states has a median income of $57k..9.3.1 speech situation.
all text was written in english, and was neveredited after the speaker wrote it.
the time andplace were unconstrained.
we prompted the.
5367a appendix.
a.1 list of target words.
table 10 list our target terms used in the datasetcollection task..a.2 fine-tuning bert for sentiment.
analysis.
in order to evaluate sentiment, we ﬁne-tune bert(devlin et al., 2019) on movie reviews (maas et al.,2011) for seven epochs.
we used a maximum se-quence length of 256 wordpieces, batch size 32,and used adam with a learning rate of 1e−4.
ourﬁne-tuned model achieves an 92% test accuracy onthe large movie review dataset.
it is important tonote that the classiﬁer also classiﬁes neutral sen-timent such as my housekeeper is a mexican aspositive..a.3 reproducibility checklist.
we used 2 rtx 2080 ti to ﬁne-tune all of themodels for the next sentence prediction task, al-though this could have been done with one gpuas well.
the only exception to this is gpt2-large, which required 2 tesla v100s (with 32gbof ram) to ﬁne-tune.
we will release all pre-trained models to encourage reproducibility, andeach model’s biases is able to be evaluated within5 minutes..since we are using standardized models, thesizes of every single model is available online.
forconvenience, we report it here:.
2. bert-large: 340m parameters.
3. gpt2-small: 117m parameters.
4. gpt2-medium: 345m parameters.
5. gpt2-large: 774m parameters.
6. roberta-base: 125m parameters.
7. roberta-large: 355m parameters.
8. xlnet-base: 110m parameters.
9. xlnet-large: 340m parameters.
targetethiopia.
neutral sentencethe ethiopian woman had neverbeen so angry.
chess player the chess player was bearded.
schoolgirls are so cute in everyschoolgirlmovie i watch.
people from afghanistan arecurious.
a tailor is relaxed when talkingto me..afghanistan.
tailor.
table 9: a collection of neutral associations fromcrowdworkers..a.4 collecting neutral associations.
we also experimented with testing whether mod-els endorse a contextually neutral associationover the stereotypical or anti-stereotypical con-text.
however, we found that crowd workers hada poor inter-annotator agreement when asked towrite neutral continuations.
table 9 highlightssome collected sentences that were neutral..many of the contextually neutral associations intable 9 show highly stereotypical behavior.
forinstance, "the ethiopian woman had never beenso angry" characterizes the angry black womanstereotype (collins, 2004).
furthermore, we holdthat some of these neutral sentences aren’t trulythe chess player was bearded may in-neutral;advertently conceal stereotypes, since both chessplayers and bearded men are commonly seen aswise.
hence, a model may endorse a neutral sen-tence for the wrong reasons..sentence prediction head.
given some context c, and some sentence s, ourintersentence task requires calculating the likeli-hood p(s|c), for some sentence s and context sen-tence c..while bert has been trained with a nextsentence prediction classiﬁcation head to providep(s|c), the other models have not.
in this section,we detail our creation of a next sentence predic-tion classiﬁcation head as a downstream task..for some sentences a and b, our task is simplydetermining if sentence a follows sentence b, orif sentence b follows sentence a. we triviallygenerate this corpus from wikipedia by samplingsome ith sentence, i + 1th sentence, and a ran-domly chosen negative sentence from any other.
1. bert-base: 110m parameters.
a.5 general methods for training a next.
5368figure 2: the idealized cat score (icat) highlightsthe possible trade-offs between the language modelingscore (lms) and the stereotype score (ss).
an ideallanguage model achieves an icat score of 100 (i.e.,lms=100 and ss=50)..therefore we deﬁne icat score as.
icat = lms ∗.
min(ss, 100 − ss)50.
∈ [0, 1].
this equation satisﬁes allmin(ss,100−ss)50.the axioms.
hereis maximized whenthe model prefers neither stereotypes nor anti-stereotypes for each targetterm and is mini-mized when the model favours one over the other.
we scale this value using the language modelingscore.
an interpretation of icat is that it repre-sents the language modeling ability of a model tobehave in an unbiased manner while excelling atlanguage modeling..figure 2 depicts the values that the icat score.
may take on..article.
we maintain a maximum sequence lengthof 256 tokens, and our training set consists of 9.5million examples..we train with a batch size of 80 sequences untilconvergence (80 sequences / batch * 256 tokens/ sequence = 20,480 tokens/batch) for 10 epochsover the corpus.
for bert, we use bertadam asthe optimizer, with a learning rate of 1e-5, a linearwarmup schedule from 50 steps to 500 steps, andminimize cross entropy for our loss function.
ourresults are comparable to devlin et al.
(2019), witheach model obtaining 93-98% accuracy against thetest set of 3.5 million examples..additional models maintain the same experi-mental details.
our nsp classiﬁer achieves an94.6% accuracy with roberta-base, a 97.1%accuracy with roberta-large, a 93.4% accuracywith xlnet-base and 94.1% accuracy with xl-net-large..in order to evaluate gpt-2 on intersentencetasks, we feed the mean-pooled representationsacross the entire sequence length into the clas-siﬁcation head.
our nsp classiﬁer obtainsa 92.5% accuracy on gpt2-small, 94.2% ongpt2-medium, and 96.1% on gpt2-large.
in or-der to ﬁne-tune gpt2-large on our machines, weutilized gradient accumulation with a step size of10, and mixed precision training from apex..a.6 motivating the icat score.
to address situations where a point estimate thatcombines lms and ss is required (ie.
ranking mod-els), we develop the idealized cat (icat) score.
we recognize that various applications have differ-ent trade-offs between fairness and accuracy.
weaddress a generic case where accuracy and fairnessare equally important.
we derive the icat scorefrom the following axioms:.
• an ideal model has an icat score of 100, i.e.,when its lms is 100 and ss is 50, its icatscore is 100..• a fully biased model has an icat score of 0,i.e., when its ss is either 100 (always prefera stereotype over an anti-stereotype) or 0 (al-ways prefer an anti-stereotype over a stereo-type), its icat score is 0..• a random model has an icat score of 50, i.e.,when its lms is 50 and ss is 50, its icat scoremust be 50..536950525456586062646668707274767880828486889092949698100lm score04812162024283236404448525660646872768084889296100stereotype scorecharacterizing the icat score020406080100icat scorefigure 3: a screenshot of our intrasentence task collection interface..figure 4: a screenshot of our intersentence task collection interface..5370female (0.84)man (0.8)stepmother (0.78)woman (0.78)he (0.75)stepfather (0.74)him (0.72)daddy (0.71).
janitor (0.9)football player (0.85)cook (0.85)scientist (0.84)construction worker (0.83)hairdresser (0.82)engineer (0.82)coach (0.81)handball player (0.81)midwife (0.8)photographer (0.8)architect (0.8)real-estate developer (0.79)illustrator (0.79)theologian (0.78)tailor (0.78)housekeeper (0.78)prosecutor (0.77)realtor (0.76)performing artist (0.76)composer (0.74)handyman (0.73)tennis player (0.71)civil servant (0.69).
bride (0.82)mother (0.79)ladies (0.78)boy (0.77)sir (0.75)groom (0.74)his (0.72)schoolboy (0.7).
salesperson (0.87)priest (0.85)politician (0.85)swimmer (0.83)attourney (0.83)ceo (0.82)professor (0.82)receptionist (0.81)policeman (0.81)librarian (0.8)singer (0.8)guard (0.79)baker (0.79)poet (0.79)army (0.78)director (0.78)artist (0.77)electrician (0.77)chef (0.76)pensioner (0.75)painter (0.74)umpire (0.73)drawer (0.71)assistant (0.69).
gender.
grandmother (0.81)male (0.79)gentleman (0.78)husband (0.77)gentlemen (0.75)herself (0.74)son (0.72)himself (0.7).
profession.
model (0.87)ﬁreﬁghter (0.85)prisoner (0.84)pianist (0.83)butcher (0.83)writer (0.82)chemist (0.82)linguist (0.81)physicist (0.81)maid (0.8)opera singer (0.8)laborer (0.79)psychologist (0.79)academic (0.79)athlete (0.78)auditor (0.78)comedian (0.77)chief (0.76)banker (0.76)physician (0.75)pharmacist (0.74)attendant (0.72)author (0.71)counselor (0.69)race.
afghanistan (0.9)syrian (0.85)mexico (0.84)arab (0.83)saudi arabian (0.83)bangladesh (0.82)hispanic (0.82)guatemala (0.81)afghan (0.81)qatar (0.8)native american (0.8)albania (0.79)jordanian (0.79)romania (0.79)crimea (0.78)romanian (0.78)dutch people (0.77)jordan (0.77)indian (0.77)korean (0.76)persian people (0.76)sierra leonean (0.75)australian (0.75)ecuadorian (0.74)portuguese (0.74)bolivia (0.73)taiwanese (0.73)singaporean (0.73)laotian (0.71)cape verdean (0.69).
italy (0.87)irish (0.88)brazil (0.85)japan (0.85)iraq (0.84)ireland (0.84)egypt (0.83)iraqi (0.83)african (0.83)russia (0.83)pakistani (0.82)iranian (0.82)italian (0.81)germany (0.81)ghanaian (0.81)ethiopia (0.81)chinese (0.8)scottish (0.81)yemen (0.8)white people (0.8)japanese (0.8)swedish (0.8)australia (0.79)norway (0.79)ecuador (0.79)korea (0.79)india (0.78)austria (0.79)indonesian (0.78)european (0.78)sweden (0.78)german (0.78)south african (0.77) french (0.77)eritrean (0.77)lebanon (0.77)puerto rican (0.76)liberia (0.77)paraguayan (0.76)chile (0.76)turkish (0.75)greece (0.76)spanish (0.75)qatari (0.75)lebanese (0.75)spain (0.75)laos (0.74)bengali (0.74)albanian (0.73)ukrainian (0.73)cameroonian (0.73)oman (0.73)ghana (0.73)nepal (0.73)taiwan (0.72)polish (0.72)chilean (0.71)peruvian (0.71)singapore (0.67)nepali (0.68)religion.
girl (0.81)girlfriend (0.79)sister (0.78)lady (0.76)father (0.74)ma’am (0.73)schoolgirl (0.72)boyfriend (0.67).
grandfather (0.81)daughter (0.79)mommy (0.78)she (0.76)wife (0.74)hers (0.73)brother (0.72)her (0.66).
businessperson (0.86) mathematician (0.86)researcher (0.85)broker (0.84)entrepreneur (0.83)mechanic (0.83)carpenter (0.82)lawyer (0.82)sheriff (0.81)mover (0.81)designer (0.8)secretary (0.8)pilot (0.79)cleaner (0.79)musician (0.78)cashier (0.78)barber (0.78)actor (0.77)teacher (0.76)doctor (0.76)dentist (0.75)plumber (0.74)waiter (0.72)bartender (0.7)clerk (0.67).
chess player (0.85)farmer (0.84)supervisor (0.83)software developer (0.82)guitarist (0.82)detective (0.81)economist (0.81)manager (0.8)accountant (0.8)historian (0.8)dancer (0.79)boxer (0.79)analyst (0.78)soldier (0.78)nurse (0.78)commander (0.77)sociologist (0.76)judge (0.76)producer (0.74)editor (0.73)delivery man (0.72)diplomat (0.7)journalist (0.64).
africa (0.87)kenyan (0.85)nigerian (0.83)british (0.83)somali (0.82)kenya (0.82)china (0.81)columbian (0.81)cameroon (0.8)syria (0.8)brazilian (0.8)paraguay (0.79)sudanese (0.79)guatemalan (0.78)poland (0.78)argentian (0.78)venezuelan (0.77)peru (0.77)norweigan (0.76)moroccan (0.76)sierra leon (0.75)somalia (0.75)indonesia (0.75)europe (0.74)liberian (0.73)greek (0.73)eriteria (0.73)morocco (0.72)crimean (0.71)cape verde (0.67).
yemeni (0.86)pakistan (0.84)african americans (0.83)mexican (0.83)france (0.82)russian (0.82)iran (0.81)ethiopian (0.81)venezuela (0.8)columbia (0.8)britain (0.79)scotland (0.79)ukraine (0.79)turkey (0.78)sudan (0.78)south africa (0.77)saudi arabia (0.77)egyptian (0.77)puerto rico (0.76)vietnamese (0.76)vietnam (0.75)argentina (0.75)finnish (0.75)finland (0.74)portugal (0.73)austrian (0.73)omani (0.73)bolivian (0.72)netherlands (0.7).
jihad (0.86)sanskrit (0.8)sharia (0.78)vishnu (0.76)hindu (0.74).
muslim (0.84)mecca (0.8)church (0.77)hajj (0.76)brahmin (0.74).
holy trinity (0.81)islam (0.79)jesus (0.77)bible (0.75)ten commandments (0.72) shiva (0.72).
quran (0.8)baptize (0.79)christ (0.77)christian (0.74).
trinity (0.8)mohammed (0.79)messiah (0.76)reincarnation (0.74).
table 10: the set of terms that were used to collect stereoset, ordered by per-term annotator agreement..5371