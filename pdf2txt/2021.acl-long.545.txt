using meta-knowledge mined from identiﬁers toimprove intent recognition in conversational systems.
claudio s. pinhanez, paulo cavalin, victor ribeiro, ana paula appel,heloisa candello, julio nogima, mauro pichiliani, melina guerra,maira gatti de bayser, gabriel malfatti, henrique ferreira.
ibm research - brazilcsantosp@br.ibm.com.
abstract.
in this paper we explore the improvement of in-tent recognition in conversational systems bythe use of meta-knowledge embedded in in-tent identiﬁers.
developers often include suchknowledge, structure as taxonomies,in thedocumentation of chatbots.
by using neuro-symbolic algorithms to incorporate those tax-onomies into embeddings of the output space,we were able to improve accuracy in intentrecognition.
in datasets with intents and ex-ample utterances from 200 professional chat-bots, we saw decreases in the equal error rate(eer) in more than 40% of the chatbots incomparison to the baseline of the same algo-rithm without the meta-knowledge.
the meta-knowledge proved also to be effective in de-tecting out-of-scope utterances, improving thefalse acceptance rate (far) in two thirds ofthe chatbots, with decreases of 0.05 or more infar in almost 40% of the chatbots.
when con-sidering only the well-developed workspaceswith a high level use of taxonomies, far de-creased more than 0.05 in 77% of them, andmore than 0.1 in 39% of the chatbots..1.introduction.
classiﬁcation of sentences into a discrete set ofclasses is a key part of professional conversationalsystems.
in fact, most of those systems requiredevelopers to deﬁne the different classes, or in-tents, by enumerating exemplars of each of them,since classiﬁcation is often performed using ma-chine learning (ml) methods.
the process of clas-sifying an input sentence into a speciﬁc intent orsignaling it as out-of-scope (oos) of the system isoften referred to as intent recognition..determining a class solely on a list of exemplarsis a practical method to implement ml systems butit is hardly a natural way for human beings to de-ﬁne a class.
in real life, people deﬁne classes oftenusing a rich mix of symbolic deﬁnitions, sometimes.
taxonomic in nature, such as in “a credit card is atype of bank card”, coupled with its sub-classes, forinstance, “basic”, “premium”, and typical featuressuch as “international”.
people also use exemplars,“card x of bank y is a credit card”, as well as par-ticular examples to describe a sub-class, such asin “card w is an international card”.
they alsouse counter examples, either categorically, “a debitcard is not a credit card”, or in examples, “cardz is not a credit card”.
deﬁning and specifyingclasses in the real world is, in fact, a cultural, con-textual, and linguistic construct, and how peopleand societies perform this process is a traditionalresearch subject in social sciences, notably in an-thropology (durkheim and mauss, 1963; needham,1979; bowker and star, 2000)..this paper explores algorithms for intent recog-nition which use both the sets of exemplars andtaxonomic-like symbolic descriptions of a class todeﬁne and train intents in conversational systemsusing ml methods.
we aim not only to providemethods more aligned to everyday class deﬁnitionpractices of developers but also to improve the accu-racy of the ml methods.
inspired by a reverse dic-tionary algorithm (kartsaklis et al., 2018) and previ-ous work on keyword-based classiﬁcation (cavalinet al., 2020), we propose three neuro-symbolic al-gorithms which combine taxonomic descriptions ofclasses with traditional exemplar-based supervisedlearning.
we show that those novel algorithms areable to decrease error rates for a signiﬁcant num-ber of datasets, particularly in the difﬁcult task ofdetecting oos cases in real, professional chatbots.
the key idea behind our algorithms is to substi-tute the typical softmax used in the output layer ofa ml text classiﬁer with a space of embeddings ofthe taxonomic descriptions of the intents.
the train-ing process uses the exemplars in standard wayswhile the recognition process is performed usingsimilarity distances in the embedding output space..proceedingsofthe59thannualmeetingoftheassociationforcomputationallinguisticsandthe11thinternationaljointconferenceonnaturallanguageprocessing,pages7014–7027august1–6,2021.©2021associationforcomputationallinguistics7014this is similar to ideas used in zero-shot learningmethods (palatucci et al., 2009; socher et al., 2013;akata et al., 2015, 2016), in which classes deﬁnedby sub-concepts are also encoded with special em-beddings to allow detection of new classes withoutexemplars..we tested our algorithms using real datasetsby exploring a common practice among develop-ers of conversational systems, who often embedsymbolic knowledge as documentation in intentidentiﬁers.
in a previous work (pinhanez et al.,2021), we observed a pattern among developersof using taxonomic-like structures to name theintents in which strings of reoccurring conceptsare used to identify and document the differentclasses.
for example, an intent about utteranceswhere users ask for the balance of a credit card maybe named “checking credit card balance”, whilean intent related to ﬁnding out the date of pay-ment of the balance could be identiﬁed as “ask-ing credit card balance payment date”..we call those structures intent proto-taxonomies,and real examples are shown in ﬁgures 1 and 2.in (pinhanez et al., 2021), we studied the use by de-velopers of intent proto-taxonomies quantitativelyand qualitatively, as well as proposed an algorithmto mine this meta-knowledge automatically, andconcluded that their use is fairly common in atleast one professional chatbot development plat-form.
this paper focuses on the algorithms to usethe meta-knowledge and on evaluating their impacton the accuracy of intent recognition..the paper starts by looking into the recent ad-vances in neuro-symbolic systems and describingbrieﬂy the practice of developers of conversationalsystems of embedding meta-knowledge within thesource code of their systems.
we follow by describ-ing the proposed three algorithms integrating suchmeta-knowledge into intent recognition ml algo-rithms and by evaluating them ﬁrst with two typicalintent recognition datasets, and then with hundredsof workspaces created in a professional tool calledhere chatworks1.
the results show most of thoseworkspaces can beneﬁt from the techniques de-scribed in this paper, notably for oos detectiontasks, often with accuracy improvements of 5% ormore solely derived from the use of the additionalsymbolic description from the documentation..1we use an anonymous name for the tool due to publication.
restrictions from the platform company..2 related work.
the value and limits of symbolic categoriza-tion in ai have been of interest since the earlydays (newell, 1973; richards, 1982; kosslyn,2006).
but our work ﬁts more in the context ofa growing belief that symbolic knowledge needs tobe included in ml systems, materialized in the socalled neuro-symbolic approaches (parisotto et al.,2017; besold et al., 2017; tenenbaum et al., 2011;bengio, 2017; mao et al., 2019; hudson and man-ning, 2019a; de raedt et al., 2019)..neuro-symbolic methods “aim to transfer prin-ciples and mechanisms between (often nonclassi-cal) logic-based computation and neural computa-tion” (besold et al., 2017).
such kind of systemsare viewed by some researchers as a way to embedhigh-level knowledge and even some form of “con-sciousness” into machine learning systems, mak-ing the language to develop them closer to “whatpasses in a man’s own mind” (bengio, 2017)..in recent years, ai has witnessed a myriad ofnovel neuro-symbolic techniques and their appli-cation to different problems, contexts, and scenar-ios (parisotto et al., 2017; manhaeve et al., 2018;d’avila garcez et al., 2019; hudson and manning,2019b; de raedt et al., 2019).
for instance, in(mao et al., 2019), an approach for image under-standing is suggested which takes the object-basedscene representations and translates sentences intoexecutable, symbolic programs.
in (oltramari et al.,2020), embeddings of knowledge graphs are usedas attention layers for tasks such as autonomousdriving (av) and question-answering.
and in (kart-saklis et al., 2018), random walks in a knowledgegraph are mapped as sentence embeddings for usein an inverse dictionary problem..one important requirement for many neuro-symbolic systems is to represent knowledge in astructured format such as knowledge graphs, on-tologies, or taxonomies (ji et al., 2020).
in somecases, such as the scene ontology for autonomousvehicles in (oltramari et al., 2020), a lot of effortwas needed for manual annotation.
nevertheless,as presented in (fossati et al., 2015), an unsuper-vised approach can sometimes be used to mine themeta-knowledge introduced by the experts, such asthe classes in wikipedia pages..considering our context, intent identiﬁers aresometimes described using high-level representa-tions of the class as we detail later.
this is sim-ilar to what is used in some zero-shot learning.
7015techniques (wang et al., 2019) in which classesmanually deﬁned by sub-concepts are encodedwith special embeddings so new classes can bedetected without training (palatucci et al., 2009;socher et al., 2013; akata et al., 2015, 2016; chenet al., 2016).
in (chen et al., 2016), for exam-ple, intent identiﬁers can be formatted as naturallanguage sentences to learn a model which mapstraining examples into those sentences, so that themeta-knowledge can be used in zero-shot learn-ing.
however, the dataset explored in that work isvery limited.
recent work has also demonstratedthat intent recognition can be improved by enhanc-ing class representations with “keywords” whichare extracted from exemplar utterances consideringtheir most common words (cavalin et al., 2020)..this work focuses on high level class represen-tations based on taxonomies and aims to exploretheir usefulness as enhancers of ml intent recogni-tion algorithms.
it also explores different ways ofembedding taxonomy-like meta-knowledge consid-ering different methods of representation..3 the knowledge embedded in intentidentiﬁers of conversational systems.
most real-world, deployed conversational systemsin use today have been built based on the rule-basedintent-action paradigm, using platforms such asluis.ai, watson assistant, or alexa skills.
each in-tent corresponds to a desired information or answerfrom the user and is deﬁned by a set of exemplarutterances by the chatbot developers.
during run-time, each utterance from the user is recognized asone of the deﬁned intents or as out-of-scope (oos),and then the associated action is outputted..in the context of the chatbots built using thechatworks platform explored in this paper, a pre-vious work of the authors of this paper (pinhanezet al., 2021) has shown that the curators and devel-opers of chatbots often store symbolic knowledgein a taxonomic form about the intent classes in adocumentation ﬁeld called nameid.
figure 1 showssome examples of those nameids, obtained from aprofessional ﬁnance chatbot, here translated fromthe original in portuguese and anonymized to pre-serve conﬁdential information..this practice was studied in workshops with de-velopers (pinhanez et al., 2021), which determinedthat the goal of the taxonomic description is toprovide the intent classes with a summarized de-scription of each intent.
such taxonomic naming.
figure 1: some nameids of intents of a ﬁnance chatbot..figure 2: the intent proto-taxonomy associated to theintents of ﬁg.
1..patterns are also common in the way people orga-nize ﬁles and e-mails in computers (civan et al.,2008; whittaker et al., 2011) and how softwaredevelopers name functions (yang et al., 2019)..as described in (pinhanez et al., 2021), suchknowledge-embedding practices are, in fact, fairlycommon among curators in the chatworks plat-form.
using the algorithm reproduced in ap-pendix a, taxonomic-like symbolic knowledge wasautomatically extracted from workspaces deﬁningalmost 7,000 professional chatbots, in two differentlanguages.
by considering the different words inthe nameids as basic concepts and consecutive con-cepts as having connections between them, we canstructure the set of nameids as a very basic knowl-edge graph (ehrlinger and w¨oß, 2016), herebyreferred as an intent proto-taxonomy..figure 2 depicts the intent proto-taxonomy asso-ciated to the nameids in ﬁg.
1. next, as proposed.
7016in (pinhanez et al., 2021), it is possible to computethe taxonomy rate of a workspace by calculatingthe ratio between the number of intents with tax-onomies and the total number of intents..in 3,840 professsional workspaces in the en-glish language, it was found that 76% of them hada taxonomy rate above 10%, almost 52% had a tax-onomy rate above 50%, and 16% had a very hightaxonomy rate, above 90%.
moreover, the distri-bution followed a sort of “step” function where,as the threshold of 32 in the number of intentsin a workspace was crossed, the majority of theworkspaces had a taxonomy rate of more than 50%.
it seems that, as the complexity of the workspaceincreases with the number of intents, more oftendevelopers resort to document them using an intentproto-taxonomy (see appendix b for details)..the use in our work of the intent proto-taxonomies as a symbolic description of classes isfeasible because: (1) they are part of the documen-tation of the conversational system, so there is noneed of acquiring knowledge from experts; (2) theyare easily mined, as described in appendix a..4 using taxonomic intent descriptions.
to improve intent recognition.
we present now a formal description of the method-ology employed in this work which takes advan-tage of the intent proto-taxonomies using a neuro-symbolic approach.
it expands some previous workwhich focused on the use of keywords as the sourceof symbolic information (cavalin et al., 2020).
..4.1 embedding the set of classes.
an intent classiﬁcation method is a function dwhich maps a set of sentences (potentially inﬁnite)s = {s1, s2, ...} into a ﬁnite set of classes ω ={ω1, ω2, ..., ωc}:.
d : s → ω d(s) = ωi.
(1).
to enable a numeric, easier handling of the inputtext, an embedding ξ : s → rn is often used,mapping the space of sentences s into a vectorspace rn, and deﬁning a classiﬁcation function e :rn → ω such that d(s) = e(ξ(s)).
in most intentclassiﬁers, e is composed of a function m whichcomputes the likelihood of s being in a given class,often a neural network, followed by some sort ofargmax function.
typically, sof tmax + argmaxis used, noted simply as sof tmax here:.
→ rn m→ rc sof tmaxξ.
→ ω.s.(2).
this paper explores how to use embeddings inthe output side of the classiﬁcation function, thatis, by embedding the set ω of classes into an-other vector space rm, in some ways resemblingthe combination of object-based recognition andsymbolic programming in (mao et al., 2019).
in-stead, we combine here standard intent recognitionmethods with an encoding of taxonomies in knowl-edge graph-like structures.
the idea is to use classembedding functions which somehow capture theknowledge in the intent proto-taxonomies..formally, we use a class embedding functionψ : ω → rm, its inverse ψ−1, and a functionm : rn → rm to map the two vector spaces sod(s) = ψ−1(m (ξ(s)))..ξ.
→ rn m→ rm ψ−1.
→ ω.s.(3).
in this paper we explore three sentence embed-ding methods to implement ξ. we use a two-layerneural network as m and employ the standardmean square error (mse) as the inverse ψ−1,to determine the closest embedding of each classωi ∈ ω to the output of m ..4.2 adapting kartsaklis method (lstm).
our basic inspiration for the algorithms of this pa-per is a text classiﬁcation method proposed in (kart-saklis et al., 2018) for the inverse dictionary prob-lem where text deﬁnitions of terms are mappedto the term they deﬁne.
the embedding of theclass set into the continuous vector space (equiv-alent to the ψ function in equation 3) is done byexpanding the knowledge graph of the dictionarywords with nodes corresponding to words relatedto those terms.
next, random walks are perfomedon the graph to compute graph embeddings relatedto each dictionary node, using the deepwalk algo-rithm (perozzi et al., 2014)..a long short-term memory (lstm) neural net-work, composed of two layers and an attentionmechanism, is used in (kartsaklis et al., 2018) formapping the input texts to the input embeddingvector space.
to map the two continuous vectorspaces representing the deﬁnitions and the dictio-nary terms, a two-layer neural network m , learnedfrom the training dataset, is used..for this work, the knowledge graph is replacedby an intent proto-taxonomy g which associateseach class to a node and connects to them nodeswhich correspond to meta-knowledge concepts re-lated to the class.
to better capture the sequential.
7017aspect of the intent proto-taxonomies, we also con-nect each class node to bigrams of concepts, i.e.,the concatenation of two subsequent concepts.
werepresent this by the function ζ, such as ζ(ω) = g,which is invertible.
substituting this in equation 3,.s lst m→ rn m→ rm deepw alk−1.
→.
ζ−1→ ω (4).
g.in practice, we compute the mapping from theclass embedding space into the class set, calledhere invg : rm → ω, simply by determiningthe distance d between the output point in rm andthe inverted projection of each class from ω andthen considering the closest class.
that is, for eachwi ∈ ω, we consider the associated node in g andcompute the mapping in rm of that node:.
invg(x) = arg min.
d(x, deepw alk(g(wi)).
(5).
wi.
by substituting this function into equation 4, we.
obtain the algorithm we call here lstm+t:.
s lst m→ rn m→ rm invg→ ω.
(6).
for comparison, the traditional correspondingclassiﬁcation method is tested, where the graphembedding and associated functions are replacedby softmax+argmax.
we call this lstm:.
s lst m→ rn m→ rc sof tmax.
→ ω.
(7).
4.3 an alternative to lstm: use.
recently, several new general-purpose languagemodels that can be used for computing sentenceembeddings have been proposed, among them theuniversal sentence enconder (use) (cer et al.,2018).
such an approach consists of a transformerneural network (vaswani et al., 2017), trained onvaried sources of data, such as wikipedia, webnews, web question-answer pages and discussionforum.
use has achieved state-of-the-art resultsin various tasks, so we decided to try it in our ex-periments as an alternative to the lstm for theembedding of input sentences..in this work we employed the version 3 of themultilingual use 2. by replacing lstm with usein eq.
6 we obtain algorithm use+t:.
s u se→ rn m→ rm invg→ ω.
(8).
2https://tfhub.dev/google/universal-sentence-encoder-.
multilingual/3.
like in the previous case, we also compute theuse algorithm with traditional discrete softmaxoutputs for comparison, called here simply use:.
s u se→ rn m→ rc sof tmax.
→ ω.
(9).
4.4 alternatives to deepwalk.
to explore variants of algorithms for embeddingthe classes and also approaches which do not needto be trained from scratch and allow on-the-ﬂy han-dling of meta-knowledge, we tried replacing deep-walk with two different methods..the ﬁrst one consists of applying use sentenceembeddings also for the class embeddings, suchas in eq.
10. to simplify notation, emb representseither lstm or use embeddings for the input text..s emb→ rn m→ rm u se−1.
→ g.ζ−1→ ω.
(10).
this approach is similar to the way deepwalkworks but instead of training the graph embeddingsfrom scratch, the class embeddings are representedby the mean sentence embedding computed fromdifferent random walks starting in the class node.
we name such methods lstm+s and use+s, foremb substituted by lstm and use, respectively.
additionally, we also evaluate the replacementof deepwalk by the convolutional deep structuredsemantic model (cdssm) proposed in (chen et al.,2016), yielding the following algorithm where embcan be either lstm or use embeddings..s emb→ rn m→ rm cdssm −1.
→ g.ζ−1→ ω.
(11).
the cdssm model consists of a three-layer con-volutional neural network trained for creating em-beddings of intent identiﬁers represented as sen-tences.
in this work, we input to cdssm the se-quence of concepts listed in the nameid of eachintent.
we refer to these algorithms as lstm+cand use+c, for emb being substituted with lstmand use, respectively..an intuitive way to understand those methodsis to consider use+t using a taxonomy as if itsconcepts had just abstract meanings: only theirrelations matter.
in comparison, use+s considersthe meaning of the concepts besides their relations,while use+c regards each nameid as a sentence,almost as if the developer had inputted a writtendescription of the intent..70184.5 out-of-scope sample detection.
in this paper we are interested both in the problemsof: (1) deciding whether an user utterance is in-scope (is) or out-of-scope (oos) of the system;and (2) determining to which class an is utterancebelongs.
for the former, a rejection mechanismbased on a pre-deﬁned threshold is used since itcan be easily applied to all of the methods describedpreviously without the need neither for any speciﬁctraining procedure nor oos training data..in detail, suppose that for each class ωi ∈ ωthere is a score denoted φi ∈ z, where |z| = |ω|.
given that max(z) represents the highest scoreassociated to a class and that a rejection thresholdθ has been deﬁned on a validation set, samples canbe classiﬁed as oos whenever max(z) < θ. ifso, they are simply rejected, i.e., no classiﬁcationoutput is produced for them.
otherwise, the sampleis considered as in-scope and the classiﬁcation isconducted normally..the scores in z are represented either by the soft-max probability for the traditional softmax-basedmethods or by the similarity of sentence and intentembeddings for the proposed three approaches.
forthe latter, the similarity is computed by means ofthe dot product between the two embeddings..5 metrics, datasets, and experiments.
in this section we present the experiments to evalu-ate the three algorithms described in the previoussection, using each of the input embeddings lstmand use.
we explore the impact on intent recog-nition both in terms of classifying correctly utter-ances (is accuracy) and of ﬁnding which utterancesare not covered by the intents (oos accuracy)..5.1 evaluation metrics.
we employ a commonly-used metric for oos dec-tection, equal error rate (eer) (tan et al., 2019),which corresponds to the classiﬁcation error ratewhen the threshold θ is set to a value where false ac-ceptance rate (far) and false rejection rate (frr)are the closest.
these two metrics are deﬁned as:.
f ar =.
number of accepted oos samplestotal of oos samples.
f rr =.
number of rejected is samplestotal of is samples.
(12).
(13).
in addition, in-scope error rate (iser) is consid-ered to report is performance, i.e.
the error rateconsidering only is samples when θ is set to zero,.
similar to the class error rate in (tan et al., 2019).
this metric is important to evaluate whether theproposed classiﬁcation methods are able to keep upwith the performance of the baselines in the mainclassiﬁcation task..5.2 the larson and telco datasets.
during the development and initial testing of thealgorithms, we used two english datasets for in-depth experimentation.
the ﬁrst is the publicly-available larson dataset (larson et al., 2019); thesecond is a private real-world chatbot dataset usedby a telecommunications provider for customercare, called here the telco dataset.
in the larsondataset, we created an intent proto-taxonomy byhand, expanding the original identiﬁers of intents.
the goal of the adjustments was to avoid spuriousinterference from taxonomy shortcomings or errorsin the results.
the complete list of the createdtaxonomic description of intents is listed in theappendix c to allow the reproduction of our resultsand further experimentation.
in the telco dataset,we created by hand the intent proto-taxonomy..in the larson dataset there is a total of 22,500is exemplars, evenly distributed across 150 classes,where 18,000 were used for training and 4,500for testing.
we conducted a simulation of oosdetection with the is exemplars by doing 5 ran-dom samplings where we took out 30 intents and3,600 training exemplars.
we trained only withthe remaining 120 intents and 14,400 exemplars.
the test was then conducted using all the non-used4,500 exemplars, where the 3,600 associated to thetrained classes were considered the is samples andthe remaining 900 became oos samples..the telco dataset contains 4,093 exemplars and87 intents.
from those, 3,069 exemplars were usedfor training and 1,024 for testing.
the oos sce-nario was simulated by extracting different randomsamplings where 5 intents were removed.
giventhe smaller size of this dataset compared to larson,we conducted 20 samplings instead of 5..for both sets we considered the following setupdeﬁned after preliminary evaluations.
for thelstm-based methods, the input sentence embed-ding size was set to 150 and output embeddingsto 200. deepwalk walk sizes were set to 20 forlstm+t and use+t.
for both use- and softmax-based methods we trained a two-layer neural net-work with 800 hidden neurons for 50 epochs..7019line seemed to be similar, possibly slightly betterfor the use algorithm.
second, and most impor-tantly, we saw much more improvement in the useof the intent proto-taxonomy in the larson than inthe telco dataset, in spite of the similar nature ofthe datasets and the intent proto-taxonomies.
thismotivated us to try out the ideas in a larger andmore diverse number of workspaces and solely fo-cusing on use to simplify the experiments..5.4 the chatworks dataset.
to test our algorithms in a context of high diversityand realism, we used the same large set of real, pro-fessional workspaces explored in (pinhanez et al.,2021), which come from the professional chatbotdevelopment platform chatworks..we started with the 3,840 workspaces availablein english.
to eliminate possible problems dueto workspaces with poor quality, we employedthe 3σ-rule, where values smaller greater than 3standard deviations from the mean are not con-sidered.workspaces with the number of intents orexemplars below and above those thresholds wereremoved.
also, to avoid workspaces with few ex-emplars per intent, the ratio of the number of ex-emplars to the number of intents had to be greaterthan 10. from the ﬁltered set we randomly selected200 workspaces for testing..the evaluation involved the execution of 20 itera-tions for each workspace.
the tests were performedfor all use-based methods (use, use+t, use+s,and use+c).
first, the workspaces were split intotraining and test datasets (75% and 25%, respec-tively).
next, the four methods were trained andtested on these datasets.
the evaluation metrics(eer, far, and iser) were then measured on theresults for the test datasets and the average errorsand their standard deviations were computed..5.5 results in the chatworks dataset.
appendix d contains a table with the results foreach of the 200 workspaces in the chatworksdataset.
figure 5 summarizes the results ofthe experiments showing the distribution of the200 workspaces according to ranges of the improve-ment of each of the three methods compared to thebaseline of use.
improvement is calculated by sub-tracting the errors in each of our proposed methodsfrom the errors in the use baseline (error valuesare scores between 0 and 1).
when one of ourmethods was worse than the baseline then diff < 0,.figure 3: different methods to incorporate the intentproto-taxonomy in larson dataset, compared to thelstm and use baselines..figure 4: different methods to incorporate the in-tent proto-taxonomy in telco dataset, compared to thelstm and use baselines..5.3 results in the larson and telco datasets.
the results on the larson dataset are graphicallydepicted in ﬁg.
3. we observed that there was aslight improvement (a decrease) in eer, especiallywith the use-based and the lstm+c methods.
more notably, there was a signiﬁcant improvementin terms of far for all use-based methods andlstm+s and lstm+c.
notice that even thoughthe proposed approaches generally did not outper-form lstm and use in iser (except lstm+c),we observed that the methods with better isertended to produce also better eer and far..in ﬁg.
4, we see that the results on the telcodataset presented a different scenario.
the pro-posed methods generally performed worse than or,at best, similar to lstm and use in eer.
in termsof far, some methods such as use+t and use+cseem to outperform the others but, considering thehigh standard deviations, the results were not sig-niﬁcant.
on the other hand, we also observed thatthe methods failed to get close in iser comparedto the softmax-based methods.
that seems to indi-cate that for the cases where making use of meta-knowledge harms too much iser, the symbolicknowledge did not decrease neither eer nor far.
there were two key ﬁndings from our experi-ments with the larson and the telco datasets.
first,the improvements using lstm or use as a base-.
7020since smaller is better, and conversely for when itis better than the baseline, i.e., diff (cid:62) 0..the results shown in ﬁg.
5 indicate that theuse+c algorithm achieved the best results in allthree metrics, although there is a signiﬁcant portionof workspaces where the other methods also didwell, especially in oos detection (far)..but, more important, the results seem to supportour claim that meta-knowledge embedded in theoutput layer of our neuro-symbolic algorithms canimprove intent recognition performance in practicalsystems.
notably in oos detection (far), 67%of the workspaces experienced a decrease in theerror rate using use+c.
besides, in 39% of theworkspaces we observed a decrease in the errorrate of more than 0.05 (in a 0 to 1 scale), and in23%, of more than 0.1. the use+t also did wellwith similar but slightly smaller decreases in error.
overall, the error rates for the eer metric alsodecreased in relation to the baseline.
figure 5shows that 41% of the workspaces had some levelof decrease in eer with the use+c algorithm, in10% of them with decreases of 0.05 or more.
how-ever, the results for the in-scope accuracy (iser)were much smaller with only about 16% of theworkspaces having any kind of decrease..the chatworks dataset, as noted before, in-cludes all kinds of workspaces.
taxonomy ratesvaries anywhere from 0 to 1, and there are verysmall and very large workspaces.
to test our meth-ods in a scenario closer to a professional, well-developed chatbot, we ﬁltered further the datasetto include only workspaces with taxonomy rategreater or equal to 0.7, with number of intents equalor more than 32, and at least an average of 25 ex-emplars per intent, resulting in 18 workspaces..figure 6 shows the distribution of the results ofthe experiments with those 18 workspaces, whichwere better than in the full chatworks dataset.
bothuse+t and use+c yielded eer decreases in 50%or more of the workspaces.
moreover, 83% of theworkspaces decreased the far error, either withuse+t or use+c, and both decreased far inmore than a third of the workspaces by more than0.1. we discuss the results and implications next..6 discussion and future work.
we started this paper by proposing the combinationof exemplars and symbolic characterizations of aclass as a way to enhance ml-based intent recog-nition.
we proposed 3 new neuro-symbolic algo-.
figure 5: distribution of performance difference (diff)to use baseline of the 3 methods according to eer,far, and iser metrics in all 200 workspaces of thechatworks dataset..figure 6: distribution of performance difference (diff)to use baseline of the 3 methods according to eer,far, and iser metrics in the 18 most developedworkspaces of the chatworks dataset..rithms and tested them using datasets built usingdata from intent identiﬁers of conversational sys-tems.
such identiﬁers often store taxonomic-likestructures, due to a common practice among devel-opers of professional conversational systems (pin-hanez et al., 2021).
the results of the experimentsindicate that the intent proto-taxonomies embeddedby those developers can indeed be used by manyworkspaces to improve accuracy in intent recogni-tion, notably in oos detection..we see as one of the main contributions of thispaper the creation of methods with which ml en-gineers can improve the accuracy of their systemsby simply mining “documentation” from chatbots,without any further data and annotation..our results show that almost 40% of the 200professional workspaces drawn from chatworkssaw decreases of more than 0.05 in oos detection.
7021error rates.
also, in 42% of them the overall errorrate was improved, using the use+c algorithm.
when considering the more well-structured anddeveloped 18 workspaces, we saw much highergains with the use+t algorithm.
those accuracyimprovements were achieved without any changein the training set but simply by incorporating themeta-knowledge into intent recognition..notice that the testing methodology used in thiswork is considerably harder than the practice ofthe majority of research papers, since it evalu-ates performance in 200 professional, non-editedworkspaces from different domains.
in reality, mostml algorithms do not perform well in all datasets,and ml practitioners often test different algorithmsand parameters until accuracy is good enough..however, the improvement in oos detection(far) was not mirrored in classiﬁcation error(iser).
first, we must keep in mind that intentclassiﬁcation is often performed in two steps, ﬁrstoos sentences detection and removal, followed byintent classiﬁcation of the is sentences.
given theimprovements observed in oos detection, it wouldmake sense to use our algorithms in the ﬁrst stepfor many of the chatworks workspaces (about 60%of them), and selectively use it for is classiﬁcationonly when it works better than the baseline..but why were there so many workspaces wherewe did not see impact?
it is important to take intoaccount that the chatworks dataset has workspacesin different stages of development and deployment.
by selecting better quality workspaces, we sawmuch higher gains.
we explored brieﬂy charac-terizations of the intent proto-taxonomy quality,such as taxonomy rate, depth of the taxonomy, andnumber of concepts, but we saw no clear correla-tion with decreases in error rates.
we believe morecomplex metrics of knowledge structure need tobe employed to characterize which intent proto-taxonomies are likely to have the greatest impacts.
we plan to do so in our future work..it is important to notice that, in the workspaceswhere we did see impact, the symbolic knowl-edge was mined from an absolutely “raw” format.
in spite of that, by using the basic graph miningmethod described in appendix a, it was possible toobtain a “meaningful” taxonomic structure, similarto a knowledge graph which could be used by ourneuro-symbolic algorithms.
to improve the qualityof the taxonomies, we are working on designing aninterface which allows the developers to manipu-.
late directly the intent proto-taxonomy to make itmore correct and complete, so to possibly decreaseeven more the intent recognition error rates..we have demonstrated in this work that combin-ing exemplar and symbolic ways of deﬁning classescan have a positive impact in the performance ofthe recognition system.
this was done in the con-text of conversational systems where developersfortuitously embed such alternative descriptions ofclasses in their name identiﬁers.
we believe it ispossible to ﬁnd in other machine learning devel-opment platforms similar patterns of knowledgeembedding..for example, we know that it is common forpeople to use similar taxonomic structures whennaming ﬁle and e-mail folders, giving names tofunctions and variables in programs and data, andwriting comments into jupyter notebooks.
also,ml platforms can further foster the use of meta-data by developers by explicitly asking them toinput, besides exemplars, categorical or textual de-scriptions of the classes..as we move along the path of creating suchneuro-symbolic systems, not only we should expectthat the job of developers becomes easier, as theyfollow their own cultural and linguistic practices,but also that machines became better in recognizingthose classes accurately.
using multiple forms ofclass deﬁnitions can be a winning proposition forboth ml systems and their developers..ethical issues.
the chatworks dataset was composed only ofworkspaces in which the developers explicitlyopted-in to share their code and content for researchand development purposes with the company whichowns the platform.
those workspaces were sharedby the company with the authors of this paper witha clear condition of not publicly sharing their con-tents and publishing only aggregated results or inan anonymous form.
we do not see any speciﬁcimpact of those limitations in the results of our re-search but they preclude easy forms of replicationof our results with that dataset.
to better enablereproducibility, we presented the analysis of thepublic larson dataset and shared the intent proto-taxonomy we created manually from its originalintent structure in appendix c..7022references.
zeynep akata, florent perronnin, zaid harchaoui, andcordelia schmid.
2016. label-embedding for imageclassiﬁcation.
ieee transactions on pattern analy-sis and machine intelligence, 38(7):1425–1438..zeynep akata, scott reed, daniel walter, honglaklee, and bernt schiele.
2015. evaluation of outputembeddings for ﬁne-grained image classiﬁcation.
inproceedings of the ieee conference on computervision and pattern recognition (cvpr)..yoshua bengio.
2017. the consciousness prior..tarek r. besold, artur d’avila garcez, sebastianbader, howard bowman, pedro domingos, pas-cal hitzler, kai-uwe kuehnberger, luis c. lamb,daniel lowd, priscila machado vieira lima, leode penning, gadi pinkas, hoifung poon, and ger-son zaverucha.
2017. neural-symbolic learning andreasoning: a survey and interpretation..geoffrey c bowker and susan leigh star.
2000. sort-ing things out: classiﬁcation and its consequences.
mit press..paulo cavalin, victor henrique alves ribeiro, ana ap-pel, and claudio pinhanez.
2020. improving out-of-scope detection in intent classiﬁcation by using em-beddings of the word graph space of the classes.
inproceedings of the 2020 conference on empiricalmethods in natural language processing (emnlp),pages 3952–3961, online.
association for computa-tional linguistics..daniel cer, yinfei yang, sheng-yi kong, nan hua,nicole limtiaco, rhomni st. john, noah constant,mario guajardo-cespedes, steve yuan, chris tar,brian strope, and ray kurzweil.
2018. universalin proceedings ofsentence encoder for english.
the 2018 conference on empirical methods in nat-ural language processing: system demonstrations,pages 169–174, brussels, belgium.
association forcomputational linguistics..y. chen, d. hakkani-t¨ur, and x. he.
2016. zero-shot learning of intent embeddings for expansionby convolutional deep structured semantic models.
in 2016 ieee international conference on acous-tics, speech and signal processing (icassp), pages6045–6049..andrea civan, william jones, predrag klasnja, andharry bruce.
2008. better to organize personal in-formation by folders or by tags?
: the devil is in thedetails.
proceedings of the american society for in-formation science and technology, 45(1):1–13..luc de raedt, r. manhaeve, s. dumancic, thomas de-meester, and a. kimmig.
2019. neuro-symbolic =neural + logical + probabilistic.
in proceedings ofthe 2019 international workshop on neural- sym-bolic learning and reasoning, page 4..emile durkheim and marcel mauss.
1963. primitiveclassiﬁcation, volume 273. university of chicagopress..lisa ehrlinger and wolfram w¨oß.
2016. towards a def-inition of knowledge graphs.
semantics (posters,demos, success), 48:1–4..marco fossati, dimitris kontokostas,.
and jenslehmann.
2015. unsupervised learning of an exten-sive and usable taxonomy for dbpedia.
in proceed-ings of the 11th international conference on seman-tic systems, sem ’15.
acm..artur d’avila garcez, marco gori, luis c. lamb, lu-ciano seraﬁni, michael spranger, and son n. tran.
2019. neural-symbolic computing: an effectivemethodology for principled integration of machinelearning and reasoning.
journal of applied logics,6(4):611–631..drew hudson and christopher d manning.
2019a.
learning by abstraction: the neural state machine.
in advances in neural information processing sys-tems, pages 5903–5916..drew hudson and christopher d manning.
2019b.
learning by abstraction: the neural state machine.
in h. wallach, h. larochelle, a. beygelzimer,f. d'alch´e-buc, e. fox, and r. garnett, editors, ad-vances in neural information processing systems32, pages 5903–5916.
curran associates, inc..shaoxiong ji, shirui pan, erik cambria, pekka mart-tinen, and philip s. yu.
2020. a survey on knowl-edge graphs: representation, acquisition and appli-cations..daniel jurafsky and james h. martin.
2009. speechand language processing (2nd edition).
prentice-hall, inc., usa..dimitri kartsaklis, mohammad taher pilehvar, andnigel collier.
2018. mapping text to knowledgein pro-graph entities using multi-sense lstms.
ceedings of the 2018 conference on empirical meth-ods in natural language processing, pages 1959–1970, brussels, belgium.
association for computa-tional linguistics..stephen m kosslyn.
2006. you can play 20 questionswith nature and win: categorical versus coordinatespatial relations as a case study.
neuropsychologia,44(9):1519–1523..stefan larson, anish mahendran, joseph j. peper,christopher clarke, andrew lee, parker hill,jonathan k. kummerfeld, kevin leach, michael a.laurenzano, lingjia tang, and jason mars.
2019.an evaluation dataset for intent classiﬁcation andtheout-of-scope prediction.
2019 conference on empirical methods in natu-ral language processing and the 9th internationaljoint conference on natural language processing(emnlp-ijcnlp), pages 1311–1316, hong kong,china.
association for computational linguistics..in proceedings of.
7023richard socher, milind ganjoo, christopher d man-ning, and andrew ng.
2013. zero-shot learningthrough cross-modal transfer.
in advances in neu-ral information processing systems, volume 26. cur-ran associates, inc..ming tan, yang yu, haoyu wang, dakuo wang, sa-loni potdar, shiyu chang, and mo yu.
2019. out-of-domain detection for low-resource text classiﬁcationin proceedings of the 2019 conference ontasks.
empirical methods in natural language processingand the 9th international joint conference on natu-ral language processing (emnlp-ijcnlp), pages3566–3572, hong kong, china.
association forcomputational linguistics..joshua b. tenenbaum, charles kemp, thomas l. grif-ﬁths, and noah d. goodman.
2011. how to grow amind: statistics, structure, and abstraction.
science,331(6022):1279–1285..ashish vaswani, noam shazeer, niki parmar, jakobuszkoreit, llion jones, aidan n gomez, łukaszkaiser, and illia polosukhin.
2017. attention is allin advances in neural information pro-you need.
cessing systems, pages 5998–6008..wei wang, vincent w zheng, han yu, and chunyanmiao.
2019. a survey of zero-shot learning: set-tings, methods, and applications.
acm transac-tions on intelligent systems and technology (tist),10(2):1–37..steve whittaker, tara matthews, julian cerruti, hernanbadenes, and john tang.
2011. am i wasting mytime organizing email?
a study of email reﬁnding.
in proceedings of the sigchi conference on hu-man factors in computing systems, chi ’11, page3449–3458, new york, ny, usa.
association forcomputing machinery..bai yang, zhang liping, and zhao fengrong.
2019.in pro-a survey on research of code comment.
ceedings of the 2019 3rd international conferenceon management engineering, software engineeringand service sciences, icmss 2019, page 45–51,new york, ny, usa.
association for computingmachinery..robin manhaeve, sebastijan dumancic, angelika kim-mig, thomas demeester, and luc de raedt.
2018.deepproblog: neural probabilistic logic program-ming.
in s. bengio, h. wallach, h. larochelle,k. grauman, n. cesa-bianchi, and r. garnett, ed-itors, advances in neural information processingsystems 31, pages 3749–3759.
curran associates,inc..christopher manning and hinrich schutze.
1999.foundations of statistical natural language process-ing.
mit press..jiayuan mao, chuang gan, pushmeet kohli, joshua b.tenenbaum, and jiajun wu.
2019.the neuro-interpreting scenes,symbolic concept learner:words, and sentences from natural supervision.
ininternational conference on learning representa-tions..rodney needham.
1979..symbolic classiﬁcation..goodyear publishing company..allen newell.
1973. you can’t play 20 questions withnature and win: projective comments on the papersin w.g.
chase, editor, visualof this symposium.
information processing.
academic press..alessandro oltramari, jonathan francis, cory hen-son, kaixin ma, and ruwan wickramarachchi.
2020. neuro-symbolic architectures for context un-derstanding..mark palatucci, dean pomerleau, geoffrey e hinton,and tom m mitchell.
2009. zero-shot learning withsemantic output codes.
in advances in neural infor-mation processing systems, volume 22. curran as-sociates, inc..emilio parisotto, abdel-rahman mohamed, rishabhsingh, lihong li, dengyong zhou, and pushmeetkohli.
2017. neuro-symbolic program synthesis.
ininternational conference on learning representa-tions (iclr)..bryan perozzi, rami al-rfou, and steven skiena.
2014. deepwalk: online learning of social represen-tations.
in proceedings of the 20th acm sigkddinternational conference on knowledge discoveryand data mining, pages 701–710..claudio santos pinhanez, heloisa candello, paulo cav-alin, mauro carlos pichiliani, ana paula appel, vic-tor henrique alves ribeiro, julio nogima, mairade bayser, melina guerra, henrique ferreira, et al.
2021. integrating machine learning data with sym-bolic knowledge from collaboration practices of cu-in pro-rators to improve conversational systems.
ceedings of the 2021 acm conference on humanfactors in computing systems (chi’21), pages 1–13..whitman richards.
1982. how to play twenty ques-.
tions with nature and win..7024a an algorithm to extract intent.
proto-taxonomies from nameids.
in our previous work (pinhanez et al., 2021) wedescribed an algorithm to mine proto-taxonomiesfrom the nameids of a workspace.
for complete-ness, we include it here.
it consists of three steps:.
1. finding the best separator to split the name.
into a sequence of concepts;.
2. splitting the nameids with the selected sepa-.
rator..3. generating an intent proto-taxonomy usingthe terms split by the best separator as con-cepts and considering consecutive concepts ina nameid as having a link between them..in order to ﬁnd the best separator, our algorithmﬁrst calculates the perplexity of the bag of con-cepts using each separator.
perplexity is a measureof uncertainty for a given sequence of words (orconcepts) appearing in a language model (man-ning and schutze, 1999).
for that, we build prob-abilistic language models based on bigrams usingthe maximum likelihood estimator as in (jurafskyand martin, 2009, chapter 3), and then computethe average perplexity using a standard leave-one-intent-out evaluation scheme.
the separator whichminimizes perplexity is chosen as the separator forthe workspace..b intent proto-taxonomies as a.common practice in chatworks.
in (pinhanez et al., 2021) we evaluated the use ofproto-taxonomies by the curators of the chatworksdataset, by developing a metric, called taxonomyrate, which is the ratio of the number of nameidswhich have a proto-taxonomy embedded in it to thetotal number of intents of the workspace..to determine whether a given nameid has aproto-taxonomy embedded in it, we consideredthree criteria: (1) the nameid has two or more con-cepts; (2) there is at least one other nameid whichhas at least one identical concept at the exact samelevel; and (3) the concept does not appear in allnameids of the workspace in that level.
after all thenameids of a workspace were determined as hav-ing an embedded intent or not, the taxonomy ratewas calculated by considering the ratio between thenumber of intents with a taxonomic structure andthe number of intents of the workspace..figure 7: distribution of the number of workspacesaccording to the taxonomy rate of the 3,840 english(top) and 2,895 portuguese (bottom) workspaces.
no-tice that the x axis is in logarithmic scale..a taxonomy rate of above 10% was observedin 76% of the english workspaces and in 80% theportuguese ones.
almost 52% of all workspaces,in english, and 38% in portuguese, had a taxon-omy rate larger than 50%.
moreover, 16% and 20%of the workspaces, for english and portuguese re-spectively, had a very high taxonomy rate from90% to 100%.
considering both the english andportuguese workspaces, about 70% of them havea taxonomy rate above 20% and more than half ofthe workspaces above 64 intents have a taxonomyof 50% or more..figure 7 shows how the workspaces are dis-tributed considering both the number of intents(x axis) and the taxonomy rate (y axis).
noticethat the distribution, in both languages, follows asort of a “step” function where, as the thresholdbetween 32 and 64 intents is crossed, the majorityof the workspaces has more than 50% of taxonomyrate.
more details can be found in (pinhanez et al.,2021)..c the intent proto-taxonomy created.
for the larson dataset.
in tables 1 and 2, we list the taxonomy which wasmanually created for the larson dataset, with theoriginal nameid on the left and the created taxo-nomic representation on the right, represented as astring of concepts separated by spaces..7025nameidaccept reservationsaccount blockedalarmapplication statusaprare you a botbalancebill balancebill duebook ﬂightbook hotelcalculatorcalendarcalendar updatecaloriescancelcancel reservationcar rentalcard declinedcarry onchange accentchange ai namechange languagechange speedchange user namechange volumeconﬁrm reservationcook timecredit limitcredit limit changecredit scorecurrent locationdamaged carddatedeﬁnitiondirect depositdirectionsdistancedo you have petsexchange rateexpiration dateﬁnd phoneﬂight statusﬂip coinfood lastfreeze accountfun factgasgas typegoodbyegreetinghow busyhow old are youimprove credit scoreincomeingredient substitutioningredients listinsuranceinsurance changeinterest rateinternational feesinternational visajump startlast maintenancelost luggagemake callmaybemeal suggestionmeaning of lifemeasurement conversionmeeting schedulemin paymentmpgnew cardnext holidaynext songnonutrition infooil change howoil change whenorderorder checksorder statuspay billpaydaypin changeplay musicplug typepto balancepto request.
conceptsaccept reservationaccount blockedset alarmapplication statuswhat monthyou botwhat balancewhat bill balancewhen bill duebook ﬂightbook hotelcalculatecalendarcalendar updatecalories dishcancel actioncancel reservationcar rentalcard declinedcarry-on rulechange accentchange bot namechange languagechange speedchange user namechange volumeconﬁrm reservationcook timecredit limitcredit limit changecredit scorewhat current locationdamaged cardwhat datedeﬁnitiondirect depositwhat directionwhat distancedo you have petexchange rateexpiration dateﬁnd phoneﬂight statusﬂip coinfood lastblock accountfun factgas levelgas typegoodbyegreetinghow busyhow old youimprove credit scorewhat incomeingredient substitutioningredient listinsurance beneﬁtinsurance changeinterest rateinternational feeinternational visajump startlast maintenancelost luggagemake callmaybemeal suggestionwhat meaning lifemeasurement conversionmeeting scheduleminimum paymentwhat mpgapply cardnext holidaynext songnonutrition infohow change oilwhen change oilorder shoppingorder checkorder statuspay billwhen paydaychange pinplay musicwhat plug typepto balancepto request.
.
..nameid.
pto request statuspto usedreciperedeem rewardsreminderreminder updaterepeatreplacement card durationreport fraudreport lost cardreset settingsrestaurant reservationrestaurant reviewsrestaurant suggestionrewards balanceroll dicerollover 401kroutingschedule maintenanceschedule meetingshare locationshopping listshopping list updatesmart homespellingspending historysync devicetaxestell joketextthank youtimetimertimezonetire changetire pressuretodo listtodo list updatetrafﬁctransactionstransfertranslatetravel alerttravel notiﬁcationtravel suggestionuberupdate playlistuser namevaccinesw2weatherwhat are your hobbieswhat can i ask youwhat is your namewhat songwhere are you fromwhisper modewho do you work forwho made youyes.
concepts.
.
.
pto request statuspto usedrecipe dishredeem rewardreminder actionreminder updaterepeat actionreplacement card durationreport fraudreport lost cardreset settingrestaurant reservationrestaurant reviewrestaurant suggestionreward balanceroll dicerollover 401kﬁnd routingschedule maintenanceschedule meetingshare locationshopping listshopping list updatesmart homespelling wordspending historysync devicewhat taxestell joketext personthankwhat timeset timerset timezonetire changetire pressuretodo listtodo list updatewhat trafﬁccard transactiontransfer accounttranslate wordtravel alerttravel notiﬁcationtravel suggestionget uberupdate playlistuser namewhat vaccineget w2what weatherwhat hobbywhat ask youwhat namewhat songwhere you fromwhisper modewho you workwho made youyes.
.
..table 2: the taxonomy created for the larson dataset(cont.)..
d results of the proposed algorithms in.
the chatworks dataset.
table 3 shows the results of the proposed three al-gorithms and the baseline for each workspace in thechatworks dataset.
for each metrics, eer, far,and iser, the table shows the average accuracy andthe standard deviation over the experiments madewith the 20 random splits.
the table also lists infor-mation characterizing the workspaces: number ofintents, number of exemplars, the average numberof exemplars per intent, taxonomy rate, the averagedepth of the taxonomy graph, and the number ofdifferent concepts..table 1: the taxonomy created for the larson dataset..7026table 3: the results of the 4 algorithms for each workspace in the chatworks dataset, including the average accu-racy and the standard deviation (20 random splits).
the table also lists information characterizing each workspace..7027