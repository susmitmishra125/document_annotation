dual slot selector via local reliability veriﬁcationfor dialogue state tracking.
jinyu guo1, kai shuang1∗, jijie li1 and zihan wang2.
1state key laboratory of networking and switching technology,beijing university of posts and telecommunications2graduate school of information science and technology, the university of tokyo{guojinyu, shuangk, lijijie}@bupt.edu.cnzwang@tkl.iis.u-tokyo.ac.jp.
abstract.
the goal of dialogue state tracking (dst) is topredict the current dialogue state given all pre-vious dialogue contexts.
existing approachesgenerally predict the dialogue state at everyturn from scratch.
however, the overwhelm-ing majority of the slots in each turn shouldsimply inherit the slot values from the previ-ous turn.
therefore, the mechanism of treat-ing slots equally in each turn not only is in-efﬁcient but also may lead to additional er-rors because of the redundant slot value gen-eration.
to address this problem, we devisethe two-stage dss-dst which consists of thedual slot selector based on the current turndialogue, and the slot value generator basedon the dialogue history.
the dual slot selec-tor determines each slot whether to update slotvalue or to inherit the slot value from the pre-vious turn from two aspects:(1) if there isa strong relationship between it and the cur-rent turn dialogue utterances; (2) if a slot valuewith high reliability can be obtained for itthrough the current turn dialogue.
the slotsselected to be updated are permitted to enterthe slot value generator to update values bya hybrid method, while the other slots directlyinherit the values from the previous turn.
em-pirical results show that our method achieves56.93%, 60.73%, and 58.04% joint accuracyon multiwoz 2.0, multiwoz 2.1, and multi-woz 2.2 datasets respectively and achieves anew state-of-the-art performance with signiﬁ-cant improvements.
1.
1.introduction.
task-oriented dialogue has attracted increasing at-tention in both the research and industry communi-ties.
as a key component in task-oriented dialoguesystems, dialogue state tracking (dst) aims to.
∗ corresponding author.
1code is available at.
https://github.com/guojinyu88/dssdst.
extract user goals or intents and represent them asa compact dialogue state in the form of slot-valuepairs of each turn dialogue.
dst is an essential partof dialogue management in task-oriented dialoguesystems, where the next dialogue system action isselected based on the current dialogue state..early dialogue state tracking approaches ex-tract value for each slot predeﬁned in a singledomain (williams et al., 2014; henderson et al.,2014a,b).
these methods can be directly adaptedto multi-domain conversations by replacing slots ina single domain with domain-slot pairs predeﬁned.
in multi-domain dst, some of the previous worksstudy the scalability of the model (wu et al., 2019),some aim to fully utilizing the dialogue history andcontext (shan et al., 2020; chen et al., 2020a; quanand xiong, 2020), and some attempt to explorethe relationship between different slots (hu et al.,2020; chen et al., 2020b).
nevertheless, existingapproaches generally predict the dialogue state atevery turn from scratch.
the overwhelming major-ity of the slots in each turn should simply inheritthe slot values from the previous turn.
therefore,the mechanism of treating slots equally in eachturn not only is inefﬁcient but also may lead to ad-ditional errors because of the redundant slot valuegeneration..to address this problem, we propose a dss-dstwhich consists of the dual slot selector based onthe current turn dialogue, and the slot value gen-erator based on the dialogue history.
at each turn,all slots are judged by the dual slot selector ﬁrst,and only the selected slots are permitted to enterthe slot value generator to update their slot value,while the other slots directly inherit the slot valuefrom the previous turn.
the dual slot selector isa two-stage judging process.
it consists of a pre-liminary selector and an ultimate selector, whichjointly make a judgment for each slot accordingto the current turn dialogue.
the intuition behind.
proceedingsofthe59thannualmeetingoftheassociationforcomputationallinguisticsandthe11thinternationaljointconferenceonnaturallanguageprocessing,pages139–151august1–6,2021.©2021associationforcomputationallinguistics139this design is that the preliminary selector makesa coarse judgment to exclude most of the irrele-vant slots, and then the ultimate selector makes anintensive judgment for the slots selected by the pre-liminary selector and combines its conﬁdence withthe conﬁdence of the preliminary selector to yieldthe ﬁnal decision.
speciﬁcally, the preliminary se-lector brieﬂy touches on the relationship of currentturn dialogue utterances and each slot.
then theultimate selector obtains a temporary slot valuefor each slot and calculates its reliability.
the ratio-nale for the ultimate selector is that if a slot valuewith high reliability can be obtained through thecurrent turn dialogue, then the slot ought to be up-dated.
eventually, the selected slots enter the slotvalue generator and a hybrid way of the extractivemethod and the classiﬁcation-based method is uti-lized to generate a value according to the currentdialogue utterances and dialogue history..our proposed dss-dst achieves state-of-the-art joint accuracy on three of the most activelystudied datasets: multiwoz 2.0 (budzianowskiet al., 2018), multiwoz 2.1 (eric et al., 2019), andmultiwoz 2.2 (zang et al., 2020) with joint accu-racy of 56.93%, 60.73%, and 58.04%.
the resultsoutperform the previous state-of-the-art by +2.54%,+5.43%, and +6.34%, respectively.
furthermore, aseries of subsequent ablation studies and analysisare conducted to demonstrate the effectiveness ofthe proposed method..our contributions in this paper are three folds:.
• we devise an effective dss-dst which con-sists of the dual slot selector based on thecurrent turn dialogue and the slot value gen-erator based on the dialogue history to allevi-ate the redundant slot value generation..• we propose two complementary conditionsas the base of the judgment, which signiﬁ-cantly improves the performance of the slotselection..• empirical.
results show that our modelachieves state-of-the-art performance with sig-niﬁcant improvements..williams, 2014) or to jointly learn speech under-standing (henderson et al., 2014c; zilka and ju-rcicek, 2015; wen et al., 2017).
with the recentdevelopment of deep learning and representationlearning, most works about dst focus on encod-ing dialogue context with deep neural networks andpredicting a value for each possible slot (xu andhu, 2018; zhong et al., 2018; ren et al., 2018; xieet al., 2018).
for multi-domain dst, slot-valuepairs are extended to domain-slot-value pairs forthe target (ramadan et al., 2018; gao et al., 2019;wu et al., 2019; chen et al., 2020b; hu et al., 2020;heck et al., 2020; zhang et al., 2020a).
these mod-els greatly improve the performance of dst, but themechanism of treating slots equally is inefﬁcientand may lead to additional errors.
som-dst (kimet al., 2020) considered the dialogue state as an ex-plicit ﬁxed-size memory and proposed a selectivelyoverwriting mechanism.
nevertheless, it arguablyhas limitations because it lacks the explicit explo-ration of the relationship between slot selection andlocal dialogue information..on the other hand, dialogue state tracking andmachine reading comprehension (mrc) have simi-larities in many aspects (gao et al., 2020).
in mrctask, unanswerable questions are involved, somestudies pay attention to this topic with straightfor-ward solutions.
(liu et al., 2018) appended anempty word token to the context and added a sim-ple classiﬁcation layer to the reader.
(hu et al.,2019) used two types of auxiliary loss to predictplausible answers and the answerability of the ques-tion.
(zhang et al., 2020c) proposed a retrospectivereader that integrates both sketchy and intensivereading.
(zhang et al., 2020b) proposed a veriﬁerlayer to context embedding weighted by start andend distribution over the context words representa-tions concatenated to [cls] token representationfor bert.
the slot selection and the mechanismof local reliability veriﬁcation in our work are in-spired by the answerability prediction in machinereading comprehension..3 the proposed method.
2 related work.
traditional statistical dialogue state tracking mod-els combine semantics extracted by spoken lan-guage understanding modules to predict the currentdialogue state (williams and young, 2007; thom-son and young, 2010; wang and lemon, 2013;.
figure 1 illustrates the architecture of dss-dst.
dss-dst consists of embedding, dualslot selector, and slot value generator.
in thetask-oriented dialogue system, given a dialoguedial = {(u1, r1); (u2, r2) .
.
.
; (ut , rt )} of tturns where ut represents user utterance and rtrepresents system response of turn t. we deﬁne.
140figure 1: the architecture of the proposed dss-dst model.
the upper part of the ﬁgure is the process betweeneach module.
the four blocks in the lower part of the ﬁgure are the internal structures of the modules with thesame color above.
at each turn, all slots are judged ﬁrst, and the slots selected to be updated are permitted toenter the slot value generator to update slot values, while the other slots directly inherit the slot values from theprevious turn.
the input utterances of the slot value generator are the dialogues of the previous k − 1 turns andthe current turn, while the dual slot selector only utilizes the current turn dialogue as the input utterances..the dialogue state at turn t as bt = {(sj, v jt )|1 ≤j ≤ j}, where sj are the slots, v jt are the corre-sponding slot values, and j is the total number ofsuch slots.
following (lee et al., 2019), we use theterm “slot” to refer to the concatenation of a domainname and a slot name (e.g., “restaurant − f ood”)..3.1 embedding.
we employ the representation of the previous turndialog state bt−1 concatenated to the representa-tion of the current turn dialogue dt as input:.
xt = [cls] ⊕ dt ⊕ bt−1.
(1).
where [cls] is a special token added in frontof every turn input.
following som-dst (kimet al., 2020), we denote the representation of thedialogue at turn t as dt = rt⊕; ⊕ut ⊕ [sep],where rt is the system response and ut is theuser utterance.
; is a special token used to markthe boundary between rt and ut, and [sep] isa special token used to mark the end of a dia-logue turn.
the representation of the dialoguestate at turn t is bt = b1t , wherebjis the representa-tion of the j-th slot-value pair.
− is a special token.
t = [slot]j ⊕ sj ⊕ − ⊕ v j.t ⊕ .
.
.
⊕ bj.
t.used to mark the boundary between a slot and avalue.
[slot]j is a special token that representsthe aggregation information of the j-th slot-valuepair.
we feed a pre-trained albert (lan et al.,2019) encoder with the input xt.
speciﬁcally, theinput text is ﬁrst tokenized into subword tokens.
for each token, the input is the sum of the inputtokens xt and the segment id embeddings.
for thesegment id, we use 0 for the tokens that belong tobt−1 and 1 for the tokens that belong to dt..t.t.the output representation of the encoder is ot ∈, h[slot]jr|xt|×d, and h[cls]∈ rd are the out-puts that correspond to [cls] and [slot]j, re-spectively.
to obtain the representation of eachdialogue and state, we split the ot into ht andh bt−1 as the output representations of the dialogueat turn t and the dialogue state at turn t − 1..3.2 dual slot selector.
the dual slot selector consists of a preliminaryselector and an ultimate selector, which jointlymake a judgment for each slot according to thecurrent turn dialogue..slot-aware matching here we ﬁrst describe theslot-aware matching (sam) layer, which will be.
141total_scoretjtotal_scoretjult_scoretjult_scoretjpre_scoretjpre_scoretjembeddingpreliminary selector<0v  =  v tt-1jjv  =  v tt-1jj>0ultimate selectorfor the j-th slot<δ>δslot value generatorinherit (               )inherit (               )inherit (               )inherit (               )v  =  v tt-1jjv  =  v tt-1jjembeddingdtbt-1ht  preliminary selector[slot]t  j[slot]t  jht  [slot]t  j[slot]t  jsamsoftmax﹥ypre_scorejtjtpre_scorejttjtjht  [slot]t  j[slot]t  jextractortjtjφtjφclassifierult_scorejtult_scorejt∉vj∉vjult_scorejtult_scorejt∈vj∈vjultimate selectorslot value generatordtbt-1dt-1dt-k+1[slot]t  j[slot]t  jht  extractortjtjφ’tjφ’classifier∉vj∉vj∈vj∈vjv  =  tjv  =  tjtjtjφ’tjφ’tjvjvencoderencoderdual slot selectorused as the subsequent components.
the slot canbe regarded as a special category of questions, soinspired by the previous success of explicit atten-tion matching between passage and question inmrc (kadlec et al., 2016; dhingra et al., 2017;wang et al., 2017; seo et al., 2016), we feed a repre-sentation h and the output representation h[slot]jat turn t to the slot-aware matching layer by takingthe slot presentation as the attention to the repre-sentation h:.
t.sam(h, j, t) = softmax(h(h[slot]j.
)(cid:124)).
(2).
t.the output represents the correlation between eachposition of h and the j-th slot at turn t..preliminary selector the preliminary selectorbrieﬂy touches on the relationship of current turndialogue utterances and each slot to make an initialjudgment.
for the j-th slot (1 ≤ j ≤ j) at turn t,we feed its output representation h[slot]jand thedialogue representation ht to the sam as follows:.
t.αj.
t =sam(ht, j, t).
(3)where αjt ∈ rn ×1 denotes the correlation betweeneach position of the dialogue and the j-th slot atturn t. then we get the aggregated dialogue rep-resentation h jt ∈ rn ×d and passed it to a fullyconnected layer to get classiﬁcation the j-th slot’slogits ˆyjt composed of selected (logit selit) and fail(logit faij.
t ) elements as follows:.
h j.t ,m ht,m , 0 ≤ m < n.t , m = αjt = softmax(fc(h jˆyj.
t )).
(4).
(5).
we calculate the difference as the preliminary se-lector score for the j-th slot at turn t: pre scorejt =t −logit faijlogit seljt , and deﬁne the set of the slotindices as u1,t = {j|pre scorejt > 0}, and its sizeas j1,t = |u1,t|.
in the next paragraph, the slot inu1,t will be processed as the target object of theultimate selector..ultimate selector the ultimate selector willmake the judgment on the slots in u1,t.
the mech-anism of the ultimate selector is to obtain a tempo-rary slot value for the slot and calculate its reliabil-ity through the dialogue at turn t as its conﬁdencefor each slot.
speciﬁcally, for the j-th slot in u1,t(1 ≤ j ≤ j1,t), we ﬁrst attempt to obtain the tem-porary slot value ϕjt using the extractive method:we employ two different linear layers and feed ht.
(6).
(7).
(8).
(9).
(10).
(11).
(12).
(13).
(14).
as the input to obtain the representation h st andh et for predicting the start and end, respectively.
then we feed them to the sam with the j-th slotto obtain the correlation representation α sjt andα ej.
t as follows:.
the position of the maximum value in α sjt andt will be the start and end predictions of ϕjα ejt :.
t htt ht.
h st = w sh et = w et = sam(h st, j, t)t = sam(h et, j, t).
α sjα ej.
psj.
t = argmax.
(α sj.
t ,m ).
pej.
t = argmax.
(α ej.
t ,m ).
m.m.t = dialt[psjϕj.
t : pejt ].
if ϕj.
here we deﬁne vj, the candidate value set of thej-th slot.
t belongs to vj, we calculate itsproportion of all possible extracted temporary slotvalues and calculate the ult scorejt as the score ofthe j-th slot:.
logit spanj.
t =.
exp(α sjn −1(cid:80)p2=p1+1.
n −1(cid:80)p1=0.
t [psj.
t ] + α ej.
t [pej.
t ]).
exp(α sj.
t [p1] + α ej.
t [p2]).
logit nullj.
t =.
exp(α sjn −1(cid:80)p2=p1+1.
n −1(cid:80)p1=0.
t [0] + α ej.
t [0]).
exp(α sj.
t [p1] + α ej.
t [p2]).
ult scorej.
t = logit spanj.
t − logit nullj.
t.(15).
if ϕjt does not belong to vj, we employ theclassiﬁcation-based method instead to select a tem-porary slot value from vj.
speciﬁcally, the dia-logue representation h jt is passed to a fully con-nected layer to get the distribution of vj.
wechoose the candidate slot value corresponding tothe maximum value as the new temporary slot valueϕjt , and calculate the distribution probability differ-t and “n one” as the ult scorejence between ϕjt :t = softmax(fc(h j.α cj.
(16).
maxc = argmax.
(α cj.
m.t ))t ,m ).
ult scorej.
t = α cj.
t [maxc] − α cj.
t [0].
(17).
(18).
we choose 0 as index because vj[0] = “n one”..142threshold-based decision following previousstudies (devlin et al., 2019; yang et al., 2019;liu et al., 2019; lan et al., 2019), we adopt thethreshold-based decision to make the ﬁnal judg-ment for each slot in u1,t.
the slot-selected thresh-old δ is set and determined in our model.
thetotal score of the j-th slot is the combination ofthe predicted preliminary selector’s score and thepredicted ultimate selector’s score:.
total scorej.
t = βpre scorej.
t +(1−β)ult scorejt(19)where β is the weight.
we deﬁne the set of the slotindices as u2,t = {j|total scorejt > δ}, and itssize as j2,t = |u2,t|.
the slot in u2,t will enter theslot value generator to update the slot value..3.3 slot value generator.
after the judgment of the dual slot selector, theslots in u2,t are the ﬁnal selected slots.
for eachj-th slot in u2,t, the slot value generator generatesa value for it.
conversely, the slots that are not inu2,t will inherit the slot value of the previous turn(i.e., v it−1, 1 ≤ i ≤ j − j2,t).
for the sakeof simplicity, we sketch the process as follows be-cause this module utilizes the same hybrid way ofthe extractive method and the classiﬁcation-basedmethod as in the ultimate selector:.
t = v i.x gt = [cls] ⊕ dt ⊕ · · · ⊕ dt−k+1 ⊕ bt−1.
h gt = embedding(x gt).
ϕ gj.
t = ext method(h gt), 1 ≤ j ≤ j2,t.
t = ϕ gjv jt ∈ vjt = cls method(h gt) , ϕ gjv j.t , ϕ gj.
t /∈ vj.
(20).
(21).
(22).
(23).
(24).
signiﬁcantly, the biggest difference between theslot value generator and the ultimate selector isthat the input utterances of the slot value gener-ator are the dialogues of the previous k − 1 turnsand the current turn, while the ultimate selectoronly utilizes the current turn dialogue as the inpututterances..3.4 optimization.
during training, we optimize both dual slot selec-tor and slot value generator..1j1,t.
j1,t(cid:88).
j.
1j1,t.
j1,t(cid:88).
|vj |(cid:88).
j.i.preliminary selector we use cross-entropy as atraining objective:.
1j.j(cid:88).
j=1.
lpre,t = −.
[yj.
t log ˆyj.
t + (1 − yj.
t ) log(1 − ˆyi.
t)].
where ˆyjt denotes the prediction and yjindicating whether the slot is selected..(25)t is the target.
ultimate selector the training objectives ofboth extractive method and classiﬁcation-basedmethod are deﬁned as cross-entropy loss:.
lext,t = −.
log(logit pjt ).
(26).
lcls,t = −.
y cj.
t,i log α cj.
t,i.
(27).
where logit pjt is the target indicating the propor-tion of all possible extracted temporary slot val-ues which is calculated according to the form ofequation 13, and y cjt,i is the target indicating theprobability of candidate values..slot value generator the training objectivelgen,t of this module has the same form of trainingobjective as in the ultimate selector..4 experimental setup.
4.1 datasets and metrics.
we choose multiwoz 2.0 (budzianowski et al.,2018), multiwoz 2.1 (eric et al., 2019), and thelatest multiwoz 2.2 (zang et al., 2020) as ourtraining and evaluation datasets.
these are thethree largest publicly available multi-domain task-oriented dialogue datasets, including over 10,000dialogues, 7 domains, and 35 domain-slot pairs.
multiwoz 2.1 ﬁxes the previously existing anno-tation errors.
multiwoz 2.2 is the latest version ofthis dataset.
it identiﬁes and ﬁxes the annotation er-rors of dialogue states on multiwoz2.1, solves theinconsistency of state updates and the problems ofontology, and redeﬁnes the dataset by dividing allslots into two types: non-categorical and categori-cal.
in conclusion, it helps make a fair comparisonbetween different models and will be crucial in thefuture research of this ﬁeld..following trade (wu et al., 2019), we useﬁve domains for training, validation, and testing,including restaurant, train, hotel, taxi, attraction..143these domains contain 30 slots (i.e., j = 30).
weuse joint accuracy and slot accuracy as evaluationmetrics.
joint accuracy refers to the accuracy ofthe dialogue state in each turn.
slot accuracy onlyconsiders individual slot-level accuracy..4.2 baseline models.
we compare the performance of dss-dst with thefollowing competitive baselines:dstreader formulates the problem of dst as anextractive qa task and extracts the value of theslots from the input as a span (gao et al., 2019).
trade encodes the whole dialogue context anddecodes the value for every slot using a copy-augmented decoder (wu et al., 2019).
nadstuses a transformer-based non-autoregressive de-coder to generate the current turn dialogue state (leet al., 2019).
pin integrates an interactive encoderto jointly model the in-turn dependencies and cross-turn dependencies (chen et al., 2020a).
ds-dstuses two bert-base encoders and takes a hybridapproach (zhang et al., 2020a).
sas proposes adialogue state tracker with slot attention and slotinformation sharing to reduce redundant informa-tion’s interference (hu et al., 2020).
som-dstconsiders the dialogue state as an explicit ﬁxed-size memory and proposes a selectively overwrit-ing mechanism (kim et al., 2020).
dst-picklistperforms matchings between candidate values andslot-context encoding by considering all slots aspicklist-based slots (zhang et al., 2020a).
sstproposes a schema-guided multi-domain dialoguestate tracker with graph attention networks (chenet al., 2020b).
trippy extracts all values from thedialog context by three copy mechanisms (hecket al., 2020)..4.3 training.
we employ a pre-trained albert-large-uncasedmodel (lan et al., 2019) for the encoder of eachpart.
the hidden size of the encoder d is 1024.we use adamw optimizer (loshchilov and hutter,2018) and set the warmup proportion to 0.01 andl2 weight decay of 0.01. we set the peak learningrate to 0.03 for the preliminary selector and 0.0001for the ultimate selector and the slot value gener-ator, respectively.
the max-gradient normalizationis utilized and the threshold of gradient clippingis set to 0.1. we use a batch size of 8 and set thedropout (srivastava et al., 2014) rate to 0.1. inaddition, we utilize word dropout (bowman et al.,2016) by randomly replacing the input tokens with.
the special [unk] token with the probability of 0.1.the max sequence length for all inputs is ﬁxed to256..we train the preliminary selector for 10 epochsand train the ultimate selector and the slot valuegenerator for 30 epochs.
during training the slotvalue generator, we use the ground truth selectedslots instead of the predicted ones.
we set k to 2, βto 0.55, and δ to 0. for all experiments, we reportthe mean joint accuracy over 10 different randomseeds to reduce statistical errors..5 experimental results.
5.1 main results.
table 1 shows the joint accuracy and the slot accu-racy of our model and other baselines on the testsets of multiwoz 2.0, 2.1, and 2.2. as shown inthe table, our model achieves state-of-the-art per-formance on three datasets with joint accuracy of56.93%, 60.73%, and 58.04%, which has a sig-niﬁcant improvement over the previous best jointaccuracy.
particularly, the joint accuracy on mul-tiwoz 2.1 beyond 60%.
despite the sparsity ofexperimental result on multiwoz 2.2, our modelstill leads by a large margin in the existing publicmodels.
similar to (kim et al., 2020), our modelachieves higher joint accuracy on multiwoz 2.1than that on multiwoz 2.0. for multiwoz 2.2,the joint accuracy of categorical slots is higher thanthat of non-categorical slots.
this is because weutilize the hybrid way of the extractive method andthe classiﬁcation-based method to treat categoricalslots.
however, we can only utilize the extractivemethod for non-categorical slots since they haveno ontology (i.e., candidate value set)..5.2 ablation study.
pre-trained language model for a fair com-parison, we employ different pre-trained languagemodels with different scales as encoders for train-ing and testing on multiwoz 2.1 dataset.
asshown in table 2, the joint accuracy of other imple-mented albert and bert encoders decreases invarying degrees.
in particular, the joint accuracy ofbert-base-uncased decreased by 1.38%, but stilloutperformed the previous state-of-the-art perfor-mance on multiwoz 2.1. the result demonstratesthe effectiveness of dss-dst..separate slot selector to explore the effective-ness of the preliminary selector and ultimate se-lector respectively, we conduct an ablation study.
144model.
dstreadertradenadstpinds-dstsassom-dstdst-picklistssttrippy.
dss-dst.
multiwoz 2.0.multiwoz 2.1.multiwoz 2.2.joint.
slot.
joint.
slot.
joint.
slot.
cat-joint.
39.4148.6050.5252.44-51.0352.3254.3951.17-56.93(±0.43).
-96.92-97.28-97.20----97.55(±0.05).
36.4045.6049.0448.4051.21-53.6853.3055.2355.3060.73(±0.51).
---97.0297.35--97.40--98.05(±0.06).
-45.40--51.70-----58.04(±0.49).
----------97.66(±0.06).
-62.80--70.60-----76.32(±0.27).
noncat-joint-66.60--70.10-----73.39(±0.32).
table 1: joint accuracy (%) and slot accuracy (%) on the test sets of multiwoz 2.0, 2.1, and 2.2 vs. variousapproaches as reported in the literature.
cat-joint and noncat-joint denote joint accuracy on categorical and non-categorical slots, respectively..pre-trainedlanguage modelour modelbert (large)albert (base)bert (base).
multiwoz 2.1.
60.7360.11 (-0.62)59.98 (-0.75)59.35 (-1.38).
table 2: the ablation study of the dss-dst on themultiwoz 2.1 dataset with joint accuracy (%)..model.
our model-ultimate selector-preliminary selector-above two.
multiwoz 2.160.7358.82 (-1.91)52.22 (-8.51)40.69 (-20.04).
model.
our modeldialogue history†.
multiwoz 2.160.7358.36 (-2.37).
table 4: the ablation study of the dss-dst on the†multiwoz 2.1 dataset with joint accuracy (%).
means attaching the dialogue of the previous turn tothe current turn dialogue as the input of the dual slotselector..k.12 (our model)3.multiwoz 2.153.9660.7359.34.table 3: the ablation study of the dss-dst on themultiwoz 2.1 dataset with joint accuracy (%)..table 5: the joint accuracy (%) of different k on multi-woz 2.1 dataset.
the k represents the dialogue historyof the previous k − 1 turns..of the two slot selectors on multiwoz 2.1. asshown in table 3, we observe that the performanceof the separate preliminary selector is better thanthat of the separate ultimate selector.
this is pre-sumably because the preliminary selector is thehead of the dual slot selector, it is stable whenit handles all slots.
nevertheless, the input of theultimate selector is the slots selected by the prelim-inary selector, and its function is to make a reﬁnedjudgment.
therefore, it will be more vulnerablewhen handling all the slots independently.
in ad-dition, when the two selectors are removed, theperformance drops drastically.
this demonstrates.
that the slot selection is integral before slot valuegeneration..dialogue history for the dual slot selector asaforementioned, we consider that the slot selectiononly depends on the current turn dialogue.
in orderto verify it, we attach the dialogue of the previousturn to the current turn dialogue as the input ofthe dual slot selector.
we observe in table 4 thatthe joint accuracy decreases by 2.37%, which im-plies the redundant information of dialogue historyconfuse the slot selection in the current turn..145our model.
som-dst.
operationinheritupdate.
f1.
f1.
operation99.71 carryover 98.6690.6580.10update32.51delete2.86dontcare.
table 6: statistics of the state operations and the corre-sponding f1 scores of our model and som-dst in thetest set of multiwoz 2.1..multiwoz 2.2.domainattractionhotelrestauranttaxitrain.
joint accuracy (%)79.8862.4775.7954.8476.25.table 7: domain-speciﬁc results on the test set of mul-tiwoz 2.2. we are the ﬁrst to list domain-speciﬁc re-sults on the test set of multiwoz 2.2 to the best of ourknowledge..dialogue history for the slot value generatorwe try the number from one to three for the k toobserve the inﬂuence of the selected dialogue his-tory on the slot value generator.
as shown intable 5, the model achieves better performance onmultiwoz 2.1 when k = 2, 3 than that of k = 1.furtherly, the performance of k = 2 is better thanthat of k = 3. we conjecture that the dialogue his-tory far away from the current turn is little helpfulbecause the relevance between two sentences indialogue is strongly related to their positions..the above ablation studies show that dialoguehistory confuses the dual slot selector, but it playsa crucial role in the slot value generator.
thisdemonstrates that there are fundamental differencesbetween the two processes, and conﬁrms the neces-sity of dividing dst into these two sub-tasks..6 analysis.
6.1 comparative analysis of slot selector.
we analyze the performance of the dual slot se-lector and compare it with other previous work inmultiwoz 2.1. here we choose the som-dst andlist the state operations and the corresponding f1scores as a comparison.
the som-dst sets fourstate operations (i.e., carryover, delete,dontcare, update), while our model clas-siﬁes the slots into two classes (i.e., inherit and.
model.
our model-extractive method.
multiwoz 2.2joint cat-joint58.0450.01.
76.3266.15.table 8: the ablation study of the dss-dst on themultiwoz 2.2 dataset with joint accuracy (%) andjoint accuracy on categorical slots..update).
it means that delete, dontcare,and update in som-dst all correspond toupdate in our model.
as shown in table 6, ourmodel still achieves superior performance whendealing with update slots, which contain dont-care, delete, and other difﬁcult cases..6.2 domains and ontology.
table 7 shows the domain-speciﬁc results of ourmodel on the latest multiwoz 2.2 dataset.
wecan observe that the performance of our model intaxi domain is lower than that of the other fourdomains.
we investigate the dataset and ﬁnd thatall the slots in taxi domain are non-categoricalslots.
this indicates the reason that we can only uti-lize the extractive method for non-categorical slotssince they have no ontology.
furthermore, we testthe performance of using the separate classiﬁcation-based method for categorical slots.
as illustratedin table 8, the joint accuracy of our model andcategorical slots decreased by 8.03% and 10.17%,respectively..7 conclusion.
we introduce an effective two-stage dss-dstwhich consists of the dual slot selector based onthe current turn dialogue, and the slot value gen-erator based on the dialogue history.
the dualslot selector determines each slot whether to up-date or to inherit based on the two conditions.
theslot value generator employs a hybrid method togenerate new values for the slots selected to be up-dated according to the dialogue history.
our modelachieves state-of-the-art performance of 56.93%,60.73%, and 58.04% joint accuracy with signiﬁ-cant improvements (+2.54%, +5.43%, and +6.34%)over previous best results on multiwoz 2.0, multi-woz 2.1, and multiwoz 2.2 datasets, respectively.
the mechanism of a hybrid method is a promis-ing research direction and we will exploit a morecomprehensive and efﬁcient hybrid method for slotvalue generation in the future..146acknowledgements.
research.
development.
supported by the nationalthis work waskeyprojectand(2017yfb1400603) and the foundation forinnovative research groups of the nationalnatural science foundation of china (grant no.
61921003).
we thank the anonymous reviewersfor their insightful comments..ethical considerations.
the claims in this paper match the experimentalresults.
the model utilizes the hybrid method forslot value generation, so it is universal and scal-able to unseen domains, slots, and values.
theexperimental results can be expected to generalize..references.
samuel bowman, luke vilnis, oriol vinyals, andrewdai, rafal jozefowicz, and samy bengio.
2016.generating sentences from a continuous space.
inproceedings of the 20th signll conference oncomputational natural language learning, pages10–21..paweł budzianowski, tsung-hsien wen, bo-hsiangtseng, inigo casanueva, stefan ultes, osman ra-madan, and milica gaˇsi´c.
2018. multiwoz–alarge-scale multi-domain wizard-of-oz dataset forarxiv preprinttask-oriented dialogue modelling.
arxiv:1810.00278..junfan chen, richong zhang, yongyi mao, and jiexu.
2020a.
parallel interactive networks for multi-domain dialogue state generation.
in proceedings ofthe 2020 conference on empirical methods in nat-ural language processing (emnlp), pages 1921–1931..lu chen, boer lv, chi wang, su zhu, bowen tan, andkai yu.
2020b.
schema-guided multi-domain dia-logue state tracking with graph attention neural net-works.
in proceedings of the aaai conference onartiﬁcial intelligence, volume 34, pages 7521–7528..jacob devlin, ming-wei chang, kenton lee, andkristina toutanova.
2019. bert: pre-training ofdeep bidirectional transformers for language under-standing.
in proceedings of the 2019 conference ofthe north american chapter of the association forcomputational linguistics: human language tech-nologies, volume 1 (long and short papers), pages4171–4186..bhuwan dhingra, hanxiao liu, zhilin yang, williamcohen, and ruslan salakhutdinov.
2017. gated-in pro-attention readers for text comprehension.
ceedings of the 55th annual meeting of the associa-tion for computational linguistics (volume 1: longpapers), pages 1832–1846..mihail eric, rahul goel, shachi paul, adarsh ku-mar, abhishek sethi, peter ku, anuj kumargoyal, sanchit agarwal, shuyang gao, and dilekhakkani-tur.
2019. multiwoz 2.1: a consoli-dated multi-domain dialogue dataset with state cor-rections and state tracking baselines.
arxiv preprintarxiv:1907.01669..shuyang gao, sanchit agarwal, di jin, tagyoungchung, and dilek hakkani-tur.
2020. from ma-chine reading comprehension to dialogue state track-in proceedings of the 2nding: bridging the gap.
workshop on natural language processing for con-versational ai, pages 79–89..shuyang gao, abhishek sethi, sanchit agarwal, tagy-oung chung, and dilek hakkani-tur.
2019. dia-log state tracking: a neural reading comprehensionapproach.
in proceedings of the 20th annual sig-dial meeting on discourse and dialogue, pages 264–273..michael heck, carel van niekerk, nurul lubis, chris-tian geishauser, hsien-chin lin, marco moresi, andmilica gasic.
2020. trippy: a triple copy strategyfor value independent neural dialog state tracking.
in proceedings of the 21th annual meeting of thespecial interest group on discourse and dialogue,pages 35–44..matthew henderson, blaise thomson, and jason dwilliams.
2014a.
the second dialog state trackingchallenge.
in proceedings of the 15th annual meet-ing of the special interest group on discourse anddialogue (sigdial), pages 263–272..matthew henderson, blaise thomson, and jason dwilliams.
2014b.
the third dialog state trackingchallenge.
in 2014 ieee spoken language technol-ogy workshop (slt), pages 324–329.
ieee..matthew henderson, blaise thomson, and steveyoung.
2014c.
word-based dialog state trackingin proceedingswith recurrent neural networks.
of the 15th annual meeting of the special inter-est group on discourse and dialogue (sigdial),pages 292–299..jiaying hu, yan yang, chencai chen, zhou yu, et al.
2020. sas: dialogue state tracking via slot attentionand slot information sharing.
in proceedings of the58th annual meeting of the association for compu-tational linguistics, pages 6366–6375..minghao hu, furu wei, yuxing peng, zhen huang,nan yang, and dongsheng li.
2019. read+ verify:machine reading comprehension with unanswerablequestions.
in proceedings of the aaai conferenceon artiﬁcial intelligence, volume 33, pages 6529–6537..rudolf kadlec, martin schmid, ondˇrej bajgar, and jankleindienst.
2016. text understanding with the at-tention sum reader network.
in proceedings of the.
14754th annual meeting of the association for compu-tational linguistics (volume 1: long papers), pages908–918..objective for dialogue state tracking.
in proceedingsof the 58th annual meeting of the association forcomputational linguistics, pages 6322–6333..sungdong kim, sohee yang, gyuwan kim, and sang-woo lee.
2020. efﬁcient dialogue state tracking byselectively overwriting memory.
in proceedings ofthe 58th annual meeting of the association for com-putational linguistics, pages 567–582..nitish srivastava, geoffrey hinton, alex krizhevsky,ilya sutskever, and ruslan salakhutdinov.
2014.dropout: a simple way to prevent neural networksfrom overﬁtting.
the journal of machine learningresearch, 15(1):1929–1958..zhenzhong lan, mingda chen, sebastian goodman,kevin gimpel, piyush sharma, and radu soricut.
2019. albert: a lite bert for self-supervised learn-arxiv preprinting of language representations.
arxiv:1909.11942..hung le, richard socher, and steven ch hoi.
2019.in inter-.
non-autoregressive dialog state tracking.
national conference on learning representations..hwaran lee, jinsik lee, and tae-yoon kim.
2019.sumbt: slot-utterance matching for universal andscalable belief tracking.
in proceedings of the 57thannual meeting of the association for computa-tional linguistics, pages 5478–5483..xiaodong liu, wei li, yuwei fang, aerin kim,kevin duh, and jianfeng gao.
2018. stochasticarxiv preprintanswer networks for squad 2.0.arxiv:1809.09194..yinhan liu, myle ott, naman goyal, jingfei du, man-dar joshi, danqi chen, omer levy, mike lewis,luke zettlemoyer, and veselin stoyanov.
2019.roberta: a robustly optimized bert pretraining ap-proach..ilya loshchilov and frank hutter.
2018. fixing weight.
decay regularization in adam..jun quan and deyi xiong.
2020. modeling long con-text for task-oriented dialogue state generation.
inproceedings of the 58th annual meeting of the asso-ciation for computational linguistics, pages 7119–7124..osman ramadan, paweł budzianowski, and milica ga-sic.
2018. large-scale multi-domain belief track-ing with knowledge sharing.
in proceedings of the56th annual meeting of the association for compu-tational linguistics (volume 2: short papers), pages432–437..liliang ren, kaige xie, lu chen, and kai yu.
2018.in pro-towards universal dialogue state tracking.
ceedings of the 2018 conference on empirical meth-ods in natural language processing, pages 2780–2786..minjoon seo, aniruddha kembhavi, ali farhadi, andhannaneh hajishirzi.
2016. bidirectional attentionﬂow for machine comprehension..yong shan, zekang li, jinchao zhang, fandong meng,yang feng, cheng niu, and jie zhou.
2020. a con-textual hierarchical attention network with adaptive.
blaise thomson and steve young.
2010. bayesianupdate of dialogue state: a pomdp framework forspoken dialogue systems.
computer speech & lan-guage, 24(4):562–588..wenhui wang, nan yang, furu wei, baobao chang,and ming zhou.
2017. gated self-matching net-works for reading comprehension and question an-in proceedings of the 55th annual meet-swering.
ing of the association for computational linguistics(volume 1: long papers), pages 189–198..zhuoran wang and oliver lemon.
2013. a simpleand generic belief tracking mechanism for the dialogstate tracking challenge: on the believability of ob-served information.
in proceedings of the sigdial2013 conference, pages 423–432..tsung-hsien wen, david vandyke, nikola mrksic,milica gasic, lina maria rojas-barahona, pei-haosu, stefan ultes, and steve j young.
2017. anetwork-based end-to-end trainable task-oriented di-alogue system.
in eacl (1)..jason d williams.
2014. web-style ranking and sluin proceed-combination for dialog state tracking.
ings of the 15th annual meeting of the special inter-est group on discourse and dialogue (sigdial),pages 282–291..jason d williams, matthew henderson, antoine raux,blaise thomson, alan black, and deepak ra-machandran.
2014. the dialog state tracking chal-lenge series.
ai magazine, 35(4):121–124..jason d williams and steve young.
2007. partiallyobservable markov decision processes for spokendialog systems.
computer speech & language,21(2):393–422..chien-sheng wu, andrea madotto, ehsan hosseini-asl, caiming xiong, richard socher, and pascalefung.
2019. transferable multi-domain state gen-arxiverator for task-oriented dialogue systems.
preprint arxiv:1905.08743..kaige xie, cheng chang, liliang ren, lu chen, andkai yu.
2018. cost-sensitive active learning for dia-logue state tracking.
in proceedings of the 19th an-nual sigdial meeting on discourse and dialogue,pages 209–213..puyang xu and qi hu.
2018. an end-to-end approachfor handling unknown slot values in dialogue statetracking.
in proceedings of the 56th annual meet-ing of the association for computational linguistics(volume 1: long papers), pages 1448–1457..148zhilin yang, zihang dai, yiming yang, jaime car-bonell, russ r salakhutdinov, and quoc v le.
2019.xlnet: generalized autoregressive pretraining forlanguage understanding.
advances in neural infor-mation processing systems, 32:5753–5763..xiaoxue zang, abhinav rastogi, and jindong chen.
2020. multiwoz 2.2: a dialogue dataset with addi-tional annotation corrections and state tracking base-lines.
in proceedings of the 2nd workshop on nat-ural language processing for conversational ai,pages 109–117..jianguo zhang, kazuma hashimoto, chien-sheng wu,yao wang, s yu philip, richard socher, and caim-ing xiong.
2020a.
find or classify?
dual strategy forslot-value predictions on multi-domain dialog statein proceedings of the ninth joint con-tracking.
ference on lexical and computational semantics,pages 154–167..zhuosheng zhang, yuwei wu, junru zhou, sufengduan, hai zhao, and rui wang.
2020b.
sg-net:syntax-guided machine reading comprehension.
inproceedings of the aaai conference on artiﬁcial in-telligence, volume 34, pages 9636–9643..zhuosheng zhang, junjie yang, and hai zhao.
2020c.
retrospective reader for machine reading compre-hension.
arxiv preprint arxiv:2001.09694..victor zhong, caiming xiong, and richard socher.
2018. global-locally self-attentive encoder for di-alogue state tracking.
in proceedings of the 56th an-nual meeting of the association for computationallinguistics (volume 1: long papers), pages 1458–1467..lukas zilka and filip jurcicek.
2015. incremental lstm-in 2015 ieee workshopbased dialog state tracker.
on automatic speech recognition and understand-ing (asru), pages 757–762.
ieee..149appendices.
testset.
a accuracy per slot on multiwoz 2.2.domain-slotattraction-areaattraction-nameattraction-typehotel-areahotel-book dayhotel-book peoplehotel-book stayhotel-internethotel-namehotel-parkinghotel-price rangehotel-starshotel-typerestaurant-arearestaurant-book dayrestaurant-book peoplerestaurant-book timerestaurant-foodrestaurant-namerestaurant-price rangetaxi-arrive bytaxi-departuretaxi-destinationtaxi-leave attrain-arrive bytrain-book peopletrain-daytrain-departuretrain-destinationtrain-leave at.
our model97.9593.3897.3797.2910010010094.9495.2995.2697.6797.9893.2497.3410010010096.7694.2697.8898.6897.2497.0599.2596.6310099.5998.3298.4894.14.table 9: the detailed results of accuracy (%) per sloton multiwoz 2.2 test set.
we sort them according totheir domains..150b data statistics.
domain.
dialoguestrain valid test.
turnsvalid.
train.
test.
slotsprice range,type,parking,book stay,book day,book people,area, stars,internet,namearea, name,typefood, pricerange, area,name, booktime, bookday, bookpeopleleave at,destination,departure,arrive bydestination,day,departure,arrive by,book people,leave at.
hotel.
3,381.
416.
394.
14,793.
1,781.
1,756.attraction.
2,717.
401.
395.
8,073.
1,220.
1,256.restaurant.
3,813.
438.
437.
15,367.
1,708.
1,726.taxi.
1,654.
207.
195.
4,618.
690.
654.train.
3,103.
484.
494.
12,133.
1,972.
1,976.table 10: data statistics of multiwoz 2.1..151