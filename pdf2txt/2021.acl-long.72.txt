declutr: deep contrastive learning for unsupervised textualrepresentations.
john giorgi1,5,6 osvald nitski2,7 bo wang1,4,6,7,† gary bader1,3,5,†1department of computer science, university of toronto2faculty of applied science and engineering, university of toronto3department of molecular genetics, university of toronto4department of laboratory medicine and pathobiology, university of toronto5terrence donnelly centre for cellular & biomolecular research6vector institute for artiﬁcial intelligence7peter munk cardiac center, university health network†co-senior authors{john.giorgi, osvald.nitski, gary.bader}@mail.utoronto.cabowang@vectorinstitute.ai.
abstract.
sentence embeddings are an important com-ponent of many natural language processing(nlp) systems.
like word embeddings, sen-tence embeddings are typically learned onlarge text corpora and then transferred to var-ious downstream tasks, such as clusteringand retrieval.
unlike word embeddings, thehighest performing solutions for learning sen-tence embeddings require labelled data, limit-ing their usefulness to languages and domainswhere labelled data is abundant.
in this pa-per, we present declutr: deep contrastivelearning for unsupervised textual represen-tations.
inspired by recent advances in deepmetric learning (dml), we carefully design aself-supervised objective for learning univer-sal sentence embeddings that does not requirelabelled training data.
when used to extendthe pretraining of transformer-based languagemodels, our approach closes the performancegap between unsupervised and supervised pre-training for universal sentence encoders.
im-portantly, our experiments suggest that thequality of the learned embeddings scale withboth the number of trainable parameters andthe amount of unlabelled training data.
ourcode and pretrained models are publicly avail-able and can be easily adapted to new domainsor used to embed unseen text.1.
1.introduction.
due to the limited amount of labelled training dataavailable for many natural language processing(nlp) tasks, transfer learning has become ubiq-uitous (ruder et al., 2019).
for some time, transferlearning in nlp was limited to pretrained word em-beddings (mikolov et al., 2013; pennington et al.,.
1https://github.com/johngiorgi/declutr.
2014).
recent work has demonstrated strong trans-fer task performance using pretrained sentence em-beddings.
these ﬁxed-length vectors, often re-ferred to as “universal” sentence embeddings, aretypically learned on large corpora and then trans-ferred to various downstream tasks, such as cluster-ing (e.g.
topic modelling) and retrieval (e.g.
seman-tic search).
indeed, sentence embeddings have be-come an area of focus, and many supervised (con-neau et al., 2017), semi-supervised (subramanianet al., 2018; phang et al., 2018; cer et al., 2018;reimers and gurevych, 2019) and unsupervised(le and mikolov, 2014; jernite et al., 2017; kiroset al., 2015; hill et al., 2016; logeswaran and lee,2018) approaches have been proposed.
however,the highest performing solutions require labelleddata, limiting their usefulness to languages and do-mains where labelled data is abundant.
therefore,closing the performance gap between unsupervisedand supervised universal sentence embedding meth-ods is an important goal..pretraining transformer-based language modelshas become the primary method for learning textualrepresentations from unlabelled corpora (radfordet al., 2018; devlin et al., 2019; dai et al., 2019;yang et al., 2019; liu et al., 2019; clark et al.,2020).
this success has primarily been drivenby masked language modelling (mlm).
this self-supervised token-level objective requires the modelto predict the identity of some randomly masked to-kens from the input sequence.
in addition to mlm,some of these models have mechanisms for learn-ing sentence-level embeddings via self-supervision.
in bert (devlin et al., 2019), a special classiﬁca-tion token is prepended to every input sequence,and its representation is used in a binary classiﬁ-.
proceedingsofthe59thannualmeetingoftheassociationforcomputationallinguisticsandthe11thinternationaljointconferenceonnaturallanguageprocessing,pages879–895august1–6,2021.©2021associationforcomputationallinguistics879cation task to predict whether one textual segmentfollows another in the training corpus, denotednext sentence prediction (nsp).
however, recentwork has called into question the effectiveness ofnsp (conneau and lample, 2019; you et al., 1904;joshi et al., 2020).
in roberta (liu et al., 2019),the authors demonstrated that removing nsp dur-ing pretraining leads to unchanged or even slightlyimproved performance on downstream sentence-level tasks (including semantic text similarity andnatural language inference).
in albert (lanet al., 2020), the authors hypothesize that nspconﬂates topic prediction and coherence predic-tion, and instead propose a sentence-order pre-diction objective (sop), suggesting that it bettermodels inter-sentence coherence.
in preliminaryevaluations, we found that neither objective pro-duces good universal sentence embeddings (seeappendix a).
thus, we propose a simple but ef-fective self-supervised, sentence-level objective in-spired by recent advances in metric learning..metric learning is a type of representationlearning that aims to learn an embedding spacewhere the vector representations of similar dataare mapped close together, and vice versa (lowe,1995; mika et al., 1999; xing et al., 2002).
incomputer vision (cv), deep metric learning (dml)has been widely used for learning visual represen-tations (wohlhart and lepetit, 2015; wen et al.,2016; zhang and saligrama, 2016; bucher et al.,2016; leal-taix´e et al., 2016; tao et al., 2016; yuanet al., 2020; he et al., 2018; grabner et al., 2018;yelamarthi et al., 2018; yu et al., 2018).
gener-ally speaking, dml is approached as follows: a“pretext” task (often self-supervised, e.g.
colouriza-tion or inpainting) is carefully designed and usedto train deep neural networks to generate usefulfeature representations.
here, “useful” means a rep-resentation that is easily adaptable to other down-stream tasks, unknown at training time.
down-stream tasks (e.g.
object recognition) are then usedto evaluate the quality of the learned features (inde-pendent of the model that produced them), often bytraining a linear classiﬁer on the task using thesefeatures as input.
the most successful approach todate has been to design a pretext task for learningwith a pair-based contrastive loss function.
for agiven anchor data point, contrastive losses attemptto make the distance between the anchor and somepositive data points (those that are similar) smallerthan the distance between the anchor and some neg-.
ative data points (those that are dissimilar) (had-sell et al., 2006).
the highest-performing methodsgenerate anchor-positive pairs by randomly aug-menting the same image (e.g.
using crops, ﬂipsand colour distortions); anchor-negative pairs arerandomly chosen, augmented views of different im-ages (bachman et al., 2019; tian et al., 2020; heet al., 2020; chen et al., 2020).
in fact, kong et al.,2020 demonstrate that the mlm and nsp objec-tives are also instances of contrastive learning..inspired by this approach, we propose a self-supervised, contrastive objective that can be usedto pretrain a sentence encoder.
our objective learnsuniversal sentence embeddings by training an en-coder to minimize the distance between the embed-dings of textual segments randomly sampled fromnearby in the same document.
we demonstrateour objective’s effectiveness by using it to extendthe pretraining of a transformer-based languagemodel and obtain state-of-the-art results on sente-val (conneau and kiela, 2018) – a benchmark of28 tasks designed to evaluate universal sentenceembeddings.
our primary contributions are:.
• we propose a self-supervised sentence-levelobjective that can be used alongside mlmto pretrain transformer-based language mod-inducing generalized embeddings forels,sentence- and paragraph-length text withoutany labelled data (subsection 5.1)..• we perform extensive ablations to determinewhich factors are important for learning high-quality embeddings (subsection 5.2)..• we demonstrate that the quality of the learnedembeddings scale with model and data size.
therefore, performance can likely be im-proved simply by collecting more unlabelledtext or using a larger encoder (subsection 5.3)..• we open-source our solution and provide de-tailed instructions for training it on new dataor embedding unseen text.2.
2 related work.
previous works on universal sentence embeddingscan be broadly grouped by whether or not they uselabelled data in their pretraining step(s), which werefer to simply as supervised or semi-supervisedand unsupervised, respectively..2https://github.com/johngiorgi/declutr.
880supervised or semi-supervised the highest per-forming universal sentence encoders are pretrainedon the human-labelled natural language inference(nli) datasets stanford nli (snli) (bowmanet al., 2015) and multinli (williams et al., 2018).
nli is the task of classifying a pair of sentences (de-noted the “hypothesis” and the “premise”) into oneof three relationships: entailment, contradictionor neutral.
the effectiveness of nli for traininguniversal sentence encoders was demonstrated bythe supervised method infersent (conneau et al.,2017).
universal sentence encoder (use) (ceret al., 2018) is semi-supervised, augmenting an un-supervised, skip-thoughts-like task (kiros et al.
2015, see section 2) with supervised training onthe snli corpus.
the recently published sen-tence transformers (reimers and gurevych, 2019)method ﬁne-tunes pretrained, transformer-basedlanguage models like bert (devlin et al., 2019)using labelled nli datasets..unsupervised skip-thoughts(kiros et al.,2015) and fastsent (hill et al., 2016) are popularunsupervised techniques that learn sentence em-beddings by using an encoding of a sentence topredict words in neighbouring sentences.
however,in addition to being computationally expensive, thisgenerative objective forces the model to reconstructthe surface form of a sentence, which may captureinformation irrelevant to the meaning of a sentence.
quickthoughts (logeswaran and lee, 2018) ad-dresses these shortcomings with a simple discrim-inative objective; given a sentence and its context(adjacent sentences), it learns sentence represen-tations by training a classiﬁer to distinguish con-text sentences from non-context sentences.
theunifying theme of unsupervised approaches is thatthey exploit the “distributional hypothesis”, namelythat the meaning of a word (and by extension, asentence) is characterized by the word context inwhich it appears..our overall approach is most similar to sen-tence transformers – we extend the pretrainingof a transformer-based language model to produceuseful sentence embeddings – but our proposedobjective is self-supervised.
removing the depen-dence on labelled data allows us to exploit the vastamount of unlabelled text on the web without beingrestricted to languages or domains where labelleddata is plentiful (e.g.
english wikipedia).
ourobjective most closely resembles quickthoughts;some distinctions include: we relax our sampling to.
figure 1: overview of the self-supervised contrastive(a) for each document d in a minibatchobjective.
of size n , we sample a anchor spans per documentand p positive spans per anchor.
for simplicity, weillustrate the case where a = p = 1 and denotethe anchor-positive span pair as si, sj.
both spansare fed through the same encoder f (·) and poolerg(·) to produce the corresponding embeddings ei =g(f (si)), ej = g(f (sj)).
the encoder and pooler aretrained to minimize the distance between embeddingsvia a contrastive prediction task, where the other em-beddings in a minibatch are treated as negatives (omit-ted here for simplicity).
(b) positive spans can overlapwith, be adjacent to or be subsumed by the sampledanchor span.
(c) the length of anchors and positivesare randomly sampled from beta distributions, skewedtoward longer and shorter spans, respectively..textual segments of up to paragraph length (ratherthan natural sentences), we sample one or morepositive segments per anchor (rather than strictlyone), and we allow these segments to be adjacent,overlapping or subsuming (rather than strictly adja-cent; see figure 1, b)..3 model.
3.1 self-supervised contrastive loss.
our method learns textual representations via acontrastive loss by maximizing agreement betweentextual segments (referred to as “spans” in the restof the paper) sampled from nearby in the samedocument.
illustrated in figure 1, this approachcomprises the following components:.
• a data loading step randomly samples pairedanchor-positive spans from each document ina minibatch of size n .
let a be the number ofanchor spans sampled per document, p be thenumber of positive spans sampled per anchorand i ∈ {1 .
.
.
an } be the index of an arbi-trary anchor span.
we denote an anchor span.
881and its corresponding p ∈ {1 .
.
.
p } positivespans as si and si+pan respectively.
this pro-cedure is designed to maximize the chance ofsampling semantically similar anchor-positivepairs (see subsection 3.2)..• an encoder f (·) maps each token in the inputspans to an embedding.
although our methodplaces no constraints on the choice of encoder,we chose f (·) to be a transformer-based lan-guage model, as this represents the state-of-the-art for text encoders (see subsection 3.3)..• a pooler g(·) maps the encoded spans f (si)and f (si+pan ) to ﬁxed-length embeddingsei = g(f (si)) and its corresponding meanpositive embedding.
ei+an =.
g(f (si+pan )).
1p.p(cid:88).
p=1.
similar to reimers and gurevych 2019, wefound that choosing g(·) to be the mean ofthe token-level embeddings (referred to as“mean pooling” in the rest of the paper) per-forms well (see appendix, table 4).
we paireach anchor embedding with the mean ofmultiple positive embeddings.
this strategywas proposed by saunshi et al.
2019, whodemonstrated theoretical and empirical im-provements compared to using a single posi-tive example for each anchor..• a contrastive loss function deﬁned for a con-trastive prediction task.
given a set of embed-ded spans {ek} including a positive pair of ex-amples ei and ei+an , the contrastive predic-tion task aims to identify ei+an in {ek}k(cid:54)=ifor a given ei.
during training, we randomly sample mini-batches of n documents from the train set anddeﬁne the contrastive prediction task on anchor-positive pairs ei, ei+an derived from the n docu-ments, resulting in 2an data points.
as proposedin (sohn, 2016), we treat the other 2(an − 1) in-stances within a minibatch as negative examples.
the cost function takes the following form.
lcontrastive =.
(cid:96)(i, i + an ) + (cid:96)(i + an, i).
an(cid:88).
i=1.
this is the infonce loss used in previous works(sohn, 2016; wu et al., 2018; oord et al., 2018)and denoted normalized temperature-scale cross-entropy loss or “nt-xent” in (chen et al., 2020).
to embed text with a trained model, we simplypass batches of tokenized text through the model,without sampling spans.
therefore, the computa-tional cost of our method at test time is the cost ofthe encoder, f (·), plus the cost of the pooler, g(·),which is negligible when using mean pooling..3.2 span sampling.
we start by choosing a minimum and maxi-mum span length; in this paper, (cid:96)min = 32 and(cid:96)max = 512, the maximum input size for manypretrained transformers.
next, a document d is to-kenized to produce a sequence of n tokens xd =(x1, x2 .
.
.
xn).
to sample an anchor span si fromxd, we ﬁrst sample its length (cid:96)anchor from a betadistribution and then randomly (uniformly) sampleits starting position sstart.
i.
(cid:96)anchor = (cid:4)panchor × ((cid:96)max − (cid:96)min) + (cid:96)minsstarti ∼ {0, .
.
.
, n − (cid:96)anchor}i = sstartsendi + (cid:96)anchorsi = xd.
(cid:5).
sstarti.:sendi.
(cid:96)(i, j) = − log.
exp(sim(ei, ej)/τ )k=1 1[i(cid:54)=k] · exp(sim(ei, ek)/τ ).
(cid:80)2an.
we then sample p ∈ {1 .
.
.
p } corresponding posi-tive spans si+pan independently following a simi-lar procedure.
where sim(u, v) = ut v/||u||2||v||2 denotesthe cosine similarity of two vectors u andv, 1[i(cid:54)=k] ∈ {0, 1} is an indicator functionevaluating to 1 if i (cid:54)= k, and τ > 0 denotesthe temperature hyperparameter..(cid:5).
(cid:96)positive = (cid:4)ppositive × ((cid:96)max − (cid:96)min) + (cid:96)minsstarti+pan ∼ {sstarti+pan = sstartsendsi+pan = xd.
i − (cid:96)positive, .
.
.
, sendi }i+pan + (cid:96)positive.
i+pan :sendsstart.
i+pan.
882where panchor ∼ beta(α = 4, β = 2), whichskews anchor sampling towards longer spans, andppositive ∼ beta(α = 2, β = 4), which skewspositive sampling towards shorter spans (figure 1,c).
in practice, we restrict the sampling of anchorspans from the same document such that they are aminimum of 2 ∗ (cid:96)max tokens apart.
in appendix b,we show examples of text that has been sampled byour method.
we note several carefully considereddecisions in the design of our sampling procedure:.
• sampling span lengths from a distributionclipped at (cid:96)min = 32 and (cid:96)max = 512 encour-ages the model to produce good embeddingsfor text ranging from sentence- to paragraph-length.
at test time, we expect our model tobe able to embed up-to paragraph-length texts..• we found that sampling longer lengths for theanchor span than the positive spans improvesperformance in downstream tasks (we did notﬁnd performance to be sensitive to the speciﬁcchoice of α and β).
the rationale for this istwofold.
first, it enables the model to learnglobal-to-local view prediction as in (hjelmet al., 2019; bachman et al., 2019; chen et al.,2020) (referred to as “subsumed view” in fig-ure 1, b).
second, when p > 1, it encouragesdiversity among positives spans by loweringthe amount of repeated text..• sampling positives nearby to the anchor ex-ploits the distributional hypothesis and in-creases the chances of sampling valid (i.e.
se-mantically similar) anchor-positive pairs..• by sampling multiple anchors per document,each anchor-positive pair is contrasted againstboth easy negatives (anchors and positivessampled from other documents in a mini-batch) and hard negatives (anchors and posi-tives sampled from the same document)..in conclusion, the sampling procedure producesthree types of positives: positives that partiallyoverlap with the anchor, positives adjacent to theanchor, and positives subsumed by the anchor (fig-ure 1, b) and two types of negatives: easy nega-tives sampled from a different document than theanchor, and hard negatives sampled from the samedocument as the anchor.
thus, our stochasticallygenerated training set and contrastive loss implic-itly deﬁne a family of predictive tasks which can be.
used to train a model, independent of any speciﬁcencoder architecture..3.3 continued mlm pretraining.
we use our objective to extend the pretraining of atransformer-based language model (vaswani et al.,2017), as this represents the state-of-the-art encoderin nlp.
we implement the mlm objective as de-scribed in (devlin et al., 2019) on each anchor spanin a minibatch and sum the losses from the mlmand contrastive objectives before backpropagating.
l = lcontrastive + lmlm.
this is similar to existing pretraining strategies,where an mlm loss is paired with a sentence-levelloss such as nsp (devlin et al., 2019) or sop (lanet al., 2020).
to make the computational require-ments feasible, we do not train from scratch, butrather we continue training a model that has beenpretrained with the mlm objective.
speciﬁcally,we use both roberta-base (liu et al., 2019) anddistilroberta (sanh et al., 2019) (a distilled ver-insion of roberta-base) in our experiments.
the rest of the paper, we refer to our method asdeclutr-small (when extending distilrobertapretraining) and declutr-base (when extendingroberta-base pretraining)..4 experimental setup.
4.1 dataset, training, and implementation.
dataset we collected all documents with a min-imum token length of 2048 from openwebtext(gokaslan and cohen, 2019) an open-access sub-set of the webtext corpus (radford et al., 2019),yielding 497,868 documents in total.
for refer-ence, google’s use was trained on 570,000 human-labelled sentence pairs from the snli dataset(among other unlabelled datasets).
infersent andsentence transformer models were trained on bothsnli and multinli, a total of 1 million human-labelled sentence pairs..implementation we implemented our model inpytorch (paszke et al., 2017) using allennlp(gardner et al., 2018).
we used the nt-xentloss function implemented by the pytorch met-ric learning library (musgrave et al., 2019) andthe pretrained transformer architecture and weightsfrom the transformers library (wolf et al., 2020).
all models were trained on up to four nvidiatesla v100 16 or 32gb gpus..883table 1: trainable model parameter counts and sen-tence embedding dimensions.
declutr-small anddeclutr-base are pretrained distilroberta androberta-base models respectively after continuedpretraining with our method..parameters embedding dim..bag-of-words (bow) baselines.
model.
glovefasttext.
supervised and semi-supervised.
infersentuniversal sentence encodersentence transformers.
unsupervised.
quickthoughtsdeclutr-smalldeclutr-base.
––.
38m147m125m.
73m82m125m.
300300.
4096512768.
4800768768.training unless speciﬁed otherwise, we trainfor one to three epochs over the 497,868 docu-ments with a minibatch size of 16 and a temper-ature τ = 5 × 10−2 using the adamw optimizer(loshchilov and hutter, 2019) with a learning rate(lr) of 5 × 10−5 and a weight decay of 0.1. forevery document in a minibatch, we sample twoanchor spans (a = 2) and two positive spans peranchor (p = 2).
we use the slanted triangular lrscheduler (howard and ruder, 2018) with a num-ber of train steps equal to training instances and acut fraction of 0.1. the remaining hyperparame-ters of the underlying pretrained transformer (i.e.
distilroberta or roberta-base) are left at theirdefaults.
all gradients are scaled to a vector normof 1.0 before backpropagating.
hyperparameterswere tuned on the senteval validation sets..4.2 evaluation.
we evaluate all methods on the senteval bench-mark, a widely-used toolkit for evaluating general-purpose, ﬁxed-length sentence representations.
senteval is divided into 18 downstream tasks – rep-resentative nlp tasks such as sentiment analysis,natural language inference, paraphrase detectionand image-caption retrieval – and ten probing tasks,which are designed to evaluate what linguistic prop-erties are encoded in a sentence representation.
wereport scores obtained by our model and the rel-evant baselines on the downstream and probingtasks using the senteval toolkit3 with default pa-rameters (see appendix c for details).
note that.
3https://github.com/facebookresearch/.
senteval.
all the supervised approaches we compare to aretrained on the snli corpus, which is included asa downstream task in senteval.
to avoid train-testcontamination, we compute average downstreamscores without considering snli when comparingto these approaches in table 2..4.2.1 baselineswe compare to the highest performing, mostpopular sentence embedding methods: infersent,google’s use and sentence transformers.
for in-fersent, we compare to the latest model.4 we usethe latest “large” use model5, as it is most similarin terms of architecture and number of parametersto declutr-base.
for sentence transformers,we compare to “roberta-base-nli-mean-tokens”6,which, like declutr-base, uses the roberta-base architecture and pretrained weights.
the onlydifference is each method’s extended pretrainingstrategy.
we include the performance of averagedglove7 and fasttext8 word vectors as weak base-lines.
trainable model parameter counts and sen-tence embedding dimensions are listed in table 1.despite our best efforts, we could not evaluate thepretrained quickthought models against the fullsenteval benchmark.
we cite the scores from thepaper directly.
finally, we evaluate the pretrainedtransformer model’s performance before it is sub-jected to training with our contrastive objective,denoted “transformer-*”.
we use mean pooling onthe pretrained transformers token-level output toproduce sentence embeddings – the same poolingstrategy used in our method..5 results.
in subsection 5.1, we compare the performance ofour model against the relevant baselines.
in theremaining sections, we explore which componentscontribute to the quality of the learned embeddings..5.1 comparison to baselines.
downstream task performance compared tothe underlying pretrained models distilroberta.
4https://dl.fbaipublicfiles.com/.
infersent/infersent2.pkl.
5https://tfhub.dev/google/.
universal-sentence-encoder-large/5.
6https://www.sbert.net/docs/.
pretrained_models.html.
7http://nlp.stanford.edu/data/glove..840b.300d.zip.
8https://dl.fbaipublicfiles.com/.
fasttext/vectors-english/crawl-300d-2m.
vec.zip.
884table 2: results on the downstream tasks from the test set of senteval.
quickthoughts scores are taken di-rectly from (logeswaran and lee, 2018).
use: google’s universal sentence encoder.
transformer-small andtransformer-base are pretrained distilroberta and roberta-base models respectively, using mean pooling.
declutr-small and declutr-base are pretrained distilroberta and roberta-base models respectively af-ter continued pretraining with our method.
average scores across all tasks, excluding snli, are shown in the tophalf of the table.
bold: best scores.
∆: difference to declutr-base average score.
↑ and ↓ denote increased ordecreased performance with respect to the underlying pretrained model.
*: unsupervised evaluations..model.
cr.
mr.mpqa.
subj.
sst2.
sst5.
trec.
mrpc.
snli.
avg..∆.
glovefasttext.
78.7879.18.
77.7078.45.
87.7687.88.
91.2591.53.
80.2982.15.
44.4845.16.
83.0083.60.
73.39/81.4574.49/82.44.
65.8568.79.
65.4768.56.
-13.63-10.54.bag-of-words (bow) weak baselines.
supervised and semi-supervised.
infersentusesent.
transformers.
84.3785.7090.78.
79.4279.3884.98.
89.0488.8988.72.
93.0393.1192.67.
84.2484.9090.55.
45.3446.1152.76.
90.8095.0087.40.
76.35/83.4872.41/82.0176.64/82.99.
84.1683.2584.18.
76.0078.8977.19.model.
sick-e sick-r sts-b.
coco sts12* sts13* sts14*.
sts15*.
sts16*.
unsupervised.
quickthoughtstransformer-smalltransformer-basedeclutr-smalldeclutr-base.
glovefasttextinfersentusesent.
transformersquickthoughtstransformer-smalltransformer-basedeclutr-smalldeclutr-base.
86.0086.6088.1987.52 ↑90.68 ↑.
78.8979.0186.3085.3782.97–81.9680.2983.46 ↑83.84 ↑.
82.4082.1284.3582.79 ↑85.16 ↑.
72.3072.9883.0681.5379.17–77.5176.8477.66 ↑78.62 ↑.
90.2087.0486.4987.87 ↑88.52 ↑.
94.8094.7795.2894.96 ↑95.78 ↑.
87.6088.0389.4687.64 ↓90.01 ↑.
62.8668.2678.4881.5074.28–70.3169.6277.51 ↑79.39 ↑.
0.400.4065.8462.4260.9660.5560.4860.1460.85 ↑62.35 ↑.
53.4458.8562.9068.8764.10–53.9953.2863.66 ↑63.56 ↑.
–49.5051.2748.42 ↓51.18 ↓.
51.2458.8356.0871.7065.63–45.5346.1068.93 ↑72.58 ↑.
92.4091.6093.2090.80 ↓93.20 ↑.
55.7163.4266.3672.7669.80–57.2356.1770.40 ↑71.70 ↑.
76.90/84.0074.55/81.7574.20/81.4475.36/82.70 ↑74.61/82.65 ↑.
–71.8872.1973.59 ↑74.74 ↑.
–72.5872.7077.50 ↑79.10 ↑.
59.6269.0574.0183.8874.71.
–.
65.5764.6978.25 ↑79.95 ↑.
57.9368.2472.8982.7872.85–63.5162.7977.74 ↑79.59 ↑.
––––––––––.
table 3: results on the probing tasks from the test set of senteval.
use: google’s universal sentence encoder.
transformer-small and transformer-base are pretrained distilroberta and roberta-base models respectively,using mean pooling.
declutr-small and declutr-base are pretrained distilroberta and roberta-basemodels respectively after continued pretraining with our method.
bold: best scores.
↑ and ↓ denote increased ordecreased performance with respect to the underlying pretrained model..model.
glovefasttext.
sentlen wc.
treedepth topconst bshift.
tense.
subjnum objnum somo coordinv avg..57.8255.46.
81.1082.10.
31.4132.74.
49.7450.16.
83.5886.68.
78.3979.75.
76.3179.81.
49.5550.21.
53.6251.41.infersentusesent.
transformers.
78.7673.1469.21.
89.5069.4451.79.
37.7230.8730.08.
61.4158.8869.70.
88.5683.8183.02.
86.8380.3479.74.
83.9179.1477.85.
52.1156.9760.10.
66.8861.1360.33.transformer-smalltransformer-basedeclutr-small (ours)declutr-base (ours).
88.6281.9688.85 ↑84.62 ↑.
65.0059.6774.87 ↑68.98 ↑.
40.8738.8438.48 ↓38.35 ↓.
75.3874.0275.17 ↓74.78 ↑.
88.6390.0886.12 ↓87.85 ↓.
87.8488.5988.71 ↑88.82 ↑.
86.6885.5186.31 ↓86.56 ↑.
84.1783.3384.30 ↑83.88 ↑.
63.7568.5461.27 ↓65.08 ↓.
64.7871.3262.98 ↓67.54 ↓.
74.5774.1974.71 ↑74.65 ↑.
bag-of-words (bow) weak baselines.
supervised and semi-supervised.
62.7063.32.
80.1673.2750.38.unsupervised.
-3.10-0.21-1.91.
–-6.52-6.40-1.60–.
––––––––––.
62.4263.16.
72.5866.7063.22.and roberta-base, declutr-smallanddeclutr-base obtain large boosts in averagedownstream performance, +4% and +6% respec-tively (table 2).
declutr-base leads to improvedor equivalent performance for every downstreamtask but one (sst5) and declutr-small for allbut three (sst2, sst5 and trec).
compared.
to existing methods, declutr-base matches oreven outperforms average performance withoutusing any hand-labelled training data.
surprisingly,we also ﬁnd that declutr-small outperformssentence transformers while using ∼34% lesstrainable parameters..885probing task performance with the exceptionof infersent, existing methods perform poorly onthe probing tasks of senteval (table 3).
sentencetransformers, which begins with a pretrained trans-former model and ﬁne-tunes it on nli datasets,scores approximately 10% lower on the probingtasks than the model it ﬁne-tunes.
in contrast, bothdeclutr-small and declutr-base performcomparably to the underlying pretrained model interms of average performance.
we note that thepurpose of the probing tasks is not the developmentof ad-hoc models that attain top performance onthem (conneau et al., 2018).
however, it is still in-teresting to note that high downstream task perfor-mance can be obtained without sacriﬁcing probingtask performance.
furthermore, these results sug-gest that ﬁne-tuning transformer-based languagemodels on nli datasets may discard some of thelinguistic information captured by the pretrainedmodel’s weights.
we suspect that the inclusion ofmlm in our training objective is responsible fordeclutr’s relatively high performance on theprobing tasks..supervised vs. unsupervised downstream tasksthe downstream evaluation of senteval includessupervised and unsupervised tasks.
in the unsu-pervised tasks, the embeddings of the method toevaluate are used as-is without any further train-ing (see appendix c for details).
interestingly, weﬁnd that use performs particularly well acrossthe unsupervised evaluations in senteval (tasksmarked with a * in table 2).
given the similarityof the use architecture to sentence transformersand declutr and the similarity of its supervisednli training objective to infersent and sentencetransformers, we suspect the most likely cause isone or more of its additional training objectives.
these include a conversational response predictiontask (henderson et al., 2017) and a skip-thoughts(kiros et al., 2015) like task..5.2 ablation of the sampling procedure.
we ablate several components of the sampling pro-cedure, including the number of anchors sampledper document a, the number of positives sampledper anchor p , and the sampling strategy for thosepositives (figure 2).
we note that when a = 2, themodel is trained on twice the number of spans andtwice the effective batch size (2an , where n is thenumber of documents in a minibatch) as comparedto when a = 1. to control for this, all experi-.
figure 2: effect of the number of anchor spans sampledper document (a), the number of positive spans sampledper anchor (b), and the sampling strategy (c).
averageddownstream task scores are reported from the valida-tion set of senteval.
performance is computed overa grid of hyperparameters and plotted as a distribution.
the grid is deﬁned by all permutations of number of an-chors a = {1, 2}, number of positives p = {1, 2, 4},temperatures τ = {5 × 10−3, 1 × 10−2, 5 × 10−2} andlearning rates α = {5 × 10−5, 1 × 10−4}.
p = 4 isomitted for declutr-base as these experiments didnot ﬁt into gpu memory..ments where a = 1 are trained for two epochs(twice the number of epochs as when a = 2) andfor two times the minibatch size (2n ).
thus, bothsets of experiments are trained on the same numberof spans and the same effective batch size (4n ),and the only difference is the number of anchorssampled per document (a)..we ﬁnd that sampling multiple anchors per doc-ument has a large positive impact on the qual-ity of learned embeddings.
we hypothesize thisis because the difﬁculty of the contrastive objec-tive increases when a > 1. recall that a mini-batch is composed of random documents, and eachanchor-positive pair sampled from a document iscontrasted against all other anchor-positive pairsin the minibatch.
when a > 1, anchor-positivepairs will be contrasted against other anchors andpositives from the same document, increasing thedifﬁculty of the contrastive objective, thus lead-ing to better representations.
we also ﬁnd that apositive sampling strategy that allows positives tobe adjacent to and subsumed by the anchor out-performs a strategy that only allows adjacent orsubsuming views, suggesting that the informationcaptured by these views is complementary.
finally,we note that sampling multiple positives per anchor(p > 1) has minimal impact on performance.
thisis in contrast to (saunshi et al., 2019), who foundboth theoretical and empirical improvements when.
886val benchmark, which contains a total of 28 tasksdesigned to evaluate the transferability and linguis-tic properties of sentence representations.
whenused to extend the pretraining of a transformer-based language model, our self-supervised objec-tive closes the performance gap with existing meth-ods that require human-labelled training data.
ourexperiments suggest that the learned embeddings’quality can be further improved by increasing themodel and train set size.
together, these resultsdemonstrate the effectiveness and feasibility of re-placing hand-labelled data with carefully designedself-supervised objectives for learning universalsentence embeddings.
we release our model andcode publicly in the hopes that it will be extendedto new domains and non-english languages..acknowledgments.
in.
part.
enabled.
research wasprovided.
thisbycompute ontariobysupport(https://computeontario.ca/), compute canada(www.computecanada.ca) and the cifar aichairs program and partially funded by theus national institutes of health (nih) [u41hg006623, u41 hg003751)..references.
eneko agirre, carmen banea, claire cardie, danielcer, mona diab, aitor gonzalez-agirre, weiweiguo, i˜nigo lopez-gazpio, montse maritxalar, radamihalcea, german rigau, larraitz uria, and janycewiebe.
2015. semeval-2015 task 2: semantic tex-tual similarity, english, spanish and pilot on inter-pretability.
in proceedings of the 9th internationalworkshop on semantic evaluation (semeval 2015),pages 252–263, denver, colorado.
association forcomputational linguistics..eneko agirre, carmen banea, claire cardie, danielcer, mona diab, aitor gonzalez-agirre, weiweiguo, rada mihalcea, german rigau, and janycewiebe.
2014. semeval-2014 task 10: multilingualin proceedings of thesemantic textual similarity.
8th international workshop on semantic evaluation(semeval 2014), pages 81–91, dublin, ireland.
as-sociation for computational linguistics..eneko agirre, carmen banea, daniel cer, mona diab,aitor gonzalez-agirre, rada mihalcea, germanrigau, and janyce wiebe.
2016. semeval-2016task 1: semantic textual similarity, monolingualand cross-lingual evaluation.
in proceedings of the10th international workshop on semantic evalua-tion (semeval-2016), pages 497–511, san diego,california.
association for computational linguis-tics..figure 3: effect of training objective, train set size andmodel capacity on senteval performance.
declutr-small has 6 layers and ∼82m parameters.
declutr-base has 12 layers and ∼125m parameters.
averageddownstream task scores are reported from the valida-tion set of senteval.
100% corresponds to 1 epochof training with all 497,868 documents from our open-webtext subset..multiple positives are averaged and paired with agiven anchor..5.3 training objective, train set size and.
model capacity.
to determine the importance of the training objec-tives, train set size, and model capacity, we trainedtwo sizes of the model with 10% to 100% (1 fullepoch) of the train set (figure 3).
pretraining themodel with both the mlm and contrastive objec-tives improves performance over training with ei-ther objective alone.
including mlm alongsidethe contrastive objective leads to monotonic im-provement as the train set size is increased.
wehypothesize that including the mlm loss acts asa form of regularization, preventing the weightsof the pretrained model (which itself was trainedwith an mlm loss) from diverging too dramati-cally, a phenomenon known as “catastrophic for-getting” (mccloskey and cohen, 1989; ratcliff,1990).
these results suggest that the quality of em-beddings learned by our approach scale in terms ofmodel capacity and train set size; because the train-ing method is completely self-supervised, scalingthe train set would simply involve collecting moreunlabelled text..6 discussion and conclusion.
in this paper, we proposed a self-supervised ob-jective for learning universal sentence embeddings.
our objective does not require labelled trainingdata and is applicable to any text encoder.
wedemonstrated the effectiveness of our objective byevaluating the learned embeddings on the sente-.
887eneko agirre, daniel cer, mona diab, and aitorgonzalez-agirre.
2012. semeval-2012 task 6: apilot on semantic textual similarity.
in *sem 2012:the first joint conference on lexical and compu-tational semantics – volume 1: proceedings of themain conference and the shared task, and volume2: proceedings of the sixth international workshopon semantic evaluation (semeval 2012), pages 385–393, montr´eal, canada.
association for computa-tional linguistics..eneko agirre, daniel cer, mona diab, aitor gonzalez-agirre, and weiwei guo.
2013.
*sem 2013 sharedin second jointtask: semantic textual similarity.
conference on lexical and computational seman-tics (*sem), volume 1: proceedings of the mainconference and the shared task: semantic textualsimilarity, pages 32–43..philip bachman, r. devon hjelm, and william buch-walter.
2019. learning representations by maximiz-ing mutual information across views.
in advancesin neural information processing systems 32: an-nual conference on neural information processingsystems 2019, neurips 2019, december 8-14, 2019,vancouver, bc, canada, pages 15509–15519..samuel r. bowman, gabor angeli, christopher potts,and christopher d. manning.
2015. a large anno-tated corpus for learning natural language inference.
in proceedings of the 2015 conference on empiri-cal methods in natural language processing, pages632–642, lisbon, portugal.
association for compu-tational linguistics..maxime bucher, st´ephane herbin, and fr´ed´eric jurie.
improving semantic embedding consistency2016.by metric learning for zero-shot classifﬁcation.
ineuropean conference on computer vision, pages730–746.
springer..kevin clark, minh-thang luong, quoc v. le, andchristopher d. manning.
2020. electra: pre-training text encoders as discriminators rather thanin 8th international conference ongenerators.
learning representations, iclr 2020, addis ababa,ethiopia, april 26-30, 2020. openreview.net..alexis conneau and douwe kiela.
2018. senteval: anevaluation toolkit for universal sentence representa-tions.
in proceedings of the eleventh internationalconference on language resources and evaluation(lrec 2018), miyazaki, japan.
european languageresources association (elra)..alexis conneau, douwe kiela, holger schwenk, lo¨ıcbarrault, and antoine bordes.
2017. supervisedlearning of universal sentence representations fromnatural language inference data.
in proceedings ofthe 2017 conference on empirical methods in nat-ural language processing, pages 670–680, copen-hagen, denmark.
association for computationallinguistics..alexis conneau, german kruszewski, guillaume lam-ple, lo¨ıc barrault, and marco baroni.
2018. whatyou can cram into a single $&!#* vector: probingsentence embeddings for linguistic properties.
inproceedings of the 56th annual meeting of the as-sociation for computational linguistics (volume 1:long papers), pages 2126–2136, melbourne, aus-tralia.
association for computational linguistics..alexis conneau and guillaume lample.
2019. cross-in advanceslingual language model pretraining.
in neural information processing systems 32: an-nual conference on neural information processingsystems 2019, neurips 2019, december 8-14, 2019,vancouver, bc, canada, pages 7057–7067..daniel cer, mona diab, eneko agirre, i˜nigo lopez-gazpio, and lucia specia.
2017. semeval-2017task 1: semantic textual similarity multilingual andin proceedingscrosslingual focused evaluation.
of the 11th international workshop on semanticevaluation (semeval-2017), pages 1–14, vancouver,canada.
association for computational linguistics..zihang dai, zhilin yang, yiming yang, jaime car-bonell, quoc le, and ruslan salakhutdinov.
2019.transformer-xl: attentive language models beyondin proceedings of the 57tha ﬁxed-length context.
annual meeting of the association for computa-tional linguistics, pages 2978–2988, florence, italy.
association for computational linguistics..daniel cer, yinfei yang, sheng-yi kong, nan hua,nicole limtiaco, rhomni st. john, noah constant,mario guajardo-cespedes, steve yuan, chris tar,brian strope, and ray kurzweil.
2018. universalin proceedings ofsentence encoder for english.
the 2018 conference on empirical methods in nat-ural language processing: system demonstrations,pages 169–174, brussels, belgium.
association forcomputational linguistics..ting chen, simon kornblith, mohammad norouzi,and geoffrey e. hinton.
2020. a simple frameworkfor contrastive learning of visual representations.
inproceedings of the 37th international conference onmachine learning, icml 2020, 13-18 july 2020,virtual event, volume 119 of proceedings of ma-chine learning research, pages 1597–1607.
pmlr..jacob devlin, ming-wei chang, kenton lee, andkristina toutanova.
2019. bert: pre-training ofdeep bidirectional transformers for language under-in proceedings of the 2019 conferencestanding.
of the north american chapter of the associationfor computational linguistics: human languagetechnologies, volume 1 (long and short papers),pages 4171–4186, minneapolis, minnesota.
associ-ation for computational linguistics..bill dolan, chris quirk, and chris brockett.
2004.unsupervised construction of large paraphrase cor-pora: exploiting massively parallel news sources.
in coling 2004: proceedings of the 20th inter-national conference on computational linguistics,pages 350–356, geneva, switzerland.
coling..888matt gardner, joel grus, mark neumann, oyvindtafjord, pradeep dasigi, nelson f. liu, matthew pe-ters, michael schmitz, and luke zettlemoyer.
2018.allennlp: a deep semantic natural language pro-in proceedings of workshop forcessing platform.
nlp open source software (nlp-oss), pages 1–6, melbourne, australia.
association for computa-tional linguistics..aaron gokaslan and vanya cohen.
2019. openweb-text corpus.
http://skylion007.github.io/openwebtextcorpus..alexander grabner, peter m. roth, and vincent lep-etit.
2018.
3d pose estimation and 3d model retrievalin 2018 ieee conferencefor objects in the wild.
on computer vision and pattern recognition, cvpr2018, salt lake city, ut, usa, june 18-22, 2018,pages 3022–3031.
ieee computer society..raia hadsell, sumit chopra, and yann lecun.
2006.dimensionality reduction by learning an invariantmapping.
in 2006 ieee computer society confer-ence on computer vision and pattern recognition(cvpr’06), volume 2, pages 1735–1742.
ieee..kaiming he, haoqi fan, yuxin wu, saining xie, andross b. girshick.
2020. momentum contrast for un-in 2020supervised visual representation learning.
ieee/cvf conference on computer vision and pat-tern recognition, cvpr 2020, seattle, wa, usa,june 13-19, 2020, pages 9726–9735.
ieee..kaiming he, xiangyu zhang, shaoqing ren, and jiansun.
2016. deep residual learning for image recog-nition.
in 2016 ieee conference on computer vi-sion and pattern recognition, cvpr 2016, las ve-gas, nv, usa, june 27-30, 2016, pages 770–778.
ieee computer society..xinwei he, yang zhou, zhichao zhou, song bai, andxiang bai.
2018. triplet-center loss for multi-viewin 2018 ieee conference on3d object retrieval.
computer vision and pattern recognition, cvpr2018, salt lake city, ut, usa, june 18-22, 2018,pages 1945–1954.
ieee computer society..matthew henderson, rami al-rfou, brian strope, yun-hsuan sung, l´aszl´o luk´acs, ruiqi guo, sanjiv ku-mar, balint miklos, and ray kurzweil.
2017. efﬁ-cient natural language response suggestion for smartreply.
arxiv preprint arxiv:1705.00652..felix hill, kyunghyun cho, and anna korhonen.
2016. learning distributed representations of sen-tences from unlabelled data.
in proceedings of the2016 conference of the north american chapter ofthe association for computational linguistics: hu-man language technologies, pages 1367–1377, sandiego, california.
association for computationallinguistics..r. devon hjelm, alex fedorov, samuel lavoie-marchildon, karan grewal, philip bachman, adamtrischler, and yoshua bengio.
2019. learning deep.
representations by mutual information estimationand maximization.
in 7th international conferenceon learning representations, iclr 2019, new or-leans, la, usa, may 6-9, 2019. openreview.net..jeremy howard and sebastian ruder.
2018. universallanguage model ﬁne-tuning for text classiﬁcation.
inproceedings of the 56th annual meeting of the as-sociation for computational linguistics (volume 1:long papers), pages 328–339, melbourne, australia.
association for computational linguistics..minqing hu and bing liu.
2004. mining and summa-rizing customer reviews.
in proceedings of the tenthacm sigkdd international conference on knowl-edge discovery and data mining, pages 168–177..yacine jernite, samuel r. bowman, and david a. son-tag.
2017. discourse-based objectives for fast un-supervised sentence representation learning.
corr,abs/1705.00557..mandar joshi, danqi chen, yinhan liu, daniel s.weld, luke zettlemoyer, and omer levy.
2020.spanbert: improving pre-training by representingand predicting spans.
transactions of the associa-tion for computational linguistics, 8:64–77..ryan kiros, yukun zhu, ruslan salakhutdinov,richard s. zemel, raquel urtasun, antonio tor-ralba, and sanja fidler.
2015. skip-thought vec-tors.
in advances in neural information processingsystems 28: annual conference on neural informa-tion processing systems 2015, december 7-12, 2015,montreal, quebec, canada, pages 3294–3302..lingpeng kong, cyprien de masson d’autume, leiyu, wang ling, zihang dai, and dani yogatama.
2020. a mutual information maximization perspec-in 8thtive of language representation learning.
international conference on learning representa-tions, iclr 2020, addis ababa, ethiopia, april 26-30, 2020. openreview.net..zhenzhong lan, mingda chen, sebastian goodman,kevin gimpel, piyush sharma, and radu soricut.
2020. albert: a lite bert for self-supervisedin 8th inter-learning of language representations.
national conference on learning representations,iclr 2020, addis ababa, ethiopia, april 26-30,2020. openreview.net..quoc v. le and tom´as mikolov.
2014. distributedrepresentations of sentences and documents.
inproceedings of the 31th international conferenceon machine learning, icml 2014, beijing, china,21-26 june 2014, volume 32 of jmlr workshopand conference proceedings, pages 1188–1196.
jmlr.org..laura leal-taix´e, cristian canton-ferrer, and konradschindler.
2016. learning by tracking: siamese cnnfor robust target association.
in proceedings of theieee conference on computer vision and patternrecognition workshops, pages 33–40..889tsung-yi lin, michael maire, serge belongie, jameshays, pietro perona, deva ramanan, piotr doll´ar,and c lawrence zitnick.
2014. microsoft coco:in european confer-common objects in context.
ence on computer vision, pages 740–755.
springer..yinhan liu, myle ott, naman goyal, jingfei du, man-dar joshi, danqi chen, omer levy, mike lewis,luke zettlemoyer, and veselin stoyanov.
2019.roberta: a robustly optimized bert pretraining ap-proach.
arxiv preprint arxiv:1907.11692..lajanugen logeswaran and honglak lee.
2018. anefﬁcient framework for learning sentence represen-tations.
in 6th international conference on learn-ing representations, iclr 2018, vancouver, bc,canada, april 30 - may 3, 2018, conference trackproceedings.
openreview.net..ilya loshchilov and frank hutter.
2019. decou-in 7th inter-pled weight decay regularization.
national conference on learning representations,iclr 2019, new orleans, la, usa, may 6-9, 2019.openreview.net..david g lowe.
1995. similarity metric learning fora variable-kernel classiﬁer.
neural computation,7(1):72–85..marco marelli, stefano menini, marco baroni, luisabentivogli, raffaella bernardi, and roberto zampar-elli.
2014. a sick cure for the evaluation of compo-sitional distributional semantic models.
in proceed-ings of the ninth international conference on lan-guage resources and evaluation (lrec’14), pages216–223, reykjavik, iceland.
european languageresources association (elra)..michael mccloskey and neal j cohen.
1989. catas-trophic interference in connectionist networks: thesequential learning problem.
in psychology of learn-ing and motivation, volume 24, pages 109–165.
el-sevier..sebastian mika, gunnar ratsch, jason weston, bern-hard scholkopf, and klaus-robert mullers.
1999.fisher discriminant analysis with kernels.
in neuralnetworks for signal processing ix: proceedings ofthe 1999 ieee signal processing society workshop(cat.
no.
98th8468), pages 41–48.
ieee..tom´as mikolov, ilya sutskever, kai chen, gregory s.corrado, and jeffrey dean.
2013. distributed rep-resentations of words and phrases and their com-in advances in neural informationpositionality.
processing systems 26: 27th annual conference onneural information processing systems 2013. pro-ceedings of a meeting held december 5-8, 2013,lake tahoe, nevada, united states, pages 3111–3119..ser-nam lim,.
kevin musgrave,belongie.
2019.https://github.com/kevinmusgrave/pytorch-metric-learning..pytorch metric.
and.
sergelearning..aaron van den oord, yazhe li, and oriol vinyals.
2018. representation learning with contrastive pre-dictive coding.
arxiv preprint arxiv:1807.03748..bo pang and lillian lee.
2004. a sentimental edu-cation: sentiment analysis using subjectivity sum-in proceed-marization based on minimum cuts.
ings of the 42nd annual meeting of the associationfor computational linguistics (acl-04), pages 271–278, barcelona, spain..bo pang and lillian lee.
2005. seeing stars: ex-ploiting class relationships for sentiment categoriza-in proceed-tion with respect to rating scales.
ings of the 43rd annual meeting of the associationfor computational linguistics (acl’05), pages 115–124, ann arbor, michigan.
association for compu-tational linguistics..adam paszke, sam gross, soumith chintala, gregorychanan, edward yang, zachary devito, zeminglin, alban desmaison, luca antiga, and adamlerer.
2017. automatic differentiation in pytorch.
in nips autodiff workshop..jeffrey pennington, richard socher, and christophermanning.
2014. glove: global vectors for wordrepresentation.
in proceedings of the 2014 confer-ence on empirical methods in natural languageprocessing (emnlp), pages 1532–1543, doha,qatar.
association for computational linguistics..jason phang, thibault f´evry, and samuel r. bowman.
2018. sentence encoders on stilts: supplementarytraining on intermediate labeled-data tasks.
corr,abs/1811.01088..alec radford, karthik narasimhan, tim salimans,and ilya sutskever.
2018.improving languageunderstanding by generative pre-training.
urlhttps://s3-us-west-2.
com/openai-assets/researchcovers/languageunsupervised/languageunderstanding paper.
pdf..amazonaws..alec radford, jeffrey wu, rewon child, david luan,dario amodei, and ilya sutskever.
2019. languagemodels are unsupervised multitask learners.
openaiblog, 1(8):9..roger ratcliff.
1990. connectionist models of recog-nition memory: constraints imposed by learningpsychological review,and forgetting functions.
97(2):285..nils reimers and iryna gurevych.
2019. sentence-bert: sentence embeddings using siamese bert-networks.
in proceedings of the 2019 conference onempirical methods in natural language processingand the 9th international joint conference on natu-ral language processing (emnlp-ijcnlp), pages3982–3992, hong kong, china.
association forcomputational linguistics..sebastian ruder, matthew e. peters,.
swayamdipta, and thomas wolf.
2019.fer learning in natural language processing..swabhatrans-in.
890proceedings of the 2019 conference of the norththe association for com-american chapter ofputational linguistics:tutorials, pages 15–18,minneapolis, minnesota.
association for computa-tional linguistics..victor sanh, lysandre debut, julien chaumond, andthomas wolf.
2019. distilbert, a distilled versionof bert: smaller, faster, cheaper and lighter.
arxivpreprint arxiv:1910.01108..nikunj saunshi, orestis plevrakis, sanjeev arora,mikhail khodak, and hrishikesh khandeparkar.
2019. a theoretical analysis of contrastive unsuper-vised representation learning.
in proceedings of the36th international conference on machine learn-ing, icml 2019, 9-15 june 2019, long beach, cali-fornia, usa, volume 97 of proceedings of machinelearning research, pages 5628–5637.
pmlr..richard socher, alex perelygin, jean wu, jasonchuang, christopher d. manning, andrew ng, andchristopher potts.
2013. recursive deep modelsfor semantic compositionality over a sentiment tree-in proceedings of the 2013 conference onbank.
empirical methods in natural language processing,pages 1631–1642, seattle, washington, usa.
asso-ciation for computational linguistics..kihyuk sohn.
2016..improved deep metric learningwith multi-class n-pair loss objective.
in advancesin neural information processing systems 29: an-nual conference on neural information processingsystems 2016, december 5-10, 2016, barcelona,spain, pages 1849–1857..sandeep subramanian, adam trischler, yoshua ben-gio, and christopher j. pal.
2018. learning gen-eral purpose distributed sentence representationsin 6th inter-via large scale multi-task learning.
national conference on learning representations,iclr 2018, vancouver, bc, canada, april 30 - may3, 2018, conference track proceedings.
openre-view.net..ran tao, efstratios gavves, and arnold w. m. smeul-ders.
2016. siamese instance search for tracking.
in2016 ieee conference on computer vision and pat-tern recognition, cvpr 2016, las vegas, nv, usa,june 27-30, 2016, pages 1420–1429.
ieee com-puter society..yonglong tian, dilip krishnan, and phillip isola.
2020..contrastive multiview coding.
in eccv..ashish vaswani, noam shazeer, niki parmar, jakobuszkoreit, llion jones, aidan n. gomez, lukaszkaiser, and illia polosukhin.
2017. attention is allyou need.
in advances in neural information pro-cessing systems 30: annual conference on neuralinformation processing systems 2017, december 4-9, 2017, long beach, ca, usa, pages 5998–6008..ellen m voorhees and dawn m tice.
2000. buildinga question answering test collection.
in proceedings.
of the 23rd annual international acm sigir confer-ence on research and development in informationretrieval, pages 200–207..alex wang, amanpreet singh, julian michael, felixhill, omer levy, and samuel r. bowman.
2019.glue: a multi-task benchmark and analysis plat-in 7thform for natural language understanding.
international conference on learning representa-tions, iclr 2019, new orleans, la, usa, may 6-9,2019. openreview.net..yandong wen, kaipeng zhang, zhifeng li, andyu qiao.
2016. a discriminative feature learn-in euro-ing approach for deep face recognition.
pean conference on computer vision, pages 499–515.
springer..janyce wiebe, theresa wilson, and claire cardie.
2005. annotating expressions of opinions and emo-tions in language.
language resources and evalua-tion, 39(2-3):165–210..adina williams, nikita nangia, and samuel bowman.
2018. a broad-coverage challenge corpus for sen-tence understanding through inference.
in proceed-ings of the 2018 conference of the north americanchapter of the association for computational lin-guistics: human language technologies, volume1 (long papers), pages 1112–1122, new orleans,louisiana.
association for computational linguis-tics..paul wohlhart and vincent lepetit.
2015. learning de-scriptors for object recognition and 3d pose estima-tion.
in ieee conference on computer vision andpattern recognition, cvpr 2015, boston, ma, usa,june 7-12, 2015, pages 3109–3118.
ieee computersociety..thomas wolf, lysandre debut, victor sanh, julienchaumond, clement delangue, anthony moi, pier-ric cistac, tim rault, remi louf, morgan funtow-icz, joe davison, sam shleifer, patrick von platen,clara ma, yacine jernite, julien plu, canwen xu,teven le scao, sylvain gugger, mariama drame,quentin lhoest, and alexander rush.
2020. trans-formers: state-of-the-art natural language process-ing.
in proceedings of the 2020 conference on em-pirical methods in natural language processing:system demonstrations, pages 38–45, online.
asso-ciation for computational linguistics..zhirong wu, yuanjun xiong, stella x. yu, and dahualin.
2018. unsupervised feature learning via non-in 2018 ieeeparametric instance discrimination.
conference on computer vision and pattern recog-nition, cvpr 2018, salt lake city, ut, usa, june18-22, 2018, pages 3733–3742.
ieee computer so-ciety..eric p. xing, andrew y. ng, michael i. jordan, and stu-art j. russell.
2002. distance metric learning withapplication to clustering with side-information.
inadvances in neural information processing systems.
89115 [neural information processing systems, nips2002, december 9-14, 2002, vancouver, britishcolumbia, canada], pages 505–512.
mit press..zhilin yang, zihang dai, yiming yang, jaime g. car-bonell, ruslan salakhutdinov, and quoc v. le.
2019.xlnet: generalized autoregressive pretraining forin advances in neurallanguage understanding.
information processing systems 32: annual con-ference on neural information processing systems2019, neurips 2019, december 8-14, 2019, vancou-ver, bc, canada, pages 5754–5764..sasi kiran yelamarthi, shiva krishna reddy, ashishmishra, and anurag mittal.
2018. a zero-shotframework for sketch based image retrieval.
in eu-ropean conference on computer vision, pages 316–333. springer..yang you, jing li, jonathan hseu, xiaodan song,james demmel, and c hsieh.
1904. reducing bertpre-training time from 3 days to 76 minutes.
corrabs/1904.00962 (2019)..rui yu, zhiyong dou, song bai, zhaoxiang zhang,yongchao xu, and xiang bai.
2018. hard-awarepoint-to-set deep metric for person re-identiﬁcation.
in proceedings of the european conference on com-puter vision (eccv), pages 188–204..ye yuan, wuyang chen, yang yang, and zhangyangwang.
2020.in defense of the triplet loss again:learning robust person re-identiﬁcation with fast ap-proximated triplet loss and label distillation.
in pro-ceedings of the ieee/cvf conference on computervision and pattern recognition (cvpr) workshops..ziming zhang and venkatesh saligrama.
2016. zero-shot learning via joint latent similarity embedding.
in 2016 ieee conference on computer vision andpattern recognition, cvpr 2016, las vegas, nv,usa, june 27-30, 2016, pages 6034–6042.
ieeecomputer society..a pretrained transformers make poor.
universal sentence encoders.
certain pretrained transformers, such as bert andalbert, have mechanisms for learning sequence-level embeddings via self-supervision.
these mod-els prepend every input sequence with a specialclassiﬁcation token (e.g.
“[cls]”), and its repre-sentation is learned using a simple classiﬁcationtask, such as next sentence prediction (nsp) orsentence-order prediction (sop) (see devlin et al.
2019 and lan et al.
2020 respectively for details onthese tasks).
however, during preliminary experi-ments, we noticed that these models are not gooduniversal sentence encoders, as measured by theirperformance on the senteval benchmark (conneauand kiela, 2018).
as a simple experiment, we.
evaluated three pretrained transformer models onsenteval: one trained with the nsp loss (bert),one trained with the sop loss (albert) and onetrained with neither, roberta (liu et al., 2019).
we did not ﬁnd that the cls embeddings producedby models trained against the nsp or sop losses tooutperform that of a model trained without eitherloss and sometimes failed to outperform a bag-of-words (bow) baseline (table 4).
furthermore, weﬁnd that pooling token embeddings via averaging(referred to as “mean pooling” in our paper) out-performs pooling via the cls classiﬁcation token.
our results are corroborated by liu et al.
2019,who ﬁnd that removing nsp loss leads to the sameor better results on downstream tasks and reimersand gurevych 2019, who ﬁnd that directly usingthe output of bert as sentence embeddings leadsto poor performances on the semantic similaritytasks of senteval..b examples of sampled spans.
in table 5, we present examples of anchor-positiveand anchor-negative pairs generated by our sam-pling procedure.
we show one example for eachpossible view of a sampled positive, e.g.
posi-tives adjacent to, overlapping with, or subsumedby the anchor.
for each anchor-positive pair, weshow examples of both a hard negative (derivedfrom the same document) and an easy negative(derived from another document).
recall that aminibatch is composed of random documents, andeach anchor-positive pair sampled from a docu-ment is contrasted against all other anchor-positivepairs in the minibatch.
thus, hard negatives, as wehave described them here, are generated only whensampling multiple anchors per document (a > 1)..c senteval evaluation details.
senteval is a benchmark for evaluating the qual-ity of ﬁxed-length sentence embeddings.
it is di-vided into 18 downstream tasks, and 10 probingtasks.
sentence embedding methods are evaluatedon these tasks via a simple interface9, which stan-dardizes training, evaluation and hyperparameters.
for most tasks, the method to evaluate is usedto produce ﬁx-length sentence embeddings, anda simple logistic regression (lr) or multi-layer per-ception (mlp) model is trained on the task usingthese embeddings as input.
for other tasks (namely.
9https://github.com/facebookresearch/.
senteval.
892table 4: results on the downstream and probing tasks from the validation set of senteval.
we compare modelstrained with the next sentence prediction (nsp) and sentence-order prediction (sop) losses to a model trainedwith neither, using two different pooling strategies: ”*-cls”, where the special classiﬁcation token is used as itssentence representation, and ”*-mean”, where each sentence is represented by the mean of its token embeddings..model.
glovefasttext.
bert-base-clsbert-base-mean.
albert-base-v2-clsalbert-base-v2-mean.
roberta-base-clsroberta-base-mean.
parameters.
embed.
dim..downstream.
probing.
bag-of-words (bow) weak baselines.
senteval.
trained with next sentence prediction (nsp) loss.
trained with sentence-order prediction (sop) loss.
trained with neither nsp or sop losses.
––.
110m110m.
11m11m.
125m125m.
300300.
768768.
768768.
768768.
66.0568.75.
63.5371.98.
58.7569.39.
68.5372.84.
62.9363.46.
69.5773.37.
69.8874.83.
66.9274.59.datasets (also known as natural languageinference or nli), including sick-e (marelliet al., 2014) and the stanford nli dataset(snli)(bowman et al., 2015) as wellas multiple semantic relatedness datasetsincluding sick-r and sts-b (cer et al.,2017)..• semantic textual similarity these tasks(sts12 agirre et al.
2012, sts13 agirre et al.
2013, sts14 agirre et al.
2014, sts15 agirreet al.
2015 and sts16 agirre et al.
2016) aresimilar to the semantic relatedness tasks, ex-cept the embeddings produced by the encoderare used as-is in a cosine similarity to deter-mine the semantic similarity of two sentences.
no additional model is trained on top of theencoder’s output..• paraphrase detection evaluated on the mi-crosoft research paraphrase corpus (mrpc)(dolan et al., 2004), this binary classiﬁcationtask is comprised of human-labelled sentencepairs, annotated according to whether theycapture a paraphrase/semantic equivalence re-lationship..several semantic text similarity tasks), the embed-dings are used as-is without any further training.
note that this setup is different from evaluations onthe popular glue benchmark (wang et al., 2019),which typically use the task data to ﬁne-tune theparameters of the sentence embedding model..in subsection c.1, we present the individualtasks of the senteval benchmark.
in subsection c.2,we explain our method for computing the averagedownstream and average probing scores presentedin our paper..c.1 senteval tasks.
the downstream tasks of senteval are representa-tive nlp tasks used to evaluate the transferabilityof ﬁxed-length sentence embeddings.
we give abrief overview of the broad categories that dividethe tasks below (see conneau and kiela 2018 formore details):.
• binary.
and multi-class.
classiﬁcation:these tasks cover various types of sentenceincluding sentiment analy-classiﬁcation,sis (mr pang and lee 2005, sst2 andsst5 socher et al.
2013), question-type(trec) (voorhees and tice, 2000), productreviews (cr) (hu and liu, 2004), subjectiv-ity/objectivity (subj) (pang and lee, 2004)and opinion polarity (mpqa) (wiebe et al.,2005)..• entailment and semantic.
relatedness:cover multiple entailment.
these tasks.
893ew.,rohcnahcae.rof...hctabinimani.tnemucodyrevemorf.srohcna.erom.roenoelpmasylmodnar.ew.,gniniartgnirud..
dohtem.ruoybdetarenegsnaps.txet.foselpmaxe.:5elbat.riap.evitisop-rohcna.rehto.yreve.hti.wdetsartnoc.era.sriap.evitisop-rohcna.ll.a..
rohcna.eht.yb.demusbus.ro.,hti.wgnippalrevo.,ot.tnecajda.sevitisop.erom.ro.eno.elpmas.ylmodnar.eht.morfdelpmas.sevitisopdna.srohcna(.
sevitagen.drahdna.)
hctabinimani.stnemucod.rehtomorfdelpmas.sevitisopdna.srohcna(.
sevitagenysaeot.sdael.siht...hctabinimehtni..
snekot215.fohtgnel.aotpu.snaps.elpmas.ew.,gniniart.gnirud..
snekot.46fo.htgnel.mumixama.ta.deppac.era.selpmaxe.,ereh..)
tnemucod.emas.evitagenysae.evitagen.drah.evitisop.rohcna.weiv.gnippalrevo..
stned.weiv.tnecajda.weivdemusbus.-olpxe.rof.elbaliavawon.sinoitacol.wena.taht.-i.mm.i.egral.hti.wyrtnuoc.eht.fo.strap.larebil.erewemos..
ecilop.ot.gniklat.elbatrofmoc.leef.-ecrofnewal.dna.setacovda.sthgir-tnargimm.i.a.ekil.sleef.,.
weiv.ymni.,aera.dooga..
noitar.niytnuocaralcatnasekil.,snoitalupoptnarg.ot.yllaer.erewsnoitnetni.s’ec.i.taht.lacitpeks.weneht.folacitpeks.erewslanoisseforptnem.t’nseod.ti.dlrowemag.a.fo.noissergorp.larutan.deerga.,sionilliniytnuockoocdna.ainrofilac.-ed.ot.ylpm.is.naht.rehtar.,ytefas.cilbup.tcetorp.ecrofneot.spoc.lacolybtroffeyna.
..margorp.sdeennrutni.taht..
yrartibra.ronodekcat.mees.yeht..
seitinummocerucesfo.scitirc.eht.hti.w..
ylisae.erom.stnargimm.i.dezirohtuanu.trop.rofdabebdluow.,tlefyeht.,swalnoitargimm.i.etaler.ot.ti.dluowmargorp.eht.gnitnemelpm.i.taht.deirrow.-iser.tnargimm.ihti.wspihsnoitaler.riehtniarts.sm.itciv.tnargimm.i.ecnis.,gnicilop.ytinummoc.-trofmoc.leef.t’ndluowem.irc.fo.sessenti.w.ro..
ecilopot.gniklat.elba.detelpmoc.neeb.evah.ot.snoitarepo.lla.sredis.txenehtot.snoitisnartdnaesahptnerruceht.rof..
esahp.sihtnehw..
redroofifnidessecorpera.stneve.ni.dednuorgsaw.tejobmuj.syaw.ri.ahsitir.ba.eb.nac.cilbup.eht”.
.
og.lli.w.hsa.eht.erehw.onaclov.eht.fo.tuo.gnihcleb.spots.hsa.eht.fi.
-noc.pool.tneve.eht.,deitpme.si.eueuqkcittxen.senigne.eht.sraefgniwollofyadnusnoadanac.elba.ylno.era.senilria.taht.tnedﬁnoc.yletulosba.evah.lli.wmelborp.eht.,syadwef.a.retfa.,neht.hsa.cinaclovhti.wdetanimatnoc.neeb.dah.rianayr”.
os.od.ot.efas.si.ti.nehwetarepo.ot.rehtoeht”.
.
srotcaf.eht.foenos’tahtos.,deraelc.duolc.hsa.yna.ees.ton.dluoc.ti.dias.-omeht.t.a”.
noitcerid.dna.deeps.dniweht.si.elitalov.yrev.era.snrettap.rehtaew.eht.tnem.-nu.,tlucﬁfid.etiuq.ti.gnikamsi.tahwsi.hcihw.tciderp.ot.,raey.tsal.ekil.woh.rettamon.dna..
rof.thguof.eb.ot.evila.llits.erewerehtelihw.,gnitruhsaw.fles.mihehhcum..
no.og.dluowehmih.dedeen.ohwelpoep.llits.rof.weﬂ.dna.tuo.deirc.sekwaf..
sgnileef.hcus.ot.namsebirt.ajnew.a.foelor.ehtnoekat.sreyalp.rof.ediwdlrow.desaeler.saw.ti..
tfosibu.yb.oediv.erutnevda-noitca.na.si.lam.irp.yrc.raf.-ruf...dewollof.erodelbmudsubladna.,.
daeha.onhti.wsoronidednarts.siohw.,rakkatdeman.,32yraurbefnoenoxobxdna4noitatsyalp.-bup.dna.laertnom.tfosibuyb.depoleved.emag.erewelpoep.,htap.’srotnemedeht.gnola.reht.dehsubma.si.ytrap.gnitnuh.sih.retfa.snopaew.,1.hcra.mno.swodniw.tfosorci.m.rof.dna.,6102.ediwdlrowdesaeler.saw.ti..
tfosibuybdehsil..
regithtoot-rebasa.yb.rafniameht.foffo-nips.a.si.emageht..
6102.
-urbef.no.eno.xobx.dna.
4.noitatsyalp.rof..
seiresyrc.noswodniw.tfosorci.m.rofdna.,.
6102,32yra.eht.foffo-nips.a.si.emageht..
6102,1hcra.m.emagyrc.raf.tsrﬁeht.si.ti..
seiresyrc.rafniam..
egacihtilose.m.eht.ni.tes.894• object number (objnum): a binary classi-ﬁcation task, analogous to subjnum, wherethe goal is to predict the number (singular orplural) of the direct object of the main clause..• semantic odd man out (somo): a binaryclassiﬁcation task where the goal is to predictwhether a sentence has had a single randomlypicked noun or verb replaced with anotherword with the same part-of-speech..• coordinate inversion (coordinv): a binaryclassiﬁcation task where the goal is to predictwhether the order of two coordinate clausesin a sentence has been inverted..c.2 computing an average score.
in our paper, we present averaged downstreamand probing scores.
computing averaged probingscores was straightforward; each of the ten probingtasks reports a simple accuracy, which we averaged.
to compute an averaged downstream score, we dothe following:.
• if a task reports spearman correlation (i.e.
sick-r, sts-b), we use this score when com-puting the average downstream task score.
ifthe task reports a mean spearman correlationfor multiple subtasks (i.e.
sts12, sts13,sts14, sts15, sts16), we use this score..• if a task reports both an accuracy and an f1-score (i.e.
mrpc), we use the average ofthese two scores..• for the caption-image retrieval task, we re-port the average of the recall@k, wherek ∈ {1, 5, 10} for the image and captionretrieval tasks (a total of six scores).
this isthe default behaviour of senteval..• otherwise, we use the reported accuracy..• caption-image retrieval this task is com-prised of two sub-tasks: ranking a large col-lection of images by their relevance for somegiven query text (image retrieval) and rank-ing captions by their relevance for some givenquery image (caption retrieval).
both tasksare evaluated on data from the coco dataset(lin et al., 2014).
each image is representedby a pretrained, 2048-dimensional embeddingproduced by a resnet-101 (he et al., 2016)..the probing tasks are designed to evaluate whatlinguistic properties are encoded in a sentence rep-resentation.
all tasks are binary or multi-class clas-siﬁcation.
we give a brief overview of each taskbelow (see conneau et al.
2018 for more details):.
• sentence length (sentlen): a multi-classclassiﬁcation task where a model is trainedto predict the length of a given input sen-tence, which is binned into six possible lengthranges..• word content (wc): a multi-class classiﬁca-tion task where, given 1000 words as targets,the goal is to predict which of the target wordsappears in a given input sentence.
each sen-tence contains a single target word, and theword occurs exactly once in the sentence..• tree depth (treedepth): a multi-class clas-siﬁcation task where the goal is to predict themaximum depth (with values ranging from 5to 12) of a given input sentence’s syntactictree..• bigram shift (bshift): a multi-class clas-siﬁcation task where the goal is to predictwhether two consecutive tokens within a givensentence have been inverted..• top constituents (topconst): a multi-classclassiﬁcation task where the goal is to predictthe top constituents (from a choice of 19) im-mediately below the sentence (s) node of thesentence’s syntactic tree..• tense: a binary classiﬁcation task where thegoal is to predict the tense (past or present) ofthe main verb in a sentence..• subject number (subjnum): a binary clas-siﬁcation task where the goal is to predict thenumber (singular or plural) of the subject ofthe main clause..895