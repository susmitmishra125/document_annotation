generating soap notes from doctor-patient conversationsusing modular summarization techniques.
kundan krishna, sopan khosla, jeffrey bigham, zachary c. liptoncarnegie mellon university5000 forbes avenuepittsburgh, pa{kundank,sopank,jbigham,zlipton}@andrew.cmu.edu.
abstract.
following each patient visit, physicians draftlong semi-structured clinical summaries calledsoap notes.
while invaluable to clini-cians and researchers, creating digital soapnotes is burdensome, contributing to physicianburnout.
in this paper, we introduce the ﬁrstcomplete pipelines to leverage deep summa-rization models to generate these notes basedon transcripts of conversations between physi-cians and patients.
after exploring a spectrumof methods across the extractive-abstractivespectrum, we propose cluster2sent, an al-gorithm that (i) extracts important utterancesrelevant to each summary section; (ii) clus-ters together related utterances; and then (iii)generates one summary sentence per cluster.
cluster2sent outperforms its purely ab-stractive counterpart by 8 rouge-1 points,and produces signiﬁcantly more factual and co-herent sentences as assessed by expert humanevaluators.
for reproducibility, we demon-strate similar beneﬁts on the publicly availableami dataset.
our results speak to the beneﬁtsof structuring summaries into sections and an-notating supporting evidence when construct-ing summarization corpora..1.introduction.
electronic health records (ehr) play a crucial rolein patient care.
however, populating them can takeas much time as attending to patients (sinsky et al.,2016) and constitutes a major cause of physicianburnout (kumar and mezoff, 2020).
in particular,doctors document patient encounters with soapnotes, semi-structured written accounts containingfour sections: (s)ubjective information reported bythe patient; (o)bjective observations, e.g., lab re-sults; (a)ssessments made by the doctor (typically,the diagnosis); and a (p)lan for future care, includ-ing diagnostic tests, medications, and treatments.
sections can be subdivided into 15 subsections..in a parallel development, patients increasinglyrecord their doctor’s visits, either in lieu of takingnotes or to share with a family member.
a buddingline of research has sought to leverage transcriptsof these clinical conversations both to provide in-sights to patients and to extract structured data tobe entered into ehrs (liu et al., 2019b; schlossand konam, 2020; krishna et al., 2021)..in this paper, we introduce the ﬁrst end-to-endmethods for generating whole soap notes basedon clinical conversations.
our work builds on aunique corpus, developed in collaboration withabridge ai, inc.1), that consists of thousands oftranscripts of recorded clinical conversations to-gether with associated soap notes drafted by awork force trained in the ofﬁcial style of soapnote documentation.
on one hand, this task ismuch harder than traditional summarization bench-marks, in part, because soap notes are longer(320 words on average) than summaries in popu-lar datasets like cnn/dailymail (nallapati et al.,2016), newsroom (grusky et al., 2018), and sam-sum (gliwa et al., 2019) (55, 27, and 24 wordson average).
on the other hand, our dataset offersuseful structure: (i) segmentation of each soapnote into subsections; and (ii) a set of supportingutterances that provide evidence for each sentencein the soap note.
exploiting this structure, ourmethods outperform appropriate baselines..our ﬁrst methodological contribution is to pro-pose a spectrum of methods, for decomposing sum-marizaton tasks into extractive and abstractive sub-tasks.
starting from a straightforward sequence-to-sequence model, our methods shift progressivelymore work from the abstractive to the extractivecomponent: (i) conv2note: the extractive mod-ule does nothing, placing the full burden of summa-rization on an end-to-end abstractive module.
(ii).
1http://abridge.com.
proceedingsofthe59thannualmeetingoftheassociationforcomputationallinguisticsandthe11thinternationaljointconferenceonnaturallanguageprocessing,pages4958–4972august1–6,2021.©2021associationforcomputationallinguistics4958figure 1: workﬂow of our best performing approach involving extraction and clustering of noteworthy conversa-tion utterances followed by abstractive summarization of each cluster (ﬁctitious data).
ext2note: the extractive module selects all utter-ances that are noteworthy (i.e., likely to be markedas supporting utterances for at least one soapnote sentence), and the decoder is conditioned onlyon these utterances; (iii) ext2sec: the extractivemodule extracts per-subsection noteworthy utter-ances and the decoder generates each subsection,conditioned only on the corresponding utterances;(iv) cluster2sent: the extractive module notonly extracts per-subsection noteworthy utterancesbut clusters together those likely to support thesame soap sentence—here, the decoder producesa single sentence at a time, each conditioned upona single cluster of utterances and a token indicatingthe soap subsection.
we see consistent beneﬁtsas we move from approach (i) through (iv)..both to demonstrate the generality of our meth-ods and to provide a reproducible benchmark, weconduct parallel experiments on the (publicly avail-able) ami corpus (carletta, 2007)2 like our med-ical conversations dataset, the ami corpus ex-hibits section-structured summaries and containsannotations that link summary sentences to corre-sponding supporting utterances.
our experimentswith ami data show the same trends, favoringpipelines that demand more from the extractivecomponent.
these results speak to the wider use-fulness of our proposed approaches, ext2secand cluster2sent, whenever section-structuredsummaries and annotated evidence utterances areavailable..our best performing model, cluster2sent(figure 1), demands the most of the extractivemodule, requiring that it both select and groupeach subsection’s noteworthy utterances.
interest-ingly, we observe that given oracle (per-subsection)noteworthy utterances, a simple proximity-based.
clustering heuristic leads to similar performanceon soap note generation as we obtain when us-ing ground-truth clusters—even though the groundtruth noteworthy utterances are not always local-ized.
applied with predicted noteworthy utterancesand clusters, this approach achieves the highestrouge scores and produces the most useful (fac-tual, coherent, and non-repetitive) sentences asrated by human experts.
as an additional beneﬁt ofthis approach, due to the smaller lengths of the in-put and output sequences involved, we can feasiblytrain large transformer-based abstractive summa-rization models (e.g., t5), whose memory require-ments grow quadratically with sequence length.
additionally, our approach localizes the preciseutterances upon which each soap note sentencedepends, enabling physicians to verify the correct-ness of each sentence and potentially to improvethe draft by highlighting the correct noteworthyutterances (versus revising the text directly).
in summary, we contribute the following:.
• the ﬁrst pipeline for drafting entire soap.
notes from doctor-patient conversations..• a new collection of extractive-abstractiveapproaches for generating long section-segmented summaries of conversations, in-cluding new methods that leverage annota-tions attributing summary sentences to conver-sation utterances..• a rigorous quantitative evaluation of our pro-posed models and appropriate baselines forboth the extractive and abstractive compo-nents, including sensitivity of the pipeline tosimulated asr errors..code.
2ourdataset:modular-summarization.
trained models.
andthe amihttps://github.com/acmi-lab/.
for.
• a detailed human study to evaluate the fac-tuality and quality of generated soap notes,and qualitative error analysis..4959...dr: so are you taking the monteluekastregularly?pt: yeah, one everyday like you said.dr: good.
and is it helping?
do you havechest pains anymore?pt: no.
no chest pains.dr:that's good.pt: although i do still have some cough.dr: i see.
and do you get, like, mucouswith it or is it dry?pt: umm no it's usually dry.
no mucous....subejctivechief complaint - post viral respiratory allergyreview of systems - denies chest pain.
conﬁrms dry cough.
medications - monteleukast.
fluticasoneobjectivelab results -assessmentassessment - patient feeling better after taking inhaler.
stillhas some cough but no chest pain.
plandiagnostics and appointments - followup in 1 week toassess condition and decide when to stop using the inhaler.dr: good.
and is it helping?
do you havechest pains anymore?pt: no.
no chest painspt: although i do still have some cough.dr: i see.
and do you get, like, mucouswith it or is it dry?pt: umm no it's usually dry.
no mucous.dr: so are you taking the monteluekastregularly?pt: yeah, one everyday like you said.noteworthy for review of systemsnoteworthy for medicationsconversation(2) cluster(3) generatedr: good.
and is it helping?
do youhave chest pains anymore?pt: no.
no chest pains .pt: although i do still have some cough.dr: i see.
and do you get, like, mucouswith it or is it dry?pt: umm no it's usually dry.
no mucous.dr: so are you taking the monteluekastregularly?pt: yeah, one everyday like you said.
(1) extract2 related work.
3.1 medical dataset.
summarization is a well-studied problem innlp (nenkova et al., 2011).
while early works fo-cused on simply extracting important content froma document (erkan and radev, 2004; wong et al.,2008), later approaches attempted to paraphrasethe content into new sentences (abstractive summa-rization) (filippova, 2010; berg-kirkpatrick et al.,2011; wang and cardie, 2013).
following the de-velopment of neural sequence models (sutskeveret al., 2014), more research focuses on neural gen-eration of abstractive summaries (nallapati et al.,2016; see et al., 2017; celikyilmaz et al., 2018).
while many papers summarize news articles, oth-ers summarize conversations, in business meetings(wang and cardie, 2013; zhu et al., 2020), cus-tomer service (liu et al., 2019a), and tourist infor-mation center (yuan and yu, 2019) contexts..in the space of two-step extractive-abstractivesummarization approaches, subramanian et al.
(2019) summarize scientiﬁc papers by ﬁrst extract-ing sentences from it and then abstractively summa-rizing them.
chen and bansal (2018) extract impor-tant sentences from the input and then paraphraseeach of them to generate the abstractive summary.
while they assume that each summary sentence issupported by exactly one source sentence, in ourmedical conversations, many summary sentencessynthesize content spread across multiple dialogueturns (e.g., a series of questions and answers)..past work on abstractive summarization of med-ical conversations has focused on summarizingpatient-nurse conversations with goals includingcapturing symptoms of interest (liu et al., 2019c)and past medical history (joshi et al., 2020).
thesetasks are respectively similar to generating the re-view of systems and past medical history subsec-tions of a soap note.
in contrast, we aim to gen-erate a full-length soap note containing up to15 subsections, and propose methods to addressthis challenge by extracting supporting context forsmaller parts and generating them independently..3 dataset.
we use two different datasets in this work.
the pri-mary medical dataset, developed through a collab-oration with abridge ai, consists of doctor-patientconversations with annotated soap notes.
addi-tionally, we evaluate our summarization methodson the ami dataset (carletta, 2007), comprised ofbusiness meeting transcripts and their summaries..our work builds on a unique resource: a corpus con-sisting of thousands of recorded english-languageclinical conversations, with associated soap notescreated by a work force trained in soap note doc-umentation standards.
our dataset consists of tran-scripts from real-life patient-physician visits fromwhich sensitive information such as names havebeen de-identiﬁed.
the full medical dataset con-sists of 6862 visits consisting of 2732 cardiologistvisits, 2731 visits for family medicine, 989 inter-ventional cardiologist visits, and 410 internist visits.
owing to the sensitive nature of the data, we can-not share it publicly (an occupational hazard ofresearch on machine learning for healthcare)..for each visit, our dataset contains a human-generated transcript of the conversation.
the tran-script is segmented into utterances, each annotatedwith a timestamp and speaker id.
the average con-versation lasts 9.43 minutes and consists of around1.5k words (appendix figure a1).
associatedwith each conversation, we have a human-draftedsoap note created by trained, professional annota-tors.
the annotators who created the soap notesworked in either clinical transcription, billing, or re-lated documentation-related departments, but werenot necessarily professional medical scribes.
thedataset is divided into train, validation and testsplits of size 5770, 500 and 592, respectively..our annotated soap notes contain (up to) 15subsections, each of which may contain multiplesentences.
the subsections vary in length.
theallergies subsections is most often empty, whilethe assessment subsection contains 5.16 sentenceson average (table 1).
the average soap note con-tains 27.47 sentences.
the different subsectionsalso differ in the style of writing.
the medicationssubsection usually consists of bulleted names ofmedicines and their dosages, while the assessmentsubsection typically contains full sentences.
on av-erage, the fraction of novel (i.e., not present in theconversation) unigrams, bigrams, and trigrams, ineach soap note are 24.09%, 67.79% and 85.22%,respectively..each soap note sentence is also annotated withutterances from the conversation which provide evi-dence for that sentence.
a soap note sentence canhave one or more supporting utterances.
on aver-age, each soap sentence has 3.84 supporting utter-ances, but the mode is 1 (appendix figure a1).
werefer to these utterances as noteworthy utterances.
4960subsection.
mean length.
family medical historypast surgical historyreview of systemschief complaintmiscellaneousallergiespast medical historysocial historymedications.
immunizationslaboratory and imaging results.
assessment.
diagnostics and appointmentsprescriptions and therapeutics.
healthcare complaints.
0.230.583.652.172.810.062.930.273.74.
0.112.27.
5.16.
1.651.75.
0.09.table 1: average number of sentences in differentsoap note subsections grouped by parent sections(subjective, objective, assessment, plan, others resp.).
throughout this paper.
throughout this work, wedeal with the 15 more granular subsections ratherthan the 4 coarse sections of soap notes, and thusfor convenience, all further mentions of sectiontechnically denote a soap subsection..3.2 ami dataset.
the ami dataset is a collection of 138 businessmeetings, each with 4 participants with variousroles (e.g., marketing expert, product manager,etc.).
each meeting transcript comes with an asso-ciated abstractive summary that is divided into foursections—abstract, decisions, actions, and prob-lems.
each conversation also has an associatedextractive summary, and there are additional anno-tations linking the utterances in the extractive sum-mary to sentences in the abstractive summary.
forany given sentence in the abstractive summary, werefer to the linked set of utterances in the extractivesummary as its noteworthy utterances.
we note that7.9% of the abstractive summary sentences haveno annotated noteworthy utterances.
to simplifythe analysis, we remove these sentences from sum-maries in the training, validation, and test splits..4 methods.
we investigate the following four decompositionsof the summarization problem into extractive andabstractive phases, ordered from abstraction-heavy.
to extraction-heavy: conv2note takes an end-to-end approach, generating the entire soapnote from the entire conversation in one shot.
ext2note ﬁrst predicts all of the noteworthy ut-terances in the conversation (without regard to theassociated section) and then generates the entiresoap note in one shot from only those utterances.
ext2sec extracts noteworthy utterances, whilealso predicting the section(s) for which they are rel-evant, and then generates each soap section sepa-rately using only that section’s predicted notewor-thy utterances.
cluster2sent attempts to grouptogether the set of noteworthy utterances associ-ated with each summary sentence.
here, we clusterseparately among each set of section-speciﬁc note-worthy utterances and then generate each sectionone sentence at a time, conditioning each on theassociated cluster of utterances..each of these pipelines leaves open manychoices for speciﬁc models to employ for each sub-task.
for the abstractive modules of conv2noteand ext2note, we use a pointer-generator net-work.
the abstractive modules of ext2sec andcluster2sent, which require conditioning onsection are modeled using conditioned pointer-generator networks (described in section 5), andﬁne-tuned t5 models which condition on the sec-tion being generated by means of prepending itto the input.
t5 models could not be used in theconv2note and ext2note settings becausetheir high memory requirement for long inputscould not be accommodated even with 48gb ofgpu memory..for noteworthy utterance extraction, we primar-ily use a hierarchical lstm model and a bert-lstm model as described in the next section.
allmodels are conﬁgured to have a scalar output forbinary classiﬁcation in ext2note, whereas forext2sec and cluster2sent, they have multi-label output separately predicting noteworthinessfor each section.
note that the same utterance canbe noteworthy with respect to multiple sections.
we use the same trained utterance extraction mod-els for both ext2sec and cluster2sent..for the clustering module in cluster2sent,we propose a heuristic that groups together anytwo supporting utterances that are close, meaningthey have less than or equal to τ utterances sep-arating them, where τ is a hyperparameter.
thisprocess is iterated, with the clusters growing in sizeby merging with other singletons or clusters, until.
4961every pair of close utterances have the same clustermembership.
the value of τ is tuned on the vali-dation set.
since each cluster necessarily producesone sentence in the soap note, having too manyor too few clusters can make the soap note toolong or too short, respectively.
therefore, for anygiven value of the hyper-parameter τ and any givensection, the prediction thresholds of the extractorare tuned on the validation set to produce approxi-mately the same number of clusters over the entirevalidation set as present in the ground truth for thatsection.
among ground truth clusters containingmultiple noteworthy utterances, 82% are contigu-ous.
in an experiment where the heuristic is used tocluster the oracle noteworthy utterances for eachsection, and summaries are subsequently generatedvia the abstractive modules from cluster2sent,rouge-1 and rouge-2 metrics deteriorate byless than 1 point as compared to oracle clusterings(appendix table a3), demonstrating our heuristic’seffectiveness..5 model architectures.
pointer-generator networkwe use thepointer-generator network introduced by see et al.
(2017) for conv2note and ext2note.
themodel is a bidirectional lstm-based encoder-decoder model with attention.
it employs a pointermechanism to copy tokens directly from the inputin addition to generating them by predicting gen-eration probabilities for the entire vocabulary.
themodel also computes the weights that govern copy-ing versus generating at each decoding timestep..section-conditioned pointer-generator net-work we modify the pointer-generator networkfor algorithms ext2sec and cluster2sent, tocondition on the (sub)section of the summary to begenerated.
the network uses a new lookup table toembed the section z into an embedding ez.
thesection embedding is concatenated to each inputword embedding fed into the encoder.
the sectionembedding is also appended to the inputs of thedecoder lstm in the same fashion..t5 we use the recently released t5 model (raf-fel et al., 2020) as an abstractive module.
it is anencoder-decoder model, where both encoder anddecoder consist of a stack of transformer layers.
the t5 model is pre-trained on 5 tasks, includ-ing summarization, translation etc.
we use thepre-trained t5 model parameters and ﬁne-tune it.
on our task dataset.
for introducing the section-conditioning in ext2sec and cluster2sent,we simply add the name of the section being gener-ated to the beginning of the input..hierarchical lstm classiﬁer(h-lstm)inthis model, we ﬁrst encode each utterance ui in-dependently by passing its tokens through a bidi-rectional lstm and mean-pooling their encodedrepresentations to get the utterance representationhi.
we pass the sequence of utterance represen-tations {h1, h2, ..., hn} through another bidirec-tional lstm to get new utterance representationswhich incorporate neighboring contexts.
these arethen passed through a sigmoid activated linear layerto predict each utterance’s probability of notewor-thiness with respect to each section..bert-lstm classiﬁer(b-lstm)in thistokens in the utterance ui are passedmodel,through a bert encoder to obtain their contextual-ized representations, which are mean-pooled to getthe utterance representation hi.
the subsequent ar-chitecture exactly mirrors hierarchical lstm, andinvolves passing utterance representations througha bidirectional lstm and linear layer to get out-put probabilities.
bert-lstm is ﬁne-tuned in anend-to-end manner..6 experiments.
we ﬁrst establish two baselines.
randomnoterandomly and uniformly samples a soap notefrom the training set and outputs it as the sum-mary for any input conversation.
oracleextpresents all the ground truth noteworthy utterances(evidence) from the conversation as the soap notewithout any abstractive summarization.
thus, theoracleext baseline has the advantage of con-taining all the desired information (e.g., names ofmedicines) from the conversation, but the disadvan-tage of not being expressed in the linguistic styleof a soap note which leads to lower n-gram over-lap.
the opposite is true for the randomnotebaseline.
both baselines give similar performanceand are outperformed by the simple conv2noteapproach (table 2)..we train the abstractive modules for the 4 ap-proaches described in section 4 with the groundtruth noteworthy utterances as inputs.
to estimatean upper bound on the performance we can reason-ably hope to achieve by improving our noteworthyutterance extractors, we test our models with oracle.
4962noteworthy utterances in the test set.
all algorithmsrelying on oracle noteworthy utterances outperformconv2note, and exhibit a monotonic and signiﬁ-cant rise in rouge scores as we move towards theextraction-heavy end of the spectrum (table 3)3..for predicting noteworthy utterances, we usetwo baselines: (i) logistic regression on tf-idfutterance representations; and (ii) a model witha bidirectional lstm to compute token-averagedutterance representations, followed by a linear clas-siﬁcation layer.
these two models make the pre-dictions for each utterance independent of others.
in contrast, we also use models which incorporatecontext from neighboring utterances: (a) a hierar-chical lstm; and (b) a bert-lstm model asdescribed in section 5. the latter two methodsperform much better (table 5), demonstrating thebeneﬁt of incorporating neighboring context, withbert-lstm performing the best (see appendixtable a6 for section-wise performance)..using predicted noteworthy utterances andclusters instead of oracle ones leads to a dropin rouge scores, butthe performance ofext2sec and cluster2sent is still better thanconv2note (table 2).
for the medical dataset,using a bert-lstm extractor leads to the bestperformance, with cluster2sent outperformingconv2note by about 8 points in rouge-1 (seeappendix table a5 for section-wise performance).
interestingly, the t5-small variant achieves similarperformance to t5-base, despite being only abouta quarter of the latter’s size..performance on ami dataset we see a sim-ilar trend in the rouge scores when applyingthese methods on the ami dataset.
one excep-tion is the poor performance of pointer-generatorbased ext2note, which excessively repeated sen-tences despite using a high coverage loss coef-ﬁcient.
there is a larger gap between the per-formance of the t5-small and t5-base abstrac-tive models on this dataset.
as an extractor, theperformance of bert-lstm is again better thanhlstm (table 5), but when used in tandem withthe abstractive module, rouge scores achieved bythe overall pipeline do not always follow the sameorder.
we also observe that the clustering heuristicdoes not work as well on this dataset.
speciﬁcally,tuning the thresholds of the extractive model, whileﬁxing the clustering threshold τ gave worse resultson this dataset.
tuning the thresholds independent.
3the character ‘-’ represents gpu memory overﬂow.
of the clusters performed better.
however, the bestmethod still outperforms conv2note by about11 rouge-1 points (table 2)..performance with asr errorsin the absenceof human-generated transcripts of conversations,automatic speech recognition (asr) techniquescan be used to transcribe the conversations for useby our models.
to account for asr errors, we ar-tiﬁcially added errors in transcripts of the medicaldataset by randomly selecting some percentage ofthe words and replacing them with phoneticallysimilar words using reﬁnedsoundex (commons)(details in the appendix).
models trained on cleandataset perform worse on a 10% corrupted testdataset (table 4).
since asr errors lead to re-placement of a correct word by only a small setof phonetically similar words, there is still someinformation indicating the original word that canbe used by the models.
when we train our mod-els on data corrupted at the 10% asr error rate,our models recover much of the performance drop(table 4).
notably when simulated asr errors aredialed up to a 30% error rate, (both at train andtest time) we see a smaller performance drop forcluster2sent as compared to conv2note..7 qualitative analysis.
the conditioned pointer-generator and t5 modelsused in cluster2sent learn to place informationregarding different topics in appropriate sections.
hence, given a cluster of supporting utterances,the models can generate different summaries formultiple sections (figure 2).
for example, giventhe same supporting utterances discussing the pa-tient’s usage of lisinopril for low blood pressure, amodel generates “low blood pressure” in the reviewof systems section, and “lisinopril” in medicationssection.
we direct the reader to the appendix forexamples of full-length generated soap notes..interestingly, when the abstractive model isgiven a cluster of utterances that are not relevant tothe section being generated, the model sometimesoutputs fabricated information relevant to that sec-tion such as saying the patient is a non-smoker insocial history, or that the patient has taken a ﬂushot in immunizations .
hence, the quality of pro-duced summaries heavily depends on the ability ofthe extractive step to classify the extracted utter-ances to the correct section.
another cause of falseinformation is the usage of pronouns in clusterswithout a mention of the referred entity.
in such.
4963method.
randomnoteoracleext.
medical dataset.
ami corpus.
r-1.
r-2.
r-l.r-1.
r-2.
r-l.34.9933.07.
12.6912.22.
21.3717.42.
42.4739.97.
11.5511.17.
21.4720.91.conv2note (pg).
49.56.
25.68.
32.87.
39.62.
13.16.
23.95.ext2note (pg + hlstm)ext2note (pg + blstm)ext2note (t5-small + hlstm)ext2note (t5-small + blstm).
ext2sec (pg + hlstm)ext2sec (pg + blstm)ext2sec (t5-small + hlstm)ext2sec (t5-small + blstm).
cluster2sent (pg + hlstm)cluster2sent (pg + blstm)cluster2sent (t5-small + hlstm)cluster2sent (t5-small + blstm)cluster2sent (t5-base + hlstm)cluster2sent (t5-base + blstm).
49.5850.50--.
55.2355.7455.7756.00.
55.4655.6056.8857.1457.2757.51.
24.9125.4--.
27.1427.5428.6429.16.
27.4127.6828.6329.1129.1029.56.
31.6831.93--.
35.1536.0937.5038.38.
35.8136.2936.7837.4337.3838.06.
21.2821.7140.4840.36.
43.7540.4842.4545.44.
46.1942.3145.1042.3850.5245.91.
7.066.8313.8213.73.
15.2515.6115.2016.59.
16.6415.9215.0615.3617.5617.70.
15.9615.6924.6424.13.
23.4623.3123.9226.14.
24.2923.5123.5223.924.8925.24.table 2: rouge scores achieved by different methods on the two datasets.
figure 2: noteworthy utterance clusters summarized in different ways for different sections by the abstractivesummarization modules of cluster2sent (utterances were slightly obfuscated for privacy reasons).
situations, t5 models frequently replace the pro-noun with some arbitrary entity (e.g.
“she” with“daughter”, compounds with “haemoglobin”, andmedicines with “lisinopril”)..occasionally, the abstractive module producesnew inferred information that is not mentioned ex-plicitly in the conversation.
in one instance, themodel generated that the patient has a history ofheart disease conditioned on a cluster that men-.
tioned he/she takes digoxin, a popular medicine forheart disease.
similarly, the model can infer pastmedical history of “high cholesterol” upon seeingpravastatin usage.
such inferences can also lead toincorrect summaries, e.g., when a doctor explainedthat a patient has leaky heart valves, a model addeda sentence to the diagnostics and appointments sec-tion saying “check valves”..cluster2sent summarizes localized regions.
4964cluster of utterances subsection summary-pg summary-t5 dr that one thing that we can do to reduce risk with that cholesterol is 100 mg metoprolol.
dr but i want you on two a day.
prescriptions and therapeutics metoprolol 100 mg twice a day.
metoprolol 100 mg twice a day.
dr um, the first thing i didn't get was that, um, are you, you 're on digoxin, right?
pt um-hum.
past medical history  history of heart disease.
patient is on digoxin.
medications digoxin.
digoxin.
assessment the patient is on digoxin.
patient is on digoxin.
dr uh, and have you had any more chest pain?
pt i did, yeah, i do.
review of systems confirms chest pain.
confirms chest pain.
dr uh, and have you had any more chest pain?
pt not really.
no.
review of systems denies chest pain.
denies chest pain.
dr this one, this amlodipine that you are taking it's a good pill for high blood pressure.
pt okay dr but right now your blood pressure is a bit low.
pt um-hum dr so i will reduce it to half a pill per day, alright?
chief complaint high blood pressure.
low blood pressure.
review of systems blood pressure is a bit low.
blood pressure is a bit low.
past medical history high blood pressure.
low blood pressure.
prescriptions and therapeutics amlodipine half a pill a day.
reduce amlodipine to half a pill per day.
dr and nothing like that?
pt i , and , of course , when you break something , like i fractured my leg , i don't think that whatever that feeling is ever goes away completely.
chief complaint leg swelling.
fractured leg.
past medical history leg pain.
fractured leg.
medications patient is on leg.
xarelto.
immunizations patient had a flu shot in the past.
patient had immunizations.
diagnostics and appointments the patient will undergo leg surgery.
follow-up.
medical dataset.
ami corpus.
medical conversations.
ami corpus.
method.
pg.
t5-small.
pg.
t5-small.
metric.
lr.
ls hls bls.
hls.
bls.
ext2noteext2seccluster2sent.
52.9561.0063.63.
-62.3766.50.
20.4443.3251.86.
41.1046.8554.23.table 3: rouge-1 achieved on test set when using theabstractive models with oracle noteworthy utterancesand clusters (more results with oracle in the appendix).
method.
r-1.
r-2.
r-l.train on clean data + test on data with 10% error rate.
conv2note(pg)cluster2sent(pg + bls)cluster2sent(t5-base+ bls).
46.5251.8454.88.
22.6023.7426.65.
30.4532.9435.88.train and test on data with 10% error rate.
conv2note(pg)cluster2sent(pg + bls)cluster2sent(t5-base+ bls).
48.8554.6856.35.
24.8526.5928.50.
31.2735.7037.04.train and test on data with 30% error rate.
conv2note(pg)cluster2sent(pg + bls)cluster2sent(t5-base+ bls).
45.1653.6955.90.
22.2625.8827.73.
30.1435.1236.06.table 4: performance of models trained and tested ondata with different simulated asr error rates.
bls:bert-lstm.
of the conversation independently, which may leadto contradictions in the soap note.
in one visit,the patient was asked about chest pain twice—oncein the beginning to get to know his/her current state,and once as a question about how he/she felt justbefore experiencing a fall in the past.
this led tothe model generating both that the patient deniedchest pain as well as conﬁrmed chest pain, withoutclarifying that one statement was for the presentand another for the past..8 human evaluation.
we asked trained human annotators to evaluate gen-erated soap notes for 45 conversations.
every sen-tence in each soap note was labeled according tovarious quality dimensions such whether it was fac-tually correct, incoherent, irrelevant, redundant, orplaced under an inappropriate section.
the detailedstatistics of annotations received for each qualitydimension are provided in the appendix.
we alsocollected aggregate annotations for the comprehen-siveness of each soap note and the extent to whichit verbatim copied the transcript on a 5-point likertscale..human raters were presented with a web in-.
accuracy96.0ma-auc 78.129.5ma-f187.3mi-auc31.2mi-f1.
96.179.331.087.632.9.
96.590.038.692.739.6.
96.590.540.993.341.1.
93.7783.8119.9593.2143.76.
94.1690.7633.0894.9049.93.table 5:performance on multilabel classiﬁca-tion of noteworthy utterances with logistic regres-sion(lr), lstm(ls), hierarchical-lstm(hls) andbert-lstm(bls).
ma:macro-averaged.
mi:micro-averaged.
terface showing the conversation, along witha search feature to help them in looking updesired information.
the summaries gener-ated by three methods (conv2note(pointer-generator), cluster2sent(pointer-generator)and cluster2sent(t5-base)) were presented inrandom order to hide their identities.
for eachsentence, we asked for (i) factual correctness ofthe sentence; (ii) if the statement is simply repeat-ing what has already been mentioned before; (iii)if the statement is clinically irrelevant; (iv) if thestatement is incoherent (not understandable due togrammatical or semantic errors); and (v) if the state-ment’s topic does not match the section in which itis placed.
in addition, we asked two separate ques-tions for rating the overall summary on a scale of1-5 for its (i) comprehensiveness and (ii) extent ofverbatim copying from conversation.
the humanevaluation of the soap notes was done by work-ers who had also participated in the creation of thedataset of soap notes.
hence, they had alreadybeen extensively trained in the task of soap notecreation, which gave them appropriate knowledgeto judge the soap notes..to quantify the performance among differentmethods, we consider a scenario where each gener-ated soap note has to be post-edited by discard-ing undesirable sentences.
for a generated soapnote, we deﬁne its yield as the fraction of its totalsentences that are not discarded.
the sentencesthat are retained are those that are both factuallycorrect and were not labeled as either repetitiveor incoherent.
the human annotations show thatboth cluster2sent-based methods tested pro-duced a higher yield than the conv2note base-line (p< 0.02).
t5-base performs better than condi-tioned pointer-generator as the abstractive modulein cluster2sent setting, producing signiﬁcantlymore yield (table 6).
t5 also produces fewer inco-.
4965medical conversations.
ami corpus.
metric c2n c2s-p c2s-t c2n c2s-p c2s-t.length%yieldcompcopy.
21.262.02.442.18.
28.269.02.422.64.
28.474.72.762.76.
20.727.222.301.80.
17.930.222.551.80.
19.0559.453.751.90.
6:.
averages.
different metrics.
fortablewithconv2note(c2n),(c2s-t).
pointer-generatorcomp:comprehensiveness, copy:amount of copying.
length: number of sentences generated..ofcluster2sentand t5-base.
(c2s-p).
herent sentences (appendix table a4) likely dueto its exposure to a large number of well-formedcoherent sentences during pretraining..we conducted an analogous human evaluationof summaries generated for all 20 conversations inthe test set of the ami corpus, and saw a similartrend in the expected yield for different methods.
notably, for the ami corpus, conv2note pro-duced a very high proportion of redundant sen-tences (> 0.5) despite using the coverage loss,while the pointer-generator based cluster2sentproduced a high proportion of incoherent sentences(appendix table a4)..9 conclusion.
this paper represents the ﬁrst attempt at generatingfull-length soap notes by summarizing transcriptsof doctor-patient conversations.
we proposed aspectrum of extractive-abstractive summarizationmethods that leverage: (i) section-structured formof the soap notes and (ii) linked conversationutterances associated with every soap note sen-tence.
the proposed methods perform better than afully abstractive approach and standard extractive-abstractive approaches that do not take advantageof these annotations.
we demonstrate the widerapplicability of proposed approaches by showingsimilar results on the public ami corpus which hassimilar annotations and structure.
our work demon-strates the beneﬁts of creating section-structuredsummaries (when feasible) and collecting evidencefor each summary sentence when creating any newsummarization dataset..ethics statement.
the methods proposed in this work to generatesoap notes involve neural models that sometimesgenerate factually incorrect text (maynez et al.,2020).
the detection and correction of such factual.
errors in automatically generated summaries is anactive area of research (cao et al., 2018; zhanget al., 2020; dong et al., 2020).
we emphasize thatthe methods are intended to be used with super-vision from a medical practitioner who can checkfor factual errors and edit the the generated soapnote if needed.
we have estimated the frequency ofsuch factual errors (appendix table a4) and char-acterized multiple types of errors seen in generatedsoap notes in section 7, for which the medicalpractitioners should remain vigilant.
for example,there is a bias to incorrectly generate informationthat occur frequently in speciﬁc sections (e.g.
“pa-tient took ﬂu shot”), and to replace pronouns withfrequently seen entities (such as “lisinopril” for ref-erences to medicine).
all data used in this studywas manually de-identiﬁed before we accessed it.
deploying the proposed methods does not requirelong-term storage of conversations.
after the corre-sponding soap notes are generated, conversationscan be discarded.
hence, we do not anticipate anyadditional privacy risks from using the proposedmethods..acknowledgements.
this work was funded by the center for machinelearning and health in a joint venture betweenupmc and carnegie mellon university.
we grate-fully acknowledge support from abridge ai, inc.for creating the dataset of soap notes and provid-ing human resources for evaluation..references.
taylor berg-kirkpatrick, dan gillick, and dan klein.
2011. jointly learning to extract and compress.
inproceedings of the 49th annual meeting of the asso-ciation for computational linguistics: human lan-guage technologies-volume 1, pages 481–490.
as-sociation for computational linguistics..ziqiang cao, furu wei, wenjie li, and sujian li.
2018.faithful to the original: fact aware neural abstrac-in proceedings of the aaaitive summarization.
conference on artiﬁcial intelligence, volume 32..jean carletta.
2007. unleashing the killer corpus: ex-periences in creating the multi-everything ami meet-ing corpus.
language resources and evaluation,41(2):181–190..asli celikyilmaz, antoine bosselut, xiaodong he, andyejin choi.
2018. deep communicating agents forin proceedings of theabstractive summarization.
2018 conference of the north american chapter of.
4966the association for computational linguistics: hu-man language technologies, volume 1 (long pa-pers), pages 1662–1675..yen-chun chen and mohit bansal.
2018. fast abstrac-tive summarization with reinforce-selected sentencerewriting.
in proceedings of the 56th annual meet-ing of the association for computational linguistics(volume 1: long papers), pages 675–686..apache commons.
reﬁnedsoundex..yue dong, shuohang wang, zhe gan, yu cheng,jackie chi kit cheung, and jingjing liu.
2020.multi-fact correction in abstractive text summariza-in proceedings of the 2020 conference ontion.
empirical methods in natural language processing(emnlp), pages 9320–9331..g¨unes erkan and dragomir r radev.
2004. lexrank:graph-based lexical centrality as salience in textsummarization.
journal of artiﬁcial intelligence re-search, 22:457–479..katja filippova.
2010. multi-sentence compression:finding shortest paths in word graphs.
in proceed-ings of the 23rd international conference on compu-tational linguistics, pages 322–330.
association forcomputational linguistics..bogdan gliwa, iwona mochol, maciej biesek, andaleksander wawer.
2019.samsum corpus: ahuman-annotated dialogue dataset for abstractivesummarization.
in proceedings of the 2nd workshopon new frontiers in summarization, pages 70–79..max grusky, mor naaman, and yoav artzi.
2018.newsroom: a dataset of 1.3 million summariesarxiv preprintwith diverse extractive strategies.
arxiv:1804.11283..anirudh joshi, namit katariya, xavier amatriain, andanitha kannan.
2020. dr. summarize: global sum-marization of medical dialogue by exploiting localstructures.
arxiv preprint arxiv:2009.08666..kundan krishna, amy pavel, benjamin schloss, jef-frey p bigham, and zachary c lipton.
2021. ex-tracting structured data from physician-patient con-versations by predicting noteworthy utterances.
inexplainable ai in healthcare and medicine, pages155–169.
springer..gogi kumar and adam mezoff.
2020..physicianburnout at a children’s hospital: incidence, interven-tions, and impact.
pediatric quality & safety, 5(5)..chunyi liu, peng wang, jiang xu, zang li, andjieping ye.
2019a.
automatic dialogue summarygeneration for customer service.
in proceedings ofthe 25th acm sigkdd international conference onknowledge discovery & data mining, pages 1957–1965..zhengyuan liu, jia hui hazel lim, nur farah ainsuhaimi, shao chuen tong, sharon ong, angelang, sheldon lee shao guang, michael ross mac-donald, savitha ramasamy, pavitra krishnaswamy,et al.
2019b.
fast prototyping a dialogue compre-hension system for nurse-patient conversations onsymptom monitoring.
in naacl-hlt (2)..zhengyuan liu, angela ng, sheldon lee, ai ti aw,and nancy f chen.
2019c.
topic-aware pointer-generator networks for summarizing spoken conver-sations.
arxiv preprint arxiv:1910.01335..joshua maynez, shashi narayan, bernd bohnet, andryan mcdonald.
2020. on faithfulness and factu-ality in abstractive summarization.
in proceedingsof the 58th annual meeting of the association forcomputational linguistics, pages 1906–1919..ramesh nallapati, bowen zhou, cicero dos santos,c¸ a˘glar gulc¸ehre, and bing xiang.
2016. abstrac-tive text summarization using sequence-to-sequencein proceedings of the 20thrnns and beyond.
signll conference on computational natural lan-guage learning, pages 280–290..ani nenkova, kathleen mckeown, et al.
2011. auto-matic summarization.
foundations and trends® ininformation retrieval, 5(2–3):103–233..colin raffel, noam shazeer, adam roberts, katherinelee, sharan narang, michael matena, yanqi zhou,wei li, and peter j liu.
2020. exploring the lim-its of transfer learning with a uniﬁed text-to-texttransformer.
journal of machine learning research,21(140):1–67..benjamin schloss and sandeep konam.
2020. towardsan automated soap note: classifying utterances fromin machine learning formedical conversations.
healthcare conference, pages 610–631.
pmlr..abigail see, peter j liu, and christopher d manning.
2017. get to the point: summarization with pointer-generator networks.
in proceedings of the 55th an-nual meeting of the association for computationallinguistics (volume 1: long papers), pages 1073–1083..christine sinsky, lacey colligan, ling li, mirelaprgomet, sam reynolds, lindsey goeders, johannawestbrook, michael tutty, and george blike.
2016.allocation of physician time in ambulatory practice:a time and motion study in 4 specialties.
annals ofinternal medicine..sandeep subramanian, raymond li, jonathan pi-lault, and christopher pal.
2019.on extrac-tive and abstractive neural document summarizationwith transformer language models.
arxiv preprintarxiv:1909.03186..ilya sutskever, oriol vinyals, and quoc v le.
2014.sequence to sequence learning with neural networks.
in advances in neural information processing sys-tems, pages 3104–3112..4967zhaopeng tu, zhengdong lu, yang liu, xiaohua liu,and hang li.
2016. modeling coverage for neuralmachine translation.
in acl, pages 76–85..lu wang and claire cardie.
2013..domain-independent abstract generation for focused meet-ing summarization.
in proceedings of the 51st an-nual meeting of the association for computationallinguistics (volume 1: long papers), pages 1395–1405..kam-fai wong, mingli wu, and wenjie li.
2008. ex-tractive summarization using supervised and semi-supervised learning.
in proceedings of the 22nd in-ternational conference on computational linguistics(coling 2008), pages 985–992..lin yuan and zhou yu.
2019. abstractive dialog sum-marization with semantic scaffolds.
arxiv preprintarxiv:1910.00825..yuhao zhang, derek merck, emily tsai, christo-pher d. manning, and curtis langlotz.
2020. op-timizing the factual correctness of a summary: astudy of summarizing radiology reports.
in proceed-ings of the 58th annual meeting of the associationfor computational linguistics, pages 5108–5120..chenguang zhu, ruochen xu, michael zeng, andend-to-end abstractivearxiv preprint.
xuedong huang.
2020.summarization for meetings.
arxiv:2004.02016..4968worse than conv2note, highlighting the impor-tance of pretraining..sample generated soap notes.
due to privacy concerns, we can not publish conver-sations from our dataset.
here, we present an obfus-cated conversation from our test dataset, modiﬁedby changing sensitive content such as medicines,diseases, dosages (figure a2).
we also present thesoap note generated by our best method, as wellas the ground truth..model implementation details.
for the hierarchical lstm classiﬁer, we have aword embedding size of 128 and both bidirectionallstms have a hidden size of 256. for bert-lstm, the bert embeddings are initialized frombert-base-uncased (768 dimensions).
lstms ineither direction have a hidden-layer of size 512 andthe entire model is optimized end-to-end with alearning-rate of 0.001. for bert-lstm, an inputconversation is divided into chunks of 128 utter-ances.
due to gpu constraints, these chunks areprocessed one at a time.
the pointer-generatormodels have a word embedding size of 128, anda hidden size of 256 for both the encoder and thedecoder.
the section embeddings used in section-conditioned pointer-generator network have 32 di-mensions.
during training of all pointer-generatormodels, the model is ﬁrst trained without cover-age loss (tu et al., 2016) to convergence, and thentrained further with coverage loss added.
we triedcoverage loss coefﬁcients varying from 0.5 to 8..the pointer-generator models were trained us-ing adam optimizer before coverage and usingsgd after adding coverage.
we tried learningrates between 10−4 and 10−3 with adam.
thenext word prediction accuracy was used as thevalidation criterion for early stopping while train-.
appendix.
decoder results with oracle extracts.
we present additional quantitative results (ta-ble a3), including (i) the rouge scores onthe test set when using oracle noteworthy utter-ances with both oracle and predicted clusters (forcluster2sent models).
(ii) two ablations onext2sec: allext2sec uses binary classiﬁca-tion to extract all noteworthy utterances (not per-section), and an abstractive decoder that condi-tions on the section; while ext2secnoconduses a multilabel classiﬁcation based extractor butdoes not use section-conditioning in the abstrac-tive module.
both methods mostly perform worsethan ext2sec demonstrating the beneﬁt of us-ing both section-speciﬁc extraction and section-conditioning in abstractive decoder..impact of copy mechanism.
when we do not use copy mechanism in the pointer-generator model, we observed a drop in its per-formance in the cluster2sent setting with or-acle noteworthy noteworthy utterances and clus-ters(table a1).
hence, we have used copy mecha-nism in all the pointer-generator models we trainin this work..impact of pretraining.
when training a randomly initialized t5-basemodel on the medical dataset, even in clus-ter2sent setting with oracle clusters, it only gota rouge-1 around 40 (table a2).
this is over16 points lower than what we get by starting withoff-the-shelf pretrained t5 parameters, and is even.
copy mechanism r-1.
r-2.
r-l.presentabsent.
63.6361.92.
35.6234.37.
48.8547.86.table a1: impact of copy mechanism in peformance ofa pointer-generator model on medical dataset in clus-ter2sent using oracle noteworthy utterance clusters.
model.
r-1.
r-2.
r-l.pretrained t5randomly initialized t5.
66.4540.07.
39.0120.95.
52.4632.42.table a2: impact of pretraining on performance of t5-base model on medical dataset with cluster2sentusing oracle noteworthy utterance clusters.
figure a1: histogram of number of words in a conver-sation and the number of evidence utterances per sum-mary sentence for the medical dataset.
4969table a3: rouge scores achieved by different abstractive decoders using oracle noteworthy utterances.
method.
ext2note (pg)ext2note (t5-small)allext2sec (pg)allext2sec (t5-small)ext2secnocond (pg)ext2secnocond (t5-small)ext2sec (pg)ext2sec (t5-small)cluster2sent (pg)cluster2sent (t5-small)cluster2sent (t5-base).
cluster2sent (pg+clustering heuristic)cluster2sent (t5-small+clustering heuristic)cluster2sent (t5-base+clustering heuristic).
ing abstractive modules, with the exception ofcoverage-augmented models that used a combina-tion of crossentropy and coverage loss.
micro-averaged auc was used as the validation criterionfor training of extractive modules..we employ beam search with beam size 4 todecode outputs from both models.
for the vanillapointer-generator model used in conv2note andext2note, we modiﬁed the beam search proce-dure to make sure that all the soap note sectionsare generated in proper order.
we start the beamsearch procedure by feeding the header of the ﬁrstsection (chief complaint).
whenever the model pre-dicts a section header as the next word and it showsup in a beam, we check if it is the next section tobe generated.
if not, we replace it with the correctnext section’s header.
any end-of-summarytokens generated before all the sections have beenproduced are also replaced similarly.
note that pro-ducing all sections simply means that the headersfor each section have to be generated, and a sectioncan be left empty by starting the next section imme-diately after generating the previous header.
thedecoding length for beam search is constrainedto be between 5th and 95th percentile of the tar-get sequence length distribution, calculated on thetraining set..medical dataset.
ami corpus.
r-1.
r-2.
r-l.r-1.
r-2.
r-l.52.95-50.74-56.1058.6961.0062.3763.6366.5066.45.
63.1266.0865.94.
27.6-24.33-32.0534.9233.6436.3935.6238.4139.01.
35.0837.7338.26.
32.87-32.18-43.2347.2445.249.1148.8551.7352.46.
47.9650.6651.31.
21.2341.1040.2041.6842.5148.1443.3046.8551.8654.2357.42.
47.1747.5351.24.
6.7114.1213.7115.4315.7118.4916.5618.1921.8622.9024.45.
18.9919.7021.47.
14.9525.0322.5224.7223.7928.2324.8328.7431.8434.5435.70.
27.3128.9529.81.words in the conversation and replacing them withphonetically similar words.
to reduce the searchspace of possible candidates for each word, we usethe suggest() function taken from the pyenchant4library that provides auto-correct suggestions forthe input word.
each suggestion is then passedthrough the reﬁned soundex algorithm to ﬁndthe phonetic distance between the original and thesuggested word.
we use the pyphonetics5 packagefor a python implementation of this algorithm.
forour ﬁnal candidate list, we choose words that areat phonetic distance of 1 from the original word.
finally, a candidate is chosen at random from thislist to replace the original..more experimental details.
standard.
we trained models on multiple nvidia quadrortx 8000, rtx 2080ti and v100 gpus.
the extractive modules were evaluated us-ingfromscikit-learn 6 and quality of summarieswere evaluated using rouge scores calculatedwith the pyrouge python package 7 which is awrapper around the rouge-1.5.5 perl script..classiﬁcation metrics.
simulating asr errors.
we simulate asr errors at any given percentagerate by randomly selecting the percentage of the.
4https://pypi.org/project/pyenchant/5https://pypi.org/project/pyphonetics/6https://scikit-learn.org7https://pypi.org/project/pyrouge.
4970medical conversations.
ami corpus.
count.
c2n c2s-p c2s-t.c2n c2s-p c2s-t.total sentencesrepetitiveincoherenttrue statementsfalse statementstruthfulness undecidedirrelevantunder incorrect section.
95696162587100112556.
1268127158848116193442.
127714758931125162439.
4142139897132144.
358141341037532152.
38114272276845218.table a4: number of sentences produced by different methods that were judged to have different listedcharacteristics by human raters.
c2n:conv2note, c2s-p:cluster2sent with pointer-generator, c2s-t:cluster2sent with t5-base.
bert-lstm used for medical dataset, hierarchical-lstm used for ami corpus..subsection.
rouge-1 rouge-2 rouge-l.n.l.chief complaintreview of systemspast medical historypast surgical historyfamily medical historysocial historymedicationsallergiesmiscellaneousimmunizationslaboratory and imaging resultsassessmentdiagnostics and appointmentsprescriptions and therapeuticshealthcare complaints.
28.1228.3537.7043.0836.4937.8223.536.6311.6127.4941.1815.3135.7033.5115.79.
43.5943.2851.8057.0450.1356.3047.6438.3224.9054.8155.1125.3550.4348.1029.57.
5925145472307297549214152544857048844643.
11.4629.2417.8110.3616.1410.3315.288.5734.447.3219.37132.4117.6718.7316.74.table a5: average rouge scores (from cluster2sent t5base+blstm) for each section of soap note (n-number of test datapoints with the section populated, l-average number of words in ground truth).
section.
base rate(%).
precision recall.
f1 accuracy.
auc.
chief complaintreview of systemspast medical historypast surgical historyfamily medical historysocial historymedicationsallergiesmiscellaneousimmunizationslaboratory and imaging resultsassessmentdiagnostics and appointmentsprescriptions and therapeuticshealthcare complaints.
34.7151.3536.0033.8052.3159.8151.6630.8624.0663.6450.0038.0955.6041.2820.47.
33.9351.8236.5234.5045.2554.8749.1312.4416.1764.6255.1542.0140.1638.4321.90.
34.3151.5836.2634.1448.5357.2350.3617.7319.3464.1252.4539.9646.6339.8121.17.
95.9595.0495.6398.6899.7099.5695.6999.8295.0099.9697.5482.0898.0796.3999.60.
86.8193.1288.0093.7499.2395.4192.0289.4680.0597.6393.8476.8994.2292.4085.93.table a6: performance of bert-lstm on extracting noteworthy utterances for various soap sections.
44.3446.8853.4858.4451.9457.7249.5639.2928.8755.9558.3639.0152.8550.5330.11.
3.125.103.410.990.310.534.450.163.710.052.4614.192.103.110.25.
4971figure a2: sample conversation (obfuscated) with soap note generated by the best method and the ground truth.
4972 predicted relevant subsections  conversation utterances  (pt) (a)  dr okay, so, um, we are going to talk a little bit about being a metformin candidate .
(cc) (pmh) (a)  dr um , we have talked about your hemoglobin and the things , what are , so what are the things that , that keep you from , um , from managing your anemia well ?
dr, i know there’s a lot of stuff that troubles you.
(m)  pt snacking and stress eating.
pt eating late in the evenings instead of, um, at a reasonable time -    dr right.
pt at night, late.
(m) (a)  pt poor meal planning.
(pmh) (lir) (a)  dr right, and i think that’s in the, we can all take a little note for but one of things that really got me worried because your last  hemoglobin was really low -    pt uh-huh.
(lir) (a)  dr it was below , it was below 10 , and we 've had this consistent pattern and you 've really , i mean , you really have given it an effort and i have to give it up to you that you 've been trying and , um , so we 're down to like just a couple of options and so i want to just kind of put them before you .
(a) (pt) (med)  dr i 've got, i 'm, i 'm considering once a day metformin with you at some point .
(a)  dr um, i do n't want to use that as a threat.
(a)  dr i do n't want to use it as like a, oh , you 've been a bad patient you deserve to be on metformin .
(a) (pt)  dr um , i do have one other option , um , but i want to counsel you that , that metformin , even if , if we did , we do go to it , it is not a punishment .
(a)  dr it is something to kind of get your baseline down to a regular, regular situation and you only have to do it once a day.
(a)  dr um, and i know that one of the things that we have for anemics is their eating habits .
(a) (pt)  dr and, so , i am proposing as instead of using metformin this time , um , that we use something called lipitor for the , for the eating at nighttime .
(a)   dr um, it’s supposed to reduce the incidence of having those nighttime cravings so that you can work , you can do your things , you can plan a little bit better .
(a)  dr it 's , it’s originally for adhd so some people actually feel a little bit more focused , um , and controlled but it also affects appetite centers and so it’s supposed to do it for the longer term as opposed to using like a fen phen , um , so , which is short term .
dr so, um , i 'm really hoping with your interest in it and with the coverage hopefully , i know , with your particular plan it should be covered and we can get a discount .
(pt)  dr um, we do it once a day with your other medications , which are actually pretty minor .
(da)  dr um, and then we check you again in eight weeks .
(da)  pt okay.
dr all right?
(a) (da)  dr and, so what we do is we say , you know , it should be , we usually will do three months but then eight weeks we should see some difference from today .
dr we should see some kind of improvement and then we can sort of celebrate that in and of itself, if that’s okay with you.
pt that sounds great.
(da)  dr cool, all right well we will plan to meet again in eight weeks .
pt okay.
dr and, uh , and we 'll go from there .
pt okay.
dr cool, all right , cool .
cluster2sent+t5base  chief complaint: anemia .
past medical history: anemia .
medications: metformin .
miscellaneous: patient has snacking and stress eating .
poor meal planning .
laboratory and imaging results: last hemoglobin was low at 10 .
assessment: discussed about being a metformin candidate .
discussed about hemoglobin and the things that keep patient from managing anemia well.
discussed that patient 's last hemoglobin was really low , it was really low , it was really low , it was really low , it was really low , it was really low , and we have had this consistent pattern and you really have given it effort and we have had this.
followup in 8 weeks .
diagnostics and appointments: followup in 8 weeks .
prescriptions and therapeutics: the patient will be a metformin candidate.
metformin once a day.
coumadin twice a day with other medications, which are actually pretty minor .
ground truth                                                                   chief complaint: follow-up.
anemia.
past medical history: anemia.
medications: metformin miscellaneous: patient is not following a correct diet plan (snacking and stress eating).
laboratory and imaging results: hemoglobin was really low below 10. assessment: anemia.
night time eating.
discussed with the patient the importance of bringing up the hemoglobin to a considerable level and also discussed couple of other options.
discussed the new medication called lipitor which will help the patient bringing up the hemoglobin and can take it once a day with other medications.
discussed that the lipitor will reduce the nighttime cravings so that the patient can plan better (originally for adhd to better focus).
discussed with the patient that with the current insurance coverage , the patient may get a discount with lipitor.
diagnostics and appointments: advised to follow up in 8 weeks.
prescriptions and therapeutics: metformin.
lipitor.