focus attention: promoting faithfulness and diversity in summarization.
rahul aralikatte∗university of copenhagenrahul@di.ku.dk.
shashi narayangoogle researchshashinarayan@google.com.
joshua maynezgoogle researchjoshuahm@google.com.
sascha rothegoogle researchrothe@google.com.
ryan mcdonald∗asappryanmcd@asapp.com.
abstract.
professionalsummaries are written withdocument-level information, such as the themeof the document, in mind.
this is in contrastwith most seq2seq decoders which simultane-ously learn to focus on salient content, whiledeciding what to generate, at each decodingstep.
with the motivation to narrow this gap,we introduce focus attention mechanism, asimple yet effective method to encourage de-coders to proactively generate tokens that aresimilar or topical to the input document.
fur-ther, we propose a focus sampling methodto enable generation of diverse summaries, anarea currently understudied in summarization.
when evaluated on the bbc extreme summa-rization task, two state-of-the-art models aug-mented with focus attention generate sum-maries that are closer to the target and morefaithful to their input documents, outperform-ing their vanilla counterparts on rouge andmultiple faithfulness measures.
we also em-pirically demonstrate that focus sampling ismore effective in generating diverse and faith-ful summaries than top-k or nucleus sampling-based decoding methods..1.introduction.
document summarization — producing the shorterversion of a document while preserving salient in-formation (mani, 2001; nenkova and mckeown,2011) — is challenging even for humans.
today,systems can generate summaries with a high levelof ﬂuency and coherence.
this is due to recentadvances such as sequence-to-sequence architec-tures (seq2seq) with attention and copy mechanism(hochreiter and schmidhuber, 1997; bahdanauet al., 2015; gu et al., 2016), fully attention-basedtransformer architectures (vaswani et al., 2017),and large pretrained language models (devlin et al.,.
∗work done when authors were interning/working at.
google..a.b.c.gold: australia has expelled an israeli diplomat saying israel wasbehind the forging of australian passports linked to the murder of ahamas operative in dubai.
pegasus: australia has expelled an israeli diplomat after concludingthat forged australian passports used in the killing of a hamas militantin dubai were issued by israel.
our pegfame model: the australian government has expelled anisraeli diplomat over the use of forged australian passports in the killingof a hamas militant in dubai..pegasus with top-k samplingisrael has summoned the australian ambassador to complain after theaustralian government said forged passports used in the killing of ahamas operative in dubai belonged to netanyahu’s foreign ministry.
the australian government has ordered israel to withdraw an ofﬁcerover the use of forged australian passports used by the 2013 murder ofa lebanese opposition ﬁgure in dubai.
pegasus with nucleus samplingisrael hasracuse withdrawn an envoy after the australian governmentsaid it concluded that israeli agents used forged passports used to kill adubai bendigo businessman.
the australian government has recalled an israeli diplomat over accu-sation that fake australian passports used 436 kilometres (300 miles)from canberra in the death of a hamas militant were stolen by israeliagents..our pegfame model with novel focus samplingaustralia has expelled an israeli diplomatic staff after accusing the coun-try’s security agency, the israeli military’s intelligence agency, of beingresponsible for the use of australian visas used in the killing of a pales-tinian.
the australian government has expelled an israeli diplomatic staff afterit said the country was responsible for the use of australian visas usedin the killing of a palestinian in the middle east..figure 1: block a shows the best predictions frompegasus and our pegfame (pegasus with fame)model, along with the gold summary for an xsumarticle.
block b presents diverse summaries gener-ated from pegasus using top-k and nucleus sampling.
block c shows diverse summaries generated using ourpegfame model with focus sampling.
the text in or-ange is not supported by the input article..2019; radford et al., 2018; yang et al., 2019; liuet al., 2019; dong et al., 2019a; song et al., 2019;lewis et al., 2019; rothe et al., 2020; raffel et al.,2019; zhang et al., 2019)..however, in terms of summary quality, many chal-lenges remain.
for example, generating summariesthat are faithful to the input is an unsolved prob-lem (kryscinski et al., 2020; maynez et al., 2020;gabriel et al., 2020).
furthermore, there can bemultiple equally good summaries per source docu-.
proceedingsofthe59thannualmeetingoftheassociationforcomputationallinguisticsandthe11thinternationaljointconferenceonnaturallanguageprocessing,pages6078–6095august1–6,2021.©2021associationforcomputationallinguistics6078ment.
neural generation models fail to account forthis and tend to generate outputs with low diversitydue to standard likelihood training, approximatedecoding objectives, and lack of high quality multi-reference datasets (fan et al., 2018; kulikov et al.,2019; freitag et al., 2020; choi et al., 2020).
notmuch attention has been given to generation of di-verse, yet faithful summaries – two goals are oftenchallenging to achieve simultaneously (hashimotoet al., 2019); a model can produce diverse outputsthrough sampling (fan et al., 2018; holtzman et al.,2020), but at the cost of quality..in this paper we introduce a focus atten-tion mechanism (or fame) to transformer-basedseq2seq architectures.
fame is inspired by how hu-mans write summaries.
speciﬁcally, fame aims toperform source-side planning to focus the summaryon supported and topical content.
fame achievesthis through a novel technique which augmentsstandard contextual representations with a dynamicsource-conditioned vocabulary biasing layer.
wepresent the following experimental ﬁndings:.
to thefame promotes summaries faithfulsource when evaluated on the bbc extremesummarization task (xsum; narayan et al., 2018),experiments with two state-of-the-art summarizers– robertas2s (rothe et al., 2020) and pega-sus (zhang et al., 2019) – show that both modelsgenerate summaries that are more faithful to theirinput documents when augmented with fame, incomparison with their vanilla counterparts.1 faith-fulness is measured through a variety of previouslyproposed metrics.
in addition, we leverage themanually annotated document-summary pairs forfaithfulness from maynez et al.
(2020) and traina scorer which serves as an efﬁcient proxy for ex-pensive human evaluations.
we call this metricbertfaithful..fame enables diverse summaries fame, bydesign, supports focus sampling – a techniquethat is more effective in sampling topically rele-vant tokens to generate diverse, yet topically con-sistent and faithful outputs, than other samplingmethods (fan et al., 2018; holtzman et al., 2020).
figure 1 illustrates how focus sampling generatesbetter summaries than other sampling methods.
wedemonstrate the effectiveness of our new focus.
1in the paper we focus on assessing fame on xsum.
butother summarization and text editing results can be found inappendix b and c..sampling technique using a variety of existing di-versity and faithfulness measures.
empirically, weﬁnd that optimizing for high diversity often comesat the cost of faithfulness.
thus fame provides amechanism for trading-off high faithfulness withbetter diversity in summarization..2 related work.
task-speciﬁc architectural priors severalworks enhance seq2seq architectures with task-speciﬁc priors.
pointer-generator style models(see et al., 2017; xu et al., 2020) can accuratelygenerate mostly extractive summaries by copyingwords from the source text via pointing.
textediting models (malmi et al., 2019; dong et al.,2019b; mallinson et al., 2020) cast text generationas a sequence tagging problem with carefullyselected edit operations required for the task.
others focus on improving content selection tobetter constrain the model to likely input phrases(gehrmann et al., 2018) or by improving therepresentation of relevant input tokens (zhou et al.,2017).
instead of directly modeling such priors,fame learns the theme of the document throughdynamic vocabulary biasing.
thus, fame can beseen as a generalization of pointer-generator ortext-editing models via soft vocabulary learning.
in fact, our fame models achieve state-of-the-arton text-editing tasks (appendix c)..topic-aware generation models the idea ofcapturing document-level semantic information hasbeen widely explored in the summarization com-munity.
barzilay and elhadad (1997) use wordnet(fellbaum, 1998) to model a text’s content relativeto a topic based on lexical chains.
lin and hovy(2000) propose to learn topic signatures for summa-rizing documents.
recently, document-level topicinformation has been used for improving neural lan-guage models (mikolov and zweig, 2012; ghoshet al., 2016; dieng et al., 2017; karmaker santuet al., 2019), neural response generators (xing et al.,2017; dziri et al., 2019), and not surprisingly, neu-ral summarizers (narayan et al., 2018; ailem et al.,2019; wang et al., 2020c).
both, narayan et al.
(2018) and ailem et al.
(2019), use a pretrainedlatent dirichlet allocation (lda; blei et al., 2003)model, whereas, wang et al.
(2020c) use poissonfactor analysis (zhou et al., 2012), to synthesizetopic vectors for the input.
instead, we dynamicallylearn a target-induced topic distribution for the in-put under the assumption that the human-written.
6079summary is a good proxy for the input document..faithful generation models cao et al.
(2017)force faithful generation by conditioning on bothsource text and extracted fact descriptions from thesource text.
song et al.
(2020) propose to jointlygenerate a sentence and its syntactic dependencyparse to induce grammaticality and faithfulness.
tian et al.
(2019) learn a conﬁdence score to en-sure that the model attends to the source whenevernecessary.
wang et al.
(2020d) introduce new input-output matching and embedding similarity losses toalleviate hallucination issues.
yet, the task of gen-erating text that is consistent with the input remainsan open problem (gabriel et al., 2020)..diverse generation models there has been asurge of interest in making language models gener-ate more diverse and human-like outputs.
vijayaku-mar et al.
(2018) and kulikov et al.
(2019) diversifybeam search, using a task-speciﬁc scoring function,or constrain beam hypotheses to be sufﬁciently dif-ferent.
others avoid text degeneration by truncatingthe unreliable tail of the probability distribution ateach decoding step, either by sampling from thetop-k tokens (top-k sampling; fan et al., 2018)or by sampling from a dynamic nucleus of tokenswith the bulk of the probability mass (nucleus sam-pling; holtzman et al., 2020).
others modify thetraining objective to make the distribution sparse(martins et al., 2020) or assign lower probability tounlikely generations (welleck et al., 2019a)..for conditional text generation, most work fo-cuses on generating diverse questions (narayanet al., 2016; dong et al., 2017; sultan et al., 2020;wang et al., 2020b) or paraphrases (li et al., 2016b;dai et al., 2017; xu et al., 2018; cao and wan,2020).
following gehrmann et al.
(2018), choet al.
(2019) use a mixture of experts to sampledifferent binary masks on the source sequence fordiverse content selection for summarization.
our focus sampling is similar to top-k and nucleussampling methods; in that it truncates the tail ofthe probability distribution.
however, instead oftruncating it at each decoding step, it biases thedecoder proactively to generate output from a setof tokens which are topically-relevant to the input..3 summarization with focus attention.
figure 2: a transformer-based encoder-decoder archi-tecture with fame..lem using seq2seq architectures with transformerencoder and decoder, augmented with fame, asdepicted in figure 2. fame learns a distribution txifor each input token xi over the vocabulary, mea-suring similarity of xi (in context) to the tokens inthe vocabulary.
the vocabulary distributions, txi,for all xi are combined to form a dynamic vocabu-lary bias that is added to the decoder logits.
thismechanism enhances the conditioning on the in-put source and encourages the decoder to generatetokens that are topically similar to the input..transformer-based seq2seq model the en-coder uses bert transformer layers with multi-headed self-attention to encode x to a vector se-quence x = x1, .
.
.
, xn, with xi ∈ rh, where his the size of hidden representation.
the decoderuses an identical architecture, except that at decod-ing step t, layer l adds a conditional representationt ∈ rh for the token yt by attending to the outputylrepresentation y l−1t−1 generated1so far through self-attention and by attending to theinput contextual representation x through encoder-decoder attention.
the probability of predicting thenext token yt from a vocabulary v is:.
1:t−1 = yl−1.
, .
.
.
, yl−1.
p(yt|y1:t−1, x; θ) = softmax(eyl.
t ),.
(1).
given an input document x1:n, we aim to gener-ate its summary y1:m, where n and m are inputand output sequence lengths.
we address this prob-.
where, ylis the representation from the ﬁnal de-tcoder layer l, e ∈ r|v |×h the embedding matrixand θ the model parameters.
parameters are trained.
6080x1x2x3...xninputtokensembedding matrixencoderx1x2x3...xntx1tx2tx3...txndensedensegeluy1y2y3...yt-1generated outputdecoderaltsoftmaxytfttxfameby minimizing cross-entropy at each decoding step:.
lmle(θ) = −.
log p(ˆyt| ˆy1:t−1, x; θ),.
1m.m(cid:88).
i=1.
where, ˆy1:m is the human-written summary..focus attention mechansim (fame)it is chal-lenging for a decoder to obtain all relevant informa-tion from the conditional representation ylt to learnthe vocabulary output logits such that predictionsyt are consistent with the input.
other modelingfactors, speciﬁcally the decoder language model,can overwhelm model predictions.
fame (fig-ure 2) addresses this by introducing a short-circuitfrom the source to the vocabulary output logits viaa source-conditioned bias on vocabulary items..we take the encoder representation x =x1, .
.
.
, xn and learn a token-level vocabulary dis-tribution txi = gelu(xiw1)w2e ∈ r|v |, foreach token xi in the input sequence x. txi mea-sures the contextual similarity of the input token xito the tokens in the vocabulary; w1 ∈ rh×h(cid:48)andw2 ∈ rh(cid:48)×h are parameters of newly introduceddense layers, h(cid:48) is the intermediate ﬁlter size.
wedeﬁne a source-conditioned vocabulary distribu-tion as tx = 1/n (cid:80)ni=1 txi ∈ r|v | as an averageof token-level vocabulary distributions for tokenspresent in the input sequence x, capturing the sim-ilarity of x to the tokens in the vocabulary..let al.
t ∈ rn be the encoder-decoder attentiondistribution over the source tokens for the output to-ken yt and the ﬁnal decoder layer l. we use alt toproduce a weighted sum of the token-level vocabu-lary distributions to compute a dynamic vocabularybias, or focus bias ft = (cid:80)nt,itxi ∈ r|v | atdecoding step t. we modify the probability of pre-dicting the next token yt from a vocabulary v as:.
i=1 al.
p(yt|y1:t−1, x; θ) = softmax(yl.
t e + ft).
(2).
we call this focused probability distribution, andit modiﬁes the output logits dynamically to putmore focus on those tokens in the vocabulary whichare similar to the attended tokens in x. the focusbias introduces a human-inspired control to themodel where we do not generate the output in afully abstractive manner (as in eq.
(1)), but weproactively generate output tokens that are similarto the input tokens (as in eq.
(2))..representative of the topical content relevant for thetask.
we achieve this by using the human-writtensummary ˆy as a proxy for the topical content ofthe input and impose the following prior on thesource-conditioned vocabulary distribution tx :.
ltopic(θ) = −.
|v |(cid:88).
([vi ∈ ˆy ] log(σ(tx,i)).
1|v |+ [vi /∈ ˆy ] log(1 − σ(tx,i))).(3).
i=1.
we further reﬁne eq.
(3) by replacing ˆy with ˆyc =ˆy −f , where f is a set of |f | most frequent tokensin the vocabulary,2 to improve focus on contentwords.
our ﬁnal loss function is then.
l = λlmle + (1 − λ)ltopic,.
(4).
where, λ is an hyper parameter.3.
by enforcing tx to be a topic distribution for theinput x, we encourage the focus bias ft to promotetopically relevant tokens, and subsequently gener-ate topically consistent outputs.
importantly, ourfocus bias with target-induced topic distributionis task-agnostic and less vulnerable to referencedivergence issues (dhingra et al., 2019; maynezet al., 2020), and can learn any property embodiedin the target relevant for the task.
for example,depending on the task, ft can learn to favour inputtokens (e.g., for mostly extractive summaries) ornew tokens (e.g., for mostly abstractive summaries).
this is in sharp contrast to models that introducetask-speciﬁc priors, e.g., the pointer-generator net-work (see et al., 2017) that can copy words fromthe source text, but does not do well on extremesummarization which is highly abstractive in nature(narayan et al., 2018)..focus sampling: promoting diversity in faith-ful generation we introduce focus samplingwith fame to construct a subset vk ⊆ v bysampling k tokens from the topic distribution tx(focussample,k).
then, we modify eq.
(2) as.
(cid:40).
p(yt|y1:t−1, x; θ) =softmax(yl0,.t e + ft)i.if vi ∈ vk ∪ fotherwise..(5).
for document summarization, the subset vk willcapture topically salient tokens necessary to gener-ate a summary; f is always added to vk to ensure.
summary-induced topic focused distributionwe aim to guide our focus bias ft to be a better.
2which are usually articles or other function words.
3λ is set to 0.5 for all experiments..6081that the model has access to function words.
bytuning the parameters of sampling, we can enforcethe model to control the faithfulness or diversity ofthe outputs..focus sampling has similarities to top-k(divtop,k; fan et al., 2018) and nucleus sampling(divnucleus; holtzman et al., 2020); in that they allaim to promote diversity.
at each decoding step,the top-k sampling diversiﬁes the generation pro-cess by sampling a token from the top k tokensin the ﬁnal output distribution.
similarly, nucleussampling samples from a dynamic nucleus of to-kens containing the vast majority (with a cumula-tive probability p) of the probability distribution.
both top-k and nucleus sampling shorten the tailof the output distribution at each decoding step,whereas focus sampling constrains the decoder touse a ﬁxed and topically relevant vocabulary vk.
unlike the other two techniques, focussample,k canalso beneﬁt from standard beam search decoding,leading to superior generation that is not only di-verse, but also consistent with the input document..4 experimental setup.
in this section we present our experimental setup toassess the ability of our fame models to generatefaithful summaries and to demonstrate that focussampling is more effective in generating diverseand faithful summaries than other sampling-baseddecoding methods..4.1 extreme summarization.
we evaluate fame models on extreme documentsummarization (xsum; narayan et al., 2018).
thexsum summaries, are extreme in that the docu-ments are summarized into single-sentence sum-maries.
these summaries demonstrate a high levelof abstractiveness, and generating them automat-ically requires document-level inference, abstrac-tion, and paraphrasing.
due to their extreme nature,xsum summaries are ideal to evaluate fame mod-els’ ability to capture the theme of the document.4we use on the original cased version consistingof 204,045/11,332/11,334 training/validation/testdocument-summary pairs.
during training, the in-put documents are truncated to 512 tokens.
the.
4we further experiment with long-form story highlightgeneration (cnn/dm; hermann et al., 2015) and two text edit-ing tasks: sentence fusion (geva et al., 2019) and sentencesplitting (botha et al., 2018).
their results can be found inappendix b and c. our fame models achieve sota on bothtext-editing tasks..length of the summaries are limited to 64..4.2 pretrained models with fame.
we introduce fame to two popular seq2seqroberta initialized seq2seqarchitectures:(robertas2s, rothe et al., 2020) and pegasus(zhang et al., 2019).
we refer robertas2s mod-els with fame as robfame and pegasus withfame with pegfame..we experiment with robertas2s-large withshared encoder and decoder; it has 24 layers, ahidden size of 1024, ﬁlter size of 4096, 16 attentionheads, and a vocabulary with 50k sentence pieces(kudo and richardson, 2018).
robertas2s hasaround 455m parameters and robfame has anadditional 8m parameters..the best-performing pegasus model fromzhang et al.
(2019) is not directly comparable withrobertas2s.
it does not share the encoder anddecoder, it only has 16 layers, a hidden size of1024, ﬁlter size of 4096, 16 attention heads, with atotal of 568m parameters, and it also uses a muchlarger vocabulary with 91k sentence pieces.
hence,we trained our own pegasus model.
we use thesame architecture as robertas2s and pretrain iton a mixture of c4 (raffel et al., 2019) and huge-news (zhang et al., 2019) datasets with the originalobjective of generating salient gap-sentences..our experiments focus on this newly trainedpegasus model which has same number of pa-rameters and vocabulary as robertas2s.
butin contrast to robertas2s, the encoder-decoderattention in pegasus is pretrained.
this al-lows us to analyse how focus attention affectspretrained (pegasus) vs randomly-initialized(robertas2s) encoder-decoder attentions.5.
4.3 evaluation metrics.
lexical overlap we report rouge f1 scores(lin and hovy, 2003) against reference summaries;in particular, we report on rouge-1 and rouge-2for informativeness and rouge-l for ﬂuency.6.
semantic similarity we report bertscore(zhang et al., 2020) which computes the contextualsimilarity between a candidate and its referencesummary..5see appendix a for implementation details and hyperpa-.
rameter settings..6we lowercased candidate and reference summaries and.
used pyrouge with parameters “-a -c 95 -m -n 4 -w 1.2.”.
6082models.
lexical overlap (w/ ref).
sem.
sim..r1.
r2.
rl.
bertsc..robertas2srobfame.
pegasuspegfame.
41.4542.15.
44.8545.31.
18.7919.68.
22.2622.75.
33.9034.81.
37.0337.46.
80.680.8.
81.781.9.faithfulness.
others.
ent..feqa.
39.141.3.
43.644.8.
19.821.2.
24.524.8.bertfaithfulconf.
%.
21.522.7.
27.027.3.
0.2160.226.
0.2630.269.len.
rep.(↓).
r1(p%)with doc..21.220.8.
21.120.8.
24.220.7.
6.05.3.
71.172.5.
73.874.3.table 1: abstractive summarization results on xsum test set comparing fame models with their baselines.
forall our models, we use standard beam decoding with a beam size of 4 to generate the single best summary for adocument.
focus sampling is not used here.
see section 4.3 for details on the evaluation metrics reported.
bestnumber for each metric is boldfaced..faithfulness rouge and bertscore do not cor-relate well with faithfulness of the generated sum-maries (maynez et al., 2020).
human evaluationis traditionally considered as the gold standard formeasuring faithfulness.
but recent research hasshown that even human evaluation has shortcom-ings (schoch et al., 2020).
moreover, it is pro-hibitively expensive.
this has led to the proposalof meta-evaluation metrics for various generationtasks (durmus et al., 2020; kry´sci´nski et al., 2019;sellam et al., 2020; rei et al., 2020)..we evaluate fame models on semantic inferencemetrics such as textual entailment (pasunuru andbansal, 2018; welleck et al., 2019b; falke et al.,2019; kryscinski et al., 2019) and question answer-ing (arumae and liu, 2019; wang et al., 2020a).
in particular, we report the probability of a sum-mary entailing (ent.)
its input document (maynezet al., 2020) and qa-based feqa scores (durmuset al., 2020).
for ent.
scores, we train an entail-ment classiﬁer by ﬁne-tuning a bert-large pre-trained model (devlin et al., 2019) on the multi-nli dataset (williams et al., 2018).
for feqa,we use a ﬁne-tuned bart (lewis et al., 2019)language model for question generation to gener-ate questions from the summaries, and a bert-base model ﬁne-tuned on squad (rajpurkar et al.,2018) to answer the generated questions with inputdocument as context.7.
in addition to ent.
and feqa, we train a scorerleveraging manually annotated document-summarypairs for faithfulness, as a surrogate for humanevaluation and call this metric bertfaithful.8in particular, we ﬁnetune a bert-base classi-.
7we used the feqa code available here: https://.
github.com/esdurmus/feqa/..8a very similar scorer was used in the gem benchmark(gehrmann et al., 2021) to identify and extract the subset withfaithful reference summaries from the xsum dataset (narayanet al., 2018)..ﬁer on 500 manually annotated document andgold summary pairs for the xsum dataset frommaynez et al.
(2020) to predict whether a sum-mary is faithful to the input document or not.9we report the percentage of summaries that were1[pi(faithful) > 0.5]) and thefaithful ( 1nmodel’s conﬁdence to generate faithful summaries( 1i pi(faithful)); n is the total number of ex-namples in the test set..(cid:80)i.
(cid:80).
diversity we report the number of times (outof n), a model is able to generate a completelynew summary (unique), and distinct-n (li et al.,2016a), measuring the lexical diversity in the gen-erated summaries.
distinct-n is estimated as thenumber of distinct n-grams of order n divided bythe total number of n-grams of the same order, inall generated summaries..finally, we also report the average length of sum-maries (len.
), repetition errors (rep., estimated asthe percentage of summaries with at least one rep-etition of rare or content words), and rouge-1precision against the input document (r1, p%), tobetter understand their quality..5 results.
fame summaries are more fluent, informa-tive and faithful.
table 1 presents results com-paring our fame models, robfame and peg-fame, against their counterparts robertas2s.
9out of 500, 90% of the document-summary pairs wereused for training and the rest 50 document-summary pairswere used for validation.
we used the validation set to estimatespearman’s correlation coefﬁcients of different metrics withthe human assessment for faithfulness.
we found that bothentailment scores (ent.)
and bertfaithful are moderatelycorrelated with faithfulness with correlation coefﬁcients of0.4387 and 0.3889, respectively.
as such, we believe thatbertfaithful works as an efﬁcient proxy for expensive humanevaluation for faithfulness for xsum summaries.
more workis needed to understand if bertfaithful generalizes to otherdatasets..6083metrics unique.
dist.-n2.rouger2.
ent..bertsc..robertas2s (divtop,k)robertas2s (divnucleus).
robfame (divtop,k)robfame (divnucleus)robfame (focussample,k).
robfame (focussample,k, divtop,k)robfame (focussample,k, divnucleus).
pegasus (divtop,k)pegasus (divnucleus).
pegfame (divtop,k)pegfame (divnucleus)pegfame (focussample,k).
pegfame (focussample,k, divtop,k)pegfame (focussample,k, divnucleus).
9.989.99.
9.999.991.61.
9.999.98.
9.989.99.
9.989.992.77.
8.999.98.
1.
2.54.1.
2.34.13.5.
2.11.9.
1.93.8.
1.93.82.4.
2.82.6.
3.
57.762.2.
58.163.243.9.
51.848.2.
55.363.1.
55.563.134.2.
54.750.9.r1.
33.632.4.
32.731.338.0.
31.832.9.
36.634.1.
36.734.237.5.
31.532.5.
25.030.1.
25.030.722.4.
20.318.4.
23.230.5.
23.230.416.5.
23.020.8.
12.011.4.
11.310.615.7.
10.211.1.
14.312.8.
14.512.815.4.
10.311.0.rl.
26.525.6.
25.724.731.0.
24.725.8.
28.826.9.
29.027.030.3.
24.425.3.
21.819.7.
20.318.034.3.
24.325.9.
27.722.7.
28.523.233.6.
22.824.8.
76.975.7.
76.675.478.6.
75.476.1.
78.476.5.
78.576.677.9.
74.775.3.table 2: assessment of diversity, relevance and faithfulness with focus sampling on the xsum test set..and pegasus, respectively.
both fame mod-els clearly outperform their vanilla counterpartsin terms of generating summaries that are moreﬂuent (see rl and rep.), more informative (seer1, r2 and bertsc.)
and more faithful (see ent.,feqa and bertfaithful).
among all four models,pegfame summaries are most ﬂuent, informativeand faithful..we further did pairwise comparisons for all mea-sures in table 1 and found that all differencesare statistically signiﬁcant except for bertscoreand faithfulness measures between pegasus andpegfame.10 these assessments demonstrate thatfame models aid both robertas2s and pega-sus in generating ﬂuent, faithful and relevant sum-maries, but are more effective in robertas2sthan in pegasus for extreme summarization..generating diverse and faithful summarieswith focus sampling.
table 2 presents re-sults assessing focus sampling (focussample,k),top-k sampling (divtop,k) and nucleus sampling(divnucleus), for their abilities to generate diverseand faithful summaries.
for focussample,k, wechoose k = 10, 000. we follow holtzman et al.
(2020) and choose k = 640 and the nucleus prob-ability p = 0.95, for divtop,k and divnucleus, re-spectively.
for focussample,k, we decode with abeam size of 4. we also report focussample,k withdivtop,k and divnucleus to assess if they can bene-ﬁt one-another.
in each setting we sample 10 sum-.
maries for each input document.
for all metrics,we report the average over all 10 samples.11.
both divtop,k and divnucleus almost always gen-erate a new summary.
in comparison focussample,kgenerates 1.61 and 2.77 unique summaries us-ing robfame and pegfame models, respec-tively.
divnucleus tends to generate the most dis-tinct unigrams, bigrams, and trigrams.
interest-ingly, focussample,k summaries have a more di-verse collection of unigrams than in divtop,k sum-maries (3.5% vs 2.3% for robfame and 2.4% vs1.9% for pegfame)..the high diversity in divtop,k and divnucleuscomes at the cost of faithfulness; summaries gener-ated with these sampling techniques have poor en-tailment scores.
focussample,k, on the other hand,generates summaries which entail documents themost.
it also has the highest rouge scores acrossthe board.
some of the generated examples canbe seen in figure 1. more predictions from othermodels can be found in appendix e. augmentingdivtop,k and divnucleus with focussample,k is notdesirable because, though it increases diversity interms of uniqueness and distinct-3 scores, faithful-ness suffers again..comparing results in table 2 to the results in ta-ble 1, it is clear that diversity comes at the costscores for robfameof quality (e.g., rl/ent.
and robfame-focussample,k are 34.81/41.3 and31.0/34.3, respectively).
however, focussample,kis superior to both divtop,k and divnucleus in gen-.
11feqa and bertfaithful scores are dropped due to time.
10all signiﬁcance tests in this work are pairwise com-parisons (one-way anova with posthoc tukey hsd tests;p < 0.01)..constraints..6084erating better quality summaries..figure 3: top 40 sentence pieces and their logits fromtopic distribution tx in robfame and pegfame forthe xsum article discussed in figure 1..figure 4: rouge-1 f1 scores of robfame andpegfame models with different top-k vocabularies(eq.
(5)) on the xsum test set.
similar patters are ob-served for rouge-2 and rouge-l scores..focus attention and sampling work differ-ently in robfame and pegfame.
since bothencoder-decoder and focus attention parameters ofrobfame are randomly initialized, they learn tocompliment each other and learn a peaky topic dis-tribution.
on the other hand, since pegfame’sencoder-decoder attention is pre-trained, there isa push-pull effect between it and focus attention.
this results in a smoother topic distribution, as seenin figure 3.12.although we see that both models’ token setscapture the target intent well, the peaky distribu-.
12this difference in topic distributions is consistent acrossthe whole test set.
we compute the peakiness score of a topicdistribution as the slope of the line connecting logits of thetop-1st token to the top-100th token.
the average peakinessscores across the xsum testset for robfame and pegfameare 1.25 (51◦) and 0.45 (24.3◦), respectively..models.
r1.
r2.
rl.
leadptgen (see et al., 2017)convs2s (narayan et al., 2018)mmn (kim et al., 2019)mass (song et al., 2019)bart (lewis et al., 2019)pegasus (zhang et al., 2019).
robertas2s (rothe et al., 2020)robfame (w/o eq.
(3))robfameoraclepegasus (ours)pegfame (w/o eq.
(3))pegfameoracle.
16.3029.7031.8932.0039.7545.1447.21.
41.4541.2742.1572.2244.8544.5445.3182.39.
1.619.2111.5412.1017.2422.2724.56.
18.7918.8619.6842.2222.2622.0022.7560.61.
11.9523.2425.7526.0031.9537.2539.25.
33.9033.9034.8153.8937.0336.8337.4669.19.table 3: ablations and sota comparisons on xsumdataset.
the underlined bold results are from the bestperforming models from literature and the bold resultsare the best performing fame models..tion of robfame enables more accurate predic-tions than that of pegfame, in a controlled gen-eration setting.
a comparison is presented in fig-ure 4 where we show how rouge-1 scores varywhen we use only top-k tokens from tx for gener-ation.13 we observe that robfame consistentlyoutperforms pegfame with the lower values ofk ∈ {50, 100, 200, 500, 1000}..further, we observe that robfame gener-ates fewer unique summaries (1.61 vs 2.77) buthas higher distinct-n scores (3.5/22.4/43.9 vs2.4/16.5/34.2) than pegfame, with focussample,kin table 2. this can be again be attributed to howfame works differently in robfame and peg-fame.
when vk is sampled from robfame’speaky distribution, the beam search decoding oftentends to generate similar summaries (leading to alower uniqueness score) as the sampled vks donot diverge by much from each other.
but when itdoes diverge, the decoder tends to generate com-pletely new summaries (leading to higher distinct-n scores)..currently, we set k = 10, 000 for our focus sam-pling experiments following our observations infigure 4. future work will focus on how to bet-ter leverage trade-off between diversity and faith-fulness by controlling the peakiness of the topicdistribution tx ..ablations and sota comparisons we empha-size that fame or focus sampling does not aim toimprove on state-of-the-results in terms of rouge,but to generate more faithful or diverse summaries.
13additional results and model predictions for these experi-.
ments can be found in appendix d..6085while maintaining their quality.
for completeness,we compare our robfame and pegfame modelsto their ablations and other state-of-the-art modelson xsum in table 3..we report rouge scores for fame in the idealscenario (oracle) where it focuses on all thecorrect tokens in the input, i.e., the topic distri-bution tx is identical to the distribution observedin the reference summary.
these models generatesummaries with very high rouge scores when themodel is given the correct tokens to focus on.
thegap between the oracle and fame scores sug-gests that there is still a lot of work to be done inthis space.
focus attention without any topical su-pervision (models w/o eq.
(3)) is not signiﬁcantlybetter than the baselines.
but robfame and peg-fame (trained with joint supervision in eq.
(4))signiﬁcantly outperform robertas2s and pega-sus, respectively..our best model pegfame performs better thanptgen (see et al., 2017), convs2s (narayan et al.,2018), mmn (kim et al., 2019), mass (songet al., 2019) and bart (lewis et al., 2019), butworse when the original pegasus (zhang et al.,2019).
this can be expected as the number ofparameters in pegfame is far less than that in theoriginal pegasus..6 conclusion.
we introduced fame, a new attention mechanismwhich dynamically biases the decoder to proac-tively generate tokens that are topically similar tothe input.
fame enhances the faithfulness of exist-ing state-of-the-art abstract summarization modelswhile improving their overall rouge scores.
fi-nally, our newly introduced focus sampling tech-nique is a better alternative to top-k or nucleussampling to generate diverse set of faithful sum-maries..faithfulness and factuality since models cre-ate new text, there is the danger that they may nei-ther be faithful to the source material nor factual.
this can be exacerbated when the data itself hashighly abstractive targets, which require the modelto generate words not seen in the source materialduring training.
this often leads the model to gen-erate content inconsistent with the source mate-rial (kryscinski et al., 2020; maynez et al., 2020;gabriel et al., 2020)..trustworthy data if the data itself is not trust-worthy (comes from suspect or malicious sources)the model itself will naturally become untrustwor-thy as it will ultimately learn the language andtopics of the training data.
for instance, if the train-ing data is about obama birther conspiracies, andthe model is asked to generate information aboutthe early life of obama, there is a risk that suchfalse claims will be predicted by the model..bias in data similarly, biases in the data aroundgender, race, etc., risk being propagated in themodel predictions, which is common for most nlptasks.
this is especially true when the models aretrained from non-contemporary data that do notrepresent current norms and practices (blodgettet al., 2020)..the above considerations are non-malicious, inthat the model is merely learning to behave as itsunderlying source material.
if users of such modelsare not aware of these issues and do not accountfor them, e.g., with better data selection, evalu-ation, etc., then the generated text can be damaging..generation models can also be misused inmalicious ways.
these include generating fakenews, spam, and other text meant to mislead largeparts of the general population..acknowledgements.
references.
we thank sebastian gehrmann, slav petrov, thereviewers, and the action editor for their invaluablefeedback..ethical considerations.
the nature of text generation leads to multiple eth-ical considerations when applied to applications.
the main failure mode is that the model can learnto mimic target properties in the training data thatare not desirable..melissa ailem, bowen zhang, and fei sha.
2019.topic augmented generator for abstractive summa-rization.
corr, abs/1908.07026..kristjan arumae and fei liu.
2019. guiding extractivesummarization with question-answering rewards.
inproceedings of the 2019 conference of the northamerican chapter of the association for computa-tional linguistics: human language technologies,pages 2566–2577, minneapolis, minnesota..dzmitry bahdanau, kyunghyun cho, and yoshuaneural machine translation by.
bengio.
2015..6086jointly learning to align and translate.
abs/1409.0473..corr,.
technologies, pages 4171–4186, minneapolis, min-nesota.
association for computational linguistics..regina barzilay and michael elhadad.
1997. usinglexical chains for text summarization.
in intelligentscalable text summarization..david m. blei, andrew y. ng, and michael i. jordan.
2003. latent dirichlet allocation.
the journal ofmachine learning research, 3:993–1022..su lin blodgett, solon barocas, hal daum´e iii, andhanna wallach.
2020. language (technology) ispower: a critical survey of “bias” in nlp.
in pro-ceedings of the 58th annual meeting of the asso-ciation for computational linguistics, pages 5454–5476, online.
association for computational lin-guistics..jan a. botha, manaal faruqui, john alex, jasonbaldridge, and dipanjan das.
2018. learning tosplit and rephrase from wikipedia edit history.
inproceedings of the 2018 conference on empiricalmethods in natural language processing, pages732–737, brussels, belgium.
association for com-putational linguistics..yue cao and xiaojun wan.
2020. divgan: towardsdiverse paraphrase generation via diversiﬁed genera-tive adversarial network.
in findings of the associ-ation for computational linguistics: emnlp 2020,pages 2411–2421, online.
association for computa-tional linguistics..ziqiang cao, furu wei, wenjie li, and sujian li.
2017.faithful to the original: fact aware neural abstrac-tive summarization..jaemin cho, minjoon seo, and hannaneh hajishirzi.
2019. mixture content selection for diverse se-quence generation.
in proceedings of the 2019 con-ference on empirical methods in natural languageprocessing and the 9th international joint confer-ence on natural language processing (emnlp-ijcnlp), pages 3121–3131, hong kong, china.
as-sociation for computational linguistics..byung-ju choi,.
jimin hong, david park,.
andsang wan lee.
2020. fˆ2-softmax: diversifyingneural text generation via frequency factorized soft-in proceedings of the 2020 conference onmax.
empirical methods in natural language process-ing (emnlp), pages 9167–9182, online.
associa-tion for computational linguistics..bo dai, dahua lin, raquel urtasun, and sanjafidler.
2017.im-age descriptions via a conditional gan.
corr,abs/1703.06029..towards diverse and natural.
jacob devlin, ming-wei chang, kenton lee, andkristina toutanova.
2019. bert: pre-training ofdeep bidirectional transformers for language under-in proceedings of the 2019 conferencestanding.
of the north american chapter of the associationfor computational linguistics: human language.
bhuwan dhingra, manaal faruqui, ankur parikh,ming-wei chang, dipanjan das, and william co-hen.
2019. handling divergent reference texts whenevaluating table-to-text generation.
in proceedingsof the 57th annual meeting of the association forcomputational linguistics, pages 4884–4895, flo-rence, italy..adji b. dieng, chong wang, jianfeng gao, and johnpaisley.
2017. topicrnn: a recurrent neural net-work with long-range semantic dependency.
inproceedings of the 5th international conference onlearning representations, toulon, france..li dong, jonathan mallinson, siva reddy, and mirellalapata.
2017. learning to paraphrase for questionanswering.
in proceedings of the 2017 conferenceon empirical methods in natural language process-ing, pages 875–886, copenhagen, denmark.
associ-ation for computational linguistics..li dong, nan yang, wenhui wang, furu wei, xi-aodong liu, yu wang, jianfeng gao, ming zhou,and hsiao-wuen hon.
2019a.
uniﬁed languagemodel pre-training for natural language understand-in h. wallach, h. larochelle,ing and generation.
a. beygelzimer, f. alch´e-buc, e. fox, and r. gar-nett, editors, advances in neural information pro-cessing systems 32, pages 13042–13054.
curran as-sociates, inc..yue dong, zichao li, mehdi rezagholizadeh, andjackie chi kit cheung.
2019b.
editnts: an neuralprogrammer-interpreter model for sentence simpliﬁ-in proceedings ofcation through explicit editing.
the 57th annual meeting of the association for com-putational linguistics, pages 3393–3402, florence,italy.
association for computational linguistics..esin durmus, he he, and mona diab.
2020. feqa: aquestion answering evaluation framework for faith-fulness assessment in abstractive summarization.
inproceedings of the 58th annual meeting of the asso-ciation for computational linguistics, pages 5055–5070, online.
association for computational lin-guistics..nouha dziri, ehsan kamalloo, kory mathewson, andosmar zaiane.
2019. augmenting neural responsegeneration with context-aware topical attention.
inproceedings of the first workshop on nlp for con-versational ai, pages 18–31, florence, italy.
associ-ation for computational linguistics..tobias falke, leonardo f. r. ribeiro, prasetya ajieutama,ido dagan, and iryna gurevych.
2019.ranking generated summaries by correctness: an in-teresting but challenging application for natural lan-guage inference.
in proceedings of the 57th annualmeeting of the association for computational lin-guistics, pages 2214–2220, florence, italy..6087angela fan, mike lewis, and yann dauphin.
2018. hi-in proceedingserarchical neural story generation.
of the 56th annual meeting of the association forcomputational linguistics (volume 1: long papers),pages 889–898, melbourne, australia.
associationfor computational linguistics..christiane fellbaum, editor.
1998. wordnet: an elec-.
tronic lexical database.
mit press..markus freitag, david grangier, and isaac caswell.
2020. bleu might be guilty but references are notin proceedings of the 2020 conferenceinnocent.
on empirical methods in natural language process-ing (emnlp), pages 61–71, online.
association forcomputational linguistics..saadia gabriel, asli celikyilmaz, rahul jha, yejinchoi, and jianfeng gao.
2020. go ﬁgure!
a metaevaluation of factuality in summarization..sebastian gehrmann, tosin p. adewumi, karmanyaaggarwal, pawan sasanka ammanamanchi, aremuanuoluwapo, antoine bosselut, khyathi raghavichandu, miruna-adriana clinciu, dipanjan das,kaustubh d. dhole, wanyu du, esin durmus,ondrej dusek, chris emezue, varun gangal,cristina garbacea, tatsunori hashimoto, yufanghou, yacine jernite, harsh jhamtani, yangfengji, shailza jolly, dhruv kumar, faisal ladhak,aman madaan, mounica maddela, khyati mahajan,saad mahamood, bodhisattwa prasad majumder,pedro henrique martins, angelina mcmillan-major,simon mille, emiel van miltenburg, moin nadeem,shashi narayan, vitaly nikolaev, rubungo an-dre niyongabo, salomey osei, ankur p. parikh,laura perez-beltrachini, niranjan ramesh rao,vikas raunak, juan diego rodriguez, sashanksanthanam, jo˜ao sedoc, thibault sellam, samirashaikh, anastasia shimorina, marco antonio so-brevilla cabezudo, hendrik strobelt, nishant sub-ramani, wei xu, diyi yang, akhila yerukola, andjiawei zhou.
2021. the gem benchmark: natu-ral language generation, its evaluation and metrics.
corr, abs/2102.01672..sebastian gehrmann, yuntian deng, and alexanderrush.
2018. bottom-up abstractive summarization.
the 2018 conference on em-in proceedings ofpirical methods in natural language processing,pages 4098–4109, brussels, belgium.
associationfor computational linguistics..mor geva, eric malmi, idan szpektor, and jonathanberant.
2019. discofuse: a large-scale dataset fordiscourse-based sentence fusion.
in proceedings ofthe 2019 conference of the north american chap-ter of the association for computational linguistics:human language technologies, pages 3443–3455,minneapolis, minnesota.
association for computa-tional linguistics..shalini ghosh, oriol vinyals, brian strope, scott roy,tom dean, and larry heck.
2016. contextuallstm (clstm) models for large scale nlp tasks.
corr, abs/1602.06291..jiatao gu, zhengdong lu, hang li, and victor o.k.
incorporating copying mechanism inli.
2016.in proceedings ofsequence-to-sequence learning.
the 54th annual meeting of the association for com-putational linguistics (volume 1: long papers),pages 1631–1640, berlin, germany.
association forcomputational linguistics..tatsunori hashimoto, hugh zhang, and percy liang.
2019. unifying human and statistical evaluation fornatural language generation.
in proceedings of the2019 conference of the north american chapter ofthe association for computational linguistics: hu-man language technologies, volume 1 (long andshort papers), pages 1689–1701, minneapolis, min-nesota.
association for computational linguistics..karl moritz hermann, tomas kocisky, edward grefen-stette, lasse espeholt, will kay, mustafa suleyman,and phil blunsom.
2015. teaching machines to readand comprehend.
in c. cortes, n. d. lawrence,d. d. lee, m. sugiyama, and r. garnett, editors,advances in neural information processing systems28, pages 1693–1701.
curran associates, inc..sepp hochreiter and j¨urgen schmidhuber.
1997.neural computation,.
long short-term memory.
9(8):1735–1780..ari holtzman, jan buys, li du, maxwell forbes, andyejin choi.
2020. the curious case of neural text de-in international conference on learn-generation.
ing representations..shubhra kanti karmaker santu, kalyan veeramacha-neni, and chengxiang zhai.
2019. tilm: neurallanguage models with evolving topical inﬂuence.
inproceedings of the 23rd conference on computa-tional natural language learning (conll), pages778–788, hong kong, china.
association for com-putational linguistics..byeongchang kim, hyunwoo kim, and gunhee kim.
2019. abstractive summarization of reddit postswith multi-level memory networks.
in proceedingsof the 2019 conference of the north american chap-ter of the association for computational linguistics:human language technologies, pages 2519–2531,minneapolis, minnesota.
association for computa-tional linguistics..wojciech kryscinski, bryan mccann, caiming xiong,and richard socher.
2019.evaluating the fac-tual consistency of abstractive text summarization.
corr, abs/1910.12840..wojciech kryscinski, bryan mccann, caiming xiong,and richard socher.
2020. evaluating the factualconsistency of abstractive text summarization.
inproceedings of the 2020 conference on empiricalmethods in natural language processing (emnlp),pages 9332–9346, online.
association for computa-tional linguistics..6088wojciech kry´sci´nski, bryan mccann, caiming xiong,and richard socher.
2019. evaluating the factualconsistency of abstractive text summarization..taku kudo and john richardson.
2018. sentencepiece:a simple and language independent subword tok-enizer and detokenizer for neural text processing.
inproceedings of the 2018 conference on empiricalmethods in natural language processing: systemdemonstrations, pages 66–71, brussels, belgium.
association for computational linguistics..ilia kulikov, alexander miller, kyunghyun cho, andjason weston.
2019. importance of search and eval-uation strategies in neural dialogue modeling.
inproceedings of the 12th international conference onnatural language generation, pages 76–87, tokyo,japan.
association for computational linguistics..mike lewis, yinhan liu, naman goyal, mar-jan ghazvininejad, abdelrahman mohamed, omerlevy, ves stoyanov,and luke zettlemoyer.
2019. bart: denoising sequence-to-sequence pre-training for natural language generation, translation,and comprehension.
corr, abs/1910.13461..jiwei li, michel galley, chris brockett, jianfeng gao,and bill dolan.
2016a.
a diversity-promoting ob-jective function for neural conversation models.
inproceedings of the 2016 conference of the northamerican chapter of the association for computa-tional linguistics: human language technologies,pages 110–119, san diego, california.
associationfor computational linguistics..jiwei li, will monroe, and dan jurafsky.
2016b.
asimple, fast diverse decoding algorithm for neuralgeneration.
corr, abs/1611.08562..chin-yew lin and eduard hovy.
2000. the automatedacquisition of topic signatures for text summariza-tion.
in coling 2000 volume 1: the 18th interna-tional conference on computational linguistics..chin yew lin and eduard hovy.
2003. automatic eval-uation of summaries using n-gram co-occurrencestatistics.
in proceedings of the 2003 human lan-guage technology conference of the north ameri-can chapter of the association for computationallinguistics, pages 150–157..yinhan liu, myle ott, naman goyal, jingfei du, man-dar joshi, danqi chen, omer levy, mike lewis,luke zettlemoyer, and veselin stoyanov.
2019.roberta: a robustly optimized bert pretrainingapproach.
corr, abs/1907.11692..jonathan mallinson, aliaksei severyn, eric malmi, andguillermo garrido.
2020. felix: flexible text edit-ing through tagging and insertion..in natural language processing and the 9th inter-national joint conference on natural language pro-cessing (emnlp-ijcnlp), pages 5054–5065, hongkong, china.
association for computational lin-guistics..inderjeet mani.
2001. automatic summarization, vol-.
ume 3. john benjamins publishing..pedro henrique martins, zita marinho, and andr´e f. t.martins.
2020. sparse text generation.
in proceed-ings of the 2020 conference on empirical methodsin natural language processing (emnlp), pages4252–4273, online.
association for computationallinguistics..joshua maynez, shashi narayan, bernd bohnet, andryan mcdonald.
2020. on faithfulness and factu-ality in abstractive summarization.
in proceedingsof the 58th annual meeting of the association forcomputational linguistics, pages 1906–1919, on-line.
association for computational linguistics..tomas mikolov and geoffrey zweig.
2012. contextdependent recurrent neural network language model.
in proceedings of the spoken language technologyworkshop, pages 234–239.
ieee..shashi narayan, shay b. cohen, and mirella lapata.
2018. don’t give me the details, just the summary!
topic-aware convolutional neural networks for ex-in proceedings of the 2018treme summarization.
conference on empirical methods in natural lan-guage processing, pages 1797–1807, brussels, bel-gium.
association for computational linguistics..shashi narayan, siva reddy, and shay b. cohen.
2016.paraphrase generation from latent-variable pcfgsin proceedings of the 9thfor semantic parsing.
international natural language generation confer-ence, pages 153–162, edinburgh, uk.
associationfor computational linguistics..ani nenkova and kathleen mckeown.
2011. auto-matic summarization.
foundations and trends ininformation retrieval, 5(2–3):103–233..ramakanth pasunuru and mohit bansal.
2018. multi-reward reinforced summarization with saliency andentailment.
in proceedings of the 2018 conferenceof the north american chapter of the associationfor computational linguistics: human languagetechnologies, pages 646–653.
association for com-putational linguistics..weizhen qi, yu yan, yeyun gong, dayiheng liu,nan duan, jiusheng chen, ruofei zhang, and mingzhou.
2020. prophetnet: predicting future n-gramfor sequence-to-sequencepre-training.
in findingsof the association for computational linguistics:emnlp 2020, pages 2401–2410, online.
associa-tion for computational linguistics..eric malmi, sebastian krause, sascha rothe, daniilmirylenka, and aliaksei severyn.
2019. encode,tag, realize: high-precision text editing.
in proceed-ings of the 2019 conference on empirical methods.
alec radford, karthik narasimhan, tim salimans, andilya sutskever.
2018.improving language under-standing by generative pre-training.
technical re-port, openai..6089colin raffel, noam shazeer, adam roberts, katherinelee, sharan narang, michael matena, yanqi zhou,wei li, and peter j. liu.
2019. exploring the limitsof transfer learning with a uniﬁed text-to-text trans-former.
corr, abs/1901.07291..pranav rajpurkar, robin jia, and percy liang.
2018.know what you don’t know: unanswerable ques-tions for squad.
in proceedings of the 56th annualmeeting of the association for computational lin-guistics, pages 784–789, melbourne, australia.
as-sociation for computational linguistics..ricardo rei, craig stewart, ana c farinha, and alonlavie.
2020. comet: a neural framework for mtevaluation.
in proceedings of the 2020 conferenceon empirical methods in natural language process-ing (emnlp), pages 2685–2702, online.
associa-tion for computational linguistics..sascha rothe, shashi narayan, and aliaksei severyn.
2020. leveraging pre-trained checkpoints for se-quence generation tasks.
transactions of the asso-ciation for computational linguistics, 8:264–280..stephanie schoch, diyi yang, and yangfeng ji.
2020.
“this is a problem, don’t you agree?” framing andbias in human evaluation for natural language gener-ation.
in proceedings of the 1st workshop on evalu-ating nlg evaluation, pages 10–16, online (dublin,ireland).
association for computational linguistics..abigail see, peter j. liu, and christopher d. manning.
2017. get to the point: summarization with pointer-generator networks.
in proceedings of the 55th an-nual meeting of the association for computationallinguistics, pages 1073–1083, vancouver, canada.
association for computational linguistics..thibault sellam, dipanjan das, and ankur parikh.
2020. bleurt: learning robust metrics for textgeneration.
in proceedings of the 58th annual meet-ing of the association for computational linguistics,pages 7881–7892, online.
association for computa-tional linguistics..kaiqiang song, logan lebanoff, q. guo, xipeng qiu,x. xue, chen li, dong yu, and fei liu.
2020. jointparsing and generation for abstractive summariza-tion.
in aaai..kaitao song, xu tan, tao qin, jianfeng lu, and tie-yan liu.
2019. mass: masked sequence to se-quence pre-training for language generation.
in pro-ceedings of the 36th international conference onmachine learning, volume 97, pages 5926–5936.
pmlr..md arafat sultan, shubham chandel, ram´on fernan-dez astudillo, and vittorio castelli.
2020. on theimportance of diversity in question generation forin proceedings of the 58th annual meetingqa.
of the association for computational linguistics,pages 5651–5656, online.
association for compu-tational linguistics..ran tian, shashi narayan, thibault sellam, andankur p. parikh.
2019. sticking to the facts: con-ﬁdent decoding for faithful data-to-text generation..ashish vaswani, noam shazeer, niki parmar, jakobuszkoreit, llion jones, aidan n gomez, lukaszkaiser, and illia polosukhin.
2017. attention is allyou need.
in advances in neural information pro-cessing systems 30, pages 5998–6008..ashwin k. vijayakumar, michael cogswell, ram-prasaath r. selvaraju, qing sun, stefan lee, david j.crandall, and dhruv batra.
2018. diverse beamsearch for improved description of complex scenes.
in proceedings of the thirty-second aaai confer-ence on artiﬁcial intelligence, (aaai-18), the 30thinnovative applications of artiﬁcial intelligence(iaai-18), and the 8th aaai symposium on educa-tional advances in artiﬁcial intelligence (eaai-18),new orleans, louisiana, usa, february 2-7, 2018,pages 7371–7379.
aaai press..alex wang, kyunghyun cho, and mike lewis.
2020a.
asking and answering questions to evaluate the fac-in proceedings oftual consistency of summaries.
the 58th annual meeting of the association for com-putational linguistics, pages 5008–5020, online.
association for computational linguistics..zhen wang, siwei rao,.
jie zhang, zhen qin,guangjian tian, and jun wang.
2020b.
diversifyquestion generation with continuous content selec-tors and question type modeling.
in findings of theassociation for computational linguistics: emnlp2020, pages 2134–2143, online.
association forcomputational linguistics..zhengjue wang, zhibin duan, hao zhang, chaojiewang, long tian, bo chen, and mingyuan zhou.
friendly topic assistant for transformer2020c.
based abstractive summarization.
in proceedings ofthe 2020 conference on empirical methods in natu-ral language processing (emnlp), pages 485–497,online.
association for computational linguistics..zhenyi wang, xiaoyang wang, bang an, dong yu,and changyou chen.
2020d.
towards faithful neuraltable-to-text generation with content-matching con-straints..sean welleck, ilia kulikov, stephen roller, emily di-nan, kyunghyun cho, and jason weston.
2019a.
neural text generation with unlikelihood training.
corr, abs/1908.04319..sean welleck, jason weston, arthur szlam, andkyunghyun cho.
2019b.
dialogue natural languageinference.
in proceedings of the 57th annual meet-ing of the association for computational linguistics,pages 3731–3741, florence, italy..adina williams, nikita nangia, and samuel bowman.
2018. a broad-coverage challenge corpus for sen-tence understanding through inference.
in proceed-ings of the 2018 conference of the north ameri-can chapter of the association for computational.
6090linguistics: human language technologies, pages1112–1122.
association for computational linguis-tics..chen xing, wei wu, yu wu, jie liu, yalou huang,ming zhou, and wei-ying ma.
2017. topic awarein proceedings of theneural response generation.
thirty-first aaai conference on artiﬁcial intelli-gence, aaai’17, page 3351–3357.
aaai press..jingjing xu, xuancheng ren,.
junyang lin, andxu sun.
2018. diversity-promoting gan: a cross-entropy based generative adversarial network for di-versiﬁed text generation.
in proceedings of the 2018conference on empirical methods in natural lan-guage processing, pages 3940–3949, brussels, bel-gium.
association for computational linguistics..song xu, haoran li, peng yuan, youzheng wu, xi-aodong he, and bowen zhou.
2020. self-attentionguided copy mechanism for abstractive summariza-in proceedings of the 58th annual meetingtion.
of the association for computational linguistics,pages 1355–1362, online.
association for compu-tational linguistics..wei xu, courtney napoles, ellie pavlick, quanzechen, and chris callison-burch.
2016. optimizingstatistical machine translation for text simpliﬁcation.
transactions of the association for computationallinguistics, 4:401–415..zhilin yang, zihang dai, yiming yang, jaime g. car-bonell, ruslan salakhutdinov, and quoc v. le.
2019.xlnet: generalized autoregressive pretraining forlanguage understanding.
corr, abs/1906.08237..jingqing zhang, yao zhao, mohammad saleh, and pe-ter j. liu.
2019. pegasus: pre-training with ex-tracted gap-sentences for abstractive summarization..tianyi zhang, varsha kishore, felix wu, kilian q.weinberger, and yoav artzi.
2020. bertscore:evaluating text generation with bert.
in proceed-ings of the 8th international conference on learn-ing representations, virtual conference, formerlyaddis ababa ethiopia..mingyuan zhou, lauren hannah, david dunson, andlawrence carin.
2012. beta-negative binomial pro-cess and poisson factor analysis.
in proceedings ofthe fifteenth international conference on artiﬁcialintelligence and statistics, volume 22 of proceed-ings of machine learning research, pages 1462–1471, la palma, canary islands.
pmlr..qingyu zhou, nan yang, furu wei, and ming zhou.
2017. selective encoding for abstractive sentencesummarization.
in proceedings of the 55th annualmeeting of the association for computational lin-guistics (volume 1: long papers), pages 1095–1104,vancouver, canada.
association for computationallinguistics..6091a implementation and reproducibility.
details.
following rothe et al.
(2020), the encoder and de-coder of robertas2s and robfame models areinitialized with public roberta checkpoints.
theencoder and decoder parameters are shared in bothcases.
only the encoder-decoder attention parame-ters are initialized randomly.
for robfame, thefocus attention parameters are also randomly initial-ized.
we experiment with large roberta check-points with 24 layers, a hidden size of 1024, ﬁltersize of 4096, 16 attention heads, and a vocabularywith 50k sentence pieces (kudo and richardson,2018).
robertas2s has around 455m param-eters and robfame has 463m parameters, withan additional 8m parameters.
our pegasus andpegfame implementation also have the same con-ﬁguration, except for the encoder-decoder attentionparameters which are pretrained..we used cloud tpu v3 accelerators for training.
all models are ﬁne-tuned on the target task usingadam with a learning rate of 0.05. we use a linearlearning rate warm up with 40k steps, normalizedby the square root of the hidden size, and a squareroot decay.
we do not perform any tuning on thesehyperparameters.
we use a global batch size of 128document-summary pairs.
we adapt to differentnumber of training steps depending on the trainingdata sizes.
models are trained for 400k and 200ksteps for cnn/dm and xsum respectively, sav-ing check-points every 1000 steps.
we choose thebest model based on rouge-l performance on therespective validation set..the vocabulary for functional tokens f is con-structed by taking the most frequent sentencepieces in the training set.
we tune |f | using the re-spective validation sets; for xsum, we choose f =500 frequent sentence pieces and for cnn/dm,f = 1000. for all our experiments with the famemodels, the beam size is set to 4..we use cloud tpu v3 accelerators for comput-ing entailment scores which takes about 20 minutesfor the two datasets’ test sets.
question generationand answering for feqa are run on a nvidia v100gpu, and it takes between 8-12 hours for one set-ting of each test set..b abstractive summarization results on.
cnn/dailymail.
the cnn/dm dataset (hermann et al., 2015)train-consists.
287,227/13,368/11,490.
of.
models.
cnn/dmr2.
leadptgen (see et al., 2017)bottom-up (gehrmann et al., 2018)sagcopy (xu et al., 2020)mass (song et al., 2019)unilm (dong et al., 2019a)bart (lewis et al., 2019)t5 (raffel et al., 2019)pegasus (c4, zhang et al., 2019)pegasus (hugenews, zhang et al., 2019)prophetnet (qi et al., 2020).
robertas2s (rothe et al., 2020)robfame (ours)pegasus (ours)pegfame (ours).
r1.
39.6039.5341.2242.5342.1243.3344.1643.5243.9044.1744.20.
39.8840.2742.6242.95.
17.7017.2818.6819.9219.5020.2121.2821.5521.2021.4721.17.
18.6618.4320.3820.79.rl.
36.2036.3838.3439.4439.0140.5140.9040.6940.7641.1141.30.
37.2237.5139.6139.90.abstractive summarization results ontable 4:cnn/dm datasets.
the underlined bold results arefrom the best performing models from literature andthe bold results are the best performing fame models..document-summary.
ing/validation/testpairs.
the cnn/dm summaries are in the form ofbullet-point story highlights and exhibit a highdegree of extraction, requiring the models tolearn to copy from the source documents.
thexsum summaries, on the other hand, are extreme,the documents are summarized intoin thatsingle-sentence summaries with a high level ofabstractiveness.
for comparison, the xsum sum-maries show a much larger percentages of novelconstructions than found in cnn/dm summaries(35.8/83.5/95.5/98.5 vs 16.8/54.3/72.4/80.4novel 1/2/3/4-grams).
we use the original casedversion.
during training, the input documentsare truncated to 512 tokens and the length of thesummaries are limited to 128 tokens..table 4 and 5 present complete results forcnn/dm dataset.
we see similar kind of improve-ments as observed in table 1, except for rouge-2for robfame which is 0.23 points worse thanthe robertas2s baseline.
our best model peg-fame performs better than both copy mechanismmodels: lstm-based ptgen (see et al., 2017)and transformer-based sagcopy (xu et al., 2020).
pegfame performs worse when compared with t5(raffel et al., 2019), the original pegasus (zhanget al., 2019) and prophetnet (qi et al., 2020).
thiscan be expected as the number of parameters inpegfame is almost half of t5 or prophetnet, andis 100m less than that in the original pegasus..robfame performs worse than robertas2son both ent.
and feqa measures for cnn/dm, sim-ilar to rouge-2 in table 4. we hypothesize thatthis is due to the extractive nature of the cnn/dmdataset and the fact that it is not able to copy to-.
6092models.
len..rep..r1(p%)% with doc..doc.
→ sum.
ent.
(↑) ¬ cont..feqa.
acc..
avg.(#q).
bertsc..robertas2srobfamepegasuspegfame.
52.155.558.158.5.
77.679.669.471.0.
92.792.595.095.3.
88.887.390.991.0.
96.496.397.597.6.
37.335.240.341.1.
18.119.321.021.1.
76.076.176.876.9.table 5: faithfulness and qualitative assessment of summaries on cnn/dm dataset..kens from the input to the necessary extent as theencoder-decoder attention is not pre-trained.
more-over, feqa scores for robertas2s and robfamemay not be fully comparable due to variation intheir summary lengths and the number of feqaquestions generated; the robfame summaries, onaverage, are 3 words longer and generate 1.2 morequestions than that of robertas2s.
nevertheless,we don’t see this kind of drop in ¬cont.
scores (i.e.,summary not contradicting, either entailed by orneutral to the document) and bertscores..c text editing results.
we also train the fame models on two text editingtasks: (i) for sentence fusion – the problem of com-bining multiple sentences into a single coherentsentence – we used the “balanced wikipedia” por-tion of the discofuse dataset (geva et al., 2019),and (ii) for split-and-rephrase – the reverse task ofsentence fusion – we used the wikisplit dataset(botha et al., 2018), which consists of 1m exam-ples of sentence splits extracted from the wikipediaedit history.
as the name suggests, both text editingtasks require a low degree of abstraction..for both the tasks, we train the models for 300ksteps with a global batch size of 256. the input andoutput are padded to a length of 128, which covers100% of the training, evaluation and test data.
thevocabulary for functional tokens f is constructedby taking the top 100 and 500 sentence pieces fordiscofuse and wikisplit respectively..we report corpus-level bleu14, the exact matchaccuracy, and sari scores (xu et al., 2016)15. theresults can be seen in table 6. the vanilla pega-sus model already beats the current state-of-the-arton both discofuse and wikisplit.
the pegfame.
discofuse.
exact.
sari.
bleu.
(geva et al., 2019)lasertagger (malmi et al., 2019)felix (mallinson et al., 2020)robertas2s (rothe et al., 2020)pegasus (ours)pegfame (ours).
(botha et al., 2018)lasetagger (malmi et al., 2019)robertas2s (rothe et al., 2020)pegasus (ours)pegfame (ours).
51.153.861.366.667.467.8.
14.315.216.416.616.8.
84.585.588.890.390.590.7.
61.561.763.864.164.1.
––––95.895.9.
76.476.377.477.477.3.wikisplit.
exact.
sari.
bleu.
table 6: text editing results on discofuse and wik-isplit.
the underlined scores beat the current state-of-the-art and the bold scores are the new state-of-the-art..model performs better, albeit by a small margin, onall metrics on discofuse.
on wikisplit, it has ahigher exact match accuracy while maintaining thesari score and performs 0.1 bleu worse thanpegasus..d controlled generation with focusattention using top-k tokens.
table 7 presents results from our controlled sum-mary generation experiments with top-k tokensfrom tx using focus attention (focustop,k) on thexsum test set.
in figures 3 and 4, we describe howrobfame consistently outperforms pegfame atlower values of k ∈ {50, 100, 200, 500, 1000} dueto their peaky and smooth tx , respectively.
whilefigure 4 only plots rouge-1 f1 scores, table 7 ad-ditionally reports rouge-2, rouge-l, entailment,feqa, and bertscores.
figure 6 presents predic-tions from models using focustop,k for the articlepresented in figures 1 and 5..e diverse summarization with divtop,k,.
divnucleus and focussample,k.
14we use nltk v3.2.2 with case sensitive scoring to esti-.
mate bleu scores..15sari.
is a lexical similarity metric which com-pares the model’s outputto multiple references andthe input in order to assess the model’s ability to add,delete, and keep an n-gram.
it’s implementation isavailable at: https://github.com/tensorflow/tensor2tensor/blob/master/tensor2tensor/utils/sari_hook.py..figures 7 show the diverse summaries generated us-ing focussample,k for the article shown in figure 5.the predictions from divtop,k and divnucleus areomitted due to the prescribed limit on the num-ber of pages allowed for the appendix.
please ﬁndthem on the arxiv version at https://arxiv.org/abs/2105.11921..6093metrics.
rouger2.
ent..feqa bertscore.
robertas2srobfamerobfame (focustop,k=50)robfame (focustop,k=100)robfame (focustop,k=200)robfame (focustop,k=500)robfame (focustop,k=1000)robfame (focustop,k=10000).
pegasuspegfamepegfame (focustop,k=50)pegfame (focustop,k=100)pegfame (focustop,k=200)pegfame (focustop,k=500)pegfame (focustop,k=1000)pegfame (focustop,k=10000).
r1.
41.4542.1530.9033.6235.9938.2939.5841.58.
44.8545.3124.3027.7731.0534.9937.4042.76.
18.7919.6810.6012.3914.1216.0417.1819.13.
22.2622.757.529.2611.1413.6515.3019.89.rl.
33.9034.8124.8527.1429.2331.3032.4934.30.
37.0337.4619.3222.0924.8228.1930.1634.97.
39.141.327.130.332.435.837.340.7.
43.644.820.824.127.031.033.640.2.
19.821.210.612.413.915.917.320.2.
24.524.88.09.310.813.014.920.1.
80.680.874.274.277.378.679.380.5.
81.781.968.871.373.676.275.980.5.table 7: assessment of controlled summary generation with focus sampling focustop,k on the xsum test set.
we experiment with limiting fame models to different sizes of vocabulary vk using the topic distribution tx ; inparticular, we experiment with k = {50, 100, 200, 500, 1000, 10000}.
we also report numbers for robertas2s,robfame, pegasus and pegfame, using the whole vocabulary of size 50k.
the bold results in each block arethe best performing robertas2s-based and pegasus-based models..gold.
article.
australia has expelled an israeli diplomat saying israel was behind the forging of australianpassports linked to the murder of a hamas operative in dubai..australia’s foreign minister said these were “not the actions of a friend”.
the uk took similar action in march, after concluding that israel was responsible for the use offorged uk passports in the plot.
the israeli foreign ministry said australia’s decision was disappointing.
ministry spokesman yigal palmor said it was “not in line with the importance and the quality ofthe relationship between our countries”.
’sorrow not anger’at least four forged australian passports were used in the killing of mahmoud al-mabhouh indubai in january.
the originals belonged to australians living in israel.
the australian government said a police investigation had left it in no doubt that the israeliauthorities were behind “the abuse and counterfeiting of the passports”.
as a result foreign minister stephen smith asked israel to withdraw a diplomat, whom he didnot identify.
“the decision to ask israel to remove from australia one of its ofﬁcers at the israeli embassy incanberra is not something which ﬁlls the australian government with any joy,” he said.
“on the contrary, the decision was made much more in sorrow than in anger.”passports from france, ireland, germany and britain were used in the operation, and in march,the british government expelled an israeli diplomat from london.
the israeli government has said there is no proof that it was behind the killing, although dubaiofﬁcials have said they are 99.9% sure that agents from mossad were responsible..robertas2s australia has asked australia to withdraw an israeli diplomat from its embassy in canberra after.
robfame.
pegasus.
pegfame.
an alleged plot to kill a abu dhabi militant in dubai.
australia has asked israel to withdraw one of its diplomats from its embassy in canberra after itadmitted it used forged passports.
australia has expelled an israeli diplomat after concluding that forged australian passports usedin the killing of a hamas militant in dubai were issued by israel.
the australian government has expelled an israeli diplomat over the use of forged australianpassports in the killing of a hamas militant in dubai..figure 5: a 2010 bbc article from the xsum testset, its human written summary and model predictions fromrobertas2s, and pegasus, with and without fame.
the text in orange is not supported by the input article..6094robfame (focustop,k=50).
robfame (focustop,k=100).
robfame (focustop,k=200).
robfame (focustop,k=500).
robfame (focustop,k=1000).
australia has said it will not be expelled an ambassador from australia following thealleged s agent for the so-called arab arab state.
australia has said it will not be expelled an ambassador from australia following thekilling of a terror agent in the arab world.
australia has said it will not be expelled an ambassador from australia following thekilling of an australian terror suspect in the arab world.
australia has asked israel to end its diplomatic investigation into an alleged plot tomurder an australian terror suspect.
australia has asked israel to strip an ambassador from its embassy following the deathof an arab man in dubai..robfame (focustop,k=10000) australia has asked israel to withdraw one of its diplomats from its embassy in canberra.
following the death of a terror suspect..pegfame (focustop,k=50).
pegfame (focustop,k=200).
pegfame (focustop,k=100).
the israeli government has been expelled from the country after it was found that thecountry’s security agency, the israeli intelligence agency, was to be to be found to haveused a number of the country’s out-of-country p when it was used in the emirates car-jbest.
the israeli government has been expelled from the country after it was found that thecountry’s security agency, the israeli intelligence agency, had used the country’s visas inthe emirates terror.
the australian government has expelled an israeli diplomats after it found that thecountry’s security agency, the israeli intelligence agency, had used the country’s visas inthe emirates terror attack.
the australian government has expelled an israeli diplomatic staff after accusing thecountry’s security agency, the israeli intelligence agency, of using a number of australianvisas in the emirates terror attack.
australia has expelled an israeli diplomatic staff after accusing the country’s securityagency, the israeli military’s intelligence agency, of being responsible for the use ofaustralian visas used in the killing of a palestinian.
pegfame (focustop,k=10000) australia has expelled an israeli diplomat over the use of forged australian passports in.
pegfame (focustop,k=1000).
pegfame (focustop,k=500).
the killing of a hamas militant in dubai..figure 6: model predictions with focus sampling focustop,k, a controlled generation setting.
the text in orange isnot supported by the input article.
we note that with smaller values of k, both robertas2s-based and pegasus-based models tend to hallucinate more often..robfame (focussample,k)australia has asked israel to strip one of its diplomats from its embassy following the death of an arab man in dubai.
australia has asked israel to end its diplomatic investigation into an alleged plot to murder an australian terror suspect.
australia has asked israel to strip one of its diplomats from its embassy in australia over the death of a terror suspect..pegfame (focussample,k)the australian government has expelled an israeli diplomatic staff after accusing it of using a number of australianvisas in the killing of a palestinian in a car bombing.
the australian government has expelled an israeli diplomatic staff after it said the country was responsible for the use ofaustralian visas used in the killing of a palestinian in a car bombing.
australia has expelled an israeli diplomatic staff after accusing the country’s security agency, the israeli military’sintelligence agency, of being responsible for the use of australian visas used in the killing of a palestinian.
australia has expelled an israeli diplomatic mission after accusing the country’s security agency, the israeli military’sintelligence agency, of being responsible for the use of australian visas used in the killing of a palestinian in the arabcity of emirates.
the australian government has expelled an israeli diplomatic staff after it said the country was responsible for the use ofaustralian visas used in the killing of a palestinian in the middle east..figure 7: fame model predictions with focussample,k (k = 10000).
the text in orange is not supported by theinput article..6095