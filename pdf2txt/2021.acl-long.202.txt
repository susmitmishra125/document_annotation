unimo: towards uniﬁed-modal understanding and generation viacross-modal contrastive learning.
wei li∗, can gao∗, guocheng niu∗, xinyan xiao∗,hao liu, jiachen liu, hua wu, haifeng wangbaidu inc., beijing, china{liwei85,gaocan01,niuguocheng,xiaoxinyan,liuhao24,liujiachen,wu hua,wanghaifeng}@baidu.com.
abstract.
existed pre-training methods either focus onsingle-modal tasks or multi-modal tasks, andcannot effectively adapt to each other.
theycan only utilize single-modal data (i.e., textor image) or limited multi-modal data (i.e.,image-text pairs).
in this work, we pro-pose a uniﬁed-modal pre-training architec-ture, namely unimo, which can effectivelyadapt to both single-modal and multi-modalunderstanding and generation tasks.
largescale of free text corpus and image collec-tions are utilized to improve the capability ofvisual and textual understanding, and cross-modal contrastive learning (cmcl) is lever-aged to align the textual and visual informa-tion into a uniﬁed semantic space, over acorpus of image-text pairs augmented withrelated images and texts.
with the helpof rich non-paired single-modal data, ouris able to learn more generalizablemodelrepresentations, by allowing textual knowl-edge and visual knowledge to enhance eachother in the uniﬁed semantic space.
the ex-perimental results show that unimo greatlyimproves the performance of several single-modal and multi-modal downstream tasks.
our code and pre-trained models are publicat https://github.com/paddlepaddle/research/tree/master/nlp/unimo..1.introduction.
large-scale pre-training has drawn much atten-tion in both the community of compute vision(cv) and natural language processing (nlp) dueto its strong capability of generalization and efﬁ-cient usage of large-scale data.
firstly in cv, aseries of models were designed and pre-trained onthe large-scale dataset imagenet, such as alexnet(krizhevsky et al., 2017), vgg (simonyan and.
∗ these authors contribute equally to this study and are.
listed with random order..figure 1: an illustrative example for the necessity ofuniﬁed-modal learning.
we can only determine the cor-rect answer to the visual question based on the textualbackground information..zisserman, 2014) and resnet (he et al., 2016),which effectively improved the capability of im-age recognition for numerous tasks.
recent yearshave witnessed the burst of pre-training in nlp,such as bert (devlin et al., 2019), roberta (liuet al., 2019), xlnet (yang et al., 2019) and unilm(dong et al., 2019), which greatly improve the capa-bilities of language understanding and generation.
however, the above researches focus on the single-modal learning and can only be effectively used insingle-modal (i.e., only text or image) scenarios.
inorder to adapt to multi-modal scenarios, a series ofmulti-modal pre-training methods were proposedand pre-trained on the corpus of image-text pairs,such as vilbert (lu et al., 2019), visualbert(li et al., 2019b) and uniter (chen et al., 2020b),which greatly improve the ability to process multi-modal information.
however, these models canonly utilize the limited corpus of image-text pairsand cannot be effectively adapted to single-modalscenarios (lin et al., 2020b)..a smarter ai system should be able to pro-cess different modalities of information effectively.
there are large scale of data in different modalitieson the web, mainly textual and visual information.
the textual knowledge and the visual knowledge.
proceedingsofthe59thannualmeetingoftheassociationforcomputationallinguisticsandthe11thinternationaljointconferenceonnaturallanguageprocessing,pages2592–2607august1–6,2021.©2021associationforcomputationallinguistics2592who is standing behind the baseball player?
(a) cocaher(b) umpire(c) spectatorany baseball game involves one or more umpires, who make rulings on the outcome of each play.
at a minimum, one umpire will stand behind the catcher, to have a good view of the strike zone, and call balls and strikes.
additional umpires may be stationed near the other bases …from wikipediasection 4.2).
in this work, unimo learns visualrepresentations and textual representations simulta-neously, and uniﬁes them into the same semanticspace via cross-modal contrastive learning (cmcl)based on a large-scale corpus of image collections,text corpus and image-text pairs..unimo effectively utilizes the large-scale oftext corpus and image collections to learn gen-eral textual and visual representations.
the cmclaligns the visual representations and textual repre-sentations, and uniﬁes them into the same semanticspace based on image-text pairs.
as shown in fig-ure 3, to facilitate different levels of semantic align-ment between vision and language, we proposeto utilize a series of text rewriting techniques toimprove the diversity of cross-modal information.
speciﬁcally, for an image-text pair, various positiveexamples and hard negative examples can be ob-tained by rewriting the original caption at differentlevels.
moreover, to incorporate more backgroundinformation from the single-modal data, text andimage retrieval are also applied to augment eachimage-text pair with various related texts and im-ages.
the positive pairs, negative pairs, relatedimages and texts are learned jointly by cmcl.
inthis way, our model can effectively unify differentlevels of visual and textual representations into thesame semantic space, and incorporate more single-modal knowledge to enhance each other..the uniﬁed-modal architecture mainly has thefollowing advantages compared with previousmethods:.
• we can utilize large scale of non-paired textcorpus and image collections on the web tolearn more generalizable textual and visualrepresentations, and improve the capability ofvision and language understanding and gener-ation..• our model can be effectively ﬁne-tuned forboth single-modal and multi-modal under-standing and generation downstream tasks..• the visual knowledge and textual knowledgecan enhance each other to achieve better per-formance on several single-modal and multi-modal tasks than previous methods..2 unimo.
humans perceive the world through many modal-ities, such as sound, vision and language.
even.
figure 2: illustration of the uniﬁed-modal pre-trainingarchitecture.
both image collections, text corpus andimage-text pairs can be effectively utilized for repre-sentation learning..usually can enhance and complement each other.
as the example shown in figure 1, it’s difﬁcult toanswer the question correctly only with the visualinformation in the image.
however, if we connectthe visual information to the textual informationwhich describes the background of a baseball game,it’s very easy to determine the correct answer.
also,the visual information can make it easier to under-stand the scene described by the text.
the researchin neuroscience by van ackeren et al.
(2018) re-veals that the parts of the human brain responsiblefor vision can learn to process other kinds of in-formation, including touch and sound.
inspired bythis research, we propose to design a uniﬁed-modalarchitecture unimo which aims to process multi-scene and multi-modal data input with one model,including textual, visual and vision-and-languagedata, as shown in figure 2..the greatest challenge to unify different modali-ties is to align and unify them into the same se-mantic space which are generalizable to differ-ent modalities of data.
existed cross-modal pre-training methods try to learn cross-modal represen-tations based on only limited image-text pairs bysimple image-text matching and masked languagemodeling (chen et al., 2020b).
they can only learnspeciﬁc representations for image-text pairs, andthus fail to generalize to single-modal scenarios.
so their performance will drop dramatically whenapplied to language tasks (lin et al., 2020b), whichhas also been revealed by our experiments (see.
2593[img][roi1][roi2][roi3][roi4][roi5][roi6][roin]image collectionstext corpusany baseball game involves one or more umpires… at a minimum, one umpire will stand behind the catcher…semantic spaceimage representationtext representation[cls][tok1][tok2][tok3][tok4][tok5][tokn][sep]uniﬁed-modal transformerimage-text pairsthe baseball player readies to swing at the pitch while the umpire behind him looks on.……[sep][img][cls]theon……………figure 3: illustration of the cmcl.
a series of text rewriting techniques are utilized to create positive image-text pairs x + and hard negative image-text pairs x −.
image and text retrieval are also utilized to obtain relatedimages x i and texts x t from single-modal data, which are treated as single-modal positive samples during cross-modal learning.
all of them are encoded by the same uniﬁed-modal transformer in pairs or individually, and therepresentations of images and texts are extracted to compute the contrastive loss..though any individual modality might be incom-plete or noisy, important information is still perceiv-able since they tend to be shared or enhanced eachother.
with this motivation, we propose a uniﬁed-modal pre-training method unimo to learn repre-sentations that capture modality-invariant informa-tion at the semantic level.
different from previousmethods, unimo learns from different modali-ties of data, including images, texts and image-textpairs, thus achieving more robust and generalizablerepresentations for both textual and visual input..as shown in figure 2, unimo employsmulti-layer self-attention transformers to learnuniﬁed semantic representations for both tex-tual and visual data.
for a textual input w,it is ﬁrstly split into a sequence of subwordsw = {[cls], w1, ..., wn, [sep ]} by byte-pairencoding (bpe) (sennrich et al., 2016), andthen the self-attention mechanism islever-aged to learn contextual token representations{h[cls], hw1, ..., hwn, h[sep ]}.
the specialtokens [cls] and [sep ] denote the start and endof the textual sequence, respectively.
similarly, for.
similar.
an image v, it is ﬁrstly converted to a sequenceof region features v = {[im g], v1, ..., vt}([im g] denotes the representation of the entireimage), and then the self-attention mechanismis leveraged to learn contextual region repre-sentations {h[im g], hv1, ..., hvt}.
toprevious work (chen et al., 2020b), we use fasterr-cnn (ren et al., 2016) to detect the salientimage regions and extract the visual features(pooled roi features) for each region.
for animage-text pair (v, w ), its visual features andtextual tokens are concatenated as a sequence{[im g], v1, ..., vt, [cls], w1, ..., wn, [sep ]}.
then the sequence is feed into the multi-layertransformer network to learn cross-modal con-textual representations for both the textual tokensand image regions.
we extract the representationsh[im g] and h[cls] as the semantic representationsof image v and text w , respectively..based on large volumes of image collectionstext corpus {w } and image-text pairs{v },{(v, w )}, unimo learns generalizable visual andtextual representations in similar ways by masked.
2594…uniﬁed-modal transformer[img][cls]uniﬁed-modal transformer[img][cls]uniﬁed-modal transformercross-modal contrastive learning…when the referee looked up, the player was about to swing a baseball bat on the pitch.when the referee looked up, the baseball player was about to swing a bat on the court.positive pairsany baseball game involves one or more umpires, who make rulings on the outcome of each play.at a minimum, one umpire will stand behind the catcher, to have a good view of the strike zone, and call balls and strikes.…positive textspositive imagestext rewritingthe baseball player readies to swing at the pitch while the umpire behind him looks on.text/image retrievalthe man rides a horse in the court while the coach watches him.
the baseball player readies to swing in the room while the umpire behind him looks on.…negative pairsprediction, and unify them into the same semanticspace via cmcl.
joint visual learning on imagecollections, language learning on text corpus andcross-modal learning on image-text pairs not onlyimprove the capability of visual and language un-derstanding and generation, but also enable the tex-tual knowledge and visual knowledge to enhanceeach other in the uniﬁed semantic space..2.1 cross-modal contrastive learning.
the greatest challenge to unify different modali-ties is to align and unify their representations atdifferent levels.
for the example shown in figure2, the model not only needs to connect the sceneshown in the whole image to an article describinga baseball game, but also needs to align the twomen and their location relationship in the imagewith “baseball player”, “umpire” and “behind” inthe text, respectively.
several existing cross-modalpre-training methods try to align visual and textualrepresentations by simply image-text matching (liet al., 2019a; chen et al., 2020b) based on a limitedcorpus of image-text pairs.
they randomly samplea negative image or text from the same trainingbatch for each image-text pair, and utilize a clas-siﬁer to determine whether the image and text arematching.
as the randomly sampled negative textor image is usually very different from the originaltext or image, they can only learn very coarse align-ment between textual and visual representations.
in this work, we propose a novel cmcl method toalign and unify different levels of textual and visualrepresentations into the same semantic space..the main idea is to let the representations of thepaired image and text near in the representationspace while the non-paired far away.
the represen-tations of image v and text w are used to computethe similarity between them to measure their dis-tance d(v, w ).
as shown in figure 3, to facilitatesemantic alignment between vision and language atdifferent levels, we design several novel text rewrit-ing techniques to rewrite the original caption of animage either at word, phrase or sentence level.
inthis way, we can create large volumes of positiveexamples x + and negative examples x − for eachimage-text pair (v, w ).
moreover, to augmentcross-modal learning with single-modal informa-tion, text and image retrieval are applied to obtainvarious related texts x t and images x i for eachimage-text pair (v, w ).
different from the positiveand negative image-text pairs, the retrieved images.
and texts are encoded individually as they mainlycarry weak correlations, as shown in the right partof figure 3. based on these positive and negativeexamples, the following contrastive loss lcm clis utilized to learn detailed semantic alignmentsacross vision and language:.
(cid:34).
(cid:80).
ev,w.
−log.
(v +,w +)∈x {+,i,t } exp(d(v +, w +)/τ )(v (cid:48),w (cid:48))∈x {−,+,i,t } exp(d(v (cid:48), w (cid:48))/τ ).
(cid:80).
(cid:35).
(1)where τ denotes the temperature parameter.
notethat, for single-modal images x i and texts x t , theoriginal text w and image v are used to computethe cross-modal relevance, respectively.
to the bestof our knowledge, this is the ﬁrst work that explorescmcl to unify visual and textual semantic space..text rewriting to enhance multi-granularity ofsemantic alignment between image and text, werewrite the caption of an image at different levels,including sentence-level, phrase-level and word-level.
for sentence-level rewriting, we utilize theback-translation techniques (edunov et al., 2018)to obtain several positive samples for each image-text pair.
speciﬁcally, each caption of an imageis translated into another language and then trans-lated back to the original language.
in this way,several similar captions can be obtained for an im-age.
furthermore, for each image-text pair, themost similar captions of other images are retrievedbased on tf-idf similarity.
the retrieved resultsare very similar to the original caption but doesn’taccurately describe the corresponding image, sothey can be used as hard negative samples to en-hance the sentence-level alignment between imageand text.
for phrase-level and word-level rewriting,we ﬁrst parse the image caption into a scene graph(wang et al., 2018), then randomly replacing theobject, attribute or relation nodes of the scene graphwith a different object, attribute or relation from thecorresponding vocabularies.
instead of randomlysampling negative samples as previous methods,text rewriting can generate large volumes of hardnegative samples.
in this way, we can help themodel to learn more detailed semantic alignmentfrom different levels between image and text..image/text retrievalin order to incorporatemore single-modal information during cross-modallearning, each image-text pair is further augmentedwith various related images and texts that retrievedfrom the single-modal data.
speciﬁcally, for animage, other images in the image collections will.
2595be ordered by their visual similarities.
those im-ages that have highly overlapped objects with theoriginal image will be extracted to provide relevantvisual information.
similarly, sentences that aresemantically related with the original caption areextracted based on semantic similarity to providebackground language information.
the retrievedimages and texts are encoded individually by theuniﬁed-modal transformer as shown in figure 3,then their representations are extracted to com-pute the cross-modal contrastive loss in equation 1.these retrieved single-modal information providerich background information for better cross-modallearning..2.2 visual learning.
similar to the masked language modeling in bert,we sample image regions and mask their visualfeatures with a probability of 15%.
the visual fea-tures of the masked regions are replaced by zeros.
as the regions from an image usually are highlyoverlapped with each other, we choose to maskall regions that have a high proportion of mutualintersection to avoid information leakage.
similarto lin et al.
(2020b), we randomly choose regionsas masking anchors and mask the regions whoseoverlapping ratios with the anchors are larger than0.3. for an image v , the model is trained to recon-struct the masked regions vm given the remainingregions v\m:.
lv = ev ∈dfθ(vm|v\m).
(2).
similarly, for an image-text pair (v, w ), the modelis trained to reconstruct the masked regions vmgiven the text w and the remaining regions v\m:.
lv = ev,w ∈dfθ(vm|v\m, w ).
(3).
as the visual features are high-dimensional andcontinuous, we utilize both feature regression andregion classiﬁcation objective to learn better visualrepresentations.
the feature regression learns toregress the contextualized visual representationshvi to its visual features vi, which can be formu-lated as: fθ(vm|v\m) = (cid:80)mi=1 (cid:107)r(hvi) − vi(cid:107)2,where r indicates an fc layer to convert hviinto a vector of the same dimension as vi.
theregion classiﬁcation learns to recognize the ob-ject semantic class of each masked region basedon its contextualized visual representation hvi.
an fc layer is utilized to compute the scoresfor k object classes s(hvi), which further goes.
through a sof tmax function to obtain the nor-malized distribution.
the ﬁnal objective mini-mizes the cross-entropy (ce) loss between the pre-dicted distribution and the object detection out-put c(vi) from faster r-cnn: fθ(vm|v\m) =(cid:80)mi=1 ce(sof tmax(s(hvi)), c(vi)).
the scorefunction fθ(vm|v\m, w ) is formulated similarly..2.3 language learning.
to learn general language representations for bothlanguage understanding and generation tasks, ourmodel is trained as a uniﬁed encoder-decodermodel with two types of language modeling tasks:bidirectional prediction and sequence-to-sequence(seq2seq) generation.
the uniﬁed modeling isachieved by utilizing speciﬁc self-attention masksto control what context the prediction conditionson, inspired by dong et al.
(2019).
to improve thelanguage learning process, we ﬁrstly detect seman-ticly complete phrases from the text, such as nameentities by syntactic parsing, and then treat them asa whole in the following masking strategies.
dif-ferent from previous work, we always sample asequence of complete words or phrases instead ofsubword tokens, for both bidirectional predictionand seq2seq generation..bidirectional prediction.
given a sequence oftokens w = {[cls], w1, ..., wn, [sep ]}, we it-eratively sampling spans of text until totally 15%tokens have been selected.
we sample the spanlength from a geometric distribution l ∼ geo(p),where p is set as 0.2, similar to spanbert (joshiet al., 2020).
all tokens in the selected spans arereplaced with either a special [m ask] token, arandom token or the original token with probability80%, 10% and 10%, respectively.
the goal is topredict these masked tokens wm based on their sur-rounding context w\m, by minimizing the negativelog-likelihood:.
lbidirectional = −ew ∈dlogpθ(wm|w\m).
(4).
seq2seq generation.
for the seq2seq genera-tion task, we iteratively sample fragments fromthe token sequence until the 25% budget has beenspent, inspired by xiao et al.
(2020).
for eachiterate, we ﬁrst sample a fragment length from auniform distribution l ∼ u (4, 32), and then sam-ple a fragment with the speciﬁed length.
everyselected fragment {wi, ..., wj} is further appendedwith two special tokens [cls] and [sep ] (i.e.,{[cls], wi, ..., wj, [sep ]}), which denotes the.
2596beginning and end of the fragment.
all selectedfragments are removed from the text and concate-nated as the target sequence t while the remainingparts are concatenated as the source sequence s.the model is trained to generate the target sequenceauto-regressively condition on the source sequence:.
5e-5 and a learning rate linear decay schedule is uti-lized.
by virtue of ﬂoat16 mixed precision training,it takes almost 7 days for training unimo-basewith 32 nvidia telsa v100 32gb gpu and 10days for unimo-large with 64 nvidia telsa v10032gb gpu..lseq2seq = −e(s,t )∈dlogpθ(t |s).
(5).
where pθ(t |s) = (cid:81)|t |j=1 pθ(tj|t<j, s).
duringpre-training, we alternate between the bidirectionalprediction objective and the seq2seq generationobjective uniformly.
for image-text pairs, the twoobjectives are applied to the captions similarly tolearn cross-modal understanding and generation..3 experimental settings.
in this section, we introduce the pre-training andﬁnetuning experimental settings..3.1 pre-training dataset.
our pre-training datasets consist of three types:text corpus, image collections and image-text pairs.
the text corpus includes two large-scale corpora:bookwiki and openwebtext, which are part ofthe training dataset of roberta.
bookwiki iscomposed of english wikipedia and bookcorpus(zhu et al., 2015), and openwebtext is an openrecreation of the webtext corpora.
the imagecollections are images without textual descriptions,including a subset of openimages (krasin et al.,2017) and coco unlabel.
the image-text pairs arecomposed of four existing multi-modal datasets:coco (lin et al., 2014), visual genome (vg)(krishna et al., 2017), conceptual captions (cc)(sharma et al., 2018) and sbu captions (ordonezet al., 2011), which have also been widely usedin previous multi-modal pre-training models.
thestatistics of them are shown in appendix a..3.2.implementation detail.
we evaluate unimo on two model sizes: unimo-base with 12 layers of transformer block andunimo-large with 24 layers of transformer block.
the maximum sequence length of text tokens andimage-region features are set as 512 and 100, re-spectively.
we pre-train unimo-base by initial-izing from roberta-base, and unimo-large byinitializing from roberta-large.
both unimo-base and unimo-large are trained for at least 500ksteps.
an adam optimizer with initial learning rate.
for visual learning, we adopt faster r-cnn(ren et al., 2016) pre-trained on the visual-genome dataset to select salient image regions andextract region features from images.
the regionswith class detection probability exceeds a conﬁ-dence threshold of 0.2 are selected and 100 boxesare kept.
for cmcl, we utilize back-translationto create 3 positive samples and apply rewriting toobtain 100 hard negative samples for each image-text pair.
the most similar of 100 images and 100sentences are retrieved from the single-modal im-age collections and text corpus for each image-textpair, respectively.
more details are described inappendix a..3.3 finetuning tasks.
we ﬁne-tune our model on two categories ofdownstream tasks:(1) single-modal languageunderstanding and generation tasks; (2) multi-modal vision-language understanding and genera-tion tasks.
the single-modal generation tasks in-clude: generative conversational question answer-ing on the coqa dataset (reddy et al., 2019),question generation on the squad 1.1 dataset (ra-jpurkar et al., 2016), abstractive summarization onthe cnn/dailymail (cnndm) dataset (hermannet al., 2015), and sentence compression on the giga-word dataset (rush et al., 2015).
the single-modalunderstanding tasks include: sentiment classiﬁca-tion on the sst-2 dataset (socher et al., 2013),natural language inference on the mnli dataset(williams et al., 2017), linguistic acceptability anal-ysis on the cola dataset (warstadt et al., 2019) andsemantic similarity analysis on the sts-b dataset(cer et al., 2017).
the multi-modal tasks include:visual question answering (vqa) on the vqav2.0 dataset (goyal et al., 2017), image captionon the microsoft coco captions dataset (chenet al., 2015), visual entailment on the snli-vedataset (xie et al., 2019) and image-text retrievalon flickr30k datasets (young et al., 2014).
thedetail statistics of the datasets and hyper-parametersettings for the above tasks are described in ap-pendix b..2597model.
vilbert-basevlp-baseuniter-baseoscar-basevilla-baseernie-vil-baseunimo-baseuniter-largeoscar-largevilla-largeernie-vil-largeunimo-large.
flickr30k-ir.
flickr30k-tr.
r@1 / r@5 / r@10 r@1 / r@5 / r@1058.20 / 84.90 / 91.52-72.52 / 92.36 / 96.08-74.74 / 92.86 / 95.8274.44 / 92.72 / 95.9474.66 / 93.40 / 96.0875.56 / 94.08 / 96.76-76.26 / 94.24 / 96.8476.70 / 93.58 / 96.4478.04 / 94.24 / 97.12.
--85.90 / 97.10 / 98.80-86.60 / 97.90 / 99.2086.70 / 97.80 / 99.0089.70 / 98.40 / 99.1087.30 / 98.00 / 99.20-87.90 / 97.50 / 98.8088.10 / 98.00 / 99.2089.40 / 98.90 / 99.80.snli-veval / test--78.59 / 78.28-79.47 / 79.03-80.00 / 79.1079.39 / 79.38-80.18 / 80.02-81.11 / 80.63.vqa.
coco captiontest-dev / -std blue4 / cider70.55 / 70.9270.5 / 70.772.70 / 72.9173.16 / 73.4473.59 / 73.6772.62 / 72.8573.79 / 74.0273.82 / 74.0273.61 / 73.8274.69 / 74.8774.75 / 74.9375.06 / 75.27.
-36.5 / 116.9-36.5 / 123.7--38.8 / 124.4-37.4 / 127.8--39.6 / 127.7.table 1: evaluation results on the multi-modal downstream tasks..model.
sst-2mnliacc acc-(m/mm) mat84.4 / -92.7bert-base-94.8roberta-base86.8/86.7unimo-base95.159.9/64.9w/o single-modal 82.086.6/-93.2bert-large90.2/90.296.4roberta-large89.8/-95.6xlnet-large87.0/85.994.5unilm-large96.8unimo-large89.8/89.5.
cola sts-b coqaper--91.088.890.092.491.887.792.6.acc-77.480.267.1-85.1-82.584.9.
-63.665.415.060.668.063.661.168.5.gigawordr-1/2/l-.
squad-qgb4/me/r-l-.
cnndmr-1/2/l-22.15/24.58/51.12 42.31/20.04/39.49 38.65/19.66/36.0422.78/25.24/51.34 42.42/20.12/39.61 38.80/19.99/36.2717.09/21.04/46.47 41.06/19.01/38.23 38.06/18.91/35.41-23.39/25.73/52.11 43.10/20.29/40.24 39.32/20.01/36.58-22.12/25.06/51.07 43.33/20.21/40.51 38.45/19.45/35.7524.59/26.39/52.47 43.51/20.65/40.63 39.71/20.37/36.88.
-.
-.
-.
-.
table 2: comparison on the single-modal downstream tasks.
r-1, r-2 and r-l denote rouge-1, rouge-2and rouge-l, respectively.
mat, per, b4 and me denote matthews correlation coefﬁcient, pearson correlationcoefﬁcient, blue4 and meteor (lavie and agarwal, 2007), respectively.
“w/o single-modal” denotes removingthe single-modal learning process on the single-modal data from unimo, which is similar to uniter-base (chenet al., 2020b).
the results on sst-2, mnli, cola, sts-b and coqa are evaluated on the dev set.
the resultsof roberta on the generation tasks coqa, squad-qg, cnndm and gigaword are evaluated by utilizing theunimo architecture initialized with pre-trained parameters of roberta..4 results and analysis.
in this section, we report the evaluation resultson both the multi-modal and single-modal tasksto show the adaptability and generalizability ofunimo to different scenarios.
we further makeseveral ablation studies to validate that textualknowledge and visual knowledge can enhance eachother in the uniﬁed semantic space.
the visual-ization and case analysis of the model results areappended in appendix c..4.1 multi-modal tasks.
the evaluation results on the multi-modal tasksare shown in table 1. we compare with most ofthe existed multi-modal pre-training models, in-cluding vilbert (lu et al., 2019), vlp (zhouet al., 2020), uniter (chen et al., 2020b), os-car (li et al., 2020), villa (gan et al., 2020) andernie-vil (yu et al., 2020).
the results show thatunimo achieves the best results against almost allbenchmarks under both the base and large size of.
models.
particularly, unimo-large outperformsprevious best performing model ernie-vil-largeby 1.34 r@1 on image retrieval and 1.3 r@1 ontext retrieval, which are great improvements forthe image-text retrieval tasks.
on the image cap-tion task, unimo outperforms the best perform-ing model oscar by more than 2 blue4 score.
unimo achieves better performance on both themulti-modal understanding and generation tasks,while previous methods usually focus on eitherthe understanding or generation tasks.
the aboveresults demonstrate the effectiveness of the uniﬁed-modal learning architecture that takes advantage ofthe large scale of single-modal images and texts forcross-modal learning..4.2 single-modal tasks.
previous multi-modal pre-training models usuallycannot effectively adapt to single-modal scenar-ios.to further validate that, we remove the single-modal learning processes on the text corpus and.
2598model.
unimo-basew/o texts.
flickr30k-ir.
flickr30k-tr.
r@1 / r@5 / r@10 r@1 / r@5 / r@1089.70 / 98.40 / 99.1074.66 / 93.40 / 96.0885.80 / 97.90 / 99.1072.04 / 91.62 / 95.30.snli-veval80.0079.52.vqacoco captiontest-dev blue4 / cider73.7973.77.
38.8 / 124.438.3 / 123.2.table 3: analyzing the effectiveness of textual knowledge to multi-modal tasks..model.
sst-2mnliacc acc-(m/mm) mat65.4unimo-base95.186.8/86.787.4/86.862.8w/o pairs&images 94.7.cola sts-b coqaper91.090.6.acc80.278.1.squad-qgb4/me/r-l.cnndmr-1/2/l22.78/25.24/51.34 42.42/20.12/39.61 38.80/19.99/36.2721.26/24.02/50.04 42.26/20.09/39.41 38.22/19.43/35.71.
gigawordr-1/2/l.
table 4: analyzing the effectiveness of visual knowledge to language tasks..image collections (i.e., “w/o single-modal”) fromunimo and replace the cmcl with an image-textmatching objective.
then, the model “w/o single-modal” is just a multi-modal pre-training methodsimilar to uniter (chen et al., 2020b).
as shownin table 2, the performance of the model on allthe language understanding and generation tasksdrop dramatically compared to unimo, whichdemonstrates that multi-modal pre-training onlyon image-text pairs cannot effectively adapt to thesingle-modal tasks..to show the effectiveness of unimo on thelanguage understanding and generation tasks, wefurther compare with existed pre-trained languagemodels (plms), including bert (devlin et al.,2019), roberta (liu et al., 2019), xlnet (yanget al., 2019) and unilm (dong et al., 2019).
thecomparison results in table 2 demonstrate thatunimo achieves better or comparable perfor-mance than existed plms on both the languageunderstanding and generation tasks.
speciﬁcally,unilm (dong et al., 2019) is designed for bothnatural language understanding and generation.
unimo outperforms unilm on most of the taskswith a large margin, which demonstrates the effec-tiveness of unimo on the single-modal scenarios.
in all, unimo not only achieves the best perfor-mance on the multi-modal tasks, but also performsvery well on the single-modal tasks, which demon-strate the superiority of our uniﬁed-modal learningarchitecture..the cross-modal learning, we remove the languagelearning process on the text corpus from unimo(i.e., “w/o texts”), and compare their performanceon the multi-modal tasks.
table 3 summarizesthe comparison results, which show that the per-formance of the model “w/o texts” declines con-sistently on both the multi-modal understandingand generation tasks.
the results demonstrate thatthe textual knowledge in the text corpus beneﬁtthe vision-language tasks by enhancing the cross-modal learning with more textual information..vision enhance text to further validate thatthe visual knowledge in the image collections andimage-text pairs facilitates the language learning,we remove the images and image-text pairs fromthe pre-training dataset (i.e., “w/o pairs&images”)and compare their performance on the single-modallanguage tasks.
after removing the images andimage-text pairs, our model is trained by only thelanguage learning objectives, which are similar toprevious pre-trained language models bert andunilm.
table 4 summarizes the comparison re-sults, which demonstrate that after removing thevisual data, the performance of the model “w/opairs&images” drops obviously on most of the lan-guage understanding tasks and all the language gen-eration tasks.
the results reveal that visual knowl-edge can enhance the language tasks by enablingthe model to learn more robust and generalizablerepresentations in a uniﬁed semantic space..4.3 mutual enhancement of text and vision.
5 related work.
we further make several ablation studies to showthat the uniﬁed-modal architecture can help textualknowledge and visual knowledge mutually enhanceeach other in the uniﬁed semantic space..text enhance vision to explore whether thetextual knowledge in the text corpus facilitates.
existing researches on pre-training can be mainlyclassiﬁed into two categories: single-modal pre-training and multi-modal pre-training.
the single-modal pre-training methods only focus on single-modal tasks, while the multi-modal pre-trainingmethods only focus on multi-modal tasks..2599single-modal pre-training the single-modalpre-training methods mainly consist of visual pre-training and language pre-training.
most visualpre-training methods are based on the multi-layercnn architecture such as vgg (simonyan andzisserman, 2014) and resnet (he et al., 2016),and trained on the imagenet dataset.
recently,contrastive self-supervised learning like simclr(chen et al., 2020a) and moco (he et al., 2020)also greatly improve the performance of visual rep-resentation learning.
these pre-trained models onlyfocus on visual tasks (e.g.
image classiﬁcation etc.
),however, they cannot be used in textual or multi-modal (i.e., with both text and image) tasks.
thelanguage pre-training methods based on the trans-former architecture are also very popular in nlpmodels, such as gpt (radford et al., 2018), bert(devlin et al., 2019), xlnet (yang et al., 2019) andbart (lewis et al., 2020).
however, they mainlyfocus on textual tasks.
they cannot effectivelydeal with the multi-modal tasks, such as image-textretrieval, image captioning, multimodal machinetranslation (lin et al., 2020a; su et al., 2021) andvisual dialog (murahari et al., 2020)..multi-modal pre-training recently, multi-modal pre-training methods have been moreand more popular for solving the multi-modaltasks.
all of them are trained on a corpus ofimage-text pairs, such as vilbert (lu et al.,2019), visualbert (li et al., 2019b), vl-bert(su et al., 2019), unicoder-vl (li et al., 2019a)and uniter (chen et al., 2020b).
based on themulti-layer transformer network, they all employthe bert-like objectives to learn multi-modalrepresentations from a concatenated-sequence ofvision features and language embeddings.
theirarchitectures can be mainly classiﬁed into twocategories: single-stream and two-stream.
thetwo-stream methods, such as vilbert, utilizetwo single-modal transformer to process visualfeatures and language embeddings respectively,and then learn their interactions based on a cross-modal transformer.
the single-stream methodsdirectly utilize a single transformer network tomodel both the visual features and the languageembeddings.
visualbert, vl-bert, unicoder-vl and uniter all utilize the single-streamarchitecture, which show that fusing cross-modalinformation early and freely by a single-streamnetwork can achieve better performance..recently, several contrastive learning-based.
multi-modal pre-training methods have also beenproposed.
openai clip (radford et al., 2021)leverages large-scale image-text pairs to learn trans-ferrable visual representations by image-text match-ing, which enables zero-shot transfer of the modelto various visual classiﬁcation tasks.
wenlan (huoet al., 2021) further proposes a similar two-towerchinese multi-modal pre-training model and adaptsmoco (he et al., 2020) to improve the contrastivecross-modal learning process.
instead of extractingsalient image regions by pre-trained object detec-tion models like faster-rcnn (ren et al., 2016),the end-to-end vision-language pre-training archi-tecture soho (huang et al., 2021) proposes tojointly learn convolutional neural network (cnn)and transformer for cross-modal alignments frommillions of image-text pairs..all existed multi-modal pre-training methodsonly focus on multi-modal tasks with both visionand language inputs.
however, they cannot beeffectively adapted to single-modal tasks.
more-over, they can only utilize the limited corpus ofimage-text pairs.
by contrast, our uniﬁed-modalpre-training method unimo can employ large vol-umes of text corpus and image collections to en-hance each other, and can be effectively adapted toboth textual and multi-modal scenarios.
unimoalso achieves the best performance on multi-modaltasks including image-text retrieval, visual entail-ment, vqa and image caption..6 conclusion.
in this work, we propose unimo, a uniﬁed-modalpre-training architecture to leverage the large scaleof non-paired text corpus and image collections forcross-modal learning.
we verify that unimo pro-vides an effective way for textual knowledge andvisual knowledge to mutually enhance each otherin a uniﬁed semantic space, and unimo success-fully adapts to both single-modal and multi-modalunderstanding and generation tasks.
in this way,unimo outperforms previous methods on both themulti-modal and single-modal downstream tasks.
in the future work, we will focus on end-to-end vi-sual and language uniﬁed learning, and much largerscale of model size and data volumes..acknowledgments.
this work was supported by the national key re-search and development project of china (no.
2018aaa0101900).
2600references.
daniel cer, mona diab, eneko agirre, i˜nigo lopez-gazpio, and lucia specia.
2017. semeval-2017task 1: semantic textual similarity multilingual andin proceedingscrosslingual focused evaluation.
of the 11th international workshop on semanticevaluation (semeval-2017), pages 1–14, vancouver,canada.
association for computational linguistics..ting chen, simon kornblith, mohammad norouzi,and geoffrey hinton.
2020a.
a simple frameworkfor contrastive learning of visual representations.
in international conference on machine learning,pages 1597–1607.
pmlr..xinlei chen, hao fang, tsung-yi lin, ramakr-ishna vedantam, saurabh gupta, piotr doll´ar, andc lawrence zitnick.
2015. microsoft coco captions:data collection and evaluation server.
arxiv preprintarxiv:1504.00325..yen-chun chen, linjie li, licheng yu, ahmedel kholy, faisal ahmed, zhe gan, yu cheng, andjingjing liu.
2020b.
uniter: universal image-textrepresentation learning.
in european conference oncomputer vision, pages 104–120.
springer..jacob devlin, ming-wei chang, kenton lee, andkristina toutanova.
2019. bert: pre-training ofdeep bidirectional transformers for language under-in proceedings of the 2019 conferencestanding.
of the north american chapter of the associationfor computational linguistics: human languagetechnologies, volume 1 (long and short papers),pages 4171–4186, minneapolis, minnesota.
associ-ation for computational linguistics..li dong, nan yang, wenhui wang, furu wei, xi-aodong liu, yu wang, jianfeng gao, ming zhou,and hsiao-wuen hon.
2019. uniﬁed languagemodel pre-training for natural language understand-ing and generation.
in advances in neural informa-tion processing systems, pages 13063–13075..sergey edunov, myle ott, michael auli, and davidgrangier.
2018. understanding back-translation atin proceedings of the 2018 conference onscale.
empirical methods in natural language processing,pages 489–500, brussels, belgium.
association forcomputational linguistics..zhe gan, yen-chun chen, linjie li, chen zhu,yu cheng, and jingjing liu.
2020. large-scale ad-versarial training for vision-and-language represen-tation learning.
arxiv preprint arxiv:2006.06195..yash goyal, tejas khot, douglas summers-stay,dhruv batra, and devi parikh.
2017. making thev in vqa matter: elevating the role of image under-standing in visual question answering.
in proceed-ings of the ieee conference on computer visionand pattern recognition, pages 6904–6913..kaiming he, haoqi fan, yuxin wu, saining xie, andross girshick.
2020. momentum contrast for unsu-pervised visual representation learning.
in proceed-ings of the ieee/cvf conference on computer vi-sion and pattern recognition, pages 9729–9738..kaiming he, xiangyu zhang, shaoqing ren, and jiansun.
2016. deep residual learning for image recog-in proceedings of the ieee conference onnition.
computer vision and pattern recognition, pages 770–778..karl moritz hermann, tomas kocisky, edward grefen-stette, lasse espeholt, will kay, mustafa suleyman,and phil blunsom.
2015. teaching machines to readand comprehend.
advances in neural informationprocessing systems, 28:1693–1701..zhicheng huang, zhaoyang zeng, yupan huang, beiliu, dongmei fu, and jianlong fu.
2021. seeingout of the box: end-to-end pre-training for vision-arxiv preprintlanguage representation learning.
arxiv:2104.03135..yuqi huo, manli zhang, guangzhen liu, haoyu lu,yizhao gao, guoxing yang, jingyuan wen, hengzhang, baogui xu, weihao zheng, et al.
2021.wenlan: bridging vision and language by large-arxiv preprintscale multi-modal pre-training.
arxiv:2103.06561..mandar joshi, danqi chen, yinhan liu, daniel s.weld, luke zettlemoyer, and omer levy.
2020.spanbert: improving pre-training by representingand predicting spans.
transactions of the associa-tion for computational linguistics, 8:64–77..ivan krasin, tom duerig, neil alldrin, vittorio fer-rari, sami abu-el-haija, alina kuznetsova, has-san rom, jasper uijlings, stefan popov, andreasveit, et al.
2017. openimages: a public dataset forlarge-scale multi-label and multi-class image clas-siﬁcation.
dataset available from https://github.
com/openimages, 2(3):2–3..ranjay krishna, yuke zhu, oliver groth, justin john-son, kenji hata, joshua kravitz, stephanie chen,yannis kalantidis, li-jia li, david a shamma, et al.
2017. visual genome: connecting language and vi-sion using crowdsourced dense image annotations.
international journal of computer vision, 123(1):32–73..alex krizhevsky, ilya sutskever, and geoffrey e hin-ton.
2017.imagenet classiﬁcation with deep con-volutional neural networks.
communications of theacm, 60(6):84–90..alon lavie and abhaya agarwal.
2007. meteor: anautomatic metric for mt evaluation with high levelsin proceed-of correlation with human judgments.
ings of the second workshop on statistical machinetranslation, pages 228–231..2601mike lewis, yinhan liu, naman goyal, mar-jan ghazvininejad, abdelrahman mohamed, omerlevy, veselin stoyanov, and luke zettlemoyer.
2020. bart: denoising sequence-to-sequence pre-training for natural language generation, translation,and comprehension.
in proceedings of the 58th an-nual meeting of the association for computationallinguistics, pages 7871–7880, online.
associationfor computational linguistics..gen li, nan duan, yuejian fang, daxin jiang, andming zhou.
2019a.
unicoder-vl: a universal en-coder for vision and language by cross-modal pre-training.
arxiv preprint arxiv:1908.06066..liunian harold li, mark yatskar, da yin, cho-juihsieh, and kai-wei chang.
2019b.
visualbert: asimple and performant baseline for vision and lan-guage.
arxiv preprint arxiv:1908.03557..xiujun li, xi yin, chunyuan li, pengchuan zhang, xi-aowei hu, lei zhang, lijuan wang, houdong hu,li dong, furu wei, et al.
2020. oscar: object-semantics aligned pre-training for vision-languagetasks.
in european conference on computer vision,pages 121–137.
springer..huan lin, fandong meng, jinsong su, yongjing yin,zhengyuan yang, yubin ge, jie zhou, and jieboluo.
2020a.
dynamic context-guided capsule net-in pro-work for multimodal machine translation.
ceedings of the 28th acm international conferenceon multimedia, pages 1320–1329..junyang lin, an yang, yichang zhang, jie liu, jingrenzhou, and hongxia yang.
2020b.
interbert: vision-and-language interaction for multi-modal pretrain-ing.
arxiv preprint arxiv:2003.13198..tsung-yi lin, michael maire, serge belongie, jameshays, pietro perona, deva ramanan, piotr doll´ar,and c lawrence zitnick.
2014. microsoft coco:in european confer-common objects in context.
ence on computer vision, pages 740–755.
springer..yinhan liu, myle ott, naman goyal, jingfei du, man-dar joshi, danqi chen, omer levy, mike lewis,luke zettlemoyer, and veselin stoyanov.
2019.roberta: a robustly optimized bert pretraining ap-proach.
arxiv preprint arxiv:1907.11692..jiasen lu, dhruv batra, devi parikh, and stefanlee.
2019. vilbert: pretraining task-agnostic visi-olinguistic representations for vision-and-languagetasks.
in advances in neural information process-ing systems, pages 13–23..vishvak murahari, dhruv batra, devi parikh, and ab-hishek das.
2020. large-scale pretraining for visualdialog: a simple state-of-the-art baseline.
in euro-pean conference on computer vision, pages 336–352. springer..vicente ordonez, girish kulkarni, and tamara berg.
2011. im2text: describing images using 1 millioncaptioned photographs.
advances in neural infor-mation processing systems, 24:1143–1151..alec radford, jong wook kim, chris hallacy, adityaramesh, gabriel goh, sandhini agarwal, girishsastry, amanda askell, pamela mishkin, jack clark,et al.
2021. learning transferable visual modelsfrom natural language supervision.
arxiv preprintarxiv:2103.00020..alec radford, karthik narasimhan, tim salimans, andimproving language under-.
ilya sutskever.
2018.standing by generative pre-training..pranav rajpurkar, jian zhang, konstantin lopyrev, andpercy liang.
2016. squad: 100,000+ questions formachine comprehension of text.
in proceedings ofthe 2016 conference on empirical methods in natu-ral language processing, pages 2383–2392, austin,texas.
association for computational linguistics..siva reddy, danqi chen, and christopher d. manning.
2019. coqa: a conversational question answeringchallenge.
transactions of the association for com-putational linguistics, 7:249–266..shaoqing ren, kaiming he, ross girshick, and jiansun.
2016. faster r-cnn: towards real-time ob-ject detection with region proposal networks.
ieeetransactions on pattern analysis and machine intelli-gence, 39(6):1137–1149..alexander m. rush, sumit chopra, and jason weston.
2015. a neural attention model for abstractive sen-in proceedings of the 2015tence summarization.
conference on empirical methods in natural lan-guage processing, pages 379–389, lisbon, portugal.
association for computational linguistics..rico sennrich, barry haddow, and alexandra birch.
2016. neural machine translation of rare wordswith subword units.
in proceedings of the 54th an-nual meeting of the association for computationallinguistics (volume 1: long papers), pages 1715–1725, berlin, germany.
association for computa-tional linguistics..piyush sharma, nan ding, sebastian goodman, andradu soricut.
2018.conceptual captions: acleaned, hypernymed, image alt-text dataset for au-in proceedings of thetomatic image captioning.
56th annual meeting of the association for compu-tational linguistics (volume 1: long papers), pages2556–2565, melbourne, australia.
association forcomputational linguistics..karen simonyan and andrew zisserman.
2014. verydeep convolutional networks for large-scale imagerecognition.
arxiv preprint arxiv:1409.1556..richard socher, alex perelygin, jean wu, jasonchuang, christopher d. manning, andrew ng, andchristopher potts.
2013. recursive deep modelsfor semantic compositionality over a sentiment tree-in proceedings of the 2013 conference onbank.
empirical methods in natural language processing,pages 1631–1642, seattle, washington, usa.
asso-ciation for computational linguistics..2602luowei zhou, hamid palangi, lei zhang, houdonghu, jason corso, and jianfeng gao.
2020. uni-ﬁed vision-language pre-training for image caption-ing and vqa.
in proceedings of the aaai conferenceon artiﬁcial intelligence, volume 34, pages 13041–13049..yukun zhu, ryan kiros, rich zemel, ruslan salakhut-dinov, raquel urtasun, antonio torralba, and sanjafidler.
2015. aligning books and movies: towardsstory-like visual explanations by watching moviesand reading books.
in proceedings of the ieee inter-national conference on computer vision, pages 19–27..jinsong su, jinchang chen, hui jiang, chulun zhou,huan lin, yubin ge, qingqiang wu, and yongx-uan lai.
2021. multi-modal neural machine trans-lation with deep semantic interactions.
informationsciences, 554:47–60..weijie su, xizhou zhu, yue cao, bin li, lewei lu,furu wei, and jifeng dai.
2019. vl-bert: pre-training of generic visual-linguistic representations.
arxiv preprint arxiv:1908.08530..markus johannes van ackeren, francesca m barbero,stefania mattioni, roberto bottini, and olivier col-lignon.
2018. neuronal populations in the occipitalcortex of the blind synchronize to the temporal dy-namics of speech.
elife, 7:e31640..yu-siang wang, chenxi liu, xiaohui zeng, and alanyuille.
2018. scene graph parsing as dependencyparsing.
in proceedings of the 2018 conference ofthe north american chapter of the association forcomputational linguistics: human language tech-nologies, volume 1 (long papers), pages 397–407,new orleans, louisiana.
association for computa-tional linguistics..alex warstadt, amanpreet singh, and samuel r bow-man.
2019. cola: the corpus of linguistic accept-ability (with added annotations)..adina williams, nikita nangia, and samuel r bow-man.
2017. a broad-coverage challenge corpus forarxivsentence understanding through inference.
preprint arxiv:1704.05426..dongling xiao, han zhang, yukun li, yu sun, haotian, hua wu, and haifeng wang.
2020. ernie-gen:an enhanced multi-ﬂow pre-training and ﬁne-tuningframework for natural language generation.
arxivpreprint arxiv:2001.11314..ning xie, farley lai, derek doran, and asim ka-dav.
2019. visual entailment: a novel task forﬁne-grained image understanding.
arxiv preprintarxiv:1901.06706..zhilin yang, zihang dai, yiming yang, jaime car-bonell, russ r salakhutdinov, and quoc v le.
2019.xlnet: generalized autoregressive pretraining forlanguage understanding.
in advances in neural in-formation processing systems, pages 5753–5763..peter young, alice lai, micah hodosh, and julia hock-enmaier.
2014. from image descriptions to visualdenotations: new similarity metrics for semantic in-ference over event descriptions.
transactions of theassociation for computational linguistics, 2:67–78..fei yu, jiji tang, weichong yin, yu sun, haotian, hua wu, and haifeng wang.
2020. ernie-vil: knowledge enhanced vision-language repre-arxiv preprintsentations through scene graph.
arxiv:2006.16934..2603a pre-training settings.
data processing the pre-training datasets con-sist of text corpus, image collections and image-text pairs.
the detail statistics of them are shownin table 5. for uniﬁed-modal learning, all data(including images, texts and image-text pairs) arerepresented in the same format with both visualand textual input as “[img] [box1] ... [box100][cls] [tok1] ... [tokn] [sep]”, which “[box]” and“[tok]” denote an image region and subword token,respectively.
for single-modal images, a pseudotoken sequence “[cls] [pad] ... [sep]” is treatedas the textual input during pre-training.
during vi-sual learning on images, the pseudo token sequencewill be masked out by special self-attention masksto eliminate its effect to the visual learning process.
the language learning process will not be appliedon the pseudo token sequence.
so the single-modalimages are equivalent to be encoded individuallyrather than in pair.
similarly, for single-modaltexts, a pseudo image-region sequence “[img] [0]... [0]” will be utilized as the visual input, where“[0]” denotes a zero-value feature embedding.
dur-ing language learning, the pseudo image-regionsequence will be masked out.
based on the abovetechniques, both images and texts are representedin the same format as image-text pairs.
for image-text pairs, both the visual learning and languagelearning are applied on the images and captions si-multaneously to learn cross-modal representations..training details during pre-training, the sam-ples of image collections, text corpus and image-text pairs are randomly mixed together with ra-tio 1:1:5. the objectives of language learning, vi-sual learning and cross-modal contrastive learning(cmcl) are trained jointly.
the hyper-parametersfor both unimo-base and unimo-large areshown in table 6. for cmcl, each positive image-text pair is appended with several hard negativesamples by text rewriting, as well as several posi-tive images and texts by image/text retrieval.
allsamples for other image-text pairs in the trainingbatch are also treated as the negative samples (in-cluding negative images and negative texts), whichare more than 6k for unimo-base and 3k forunimo-large.
for an image-text pair (v, w ), thedetail formula of the cmcl loss lcm cl(v, w )is as follows:.
−log.
posp + posi + post(negp + negi + negt ) + (posp + posi + post ).
posp =.
(cid:88).
exp(d(v +, w +)/τ ).
.
.
posi =.
exp(d(v r, w )/τ ).
post =.
exp(d(v, w r)/τ ).
(v +,w +)∈x +.
(cid:88).
v r ∈x i(cid:88).
w r ∈x t.(cid:88).
(v −,w −)∈x −.
negi =.
(cid:88).
exp(d(v (cid:48), w )/τ ).
negt =.
exp(d(v, w (cid:48))/τ ).
v (cid:48)∈y i(cid:88).
w (cid:48)∈y t.negp =.
exp(d(v −, w −)/τ ).
(7).
where posp , posi and post denote the scores ofpositive image-text pairs x +, related images x iand related texts x t , respectively.
also, negp ,negi and negt denote the scores of negative image-text pairs x −, negative images y i and negativetexts y t , respectively.
the objective is to maxi-mize the positive score posp + posi + post whileminimizing the negative score negp +negi +negt ,while help aligns and uniﬁes the visual and textualrepresentation spaces.
the pre-training process ofunimo is described in algorithm 1 in pseudo-code style..data augmentations we apply two types ofdata augmentation techniques in the cmcl: textrewriting and image/text retrieval.
the text rewrit-ing techniques are utilized to create positive andnegative examples for cmcl.
to create more posi-tive image-text pairs, we apply back-translation toall captions in the image-text pairs.
each captionis translated into 3 kinds of languages, includingchinese, french and spanish, by our translationtool in house, and then translated back to english.
for the phrase-level and word-level rewriting, eachcaption in the image-text pairs is ﬁrstly parsed intoa scene graph by the stanford scene graph parser1.
all objects, attributes and relations are extracted tobuild an object vocabulary, an attribute vocabularyand a relation vocabulary.
for each caption, the ob-jects, attributes or relations are randomly replacedwith other similar objects, attributes or relations inthe corresponding vocabularies, respectively.
therewritten captions are ranked based on their linguis-tic ﬂuency, and the top 100 captions are selectedto create hard negative image-text pairs by com-posing with the original image.
furthermore, theimage and text retrieval techniques are utilized to.
1https://nlp.stanford.edu/software/scenegraph-.
(6).
parser.shtml.
2604typedatasettrainval.
coco vg533k25k.
image-text pairs.
cc.
sbu.
images.
5.06m 3.0m 990k 1.7m106k.
10k.
14k.
text corpusbookwiki openwebtext16g.
38g.
table 5: statistics of the image-text pairs, image collections and text corpus for pre-training..hyper-parametersnum of layershidden sizeffn hidden sizeattention headshead sizedropoutattention dropoutwarmup stepspeak learning ratebatch sizeweight decaymax training stepslearning rate decayadam (cid:15)adam β1adam β2gradient clipping.
unimo-base unimo-large12768307212640.10.124k5e-56k0.011mlinear1e-60.90.9991.0.
241024409616640.10.130k5e-53k0.011mlinear1e-60.90.9991.0.table 6: hyper-parameters for unimo pre-training..augment each image-text pair with various relatedimages and texts from the single-modal image col-lections and text corpus.
for image-retrieval, eachimage is transformed into 100 image regions andthe object labels are detected for all regions byfaster r-cnn.
the object labels are utilized tocreate a tf-idf feature vector for each image, andthe cosine similarity between images are computed.
for each image in the image-text pairs, 100 of themost similar images are retrieved from the imagecollections, which are treated as positive imagesin the cmcl.
for text retrieval, we ﬁrstly buildan inverted index for all image captions and sen-tences in the text corpus, then ﬁlter non-relevantsentences from the text corpus based on the in-verted index.
for each caption in the image-textpairs, the tf-idf similarities between the captionand the relevant sentences retrieved by the invertedindex are calculated, and the top-1000 sentencesare extracted.
further, bert-based embeddingsimilarities are computed between the caption andthe 1000 sentences to rank them, and the top-100sentences are extracted as the positive texts for thecmcl..b finetuning settings.
task deﬁnition and details the multi-modalﬁnetuning tasks include: (1) vqa requires themodel to answer natural language questions by se-.
algorithm 1 unimo’s pre-training process in apython-like style..# the training details of unimofunction pretraining processfor step in all steps do.
batch = []# load x image samplesimgs = get data(imgcollections, x)# load y text samplestexts = get data(t extcorpus, y)# load z image-text pairsimg text pairs = get data(p airs, z)# load cmcl data for each image-text pairfor pair in img text pairs do.
samples = cmcl data loader(pair)batch.extend(samples).
end forbatch.extend(texts)batch.extend(imgs)v loss, l loss, cmcl loss = unimo(batch)loss = v loss + l loss + cmcl lossloss.backward().
end forend function.
# build cmcl samples for each image-text pairfunction cmcl data loader.
samples = []# sample a positive pairs from back-translationpos pairs = sample pos pairs(pair, a)# sample b negative pairs from text rewritingneg pairs = sample neg pairs(pair, b)# sample c sentences from text retrievalpos imgs = sample pos imgs(pair, c)# sample d images from image retrievalpos texts = sample pos texts(pair, d)samples.extend(pair)samples.extend(pos pairs)samples.extend(neg pairs)samples.extend(pos imgs)samples.extend(pos texts)return samples.
end function.
lecting the correct answer from a multi-choice listbased on an image.
we conduct experiments onthe widely-used vqa v2.0 dataset, which is builtbased on the coco images.
similar to previouswork, both training and validation sets are usedfor training for the results on both the test-std and(2) image caption requires thetest-dev splits.
model to generate a natural language description ofan image.
we report our results on the microsoftcoco captions dataset.
following karpathy’ssplit, the dataset contains 113.2k/5k/5k images for.
2605#images (#text).
task.
image src..train.
val.
vqaimage captionvisual entailmentimage-text retrieval.
cocococoflickr30kflickr30k.
83k(444k)113.2k529.5k29k(145k).
41k(214k)5k17.9k1k(5k).
test.
test-std81k(107k)5k17.9k1k(5k).
test-dev81k(448k)---.
table 7: statistics of the datasets for the multi-modal downstream tasks..hyper-parametersbatch sizeepoch.
learning rate.
warmup ratioweight decay.
image-text retrieval64/32405e-6 for epoch=[0,24]5e-7 for epoch=[24,32]5e-8 for epoch=[32,40]-0.01.
1e-5.
0.060.0.snli-ve vqa192/6410.
256/256121e-4/4e-5 for epoch=[0,5]1e-5/4e-6 for epoch=[6,8]1e-6/4e-7 for epoch=[9,12]-0.01.
0.060.01.coco caption64/32101e-5/5e-6.
table 8: hyper-parameters (base/large) for ﬁne-tuning multi-modal tasks ..hyper-parameterslearning ratebatch sizeepochswarmup raitobeam sizelength penaltytrigram blocking.
sst-2/mnli/cola/sts-b{1e-5, 2e-5, 3e-5}{16, 32}100.06---.
cnndm gigaword4e-5/2e-532200.0660.6/1.2true.
3e-5128100.0660.6/1.2false.
squad-qg1.25e-5/5e-632200.0661.0/1.2false.
coqa1e-5/8e-632200.0630.0false.
table 9: hyper-parameters (base/large) for ﬁne-tuning single-modal tasks..train/val/test splits respectively.
(3) visual entail-ment (snli-ve) is evaluated on the slni-vedataset which was derived from flickr30k imagesand stanford natural language inference (snli)dataset.
the task is to determine the logical rela-tionship (i.e., “entailment”, “neutral” and “contra-diction”) between a natural language statement andan image.
(4) image-text retrieval is evaluatedon the flickr30k dataset, which contains two sub-tasks: image retrieval (flickr30k-ir) and text re-trieval (flickr30k-tr), depending on which modal-ity is used as the retrieved target.
we report the top-k retrieval results on the test sets, including r@1,r@5 and r@10 (r denotes recall).
the statisticsof the datasets for the above multimodal-tasks aredescribed in table 7. the hyper-parameters forall the downstream tasks, including both the multi-modal tasks and single-modal tasks are shown intable 8 and 9..c visualization and analysis.
to intuitively show the effectiveness of the uniﬁed-modal learning on the corpus of images, texts andimage-text pairs, we utilize 2-dimensional visual-ization of the embeddings by principal componentanalysis (pca).
the nearest neighbors of the center.
word are shown in the embedding space.
unimois compared with two ablation models describedin section 4.3. the ﬁgure shows that the model“unimo-w/o texts” can ﬁnd more visual relevantwords than “unimo-w/o image&pairs”, whichdemonstrates the effectiveness of the visual learn-ing on images.
however, unimo not only ﬁndsmany visually relevant words, but also ﬁnds somesemantic relevant background words.
for example,unimo ﬁnds “lunch” and “airplanes” for the cen-ter word “hamburger”, which denotes people usu-ally eat hamburger at lunch and often eat it whileﬂying.
also, for the second example, unimo ﬁndsrelevant concepts “meter”, “steps” and “soccer” for“foot”, which enrich the concept and connect it withrich relevant information..to further intuitively show the advantages ofthe uniﬁed-modal learning with rich single-modaldata, we compare unimo with the multimodalpre-training model “w/o single modal” (describedin section 4.2), on both the text retrieval and im-age retrieval tasks.
the examples of text retrievalresults in figure 5 show that the retrieved captionsby unimo describes the images more accuratelyby including different levels of information, includ-ing objects, attributes and relations in images.
the.
2606figure 4: 2-dimensional visualization by pca..figure 5: text retrieval examples by r@1. the greencolor denotes accurate visual information while the reddenotes wrong information..examples of the image retrieval results in figure6 also show that the retrieved images better matchthe captions with more detail semantic alignments..figure 6: image retrieval examples by r@1. the bluecolor denotes the important information that has beenneglected by the baseline model, but is accurately rec-ognized by unimo..2607(a) unimo - w/o images&pairs(b) unimo - w/o texts(c) unimounimo: two men are in a subway station getting ready to mop.baseline: two men are standing at telephone booths outside.unimo: a child dressed in blue jeans with rolled cuffs and a pink hoodie waits outdoors at the foot of the stairs with an axe.baseline: young boy with a broom sweeps a deck in a wooded area.unimo: three guys are jumping on some grass and making funny faces, you can see their shadows on the ground.baseline: a group of young men are running a race.unimo: two bicyclists are racing each other on a dirt track.baseline: three runners are on a track and two of them are jumping hurdles.a group of men are loading cotton onto a truckunimobaselinetexta woman in a red shirt playing the cello.children enjoying themselves on an amusement park ride.a man and a little boy beating drums.two men are smiling and riding bicycles.