bad seeds: evaluating lexical methods for bias measurement.
maria antoniakcornell universitymaa343@cornell.edu.
david mimnocornell universitymimno@cornell.edu.
abstract.
a common factor in bias measurement meth-ods is the use of hand-curated seed lexicons,but there remains little guidance for their selec-tion.
we gather seeds used in prior work, docu-menting their common sources and rationales,and in case studies of three english-languagecorpora, we enumerate the different types ofsocial biases and linguistic features that, onceencoded in the seeds, can affect subsequentbias measurements.
seeds developed in onecontext are often re-used in other contexts, butdocumentation and evaluation remain neces-sary precursors to relying on seeds for sensi-tive measurements..1.introduction.
there has been increasing concern in the nlp com-munity over bias and stereotypes contained in mod-els and how these biases can trickle downstreamto practical applications, such as serving job ad-vertisements.
in particular, there has been muchrecent scrutiny of word representations, with manystudies ﬁnding harmful associations encoded in em-bedding models.
combating such biases requiresmeasuring the bias encoded in a model so that re-searchers can establish improvements, and manyvariants of embedding-based measurement tech-niques have been proposed (bolukbasi et al., 2016;caliskan et al., 2017; manzini et al., 2019)..these measurements have had the additional up-stream beneﬁt of providing computational socialscience and digital humanities scholars with a newmeans of quantifying bias in datasets of social, po-litical, or literary interest.
researchers increasinglyuse embeddings (garg et al., 2018; knoche et al.,2019a; hoyle et al., 2019) and other lexicon-basedmethods (saez-trumper et al., 2013; fast et al.,2016; rudinger et al., 2017) to provide quantitativeanswers to otherwise elusive political and social.
target concept.
highlighted seeds.
unpleasant.
divorce, jail, poverty, cancer, ....african american.
tanisha, tia, lakisha, latoya, ....domestic work.
mom, mum, ....ugliness.
fat, chubby, obese, fatty,overweight, disformed, disﬁgured,wrinkle, wrinkled, ....table 1: examples of real seed terms used in recentwork to measure biases in corpora..questions about the biases in a corpus and its au-thors.
this work often involves comparing biasmeasurements across different corpora, which re-quires reliable, ﬁne-grained measurements..while there is a wide range of bias measure-ment methods, every one of them relies on lexi-cons of seed terms to specify stereotypes or dimen-sions of interest.
but the rationale for choosingspeciﬁc seeds is often unclear; sometimes seedsare crowd-sourced, sometimes hand-selected by re-searchers, and sometimes drawn from prior workin the social sciences.
the impact of the seeds isnot well-understood, and many previous seed setshave serious limitations.
as shown in table 1, theseeds used for bias measurement can themselvesexhibit cultural and cognitive biases (e.g., reduc-tive deﬁnitions), and in addition, linguistic featuresof the seeds (e.g., frequency) can affect bias mea-surements (ethayarajh et al., 2019).
though theyare often re-used, the suitability of these seeds tonovel corpora is uncertain, and while evaluationssometimes include permutation tests, distinct setsof seeds are rarely compared..we use a mixture of literature survey, qualitativeanalysis of seed terms, and analytic methods toexplore the use of seed sets for bias measurementthrough two overarching research questions.
(1)we explore how seeds are selected and from which.
proceedingsofthe59thannualmeetingoftheassociationforcomputationallinguisticsandthe11thinternationaljointconferenceonnaturallanguageprocessing,pages1889–1904august1–6,2021.©2021associationforcomputationallinguistics1889sources they are drawn to better understand ratio-nales and assumptions underlying common seedsets.
(2) we explore which features of seeds cancause instability, including both social biases andlinguistic dimensions in our analysis..our work provides the following contributions.
documentation: we document and test 178 seedsets used in prior work, and we release this docu-mentation as a resource for the research commu-nity.1 analysis: we provide a systematic frame-work for understanding the different sources ofinstability in seed sets that can affect bias measure-ments.
we compare the gathered seeds to largersets of artiﬁcially created seed sets, and we investi-gate the reliability of seed terms used for two popu-lar embedding-based bias measurement methods incase studies on three datasets.
recommendations:with this larger perspective, we discuss how seedsets should be examined versus how these sets arepopularly considered and what kind of documenta-tion best practices should be followed.
seeds are abrittle but unavoidable element of current bias mea-surement algorithms, with weaknesses that needprobing even for embedding-based measurements..2 background and related work.
the term “bias” has many deﬁnitions, from a value-neutral meaning in statistics to a more normativemeaning in socio-cultural studies.
in the bias mea-surement literature in nlp, lack of precise deﬁni-tions and problem speciﬁcations (blodgett et al.,2020) has led to many of the errors we explorein this paper.
in general, “bias” in nlp most of-ten represents harmful prejudices (caliskan et al.,2017) whose spurious and undesirable inﬂuencecan affect model outputs.
while these downstreameffects have inspired work on “removing” bias fromembedding models (bolukbasi et al., 2016), therehave also been critiques of these efforts (gonen andgoldberg, 2019), and we do not focus on this usecase in our study.
instead, we focus on bias mea-surement as a tool used in diverse settings to makecomparisons across speciﬁc corpora of interest..unsupervised methods for bias measurementinformationhave included pointwise mutual(rudinger et al., 2017), normalized frequenciesand cosine similarity of tf-idf weighted wordvectors (saez-trumper et al., 2013), generativemodels (joseph et al., 2017; hoyle et al., 2019),.
1seeds and documentation are available at https://gi.
thub.com/maria-antoniak/bad-seeds.
and a combination of odds ratios, embeddings, andcrowd-sourcing (fast et al., 2016).
all of thesemethods rely on sets of seed terms.
while muchrecent nlp work has focused on contextual embed-dings, most recent bias-detection work has focusedon vocabulary-based embeddings and word rep-resentations.
researchers have increasingly usedembedding-based methods to measure biases anddraw comparisons in training corpora of social in-terest (kim et al., 2014; hamilton et al., 2016;kulkarni et al., 2016; phillips et al., 2017; ko-zlowski et al., 2019).
for example, bhatia et al.
(2018) train embedding models on news sources tocompare trait associations for political candidates.
we believe that our results should extend to contex-tual embedding methods (zhao et al., 2019; sedocand ungar, 2019), but vocabulary-based embed-dings are easier to analyze..we discuss several recent studies that includeanalysis of seed sets (kozlowski et al., 2019; etha-yarajh et al., 2019; sedoc and ungar, 2019) in §8..3 data description.
training corpora.
our dataset choices areguided by our focus on the upstream use case,where embeddings are trained on relatively small,special-purpose collections to answer social andhumanist questions about the training corpus.
thescope of these datasets ﬁts the use case of a socialscientist interested in measuring bias during a smalltime window, across speciﬁc genres, or in a partic-ular set of authors.
table 2 shows an overview ofthe data, and more details are in the appendix..our datasets include: new york times arti-cles from april 15th-june 30th, 2016; high qual-ity wikitext articles, using the full wikitext-103training set (merity et al., 2016); and goodreadsbook reviews for the romance and history and bi-ography genres, sampled from the ucsd bookgraph (wan and mcauley, 2018; wan et al., 2019).
for added validity, we also replicate existing stud-ies, using a pre-trained model on a large googlenews corpus (mikolov et al., 2013)..for each dataset, we lowercase all text, parseand obtain pos tags using spacy (honnibal et al.,2020), tokenize the text into unigrams, and ﬁlterwords that occur fewer than 10 times in the trainingdataset.
lowercasing controls for the varying levelsof capitalization used in the gathered seeds.
weleave analysis of bigram seeds to future work andrely on unigrams as a simplifying assumption..1890dataset.
total words.
vocabulary size.
nytwikitextgoodreads (romance)goodreads (history/biography).
totaldocuments8,888 articles28,472 articles197,000 reviews136,000 reviews.
7,244,457 words99,197,146 words24,856,924 words14,324,947 words.
162,998 unique words546,828 unique words214,572 unique words163,171 unique words.
meandocument length815 words3,484 words126 words105 words.
table 2: summary statistics for our test datasets.
in contrast to the large, generic datasets often used for downstreamapplications, these datasets are small and culturally speciﬁc..of these seeds and the rationales for using themare not always explained by researchers, but incases where we were able to determine a source orrationale, we group them into the following cate-gories.
table 3 overviews the source frequencies.
we emphasize that each source comes with risksand beneﬁts; there is no one correct method to se-lecting seeds, but awareness of pros and cons canhelp guide decisions and evaluation methods..borrowed from social sciences.
seed sets areoften borrowed from prior work in psychology andother social sciences, usually in an effort to eitherreplicate results or build conﬁdence from previ-ously validated work.
for example, caliskan et al.
(2017) validate prompts from the implicit associa-tion test (greenwald et al., 1998), while garg et al.
(2018) and hoyle et al.
(2019) use personality traitsfrom williams and bennett (1975); williams andbest (1977, 1990).
sometimes the seeds appealfor validity via highly cited resources, like liwc(pennebaker et al., 2001), despite critiques aboutunreliability (panger, 2016; forscher et al., 2017).
borrowing seeds does not absolve researchers fromexamining and validating seeds..crowd-sourced.
custom seed sets can be cre-ated through crowd-based annotation.
fast et al.
(2016) use mechanical turk to validate the inclu-sion of terms in their seed sets; the ﬁnal terms arethen included in packaged code for researchers andpractitioners.
kozlowski et al.
(2019) use mechan-ical turk to gather ratings of items scaled alonggender, race, and class.
crowd-sourcing can aidin gathering contemporary associations and stereo-types.
however, controlling for crowd demograph-ics can be difﬁcult, and crowd-sourcing can resultin alarming errors, in which popular stereotypesare hard-coded into the seeds (as in table 1)..figure 1: overview of the gathered seed sets, showingquartiles and medians.
outliers are truncated on theplot showing the number of seeds per set; the maximumnumber is 1,460 seeds..corpus-derivedre-usedborrowed from social sciencescuratedadapted from lexical resourcescrowd-sourcedpopulation-derived.
7/18 papers7/18 papers6/18 papers5/18 papers3/18 papers2/18 papers2/18 papers.
table 3: overview of the surveyed seed sources..gathered seeds sets.
we gather 178 seed setsused in a representative sample of 18 highly-citedprior works on bias measurement.
seeds includeboth embedding-based and non-embedding-basedbias detection methods as there is often crossoverand re-use of seed sets.
because we use word em-bedding models trained on unigrams, we do notinclude bigram seeds in our analysis, and in eachexperiment, we omit words that were not presentin our training set.
while these choices could beseen as limitations, we see them as realistic appli-cations of seeds to constrained datasets, reﬂectingthe scenario in which biases are compared acrossspeciﬁc corpora.
figure 1 overviews the seed sets,examples used in the paper are documented in theappendix, and the full collection is shared in thesupplementary materials and is available online..4 how are seeds selected?.
how do researchers select seeds, and from whichsources are they popularly drawn?
we explore thisquestion using the gathered seed sets from priorworks on unsupervised bias detection.
the origins.
population-derived.
some seed sets are derivedfrom government-collected population datasets.
popular sources include u.s. census data (boluk-basi et al., 2016; caliskan et al., 2017), the u.s. bu-.
1891020406080100number of seeds per set05101520253035number of sets per paperreau of labor statistics (caliskan et al., 2017), andthe u.s. social security administration (garg et al.,2018).
these sources are usually used to gathernames and occupations common to certain demo-graphic groups.
these sources tend to be u.s.-centric, though the training data for the embed-ding does not always match (e.g., large wikipediadatasets are not guaranteed to have u.s. authors).
reliance on these sources is particularly vulnerableto reductive deﬁnitions of the target concepts—e.g.,gender (keyes, 2017)—and assumes a level of trustand representation in the data collection that mightnot exist evenly across groups..adapted from lexical resources.
some seedsets are drawn from existing dictionaries, lexicons,and other public resources, such as semeval tasks(zhao et al., 2018) and conceptnet (fast et al.,2016).
pre-packaged sentiment lexicons are a popu-lar source (saez-trumper et al., 2013; sweeney andnajaﬁan, 2019); these lexicons include the affec-tive norms for english words (anew) (bradleyand lang, 1999) and negative/positive sentimentwords from hu and liu (2004).
these seeds havethe advantage of previous rounds of validation, butthis does not guarantee validity for new domains..corpus-derived.
quantitative methods can beused to extract seed terms from a corpus of inter-est.
for example, saez-trumper et al.
(2013) usesorted lists of named entities extracted from a tar-get dataset to create seed sets for personas of inter-est.
similarly, sweeney and najaﬁan (2019) extracthigh frequency identity terms from a wikipedia cor-pus.
these methods have the advantage of ensuringhigh frequency terms in the target dataset, but theypose similar risks to crowd-sourcing; unless an ex-tra round of cleaning and curation is completed bythe researchers, terms with unintended effects canbe included in the seed sets..curated.
seed sets are sometimes hand-selectedby the authors, usually after close reading of thecorpus of interest.
for example, rudinger et al.
(2017) hand-select a set of seed terms that corre-spond to a set of demographic categories of interest,and joseph et al.
(2017) hand-select a set of identityseeds based on their frequency in a twitter dataset.
often, even when papers rely on other seed sources,manual curation is included as a step in the seedcreation process.
hand-curation can result in highprecision seeds, but this method relies on the au-thors’ correction for their own social biases..re-used.
finally, many papers rely on prior biasmeasurement research for seed terms.
the mostpopular sources in our survey include early paperson bias in embeddings such as bolukbasi et al.
(2016) and caliskan et al.
(2017).
this repetitionmeans that the seeds are tested on many differentdatasets, but they should not be trusted without val-idation; there can be mismatches in frequency andcontextual meaning between datasets..5 bias measurement algorithms.
in the upstream use case, locally trained wordembeddings remain state of the art because ﬁne-tuned pre-trained contextual models might intro-duce extrinsic information, and it is not feasibleto pre-train contemporary contextual embeddingson such small collections.
here, we focus ontwo popular seed-based methods to detect bias inword embeddings.
bolukbasi et al.
(2016) andcaliskan et al.
(2017) both introduce embedding-based methods for bias detection that rely on setsof seed words.
each of these methods requirestwo sets of seed words, x and y, and one ad-ditionally requires matched pairs of seed words{(x1, y1), (x2, y2), ...}..weat.
given a set of embedding vectors w,the word embedding association test (weat)(caliskan et al., 2017) deﬁnes a vector based on thedifference between the mean vector of the two tar-get sets, and then measures the cosine similarity ofa set of attribute words to that vector.
the strengthof the association between the target sets x and y,and the sets of attributes, a, and b, is given by.
s(x , y, a, b) =.
s(x, a, b) −.
s(y, a, b).
(cid:88).
x∈x.
(cid:88).
y∈y.
where s(w, a, b) is equal to the difference in av-erage cosine similarities between a query w andeach term in a and w and each term in b. totest whether the resulting difference s(x , y, a, b)is signiﬁcant, this result is compared to the samefunction applied to randomly permuted sets drawnfrom x and y. caliskan et al.
(2017) use weatto measure stereotypical associations between setsof targets and attributes, where, for example, thetarget terms might be arts and science terms, andthe attribute terms might be male and female terms..pca.
the principal component analysis (pca)method tests how much variability there is in thedifference vectors between pairs of word vectors.
1892(bolukbasi et al., 2016).
if the vector differencebetween pairs of seed terms can be approximatedwell by a single constant vector c, then this vectorrepresents a bias subspace.
in this case, the sub-space is simply a one dimensional vector, thoughthis process could be extended to more dimensions.
for each pair of embedding vectors correspondingto one seed word from set x and one from set y,bolukbasi et al.
(2016) calculate the mean vectorof those two vectors and then include the two re-sulting half vectors from that mean to the two seedvectors as columns in the input matrix..6 quantifying variation from seeds.
to quantify how large an effect seed features canhave on bias measurements, we calculate a set ofmetrics for both pca and weat methods that sum-marizes how well the bias subspace represents thetarget seeds.
for each dataset, we use the popu-lar skip-gram with negative sampling (sgns) al-gorithm to train a word2vec model.
we use thegensim package for training ( ˇreh˚uˇrek and sojka,2010).
we use a window size of 5, a minimumword count of 10, and a vector size of 100 for allexperiments.
we repeat this process across 20 boot-strapped samples of each dataset..for pca, we calculate the difference vector be-tween the embedding vectors for each pair of wordsin the two seed sets.
for each set of paired seedsets, we run pca and plot the percent of varianceexplained by each component.
for the gatheredseeds, we only use pairings documented in priorwork.
we perform a manual conﬁrmation that theﬁrst component g indeed represents the bias sub-space by ranking all the words in the vocabulary bytheir cosine similarity to g..for weat, we hold the attribute terms constant,where a = [“good”] and b = [“bad”], while ourgenerated seed sets take the place of the targetterms x and y. holding the attribute terms con-stant is a simplifying assumption; our goal is notto test all possible attribute terms but to show thatsigniﬁcant variation is possible.
we then calculatethe weat test statistic and signiﬁcance..coherence.
in addition to the pca explainedvariance and weat test statistic, we also mea-sure the coherence of each pairing of seed sets afterbeing mapped to the bias subspace.
ideally, whenwe project all the words in the vocabulary onto thesubspace, the two sets would be drawn as far apartas possible.
we rank all words by cosine similarity.
figure 2: bias measurements depend on seeds.
we cal-culate the cosine similarities between different femaleseed sets and an averaged upleasantness vector fromtwo embedding models.
results are consistent acrossseeds for romance review embeddings, but vary widelybetween sets for history and biography.
we ﬁnd similarvariation even for a pretrained google news model..to the bias subspace, and we measure the absolutedifference in mean rank of the paired seed sets:.
coherence(x1, y2) = |r1 − r2|,.
where x1 and y2 are seed sets and r1 and r2 aretheir mean ranks in the bias subspace.
finally, wenormalize the scores to a [0, 1] range.
higher co-herence scores indicate that the seed sets have verydifferent mean ranks, i.e., the seeds are separatedby more of the vocabulary.
for example, in fig-ure 4, ordered seeds (a) produce a subspace withgreater coherence (sets are further apart in the biassubspace) than shufﬂed seeds (b)..generated seed sets.
in order to control for fre-quency and pos when measuring instabilities dueto semantic similarity and word order, we generatea large collection of artiﬁcial, randomized seed sets.
we select a target term at random from the model’svocabulary, ﬁltered by pos.
each seed set consistsof this target term and its four nearest neighbors,ranked by cosine similarity.
we repeat this processfor each of the models trained on the bootstrappedsamples of the corpus.
we choose seed sets thatare semantically similar (rather than randomly se-lecting seeds) because we expect that seed sets ofrealistic research interest would be coherent.
weemphasize that researchers have used bias measure-ment methods for increasingly creative purposes,moving beyond gender and race, and similar biasmeasurement techniques can be used for aspect de-tection and other seed-based tasks.
example seedsare shown in table 4..18930.00.20.4similarity to unpleasantness vectorwoman, women, she, her, her,...(kozlowski et al 2019)sister, female, woman, girl, daughter,...(caliskan et al 2017)woman, girl, she, mother, gal,...(bolukbasi et al 2016)woman, girl, mother, daughter, sister,...(hoyle et al 2019)lady, nun, heroine, actress, businesswoman,...(zhao et al 2018)baker, counselor, nanny, librarians, socialite,...(zhao et al 2018)seedsromancehistory + biographydistinctions, similarities, friction, parallels, similarity murder, rape, manslaughter, felony, assault.
generated seed set a generated seed set b.coherence1.000mile, miles, yards, yard, feet1.000shop, restaurant, kitchen, cafe, store1.000......0.711 ambush, bombardment, escalation, altercation, militiamenentrance, terrace, subway, cafe, lawn0.689sticks, onions, tops, banana, mozzarella0.552.example, instance, purposes, explanation, shorthandsports, soccer, football, competitions, basketball...corruption, terrorism, graft, bribery, abusescourtside, bamboo, freeway, shorts, sailboatpotatoes, onions, lemon, herbs, meats.
coherence0.9330.9100.909...0.3750.1100.050.gathered seed set a gathered seed set b.career: executive, management, professional...asian: asian, asian, asian, asia, china....family: home, parents, children, family, cousins...caucasian: caucasian, caucasian, white, america....female: sister, mother, aunt, grandmother... male: brother, father, uncle, grandfather, son.......female: countrywoman, sororal, witches... male: countryman, fraternal, wizards, manservant........names asian: cho, wong, tang, huang, chu... names chinese: chung, liu, wong, huang, ng....names black: harris, robinson, howard... names white: harris, nelson, robinson....table 4: when two seed sets are more semantically distinct they are more distinguishable in the resulting geometricsubspace.
the top table shows pairs of artiﬁcially generated seed sets, ranked by their coherence for weat in thenyt dataset.
the bottom table shows pairs of seed sets gathered from published papers, ranked by their coherencefor weat in the wikitext dataset.
scores are averaged across 20 bootstrapped samples of the training data, andvalues are rounded; no coherence scores are exactly 1.0. higher coherence scores indicate that the seeds pairs wereprojected farther apart in the bias subspace..7 seed choice affects bias measurement.
8 factors causing instability.
before moving to speciﬁc seed features, we presentsome general results showing the instability of mea-surements using seeds.
figure 2 shows a motivatingexample, in which we imagine a digital humanitiesscholar interested in measuring whether womenare portrayed more negatively in different genresof book reviews.
as in the weat test, each seed isplotted according to its cosine similarity to an aver-aged unpleasantness vector (caliskan et al., 2017).
for some sets, no signiﬁcant difference is visible,while for other sets, there are much larger differ-ences, causing the researcher to draw different con-clusions when comparing biases across datasets..table 4 shows both the generated and gatheredseed sets ordered by their coherence after using theweat method to discover a bias subspace.
theseexamples highlight factors contributing to lowercoherence (e.g., similarity of the seed sets) whichwe discuss in §8.
they also highlight the generaldifﬁculty in constructing seed sets; e.g., as notedby garg et al.
(2018), the ﬁnal row demonstratesthat some u.s. racial categories are not distinguish-able from available census data.
similar challengesarise when seeds do not occur in the target dataset,which is often true for names.
the wide variationin coherence scores, especially for the generatedseeds which are less likely to contain overlappingterms, indicates that different seed sets can havewidely differing “success” for bias measurement..sometimes seeds can reﬂect the curator’s (orcrowd’s) personal biases.
instabilities can also arisefrom the organization of the seeds and seeminglyinnocuous linguistic features.
we describe a se-ries of distinct sources of instability that can beencoded in seed sets and discuss the implicationsof each.
we rely on a combination of literaturereview, qualitative close reading of example seeds,and quantitative tests of seed features.
we iteratedthrough the seeds, ﬂagging problematic sets, andthen manually clustered and labeled the factors thatcould cause instability..our identiﬁed factors can be categorized as deﬁ-nitional factors (reductive deﬁnitions, inclusion ofconfounding concepts), lexical factors (frequency,pos of individual seeds), and set factors (numberand order of seeds, similarity of seed sets)..reductive deﬁnitions.
the seeds can be reduc-tive and essentializing, codifying life experiencesinto traditional categories.
using names as place-holders for concepts like race (nguyen et al., 2014;sen and wasow, 2016) or reducing gender to abinary with two extremes (bolukbasi et al., 2016;caliskan et al., 2017) can create a distorted view ofthe source data.
sometimes these are simplifyingassumptions, made in an effort to measure biasesthat would otherwise go unexamined.
however,these decisions run the risk of further entrenchingthese category deﬁnitions—e.g., see discussions.
1894(a) gender pairs.
(b) social class pairs.
(c) chinese-hispanic name pairs.
figure 3: we replicate previous gender bias results and experiment on other ordered pairs, using the nyt dataset.
the ﬁrst pca component dominates for ordered gender pairs but not for shufﬂed gender pairs (a), while shufﬂingcan produce a component that explains more variance for class (b) and pleasantness (c) pairs.
we ﬁnd similarinstabilities using the pretrained model used in bolukbasi et al.
(2016).
error bars show standard deviation overthe 20 bootstrapped models.
seeds pairs are listed in the appendix..can contain confounding terms (e.g., in table 1, un-pleasant contains “cancer” which in some datasetsmight be more prevalent for certain demographicgroups) or terms from the target group (e.g., domes-tic work includes the gendered terms “mom” and“mum”).
similarly, the seeds can manifest culturalstigmas: for example, including “fat” and “wrin-kled” in an ugliness category (fast et al., 2016)results in a seed set that itself contains stereotypes..these stigmas are harmful and can interact withother demographic features like gender or age (puhland heuer, 2009), and unless their inclusion is in-tentional, they can accidentally inﬂate measure-ments towards certain groups.
predicting all sucherrors is impossible, and there can be cases whereresearchers intentionally include such terms (e.g.,to capture a particular stereotype)—but this shouldbe a conscious decision by each researcher usingthe seeds, and at a minimum, researchers shouldclearly deﬁne their target concepts..lexical factors.
prior work examining seedshas shown that the frequency and part of speech ofseeds can affect the resulting bias measurements.
ethayarajh et al.
(2019) show that the weat testrequires that the paired seeds occur at similar fre-quencies and that seed sets can be manipulated toproduce certain measurements.
brunet et al.
(2019)explore the effects of perturbing the training cor-pus, ﬁnding that (1) second-order neighbors to theseeds can have a strong impact on the bias mea-surement effect size and (2) effects are strongerfor rarer words.
using contextual embeddings, se-doc and ungar (2019) show that different classesof words (e.g., names vs. pronouns) can result indifferent bias subspaces and that sometimes thesesubspaces represent an unintended dimension (e.g.,age instead of gender)..(a) gender pairs.
(b) random pairs.
(c) shufﬂed.
gender pairs.
figure 4: ranking word vectors by cosine similaritywith the top principle component vector for the origi-nal gender seed pairs (a) appears to identify female andmale gendered words much better than random (b).
butshufﬂing the pairing of seed words (c) maintains corre-lation with gender but to a less clear degree.
results areshown for the nyt corpus with a frequency thresholdof 100 and bootstrap resampling..in keyes (2017); larson (2017) for the mistakesand harms that can be caused by mapping namesto genders—and these trade-offs should be evalu-ated and documented.
more broadly, recent workhas critiqued nlp and ml bias research for notsuccessfully connecting with the literature in soci-ology and critical race studies (hanna et al., 2020;blodgett et al., 2020).
engaging with this litera-ture would provide a better foundation for decision-making about seed sets and provide context forfuture researchers..imprecise deﬁnitions.
if the target concept isnot well-deﬁned, the resulting seed terms can betoo broad and include multiple concepts, riskingthe creation of confounded or circular arguments.
similarly, the unexamined use of pre-existing setsand over-reliance on the category labels from priorwork can result in a series of errors.
the seeds.
189512345678910component0.00.20.4explained variancesettingoriginal ordershuffled12345678component0.00.10.20.3explained variancesettingoriginal ordershuffled12345678910component0.00.10.2explained variancesettingoriginal ordershuffledherselfmshershepregnantpitchingbaseballsyndergaardhimselfhis0.500.490.490.410.40-0.36-0.36-0.38-0.39-0.42likelihoodeurozoneincentivedownturnsetbackphotographedtaleshoodgarciadanced0.360.340.340.310.30-0.39-0.41-0.42-0.45-0.59outcomessonfathermotherauntpotentiallymalehoodgarciamd0.260.260.260.260.25-0.19-0.19-0.29-0.29-0.39set size and alignment.
the number of seedsincluded in each set can affect the resulting biassubspace; kozlowski et al.
(2019) ﬁnd small in-creases in performance when using more seed pairs.
the alignment of the seeds in matched sets (i.e., theordering or pairing of seeds in one set with seedsin another set) can also affect the bias subspace.
inthe pca method, each term in one seed set is ex-plicitly linked to a single term in the other seed set.
the speciﬁc alignment between paired words mat-ters; altering the pairing can result in dramaticallydifferent results, even for cases like gender, whichis marked in english.
however, we observe con-scious pairings of seeds only for obvious cases, andsometimes “obvious” pairings produce subspacesthat explain less variance..we replicate a study previously carried out onembeddings trained on internet-scale collections(bolukbasi et al., 2016) using both a large, pre-trained embedding and the relatively small nytdataset.
figure 3 shows how much variance is ex-plained by the ﬁrst ten principal components ofthree difference matrices.
when we use the origi-nal paired male-female seed words from bolukbasiet al.
(2016) (e.g., man-woman, he-she), we seea single dominant ﬁrst component, suggesting astrong male-female axis.
as previously reported,the variances fall off gradually when the seeds are aset of random words.
when we shufﬂe the order ofthe seed words, the drop off is steeper than for ran-dom pairs, but there is no longer a single dominantprincipal component..similarly, figure 4 shows that when we used theordered gender pairs, the ranked words roughly di-vide into groups correlated with gender, while if weuse shufﬂed pairs, the lists of high and low rankedwords are not as easily distinguishable as masculineor feminine.
we ﬁnd an opposite effect social classpairs (kozlowski et al., 2019); when we shufﬂe, weﬁnd a subspace that explains more variance than theexplicitly ordered pairs (e.g., “richest”-“poorest”).
we ﬁnd similar differences when testing some seedsets that lack intuitive pairings, e.g., the matchedpleasantness and unpleasantness seeds (caliskanet al., 2017) and the matched christianity and islamseeds (garg et al., 2018)..order does not always affect the subspace—e.g,we found no signiﬁcant difference when shufﬂingsets of names—but we have shown that it can af-fect the subspace, and so to build conﬁdence inmeasurements, testing is required..figure 5: identifying bias is less effective when setpairs are similar.
generated seeds are frequency-controlled nouns from the wikitext dataset.
we high-light two sets of gathered seeds; both target similarracial categories but the name-based sets are more simi-lar and explain less variance.
we ﬁnd similar trends forweat, coherence, and the other corpora and pos..set similarity.
by sampling random seed setswe ﬁnd that it is more difﬁcult to represent thevariance of seed sets that are too close together.
figure 5 shows that set similarity (cosine similar-ity between the set mean vectors) is signiﬁcantlycorrelated with explained variance for generatedsets (pearson r = −0.67, p < 0.05).
we highlighttwo comparisons between gathered sets intended tomeasure racial bias that explain different degreesof variance.
synthetic pairings generally explainmore variance than pairings of gathered sets ofequal similarity, although for gathered sets we can-not control for pos and frequency.
table 4 showsthe generated seed sets ranked by coherence, wherehigher scores indicate that the bias subspace wasable to separate the seed sets.
similar seed sets andsets with duplicates (e.g., the pairing in the tablein which both generated sets contain food terms)have low coherence scores..9 conclusion: biases all the way down.
almost all recent work on bias measurement relieson sets of seed terms to ground cultural concepts inlanguage.
if we do not pay attention to the seeds,these methods will lack foundation and the claimsthey support will be left open to criticism and dis-missal.
seeds and their rationales need to be testedand documented, rather than hidden in code orcopied without examination..some of the risks discussed in this paper mayseem obvious in retrospect, but our literature surveysuggests there are widely varying levels of evalu-ation and documentation.
rationales for picking.
1896−0.20.00.20.40.60.8set similarity0.20.40.60.8explained varianceblack vswhite namesblack vswhite rolessourcegeneratedgatheredsources or seeds are not always explained, or thereader is left to assume that prior work has ade-quately validated the seeds.
tests for frequency,semantic similarity, and other features are rare ornon-existent, and clear deﬁnitions and discussion oflimitations are often missing.
permutation tests aresometimes used, but these do not account for seedsoutside of those already selected.
signiﬁcantly dif-ferent results can be found using alternative seedssets for the same target concept, and ﬁne-grainedcomparisons require validation on multiple sets..we faced a number of challenges in gathering178 seed sets from prior work.
sometimes seedsare shared online at an undocumented location andsometimes hard-coded into code repositories; thiscan signiﬁcantly obscure the seeds from publicview, which is troubling for tools intended for wideuse on sensitive topics.
documentation is oftenscattered across locations, and in more than onecase, we found contradictions between differentsources for a single project.
in one case, we wereunable to ﬁnd the full list of seeds used in the paper,and in several cases, it was unclear which seed setswere used for which experiments.
while someauthors went to commendable lengths to documenttheir materials, there is a need for more consistentand transparent documentation..we recommend that researchers carefully tracethe origins of seed sets, with attention to therisks associated with the origin type.
we also rec-ommend that researchers examine seed features.
pos, frequency, semantic similarity, and pairingorder can signiﬁcantly affect the results of bias mea-surements.
seeds should be both examined man-ually and tested as shown in §8; importantly, theyshould be compared to alternative seeds with dif-ferent attributes, as in §7.
to assist this we releasea compilation of 178 seed sets from prior work.
these tests are particularly important when com-paring biases across datasets.
finally, researchersshould document all seeds and the rationales un-derlying their design, including concept deﬁnitions.
we add to recent calls for better documentation andproblem speciﬁcation in machine learning (benderand friedman, 2018; gebru et al., 2018; mitchellet al., 2019; blodgett et al., 2020) and in studies ofsocial biases in technology (olteanu et al., 2019).
speciﬁcally, when the seeds intentionally encodeharmful stereotypes or slurs, it can be beneﬁcial toinclude a trigger warning or not highlight the seedsin the paper; however, full seed lists should always.
be accessible, not hard-coded, with unique labelsmatched to experiments..ultimately, our goal is not to eliminate a prob-lem but to illuminate it:2 to help practitioners thinkthrough the potential risks posed by seed sets usedfor bias detection.
we encourage thoughtful, criti-cal studies, but we observe a trend in which seedsets are used in new research and applications sim-ply because they have been used in prior publishedwork, without additional vetting.
research prece-dents can take on a life of their own and we havea responsibility to explore and document possiblesources of error.
we believe that seed sets can beuseful and are probably unavoidable, but that notechnical tool can absolve researchers from the dutyto choose seeds carefully and intentionally..acknowledgements.
thank you to our anonymous reviewers whosecomments substantially inﬂuenced and improvedthis paper.
thank you to rishi bommasani, for-rest davis, os keyes, lauren kilgour, rosamundthalken, marten van schijndel, melanie walsh,and gregory yauney for their many helpful sugges-tions.
this work was funded through nsf grant#1652536..references.
emily m. bender and batya friedman.
2018. datastatements for natural language processing: towardmitigating system bias and enabling better science.
transactions of the association for computationallinguistics, 6:587–604..sudeep bhatia, geoffrey p goodwin, and lukaszwalasek.
2018. trait associations for hillary clin-ton and donald trump in news media: a computa-tional analysis.
social psychological and personal-ity science, 9(2):123–130..su lin blodgett, solon barocas, hal daum´e iii, andhanna wallach.
2020. language (technology) ispower: a critical survey of “bias” in nlp.
in pro-ceedings of the 58th annual meeting of the asso-ciation for computational linguistics, pages 5454–5476, online.
association for computational lin-guistics..tolga bolukbasi, kai-wei chang, james y zou,venkatesh saligrama, and adam t kalai.
2016.man is to computer programmer as woman is to.
2“all problems can be illuminated; not all problems canbe solved.”—ursula franklin (quoted by m. meredith viaolteanu et al.
(2019) in http://bb9.berlinbiennale.de/all-problems-can-be-illuminated-not-all-problems-can-be-solved/).
1897in ad-homemaker?
debiasing word embeddings.
vances in neural information processing systems,pages 4349–4357..margaret m bradley and peter j lang.
1999. affectivenorms for english words (anew): instruction manualand affective ratings.
technical report.
the cen-ter for research in psychophysiology, university offlorida..marc-etienne brunet, colleen alkalay-houlihan, ash-ton anderson, and richard zemel.
2019. under-standing the origins of bias in word embeddings.
in international conference on machine learning,pages 803–811..aylin caliskan,.
and arvindjoanna j bryson,narayanan.
2017. semantics derived automaticallyfrom language corpora contain human-like biases.
science, 356(6334):183–186..kawin ethayarajh, david duvenaud, and graeme hirst.
2019. understanding undesirable word embeddingin proceedings of the 57th annualassociations.
meeting of the association for computational lin-guistics, pages 1696–1705, florence, italy.
associa-tion for computational linguistics..ethan fast, binbin chen, and michael s. bernstein.
2016. empath: understanding topic signals in large-scale text.
in proceedings of the 2016 chi confer-ence on human factors in computing systems, chi’16, page 4647–4657, new york, ny, usa.
associ-ation for computing machinery..patrick s forscher, calvin k lai, jordan r axt,charles r ebersole, michelle herman, patricia gdevine, and brian a nosek.
2017. a meta-analysisof change in implicit bias.
psychological bulletin..nikhil garg, londa schiebinger, dan jurafsky, andjames zou.
2018. word embeddings quantify100 years of gender and ethnic stereotypes.
pro-ceedings ofthe national academy of sciences,115(16):e3635–e3644..timnit gebru, jamie morgenstern, briana vecchione,jennifer wortman vaughan, hanna wallach, haldaum´e iii, and kate crawford.
2018. datasheets fordatasets.
proceedings of the 5th workshop on fair-ness, accountability, and transparency in machinelearning, pmlr..hila gonen and yoav goldberg.
2019. lipstick on apig: debiasing methods cover up systematic genderbiases in word embeddings but do not remove them.
in proceedings of the 2019 conference of the northamerican chapter of the association for computa-tional linguistics: human language technologies,volume 1 (long and short papers), pages 609–614,minneapolis, minnesota.
association for computa-tional linguistics..anthony g greenwald, debbie e mcghee, and jor-dan lk schwartz.
1998. measuring individual dif-ferences in implicit cognition: the implicit associa-tion test.
journal of personality and social psychol-ogy, 74(6):1464..william l. hamilton, jure leskovec, and dan jurafsky.
2016. diachronic word embeddings reveal statisti-cal laws of semantic change.
in proceedings of the54th annual meeting of the association for compu-tational linguistics (volume 1: long papers), pages1489–1501, berlin, germany.
association for com-putational linguistics..alex hanna, emily denton, andrew smart, and jamilasmith-loud.
2020. towards a critical race method-ology in algorithmic fairness.
in proceedings of the2020 conference on fairness, accountability, andtransparency, fat* ’20, page 501–512, new york,ny, usa.
association for computing machinery..matthew honnibal,.
ines montani, soﬁe van lan-deghem,spacy:and adriane boyd.
2020.industrial-strength natural language processing inpython..alexander miserlis hoyle, lawrence wolf-sonkin,hanna wallach, isabelle augenstein, and ryan cot-terell.
2019. unsupervised discovery of genderedlanguage through latent-variable modeling.
in pro-ceedings of the 57th annual meeting of the asso-ciation for computational linguistics, pages 1706–1716, florence, italy.
association for computationallinguistics..minqing hu and bing liu.
2004. mining and sum-in proceedings of themarizing customer reviews.
tenth acm sigkdd international conference onknowledge discovery and data mining, kdd ’04,page 168–177, new york, ny, usa.
association forcomputing machinery..kenneth joseph, wei wei, and kathleen m carley.
2017. girls rule, boys drool: extracting seman-in pro-tic and affective stereotypes from twitter.
ceedings of the 2017 acm conference on computersupported cooperative work and social computing,pages 1362–1374.
acm..os keyes.
2017..stop mapping names to gender.
https://ironholds.org/names- gender/.
accessed: 2021-05-26..yoon kim, yi-i chiu, kentaro hanaki, darshan hegde,and slav petrov.
2014. temporal analysis of lan-guage through neural language models.
in proceed-ings of the acl 2014 workshop on language tech-nologies and computational social science, pages61–65, baltimore, md, usa.
association for com-putational linguistics..markus knoche, radomir popovi´c, florian lem-merich, and markus strohmaier.
2019a.
identifyingbiases in politically biased wikis through word em-beddings.
in proceedings of the 30th acm confer-ence on hypertext and social media, ht ’19, pages253–257, new york, ny, usa.
acm..1898markus knoche, radomir popovi´c, florian lem-merich, and markus strohmaier.
2019b.
identifyingbiases in politically biased wikis through word em-beddings.
in proceedings of the 30th acm confer-ence on hypertext and social media, pages 253–257.
acm..austin c kozlowski, matt taddy, and james a evans.
2019. the geometry of culture: analyzing the mean-ings of class through word embeddings.
americansociological review, 84(5):905–949..vivek kulkarni, bryan perozzi, and steven skiena.
2016. freshman or fresher?
quantifying the geo-graphic variation of language in online social media.
proceedings of the international aaai conferenceon web and social media, 10(1)..brian larson.
2017. gender as a variable in natural-language processing: ethical considerations.
in pro-ceedings of the first acl workshop on ethics innatural language processing, pages 1–11, valencia,spain.
association for computational linguistics..thomas manzini, lim yao chong, alan w black,and yulia tsvetkov.
2019. black is to criminalas caucasian is to police: detecting and removingin proceed-multiclass bias in word embeddings.
ings of the 2019 conference of the north americanchapter of the association for computational lin-guistics: human language technologies, volume 1(long and short papers), pages 615–621, minneapo-lis, minnesota.
association for computational lin-guistics..stephen merity, caiming xiong, james bradbury, andrichard socher.
2016. pointer sentinel mixture mod-els..tomas mikolov, ilya sutskever, kai chen, greg s cor-rado, and jeff dean.
2013. distributed representa-tions of words and phrases and their compositional-ity.
in advances in neural information processingsystems, pages 3111–3119..margaret mitchell, simone wu, andrew zaldivar,parker barnes, lucy vasserman, ben hutchinson,elena spitzer, inioluwa deborah raji, and timnitgebru.
2019. model cards for model reporting.
inproceedings of the conference on fairness, account-ability, and transparency, fat* ’19, page 220–229,new york, ny, usa.
association for computingmachinery..dong nguyen, dolf trieschnigg, a. seza do˘gru¨oz, ri-lana gravel, mari¨et theune, theo meder, and fran-ciska de jong.
2014. why gender and age predic-tion from tweets is hard: lessons from a crowd-in proceedings of colingsourcing experiment.
2014, the 25th international conference on compu-tational linguistics: technical papers, pages 1950–1961, dublin, ireland.
dublin city university andassociation for computational linguistics..alexandra olteanu, carlos castillo, fernando diaz,social data: bi-and emre kıcıman.
2019.ases, methodological pitfalls, and ethical boundaries.
frontiers in big data, 2:13..galen panger.
2016. reassessing the facebook experi-ment: critical thinking about the validity of big datainformation, communication & society,research.
19(8):1108–1126..james w pennebaker, martha e francis, and roger jbooth.
2001. linguistic inquiry and word count:liwc 2001. mahway: lawrence erlbaum asso-ciates, 71(2001):2001..lawrence phillips, kyle shaffer, dustin arendt,nathan hodas, and svitlana volkova.
2017. intrin-sic and extrinsic evaluation of spatiotemporal textrepresentations in twitter streams.
in proceedingsof the 2nd workshop on representation learning fornlp, pages 201–210..rebecca m puhl and chelsea a heuer.
2009. thestigma of obesity: a review and update.
obesity,17(5):941–964..radim ˇreh˚uˇrek and petr sojka.
2010. software frame-work for topic modelling with large corpora.
inproceedings of the lrec 2010 workshop on newchallenges for nlp frameworks, pages 45–50, val-letta, malta.
elra.
http://is.muni.cz/publication/884893/en..rachel rudinger, chandler may,.
and benjaminvan durme.
2017. social bias in elicited natural lan-guage inferences.
in proceedings of the first aclworkshop on ethics in natural language process-ing, pages 74–79, valencia, spain.
association forcomputational linguistics..diego saez-trumper, carlos castillo, and mounia lal-mas.
2013. social media news communities: gate-in pro-keeping, coverage, and statement bias.
ceedings of the 22nd acm international conferenceon information & knowledge management, pages1679–1684.
acm..jo˜ao sedoc and lyle ungar.
2019. the role of pro-tected class word lists in bias identiﬁcation of con-textualized word representations.
in proceedings ofthe first workshop on gender bias in natural lan-guage processing, pages 55–61, florence, italy.
as-sociation for computational linguistics..maya sen and omar wasow.
2016. race as a bundleof sticks: designs that estimate effects of seeminglyimmutable characteristics.
annual review of politi-cal science, 19..chris sweeney and maryam najaﬁan.
2019. a trans-parent framework for evaluating unintended demo-graphic bias in word embeddings.
in proceedings ofthe 57th annual meeting of the association for com-putational linguistics, pages 1662–1667, florence,italy.
association for computational linguistics..1899mengting wan and julian j. mcauley.
2018..itemrecommendation on monotonic behavior chains.
inproceedings of the 12th acm conference on rec-ommender systems, recsys 2018, vancouver, bc,canada, october 2-7, 2018, pages 86–94.
acm..mengting wan, rishabh misra, ndapa nakashole, andjulian j. mcauley.
2019. fine-grained spoiler de-in pro-tection from large-scale review corpora.
ceedings of the 57th conference of the associationfor computational linguistics, acl 2019, florence,italy, july 28- august 2, 2019, volume 1: long pa-pers, pages 2605–2610.
association for computa-tional linguistics..john e williams and susan m bennett.
1975. the def-inition of sex stereotypes via the adjective check list.
sex roles, 1(4)..john e williams and deborah l best.
1977. sex stereo-types and trait favorability on the adjective check list.
educational and psychological measurement..john e williams and deborah l best.
1990. measur-ing sex stereotypes: a multination study, rev.
sagepublications, inc..jieyu zhao, tianlu wang, mark yatskar, ryan cot-terell, vicente ordonez, and kai-wei chang.
2019.gender bias in contextualized word embeddings.
inproceedings of the 2019 conference of the northamerican chapter of the association for computa-tional linguistics: human language technologies,volume 1 (long and short papers), pages 629–634,minneapolis, minnesota.
association for computa-tional linguistics..jieyu zhao, yichao zhou, zeyu li, wei wang, and kai-wei chang.
2018. learning gender-neutral wordembeddings.
in proceedings of the 2018 conferenceon empirical methods in natural language process-ing, pages 4847–4853, brussels, belgium.
associa-tion for computational linguistics..1900a appendix.
a.1 datasets.
new york times.
this dataset contains 165,900paragraphs from articles published between april15th and june 30th, 2016.3 the articles are drawnfrom all sections of the english language news, in-cluding movies, sports, technology, world, u.s.,arts, business, books, ny region, health, sci-ence, and fashion.
this dataset is small in compar-ison to the large training datasets used for down-stream features; its scope ﬁts the use case of asocial scientist interested in measuring bias duringa small time window at a particular publication..wikitext.
the wikitext training corpus con-tains the texts of 28,000 manually veriﬁed high-quality articles from wikipedia.org (merity et al.,2016).
lists have been removed, along with htmlerrors, math, and code.
we use the full trainingdataset, wikitext-103.4 this dataset is much largerthan the nyt dataset but is still of focused inter-est in a particular online community (wikipediaauthors)..goodreads.
we sample 500 goodreads book re-views for books in the romance and history andbiography genres, removing books with fewer than500 reviews and reviews containing fewer than 20characters.
we use the provided genre samplesfrom the ucsd book graph (wan and mcauley,2018; wan et al., 2019).5.google news.
for some of our experiments, asa comparison for the smaller datasets, we use amodel pre-trained on part of the google newsdataset.6 this is a popular model, used in boluk-basi et al.
(2016) and many other studies.
thisdata originates from an internal google dataset(mikolov et al., 2013), and we could not ﬁnd acomprehensive description of the data beyond itsvocabulary size: 3 million unique words and 100billion tokens..3https://www.kaggle.com/nzalake52/new.
-york-times-articles.
4https://blog.einstein.ai/the-wikitext-long-term-dependency\-language-modeling-dataset/.
5https://sites.google.com/eng.ucsd.ed.
u/ucsdbookgraph/home.
6https://code.google.com/archive/p/wo.
rd2vec/.
a.2 seed terms.
because of the appendix page limit, we cannot listhere all the seed sets gathered from prior work.
in-stead, the full seed sets in addition to the rationalesand sources used for their curation are released asa supplementary json ﬁle.
after publication, theseeds will also be documented at a public website.
below, we list all the seeds used as examples (inﬁgures or text) in the main paper.
the seed ids cor-respond to a matching id ﬁeld in the supplementaryjson ﬁle..table 1.
• seeds id:.
unpleasant-caliskan et al 2017used in: caliskan et al.
(2017)seeds:[abuse, crash, ﬁlth, murder, sick-ness, accident, death, grief, poison, stink, as-sault, disaster, hatred, pollute, tragedy, di-vorce, jail, poverty, ugly, cancer, kill, rotten,vomit, agony, prison].
• seeds id:.
african american names-caliskan et al 2017used in: caliskan et al.
(2017)seeds:[alonzo, jamel, theo, alphonse,jerome, leroy, torrance, darnell, lamar, li-onel, tyree, deion, lamont, malik, terrence,tyrone, lavon, marcellus, wardell, nichelle,shereen, ebony, latisha, shaniqua, jasmine,tanisha, tia, lakisha, latoya, yolanda,malika, yvette].
• seeds id:.
domestic work-fast et al 2016used in: fast et al.
(2016)seeds:[chore, mom, vacuum, scrubbing,cook, washing, baking, wash, morning, meal,house, chef, laundry, bake, organizing, cook-ing, spotless, mum, washer, remodeling, par-ent, job, nanny, kitchen, dishwasher, clean-ing, family, cleaner, bathroom, errand, sit-ter, housekeeper, serve, housekeeping, tidy,cleaned, housework, scrub, organize, home,clean].
• seeds id:.
ugliness-fast et al 2016used in: fast et al.
(2016)seeds:.
[despise, balding,.
slimy, acne,.
1901• used in: caliskan et al.
(2017).
• used in: garg et al.
(2018).
grotesque, degrading, horrible, fat, diseased,repulsive, awful, nasty, brutish, grotesquely,distasteful, unworthy, scruffy, chubby, gross,insulting, crooked, revolting, unappealing,hairy, pathetic, cockroach, abnormally, un-sightly, crippled, lousy, wrinkled, freakish,disﬁgured, disgusting, pudgy, tacky, obese,disgust, degrade, horrid, deformed, hideous,bloated, ugly, scum, demeaning, pig, obnox-ious, blob, wart, disgraceful, fatty, bald, over-weight, disgusted, unattractive, wrinkle, ﬁlthy,loathsome].
table 4.seeds id 1:career-caliskan et al 2017seeds id 2:family-caliskan et al 2017seeds 1:[executive, management, profes-sional, corporation, salary, ofﬁce, business,career]seeds 2: [home, parents, children, family,cousins, marriage, wedding, relatives].
• used in: manzini et al.
(2019).
seeds id 1:asian-manzini et al 2019seeds id 2:caucasian-manzini et al 2019seeds 1: [asian, asian, asian, asia, china, asia]seeds 2: [caucasian, caucasian, white, amer-ica, america, europe].
• used in: caliskan et al.
(2017).
seeds id 1:female 2-caliskan et al 2017seeds id 2:male 2-caliskan et al 2017seeds 1: [sister, mother, aunt, grandmother,daughter, she, hers, her]seeds 2: [brother, father, uncle, grandfather,son, he, his, him].
• used in: zhao et al.
(2018).
seeds id 1:female deﬁnition words 1-zhao et al 2018seeds id 2:male deﬁnition words 1-zhao et al 2018seeds 1: [countrywoman, sororal, witches,maidservant, mothers, diva, actress, spinster,mama, duchesses, barwoman, countrywomen,.
dowry, hostesses, airwomen, menopause, cli-toris, princess, governesses, abbess, women,widow, ladies, sorceresses, madam, brides,baroness, housewives, godesses, niece, wid-ows, lady, sister... (see supplementary materi-als for full list)]seeds 2:[countryman, fraternal, wizards,manservant, fathers, divo, actor, bachelor,papa, dukes, barman, countrymen, brideprice,hosts, airmen, andropause, penis, prince, gov-ernors, abbot, men, widower, gentlemen, sor-cerers, sir, bridegrooms, baron, househus-bands, gods, nephew, widowers, lord, brother,(see supplementary materials for full list)].
seeds id 1:names asian-garg et al 2018seeds id 2:names chinese-garg et al 2018seeds 1: [cho, wong, tang, huang, chu, chung,ng, wu, liu, chen, lin, yang, kim, chang, shah,wang, li, khan, singh, hong]seeds 2: [chung, liu, wong, huang, ng, hu,chu, chen, lin, liang, wang, wu, yang, tang,chang, hong, li].
• used in: garg et al.
(2018).
seeds id 1:names black-garg et al 2018seeds id 2:names white-garg et al 2018seeds 1: [harris, robinson, howard, thompson,moore, wright, anderson, clark, jackson, tay-lor, scott, davis, allen, adams, lewis, williams,jones, wilson, martin, johnson]seeds 2: [harris, nelson, robinson, thompson,moore, wright, anderson, clark, jackson, tay-lor, scott, davis, allen, adams, lewis, williams,jones, wilson, martin, johnson].
figure 2.
• seeds id:.
female-kozlowski et al 2019seeds: [woman, women, she, her, her, hers,girl, girls, female, feminine].
• seeds id:.
female 1-caliskan et al 2017seeds: [sister, female, woman, girl, daughter,she, hers, her].
• seeds id:.
deﬁnitional female-bolukbasi et al 2016.
1902seeds: [woman, girl, she, mother, daughter,gal, female, her, herself, mary].
• seeds 2 id:.
lowerclass-kozlowski et al 2019.
• seeds id:.
female singular-hoyle et al 2019seeds: [woman, girl, mother, daughter, sister,wife, aunt, niece, empress, queen, princess,duchess, lady, dame, waitress, actress, god-dess, policewoman, postwoman, heroine,witch, stewardess, she].
• seeds id:.
female deﬁnition words 2-zhao et al 2018seeds: [lady, saleswoman, noblewoman, host-ess, coquette, nun, heroine, actress, chair-woman, businesswoman, spokeswoman, wait-ress, councilwoman, stateswoman, police-woman, countrywomen, horsewoman, head-mistress, governess, widow, witch, ﬁancee].
• seeds id:.
female stereotype words-zhao et al 2018seeds: [baker, counselor, nanny, librarians,socialite, assistant, tailor, dancer, hairdresser,cashier, secretary, clerk, stenographer, op-tometrist, housekeeper, bookkeeper, home-maker, nurse, stylist, receptionist].
figure 3 (a).
• used in: bolukbasi et al.
(2016).
deﬁnitional female-bolukbasi et al 2016.
• seeds 1 id:.
• seeds 2 id:.
deﬁnitional male-bolukbasi et al 2016.
• seeds 1: [she, her, woman, mary, herself,.
daughter, mother, gal, girl, female].
• seeds 1: [rich, richer, richest, afﬂuence, afﬂu-.
ent, expensive, luxury, opulent].
• seeds 2: [poor, poorer, poorest, poverty, im-.
poverished, inexpensive, cheap, needy].
• seeds 1 shufﬂed: [richer, opulent, luxury,afﬂuent, rich, afﬂuence, richest, expensive].
• seeds 2 shufﬂed:.
[poorer, impoverished,poorest, cheap, needy, poverty, inexpensive,poor].
figure 3 (c).
• used in: garg et al.
(2018).
• seeds 1 id:.
names chinese-garg et al 2018.
• seeds 2 id:.
names hispanic-garg et al 2018.
• seeds 1: [chung, liu, wong, huang, ng, hu,chu, chen, lin, liang, wang, wu, yang, tang,chang, hong, li].
• seeds 2:.
[ruiz, alvarez, vargas, castillo,gomez, soto, gonzalez, sanchez, rivera, men-doza, martinez, torres, rodriguez, perez, lopez,medina, diaz, garcia, castro, cruz].
• seeds 1 shufﬂed: [tang, chang, chu, yang,wu, hong, huang, wong, hu, liu, lin, chen,liang, chung, li, ng, wang].
• seeds 2 shufﬂed:.
[ruiz, rodriguez, diaz,perez, lopez, vargas, alvarez, garcia, cruz, tor-res, gonzalez, soto, martinez, medina, rivera,castillo, castro, mendoza, sanchez, gomez].
• seeds 2: [he, his, man, john, himself, son,.
father, guy, boy, male].
figure 4 (a).
• seeds 1 shufﬂed: [herself, woman, daughter,.
mary, her, girl, mother, she, female, gal].
• seeds 1 id:.
• seeds 2 shufﬂed: [man, his, he, son, guy,.
himself, father, boy, male, john].
• seeds 2 id:.
• used in: bolukbasi et al.
(2016).
deﬁnitional female-bolukbasi et al 2016.figure 3 (b).
• used in: kozlowski et al.
(2019).
deﬁnitional male-bolukbasi et al 2016.
• seeds 1: [she, her, woman, mary, herself,.
daughter, mother, gal, girl, female].
• seeds 1 id:.
• seeds 2: [he, his, man, john, himself, son,.
upperclass-kozlowski et al 2019.father, guy, boy, male].
1903figure 4 (b).
• used in: n/a (random seeds).
• seeds 1 id: n/a.
• seeds 2 id: n/a.
shaniqua, tameisha, teretha, jasmine, latonya,shanise, tanisha, tia, lakisha, latoya, sharise,tashika, yolanda, lashandra, malika, shavonn,tawanda, yvette, hakim, jermaine, kareem, ja-mal, rasheed, aisha, keisha, kenya, tamika].
• seeds 1: [negatives, vel, theirs, canoe, meet,.
bilingual, mor, facets, fari, lily].
• seeds 2: [chun, brush, dictates, caesar, fewest,.
breitbart, rod, heaped, julianna, longest].
figure 5 (black vs white roles).
• used in: manzini et al.
(2019).
• seeds 1 id:.
black roles-manzini et al 2019.
• seeds 2 id:.
caucasian roles-manzini et al 2019.
• seeds 1: [slave, musician, runner, criminal,.
homeless].
• seeds 2: [manager, executive, redneck, hill-.
billy, leader, farmer].
figure 4 (c).
• used in: bolukbasi et al.
(2016).
deﬁnitional female-bolukbasi et al 2016.
• seeds 1 id:.
• seeds 2 id:.
deﬁnitional male-bolukbasi et al 2016.
• shufﬂed seeds 1: [female, she, woman, gal,her, daughter, girl, herself, mother, mary].
• shufﬂed seeds 2: [john, man, son, father,.
male, himself, guy, he, his].
figure 5 (black vs white names).
• used in: knoche et al.
(2019b).
• seeds 1 id:.
white names-knoche et al 2019.
• seeds 2 id:.
black names-knoche et al 2019.
• seeds 1:.
[adam, chip, harry, josh, roger,alan, frank, ian, justin, ryan, andrew, fred,jack, matthew, stephen, brad, greg, jed, paul,todd, brandon, hank, jonathan, peter, wilbur,amanda, courtney, heather, melanie, sara, am-ber, crystal, katie, meredith, shannon, betsy,donna, kristin, nancy, stephanie, bobbie-sue,ellen, lauren, peggy, sue-ellen, colleen, emily,megan, rachel, wendy, brendan, geoffrey,brett, jay, neil, anne, carrie, jill, laurie, kristen,sarah].
• seeds 2: [alonzo, jamel, lerone, percell, theo,alphonse, jerome, leroy, rasaan, torrance, dar-nell, lamar, lionel, rashaun, tyree, deion, la-mont, malik, terrence, tyrone, everol, lavon,marcellus, terryl, wardell, aiesha, lashelle,latisha,nichelle, shereen,.
temeka, ebony,.
1904