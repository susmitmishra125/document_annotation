interpretable and low-resource entity matchingvia decoupling feature learning from decision making.
zijun yao1,2 chengjiang li1,2 tiansi dong3 xin lv1,2.
jifan yu1,2.
lei hou1,2âˆ—.
juanzi li1,2.
yichi zhang4.
zelin dai4.
1department of computer science and technology, bnrist;2kirc, institute for artiï¬cial intelligencetsinghua university, beijing 100084, china3b-it, university of bonn, germany4alibaba group, hangzhou, china{yaozj20@mails., houlei@}tsinghua.edu.cndongt@bit.uni-bonn.de.
abstract.
entity matching (em) aims at recognizing en-tity records that denote the same real-world ob-ject.
neural em models learn vector represen-tation of entity descriptions and match entitiesend-to-end.
though robust, these methods re-quire many annotated resources for training,and lack of interpretability.
in this paper, wepropose a novel em framework that consists ofheterogeneous information fusion (hif) andkey attribute tree (kat) induction to decou-ple feature representation from matching deci-sion.
using self-supervised learning and maskmechanism in pre-trained language modeling,hif learns the embeddings of noisy attributevalues by inter-attribute attention with unla-beled data.
using a set of comparison fea-tures and a limited amount of annotated data,kat induction learns an efï¬cient decision treethat can be interpreted by generating entitymatching rules whose structure is advocatedby domain experts.
experiments on 6 pub-lic datasets and 3 industrial datasets show thatour method is highly efï¬cient and outperformssota em models in most cases.
our codesand datasets can be obtained from https://github.com/thu-keg/hif-kat..1.introduction.
entity matching (em) aims at identifying whethertwo records from different sources refer to the samereal-world entity.
this is a fundamental researchtask in knowledge graph integration (dong et al.,2014; daniel et al., 2020; christophides et al., 2015;christen, 2012) and text mining (zhao et al., 2014).
in real applications, it is not easy to decide whethertwo records with ad hoc linguistic descriptions referto the same entity.
in figure 1, e2 and e3 refer tothe same publication, while e1 refers to a differentâˆ— corresponding to l.hou (houlei@tsinghua.edu.cn).
figure 1: published papers as entity records..one.
venues of e2 and e3 have different expressions;authors of e3 is misplaced in its title ï¬eld..early works include feature engineering (wanget al., 2011) and rule matching (singh et al., 2017;fan et al., 2009).
recently, the robustness of en-tity matching has been improved by deep learningmodels, such as distributed representation basedmodels (ebraheem et al., 2018), attention basedmodels (mudgal et al., 2018; fu et al., 2019, 2020),and pre-trained language model based models (liet al., 2020).
nevertheless, these modern neuralem models suffer from two limitations as follows.
low-resource training.
supervised deep learn-ing em relies on large amounts of labeled train-ing data, which is extremely costly in reality.
at-tempts have been made to leverage external datavia transfer learning (zhao and he, 2019; thirumu-ruganathan et al., 2018; kasai et al., 2019; losteret al., 2021) and pre-trained language model basedmethods (li et al., 2020).
other attempts havealso been made to improve labeling efï¬ciency viaactive learning (nafa et al., 2020) and crowdsourc-ing techniques (gokhale et al., 2014; wang et al.,2012).
however, external information may intro-duce noises, and active learning and crowdsourcingstill require additional labeling work.
lack of interpretability.
it is important to knowwhy two entity records are equivalent (chen et al.,2020), however, deep learning em lacks inter-.
proceedingsofthe59thannualmeetingoftheassociationforcomputationallinguisticsandthe11thinternationaljointconferenceonnaturallanguageprocessing,pages2770â€“2781august1â€“6,2021.Â©2021associationforcomputationallinguistics2770titleauthorvenueconference (redundant)ğ‘’!data mining techniquesmissingsigmod conferenceinternational conference on management of datağ‘’"data mining: concepts and techniquesj.
han, j. pei, m. kambersigmod recordmissingğ‘’#data mining: concepts & techniques by jiawei hanmisplacedacm sigmod recordmissingpretability.
though some neural em models an-alyze the model behavior from the perspective ofattention (nie et al., 2019), attention is not a safeindicator for interpretability (serrano and smith,2019).
deep learning em also fails to generateinterpretable em rules in the sense that they meetthe criteria by domain experts (fan et al., 2009)..to address the two limitations, we propose anovel em framework to decouple feature represen-tation from matching decision.
our framework con-sists of heterogeneous information fusion (hif)and key attribute tree (kat) matching decisionfor low-resource settings.
hif is robust for featurerepresentation from noisy inputs, and kat carriesout interpretable decisions for entity matching..in particular, hif learns from unlabeled data amapping function, which converts each noisy at-tribute value of entity into a vector representation.
this is carried out by a novel self-supervised at-tention training schema to leverage the redundancywithin attribute values and propagate informationacross attributes..kat matching decision learns kat using deci-sion tree classiï¬cation.
after training, kat carriesout entity matching as a task of the classiï¬cationtree.
for each entity pair, it ï¬rst computes multiplesimilarity scores for each attribute using a familyof metrics and concatenates them into a compari-son feature vector.
this classiï¬cation tree can bedirectly interpreted as em rules that share a similarstructure with em rules derived by domain experts.
our em method achieves at least sota perfor-mance on 9 datasets (3 structured datasets, 3 dirtydatasets, and 3 industrial datasets) under variousextremely low-resource settings.
moreover, whenthe number of labeled training data decreases from60% to 10%, our method achieves almost the sameperformance.
in contrast, other methodsâ€™ perfor-mances decrease greatly..the rest of the paper is structured as follows.
section 2 deï¬nes the em task; section 3 presentshif and kat-induction in details; section 4 reportsa series of comparative experiments that show therobustness and the interpretability our methods inlow-resource settings; section 5 lists some relatedworks; section 6 concludes the paper..ues of entity record e as e[ai].
entity match-ing aims to determine whether e1 and e2 refer tothe same real-world object or not.
formally, en-tity matching is viewed as a binary classiï¬cationfunction t1 Ã— t2 â†’ {t rue, f alse} that takes(e1, e2) âˆˆ t1 Ã— t2 as input, and outputs t rue(f alse), if e1 and e2 are matched (not matched)..current neural em approaches simultaneouslyembed entities in low-dimensional vector spacesand obtain entity matching by computations ontheir vector representations.
supervised deep learn-ing em relies on large amounts of labeled trainingdata, which is time-consuming and needs costlymanual efforts.
large unlabelled data also containentity feature information useful for em, yet hasnot been fully exploited by the existing neural emmethods.
in this paper, we aim at decoupling fea-ture representation from matching decision.
ournovel em model consists of two sub-tasks: learn-ing feature representation from unlabeled data andem decision making..feature representation from noisy inputs.
entity records are gathered from different sourceswith three typical noises in attribute values: mis-placing, missing, or synonym.
misplacing meansthat attribute value of ai drifts to aj(i (cid:54)= j); miss-ing means that attribute values are empty; synonymmeans that attribute values with the same mean-ing have different literal forms.
our ï¬rst task is tofusion noisy heterogeneous information in a self-supervised manner with unlabelled data..interpretable em.
domain experts have somevaluable speciï¬cations on em rules as follow: (1)an em rule is an if-then rule of feature comparison;(2) it only selects a part of key attributes from allentity attributes for decision making; (3) featurecomparison is limited to a number of similarity con-straints, such as =, â‰ˆ (fan et al., 2009; singh et al.,2017).
our second task is to realize an interpretableem decision process by comparing feature repre-sentation per attribute by utilizing a ï¬xed numberof quantitative similarity metrics and then traininga decision tree using a limited amount of labeleddata.
our interpretable em decision making willease the collaboration with domain experts..2 task deï¬nitions.
3 methodology.
entity matching.
let t1 and t2 be two collec-tions of entity records with m aligned attributes{a1, Â· Â· Â· am}.
we denote the ith attribute val-.
in this section, we introduce (1) a neural model,heterogeneous information fusion (hif), for thetask of feature representation, and (2) a decision.
2771figure 2: the decoupled em model comprising the heterogeneous information fusion module and the matchingdecision making module.
we use circles and rectangles to denote words and vectors, respectively.
cyan lineswith arrow indicate word information aggregation via intra-attribute attention.
red lines with arrow show attributeinformation propagation.
in the comparison features vector, blue squares are similarity scores by comparing onhif(e1)[ai], hif(e2)[ai] and yellow squares are similarity scores by comparing on e1[ai], e2[ai] directly.
emb,agg, prop, cfc, and kat-induction are calculation components speciï¬ed in section 3..tree, key attribute tree (kat), for the task of inter-pretable em.
figure 2 illustrates the overall work-ï¬‚ow of our method.
the following subsectionsdive into details of the two tasks and propose anovel training scheme for low resource settings byexploiting unlabelled entity records..3.1 hif for entity attribute embedding.
hif : t â†’ rmÃ—d is a function that maps entityrecords into vector representations.
an attributevalue e[ai] of a record e is mapped to a d dimen-sional vector, written as hif(e)[ai] âˆˆ rd.
hiftreats attribute values as strings of words and per-forms word embedding (emb), word informationaggregation (agg), and attribute information prop-agation (prop) successively..word embedding (emb).
word embedding isa pre-train language model that contains featureslearned from a large corpus.
we convert numericaland encoded attribute values into strings of digitsor alphabets.
for chinese attribute values, we doword-segmentation using pkuseg (luo et al., 2019).
then, we mark the beginning and the end of anattribute value with two special tokens, namely(cid:104)beg(cid:105) and (cid:104)end(cid:105).
finally, we pad each attributevalue with (cid:104)pad(cid:105) so that they are represented inthe same length l. the representation after padding.
is illustrated as below:.
((cid:104)beg(cid:105), w1, w2, Â· Â· Â· (cid:104)end(cid:105), (cid:104)pad(cid:105), Â· Â· Â· , (cid:104)pad(cid:105))(cid:124)(cid:125)(cid:123)(cid:122)length = l.let w be the set of words, each word w âˆˆ wis mapped into a vector, and each attribute value ismapped into a matrix.
formally, emb : w n â†’rn Ã—de maps n words into an n Ã— de matrix byexecuting a look-up-table operation.
n is the dic-tionary size.
in particular, we have emb(e)[ai] âˆˆrlÃ—de, in which de is the dimension of word em-it is worth noting that (cid:104)pad(cid:105)bedding vectors.
is embedded to zero vector to ensure that it doesnot interfere with other non-padding words in thefollowing step..word information aggregation (agg).
sum-ming up the l word embeddings as the embeddingof an attribute value will neglect the importanceweight among the l words.
we leverage a moreï¬‚exible framework, which aggregates word infor-mation by weighted pooling.
the weighting co-efï¬cients Î±i for different words are extracted bymultiplying its embedding vector with a learnable,and attribute-speciï¬c vector ai âˆˆ rdeÃ—1.
subscripti implies that Î±i and ai are associated with theith attribute ai.
the weighting coefï¬cients arenormalized by softmax function among words.
fi-nally, we enable a non-linear transformation (e.g.,.
2772titleauthorvenueyeardata mining â€¦â€¦ by jiaweisigmodconference2002titleauthorvenueyeardata mining : concepts â€¦â€¦j han, acm sigmodauthorcosine simd=0.9venuecosine simd=0.88truetitlejaccard simd=0.7falsefalsetrue>â‰¤>>â‰¤â‰¤hif: heterogeneous information fusionâ€¦â€¦â€¦â€¦comparisonmetricskat matching decisioncomparisonmetricscomparisonmetricscomparisonmetricsembğ‘’![ğ’œ!]embğ‘’![ğ’œ"]embğ‘’![ğ’œ#]embğ‘’![ğ’œ$]embğ‘’"[ğ’œ!]embğ‘’"[ğ’œ"]embğ‘’"[ğ’œ#]embğ‘’"[ğ’œ$]hifğ‘’![ğ’œ!]hifğ‘’![ğ’œ"]hifğ‘’![ğ’œ#]hifğ‘’![ğ’œ$]hifğ‘’"[ğ’œ!]hifğ‘’"[ğ’œ"]hifğ‘’"[ğ’œ#]hifğ‘’"[ğ’œ$]propaggembcfckatinductionğ’œ!ğ’œ"ğ’œ#ğ’œ$ğ‘’![ğ’œ!]hifğ‘’![ğ’œ!]ğ‘’"[ğ’œ!]hifğ‘’"[ğ’œ!
]comparisonfeaturesrelu) during information aggregation with param-eters wai âˆˆ rdeÃ—da.
formally, agg maps eachattribute value of entity record e into a da dimen-sional vector agg(emb(e)[ai]) âˆˆ rda as below:.
agg(emb(e)[ai]) = relu (Î±i emb(e)[ai] wai).
Î±i = softmax(emb(e)[ai] ai)(cid:62) âˆˆ r1Ã—l.
attribute information propagation (prop).
the mechanism of attribute information propaga-tion is the key component for noise reduction andrepresentation uniï¬cation.
this mechanism is in-spired by the observation that missing attribute val-ues often appear in other attributes (e.g., venue andconference in figure 1, mudgal et al.
(2018) alsoreported the misplacing issue)..we use â€œscaled dot-product attentionâ€ (ashishet al., 2017) to propagate information among differ-ent attribute values.
we use parameters q, k, vito convert agg(emb(e)[ai]) into query, key, andvalue vectors, respectively (notice that only viis attribute-speciï¬c).
a âˆˆ rmÃ—m is the attentionmatrix.
aij denotes the attention coefï¬cients fromthe ith attribute to the jth attribute:(cid:18) qi Â· kjâˆšm.aij = softmax.
(cid:19).
qi = agg(emb(e)[ai]) qkj = agg(emb(e)[ai]) kvi = agg(emb(e)[ai]) vi.
record notation e is omitted in vectors q, k, v forbrevity.
to keep the identity information, each at-tribute value after attribute information propagationis represented by the concatenation of the contextand the value vector:.
prop(agg(e))[ai] = relu.
ï£­vi.
aijvj.
ï£¸.
ï£«.
ï£¶.
(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)(cid:13).
(cid:88).
j(cid:54)=i.
hif outputs with multiple layer perceptron (mlp).
the whole process can be summarized as follows:.
hif(e) = mlp â—¦ prop â—¦ agg â—¦ emb(e) âˆˆ rmÃ—d.
after hif, each attribute ai of an entity record ehas a feature embedding hif(e)[ai]..3.2 kat for matching decision.
kat matching decision consists of two steps: com-parison feature computation (cfc) and decisionmaking with kat.
cfc computes similarity score.
for each paired attribute features by utilizing a fam-ily of well-selected metrics, and concatenate thesesimilarity scores into a vector (comparison feature).
kat takes comparison feature as inputs, and per-form entity matching with a decision tree..feature computing.
comparison(cfc).
given a record pair (e1, e2), cfc implementsa function that maps (e1, e2) to a vector ofsimilarity scores cfc(e1, e2).
the similar-ity score cfc(e1, e2) is a concatenation ofa similarity vector between paired attributevalues (i.e., e1[ai], e2[ai]) and a similarityvector between their vector embeddings (i.e.,hif(e1)[ai], hif(e2)[ai])..to compare paired attribute values, we followkonda et al.
(2016) and classify attribute valuesinto 6 categories, according to the type and thelength, each with a set of comparison metrics forsimilarity measurement, such as jaccard similar-ity, levenshtein similarity, monge-elkan similarity,etc.
more details are presented in table 1..for attribute value embeddings, we choose threemetrics: the cosine similarity, the l2 distance, andthe pearson coefï¬ciency.
in this way, we convertentity record pair into similarity score vector ofattributes.
each dimension indicates the similaritydegree of one attribute from a certain perspective..kat induction.
in the matching decision, wetake cfc(e1, e2) as input, and output binary classi-ï¬cation results.
we propose key attribute tree, adecision tree, to make the matching decision basedon key attribute heuristic, in the sense that someattributes are more important than others for em.
for example, we can decide whether two recordsof research articles are the same by only check-ing their title and venue without examining theirconference.
focusing only on key attributes notonly saves computations, but also introduces inter-pretability that has two-folded meanings: (1) eachdimension of cfc(e1, e2) is a candidate featurematching which can be interpreted as a componentof an em rule; (2) the decision tree learned bykat can be converted into em rules that follow thesame heuristics as the em rules made by domainexperts (fan et al., 2009)..3.3 model training.
hif and kat induction are trained separately..hif training.
we design a self-supervised train-ing method for hif to learn from unlabeled data..2773attribute type.
boolean.
number.
string of length 1.comparison metrics.
exact matching distance.
exact matching distance, absolute distance,levenshtein distance, levenshtein similarity.
levenshtein distance, levenshtein similarity,jaro similarity, jaro winkler similarity,exact matching distance, jaccard similarity with qgram tokenizer,.
string of length [2, 5].
string of length [6, 10].
jaccard similarity with qgram tokenizer, jaccard similarity with delimiter tokenizer,levenshtein distance, levenshtein similaritycosine similarity with delimiter tokenizer,monge elkan similarity, smith waterman similarity,.
jaccard similarity with qgram tokenizer, cosine similarity with delimiter tokenizer,levenshtein distance, levenshtein similarity,monge elkan similarity.
string of length [10, âˆ].
jaccard similarity with qgram tokenizer, cosine similarity with delimiter tokenizer.
table 1: the attributes are classiï¬ed into 6 categories according to their type and their string lengths.
for differenttypes of attributes, we use different comparison metrics..our strategy is to let the hif model predict manu-ally masked attribute values.
we ï¬rst represent at-tribute values, as strings of words, by weighted bagof words (wbow) vectors, whose dimensionsrepresent word frequencies.
then, we manuallycorrupt a small portion of entity records in t1 âˆª t2by randomly replacing (mask) their attribute valueswith an empty string, which forms a new table t (cid:48).
hif takes t (cid:48) as input and uses another mlp to pre-dict the wbow of masked attribute values.
hif istrained by minimizing the cross-entropy betweenthe prediction and the ground-truth wbow:.
crossentropy (cid:0)mlp(hif(t (cid:48))), wbow(cid:1).
minhif.
kat induction training.
kat is trained with anormal decision tree algorithm.
we constrain itsdepth, in part to maintain the interpretability oftransformed em rules.
we use xgboost (tianqiand carlos, 2016) and id3 algorithm (quinlan,1986) in the experiments.
to preserve interpretabil-ity, the booster number of xgboost is set to 1,which means it only learns one decision tree.
for(e1, e2, t rue) âˆˆ d, kat takes cfc(e1, e2) as in-put, and t rue as the target classiï¬cation output..4 experiments.
4.1 experimental setup.
4.1.1 datasets.
in order to evaluate our model comprehensively, wecollect multi-scaled datasets ranging from englishcorpus and chinese corpus, including structureddatasets, dirty datasets, and real datasets.
struc-.
type.
dataset #attr.
#rec.
#pos.
#neg.
rate.
structured.
dirty.
real.
i-a1d-a1d-s1.
i-a2d-a2d-s2.
phoneskirttoner.
844.
844.
362013.
132.
407.
10%2,9084,739 2,220 10,143 1%13,270 5,347 23,360 1%.
132.
407.
2,90810%4,739 2,220 10,143 1%13,270 5,347 23,360 1%.
9401,099 2,241 10%9,708 6,371 18,202 1%7,065 4,551 13,481 1%.
table 2: statistics of the datasets.
#attr.
is the numberof attributes, #rec.
is the number of entity records, and#pos.
(#neg.)
is the number of labeled positive (neg-i-a indicates matching between itunes-ative) pairs.
amazon.
d-a indicates matching between dblp-acm.
d-s indicates matching between dblp-googlescholar.
we use subscripts 1, 2 to distinguish betweenstructured and dirty data..tured and dirty datasets are benchmark datasets1released in (mudgal et al., 2018).
the real datasetsare sampled from taobaoâ€”one of the biggest e-commerce platform in china, a portion of whichare manually labeled to indicate whether they arethe same entity or not.
the real datasets have no-tably more attributes than the structured or dirtydatasets..statistics of these datasets are listed in table 2.we focus on setting of low resource em and userate% of labelled data as training set.
the valida-tion set uses the last 20% labeled pairs, and the restpairs in the middle are the test set.
this splitting is.
1http://pages.cs.wisc.edu/Ëœanhai/.
data1/deepmatcher_data/.
2774different from the sufï¬cient resource em (mudgalet al., 2018; konda et al., 2016) where up to 60%pairs are used in the training set.
for i-a1, i-a2,and phone, we use 10% labeled pairs as trainingdata, because some of the baselines will crash, ifthe training data is too small..we remove trivial entity pairs from the realdatasets, as structured and dirty datasets have beenreleased.
for real datasets, we remove matchingpairs with large jaccard similarity (0.32 for phone,0.36 for others) and non-matching pairs with smalljaccard similarity (0.3 for phone, 0.332 for others)..4.1.2 baselines.
we implement 3 variants of our methods with dif-ferent kat induction algorithms.
hif+katid3 andhif+katxgb inducts kat with id3 algorithm andxgboost respectively constraining maximum depthto 3. hif+dt inducts kat with id3 algorithmwith no constraints on the tree depth.
we includereproducibility details in appendix b..we compare our methods with three sota emmethods, among which two are publicly availableend-to-end neural methods, and one is feature engi-neering based method..1. deepmatcher (mudgal et al., 2018) (dm)is a general deep-learning based em frame-work with multiple variantsâ€”rnn dm-rnn,attention dm-att, and hybrid dm-hybâ€”depending on what building block it choosesto construct2..2. hiermatcher (fu et al., 2020) is also an end-to-end neural em method that compare entityrecords at the word level3..3. magellan (konda et al., 2016) integrates bothautomatic feature engineering for em andclassiï¬ers.
decision tree is used as the classi-ï¬er of magellan in our experiments..for ablation analysis, we replace a single com-ponent of our model with a new model as fol-lows: hif+ln replaces kat with a linear clas-siï¬er; hif+lr replaces kat with a logistic re-gression classiï¬er; hif-alone removes com-parison metrics of attribute values (yellow seg-ment of comparison features in figure 2).
we.
2https://github.com/anhaidgroup/.
deepmatcher.
entitymatcher.
3https://github.com/cipnlu/.
methods.
i-a1 d-a1 d-s1 i-a2 d-a2 d-s2 phone skirt toner.
63.6 85.4 74.8 42.3 45.7 39.0 90.0 67.6 68.6dm-rnn55.8 82.5 79.0 46.5 45.2 57.8 80.3 54.4 48.8dm-attdm-hyb60.9 86.6 78.0 49.5 46.2 60.4 91.9 64.2 67.4hiermatcher 61.9 37.5 68.2 37.8 32.6 45.8 86.2 61.7 55.292.3 93.7 85.1 50.6 65.6 71.1 93.6 96.6 97.2magellan.
96.0 96.4 87.5 54.9 80.1 74.2 94.9 96.7 97.2hif+dt95.8 96.6 88.2 51.6 79.0 79.5 94.5 96.7 97.2hif+katid3hif+katxgb 90.6 93.3 87.9 41.5 80.3 79.5 94.4 96.2 97.2.hif+lnhif+lr.
77.9 21.0 54.7 41.684.2 87.1 84.6 46.5.
--.
78.5 72.2 62.8 86.068.1 87.5 41.7 62.0.hif-wbow 93.0 92.7 75.4 43.2 47.9 43.7 91.6 66.3 74.0hif-emb91.1 90.9 76.6 30.8 53.9 46.8 89.9 65.7 79.8hif-alone 94.6 96.1 82.9 45.6 73.5 63.2 91.8 63.0 72.9.table 3: f1 score of all methods under low resource set-ting(%).
dash (-) indicates classiï¬er fails to converge..also do ablation analysis for hif-alone as fol-lows: hif-wbow replaces outputs of hif withd-dimensional wbow vectors using pca.
hif-emb replaces the outputs of hif with the meanpooling of word embeddings..4.1.3 evaluation metrics.
we use f1 score as the evaluation metric.
experi-ment results are listed in table 3 and table 5. allthe reported results are averaged over 10 runs withdifferent random seeds..4.2 experimental results.
general results.
we evaluate the performanceof our model against 3 sota models under lowresource settings, where only 1% or 10% of thetotal amount of labeled pairs are used for training(see table 2).
comparative experiment results onthe 9 datasets are listed in table 3..our decoupled framework achieves sota emresults on all the nine datasets, and demonstratessigniï¬cant performance on dirty datasets, with aboosting of 4.3%, 14.7%, and 8.4% in terms of f1score on i-a2, d-a2, d-s2, compared to the bestperformance of baselines on their correspondingdatasets.
our methods also outperforms all base-lines on structured and two real datasets (the sameas magellan on toner).
the out-performance onreal datasets is marginal because attribute values inreal datasets are quite standard, which means thatour model does not have many chances to ï¬x noisyattribute values.
still, our methods achieve a highf1 score (â‰¥ 94.9%) in real datasets.
these resultsindicate out methods are both effective under lowresource settings and robust to noisy data..2775figure 3: results for robustness.
hif+kat refers to hif+katxgb.
each two subgraphs in the same columncorrespond to the same drop rate (drop rate is marked on the top of each column).
each ï¬ve subgraphs in the samerow correspond to the same dataset.
x-axis is the rate of labelled data used in training.
y-axis is the f1 score..effectiveness to low resource settings we re-duce the training rate from 60% to 10% to seewhether our method is sensitive to the number oflabeled record pairs as training resources.
exper-imental results are shown in figure 3. hif+kat(red line) achieves a stable performance as the num-ber of labeled record pairs decreases, while the f1score of deepmatcher and hiermatcher decreasesimultaneously.
besides, our methods continuouslyoutperform deepmatcher and hiermatcher, rang-ing from low resource setting to sufï¬cient resourcesetting.
these results indicate that by exploring un-labelled data, hif alleviates the reliance on labeledrecord pairs..effectiveness to noisy heterogeneous data.
we manually aggravate the quality of datasets byrandomly dropping p% of attribute values (p%ranges from 0% to 40%), and see to what degreethe feature representations delivered by hif willaffect the em decision matching.
from left to right,columns of subgraphs in figure 3 demonstrates re-sults with increasing dropping rate.
on the i-a1dataset, the inï¬‚uence of dropping rate is marginal tohif+kat , whose f1 score ï¬‚uctuates around 95%.
in contrast, f1 scores of both deepmatcher andhiermatcher will decrease if more attribute valuesare dropped.
on the phone dataset, the droppingrateâ€™s inï¬‚uence is not severe to hif+kat, especiallywhen the training rate is low.
these results showthat hif is efï¬cient in recovering noisy heteroge-neous inputs..4.3 case study for interpretablity.
the interpretability of our model means that theprocess of decision making of kat can be easilytransformed into em rules whose structure is rec-ommended by domain experts.
figure 4 illustratesa tree decision process of kat that determineswhether two records denote the same publicationin the d-a1 (dblp and acm) datasets.
each pathfrom the root to a leaf node of the tree structure canbe converted into an em rule as follows:.
rule 1: if l2 (hif(e1), hif(e2)) [authors] â‰¥ 10.21.then e1, e2 are not a match;.
rule 2: if l2 (hif(e1), hif(e2)) [authors] < 10.21.
âˆ§ l2 (hif(e1), hif(e2)) [title] < 0.73.then e1, e2 are a match;.
rule 3: if l2 (hif(e1), hif(e2)) [authors] < 10.21.
âˆ§ l2 (hif(e1), hif(e2)) [title] â‰¥ 0.73.then e1, e2 are not a match.
they can be further read as descriptive rules:rule 1: if two records have different authors, theywill be different publications.
rule 2: if two records have similar authors andsimilar titles, they will be the same publication.
rule 3: if two records have similar authors and dis-similar titles, they will not be the same publication.
the soundness of such rules can be examined byour experience..important features of kat are as follows: (1)kat is conditioned on attribute comparison; (2)kat only selects a few key attributes to comparefeatures.
in our example, there are 4 attributes, au-thor, title, venue and conference in d-a1 dataset,.
2776102030405060405060708090100f1 score (i-a1) %drop rate = 0%102030405060707580859095100f1 score (phone) %102030405060405060708090100drop rate = 10%102030405060707580859095100102030405060405060708090100drop rate = 20%102030405060training rate %707580859095100102030405060405060708090100drop rate = 30%102030405060707580859095100102030405060405060708090100drop rate = 40%102030405060707580859095100hif-katdm-hybdm-attdm-rnnhiermatcherepoch.
i-a1 d-a1 d-s1 phone skirt toner.
dm-hyb 0.98hiermatcher 0.47hif+katid3 0.45.
1.00.31.0.
2.30.71.5.
12.741.72.2.
5.14.05.5.
2.51.43.2.train.
i-a1 d-a1 d-s1 phone skirt toner.
dm-hybhiermatcherhif+katid3.
8637344.
434139819.
9583091,085 1,097 1,669.
1,418 2,984 1,4733,799 2,809 1,082968.test.
i-a1 d-a1 d-s1 phone skirt toner.
dm-hybhiermatcherhif+katid3.
2.42.00.4.
31.725.11.0.
67.150.11.4.
56.9113.0 181.15.42.2.
229.6 113.974.43.1.table 4: (epoch) training time for one epoch & (train)training time until ï¬nish & (test) testing time.
all theresults are recorded in seconds..methods.
i-a1 d-a1 d-s1 i-a2 d-a2 d-s2 phone skirt toner.
83.1 98.8 93.5 67.1 94.8 89.6 98.2 91.6 90.9dm-rnn83.8 98.8 93.7 62.2 94.1 90.4 95.7 93.2 91.6dm-attdm-hyb83.5 98.8 95.0 64.0 95.9 92.6 98.7 94.2 92.0hiermatcher 79.1 98.5 94.3 77.1 96.1 93.0 96.5 95.4 94.7.
95.5 97.6 91.7 60.0 87.8 77.1 97.5 99.7 99.8hif+dthif+katid395.9 98.1 90.2 59.3 89.7 80.5 94.9 99.3 99.6hif+katxgb 95.5 98.1 90.1 63.3 89.3 80.4 96.5 99.7 99.9.table 5: f1 scores of all methods under sufï¬cient re-source setting(%)..may disturb decision making..efï¬ciency.
table 4 shows the running times ofour methods and of the two neural baselines.
ourmethods are highly efï¬cient for inference, becauseour methods are highly parallel and are memory-saving.
for example, on phone datasets our meth-ods can inference in a single batch, while hier-matcher can only run in a batch size of 4 with24gib ram.
the training efï¬ciency of our methodis comparable with baselines, because when thetraining data is small enough, baseline models mayï¬nish one epoch training with only few batches..sufï¬cient resource em.
table 5 shows the re-sults with sufï¬cient training data following thesplit method of mudgal et al.
(2018); fu et al.
(2020).
our method outperforms other methods on4 datasets, and slightly fall behind on 5 datasets..5 related works.
the way of extracting comparison features fallsinto two categories: monotonic and non-monotonic.
monotonic features are (negatively) proportionalsimilarities between attribute values.
they can.
figure 4: the key attribute tree generated byhif+katxgb for d-a1 dataset..kat only selects title and author for em decisionmaking.
the transformed rules meet the speciï¬-cations of manually designed em rules of domainexperts (fan et al., 2009; singh et al., 2017).
thiskind of interpretability will ease the collaborationwith domain experts, and increase the trustworthi-ness, compared with uninterpretable end-to-enddeep learning em models..4.4 discussions.
ablation analysis.
experiment results for abla-tion models are listed in table 3. on the one hand,hif+ln and hif+lr generally outperforms deep-matcher and hiermatcher on 7 datasets with on-parperformance on 2 real datasets.
this indicates thathif and cfc together extract better comparisonfeatures than end-to-end neural methods under lowresource settings.
on the other hand, hif+ln andhif+lr are weaker than the tree induction classi-ï¬er, suggesting that kat is more reliable..compared with hif-katid3, magellan, and hif-alone, hif-katid3 achieves the highest perfor-mance, indicating that comparison on both attributevalue embeddings and the original attribute valuesare important.
compared with hif-alone, hif-wbow, and hif-emb, hif-alone outperformshif-wbow and hif-emb on the dirty datasets,showing the positive effects of its information re-construction..finally, comparing hif+kat with hif+dt, weï¬nd that hif+kat has better performances thanhif+dt on most of the datasets, except for (i-a2and phone).
this shows that non-key attributes.
2777<(cid:1)<(cid:1)<(cid:1)<(cid:1)<(cid:1)be calculated by symbolic rules, such as jaccardsimilarity, levenshtein similarity (fan et al., 2009;wang et al., 2011; konda et al., 2016; singh et al.,2017), or learned from differentiable comparisonoperations, such as subtracting, point-wise multi-plication (fu et al., 2019; ebraheem et al., 2018;fu et al., 2019).
non-monotonic features arehidden representations of end-to-end neural net-works, such as softmax or sigmoid based sim-ilarity scores (fu et al., 2020), attention basedscores (nie et al., 2019), or simply embeddingbased features (mudgal et al., 2018; li et al., 2020)..em with limited resources has recently intriguedresearch interest (thirumuruganathan et al., 2018;kasai et al., 2019).
existing explorations seeksolution from leveraging external data to improv-ing annotation efï¬ciency.
external data can beaggregated via transfer learning (zhao and he,2019; thirumuruganathan et al., 2018; kasai et al.,2019; loster et al., 2021), or via pre-training lan-guage models (li et al., 2020).
for better annota-tions, researchers tried active learning (kasai et al.,2019; nafa et al., 2020; sarawagi and bhamidipaty,2002; arasu et al., 2010), or crowd sourcing tech-niques (wang et al., 2012; gokhale et al., 2014)..the interpretability of neural models will con-tribute to the trust and the safety.
it has becomeone of the central issues in machine learning.
chenet al.
(2020) examines interpretability in em riskanalysis.
there are also attempts to explain fromthe perspective of attention coefï¬cients (mudgalet al., 2018; nie et al., 2019)..6 conclusion.
we present a decoupled framework for inter-pretable entity matching.
it is robust to both noisyheterogeneous input and the scale of training re-sources.
experiments show that our method canbe converted to interpretable rules, which can beinspect by domain experts and make em processmore reliable..in the future, it is intriguing to explore more efï¬-cient ways to explore unlabeled data, such as lev-ering connections among entities, or combine withpre-trained language models.
it is also valuable toexplore how to use our heterogeneous informationfusion module to boost other em methods, suchas injecting hif representation as supplementaryinformation into end-to-end models..acknowledgments.
this work is supported by science and technologyinnovation 2030 - new generation of artiï¬cial in-telligence project (2020aaa0106501), the nsfckey project (u1736204), the nsfc youth project(62006136), the federal ministry of education andresearch of germany as part of the competencecenter for machine learning ml2r (01is18038c),and the grant from alibaba inc..ethical considerations.
intended use.
the reported technique is in-tended for reliable entity matching in large scalee-commercial products, where attribute values aremostly heterogeneous descriptive sentences.
theâ€˜low resourceâ€™ feature is intended to avoid heavylabor force.
the â€˜interpretabilityâ€™ is intended torisk control in entity matching..misuse potential.
as matching/alignment tech-nique, our method may be misused in matchingprivate information..failure modes.
our method provides a promis-ing way to have domain experts check the gener-ated rules, thus reducing the failure risk..energy and carbon costs.
the efï¬ciency testin section 4.4 shows that our method costs lesscomputations and is more energy saving than exist-ing methods..references.
arvind arasu, michaela gÂ¨otz, and raghav kaushik.
2010. on active learning of record matching pack-ages.
in sigmodâ€™10..vaswani ashish, shazeer noam, parmar niki, uszko-reit jakob, jones lion, aidan n. gomez, kaiserlukasz, and polosukhin illia.
2017. attention is allyou need.
in nipsâ€™17..piotr bojanowski, edouard grave, armand joulin, andtomas mikolov.
2017. enriching word vectors withsubword information.
tacl, 5..zhaoqiang chen, qun chen, boyi hou, zhanhuai li,and guoliang li.
2020. towards interpretable andlearnable risk analysis for entity resolution.
in sig-modâ€™20..p christen.
2012. data matching: concepts and tech-niques for record linkage, entity resolution, and du-plicate detection.
springer: data-centric systemsand applications..2778vassilis christophides, vasilis efthymiou, and kostasstefanidis.
2015. entity resolution in the web ofdata.
synthesis lectures on the semantic web..obraczka daniel, schuchart jonathan, and rahm er-hard.
2020. eager: embedding-assisted entity res-olution for knowledge graphs.
in icdmâ€™20..xin dong, evgeniy gabrilovich, geremy heitz, wilkohorn, ni lao, kevin murphy, thomas strohmann,shaohua sun, and wei zhang.
2014. knowledgevault: a web-scale approach to probabilistic knowl-edge fusion.
in sigkddâ€™14..muhammad ebraheem, saravanan thirumuruganathan,shaï¬q joty, mourad ouzzani, and nan tang.
2018.distributed representations of tuples for entity reso-lution.
in vldbâ€™18..wenfei fan, xibei jia, jianzhong li, and shuai ma.
in2009. reasoning about record matching rules.
vldbâ€™09..cheng fu, xianpei han, jiaming he, and le sun.
2020.hierarchical matching network for heterogeneousentity resolution.
in ijcaiâ€™20..cheng fu, xianpei han, le sun, bo chen, wei zhang,suhui wu, and hao kong.
2019. end-to-end multi-in ij-perspective matching for entity resolution.
caiâ€™19..chaitanya gokhale, sanjib das, anhai doan, jeffrey f.naughton, narasimhan rampalli, jude w. shavlik,and xiaojin zhu.
2014. corleone: hands-off crowd-sourcing for entity matching.
in sigmodâ€™14..jungo kasai, kun qian, sairam gurajada, yunyao li,and lucian popa.
2019. low-resource deep en-tity resolution with transfer and active learning.
inaclâ€™19..sidharth mudgal, han li, theodoros rekatsinas, an-hai doan, youngchoon park, ganesh krishnan, ro-hit deep, esteban arcaute, and vijay raghavendra.
2018. deep learning for entity matching: a designspace exploration.
in sigmodâ€™18..youcef nafa, qun chen, zhaoqiang chen, xingyu lu,haiyang he, tianyi duan, and zhanhuai li.
2020.active deep learning on entity resolution by risksampling.
corr, abs/2012.12960..hao nie, xianpei han, ben he, le sun, bo chen,wei zhang, suhui wu, and hao kong.
2019. deepsequence-to-sequence entity matching for heteroge-neous entity resolution.
in cikmâ€™19..j. ross quinlan.
1986. induction of decision trees.
ma-.
chine learning, 1(1):81â€“106..sunita sarawagi and anuradha bhamidipaty.
2002. in-in.
teractive deduplication using active learning.
sigkddâ€™02..soï¬a serrano and noah a smith.
2019..is attention.
interpretable?
in aclâ€™19..rohit singh, venkata vamsikrishna meduri, ahmed k.elmagarmid, samuel madden, paolo papotti, jorge-arnulfo quianÂ´e-ruiz, armando solar-lezama, andnan tang.
2017. generating concise entity match-ing rules.
in sigmodâ€™17..yan song, shuming shi, jing li, and haisong zhang.
2018. directional skip-gram: explicitly distinguish-ing left and right context for word embeddings.
innaaclâ€™18..saravanan thirumuruganathan, shameem a puthiyaparambath, mourad ouzzani, nan tang,andshaï¬q joty.
2018. reuse and adaptation for en-tity resolution through transfer learning.
corr,abs/1809.11084..diederik p. kingma and jimmy ba.
2015. adam: amethod for stochastic optimization.
in iclrâ€™15..chen tianqi and guestrin carlos.
2016. xgboost: a.scalable tree boosting system.
in sigkddâ€™16..pradap konda, sanjib das, suganthan g. c. paul,anhai doan, adel ardalan, jeffrey r. ballard,han li, fatemah panahi, haojun zhang, jeffrey f.naughton, shishir prasad, ganesh krishnan, rohitdeep, and vijay raghavendra.
2016. magellan: to-ward building entity matching management systems.
in vldbâ€™16..yuliang li, jinfeng li, yoshihiko suhara, anhai doan,and wang-chiew tan.
2020. deep entity matchingwith pre-trained language models.
in vldbâ€™20..michael loster, ioannis koumarelas, and felix nau-mann.
2021. knowledge transfer for entity resolu-tion with siamese neural networks.
journal of dataand information quality (jdiq)..ruixuan luo, jingjing xu, yi zhang, xuanchengren, and xu sun.
2019. pkuseg: a toolkit formulti-domain chinese word segmentation.
corr,abs/1906.11455..jiannan wang, tim kraska, michael j. franklin, andjianhua feng.
2012. crowder: crowdsourcing en-tity resolution.
vldbâ€™12..jiannan wang, guoliang li, jeffrey xu yu, and jianhuafeng.
2011. entity matching: how similar is similar.
in vldb..chen zhao and yeye he.
2019. auto-em: end-to-endfuzzy entity-matching using pre-trained deep mod-els and transfer learning.
in wwwâ€™19..wayne xin zhao, yuexin wu, hongfei yan, and xi-aoming li.
2014. group based self training for e-commerce product record linkage.
in colingâ€™14..2779methods.
dm-rnndm-attdm-hybhiermatchermagellan.
hif+lnhif+lr.
hif+dthif+katid3hif+katxgb.
methods.
dm-rnndm-attdm-hybhiermatchermagellan.
hif+lnhif+lr.
hif+dthif+katid3hif+katxgb.
dm-rnndm-attdm-hybhiermatchermagellan.
hif+lnhif+lr.
hif+dthif+katid3hif+katxgb.
i-a1.
r.60.958.464.161.892.7.
73.089.1.
94.994.794.0.i-a2.
r.42.450.454.543.949.4.
34.044.5.
54.553.451.0.r.92.183.890.189.292.1.
65.580.0.
97.096.996.1.p.69.154.258.464.192.3.
84.179.9.
97.197.187.7.p.43.346.451.141.251.8.
54.149.5.
55.650.635.9.p.88.177.193.983.695.1.
80.597.3.
93.092.292.6.d-a1.
r.90.391.289.238.992.2.
97.195.7.
97.097.495.7.d-a2.
r.55.548.344.627.874.8.
--.
85.585.486.1.skirt.
r.73.870.176.177.097.2.
51.526.4.
96.796.693.5.p.81.775.384.341.695.4.
15.086.7.
95.995.891.1.p.39.142.548.848.558.5.
--.
75.473.675.4.p.62.344.555.651.796.1.
93.899.9.
96.796.999.0.f1.
85.482.586.637.593.7.
21.087.1.
96.496.693.3.f1.
45.745.246.232.665.6.
--.
80.179.080.3.f1.
67.654.464.261.796.6.
62.841.7.
96.796.796.2.d-s1.
d-s2.
r.80.983.582.467.290.2.
44.384.2.
85.188.787.4.r.50.760.465.144.169.7.
84.775.7.
70.977.277.1.r.80.862.287.367.997.6.
83.889.8.
96.796.796.8.toner.
p.69.975.074.372.180.7.
96.185.2.
90.087.888.4.p.31.955.557.350.472.6.
73.162.1.
77.881.982.1.p.60.340.655.046.796.7.
88.462.6.
97.697.697.6.f1.
74.879.078.068.285.1.
54.784.6.
87.588.287.9.f1.
39.057.860.445.871.1.
78.568.1.
74.279.579.5.f1.
68.648.867.455.297.2.
86.062.0.
97.297.297.2.f1.
63.655.860.961.992.3.
77.984.2.
96.095.890.6.f1.
42.346.549.537.850.6.
41.646.5.
54.951.641.5.f1.
90.080.391.986.293.6.
72.287.5.
94.994.594.4.methods.
phone.
table 6: experimental results under low-resource setting with precision, recall, and f1 measure (%).
dash (-)indicates these methods fail to converge on the datasets..a more experimental results.
table 6 in the main text only shows the f1 measureof the all the methods.
here, we supplement thetpexperimental results with precision (p =tp+fp ),recall (r =tp+fn ) on the 9 datasets for more com-prehensive analysis.
experimental results are listedin table 6. our methods achieve the highest preci-sion and recall on most of the datasets..tp.
b reproducibility details.
each epoch of hif training is evenly divided into 3batches.
the title attribute values were padded tol = 64, and the other attribute values are all paddedto l = 32. we modify the padding size on large.
datasets, so that our the experiments can be con-ducted on a single gpu.
chinese datasets are em-bedded with tencent embedding (song et al., 2018)and english datasets use fasttext embeddings (bo-janowski et al., 2017).
multi-head mechanism isused in the attention module.
the embedding sizede for chinese is 300, and for english is 200. aggconverts embedding into da dimensional vectors,where da = 100. prop further outputs with a 2-layer mlp with dimension size d = 64. the queryvector and the key vector in the attention layer ofprop are 16 dimensional vectors.
during train-ing, attribute values are masked at a probabilityp = 0.4. the adam optimizer (kingma and ba,.
27802015) is used for hif .
training rate and l2 weightdecay are 0.01 and 10âˆ’5..katxgb is implemented using xgboost 0.9with objective function binary: logistic.
katid3is implemented using scikit-learn 0.24.hif is implemented with pytorch 1.4.0in python 3.7.6.the comparison fea-ture metrics in table 1 are implemented withpy-entitymatching 0.4.0. we also usenumpy 1.19.2 for matrix calculation.
all theexperiments are evaluated on a single nvidia3090 gpu with 24gib gram..2781