reﬂective decoding: beyond unidirectional generation withoff-the-shelf language models.
peter west1,2 ximing lu1,2 ari holtzman1chandra bhagavatula2 jena hwang2 yejin choi1,21paul g. allen school of computer science & engineering, university of washington2allen institute for artiﬁcial intelligence{pawest, ahai, yejin}@cs.washington.edu{ximinglu, chandrab, jenah}@allenai.org.
abstract.
publicly available, large pretrained languagemodels (lms) generate text with remarkablequality, but only sequentially from left to right.
as a result, they are not immediately appli-cable to generation tasks that break the unidi-rectional assumption, such as paraphrasing ortext-inﬁlling, necessitating task-speciﬁc super-vision..in this paper, we present reflective de-coding, a novel unsupervised algorithm thatallows for direct application of unidirectionallms to non-sequential tasks.
our 2-step ap-proach requires no supervision or even paral-lel corpora, only two off-the-shelf pretrainedlms in opposite directions: forward and back-ward.
first, in the contextualization step, weuse lms to generate ensembles of past andfuture contexts which collectively capture theinput (e.g.
the source sentence for paraphras-ing).
second, in the reﬂection step, we condi-tion on these “context ensembles”, generatingoutputs that are compatible with them.
com-prehensive empirical results1 demonstrate thatreflective decoding outperforms strongunsupervised baselines on both paraphrasingand abductive text inﬁlling, signiﬁcantly nar-rowing the gap between unsupervised and su-pervised methods.
reflective decodingsurpasses multiple supervised baselines on var-ious metrics including human evaluation..1.introduction.
language models (lms) like gpt-2 (radford et al.,2019), trained over vast unstructured data, canleverage enhanced generation methods (holtzmanet al., 2020; martins et al., 2020; welleck et al.,2019) to give ﬂuent and coherent continuationsto given input text—e.g.
news articles or stories..1further.
results.
athttps://homes.cs.washington.edu/˜pawest/reflectivedecoding.html.
available.
resource.
and.
are.
figure 1: illustration of reflective decoding ap-plied to paraphrasing and abductive inﬁlling (αnlgbhagavatula et al., 2020).
only the right-context isshown, although both right- and left-contexts are usedin practice.
first the contextualization step (1) capturesaspects of an input by generating many representativecontexts for it.
then in the reﬂection step (2) we sam-ple generations that can replace the input and ﬁt theserepresentative contexts.
←−rd..gpt-3 (brown et al., 2020) takes this a step fur-ther: given a small number of examples and awell-constructed prompt, it shows remarkable per-formance on tasks where vast quantities of super-vised data and ﬁnetuning were thought to be nec-essary.
while this demonstrates the potential forlm-decoding in few-shot or even zero-shot out-of-the-box settings, limited access to gpt-3 andimmense computational cost keep this from beinga widely usable or efﬁcient solution..yet recent work shows that gpt-2 may hold sim-ilar capabilities when it is primed correctly.
li and.
proceedingsofthe59thannualmeetingoftheassociationforcomputationallinguisticsandthe11thinternationaljointconferenceonnaturallanguageprocessing,pages1435–1450august1–6,2021.©2021associationforcomputationallinguistics1435!!
"#: how are circulatory system tissues formed?"!
: how do circulatory systems form?inputparaphrasegenerated contextsparaphrasing1sample contexts  #$~%&(#|)%&')2sample "!from +,+,()%&')c1: this is a medical question best answered by a doctor……c2: as with all tissue in the body, this begins with cell division …c3: is one of  many key questions about the circulatory system … /: i picked her up and took her to san francisco general hospital.inputhypothesis1sample contexts  #$~%&(#|2*,2+)2sample "!from +,…c1: the day after her discharge she told me she was a lot better …1(: amy had heart palpitations after a lot of caffiene1): by the time she arrived her heart felt much better+,(2*,2+)-nlggenerated contextsliang (2021) achieve supervised-level performancein a few-shot setting using smaller, accessible mod-els like gpt-2.
they learn a small number of task-speciﬁc vectors as a preﬁx to the input, withouttuning the model itself.
off-the-shelf gpt-2 is ca-pable of few-shot learning given the right setup; ourwork aims to push this concept further, by showingthat out-of-the-box lms can solve complex gener-ation problems simply by using the right decodingalgorithm..we introduce reflective decoding—anovel decoding method that allows lms to be ap-plied to generation tasks that break the “text con-tinuation” paradigm, such as paraphrasing and text-inﬁlling.
reflective decoding requires nosupervision, only two complementary off-the-shelf←−lms—one forward (lm).
that means no per-task ﬁnetuning, even on unstruc-tured text in the target domain..−→lm) and one backward (.
−→lm and.
inspired by the distributional hypothesis (firth,1957), reflective decoding works by generat-ing text that might occupy the same contexts as an←−lm) to generateinput.
we use two lms (contexts for a given input, which implicitly captureaspects of its meaning (the contextualization step).
then, in the reﬂection step, we condition on thisensemble of contexts, decoding over the input withgenerations that are distributionally related to—orreplace—the input..paraphrasing is a natural application: a goodparaphrase should intuitively be compatible withthe same contexts as the original text.
reflec-tive decoding shows strong unsupervised para-phrasing performance: on the quora question pairdataset, we ﬁnd one variant of our model (rd30)outperforms unsupervised baselines on all but onemetric, and supervised baselines on both the sarimetric and human evaluation.
we see the sametrends on the twitter url corpus (lan et al., 2017).
reflective decoding can also be applied totasks that only replace part of the input, or gener-ate within it, like inﬁlling; on αnlg (bhagavatulaet al., 2020), we outperform the best unsupervisedbaseline on overall quality, effectively halving thegap with supervised methods.
in both applications,reflective decoding directly applies off-the-shelf pretrained models, without ﬁnetuning on thetask or target domain.
this provides evidence thatoff-the-shelf language models can excel at sur-prising applications, when paired with decodingalgorithms designed to elicit speciﬁc kinds of infor-.
mation..2 method.
2.1 notation.
arrows indicate the order in which sampling func-tions condition on and generate tokens: −→ indi-cates generating from the left-most token to theright (left-to-right), while ←− indicates going right-to-left.
for language models (lms), this means−→lm is what is often called a “forward lm”, while←−lm is a “backward lm”.
for our sampling function(rd), this also indicates which generated context−→is being conditioned on, e.g.
rd conditions on leftcontext, extending it to the right to generate output..2.2 overview.
currently, lm-decoding is limited to a text con-tinuation paradigm.
given an input text sinput,lm(c|sinput) generates contexts c that might come−→after (forward, i.e.
lm) or before (backward, i.e.
←−lm) the input.
lm-decoding generates outside ofthe input by continuing it, but many tasks requireus to generate over or within the input: paraphras-ing requires reformulating the input, while inﬁllingrequires inserting text in the middle of it..reﬂective decoding approaches this shortcom-ing by turning conventional lm-decoding around.
while lm(c|sinput) generates the kinds of contextsc the input might appear in, rd generates s thatmight replace sinput in these same contexts.
thedistributional hypothesis (firth, 1957) suggests se-mantically similar texts appear in similar contexts,meaning rd is also likely to sample in the semanticneighborhood of sinput..concretely, reflective decoding sampless that ﬁts the same contexts as sinput in 2 simplesteps.
we ﬁrst sample many representative contexts−→ci that could neighbor the input, e.g.
usinglmin figure 1. this is the contextualization step.
second, in the reﬂection step, we generate text←−lm in figure 1),in the opposite direction (usingwhich ﬁts these contexts as well as sinput ﬁts them.
to consider all ci’s while decoding, we ensemblethe different distributions imposed by conditioningon each ci:.
←−rd (s) =.
(cid:81).
←−lm(s|ci)wiiz(s, c, w).
(1).
where z normalizes the fraction to a proper proba-bility distribution (see equation 2).
in essence, this.
1436algorithm 1: learn reflective decoder.
←−rd.
input: forward language model.
backward language modelsource text sinput1: sample contexts, c1...cnc ∼2: initialize parameters w = w1...wnc s.t..−→lm(c|sinput).
−→lm.
←−lm.
(cid:80) wi = 1, wi ≥ 03: learn w = arg maxw.
s.t.
(cid:80) wi = 1, wi ≥ 0.output:.
←−rd.
←−rd(sinput).
←−rd restricts generations to ﬁt all contexts−→−→lmrd is the same, except it uses.
ensembleci.
reversedwith left contexts ci generated by.
←−lm..by ensembling the contexts in a product of ex-perts (hinton, 2002) framework, we can generatea hypothesis s that ﬁts the full contextual ﬁnger-print.
yet, some contexts are more informative thanothers: probable but generic contexts like “see theappendix for details.” are not descriptive of neigh-boring text.
we learn weights wi to prioritize con-texts ci in the ensemble that are most informativefor sinput, by maximizing the probability of sinputunder equation 1 (described in algorithm 1).
ineffect, we are learning an on-the-ﬂy autoencoderat inference time, using weighted ensembles ofcontexts as a representation (see §2.7, §a.1)..to motivate how this method functions, considerthe paraphrasing example from figure 1 with in-put sinput = how are circulatory system tissuesformed?
generated contexts reﬂect different as-pects of sinput: c1 situates sinput as a question(this is a medical question...), while c2 and c3 ex-plore central concepts (as with all tissue...; aboutthe circulatory system).
even though each contextcould follow many sentences, together they forma ﬁngerprint for sinput.
a sentence that could befollowed by all of c1, c2, c3 will likely be a question(c1), about tissue formation (c2), and the circulatorysystem (c3), and generally occupy the same seman-tic neighborhood as sinput, e.g.
how do circulatorysystems form?.
in the case of paraphrasing, our task is to replaceall of sinput with something that might appear inthe same contexts.
other tasks, however, mightrequire us to replace only part of a sentence (e.g.
in-context paraphrasing) or even insert text at a givenposition (e.g.
inﬁlling).
reflective decodingmakes this easy: simply hold part of sinput staticwhen we generate from rd..2.3 reflective decoding.
here we dive into the details of reflective de-coding, by considering the right-hand context←−ensemble (rd), keeping in mind that the processis repeated on the left-hand as well (.
−→rd)..first, in the contextualization step (line 1 ofalgorithm 1), we sample many right-hand contexts−→ci for sinput, usinglm.
these will be used as arepresentative sample of the contexts sinput appearsin.
second, in the reﬂection step (lines 2 & 3) our←−rd thatgoal is to construct a sampling function←−will yield texts similar to sinput.
we deﬁnerd as:.
(cid:81).
(cid:81)|s|.
←−rd(s) =.
←−lm(s|ci)wi←−lm(t|sj+1:|s| + ci)wi(2)this is equivalent to equation 1, but giving theexact normalization factor in the denominator..i(cid:81).
(cid:80).
j=0.
t∈v.
i.equation 2 is a token-wise product of expertsmodel, that captures the semantic neighborhoodof sinput via the combination of contexts ci andtheir weights wi (§2.7).
we learn wi that maximize←−rd(sinput) (probability of generating sinput un-←−derrd), thereby up-weighting contexts speciﬁcto sinput.
we initialize these weights (line 2),then train them (line 3) using the adam optimizer(kingma and ba, 2014).
we normalize weightsinto a proper probability distribution at every step.
−→rd is learned symmetrically,reverse-direction←−−→lm and sampling left-lm andﬂipping the roles ofhand context instead (see §b.1 for details).
finally,−→we generate fromrd), sampling outputsthat would appear in the same contexts as sinput.
depending on the application, we rank and select−→lma ﬁnal output in different ways, always usingand.
←−lm together to capture bidirectional ﬁt..←−rd (and.
2.4.implementation.
weight learning and pruning context weightswi are learned using the adam optimizer (kingmaand ba, 2014).
in practice this takes under 100steps (negligible time compared to lm decoding).
while we sample tens of contexts (line 1 of algo-rithm 1), many end up with negligible weight underthe learned distribution (equation 2).
to efﬁciently−→sample fromrd, we drop all but the topkc contexts and renormalize weights: kc < nc con-texts are used during the reﬂection step..←−rd and.
parameters we sample nc contexts to describethe source sinput.
we use nucleus sampling (holtz-.
1437figure 2: example generations of reflective decoding on paraphrasing and abductive text inﬁlling (αnlg).
rd45 encourages more difference from the input than rd30 (§3.1)..−→rd and.
man et al., 2020) with parameter pc, and a maxi-←−mum length of lenc.
oncerd are learned,we sample ns generations from each, of lengthlens.
we again use nucleus sampling, but chooseps per-example to account for vastly different en-tropy in rd (§b.3).
values for all hyperparametersare available in §b.4..−→language models we train large forward (lm)←−lm) language models based onand backward (gpt-2 (radford et al., 2019) using the openweb-text training corpus (gokaslan and cohen, 2019)2.our implementation details follow those of pastwork retraining gpt-2 (zellers et al., 2019)..2.5 application: paraphrasing.
to paraphrase, we begin by generating candidateoutputs.
following §2.3 the reflective decod-ing sampling function is learned in each direction←−−→rd) using the source sentence sinput.
then,rd,(←−ns generations are sampled from bothrd:.
−→rd and.
s1, ..., sns ∼.
−→rd, sns+1, ..., s2∗ns ∼.
←−rd.
this gives a robust set of candidates that are com-patible with the same left and right contexts assinput.
many of these will be semantically relatedto sinput, but must be scored and ranked in orderto select true paraphrases.
reflective decod-ing is based on the notion that good “ﬁt” with thesame contexts is a robust measurement of similar-ity, yielding a natural “contextual scoring function”(equation 7 and §2.7).
we measure how likely can-didate s is to generate the same contexts that sinput−→rd anddid when constructing.
←−rd:.
score(s) =.
(cid:88).
−→lm(crh|s)+.
(cid:88).
←−lm(clh|s).
1nc.
crh.
1nc.
clh.
where crh are the generated contexts used in.
2https://github.com/yet-another-account/openwebtext.
(3).
←−rd,.
−→and clh forrd.
this explicitly estimates how sim-ilar the contexts of s and sinput are on both sides,the underlying objective of reflective decod-ing..2.6 application: abductive reasoning.
abductive natural language generation (αnlgfrom bhagavatula et al.
2020) is the task of ﬁll-ing in the blank between 2 observations o1 and o2,with a hypothesis h that abductively explains them.
the challenge for lm-decoding is making use ofcontext from both sides (o1 on the left and o2 onthe right).
this is particularly challenging for unsu-pervised decoding methods because unidirectionallms cannot naturally condition on both sides whengenerating h..←−rd or.
reflective decoding simpliﬁes this prob-lem by capturing information about both o1 and−→o2 in a single decoding function (rd), thenholding o1 and o2 static at generation time (i.e.
teacher forcing).
concretely, we use concatenatedo1 +o2 as sinput in algorithm 1, and construct sam-←−pling functionsrd informed by both observa-tions.
we are interested in sampling in between o1←−and o2, so when sampling hypotheses h fromrdwe condition on the right-side observation o2 (and−→rd and o1).
this is equivalent to ap-vice-versa forpending the given observation to sampled contexts:.
−→rd,.
←−rd(h|o2)−→rd(h|o1).
h1, ..., hnˆs ∼hnˆs+1, ..., h2∗nˆs ∼←−−→rd contain informationrd andabout both o1 and o2, effectively turning a 2-sidedcontextual constraint into a 1-sided one..note that both.
(4).
we also use a task-speciﬁc scoring function torank sampled hypotheses.
we would like a hypoth-esis h that best explains both observations, and souse language models to measure this:.
score(h) =.
←−lm(o1|h+o2)+.
−→lm(o2|o1 +h) (5).
1438task:!nlg%!
:ray hung a tire on a rope to make his daughter a swing.__?__%":ray ran to his daughter to make sure she was okay.rdhe put her on the swing, and while she was on the swing, she fell off and was lying on the ground.%!
:tom and his family were camping in a yurt.__?__%":he chased it around until it left the yurt.rdhe went to the yurt and found a bearthat was in the yurttask: paraphrasingwhat is it like to have a midlife crisis?rd30what does it meanto have a midlife crisis?rd45what do you dowhen you have a midlife crisis?is it possible to make money as a film critic?rd30is there a way to make money as a film critic?rd45is it possible to make a living as a movie critic?
adding h should help to “explain” each observa-tion given the other, i.e.
that o2 follows from o1 + hand o1 from h + o2.
to ﬁlter hypotheses that onlyexplain one of the two observations, we removeany that make either observation less probable thanthe empty hypothesis, imposing:.
←−lm(o1|h + o2) >−→lm(o2|o1 + h) >.
←−lm(o1|o2)−→lm(o2|o1).
2.7.intuitions and theory.
here we discuss the theoretical intuition for re-flective decoding, as a way to sample genera-tions that share contextual “ﬁt” with a source text,deriving the sampling function of equation 2..we start by considering how to relate the mean-ing of two texts, generation s and input sinput.
wefollow a distributional intuition (firth, 1957), thatmeaning can be understood through the contexts inwhich text appears.
many distributional approacheslearn contentful neural representations by predict-ing context given input text (mikolov et al., 2013;kiros et al., 2015), then compare these represen-tations to establish semantic similarity.
we can,instead, compare contexts directly—judging thedifference in meaning between texts sinput and sby their divergence:.
approximately the same contextual hole, minimiz-ing the value of this “contextual distance”..in this form, ˆh compares 2 complete texts–s andsinput–but we are trying to generate s for whichthe divergence from sinput is low.
we ﬂip the roleof “text” and “context”3 to deﬁne a function fromwhich we can sample s:.
←−rd(sj|, sj+1:n) =.
(cid:81).
i.t∈v.
(cid:80).
←−lm(sj|sj+1:n + ci)wi(cid:81).
←−lm(t|sj+1:n + ci)wi(8).
i.
(equivalent to equation 2, derived in §a.1) sj is thejth token in s (sampled right-to-left from n to 0),and v is the vocabulary.
weights wi are learned bymaximizing the probability of sinput..equation 8, estimates the probability of predict-ing sinput and s from a ﬁnite set of contexts cigenerated from sinput.
this approximately mini-mizes equation 6, as being generated by the sameweighted ensemble of contexts strongly correlateswith generating the same contexts in the same pro-portions, i.e.
low divergence, due to the sparsityof language.
we can sample s with low contextual←−distance from sinput usingrd.
further, we can use−→left context to constructrd by simply reversingthe directions of the lms used..dkl(.
−→lm(c|sinput),.
−→lm(c|ˆs)).
(6).
3 experiments.
−→lm to interchangeably denote the theoreti-we usecal left-to-right distribution of text, and the lm es-−→lm(c|s) is the distribution overtimating it.
thus,right contexts c given sentence s, and equation 6can be understood as the “contextual informationdifference” we expect s to have from sinput.
note,←−lmwe could similarly use left-hand context and—and do so in practice..we use ﬁnite-sample cross entropy as an effec-.
tive empirical proxy for dkl:.
ˆh(.
−→lm(c|sinput),.
−→lm(c|s)) =1(cid:88)n.−→lm(c|sinput).
ci∼.
−log.
−→lm(ci|s).
(7).
−→lm(c|sinput) indicates sampling con-where ci ∼−→texts for sinput fromlm.
intuitively, we want tominimize this score when generating s: an optimaloutput has a similar meaning to sinput and so ﬁlls.
3.1 task: paraphrase generation.
task: following past work, we test our para-phrasing method (§2.5) on the quora question pairdataset.
we hold out 1000 examples for testing,with the rest for training and validation (used bysupervised baselines), disallowing overlap with thetest set.
we test a subset of models (compatibleunsupervised models, mt) on the twitter url cor-pus (lan et al., 2017), using 1000 examples fromthe canonical test split..metrics: following past work, we include auto-matic metrics bleu (papineni et al., 2002), me-teor (denkowski and lavie, 2014), and terp(snover et al., 2009).
these measure agreementwith references, but high reference/input overlapmeans copying is rewarded (mao and lee, 2019);indeed, copying source sentences as-is wins onthese metrics (table 1), meaning both bleu andmeteor can be easily gamed..3context is a symmetric relation: a given text serves as the.
one-sided context of its own context..1439human.
supervised.
supervised (bilingual) mt.
unsupervised.
method.
sari↑ bleu↑ meteor↑.
terp ↓ human↑ n ovelty ↑.
sourcereference.
pg-ildipsbart.
r-vqvaecgmht opcgmh30cgmh45upsa.
17.891.9.
32.838.836.1.
35.6.
27.232.333.932.634.0.
56.0100.0.
49.141.044.7.
48.1.
43.642.040.933.836.6.rdt op (us)rd30 (us)rd45 (us).
29.040.0*38.6.
49.9*46.839.9.
37.6100.0.
33.827.934.7*.
33.5.
32.328.227.523.426.7.
33.932.228.9.
48.00.0.
49.0*56.066.0.
52.0.
60.059.060.065.070.0.
52.057.065.0.
-71.7.
29.436.646.1.
59.3.
33.527.031.515.837.8.
27.563.2*61.1.
0.043.9.
24.448.5*35.2.
26.8.
26.227.629.744.544.4.
20.830.045.0.table 1: model performance on the quora test split.
bold indicates best for model-type, * indicates best overall(excluding human), underline indicates second-best for unsupervised.
the ﬁrst 5 columns are measures of quality,while the last measures novelty (equation 9) or difference from the input.
we rerun evaluations from past work..past work has emphasized the important chal-lenge of generating novel paraphrases (liu et al.,2010; chen and dolan, 2011) we address this in 3ways.
first, we explicitly quantify a simple notionof novelty:.
n ovelty(ˆs) = 100 − bleu (ˆs, sinput).
(9).
to quantify the novelty-quality trade-off.
second,we include the sari metric (xu et al., 2016) whichexplicitly balances novelty from input with refer-ence overlap.
third, we quantify an overall humanquality metric accounting for this..we have humans evaluate ﬂuency, consistency,and novelty on amazon mechanical turk.
theoverall score (“human” in table 1) is the rate ex-amples meet thresholds for all 3: ﬂuent enoughto understand, with at most minor differences inmeaning and at least minor differences in word-ing.
on quora, we test 200 examples, with agree-ment (fleiss’ κ fleiss, 1971) of 0.40 (ﬂuency),0.54 (consistency), 0.77 (novelty) and 0.48 (over-all) i.e.
moderate to substantial agreement (landisand koch, 1977).
on the twitter corpus, we use100 examples with agreement of 0.39, 0.42, 0.54,and 0.36, indicating fair to moderate agreement.
on both we have 3 raters per example.
see §c.2for more..baselines: parameters for reflective decod-ing are given in §b.4.
we mainly compare against3 unsupervised baselines: controlled sentencegeneration by metropolis hastings (cgmh frommiao et al.
2019), simulated annealing (upsa.
from liu et al.
2019) and the residual vq-vae ofroy and grangier (2019a) (r-vqvae).
this is across-section of recent approaches (vae, editing).
we also compare against a machine-translationapproach (see sec 6), pivoting through germanusing transformer (vaswani et al., 2017) modelstrained on wmt19 data (barrault et al., 2019).
mtis included in a separate section in our results as ituses supervised bilingual data (table 1).
we include supervised baselines:.
the pointergenerator trained by imitation learning (pg-il) asin du and ji (2019), the diversity-promoting dipsmodel (kumar et al., 2019), and a ﬁnetuned bartmodel (lewis et al., 2019), which uses a morecomplex pretraining method than our lms.
notethat dips generates multiple diverse paraphrasesso we pick one at random..cgmh and reflective decoding both re-turn multiple sampled, ranked paraphrases.
we caneasily control for n ovelty by taking the highest-ranked output that meets a n ovelty threshold.
forboth, we have a version with no threshold (t op),and with thresholds such that average n oveltyis 30 and 45. n ovelty cutoffs do not dependon the reference, only the source, and are equiv-alent to selecting with bleu-ori (n ovelty is100 − bleu-ori) by miao et al.
(2019) or baoet al.
(2019)..3.2 task: abductive nlg.
task: the abductive natural language genera-tion task (αnlg) presented in bhagavatula et al.
(2020) requires generating a hypothesis that ﬁts.
1440sari↑ human ↑ n ovelty ↑.
method.
sourcereference.
mt.
r-vqvaecgmht opcgmh30cgmh45rdt op/30 (us)rd45 (us).
13.690.7.
36.1.
31.132.733.231.831.436.4.
-51.3.
70.9.
32.327.825.113.546.556.9.
0.063.3.
30.4.
40.425.530.145.237.045.3.table 2: model performance on the twitter url testsplit.
bold indicates best for model-type.
we showonly metrics accounting for novelty (more in §c.3).
between observations o1 and o2, and explains them.
we apply reflective decoding to this problemas outlined in §2.6, using the given data splits..metrics: for human evaluation, over 200 exam-ples we ask 3 raters on amazon mechanical turkabout coherence between h and o1, o2, o1 + o2, andoverall quality on 4-value likert scales.
we foundfleiss’ kappa (fleiss, 1971) of 0.32, 0.40, 0.41,and 0.41 respectively, indicating fair to moderateagreement (landis and koch, 1977)..baselines: parameters for reflective decod-ing are given in §b.4.
we include baselines fromthe original work: different supervised variants ofgpt-2 large with access to the observations, and op-tionally commonsense embeddings or generationsfrom comet (bosselut et al., 2019).
we includeunsupervised baselines of gpt-2 conditioned on o1+ o2 directly, the gradient-based delorean modelof qin et al.
(2020), and ilm inﬁlling model ofdonahue et al.
(2020), representing recent unsuper-vised methods..4 results and analysis.
paraphrasing first, the quora dataset: on auto-matic metrics from past works (bleu, meteor,terp ) our lowest-n ovelty model setting (rdt op)achieves the highest unsupervised scores, and high-est overall on bleu.
other high scoring rows(source, pg-il) are similarly low-n ovelty.
thesari metric explicitly balances n ovelty with sim-ilarity to reference.
on sari we see such low-n ovelty models perform worse.
the best over-all model on sari is our medium-n ovelty set-ting (rd30) which outperforms mt and supervisedmodels..our human evaluation measures what fraction of.
outputs are found to be ﬂuent, consistent, and novel.
as with sari, both our mid and high-n oveltymodels perform quite well, again with the medium-n ovelty setting outperforming all baselines.
asfurther validation for sari as a proxy for human,they share the same top-5 models..results on the twitter url corpus largely sup-port those on quora.
reflective decodingachieves the best unsupervised scores on novelty-aware metrics (table 2), with the best overall sari,even outperforming reference on the human metric,although mt achieves the highest overall..in sum, reflective decoding is able to com-pete on previously used quality metrics favoringlow-n ovelty, but can produce more varied out-puts preferred by humans.
rd45 is among the bestmodels by sari and human on quora despite ex-ceeding the novelty of even the reference..αnlg results on αnlg (table 3) present astrong case that reflective decoding can ef-fectively use bidirectional context.
strong hypothe-ses use information from both initial the observa-tion o1 and the future observation o2.
humansranked the ability of reflective decoding tocapture this 42.4, about 17 points above the next-best unsupervised baseline and only 15 points be-low the best supervised method tested.
we seesimilar results for overall evaluation.
a likely fac-tor in this is the (comparatively) high degree ofcoherence between h and o2 by reflective de-coding.
where other methods seem to pay moreattention to observation o1 (the o2 column generallyhas much lower values), reflective decodinghas comparably high coherence with left-hand (o1)and right-hand (o2) contexts..we also include example generations in figure 2to demonstrate the ability of reflective decod-ing to combine o1 and o2.
for example, h = heput her on the swing, and while she was on theswing, she fell off and was lying on the ground.
incorporates information from both observations.
speciﬁcally, it takes into account the swing thatray is building for his daughter which is only men-tioned in o1, and hypothesizes about a potentialinjury due to ray checking on his daughter in o2.
see appendix for more generations..overall, the strong performance of reflectivedecoding on αnlg shows that unsupervisedgeneration with context ensemble applies to inﬁll-ing in addition to paraphrasing..14415 discussion.
reflective decoding out-of-the-box amajor advantage to applying reflective decod-ing is ease-of-use: armed with our pretrained lan-guage models, practitioners can immediately begingenerating.
with general pretrained models and un-derlying principles that are domain-agnostic, re-flective decoding works across a broad rangeof text style–no ﬁnetuning required–making explo-ration and adaptation simple.
multiple rounds ofgeneration mean reflective decoding mayrun slower than other methods at inference time4,but it avoids training time.
there are clearly set-tings that favor supervised learning (narrow, knowndomain with abundant training data), but reflec-tive decoding is a good option to begin gener-ating and exploring immediately with high qualitygeneration..a useful abstraction for understanding re-flective decoding for current applications is“prompting”, i.e., writing a preﬁx to implicitly or ex-plicitly describe a task for a pretrained model.
re-flective decoding generates natural contextsthat the desired generation would appear in.
thisbreaks from other methods of automatic prompt-ing, which often forego “natural” prompts (shinet al., 2020; reynolds and mcdonell, 2021), evenmaking them continuous (li and liang, 2021; ham-bardzumyan et al., 2021; lester et al., 2021; qinand eisner, 2021).
reflective decoding alsonotably creates a set of prompts (contexts) for eachexample, where other methods attempt to learn anoverall task prompt.
still, all of these are connectedby the popular intuition that useful behavior in pre-trained models can be induced through contextualinput..future applications reflective decodingcan extend beyond our experiments here, however.
a simple example is in-context paraphrasing, i.e.
writing a paraphrase that ﬁts the true context thatthe original sentence appears in.
most existingparaphrasing methods consider only out-of-contextsentences, and would require signiﬁcant changes toconsider context as a constraint; for reflectivedecoding we can simply combine true and gen-erated contexts without with the same algorithm.
driving reflective decoding is a notionof context as a representation, with clear poten-.
4depending on parameters we found most baselines tookmultiple seconds per example vs. 10s of seconds for reflec-tive decoding on a multi-gpu machine..tial for future work.
pretrained lms capture richinformation about text spans, but accessing it with-out ﬁne-tuning is nontrivial; within the model itis an uninterpretable mass of parameters and acti-vation weights.
our work observes that unidirec-tional lms are only capturing this information topredict adjacent context–this is the sole learningsignal–so all of this information is expressed inthe model’s context prediction.
thus, we capturesome of this rich information to represent spans, bycapturing a ﬁnite-sample version of this full predic-tive distribution in generated contexts.
in reflec-tive decoding speciﬁcally, we use this formof representation to generate back into the sourcespan–paraphrasing or inﬁlling–but the notion canbe applied much more generally.
in translation forinstance, we might ﬁrst generate contexts for thesource sentence that represent its meaning, noisilytranslate these contexts, then impose that any trans-lations for the source ﬁt the same contexts under atranslation-language lm.
constraining translationsin this way can add robustness to existing systemsby anchoring translations to informative contexts.
beyond explicit generation even, we might use avery large lm like gpt-3 to deﬁne a strong scoringfunction or metric as in equation 7, ﬁrst generat-ing contexts for some target sentence, then scoringcandidates by how well they generate these samecontexts.
as in our work, such a score could indi-cate how well the option ﬁlls the same contextualrole as the target, harnessing the strong reasoningof whatever model is used..6 related work.
distributional intuitions a key aspect of re-flective decoding is using a distributional in-tuition to represent the meaning of a text throughmany contexts.
kiros et al.
(2015); miao et al.
(2019) quantify semantic relationships and lin andpantel (2001) identify paraphrastic relationshipsunder similar intuitions.
a major point of differ-ence between past work and ours is that we sampleexplicit contexts, allowing unsupervised generationback from these contexts, while past work typicallylearns a neural representation based on contextsand conditions on this vector-encoded representa-tion..unsupervised paraphrasing some approachestrain neural variational auto-encoders unsuper-vised to represent source sentences, then decodesfrom these representations to paraphrase (roy.
1442o1.
o2.
o1 + o2.
all.
86.3.
89.1.
85.1.
84.4.human.
supervised.
cometemb+gpt2comettxt+gpt2o1-o2-only.
unsupervised.
gpt2-fixeddeloreanilmreﬂective decoding.
69.368.969.2.
20.648.745.953.4.
60.154.857.7.
13.924.627.351.7.
56.451.954.3.
10.823.625.342.4.
56.350.653.8.
10.322.525.041.9.table 3: model performance on αnlg.
the ﬁrst 3scores query agreement between hypothesis and givenobservation(s), “all” indicates overall judgement.
re-flective decoding signiﬁcantly outperforms all un-supervised baselines..and grangier, 2019b; bao et al., 2019).
this re-quires training specialized representations, whereasreflective decoding applies general-purposelms.
we compare to roy and grangier (2019b)..paraphrasing by editing the input (miao et al.,2019; liu et al., 2019) has shown promise.
likereflective decoding, these approaches canbe applied without training specialized models, butare necessarily limited by edit-paths and local min-ima, as edits are often restricted to single-wordreplacement, insertion, and deletion.
generatedparaphrases must follow a continuous local editpath, while reflective decoding can generatenew sentences from scratch..reflective decoding and mt-based para-phrasing both pivot through an alternative textualform to paraphrase (context and translation, re-spectively).
but mt paraphrasing systems cycle-translate through a pivot language (federmannet al., 2019; wieting and gimpel, 2018), whichrequires supervised bilingual translation data, withan implicit notion of interlingual paraphrasing..abductive text inﬁlling αnlg (bhagavatulaet al., 2020) is a text inﬁlling task that speciﬁcallymeasures the ability of models to explain bidirec-tional context (observations o1, o2) with a hypoth-esis that ﬁts between them.
this naturally ﬁts re-flective decoding, which ﬁlls in contextualgaps.
recent work has directly addressed this task(qin et al., 2020) while the inﬁlling literature isalso quite applicable (donahue et al., 2020).
wecompare to both of these methods on abductiveinﬁlling, showing superior results..7 conclusions.
we present reflective decoding, a novel un-supervised text generation method for tasks thatdo not ﬁt the text continuation paradigm.
it usesjust two pretrained language models to generatecontexts that capture aspects of input text, generat-ing back into the input from there.
it signiﬁcantlyoutperforms unsupervised baselines in quality andnovelty for paraphrasing.
further, in abductive nat-ural language generation it outperforms unsuper-vised baselines by a signiﬁcant margin and halvesthe gap with supervised models.
reflective de-coding uses the concept of representing meaningwith generated contexts, offering new possibilitiesfor unsupervised conditional text generation..acknowledgements.
we thank anonymous reviewers for many help-ful comments.
this research is supported inpart by the natural sciences and engineering re-search council of canada (nserc) (funding refer-ence number 401233309), darpa cwc througharo (w911nf15-1-0543), darpa mcs programthrough niwc paciﬁc (n66001-19-2-4031), theallen institute for ai, and a gift from intel labscognitive computing research..ethical considerations.
novelty in paraphrasing mao and lee (2019)observe that paraphrases close to the source oftenwin on automatic quality metrics.
however, dis-similarity from the source correlates with humannotions of paraphrasing (liu et al., 2010).
ku-mar et al.
(2019) increase novelty through theirdiversity-promoting sampling method.
alterna-tive metrics that consider novelty alongside qualityhave been proposed (sun and zhou, 2012; feder-mann et al., 2019).
the sari metric (xu et al.,2016), included here, combines these notions..in order to complete our human evaluation we usedamazon mechanical turk.
we estimated the rangeof times we expected our task to take, and madesure that at minimum workers would be paid awage of $15.00 per hour if they were solely com-pleting our task..as part of this effort, we plan to release ourcode and model.
our forward and backward lan-guage models are the same size as the publiclyavailable gpt-2 (radford et al., 2019).
trainingtime/energy was likely signiﬁcantly smaller than.
1443the original release; existing code and hyperparam-eters were available, and we use a smaller dataset.
further, there is no publicly available backwardgpt-2 model that we are aware of, so releasinga pair of forward and backward models that weretrained on the same data allows for proper compar-isons about left-to-right vs. right-to-left processingof english text..we estimate that the potential dangers of releas-ing this from a malicious generation perspectiveare low.
our forward model is similar to already re-leased gpt-2 models.
while the backward modeladds new generation potential and scientiﬁc nov-elty, it is unlikely to compare to gpt-3 (brownet al., 2020) which many hobbyists and privatecompanies now have access to.
we believe that re-leasing a pair of forward and backward models willbe more useful to researchers who wish to studythe symmetries and asymmetries of the linguisticdistribution..references.
yu bao, hao zhou, shujian huang, lei li, lili mou,olga vechtomova, xinyu dai, and jiajun chen.
2019. generating sentences from disentangled syn-in proceedings of thetactic and semantic spaces.
57th annual meeting of the association for compu-tational linguistics, pages 6008–6019..lo¨ıc barrault, ondˇrej bojar, marta r costa-juss`a,christian federmann, mark fishel, yvette gra-ham, barry haddow, matthias huck, philipp koehn,shervin malmasi, et al.
2019. findings of the 2019conference on machine translation (wmt19).
inproceedings of the fourth conference on machinetranslation (volume 2: shared task papers, day 1),pages 1–61..chandra bhagavatula, ronan le bras, chaitanyamalaviya, keisuke sakaguchi, ari holtzman, han-nah rashkin, doug downey, scott yih, and yejinchoi.
2020. abductive commonsense reasoning.
iclr..antoine bosselut, hannah rashkin, maarten sap, chai-tanya malaviya, asli celikyilmaz, and yejin choi.
2019. comet: commonsense transformers for au-tomatic knowledge graph construction.
in proceed-ings of the 57th annual meeting of the associationfor computational linguistics, pages 4762–4779..t. brown, b. mann, nick ryder, melanie subbiah,j. kaplan, prafulla dhariwal, arvind neelakan-tan, pranav shyam, girish sastry, amanda askell,sandhini agarwal, ariel herbert-voss, g. kr¨uger,t. henighan, r. child, aditya ramesh, d. ziegler,jeffrey wu, clemens winter, christopher hesse,mark chen, e. sigler, mateusz litwin, scott gray,.
benjamin chess, j. clark, christopher berner, sammccandlish, a. radford, ilya sutskever, and darioamodei.
2020. language models are few-shot learn-ers.
arxiv, abs/2005.14165..david chen and william dolan.
2011. collectinghighly parallel data for paraphrase evaluation.
inproceedings of the 49th annual meeting of the asso-ciation for computational linguistics: human lan-guage technologies, pages 190–200, portland, ore-gon, usa.
association for computational linguis-tics..michael denkowski and alon lavie.
2014. meteoruniversal: language speciﬁc translation evaluationfor any target language.
in proceedings of the ninthworkshop on statistical machine translation, pages376–380..chris donahue, mina lee, and percy liang.
2020. en-abling language models to ﬁll in the blanks.
in pro-ceedings of the 58th annual meeting of the asso-ciation for computational linguistics, pages 2492–2501, online.
association for computational lin-guistics..wanyu du and yangfeng ji.
2019..an empir-ical comparison on imitation learning and rein-inforcement learning for paraphrase generation.
emnlp/ijcnlp..christian federmann, oussama elachqar, and chrisquirk.
2019. multilingual whispers: generatingin proceedings ofparaphrases with translation.
the 5th workshop on noisy user-generated text (w-nut 2019), pages 17–26, hong kong, china.
asso-ciation for computational linguistics..john r firth.
1957. a synopsis of linguistic theory,.
1930-1955. studies in linguistic analysis..joseph l fleiss.
1971. measuring nominal scale agree-ment among many raters.
psychological bulletin,76(5):378..aaron gokaslan and vanya cohen.
2019. openweb-.
text corpus..karen hambardzumyan, hrant khachatrian,.
andjonathan may.
2021. warp: word-level adversarialreprogramming.
arxiv preprint arxiv:2101.00121..geoffrey e hinton.
2002. training products of expertsby minimizing contrastive divergence.
neural com-putation, 14(8):1771–1800..ari holtzman, jan buys, maxwell forbes, and yejinchoi.
2020. the curious case of neural text degener-ation.
iclr..diederik p kingma and jimmy ba.
2014. adam: amethod for stochastic optimization.
arxiv preprintarxiv:1412.6980..1444ryan kiros, yukun zhu, russ r salakhutdinov,richard zemel, raquel urtasun, antonio torralba,and sanja fidler.
2015. skip-thought vectors.
inadvances in neural information processing systems,pages 3294–3302..ning miao, hao zhou, lili mou, rui yan, and leili.
2019. cgmh: constrained sentence generationby metropolis-hastings sampling.
in proceedings ofthe aaai conference on artiﬁcial intelligence, vol-ume 33, pages 6834–6842..ashutosh kumar, satwik bhattamishra, manik bhan-dari, and partha talukdar.
2019.submodularoptimization-based diverse paraphrasing and its ef-fectiveness in data augmentation.
in naacl-hlt..wuwei lan, siyu qiu, hua he, and wei xu.
2017.a continuously growing dataset of sentential para-phrases.
in proceedings of the 2017 conference onempirical methods on natural language process-ing (emnlp), pages 1235–1245.
association forcomputational linguistics..j richard landis and gary g koch.
1977. the mea-surement of observer agreement for categorical data.
biometrics, pages 159–174..brian lester, rami al-rfou, and noah constant.
2021.the power of scale for parameter-efﬁcient prompttuning.
arxiv preprint arxiv:2104.08691..mike lewis, yinhan liu, naman goyal, mar-jan ghazvininejad, abdelrahman mohamed, omerlevy, ves stoyanov, and luke zettlemoyer.
2019.bart: denoising sequence-to-sequence pre-trainingfor natural language generation,translation, andcomprehension.
arxiv preprint arxiv:1910.13461..xiang lisa li and p. liang.
2021. preﬁx-tuning: op-timizing continuous prompts for generation.
arxiv,abs/2101.00190..chin-yew lin.
2004. rouge: a package for auto-matic evaluation of summaries.
in text summariza-tion branches out, pages 74–81, barcelona, spain.
association for computational linguistics..dekang lin and patrick pantel.
2001. dirt@ sbt@discovery of inference rules from text.
in proceed-ings of the seventh acm sigkdd international con-ference on knowledge discovery and data mining,pages 323–328..chang liu, daniel dahlmeier, and hwee tou ng.
2010.pem: a paraphrase evaluation metric exploiting par-allel texts.
in proceedings of the 2010 conferenceon empirical methods in natural language process-ing, pages 923–932..xianggen liu, lili mou, fandong meng, hao zhou,jie zhou, and sen song.
2019. unsupervised para-arxiv preprintphrasing by simulated annealing.
arxiv:1909.03588..hongren mao and hungyi lee.
2019..pollywant a cracker: analyzing performance of par-roting on paraphrase generation datasets.
inemnlp/ijcnlp..pedro henrique martins, zita marinho, and andr´e f. t..martins.
2020. sparse text generation..tomas mikolov, kai chen, greg s. corrado, and jef-frey dean.
2013. efﬁcient estimation of word repre-sentations in vector space..kishore papineni, salim roukos, todd ward, and wei-jing zhu.
2002. bleu: a method for automatic eval-in proceedings ofuation of machine translation.
the 40th annual meeting on association for compu-tational linguistics, pages 311–318.
association forcomputational linguistics..guanghui qin and jason eisner.
2021. learning howto ask: querying lms with mixtures of soft prompts.
arxiv preprint arxiv:2104.06599..lianhui qin, vered shwartz, peter west, chandra bha-gavatula, jena d. hwang, ronan le bras, antoinebosselut, and yejin choi.
2020. back to the future:unsupervised backprop-based decoding for counter-factual and abductive commonsense reasoning.
inproceedings of the 2020 conference on empiricalmethods in natural language processing (emnlp),pages 794–805, online.
association for computa-tional linguistics..alec radford, jeffrey wu, rewon child, david luan,dario amodei, and ilya sutskever.
2019. languagemodels are unsupervised multitask learners.
unpub-lished manuscript..laria reynolds and kyle mcdonell.
2021. prompt pro-gramming for large language models: beyond thein extended abstracts of thefew-shot paradigm.
2021 chi conference on human factors in com-puting systems, pages 1–7..aurko roy and david grangier.
2019a.
unsupervisedparaphrasing without translation.
in proceedings ofthe 57th annual meeting of the association for com-putational linguistics, pages 6033–6039..aurko roy and david grangier.
2019b.
unsupervised.
paraphrasing without translation.
in acl..hubert ja schouten.
1986. nominal scale agreementamong observers.
psychometrika, 51(3):453–466..thibault sellam, dipanjan das, and ankur p. parikh.
2020. bleurt: learning robust metrics for text gen-eration.
in acl..taylor shin, yasaman razeghi, robert l. logan iv,eric wallace, and sameer singh.
2020. autoprompt:eliciting knowledge from language models within proceed-automatically generated prompts.
ings of the 2020 conference on empirical methodsin natural language processing (emnlp), pages4222–4235, online.
association for computationallinguistics..1445matthew g snover, nitin madnani, bonnie dorr, andrichard schwartz.
2009. ter-plus: paraphrase, se-mantic, and alignment enhancements to translationedit rate.
machine translation, 23(2-3):117–127..hong sun and ming zhou.
2012. joint learning of adual smt system for paraphrase generation.
in pro-ceedings of the 50th annual meeting of the associa-tion for computational linguistics (volume 2: shortpapers), pages 38–42, jeju island, korea.
associa-tion for computational linguistics..ashish vaswani, noam shazeer, niki parmar, jakobuszkoreit, llion jones, aidan n gomez, łukaszkaiser, and illia polosukhin.
2017. attention is allin advances in neural information pro-you need.
cessing systems, pages 5998–6008..sean welleck, ilia kulikov, stephen roller, emily di-nan, kyunghyun cho, and jason weston.
2019. neu-ral text generation with unlikelihood training.
arxivpreprint arxiv:1908.04319..john wieting and kevin gimpel.
2018. paranmt-50m:pushing the limits of paraphrastic sentence embed-dings with millions of machine translations.
in pro-ceedings of the 56th annual meeting of the associa-tion for computational linguistics (volume 1: longpapers), pages 451–462..wei xu, courtney napoles, ellie pavlick, quanzechen, and chris callison-burch.
2016. optimizingstatistical machine translation for text simpliﬁcation.
transactions of the association for computationallinguistics, 4:401–415..rowan zellers, ari holtzman, hannah rashkin,yonatan bisk, ali farhadi, franziska roesner, andyejin choi.
2019. defending against neural fakenews.
in advances in neural information process-ing systems, pages 9051–9062..tianyi zhang, v. kishore, felix wu, kilian q. wein-berger, and yoav artzi.
2020. bertscore: evaluatingtext generation with bert.
arxiv, abs/1904.09675..1446a appendix.
a.1 derivation of sampling function.
here we derive the sampling function used forreflective decoding, which allows genera-tion using contextual similarity.
this supplements§2.7.
pc|s denotes the distribution of contexts cfor sentence s. this will be 1-sided context, forinstance right-hand context crh (i.e.
pc|s would−→be estimated by left-to-rightlm conditioned on−→lm(c|s)).
reversed ps|c goes back from con-stext towards text.
with right-hand context, this isestimated by.
←−lm(s|c)..in §2.7, we consider the task of comparing asource sentence ssrc with another sentence s. forinstance, we may want to know if s is a paraphraseof ssrc.
following a distributional intuition (firth,1957) we deﬁne a simple way to compare meaning:.
(10).
dkl(pc|ssrc, pc|s)where dkl is the kullback–leibler divergencemeasuring the difference between distributionspc|ssrc and pc|s.
this captures a notion above: wetake the amount the contexts of ssrc and s differ asa proxy for their difference in meaning..in paraphrase generation, we want to select forcontextual closeness, and thus only need to rankoptions.
we will then use cross-entropy:.
−→lm(c|ssrc),.
h(.
−→lm(c|s))(cid:88).
=.
c.−→lm(c|ssrc)log(pc|s(c)).
−.
(11).
which is equivalent to dkl up to a constant offset,and is easier to estimate.
here, the sum over cindicates every possible context c, but in practicewe us ﬁnite samples..from sec 2.7, this quantiﬁes contextual differ-ence in meaning.
for paraphrasing, we want asentence s that minimizes this, which is equivalentto maximizing the exponent of its negation:.
score(s) = e.(cid:80).
c −pc|slog(pc|s(c)).
(cid:89).
(cid:18) ps|c(s)p (c)p (s).
(cid:19)pc|s(c).
(12).
=.
=.
ca0p (s).
(cid:89).
c.ps|c(s)pc|s(c).
constant a0 results from factors of p (c).
the resultis a product of experts (hinton, 2002).
p (s)−1 will.
prioritize more context-speciﬁc paraphrases (lowprobability but likely in context).
however, ourlms are not well equipped to handle unlikely text,(expressivity is likely spent on likely text).
second,while less likely text can have higher similarity,this may not be the goal of our system.
ratherwe want related sentences that are also ﬂuent andreasonable, so we drop p (s)−1, the equivalent ofmultiplying in p (s), biasing the model towardslikely sequences:.
score(s) = c0.
ps|c(s)pc|s(c).
(13).
(cid:89).
c.a product of experts of the form:.
score(s) =.
(cid:89).
ps|c(s)wc|s.
(14).
cwe must set the weights wc|s in the ﬁnite samplesetting.
to keep in line with this the format, wewill enforce that weights constitute a proper dis-tribution.
in the limiting case (unlimited samples)wc|s should be set to pc|s(c).
however, these arelikely not efﬁcient estimation weights.
further, ex-ponentiating by this estimate will magnify errors.
instead, we learn these weights using a heuristic,discussed later..next, we move to the ﬁnite-sample setting, re-placing distributions with lm estimates.
here wewill consider right-context (meaning ps|c is esti-←−lm) but the left-context case proceedsmated bysymmetrically.
substituting in the lm distribution:.
score(s) =.
(cid:89).
←−lm(s|c)wc|s.
(15).
c.where now the product over c indicates productover the ﬁnite sampled contexts.
we convert this toa sampling function, decomposing into tokens ofgeneration s = s0...sn:.
score(s0:n) =.
(cid:89).
(cid:89).
←−lm(sj|sj+1:n)wc|s.
(16).
j.c.this restates equation 15 factorizing lm proba-bility by tokens.
renormalizing and decomposingby token position gives a natural distribution tosample from:.
psample(sj|sj+1:n) =(cid:81).
c.←−lm(sj|sj+1:n)wc|s(cid:81).
←−lm(t|sj+1:n)wc|s.
t∈v.
c.(cid:80).
(17).
1447algorithm 2: learn reflective decoding sampling.
function (left-to-right).
input: left to right language modelright to left language modelsource text: ssrc1: sample contexts, c1...cnc ∼.
←−lm(c|ssrc).
−→lm←−lm.
2: initialize parameters w = w1...wnc s.t..(cid:80) wi = 1, wi ≥ 0.
3: learn w = arg maxw.
−→rd(ssrc)under (cid:80) wi = 1, wi ≥ 0−→rd.
output:.
normalizing token-wise over the vocabulary v toa proper distribution (sampling right-to-left, indexn down, to match convention).
this is referred←−to asrd in the body of the paper, and stated inequation 8. this samples candidate generationsthat encourage adherence to the contextual scoringfunction..finally, we learn the weights (a proper distri-bution): ssrc should receive the highest score (orsimilarly, should have the lowest contextual differ-ence with itself, as it is likely in its own contexts)..b implementation details.
lens.
lenc.
pphraseαnlg.
inp + 520.
5050.ns.
3020.nc.
8050.h.4.
6..pc.
0.70.9.kc.
66.table 4: most parameters are explained in §2.4.
h isentropy for calibration in §b.3.
generations, generation parameters (truncation pa-rameter ps from nucleus sampling, in paraphras-ing) control how “greedy” or stochastic samplingis.
however, the effect of ps depends on many dy-namic (example-wise) factors.
setting ps too lowmay sample only the most likely option, too highgives off-topic candidates.
the “correct” value ofps is highly example-dependent..we deﬁne entropy calibration to control howmuch “randomness” is used in sampling in a robustway.
rather than directly setting a ps for all exam-ples, this speciﬁes the approximate entropy ˆh tosample with for each example.
in the greedy casefor instance, the desired entropy ˆh is set to 0 (i.e.
picking from a set of 1 possible option)..we search for ps in each case that is expectedto give the correct entropy for the full generation,although ps is a token-level parameter.
to estimatethis, we take sampling entropy over the source texts0...sn under the nucleus-sampling truncated dis-tribution pp:.
−pp(w|s0...si−1)logpp(w|s0...si−1).
(19).
(20).
b.1 left-to-right reflective decoding.
sampling function.
←−−→rd, switchingrd is learned similar tofrom §2.3,←−the roles oflm in algorithm 1. first,the roles of the language models are ﬂipped in thesampling function:.
−→lm and.
ˆh =(cid:88).
(cid:88).
i.w∈vp.
−→rd (s) =.
(cid:81).
−→lm(s|ci)wii(cid:81).
−→lm(t|s0:j−1 + ci)wi.
(cid:81)|s|.
(cid:80).
j=0.
t∈v.
i.vp is the truncated vocabulary with parameter ps.
we select ps that gives a desired entropy, setthingthis to 4 or 6 which we found effective (app.
b.4)..(18)←−lm (i.e.
left-.
b.4 parameters.
ci are now generated by right-to-leftcontexts).
see algorithm 2..b.2 post-processing generations.
without learning stop-tokens, reflective de-coding samples ﬁxed number (lens) of tokens.
candidates are extracted from raw generations us-ing sentence tokenization..b.3 entropy calibration.
entropy calibration is used when sampling candi-date generations (§2.4).
when sampling output.
here, we give model settings for our 2 experimen-tal settings, paraphrasing and αnlg.
see table 4.αnlg requires higher variety (higher hsample, pc),and fewer generated contexts (nc).
we experi-mented with different reasonable values on the devset of each model, evaluating manually.
we usetransformer language models (mega size) trainedon tpu pods (tensorflow) of size 512. these willbe made publicly available.
for generation we used2 nvidia titan xp gpus..1448figure 3: example generations of baselines on quora paraphrasing dataset (§3)..figure 4: example generations of baselines on αnlg dataset (§3).
models attempt to ﬁll in the blank betweeno1, o2 to explain them both..c evaluation.
c.1 automatic metrics.
links to the automatic metrics: rouge, bleu,meteor, terp , sari, bertscore,bleurt.
we include extra further metrics tested for quorain table 5: rouge (lin, 2004), bleurt (sellamet al., 2020), bertscore (zhang et al., 2020).
forbleurt, and bertscore we use default settings.
we also include ibleu sun and zhou (2012) withα = 0.9..c.2 human evaluation.
human evaluation for quora and twitter are largelydescribed in §3.
we reiterate that thresholds areused for each measure, and “overall” is the ratethat all thresholds are met.
agreement is calculatedon these binary combined threshold categories (fol-.
lowing schouten 1986).
full human results forparaphrasing are in table 6. human eval for αnlgis described in §3..c.3 twitter dataset.
we include here the full results for paraphrasingon the twitter url corpus (lan et al., 2017), aset of paraphrase pairs created by linking tweetswith matching shared urls.
we test unsupervisedmodels cgmh, r-vqvae (upsa twitter modelis not available), and the backtranslation mt model.
we include supplementary results to the main paperin table 7..d further generations.
see figures 3,4 for outputs of reflective de-coding and baselines..1449what is your creative process?what are some tips for keeping ice cream from melting?unsupervisedrdtop(us)what is your creative process?what's the best way to keep ice cream from melting?rd30 (us)what’s your creative process?what's the best way to keep ice cream from melting?rd45 (us)what’s your creative process like?what's the best way to keep ice cream from melting?r-vqvaewhat is your creative process?what tips are for keeping some ice cream from melting?upsawhat is in your career choice process?what are some good tips for making ice cream with hair loss treatment?cgmhtopwhat is your dream key?what are some arguments for keeping crude cream from?cgmh30what is your dream key?what are some arguments for keeping crude cream from?cgmh45what is your dream key?what are some arguments for keeping crude cream from?supervisedpg-ilwhat is your creative process?what are some tips for ice cream from melting?dipswhat is your creative strategy?how do i cure ice cream from melting?bartwhat is your creative process?what are some ways to keep ice cream from melting?bilingualmtwhat is your creative process?what tips are there to prevent ice from melting?o1: i once knew a girl named sammyo2:she got help and everything was alrighto1: ray hung a tire on a rope to make his daughter a swingo2: ray ran to his daughter to make sure she was okay.unsupervisedrd (us)she had problems and needed help.he put her on the swing, and while she was on the swing, she fell off and was lying on the ground.gpt-2-fixedi didn’t think to her, this was a normal situationof course, that's what he does, right?deloreansammy was a very sweet girlshe hit the rope and the tire fell on top of her.ilmshe wanted my daughter to have a new boyfriendhis daughter was flying on the rope.supervisedcomet-embsammy was in a car accidentray's daughter fell off the swing.comet-txtsammy got into a bad accident and her car broke downray's daughter fell and fell off the swing.o1 + o2sammy got hit by a drunk driverray's daughter fell off the swing.
method.
r-1↑.
r-2↑.
bleurt↑ bertscore↑.
ibleu↑ n ovelty ↑.
human.
supervised.
supervised (bilingual) mt.
unsupervised.
sourcereference.
70.1100.0.
47.0100.0.pg-ildipsbart.
r-vqvaecgmht opcgmh30cgmh45upsardt op (us)rd30 (us)rd45 (us).
66.656.763.6.
64.7.
68.255.654.548.556.265.862.156.8.
44.033.741.6.
39.8.
32.029.628.322.130.442.338.031.1.
19.999.3.
11.1-29.59.6.
16.7.
-7.6-53.6-58.9-80.9-44.515.37.7-1.9.
95.2100.0.
94.792.794.4.
94.8.
93.292.191.990.790.794.894.293.5.
40.484.4.
36.731.833.8.
35.9.
31.930.629.824.927.337.035.130.4.
0.043.9.
24.448.535.2.
26.8.
26.227.629.744.544.420.830.045.0.table 5: model performance on the quora test split.
included here are extra metrics beyond what is in the mainpaper.
r-1 and r-2 refer to rouge-1 and rouge-2..quorafluency consistency novelty overall (%).
twitterfluency consistency novelty overall (%).
method.
reference.
pg-ildipsbart.
mt.
r-vqvaecgmht opcgmh30cgmh45upsa.
rdt op (us)rd30 (us)rd45 (us).
98.7.
95.985.697.2.
98.7.
84.279.478.871.684.4.
98.098.797.5.
78.3.
79.945.177.6.
88.7.
76.343.137.919.946.7.
84.675.367.3.
94.0.
51.093.368.8.
71.2.
60.385.696.498.591.6.
43.588.295.3.
71.7.
29.436.646.1.
59.3.
33.527.031.515.837.8.
27.563.262.1.
91.7.
58.7.
95.3.
51.3.
---.
99.0.
65.371.967.251.5-.
98.798.798.0.
---.
90.0.
44.048.835.820.9-.
70.970.964.5.
---.
80.9.
94.382.692.096.3-.
76.376.392.6.
---.
70.9.
32.327.825.113.5-.
46.546.556.9.table 6: human evaluation results on both datasets for tested models.
see §c.2..human.
supervised (bilingual) mt.
unsupervised.
method.
sourcereference.
r-vqvaecgmht opcgmh30cgmh45rdt op/30 (us)rd45 (us).
sari↑ bleu↑ meteor↑.
terp ↓ n ovelty ↑.
13.690.7.
36.1.
31.132.733.231.831.436.1.
36.7100.0.
29.4.
25.228.226.320.527.225.6.
25.0100.0.
22.1.
21.019.818.715.419.919.0.
75.00.0.
80.0.
90.077.078.082.086.088.0.
0.063.3.
30.4.
40.425.530.145.737.045.3.table 7: model performance on the twitter url test split.
note: diversity of rdt op is over 30 and so this modelis equivalent to rd30 here..1450