herald: an annotation eﬃcient method to detect user disengagementin social conversations.
weixin liang1stanford universitywxliang@stanford.edu.
kai-hui liang1columbia universitykaihui.liang@columbia.edu.
zhou yucolumbia universityzy2461@columbia.edu.
abstract.
open-domain dialog systems have a user-centric goal:to provide humans with an en-gaging conversation experience.
user engage-ment is one of the most important metricsfor evaluating open-domain dialog systems,and could also be used as real-time feedbackto beneﬁt dialog policy learning.
existingwork on detecting user disengagement typi-cally requires hand-labeling many dialog sam-ples.
we propose herald, an eﬃcient an-notation framework that reframes the trainingdata annotation process as a denoising prob-lem.
speciﬁcally, instead of manually label-ing training samples, we ﬁrst use a set of la-beling heuristics to label training samples au-tomatically.
we then denoise the weakly la-beled data using the shapley algorithm.
fi-nally, we use the denoised data to train a userengagement detector.
our experiments showthat herald improves annotation eﬃciencysigniﬁcantly and achieves 86% user disengage-ment detection accuracy in two dialog corpora.
our implementation is available at https://github.com/weixin-liang/herald/..1.introduction.
evaluation metrics heavily inﬂuence a ﬁeld’s re-search direction.
the ultimate goal of open-domaindialog systems is to provide an enjoyable experi-ence to users.
previous research mainly focuseson optimizing automatic dialog evaluation metricssuch as bleu, which models the distance betweenthe system responses and a limited number of ref-erences available.
however, it has been shown thatthese metrics correlate poorly with human judg-ments (liu et al., 2016)..open-domain dialog system evaluation has longbeen one of the most diﬃcult challenges in the dia-log community for several reasons: (1) the goal of.
1equal contribution..dialog evaluation should be to evaluate users’ con-versational experience.
existing automatic evalua-tion metrics such as bleu are mostly constrainedto a static corpus, and do not capture the user experi-ence in a realistic interactive setting.
(2) currently,self-reported user ratings are widely used to evalu-ate open-domain dialogs.
however, self-reportedratings suﬀer from bias and variance among diﬀer-ent users (liang et al., 2020e).
although we couldtell which dialog system is better by running statis-tical tests on a large number of noisy ratings, it ischallenging to locate dialogs with bad performancereliably.
only by identifying these bad dialogs ef-fectively can we correct errors in these samples toimprove dialog system quality..user engagement has been recognized as oneof the essential metrics for open-domain dialogevaluation (ram et al., 2018).
previous researchalso conﬁrms that incorporating user engagementas real-time feedback beneﬁts dialog policy learn-ing (yu et al., 2016).
one of the most costly bot-tlenecks of learning to detect user disengagementis to annotate many turn-level user engagement la-bels (ghazarian et al., 2020).
in addition, the dataannotation process becomes more expensive andchallenging for privacy-sensitive dialog corpora,due to the privacy concerns in crowdsourcing (xiaand mckernan, 2020)..to improve annotation eﬃciency, we reframethe training data annotation process as a denois-ing problem.
speciﬁcally, instead of manuallylabeling each training datum, we automaticallylabel the training samples with a set of labelingheuristics.
the heuristic functions primarily con-sist of regular expressions (regexes) and incorpo-rate open-sourced natural language understanding(nlu) services.
since the automatically gener-ated labels might contain noise, we then denoisethe labeled data using the shapley algorithm (jiaet al., 2019a,b).
we use the shapley algorithm to.
proceedingsofthe59thannualmeetingoftheassociationforcomputationallinguisticsandthe11thinternationaljointconferenceonnaturallanguageprocessing,pages3652–3665august1–6,2021.©2021associationforcomputationallinguistics3652quantify the contribution of each training datum,so that we can identify the noisy data points withnegative contribution and then correct their labels.
our experiments show that herald achieves 86%accuracy in user disengagement detection in twodialog corpora..our proposed framework herald is conceptu-ally simple and suitable for a wide range of applica-tion scenarios: first, since our model could detectuser engagement in real-time (i.e., after each userutterance), our model could be plugged into exist-ing dialog systems as a real-time user experiencemonitor module.
in this way, dialog systems coulddetect and react to user’s disengagement in bothopen-domain dialogs (yu et al., 2016) and task-oriented dialogs (yu et al., 2017).
during training,our model could also be used as real-time feed-back to beneﬁt dialog policy learning (yi et al.,2019).
second, herald could quantify user en-gagement and be used as an automatic dialog eval-uation metric.
it could locate dialogs with pooruser experience reliably to improve dialog systemquality (ghazarian et al., 2020; choi et al., 2019).
third, user engagement is an essential objective ofdialog systems, but few dialog datasets with userengagement ratings are available.
our heuristicfunctions, combined with the proposed workﬂow,can be readily deployed to annotate new dialogdatasets..2 related work.
2.1 open-domain dialog system evaluation.
open-domain dialog system evaluation is a long-it has been shown that exist-lasting challenge.
ing automatic dialog evaluation metrics correlatepoorly with human judgments (liu et al., 2016;lowe et al., 2017; novikova et al., 2017).
a well-known reason is that these automatic dialog evalua-tion metrics rely on modeling the distance betweenthe generated response and a limited number of ref-erences available.
the fundamental gap betweenthe open-ended nature of the conversations and thelimited references (gupta et al., 2019) is not ad-dressed in methods that are lexical-level based (pa-pineni et al., 2002; lin, 2004; banerjee and lavie,2005), embedding based (rus and lintean, 2012;forgues et al., 2014), perplexity based (adiwar-dana et al., 2020), or learning based (tao et al.,2018; lowe et al., 2017).
mehri and eskénazi(2020) simulate user response using dialoggptand evaluate the probability of user complaint..given the limitations above, self-reported user rat-ings are widely used to evaluate open-domain di-alogs.
however, self-reported ratings suﬀer frombias and variance among diﬀerent users (venkateshet al., 2018).
denoising human ratings is still anopen research problem (liang et al., 2020e; li et al.,2019)..2.2 user engagement in dialogs.
user engagement is commonly deﬁned as the user’swillingness to continue conversing with the dia-log system (yu et al., 2016, 2017).
existing workon measuring user engagement primarily resortsto human rating (yi et al., 2019; hancock et al.,2019), or proxy metrics.
example proxy metricsinclude conversation length like number of dialogturns (venkatesh et al., 2018; ram et al., 2018), andconversational breadth like topical diversity (guoet al., 2018).
sporadic attempts have been made todetecting user disengagement in dialogs (yu et al.,2004; ghazarian et al., 2020; choi et al., 2019).
a major bottleneck of these methods is that theyrequire hand-labeling many dialog samples for in-dividual datasets.
although liang et al.
(2020e)denoise user self-reported ratings with the shap-ley algorithm for dialog system evaluation, theirmethod cannot be directly applied to dialogs with-out user ratings as in our setting.
our work isfocusing on the problem that it is expensive anddiﬃcult to obtain user ratings.
the core insight ofour work is to reframe the training data annotationprocess as a process of denoising labels created byheuristic functions pre-deﬁned.
to the best of ourknowledge, we are the ﬁrst to combine automaticdata labeling with the shapley algorithm to performdialog evaluation.
our method could potentiallygeneralize to other classiﬁcation tasks if diﬀerentweak labelers are provided..2.3 learning from weak supervision.
learning from weak supervision reduces annota-tion costs by utilizing noisy but cost-eﬃcient la-bels (ratner et al., 2020, 2016; liang et al., 2020e).
one of the most popular forms of weak supervisionis distant supervision, in which the records of anexternal knowledge base are heuristically alignedwith data points to produce noisy labels for rela-tionship extraction tasks (bunescu and mooney,2007; mintz et al., 2009; hancock et al., 2018).
other applications of weak supervision to scenegraph prediction (krishna et al., 2019), intent clas-siﬁcation (mallinar et al., 2019), and medical imag-.
3653figure 1: schematic of the herald two-stage workﬂow.
stage 1: auto-label training data with heuristic func-tions.
we ﬁrst design heuristics rules for detecting user disengagement by investigating multiple dialog corpora.
the heuristics rules are implemented as heuristic functions based on regular expressions and dialog acts.
then, weuse the heuristic function to label the training set automatically.
stage 2: denoise weakly-labeled training datawith shapley algorithm.
we calculate the shapley value for each data point and correct the noisy data points withnegative shapely values by ﬂipping their labels.
finally, we ﬁne-tune the model on the denoised training data..ing (varma et al., 2017) have observed similar ben-eﬁts in annotation eﬃciency.
unlike the existingwork, we leverage weak supervision to improveannotation eﬃciency for detecting user disengage-ment in social conversations..4 data.
3 problem formulation.
we deﬁned engagement as the degree to whichusers are willing to continue conversing with thedialog system yu et al.
(2016, 2017).
we focus onidentifying the dialog turns with “disengaged” userresponse, since they usually indicate poor conversa-tion experience.
we formulate the user engagementprediction as a binary classiﬁcation problem: ourgoal is to learn a parameterized user engagementpredictor mθ that, given a dialog turn (along withits dialog context) x ∈ x, predicts the turn-leveluser engagement label y ∈ y = {0, 1}, where la-bel y = 1 means “disengaged” and y = 0 means“engaged”.
we start from an unlabeled train setdtrain = {xi}ntrainwithout any label yi.
the test setdtest = {(xi, yi)}ntestcontains the ground-truth labelyi.
the development set ddev has a similar structureas the test set dtest but the development set can bemuch smaller than a train set (i.e., ndev (cid:28) ntrain),making it economical to obtain.
following thegeneral architecture of neural classiﬁers, we for-mulate our model mθ = m(φ, f ) = f (φ(x)): herebert (devlin et al., 2019)-based φ is a text en-coder that maps each dialog turn x to a featurespace φ(x) ∈ rd.
f is the ﬁnal linear layer withsoftmax activation..1.
1.to ensure our framework is generalized to vari-ous corpora, we investigate multiple open-domaindialog datasets ranging from asr-based (gun-rock (liang et al., 2020a)) to text-based (con-vai2 (dinan et al., 2019), blender (roller et al.,2020), and meena (adiwardana et al., 2020)) dia-log systems..gunrock movie dataset gunrock moviedataset consists of dialog data collected fromgunrock, an asr-based open-domain socialchatbot originally designed for amazon alexaprize (liang et al., 2020a).
the gunrock datasetcomes from a user study where in-lab users wererecruited to carry on conversations.
we haveconsent to use the data and we also removed anysensitive information in the conversation.
twodialog experts (co-authors of this paper) randomlyannotated 134 dialogs and split them evenly intothe test set and development set.
in total, theexperts labeled 519 turn-level disengaging userresponses and 2,312 engaging user responses.
they reached a high inter-annotator agreementscore (cohen, 1968) with kappa κ = 0.78. thetraining set contains 276 unlabeled dialogs, with5644 dialog turns.
in addition, we ensure thatthe data annotation is independent of the labelingheuristics collection, so there is no data leakageproblem.
a full example dialog can be found inappendix a.4..convai2 dataset convai2 dataset containstext-based dialog collected from the second conver-.
3654labeling heuristics.
coverage (%).
heuristics group.
disengaged intents.
gunrock convai2.
example disengaged user responses.
(1) complainsystem responses.
complain system repetitioncomplain system ignoring themcomplain system misunderstandingnot understanding systemcurse systemexpress frustration.
(2) dislikecurrent topic.
express negative opinionshow low interests.
(3) request to endtopic or conversation.
request topic changerequest termination.
(4) end withnon-positive responses.
end with negative answerend with unsure answerend with back-channelingend with hesitation.
{ you already asked me that.
| i already told you.
remember? }
{ you’re not listening.
| you didn’t answer my question. }
{ i never said i don’t eat my favorite seafood. }
{ what are you talking about? }
{ you’re dumb. }
{ sigh. }
{ i don’t like music.
| it’s boring. }
{ i don’t care.
}.
1.93.
1.95.
1.90.
3.45.
5.20.
2.92.
{ let’s talk about something else. }
{ stop.
| bye.
}.
20.13.
4.86.
{ no.
| i have not. }
{ i don’t know.
| i don’t remember.
| well, maybe. }
{ yeah.
| okay. }
{ hmm... | that’s a hard one, let me think.
}.
table 1: our labeling heuristics designed to capture user disengagement in dialogs.
a dialog turn is considereddisengaged if any of the heuristic rules apply to the user responses..sational intelligence (convai) challenge (dinanet al., 2019).
we select dialogs from the main eightparticipated chatbots (bot 1, 2, 3, 4, 6, 9, 11) andexclude dialogs that are one-sided or shorter thanthree turns.
the dialog experts annotated 207 di-alogs in total.
the dialogs are evenly distributedover all the eight bots to ensure system diversity,and are randomly sampled within each bot.
theannotated data consist of 209 disengaging turnsand 1684 non-disengaging turns.
they reached ahigh inter-annotator agreement score (cohen, 1968)with kappa κ = 0.76. we split the annotated dialogsevenly into the test set and develop set.
the train-ing set contains 2,226 dialogs, with 18,306 dialogturns..google meena dataset meena (adiwardanaet al., 2020) is the largest end-to-end neural chat-bot so far, trained on 867m public domain socialmedia conversations.
we study the 93 examplehuman-menna conversations released by google..facebook blender dataset the blender bot(roller et al., 2020) is an open-domain chatbotwith several conversational skills: providing engag-ing talking points and listening to their partners,displaying knowledge, empathy, and personalityappropriately while maintaining a consistent per-sona.
we study the 108 example human-blenderconversations released by facebook..5 method.
our goal is to train a user engagement detectorwith minimum data annotation eﬀorts.
traditionalsupervised learning paradigms require annotatingmany training samples.
in addition, it requires addi-tional data annotation to extend the model to a new.
dialog corpus.
to reduce annotation work, we pro-pose herald, a two-stage pipeline that annotateslarge-scale training data eﬃciently and accurately(figure 1).
instead of hand-labeling training datapoints, we use heuristic functions to label eachtraining datum automatically.
the heuristic func-tions are built upon a set of user disengagementheuristics rules.
since the training data are auto-matically labeled, their labels would be noisy.
wethen clean the noisy training data with shapley al-gorithm (ghorbani and zou, 2019) to improve thelabeling accuracy.
the shapley algorithm denoisestraining data by identifying data with wrong labelsand ﬂip their labels.
finally, as we received cleantraining data, we use them to ﬁne-tune a bert-based model and obtain the ﬁnal user disengage-ment detection model..5.1 stage 1: auto-label training data with.
heuristic functions.
since labeling large-scale training data is time-consuming, we propose heuristic labeling functionsto label training data automatically.
the heuristicfunctions focus on detecting disengagement fromuser responses, as it directly indicates poor userexperience.
to build the heuristics functions, weﬁrst summarize the heuristic rules shared amongusers.
we investigate the disengaged dialog turnsfrom the four datasets mentioned above and iden-tify four groups of user disengagement patterns:“complain system responses”, “dislike current top-ics”, “terminate or change topics”, and “end withnon-positive responses” (table 1).
we then discussthe implementation of heuristics functions..36555.1.1 disengagement heuristic rulesgroup 1: complain system responses.
com-plaints are an evident sign of user disengagement.
we identify six related disengaged intents.
theﬁrst three intents (“complain system repetition”,“complain system ignoring them” and “complainsystem misunderstanding”) usually appear whenthe bot makes errors like repeating the same con-tent, ignoring, forgetting, and misunderstanding theuser’s response.
in these cases, users express theirdisengagement by indicating the bot’s error (e.g.
“you already told me that”, “you’re not listening”).
another intent “not understanding system” hap-pens when users cannot understand the system’sresponse (e.g.
“i don’t know what you’re talkingabout.”).
in the last two intents, users reveal nega-tive emotions by cursing the system (e.g.
“you’redumb”) or express frustration (e.g.
“sigh”) aboutthe conversation..group 2: dislike current topics.
when dis-cussing a given topic, users might show their disen-gagement by expressing negative opinions or lowinterest.
for example, given the bot’s response, “iwrite romantic novels under a pen name.
”, forusers who are not interested in reading, users mightsay “reading is boring”, “i don’t like to read”, or“i’m not interested in this”.
we also make sure tohandle the corner cases where the user utteranceshould be labeled as engaged but contains nega-tive opinions.
for instance, to respond to the bot’squestion, “do you want to not work?”, a user mightsay, “yes.
my job is boring.
i have to work withmail”.
though the user mentions a negative feeling(“boring”), the user agrees with the bot and sharesfurther information..group 3: terminate or change topics group 3considers the cases where users express disengage-ment to the current topic in a more straightforwardfashion.
for example, if users are not interested inthe current topic, instead of just expressing theirdislike to it, they may request to switch topics with“let’s talk about something else”.
in some cases,users might show strong disengagement by request-ing to end the conversation if the user is no longerinterested in continuing the conversation..group 4: end with non-positive responses amore subtle but common clue of disengagementis when users end the response with non-positivecontent.
for example, non-positive responses like“i don’t know”, “no”, “yeah”, “uh”, “probably”,.
imply that users do not have much to talk aboutthe current topic.
to keep the precision of ourheuristics high, we carefully consider the coun-terexamples.
one case is that the user follows upwith more responses such as questions (e.g., bot:“have you seen any movies lately?
”, user: “no.
have you?”), and opinion (e.g.
bot: “what’s yourfavorite animation movie?”, user: “i don’t know,but it might actually be frozen two.
my sister lovesit.”) in the same dialog turn.
these turns shouldnot be labeled as disengaged since the user is stillinterested in sharing more content or asking follow-up questions.
therefore, we take a conservativeapproach: we label the dialog turn as disengagedonly if no more responses follow the non-positiveresponse..5.1.2 heuristic functions implementation.
next, we discuss how to use heuristic functions toauto-label disengaged user utterances.
first, wesplit user responses into segments since user re-sponses may consist of multiple units with diﬀer-ent semantic meanings.
we use nltk sentencetokenizer for text-based system, and a segmenta-tion model (chen et al., 2018) for asr (automaticspeech recognition)-based system as the segmen-tation tool.
we then apply the heuristic functionson each segment to detect disengaged intents.
forheuristic groups 1 to 3, if any segment contains adisengaged intent, the user response is auto-labeledas disengaged.
for heuristic group 4 (“end withnon-positive responses”), we assign disengaged la-bels only if the disengaged intents are detected inthe last segment..we detect disengaged intents with regexes.
thebeneﬁt of using regexes is that they have mini-mum dependencies and are easy to modify.
wedesign regexes for each intent.
following com-mon regexes complexity metrics (luo et al., 2018),our regexes for each intent contains 43.9 regexesgroups and 87.7 or clauses on average..our framework also supports incorporating ad-ditional resources to improve the intent detectionaccuracy for automatic training data labeling.
forexample, we can enhance the recall of regexesintent detection by incorporating existing deeplearning-based nlu (natural language under-standing) models.
speciﬁcally, we re-purpose anopen-sourced dialog act classiﬁcation model (yuand yu, 2021) to enhance disengagement intentdetection: we select 6 out of the 23 supporteddialog act labels that are associated with disen-.
3656gaged intents, and map each selected dialog actlabel to the heuristic groups.
the dialog act “com-plaint” is mapped to the heuristic group “com-plain system repetition”;“closing” is mapped tothe disengaged intent “request termination”; “hold”to “hesitation”;“other_answers” to “unsure an-swer”; “back-channeling” to “back-channeling”,and “neg_answer“ to ‘negative answer‘”.
if a userutterance is detected with disengaged intent by ei-ther regexes or the deep learning model, then theutterance is auto-labeled as disengaged..5.2 stage 2: denoise with shapley algorithm.
& fine-tune.
overview next, we denoise the labeled data us-ing shapley algorithm (ghorbani and zou, 2019).
shapley algorithm has been studied in the co-operative game theory (dubey, 1975) and eco-nomics (gul, 1989) as a fair distribution method.
shapley algorithm computes a shapley value foreach training datum, which quantiﬁes the contribu-tion of each training datum to the prediction andperformance of a deep network.
low shapley valuedata capture outliers and corruptions.
therefore,we can identify and denoise the incorrectly labeleddata by computing their shapley values and ﬁne-tune the model on the cleaned training set..shapley algorithm shapley algorithm comesoriginally from cooperative game theory (dubey,1975).
consider a cooperative game with n playersd = {1, ..., n} and a utility function v : 2[n] → rwhich assigns a reward to each of 2n subsets ofplayers: v(s ) is the reward if the players in subsets ⊆ d cooperate.
shapley value deﬁnes a uniquescheme to distribute the total gains generated bythe coalition of all players v(d) with a set of ap-pealing mathematical properties.
in our setting, wecan consider dtrain = {(xi, yi)}ntrainas ntrain players.
we deﬁne the utility function v(s ) as the perfor-mance on the development set ddev.
the shapleyvalue for player i is deﬁned as the average marginalcontribution of {(xi, yi)} to all possible subsets thatare formed by other players (jia et al., 2019a,b):.
1.
(cid:88).
si = 1n.s ⊆dtrain\{xi}.
1(cid:16)n−1|s |.
(cid:17) [v(s ∪ {xi}) − v(s )].
as suggested by the deﬁnition of shapley value,computing shapley value requires an exponen-tially large number of computations to enumer-ate o(2ntrain) possible subsets and train the modelmθ on each subset, which is intractable.
inspired.
by (jia et al., 2019a,b), herald tackles thisissue by reducing the deep model mθ to a k-nearest neighbors (knn) model and then applythe closed-form solution of shapley value on knn:we reduce our bert-based classiﬁcation modelmθ = m(φ, f ) = f (φ(x)) to a knn by ﬁrst ﬁne-tuning mθ on the auto-labeled training samples.
we then use the feature extractor φ to map eachtraining datum to the feature space {φ(xi)}ntrain.
weconstruct a knn classiﬁer in the feature space tocompute the closed-form shapley value..1.next, we discuss the closed-form solution ofshapley value.
we ﬁrst consider a special casewhere the development set ddev only contains onedatum ddev = {(xdev, ydev)}.
given any nonemptysubset s ⊆ dtrain, we use the knn classiﬁer toclassify xdev.
to do this, we sort the data points inthe training set {xi}ntrainbased on their euclidean dis-tance in the feature space φ(x) to the datum in thedevelopment set xdev, yielding (xα1, xα2, ..., xα|s |)with xα1, ..., xαk as the top-k most similar datapoints to xdev.
the knn classiﬁer outputs theprobability of xdev taking the label ydev as p[xdev →= ydev], where αk is the indexydev] = 1kof the kth nearest neighbor.
we deﬁne the utilityfunction as the likelihood of the correct label:.
1[yαk.
(cid:80)k.k=1.
1.ν(s ) = 1k.min{k,|s |}(cid:88).
k=1.
1[yαk(s ) = ydev].
(1).
jia et al.
(2019a,b) proves that the shapley value ofeach training point sαi can be calculated recursivelyin o(n log n) time as follows:.
1[yαn.
=.
sαn.
sαi.
= sαi+1.
= ydev]n+ min{k, i}i × k.(cid:0)1[yαi.
=ydev]−1[yαi+1.
=ydev](cid:1).
the above result for a single point in ddev couldbe readily extended to the multiple-point case, inwhich the utility function is deﬁned by.
ν(s ) = 1ndev.
ndev(cid:88).
j=1.
1k.min{k,|s |}(cid:88).
k=1.
1[yα( j).
k (s ).
= ydev, j].
where α( j)k (s ) is the index of the kth nearest neigh-bor in s to xdev, j. jia et al.
(2019a,b) also provethat the shapley value in this case is the average ofthe shapley value for every single dev point..denoising procedure our denoising procedureworks as follows: (1) we ﬁrst ﬁne-tune our bert-based classiﬁcation model mθ = m(φ, f ) = f (φ(x)).
3657no..method.
gunrock movie convai2.
bacc f2score bacc f2score.
(1)heuristics 78.32(2) heuristics (regex only) 62.81(3) heuristics (nlu only) 72.68(4) heuristics w/o group 1 78.21(5) heuristics w/o group 2 77.96(6) heuristics w/o group 3 71.52(7) heuristics w/o group 4 58.34.
65.0935.4656.32.
64.8864.4955.3623.97.
76.58 58.1672.04 49.9063.62 32.86.
71.20 48.4475.45 56.2271.96 49.8068.32 42.68.
(8)(9)(10)(11).
bert(dev) 73.98bert(auto) 80.55bert(auto+dev) 80.73.
74.97 55.4078.76 63.1380.46 64.54herald 86.17* 80.01* 86.22* 70.49*.
60.7471.7772.16.table 2: evaluation results comparison among variantsof herald.
* indicates that the model is statisticallysigniﬁcantly better than baseline models.
all numbersin the table are in percentage..1.on the auto-labeled training samples.
this step in-jects the knowledge in the labeling heuristic into themodel mθ.
(2) we then map each auto-labeled train-ing datum to the feature space {φ(xi)}ntrain, since wewant to apply the closed-form knn formula ofshapley value in the feature space.
(3) next, fora binary classiﬁcation problem, we duplicate eachtraining datum 2 times with labels [0, 1].
this gen-erates a large training set dlarge with 2 × ntrain datapoints, and we note that the origin training set dtrainis a subset of dlarge, since dlarge enumerates all cpossible labels for each each training datum.
(4)we then calculate shapley value for the 2 × ntraindata points in dlarge using the closed-form knnformula.
(5) we remove the data with negativeshapley value in dlarge, and get a cleaned trainingset dclean.
the duplicate-and-remove procedure“ﬂips” the labels of the noisy data points with lowshapley value.
(6) finally, we ﬁne-tune the clas-siﬁcation model mθ on dclean to get the ﬁnal userdisengagement detection model..to sum up, the shapley value quantiﬁes the con-tribution of each training datum.
low shapleyvalue data capture outliers and corruptions that arenot consistent with the distribution of other datapoints.
we identify and correct these outliers andcorruptions to provide a clean training set..6 experiments.
model setup we use k = 10 for the knnclassiﬁer.
we use bert (devlin et al., 2019)as the text encoder φ of our classiﬁcation modelmθ = m(φ, f ) = f (φ(x)).
additional implementa-.
tion details are included in appendix..model comparisons and ablations we com-pare herald to its several ablations (table 2)and evaluate the performance on the test set.
wereport balanced accuracy (bacc) and fβ scorewith β = 2 (baeza-yates et al., 1999).
(1) heuris-tics uses the labeling heuristic function with bothregex and dialog act to predict the test set.
(2)heuristics (regex only) uses the labeling heuristicfunction only with regex to predict on the test set.
(3) heuristics (nlu only) uses the labeling heuris-tic function only with nlu.
(4-7) show the ablationof the heuristics function prediction baseline by ex-cluding each heuristic group.
(8) bert(dev) ﬁne-tunes bert on the expert-annotated developmentset.
(9) bert(auto) ﬁne-tunes bert on the auto-labeled training samples.
(10) bert(auto+dev)ﬁne-tunes bert on both the auto-labeled trainingsamples and the development set.
(11) heraldreports the performance of the ﬁnal model trainedon dclean..results our ﬁrst takeaway is that our labelingheuristics produce decent predictions and gener-alize to diﬀerent datasets.
as shown in table 2,heuristics prediction (heuristic, 78.32%, 76.58%)is better than the bert-based model with limitedtraining samples (bert(dev), 73.98%, 74.94%)on both datasets.
it also shows that our labelingheuristics are generalizable to diﬀerent corpora..our second takeaway is that learning from alarge number of noisy labels works better thanlearning from a limited number of clean labels.
as shown in table 2, bert ﬁne-tuned on the auto-labeled training set (bert(auto), 80.55, 78.76)outperforms bert ﬁne-tuned on clean but smalldevelopment set (bert(dev), 73.98, 74.94) by alarge margin.
in addition, we also observe that thebert model ﬁne-tuned on the auto labeled trainingdata (bert(auto), 80.55%, 78.76%) generalizesbeyond the labeling heuristics (heuristics, 78.32%,76.58%)..our third takeaway is that using the expert-annotated development setfor denoising ismore eﬃcient than using the development setas additional training data.
after ﬁne-tuningbert on the weakly labeled training data(bert(auto), 80.55%, 78.76%), having an ad-ditional ﬁne-tuning step using the developmentset slightly improves the model’s performance(bert(auto+dev), 80.73%, 80.46%).
in contrast,.
3658using the development set for the shapley denois-ing algorithm gives a signiﬁcant performance gain(herald, 86.17%, 86.22%)..their responses.
hence, heuristics group 4 coveringthese responses happen more in gunrock moviethan convai2..generalizability of heuristic functions theresults show that our heuristic functions are gener-alized to both asr-based and text-based systems.
as indicated in table 2, our regexes reach a de-cent accuracy of 62.81% and 72.04% on the expertannotated test set respectively on gunrock movieand convai2 dataset, and thus can serve as a rela-tively reliable source for auto-labeling.
in addition,although the dialog act model (midas) is initiallydesigned for asr-based systems and thus has abetter performance on the gunrock movie data, itshould be generalizable to other asr-based sys-tems, as the six selected dialog acts are general andindependent of topics.
therefore, the combinationof dialog acts and regexes should be suﬃcient tobe applied to various corpora..figure 3: an example dialog turn from the gun-rock movie dataset with an incorrect auto label “non-in this case,disengaged” identiﬁed by data shapley.
the user actually says “i don’t wanna talk about moviesanymore,” but an asr error happens, and thus the la-beling heuristics fail to capture this dialog turn..figure 4: an example dialog turn from gunrock moviedataset that is incorrectly auto-labeled as “disengaged”because the labeling heuristics see the negative word“disagree”.
this data point is also identiﬁed and cor-rected by data shapley..shapley value analysis we also present an anal-ysis to show how shapley denoising works, asshown in figure 2. we examine the shapley valuefor each training datum in stage 2. we ﬁrst showtwo example dialog turns from the gunrock moviedataset with a negative shapley value in figure 3and figure 4. in figure 3, the dialog turn is incor-rectly auto-labeled as “non-disengaged”.
this isbecause an asr error happens, and the user utter-ance “i don’t wanna talk about movies anymore”.
figure 2: removing data with low shapley values(shapley with ktest = 1, 5, 10, 25, 50) improves theperformance of the knn in gunrock movie datasetwhile removing data with high shapley values and re-tain data with low shapley values (“retainhurtful”)leads to worse performance..annotation cost the cost of annotating thedev set is small for the shapley algorithm.
forgunrock movie dataset, we used 67 annotated di-alogs as the dev set.
for convai2, we used 52annotated dialogs as the dev set.
the annotationtakes less than 1 hour in both cases, which is negli-gible compared to the cost of annotating all trainingdata..heuristics group analysis we perform abla-tion studies to analyze the importance of each ofthe four heuristics groups in table 1. as shown intable 2, excluding heuristics group 4 leads to themost signiﬁcant performance drop in both datasets(heuristics w/o group 4, 58.34%, 68.32%), indi-cating that “end with non-positive response” is themost prevalent form of user disengagement..in addition, each heuristics group has diﬀer-ent importance in diﬀerent datasets.
for exam-ple, dropping heuristics group 1 (“complain systemresponses”) only leads to a marginal performancedrop on the gunrock movie dataset but incurs a sig-niﬁcant performance drop on the convai2 dataset.
we also notice that heuristic group 4 (“end withnon-positive responses”) plays a more critical rolein the gunrock movie dataset than in the convai2dataset.
this might be mainly due to the diﬀer-ence between asr-based (gunrock movie) andtext-based (convai2) systems.
when asked anopen-ended question in asr-based systems, sinceusers have less time to think, they are more likelyto reply with responses such as “i’m not sure”,“let me think”.
while in text-based systems (con-vai2), users have more time to think and formulate.
3659is transcribed as “i wanna talk about movies any-more”.
in figure 4, the user says, “oh i disagree.
i think the movie was fantastic!”.
the labelingheuristics see the negative word “disagree” andauto-label this turn as “disengaged”.
both datapoints are with negative shapley values and arecorrected in stage 3..next, we present a quantitative analysis of shap-ley value.
according to the shapley value, weremove data points one by one, starting from theleast valuable (low shapley values) to the mostvaluable (high shapley values).
each time, afterremoving the data point, we create new knn clas-siﬁer models on the remaining dialog turns andlabels and evaluate them on the test set with ex-pert annotations.
as shown in figure 2, removingtraining data with low shapley values increases theperformance to a certain point before convergencefor k of all choices.
we observe a similar trendwhen re-training a model on the remaining data.
incontrast, removing data randomly or removing datastarting from high shapley values decreases the per-formance on the test set (“random” and “retain-hurtful” in figure 2).
this shows that low shapleyvalue data eﬀectively capture outliers and corrup-tions, which further justiﬁes our design choice ofdenoising with shapley value..alternative data valuation methods we alsoexplored alternative methods to data shapley likeinﬂuence function (koh and liang, 2017) andtracin (pruthi et al., 2020): on gunrock movie,inﬂuence functions and tracin achieve 82.96%and 83.15% accuracy, respectively.
both meth-ods outperform bert(auto+dev) (80.73%) signif-icantly but perform slightly worse than herald(86.17%).
overall, results show that our data an-notation workﬂow also works well with other datavaluation methods..figure 5: an error case where the low engagement dia-log turn that is not captured by herald..error analysis figure 5 shows an error exam-ple of herald, where both the labeling heuris-tics and the shapley algorithm fail to identify thisturn as low engagement.
in this example, the chat-bot system asks whether the user is interested in.
movies, but the user does not directly answer thequestion.
instead, the user says “i have a ques-tion for you social bot”, indicating that the userdoes not like the current topic and wants to talkabout something else.
herald fails to identifythis dialog turn as low engagement, partly becausethe regexes in the “request topic change” heuris-tic rule does not cover this example.
one way toﬁx this error is to upgrade the regexes.
a moregeneral solution is to consider the chatbot system’sexpectations on user responses conditioned on thechatbot’s question.
if the chatbot receives an “un-expected” user response, then the user is probablynot interested in discussing the current topic..7 conclusion.
the ultimate chatbot evaluation metric should beuser-centric, as chatbots are there to provide hu-mans with enjoyable experiences.
previously de-tecting user disengagement typically requires an-notating many dialog samples for each individualdataset.
we propose a two-stage pipeline her-ald to automatically label and denoise trainingdata and, at the same time, build a user disengage-ment detector.
our experiment shows that her-ald signiﬁcantly reduces the annotation cost ofa new corpus.
herald’s disengagement detec-tion results highly correlate with expert judgmentson user disengagement in both datasets (86.17%bacc in gunrock movie, 86.22% in convai2)..acknowledgments.
we thank acl 2021 chairs and reviewers fortheir review eﬀorts and constructive feedback.
wewould also like to thank yu li and minh nguyenfor revising the regexes..references.
daniel adiwardana, minh-thang luong, david r. so,jamie hall, noah fiedel, romal thoppilan, zi yang,apoorv kulshreshtha, gaurav nemade, yifeng lu,and quoc v. le.
2020. towards a human-like open-domain chatbot.
corr, abs/2001.09977..ricardo baeza-yates, berthier ribeiro-neto, et al.
1999. modern information retrieval, volume 463.acm press new york..satanjeev banerjee and alon lavie.
2005. meteor:an automatic metric for mt evaluation with im-proved correlation with human judgments.
in iee-valuation@acl, pages 65–72.
association for com-putational linguistics..3660razvan c. bunescu and raymond j. mooney.
2007.learning to extract relations from the web using min-imal supervision.
in acl.
the association for com-putational linguistics..chun-yen chen, dian yu, weiming wen, yi mangyang, jiaping zhang, mingyang zhou, kevin jesse,austin chau, antara bhowmick, shreenath iyer,et al.
2018. gunrock: building a human-like socialbot by leveraging large scale real user data.
alexaprize proceedings..jason ingyu choi, ali ahmadvand, and eugeneagichtein.
2019. oﬄine and online satisfaction pre-diction in open-domain conversational systems.
incikm, pages 1281–1290.
acm..jacob cohen.
1968. weighted kappa: nominal scaleagreement provision for scaled disagreement or par-tial credit.
psychological bulletin, 70(4):213..jacob devlin, ming-wei chang, kenton lee, andkristina toutanova.
2019. bert: pre-training ofdeep bidirectional transformers for language under-standing.
in naacl-hlt (1), pages 4171–4186.
as-sociation for computational linguistics..emily dinan, varvara logacheva, valentin malykh,jack urbanek,alexander miller, kurt shuster,douwe kiela, arthur szlam, iulian serban, ryanthe second conversationallowe, et al.
2019.arxiv preprintintelligence challenge (convai2).
arxiv:1902.00098..pradeep dubey.
1975. on the uniqueness of the shap-international journal of game theory,.
ley value.
4(3):131–139..gabriel.
joelle.
pineau,.
forgues,.
jean-marielarchevêque, and réal tremblay.
2014.boot-strapping dialog systems with word embeddings.
in nips, modern machine learning and naturallanguage processing workshop, volume 2..sarik ghazarian, ralph m. weischedel, aram gal-styan, and nanyun peng.
2020. predictive engage-ment: an eﬃcient metric for automatic evaluationof open-domain dialogue systems.
in aaai, pages7789–7796.
aaai press..amirata ghorbani and james y. zou.
2019. data shap-ley: equitable valuation of data for machine learn-ing.
in icml, volume 97 of proceedings of machinelearning research, pages 2242–2251.
pmlr..faruk gul.
1989. bargaining foundations of shapleyvalue.
econometrica: journal of the econometricsociety, pages 81–95..fenfei guo, angeliki metallinou, chandra khatri,anirudh raju, anu venkatesh, and ashwin ram.
2018. topic-based evaluation for conversationalbots.
corr, abs/1801.03622..prakhar gupta, shikib mehri, tiancheng zhao, amypavel, maxine eskénazi, and jeﬀrey p. bigham.
2019.investigating evaluation of open-domain di-alogue systems with human generated multiple ref-erences.
corr, abs/1907.10568..braden hancock, antoine bordes, pierre-emmanuelmazaré, and jason weston.
2019. learning fromdialogue after deployment: feed yourself, chatbot!
in acl (1), pages 3667–3684.
association for com-putational linguistics..braden hancock, paroma varma, stephanie wang,martin bringmann, percy liang, and christopherré.
2018. training classiﬁers with natural languageexplanations.
in acl (1), pages 1884–1895.
associ-ation for computational linguistics..ruoxi jia, david dao, boxin wang, frances ann hu-bis, nezihe merve gürel, bo li, ce zhang, costas j.spanos, and dawn song.
2019a.
eﬃcient task-speciﬁc data valuation for nearest neighbor algo-rithms.
pvldb, 12(11):1610–1623..ruoxi jia, david dao, boxin wang, frances annhubis, nick hynes, nezihe merve gürel, bo li,ce zhang, dawn song, and costas j. spanos.
2019b.
towards eﬃcient data valuation based on the shapleyvalue.
in aistats, volume 89 of proceedings of ma-chine learning research, pages 1167–1176.
pmlr..pang wei koh and percy liang.
2017. understand-ing black-box predictions via inﬂuence functions.
inicml, volume 70 of proceedings of machine learn-ing research, pages 1885–1894.
pmlr..ranjay krishna, vincent s. chen, paroma varma,michael bernstein, christopher ré, and fei-fei li.
2019. scene graph prediction with limited labels.
iniccv, pages 2580–2590.
ieee..margaret li, jason weston, and stephen roller.
2019.acute-eval: improved dialogue evaluation withoptimized questions and multi-turn comparisons.
corr, abs/1909.03087..kaihui liang, austin chau, yu li, xueyuan lu, dianyu, mingyang zhou, ishan jain, sam davidson, josharnold, minh nguyen, et al.
2020a.
gunrock 2.0: auser adaptive social conversational system.
arxivpreprint arxiv:2011.08906..weixin liang, yanhao jiang, and zixuan liu.
2021.graghvqa: language-guided graph neural net-works for graph-based visual question answering.
inmai@naacl-hlt.
association for computationallinguistics..weixin liang, feiyang niu, aishwarya n. reganti,govind thattai, and gökhan tür.
2020b.
lrta:a transparent neural-symbolic reasoning frameworkwith modular supervision for visual question answer-ing.
corr, abs/2011.10731..3661weixin liang, youzhi tian, chengcai chen, and zhouyu.
2020c.
moss: end-to-end dialog system frame-in aaai, pageswork with modular supervision.
8327–8335.
aaai press..weixin liang and james zou.
2021. neural group test-in ieee interna-ing to accelerate deep learning.
tional symposium on information theory, isit 2021.ieee..weixin liang, james zou, and zhou yu.
2020d.
al-ice: active learning with contrastive natural lan-in emnlp (1), pages 4380–guage explanations.
4391. association for computational linguistics..weixin liang, james zou, and zhou yu.
2020e.
be-yond user self-reported likert scale ratings: a com-parison model for automatic dialog evaluation.
inacl, pages 1363–1374.
association for computa-tional linguistics..chin-yew lin.
2004. rouge: a package for auto-matic evaluation of summaries.
in text summariza-tion branches out, pages 74–81, barcelona, spain.
association for computational linguistics..chia-wei liu, ryan lowe, iulian serban, michaelnoseworthy, laurent charlin, and joelle pineau.
2016. how not to evaluate your dialogue system:an empirical study of unsupervised evaluation met-in emnlp,rics for dialogue response generation.
pages 2122–2132.
the association for computa-tional linguistics..ryan lowe, michael noseworthy, iulian vlad ser-ban, nicolas angelard-gontier, yoshua bengio, andjoelle pineau.
2017. towards an automatic turingtest: learning to evaluate dialogue responses.
inacl (1), pages 1116–1126.
association for compu-tational linguistics..bingfeng luo, yansong feng, zheng wang, songfanghuang, rui yan, and dongyan zhao.
2018. mar-rying up regular expressions with neural networks:a case study for spoken language understanding.
arxiv preprint arxiv:1805.05588..neil mallinar, abhishek shah, rajendra ugrani, ayushgupta, manikandan gurusankar, tin kam ho,q. vera liao, yunfeng zhang, rachel k. e. bel-lamy, robert yates, chris desmarais, and blake mc-gregor.
2019. bootstrapping conversational agentswith weak supervision.
in aaai, pages 9528–9533.
aaai press..shikib mehri and maxine eskénazi.
2020. unsuper-vised evaluation of interactive dialog with dialogpt.
in sigdial, pages 225–235.
association for compu-tational linguistics..mike mintz, steven bills, rion snow, and daniel ju-rafsky.
2009. distant supervision for relation extrac-in acl/ijcnlp, pagestion without labeled data.
1003–1011.
the association for computer linguis-tics..jekaterina novikova, ondrej dusek, amanda cercascurry, and verena rieser.
2017. why we neednew evaluation metrics for nlg.
in emnlp, pages2241–2252.
association for computational linguis-tics..kishore papineni, salim roukos, todd ward, and wei-jing zhu.
2002. bleu: a method for automatic eval-uation of machine translation.
in acl, pages 311–318. acl..garima pruthi, frederick liu, satyen kale, andmukund sundararajan.
2020. estimating trainingdata inﬂuence by tracing gradient descent.
inneurips..ashwin ram, rohit prasad, chandra khatri, anuvenkatesh, raefer gabriel, qing liu, jeﬀ nunn,behnam hedayatnia, ming cheng, ashish nagar,eric king, kate bland, amanda wartick, yi pan,han song, sk jayadevan, gene hwang, and art pet-tigrue.
2018. conversational ai: the science behindthe alexa prize.
corr, abs/1801.03604..alexander ratner, stephen h. bach, henry r. ehren-berg, jason a. fries, sen wu, and christopher ré.
2020. snorkel:rapid training data creation withweak supervision.
vldb j., 29(2-3):709–730..alexander j. ratner, christopher de sa, sen wu,daniel selsam, and christopher ré.
2016. data pro-gramming: creating large training sets, quickly.
innips, pages 3567–3575..stephen roller, emily dinan, naman goyal, da ju,mary williamson, yinhan liu, jing xu, myle ott,kurt shuster, eric michael smith, y-lan boureau,and jason weston.
2020. recipes for building anopen-domain chatbot.
corr, abs/2004.13637..vasile rus and mihai c. lintean.
2012. a compar-ison of greedy and optimal assessment of naturallanguage student input using word-to-word similar-ity metrics.
in bea@naacl-hlt, pages 157–162.
the association for computer linguistics..chongyang tao, lili mou, dongyan zhao, and ruiyan.
2018. ruber: an unsupervised method for au-tomatic evaluation of open-domain dialog systems.
in aaai, pages 722–729.
aaai press..paroma varma, bryan d. he, payal bajaj, nishithkhandwala, imon banerjee, daniel l. rubin, andchristopher ré.
2017.inferring generative modelstructure with static analysis.
in nips, pages 240–250..anu venkatesh, chandra khatri, ashwin ram, fenfeiguo, raefer gabriel, ashish nagar, rohit prasad,ming cheng, behnam hedayatnia, angeliki metalli-nou, rahul goel, shaohua yang, and anirudh raju.
2018. on evaluating and comparing conversationalagents.
corr, abs/1801.03625..3662huichuan xia and brian mckernan.
2020. privacya review of the threats andin crowdsourcing:challenges.
comput.
support.
cooperative work.,29(3):263–301..sanghyun yi, rahul goel, chandra khatri, tagyoungchung, behnam hedayatnia, anu venkatesh, raefergabriel, and dilek hakkani-tür.
2019. towards co-herent and engaging spoken dialog response genera-tion using automatic conversation evaluators.
corr,abs/1904.13015..chen yu, paul m. aoki, and allison woodruﬀ.
2004.detecting user engagement in everyday conversa-tions.
in interspeech.
isca..dian yu and zhou yu.
2021. midas: a dialog act an-notation scheme for open domain human machinein proceedings of the 16thspoken conversations.
conference of the european chapter of the associ-ation for computational linguistics: main volume,page 1103–1120..zhou yu, leah nicolich-henkin, alan w. black, andalexander i. rudnicky.
2016. a wizard-of-oz studyon a non-task-oriented dialog systems that reacts toin sigdial conference, pagesuser engagement.
55–63.
the association for computer linguistics..zhou yu, vikram ramanarayanan, patrick l. lange,and david suendermann-oeft.
2017. an open-source dialog system with real-time engagementtracking for job interview training applications.
iniwsds, volume 510 of lecture notes in electricalengineering, pages 199–207.
springer..3663a appendix.
a.1implementation details of heraldwe use k = 10 for the knn regressor.
we loadand ﬁne-tune pre-trained bert as the feature ex-tractor φ. the details of extending bert to encodemulti-turn dialogs are as follows.
each dialog turn(along with its dialog context) is represented asa sequence of tokens in the following input for-mat (liang et al., 2020c): starting with a specialstarting token [cls ], we concatenate tokenizeduser and system utterances in chronological orderwith [s ep] as the separators for adjacent utter-ance.
in other words, we represent each dialogas a sequence: [cls ], s 1,1, s 1,2, ..., [s ep], u1,1,u1,2, ..., [s ep], s 2,1, s 2,2, ..., [s ep] where s i, jand ui, j are the jth token of the system and userutterance in the ith turn.
following bert, we alsoadd a learned embedding to every token indicatingwhether it comes from user utterances or systemutterances .
in addition, since the disengaging classand the non-disengaging class are imbalanced, weup-sample the disengaging dialog turns for boththe training set and the development set.
thoughit is also possible to handle the imbalanced classesby adding weights for two classes, we did not takethis approach because we do not have a closed-form solution for calculating the shapley value forweighted knn in o(n log n) time.
improving thearchitecture of herald and extending heraldto other machine learning tasks (liang and zou,2021; liang et al., 2020d,b, 2021) are interestingdirections of future work..a.2 reproducibility.
the source code of herald can be found in thesupplementary materials.
we run experimentson a server of eight gtx 1080 gpus.
theaverage runtime for all stages of herald is lessthan 10 minutes.
the number of parameters issimilar to bert.
we use the default hyperparam-eters of bert.
the public examples of googlemeena dataset can be downloaded from https://github.com/google-research/google-research/blob/master/meena/meena.txt the public examplesof facebook blender dataset can be down-loaded from https://parl.ai/projects/recipes/chatlog_2.7b_render.html the public examplesof convai2 dataset can be downloaded fromhttp://convai.io/data/data_volunteers.json andhttp://convai.io/data/summer_wild_evaluation_.
dialogs.json.
(a) denoising with shapley value in gunrockmovie dataset.
(b) denoising with shapley value in convai2dataset.
figure 6: removing data points with low shapley valueimproves the performance of the knn classiﬁer..additional shapley value analysis we alsopresent addition analysis to show how shapley de-noising works as shown in figure 6. we presentthe experiments on both gunrock movie datasetand convai2 dataset.
figure 6 presents a quanti-tative analysis of shapley value.
according to theshapley value, we remove data points one by onestarting from the least valuable to the most valu-able.
each time, after the data point is removed, wecreate new knn classiﬁer models on the remain-ing dialog turns and labels and evaluate them onthe test set with expert annotations.
as shown infigure 6, removing training data with low shap-ley values increases the performance to a certainpoint before convergence for k of all choices.
weobserve a similar trend when re-training a modelon the remaining data.
in contrast, removing datarandomly or removing data from the most to leastvaluable data decreases the performance on the testset.
this shows that low shapley value data ef-fectively capture outliers and corruptions, whichfurther justiﬁes our design choice of denoising withshapely value..a.3 addition dialog examples.
we show additional dialog examples.
figure 7shows a full dialog example from convai dataset.
figure 8 shows a full dialog example from gunrockmovie dataset..3664figure 7: a full example from convai dataset..figure 8: a full example from gunrock movie dataset..3665