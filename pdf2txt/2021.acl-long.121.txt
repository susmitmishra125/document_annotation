mect: multi-metadata embedding based cross-transformer forchinese named entity recognition.
shuang wu1, xiaoning song1∗, zhenhua feng2,3∗1school of artiﬁcial intelligence and computer science, jiangnan university, china2department of computer science, university of surrey, uk3centre for vision, speech and signal processing, university of surrey, ukshuangwu@stu.jiangnan.edu.cnx.song@jiangnan.edu.cn, z.feng@surrey.ac.uk.
abstract.
recently, word enhancement has become verypopular for chinese named entity recogni-tion (ner), reducing segmentation errors andincreasing the semantic and boundary informa-tion of chinese words.
however, these meth-ods tend to ignore the information of the chi-nese character structure after integrating thelexical information.
chinese characters haveevolved from pictographs since ancient times,and their structure often reﬂects more informa-tion about the characters.
this paper presents anovel multi-metadata embedding based cross-transformer (mect) to improve the perfor-mance of chinese ner by fusing the structuralinformation of chinese characters.
speciﬁ-cally, we use multi-metadata embedding in atwo-stream transformer to integrate chinesecharacter features with the radical-level em-bedding.
with the structural characteristicsof chinese characters, mect can better cap-ture the semantic information of chinese char-acters for ner.
the experimental results ob-tained on several well-known benchmarkingdatasets demonstrate the merits and superior-ity of the proposed mect method.1.
1.introduction.
named entity recognition (ner) plays an essen-tial role in structuring of unstructured text.
itis a sequence tagging task that extracts namedentities from unstructured text.
common cate-gories of ner include names of people, places,organizations, time, quantity, currency, and someproper nouns.
ner is the basis for many nat-ural language processing (nlp) tasks such asevent extraction (chen et al., 2015), question an-swering (diefenbach et al., 2018), information re-.
∗corresponding author.
1the source code of the proposed method is publiclyat https://github.com/codermusou/.
availablemect4cner..character.
cr.
题 (topic)榆 (elm)渡 (ferry)脸 (face).
页木氵月.ht.
是页木俞氵度月佥.
sc.
日一走页木人一月刂氵广廿又月人一ツ一.
table 1: structure decomposition of chinese charac-ters: ‘cr’ denotes the chinese radical, ‘ht’ denotesthe head and tail, and ‘sc’ denotes the structural com-ponents of chinese characters..trieval (khalid et al., 2008), knowledge graph con-struction (riedel et al., 2013), etc..compared with english, there is no space be-tween chinese characters as word delimiters.
chi-nese word segmentation is mostly distinguishedby readers through the semantic information ofsentences, posing many difﬁculties to chinesener (duan and zheng, 2011; ma et al., 2020).
besides, the task also has many other challenges,such as complex combinations, entity nesting, andindeﬁnite length (dong et al., 2016)..in english, different words may have the sameroot or afﬁx that better represents the word’s seman-tics.
for example, physiology, psychology, sociol-ogy, technology and zoology contain the same suf-ﬁx, ‘-logy’, which helps identify the entity of a sub-ject name.
besides, according to the informationof english words, root or afﬁxes often determinegeneral meanings (yadav et al., 2018).
the root,such as ‘ophthalmo-’ (ophthalmology), ‘esophage-’ (esophagus) and ‘epithelio-’ (epithelium), canhelp human or machine to better recognize profes-sional nouns in medicine.
therefore, even the state-of-the-art methods, such as bert (devlin et al.,2019) and gpt (radford et al., 2018), trained onlarge-scale datasets, adopt this delicate word seg-mentation method for performance boost..for chinese characters, there is also a structure.
proceedingsofthe59thannualmeetingoftheassociationforcomputationallinguisticsandthe11thinternationaljointconferenceonnaturallanguageprocessing,pages1529–1539august1–6,2021.©2021associationforcomputationallinguistics1529radicals.
denotation.
examples.
鸟 (bird)艹 (grass)月 (meat).
birdsherbaceous plantsbody parts.
鸡 (chicken), 鸭 (duck), 鹅 (goose), 鹰 (eagle)花 (flower), 草 (grass), 菜 (vegetable), 茶 (tea)肾 (kidney), 脚 (foot), 腿 (leg), 脑 (brain).
table 2: some examples of chinese radicals, including ‘鸟’(bird), ‘艹’(grass) and ‘月’(meat)..similar to the root and afﬁxes in english.
accord-ing to the examples in table 1, we can see thatthe structure of chinese characters has differentdecomposition methods, including the chinese rad-ical (cr), head and tail (ht) and structural compo-nents (sc).
chinese characters have evolved fromhieroglyphs since ancient times, and their structureoften reﬂects more information about them.
thereare some examples in table 2. the glyph structurecan enrich the semantics of chinese characters andimprove the performance of ner.
for example, thebi-lstm-crf method (dong et al., 2016) ﬁrstlyobtains character-level embedding through the dis-assembly of chinese character structure to improvethe performance of ner.
however, lstm is basedon time series modeling, and the input of each celldepends on the output of the previous cell.
so thelstm-based model is relatively complicated andthe parallel ability is limited..to address the aforementioned issues, wetake the advantages of flat-lattice transformer(flat) (li et al., 2020) in efﬁcient parallel com-puting and excellent lexicon learning, and intro-duce the radical stream as an extension on its ba-sis.
by combining the radical information, we pro-pose a multi-metadata embedding based cross-transformer (mect).
mect has the lattice- andradical-streams, which not only possesses flat’sword boundary and semantic learning ability butalso increases the structure information of chinesecharacter radicals.
this is very effective for nertasks, and has improved the baseline method ondifferent benchmarks.
the main contributions ofthe proposed method include:.
• the use of multi-metadata feature embedding.
of chinese characters in chinese ner..• a novel two-stream model that combines theradicals, characters and words of chinesecharacters to improve the performance of theproposed mect method..• the proposed method is evaluated on sev-eral well-known chinese ner benchmarking.
datasets, demonstrating the merits and superi-ority of the proposed approach over the state-of-the-art methods..2 related work.
the key of the proposed mect method is to usethe radical information of chinese characters to en-hance the chinese ner model.
so we focus on themainstream information enhancement methods inthe literature.
there are two main types of chinesener enhancement methods, including lexical in-formation fusion and glyph-structural informationfusion..lexical enhancement in chinese ner, manyrecent studies use word matching methods to en-hance character-based models.
a typical method isthe lattice-lstm model (zhang and yang, 2018)that improves the ner performance by encodingand matching words in the lexicon.
recently, somelexical enhancement methods were proposed usingcnn models, such as lr-cnn (gui et al., 2019a),can-ner (zhu and wang, 2019).
graph networkshave also been used with lexical enhancement.
thetypical one is lgn (gui et al., 2019b).
besides,there are transformer-based lexical enhancementmethods, such as plt (xue et al., 2019) and flat.
and softlexicon (ma et al., 2020) introduces lexi-cal information through label and probability meth-ods at the character representation layer..glyph-structural enhancement some studiesalso use the glyph structure information in chi-nese ner.
for example, dong et al.
(2016) werethe ﬁrst to study the application of radical-levelinformation in chinese ner.
they used bi-lstmto extract radical-level embedding and then con-catenated it with the embedding of characters asthe ﬁnal input.
the radical information used inbi-lstm is structural components (sc) as shownin table 1, which achieved state-of-the-art perfor-mance on the msra dataset.
the glyce (menget al., 2019) model used chinese character imagesto extract features such as strokes and structureof chinese characters, achieving promising perfor-.
1530figure 1: the input and output of flat..mance in chinese ner.
some other methods (xuet al., 2019; song et al., 2020) also proposed touse radical information and tencent’s pre-trainedembedding2 to improve the performance.
in theseworks, the structural components of chinese char-acters have been proven to be able to enrich thesemantics of the characters, resulting in better nerperformance..3 background.
the proposed method is based on the flat-latticetransformer (flat) model.
thus, we ﬁrst brieﬂyintroduce flat that improves the encoder structureof transformer by adding word lattice information,including semantic and position boundary infor-mation.
these word lattices are obtained throughdictionary matching..figure 1 shows the input and output of flat.
ituses the relative position encoding transformed byhead and tail position to ﬁt the word’s boundaryinformation.
the relative position encoding, rij,is calculated as follows:.
rij = relu(wr(phi−hj ⊕ phi−tj.
⊕ pti−hj ⊕ pti−tj )),.
where wr is a learnable parameter, hi and ti repre-sent the head position and tail position of the i-thcharacter, ⊕ denotes the concatenation operation,and pspan is obtained as in vaswani et al.
(2017):.
p(2k).
span = sin(.
p(2k+1).
span = cos(.
span100002k/dmodelspan100002k/dmodel.
),.
),.
where pspan corresponds to p in eq.
(1), and spandenotes hi − hj, hi − tj, ti − hj and ti − tj.
thenthe scaled dot-product attention is obtained by:.
att(a, v ) = softmax(a)v ,.
aij = (qi + u)(cid:62)kj + (qi + v)(cid:62)r∗ij,[q, k, v ] = ex[wq, wk, wv],.
2https://ai.tencent.com/ailab/nlp/en/.
embedding.html.
(1).
(2).
(3).
(4).
(5).
(6).
where r∗parameters..ij = rij ·wr.
u, v and w(cid:50) are learnable.
4 the proposed mect method.
to better integrate the information of chinese char-acter components, we use chinese character struc-ture as another metadata and design a two-streamform of multi-metadata embedding network.
thearchitecture of the proposed network is shown infigure 2a.
the proposed method is based on theencoder structure of transformer and the flatmethod, in which we integrate the meaning andboundary information of chinese words.
the pro-posed two-stream model uses a cross-transformermodule similar to the self-attention structure to fusethe information of chinese character components.
in our method, we also use the multi-modal col-laborative attention method that is widely used invision-language tasks (lu et al., 2019).
the differ-ence is that we add a randomly initialized attentionmatrix to calculate the attention bias for the twotypes of metadata embedding..4.1 cnn for radical-level embedding.
chinese characters are based on pictographs, andtheir meanings are expressed in the shape of ob-jects.
in this case, the structure of chinese char-acters has certain useful information for ner.
forexample, the radicals such as ‘艹’ (grass) and ‘木’(wood) generally represent plants, enhancing chi-nese medicine entity recognition.
for another ex-ample, ‘月’ (body) represents human body partsor organs, and ‘疒’ (disease) represents diseases,which beneﬁts chinese ner for the medical ﬁeld.
besides, the chinese have their own culture and be-lief in naming.
radicals ‘钅’ (metal), ‘木’ (wood),‘氵’ (water), ‘火’ (fire), and ‘土’ (earth) rep-resented by the wu-xing (five elements) theoryare often used as names of people or companies.
but ‘锈’ (rust), ‘杀’ (kill), ‘污’ (dirt), ‘灾’(disaster) and ‘堕’ (fall) are usually not usedas names, even if they contain some elements ofthe wu-xing theory.
it is because the other rad-ical components also determine the semantics ofchinese characters.
radicals that generally appearnegative or conﬂict with chinese cultural beliefsare usually not used for naming..therefore, we choose the more informativestructural components (sc) in table 1 as radical-level features of chinese characters and use convo-lutional neural network (cnn) to extract character.
1531transformerencoder南south11京capital22市city33长long44江river55大big66桥bridge77南京nanjing12南京市nanjingcity13市长mayor34长江yangtzeriver45长江大桥yangtzeriver bridge47大桥bridge67maskb-loci-loci-locb-loci-loci-loci-loc(a) the whole architecture.
(b) the cross-transformer module.
figure 2: the proposed mect method: (a) the overall structure of mect; (b) the cross-transformer module..tained by the linear transformation of lattice andradical-level feature embedding:.
(cid:62).
.
.
.
ql(r),ikl(r),ivl(r),i.
.
= el(r),i.
.
.
wl(r),qiwl(r),v.(cid:62).
.
.
,.
(7).
where el and er are lattice embedding andradical-level embedding, i is the identity matrix,and each w is a learnable parameter.
then we usethe relative position encoding in flat to representthe boundary information of a word and calculatethe attention score in our cross-transformer:.
attl(ar, vl) = softmax(ar)vl,attr(al, vr) = softmax(al)vr,al(r),ij = (ql(r),i + ul(r))(cid:62)er(l),jl(r),ij,.
+ (ql(r),i + vl(r))(cid:62)r∗.
(8).
(9).
(10).
where u and v are learnable parameters for atten-tion bias in eq.
(10), al is the lattice attentionscore, and ar denotes the radical attention score.
and r∗ij = rij · wr.
wr are learnable parame-ters.
the relative position encoding, rij, is calcu-lated as follows:.
rij = relu(wr(phi−hj ⊕ pti−tj ))..(11).
4.3 random attention.
we empirically found that the use of random at-tention in cross-transformer can improve the per-formance of the proposed method.
this may bedue to the requirement of attention bias in latticeand radical feature embedding, which can betteradapt to the scores of two subspaces.
random at-tention is a randomly initialized parameter matrix.
figure 3: cnn for radical feature extraction..features.
the structure diagram of the cnn net-work is shown in figure 3. we ﬁrst disassemble thechinese characters into sc and then input the radi-cals into cnn.
last, we use the max-pooling andfully connected layers to get the feature embeddingof chinese characters at the radical-level..4.2 the cross-transformer module.
after radical feature extraction, we propose a cross-transformer network to obtain the supplementarysemantic information of the structure of chinesecharacters.
it also uses contextual and lexical infor-mation to enrich the semantics of chinese charac-ters.
the cross-transformer network is illustratedin figure 2b.
we use two transformer encodersto cross the lattice and radical information of chi-nese characters, which is different from the self-attention method in transformer..the input ql(qr), kl(kr), vl(vr) are ob-.
1532南京市长江大桥南京市长江大桥南京南京市市长长江长江大桥大桥南京市长江大桥<pad><pad><pad><pad><pad><pad>十冂丫二亠口小亠巾长氵工大木乔latticeembeddingradical-levelembeddingsplit characters into scsouthcapitalcitylongriverbigbridgecnnforradical-levelembeddingb-loci-loci-locb-loci-loci-loci-locmasksouthcapitalcitylongriverbignanjingcitymayoryangtzeriveryangtzeriverbridgebridgeconstruct latticebridgenanjingsouthcapitalcitylongriverbigbridgemulti-head attention&random attentionmulti-head attention&random attentionadd&normadd&normfeed-forwardnetworkfeed-forwardnetworkadd&normadd&normlatticeembeddingradical-levelembeddingvllinear&crfqlvrqrkrklconcatenatelatticeattentionradicalattention木人一月刂<pad><pad>●●●●●●represent:character radicalsrepresent:radicalembeddinglayer:convolutionlayer:maxpoolingrepresent:radical-levelembeddinglayer:fullyconnectedbmax len×max len that is added to the previous at-tention score to obtain a total attention score:.
v ∗l = softmax(ar + b)vl,v ∗r = softmax(al + b)vr..(12).
(13).
4.4 the fusion method.
to reduce information loss, we directly concatenatethe lattice and radical features and input them intoa fully connected layer for information fusion:.
fusion(v ∗.
l , v ∗.
r ) = (v ∗.
r ⊕ v ∗.
l )w o + b,.
(14).
where ⊕ denotes the concatenation operation, w oand b are learnable parameters..after the fusion step, we mask the word part andpass the fused feature to a conditional randomfield (crf) (lafferty et al., 2001) module..5 experimental results.
in this section, we evaluate the proposed mectmethod on four datasets.
to make the experimen-tal results more reasonable, we also set up twoadditional working methods for assessing the per-formance of radicals in a two-stream model.
weuse the span method to calculate f1-score (f1), pre-cision (p), and recall (r) as the evaluation metrics..we use four mainstream chinese ner benchmark-ing datasets: weibo (peng and dredze, 2015;he and sun, 2016), resume (zhang and yang,2018), msra (levow, 2006), and ontonotes4.0 (weischedel and consortium, 2013).
the cor-pus of msra and ontonotes 4.0 comes from news,the corpus of weibo comes from social media, andthe corpus of resume comes from the resume datain sina finance.
table 3 shows the statistical infor-mation of these datasets.
among them, the weibodataset has four types of entities, including per,org, loc, and gpe.
resume has eight types ofentities, including cont, edu, loc, per, org,pro, race, and title.
ontonotes 4.0 has fourtypes of entities: per, org, loc, and gpe.
themsra dataset contains three types of entities, i.e.,org, per, and loc..we use the state of the art method, flat, as thebaseline model.
flat is a chinese ner modelbased on transformer and combined with lattice.
besides, we also compared the proposed methodwith both classic and innovative chinese ner mod-els.
we use the more informative ‘sc’ as the radi-cal feature, which comes from the online xinhua.
datasets.
types.
train.
dev.
test.
weibo.
resume.
ontonotes.
msra.
sentencesentitiessentencesentitiessentencesentitiessentencesentities.
1.35k1.89k3.8k1.34k15.7k13.4k46.4k74.8k.
0.27k0.39k0.46k0.16k4.3k6.95k--.
0.27k0.42k0.48k0.15k4.3k7.7k4.4k6.2k.
table 3: statistics of the benchmarking datasets..models.
ne.
nm overall.
peng and dredze (2015)peng and dredze (2016)∗he and sun (2017a)he and sun (2017b)∗cao et al.
(2018)lattice-lstmcan-nerlr-cnnlgnpltsoftlexicon (lstm)baselinemectbertbert + mect.
51.9655.2850.6054.5054.3453.0455.3857.1455.3453.5559.08-61.91--.
61.0562.9759.3262.1757.3562.2562.9866.6764.9864.9062.22-62.51--.
56.0558.9954.8258.2358.7058.7959.3159.9260.2159.7661.4260.3263.3068.2070.43.dictionary3.
the pre-trained embedding of charac-ters and words are the same as flat..for hyper-parameters, we used 30 1-d convolu-tion kernels with the size of 3 for cnn.
we usedthe smac (hutter et al., 2011) algorithm to searchfor the optimal hyper-parameters.
besides, we set adifferent learning rate for the training of the radical-level embedding with cnn.
readers can refer tothe appendix for our hyper-parameter settings..5.2 comparison with sota methods.
in this section, we evaluate and analyze the pro-posed mect method with a comparison to boththe classic and state of the art methods.
the experi-mental results are reported in tables 4−74.
eachtable is divided into four blocks.
the ﬁrst blockincludes classical chinese ner methods.
the sec-ond one reports the results obtained by state of theart approaches published recently.
the third and.
3http://tool.httpcn.com/zi/.
4in tables 4−7, ‘∗’ denotes the use of external labeleddata for semi-supervised learning and ‘†’ denotes the use ofdiscrete features..5.1 experimental settings.
table 4: results obtained on weibo (%)..1533models.
p.r.f1.
models.
p.r.f1.
zhang and yang (2018)azhang and yang (2018)bzhang and yang (2018)czhang and yang (2018)dlattice-lstmcan-nerlr-cnnlgnpltsoftlexicon (lstm).
+ bichar.
baselinemectbertbert + mect.
93.7294.0793.6694.5394.8195.0595.3795.2895.3495.3095.71-96.40--.
93.4494.4293.3194.2994.1194.8294.8495.4695.4695.7795.77-95.39--.
93.5894.2493.4894.4194.4694.9495.1195.3795.4095.5395.7495.4595.8995.5395.98.table 5: results obtained on resume (%).
for zhangand yang (2018), a represents word-based lstm’, bindicates ‘word-based + char + bichar lstm’, c repre-sents the ‘char-based lstm’ model, and d is the ‘char-based + bichar + softword lstm’ model..fourth ones are the results obtained by the proposedmect method as well as the baseline models..weibo: table 4 shows the results obtained onweibo in terms of the f1 scores of named enti-ties (ne), nominal entities (nm), and both (over-all).
from the results, we can observe that mectachieves the state-of-the-art performance.
com-pared with the baseline method, mect improves2.98% in terms of the f1 metric.
for the ne metric,the proposed method achieves 61.91%, beating allthe other approaches..resume: the results obtained on the resumedataset are reported in table 5. the ﬁrst blockshows zhang and yang (2018) comparative resultson the character-level and word-level models.
wecan observe that the performance of incorporatingword features into the character-level model is bet-ter than other models.
additionally, mect com-bines lexical and radical features, and the f1 scoreis higher than the other models and the baselinemethod..ontonotes 4.0: table 6 shows the results ob-tained on ontonotes 4.0. the symbol ‘§’ indicatesgold segmentation, and the symbol ‘¶’ denotes au-tomated segmentation.
other models have no seg-mentation and use lexical matching.
comparedto the baseline method, the f1 score of mect isincreased by 0.47%.
mect also achieves a highrecall rate, keeping the precision rate and recall raterelatively stable..yang et al.
(2018)§yang et al.
(2018)§∗†che et al.
(2013)§∗wang et al.
(2013)§∗zhang and yang (2018)b§zhang and yang (2018)b¶lattice-lstmcan-nerlr-cnnlgnpltsoftlexicon (lstm).
+ bichar.
baselinemectbertbert + mect.
65.5972.9877.7176.4378.6273.3676.3575.0576.4076.1376.7877.2877.13-77.57--.
71.8480.1572.5172.3273.1370.1271.5672.2972.6073.6872.5474.0775.22-76.27--.
68.5776.4075.0274.3275.7771.7073.8873.6474.4574.8974.6075.6476.1676.4576.9280.1482.57.table 6: results on ontonotes 4.0 (%), where ‘§’ de-notes gold segmentation and ‘¶’ denotes auto segmen-tation..msra: table 7 shows the experimental resultsobtained on msra.
in the ﬁrst block, the result pro-posed by dong et al.
(2016) is the ﬁrst method us-ing radical information in chinese ner.
from thetable, we can observe that the overall performanceof mect is higher than the existing sota meth-ods.
similarly, our recall rate achieves a higherperformance so that the ﬁnal f1 has a certain per-formance boosting..with bert: besides the single-model evalu-ation on the four datasets, we also evaluated theproposed method when combining with the sotamethod, bert.
the bert model is the same asflat using the ‘bert-wwm’ released by cui et al.
(2020).
the results are shown in the fourth blockof each table.
the results of bert are taken fromthe flat paper.
we can ﬁnd that mect furtherimproves the performance of bert signiﬁcantly..5.3 effectiveness of cross-transformer.
there are two sub-modules in the proposed cross-transformer method: lattice and radical attentions.
figure 4 includes two heatmaps for the normal-ization of the attention scores of the two modules.
from the two ﬁgures, we can see that lattice atten-tion pays more attention to the relationship betweenwords and characters so that the model can obtainthe position information and boundary informationof words.
radical attention focuses on global in-formation and corrects the semantic information of.
1534models.
p.r.chen et al.
(2006)zhang et al.
(2006)∗zhou et al.
(2013)lu et al.
(2016)dong et al.
(2016)lattice-lstmcan-nerlr-cnnlgnpltsoftlexicon (lstm).
+ bichar.
baselinemectbertbert + mect.
91.2292.2091.86-91.2893.5793.5394.5094.1994.2594.6394.73-94.55--.
81.7190.1888.75-90.6292.7992.4292.9392.7392.3092.7093.40-94.09--.
f1.
86.2091.1890.2887.9490.9593.1892.9793.7193.4693.2693.6694.0694.1294.3294.9596.24.table 7: results obtained on msra (%)..(a) radical attention.
(b) lattice attention.
figure 4: visualization of cross-attention, in which thecoordinates 0-15 are used for the characters part andthe coordinates 16-24 are for the words part.
the twosub-ﬁgures show the radical and lattice attention scoresrespectively..each character through radical features.
therefore,lattice and radical attentions provide complemen-tary information for the performance-boosting ofthe proposed mect method in chinese ner..5.4.impact of radicals.
we visualized the radical-level embedding obtainedby the cnn network and found that the cosine dis-tance of chinese characters with the same radicalor similar structure is smaller.
for example, figure5 shows part of the chinese character embeddingtrained on the resume dataset.
the highlighteddots represent chinese characters that are close tothe character ‘华’.
we can see that they have thesame radicals or similar structure.
it can enhancethe semantic information of chinese characters toa certain extent..figure 5: embedding visualization of the charactersrelated to ‘华’ in two-dimensional space.
gray dotsindicate larger cosine distances..percentage like ‘百分之四十三点二 (43.2%)’ isincorrectly labelled as per in the training dataset,which causes flat to mark the percentage ofwords with per on the test dataset, while mectavoids this situation.
there are also some wordssuch as ‘田时’ and ‘以国’ that appear in the lex-icon, which was mistakenly identiﬁed as validwords by flat, leading to recognition errors.
ourmect addresses these issues by paying globalattention to the radical information.
besides, inflat, some numbers and letters are incorrectlymarked as per, org, or others.
we compared theper label accuracy of flat and mect on the testdataset.
flat achieves 81.6%, and mect reaches86.96%, which is a very signiﬁcant improvement..5.5 analysis in efﬁciency and model size.
we use the same flat method to evaluate the par-allel and non-parallel inference speed of mecton an nvidia geforce rtx 2080ti card, usingbatch size = 16 and batch size = 1. we use thenon-parallel version of flat as the standard andcalculate the other models’ relative inference speed.
the results are shown in figure 6. according tothe ﬁgure, even if mect adds a transformer en-coder to flat, the speed is only reduced by 0.15 interms of the parallel inference speed.
our model’sspeed is considerable relative to lstm, cnn, andsome graph-based network models.
because trans-former can make full use of the gpu’s parallelcomputing power, the speed of mect does notdrop too much, but it is still faster than other mod-els.
the model’s parameter is between 2 and 4million, determined by the max sentence length inthe dataset and the dmodel size in the model..5.6 ablation study.
we also examined the inference results of mectand flat on ontonotes 4.0 and found many ex-citing results.
for example, some words with a.to validate the effectiveness of the main compo-nents of the proposed method, we set up two exper-iments in figure 7. in experiment a, we only use a.
1535figure 6: relative inference speed of each modelbased on non-parallel flat♣.
where ‘♣’ representsthe inference speed under non-parallel conditions, ‘♠’represents the inference speed under parallel condi-tions, and the value of ‘♦’ is derived from the relativespeed above flat..single-stream model with a modiﬁed self-attention,which is similar to the original flat model.
thedifference is that we use a randomly initialized at-tention matrix (random attention) for the attentioncalculation.
we combine lattice embedding andradical-level embedding as the input of the model.
the purpose is to verify the performance of the two-stream model relative to the single-stream model.
in experiment b, we do not exchange the query’sfeature vector.
we replace the cross-attention withtwo sets of modiﬁed self-attention and follow thetwo modules’ output with the same fusion methodas mect.
the purpose of experiment b is to ver-ify the effectiveness of mect relative to the two-stream model without crossover.
besides, we eval-uate the proposed mect method by removing therandom attention module..table 8 shows the ablation study results.
1) bycomparing the results of experiment a with theresults of experiment b and mect, we can ﬁndthat the two-stream model works better.
the useof lattice-level and radical-level features as the twostreams of the model helps the model to better un-derstand and extract the semantic features of chi-nese characters.
2) based on the results of experi-ment b and mect, we can see that by exchangingthe two query feature vectors, the model can extractfeatures more effectively at the lattice and radicallevels.
they have different attention mechanisms toobtain contextual information, resulting in globaland local attention interaction.
this provides betterinformation extraction capabilities for the proposedmethod in a complementary way.
3) last, the per-formance of mect drops on all the datasets by.
(a) experiment a.
(b) experiment b.figure 7: two interactive attention experiment set-tings..experiments weibo resume ontonotes msra.
exp.
aexp.
bmect- ra.
60.776162.6961.53.
95.4295.5495.8995.31.
76.4376.7876.9276.64.
94.2094.1894.3294.25.table 8: the f1 scores (%) of the four experimentalmethods on different datasets.
ra stands for randomattention.
we verify all the labels (ne and nm) onweibo..removing the random attention module (the lastrow).
this indicates that, as an attention bias, ran-dom attention can eliminate the differences causedby different embeddings, thereby improving themodel’s performance further..6 conclusion.
this paper presented a novel two-stream network,namely mect, for chinese ner.
the proposedmethod uses multi-metadata embedding that fusesthe information of radicals, characters and wordsthrough a cross-transformer network.
addition-ally, random attention was used for further perfor-mance boost.
experimental results obtained on fourbenchmarks demonstrate that the radical informa-tion of chinese characters can effectively improvethe performance for chinese ner..the proposed mect method with the radicalstream increases the complexity of a model.
in thefuture, we will consider how to integrate the char-acters, words and radical information of chinesecharacters with a more efﬁcient way in two-streamor multi-stream networks to improve the perfor-mance of chinese ner and extend it to other nlptasks..acknowledgments.
this work was supported in part by the nationalkey research and development program of china.
1536latticelstmlatticelstmlr-cnnlgncgnflatflatmectmect02468relative speed×0.3×0.64×0.75×0.84×1.45×1.0×4.97×0.96×4.2latticeembeddingradical-levelembeddingadaptselfattentionradical-levelembeddinglatticeembeddingadaptselfattentionadaptselfattention(2017yfc1601800), the national natural sciencefoundation of china (61876072, 61902153) andthe six talent peaks project of jiangsu province(xydxx-012).
we also thank xiaotong xiang andjun quan for their help on editing the manuscript..references.
pengfei cao, yubo chen, kang liu, jun zhao, andshengping liu.
2018. adversarial transfer learn-ing for chinese named entity recognition with self-in proceedings of the 2018attention mechanism.
conference on empirical methods in natural lan-guage processing, pages 182–192, brussels, bel-gium.
association for computational linguistics..wanxiang che, mengqiu wang, christopher d. man-ning, and ting liu.
2013. named entity recogni-tion with bilingual constraints.
in proceedings of the2013 conference of the north american chapter ofthe association for computational linguistics: hu-man language technologies, pages 52–62, atlanta,georgia.
association for computational linguistics..aitao chen, fuchun peng, roy shan, and gordon sun.
2006. chinese named entity recognition with condi-tional probabilistic models.
in sighan workshopon chinese language processing..yubo chen, liheng xu, kang liu, daojian zeng, andevent extraction via dynamicin.
jun zhao.
2015.multi-pooling convolutional neural networks.
acl—ijcnlp, volume 1, pages 167–176..yiming cui, wanxiang che, ting liu, bing qin, shi-jin wang, and guoping hu.
2020. revisiting pre-trained models for chinese natural language process-in findings of the association for computa-ing.
tional linguistics: emnlp 2020, pages 657–668,online.
association for computational linguistics..jacob devlin, ming-wei chang, kenton lee, andkristina toutanova.
2019. bert: pre-training ofdeep bidirectional transformers for language under-in proceedings of the 2019 conferencestanding.
of the north american chapter of the associationfor computational linguistics: human languagetechnologies, volume 1 (long and short papers),pages 4171–4186, minneapolis, minnesota.
associ-ation for computational linguistics..dennis diefenbach, vanessa lopez, kamal singh, andpierre maret.
2018. core techniques of questionanswering systems over knowledge bases: a survey.
kais, 55(3):529–569..chuanhai dong,.
jiajun zhang, chengqing zong,masanori hattori, and hui di.
2016. character-based lstm-crf with radical-level features for chinesenamed entity recognition.
in natural language un-derstanding and intelligent applications, pages 239–250. springer..huanzhong duan and yan zheng.
2011. a studyon features of the crfs-based chinese named entityrecognition.
international journal of advanced in-telligence, 3(2):287–294..tao gui, ruotian ma, qi zhang, lujun zhao, yu-gangjiang, and xuanjing huang.
2019a.
cnn-based chi-nese ner with lexicon rethinking.
in proceedings ofthe twenty-eighth international joint conference onartiﬁcial intelligence, ijcai-19, pages 4982–4988.
international joint conferences on artiﬁcial intelli-gence organization..tao gui, yicheng zou, qi zhang, minlong peng, jinlanfu, zhongyu wei, and xuan-jing huang.
2019b.
alexicon-based graph neural network for chinese ner.
in proceedings of the 2019 conference on empiricalmethods in natural language processing and the9th international joint conference on natural lan-guage processing (emnlp-ijcnlp), pages 1039–1049..hangfeng he and xu sun.
2016. f-score driven maxmargin neural network for named entity recognitionin chinese social media.
corr, abs/1611.04234..hangfeng he and xu sun.
2017a.
f-score driven maxmargin neural network for named entity recognitionin chinese social media.
in proceedings of the 15thconference of the european chapter of the associa-tion for computational linguistics: volume 2, shortpapers, pages 713–718, valencia, spain.
associa-tion for computational linguistics..hangfeng he and xu sun.
2017b.
a uniﬁed modelfor cross-domain and semi-supervised named entityrecognition in chinese social media.
in proceedingsof the thirty-first aaai conference on artiﬁcial in-telligence, aaai’17, page 3216–3222.
aaai press..frank hutter, holger h hoos, and kevin leyton-sequential model-based optimiza-brown.
2011.in inter-tion for general algorithm conﬁguration.
national conference on learning and intelligent opti-mization, pages 507–523.
springer..mahboob alam khalid, valentin jijkoun, and maartende rijke.
2008. the impact of named entity normal-ization on information retrieval for question answer-in advances in information retrieval, pagesing.
705–710, berlin, heidelberg.
springer berlin hei-delberg..john d. lafferty, andrew mccallum, and fernandoc. n. pereira.
2001. conditional random ﬁelds:probabilistic models for segmenting and labeling se-quence data.
in proceedings of the eighteenth inter-national conference on machine learning, icml’01, pages 282–289, san francisco, ca, usa.
mor-gan kaufmann publishers inc..gina-anne levow.
2006. the third international chi-nese language processing bakeoff: word segmen-in proceed-tation and named entity recognition.
ings of the fifth sighan workshop on chinese.
1537language processing, pages 108–117, sydney, aus-tralia.
association for computational linguistics..xiaonan li, hang yan, xipeng qiu, and xuanjinghuang.
2020. flat: chinese ner using ﬂat-latticein proceedings of the 58th annualtransformer.
meeting of the association for computational lin-guistics, pages 6836–6842, online.
association forcomputational linguistics..jiasen lu, dhruv batra, devi parikh, and stefanlee.
2019. vilbert: pretraining task-agnostic visi-olinguistic representations for vision-and-languagetasks.
in advances in neural information process-ing systems, volume 32, pages 13–23.
curran asso-ciates, inc..yanan lu, yue zhang, and dong-hong ji.
2016. multi-prototype chinese character embedding.
in lrec..ruotian ma, minlong peng, qi zhang, zhongyu wei,and xuanjing huang.
2020. simplify the usage ofin proceedings of thelexicon in chinese ner.
58th annual meeting of the association for compu-tational linguistics, pages 5951–5960, online.
as-sociation for computational linguistics..yuxian meng, wei wu, fei wang, xiaoya li, ping nie,fan yin, muyu li, qinghong han, xiaofei sun, andjiwei li.
2019. glyce: glyph-vectors for chinesein advances in neuralcharacter representations.
information processing systems, volume 32, pages2746–2757.
curran associates, inc..nanyun peng and mark dredze.
2015. named entityrecognition for chinese social media with jointlyin proceedings of the 2015trained embeddings.
conference on empirical methods in natural lan-guage processing, pages 548–554, lisbon, portugal.
association for computational linguistics..nanyun peng and mark dredze.
2016..improvingnamed entity recognition for chinese social mediawith word segmentation representation learning.
inproceedings of the 54th annual meeting of the as-sociation for computational linguistics (volume 2:short papers), pages 149–155, berlin, germany.
as-sociation for computational linguistics..alec radford, karthik narasimhan, tim salimans, andimproving language under-.
ilya sutskever.
2018.standing by generative pre-training..sebastian riedel, limin yao, andrew mccallum, andbenjamin m marlin.
2013. relation extraction withmatrix factorization and universal schemas.
in pro-ceedings of the 2013 conference of the north amer-ican chapter of the association for computationallinguistics: human language technologies, pages74–84..ashish vaswani, noam shazeer, niki parmar, jakobuszkoreit, llion jones, aidan n gomez, ł ukaszkaiser, and illia polosukhin.
2017. attention is allyou need.
in advances in neural information pro-cessing systems, volume 30, pages 5998–6008.
cur-ran associates, inc..mengqiu wang, wanxiang che, and christopher d.manning.
2013. effective bilingual constraints forsemi-supervised learning of named entity recogniz-in proceedings of the twenty-seventh aaaiers.
conference on artiﬁcialintelligence, aaai’13,page 919–925.
aaai press..ralph m weischedel and linguistic data consortium.
2013. ontonotes release 5.0. title from disc label..canwen xu, feiyang wang, jialong han, and chen-liang li.
2019. exploiting multiple embeddings forchinese named entity recognition.
in proceedings ofthe 28th acm international conference on informa-tion and knowledge management, cikm ’19, page2269–2272, new york, ny, usa.
association forcomputing machinery..mengge xue, bowen yu, tingwen liu, bin wang, erlimeng, and quangang li.
2019. porous lattice-basedtransformer encoder for chinese ner.
arxiv preprintarxiv:1911.02733..vikas yadav, rebecca sharp, and steven bethard.
2018.deep afﬁx features improve neural named entity rec-ognizers.
in proceedings of the seventh joint con-ference on lexical and computational semantics,pages 167–172, new orleans, louisiana.
associa-tion for computational linguistics..jie yang, zhiyang teng, meishan zhang, and yuezhang.
2018. combining discrete and neural fea-tures for sequence labeling.
in computational lin-guistics and intelligent text processing, pages 140–154, cham.
springer international publishing..suxiang zhang, ying qin, juan wen, and xiaojiewang.
2006. word segmentation and named entityrecognition for sighan bakeoff3.
in sighan work-shop on chinese language processing, pages 158–161..yue zhang and jie yang.
2018. chinese ner us-in proceedings of the 56th an-ing lattice lstm.
nual meeting of the association for computationallinguistics (volume 1: long papers), pages 1554–1564, melbourne, australia.
association for compu-tational linguistics..junsheng zhou, weiguang qu, and fen zhang.
2013.chinese named entity recognition via joint identiﬁ-cation and categorization.
chinese journal of elec-tronics, 22(2):225–230..c. song, y. xiong, w. huang, and l. ma.
2020.joint self-attention and multi-embeddings for chi-nese named entity recognition.
in 2020 6th interna-tional conference on big data computing and com-munications (bigcom), pages 76–80..yuying zhu and guoxin wang.
2019. can-ner: con-volutional attention network for chinese namedin proceedings of the 2019entity recognition.
conference of the north american chapter of theassociation for computational linguistics: human.
1538language technologies, volume 1 (long and shortpapers), pages 3384–3393, minneapolis, minnesota.
association for computational linguistics..a appendix.
a.1 range of hyper-parameters.
we manually selected parameters on the two large-scale datasets, including ontonotes 4.0 and msra.
for the two small datasets, weibo and resume, weused the smac algorithm to search for the besthyper-parameters.
the range of parameters is listedin table 9..hyper-parameter.
range.
output dropoutlattice dropoutradical dropoutwarm uphead numdheaddmodellrradical lrmomentum.
[0.1, 0.2, 0.3][0.1, 0.2, 0.3][0.1, 0.2, 0.3, 0.4][0.1, 0.2, 0.3][8][16, 20][128, 160][1e-3, 25e-4][6e-4, 25e-4][0.85, 0.97].
table 9: the searching range of hyper-parameters..1539