comprehensive study: how the context information of differentgranularity affects dialogue state tracking?.
puhai yang and heyan huang∗ and xian-ling maoschool of computer science and technology,beijing institute of technology, beijing, chinabeijing engineering research center of high volume languageinformation processing and cloud computing applications, beijing, chinasoutheast academy of information technology, beijing institute of technology, fujian, china{phyang, hhy63, maoxl}@bit.edu.cn.
abstract.
dialogue state tracking (dst) plays a key rolein task-oriented dialogue systems to monitorthe user’s goal.
in general, there are two strate-gies to track a dialogue state: predicting itfrom scratch and updating it from previousstate.
the scratch-based strategy obtains eachslot value by inquiring all the dialogue his-tory, and the previous-based strategy relies onthe current turn dialogue to update the previ-ous dialogue state.
however, it is hard for thescratch-based strategy to correctly track short-dependency dialogue state because of noise;meanwhile, the previous-based strategy is notvery useful for long-dependency dialogue statetracking.
obviously, it plays different roles forthe context information of different granular-ity to track different kinds of dialogue states.
thus, in this paper, we will study and discusshow the context information of different granu-larity affects dialogue state tracking.
first, weexplore how greatly different granularities af-fect dialogue state tracking.
then, we furtherdiscuss how to combine multiple granularitiesfor dialogue state tracking.
finally, we applythe ﬁndings about context granularity to few-shot learning scenario.
besides, we have pub-licly released all codes..1.introduction.
currently, task-oriented dialogue systems have at-tracted great attention in academia and industry(chen et al., 2017), which aim to assist the userto complete certain tasks, such as buying prod-ucts, booking a restaurant, etc.
as a key compo-nent of task-oriented dialogue system, dialoguestate tracking plays a important role in understand-ing the natural language given by the user andexpressing it as a certain dialogue state (rastogiet al., 2017, 2018; goel et al., 2018).
the dialogue.
∗corresponding author.
figure 1: examples of dialogue state tracking with con-text information of different granularity at the sixth turnof a dialogue.
slot in a dialogue state refers to theconcatenation of a domain name and a slot name.
inthe ﬁgure, (a) represents predicting the dialogue statefrom scratch, where slots in three domains need to bepredicted and the challenge of encoding longer text isfaced; (b) indicates updating dialogue state from theprevious state, the slot taxi − departure cannot bepredicted due to the absence of corresponding dialoguehistory content; (c) represents dialogue state trackingwith context information of granularity 4, which tracksfrom the second turn and uses less dialogue history con-tent (4 turns) to provide evidence for the prediction ofall slots..state for each turn of a dialogue is typically pre-sented as a series of slot value pairs that representinformation about the user’s goal up to the cur-rent turn.
for example, in figure 1, the dialoguestate at turn 2 is {(attraction − type, cinema),(attraction − area, south)}..in general, there are two strategies to track adialogue state: predicting it from scratch and up-dating it from previous state.
the scratch-based.
proceedingsofthe59thannualmeetingoftheassociationforcomputationallinguisticsandthe11thinternationaljointconferenceonnaturallanguageprocessing,pages2481–2491august1–6,2021.©2021associationforcomputationallinguistics2481u: i am looking for a swimming pool in the south part of town .s: i am sorry , there are no swimming pools .. .
would you be interested in visiting a different attraction ?u: how about a cinema instead ?s: cineworld cinema in the south side is located at cambridge leisure park .u: what s the postcode and enterance fee ?s: ... can i help you with anything else ?u: i am also looking for a restaurant that serve portuguese food in the south side of town .s: nandos meets your criteria .
would you like to book a table ?u: yes .
book for 7 people on saturday at 15:15 .s: it will be reserved for 15 minutes .
the reference number is x361i811 .u: i also need to book a taxi to commute between the 2. i need to get to the restaurant by the booked time.turn 2turn 3turn 4turn 5turn 6turn 1dialogue state at turn 6slotvaluerestaurant-book dayrestaurant-book peoplerestaurant-book timerestaurant-foodrestaurant-namerestaurant-areaattraction-typeattraction-areacinemasouthsaturday715:15portugesenandossouthtaxi-destinationtaxi-departuretaxi-arrivebynandoscineworld cinema15:15dialogue state at turn 6slotvaluerestaurant-book dayrestaurant-book peoplerestaurant-book timerestaurant-foodrestaurant-namerestaurant-areaattraction-typeattraction-areacinemasouthsaturday715:15portugesenandossouthtaxi-destinationtaxi-departuretaxi-arrivebynandoscineworld cinema15:15dialogue state at turn 2slotvalueattraction-typeattraction-areacinemasouthdialogue state at turn 2slotvalueattraction-typeattraction-areacinemasouthdialogue state at turn 5slotvaluerestaurant-book dayrestaurant-book peoplerestaurant-book timerestaurant-foodrestaurant-namerestaurant-areaattraction-typeattraction-areacinemasouthsaturday715:15portugesenandossouthdialogue state at turn 5slotvaluerestaurant-book dayrestaurant-book peoplerestaurant-book timerestaurant-foodrestaurant-namerestaurant-areaattraction-typeattraction-areacinemasouthsaturday715:15portugesenandossouthinitial dialog stateslotvaluedialogue contentturn 1...turn 6dialogue contentturn 1...turn 6dialogue contentturn 6dialogue contentturn 6dialogue contentturn 3turn 4turn 5turn 6dialogue contentturn 3turn 4turn 5turn 6(a)(b)(c)strategy obtains each slot value in dialogue stateby inquiring all the dialogue history (xu and hu,2018; lei et al., 2018; goel et al., 2019; ren et al.,2019; wu et al., 2019; shan et al., 2020; zhanget al., 2020), the advantage of this strategy is toensure the integrity of the dialogue information.
the previous-based strategy relies on the currentturn dialogue to update the previous dialogue state(mrkˇsi´c et al., 2017; chao and lane, 2019; kimet al., 2020; heck et al., 2020; zhu et al., 2020),the main character of this strategy is to greatly im-prove the efﬁciency of dialogue state predictionand avoid the computational cost of encoding alldialogue history..however, both kinds of strategies above havegreat defects because of their own characters.
forthe scratch-based strategy, it is hard to correctlytrack short-dependency dialogue state because ofthe noise associated with encoding all dialogue his-tory.
for example, the dialogue history of turn 1to 3 in figure 1 (a) does not contribute to the pre-diction of slot values in the restaurant domain.
for the previous-based strategy, it is difﬁcult tosolve the problem of long-dependency dialoguestate tracking because it utilizes only limited dia-logue information from the current turn dialogueand the previous state.
as in figure 1 (b), the slottaxi − departure cannot be predicted due to theabsence of corresponding dialogue history content.
obviously, it plays different roles for the contextinformation of different granularity to track differ-ent kinds of dialogue states.
intuitively, less contextinformation is needed for short-dependency dia-logue state, while more context information mustbe taken into account for long-dependency dialoguestate tracking.
for example, the dialogue state infigure 1 (c) is tracked from turn 2, which utilizescontext information of granularity 4 (turn 3 to 6),providing evidence for the prediction of all slotswhile bringing as little noise as possible..thus, in this paper, we will study and discusshow the context information of different granular-ity affects dialogue state tracking.
the contributionof this paper is that it is, to the best of our knowl-edge, the ﬁrst detailed investigation of the impactof context granularity in dialogue state tracking andpromotes the research on dialogue state trackingstrategy.
our investigation mainly focuses on threepoints1:.
• how greatly different granularities affect dia-.
logue state tracking?.
• how to combine multiple granularities for di-.
alogue state tracking?.
• application of context information granularity.
in few-shot learning scenario..the rest of paper is organized as follows: therelevant deﬁnitions and formulas in the dialoguestate tracking strategy are introduced in section 2.section 3 lists the detailed experimental settings.
section 4 presents the survey report and results,followed by conclusions in section 5..2 preliminary.
to describe the dialogue state tracking strategy,let’s introduce the formula deﬁnitions used in thispaper:.
dialogue content: d = (t1, t2, ..., tn ) is de-ﬁned as the dialogue of length n , where ti =(si, ui) is the dialogue content of i-th turn, whichincludes the system utterance si and the user utter-ance ui..dialogue state: we deﬁne e = (b0, b1, b2,..., bn ) as all dialogue states up to the n -th turnof the dialogue, where bi is the set of slot valuepairs representing the information provided by theuser up to the i-th turn.
in particular, b0 is theinitial dialogue state which is an empty set..granularity:in dialogue state tracking, the num-ber of dialogue turns spanning from a certain di-alogue state bm in the dialogue to the currentdialogue state bn is called granularity, that is,g = |(tm+1, ..., tn)|.
for example, the granu-larities of context information in (a), (b), and (c) infigure 1 are 6, 1, and 4, respectively..assuming that the dialogue state of the n -thturn is currently required to be inferred, the dia-logue state tracking under a certain granularity isas follows:.
bn = tracker((tn −g+1, ..., tn ), bn −g).
where g ∈ {1, 2, ..., n } is the granularity of con-text information and tracker represents a dialoguestate tracking model..in particular, if g = 1, then:.
1the code is released at https://github.com/.
yangpuhai/granularity-in-dst.
bn = tracker(tn , bn −1).
2482dataset.
# domains.
# slots avg.
turns.
sim-msim-rwoz2.0dstc2multiwoz2.1.
11115.
593330.
# dialoguesdev1203492005061,000.test2647754001,117999.train3841,1166001,6128,420.
# turnsdev6271,4898303,9347,371.train1,9736,1752,53611,67754,984.test1,3643,4361,6469,8907,368.table 1: data statistics of sim-m, sim-r, woz2.0, dstc2 and multiwoz2.1.
avg.
turns indicates the averagenumber of turns involved in the dialogue in the training data..modelsspanptr (xu and hu, 2018)trade (wu et al., 2019)bertdst (chao and lane, 2019)somdst (kim et al., 2020)sumbt (lee et al., 2019).
open vocabulary encoder.
rnnrnnbertbertbert.
decoderextractivegenerativeextractivegenerativeclassiﬁcation.
tracking strategyscratch-basedscratch-basedprevious-basedprevious-basedprevious-based.
table 2: statistics on the characteristics of the 5 baselines studied in the paper.
in the decoder, the extractivemode refers to the extraction of slot values directly from the dialogue context, the generative mode refers to thevocabulary-dependent sequence decoding, and the classiﬁcation mode is the slot value ontology-based classiﬁca-tion..5.145.534.237.246.53.
(cid:88)(cid:88)(cid:88)(cid:88)×.
this case corresponds to the strategy of updatingfrom previous state.
therefore, the previous-basedstrategy is a special case where context granularityis minimal in dialogue state tracking..2018), woz2.0 (wen et al., 2016), dstc2 (hen-derson et al., 2014) and multiwoz2.1 (eric et al.,2019).
the statistics for all datasets are shown intable 1..if g = n , then:.
bn = tracker((t1, ..., tn ), b0).
this case corresponds to the strategy of predictingstate from scratch.
similarly, the scratch-basedstrategy is also a special case of dialogue statetracking, with the context information of maximumgranularity.
since the size of the maximum granu-larity n is different in different dialogues, so 0 isused in the paper to refer to the maximum granu-larity n , -1 to refer to granularity n − 1, and soon..3 experimental settings.
in order to investigate how the context informa-tion of different granularity affects dialogue statetracking, we analyze the performance of severaldifferent types of dialogue state tracking modelson different datasets.
for a clearer illustration, thedetailed settings are introduced in this section..3.1 datasets.
our experiments were carried out on 5 datasets,sim-m (shah et al., 2018), sim-r (shah et al.,.
sim-m and sim-r are multi-turn dialoguedatasets in the movie and restaurant domains, re-spectively, which are specially designed to evaluatethe scalability of dialogue state tracking model.
alarge number of unknown slot values are includedin their test set, so the generalization ability of themodel can be reﬂected more accurately..woz2.0 and dstc2 datasets are both collectedin the restaurant domain and have the samethree slots f ood, area, and price range.
thesetwo datasets provide automatic speech recognition(asr) hypotheses of user utterances and can there-fore be used to verify the robustness of the modelagainst asr errors.
as in previous works, we usemanuscript user utterance for training and top asrhypothesis for testing..multiwoz2.1 is the corrected version of themultiwoz (budzianowski et al., 2018).
comparedto the four datasets above, multiwoz2.1 is a morechallenging and currently widely used benchmarkfor multi-turn multi-domain dialogue state track-ing, consisting of 7 domains, over 30 slots, andover 4500 possible slot values.
following previousworks (wu et al., 2019; kim et al., 2020; heck et al.,2020; zhu et al., 2020), we only use 5 domains.
2483(restaurant, train, hotel, taxi, attraction) thatcontain a total of 30 slots..3.2 baselines.
we use 5 different types of baselines whose charac-teristics are shown in table 2..spanptr: this is the ﬁrst model to extract slotvalues directly from dialogue context without anontology, it encodes the whole dialogue historywith a bidirectional rnn and extracts slot value foreach slot by generating the start and end positionsin dialogue history (xu and hu, 2018)..trade: this model is the ﬁrst to considerknowledge transfer between domains in the multi-domain dialogue state tracking task.
it representsa slot as a concatenation of domain name and slotname, encodes all dialogue history using bidirec-tional rnn, and ﬁnally decodes each slot value us-ing a pointer-generator network (wu et al., 2019)..bertdst: this model decodes only the slotvalues of the slots mentioned in the current turn ofdialogue, and then uses a rule-based update mecha-nism to update from the previous state to the cur-rent turn state.
it uses bert to encode the currentturn of dialogue and extracts slot values from thedialogue as spans (chao and lane, 2019)..somdst: this model takes the dialogue stateas an explicit memory that can be selectively over-written, and inputs it into bert together with thecurrent turn dialogue.
it then decomposes the pre-diction for each slot value into operation predictionand slot generation (kim et al., 2020)..sumbt: this model uses an ontology and istrained and evaluated on the dialogue session levelinstead of the dialogue turn level.
bert is used inthe model to encode turn level dialogues, and anunidirectional rnn is used to capture session-levelrepresentation (lee et al., 2019)..3.3 conﬁgurations and metrics.
our deployments are based on the ofﬁcial imple-mentation source code of somdst2 and sumbt3,in which spanptr, trade and bertdst are re-produced in this paper.
bert in all models usespre-trained bert (vaswani et al., 2017) (bert-base, uncased) which has 12 hidden layers of 768units and 12 self-attention heads, while rnn uses.
2https://github.com/clovaai/som-dst3https://github.com/sktbrain/sumbt.
gru (cho et al., 2014).
we use adam (kingma andba, 2014) as the optimizer and use greedy decoding.
we customize the training epochs for all models,and the training stopped early when the model’sperformance on development set failed to improvefor 15 consecutive epochs, and all the results wereaveraged over the three runs with different randomseeds.
the detailed setting of the hyperparametersis given in appendix a..since the length of the dialogue history is relatedto the granularity, the input length of the modelneeds to adapt to the granularity.
especially forthe model with bert as the encoder, in order toprevent the input from being truncated, we set themax sequence length to exceed almost all the inputsunder different granularity.
see appendix a fordetails on the max sequence length settings..following previous works (xu and hu, 2018;wu et al., 2019; kim et al., 2020; heck et al., 2020),the joint accuracy (joint acc) and slot accuracy(slot acc) are used for evaluation.
the joint ac-curacy is the accuracy that checks whether all thepredicted slot values in each turn are exactly thesame as the ground truth slot values.
the slot accu-racy is the average accuracy of slot value predictionin all turns..4 experimental analysis.
this section presents our detailed investigation ofhow the context information of different granular-ity affects dialogue state tracking, focusing on theimpact of granularity on dialogue state tracking,the combination of multiple granularities, and theapplication of context granularity in few-shot learn-ing scenario.
for simplicity, in all experimentalresults, the maximum granularity is expressed as 0,the maximum granularity minus 1 is expressed as-1, and so on..4.1 how greatly different granularities affect.
dialogue state tracking?.
the ﬁrst part of our investigation look at the validityof the context granularity used by the current vari-ous dialogue state tracking models and try to ﬁgureout how different granularities affect dialogue statetracking.
the experimental results are shown intable 3..it can be found that some dialogue state trackingmodels do not take the appropriate granularity, andtheir performance is greatly improved when theyare trained with the the context of appropriate gran-.
2484models.
tg ig.
woz2.0joint acc slot acc.
dstc2joint acc slot acc.
multiwoz2.1joint acc slot acc.
spanptr.
trade.
bertdst.
somdst.
0*-1-2-30*-1-2-31*2341*234.
0*-1-2-30*-1-2-31*2341*234.
0.44550.50120.58810.63300.58080.51940.56800.52920.81940.82200.81900.82560.85400.82740.82800.8620.
0.74750.77860.81210.83500.81860.78330.81070.78860.93070.93180.93180.93440.94710.93410.93560.9491.
0.62340.58290.48250.47370.64930.50130.41850.51710.63950.58300.56140.56660.69750.70220.71210.7176.
0.84610.82510.77280.76280.85900.78340.74880.79630.85370.82710.81030.81520.88280.88080.88510.8882.
0.44150.38680.37260.37450.44200.39630.35280.35640.41400.45860.47720.49170.50290.51790.51280.5085.
0.95700.94950.94990.95070.96550.96130.95690.95520.95840.96360.96460.96590.97150.97300.97200.9718.table 3: joint accuracy and slot accuracy on woz2.0, dstc2 and multiwoz2.1 when the same granularitiesare used in the training and inference phases.
tg and ig are the training granularity and inference granularity,respectively.
* refers to the granularity originally used in the baseline..ularity.
for example, the joint accuracy of spanptrwith granularity -3 on woz2.0 improved by 42%,while the joint accuracy of bertdst with granu-larity 4 on multiwoz2.1 improved by 19%.
theseresults suggest that there are signiﬁcant differencesin dialogue state tracking at different granularities,therefore, we should be careful to determine thegranularity to be used according to the characteris-tics of the model and dataset..by observing the experimental comparison re-sults on different models and datasets in table 3, itcan be found that:.
• for different models, the model with gener-ative decoding prefer larger granularity, be-cause it requires more context informationto effectively learn vocabulary-based distribu-tion.
for example, trade and somdstboth perform better in larger granularity.
meanwhile, the model with extractive decod-ing is more dependent on the characteristicsof the dataset.
besides, in general, the modelwith generative decoding has obvious advan-tages over the model with extractive decoding..• for different datasets, when the dataset in-volves multiple domains and there are a largenumber of long-dependency dialogue states,context information of larger granularity can.
figure 2: joint accuracy of baseline model when con-text information with different granularity is used intraining and inference phases.
ig is the inference gran-ularity..be used to more effectively capture the long-dependency relationship in the data for dia-logue state tracking, such as multiwoz2.1dataset.
for simpler single-domain datasets,where a large number of short dependencies.
2485(a) joint accuracy on woz2.0 with ig=0(c) joint accuracy on dstc2 with ig=0(e) joint accuracy on multiwoz2.1 with ig=0(b) joint accuracy on woz2.0 with ig=1(d) joint accuracy on dstc2 with ig=1(f) joint accuracy on multiwoz2.1 with ig=1models.
tg ig.
woz2.0joint acc slot acc.
dstc2joint acc slot acc.
multiwoz2.1joint acc slot acc.
spanptr.
trade.
bertdst.
somdst.
sumbt.
0*0, -10*0, -11*1, 21*1, 21*1, 2.
0*00*01*11*11*1.
0.44550.48040.58080.61020.81940.83310.85400.85720.90520.9089.
0.74750.74280.81860.83570.93070.93680.94710.94790.96650.9677.
0.62340.60780.64930.60300.63950.58240.69750.70770.65710.6739.
0.84610.83710.85900.84130.85370.82900.88280.88660.86640.8716.
0.44150.44300.44200.44100.41400.42290.50290.51260.46320.4725.
0.95700.95650.96550.96550.95840.96020.97150.97230.96550.9663.table 4: comparison of different baseline models on woz2.0, dstc2 and multiwoz2.1 before and after apply-ing multi-granularity combination.
tg and ig are the training granularity and inference granularity, respectively.
* refers to the granularity originally used in the baseline..figure 3: joint accuracy of baseline model with different multi-granularity combinations is adopted in trainingphase.
ig is the inference granularity..determine the effectiveness of small granular-ity in dialogue state tracking.
however, whenthere are more turns of dialogue resulting inless information in each turn, a larger gran-ularity may be required to provide enoughinformation, for example, spanptr performsbest on the dstc2 dataset at maximum gran-ularity..as can be seen from the above analysis, differentgranularities have their own advantages in differentsituations of dialogue, so it is natural to wonderwhether multiple granularities can be combined toachieve better dialogue state tracking.
next, let’s.
discuss the issue of multi-granularity combination..4.2 how to combine multiple granularities.
for dialogue state tracking?.
following the above analysis, here we mainly dis-cuss how to combine multiple granularities in di-alogue state tracking, mainly focusing on threeaspects: (1) the relationship between granularities,(2) performance of multi-granularity combinationand (3) limitations of multi-granularity combina-tion..the relationship between granularities: first,we use different granularities in the training and.
2486(a) spanptr on woz2.0 with ig=0(b) trade on woz2.0 with ig=0(c) bertdst on woz2.0 with ig=1(d) somdst on woz2.0 with ig=1(e) spanptr on dstc2 with ig=0(f) trade on dstc2 with ig=0(g) bertdst on dstc2 with ig=1(h) somdst on dstc2 with ig=1(i) spanptr on multiwoz2.1 with ig=0(j) trade on multiwoz2.1 with ig=0(k) bertdst on multiwoz2.1 with ig=1(l) somdst on multiwoz2.1 with ig=10.440.460.480.500,-10,-1,-20,-1,-2,-3accuracytraining granularityspanptr0.580.590.60.6100,-10,-1,-20,-1,-2,-3accuracytraining granularitytrade0.810.820.830.8411,21,2,31,2,3,4accuracytraining granularitybertdst0.850.860.870.8811,21,2,31,2,3,4accuracytraining granularitysomdst0.580.60.620.6400,-10,-1,-20,-1,-2,-3accuracytraining granularityspanptr0.550.590.630.6700,-10,-1,-20,-1,-2,-3accuracytraining granularitytrade0.50.550.60.6511,21,2,31,2,3,4accuracytraining granularitybertdst0.6950.70.7050.7111,21,2,31,2,3,4accuracytraining granularitysomdst0.4350.440.4450.4500,-10,-1,-20,-1,-2,-3accuracytraining granularityspanptr0.430.4350.440.44500,-10,-1,-20,-1,-2,-3accuracytraining granularitytrade0.410.4150.420.42511,21,2,31,2,3,4accuracytraining granularitybertdst0.50.5050.510.51511,21,2,31,2,3,4accuracytraining granularitysomdstinference phases of dialogue state tracking to ﬁgureout the relationship between different granularities,as shown in figure 2. it can be seen that whenwe ﬁx the granularity of context information in theinference phase, the dialogue state tracking modeltrained with other granularity still obtains the gener-alization under this inference granularity.
and evensome models learned at other granularity, such asthe bertdst in figure 2 (b) and (f), can performbetter.
meanwhile, it can also be found that as thegranularity gap increases, the context informationbecomes more and more inconsistent, and eventu-ally the ability of the model to generalize acrossgranularity is gradually reduced.
through thesephenomena, we can summarize as follows: theknowledge learned by the dialogue state trackingmodel in context information of different granular-ity is transferable and the smaller the gap betweengranularity can bring more knowledge transfer ef-fect..performance of multi-granularity combination:then, we use the knowledge transfer between con-text information of different granularity to improvethe baseline.
in the speciﬁc experiment, we add themost adjacent granularity to the training phase ofthe model, that is, the context under two granulari-ties is used for training, while the inference phaseremains unchanged, as shown in table 4. it canbe observed that in most cases, the performanceof the baseline models is signiﬁcantly enhanced,suggesting that adding more granularity contextinformation to the training phase of the model canindeed improve the generalization of the dialoguestate tracking model.
of course, in some cases,multi-granularity combination results in a reduc-tion in performance, such as spanptr, trade, andbertdst on dstc2 dataset.
the main reason forthis phenomenon should be the large deviation be-tween the context information of different granular-ity in the multi-granularity combination, as can beseen from the large reduction of spanptr, trade,and bertdst on the dstc2 dataset with othergranularity in table 3..limitations of multi-granularity combination:given that multi-granularity combination can leadto improved generalization performance, is it betterto have more context information of different gran-ularity in training phase?
to answer this question,we gradually add more granularities to the train-ing phase while keeping the inference granularity.
figure 4: joint accuracy of baseline model and im-provement ratio of multi-granularity combination un-der different scales of training data.
items in differentcolors represent different granularity combinations intraining phase, and ig is the inference granularity..unchanged, the experimental results are shown infigure 3. it can be found that there is an upper limitto the use of multi-granularity combination in thetraining phase.
generally, adding the granularitywith the smallest gap can bring the best effect, afterthat, with the increase of granularity number, theperformance will decline..4.3 application of context information.
granularity in few-shot learning scenario.
considering the knowledge transfer between granu-larity in multi-granularity combination, we explorethe application of multi-granularity combination infew-shot learning scenario..figure 4 shows the joint accuracy of the modelwith different multi-granularity combinations andthe percentage improvement relative to the base-line model on the woz2.0 dataset with different.
2487(a) joint accuracy and improvement  of spanptr on woz2.0 with ig=00.10.20.30.40.5100%80%60%40%20%10%accuracyscaleof training data00,-10,-1,-20,-1,-2,-3-20-10010203040100%80%60%40%20%10%improvement ratio (%)scaleof training data00,-10,-1,-20,-1,-2,-3(a) joint accuracy and improvement  of spanptr on woz2.0 with ig=00.10.20.30.40.5100%80%60%40%20%10%accuracyscaleof training data00,-10,-1,-20,-1,-2,-3-20-10010203040100%80%60%40%20%10%improvement ratio (%)scaleof training data00,-10,-1,-20,-1,-2,-3(b) joint accuracy and improvement of trade on woz2.0 with ig=00.10.20.30.40.50.60.7100%80%60%40%20%10%accuracyscaleof training data00,-10,-1,-20,-1,-2,-3-2024681012100%80%60%40%20%10%improvement ratio (%)scale of training data00,-10,-1,-20,-1,-2,-3(b) joint accuracy and improvement of trade on woz2.0 with ig=00.10.20.30.40.50.60.7100%80%60%40%20%10%accuracyscaleof training data00,-10,-1,-20,-1,-2,-3-2024681012100%80%60%40%20%10%improvement ratio (%)scale of training data00,-10,-1,-20,-1,-2,-3(c) joint accuracy and improvement of bertdst on woz2.0 with ig=10.50.60.70.80.9100%80%60%40%20%10%accuracyscaleof training data11,21,2,31,2,3,4-4-202468100%80%60%40%20%10%improvement ratio (%)scale of training data11,21,2,31,2,3,4(c) joint accuracy and improvement of bertdst on woz2.0 with ig=10.50.60.70.80.9100%80%60%40%20%10%accuracyscaleof training data11,21,2,31,2,3,4-4-202468100%80%60%40%20%10%improvement ratio (%)scale of training data11,21,2,31,2,3,4(d) joint accuracy and improvement of somdst on woz2.0 with ig=10.40.50.60.70.80.9100%80%60%40%20%10%accuracyscaleof training data11,21,2,31,2,3,4024681012100%80%60%40%20%10%improvement ratio (%)scale of training data11,21,2,31,2,3,4(d) joint accuracy and improvement of somdst on woz2.0 with ig=10.40.50.60.70.80.9100%80%60%40%20%10%accuracyscaleof training data11,21,2,31,2,3,4024681012100%80%60%40%20%10%improvement ratio (%)scale of training data11,21,2,31,2,3,4(e) joint accuracy and improvement of sumbt on woz2.0 with ig=10.40.50.60.70.80.91100%80%60%40%20%10%accuracyscaleof training data11,21,2,31,2,3,4-5051015100%80%60%40%20%10%improvement ratio (%)scale of training data11,21,2,31,2,3,4(e) joint accuracy and improvement of sumbt on woz2.0 with ig=10.40.50.60.70.80.91100%80%60%40%20%10%accuracyscaleof training data11,21,2,31,2,3,4-5051015100%80%60%40%20%10%improvement ratio (%)scale of training data11,21,2,31,2,3,4models.
tg.
sim-m sim-r woz2.0 dstc2 multiwoz2.1.
10%.
10%.
10%.
5%.
ig.
0*0000*0001*1111*1111*111.
0*0, -10, -1, -20, -1, -2, -30*0, -10, -1, -20, -1, -2, -31*1, 21, 2, 31, 2, 3, 41*1, 21, 2, 31, 2, 3, 41*1, 21, 2, 31, 2, 3, 4.spanptr.
trade.
bertdst.
somdst.
sumbt.
0.14660.11880.09850.08720.07800.08800.08920.09210.48140.62190.59260.60750.27080.27540.25490.21040.09820.09800.09800.0968.
0.51470.56310.57520.58050.65310.65120.66120.65690.70660.72950.73760.72410.47000.51010.51660.51420.65260.65460.63900.6464.
0.17440.22110.21650.23130.20470.21530.21930.20980.58000.57700.61380.61360.51400.55630.56620.53300.45810.46900.48480.4708.
0.45230.46400.48390.48730.52900.51080.51730.51010.46970.51370.47120.49290.39670.51510.53070.52380.46890.54930.52650.5611.
5%.
0.27000.27030.27650.27620.25310.24000.24700.24610.34140.34910.34500.33770.35960.37060.36130.35720.29640.35350.36960.3637.table 5: joint accuracy of baseline models in few-shot learning before and after applying multi-granularity com-bination in training phase.
tg and ig are the training granularity and inference granularity, respectively.
* refersto the granularity originally used in the baseline.
10% and 5% refer to the scale of the training data..training data scales.
it can be found that underdifferent scales of training data, multi-granularitycombination can achieve better performance com-pared with single-granularity in most cases.
more-over, it can be seen from (a), (d) and (e) that theadvantages of multi-granularity combination aregradually expanding with the decrease of the scaleof training dataset.
therefore, the performance ofmulti-granularity combination in few-shot learningis worth exploring..we conduct detailed experiments on all the 5datasets in the paper to fully explore the poten-tial of multi-granularity combination in few-shotlearning, as shown in table 5. it can be found thatmulti-granularity combination has a very signiﬁ-cant effect in few-shot learning, and in some casescan even achieve a relative improvement of morethan 10%, such as spanptr on sim-r and woz2.0,bertdst on sim-m, somdst on woz2.0 anddstc2.
meanwhile, in few-shot learning, the up-per limit of multi-granularity combination can behigher, and better performance can be achievedwhen more granularities are added in the trainingphase..the above experimental.
results of multi-granularity combination in few-shot learning showthat, there is indeed knowledge transfer betweendifferent granularity contexts, and the model canobtain more adequate modeling of dialogue bylearning context dialogues of different granularity..5 conclusion.
in the paper, we analyze the defects of two existingtraditional dialogue state tracking strategies whendealing with context of different granularity andmake a comprehensive study on how the context in-formation of different granularity affects dialoguestate tracking.
extensive experimental results andanalysis show that: (1) different granularities havetheir own advantages in different situations of dia-logue state tracking; (2) the multi-granularity com-bination can effectively improve the dialogue statetracking; (3) the application of multi-granularitycombination in few-shot learning can bring sig-niﬁcant effects.
in future work, dynamic contextgranularity can be used in training and inference tofurther improve dialogue state tracking..24886 ethical consideration.
this work may contribute to the development ofconversational systems.
in the narrow sense, thiswork focuses on dialogue state tracking in task-oriented dialogue system, hoping to improve theability of conversational ai to understand humannatural language.
if so, these improvements couldhave a positive impact on the research and applica-tion of conversational ai, which could help humansto complete goals more effectively in a more intel-ligent way of communication.
however, we neverforget the other side of the coin.
the agent substitu-tion of conversational ai may affect the humanizedcommunication and may lead to human-machineconﬂict problems, which need to be consideredmore broadly in the ﬁeld of conversational ai..acknowledgments.
we thank the anonymous reviewers for their in-sightful comments.
this work was supported bynational key r&d plan (no.
2020aaa0106600)and national natural science foundation of china(grant no.
u19b2020 and no.
61772076)..references.
paweł budzianowski, tsung-hsien wen, bo-hsiangtseng, i˜nigo casanueva, stefan ultes, osman ra-madan, and milica gasic.
2018. multiwoz-a large-scale multi-domain wizard-of-oz dataset for task-oriented dialogue modelling.
in proceedings of the2018 conference on empirical methods in naturallanguage processing, pages 5016–5026..guan-lin chao and ian lane.
2019. bert-dst: scalableend-to-end dialogue state tracking with bidirectionalencoder representations from transformer.
proc.
in-terspeech 2019, pages 1468–1472..hongshen chen, xiaorui liu, dawei yin, and jiliangtang.
2017. a survey on dialogue systems: re-cent advances and new frontiers.
acm sigkdd ex-plorations newsletter, 19(2):25–35..kyunghyun cho, b van merrienboer, caglar gulcehre,f bougares, h schwenk, and yoshua bengio.
2014.learning phrase representations using rnn encoder-decoder for statistical machine translation.
in con-ference on empirical methods in natural languageprocessing (emnlp 2014)..rahul goel, shachi paul, tagyoung chung, jeremielecomte, arindam mandal, and dilek hakkani-tur.
2018. flexible and scalable state tracking frame-arxivwork for goal-oriented dialogue systems.
preprint arxiv:1811.12891..rahul goel, shachi paul, and dilek hakkani-t´ur.
2019.hyst: a hybrid approach for ﬂexible and accurate di-alogue state tracking.
proc.
interspeech 2019, pages1458–1462..michael heck, carel van niekerk, nurul lubis, chris-tian geishauser, hsien-chin lin, marco moresi, andmilica gasic.
2020. trippy: a triple copy strategyfor value independent neural dialog state tracking.
in proceedings of the 21th annual meeting of thespecial interest group on discourse and dialogue,pages 35–44..matthew henderson, blaise thomson, and jason dwilliams.
2014. the second dialog state trackingchallenge.
in proceedings of the 15th annual meet-ing of the special interest group on discourse anddialogue (sigdial), pages 263–272..sungdong kim, sohee yang, gyuwan kim, and sang-woo lee.
2020. efﬁcient dialogue state tracking byselectively overwriting memory.
in proceedings ofthe 58th annual meeting of the association for com-putational linguistics, pages 567–582..diederik p kingma and jimmy ba.
2014. adam: amethod for stochastic optimization.
arxiv preprintarxiv:1412.6980..hwaran lee, jinsik lee, and tae-yoon kim.
2019.sumbt: slot-utterance matching for universal andscalable belief tracking.
in proceedings of the 57thannual meeting of the association for computa-tional linguistics, pages 5478–5483..wenqiang lei, xisen jin, min-yen kan, zhaochunren, xiangnan he, and dawei yin.
2018. sequicity:simplifying task-oriented dialogue systems with sin-gle sequence-to-sequence architectures.
in proceed-ings of the 56th annual meeting of the associationfor computational linguistics (volume 1: long pa-pers), pages 1437–1447..nikola mrkˇsi´c, diarmuid ´o s´eaghdha, tsung-hsienwen, blaise thomson, and steve young.
2017. neu-ral belief tracker: data-driven dialogue state track-ing.
in proceedings of the 55th annual meeting ofthe association for computational linguistics (vol-ume 1: long papers), pages 1777–1788..abhinav rastogi, raghav gupta, and dilek hakkani-tur.
2018. multi-task learning for joint languagein pro-understanding and dialogue state tracking.
ceedings of the 19th annual sigdial meeting on dis-course and dialogue, pages 376–384..mihail eric, rahul goel, shachi paul, abhishek sethi,sanchit agarwal, shuyag gao, and dilek hakkani-tur.
2019. multiwoz 2.1: multi-domain dialoguestate corrections and state tracking baselines.
arxivpreprint arxiv:1907.01669..abhinav rastogi, dilek hakkani-t¨ur, and larry heck.
2017. scalable multi-domain dialogue state track-in 2017 ieee automatic speech recognitioning.
and understanding workshop (asru), pages 561–568. ieee..2489liliang ren, jianmo ni, and julian mcauley.
2019.scalable and accurate dialogue state tracking via hi-in proceedings oferarchical sequence generation.
the 2019 conference on empirical methods in nat-ural language processing and the 9th internationaljoint conference on natural language processing(emnlp-ijcnlp), pages 1876–1885..pararth shah, dilek hakkani-t¨ur, gokhan t¨ur, ab-hinav rastogi, ankur bapna, neha nayak, andlarry heck.
2018. building a conversational agentovernight with dialogue self-play.
arxiv preprintarxiv:1801.04871..yong shan, zekang li, jinchao zhang, fandong meng,yang feng, cheng niu, and jie zhou.
2020. a con-textual hierarchical attention network with adaptiveobjective for dialogue state tracking.
in proceedingsof the 58th annual meeting of the association forcomputational linguistics, pages 6322–6333..ashish vaswani, noam shazeer, niki parmar, jakobuszkoreit, llion jones, aidan n gomez, łukaszkaiser, and illia polosukhin.
2017. attention is allyou need.
in proceedings of the 31st internationalconference on neural information processing sys-tems, pages 6000–6010..tsung-hsien wen, david vandyke, nikola mrksic,milica gasic, lina m rojas-barahona, pei-hao su,stefan ultes, and steve young.
2016. a network-based end-to-end trainable task-oriented dialoguesystem.
arxiv preprint arxiv:1604.04562..chien-sheng wu, andrea madotto, ehsan hosseini-asl, caiming xiong, richard socher, and pascalefung.
2019. transferable multi-domain state gener-ator for task-oriented dialogue systems.
in proceed-ings of the 57th annual meeting of the associationfor computational linguistics, pages 808–819..puyang xu and qi hu.
2018. an end-to-end approachfor handling unknown slot values in dialogue statetracking.
in proceedings of the 56th annual meet-ing of the association for computational linguistics(volume 1: long papers), pages 1448–1457..jianguo zhang, kazuma hashimoto, chien-sheng wu,yao wang, s yu philip, richard socher, and caim-ing xiong.
2020. find or classify?
dual strategy forslot-value predictions on multi-domain dialog statein proceedings of the ninth joint con-tracking.
ference on lexical and computational semantics,pages 154–167..su zhu, jieyu li, lu chen, and kai yu.
2020. efﬁ-cient context and schema fusion networks for multi-in proceedings ofdomain dialogue state tracking.
the 2020 conference on empirical methods in natu-ral language processing: findings, pages 766–781..2490appendices.
a settings.
hyperparameters.
spanptr.
trade bertdst somdst sumbt.
batch sizetraining epochsearly stop evaluationdecoder teacher forcingdropoutword dropoutrnn hidden size.
32100joint acc-0.10.1400.
32100joint acc0.50.10.1400.learning rate.
1e-4.
1e-3.
1616200200joint accjoint acc0.5-0.10.10.10.1768768enc: 4e-5enc: 4e-5dec: 1e-4 dec: 1e-4.
warmup proportion.
-.
-.
0.1.
0.1.
4300loss---300.
5e-5.
0.1.table 6: the detailed setting of hyperparameters.
word dropout means to randomly replace the input tokens withthe special [unk] with a certain probability..models.
tg sim-m sim-r woz2.0 dstc2 multiwoz2.1.
spanptrtrade.
bertdst.
somdst.
sumbt.
--123412341234.
--70901201501201501802006090120130.
--7012015018012015019022070120140160.
--100150170200120150190220100120140160.
--6080110140701001301605070100120.
--100150210260320360410460100150210260.table 7: the setting of the max sequence length of bert in encoders of different models.
to minimize truncationof the input, the max sequence length exceeds the length of almost all input sequences in the dataset.
spanptr andtrade use gru as encoders.
tg is the training granularity..2491