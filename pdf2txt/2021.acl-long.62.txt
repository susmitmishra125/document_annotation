compare to the knowledge: graph neural fake news detectionwith external knowledge.
linmei hu1, tianchi yang1, luhao zhang2, wanjun zhong3, duyu tang4,chuan shi1∗ , nan duan4, ming zhou41school of computer science, beijing university of posts and telecommunications2meituan 3sun yat-sen university 4microsoft research asia{hulinmei,yangtianchi,shichuan}@bupt.edu.cnzhangluhao@meituan.comzhongwj25@mail2.sysu.edu.cn{dutang,nanduan,mingzhou}@microsoft.com.
abstract.
nowadays, fake news detection, which aims toverify whether a news document is trusted orfake, has become urgent and important.
mostexisting methods rely heavily on linguistic andsemantic features from the news content, andfail to effectively exploit external knowledgewhich could help determine whether the newsin this paper, we pro-document is trusted.
pose a novel end-to-end graph neural modelcalled comparenet, which compares the newsto the knowledge base (kb) through entitiesfor fake news detection.
considering that fakenews detection is correlated with topics, wealso incorporate topics to enrich the news rep-resentation.
speciﬁcally, we ﬁrst constructa directed heterogeneous document graph foreach news incorporating topics and entities.
based on the graph, we develop a heteroge-neous graph attention network for learning thetopic-enriched news representation as well asthe contextual entity representations that en-code the semantics of the news content.
thecontextual entity representations are then com-pared to the corresponding kb-based entityrepresentations through a carefully designedentity comparison network, to capture the con-sistency between the news content and kb.
fi-nally, the topic-enriched news representationcombining the entity comparison features arefed into a fake news classiﬁer.
experimen-tal results on two benchmark datasets demon-strate that comparenet signiﬁcantly outper-forms state-of-the-art methods..1.introduction.
with the rapid development of the internet, thereare increasingly huge opportunities for fake news.
∗the work was done while visiting micorosft research.
asia..production, dissemination and consumption.
fakenews are news documents that are intentionally andveriﬁably false, and could mislead readers (allcottand gentzkow, 2017).
fake news can easily mis-guide public opinion, cause the crisis of conﬁdence,and disturb the social order (vosoughi et al., 2018).
it is well known that fake news exerted an inﬂuencein the past 2016 us presidential elections (allcottand gentzkow, 2017).
thus, it is very importantto develop effective methods for early fake newsdetection based on the textual content of the newsdocument..some existing fake news detection methods relyheavily on various hand-crafted linguistic and se-mantic features for differentiating between newsdocuments (conroy et al., 2015; rubin et al., 2016;rashkin et al., 2017; khurana and intelligentie,2017; shu et al., 2020).
to avoid feature engi-neering, deep neural models such as bi-lstm andconvolutional neural networks (cnn) have beenemployed (oshikawa et al., 2020; wang, 2017;rodr´ıguez and iglesias, 2019).
however, they failto consider the sentence interactions in the docu-ment.
vaibhav et al.
showed that trusted news andfake news have different patterns of sentence in-teractions (vaibhav et al., 2019).
they modeled anews document as a fully connected sentence graphand proposed a graph attention model for fake newsdetection.
although these existing approaches canbe effective, they fail to fully exploit external kbwhich could help determine whether the news isfake or trusted..external kb such as wikipedia contains alarge amount of high-quality structured subject-predicate-object triplets and unstructured entity de-scriptions, which could serve as evidence for de-tecting fake news.
as shown in figure 4, the news.
proceedingsofthe59thannualmeetingoftheassociationforcomputationallinguisticsandthe11thinternationaljointconferenceonnaturallanguageprocessing,pages754–763august1–6,2021.©2021associationforcomputationallinguistics754document about “mammograms are not effectiveat detecting breast tumors” is likely to be detectedas fake news with the knowledge that “ the goal ofmammography is the early detection of breast can-cer” in the wikipedia entity description page 1. panet al.
proposed to construct knowledge graphs frompositive and negative news, and apply transe tolearn triplet scores for fake news detection (panet al., 2018).
nevertheless, the performance islargely inﬂuenced by construction of the knowl-edge graph.
in this paper, to take full advantage ofthe external knowledge, we propose a novel end-to-end graph neural model comparenet which di-rectly compares the news to the kb through entitiesfor fake news detection.
in comparenet, we alsoconsider using topics to enrich the news documentrepresentation for improving fake news detection,since fake news detection and topics are highly cor-related (zhang et al., 2020; jin et al., 2016).
forexample, the news documents in the “health” topicare inclined towards false, while the documentsbelonging to the “economy” topic are biased to betrusted instead..particularly, we ﬁrst construct a directed het-erogeneous document graph for each news doc-ument, containing sentences, topics and entitiesas nodes.the sentences are fully connected in bi-direction.
each sentence is also connected withits top relevant topics in bi-direction.
if a sen-tence contains an entity, one directed link is builtfrom the sentence to the entity.
the reason forbuilding one-way links from sentences to entitiesis to ensure that we can learn contextual entityrepresentations that encode the semantics of thenews, while avoiding the inﬂuence of the true en-tity knowledge to the news representation.
basedon the directed heterogeneous document graph, wedevelop a heterogeneous graph attention networkto learn topic-enriched news representations andcontextual entity representations.
the learned con-textual entity representations are then compared tothe corresponding kb-based entity representationswith a carefully designed entity comparison net-work, in order to capture the semantic consistencybetween the news content and external kb.
finally,the topic-enriched news representations and theentity comparison features are combined for fakenews classiﬁcation.
to facilitate related researches,we release both our code and dataset to the public2..1https://en.wikipedia.org/wiki/mammography2https://github.com/ytc272098215/fakenewsdetection.
in summary, our main contributions include:.
1) in this paper, we propose a novel end-to-endgraph neural model comparenet which com-pares the news to the external knowledgethrough entities for fake news detection..2) in comparenet, we also consider the usefultopic information.
we construct a directedheterogeneous document graph incorporatingtopics and entities.
then we develop heteroge-neous graph attention networks to learn topic-enriched news representations.
a novel entitycomparison network is designed to comparethe news to the kb..3) extensive experiments on two benchmarkdatasets demonstrate that our model signiﬁ-cantly outperforms state-of-the-art models onfake news detection by effectively incorporat-ing external knowledge and topic information..2 related work.
fake news detection has attracted much attention inrecent years (zhou and zafarani, 2020; oshikawaet al., 2020).
a lot of works also focus on therelated problem, i.e., fact checking, which aims tosearch evidence from external knowledge to verifythe veracity of a claim (e.g., a subject-predicate-object triple) (thorne et al., 2018; zhou et al., 2019;zhong et al., 2020).
generally, fake news detectionusually focuses on news events while fact-checkingis broader (oshikawa et al., 2020).
the approachesfor fake news detection can be divided into twocategories: social-based and content-based..2.1 social-based fake news detection.
social context related to news documents con-tains rich information such as user proﬁles andsocial relationships to help detect fake news.
so-cial based models basically include stance-basedand propagation-based.
stance-based models uti-lize users’ opinions to infer news veracity (jinet al., 2016; wu et al., 2019).
tacchini et al.
con-structed a bipartite network of user and posts with‘like’ stance information, and proposed a semi-supervised probabilistic model to predict the likeli-hood of posts being hoaxes (tacchini et al., 2017).
propagation-based approaches for fake news de-tection are based on the basic assumption that thecredibility of a news event is highly related to thecredibilities of relevant social media posts.
both.
755figure 1: an example of directed heterogeneous document graph incorporating topics and entities..homogeneous (jin et al., 2016) and heterogeneouscredibility networks (gupta et al., 2012; shu et al.,2019; zhang et al., 2020) have been built to modelthe propagation process.
for instance, (zhanget al., 2020) constructed a heterogeneous networkof news articles, creators and news subjects, andproposed a deep diffusive network model for in-corporating the network structure information tosimultaneously detect fake news articles, creatorsand subjects..parenet which directly compares the news to ex-ternal knowledge for fake news detection.
consid-ering that the detection of fake news is correlatedwith topics, we also use topics to enrich the newsrepresentation for improving fake news detection..some works (wang, 2017; khattar et al., 2019;wang et al., 2020) also consider incorporatingmulti-modal features such as images for improvingfake news detection..2.2 content-based fake news detection.
on the other hand, news contents contain the cluesto differentiate fake and trusted news.
a lot of ex-isting works extract speciﬁc writing styles such aslexical and syntactic features (conroy et al., 2015;rubin et al., 2016; khurana and intelligentie, 2017;rashkin et al., 2017; shu et al., 2020; oshikawaet al., 2020) and sensational headlines (potthastet al., 2018; sitaula et al., 2019) for fake news clas-siﬁer.
to avoid hand-crafted feature engineering,neural models have been proposed (wang, 2017;rodr´ıguez and iglesias, 2019).
for example, ibrainet al.
applied deep neural networks, such as bi-lstm and convolutional neural networks (cnn)for fake news detection (rodr´ıguez and iglesias,2019).
however, these works fail to consider differ-ent sentence interaction patterns between trustedand fake news documents.
vaibhav et al.
proposedto model a document as a sentence graph captur-ing the sentence interactions and applied graphattention networks for learning document represen-tation (vaibhav et al., 2019).
pan et al.
proposedto construct knowledge graphs from positive andnegative news, and apply transe to learn tripletscores for fake news detection (pan et al., 2018).
nevertheless, they relied heavily on the quality ofthe construction of knowledge graphs.
in this pa-per, we propose a novel graph neural model com-.
3 our proposed comparenet.
in this section, we detail our proposed fake newsdetection model comparenet, which directly com-pares the news to external knowledge for fake newsdetection.
as shown in figure 2, we also con-sider topics for enriching news representation sincefake news detection is highly correlated with topics(zhang et al., 2020).
speciﬁcally, we ﬁrst constructa directed heterogeneous document graph for eachnews document incorporating topics and entitiesas shown in figure 1. the graph well captures theinteractions among sentences, topics and entities.
based on the graph, we develop a heterogeneousgraph attention network to learn the topic-enrichednews representation as well as the contextual entityrepresentations that encode the semantics of thenews document.
to fully leverage external kb, wetake the entities as the bridge between the newsdocument and the kb.
we compare the contex-tual entity representations with the correspondingkb-based entity representations using a carefullydesigned entity comparison network.
finally, theobtained entity comparison features are combinedwith the topic-enriched news document representa-tion for fake news detection..756figure 2: the overview of our proposed model comparenet..3.1 directed heterogeneous document.
graph.
for each news document d, we construct a directedheterogeneous document graph g = (v, e) incor-porating topics and entities, as shown in figure1. there are three kinds of nodes in the graph:sentences s = {s1, s2, · · ·, sm}, topics t ={t1, t2, · · ·, tk} and entities e = {e1, e2, · · ·, en},i.e., v = s ∪ t ∪ e. the set of edges e representthe relations among sentences, topics and entities.
the details of constructing the graph are describedas follows..we ﬁrst split the news document as a set of sen-tences.
sentences are bidirectionally connectedwith each other in the graph, capturing the inter-action of each sentence with every other sentence.
since topic information is important for fake newsdetection (zhang et al., 2020), we apply the unsu-pervised lda (blei et al., 2003) (the total topicnumber k is set as 100) to mine the latent topics tfrom all the sentences of all the documents in ourdataset.
speciﬁcally, each sentence is taken as apseudo-document and is assigned to the top p rele-vant topics with the largest probabilities.
thus,each sentence is also connected with its top passigned topics in bi-direction, allowing the use-ful topic information to propagate among the sen-tences.
note that we can also deal with new comingnews documents by inferring the topics with trainedlda.
we identify the entities e in the document dand map them to wikipedia using the entity linking.
tool tagme3.
if a sentence s contains an entity e,we build a one-way directed edge from a sentenceto the entity e, in order to allow only informationpropagation from sentences to entities.
in this way,we can avoid integrating true entity knowledge di-rectly into news representation, which may misleadthe detection of fake news..3.2 heterogeneous graph convolution.
based on the above directed heterogeneous docu-ment graph g, we develop a heterogeneous graphattention network for learning the news representa-tion as well as the contextual entity representations.
it considers not only the weights of different nodeswith different types (hu et al., 2019) but also theedge directions in the heterogeneous graph..formally, we have three types t = {τ1, τ2, τ3}of nodes: sentences s, topics t and entities e withdifferent feature spaces.
we apply lstm to encodea sentence s = {w1, · · ·, wm} and get its featurevector xs ∈ rm .
the entity e ∈ e is initializedwith the entity representations ekb ∈ rm learnedfrom the external kb (see subsection 3.3.1).
thetopic t ∈ t is initialized with one-hot vector xt ∈rk..next, consider the graph g = (v, e) where vand e represent the set of nodes and edges respec-tively.
let x ∈ r|v|×m be a matrix containingthe nodes with their features xv ∈ rm (each rowxv is a feature vector for a node v).
a and d are.
3https://sobigdata.d4science.org/group/tagme/.
757the adjacency matrix and the degree matrix, re-spectively.
the heterogeneous convolution layerupdates the (l + 1)-th layer representation of thenodes h(l+1) by aggregating the features of theirneighboring nodes h(l)τ with different types τ .
(ini-tially, h(0) = x):.
h(l+1) = σ(.
bτ · h(l)τ.
· w(l).
τ ),.
(1).
(cid:88).
τ ∈t.
τ .
the transformation matrix w(l).
where σ(·) denotes the activation function.
nodeswith different types τ have different transformationmatrix w(l)τ con-siders the different feature spaces and projects theminto an implicit common space.
bτ ∈ r|v|×|vτ |is the attention matrix, whose rows represent allthe nodes and columns represent their neighboringnodes with the type τ .
its element βvv(cid:48) in the v-throw and the v(cid:48)-th column is computed as follows:.
βvv(cid:48) = softmaxv(cid:48)(σ(νt · ατ [hv, hv(cid:48)])),.
(2).
where ν is the attention vector and ατ is the type-level attention weight.
hv and hv(cid:48) are respectivelythe representation of the current node v and itsneighboring node v(cid:48).
softmax function is appliedto normalize across the neighboring nodes of nodev..we calculate the type-level attention weights ατbased on the current node embedding hv and thetype embedding hτ = (cid:80)v(cid:48) ˜avv(cid:48)hv(cid:48) (the weightedsum of the neighboring node embeddings hv(cid:48) withthe type τ , where the weight matrix ˜a = d− 12 (a+i)d− 12 is the normalized adjacency matrix withadded self-connections) as follows:.
ατ = softmaxτ (σ(µt.
τ · [hv, hτ ])),.
(3).
where µτ is the attention vector for the type τ .
soft-max function is applied to normalize across all thetypes..after l-layer heterogeneous graph convolution,we can ﬁnally get all the node (including sentencesand entities) representations aggregating neighbor-hood semantics.
we use max pooling over therepresentations of the sentence nodes hs ∈ rnto obtain the ﬁnal topic-enriched news documentembedding hd ∈ rn .
the learned entity represen-tations that encode the contextual semantics of thedocument are taken as contextual entity representa-tions ec ∈ rn ..3.3 entity comparison network.
in this subsection, we detail our entity comparisonnetwork which compares the learned contextualentity embeddings ec to the corresponding kb-based entity embeddings ekb.
we believe entitycomparison features could improve fake news de-tection based on the assumption that ec learnedfrom trusted news document can be better alignedwith the corresponding ekb; while inverse for fakenews..3.3.1 kb-based entity representationwe ﬁrst illustrate how to take full advantage of bothstructured subject-predicate-object triplets and un-structured textual entity descriptions in the kb(i.e., wikipedia) to learn kb-based entity represen-tations ekb..structural embedding.
a wide range of knowl-edge graph embedding methods can be appliedto obtain structured entity embeddings.
due tothe simplicity of transe (bordes et al., 2013),we adopted transe to learn entity representationses ∈ rm from the triplets.
formally, given a triplet(h, r, t), transe regards a relationship r as a trans-lation vector r from the head entity h to the tailentity t, namely h + r = t..textual embedding.
for each entity, we takethe ﬁrst paragraph of the corresponding wikipediapage as its text description.
then we apply lstm(hochreiter and schmidhuber, 1997) to learn entityrepresentations ed ∈ rm that encode the entitydescriptions..gating integration.
since both the structuraltriplets and textual description provide valuableinformation for an entity, we integrate these infor-mation into a joint representation.
particularly, aswe have the structural embedding es and textualembedding ed, we adopt a learnable gating func-tion to integrate entity embeddings from the twosources.
formally,.
ekb = ge (cid:12) es + (1 − ge) (cid:12) ed,.
(4).
where ge ∈ rm is a gating vector (w.r.t.
the entitye) to trade-off information from the two sourcesand its elements are in [0, 1].
(cid:12) denotes element-wise multiplication.
the gating vector ge meansthat each dimension of es and ed are summed bydifferent weights.
to constrain the value of eachelement in [0, 1], we compute the gate ge with thesigmoid function:.
ge = σ(˜ge),.
(5).
758where ˜ge ∈ rm is a real-value vector and islearned in the training process..after fusing the two types of embeddings withthe gating function, we obtain the ﬁnal kb-basedentity embeddings ekb ∈ rm which encode bothstructural information from the triplets and textualinformation from the entity descriptions in the kb..3.3.2 entity comparisonwe then perform entity-to-entity comparison be-tween the news document and the kb, to capturethe semantic consistency between the news con-tent and the kb.
we calculate a comparison vectorai between each contextual entity representationec ∈ rn and its corresponding kb-based entityembedding ekb ∈ rm ..ai = fcmp(ec, we · ekb) ,.
(6).
where fcmp() denotes the comparison function, andwe ∈ rn ×m is a transformation matrix.
to mea-sure the embedding closeness and relevance (shenet al., 2018), we design our comparison functionas:.
fcmp(x, y) = wa[x − y, x (cid:12) y],(7)where wa ∈ rn ×2n is a transformation matrixand (cid:12) is hadamard product, i.e., element-wise prod-uct.
the ﬁnal output comparison feature vectorc ∈ rn is obtained by the max pooling over thealignment vectors a = [a1, a2, ..., an] of all theentities e = {e1, e2, ..., en} in the news document..3.4 model trainingafter obtaining the comparison vector c ∈ rnand the ﬁnal news document representation vectorhd ∈ rn , we concatenate and feed them into asoftmax layer for fake news classiﬁcation.
for-mally,.
z = softmax(wo[hd, c] + bo),.
(8).
where wo and bo are the parameter matrix andvection of a linear transformation.
during modeltraining, we exploit the cross-entropy loss over thetraining data with the l2-norm of the parameters:.
l = −.
yij · log zij + η (cid:107)θ(cid:107)2,.
(9).
(cid:88).
(cid:88).
i∈dtrain.
j=1.
where dtrain is the set of news documents for train-ing, y is the corresponding label indicator matrix,θ is the model parameters, and η is regularizationfactor.
for model optimization, we adopt the gradi-ent descent algorithm..4 experiments.
we conduct extensive experiments across varioussettings and datasets.
following the previous work(vaibhav et al., 2019), we use sln: satirical andlegitimate news database (rubin et al., 2016), andlun: labeled unreliable news dataset (rashkinet al., 2017) for our experiments.
table 1 showsthe statistics..our baseline models include deep neural models:lstm (hochreiter and schmidhuber, 1997), cnn(kim, 2014), bert+lstm (vaibhav et al., 2019)(bert for sentence encoder and then lstm fordocument encoder) and bert (devlin et al., 2019)(directly for document encoder).
we also compareour model with graph neural models: gcn andgat based on an undirected fully-connected sen-tence graph, which use attention pooling or maxpooling for learning news document representa-tion.
for fair comparison with the previous work(vaibhav et al., 2019), we use lstm to encodesentences with randomly initialized word embed-dings, which is the same as all the graph neuralbaselines.
we run our model 5 times and reportthe micro-averaged (precision = recall = f1) andmacro-averaged scores (precision, recall, f1) inall the settings including 2-way and 4-way classiﬁ-cation..2-way classiﬁcation: we use the satirical andtrusted news articles from lun-train for training,lun-test for validation and evaluate our model onthe entire sln dataset.
this is done to emulatea real-world scenario where we want to see theperformance of our model on an out-of-domaindataset..4-way classiﬁcation: we split the lun-train intoa 80:20 split to create our training and validationset.
we use the lun-test as our in-domain test set.
experimental setting.
in our experiments, weset the number of topics k = 100 in lda.
eachsentence is assigned to top p = 2 topics withthe largest probabilities.
the layer number of ourheterogeneous graph convolution is set as l = 1.these parameters are chosen according to the bestexperimental results on validation set.
the otherhyper-parameters are set as the same as the baseline(vaibhav et al., 2019) for fair comparison.
speciﬁ-cally, all the hidden dimensions used in our modelare set as m = 100. the node embedding dimen-sion n = 32. for gcn, gat and comparenet, weset the activation function as leakyrelu with slope0.2. for model training, we train the models for a.
759dataset.
trusted (#docs).
satire (#docs).
hoax (#docs).
propaganda (#docs).
lun-train gn except ‘apw’ and ‘wpb’ (9,995)lun-testsln.
gn only ‘apw’ and ‘wpb’ (750)the toronto star, the ny times (180).
the onion (14,047)the borowitz report, clickhole (750)the onion, the beaverton (180).
american news (6,942) activist report (17,870)the natural news (750)-.
dc gazette (750)-.
table 1: statistics of the datasets.
gn refers to gigaword news..maximum of 15 epochs and use adam optimizerwith learning rate 0.001. we set l2 normalizationfactor η as 1e-6..model.
4.1 overall results.
table 2 shows the results for the two-way clas-siﬁcation between satirical and trusted news ar-ticles.
we report only micro f1 since microprecision=recall=f1.
as we can see, our pro-posed model comparenet signiﬁcantly outper-forms all the state-of-the-art baselines in termsof all the metrics.
compared to the best baselinemodel, comparenet improves both micro f1 andmacro f1 by nearly 3%.
we can also ﬁnd thatthe graph neural network based models gcn andgat all perform better than the deep neural mod-els including cnn, lstm and bert.
the reasonis that the deep neural models fail to consider theinteractions between sentences, which is importantfor fake news detection since different interactionpatterns are observed in trusted and fake news doc-uments (vaibhav et al., 2019).
our model com-parenet further improves fake news detection byeffectively exploiting the topics as well as the exter-nal kb.
the topics enrich the news representation,and the external kb offers evidences for fake newsdetection..we also present the results of four-way classi-ﬁcation in table 3. consistently, all graph neuralmodels capturing sentence interactions outperformthe deep neural models.
our model comparenetachieves the best performance in terms of all met-rics.
we believe that our model comparenet bene-ﬁts from the topics and external knowledge..4.2 ablation study.
in this subsection, we conduct experiments to studythe effectiveness of each module in comparenetand the way we incorporate external knowledge.
we study the average performance of 5 runs on thelun-test set.
as shown in table 4, we test theperformance of comparenet removing structuredtriplets, removing the entire external knowledge,removing topics, and removing both topics and ex-ternal knowledge.
in the last two rows, we further.
micro.
macro.
f1.
prec recall.
f1.
cnnlstmbert+lstmbert(rubin et al., 2016).
gcn + maxgcn + attngat + maxgat + attn (2019).
67.5081.1175.8384.16-.
85.8385.2786.3984.72.
67.7982.1276.6284.7388.00.
86.1685.5986.4485.65.
67.5081.1175.8384.1682.00.
85.8385.2786.3884.72.
67.3780.9675.6584.10-.
85.8085.2486.3884.62.comparenet.
89.17.
89.82.
89.17.
89.12.table 2: 2-way classiﬁcation results on sln dataset..model.
micro.
macro.
f1.
prec recall.
f1.
cnnlstmbert+lstmbert(rashkin et al., 2017).
gcn + maxgcn + attngat + maxgat + attn (2019).
54.0355.0655.5664.66-.
65.0067.0865.5066.95.
54.5058.8857.4560.89-.
66.7568.6069.4568.05.
54.0355.0654.8664.46-.
64.8467.0065.3366.86.
52.6052.5054.0058.8065.00.
63.7966.4263.8366.37.comparenet.
69.05.
72.94.
69.04.
68.26.table 3: 4-way classiﬁcation results on lun dataset..examine the constructed directed heterogeneousdocument graph and the designed entity compari-son function.
the variant comparenet (undirected)does not consider the edge directions of the directedheterogeneous document graph.
the variant modelcomparenet (concatenation) replaces the entitycomparison function as the simple concatenationoperation.
as we can see from table 4, removingstructural entity knowledge (i.e., w/o structuredtriplets) leads to slight performance drop.
if we re-move the entire external knowledge (i.e., w/o entitycmp), the performance decreases by around 1.3%and 1.8% on micro f1 and macro f1, respectively.
removing topics (i.e., w/o topics) will comparablyimpair the performance, which shows that the topic.
760variants.
micro.
macro.
f1.
prec recall.
f1.
comparenet- w/o structured triplets- w/o entity cmp- w/o topics- w/o bothcomparenet (undirected)comparenet (concatenation).
69.0568.7467.4667.4065.0066.3567.40.
72.9469.3470.3869.7566.7568.1170.05.
69.0468.7967.4367.4164.8466.3667.39.
68.2668.1766.3566.7363.7965.7466.25.table 4: ablation study of modules..figure 3: effect of top assigned topic number p ..information is as important as the external knowl-edge.
removing both topics and external knowl-edge (i.e., w/o both) will lead to substantial perfor-mance drop (4.0-5.0%).
it demonstrates the impor-tance of both topics and external knowledge.
thevariant model comparenet (undirected) althoughincorporating both topics and external knowledgeachieves lower performance than comparenet w/oentity cmp and comparenet w/o topics.
the rea-son could be that comparenet (undirected) directlyaggregates the true entity knowledge into the newsrepresentation in graph convolution without consid-ering the directed edges, which misleads the classi-ﬁer for differentiating fake news.
this veriﬁes theappropriateness of our constructed directed hetero-geneous document graph.
the last variant com-parenet (concatenation) also performs lower thancomparenet w/o entity cmp, further indicatingthat directly concatenating true entity knowledge isnot a good way for incorporating entity knowledge.
its performance drops by around 2.0% comparedto comparenet.
these demonstrate the effective-ness of the carefully designed entity comparisonnetwork in comparenet..4.3 analysis of top assigned topic number.
figure 3 shows the performance (micro and macrof1) of our model comparenet on lun validationset with different number of top assigned topicsp to each sentence.
as we can see clearly, microf1 and macro f1 ﬁrst consistently rises with theincrease of p and then drops when p is larger than.
figure 4: two news examples from the lun-test set..2. this may because that connecting too many low-probability topics will introduce some noise.
thus,in our experiments, we set p = 2..4.4 case study.
to further illustrate why our model outperformsstate-of-the-art baseline gat+attn (vaibhav et al.,2019), we present two real news examples fromthe lun-test set.
the baseline model gat+attnand the variant model comparenet w/o entitycmp mistakenly predict these two examples astrusted news, while our model comparenet cansuccessfully predict both of them.
as we cansee from figure 4, the content of the news doc-ument is in conﬂict with the entity description fromwikipedia.
speciﬁcally, the news about “fda tar-get and threaten the natural health community” de-livers contrary meaning from the entity descriptionthat “fda is responsible for protecting and promot-ing public health” 4. similarly, the news documentabout “mammograms are not effective at detect-ing breast tumors” conveys different meaning fromthe entity description of “mammograms”.
we be-lieve that our model comparenet beneﬁts from thecomparison to wikipedia knowledge by the entitycomparison network.
we ﬁnd there are also unsuc-cessful cases since an entity could be mistakenlylinked to a wrong entity in the wikipedia..5 conclusion.
in this paper, we propose a novel end-to-end graphneural model comparenet which compares thenews to the external knowledge for fake news de-tection.
considering that the detection of fake newsis correlated with topics, in our model, we also usetopics to enrich the news document representationfor improving fake news detection.
particularly,we ﬁrst construct a directed heterogeneous docu-ment graph for each news document capturing theinteractions among sentences, topics and entities..4https://en.wikipedia.org/wiki/food and drug.
administration.
76112345656667686970micro f1macro f1newsentitydescriptionthat may easily be misused bythefda to target and threaten the natural health community…the fda could have illegitimately used it to target practically any company it wanted to.…the fda is responsible for protecting and promoting public health through the control and supervision of food safety, tobacco products, dietary supplements…mammography is the process of using low-energy x-rays to examine the human breast for diagnosis and screening.
the goal of mammography is the early detection of breast cancer……women referred to oncologists for treatment after mammograms did not actually have cancer.
…mammograms are not effective at detectingbreast tumors…based on the graph, we develop a heterogeneousgraph attention network for learning topic-enrichednews representation as well as contextual entityrepresentations that encode the semantics of thecontent of the news document.
to capture the se-mantic consistency of the news content and the kb,the learned contextual entity representations arethen compared to the kb-based entity representa-tions, with a carefully designed entity comparisonnetwork.
finally, the obtained entity comparisonfeatures are combined with the news representationfor an improved fake news classiﬁer.
experimentson two benchmark datasets have demonstrated theeffectiveness of the way we incorporate the externalknowledge and topics..in future work, we will explore a better wayto combine multi-modal data (e.g., images) andexternal knowledge for fake news detection..acknowledgments.
the work is supported by the national natu-ral science fundation of china (no.
61806020,u1936220, 61972047, 62076245) and the mi-crosoft research asia’s star track project..references.
hunt allcott and matthew gentzkow.
2017. social me-dia and fake news in the 2016 election.
journal ofeconomic perspectives, 31(2):211–236..david m blei, andrew y ng, and michael i jordan.
2003. latent dirichlet allocation.
journal of ma-chine learning research, 3(jan):993–1022..antoine bordes, nicolas usunier, alberto garcia-jason weston, and oksana yakhnenko.
duran,2013. translating embeddings for modeling multi-relational data.
in advances in neural informationprocessing systems 26, pages 2787–2795..nadia k conroy, victoria l rubin, and yimin chen.
2015. automatic deception detection: methods forﬁnding fake news.
proceedings of the associationfor information science and technology, 52(1):1–4..jacob devlin, ming-wei chang, kenton lee, andkristina toutanova.
2019. bert: pre-training ofdeep bidirectional transformers for language under-standing.
in proceedings of the 2019 conference ofthe north american chapter of the association forcomputational linguistics: human language tech-nologies, pages 4171–4186..manish gupta, peixiang zhao, and jiawei han.
2012.evaluating event credibility on twitter.
in proceed-ings of the twelfth siam international conferenceon data mining, pages 153–164..sepp hochreiter and j¨urgen schmidhuber.
1997.neural computation,.
long short-term memory.
9(8):1735–1780..linmei hu, tianchi yang, chuan shi, houye ji, andxiaoli li.
2019. heterogeneous graph attention net-works for semi-supervised short text classiﬁcation.
in proceedings of the 2019 conference on empiri-cal methods in natural language processing andthe 9th international joint conference on naturallanguage processing, pages 4821–4830..zhiwei jin, juan cao, yongdong zhang, and jiebo luo.
2016. news veriﬁcation by exploiting conﬂicting so-cial viewpoints in microblogs.
in proceedings of thethirtieth aaai conference on artiﬁcial intelligence,pages 2972–2978..dhruv khattar, jaipal singh goud, manish gupta, andvasudeva varma.
2019. mvae: multimodal varia-tional autoencoder for fake news detection.
in theworld wide web conference, page 2915–2921..urja khurana and bachelor opleiding kunstmatige in-telligentie.
2017. the linguistic features of fakenews headlines and statements.
ph.d. thesis, mas-ter’s thesis, university of amsterdam..yoon kim.
2014. convolutional neural networks forsentence classiﬁcation.
in proceedings of the 2014conference on empirical methods in natural lan-guage processing, pages 1746–1751..ray oshikawa, jing qian, and william yang wang.
2020. a survey on natural language processing forfake news detection.
arxiv, abs/1811.00770..jeff z. pan, siyana pavlova, chenxi li, ningxi li,yangmei li, and jinshuo liu.
2018. content basedfake news detection using knowledge graphs.
in thesemantic web - iswc 2018 - 17th international se-mantic web conference, volume 11136, pages 669–683..martin potthast, johannes kiesel, kevin reinartz,janek bevendorff, and benno stein.
2018. a stylo-metric inquiry into hyperpartisan and fake news.
inproceedings of the 56th annual meeting of the as-sociation for computational linguistics, pages 231–240..hannah rashkin, eunsol choi, jin yea jang, svitlanavolkova, and yejin choi.
2017. truth of varyingshades: analyzing language in fake news and polit-ical fact-checking.
in proceedings of the 2017 con-ference on empirical methods in natural languageprocessing, pages 2931–2937..´alvaro ibrain rodr´ıguez and lara lloret iglesias.
2019.fake news detection using deep learning.
corr,abs/1910.03496..victoria rubin, niall conroy, yimin chen, and sarahcornwell.
2016. fake news or truth?
using satiricalcues to detect potentially misleading news.
in pro-ceedings of the second workshop on computationalapproaches to deception detection, pages 7–17..762processing and the 9th international joint confer-ence on natural language processing, pages 4643–4652..jiawei zhang, bowen dong, and philip s. yu.
2020.fakedetector: effective fake news detection within in proceedingsdeep diffusive neural network.
of the 36th ieee international conference on dataengineering, pages 1826–1829..wanjun zhong, jingjing xu, duyu tang, zenan xu,nan duan, ming zhou, jiahai wang, and jian yin.
2020. reasoning over semantic-level graph for factchecking.
in proceedings of the 58th annual meet-ing of the association for computational linguistics,pages 6170–6180..jie zhou, xu han, cheng yang, zhiyuan liu, lifengwang, changcheng li, and maosong sun.
2019.gear: graph-based evidence aggregating and rea-in proceedings of thesoning for fact veriﬁcation.
57th annual meeting of the association for compu-tational linguistics, pages 892–901..xinyi zhou and reza zafarani.
2020. a survey of fakenews: fundamental theories, detection methods, andopportunities.
acm computing surveys (csur),53(5):1–40..dinghan shen, xinyuan zhang, ricardo henao, andimproved semantic-awarelawrence carin.
2018.network embedding with ﬁne-grained word align-in proceedings of the 2018 conference onment.
empirical methods in natural language processing,pages 1829–1838..kai shu, deepak mahudeswaran, suhang wang, dong-won lee, and huan liu.
2020. fakenewsnet: a datarepository with news content, social context, andspatiotemporal information for studying fake newson social media.
big data, 8(3):171–188..kai shu, suhang wang, and huan liu.
2019. beyondnews contents: the role of social context for fakenews detection.
in proceedings of the twelfth acminternational conference on web search and datamining, pages 312–320..niraj sitaula, chilukuri k. mohan, jennifer grygiel,xinyi zhou, and reza zafarani.
2019. credibility-based fake news detection.
corr, abs/1911.00643..eugenio tacchini, gabriele ballarin, marco l. dellavedova, stefano moret, and luca de alfaro.
2017.some like it hoax: automated fake news detectionin social networks.
corr, abs/1704.07506..james.
andreas vlachos,.
and arpit mittal..christosthorne,2018.christodoulopoulos,fever: a large-scale dataset for fact extractionin proceedings of the 2018and veriﬁcation.
conference of the north american chapter of theassociation for computational linguistics: humanlanguage technologies, pages 809–819..vaibhav vaibhav, raghuram mandyam, and eduardhovy.
2019. do sentence interactions matter?
lever-aging sentence level representations for fake newsin proceedings of the thirteenthclassiﬁcation.
workshop on graph-based methods for naturallanguage processing (textgraphs-13), pages 134–139..soroush vosoughi, deb roy, and sinan aral.
2018.the spread of true and false news online.
science,359(6380):1146–1151..william yang wang.
2017.
“liar, liar pants on ﬁre”:a new benchmark dataset for fake news detection.
in proceedings of the 55th annual meeting of theassociation for computational linguistics (volume2: short papers), pages 422–426..youze wang, shengsheng qian, jun hu, quan fang,and changsheng xu.
2020. fake news detection viaknowledge-driven multimodal graph convolutionalnetworks.
in proceedings of the 2020 internationalconference on multimedia retrieval, pages 540–547..lianwei wu, yuan rao, haolin jin, ambreen nazir,and ling sun.
2019. different absorption from thesame sharing: sifted multi-task learning for fakein proceedings of the 2019 con-news detection.
ference on empirical methods in natural language.
763