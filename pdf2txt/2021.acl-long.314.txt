supporting cognitive and emotional empathic writing of studentsthiemo wambsganss1,2, christina niklaus1, 3, matthias s¨ollner4,siegfried handschuh1, 3 and jan marco leimeister1, 41 university of st.gallen{thiemo.wambsganss, christina.niklaus,siegfried.handschuh, janmarco.leimeister}@unisg.ch2 carnegie mellon universitytwambsga@andrew.cmu.edu3 university of passau{christina.niklaus, siegfried.handschuh}@uni-passau.de4 university of kassel{soellner, leimeister}@uni-kassel.de.
abstract.
we present an annotation approach to cap-turing emotional and cognitive empathy instudent-written peer reviews on business mod-els in german.
we propose an annotationscheme that allows us to model emotional andcognitive empathy scores based on three typesof review components.
also, we conducted anannotation study with three annotators basedon 92 student essays to evaluate our annota-tion scheme.
the obtained inter-rater agree-ment of α=0.79 for the components and themulti-π=0.41 for the empathy scores indicatethat the proposed annotation scheme success-fully guides annotators to a substantial to mod-erate agreement.
moreover, we trained predic-tive models to detect the annotated empathystructures and embedded them in an adaptivewriting support system for students to receiveindividual empathy feedback independent ofan instructor, time, and location.
we evalu-ated our tool in a peer learning exercise with58 students and found promising results forperceived empathy skill learning, perceivedfeedback accuracy, and intention to use.
fi-nally, we present our freely available corpus of500 empathy-annotated, student-written peerreviews on business models and our annotationguidelines to encourage future research on thedesign and development of empathy supportsystems..1.introduction.
empathy is an elementary skill in society for dailyinteraction and professional communication andis therefore elementary for educational curricula(e.g., learning framework 2030 (oecd, 2018)).
it is the “ability to simply understand the otherperson’s perspective [.
.
.]
and to react to the ob-.
figure 1: empathy annotation scheme.
first, a textparagraph is classiﬁed into a peer review component(strengths, weakness, improvement suggestions).
sec-ond, the same annotator is then scoring the cognitiveand emotional empathy level of the components basedon our annotation guideline on a 1-to-5 scale..served experiences of another,” (davis, 1983, p.1)1.empathy skills not only pave the foundation forsuccessful interactions in digital companies, e.g.,in agile work environments (luca and tarricone,2001), but they are also one of the key abilities inthe future that will distinguish the human work-force and artiﬁcial intelligence agents from oneanother (poser and bittner, 2020).
however, be-sides the growing importance of empathy, researchhas shown that empathy skills of us college stu-dents decreased from 1979 to 2009 by more thanthirty percent and even more rapidly between 2000to 2009 (konrath et al., 2011).
on these grounds,the organization for economic cooperation anddevelopment (oecd) claims that the training forempathy skills should receive a more prominentrole in today’s higher education (oecd, 2018)..1being aware that empathy is a multidimensional construct,in this study, we focus on emotional and cognitive empathy(spreng et al., 2009; davis, 1983)..proceedingsofthe59thannualmeetingoftheassociationforcomputationallinguisticsandthe11thinternationaljointconferenceonnaturallanguageprocessing,pages4063–4077august1–6,2021.©2021associationforcomputationallinguistics4063to train students with regard to empathy, educa-tional institutions traditionally rely on experientiallearning scenarios, such as shadowing, commu-nication skills training, or role playing (lok andfoster, 2019; van berkhout and malouff, 2016).
individual empathy training is only available for alimited number of students since individual feed-back through a student’s learning journey is oftenhindered due to large-scale lectures or the growingﬁeld of distance learning scenarios such as mas-sive open online classes (moocs) (seaman et al.,2018; hattie and timperley, 2007)..one possible path for providing individual learn-ing conditions is to leverage recent developments incomputational linguistics.
language-based modelsenable the development of writing support systemsthat provide tailored feedback and recommenda-tions (santos et al., 2018), e.g., like those alreadyused for argumentation skill learning (wambsgansset al., 2020a, 2021b).
recently, studies have startedinvestigating elaborated models of human emotions(e.g., wang et al.
(2016), abdul-mageed and un-gar (2017), buechel and hahn (2018), or sharmaet al.
(2020)), but available corpora for empathydetection are still rare.
only a few studies addressthe detection and prediction of empathy in naturaltexts (khanpour et al., 2017; xiao et al., 2012), and,to the best of our knowledge, only one corpus ispublicly available for empathy modelling based onnews story reactions (buechel et al., 2018).
pastliterature therefore lacks 1) publicly available em-pathy annotated data sets, 2) empathy annotationmodels based on rigorous annotation guidelinescombined with annotation studies to assess thequality of the data, 3) the alignment of empathyin literature on psychological constructs and theo-ries, and 4) an embedding and real-world evalua-tion of novel modelling approaches in collaborativelearning scenarios (ros´e et al., 2008)..we introduce an empathy annotation schemeand a corpus of 500 student-written reviews thatare annotated for the three types of review compo-nents, strengths, weaknesses, and suggestions forimprovements, and their embedded emotional andcognitive empathy level based on psychological the-ory (davis, 1983; spreng et al., 2009).
we traineddifferent models and embedded them as feedbackalgorithms in a novel writing support tool, whichprovided students with individual empathy feed-back and recommendations in peer learning scenar-ios.
the measured empathy skill learning (spreng.
et al., 2009), the perceived feedback accuracy (pod-sakoff and farh, 1989), and the intention to use(venkatesh and bala, 2008) in a controlled evalu-ation with 58 students provided promising resultsfor using our approach in different peer learningscenarios to offer quality education independent ofan instructor, time, and location..our contribution is fourfold: 1) we derive anovel annotation scheme for empathy modelingbased on psychological theory and previous workon empathy annotation (buechel et al., 2018); 2)we present an annotation study based on 92 studentpeer reviews and three annotators to show that theannotation of empathy in student peer reviews is re-liably possible; 3) to the best of our knowledge, wepresent the second freely available corpus for em-pathy detection in general and the ﬁrst corpus forempathy detection in the educational domain basedon 500 student peer reviews collected in our lectureabout business innovation in german; 4) we embed-ded our annotation approach as predictive modelsin a writing support system and evaluated it with 58students in a controlled peer learning scenario.
wehope to encourage research on student-written em-pathetic texts and writing support systems to trainstudents’ empathy skills based on nlp towards aquality education independent of a student’s loca-tion or instructors..2 background.
the construct of empathy the ability to per-ceive the feelings of another person and react totheir emotions in the right way requires empathy– the ability “of one individual to react to the ob-served experiences of another” (davis (1983), p.1).
empathy plays an essential role in daily life inmany practical situations, such as client communi-cation, leadership, or agile teamwork.
despite theinterdisciplinary research interest, the term empa-thy is deﬁned from multiple perspectives in termsof its dimensions or components (decety and jack-son, 2004).
aware of the multiple perspectives onempathy, in this annotation study, we focused onthe cognitive and emotional components of em-pathy as deﬁned by davis (1983) and lawrenceet al.
(2004).
therefore, we follow the ‘torontoempathy scale’ (spreng et al., 2009) as a synthesisof instruments for measuring and validating empa-thy.
hence, empathy consists of both emotionaland cognitive components (spreng et al., 2009).
while emotional empathy lets us perceive what.
4064other people feel, cognitive empathy is the humanability to recognize and understand other individu-als (lawrence et al., 2004)..2018)..emotion and empathy detection in nlp, thedetection of empathy in texts is usually regardedas a subset of emotion detection, which in turnis often referred to as part of sentiment analysis.
the detection of emotions in texts has made ma-jor progress, with sentiment analysis being oneof the most prominent areas in recent years (liu,2015).
however, most scientiﬁc studies have beenfocusing on the prediction of the polarity of wordsfor assessing negative and positive notions (e.g.,in online forums (abbasi et al., 2008) or twitterpostings (rosenthal et al., 2018)).
moreover, re-searchers have also started investigating more elab-orated models of human emotions (e.g., wang et al.
(2016), abdul-mageed and ungar (2017), and mo-hammad and bravo-marquez (2017)).
several cor-pora exist where researchers have annotated andassessed the emotional level of texts.
for example,scherer and wallbott (1994) published an emotion-labelled corpus based on seven different emotionalstates.
strapparava and mihalcea (2007) classiﬁednews headlines based on the basic emotions scaleof ekman (1992) (i.e., anger, disgust, fear, happi-ness, sadness and surprise).
more recently, chenet al.
(2018) published emotionlines, an emotioncorpus of multi-party conversations, as the ﬁrstdata set with emotion labels for all utterances wasonly based on their textual content.
bostan andklinger (2018) presented a novel uniﬁed domain-independent corpus based on eleven emotions asthe common label set.
however, besides the multi-ple corpora available for emotion detection in texts,corpora for empathy detection are rather rare.
asbuechel et al.
(2018) also outline, the construc-tion of corpora for empathy detection and empathymodelling might be less investigated due to vari-ous psychological perspectives on the construct ofempathy.
most of the works for empathy detectionfocus, therefore, on spoken dialogue, addressingconversational agents, psychological interventions,or call center applications (e.g., mcquiggan andlester (2007), p´erez-rosas et al.
(2017), alam et al.
(2018), sharma et al.
(2020)) rather than writtentexts.
consequently, there are hardly any corporaavailable in different domains and languages thatenable researchers in training models to detect theempathy level in texts, e.g., by providing studentswith individual empathy feedback (buechel et al.,.
empathy annotated corpora and annotationschemes only a few studies address the detec-tion and prediction of empathy in natural languagetexts (e.g., khanpour et al.
(2017) and xiao et al.
(2012)).
presenting the ﬁrst and only available goldstandard data set for empathy detection, buechelet al.
(2018) constructed a corpus in which crowd-workers were asked to write emphatic reactions tonews stories.
before the writing tasks, the crowd-workers were asked to conduct a short survey withself-reported items to measure their empathy leveland their personal distress based on batson et al.
(1987).
the scores from the survey were then takenas the annotation score for the overall news reac-tion message.
the ﬁnal corpus consisted of 1,860annotated messages (buechel et al., 2018).
never-theless, previous empathy annotations on naturaltexts merely focused on intuition-based labels in-stead of rigorous annotation guidelines combinedwith annotation studies by researchers to assessthe quality of the corpora (i.e., as is done for cor-pora of other writing support tasks, e.g., argumen-tative student essays by stab and gurevych (2017)).
moreover, previous annotations have mostly beenconducted at the overall document level, resultingin one generic score for the whole document, whichmakes the corpus harder to apply to writing supportsystems..consequently, there is a lack of linguistic cor-pora for empathy detection in general and, morespeciﬁcally, for training models that provide stu-dents with adaptive support and feedback abouttheir empathy in common pedagogical scenarioslike large-scale lectures or the growing ﬁeld ofmoocs (wambsganss et al., 2021c, 2020b).
infact, in the literature about computer-supported col-laborative learning (dillenbourg et al., 2009), wefound only one approach by santos et al.
(2018)that used a dictionary-based approach to providestudents with feedback on the empathy level oftheir texts.
we aim to address this literature gapby presenting and evaluating an annotation schemeand an annotated empathy corpus built on student-written texts with the objective to develop intelli-gent and accurate empathy writing support systemsfor students..3 corpus construction.
we compiled a corpus of 500 student-generatedpeer reviews in which students provided each other.
4065with feedback on previously developed businessmodels.
peer reviews are a modern learning sce-nario in large-scale lectures, enabling students toreﬂect on their content, receive individual feedbackfrom peers, and thus deepen their understandingof the content (rietsche and s¨ollner, 2019).
more-over, they are easy to set up in traditional large-scale learning scenarios or the growing ﬁeld ofdistance-learning scenarios such as moocs.
thiscan be leveraged to train skills such as the abilityto appropriately react to other students’ perspec-tives (e.g., santos et al.
(2018)).
therefore, we aimto create an annotated corpus to provide empathyfeedback based on a data set that a) is based onreal-world student peer reviews, b) consists of asufﬁcient corpus size to be able to train models in areal-world scenario and c) follows a novel annota-tion guideline for guiding the annotators towards anadequate agreement.
hence, we propose a new an-notation scheme to model peer review componentsand their emotional and cognitive empathy levelsthat reﬂect the feedback discourse in peer reviewtexts.
we base our empathy annotation scheme onemotional and cognitive empathy following davis(1983) and spreng et al.
(2009) guided by the studyof buechel et al.
(2018).
to build a reliable cor-pus, we followed a 4-step methodology: 1) weexamined scientiﬁc literature and theory on theconstruct of empathy and on how to model empa-thy structures in texts from different domains; 2)we randomly sampled 92 student-generated peerreviews and, on the basis of our ﬁndings from lit-erature and theory, developed a set of annotationguidelines consisting of rules and limitations onhow to annotate emphatic review discourse struc-tures; 3) we applied, evaluated, and improved ourguidelines with three native speakers of german ina total of eight consecutive workshops to resolveannotation ambiguities; 4) we followed the ﬁnal an-notation scheme based on our 14-page guidelinesto annotate a corpus of 500 student-generated peerreviews.2.
3.1 data source.
we gathered a corpus of 500 student-generated peerreviews written in german.
the data was collectedin a business innovation lecture in a master’s pro-gram at a western european university.
in thislecture, around 200 students develop and present a.
2the annotation guidelines as well as the entire corpuscan be accessed at https://github.com/thiemowa/empathy_annotated_peer_reviews..new business model for which they receive threepeer reviews each.
here, a fellow student fromthe same course elaborates on the strengths andweaknesses of the business model and gives rec-ommendations on what could be improved.
wecollected a random subset of 500 of these reviewsfrom around 7,000 documents collected from theyears 2014 to 2018 in line with the ethical guide-lines of our university and with approval from thestudents to utilize the writings for scientiﬁc pur-poses.
an average peer review consists of 200 to300 tokens (in our corpus we counted a mean of 19sentences and 254 tokens per document).
a peerreview example is displayed in figure 2..3.2 annotation scheme.
our objective is to model the empathy structures ofstudent-generated peer reviews by annotating thereview components and their emotional and cog-nitive empathy levels.
most of the peer reviewsin our corpus followed a similar structure.
theydescribed several strengths or weaknesses of thebusiness model under consideration, backing themup by examples or further elaboration.
moreover,the students formulated certain suggestions for im-provements of the business model.
these reviewcomponents (i.e., strengths, weaknesses, and sug-gestions for improvement) were written with differ-ent empathetic levels, sometimes directly criticiz-ing the content harshly, sometimes empatheticallyreferring to weaknesses as further potentials forimprovement with examples and explanation.
weaim to capture these empathic differences betweenthe peer reviews with two empathy level scores, thecognitive empathy level of a certain review compo-nent and the emotional empathy level of a certaincomponent.
our basic annotation scheme is illus-trated in figure 1..3.2.1 review components.
for the review components, we follow establishedmodels of feedback structures suggested by feed-back theory (e.g., hattie and timperley (2007) orblack and wiliam (2009)).
a typical peer review,therefore, consists of three parts: 1) elaborationof strengths, 2) elaboration of weaknesses, and 3)suggestions for improvements (to answer “wheream i going and how am i going?” and “wheredo i go next?”, i.e., hattie and timperley (2007)).
accordingly, the content of a review consists ofmultiple components, including several controver-sial statements (e.g., a claim about a strength or.
4066weakness of a business model) that are usually sup-ported by elaborations or examples (i.e., a premise)(toulmin, 1984).
also, in the domain of student-written peer reviews, we found that a standpointand its elaboration are the central element of a re-view component.
accordingly, we summarizedall the claims and premises which described posi-tive aspects of a business model as strengths.
allcontent (claims and premises) describing negativeaspects were modelled as weaknesses, while claimsand premises with certain content for improvementwere modelled as suggestions for improvement, fol-lowing the structure of a typical review.
besidesthe content, syntactical elements and key wordswere used as characteristics for the compound clas-siﬁcation, e.g., most students introduced a reviewcomponent by starting with structural indicationssuch as ”strengths:” or ”weaknesses:” in theirpeer review texts..3.2.2 empathy level.
to capture the differences in the empathy levelsof the peer reviews (i.e., the way the writer wasconveying their feedback (hattie and timperley,2007)), we followed the approach of davis (1983)and spreng et al.
(2009) for cognitive and emo-tional empathy.
cognitive empathy (perspectivetaking) is the writer’s ability to use cognitive pro-cesses, such as role taking, perspective taking, or“decentering,” while evaluating the peers’ submit-ted tasks.
the student sets aside their own perspec-tive and “steps into the shoes of the other.” cogni-tive empathy can happen purely cognitively, in thatthere is no reference to any affective state, (baron-cohen and wheelwright, 2004) but it mostly in-cludes understanding the other’s emotional stateas well.
the following example displays high cog-nitive empathy: “you could then say, for exam-ple, ‘since market services are not differentiatedaccording to customer segments and locations, thefollowing business areas result... and that due tothe given scope of this task you will focus on theconcierge-service business segment.’ after that,you have correctly only dealt with this business seg-ment.” emotional empathy (emphatic concern) isthe writer’s emotional response to the peers’ affec-tive state.
the students can either show the sameemotions as read in the review or simply state anappropriate feeling towards the peer.
typical exam-ples include sharing excitement with the peer aboutthe business model submitted or showing concernover the peer’s opinion.
the following example de-.
picts high emotional empathy: “i think your ideais brilliant!”..both constructs are measured on a scale from1-5 following the empathy scale range of moyersand martin (2010), with every level being preciselydeﬁned in our annotation guidelines.
a summaryof the deﬁnitions for both empathy level scoresare displayed in table 1 and table 2. a more de-tailed description of both scores can be found inthe appendix in table 7 and table 8.3.figure 2 illustrates an example of an entire peerreview that is annotated for strength, weakness andsuggestion for improvement and the cognitive andemotional empathy scores.4.
figure 2: fully annotated example of a peer review..3.3 annotation process.
three native german speakers annotated the peerreviews independently from each other for the com-ponents strengths, weaknesses and suggestions forimprovement, as well as their cognitive and emo-tional empathy levels according to the annotationguidelines we speciﬁed.
the annotators were mas-ter’s students in business innovation from a euro-pean university with bachelor’s degrees in businessadministration and were, therefore, domain expertsin the ﬁeld of business models.
inspired by stab.
3more elaborated deﬁnitions, examples, and key wordlists for both empathy scales can be found in our annotationguidelines..4since the original texts are written in german, we trans-.
lated the examples to english for the sake of this paper..4067scoredescription5.the student fully understands the peer’s thoughts.
she completely stepped outside her own perspective and thinks fromthe peer’s perspective.
she does that by carefully evaluating the peer’s idea with rich explanations.
questions, personalpronouns, or direct addressing of the author could be used in order to better understand and elaborate on the peer’sperspective.
the student thinks from the perspective of the peer.
she elaborates in a way that serves the peer best to further establishthe idea or activity.
each component is afﬁrmed with further explanations.
the student tries to understand the perspective of the peer and adds further elaborations to her statements.
however, herelaborations are not completely thought through, and her feedback is missing some essential explanations, examples, orquestions to make sure she understood everything correctly.
the student did not try to understand the peer’s perspective.
the student rather just tried to accomplish the task of givingfeedback.
the student’s feedback is very short and does not include the peer’s perspective.
she does not add any further elaborationin her thoughts..table 1: description of the cognitive empathy scores..scoredescription5.the student was able to respond very emotionally to the peer’s work and fully represents the affectional state in herentire review.
she illustrates this by writing in a very emotional and personal manner and expressing her feelings(positive or negative) throughout the review.
strong expressions include exclamation marks (!).
the student was able to respond emotionally to the peer’s submitted activity with suitable emotions (positive or negative).
she returns emotions in her feedback on various locations and expresses her feelings by using the personal pronouns(“i”, “you”).
some sentences might include exclamations marks (!).
the student occasionally includes emotions or personal emotional statements in the peer review.
they could be quitestrong.
however, the student’s review is missing personal pronouns (“i”, “you”) and is mostly written in third person.
emotions can both be positive or negative.
negative emotions can be demonstrated with concern, missing understandingor insecurity (e.g., with modal verbs or words such as rather, perhaps).
mostly, the student does not respond emotionally to the peer’s work.
only very minor and weak emotions or personalemotional statements are integrated.
the student writes mostly objectively (e.g., “okay”, “this should be added”, “thetask was done correctly”, etc.).
in comparison to level 1, she might be using modal verbs (might, could, etc.)
or wordsto show insecurity in her feedback (rather, maybe, possibly).
the student does not respond emotionally to the peer’s work at all.
she does not show her feelings towards the peer andwrites objectively (e.g., no “i feel”, “personally” “i ﬁnd this...” and no emotions such as “good”, “great”, “fantastic”,“concerned”, etc.).
typical examples would be “add a picture.” or “the value gap xy is missing.”..4.
3.
2.
1.
4.
3.
2.
1.table 2: description of the emotional empathy scores..and gurevych (2017), our guidelines consisted of14 pages, including deﬁnitions and rules for howthe review components should be composed, whichannotation scheme was to be used, and how thecognitive and emotional empathy level were to bejudged.
several individual training sessions andeight team workshops were performed to resolvedisagreements among the annotators and to reacha common understanding of the annotation guide-lines on the cognitive and emotional empathy struc-tures.
we used the tagtog annotation tool,5 whichoffers an environment for cloud-based annotationin a team.
first, a text was classiﬁed into peerreview components (strengths, weaknesses, sug-gestions for improvement, or none) by the trainedannotators.
second, the same annotator then scoredthe cognitive and emotional empathy levels of eachcomponent based on our annotation guideline ona one to ﬁve scale.
after the ﬁrst 92 reviews were.
5https://tagtog.net/.
annotated by all three annotators, we calculatedthe inter-annotator agreement (iaa) scores (seesection 4.1).6 as we obtained satisfying results,we proceeded with two annotators annotating 130remaining documents each and the senior annota-tor annotating 148 peer reviews, resulting in 408additional annotated documents.
together with the92 annotations of the annotation study of the seniorannotator (the annotator with the most reviewingexperience), we counted 500 annotated documentsin our ﬁnal corpus..4 corpus analysis.
4.1.inter-annotator agreement.
to evaluate the reliability of the review componentsand empathy level annotations, we followed theapproach of stab and gurevych (2014)..6our intention was to capture the annotation of 100 ran-domly selected essays.
however, we discarded 8 of the 100essays as they contained less than 2 review components..4068review components concerning the reviewcomponents, two strategies were used.
since therewere no predeﬁned markables, annotators not onlyhad to identify the type of review component butalso its boundaries.
in order to assess the latter,we use krippendorff’s αu (krippendorff, 2004),which allows for an assessment of the reliability ofan annotated corpus, considering the differencesin the markable boundaries.
to evaluate the an-notators’ agreement in terms of the selected cate-gory of a review component for a given sentence,we calculated the percentage agreement and twochance-corrected measures, multi-π (fleiss, 1971)and krippendorff’s α (krippendorff, 1980).
sinceeach annotation always covered a full sentence (or asequence of sentences), we operated at the sentencelevel for calculating the reliability of the annota-tions in terms of the iaa..strengthweaknesssuggestionsnone.
%0.96410.88930.89480.9330.multi-π0.88710.74340.68750.8312.kripp.
α0.88710.74340.68750.8312.kripp.
αu0.51810.31090.35120.9032.table 3: iaa of review component annotations..table 3 displays the resulting iaa scores.
theobtained scores for krippendorff’s α indicated analmost perfect agreement for the strengths compo-nents and a substantial agreement for both the weak-nesses and the suggestions for improvement com-ponents.
the unitized α of strengths, weaknessesand suggestions for improvement annotations wasslightly smaller compared to the sentence-levelagreement.
thus, the boundaries of review com-ponents were less precisely identiﬁed in compar-ison to the classiﬁcation into review components.
yet the scores still suggest that there was a moder-ate level of agreement between the annotators forthe strengths and a fair agreement for the weak-nesses and the suggestions for improvement.
witha score of αu=90.32%, the boundaries of the non-annotated text units were more reliably detected,indicating an almost perfect agreement betweenthe annotators.
percentage agreement, multi-π, andkrippendorff’s α were considerably higher for thenon-annotated spans as compared to the strengths,weaknesses, and suggestions for improvement, in-dicating an almost perfect agreement between theannotators.
hence, we conclude that the annotationof the review components in student-written peerreviews is reliably possible ..empathy level to assess the reliability of thecognitive and emotional empathy level annotations,we calculated the multi-π for both scales.
for thecognitive empathy level, we received a multi-π of0.41 for both the emotional and cognitive empathylevel, suggesting a moderate agreement betweenthe annotators in both cases.
thus, we concludethat the empathy level can also be reliably anno-tated in student-generated peer reviews..to analyze the disagreement between the threeannotators, we created a confusion probability ma-trix (cpm) (cinkov´a et al., 2012) for the reviewcomponents and the empathy level scores.
theresults can be found in section c of the appendix..4.2 corpus statistics.
the corpus we compiled consists of 500 student-written peer reviews in german that were com-posed of 9,614 sentences with 126,887 tokens in to-tal.
hence, on average, each document had 19 sen-tences and 254 tokens.
a total of 2,107 strengths,3,505 weaknesses and 2,140 suggestions for im-provement were annotated..tables 4, 5, and 6 present some detailed statistics.
on the ﬁnal corpus..sentencestokens.
total9,614126,887.mean19.23253.77.std dev10.39134.18.min110.max851026.median17228.table 4: distribution of sentences and tokens in the cre-ated corpus.
mean, std dev, min, max and median referto the number of sentences and tokens per document..str.
weak.
sug..total2,1073,5052,140.mean4.217.014.28.std dev2.716.105.49.min100.max204159.median453.
%0.270.450.28.table 5: distribution of the review components..cognitive elemotional el.
mean2.943.22.std dev0.991.03.min11.max55.median33.table 6: distribution of the empathy level (el) scores..moreover, figure 3 displays the distribution ofthe empathy scores in the annotated dataset.
boththe cognitive and the emotional empathy levelsapproximately follow a normal distribution witha mean score of 2.94 and 3.22, respectively (seetable 6).
we measured only a low correlation of0.38 between the scores of cognitive and emotionalempathy..4069figure 3: distribution of the cognitive (left) and emotional (right) empathy scores (1-5 scale)..5 providing students adaptive feedback.
modelling cognitive and emotional empathythe empathy detection task is considered aparagraph-based, multi-class classiﬁcation task,where each paragraph is either considered to bea strength, weakness, or a suggestion for improve-ment and has a “non-empathic”, “neutral”, or “em-pathic” cognitive and emotional empathy level.
therefore, we assigned the levels of our cogni-tive and emotional empathy scores to three differ-ent labels: level 1 and 2 were assigned to a “non-empathic” text label, level 3 to a “neutral” label,and levels 4 and 5 to an“empathic” label .
wesplit the data into 70% training, 20% validation,and 10% test data.
to apply the model, the cor-pus texts were split into word tokens.
the modelperformances were measured in terms of accuracy,precision, recall, and f1-score..we trained a predictive model following thearchitecture of bidirectional encoder represen-tations from transformers (bert) proposed bydevlin et al.
(2018).
we used the bert modelfrom deepset,7 since it is available in german andprovides a deep pretrained model that was unsu-pervised while training on domain-agnostic ger-man corpora (e.g., the german wikipedia).
thebest performing paramenter combination for ourbert model incorporated a dropout probability of10% and a learning rate of 3e-5, and the number ofepochs were 3. after several iterations, we reacheda micro f1-score of 74.96% for the detection ofthe emotional empathy level and 69.98% for thedetection of the cognitive empathy level of a textparagraph.
moreover, we reached an f1-score of94.83% to predict a text paragraph as a strength, a64.28% to predict a text paragraph as a weakness,.
7https://github.com/deepset-ai/farm.
and 59.79% to predict suggestions for improve-ment.
to ensure the validity of our bert model,we benchmarked against bidirectional long-short-term-memory-conditional-random-fields classi-ﬁers (bilstm-crf).
in combination with the cor-responding embeddings vocabulary (glove) (pen-nington et al., 2014), our lstm reached an unsat-isfying f1-score of 61% for detecting the emotionalempathy level and 51% for detecting the cognitiveempathy level..evaluation in a peer learning setting we de-signed and built an adaptive writing support systemthat provides students with individual feedback ontheir cognitive and emotional empathy skills.
theapplication is illustrated in figure 4. we embed-ded our system into a peer writing exercise wherestudents were asked to write a peer review on abusiness model.
during this writing task, theyreceived adaptive feedback on the cognitive andemotional empathy level based on our model.
theevaluation was conducted as a web experiment fa-cilitated by the behavioral lab of our university, andthus, designed and reviewed according to the eth-ical guidelines of the lab and the university.
wereceived 58 valid results (mean age = 23.89, sd=3.07, 30 were male, 28 female).
the participantswere told to read an essay about a business modelof a peer student.
afterwards, they were askedto write a business model review for the peer byproviding feedback on the strengths, weaknesses,and suggestions for improvement of the particularbusiness model.
after the treatment, we measuredthe intention to use (itu) (venkatesh and bala,2008) by asking three items.
we also asked theparticipants to judge their perceived empathy skilllearning (pesl) by asking two items that coveredcognitive and emotional empathy skills (spreng.
4070figure 4: screenshot of a trained model on our corpus as an adaptive writing support system..et al., 2009; davis, 1983).
finally, we surveyed theperceived feedback accuracy (pfa) (podsakoff andfarh, 1989) to control the accuracy of our model.
all constructs were measured with a 1-to-7 pointlikert scale (1: totally disagree to 7: totally agree,with 4 being a neutral statement).8 furthermore,we asked three qualitative questions: “what didyou particularly like about the use of the tool?”,“what else could be improved?”, and “do you haveany other ideas?” and captured the demographics.
in total, we asked 13 questions.
all participantswere compensated with an equivalent of about 12usd for a 25 to 30 minute experiment..results participants judged their empathy skilllearning with a mean of 5.03 (sd= 1.05).
concern-ing the pfa, the subjects rated the construct with amean of 4.93 (sd= 0.94).
the mean value of inten-tion to use of the participants using our applicationas a writing support tool in peer learning scenar-ios was 5.14 (sd= 1.14).
the mean values of allthree constructs were very promising when com-paring the results to the midpoints.
all results werebetter than the neutral value of 4, indicating a posi-tive evaluation of our application for peer learningtasks.
we also asked open questions in our surveyto receive the participants’ opinions about the toolthey used.
the general attitude was very positive.
participants positively mentioned the simple andeasy interaction, the distinction between cognitiveand emotional empathy feedback, and the overallempathy score together with the adaptive feedbackmessage several times.
however, participants alsosaid that the tool should provide even more detailedfeedback based on more categories and should pro-.
8the exact items are listed in the appendix..vide concrete text examples on how to improvetheir empathy score.
we translated the responsesfrom german and clustered the most representativeresponses in table 16 in the appendix..6 conclusion.
we introduce a novel empathy annotation schemeand an annotated corpus of student-written peerreviews extracted from a real-world learning sce-nario.
our corpus consisted of 500 student-writtenpeer reviews that were annotated for review com-ponents and their emotional and cognitive empathylevels.
our contribution is threefold: 1) we deriveda novel annotation scheme for empathy modelingbased on psychological theory and previous workfor empathy modeling (buechel et al., 2018); 2) wepresent an annotation study based on 92 studentpeer reviews and three annotators to show that theannotation of empathy in student peer reviews isreliably possible ; and 3) to the best of our knowl-edge, we present the second freely available corpusfor empathy detection and the ﬁrst corpus for em-pathy detection in the educational domain basedon 500 student peer reviews in german.
for futureresearch, this corpus could be leveraged to supportstudents’ learning processes, e.g., through a conver-sational interaction (zierau et al., 2020).
however,we would also encourage research on the ethicalconsiderations of empathy detection models in user-based research (i.e., wambsganss et al.
(2021a)).
we, therefore, hope to encourage future research onstudent-generated empathetic texts and on writingsupport systems to train empathy skills of studentsbased on nlp towards quality education indepen-dent of a student’s location or instructors..4071references.
ahmed abbasi, hsinchun chen, and arab salem.
2008.sentiment analysis in multiple languages: featureselection for opinion classiﬁcation in web forums.
acm transactions on information systems, 26(3):1–34..muhammad abdul-mageed and lyle ungar.
2017.emonet: fine-grained emotion detection with gatedrecurrent neural networks.
acl 2017 - 55th annualmeeting of the association for computational lin-guistics, proceedings of the conference (long pa-pers), 1:718–728..silvie cinkov´a, martin holub, and vincent kr´ıˇz.
2012.managing uncertainty in semantic tagging.
in pro-ceedings of the 13th conference of the europeanchapter of the association for computational lin-guistics, pages 840–850, avignon, france.
associa-tion for computational linguistics..mark h. davis.
1983. measuring individual differ-ences in empathy: evidence for a multidimensionalapproach.
journal of personality and social psy-chology, 44(1):113–126..jean decety and philip l. jackson.
2004. the func-.
tional architecture of human empathy..firoj alam, morena danieli, and giuseppe riccardi.
2018. annotating and modeling empathy in spo-ken conversations.
computer speech and language,50:40–61..jacob devlin, ming-wei chang, kenton lee, andkristina toutanova.
2018. bert: pre-training ofdeep bidirectional transformers for language un-derstanding..simon baron-cohen and sally wheelwright.
2004.the empathy quotient: an investigation of adultswith asperger syndrome or high functioningautism, and normal sex differences.
technical re-port 2..c. daniel batson, jim fultz, and patricia a. schoen-rade.
1987. distress and empathy: two qualita-tively distinct vicarious emotions with differentmotivational consequences.
journal of personality,55(1):19–39..emily teding van berkhout and john m. malouff.
2016. the efﬁcacy of empathy training: a meta-analysis of randomized controlled trials.
journal ofcounseling psychology, 63(1):32–41..paul black and dylan wiliam.
2009. developing thetheory of formative assessment.
educational assess-ment, evaluation and accountability, 21(1):5–31..laura ana maria bostan and roman klinger.
2018.an analysis of annotated corpora for emotionclassiﬁcation in text title and abstract in german.
proceedings of the 27th international conference oncomputational linguistics, pages 2104–2119..sven buechel, anneke buffone, barry slaff, lyle un-gar, and jo˜ao sedoc.
2018. modeling empathy anddistress in reaction to news stories.
proceedingsof the 2018 conference on empirical methods innatural language processing, emnlp 2018, pages4758–4765..sven buechel and udo hahn.
2018. emotion repre-sentation mapping for automatic lexicon construc-tion (mostly) performs on human level.
pages2892–2904..sheng-yeh chen, chao-chun hsu, chuan-chun kuo,ting-hao, huang, and lun-wei ku.
2018. emo-tionlines: an emotion corpus of multi-party con-versations.
lrec 2018 - 11th international confer-ence on language resources and evaluation, pages1597–1601..pierre dillenbourg, sanna j¨arvel¨a, and frank fischer.
2009. the evolution of research on computer-supported collaborative learning.
in nicolas bal-acheff, sten ludvigsen, ton de jong, ard lazonder,and sally barnes, editors, technology-enhancedlearning: principles and products, pages 3–19.
springer netherlands, dordrecht..paul ekman.
1992. an argument for basic emotions.
cognition and emotion, 6(3/4):169–200..j.l.
fleiss.
1971. measuring nominal scale agree-ment among many raters.
psychological bulletin,76(5):378–382..john hattie and helen timperley.
2007. the powerreview of educational research,.
of feedback.
77(1):81–112..hamed khanpour, cornelia caragea, and prakharbiyani.
2017. identifying empathetic messages inonline health communities.
technical report..sara h. konrath, edward h. o’brien, and courtneyhsing.
2011. changes in dispositional empathyin american college students over time: a meta-analysis.
personality and social psychology review,15(2):180–198..klaus krippendorff.
1980. content analysis: an in-troduction to methodology.
sage publications, inc.,beverly hills, ca..klaus krippendorff.
2004. measuring the reliability ofqualitative text analysis data.
quality and quantity,38(6):787–800..e. j. lawrence, p. shaw, d. baker, s. baron-cohen,and anthony s. david.
2004. measuring empathy:reliability and validity of the empathy quotient.
psychological medicine, 34(5):911–919..bing liu.
2015. sentiment analysis: mining opinions,sentiments, and emotions.
cambridge universitypress..4072benjamin lok and adriana e. foster.
2019. can vir-in teaching empa-tual humans teach empathy?
thy in healthcare, pages 143–163.
springer interna-tional publishing..joseph luca and pina tarricone.
2001. does emo-tional intelligence affect successful teamwork?
proceedings of the 18th annual conference of theaustralasian society for computers in learning intertiary education, (december 2001):367–376..scott w. mcquiggan and james c. lester.
2007. mod-eling and evaluating empathy in embodied compan-international journal of human com-ion agents.
puter studies, 65(4):348–360..saif m. mohammad and felipe bravo-marquez.
2017.emotion intensities in tweets.
*sem 2017 - 6thjoint conference on lexical and computational se-mantics, proceedings, pages 65–77..tb moyers and t martin.
2010. revised global scales:motivational interviewing treatment integrity 3.1.1(miti 3.1.1).
university of new .
.
.
, 1(january):1–29..oecd.
2018. the future of education and skills - ed-.
ucation 2030..inter-computer-supported collaborative learning.
national journal of computer-supported collabora-tive learning, 3(3):237–271..sara rosenthal, noura farra, and preslav nakov.
2018.semeval-2017 task 4: sentiment analysis in twit-ter.
pages 502–518..breno santana santos, methanias colaqo junior, andjanisson gois de souza.
2018. an experimentalevaluation of the neuromessenger: a collaborativetool to improve the empathy of text interactions.
proceedings - ieee symposium on computers andcommunications, 2018-june:573–579..klaus r. scherer and harald g. wallbott.
1994. evi-dence for universality and cultural variation of dif-ferential emotion response patterning.
journal ofpersonality and social psychology, 66(2):310–328..julia e. seaman, i. e. allen, and jeff seaman.
2018.higher education reports - babson survey re-search group.
technical report..ashish sharma, adam s. miner, david c. atkins, andtim althoff.
2020. a computational approach tounderstanding empathy expressed in text-basedmental health support.
pages 5263–5276..jeffrey pennington, richard socher, and christopher d.manning.
2014. glove: global vectors for word rep-resentation.
in emnlp 2014 - 2014 conference onempirical methods in natural language processing,proceedings of the conference, pages 1532–1543.
association for computational linguistics (acl)..r. nathan spreng, margaret c. mckinnon, ray-mond a. mar, and brian levine.
2009. the torontoempathy questionnaire: scale development and ini-tial validation of a factor-analytic solution to multi-ple empathy measures.
journal of personality as-sessment, 91(1):62–71..ver´onica p´erez-rosas, rada mihalcea, kenneth resni-cow, satinder singh, and lawrence an.
2017. un-derstanding and predicting empathic behavior incounseling therapy.
acl 2017 - 55th annual meet-ing of the association for computational linguis-tics, proceedings of the conference (long papers),1:1426–1435..philip m. podsakoff and jiing lih farh.
1989. effectsof feedback sign and credibility on goal setting andtask performance.
organizational behavior and hu-man decision processes, 44(1):45–67..mathis poser and eva a. c. bittner.
2020. hybridteamwork: consideration of teamwork concepts toreach naturalistic interaction between humans andconversational agents.
in wi2020.
gito verlag..roman rietsche and matthias s¨ollner.
2019. insightsinto using it-based peer feedback to practice thestudents providing feedback skill.
proceedings ofthe 52nd hawaii international conference on sys-tem sciences..carolyn ros´e, yi chia wang, yue cui,.
jaimearguello, karsten stegmann, armin weinberger,and frank fischer.
2008.analyzing collabo-rative learning processes automatically: exploit-ing the advances of computational linguistics in.
christian stab and iryna gurevych.
2014. annotatingargument components and relations in persuasiveessays.
in proceedings of coling 2014, the 25thinternational conference on computational linguis-tics: technical papers ,, pages 1501–1510..christian stab and iryna gurevych.
2017. parsing ar-gumentation structures in persuasive essays.
com-putational linguistics, 43(3):619–659..carlo strapparava and rada mihalcea.
2007. semeval-2007 task 14: affective text.
acl 2007 - semeval2007 - proceedings of the 4th international work-shop on semantic evaluations, (june):70–74..stephen e. toulmin.
1984. introduction to reasoning..viswanath venkatesh and hillol bala.
2008. technol-ogy acceptance model 3 and a research agenda oninterventions.
decision sciences, 39(2):273–315..thiemo wambsganss, anne h¨och, naim zierau, andmatthias s¨ollner.
2021a.
ethical design of con-versational agents: towards principles for a value-sensitive design.
in proceedings of the 16th inter-national conference on wirtschaftsinformatik (wi)..thiemo wambsganss, tobias k¨ung, matthias s¨ollner,and jan marco leimeister.
2021b.
arguetutor: an.
4073adaptive dialog-based learning system for argu-in proceedings of the 2021 chimentation skills.
conference on human factors in computing sys-tems..thiemo wambsganss, christina niklaus, matthiascetto, matthias s¨ollner, jan marco leimeister, andsiegfried handschuh.
2020a.
al : an adaptivelearning support system for argumentation skills.
in acm chi conference on human factors in com-puting systems, pages 1–14..thiemo wambsganss, christina niklaus, matthiass¨ollner, siegfried handschuh, and jan marcoleimeister.
2020b.
a corpus for argumentative writ-ing support in german.
in proceedings of the 28thinternational conference on computational linguis-tics, pages 856–869, barcelona, spain (online).
in-ternational committee on computational linguis-tics..thiemo wambsganss, florian weber, and matthiass¨ollner.
2021c.
design and evaluation of an adap-in hawaii interna-tive empathy learning tool.
tional conference on system sciences (hicss)..jin wang, liang chih yu, k. robert lai, and xuejiezhang.
2016. dimensional sentiment analysis usinga regional cnn-lstm model.
54th annual meet-ing of the association for computational linguistics,acl 2016 - short papers, pages 225–230..bo xiao, dogan can, panayiotis g georgiou, davidatkins, and shrikanth s narayanan.
2012. analyz-ing the language of therapist empathy in motiva-tional interview based psychotherapy.
signal andinformation processing association annual sum-mit and conference (apsipa), ... asia-paciﬁc.
asia-paciﬁc signal and information processing associa-tion annual summit and conference, 2012..n. zierau, t wambsganss, andreas janson, soﬁasch¨obel, and jan marco leimeister.
2020. theanatomy of user experience with conversationalagents : a taxonomy and propositions of serviceclues.
in international conference on informationsystems (icis)., pages 1–17..a details on the description of the.
annotation scheme9.
a more detailed description of the cognitive andemotional empathy scores can be found in table 7and table 8..b details on the annotation process.
the annotation process was split into three steps:.
1. reading of the entire peer review: theannotators are confronted with the student-written peer review and are asked to read the.
9further examples and descriptions can be found in our.
annotation guideline..whole document.
this helps to get a ﬁrst im-pression of the review and get an overview ofthe single components and the structure of it..2. labeling the components and elabora-tions: after reading the entire student-writtenpeer review, the annotator is asked to label thethree different components (strengths, weak-nesses and suggestions for improvement).
ev-ery supporting sentence (such as explanation,example, etc.)
is annotated together with thereferred component..3. classiﬁcation of the cognitive and emo-tional empathy levels: each component isassessed on its level of cognitive and emo-tional empathy by giving a number between1-5. each category is carefully deﬁned anddelimited according to table 7 and table 8..c disagreement analysis.
to analyze the disagreement between the three an-notators, we created a confusion probability matrix(cpm) (cinkov´a et al., 2012) for the review com-ponents and the empathy level scores.
a cpmcontains the conditional probabilities that an anno-tator assigns to a certain category (column) giventhat another annotator has chosen the category inthe row for a speciﬁc item.
in contrast to tradi-tional confusion matrices, a cpm also allows forthe evaluation of confusions if more than two an-notators are involved in an annotation study (staband gurevych, 2014)..table 9 shows that there is a broad agreement be-tween the annotators in distinguishing between thedifferent types of review components.
the majordisagreement is between suggestions and weak-nesses, though with a score of 60%, the agreementis still fairly high.
consequently, the annotationof review components in terms of strengths, weak-nesses, and suggestions for improvements yieldshighly reliable results..the cpms for the empathy levels (see tables10 and 11 reveal that there is a higher confusionbetween the scores assigned by the three reviewers,as compared to the annotation of the review com-ponents.
however, when analyzed more closely,one can see that the scores mostly vary only withina small window of two or three neighboring scores.
therefore, we conclude that the annotation of cog-nitive and emotional empathy scores is reliablypossible, too..4074score5 = strong.
4 = fairly strong.
3 = slightly weak / equal.
2 = very weak.
1 = absolutely weak.
descriptionthe student fully understands the peer’s thoughts.
she completely steps outside her ownperspective and thinks from the peer’s perspective.
she does that by carefully evaluating thepeer’s idea with rich explanations.
questions, personal pronouns, or direct addressing of theauthor can be used in order to better understand and elaborate on the peer’s perspective.
strengths: the student fully grasps the idea of the peer.
she elaborates on strengths that areimportant for the peer for her continuation of the task and adds explanations, thoughts, orexamples to her statements and reasons why the strength is/strengths are important for thebusiness idea.
weaknesses: the student thinks completely from the peer’s perspective and what would helphim/her to further succeed with the task.
the student explains the weakness in a very detailedmanner and describes why the weakness is important to consider.
he can also give counterargu-ments or ask questions to illustrate the weakness.
suggestions for improvement: the student suggests improvements as if he were in the peer’sposition in creating the best possible solution.
the student completes his suggestions with richexplanations on why he/she would do so and elaborates on the improvements in a very concreteand detailed way.
almost every suggestion is supported by further explanations.
the student thinks from the perspective of the peer.
she elaborates in a way that serves thepeer best to further establish the idea or activity.
each component is afﬁrmed with furtherexplanations.
strengths: the student is able to recognize one or more strengths that are helpful for the peer toafﬁrm their business idea and activity.
he/she highlights contextual strengths rather than formalstrengths.
the student supports most statements with examples or further personal thoughts onthe topic but might still be missing some reasonings.
weaknesses: the student thinks from the peer’s perspective and what would help him/her tofurther succeed with the task.
this could be demonstrated by stating various questions andestablishing further thoughts.
the student explains the weakness and adds examples, but he/sheis still missing some reasonings.
suggestions for improvement: the student suggests one or more improvements that are relevantfor the further establishment of the activity and idea from the perspective of the peer.
mostsuggestions are written concretely and, if applicable, supported by examples.
in most cases, thestudent explains why he/she suggests a change.
the student tries to understand the perspective of the peer and adds further elaborations to herstatements.
however, her elaborations are not completely thought through and her feedbackis missing some essential explanations, examples, or questions to make sure she understoodeverything correctly.
strengths: the student mentions one or more strengths and explains some of them with minorexplanations or examples on why it is seen as a strength.
however, most strengths focus onformal aspects rather than contextual aspects.
weaknesses: the student states one or more weaknesses and explains some of them with minorexplanations or examples.
the student could also just state questions to illustrate the weaknessin the peer’s business idea.
most weaknesses are not explained why they are such.
suggestions from improvements: the student suggests one or more improvements that are mostlyrelevant for the further establishment of the activity.
the suggestions are written only on ahigh-level and most of them do not include further explanations or examples.
the studentexplains only occasionally why he/she suggests a change or how it could be implemented.
the student does not try to understand the peer’s perspective.
the student rather just tries toaccomplish the task of giving feedback.
strengths: the student mentions one or more strengths.
they could be relevant for the peer.
however, he does not add any further explanation or details.
weaknesses: the student states one or more weaknesses without explaining why they are seen assuch.
they could be relevant for the peer.
however, the statements do not include any furtherelaboration on the mentioned weakness.
suggestions for improvement: the student suggests one or more improvements that could berelevant for the peer.
however, the student does not explain why he/she suggests the change orhow the suggestions for improvement could be implemented.
the student’s feedback is very short and does not include the peer’s perspective.
she does notadd any further elaboration in her thoughts.
strengths: the student only mentions one strength.
this might not be relevant at all and lacksany further explanation, detail, or example.
weakness: the student only mentions one weakness.
this might not be relevant at all and lacksany further explanation, detail, or example.
suggestions for improvement: the student only mentions one suggestion.
the suggestion is notfollowed by any explanation or example and might not be relevant for the further revision of thepeer..table 7: detailed description of the cognitive empathy scores..4075score5 = strong.
4 = fairly strong.
3 = slightly weak / equal.
2 = very weak.
1 = absolutely weak.
descriptionthe student is able to respond very emotionally to the peer’s work and fully represents theaffectional state in her entire review.
she illustrates this by writing in a very emotional andpersonal manner and expresses her feelings (positive or negative) throughout the review.
strongexpressions include exclamation marks (!).
typical feedback in this category includes phrasessuch as “brilliant!”, “fantastic”, “excellent”, “i am totally on the same page as you”, “i am veryconvinced”, “personally, i ﬁnd this very important, too”, “i am very unsure”, “i ﬁnd this critical”,“i am very sure you feel”, “this is compelling for me”, etc.
the student is able to respond emotionally to the peer’s submitted activity with suitable emotions(positive or negative).
she returns emotions in her feedback on various locations and expresses herfeelings by using the personal pronoun (“i”, “you”).
some sentences might include exclamationsmarks (!).
typical feedback in this category includes phrases such as “i am excited”, “this isvery good!”, “i am impressed by your idea”, “i feel concerned about”, “i ﬁnd this very...”, “inmy opinion”, “unfortunately, i do not understand”, “i am very challenged by your submission”,“i am missing”, “you did a very good job”, etc.
the student occasionally includes emotions or personal emotional statements in the peer review.
they could be quite strong.
however, the student’s review is missing personal pronouns (“i”,“you”) and is mostly written in third person.
emotions can both be positive or negative.
negativeemotions can be demonstrated with concern, missing understanding or insecurity (e. g., withmodal verbs or words such as rather, perhaps).
typically, scale 3 includes phrases such as “it’simportant”, “the idea is very good”, ”the idea is comprehensible”, “it would make sense”, “thetask was done very nicely”, “it could probably be that”, etc.
mostly, the student does not respond emotionally to the peer’s work.
only very minor and weakemotions or personal emotional statements are integrated.
the student writes mostly objectively(e.g., “okay”, “this should be added”, “the task was done correctly”, etc.).
in comparisonto level 1, she might use modal verbs (might, could, etc.)
or words to show insecurity in herfeedback (rather, maybe, possibly).
the student does not respond emotionally to the peer’s work at all.
she does not show herfeelings towards the peer and writes objectively (e.g., no “i feel”, “personally” “i ﬁnd this..” andno emotions, such as “good”, “great”, “fantastic”, “concerned”, etc.).
typical examples wouldbe “add a picture.” or “the value gap xy is missing.”.
table 8: detailed description of the emotional empathy scores..suggestions weakness.
suggestionsweaknessstrengthnone.
0.60560.21390.02640.0662.
0.29700.70090.03470.0784.strength0.02140.02030.83400.0742.none0.07590.06480.10490.7812.table 9: cpm for review component annotations..table 10: cpm for cognitive empathy level annota-tions..1.113.125.025.014.021.
1.106.154.059.026.043.
2.387.266.159.054.014.
2.459.234.282.115.061.
3.175.362.223.283.105.
3.286.455.350.347.227.
4.165.211.482.300.556.
4.086.128.240.295.501.
5.160.035.112.349.303.
5.063.029.068.218.168.
12345.
12345.non-empathicempathicneutralnonemicro avgmacro avgweighted avgsamples avg.
precision0.57460.63640.52400.98630.73220.68030.73630.7248.recall0.56620.56250.57070.97290.73020.66810.73020.7302.f1-score0.57040.59720.54640.97950.74820.67340.73270.7266.support136112191295734734734734.table 12: bert model results for emotional empathy..non-empathicempathicneutralnonemicro avgmacro avgweighted avgsamples avg.
precision0.57390.64340.30620.98410.69490.62690.72250.6861.recall0.35870.54900.47470.98020.69250.59070.69250.6925.f1-score0.44150.59250.37230.98220.69370.59710.69960.6882.support1842861985061174117411741174.table 13: bert model results for cognitive empathy..table 11: cpm for emotional empathy level annota-tions..d details on application and evaluation.
of writing support tool.
to ensure the validity of our bert model, webenchmarked against bidirectional long-short-.
4076performance”.
all constructs were measured witha 1- to 7-point likert scale (1: totally disagree to 7:totally agree, with 4 being a neutral statement)..clusteronthyreaction.
empa-feedback.
on the feedbackfor skilllearn-ing.
oncognitiveand emotionalempathy.
improvementsonfeedbackgranularityimprovementson feedback rec-ommendations.
feature”i think that this tool could help me notonly to put myself in the position of a per-son in terms of content and make sugges-tions but also to communicate to thembetter””the empathy feedback was clear andcould be easily implemented.
i had thefeeling i learned something.would use itagain!””it was helpful that a distinction wasmade between the two categories of em-pathy.
this again clearly showed methat i do not show emotional empathyenough.
it was also useful that the toolsaid how to show emotional empathy(feelings when reading the business ideaetc.
).””it would be better if the feedback wasmore s elective or with detailed cate-gories about empathy.””even more detailed information on howi can improve my empathy writing wouldbe helpful, e.g., with review examples.”.
table 16: representative examples of qualitative userresponses after the usage of our empathy support tool..non-empathicneutralempathicnonef1 avgweighted avg.
precision0.57390.30620.64340.98410.640.73.recall0.35870.47470.54900.98020.640.73.f1-score0.44150.37230.59250.98220.640.73.support184198286506368368.table 14: results for the lstm for emotional empathy..non-empathicneutralempathicnonef1 avgweighted avg.
precision0.740.430.350.990.630.75.recall0.280.550.630.940.600.68.f1-score0.400.490.450.970.580.68.support836057168368368.table 15: results for the lstm for cognitive empathy..term-memory-conditional-random-fields classi-ﬁers (bilstm-crf).
in combination with the cor-responding embeddings vocabulary (glove) (pen-nington et al., 2014), our lstm reached an unsat-isfying f1-score of 61% for detecting the emotionalempathy level and 51% for detecting the cognitiveempathy level..more information on the results of our bertmodel and the lstm for emotional and cognitiveempathy detection can be found in the tables 12,13, 15, and 15..in the post-survey, we measured perceived use-fulness following the technology acceptance model(venkatesh and bala, 2008).
the items for theconstructs were: ”imagine the tool was availablein your next course, would you use it?”, ”assum-ing the learning tool would be available at a nextcourse, i would plan to use it.”, or ”using the learn-ing tool helps me to write more emotional and cog-nitive empathic reviews.
” moreover, we asked theparticipants to judge their perceived empathy skilllearning (pesl) by asking two items that cover cog-nitive and emotional empathy skills (spreng et al.,2009; davis, 1983): “i assume that the tool wouldhelp me improve my ability to give appropriateemotional feedback.” and “i assume that the toolwould help me improve my ability to empathize withothers when writing reviews.” finally, we surveyedthe perceived feedback accuracy (pfa) (podsakoffand farh, 1989) of both learning tools by askingthree items: “the feedback i received reﬂected mytrue performance.”, “the tool accurately evaluatedmy performance.”, and “the feedback i receivedfrom the tool was an accurate evaluation of my.
4077