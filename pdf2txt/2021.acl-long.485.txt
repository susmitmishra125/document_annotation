an end-to-end progressive multi-task learning framework for medicalnamed entity recognition and normalization.
baohang zhou1,3, xiangrui cai2,3, ying zhang1,3,∗, xiaojie yuan1,31 college of computer science, nankai university, tianjin 300350, china2 college of cyber science, nankai university, tianjin 300350, china3 tianjin key laboratory of network and data security technology, tianjin 300350, chinazhoubh@mail.nankai.edu.cn, {yingzhang,caixr,yuanxj}@nankai.edu.cn.
abstract.
medical named entity recognition (ner) andnormalization (nen) are fundamental for con-structing knowledge graphs and building qasystems.
existing implementations for medi-cal ner and nen are suffered from the er-ror propagation between the two tasks.
themispredicted mentions from ner will directlyinﬂuence the results of nen.
therefore, thener module is the bottleneck of the wholesystem.
besides, the learnable features forboth tasks are beneﬁcialto improving themodel performance.
to avoid the disadvan-tages of existing models and exploit the gener-alized representation across the two tasks, wedesign an end-to-end progressive multi-tasklearning model for jointly modeling medicalner and nen in an effective way.
there arethree level tasks with progressive difﬁculty inthe framework.
the progressive tasks can re-duce the error propagation with the incremen-tal task settings which implies the lower leveltasks gain the supervised signals other thanerrors from the higher level tasks to improvetheir performances.
besides, the context fea-tures are exploited to enrich the semantic infor-mation of entity mentions extracted by ner.
the performance of nen proﬁts from the en-hanced entity mention features.
the stan-dard entities from knowledge bases are intro-duced into the ner module for extracting cor-responding entity mentions correctly.
the em-pirical results on two publicly available med-ical literature datasets demonstrate the superi-ority of our method over nine typical methods..1.introduction.
to dig into the large amount of electronic medicalrecords, there has been an increasing interest inapplying information extraction to them.
thesetechniques can generate tremendous beneﬁt for cor-responding research and applications, such as med-.
∗corresponding author..figure 1: the overall frameworks for medical namedentity recognition and normalization..ical knowledge graph (wu et al., 2019) and qasystems (lamurias and couto, 2019).
among themedical text mining tasks, medical named entityrecognition and normalization are the most funda-mental tasks..named entity recognition tries to ﬁnd the bound-aries of mentions from the medical texts.
andnamed entity normalization maps mentions ex-tracted from the medical text to standard identiﬁers,such as mesh and omim (zhao et al., 2019).
theinitial pipeline implementations for medical nerand nen have a main limitation: error extractionsfrom ner cascade into nen which result in nor-malization errors.
besides, the mutual use betweenrecognition and normalization is not utilized in thepipeline models.
to alleviate the limitations andachieve a higher performance, some researchers fo-cused on jointly modeling these two tasks.
leamanand lu (2016) proposed a joint scoring function formedical ner and nen.
lou et al.
(2017) castedthe output construction process of the two tasks.
proceedingsofthe59thannualmeetingoftheassociationforcomputationallinguisticsandthe11thinternationaljointconferenceonnaturallanguageprocessing,pages6214–6224august1–6,2021.©2021associationforcomputationallinguistics6214nernenmedical textmedical textnernenfeature extractorstandard entity base( meshor omim )pipeline frameworkparallel multi-task frameworklow-level taskend-to-end progressive multi-task frameworkmedical textmid-level taskhigh-level taskas a state transition process to perform medicalnamed entity recognition and normalization.
tocapture the semantic features of two tasks, zhaoet al.
(2019) proposed a multi-task learning frame-work with an explicit feedback strategy for medicalner and nen..as shown in figure 1, there are two commonframeworks: pipeline and parallel multi-task frame-work.
the former one is formulated to maxi-mize the posterior probabilities p(yner |x ) andp(ynen |m, e ) where x is the medical text, m isthe medical mentions extracted by a recognitionmodel, e is the standard entity, yner and ynen arethe labels.
the latter one tries to maximize the pos-terior probabilities p(yner, ynen |x ) (zhao et al.,2019).
both of these are struggled with the bot-tleneck that is named entity recognition.
in theabove frameworks, the ner module is trained tomemorize the medial mentions in the training set.
however, the medical mentions are various andthere is a gap between the training and test set.
it isnatural that the unseen mentions in training set arehard to recognize during the testing phase.
there-fore, the conventional frameworks do not gain moreideal generalization ability..to overcome the disadvantage mentioned above,we reconsidered the process of medical named en-tity recognition and normalization.
the ultimategoal is to map the extracted medical mentions to thestandard entity base.
therefore, the target standardentity base can be regarded as a dictionary.
the ini-tial process of nen and ner can be reconsideredas detecting whether the medical text contains thecandidate standard entity and ﬁnding the mentionsshould be replaced.
based on this idea, we pro-pose an end-to-end progressive multi-task learningframework for medical named entity recognitionand normalization (e2emern1).
compared withordinary multi-task learning, progressive multi-tasklearning focuses on the aggregation logic of tasks’speciﬁc features (hong et al., 2020).
a difﬁculttarget is divided into a few tasks that are intercon-nected through the combination of features.
to takefull advantage of the data attributes, we propose theframework including three tasks with progressivedifﬁculty extended from the conventional ner andnen tasks.
the low-level task is the traditionalner which tries to extract all entities in the med-ical text.
the mid-level task is deﬁned to iden-.
1when ready, the code will be published at https://.
github.com/zhoubaohang/e2emern.
tify whether there exist medical mentions in thetext that should be mapped to the candidate stan-dard entity.
the high-level task combines the ﬁrsttwo level tasks, and targets to extract the mentionswhich should be mapped to the candidate standardentity..unlike the existing frameworks, e2emern ex-ploits the progressive tasks to learn the ﬁne-grainedrepresentations.
the mid-level and high-level tasksfacilitate the framework learning the correspond-ing features between the medical mentions andstandard entities.
the low-level task can gain thesupervised signals from the higher level tasks toextract medical mentions corresponded to standardentities in the knowledge bases more exactly.
ourcontributions in this manuscript can be summarizedas follows:.
1. we reconsider the process of the ner andnen tasks, and ﬁrstly propose to exploit thethree tasks with progressive difﬁculty to trainthe end-to-end medical named entity recogni-tion and normalization framework..2. the experimental results on two medicalbenchmarks demonstrate that our frameworkoutperforms the existing medical named entityrecognition and normalization models.
andwe conducted detailed analysis on the frame-work to represent its superiority..2 related work.
2.1 medical named entity recognition and.
normalization.
medical named entity recognition and normaliza-tion are two basic tasks for the medical text mining.
the conventional pipeline frameworks contains thener model and nen one separately (v´azquezet al., 2008; leaman and lu, 2014; sahu andanand, 2016; zhou et al., 2020).
ner modelsextract medical mentions in texts and then nenmodels map these mentions to standard entity iden-tiﬁers.
to reduce the error propagation in thepipeline frameworks, some researchers proposed tomodel ner and nen jointly.
leaman et al.
(2015)combined two traditional machine learning modelsas an ensemble ner and nen model.
and to learnthe joint probability distribution of the ner andnen tasks, a semi-markov based model was pro-posed by leaman and lu (2016).
however, tradi-tional methods depend on the human-based featureengineering.
with the development of the deep.
6215learning, recurrent neural networks (rnn) havereplaced human effort and been utilized to extractfeatures of raw texts.
zhao et al.
(2019) designedan rnn-based network architecture with feedbackstrategy to model the two tasks jointly.
recently,the pre-trained models, such as bert (devlin et al.,2019), biobert (lee et al., 2020), make impres-sive progress in the natural language processing(nlp) area.
xiong et al.
(2020) used bert as thebase module and proposed a machine reading com-prehension framework to solve the ner and nenproblems jointly..2.2 sequence labeling.
named entity recognition can be regarded as asequence labeling problem.
sequence labelingwas explored extensively as a basic task in nlp.
probabilistic graphical models, such as: hiddenmarkov model (xiao et al., 2005) and conditionalrandom ﬁelds (crf) (lafferty et al., 2001) arethe typical methods to solve the problem.
withdeep learning modules gradually replacing man-ual feature engineering, long short-term memory(lstm) (hochreiter and schmidhuber, 1997) net-work stacked with crf (xu et al., 2008) has beena benchmark model for sequence labeling (lampleet al., 2016).
some researchers utilized multi-tasklearning to model relevant nlp tasks and gainedbetter performances on these tasks including se-quence labeling (aguilar et al., 2017; cao et al.,2018).
besides, the attributes of the data them-selves are used to design the multi-task learningmodel.
considering whether sentences containentities, wang et al.
(2019) proposed the multi-task learning model to predict whether input datahave entities and then extract corresponding enti-ties.
kruengkrai et al.
(2020) exploited sentence-level labels and token-level labels to propose a jointmodel supporting multi-class classiﬁcation..2.3 short text matching.
named entity normalization is formulated as a shorttext matching problem.
the information retrievalmethod, such as: bm25 (robertson et al., 1994),is a universal model to solve this problem.
withthe development of neural language model, text se-mantic is exploited to model the similarity betweentwo short texts.
the distributed representations oftexts, such as: word2vec (mikolov et al., 2013)and glove (pennington et al., 2014), are utilized tocalculate the similarity distance between two texts.
some medical named entity normalization models.
are based on this method (leaman and lu, 2014;zhou et al., 2020).
considering local texts are moreimportant than global ones, some researchers uti-lized convolution neural networks (cnn) to extractlocal features and exploited interactive attentionmechanism to match the semantic similarity of twotexts (yin et al., 2016; chen et al., 2018)..3 methodology.
we introduce the notations about ner and nenbefore getting into the details of the framework.
for ner task, we denote {(xi, yi)}n si=1 as a train-ing set with n s samples, where xi is the medicaltext and yi is the ner label.
given a sentencewith n w words, the medical text can be formulatedas x = {x1, x2, .
.
.
, xn w } and the ner label isy = {y1, y2, .
.
.
, yn w }.
to solve the ner task, wetry to maximize the posterior probability p(y |x ).
according to the ner label, we can extract themedical mentions {mi}n mi=1 from the medical text,where n m is the number of the mentions.
fornen task, we need to map each mention m to astandard entity e in the entity base b = {ei}n ei=1.
we formulate the object of nen task as a posteriorprobability p(e |m, b ), and e is the standard entitywhich the mention m should be mapped to..3.1 progressive tasks.
with the help of ner and nen, we can map medi-cal mentions in the raw texts to the correspondingstandard entities.
traditional pipeline implementa-tions for the two tasks are composed of the individ-ual ner and nen models.
the simple partitioningof the two models leads to the error propagation be-tween them.
considering the correlation betweenthe two tasks, zhao et al.
(2019) proposed the par-allel task framework to improve the performance ofthe model.
however, the intuitive feedback strategyfor the output layers of two tasks is not beneﬁcialto modeling the ﬁne-grained features between twotasks.
the above implementations lack thinkingabout the learning process.
the process of hu-man learning often goes from easy to difﬁcult (xuet al., 2020).
especially for the correlated tasks,humans can dig into the hidden knowledge andextract them from the easy tasks for completingthe hard ones.
based on this idea, we reconsiderthe process of conventional ner and nen tasks,and propose three correlated tasks with progressivedifﬁculty.
as shown in figure 2, we take a medicaltext from the real dataset ncbi (dogan et al., 2014).
6216figure 2: the end-to-end progressive multi-task learning framework for medical named entity recognition andnormalization.
the left part is the implementation details of the framework.
the right part is the real example todescribe the three progressive tasks..as an example to describe the tasks.
the medicaltext is “familial mediterranean fever is a reces-sive disorder” and its corresponding ner label is“b-disease i-disease i-disease o o b-disease i-disease”.
among the tokens, medical mentions“familial mediterranean fever” and “recessive dis-order” are mapped to the standard entity identiﬁers“d010505” and “d030342” respectively..low-level task is deﬁned to memorize all medi-cal mentions seen in training set.
given the medi-cal text mentioned above, this task needs to predictthe ner label and extract the mentions “famil-ial mediterranean fever” and “recessive disorder”.
similar to the process of human learning vocab-ulary, the low-level task forces the framework tolearn the medical mentions indiscriminantly.
how-ever, the ﬁnal target is to map mentions to standardentities.
we should continue to bridge the gap be-tween medical mentions in raw texts and standardentities in the database..mid-level task targets to determine whethermedical texts implicit the query standard entities.
with the above medical text and the standard en-tity “d010505” as input, this task should inferencethe text contains this entity.
through this task, theframework establishes the coarse-grained relation-ship between the mentions with contexts and thequery standard entities.
however, the mentions areincomplete correspondence to the query standardentities.
because there is more than one mention inthe raw text which should be extracted and mapped.
to the corresponding standard entities.
we needto specify which mention in the text should bemapped to the input standard entity..high-level task is proposed to extract the men-tions which should be mapped to the query standardentity.
after acquiring the above medical text andthe standard entity “d030342”, this task shouldextract the mention “recessive disorder”.
if theinput text contains no mention which should bemapped to the query entity, the output of this taskis empty.
the effect of this task is the same as thatof nen, but it is harder than nen.
to accomplishthe high-level task, we need to build on the ﬁrst twotasks.
the low-level task provides the representa-tions of the medical mentions with contexts whichis beneﬁcial to locating them in raw texts.
the mid-level task forces the model to learn the correlatedfeatures between mentions with standard entities.
with the help of two pre-tasks, the high-level taskcan be accomplished in an effective way..3.2.implementation details.
we build on the progressive tasks to implementthe framework e2emern as shown in figure 2.considering the logic of feature aggregation andthe strategies for training different tasks, we needto give detailed explanations by the level of tasks.
for a given sentence x = {x1, x2, .
.
.
, xn w },we need to map it to the dense vector representa-tions.
with the impressive performances of pre-trained models, we utilize bert (devlin et al.,.
6217attention mechanismbertmostcoloncancersarisefrommutationstokenizeid: d003110name: colonic neoplasmscontent: tumors or cancer…id: d003110name: colonic neoplasmscontent: tumors or cancer…bert0/1dense layerob-diseaseoooi-diseasehigh-level taskmid-level tasklow-level taskdense layerob-diseaseoob-diseasei-diseasegate mechanismstandard entity base( mesh/ omim )input medical text:familialmediterraneanfeverisarecessivedisorder.memorizeextract medical mentions:familialmediterraneanfeverrecessivedisorderinput medical text:familialmediterraneanfeverisarecessivedisorder.input standard entity: d010505implicitdetermine whether the text contains the standard entity:yesinput medical text:familialmediterraneanfeverisarecessivedisorder.input standard entity: d010505extractionextract mentions correlated to the standard entity:familialmediterraneanfever2019) as feature extractors to acquire the dis-tributed representations of sentences.
the bertarchitecture is composed by the transformer net-works and its weights are trained with large numberof corpus.
the feature extraction process is sim-pliﬁed as bert(x) = {h1, h2, .
.
.
, hn w }, whereh ∈ r1024×1.
the low-level task is deﬁned asthe same as ner, and we utilize the ner labelsas the target.
the sentence features {hi}n wi=1 arefed into the softmax layer, and we can computethe prediction probabilities of low-level task as:ˆyi = softmax(wlhi + bl) where wl and bl aretrainable parameters.
for training, we utilize thecross-entropy loss as the objective function.
theloss function of low-level task is deﬁned as follows:.
llow = −.
yi log ˆyi..(1).
n w(cid:88).
i=1.
the sample for the mid-level task is deﬁned asa tuple (x, e, ym).
if the text x contain the men-tions which should be mapped to the entity e, ymis assigned 1 otherwise 0. to bridge the gap be-tween the mentions and standard entities in themid-level task, we need also to extract the fea-tures of standard entities.
the standard entity eis described with the speciﬁc name and some med-ical contents.
we feed the name (or contents) ofthe entity into the bert and perform the aver-age pooling on the output of bert.
the featurevector of i-th standard entity in the database is de-ﬁned as hei .
considering the words of mentionsin raw texts are more correlated to the standardentity, we adopt the attention mechanism (zhouet al., 2016) to focus on the local words of sen-tences.
the attention weighted average feature canbe calculated as: ha = (cid:80)n wi=1 αixi.
and the atten-tion score α is deﬁned as: αi = exp (s(xi,he))i=1 exp (s(xi,he))where s(xi, he) = wa[xi; he] + ba.
wa and ba aretrainable weights in the attention module.
after ac-quiring the entity-attention feature ha and standardentity feature he, we can calculate the predictionprobabilities ˆym = σ(wm[he; ha] + bm) where σis the sigmoid function.
the loss function for themid-level task is formulated as the cross-entropy:.
(cid:80)n w.lmid = −(ym log ˆym + (1 − ym) log(1 − ˆym)).
(2)we deﬁne the tuple (x, e, yh) as the sample forthe high-level task where yh = {yhi=1.
giventhat the medical text x is “familial mediterraneanfever is a recessive disorder.” and standard en-.
i }n w.figure 3: the original sample is from the dataset ncbi.
the extended samples are built on the original one andused to train the model.
the 3rd sample is generated bynegative sampling..tity e is “d030342”, the label sequence yh shouldbe “o o o o o b-disease i-disease”.
to takeadvantage of the pre-tasks, we propose the gatemechanism to aggregate the different features forsolving this task.
the sentence feature {hi}n wi=1implicit the medical mentions while the entity at-tention feature ha contains clearer locations of thecorresponding mentions.
therefore, we proposethe gate mechanism to focus on the ﬁne-grainedfeature dimensions.
the formulation of the gatemechanism is g(h, ha) = σ(wg[h; ha] + bg)where h = {hi}n wi=1 and ha = [ha; .
.
.
; ha] ∈r1024×n w.
considering the semantic differencebetween the mentions and corresponding standardentities, we exploit the gate mechanism to fusethe standard entity feature with the sentence fea-ture.
the fusion sentence feature is formulated as:hf = h(cid:12)(1−g(h, ha))+he (cid:12)g(h, ha) where(cid:12) is the element-wise production, hf = {hfi }n wi=1and he = [he; .
.
.
; he] ∈ r1024×n w.
we feed thefusion feature into the softmax layer to predict thei = softmax(whhfprobabilities ˆyhi + bh).
as thesame as the low-level task, we utilize the cross-entropy loss function as follows:.
lhigh = −.
i log ˆyhyhi ..(3).
n w(cid:88).
i=1.
3.3 training process.
for the framework, we denote the training sampleas (x, y, e, ym, yh).
according to the deﬁnitionsof the three tasks, we can generate the task labelscorresponding to the input sentence.
the exampleis shown in figure 3. given the medical text x,the label y for the low-level task is the same as theoriginal ner label.
we use the standard entitieswhich the mentions {mi}n mi=1 should be mapped toas the input entity e respectively.
the high-leveltask label yh is based on y, and it only keeps theoriginal labels of y which are correlated to the inpute. besides, we adopt the negative sampling strategy.
6218familialmediterraneanfeverisarecessivedisorderb-diseasei-diseasei-diseaseoob-diseasei-disease𝑿:𝒚:d010505d030342standard entity:original sampleextended sample1.
(𝑿,𝒚,d010505,1,`b-disease i-disease i-disease o o o o(cid:4593))2.
(𝑿,𝒚,d030342,1,`o o o o o b-disease i-disease(cid:4593))3.
(𝑿,𝒚,d016870,0,`o o o o o o o(cid:4593))to select the standard entity which is not related tothe input sentence x as the input entity e..to tackle the three level tasks at once, we intro-duce two hyper-parameters to sum eqn.
1, eqn.
2 and eqn.
3. the overall loss function for theframework is deﬁned as follows:.
l = llow + λ · lmid + µ · lhigh.
(4).
where λ and µ are hyper-parameters for balancingdifferent task losses.
after generating samples, wefeed them into the model and then calculate theloss according to eqn.
4. following the back-propagation method, we update the weights of thenetworks with the acquired loss.
after every epochof training, we re-sample the training samples forbetter generalization of the model..4 experiments.
4.1 datasets and experiment settings.
we compare our framework with the existing meth-ods on two medical benchmark datasets.
table 1presents the detailed statistical information of thetwo datasets.
there are 798 public medical ab-stracts in the ncbi dataset (dogan et al., 2014).
each medical mention in the text is annotated withmesh/omim identiﬁers.
bc5cdr dataset (liet al., 2016) contains 1500 public medical abstractswhich are also annotated with mesh identiﬁers.
we split each abstract into sentence samples withan average of 40 words according to the ends ofsentences.
the padding char is used for ﬁlling theunequal length samples to the ﬁxed length..during the training process, we ﬁrst train themodel on the training set and test it on the develop-ment set for searching the best hyper-parameters.
then, we ﬁx the best hyper-parameters and trainthe model on the set composed of the training anddevelopment sets.
before the model is trained tothe searched maximum number of epochs, we takethe f1 score as the reported result when the lossgets the lowest.
in our experiments, we set thehyper-parameters λ, µ and learning rate to 0.125,0.1 and 1e-5 respectively.
to train the model, weuse the adam (kingma and ba, 2015) algorithmto update the weights.
and all experiments are ac-celerated by the two nvidia gtx 2080ti devices..4.2 compared methods.
to represent the effectiveness of our framework,we adopt the competitive models as the compared.
item.
ncbi.
bc5cdr.
train setdev settest set# entities# ner labels# nen labels.
542492394070253743.
4560458147972854552311.table 1: the statistical information of the ncbi datasetand the bc5cdr dataset in our experimental settings..methods including traditional machine learningmethods and impressive deep learning models..dnorm (leaman et al., 2013) is the pipelinemodel for medical ner and nen.
it utilizes thetf-idf feature to learn the bilinear mapping ma-trix for the normalization task.
leadmine (loweet al., 2015) considers wikipedia as dictionaryfeatures for normalizing the medical mentions.
taggerone (leaman and lu, 2016) is the semi-markov based model for jointly modeling medi-cal ner and nen.
transition-based model (louet al., 2017) consists of the state transformationfunction for the output of ner and nen..to reduce human feature engineering,re-searchers focus on the deep learning for model-ing ner and nen.
idcnn (strubell et al., 2017)was proposed with an improved cnn module forner.
mcnn (zhao et al., 2017) was composed ofthe multiple-label cnn modules for better perfor-mances on ner.
collabonet (yoon et al., 2019)exploited the multi-source datasets for training themulti-task model and gained better results on allbenchmark datasets.
mtl-mern (zhao et al.,2019) consists of the ner and nen parallel frame-work and utilizes the feedback strategy to improvethe performances on two tasks..with the impressive performance of pre-trainedmodels, biobert (lee et al., 2020) is built onthe bert (devlin et al., 2019) and trained witha large medical corpus.
and it achieves state-of-the-art results on medical ner datasets.
therefore,we use the biobert as the feature extractor andcompare it with our framework..4.3 experimental results.
we compare e2emern with the baseline methodson the named entity recognition and normaliza-tion.
the detailed experiment results on ncbiand bc5cdr are shown in table 2. the ﬁrst.
6219ncbi.
bc5cdr.
recognition normalization recognition normalization.
method.
dnorm (leaman et al., 2013)leadmine (lowe et al., 2015)taggerone (leaman and lu, 2016)transition-based model (lou et al., 2017)idcnn (strubell et al., 2017)mcnn (zhao et al., 2017)collabonet (yoon et al., 2019)mtl-mern (zhao et al., 2019)biobert (lee et al., 2020).
e2emernw/o mid-level taskw/o high-level taskw/o gate mechanismw/o attention mechanism.
0.7980-0.82900.82050.79830.85170.86360.87430.8971.
0.91510.87330.88620.88850.8767.
0.7820-0.80700.82620.7425--0.8823-.
0.89010.8890-0.82240.8675.
--0.82600.83820.80110.87830.88180.87630.9029.
0.91750.90730.90650.91000.9092.
0.80640.86120.83700.85620.8107--0.8645-.
0.89650.8600-0.86810.8676.table 2: the f1 scores of the models on ncbi and bc5cdr.
our model can outperform the baseline methods.
theresults for ablation study of e2emern are also presented.
“w/o gate mechanism” means that the gate mechanismis replaced with the simple feature concatenation strategy in the framework.
“w/o attention mechanism” is thesame as the above one..four in the table is the traditional machine learn-ing methods.
among them, the joint models, suchas taggerone and transition-based model, outper-form the pipeline ones including dnorm and lead-mine.
when deep learning was introduced into thepipeline frameworks, idcnn can make a progressover conventional methods, such as dnorm.
com-pared with mcnn, collabonet utilizes the multi-source dataset as input and performs multi-tasklearning to improve the performances on ner task.
mtl-mern takes full advantage of multi-tasklearning and deep semantic representations andoutperforms the above methods.
by virtue of thedynamic language features, biobert can bettermodel the language semantics and outperform theabove ner models..compared with baseline methods, e2emerncan always achieve the best results on ner andnen.
the ner results of e2emern increaseby 1% ∼ 2% over biobert.
because our frame-work takes full advantage of the correlation be-tween ner and nen.
unlike the simple strategyof mtl-mern, e2emern consists of three pro-gressive tasks that are well-designed for modelingthe ﬁne-grained features between medical mentionsin raw texts and standard entities.
the standardentity information of nen is introduced into thener module by the mechanisms in our framework.
with the help of the dynamic language featuresand progressive multi-task learning, the framework.
can extract the medical mentions more exactly andmap them to standard entities.
and the semanticcorrelation between medical mentions and standardentities is built on the three progressive tasks fromlow to high.
the rich semantics captured by theprogressive tasks are beneﬁcial to ner and nen..4.4 further discussion.
to dig into the framework, we conduct the detailedanalysis for presenting it in different aspects.
theablation study is conducted to present the effective-ness of the mechanisms proposed in the framework.
besides the supervised learning, our framework ex-ploits the standard entity information in the nertask and is potential in a zero-shot scenario com-pared with biobert.
we conduct the case studyto analyze the prediction results and visualize theattention mechanism to prove its effectiveness..4.4.1 ablation studyas shown in table 2, we conduct the ablation studyto present the effectiveness of the progressive tasksand different mechanisms.
when free from com-pleting the mid- or high-level tasks, e2emerngains worse results on ner and nen.
the pro-gressive tasks improves the ability of the frame-work to learn the multi-grained features betweenoriginal texts and standard entities.
besides, wereplace the gate and attention mechanisms withthe simple feature concatenation strategy as com-pared methods.
when removed the attention mech-.
6220text1:ground truth:biobert:.
theoo.vonb-diseaseb-disease.
hippeli-diseasei-disease.
-i-diseaseo.lindaui-diseaseo.tumori-diseaseo.suppressoroo.geneoo.requiredoo.foroo.celloo.cycleoo.exitoo.uponoo.o.b-disease.
i-disease.
i-disease.
i-disease.
i-disease.
o.o.o.o.o.o.o.o.e2emern:+ mesh:d006623.
e2emern:+ mesh:d006223.
o.
-oo.o.inoo.o.text2:ground truth:biobert:.
genotypeoo.phenotypeoo.analysesoo.cowdenb-diseaseb-disease.
diseasei-diseasei-disease.
andoo.bannayanb-diseaseo.
-i-diseaseo.zonanai-diseaseo.syndromei-diseaseo.twooo.hamartomab-diseaseo.syndromes withi-diseaseo.oo.germlineoo.pten mutationoo.oo.o.o.b-disease.
i-disease.
o.b-disease.
i-disease.
i-disease.
i-disease.
o.b-disease.
i-disease.
o.o.o.o.text3:ground truth:biobert:.
reasonsoo.foroo.seizuresb-diseaseb-disease.
wereoo.ruledoo.outoo.andoo.theoo.convulsionsoo.stoppedoo.few hoursoo.oo.afteroo.cessationoo.morphineb-chemicalb-chemical.
andoo.didoo.notoo.reoccuroo.
8 monthsoo.oo.serum withdrawal.
oo.o.oo.o.isoo.o.,oo.o.ofoo.o.o.o.b-disease.
o.o.o.o.o.o.o.o.o.o.o.b-chemical.
o.o.o.o.o.o.maleoo.spragueoo.dawleyoo.rats wereoo.oo.o.o.o.o.o.treated with.
betaineb-chemicalo.b-chemical.
(oo.o.
100oo.o.,oo.o.
200oo.o.,oo.o.andoo.
400 mgoooo.o.o.o.
/oo.o.kgoo.o.)
oo.o.orallyoo.o.oo.o.oo.o.
40oo.o.daysoo.o.e2emern:+ mesh:d009020.
text4:ground truth:biobert:.
e2emern:+ mesh:d001622.
inoo.o.foroo.o..
oo.o..
oo.o..
oo.o..
oo.o.table 3: the case study results on ncbi and bc5cdr.
“text1” and “text2” are from ncbi, and the other twoare from bd5cdr.
“text2” and “text4” are the unseen samples from the test set of two datasets.
the standardentities coupled with each text are the input of e2emern..anism, e2emern achieves worse results on twotasks.
it proves that the supervised signals frommid-level task are beneﬁcial to the low-task.
andthe entity-attention feature generated by the mecha-nism contributes to the high-level task.
e2emernwithout the gate mechanism gains the worse resultson nen.
because the mechanism aggregates thefeatures from lower level tasks which provides themulti-grained information between mentions andstandard entities.
the ablation study proves theimportance of the two mechanisms to e2emern..4.4.2 results on unseen sampleswe conduct the statistic analysis on the test set ofncbi and bc5cdr.
as shown in figure 4, thereare about 40% ∼ 50% samples contain the wordsor medial mentions which do not appear in thetraining set.
therefore, we need to evaluate the gen-eralization ability of models on the unseen samples.
we compare e2emern with biobert on the un-seen samples in the test set.
to a certain extent,our framework can outperform the existing state-of-the-art ner model.
compared with biobert,e2emern introduces the standard entity base intothe framework.
the ﬁne-grained location infor-mation of medical mentions from the high-leveltask is propagated to the low-level task.
with thehelp of standard entity information and progressivemulti-task learning, e2emern can gain the bettergeneralization ability on unseen samples..4.4.3 case studywe present the case study results in table 3. com-pared with biobert, our framework can extractthe medical mentions which biobert can not ex-tract.
we draw the label results of e2emern with.
figure 4: the results on unseen samples.
the left partis the proportions of seen and unseen samples in testsets.
the unseen samples mean that the words or med-ical mentions included of them do not appear in thetraining and development sets.
the right part is thener results of unseen samples in test sets..the heat map.
as the color deepens, the impor-tance of the token in the sentence increases.
thevisualization results prove that the attention mech-anism in e2emern focuses on the tokens whichmake of medical mentions.
although “text2” and“text4” are unseen samples, e2emern can alsoextract the mentions in them.
the token “convul-sions” is paid more attention than “seizures” in“text3”.
but convulsion is the symptom of seizures.
with the help of medical correlation between them,e2emern can extract the token “seizures” as med-ical mention.
to some extent, the effectiveness ofe2emern can be proved by the case study..6221seen47.1%unseen52.9%ncbiseen61.4%unseen38.6%bc5cdrncbibc5cdr0.00.10.20.30.40.50.60.70.80.91.0f10.45210.52840.76220.7092nerbioberte2emern5 conclusion.
in this paper, we reconsider the process of ner andnen and propose the end-to-end progressive multi-task learning framework for medical named entityrecognition and normalization.
compared with ex-isting methods, the framework consists of threetasks with progressive difﬁculty which contributesto modeling the ﬁne-grained features between med-ical mentions in raw texts and standard entities.
furthermore, the detailed analysis of e2emernproves its effectiveness.
considering the medicalarea is various, we will try to adapt the frameworkto the cross domain problem..acknowledgments.
we would like to thank three anonymous reviewersfor their insightful comments.
this research is sup-ported by the chinese scientiﬁc and technical in-novation project 2030 (2018aaa0102100), nsfc-general technology joint fund for basic research(no.
u1936206), nsfc-xinjiang joint fund (no.
u1903128), national natural science foundationof china (no.
62002178, no.
62077031), andnatural science foundation of tianjin, china (no.
20jcqnjc01730)..references.
gustavo aguilar, suraj maharjan, adrian pastor l´opez-monroy, and thamar solorio.
2017. a multi-task ap-proach for named entity recognition in social mediadata.
in proceedings of the 3rd workshop on noisyuser-generated text, pages 148–153, copenhagen,denmark.
association for computational linguis-tics..pengfei cao, yubo chen, kang liu, jun zhao, andshengping liu.
2018. adversarial transfer learn-ing for chinese named entity recognition with self-in proceedings of the 2018attention mechanism.
conference on empirical methods in natural lan-guage processing, pages 182–192, brussels, bel-gium.
association for computational linguistics..haolan chen, fred x. han, di niu, dong liu, kun-feng lai, chenglin wu, and yu xu.
2018. mix:multi-channel information crossing for text match-ing.
in proceedings of the 24th acm sigkdd in-ternational conference on knowledge discovery &data mining, pages 110–119..jacob devlin, ming-wei chang, kenton lee, andkristina toutanova.
2019. bert: pre-training ofdeep bidirectional transformers for language under-standing.
in proceedings of the 2019 conference ofthe north american chapter of the association for.
computational linguistics: human language tech-nologies, pages 4171–4186..rezarta islamaj dogan, robert leaman, and zhiyonglu.
2014. ncbi disease corpus: a resource fordisease name recognition and concept normalization.
journal of biomedical informatics, 47:1–10..sepp hochreiter and j¨urgen schmidhuber.
1997. longshort-term memory.
neural comput., 9(8):1735–1780..yanfei hong, benzheng wei, zhongyi han, xiangli, yuanjie zheng, and shuo li.
2020. mmcl-net:spinal disease diagnosis in global mode using pro-gressive multi-task joint learning.
neurocomputing,399:307–316..diederik p. kingma and jimmy ba.
2015. adam: amethod for stochastic optimization.
in 3rd iclr..canasai kruengkrai, thien hai nguyen, sharifah ma-hani aljunied, and lidong bing.
2020. improvinglow-resource named entity recognition using jointsentence and token labeling.
in proceedings of the58th annual meeting of the association for compu-tational linguistics, pages 5898–5905, online.
as-sociation for computational linguistics..john d. lafferty, andrew mccallum, and fernandoc. n. pereira.
2001. conditional random ﬁelds:probabilistic models for segmenting and labeling se-in proceedings of the eighteenth in-quence data.
ternational conference on machine learning, pages282–289..guillaume lample, miguel ballesteros, sandeep sub-ramanian, kazuya kawakami, and chris dyer.
2016.neural architectures for named entity recognition.
in proceedings of the 2016 conference of the northamerican chapter of the association for computa-tional linguistics: human language technologies,pages 260–270, san diego, california.
associationfor computational linguistics..andre lamurias and francisco m. couto.
2019.lasigebiotm at mediqa 2019: biomedical ques-tion answering using bidirectional transformers andin proceedings of thenamed entity recognition.
18th bionlp workshop on acl, pages 523–527..robert leaman, rezarta islamaj dogan, and zhiyonglu.
2013. dnorm: disease name normalization withpairwise learning to rank.
bioinform., 29(22):2909–2917..robert leaman and zhiyong lu.
2014. disease namedentity recognition and normalization with dnorm.
inproceedings of the 5th acm conference on bioin-formatics, computational biology, and health infor-matics, page 587..robert leaman and zhiyong lu.
2016..tag-gerone:joint named entity recognition and nor-malization with semi-markov models.
bioinform.,32(18):2839–2846..6222robert leaman, chih-hsuan wei, and zhiyong lu.
tmchem: a high performance approach for2015.chemical named entity recognition and normaliza-tion.
j. cheminformatics, 7(s-1):s3..jinhyuk lee, wonjin yoon,.
sungdong kim,donghyeon kim, sunkyu kim, chan ho so,and jaewoo kang.
2020. biobert: a pre-trainedbiomedicalforbiomedical text mining.
bioinform., 36(4):1234–1240..language representation model.
jiao li, yueping sun, robin j. johnson, daniela sci-aky, chih-hsuan wei, robert leaman, allan peterdavis, carolyn j. mattingly, thomas c. wiegers,and zhiyong lu.
2016. biocreative v cdr task cor-pus: a resource for chemical disease relation extrac-tion.
database..yinxia lou, yue zhang, tao qian, fei li, shufengxiong, and donghong ji.
2017. a transition-basedjoint model for disease named entity recognition andnormalization.
bioinform., 33(15):2363–2371..daniel m lowe, noel m o’boyle, and roger a sayle.
2015. leadmine: disease identiﬁcation and conceptin proceedings of themapping using wikipedia.
fifth biocreative challenge evaluation workshop,pages 240–246..tom´as mikolov, ilya sutskever, kai chen, gregory s.corrado, and jeffrey dean.
2013. distributed rep-resentations of words and phrases and their compo-in 27th annual conference on neuralsitionality.
information processing systems., pages 3111–3119..jeffrey pennington, richard socher, and christophermanning.
2014. glove: global vectors for wordrepresentation.
in proceedings of the 2014 confer-ence on empirical methods in natural languageprocessing (emnlp), pages 1532–1543, doha,qatar.
association for computational linguistics..stephen e. robertson, steve walker, susan jones,micheline hancock-beaulieu, and mike gatford.
in proceedings of1994.the third text retrieval conference, trec 1994,gaithersburg, maryland, usa, november 2-4, 1994,pages 109–126..okapi at trec-3..sunil sahu and ashish anand.
2016. recurrent neu-ral network models for disease name recognition us-ing domain invariant features.
in proceedings of the54th annual meeting of the association for compu-tational linguistics (volume 1: long papers), pages2216–2225, berlin, germany.
association for com-putational linguistics..emma strubell, patrick verga, david belanger, andandrew mccallum.
2017. fast and accurate en-tity recognition with iterated dilated convolutions.
in proceedings of the 2017 conference on empiri-cal methods in natural language processing, pages2670–2680, copenhagen, denmark.
association forcomputational linguistics..miguel v´azquez, monica chagoyen, and alberto d.pascual-montano.
2008. named entity recognitionand normalization: a domain-speciﬁc language ap-proach.
in 2nd international workshop on practicalapplications of computational biology and bioin-formatics, iwpacbb 2008, salamanca, spain, 22th-24th october 2008, pages 147–155..yu wang, yun li, ziye zhu, bin xia, and zheng liu.
2019.sc-ner: a sequence-to-sequence modelwith sentence classiﬁcation for named entity recog-in advances in knowledge discovery andnition.
data mining - 23rd paciﬁc-asia conference, pages198–209..yuting wu, xiao liu, yansong feng, zheng wang, ruiyan, and dongyan zhao.
2019. relation-aware en-tity alignment for heterogeneous knowledge graphs.
in proceedings of the 28th ijcai, pages 5278–5284..jinghui xiao, bingquan liu, and xiaolong wang.
2005. principles of non-stationary hidden markovmodel and its applications to sequence labeling task.
in natural language processing - ijcnlp 2005,second international joint conference, pages 827–837..ying xiong, yuanhang huang, qingcai chen, xiao-long wang, yuan nic, and buzhou tang.
2020. ajoint model for medical named entity recognitionin proceedings of the iberianand normalization.
languages evaluation forum (iberlef 2020) co-located with 36th conference of the spanish soci-ety for natural language processing (sepln 2020),pages 499–504..benfeng xu, licheng zhang, zhendong mao, quanwang, hongtao xie, and yongdong zhang.
2020.curriculum learning for naturallanguage under-standing.
in proceedings of the 58th annual meet-ing of the association for computational linguistics,pages 6095–6104, online.
association for computa-tional linguistics..zhiting xu, xian qian, yuejie zhang, and yaqian zhou.
2008. crf-based hybrid model for word segmenta-tion, ner and even pos tagging.
in third interna-tional joint conference on natural language pro-cessing, pages 167–170..wenpeng yin, hinrich sch¨utze, bing xiang, andbowen zhou.
2016. abcnn: attention-based con-volutional neural network for modeling sentencepairs.
transactions of the association for compu-tational linguistics, 4:259–272..wonjin yoon, chan ho so, jinhyuk lee, and jaewookang.
2019. collabonet: collaboration of deep neu-ral networks for biomedical named entity recogni-tion.
bmc bioinform., 20-s(10):55–65..sendong zhao, ting liu, sicheng zhao, and fei wang.
2019. a neural multi-task learning framework tojointly model medical named entity recognition andin proceedings of the 33th aaai,normalization.
pages 817–824..6223zhehuan zhao, zhihao yang, ling luo, lei wang, yinzhang, hongfei lin, and jian wang.
2017. diseasenamed entity recognition from biomedical literatureusing a novel convolutional neural network.
bmcmedical genomics, 10..huiwei zhou, shixian ning, zhe liu, chengkun lang,zhuang liu, and bizun lei.
2020. knowledge-enhanced biomedical named entity recognition andnormalization: application to proteins and genes.
bioinform., 21(1):35..peng zhou, wei shi, jun tian, zhenyu qi, bingchen li,hongwei hao, and bo xu.
2016. attention-basedbidirectional long short-term memory networks forin proceedings of the 54threlation classiﬁcation.
annual meeting of the association for computa-tional linguistics (volume 2: short papers), pages207–212, berlin, germany.
association for compu-tational linguistics..6224