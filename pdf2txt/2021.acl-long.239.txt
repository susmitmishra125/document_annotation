few-shot question answering by pretraining span selection.
ori ram∗,1 yuval kirstain∗,1.
jonathan berant1,2 amir globerson1 omer levy1.
blavatnik school of computer science, tel aviv university1allen institute for ai2{ori.ram,yuval.kirstain,joberant,gamir,levyomer}@cs.tau.ac.il.
abstract.
in several question answering benchmarks,pretrained models have reached human paritythrough ﬁne-tuning on an order of 100,000 an-notated questions and answers.
we explorethe more realistic few-shot setting, where onlya few hundred training examples are avail-able, and observe that standard models per-form poorly, highlighting the discrepancy be-tween current pretraining objectives and ques-tion answering.
we propose a new pretrain-ing scheme tailored for question answering: re-curring span selection.
given a passage withmultiple sets of recurring spans, we mask ineach set all recurring spans but one, and askthe model to select the correct span in the pas-sage for each masked span.
masked spansare replaced with a special token, viewed as aquestion representation, that is later used dur-ing ﬁne-tuning to select the answer span.
theresulting model obtains surprisingly good re-sults on multiple benchmarks (e.g., 72.7 f1on squad with only 128 training examples),while maintaining competitive performance inthe high-resource setting.1.
1.introduction.
the standard approach to question answering is topretrain a masked language model on raw text, andthen ﬁne-tune it with a span selection layer on top(devlin et al., 2019; joshi et al., 2020; liu et al.,2019).
while this approach is effective, and some-times exceeds human performance, its success isbased on the assumption that large quantities of an-notated question answering examples are available.
for instance, both squad (rajpurkar et al., 2016,2018) and natural questions (kwiatkowski et al.,2019) contain an order of 100,000 question and.
∗ equal contribution.
1our code, models, and datasets are publicly available:.
https://github.com/oriram/splinter..figure 1:performance of spanbert (red) androberta (yellow) base-size models on squad, givendifferent amounts of training examples.
our model(splinter, green) dramatically improves performance.
spanbert-base trained on the full training data ofsquad (blue, dashed) is shown for reference..answer pairs in their training data.
this assump-tion quickly becomes unrealistic as we venture out-side the lab conditions of english wikipedia, andattempt to crowdsource question-answer pairs inother languages or domains of expertise (tsatsaro-nis et al., 2015; kembhavi et al., 2017).
how doquestion answering models fare in the more practi-cal case, where an in-house annotation effort canonly produce a couple hundred training examples?.
we investigate the task of few-shot question an-swering by sampling small training sets from exist-ing question answering benchmarks.
despite theuse of pretrained models, the standard approachyields poor results when ﬁne-tuning on few exam-ples (figure 1).
for example, roberta-base ﬁne-tuned on 128 question-answer pairs from squadobtains around 40 f1.
this is somewhat expected,since the pretraining objective is quite differentfrom the ﬁne-tuning task.
while masked languagemodeling requires mainly local context around themasked token, question answering needs to alignthe question with the global context of the pas-.
proceedingsofthe59thannualmeetingoftheassociationforcomputationallinguisticsandthe11thinternationaljointconferenceonnaturallanguageprocessing,pages3066–3079august1–6,2021.©2021associationforcomputationallinguistics3066# examplesf10.020.040.060.080.0100.0163264128256spanbert (full data)robertaspanbertsplinter (ours)figure 2: an example paragraph before (a) and after (b) masking recurring spans.
each color represents a differentcluster of spans.
after masking recurring spans (replacing each with a single [question] token), only one spanfrom each cluster remains unmasked, and is considered the correct answer to the masked spans in the cluster.
thepretraining task is to predict the correct answer for each [question]..sage.
to bridge this gap, we propose (1) a novelself-supervised method for pretraining span selec-tion models, and (2) a question answering layerthat aligns a representation of the question with thetext..we introduce splinter (span-level pointer), apretrained model for few-shot question answering.
the challenge in deﬁning such a self-supervisedtask is how to create question-answer pairs fromunlabeled data.
our key observation is that onecan leverage recurring spans: n-grams, such asnamed entities, which tend to occur multiple timesin a given passage (e.g., “roosevelt” in figure 2).
we emulate question answering by masking all butone instance of each recurring span with a special[question] token, and asking the model to se-lect the correct span for each such token..to select an answer span for each [question]token in parallel, we introduce a question-awarespan selection (qass) layer, which uses the[question] token’s representation to select theanswer span.
the qass layer seamlessly inte-grates with ﬁne-tuning on real question-answerpairs.
we simply append the [question] to-ken to the input question, and use the qass layerto select the answer span (figure 3).
this is unlikeexisting models for span selection, which do notinclude an explicit question representation.
thecompatibility between pretraining and ﬁne-tuningmakes splinter an effective few-shot learner..splinter exhibits surprisingly high performancegiven only a few training examples throughout a va-.
riety of benchmarks from the mrqa 2019 sharedtask (fisch et al., 2019).
for example, splinter-baseachieves 72.7 f1 on squad with only 128 exam-ples, outperforming all baselines by a very widemargin.
an ablation study shows that the pretrain-ing method and the qass layer itself (even withoutpretraining) both contribute to improved perfor-mance.
analysis indicates that splinter’s represen-tations change signiﬁcantly less during ﬁne-tuningcompared to the baselines, suggesting that our pre-training is more adequate for question answering.
overall, our results highlight the importance of de-signing objectives and architectures in the few-shotsetting, where an appropriate inductive bias canlead to dramatic performance improvements..2 background.
extractive question answering is a common task innlp, where the goal is to select a contiguous spana from a given text t that answers a question q.this format was popularized by squad (rajpurkaret al., 2016), and has since been adopted by severaldatasets in various domains (trischler et al., 2017;kembhavi et al., 2017) and languages (lewis et al.,2020; clark et al., 2020), with some extensionsallowing for unanswerable questions (levy et al.,2017; rajpurkar et al., 2018) or multiple answerspans (dua et al., 2019; dasigi et al., 2019).
inthis work, we follow the assumptions in the recentmrqa 2019 shared task (fisch et al., 2019) andfocus on questions whose answer is a single span.
the standard approach uses a pretrained encoder,.
3067figure 3: an example of our ﬁne-tuning setup, taken from the development set of squad.
the question, followedby the [question] token, is concatenated to the context.
the [question] token’s representation is then usedto select the answer span..such as bert (devlin et al., 2019), and adds twoparameter vectors s, e to the pretrained model inorder to detect the start position s and end positione of the answer span a, respectively.
the inputtext t and question q are concatenated and fedinto the encoder, producing a contextualized tokenrepresentation xi for each token in the sequence.
to predict the start position of the answer span,a probability distribution is induced over the en-tire sequence by computing the inner product ofa learned vector s with every token representation(the end position is computed similarly using avector e):.
p (s = i | t, q) =.
p (e = i | t, q) =.
j s).
exp(x(cid:62)i s)j exp(x(cid:62)exp(x(cid:62)i e)j exp(x(cid:62).
(cid:80).
(cid:80).
j e).
,.
..the parameters s, e are trained during ﬁne-tuning,using the cross-entropy loss with the start and endpositions of the gold answer span..this approach assumes that each token repre-sentation xi is contextualized with respect to thequestion.
however, the masked language model-ing objective does not necessarily encourage thisform of long-range contextualization in the pre-trained model, since many of the masked tokenscan be resolved from local cues.
fine-tuning theattention patterns of pretrained masked languagemodels may thus entail an extensive learning effort,difﬁcult to achieve with only a handful of trainingexamples.
we overcome this issue by (1) pretrain-ing directly for span selection, and (2) explicitlyrepresenting the question with a single vector, usedto detect the answer in the input text..3 splinter.
we formulate a new task for pretraining ques-tion answering from unlabeled text: recurringspan selection.
we replace spans that appearmultiple times in the given text with a special[question] token, except for one occurrence,which acts as the “answer” span for each (masked)cloze-style “question”.
the prediction layer is amodiﬁcation of the standard span selection layer,which replaces the static start and end parame-ter vectors, s and e, with dynamically-computedboundary detectors based on the contextualized rep-resentation of each [question] token.
we reusethis architecture when ﬁne-tuning on question-answer pairs by adding a [question] token atthe end of the actual question, thus aligning thepretraining objective with the ﬁne-tuning task.
werefer to our pretrained model as splinter..3.1 pretraining: recurring span selection.
given an input text t , we ﬁnd all recurring spans:arbitrary n-grams that appear more than once in thesame text.
for each set of identical recurring spansr, we select a single occurrence as the answera and replace all other occurrences with a single[question] token.2 the goal of recurring spanselection is to predict the correct answer a for agiven [question] token q ∈ r \ {a}, each qthus acting as an independent cloze-style question.
figure 2 illustrates this process.
in the given pas-sage, the span “roosevelt” appears three times.
two of its instances (the second and third) arereplaced with [question], while one instance(the ﬁrst) becomes the answer, and remains intact.
after masking, the sequence is passed through atransformer encoder, producing contextualized to-.
2in practice, only some sets of recurring spans are pro-.
cessed; see cluster selection below..3068ken representations.
the model is then tasked withpredicting the start and end positions of the answergiven each [question] token representation.
infigure 2b, we observe four instances of this predic-tion task: two for the “roosevelt” cluster, one forthe “allied countries” cluster, and one for “decla-ration by united nations”..taking advantage of recurring words in a pas-sage (restricted to nouns or named entities) wasproposed in past work as a signal for coreference(kocijan et al., 2019; ye et al., 2020).
we furtherdiscuss this connection in section 7..span filtering to focus pretraining on semanti-cally meaningful spans, we use the following deﬁ-nition for “spans”, which ﬁlters out recurring spansthat are likely to be uninformative: (1) spans mustbegin and end at word boundaries, (2) we consideronly maximal recurring spans, (3) spans containingonly stop words are ignored, (4) spans are limitedto a maximum of 10 tokens.
these simple heuristicﬁlters do not require a model, as opposed to mask-ing schemes in related work (glass et al., 2020; yeet al., 2020; guu et al., 2020), which require part-of-speech taggers, constituency parsers, or namedentity recognizers..cluster selection we mask a random subset ofrecurring span clusters in each text, leaving somerecurring spans untouched.
speciﬁcally, we replaceup to 30 spans with [question] from each inputpassage.3 this number was chosen to resemblethe 15% token-masking ratio of joshi et al.
(2020).
note that in our case, the number of masked tokensis greater than the number of questions..3.2 model: question-aware span selection.
our approach converts texts into a set of questionsthat need to be answered simultaneously.
the stan-dard approach for extractive question answering(devlin et al., 2019) is inapplicable, because it usesﬁxed start and end vectors.
since we have multiplequestions, we replace the standard parameter vec-tors s, e with dynamic start and end vectors sq, eq,computed from each [question] token q:.
sq = sxq.
eq = exq.
here, s, e are parameter matrices, which extractad hoc start and end position detectors sq, eq fromthe given [question] token’s representation xq..3in some cases, the last cluster may have more than one.
unmasked span..the rest of our model follows the standard spanselection model by computing the start and endposition probability distributions.
the model canalso be viewed as two bilinear functions of thequestion representation xq with each token in thesequence xi, similar to dozat and manning (2017):.
p (s = i | t, q) =.
p (e = i | t, q) =.
i sxq).
j sxq).
i exq).
exp(x(cid:62)j exp(x(cid:62)exp(x(cid:62)j exp(x(cid:62).
(cid:80).
(cid:80).
j exq).
finally, we use the answer’s gold start and endpoints (sa, ea) to compute the cross-entropy loss:.
− log p (s = sa | t, q) − log p (e = ea | t, q).
we refer to this architecture as the question-awarespan selection (qass) layer..3.3 fine-tuning.
after pretraining, we assume access to labeled ex-amples, where each training instance is a text t ,a question q, and an answer a that is a span int .
to make this setting similar to pretraining, wesimply append a [question] token to the inputsequence, immediately after the question q (seefigure 3).
selecting the answer span then proceedsexactly as during pretraining.
indeed, the advan-tage of our approach is that in both pretraining andﬁne-tuning, the [question] token representa-tion captures information about the question that isthen used to select the span from context..4 a few-shot qa benchmark.
to evaluate how pretrained models work when onlya small amount of labeled data is available for ﬁne-tuning, we simulate various low-data scenarios bysampling subsets of training examples from largerdatasets.
we use a subset of the mrqa 2019shared task (fisch et al., 2019), which containsextractive question answering datasets in a uniﬁedformat, where the answer is a single span in thegiven text passage..split i of the mrqa shared task contains 6 largequestion answering datasets: squad (rajpurkaret al., 2016), newsqa (trischler et al., 2017), triv-iaqa (joshi et al., 2017), searchqa (dunn et al.,2017), hotpotqa (yang et al., 2018), and naturalquestions (kwiatkowski et al., 2019).
for eachdataset, we sample smaller training datasets fromthe original training set with sizes changing on a.
3069logarithmic scale, from 16 to 1,024 examples.
toreduce variance, for each training set size, we sam-ple 5 training sets using different random seeds andreport average performance across training sets.
we also experiment with ﬁne-tuning the models onthe full training sets.
since split i of the mrqashared task does not contain test sets, we evaluateusing the ofﬁcial development sets as our test sets.
we also select two datasets from split ii of themrqa shared task that were annotated by do-main experts: bioasq (tsatsaronis et al., 2015)and textbookqa (kembhavi et al., 2017).
eachof these datasets only has a development set thatis publicly available in mrqa, containing about1,500 examples.
for each dataset, we sample 400examples for evaluation (test set), and follow thesame protocol we used for large datasets to sam-ple training sets of 16 to 1,024 examples from theremaining data..to maintain the few-shot setting, every datasetin our benchmark has well-deﬁned training and testsets.
to tune hyperparameters, one needs to extractvalidation data from each training set.
for simplic-ity, we do not perform hyperparameter tuning ormodel selection (see section 5), and thus use all ofthe available few-shot data for training..token from the representations of the unmaskedtokens at the start and end of the masked span..spanbert (reimpl) our reimplementation ofspanbert, using exactly the same code, data, andhyperparameters as splinter.
this baseline aimsto control for implementation differences and mea-sures the effect of replacing masked language mod-eling with recurring span selection.
also, this ver-sion does not use the span boundary objective, asjoshi et al.
(2020) reported no signiﬁcant improve-ments from using it in question answering..5.2 pretraining implementation.
we train splinter-base using adam (kingma andba, 2015) for 2.4m training steps with batches of256 sequences of length 512.4 the learning rate iswarmed up for 10k steps to a maximum value of10−4, after which it decays linearly.
as in previouswork, we use a dropout rate of 0.1 across all layers.
we follow devlin et al.
(2019) and train on en-glish wikipedia (preprocessed by wikiextractoras in attardi (2015)) and the toronto bookcorpus(zhu et al., 2015).
we base our implementation onthe ofﬁcial tensorflow implementation of bert,and train on a single eight-core v3 tpu (v3-8) onthe google cloud platform..5 experimental setup.
5.3 fine-tuning implementation.
we describe our experimental setup in detail, in-cluding all models and baselines..5.1 baselines.
splinter-base shares the same architecture (trans-former encoder (vaswani et al., 2017)), vocabu-lary (cased wordpieces), and number of parameters(110m) with spanbert-base (joshi et al., 2020).
in all experiments, we compare splinter-base tothree baselines of the same capacity:.
roberta (liu et al., 2019) a highly-tuned andoptimized version of bert, which is known toperform well on a wide range of natural languageunderstanding tasks..for ﬁne-tuning, we use the hyperparameters fromthe default conﬁguration of the huggingface trans-formers package (wolf et al., 2020).5 speciﬁcally,we train all models using adam (kingma and ba,2015) with bias-corrected moment estimates forfew-shot learning (zhang et al., 2021).
when ﬁne-tuning on 1024 examples or less, we train for either10 epochs or 200 steps (whichever is larger).
forfull-size datasets, we train for 2 epochs.
we set thebatch size to 12 and use a maximal learning rate of3 · 10−5, which warms up in the ﬁrst 10% of thesteps, and then decays linearly..an interesting question is how to ﬁne-tune theqass layer parameters (i.e., the s and e matri-ces in section 3.2).
in our implementation, wechose to discard the pretrained values and ﬁne-tune.
spanbert (joshi et al., 2020) a bert-stylemodel that focuses on span representations.
span-bert is trained by masking contiguous spans oftokens and optimizing two objectives: (a) maskedlanguage modeling, which predicts each masked to-ken from its own vector representation; (b) the spanboundary objective, which predicts each masked.
4we used this setting to approximate spanbert’s hyperpa-rameter setting in terms of epochs.
that said, spanbert-basewas trained for a quarter of the steps (600k steps) using fourtimes as many examples per batch (1024 sequences).
see sec-tion 5.1 for additional baselines that control for this difference.
5we did rudimentary tuning on the number of steps only,using a held-out portion of the squad training set, sinceour training sets can be too small for the default values (e.g.,running 10 epochs on 16 examples results in 20 update steps)..3070figure 4: performance (f1) of splinter-base (green), compared to all baselines as a function of the number oftraining examples on two datasets.
each point reﬂects the average performance across 5 randomly-sampled trainingsets of the same size..from a random initialization, due to the possiblediscrepancy between span statistics in pretrainingand ﬁne-tuning datasets.
however, we report re-sults on ﬁne-tuning without resetting the qassparameters as an ablation study (section 6.3)..6 results.
our experiments show that splinter dramaticallyimproves performance in the challenging few-shotsetting, unlocking the ability to train question an-swering models with only hundreds of examples.
when trained on large datasets with an order of100,000 examples, splinter is competitive with(and often better than) the baselines.
ablation stud-ies demonstrate the contributions of both recurringspan selection pretraining and the qass layer..6.1 few-shot learning.
figure 4 shows the f1 score (rajpurkar et al., 2016)of splinter-base, plotted against all baselines fortwo datasets, triviaqa and textbookqa, as a func-tion of the number of training examples (see fig-ure 6 in the appendix for the remaining datasets).
inaddition, table 1 shows the performance of individ-ual models when given 16, 128, and 1024 trainingexamples across all datasets (see table 3 in theappendix for additional performance and standarddeviation statistics).
it is evident that splinter out-performs all baselines by large margins..let us examine the results on squad, for exam-ple.
given 16 training examples, splinter obtains54.6 f1, signiﬁcantly higher than the best base-line’s 18.2 f1.
when the number of training exam-ples is 128, splinter achieves 72.7 f1, outperform-ing the baselines by 17 points (our reimplementa-tion of spanbert) to 30 (roberta).
when con-sidering 1024 examples, there is a 5-point margin.
between splinter (82.8 f1) and spanbert (77.8f1).
the same trend is seen in the other datasets,whether they are in-domain sampled from largerdatasets (e.g.
triviaqa) or not; in textbookqa,for instance, we observe absolute gaps of 9 to 23f1 between splinter and the next-best baseline..6.2 high-resource regime.
table 1 also shows the performance when ﬁne-tuning on the entire training set, when an orderof 100,000 examples are available.
even thoughsplinter was designed for few-shot question an-swering, it reaches the best result in ﬁve out of sixdatasets.
this result suggests that when the targettask is extractive question answering, it is better topretrain with our recurring span selection task thanwith masked langauge modeling, regardless of thenumber of annotated training examples..6.3 ablation study.
we perform an ablation study to better understandthe independent contributions of the pretrainingscheme and the qass layer.
we ﬁrst ablate theeffect of pretraining on recurring span selection byapplying the qass layer to pretrained masked lan-guage models.
we then test whether the qasslayer’s pretrained parameters can be reused insplinter during ﬁne-tuning without reinitializion..independent contribution of the qass layerwhile the qass layer is motivated by our pretrain-ing scheme, it can also be used without pretraining.
we apply a randomly-initialized qass layer to ourimplementation of spanbert, and ﬁne-tune it inthe few-shot setting.
figure 5 shows the results ofthis ablation study for two datasets (see figure 7in the appendix for more datasets).
we observe.
3071# examplesf10.020.040.060.080.01632641282565121024robertaspanbertspanbert (reimpl)splintertriviaqa# examplesf10.020.040.060.080.01632641282565121024robertaspanbertspanbert (reimpl)splintertextbookqasquad triviaqa.
nq newsqa searchqa hotpotqa bioasq textbookqa.
model.
16 examples.
robertaspanbertspanbert (reimpl)splinter.
128 examples.
robertaspanbertspanbert (reimpl)splinter.
1024 examples.
robertaspanbertspanbert (reimpl)splinter.
full dataset.
robertaspanbertspanbert (reimpl)splinter.
7.712.518.254.6.
43.048.555.872.7.
73.877.877.882.8.
90.392.092.092.2.
7.512.811.618.9.
19.124.226.344.7.
46.850.355.564.8.
74.077.275.876.5.
17.319.719.627.4.
30.132.236.046.3.
54.257.559.565.5.
79.680.680.581.0.
1.46.07.620.8.
16.717.429.543.5.
47.549.352.257.3.
69.871.371.171.3.
6.913.013.326.3.
27.834.326.347.2.
54.360.158.967.3.
81.580.181.483.0.
10.512.612.524.0.
27.335.136.654.7.
61.867.464.670.3.
78.779.679.780.7.
16.722.015.928.2.
46.155.352.263.2.
84.189.389.091.0.
----.
3.35.67.519.4.
8.29.420.942.6.
35.842.345.754.5.
----.
table 1: performance (f1) across all datasets when the number of training examples is 16, 128, and 1024. wealso show performance when training on the full-sized large datasets (mrqa version).
all models have the samecapacity to bert-base (110m parameters).
nq stands for natural questions..that replacing the static span selection layer withqass can signiﬁcantly improve performance onfew-shot question answering.
having said that,most of splinter’s improvements in the extremelylow data regime do stem from combining the qasslayer with our pretraining scheme, and this com-bination still outperforms all other variants as theamount of data grows..qass reinitialization between pretraining andﬁne-tuning, we randomly reinitialize the parame-ters of the qass layer.
we now test the effectof ﬁne-tuning with the qass layer’s pretrainedparameters; intuitively, the more similar the pre-training data is to the task, the better the pretrainedlayer will perform.
figure 5 shows that the ad-vantage of reusing the pretrained qass is data-dependent, and can result in both performancegains (e.g.
extremely low data in squad) and stag-nation (e.g.
bioasq with 256 examples or more).
other datasets exhibit similar trends (see appendix).
we identify three conditions that determine whetherkeeping the pretrained head is preferable: (1) whenthe number of training examples is extremely low,(2) when the target domain is similar to that usedat pretraining (e.g.
wikipedia), and (3) when thequestions are relatively simple (e.g.
squad versushotpotqa).
the latter two conditions pertain to the.
model.
representation similarity.
robertaspanbertspanbert (reimpl)splinter.
0.290.230.190.89.table 2: cosine similarity of the representations pro-duced by the transformer encoder before and after ﬁne-tuning on 128 squad examples..compatibility between pretraining and ﬁne-tuningtasks; the information learned in the qass layer isuseful as long as the input and output distributionof the task are close to those seen at pretrainingtime..6.4 analysis.
the recurring span selection objective was de-signed to emulate extractive question answeringusing unlabeled text.
how similar is it to the actualtarget task?
to answer this question, we measurehow much each pretrained model’s functionalityhas changed after ﬁne-tuning on 128 examples ofsquad.
for the purpose of this analysis, we mea-sure change in functionality by examining the vec-tor representation of each token as produced by thetransformer encoder; speciﬁcally, we measure thecosine similarity between the vector produced by.
3072figure 5: ablation studies on squad and bioasq datasets.
we examine the role of the qass layer by ﬁne-tuningit on top of our reimplementation of spanbert.
in addition, we test whether it is beneﬁcial to keep the pretrainedparameters of the qass layer when ﬁne-tuning splinter..the pretrained model and the one produced by theﬁne-tuned model, given exactly the same input.
weaverage these similarities across every token of 200examples from squad’s test set..table 2 shows that splinter’s outputs are verysimilar before and after ﬁne-tuning (0.89 aver-age cosine similarity), while the other models’representations seem to change drastically.
thissuggests that ﬁne-tuning with even 128 question-answering examples makes signiﬁcant modiﬁca-tions to the functionality of pretrained masked lan-guage models.
splinter’s pretraining, on the otherhand, is much more similar to the ﬁne-tuning task,resulting in much more modest changes to the pro-duced vector representations..7 related work.
the remarkable results of gpt-3 (brown et al.,2020) have inspired a renewed interest in few-shotlearning.
while some work focuses on classiﬁca-tion tasks (schick and sch¨utze, 2020; gao et al.,2021), our work investigates few-shot learning inthe context of extractive question answering..one approach to this problem is to create syn-thetic text-question-answer examples.
both lewiset al.
(2019) and glass et al.
(2020) use the tra-ditional nlp pipeline to select noun phrases andnamed entities in wikipedia paragraphs as potentialanswers, which are then masked from the contextto create pseudo-questions.
lewis et al.
(2019) usemethods from unsupervised machine translationto translate the pseudo-questions into real ones,while glass et al.
(2020) keep the pseudo-questionsbut use information retrieval to ﬁnd new text pas-sages that can answer them.
both works assumeaccess to language- and domain-speciﬁc nlp toolssuch as part-of-speech taggers, syntactic parsers,.
and named-entity recognizers, which might not al-ways be available.
our work deviates from thisapproach by exploiting the natural phenomenonof recurring spans in order to generate multiplequestion-answer pairs per text passage, without as-suming any language- or domain-speciﬁc modelsor resources are available beyond plain text..similar ideas to recurring span selection wereused for creating synthetic coreference resolutionexamples (kocijan et al., 2019; varkel and glober-son, 2020), which mask single words that occurmultiple times in the same context.
corefbert(ye et al., 2020) combines this approach with acopy mechanism for predicting the masked wordduring pretraining, alongside the masked languagemodeling objective.
unlike our approach, whichwas designed to align well with span selection,corefbert masks only single-word nouns (ratherthan arbitrary spans) and replaces each token in theword with a separate mask token (rather than a sin-gle mask for the entire multi-token word).
there-fore, it does not emulate extractive question an-swering.
we did not add corefbert as a baselinesince the performance of both corefbert-baseand corefbert-large was lower than spanbert-base’s performance on the full-data mrqa bench-mark, and pretraining corefbert from scratchwas beyond our available computational resources..8 conclusion.
we explore the few-shot setting of extractive ques-tion answering, and demonstrate that existing meth-ods, based on ﬁne-tuning large pretrained languagemodels, fail in this setup.
we propose a new pre-training scheme and architecture for span selectionthat lead to dramatic improvements, reaching sur-prisingly good results even when only an order of.
3073# examplesf10.020.040.060.080.0100.01632641282565121024spanbert (reimpl)spanbert (reimpl) + qasssplintersplinter (with pretrained qass)squad# examplesf10.020.040.060.080.0100.01632641282565121024spanbert (reimpl)spanbert (reimpl) + qasssplintersplinter (with pretrained qass)bioasqa hundred examples are available.
our work showsthat choices that are often deemed unimportantwhen enough data is available, again become cru-cial in the few-shot setting, opening the door to newmethods that take advantage of prior knowledge onthe downstream task during model development..acknowledgements.
this project was funded by the european re-search council (erc) under the european unionshorizon 2020 research and innovation programme(grant erc holi 819080), the blavatnik fund,the alon scholarship, the yandex initiative for ma-chine learning and intel corporation.
we thankgoogle’s tpu research cloud (trc) for their sup-port in providing tpus for this research..references.
giusepppe attardi.
2015. wikiextractor..tom b. brown, benjamin mann, nick ryder, melaniesubbiah, jared kaplan, prafulla dhariwal, arvindneelakantan, pranav shyam, girish sastry, amandaaskell, sandhini agarwal, ariel herbert-voss,gretchen krueger, tom henighan, rewon child,aditya ramesh, daniel m. ziegler, jeffrey wu,clemens winter, christopher hesse, mark chen,eric sigler, mateusz litwin, scott gray, benjaminchess, jack clark, christopher berner, sam mc-candlish, alec radford, ilya sutskever, and darioamodei.
2020. language models are few-shot learn-ers.
in advances in neural information processingsystems..jonathan h. clark, eunsol choi, michael collins, dangarrette, tom kwiatkowski, vitaly nikolaev, andjennimaria palomaki.
2020. tydi qa: a bench-mark for information-seeking question answering intypologically diverse languages.
transactions of theassociation for computational linguistics, 8:454–470..pradeep dasigi, nelson f. liu, ana marasovi´c,noah a. smith, and matt gardner.
2019. quoref:a reading comprehension dataset with questions re-in proceedings ofquiring coreferential reasoning.
the 2019 conference on empirical methods in nat-ural language processing and the 9th internationaljoint conference on natural language processing(emnlp-ijcnlp), pages 5925–5932, hong kong,china.
association for computational linguistics..jacob devlin, ming-wei chang, kenton lee, andkristina toutanova.
2019. bert: pre-training ofdeep bidirectional transformers for language under-in proceedings of the 2019 conferencestanding.
of the north american chapter of the associationfor computational linguistics: human languagetechnologies, volume 1 (long and short papers),.
pages 4171–4186, minneapolis, minnesota.
associ-ation for computational linguistics..timothy dozat and christopher d. manning.
2017.deep biafﬁne attention for neural dependency pars-ing.
in iclr 2017..dheeru dua, yizhong wang, pradeep dasigi, gabrielstanovsky, sameer singh, and matt gardner.
2019.drop: a reading comprehension benchmark requir-ing discrete reasoning over paragraphs.
in proceed-ings of the 2019 conference of the north americanchapter of the association for computational lin-guistics: human language technologies, volume 1(long and short papers), pages 2368–2378, min-neapolis, minnesota.
association for computationallinguistics..matthew dunn, levent sagun, mike higgins, v. ugurguney, volkan cirik, and kyunghyun cho.
2017.searchqa: a new q&a dataset augmented withcontext from a search engine..adam fisch, alon talmor, robin jia, minjoon seo, eu-nsol choi, and danqi chen.
2019. mrqa 2019shared task: evaluating generalization in readingin proceedings of the 2nd work-comprehension.
shop on machine reading for question answering,pages 1–13, hong kong, china.
association forcomputational linguistics..tianyu gao, adam fisch, and danqi chen.
2021.making pre-trained language models better few-shotlearners.
in association for computational linguis-tics (acl)..michael glass, alﬁo gliozzo, rishav chakravarti, an-thony ferritto, lin pan, g p shrivatsa bhargav, di-nesh garg, and avi sil.
2020. span selection pre-in proceedingstraining for question answering.
of the 58th annual meeting of the association forcomputational linguistics, pages 2773–2782, on-line.
association for computational linguistics..kelvin guu, kenton lee, zora tung, panupong pa-supat, and mingwei chang.
2020. retrieval aug-in proceed-mented language model pre-training.
ings of the 37th international conference on ma-chine learning, volume 119 of proceedings of ma-chine learning research, pages 3929–3938.
pmlr..mandar joshi, danqi chen, yinhan liu, daniel s.weld, luke zettlemoyer, and omer levy.
2020.spanbert: improving pre-training by representingand predicting spans.
transactions of the associa-tion for computational linguistics, 8:64–77..mandar joshi, eunsol choi, daniel weld, and lukezettlemoyer.
2017. triviaqa: a large scale dis-tantly supervised challenge dataset for reading com-prehension.
in proceedings of the 55th annual meet-ing of the association for computational linguistics(volume 1: long papers), pages 1601–1611, van-couver, canada.
association for computational lin-guistics..3074aniruddha kembhavi, minjoon seo, dustin schwenk,jonghyun choi, ali farhadi, and hannaneh ha-jishirzi.
2017. are you smarter than a sixth grader?
textbook question answering for multimodal ma-chine comprehension.
in proceedings of the ieeeconference on computer vision and pattern recog-nition (cvpr)..diederik p. kingma and jimmy ba.
2015. adam: amethod for stochastic optimization.
in iclr 2015..vid kocijan, oana-maria camburu, ana-maria cretu,yordan yordanov, phil blunsom, and thomaslukasiewicz.
2019. wikicrem: a large unsuper-vised corpus for coreference resolution.
in proceed-ings of the 2019 conference on empirical methodsin natural language processing and the 9th inter-national joint conference on natural language pro-cessing (emnlp-ijcnlp), pages 4303–4312, hongkong, china.
association for computational lin-guistics..tom kwiatkowski, jennimaria palomaki, olivia red-ﬁeld, michael collins, ankur parikh, chris al-berti, danielle epstein, illia polosukhin, jacob de-vlin, kenton lee, kristina toutanova, llion jones,matthew kelcey, ming-wei chang, andrew m. dai,jakob uszkoreit, quoc le, and slav petrov.
2019.natural questions: a benchmark for question an-swering research.
transactions of the associationfor computational linguistics, 7:453–466..omer levy, minjoon seo, eunsol choi, and lukezettlemoyer.
2017. zero-shot relation extraction viareading comprehension.
in proceedings of the 21stconference on computational natural languagelearning (conll 2017), pages 333–342, vancou-ver, canada.
association for computational linguis-tics..patrick lewis, ludovic denoyer, and sebastian riedel.
2019. unsupervised question answering by clozetranslation.
in proceedings of the 57th annual meet-ing of the association for computational linguistics,pages 4896–4910, florence, italy.
association forcomputational linguistics..patrick lewis, barlas oguz, ruty rinott, sebastianriedel, and holger schwenk.
2020. mlqa: evalu-ating cross-lingual extractive question answering.
inproceedings of the 58th annual meeting of the asso-ciation for computational linguistics, pages 7315–7330, online.
association for computational lin-guistics..yinhan liu, myle ott, naman goyal, jingfei du, man-dar joshi, danqi chen, omer levy, mike lewis,luke zettlemoyer, and veselin stoyanov.
2019.roberta: a robustly optimized bert pretraining ap-proach..pranav rajpurkar, robin jia, and percy liang.
2018.know what you don’t know: unanswerable ques-in proceedings of the 56th an-tions for squad.
nual meeting of the association for computational.
linguistics (volume 2: short papers), pages 784–789, melbourne, australia.
association for compu-tational linguistics..pranav rajpurkar, jian zhang, konstantin lopyrev, andpercy liang.
2016. squad: 100,000+ questions formachine comprehension of text.
in proceedings ofthe 2016 conference on empirical methods in natu-ral language processing, pages 2383–2392, austin,texas.
association for computational linguistics..timo schick and hinrich sch¨utze.
2020. exploitingcloze questions for few shot text classiﬁcation andnatural language inference..adam trischler, tong wang, xingdi yuan, justin har-ris, alessandro sordoni, philip bachman, and ka-heer suleman.
2017. newsqa: a machine compre-in proceedings of the 2nd work-hension dataset.
shop on representation learning for nlp, pages191–200, vancouver, canada.
association for com-putational linguistics..george tsatsaronis, georgios balikas, prodromosmalakasiotis, ioannis partalas, matthias zschunke,michael r. alvers, dirk weissenborn, anastasiakrithara, sergios petridis, dimitris polychronopou-los, yannis almirantis, john pavlopoulos, nico-las baskiotis, patrick gallinari, thierry arti´eres,axel-cyrille ngonga ngomo, norman heino, ericgaussier, liliana barrio-alvers, michael schroeder,ion androutsopoulos, and georgios paliouras.
2015.an overview of the bioasq large-scale biomedicalsemantic indexing and question answering competi-tion.
bmc bioinformatics, 16(1):138..yuval varkel and amir globerson.
2020. pre-trainingmention representations in coreference models.
inproceedings of the 2020 conference on empiricalmethods in natural language processing (emnlp),pages 8534–8540, online.
association for computa-tional linguistics..ashish vaswani, noam shazeer, niki parmar, jakobuszkoreit, llion jones, aidan n gomez, ł ukaszkaiser, and illia polosukhin.
2017. attention is allyou need.
in advances in neural information pro-cessing systems 30, pages 5998–6008..thomas wolf, lysandre debut, victor sanh, julienchaumond, clement delangue, anthony moi, pier-ric cistac, tim rault, r´emi louf, morgan funtow-icz, joe davison, sam shleifer, patrick von platen,clara ma, yacine jernite, julien plu, canwen xu,teven le scao, sylvain gugger, mariama drame,quentin lhoest, and alexander m. rush.
2020.transformers: state-of-the-art natural language pro-cessing.
in proceedings of the 2020 conference onempirical methods in natural language processing:system demonstrations, pages 38–45, online.
asso-ciation for computational linguistics..zhilin yang, peng qi, saizheng zhang, yoshua bengio,william cohen, ruslan salakhutdinov, and christo-pher d. manning.
2018. hotpotqa: a dataset.
3075for diverse, explainable multi-hop question answer-ing.
in proceedings of the 2018 conference on em-pirical methods in natural language processing,pages 2369–2380, brussels, belgium.
associationfor computational linguistics..deming ye, yankai lin, jiaju du, zhenghao liu, pengli, maosong sun, and zhiyuan liu.
2020. corefer-ential reasoning learning for language represen-in proceedings of the 2020 conference ontation.
empirical methods in natural language process-ing (emnlp), pages 7170–7186, online.
associa-tion for computational linguistics..tianyi zhang, felix wu, arzoo katiyar, kilian q.weinberger, and yoav artzi.
2021. revisiting few-sample bert ﬁne-tuning.
in iclr..yukun zhu, ryan kiros, rich zemel, ruslan salakhut-dinov, raquel urtasun, antonio torralba, and sanjafidler.
2015. aligning books and movies: towardsstory-like visual explanations by watching moviesand reading books.
in the ieee international con-ference on computer vision (iccv)..a additional results.
few-shot results figure 6 shows the results onthe six few-shot question answering datasets not in-cluded in figure 4. in addition, we give the full rawresults (including standard deviation) in table 3..ablation studies figure 7 shows results of abla-tion studies on the six question answering datasetsnot included in figure 5..3076figure 6: results complementary to table 1. performance (f1) of splinter-base (green line, triangular points),compared to all baselines as a function of the number of training examples on 4 datasets.
each point reﬂects theaverage performance across 5 randomly-sampled training sets of the same size..3077# examplesf10.020.040.060.080.0100.01632641282565121024robertaspanbertspanbert (reimpl)splintersquad# examplesf10.020.040.060.080.01632641282565121024robertaspanbertspanbert (reimpl)splinternatural questions# examplesf10.020.040.060.080.01632641282565121024robertaspanbertspanbert (reimpl)splinternewsqa# examplesf10.020.040.060.080.01632641282565121024robertaspanbertspanbert (reimpl)splintersearchqa# examplesf10.020.040.060.080.01632641282565121024robertaspanbertspanbert (reimpl)splinterhotpotqa# examplesf10.020.040.060.080.0100.01632641282565121024robertaspanbertspanbert (reimpl)splinterbioasqmodel.
squad.
triviaqa.
nq.
newsqa searchqa hotpotqa.
bioasq.
tbqa.
16 examples.
robertaspanbert(reimpl).
splinter.
32 examples.
robertaspanbert(reimpl).
splinter.
64 examples.
robertaspanbert(reimpl).
splinter.
128 examples.
robertaspanbert(reimpl).
splinter.
256 examples.
robertaspanbert(reimpl).
splinter.
512 examples.
robertaspanbert(reimpl).
splinter.
1024 examples.
robertaspanbert(reimpl).
splinter.
7.7 (4.3)12.5 (5.7)18.2 (6.7)54.6 (6.4).
7.5 (4.4)12.8 (5.4)11.6 (2.1)18.9 (4.1).
17.3 (3.3)19.7 (3.6)19.6 (3.0)27.4 (4.6).
1.4 (0.8)6.0 (1.6)7.6 (4.1)20.8 (2.7).
6.9 (2.7)13.0 (4.2)13.3 (6.0)26.3 (3.9).
10.5 (2.5)12.6 (4.3)12.5 (5.5)24.0 (5.0).
16.7 (7.1)22.0 (4.6)15.9 (4.4)28.2 (4.9).
3.3 (2.1)5.6 (2.5)7.5 (2.9)19.4 (4.6).
18.2 (5.1)19.0 (4.6)25.8 (7.7)59.2 (2.1).
10.5 (1.8)19.0 (4.8)15.1 (6.4)28.9 (3.1).
22.9 (0.7)23.5 (0.9)25.1 (1.6)33.6 (2.4).
3.2 (1.7)7.5 (1.3)7.2 (4.6)27.5 (3.2).
13.5 (1.8)20.1 (3.9)14.6 (8.5)34.8 (1.8).
10.4 (1.9)14.4 (2.9)13.2 (3.5)34.7 (3.9).
23.3 (6.6)32.5 (3.5)25.1 (3.3)36.5 (3.2).
4.3 (0.9)7.4 (1.1)7.6 (2.3)27.6 (4.3).
28.4 (1.7)33.6 (4.3)45.8 (3.3)65.2 (1.4).
12.5 (1.4)22.8 (2.6)15.9 (6.4)35.5 (3.7).
24.2 (1.0)28.4 (1.8)29.7 (1.5)38.2 (2.3).
4.6 (2.8)8.8 (2.4)12.5 (4.3)37.4 (1.2).
19.8 (2.4)26.7 (2.9)18.0 (4.6)39.8 (3.6).
15.0 (3.9)21.8 (1.5)23.3 (1.1)45.4 (2.3).
34.0 (1.8)43.9 (4.5)35.3 (3.1)49.5 (3.6).
5.4 (1.1)7.4 (1.2)13.0 (6.9)35.9 (3.1).
43.0 (7.1)48.5 (7.3)55.8 (3.7)72.7 (1.0).
19.1 (2.9)24.2 (2.1)26.3 (2.1)44.7 (3.9).
30.1 (1.9)32.2 (3.2)36.0 (1.9)46.3 (0.8).
16.7 (3.8)17.4 (3.1)29.5 (7.3)43.5 (1.3).
27.8 (2.5)34.3 (1.1)26.3 (4.3)47.2 (3.5).
27.3 (3.9)35.1 (4.2)36.6 (3.4)54.7 (1.4).
46.1 (1.4)55.3 (3.8)52.2 (3.2)63.2 (4.1).
8.2 (1.1)9.4 (3.0)20.9 (5.1)42.6 (2.5).
56.1 (5.2)55.2 (8.8)67.1 (2.1)76.8 (0.6).
26.9 (3.5)34.0 (5.7)39.4 (4.0)57.2 (2.2).
36.0 (3.2)41.3 (2.2)44.4 (3.2)54.6 (1.2).
31.2 (2.4)34.7 (4.1)41.8 (1.8)49.0 (0.4).
37.5 (1.7)42.3 (4.1)41.5 (3.2)55.7 (1.9).
42.7 (3.1)49.4 (4.0)51.5 (2.8)62.0 (1.6).
63.5 (1.8)67.5 (3.9)66.4 (2.8)77.4 (2.0).
13.5 (1.9)18.2 (4.5)31.1 (3.4)48.5 (2.2).
67.3 (0.7)70.0 (4.3)73.4 (0.4)80.1 (0.4).
38.7 (3.8)44.2 (2.9)50.4 (2.8)61.9 (1.8).
46.7 (2.2)51.5 (1.8)52.5 (1.9)61.4 (1.1).
41.5 (2.2)42.4 (2.6)47.6 (1.3)53.2 (0.9).
46.9 (1.6)53.9 (3.2)48.8 (4.1)63.1 (1.6).
56.7 (1.3)61.6 (1.7)59.5 (1.5)66.2 (0.6).
77.0 (1.9)80.3 (3.0)79.0 (1.9)84.8 (0.9).
27.0 (2.2)33.7 (3.4)40.2 (0.8)54.2 (1.7).
73.8 (0.8)77.8 (0.9)77.8 (0.6)82.8 (0.8).
46.8 (0.9)50.3 (4.0)55.5 (1.9)64.8 (0.9).
54.2 (1.1)57.5 (0.9)59.5 (1.7)65.5 (0.5).
47.5 (1.1)49.3 (2.0)52.2 (1.2)57.3 (0.8).
54.3 (1.2)60.1 (2.2)58.9 (1.9)67.3 (1.3).
61.8 (1.3)67.4 (1.6)64.6 (1.2)70.3 (0.8).
84.1 (1.1)89.3 (0.6)89.0 (1.8)91.0 (1.0).
35.8 (2.0)42.3 (1.9)45.7 (1.5)54.5 (1.5).
table 3: average performance (f1) across all datasets and training set sizes.
we add the standard deviation over theﬁve seeds for each setting in parentheses.
nq and tbqa stand for natural questions and textbookqa respectively.
(reimpl) stands for the spanbert (reimpl) baseline (see section 5.1)..3078figure 7: results complementary to ablation studies (section 6.3).
we examine the role of qass layer by ﬁne-tuning it on top of our spanbert.
in addition, we test whether it is beneﬁcial to keep the parameters of qassfrom pretraining (splinter with head)..3079# examplesf10.020.040.060.080.0100.01632641282565121024spanbert (reimpl)spanbert (reimpl) + qasssplintersplinter (with pretrained qass)natural questions# examplesf10.020.040.060.080.0100.01632641282565121024spanbert (reimpl)spanbert (reimpl) + qasssplintersplinter (with pretrained qass)triviaqa# examplesf10.020.040.060.080.0100.01632641282565121024spanbert (reimpl)spanbert (reimpl) + qasssplintersplinter (with pretrained qass)searchqa# examplesf10.020.040.060.080.0100.01632641282565121024spanbert (reimpl)spanbert (reimpl) + qasssplintersplinter (with pretrained qass)newsqa# examplesf10.020.040.060.080.0100.01632641282565121024spanbert (reimpl)spanbert (reimpl) + qasssplintersplinter (with pretrained qass)hotpotqa# examplesf10.020.040.060.080.0100.01632641282565121024spanbert (reimpl)spanbert (reimpl) + qasssplintersplinter (with pretrained qass)textbookqa