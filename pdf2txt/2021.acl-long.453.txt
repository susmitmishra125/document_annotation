mulda: a multilingual data augmentation frameworkfor low-resource cross-lingual ner.
linlin liu∗12 bosheng ding∗12 lidong bing2 shaﬁq joty1 luo si2 chunyan miao11nanyang technological university, singapore 2damo academy, alibaba group{linlin.liu, bosheng.ding, l.bing, luo.si}@alibaba-inc.com{srjoty, ascymiao}@ntu.edu.sg.
abstract.
named entity recognition (ner) for low-resource languages is a both practical andthis paperchallenging research problem.
addresses zero-shot transfer for cross-lingualner, especially when the amount of source-language training data is also limited.
thepaper ﬁrst proposes a simple but effective la-beled sequence translation method to trans-late source-language training data to targetlanguages and avoids problems such as wordorder change and entity span determination.
with the source-language data as well as thetranslated data, a generation-based multilin-gual data augmentation method is introducedto further increase diversity by generatingsynthetic labeled data in multiple languages.
these augmented data enable the languagemodel based ner models to generalize bet-ter with both the language-speciﬁc featuresfrom the target-language synthetic data and thelanguage-independent features from multilin-gual synthetic data.
an extensive set of ex-periments were conducted to demonstrate en-couraging cross-lingual transfer performanceof the new research on a wide variety of targetlanguages.1.
1.introduction.
named entity recognition (ner) aims to identifyand classify entities in a text into predeﬁned types,which is an essential tool for information extraction.
it has also been proven to be useful in various down-stream natural language processing (nlp) tasks,including information retrieval (banerjee et al.,2019), question answering (fabbri et al., 2020)and text summarization (nallapati et al., 2016).
however, except for some resource-rich languages.
∗equal contribution, order decided by coin ﬂip.
linlin liuand bosheng ding are under the joint phd program betweenalibaba and nanyang technological university..1our code is available at https://ntunlpsg..github.io/project/mulda/..
(e.g., english, german), training sets for most ofthe other languages are still very limited.
more-over, it is usually expensive and time-consumingto annotate such data, particularly for low-resourcelanguages (kruengkrai et al., 2020).
therefore,zero-shot cross-lingual ner has attracted growinginterest recently, especially with the inﬂux of deeplearning methods (mayhew et al., 2017; joty et al.,2017; jain et al., 2019; bari et al., 2021)..existing approaches to cross-lingual ner canbe roughly grouped into two main categories:instance-based transfer via machine translation(mt) and label projection (mayhew et al., 2017;jain et al., 2019), and model-based transfer withaligned cross-lingual word representations or pre-trained multilingual language models (joty et al.,2017; baumann, 2019; wang et al., 2020; conneauet al., 2020; bari et al., 2021).
recently, wu et al.
(2020) unify instance-based and model-based trans-fer via knowledge distillation..these recent methods have demonstrated promis-ing zero-shot cross-lingual ner performance.
however, most of them assume the availability of aconsiderable amount of training data in the sourcelanguage.
when we reduce the size of the trainingdata, we observe signiﬁcant performance decrease.
for instance-based transfer, decreasing trainingset size also ampliﬁes the negative impact of thenoise introduced by mt and label projection.
formodel-based transfer, although the large-scale pre-trained multilingual language models (lm) (con-neau et al., 2020; liu et al., 2020) have achievedstate-of-the-art performance on many cross-lingualtransfer tasks, simply ﬁne-tuning them on a smalltraining set is prone to over-ﬁtting (wu et al., 2018;si et al., 2020; kou et al., 2020)..to address the above problems under the set-ting of low-resource cross-lingual ner, we pro-pose a multilingual data augmentation (mulda)framework to make better use of the cross-lingual.
proceedingsofthe59thannualmeetingoftheassociationforcomputationallinguisticsandthe11thinternationaljointconferenceonnaturallanguageprocessing,pages5834–5846august1–6,2021.©2021associationforcomputationallinguistics5834generalization ability of the pretrained multilinguallms.
speciﬁcally, we consider a low-resource set-ting for cross-lingual ner, where there is verylimited source-language training data and no target-language train/dev data.
such setting is practicaland useful in many real scenarios..our proposed framework seeks the initial helpfrom the instance-based transfer (i.e., translatetrain) paradigm (li et al., 2020; fang et al., 2020).
we ﬁrst introduce a novel labeled sequence trans-lation method to translate the training data to thetarget language as well as to other languages.
thisallows us to ﬁnetune the lm based ner modelon multilingual data rather than on the source-language data only, which helps prevent over-ﬁttingon the language-speciﬁc features.
one commonlyused tool for translation is the off-the-shelf googletranslate system2, which supports more than 100languages.
alternatively, there are also many pre-trained mt models conveniently accessible, e.g.,more than 1,000 marianmt (junczys-dowmuntet al., 2018; kim et al., 2019) models have beenreleased on the hugging face model hub.3.
note that the instance-based transfer methodsadd limited semantic variety to the training set,since they only translate entities and the corre-sponding contexts to a different language.
in con-trast, data augmentation has been proven to be asuccessful method for tackling the data scarcityproblem.
inspired by a recent monolingual dataaugmentation method (ding et al., 2020), we pro-pose a generation-based multilingual data augmen-tation method to increase the diversity, where lmsare trained on multilingual labeled data and thenused to generate more synthetic training data..we conduct extensive experiments and analysisto verify the effectiveness of our methods.
ourmain contributions can be summarized as follows:.
• we propose a simple but effective labeled se-quence translation method to translate the sourcetraining data to a desired language.
comparedwith exiting methods, our labeled sentence trans-lation approach leverages placeholders for la-bel projection, which effectively avoids manyissues faced during word alignment, such as wordorder change, entity span determination, noise-sensitive similarity metrics and so on..• we propose a generation-based multilingual data.
2https://cloud.google.com/translate3https://huggingface.co/transformers/model doc/marian.html.
augmentation method for ner, which leveragesthe multilingual language models to add morediversity to the training data..• through empirical experiments, we observe thatwhen ﬁne-tuning pretrained multilingual lms forlow-resource cross-lingual ner, translations tomore languages can also be used as an effectivedata augmentation method, which helps improveperformance of both the source and the targetlanguages..2 mulda: our multilingual dataaugmentation framework.
we propose a multilingual data augmentationframework that leverages the advantages of bothinstance-based and model-based transfer for cross-lingual ner.
in our framework, a novel labeledsequence translation method is ﬁrst introducedto translate the annotated training data from thesource language s to a set of target languagest = {t1, .
.
.
, tn}.
then language models aretrained on {ds, dt1, ..., dtn} to generate multi-lingual synthetic data, where ds is the source-language training data, and dti is the translateddata in language ti.
finally, we post-process andﬁlter the augmented data to train multilingual nermodels for inference on target-language test sets..2.1 labeled sequence translation.
we leverage labeled sequence translation for thetraining data of the source language to generatemultilingual ner training data, which can alsobe viewed a method for data augmentation.
priormethods (jain et al., 2019; li et al., 2020) usuallyperform translation and label projection in two sep-arate steps: 1) translate source-language trainingsentences to the target language; 2) propagate la-bels from the source training data to the translatedsentences via word-to-word/phrase-to-phrase map-ping with alignment models or algorithms.
how-ever, these methods suffer from a few label projec-tion problems, such as word order change, word-span determination (li et al., 2020), and so on.
analternative to avoid the label projection problemsis word-by-word translation (xie et al., 2018), butoften at the sacriﬁce of the translation quality..we address the problems identiﬁed above byﬁrst replacing named entities with contextual place-holders before sentence translation, and then aftertranslation, we replace placeholders in translated.
5835labeled sentence in the source language:[per jamie valentine] was born in [loc london]..1. translate sentence with placeholders:src: per0 was born in loc1.
tgt: per0 naci´o en loc1..2. translate entities with context:per0src: [jamie valentine] was born in london.
tgt: [jamie valentine] naci´o en londres..loc1src: jamie valentine was born in [london].
tgt: jamie valentine naci´o en [londres]..3. replace placeholders with translated entities:[per jamie valentine] naci´o en [loc londres]..figure 1: an example of labeled sentence translation,where src and tgt are the translation model inputs andoutputs, respectively.
for the example shown in thisﬁgure, google translation system and the marianmtmodel generate the same translations in step 1 and 2..sentences with the corresponding translated entities.
an illustration of the method is shown in figure 1.assume a sentence x s = {x1, .
.
.
, xm } ∈ dsand the corresponding ner tags {y1, .
.
.
, ym } aregiven, where xi’s are the sentence tokens and m isthe sentence length.
let {e1, .
.
.
, en} denote thepredeﬁned named entity types.
our method ﬁrstreplaces all entities in {x1, .
.
.
, xm } with place-holders (src of step 1 in figure 1).
placeholdersek are reconstructed tokens with the correspond-ing entity type e as preﬁx and the index of theentity k as sufﬁx.
assume {xi, .
.
.
, xj} is the kthentity in the source sentence, and the correspondingtype is ez, then we can replace the entity with theplaceholder ezk to get {.
.
.
, xi−1, ezk, xj+1, .
.
.}.
we use x s∗ to denote the generated sentence afterreplacing all entities with placeholders.
x s∗ is fedinto an mt model to get the translation x t∗ in thetarget language t .
with such design, the place-holder preﬁx e can provide the mt model4 withrelevant contextual information about the entities,so that the model can translate the sentence withreasonably good quality.
besides, we observe mostof placeholders are unchanged after translation,5which can be used to help locate the position ofentities..in the second step, we translate each entity.
4when the mt model use subword vocabularies.
5see appendix for more examples..b-per e-pero o o s-loc ojamie valentine was born in london ..⇓ linearization.
b-per jamie e-per valentine was born in s-loc london ..figure 2: an example of labeled sequence linearization..figure 3: training of multilingual lstm-lm on thelinearized sequences..with the corresponding context.
more speciﬁ-cally, we use brackets to mark the span of eachentity and translate it to the target language suc-cessively, one at a time (src of step 2 in figure 1).
for example, to translate entity {xi, .
.
.
, xj}, wefeed {.
.
.
, xi−1, [xi, .
.
.
, xj], xj+1, .
.
.}
into themt model.
then we can get entity translations byextracting the square bracket marked tokens fromthe translated sentences.
we translate the entitiesdirectly if the square brackets are not found.
finally, we can replace placeholders in x t∗ (ob-tained from the ﬁrst step) with the correspondingentity translations (obtained from the second step)and copy placeholder preﬁx as entity labels to gen-erate the synthetic training data in the target lan-guage (step 3 in figure 1).
we tested the proposedmethod with google translate and the marianmt(junczys-dowmunt et al., 2018; kim et al., 2019)models, and we found that both produce high qual-ity synthetic data as we had expected..2.2 synthetic data generation with.
language models.
although labeled sequence translation generateshigh quality multilingual ner training data, it addslimited variety since translation does not introducenew entities or contexts.
inspired by daga (dinget al., 2020), we propose a generation-based mul-tilingual data augmentation method to add morediversity to the training data.
daga is a mono-lingual data augmentation method designed for se-quence labeling tasks, which has been shown to.
5836be able to add signiﬁcant diversity to the trainingdata.
as the example shown in figure 2, it ﬁrst lin-earizes labeled sequences by adding the entity typebefore sentence tokens.
then an lstm-based lm(lstm-lm) is trained on the linearized sequencesin an autoregressive way, after which the begin-of-sentence token [bos] is fed into the lstm-lmto generate synthetic training data autoregressively.
the monolingual lstm-lm of daga is trainedin a similar way as the example shown in figure 3,except that there is no language tag [en]..to extend this method for multilingual data aug-mentation, we add special tokens at the beginningof each sentence to indicate the language that itbelongs to.
the source-language data and the mul-tilingual data obtained via translation are concate-nated to train/ﬁnetune multilingual lms with ashared vocabulary (as shown in figure 5).
given alabeled sequence {x1, .
.
.
, xm } from the multilin-gual training data, the lms are trained to maximizethe probability p(x1, .
.
.
, xm ) in eq.
1:.
p(x1, .
.
.
, xm ) =.
pθ(xt|x<t).
(1).
m(cid:89).
t=1.
where θ is the parameterto optimize, andpθ(xt|x<t) is the probability of the next tokengiven the previous tokens in the sequence, which isusually computed with the softmax function.
fig-ure 3 shows an example of how the multilinguallstm-lm is trained in the autoregressive way.
af-ter training the lstm-lm, we can feed the [bos]token and a language token to the model to generatesynthetic training data for the speciﬁed language.
besides, to leverage the cross-lingual general-ization ability of large scale pretrained multilin-gual lms, we also ﬁnetune a recent state-of-the-artseq2seq model mbart (liu et al., 2020), which ispretrained with multilingual denoising tasks.
sen-tence permutation and word-span masking are thetwo noise injection methods used to add noise tooriginal sentence x = {x1, .
.
.
, xm } to outputg(x), where g(.)
is used to denote the noise in-jection function.
after encoding g(x) with thetransformer encoder, the transformer decoder istrained to generate the original sequence x autore-gressively by maximizing eq.
1..denoising word-span masked sequences is themost relevant to our data augmentation method,since only small modiﬁcations are required to makeour ﬁnetuning task as consistent to the pretrain-ing task as possible.
more speciﬁcally, we design.
our ﬁnetuning task with the following changes:1) use the linearized labeled sequences (as shownin figure 5) as input x; 2) modify g(.)
to maskrandom trailing sub-sequences such that g(x) ={x1, .
.
.
, xz, [mask]}, where 1 ≤ z ≤ |x| is arandom integer.
after ﬁnetuning with such task,we can conveniently feed a randomly masked se-quence {x1, .
.
.
, xz, [mask]} into mbart to gen-erate synthetic data.
figure 4 shows a more con-crete example to illustrate how mbart is ﬁnetunedwith the linearized sequences in our work..2.3 semi-supervised method.
unlabeled multilingual sentences are usually easyto get, for example, data from the wikimedia6.
tomake better use of these unlabeled multilingualdata, we propose a semi-supervised method to pre-pare more pseudo labeled data for ﬁnetuning multi-lingual lms.
inspired by self-training (zoph et al.,2020; xie et al., 2020), we use the ner modeltrained on the multilingual translated data to anno-tate the unlabeled sentences.
after that, we use twoadditional ner models trained with different ran-dom seeds to ﬁlter the annotated data by removingthose with different tag predictions..2.4 post-processing.
we also design several straightforward methods topost-process and ﬁlter the augmented data gener-ated by the lms:.
• delete sequences that contain only o (other) tags..• convert the generated labeled sequences to thesame format as gold data by separating sentencetokens and ner tags..• use the ner model trained on the multilingualtranslated data to label the generated sequences(after tag removal).
then compare the tags gener-ated by the lm and ner model predictions, andremove the sentences with inconsistencies..3 experiments.
we conduct experiments to evaluate the effective-ness of the proposed multilingual data augmen-tation framework.
firstly, we compare our la-beled sequence translation method with the pre-vious instance-based transfer (i.e., translate train)methods.
following that, we show the beneﬁt ofadding multilingual translations.
then we continue.
6https://dumps.wikimedia.org/.
5837figure 4: finetune mbart with the linearized sequences.
the transformer decoder is trained to generate labeledsequences autoregressively.
following the mbart pretraining tasks, we add language tokens at the end of maskedsequences when feed them into encoder..[bos] [en] b-per jamie e-per valentine was born in s-loc london.
[bos] [de] b-per jamie e-per valentine wurde in s-loc london geboren.
[bos] [es] b-per jamie e-per valentine naci´o en s-loc londres.
[bos] [nl] b-per jamie e-per valentine werd geboren in s-loc londen...
.
..figure 5: the source-language data and the multilin-gual data obtained via translation are concatenated totrain/ﬁnetune multilingual lms..to evaluate the generation-based multilingual dataaugmentation method by comparing cross-lingualner performance of the models trained on mono-lingual, bilingual, and multilingual augmented data,respectively.
finally, we further evaluate our meth-ods on a wider range of distant languages..we use the most typical transformer-based nermodel7 in our experiments, which is implementedby adding a randomly initialized feed forward layerto the transformer ﬁnal layer for label classiﬁca-tion.
speciﬁcally, to demonstrate that our frame-work can help achieve additional performance gaineven on the top of the state-of-the-art multilinguallms, the checkpoint of the pretrained xlm-rlarge (conneau et al., 2020) model is used to ini-tialize our ner models..3.1 labeled sequence translation.
we ﬁnetune the ner model on the translated target-language data to compare our labeled sequencetranslation method (§2.1) with the existing instance-based transfer methods..experimental settings the conll02/03 nerdataset (tjong kim sang, 2002; tjong kim sangand de meulder, 2003) is used for evaluation,which contains data in four different languages: en-glish, german, dutch and spanish.
all of the dataare annotated with the same set of ner tags.
wefollow the steps described in §2.1 to translate en-.
7similar.
to.
the.
token.
classiﬁcation model.
in.
https://github.com/huggingface/transformers..glish train data to the other three languages.
follow-ing jain et al.
(2019) and li et al.
(2020), googletranslation system is used in the experiments.
sinceour ner model is more powerful than those usedby jain et al.
(2019) and li et al.
(2020), we re-produce their results with xlm-r large for a faircomparison.
all of the ner models are ﬁnetunedon the translated target-language sentences onlyfor 10 epochs with the best model selected usingenglish dev data, and then evaluated on the target-language original test data..method.
de.
es.
nl.
avg.
mayhew et al.
(2017)xie et al.
(2018)jain et al.
(2019)bari et al.
(2020)li et al.
(2020)†jain et al.
(2019)†ours.
60.157.861.565.2466.9070.9973.89.
65.072.473.575.9370.4974.6475.48.
67.670.469.974.6173.4676.6379.60.
64.2366.8768.3071.9370.2874.0976.32.thetable 1: cross-lingual ner performance ofinstance-based transfer methods.
† denotes the repro-duced results with xlm-r large..results we present the results in table 1. as wecan see, our method outperforms the best baselinemethod by 2.90 and 2.97 on german and dutchrespectively, and by 2.23 on average.
since ourmodels are only ﬁnetuned with the data generatedby the labeled sequence translation method, the re-sults directly demonstrate the effectiveness of ourmethod.
moreover, compared with the two recentbaseline methods (jain et al., 2019; li et al., 2020),our method does not rely on complex label projec-tion algorithms and is much easier to implement..3.2 multilingual translation as data.
augmentation.
after showing that our labeled sequence transla-tion method can generate high quality labeled datain the target language, in this section, we run ex-.
5838periments to verify the hypothesis that multilin-gual translation may help improve the cross-lingualtransfer performance of multilingual lms in lowresource scenarios..experimental settings we use the same nerdataset as above.
in order to simulate low resourcescenarios, we randomly sample 500, 1k and 2ksentences from the gold english train set.
our la-beled sequence translation method is used to trans-late the sampled data to pseudo labeled data inthe three target languages, german, spanish anddutch.
to better demonstrate how the training dataaffects cross-lingual ner performance, we trainthe ner model on four different conditions: 1) en:train the models on english data only; 2) tgt-tran:train the models on the pseudo labeled data in acertain target language only; 3) en + tgt-tran:train the models on the combination of englishdata and pseudo labeled target-language data; 4)en + multi-tran: train one single model on thecombination of english data and pseudo labeleddata in all three target languages.
we ﬁnd ﬁlter-ing the translated sentences can further improvecross-lingual transfer performance, so we use anner model trained on the sampled english datato label the translated sentences, count the numberof entities in each sentence different from nermodel predictions, and then remove the top 20%sentences with the most inconsistent entities.
thisis similar to the third step described in §2.4, exceptthat we remove all the inconsistent sentences fromthe augmented data, since the lms can be usedto generate a large number of candidate sentences.
we set max number of epochs to 10 and use 500sentences randomly sampled from the english devdata to select the best models for each setting.
thenthe best models are evaluated on the original targetlanguage test sets..results table 2 compares the cross-lingual nerperformance of the models trained on the differenttraining sets.
although the performances of en andtgt-tran are relatively bad in most of the cases,combining them can always boost the performancesigniﬁcantly, especially when the dataset size issmall.
adding multilingual translated data furtherimproves cross-lingual performance by more than1% on average when english data size is 1k or less.
therefore, multilingual translation can be used asan effective data augmentation approach in the lowresource scenarios of cross-lingual ner.
moreover,.
en size method.
de.
es.
nl.
avg.
500.
1k.
2k.
entgt-tranen + tgt-tranen + multi-tran.
entgt-tranen + tgt-tranen + multi-tran.
entgt-tranen + tgt-tranen + multi-tran.
60.1859.9769.1670.40.
68.9570.373.6373.42.
69.4771.9374.4575.91.
55.6853.5364.5765.70.
67.367.2269.8172.71.
75.272.9475.8876.04.
66.0960.3971.4072.20.
73.4373.9875.8376.74.
77.6477.9578.4077.85.
60.6557.9668.3869.43.
69.8970.5073.0974.29.
74.1074.2776.2476.60.table 2: cross-lingual ner performance of the modelstrained on different combinations of training sets..method.
500.
1k.
2k.
entgt-tran (avg)en + tgt-tran (avg)en + multi-tran.
78.6270.0784.6285.35.
87.0083.2788.6288.99.
89.5687.1090.5190.98.table 3: ner model performance on english test data..the trained single model with en + multi-tran canbe applied to all target languages..besides, we also observe that multilingual trans-lated data can even help improve ner performanceof the source language.
table 3 summarizes en-glish test data results for the above settings.
tgt-tran (avg) is the average english results of themodels trained on three different tgt-tran of ger-man, spanish and dutch respectively.
en + tgt-tran (avg) is the average for combining en witheach of the three different tgt-tran.
as we cansee, adding additional translated data consistentlyimproves english ner performance.
particularly,en + multi-tran achieves the best performance.
therefore, we can also use multilingual translateddata to improve low-resource monolingual nerperformance..3.3 generation-based multilingual data.
augmentation.
in this section, we run experiments to verifywhether applying generation-based data augmen-tation methods to the multilingual translated datacan further improve cross-lingual performance inthe low resource scenarios..experimental settings we follow the steps de-scribed in §2.2 to implement the proposed data aug-mentation framework on top of lstm-lm (kru-engkrai, 2019) and mbart (liu et al., 2020) sep-.
5839method.
de.
es.
nl.
avg.
de.
es.
nl.
avg.
de.
es.
nl.
avg.
500.
1k.
2k.
70.40en + multi-tranmulda-lstm70.04mulda-mbart 72.37.en + tgt-tranbida-lstm.
69.1672.51.
65.7067.3868.19.
64.5768.77.
72.2072.8174.59.
71.4072.65.
69.4370.0871.72.
68.3871.31.
73.4274.8075.04.
73.6374.97.
72.7174.2774.56.
69.8173.69.
76.7477.2177.78.
75.8377.51.
74.2975.4275.79.
73.0975.39.
75.9176.0577.54.
74.4576.59.
76.0476.0576.32.
75.8876.47.
77.8578.4678.21.
78.4078.97.
76.6076.8577.36.
76.2477.34.table 4: cross-lingual ner results of models trained on multilingual augmented data..method.
af.
ar.
bg.
bn.
de.
el.
en.
es.
et.
eu.
fa.
ﬁ.fr.
he.
hi.
hu.
id.
it.
ja.
jv.
70.87en74.01en + multi-tran73.75weak taggermulda-lstm74.25mulda-mbart 74.58.
40.4542.7738.5444.9553.62.
53.10en64.27en + multi-tran64.98weak taggermulda-lstm67.27mulda-mbart 67.68.
42.7045.1046.5046.1043.12.
73.1875.5476.1276.5476.99.
46.4950.8650.1352.6952.46.
67.9673.2174.5274.1974.29.
55.6360.5158.4262.5358.47.
72.8674.2575.2274.9573.80.
54.6659.8459.3763.5461.49.
69.9171.3872.8071.4373.66.
56.7367.4867.7968.7967.70.
74.8177.2778.1878.2378.79.
44.9150.7153.5452.6252.06.
67.4766.1365.7765.8866.88.
77.0478.1779.2978.2278.86.
70.3873.2373.8173.3172.63.
72.6874.4273.8774.5676.15.
56.1756.1158.5261.9455.66.
55.6260.8163.1864.2865.00.
48.9151.2843.5748.4048.05.
64.5967.8169.1768.7767.40.
72.9274.5175.0075.5674.66.
48.3756.7957.0758.9859.30.
72.1075.2174.7875.1775.53.
43.8148.9051.1450.8848.95.
41.7653.7553.8055.0455.11.th.
2.563.674.375.135.31.
58.9667.5266.7567.4967.46.
67.2672.8773.1174.9774.57.
72.6273.5875.0974.6474.57.
73.0773.5178.4176.0574.75.
47.2854.2350.1150.9453.44.
51.0855.7050.3452.3748.86.
73.4276.7376.5275.7376.37.
9.2934.5136.1336.1537.05.
59.3260.5659.3862.0360.80.
65.0768.5471.0469.2270.25.
44.6249.7552.2848.0952.97.
13.4639.4038.5741.7741.30.method.
ka.
kk.
ko.
ml.
mr.ms.my.
nl.
pt.
ru.
sw.ta.
te.
tl.
tr.
ur.
vi.
yo.
zh.
table 5: cross-lingual ner f1 for wikiann when only 1k annotated english sentences are available.
we assumemt models are only available for the languages highlighted with green background..arately, and then use them to augment the dataprocessed in §3.2.
we concatenate english golddata and the ﬁltered multilingual translated data totrain/ﬁnetune the modiﬁed lms, where lstm-lmis trained from scratch and mbart is intializedwith the mbart cc25 checkpoint8 for ﬁnetun-ing.
mbart cc25 is a model with 12 encoderand decoder layers trained on 25 languages.
wefollow the steps described in §2.4 to post-processthe augmented data, and concatenate them with thecorresponding english gold and translated multi-lingual data to train the ner models.
the size ofthe augmented data used in each setting is the sameas the size of the corresponding english gold data.
mulda-lstm and mulda-mbart are used to de-note the methods that use lstm-lm and mbartaugmented data respectively.
in addition, we alsoreport a bilingual version of our method, denotedwith bida-lstm, which performs data augmenta-tion on english and the translated target-languagedata only.
we follow the same settings as aboveto evaluate cross-lingual performance of the nermodels trained on different data..results average results of 5 runs are reportedin table 4. note that mulda-lstm and mulda-mbart train a single model for all the target lan-guages in each setting, while bida-lstm trainsone model for each target language in each set-ting.
therefore, we compare bida-lstm with.
8https://github.com/pytorch/fairseq/blob/master/.
examples/mbart/readme.md.
en + tgt-tran only.
as we can see, the proposedmultilingual data augmentation methods further im-prove cross-lingual ner performance consistently.
for the 1k and 2k setting, mulda-lstm achievescomparable average performance as bida-lstm..3.4 evaluation on more distant languages.
we evaluate the proposed method on a wider rangeof target languages in this section..experimental settings the wikiann ner data(pan et al., 2017) processed by hu et al.
(2020) isused in these experiments.
1k english sentences(ds1k) are sampled from the gold train data to sim-ulate the low resource scenarios.
we also assumemt models are not available for all of the targetlanguages, so we only translate the sampled en-glish sentences to 6 target languages: ar, fr, it, ja,tr and zh.
dttrans is used to denote the translatedtarget-language sentences by following steps de-scribed in §2.1.
the low quality translated sen-tences are ﬁltered out in the same way as §3.2.
toevaluate our method in the semi-supervised setting,we also sample 5,000 sentences from the trainingdata of the 6 target languages and then removethe ner tags to create unlabeled data dtunlabeled.
we follow the steps described in §2.3 to anno-tate dtunlabeled with one ner model trained on{dstrans}, and then ﬁlter the pseudo labeleddata with two other ner models trained on thesame data but with different random seeds.
weuse dtsemi to denote the data generated with this.
1k, dt.
5840trans, dt.
semi-supervised approach.
finally, we concatenate1k, dt{dssemi} to generate augmented datadtaug following the steps in §2.2 and §2.4.
withthe augmented data above, we train ner modelson the concatenated data of {dsaug}for cross-lingual ner evaluation.
we also train anner model on {dssemi} for compari-son, denoted as weak tagger.
the other settingsare same as the above experiments..trans, dt.
trans, dt.
1k, dt.
1k, dt.
method.
en.
tran-train zero shot.
all.
74.81en77.27en + multi-tran78.18weak taggermulda-lstm78.23mulda-mbart 78.79.
47.1056.9157.1958.3759.62.
57.4761.7061.8162.5862.24.
56.3561.3761.5262.3462.26.example 1gold en: .
.
.
(org association for relations across thetaiwan straits) .
.
.
jain et al.
(2019): .
.
.
(org vereinigung f¨ur beziehungen)¨uber die taiwanstraße .
.
.
li et al.
(2020): .
.
.
(org vereinigung f¨ur beziehungen) ¨uber(org die taiwanstraße) .
.
.
ours:taiwanstraße) .
.
...
.
.
(org vereinigung f¨ur beziehungen ¨uber die.
example 2gold en: .
.
.
(loc u.s. midwest) .
.
.
jain et al.
(2019): .
.
.
(loc mittlerer westen) der (loc usa).
.
.
li et al.
(2020): .
.
.
mittlerer (loc westen) der (loc usa).
.
.
ours: .
.
.
(loc mittlerer westen der usa) .
.
..table 6: summary of the cross-lingual ner perfor-mance on wikiann..figure 6: two examples that the previous methods failto ﬁnd the correct entity boundaries..results we summarize the results in table 6.tran-train is the average performance of the 6languages that have corresponding training datatranslated from english.
zero shot is the averageperformance of the other target languages.
mulda-lstm demonstrates promising performance im-provements on both the tran-train and zero shotlanguages.
the performance of mulda-mbart isslightly lower, one possible reason is the noise in-troduced by the sentences labeled at character level.
we follow the gold data format to label translatedzh and ja sequences at character level, which is in-consistent with how mbart is pretrained.
pleaserefer to table 5 for the detailed cross-lingual nerresults of each language..3.5 case study.
3.5.1 effectiveness in label projection.
the label projection step of the previous methodsneeds to locate the entities and determine theirboundaries, which is vulnerable to many prob-lems, such as word order change, long entities,etc.
our method effectively avoids these problemswith placeholders.
in the two examples shown infigure 6, jain et al.
(2019) either labeled only partof the whole entity or incorrectly split the entityinto two, li et al.
(2020) incorrectly split the enti-ties into two in both examples, while our methodcan correctly map the labels..3.5.2 multilingual data augmentation.
we look into the data generated by our multilingualdata augmentation method.
during lm training,.
figure 7: examples of multilingual sentences..the ner tags can be viewed as a shared vocabularybetween different languages.
as a result, we ﬁndthat some generated sentences contain tokens frommultiple languages, which are useful to help im-prove cross-lingual transfer (tan and joty, 2021).
two examples are shown in figure 7..4 related work.
cross-lingual ner there has been growing in-terest in cross-lingual ner.
prior approaches canbe grouped into two main categories, instance-based transfer and model-based transfer.
instance-based transfer translates source-language trainingdata to target language, and then apply label pro-jection to annotate the translated data (tiedemannet al., 2014; jain et al., 2019).
instead of mt, someearlier approaches also use parallel corpora to con-struct pseudo training data in the target language(yarowsky et al., 2001; fu et al., 2014).
to mini-mize resource requirement, mayhew et al.
(2017)and xie et al.
(2018) design frameworks that onlyrely on word-to-word/phrase-to-phrase translationwith bilingual dictionaries.
besides, there are alsomany studies on improving label projection quality.
5841with additional feature or better mapping methods(tsai et al., 2016; li et al., 2020).
different fromthese methods, our labeled sentence translation ap-proach leverages placeholders to determine the po-sition of entities after translation, which effectivelyavoids many issues during label projection, suchas word order change, entity span determination,noise-sensitive similarity metrics and so on..model-based transfer directly applies the modeltrained on the source language to the target-language test data (t¨ackstr¨om et al., 2012; ni et al.,2017; joty et al., 2017; chaudhary et al., 2018),which heavily relies on the quality of cross-lingualrepresentations.
recent methods have achieved sig-niﬁcant performance improvement by ﬁne-tuninglarge scale pretrained multilingual lms (devlinet al., 2019; keung et al., 2019; conneau et al.,2020).
besides, there are also some approaches thatcombine instance-based and model-based transfer(xu et al., 2020; wu et al., 2020).
compared withthese methods, our approach leverages mt modelsand lms to add more diversity to the training data,and prevents over-ﬁtting on language-speciﬁc fea-tures by ﬁne-tuning ner models on multilingualdata..data augmentation data augmentation (simardet al., 1998) adds more diversity to training datato help improve model generalization, which hasbeen widely used in many ﬁelds, such as computervision (zhang et al., 2018), speech (cui et al., 2015;park et al., 2019), nlp (wang and eisner, 2016;sun et al., 2020) and so on.
for nlp, back trans-lation (sennrich et al., 2016) is one of the mostsuccessful data augmentation approaches, whichtranslates target-language monolingual data to thesource language to generate more parallel data formt model training.
other popular approachesinclude synonym replacement (kobayashi, 2018),random deletion/swap/insertion (sun et al., 2020;kumar et al., 2020), generation (ding et al., 2020),etc.
data augmentation has also been proven to beuseful in the cross-lingual settings (zhang et al.,2019; singh et al., 2020; riabi et al., 2020; qinet al., 2020; bari et al., 2021; mohiuddin et al.,2021), but most of the exiting methods overlookthe better utilization of multilingual training datawhen such resources are available..5 conclusions.
ner.
our labeled sequence translation method ef-fectively avoids many label projection related prob-lems by leveraging placeholders during mt.
ourgeneration-based multilingual data augmentationmethod generates high quality synthetic trainingdata to add more diversity.
the proposed frame-work has demonstrated encouraging performanceimprovement in various low-resource settings andacross a wide range of target languages..acknowledgements.
this research is partly supported by the alibaba-ntu singapore joint research institute, nanyangtechnological university.
linlin liu would like tothank the support from interdisciplinary graduateschool, nanyang technological university.
wewould like to thank the help from our alibaba col-leagues, ruidan he and qingyu tan in this workas well..references.
partha sarathy banerjee, baisakhi chakraborty,deepak tripathi, hardik gupta, and sourabh skumar.
2019. a information retrieval based onquestion and answering and ner for unstructuredinformation without using sql.
wireless personalcommunications, 108(3):1909–1931..m saiful bari, shaﬁq joty, and prathyusha jwalapuram.
2020. zero-resource cross-lingual named entityrecognition.
in proceedings of the 34th aaai con-ference on artiﬁcal intelligence, aaai ’20, newyork, usa.
aaai..m saiful bari, tasnim mohiuddin, and shaﬁq joty.
2021. uxla: a robust unsupervised data aug-inmentation framework for cross-lingual nlp.
proceedings of the joint conference of the 59thannual meeting of the association for computa-tional linguistics and the 11th international jointconference on natural language processing (acl-ijcnlp 2021), online.
association for computa-tional linguistics..antonia baumann.
2019. multilingual language mod-els for named entity recognition in german and en-glish.
in proceedings of the student research work-shop associated with ranlp 2019, pages 21–27,varna, bulgaria.
incoma ltd..aditi chaudhary, chunting zhou, lori levin, grahamneubig, david r mortensen, and jaime g carbonell.
2018. adapting word embeddings to new languageswith morphological and phonological subword rep-resentations.
arxiv preprint arxiv:1808.09500..we have proposed a multilingual data augmen-tation framework for low resource cross-lingual.
alexis conneau, kartikay khandelwal, naman goyal,vishrav chaudhary, guillaume wenzek, francisco.
5842guzm´an, edouard grave, myle ott, luke zettle-moyer, and veselin stoyanov.
2020. unsupervisedcross-lingual representation learning at scale.
inproceedings of the 58th annual meeting of the asso-ciation for computational linguistics, pages 8440–8451, online.
association for computational lin-guistics..xiaodong cui, vaibhava goel, and brian kingsbury.
2015. data augmentation for deep neural networkacoustic modeling.
ieee/acm transactions on au-dio, speech, and language processing, 23(9):1469–1477..jacob devlin, ming-wei chang, kenton lee, andkristina toutanova.
2019. bert: pre-training ofdeep bidirectional transformers for language under-in proceedings of the 2019 conferencestanding.
of the north american chapter of the associationfor computational linguistics: human languagetechnologies, volume 1 (long and short papers),pages 4171–4186, minneapolis, minnesota.
associ-ation for computational linguistics..bosheng ding, linlin liu, lidong bing, canasai kru-engkrai, thien hai nguyen, shaﬁq joty, luo si, andchunyan miao.
2020. daga: data augmentationwith a generation approach for low-resource taggingin proceedings of the 2020 conference ontasks.
empirical methods in natural language process-ing (emnlp), pages 6045–6057, online.
associa-tion for computational linguistics..alexander fabbri, patrick ng, zhiguo wang, rameshnallapati, and bing xiang.
2020. template-basedquestion generation from retrieved sentences for im-in pro-proved unsupervised question answering.
ceedings of the 58th annual meeting of the asso-ciation for computational linguistics, pages 4508–4513, online.
association for computational lin-guistics..yuwei fang, shuohang wang, zhe gan, siqi sun, andjingjing liu.
2020. filter: an enhanced fusionmethod for cross-lingual language understanding..ruiji fu, bing qin, and ting liu.
2014. generat-ing chinese named entity data from parallel corpora.
frontiers of computer science, 8(4):629–641..junjie hu, sebastian ruder, aditya siddhant, gra-ham neubig, orhan firat, and melvin johnson.
2020. xtreme: a massively multilingual multi-task benchmark for evaluating cross-lingual gener-alisation.
in international conference on machinelearning, pages 4411–4421.
pmlr..alankar jain, bhargavi paranjape, and zachary c. lip-ton.
2019. entity projection via machine transla-in proceedings of thetion for cross-lingual ner.
2019 conference on empirical methods in natu-ral language processing and the 9th internationaljoint conference on natural language processing(emnlp-ijcnlp), pages 1083–1092, hong kong,china.
association for computational linguistics..shaﬁq joty, preslav nakov, llu´ıs m`arquez, and is-raa jaradat.
2017. cross-language learning with ad-versarial neural networks: application to commu-in proceedings of thenity question answering.
signll conference on computational natural lan-guage learning, conll’17, pages 226–237, van-couver, canada.
association for computational lin-guistics..marcin junczys-dowmunt, roman grundkiewicz,tomasz dwojak, hieu hoang, kenneth heaﬁeld,tom neckermann, frank seide, ulrich germann,alham fikri aji, nikolay bogoychev, andr´e f. t.martins, and alexandra birch.
2018. marian: fastneural machine translation in c++.
in proceedingsof acl 2018, system demonstrations, pages 116–121, melbourne, australia.
association for compu-tational linguistics..phillip keung, yichao lu, and vikas bhardwaj.
2019.adversarial learning with contextual embeddings forzero-resource cross-lingual classiﬁcation and ner.
in proceedings of the 2019 conference on empiricalmethods in natural language processing and the9th international joint conference on natural lan-guage processing (emnlp-ijcnlp), pages 1355–1360, hong kong, china.
association for computa-tional linguistics..young jin kim, marcin junczys-dowmunt, hany has-san, alham fikri fikri aji, kenneth heaﬁeld, ro-man grundkiewicz, and nikolay bogoychev.
2019.from research to production and back: ludicrouslyin proceedingsfast neural machine translation.
of the third workshop on neural generation andtranslation, hong kong.
association for computa-tional linguistics..sosuke kobayashi.
2018. contextual augmentation:data augmentation by words with paradigmatic re-in proceedings of the 2018 conference oflations.
the north american chapter of the association forcomputational linguistics: human language tech-nologies, volume 2 (short papers), pages 452–457,new orleans, louisiana.
association for computa-tional linguistics..xiaoyu kou, yaming yang, yujing wang, ce zhang,yiren chen, yunhai tong, yan zhang, and jing bai.
2020. improving bert with self-supervised attention..canasai kruengkrai.
2019. better exploiting latentin proceedings of thevariables in text modeling.
57th annual meeting of the association for com-putational linguistics, pages 5527–5532, florence,italy.
association for computational linguistics..canasai kruengkrai, thien hai nguyen, sharifah ma-hani aljunied, and lidong bing.
2020. improvinglow-resource named entity recognition using jointsentence and token labeling.
in proceedings of the58th annual meeting of the association for compu-tational linguistics, pages 5898–5905..5843varun kumar, ashutosh choudhary, and eunah cho.
2020. data augmentation using pre-trained trans-former models.
in proceedings of the 2nd workshopon life-long learning for spoken language systems,pages 18–26, suzhou, china.
association for com-putational linguistics..data augmentation for zero-shot cross-lingual nlp.
in proceedings of the twenty-ninth internationaljoint conference on artiﬁcial intelligence, ijcai-20, pages 3853–3860.
international joint confer-ences on artiﬁcial intelligence organization.
maintrack..xin li, lidong bing, wenxuan zhang, zheng li, andwai lam.
2020. unsupervised cross-lingual adapta-tion for sequence tagging and beyond..yinhan liu, jiatao gu, naman goyal, xian li, sergeyedunov, marjan ghazvininejad, mike lewis, andluke zettlemoyer.
2020. multilingual denoisingpre-training for neural machine translation.
transac-tions of the association for computational linguis-tics, 8:726–742..stephen mayhew, chen-tse tsai, and dan roth.
2017.cheap translation for cross-lingual named entityrecognition.
in proceedings of the 2017 conferenceon empirical methods in natural language process-ing, pages 2536–2545, copenhagen, denmark.
as-sociation for computational linguistics..tasnim mohiuddin, m saiful bari, and shaﬁq joty.
2021. augvic: exploiting bitext vicinity for low-in findings of the association forresource nmt.
computational linguistics: acl-ijcnlp 2021, on-line.
association for computational linguistics..ramesh nallapati, bowen zhou, cicero dos santos,c¸ a˘glar gu`i‡lc¸ehre, and bing xiang.
2016. abstrac-tive text summarization using sequence-to-sequencein proceedings of the 20thrnns and beyond.
signll conference on computational natural lan-guage learning, pages 280–290, berlin, germany.
association for computational linguistics..jian ni, georgiana dinu, and radu florian.
2017.weakly supervised cross-lingual named entity recog-nition via effective annotation and representationprojection.
in proceedings of the 55th annual meet-ing of the association for computational linguistics(volume 1: long papers), pages 1470–1480, van-couver, canada.
association for computational lin-guistics..xiaoman pan, boliang zhang, jonathan may, joelnothman, kevin knight, and heng ji.
2017. cross-lingual name tagging and linking for 282 languages.
in proceedings of the 55th annual meeting of theassociation for computational linguistics (volume1: long papers), pages 1946–1958, vancouver,canada.
association for computational linguistics..daniel s. park, william chan, yu zhang, chung-cheng chiu, barret zoph, ekin d. cubuk, andquoc v. le.
2019. specaugment: a simple dataaugmentation method for automatic speech recog-in proc.
interspeech 2019, pages 2613–nition.
2617..libo qin, minheng ni, yue zhang, and wanxiangche.
2020. cosda-ml: multi-lingual code-switching.
arij riabi, thomas scialom, rachel keraron, benoˆıtsagot, djam´e seddah, and jacopo staiano.
2020.synthetic data augmentation for zero-shot cross-lingual question answering..rico sennrich, barry haddow, and alexandra birch.
improving neural machine translation mod-2016.in proceedings of theels with monolingual data.
54th annual meeting of the association for compu-tational linguistics (volume 1: long papers), pages86–96, berlin, germany.
association for computa-tional linguistics..shijing si, rui wang, jedrek wosik, hao zhang, daviddov, guoyin wang, and lawrence carin.
2020.students need more attention: bert-based attentionmodel for small data with application to automaticin proceedings of the 5thpatient message triage.
machine learning for healthcare conference, vol-ume 126 of proceedings of machine learning re-search, pages 436–456, virtual.
pmlr..patrice y. simard, yann a. lecun, john s. denker, andbernard victorri.
1998. transformation invariancein pattern recognition — tangent distance and tan-gent propagation, pages 239–274.
springer berlinheidelberg, berlin, heidelberg..jasdeep singh, bryan mccann, nitish shirishkeskar, caiming xiong, and richard socher.
2020.
{xlda}: cross-lingual data augmentation for natu-ral language inference and question answering..lichao sun, congying xia, wenpeng yin, tingtingliang, philip yu, and lifang he.
2020. mixup-transformer: dynamic data augmentation for nlptasks.
in proceedings of the 28th international con-ference on computational linguistics, pages 3436–3440, barcelona, spain (online).
international com-mittee on computational linguistics..oscar t¨ackstr¨om, ryan mcdonald, and jakob uszko-reit.
2012. cross-lingual word clusters for directtransfer of linguistic structure.
in the 2012 confer-ence of the north american chapter of the associ-ation for computational linguistics: human lan-guage technologies (naacl-hlt 2012)..samson tan and shaﬁq joty.
2021. code-mixing onsesame street: dawn of the adversarial polyglots.
inproceedings of the north american chapter of theassociation for computational linguistics: humanlanguage technologies, naacl’21, mexico city,mexico.
acl..j¨org tiedemann, ˇzeljko agi´c, and joakim nivre.
2014.treebank translation for cross-lingual parser induc-in proceedings of the eighteenth confer-tion.
ence on computational natural language learning,.
5844pages 130–140, ann arbor, michigan.
associationfor computational linguistics..erik f. tjong kim sang.
2002..introduction to theconll-2002 shared task: language-independentin coling-02: thenamed entity recognition.
6th conference on natural language learning 2002(conll-2002)..erik f. tjong kim sang and fien de meulder.
2003. introduction to the conll-2003 shared task:language-independent named entity recognition.
inproceedings of the seventh conference on natu-ral language learning at hlt-naacl 2003, pages142–147..chen-tse tsai, stephen mayhew, and dan roth.
2016.cross-lingual named entity recognition via wikiﬁca-in proceedings of the 20th signll confer-tion.
ence on computational natural language learning,pages 219–228..dingquan wang and jason eisner.
2016. the galacticdependencies treebanks: getting more data by syn-thesizing new languages.
transactions of the asso-ciation for computational linguistics, 4:491–505..zirui wang, jiateng xie, ruochen xu, yiming yang,graham neubig, and jaime g. carbonell.
2020.cross-lingual alignment vs joint training: a compar-ative study and a simple uniﬁed framework.
in inter-national conference on learning representations..qianhui wu, zijia lin, b¨orje f. karlsson, biqinghuang, and jian-guang lou.
2020. unitrans: unify-ing model transfer and data transfer for cross-lingualnamed entity recognition with unlabeled data..xing wu, shangwen lv, liangjun zang, jizhong han,and songlin hu.
2018. conditional bert contextualaugmentation..jiateng xie, zhilin yang, graham neubig, noah a.smith, and jaime carbonell.
2018. neural cross-lingual named entity recognition with minimal re-sources..qizhe xie, minh-thang luong, eduard hovy, andquoc v. le.
2020. self-training with noisy studentimproves imagenet classiﬁcation.
in proceedings ofthe ieee/cvf conference on computer vision andpattern recognition (cvpr)..weijia xu, batool haider, and saab mansour.
2020.end-to-end slot alignment and recognition for cross-in proceedings of the 2020 confer-lingual nlu.
ence on empirical methods in natural languageprocessing (emnlp), pages 5052–5063, online.
as-sociation for computational linguistics..david yarowsky, grace ngai, and richard wicen-inducing multilingual text analysistowski.
2001.tools via robust projection across aligned corpora.
inproceedings of the first international conference onhuman language technology research..hongyi zhang, moustapha cisse, yann n. dauphin,and david lopez-paz.
2018. mixup: beyond empir-ical risk minimization.
in international conferenceon learning representations..meishan zhang, yue zhang, and guohong fu.
2019.cross-lingual dependency parsing using code-mixedtreebank.
in proceedings of the 2019 conferenceon empirical methods in natural language process-ing and the 9th international joint conference onnatural language processing (emnlp-ijcnlp),pages 997–1006, hong kong, china.
associationfor computational linguistics..barret zoph, golnaz ghiasi, tsung-yi lin, yin cui,hanxiao liu, ekin d cubuk, and quoc v le.
2020.arxivrethinking pre-training and self-training.
preprint arxiv:2006.06882..a appendix.
a.1 translation with placeholders.
figure 8 shows more examples of translating thesequence “per0 was born in loc1.” to differentlanguages.
we can see that the placeholders areall well kept.
meanwhile, the translation quality isalso good..source sentence:en: per0 was born in loc1..translations:de: per0 wurde in loc1 geboren.
es: per0 naci´o en loc1.
nl: per0 is geboren in loc1.
vi: per0 được sinh ra ở loc1.
fr: per0 est n´e en loc1.
zh: per0出生于loc1。.
figure 8: translations of “per0 was born in loc1.”to different languages with google translation system..a.2 number of entities in translated data.
we count the total number of entities in gold endata and the translated data.
as shown in table 7,the number of entities in our translated data is themost close to that of the gold en data..method.
de.
es.
nl.
jain et al.
(2019)†li et al.
(2020)†ours.
230682384423418.gold en.
232752393023475.
234422333523473.
23499.table 7: number of entities in translated data.
the boldtext denotes the numbers most to that of the gold endata.
† denotes the reproduced results..5845a.3 visualization of entity representations.
we visualize the last layer transformer outputs ofthe ﬁnetuned ner model with t-sne.
we ﬁnetunetwo xlm-r initialized ner models on englishand mulda-lstm respectively, and generate lastlayer representations with chinese test data.
onlythe token representations corresponding to the band i tags are saved.
the two dimensional t-snevisualizations are shown in figures 9 and 10. as wecan see, the representation clusters correspondingto different ner entities in figure 10 (mulda-lstm) are further separated than that in figure 9(english)..figure 9: entity representation distribution of the nermodel trained on english..figure 10: entity representation distribution of thener model trained on mulda-lstm augmented data..a.4 parameters.
the parameters used for ner model ﬁne-tuningare shown in table 8..parameters.
values.
batch sizeoptimizerlearning ratebetasmax number of epochs.
16adamw2e-5(0.9, 0.999)10.table 8: parameters used for ner model ﬁne-tuning..5846