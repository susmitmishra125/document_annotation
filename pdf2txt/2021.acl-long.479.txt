photochat: a human-human dialogue dataset with photo sharingbehavior for joint image-text modeling.
xiaoxue zang1, lijuan liu1, maria wang1, yang song2∗, hao zhang1, jindong chen11 google research, 2 kuaishou technology1{xiaoxuez, lijuanliu, mariawang, haozhangthu, jdchen}@google.com,2 yangsong@kuaishou.com.
abstract.
we present a new human-human dialoguedataset - photochat, the ﬁrst dataset that castslight on the photo sharing behavior in onlinemessaging.
photochat contains 12k dialogues,each of which is paired with a user photothat is shared during the conversation.
basedon this dataset, we propose two tasks to fa-cilitate research on image-text modeling: aphoto-sharing intent prediction task that pre-dicts whether one intends to share a photoin the next conversation turn, and a photoretrieval task that retrieves the most relevantinphoto according to the dialogue context.
addition, for both tasks, we provide baselinemodels using the state-of-the-art models andreport their benchmark performances.
thebest image retrieval model achieves 10.4% re-call@1 (out of 1000 candidates) and the bestphoto intent prediction model achieves 58.1%f1 score, indicating that the dataset presents in-teresting yet challenging real-world problems.
we are releasing photochat to facilitate futureresearch work among the community..1.introduction.
as instant messaging tools gain enormous pop-ularity in the recent decades, sharing photos asan approach to enhance the engagement of an on-line messaging conversation has become a perva-sive routine communicative act (lobinger, 2016).
a survey conducted in 2010 reveals that 74% ofteenagers in the us reported messaging a photo orvideo using their cell phone (lenhart et al., 2010).
in britain, almost 70% of the internet users sharedphotos in 2013 (dutton and blank, 2013).
consid-ering the proliferation of photo sharing, it’s desir-able to have an intelligent system that can assistusers efﬁciently engaging in this process, i.e.
sug-gesting the most relevant photos in correct timings.
in order to achieve this goal, the intelligent systemis expected to not only understand how humans.
∗research conducted while working at google..figure 1: an example of how people share photos in adaily conversation..communicate with each other, e.g.
the natural lan-guage human speak, but also perceive images ashuman do.
how to facilitate building such multi-modal system is the goal of this paper..though recently many image-text tasks havebeen proposed and are being actively studied tobridge language and vision, the majority of themare formulated as choosing or composing the textbased on the understanding of given images, e.g.
image captioning (anderson et al., 2018), visualquestion answering (antol et al., 2015), visualcommonsense reasoning (zellers et al., 2019), andimage-grounded dialogue generation (shuster et al.,2020).
contrary to these tasks, the photo sharingtask focuses on the reverse process, i.e.
selectingthe image based on the understanding of text, aswell as proposing different and unique challenges.
firstly, different from the above popular multi-modal tasks, in photo-sharing task, the dialoguedoesn’t often explicitly mention the main visiblecontent in the image.
instead of the main object ofthe photo, sometimes the background story, com-plemented by human imaginations, can be the focusof the chat.
figure 1 shows such an example, inwhich the person who shares the photo describesthe event location “court” and the occupation “at-torney” instead of the main object “lady” in theimage.
secondly, the dialogue is not guaranteed.
proceedingsofthe59thannualmeetingoftheassociationforcomputationallinguisticsandthe11thinternationaljointconferenceonnaturallanguageprocessing,pages6142–6152august1–6,2021.©2021associationforcomputationallinguistics6142to be relevant to the image.
for instance, it oftencontains greetings and chit-chats of other topics, asthe ﬁrst two turns in figure 1 shows.
in order tosuggest the relevant photo, a smart system needsto decide which part of the dialogue can be usedfor suggesting the image.
in contrast, in the tradi-tional image-text tasks, the correct text is designedto be highly correlated with the image and has fewdistracting content.
these photo sharing character-istics makes inferring the connection between theimage and textual utterances challenging..to highlight these challenges, we create pho-tochat - a human-human dialogue dataset in whichone photo is shared from one person to the otherduring the conversation1.
it is, as far as we know,the ﬁrst dataset that captures the photo sharing ac-tivities.
we selected images from openimage v4dataset (kuznetsova et al., 2020) as shared pho-tos and used crowdsourcing plugins to generate12,286 dialogues with an average of 10 turns perdialogue.
during the dialogue collection, the photois only visible to the side who is instructed to sharethe photo and then to both sides after it is beingshared.
based on the collected dataset, we pro-pose two tasks that are essential for building aphoto suggest system: photo-sharing intent pre-diction task that predicts whether one intends toshare the photo in the next conversation turn, anddialogue-based image retrieval task that retrievesthe most relevant photo given the dialogue context.
for both, we build baseline models, report and an-alyze their performances.
the best photo-sharingintent prediction baseline model achieves 58.1% f1score with 58.2% precision and 57.9% recall.
thebest cross-attention image retrieval model achieves10.4% recall@1 out of 1000 candidates.
we alsopropose a dual-encoder model that leverages objectlabels to encode image features, which achievesthe best performance among all the models w/ocross-attention mechanisms..in summary, our main contributions are:• we create the ﬁrst human-human dialoguewith photo sharing acts via crowd-sourcing.
• we propose two new tasks to promote build-ing an intelligent photo suggest system.
• we build baseline models and provide bench-marks for the new tasks.
our proposed imageretrieval model outperforms all the prior mod-els w/o cross-attention mechanisms.
we im-.
1https://github.com/google-research/google-.
research/tree/master/multimodalchat/.
plement comprehensive analysis and ablationstudy to provide more insights..2 related work.
with the recent advances in deep learning, plentyof image-text datasets have been created and newimage-text tasks are proposed based on them.
these datasets have greatly stimulated the devel-opment of joint image-text models.
in this section,we review the widely used image-text datasets andthe state-of-the-art (sota) approaches for solvingthe image-text problems..2.1.image-text dataset.
image-captioning datasets are ﬁrst widely used forjoint image-text modeling.
mscoco (lin et al.,2014) and flickr30k (young et al., 2014) that bothcontain ﬁve written caption descriptions for eachimage are the representative ones used for auto-mated caption generation and cross-modal retrievaltasks.
conceptual caption (sharma et al., 2018)is yet another popular image caption dataset butcontains an order of magnitude more images thanmscoco.
because image captions usually onlydescribe the main objects in the image and omit de-tails, to facilitate understanding details of an imagealong with the reasoning behind them, antol et al.
(2015) introduced vqa which contains three ques-tion answer pairs for each image.
a further work isvcr (zellers et al., 2019) that not only requires amodel to answer the question derived from the im-age but also provides a rationale explaining why itsanswer is right.
it was created to teach the modelto learn higher-order cognition and commonsensereasoning about the world..compared to the work above, image-chat (shus-ter et al., 2020) and iga (mostafazadeh et al.,2017), which focus on the dialogues grounded inthe image, are the most related work to ours.
igaincludes 4k dialogues where each contains an im-age with a textual description of it, along with thequestions and responses around the image.
due toits small scale, iga can only be used for evaluation.
image-chat is a larger scale dataset that consistsof 202k image-grounded dialogues.
however, bothof them were created by asking the crowd workersto talk about a shared image to generate engagingconversation, which is different from the scenarioof photo sharing where only one side can access thephoto at the start of the conversation.
thus, neithercan be used to build a photo-suggest system.
in our.
6143work, we build a new dataset that highlights thechallenges of building a photo-suggest system andis the ﬁrst of its kind to the best of our knowledge..2.2.image-text modeling.
as the challenge for the photo-suggest system isto retrieve the most relevant image based on thetextual utterances, we only review the related workon cross-modal retrieval..many models have been proposed for image-caption retrieval where one is required to retrievethe most relevant caption given an image or viceversa.
the typical architecture consists of two sep-arate encoders for image and text to ﬁrst generatevisual and textual embeddings.
on top of them, afusion layer, which can simply be a dot product, isused to generate the relevance score for each pair(frome et al., 2013; kiros et al., 2014; parekh et al.,2020; karpathy and fei-fei, 2015; faghri et al.,2018).
then a triplet ranking loss or cross-entropyloss is employed to learn the latent visual-semanticalignment.
vse++ (faghri et al., 2018) emphasizeson the hardest negatives by using the max of thehinge loss as the objectives and yielded a signiﬁcantperformance improvement.
stacked cross atten-tion network (scan) (lee et al., 2018) furtherimproves the performance by introducing the crossattention between image regions and word features.
recently, cross-modal transformer based architec-ture that are pretrained on large-scale image-textdatasets via self-supervised learning has showngreat advantages in bridging visual and textualembeddings.
multiple concurrent work (lu et al.,2019; chen et al., 2020; li et al., 2019) have re-freshed the best records on the benchmark datasetsfor the image-text retrieval tasks..3 dataset creation.
we select photos from open image dataset v4(oid) (kuznetsova et al., 2020) and collect open-ended conversations on amazon mechanical turk.
below describes the detailed image ﬁltering, con-versation generation, and data veriﬁcation steps toensure data quality..3.1.image-based filtering.
since oid is large-scale and comprehensive, itcontains images that are unlikely to be shared in thedaily dialogue, such as images only about remotecontrols or ﬁre hydrants.
to create a dataset that isclose to the reality, we ﬁlter images based on the.
annotated object labels provided with oid..based on our.
investigation of.
the image-grounded dialogues and daily experiences, photosabout four themes are commonly shared: people,food, animal, and product (in the shopping sce-nario), which are our focus in the dataset creation.
from all the 600 object labels that appear in oid,we ﬁrst enlist the labels that both belong to one ofthe four themes and have a high chance to appear inthe commonly-shared photos.
labels like “trafﬁclight”, “nail”, and “reptile” are excluded and labelslike “girl”, “bagel”, and “camera” are included.
this process selects 89 object labels (appendix).
we then generate an image pool by selecting thosethat contain any of the objects in the list.
notethat for the objects of the people category, we addanother criteria that it must be the main object, i.e.
neither positioned in the margin of the image2 norextremely small 3 to exclude images that only havepeople as the background.
images are randomlyselected from the image pool to generate conversa-tions in the next step..3.2 conversation generation.
we randomly assigned two crowd workers to gener-ate a conversation based on a given image.
theimage comes with an image description whichpresents the list of objects labels in the image.
when the image contains humans, we assign arandom name and relationship to one of the hu-mans to help the workers refer to it and unfold thestory.
they are instructed to imagine talking withtheir friend.
at the start of the task, only one sidehas access to the image and is instructed to drivethe dialogue until it is ﬁt to share the image withthe other (website interfaces are shown in the ap-pendix).
it is not restricted that they must messagealternatively but the worker with the photo can’tshare the photo until the total number of the conver-sation turns reaches ﬁve.
after sharing the photo,they can continue to chat until they wish to end theconversation and submit the dialogue..3.3.image&text-based veriﬁcation.
lastly, we use another set of in-house professionalcrowd workers to ﬁlter out the invalid dialoguesgenerated in the above step.
dialogues are dis-carded if the association between the image and thedialogue is in-evident before the photo sharing act.
2center of the object is located within 0.1 of the image.
width/height to the border..3object width/length < 0.3 × (image width/length)..6144good example.
good example.
bad example.
a: hows it going?
b: just got back from vacation!!
a: how was vacation?
did you havefun?
b: it was exciting!
i took my grand-daughter to greece and we saw so manybeautiful ruins!
a: oh wow!
greece, that’s amazing.
i bet you got amazing pictures of theruinsb: yeah, we saw ancient temples andbattleﬁeldsb: share the photoa: wow!
should post it on insta too.
b: great idea!
thanks!.
that’s a great photo.
you.
a: hey guess what i’m doing now ??
b: what are you up to today?
a: i’m preparing a pizza for the ﬁrsttime i include tomatoes,onions and soonb: wow, you must be daring!
whoevertaught you should have been conﬁdenton your progress.
a: hey..... i’m almost doneb: must be yummy¿a: wanna see my preparation?
a: share the photo.
a: how are you?
b: i’m doing well.
i’ve been watchingnetﬂix because i can’t go outside.
a: yeah, same here.
which show?
a: and actually, i just found this pic-ture of someone who should be a pho-tographer.
b: the ofﬁce has been my go to.
b: really?
share the photo to me.
a: share the photob: whoa!
you were totally righta: it’s a boy in neon green who i thinkwants to take photos in academic set-tings.
b: this photo is so cool.
figure 2: examples of photochat dataset.
the ﬁrst two examples are included in the dataset while the last exampleis excluded in the veriﬁcation step.
share the photo denotes the photo sharing act..or the content is unnatural, contains inappropriatewords, too many typos or broken english.
figure 2displays examples of qualiﬁed and unqualiﬁed data.
note that the third unqualiﬁed dialogue can hap-pen in a real conversation, yet the content/eventof the image is not mentioned until the photo be-ing shared, making it impossible for a model tolearn the connection between the dialogue and theimages and to suggest a photo in advance.
suchdialogues are removed from the dataset in this step..4 dataset statistics.
the table.
in addition, some images in the trainingset are used in multiple dialogues..based on the statistics in the table, the averagenumber of turns per dialogue is 12.7 and the aver-age number of tokens per turn is 6.3. since twosides are not restricted to speak alternatively, ifthe consecutive turns from the same side are com-bined as one turn, which is the conventional settingof other dialogue datasets, the average number ofturns per dialogue and the average number of to-kens per turn become 9.5 and 8.5. on average, peo-ple converse for 7 turns before sharing the photo..the collected dataset consists of 10,917 unique im-ages and 12,286 dialogues.
one image is sharedin each dialogue.
based on the object labels of theshared image, we classify the dialogues into fourcategories: people, food, animals, and daily prod-ucts.
we split the dialogues into 10,086 train, 1,000dev, and 1,000 test sets while keeping roughly thesame distribution of the category across the splits.
the detailed statistics of each split and in totalare shown in table 1. note that the dialogue canhave multiple category labels.
for instance, if theshared image is about a girl playing with dogs, thedialogue belongs to both people and animals cat-egories.
thus, the sum of the dialogues of eachcategory (people/animal/food/product dial #) ex-ceeds the total number of the dialogues (dial #) in.
5 task deﬁnition.
we decompose the problem of building a smartphoto-suggest system into two separate tasks.
theﬁrst is to detect if the user has the intent to share thephoto in the next turn, which we call photo-sharingintent prediction task.
the second is to retrieve thephoto based on the dialogue context, which we callimage retrieval task.
below describes the formalformulation of the problem settings..let p = {p1, p2, ..., pm } be the photo set whereeach pi = (ai, li), i ∈ [1, m ] consists of image aiand a list of objects li in it.
given the dialogue d ={t1, ..., th, pk, th+1, ..., tn } where two participantsspeak alternatively, tj (j ∈ [1, n ]) and pk ∈ prespectively represent the utterance of turn j and.
6145table 1: photochat statistics.
table shows the aggregated numbers.
from left to right starting from the secondcolumn, the name of each column means “the unique number of images”, “the number of dialogues”, “the numberof dialogues about people/food/animal/product”, “the number of turns”, “the number of turns when counting con-secutive turns of the same speaker as one turn”, and “the number of tokens”.
turns in which photos are shared areexcluded in the calculation..split.
traindevtesttotal.
uniqueimg #8,9171,0001,00010,917.dial #.
10,2861,0001,00012,286.peopledial #6,3766066157,597.fooddial #4,4654244195,308.animaldial #1,07287901,249.productdial #8841091081,101.turn #.
turn* #.
token #.
130,54612,70112,852156,099.
97,5869,5339,590116,709.
827,15480,21480,847988,215.the shared image.
th is the turn immediately beforea photo sharing act.
we also deﬁne the speakerinformation s = {s1, s2, ..., sn } where sj (j ∈[1, n ]), either 0 or 1, denotes the speaker of turn j.photo-sharing intent prediction: the goal ofthe intent prediction task is to predict whether aphoto will be shared in the next turn for any tj givenall the turns before.
in equation, it’s formulated asa binary classiﬁcation task:.
∀j ∈ [1, h], c(t1:j, s1:j) ∈ {0, 1},.
(1).
where c is the intent prediction model taking theutterances and the speaker information of all theprevious turns as the input and outputs a binaryvalue.
in the above case, it should only predicts1 when j = h, otherwise 0. note that whetherthe model make use of all the previous turns andthe speaker information depends on the model de-sign.
we use f1 score, precision, and recall as theevaluation metrics for this task..image retrieval: under the same settings,model r of the image retrieval task is expectedto correctly retrieve pk from p given the dialogue:.
r(t1:h, s1:h, p ) ∈ [1, m ]..(2).
during training, the candidate pool p is usuallycomprised of in-batch images while during evalua-tion, p contains all images in the test set.
follow-ing karpathy and fei-fei (2015), we use recall@k(r@k), computed as “the fraction of times a cor-rect item was found among the top k results” as theevaluation metrics.
speciﬁcally, we choose r@1,r@5, and r@10, as well as the sum of them whichwe denote as “sum(r@1, 5, 10)” to evaluate themodels..6 baselines.
6.1 photo-sharing intent prediction model.
to establish the baselines, we ﬁne-tune three sotapretrained models - bert (devlin et al., 2018a),.
albert (lan et al., 2020), and t5 (raffel et al.,2020), as the pretrained models have achieved re-markable performance in many nlp tasks..to adapt bert and albert to our settings,we concatenate all the previous turns (t1:j in equa-tion 1) by [sep] and prepend the concatenatedtext with [cls] to generate the input to the model.
we use the speaker information s1:j as the segmentid of the input.
the output of [cls] token is fedinto two fully-connected layers, of which the outputdimensions are respectively 128 and 2 to generatethe ﬁnal prediction.
to utilize t5, we concatenatet1:j by [sep] and prepend the text with “predictshare intent:” as the model input.
we use crossentropy loss for all three models..6.2.image retrieval model.
our baselines consists of both statistical and neuralnetwork-based approaches, as elaborated below:.
dual encoder: we built a dual-encoder modelsimilar to parekh et al.
(2020); gillick et al.
(2018),which separately encodes image and text leveragingsota pre-trained models.
its entire architecture isshown in figure 3..to encode the image, for each pi = (ai, li) weﬁrst resize the image ai to 224 × 224 and feed itinto a pretrained resnet (he et al., 2016) to gen-erate ai.
a pretrained bert is used to encodeli to achieve the label embedding li which is theoutput of [cls] token.
li is concatenated withai to generate the image embedding.
for encodingthe dialogue context, we use a second pretrainedbert (devlin et al., 2018b).
its input is the con-catenation of all the prior utterances of the speakerwho shares the photo.
the output of [cls] tokenis used as the contextual text embedding.
two fullyconnected layers are then used to separately projectimage and text embeddings into a joint image-textembedding space of dimension h. then, the dotproduct of the normalized image embedding bi.
6146figure 3: our dual encoder.
the ﬁrst dialogue in figure 2 is used as the input example.
image and text are encodedseparately to generate their embeddings.
the dot product of them is then used to compute the similarity score..and text embedding tj is used as the similarityscore s(bi, tj).
following young et al.
(2014);gillick et al.
(2018), bidirectional in-batch sampledcross entropy loss is employed:.
lsm(bi, tj) = −(s(bi, tj) − log.
(cid:88).
es(bi, ˆtj )).
−(s(bi, tj) − log.
es( ˆbi,tj )),.
ˆtj(cid:88).
ˆbi.
where ˆbi and ˆtj are the image embeddings andtext embeddings of the other examples in the batch.
we also experiment with bidirectional in-batch.
hinge loss, deﬁned as:.
lsh(bi, tj) =.
[α − s(bi, tj) + s(bi, ˆtj)]+.
+.
[α − s(bi, tj) + s( ˆbi, tj)]+,.
(cid:88).
ˆtj(cid:88).
ˆbi.
where α is the margin parameter and [x]+ ≡max(x, 0) .
in our preliminary experiments, weobserve cross entropy loss works better and imple-ment most experiments with cross entropy loss..vse++: vse++ (faghri et al., 2018) is a simpleand effective dual encoder model.
it encodes theimage and the text, which is the concatenation ofall the previous utterances of the person who sharesthe photo in our case, separately by resnet152 (heet al., 2016) and gru (cho et al., 2014).
it is thenfollowed by linear projections to map them into thejoint embedding space.
finally, dot products of thenormalized embeddings are used to compute theranking scores.
they innovatively make use of thehardest negatives, which are the negatives closestto the query, in the ranking loss function:lmh(bi, tj) = [α − s(bi, tj) + s(bi, ˆt h.j )]+.
+[α − s(bi, tj) + s( ˆbhi , tj)]+,where ˆt hj = argmax(s(bi, ˆtj)) and ˆbhargmax(s( ˆbi, tj)) are the hardest negatives..i =.
scan: scan (lee et al., 2018) is a full crossattention model that captures the ﬁne-grained inter-play between image regions and text tokens to inferimage-text similarity.
it uses fasterrcnn (renet al., 2017) in conjucntion with resnet-101 tocompute image region embeddings and bidirec-tional gru to achieve text embeddings.
sameas vse++, scan uses hard negatives in the tripleranking loss function.
though it beats vse++ onthe image captioninig tasks, it doesn’t scale wellto large-scale retrieval problems due to the highcomputational cost of cross attention..bm25: bm25 (amati, 2009) is a probabilis-tic retrieval function widely used for documentretrieval.
to adapt it to our settings, we directlyutilize the object labels of each image lj, j ∈ [1, m]as the document term.
all the utterances beforephoto is shared are concatenated, tokenized andused as the query term to retrieve the image..7 experiments.
7.1 setup.
the maximum sequence length of bert, albert,and t5 for the photo-sharing intent prediction taskis 512. we choose checkpoints that achieve thebest f1 score on the dev set for evaluation on thetest set..for our dual encoder model, the maximum se-quence length of bert is 128, the dimension of thejoint image-text embedding space h is 512, andmargin parameter α is 0.2 for all the experiments.
all parameters are trainable.
we use the adamoptimizer (β1 = 0.9, β2 = 0.999) and a learningrate that starts at 5e-5 and decays by 0.1% every1000 steps.
the models are trained on 32-core podslices of cloud tpu v3 pod, with a per-replicabatch size of 4. the loss is computed on item pairsaggregated from all replicas, which is ovegr theglobal batch of 128 samples in this case..for vse++ and scan models, as gru is not apretrained encoder, directly training them on pho-.
6147bertresnetfc[cls] clothing girl face [sep]  normbert[cls] just got back …... temples and battlefields [sep]fcnormbitj dot products(bi ,tj ) liailiaitable 2: experimental results of the baseline modelsfor the photo-sharing intent prediction task.
all num-bers are in percentage..modelalbert-basebert-baset5-baset5-3b.
f1 ↑52.253.258.158.9.precision ↑ recall ↑.
44.856.158.254.1.
62.750.657.964.6.table 3: number of negative turns and positive turnsin each split of the dataset for the photo-sharing intentprediction task..split number of negatives number of positivestraindevtest.
10,2861,0001,000.
68,7956,8026,748.tochat yields unpleasant results.
as such, we ﬁrsttrain them on mscoco and ﬁnetune them on pho-tochat for 20 epochs.
we utilize the same settingas the single models that are reported to performthe best on the image-retrieval task on mscoco;more speciﬁcally, vse++ (resnet, ft) and scant-i avg (λ1 = 9) following the annotations in theoriginal papers..7.2 results of intent prediction.
table 2 presents model performance on the test set.
we observe that t5 outperforms bert and al-bert in all metrics.
note that our dataset suffersfrom class imbalance that the negative examplesoutnumber the positive examples 3 , which we sus-pect causes the low precision across all the models.
figure 4 shows examples of the prediction by t5-3b model.
though a few turns are falsely predictedas positive (e.g.
“they were really pretty.” and thesecond to last turn in example 2), it’s possible forthe speaker to share the photo after this turn inreal life, indicating that when to share a photo issubjective and the model may be more viable thanthe low precision would suggest.
we also anticipateif the model has access to the set of photos thespeaker can share, the accuracy can be elevated.
in this case, the model will be able to infer thatthe photo in example 1 and 2 of figure 4 are morelikely to follow utterances about food and statues..7.3 results of image retrieval.
table 4 lists the experimental results on photochat.
our dual encoder model is denoted as de.
deimgand delabel are the ablation models that only takethe image ai or image labels li as the input com-pared to the default architecture in figure 3. ce,sh, mh represents cross entropy loss, hinge loss,.
example 1.example 2.
...b: that’s good.
i took theday off to spend with isa.
a: wowb: it’s our anniversary.
a: really needed some-timesb: we are getting brunchright now.
have you beento the blue herron cafe?
a: no i haven’t.
b: they have a beautifulbalcony.
a: tell me about it anythingto share?
b: check out these amaz-ing wafﬂes!.
...b: pretty good, i spent theday at the beach with myfamilya: that sounds fun whereat?
b: spain.
they had manystatues out on the beacha: i love the beach wowsounds beautiful!!!
b: they wereprettya: did you take pics?
b: i think so... therewas this one sculpturethat was unique... and thebirds seemed to like it toohahaa: oh let me see that!.
really.
figure 4: predictions by t5-3b model for the intentprediction task.
turns with underline are predicted aspositive.
false positives are marked in red while truepositives are marked in blue.
best viewed in color..and hinge loss using hard negatives.
we attempttraining de on mscoco ﬁrst and ﬁnetuning it onphotochat.
these models are specially annotatedwith *.
we also experiment with different imageencoders: resnet-50 and resnet-152, in combina-tion with different label encoders: bert-base andbert-tiny.
they are annotated in the brackets af-ter the model names in table 4. among all themodels, scan achieves the best performance with10.4% r@1, 27% r@5, and 37.1% r@10, whichis consistent with the prior work (lee et al., 2018),demonstrating the power of the bottom-up crossattention.
among all the models that don’t havecross-attention, our model de*(resnet-152, bert-tiny) performs the best and beats a strong priorwork vse++, indicating the effectiveness of usingimage labels in the retrieval task..ablation study: by comparing delabel(bert-base) and deimg(resnet-152), we ﬁnd that usingimage features is more effective than using imagelabel features, which is expected as images con-tain more information.
compared to the modelusing only image pixel values (deimg(resnet-152)), adding the label features contributes to anincrease of 1.3% in sum(r@1, 5, 10) to 66.4%(de(resnet-152, bert-base)).
pretraining themodel on mscoco further boosts it by 3.5% to.
6148table 4: experimental results of the baseline models on image retrieval task.
de stands for our proposing dualencoders.
deimg only uses the image pixel values and delabel only uses image labels to extract image features.
de* is the model pretrained on mscoco.
all numbers are in percentage..loss function r@1 ↑ r@5 ↑ r@10 ↑.
modelbm25delabel(bert-base)deimg(resnet-50)deimg(resnet-152)de(resnet-152, bert-base)de*(resnet-152, bert-base)de*(resnet-152, bert-tiny)de*(resnet-152, bert-base)de*(resnet-152, bert-tiny)vse++scan.
-cecececeshshcecemhmh.
6.66.76.76.88.18.07.18.59.010.210.4.
15.422.121.924.023.722.023.326.126.425.427.
23.031.232.334.334.631.033.035.335.734.237.1.sum(r@1, 5, 10)↑45.060.060.965.166.461.063.469.971.169.874.5.
69.9% (de*(resnet-152, bert-base))..effect of encoders: we observe that usinga smaller model (bert-tiny) to encode image la-bels yields better performance regardless of theloss function.
de*(resnet-152, bert-tiny) im-proves sum(r@1, 5, 10) by 1.2% compared tode*(resnet-152, bert-base) when using cross en-tropy loss and 2.4% when using hinge loss.
the rea-son might be that labels are a compact list of tokensand thus, using a smaller model alleviate the prob-lem of overﬁtting.
on the other hand, using a largerimage encoder resnet-152 produces better resultsthat deimg(resnet-152) beats deimg(resnet-50)in sum(r@1, 5, 10) by 4.2%..effect of loss function: our dual encoders worksigniﬁcantly better with cross entropy loss thanhinge loss and their gap is about 8% in sum(r@1,5, 10) as we compare the results of de*(resnet-152, bert-base) and de*(resnet-152, bert-tiny)models under different loss functions..error analysis: figure 5 shows the qualitativeresults of de*(resnet-152, bert-tiny) given a textquery.
in the ﬁrst example, the model ranks therelevant images of wine glasses and black tea at topinstead of the groundtruth image where a man isholding a wine glass, which is easy to be neglected.
in the second example, the model fails to distin-guish pufﬁns with ducks and infer the backgroundfrom keyword “atlantic”.
it illustrates the challengeof the image retrieval task under the dialogue con-text that it requires a model to pay attention to thedetails and the event, as discussed in section 1.figure 6 presents more prediction results includingsome wrong predictions by the model..figure 5: predictions by de*(resnet-152, bert-tiny)for the image retrieval task.
for each dialogue query,we show the groundtruth (ﬁrst image in green) and thetop-2 ranked images (in red).
best viewed in color..8 conclusion.
we collected a 12k high-quality dialogue datasetthat contains photo sharing activity via crowd-sourcing.
to facilitate research on building intel-ligent photo-suggest system, we have introducedtwo new challenging tasks that aim at improvingthe photo-sharing experience: photo-sharing intentprediction task and image retrieval task.
that is,when given a dialogue, the system should predictwhether the user has the intention to share the photoand which photo is suitable to be shared.
we builtbaseline models for both tasks and report their per-formance with detailed analysis..besides the proposed two new tasks, our datasetcan potentially be used in other dialogue relatedtasks, such as dialogue generation in the multi-modal dialogues, as well as inspiring new research.
6149a: we're missing you over here at the bar!b: oh..that was unfaira: yeah, sorry you couldn't make it.
we're having wineb: oh..that is interesting to know enjoy guys..a: i forget, do you prefer red or white?
we got a nice red for the tablea: do you like puffins in atlanticb: never heard of thata: i had a photo of puffinb: coola: i thought you would like thatfigure 6: predictions by de*(resnet-152, bert-tiny) for the image retrieval task.
for each dialogue query, weshow the top-5 ranked images from left to right.
the ground-truth image is marked in green while the others are inred.
best viewed in color..topics, such as composing automatic reply to thephotos sent from others.
we hope our dataset andmodeling work can be beneﬁcial for studies thatfocus on the interplay between image and dialogue..acknowledgments.
we thank pranav khaitan and blaise aguera y arcas for thesupport and assistance; yinfei yang, david bieber for review-ing the draft and providing the feedback; janel thamkul andtulsee doshi for doing the legal review of the dataset..6150a: heu, how are you?b: im fine, having a day at the aquarium!a: oh, i love going to the aquarium!b: next time i will invite you!
we are enjoying the sea lionsa: thanks, i would go for sure!
what all did you see there?b: the water looks perfectly clean todaya: the sea lions are always fun to watch!
what was your favorite thing?b: the sea lion swimming there are my favorite i love yo see them enjoying the watera: they have lots of funny anticsa: how are you?
i just bought a new jacketb: ok, trying to keep busy these days.a: makes sense my jacket came in two sizes too smallb: cool - i hope you really like your new jacketa: it's not great i'm going to return itb: oh, no - can you exchange it?a: no i might get money baack not entirely sureb: i hope you can get your money backa: it's a children's size, but i'm an adulta: how do you like the school picnic photo ?
b: yes i doa: i think alisha and her friends are adorable !
b: that is lovelya: i hope to get it copied and framed for each of the parents as well as the teacherb: that greata: how happy they all look , makes me so envious as to being a child .
b: smilesa: my day was pretty uneventful.
just watching tv nowb: that is how most of my days go by.
any plans for vacation?a: going to big bear for 4th of july, you?b: i want to take my kid to where you went not too long ago with jordana: where?
i"ve been lots of places recentlyb: i think it was disney but not too sure.
thats what i wanted to ask you there is a picture of your kid smiling with a chipmunk.
thats where i want to take hima: i went to disney world last summer.
still need to try disney land!
references.
giambattista amati.
2009. bm25, pages 257–260..springer us, boston, ma..peter anderson, xiaodong he, chris buehler, damienteney, mark johnson, stephen gould, and leizhang.
2018. bottom-up and top-down attention forimage captioning and visual question answering..stanislaw antol, aishwarya agrawal, jiasen lu, mar-garet mitchell, dhruv batra, c. lawrence zitnick,and devi parikh.
2015. vqa: visual question an-swering.
in international conference on computervision (iccv)..yen-chun chen, linjie li, licheng yu, ahmed elkholy, faisal ahmed, zhe gan, yu cheng, andjingjing liu.
2020. uniter: universal image-textrepresentation learning..kyunghyun cho, bart van merri¨enboer, caglar gul-cehre, dzmitry bahdanau, fethi bougares, holgerschwenk, and yoshua bengio.
2014.learningphrase representations using rnn encoder–decoderfor statistical machine translation.
in proceedings ofthe 2014 conference on empirical methods in nat-ural language processing (emnlp), pages 1724–1734, doha, qatar.
association for computationallinguistics..jacob devlin, ming-wei chang, kenton lee, andkristina toutanova.
2018a.
bert: pre-training ofdeep bidirectional transformers for language under-standing..jacob devlin, ming-wei chang, kenton lee, andkristina toutanova.
2018b.
bert: pre-training ofdeep bidirectional transformers for language under-standing.
arxiv preprint arxiv:1810.04805..william dutton and grant blank.
2013. cultures ofthe internet: the internet in britain.
oxford internetsurvey 2013 report..fartash faghri, david j fleet, jamie ryan kiros, andimproving visual-.
sanja fidler.
2018. vse++:semantic embeddings with hard negatives..andrea frome, greg s corrado, jon shlens, samy ben-gio, jeff dean, marc aurelio ranzato, and tomasmikolov.
2013. devise: a deep visual-semanticembedding model.
in c. j. c. burges, l. bottou,m. welling, z. ghahramani, and k. q. weinberger,editors, advances in neural information processingsystems 26, pages 2121–2129.
curran associates,inc..daniel gillick, alessandro presta, and gaurav singhtomar.
2018. end-to-end retrieval in continuousspace..k. he, x. zhang, s. ren, and j. sun.
2016. deep resid-in 2016 ieeeual learning for image recognition.
conference on computer vision and pattern recog-nition (cvpr), pages 770–778..kaiming he, xiangyu zhang, shaoqing ren, and jiansun.
2016. deep residual learning for image recog-in proceedings of the ieee conference onnition.
computer vision and pattern recognition, pages 770–778..a. karpathy and l. fei-fei.
2015. deep visual-semantic alignments for generating image descrip-tions.
in 2015 ieee conference on computer visionand pattern recognition (cvpr), pages 3128–3137..ryan kiros, ruslan salakhutdinov, and richard s.zemel.
2014. unifying visual-semantic embeddingswith multimodal neural language models..alina kuznetsova, hassan rom, neil alldrin, jasperuijlings, ivan krasin, jordi pont-tuset, shahabkamali, stefan popov, matteo malloci, alexanderkolesnikov, tom duerig, and vittorio ferrari.
2020.the open images dataset v4: uniﬁed image classiﬁ-cation, object detection, and visual relationship de-tection at scale.
ijcv..zhenzhong lan, mingda chen, sebastian goodman,kevin gimpel, piyush sharma, and radu soricut.
2020. albert: a lite bert for self-supervised learn-ing of language representations..kuang-huei lee, xi chen, gang hua, houdongstacked cross at-arxiv preprint.
hu, and xiaodong he.
2018.tention for image-text matching.
arxiv:1803.08024..a. lenhart, rich ling, s. campbell, and k. purcell.
2010. teens and mobile phones: text messagingexplodes as teens embrace it as the centerpiece oftheir communication strategies with friends..gen li, nan duan, yuejian fang, ming gong, daxinjiang, and ming zhou.
2019. unicoder-vl: a univer-sal encoder for vision and language by cross-modalpre-training..tsung-yi lin, michael maire, serge belongie, jameshays, pietro perona, deva ramanan, piotr doll´ar,and c. lawrence zitnick.
2014. microsoft coco:common objects in context.
lecture notes in com-puter science, page 740–755..katharina lobinger.
2016. photographs as things –photographs of things.
a texto-material perspectiveon photo-sharing practices.
information, communi-cation & society, 19(4):475–488..jiasen lu, dhruv batra, devi parikh, and stefanlee.
2019. vilbert: pretraining task-agnostic visi-olinguistic representations for vision-and-languagetasks..nasrin mostafazadeh, chris brockett, bill dolan,michel galley, jianfeng gao, georgios spithourakis,and lucy vanderwende.
2017.image-groundedconversations: multimodal context for natural ques-in proceedings oftion and response generation.
the eighth international joint conference on natu-ral language processing (volume 1: long papers),.
6151pages 462–472, taipei, taiwan.
asian federation ofnatural language processing..zarana parekh, jason baldridge, daniel cer, austinwaters, and yinfei yang.
2020. crisscrossed cap-tions: extended intramodal and intermodal semanticsimilarity judgments for ms-coco..colin raffel, noam shazeer, adam roberts, kather-ine lee, sharan narang, michael matena, yanqizhou, wei li, and peter j. liu.
2020. exploringthe limits of transfer learning with a uniﬁed text-to-text transformer.
journal of machine learning re-search, 21(140):1–67..shaoqing ren, kaiming he, ross girshick, and jiansun.
2017. faster r-cnn: towards real-time ob-ject detection with region proposal networks.
ieeetrans.
pattern anal.
mach.
intell., 39(6):1137–1149..piyush sharma, nan ding, sebastian goodman, andradu soricut.
2018.conceptual captions: acleaned, hypernymed, image alt-text dataset for au-tomatic image captioning.
in proceedings of acl..kurt shuster, samuel humeau, antoine bordes, and ja-son weston.
2020. image-chat: engaging groundedin proceedings of the 58th annualconversations.
meeting of the association for computational lin-guistics, pages 2414–2429, online.
association forcomputational linguistics..peter young, alice lai, micah hodosh, and julia hock-enmaier.
2014. from image descriptions to visualdenotations: new similarity metrics for semantic in-ference over event descriptions.
transactions of theassociation for computational linguistics, 2:67–78..rowan zellers, yonatan bisk, ali farhadi, and yejinchoi.
2019. from recognition to cognition: vi-sual commonsense reasoning.
in the ieee confer-ence on computer vision and pattern recognition(cvpr)..a dataset creation & details.
the website interfaces used to collect dialogues arepresented in figure 7 and 8..table 5 shows the 89 object labels that we usedto select the photos from open image dataset forgenerating dialogues..figure 7: website interface of the conversation gener-ation task.
it is only visible to the side who shares thephoto..figure 8: website interface of the conversation genera-tion task.
it is only visible to the side who receives thephoto..table 5: object labels we use for image ﬁltering..cate-gorypeople.
food.
ani-mals.
prod-ucts.
object labels.
woman, man, girl, boy, human body, facebagel, baked goods, beer, bread, burrito,cake, candy, cheese, cocktail, coffee,cookie, croissant, dessert, doughnut, drink,fast food, french fries, hamburger, hot dog,ice cream, juice, milk, pancake, pasta, pizza,popcorn, salad, sandwich, seafood, snack,taco, tart, tea, wafﬂe, wine, guacamole.
animal.
alarm clock, backpack, blender, banjo, bed,belt, computer keyboard, computer mouse,curtain, guitar, hair dryer, hair spray,harmonica, humidiﬁer, jacket, jeans, dress,earrings, necklace, fashion accessory,bicycle, blender, calculator, camera, foodprocessor, jug, mixing bowl, nightstand,oboe, oven, paper cutter, pencil case,perfume, pillow, personal care, pizza cutter,pressure cooker, printer, refridgerator, highheels, skateboard, slow cooker, teddy bear,teapot, vase, wall clock.
6152