smurf: semantic and linguistic understanding fusion for captionevaluation via typicality analysis.
joshua feinglass and yezhou yangarizona state university{joshua.feinglass,yz.yang}@asu.edu.
abstract.
the open-ended nature of visual captioningmakes it a challenging area for evaluation.
themajority of proposed models rely on special-ized training to improve human-correlation, re-sulting in limited adoption, generalizability,and explainabilty.
we introduce “typicality”,a new formulation of evaluation rooted in in-formation theory, which is uniquely suited forproblems lacking a deﬁnite ground truth.
typ-icality serves as our framework to develop anovel semantic comparison, sparcs, as wellas referenceless ﬂuency evaluation metrics.
over the course of our analysis, two separatedimensions of ﬂuency naturally emerge: style,captured by metric spurts, and grammar,captured in the form of grammatical outlierpenalties.
through extensive experiments andablation studies on benchmark datasets, weshow how these decomposed dimensions ofsemantics and ﬂuency provide greater system-level insight into captioner differences.
ourproposed metrics along with their combina-tion, smurf, achieve state-of-the-art corre-lation with human judgment when comparedwith other rule-based evaluation metrics1..figure 1: scatter plot utilizing standardizations ofsparcs and spurts.
the ground truth captionsare sourced from the karpathy test split of the cocodataset (chen et al., 2015; karpathy and fei-fei, 2015)with one used as a baseline for automatic caption-ers (cornia et al., 2020; pan et al., 2020; vinyals et al.,2015).
for each captioner, a 75% conﬁdence ellipse(1.15 standard deviations from the mean) is generated.
a caption near the centroid of each captioner is shownas an example along with the caption scores from 100randomly sampled images.
the normalized ellipseoverlap between an automatic captioner and humancaptions, h∩m aream area , gives an overall evaluation of typ-ical performance at a system-level on a scale of 0 to 1,with 1 being human-caption level..1.introduction.
visual captioning serves as a foundation for im-age/video understanding tools and relies on cap-tion evaluation for identifying promising researchdirections.
rule-based caption evaluation ap-proaches like the n-gram based cider (vedantamet al., 2015) and parsed semantic proposal basedspice (anderson et al., 2016) speciﬁcally are ableto provide researchers with meaningful feedbackon what their algorithm is lacking.
however, n-gram based methods are sensitive to stop wordsand sentence parsers are often inconsistent, leadingto liu et al.
(2017) showing that neither method.
1smurf source codes and data will be released at https:.
//github.com/joshuafeinglass/smurf..fully captures either the ﬂuency or the semanticmeaning of text.
more recently proposed metricsattempt to learn cues of caption quality by trainingmodels via image grounding techniques (cui et al.,2018) or human and generated captions (sellamet al., 2020).
these approaches, however, lack gen-erality, require domain speciﬁc training, and offerlittle insight for improving captioners, leading tonone of the proposed models being adopted for useas a caption evaluation benchmark.
we insteadpostulate that quality in semantics and descriptivelanguage is universally recognizable..the primary difﬁculty of caption evaluation isits cross-modal nature introducing ambiguity intothe expected output, resulting in a ground truth that.
proceedingsofthe59thannualmeetingoftheassociationforcomputationallinguisticsandthe11thinternationaljointconferenceonnaturallanguageprocessing,pages2250–2260august1–6,2021.©2021associationforcomputationallinguistics2250is no longer a single outcome, but a large set of po-tential outcomes of varying levels of quality.
fromthis problem setting, the novel concept of “typical-ity” arises naturally.
a desirable caption is one thatis atypical enough linguistically that it uniquely de-scribes the scene, follows typical natural languageprotocols, and matches a typical semantic descrip-tion of a scene..linguistically, the number of typical sequencesis characterized by the entropy rate (cover, 1999).
current work estimates the english language ashaving an entropy rate of only 1.44 bits/letter (taka-hashi and tanaka-ishii, 2018), implying that thetypical set of english is only a tiny fraction of thefull space of potential text.
self-attention trans-formers are language models that are able to iden-tify the distinguishing contextual features of thistypical set and as a result have now become the sta-ple of natural language understanding tasks.
herewe deﬁne typicality based on the distance of a can-didate text’s features from expected features of thetypical set.
we call this linguistic typicality esti-mation method model-integrated meta-analysis(mima) and use the function, fmima, to createreferenceless ﬂuency metrics attune to captioningneeds.
rather than assuming a predeﬁned evalu-ation task and introducing bias by ﬁne-tuning theself-attention transformer, our method extracts theinherent properties of language learned by trans-formers (devlin et al., 2019; liu et al., 2019) bytreating self-attention layers as probability distribu-tions as demonstrated in clark et al.
(2019).
ourapproach represents the ﬁrst integration of a ﬂu-ency speciﬁc metric that demonstrably improvescorrelation with human judgment for caption eval-uation..by removing stop words from the candidate text,fmima is able to create a metric that assesses a rel-atively new ﬂuency criteria in captioning: style.
we refer to this metric as stochastic process un-derstanding rating using typical sets (spurts).
style can be thought of as the instantiation of dic-tion and is necessary for generating human-levelquality captions.
stylized captions describe a muchsmaller set of media, leading to machines insteadgenerating the most typical caption that is still se-mantically correct.
this results in a signiﬁcant gapbetween machine and human captioners that can beseen in diction-based examples such as the use ofthe common words like “dog” and “food” insteadof more descriptive words like “schnauzer” and.
“lasagna”.
the other aspect of ﬂuency assessed byfmima is grammar.
unlike style, grammar is not es-sential for caption quality, however, highly atypicalsyntax can potentially lead to awkward captions, sowe develop a separate grammatical outlier penalty.
we then deﬁne a lightweight and reliable typi-cality based semantic similarity measure, semanticproposal alikeness rating using concept similar-ity (sparcs), which complements our reference-less metrics and grounds them to the reference cap-tions.
by matching word sequences, current meth-ods limit the scope of their evaluation.
instead, wetake non-stopword unigrams and further coalescethem into concepts through stemming, then com-bine the reference texts, like in yi et al.
(2020),using a novel semantic typicality measure of thereference text’s concepts to evaluate the semanticsimilarity of a candidate and reference text..spurts and sparcs can be used to assesssystem-level differences between captioners asshown in figure 1. based on this analysis, them 2 transformer lags behind 2015 models in termsof similarity to human captions, even though both2020 captioners achieved state-of-the-art resultsbased on cider standards.
this difference be-comes even more signiﬁcant when you considerthat the use of style makes it more difﬁcult for acaption to be semantically correct.
human cap-tions, m2 transformer (cornia et al., 2020), x-transformer (pan et al., 2020), and google (vinyalset al., 2015) incur a total grammar outlier penaltyof −44.93, −7.47, −7.56, and −4.46, respectively.
in order to provide caption-level insight as well,we combine spurts, sparcs, and our grammaroutlier penalty into one metric - semantic and lin-guistic understanding fusion (smurf) - whichrewards captions based on semantics and ﬂuency.
contributions: our key contributions are:1. a novel and widely-applicable model meta-analysis technique, mima, which estimates thetypicality of candidate text and which provides ameans of assessing transformer robustness.
2. three novel evaluation metrics useful for bothcaption-level and system-level evaluation: style-focused spurts, semantic-focused sparcs, andtheir combination which incorporates grammaticaloutliers as well, smurf.
3.experiments showing that sparcs andsmurf achieve sota performance in their re-spective areas of semantic evaluation and human-machine evaluation at both a system and caption-.
2251level.
4. evidence showing that the performance of auto-matic evaluation metrics has been underestimatedrelative to voting-based human evaluation metrics..2 related work.
originally, popular rule-based metrics from ma-chine translation that were mostly n-gram based,namely meteor (banerjee and lavie, 2005),bleu (papineni et al., 2002), and rouge (lin,2004), were used for caption evaluation.
vedan-tam et al.
(2015) introduced the more semanticallysensitive cider which uses tf-idf to identify distin-guishing n-grams and then compares them usingcosine similarity.
spice (anderson et al., 2016)greatly improved upon n-gram based approachesby using a sentence parser to generate semanticpropositions.
word moving distance scores (zhaoet al., 2019; kilickaya et al., 2017) have also beenused for semantic evaluation with limited success.
bertscore (zhang et al., 2019) used cosine simi-larity of embeddings from the self-attention trans-former, bert, and achieved state-of-the-art resultson coco but provided little interpretation of theirapproach..domain speciﬁc training approaches have alsobeen introduced with limited adoption.
cui et al.
(2018); jiang et al.
(2019); sharif et al.
(2019)present a training approach for caption evalua-tion where an image grounding and/or captionbased turing test is learned based on training datafrom human and machine captioners.
an adjustedbertscore (yi et al., 2020), bleurt (sellamet al., 2020), and nubia (kane et al., 2020) uti-lize transformer embeddings for comparison be-tween reference and candidate text, then performcaption dataset speciﬁc ﬁne-tuning of the modeldownstream..the importance of ﬂuency in captioning has beenwidely recognized.
liu et al.
(2017) attempted tointegrate cider and spice to create a cost func-tion attune to both lexicographical and semanticqualities for captioning optimization.
cui et al.
(2018) identiﬁed the presence of less frequent, dis-tinguishing words within human-generated text inthe coco dataset.
mathews et al.
(2018) recog-nized the importance of style in captions and in-tegrated it into their model without sacriﬁcing se-mantics..referenceless evaluation, ﬁrst proposed innapoles et al.
(2016) as a referenceless grammar.
error correction (gec) evaluation metric, hasbeen recognized as an effective avenue for ﬂuencyevaluation as a whole (asano et al., 2017), alongwith combined approaches (choshen and abend,2018).
more recently, perception score (gu et al.,2021) outlined a general paradigm for trainingreferenceless quality evaluation..3 our approach.
3.1 self-attention transformer background.
first introduced in vaswani et al.
(2017), transform-ers are made of layers of parallel attention headswhich extract contextual information about inputsusing attention.
they take in a sequence vectorof tokenized words from candidate text, yn, addstart and separator/end tokens, and pass the inputthrough a series of separate linear transforms withparameters, p, to create query, key, and value vec-tors, denoted as qi,ki,vi, respectively.
these vec-tors are then used to compute the attention weightparameters of the heads as shown:.
αij(yn, p) =.
exp(qti kj)l=1 exp(qt.
i kl).
,.
(cid:80)n.oi(yn, p) =.
αijvj,.
n(cid:88).
j=1.
(1).
(2).
i αij(yn, p)..where αij and oi are each layer’s attention weightsand output, respectively.
here αij(yn, p) is a jointdistribution with marginal distributions αi(yn, p) =(cid:80).
j αij(yn, p) and αj(yn, p) = (cid:80)bert (devlin et al., 2019) and roberta (liuet al., 2019) are encoder-decoder instantiations oftransformers, pretrained on fundamental languagetasks over large corpora.
both bert and robertahave achieved state-of-the-art results in various lan-guage understanding tasks.
in order to speed upinference time, many papers have employed knowl-edge distillation to reduce the number of parame-ters these transformers require while still preserv-ing their inference capabilities (sun et al., 2019;sanh et al., 2019; chen et al., 2020)..3.2.information theory background.
transformers like bert and roberta take texttokenized into sub-word components as input, cap-turing both the syntax and morphology of the text.
the text sequences used as training data, xn, canbe modelled as a stationary ergodic stochastic pro-cess, {xk}∞k=1, with instantiations limited to ﬁnite.
2252figure 2: visualization of the typicality formulation in-troducing the concept of a typical set on the left andshowing the distance proportional to typicality on theright..alphabet x and based on joint probability distribu-tion, p (x1 = x1, ..., xn = xn), whose transitionpredictability is governed by entropy rate, h(x ).
the entropy of a distribution, or entropy rate inthe case of a stochastic process, can be used todescribe the number of instantiations expected tobe observed from a random variable or process, re-ferred to as the typical set.
from the asymptoticequipartition property (aep), it is known that thesize of the typical set of sequences is bounded by.
|a(cid:15).
n| ≤ 2n(h(x )+(cid:15)),.
(3).
where 2nh(x ) estimates the size of the typical set..3.3 model-integrated meta-analysis.
we assume that a self-attention transformer learnsto ﬁll in words from a sentence by extracting fea-tures, f .
the quality of a piece of text can thenbe assessed by determining the distance of featurestaken by the model from candidate text, y n = yn,from the expected value of features taken fromcorrectly written text, x n = (xn ∈ a(cid:15)n), shown vi-sually in figure 2 and mathematically in equation 4.dtypical = dist(f | yn, e[f | (xn ∈ a(cid:15).
n)])..(4).
here dist does not does not refer to a speciﬁc dis-tance metric and is instead an unspeciﬁed normthat exists in some realizable projection space.
wethen postulate the existence of a surrogate func-tion, fmima, which maps the sequence input andtransformer parameter set, p, such that.
fmima(yn, p) ∝ −dtypical,.
(5).
resulting in a value indicating the typicality of acandidate input sequence.
this value can be usedto characterize the input for evaluation purposes..figure 3: information ﬂow used by fmima for estimat-ing typicality of input in distilbert architecture..3.4 attention-based information flow as.
mima function.
we postulate that input text that differs more greatlyfrom members of the typical set generates a greater“spark of interest” in a transformer, resulting ingreater information ﬂow through parts of the net-work as shown in figure 3. conversely, if theinput text is similar to the positive examples thetransformer trains on, less information ﬂows inthrough the layer, indicating that the model has al-ready captured information about the sequence pre-viously.
we formulate information ﬂow in terms ofthe attention dimensions αi(yn, p), αj(yn, p), andtheir joint distribution αij(yn, p) as deﬁned in sec-tion 3.1. we consider information ﬂow based onthe redundancy between αi(yn, p) and αj(yn, p)and use normalized mutual information (mi):.
if low(yn, p) = m i.
=.
2∗h(αi(yn, p)) + h(αj(yn, p)) − h(αij(yn, p))h(αi(yn, p)) + h(αj(yn, p)).
,.
(6)as deﬁned in witten and frank (2005) to capturethis redundancy..we are interested in attention heads with largeinformation ﬂow values, but ﬁnd empirically thatheads with the largest information ﬂow values de-pend very little on the input and simply functionas all-pass layers.
thus, we downselect to a singleattention head information ﬂow value to obtain.
fmima(yn, p)= 1 − medianlayer(maxhead[if low(yn, p)])..(7)here, the max over a given layer’s attention headscaptures the largest “spark of interest”.
the medianremoves outlier layers that have largely invariantinformation ﬂow values..2253between the reference concepts and candidate con-cepts..the ﬁrst portion of the f1 score is precision,corresponding to caption correctness.
our adjustedprecision is.
(cid:80)i.dfgt(s)(ci)|gt(s)|.
,.
i(.
(cid:80).
dfgt(s)(ci)|gt(s)| + i[dfgt(s)(ci) = 0])(9)where c is the candidate concept set and gt(s) isthe reference caption set.
our approach equallyweights correct and incorrect concepts if only onereference is used, but as the number increases, grad-ually decreases the importance of less commoncorrect concepts..the second portion of the f1 score is recall, cor-responding to caption detail.
our adjusted recall is.
r(c, s) =.
(10).
(cid:80)(cid:80).
i dfgt(s)(ci)i dfgt(s)(si).
..where a candidate concept set, c, which includedall concepts from the reference set, s, wouldachieve a score of 1..we then use the standard f1 score combination.
sparcs = f1(c, s) =.
2 ∗ p (c, s) ∗ r(c, s)p (c, s) + r(c, s).
..(11)to give an overall evaluation of performance,we fuse the proposed metrics.
to begin, we stan-dardize the output score distribution of humangenerated captions for each metric using the cap-tions from the coco karpathy test split fromfigure 1, metric(cid:48) = metric−e[metric(cocotest)],creating sparcs(cid:48), spurts(cid:48), and f (cid:48)mima.
utiliz-ing the standardization, we use threshold, t =−1.96, corresponding to the left tail of a 95%conﬁdence interval, to represent the lower boundof expected human captioning performance.
wethen use t to deﬁne a grammatical outlier penaltyg = min(mima(cid:48) −t, 0) and a style reward d =max(spurts(cid:48) − t, 0).
the quantities are com-bined as follows.
σ(metric(cocotest)).
(cid:40).
smurf =.
sparcs(cid:48) + g if sparcs(cid:48) < t,sparcs(cid:48) + d + g otherwise..(12)it can be interpreted as applying a semantic thresh-old, then incorporating the style reward since styleis only beneﬁcial for caption quality if the caption.
figure 4: aspects of caption quality color-coded to cor-responding words from evaluated coco examples..p (c, s) =.
3.5 caption evaluation.
mima provides us with a foundation for comput-ing the ﬂuency of input text.
we divide ﬂuency intotwo categories: grammar and style.
grammar de-pends on the typicality of the sequence as a whole,fmima, and is computed using the distilled bertmodel since it achieves the highest pearson cor-relation in the grammar experiment from table 1.style depends on the distinctness, or atypicality,of the words directly associated with the imagedescription, which we evaluate by removing thestop words from the text, then computing what wedeﬁne as spurts as shown.
spurts = 1 − fmima(yw/o, p),.
(8).
where yw/o is the candidate sequence without stopwords and fmima is computed using the distilledroberta model since it performs well on out-of-distribution text as shown in figure 5..we formulate semantic similarity using typical-ity as well.
assuming a comprehensive set of allvalid captions for a single image were available, weconsider the distribution of all concepts, s. herewe deﬁne concepts as the set stem terms that wouldremain if all stop words and afﬁx/sufﬁxes were re-moved from the text.
the distribution of conceptssampled from such a set of captions, sm, wouldhave a typical set, sβm, of the most relevant con-cepts.
thus, a valid caption that is representative ofthe image semantically and demonstrates ﬂuencyshould contain concepts that are members of thetypical set of concepts, sβm, and be a member of thetypical set of correctly formed language sequencesdeﬁned in section 3.2, a(cid:15)n, as shown in figure 4.to extract concepts from a caption, we use astemmer on yw/s and estimate the typicality of eachreference concept using the document frequency,df , of the concept across the available referencecaptions, gt(s), where gt is the function that mapsconcepts to a reference caption set.
we then usean adjusted f1 score to determine the similarity.
2254corrected sentences from 12 competition submis-sions was presented in grundkiewicz et al.
(2015).
participants were asked to rate how natural the cor-rected sentences sounded and did not have accessto any reference sentence.
microsoft coco 2014 we use the microsoftcoco validation set (chen et al., 2015), comprisedof 40,504 images, for a system-level human correla-tion experiment.
these images are annotated withﬁve human-generated captions, one of which isused as a baseline caption candidate.
human eval-uations of competition entries were collected usingamazon mechanical turk (amt).
these evalua-tions were framed as questions from which 2 pri-mary dimensions of system-level caption qualitywere derived as a ground truth to rank competitors:m1 (percentage better than or equal to human de-scription) and m2 (percentage passing the turingtest).
three additional categories were also in-cluded as an experimental ablation study but werenot considered in the ﬁnal competition ranking.
intotal, 255,000 evaluations were collected.
flickr 8k we use the graded human quality scoresfor the 5,822 remapped captions from the flickr8k dataset (hodosh et al., 2013) for a caption-levelsemantic human correlation study.
the dataset wasformed by selecting captions from one image andassigning them to another.
these captions are thengraded based on how well they align with the imageusing two different standards.
the ﬁrst standardis expert annotation, where human experts ratethe image-caption pairing on a scale of 1 (captionand image unrelated) to 4 (caption describes imagewith no errors).
each caption-image pairing has 3scores, which we combine by taking the average.
the second standard is crowd flower annotation,where at least 3 students vote yes or no on whetherthe caption and image are aligned.
composite dataset an additional dataset forcaption-level study of semantic human correlationfrom aditya et al.
(2018).
it contains 11,095 hu-man judgments (on a scale of 1-5) over flickr 8k,flickr 30k (young et al., 2014), and coco and incontrast to the flickr 8k dataset, includes machinegenerated captions in addition to human referencecaptions as candidates.
each evaluation is eitherbased purely on correctness or detailedness.
pascal-50s human evaluators were asked toidentify which of two sentences, b or c,ismore similar to reference sentence a. unlikeother caption datasets, human evaluators in pascal-.
figure 5: degradation iteration example and plot ofeach model’s average fmima value as text degrades..is semantically correct.
for all of our proposed met-rics, a larger value corresponds to higher qualitycaption..4 experiments.
4.1 preliminary experiment.
we ﬁrst seek to validate that our proposed fmima,extracted from the attention layers of bert,roberta, and their knowledge distilled versions,is proportional to the distance from the expectedvalue of features of the typical set.
to this end, wecreate an experiment where we can control the ran-domness of input text.
we begin with 11 differentparagraphs from unrelated wikipedia articles.
weextract all the words from the paragraphs and createa word set corpus.
we then sample 25 sentencesfrom the paragraphs randomly.
each sentence isiteratively degraded by substituting a fraction of thewords with random words from the word set cor-pus.
at each iteration step, the sentences are passedthrough the transformers and the value of fmima iscomputed.
eventually the sentence is incoherentand bears no resemblance to “natural” text.
theprocess and results can be seen in figure 5. theaverage fmima value for our information ﬂow for-mulation shows a strong correlation with the degra-dation in both models up until about 10% of the to-kens have been replaced, beyond which robertaremains reliable but bert does not, demonstratingroberta’s superior robustness..4.2 datasets.
conll-2014 the conll-2014 competition (nget al., 2014) was a shared task of correcting gram-matical errors of all types present in different sen-tences of an essay written by a learner of englishas a second language.
the essay consisted of 1312separate sections to correct.
a system-level humanevaluation study of the grammatical quality of the.
2255proof is a matter of rig orwhile is a ocer of rig orwhile is a ocer both rig orwhile and a ocer both rig awhile and parts time both all aiteration50s (vedantam et al., 2015) did not have accessto the original image.
the captions for sentencea were sourced from a 1000 image subset ofthe uiuc pascal sentence dataset (rashtchianet al., 2010) for which additional human captionswere collected using amt.
sentence b and c weresourced from both human and machine generatedcaptions.
the human captions were sourced fromthe original pascal dataset, resulting in four dif-ferent pairing combinations: human-correct (hc),human-incorrect (hi), human-model (hm), andmodel-model (mm)..4.3 system-level human correlation.
system-level experiments evaluate how closelyhuman evaluation and automatic evaluation mod-els align in terms of their overall evaluation ofcaptioning models.
to conﬁrm that fmima cancapture grammar information, we replicate theexperiment performed in napoles et al.
(2016)and show improved performance over previousbenchmarks in table 1. gleu (napoles et al.,2015), i-measure (felice and briscoe, 2015), andm2 (dahlmeier and ng, 2012) are reference-basedwhile their proposed er, lt, and lfm are ref-erenceless and based on linguistic features likefmima..metricgleuerlti-measurelfmm2bertmimarobertamima.
spearman’s ρ0.8520.8520.8080.7690.7800.6480.8520.885.pearson’s r0.8380.8290.8110.7530.7420.6410.9130.878.table 1: conll system-level human correlation exper-iment results utilizing distilled versions of bert androberta..we then benchmark our proposed caption eval-uation metrics against the rule-based metrics usedin the microsoft coco 2015 captioning compe-tition, which still serve as the standard for cap-tion evaluation, and the recall-idf conﬁguration ofbertscore.
we observe that the original cocosubmissions and many of the original codebasesfor the submissions are not publicly available ordo not provide pretrained models.
other authorsattempt to reproduce the submissions using opensource reimplementations that they have trainedthemselves, which will not be consistent with thesubmissions for which the human evaluations were.
performed.
thus, we instead opt to use the 4 repre-sentative baseline caption sets (vinyals et al., 2015;xu et al., 2015; karpathy and fei-fei, 2015) pro-vided publicly by cui et al.
(2018), which include 3competition submissions from open sourced mod-els and 1 human caption baseline.
these are guar-anteed to be consistent with their work and repro-ducible.
in table 2, we show the coco results forsparcs, spurts, and smurf..smurf and bertscore demonstrate the high-est correlation with human judgment in this dataset.
bertscore’s performance is partially due to incor-poration of idf dataset priors also used by cider,which we do not utilize to keep our metrics as gen-eral and consistent as possible.
to illustrate thispoint, we also report bertscore’s correlation with-out idf weighting (bs-w/oidf) for this experiment.
despite its simplicity, sparcs also performs wellalong with spurts.
the rest of the metrics fail toadequately reﬂect human judgment..m1.
m2.
ρ0.9860.374-0.279-0.709-0.8120.4790.0230.9560.8740.9560.984.p-value(0.014)(0.626)(0.721)(0.291)(0.188)(0.521)(0.977)(0.044)(0.126)(0.044)(0.016).
bertscorebs-w/oidfbleu-1bleu-2rouge-lmeteorciderspicesparcsspurtssmurfm1: percentage of captions that are evaluated as betteror equal to human caption.
m2: percentage of captions that pass the turing test..ρ0.9850.419-0.263-0.696-0.8020.5340.0820.9730.8940.9550.993.p-value(0.015)(0.581)(0.737)(0.304)(0.198)(0.466)(0.918)(0.027)(0.106)(0.045)(0.007).
table 2: microsoft coco system-level human correla-tion measured with pearson’s r experiment results..4.4 caption-level human correlation.
caption level experiments evaluate how closelyhuman evaluation and automatic evaluation modelsalign for each individual caption.
we begin withthe pascal-50s dataset in table 3. we follow theprocedure used in anderson et al.
(2016) and usethe ﬁrst 5 sentence a entries of each image..the pascal-50s dataset is based on a direct com-parison between the reference and candidate cap-tions, which gives similarity based metrics a dis-tinct advantage.
as a result, sparcs achieves thetop score in this experiment.
another interestingresult is the fact that spurts performs reasonablywell in the human-machine category despite having.
2256no access to the reference sentence.
this showsspurts effectiveness as a turing test at both asystem and caption-level, independent of semanticinformation.
the additional information providedby spurts to smurf in the human-machine cat-egory actually improves its performance..metricbertscorebleu-1bleu-2rouge-lmeteorciderspicesparcsspurtssmurf.
hc0.6400.6190.6160.6030.6430.6330.6280.6510.4960.621.hi0.9380.9030.9030.9060.9480.9490.9380.9580.5030.939.hm0.9250.8830.8610.8970.9080.8660.8660.8960.6040.912.mm0.5340.5550.5320.5890.6170.6390.6370.6440.4850.610.all0.7590.7400.7280.7490.7790.7720.7670.7870.5220.771.table 3: pascal-50s caption-level classiﬁcation ac-curacy for matching human evaluation results..to evaluate our semantic metric speciﬁcally, weuse the flickr 8k and composite dataset and followthe experiments speciﬁed in anderson et al.
(2016).
however, we have discovered a ﬂaw in previouscomparisons between the correlation of automaticevaluation metrics with expert evaluation and inter-human correlation using the flickr 8k dataset.
onlya small subset of annotations between the crowdflower and expert annotations overlap, which of-ten consists of ties causing the ranking metric tofail.
to give a fair comparison, we also test the au-tomatic metrics on a tie-free subset of the flickr 8kdata and use these results for human comparison.
all of these results can be seen in table 4..sparcs outperforms other metrics in the flickr8k dataset.
however, spice outperforms sparcson the composite dataset.
this is likely due to thefact that evaluations of “correctness” in the com-posite dataset are based on semantic propositionsand do not consider partial correctness..additionally, these new results show that auto-matic metrics can actually outperform voting-basedhuman metrics in terms of their correlation withexperts, further motivating their use.
this warrantsfurther study as some recent datasets opt to usevoting-based human metrics due to their ease ofcollection (levinboim et al., 2021)..4.5 generalization/robustness study.
we perform a caption-level generalizability and ro-bustness case study on the most commonly usedcaption evaluation algorithms using the coco val-idation set in table 5. we deﬁne a critical fail-.
metricbertscorebleu-1bleu-2rouge-lmeteorciderspicesparcsinter-human.
composite0.3880.3860.3940.3930.4040.4070.4450.431-.
flickr 8k flickr sub.
0.3620.3050.3160.2770.4110.4180.4750.481-.
0.5300.5270.5770.5110.6110.6500.6490.7160.655.table 4: kendall’s τ rank correlation with human judg-ment for the flickr 8k and composite datasets at acaption-level..ure, f , as a disparity of greater than 1 betweensystem-level human (m2) and caption-level algo-rithm correlation of a reference evaluation metricand a tested evaluation metric for a given captionset of an image.
the last column of table 5 showsthe likelihood of a critical failure occurring for eachmetric..in a human study, we identify the primary causeof critical failure in the 20 most severe discrep-ancies in order to identify potential areas for im-provement for each metric.
we use smurf asa reference evaluator for the other evaluators andspice as a reference for smurf.
the estimatedprobability of each of these failure causes is shownin the ﬁrst three columns of table 5..the ﬁrst failure cause, c1, refers to a scenariowhere the metric fails despite there being enoughword overlap between the candidate and referencecaptions for a correct judgment to be made.
thisimplies that the choice of words/sequences madeby the metric for the comparison needs improve-ment.
the second failure cause, c2, refers to theuse of correct and distinct words or phrases by thehuman captioner that are not seen in the references.
lastly, we include the case where the referenceevaluator may have incorrectly identiﬁed the cor-rect caption ranking (according to the human anno-tator) as matching system-level human judgment.
we refer to this as a reference failure, rf ..metriccidermeteorspicesmurf.
p (c1|f )0.350.650.650.40.p (c2|f )0.650.350.300.30.p (rf |f )0.000.000.050.30.p (f )0.2370.2050.1080.038.table 5: likelihood of critical failure and its causes..the focus of previous studies has been robust-ness to distractors (sharif et al., 2019; cui et al.,2018; hodosh and hockenmaier, 2016).
we ob-.
2257serve no captions where this is a primary cause offailure.
on the contrary, we ﬁnd that each metric ishighly susceptible to speciﬁc c1 scenarios:n-gram based: both cider and meteor are sen-sitive to stopwords, leading to rewards for words orsequences that supply no additional information.
spice: semantic proposal formation or sentenceparsing issues can lead to the metric unpredictablyfailing to recognize highly informative proposals.
smurf: the metric may fail to adequately re-ward additional information if the words used aretoo common, like ‘few’ or ‘some’..5 conclusion and future work.
in this paper, we use information theory based typ-icality analysis to capture a new perspective onthe problem of caption evaluation.
our analysisleads us to two caption evaluation metrics that cap-ture separate dimensions of caption quality anda fused metric.
we have performed experimentsdemonstrating their correlation with human judg-ment, showed how these methods could be used toperform multi-aspect system-level analysis of al-gorithm performance, and performed caption-levelstudies explaining why combining these two al-gorithms leads to more robust and generalizableevaluations.
the underlying mechanism, mima,opens many new avenues for the analysis of self-attention transformers and potentially other models.
future work could also focus on optimal weightingbetween semantics and style..6 ethical impact.
harmful bias, especially towards gender (hen-dricks et al., 2018), has been shown to be presentin image caption datasets and is often further mag-niﬁed by automatic captioners.
prior caption eval-uation methods have the potential to further ex-acerbate the problem by rewarding such captionsdue to their reliance on dataset speciﬁc images orcaptions.
referenceless evaluations like our stylemetric, spurts, offer a preemptive approach formitigating harmful dataset bias, like in simpson’sparadox (mehrabi et al., 2019), by utilizing intrin-sic properties of descriptive language learned byself-attention models over far larger and more di-verse corpora.
this gives the evaluator a morewholistic view of caption quality rather than view-ing the world through the lens of a single visualdataset..acknowledgments.
the authors acknowledge support from the nsfproject vr-k #1750082, the darpa kairosprogram (lestat project), and the anonymousreviewers for their insightful discussion.
any opin-ions, ﬁndings, and conclusions in this publicationare those of the authors and do not necessarily re-ﬂect the view of the funding agencies..references.
somak aditya, yezhou yang, chitta baral, yiannisaloimonos, and cornelia ferm¨uller.
2018.imageunderstanding using vision and reasoning throughscene description graph.
computer vision and im-age understanding, 173:33–45..peter anderson, basura fernando, mark johnson, andstephen gould.
2016. spice: semantic proposi-tional image caption evaluation.
in eccv..hiroki asano, tomoya mizumoto, and kentaro inui.
2017. reference-based metrics can be replacedwith reference-less metrics in evaluating grammat-in proceedings ofical error correction systems.
the eighth international joint conference on natu-ral language processing (volume 2: short papers),pages 343–348..satanjeev banerjee and alon lavie.
2005. meteor: anautomatic metric for mt evaluation with improvedcorrelation with human judgments.
in proceedingsof the acl workshop on intrinsic and extrinsic evalu-ation measures for machine translation and/or sum-marization, pages 65–72..xinlei chen, hao fang, tsung-yi lin, ramakr-ishna vedantam, saurabh gupta, piotr doll´ar, andc lawrence zitnick.
2015. microsoft coco captions:data collection and evaluation server.
arxiv preprintarxiv:1504.00325..yen-chun chen, zhe gan, yu cheng, jingzhou liu,and jingjing liu.
2020.distilling knowledgelearned in bert for text generation.
in proceedingsof the 58th annual meeting of the association forcomputational linguistics, pages 7893–7905..leshem choshen and omri abend.
2018. reference-less measure of faithfulness for grammatical errorcorrection.
in proceedings of the 2018 conferenceof the north american chapter of the associationfor computational linguistics: human languagetechnologies, volume 2 (short papers), pages 124–129, new orleans, louisiana.
association for com-putational linguistics..kevin clark, urvashi khandelwal, omer levy, andchristopher d manning.
2019. what does bert lookat?
an analysis of bert’s attention.
arxiv preprintarxiv:1906.04341..2258marcella cornia, matteo stefanini, lorenzo baraldi,and rita cucchiara.
2020. meshed-memory trans-former for image captioning.
in proceedings of theieee/cvf conference on computer vision and pat-tern recognition, pages 10578–10587..thomas m cover.
1999. elements of information the-.
ory.
john wiley & sons..y. cui, g. yang, a. veit, x. huang, and s. belongie.
in2018. learning to evaluate image captioning.
2018 ieee/cvf conference on computer visionand pattern recognition, pages 5804–5812..daniel dahlmeier and hwee tou ng.
2012. a beam-search decoder for grammatical error correction.
inproceedings of the 2012 joint conference on empir-ical methods in natural language processing andcomputational natural language learning, pages568–578..jacob devlin, ming-wei chang, kenton lee, andkristina toutanova.
2019. bert: pre-training ofdeep bidirectional transformers for language under-in proceedings of the 2019 conferencestanding.
of the north american chapter of the associationfor computational linguistics: human languagetechnologies, volume 1 (long and short papers),pages 4171–4186, minneapolis, minnesota.
associ-ation for computational linguistics..mariano felice and ted briscoe.
2015. towards a stan-dard evaluation method for grammatical error detec-tion and correction.
in proceedings of the 2015 con-ference of the north american chapter of the asso-ciation for computational linguistics: human lan-guage technologies, pages 578–587..roman grundkiewicz, marcin junczys-dowmunt, andedward gillian.
2015. human evaluation of gram-matical error correction systems.
in proceedings ofthe 2015 conference on empirical methods in natu-ral language processing, pages 461–470..jing gu, qingyang wu, and zhou yu.
2021. percep-tion score: a learned metric for open-ended textgeneration evaluation.
proceedings of the aaaiconference on artiﬁcial intelligence, 35(14):12902–12910..lisa anne hendricks, kaylee burns, kate saenko,trevor darrell, and anna rohrbach.
2018. womenalso snowboard: overcoming bias in captioningmodels.
in proceedings of the european conferenceon computer vision (eccv), pages 771–787..micah hodosh and julia hockenmaier.
2016. focusedevaluation for image description with binary forced-choice tasks.
in proceedings of the 5th workshop onvision and language, pages 19–28..micah hodosh, peter young, and julia hockenmaier.
2013. framing image description as a ranking task:data, models and evaluation metrics.
journal of ar-tiﬁcial intelligence research, 47:853–899..ming jiang, qiuyuan huang, lei zhang, xin wang,pengchuan zhang, zhe gan, jana diesner, and jian-feng gao.
2019. tiger: text-to-image groundingin proceedings offor image caption evaluation.
the 2019 conference on empirical methods in nat-ural language processing and the 9th internationaljoint conference on natural language processing(emnlp-ijcnlp), pages 2141–2152, hong kong,china.
association for computational linguistics..hassan kane, muhammed yusuf kocyigit, ali abdalla,pelkins ajanoh, and mohamed coulibali.
2020. nu-bia: neural based interchangeability assessor fortext generation.
arxiv preprint arxiv:2004.14667..andrej karpathy and li fei-fei.
2015. deep visual-semantic alignments for generating image descrip-tions..mert kilickaya, aykut erdem, nazli ikizler-cinbis,and erkut erdem.
2017. re-evaluating automaticmetrics for image captioning.
in proceedings of the15th conference of the european chapter of the as-sociation for computational linguistics: volume 1,long papers, pages 199–209, valencia, spain.
asso-ciation for computational linguistics..tomer levinboim, ashish v. thapliyal, piyushsharma, and radu soricut.
2021. quality estimationfor image captions based on large-scale human eval-uations.
in proceedings of the 2021 conference ofthe north american chapter of the association forcomputational linguistics: human language tech-nologies, pages 3157–3166, online.
association forcomputational linguistics..chin-yew lin.
2004. rouge: a package for automaticin text summarization.
evaluation of summaries.
branches out, pages 74–81..siqi liu, zhenhai zhu, ning ye, sergio guadarrama,and kevin murphy.
2017. improved image caption-ing via policy gradient optimization of spider.
inproceedings of the ieee international conference oncomputer vision, pages 873–881..yinhan liu, myle ott, naman goyal, jingfei du, man-dar joshi, danqi chen, omer levy, mike lewis,luke zettlemoyer, and veselin stoyanov.
2019.roberta: a robustly optimized bert pretraining ap-proach.
arxiv preprint arxiv:1907.11692..a. mathews, l. xie, and x. he.
2018. semstyle:learning to generate stylised image captions usingin 2018 ieee/cvf conference onunaligned text.
computer vision and pattern recognition, pages8591–8600..ninareh mehrabi, fred morstatter, nripsuta saxena,kristina lerman, and aram galstyan.
2019. a sur-vey on bias and fairness in machine learning.
arxivpreprint arxiv:1908.09635..courtney napoles, keisuke sakaguchi, matt post, andjoel tetreault.
2015. ground truth for grammati-cal error correction metrics.
in proceedings of the.
2259ashish vaswani, noam shazeer, niki parmar, jakobuszkoreit, llion jones, aidan n gomez, łukaszkaiser, and illia polosukhin.
2017. attention is allin advances in neural information pro-you need.
cessing systems, pages 5998–6008..r. vedantam, c. l. zitnick, and d. parikh.
2015. cider:consensus-based image description evaluation.
in2015 ieee conference on computer vision and pat-tern recognition (cvpr), pages 4566–4575..oriol vinyals, alexander toshev, samy bengio, anddumitru erhan.
2015. show and tell: a neural im-age caption generator.
in proceedings of the ieeeconference on computer vision and pattern recogni-tion, pages 3156–3164..ian h. witten and eibe frank.
2005. data mining:practical machine learning tools and techniques.
morgan kaufmann, amsterdam..kelvin xu, jimmy ba, ryan kiros, kyunghyun cho,aaron courville, ruslan salakhudinov, rich zemel,and yoshua bengio.
2015. show, attend and tell:neural image caption generation with visual atten-tion.
in international conference on machine learn-ing, pages 2048–2057.
pmlr..yanzhi yi, hangyu deng, and jinglu hu.
2020..im-proving image captioning evaluation by consideringin proceedings of theinter references variance.
58th annual meeting of the association for compu-tational linguistics, pages 985–994, online.
associ-ation for computational linguistics..peter young, alice lai, micah hodosh, and julia hock-enmaier.
2014. from image descriptions to visualdenotations: new similarity metrics for semantic in-ference over event descriptions.
transactions of theassociation for computational linguistics, 2:67–78..tianyi zhang, varsha kishore, felix wu, kilian qweinberger, and yoav artzi.
2019. bertscore: eval-arxiv preprintuating text generation with bert.
arxiv:1904.09675..wei zhao, maxime peyrard, fei liu, yang gao, chris-tian m meyer, and steffen eger.
2019. moverscore:text generation evaluating with contextualized em-beddings and earth mover distance.
arxiv preprintarxiv:1909.02622..53rd annual meeting of the association for compu-tational linguistics and the 7th international jointconference on natural language processing (vol-ume 2: short papers), pages 588–593..courtney napoles, keisuke sakaguchi, and joeltetreault.
2016. there’s no comparison: reference-less evaluation metrics in grammatical error correc-in proceedings of the 2016 conference ontion.
empirical methods in natural language process-ing, pages 2109–2115, austin, texas.
associationfor computational linguistics..hwee tou ng, siew mei wu, ted briscoe, christianhadiwinoto, raymond hendy susanto, and christo-pher bryant.
2014. the conll-2014 shared task ongrammatical error correction.
in proceedings of theeighteenth conference on computational naturallanguage learning: shared task, pages 1–14, balti-more, maryland.
association for computational lin-guistics..yingwei pan, ting yao, yehao li, and tao mei.
2020.x-linear attention networks for image captioning.
inproceedings of the ieee/cvf conference on com-puter vision and pattern recognition, pages 10971–10980..kishore papineni, salim roukos, todd ward, and wei-jing zhu.
2002. bleu: a method for automatic eval-uation of machine translation.
in proceedings of the40th annual meeting of the association for compu-tational linguistics, pages 311–318..cyrus rashtchian, peter young, micah hodosh, andjulia hockenmaier.
2010. collecting image anno-tations using amazon’s mechanical turk.
in proceed-ings of the naacl hlt 2010 workshop on creatingspeech and language data with amazon’s mechan-ical turk, pages 139–147..victor sanh, lysandre debut, julien chaumond, andthomas wolf.
2019. distilbert, a distilled versionof bert: smaller, faster, cheaper and lighter.
arxivpreprint arxiv:1910.01108..thibault sellam, dipanjan das, and ankur parikh.
2020. bleurt: learning robust metrics for textgeneration.
in proceedings of the 58th annual meet-ing of the association for computational linguistics,pages 7881–7892, online.
association for computa-tional linguistics..naeha sharif, lyndon white, mohammed ben-namoun, wei liu, and syed afaq ali shah.
2019.lceval: learned composite metric for caption eval-international journal of computer vision,uation.
127(10):1586–1610..siqi sun, yu cheng, zhe gan, and jingjing liu.
2019.patient knowledge distillation for bert model com-pression.
arxiv preprint arxiv:1908.09355..shuntaro takahashi and kumiko tanaka-ishii.
2018.cross entropy of neural language models at inﬁn-ity—a new bound of the entropy rate.
entropy,20(11):839..2260