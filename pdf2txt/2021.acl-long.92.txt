comparing test sets with item response theory.
clara vania♠∗† phu mon htut♣∗ william huang♣∗dhara mungra♣ richard yuanzhe pang♣ jason phang♣haokun liu♦† kyunghyun cho♣ samuel r. bowman♣♠amazon ♣new york university♦allen institute for aivaniclar@amazon.co.uk, bowman@nyu.edu.
abstract.
recent years have seen numerous nlpdatasets introduced to evaluate the perfor-mance of ﬁne-tuned models on natural lan-guage understanding tasks.
recent resultsfrom large pretrained models, though, showthat many of these datasets are largely satu-rated and unlikely to be able to detect furtherprogress.
what kind of datasets are still ef-fective at discriminating among strong mod-els, and what kind of datasets should we ex-pect to be able to detect future improvements?
to measure this uniformly across datasets,we draw on item response theory and eval-uate 29 datasets using predictions from 18pretrained transformer models on individualtest examples.
we ﬁnd that quoref, hel-laswag, and mc-taco are best suited fordistinguishing among state-of-the-art models,while snli, mnli, and commitmentbankseem to be saturated for current strong mod-els.
we also observe span selection task for-mat, which is used for qa datasets like qamror squad2.0, is effective in differentiating be-tween strong and weak models..1.introduction.
many datasets have been created to evaluate var-ious aspects of natural language understanding(nlu) in english.
these datasets are useful to mea-sure progress; however, it is evident from variousleaderboards (wang et al., 2018, 2019b; rajpurkaret al., 2016; zellers et al., 2018) that many of themare no longer challenging or discriminative enoughto differentiate strong models such as those basedon transformers (vaswani et al., 2017).1 evenif these benchmarks are sound tests of important.
∗equal contribution.
† work done while at new york university.
1for example, the recent deberta model (he et al., 2020)achieves parity with human annotators on the supergluebenchmark score: https://super.gluebenchmark.
com/leaderboard..(and potentially unsolved) tasks, their usefulnessis limited if they cannot measure further progress.
in this paper, we ask: which datasets are best indistinguishing current and possible future strongmodels?.
we aim to compare datasets using a single metricthat accounts for their effectiveness in separatingcurrent stronger and weaker models.
to that end,we use item response theory (irt; baker andkim, 1993), a statistical framework from psycho-metrics that is widely used for the evaluation of testitems in educational assessment.
irt assumes thatthe probability that a model will correctly handlean example in a test set depends on the model’slatent ability parameter and three example-speciﬁcparameters, typically measuring example difﬁculty(how strong does a model have to be to get it right),discrimination (how effective the example is fordifferentiating between similar models), and guess-ing (how likely a weak model is to get the exampleright for spurious reasons)..this paper presents a large-scale irt analy-sis of existing english nlu datasets.
unlikeprevious work which focuses on example-levelanalysis within individual datasets (lalor et al.,2016, 2018), here we analyze example charac-teristics from a larger perspective by compar-ing individual examples across datasets.
weevaluate test sets from 29 datasets in differentformats—classiﬁcation, multiple-choice qa, andspan-selection qa.
as responses, we use modelpredictions from 18 transformer-based models, in-cluding some limited-capacity models chosen toexpose better the dataset’s ability to discriminateweaker from stronger predictors.
we then ﬁt asingle irt model on these responses using a varia-tional inference method.2.
2our data and code can be found at https://github..com/nyu-mll/nlu-test-sets..proceedingsofthe59thannualmeetingoftheassociationforcomputationallinguisticsandthe11thinternationaljointconferenceonnaturallanguageprocessing,pages1141–1158august1–6,2021.©2021associationforcomputationallinguistics1141figure 1: distribution of test examples according to our proposed locally estimated headroom (leh) scores(§ 4.1.1), which measure the local slope of the item characteristic curve (icc) for an example at the abilitylevel corresponding to the best model, and thus reﬂect the effectiveness of that single example at distinguishingbetween near-state-of-the-art models.
datasets are grouped by task format: classiﬁcation (green), sentence-levelmultiple-choice (blue), paragraph-level multiple-choice (red), and span selection (grey).
within each format, thedatasets are sorted by their release date.
more details on the datasets are given in table 1..we ﬁnd:.
• quoref, hellaswag, and mc-taco containthe highest number of examples that can dif-ferentiate between near-state-of-the-art mod-els, making them very likely to be effective attracking near-future progress on the skills thatthey actually test (figure 1)..• squad2.0, newsqa, quail, mc-taco,and arc-challenge have the most difﬁcultexamples..• span-based qa is an effective task formatfor discriminating between strong and weakmodels..• cosmosqa, mc-taco, winogrande, andarc-challenge consist mostly of hard exam-ples, while for most datasets, the example dif-ﬁculty levels are more widely distributed..figure 2: an example of item characteristic curves(iccs) with different values for discrimination (α), dif-ﬁculty (β), and guessing (γ) parameters.
p(θ) is theprobability of a correct answer for a given θ. θ mea-sures a model’s ability level (higher is better).
α gov-erns the steepness of the function, β determines the θvalue at which the curve is the steepest, while γ deﬁnesthe baseline likelihood that an arbitrarily weak modelcan guess correctly..2.item response theory.
baker and kim (1993) introduce item responsetheory (irt), a statistical framework to measurethe probability of a responder (human or ai system)predicting a correct answer for a given item (testexample).
the probability of a responder i answer-ing an item j correctly is estimated as a function ofthe responder’s latent ability θi and the item charac-teristics, referred to as the item characteristic curve(icc)..we use the 3-parameter (3pl) irt model, whereitem behavior is governed by discrimination, difﬁ-culty, and guessing parameters.
the discrimination.
parameter (α) deﬁnes how effective an item is fordistinguishing predictors along the ability axis.
thedifﬁculty parameter (β) deﬁnes a minimum levelof ability at which we expect to see high responderperformance.
the guessing parameter (γ) deﬁnesthe probability of correctly answering an item byrandom guessing.
figure 2 shows example iccswith different parameter values..formally, the probability of individual i answer-.
ing item j correctly is modeled as:.
pj(θi) = γj +.
1 − γj1 + e−αj (θi−βj ).
..(1).
11422.1.irt with variational inference.
we use variational inference to infer irt parame-ters from model response patterns using pyro (ran-ganath et al., 2014; bingham et al., 2019).
laloret al.
(2019) found this method effective when ﬁt-ting irt models to responses on snli.
let n bethe number of items and let m be the number ofresponders.
the response patterns is y ∈ rn×m,where the i-th row corresponds to responder i andthe j-th column corresponds to item j. we deﬁneyij ∈ [0, 1] as the response of model i to item j,where yij = 1 indicates a correct response andyij = 0 indicates an incorrect response.
we ap-proximate the joint probability of the parametersπ(θ, α, β, γ | y) with a variational posterior:.
q(θ, α, β, γ) =.
πθi (θi).
j (αi)πβπα.
j (βi)πγ.
j (γi).
i(cid:89).
i=1.
j(cid:89).
j=1.
where πρ(·) denotes the density for parameter ρ.for each parameter, we choose the following distri-butions:.
θ ∼ n (µθ, σ2θ )log α ∼ n (µα, σ2α)β ∼ n (µβ, σ2β)sigmoid−1(γ) ∼ n (µγ, σ2γ).
we ﬁt the posterior parameters by minimizing theevidence lower bound (elbo).
when calculatingthe elbo, we weight the log-likelihoods of eachitem’s parameter by the inverse of the item’s datasetsize to control for test set size..following lalor et al.
(2019), we use a prior ofn (0, 1) for θ, β, and sigmoid−1(γ).
while laloret al.
(2019) uses n (0, 103) for item parameter pri-ors, we encountered degenerate runs and insteaduse n (0, 1).
for log α, we use n (0, σ2α) wherewe set σα by searching [0.25, 0.5] by increments of0.05 and use the value yielding the highest elboafter excluding degenerate runs.
we use a sigmoidtransformation for γ to constrain the guessing prob-ability to (0, 1)..3 experiments.
3.1 datasets.
(2).
(3).
(4).
(5).
(6).
that end, we choose datasets based on the followingcriteria:.
• they are plausibly unsolved, in that the best-reported model performance does not exceedestimated human performance (if available)by more than three metric points..• they are relatively easy to use with currentlarge pretrained models, and in particular,their inputs ﬁt within a typical pretrainedtransformer’s 512-token limits.
(this rulesout tasks with full-document contexts or re-trieval components.).
• they are evaluated at example-level, i.e., wefocus our analysis on qa and other classi-ﬁcation datasets, where each example corre-sponds to one item in the irt.
(this rules outstructured prediction and sequence taggingtasks.).
• they have simple and reliable automatic met-(this rules out.
rics at the example level.
generation-based tasks.).
table 1 lists the datasets we evaluate.
for mnli,we combine the matched and mismatched portionsof the development and custom test sets for ouranalysis.
for anli, we train models on snli,mnli, and anli training examples.
similarto mnli, we combine anli’s three evaluationrounds of the development and the test sets for ouranalysis..custom test splits some of our selecteddatasets do not have publicly available labeled testexamples.
for such cases, we create a new customsplit by randomly sampling 50% of the validationexamples as a new test set and keeping the rest forvalidation (“cust.” column in table 1).
for nat-ural questions, we use the mrqa 2019 version(fisch et al., 2019), as the original version includessome examples with very long contexts.3 for mc-taco, the original dataset does not come with atraining set.
for our experiment, we use 80% ofthe validation set as our training set and the rest asa our validation set while leaving the original testset untouched..our goal is to perform a ﬁne-grained evaluation ofenglish nlu datasets that appear to discriminateamong widely used transformer-based models.
to.
3https://github.com/mrqa/.
mrqa-shared-task-2019.
114387.692.789.790.550.8.
86.078.874.655.979.971.585.077.677.3.
62.537.586.792.885.779.484.187.877.973.3.
79.657.891.569.978.7.
93.6–92.095.8–.
100.0100.088.975.888.180.092.994.994.0.
––79.898.289.094.095.693.893.0–.
–46.586.8–93.0.
|train|.
|dev|.
|test| cust.
metric roberta human.
rte (dagan et al., 2005, et seq.)
snli (bowman et al., 2015)mnli (williams et al., 2018)commitmentbank (cb; de marneffe et al., 2019)anli (nie et al., 2020).
2,490.
138.
550,152 10,000 10,0009,823392,702282503,2001,105,719.
3,200.
-ﬁissalc.noitac.level-ecnetnes.eciohcelpitlum.level-hpargarap.eciohcelpitlum.copa (roemmele et al., 2011)wsc (levesque et al., 2012)commonsenseqa (csqa; talmor et al., 2019)mc-taco (zhou et al., 2019)socialiqa (sap et al., 2019)wic (pilehvar and camacho-collados, 2019)abductive nli (abductnli; bhagavatula et al., 2020)piqa (bisk et al., 2020)winogrande (sakaguchi et al., 2020).
arc-easy (clark et al., 2018)arc-challenge (clark et al., 2018)arct (habernal et al., 2018)mcscript (ostermann et al., 2018)boolq (clark et al., 2019)cosmos qa (huang et al., 2019)hellaswag (zellers et al., 2019)mutual (cui et al., 2020)mutual+ (cui et al., 2020)quail (rogers et al., 2020).
139 (cid:51) acc.
acc.
9,824 (cid:51) acc.
28 (cid:51) acc.
acc.
50 (cid:51) acc.
52 (cid:51) acc.
611 (cid:51) acc.
9,442 (cid:51) em977 (cid:51) acc.
319 (cid:51) acc.
766 (cid:51) acc.
919 (cid:51) acc.
634 (cid:51) acc..acc.
2,376acc.
1,172acc.
4453,610acc.
1,635 (cid:51) acc.
1,493 (cid:51) acc.
5,021 (cid:51) acc.
443 (cid:51) acc.
443 (cid:51) acc.
acc.
556.
4005549,7413,02633,4105,428169,65416,11340,398.
2,2511,1191,21114,1919,42725,26239,9057,0887,08810,246.
5052610757977319766919633.
5702993172,0201,6351,4925,0214434432,164.naps.noitceles.qamr (michael et al., 2018)newsqa (trischler et al., 2017)squad2.0 (rajpurkar et al., 2018)mrqa-nq (kwiatkowski et al., 2019)quoref (dasigi et al., 2019).
em50,615 18,908 18,7704,2934,34376,568em6,198 (cid:51) em5,675130,3196,418 (cid:51) em6,418104,0711,209 (cid:51) em1,20919,399.table 1: datasets grouped by their task format and ordered by release year.
cust.
denotes cases when weuse our own custom split.
metric: evaluation metric used in this study.
roberta: model performance usingrobertalarge.
human: human performance..3.2 models.
we aim to understand how examples from differ-ent datasets contribute to the evaluations of mod-els with near-state-of-the-art abilities, so we in-clude several pretrained transformer-based modelsto approximate this.
however, using only high-performing models could result in a poor irtmodel ﬁt (martínez-plumed et al., 2019) to avoidthis, we add both weaker models and under-trainedversions of our original models.
we use albert-xxl-v2 (lan et al., 2020), robertalarge androbertabase (liu et al., 2019), bertlarge andbertbase (devlin et al., 2019), xlm-r (conneauet al., 2020), and 12 minibertas (zhang et al.,2021b).
4 for each of the 18 transformer-basedmodels, we evaluate ﬁve different checkpoints—at1%, 10%, 25%, and 50% of the maximum steps of.
4the minibertas are roberta models pretrained on1m, 10m, 100m, or 1b words of raw text, and varying slightlyin model size.
there are three pretrained models for eachpretraining data quantity, which are pretrained using differentnear-optimal hyperparameter values.
we use all three variantsin producing responses for irt..the maximum epochs (section 3.3), as well as thebest checkpoint on the validation set, which neednot be one of the other four.
this yields a total of90 model predictions for each test example..3.3 experimental setup.
optimization we perform a hyperparametersweep on each dataset, varying the learning rate∈ {1e − 5, 3e − 5, 5e − 6}.
we tune the maximumepochs ∈ {10, 40} for small datasets (< 5k train-ing examples), and ∈ {3, 10} for other datasets(zhang et al., 2021a).
we use the jiant (pruk-sachatkun et al., 2020b) library which is based onpytorch (paszke et al., 2019) and huggingfacetransformers (wolf et al., 2020)..we only perform hyperparameter tuning withthe robertalarge model and apply the best con-ﬁguration to train all the other transformer models.
we use nvidia v100 tensor core gpus for ourexperiments.
on average, it takes approximatelyfour hours to train roberta on small datasets(< 3k training examples), one day for medium-.
1144figure 3: the best validation performance of albert-xxl-v2, robertalarge, and the smallest miniberta(roberta-med-small-1m-2) on each dataset.
the full results table with performance of all models is reported inthe appendix (table 3).
sized datasets (< 10k), and four days for largedatasets (> 10k)..4 results and analysis.
figure 3 shows the performance of robertalarge,albert-xxl-v2, and one of the low performingminibertas (roberta-med-small-1m-2) on allvalidation sets.
unsurprisingly, albert-xxl-v2and robertalarge are the best-performing models,while the small miniberta model achieves muchlower performance.
full results using all 18 modelscan be found in the appendix (table 3)..4.1.irt analysis.
item characteristics.
4.1.1metric as our primary metric, we introduce lo-cally estimated headroom (leh) score, whichmeasures the ability of each test example to con-tribute to the evaluation of near-future progress.
wecalculate it as the derivative of the example’s icc(figure 2) with respect to the highest latent abilityscore, which corresponds to albert-xxl-v2.
ahigh leh score indicates that the best-performingmodel is still far from the example’s saturationpoints—the ﬂat sections of icc inferred by ourmodel.
there is enough space along the curve thatthe irt model expects the example to be able todifferentiate future state-of-the-art models.
typ-ically, different near-state-of-the-art models bothsucceed and fail on this kind of example, whileweaker models mostly fail.
a high leh score im-plies that there is still enough room for potentiallystronger models to perform better on this dataset..to validate the use of leh scores for detect-ing near-future improvements, we compare twoirt models.
the ﬁrst is ﬁtted using responsesfrom all models, while the second is ﬁtted basedon responses from bert and other weaker models.
(excluding robertalarge, robertabase, xlm-r, and albert-xxl-v2).
after that, we com-pute the correlation between the two sets of lehscores, focusing on the 75th percentile for eachdataset.
the pearson correlation is 95.5% with amedian absolute difference of 0.007 and a standarddeviation of 0.011. out of the 29 datasets, onlysquad2.0, commensenseqa, mutual, quoref,and hellaswag have more than 0.02 absolute dif-ference in leh scores.
this strong correlationsuggests that our iccs ﬁts are not overly sensitiveto the exact characteristics of current state of theart models..analysis by leh scores figure 1 shows the dis-tribution of test examples for each dataset based ontheir leh scores.
for our analysis, we focus onthe 75th percentile examples in each dataset as arough proxy for how likely a dataset is to have asigniﬁcant number of examples that are difﬁcult ordiscriminative for near-future models..we observe that quoref, hellaswag, and mc-taco have examples with the highest leh scores,suggesting sufﬁcient headroom for future state-of-the-art models with a higher ability to achievesnli,better performance on these datasets.
commitmentbank, and mnli have relatively lowleh scores, indicating that performance on thesedatasets is largely saturated.
additionally, we alsomeasure how the 75th percentile leh scores corre-late with human-roberta gap.
using 22 datasetsthat have human performance numbers (table 1),we ﬁnd that the pearson correlation between thetwo is weakly positive (0.21)..analysis by item parameters next, we analyzethe distribution of test examples according to theirdiscrimination and difﬁculty parameters (figure 4).
we observe that datasets with span selection for-.
1145figure 4: distribution of test examples for each dataset based on the log discrimination (log α) parameter (top) andthe difﬁculty (β) parameter (bottom)..mat (qamr, newsqa, squad, mrqa-nq, andquoref) have the highest discrimination scores thanother datasets, highlighting span selection as an ef-fective task format for discriminating among strongand weak models.
however, this might be becausethis task format typically features a much largerspace of possible model outputs than the other for-mats we consider.
it does not necessarily mean thatspan selection is the most suitable to test models’ability to understand language.
as the span-basedformat restricts answers to be text spans in the givenpassage, there are concerns that it rarely requiresreasoning ability which often involves answers notmentioned in the passage, and thus not reﬂectingcomprehension ability of humans (lai et al., 2017;sugawara et al., 2018)..for the difﬁculty parameter, we do not observea narrow task format that is superior to the oth-ers.
however, we notice that the highest difﬁ-culty scores are obtained by qa datasets such assquad2.0, newsqa, quail, arc-challenge,and mc-taco.
anli, which is created with adver-sarial model-in-the-loop crowdsourcing, also has ofmany hard examples.
impressionistically, trainingset size and creation date do not seem to correlatewith either example’s difﬁculty or discriminationparameters..figure 5 shows the distribution of examplesjointly according to their difﬁculty and log dis-crimination parameters.
we notice a half-moonshape pattern in most datasets, which indicates that.
most of the discriminative examples are either veryeasy or very difﬁcult.
referring to the icc curve(figure 2), this indicates that there is high agree-ment among strong models or weak models, whichcorresponds to one of the saturation points in theicc curve (upper or lower).
the only dataset thatdoes not have this pattern is winogrande, which isdifﬁcult for all models..arc-challenge, quail, hellaswag, common-senseqa, and mc-taco show clusters with highdensity on the top right regions, indicating a largenumber of examples with high discrimination anddifﬁculty scores.
other datasets have more scat-tered distributions.
snli, mnli, and mcscriptshow higher density on the bottom right regions,while newsqa, squad2.0, and mrqa-nq showhigher density on both the top and bottom right re-gions.
further analysis of the guessing parameterscan be found in appendix a..4.2 examples with unanimous responses.
when ﬁtting icc on examples that have only cor-rect responses or only incorrect responses, the dis-crimination parameter is unconstrained.
we ﬁndthat these examples make up 4% of our data.
13 ofthe 29 datasets contain at least one such example.
roughly 16% of newsqa examples are incorrectlyanswered by all models, while the remaining 12datasets have less than 10% of all correct or in-correct examples.
to study the effect of exampleswith all correct or incorrect responses, we ﬁt an.
1146figure 5: distributions of log discrimination (log α) versus the difﬁculty (β) parameters for each dataset...irt model on responses excluding such examplesand compare against parameters from the full setof responses.
we ﬁnd that the pearson correla-tion for the discrimination at the 75th percentile is97.2%, with a median absolute difference of 0.016and standard deviation of 0.015. mc-taco, com-mitmentbank, and wsc differ by more than 0.04.further, we ﬁnd that the pearson correlation forthe leh score at the 75th percentile is 98.9%, witha median absolute difference of 0.006 and stan-dard deviation of 0.005. rte, wic, winogrande,qamr, newsqa, mrqa-nq, mc-taco, andboolq differ by 0.01. given these high correlations,we do not exclude these examples when reportingour main results..4.3 analysis by task group.
next, we analyze each task-type group in moredetail, focusing on the example’s scores around the75th percentile..classiﬁcation we observe that all datasets havemoderate discrimination scores.
most anli ex-amples have relatively high difﬁculty scores, whilesnli, mnli, and commitmentbank have the low-est difﬁculty scores..sentence-level multiple choice all of thedatasets in this group have relatively low discrimi-nation scores compared to span selection datasets.
figure 5 shows that mc-taco, winogrande, andcommonsenseqa all have a higher density of dif-ﬁcult examples, while for other datasets the distri-.
bution is more spread..paragraph-level multiple choice quail andarc-challenge examples have high difﬁculty butmoderate discrimination scores.
as seen in fig-ure 5, these datasets have a higher density in thetop right regions, showing a large proportion ofdifﬁcult examples.
arct shows moderate difﬁ-culty despite its known artifacts (niven and kao,2019), indicating that it can still be challenging formodels.
compared to other datasets, boolq has thehighest number of easy examples.
however, as itis a binary classiﬁcation task, the random baselineperformance is already high..to investigate this, we calculate the number ofexamples in each test set that have γ parameterbelow 0.5. in general, we ﬁnd that 88% of thetest examples have γ < 0.5, implying that mostof the examples contributed to the inferences ofα, β, and θ. boolq was the only exception inwhich approximately 56% of examples were as-signed γ > 0.5. after ﬁltering out these guessableexamples in boolq, we ﬁnd that its test exampleshave slightly higher discrimination scores with lit-tle change in difﬁculty scores..span selection we observe that span selectiondatasets are the most discriminative.
however, interms of difﬁculty, only squad2.0 and newsqaare among the top ﬁve..4.3.1 analysis on model abilityfor a sanity check, we further analyze how eachmodel scores according to our ﬁtted irt parame-.
1147name.
example.
mnli.
premise: and, you know, with this, you know, it wasn’t many opportunities for kids to be special,because kids weren’t, you know, you were pushed out of adult conversation, and just reallypushed to the side.
hypothesis: children were pushed out of adult conversation, and really just pushed to the side ingeneral.
label: entailment.
mnli.
premise: look, it’s your skin, but you’re going to be in trouble if you don’t get busy.
hypothesis: the boss will ﬁre you if he sees you slacking off..label: neutral.
mc-taco.
mc-taco.
the beatles are giving a press conference about their new ﬁlm, magical mystery tour .whattime of day was the press conference?
(1) 4:00 pm (cid:51) (2) 12:00 pm (cid:51) (3) 3 p.m (cid:51) (4) 6:00 am (cid:55).
because then they feel like they are forced to stay in that situation.
"on average, how often dothey feel stuck in the situation?
(1) 54 months (cid:55) (2) 6 centuries (cid:55) (3) once every 6 years (cid:55)(4) every few seconds (cid:55) (5) once every 2 seconds (cid:55) (6) once every 18 years (cid:55).
difﬁculty (β).
3.27.
-1.87.
2.86.
-1.67.table 2: hardest and easiest examples along with their estimated difﬁculty score for mnli and mc-taco..ters.
we observe a positive correlation betweenability and average model accuracy (appendixb).
generally, within a model, the best validationcheckpoint obtains the highest average model accu-racy and/or ability score.
across models, albert-xxl-v2 performs typically best..4.4 qualitative analysis.
to better understand what kinds of examples aredifﬁcult or discriminating, we analyze the 20 exam-ples with the lowest and highest scores for the dis-crimination and the difﬁculty parameters from ﬁvedatasets: squad2.0, mc-taco, quail, mnli,and boolq.
the ﬁrst three are datasets with highdiscrimination and/or difﬁculty scores.
mnli andboolq have moderate discrimination and difﬁcultyscores and low label entropy (three-class classiﬁca-tion for mnli and binary choice for boolq)..we observe that the 20 most difﬁcult boolq ex-amples are labeled false (the minority class), while19 of the 20 easiest examples are labeled true.
formnli, we ﬁnd that the 20 easiest mnli examplesare labeled neutral while the 20 hardest examplesare a mixture of entailment and contradiction..in mc-taco, each example contains a vary-ing number of answer choices.
for each choice, amodel needs to predict whether the answer is trueor false.
we ﬁnd that all answer choices in top20 easiest examples are labeled false (the majorityclass), whereas for difﬁcult examples the answerchoices are either all true or a mix of true andfalse (table 2).
for squad2.0 and quail, we.
analyze the context length, the answerability ofa question, and the lexical overlap between con-text and questions.
however, we do not ﬁnd anyclear evidence that any of them might indicate thedifﬁculty level of test examples..for boolq, we observe that the 20 most discrim-inating examples are all labeled false while 13 ofthe 20 least discriminating examples are labeledtrue.
table 2 shows the hardest and the easiestexamples of mnli and mc-taco..5 related work.
prior work on using irt to evaluate nlp systemsmostly relies on human responses.
hopkins andmay (2013) use irt to estimate the relative abil-ity of a set of machine translation systems usingresponses from pairwise comparison of system out-puts by human judges.
otani et al.
(2016) extendthis work by including a baseline translation tothe pairwise comparison.
lalor et al.
(2016, 2018)use irt to identify hard examples in natural lan-guage inference data based on human responses.
in a follow-up study, lalor et al.
(2019) comparehuman versus model responses and ﬁnd that bothare positively correlated and demonstrate the usecases of irt parameters in training set ﬁltering.
se-doc and ungar (2020) use irt to evaluate chatbotsystems..the work by martínez-plumed et al.
(2019) isthe ﬁrst to study the idea of using model responses(as opposed to human responses) for irt in ma-chine learning research.
for nlu, lalor and yu.
1148(2020) use model responses to estimate difﬁcultyparameters of several glue datasets for dynamicdata selection in curriculum learning.
in concurrentwork, rodriguez et al.
(2021) study how irt canbe used for more nuanced leaderboard evaluations.
their experiments demonstrate that irt can pro-duce a more reliable ranking of models than thetraditional metrics.
they also show that irt is notonly useful for better understanding of individualexamples in the dataset and task, but also effectivein identifying annotation errors..for other dataset evaluations, in addition toproviding a benchmark, the superglue paperalso compares a set of candidate datasets using aﬁxed pool of machine learning models and humanannotators (nangia and bowman, 2019).
wanget al.
(2019a) investigate pretraining tasks andparadigms for effective transfer learning methods.
pruksachatkun et al.
(2020a) study when and whyintermediate-task training is useful for a given tar-get task.
vu et al.
(2020) introduce task embed-dings to predict the most beneﬁcial source task for agiven target task.
schlegel et al.
(2020) propose anevaluation framework for machine reading compre-hension (mrc) datasets and reveal some concernsregarding factual correctness and the presence oflinguistic cues in existing mrc gold datasets..6 conclusion.
given the large number of nlu datasets introducedin recent years, what kinds of datasets are effec-tive to measure near-future progress?
our analysison 29 test sets using irt gives us reason to be-lieve that, among the datasets we evaluate, quoref,hellaswag, and mc-taco are best able to dis-criminate among current (and likely future) strongmodels.
meanwhile, snli, mnli, and commit-mentbank seem to be saturated and ineffective formeasuring future progress..our analysis of examples’ difﬁculty and discrim-ination parameters shows that datasets with manyhard examples do not always contain examples thatcan discriminate between strong and weak mod-els.
we ﬁnd that qa datasets are more difﬁcultthan other datasets.
we also ﬁnd span selection asthe most effective task format for discriminatingbetween strong and weak models..according to our leh score, datasets that seemto be solved are unlikely to see improvements withfuture pretrained models.
therefore, the skills theyintend to test are either largely solved, to the extent.
that they are solvable, or not well isolated (e.g.,due to data artifacts).
focusing on the skills forwhich these solved test sets are originally designedto evaluate would most likely require a new datasetthat better isolates the reasoning ability of interest.
on the other hand, datasets that perform wellaccording to our leh metric show the best signsof being amenable to future hill-climbing.
thisdoes not entail that we should focus future researchon these benchmarks, since we do not evaluatewhether they test the skills they mean to test, orwhether these skills are important for scientiﬁc orpractical progress on natural language understand-ing.
finally, we argue that this evaluation should bedone periodically, as datasets and models improveover time.
for.
one can study multi-dimensional variables for both model ability anditem parameters, which could reveal a factorizationof datasets by skills.
other potential directions in-clude expanding our analysis to a broader range oftasks and analyzing the relationship between theestimated irt parameters and the human-modelgap..future work,.
acknowledgments.
we thank john lalor, joão sedoc, nikita nangia,sebastian schuster, iacer calixto, and the anony-mous reviewers for feedback.
this work has ben-eﬁted from ﬁnancial support to sb by eric andwendy schmidt (made by recommendation of theschmidt futures program), samsung research (un-der the project improving deep learning usinglatent structure), apple, and intuit, and from in-kind support by the nyu high-performance com-puting center and by nvidia corporation (withthe donation of a titan v gpu).
this material isbased upon work supported by the national sci-ence foundation under grant no.
1922658. anyopinions, ﬁndings, and conclusions or recommen-dations expressed in this material are those of theauthor(s) and do not necessarily reﬂect the viewsof the national science foundation..ethical considerations.
we present an objective approach for comparing thedifﬁculty of test sets examples across datasets anddemonstrate it on a large set of established datasets.
we expect this to contribute to the developmentof more challenging benchmarks for nlp datasetsand potentially to develop more challenging mod-.
1149els.
one concern worth noting is that most of theevaluation datasets we study are crowdsourced ordrawn from naturally occurring data.
thus, theylikely demonstrate harmful stereotypes to some de-gree or even score models more highly for demon-strating them.
in general, models that perform wellon these datasets should not be deployed directlywithout additional measures to measure and elim-inate any harms that stereotypes like these couldcause in the target application settings..references.
frank b. baker and seock-ho kim.
1993..item re-sponse theory : parameter estimation techniques.
journal ofthe american statistical association,88:707..luisa bentivogli, peter clark, ido dagan, and danilogiampiccolo.
2009. the fifth pascal recogniz-ing textual entailment challenge.
in tac..chandra bhagavatula, ronan le bras, chaitanyamalaviya, keisuke sakaguchi, ari holtzman, han-nah rashkin, doug downey, wen tau yih, and yejinchoi.
2020. abductive commonsense reasoning.
in international conference on learning represen-tations..eli bingham, jonathan p. chen, martin jankowiak,fritz obermeyer, neeraj pradhan, theofanis kar-aletsos, rohit singh, paul a. szerlip, paul horsfall,and noah d. goodman.
2019. pyro: deep universalprobabilistic programming.
j. mach.
learn.
res.,20:28:1–28:6..yonatan bisk, rowan zellers, ronan lebras, jianfenggao, and yejin choi.
2020. piqa: reasoning aboutphysical commonsense in natural language.
inthe thirty-fourth aaai conference on artiﬁcial in-telligence, aaai 2020, the thirty-second innova-tive applications of artiﬁcial intelligence confer-ence, iaai 2020, the tenth aaai symposium on ed-ucational advances in artiﬁcial intelligence, eaai2020, new york, ny, usa, february 7-12, 2020,pages 7432–7439.
aaai press..samuel r. bowman, gabor angeli, christopher potts,and christopher d. manning.
2015. a large anno-tated corpus for learning natural language inference.
in proceedings of the 2015 conference on empiri-cal methods in natural language processing, pages632–642, lisbon, portugal.
association for compu-tational linguistics..(long and short papers), pages 2924–2936, min-neapolis, minnesota.
association for computationallinguistics..peter clark, isaac cowhey, oren etzioni, tushar khot,ashish sabharwal, carissa schoenick, and oyvindtafjord.
2018. think you have solved question an-swering?
try arc, the ai2 reasoning challenge.
arxiv preprint arxiv:1803.05457..alexis conneau, kartikay khandelwal, naman goyal,vishrav chaudhary, guillaume wenzek, franciscoguzmán, edouard grave, myle ott, luke zettle-moyer, and veselin stoyanov.
2020. unsupervisedcross-lingual representation learning at scale.
inproceedings of the 58th annual meeting of the asso-ciation for computational linguistics, pages 8440–8451, online.
association for computational lin-guistics..leyang cui, yu wu, shujie liu, yue zhang, and mingzhou.
2020. mutual: a dataset for multi-turn dia-logue reasoning.
in proceedings of the 58th annualmeeting of the association for computational lin-guistics, pages 1406–1416, online.
association forcomputational linguistics..ido dagan, oren glickman, and bernardo magnini.
2005. the pascal recognising textual entailmentchallenge.
in machine learning challenges work-shop, pages 177–190.
springer..pradeep dasigi, nelson f. liu, ana marasovi´c,noah a. smith, and matt gardner.
2019. quoref:a reading comprehension dataset with questions re-in proceedings ofquiring coreferential reasoning.
the 2019 conference on empirical methods in nat-ural language processing and the 9th internationaljoint conference on natural language processing(emnlp-ijcnlp), pages 5925–5932, hong kong,china.
association for computational linguistics..marie-catherine de marneffe, mandy simons, andjudith tonhauser.
2019. the commitmentbank:investigating projection in naturally occurring dis-course.
in proceedings of sinn und bedeutung, vol-ume 23, pages 107–124..jacob devlin, ming-wei chang, kenton lee, andkristina toutanova.
2019. bert: pre-training ofdeep bidirectional transformers for language under-in proceedings of the 2019 conferencestanding.
of the north american chapter of the associationfor computational linguistics: human languagetechnologies, volume 1 (long and short papers),pages 4171–4186, minneapolis, minnesota.
associ-ation for computational linguistics..christopher clark, kenton lee, ming-wei chang,tom kwiatkowski, michael collins, and kristinatoutanova.
2019. boolq: exploring the surprisingin proceed-difﬁculty of natural yes/no questions.
ings of the 2019 conference of the north americanchapter of the association for computational lin-guistics: human language technologies, volume 1.adam fisch, alon talmor, robin jia, minjoon seo, eu-nsol choi, and danqi chen.
2019. mrqa 2019shared task: evaluating generalization in readingin proceedings of the 2nd work-comprehension.
shop on machine reading for question answering,pages 1–13, hong kong, china.
association forcomputational linguistics..1150danilo giampiccolo, bernardo magnini, ido dagan,and bill dolan.
2007. the third pascal recogniz-ing textual entailment challenge.
in proceedings ofthe acl-pascal workshop on textual entailmentand paraphrasing, pages 1–9, prague.
associationfor computational linguistics..formance through an examination of test set difﬁ-culty: a psychometric case study.
in proceedings ofthe 2018 conference on empirical methods in nat-ural language processing, pages 4711–4716, brus-sels, belgium.
association for computational lin-guistics..ivan habernal, henning wachsmuth, iryna gurevych,and benno stein.
2018. the argument reasoningcomprehension task: identiﬁcation and reconstruc-tion of implicit warrants.
in proceedings of the 2018conference of the north american chapter of theassociation for computational linguistics: humanlanguage technologies, volume 1 (long papers),pages 1930–1940, new orleans, louisiana.
associ-ation for computational linguistics..r bar haim, ido dagan, bill dolan, lisa ferro, danilogiampiccolo, bernardo magnini, and idan szpektor.
2006. the second pascal recognising textual entail-ment challenge.
in proceedings of the second pas-cal challenges workshop on recognising textualentailment..pengcheng he, xiaodong liu, jianfeng gao, andweizhu chen.
2020. deberta: decoding-enhancedbert with disentangled attention..mark hopkins and jonathan may.
2013. models oftranslation competitions.
in proceedings of the 51stannual meeting of the association for computa-tional linguistics (volume 1: long papers), pages1416–1424, soﬁa, bulgaria.
association for compu-tational linguistics..lifu huang, ronan le bras, chandra bhagavatula, andyejin choi.
2019. cosmos qa: machine readingcomprehension with contextual commonsense rea-soning.
in proceedings of the 2019 conference onempirical methods in natural language processingand the 9th international joint conference on natu-ral language processing (emnlp-ijcnlp), pages2391–2401, hong kong, china.
association forcomputational linguistics..tom kwiatkowski, jennimaria palomaki, olivia red-ﬁeld, michael collins, ankur parikh, chris al-berti, danielle epstein, illia polosukhin, jacob de-vlin, kenton lee, kristina toutanova, llion jones,matthew kelcey, ming-wei chang, andrew m. dai,jakob uszkoreit, quoc le, and slav petrov.
2019.natural questions: a benchmark for question an-swering research.
transactions of the associationfor computational linguistics, 7:452–466..guokun lai, qizhe xie, hanxiao liu, yiming yang,and eduard hovy.
2017. race: large-scale read-ining comprehension dataset from examinations.
proceedings of the 2017 conference on empiricalmethods in natural language processing, pages785–794, copenhagen, denmark.
association forcomputational linguistics..john p. lalor, hao wu, tsendsuren munkhdalai, andhong yu.
2018. understanding deep learning per-.
john p. lalor, hao wu, and hong yu.
2016. build-ing an evaluation scale using item response theory.
in proceedings of the 2016 conference on empiri-cal methods in natural language processing, pages648–657, austin, texas.
association for computa-tional linguistics..john p. lalor, hao wu, and hong yu.
2019. learn-ing latent parameters without human response pat-terns: item response theory with artiﬁcial crowds.
inproceedings of the 2019 conference on empiricalmethods in natural language processing and the9th international joint conference on natural lan-guage processing (emnlp-ijcnlp), pages 4249–4259, hong kong, china.
association for computa-tional linguistics..john p. lalor and hong yu.
2020. dynamic data selec-tion for curriculum learning via ability estimation.
in findings of the association for computationallinguistics: emnlp 2020, pages 545–555, online.
association for computational linguistics..zhenzhong lan, mingda chen, sebastian goodman,kevin gimpel, piyush sharma, and radu soricut.
2020. albert: a lite bert for self-supervisedin 8th inter-learning of language representations.
national conference on learning representations,iclr 2020..hector levesque, ernest davis, and leora morgen-stern.
2012. the winograd schema challenge.
inthirteenth international conference on the princi-ples of knowledge representation and reasoning.
citeseer..yinhan liu, myle ott, naman goyal, jingfei du, man-dar joshi, danqi chen, omer levy, mike lewis,luke zettlemoyer, and veselin stoyanov.
2019.roberta: a robustly optimized bert pretraining ap-proach.
unpublished manuscript available on arxiv..fernando martínez-plumed, ricardo b.c.
prudêncio,adolfo martínez-usó, and josé hernández-orallo.
item response theory in ai: analysing ma-2019.chine learning classiﬁers at the instance level.
artiﬁ-cial intelligence, 271:18 – 42..julian michael, gabriel stanovsky, luheng he, ido da-gan, and luke zettlemoyer.
2018. crowdsourcingin pro-question-answer meaning representations.
ceedings of the 2018 conference of the north amer-ican chapter of the association for computationallinguistics: human language technologies, vol-ume 2 (short papers), pages 560–568, new orleans,louisiana.
association for computational linguis-tics..1151nikita nangia and samuel r. bowman.
2019. humanvs. muppet: a conservative estimate of human per-formance on the glue benchmark.
in proceedingsof the 57th annual meeting of the association forcomputational linguistics, pages 4566–4575, flo-rence, italy.
association for computational linguis-tics..yixin nie, adina williams, emily dinan, mohitbansal, jason weston, and douwe kiela.
2020. ad-versarial nli: a new benchmark for natural lan-guage understanding.
in proceedings of the 58th an-nual meeting of the association for computationallinguistics, pages 4885–4901, online.
associationfor computational linguistics..timothy niven and hung-yu kao.
2019. probing neu-ral network comprehension of natural language ar-guments.
in proceedings of the 57th annual meet-ing of the association for computational linguis-tics, pages 4658–4664, florence, italy.
associationfor computational linguistics..simon ostermann, ashutosh modi, michael roth, ste-fan thater, and manfred pinkal.
2018. mcscript:a novel dataset for assessing machine comprehen-in proceedings ofsion using script knowledge.
the eleventh international conference on languageresources and evaluation (lrec 2018), miyazaki,japan.
european language resources association(elra)..naoki otani, toshiaki nakazawa, daisuke kawahara,irt-based aggrega-and sadao kurohashi.
2016.tion model of crowdsourced pairwise comparisonin proceed-for evaluating machine translations.
ings of the 2016 conference on empirical methodsin natural language processing, pages 511–520,austin, texas.
association for computational lin-guistics..adam paszke, sam gross, francisco massa, adamlerer, james bradbury, gregory chanan, trevorkilleen, zeming lin, natalia gimelshein, lucaantiga, alban desmaison, andreas kopf, edwardyang, zachary devito, martin raison, alykhan te-jani, sasank chilamkurthy, benoit steiner, lu fang,junjie bai, and soumith chintala.
2019. pytorch:an imperative style, high-performance deep learn-ing library.
in h. wallach, h. larochelle,a. beygelzimer, f. d’ alché-buc, e. fox, and r. gar-nett, editors, advances in neural information pro-cessing systems 32, pages 8024–8035.
curran asso-ciates, inc..mohammad taher pilehvar and jose camacho-collados.
2019. wic: the word-in-context datasetfor evaluating context-sensitive meaning represen-in proceedings of the 2019 conferencetations.
of the north american chapter of the associationfor computational linguistics: human languagetechnologies, volume 1 (long and short papers),pages 1267–1273, minneapolis, minnesota.
associ-ation for computational linguistics..yada pruksachatkun,.
jason phang, haokun liu,phu mon htut, xiaoyi zhang, richard yuanzhepang, clara vania, katharina kann, and samuel r.bowman.
2020a.
intermediate-task transfer learningwith pretrained language models: when and whyin proceedings of the 58th annualdoes it work?
meeting of the association for computational lin-guistics, pages 5231–5247, online.
association forcomputational linguistics..yada pruksachatkun, phil yeres, haokun liu, jasonphang, phu mon htut, alex wang, ian tenney, andsamuel r. bowman.
2020b.
jiant: a softwaretoolkit for research on general-purpose text under-standing models.
in proceedings of the 58th annualmeeting of the association for computational lin-guistics: system demonstrations, pages 109–117,online.
association for computational linguistics..pranav rajpurkar, robin jia, and percy liang.
2018.know what you don’t know: unanswerable ques-in proceedings of the 56th an-tions for squad.
nual meeting of the association for computationallinguistics (volume 2: short papers), pages 784–789, melbourne, australia.
association for compu-tational linguistics..pranav rajpurkar, jian zhang, konstantin lopyrev, andpercy liang.
2016. squad: 100,000+ questions formachine comprehension of text.
in proceedings ofthe 2016 conference on empirical methods in natu-ral language processing, pages 2383–2392, austin,texas.
association for computational linguistics..rajesh ranganath, sean gerrish, and david m. blei.
2014. black box variational inference.
in proceed-ings of the seventeenth international conference onartiﬁcial intelligence and statistics, aistats 2014,reykjavik, iceland, april 22-25, 2014, volume 33of jmlr workshop and conference proceedings,pages 814–822.
jmlr.org..pedro rodriguez, joe barrow, alexander hoyle, john p.lalor, robin jia, and boyd-graber jordan.
2021.evaluation examples are not equally informative:how should that change nlp leaderboards?
in pro-ceedings of the 59th annual meeting of the associa-tion for computational linguistics, online.
associa-tion for computational linguistics..melissa roemmele, cosmin adrian bejan, and an-drew s gordon.
2011. choice of plausible alterna-tives: an evaluation of commonsense causal rea-soning.
in aaai spring symposium: logical formal-izations of commonsense reasoning, pages 90–95..anna rogers, olga kovaleva, matthew downey, andanna rumshisky.
2020. getting closer to ai com-plete question answering: a set of prerequisitein the thirty-fourth aaai conferencereal tasks.
on artiﬁcial intelligence, aaai 2020, the thirty-second innovative applications of artiﬁcial intelli-gence conference, iaai 2020, the tenth aaai sym-posium on educational advances in artiﬁcial intel-ligence, eaai 2020, new york, ny, usa, february7-12, 2020, pages 8722–8731.
aaai press..1152keisuke sakaguchi, ronan le bras, chandra bhagavat-ula, and yejin choi.
2020. winogrande: an adver-sarial winograd schema challenge at scale.
in pro-ceedings of the aaai conference on artiﬁcial intel-ligence, volume 34, pages 8732–8740..maarten sap, hannah rashkin, derek chen, ronanle bras, and yejin choi.
2019. social iqa: com-monsense reasoning about social interactions.
inproceedings of the 2019 conference on empiricalmethods in natural language processing and the9th international joint conference on natural lan-guage processing (emnlp-ijcnlp), pages 4463–4473, hong kong, china.
association for computa-tional linguistics..viktor schlegel, marco valentino, andre freitas,goran nenadic, and riza batista-navarro.
2020. aframework for evaluation of machine reading com-in proceedings of theprehension gold standards.
12th language resources and evaluation confer-ence, pages 5359–5369, marseille, france.
euro-pean language resources association..joão sedoc and lyle ungar.
2020. item response the-ory for efﬁcient human evaluation of chatbots.
inproceedings of the first workshop on evaluationand comparison of nlp systems, pages 21–33, on-line.
association for computational linguistics..saku sugawara, kentaro inui, satoshi sekine, andakiko aizawa.
2018. what makes reading com-in proceedings ofprehension questions easier?
the 2018 conference on empirical methods in nat-ural language processing, pages 4208–4219, brus-sels, belgium.
association for computational lin-guistics..micke, subhransu maji, and mohit iyyer.
2020. ex-ploring and predicting transferability across nlpin proceedings of the 2020 conference ontasks.
empirical methods in natural language process-ing (emnlp), pages 7882–7926, online.
associa-tion for computational linguistics..alex wang, jan hula, patrick xia, raghavendra pap-pagari, r. thomas mccoy, roma patel, najoungkim, ian tenney, yinghui huang, katherin yu,shuning jin, berlin chen, benjamin van durme,edouard grave, ellie pavlick, and samuel r. bow-man.
2019a.
can you tell me how to get past sesamestreet?
sentence-level pretraining beyond languagemodeling.
in proceedings of the 57th annual meet-ing of the association for computational linguistics,pages 4465–4476, florence, italy.
association forcomputational linguistics..alex wang, yada pruksachatkun, nikita nangia,amanpreet singh, julian michael, felix hill, omerlevy, and samuel bowman.
2019b.
superglue:a stickier benchmark for general-purpose lan-guage understanding systems.
in h. wallach,h. larochelle, a. beygelzimer, f. dálché-buc,e. fox, and r. garnett, editors, advances in neu-ral information processing systems 32, pages 3266–3280. curran associates, inc..alex wang, amanpreet singh, julian michael, fe-lix hill, omer levy, and samuel bowman.
2018.glue: a multi-task benchmark and analysis plat-in pro-form for natural language understanding.
the 2018 emnlp workshop black-ceedings ofboxnlp: analyzing and interpreting neural net-works for nlp, pages 353–355, brussels, belgium.
association for computational linguistics..alon talmor, jonathan herzig, nicholas lourie, andjonathan berant.
2019. commonsenseqa: a ques-tion answering challenge targeting commonsenseknowledge.
in proceedings of the 2019 conferenceof the north american chapter of the associationfor computational linguistics: human languagetechnologies, volume 1 (long and short papers),pages 4149–4158, minneapolis, minnesota.
associ-ation for computational linguistics..adina williams, nikita nangia, and samuel bowman.
2018. a broad-coverage challenge corpus for sen-tence understanding through inference.
in proceed-ings of the 2018 conference of the north americanchapter of the association for computational lin-guistics: human language technologies, volume1 (long papers), pages 1112–1122, new orleans,louisiana.
association for computational linguis-tics..adam trischler, tong wang, xingdi yuan, justin har-ris, alessandro sordoni, philip bachman, and ka-heer suleman.
2017. newsqa: a machine compre-in proceedings of the 2nd work-hension dataset.
shop on representation learning for nlp, pages191–200, vancouver, canada.
association for com-putational linguistics..ashish vaswani, noam shazeer, niki parmar, jakobuszkoreit, llion jones, aidan n gomez, ł ukaszkaiser, and illia polosukhin.
2017. attention is allyou need.
in advances in neural information pro-cessing systems, volume 30, pages 5998–6008.
cur-ran associates, inc..tu vu, tong wang, tsendsuren munkhdalai, alessan-dro sordoni, adam trischler, andrew mattarella-.
thomas wolf, lysandre debut, victor sanh, julienchaumond, clement delangue, anthony moi, pier-ric cistac, tim rault, remi louf, morgan funtow-icz, joe davison, sam shleifer, patrick von platen,clara ma, yacine jernite, julien plu, canwen xu,teven le scao, sylvain gugger, mariama drame,quentin lhoest, and alexander rush.
2020. trans-formers: state-of-the-art natural language process-ing.
in proceedings of the 2020 conference on em-pirical methods in natural language processing:system demonstrations, pages 38–45, online.
asso-ciation for computational linguistics..rowan zellers, yonatan bisk, roy schwartz, andyejin choi.
2018. swag: a large-scale adversar-ial dataset for grounded commonsense inference.
inproceedings of the 2018 conference on empirical.
1153methods in natural language processing, pages 93–104, brussels, belgium.
association for computa-tional linguistics..rowan zellers, ari holtzman, yonatan bisk, alifarhadi, and yejin choi.
2019. hellaswag: canin pro-a machine really ﬁnish your sentence?
ceedings of the 57th annual meeting of the asso-ciation for computational linguistics, pages 4791–4800, florence, italy.
association for computationallinguistics..tianyi zhang, felix wu, arzoo katiyar, kilian q wein-berger, and yoav artzi.
2021a.
revisiting few-sample bert ﬁne-tuning.
in international confer-ence on learning representations..yian zhang, alex warstadt, haau-sing li, andsamuel r. bowman.
2021b.
when do you need bil-lions of words of pretraining data?
in proceedingsof the 59th annual meeting of the association forcomputational linguistics, online.
association forcomputational linguistics..ben zhou, daniel khashabi, qiang ning, and danroth.
2019.
“going on a vacation” takes longerthan “going for a walk”: a study of temporal com-in proceedings of themonsense understanding.
2019 conference on empirical methods in natu-ral language processing and the 9th internationaljoint conference on natural language processing(emnlp-ijcnlp), pages 3363–3369, hong kong,china.
association for computational linguistics..1154a discrimination vs. guessing.
in addition to the analysis of discrimination versusdifﬁculty parameters, we also look at the distri-bution of the guessing (γ) parameters.
from fig-ure 6, we observe that all qa datasets with spanselection format generally have low guessing pa-rameters, meaning that they are difﬁcult to predictcorrectly by random guessing.
this makes sense asspan selection has higher label entropy than clas-siﬁcation or multiple-choice task.
we ﬁnd thatseveral datasets have examples with varying guess-ing parameters: for snli we see a high density ofexamples that can be predicted easily by randomguessing while for mnli, hellaswag, and mc-script, there are more examples with low guessingparameters..b additional analysis on model ability.
figure 7 plots model abilities θ against their aver-age accuracy over all test examples, where eachpoint represents a model checkpoint (section 3.2).
we use different colors for different models (e.g.,dark blue for albert-xxl-v2), and differentshapes to mark different checkpoints..since we only perform tuning on robertalarge,some of these models might have worse perfor-mance than if they were individually tuned..c task descriptions.
in this section, we provide a short description foreach dataset..rte the series of recognizing textual entail-ment datasets (dagan et al., 2005; haim et al.,2006; giampiccolo et al., 2007; bentivogli et al.,2009) correspond to a two-class textual entailmentclassiﬁcation task.
given a premise sentence and ahypothesis sentence, the task is to decide whetherthe premise entails the hypothesis..snli the stanford natural language inferencecorpus (bowman et al., 2015) is a textual entail-ment dataset, formulated as a three-class classiﬁ-cation task.
given a premise sentence and a hy-pothesis sentence, the task is to determine if thepremise entails the hypothesis, contradicts it, or nei-ther.
the snli dataset is created using premisestaken from image captions..mnli the multi-genre natural language infer-ence corpus (williams et al., 2018) is also a textualentailment dataset, similar to that of snli.
the.
mnli dataset is built to cover a broad range ofgenres, including written and spoken text.
half ofits test set is created from text that is out of domainrelative to the training set..commitmentbank commitmentbank(de marneffe et al., 2019) is a dataset for-mulated as a three-classtextual entailmentclassiﬁcation task.
given a piece of text and anembedded clause, models must decide whether theembedded clause is entailed by the text..arct the argument reasoning comprehen-sion task (habernal et al., 2018) is a multiple-choice question answering dataset.
given an ar-gument, a claim, and a premise, the task is to selectthe correct implicit warrant (which explains whythe premise implies the claim) from two choices..arc-easy arc (clark et al., 2018)is amultiple-choice qa dataset composed of realmultiple-choice science questions in grade schools.
arc-easy is composed of the easier questionsthat do not satisfy the criteria used to built arc-challenge (described below)..arc-challenge arc-challenge (clark et al.,2018) is the subset of arc that contains questionsthat are incorrectly answered by both a retrieval-based algorithm and a word co-occurrence algo-rithm..mcscript the mcscript (ostermann et al.,2018) is a qa dataset with multiple-choice format.
the dataset tests models’ commonsense knowl-edge, in particular, script knowledge which cor-responds to the sequence of actions people do in aparticular situation..cosmos qa cosmos qa (huang et al., 2019) isa multiple-choice reading comprehension dataset,and it is intended to require extensive abstrac-tive commonsense reasoning.
unlike common-senseqa, cosmos qa requires comprehensionover an auxiliary article, instead of simply respond-ing to a free-standing question..hellaswag hellaswag (zellers et al., 2019) is acommonsense reasoning multiple-choice dataset.
it is built using adversarial ﬁltering with bert.
given a story, the task is to select the most plausiblecontinuation..boolq boolq (clark et al., 2019) is a boolean(yes/no) reading comprehension qa dataset built.
1155figure 6: plots of the log discrimination (log α) versus the guessing (γ) parameters for each dataset..figure 7: average model accuracy over all datasets vs. ability (θ).
the three different hyperparameter conﬁgura-tions of each miniberta are represented by a single color for ease of readability.
best viewed in color..using the same pipeline used to produce the (non-boolean) natural questions (kwiatkowski et al.,2019)..mutual mutual (cui et al., 2020) is a multiple-choice qa dataset for multi-turn dialogue reason-ing.
the dataset is created from chinese students’english listening comprehension exams, and it isintended to require a variety of commonsense rea-soning skills..mutual-plus mutual-plus (cui et al., 2020) isa variant of mutual, in which one of the choices ineach set of answers is replaced by a safe response(i.e., “could you repeat that”).
if all other choicesare incorrect, then the model is supposed to selectthe safe response.
this variant of mutual is builtso that we can evaluate if the model can select thesafe response when all other options are incorrect..quail quail (rogers et al., 2020) is a read-ing comprehension dataset formulated as a mul-tiple choice task.
one feature of quail is thatit combines “commonsense, text-based, and unan-swerable questions.” it is also designed such that ithas a balanced distribution of genres and reasoningtypes..copa choice of plausible alternatives (roem-mele et al., 2011) is a dataset for sentence-levelmultiple-choice task.
given a premise and a ques-tion that asks for the cause or effect of the premise,the task is to choose the most plausible hypothesisfrom two options..wsc the winogradschema challenge(levesque et al., 2012) is a sentence-level multiple-choice commonsense reasoning dataset.
givena piece of text, a pronoun, and a list of possiblenoun phrases, the model must choose the correct.
1156where the question-answer pairs are created fromsentences’ predicate-argument relationships..newsqa newsqa (trischler et al., 2017) is aqa dataset formulated as span selection task.
thedataset is built by crowdworkers using passagestaken from cnn news articles..squad2.0 squad2.0 (rajpurkar et al., 2018)is a qa dataset that combines the span-selectionreading-comprehension questions in squad 1.1(rajpurkar et al., 2016) with over 50,000 unanswer-able questions.
the unanswerable questions werewritten by crowdworkers to look like the answer-able ones.
a model must either select an answerspan or decline to answer..quoref quoref (dasigi et al., 2019) is a qadataset that is designed to test coreferential rea-soning ability.
the dataset is formulated as a spanselection qa task..mrqa natural questions the natural ques-tions dataset (kwiatkowski et al., 2019) is a datasetdesigned to test a model’s ability in reading compre-hension.
the questions are taken from real-wordqueries, while the context passages are taken fromwikipedia articles.
we use the mrqa version of itwhich contains a preprocessed version of a subsetof questions in natural questions..anli the adversarial natural language infer-ence dataset (nie et al., 2020) is a textual entail-ment dataset built using an iterative human-and-model-in-the-loop procedure in order to ﬁnd hardexamples..referent to the pronoun.
the dataset is designedsuch that world knowledge is required to make thecorrect choices.
we use the superglue (wanget al., 2019b) version of the dataset..commonsenseqa commonsenseqa (talmoret al., 2019) is a multiple-choice qa dataset whichis designed to test a range of commonsense knowl-edge..socialiqa socialiqa (sap et al., 2019) is adataset that is speciﬁcally designed to test a models’capabilities related to emotional and social intelli-gence in everyday situations..mc-taco mc-taco (zhou et al., 2019) isa multiple-choice qa dataset that is designed totest temporal commonsense reasoning, in particu-lar: duration, temporal ordering, typical time, fre-quency, and stationarity.
each question consistsof a varying number of choices, and for each an-swer choice, a model needs to predict whether theanswer is correct or incorrect..(pilehvar.
wic the word-in-contextandcamacho-collados, 2019) dataset which isdesigned to test the word sense disambiguationskill of a model.
given two pieces of text (a phraseor a sentence) with a polysemous word in both, amodel needs to predict whether the two words areused in the same sense..piqa the physical interaction question an-swering dataset (bisk et al., 2020) is a multiple-choice qa dataset that is designed to test the physi-cal commonsense reasoning skill.
given a physicaltask expressed in text, a model needs to select themost sensible solution..winogrande the winogrande dataset (sak-aguchi et al., 2020) is built through a crowdsourc-ing procedure that incorporates adversarial ﬁltering.
given a sentence with a blank (where the blank cor-responds to a noun phrase), the task is to select thecorrect ﬁller.
the dataset is designed to test thecommonsense reasoning skill..abductive nli the abductive natural lan-guage inference dataset (bhagavatula et al., 2020)is a multiple-choice dataset.
given a premise, thetask is to select the most likely explanation fromthe given hypotheses..qamr the question-answer meaning repre-sentations (michael et al., 2018) is a qa dataset.
11573-m1-smr.2-m1-smr.1-m1-smr.3-b1-br.2-b1-br.1-b1-br.3-m01-br.2-m01-br.1-m01-br.3-m001-br.2-m001-br.1-m001.
-.
br.bb.
-.
rmlx.br.trebla.tseb.
0.
85.
3.
97.
9.
06.
3.
26.
7.
75.
7.
93.
4.
63.
5.
33.
0.
25.
3.
76.
1.
52.
3.
42.
1.
44.
9.
95.
9.
35.
9.
55.
7.
84.
8.
92.
8.
92.
6.
46.
1.
06.
9.
66.
6.
04.
4.
92.
4.
24.
2.
04.
8.
14.
7.
9.
8.
72.
2.
85.
4.
24.
8.
61.
9.
06.
5.
87.
6.
26.
3.
26.
5.
06.
8.
93.
6.
63.
2.
43.
0.
45.
9.
15.
5.
22.
0.
72.
1.
64.
2.
95.
3.
35.
9.
55.
9.
05.
1.
72.
9.
13.
6.
46.
7.
16.
6.
86.
0.
24.
0.
03.
3.
14.
4.
04.
7.
83.
9.
8.
7.
82.
2.
85.
1.
34.
6.
12.
9.
06.
1.
97.
3.
16.
6.
26.
8.
36.
8.
73.
1.
63.
8.
13.
0.
85.
6.
95.
6.
12.
7.
02.
5.
34.
2.
06.
9.
25.
0.
55.
6.
74.
8.
72.
8.
13.
1.
66.
6.
06.
3.
76.
9.
93.
9.
92.
3.
34.
9.
53.
7.
04.
5.
6.
0.
42.
1.
75.
0.
14.
6.
41.
7.
17.
2.
98.
8.
97.
8.
08.
8.
48.
1.
15.
3.
14.
7.
14.
0.
27.
5.
16.
0.
34.
5.
04.
6.
55.
3.
86.
3.
06.
2.
26.
8.
25.
8.
23.
5.
64.
3.
07.
8.
77.
1.
47.
0.
45.
5.
63.
5.
26.
5.
35.
3.
75.
6.
47.
9.
54.
8.
48.
5.
16.
0.
65.
5.
46.
4.
88.
9.
77.
9.
87.
2.
38.
2.
74.
2.
04.
0.
14.
0.
27.
8.
55.
7.
83.
4.
14.
9.
65.
5.
66.
1.
65.
3.
06.
6.
55.
8.
03.
0.
34.
7.
86.
2.
67.
3.
47.
3.
45.
4.
63.
1.
95.
5.
35.
1.
45.
1.
37.
4.
24.
5.
38.
6.
06.
1.
05.
4.
76.
7.
88.
8.
77.
7.
97.
5.
38.
5.
94.
4.
04.
7.
04.
0.
47.
6.
95.
3.
14.
8.
73.
5.
85.
8.
56.
7.
75.
9.
26.
7.
35.
4.
92.
8.
14.
3.
07.
7.
37.
6.
37.
9.
55.
5.
73.
9.
36.
9.
45.
2.
45.
7.
37.
4.
04.
7.
28.
1.
26.
0.
15.
2.
75.
7.
58.
6.
86.
8.
07.
3.
77.
4.
74.
5.
04.
3.
83.
0.
07.
0.
05.
3.
32.
8.
82.
8.
05.
4.
26.
0.
55.
0.
06.
2.
15.
4.
03.
0.
43.
5.
66.
7.
96.
9.
17.
9.
84.
0.
33.
3.
35.
8.
54.
5.
84.
6.
26.
5.
72.
8.
57.
8.
25.
5.
43.
4.
95.
1.
68.
6.
07.
2.
17.
6.
36.
6.
84.
0.
24.
2.
83.
0.
85.
6.
95.
1.
62.
2.
43.
9.
94.
3.
36.
8.
45.
4.
06.
8.
15.
4.
03.
0.
63.
1.
66.
5.
07.
7.
27.
8.
64.
6.
33.
8.
25.
8.
34.
0.
05.
4.
46.
6.
92.
2.
77.
0.
35.
6.
53.
7.
85.
1.
58.
3.
96.
6.
07.
5.
36.
6.
54.
7.
93.
0.
83.
0.
86.
8.
55.
1.
62.
1.
53.
8.
15.
4.
16.
0.
55.
6.
06.
2.
15.
8.
03.
0.
73.
6.
46.
9.
86.
5.
96.
0.
44.
8.
33.
8.
05.
2.
74.
8.
74.
4.
36.
1.
92.
8.
57.
9.
25.
9.
43.
7.
66.
5.
78.
6.
47.
5.
57.
5.
08.
6.
64.
1.
93.
7.
04.
0.
07.
6.
95.
5.
23.
8.
73.
0.
65.
0.
86.
7.
65.
5.
16.
6.
25.
8.
92.
0.
04.
5.
66.
5.
37.
5.
27.
5.
15.
7.
43.
8.
75.
6.
05.
6.
35.
4.
07.
7.
43.
7.
08.
5.
75.
9.
34.
7.
66.
8.
88.
6.
67.
4.
77.
3.
77.
1.
84.
9.
04.
8.
04.
0.
47.
5.
16.
1.
63.
5.
04.
2.
55.
9.
46.
4.
75.
7.
16.
5.
45.
4.
03.
6.
14.
0.
86.
7.
47.
5.
37.
4.
35.
4.
53.
7.
85.
7.
94.
6.
35.
8.
17.
5.
83.
9.
28.
9.
85.
7.
64.
9.
06.
6.
78.
5.
57.
1.
77.
7.
36.
7.
74.
8.
83.
3.
83.
0.
66.
5.
16.
8.
33.
8.
73.
4.
45.
3.
36.
9.
65.
7.
06.
9.
05.
8.
13.
1.
93.
5.
66.
1.
47.
3.
47.
7.
45.
9.
63.
2.
65.
9.
15.
2.
25.
1.
07.
2.
63.
0.
28.
0.
95.
0.
84.
9.
37.
6.
09.
4.
08.
1.
18.
7.
87.
6.
25.
5.
44.
1.
24.
0.
86.
7.
75.
4.
75.
7.
83.
9.
95.
7.
86.
4.
26.
7.
95.
0.
25.
8.
13.
8.
65.
2.
27.
7.
08.
7.
57.
0.
65.
2.
73.
5.
56.
2.
45.
4.
45.
5.
37.
0.
84.
3.
68.
1.
66.
9.
26.lb.
9.
18.
5.
09.
3.
58.
0.
58.
6.
48.
5.
85.
9.
24.
8.
34.
0.
08.
4.
56.
8.
06.
2.
34.
1.
66.
6.
96.
4.
66.
7.
56.
9.
25.
8.
93.
4.
16.
4.
47.
1.
48.
1.
67.
9.
46.
9.
34.
5.
27.
9.
56.
9.
25.
4.
67.
5.
35.
6.
88.
3.
76.
8.
96.
2.
75.
7.
19.
5.
78.
8.
78.
9.
96.
1.
06.
5.
14.
0.
24.
0.
26.
5.
16.
8.
32.
7.
74.
9.
83.
5.
17.
4.
67.
5.
35.
6.
25.
1.
83.
9.
04.
6.
67.
1.
09.
2.
48.
7.
17.
5.
47.
9.
62.
5.
56.
9.
36.
3.
97.
2.
65.
0.
78.
9.
66.
3.
67.
4.
08.
7.
19.
5.
68.
2.
68.
0.
88.
1.
65.
5.
14.
8.
04.
0.
27.
9.
67.
5.
85.
6.
84.
5.
07.
2.
17.
2.
27.
2.
86.
6.
46.
8.
13.
2.
35.
3.
67.
0.
58.
2.
28.
8.
66.
6.
16.
3.
37.
9.
36.
0.
76.
6.
77.
1.
35.
6.
88.
0.
56.
7.
66.lr.6.
78.
7.
29.
8.
98.
5.
98.
5.
09.
6.
66.
6.
44.
3.
14.
0.
68.
8.
87.
6.
47.
9.
55.
9.
97.
5.
17.
0.
58.
6.
77.
7.
77.
5.
73.
5.
26.
7.
68.
8.
29.
7.
58.
4.
97.
1.
48.
8.
78.
9.
77.
3.
37.
6.
97.
8.
75.
5.
19.
9.
96.
7.
87.
2.
18.
4.
29.
3.
88.
5.
88.
6.
08.
9.
57.
7.
75.
4.
45.
0.
69.
8.
87.
5.
08.
9.
55.
4.
97.
3.
47.
8.
38.
6.
08.
6.
58.
5.
74.
3.
96.
5.
38.
0.
69.
3.
78.
5.
68.
8.
98.
8.
98.
8.
28.
0.
87.
6.
97.
6.
95.
9.
98.
5.
17.
7.
38.
6.
68.
6.
29.
2.
09.
2.
09.
5.
09.
8.
37.
9.
84.
4.
44.
1.
97.
0.
98.
1.
27.
0.
44.
5.
87.
5.
07.
9.
38.
1.
77.
3.
97.
–.
0.
66.
1.
07.
0.
09.
1.
78.
9.
18.
2.
58.
3.
17.
6.
26.
9.
74.
1.
97.
–.
8.
68.
4.
75.
9.
47.mm.-ilnm.m.-ilnm.1r.-ilna.
2r.-ilna.
3r.-ilna.bc.tesatad.ilns.etr.apoc.csw.aqesnesnommoc.ilnevitcudba.ednargoniw.aqip.ocat-cm.aq.ilaicos.i.cw.tpircscm.qloob.aqsomsoc.gawsalleh.+lautum.lautum.liauq.
0.
2dauqs.-.
qnaqrm.ferouq.aqswen.rmaq.
-.
ccra.e-cra.tcra.setoned.tseb..
llams-dem.-atrebor.:smr.,esabtrebbb.:.
,egraltreblb.:.
,esabatrebor.:.
br.,egralatrebor.:.
lr..
tes.noitadilav.hcae.no.sledom.ruo.lla.fo.stluser.:3.elbat..
tes.noitadilav.s’tesatadlanigiro.eht.no.ecnamrofrep.nwonk.tseb.
1158