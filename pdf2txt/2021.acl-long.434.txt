learning to perturb word embeddings for out-of-distribution qa.
seanie lee1‚àó minki kang1‚àó juho lee1, sung ju hwang1,2kaist1, aitrics2, south korea{lsnfamily02, zzx1133, juholee, sjhwang82}@kaist.ac.kr.
abstract.
qa models based on pretrained language mod-els have achieved remarkable performance onvarious benchmark datasets.
however, qamodels do not generalize well to unseen datathat falls outside the training distribution, dueto distributional shifts.
data augmentation(da) techniques which drop/replace wordshave shown to be effective in regularizing themodel from overÔ¨Åtting to the training data.
yet, they may adversely affect the qa taskssince they incur semantic changes that maylead to wrong answers for the qa task.
totackle this problem, we propose a simple yeteffective da method based on a stochasticnoise generator, which learns to perturb theword embedding of the input questions andcontext without changing their semantics.
wevalidate the performance of the qa modelstrained with our word embedding perturbationon a single source dataset, on Ô¨Åve differenttarget domains.
the results show that ourmethod signiÔ¨Åcantly outperforms the baselineda methods.
notably, the model trained withours outperforms the model trained with morethan 240k artiÔ¨Åcially generated qa pairs..1.introduction.
deep learning models have achieved impressiveperformances on a variety of real-world natural lan-guage understanding tasks such as text classiÔ¨Åca-tion, machine translation, question answering, andtext generation to name a few (vaswani et al., 2017;seo et al., 2017).
recently, language models thatare pretrained with a large amount of unlabeled datahave achieved breakthrough in the performance onthese downstream tasks (devlin et al., 2019), evensurpassing human performance on some of them.
the success of such data-driven language modelpretraining heavily depends on the amount anddiversity of training data available, since when.
‚àó* equal contribution.
trained with a small amount of highly-biased data,the pretrained models can overÔ¨Åt and may notgeneralize well to out-of-distribution data.
dataaugmentation (da) techniques (krizhevsky et al.,2012; verma et al., 2019a; yun et al., 2019; sen-nrich et al., 2016) can prevent this to a certain ex-tent, but most of them are developed for imagedomains and are not directly applicable to augment-ing words and texts.
perhaps the most importantdesiderata for an augmentation method in super-vised learning, is that it should not change the labelof an example.
for image domains, there existseveral well-deÔ¨Åned data augmentation techniquesthat can produce diverse augmented images with-out changing the semantics.
in contrast, for naturallanguage processing (nlp), it is not straightfor-ward to augment the input texts without changingtheir semantics.
a simple augmentation techniquethat preserves semantics is replacing words withsynonyms or using back translation (sennrich et al.,2016).
however, they do not effectively improvethe generalization performance because the diver-sity of viable transformations with such techniquesis highly limited (pham et al., 2021)..some recent works (wei and zou, 2019; nget al., 2020) propose data augmentation methodstailored for nlp tasks based on dropping or re-placing words and show that such augmentationtechniques improve the performance on the out-of-domain as well as the in-domain tasks.
as shown infig.
1, however, we have observed that most exist-ing data augmentation methods for nlp change thesemantics of original inputs.
while such changein the semantics may not be a serious problemfor certain tasks, it could be critical for questionanswering (qa) task since its sensitivity to thesemantic of inputs.
for instance, replacing a sin-gle word with a synonym (hesburgh ‚Üí vanroth infig.
1) might cause the drastic semantic drift of theanswer (jia and liang, 2017).
thus, word-based.
proceedingsofthe59thannualmeetingoftheassociationforcomputationallinguisticsandthe11thinternationaljointconferenceonnaturallanguageprocessing,pages5583‚Äì5595august1‚Äì6,2021.¬©2021associationforcomputationallinguistics5583figure 1: concept.
our model swep perturbs word embedding and feeds the perturbed embedding to the qa model.
whilethe input-level perturbation method (ssmba) changes the words a lot, our method preserves the original words if we projectperturbed embedding back to the words..augmentations are ineffective for qa tasks, andmost existing works on data augmentation for qatasks resort to question or qa-pair generation.
yet,this approach requires a large amount of trainingtime, since we have to train a separate generator,generate qa pairs from them, and then use the gen-erated pairs to train the qa model.
also, qa-pairgeneration methods are not sample-efÔ¨Åcient sincethey usually require a large amount of generatedpairs to achieve meaningful performance gains..to address such limitations of the existing dataaugmentation techniques for qa, we propose anovel da method based on learnable word-levelperturbation, which effectively regularizes themodel to improve its generalization to unseen ques-tions and contexts with distributional shifts.
specif-ically, we train a stochastic perturbation function tolearn how to perturb each word embedding of theinput without changing its semantic, and augmentthe training data with the perturbed samples.
we re-fer to this data augmentation method as stochasticword embedding perturbation (swep)..the objective of the noise generator is to maxi-mize the log-likelihood of the answer of the inputwith perturbation, while minimizing the kullback-leibler (kl) divergence between prior noise distri-bution and conditional noise distribution of giveninput.
since the perturbation function maximizesthe likelihood of the answer of the perturbed in-put, it learns how to add noise without changingthe semantics of the original input.
furthermore,minimizing the kl divergence prevents generatingidentical noise as the variance of the prior distribu-tion is non-zero, i.e.
we can sample diverse noisefor the same input..we train the qa model on the squad dataset(rajpurkar et al., 2016) with our learned perturba-tions, and evaluate the trained model on the Ô¨Åvedifferent domains ‚Äî bioasq (tsatsaronis et al.,2012), new york times, reddit post, amazon re-view, and wikipedia (miller et al., 2020) as well assquad to measure the generalization performanceon out-of-domain and in-domain data.
the experi-mental results show that our method improves thein-domain performance as well as out-of-domainrobustness of the model with this simple yet effec-tive approach, while existing baseline methods of-ten degrade the performance of the qa model, dueto semantics changes in the words.
notably, ourmodel trained only with the squad dataset showseven better performance than the model trainedwith 240,422 synthetic qa pairs generated froma question generation model.
our contribution inthis work is threefold..‚Ä¢ we propose a simple yet effective data aug-mentation method to improve the generaliza-tion performance of pretrained language mod-els for qa tasks..‚Ä¢ we show that our learned input-dependent per-turbation function transforms the original in-put without changing its semantics, which iscrucial to the success of da for question an-swering..‚Ä¢ we extensively validate our method for do-main generalization tasks on diverse datasets,on which it largely outperforms strong base-lines, including a qa-pair generation method..2 related work.
we empirically validate our data augmentationmethod on both extractive and generative qa tasks..data augmentation asdo-mains (krizhevsky et al., 2012; volpi et al.,.
image.
in.
5584q: in what year was the theodore m. hesburgh library at notre dame finished?c:(‚Ä¶) the main building is the 14 ‚Äìstory theodore m. hesburgh library, completed in 1963, (‚Ä¶) this mural is popularly known as ‚Äútouchdown jesus‚Äù because of its proximity ‚Ä¶q: in what year was the theodore m. hesburghlibrary at notre dame finished?q: each last year was the theodore m. vanrothlibrary at notre dame finished.c:(‚Ä¶) the firstbuilding is the 14 ‚Äìstory theodore p von hesburgh library, completed in 1963; (‚Ä¶) this mural is popularly known as ourconfessionjesuschristbecause allits ‚Ä¶c:(‚Ä¶) the mainbuilding is the 14 ‚Äìstory theodore m.hesburgh library, completed in 1963,(‚Ä¶) this mural is popularly known as ‚Äútouchdown jesus‚Äùbecause ofits proximity ‚Ä¶[original data][ssmba augmented][restored from perturbed embedding]stochastic wordembeddingperturbation(swep)proximityfinishedschools##ardcompletedjesusappearingthirdschools##ardjesuscompletedrestorethirdproximityfinishedappearing(embedding space)word embedding projectiontransformers ùëìùëì(ÔøΩ;ùúÉùúÉùëìùëì)classifier ùëîùëî(ÔøΩ;ùúÉùúÉùëîùëî)qa modelforward(embedding space)input-levelperturbation2018; yun et al., 2019), data augmentation meth-ods are known to be an effective regularizer in textdomain (sennrich et al., 2016).
however, unlikethe image transformations that do not changetheir semantics, transforming raw texts withoutchanging their semantics is difÔ¨Åcult since they arecomposed of discrete tokens.
the most commonapproach for data augmentation in nlp is applyingsimple perturbations to raw words, by eitherdeleting a word or replacing it with synonyms (weiand zou, 2019).
in addition, back-translation withneural machine translation has also been shown tobe effective, as it paraphrases the original sentencewith a different set and ordering of words whilepreserving the semantics to some extent (xie et al.,2020).
beyond such simple heuristics, ng et al.
(2020) propose to mask the tokens and reconstructthem with pretrained language model to augmenttraining data for text classiÔ¨Åcation and machinetranslation.
for qa tasks, question or qa-pairgeneration (zhang and bansal, 2019; lee et al.,2020) are also popular augmentation techniques,which generate questions or question-answer pairsfrom an unlabeled paragraph, thus they can beutilized as additional data to train the model..domain generalization unlike domain adapta-tion in which the target domains are Ô¨Åxed and wecan access unlabeled data from them, domain gen-eralization aims to generalize to unseen target do-mains without access to data from the target distri-bution.
several prior works (li et al., 2018; bal-aji et al., 2018; tseng et al., 2020) propose meta-learning frameworks to tackle domain generaliza-tion, focusing on image domains.
for extractiveqa, lee et al.
(2019) leverage adversarial trainingto learn a domain-invariant representation of ques-tion and context.
however, they require multipleheterogeneous source datasets to train the model tobe robust to out-of-domain data.
in contrast, volpiet al.
(2018) leverage adversarial perturbation togenerate Ô¨Åctitious examples from a single sourcedataset, that can generalize to unseen domains..3 method.
3.1 brief summary of backgrounds.
the goal of extractive question answering (qa)is to point out the start and end position of theanswer span y = (ystart, yend) from a paragraph(context) c = (c1, .
.
.
, cl) with length l for aquestion x = (x1, .
.
.
, xm ).
for generative qa, it.
aims to generate answer y = (y1, .
.
.
, yk) insteadof predicting the position of answer spans fromthe context.
a typical approach to the qa is totrain a neural networks to model the conditionaldistribution pŒ∏(y|x, c), where Œ∏ are composed ofŒ∏f and Œ∏g denoted for the parameters of the encoderf (¬∑; Œ∏f ) and classiÔ¨Åer or decoder g(¬∑; Œ∏g) on topof the encoder.
we estimate the parameter Œ∏ tomaximize the log likelihood with n observations{x(i), y(i), c(i)}ni=1, which are drawn from someunknown distribution ptrain, as follows:n(cid:88).
log pŒ∏(y(i)|x(i), c(i)).
lm le(Œ∏) :=.
(1).
i=1.
for convenience, we set the length t := l+m +3and abuse notations to deÔ¨Åne the concatenated se-quence of the question x and context c as x :=(x0, .
.
.
, xl, c0, .
.
.
, cm +1) where x0, c0, cm +1 de-note start, separation, and end symbol, respectively.
however, the model trained to maximize the like-lihood in eq.
(1) is prone to overÔ¨Åtting and brittle todistributional shifts where target distribution ptest isdifferent from ptrain.
in order to tackle this problem,we train the model with additional data drawn fromdifferent generative process to increase the supportof training distribution, to achieve better general-ization on novel data with distributional shifts.
wewill describe it in the next section..3.2 learning to perturb word embeddings.
several methods for data augmentation have beenproposed in text domain, however, unlike in im-age domains (verma et al., 2019a,b; yun et al.,2019), there does not exist a set of well-deÔ¨Åneddata augmentation methods which transform theinput without changing its semantics.
we proposea new data augmentation scheme where we sam-ple a noise z = (z1, .
.
.
, zt ) from a distributionqœÜ(z|x) and perturb the input x with the samplednoise without altering its semantics.
to this end,the likelihood pŒ∏(y|x, z) should be kept high evenafter the perturbation, while the perturbed instanceshould not collapse to the original input.
we esti-mate such parameters œÜ and Œ∏ by maximizing thefollowing objective:.
lnoise(œÜ, Œ∏) :=.
qœÜ(z|x(i))[log pŒ∏(y(i)|x(i), z)]e.n(cid:88).
i=1t(cid:88).
t=1.
‚àí Œ≤.dkl(qœÜ(zt|x(i)) (cid:107) pœà(zt)).
(2).
5585where Œ≤ ‚â• 0 is a hyper-parameter which controlsthe effect of kl-term.
we assume that zt and zt(cid:48)are conditionally independent given x if t (cid:54)= t(cid:48),i.e., qœÜ(z|x) = (cid:81)tt=1 qœÜ(zt|x).
the parameter ofprior œà is a hyper-parameter to be speciÔ¨Åed.
whenŒ≤ = 1, the objective corresponds to the evidencelower bound (elbo) of the marginal likelihood..maximizing the expected log-likelihood term ineq.
(2) increases the likelihoods evaluated with theperturbed embeddings, and therefore the seman-tics of the inputs after perturbations are likely tobe preserved.
the kl divergence term in eq.
(2)penalizes the perturbation distribution qœÜ(z|x) de-viating too much from the prior distribution pœà(z).
we assume that the prior distribution is fully fac-torized, i.e.
pœà(z1, .
.
.
, zt ) = (cid:81)tt=1 pœà(zt).
fur-thermore, we set each distribution pœà(zt) as a mul-tivariate gaussian distribution n (1, Œ±id), where1 = (1, .
.
.
, 1) ‚àà rd, id, Œ± denotes a vector withones, identity matrix, and positive real number, re-spectively.
hence, we expect the inputs perturbedwith the multiplicative noises remain close to theoriginal inputs on average.
note that the choice ofthe prior is closely related to gaussian dropout (sri-vastava et al., 2014); we will elaborate on this con-nection later..the parameterization of the perturbation func-tion qœÜ heavily affects the success of the learningwith the objective (2).
the function needs to con-trol the intensity of perturbation for each token of xwithout changing the semantics.
since the meaningof each word varies across linguistic contexts, thefunction should be expressive enough to encode thesentence x into a meaningful latent space embed-ding to contextualize the subtle meaning of eachword in the sentence..to this end, we share the encoder functionf (¬∑; Œ∏f ) to contextualize the input x into hiddenrepresentation (h1, .
.
.
, ht ) and feed it into theperturbation function as input as shown in the leftside of fig.
2. however, we stop the gradient of œÜwith respect to l(œÜ, Œ∏) propagating to the encoderf (¬∑; Œ∏f ).
intuitively, it prevents noisy gradient fromÔ¨Çowing to pŒ∏ for early stage of training.
on topof the encoder, we stack two layer feed forwardneural network with relu activation, which out-puts mean ¬µt ‚àà rd and variance œÉ2t ‚àà rd for eachtoken, following kingma and welling (2014).
weleverage the reparameterization trick (kingma andwelling, 2014) to sample zt ‚àà rd.
since x is asequence of discrete tokens, we map each token.
figure 2: architecture.
overview of how the inputis perturbed with swep.
it encodes the input to hid-den representation with transformers and outputs a de-sirable noise for each word embedding.
the noise ismultiplied with the word embedding..xt to corresponding word embedding et and multi-ply it with the noise zt in element-wise manner asfollows:.
et = wordembedding(xt)(h1, .
.
.
, ht ) = f (e1, .
.
.
, et ; Œ∏f ).
¬µt, œÉ2.
t = mlp(ht).
(3).
zt = ¬µt + œÉt (cid:12) (cid:15), where (cid:15) ‚àº n (0, id)Àúet = et (cid:12) zt.
where (cid:12) denotes element-wise multiplication.
wefeed (Àúe1, .
.
.
, Àúet ) to the g ‚ó¶ f to compute the like-lihood pŒ∏(y|x, z) as shown in fig.
2..3.3 learning objective.
as described in the section 3.2, we can jointly op-timize the parameters Œ∏, œÜ with gradient ascent.
however, we want to train the qa model withadditional data drawn from the different generativeprocess as well as the given training data to increasethe support of training distribution, which leads tobetter regularization and robustness to the distribu-tional shift.
therefore, our Ô¨Ånal learning objectivefunction is a convex combination of lm le(Œ∏) andlnoise(œÜ, Œ∏) as follows:.
l(œÜ, Œ∏) = Œªlm le(Œ∏) + (1 ‚àí Œª)lnoise(œÜ, Œ∏) (4).
where 0 < Œª < 1 is a hyper-parameter whichcontrols the importance of each objective.
for allthe experiments, we set Œª as 0.5. in other words,we train the qa model to maximize the conditionallog-likelihood of the original input and perturbedone with stochastic gradient ascent..3.4 connection to dropout.
since each random variable of the perturbation vec-tor zt = (zt,1, .
.
.
, zt,d) is independent, we only.
5586‚Ñéùë°ùë°ùë°ùë°=1,‚Ä¶,ùëáùëá[cls] when did tesla come to the us?
[sep] tesla gained experience ‚Ä¶ ùëíùëíùë°ùë°ùë°ùë°=1,‚Ä¶,ùëáùëánoise generatorùëûùëûùúôùúô(ùëßùëß|ùë•ùë•)‚Ñéùë°ùë°ùë°ùë°=1,‚Ä¶,ùëáùëámlpùúáùúáùë°ùë°ùë°ùë°=1,‚Ä¶,ùëáùëáùúéùúéùë°ùë°2~ùëßùëßùë°ùë°ùë°ùë°=1,‚Ä¶,ùëáùëámultiplysampling‚Ñéùë°ùë°ùë°ùë°=1,‚Ä¶,ùëáùëá~classifier ùëîùëî(ÔøΩ;ùúÉùúÉùëîùëî)stopgradtransformers ùëìùëì(ÔøΩ;ùúÉùúÉùëìùëì)embeddingÃÉùëíùëíùë°ùë°ùë°ùë°=1,‚Ä¶,ùëáùëáconsider the i‚àíth coordinate.
with the reparame-terization trick, we can write zt,i = ¬µt,i + œÉt,i (cid:12) (cid:15)i,iid‚àº n (0, 1) and ¬µt,i, œÉt,i are i‚àíthwhere each (cid:15)icomponent of ¬µt, œÉt which are outputs of neuralnetwork as described in eq.
(3).
simply, each noiseelement zt,i is sampled from n (¬µt,i, œÉ2t,i).
assumethat Àúz is the noise sampled from the prior distribu-tion n (1, Œ±), i.e.
Àúz = 1 + Œ± ¬∑ (cid:15) where (cid:15) ‚àº n (0, 1).
then, zt,i can be expressed in terms of Àúz as follows:.
zt,i =.
Àúz + (¬µ ‚àí.
(5).
œÉŒ±.œÉŒ±.
).
if we set Œ± = (1 ‚àí p)/p where p is the reten-tion probability, we can consider Àúz as a gaussiandropout mask sampled from n (1, 1‚àípp ), whichshows comparable performance to dropout masksampled from bernoulli distribution with proba-bility p (srivastava et al., 2014).
then, we caninterpret our perturbation function as the input de-pendent dropout which scales and translates thegaussian dropout mask, and thus it Ô¨Çexibly con-trols the intensity of perturbation adaptively to eachword embedding of the input x..4 experiment.
4.1 task.
our goal is to regularize the qa model to gener-alize to unseen domains, such that it is able to an-swer the questions from the new domain.
we con-sider a more challenging setting where the modelis trained with a single source dataset and evaluateit on the datasets from the unseen domains as wellas on unseen examples from the source domain.
speciÔ¨Åcally, we train the qa model with squaddataset (rajpurkar et al., 2016) as source domain,test the model with several different target domainqa datasets ‚Äî bioasq (tsatsaronis et al., 2012),new wikipedia (wiki), new york times (nyt),reddit posts, and amazon reviews (miller et al.,2020).
we evaluate the qa model with f1 andexact match (em) score, following the conventionfor extractive qa tasks.
for the bioasq dataset,we use the dataset provided in the mrqa sharedtask (fisch et al., 2019).
we downloaded the otherdatasets from the ofÔ¨Åcial website of miller et al.
(2020)..4.2 experimental setup.
implementation detail as for the encoder f ,we use the pretrained language model ‚Äî bert-base (devlin et al., 2019), electra-small (clark.
et al., 2020) for extractive qa and randomly ini-tialize an afÔ¨Åne transformation layer for g. forthe generative qa task, we use a t5-small (raf-fel et al., 2020) for f ‚ó¶ g as an encoder-decodermodel.
for the perturbation function qœÜ, we stacktwo feed-forward layers with relu on the encoderas described in section 3.2. for the extractive qatask, we train the model for 2 epochs with the batchsize 8 and use adamw optimizer (loshchilov andhutter, 2019) with the learning rate 3 ¬∑ 10‚àí5.
forthe t5 model, we train it for 4 epochs with batchsize 64 and use adafactor optimizer (shazeer andstern, 2018) with learning rate 10‚àí4.
we use beamsearch with width 4 to generate answers for gener-ative question answering..baselines we experiment with our model swepand its variant against several baselines..1. mle: this is the base qa model Ô¨Åne-tuned to.
maximize lm le(Œ∏)..2. adv-aug: following volpi et al.
(2018), we per-turb the word embeddings of the input x withan adversarial objective and use them as addi-tional training data to maximize lm le(Œ∏).
weassume that the answer for each question andcontext remains the same after the adversarialperturbation..3. gaussian-dropout this is the model whoseword embedding is perturbed with dropoutmask sampled from a gaussian distributionn (1, 1‚àípp ), where p is dropout probability andset to be 0.1 (srivastava et al., 2014)..4. bernoulli-dropout this is the model of whichword embedding is perturbed with dropout masksampled from bernoulli distribution ber(1 ‚àí p),where p is dropout probability and set to be 0.1(srivastava et al., 2014)..5. word-dropout: this is the model trained tomaximize lm le(Œ∏) with word dropout (sen-nrich et al., 2016) where the tokens of x arerandomly set to a zero embedding..6. ssmba: this is the qa model trained to maxi-mize lm le(Œ∏), with additional examples gen-erated by the technique proposed in (ng et al.,2020), which are generated by corrupting thetarget sequences and reconstructing them usinga masked language model, bert..7. prior-aug this is variant of swep trained withadditional perturbed data, where the noise isdrawn from the prior distribution pœà(z) ratherthan qœÜ(z|x)..5587method.
squad.
wiki.
nyt.
bioasq.
reddit.
amazon.
bert-base-uncased (em / f1).
mleadv-augword-dropoutgaussian-dropoutbernoulli-dropoutssmba.
81.32 / 88.6281.39 / 88.7181.03 / 88.2181.47 / 88.7881.46 / 88.7678.17 / 86.53.
76.42 / 87.0277.29 / 88.3876.94 / 87.3077.28 / 87.2377.34 / 87.4074.33 / 85.26.
77.54 / 86.5477.67 / 86.5376.67 / 85.9977.25 / 86.3577.16 / 86.3574.31 / 83.98.
45.34 / 59.7745.47 / 60.3044.34 / 58.9345.27 / 61.3744.21 / 59.3339.96 / 54.49.
63.94 / 76.9764.55 / 77.6165.05 / 77.9665.19 / 77.7364.53 / 77.2559.29 / 73.50.
60.74 / 75.3861.38 / 75.8360.87 / 75.7161.67 / 75.9861.27 / 75.8556.57 / 71.81.prior-augswep.
81.77 / 89.0482.24 / 89.43.
77.95 / 87.8378.60 / 88.28.
77.92 / 86.8178.11 / 86.92.
46.40 / 60.8047.27 / 61.72.
65.50 / 78.1665.93 / 78.45.
61.57 / 76.2262.42 / 76.84.electra-small-uncased (em / f1).
mleadv-augword-dropoutgaussian-dropoutbernoulli-dropoutssmba.
76.95 / 84.9275.81 / 84.4075.81 / 84.1976.42 / 84.5376.31 / 84.5077.75 / 85.81.
73.57 / 84.3073.69 / 84.2372.94 / 83.9073.31 / 84.1173.50 / 84.0874.90 / 85.21.
73.68 / 82.9373.37 / 82.8972.96 / 82.2473.27 / 82.5173.35 / 82.7573.25 / 82.62.
38.63 / 54.3238.23 / 53.439.29 / 54.0237.30 / 52.4637.10 / 52.3739.02 / 53.32.
59.59 / 72.3359.97 / 73.3359.04 / 72.1259.29 / 72.3159.33 / 72.5658.97 / 72.83.
57.93 / 72.0659.44 / 73.3658.49 / 72.4157.50 / 71.6557.71 / 71.9956.66 / 71.89.prior-augswep.
77.70 / 85.6077.78 / 85.86.
74.65 / 85.0274.25 / 85.20.
74.38 / 83.4775.18 / 84.18.
38.96 / 54.1940.35 / 55.72.
59.92 / 73.1059.68 / 73.97.
59.01 / 73.1160.89 / 74.06.table 1: experimental results of extractive qa with bert and electra model on six different test dataset..8. swep: this is our full model which maximizes.
the objective function in eq.
(4)..4.3 experimental result.
we compare swep and its variant prior-aug withthe baselines as described in section 4.1. as shownin table 1, our model outperforms all the baselines,whose backbone networks are bert or electra,on most of the datasets.
the data augmentationwith ssmba improves the performance of elec-tra on in-domain dataset squad and wiki.
how-ever, it signiÔ¨Åcantly underperforms ours on out-of-domain datasets even if the data augmentation withssmba use 4.8 times more data than ours.
simi-larly, table 2 shows that the t5 model trained withour method consistently improves the performanceof the model trained with mle on most of thedatasets..contrary to ours, ssmba signiÔ¨Åcantly degradesthe performance of the bert and t5 model both onin-domain and out-of-domain datasets.
since mask-ing and reconstructing some of the tokens froma sentence with a masked language model maycause a semantic drift, those transformations makesome questions unanswerable.
as a result, the dataaugmentation with ssmba often hurts the perfor-mance of the qa model.
similarly, word-dropoutrandomly zeros out word embedding of tokens, butsome of zeroed out words are critical for answeringquestions.
adv-aug marginally improves the per-formance, but it requires an additional backwardpass to compute the gradient for adversarial pertur-bation, which slows down the training procedure..figure 3: em or f1 score on squad and amazon vs. per-centage of qa pairs from squad..4.4 low resource qa.
we empirically show that our data augmenta-tion swep is an effective regularizer in the set-ting where there are only a few annotated train-ing examples.
to simulate such a scenario, wereduce the number of labeled squad data to80%, 50%, 30%, and 10% and train the modelwith the same experimental setup as described insection 4.2. fig.
3 shows the accuracy as a functionof the percentage of qa pairs.
ours consistentlyimproves the performance of the qa model at anyratios of labeled data.
even with 10% of labeleddata, it increases em and f1 score by 1%..4.5 data augmentation with qg.
we show that our data augmentation is sample-efÔ¨Åcient and further improves the performance of.
558810305075100808590f1 scoresquad10305075100657075amazonmleswep10305075100compression ratio (%)707580exact match10305075100compression ratio (%)505560method.
squad.
wiki.
nyt.
bioasq.
reddit.
amazon.
t5-small (em / f1).
mleadv-augword-dropoutgaussian-dropoutbernoulli-dropoutssmba.
77.19 / 85.6674.90 / 84.1975.20 / 84.3376.25 / 84.8675.15 / 84.3474.94 / 84.19.
72.88 / 84.1771.03 / 82.9472.19 / 83.4672.56 / 83.6971.64 / 83.3371.97 / 83.85.
75.10 / 83.8873.46 / 82.8474.27 / 83.2474.76 / 83.5773.81 / 83.0673.29 / 82.79.
40.82 / 54.1838.76 / 52.7938.96 / 52.8441.15 / 54.6439.42 / 53.7737.96 / 51.57.
61.19 / 74.2558.78 / 72.5759.32 / 72.4060.14 / 73.4059.06 / 72.4858.54 / 72.51.
57.52 / 72.1654.73 / 70.1055.58 / 70.4957.01 / 71.5255.22 / 70.4655.05 / 70.62.prior-augswep.
76.88 / 85.4777.12 / 85.67.
73.11 / 84.1873.34 / 84.35.
75.52 / 84.0476.42 / 84.81.
40.49 / 54.4743.01 / 55.80.
60.92 / 74.0460.78 / 73.93.
57.99 / 72.3857.75 / 72.20.table 2: experimental results of generative qa with t5-small model on six different test dataset..electra-small.
bioasq.
nyt.
amazon.
prior-augswep.
38.96 / 54.1940.35 / 55.72.
74.38 / 83.4775.18 / 84.18.
59.01 / 73.1160.89 / 74.97.additive perturb.
w/ Ô¨Åxed ¬µ = 1w/ Ô¨Åxed œÉ = idw/o (cid:15) ‚àº n (0, id)w/o dklw/o lm le(Œ∏).
39.16 / 55.1538.36 / 54.2938.90 / 54.7438.83 / 54.3838.90 / 54.6537.89 / 53.80.
73.87 / 83.174.51 / 83.6873.34 / 82.7974.62 / 83.6073.32 / 82.6672.58 / 82.88.
59.07 / 73.5359.99 / 73.6559.09 / 72.8059.69 / 73.3159.10 / 72.7458.16 / 72.59.table 3: ablation study on electra model..denote w/ Ô¨Åxed ¬µ and w/ Ô¨Åxed œÉ. for all the timestep t, we set ¬µt as (1, .
.
.
, 1) ‚àà rd for w/ Ô¨Åxedt as (1, .
.
.
, 1) ‚àà rd,¬µ. for w/ Ô¨Åxed œÉ, we set œÉ2i.e.
we use the identity matrix id as the covarianceof qœÜ(z|x).
as shown in table 3, Ô¨Åxing ¬µt or œÉ2twith predeÔ¨Åned values achieves slightly better per-formance than the prior-aug, but it degrades theperformance of the full model.
based on this exper-imental results, we verify that learning ¬µt or œÉ2t foreach word embedding et is crucial to the successof the perturbation function, as it can delicatelyperturb each words with more Ô¨Çexibility..furthermore, we convert the stochastic perturba-tion to deterministic one, which we denote as w/o(cid:15) ‚àº n (0, id).
to be speciÔ¨Åc, the mlp(ht) in eq.
(2) only outputs ¬µt alone and we multiply it with etwithout any sampling, i.e.
Àúet = et (cid:12) ¬µt.
as shownin table 3, the deterministic perturbation largelyunderperforms the full model.
in terms of the objec-tive function, we observe that removing lm le(Œ∏)results in larger performance drops, suggesting thatusing both augmented and original instance as asingle batch is crucial for performance improve-ment.
in addition, the experiment without dklshows the importance of imposing a constraint onthe distribution of perturbation with the kl-term..5.2 quantitative analysis.
we quantitatively analyze the intensity of pertur-bations given to the input during the training.
toquantitatively measure the semantic drift, we mea-sure the extent to how many words are replaced.
figure 4: em or f1 score on squad vs.the number ofgenerated qa pairs.
dashed lines indicate results without anysynthetic qa pairs..the qa model trained with additional syntheticdata generated from the question-answer genera-tion model (qg).
we use info-hcvae (lee et al.,2020) to generate qa pairs from unlabeled para-graphs and train the bert model with human-annotated and synthetic qa pairs, while varyingthe number of the generated pairs.
as shown in fig.
4, swep trained only with squad already out-performs the model trained with 240,422 syntheticqa pairs generated with info-hcvae.
moreover,when combining the two methods, we achieve evenlarger performance gains compared to when usingeither swep or info-hcvae alone, as the twoapproaches are orthogonal..5 analysis and discussion.
5.1 ablation study.
we further perform an ablation study to verify theeffectiveness of each component of swep.
in ta-ble 3, we present the experimental results while re-moving various parts of our model.
first of all, wereplace the elementwise multiplicative noise withelementwise additive noise and set the prior dis-tribution as n (0, Œ±id).
we observe that the noisegenerator does not learn meaningful perturbation,which leads to performance degradation.
moreover,instead of learning ¬µt or œÉt from the data, we Ô¨Åxeither of them and perform experiments, which we.
55890204060# of generated qa pairs1e4888990f1 score0204060# of generated qa pairs1e480818283exact matchmleswepinfo-hcvaeinfo-hcvae + swepfigure 5: visualization of the perturbation.
dark red color indicates the perturbation is near to one, i.e.
the correspondingword is rarely perturbed.
in contrast, dark blue color indicates the word is relatively more perturbed than others..in the very early stage of training.
this observationimplies that swep learns the range of perturbationthat preserves the semantics of the original input,which is important when augmenting data for qatasks and veriÔ¨Åes our concept described in fig.
1..5.3 qualitative analysis.
in fig.
5, we visualize the value of the l2 distancebetween the original word and one with the per-turbation after the training.
we observe that theperturbation function qœÜ learns to generate adap-tive perturbations for each word (i.e.
the lowestintensity of perturbation on answer-like words ‚Äúpro-fessor jerome green‚Äù).
however, it is still unknownwhy the intensity of certain word is higher than theothers and how much difference affects the dynam-ics of training.
we have included more observationsuch as embedding space visualization in figure 7..6 conclusion.
we proposed a simple yet effective data augmen-tation method based on a stochastic word embed-ding perturbation for out-of-distribution qa tasks.
speciÔ¨Åcally, our stochastic noise generator learnsto generate the adaptive noise depending on the con-textualized embedding of each word.
it maximizesthe likelihood of input with perturbation, such thatit learns to modulate the intensity of perturbationfor each word embedding without changing the se-mantic of the given question and paragraph.
weaugmented the training data with the perturbedsamples using our method, and trained the modelwith only a single source dataset and evaluate iton datasets from Ô¨Åve different domains as well asthe in-domain dataset.
based on the experimentalresults, we veriÔ¨Åed that our method improves boththe performance of in-domain generalization androbustness to distributional shifts, outperformingthe baseline data augmentation methods.
furtherquantitative and qualitative analysis suggest thatour method learns to generate adaptive perturbationwithout a semantic drift..figure 6: quantitative analysis.
plot the extent to howmany words changed by perturbation during training..with another word during training for each dataaugmentation method and plot it in fig.
6. unlikessmba, which replaces the predeÔ¨Åned percentageof words with others, the adversarial augmentation(adv-aug) or swep perturbs the word embed-dings in the latent space.
we project the perturbedembedding back to the input space to count howmany words are changed.
speciÔ¨Åcally, each wordwt ‚àà r|v| is represented as the one-hot vectorand mapped to word vector as et = wewt, wherev denotes the vocabulary for training data andwe ‚àà rd√ó|v| is the word embedding matrix.
then,the perturbed word embedding Àúet is projected backto one-hot vector Àúwt as follows:.
(v1, .
.
.
, vd)(cid:62) = w (cid:62).
e Àúet.
j = arg max.
{v1, .
.
.
, vi, .
.
.
, vd}.
(6).
i.
Àúwt = one-hot(j, |v|).
where one-hot(j, |v|) makes a one hot vector ofwhich j-th component is one with the length |v|.
in fig.
6, we plot the ratio of how many wordsare replaced with others in raw data before and aftereach perturbation for each batch as training goes on.
in fig.
1, for example, ssmba changes about 11raw words while swep does not change any words.
we observe that around 20% of perturbed words arenot projected back to each original word if we applythe adversarial augmentation.
also, we see thatthe adversarial augmentation largely changes thesemantics of the words although the perturbation atthe Ô¨Ånal layer is within the epsilon neighborhood ofits latent embedding.
in contrast, the perturbationby swep rarely changes the original words except.
55900.000.250.500.751.001.251.501.752.00steps1e40.00.10.20.30.40.5word change ratioadv-augssmbaswepbroader impact.
our data augmentation method swep efÔ¨Åcientlyimproves the robustness of the qa model to un-seen out-of-domain data with a few additional com-putational cost.
this robustness is crucial to thesuccess of the real-world qa models, since theyfrequently encounter questions for unseen domains,from the end-users.
while previous works such as(lee et al., 2019) require a set of several hetero-geneous datasets to learn domain-invariant repre-sentations, such is not a sample-efÔ¨Åcient method,while our method is simple yet effective and canimprove the robustness of the qa model only whentrained on a single source dataset..acknowledgement.
this work was supported by institute of infor-mation & communications technology planning& evaluation (iitp) grant funded by the koreagovernment (msit) (no.2019-0-00075, artiÔ¨Åcialintelligence graduate school program(kaist)),samsung electronics co., ltd, 42maru, and theengineering research center program through thenational research foundation of korea (nrf)funded by the korean government msit (nrf-2018r1a5a1059921)..references.
yogesh balaji, swami sankaranarayanan, and ramachellappa.
2018. metareg: towards domain gen-eralization using meta-regularization.
advances inneural information processing systems, 31:998‚Äì1008..kevin clark, minh-thang luong, quoc v. le, andchristopher d. manning.
2020. electra: pre-training text encoders as discriminators rather thanin 8th international conference ongenerators.
learning representations, iclr 2020, addis ababa,ethiopia, april 26-30, 2020..jacob devlin, ming-wei chang, kenton lee, andkristina toutanova.
2019. bert: pre-training ofdeep bidirectional transformers for language under-in proceedings of the 2019 conferencestanding.
of the north american chapter of the associationfor computational linguistics: human languagetechnologies, naacl-hlt 2019, minneapolis, mn,usa, june 2-7, 2019, volume 1 (long and short pa-pers), pages 4171‚Äì4186..xinya du and claire cardie.
2018..harvest-ing paragraph-level question-answer pairs fromwikipedia.
in proceedings of the 56th annual meet-ing of the association for computational linguistics(volume 1: long papers), pages 1907‚Äì1917..adam fisch, alon talmor, robin jia, minjoon seo, eu-nsol choi, and danqi chen.
2019. mrqa 2019shared task: evaluating generalization in readingin proceedings of the 2nd work-comprehension.
shop on machine reading for question answering,mrqa@emnlp 2019, hong kong, china, novem-ber 4, 2019, pages 1‚Äì13..robin jia and percy liang.
2017. adversarial ex-amples for evaluating reading comprehension sys-in proceedings of the 2017 conference ontems.
empirical methods in natural language processing,emnlp 2017, copenhagen, denmark, september 9-11, 2017, pages 2021‚Äì2031..diederik p. kingma and max welling.
2014. auto-encoding variational bayes.
in international confer-ence on learning representations, iclr 2014,..alex krizhevsky, ilya sutskever, and geoffrey e. hin-ton.
2012. imagenet classiÔ¨Åcation with deep convo-lutional neural networks.
in advances in neural in-formation processing systems 25: 26th annual con-ference on neural information processing systems2012. proceedings of a meeting held december 3-6,2012, lake tahoe, nevada, united states..dong bok lee, seanie lee, woo tae jeong, dongh-wan kim, and sung ju hwang.
2020. gener-ating diverse and consistent qa pairs from con-texts with information-maximizing hierarchical con-in proceedings of the 58th annualditional vaes.
meeting of the association for computational lin-guistics, acl 2020, online, july 5-10, 2020, pages208‚Äì224..training..seanie lee, donggyu kim, and jangwon park.
2019.domain-agnostic question-answering with adversar-the 2nd work-in proceedings ofialshop on machine reading for question answering,mrqa@emnlp 2019, hong kong, china, novem-ber 4, 2019, pages 196‚Äì202..bohan li, hao zhou, junxian he, mingxuan wang,yiming yang, and lei li.
2020. on the sentenceembeddings from pre-trained language models.
inproceedings of the 2020 conference on empiricalmethods in natural language processing, emnlp2020, online, november 16-20, 2020, pages 9119‚Äì9130..da li, yongxin yang, yi-zhe song, and timothyhospedales.
2018. learning to generalize: meta-learning for domain generalization.
in proceedingsof the aaai conference on artiÔ¨Åcial intelligence..ilya loshchilov and frank hutter.
2019. decoupledin international con-.
weight decay regularization.
ference on learning representations..l. v. d. maaten and geoffrey e. hinton.
2008. visual-izing data using t-sne.
journal of machine learningresearch, 9:2579‚Äì2605..5591john miller, karl krauth, benjamin recht, and lud-wig schmidt.
2020. the effect of natural distribu-in inter-tion shift on question answering models.
national conference on machine learning, pages6905‚Äì6916.
pmlr..nathan ng, kyunghyun cho, and marzyeh ghassemi.
2020. ssmba: self-supervised manifold based dataaugmentation for improving out-of-domain robust-in proceedings of the 2020 conference onness.
empirical methods in natural language processing(emnlp), pages 1268‚Äì1283..hieu pham, xinyi wang, yiming yang, and grahamin interna-.
neubig.
2021. meta back-translation.
tional conference on learning representations..colin raffel, noam shazeer, adam roberts, katherinelee, sharan narang, michael matena, yanqi zhou,wei li, and peter j. liu.
2020. exploring the lim-its of transfer learning with a uniÔ¨Åed text-to-texttransformer.
journal of machine learning research,21(140):1‚Äì67..pranav rajpurkar, jian zhang, konstantin lopyrev, andpercy liang.
2016. squad: 100, 000+ questions formachine comprehension of text.
in proceedings ofthe 2016 conference on empirical methods in nat-ural language processing, emnlp 2016, austin,texas, usa, november 1-4, 2016, pages 2383‚Äì2392..rico sennrich, barry haddow, and alexandra birch.
2016. edinburgh neural machine translation sys-tems for wmt 16. in proceedings of the first con-ference on machine translation: volume 2, sharedtask papers..min joon seo, aniruddha kembhavi, ali farhadi,and hannaneh hajishirzi.
2017. bidirectional at-in inter-tention Ô¨Çow for machine comprehension.
national conference on learning representations,iclr 2017,..noam shazeer and mitchell stern.
2018. adafactor:adaptive learning rates with sublinear memory cost.
in international conference on machine learning..nitish srivastava, geoffrey hinton, alex krizhevsky,ilya sutskever, and ruslan salakhutdinov.
2014.dropout: a simple way to prevent neural networksfrom overÔ¨Åtting.
the journal of machine learningresearch..george tsatsaronis, michael schroeder, georgiospaliouras, yannis almirantis, ion androutsopoulos,¬¥eric gaussier, patrick gallinari, thierry arti`eres,michael r. alvers, matthias zschunke, and axel-cyrille ngonga ngomo.
2012. bioasq: a chal-lenge on large-scale biomedical semantic indexingin information retrievaland question answering.
and knowledge discovery in biomedical text, pa-pers from the 2012 aaai fall symposium..hung-yu tseng, hsin-ying lee, jia-bin huang, andming-hsuan yang.
2020. cross-domain few-shot.
classiÔ¨Åcation via learned feature-wise transforma-tion.
in international conference on learning rep-resentations..ashish vaswani, noam shazeer, niki parmar, jakobuszkoreit, llion jones, aidan n. gomez, lukaszkaiser, and illia polosukhin.
2017. attention is allyou need.
in advances in neural information pro-cessing systems 30: annual conference on neuralinformation processing systems 2017, 4-9 decem-ber 2017, long beach, ca, usa, pages 5998‚Äì6008..vikas verma, alex lamb, christopher beckham, amirnajaÔ¨Å, ioannis mitliagkas, david lopez-paz, andyoshua bengio.
2019a.
manifold mixup: betterinrepresentations by interpolating hidden states.
proceedings of the 36th international conferenceon machine learning, icml 2019, 9-15 june 2019,long beach, california, usa, pages 6438‚Äì6447..vikas verma, alex lamb, christopher beckham, amirnajaÔ¨Å, ioannis mitliagkas, david lopez-paz, andyoshua bengio.
2019b.
manifold mixup: better rep-resentations by interpolating hidden states.
in inter-national conference on machine learning.
pmlr..riccardo volpi, hongseok namkoong, ozan sener,john c. duchi, vittorio murino, and silvio savarese.
2018. generalizing to unseen domains via adver-in advances in neu-sarial data augmentation.
ral information processing systems 31: annualconference on neural information processing sys-tems 2018, neurips 2018, december 3-8, 2018,montr¬¥eal, canada, pages 5339‚Äì5349..jason w. wei and kai zou.
2019. eda: easy dataaugmentation techniques for boosting performancein proceedings of theon text classiÔ¨Åcation tasks.
2019 conference on empirical methods in natu-ral language processing and the 9th internationaljoint conference on natural language processing,emnlp-ijcnlp 2019, hong kong, china, novem-ber 3-7, 2019, pages 6381‚Äì6387..qizhe xie, zihang dai, eduard h. hovy, thang luong,and quoc le.
2020. unsupervised data augmenta-tion for consistency training.
in advances in neuralinformation processing systems 33: annual con-ference on neural information processing systems2020, neurips 2020, december 6-12, 2020, virtual..sangdoo yun, dongyoon han, seong joon oh,sanghyuk chun, junsuk choe, and youngjoon yoo.
2019. cutmix: regularization strategy to trainstrong classiÔ¨Åers with localizable features.
in pro-ceedings of the ieee international conference oncomputer vision..shiyue zhang and mohit bansal.
2019. address-ing semantic drift in question generation for semi-in proceedings ofsupervised question answering.
the 2019 conference on empirical methods in nat-ural language processing and the 9th internationaljoint conference on natural language processing(emnlp-ijcnlp), pages 2495‚Äì2509..5592a experimental setup.
a.1 dataset statistics.
table 4 describes detailed dataset statistics..a.2 baselines.
1. word-dropout we set the same dropoutprobability as 0.1, which is the same dropoutprobability of the backbone networks ‚Äîbert, electra, and t5 model..2. adv-aug we follow the adversarial perturba-tion from (volpi et al., 2018).
we set the num-ber of iteration for perturbation as 5, which ismuch fewer steps than the original paper dueto the computational cost..3. ssmba we use the ofÔ¨Åcial code of the origi-nal paper1 to augment the training data fromsquad.
we set the probability of masking0.25 and sample 8 different examples for eachtraining data instance.
in total, we synthesize426,266 additional training instances..4. prior-aug we set the Œ± as 0.1 which is thedropout probability of the backbone networks..a.3 data augmentation with qg.
following the experimental setup from lee et al.
(2020), we split the original squad validationdataset by half into new validation and test set.
we download the synthetic qa pairs generatedby their generative model info-hcvae from thegithub2 and augment squad training data withthem.
they leverage the generative model to sam-ple qa pairs from unlabeled paragraph of harvest-ingqa dataset3 (du and cardie, 2018), varying thedifferent portion of unlabeled paragraph (denotedas h√ó5%-h√ó50%).
we Ô¨Årst Ô¨Ånetune bert-baseqa model with the synthetic qa pairs generatedfor 2 epochs and further train it with the originalsquad training data for another 2 epochs.
we useadamw optimizer (loshchilov and hutter, 2019)and set learning rate 2 ¬∑ 10‚àí5 and 3 ¬∑ 10‚àí5 for pre-training and Ô¨Ånetuning, respectively with batch size32. we choose the best checkpoint based on the f1score from the new validation dataset and evaluatef1 and exact match (em) score on the new testdataset..1https://github.com/nng555/ssmba2https://github.com/seanie12/.
info-hcvae.
harvestingqa.
3https://github.com/xinyadu/.
datasets.
squad.
train (#) valid (#) test (#).
86,588.
10,507.
-.
bioasqnew wikipedianew york timesredditamazon.
-----.
harvestqa.
1,259,691.
-----.
-.
1,5047,93810,0659,8039,885.
-.
table 4: the statistics and the data source of squad,bioasq, new wikipedia, new york times, reddit,amazon, and harvesting qa..a.4 computational cost.
the number of parameters our swep modelrequires few additional learnable parameters rela-tive to the size of the language model.
speciÔ¨Åcally,our model costs only 3d2 +3d number of additionalparameters, which is less than 2m in the case ofbert-base model where d = 768. compared to110m parameters of bert-base model, our modeldoes not increase the number of parameters a lot..computing infrastructure and runtimein thecase of the bert-base model, the Ô¨Åne-tuning withswep costs less than 4 gpu hours with a singletitan xp gpu..b algorithm.
we describe the whole training procedure describedin the section 3.3 as follows:.
algorithm 1 swep.
1: input:.
pre-trained language model Œ∏dataset d = {(x(1), y(1)), ..., (x(n ), y(n ))}.
2: while training do3:.
for (x(i), y(i)) in d do.
4:.
5:.
6:.
7:.
forward data without perturbation to com-pute log pŒ∏(y(i)|x(i))sample z ‚àº qœÜ(z|x(i))forward data with perturbation and com-pute lnoise(œÜ, Œ∏)update Œ∏, œÜ with l(œÜ, Œ∏).
end for8:9: end while.
c further analysis.
motivated by observations from (li et al., 2020),we further analyze the adaptive perturbation for.
5593word frequency rank.
[1, 100].
(100, 500].
(500, 5k].
(5k, 10k].
k-nn l2-dist.
(k = 5).
l2-dist.
between before / after perturb.
mean ¬µ.
0.6618.
0.23861.2153.
0.7893.
0.29891.2402.
0.8474.
0.35421.2495.
0.8973.
0.40991.2543.table 5: the l2 distance of k-nn nearest neighbor, l2 distance between embeddings before and after perturbation,and the average ¬µ value of the word embedding from bert, segmented by the word frequency rank (lower rankindicates high-frequency word)..inal word embedding, however, the contextualizedembedding is not much changed by the perturba-tion.
note that absolute positions are different ineach plot because of the randomness inherent inthe t-sne algorithms..each word.
li et al.
(2020) observe that low-frequency words disperse sparsely while high-frequency words concentrate densely on the wordembedding space of bert.
following the settingof (li et al., 2020), we Ô¨Årst measure the l2 dis-tance between k-nearest neighbors of each wordembedding.
speciÔ¨Åcally, we rank each word (word-piece tokens) by frequency counted based on thesquad train set and sample 100 examples fromthe squad train set for analysis.
in table 5, wealso observe that low-frequency words have moredistance to their neighbor than high-frequencywords.
then, we measure the average l2 distanceof word embedding before and after perturbationand the average perturbation size for each word as1i=1 ¬µt,i after the training.
we observe that low-dfrequency words tend to be perturbed more thanhigh-frequency words.
this observation suggeststhat the noise generator can recognize acceptableextents to perturb words depend on the word em-bedding distribution then tends to generate moreperturbation on sparsely dispersed low-frequencywords and less perturbation on densely concen-trated high-frequency words.
note that we use betaannealing to magnify the difference for analysis sothat the Œ≤ becomes zero in the second epoch..(cid:80)d.d embedding space visualization.
in figure 7, we visualize the embedding spaceusing t-sne (maaten and hinton, 2008) for bothword embedding ((a), (b)) and contextualized em-bedding ((c), (d)) before and after perturbationfrom electra-small model.
we sample the ex-ample from the squad training set, which is thesame example as figure 1 in the main paper.
swepencodes each input tokens xt to hidden representa-tion ht with transformers and outputs a desirablenoise for each word embedding.
the noise zt ismultiplied with the word embedding et of eachtoken xt.
we observe that the perturbed word em-bedding is mapped to a different space against orig-.
5594figure 7: visualization.
overview of how the input is perturbed with swep.
contextualized embedding indicatesthe hidden states from the last layer of transformers.
blue points indicate embeddings after perturbation..
5595[cls]inwhatyearwasthetheodorem.he##sburg##hlibraryatnotredamefinished?[sep]thelibrarysystemoftheuniversityisdividedbetweenthemainlibraryandeachofthecollegesandschools.themainbuildingisthe14-storytheodorem.he##sburg##hlibrary,completedin1963,whichisthethirdbuildingtohousethemaincollectionofbooks.thefrontofthelibraryisadornedwiththewordoflifemuraldesignedbyartistmill##ardsheets.thismuralispopularlyknownas"touchdownjesus"becauseofitsproximitytonotredamestadiumandjesus'armsappearingtomakethesignalforatouchdown.
[sep](a) word embedding before perturbation[cls]inwhatyearwasthetheodorem.he##sburg##hlibraryatnotredamefinished?[sep]thelibrarysystemoftheuniversityisdividedbetweenthemainlibraryandeachofthecollegesandschools.themainbuildingisthe14-storytheodorem.he##sburg##hlibrary,completedin1963,whichisthethirdbuildingtohousethemaincollectionofbooks.thefrontofthelibraryisadornedwiththewordoflifemuraldesignedbyartistmill##ardsheets.thismuralispopularlyknownas"touchdownjesus"becauseofitsproximitytonotredamestadiumandjesus'armsappearingtomakethesignalforatouchdown.
[sep](b) word embedding after perturbation[cls]inwhatyearwasthetheodorem.he##sburg##hlibraryatnotredamefinished?[sep]thelibrarysystemoftheuniversityisdividedbetweenthemainlibraryandeachofthecollegesandschools.themainbuildingisthe14-storytheodorem.he##sburg##hlibrary,completedin1963,whichisthethirdbuildingtohousethemaincollectionofbooks.thefrontofthelibraryisadornedwiththewordoflifemuraldesignedbyartistmill##ardsheets.thismuralispopularlyknownas"touchdownjesus"becauseofitsproximitytonotredamestadiumandjesus'armsappearingtomakethesignalforatouchdown.
[sep](c) contextualized embedding before perturbation[cls]inwhatyearwasthetheodorem.he##sburg##hlibraryatnotredamefinished?[sep]thelibrarysystemoftheuniversityisdividedbetweenthemainlibraryandeachofthecollegesandschools.themainbuildingisthe14-storytheodorem.he##sburg##hlibrary,completedin1963,whichisthethirdbuildingtohousethemaincollectionofbooks.thefrontofthelibraryisadornedwiththewordoflifemuraldesignedbyartistmill##ardsheets.thismuralispopularlyknownas"touchdownjesus"becauseofitsproximitytonotredamestadiumandjesus'armsappearingtomakethesignalforatouchdown.
[sep](d) contextualized embedding after perturbation