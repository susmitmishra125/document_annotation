stereorel: relational triple extraction from a stereoscopic perspective.
xuetao tian1, liping jing1,∗, lu he2, feng liu21 technology beijing key lab of trafﬁc data analysis and mining2 engineering research center of network management technology for high speed railwayschool of computer and information technology, beijing jiaotong university,beijing, china, 100044{xttian, lpjing, helu19, fliu}@bjtu.edu.cn.
abstract.
relational triple extraction is critical to under-standing massive text corpora and construct-ing large-scale knowledge graph, which has at-tracted increasing research interest.
however,existing studies still face some challenging is-sues, including information loss, error prop-agation and ignoring the interaction betweenentity and relation.
to intuitively explore theabove issues and address them, in this paper,we provide a revealing insight into relationaltriple extraction from a stereoscopic perspec-tive, which rationalizes the occurrence of theseissues and exposes the shortcomings of exist-ing methods.
further, a novel model is pro-posed for relational triple extraction, whichmaps relational triples to a three-dimension (3-d) space and leverages three decoders to ex-tract them, aimed at simultaneously handlingthe above issues.
extensive experiments areconducted on ﬁve public datasets, demonstrat-ing that the proposed model outperforms therecent advanced baselines..1.introduction.
relational triple is a common structural represen-tation of semantic facts.
a triple is always in formof (subject, relation, object), where subject and ob-ject are two entities connected by a type of prede-ﬁned semantic relation.
relational triple extractionfrom unstructured texts is critical to understand-ing massive text corpora and constructing large-scale knowledge graph (ren et al., 2017; wei et al.,2020), which is widely concerned in recent years.
early researches (zhou et al., 2005; chan androth, 2011; zhang et al., 2017) ﬁrst recognizeentities and predict the relations for each entitypair.
such approaches suffer from error propaga-tion problem and thus recent researches (zhenget al., 2017; zeng et al., 2018; fu et al., 2019;.
∗ corresponding author: liping jing..nayak and ng, 2020; wei et al., 2020; liu et al.,2020) try to build a jointly-decoding schema forentities and relations.
however, relational triple ex-traction still faces the following challenging issues:.
• information loss (i-il).
information loss in-cludes entity incompleteness (zeng et al.,2020) and entity overlapping (zeng et al.,2018; wei et al., 2020).
entity incomplete-ness (i-il-ei) refers to that only head or tailtoken rather than completed entity is recog-nized, while entity overlapping (i-il-eo) isthat one entity belonging to multiple triplescannot be marked..• error propagation (i-ep).
error propagationcomes from the prediction process with strictorder.
for examples, pipeline models (zhanget al., 2017; takanobu et al., 2019) recognizeentities ﬁrst and predict relations based oneach speciﬁc entity pair.
generative models(zeng et al., 2018, 2019) extract subject, ob-ject and relation with a predetermined order..• ignoring the interaction between entity andrelation (i-ii).
subjects (or objects) in differ-ent predeﬁned relations should have differentrecognition patterns, which are not modelledwhen ignoring the interaction between entityand relation..to intuitively explore the above issues and ad-dress them, from a stereoscopic perspective, wemap the relational triples of a text to a three-dimensional (3-d) space, which is like a cube asfigure 1. the relational triples are actually somesmall cubes in the whole cube.
existing researchesare actually to model the cube from different per-spectives and further extract the triples.
based onthe representation of triples in 3-d space, threeoperations (i.e.
slice, projection and shrinkage).
proceedingsofthe59thannualmeetingoftheassociationforcomputationallinguisticsandthe11thinternationaljointconferenceonnaturallanguageprocessing,pages4851–4861august1–6,2021.©2021associationforcomputationallinguistics4851figure 1: the representation of triples in 3-d space, where a text corresponds to a cube with size being (|l| ×|t |) × (|l| × |t |) × |r|, while each triple is mapped to a small cube with size being (m × |t |) × (n × |t |) × 1..are deﬁned as figure 2, to understand why exist-ing methods suffer from the above issues.
further-more, we propose a novel model for relational tripleextraction, which can simultaneously handle theabove issues, named stereorel.
more precisely,the cube is modelled from three perspectives, in-cluding (x, z)-plane projection, (y, z)-plane projec-tion and z-slices, which indicates the subjects, ob-jects and their correspondences for each predeﬁnedrelation.
correspondingly, the proposed methodleverages three decoders to extract relational triplesin a uniﬁed model..this work has the following main contributions:.
• we provide a revealing insight into relationaltriple extraction from a stereoscopic perspec-tive, where the occurrence of several challeng-ing issues and shortcomings of existing meth-ods are rationalized..• we propose a novel stereorel model for re-lational triple extraction, which can simulta-neously reduce information loss, avoid errorpropagation and not ignore the interaction be-tween entity and relation..• extensive experiments are conducted on ﬁvepublic datasets, demonstrating that the pro-posed model outperforms the recent advancedbaselines..2 relational triple extraction from 3-d.perspective.
in form of (subject, relation, object), triples cannaturally be mapped to a three-dimensional (3-d)space, which is elaborated in this section.
mean-while, we deﬁne three operations (i.e.
slice, projec-tion and shrinkage) in 3-d space, to make it easyto understand the strengths and shortcomings ofprevious researches..2.1 triple representation in 3-d space.
given a text l with length being |l| and a prede-ﬁned relation set r having |r| relations, l mayhave several triples, that is, p([s, r, o]|l) where[·] represents a collection.
each triple consists ofa subject (s), an object (o) and one relation (r)belonging to r. subject is one entity, that is, n-gram in l and so does object.
to model p([s]|l)or p([o]|l), the common strategy is to leveragesequence tagging on l, which has some existingstrategies, such as bmes tagging (zhang and yang,2018; li et al., 2020) and start-and-end binary tag-ging (wei et al., 2020; sui et al., 2020).
anyway,there is a tag set t , and thus p([s]|l) or p([o]|l)can be represented by a vector with length being|l| × |t |.
meanwhile, due to r ∈ r, modelingp([r]|l) can be taken as a classiﬁcation task, whichrequires a vector with length being |r| to represent.
therefore, when modeling p([s, r, o]|l) by consid-.
4852table 1: the correspondence between the issues of rela-tional triple extraction and the operations in 3-d space..operation.
sli(·)prox/y(·)proxy(·)shr(·)only.
i-il.
i-il-ei.
i-il-eo.
i-ep i-ii.
+.
++.
+.
+.
all speciﬁed, there is an xyz-slice with size being(m × |t |) × (n × |t |) × 1, that is, a triple.
projection, denoted as pro(·).
as depicted in fig-ure 2(b), two types of projection are deﬁned, cube-to-plane and plane-to-axis.
the former seems tolook at the whole cube from a certain plane.
for ex-ample, in the projection from cube to (x, y)-plane,two triples with the same subject and object areindistinguishable.
similarly, in the projection fromcube to (x, z)-plane, there is only subject and re-lation information but no object information.
thelater seems to look at a plane from a certain axis,such as (x, z)-plane to x-axis projection, where thesubjects in different z-slices may have the same rep-resentation on x-axis.
hereafter, for easy reading,(x, z)-plane to x-axis and (y, z)-plane to y-axisprojections are denoted as prox(·) and proy(·)respectively.
the projection from cube to (x, y)-plane is denoted as proxy(·).
the rest ones aresimilar.
shrinkage, denoted as shr(·).
in the cube rep-resentation, each token pair is represented by anxy-slice with size being |t | × |t | × |r|.
suchan xy-slice can reﬂect all possible entity-taggingcombinations of a token pair.
as described in fig-ure 2(c), a shrinkage over a cube only representwhether the token pair satisﬁes one speciﬁc entity-tagging combination, such as (start, start).
thus,the size of a shrinkage is |l| × |l| × |r|..2.2 analysis of previous researches.
as aforementioned, relational triple extractionfaces three challenging issues: information loss (i-il-ei or i-il-eo), error propagation (i-ep) andignoring the interaction between entity and relation(i-ii).
it is clear to match them to the three opera-tions in 3-d space as table 1. sli(·) correspondsto the prediction process with strict order, and thusleads to error propagation.
without consideringthe nested entities in a text, proxz/yz/z(·) do notresult in any problems, while prox/y/xy(·) is the.
(a) slice..(b) projection..(c) shrinkage..figure 2: our deﬁned operations for relational triplerepresentation in 3-d space..ering all possible connections, it should be equiva-lent to a cube with size being (|l| × |t |)2 × |r|in a 3-d space..as shown in figure 1, the line segments ofthe cube mapping on x-axis, y-axis and z-axisare respectively regarded as the representations ofsubjects, objects and relations, that is, p([s]|l),p([o]|l) and p([r]|l).
similarly, the rectangles ofthe cube mapping on (x, y)-plane, (x, z)-plane and(y, z)-plane are respectively regarded as p([s, o]|l),p([s, r]|l) and p([o, r]|l).
further, each triple ismapped to a small cube in the space.
based on thestereoscopic representation of relational triples, wedeﬁne the following operations.
slice, denoted as sli(·).
as shown in figure 2(a),when some elements (i.e.
subject, object or rela-tion) are speciﬁed, the representation space willbe reduced.
the operation is like slicing the cube.
for instances, a speciﬁc relation corresponds toa z-slice with size being (|l| × |t |)2 × 1. bothsubject and object being speciﬁed leads to an xy-slice with size being (m × |t |) × (n × |t |) × |r|,which can be seen as the intersection of an x-sliceand a y-slice.
if subject, object or relation are.
4853table 2: the analysis of previous researches from the stereoscopic perspective.
for the mentioned issues,indicates that a model can handle a certain issue and × is the opposite..√.
i-il.
i-il-ei.
i-il-eo.
i-ep i-ii.
models.
pipeline.
mhs.
perspective in 3-d space (for modeling p([s, r, o]|l)).
si.
(cid:80)[s]∪[o]oj (cid:54)=si.
p([s] ∪ [o]|l) + (cid:80)[s]∪[o]where p([s] ∪ [o]|l) = p(prox(proxz([s, r, o])) ∪ proy(proyz([s, r, o]))|l).
p([s] ∪ [o]|l) + (cid:80)[s]∪[o]where p([s] ∪ [o]|l) = p(prox(proxz([s, r, o])) ∪ proy(proyz([s, r, o]))|l)..p([si, r, oj]|l, slixy(si, oj)),.
p([si, r, o]|l, slix(si)),.
si.
noveltagging.
p(prox(proxy([s, r, o])) ∪ proy(proxy([s, r, o]))|l)..pa-tagging.
copyrecopyrrl.
p(proxy([s, r, o])|l).
p(shr([s, r, o])|l) = (cid:81)where p(slixyz(si, ri, oi)) =p(sliz(ri)) · p(slixz(si, ri)|sliz(ri)) · p(slixyz(si, ri, oi)|slixz(si, ri))..i p(slixyz(si, ri, oi)|l, [slixyz(sj, rj, oj)]j<i),.
graphrel.
p(proxy(shr([s, r, o]))|l)..copymtl.
wdecpndec.
casrel.
att-as-rel.
tplinker.
ours.
p(shr([s, r, o])|l) + p([s] ∪ [o]|l),where p(shr([s, r, o])|l) is the same as copyre,and p([s] ∪ [o]|l) = p(prox(proxz([s, r, o])) ∪ proy(proyz([s, r, o]))|l).
(cid:81).
i p(slixyz(si, ri, oi)|l, [slixyz(sj, rj, oj)]j<i),.
(cid:80)|r|.
where p(slixyz(si, ri, oi)) =p(slix(si)) · p(slixy(si, oi)|slix(si)) · p(slixyz(si, ri, oi)|slixy(si, oi)).
p([s]|l) + (cid:80)[s]j p([si, rj, o]|l, slix(si)),siwhere p([s]|l) = p(prox(proxz([s, r, o]))|l).
p(shr([s, r, o])|l) + p([s] ∪ [o]|l),where p(shr([s, r, o]) = (cid:80)|r|and p([s] ∪ [o]|l) = p(prox(proxz([s, r, o])) ∪ proy(proyz([s, r, o]))|l).
2 × p(shr([s, r, o])|l) + p([s] ∪ [o]|l),where one shr([s, r, o]) is (start, start) shr(·), the other is (end, end) shr(·),and p([s] ∪ [o]|l) = p(prox(proxz([s, r, o])) ∪ proy(proyz([s, r, o]))|l).
(cid:80)|r|iwhere (cid:80)|r|(cid:80)|r|i.
[p([s, ri]|l) + p([ri, o]|l) + p(shr([s, ri, o])|l)],p([s, ri]|l) = p(proxz([s, r, o])|l),.
p(shr([s, ri, o])|l),.
i.i.p([ri, o]|l) = p(proyz([s, r, o])|l), and shr([s, r, o]) is one speciﬁc shr(·)..√.
√.
√.
√.
×.
×.
√.
√.
√.
√.
√.
√.
√.
√.
×.
×.
√.
×.
√.
√.
√.
√.
√.
√.
×.
×.
√.
√.
×.
√.
×.
×.
×.
√.
√.
√.
×.
×.
×.
×.
√.
×.
×.
√.
×.
×.
×.
√.
opposite.
both prox/y(·) and proxy(·) lead toignoring the interaction between entity and rela-tion.
meanwhile, proxy(·) makes the triples withoverlapped entities indistinguishable.
the cube canbe disassembled into |t | × |t | shr(·).
modelingonly one shr(·) will cause entity incompleteness.
to get deep insights on relational triple extraction,based on the correspondence between the opera-tions and issues, we analyze previous researches asshown in table 2..early researches (zelenko et al., 2002; zhouet al., 2005; chan and roth, 2011) adopt pipelineapproaches, where the entities are recognized ﬁrstand the relations for each entity pair are pre-dicted.
arguing that such approaches neglectthe inherent relevance between entity recognitionand relation extraction, some solutions (miwaand bansal, 2016; zhang et al., 2017; takanobuet al., 2019) still extract entities and relations se-quentially, but make two tasks share the sameencoder.
these methods model p([s, r, o]|l) asp([s] ∪ [o]|l) and p([si, r, oj]|l, slixy(si, oj)),.
where p([s]∪[o]|l) = p(prox(proxz([s, r, o]))∪proy(proyz([s, r, o]))|l).
therefore, pipelineparadigm suffers from i-ep and i-ii issues.
mhs(bekoulis et al., 2018) is another two-stage method.
the model recognizes entities ﬁrstly and extractsrelational triples with a multi-head selection strat-egy on each subject, where prox/y(·) and slix(·)lead to i-ep and i-ii issues respectively..in the following researches on relational tripleextraction, several methods with joint decodingschema are proposed.
speciﬁcally, noveltag-ging (zheng et al., 2017) and pa-tagging (daiet al., 2019) achieve joint decoding by design-ing a uniﬁed tagging scheme and convert rela-tional triple extraction to an end-to-end sequencetagging problem.
such a tagging schema hasto model p(proxy([s, r, o])|l) and thus suffersfrom i-il-eo and i-ii issues.
copyre (zenget al., 2018) and copyrrl (zeng et al., 2019)leverage sequence-to-sequence model with copymechanism.
graphrel (fu et al., 2019) introducesgraph convolutional network jointly learn entities.
4854and relations.
despite their initial success, thethree methods only model p(shr([s, r, o])|l) andthus suffer from i-il-ei issue.
sequence gen-eration models, copyre and copyrrl, predicttriples one by one and model p(slixyz(si, ri, oi))via p(sliz(ri)) × p(slixz(si, ri)|sliz(ri)) ×p(slixyz(si, ri, oi)|slixz(si, ri)), which leads toi-ep issue.
graphrel cannot avoid i-il-eo andi-ii issues due to its utilizing proxy(·)..recently,.
to address i-il issue, copymtl(zeng et al., 2020) proposes a multi-task learn-ing framework based on copyre, to simultane-ously predict completed entities and capture re-lational triples.
however, the model still doesnot solve i-ep issue.
meanwhile, entity recog-nition is implemented by modeling p([s] ∪ [o]|l)via a standalone module, which leads to i-ii is-sue.
following sequence-to-sequence schema,wdec and pndec (nayak and ng, 2020) de-sign speciﬁc decoder block which can gener-ate triples with completed entities.
such mod-els ease i-ii issue, but still suffers from i-epit models p(slixyz(si, ri, oi))issue since thatvia p(slix(si)) × p(slixy(si, oi)|slix(si)) ×p(slixyz(si, ri, oi)|slixy(si, oi)).
casrel (weiet al., 2020) regards relations as functions that mapsubjects to objects in a text.
it is necessary to rec-ognize subjects ﬁrst and then objects, which leadsto i-ep issue.
to recognize subjects, p([s]|l) ismodelled via p(prox(proxz([s, r, o]))|l), whereprox(·) leads to i-ii issue.
att-as-rel (liu et al.,2020) models the triples by multi-head attention,where completed entities are recognized by model-ing p([s] ∪ [o]|l) separately and thus there is i-iiissue.
similarly, tplinker (wang et al., 2020b)regards joint extraction as a token pair linking prob-lem, where entity recognition is also modelled sep-arately via p([s] ∪ [o]|l)..3 the proposed stereorel model.
i.to handle the above three issues simultaneously,we avoid to make the operations in table 1 andtry to model p([s, r, o]|l) via (cid:80)|r|[p([s, ri]|l)+p([ri, o]|l) + p(shr([s, ri, o])|l)].
as depictedin figure 3, the proposed stereorel model ﬁrstleverages bert encoder to extract the text rep-resentation for the original text.
then, for eachpredeﬁned relation, the text representation is trans-formed to its subject and object spaces.
based onthem, three decoders are built to separately modelp(proxz([s, r, o])|l), p(proyz([s, r, o])|l) and.
p(shr([s, r, o])|l).
the ﬁrst two will cause noissue and provide complete entities for the lastshr(·) operation..3.1 bert encoder.
to sufﬁciently capture the textual information, theencoder is built by a pre-trained language model,bert (devlin et al., 2019).
bert encoder tok-enizes a text l using a predeﬁned vocabulary andgenerates a corresponding sequence ˇl by concate-nating a [cls] token, the tokenized text and a[sep] token.
the detailed steps can be referredto (devlin et al., 2019).
bert encoder will embeda text l into a matrix t ∈ r(|l|+2)×db, where dbis the hidden size of bert, and tj can be seen asthe word embedding of j-th token, j ∈ [0, |l| + 1].
after this, for each relation ri, t is transformed toa new text representation ti ∈ r(|l|+2)×dr by:.
ti = φ(t wi + bi),i=1 ∈ rdb×dr , {bi}|r|.
where {wi}|r|.
i=1 ∈ r1×drare trainable parameters and φ(·) is predeterminedactivation function..(1).
i.
3.2 subject decodersubject decoder is to model (cid:80)|r|p([s, ri]|l), thatis, (x, z)-plane projection p(proxz([s, r, o])|l),which recognizes the subjects for each predeﬁnedrelation.
for one speciﬁc relation ri, we transformi ∈ r(|l|+2)×de in ri’sits text representation to tsubsubject space with de being the hidden size.
thetransformation is implemented by.
tsub.
i = tsub−q.
i.
+ tsub−ki.
+ tsub−bi.,.
(2).
i.i.i.i., tsub−ki.tsub−q/k/b.
+ bsub−q/k/b.
= φ(ti wsub−q/k/bi.
),(3)where tsub−q, tsub−bare linear trans-iformations on top of ti.
tsub−q, tsub−kwill beiused by shrinkage decoder, while tsub−bonlyi}|r|{wsub−qi=1,works for subject decoder.
i}|r|{wsub−b{wsub−krdr×de,i=1 ∈ii}|r|}|r|{bsub−qi=1, {bsub−bi=1 ∈ r1×deiiare trainable parameters and φ(·) is predeter-mined activation function.
based on tsub, allpossible subjects in relation ri’s subject space arerecognized by a sequential conditional randomﬁeld (crf) (lafferty et al., 2001) layer with.
}|r|i=1,}|r|i=1, {bsub−ki.i.
4855figure 3: the proposed stereorel model.
there are three decoders, respectively extracting subjects, objects andtheir correspondences for each predeﬁned relation..[begin, inside, outside] tagging schema,where the probability of the ﬁnal label sequence,ysubi2 , ..., ysubi|l|],is modeled asifollows:.
i1 , ysub.
= [ysub.
p(ysubi.
|l) =.
(cid:81)|l|.
j=1 φj(ysub(cid:81)|l|.
i(j−1), ysubijj−1, y(cid:48).
j=1 φj(y(cid:48).
y(cid:48) ∈y.
|l).
j|l).
(cid:80).
, (4).
i.
{wobj−qi., tobj−ki.
}|r|i=1,}|r|i=1, {bobj−k.
where tobj−q, tobj−bare lineari}|r|transformations on top of ti.
i=1,}|r|{wobj−b{wobj−krdr×de,i=1 ∈ii}|r|}|r|{bobj−qi=1, {bobj−bi=1 ∈ r1×deiin like wise, objectsare trainable parameters.
of the i-th predeﬁned relation are tagged asyobji = [yobji|l|] via another crf layeras:.
i2 , ..., yobj.
i1 , yobj.
i.i.φj(y, ˆy|l) = exp(tsubwcrfsub.
y,ˆy + bcrfsub.
y,ˆy.
),.
(5).
where y denotes all possible label sequence ofl. wcrfsuband bcrfsubare trainable parametersy,ˆycorresponding to the label pair (y, ˆy)..y,ˆy.
3.3 object decoderobject decoder is to model (cid:80)|r|p([ri, o]|l), thatis, (y, z)-plane projection p(proyz([s, r, o])|l),which recognizes the objects for each predeﬁnedrelation.
similar to subject decoder, the text repre-sentation in object space, tobji ∈ r|r|×(|l|+2)×de,is obtained in object decoder as:.
i.tobj.
i = tobj−q.
i.
+ tobj−ki.
+ tobj−bi.,.
(6).
tobj−q/k/b.
i.
= φ(ti wobj−q/k/bi.
+ bobj−q/k/b.
i.
),(7).
p(yobji.
|l) =.
(cid:81)|l|.
j=1 φj(yobj(cid:81)|l|.
i(j−1), yobjj−1, y(cid:48).
j=1 φj(y(cid:48).
ij |l).
j|l).
y(cid:48) ∈y.
(cid:80).
, (8).
φj(y, ˆy|l) = exp(tobjwcrfobj.
y,ˆy + bcrfobj.
y,ˆy.
),.
(9).
where wcrfobj.
y,ˆy.
and bcrfobjy,ˆy.
are trainable parameters..3.4 shrinkage decoder.
i.to extract the correspondences between subjectsand objects, shrinkage decoder is leveraged tomodel (cid:80)|r|p(shr([s, ri, o])|l), where each ele-ment of shr(·) denotes whether the correspondingtoken pair is one speciﬁc position of a (subject, ob-ject) pair, such as (start, start) or (end, end).
tomodel this, a pair-wise classiﬁcation function f isestablished as:.
pshrijj(cid:48) = f (tsub.
ij , tobj.
ij(cid:48) ),.
(10).
4856indicating the probability that j-token and j (cid:48)-tokenis the specifc position of a (subject, object) pair,which satisﬁes the i-th predeﬁned relation.
wedesign the function as follows:.
ijj(cid:48) = ξ(psub→objpshr.
ijj(cid:48).
, pobj→subijj(cid:48).
),.
(11).
table 3: statistics of datasets..datasetnytwebnlgnyt10nyt11wiki-kbp.
rel-num train valid500056196501950070339 (0.5%)62648 (0.5%)79934.test50007034006369(10%) 289.
24171291213.psub→objijj(cid:48).
= softmaxj(ψ(tsub−q.
ij.
, tobj−kij(cid:48).
)),.
(12).
4 experiments.
ij(cid:48).
, tsub−kij.
pobj→subijj(cid:48).
= softmaxj(cid:48) (ψ(tobj−q.
)),(13)where ψ(·) is implemented by dot product or neuralnetwork to provide an initial probability.
psub→objand pobj→subrespectively indicate the probabilityijj(cid:48)distributions for a subject searching for its objectsand an object searching for its subjects, which areintegrated via a predetermined function ξ(·), suchas minimum, maximum and multiplication..ijj(cid:48).
3.5 learning and inference.
subject and object decoders are learned by text-level log-likelihood loss, while shrinkage decoderis learned by token-level binary cross-entropy loss.
thus, the uniﬁed model is learned by a combinedloss function ltotal = lsub + lobj + lshr, where.
lsub = −.
log(p(ysub.
|l)),.
(14).
lobj = −.
log(p(yobj.
|l)),.
(15).
|r|(cid:88).
|r|(cid:88).
i.i.i.i.
|r|(cid:88).
|l|(cid:88).
|l|(cid:88).
lshr = −.
(cid:104)ijj(cid:48) log(pshrˆpshr.
ijj(cid:48) )+.
i.j.j(cid:48)(cid:105)ijj(cid:48) ) log(1 − pshrijj(cid:48) ).
..(1 − ˆpshr.
(16).
and yobj.
the relational triples can be inferred based onthe three decoders.
concretely, for each predeﬁnedrelation ri, the subjects and objects can be obtainedby ysubrespectively.
for the subject sijiand object oij(cid:48) with (j-th token, j (cid:48)-th token) satis-fying the speciﬁc position, if pshrijj(cid:48) is greater thana predetermined threshold δ, (sij, ri, oij(cid:48) ) will beextracted as a relational triple..i.to evaluate the proposed stereorel model, weconduct a performance comparison on ﬁve publicdatasets in this section..4.1 experimental settings.
evaluation metrics and datasets.
generally, theperformance on relational triple extraction is evalu-ated by precision (pre.
), recall (rec.)
and f1-score(f1) , where a triple is regarded as correct if subject,relation and object are all matched.
notably, in pre-vious works, there are two evaluation modes: par-tial match and exact match.
the former holds thatsubject (or object) is correct as long as its head ortail is correct, while the latter requires it to be recog-nized completely.
to properly compare our modelwith various baselines, benchmark datasets are se-lected for the two modes separately.
concretely,we utilize nyt (riedel et al., 2010), webnlg(gardent et al., 2017), nyt10 (takanobu et al.,2019) and nyt11 (takanobu et al., 2019) datasetsfor partial match, while nyt (riedel et al., 2010)and wiki-kbp (dai et al., 2019) datasets for ex-act match.
the details are shown in table 3. thesplits of validation set are the same as previousresearches.
implementation details.
for making a fair com-parison, we utilize the cased bert-base1 model inour experiments, which is the same as casrel (weiet al., 2020) and tplinker (wang et al., 2020b),and thus db = 768. adam optimizer (kingma andba, 2015) is utilized to train the proposed methodwith initial learning rate being 1e-5.
the hiddensize dr, de are set as 64, 32. the threshold δ istuned for each relation and determined by the val-idation set.
φ(·) is set as relu activation function.
ψ(·) is set as dot product.
ξ(·) is set as the multi-plication function..1https://storage.googleapis.com/bert_models/2018_10_18/cased_l-12_h-768_a-12.
zip.
4857table 4: performance comparison by partial match onnyt.
table 6: performance comparison by partial match onnyt10.
table 5: performance comparison by partial match onwebnlg.
modelnoveltaggingcopyre-onecopyre-mulgraphrel-1pgraphrel-2pcopyrrlwdeccasreltplinkerstereorel (ours).
pre.
rec.
31.762.453.159.456.661.057.362.960.063.967.277.976.294.589.589.792.591.392.392.0.modelnoveltaggingcopyre-onecopyre-mulgraphrel-1pgraphrel-2pcopyrrlwdeccopymtl-onecopymtl-mulatt-as-relcasreltplinkerstereorel (ours).
pre.
rec.
19.352.528.932.236.437.739.242.341.144.759.963.351.388.660.157.854.958.086.089.590.193.492.091.892.691.6.f142.056.058.760.061.972.184.489.691.992.2.f128.330.537.140.742.961.665.058.956.487.791.891.992.1.
4.2 performance comparison.
we employ some recent advanced methods as base-lines, mainly including the models analyzed in ta-ble 2. table 4, 5, 6 and 7 report the results of ourmethod against the baselines for partial match eval-uation mode, and table 8 and 9 report the resultsfor exact match.
the models before casrel do notemploy bert encoder and the rest does..as aforementioned in table 2, existing modelsdo not handle three challenging issues simultane-ously, while our proposed stereorel model does.
among the baselines, att-as-rel is the ﬁrst workto extract triples for each predeﬁned relation withno i-il and i-ep issues, and thus achieves a hugeperformance improvement compared with previ-ous methods.
based on bert encoder, the per-formance on relational triple extraction has beenfurther improved by casrel and tplinker.
due.
modelnoveltaggingcopyre-mulhrlwdecpndeccasrelstereorel (ours).
pre.
rec.
38.159.345.256.958.671.462.184.663.981.568.877.767.480.0.f146.450.464.471.671.673.073.2.table 7: performance comparison by partial match onnyt11.
modelnoveltaggingcopyre-mulhrlcasrelstereorel (ours).
pre.
rec.
48.946.953.434.753.853.858.450.155.453.8.f147.942.153.853.954.6.to no i-ep issue, tplinker outperforms casrel.
however, tplinker still suffers from i-ii issue.
our proposed stereorel model further considersit and achieves a better performance.
from the re-sults, comparing with the second best baseline, theperformance improvement of the existing best base-line on the ﬁve datasets were 2.5%, 0.1%, 1.4%,0.1% and 1.5% respectively, in terms of f1-score.
our model obtains performance gain about 0.3%,0.2%, 0.2%, 0.7% and 0.6% in terms of the bestbaseline.
it can be seen that the improvement issatisﬁed..5 discussions and perspectives.
for relational triple extraction, from the stereo-scopic perspective, there are the following two as-pects worthy of discussion.
the ﬁrst one is aboutlearning strategy.
most of previous studies and oursemploy binary cross-entropy loss to learn the mod-els.
however, since the label space of relationaltriple in 3-d space is huge, binary cross-entropy isavailable but not necessarily optimal.
meanwhile,cross-entropy is permutation-sensitive loss func-tion (sui et al., 2020), which is incompatible withgenerative models (zeng et al., 2018, 2019, 2020)since it is necessary to predetermine extraction or-der of multiple triples.
to this question, cgt (yeet al., 2020) incorporates contrastive learning strat-egy and spn (sui et al., 2020) transforms relational.
4858table 9: performance comparison by exact match onwiki-kbp.
acknowledgments.
table 8: performance comparison by exact match onnyt.
modelnoveltagginghrlmhswdecpndeccopymtl-onecopymtl-mulatt-as-reltplinkerstereorel (ours).
pre.
rec.
30.632.877.178.158.660.776.188.177.380.669.272.768.775.778.588.192.691.492.392.0.f131.777.659.681.778.970.972.083.092.092.2.modelnoveltaggingpa-taggingcasrelstereorel (ours).
pre.
rec.
30.353.639.351.142.749.842.950.8.f138.744.445.946.5.triple extraction into set prediction problem learnedby bipartite matching loss.
these ideas may be in-troduced in the future..the second one is to recognize nested entitiesin relational triples.
nested entities are the entitiesamong which there are substring relationships, like“u.n.” being a substring of “u.n.
ambassador”.
such entities deﬁnitely affect the overall perfor-mance on exact match mode.
take nyt dataset asan example, there are about 2.5% sentences contain-ing nested entities.
nested entity recognition hasbeen widely studied (li et al.
; wang et al., 2020a),but most studies on relational triple extraction havenot considered it.
tplinker (wang et al., 2020b)provides a solution to recognize nested entities viaa token pair tagging, but it ignores the interactionbetween entity and relation.
for stereorel model,although not focusing on nested entities, it has notbeen much affected.
the reason is that stereorelrecognizes subjects and objects for each predeﬁnedrelation separately.
in this case, only 0.06% ofnested entities in nyt cannot be marked.
anyway,modeling nested entities from the stereoscopic per-spective is worth exploring in the future..6 conclusions.
relational triple extraction is critical to understand-ing massive text corpora.
however, existing studies.
face some challenging issues, including informa-tion loss, error propagation and ignoring the inter-action between entity and relation.
in this paper,aiming at simultaneously handling the above is-sues, we provide a revealing insight into relationaltriple extraction from a stereoscopic perspective,which rationalizes the occurrence of these issuesand exposes the shortcomings of existing methods.
further, we propose a novel model leveraging threedecoders to respectively extract subjects, objectsand their correspondences for each predeﬁned rela-tion.
extensive experiments are conducted on ﬁvepublic datasets, demonstrating that the proposedmodel outperforms the recent advanced baselines..this work was supported in part by the nationalnatural science foundation of china under grant61822601 and 61773050; the beijing natural sci-ence foundation under grant z180006; the openproject program foundation of the key labora-tory of opto-electronics information processing,chinese academy of sciences(oeip-o-202004);national key research and development projectno.
2019yfb1405202..references.
giannis bekoulis, johannes deleu, thomas demeester,and chris develder.
2018.joint entity recogni-tion and relation extraction as a multi-head selectionproblem.
expert syst.
appl., 114:34–45..yee seng chan and dan roth.
2011..exploitingsyntactico-semantic structures for relation extrac-tion.
in the 49th annual meeting of the associationfor computational linguistics: human languagetechnologies, proceedings of the conference, 19-24june, 2011, portland, oregon, usa, pages 551–560..dai dai, xinyan xiao, yajuan lyu, shan dou, qiao-qiao she, and haifeng wang.
2019.joint ex-traction of entities and overlapping relations us-in theing position-attentive sequence labeling.
thirty-third aaai conference on artiﬁcial intelli-gence, aaai 2019, the thirty-first innovative ap-plications of artiﬁcial intelligence conference, iaai2019, the ninth aaai symposium on educationaladvances in artiﬁcial intelligence, eaai 2019, hon-olulu, hawaii, usa, january 27 - february 1, 2019,pages 6300–6308..jacob devlin, ming-wei chang, kenton lee, andkristina toutanova.
2019. bert: pre-training of deepbidirectional transformers for language understand-in proceedings of the 17th conference of theing..4859north american chapter of the association for com-putational linguistics: human language technolo-gies, pages 4171–4186..tsu-jui fu, peng-hsuan li, and wei-yun ma.
2019.graphrel: modeling text as relational graphs forjoint entity and relation extraction.
in proceedingsof the 57th conference of the association for compu-tational linguistics, acl 2019, florence, italy, july28- august 2, 2019, volume 1: long papers, pages1409–1418..claire gardent, anastasia shimorina, shashi narayan,and laura perez-beltrachini.
2017. creating train-in proceed-ing corpora for nlg micro-planners.
ings of the 55th annual meeting of the associationfor computational linguistics, acl 2017, vancou-ver, canada, july 30 - august 4, volume 1: longpapers, pages 179–188..diederik p. kingma and jimmy ba.
2015. adam: amethod for stochastic optimization.
in proceedingsof the 3rd international conference on learningrepresentations..john d. lafferty, andrew mccallum, and fernandoc. n. pereira.
2001. conditional random ﬁelds:probabilistic models for segmenting and labelingin icml 2001, williams college,sequence data.
williamstown, ma, usa, june 28 - july 1, 2001,pages 282–289..bing li, shifeng liu, yifang sun, wei wang, and xi-ang zhao.
recursively binary modiﬁcation modelfor nested named entity recognition.
in the thirty-fourth aaai conference on artiﬁcial intelligence,aaai 2020, the thirty-second innovative appli-cations of artiﬁcial intelligence conference, iaai2020, the tenth aaai symposium on educationaladvances in artiﬁcial intelligence, eaai 2020, newyork, ny, usa, february 7-12, 2020, pages 8164–8171..xiaonan li, hang yan, xipeng qiu, and xuanjinghuang.
2020. flat: chinese ner using ﬂat-latticein proceedings of the 58th annualtransformer.
meeting of the association for computational lin-guistics, acl 2020, online, july 5-10, 2020, pages6836–6842..jie liu, shaowei chen, bingquan wang, jiaxin zhang,na li, and tong xu.
2020. attention as relation:learning supervised multi-head self-attention for re-in proceedings of the twenty-lation extraction.
ninth international joint conference on artiﬁcial in-telligence, ijcai 2020, pages 3787–3793..makoto miwa and mohit bansal.
2016. end-to-end re-lation extraction using lstms on sequences and treestructures.
in proceedings of the 54th annual meet-ing of the association for computational linguistics,acl 2016, august 7-12, 2016, berlin, germany, vol-ume 1: long papers..tapas nayak and hwee tou ng.
2020. effective mod-eling of encoder-decoder architecture for joint entityand relation extraction.
in the thirty-fourth aaaiconference on artiﬁcial intelligence, aaai 2020,the thirty-second innovative applications of arti-ﬁcial intelligence conference, iaai 2020, the tenthaaai symposium on educational advances in arti-ﬁcial intelligence, eaai 2020, new york, ny, usa,february 7-12, 2020, pages 8528–8535..xiang ren, zeqiu wu, wenqi he, meng qu, clare r.voss, heng ji, tarek f. abdelzaher, and jiawei han.
2017. cotype: joint extraction of typed entities andrelations with knowledge bases.
in proceedings ofthe 26th international conference on world wideweb, www 2017, perth, australia, april 3-7, 2017,pages 1015–1024..sebastian riedel, limin yao, and andrew mccallum.
2010. modeling relations and their mentions with-out labeled text.
in machine learning and knowl-edge discovery in databases, european conference,ecml pkdd 2010, barcelona, spain, september20-24, 2010, proceedings, part iii, pages 148–163..dianbo sui, yubo chen, kang liu, jun zhao, xian-grong zeng, and shengping liu.
2020. joint entityand relation extraction with set prediction networks.
corr, abs/2011.01675..ryuichi takanobu, tianyang zhang, jiexi liu, andminlie huang.
2019. a hierarchical frameworkfor relation extraction with reinforcement learning.
in the thirty-third aaai conference on artiﬁcialintelligence, aaai 2019, the thirty-first innova-tive applications of artiﬁcial intelligence confer-ence, iaai 2019, the ninth aaai symposium on ed-ucational advances in artiﬁcial intelligence, eaai2019, honolulu, hawaii, usa, january 27 - febru-ary 1, 2019, pages 7072–7079..jue wang, lidan shou, ke chen, and gang chen.
2020a.
pyramid: a layered model for nested namedin proceedings of the 58th an-entity recognition.
nual meeting of the association for computationallinguistics, acl 2020, online, july 5-10, 2020,pages 5918–5928..yucheng wang, bowen yu, yueyang zhang, tingwenliu, hongsong zhu,and limin sun.
2020b.
tplinker: single-stage joint extraction of entitiesand relations through token pair linking.
in proceed-ings of the 28th international conference on com-putational linguistics, coling 2020, barcelona,spain (online), december 8-13, 2020, pages 1572–1582..zhepei wei, jianlin su, yue wang, yuan tian, andyi chang.
2020. a novel cascade binary taggingin pro-framework for relational triple extraction.
ceedings of the 58th annual meeting of the associ-ation for computational linguistics, acl 2020, on-line, july 5-10, 2020, pages 1476–1488..hongbin ye, ningyu zhang, shumin deng, moshachen, chuanqi tan, fei huang, and huajun chen..48602020. contrastive triple extraction with generativetransformer.
corr, abs/2009.06207..dmitry zelenko, chinatsu aone,.
and anthonyrichardella.
2002. kernel methods for relation ex-in proceedings of the 2002 conferencetraction.
on empirical methods in natural language process-ing, emnlp 2002, philadelphia, pa, usa, july 6-7,2002, pages 71–78..daojian zeng, haoran zhang, and qianying liu.
2020.copymtl: copy mechanism for joint extraction ofentities and relations with multi-task learning.
inthe thirty-fourth aaai conference on artiﬁcial in-telligence, aaai 2020, the thirty-second innova-tive applications of artiﬁcial intelligence confer-ence, iaai 2020, the tenth aaai symposium on ed-ucational advances in artiﬁcial intelligence, eaai2020, new york, ny, usa, february 7-12, 2020,pages 9507–9514..xiangrong zeng, shizhu he, daojian zeng, kang liu,shengping liu, and jun zhao.
2019. learning theextraction order of multiple relational facts in a sen-tence with reinforcement learning.
in proceedingsof the 2019 conference on empirical methods innatural language processing and the 9th interna-tional joint conference on natural language pro-cessing, emnlp-ijcnlp 2019, hong kong, china,november 3-7, 2019, pages 367–377..xiangrong zeng, daojian zeng, shizhu he, kang liu,and jun zhao.
2018. extracting relational facts byan end-to-end neural model with copy mechanism.
in proceedings of the 56th annual meeting of the as-sociation for computational linguistics, acl 2018,melbourne, australia, july 15-20, 2018, volume 1:long papers, pages 506–514..meishan zhang, yue zhang, and guohong fu.
2017.end-to-end neural relation extraction with global op-timization.
in proceedings of the 2017 conferenceon empirical methods in natural language process-ing, emnlp 2017, copenhagen, denmark, septem-ber 9-11, 2017, pages 1730–1740..yue zhang and jie yang.
2018. chinese ner using lat-tice lstm.
in proceedings of the 56th annual meet-ing of the association for computational linguistics,acl 2018, melbourne, australia, july 15-20, 2018,volume 1: long papers, pages 1554–1564..suncong zheng, feng wang, hongyun bao, yuexinghao, peng zhou, and bo xu.
2017. joint extractionof entities and relations based on a novel taggingin proceedings of the 55th annual meet-scheme.
ing of the association for computational linguistics,acl 2017, vancouver, canada, july 30 - august 4,volume 1: long papers, pages 1227–1236..guodong zhou, jian su, jie zhang, and min zhang.
2005. exploring various knowledge in relation ex-traction.
in acl 2005, 43rd annual meeting of theassociation for computational linguistics, proceed-ings of the conference, 25-30 june 2005, universityof michigan, usa, pages 427–434..4861