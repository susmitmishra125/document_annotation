multimet: a multimodal dataset for metaphor understanding.
dongyu zhang1, minghao zhang1, heting zhang1, liang yang2, hongfei lin21key laboratory for ubiquitous network and service software of liaoning province,school of software, dalian university of technology, china2school of computer science and technology, dalian university of technology, chinahflin@dlut.edu.cn.
abstract.
metaphor involves not only a linguistic phe-nomenon, but also a cognitive phenomenonstructuring human thought, which makes un-derstanding it challenging.
as a means ofcognition, metaphor is rendered by more thantexts alone, and multimodal information inwhich vision/audio content is integrated withthe text can play an important role in ex-pressing and understanding metaphor.
how-ever, previous metaphor processing and under-standing has focused on texts, partly due tothe unavailability of large-scale datasets withground truth labels of multimodal metaphor.
in this paper, we introduce multimet, anovel multimodal metaphor dataset to facil-itate understanding metaphorical informationfrom multimodalit con-tains 10,437 text-image pairs from a range ofsources with multimodal annotations of theoccurrence of metaphors, domain relations,sentiments metaphors convey, and author in-tents.
multimet opens the door to automaticmetaphor understanding by investigating mul-timodal cues and their interplay.
moreover, wepropose a range of strong baselines and showthe importance of combining multimodal cuesfor metaphor understanding.
multimet willbe released publicly for research..text and image..1.introduction.
metaphor is frequently employed in human lan-guage and its ubiquity in everyday communicationhas been established in empirical studies (cameron,2003; steen, 2010; shutova et al., 2010).
sincelakoff and johnson (1980) introduced conceptualmetaphor theory (cmt), metaphor has been re-garded as not only a linguistic, but also a cogni-tive phenomenon for structuring human thought.
individuals use one usually concrete concept inmetaphors to render another usually abstract onefor reasoning and communication.
for example,.
(a) a ﬁre in the sky tonight..(b) smoking causes lung cancer..figure 1: examples of multimodal metaphor.
in the metaphorical utterance “knowledge is trea-sure,” knowledge is viewed in terms of treasureto express that knowledge can be valuable.
ac-cording to cmt, metaphor involves the mappingprocess by which a target domain is conceptualizedor understood in terms of a source domain..as a means of cognition and communication,metaphor can occur in more modes than text alone.
multimodal information in which vision/audio con-tent is integrated with the text can also contributeto metaphoric conceptualization (forceville andurios-aparisi, 2009; ventola et al., 2004).
a mul-timodal metaphor is deﬁned as a mapping of do-mains from different modes such as text and image,text and sound, or image and sound (forcevilleand urios-aparisi, 2009).
for example, in figure1 (a), the metaphorical message of ﬁre in the skyis conveyed by a mapping between the target do-main “sky” (sunset) and the source domain “ﬁre”from two modalities.
figure 1 (b) offers anotherexample with the metaphor of lungs made fromcigarettes so a relation is triggered between twodifferent entities, lung and cigarette, with the per-ceptual idea that smoking causes lung cancer.
thesource domain “cigarette” comes from the image,while the target domain “lung” appears in both textand image.
understanding multimodal metaphor.
proceedingsofthe59thannualmeetingoftheassociationforcomputationallinguisticsandthe11thinternationaljointconferenceonnaturallanguageprocessing,pages3214–3225august1–6,2021.©2021associationforcomputationallinguistics3214requires decoding metaphorical messages and in-volves many cognitive efforts such as identifyingthe semantic relationship between two domains(coulson and van petten, 2002; yang et al., 2013),interpreting authorial intent from multimodal mes-sages (evan nelson, 2008), analyzing the sentimentmetaphors convey (ervas, 2019), which might bedifﬁcult for computers to do..qualitative studies have investigated the in-terplay between different modes underlying theunderstanding of multimodal metaphors in com-municative environments such as advertisements(forceville et al., 2017; urios-aparisi, 2009),movies (forceville, 2016; kappelhoff and m¨uller,2011), songs (forceville and urios-aparisi, 2009;way and mckerrell, 2017), and cartoons (refaie,2003; xiufeng, 2013).
in particular, with the de-velopment of mass communication, texts nowa-days are often combined with other modalities suchas images and videos to achieve a vivid, appeal-ing, persuasive, or aesthetic effect for the audience.
this rapidly growing trend toward multimodalityrequires a shift to extend metaphor studies frommonomodality to multimodality, as well as fromtheory-driven analysis to data-driven empirical test-ing for in-depth metaphor understanding..despite the potential and importance of multi-modal information for metaphor research, therehas been little work on the automatic understand-ing of multimodal metaphors.
while a numberof approaches to metaphor processing have beenproposed with a focus on text in the nlp com-munity (shutova et al., 2010; mohler et al., 2013;jang et al., 2015, 2017; shutova et al., 2017; pra-manick et al., 2018; liu et al., 2020), multimodalmetaphors have not received the full attention theydeserve, partly due to the severe lack of multimodalmetaphor datasets with their challenging and time-and labor-consuming creation..to overcome the above limitations, we proposea novel multimodal metaphor dataset (multimet)consisting of text-image pairs (text and its corre-sponding image counterparts) manually annotatedfor metaphor understanding.
multimet will ex-pand metaphor understanding from monomodalityto multimodality and help to improve the perfor-mance of automatic metaphor comprehension sys-tems by investigating multimodal cues.
our maincontributions are as follows:.
• we create a novel multimodal dataset consist-ing of 10,437 text-image pair samples from.
a range of resources including social media(twitter and facebook), and advertisements.
multimet will be released publicly for re-search..• we present ﬁne-grain manual multimodal an-notations of the occurrence of metaphors,metaphor category, what sentiment metaphorsevoke, and author intent.
the quality controland agreement analyses for multiple annota-tors are described..• we quantitatively show the role of textualand visual modalities for metaphor detection;whether and to what extent metaphor affectsthe distribution of sentiment and intention,which quantitatively explores the mechanismof multimodal metaphor..• we propose three tasks to evaluate ﬁne-grained multimodal metaphor understandingabilities, including metaphor detection, senti-ment analysis, and intent detection in multi-modal metaphor.
a range of baselines withbenchmark results are reported to show thepotential and usefulness of the multimet forfuture research..2 related work.
2.1 metaphor datasets.
although datasets of multimodal metaphors arescarce, a variety of monomodal datasets formetaphor studies have been created in recent years.
table 1 lists these datasets with their properties..numerous text metaphor datasets have been pub-lished for metaphor processing in the nlp commu-nity including several popular ones, e.g., the vuamsterdam metaphor corpus (vuamc) (steen,2010), trofi example base (birke and sarkar,2006), and moh-x (mohammad et al., 2016).
thelargest one, vuamc, consists of over 10,000 sam-ples spread across 16,000 sentences, while otherscontain less than 5,000 samples.
however, most ex-isting metaphor datasets contain only textual data.
image metaphor datasets are few and they are prettylimited in the size and the scope of the data, such asvismet (steen, 2018), which is a visual metaphoronline resource consisting of only 353 image sam-ples.
although shutova et al.
(2016) constructedboth text and image samples, their images were ob-tained by using a given phrase and queried google.
3215metaphor dataset.
modality.
data source.
annotation.
trofi (birke and sarkar, 2006)vuamc (steen, 2010)tsv (tsvetkov et al., 2014)lcc (mohler et al., 2016)moh (mohammad et al., 2016)zayed’s tweets (zayed et al., 2019)visual met (steen, 2018)shutova et al.
(2016)multimet (ours).
size.
sample(%metaphor)3,737 (44%)16,000 (12.5%)3,334 (50%)16,265 (19%)1,639 (25%)2,500 (54%)353 (100%)2,415 (50%)10,437 (58%).
texttexttexttexttexttextimagetext,image wordnettext,image social media, adv metaphor, sentiment, intent.
metaphor (metaphoricity)wsjmetaphorbnc babymetaphor, affectwebmetaphorclueweb09metaphorwordnettwittermetaphoradv, arts, cartoons metaphormetaphor.
table 1: comparison of various metaphor datasets.
images.
in that way, words and images in theirwork may be not suitably presented by each other.
the cognitive nature of metaphor implies thatnot only one modal isolation, but rather inte-grated multimodal information may contribute tometaphor expression and understanding, whichmakes our dataset multimet, which is large scaleand contains both natural text and image mes-sages and their annotations, different from existingdatasets and more important for metaphor studies..2.2 metaphor understanding.
automatic metaphor understanding requires ac-complishing certain tasks to decode metaphoricalin this paper, we focus on three im-messages.
portant tasks for nlp in understanding metaphor:metaphor detection, sentiment analysis, and authorintent detection.
there has been increasing interestin nlp in various approaches to metaphor detec-tion based on monomodal text.
early metaphorstudies have focused on hand-constructed knowl-edge and machine learning techniques (mason,2004; turney et al., 2011; tsvetkov et al., 2014;hovy et al., 2013).
others have also used distribu-tional clustering (shutova et al., 2013) and unsuper-vised approaches (shutova et al., 2017; mao et al.,2018).
more recently, deep learning models havebeen explored to understand metaphor.
however,little has been explored in multimodal metaphordetection except by shutova et al.
(2016), who areamong the very few to explore the fusion of tex-tual and image modalities to detect multimodalmetaphor.
their results demonstrate the positiveeffect of combining textual and image features formetaphor detection..however, in their work, image features are ex-tracted from a small size of constructed examplesrather than natural samples of texts integrated within addi-images, like multimet in our work..tion, apart from multimodal metaphor detection,the tasks related to metaphor understanding likesentiment detection and author intent detection inmultimodal metaphor also have rarely been stud-ied, although there exist similar multimodal studiesin different tasks (wang et al., 2017; zadeh et al.,2017; kruk et al., 2019)..3 the multimet dataset.
3.1 data collection.
with the goal of creating a large-scale multimodalmetaphor dataset to support research on understand-ing metaphors, we collect data that contains bothtext and image from a range of sources includingsocial media (twitter and facebook), and advertise-ments.
table 2 shows an overview for the statisticsof the dataset..social media.
to collect potential metaphoricalsamples from twitter and facebook, we retrievedposts by querying hashtags metaphor or metaphor-ical.
we collected publicly available twitter andfacebook posts using twitter and facebook apiscomplying with twitter and facebook’s terms ofservice.
what the author labels as metaphoricalis not always aligned with the actual deﬁnitionof metaphor in our study.
to collect metaphorswhose nature accorded with what we deﬁne as mul-timodal metaphors, we re-annotated “metaphoricalor literal” in the below section to potential twitterand facebook posts that other authors annotated asmetaphor with hashtags..advertisements.
based on our review of lin-guistic literature on multimodal metaphor, we fo-cused on an important source that is the main con-text of study: advertisements.
metaphorical mes-sages abound in advertisements , which offer a nat-ural and rich resource of data on metaphor and howtextual and visual factors combine and interact (so-brino, 2017; forceville et al., 2017).
we collected.
3216only when both text and images were repeated..3.3 annotation model.
we annotated the text-image pairs with the occur-rence of metaphors (literal or metaphorical); (ifmetaphorical) relations of target and source do-main (target/source: target/source vocabulary intext or verbalized target/source vocabulary in im-age); target/source modality (text, image, or text +image), metaphor category (text-dominant, image-dominant, or complementary); sentiment category(the sentiment metaphors evoke, namely very neg-ative, negative, neutral, positive, or very positive),and author intents (descriptive, expressive, persua-sive, or other).
the annotation model was anno-tationmodel = (occurrence, target, source, tar-getmodality, sourcemodality, metaphorcategory,sentimentcategory, intent, datasource).
figure 3is an annotation example..figure 3: an example of a text+image annotation.
3.4 metaphor annotation.
metaphor category.
there are a variety of waysin which texts and images are combined in multi-modal content (hendricks et al., 2016; chen et al.,2017).
based on our review of the literature and ob-servation of the samples in our dataset, we followtasi´c and stamenkovi´c (2015) and divide multi-modal metaphor into three categories: text domi-nant, image dominant, and complementary.
some-times metaphors are expressed through texts with amapping between source and target domains whilethe accompanying images serve as a visual illus-tration of the metaphors in the text, which is textdominant.
as in figure 2 (a), the text itself is sufﬁ-cient to convey metaphorical information and canbe identiﬁed as metaphorical expressions.
“high-way” is a visual illustration of the source domainin a textual modality.
by contrast, in the imagedominant category, images play the dominant rolein conveying metaphorical information and theyprovide sufﬁcient information for readers to under-.
(a) life is a highway.
(b).
sometimes, with-out knowing why, yourheart beats faster.
newbeetle..(c) a kitten is kissinga ﬂower.
butterﬂies arenot insects..figure 2: examples of metaphor categories.
item.
adv.
total.
total samplesmetaphorical samplesliteral samplestotal wordsavg words of samplestrain set sizevalidation set sizetest set size.
socialmedia6,1093,4892,62079,417132,791349349.
4,3282,5371,79151,936122,029254254.
10,4376,0264,411131,353134,820603603.table 2: multimet dataset statistics.
potential metaphorical samples of advertising froma large, publicly released dataset of 64,832 imageadvertisements that contain both images and insidetext (ye et al., 2019).
to obtain the textual infor-mation, we extracted inside text from images usingthe api provided by baidu ai.
after that, humanannotators rectiﬁed the extracted inaccurate text, re-moved any blurred text, and obtained text + imagepairs from advertisements..3.2 data filter.
for text data, we removed external links and men-tions (@username); we removed non-english textusing the langid (lui and baldwin, 2012) li-brary to label each piece of data with a languagetag; we removed strange symbols such as emo-jis; we removed “metaphor” or “metaphoric” whenthey were regular words rather than hashtags, be-cause explicit metaphorical expressions are not ourinterest (e.g., “this metaphor is very appropriate”);we removed text with fewer than 3 words or morethan 40 words.
for image data, we removed text-based images (all the words are in the image), aswell as images with low resolution.
because thistask is about multimodal metaphor, it is necessaryto maintain consistency of data between models.
in other words, either both the image data and thetext data should be removed, or neither.
in addition,in the de-duplication step, we considered removal.
3217stand the metaphors.
in figure 2 (b), where wesee the metaphorical message “beetle (cars) areblood cells,” the text enriches the understandingof metaphorical meaning by adding an explana-tion “your heart beats faster” to the visual mani-festation.
the complementary category involves aroughly equal role of texts and images in renderingmetaphorical information.
the understanding ofmetaphor depends on the interaction of and balancebetween different modalities.
if texts and imagesare interpreted separately, metaphors cannot be un-derstood.
in figure 2 (c), when people read thetext, “a kitten is kissing a ﬂower,” and the insidetext “butterﬂies are not insects,” they do not realizethe metaphorical use until they observe the butter-ﬂy in the corresponding image and infer that thetarget “butterﬂy” is expressed in term of the source“ﬂower”..metaphorical or literal.
our annotations fo-cus on the dimension of expression, which in-volves identiﬁcation of metaphorical and literalexpressions by verbal means and visual means(forceville, 1996; phillips and mcquarrie, 2004).
the metaphor annotation takes place at the rela-tional level, which involves the identiﬁcation ofmetaphorical relations between source and targetdomain expressions.
for text modality, source andtarget domain expressions mean source and tar-get domain words used in metaphorical texts.
forimage modality, source and target domain expres-sions mean words’ verbalized source and targetdomain in the visual modality.
that is, the anno-tation of metaphorical relations represented in themodality of image involve the verbalization of themetaphor’s domains.
annotations involve namingand labeling what is linguistically familiar.
unliketext modality, which relies on explicit linguisticcues, for image modality, metaphorical relationsare annotated based on perceptions of visual uni-ties, and they determine the linguistic familiarity ofimages as well as existing words in the metaphor’sdomains.
following ˇsorm and steen (2018), anno-tators identiﬁed the metaphorical text+image pairsby looking at the incongruous units and explain-ing one non-reversible “a is b” identity relation,where two domains were expressed by differentmodalites..3.5.intent and sentiment annotation.
interpreting authorial intent from multimodal mes-sages in metaphor seems to be important for under-.
standing metaphors.
as mentioned above, withincmt, the essence of metaphor is using one thingfrom a source domain to express and describe an-other from a target domain.
this implies thatone important intent of creating metaphor couldbe to enable readers to understand the entities be-ing described better.
“perceptual resemblance” isa major means of triggering a metaphorical rela-tion between two different entities (forceville andurios-aparisi, 2009).
we name it descriptive intent,which involves visual and textual representationsregarding the object, event, concept, information,action or character, etc.
moreover, in modern times,the increasing ubiquity of multimodal metaphorsmeans that people cannot ignore its power of per-suasion (urios-aparisi, 2009).
people often lever-age metaphor in communication environments suchas advertisements and social media to persuadereaders to buy or do things.
we name this in-tent as persuasive.
in addition, inspired by a vari-ety of arousing, humorous, or aesthetic effects ofmetaphors (christmann et al., 2011), the expres-sive is included in our intent annotation within theenlarged deﬁnition: expressing attitude, thought,emotion, feeling, attachment, etc.
based on thesefactors as well as investigation of the samples in ourdatasets, we generalized their taxonomy and listedthe categories of the author intent in metaphor asdescriptive,persuasive, expressive, and others..numerous studies show that metaphorical lan-guage frequently expresses sentiments or emotionsimplicitly (goatly, 2007; k¨ovecses, 1995, 2003).
compared to literal expressions, metaphors elicitmore emotional activation of the human brain in thesame context (citron and goldberg, 2014).
thuswe also added the sentiment in our annotation, totest whether the sentiment impact of metaphors isstronger than literary messages from a multimodalperspective.
the sentiment was placed in one of theﬁve categories of very negative, negative, neutral,positive, or very positive..3.6 annotation process.
we took two independent annotation approachesfor two different types of tasks: selecting typesof sentiment and intent and the annotation ofmetaphor.
to select the options for sentiment andintent, we used a majority vote through crowd-flower, the crowdsourcing platform.
the partici-pants were randomly presented with both the textand vision components with the instruction on the.
3218(a) metaphor category..(b) metaphor and non-metaphor intent..(c) metaphor and non-metaphor sentiment..figure 4: dataset distribution.
top of each text + image pair for options..the annotation of metaphors includes metaphoroccurrence, metaphor category and domain rela-tion annotation.
for metaphor annotation, we usedexpert annotators to complete the challenging an-notation task, which required relatively deep under-standing of metaphorical units and the completetask of verbalization of domains in image.
theannotator team comprised ﬁve annotators who arepostgraduate student researchers majoring in com-putational linguistics with metaphor study back-grounds.
the annotators formed groups of two,plus one extra person.
using cross-validation, thetwo-member groups annotated, and the ﬁfth personintervened if they disagreed..3.7 quality control and inner agreement.
annotations of multimodal metaphors rely on an-notators’ opinions and introspection, which mightbe subjective.
thus we took corresponding, dif-ferent measures for different types of annotationsto achieve high-quality annotation.
to select op-tions, we established strict criteria for the choiceof category.
each text-image pair was annotatedby at least 10 annotators and we used a major-ity vote through crowdflower, the crowdsourcingplatform.
following shutova (2017), we chose thecategory of annotated options on which 70% ormore annotators agreed as the answer to each ques-tion (ﬁnal decision) to provide high conﬁdence ofannotation.
for metaphor annotation, we added aguideline course, detailed instruction, and manysamples, and we held regular meetings to discussannotation problems and matters that needed at-tention.
the guidelines changed three times whennew problems emerged or good improvement meth-ods were found.
the kappa score, κ, was used tomeasure inter-annotator agreements (fleiss, 1971).
the agreement on the identiﬁcation of literal ormetaphorical was κ = 0.67; identiﬁcation of textdominant, image dominant or complementary was.
κ = 0.79; the identiﬁcation of source and targetdomain relation was κ = 0.58, which means theyare substantially reliable..4 dataset analysis.
metaphor category.
we analyzed the role oftextual and visual modalities to detect metaphors.
from figure 4 (a), we can see a complementarycategory among the three kinds of multimodalmetaphors, which requires the interplay of textualand visual modality to understand the metaphori-cal meaning.
it accounts for the largest proportionof metaphors, followed by the text-dominant andimage-dominant categories.
it shows the contribu-tion of visual factors, which are similarly impor-tant in detecting metaphors.
we therefore present aquantitative study of the role of textual and visualmodalities in metaphor detection through humanannotations and conﬁrm the role and contributionof visuals in metaphor occurrence in natural lan-guage..author intent.
figure 4 (b) shows that ex-pressive and persuasive intentions occur most fre-quently in the metaphorical data.
however, de-scriptive intention occurs most frequently in thenon-metaphorical data.
this suggests that on theone hand, we are more likely to use metaphoricalexpressions when expressing our feelings, express-ing emotions, or trying to persuade others.
on theother hand, we tend to use literal expressions tomake relatively objective statements..sentiment.
figure 4 (c) shows that there aresome differences in the distribution of sentiment be-tween the metaphorical data and non-metaphoricaldata.
in the non-metaphorical data, neutral senti-ment accounted for the largest proportion of 51%,followed by positive sentiment (33%), strong posi-tive sentiment (7%), negative sentiment (7%), andstrong negative sentiment (2%).
in the metaphor-ical data, positive sentiment accounted for thelargest proportion of 42%, followed by neutral sen-.
3219valuehyper-parameter300word embedding sizehidden size of lstm 2560.4dropout30text padding48batch size5e-4learning rate10gradient clipping10early stop patience.
table 3: hyperparameters..timents (39%), strong positive sentiment (8%), neg-ative sentiment (8%), and strong negative sentiment(3%).
it turns out that there are more non-neutralsentiments in metaphor expression than in non-metaphorical expression, and that metaphors aremore frequently used to convey sentiments.
ourﬁndings accord with the results of previous studieson monomodal textual metaphors that metaphorsconvey more sentiments or emotions than literarytext (mohammad et al., 2016).
we conﬁrm thestronger emotional impact of metaphors than liter-ary messages from a multimodal perspective..in positive sentiment, the most common wordsin the source domain are person, face, and ﬂower;the most common words in the target domain arelove, life, and success.
in negative sentiment, heart,food, and smoke are the most common words inthe source domain, and the world, disaster, and lifeare the most common words in the target domain.
this shows that sentiment tendency can inﬂuencethe category in the source and target domains tosome extent..5 experiment.
for the dataset constructed for this paper, wepropose three tasks and provide their baselines,namely multimodal metaphor detection, multi-modal metaphor sentiment analysis, and multi-modal metaphor author intent detection..we used the model shown in figure 5 to detectmetaphors, metaphorical sentiments, and metaphor-ical intentions.
for text input, we used a text en-coder to encode the text and to get the feature vectorof the text.
this paper used two different methodsto encode the text, namely the pre-trained bidirec-tional encoder representations from transform-ers (bert) model (devlin et al., 2019) and bi-directional long-short term memory (bi-lstm)networks (medsker and jain, 2001).
similarly, forimage input, we used an image encoder to extractimage features.
we used three different image pre-.
training models: vgg16 (simonyan and zisser-man, 2014), resnet50 (he et al., 2016), and efﬁ-cientnet (tan and le, 2019).
these methods havebeen widely used by researchers in feature extrac-tion for various tasks..after obtaining the text feature vector and theimage feature vector, we used four different fea-ture fusion methods to combine the vectors, namelyconcatenation (suryawanshi et al., 2020), element-wise multiply (mai et al., 2020), element-wise add(cai et al., 2019), and maximum (das, 2019).
fi-nally, we inputted the fusion vector into a fullyconnected layer and obtained the probabilities ofdifferent categories through the softmax activationfunction..figure 5: multimodal model for integrating text andimage data..5.1 experiment settings.
we used pytorch (paszke et al., 2019) to build themodel.
the pre-trained models are available in py-torch.
the word embeddings have been trained ona wikipedia dataset by glove (pennington et al.,2014).
in the training process, we did not updatethe parameters in the pre-training models.
whenthe model gradually tended to converge, we up-dated the parameters of the pre-training modelswith training data to avoid overﬁtting.
we usedthe adam optimizer (kingma and ba, 2014) to op-timize the loss function, and the training methodof gradient clipping (zhang et al., 2019) to avoidgradient explosion.
other hyper-parameter settingsare shown in table 3..5.2 results.
the classiﬁcation results are shown in table 4.
“random” means that random predictions weremade using the data as a baseline.
in general,the model performed best on metaphor detection,followed by metaphor intention detection, and ﬁ-nally metaphor sentiment detection.
for image and.
3220typerandom.
text.
image.
text + image.
image---vgg16efﬁcientnetresnet50vgg16.
text-bi-lstmbert---bi-lstmbi-lstm efﬁcientnetresnet50bi-lstmvgg16bertefﬁcientnetbertresnetbert.
metaphor.
sentiment.
intention.
validation0.50630.74580.77420.73150.74670.76770.77350.78320.79880.80330.79750.8276.test0.49230.74340.77360.73450.74050.76460.76580.77950.79120.80720.80330.8286.validation0.22220.57050.59580.59530.55630.57150.61950.57230.62630.62890.61520.6462.test0.20230.57140.59270.59140.55480.57140.61570.57140.62200.61880.61250.6422.validation0.34160.65970.67940.66720.64410.66580.68430.66720.70360.70120.68330.7278.test0.36090.65930.67200.66580.63240.66530.68120.67320.68430.70000.67570.7245.table 4: results on three tasks with a combination method of concatenate..combination methods validation.
addmultiplymaximumconcatenate.
metaphor.
sentiment.
intention.
0.78680.75960.78270.8276.test0.78340.75830.77590.8286.validation0.62050.56850.61130.6462.test0.61860.56360.60740.6422.validation0.68270.64420.70350.7278.test0.67790.64570.69930.7245.table 5: results on different multimodal combinations for bert + resnet..multimodal classiﬁcation, the resnet50 performedbest, followed by vgg16, and ﬁnally efﬁcientnet.
because resnet solved the problem of gradientdisappearance through the method of residual con-nection, the classiﬁcation performance was betterthan vgg16 and efﬁcientnet.
for text and multi-modal classiﬁcation, bert performed better thanbi-lstm.
bert has been fully trained in a large-scale corpus, using transfer learning technology toﬁne-tune our three tasks and data, so it can achievebetter performance.
from the perspective of differ-ent features, multimodal features perform best, fol-lowed by text-only features, and ﬁnally image-onlyfeatures.
multimodal fusion helps to improve theclassiﬁcation performance by 6%.
this shows thatthe combination of image and text features is in-deed helpful for the detection and understanding ofmetaphors, especially the detection of sentimentsand intentions in metaphors.
in addition, the im-portance of text modal data is explained.
withouttext description, it is difﬁcult to detect metaphorscorrectly using only visual modal data..to verify the inﬂuence of feature fusion on classi-ﬁcation, we compared four different feature fusionmethods.
the results are shown in table 5. theconcatenate method to merge image and text fea-tures produces the highest accuracy.
it shows thatconcatenate can make full use of the complemen-tarity between different modal data, eliminate thenoise generated by the fusion of different modaldata, and improve the detection effect.
in contrast,.
the other three fusion methods cannot effectivelyeliminate the inﬂuence of noise introduced by dif-ferent modal data, and it therefore interferes withthe training of the model.
overall, the multimodemodel that combines the bert text function andthe resnet50 image function through the concate-nation method performs best on our three tasks..6 conclusion.
this paper presents the creation of a novel re-source, a large-scale multimodal metaphor dataset,multimet, with manual ﬁne-gained annotationfor metaphor understanding and research.
ourdataset enables the quantitative study of the inter-play of multimodalities for metaphor detection andconﬁrms the contribution of visuals in metaphoroccurrence in natural language.
it also offers aset of baseline results of various tasks and showsthe importance of combining multimodal cues formetaphor understanding.
we hope multimetprovides future researchers with valuable multi-modal training data for the challenging tasks ofmultimodal metaphor processing and understand-ing ranging from metaphor detection to sentimentanalysis of metaphor.
we also hope that multi-met will help to expand metaphor research frommonomodality to multimodality and improve theperformance of automatic metaphor understandingsystems and contribute to the in-depth understand-ing and research development of metaphors.
thedataset will be publicly available for research..3221ethical considerations.
this research was granted ethical approval by ourinstitutional review board (approval code: du-tiee190725 01).
we collected publicly availabletwitter and facebook data using twitter and face-book apis complying with twitter and facebook’sterms of service.
we did not store any personaldata (e.g., user ids, usernames) and we annotatedthe data without knowledge of individual identities.
we annotated all our data using two indepen-dent approaches (expert based and crowdsourcingbased) for two different types of tasks: the anno-tation of metaphor and the selection of types ofsentiment and intent.
for metaphor annotation, adeep understanding of metaphorical units was nec-essary.
this challenging task was completed byﬁve researchers who involved in this project.
to an-notate sentiment and intent, we used crowdflower,the crowdsourcing platform.
to ensure that crowdworkers were fairly compensated, we paid themat an hourly rate of 15 usd per hour, which isa fair and reasonable rate of pay for crowdsourc-ing (whiting et al., 2019).
we launched small pi-lots through crowdflower.
the pilot for sentimentoptions took on average 43 seconds, and crowdworkers were thus paid 0.18 usd per judgment, inaccordance with an hourly wage of 15 usd.
at thesame time, the annotation of author intent took onaverage 23 seconds, and we thus paid 0.10 usdper judgment, corresponding to an hourly wage of15 usd..acknowledgments.
we would like to thank the anonymous review-ers for their insightful and valuable comments.
this work is supported by nsfc programs(no.62076051)..references.
julia birke and anoop sarkar.
2006. a clustering ap-proach for nearly unsupervised recognition of non-literal language.
in proceedings of the 11th confer-ence of the european chapter of the association forcomputational linguistics, pages 329–336, trento,italy..yitao cai, huiyu cai, and xiaojun wan.
2019. multi-modal sarcasm detection in twitter with hierarchicalin proceedings of the 57th annualfusion model.
meeting of the association for computational lin-guistics, pages 2506–2515, florence, italy..lynne cameron.
2003. metaphor in educational dis-.
course.
a&c black, london, uk..tseng-hung chen, yuan-hong liao, ching-yaochuang, wan-ting hsu, jianlong fu, and min sun.
2017. show, adapt and tell: adversarial trainingin proceedingsof cross-domain image captioner.
of the 2017 ieee international conference on com-puter vision, pages 521–530, venice, italy..ursula christmann, lena wimmer, and norbertgroeben.
2011. the aesthetic paradox in process-ing conventional and non-conventional metaphors:a reaction time study.
scientiﬁc study of literature,1(2):199–240..francesca mm citron and adele e goldberg.
2014.metaphorical sentences are more emotionally engag-ing than their literal counterparts.
journal of cogni-tive neuroscience, 26(11):2585–2595..seana coulson and cyma van petten.
2002. concep-tual integration and metaphor: an event-related po-tential study.
memory & cognition, 30(6):958–968..dipto das.
2019. a multimodal approach to sarcasmdetection on social media.
ph.d. thesis, missouristate university..jacob devlin, ming-wei chang, kenton lee, andkristina toutanova.
2019. bert: pre-training ofdeep bidirectional transformers for language under-standing.
in proceedings of the 2019 conference ofthe north american chapter of the association forcomputational linguistics: human language tech-nologies, volume 1 (long and short papers), pages4171–4186, minneapolis, usa..francesca ervas.
2019. metaphor, ignorance and thesentiment of (ir) rationality.
synthese, pages 1–25..mark evan nelson.
2008. multimodal synthesis andthe voice of the multimedia author in a japaneseeﬂ context.
innovation in language learning andteaching, 2(1):65–82..joseph l fleiss.
1971. measuring nominal scale agree-ment among many raters.
psychological bulletin,76(5):378–382..charles forceville.
1996. pictorial metaphor in adver-.
tising.
psychology press, east sussex, uk..charles forceville.
2016. visual and multimodalin embodied metaphors in ﬁlm,metaphor in ﬁlm.
television, and video games: cognitive approaches,pages 17–32.
routledge, abingdon, usa..charles forceville and eduardo urios-aparisi.
2009.multimodal metaphor, volume 11. walter degruyter, berlin, germany..charles forceville et al.
2017. visual and multi-modal metaphor in advertising: cultural perspec-tives.
styles of communication, 9(2):26–41..3222andrew goatly.
2007. washing the brain: metaphorand hidden ideology, volume 23. john benjaminspublishing, amsterdam, netherlands..george lakoff and mark johnson.
1980. metaphorswe live by.
university of chicago press, chicago,usa..kaiming he, xiangyu zhang, shaoqing ren, and jiansun.
2016. deep residual learning for image recog-in proceedings of the ieee conferencenition.
on computer vision and pattern recognition 2016,pages 770–778, las vegas, usa..lisa anne hendricks, subhashini venugopalan, mar-cus rohrbach, raymond mooney, kate saenko, andtrevor darrell.
2016. deep compositional caption-ing: describing novel object categories withoutin proceedings of the ieeepaired training data.
conference on computer vision and pattern recog-nition 2016, pages 1–10, las vegas, usa..dirk hovy, shashank srivastava, sujay kumar jauhar,mrinmaya sachan, kartik goyal, huying li, whit-identifyingney sanders, and eduard hovy.
2013.metaphorical word use with tree kernels.
in proceed-ings of the 1st workshop on metaphor in nlp, pages52–57, atlanta, georgia..hyeju jang, keith maki, eduard hovy, and carolynrose.
2017.finding structure in ﬁgurative lan-guage: metaphor detection with topic-based frames.
in proceedings of the 18th annual sigdial meet-ing on discourse and dialogue, pages 320–330,saarbr¨ucken, germany..hyeju jang, seungwhan moon, yohan jo, and car-olyn rose.
2015. metaphor detection in discourse.
in proceedings of the 16th annual meeting of thespecial interest group on discourse and dialogue,pages 384–392, prague, czech republic..hermann kappelhoff and cornelia m¨uller.
2011. em-bodied meaning construction: multimodal metaphorand expressive movement in speech, gesture, andfeature ﬁlm.
metaphor and the social world,1(2):121–153..diederik p kingma and jimmy ba.
2014. adam: amethod for stochastic optimization.
arxiv e-prints,page arxiv:1412.6980..zolt´an k¨ovecses.
1995. anger:.
its language, con-ceptualization, and physiology in the light of cross-cultural evidence.
language and the cognitive con-strual of the world, pages 181–196..zolt´an k¨ovecses.
2003. metaphor and emotion: lan-guage, culture, and body in human feeling.
cam-bridge university press, cambridge, uk..julia kruk, jonah lubin, karan sikka, xiao lin, danintegratingjurafsky, and ajay divakaran.
2019.text and image: determining multimodal documentin proceedings of theintent in instagram posts.
2019 conference on empirical methods in natu-ral language processing and the 9th internationaljoint conference on natural language processing(emnlp-ijcnlp), pages 4614–4624, hong kong,china..jerry liu, nathan o’hara, alexander rubin, racheldraelos, and cynthia rudin.
2020. metaphor detec-tion using contextual word embeddings from trans-in proceedings of the 2nd workshop onformers.
figurative language processing, pages 250–255,online..marco lui and timothy baldwin.
2012..langid.
py:an off-the-shelf language identiﬁcation tool.
in pro-ceedings of the acl 2012 system demonstrations,pages 25–30, jeju island, korea..sijie mai, haifeng hu, jia xu, and songlong xing.
2020. multi-fusion residual memory network formultimodal human sentiment comprehension.
ieeetransactions on affective computing, pages 1–15..rui mao, chenghua lin, and frank guerin.
2018.word embedding and wordnet based metaphor iden-tiﬁcation and interpretation.
in proceedings of the56th annual meeting of the association for compu-tational linguistics (volume 1: long papers), pages1222–1231, melbourne, australia..zachary j mason.
2004. cormet: a computational,corpus-based conventional metaphor extraction sys-tem.
computational linguistics, 30(1):23–44..larry r medsker and lc jain.
2001. recurrent neural.
networks.
design and applications, 5:1–391..saif mohammad, ekaterina shutova, and peter turney.
2016. metaphor as a medium for emotion: an em-pirical study.
in proceedings of the 15th joint con-ference on lexical and computational semantics,pages 23–33, berlin, germany..michael mohler, david bracewell, marc tomlinson,and david hinote.
2013. semantic signatures forinexample-based linguistic metaphor detection.
proceedings of the 1st workshop on metaphor innlp, pages 27–35, atlanta, usa..michael mohler, mary brunson, bryan rink, and marcintroducing the lcc metaphortomlinson.
2016.in proceedings of the 10th internationaldatasets.
conference on language resources and evaluation,pages 4221–4227, portoroˇz, slovenia..adam paszke, sam gross, francisco massa, adamlerer, james bradbury, gregory chanan, trevorkilleen, zeming lin, natalia gimelshein, lucaantiga, et al.
2019. pytorch: an imperative style,high-performance deep learning library.
arxiv e-prints, page arxiv:1912.01703..jeffrey pennington, richard socher, and christopher dmanning.
2014. glove: global vectors for word rep-resentation.
in proceedings of the 2014 conferenceon empirical methods in natural language process-ing, pages 1532–1543, doha, qatar..3223barbara j phillips and edward f mcquarrie.
2004.beyond visual metaphor: a new typology of vi-sual rhetoric in advertising.
marketing theory, 4(1-2):113–136..malay pramanick, ashim gupta, and pabitra mitra.
2018. an lstm-crf based approach to token-levelmetaphor detection.
in proceedings of the workshopon figurative language processing 2018, pages 67–75, new orleans, usa..elisabeth el refaie.
2003..understanding visualmetaphor: the example of newspaper cartoons.
vi-sual communication, 2(1):75–95..ekaterina shutova.
2017. annotation of linguistic andin handbook of linguisticconceptual metaphor.
annotation, pages 1073–1100.
springer, new york,usa..ekaterina shutova, douwe kiela, and jean maillard.
2016. black holes and white rabbits: metaphor iden-tiﬁcation with visual features.
in proceedings of the2016 conference of the north american chapter ofthe association for computational linguistics: hu-man language technologies, pages 160–170, sandiego, usa..ekaterina shutova, lin sun, elkin dar´ıo guti´errez, pa-tricia lichtenstein, and srini narayanan.
2017. mul-tilingual metaphor processing: experiments withsemi-supervised and unsupervised learning.
com-putational linguistics, 43(1):71–123..ekaterina shutova, lin sun, and anna korhonen.
2010.metaphor identiﬁcation using verb and noun cluster-ing.
in proceedings of the 23rd international con-ference on computational linguistics, pages 1002–1010, beijing, china..ekaterina shutova, simone teufel, and anna korho-nen.
2013. statistical metaphor processing.
compu-tational linguistics, 39(2):301–353..karen simonyan and andrew zisserman.
2014. verydeep convolutional networks for large-scale imagerecognition.
arxiv e-prints, page arxiv:1409.1556..paula p´erez sobrino.
2017. multimodal metaphor andmetonymy in advertising, volume 2. john benjaminspublishing company, amsterdam, netherlands..esther ˇsorm and gerard steen.
2018..towards amethod for visual metaphor identiﬁcation.
in visualmetaphor: structure and process, volume 18, pages47–88.
john benjamins publishing company, ams-terdam, netherlands..gerard steen.
2010. a method for linguistic metaphoridentiﬁcation: from mip to mipvu, volume 14.john benjamins publishing, amsterdam, nether-lands..gerard j steen.
2018. visual metaphor: structure andjohn benjamins publishing.
process, volume 18.company, amsterdam, netherlands..shardul suryawanshi, bharathi raja chakravarthi, mi-hael arcan, and paul buitelaar.
2020. multimodalmeme dataset (multioff) for identifying offensivecontent in image and text.
in proceedings of the 2ndworkshop on trolling, aggression and cyberbully-ing, pages 32–41, marseille, france..mingxing tan and quoc le.
2019. efﬁcientnet: re-thinking model scaling for convolutional neural net-works.
in proceedings of international conferenceon machine learning 2019, pages 6105–6114, cali-fornia, usa..miloˇs tasi´c and duˇsan stamenkovi´c.
2015. the inter-play of words and images in expressing multimodalmetaphors in comics.
procedia-social and behav-ioral sciences, 212:117–122..yulia tsvetkov, leonid boytsov, anatole gershman,eric nyberg, and chris dyer.
2014. metaphor detec-tion with cross-lingual model transfer.
in proceed-ings of the 52nd annual meeting of the associationfor computational linguistics (volume 1: long pa-pers), pages 248–258, maryland, usa..peter turney, yair neuman, dan assaf, and yohai co-hen.
2011. literal and metaphorical sense identiﬁca-tion through concrete and abstract context.
in pro-ceedings of the 2011 conference on empirical meth-ods in natural language processing, pages 680–690, scotland, uk..eduardo urios-aparisi.
2009..interaction of multi-modal metaphor and metonymy in tv commercials:four case studies.
multimodal metaphor, 11:95–116..eija ventola, cassily charles, and martin kaltenbacher.
2004. perspectives on multimodality, volume 6.john benjamins publishing, amsterdam, nether-lands..haohan wang, aaksha meghawat, louis-philippemorency, and eric p xing.
2017. select-additiveimproving generalization in multimodallearning:in proceeding of 2017 ieeesentiment analysis.
international conference on multimedia and expo,pages 949–954, hong kong, china..lyndon cs way and simon mckerrell.
2017. music asmultimodal discourse: semiotics, power and protest.
bloomsbury publishing, london, uk..mark e whiting, grant hugh, and michael s bernstein.
2019. fair work: crowd work minimum wage withone line of code.
in proceedings of the aaai con-ference on human computation and crowdsourcing,pages 197–206, hilversum, the netherlands..zhao xiufeng.
2013. the conceptual integration modelof multimodal metaphor construction: a case studyof a political cartoon.
foreign languages research,5:1–8..3224fan-pei gloria yang, kailyn bradley, madiha huq,dai-lin wu, and daniel c krawczyk.
2013. contex-tual effects on conceptual blending in metaphors: anevent-related potential study.
journal of neurolin-guistics, 26(2):312–326..keren ye, narges honarvar nazari, james hahn, za-eem hussain, mingda zhang, and adriana ko-vashka.
2019. interpreting the rhetoric of visual ad-vertisements.
ieee transactions on pattern analy-sis and machine intelligence, pages 1–16..amir zadeh, minghai chen, soujanya poria, erik cam-bria, and louis-philippe morency.
2017. tensorfusion network for multimodal sentiment analysis.
in proceedings of the 2017 conference on empiri-cal methods in natural language processing, pages1103–1114, copenhagen, denmark..omnia zayed, john p mccrae, and paul buitelaar.
2019. crowd-sourcing a high-quality dataset formetaphor identiﬁcation in tweets.
in proceedings ofthe 2nd conference on language, data and knowl-edge, pages 1–17, leipzig, germany..jingzhao zhang, tianxing he, suvrit sra, and ali jad-babaie.
2019. why gradient clipping acceleratestraining: a theoretical justiﬁcation for adaptivity.
arxiv e-prints, page arxiv:1905.11881..3225