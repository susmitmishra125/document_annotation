erica: improving entity and relation understanding forpre-trained language models via contrastive learning.
yujia qin♣♠♦, yankai lin♦, ryuichi takanobu♣♦, zhiyuan liu♣∗, peng li♦, heng ji♠∗,minlie huang♣, maosong sun♣, jie zhou♦♣department of computer science and technology, tsinghua university, beijing, china♠university of illinois at urbana-champaign♦pattern recognition center, wechat ai, tencent inc.yujiaqin16@gmail.com.
abstract.
pre-trained language models (plms) haveshown superior performance on various down-stream natural language processing (nlp)tasks.
however, conventional pre-training ob-jectives do not explicitly model relational factsin text, which are crucial for textual under-standing.
to address this issue, we propose anovel contrastive learning framework ericato obtain a deep understanding of the entitiesand their relations in text.
speciﬁcally, we de-ﬁne two novel pre-training tasks to better un-derstand entities and relations: (1) the entitydiscrimination task to distinguish which tailentity can be inferred by the given head en-tity and relation; (2) the relation discriminationtask to distinguish whether two relations areclose or not semantically, which involves com-plex relational reasoning.
experimental resultsdemonstrate that erica can improve typicalplms (bert and roberta) on several lan-guage understanding tasks, including relationextraction, entity typing and question answer-ing, especially under low-resource settings.1.
1.introduction.
pre-trained language models (plms) (devlinet al., 2018; yang et al., 2019; liu et al., 2019) haveshown superior performance on various naturallanguage processing (nlp) tasks such as text clas-siﬁcation (wang et al., 2018), named entity recog-nition (sang and de meulder, 2003), and questionanswering (talmor and berant, 2019).
beneﬁtingfrom designing various effective self-supervisedlearning objectives, such as masked language mod-eling (devlin et al., 2018), plms can effectivelycapture the syntax and semantics in text to gener-ate informative language representations for down-stream nlp tasks..∗corresponding author.
1our code and data are publicly available at https://.
github.com/thunlp/erica..figure 1: an example for a document “culiacán”, inwhich all entities are underlined.
we show entities andtheir relations as a relational graph, and highlight theimportant entities and relations to ﬁnd out “where isguadalajara”..however, conventional pre-training objectivesdo not explicitly model relational facts, which fre-quently distribute in text and are crucial for under-standing the whole text.
to address this issue, somerecent studies attempt to improve plms to betterunderstand relations between entities (soares et al.,2019; peng et al., 2020).
however, they mainlyfocus on within-sentence relations in isolation, ig-noring the understanding of entities, and the inter-actions among multiple entities at document level,whose relation understanding involves complex rea-soning patterns.
according to the statistics on ahuman-annotated corpus sampled from wikipediadocuments by yao et al.
(2019), at least 40.7% re-lational facts require to be extracted from multiplesentences.
speciﬁcally, we show an example in fig-ure 1, to understand that “guadalajara is located inmexico”, we need to consider the following cluesjointly: (i) “mexico” is the country of “culiacán”from sentence 1; (ii) “culiacán” is a rail junction lo-.
proceedingsofthe59thannualmeetingoftheassociationforcomputationallinguisticsandthe11thinternationaljointconferenceonnaturallanguageprocessing,pages3350–3363august1–6,2021.©2021associationforcomputationallinguistics3350[1]culiacánisacityinnorthwesternmexico.[2]culiacánisthecapitalofthestateofsinaloa.[3]culiacánisalsotheseatofculiacánmunicipality.[4]ithadanurbanpopulationof785,800in2015while905,660livedintheentiremunicipality.[5]whileculiacánmunicipalityhasatotalareaof4,758k!!,culiacánitselfisconsiderablysmaller,measuringonly.[6]culiacánisarailjunctionandislocatedonthepanamericanhighwaythatrunssouthtoguadalajaraandmexicocity.
[7]culiacánisconnectedtothenorthwithlosmochis,andtothesouthwithmazatlán,tepic.culiacánq: where is guadalajara?culiacánmexicopanamericanhighwaycity ofsouth tolocate ona: mexico.culiacánmunicipalitysinaloaguadalajaramexico citylos mochiscated on “panamerican highway” from sentence 6;(iii) “panamerican highway” connects to “guadala-jara” from sentence 6. from the example, we cansee that there are two main challenges to capturethe in-text relational facts:.
1. to understand an entity, we should considerits relations to other entities comprehensively.
inthe example, the entity “culiacán”, occurring insentence 1, 2, 3, 5, 6 and 7, plays an importantrole in ﬁnding out the answer.
to understand “culi-acán”, we should consider all its connected entitiesand diverse relations among them..2. to understand a relation, we should considerthe complex reasoning patterns in text.
for exam-ple, to understand the complex inference chain inthe example, we need to perform multi-hop reason-ing, i.e., inferring that “panamerican highway” islocated in “mexico” through the ﬁrst two clues..in this paper, we propose erica, a novel frame-work to improve plms’ capability of entity andrelation understanding via contrastive learning,aiming to better capture in-text relational facts byconsidering the interactions among entities and re-lations comprehensively.
speciﬁcally, we deﬁnetwo novel pre-training tasks: (1) the entity discrim-ination task to distinguish which tail entity canbe inferred by the given head entity and relation.
it improves the understanding of each entity viaconsidering its relations to other entities in text;(2) the relation discrimination task to distinguishwhether two relations are close or not semantically.
through constructing entity pairs with document-level distant supervision, it takes complex relationalreasoning chains into consideration in an implicitway and thus improves relation understanding..we conduct experiments on a suite of languageunderstanding tasks, including relation extraction,entity typing and question answering.
the experi-mental results show that erica improves the per-formance of typical plms (bert and roberta)and outperforms baselines, especially under low-resource settings, which demonstrates that ericaeffectively improves plms’ entity and relation un-derstanding and captures the in-text relational facts..2 related work.
dai and le (2015) and howard and ruder (2018)propose to pre-train universal language representa-tions on unlabeled text, and perform task-speciﬁcﬁne-tuning.
with the advance of computing power,plms such as openai gpt (radford et al., 2018),.
bert (devlin et al., 2018) and xlnet (yang et al.,2019) based on deep transformer (vaswani et al.,2017) architecture demonstrate their superiority invarious downstream nlp tasks.
since then, nu-merous plm extensions have been proposed tofurther explore the impacts of various model ar-chitectures (song et al., 2019; raffel et al., 2020),larger model size (raffel et al., 2020; lan et al.,2020; fedus et al., 2021), more pre-training cor-pora (liu et al., 2019), etc., to obtain better generallanguage understanding ability.
although achiev-ing great success, these plms usually regard wordsas basic units in textual understanding, ignoring theinformative entities and their relations, which arecrucial for understanding the whole text..to improve the entity and relation understand-ing of plms, a typical line of work is knowledge-guided plm, which incorporates external knowl-edge such as knowledge graphs (kgs) into plmsto enhance the entity and relation understanding.
some enforce plms to memorize informationabout real-world entities and propose novel pre-training objectives (xiong et al., 2019; wang et al.,2019; sun et al., 2020; yamada et al., 2020).
oth-ers modify the internal structures of plms to fuseboth textual and kg’s information (zhang et al.,2019; peters et al., 2019; wang et al., 2020; heet al., 2020).
although knowledge-guided plmsintroduce extra factual knowledge in kgs, thesemethods ignore the intrinsic relational facts in text,making it hard to understand out-of-kg entities orknowledge in downstream tasks, let alone the errorsand incompleteness of kgs.
this veriﬁes the ne-cessity of teaching plms to understand relationalfacts from contexts..another line of work is to directly model entitiesor relations in text in pre-training stage to breakthe limitations of individual token representations.
some focus on obtaining better span representa-tions, including entity mentions, via span-basedpre-training (sun et al., 2019; joshi et al., 2020;kong et al., 2020; ye et al., 2020).
others learnto extract relation-aware semantics from text bycomparing the sentences that share the same entitypair or distantly supervised relation in kgs (soareset al., 2019; peng et al., 2020).
however, thesemethods only consider either individual entities orwithin-sentence relations, which limits the perfor-mance in dealing with multiple entities and rela-tions at document level.
in contrast, our ericaconsiders the interactions among multiple entities.
3351{h1, h2, ..., h|di|}, then we apply mean pooling op-eration over the consecutive tokens that mention eijto obtain local entity representations.
note eij mayappear multiple times in di, the k-th occurrence ofeij, which contains the tokens from index nkstart tonkend, is represented as:.
mk.
eij = meanpool(hnk.
, ..., hnk.
)..end.
start.
(1).
to aggregate all information about eij, we aver-.
age2 all representations of each occurrence mkeijas the global entity representation eij.
follow-ing soares et al.
(2019), we concatenate the ﬁnalrepresentations of two entities eij1 and eij2 as theirrelation representation, i.e., ri.
= [eij1; eij2]..j1j2.
3.3 entity discrimination task.
entity discrimination (ed) task aims at inferringthe tail entity in a document given a head entityand a relation.
by distinguishing the ground-truthtail entity from other entities in the text, it teachesplms to understand an entity via considering itsrelations with other entities..as shown in figure 2, in practice, we ﬁrstsample a tuple tijk, eik) from t +,jk = (di, eij, riplms are then asked to distinguish the ground-truth tail entity eik from other entities in thedocument di.
to inform plms of which headentity and relation to be conditioned on, weconcatenate the relation name of rijk, the men-tion of head entity eij and a separation token[sep] in front of di, i.e., d∗i =“relation_nameentity_mention[sep] di”3.
the goal of entitydiscrimination task is equivalent to maximizing theposterior p(eik|eij, rijk) = softmax(f (eik)) (f (·)indicates an entity classiﬁer).
however, we empiri-cally ﬁnd directly optimizing the posterior cannotwell consider the relations among entities.
hence,we borrow the idea of contrastive learning (hadsellet al., 2006) and push the representations of pos-itive pair (eij, eik) closer than negative pairs, theloss function of ed task can be formulated as:.
led = −.
(cid:88).
log.
tijk∈t +.
exp(cos(eij, eik)/τ )|ei|(cid:80)l=1, l(cid:54)=j.
exp(cos(eij, eil)/τ ).
,.
(2).
2although weighted summation by attention mechanismis an alternative, the speciﬁc method of entity informationaggregation is not our main concern..3here we encode the modiﬁed document d∗.
i to obtain theentity representations.
the newly added entity_mention isnot considered for head entity representation..figure 2: an example of entity discrimination task.
for an entity pair with its distantly supervised relationin text, the ed task requires the ground-truth tail entityto be closer to the head entity than other entities..and relations comprehensively, achieving a betterunderstanding of in-text relational facts..3 methodology.
in this section, we introduce the details of erica.
we ﬁrst describe the notations and how to represententities and relations in documents.
then we detailthe two novel pre-training tasks: entity discrimi-nation (ed) task and relation discrimination (rd)task, followed by the overall training objective..3.1 notations.
erica is trained on a large-scale unlabeled cor-pus leveraging the distant supervision from an ex-ternal kg k. formally, let d = {di}|d|i=1 be abatch of documents and ei = {eij}|ei|j=1 be allnamed entities in di, where eij is the j-th entityin di.
for each document di, we enumerate allentity pairs (eij, eik) and link them to their corre-sponding relation rijk in k (if possible) and obtainjk = (di, eij, ria tuple set ti = {tijk, eik)|j (cid:54)= k}.
we assign no_relation to those entity pairs with-out relation annotation in k. then we obtain the(cid:83) ... (cid:83) t|d| for thisoverall tuple set t = t1batch.
the positive tuple set t + is constructedby removing all tuples with no_relation fromt .
beneﬁting from document-level distant su-pervision, t + includes both intra-sentence (rel-atively simple cases) and inter-sentence entity pairs(hard cases), whose relation understanding involvescross-sentence, multi-hop, or coreferential reason-(cid:83) t +ing, i.e., t + = t +.
(cid:83) t2.
cross..single.
3.2 entity & relation representation.
for each document di, we ﬁrst use a plm toencode it and obtain a series of hidden states.
3352where n is a hyper-parameter.
we ensure tb issampled in z and construct n − 1 negative exam-ples by sampling tc (ra (cid:54)= rc) from t , insteadof t +5.
by additionally considering the last threeterms of lrd in eq.3, which require the model todistinguish complex inter-sentence relations withother relations in the text, our model could have bet-ter coverage and generality of the reasoning chains.
plms are trained to perform reasoning in an im-plicit way to understand those “hard” inter-sentencecases..3.5 overall objective.
now we present the overall training objectiveof erica.
to avoid catastrophic forgetting (mc-closkey and cohen, 1989) of general languageunderstanding ability, we train masked languagemodeling task (lmlm) together with ed and rdtasks.
hence, the overall learning objective is for-mulated as follows:.
l = led + lrd + lmlm..(4).
it is worth mentioning that we also try to maskentities as suggested by soares et al.
(2019) andpeng et al.
(2020), aiming to avoid simply relearn-ing an entity linking system.
however, we do notobserve performance gain by such a masking strat-egy.
we conjecture that in our document-level set-ting, it is hard for plms to overﬁt on memoriz-ing entity mentions due to the better coverage andgenerality of document-level distant supervision.
besides, masking entities creates a gap betweenpre-training and ﬁne-tuning, which may be a short-coming of previous relation-enhanced plms..4 experiments.
in this section, we ﬁrst describe how we constructthe distantly supervised dataset and pre-trainingdetails for erica.
then we introduce the experi-ments we conduct on several language understand-ing tasks, including relation extraction (re), en-tity typing (et) and question answering (qa).
we test erica on two typical plms, includingbert and roberta (denoted as ericabert andericaroberta)6. we leave the training details.
5in experiments, we ﬁnd introducing no_relation entitypairs as negative samples further improves the performanceand the reason is that increasing the diversity of training entitypairs is beneﬁcial to plms..6since our main focus is to demonstrate the superiorityof erica in improving plms to capture relational facts andadvance further research explorations, we choose base models.
figure 3: an example of relation discrimination task.
for entity pairs belonging to the same relations, the rdtask requires their relation representations to be closer..where cos(·, ·) denotes the cosine similarity be-tween two entity representations and τ (temper-ature) is a hyper-parameter..3.4 relation discrimination task.
relation discrimination (rd) task aims at distin-guishing whether two relations are close or notsemantically.
compared with existing relation-enhanced plms, we employ document-level ratherthan sentence-level distant supervision to furthermake plms comprehend the complex reasoningchains in real-world scenarios and thus improveplms’ relation understanding..as depicted in figure 3, we train the text-basedrelation representations of the entity pairs that sharethe same relations to be closer in the semanticin practice, we linearly4 sample a tuplespace.
pair ta = (da, ea1, ra, ea2) and tb = (db, eb1,rb, eb2) from t +cross), wheresra = rb.
using the method mentioned in sec.
3.2,we obtain the positive relation representations rtaand rtb for ta and tb.
to discriminate positiveexamples from negative ones, similarly, we adoptcontrastive learning and deﬁne the loss function ofrd task as follows:.
single) or t +c.(t +.
(t +.
lt1,t2.
rd = −.
(cid:88).
log.
exp(cos(rta, rtb )/τ )z.,.
ta∈t1,tb∈t2.
n(cid:88).
z =.
exp(cos(rta, rtc )/τ ),.
lrd = lt +.
tc ∈t /{ta}s ,t +srd.
s ,t ++ lt +crd.
c ,t ++ lt +srd.
c ,t ++ lt +crd.
,.
(3).
4the sampling rate of each relation is proportional to its.
total number in the current batch..3353document 1document 2document 3document 3single-sentencecross-sentencesingle-sentencecross-sentencefounded by…since1773,whentheroyalswedishoperawasfoundedbygustaviiiofsweden……gatesisanamericanbusinessmagnate,softwaredeveloper,andphilanthropist…helefthisboardpositionsatmicrosoft……samarindaisthecapitalofeastkalimantan,indonesia,ontheislandofborneo…samarindaisknownforitstraditionalfoodamplang,aswellastheclothsarungsamarinda……samarindaisthecapitalofeastkalimantan,indonesia,ontheislandofborneo…samarindaisknownforitstraditionalfoodamplang,aswellastheclothsarungsamarinda…founded bycapital ofcountrypre-trained language modelfor downstream tasks and experiments on gluebenchmark (wang et al., 2018) in the appendix..4.1 distantly supervised dataset.
construction.
following yao et al.
(2019), we construct our pre-training dataset leveraging distant supervision fromthe english wikipedia and wikidata.
first, weuse spacy7 to perform named entity recognition,and then link these entity mentions as well aswikipedia’s mentions with hyper-links to wikidataitems, thus we obtain the wikidata id for each en-tity.
the relations between different entities areannotated distantly by querying wikidata.
we keepthe documents containing at least 128 words, 4entities and 4 relational triples.
in addition, weignore those entity pairs appearing in the test setsof re and qa tasks to avoid test set leakage.
inthe end, we collect 1, 000, 000 documents (about1g storage) in total with more than 4, 000 relationsannotated distantly.
on average, each documentcontains 186.9 tokens, 12.9 entities and 7.2 rela-tional triples, an entity appears 1.3 times per docu-ment.
based on the human evaluation on a randomsample of the dataset, we ﬁnd that it achieves an f1score of 84.7% for named entity recognition, andan f1 score of 25.4% for relation extraction..4.2 pre-training details.
we initialize ericabert and ericaroberta withbert-base-uncased and roberta-base checkpointsreleased by google8 and huggingface9.
we adoptadamw (loshchilov and hutter, 2017) as the opti-mizer, warm up the learning rate for the ﬁrst 20%steps and then linearly decay it.
we set the learningrate to 3 × 10−5, weight decay to 1 × 10−5, batchsize to 2, 048 and temperature τ to 5 × 10−2.
forlrd, we randomly select up to 64 negative sam-ples per document.
we train both models with 8nvidia tesla p40 gpus for 2, 500 steps..4.3 relation extraction.
relation extraction aims to extract the relation be-tween two recognized entities from a pre-deﬁnedrelation set.
we conduct experiments on bothdocument-level and sentence-level re.
we test.
for experiments..7https://spacy.io/8https://github.com/google-research/bert9https://github.com/huggingface/.
transformers.
size.
metrics.
cnnbilstm.
berthinbertcorefbertspanberterniemtbcpericabert.
1%.
10%.
100%.
f1.
igf1 f1.
igf1 f1.
igf1.
--.
--.
42.3 40.351.1 50.3.
-.
30.4 28.9 47.1 44.9 56.8 54.555.6 53.7-32.8 31.2 46.0 43.7 57.0 54.532.2 30.4 46.4 44.5 57.3 55.026.7 25.5 46.7 44.2 56.6 54.229.0 27.6 46.1 44.1 56.9 54.330.3 28.7 44.8 42.6 55.2 52.737.8 36.0 50.8 48.3 58.2 55.9.roberta35.3 33.5 48.0 45.9 58.5 56.1ericaroberta 40.1 38.0 50.3 48.3 59.0 56.6.table 1: results on document-level re (docred).
wereport micro f1 (f1) and micro ignore f1 (igf1) on testset.
igf1 metric ignores the relational facts shared bythe train and dev/test sets..dataset.
size.
bertmtbcpericabert.
tacred.
semeval.
1% 10% 100% 1% 10% 100%.
36.0 58.535.7 58.837.1 60.636.5 59.7.
68.168.268.168.5.
69.769.8.
43.6 79.344.2 79.240.3 80.047.9 80.1.
46.0 80.346.3 80.4.
88.188.288.588.0.
88.889.2.
26.3 61.2robertaericaroberta 40.0 61.9.table 2: results (test f1) on sentence-level re (ta-cred and semeval-2010 task8) on three splits (1%,10% and 100%)..three partitions of the training set (1%, 10% and100%) and report results on test sets..document-level re for document-level re, wechoose docred (yao et al., 2019), which requiresreading multiple sentences in a document and syn-thesizing all the information to identify the relationbetween two entities.
we encode all entities in thesame way as in pre-training phase.
the relation rep-resentations are obtained by adding a bilinear layeron top of two entity representations.
we choose thefollowing baselines: (1) cnn (zeng et al., 2014),bilstm (hochreiter and schmidhuber, 1997),bert (devlin et al., 2018) and roberta (liuet al., 2019), which are widely used as text encodersfor relation extraction tasks; (2) hinbert (tanget al., 2020) which employs a hierarchical infer-ence network to leverage the abundant informationfrom different sources; (3) corefbert (ye et al.,2020) which proposes a pre-training method to helpbert capture the coreferential relations in context;(4) spanbert (joshi et al., 2020) which masks.
3354metrics.
macro f1 micro f1.
bertmtbcpernieericabert.
robertaericaroberta.
75.5076.3776.2776.5177.85.
79.2480.77.
72.6872.9472.4873.3974.71.
76.3877.04.table 3: results on entity typing (figer).
we reportmacro f1 and micro f1 on the test set..and predicts contiguous random spans instead ofrandom tokens; (5) ernie (zhang et al., 2019)which incorporates kg information into bert toenhance entity representations; (6) mtb (soareset al., 2019) and cp (peng et al., 2020) which in-troduce sentence-level relation contrastive learningfor bert via distant supervision.
for fair compari-son, we pre-train these baselines on our constructedpre-training data10 based on the implementation re-leased by peng et al.
(2020)11. from the resultsshown in table 1, we can see that: (1) ericaoutperforms all baselines signiﬁcantly on each su-pervised data size, which demonstrates that er-ica could better understand the relations amongentities in the document via implicitly consideringtheir complex reasoning patterns in the pre-training;(2) both mtb and cp achieve worse results thanbert, which means sentence-level pre-training,lacking consideration for complex reasoning pat-terns, hurts plm’s performance on document-levelre tasks to some extent; (3) erica outperformsbaselines by a larger margin on smaller trainingsets, which means erica has gained pretty gooddocument-level relation reasoning ability in con-trastive learning, and thus obtains improvementsmore extensively under low-resource settings..sentence-level re for sentence-level re, wechoose two widely used datasets: tacred (zhanget al., 2017) and semeval-2010 task 8 (hendrickxet al., 2019).
we insert extra marker tokens to in-dicate the head and tail entities in each sentence.
for baselines, we compare erica with bert,roberta, mtb and cp.
from the results shownin table 2, we observe that erica achieves almostcomparable results on sentence-level re tasks withcp, which means document-level pre-training in.
10in practice, documents are split into sentences and we.
only keep within-sentence entity pairs.
11https://github.com/thunlp/.
re-context-or-names.
setting.
size.
fastqabidaf.
standard.
masked.
1% 10% 100% 1% 10% 100%.
--.
bertcorefbertspanbertmtbcpericabert.
35.8 53.738.1 54.433.1 56.436.6 51.734.6 50.446.5 57.8.roberta37.3 57.4ericaroberta 47.4 58.8.
27.249.7.
69.568.870.768.467.469.7.
70.971.2.
--.
37.9 53.139.0 53.534.0 55.436.2 50.934.1 47.140.2 58.1.
41.2 58.746.8 63.4.
38.059.8.
73.170.773.271.769.473.9.
75.576.6.table 4: results (accuracy) on the dev set of wikihop.
we test both the standard and masked settings on threesplits (1%, 10% and 100%)..setting.
size.
bertmtbcpericabert.
squad.
triviaqa.
naturalqa.
10% 100% 10% 100% 10% 100%.
79.763.569.081.8.
88.987.187.188.9.
90.590.4.
60.852.052.963.5.
63.663.6.
70.767.868.171.9.
72.072.1.
68.461.263.370.2.
71.873.7.
78.476.777.379.1.
80.080.5.roberta82.9ericaroberta 85.0.table 5: results (f1) on extractive qa (squad, triv-iaqa and naturalqa) on two splits (10% and 100%).
results on 1% split are left in the appendix..erica does not impair plms’ performance onsentence-level relation understanding..4.4 entity typing.
entity typing aims at classifying entity men-tions into pre-deﬁned entity types.
we choosefiger (ling et al., 2015), which is a sentence-level entity typing dataset labeled with distantsupervision.
bert, roberta, mtb, cp andernie are chosen as baselines.
from the resultslisted in table 3, we observe that, erica outper-forms all baselines, which demonstrates that er-ica could better represent entities and distinguishthem in text via both entity-level and relation-levelcontrastive learning..4.5 question answering.
question answering aims to extract a speciﬁc an-swer span in text given a question.
we conductexperiments on both multi-choice and extractiveqa.
we test multiple partitions of the training set..multi-choice qa for multi-choice qa, wechoose wikihop (welbl et al., 2018), which re-quires models to answer speciﬁc properties of an.
3355entity after reading multiple documents and con-it has both stan-ducting multi-hop reasoning.
dard and masked settings, where the latter settingmasks all entities with random ids to avoid in-formation leakage.
we ﬁrst concatenate the ques-tion and documents into a long sequence, thenwe ﬁnd all the occurrences of an entity in thedocuments, encode them into hidden representa-tions and obtain the global entity representationby applying mean pooling on these hidden rep-resentations.
finally, we use a classiﬁer on topof the entity representation for prediction.
wechoose the following baselines: (1) fastqa (weis-senborn et al., 2017) and bidaf (seo et al., 2016),which are widely used question answering systems;(2) bert, roberta, corefbert, spanbert,mtb and cp, which are introduced in previoussections.
from the results listed in table 4, we ob-serve that erica outperforms baselines in both set-tings, indicating that erica can better understandentities and their relations in the documents andextract the true answer according to queries.
thesigniﬁcant improvements in the masked setting alsoindicate that erica can better perform multi-hopreasoning to synthesize and analyze informationfrom contexts, instead of relying on entity mention“shortcuts” (jiang and bansal, 2019)..extractive qa for extractive qa, we adoptthree widely-used datasets: squad (rajpurkaret al., 2016), triviaqa (joshi et al., 2017) and natu-ralqa (kwiatkowski et al., 2019) in mrqa (fischet al., 2019) to evaluate erica in various domains.
since mrqa does not provide the test set for eachdataset, we randomly split the original dev set intotwo halves and obtain the new dev/test set.
we fol-low the qa setting of bert (devlin et al., 2018):we concatenate the given question and passage intoone long sequence, encode the sequence by plmsand adopt two classiﬁers to predict the start and endindex of the answer.
we choose bert, roberta,mtb and cp as baselines.
from the results listedin table 5, we observe that erica outperformsall baselines, indicating that through the enhance-ment of entity and relation understanding, ericais more capable of capturing in-text relational factsand synthesizing information of entities.
this abil-ity further improves plms for question answering..5 analysis.
in this section, we ﬁrst conduct a suite of ablationstudies to explore how led and lrd contribute to.
dataset.
bert.
+.
-nsp-nsp+led-nsp+ltrd+-nsp+ltrd-nsp+lrd.
c ,t.+c.s ,t.+s.ericabert.
docred figer wikihop.
44.945.247.646.447.348.048.3.
72.772.673.872.673.574.074.7.
53.153.659.852.251.252.058.1.table 6: ablation study.
we report test igf1 on do-cred (10%), test micro f1 on figer and dev accu-racy on the masked setting of wikihop (10%)..erica.
then we give a thorough analysis on howpre-training data’s domain / size and methods forentity encoding impact the performance.
lastly,we visualize the entity and relation embeddingslearned by erica..5.1 ablation study.
to demonstrate that the superior performance oferica is not owing to its longer pretraining (2500steps) on masked language modeling, we includea baseline by optimizing lmlm only (removingthe next sentence prediction (-nsp) loss (devlinet al., 2018)).
in addition, to explore how led andlrd impact the performance, we keep only one ofthese two losses and compare the results.
lastly,to evaluate how intra-sentence and inter-sentenceentity pairs contribute to rd task, we comparethe performances of only sampling intra-sentences ,t +entity pairs (lt +s) or inter-sentence entity pairsrd(lt +c ,t +), and sampling both of them (lrd) duringcrdpre-training.
we conduct experiments on docred,wikihop (masked version) and figer.
for do-cred and wikihop, we show the results on 10%splits and the full results are left in the appendix..from the results shown in table 6, we can seethat: (1) extra pretraining (-nsp) only contributes alittle to the overall improvement.
(2) for docredand figer, either led or lrd is beneﬁcial, andcombining them further improves the performance;for wikihop, led dominates the improvementwhile lrd hurts the performance slightly, this ispossibly because question answering more resem-bles the tail entity discrimination process, whilethe relation discrimination process may have con-ﬂicts with it.
(3) for lrd, both intra-sentence andinter-sentence entity pairs contribute, which demon-strates that incorporating both of them is necessaryfor plms to understand relations between entitiesin text comprehensively.
we also found empiri-.
3356size.
1% 10% 100%.
bertericabertericadocred.
bert.
28.936.036.3.
44.948.348.6.
54.555.955.9.table 7: effects of pre-training data’s entity distribu-tion shifting.
we report test igf1 on docred..figure 4: impacts of relation distribution shifting.
xaxis denotes different ratios of relations, y axis denotestest igf1 on different partitions of docred..cally that when these two auxiliary objectives areonly added into the ﬁne-tuning stage, the modeldoes not have performance gain.
the reason is thatthe size and diversity of entities and relations indownstream training data are limited.
instead, pre-training with distant supervision on a large corpusprovides a solution for increasing the diversity andquantity of training examples..5.2 effects of domain shifting.
we investigate two domain shifting factors: entitydistribution and relation distribution, to explorehow they impact erica’s performance..entity distribution shifting the entities in su-pervised datasets of docred are recognized byhuman annotators while our pre-training data isprocessed by spacy.
hence there may exist an en-tity distribution gap between pre-training and ﬁne-tuning.
to study the impacts of entity distributionshifting, we ﬁne-tune a bert model on trainingset of docred for ner tagging and re-tag enti-ties in our pre-training dataset.
then we pre-trainerica on the newly-labeled training corpus (de-noted as ericadocred).
from the results shown intable 7, we observe that it performs better than theoriginal erica, indicating that pre-training on adataset that shares similar entity distributions withdownstream tasks is beneﬁcial..bert.
relation distribution shifting our pre-trainingdata contains over 4, 000 wikidata relations.
toinvestigate whether training on a more diverse rela-tion domain beneﬁts erica, we train it with thepre-training corpus that randomly keeps only 30%,50% and 70% the original relations, and compare.
figure 5: impacts of pre-training data’s size.
x axisdenotes different ratios of pre-training data, y axis de-notes test igf1 on different partitions of docred..size.
metrics.
mean poolbertericabertericadocred.
bert.
entity markerbertericabertericadocred.
bert.
1%.
10%.
100%.
f1.
igf1 f1.
igf1 f1.
igf1.
30.4 28.9 47.1 44.9 56.8 54.537.8 36.0 50.8 48.3 58.2 55.938.5 36.3 51.0 48.6 58.2 55.9.
23.0 21.8 46.5 44.3 58.0 55.634.9 33.0 50.2 48.0 59.9 57.636.9 34.8 52.5 50.3 60.8 58.4.table 8: results (igf1) on how entity encoding strat-egy inﬂuences erica’s performance on docred.
wealso show the impacts of entity distribution shifting(ericadocred) as is mentioned in thebertmain paper..and ericadocred.
bert.
their performances.
from the results in figure 4,we observe that the performance of erica im-proves constantly as the diversity of relation do-main increases, which reveals the importance ofusing diverse training data on relation-related tasks.
through detailed analysis, we further ﬁnd that er-ica is less competent at handling unseen relationsin the corpus.
this may result from the construc-tion of our pre-training dataset: all the relations areannotated distantly through an existing kg witha pre-deﬁned relation set.
it would be promisingto introduce more diverse relation domains duringdata preparation in future..5.3 effects of pre-training data’s size.
to explore the effects of pre-training data’s size, wetrain erica on 10%, 30%, 50% and 70% of theoriginal pre-training dataset, respectively.
we re-port the results in figure 5, from which we observethat with the scale of pre-training data becominglarger, erica is performing better..5.4 effects of methods for entity encoding.
for all the experiments mentioned above, we en-code each occurrence of an entity by mean poolingover all its tokens in both pre-training and down-stream tasks.
ideally, erica should have consis-.
33570%30%50%70%100%1% docred30323436 0%30%50%70%100%10% docred45464748 0%30%50%70%100%100% docred54.555.055.5 0%10%30%50%70%100%1% docred30323436 0%10%30%50%70%100%10% docred45464748 0%10%30%50%70%100%100% docred54.555.055.5 tent improvements on other kinds of methods forentity encoding.
to demonstrate this, we try an-other entity encoding method mentioned by soareset al.
(2019) on three splits of docred (1%, 10%and 100%).
speciﬁcally, we insert a special starttoken [s] in front of an entity and an end token[e] after it.
the representation for this entity iscalculated by averaging the representations of allits start tokens in the document.
to help plmsdiscriminate different entities, we randomly assigndifferent marker pairs ([s1], [e1]; [s2], [e2], ...)for each entity in a document in both pre-trainingand downstream tasks12.
all occurrences of oneentity in a document share the same marker pair.
we show in table 8 that erica achieves consistentperformance improvements for both methods (de-noted as mean pool and entity marker), indicat-ing that erica is applicable to different methodsfor entity encoding.
speciﬁcally, entity markerachieves better performance when the scale of train-ing data is large while mean pool is more powerfulunder low-resource settings.
we also notice thattraining on a dataset that shares similar entity dis-tributions is more helpful for mean pool, whereericadocredachieves 60.8 (f1) and 58.4 (igf1)on 100% training data..bert.
5.5 embedding visualization.
in figure 6, we show the learned entity and re-lation embeddings of bert and ericabert ondocred’s dev set by t-distributed stochastic neigh-bor embedding (t-sne) (hinton and roweis, 2002).
we label points with different colors to representits corresponding category of entities or relations13in wikidata and only visualize the most frequent 10relations.
from the ﬁgure, we can see that jointlytraining lmlm with led and lrd leads to a morecompact clustering of both entities and relationsbelonging to the same category.
in contrast, onlytraining lmlm exhibits random distribution.
thisveriﬁes that erica could better understand andrepresent both entities and relations in the text..12in practice, we randomly initialize 100 entity marker.
pairs..13(key, value) pairs for relations deﬁned in wikidata are:(p176, manufacturer); (p150, contains administrative territo-rial entity); (p17, country); (p131, located in the administra-tive territorial entity); (p175, performer); (p27, country ofcitizenship); (p569, date of birth); (p1001, applies to jurisdic-tion); (p57, director); (p179, part of the series)..figure 6:t-sne plots of learned entity and rela-tion embeddings on docred comparing bert andericabert..6 conclusions.
in this paper, we present erica, a general frame-work for plms to improve entity and relation un-derstanding via contrastive learning.
we demon-strate the effectiveness of our method on severallanguage understanding tasks, including relationextraction, entity typing and question answering.
the experimental results show that erica outper-forms all baselines, especially under low-resourcesettings, which means erica helps plms bettercapture the in-text relational facts and synthesizeinformation about entities and their relations..acknowledgments.
this work is supported by the national key re-search and development program of china (no.
2020aaa0106501) and beijing academy of arti-ﬁcial intelligence (baai).
this work is also sup-ported by the pattern recognition center, wechatai, tencent inc..references.
andrew m dai and quoc v le.
2015. semi-supervisedsequence learning.
in advances in neural informa-tion processing systems, pages 3079–3087..jacob devlin, ming-wei chang, kenton lee, andkristina toutanova.
2018. bert: pre-training ofdeep bidirectional transformers for language under-in proceedings of the 2019 conferencestanding.
of the north american chapter of the association.
3358bert: entity entitymiscorgperloctimenumerica-bert: entity entitytimemiscorglocpernumbert: relation relationp17p131p1001p27p150p175p179p57p176p569erica-bert: relation relationp176p150p17p131p175p27p569p1001p57p179for computational linguistics: human languagetechnologies, naacl-hlt 2019, minneapolis, mn,usa, june 2-7, 2019, volume 1 (long and short pa-pers), pages 4171–4186..markus eberts and adrian ulges.
2019. span-basedjoint entity and relation extraction with transformerpre-training.
corr, abs/1909.07755..william fedus, barret zoph, and noam shazeer.
2021.switch transformers: scaling to trillion parametermodels with simple and efﬁcient sparsity.
arxivpreprint arxiv:2101.03961..adam fisch, alon talmor, robin jia, minjoon seo, eu-nsol choi, and danqi chen.
2019. mrqa 2019shared task: evaluating generalization in readingin proceedings of the 2nd work-comprehension.
shop on machine reading for question answering,pages 1–13, hong kong, china.
association forcomputational linguistics..harsha gurulingappa, abdul mateen rajput, angusroberts, juliane fluck, martin hofmann-apitius,and luca toldo.
2012. development of a bench-mark corpus to support the automatic extraction ofdrug-related adverse effects from medical case re-ports.
journal of biomedical informatics, 45(5):885–892..raia hadsell, sumit chopra, and yann lecun.
2006.dimensionality reduction by learning an invariantmapping.
in 2006 ieee computer society confer-ence on computer vision and pattern recognition(cvpr’06), volume 2, pages 1735–1742.
ieee..bin he, di zhou, jinghui xiao, xin jiang, qun liu,nicholas jing yuan, and tong xu.
2020. bert-mk: integrating graph contextualized knowledgeinto pre-trained language models.
in findings of theassociation for computational linguistics: emnlp2020, pages 2281–2290, online.
association forcomputational linguistics..iris hendrickx, su nam kim, zornitsa kozareva,preslav nakov, diarmuid o séaghdha, sebastianpadó, marco pennacchiotti, lorenza romano, andsemeval-2010 task 8:stan szpakowicz.
2019.multi-way classiﬁcation of semantic relations be-in proceedings of thetween pairs of nominals.
workshop on semantic evaluations: recent achieve-ments and future directions (sew-2009), pages 94–99..geoffrey e hinton and sam roweis.
2002. stochas-tic neighbor embedding.
in advances in neural in-formation processing systems 15: 16th annual con-ference on neural information processing systems2002. proceedings of a meeting held september 12,2002, vancouver, british columbia, canada, vol-ume 15, pages 857–864..sepp hochreiter and jürgen schmidhuber.
1997.neural computation,.
long short-term memory.
9(8):1735–1780..jeremy howard and sebastian ruder.
2018. universallanguage model ﬁne-tuning for text classiﬁcation.
inproceedings of the 56th annual meeting of the as-sociation for computational linguistics (volume 1:long papers), pages 328–339, melbourne, australia.
association for computational linguistics..yichen jiang and mohit bansal.
2019. avoiding rea-soning shortcuts: adversarial evaluation, training,in pro-and model development for multi-hop qa.
ceedings of the 57th annual meeting of the associa-tion for computational linguistics, acl 2019, july28, 2019, florence, italy, pages 2726–2736.
associ-ation for computational linguistics..mandar joshi, danqi chen, yinhan liu, daniel s.weld, luke zettlemoyer, and omer levy.
2020.spanbert: improving pre-training by representingand predicting spans.
transactions of the associa-tion for computational linguistics, 8:64–77..mandar joshi, eunsol choi, daniel weld, and lukezettlemoyer.
2017. triviaqa: a large scale dis-tantly supervised challenge dataset for reading com-prehension.
in proceedings of the 55th annual meet-ing of the association for computational linguistics(volume 1: long papers), pages 1601–1611, van-couver, canada.
association for computational lin-guistics..diederik p kingma and jimmy ba.
2014. adam: ain 3rd inter-method for stochastic optimization.
national conference on learning representations,iclr 2015, san diego, ca, usa, may 7, 2015, con-ference track proceedings..lingpeng kong, cyprien de masson d’autume, lei yu,wang ling, zihang dai, and dani yogatama.
2020.a mutual information maximization perspective ofin proceedingslanguage representation learning.
of 8th international conference on learning repre-sentations, iclr 2020, virtual conference, april 26,2020, conference track proceedings..tom kwiatkowski, jennimaria palomaki, olivia red-ﬁeld, michael collins, ankur parikh, chris alberti,danielle epstein, illia polosukhin, jacob devlin,kenton lee, et al.
2019. natural questions: a bench-mark for question answering research.
transactionsof the association for computational linguistics,7:453–466..zhenzhong lan, mingda chen, sebastian goodman,kevin gimpel, piyush sharma, and radu soricut.
2020. albert: a lite bert for self-supervisedin proceed-learning of language representations.
ings of 8th international conference on learningrepresentations, iclr 2020, virtual conference,april 26, 2020, conference track proceedings..xiao ling, sameer singh, and daniel s weld.
2015.design challenges for entity linking.
transactionsof the association for computational linguistics,3:315–328..3359yinhan liu, myle ott, naman goyal, jingfei du, man-dar joshi, danqi chen, omer levy, mike lewis,luke zettlemoyer, and veselin stoyanov.
2019.roberta: a robustly optimized bert pretrainingapproach.
corr, abs/1907.11692..ilya loshchilov and frank hutter.
2017. decoupledweight decay regularization.
in proceedings of 7thinternational conference on learning representa-tions, iclr 2019..michael mccloskey and neal j cohen.
1989. catas-trophic interference in connectionist networks: thesequential learning problem.
in psychology of learn-ing and motivation, volume 24, pages 109–165.
el-sevier..hao peng, tianyu gao, xu han, yankai lin, pengli, zhiyuan liu, maosong sun, and jie zhou.
2020.learning from context or names?
an empirical studyon neural relation extraction.
in proceedings of the2020 conference on empirical methods in naturallanguage processing (emnlp), pages 3661–3672,online.
association for computational linguistics..matthew e peters, mark neumann, robert l logan iv,roy schwartz, vidur joshi, sameer singh, andnoah a smith.
2019. knowledge enhanced con-textual word representations.
in proceedings of the2019 conference on empirical methods in natu-ral language processing and the 9th internationaljoint conference on natural language processing(emnlp-ijcnlp).
association for computationallinguistics..alec radford, karthik narasimhan, tim salimans, andimproving language under-.
ilya sutskever.
2018.standing by generative pre-training..colin raffel, noam shazeer, adam roberts, katherinelee, sharan narang, michael matena, yanqi zhou,wei li, and peter j liu.
2020. exploring the limitsof transfer learning with a uniﬁed text-to-text trans-journal of machine learning research,former.
21:1–67..pranav rajpurkar, jian zhang, konstantin lopyrev, andpercy liang.
2016. squad: 100,000+ questions formachine comprehension of text.
in proceedings ofthe 2016 conference on empirical methods in natu-ral language processing, pages 2383–2392, austin,texas.
association for computational linguistics..dan roth and wen-tau yih.
2004. a linear program-ming formulation for global inference in natural lan-in proceedings of the eighth confer-guage tasks.
ence on computational natural language learn-ing (conll-2004) at hlt-naacl 2004, pages 1–8,boston, massachusetts, usa.
association for com-putational linguistics..erik f sang and fien de meulder.
2003..intro-duction to the conll-2003 shared task: language-independent named entity recognition.
in proceed-ings of the seventh conference on natural languagelearning at hlt-naacl 2003..minjoon seo, aniruddha kembhavi, ali farhadi, andhannaneh hajishirzi.
2016. bidirectional attentionﬂow for machine comprehension.
in proceedings of5th international conference on learning represen-tations, iclr 2017, toulon, france, april 24, 2017,con- ference track proceedings..livio baldini soares, nicholas fitzgerald, jeffreyling, and tom kwiatkowski.
2019. matching theblanks: distributional similarity for relation learn-in proceedings of the 57th annual meetinging.
of the association for computational linguistics,pages 2895–2905.
association for computationallinguistics..kaitao song, xu tan, tao qin, jianfeng lu, and tie-yan liu.
2019. mass: masked sequence to sequencein proceed-pre-training for language generation.
ings of international conference on machine learn-ing, pages 5926–5936.
pmlr..tianxiang sun, yunfan shao, xipeng qiu, qipeng guo,yaru hu, xuanjing huang, and zheng zhang.
2020.colake: contextualized language and knowledgethe 28th inter-in proceedings ofembedding.
national conference on computational linguistics,pages 3660–3670, barcelona, spain (online).
inter-national committee on computational linguistics..yu sun, shuohuan wang, yukun li, shikun feng, xuyichen, han zhang, xin tian, danxiang zhu, haotian, and hua wu.
2019. ernie: enhanced rep-resentation through knowledge integration.
arxivpreprint arxiv:1904.09223..alon talmor and jonathan berant.
2019. multiqa: anempirical investigation of generalization and trans-fer in reading comprehension.
in proceedings of the57th annual meeting of the association for compu-tational linguistics, pages 4911–4921.
associationfor computational linguistics..hengzhu tang, yanan cao, zhenyu zhang, jiangxiacao, fang fang, shi wang, and pengfei yin.
2020.hin: hierarchical inference network for document-level relation extraction.
in advances in knowledgediscovery and data mining-24th paciﬁc-asia con-ference, pakdd 2020, singapore, may 11, 2020,proceedings, part i, volume 12084 of lecture notesin computer science, pages 197–209.
springer..ashish vaswani, noam shazeer, niki parmar, jakobuszkoreit, llion jones, aidan n gomez, łukaszkaiser, and illia polosukhin.
2017. attention is allyou need.
in advances in neural information pro-cessing systems 30: annual conference on neuralinformation processing systems 2017, 4 december2017, long beach, ca, usa, pages 5998–6008..alex wang, amanpreet singh, julian michael, felixhill, omer levy, and samuel r bowman.
2018.glue: a multi-task benchmark and analysis plat-in pro-form for natural language understanding.
ceedings ofthe 2018 emnlp workshop black-boxnlp: analyzing and interpreting neural net-.
3360in proceedings of the 2020 conference on empiricalmethods in natural language processing (emnlp),pages 7170–7186.
association for computationallinguistics..daojian zeng, kang liu, siwei lai, guangyou zhou,and jun zhao.
2014. relation classiﬁcation via con-volutional deep neural network.
in proceedings ofcoling 2014, the 25th international conferenceon computational linguistics: technical papers,pages 2335–2344.
dublin city university and asso-ciation for computational linguistics..yuhao zhang, victor zhong, danqi chen, gabor an-geli, and christopher d manning.
2017. position-aware attention and supervised data improve slot ﬁll-ing.
in proceedings of the 2017 conference on em-pirical methods in natural language processing,pages 35–45.
association for computational lin-guistics..zhengyan zhang, xu han, zhiyuan liu, xin jiang,maosong sun, and qun liu.
2019. ernie: en-hanced language representation with informative en-in proceedings of the 57th annual meet-tities.
ing of the association for computational linguis-tics, pages 1441–1451.
association for computa-tional linguistics..works for nlp1.
association for computational lin-guistics..ruize wang, duyu tang, nan duan, zhongyu wei,xuanjing huang, cuihong cao, daxin jiang, mingzhou, et al.
2020. k-adapter:infusing knowl-edge into pre-trained models with adapters.
arxivpreprint arxiv:2002.01808..xiaozhi wang, tianyu gao, zhaocheng zhu, zhiyuanliu, juanzi li, and jian tang.
2019. kepler: auniﬁed model for knowledge embedding and pre-trained language representation.
transactions of theassociation for computational linguistics..dirk weissenborn, georg wiese, and laura seiffe.
2017. making neural qa as simple as possiblein proceedings of the 21st con-but not simpler.
ference on computational natural language learn-ing (conll 2017), pages 271–280.
association forcomputational linguistics..johannes welbl, pontus stenetorp, and sebastianriedel.
2018. constructing datasets for multi-hopreading comprehension across documents.
transac-tions of the association for computational linguis-tics, 6:287–302..wenhan xiong, jingfei du, william yang wang, andveselin stoyanov.
2019. pretrained encyclopedia:weakly supervised knowledge-pretrained languagemodel.
in proceedings of 8th international confer-ence on learning representations, iclr 2020, vir-tual conference, april 26, 2020, conference trackproceedings..ikuya yamada, akari asai, hiroyuki shindo, hideakitakeda, and yuji matsumoto.
2020. luke: deepcontextualized entity representations with entity-in proceedings of the 2020aware self-attention.
conference on empirical methods in natural lan-guage processing (emnlp).
association for com-putational linguistics..zhilin yang, zihang dai, yiming yang, jaime car-bonell, russ r salakhutdinov, and quoc v le.
2019.xlnet: generalized autoregressive pretraining forin advances in neurallanguage understanding.
information processing systems 32: annual con-ference on neural information processing systems2019, neurips 2019, 8-14 december 2019, vancou-ver, bc, canada..yuan yao, deming ye, peng li, xu han, yankai lin,zhenghao liu, zhiyuan liu, lixin huang, jie zhou,and maosong sun.
2019. docred: a large-scalein pro-document-level relation extraction dataset.
ceedings of the 57th conference of the associationfor computational linguistics, acl 2019, florence,italy, july 28- august 2, 2019, volume 1: long pa-pers, pages 764–777..deming ye, yankai lin, jiaju du, zhenghao liu,maosong sun, and zhiyuan liu.
2020. coreferen-tial reasoning learning for language representation..3361appendices.
tasks.
a training details for downstream.
in this section, we introduce the training details fordownstream tasks (relation extraction, entity typingand question answering).
we implement all modelsbased on huggingface transformers14..a.1 relation extraction.
document-level relationextraction fordocument-level relation extraction, we did ex-periments on docred (yao et al., 2019).
wemodify the ofﬁcial code15 for implementation.
forexperiments on three partitions of the originaltraining set (1%, 10% and 100%), we adoptbatch size of 10, 32, 32 and training epochs of400, 400, 200, respectively.
we choose adamoptimizer (kingma and ba, 2014) as the optimizerand the learning rate is set to 4 × 10−5.
weevaluate on dev set every 20/20/5 epochs and thentest the best checkpoint on test set on the ofﬁcialevaluation server16..relation.
sentence-levelextraction forsentence-level relation extraction, we did ex-periments on tacred (zhang et al., 2017)and semeval-2010 task 8 (hendrickx et al.,2019) based on the implementation of peng et al.
(2020)17. we did experiments on three partitions(1%, 10% and 100%) of the original training set.
the relation representation for each entity pair isobtained in the same way as in pre-training phase.
other settings are kept the same as peng et al.
(2020) for fair comparison..a.2 entity tying.
for entity typing, we choose figer (ling et al.,2015), whose training set is labeled with distantsupervision.
we modify the implementation ofernie (zhang et al., 2019)18.in ﬁne-tuningphrase, we encode the entities in the same wayas in pre-training phase.
we set the learning rate to3 × 10−5 and batch size to 256, and ﬁne-tune the.
14https://github.com/huggingface/.
transformers.
15https://github.com/thunlp/docred16https://competitions.codalab.org/.
competitions/20717.
17https://github.com/thunlp/.
re-context-or-names.
18https://github.com/thunlp/ernie.
models for three epochs, other hyper-parametersare kept the same as ernie..a.3 question answering.
multi-choice qa for multi-choice question an-swering, we choose wikihop (welbl et al., 2018).
since the standard setting of wikihop does notprovide the index for each candidate, we then ﬁndthem by exactly matching them in the documents.
we did experiments on three partitions of the origi-nal training data (1%, 10% and 100%).
we set thebatch size to 8 and learning rate to 5 × 10−5, andtrain for two epochs..extractive qa for extractive question answer-ing, we adopt mrqa (fisch et al., 2019) as thetestbed and choose three datasets: squad (ra-jpurkar et al., 2016), triviaqa (joshi et al., 2017)and naturalqa (kwiatkowski et al., 2019).
weadopt adam as the optimizer, set the learning rateto 3 × 10−5 and train for two epochs.
in the mainpaper, we report results on two splits (10% and100%) and results on 1% are listed in table 11..b generalized language understanding.
(glue).
the general language understanding evalua-tion (glue) benchmark (wang et al., 2018) pro-vides several natural language understanding tasks,which is often used to evaluate plms.
to testwhether led and lrd impair the plms’ per-formance on these tasks, we compare bert,ericabert, roberta and ericaroberta.
we fol-low the widely used setting and use the [cls] to-ken as representation for the whole sentence orsentence pair for classiﬁcation or regression.
ta-ble 9 shows the results on dev sets of glue bench-mark.
it can be observed that both ericabert andericaroberta achieve comparable performancethan the original model, which suggests that jointlytraining led and lrd with lmlm does not hurtplms’ general ability of language understanding..c full results of ablation study.
full results of ablation study (docred, wikihopand figer) are listed in table 10..d joint named entity recognition and.
relation extraction.
joint named entity recognition (ner) and re-lation extraction (re) aims at identifying enti-ties in text and the relations between them.
we.
3362dataset.
mnli(m/mm) qqp qnli.
sst-2 cola sts-b mrpc rte.
bertericabert.
robertaericaroberta.
84.0/84.484.5/84.7.
87.5/87.387.5/87.5.
88.988.3.
91.991.6.
90.690.7.
92.892.6.
92.492.8.
94.895.0.
57.257.9.
63.663.5.
89.789.5.
91.290.7.
89.489.5.
90.291.5.
70.169.6.
78.778.5.table 9: results on dev sets of glue benchmark.
we report matched/mismatched (m/mm) accuracy for mnli,f1 score for qqp and mrpc, spearman correlation for sts-b and accuracy for other tasks..dataset.
size.
bert.
+.
-nsp-nsp+led-nsp+ltrd+-nsp+ltrd-nsp+lrd.
c ,t.+c.s ,t.+s.ericabert.
docred.
wikihop (m).
figer.
1% 10% 100% 1% 10% 100% 100%.
28.930.134.434.833.935.936.0.
44.945.247.646.447.348.048.3.
54.554.655.854.755.555.655.9.
37.938.241.137.438.037.240.2.
53.153.659.852.251.252.058.1.
73.173.374.872.872.572.773.9.
72.772.673.872.673.574.074.7.table 10: full results of ablation study.
we report test igf1 on docred, dev accuracy on the masked (m) settingof wikihop and test micro f1 on figer..helping plms better understand and represent bothentities and relations in text..setting.
squad triviaqa naturalqa.
bertmtbcpericabert.
robertaericaroberta.
15.811.212.551.3.
22.157.6.
28.722.025.651.4.
40.651.3.
31.528.429.442.9.
34.057.6.table 11: results (f1) on extractive qa (squad, triv-iaqa and naturalqa) on 1% split..model.
conll04.
ade.
ner.
re.
ner.
re.
bertericabert.
robertaericaroberta.
88.589.3.
89.890.0.
70.371.5.
72.072.8.
89.289.5.
89.790.2.
79.280.2.
81.682.4.table 12: results (f1) on joint ner&re..adopt spert (eberts and ulges, 2019) as the basemodel and conduct experiments on two datasets:conll04 (roth and yih, 2004) and ade (gu-rulingappa et al., 2012) by replacing the base en-coders (bert and roberta) with ericabert andericaroberta, respectively.
we modify the imple-mentation of spert19 and keep all the settings thesame.
from the results listed in table 12, we cansee that erica outperforms all baselines, whichagain demonstrates the superiority of erica in.
19https://github.com/markus-eberts/spert.
3363