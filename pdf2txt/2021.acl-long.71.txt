integrated directional gradients: feature interaction attribution forneural nlp models.
sandipan sikdar∗rwth aachen universitysandipan.sikdar@cssh.rwth-aachen.de.
parantapa bhattacharya∗university of virginiaparantapa@virginia.edu.
kieran heeseuniversity of virginiakh8fb@virginia.edu.
abstract.
in this paper, we introduce integrated direc-tional gradients (idg), a method for attribut-ing importance scores to groups of features,indicating their relevance to the output of aneural network model for a given input.
thesuccess of deep neural networks has been at-tributed to their ability to capture higher levelfeature interactions.
hence, in the last fewyears capturing the importance of these fea-ture interactions has received increased promi-nence in ml interpretability literature.
in thispaper, we formally deﬁne the feature groupattribution problem and outline a set of ax-ioms that any intuitive feature group attribu-tion method should satisfy.
earlier, cooper-ative game theory inspired axiomatic meth-ods only borrowed axioms from solution con-cepts (such as shapley value) for individualfeature attributions and introduced their ownextensions to model interactions.
in contrast,our formulation is inspired by axioms satis-ﬁed by characteristic functions as well as solu-tion concepts in cooperative game theory liter-ature.
we believe that characteristic functionsare much better suited to model importance ofgroups compared to just solution concepts.
wedemonstrate that our proposed method, idg,satisﬁes all the axioms.
using idg we an-alyze two state-of-the-art text classiﬁers onthree benchmark datasets for sentiment analy-sis.
our experiments show that idg is able toeffectively capture semantic interactions in lin-guistic models via negations and conjunctions..1.introduction.
in the last decade deep neural networks (dnn)have been immensely successful.
much of thissuccess can be attributed to their ability to learnfrom complex higher order interactions from rawfeatures (goodfellow et al., 2016).
this success ofdnns has led to them being increasingly adopted.
∗equal contribution.
for algorithmic decision making.
this in turnhas led to increasing concerns over explainabil-ity and interpretability of these models, given theimportant role they are beginning to take in soci-ety (selbst and barocas, 2018)..one area of work that has emerged in recentyears is that of black box model explanation strate-gies that “explain” the output of a dnn for a giveninput using feature attribution scores or saliencymaps (sundararajan et al., 2017; shrikumar et al.,2017).
numerous studies have been published inrecent years proposing different strategies to an-swer the question “which features in the input weremost important in deciding the output of the dnn?”however, modern dnns take as input raw data asfeatures, and learn from higher order interactionof those features.
thus in the past year a numberof studies have instead focused on explaining fea-ture interactions rather than explaining individualfeatures (chen and jordan, 2020; jin et al., 2019;sundararajan et al., 2020; chen et al., 2020; tsanget al., 2020)..one issue that remains, however, is that giventwo methods for attributing importance scores, it isnot entirely straight forward to objectively comparethem.
as has been noted by earlier studies (sun-dararajan et al., 2017), if the output of an attributionmethod seems non-intuitive it is not easy to answerif that is caused by (i) limitations of the attributionmethod, (ii) limitations of the dnn model beingexplained, or (iii) limitation of the data on whichthe dnn model was trained.
like multiple previ-ous studies (chen and jordan, 2020; sundararajanet al., 2020; tsang et al., 2020) we take an ax-iomatic approach to this problem, whereby we ﬁrstdeﬁne the set of properties/axioms that a “good”solution must satisfy, followed by development ofa solution that satisﬁes those axioms..the method for computing feature group attri-bution (interchangeably referred to as feature inter-.
proceedingsofthe59thannualmeetingoftheassociationforcomputationallinguisticsandthe11thinternationaljointconferenceonnaturallanguageprocessing,pages865–878august1–6,2021.©2021associationforcomputationallinguistics865action attribution) presented in this study is calledintegrated directional gradients or idg.
likemultiple earlier methods in this area, idg is a co-operative game theory inspired method.
however,unlike earlier cooperative game theory inspired ax-iomatic methods which only borrowed axioms fromsolution concepts (such as shapley value) for indi-vidual feature attributions and introduced their ownextensions to model interactions, our formulationis inspired by axioms satisﬁed by well behavedcharacteristic functions as well as solution con-cepts in cooperative game theory literature.
weﬁnd that well behaved characteristic functions pro-vide a much simpler and intuitive framework fordeﬁning axioms for group attributions..we apply idg on state-of-the-art models on thenlp domain.
as part of its input idg requires aset of meaningful feature sets, that have a hierar-chical structure (section 2.1).
in this paper we useparse tree of sentences to construct the meaningfulfeature structures.
figure 1 shows an illustrative ex-ample of the nature of explanations and attributionscomputed using idg..the major contributions of the current work are.
as follows:.
• first, we formally deﬁne the feature groupattribution problem as an extension to the fea-ture attribution problem (section 2.1)..• second, we state a set of axioms that a well be-haved feature group attribution method shouldsatisfy (section 2.2)..• third, we present the method of integrateddirectional gradients or idg as a solutionto the feature group attribution problem thatsatisﬁes the stated axioms (section 2.3)..• fourth, we propose an efﬁcient algorithm tocompute idg for a given set of feature groupswith a hierarchical structure (section 2.4).
• finally, we compare idg with other recentlyproposed related methods for computing fea-ture interactions attribution.
(section 3)..• to facilitate reproducibility, the implementa-tion of idg has been made publicly avail-able1..2 methodology.
2.1 problem deﬁnition.
in this section we formally state the problem ofassigning attribution scores to meaningful feature.
1https://github.com/parantapa/integrated-directional-gradients.
1.0.
0.861.
0.454.
0.407.
0.289.
0.269.
0.137.
0.165.
0.099.
0.189.
0.138.frenetic.
but.
not.
really.
funny.
..fre.
net.
ic.
0.112.
0.080.
0.076.d1.
d2.
d3.
………….
d768.
0.0005.
0.0002.
0.0003 ………… 0.0006.figure 1: computation of attribution score (value func-tion v) for an example sentence frenetic but notreally funny.
magenta and green respectively de-note negative or positive contribution to the inferredclass and the importance is represented by the color in-tensity.
constituency parse tree is used to obtain mean-ingful feature groups.
note that each word is further di-vided into tokens (owing to byte pair encodings) eachof which has 768 dimensions.
idg computes impor-tance scores in a bottom-up manner starting from theindividual embedding dimensions (di) working its wayup to tokens, words, phrases and ﬁnally the sentence..groups..let f (x) be a deep neural network function, thattakes as input a n dimensional real valued vectorx ∈ rn and produces a real valued scalar output.
let a = {a1, a2, .
.
.
, an} refer to the set of fea-tures, with xi referring to the value of feature ai infeature vector x..then the feature group attribution problem isdeﬁned as follows: given an input x, a baselineb ∈ rn, and a family of meaningful feature sub-sets m ⊆ p(a), assign to every subset of featuress ⊆ a a value/importance score v(s).
here, p(a)represents the power set of the feature set..the above formulation is inspired by coopera-tive game theory literature.
intuitively, we think offeatures as players in a co-operative game tryingto “help” the dnn model reach its output.
the ob-jective then is to design a “good” value/importancefunction (characteristic function in cooperativegame theory literature) for each feature subset(coalition of players)..note that the above formulation is very differentfrom existing cooperative game theory inspiredfeature attribution methods.
most existing methodsassume that the value/characteristic function exists.
866and then compute a payoff assignment vector forindividual features, typically using shapley values.
similar to earlier studies, in our formulation weassume that the baseline b represents the “zero”input or absence of contribution from any feature.
the “family of meaningful feature subsets” mcaptures the notion that not all subsets of featuresrepresent “meaningful” parts of input.
anotherintuitive way to think about this is that not all fea-tures can collaborate directly, but need to be partof groups that can directly collaborate..in general we will assume that m has a hierar-chical containment structure, that is feature groupsin m can be represented as a directed acyclic graph— with tree being a special case.
further, we willalso assume that every individual feature is in m— that is {ai} ∈ m for i ∈ {1, 2, .
.
.
, n} — andrepresents the leaf nodes in the hierarchy, while theset of all features is also in m — that is a ∈ mand represents the root of the hierarchy..2.2 solution axioms.
in this section we present a set of axioms that a wellbehaved value/importance function should satisfy.
note that, the following four axioms are variantsof standard axioms for characteristic functions incooperative game theory literature..axiom 1 (non-negativity) every feature subsethas a non-negative value, v(s) ≥ 0..axiom 2 (normality) the value of the empty setof features is zero, v(∅) = 0..axiom 3 (monotonicity) the value of a set of fea-tures is greater than or equal to the value of any ofits subsets; if s ⊆ t , then v(s) ≤ v(t )..axiom 4 (superadditivity) the value oftheunion of two disjoint sets of features is greaterthan or equal to the sum of the values of the twosets; if s ∩ t = ∅ then v(s ∪ t ) ≥ v(s) + v(t )..since the value function represents the impor-tance of a set of features, which is intuitivelya direction less quantity, the non-negativity ax-iom ensures that every feature has a non-negativevalue/importance score.
similarly, the normal-ity axiom ensures that the importance score as-signed to the empty set of features is zero.
sincein the current framework the features in a deepneural network “collaborate”, with the assump-tion that collaboration can only be beneﬁcial, theaxioms of monotonicity and superadditivity en-sure that collaboration doesn’t lead to diminished.
value/importance.
note that superadditivity to-gether with non-negativity implies monotonicity.
in a cooperative game, players cooperate to gen-erate the maximum value.
a sometimes implicit as-sumption in these games is that it is always possiblefor a player to do nothing, in which case they gener-ate zero value.
thus if doing something generatesnegative value a rational player will always chooseto do nothing.
this is the essence of axiom 1. inaxiomatic ml explanation literature, features arethought of as players cooperating to predict the out-put.
one can also think of the value provided by afeature (importance of the feature) as the informa-tion contained in the feature that is effectively usedby the model.
this view also supports assumptionof axiom 1 as quantities of information (entropy)is also a non-negative quantity..axioms 1–3 are some of the foundational ax-ioms of cooperative game theory (chalkiadakiset al., 2011).
while much mathematical theory hasbeen published for computing solution concepts ingames where these assumptions do not hold, weargue that those games themselves can be difﬁcultto interpret and thus are less suitable for developinginterpretability/explainability methods..the following three axioms are variations of ax-ioms of the same name presented in the (sundarara-jan et al., 2017).
the modiﬁcations presented hereare necessary to incorporate the complexities re-sulting from assigning attribution scores to groupsof features rather than individual features..axiom 5 (sensitivity (a)) let there be a featureai such that, f (x) 6= f (b) for every input featurevector x and baseline vector b that only differ inai.
then v({ai}) > 0 and v(s) > 0 for every setof features s such that ai ∈ s.axiom 6 (sensitivity (b)) let there be a featureaj such that, f (x) = f (b) for every input featurevector x and baseline vector b that only differ inaj.
then v({aj}) = 0 and v(s) = v(s r {aj})for every set of features s such that aj ∈ s..in essence the axiom sensitivity (a) ensures thatfeatures that does effect the output of the dnnare not assigned a zero value/importance.
con-sequently, any feature group that includes such afeature must also be assigned a non-zero value.
conversely, the axiom sensitivity (b) ensures thatany feature that does not effect the output of thednn is assigned a zero value, and that it doesn’tcontribute any value to any feature group that it isincluded in..867axiom 7 (symmetry preservation) two featuresai and aj are said to be functionally equivalent iff (x) = f (y) for every pair of input vectors x andy such that xi = yj, xj = yi, and xk = yk fork 6∈ {i, j}.
two features ai and aj are said tobe structurally equivalent with respect to a familyof meaningful feature subsets m if ai ∈ s ands 6= {ai} implies aj ∈ s for all feature subsetss ∈ m and vice versa.
if two features ai and aj are both functionally andstructurally equivalent and if the given input vectorx and baseline vector b are such that xi = xj andbi = bj then v(s ∪ {ai}) = v(s ∪ {aj}) for everysubset of features s ⊆ a r {ai, aj}..the symmetry preservation axiom ﬁrst deﬁnestwo different types of feature equivalence: func-tional and structural.
two features are said to befunctionally equivalent if swapping the values ofthose features doesn’t effect the output of the dnn.
where as structural equivalence of features on theother hand refers to them having equivalent posi-tion in the structure imposed by the set of meaning-ful features m .
finally, the symmetry preservationaxiom ensures that features that are both function-ally and structurally equivalent contribute equalvalue/importance to all feature subsets they are in-cluded in..axiom 8 (implementation invariance) two neu-ral networks f ′() and f ′′() are functionally equiv-alent if f ′(x) = f ′′(x) for all x. let the valuefunctions for them be denoted by v′() and v′′() re-spectively.
then v′(s) = v′′(s) for all subset offeatures s ⊆ a..the implementation invariance axiom simplyensures that different implementations of the samednn function result in same value/importance as-signment to all feature subsets..2.3 our method: integrated directional.
gradients.
in this section we present a solution to the “fea-ture group attribution problem” that we call theintegrated directional gradients method or idg.
this method is inspired by the integrated gradientsmethod (sundararajan et al., 2017) and by harsanyidividends (harsanyi, 1963) in cooperative gametheory.
the high level idea of the method is to con-struct the value function in terms of the “dividends”generated by each meaningful feature subset.
inthis formulation, each meaningful feature groupcontributes “additional value” to the dnn model,.
that we call “dividend” of the group.
the dividendof a feature group s is represented by d(s) andd(s) ∈ [0, 1)..the dividend of a single feature is also its valueand a measure of its importance.
one of the sim-plest measures of importance of a feature is the par-tial derivative of the dnn function with respect tothe feature.
the partial derivative also has an intu-itive notion that it represents the amount of changein the output of the dnn function per unit changein the input, in the direction of the feature.
how-ever, as noted in the earlier studies (sundararajanet al., 2017), due to effects such as gradient satu-ration, partial derivatives can’t be directly used formeasuring the importance of a feature.
to alleviatethis issue the authors of the integrated gradientsmethod recommend taking a path integral of thepartial gradient over the straight line path connect-ing the baseline b to the input x. for this study, wetake a similar approach, and take the absolute valueof the path integral of the partial gradient as thedividend of a single feature..the dividend of a group of features is distinctfrom its value and is the measure of the importanceof the interaction of the features in the group.
forthis study we consider the directional derivative ofthe dnn function in the direction of the given setof features to be representative of the importance ofthe interaction of the given set of features.
similarto the single feature case this also has the intuitivenotion that it represents the amount of change in theoutput of dnn function per unit change in input,in the direction of the subset of features.
however,as in the case with single features, issues such asgradient saturation still need to be addressed for di-rectional gradients as well.
thus we propose to useabsolute value of idg, which is the path integral ofthe directional gradient over the straight line pathfrom the baseline b to the input x as the dividend ofthe feature group.
further, the sign of idg may beused to signify the nature of contribution (positiveor negative) to model output..zsi =.
xi − bi0.
(.
if ai ∈ sotherwise.
∇sf (x) = ∇f (x) · ˆzs where ˆzs =.
(1).
(2).
zskzsk.
idg(s) =.
∇sf (b + α(x − b)) dα (3).
1.α=0.
z.
868d(s) =.
|idg(s)|z.if s ∈ m.otherwise.
.
0.
z =.
|idg(s)|.
s∈mx.v(s) =.
d(t ).
xt ∈{t |t ⊆s∧s∈m }equations 1 to 6 describe the process of com-puting the value/importance v(s) of a subset offeatures using the idg method.
given a featuresubset s ﬁrst the feature subset difference vectorzs is computed from the input feature vector x andthe baseline vector b. next, idg(s) is computedby integrating over the directional derivative, in thedirection of zs over the straight line path from thebaseline b to the input x. the dividend d(s) of thefeature subset s is then computed by normalizingthe absolute value of idg(s) over all meaning-ful subsets, such that the sum of the dividends ofall meaningful features subsets add up to 1. fi-nally the value v(s) of the given feature subset sis computed by adding up the dividends of all themeaningful subsets contained in s, including itself.
proposition 1 v(s) satisﬁes axioms 1 to 82..2.4 efﬁciently computing integrated.
directional gradients.
similar to (sundararajan et al., 2017), we approxi-mate the integral in idg, by simply summing overthe gradients at points occurring at small intervalsalong the path from baseline b to the input x. theapproximated idg(s) is computed as:.
aidg(s) =.
∇sf.
b +.
(x − b).
1m + 1.m.k=0x.km.(cid:18).
(cid:19)(7).
here m denotes the number of steps in thereimann approximation of the integral.
we nowpropose a polynomial time dynamic programmingalgorithm (1) for calculating the attribution score(i.e., value function v) for all the meaningful sub-sets in m for a given input x and a baseline b..first, ∇f is calculated for each of the m + 1intermediate positions between x and b. next wecompute aidg(s) for all feature groups in m .
this is followed by the computation of z, which is.
2detailed proofs are available in appendix..(4).
(5).
(6).
m (x − b)).
compute aidg(s).
end forfor s ∈ m do.
for k ∈ {0, 1, .
.
.
, m} docompute ∇f (b + k.algorithm 11: procedure computeattribution(x, b, m, m)2:3:4:5:6:7:8:9:10:11:12:13:14:15: end procedure.
end forz ← ps∈m |aidg(s)|for s ∈ m do.
end forfor s ∈ m do.
compute v(s).
compute d(s).
end for.
⊲ using eq.
7.
⊲ using eq.
4.
⊲ using eq.
6.simply the sim of the aidg(s) scores for reachof the meaningful subsets.
given z and the individ-ual scores the divided d(s) can easily be computedusing eq.
4. finally, given the dividend of all mean-ingful subsets of s is known, the value functionv(s) for each of the meaningful subsets of s canbe computed using eq.
6..we illustrate the computation of attributionscores using an example sentence frenetic butnot really funny taken from sst dataset (fig-ure 1).
the task is sentiment classiﬁcation and theinferred class for this sentence is negative.
themodel used for classiﬁcation is xlnet-base (referto section 3 for details on dataset, model and train-ing procedure).
we leverage the constituency parsetree of the sentence to obtain meaningful featuregroups.
note that xlnet tokenizer uses byte pairencoding.
hence the word “frenetic” is furtherdecomposed into “fre”, “net” and “ic”.
each tokenis further represented by an embedding of size 768.the value function is calculated in a bottom-upmanner starting from each embedding dimensionof the constituent tokens (referred as di in figure 1).
these are then combined to obtain the value func-tion score for each token.
we then follow the parsetree to calculate the score for each phrase.
for ex-ample the score for phrase frenetic but is 0.407while that of not really funny is 0.454..the overall time complexity of algorithm 1 iso (m(f + b + v · |a|) + v + e), where f andb are the time complexity of a single forward andbackward pass of the neural network, v and eare, respectively, the number vertices and edgesin the graph structure induced by the family ofmeaningful feature subsets m , |a| is the numberof features, and m the number of approximationsteps used to compute aidg(s).
for more details.
869on the complexity result, refer to appendix..3 evaluation.
3.1 comparison with existing methods.
it has been noted that when a dnn explanationmethod returns a non-intuitive result, it is not pos-sible to disentangle which part of the pipeline —training data, trained model, or the explanationmethod — is to blame for the result (sundarara-jan et al., 2017).
thus many studies (sundararajanet al., 2017; chen and jordan, 2020; sundararajanet al., 2020; tsang et al., 2020) have taken the ax-iomatic strategy instead to compare methods quali-tatively.
taking a similar approach, we present intable 1 a qualitative comparison of recent featureinteraction attribution methods most similar to ourwork..we group the comparison into four major cat-egories.
first, in most cooperative game theoryliterature players are assumed to cooperate.
it isthus intuitive that more cooperation will not leadto lesser beneﬁt, and it is generally assumed thatthe grand coalition will form (chalkiadakis et al.,2011).
while there are mathematical formulationsthat work in absence of this assumption, we arguethat they lead to non-intuitive results when appliedto the task of feature interaction attribution.
theseassumptions are manifested by well-behavednessproperties of the characteristic/value function.
intable 1 we see that existing cooperative game the-ory inspired methods generally ignore this aspectwhen computing importance attributions..second, to compute the effect of a model in ab-sence of a feature, attribution methods generallymask out the feature, generally replacing it with azero or pad token.
it has been noted that this re-quires the dnn model to be evaluated in an regionof the input space for which it has not received anytraining data and for which its accuracy was neverevaluated (sundararajan et al., 2017; kumar et al.,2020).
thus the results that model produces forthese out-of-distribution inputs is questionable.
intable 1 we see that all existing methods computetheir attributions by evaluating the model for theseout-of-distribution inputs..third, in a cooperative game theoretic settingwhen players (here features) are assumed to coop-erate, it is intuitive that as the size of the coalitiongrows the coalition will not become less important.
this is the key intuition behind axioms 1–4.
how-ever, in table 1 we see that none of the existing.
methods ensure that their attributions adhere to thiskey intuition..finally, cooperative game theory based meth-ods generally ensure that axioms of completeness(a.k.a.
efﬁciency), symmetry preservation, lin-earity, and sensitivity (a.k.a null/dummy player)are warranted by their attributions.
in this paperwe follow the lead of (sundararajan et al., 2017)and use the nomenclature from (aumann and shap-ley, 2015), which additionally introduces the axiomof implementation invariance.
in table 1, we seethat for ls-tree (chen and jordan, 2020), shapley-taylor interaction index (sundararajan et al., 2020),and archipelago (tsang et al., 2020), which are co-operative game theory inspired methods, these as-sumptions hold.
however for scd/soc (jin et al.,2019) and hedge (chen et al., 2020) which arenot axiomatic formulations, these assumptions donot hold.
for our method, idg, all but the axiomof linearity holds.
in section 5.2 we argue thatthis is not a major limitation and refer to existingliterature that even argues for doing away with thelinearity axiom..3.2 evaluating idg on state-of-the-art.
models.
we deploy our model for the task of sentiment clas-siﬁcation across three different datasets - stanfordsentiment treebank (sst) (socher et al., 2013),yelp reviews (zhang et al., 2015) and imdb (maaset al., 2011).
for each dataset, we train three state-of-the-art models - xlnet-base (yang et al., 2019),xlnet-large (yang et al., 2019) and bert-itpt (sunet al., 2019).
we use the same hyperparameterconﬁguration as mentioned in the original papers.
they are summarized in appendix as well.
theperformance of these models are summarized intable 2..4 results.
to precisely visualize the interactions betweenphrases, we search over the test examples for in-stances of negations.
we follow the methodologyproposed in (murdoch et al., 2018).
in speciﬁc, welook into the parse tree for each review and checkif the left child consists of a negation phrase (e.g.,lacks, never etc.)
in the ﬁrst two words and theright child has a positive or a negative sentiment.
since for sst, each phrase is also annotated withtheir corresponding sentiment labels in the formof a constituency parse tree, this can be easily ob-.
870axioms/properties.
scd/soc hedge.
ls-tree.
sti archipelago.
idg.
well-behaved characteristic function na.
na.
in distribution evaluations.
non-negativitynormalitymonotonicitysuperadditivity.
sensitivitysymmetry preservationlinearitycompletenessimplementation invariance.
✗.
✗✗✗✗.
✗✗✗✗✗.
✗.
✗.
✗✗✗✗.
✓✓✓✓✓.
✗.
✗.
✗✗✗✗.
✓✓✓✓✓.
✗.
✗.
✗✗✗✗.
✓✓✓✓✓.
✗.
✗✗✗✗.
✗✗✗✗✗.
✓.
✓.
✓✓✓✓.
✓✓✗✓✓.
table 1: a comparison of axiomatic guarantees / properties of feature interaction attribution methods:scd/soc (jin et al., 2019), hedge (chen et al., 2020), ls-tree (chen and jordan, 2020), shapley-taylor in-teraction index (sti) (sundararajan et al., 2020), archipelago (tsang et al., 2020), and idg (proposed method).
note since v(∅) = 0 and v(a) = 1, idg satisﬁes completeness trivially..test/trainsplit6920/872560k/38k25k/25k.
xlnet-base0.9150.9790.967.xlnet-large0.9160.9830.967.bert-itpt.
0.7690.9470.957.sstyelpimdb.
our framework.
we also report the results on imdbreviews (maas et al., 2011) in appendix..5 discussion.
table 2: accuracy of the trained models on the threedatasets..5.1 quantitative evaluations and human.
judgement experiments.
tained.
for yelp and imdb, we look for presenceof negation phrases in the reviews and then man-ually select 100 such examples from the ﬁlteredset.
since the parse trees for the reviews are notexplicitly available for yelp and imdb, we deploya state-of-the-art constituency parser (mrini et al.,2019) to obtain them..the ﬁrst part.
we illustrate with one example each from sstand yelp datasets in figures 2(a) and 2(b) re-spectively.
additional examples can be foundfor figure 2(a) the classiﬁ-in appendix.
cation modelis xlnet-base and the groundtruth as well as the inferred class is neg-ative.
(though everythingmight be literate and smart) has a posi-tive sense.
but when appended with thesecond part (it never took off and alwaysseemed static), a negative sense is manifested.
this is captured by the classiﬁcation model asdemonstrated by our framework.
for the exam-ple in figure 2(b), the classiﬁer model is bert-itpt and the inferred as well as the ground-truthclass is negative.
this example consists of twosentences while the ﬁrst one nice atmospherehas a positive sense, when combined with the sec-ond sentence cheeseburger was not at allthat, the overall sense turns negative.
this is againconveniently manifested in the scores assigned by.
as noted by (sundararajan et al., 2017), when theresults of an explanation method is non-intuitive,it is not obvious which part of the ml pipeline —the data, the model being explained, the explana-tion method — is to be blamed and by how much.
due to this issue many authors (sundararajan et al.,2017; chen and jordan, 2020; sundararajan et al.,2020; tsang et al., 2020) have chosen to take theaxiomatic/theoretical path, where they state theproperties of the proposed method and compare ex-planation methods based on the axioms/propertiesthey satisfy..nevertheless, many recent studies (singh et al.,2018; jin et al., 2019; chen et al., 2020) haveproposed new explanation methods and providedevaluations using quantitative metrics such asaopc (nguyen, 2018), log odds (shrikumar et al.,2017), and cohesion score (jin et al., 2019)..one common strategy is to perturb the input— such as removing of top-k most importantwords/features — followed by measuring the dropin performance.
we argue that these methods ofevaluation have issues because they generally in-volve measuring model performance on out-of-distribution inputs.
and as stated earlier, measuringthe outputs of models on out-of-distribution inputs,that is inputs, on which the model has neither beentrained or tested on, is questionable..871     .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
nice    .
atmosphere          .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
though.
.
everything.
.
might be literate and smart.
.
.
.
.
.
,.
.
it.
.
never took off.
.
.
.
and always.
.
.
seemed.
static.
.
.
..cheeseburger.
.
was   .
not   .
all   .
that    .
.. .
(a).
(b).
figure 2: the value function scores assigned by our framework for different coalitions (interactions) betweenphrases for two reviews from sst (a) and yelp (b) respectively.
magenta and green respectively denote negativeor positive contribution to the inferred class and the magnitude of importance is represented by the color intensity.
note that the interactions are correctly captured by the classiﬁer model in both the cases as demonstrated by idg..the other strategy is to perturb the model — suchas by adding noise to model weights — followedby measuring the drop in performance.
(hookeret al., 2019) proposed a similar solution for theinput perturbation case as well, that is by retrain-ing the model after perturbing all training samples.
however, in this scenario if two explanation meth-ods provided different explanations/attributions forthe different models, it is not obvious if the modelsare to blame or the explanation methods.
simi-lar issues exist for human judgement experimentsas well.
due to the above issues for the currentwork we too have chosen to take the qualitativecomparison path..5.2 linearity and uniqueness.
one of the common axioms of solution conceptsin cooperative game theory is linearity.
theaxiom of linearity (a.k.a additivity) states thatif the characteristic/value function has the formv(s) = v1(s) + v2(s) and φ1(s) and φ2(s) arethe attributions due to v1(s) and v2(s) then theattribution due to v(s) should be given by φ(s) =φ1(s) + φ2(s)..during our design and experimentation we foundthat having the attributions normalized, that isv(∅) = 0 and v(a) = 1, provided much more in-tuitive results.
such normalization, however, runscounter to the possibility of an attribution methodthat satisﬁes linearity..further, it has been argued by some game the-orists that the axiom of linearity was added as amathematical convenience and also to constrainthe attributions such that it is unique (osborne andrubinstein, 1994).
further, (kumar et al., 2020).
argue that enforcing such uniqueness constraintsby this method limits the kind of models that canbe explained by these attributions..thus, idg is also not an unique solution to thefeature group attribution problem, due to its sacri-ﬁce of linearity.
however, given that recent studieshave found (sundararajan and najmi, 2020) thatshapley values can and have been used in manydifferent ways, each of which claiming uniqueness,the importance of uniqueness claims is signiﬁcantlydiminished..6 related work.
feature attribution based method.
these meth-ods essentially assign importance scores to indi-vidual features thereby explaining the decisions ofthe classiﬁer model.
the scores are mostly cal-culated by either backpropagating a custom rele-vance score (sixt et al., 2020) or directly usingthe gradients.
the gradient based methods aimto calculate the sensitivity of the inference func-tion with respect to the input features and therebymeasuring its importance.
the method was ﬁrstintroduced in (springenberg et al., 2015) and fur-ther investigated in (selvaraju et al., 2017; kimet al., 2019).
(sundararajan et al., 2017) adopts anaxiomatic approach and deem it to be more suit-able as the feature attribution methods are hard toevaluate empirically.
the other set of methods usu-ally backpropagates their custom relevance scoresdown to the input to identify relevance of an inputfeature (bach et al., 2015; shrikumar et al., 2017;zhang et al., 2018).
unlike the gradient basedmethods, these are not implementation invariant.
872(i.e., the back propagation process is architecturespeciﬁc).
game theoretic aspect.
(lundberg and lee, 2017)adopts results (shapely values in speciﬁc) fromcoalition game theory to obtain feature attributionscores.
the key idea is to consider the features asindividual players involved in a coalition game ofprediction which is considered the payout.
the pay-out then can be fairly distributed among the players(features) to measure their importance.
this hasbeen further explored in (lundberg et al., 2020;ghorbani and zou, 2020; sundararajan and najmi,2020; frye et al., 2020).
quantifying feature interactions.
the methodsmentioned above fail to properly capture the impor-tance of feature interaction.
(janizek et al., 2020)proposes to capture pair-wise interaction by build-ing upon integrated gradients framework.
(cuiet al., 2020) learns global pair-wise interactions inbayesian neural networks.
(murdoch et al., 2018)introduces contextual decomposition to capture in-teraction among words in a text for a lstm-basedclassiﬁer.
(singh et al., 2018) further extends themethod to other architectures.
more recent researchendeavors in this direction include (tsang et al.,2020; liu et al., 2020; chen et al., 2020).
we elab-orate more on the methods closest to our work insection 3..7 conclusion.
in this paper we investigated the problem of fea-ture group attribution and proposed a set of axiomsthat any framework for feature group attributionshould fulﬁll.
we then introduced idg, a novelmethod, as a solution to the problem and demon-strated that it satisﬁes all the axioms.
throughexperiments on real-world datasets with state-of-the-art dnn based classiﬁers we demonstrated theeffectiveness of idg in capturing the importanceof feature groups as deemed by the classiﬁer..acknowledgements.
sandipan sikdar was supported in part by rwthstuppd384-20.
aachen startup grant no.
parantapa bhattacharya was supported in part bythe dense threat reduction agency (dtra) un-der contract no.
hdtra1-19-d-0007, by thenational science foundation (nsf) under grantno.
ccf-1918656, and by the defense ad-vanced research projects agency (darpa) un-der contract no.
fa8650-19-c-7923.
the.
authors would also like to thank the research com-puting center at university of virginia for computetime grant on the rivanna cluster..references.
robert j aumann and lloyd s shapley.
2015. valuesof non-atomic games.
princeton university press..sebastian bach, alexander binder, gr´egoire mon-tavon, frederick klauschen, klaus-robert m¨uller,and wojciech samek.
2015. on pixel-wise explana-tions for non-linear classiﬁer decisions by layer-wiserelevance propagation.
plos one, 10(7):e0130140..georgios chalkiadakis, edith elkind, and michaelwooldridge.
2011. computational aspects of coop-erative game theory.
synthesis lectures on artiﬁcialintelligence and machine learning, 5(6):1–168..hanjie chen, guangtao zheng, and yangfeng ji.
2020.generating hierarchical explanations on text classi-ﬁcation via feature interaction detection.
in annualmeeting of the association for computational lin-guistics, pages 5578–5593..jianbo chen and michael jordan.
2020..ls-tree:model interpretation when the data are linguistic.
in aaai conference on artiﬁcial intelligence, vol-ume 34, pages 3454–3461..tianyu cui, pekka marttinen, samuel kaski, et al.
2020. learning global pairwise interactions withbayesian neural networks.
in european conferenceon artiﬁcial intelligence.
ios press..christopher frye, colin rowat, and ilya feige.
2020.asymmetric shapley values:incorporating causalknowledge into model-agnostic explainability.
ad-vances in neural information processing systems,33..amirata ghorbani and james zou.
2020. neuron shap-ley: discovering the responsible neurons.
arxivpreprint arxiv:2002.09815..ian goodfellow, yoshua bengio, aaron courville, andyoshua bengio.
2016. deep learning, volume 1.mit press cambridge..john c harsanyi.
1963. a simpliﬁed bargaining modelinternational.
for the n-person cooperative game.
economic review, 4(2):194–220..sara hooker, dumitru erhan, pieter-jan kindermans,and been kim.
2019. a benchmark for interpretabil-ity methods in deep neural networks.
in advances inneural information processing systems..joseph d janizek, pascal sturmfels, and su-in lee.
2020.explaining explanations: axiomatic fea-ture interactions for deep networks.
arxiv preprintarxiv:2002.04138..873xisen jin, zhongyu wei, junyi du, xiangyang xue,and xiang ren.
2019. towards hierarchical impor-tance attribution: explaining compositional seman-in internationaltics for neural sequence models.
conference on learning representations..beomsu kim, junghoon seo, seunghyeon jeon, jamy-oung koo, jeongyeol choe, and taegyun jeon.
2019.why are saliency maps noisy?
cause of and solu-in ieee/cvf inter-tion to noisy saliency maps.
national conference on computer vision workshop(iccvw), pages 4149–4157.
ieee..i elizabeth kumar, suresh venkatasubramanian, car-los scheidegger, and sorelle friedler.
2020. prob-lems with shapley-value-based explanations as fea-in international con-ture importance measures.
ference on machine learning, pages 5491–5500.
pmlr..zirui liu, qingquan song, kaixiong zhou, ting-hsiang wang, ying shan, and xia hu.
2020. de-tecting interactions from neural networks via topo-logical analysis.
advances in neural informationprocessing systems, 33..ilya loshchilov and frank hutter.
2018. decoupledin international con-.
weight decay regularization.
ference on learning representations..scott m lundberg, gabriel erion, hugh chen, alexdegrave, jordan m prutkin, bala nair, ronit katz,jonathan himmelfarb, nisha bansal, and su-in lee.
2020. from local explanations to global understand-ing with explainable ai for trees.
nature machineintelligence, 2(1):2522–5839..scott m lundberg and su-in lee.
2017. a uniﬁedin ad-approach to interpreting model predictions.
vances in neural information processing systems,pages 4765–4774..andrew maas, raymond e daly, peter t pham, danhuang, andrew y ng, and christopher potts.
2011.learning word vectors for sentiment analysis.
in an-nual meeting of the association for computationallinguistics: human language technologies, pages142–150..khalil mrini, franck dernoncourt, trung bui, wal-re-an interpretable self-arxiv preprint.
ter chang, and ndapa nakashole.
2019.thinking self-attention:attentive encoder-decoder parser.
arxiv:1911.03875..w james murdoch, peter j liu, and bin yu.
2018. be-yond word importance: contextual decompositionto extract interactions from lstms.
in internationalconference on learning representations..dong nguyen.
2018. comparing automatic and humanevaluation of local explanations for text classiﬁca-in conference of the north american chap-tion.
ter of the association for computational linguistics:human language technologies, volume 1 (long pa-pers), pages 1069–1078..martin j osborne and ariel rubinstein.
1994. a course.
in game theory.
mit press..andrew d selbst and solon barocas.
2018. the in-tuitive appeal of explainable machines.
fordham l.
rev., 87:1085..ramprasaath r selvaraju, michael cogswell, ab-hishek das, ramakrishna vedantam, devi parikh,and dhruv batra.
2017. grad-cam: visual explana-tions from deep networks via gradient-based local-in ieee international conference on com-ization.
puter vision, pages 618–626..avanti shrikumar, peyton greenside, and anshul kun-daje.
2017. learning important features throughpropagating activation differences.
in internationalconference on machine learning, pages 3145–3153.
pmlr..chandan singh, w james murdoch, and bin yu.
2018.hierarchical interpretations for neural network pre-dictions.
in international conference on learningrepresentations..leon sixt, maximilian granz, and tim landgraf.
2020.when explanations lie: why many modiﬁed bp at-tributions fail.
in international conference on ma-chine learning, pages 9046–9057.
pmlr..dylan slack, sophie hilgard, emily jia, sameer singh,and himabindu lakkaraju.
2020. fooling lime andshap: adversarial attacks on post hoc explanationin aaai/acm conference on ai, ethics,methods.
and society, pages 180–186..richard socher, alex perelygin, jean wu, jasonchuang, christopher d manning, andrew y ng,and christopher potts.
2013. recursive deep mod-els for semantic compositionality over a sentimentin conference on empirical methods intreebank.
natural language processing, pages 1631–1642..j springenberg, alexey dosovitskiy, thomas brox,and m riedmiller.
2015. striving for simplicity:the all convolutional net.
in iclr (workshop track)..chi sun, xipeng qiu, yige xu, and xuanjing huang.
2019. how to ﬁne-tune bert for text classiﬁcation?
in china national conference on chinese computa-tional linguistics, pages 194–206.
springer..mukund sundararajan, kedar dhamdhere, and ashishagarwal.
2020. the shapley taylor interaction index.
in international conference on machine learning,pages 9259–9268.
pmlr..mukund sundararajan and amir najmi.
2020. themany shapley values for model explanation.
in in-ternational conference on machine learning, pages9269–9278.
pmlr..mukund sundararajan, ankur taly, and qiqi yan.
2017.in inter-axiomatic attribution for deep networks.
national conference on machine learning, pages3319–3328.
pmlr..874michael tsang, sirisha rambhatla, and yan liu.
2020.how does this interaction affect me?
interpretableattribution for feature interactions.
advances in neu-ral information processing systems, 33..junlin wang, jens tuyls, eric wallace, and sameersingh.
2020. gradient-based analysis of nlp mod-in conference on empiricalels is manipulable.
methods in natural language processing: findings,pages 247–258..zhilin yang, zihang dai, yiming yang, jaime car-bonell, russ r salakhutdinov, and quoc v le.
2019.xlnet: generalized autoregressive pretraining forlanguage understanding.
in advances in neural in-formation processing systems, pages 5753–5763..jianming zhang, sarah adel bargal, zhe lin, jonathanbrandt, xiaohui shen, and stan sclaroff.
2018. top-down neural attention by excitation backprop.
inter-national journal of computer vision, 126(10):1084–1102..xiang zhang, junbo zhao, and yann lecun.
2015.character-level convolutional networks for text clas-in advances in neural information pro-siﬁcation.
cessing systems, pages 649–657..8758 appendix.
8.1 detailed proof of theorems.
given dividend d(s) is constructed to be non-negative, it is straight forward to show that v(s)satisﬁes axioms 1 to 4, given it is a sum of one ormore non-negative dividends..lemma 1 v(s) satisﬁes sensitivity (a)proof 1 let there be a feature ai such that, f (x) 6=f (b) for given input x and baseline b that onlydiffer in ai.
to prove v(s) satisﬁes sensitivity (a)it is sufﬁcient to prove that in the above scenarioidg({ai}) 6= 0. then from (eq 2).
ˆz{ai}j =.
10.
(.
if j = iotherwise.
lemma 3 v(s) satisﬁes symmetry preservation.
proof 3 to prove that v(s) satisﬁes symmetrypreservation, it is sufﬁcient to prove that for anyfeature subset s ⊆ ar{ai, aj}, idg(s ∪{ai}) =idg(s ∪ {aj}).
the precondition of functionalequivalence implies that if in a given feature vectorx, xi = xj then.
∂∂xi.
f (x) =.
f (x).
∂∂xj.
additionally, when considering xi = xj and bi =bj, we have.
∇s∪{ai}f (x) = ∇s∪{aj }f (x).
since, in the given case, xi is the only feature thatvaries on the straight line path connecting b and x,we can rewrite f (x) = g(xi).
therefore.
further, this also implies that xi = xj on everypoint on the straight line connecting b and x. theabove imples that idg(s ∪ {ai}) = idg(s ∪(cid:4){aj})..∇{ai}f (x) =.
f (x) =.
g(xi).
ddxi.
lemma 4 v(s)invariance.
satisﬁes.
implementation-.
thus.
idg({ai}) =.
f (b + α(x − b)) dα.
proof 4 v(s) satisﬁes implementation invariancesince they only depend on gradients of the nerual(cid:4)network function and its evaluations..g (bi + α(xi − bi)) dα.
8.2 complexity of algorithm 1.ddxi.
g(xi) dxi.
∂∂xi.
∂∂xiddxi.
1.α=01.z.
=.
=.
=.
=.
z.xi.
α=01xi − bi zxi=big(xi) − g(bi)xi − bif (x) − f (b)xi − bi.
6= 0.in algorithm 1, the for loop on line 2 computes m+1 forward and backward backward passes of theneural network.
let the graph structure induced bym contain v vertices and e edges.
then the loopof line 5 requires v computations of aidg(s)each of which requires o(m · |a|) computationtime.
next, z can be computed in o(v ) time.
each iteration of the loop on line 9 takes o(1) time.
finally the loop on line 12 can be computed ino(e) time..thus, the overall time complexity of algorithm 1is o (m(f + b + v · |a|) + v + e), where fand b are the time complexity of a single forwardand backward pass of the neural network, v ande are, respectively, the number vertices and edgesin the graph structure induced by the family ofmeaningful feature subsets m , |a| is the numberof features, and m the number of approximationsteps used to compute aidg(s)..(cid:4).
lemma 2 v(s) satisﬁes sensitivity (b)proof 2 let there be a feature ai such that, f (x) =f (y) for every input x and y that only differ in ai.
to prove v(s) satisﬁes sensitivity (b) it is sufﬁcientto prove that idg(s) = idg(s′), for all s suchthat ai ∈ s, and s′ = s r {ai}.
the preconditionof sensitivity (b) implies that.
∂∂xi.
f (x) = 0.therefore for any s and s′ such that s′ = s r{ai}.
8.3 additional results.
∇sf (x) = ∇s′f (x).
which implies that idg(s) = idg(s′)..(cid:4).
imdb.
the dataset (maas et al., 2011) consists of25k positive labeled and 25k negatively labeledreviews posted on imdb..876all these models were trained on cluster with 2cpus each with 20 cores, 384 gb ddr4 ram andinter xeon gold 6148 processor.
the distributedset up was connected through mellanox connectx-5 network and used lustre ﬁle system.
the set upalso utilized 4 nvidia tesla v100 gpus eachwith 32 gb memory..experiments with idg were performed on a sys-tem with intel core i7-8550u 1.80ghz cpu with16 gb ram..8.5 adversarial attacks against explanations.
in (selbst and barocas, 2018) the authors arguethat one of the main reasons to develop explana-tion techniques is to enable humans to understandhow automated decision systems work which inturn enable us to debate on whether the model’srules for decision making are justiﬁable.
on the ﬂipside security researchers (slack et al., 2020) havehave shown that such efforts can be stiﬂed usingadversarial attack techniques.
in particular (slacket al., 2020) showed that models can be trained todeceive blackbox explanation methods, such that itprovides ‘unfair’ results on in-distribution sampleswhile exhibiting different behavior when explainedusing kernelshap.
in a recent study (wang et al.,2020) the researchers have explored creation of de-ceptive models that can fool gradient based meth-ods such as intgrad (sundararajan et al., 2017).
in (slack et al., 2020) the authors showed that eval-uating models on out-of-distribution inputs, that isthe inputs that the original model was not testedon, is a large potential attack surface for such de-ceptive techniques.
while unlike existing studies,idg doesn’t evaluate out-of-distribution values, itseems certainly possible to use adversarial train-ing methods to deceive idg.
while for the currentwork evaluation against adversarial attack was outof scope, we consider it as an important future di-rection..for evaluation, we deploy the same procedureas in case of yelp to obtain 100 representative ex-amples.
two illustrative examples are provided infigures 3 and 4.negative example.
we consider an example fromthe sst dataset where the classiﬁer model madewrong inference.
the ground truth class was nega-tive while the inferred class was positive.
the valuefunction scores for all the valid coalitions are pro-vided in figure 5. the results show that althoughthe classiﬁer was able to distinguish between thepositive sense manifested in the ﬁrst part and thenegative sense in the second, it made a positiveinference overall.
this might be due to the lowconﬁdence of the classiﬁer in inferring the ﬁnalclass as demonstrated by the probabilities - 0.44for negative and 0.56 for positive class.
however,further investigations are required before strongerclaims can be made..8.4 training models.
sst.
the xlnet-base model was trained with batchsize 24 for 4 epochs.
we use adamw (loshchilovand hutter, 2018) as optimizer with learning rate2e−05 and weight decay 0.01. the model achievedan accuracy of 0.915 on the test set.
the xlnet-large model was trained with same batch size, forsame number of epochs and with same optimizer.
the learning rate and weight decay were 5e−06and 0.01 respectively.
an accuracy of 0.916 wasobtained on the test set for this model.
bert-itptwas trained with a batch size of 24 and optmizedwith adamw with learning rate 1e−5 and weightdecay 0.01. the embedding layers were not frozenduring training.
yelp.
the bert-itpt model was trained with train-ing batch size of 24, for 3 epochs and withadamw (learning rate 1e−05, weight decay 0.01)and achieved an accuracy of 0.947 on the test set.
we further trained an xlnet models with similartraining hyperparameters and achieved an accuracyof 0.983.imdb.
the two models bert-itpt and xlnet-largewere both trained on 25k training examples andtested on the rest.
the batch sizes were 24 and32 respectively.
adamw was used as optimizerfor both models with same weight decay of 0.01but learning rates 2e−05 and 2e−05 respectively forbert-itpt and xlnet-large.
we could obtain testingaccuracy of 0.957 and 0.967 respectively for thetwo models..877the value function scores assigned by our framework for different coalitions (interactions) be-figure 3:tween phrases for the review apart from helena bonham carter, there is nothing worthy aboutand the surprise ending?!
the thought of a sequel is even more annoying.
this movie.
save your money, wait for the video and ignore that too.
the inferred class is negative.
idgcorrectly captures the positive sense (even though the overall sense is negative) of the phrase apart fromhelena bonham carter as it contributes oppositely to the overall inference result..figure 4:the value function scores assigned by our framework for different coalitions (interactions) be-tween phrases for the review aside for being a classic in the aspect of its cheesy lines andterrible acting, this film should never be watched unless you are looking for a goodcure for your insomnia.
movie’’.
the inferred class is negative.
idg shows how the classiﬁer captures the positive sense (even thoughthe overall sense is negative) of the phrase aside for being a classic in the aspect of cheesylines and terrible acting as it contributes oppositely to the overall inference result..i can’t imagine anyone actually thinking this was a ‘‘good.
figure 5:the value function scores assigned by our framework for different coalitions (interactions) be-tween phrases for the review though ganesh is successful in a midlevel sort of way, there’snothing so striking or fascinating or metaphorically significant about his career asto rate two hours of our attention.
the inferred class is positive while the ground truth class isnegative..
8781.0000.3250.1070.2040.364apartfromhelenbonhamcarter,thereisnothingworthyaboutthismovie.0.0280.0240.0220.0440.0230.0150.0170.0180.0250.0350.0210.0190.0190.0150.1410.1370.1130.1190.0890.0580.038andthesurpriseending?!0.0150.0150.0180.0220.0180.0180.055thethoughtofasequelisevenmoreannoying.0.0170.0170.0160.0170.0240.0180.0190.0210.0340.0210.0910.0910.0340.0570.0730.0410.040saveyourmoney,waitforthevideoandignorethattoo.0.0280.0190.0240.0190.0240.0230.0250.0350.0300.0470.0300.0290.0300.3340.0710.1070.1070.0430.0820.0601.0000.6200.380asideforbeingclassicintheaspectofitscheesylinesandterribleacting,thisfilmshouldneverbewatchedunlessyouarelookingforagoodcureforyourinsomnia.0.0270.0220.0230.0350.0190.0130.0130.0110.0130.0380.0130.0120.0140.0140.0130.0150.0160.0200.0180.0170.0300.0250.0150.0130.0120.0130.0120.0150.0150.0130.0150.0580.0170.2680.0300.2920.2410.2540.2190.2370.2190.2070.1610.1820.1420.1670.0270.1150.1540.1040.1410.0910.1290.0510.0280.0420.0870.073ican'timagineanyoneactuallythinkingthiswasa``goodmovie.
''0.0180.0150.0530.0230.0280.0310.0270.0260.0220.0240.0360.0160.0140.0130.0330.3160.2480.2250.1960.1380.1380.1110.090thoughganeshissuccessfulinamidlevelsortofway,there'snothingsostrikingorfascinatingormetaphoricallysignificantabouthiscareerastoratetwohoursofourattention.0.0300.0270.0130.0190.0120.0110.0260.0120.0120.0130.0280.0390.0870.1300.0470.0450.0220.0400.0180.0380.0310.0240.0170.0250.0250.0200.0400.0200.0270.0180.0220.0340.0261.0000.1760.8240.1460.7960.1180.7570.1050.7310.0860.5240.2070.0740.2170.3070.1820.0500.0240.1720.1360.1620.0390.1540.0980.0880.0750.1070.0670.0470.0560.0670.043