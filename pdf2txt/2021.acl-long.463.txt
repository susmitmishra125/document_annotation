automatic icd coding via interactive shared representation networkswith self-distillation mechanism.
tong zhou1,3,∗, pengfei cao1,2, yubo chen1,2, kang liu1,2, jun zhao1,2, kun niu3weifeng chong4, shengping liu41 national laboratory of pattern recognition, institute of automation,chinese academy of sciences2 school of artiﬁcial intelligence university of chinese academy of sciences3 beijing university of posts and telecommunications4 beijing unisound information technology co., ltd{tongzhou21, niukun}@bupt.edu.cn{pengfei.cao,yubo.chen,kliu,jzhao}@nlpr.ia.ac.cn{chongweifeng,liushengping}@unisound.com.
abstract.
the icd coding task aims at assigning codesof the international classiﬁcation of diseasesin clinical notes.
since manual coding is verylaborious and prone to errors, many methodshave been proposed for the automatic icd cod-ing task.
however, existing works either ig-nore the long-tail of code frequency or thenoisy clinical notes.
to address the above is-sues, we propose an interactive shared repre-sentation network with self-distillation mech-anism.
speciﬁcally, an interactive shared rep-resentation network targets building connec-tions among codes while modeling the co-occurrence, consequently alleviating the long-tail problem.
moreover, to cope with the noisytext issue, we encourage the model to focus onthe clinical note’s noteworthy part and extractvaluable information through a self-distillationlearning mechanism.
experimental results ontwo mimic datasets demonstrate the effective-ness of our method..1.introduction.
the international classiﬁcation of diseases (icd)is a healthcare classiﬁcation system launched bythe world health organization.
it contains a uniquecode for each disease, symptom, sign and so on.
analyzing clinical data and monitoring health is-sues would become more convenient with the pro-motion of icd codes (shull, 2019) (choi et al.,2016) (avati et al., 2018).
the icd coding taskaims at assigns proper icd codes to a clinical note.
it has drawn much attention due to the importanceof icd codes.
this task is usually undertaken byexperienced coders manually.
however, the man-ually process is inclined to be labor-intensive and.
*work was done during an internship at national labora-tory of pattern recognition, institute of automation, chineseacademy of sciences..figure 1: an example of automatic icd coding task..error-prone (adams et al., 2002).
a knowledge-able coder with medical experience has to readthe whole clinical note with thousands of words inmedical terms and assigning multiple codes froma large number of candidate codes, such as 15,000and 60,000 codes in the ninth version (icd-9) andthe tenth version (icd-10) of icd taxonomies.
onthe one hand, medical expert with specialized icdcoding skills is hard to train.
on the other hand,it is a challenge task even for professional coders,due to the large candidate code set and tedious clin-ical notes.
as statistics, the cost incurred by codingerrors and the ﬁnancial investment spent on improv-ing coding quality are estimated to be $25 billionper year in the us (lang, 2007)..automatic icd coding methods (stanﬁll et al.,2010) have been proposed to resolve the deﬁciencyof manual annotation, regarding it as a multi-labeltext classiﬁcation task.
as shown in figure 1, givena plain clinical text, the model tries to predict all thestandardized codes from icd-9.
recently, neuralnetworks were introduced (mullenbach et al., 2018)(falis et al., 2019) (cao et al., 2020) to alleviate thedeﬁciency of manual feature engineering processof traditional machine learning method (larkey andcroft, 1996) (perotte et al., 2014) in icd codingtask, and great progresses have been made.
al-though effective, those methods either ignore the.
proceedingsofthe59thannualmeetingoftheassociationforcomputationallinguisticsandthe11thinternationaljointconferenceonnaturallanguageprocessing,pages5948–5957august1–6,2021.©2021associationforcomputationallinguistics5948long-tail distribution of the code frequency or nottarget the noisy text in clinical note.
in the follow-ing, we will introduce the two characteristics andthe reasons why they are critical for the automaticicd coding.
long-tail: the long-tail problem isunbalanced data distribution phenomenon.
andthis problem is particularly noticeable in accompa-nied by a large target label set..according to our statistics, the proportion of thetop 10% high-frequency codes in mimic-iii (john-son et al., 2016) occupied 85% of total occurrence.
and 22% of the codes have less than two annotatedsamples.
this is intuitive because people usuallycatch a cold but seldom have cancer.
trained withthese long-tail data, neural automatic icd codingmethod would inclined to make wrong predictionswith high-frequency codes.
fortunately, intrinsicrelationships among different diseases could be uti-lized to mitigate the deﬁciency caused by long-tail.
for example, polyneuropathy in diabetes is a com-plication of diabetes, with a lower probability thanother complications since the long term effect ofvessel lesion reﬂect at nerve would come out in thelate-stage.
if a model could learn shared informa-tion between polyneuropathy in diabetes and morecommon diseases diabetes, the prediction spacewould range to a set of complication of diabetes.
further, utilizing the dynamic code co-occurrence,(the cascade relationship among complications ofdiabetes) the conﬁdence of predicting polyneuropa-thy in diabetes is gradually increased with the oc-currence of vessel blockages, angina pectoris, hy-pertorphy of kidney, respectively.
therefore, howto learn shared information with considering dy-namic code co-occurrence characteristics, is a cru-cial and challenging issue..noisy text: the noisy text problem means thatplentiful of information showing in clinical notesare redundant or misleading for icd coding task.
clinical notes are usually written by doctors andnurses with different writing styles, accompaniedby polysemous abbreviations, abundant medicationrecords and repetitive records of physical indica-tors.
according to our statistics1, about 10% ofwords in a clinical note contribute to the code as-sign task, on average.
other words are abundantmedication records and repetitive records of physi-cal indicators.
these words are not just redundantbut also misleading to the icd coding task.
for.
1we randomly select 20 clinical notes in mimic-iii and.
manually highlight the essential words..example, two critical patients with entirely differ-ent diseases could take similar medicines and havesimilar physical indicators in the rescue course.
weargue that the noisy clinical notes are hard to readfor both humans and machines.
training with suchnoisy text would confuse the model about whereto focus on, and make wrong decisions due to thesemantic deviation.
therefore, another challengingproblem is how to deal with the noisy text in icdcoding task..in this paper, we propose an interactive sharedrepresentation network with self-distillationmechanism (isd) to address the above issues..to mitigate the disadvantage caused by the long-tail issue, we extract shared representations amonghigh-frequency and low-frequency codes from clin-ical notes.
codes with different occurrence fre-quencies all make binary decisions based on sharedinformation rather than individually learning atten-tion distributions.
additional experiments indicatethat those shared representations could extract com-mon information relevant to icd codes.
further,we process the shared representations to an interac-tion decoder for polishing.
the decoder additionalsupervised by two code completion tasks to en-sure the dynamic code co-occurrence patterns werelearned..to alleviate the noisy text issue, we further pro-pose a self-distillation learning mechanism to en-sure the extracted shared representations focuson the long clinical note’s noteworthy part.
theteacher part makes predictions through constructedpuriﬁed text with all crucial information; mean-while, the student part takes the origin clinical noteas a reference.
the student is forced to learn theteacher’s shared representations with identical tar-get codes..the contributions of this paper are as follows:.
1) we propose a framework capable of dealingwith the long-tail and noisy text issues in theicd coding task simultaneously..2) to relieve the long-tail issue, we proposean interactive shared representation network,which can capture the internal connectionsamong codes with different frequencies.
tohandle the noisy text, we devise a self-distillation learning mechanism, guiding themodel focus on important parts of clinicalnotes..3) experiments on two widely used icd codingdatasets, mimic-ii and mimic-iii, show our.
5949figure 2: the architecture of interactive shared representation networks..method outperforms state-of-the-art methodsin macro f1 with 4% and 2%, respectively.
the source code is available at www.github.
com/tongzhou21/isd..2 related work.
icd coding is an important task in the limelightfor decades.
feature based methods ﬁrstly broughtto solve this task.
(larkey and croft, 1996) ex-plored traditional machine learning algorithms, in-cluding knn, relevance feedback, and bayesianapplying to icd coding.
(perotte et al., 2014) uti-lized svm for classiﬁcation in consideration of thehierarchy of icd codes.
with the popularity ofneural networks, researchers have proven the effec-tiveness of cnn and lstm in icd coding task.
(mullenbach et al., 2018) propose a convolutionalneural network with an attention mechanism to cap-ture each code’s desire information in source textalso exhibit interpretability.
(xie and xing, 2018)develop tree lstm to utilize code descriptions.
to further improve the performance, customizedstructures were introduced to utilize the code co-occurrence and code hierarchy of icd taxonomies.
(cao et al., 2020) embedded the icd codes intohyperbolic space to explore their hierarchical na-ture and constructed a co-graph to import codeco-occurrence prior.
we argue that they capturecode co-occurrence in a static manner rather thandynamic multi-hop relations.
(vu et al., 2020) con-sider learning attention distribution for each codeand introduce hierarchical joint learning architec-ture to handle the tail codes.
taking advantageof a set of middle representations to deal with thelong-tail issue is similar to our shared representa-tion setting, while our method enables every labelto choose its desire representation from shared at-tention rather than its upper-level node, with moreﬂexibility..the direct solution to deal with an imbalancelabel set is re-sampling the training data (japkow-icz and stephen, 2002) (shen et al., 2016) or re-weighting the labels in the loss function (wang.
et al., 2017) (huang et al., 2016).
some studiestreat the classiﬁcation of tail labels as few-shotlearning task.
(song et al., 2019) use gan to gen-erate label-wise features according to icd code de-scriptions.
(huynh and elhamifar, 2020) proposedshared multi-attention for multi-label image label-ing.
our work further constructs a label interactionmodule for label relevant shared representation toutilize dynamic label co-occurrence..lots of effects tried to normalize noisy texts be-fore inputting to downstream tasks.
(vateekul andkoomsubha, 2016) (joshi and deshpande, 2018)apply pre-processing techniques on twitter data forsentiment classiﬁcation.
(lourentzou et al., 2019)utilized seq2seq model for text normalization.
oth-ers targeted at noisy input in an end2end mannerby designing customized architecture.
(sergio andlee, 2020) (sergio et al., 2020).
different fromprevious works on noisy text, our method neitherneed extra text processing nor bring in speciﬁc pa-rameters..3 method.
this section describes our interactive shared repre-sentation learning mechanism and self-distillationlearning paradigm for icd coding.
figure 2 showsthe architecture of interactive shared representationnetworks and manifest the inference workﬂow ofour method.
we ﬁrst encode the source clinicalnote to the hidden state with a multi-scale con-volution neural network.
then a shared attentionmodule further extracts code relevant informationshared among all codes.
a multi-layer bidirectionaltransformer decoder insert between the shared at-tention representation extraction module and codeprediction, establishes connections among sharedcode relevant representations..3.1 multi-scale convolutional encoder.
we employ convolutional neural networks (cnn)for source text representation because the compu-tation complexity affected by the length of clini-cal notes is non-negligible, although other sequen-.
5950figure 3: the workﬂow of our method (isd) during the training stage.
we take the example of training data witha clinical note and annotated four target codes..tial encoders such as recurrent neural networksor transformer(vaswani et al., 2017) could cap-ture longer dependency of text, theoretically.
cnncould encode local n-gram pattern, critical in textclassiﬁcation, and with high computational efﬁ-ciency.
the words in source text are ﬁrst mappedinto low-dimensional word embedding space, con-stitute a matrix e = {e1, e2, ..., enx}.
note thatnx is the clinical note’s length, e is the word vec-tor with dimension de.
as shown in eq.
1 and2, we concatenate the convolutional representa-tion from kernel set c = {c1, c2, ..., cs} withdifferent size kc to hidden representation matrixh = {h1, h2, ..., hnx} with size nx × dl:.
hcji = tanh(wc ∗ xi:i+kcj −1 + bcj ).
hi = {hc0.
i ; hc1.
i ; ...; hcsi }.
(1).
(2).
3.2 shared attention.
the label attention method tends to learn relevantdocument representations for each code.
we ar-gue that the attention of rare code could not bewell learned due to lacking training data.
moti-vated by (huynh and elhamifar, 2020) we proposeshared attention to bridge the gap between high-frequency and low-frequency codes by learningshared representations h s through attention.
codeset with total number of nl codes represents in.
1, el.
2, ..., elnl.
code embedding el = {el} accordingto their text descriptions.
a set of trainable sharedqueries for attention with size nq ×dl is introduced,2, ..., eqnoted as eq = {eq}, where nq is thenqtotal number of shared queries as a hyperparameter.
then eq calculates shared attention representationh s = {hs} with hidden representa-tion h in eq.
3 to 5:.
2 , ..., hsnq.
1 , hs.
1, eq.
attention(q, k, v ) = softmax(.
v ).
(3).
qkt√dk.
αi = attention(eq.
i , h, h).
hsi = h · αi.
in ideal conditions, those shared representations re-ﬂect the code relevant information corresponding tothe source text.
we can predict codes through h s.each code i has its right to choose a shared repre-sentation in h s for code-speciﬁc vector throughthe highest dot product score si..si = max(h s · eli).
the product score was further applying to calculatethe ﬁnal score ˆyl through the sigmoid function..ˆyi = σ(si).
with the supervision of binary cross-entropy lossfunction, the shared representation should have.
(4).
(5).
(6).
(7).
5951learned to represent code relevant information..lpred =.
[−yilog(ˆyi) − (1 − yi)log(1 − ˆyi)].
nl(cid:88).
i=1.
(8).
3.3.interactive shared attention.
above shared attention mechanism lacks interac-tion among code relevant information, which isof great importance in the icd coding task.
weimplement this interaction through a bidirectionalmulti-layer transformer decoder d with an addi-tional code completion task.
the shared represen-tation h s is considered the orderless sequentialinput of the decoder d. each layer of the trans-former contains interaction among shared repre-sentation h s through self-attention and interac-tion between shared representation and source textthrough source sequential attention..to make sure the decoder could model the dy-namic code co-occurrence pattern, we propose twocode set completion tasks, shown at the bottom offigure 3..(1) missing code completion: we construct acode sequence ltgt of a real clinical note x in thetraining set, randomly masking one code lmis.
thedecoder takes this code sequence as input to predictthe masked code..lmis = −logp (lmis|ltgt \ lmis ∪ lmask, x) (9).
(2) wrong code removal: similar to the abovetask, we construct a code sequence ltgt, but byrandomly adding a wrong code lwro.
the decoderis aiming to fade the wrong code’s representationwith a special mask representation lmask..lrem = −logp (lmask|ltgt ∪ lwro, x).
(10).
the decoder could generate puriﬁcatory code rel-evant information with higher rationality with theabove two tasks’ learning.
the decoder is pluggedto reﬁne the shared representation h s to h s(cid:48), sothe subsequent dot product score is calculated byh s(cid:48)..si = max(h s(cid:48) · eli).
(11).
3.4 self-distillation learning mechanism.
we argue that learning the desired shared attentiondistribution over such a long clinical text is difﬁcult,and the αi tends to be smooth, brings lots of unnec-essary noise information.
therefore we propose aself-distillation learning mechanism showing in the.
gray dotted lines of figure 3. with this mechanism,the model could learn superior intermediate repre-sentations from itself without introducing anothertrained model..1, x l.2, ..., x l.considering a single clinical note x with tar-get code set ltgt for training, we derive two pathsinputted to the model.
the teacher’s trainingdata consists of the text descriptions x ltgt ={x l}.
we handle those code de-scriptions separately through the encoder and con-catenate them into a ﬂat sequence of hidden statelnltgt }, where nltgt ish ltgt = {h l1; h l2; ...; hthe number of code in ltgt, so the subsequent pro-cess in our model is not affected..nltgt.
we optimize the teacher’s prediction result ˆytgt.
i.through binary cross-entropy loss..nl(cid:88).
i=1.
ltgt =.
[−yilog(ˆytgt.
i ) − (1 − yi)log(1 − ˆytgti )].
(12)student takes origin clinical note xas input andalso have bce loss to optimize.
we assume thatan origin clinical note with thousands of wordscontains all desired codes’ information, as well asless essential words.
the teacher’s input containsall desired information that indicates codes to bepredicted without any noise.
ideal shared repre-sentations obtained from attention are supposed tocollect code relevant information only.
hence wetreat the teacher’s share representation h ltgt as aperfect example to the student.
a distillation lossencourages those two representation sequences tobe similar..n(cid:88).
cosine(h a, h b) =.
i (cid:107)ldist = min{1 − cosine(h s(cid:48), h ltgt(cid:48))}.
i.
(cid:107) ha.
ha· hbiii (cid:107) (cid:107) hb.
(13).
(14).
since we treat the shared representations with-out order restrict, every teacher have its rights tochoose a suitable student, meanwhile, consider-ing other teachers’ appropriateness.
it implementswith hungarian algorithm (kuhn, 1955) to calcu-lates the cosine distance globally minimum.
where(cid:48) denotes any shufﬂe version of the origin represen-tation sequence..3.5 training.
the complete training pipeline of our method isshown in figure 3. the ﬁnal loss function is the.
5952model.
auc.
mimic-iii-fullf1.
auc.
mimic-iii 50f1.
camldr-camlmsatt-kgmultirescnnhypercorelaatjointlaat.
isd (ours).
p@8.p@5.macro0.6090.8950.6180.8970.6440.9100.6410.9100.6320.9300.6750.9190.6710.9210.9380.682±0.003 ±0.003 ±0.002 ±0.002 ±0.001 ±0.004 ±0.001 ±0.009 ±0.003 ±0.005.
macro0.5320.5760.6380.6060.6090.6660.6610.679.macro0.8750.8840.9140.8990.8950.9250.9250.935.macro0.0880.0860.0900.0850.0900.0990.1070.119.micro0.5390.5290.5530.5520.5510.5750.5750.559.micro0.9090.9160.9360.9280.9290.9460.9460.949.micro0.6140.6330.6840.6700.6630.7150.7160.717.micro0.9860.9850.9920.9860.9890.9880.9880.990.
0.7090.6900.7280.7340.7220.7380.7350.745.table 1: comparison of our model and other baselines on the mimic-iii dataset.
we run our model 10 times andeach time we use different random seeds for initialization.
we report the mean ± standard deviation of each result..p@8.
4.2 metrics and parameter settings.
model.
ha-grucamldr-camlmultirescnnhypercorelaatjointlaat.
isd (ours).
auc.
f1.
macro micro macro micro0.3660.4420.4570.4640.4770.4860.4910.498.
---0.5230.0480.8200.5150.0490.8260.5440.0520.8500.5370.0700.8850.5500.0590.8680.5510.0680.8710.9010.5640.101±0.004 ±0.002 ±0.004 ±0.002 ±0.002.
-0.9660.9660.9680.9710.9730.9720.977.table 2: experimental results are shown in means ±standard deviations on the mimic-ii dataset..weighting sum of the above losses..l = λpredlpred+λmislmis + λremlrem+.
λtgtltgt + λdistldist.
(15).
4 experiments.
4.1 datasets.
for fair comparison, we follow the datasets usedby previous work on icd coding (mullenbachet al., 2018) (cao et al., 2020), including mimic-ii (jouhet et al., 2012) and mimic-iii (johnsonet al., 2016).
the third edition is the extension ofii.
both datasets contain discharge summaries thatare tagged manually with a set of icd-9 codes.
the dataset preprocessing process is consistentwith (mullenbach et al., 2018).
for mimic-iiifull dataset, there are 47719, 1631, 3372 differentpatients’ discharge summaries for training, devel-opment, and testing, respectively.
totally 8921unique codes occur in those three parts.
mimic-iii 50 dataset only retains the most frequent codesappear in full setting, leave 8067, 1574, 1730 dis-charge summaries for training, development, andtesting, respectively.
mimic-ii dataset contains5031 unique codes divided into 20533 and 2282clinical notes for training and testing, respectively..as in previous works (mullenbach et al., 2018),we evaluate our method using both the micro andmacro, f1 and auc metrics.
as well as p@8 in-dicates the proportion of the correctly-predictedcodes in the top-8 predicted codes.
pytorch(paszke et al., 2019) is chosen for our method’simplementation.
we perform a grid search over allhyperparameters for each dataset.
the parameterselections are based on the tradeoff between val-idation performance and training efﬁciency.
weset the word embedding size to 100. we build thevocabulary set using the cbow word2vec method(mikolov et al., 2013) to pre-train word embed-dings based on words in all mimic data, resultingin the most frequent 52254 words included.
themulti-scale convolution ﬁlter size is 5, 7, 9, 11,respectively.
the size of each ﬁlter output is one-quarter of the code embedding size.
we set codeembedding size to 128 and 256 for the mimic-iiand mimic-iii, respectively.
the size of sharedrepresentation is 64. we utilize a two-layer trans-former for the interactive decoder.
for the lossfunction, we set λmis = 0.5, λmis = 5e − 4,λrem = 5e − 4, λtgt = 0.5, and λdist = 1e − 3 toadjust the scale of different supervisory signals.
weuse adam for optimization with an initial learningrate of 3e-4, and other settings keep the default..4.3 baselines.
we compare our method with the following base-lines:.
ha-gru: a hierarchical attention gated re-current unit model is proposed by (baumel et al.,2017) to predict icd codes on the mimic-iidataset..caml & dr-caml: (mullenbach et al.,2018) proposed the convolutional attention net-.
5953model.
isd (ours)w/o distillation lossw/o self-distillationw/o code completion taskw/o co-occurrence decoder.
auc.
f1.
macro micro macro micro0.5590.9380.5510.9350.5470.9340.5220.9310.5470.936.
0.1190.1030.0990.0610.084.
0.9900.9860.9810.9880.989.p@8.
0.7450.7430.7240.7280.743.table 3: ablation results on the mimic-iii-full test set..work for multi-label classiﬁcation (caml),which learning attention distribution for each la-bel.
dr-caml indicates description regularizedcaml, an extension incorporating the text descrip-tion of codes..msatt-kg: the multi-scale feature atten-tion and structured knowledge graph propagationwas proposed by (xie et al., 2019) they capturevariable n-gram features and select multi-scale fea-tures through densely connected cnn and a multi-scale feature attention mechanism.
gcn is alsoemployed to capture the hierarchical relationshipsamong medical codes..multirescnn: the multi-filter residual con-volutional neural network was proposed by (liand yu, 2020).
they utilize the multi-ﬁlter convo-lutional layer capture variable n-gram patterns andresidual mechanism to enlarge the receptive ﬁeld.
hypercore: hyperbolic and co-graph repre-sentation was proposed by (cao et al., 2020).
theyexplicitly model code hierarchy through hyper-bolic embedding and learning code co-occurrencethought gcn..laat & jointlaat: (vu et al., 2020) labelattention model (laat) for icd coding was pro-posed by (vu et al., 2020), learning attention dis-tributions over lstm encoding hidden states foreach code.
jointlaat is an extension of laatwith hierarchical joint learning..4.4 compared with state-of-the-art methods.
the left part of table 1 and table 2 show the resultsof our method on the mimic-iii and mimic-iidataset with the whole icd code set.
comparedwith previous methods generating attention distri-bution for each code, our method achieves betterresults on most metrics, indicating the shared atten-tion mechanism’s effectiveness.
it is noteworthythat the macro results have more signiﬁcant im-provement compare to micro than previous meth-ods.
since the macro indicators are mainly affectedby tail codes’ performance, our approach beneﬁts.
from the interactive shared representations amongcodes with different frequencies..compared with the static code interaction of co-occurrence implemented in (cao et al., 2020), ourmethod achieves higher scores, indicating that thedynamic code interaction module could capturemore complex code interactive information otherthan limit steps of message passing in gcn..the right part of table 1 shows the results of ourmethod on the mimic-iii dataset with the mostfrequent 50 codes.
it proved that our approach’sperformance would not fall behind with a morebalanced label set..4.5 ablation experiments.
to investigate the effectiveness of our proposedcomponents of the method, we also perform the ab-lation experiments on the mimic-iii-full dataset.
the ablation results are shown in table 3, indicat-ing that none of these models can achieve a compa-rable result with our full version.
demonstrate thatall those factors contribute a certain improvementto our model..(w/o self-distillation),.
(1) effectiveness of self-distillation.
speciﬁ-cally, when we discard the whole self-distillationpartthe performancedrops, demonstrate the effectiveness of the self-distillation.
to further investigate the contributionof the self-distillation module, whether the moretraining data we constructed, we retain the teacherpath and remove the loss between shared represen-tations (w/o distillation loss), the performance stillslightly drops.
it can be concluded that althoughthe positive effects of the constructed training datain the teacher path, the distillation still plays a role.
(2) effectiveness of shared representation.
when we remove the self-distillation mechanism(w/o self-distillation), the contribution of sharedrepresentation part can be deduced compared to theperformance of caml.
result showing our versionstill have 1.1% advantage in macro f1, indicatingthe effectiveness of shared representation..5954size.
132641281159.auc.
f1.
macro micro macro micro0.5320.8990.5570.9370.5590.9380.5580.9380.5430.935.
0.9800.9900.9900.9880.990.
0.0810.1040.1190.1240.116.p@8.
0.7230.7370.7450.7430.731.table 4: experimental results of our method with dif-ferent size of shared representations on mimic-iii-fulldataset..(3) effectiveness of code completion task.
when we neglect the missing code completion taskand wrong code removal task (w/o code comple-tion tasks), the code interactive decoder optimizeswith ﬁnal prediction loss only.
the performance iseven worse than the model without the whole codeinteraction module (w/o co-occurrence decoder).
it indicates that the additional code completiontask is the guarantee of modeling dynamic codeco-occurrence characteristics.
further comparedwith the model with label attention rather than ourproposed shared representations (w/o shared repre-sentation), the performance even worse, showingthe code completion task is also the guarantee ofthe effectiveness of shared representations.
with-out this self-supervised task, the shared informationis obscure and the performance drops due to thejoin of dubiously oriented model parameters..4.6 discussion.
to further explore our proposed interactive sharedattention mechanism, we conduct comparisonsamong various numbers of shared representationsin our method.
and visualization the attentiondistribution over source text of different sharedrepresentations, as well as the information theyextracted..(1) the analysis of shared representationssize.
as shown in table 4, both large or smallsize would harm the ﬁnal performance.
when theshared size is set to 1, the shared representationdegrades into a global representation.
a singlevector compelled to predict multiple codes causesthe performance drops, as table 4 shows.
we alsoinitialize the shared embeddings with icd’s hier-archical parent node.
speciﬁcally, there are 1159unique ﬁrst three characters in the raw icd codeset of mimic-iii-full.
we initialize those sharedembeddings with the mean vector of their corre-sponding child codes.
although the hierarchicalpriori knowledge is introduced, the computation.
clinical note: chief complaint elective admitmajor surgical or invasive procedure recoilingacomm aneurysm history of present illness onshe had a crushing headache but stayed at homethe next day ... angiogram with embolizationand or stent placement medication take aspirin325mg ...codes:437.3 (cerebral aneurysm, nonruptured);39.75 (endovascular repair of vessel);88.41 (arteriography of cerebral arteries).
table 5: the attention distribution visualization overa clinical note of different shared representations.
wedetermine the shared representations according to thetarget codes’ choice.
since we calculate the attentionscore over hidden states encoded by multi-scale cnn,we take the most salient word as the center word of 5-gram and highlight..modelisd (ours)w/o self-distillation.
standard deviation0.0139920.004605.table 6: the average standard deviation calculatedfrom the attention weights of clinical text in mimic-iii-full dataset..complexity and uneven node selection could causethe model to be hard to optimize and overﬁt highfrequent parent nodes..(2) visualization of shared attention distri-bution.
the attention distribution of differentshared representations shown in table 5 indicatesthat they have learned to focus on different sourcetext patterns in the noisy clinical note to representcode relevant information..(3) the analysis of self-distillation.
as shownin table 6, the attention weights over clinicaltext learned by model with the training of self-distillation mechanism are more sharp than originlearning process.
in combination with table 5, itcan be concluded that the self-distillation mecha-nism could help the model more focus on the desirewords of clinical text..5 conclusion.
this paper proposes an interactive shared represen-tation network and a self-distillation mechanismfor the automatic icd coding task, to address thelong-tail and noisy text issues.
the shared repre-sentations can bridge the gap between the learning.
5955process of frequent and rare codes.
and the codeinteraction module models the dynamic code co-occurrence characteristic, further improving theperformance of tail codes.
moreover, to addressthe noisy text issue, the self-distillation learningmechanism helps the shared representations focuson code-related information in noisy clinical notes.
experimental results on two mimic datasets indi-cate that our proposed model signiﬁcantly outper-forms previous state-of-the-art methods..acknowledgments.
this work is supported by the national keyresearch and development program of china(no.2017yfb1002101), the national natural sci-61806201,ence foundation of china (no.
61976211).
this work is also supportedby beijing academy of artiﬁcial intelligence(baai2019qn0301), the key research programof the chinese academy of sciences (grantno.
zdbs-ssw-jsc006), independent researchproject of national laboratory of pattern recogni-tion and the ccf-tencent open research fund..references.
diane l adams, helen norman, and valentine j bur-roughs.
2002. addressing medical coding andbilling part ii: a strategy for achieving compliance.
arisk management approach for reducing coding andbilling errors.
journal of the national medical asso-ciation, 94(6):430..anand avati, kenneth jung, stephanie harman,lance downing, andrew ng, and nigam h shah.
improving palliative care with deep learn-2018.ing.
bmc medical informatics and decision making,18(4):122..tal baumel, jumana nassour-kassis, raphael co-hen, michael elhadad, and noémie elhadad.
2017.multi-label classiﬁcation of patient notes a casearxiv preprintstudy on icd code assignment.
arxiv:1709.09587..pengfei cao, yubo chen, kang liu, jun zhao, sheng-ping liu, and weifeng chong.
2020. hypercore:hyperbolic and co-graph representation for auto-in proceedings of the 58th an-matic icd coding.
nual meeting of the association for computationallinguistics, pages 3105–3114..edward choi, mohammad taha bahadori, andyschuetz, walter f stewart, and jimeng sun.
2016.doctor ai: predicting clinical events via recurrentneural networks.
in machine learning for health-care conference, pages 301–318..matus falis, maciej pajak, aneta lisowska, patrickschrempf, lucas deckers, shadia mikhael, sotiriostsaftaris, and alison o’neil.
2019. ontological at-tention ensembles for capturing semantic conceptsin icd code prediction from clinical text.
in proceed-ings of the tenth international workshop on healthtext mining and information analysis, pages 168–177..chen huang, yining li, chen change loy, and xiaooutang.
2016. learning deep representation for im-balanced classiﬁcation.
in proceedings of the ieeeconference on computer vision and pattern recogni-tion, pages 5375–5384..dat huynh and ehsan elhamifar.
2020. a shared multi-attention framework for multi-label zero-shot learn-in proceedings of the ieee/cvf conferenceing.
on computer vision and pattern recognition, pages8776–8786..nathalie japkowicz and shaju stephen.
2002. thein-class imbalance problem: a systematic study.
telligent data analysis, 6(5):429–449..alistair ew johnson, tom j pollard, lu shen,h lehman li-wei, mengling feng, moham-mad ghassemi, benjamin moody, peter szolovits,leo anthony celi, and roger g mark.
2016. mimic-iii, a freely accessible critical care database.
scien-tiﬁc data, 3(1):1–9..shaunak joshi and deepali deshpande.
2018. twit-arxiv preprint.
ter sentiment analysis system.
arxiv:1807.07752..vianney jouhet, georges defossez, anita burgun,pierre le beux, p levillain, pierre ingrand, vin-cent claveau, et al.
2012. automated classiﬁcationof free-text pathology reports for registration of in-cident cases of cancer.
methods of information inmedicine, 51(3):242..harold w kuhn.
1955. the hungarian method for theassignment problem.
naval research logistics quar-terly, 2(1-2):83–97..dee lang.
2007. consultant report-natural languageprocessing in the health care industry.
cincinnatichildren’s hospital medical center, winter, 6..leah s larkey and w bruce croft.
1996. combiningclassiﬁers in text categorization.
in proceedings ofthe 19th annual international acm sigir confer-ence on research and development in informationretrieval, pages 289–297..fei li and hong yu.
2020..icd coding from clinicaltext using multi-ﬁlter residual convolutional neuralnetwork.
in proceedings of the aaai conference onartiﬁcial intelligence, volume 34, pages 8180–8187..ismini lourentzou, kabir manghnani, and chengxi-ang zhai.
2019. adapting sequence to sequencemodels for text normalization in social media.
inproceedings of the international aaai conference.
5956peerapon vateekul and thanabhat koomsubha.
2016.a study of sentiment analysis using deep learningtechniques on thai twitter data.
in 2016 13th inter-national joint conference on computer science andsoftware engineering (jcsse), pages 1–6.
ieee..thanh vu, dat quoc nguyen, and anthony nguyen.
2020. a label attention model for icd coding fromclinical text.
arxiv preprint arxiv:2007.06351..yu-xiong wang, deva ramanan, and martial hebert.
in advances2017. learning to model the tail.
in neural information processing systems, pages7029–7039..pengtao xie and eric xing.
2018. a neural architec-ture for automated icd coding.
in proceedings of the56th annual meeting of the association for compu-tational linguistics, pages 1066–1076..xiancheng xie, yun xiong, philip s yu, and yangy-ong zhu.
2019. ehr coding with multi-scale featureattention and structured knowledge graph propaga-tion.
in proceedings of the 28th acm internationalconference on information and knowledge manage-ment, pages 649–658..on web and social media, volume 13, pages 335–345..tomas mikolov, kai chen, greg corrado, and jef-efﬁcient estimation of wordarxiv preprint.
frey dean.
2013.representations in vector space.
arxiv:1301.3781..james mullenbach, sarah wiegreffe, jon duke, jimengsun, and jacob eisenstein.
2018. explainable pre-diction of medical codes from clinical text.
in pro-ceedings of the 2018 conference of the north amer-ican chapter of the association for computationallinguistics: human language technologies, vol-ume 1, pages 1101–1111..adam paszke, sam gross, francisco massa, adamlerer, james bradbury, gregory chanan, trevorkilleen, zeming lin, natalia gimelshein, lucaantiga, et al.
2019. pytorch: an imperative style,in ad-high-performance deep learning library.
vances in neural information processing systems,pages 8026–8037..adler perotte, rimma pivovarov, karthik natarajan,nicole weiskopf, frank wood, and noémie elhadad.
2014. diagnosis code assignment: models and eval-uation metrics.
journal of the american medical in-formatics association, 21(2):231–237..gwenaelle cunha sergio and minho lee.
2020.stacked debert: all attention in incomplete data fortext classiﬁcation.
neural networks..gwenaelle cunha sergio, dennis singh moirangthem,and minho lee.
2020. attentively embracing noisefor robust latent representation in bert.
in proceed-ings of the 28th international conference on com-putational linguistics, pages 3479–3491..li shen, zhouchen lin, and qingming huang.
2016.relay backpropagation for effective learning of deepconvolutional neural networks.
in european confer-ence on computer vision, pages 467–482.
springer..jessica germaine shull.
2019. digital health andthe state of interoperable electronic health records.
jmir medical informatics, 7(4):e12712..congzheng song,.
shanghang zhang, najmehsadoughi, pengtao xie, and eric xing.
2019.generalized zero-shot icd coding.
arxiv preprintarxiv:1909.13154..mary h stanﬁll, margaret williams, susan h fen-ton, robert a jenders, and william r hersh.
2010.a systematic literature review of automated clin-journalical coding and classiﬁcation systems.
of the american medical informatics association,17(6):646–651..ashish vaswani, noam shazeer, niki parmar, jakobuszkoreit, llion jones, aidan n gomez, łukaszkaiser, and illia polosukhin.
2017. attention is allin advances in neural information pro-you need.
cessing systems, pages 5998–6008..5957