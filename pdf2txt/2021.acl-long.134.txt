i like ﬁsh , especially dolphins.
:∗.
addressing contradictions in dialogue modeling.
yixin nie1, mary williamson2, mohit bansal1, douwe kiela2, jason weston21unc chapel hill2facebook ai research.
abstract.
to quantify how well natural language un-derstanding models can capture consistencyin a general conversation, we introduce thedialogue contradiction detection task (de-code) and a new conversational dataset con-taining both human-human and human-botcontradictory dialogues.
we show that: (i) ournewly collected dataset is notably more effec-tive at providing supervision for the dialoguecontradiction detection task than existing nlidata including those aimed to cover the dia-logue domain; (ii) transformer models that ex-plicitly hinge on utterance structures for dia-logue contradiction detection are more robustand generalize well on both analysis and out-of-distribution dialogues than standard (un-structured) transformers.
we also show thatour best contradiction detection model corre-lates well with human judgments and furtherprovide evidence for its usage in both auto-matically evaluating and improving the consis-tency of state-of-the-art generative chatbots..1.introduction.
recent progress on neural approaches to naturallanguage processing (devlin et al., 2019; brownet al., 2020), and the availability of large amountsof conversational data (lowe et al., 2015; smithet al., 2020) have triggered a resurgent inter-est on building intelligent open-domain chatbots.
newly developed end-to-end neural bots (zhanget al., 2020; adiwardana et al., 2020; roller et al.,2020) are claimed to be superior to their prede-cessors (worsnick, 2018; zhou et al., 2020) usingvarious human evaluation techniques (see et al.,2019; li et al., 2019; adiwardana et al., 2020) thataim to give a more accurate measure of what makesa good conversation.
while the success is indis-putable, there is still a long way to go before we.
∗* dolphins are mammals, not ﬁsh..figure 1: contradictory dialogues contained in our newdecode dataset.
the main train, valid and test setscontain human-written dialogues with deliberate con-tradictions (example at top), and an out-of-domain testset consists of labeled human-bot dialogues (bottom),involving state-of-the-art bots (roller et al., 2020)..arrive at human-like open-domain chatbots.
forexample, it has been shown that open-domain chat-bots frequently generate annoying errors (adiwar-dana et al., 2020; roller et al., 2020) and a notori-ous one among these is the class of contradiction,or consistency errors..when interacting with chatbots, people carryover many of the same expectations as when inter-acting with humans (nass and moon, 2000).
self-contradictions by these bots (see fig.1, bottom)are often jarring, immediately disrupt the conver-.
proceedingsofthe59thannualmeetingoftheassociationforcomputationallinguisticsandthe11thinternationaljointconferenceonnaturallanguageprocessing,pages1699–1713august1–6,2021.©2021associationforcomputationallinguistics1699humanhumanhumanblenderbot 2.7bsational ﬂow, and help support arguments aboutwhether generative models could ever really under-stand what they are saying at all (marcus, 2018).
from a listener’s perspective, such inconsistentbots fail to gain user trust and their long-term com-munication conﬁdence.
from a speaker’s perspec-tive, it violates the maxim of quality in grice’scooperative principles (grice, 1975) —”do not saywhat you believe to be false.” hence, efforts on re-ducing contradicting or inconsistent conversationsby open-domain chatbots are imperative..prior works (welleck et al., 2019) characterizedthe modeling of persona-related consistency as anatural language inference (nli) problem (daganet al., 2005; bowman et al., 2015), and constructeda dialog nli dataset based on persona-chat (zhanget al., 2018), but so far state-of-the-art chatbots(roller et al., 2020) have not been able to makeuse of nli techniques in improving dialogue con-sistency.
overall, the challenge remains that weare still unable to answer the simple yet importantquestion—“how good are we at modeling consis-tency (including persona, logic, causality, etc.)
ina general conversation?”.
the inability to measurethis obscures to what degree building new modulesor techniques can in turn help prevent contradictingresponses during generation..seeking to answer this question, we introducethe dialogue contradiction detection task (de-code)1 and collect a new conversational datasetcontaining human written dialogues where oneof the speakers deliberately contradicts what theyhave previously said at a certain point during theconversation.
we also collect an out-of-distribution(ood) set of dialogues in human-bot interac-tive settings which contain human-labeled self-contradictions made by different chatbots..we then compare a set of state-of-the-art sys-tems, including a standard unstructured approachand a proposed structured approach for utilizingnli models to detect contradictions.
in the unstruc-tured approach, a transformer nli model directlytakes in the concatenation of all utterances of the in-put dialogue for prediction, following the paradigmof nlu modeling.
in the structured approach, ut-terances are paired separately before being fed intotransformer nli models, explicitly taking accountof the natural dialogue structure..results reveal that:.
(1) our newly collected.
1decode dataset and code are publicly available at.
https://parl.ai/projects/contradiction..dataset is notably more effective at providing su-pervision for the contradiction detection task thanexisting nli data including those aimed at coveringthe dialogue domain; (2) the structured utterance-based approach for dialogue consistency modelingis more robust in our analysis and more transfer-able to ood human-bot conversation than the un-structured approach.
this ﬁnding challenges themainstream unstructured approach of simply apply-ing pre-trained transformer models and expectingthem to learn the structure, especially for ood sce-narios which are often the case when incorporatingnlu modules into nlg systems, since intermedi-ate in-domain data are scarce..finally, with such improvements on the contra-diction detection task, we show that our best result-ing detector correlates well with human judgmentsand can be suitable for use as an automatic met-ric for checking dialogue consistency.
we furtherprovide evidence for its usage in improving theconsistency of state-of-the-art generative chatbots..2 related work.
several prior works on improving dialogue con-sistency have explored using direct modeling ofthe dialogue context in generation algorithms.
the modeling can be implicit where the dialogueconsistency-related information like style (wanget al., 2017), topics, or personal facts are main-tained in distributed embeddings (li et al., 2016;zhang et al., 2019a), neural long-term memo-ries (bang et al., 2015), hierarchical neural archi-tecture (serban et al., 2016), latent variables (ser-ban et al., 2017), topical attention (dziri et al.,2019a), or even self-learned feature vectors (zhanget al., 2019b).
some works have grounded gen-eration models on explicit user input (qian et al.,2018), or designated personas (zhang et al., 2018).
although, improvements on automatic generationmetrics were often shown on guided response gen-eration based on the consistency modeling, the is-sue of contradiction has never been resolved, norhave generally applicable methods to gauge theconsistency improvements been developed.
fur-ther, simply scaling models has not made the prob-lem go away, as is evident in the largest chatbotstrained such as blenderbot with up to 9.4b param-eter transformers (roller et al., 2020)..more similar to our work is utilizing nli mod-els in dialogue consistency.
dziri et al.
(2019b)attempted to use entailment models trained on syn-.
1700thetic datasets for dialogue topic coherence eval-uation.
particularly, welleck et al.
(2019) con-structed the dialogue nli dataset and (li et al.,2020) utilized it to try to reduce inconsistency ingenerative models via unlikelihood training in apreliminary study that reports perplexity results,but did not measure actual generations or contra-diction rates.
we note that the dialogue nli datasetis only semi-automatically generated, with limitedcoverage of only persona-chat data (zhang et al.,2018), whereas our decode is human-writtenand across multiple domains.
our task also in-volves logical and context-related reasoning be-yond personal facts.
we show that transfer of de-code is subsequently more robust than dialoguenli on both human-human and human-bot chats..3 task and data.
3.1 dialogue contradiction detection.
we formalize dialogue contradiction detection asa supervised classiﬁcation task.
the input of thetask is a list of utterances x = {u0, u1, u2, ..., un}representing a dialogue or a dialogue snippet.
theoutput is y, indicating whether the last utteranceun contradicts any previously conversed informa-tion contained in the dialogue {u0, u1, ..., un−1},where y can be 0 or 1 corresponding to the non-contradiction and the contradiction label respec-tively.
preferably, the output should also includea set of indices i ⊆ {0, 1, ..., n − 1} represent-ing a subset of {u0, u1, ..., un−1} which containinformation that is actually contradicted by the lastutterance un.
the extra indices i output requiremodels to pinpoint the evidence for the contradic-tion, providing an extra layer of explainability..3.2 data collection.
our goal is ﬁrst to collect training and evaluationdata for this task.
we thus collect dialogues inwhich the last utterance contradicts some previousutterances in the dialogue history.
to obtain suchdialogues, we give annotators dialogue snippetsfrom pre-selected dialogue corpora, and then askthem to continue the conversation by writing oneor two utterances such that the last utterance by thelast speaker contradicts the dialogue history.
wealso ask annotators to mark all the utterances in thedialogue history that are involved in the contradic-tion as supporting evidence.
we ask annotators towrite contradicting utterances based partly on exist-ing dialogues rather than collecting new dialogue.
from scratch because the provided dialogues canoften convey semantic-rich contexts from differentdomains and inspire annotators to write more di-verse examples.
we don’t impose constraints on theannotation such that the annotator could have theﬂexibility to write more diverse contradictory re-sponses that might not belong to pre-deﬁned types(knowledge, emotion, persona, etc).
also note thatwe ask the annotator to write contradictory dia-logues based on pre-selected human-human dia-logue rather than collecting dialogues from human-bot interaction for the main dataset because wewant the examples to be general and less bound tospeciﬁc bots.2 we crowdsource the continuationand annotation data with amazon mechanical turkvia parlai (miller et al., 2017)..to ensure data quality, we apply three tech-niques: (i) an onboarding test every annotator hasto pass to contribute examples; (ii) each annotatorcan only create up to 20 examples; and (iii) all ex-amples in the validation and test set are veriﬁed byasking 3 additional workers.
more details aboutannotation are provided in appendix..3.3 dataset.
we collected 17,713 human-written contradictingdialogues in which 4,121 are veriﬁed by 3 anno-tators.
the pre-selected dialogue source corporaare wizard of wikipedia (dinan et al., 2019),empatheticdialogues (rashkin et al., 2019),blended skill talk (smith et al., 2020), and con-vai2 (dinan et al., 2020), covering various con-versational topics.
to facilitate the evaluation ofconsistency modeling on the dialogue contradic-tion detection classiﬁcation task, we sample anequal number of non-contradicting dialogues ac-cording to the same dialogue length distributionas the contradicting ones from the same dialoguecorpus.
then, we make the splits such that thetrain split contains unveriﬁed examples, and devand test splits only contain veriﬁed examples.
eachsplit has balanced labels between contradiction andnon-contradiction.
the breakdown of each of thedataset sources is shown in appendix..auxiliary (checklist) test sets.
we further cre-ate two auxiliary checklist evaluation sets by trans-forming the contradiction examples in the originaltest in two ways such that the ground truth label is.
2alongside the main dataset, another portion of the ex-amples are collected via human-bot interaction and used asout-of-domain evaluation..1701main (train)main (dev)main (test).
human-bot (test).
a2t (test)rct (test).
count.
27,1844,0264,216.
764.
2,0792,011.label.
balancedbalancedbalanced.
balanced.
contradictionnon-contradiction.
table 1: decode dataset summary.
the ﬁrst columnpresents the different dataset types.
“main” is the col-lected human-written dialogues.
“balanced” indicatesthat the contradiction and non-contradiction labels inthat part of the dataset are balanced.
a2t and rct arethe auxiliary test sets described in subsection 3.3..either invariant or expected to ﬂip.
the two resul-tant sets serve as diagnostic tests on the behavior,generalization and transferability of our models.
the transformations are described below:.
• add two turns (a2t) we insert a pair of ran-domly sampled utterances into the dialogue suchthat the inserted utterances are between the twooriginal contradicting utterances.
this gives anew contradicting dialogue with a longer dia-logue history..• remove contradicting turns (rct) we re-move all the turns (all pairs of utterances)3marked as supporting evidence for the contra-diction in the dialogue except the last utterance.
this results in a new non-contradiction dialogue..human-bot test set.
our main dataset involveshuman-written dialogues containing contradictingutterances based on human-human dialogues fromexisting corpora.
in practice, to evaluate the re-sponse quality of a machine rather than a human interms of its consistent responses, we care about howwell a contradiction detector can perform in human-bot interactive conversations.
to that end, we fur-ther collect human-bot dialogue data by employingcrowdworkers to interact with a diverse set of open-domain bots.
these include poly-encoder (humeauet al., 2019) based retrieval models, generative mod-els (roller et al., 2020), unlikelihood trained mod-els (li et al., 2020), retrieve-and-reﬁne models (we-ston et al., 2018; roller et al., 2020), models eitherpre-trained on a previously existing reddit dataset.
extracted and obtained by a third party that washosted by pushshift.io (baumgartner et al., 2020)or ﬁne-tuned on the blended skill talk (bst) di-alogue tasks (smith et al., 2020) – that is, all thedialogue models that are compared in the study inroller et al.
(2020).
during the collection, if thebot generates an utterance that contradicts itself, weask the worker to mark the utterance.
in some ofthe dialogues, workers are explicitly instructed togoad the bots into making contradicting utterances.
the ﬁnal human-bot test set we derive contains 764dialogues, half of which end with a contradictingutterance by the bot.
all the dialogues in the set,with either contradiction or non-contradiction la-bels, are veriﬁed by 3 additional annotators, besidethe human who actually talked to the bot..the auxiliary and human-bot test sets aim totest models’ robustness and generalizability beyondthe collected human-written test set (ribeiro et al.,2020; gardner et al., 2020), and give a more com-prehensive analysis of the task.
table 1 summarizesthe ﬁnal overall dataset.
examples are provided foreach dataset type in fig.
1 and appendix table 5..4 models.
to model the dialogue consistency task, we ﬁrst em-ploy some of the techniques used in nli sequence-to-label modeling, where the input is a pair of tex-tual sequences and the output is a label.
the beneﬁtof such modeling is that we can directly make useof existing nli datasets during training.
however,unlike previous work (welleck et al., 2019) thatdirectly utilized nli models giving a 3-way outputamong “entailment”, “contradiction”, and “neu-tral”, we modify the model with a 2-way outputbetween “contradiction” and “non-contradiction”(either “entailment” or “neutral”) labels, as our taskis centered around the detection of inconsistency.
more formally, we denote the model as ˆypred =fθ(c, u), where ˆypred is the prediction of the labely, i.e.
whether the textual response u contradictssome textual context c = {u0, u1, ..., un−1}, andθ are the parameters of the model..4.1 dialogue contradiction detectors.
we explore two distinct approaches that proposediffering fθ for the detection prediction problem..3the dataset dialogues involve two speakers taking turnsspeaking.
to maintain this structure, for each marked utter-ance, we remove a pair of utterances that represents a turn.
this also helps remove information involved in the contradic-tion such that the new label should be “non-contradiction”..unstructured approach.
in this approach, wesimply concatenate all the previous utterances inthe dialogue history to form a single textual con-text.
then, we apply fθ to the context and the last.
1702utterance to infer the probability of contradiction:.
model.
training data.
mt.
mt (strict).
hb.
se f1.
ˆypred = fθ([u0, u1, u2, ..., un−1], un).
(1).
when concatenating the utterances, we insert spe-cial tokens before each utterance to indicate thespeaker of that utterance.
this is aimed to providea signal of the dialogue structure to the models.
still, this approach assumes that the model can usethese features adequately to learn the underlyingstructure of the dialogue implicitly during training..structured utterance-based approach.
sincethe reasoning crucially depends on the last utter-ance, in this method we ﬁrst choose all the utter-ances by the last speaker to form a set s. we thenpair every utterance in the set with the last utter-ance and feed them one by one into f u b.
the ﬁnalcontradiction probability is the maximum over allthe outputs:.
θ.
ˆypred = max (cid:8)f u b.
(ui, un) : ui ∈ s(cid:9).
θ.
(2).
additionally, the utterance-based approach is ableto give a set of utterances as supporting evidencefor a contradiction decision by choosing the pairshaving contradiction probability higher than athreshold ηe:.
i = (cid:8)i : f u b.θ.
(ui, un) > ηe.
(cid:9).
(3).
this not only gives explanations for its predictionbut can also help diagnose the model itself, e.g.
wecan measure metrics of the model’s ability to pro-vide these explanations by comparing them againstgold supporting evidence annotations..one downside of this modeling approach is thatit will not be able to capture reasoning betweenspeakers.
a case for that would be a pronounby one speaker might refer to something initiatedby the other speaker.
nevertheless, the utterance-based approach explicitly adds an inductive struc-ture bias to learning and inference which we willsee can aid its generalization capability..thresholding.
for both the unstructured andutterance-based approaches, the detection of contra-diction is made by comparing ˆypred with a thresh-old τ and by default τ is 0.5..4.2 experimental setup.
unstructured approachall.
all - dnli.
all - anli-r3.
roberta.
all - decode.
utterance-based approach.
roberta.
all - anli-r3.
all - decode.
dnli.
anli-r3.
decode.
snli + mnli.
all.
all - dnli.
dnli.
anli-r3.
decode.
decode.
decode.
decode.
bert.
electra.
bart.
majority-.
-.
97.4697.4498.0484.4257.1982.2196.85.
77.4094.1994.3894.0786.6776.5481.5993.1988.8893.1794.47.
-------.
47.7080.0880.9379.3266.9563.0969.1180.8674.1481.1980.10.
77.0973.1773.5661.9160.3459.6970.03.
73.1783.6481.6882.8577.3675.2670.5284.6975.5280.7679.19.
-------.
72.488.588.488.480.671.274.387.584.387.588.2.
50.00.
50.00.
50.00.
48.7.table 2: test performance on decode for variousmethods.
“mt” and “hb” columns show model ac-curacy on the main human-human test set and thehuman-bot set, respectively.
the “mt (strict)” col-umn indicates the percentage when both the 2-waycontradiction detection and the supporting evidence re-trieval exactly match with the ground truth.
“se f1”is f1 score for supporting evidence retrieval.
“all” inthe “training data” column stands for a combinationof snli, mnli, dnli, anli-r3, decode.
“all -dnli” denotes all the datasets with dnli removed..bart (lewis et al., 2020).
they represent thestart-of-the-art language representation models andhave yielded successes in many nlu tasks.
the in-put format of fθ follows how these models handlesequence-pairs (c and u) for classiﬁcation taskswith padding, separator and other special tokenssuch as position embeddings and segment featuresinserted at designated locations accordingly..we ﬁne-tune fθ on different combinations ofnli training data including snli (bowman et al.,2015), mnli (williams et al., 2018), anli-r3 (nie et al., 2020a)4, dnli (welleck et al.,2019), as well as our decode main training set.
we convert the 3-way labels of the examples inexisting nli datasets to 2-way, as described before,and θ is optimized using cross-entropy loss.
whentraining f u bin the utterance-based approach us-ing the decode training set, the input sequences.
θ.we study four base pre-trained models variantsfor fθ: bert (devlin et al., 2019), electra (clarket al., 2020), roberta (liu et al., 2019), and.
4anli data is collected in three rounds resulting in threesubsets (r1, r2, r3).
we only used training data in r3 sinceit contains some dialogue-related examples..1703are sampled utterance pairs from the decode di-alogue.
in other scenarios, fθ or f u bare trainedwith data treated as in normal nli training..θ.the models are evaluated on the test sets de-scribed in subsection 3.3. for the utterance-basedapproach, which provides supporting evidence ut-terances (equation 3), we report f1 on evidenceretrieval.
we also report a stricter score which eval-uates whether both 2-way contradiction detectionand supporting evidence retrieval exactly matchwith the ground truth on decode main test..5 results and analysis.
5.1 performance on constructed dataset.
our main results comparing various detectors ondecode are shown in table 2. we now describeour key observations..decode is notably more effective than otherexisting nli data in providing supervision forcontradiction detection in dialogue.
we foundthat models trained on decode achieve higher ac-curacy than that of those trained on dnli or anli-r3, on all evaluation sets, with large improvements,e.g.
a 12-point jump from the same model trainingon anli-r3 and a 16-point jump from trainingon dnli using utterance-based roberta on thedecode main test set.
moreover, while train-ing on “all” datasets (snli, mnli, anli-r3,dnli & decode) is effective, the removal ofdecode from the training data induces a conse-quential downgrade on the performance.
trainingon nli data which does not cover the dialogue do-main, e.g., snli+mnli is even worse, only achiev-ing 77.4% on decode main (test) vs. 93.19%for decode and cannot even reach the majoritybaseline on the “main (test-strict)”.
further, train-ing on decode is also more helpful than dnli oranli-r3 for supporting evidence retrieval.
theseﬁndings indicate that existing nli data has lim-ited transferability to the dialogue contradictiondetection task despite their coverage of the dia-logue domain in addition to other domains and thatour decode data provides a valuable resourcefor modeling dialogue consistency and developingdata-driven approaches for contradiction detection..figure 2: comparison between utterance-based andunstructured approaches of roberta pre-trained, de-code ﬁne-tuned models on decode main (test),human-bot, and auxiliary test sets..tra, and bart trained on decode.
we can seethat roberta, electra, and bart got similar in-domain accuracy on decode, around 93%-94%.
roberta stands out when comparing their perfor-mance on the human-bot test set with the highestscore of 84.69% across the column and with betterperformance on supporting evidence retrieval aswell.
we speculate that this is due to the fact thatroberta pre-training data has a broader coveragethan electra and bart.
we hope future work ondialogue contradiction detection could explore pre-training models on more dialogue-focused corpora..the unstructured approach gets higher accu-racy on the in-domain test set.
a direct compar-ison between unstructured roberta and utterance-based roberta trained on decode reveals thatthe unstructured approach more often than not getsa higher accuracy than its corresponding utterance-based approach when other experimental setups arekept identical.
noticeably, unstructured robertatrained on all nli data got a 97.46% score, whereasutterance-based yielded 94.19%.
this seeminglyindicates that training an unstructured model is ableto yield a good representation of the consistency ofthe dialogue.
however, analysis on the human-botand auxiliary test sets shows that such high accu-racy is an over-ampliﬁcation of the model’s realunderstanding ability, as we discuss next..different pre-training models that perform sim-ilarly on the in-domain test set can have verydifferent performance on ood human-bot dia-logue.
the last four rows of the table show theresults of utterance-based roberta, bert, elec-.
the structured utterance-based approach ismore robust, and more transferable.
figure 2gives a comparison between utterance-based andunstructured roberta on each of the evaluationsets.
we can see that the utterance-based model is.
1704utterance-basedunstructured0.020.040.060.080.0100.0accuracy93.296.884.770.091.597.578.434.4decode main (test)human-bota2trctable to maintain satisfactory performance acrossall the sets whereas the unstructured model under-performs at the human-bot and rct auxiliary testsets with a 34.4% accuracy on rct compared to78.4% for utterance-based, in stark contrast to thehigh performance of the unstructured method onthe in-domain decode main test set.
this resultindicates the unstructured approach overﬁts on su-perﬁcial patterns in the decode main trainingdata which are still present due to rct’s construc-tion process.5 we also provide further analysis inappendix e, including experiments showing thatsimply removing speaker utterances not utteredby the last speaker does not greatly improve theunstructured method.
the fact that the utterance-based approach has good transferability to the oodhuman-bot test set indicates that injecting the cor-rect inductive structure bias is beneﬁcial for mod-eling dialogue consistency.
we believe this is aninteresting result generally for research using trans-formers, where there is currently a belief amongstsome practitioners that they can just use a stan-dard transformer and it will learn all the structurecorrectly on its own.
in our setting that is not thecase, and we provide a method that can rectify thatfailing..in general, there is still much room for improve-ment.
the results in table 2 also demonstratethat the modeling of dialogue consistency is a de-manding task.
on the contradiction detection task,the best score achieved by the state-of-the-art pre-trained language models on decode (test-strict)is 80.86% and the best human-bot test score is84.69%.
considering all the examples in the testsets are veriﬁed by at least 3 annotators, humans areable to swiftly identify such contradictions.
thissuggests there is a large ability gap between ourbest automatic detectors and humans.
closing thisgap is an important challenge for the community..5.2 performance in an interactive setting.
model vs. human judgment.
to further under-stand the detector predictions and how well theymight align with human judgments, we considerthe human-bot data again.
we ﬁrst divide all theutterances into two categories based on whetherthey are generated by a human or a bot.
then, thebot-generated utterances that have been markedby annotators as contradicting utterances are cat-.
5overﬁtting on superﬁcial patterns is a typical issue and.
open problem in nlu modeling (nie et al., 2020a)..figure 3: the ﬁre rate (the percentage that it predicts“contradiction”) of roberta models with different se-tups on utterances belonging to different categories.
“human” and “bot” stand for utterances by the humanor the bot prospectively.
“@n ” indicates the categorywhere n annotators agreed on the contradiction label.
the x-axis indicates different approaches and the textin parentheses denotes the training data..egorized into three sets based on the number ofannotators that agree on the contradiction label.
by design, the more annotators that agree on thecontradiction label, the more plausible that it isa contradiction.
we examine detector model ﬁrerate on the utterances in the 5 different categoriesand results are shown in figure 3. the ﬁre rate ofutterance-based roberta trained on decode onhuman utterances is 5.5% contrasting to the 74.3%on 3-agreed contradicting utterances, whereas theﬁre rates of unstructured roberta on differentcategories are more clustered together.
this ﬁnd-ing demonstrates that our models can discriminatebetween utterances with a distinct nature, and themodel predictions are aligned with human judg-ments.
moreover, a strong discriminative detectorcould be a useful tool to stratify utterances..using decode as an automatic metric.
theresults presented above indicate that the predictionof the detector can easily differentiate between thequality of utterances by humans and the utterancesby bots.
we further investigate whether it can dif-ferentiate the quality of the utterances by differentbots and be used as an automatic metric checkinggeneration consistency.
we compare the averagecontradiction score of the detector with the contra-diction rate by human judgments on the utterancesgenerated by different classes of model (bots).
thebots are the same set of models described in subsec-tion 3.3 from which we collected our human-bot.
1705utterance-based (decode)utterance-based (dnli)unstructured (decode)fire rate5.517.914.022.929.421.744.344.831.746.348.939.774.365.150.1humanbot@1@2@3model +decoding strategy.
decodecontradict% contradict%.
human.
standard generationbeam searchtop-k (k = 40)sample-and-rank.
decode re-rankingbeam searchtop-k (k = 40).
69.7%42.1%39.5%.
46.1%2.6%.
84.2%69.7%55.3%.
55.3%39.5%.
table 3: generation re-ranking using decode vs.standard methods, reporting the contradiction % asﬂagged by our contradiction detection classiﬁer (i.e.,an automatic metric, “decode contradict%”) in ad-dition to human judgments (“human contradict%”)..utterances.
table 3 presents the results..automatic metric using decode.
using oursame decode contradiction classiﬁer as the au-tomatic metric, as in subsection 5.2, we observethat by re-ranking the beam of beam search (size10) we can improve the metric.
still, 46.1% ofthe time the detector ﬂags generations as contradic-tions (vs. 69.7% without re-ranking).
upon obser-vation of the outputs, this seems to be due to thebeam of beam decoding not being diverse enough(vijayakumar et al., 2016): when the top scoringutterance is ﬂagged as contradicting, many of theother utterances in the beam are similar responseswith slight rephrases, and are ﬂagged contradictingas well.
top-k sampling fares much better, wherereranking in our test can very often ﬁnd at leastone from the k = 40 samples that does not ﬂagthe classiﬁer, leaving only a 2.6% contradictionﬁring rate.
we note we expect these numbers areover-optimistically low because the metric itself isbeing used to search (re-rank) and evaluate in thiscase..human judgments.
the last column of table 3presents human judgments of the various modelgenerations, judged using the same approach asbefore with human veriﬁers, and reporting the per-centage of contradictions.
we observe similar re-sults to the automatic metric ﬁndings.
decodere-ranking reduces the number of contradictions,particularly for top-k re-ranking vs. top-k: testingfor signiﬁcance with a wilcoxon signed-rank test,we get p = 0.051 using two human veriﬁers andp = 0.023 for three veriﬁers.
more detailed resultsand analysis can be found in appendix g..figure 4: the comparison between the average contra-diction score by the detector (y-axis) and the humanidentiﬁed contradiction rate (x-axis) on the utterancesby different bots, averaged by type of bot.
each pointin the plot is a bot which has conversed with humansand produced at least 180 utterances (with some identi-ﬁed as contradictions) in our interactive settings..dialogue examples.
the trend in figure 4 revealsthat the scores are positively correlated with humanjudgments, with a pearson correlation coefﬁcientof 0.81. we would expect that improvement on thedecode task will directly increase the correla-tion between the automatically produced detectionscore and human judgments, where use of such anautomatic metric can ease the burden on laborioushuman evaluation of consistency..5.3 generation re-ranking.
given a contradiction detector, an obvious ques-tion other than using it as an automatic metric, is:can it be used to improve the consistency of di-alogue generation models?
we consider a verysimple way to do that in the state-of-the-art genera-tive model, blenderbot (bst 2.7b) (roller et al.,2020).
during the decoding phase, for decod-ing methods that can output multiple hypotheses,we simply rerank the top scoring hypotheses us-ing the contradiction detection classiﬁer.
we useour best performing classiﬁer, our utterance-basedroberta model with decode ﬁne-tuning, andconsider three methods of decoding: beam search,top-k sampling (fan et al., 2018) and sample-and-rank (adiwardana et al., 2020), and compare thestandard and decode-reranked decoding meth-ods to each other.
for beam search we use thebest found parameters from (roller et al., 2020)which are beam size 10, minimum beam length 20and beam blocking of 3-grams.
for top-k we usek = 40. for sample-and-rank we use k=40 and20 samples.
we consider the same human-bot dia-logue logs as before, but only between blenderbotbst 2.7b and humans, selecting only contradicting.
17060.040.060.080.10.12human identified contradiction rate0.150.20.250.30.350.40.450.5avg.
contradiction probability6 conclusion.
we introduce the dialogue contradiction detec-tion task (decode) and a new conversationaldataset containing both human-human and human-bot contradictory dialogues.
training models ondecode achieves better performance than otherexisting nli data by a large margin.
we further pro-pose a structured utterance-based approach whereutterances are paired before being fed into trans-former nli models to tackle the dialogue contra-diction detection task.
we show the superiorityof such an approach when transferring to out-of-distribution dialogues compared to a standard un-structured approach representative of mainstreamnlu modeling.
we further show that our bestcontradiction detector correlates with human judg-ments, and provide evidence for its usage in bothautomatic checking and improving the consistencyof state-of-the-art generative chatbots..acknowledgement.
we thank the reviewers, and jie lei and haotan for their helpful discussions.
yn internedat facebook.
yn and mb were later sponsoredby nsf-career award 1846185, darpa mcsgrant n66001-19-2-4031, and darpa yfa17-d17ap00022..references.
daniel adiwardana, minh-thang luong, david r so,jamie hall, noah fiedel, romal thoppilan, zi yang,apoorv kulshreshtha, gaurav nemade, yifeng lu,et al.
2020. towards a human-like open-domainchatbot.
arxiv preprint arxiv:2001.09977..jeesoo bang, hyungjong noh, yonghee kim, andgary geunbae lee.
2015. example-based chat-oriented dialogue system with personalized long-term memory.
in 2015 international conference onbig data and smart computing (bigcomp), pages238–243.
ieee..jason baumgartner, savvas zannettou, brian keegan,megan squire, and jeremy blackburn.
2020. thepushshift reddit dataset.
in proceedings of the inter-national aaai conference on web and social media,volume 14, pages 830–839..samuel r. bowman, gabor angeli, christopher potts,and christopher d. manning.
2015. a large anno-tated corpus for learning natural language inference.
in proceedings of the 2015 conference on empiri-cal methods in natural language processing, pages632–642, lisbon, portugal.
association for compu-tational linguistics..andrew p bradley.
1997. the use of the area underthe roc curve in the evaluation of machine learningalgorithms.
pattern recognition, 30(7):1145–1159..tom b. brown, benjamin mann, nick ryder, melaniesubbiah, jared kaplan, prafulla dhariwal, arvindneelakantan, pranav shyam, girish sastry, amandaaskell, sandhini agarwal, ariel herbert-voss,gretchen krueger, tom henighan, rewon child,aditya ramesh, daniel m. ziegler, jeffrey wu,clemens winter, christopher hesse, mark chen,eric sigler, mateusz litwin, scott gray, benjaminchess, jack clark, christopher berner, sam mc-candlish, alec radford, ilya sutskever, and darioamodei.
2020. language models are few-shot learn-ers.
in advances in neural information processingsystems 33: annual conference on neural informa-tion processing systems 2020, neurips 2020, de-cember 6-12, 2020, virtual..kevin clark, minh-thang luong, quoc v. le, andchristopher d. manning.
2020. electra: pre-training text encoders as discriminators rather thanin 8th international conference ongenerators.
learning representations, iclr 2020, addis ababa,ethiopia, april 26-30, 2020. openreview.net..ido dagan, oren glickman, and bernardo magnini.
2005. the pascal recognising textual entailmentchallenge.
in machine learning challenges work-shop, pages 177–190.
springer..jacob devlin, ming-wei chang, kenton lee, andkristina toutanova.
2019. bert: pre-training ofdeep bidirectional transformers for language under-in proceedings of the 2019 conferencestanding.
of the north american chapter of the associationfor computational linguistics: human languagetechnologies, volume 1 (long and short papers),pages 4171–4186, minneapolis, minnesota.
associ-ation for computational linguistics..emily dinan, varvara logacheva, valentin malykh,jack urbanek,alexander miller, kurt shuster,douwe kiela, arthur szlam, iulian serban, ryanlowe, et al.
2020. the second conversational in-telligence challenge (convai2).
in the neurips’18competition, pages 187–208.
springer..emily dinan, stephen roller, kurt shuster, angelafan, michael auli, and jason weston.
2019. wizardof wikipedia: knowledge-powered conversationalin 7th international conference on learn-agents.
ing representations, iclr 2019, new orleans, la,usa, may 6-9, 2019. openreview.net..nouha dziri, ehsan kamalloo, kory mathewson, andosmar zaiane.
2019a.
augmenting neural responsegeneration with context-aware topical attention.
inproceedings of the first workshop on nlp for con-versational ai, pages 18–31, florence, italy.
associ-ation for computational linguistics..nouha dziri, ehsan kamalloo, kory mathewson, andosmar zaiane.
2019b.
evaluating coherence in dia-logue systems using entailment.
in proceedings of.
1707the 2019 conference of the north american chap-ter of the association for computational linguistics:human language technologies, volume 1 (longand short papers), pages 3806–3812, minneapolis,minnesota.
association for computational linguis-tics..angela fan, mike lewis, and yann dauphin.
2018. hi-in proceedingserarchical neural story generation.
of the 56th annual meeting of the association forcomputational linguistics (volume 1: long papers),pages 889–898, melbourne, australia.
associationfor computational linguistics..matt gardner, yoav artzi, victoria basmov, jonathanberant, ben bogin, sihao chen, pradeep dasigi,dheeru dua, yanai elazar, ananth gottumukkala,nitish gupta, hannaneh hajishirzi, gabriel ilharco,daniel khashabi, kevin lin, jiangming liu, nel-son f. liu, phoebe mulcaire, qiang ning, sameersingh, noah a. smith, sanjay subramanian, reuttsarfaty, eric wallace, ally zhang, and ben zhou.
2020. evaluating models’ local decision boundariesin findings of the associationvia contrast sets.
for computational linguistics: emnlp 2020, pages1307–1323, online.
association for computationallinguistics..mor geva, yoav goldberg, and jonathan berant.
2019.are we modeling the task or the annotator?
an inves-tigation of annotator bias in natural language under-standing datasets.
in proceedings of the 2019 con-ference on empirical methods in natural languageprocessing and the 9th international joint confer-ence on natural language processing (emnlp-ijcnlp), pages 1161–1166, hong kong, china.
as-sociation for computational linguistics..herbert p grice.
1975. logic and conversation..in.
speech acts, pages 41–58.
brill..samuel humeau, kurt shuster, marie-anne lachaux,and jason weston.
2019. poly-encoders: trans-former architectures and pre-training strategies forarxivfast and accurate multi-sentence scoring.
preprint arxiv:1905.01969..mike lewis, yinhan liu, naman goyal, mar-jan ghazvininejad, abdelrahman mohamed, omerlevy, veselin stoyanov, and luke zettlemoyer.
2020. bart: denoising sequence-to-sequence pre-training for natural language generation, translation,and comprehension.
in proceedings of the 58th an-nual meeting of the association for computationallinguistics, pages 7871–7880, online.
associationfor computational linguistics..jiwei li, michel galley, chris brockett, georgios sp-ithourakis, jianfeng gao, and bill dolan.
2016. ain pro-persona-based neural conversation model.
ceedings of the 54th annual meeting of the associa-tion for computational linguistics (volume 1: longpapers), pages 994–1003, berlin, germany.
associ-ation for computational linguistics..margaret li, stephen roller,.
ilia kulikov, seanwelleck, y-lan boureau, kyunghyun cho, and ja-son weston.
2020. don’t say that!
making inconsis-tent dialogue unlikely with unlikelihood training.
inproceedings of the 58th annual meeting of the asso-ciation for computational linguistics, pages 4715–4728, online.
association for computational lin-guistics..margaret li, jason weston, and stephen roller.
2019.acute-eval: improved dialogue evaluation with opti-mized questions and multi-turn comparisons.
arxivpreprint arxiv:1909.03087..yinhan liu, myle ott, naman goyal, jingfei du, man-dar joshi, danqi chen, omer levy, mike lewis,luke zettlemoyer, and veselin stoyanov.
2019.roberta: a robustly optimized bert pretraining ap-proach.
arxiv preprint arxiv:1907.11692..ryan lowe, nissan pow, iulian serban, and joellepineau.
2015. the ubuntu dialogue corpus: a largedataset for research in unstructured multi-turn dia-logue systems.
in proceedings of the 16th annualmeeting of the special interest group on discourseand dialogue, pages 285–294, prague, czech re-public.
association for computational linguistics..gary marcus.
2018. deep learning: a critical ap-.
praisal.
arxiv preprint arxiv:1801.00631..alexander miller, will feng, dhruv batra, antoinebordes, adam fisch, jiasen lu, devi parikh, andjason weston.
2017. parlai: a dialog research soft-in proceedings of the 2017 con-ware platform.
ference on empirical methods in natural languageprocessing: system demonstrations, pages 79–84,copenhagen, denmark.
association for computa-tional linguistics..clifford nass and youngme moon.
2000. machinesand mindlessness: social responses to computers.
journal of social issues, 56(1):81–103..yixin nie, adina williams, emily dinan, mohitbansal, jason weston, and douwe kiela.
2020a.
ad-versarial nli: a new benchmark for natural lan-guage understanding.
in proceedings of the 58th an-nual meeting of the association for computationallinguistics, pages 4885–4901, online.
associationfor computational linguistics..yixin nie, xiang zhou, and mohit bansal.
2020b.
what can we learn from collective human opinionsin proceed-on natural language inference data?
ings of the 2020 conference on empirical methodsin natural language processing (emnlp), pages9131–9143, online.
association for computationallinguistics..qiao qian, minlie huang, haizhou zhao, jingfangxu, and xiaoyan zhu.
2018. assigning personal-ity/proﬁle to a chatting machine for coherent con-versation generation.
in proceedings of the twenty-seventh international joint conference on artiﬁcial.
1708intelligence, ijcai 2018, july 13-19, 2018, stock-holm, sweden, pages 4279–4285.
ijcai.org..hannah rashkin, eric michael smith, margaret li, andy-lan boureau.
2019. towards empathetic open-domain conversation models: a new benchmark andin proceedings of the 57th annual meet-dataset.
ing of the association for computational linguis-tics, pages 5370–5381, florence, italy.
associationfor computational linguistics..marco tulio ribeiro, tongshuang wu, carlos guestrin,and sameer singh.
2020. beyond accuracy: be-havioral testing of nlp models with checklist.
inproceedings of the 58th annual meeting of the asso-ciation for computational linguistics, pages 4902–4912, online.
association for computational lin-guistics..stephen roller, emily dinan, naman goyal, da ju,mary williamson, yinhan liu, jing xu, myle ott,kurt shuster, eric m smith, et al.
2020. recipesfor building an open-domain chatbot.
arxiv preprintarxiv:2004.13637..abigail see, stephen roller, douwe kiela, and ja-son weston.
2019. what makes a good conver-sation?
how controllable attributes affect humanjudgments.
in proceedings of the 2019 conferenceof the north american chapter of the associationfor computational linguistics: human languagetechnologies, volume 1 (long and short papers),pages 1702–1723, minneapolis, minnesota.
associ-ation for computational linguistics..iulian vlad serban, alessandro sordoni, yoshua ben-gio, aaron c. courville, and joelle pineau.
2016.building end-to-end dialogue systems using gener-in pro-ative hierarchical neural network models.
ceedings of the thirtieth aaai conference on arti-ﬁcial intelligence, february 12-17, 2016, phoenix,arizona, usa, pages 3776–3784.
aaai press..iulian vlad serban, alessandro sordoni, ryan lowe,laurent charlin, joelle pineau, aaron c. courville,and yoshua bengio.
2017. a hierarchical latentvariable encoder-decoder model for generating di-in proceedings of the thirty-first aaaialogues.
conference on artiﬁcial intelligence, february 4-9,2017, san francisco, california, usa, pages 3295–3301. aaai press..eric michael smith, mary williamson, kurt shuster,jason weston, and y-lan boureau.
2020. can youput it all together: evaluating conversational agents’ability to blend skills.
in proceedings of the 58th an-nual meeting of the association for computationallinguistics, pages 2021–2030, online.
associationfor computational linguistics..ashwin k vijayakumar, michael cogswell, ram-prasath r selvaraju, qing sun, stefan lee, davidcrandall, and dhruv batra.
2016. diverse beamsearch: decoding diverse solutions from neural se-quence models.
arxiv preprint arxiv:1610.02424..di wang, nebojsa jojic, chris brockett, and eric ny-berg.
2017. steering output style and topic in neu-ral response generation.
in proceedings of the 2017conference on empirical methods in natural lan-guage processing, pages 2140–2150, copenhagen,denmark.
association for computational linguis-tics..sean welleck, jason weston, arthur szlam, andkyunghyun cho.
2019. dialogue natural languageinference.
in proceedings of the 57th annual meet-ing of the association for computational linguistics,pages 3731–3741, florence, italy.
association forcomputational linguistics..jason weston, emily dinan, and alexander miller.
2018. retrieve and reﬁne: improved sequence gen-eration models for dialogue.
in proceedings of the2018 emnlp workshop scai: the 2nd interna-tional workshop on search-oriented conversationalai, pages 87–92, brussels, belgium.
association forcomputational linguistics..adina williams, nikita nangia, and samuel bowman.
2018. a broad-coverage challenge corpus for sen-tence understanding through inference.
in proceed-ings of the 2018 conference of the north americanchapter of the association for computational lin-guistics: human language technologies, volume1 (long papers), pages 1112–1122, new orleans,louisiana.
association for computational linguis-tics..s worsnick.
2018. mitsuku wins loebner prize 2018..saizheng zhang, emily dinan, jack urbanek, arthurszlam, douwe kiela, and jason weston.
2018. per-sonalizing dialogue agents: i have a dog, do youin proceedings of the 56th an-have pets too?
nual meeting of the association for computationallinguistics (volume 1: long papers), pages 2204–2213, melbourne, australia.
association for com-putational linguistics..wei-nan zhang, qingfu zhu, yifa wang, yanyan zhao,and ting liu.
2019a.
neural personalized responsegeneration as domain adaptation.
world wide web,22(4):1427–1446..yizhe zhang, xiang gao, sungjin lee, chris brockett,michel galley, jianfeng gao, and bill dolan.
2019b.
consistent dialogue generation with self-supervisedfeature learning.
arxiv preprint arxiv:1903.05759..yizhe zhang, siqi sun, michel galley, yen-chun chen,chris brockett, xiang gao, jianfeng gao, jingjingliu, and bill dolan.
2020. dialogpt : large-scale generative pre-training for conversational re-in proceedings of the 58th an-sponse generation.
nual meeting of the association for computationallinguistics: system demonstrations, pages 270–278, online.
association for computational linguis-tics..1709li zhou, jianfeng gao, di li, and heung-yeung shum.
2020. the design and implementation of xiaoice,an empathetic social chatbot.
computational lin-guistics, 46(1):53–93..1710a annotation interface.
d examples.
in the main paper, we describe the procedure of thedata collection.
figure 5 shows the collection userinterface..b annotation quality control.
we apply the following mechanism to ensure thequality of collected data:• onboarding test: every annotator needs topass an onboarding test before they can actu-ally contribute dialogue examples.
the test isthe same dialogue contradiction detection taskas in the actual collection procedure, including5 dialogues where 3 of them have an endingutterance that contradicts the dialogue history.
the annotator needs to select the correct label(contradiction or non-contradiction) for all ﬁvedialogues to pass the test.
this mechanism testswhether an annotator understands the task..• maximum annotation count limit: themaximum number of examples one annotatorcan create is 20. this mechanism helps furtherdiversify the dialogue examples by reducing sim-ilar patterns that appear in one or a group ofannotators (geva et al., 2019)..• veriﬁcation: this subtask ensures that the dia-logue examples indeed contain an ending utter-ance that contradicts the dialogue history.
weask 3 additional annotators to verify some of thecollected examples and select the ones where allthree veriﬁers agreed on the contradiction label,and use these for our resulting validation andtests sets.
this mechanism ensures that there is aclear, agreed-upon contradiction in the dialogue,preventing the subjectivity and ambiguity issuesin some nlu tasks (nie et al., 2020b)..a pilot study with over 100 workers was con-ducted before the collection which then wentthrough an internal review process and we do notcollect any personal information of the workers..c data statistics.
table 8 shows the breakdown of dialogue sourcesand data splits.
for a subset of the contradictingdialogues in decode we asked three veriﬁersto determine whether the original writer indeedcreated a contradiction example.
table 4 showsthe veriﬁcation statistics.
note that we only useexamples on which all three veriﬁers agreed fordecode (dev) and decode (test)..as described in the main paper, decode consistsof dialogues belonging to four categories, namely,human-human, human-bot, a2t, and rct.
ta-ble 5 shows one example for each dataset type..e extra results analysis.
table 6 shows the performance of unstructuredmethod when the input consists of utterances fromboth speakers (the default unstructured approach)and when the input consists of utterances fromonly the last speaker.
the numbers for default twospeaker unstructured approach and the utterance-based approach match with that in table 2. theresult indicates that removing speaker utterancesnot uttered by the last speaker does not greatly im-prove generalization of the unstructured method.
this helps show that the out-of-domain improve-ment from the structured utterance-based methodon human-bot data comes from the structure of thearchitecture..f performance in an interactive setting.
the results discussed in the main paper evaluatemodels on constructed datasets with intentionallybalanced labels.
this facilitates the comparisonbetween models following a nlu evaluation per-spective.
in practice, we would like to evaluatehow well a model can detect contradicting utter-ances sampled naturally from interactive human-bot dialogue.
to that end, we test our trained de-tection models on the raw interactive human-botdialogue data6 having a total number of 764 dia-logues consisting of 8,933 utterances.
since thecontradiction task in naturally sampled dialoguecan be extremely unbalanced, the total number ofcontradicting utterances in the raw dialogue list isonly 3817. we apply our contradiction detectorson every bot-generated utterance and calculate theprecision, recall, and f1 on contradiction detection.
since the scores might be subjective to the thresh-old τ , we also evaluate the threshold-invariant areaunder the roc curve (auc) (bradley, 1997)..as shown in table 7, model precision on thetask is not satisfactory (23.94 at best).
however,the best model achieves acceptable scores on bothrecall and auc.
this indicates its potential us-age for strict blocking of inconsistent utterances.
6this is the same set of dialogues from which we con-.
structed the balanced human-bot test set..7the majority baseline accuracy is 95.73%..1711# of veriﬁers agreed count ratio (%).
0123.
4844971,2116,214.
7.67%7.87%19.18%65.28%.
table 4: veriﬁcation statistics.
the ﬁrst column indi-cates the number of veriﬁers that agreed upon the givencontradictions..of a generative model (bot).
the table also drawsthe same conclusion as table 2 that the structuredutterance-based roberta model trained using de-code data is the best method for contradictiondetection, comparing to training on other nli dataor using an unstructured approach.
in the follow-ing sections we thus use that best method as ourdetector for further experiments..g generation re-ranking.
we show in table 9 human judgments for our gen-eration re-reranking experiments in three settings:with at least two human veriﬁers, with three agree-ing, or treating agreements as a fractional contra-diction score.
the ﬁrst two, for a given utterance,assign a binary score (either contradicton or non-contradiction) depending on whether at least 2 or3 human veriﬁers agree on the contradiction label.
the last setting treats a given utterance as having afractional score, either 0, 1/3, 2/3, or 3/3 dependingon how many human veriﬁers label it as a contra-diction.
we then take the mean over all utterancesin each setting to give the ﬁnal contradiction countper setting..in addition to the setting in the main paper (sub-section 5.3), we also consider the setting where thedialogue examples we use consist of 76 examplesutterances that were identiﬁed by humans as beingcontradictions by blenderbot (using beam search)and 100 examples that were not.
this is in contrastto table 3 where we only considered contradict-ing utterances by blenderbot only.
the results aregiven in table 10. we ﬁnd similar results to themain paper’s results but where the model’s scoreare closer together.
this should be expected aswhen selecting many utterances that are alreadynon-contradicting in the original blenderbot gen-erations, there is not much left to improve..main (human-human)human1: i recently had a long weekend with some old friends.
it was fantastic.
human2: it must have been fun to catch up with them.
human1: it was.
we’ve spent a lot of time together and apart now, so it was good tocatchup.
human2: well i hope you guys continue to stay in touch.
human1: sometimes friends grow apart over time though.
human2: yeah, that’s true.
you should forget about them and go on with your life..human-bot:human1: it’s a beautiful day.
i’m going to take a walk later.
bot: what do you do for a living?
human1: i work in a doctor’s ofﬁce.
how about yourself?
bot: i am a stay at home mom.
human1: that’s great!
how many kids do you have?
bot: i don’t have any kids yet, but i plan on getting married soon..add two turns (a2t):human1: i hate when ash from my cigarette drops in my work pantshuman2: oof that sucks really badhuman1: yeah, i haave to wait till i get home to get the stain off, it is really embarras-inghuman2: yea i can imagine it ishuman1: every time i look at it i remember the good times we had together.
human2: well thats nicehuman1: i will have to wash the stain with soap and water.
human2: ash stains on your pants is not a big deal though..remove contradicting turns (rct):human1: i was disgusted when i noticed the food on the tablehuman2: what kind of food?
human1: it was brussel sprouts and liverhuman2: oh, disgusting.
human1: i couldn’t even bear to take a single bitehuman2: brussel sprouts and liver sounds delicious to me!.
table 5: dialogue examples for different dataset types.
underline indicates that the pair of utterances is ran-domly added.
strikethrough text indicates that thepair of utterances is removed.
dialogue examples forhuman-human, human-bot, and a2t end with a con-tradicting utterance whereas the example for rct hasan ending utterance whereby the original contradictingpair of utterances in the dialogue history are removed..approach.
mt (acc.)
hb (acc.).
unstructured (both speaker)unstructured (one speaker)utterance-based.
96.8596.6893.19.
70.0373.1784.69.table 6: performance of roberta trained on de-code data with different approaches.
“mt” and “hb”columns show model accuracy on the main human-human test set and the human-bot set, respectively..training data.
precision recall.
f1.
auc.
unstructured approach15.89all15.63all - decode17.05decode.
utterance-based approachallall - decodednlianli-r3decode.
23.3517.1716.3222.5223.94.
60.1157.7450.13.
25.1424.6025.45.
80.4771.8273.40.
71.6568.5065.0941.7374.28.
35.2327.4626.0929.2636.21.
84.9680.0979.2976.3687.16.the bot-table 7: roberta performance on allgenerated utterances from the raw interactive human-bot dialogue.
the threshold τ for prediction is 0.5..1712figure 5: the collection interface.
the task preview box (top right) gives a short description of the task beforethe annotator will work on the writing.
the collection consists of two steps.
in step 1 (on the left), the annotatorsare asked to write one or two utterances such that the last utterance will contradict some previous utterances in theconversation.
in step 2 (on the right), the annotators are asked to pick the utterances in the conversation that areinvolved in the contradiction.
we use a casual term “message” instead of “utterance” in the instructions..train.
dev.
test.
wizard of wikipediaempatheticdialoguesblended skill talkconvai2total.
6,2346,1828,5546,21427,184.
1,2081,0461,2005724,026.
1,1601,0501,3106964,216.table 8: our decode main dataset source statistics.
the labels in each split are balanced.
there are a to-tal of 2,013+2,108 contradicting examples in the devand test sets which are the collected 4,121 veriﬁed ex-amples.
the ﬁrst column indicates the source of thedialogue..model +decoding strategy.
human contradict%.
2-agree.
3-agree.
fractional.
standard generationbeam searchtop-k (k = 40)sample-and-rank.
decode re-rankingbeam searchtop-k (k = 40).
84.2%69.7%55.3%.
42.1%44.7%31.6%.
55.3%39.5%.
29.0%13.2%.
75.0%66.2%52.2%.
49.7%39.9%.
table 9: generation re-ranking using decode vs.standard methods, reporting the contradiction % asﬂagged by human judgments (“human contradict%”)in three settings: with at least two human veriﬁers, withthree agreeing, or treating agreements as a fractionalcontradiction score..model +decoding strategy.
decodecontradict% contradict%.
human.
standard generationbeam searchtop-k (k = 40)sample-and-rank.
decode re-rankingbeam searchtop-k (k = 40).
38.1%29.0%29.6%.
22.7%1.1%.
38.3%31.8%29.0%.
32.0%25.6%.
table 10: generation re-ranking using decode vs.standard methods, reporting the contradiction % asﬂagged by our contradiction detection classiﬁer (i.e.,an automatic metric, “decode contradict%”) in addi-tion to human judgments (“human contradict%”).
inthis setting, the set of dialogue examples we use con-sists of 76 examples utterances that were identiﬁed byhumans as being contradictions by blenderbot (usingbeam search) and 100 examples that were not.
(in con-trast, table 3 only considered contradicting utterancesby blenderbot only.)
we ﬁnd similar results to themain paper’s results but where the model’s score arecloser together.
this should be expected as when select-ing many utterances that are already non-contradictingthere is not much left to improve..1713