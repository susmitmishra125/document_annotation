multi-hop graph convolutional network with high-order chebyshevapproximation for text reasoning.
shuoran jiang, qingcai chen∗, xin liu, baotian hu, lisai zhangharbin institute of technology, shenzhen{shuoran.chiang, hit.xinliu, lisaizhang2016}@gmail.com{qingcai.chen, hubaotian}@hit.edu.cn.
abstract.
graph convolutional network (gcn) has be-come popular in various natural language pro-cessing (nlp) tasks with its superiority in long-term and non-consecutive word interactions.
however, existing single-hop graph reason-ing in gcn may miss some important non-consecutive dependencies.
in this study, wedeﬁne the spectral graph convolutional networkwith the high-order dynamic chebyshev ap-proximation (hdgcn), which augments themulti-hop graph reasoning by fusing messagesaggregated from direct and long-term depen-dencies into one convolutional layer.
to alle-viate the over-smoothing in high-order cheby-shev approximation, a multi-vote based cross-attention (mvcattn) with linear computationcomplexity is also proposed.
the empiri-cal results on four transductive and inductivenlp tasks and the ablation study verify theefﬁcacy of the proposed model.
our sourcecode is available at https://github.com/mathisall/hdgcn-pytorch..1.introduction.
graph neural networks (gnns) are usually used tolearn the node representations in euclidean spacefrom graph data, which have been developed toone of the hottest research topics in recent years(zhang, 2020).
the primitive gnns relied on recur-sive propagation on graphs, which takes a long timeto train (zhang et al., 2019b).
one major variantof gnns, graph convolutional networks (gcns)(kipf and welling, 2017; yao et al., 2019), takesspectral ﬁltering to replace recursive message pass-ing and needs only a shallow network to convergent,which have been used in various nlp tasks.
forexample, yao et al.
(2019) constructed the text as agraph and input it to a gcn.
this method achievedbetter results than conventional deep learning mod-els in text classiﬁcation.
afterward, the gcns.
∗corresponding author: qingcai chen.
have became popular in more tasks, such as wordembedding (zhang et al., 2020b), semantic anal-ysis (zhang et al., 2019a), document summariza-tion (wang et al., 2020), knowledge graph (wanget al., 2018), etc..the spectral graph convolution in yao’s gcn isa localized ﬁrst-order chebyshev approximation.
it is equal to a stack of 1-step markov chain (mc)layer and fully connected (fc) layer.
unlike themulti-step markov chains, the message propaga-tion in vanilla gcn lacks the node probability tran-sitions.
as a result, the multi-hop graph reason-ing is very tardy in gcn and easily causes thesuspended animation problem (zhang and meng,2019).
however, the probability transition on thegraph is useful to improve the efﬁciency in learningcontextual dependencies.
in many nlp tasks (likethe question answering (qa) system and entity re-lation extraction), the features of the two nodesneed to be aligned.
as an example, figure 1 showsa simple graph where the node n4 is a pronounof node n1.
in this example, the adjacency ma-trix is masked on nodes n2, n3, n5 to demonstratethe message passing between n1 and n4.
figure 1(c) and (d) plot the processes of feature alignmenton two nodes without and with probability tran-sitions respectively.
in this example, the featurealignment process without probability transitionneeds 10 more steps than which with probabilitytransition.
it is shown that encoding the multi-hopdependencies through the spectral graph ﬁlteringin gcn usually requires a deep network.
however,as well known that the deep neural network (dnn)is tough to train and easily causes the over-ﬁttingproblem (rong et al., 2019)..some newest studies to improve the multi-hopgraph reasoning include graph attention networks(gats) (veliˇckovi´c et al., 2018), graph residualneural network (gresnet) (zhang and meng,2019), graph diffusive neural network (difnet).
proceedingsofthe59thannualmeetingoftheassociationforcomputationallinguisticsandthe11thinternationaljointconferenceonnaturallanguageprocessing,pages6563–6573august1–6,2021.©2021associationforcomputationallinguistics6563input sequence instead of the ﬁxed graph structures.
publications have shown that transformers outper-form gcns in many nlp tasks.
graph trans-former (dwivedi and bresson, 2020) generalizesthe transformer to arbitrary graphs, and improvesinductive learning from laplacian eigenvectors ongraph topology.
however, due to the connectionsscale quadratically growth with node number nin graphs, things get out of hand for very large n .
additionally, the fully-connected graph is not aninterpretable architecture in practical tasks.
forexample, whether transformers are the best choiceto bring the text in linguistic theory?
1.to improve the efﬁciency and performance ofmulti-hop graph reasoning in spectral graph con-volution, we proposed a new graph convolutionalnetwork with high-order dynamic chebyshev ap-proximation (hdgcn).
a prime chebnet and ahigh-order dynamic (hd) chebnet are ﬁrstly ap-plied to implement this chebyshev approximation.
these two sub-networks work like a trade-off onlow-pass signals (direct dependencies) and high-pass signals (multi-hop dependencies) respectively.
the prime chebnet takes the same frame as theconvolutional layer in vanilla gcn.
it mainly ex-tracts information from direct neighbors in localcontexts.
the hd-chebnet aggregates messagesfrom multi-hop neighbors following the transitiondirection adaptively learned by the attention mech-anism.
the standard self-attention (vaswani et al.,2017) has a o (cid:0)n 2(cid:1) computation complexity andit is hard to be applied on long sequence.
even theexisting sparsity attention methods, like the star-transformer (guo et al., 2019) and extended trans-former construction (etc) (ainslie et al., 2020),have reduced the quadratic dependence limit ofsequence length to linear dependence, but the fully-connected graph structure cannot be kept.
we de-sign a multi-vote-based cross-attention (mvcattn)mechanism.
the mvcattn scales the computationcomplexity o(n 2) in self-attention to o(n )..the main contributions of this paper are listed.
below:.
• to improve the efﬁciency and performance ofmulti-hop reasoning in spectral graph convolu-tion, we propose a novel graph convolutionalnetwork with high-order dynamic chebyshevapproximation (hdgcn)..1https://towardsdatascience.com/transformers-are-graph-.
neural-networks-bca9f75412aa.
figure 1: (a): a simple graph with 5 nodes and theweighted edges, in which the nodes n4 is a pronounof n1 and the two nodes need to align features.
(b):the masked adjacency matrix on this graph.
(c) and(d): the processes of feature alignment on nodes n1and n4 without transition probability and with transitionprobability respectively..(zhang, 2020), tgmc-s (zhang et al., 2020c) andgraph transformer networks (yun et al., 2019;zhang and zhang, 2020).
gats enhance the graphreasoning by implicitly re-deﬁning the graph struc-ture with the attention on the 1-hop neighbors, butthere is equilibrial optimization on the whole graph.
gresnet solves the suspended animation prob-lem by creating extensively connected highways toinvolve raw node features and intermediate repre-sentations throughout all the model layers.
how-ever, the multi-hop dependencies are still reasonedat a slow pace.
difnet introduces a new neu-ron unit, i.e., gdu (gated diffusive unit), to modeland update the hidden node states at each layer.
difnet replaces the spectral ﬁltering with a recur-sive module and realizes the neural gate learningand graph residual learning.
but the time cost is ag-gravated in difnet compared with gcn.
tgmc-s stacks gcn layers on adjacent matrices with dif-ferent hops of trafﬁc networks.
different from theground-truth trafﬁc network in tgmc-s, it is hardto construct the multi-hop word-word relationshipsobjectively from the text.
tgmc-s hadn’t given away to improve the multi-hop message passing ingcn..transformers (vaswani et al., 2017) and cor-responding pre-trained models (xu et al., 2019)could be thought of as fully-connected graph neuralnetworks that contain the multi-hop dependencies.
they ﬁgure out the contextual dependencies on thefully-connected graph with the attention mecha-nism.
the message propagation in transformersfollows the relations self-adaptively learned from.
6564• to avoid the over-smoothing problem inhd-chebnet, we propose a multi-votebased cross-attention (mvcattn) mechanism,which adaptively learn the direction of nodeprobability transition.
mvcattn is a variantof the attention mechanism with the propertyof linear computation complexity..• the experimental results show that the pro-posed model outperforms compared sotamodels on four transductive and inductivenlp tasks..2 related work.
our work draws supports from the vanilla gcn andthe attention mechanism, so we ﬁrst give a glanceat the paradigm of these models in this section..2.1 graph convolutional network.
the gcn model proposed by (kipf and welling,2017) is the one we interested, and it is deﬁned ongraph g = {v, e}, where v is the node set and eis the edge set.
the edge (vi, vj) ∈ e representsa link between nodes vi and vj.
the graph sig-nals are attributed as x ∈ r|v|×d, and the graphrelations e can be deﬁned as an adjacency matrixa ∈ r|v|×|v| (binary or weighted)..each convolutional layer in gcn is a 1st cheby-shev approximation on spectral graph convolution,and its layer-wise propagation rule in neural net-work is deﬁned as:.
h(l+1) = σ.
(cid:101)ah(l)w(l)(cid:17)(cid:16).
, l ≥ l ≥ 0.
(cid:101)a = (d + in )− 1.
2 (a + in ) (d + in )− 12 ,.
(1).
where h(0) = x, (cid:101)a is the normalized adjacencymatrix and σ is a non-linear activation function..the node embeddings output from the last con-volutional layer are fed into a softmax classiﬁer fornode or graph classiﬁcation, and the loss functionl can be deﬁned as the cross-entropy error.
theweight set {w(l)}ll=0 can be jointly optimized byminimizing l via gradient descent..2.2 self-attention is a dynamic gcn.
the attention mechanism is an effective way toextract task-relevant features from inputs, and ithelps the model to make better decisions (leeet al., 2019).
it has various approaches to computethe attention score from features, and the scaleddot-product attention proposed in transformers.
(vaswani et al., 2017) is the most popular one..z = softmax.
(cid:18) xwqwkxt√.
(cid:19).
xwv.
(cid:124).
dk.
(cid:123)(cid:122)a.
(cid:125).
(2).
where x ∈ rn ×d is the input sequence, andweights wq ∈ rd×dk , wk ∈ rdk×d, wv ∈rd×dv are used to transform sequence to queries,keys and values..as showed in equation 2, the attention scoresa can be viewed as a dynamic adjacency matrixon sequence x. this process in self-attention issimilar to the graph convolutional layer deﬁned inequation 1. the only difference is that the adja-cency matrix in equation 2 is adaptively learnedfrom input instead of prior graph structures..3 method.
in our model, the input graph g = (v, e) takesthe same form as the one in gcn.
the nodes areattributed as x ∈ r|v|×d, and the adjacency matrixa ∈ r|v|×|v| (binary or weighted) is deﬁned ongraph edges e..the spectral graph convolution in fourier do-.
main is deﬁned as,.
gθ (cid:63) x = ugθ.
(cid:16).
(cid:17).
(cid:101)λ.ut x.
(3).
where x ∈ rd is the signal on a node, u is thematrix of eigenvectors on normalized graph lapla-2 ad− 1cian l = in − d− 12 = uλut , and theﬁlter gθ( (cid:101)λ) is a function of the eigenvalues onnormalized (cid:101)l in fourier domain..the k-th (k > 2) order truncation of cheby-shev polynomials on this spectral graph convolu-tion is,.
gθ (cid:63) x ≈.
θiuti.
(cid:16).
(cid:17).
(cid:101)λ.ut x.
(4).
k(cid:88).
i=0.
(cid:16).
(cid:17).
(cid:101)λ.where t0.
2 (cid:101)λti−1.
(cid:16).
(cid:17).
(cid:101)λ.
− ti−2.
(cid:16).
(cid:17)..(cid:101)λ.
= i, t1 = (cid:101)λ, ti>1.
(cid:16).
(cid:17).
(cid:101)λ.
=.
to replace the parameters {θi}k.parameter set {θ(i)}k/2polynomials in equation 4 are approximated as:.
i=1 with anotheri=1 , the kth-order chebyshev.
gθ (cid:63) x ≈.
(cid:16).
u (cid:101)λut (cid:17)2k (cid:16).
i − u (cid:101)λut (cid:17).
xθ(k).
(5).
k/2(cid:88).
k=0.
k/2(cid:88).
k=1.
≈.
(cid:101)a2k.
(cid:101)axθ(i).
6565figure 2: (a): the architecture of hdgcn taking the simple graph in figure 1 as an example.
(b): the schematicsof the multi-vote based cross-attention (mvcattn) in every unit in hd-chebnet..where the (cid:101)a is normalized adjacency matrix (asdeﬁned in equation 1).
as the node state transition(cid:101)a2k causes the over-smoothing problem (li et al.,2018; nt and maehara, 2019), we take the dynamicpairwise relationship ad self-adaptively learned bythe attention mechanism to turn the direction ofnode state transition..the powers of adjacency matrix (cid:101)a2k in equa-tion 5 can cause the over smoothing problem, wereplace the (cid:101)a2k with (cid:101)akakd.in our implementation, the ﬁrst-order and higher-order chebyshev polynomials in equation 5 isapproximated with a prime chebyshev network(chebnet) and high-order dynamic chebyshev net-works (hd-chebnets) respectively.
we general-ize the graph convolution on kth-order dynamicchebyshev approximation (equation 5) to the layer-wise propagation as follows,.
h ≈.
z(k),.
k/2(cid:88).
k=0(cid:16).
(cid:101)axw(0)(cid:17)z(0) = σ(cid:125)(cid:123)(cid:122)(cid:124)prime chebnet(cid:16).
(cid:16).
,.
z(k) = σ(cid:124).
(cid:101)a.a(k).
d z(k)w(k)(cid:123)(cid:122)unit in hd-chebnet.
d.(cid:17).
w(k)(cid:17)(cid:125).
,.
(6).
where k is the order and w(0), w(k), w(k)arednonlinear ﬁlters on node signals.
for the conve-nience of writing, we just deﬁne the ﬁrst layer ofhdgcn..3.1 prime chebnet.
we consider the same convolutional architecture asthe one in gcn to implement the prime chebnet,.
and it mainly aggregates messages from the directdependencies..z(0) = σ.
(cid:16).
(cid:101)axw(0)(cid:17).
,.
(7).
where w(0) ∈ rd×d and (cid:101)a is the normalized sym-metric adjacency matrix..3.2 high-order dynamic (hd) chebnet.
as the multi-hop neighbors can be interacted viathe 1-hop neighbors, we take the z(0) output fromthe prime chebnet as input of the hd-chebnet.
the multi-vote based cross-attention (mvcattn)mechanism ﬁrst adaptively learns the direction ofnode probability transition a(k)d , its schematic isshowed in figure 2 (b).
mvcattn has two phases -graph information aggregation and diffusion..graph information aggregation coarsens thenode embeddings z(k−1) to a small supernode sets(k) ∈ rm ×d, m (cid:28) |v|..the ﬁrst step is multi-vote projection (mvproj).
in which node embeddings z(k−1) are projected tomultiple votes v(k) ∈ r|v|×m ×d, and these votesare aggregated to supernode set s(k) = {s(k)m=1..m }m.s(k)m = mvproj.
(cid:16).
z(k−1)(cid:17).
= norm.
.
v wvz(k−1)m..
|v|(cid:88).
v=1.
.
(8).
m ∈ rdk×dkwhere |v| ≥ v ≥ 1, m ≥ m ≥ 1, wvis the projection weight and norm () represents thelayernorm operation..next, the forward cross-attention (fcattn) up-.
6566dates the supernode values as:z(k), s(k)(cid:17).
(cid:16).
(cid:98)s(k) = fcattn= a(k).
f z(k−1)wf v(cid:32).
a(k).
f = softmax.
(9).
(cid:33).
z(k−1)wf kwf qs(k)√d.where wf k ∈ rdk×dc, wf q ∈ rdc×dk andwf v ∈ rdk×dk ..graph information diffusion feeds the supern-odes (cid:98)s(k) back to update node set z(k).
with thenode embeddings z(k−1) and supernode embed-dings (cid:98)s(k), the backward cross-attention (bcattn)is deﬁned as,.
z(k) = bcattn.
(cid:16).
(cid:101)s(k), z(k−1)(cid:17).
= a(k).
b z(k−1)wbv(cid:32).
a(k).
b = softmax.
(cid:98)s(k)wbqwbkz(k−1)√d.(10).
(cid:33).
where wbq ∈ rdk×da, wbk ∈ rda×dk andwbv ∈ rdk×dk ..the last step is adding the probability transitionwith (cid:101)a. the output of k-th order hd-chebnet(equation a) is,.
(cid:98)z(k) = σ.
(cid:16).
(cid:101)az(k)w(k)(cid:17).
(11).
finally, the outputs from the prime chebnet andhd-chebnets are integrated as the node embed-dings,.
h = norm.
z(0) +.
k/2(cid:88).
k=1.
.
(cid:98)z(k).
 ..(12).
3.3 classiﬁer layer.
node classiﬁcation the node representations houtput from the last graph convolutional layer arestraightforward fed into a softmax classiﬁer fornode classiﬁcation..(cid:98)yv = softmax (mlp (hv))graph classiﬁcation the representation on thewhole graph is constructed via a readout layer onthe outputs h,.
(13).
hv = σ (f1 (hv)) (cid:12) tanh (f2 (hv)).
hg =.
1|v|.
|v|(cid:88).
v=1.
hv + maxpool (cid:0)h1 · · · h|v|.
(cid:1) (14).
where (cid:12) denotes the hadamard product and f1(),f2() are two non-linear functions..the graph representation hg ∈ rd is fed into the.
softmax classiﬁer to predict the graph label..(cid:98)yg = softmax (hg).
(15).
all parameters are optimized by minimizing the.
cross-entropy function:.
l = −.
yn/g log((cid:98)yn/g).
(16).
1n.n(cid:88).
n=1.
4 experiments.
in this section, we evaluate hdgcn on trans-ductive and inductive nlp tasks of text classiﬁ-cation, aspect-based sentiment classiﬁcation, nat-ural language inference, and node classiﬁcation.
in experiment, each layer of hdgcn is ﬁxedwith k = 6 order chebyshev approximationand the model stacks l = 1 layer.
the dimen-sion of input node embeddings is d = 300 ofglve or d = 768 of pre-trained bert, and thehyper-parameter dk = 64, da = 64. so thed, w(k) ∈ r64×64 andweights w(0) ∈ rd×64, wlwf k, wf q, wbq, wbk ∈ r64×64.
the number ofsuper-nodes is set as m = 10. our model is op-timized with adabelief (zhuang et al., 2020) witha learning rate 1e − 5. the schematics about thehdgcn is shown in figure 2..to analyze the effectiveness of mvcattn inavoiding over-smoothing, we report the results ofablation study - hdgcn-static in table 1, 2 5. theablation model - hdgcn-static is an implementa-tion of equation 5, in which the node state transi-tion is determined by the static adjacency matrix(cid:101)a2k..4.1 text classiﬁcation.
the ﬁrst experiment is designed to evaluate theperformance of hdgcn on the text graph classi-ﬁcation.
four small-scale text datasets2 - mr, r8,r52, ohsumed, and four large-scale text datasets- ag’s news3, sst-1, sst-24, yelp-f5 are usedin this task.
the graph structures are built onword-word co-occurrences in a sliding window.
2https://github.com/yao8839836/text gcn3http://groups.di.unipi.it/ gulli/ag corpus of news articles.html4https://nlp.stanford.edu/sentiment/treebank.html5https://www.yelp.com/dataset.
6567hdgcn outperforms the fully-connected graphmodule in transformers and corresponding pre-trained models.
additionally, these comparisonsalso demonstrates that the combination of priorgraph structures and self-adaptive graph structuresin graph convolution is able to improve the multi-hop graph reasoning..4.2 multi-hop graph reasoning in text.
graph.
(width=3 and unweighted) on individual docu-ments.
hdgcn is initialized with word embed-dings pre-trained by 300-d glove and 768-d bert-base on small and large scale datasets respectively.
the baselines include textcnn, textrnn, fast-text, swem, textgcn, graphcnn, texting,mincut, bert-base, drnn, cnn-nsu, cap-nets, lk-mtl, tinybert, star-transformer..modeltextcnn(cid:63)textrnn(cid:63)fasttext(cid:63)swem(cid:63)textgcn(cid:63)graphcnn(cid:63)mincut (bianchi et al., 2019)texting (zhang et al., 2020b)bert-base (jin et al., 2019)hdgcn-statichdgcn.
mr77.7577.6875.1476.6576.74-76.5279.8285.8079.7086.50.r895.7196.3196.1395.3297.0797.8097.4298.0497.9298.0598.45.r5287.5990.5492.8192.9493.5694.6093.5395.4896.3795.4996.57.ohsumed58.4449.2757.7063.1268.3669.4066.3770.4271.0470.7573.97.table 1: test accuracy (%) on small-scale englishdatasets, where the results labeled with (cid:63) are cited from(zhang et al., 2020b)..modelfasttext (joulin et al., 2017)drnn (wang, 2018)cnn-nsu (li et al., 2017)capnets (yang et al., 2018)lk-mtl (xiao et al., 2018)bert (xiao et al., 2018)tinybert (jiao et al., 2020)star-transformer (guo et al., 2019)hdgcn-statichdgcn.
ag92.593.6-92.6-94.594.7-94.095.5.sst-1-47.350.8-49.750.151.652.952.153.9.sst-2-86.489.486.888.589.392.6-90.892.3.yelp-f63.965.3---65.866.1-65.269.6.table 2: test accuracies (%) on large-scale englishdatasets..figure 3: the message aggregation on adjacency matrix(cid:101)a with word-word co-occurrence in document..table 1 shows the test accuracies on four small-scale english datasets, in which hdgcn ranks topwith accuracies 86.50%, 98.45%, 96.57%, 73.97%respectively.
hdgcn beats the best baselinesachieved by texting (the newest gnn model)and the ﬁne-tuned bert-base model.
our abla-tion model hdgcn-static also achieves higheraccuracies than the newest gnn models - tex-ting and mincut.
therefore, the outperformanceof hdgcn veriﬁes that (1) the node probabilitytransition in high-order chebyshev approximationimproves the spectral graph convolution; (2) themvcattn mechanism in high-order chebnet fur-ther raises the effectiveness by avoiding the over-smoothing problem..table 2 shows the test accuracies of hdgcn andother sota models on large-scale english datasets.
hdgcn achieves the best results 95.5%, 53.9%,69.6% on ag, sst-1, yelp-f respectively, and per-forms a slight gap 0.3% with the top-1 baseline(tinybert) on sst-2.
these results support that.
in the second experiment, we make a case studyon the mr dataset to visualize how the hdgcn im-prove multi-hop graph reasoning.
here, we take thepositive comment ”inside the ﬁlm’s conﬂict pow-ered plot there is a decent moral trying to get out,but it’s not that , it’s the tension that keeps you inyour seat afﬂeck and jackson are good sparringpartners” as an example..first, the word interactions on prior graph struc-ture (cid:101)a (word-word co-occurrence in a sliding win-dow with width=3) is showed in figure 3. we cansee that the word mainly interacts with its consec-utive neighbors.
it is hard for the vanilla gcn toencode multi-hop and non-consecutive word-wordinteractions as the example shown in figure 1..figure 4 shows the node interactions from nodeembeddings z(0) to supernodes (cid:98)s(1) and the graphdiffusion from (cid:98)s(1) to node embeddings z(1).
inwhich, the supernode s4 puts greater attention onthe segment - it’s the tension that keeps you inyour seat.
this segment determines its positive.
6568initializedembeddings.
glove.
bert-base.
modelaoa(cid:63)tnet-lf(cid:63)ascnn(cid:63)asgcn-dt(cid:63)asgcn-dg(cid:63)aen-bert (song et al., 2019)bert-pt (xu et al., 2019)sdgcn-bert (zhao et al., 2020)hdgcn (glove)hdgcn (bert-base).
twitterf1.
70.2071.4369.4569.6870.40---71.5271.23.acc.
72.3072.9871.0571.5372.15---73.4172.69.lap14.
rest14.
rest15.
rest16.
acc.
72.6274.6172.6274.1475.5579.9378.0781.3576.8079.15.f1.
67.5270.1466.7269.2471.0576.3175.0878.3473.1875.48.acc.
79.9780.4281.7380.8680.7783.1284.9583.5780.4385.89.f1.
70.4271.0373.1072.1972.0273.7676.9676.4770.7479.33.acc.
78.1778.4778.4779.3479.89---81.1881.18.f1.
57.0259.4758.9060.7861.89---67.4062.21.acc.
87.5089.0787.3988.6988.99---89.1287.99.f166.2170.4364.5666.6467.48---70.3771.28.table 3: test accuracy (%) and macro-f1 score on aspect-based sentiment classiﬁcation.
the results labeled with (cid:63)are cited from (zhao et al., 2020)..f ×) of the 1st hd-chebnet, where s1 ∼ s5 represent.
figure 4: the word interactions in mvcattn (a(1)a(1)bthe supernodes..polarity signiﬁcantly.
the other supernodes s1, s2,s3, s5 just aggregate messages from the globalcontext evenly.
next, the messages aggregatedin supernodes s1 ∼ s5 are mainly diffused tofour tokens - conﬂict, decent, moral, you.
thatveriﬁes the self-adaptively learned graph structurea(1)b by the mvcattn improves the multi-hop graph reasoning on nodes - conﬂict, decent,moral, you.
from the perspective of semantics,these four words determine the positive sentimentin this comment signiﬁcantly..f × a(1).
figure 5 shows the message aggregation fromnode embeddings z(1) to supernodes (cid:98)s(2) and themessage diffusion from (cid:98)s(2) to node embeddingsz(2).
we can see that the supernode s4 puts greaterattention on another segment - there is a decentmoral young to get out, which also contributes tothe sentiment polarity.
then messages aggregatedto supernodes s1 ∼ s5 are diffused to all wordsevenly.
the backward interactions from supern-.
f ×b of the 2nd hd-chebnet, where s1 ∼ s5 represent.
figure 5: the word interactions in mvcattn a(2)a(2)the supernodes..odes s1 ∼ s5 to all graph nodes do not have visi-ble differences.
these results demonstrate that themulti-hop graph reasoning in hdgcn just needsone graph convolutional layer to attain the station-ary state..4.3 aspect-based sentiment classiﬁcation.
the third experiment evaluates hdgcn’s perfor-mance on the task of aspect-based sentiment clas-siﬁcation.
this task aims to identify whether thesentiment polarities of aspect are explicitly givenin sentences (zhao et al., 2020).
the datasets usedin this task include twitter, lap14, rest14,rest15, rest16 (zhao et al., 2020).
the detailsabout the statistics on these datasets are shown infigure 6. the sota comparison models includeaoa, tnet-lf, ascnn, asgcn-dt, asgcn-dg, aen-bert, bert-pt, sdgcn-bert..each sample in this task includes a sentence pair,an aspect, and a label.
the sentence pair and theaspect are concatenated into one long sentence, andthe text graph is preprocessed with the dependency.
6569tree on this sentence.
hdgcn is tested twice withword embeddings initialized by pre-trained 300-dglove and 768-d bert-base respectively..figure 6: (a): the statistics of aspect-based sentimentclassiﬁcation datasets.
(b): the percentages of sen-tences with length ≥ 50 in 5 datasets..table 3 shows the test accuracies and micro-f1 scores on 5 datasets, where hdgcn achievesnew state-of-the-art results on twitter, rest14,rest15, rest16, and a top-3 result on the lap14.
as shown in figure 6 that the lap14 has themaximum percentage of long sentences among alldatasets.
a shallow network in hdgcn does notoutperform the sota result on the lap14.
addi-tionally, compared with the newest asgcn andattention-based aoa, hdgcn achieves the bestresults on twitter, lap14, rest15, rest16(acc) and performs very close with the highest ac-curacy on rest14 and macro-f1 score on rest16.
above comparison supports that the matching be-tween aspect and sentence pair in hdgcn is moreaccurate than the newest gnn and attention-basedmodels, which veriﬁes that the multi-hop graphreasoning is improved in hdgcn..4.4 natural language inference.
the fourth experiment evaluates hdgcn’s per-formance on the stanford natural language infer-ence (snli) task (bowman et al., 2015).
thistask aims to predict the semantic relationship isentailment or contradiction or neutral between apremise sentence and a hypothesis sentence.
allthe comparison methods include ﬁne-tuned bert-base, mt-dnn (liu et al., 2020), smart (jianget al., 2020), and ca-mtl (pilault et al., 2021)..in this task, the premise and hypothesis sen-tences are concatenated and constructed into a longsentence.
which is preprocessed to a text graphwith the dependency tree.
the word embeddingsin hdgcn were initialized from the pre-trained768-d bert-base..all test accuracies are shown in table 4, wherehdgcn achieves the new state-of-the-art results.
model.
bert-base (devlin et al., 2019)mt-dnn (liu et al., 2020)smart (jiang et al., 2020)ca-mtl (pilault et al., 2021)hdgcn.
totalparameters1.0×--1.12×1.02×.
% data used1.0%78.188.386.086.285.6.
0.1%52.581.982.782.880.3.
10%86.791.188.788.092.3.table 4: test accuracy (%) on snli, where the totalparameters take the bert-base as base..on the 10% data.
as the mt-dnn, smart andca-mtl are all ﬁne-tuned on multi-task learning,they perform better than hdgcn in low resourceregimes (0.1% and 1.0% of the data).
hdgcn justuses 0.02× more parameters than the bert-base,and it outperforms the later model on all scalesof data.
these results verify that the combinationof prior graph structure and self-adaptive graphstructure in hdgcn performs comparably withthe fully-adaptive graph structures in transformersand bert-based multi-task learning models..4.5 graph node classiﬁcation.
the ﬁfth experiment evaluates the effectiveness ofhdgcn on the node classiﬁcation task.
we usethree standard citation network benchmark datasets- cora, citeseer, and pubmed, to compare the testaccuracies on transductive node classiﬁcation.
inthe three datasets, the nodes represent the docu-ments and edges (undirected) represent citations.
the node features correspond to elements of a bag-of-words representation of a document (veliˇckovi´cet al., 2018).
we also use the ppi dataset to com-pare the results on inductive node classiﬁcation,which consists of graphs corresponding to differenthuman tissues.
the baselines for comparison in-clude gcn, gat, graph-bert, graphnas, loopy-net, hgcn, grace, gcnii.
the results of ourevaluation are recorded in table 5..modelgcn (kipf and welling, 2017)gat (veliˇckovi´c et al., 2018)graph-bert (zhang et al., 2020a)graphnas (gao et al., 2019)loopynet (zhang and meng, 2019)hgcn (chami et al., 2019)grace (zhu et al., 2020)gcnii (chen et al., 2020)hdgcn-statichdgcn.
cora85.886.484.384.283.979.983.386.484.288.6.transductive(acc, %)citeseer73.774.371.273.173.7-72.176.573.277.0.pubmed88.187.679.379.683.080.386.785.690.391.0.inductive(micro-f1)ppi69.797.3-98.6-74.696.999.550.499.5.table 5: test accuracy (%) on cora, citeseer, pubmedand micro-f1 score (%) on ppi..hdgcn achieves the new state-of-the-art re-sults on cora, citeseer and pubmed, and performsequally best with the newest gcnii on ppi.
our.
6570ablation model, hdgcn-static, also achieves closeresults with the newest gnns on cora, citeseer,pubmed, but it performs poorly on ppi.
whichveriﬁes that the high-order chebyshev approxima-tion of spectral graph convolution has more se-rious over-smoothing problem in inductive nodeclassiﬁcation than transductive node classiﬁcation.
all comparisons in this experiment demonstratethe effectiveness of mvcattn to avoid the over-smoothing problem..5 conclusions.
this study proposes a multi-hop graph convolu-tional network on high-order dynamic chebyshevapproximation (hdgcn) for text reasoning.
toimprove the multi-hop graph reasoning, each con-volutional layer in hdgcn fuses low-pass signals(direct dependencies saved in ﬁxed graph struc-tures) and high-pass signals (multi-hop dependen-cies adaptively learned by mvcattn) simultane-ously.
we also ﬁrstly propose the multi-votes basedcross-attention (mvcattn) mechanism to alleviatethe over-smoothing in high-order chebyshev ap-proximation, and it just costs the linear computa-tion complexity.
our experimental results demon-strate that hdgcn outperforms compared sotamodels on multiple transductive and inductive nlptasks..acknowledgments.
this work is supported by natural sciencefoundation of china(grant no.61872113,62006061), strategic emerging industry de-velopment special funds of shenzhen (grantno.xmht20190108009),the tencent groupscience and technology planning project ofshenzhen (grant no.jcyj20190806112210067)and shenzhen foundational research funding(grant no.jcyj20200109113403826)..references.
joshua ainslie, santiago ontanon, chris alberti, va-clav cvicek, zachary fisher, philip pham, anirudhravula, sumit sanghai, qifan wang, and li yang.
2020. etc: encoding long and structured inputs intransformers.
in proceedings of the 2020 conferenceon empirical methods in natural language process-ing (emnlp), pages 268–284..filippo maria bianchi, daniele grattarola, and cesarealippi.
2019. mincut pooling in graph neural net-works.
arxiv preprint arxiv:1907.00481..samuel r. bowman, gabor angeli, christopher potts,and christopher d. manning.
2015. a large anno-tated corpus for learning natural language inference.
in proceedings of the 2015 conference on empiricalmethods in natural language processing (emnlp).
association for computational linguistics..ines chami, rex ying, christopher r´e, and jureleskovec.
2019. hyperbolic graph convolutionalneural networks.
in neurips, 32:4869..ming chen, zhewei wei, zengfeng huang, bolin ding,and yaliang li.
2020. simple and deep graph convo-lutional networks.
in in icml..jacob devlin, ming-wei chang, kenton lee, andkristina toutanova.
2019. bert: pre-training of deepbidirectional transformers for language understand-ing.
in in naacl..vijay prakash dwivedi and xavier bresson.
2020. ageneralization of transformer networks to graphs.
arxiv preprint arxiv:2012.09699..yang gao, hong yang, peng zhang, chuan zhou, andyue hu.
2019. graphnas: graph neural architecturesearch with reinforcement learning.
arxiv preprintarxiv:1904.09981..qipeng guo, xipeng qiu, pengfei liu, yunfan shao,xiangyang xue, and zheng zhang.
2019. star-transformer.
in in naacl, pages 1315–1325..haoming jiang, pengcheng he, weizhu chen, xi-aodong liu, jianfeng gao, and tuo zhao.
2020.smart: robust and efﬁcient ﬁne-tuning for pre-trained natural language models through principledregularized optimization.
in in acl..xiaoqi jiao, yichun yin, lifeng shang, xin jiang, xiaochen, linlin li, fang wang, and qun liu.
2020.tinybert: distilling bert for natural language under-standing.
in in emnlp..di jin, zhijing jin, joey tianyi zhou, and peterszolovits.
2019. is bert really robust?
natural lan-guage attack on text classiﬁcation and entailment.
arxiv preprint arxiv:1907.11932, 2..armand joulin, ´edouard grave, piotr bojanowski, andtom´aˇs mikolov.
2017. bag of tricks for efﬁcient textclassiﬁcation.
in in eacl, pages 427–431..thomas n kipf and max welling.
2017..semi-supervised classiﬁcation with graph convolutionalnetworks.
in iclr..john boaz lee, ryan a rossi, sungchul kim, nes-reen k ahmed, and eunyee koh.
2019. attentionmodels in graphs: a survey.
in tkdd, 13(6):1–25..qimai li, zhichao han, and xiao-ming wu.
2018.deeper insights into graph convolutional networksin proceedings offor semi-supervised learning.
the aaai conference on artiﬁcial intelligence, vol-ume 32..6571shen li, zhe zhao, tao liu, renfen hu, and xiaoyongdu.
2017. initializing convolutional ﬁlters with se-mantic features for text classiﬁcation.
in in emnlp..liang yao, chengsheng mao, and yuan luo.
2019.graph convolutional networks for text classiﬁcation.
in in aaai, volume 33, pages 7370–7377..xiaodong liu, yu wang, jianshu ji, hao cheng,xueyun zhu, emmanuel awa, pengcheng he,weizhu chen, hoifung poon, guihong cao, et al.
2020. the microsoft toolkit of multi-task deep neu-ral networks for natural language understanding.
inin acl..hoang nt and takanori maehara.
2019. revisitinggraph neural networks: all we have is low-pass ﬁlters.
arxiv preprint arxiv:1905.09550..jonathan pilault, amine elhattami, and christopher pal.
2021. conditionally adaptive multi-task learning:improving transfer learning in nlp using fewer pa-rameters & less data.
in iclr..yu rong, wenbing huang, tingyang xu, and junzhouhuang.
2019. dropedge: towards deep graph convo-lutional networks on node classiﬁcation.
in in iclr..youwei song, jiahai wang, tao jiang, zhiyue liu, andyanghui rao.
2019. attentional encoder networkfor targeted sentiment classiﬁcation.
arxiv preprintarxiv:1902.09314..seongjun yun, minbyul jeong, raehyun kim, jaewookang, and hyunwoo j kim.
2019. graph transformernetworks.
advances in neural information process-ing systems..chen zhang, qiuchi li, and dawei song.
2019a.
aspect-based sentiment classiﬁcation with aspect-speciﬁc graph convolutional networks.
in in emnlp-ijcnlp..haopeng zhang and jiawei zhang.
2020. text graphtransformer for document classiﬁcation.
in proceed-ings of the 2020 conference on empirical methodsin natural language processing (emnlp), pages8322–8327, online.
association for computationallinguistics..jiawei zhang.
2020. get rid of suspended anima-tion problem: deep diffusive neural network ongraph semi-supervised classiﬁcation.
arxiv preprintarxiv:2001.07922..jiawei zhang and lin meng.
2019. gresnet: graphresidual network for reviving deep gnns from sus-pended animation.
corr, abs/1909.05729..ashish vaswani, noam shazeer, niki parmar, jakobuszkoreit, llion jones, aidan n gomez, łukaszkaiser, and illia polosukhin.
2017. attention is allyou need.
in in nips, pages 6000–6010..jiawei zhang, haopeng zhang, congying xia, andli sun.
2020a.
graph-bert: only attention is neededfor learning graph representations.
arxiv preprintarxiv:2001.05140..petar veliˇckovi´c, guillem cucurull, arantxa casanova,adriana romero, pietro li`o, and yoshua bengio.
2018. graph attention networks.
in iclr..baoxin wang.
2018. disconnected recurrent neural.
networks for text categorization.
in in acl..danqing wang, pengfei liu, yining zheng, xipeng qiu,and xuan-jing huang.
2020. heterogeneous graphneural networks for extractive document summariza-tion.
in in acl..zhichun wang, qingsong lv, xiaohan lan, andyu zhang.
2018. cross-lingual knowledge graphalignment via graph convolutional networks.
in inemnlp..liqiang xiao, honglun zhang, wenqing chen,yongkun wang, and yaohui jin.
2018. learningwhat to share: leaky multi-task network for text clas-siﬁcation.
in in coling..hu xu, bing liu, lei shu, and philip yu.
2019. bertpost-training for review reading comprehension andaspect-based sentiment analysis.
in in nacl..min yang, wei zhao, jianbo ye, zeyang lei, zhouzhao, and soufei zhang.
2018. investigating capsulenetworks with dynamic routing for text classiﬁcation.
in in emnlp..yingxue zhang, soumyasundar pal, mark coates, anddeniz ustebay.
2019b.
bayesian graph convolutionalneural networks for semi-supervised classiﬁcation.
in in aaai, volume 33, pages 5829–5836..yufeng zhang, xueli yu, zeyu cui, shu wu, zhongzhenwen, and liang wang.
2020b.
every document ownsits structure: inductive text classiﬁcation via graphneural networks.
in in acl..zhengchao zhang, meng li, xi lin, and yinhai wang.
2020c.
network-wide trafﬁc ﬂow estimation with in-sufﬁcient volume detection and crowdsourcing data.
transportation research part c: emerging technolo-gies, 121:102870..pinlong zhao, linlin hou, and ou wu.
2020. mod-eling sentiment dependencies with graph convolu-tional networks for aspect-level sentiment classiﬁca-tion.
knowledge-based systems..yanqiao zhu, yichen xu, feng yu, qiang liu, shu wu,and liang wang.
2020. deep graph contrastive repre-sentation learning.
arxiv preprint arxiv:2006.04131..juntang zhuang, tommy tang, yifan ding, sekhar ctatikonda, nicha dvornek, xenophon papademetris,and james duncan.
2020. adabelief optimizer:adapting stepsizes by the belief in observed gradi-ents.
neurips, 33..6572a appendices.
here, we give the completed proof about our high-order chebyshev approximation on the spectral graphconvolution.
we exhibit how to deduce the spectral graph convolution to 4th-order chebyshev polynomialsas follows..where gθ = gθ( (cid:101)λ) is the graph ﬁlter deﬁned in spectral domain..gθ (cid:63) x = ugθut x.gθ ≈ θ0 + θ1 (cid:101)λ + θ2.
(cid:17)4 (cid:101)λ3 − 3 (cid:101)λ2 (cid:101)λ2 − 1= θ0 + θ1 (cid:101)λ + 2θ2 (cid:101)λ2 − θ2 + 4θ3 (cid:101)λ3 − 3θ3 (cid:101)λ.
+ θ3.
(cid:16).
(cid:17).
(cid:16).
so,.
ugθutx ≈ θ0x + θ1u (cid:101)λutx + 2θ2u (cid:101)λ2utx − θ2x + 4θ3u (cid:101)λ3utx − 3θ3u (cid:101)λutx.
= θ0x + θ1u (cid:101)λutx + 2θ2u (cid:101)λutu (cid:101)λutx − θ2x + 4θ3u (cid:101)λutu (cid:101)λutu (cid:101)λutx − 3θ3u (cid:101)λutx.
θ0= u (cid:101)λutu (cid:101)λut(u (cid:101)λutu (cid:101)λut(cid:18)θ0 − θ2+u (cid:101)λutu (cid:101)λutx + u (cid:101)λutu (cid:101)λut (cid:16)(θ0 − θ2) + (θ1 − 3θ3) u (cid:101)λut(cid:17)(cid:16).
θ1u (cid:101)λutθ1 − 3θ3u (cid:101)λut.
= u (cid:101)λutu (cid:101)λut.
+ 2θ2 −.
θ2u (cid:101)λutu (cid:101)λutx + u (cid:101)λutu (cid:101)λut (cid:16).
+.
=.
(cid:19).
2θ2 + 4θ3u (cid:101)λut(cid:17).
x.
+ 4θ3u (cid:101)λut − 32θ2 + 4θ3u (cid:101)λut(cid:17).
x.θ3u (cid:101)λut.
)x.let assume θ(0) = θ0 − θ2 = −θ1 + 3θ3, θ(1) = 2θ1 = −4θ3,.
ugθutx ≈ θ(0) (cid:16).
i − u (cid:101)λut(cid:17).
x + u (cid:101)λutu (cid:101)λutθ(1) (cid:16).
i − u (cid:101)λut(cid:17).
x.
≈ θ(0) (cid:101)ax + (cid:101)a2θ(1) (cid:101)ax.
to avoid the over-smoothing problem in the node state transition (cid:101)a2, the graph structure (cid:101)a is approxi-mated by the dynamic adjacency matrix ad self-adaptively learned with attention mechanism.
this wayhave the hidden pairwise interactions to improve the multi-hop graph reasoning in high-order chebyshevpolynomials.
therefore, we deﬁne the layer-wise propagation of multi-hop graph convolutional networkas follows..h ≈.
z(k),.
k/2(cid:88).
k=0(cid:16).
(cid:101)axw(0)(cid:17)z(0) = σ(cid:124)(cid:125)(cid:123)(cid:122)prime chebnet(cid:16).
(cid:16).
,.
z(k) = σ(cid:124).
(cid:101)a.a(k).
d z(k−1)w(k)(cid:123)(cid:122)hd-chebnet.
d.(cid:17).
w(k)(cid:17)(cid:125).
where x is the input features, and we introduce two nonlinear ﬁlterings w(k) and w(k).
d on node signals..6573