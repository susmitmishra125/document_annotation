anonymisation models for text data:state of the art, challenges and future directions.
pierre lison1, ildik´o pil´an1, david s´anchez2, montserrat batet2, and lilja øvrelid3.
1norwegian computing center, oslo, norway2universitat rovira i virgili, cybercat, unesco chair in data privacy, spain3language technology group, university of oslo, norway.
{plison,pilan}@nr.no.
{david.sanchez,montserrat.batet}@urv.cat.
liljao@iﬁ.uio.no.
abstract.
this position paper investigates the problem ofautomated text anonymisation, which is a pre-requisite for secure sharing of documents con-taining sensitive information about individuals.
we summarise the key concepts behind textanonymisation and provide a review of currentapproaches.
anonymisation methods have sofar been developed in two ﬁelds with little mu-tual interaction, namely natural language pro-cessing and privacy-preserving data publish-ing.
based on a case study, we outline the ben-eﬁts and limitations of these approaches anddiscuss a number of open challenges, such as(1) how to account for multiple types of seman-tic inferences, (2) how to strike a balance be-tween disclosure risk and data utility and (3)how to evaluate the quality of the resultinganonymisation.
we lay out a case for movingbeyond sequence labelling models and incor-porate explicit measures of disclosure risk intothe text anonymisation process..1.introduction.
privacy is a fundamental human right (art.
12 ofthe universal declaration of human rights) anda critical component of any free society, amongothers to protect citizens against social control,stigmatisation, and threats to political expression.
privacy is also protected by multiple national andinternational legal frameworks, such as the generaldata protection regulation (gdpr) introduced ineurope in 2018. this right to privacy imposesconstraints on the usage and distribution of data in-cluding personal information, such as emails, courtcases or patient records.
in particular, personaldata cannot be distributed to third parties (or evenused for secondary purposes) without legal ground,such as the explicit and informed consent of theindividuals to whom the data refers..as informed consent is often difﬁcult to obtainin practice, an alternative is to rely on anonymisa-.
tion techniques that render personal data no longerpersonal.
access to anonymised data is a prerequi-site for research advances in many scientiﬁc ﬁelds,notably in medicine and the social sciences.
by fa-cilitating open data initiatives, anonymised data canalso help empower citizens and support democraticparticipation.
for structured databases, anonymi-sation can be enforced through well-establishedprivacy models such as k-anonymity (samarati,2001; samarati and sweeney, 1998) or differen-tial privacy (dwork et al., 2006).
these privacymodels and their implementations are, however,difﬁcult to apply to unstructured data such as texts.
in fact, text anonymisation has been traditionallyenforced manually, a process that is costly, time-consuming and prone to errors (bier et al., 2009).
these limitations led to the development of variouscomputational frameworks designed to extend auto-mated or semi-automated anonymisation to the textdomain (meystre et al., 2010; s´anchez and batet,2016; dernoncourt et al., 2017)..in this paper, we review the core concepts un-derlying text anonymisation, and survey the ap-proaches put forward to solve this task.
thesecan be divided into two independent research di-rections.
on the one hand, nlp approaches relyon sequence labelling to detect and remove prede-ﬁned categories of entities that are considered sen-sitive or of personal nature (such as names, phonenumbers or medical conditions).
on the otherhand, privacy-preserving data publishing (ppdp)approaches take the notion of disclosure risk asstarting point and anonymise text by enforcing a pri-vacy model.
anonymisation consists of a sequenceof transformations (such as removal or generalisa-tion) on the document to ensure the requirementsderived from the privacy model are fulﬁlled..this position paper makes the case that noneof these approaches provide a fully satisfactoryaccount of the text anonymisation problem.
we.
proceedingsofthe59thannualmeetingoftheassociationforcomputationallinguisticsandthe11thinternationaljointconferenceonnaturallanguageprocessing,pages4188–4203august1–6,2021.©2021associationforcomputationallinguistics4188illustrate their merits and shortcomings on a casestudy and discuss three open challenges:.
1. how to ensure that anonymisation is robustagainst multiple types of semantic inferences,based on background knowledge assumed tobe available to an adversary ;.
2. how to transform the text in order to minimisethe risk of disclosing personal data, yet retainas much semantic content as possible ;.
3. how to empirically evaluate the quality (interms of disclosure risk and utility preserva-tion) of the resulting anonymisation..we argue in this paper that nlp and ppdp ap-proaches should be viewed as complementary (onefocusing on linguistic patterns, the other on disclo-sure risk) and that future anonymisation approachesfor text should seek to reconcile these two views.
in particular, we contend that text anonymisationmodels should combine a data-driven editor model(which selects masking operations on the docu-ment) with an adversary seeking to infer conﬁden-tial attributes from edited documents..2 what is anonymisation?.
the most common deﬁnition of privacy amounts toself-determination, which is the ability of individ-uals, groups or organisations to seclude informa-tion about themselves selectively (westin, 1967).
information related to an identiﬁed or identiﬁableperson is known as personal data, or more preciselypersonally identiﬁable information (pii).
datasetswith pii cannot be released without control as thiswould impair the privacy of the data subjects..2.1 legal requirements.
various legal frameworks regulate how pii can becollected and processed.
in particular, the gen-eral data protection regulation introduced in eu-rope (gdpr, 2016) states that data owners musthave a legal basis for processing pii, the most im-portant one being the explicit consent of the datasubjects.alternatively, data owners may choose toanonymise the data to ensure it can no longer be at-tributed to speciﬁc individuals.
anonymised data isno longer regulated by the gdpr and can thereforebe freely released..table 1 deﬁnes some of the key terms relatedto data anonymisation (elliot et al., 2016).
thisterminology is, however, not always applied con-sistently, as several authors seem to use e.g.
the.
direct identiﬁer: a (set of) variable(s) uniquefor an individual (a name, address, phonenumber or bank account) that may be usedto directly identify the subject..quasi identiﬁer: information (such as gender,nationality, or city of residence) that in iso-lation does not enable re-identiﬁcation, butmay do so when combined with other quasi-identiﬁers and background knowledge..conﬁdential attribute: private personal infor-mation that should not be disclosed (such as amedical condition)..identity disclosure: unequivocal association ofa record/document with a subject’s identity..attribute disclosure: unequivocal inference ofa conﬁdential attribute about a subject..anonymisation: complete and irreversible re-moval from a dataset of any information that,directly or indirectly, may lead to a subject’sdata being identiﬁed..de-identiﬁcation: process of removing speciﬁc,predeﬁned direct identiﬁers from a dataset..pseudonymisation: process of replacing directidentiﬁers with pseudonyms or coded values(such ”john doe” → ”patient 3”).
the map-ping between coded values and the originalidentiﬁers is then stored separately..table 1: key terms related to data anonymisation..terms “anonymisation” and “de-identiﬁcation” in-terchangeably (chevrier et al., 2019)..gdpr-compliant anonymisation is the completeand irreversible process of removing personal iden-tiﬁers, both direct and indirect, that may lead to anindividual being identiﬁed.
direct identiﬁers cor-respond to values such as names or social securitynumbers that directly disclose the identity of theindividual.
however, removing direct identiﬁers isnot sufﬁcient to eliminate all disclosure risks, asindividuals may also be re-identiﬁed by combiningseveral pieces of information together with somebackground knowledge.
for instance, the combi-nation of gender, birth date and postal code canbe exploited to identify between 63 and 87% ofthe u.s. population, due to the public availabilityof us census data (golle, 2006).
these types ofpersonal identiﬁers are called quasi-identiﬁers andencompass a large variety of data types such as.
4189demographic and geospatial data.
anonymisationtherefore necessitates both the removal of directidentiﬁers and the masking of quasi-identiﬁers..other legal frameworks have adopted a differentapproach.
in the us, the health insurance porta-bility and accountability act (hipaa) (hipaa,2004) lists 18 data types, such as patient’s name,address or social security number, which qual-ify as protected health information (phi) andshould be removed from the data prior to release.
this process of removing predeﬁned categoriesof identiﬁers is called de-identiﬁcation1.
in otherwords, while hipaa-based de-identiﬁcation is lim-ited to speciﬁc categories of direct identiﬁers, theanonymisation process deﬁned by gdpr requiresus to consider any direct or indirect informationthat, combined with background knowledge, maylead to re-identifying an individual.
the californiaconsumer privacy act (ccpa) introduced in 2018adopts a position relatively similar to gdpr regard-ing anonymisation and asserts that any data that canbe linked directly or indirectly to a consumer mustbe considered as personal information..we highlight these legal differences as they haveimportant implications on how anonymisation toolsshould be designed and evaluated (rothstein, 2010;in particular, gdpr- or ccpa-hintze, 2017).
compliant anonymisation cannot be restricted to thedetection of predeﬁned classes of entities but mustconsider how any textual element may contributeto the disclosure risk, either directly or through se-mantic inferences using the background knowledgeassumed to be available to an adversary..2.2 disclosure risks.
legal regulations for privacy and data protection(such as gdpr and hipaa) typically focus onidentity disclosure.
however, personal informa-tion may also be disclosed without re-identiﬁcation.
in particular, attribute disclosure occurs when thevalue of a conﬁdential attribute (e.g., a medicalcondition) can be inferred from the released data,for instance when all records sharing some charac-teristics (e.g.
age) have the same conﬁdential value(e.g.
suffering from aids).
identity disclosure canbe seen as a special case of attribute disclosurewhen the conﬁdential attribute corresponds to theperson identity.
data anonymisation should pre-vent identity disclosure but, in most cases, attribute.
1gdpr also introduces the equivalent concept ofpseudonymisation, which is a useful privacy-enhancing mea-sure, but it does not qualify as full anonymisation..disclosure, which is usually more harmful from aprivacy perspective, should also be avoided..the removal of personal information necessarilyentails some data utility loss.
because the ultimatepurpose behind data releases is to produce usabledata, the best anonymisation methods are thosethat optimise the trade-off between minimising thedisclosure risk and preserving the data utility..3 nlp approaches.
3.1 de-identiﬁcation.
nlp research on text anonymisation has focusedto a large extent on the tasks of de-identiﬁcation,and, to a lesser extent, pseudonymisation.
de-identiﬁcation is generally modelled as a sequencelabelling task, similar to named entity recogni-tion (ner) (chiu and nichols, 2016; lample et al.,2016).
most work to date has been performed inthe area of clinical nlp, where the goal is to de-tect protected health information (phi) in clinicaltexts (meystre et al., 2010; aberdeen et al., 2010).
several shared tasks have contributed to increasedactivity within this area, in particular through therelease of datasets manually annotated with phis.
the 2014 i2b2/uthealth shared task (stubbs anduzuner, 2015) includes diabetic patient medicalrecords annotated for an extended set of phi cate-gories.
another inﬂuential dataset stems from the2016 cegs n-grid shared task (stubbs et al.,2017) based on psychiatric intake records, whichare particularly challenging to de-identify due to ahigher density of phis..early approaches to this task were based on rule-based and machine learning-based methods, eitheralone or in combination (yogarajan et al., 2018).
dernoncourt et al.
(2017) and liu et al.
(2017)present the ﬁrst neural models for de-identiﬁcationusing recurrent neural networks with character-level embeddings, achieving state-of-the-art per-formance on the i2b2 2014 dataset..a central challenge in clinical de-identiﬁcationis the availability of annotated data and the lack ofuniversal annotation standards for phi, making itdifﬁcult to transfer data across domains.
hartmanet al.
(2020) examine how to adapt de-identiﬁcationsystems across clinical sub-domains.
they com-pare the use of labelled or unlabelled data for do-main adaptation with in-domain testing and off-the-shelf de-identiﬁcation tools, and show that man-ual labelling of even small amounts of phi ex-amples yields performance above existing tools..4190further, embeddings trained on larger amounts ofin-domain, unlabelled data can be employed toadapt models to a new domain (yang et al., 2019).
finally, friedrich et al.
(2019) present an adversar-ial approach for learning privacy-preserving textrepresentations, thereby allowing data to be moreeasily shared to train de-identiﬁcation tools..outside of the clinical domain, medlock (2006)presents a dataset of e-mails annotated with both di-rect identiﬁers (person names, transactional codes,etc.)
and quasi-identiﬁers (organisations, coursenames, etc.).
some annotation efforts are alsogeared towards de-identiﬁcation for languagesother than english.
eder et al.
(2020) present a de-identiﬁcation dataset consisting of german e-mails.
for swedish, velupillai et al.
(2009); alfalahi et al.
(2012) present efforts to collect and standardise an-notated clinical notes, while megyesi et al.
(2018)present a pseudonymised learner language corpus.
for spanish, a recently held shared task on clini-cal de-identiﬁcation released a synthetic spanish-language dataset (marimon et al., 2019)..the problem of replacing identiﬁers with surro-gate values is rarely addressed in nlp.
most ap-proaches simply replace detected identiﬁers withdummy values such as x, although some modelsattempt to preserve the gender of person names andprovide dedicated rules for e.g.
dates and addresses(sweeney, 1996; alfalahi et al., 2012; eder et al.,2019; chen et al., 2019) or to a somewhat broaderrange of identiﬁers (volodina et al., 2020)..a few studies have analysed the re-identiﬁcationrisk of de-identiﬁed or pseudonymised texts (car-rell et al., 2013; meystre et al., 2014b).
the datautility of de-identiﬁed texts is analysed in meystreet al.
(2014a), concluding that the impact of de-identiﬁcation is small, but non-negligible..3.2 obfuscation methods.
beyond de-identiﬁcation, several research effortshave looked at detecting and obfuscating social me-dia texts based on quasi-identifying categories suchas gender (reddy and knight, 2016) or race (blod-gett et al., 2016).
a number of recent approacheshave sought to transform latent representations oftexts to protect conﬁdential attributes, using adver-sarial learning (elazar and goldberg, 2018), rein-forcement learning (mosallanezhad et al., 2019) orencryption (huang et al., 2020).
however, thosemethods operate at the level of latent vector repre-sentations and do not modify the texts themselves..one notable exception is the text rewriting ap-proach of xu et al.
(2019) which edits the textsusing back-translations..3.3 challenges.
nlp approaches to anonymisation suffer from anumber of shortcomings.
most importantly, theyare limited to predeﬁned categories of entities andignore how less conspicuous text elements mayalso play a role in re-identifying the individual.
forinstance, the family status or physical appearanceof a person may lead to re-identiﬁcation but willrarely be considered as categories to detect.
on theother hand, those methods may also end up remov-ing too much information, as they will systemati-cally remove all occurrences of a given categorywithout examining their impact on the disclosurerisk or on the utility of the remaining text..4 ppdp approaches.
privacy-preserving data publishing (ppdp) devel-ops computational techniques for releasing datawithout violating privacy (chen et al., 2009)..the ppdp approach to anonymisation is privacy-ﬁrst: a privacy model specifying an ex ante pri-vacy condition is enforced through one or severaldata masking methods, such as noise addition orgeneralisation of values (domingo-ferrer et al.,2016).
the ﬁrst widely-accepted privacy model isk-anonymity (samarati, 2001): a dataset satisﬁes k-anonymity if each combination of values of quasi-identiﬁer attributes is shared by at least k records.
with k > 1, no unequivocal re-identiﬁcations arepossible, thereby preventing identity disclosure..most of the attention of the ppdp communityhas been on structured databases.
privacy modelssuch as k-anonymity assume that datasets consist ofrecords, each one detailing the attributes of a singleindividual, and that attributes have been classiﬁedbeforehand into identiﬁers, quasi-identiﬁers andconﬁdential attributes.
moreover, most maskingmethods employed to enforce privacy models havebeen designed with numerical data in mind, andbarely (and poorly) manage categorical or nominalattributes (rodr´ıguez-garc´ıa et al., 2019)..4.1 k-anonymity and beyond.
solutions for anonymising unstructured text arescarce and mostly theoretical.
the ﬁrst approachesadapted k-anonymity for collections of documents.
in (chakaravarthy et al., 2008), the authors pre-.
4191sented the notion of k-safety.
they assume acollection of entities e to be protected against dis-closure, each one characterised by a set of termsc(e) that represent their contexts (i.e.
words co-occurring with e and that may be known to anattacker).
then, a document d containing an entitye is said to be k-safe if the terms appearing in dalso belong to the contexts of, at least, k −1 enti-ties other than e. terms not fulﬁlling the propertyare redacted before release.
the privacy guaran-tee offered by this approach is sound because theprobability of disclosing the protected entity is re-duced to 1/k.
however, it requires exhaustivecollections of contexts for all entities to be pro-tected, which is unfeasible.
it also assumes that thedetection of sensitive terms is already performed.
this approach is only feasible for very constraineddomains and non-dynamic sets of entities, such ascollections of sensitive diseases, and documentswith homogeneous contents..another approach built on k-anonymity iscumby and ghani (2011), where a multi-class clas-siﬁer is trained to map input documents to (prede-ﬁned) sensitive entities.
this aims at reproducingthe inferences that a potential attacker may per-form to disclose sensitive entities.
a documentx referring to a sensitive entity y is then said tobe k-confusable if the classiﬁer outputs at least kclasses other than y. documents are redacted viaterm removal or generalisation until the property isfulﬁlled.
to be applicable, sensitive entities shouldbe static and the documents to be protected shouldmatch that of the corpus used for training..anandan et al.
(2012) present a privacy modelfor document protection named t-plausibility.
theyseek to generalise terms identiﬁed as sensitive ac-cording to the t-plausibility property: a protecteddocument is said to fulﬁl t-plausibility if, at least,t different plausible documents can be derived byspecialising the generalised terms.
in other words,even though the privacy guarantee is intuitive, onecan hardly predict the results for a certain t, be-cause they depend on the document length, thenumber of sensitive entities and the granularity ofthe knowledge base employed to obtain term gen-eralisations.
assuming that sensitive entities havealready been detected also circumvents the mostchallenging task of document protection..4.2 c-sanitise.
s´anchez and batet (2016, 2017) tackles the.
anonymisation problem from a different perspec-tive.
instead of expressing privacy guarantees interms of probability of disclosure, it deﬁnes riskas an information theoretic characterisation of dis-closed semantics.
the proposed privacy model,c-sanitise, states that given a document d, back-ground knowledge k available to potential attack-ers, and a set of entities to protect c, d(cid:48) is thec-sanitised version of d if d(cid:48) does not containany term t that, individually or in aggregate, un-equivocally disclose the semantics encompassedby any entity in c by exploiting k. the seman-tic disclosure incurred by t on any entity in c isquantiﬁed as their pointwise mutual information(anandan and clifton, 2011) measured from theirprobability of (co-)occurrence in the web, whichis assumed to represent the most comprehensiveknowledge source (k) available to attackers (chowet al., 2008).
this approach is able to automaticallydetect terms that may cause disclosure and can en-compass dynamic collections of entities to protect.
obtaining accurate probabilities of co-occurrencefrom large corpora is, however, costly..4.3 differential privacy.
differential privacy (dp) is a privacy model thatdeﬁnes anonymisation in terms of randomised al-gorithms for computing statistics from the data(dwork et al., 2006).
dp provides guarantees thatthe statistics cannot be used to learn anything sub-stantial about any individual.
however, the goalof dp is to produce randomised responses to con-trolled queries, and applying it to data publishingleads in poor data utility (domingo-ferrer et al.,2021).
dp cannot be directly employed to edit outpersonal information from text while preservingthe content of the rest of the document, and is thusoutside the scope of this paper.
however, dp can beemployed for other privacy-related tasks such as inproducing synthetic texts (fernandes et al., 2018;bommasani et al., 2019), deriving differentially-private word representations (feyisetan et al., 2019)or learning machine learning models with privacyguarantees (mcmahan et al., 2017)..4.4 challenges.
compared to nlp approaches, proposals builtaround privacy models allow deﬁning what shouldbe protected and how.
this not only allows en-forcing privacy requirements, but also makes itpossible to tailor the trade-off between data pro-tection and utility preservation.
on the negative.
4192side, ppdp methods are hampered by practical con-straints, either because of their unfeasible assump-tions, their cost or their dependency on externalresources, such as large knowledge repositories,training corpora or social-scale probabilities.
tothe exception of c-sanitise, ppdp methods alsoassume that sensitive entities have already been de-tected in a preprocessing step.
furthermore, ppdpapproaches typically reduce documents to ﬂat col-lections of terms, which facilitates the formalisa-tion of the data semantics for each document, butalso ignores how terms are inﬂuenced by their con-text of occurrence (which is important to resolve po-tential ambiguities) and are interconnected throughmultiple layers of linguistic structures..5 case study.
to investigate the performance of nlp and ppdpmethods, we carried out a case study where 5 an-notators annotated 8 english wikipedia page ex-tracts.
the extracts were all biographies from the“20th century scientists” category, with a length be-tween 300 and 500 characters.
wikipedia articlesare generic enough not to require expert domainknowledge and are commonly adopted for the eval-uation of ppdp approaches (chow et al., 2008;s´anchez and batet, 2016).
their informativenessand density make them particularly challenging toanonymise..the annotation task2 consisted of tagging textspans that could re-identify a person either directlyor in combination with publicly available knowl-edge.
the annotators were instructed to preventidentity disclosure but otherwise seek to preserveas much semantic content as possible.
the ﬁveannotators were researchers without previous expe-rience in text anonymisation.
the guidelines wereleft intentionally general to examine how annota-tors interpret and carry out the complex task ofanonymisation – and not only de-identiﬁcation –where multiple correct solutions are possible..the task is challenging since these biographiesrelate to publicly known scientists for which ex-tensive background material can be found online.
inter-rater agreement between the ﬁve annotatorsfor the binary masking decisions was low: 0.68average observed agreement and krippendorff’sα = 0.36. this low agreement illustrates that,contrary to traditional sequence labelling, several.
2the guidelines and annotated data are publicly available:https://github.com/ildikopilan/anonymisation_acl2021.
solutions may exist for a given anonymisation prob-lem.
direct identiﬁers were generally agreed on,while quasi-identiﬁers such as professions and roles(e.g.
founder) triggered mixed decisions..to shed further light on the anonymisation prob-lem, we go on to compare the performance of ex-isting tools with the manual annotations:.
• a neural ner model (honnibal and montani,2017) trained on the ontonotes corpus with18 entity types (weischedel et al., 2011).
alldetected entities were masked.3.
• presidio4, a data protection & anonymisationapi developed by microsoft and relying on acombination of template-based and machinelearning models to detect and mask pii..• the c-sanitise privacy model (s´anchez andbatet, 2016) described in section 4, where therequired probabilities of (co-)occurrence ofterms were gathered from google..5.1 metrics.
to account for the multiple ways to anonymise adocument, we measured the performance of thethree tools above with micro-averaged scores overall annotators and texts.
note that, while micro-averages are typically used in nlp to aggregatemeasures over output classes, we are here comput-ing an average over multiple ground truths..for each annotator q ∈ q and document d ∈ d,let y qd correspond to token indices masked by qin d, and ˆyd to the token indices masked by theanonymisation tool.
precision and recall are thencomputed as:.
p =.
(cid:80).
(cid:80).
d∈d.
q∈q | ˆyd ∩ y qd |d∈d | ˆyd|.
|q| (cid:80).
r =.
(cid:80).
(cid:80).
d∈d(cid:80).
d∈d.
q∈q | ˆyd ∩ y qd |q∈q |y q(cid:80)d |.
(1).
(2).
an anonymisation tool will thus obtain a perfectmicro-averaged recall if it detects all tokens maskedby at least one annotator.
the metric implicitlyassigns a higher weight to tokens masked by severalannotators – in other words, if all ﬁve annotatorsmask a given token, not detecting it will have a.
3although ners do not speciﬁcally focus on data protec-tion, they are often used to de-identify generic texts (exceptclinical notes, for which domain-speciﬁc tools are available)..4https://github.com/microsoft/presidio.
4193ner.
presidio.
c-sanitise.
iob-exactiob-partialbinary.
iob-exactiob-partialbinary.
iob-exactiob-partialbinary.
p0.50.610.64.
0.630.740.76.
0.510.570.58.r0.490.480.51.
0.220.240.25.
0.660.680.69.f10.470.540.57.
0.330.360.38.
0.570.620.63.table 2: micro-averaged scores for ner, c-sanitiseand presidio over all texts for annotators a1, a4, a5..larger impact on the recall than a token masked bya single annotator.
recall expresses the level ofprivacy protection while precision is related to thedegree of utility preservation..the most consistent manual annotations (a1, a4,a5) were compared to system outputs at token levelboth as binary labels (keep or mask) and as iobtags expressing annotation spans5.
to go beyondtoken-level comparisons, we also computed a par-tial match score for iob tags, by assigning a weightof 0.5 to partial true positives (i.e.
i instead of btags and vice versa), as in the semeval 2013 evalu-ation scheme (diab et al., 2013)..5.2 results and error analysis.
table 2 presents the micro-averaged precision, re-call and f1 scores obtained for the three systems.
c-sanitise provided the best performance interms of recall and f1 score, while precision washigher for ner and presidio.
figure 1 illustratesthe average observed agreement for all annotatorsand tools on the binary, token-level masking deci-sions.
observed agreement with annotators was, onaverage, approximately the same for ner and c-sanitise, ca.
75% and ca.
77% for presidio.
we candistinguish two subgroups among the annotatorsin terms of mutual agreement, namely (a2, a3) and(a1, a4, a5) with 79% and 83% agreement respec-tively.
divergent choices in entity segmentation –e.g.
splitting a consecutive mention of departmentand university or not – was found to play an impor-tant role in the differences among annotators, andbetween annotators and systems..5b(eginning) represents the ﬁrst token of a span, i(nside)the subsequent tokens, and o(ut) is the label assigned to alltokens that are not part of a span..figure 1: pairwise average observed agreement.
a1 toa5 correspond to the human annotators..the proportion of masked tokens was around50% for a1, a2 and c-sanitise, < 30% for a3, a4,a5 and ner and 11% for presidio..we conducted a detailed error analysis to gaina better understanding about the advantages andshortcoming of the three anonymisation tools de-scribed above.
the ner tool masked generic enti-ties such as second world war, although this termwas not masked by any annotator or by c-sanitise.
in the phrase “a christian charity dedicated tohelping the people of cambodia”, most annotatorsdid not mask any tokens, while ner masked bothchristian and cambodia, and c-sanitise christiancharity.
on the other hand, ner ignored termsthat were highly correlated with the individual andshould have been masked, such as book titles au-thored by the person.
another interesting errorcan be found in the sentence “in 1964 and 1965he was a visiting professor at the university ofwisconsin–madison on a fulbright program fel-lowship” where the university was masked by mostannotators but left untouched by c-sanitise (as theuniversity does not frequently co-occur with thisperson in web documents).
presidio had the lowestrecall and ignored the majority of quasi-identiﬁers(including organisations).
consequently, presidio’smasking should be considered a de-identiﬁcationprocess rather than full anonymisation.
see ap-pendix a for an annotated example document..6 challenges and future directions.
the case study illustrates a number of issues fac-ing current methods for text anonymisation.
wediscuss below three overarching challenges: theneed to protect against several types of semanticinferences, the formalisation of possible maskingoperations to apply on documents, and, last but notleast, the design of evaluation metrics to empiri-cally assess the anonymisation performance..41946.1 semantic inferences.
most works on ppdp address anonymisation froma statistical perspective (batet and s´anchez, 2018).
their main focus is on the statistical properties of(numerical) data and how these may allow attackersto re-identify an individual or uncover conﬁdentialdata.
however, the most harmful inferences in textdocuments are semantic in nature – that is, they arebased on the actual meaning expressed in the textsinstead of their statistical distributions..nlp approaches do not explicitly account for se-mantic inferences, and simply mask all text spansbelonging to predeﬁned categories irrespective oftheir impact on the disclosure risk.
in many ppdpapproaches (chakaravarthy et al., 2008; cumbyand ghani, 2011; anandan et al., 2012), the ad-versary is assumed to know sets of attributes as-sociated with each entity, and semantic inferencesthus correspond to combinations of attributes en-abling the adversary to single out the entity to pro-tect.
however, in most practical settings, humanadversaries do not have access to the original doc-uments.
they do, however, make extensive use ofexternal background knowledge available, e.g., onthe web.
such external background knowledge iscaptured in s´anchez and batet (2016, 2017) using(co-)occurrence counts of terms on the web..other types of semantic inferences may betaken into account, such as lexical and taxonomicrelations (synonyms, antonyms, hypernyms, hy-ponyms) between words or entities.
for instance,the word “aids” will lead to the disclosure ofthe conﬁdential attribute “immune system disease”.
in s´anchez and batet (2017), those relations aretaken into account by enforcing consistency be-tween known taxonomic relations and the informa-tion content of each term.
semantic relations can,however, extend beyond individual terms and ex-ploit various syntactic patterns, as shown in e.g.
tex-tual entailment (dagan et al., 2013)..semantic inferences can also be drawn fromstructured data sources such as census data or med-ical knowledge bases.
in the “wisconsin-madison”example above, the search for fullbright recipientsat that university in 1964-65 would likely allow theindividual to be re-identiﬁed.
such logical infer-ences require specifying which background knowl-edge may be available to a potential intruder andwould be relevant for a given text domain..although semantic inferences have been studiedin isolation in previous work, how to integrate and.
chain together those inferential mechanisms into asingle framework remains an open question.
for-mally, assuming a document d transformed into d(cid:48)by an anonymisation tool in charge of protecting aset of entities c, one can design an adversary modeladv(c, d(cid:48), k) seeking to predict, based on docu-ment d(cid:48) and background knowledge k, whetherthe entity c was part of the original document dor not.
ideally, this adversary model should allowfor multiple types of semantic inferences based ondomain-relevant background knowledge (word co-occurrences in text corpora, taxonomic relations,knowledge bases, etc.)..
6.2 masking operations.
nlp approaches to text anonymisation essentiallyfocus on detecting personal identiﬁers and rarelydiscuss what to do with the detected text spans, gen-erally assuming that those should be either redactedor replaced with coded values.
this approach may,however, lead to unnecessary loss of data utility, asit is often possible to replace quasi-identiﬁers bymore generic (but still informative) entries..how to transform a dataset to balance disclosurerisk and data utility is a central research question inprivacy-preserving data publishing.
various trans-formations have been put forward: one can removevalues altogether, generalise them into less detailedcategories, or perturb the values by adding noiseor swapping them (domingo-ferrer et al., 2016).
in the text domain, several ppdp approacheshave shown how to generalise terms using ontolo-gies (anandan et al., 2012; s´anchez and batet,2016).
however, these approaches are intrinsi-cally limited to entities present in such ontologies,and are difﬁcult to extend to more generic textentries.
another possible transformation is to in-troduce noise into the text.
the perturbation ofdata points through noise is a common type oftransformation in data privacy (mcsherry and tal-war, 2007).
this idea of perturbation has notablybeen applied to word embeddings (feyisetan et al.,2019), but it produces perturbed word distributionsrather than readable documents.
semantic noisehas also been deﬁned to perturb nominal values(rodr´ıguez-garc´ıa et al., 2017)..formally, one can deﬁne an editor model edit(d)taking a document d and outputting an edited doc-ument d(cid:48) after applying a sequence of maskingoperations.
this model can be e.g.
expressed as aneural text editing model (mallinson et al., 2020)..4195its optimisation objective should include both min-imising the risk of letting an adversary discloseat least some of the protected entities c throughsemantic inferences (as described in the previoussection) and minimising the number of maskingoperations necessary to map d to d(cid:48)..6.3 evaluation metrics.
let d be a set of documents transformed into d(cid:48)by an anonymisation tool.
how can we empiricallyevaluate the quality of the anonymisation?.
the most common method is to rely on humanannotators to manually mark identiﬁers in each doc-ument d ∈ d, and then compare the system outputwith those human-annotated identiﬁers using ir-based metrics such as precision, recall and f1 score.
the recall can be seen as reﬂecting the degree ofprotection of the conﬁdential information, whilethe precision is correlated with the remaining datautility of the documents d(cid:48)..this evaluation procedure has a number of short-comings.
as observed in our case study, theremay be several equally valid solutions to a givenanonymisation problem.
furthermore, ir-basedmetrics typically associate uniform weights to allidentiﬁers, without taking into account the fact thatsome identiﬁers may have a much larger inﬂuenceon the disclosure risk than others.
for instance,failing to detect a full person name is more harmfulthan failing to detect a quasi-identiﬁer..finally, such type of evaluation procedure is lim-ited to the detection of direct and indirect identi-ﬁers, but ignore the subsequent step of transform-ing the textual content.
evaluating the quality ofmasking operations is tightly coupled with the prob-lem of evaluating how data utility is preservedthrough the anonymisation process (s´anchez andbatet, 2016; rodr´ıguez-garc´ıa et al., 2019).
how-ever, how to empirically measure this data utilityremains an open question..an alternative which has so far received littleattention is to conduct so-called privacy attacks onthe edited documents d(cid:48).
this can be achievedby e.g.
providing the documents d(cid:48) to human ex-perts and instruct them to re-identify those docu-ments with the help of any information source attheir disposal.
such human evaluations can helpuncover weaknesses in the anonymisation model(such as semantic inferences that had been over-looked).
however, they are also costly and time-consuming, as they must be repeated for each ver-.
sion of the anonymisation model..7 conclusion.
this position paper discussed a number of un-resolved challenges in text anonymisation.
textanonymisation is deﬁned as the removal or mask-ing of any information that, directly or indirectly,may lead to an individual being identiﬁed (givensome assumptions about the available backgroundknowledge).
as illustrated in our case study, textanonymisation is a difﬁcult task (also for humanannotators), which goes beyond the mere detectionof predeﬁned categories of entities and may allowfor several solutions.
how to properly anonymisetext data is a problem of great practical importance.
in particular, access to high-quality data is a key in-gredient for most scientiﬁc research, and the lack ofgood anonymisation methods for text documents(allowing data to be shared without compromis-ing privacy) is a limiting factor in ﬁelds such asmedicine, social sciences, psychology and law..we surveyed two families of approaches withcomplementary strengths and weaknesses: nlpmodels are well-suited to capture textual patternsbut lack any consideration of disclosure risk, whileppdp approaches provide principled accounts ofprivacy requirements, but view documents as bag-of-terms void of linguistic structure..as outlined in the last section, a promising ap-proach is to couple a neural editor model (apply-ing transformations to the text) with an adversarymodel (capturing possible semantic inferences touncover conﬁdential entities).
these two modelscan be optimised jointly using adversarial training,taking into account the necessary balance betweendisclosure risk and utility preservation..finally, we lay out a case for designing evalu-ation metrics that go beyond traditional ir-basedmeasures, and account in particular for the fact thatsome identiﬁers and quasi-identiﬁers are more im-portant than others in terms of their inﬂuence onthe disclosure risk..acknowledgements.
we acknowledge support from the norwegianresearch council (cleanup project6, grant nr.
308904), the government of catalonia (icreaacad`emia prize to d. s´anchez and grant 2017sgr 705) and the spanish government (projecttin2016-80250-r “sec-mcloud”)..6see http://cleanup.nr.no/.
4196references.
john aberdeen, samuel bayer, reyyan yeniterzi, benwellner, cheryl clark, david hanauer, bradley ma-lin, and lynette hirschman.
2010. the mitre iden-tiﬁcation scrubber toolkit: design, training, and as-international journal of medical infor-sessment.
matics, 79(12):849–859..alyaa alfalahi, sara brissman, and hercules dalia-nis.
2012. pseudonymisation of personal names andother phis in an annotated clinical swedish corpus.
in third lrec workshop on building and evaluat-ing resources for biomedical text mining (biotxtm2012), pages 49–54..balamurugan anandan and chris clifton.
2011. sig-niﬁcance of term relationships on anonymization.
in proceedings of the 2011 ieee/wic/acm inter-national joint conference on web intelligence andintelligent agent technology - workshops, wi-iat2011, pages 253–256, lyon, france..balamurugan anandan, chris clifton, wei jiang,mummoorthy murugesan,pastrana-t-plausibility:camacho, and luo si.
2012.generalizing words to desensitize text.
transac-tions on data privacy, 5(3):505–534..pedro.
montserrat batet and david s´anchez.
2018. seman-tic disclosure control: semantics meets data privacy.
online information review, 42(3):290–303..eric a. bier, richard chow, philippe golle, tracy h.king, and j. staddon.
2009. the rules of redaction:identify, protect, review (and repeat).
ieee securityand privacy magazine, 7(6):46–53..su lin blodgett, lisa green, and brendan o’connor.
2016. demographic dialectal variation in socialmedia: a case study of african-american english.
in proceedings of the 2016 conference on empiri-cal methods in natural language processing, pages1119–1130, austin, texas.
association for compu-tational linguistics..rishi bommasani, steven wu, zhiwei, and alexan-towards private syn-dra k schoﬁeld.
2019.in neurips 2019 workshopthetic text generation.
on machine learning with guarantees, vancouver,canada..david carrell, bradley malin, john aberdeen, samuelbayer, cheryl clark, ben wellner, and lynettehirschman.
2013.hiding in plain sight: useof realistic surrogates to reduce exposure of pro-tected health information in clinical text.
journalof the american medical informatics association,20(2):342–348..venkatesan t. chakaravarthy, himanshu gupta, prasanroy, and mukesh k. mohania.
2008. efﬁcient tech-in proceedingsniques for document sanitization.
of the 17th acm conference on information andknowledge management, cikm 2008, pages 843–852, napa valley, california, usa..aipeng chen,.
jitendra.
jonnagaddala, chandininekkantti, and siaw-teng liaw.
2019. generationof surrogates for de-identiﬁcation of electronicin medinfo 2019: health andhealth records.
wellbeing e-networks for all - proceedings ofthe 17th world congress on medical and healthinformatics, lyon, france, 25-30 august 2019,volume 264 of studies in health technology andinformatics, pages 70–73.
ios press..bee-chung chen, daniel kifer, kristen lefevre,privacy-foundations and.
and ashwin machanavajjhala.
2009.preserving data publishing.
trends in databases.
now publishers inc..rapha¨el chevrier, vasiliki fouﬁ, christophe gaudet-blavignac, arnaud robert, and christian lovis.
2019. use and understanding of anonymization andde-identiﬁcation in the biomedical literature: scop-ing review.
journal of medical internet research,21(5):e13484..jason p.c.
chiu and eric nichols.
2016. named entityrecognition with bidirectional lstm-cnns.
trans-actions of the association for computational lin-guistics, 4:357–370..richard chow, philippe golle, and jessica staddon.
2008. detecting privacy leaks using corpus-basedthe 14thassociation rules.
acm sigkdd international conference on knowl-edge discovery and data mining, kdd ’08, page893–901, new york, ny, usa.
association forcomputing machinery..in proceedings of.
chad m. cumby and rayid ghani.
2011. a machinelearning based system for semi-automatically redact-ing documents.
in proceedings of the twenty-thirdconference on innovative applications of artiﬁcialintelligence, pages 1628–1635, san francisco, cali-fornia, usa..ido dagan, dan roth, mark sammons, and fabio mas-simo zanzotto.
2013. recognizing textual entail-ment: models and applications.
synthesis lectureson human language technologies, 6(4):1–220..franck dernoncourt, ji young lee, ozlem uzuner,and peter szolovits.
2017. de-identiﬁcation of pa-tient notes with recurrent neural networks.
journalof the american medical informatics association,24(3):596–606..mona diab, tim baldwin, and marco baroni, editors.
2013. second joint conference on lexical and com-putational semantics (*sem), volume 1: proceed-ings of the main conference and the shared task:semantic textual similarity.
association for compu-tational linguistics, atlanta, georgia, usa..josep domingo-ferrer, david s´anchez, and albertoblanco-justicia.
2021. the limits of differential pri-vacy (and its misuse in data release and machinelearning).
communications of the acm, 64(7):34–36..4197josep domingo-ferrer, david s´anchez, and jordi soria-comas.
2016. database anonymization: privacymodels, data utility, and microaggregation-basedinter-model connections.
synthesis lectures oninformation security, privacy & trust.
morgan &claypool publishers..tzvika hartman, michael d howell,.
jeff dean,itay laish, orenshlomo hoory, ronit slyper,gilon, danny vainstein, greg corrado, katherinechou, et al.
2020. customization scenarios for de-identiﬁcation of clinical notes.
bmc medical infor-matics and decision making, 20(1):1–9..cynthia dwork, frank mcsherry, kobbi nissim, andadam smith.
2006. calibrating noise to sensitiv-ity in private data analysis.
in theory of cryptog-raphy, pages 265–284, berlin, heidelberg.
springerberlin heidelberg..elisabeth eder, ulrike krieg-holz, and udo hahn.
2019. de-identiﬁcation of emails: pseudonymiz-ing privacy-sensitive data in a german email corpus.
in proceedings of the international conference onrecent advances in natural language processing(ranlp 2019), pages 259–269, varna, bulgaria.
in-coma ltd..mike hintze.
2017. viewing the gdpr through a de-identiﬁcation lens: a tool for compliance, clariﬁca-international data privacytion, and consistency.
law, 8(1):86–101..hipaa.
2004. the health insurance portability andaccountability act.
u.s. dept.
of labor, employeebeneﬁts security administration..matthew honnibal and ines montani.
2017. spacy 2:natural language understanding with bloom embed-dings, convolutional neural networks and incremen-tal parsing.
to appear..elisabeth eder, ulrike krieg-holz, and udo hahn.
2020.code alltag 2.0 — a pseudonymizedgerman-language email corpus.
in proceedings ofthe 12th language resources and evaluation con-ference, pages 4466–4477, marseille, france.
euro-pean language resources association..yangsibo huang, zhao song, danqi chen, kai li, andsanjeev arora.
2020. texthide: tackling data pri-vacy in language understanding tasks.
in findingsof the association for computational linguistics:emnlp 2020, pages 1368–1382, online.
associa-tion for computational linguistics..yanai elazar and yoav goldberg.
2018. adversarialremoval of demographic attributes from text data.
in proceedings of the 2018 conference on empiri-cal methods in natural language processing, pages11–21, brussels, belgium.
association for computa-tional linguistics..mark elliot, elaine mackey, kieron o’hara, and car-the anonymisation decision-.
oline tudor.
2016.making framework.
ukan manchester..natasha fernandes, mark dras, and annabelle mciver.
2018. generalised differential privacy for text docu-ment processing.
corr, abs/1811.10256..oluwaseyi feyisetan, tom diethe, and thomas drake.
2019. leveraging hierarchical representations forpreserving privacy and utility in text.
in 2019 ieeeinternational conference on data mining (icdm),pages 210–219.
ieee..max friedrich, arne k¨ohn, gregor wiedemann,and chris biemann.
2019. adversarial learningof privacy-preserving text representations for de-identiﬁcation of medical records.
in proceedings ofthe 57th annual meeting of the association for com-putational linguistics, pages 5829–5839, florence,italy.
association for computational linguistics..gdpr.
2016. general data protection regulation.
eu-.
ropean union regulation 2016/679..guillaume lample, miguel ballesteros, sandeep sub-ramanian, kazuya kawakami, and chris dyer.
2016.neural architectures for named entity recognition.
in proceedings of the 2016 conference of the northamerican chapter of the association for computa-tional linguistics: human language technologies,pages 260–270, san diego, california..zengjian liu, buzhou tang, xiaolong wang, and qing-cai chen.
2017. de-identiﬁcation of clinical notesvia recurrent neural network and conditional randomﬁeld.
journal of biomedical informatics, 75:s34–s42..jonathan mallinson, aliaksei severyn, eric malmi, andguillermo garrido.
2020. felix: flexible text edit-ing through tagging and insertion.
arxiv preprintarxiv:2003.10687..montserrat marimon, aitor gonzalez-agirre, anderintxaurrondo, heidy rodriguez, jose lopez martin,marta villegas, and martin krallinger.
2019. au-tomatic de-identiﬁcation of medical texts in span-ish: the meddocan track, corpus, guidelines, meth-ods and evaluation of results.
in iberlef@ sepln,pages 618–638..h. brendan mcmahan, daniel ramage, kunal talwar,and li zhang.
2017. learning differentially privatearxiv:1710.06963recurrent language models.
[cs]..philippe golle.
2006. revisiting the uniqueness of sim-ple demographics in the us population.
in proceed-ings of the 5th acm workshop on privacy in elec-tronic society, pages 77–80.
acm..frank mcsherry and kunal talwar.
2007. mecha-nism design via differential privacy.
in 48th annualieee symposium on foundations of computer sci-ence (focs’07), pages 94–103..4198ben medlock.
2006. an introduction to nlp-basedthetextual anonymisation.
fifth international conference on language re-sources and evaluation (lrec’06), pages 1051–1056, genoa, italy.
european language resourcesassociation (elra)..in proceedings of.
be´ata megyesi, lena granstedt, soﬁa johansson, ju-lia prentice, dan ros´en, carl-johan schenstr¨om,gunl¨og sundberg, mats wir´en, and elena volodina.
2018. learner corpus anonymization in the age ofgdpr: insights from the creation of a learner cor-pus of swedish.
in proceedings of the 7th workshopon nlp for computer assisted language learning,pages 47–56, stockholm, sweden.
liu electronicpress..st´ephane m meystre,.
´oscar ferr´andez, f jeffreyfriedlin, brett r south, shuying shen,andmatthew h samore.
2014a.
text de-identiﬁcationfor privacy protection: a study of its impact on clini-cal text information content.
journal of biomedicalinformatics, 50:142–150..stephane m meystre, f jeffrey friedlin, brett r south,shuying shen, and matthew h samore.
2010. au-tomatic de-identiﬁcation of textual documents in theelectronic health record: a review of recent research.
bmc medical research methodology, 10(1):70..st´ephane meystre, shuying shen, deborah hofmann,and adi gundlapalli.
2014b.
can physicians rec-ognize their own patients in de-identiﬁed notes?
studies in health technology and informatics,205:778—782..ahmadreza mosallanezhad, ghazaleh beigi, and huanliu.
2019. deep reinforcement learning-based textanonymization against private-attribute inference.
in proceedings of the 2019 conference on empiricalmethods in natural language processing and the9th international joint conference on natural lan-guage processing (emnlp-ijcnlp), pages 2360–2369, hong kong, china.
association for computa-tional linguistics..sravana reddy and kevin knight.
2016. obfuscatingin proceedings ofgender in social media writing.
the first workshop on nlp and computational so-cial science, pages 17–26, austin, texas.
associa-tion for computational linguistics..mercedes rodr´ıguez-garc´ıa, montserrat batet, anddavid s´anchez.
2017. a semantic framework fornoise addition with nominal data.
knowledge-basedsystems, 122(c):103–118..mercedes rodr´ıguez-garc´ıa, montserrat batet, anddavid s´anchez.
2019. utility-preserving privacyprotection of nominal data sets via semantic rankswapping.
information fusion, 45:282–295..mark a. rothstein.
2010. is deidentiﬁcation sufﬁcientto protect health privacy in research?
the americanjournal of bioethics, 10(9):3–11..pierangela samarati.
2001..protecting respondents’identities in microdata release.
ieee transactionson knowledge and data engineering, 13(6):1010–1027..pierangela samarati and latanya sweeney.
1998. pro-tecting privacy when disclosing information: k-anonymity and its enforcement through generaliza-tion and suppression.
technical report, sri interna-tional..amber stubbs, michele filannino, and ¨ozlem uzuner.
2017.de-identiﬁcation of psychiatric intakerecords: overview of 2016 cegs n-grid sharedtasks track 1. journal of biomedical informatics,75:s4–s18..amber stubbs and ¨ozlem uzuner.
2015. annotatinglongitudinal clinical narratives for de-identiﬁcation:journal ofthe 2014 i2b2/uthealth corpus.
biomedical informatics, 58:s20–s29..latanya sweeney.
1996..replacing personally-identifying information in medical records, the scrubin proceedings of the amia annual fallsystem.
symposium, pages 333–337.
american medical in-formatics association..david s´anchez and montserrat batet.
2016..c-sanitized: a privacy model for document redactionand sanitization.
journal of the association for in-formation science and technology, 67(1):148–163..david s´anchez and montserrat batet.
2017. towardsensitive document release with privacy guarantees.
engineering applications of artiﬁcial intelligence,59:23–34..sumithra velupillai, hercules dalianis, martin has-sel, and gunnar h. nilsson.
2009. developing astandard for de-identifying electronic patient recordswritten in swedish: precision, recall and f -measurein a manual and computerized annotation trial.
inter-national journal of medical informatics, 78(12):19– 26..elena volodina, yousuf ali mohammed, sandra der-bring, arild matsson, and beata megyesi.
2020. to-wards privacy by design in learner corpora research:a case of on-the-ﬂy pseudonymization of swedishin proceedings of the 28th inter-learner essays.
national conference on computational linguistics,pages 357–369, barcelona, spain (online).
interna-tional committee on computational linguistics..ralph weischedel, eduard hovy, marcus.
mitchell,palmer martha s., robert belvin, sameer s. prad-han, lance ramshaw, and nianwen xue.
2011.ontonotes: a large training corpus for enhancedprocessing.
in handbook of natural language pro-cessing and machine translation: darpa globalautonomous language exploitation.
springer..alan f. westin.
1967.atheneum, new york..privacy and freedom..4199qiongkai xu, lizhen qu, chenchen xu, and ran cui.
in proceed-2019. privacy-aware text rewriting.
ings of the 12th international conference on nat-ural language generation, pages 247–257, tokyo,japan.
association for computational linguistics..xi yang, tianchen lyu, qian li, chih-yin lee,jiang bian, william r hogan, and yonghui wu.
2019. a study of deep learning methods for de-identiﬁcation of clinical notes in cross-institute set-tings.
bmc medical informatics and decision mak-ing, 19(5):232..vithya yogarajan, michael mayo, and bernharda survey of automatic de-longitudinal clinical narratives..pfahringer.
2018.identiﬁcation ofarxiv preprint arxiv:1810.06765..4200a appendix.
we present below the annotation of one short biography of a 20th century scientist (alexander frumkin)according to 5 human annotators, c-sanitize, the neural ner model and the presidio anonymisation tool(see paper for details).
the annotation task consisted of tagging text spans that could re-identify a personeither directly or in combination with publicly available knowledge.
the annotators were instructed toprevent identity disclosure, but otherwise seek to preserve the semantic content as much as possible.
theﬁve annotators were researchers in statistics and natural language processing..the ﬁrst ﬁve (gray) lines denotes the ﬁve human annotators, while the cyan line corresponds to.
c-sanitise, the blue line to the neural ner model, and the green line to the presidio tool..due to page limits, we only present here one single biography, but the annotations for all 8 texts (alongwith the annotation guidelines and raw data) are available in the github repository associated with thepaper..a.1 alexander frumkin.
alexander naumovich frumkin (александр наумович фрумкин) (october 24, 1895–may 27, 1976).
was a russian/soviet electrochemist, member of the russian academy of sciences since.
1932, founder of the russian journal of electrochemistry elektrokhimiya and receiver.
of the hero of socialist labor award.
the russian academy of sciences’ a. n. frumkin.
institute of physical chemistry and electrochemistry is named after him.
frumkin was.
born in kishinev, in the bessarabia governorate of the russian empire (present-day moldova).
to a jewish family; his father was an insurance salesman.
his family moved to odessa,.
where he received his primary schooling; he continued his education in strasbourg, and.
then at the university of bern.
frumkin’s ﬁrst published articles appeared in 1914,.
4201when he was only 19; in 1915, he received his ﬁrst degree, back in odessa.
two years.
later, the seminal article “electrocapillary phenomena and electrode potentials” was.
published.
frumkin moved to moscow in 1922 to work at the karpov institute, under a..n. bakh.
in 1930 frumkin joined the faculty of moscow university, where in 1933 he founded—and.
would head until his death—the department of electrochemistry.
during the second world.
war, frumkin led a large team of scientists and engineers involved in defense issues..this contribution did not save him from being dismissed in 1949 as the director of the.
institute of physical chemistry, when he was accused of “cosmopolitanism”.
frumkin’s.
most fundamental achievement was the fundamental theory of electrode reactions, which.
describes the inﬂuence of the structure of the interface between electrode and solution.
on the rate of electron transfer.
this theory has been conﬁrmed and extended within.
the framework of contemporary physical electron transfer models.
frumkin introduced the.
concept of the zero charge potential, the most important characteristic of a metal surface..4202alessandro volta’s question—a topic of discussion for over 120 years—about the nature.
of the emf of electrochemical circuits was resolved using frumkin’s approach.
frumkin.
developed the frumkin isotherm, an extension of the langmuir isotherm in describing certain.
adsorption phenomena.
frumkin’s students developed novel experimental methods that would,.
in time, become standard.
several applied electrochemical processes, including ones related.
to chemical sources of electrical power, industrial electrolysis, and anti-corrosion.
protection, were successfully developed under frumkin’s supervision.
frumkin was married.
three times, including a brief ﬁrst marriage to vera inber..4203