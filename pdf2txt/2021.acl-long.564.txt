mind your outliers!
investigating the negative impact of outliers onactive learning for visual question answering.
siddharth karamcheti ranjay krishna li fei-fei christopher d. manningdepartment of computer science, stanford university{skaramcheti, ranjaykrishna, feifeili, manning}@cs.stanford.edu.
abstract.
active learning promises to alleviate the mas-sive data needs of supervised machine learn-ing:it has successfully improved sample ef-ﬁciency by an order of magnitude on tradi-tional tasks like topic classiﬁcation and objectrecognition.
however, we uncover a strikingcontrast to this promise: across 5 models and4 datasets on the task of visual question an-swering, a wide variety of active learning ap-proaches fail to outperform random selection.
to understand this discrepancy, we proﬁle 8active learning methods on a per-example ba-sis, and identify the problem as collective out-liers – groups of examples that active learningmethods prefer to acquire but models fail tolearn (e.g., questions that ask about text in im-ages or require external knowledge).
throughsystematic ablation experiments and qualita-tive visualizations, we verify that collectiveoutliers are a general phenomenon responsi-ble for degrading pool-based active learning.
notably, we show that active learning sampleefﬁciency increases signiﬁcantly as the num-ber of collective outliers in the active learningpool decreases.
we conclude with a discussionand prescriptive recommendations for mitigat-ing the effects of these outliers in future work..1.introduction.
today, language-equipped vision systems such asvizwiz, taptapsee, bemyeyes, and camfind areactively being deployed across a broad spectrumof users.1 as underlying methods improve, thesesystems will be expected to operate over diverse vi-sual environments and understand myriad languageinputs (bigham et al., 2010; tellex et al., 2011;mei et al., 2016; zhu et al., 2017; anderson et al.,2018b; park et al., 2019).
visual question answer-ing (vqa), the task of answering questions about.
1applications can be found at https://vizwiz.org/,https://taptapsee.com/, https://www.bemyeyes.com/,and https://camfindapp.com/.
figure 1: we systematically evaluate active learningon vqa datasets and isolate their inability to performbetter than random sampling due to the presence of col-lective outliers.
active learning methods prefer to ac-quire these outliers, which are hard and often impos-sible for models to learn.
we show that dataset maps,like the one shown here, can heuristically identify thesecollective outliers as examples assigned low model con-ﬁdence and prediction variability during training..visual inputs, is a popular benchmark used to eval-uate progress towards such open-ended systems(agrawal et al., 2015; krishna et al., 2017; gordonet al., 2018; hudson and manning, 2019).
unfortu-nately, today’s vqa models are data hungry: theirperformance scales monotonically with more train-.
proceedingsofthe59thannualmeetingoftheassociationforcomputationallinguisticsandthe11thinternationaljointconferenceonnaturallanguageprocessing,pages7265–7281august1–6,2021.©2021associationforcomputationallinguistics7265q: what sport is she playing?a: tennisexpects models to read text:q: what is the word on the wall?a: ricerequires external knowledge:q: what is the symbol on the hood often associated with?a: piratescollective outliersdataset map for bottom-up top-down model on vqa 2useful dataq: what is the person holding?a: surfboarding data (lu et al., 2016; lin and parikh, 2017), mo-tivating the need for data acquisition mechanismssuch as active learning, which maximize perfor-mance while minimizing expensive data labeling..while active learning is often key to effectivedata acquisition when such labeled data is difﬁ-cult to obtain (lewis and catlett, 1994; tong andkoller, 2001; culotta and mccallum, 2005; settles,2009), we ﬁnd that 8 modern active learning meth-ods (gal et al., 2017; siddhant and lipton, 2018;lowell et al., 2019) show little to no improvementin sample efﬁciency across 5 models on 4 vqadatasets – indeed, in some cases performing worsethan randomly selecting data to label.
this ﬁndingis in stark contrast to the successful application ofactive learning methods on a variety of traditionaltasks, such as topic classiﬁcation (siddhant andlipton, 2018; lowell et al., 2019), object recogni-tion (deng et al., 2018), digit classiﬁcation (galet al., 2017), and named entity recognition (shenet al., 2017).
our negative results hold even whenaccounting for common active learning ailments:cold starts, correlated sampling, and uncalibrateduncertainty.
we mitigate the cold start challenge ofneeding a representative initial dataset by varyingthe size of the seed set in our experiments.
weaccount for sampling correlated data within a givenbatch by including core-set selection (sener andsavarese, 2018) in the set of active learning meth-ods we evaluate.
finally, we use deep bayesianactive learning to calibrate model uncertainty tohigh-dimensional data (houlsby et al., 2011; galand ghahramani, 2016; gal et al., 2017)..after concluding that negative results are con-sistent across all experimental conditions, we in-vestigate active learning’s ineffectiveness on vqaas a data problem and identify the existence ofcollective outliers (han and kamber, 2000) as thesource of the problem.
leveraging recent advancesin model interpretability, we build dataset maps(swayamdipta et al., 2020), which distinguish be-tween collective outliers and useful data that im-prove validation set performance (see figure 1).
while global outliers deviate from the rest of thedata and are often a consequence of labeling error,collective outliers cluster together; they may notindividually be identiﬁable as outliers but collec-tively deviate from other examples in the dataset.
for instance, vqa-2 (goyal et al., 2017) is riddledwith collections of hard questions that require exter-nal knowledge to answer (e.g., “what is the symbol.
on the hood often associated with?”) or that askthe model to read text in the images (e.g., “what isthe word on the wall?”).
similarly, gqa (hudsonand manning, 2019) asks underspeciﬁed questions(e.g., “what is the person wearing?” which canhave multiple correct answers).
collective outliersare not speciﬁc to vqa, but can similarly be foundin many open-ended tasks, including visual navi-gation (anderson et al., 2018b) (e.g., “go to thegrandfather clock” requires identifying rare grand-father clocks), and open-domain question answer-ing (kwiatkowski et al., 2019), amongst others..using dataset maps, we proﬁle active learningmethods and show that they prefer acquiring col-lective outliers that models are unable to learn, ex-plaining their poor improvements in sample efﬁ-ciency relative to random sampling.
building onthis, we use these maps to perform ablations wherewe identify and remove outliers iteratively fromthe active learning pool, observing correlated im-provements in sample efﬁciency.
this allows usto conclude that collective outliers are, indeed, re-sponsible for the ineffectiveness of active learningfor vqa.
we end with prescriptive suggestions forfuture work in building active learning methodsrobust to these types of outliers..2 related work.
our work tests the utility of multiple recent activelearning methods on the open-ended understandingtask of vqa.
we draw on the dataset analysis liter-ature to identify collective outliers as the bottleneckhindering active learning methods in this setting..active learning.
active learning strategies havebeen successfully applied to image recognition(joshi et al., 2009; sener and savarese, 2018), in-formation extraction (scheffer et al., 2001; finnand kushmerick, 2003; jones et al., 2003; culottaand mccallum, 2005), named entity recognition(hachey et al., 2005; shen et al., 2017), semanticparsing (dong et al., 2018), and text categorization(lewis and gale, 1994; hoi et al., 2006).
however,these same methods struggle to outperform a ran-dom baseline when applied to the task of vqa (linand parikh, 2017; jedoui et al., 2019).
to studythis discrepancy, we systematically apply 8 diverseactive learning methods to vqa, including meth-ods that use model uncertainty (abramson and fre-und, 2004; collins et al., 2008; joshi et al., 2009),bayesian uncertainty (gal and ghahramani, 2016;kendall and gal, 2017), disagreement (houlsby.
7266et al., 2011; gal et al., 2017), and core-set selec-tion (sener and savarese, 2018)..visual question answering.
progress on vqahas been heralded as a marker for progress on gen-eral open-ended understanding tasks, resulting inseveral benchmarks (agrawal et al., 2015; mali-nowski et al., 2015; ren et al., 2015a; johnsonet al., 2017; goyal et al., 2017; krishna et al., 2017;suhr et al., 2019; hudson and manning, 2019) andmodels (zhou et al., 2015; fukui et al., 2016; luet al., 2016; yang et al., 2016; zhu et al., 2016;wu et al., 2016; anderson et al., 2018a; tan andbansal, 2019; chen et al., 2020).
to ensure that ournegative results are not dataset or model-speciﬁc,we sample 4 datasets and 5 representative models,each utilizing unique visual and linguistic featuresand employing different inductive biases..interpreting and analyzing datasets.
giventhe prevalence of large datasets in modern machinelearning, it is critical to assess dataset propertiesto remove redundancies (gururangan et al., 2018;li and vasconcelos, 2019) or biases (torralba andefros, 2011; khosla et al., 2012; bolukbasi et al.,2016), both of which negatively impact sample ef-ﬁciency.
prior work has used training dynamicsto ﬁnd examples which are frequently forgotten(krymolowski, 2002; toneva et al., 2019) versusthose that are easy to learn (bras et al., 2020).
thiswork suggests using two model-speciﬁc measures– conﬁdence and prediction variance – as indica-tors of a training example’s “learnability” (changet al., 2017; swayamdipta et al., 2020).
datasetmaps (swayamdipta et al., 2020), a recently in-troduced framework uses these two measures toproﬁle datasets to ﬁnd learnable examples.
unlikeprior datasets analyzed by dataset maps that havea small number of global outliers as hard examples,we discover that vqa datasets contain copiousamounts of collective outliers, which are difﬁcultor even impossible for models to learn..3 active learning experimental setup.
we adopt the standard pool-based active learningsetup from prior work (lewis and gale, 1994;settles, 2009; gal et al., 2017; lin and parikh,2017), consisting of a model m, initial seed setof labeled examples (xi, yi) ∈ dseed used to ini-tialize m, an unlabeled pool of data dpool, andan acquisition function a(x, m).
we run ac-tive learning over a series of acquisition iterations.
pool size.
# answers.
vqa-sportsvqa-foodvqa-2gqa.
5,411 [5k]4,082 [4k]411,272 [400k]943,000 [900k].
202031301842.table 1: we evaluate active learning on 4 vqa datasets.
we display the total available training examples, effec-tive pool sizes we use [in brackets], and the total num-ber of possible answers for each dataset..t where at each iteration we acquire a batch ofb new examples per: ˆx ∈ dpool to label pera(x, m).
ˆx = arg maxx∈dpoolacquiring an example often refers to using anoracle or human expert to annotate a new exam-ple with a correct label.
we follow prior work tosimulate an oracle using existing datasets, formingdseed from a ﬁxed percentage of the full dataset,and using the remainder as dpool (gal et al., 2017;lin and parikh, 2017; siddhant and lipton, 2018).
we re-train m after each acquisition iteration..prior work has noted the impact of seed set sizeon active learning performance (lin and parikh,2017; misra et al., 2018; jedoui et al., 2019).
werun multiple active learning evaluations with vary-ing seed set sizes (ranging from 5% to 50% of thefull pool size).
we keep the size of each acquisitionbatch b to a constant 10% of the overall pool size..3.1 models.
visual question answering (vqa) requires rea-soning over two modalities: images and text.
mostmodels use feature “backbones” (e.g., featuresfrom object recognition models pretrained on ima-genet, and pretrained word vectors for text).
forimage features we use grid-based features fromresnet-101 (he et al., 2016), or object-based fea-tures from faster r-cnn (ren et al., 2015b) ﬁne-tuned on visual genome (anderson et al., 2018a).
we evaluate with a representative sample of exist-ing vqa models, including the following:2.logreg is a logistic regression model that useseither resnet-101 or faster r-cnn image featureswith mean-pooled glove question embeddings(pennington et al., 2014).
although these models.
2key implementation details can be found in the appendix.
in the interest of full reproducibility and further work in ac-tive learning and vqa, we release our code and results here:https://github.com/siddk/vqa-outliers..7267are not as performant as the subsequent models, lo-gistic regression has been effective on vqa (suhret al., 2019), and is pervasive in the active learningliterature (schein and ungar, 2007; yang and loog,2018; mussmann and liang, 2018)..lstm-cnn is a standard model introducedwith vqa-1 (agrawal et al., 2015).
we use moreperformant resnet-101 features instead of the orig-inal vggnet features as our visual backbone..butd (bottom-up top-down attention) usesobject-based features in tandem with attention overobjects (anderson et al., 2018a).
butd won the2017 vqa challenge (teney et al., 2018), and hasbeen a consistent baseline for recent work in vqa..lxmert is a large multi-modal transformermodel that uses butd’s object features and con-textualized bert (devlin et al., 2019) languagefeatures (tan and bansal, 2019).
lxmert is pre-trained on a corpus of aligned image-and-textualdata spanning ms coco, visual genome, vqa-2,nlvr-2, and gqa (lin et al., 2014; krishna et al.,2017; goyal et al., 2017; suhr et al., 2019; hud-son and manning, 2019), initializing a cross-modalrepresentation space conducive to ﬁne-tuning.3.
3.2 acquisition functions.
several active learning methods have been devel-oped to account for different aspects of the machinelearning training pipeline: while some acquire ex-amples with high aleotoric uncertainty (settles,2009) (having to do with the natural uncertaintyin the data) or epistemic uncertainty (gal et al.,2017) (having to do with the uncertainty in themodeling/learning process), others attempt to ac-quire examples that reﬂect the distribution of datain the pool (sener and savarese, 2018).
we samplea diverse set of these methods:.
random sampling serves as our baseline pas-sive approach for acquiring examples..least conﬁdenceest model prediction probability (settles, 2009)..acquires examples with low-.
3results for lxmert in tan and bansal (2019) are re-ported after pretraining on training and validation examplesfrom the vqa datasets we use.
while this is fair if the goal isoptimizing for test performance, this exposure to training andvalidation examples leaks important information; to remedythis, we obtained a model checkpoint from the lxmertauthors trained without vqa data.
this is also why ourlxmert results are lower than the numbers reported in theoriginal paper – however, the general boost provided by cross-modal pretraining holds..entropy acquires examples with the highest en-tropy in the model’s output (settles, 2009)..mc-dropout entropy (monte-carlo dropoutwith entropy acquisition) acquires examples withhigh entropy in the model’s output averaged overmultiple passes through a neural network with dif-ferent dropout masks (gal and ghahramani, 2016).
this process is a consequence of a theoretical cast-ing of dropout as approximate bayesian inferencein deep gaussian processes..bald (bayesian active learning by disagree-ment) builds upon monte-carlo dropout by propos-ing a decision theoretic objective; it acquires exam-ples that maximise the decrease in expected poste-rior entropy (houlsby et al., 2011; gal et al., 2017;siddhant and lipton, 2018) – capturing “disagree-ment” across different dropout masks..core-set selection samples examples that cap-ture the diversity of the data pool (sener andsavarese, 2018; coleman et al., 2020).
it acquiresexamples to minimize the distance between an ex-ample in the unlabeled pool to its closest labeledexample.
since core-set selection operates over arepresentation space (and not an output distribution,like prior strategies) and vqa models operate overtwo modalities, we employ three core-set variants:core-set (language) and core-set (vision) op-erate over their respective representation spaceswhile core-set (fused) operates over the “fused”vision and language representation space..4 experimental results.
we evaluate the 8 active learning strategies acrossthe 5 models described in the previous section.
fig-ures 2–5 show a representative sample of activelearning results across datasets.
due to space con-straints, we only visualize 4 active learning strate-gies – least-conﬁdence, bald, coreset-fused,and the random baseline – using 3 models (lstm-cnn, butd, lxmert).4 results and trends areconsistent across the different acquisition functions,models and seed set sizes (see the appendix for re-sults with other models, acquisition functions, andseed set sizes).
we now go on to provide descrip-tions of the datasets we evaluate against, and thecorresponding results..4for lxmert, running core-set selection is prohibitive,so we omit these results; please see appendix b for moredetails..7268figure 2: results for varied active learning methods on vqa-sports, a simpliﬁed vqa dataset.
strategies performon par with or worse than the random baseline, when using 10% of the full dataset as the seed set..figure 3: results for the full vqa-2 dataset, also using 10% of the full dataset as a seed set.
similar to the plotabove, all active learning methods perform similar to a random baseline..figure 4: results on vqa-2 using 50% of the dataset as a seed set.
while methods are relatively better when usinga larger seed set—conﬁrming results from (lin and parikh, 2017)—no methods outperform random..figure 5: results on gqa using 10% of the dataset for the seed set.
even with different question structures, theabove trends hold, with strategies performing worse than or equivalent to random..72695001k1.5k2k2.5k3k3.5k4k4.5k5knumber of training examples0.20.30.40.50.60.70.80.91.0validation accuracylstm-cnn - vqa-sportsrandom baselineleast-confidencebaldcore-set (fused)5001k1.5k2k2.5k3k3.5k4k4.5k5knumber of training examples0.20.30.40.50.60.70.80.91.0validation accuracybutd - vqa-sports5001k1.5k2k2.5k3k3.5k4k4.5k5knumber of training examples0.20.30.40.50.60.70.80.91.0validation accuracylxmert - vqa-sports40k80k120k160k200k240k280k320k360k400knumber of training examples0.20.30.40.50.60.70.80.91.0validation accuracylstm-cnn - vqa-2random baselineleast-confidencebaldcore-set (fused)40k80k120k160k200k240k280k320k360k400knumber of training examples0.20.30.40.50.60.70.80.91.0validation accuracybutd - vqa-240k80k120k160k200k240k280k320k360k400knumber of training examples0.20.30.40.50.60.70.80.91.0validation accuracylxmert - vqa-2200k240k280k320k360k400knumber of training examples0.20.30.40.50.60.70.80.91.0validation accuracylstm-cnn - vqa-2random baselineleast-confidencebaldcore-set (fused)200k240k280k320k360k400knumber of training examples0.20.30.40.50.60.70.80.91.0validation accuracybutd - vqa-2200k240k280k320k360k400knumber of training examples0.20.30.40.50.60.70.80.91.0validation accuracylxmert - vqa-290k180k270k360k450k540k630k720k810k900knumber of training examples0.20.30.40.50.60.70.80.91.0validation accuracylstm-cnn - gqarandom baselineleast-confidencebaldcore-set (fused)90k180k270k360k450k540k630k720k810k900knumber of training examples0.20.30.40.50.60.70.80.91.0validation accuracybutd - gqa90k180k270k360k450k540k630k720k810k900knumber of training examples0.20.30.40.50.60.70.80.91.0validation accuracylxmert - gqafigure 6: we visualize the difference in acquisition preferences between random and active learning acquisitions(least conﬁdence and bald) across multiple iterations.
active learning methods prefer to sample impossibleexamples which models are unable to learn, hurting sample efﬁciency relative to the random baseline..4.1 simpliﬁed vqa datasets.
one complexity of vqa is the size of the out-put space and the number of examples present(agrawal et al., 2015; goyal et al., 2017); vqa-2has 400k training examples, and in excess of 3k pos-sible answers (see table 1).
however, prior workin active learning focuses on smaller datasets likethe 10-class mnist dataset (gal et al., 2017), bi-nary classiﬁcation (siddhant and lipton, 2018), orsmall-cardinality (≤ 20 classes) text categorization(lowell et al., 2019).
to ensure our results and con-clusions are not due to the size of the output space,we build two meaningful, but narrow-domain vqadatasets from subsets of vqa-2.
these simpliﬁeddatasets reduce the complexity of the underlyinglearning problem and provide a fair comparison toexisting active learning literature..vqa-sports.
we generate vqa-sports by com-piling a list of 20 popular sports (e.g., soccer, foot-ball, tennis, etc.)
in vqa-2, and restricting the setof questions to those with answers in this list.
wepicked the sports categories by ranking the glovevector similarity between the word “sports” to an-swers in vqa-2, and selected the 20 most com-monly occurring answers..vqa-food.
we generate the vqa-food datasetsimilarly, compiling a list of the 20 commonly oc-curring food categories by glove vector similarityto the word “food.”.
results.
figure 2 presents results for vqa-sports, with an initial seed set restricted to 10%of the total pool (500 examples).
the appendixreports similar results on vqa-food.
for lstm-cnn, least-conﬁdence appears to be slightly moresample efﬁcient, while all other strategies perform.
on par with or worse than random.
for butd, allmethods are on par with random; for lxmert,they perform worse than random.
generally onvqa-sports, active learning performance varies,but fails to outperform random acquisition..4.2 vqa-2.
vqa-2 is the canonical dataset for evaluating vqamodels (goyal et al., 2017).
in keeping with priorwork (anderson et al., 2018a; tan and bansal,2019), we ﬁlter the training set to only includeanswers that appear at least 9 times, resulting in3130 unique answers.
unlike traditional vqa-2evaluation, which treats the task as a multi-labelbinary classiﬁcation problem, we follow prior ac-tive learning work on vqa (lin and parikh, 2017),which formulates it as a multi-class classiﬁcationproblem, enabling the use of acquisition functionssuch as uncertainty sampling and bald..results.
figures 3 and 4 show results on vqa-2with different seed set sizes – 10% (40k examples)and 50% (200k examples).
active learning per-forms relatively better with larger seed sets but stillunderperforms random.
surprisingly, when initial-ized with 50% of the pool as the seed set, the gainin validation accuracy after acquiring the entirepool of examples (400k examples total) is only 2%.
this is an indication that the lack of sample efﬁ-ciency might be a result of the underlying data, aproblem we explore in the next section..4.3 gqa.
gqa was introduced as a means for evaluatingcompositional reasoning (hudson and manning,2019).
unlike vqa’s natural human-written ques-tions, gqa contains synthetic questions of theform “what is inside the bottle the glasses are to.
727040k80k120k160k200k240k280k320k360knumber of training examples0 5k10k15k20k25k30k35k40kacquisitions by difficultyrandom baselineimpossible [p > 0.0]hard [p > 0.25]medium [p > 0.5]easy [p > 0.75]40k80k120k160k200k240k280k320k360knumber of training examples0 5k10k15k20k25k30k35k40kacquisitions by difficultyleast-confidence40k80k120k160k200k240k280k320k360knumber of training examples0 5k10k15k20k25k30k35k40kacquisitions by difficultybaldfigure 7: example groups of collective outliers in the vqa-2 and gqa datasets..the right of?”.
we use the standard gqa trainingset of 943k questions, 900k of which we use forthe active learning pool..results.
figure 5 shows results on gqa usinga seed set of 10% of the full pool (90k examples).
despite its notable differences in question structureto vqa-2, active learning still performs on parwith or slightly worse than random..5 analysis via dataset maps.
the previous section shows that active learningfails to improve over random acquisition on vqaacross models and datasets.
a simple question re-mains – why?
one hypothesis is that sample inefﬁ-ciency stems from the data itself: there is only a 2%gain in validation accuracy when training on halfversus the whole dataset.
working from this, wecharacterize the underlying datasets using datasetmaps (swayamdipta et al., 2020) and discover thatactive learning methods prefer sampling “hard-to-learn” examples, leading to poor performance..mapping vqa datasets.
a dataset map(swayamdipta et al., 2020) is a model-speciﬁcgraph for proﬁling the learnability of individualtraining examples.
dataset maps present holisticpictures of classiﬁcation datasets relative to thetraining dynamics of a given model; as a modeltrains for multiple epochs and sees the same exam-ples repeatedly, the mapping process logs statisticsabout the conﬁdence assigned to individual predic-tions.
maps then visualize these statistics againsttwo axes: the y-axis plots the average model con-ﬁdence assigned to the correct answer over train-ing epochs, while the x-axis plots the spread, orvariability, of these values.
this introduces a 2drepresentation of a dataset (viewed through its re-lationship with individual model) where examplesare placed on the map by coarse statistics describ-ing their “learnability“.
we show the dataset mapfor butd trained on vqa-2 in figure 1. for ourwork, we build this map post-hoc, training on the.
entire pool as a means for analyzing what activelearning is doing – treating it as a diagnostic toolfor identifying the root cause why active learningseems to fail for vqa..in an ideal setting, the majority of examples inthe training set should lie in the upper half of thegraph – i.e., the mean conﬁdence assigned to thecorrect answer should be relatively high.
examplestowards the upper-left side represent the “easy-to-learn” examples, as the variability in the conﬁdenceassigned by the model over time is fairly low..a curious feature of vqa-2 and other vqadatasets is the presence of the 25-30% of exam-ples in the bottom-left of the map (shown in redin figure 1) – examples that have low conﬁdenceand variability.
in other words, models are unableto learn a large proportion of training examples.
while prior work attributes examples in this quad-rant to “labeling errors” (swayamdipta et al., 2020),labeling errors in vqa are sparse, and cannot ac-count for the density of such examples in thesemaps..interpreting acquisitions.
we proﬁle the acqui-sitions made by each active learning method, con-textualizing the acquired examples via their place-ment on the associated dataset map.
we segre-gate training examples into four buckets using themap’s y-axis: easy (≥ 0.75), medium (≥ 0.50),hard (≥ 0.25), and impossible (≥ 0.00).
ideally,active learning should be robust to “hard-to-learn”examples, focusing instead on learnable, high un-certainty examples towards the upper-right portionof the dataset map.
instead, we ﬁnd that activelearning methods acquire a large proportion of im-possible examples early on and concentrate on theeasier examples only after the impossible examplesdwindle (see figure 6).
in contrast, the randombaseline acquires examples proportional to eachbucket’s density in the underlying map; acquiringeasier examples earlier and performing on par withor better than all others..7271underspeciﬁcation:what is on the shelf?multi-hop reasoning:what is the vehicle that is driving down the road the box is on the side of?gqavqa-2external knowledge:what does the symbol on the blanket mean?ocr:what is the ﬁrst word on the black car?
(a) 10% of dataset removed.
(b) 25% of dataset removed.
(c) 50% of dataset removed.
figure 8: using dataset maps, we remove hard-to-learn examples, which we identify as collective outliers.
withthe outliers removed, active learning methods demonstrate up to 2–3x sample efﬁciency versus random sampling..6 collective outliers.
this leaves two questions: 1) can we characterizethese “hard” examples, and 2) are these examplesresponsible for the ineffectiveness of active learn-ing on vqa?
we ﬁrst identify hard-to-learn exam-ples as collective outliers and explain why activelearning methods prefer to acquire them.
next,we perform ablation experiments, removing theseoutliers from the active learning pool iteratively,and demonstrate a corresponding boost in sampleefﬁciency relative to random acquisition..hard examples are collective outliers.
col-lective outliers are groups of examples that deviatefrom the rest of the examples but cluster together(han and kamber, 2000) – they often present asfundamental subproblems of a broader task.
forinstance (figure 7), in vqa-2, we identify clustersof hard-to-learn examples that require optical char-acter recognition (ocr) for reasoning about text(e.g., “what is the ﬁrst word on the black car?”); an-other cluster requires external knowledge to answer(“what is the symbol on the hood often associatedwith?”).
in gqa, we identify different clustersof collective outliers; one cluster stems from in-nate underspeciﬁcation (e.g., “what is on the shelf?”with multiple objects present on the shelf); anothercluster requires multiple reasoning hops difﬁcultfor current models (e.g., “what is the vehicle that isdriving down the road the box is on the side of?”).
we sample 100 random “hard-to-learn” exam-ples from both vqa-2 and gqa and ﬁnd that100% of the examples belong to one of the twoaforementioned collectives.
since hard-to-learn ex-amples constitute 25–30% of the data pool, activelearning methods cannot avoid them.
uncertainty-.
based methods (e.g., least-conﬁdence, entropy,monte-carlo dropout) identify them as valid ac-quisition targets because models lack the capacityto correctly answer these examples, assigning lowconﬁdence and high uncertainty.
disagreement-based methods (e.g., bald) are similar; modelconﬁdence is generally low but high variance(lower middle/lower right of the dataset maps).
finally, diversity methods (e.g., core-set selection)identify these examples as different enough fromthe existing pool to warrant acquisition, but failto learn meaningful representations, fueling a vi-cious cycle wherein they continue to pick theseexamples..ablating outliers.
to verify that collective out-liers are responsible for the degradation of activelearning performance, we re-run our experimentsusing active learning pools with varying numbersof outliers removed.
to remove these outliers, wesort and remove all examples in the data pool usingthe product of their model conﬁdence and predic-tion variability (x and y-axis values of the datasetmaps).
we systematically remove examples with alow product value and observe how active learningperformance changes (see figure 8)..we observe a 2–3x improvement in sample efﬁ-ciency when removing 50% of the entire data pool,consisting mainly of collective outliers (figure 8c).
this improvement decreases if we only remove25% of the full pool (figure 8b), and further de-grades if we remove only 10% (figure 8a).
thisablation demonstrates that active learning methodsare more sample efﬁcient than the random base-line when collective outliers are absent from theunlabelled pool..727220k40k60k80k100k120knumber of training examples0.4000.4250.4500.4750.5000.5250.5500.5750.600validation accuracybutd - vqa-2random baselineleast-confidencebaldceiling performance (400k)20k40k60k80k100k120knumber of training examples0.4000.4250.4500.4750.5000.5250.5500.5750.600validation accuracybutd - vqa-220k40k60k80k100k120knumber of training examples0.4000.4250.4500.4750.5000.5250.5500.5750.600validation accuracybutd - vqa-27 discussion and future work.
this paper asks a simple question – why does themodern neural active learning toolkit fail when ap-plied to complex, open ended tasks?
while wefocus on vqa, collective outliers are abundantin tasks such as natural language inference (bow-man et al., 2015; williams et al., 2018) and open-domain question answering (kwiatkowski et al.,2019), amongst others.
more insidious is their na-ture; collective outliers can take multiple forms,requiring external domain knowledge or “common-sense” reasoning, containing underspeciﬁcation, orrequiring capabilities beyond the scope of a givenmodel (e.g., requiring ocr ability).
while we per-form ablations in this work removing collectiveoutliers, demonstrating that active learning failsas collective outliers take up larger portions ofthe dataset, this is only an analytical tool; theseoutliers are, and will continue to be, pervasive inopen-ended datasets – and as such, we will need todevelop better tools for learning (and performingactive learning) in their presence..selective classiﬁcation.
one potential directionfor future work is to develop systems that abstainwhen they encounter collective outliers.
historicalartiﬁcial intelligence systems, such as shrdlu(winograd, 1972) and qualm (lehnert, 1977),were designed to ﬂag input sequences that theywere not designed to parse.
ideas from those meth-ods can and should be resurrected using moderntechniques; for example, recent work suggests thata simple classiﬁer can be trained to identify out-of-domain data inputs, provided a seed out-of-domaindataset (kamath et al., 2020).
active learningmethods can be augmented with a similar classi-ﬁer, which re-calibrates active learning uncertaintyscores with this classiﬁer’s predictions.
other worklearns to identify novel utterances by learning tointelligently set thresholds in representation space(karamcheti et al., 2020), a powerful idea espe-cially if combined with other representation-centricactive learning methods like core-set sampling(sener and savarese, 2018)..of this idea would be in training a discriminator todifferentiate between “learnable” examples (upperhalf of each dataset map) from the “unlearnable”,collective outliers with low conﬁdence and lowvariability.
between each active learning acquisi-tion iteration, one can generate an updated datasetmap, thereby reﬂecting what models are learningas they obtain new labeled examples..machine learning systems deployed in real-world settings will inevitably encounter open-worlddatasets, ones that contain a mixture of learnableand unlearnable inputs.
our work provides a frame-work to study when models encounter such inputs.
overall, we hope that our experiments serve as acatalyst for future work on evaluating active learn-ing methods with inputs drawn from open-worlddatasets..reproducibility.
all code for data preprocessing, model implemen-tation, and active learning algorithms is made avail-able at https://github.com/siddk/vqa-outliers.
ad-ditionally, this repository also contains the full setof results and dataset maps as well..the authors are fully committed to maintaining thisrepository, in terms of both functionality and easeof use, and will actively monitor both email andgithub issues should there be problems..acknowledgements.
we thank kaylee burns, eric mitchell, stephenmussman, dorsa sadigh, and our anonymous aclreviewers for their useful feedback on earlier ver-sions of this paper.
we are also grateful to haotan for providing us with the lxmert checkpointtrained without access to vqa datasets, as well asfor general lxmert ﬁne-tuning pointers..siddharth karamcheti is graciously supported bythe open philanthropy project ai fellowship.
christopher d. manning is a cifar fellow..references.
active learning with global reasoning.
an-other direction for future work to explore is to lever-age dataset maps to perform more global, holisticreasoning over datasets, to intelligently identifypromising examples – in a sense, baking part ofthe analysis done in this work directly into the ac-tive learning algorithms.
a possible instantiation.
yotam abramson and yoav freund.
2004. active learn-ing for visual object recognition.
technical report,university of california, san diego..aishwarya agrawal, jiasen lu, stanislaw antol, mar-garet mitchell, c. lawrence zitnick, devi parikh,and dhruv batra.
2015. vqa: visual question an-swering.
international journal of computer vision,123:4–31..7273peter anderson, x. he, c. buehler, damien teney,mark johnson, stephen gould, and lei zhang.
2018a.
bottom-up and top-down attention for imagecaptioning and visual question answering.
in com-puter vision and pattern recognition (cvpr), pages6077–6086..peter anderson, qi wu, damien teney, jake bruce,mark johnson, niko s¨underhauf, ian reid, stephengould, and anton van den hengel.
2018b.
vision-interpreting visually-and-language navigation:grounded navigation instructions in real environ-ments.
in computer vision and pattern recognition(cvpr)..jeffrey p bigham, chandrika jayant, hanjie ji, greglittle, andrew miller, robert c miller, robinmiller, aubrey tatarowicz, brandyn white, samualwhite, and tom yeh.
2010. vizwiz: nearly real-time answers to visual questions.
in user interfacesoftware and technology (uist), pages 333–342..tolga bolukbasi, kai-wei chang, james y zou,venkatesh saligrama, and adam t kalai.
2016.man is to computer programmer as woman is toin ad-homemaker?
debiasing word embeddings.
vances in neural information processing systems(neurips), pages 4349–4357..samuel bowman, gabor angeli, christopher potts, andchristopher d. manning.
2015. a large annotatedcorpus for learning natural language inference.
inempirical methods in natural language processing(emnlp)..ronan le bras, swabha swayamdipta, chandra bhaga-vatula, rowan zellers, matthew peters, ashish sab-harwal, and yejin choi.
2020. adversarial ﬁlters ofdataset biases.
in international conference on ma-chine learning (icml), pages 1078–1088..haw-shiuan chang, erik learned-miller, and andrewmccallum.
2017. active bias: training more accu-rate neural networks by emphasizing high variancein advances in neural information pro-samples.
cessing systems (neurips), pages 1002–1012..yen-chun chen, linjie li, licheng yu, ahmed elkholy, faisal ahmed, zhe gan, yu cheng, andjingjing liu.
2020. uniter: universal image-textrepresentation learning.
in european conference oncomputer vision (eccv), pages 104–120..cody coleman, christopher yeh, stephen mussmann,baharan mirzasoleiman, peter bailis, percy liang,jure leskovec, and matei zaharia.
2020. selectionvia proxy: efﬁcient data selection for deep learning.
in international conference on learning represen-tations (iclr)..aron culotta and andrew mccallum.
2005. reduc-ing labeling effort for structured prediction tasks.
inassociation for the advancement of artiﬁcial intelli-gence (aaai), pages 746–751..yue deng, kawai chen, yilin shen, and hongxia jin.
2018. adversarial active learning for sequences la-beling and generation.
in international joint confer-ence on artiﬁcial intelligence (ijcai), pages 4012–4018..jacob devlin, ming-wei chang, kenton lee, andkristina toutanova.
2019. bert: pre-training ofdeep bidirectional transformers for language under-standing.
in association for computational linguis-tics (acl), pages 4171–4186..li dong, chris quirk, and mirella lapata.
2018. con-ﬁdence modeling for neural semantic parsing.
in as-sociation for computational linguistics (acl)..aidan finn and nicolas kushmerick.
2003. activelearning selection strategies for information extrac-tion.
in proceedings of the international workshopon adaptive text extraction and mining (atem-03),pages 18–25..akira fukui, dong huk park, daylen yang, annarohrbach, trevor darrell, and marcus rohrbach.
2016. multimodal compact bilinear pooling for vi-sual question answering and visual grounding.
inempirical methods in natural language processing(emnlp)..yarin gal and zoubin ghahramani.
2016. dropout as abayesian approximation: representing model uncer-tainty in deep learning.
in international conferenceon machine learning (icml)..yarin gal, r. islam, and zoubin ghahramani.
2017.deep bayesian active learning with image data.
in international conference on machine learning(icml)..daniel gordon, aniruddha kembhavi, mohammadrastegari, joseph redmon, dieter fox, and alifarhadi.
2018. iqa: visual question answering inin computer vision andinteractive environments.
pattern recognition (cvpr)..yash goyal, tejas khot, douglas summers-stay,dhruv batra, and devi parikh.
2017. making thev in vqa matter: elevating the role of image under-standing in visual question answering.
in computervision and pattern recognition (cvpr)..suchin gururangan, swabha swayamdipta, omerlevy, roy schwartz, samuel bowman, and noah asmith.
2018. annotation artifacts in natural lan-guage inference data.
in association for computa-tional linguistics (acl), pages 107–112..brendan collins, jia deng, kai li, and li fei-fei.
2008. towards scalable dataset construction: an ac-tive learning approach.
in european conference oncomputer vision (eccv), pages 86–98..ben hachey, beatrice alex, and markus becker.
2005.investigating the effects of selective sampling onthe annotation task.
in computational natural lan-guage learning (conll), pages 144–151..7274jiawei han and micheline kamber.
2000. data min-ing: concepts and techniques.
morgan kaufmann..kaiming he, xiangyu zhang, shaoqing ren, and jiansun.
2016. deep residual learning for image recog-nition.
in computer vision and pattern recognition(cvpr)..ranjay krishna, yuke zhu, oliver groth, justin john-son, kenji hata, joshua kravitz, stephanie chen,yannis kalantidi, li-jia li, david a. shamma,michael s. bernstein, and fei-fei li.
2017. vi-sual genome: connecting language and vision us-ing crowdsourced dense image annotations.
interna-tional journal of computer vision, 123:32–73..steven ch hoi, rong jin, jianke zhu, and michael rlyu.
2006. batch mode active learning and its ap-in pro-plication to medical image classiﬁcation.
ceedings of the 23rd international conference on ma-chine learning, pages 417–424..neil houlsby, ferenc husz´ar, zoubin ghahramani,and m´at´e lengyel.
2011. bayesian active learn-ing for classiﬁcation and preference learning.
arxivpreprint arxiv:1112.5745..drew a. hudson and christopher d. manning.
2019.gqa: a new dataset for real-world visual reason-ing and compositional question answering.
in com-puter vision and pattern recognition (cvpr)..khaled jedoui, ranjay krishna, michael bernstein,and li fei-fei.
2019. deep bayesian active learn-arxiv preprinting for multiple correct outputs.
arxiv:1912.01119..justin johnson, bharath hariharan, laurens van dermaaten, li fei-fei, c lawrence zitnick, and rossgirshick.
2017. clevr: a diagnostic dataset for com-positional language and elementary visual reason-in computer vision and pattern recognitioning.
(cvpr)..rosie jones, rayid ghani, tom mitchell, and ellenriloff.
2003. active learning for information extrac-tion with multiple view feature sets.
in internationalconference on knowledge discovery and data min-ing (kdd), pages 26–34..ajay j.joshi, fatih porikli,.
and nikolaos pa-panikolopoulos.
2009. multi-class active learningin computer vision andfor image classiﬁcation.
pattern recognition (cvpr), pages 2372–2379..yuval krymolowski.
2002. distinguishing easy andhard instances.
in international conference on com-putational linguistics (coling)..tom kwiatkowski, jennimaria palomaki, olivia red-ﬁeld, michael collins, ankur parikh, chris alberti,danielle epstein, illia polosukhin, matthew kelcey,jacob devlin, kenton lee, kristina n. toutanova,llion jones, ming-wei chang, andrew dai, jakobuszkoreit, quoc le, and slav petrov.
2019. natu-ral questions: a benchmark for question answeringresearch.
in association for computational linguis-tics (acl)..wendy lehnert.
1977. the process of question an-.
swering.
ph.d. thesis, yale university..david d lewis and jason catlett.
1994. heteroge-neous uncertainty sampling for supervised learning.
in international conference on machine learning(icml), pages 148–156..david d lewis and william a gale.
1994. a sequen-in acmtial algorithm for training text classiﬁers.
special interest group on information retreival (si-gir)..yi li and nuno vasconcelos.
2019. repair: removingrepresentation bias by dataset resampling.
in com-puter vision and pattern recognition (cvpr), pages9572–9581..tsung-yi lin, michael maire, serge belongie, jameshays, pietro perona, deva ramanan, piotr doll´ar,and c. lawrence zitnick.
2014. microsoft coco:in european confer-common objects in context.
ence on computer vision (eccv), pages 740–755..amita kamath, robin jia, and percy liang.
2020. se-inlective question answering under domain shift.
association for computational linguistics (acl)..xiao lin and devi parikh.
2017. active learning for vi-sual question answering: an empirical study.
arxivpreprint arxiv:1711.01732..siddharth karamcheti, dorsa sadigh, and percy liang.
2020.learning adaptive language interfacesin emnlp workshop forthrough decomposition.
interactive and executable semantic parsing (intex-sempar)..alex kendall and yarin gal.
2017. what uncertaintiesdo we need in bayesian deep learning for computervision?
in advances in neural information process-ing systems (neurips), pages 5574–5584..aditya khosla, tinghui zhou, tomasz malisiewicz,alexei a efros, and antonio torralba.
2012. undo-ing the damage of dataset bias.
in european confer-ence on computer vision (eccv), pages 158–171..david lowell, zachary c. lipton, and byron c. wal-lace.
2019. practical obstacles to deploying activein empirical methods in natural lan-learning.
guage processing (emnlp)..jiasen lu, jianwei yang, dhruv batra, and devi parikh.
2016. hierarchical question-image co-attention forin advances in neuralvisual question answering.
information processing systems (neurips)..mateusz malinowski, marcus rohrbach, and mariofritz.
2015. ask your neurons: a neural-based ap-proach to answering questions about images.
in in-ternational conference on computer vision (iccv),pages 1–9..7275hongyuan mei, mohit bansal, and matthew r walter.
2016. listen, attend, and walk: neural mappingof navigational instructions to action sequences.
inassociation for the advancement of artiﬁcial intelli-gence (aaai)..alane suhr, stephanie zhou, ally zhang, iris zhang,huajun bai, and yoav artzi.
2019. a corpus forreasoning about natural language grounded in pho-tographs.
in association for computational linguis-tics (acl)..ishan misra, ross girshick, rob fergus, martialhebert, abhinav gupta, and laurens van dermaaten.
2018. learning by asking questions.
incomputer vision and pattern recognition (cvpr),pages 11–20..stephen mussmann and percy liang.
2018. on the re-lationship between data efﬁciency and error in activein international conference on machinelearning.
learning (icml)..junwon park, ranjay krishna, pranav khadpe, li fei-fei, and michael bernstein.
2019. ai-based requestaugmentation to increase crowdsourcing participa-tion.
in association for the advancement of artiﬁ-cial intelligence (aaai), volume 7, pages 115–124..jeffrey pennington, richard socher, and christopher dmanning.
2014. glove: global vectors for wordin empirical methods in naturalrepresentation.
language processing (emnlp), pages 1532–1543..mengye ren, ryan kiros, and richard zemel.
2015a.
exploring models and data for image question an-in advances in neural information pro-swering.
cessing systems (neurips), pages 2953–2961..shaoqing ren, kaiming he, ross b. girshick, and jiansun.
2015b.
faster r-cnn: towards real-time ob-ject detection with region proposal networks.
ieeetransactions on pattern analysis and machine intel-ligence (pami), 39:1137–1149..tobias scheffer, christian decomain, and stefan wro-bel.
2001. active hidden markov models for infor-in international symposium onmation extraction.
intelligent data analysis, pages 309–318..swabha swayamdipta, roy schwartz, nicholas lourie,yizhong wang, hannaneh hajishirzi, noah a.smith, and yejin choi.
2020. dataset cartography:mapping and diagnosing datasets with training dy-namics.
in empirical methods in natural languageprocessing (emnlp)..hao hao tan and mohit bansal.
2019. lxmert:representationsin empirical methods in natu-.
learning cross-modality encoderfrom transformers.
ral language processing (emnlp)..stefanie tellex, thomas kollar, steven dickerson,matthew r walter, ashis gopal banerjee, seth jteller, and nicholas roy.
2011. understanding nat-ural language commands for robotic navigation andin association for the ad-mobile manipulation.
vancement of artiﬁcial intelligence (aaai)..damien teney, peter anderson, xiaodong he, and an-ton v. d. hengel.
2018. tips and tricks for visualquestion answering: learnings from the 2017 chal-lenge.
in computer vision and pattern recognition(cvpr), pages 4223–4232..mariya toneva, alessandro sordoni, remi tachet descombes, adam trischler, yoshua bengio, and geof-frey j gordon.
2019. an empirical study of exam-ple forgetting during deep neural network learning.
in international conference on learning represen-tations (iclr)..simon tong and daphne koller.
2001. support vec-tor machine active learning with applications to textclassiﬁcation.
journal of machine learning research,2(0):45–66..a. schein and lyle h. ungar.
2007. active learning forlogistic regression: an evaluation.
machine learn-ing, 68:235–265..antonio torralba and alexei a efros.
2011. unbiasedlook at dataset bias.
in computer vision and patternrecognition (cvpr), pages 1521–1528..ozan sener and silvio savarese.
2018. active learn-ing for convolutional neural networks: a core-setapproach.
in international conference on learningrepresentations (iclr)..burr settles.
2009. active learning literature survey.
technical report, university of wisconsin, madison..yanyao shen, hyokun yun, zachary c lipton, yakovkronrod,and animashree anandkumar.
2017.deep active learning for named entity recognition.
in proceedings of the second workshop on repre-sentation learning for nlp (repl4nlp)..aditya siddhant and zachary c lipton.
2018. deepbayesian active learning for natural language pro-cessing: results of a large-scale empirical study.
inempirical methods in natural language processing(emnlp)..adina williams, nikita nangia, and samuel bowman.
2018. a broad-coverage challenge corpus for sen-in associ-tence understanding through inference.
ation for computational linguistics (acl), pages1112–1122..terry winograd.
1972. understanding natural lan-.
guage.
academic press..thomas wolf, lysandre debut, victor sanh, julienchaumond, clement delangue, anthony moi, pier-ric cistac, tim rault, r’emi louf, morgan funtow-icz, and jamie brew.
2019. huggingface’s trans-formers: state-of-the-art natural language process-ing.
arxiv preprint arxiv:1910.03771..qi wu, peng wang, chunhua shen, anthony dick,and anton van den hengel.
2016. ask me any-thing: free-form visual question answering based on.
7276knowledge from external sources.
in computer vi-sion and pattern recognition (cvpr), pages 4622–4630..yazhou yang and marco loog.
2018. a benchmarkand comparison of active learning for logistic regres-sion.
pattern recognition, 83..zichao yang, xiaodong he, jianfeng gao, li deng,and alex smola.
2016. stacked attention networksfor image question answering.
in computer visionand pattern recognition (cvpr)..bolei zhou, yuandong tian, sainbayar sukhbaatar,arthur szlam, and rob fergus.
2015. simple base-line for visual question answering.
arxiv preprintarxiv:1512.02167..yuke zhu, oliver groth, michael bernstein, and li fei-fei.
2016. visual7w: grounded question answeringin images.
in computer vision and pattern recogni-tion (cvpr), pages 4995–5004..yuke zhu, roozbeh mottaghi, eric kolve, joseph jlim, abhinav gupta, li fei-fei, and ali farhadi.
2017. target-driven visual navigation in indoorin in-scenes using deep reinforcement learning.
ternational conference on robotics and automation(icra), pages 3357–3364..7277a overview.
due to the broad scope of our experiments andanalysis, we were unable to ﬁt all our results inthe main body of the paper.
furthermore, giventhe limited length provided by the appendix, weprovide only salient implementation details andother representative results here; however, wemake all code, models, data, results, active learn-ing implementations available at this link: https://github.com/siddk/vqa-outliers..generally, any combination of {active learn-ing strategy × model × seed set size × analy-sis/acquisition plot} is present in this paper, and isavailable in the public code repository..b implementation details.
b.1 models & training.
where applicable, we implement our models basedon publicly available pytorch implementations.
for the lstm-cnn model, we base our implemen-tation off of this repository: https://github.com/shivanshu-gupta/visual-question-answering, whilefor the bottom-up top-down attention model,we use thishttps://github.com/hengyuan-hu/bottom-up-attention-vqa, keeping de-fault hyperparameters the same..repository:.
logistic regression.
when implementing lo-gistic regression, we base our pytorch imple-mentation on the broadly used scikit-learn (https://scikit-learn.org) implementation, using the de-fault parameters (including l2 weight decay).
weoptimize our models via stochastic gradient de-scent..lxmert.
as mentioned in section 3, the de-fault lxmert checkpoint and ﬁne-tuning codemade publicly available in tan and bansal (2019)(associated code repository: https://github.com/airsplay/lxmert) is pretrained on data from vqa-2and gqa, leaking information that could substan-tially affect our active learning results.
to mitigatethis, we contacted the authors, who kindly providedus with a checkpoint of the model without vqapretraining..however, in addition to this model obtaining dif-ferent results from those reported in the originalwork, the provided pretrained checkpoint behavesslightly differently during ﬁne-tuning, requiringdifferent hyperparameters than provided in the orig-inal repository.
we perform a coarse grid search.
over hyperparameters, using the lxmert imple-mentation provided by huggingface transformers(wolf et al., 2019), and ﬁnd that using an adamwoptimizer rather than the bert-adam optimizerused in the original work without any special learn-ing rate scheduling results in the best ﬁne-tuningperformance..b.2 acquisition functions.
we use standard implementations of the 8 activelearning strategies described, borrowing from priorimplementations (mussmann and liang, 2018)and existing code repositories (https://github.com/google/active-learning).
we provide additional de-tails below..monte-carlo dropout.
for our implementa-tions of the deep bayesian active learning meth-ods (monte-carlo dropout w/ entropy, bald), wefollow gal and ghahramani (2016) and estimate adropout distribution via test-time dropout, runningmultiple forward passes through our neural net-works, with different, randomly sampled dropoutmasks.
we use a value of k = 10 forward passesto form our dropout distribution..amortized core-set selection.
in the originalcore-set selection active learning work introducedby sener and savarese (2018), it is shown that core-set selection for active learning can be reduced toa version of the k-centers problem, which can besolved approximately (2-opt) with a greedy algo-rithm.
however, running this algorithm on high-dimensional representations, across large pools canbe prohibitive; core-set selection is batch-aware,requiring recomputing distances from each “cluster-center” (points in the set of acquired examples) toall points in the active learning pool after each ac-quisition in a batch.
while we can run this outcompletely for smaller datasets (and indeed, thisis what we do for our small datasets vqa-sportsand vqa-food), a single acquisition iteration fora large dataset for the full vqa-2 dataset takesapproximately 20 gpu-hours on the resources wehave available, or up to 9 days for a single core-setselection run.
for gqa, performing exact core-setselection takes at least twice as long..to still capture the spirit of core-set diversity-based selection in our evaluation, we instead in-troduce an amortized implementation of core-setselection, which is comprised of two steps.
we ﬁrstdownsample the high-dimensional representations.
7278r-cnn representation enables much higher perfor-mance than the resnet-101 representation, activelearning results are consistent with those reportedin the paper..c.4 other acquisition strategies.
figure 12 presents results for the four other activelearning strategies we implement – entropy, montecarlo dropout w/ entropy, core-set (language),and core-set (vision) – for the butd model.
re-sults are across vqa-sports (seed set = 10%), andvqa-2 (seed set = 10%, 50%) – despite the uniquefeatures of each strategy, the trends remain consis-tent with those in the paper..d dataset maps & acquisitions.
to provide further context around active learn-ing acquisitions across datasets, figures 13–16present dataset maps and acquisitions for thebutd model across vqa-sports, vqa-food, andgqa respectively.
interesting to note is that whilevqa-sports and vqa-food are generally easier,with fewer “hard-to-learn” examples, active learn-ing still has a bias for picking those examples.
forgqa, our earlier analysis is conﬁrmed; active learn-ing is picking the collective outliers populating thebottom half of the dataset map..(of either the fused language and text, or either uni-modal representations) via principal componentanalysis (pca) to make the distance computationfaster by an order of magnitude.
then, rather thanupdating distances from examples in our acquiredset to points in our pool after each acquisition ˆx, wedelay updates, instead only refreshing the distancecomputation every 2000 acquisitions (roughly 5%of an acquisition batch for vqa-2).
this allows usto report results for core-set selection with thethree different proposed representations (fused,language-only, vision-only) for vqa-2; unfortu-nately, for gqa and lxmert (due to the high costof training), even running this amortized versionof core-set selection is prohibitive, so we report asubset of results, and omit the rest..c active learning results.
we include further results from our study of ac-tive learning applied to vqa, including results onvqa-food (not included in the main body), activelearning results for the two logistic regression mod-els – log-reg (resnet-101) and log-reg (fasterr-cnn), as well as with the 4 acquisition strategiesnot included in the main body of the paper – en-tropy, monte-carlo dropout w/ entropy, core-set(language), and core-set (vision)..c.1 vqa-food.
figure 9 shows results on vqa-food with thelstm-cnn, butd, and lxmert models, witha seed set comprised of 10% of the total pool.
theresults are mostly similar to those reported in thepaper; strategies track or underperform randomsampling, with the exception of least-conﬁdencefor the lstm-cnn model – however, this is thesole exception, and the lstm-cnn has the highesttraining variance of all the models we try..c.2 logistic regression (resnet-101).
figure 10 shows active learning results for the lo-greg (resnet-101) model on vqa-sports (seedset = 10%), and vqa-2 (seed set = 10%, 50%).
results are similar to those reported in the paper,with active learning failing to outperform randomacqusition..c.3 logistic regression (faster r-cnn).
figure 11 presents the same set of experiments asthe prior section, except with the logreg (fasterr-cnn) model.
while the object-based faster.
7279figure 9: results for the representative active learning methods on vqa-food, a simpliﬁed vqa dataset similarto vqa-food, across lstm-cnn, butd, and lxmert..figure 10: active learning results using the logistic regression (resnet-101) model on vqa-sports (10% seedset), and vqa-2 (10% and 50% seed set).
most strategies either track or underperform random acquisition..figure 11: active learning results using the logistic regression (faster r-cnn) model on vqa-sports (10%seed set), and vqa-2 (10% and 50% seed set).
while the faster r-cnn representation leads to better validationaccuracies, active learning performance remains consistent..figure 12: results with the butd on vqa-sports, vqa-2 and gqa using the alternative 4 acquisition strategiesnot included in the main body of the paper.
unsurprisingly, results are consistent with those reported in the paper..72804008001.2k1.6k2k2.4k2.8k3.2k3.6k4knumber of training examples0.20.30.40.50.60.70.80.91.0validation accuracylstm-cnn - vqa-foodrandom baselineleast-confidencebaldcore-set (fused)4008001.2k1.6k2k2.4k2.8k3.2k3.6k4knumber of training examples0.20.30.40.50.60.70.80.91.0validation accuracybutd - vqa-foodrandom baselineleast-confidencebaldcore-set (fused)4008001.2k1.6k2k2.4k2.8k3.2k3.6k4knumber of training examples0.20.30.40.50.60.70.80.91.0validation accuracylxmert - vqa-foodrandom baselineleast-confidencebald5001k1.5k2k2.5k3k3.5k4k4.5k5knumber of training examples0.20.30.40.50.60.70.80.91.0validation accuracylogreg (resnet-101) - vqa-sportsrandom baselineleast-confidencecore-set (fused)40k80k120k160k200k240k280k320k360k400knumber of training examples0.20.30.40.50.60.70.80.91.0validation accuracylogreg (resnet-101) - vqa-2random baselineleast-confidencecore-set (fused)200k240k280k320k360k400knumber of training examples0.20.30.40.50.60.70.80.91.0validation accuracylogreg (resnet-101) - vqa-2random baselineleast-confidencecore-set (fused)5001k1.5k2k2.5k3k3.5k4k4.5k5knumber of training examples0.20.30.40.50.60.70.80.91.0validation accuracylogreg (faster r-cnn) - vqa-sportsrandom baselineleast-confidencecore-set (fused)40k80k120k160k200k240k280k320k360k400knumber of training examples0.20.30.40.50.60.70.80.91.0validation accuracylogreg (faster r-cnn) - vqa-2random baselineleast-confidencecore-set (fused)200k240k280k320k360k400knumber of training examples0.20.30.40.50.60.70.80.91.0validation accuracylogreg (faster r-cnn) - vqa-2random baselineleast-confidencecore-set (fused)5001k1.5k2k2.5k3k3.5k4k4.5k5knumber of training examples0.20.30.40.50.60.70.80.91.0validation accuracybutd - vqa-sportsrandom baselineentropymc-dropout (entropy)core-set (language)core-set (vision)40k80k120k160k200k240k280k320k360k400knumber of training examples0.20.30.40.50.60.70.80.91.0validation accuracybutd - vqa-2random baselineentropymc-dropout (entropy)core-set (language)core-set (vision)200k240k280k320k360k400knumber of training examples0.20.30.40.50.60.70.80.91.0validation accuracybutd - vqa-2random baselineentropymc-dropout (entropy)core-set (language)core-set (vision)figure 13: dataset maps for the bottom-up top-down attention model on vqa-sports, vqa-food, and gqarespectively.
note that vqa-sports and vqa-food have fewer “hard-to-learn” examples..figure 14: acquisitions with the butd model on vqa-sports.
the dataset has fewer “hard-to-learn” examples,but active learning strategies pick the medium–hard examples, which still negatively impact performance..figure 15: acquisitions with the butd model on vqa-food.
despite the sparsity of hard examples, activelearning strategies still tend towards them.
bald is high-variance, selecting examples all over the map..figure 16: acquisitions with the butd model on the full gqa dataset.
given that the map for gqa is similarto the map for vqa-2, it is not surprising that the active learning acquisitions follow a similar trend, preferring toselect “hard-to-learn” examples..72810.000.050.100.150.200.250.300.350.400.450.50variability0.00.10.20.30.40.50.60.70.80.91.0confidence0.00.20.40.60.81.0correctness0.000.050.100.150.200.250.300.350.400.450.50variability0.00.10.20.30.40.50.60.70.80.91.0confidence0.00.20.40.60.81.0correctness5001k1.5k2k2.5k3k3.5k4k4.5knumber of training examples0 100 200 300 400 500 acquisitions by difficultyrandom baselineimpossible [p > 0.0]hard [p > 0.25]medium [p > 0.5]easy [p > 0.75]5001k1.5k2k2.5k3k3.5k4k4.5knumber of training examples0 100 200 300 400 500 acquisitions by difficultyleast-confidence5001k1.5k2k2.5k3k3.5k4k4.5knumber of training examples0 100 200 300 400 500 acquisitions by difficultybald4008001.2k1.6k2k2.4k2.8k3.2k3.6knumber of training examples0 50 100 150 200 250 300 350 400 acquisitions by difficultyrandom baselineimpossible [p > 0.0]hard [p > 0.25]medium [p > 0.5]easy [p > 0.75]4008001.2k1.6k2k2.4k2.8k3.2k3.6knumber of training examples0 50 100 150 200 250 300 350 400 acquisitions by difficultyleast-confidence4008001.2k1.6k2k2.4k2.8k3.2k3.6knumber of training examples0 50 100 150 200 250 300 350 400 acquisitions by difficultybald90k180k270k360k450k540k630k720k810knumber of training examples0 20k40k60k80kacquisitions by difficultyrandom baselineimpossible [p > 0.0]hard [p > 0.25]medium [p > 0.5]easy [p > 0.75]90k180k270k360k450k540k630k720k810knumber of training examples0 20k40k60k80kacquisitions by difficultyleast-confidence90k180k270k360k450k540k630k720k810knumber of training examples0 20k40k60k80kacquisitions by difficultybald