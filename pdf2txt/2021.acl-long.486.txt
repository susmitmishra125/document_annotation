prgc: potential relation and global correspondence based jointrelational triple extraction.
hengyi zheng1,2,3, rui wen3, xi chen3,4∗, yifan yang3, yunyan zhang3ziheng zhang3, ningyu zhang5, bin qin2∗, ming xu2∗, yefeng zheng31college of electronics and information engineering, shenzhen university2information technology center, shenzhen university3tencent jarvis lab, shenzhen, china4platform and content group, tencent 5zhejiang universityzhenghengyi2019@email.szu.edu.cn, {qinbin,xuming}@szu.edu.cn{ruiwen,jasonxchen,tobyfyang,yunyanzhang,zihengzhang,yefengzheng}@tencent.com.
abstract.
joint extraction of entities and relations fromunstructured texts is a crucial task in infor-mation extraction.
recent methods achieveconsiderable performance but still suffer fromsome inherent limitations, such as redundancyof relation prediction, poor generalization ofspan-based extraction and inefﬁciency.
in thispaper, we decompose this task into three sub-tasks, relation judgement, entity extractionand subject-object alignment from a novel per-spective and then propose a joint relationaltriple extraction framework based on potentialrelation and global correspondence (prgc).
speciﬁcally, we design a component to pre-dict potential relations, which constrains thefollowing entity extraction to the predicted re-lation subset rather than all relations;thena relation-speciﬁc sequence tagging compo-nent is applied to handle the overlapping prob-lem between subjects and objects; ﬁnally, aglobal correspondence component is designedto align the subject and object into a triplewith low-complexity.
extensive experimentsshow that prgc achieves state-of-the-art per-formance on public benchmarks with higherefﬁciency and delivers consistent performancegain on complex scenarios of overlappingtriples.1.
1.introduction.
identifying entity mentions and their relationswhich are in the form of a triple (subject, rela-tion, object) from unstructured texts is an impor-tant task in information extraction.
some previousworks proposed to address the task with pipelinedapproaches which include two steps: named en-tity recognition (tjong kim sang and de meulder,2003; ratinov and roth, 2009) and relation predic-tion (zelenko et al., 2002; bunescu and mooney,.
*corresponding author.
1thesource.
datahttps://github.com/hy-struggle/prgc..code.
and.
are.
released.
at.
subtask.
relation judgement.
entity extraction.
subject-object alignment.
model.
casreltplinkerprgc.
casreltplinkerprgc.
casreltplinkerprgc.
component.
none (take all relations)none (take all relations)potential relation prediction.
span-basedspan-basedrel-spec sequence tagging.
cascade schemetoken-pair matrixglobal correspondence.
table 1: comparison of the proposed prgc and previ-ous methods in the respect of our new perspective withthree subtasks..2005; pawar et al., 2017; wang et al., 2020b).
re-cent end-to-end methods, which are based on eithermulti-task learning (wei et al., 2020) or single-stage framework (wang et al., 2020a), achievedpromising performance and proved their effective-ness, but lacked in-depth study of the task..to better comprehend the task and advance thestate of the art, we propose a novel perspective todecompose the task into three subtasks: i) rela-tion judgement which aims to identify relationsin a sentence, ii) entity extraction which aims toextract all subjects and objects in the sentence andiii) subject-object alignment which aims to alignthe subject-object pair into a triple.
on the basis,we review two end-to-end methods in table 1. forthe multi-task method named casrel (wei et al.,2020), the relational triple extraction is performedin two stages which applies object extraction toall relations.
obviously, the way to identify rela-tions is redundant which contains numerous invalidoperations, and the span-based extraction schemewhich just pays attention to start/end position of anentity leads to poor generalization.
meanwhile, it isrestricted to process one subject at a time due to itssubject-object alignment mechanism, which is inef-ﬁcient and difﬁcult to deploy.
for the single-stage.
proceedingsofthe59thannualmeetingoftheassociationforcomputationallinguisticsandthe11thinternationaljointconferenceonnaturallanguageprocessing,pages6225–6235august1–6,2021.©2021associationforcomputationallinguistics6225framework named tplinker (wang et al., 2020a),in order to avoid the exposure bias in subject-objectalignment, it exploits a rather complicated decoderwhich leads to sparse label and low convergencerate while the problems of relation redundancy andpoor generalization of span-based extraction arestill unsolved..to address aforementioned issues, we pro-pose an end-to-end framework which consists ofthree components: potential relation prediction,relation-speciﬁc sequence tagging and globalcorrespondence, which fulﬁll the three subtasksaccordingly as shown in table 1..for relation judgement, we predict potentialrelations by the potential relation prediction com-ponent rather than preserve all redundant rela-tions, which reduces computational complexityand achieves better performance, especially whenthere are many relations in the dataset.2 for en-tity extraction, we use a more robust relation-speciﬁc sequence tagging component (rel-specsequence tagging for short) to extract subjectsand objects separately, to naturally handle overlap-ping between subjects and objects.
for subject-object alignment, unlike tplinker which usesa relation-based token-pair matrix, we design arelation-independent global correspondence ma-trix to determine whether a speciﬁc subject-objectpair is valid in a triple..given a sentence, prgc ﬁrst predicts a subsetof potential relations and a global matrix whichcontains the correspondence score between all sub-jects and objects; then performs sequence taggingto extract subjects and objects for each potentialrelation in parallel; ﬁnally enumerates all predictedentity pairs, which are then pruned by the globalcorrespondence matrix.
it is worth to note thatthe experiment (described in section 5.2.1) showsthat the potential relation prediction componentof prgc is overall beneﬁcial, even though it intro-duces the exposure bias that is usually mentionedin prior single-stage methods to prove their advan-tages..experimental results show that prgc outper-forms the state-of-the-art methods on public bench-marks with higher efﬁciency and fewer parameters.
detailed experiments on complex scenarios suchas various overlapping patterns, which contain thesingle entity overlap (seo), entity pair overlap.
(epo) and subject object overlap (soo) types3show that our method owns consistent advantages.
the main contributions of this paper are as follows:.
1. we tackle the relational triple extraction taskfrom a novel perspective which decomposesthe task into three subtasks: relation judge-ment, entity extraction and subject-objectalignment, and previous works are comparedon the basis of the proposed paradigm asshown in table 1..2. following our perspective, we propose a novelend-to-end framework and design three com-ponents with respect to the subtasks whichgreatly alleviate the problems of redundant re-lation judgement, poor generalization of span-based extraction and inefﬁcient subject-objectalignment, respectively..3. we conduct extensive experiments on severalpublic benchmarks, which indicate that ourmethod achieves state-of-the-art performance,especially for complex scenarios of overlap-ping triples.
further ablation studies and anal-yses conﬁrm the effectiveness of each compo-nent in our model..4. in addition to higher accuracy, experimentsshow that our method owns signiﬁcant advan-tages in complexity, number of parameters,ﬂoating point operations (flops) and infer-ence time compared with previous works..2 related work.
traditionally, relational triple extraction has beenstudied as two separated tasks: entity extractionand relation prediction.
early works (zelenko et al.,2002; chan and roth, 2011) apply the pipelinedmethods to perform relation classiﬁcation betweenentity pairs after extracting all the entities.
to estab-lish the correlation between these two tasks, jointmodels have attracted much attention.
prior feature-based joint models (yu and lam, 2010; li and ji,2014; miwa and sasaki, 2014; ren et al., 2017) re-quire a complicated process of feature engineeringand rely on various nlp tools with cumbersomemanual operations..recently, the neural network model which re-duces manual involvement occupies the main partof the research.
zheng et al.
(2017) proposed a.
3more details about overlapping patterns are shown in.
2for example, the webnlg dataset (gardent et al., 2017)has hundreds of relations but only seven valid relations forone sentence mostly..appendix a..6226figure 1: the overall structure of prgc.
given a sentence s, prgc predicts a subset of potential relationsrpot and a global correspondence m which indicates the alignment between subjects and objects.
then foreach potential relation, a relation-speciﬁc sentence representation is constructed for sequence tagging.
finally weenumerate all possible subject-object pairs and get four candidate triples for this particular example, but only twotriples are left (marked red) after applying the constraint of global correspondence..novel tagging scheme that uniﬁed the role of theentity and the relation between entities in the anno-tations, thus the joint extraction task was convertedto a sequence labeling task but it failed to solvethe overlapping problems.
bekoulis et al.
(2018)proposed to ﬁrst extract all candidate entities, thenpredict the relation of every entity pair as a multi-head selection problem, which shared parametersbut did not decode jointly.
nayak and ng (2020)employed an encoder-decoder architecture and apointer network based decoding approach wherean entire triple was generated at each time step..to handle the problems mentioned above, weiet al.
(2020) presented a cascade framework, whichﬁrst identiﬁed all possible subjects in a sentence,then for each subject, applied span-based taggers toidentify the corresponding objects based on each re-lation.
this method leads to redundancy on relationjudgement, and is not robust due to the span-basedscheme on entity extraction.
meanwhile, the align-ment scheme of subjects and objects limits its paral-lelization.
in order to represent the relation of tripleexplicitly, yuan et al.
(2020) presented a relation-speciﬁc attention to assign different weights to thewords in context under each relation, but it ap-plied a naive heuristic nearest neighbor principle tocombine the entity pairs which means the nearestsubject and object entities will be combined intoa triple.
this is obviously not in accordance withintuition and fact.
meanwhile, it is also redundant.
on relation judgement.
the state-of-the-art methodnamed tplinker (wang et al., 2020a) employs a to-ken pair linking scheme which performs two o(n2)matrix operations for extracting entities and align-ing subjects with objects under each relation of asentence, causing extreme redundancy on relationjudgement and complexity on subject-object align-ment, respectively.
and it also suffers from thedisadvantage of span-based extraction scheme..3 method.
in this section, we ﬁrst introduce our perspectiveof relational triple extraction task with a principledproblem deﬁnition, then elaborate each componentof the prgc model.
an overview illustration ofprgc is shown in figure 1..3.1 problem deﬁnition.
the input is a sentence s = {x1, x2, ..., xn} withn tokens.
the desired outputs are relational triplesas t (s) = {(s, r, o)|s, o ∈ e, r ∈ r}, where eand r are the entity and relation sets, respectively.
in this paper, the problem is decomposed into threesubtasks:.
relation judgement for the given sentence s,this subtask predicts potential relations it con-the output of this task is yr(s) =tains.
{r1, r2, ..., rm|ri ∈ r}, where m is the size ofpotential relation subset..6227entity extraction for the given sentence s anda predicted potential relation ri, this subtask iden-tiﬁes the tag of each token with bio (i.e., begin,inside and outside) tag scheme (tjong kim sangand veenstra, 1999; ratinov and roth, 2009).
lettj denote the tag.
the output of this task isye(s, ri|ri ∈ r) = {t1, t2, ..., tn}..we model it as a multi-label binary classiﬁca-tion task, and the corresponding relation will beassigned with tag 1 if the probability exceeds a cer-tain threshold λ1 or with tag 0 otherwise (as shownin figure 1), so next we just need to apply therelation-speciﬁc sequence tagging to the predictedrelations rather than all relations..subject-object alignment for the given sen-tence s, this subtask predicts the correspondencescore between the start tokens of subjects and ob-jects.
that means only the pair of start tokens of atrue triple has a high score, while the other tokenpairs have a low score.
let m denote the globalcorrespondence matrix.
the output of this task isys(s) = m ∈ rn×n..3.2 prgc encoder.
the output of prgc encoder is yenc(s) ={h1, h2, ..., hn|hi ∈ rd×1}, where d is the em-bedding dimension, and n is the number of tokens.
we use a pre-trained bert model4 (devlin et al.,2019) to encode the input sentence for a fair com-parison, but theoretically it can be extended to otherencoders, such as glove (pennington et al., 2014)and roberta (liu et al., 2019)..3.3 prgc decoder.
in this section, we describe the instantiation ofprgc decoder that consists of three components..3.3.1 potential relation prediction.
this component is shown as the orange box infigure 1 where rpot is the potential relations.
dif-ferent from previous works (wei et al., 2020; yuanet al., 2020; wang et al., 2020a) which redundantlyperform entity extraction to every relation, givena sentence, we ﬁrst predict a subset of potentialrelations that possibly exist in the sentence, andthen the entity extraction only needs to be appliedto these potential relations.
given the embeddingh ∈ rn×d of a sentence with n tokens, each ele-ment of this component is obtained as:.
havg = avgpool(h) ∈ rd×1prel = σ(wrhavg + br).
(1).
where avgpool is the average pooling operation(lin et al., 2014), wr ∈ rd×1 is a trainable weightand σ denotes the sigmoid function..3.3.2 relation-speciﬁc sequence tagging.
as shown in figure 1, we obtain several relation-speciﬁc sentence representations of potential rela-tions described in section 3.3.1. then, we performtwo sequence tagging operations to extract subjectsand objects, respectively.
the reason why we ex-tract subjects and objects separately is to handle thespecial overlapping pattern named subject objectoverlap (soo).
we can also simplify it to one se-quence tagging operation with two types of entitiesif there are no soo patterns in the dataset.5.
for the sake of simplicity and fairness, we aban-don the traditional lstm-crf (panchendrarajanand amaresan, 2018) network but adopt the sim-ple fully connected neural network.
detailed op-erations of this component on each token are asfollows:.
psubi,j = sof tmax(wsub(hi + uj) + bsub)pobj.
i,j = sof tmax(wobj(hi + uj) + bobj).
(2).
where uj ∈ rd×1 is the j-th relation representationin a trainable embedding matrix u ∈ rd×nr wherenr is the size of full relation set, hi ∈ rd×1 isthe encoded representation of the i-th token, andwsub, wobj ∈ rd×3 are trainable weights wherethe size of tag set {b, i, o} is 3..3.3.3 global correspondence.
after sequence tagging, we acquire all possiblesubjects and objects with respect to a relation ofthe sentence, then we use a global correspondencematrix to determine the correct pairs of the sub-jects and objects.
it should be noted that the globalcorrespondence matrix can be learned simultane-ously with potential relation prediction since it isindependent of relations.
the detailed process is asfollows: ﬁrst we enumerate all the possible subject-object pairs; then we check the corresponding scorein the global matrix for each pair, retain it if thevalue exceeds a certain threshold λ2 or ﬁlter it outotherwise..4please refer to the original paper (devlin et al., 2019) for.
5for example, the soo pattern is rare in the nyt (riedel.
detailed descriptions..et al., 2010) dataset..6228#sentences.
details of test set.
dataset.
nyt*webnlg*nytwebnlg.
train.
56,1955,01956,1965,019.valid.
4,9995005,000500.test normal.
seo.
epo soo n = 1 n > 1.
#triples.
#relations.
5,0007035,000703.
3,2662453,071239.
1,2974571,273448.
978261,1686.
458411785.
3,2442663,089256.
1,7564371,911447.
8,1101,5918,6161,607.
2417124216.table 2: statistics of datasets used in our experiments where n is the number of triples in a sentence.
note thatone sentence can have seo, epo and soo overlapping patterns simultaneously, and the relation set of webnlgis bigger than webnlg*..as shown in the green matrix m in figure 1,given a sentence with n tokens, the shape of globalcorrespondence matrix will be rn×n.
each ele-ment of this matrix is about the start position of apaired subject and object, which represents the con-ﬁdence level of a subject-object pair, the higher thevalue, the higher the conﬁdence level that the pairbelongs to a triple.
for example, the value about“tom” and “jerry” at row 1, column 3 will be highif they are in a correct triple such as “(tom, like,jerry)”.
the value of each element in the matrix isobtained as follows:.
pisub,jobj = σ(wg[hsub.
i.; hobjj.]
+ bg).
(3).
i., hobj.
j ∈ rd×1 are the encoded represen-where hsubtation of the i-th token and j-th token in the inputsentence forming a potential pair of subject andobject, wg ∈ r2d×1 is a trainable weight, and σis the sigmoid function..3.4 training strategy.
we train the model jointly, optimize the combinedobjective function during training time and sharethe parameters of the prgc encoder.
the total losscan be divided into three parts as follows:.
lrel = −.
(yi log prel + (1 − yi) log (1 − prel)).
1nr.
nr(cid:88).
i=1.
lseq = −.
12 × n × npot.
r.(cid:88).
npotr(cid:88).
n(cid:88).
i,j log ptyti,j.
t∈{sub,obj}.
j=1.
i=1.
lglobal = −.
(yi,j log pisub,jobj.
1n2.
n(cid:88).
n(cid:88).
i=1.
j=1.
+ (1 − yi,j) log (1 − pisub,jobj )).
(4).
(5).
(6).
performance might be better by carefully tuning theweight of each sub-loss, but we just assign equalweights for simplicity (i.e., α = β = γ = 1)..4 experiments.
4.1 datasets and experimental settings.
for fair and comprehensive comparison, we fol-low yu et al.
(2019) and wang et al.
(2020a) toevaluate our model on two public datasets nyt(riedel et al., 2010) and webnlg (gardent et al.,2017), both of which have two versions, respec-tively.
we denote the different versions as nyt*,nyt and webnlg*, webnlg.
note that nyt*and webnlg* annotate the last word of entities,while nyt and webnlg annotate the whole entityspan.
the statistics of the datasets are describedin table 2. following wei et al.
(2020), we fur-ther characterize the test set w.r.t.
the overlappingpatterns and the number of triples per sentence..following prior works mentioned above, an ex-tracted relational triple is regarded as correct onlyif it is an exact match with ground truth, whichmeans the last word of entities or the whole en-tity span (depending on the annotation protocol)of both subject and object and the relation are allcorrect.
meanwhile, we report the standard microprecision (prec.
), recall (rec.)
and f1-score forall the baselines.
the implementation details areshown in appendix b..we compare prgc with eight strong baselinemodels and the state-of-the-art models casrel (weiet al., 2020) and tplinker (wang et al., 2020a).
all the experimental results of the baseline modelsare directly taken from wang et al.
(2020a) unlessspeciﬁed..where nr is the size of full relation set and npotisthe size of potential relation subset of the sentence.
the total loss is the sum of these three parts,.
r.ltotal = αlrel + βlseq + γlglobal..(7).
4.2 experimental results.
in this section, we present the overall results andthe results of complex scenarios, while the resultson different subtasks corresponding to different.
622919.3-54.1--82.0-83.8--84.5.
79.287.2.
77.975.694.296.195.5.
57.289.592.493.394.8.
28.3-55.7--83.1-82.1--86.7.
80.888.5.
45.976.983.790.093.0.
55.791.190.991.692.8.model.
nyt*.
webnlg*.
nyt.
webnlg.
prec.
rec..f1.
prec.
rec..f1.
prec.
rec..f1.
prec.
rec..f1.
noveltagging (zheng et al., 2017)copyre (zeng et al., 2018)multihead (bekoulis et al., 2018)graphrel (fu et al., 2019)ordercopyre (zeng et al., 2019)etl-span (yu et al., 2019)wdec (nayak and ng, 2020)rsan‡ (yuan et al., 2020)casrelrandom‡ (wei et al., 2020)casrelbert ‡ (wei et al., 2020)tplinkerbert ‡ (wang et al., 2020a)prgcrandomprgcbert.
-61.0-63.977.984.994.5-81.589.791.3.
89.693.3.
-56.6-60.067.272.376.2-75.789.592.5.
82.391.9.
-58.7-61.972.178.184.4-78.589.691.9.
85.892.6.
-37.7-44.763.384.0--84.793.491.8.
90.694.0.
-36.4-41.159.991.5--79.590.192.0.
88.592.1.
-37.1-42.961.687.6--82.091.891.9.
89.593.0.
32.8-60.7--85.5-85.7--91.4.
87.893.5.
30.6-58.6--71.7-83.6--92.6.
83.891.9.
31.7-59.6--78.0-84.6--92.0.
85.892.7.
52.5-57.5--84.3-80.5--88.9.
82.589.9.table 3: comparison (%) of the proposed prgc method with the prior works.
bold marks the highest score,underline marks the second best score and ‡ marks the results reported by the original papers..model.
normal seo epo soo.
model.
n = 1 n = 2 n = 3 n = 4 n ≥ 5.ordercopyreetl-spancasreltplinkerprgc.
ordercopyreetl-spancasreltplinkerprgc.
*tyn.*glnbew.71.288.587.390.191.0.
65.487.389.487.990.4.
69.487.691.493.494.0.
60.191.592.292.593.6.
72.860.392.094.094.5.
67.480.594.795.395.9.
--77.0§90.1§81.8.
--90.4§86.0§94.6.table 4: f1-score (%) of sentences with different over-lapping patterns.
bold marks the highest score and §marks results obtained by ofﬁcial implementations..ordercopyreetl-spancasreltplinkerprgc.
ordercopyreetl-spancasreltplinkerprgc.
*tyn.*glnbew.71.788.588.290.091.1.
63.482.189.388.089.9.
72.682.190.392.893.0.
62.286.590.890.191.6.
72.574.791.993.193.5.
64.491.494.294.695.0.table 5: f1-score (%) of sentences with different num-bers of triples where n is the number of triples in asentence.
bold marks the highest score..components in our model are described in ap-pendix c..4.2.1 overall results.
table 3 shows the results of our model against otherbaseline methods on four datasets.
our prgcmethod outperforms them in respect of almost allevaluation metrics even if compared with the recentstrongest baseline (wang et al., 2020a) which isquite complicated..at the same time, we implement prgcrandomto validate the utility of our prgc decoder, whereall parameters of the encoder bert are randomlyinitialized.
the performance of prgcrandomdemonstrates that our decoder framework (whichobtains 7% improvements than casrelrandom) isstill more competitive and robust than others even.
without taking advantage of the pre-trained bertlanguage model..it.
is important.
has more parameters.
to note that even thoughthantplinkerbertcasrelbert , it only obtains 0.1% improvementson the webnlg* dataset, and the authorsattributed this to problems with the dataset itself.
however, our model achieves a 10× improvementsthan tplinker on the webnlg* dataset and asigniﬁcant promotion on the webnlg dataset.
thereason behind this is that the relation judgementcomponent of our model greatly reduces redundantrelations particularly in the versions of webnlgwhich contain hundreds of relations.
in otherwords, the reduction in negative relations providesan additional boost compared to the models thatperform entity extraction under every relation..6230dataset.
model.
flops (m) paramsdecoder.
inference time (1 / 24) f1-score.
nyt*.
webnlg*.
casreltplinkerprgc.
casreltplinkerprgc.
complexityo(kn) → o(n2)o(kn2)o(n2)o(kn2)o(n3)o(n2).
15.051105.9232.60.
105.377879.6833.75.
75,362110,73666,085.
527,534788,994409,534.
24.2 / -38.8 / 7.713.5 / 4.4.
30.5 / -41.7 / 13.214.4 / 5.2.
89.691.992.6.
91.891.993.0.table 6: comparison of model efﬁciency on both nyt* and webnlg* datasets.
results except f1-score (%)of other methods are obtained by the ofﬁcial implementation with default conﬁguration, and bold marks the bestresult.
complexity are the computation complexity, flops and paramsdecoder are both calculated on the decoder,and we measure the inference time (ms) with the batch size of 1 and 24, respectively..4.2.2 detailed results on complex scenariosfollowing previous works (wei et al., 2020; yuanet al., 2020; wang et al., 2020a), to verify the ca-pability of our model in handling different overlap-ping patterns and sentences with different numbersof triples, we conduct further experiments on nyt*and webnlg* datasets..as shown in table 4, our model exceeds allthe baselines in all overlapping patterns in bothdatasets except the soo pattern in the nyt*dataset.
actually, the observation on the latter sce-nario is not reliable due to the very low percentageof soo in nyt* (i.e., 45 out of 8,110 as shown intable 2).
as shown in table 5, the performance ofour model is better than others almost in every sub-set regardless of the number of triples.
in general,these two further experiments adequately show theadvantages of our model in complex scenarios..5 analysis.
5.1 model efﬁciency.
ﬁciency with respect to complexity, ﬂoating pointoperations (flops) (molchanov et al., 2017), pa-rameters of the decoder (paramsdecoder) and infer-ence time6 of casrel, tplinker and prgc in twodatasets which have quite different characteristicsin the size of relation set, the average number of re-lations per sentence and the average number of sub-jects per sentence.
all experiments are conductedwith the same hardware conﬁguration.
becausethe number of subjects in a sentence varies, it isdifﬁcult for casrel to predict objects in a heteroge-neous batch, and it is restricted to set batch size to 1in the ofﬁcial implementation (wang et al., 2020a).
for the sake of fair comparison, we set batch size to1 and 24 to verify the single-thread decoding speedand parallel processing capability, respectively..the results indicate that the single-thread de-coding speed of prgc is 2× as casrel and 3×as tplinker, and our model is signiﬁcantly betterthan tplinker in terms of parallel processing.
notethat the model efﬁciency of casrel and tplinkerdecreases as the size of relation set increases butour model is not affected by the size of relationset, thus prgc overwhelmingly outperforms bothmodels in terms of all the indicators of efﬁciencyin the webnlg* dataset.
compared with the state-of-the-art model tplinker, prgc is an order ofmagnitude lower in complexity and the flops iseven 200 times lower, thus prgc has fewer param-eters and obtains 3× speedup in the inference phasewhile the f1-score is improved by 1.1%.
eventhough casrel has lower complexity and flopsin the nyt* dataset, prgc still has signiﬁcantadvantages and obtains a 5× speedup in the in-ference time and 3% improvements in f1-score.
meanwhile, figure 2 proves our advantage in con-vergence rate.
these all conﬁrm the efﬁciency of.
6the flops and paramsdecoder are calculated via:.
figure 2: f1-score with respect to the epoch numberon the webnlg* validation set of different methods.
results of casrel and tplinker are obtained by theofﬁcial implementation with default conﬁguration..as shown in table 6, we evaluate the model ef-.
https://github.com/sovrasov/ﬂops-counter.pytorch..6231figure 3: case study for the ablation study of rel-spec sequence tagging.
examples are from webnlg*, andwe supplement the whole entity span through webnlg to facilitate viewing.
the red cross marks bad cases, thecorrect entities are in bold and the correct relations are colored..our model..5.2 ablation study.
in this section, we conduct ablation experiments todemonstrate the effectiveness of each componentin prgc with results reported in table 7..model.
prec.
rec..f1.
prgc.
*tyn.–potential relation prediction–rel-spec sequence tagging–global correspondence.
* prgcglnbew.–potential relation prediction–rel-spec sequence tagging–global correspondence.
93.391.563.871.6.
94.080.033.255.9.
91.991.791.791.2.
92.188.291.391.6.
92.691.675.280.2.
93.083.948.769.4.table 7: ablation study of prgc (%)..5.2.1 effect of potential relation predictionwe use each relation in the relation set to performsequence tagging when we remove the potentialrelation prediction component to avoid the ex-posure bias.
as shown in table 7, the precisionsigniﬁcantly decreases without this component, be-cause the number of predicted triples increases dueto relations not presented in the sentences, espe-cially in the webnlg* dataset where the size ofrelation set is much bigger and brings tremendousrelation redundancy.
meanwhile, with the increaseof relation number in sentences, the training and in-ference time increases three to four times.
throughthis experiment, the validity of this component thataims to predict a potential relation subset is proved,which is not only beneﬁcial to model accuracy, butalso to efﬁciency..5.2.2 effect of rel-spec sequence taggingas a comparison for sequence tagging scheme, fol-lowing wei et al.
(2020) and wang et al.
(2020a),we perform binary classiﬁcation to detect start.
and end positions of an entity with the span-basedscheme.
as shown in table 7, span-based schemebrings signiﬁcant decline of performance..through the case study shown in figure 3, weobserve that the span-based scheme tends to extractlong entities and identify the correct subject-objectpairs but ignore their relation.
that is becausethe model is inclined to remember the positionof an entity rather than understand the underlyingsemantics.
however, the sequence tagging schemeused by prgc performs well in both cases, andexperimental results prove that our tagging schemeis more robust and generalizable..5.2.3 effect of global correspondence.
for comparison, we exploit the heuristic nearestneighbor principle to combine the subject-objectpairs which was used by zheng et al.
(2017) andyuan et al.
(2020).
as shown in table 7, the pre-cision also signiﬁcantly decreases without globalcorrespondence, because the number of predictedtriples increases with many mismatched pairs whenthe model loses the constraint imposed by this com-ponent.
this experiment proves that the globalcorrespondence component is effective and greatlyoutperforms the heuristic nearest neighbor princi-ple in the subject-object alignment task..6 conclusion.
in this paper, we presented a brand-new perspec-tive and introduced a novel joint relational extrac-tion framework based on potential relation andglobal correspondence, which greatly alleviatesthe problems of redundant relation judgement, poorgeneralization of span-based extraction and inef-ﬁcient subject-object alignment.
experimental re-sults showed that our model achieved the state-of-the-art performance in the public datasets andsuccessfully handled many complex scenarios withhigher efﬁciency..6232references.
giannis bekoulis, johannes deleu, thomas demeester,and chris develder.
2018.joint entity recogni-tion and relation extraction as a multi-head selec-tion problem.
expert systems with applications,114:34–45..razvan c. bunescu and raymond j. mooney.
2005.a shortest path dependency kernel for relation ex-in proceedings of the human languagetraction.
technology conference and conference on empiri-cal methods in natural language processing, pages724–731, vancouver, bc..yee seng chan and dan roth.
2011..exploitingsyntactico-semantic structures for relation extrac-tion.
in proceedings of the 49th annual meeting ofthe association for computational linguistics: hu-man language technologies, pages 551–560..j. devlin, ming-wei chang, kenton lee, and kristinatoutanova.
2019. bert: pre-training of deep bidirec-tional transformers for language understanding.
inannual conference of the north american chapterof the association for computational linguistics..tsu-jui fu, peng-hsuan li, and wei-yun ma.
2019.graphrel: modeling text as relational graphs forjoint entity and relation extraction.
in proceedingsof the 57th annual meeting of the association forcomputational linguistics, pages 1409–1418, flo-rence, italy.
association for computational linguis-tics..claire gardent, anastasia shimorina, shashi narayan,and laura perez-beltrachini.
2017. creating train-in proceed-ing corpora for nlg micro-planners.
ings of the 55th annual meeting of the associationfor computational linguistics (volume 1: long pa-pers), pages 179–188, vancouver, canada.
associa-tion for computational linguistics..diederik p. kingma and jimmy ba.
2015. adam: amethod for stochastic optimization.
in 3rd interna-tional conference on learning representations, sandiego, ca, usa, conference track proceedings..qi li and heng ji.
2014. incremental joint extractionin proceedingsof entity mentions and relations.
of the 52nd annual meeting of the association forcomputational linguistics (volume 1: long papers),pages 402–412, baltimore, maryland.
associationfor computational linguistics..min lin, qiang chen, and shuicheng yan.
2014. net-work in network.
in 2nd international conferenceon learning representations, iclr 2014, banff, ab,canada, april 14-16, 2014, conference track pro-ceedings..yinhan liu, myle ott, naman goyal, jingfei du, man-dar joshi, danqi chen, omer levy, mike lewis,luke zettlemoyer, and veselin stoyanov.
2019.roberta: a robustly optimized bert pretraining ap-proach.
corr, abs/1907.11692..ilya loshchilov and frank hutter.
2017.weight decay regularization in adam.
abs/1711.05101..fixingcorr,.
makoto miwa and yutaka sasaki.
2014. modelingjoint entity and relation extraction with table repre-in proceedings of the 2014 conferencesentation.
on empirical methods in natural language process-ing, pages 1858–1869, doha, qatar.
association forcomputational linguistics..p molchanov, s tyree, t karras, t aila, and j kautz.
2017. pruning convolutional neural networks for re-source efﬁcient inference.
in 5th international con-ference on learning representations, iclr 2017-conference track proceedings..tapas nayak and hwee tou ng.
2020. effective mod-eling of encoder-decoder architecture for joint entityand relation extraction.
in proceedings of the aaaiconference on artiﬁcial intelligence, pages 8528–8535..rrubaa panchendrarajan and aravindh amaresan.
2018. bidirectional lstm-crf for named entityrecognition.
in proceedings of the 32nd paciﬁc asiaconference on language, information and compu-tation, hong kong.
association for computationallinguistics..sachin pawar, girish k. palshikar, and pushpak bhat-tacharyya.
2017. relation extraction : a survey.
corr, abs/1712.05191..jeffrey pennington, richard socher, and christophermanning.
2014. glove: global vectors for wordrepresentation.
in proceedings of the 2014 confer-ence on empirical methods in natural languageprocessing (emnlp), pages 1532–1543, doha,qatar.
association for computational linguistics..lev ratinov and dan roth.
2009. design challengesand misconceptions in named entity recognition.
inproceedings of the thirteenth conference on com-putational natural language learning, pages 147–155, boulder, colorado.
association for computa-tional linguistics..xiang ren, zeqiu wu, wenqi he, meng qu, clare rvoss, heng ji, tarek f abdelzaher, and jiawei han.
2017. cotype: joint extraction of typed entities andrelations with knowledge bases.
in proceedings ofthe 26th international conference on world wideweb, pages 1015–1024..s. riedel, limin yao, and a. mccallum.
2010. mod-eling relations and their mentions without labeledtext.
in proceedings of joint european conferenceon machine learning and knowledge discovery indatabases..erik f. tjong kim sang and fien de meulder.
2003. introduction to the conll-2003 shared task:language-independent named entity recognition.
inproceedings of the seventh conference on natu-ral language learning at hlt-naacl 2003, pages142–147..6233erik f. tjong kim sang and jorn veenstra.
1999. rep-in conference of the euro-resenting text chunks.
pean chapter of the association for computationallinguistics, pages 173–179, bergen, norway.
asso-ciation for computational linguistics..an end-to-end neural model with copy mechanism.
in proceedings of the 56th annual meeting of theassociation for computational linguistics (volume1: long papers), pages 506–514, melbourne, aus-tralia.
association for computational linguistics..yucheng wang, bowen yu, yueyang zhang, tingwenliu, hongsong zhu,and limin sun.
2020a.
tplinker: single-stage joint extraction of entitiesand relations through token pair linking.
in proceed-ings of the 28th international conference on com-putational linguistics, pages 1572–1582, barcelona,spain (online).
international committee on compu-tational linguistics..suncong zheng, feng wang, hongyun bao, yuexinghao, peng zhou, and bo xu.
2017.joint extrac-tion of entities and relations based on a novel tag-in proceedings of the 55th annualging scheme.
meeting of the association for computational lin-guistics (volume 1: long papers), pages 1227–1236,vancouver, canada.
association for computationallinguistics..zifeng wang, rui wen, xi chen, shao-lun huang,ningyu zhang, and yefeng zheng.
2020b.
findinginﬂuential instances for distantly supervised relationextraction.
corr, abs/2009.09841..zhepei wei, jianlin su, yue wang, yuan tian, andyi chang.
2020. a novel cascade binary taggingin pro-framework for relational triple extraction.
ceedings of the 58th annual meeting of the asso-ciation for computational linguistics, pages 1476–1488..bowen yu, zhenyu zhang, jianlin su, yubin wang,tingwen liu, bin wang, and sujian li.
2019. jointextraction of entities and relations based on a noveldecomposition strategy.
24th european conferenceon artiﬁcial intelligence - ecai 2020..xiaofeng yu and wai lam.
2010. jointly identifyingentities and extracting relations in encyclopedia textvia a graphical model approach.
in the 28th inter-national conference on computational linguistics,pages 1399–1407, beijing, china..yue yuan, xiaofei zhou, shirui pan, qiannan zhu,zeliang song, and li guo.
2020. a relation-speciﬁcattention network for joint entity and relation extrac-tion.
in international joint conference on artiﬁcialintelligence, pages 4054–4060.
association for theadvancement of artiﬁcial intelligence..dmitry zelenko, chinatsu aone,.
and anthonyrichardella.
2002. kernel methods for relation ex-traction.
in proceedings of the conference on em-pirical methods in natural language processing -volume 10, page 71–78, usa.
association for com-putational linguistics..xiangrong zeng, shizhu he, daojian zeng, kang liu,shengping liu, and jun zhao.
2019. learning theextraction order of multiple relational facts in a sen-tence with reinforcement learning.
in proceedingsof the conference on empirical methods in naturallanguage processing and the 9th international jointconference on natural language processing, pages367–377, hong kong, china.
association for com-putational linguistics..xiangrong zeng, daojian zeng, shizhu he, kang liu,and jun zhao.
2018. extracting relational facts by.
6234appendix.
a overlapping patterns.
as shown in figure 4, the normal, seo andepo patterns are usually mentioned in priorworks (nayak and ng, 2020; wei et al., 2020; yuanet al., 2020; wang et al., 2020a), and soo is a spe-cial pattern we identiﬁed and addressed..figure 4: examples of the normal, single entity over-lap (seo), entity pair overlap (epo) and subject ob-ject overlap (soo) patterns.
the overlapping entitiesare in bold..b implementation details.
we implement our model with pytorch and opti-mize the parameters by adam (kingma and ba,2015) with batch size of 64/6 for nyt/webnlg.
the encoder learning rate for bert is set as5 × 10−5, and the decoder learning rate is set as0.001 in order to converge rapidly.
we also conductweight decay (loshchilov and hutter, 2017) with arate of 0.01..for fair comparison, we use the bert-base-cased english model7 as our encoder, and set themax length of an input sentence to 100, which isthe same as previous works (wei et al., 2020; wanget al., 2020a).
our experiments are conducted onthe workstation with an intel xeon e5 2.40 ghzcpu, 128 gb memory, an nvidia tesla v100gpu, and centos 7.2. we train the model for100 epochs and choose the last model.
the per-formance will be better if the higher the thresholdof potential relation prediction (λ1), but tuningthe threshold of global correspondence (λ2) willnot help which is consistent with the analysis inappendix c..7available at https://huggingface.co/bert-base-cased..c results on different subtasks.
to further verify the results of the three subtasks inour new perspective and the performance of eachcomponent in our model, we present more detailedevaluations on nyt* and webnlg* datasets intable 8..subtask.
nyt*.
webnlg*.
prec.
rec..f1.
prec.
rec..f1.
relation judgemententity extraction (subject)entity extraction (object)subject-object alignmentcombination of above all.
95.381.282.894.093.3.
96.395.595.892.391.9.
95.887.888.893.192.6.
92.869.472.196.094.0.
96.296.395.793.492.1.
94.580.782.294.793.0.table 8: evaluation (%) of different subtasks on thenyt* and webnlg* datasets.
each subtask corre-sponds to a component in our model.
bold marks themost important metric of each subtask..relation judgement we evaluate outputs of thepotential relation prediction component which arepotential relations contained in a sentence.
recallis more important for this task because if a truerelation is missed, it will not be recovered in thefollowing steps.
we get high recall in this taskand the results show that effectiveness of potentialrelation prediction component is not affected bythe size of relation set..entity extraction this task is related to therelation-speciﬁc sequence tagging component,and we evaluate it as a named entity recogni-tion (ner) task with two types of entities: subjectsand objects.
the predicted entities are from allpotential relations of a sentence, and recall is moreimportant for this task because most false negativescan be ﬁltered out by subject-object alignment.
experimental results show that we extract almostall correct entities, and it further proves that theinﬂuence of the exposure bias is negligible..subject-object alignment this task is relatedto the global correspondence component, and wejust evaluate the entity pair in a triple and ignore therelation.
both recall and precision are importantfor this component, experimental results indicatethat our alignment scheme is useful but still can befurther improved, especially in the recall..overall, the combination of three componentsin our model accomplishes the relational triple ex-traction task with a ﬁne-grained perspective, andachieves better and solid results..6235