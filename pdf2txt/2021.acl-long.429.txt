uniﬁed interpretation of softmax cross-entropy and negative sampling:with case study for knowledge graph embedding.
hidetaka kamigaitotokyo institute of technologykamigaito@lr.pi.titech.ac.jp.
katsuhiko hayashigunma university, riken aipkhayashi0201@gmail.com.
abstract.
in knowledge graph embedding, the theoret-ical relationship between the softmax cross-entropy and negative sampling loss functionshas not been investigated.
this makes it dif-ﬁcult to fairly compare the results of the twodifferent loss functions.
we attempted to solvethis problem by using the bregman divergenceto provide a uniﬁed interpretation of the soft-max cross-entropy and negative sampling lossfunctions.
under this interpretation, we canderive theoretical ﬁndings for fair comparison.
experimental results on the fb15k-237 andwn18rr datasets show that the theoreticalﬁndings are valid in practical settings..1.introduction.
negative sampling (ns) (mikolov et al., 2013) isan approximation of softmax cross-entropy (sce).
due to its efﬁciency in computation cost, ns is nowa fundamental loss function for various naturallanguage processing (nlp) tasks such as used inword embedding (mikolov et al., 2013), languagemodeling (melamud et al., 2017), contextualizedembedding (clark et al., 2020b,a), and knowledgegraph embedding (kge) (trouillon et al., 2016).
speciﬁcally, recent kge models commonly usens for training.
considering the current usagesof ns, we investigated the characteristics of nsby mainly focusing on kge from theoretical andempirical aspects..first, we introduce the task description of kge.
a knowledge graph is a graph that describes the re-lationships between entities.
it is an indispensableresource for knowledge-intensive nlp applicationssuch as dialogue (moon et al., 2019) and question-answering (lukovnikov et al., 2017) systems.
how-ever, to create a knowledge graph, it is necessaryto consider a large number of entity combinationsand their relationships, making it difﬁcult to con-struct a complete graph manually.
therefore, the.
prediction of links between entities is an importanttask..currently, missing relational links between en-tities are predicted using a scoring method basedon kge (bordes et al., 2011).
with this method,a score for each link is computed on vector spacerepresentations of embedded entities and relations.
we can train these representations through vari-ous loss functions.
the sce (kadlec et al., 2017)and ns (trouillon et al., 2016) loss functions arecommonly used for this purpose..several studies (rufﬁnelli et al., 2020; ali et al.,2020) have shown that link-prediction performancecan be signiﬁcantly improved by choosing the ap-propriate combination of loss functions and scoringmethods.
however, the relationship between thesce and ns loss functions has not been investi-gated in kge.
without a basis for understandingthe relationships among different loss functions, itis difﬁcult to make a fair comparison between thesce and ns results..we attempted to solve this problem by using thebregman divergence (bregman, 1967) to provide auniﬁed interpretation of the sce and ns loss func-tions.
under this interpretation, we can understandthe relationships between sce and ns in terms ofthe model’s predicted distribution at the optimal so-lution, which we called the objective distribution.
by deriving the objective distribution for a lossfunction, we can analyze different loss functions,the objective distributions of which are identicalunder certain conditions, from a uniﬁed viewpoint.
we summarize our theoretical ﬁndings not re-.
stricted to kge as follows:.
• the objective distribution of ns with uniformnoise (ns w/ uni) is equivalent to that of sce..• the objective distribution of self-adversarialnegative sampling (sans) (sun et al., 2019).
is quite similar to sce with label smoothing(sce w/ ls) (szegedy et al., 2016)..• ns with frequency-based noise (ns w/ freq)in word2vec1 has a smoothing effect on theobjective distribution..• sce has a property wherein it more strongly.
ﬁts a model to the training data than ns..to check the validity of the theoretical ﬁndingsin practical settings, we conducted experiments onthe fb15k-237 (toutanova and chen, 2015) andwn18rr (dettmers et al., 2018) datasets.
theexperimental results indicate that.
• the relationship between sce and sce w/ lsis also similar to that between ns and sansin practical settings..• ns is prone to underﬁtting because it weaklyﬁts a model to the training data compared withsce..• sce causes underﬁtting of kge models when.
their score function has a bound..• both sans and sce w/ ls perform well as.
pre-training methods..the structure of this paper is as follows: sec.
2introduces sce and bregman divergence; sec.
3induces the objective distributions for ns; sec.
4analyzes the relationships between sce and nsloss functions; sec.
5 summarizes and discussesour theoretical ﬁndings; sec.
6 discusses empir-ically investigating the validity of the theoreticalﬁndings in practical settings; sec.
7 explains the dif-ferences between this paper and related work; andsec.
8 summarizes our contributions.
our code willbe available at https://github.com/kamigaito/acl2021kge.
2 softmax cross entropy and bregman.
divergence.
2.1 sce in kge.
we denote a link representing a relationship rkbetween entities ei and e j in a knowledge graphas (ei, rk, e j).
in predicting the links from givenqueries (ei, rk, ?)
and (?, rk, e j), the model must pre-dict entities corresponding to each ?
in the queries.
we denote such a query as x and the entity to be.
1the word2vec uses unigram distribution as the frequency-.
based noise..predicted as y. by using the softmax function,the probability pθ (y|x) that y is predicted from xwith the model parameter θ given a score functionfθ (x, y) is expressed as follows:.
pθ (y|x) =.
exp ( fθ (x, y))∑y(cid:48)∈y exp ( fθ (x, y(cid:48))).
,.
(1).
where y is the set of all predictable entities.
wefurther denote the pair of an input x and its labely as (x, y).
let d = {(x1, y1), · · · , (x|d|, y|d|)} beobserved data that obey a distribution pd(x, y)..2.2 bregman divergence.
next, we introduce the bregman divergence.
letψ(z) be a differentiable function; the bregman di-vergence between two distributions f and g is de-ﬁned as follows:.
dψ(z)( f , g) = ψ( f )−ψ(g)−∇ψ(g)t ( f −g).
(2).
we can express various divergences by chang-ing ψ(z).
the diver-to take into accountgence on the entire observed data, we con-sider the expectation of dψ( f , g): bψ(z)( f , g) =∑x,y dψ(z)( f (y|x), g(y|x))pd(x, y).
to investigatethe relationship between a loss function and learneddistribution of a model at an optimal solution of theloss function, we need to focus on the minimizationof bψ(z).
gutmann and hirayama (2011) showedthat bψ(z)( f , g) = 0 means that f equals g almosteverywhere when ψ(z) is a differentiable strictlyconvex function in its domain.
note that all ψ(z)in this paper satisfy this condition.
accordingly, byﬁxing f , minimization of bψ(z)( f , g) with respectto g is equivalent to minimization of.
bψ(z)( f , g).
=∑x,y.
(cid:2)−ψ(g) + ∇ψ(g)t g − ∇ψ(g)t f (cid:3) pd(x, y).
(3).
we use bψ( f , g) to reveal a learned distribution ofa model at optimal solutions for the sce and nsloss functions..2.3 derivation of sce.
for the latter explanations, we ﬁrst derive the sceloss function from eq.
(3).
we denote a probabilityfor a label y as p(y), vector for all y as y, vector ofprobabilities for y as p(y), and dimension size ofz as len(z).
in eq.
(3), by setting f as pd(y|x) andzi log zi (banerjeeg as pθ (y|x) with ψ(z) = ∑.
len(z)i=1.
et al., 2005), we can derive the sce loss functionas follows:.
proposition 1.
(cid:96)ns(θ ) can be induced from eq.
(3)by setting ψ(z) as:.
pd(yi|x) log pθ (yi|x).
pd(x, y).
(4).
proposition 2. when (cid:96)ns(θ ) equals 0, the follow-ing equation is satisﬁed:.
ψ(z) = z log(z) − (1 + z) log(1 + z)..(7).
bψ(z)(pd(y|x), pθ (y|x)).
(cid:35).
(cid:34) |y |∑i=1.
= −∑x,y1|d| ∑.
= −.
(x,y)∈d.
log pθ (y|x)..(5).
this derivation indicates that pθ (y|x) converges tothe observed distribution pd(y|x) through minimiz-ing bψ(z)(pd(y|x), pθ (y|x)) in the sce loss func-tion.
we call the distribution of pθ (y|x) when bψ(z)equals zero an objective distribution..3 objective distribution for negative.
sampling loss.
we begin by providing a deﬁnition of ns andits relationship to the bregman divergence, fol-lowing the induction of noise contrastive estima-tion (nce) from the bregman divergence that wasestablished by gutmann and hirayama (2011).
wedenote pn(y|x) to be a known non-zero noise dis-tribution for y of a given x. given ν noise sam-ples from pn(y|x) for each (x, y) ∈ d, ns esti-mates the model parameter θ for a distributiong(y|x; θ ) = exp(− fθ (x, y))..by assigning to each (x, y) a binary class labelc: c = 1 if (x, y) is drawn from observed data dfollowing a distribution pd(x, y) and c = 0 if (x, y)is drawn from a noise distribution pn(y|x), we canmodel the posterior probabilities for the classes asfollows:.
p(c = 1, y|x; θ ) =.
11+exp(− fθ (x, y)).
=.
11 + g(y|x; θ ).
,.
=.
g(y|x; θ )1 + g(y|x; θ ).
..the objective function (cid:96)ns(θ ) of ns is deﬁned asfollows:.
(cid:96)ns(θ ) = −.
1|d| ∑.
(x,y)∈d.
(cid:104)log(p(c = 1, y|x; θ )).
+.
ν∑i=1,yi∼pn.
(cid:105)log(p(c = 0, yi|x; θ ))..(6).
by using the bregman divergence, we can inducethe following propositions for (cid:96)ns(θ )..proposition 3. the objective distribution ofpθ (y|x) for (cid:96)ns(θ ) is.
g(y|x; θ ) =.
ν pn(y|x)pd(y|x).
..pd(y|x).
pn(y|x) ∑yi∈y.
..pd (yi|x)pn(yi|x).
(8).
(9).
proof.
we give the proof of props.
1, 2, and 3 inappendix a of the supplemental material..we can also investigate the validity of props.
1,2, and 3 by comparing them with the previouslyreported result.
for this purpose, we prove thefollowing proposition:.
proposition 4. when eq.
(8) satisﬁes ν = 1 andpn(y|x) = pd(y), fθ (x, y) equals point-wise mutualinformation (pmi)..proof.
this is described in appendix b of the sup-plemental material..this observation is consistent with that by levyand goldberg (2014).
the differences betweentheir representation and ours are as follows.
(1)our noise distribution is general in the sense that itsdeﬁnition is not restricted to a unigram distribution;(2) we mainly discuss pθ (y|x) not fθ (x, y); and (3)we can compare ns- and sce-based loss functionsthrough the bregman divergence..different from the objective distribution of sce,eq.
(9) is affected by the type of noise distributionpn(y|x).
to investigate the actual objective distribu-tion for (cid:96)ns(θ ), we need to consider separate casesfor each type of noise distribution.
in this subsec-tion, we further analyze eq.
(9) for each separatecase..3.1.1 ns with uniform noisefirst, we investigated the case of a uniform distri-bution because it is one of the most common noisedistributions for (cid:96)ns(θ ) in the kge task.
from eq.
(9), we can induce the following property..p(c = 0, y|x; θ ) = 1− p(c = 1, y|x; θ ).
3.1 various noise distributions.
proposition 5. when pn(y|x) is a uniform distri-bution, eq.
(9) equals pd(y|x)..eq.
(10) to a discrete uniform distribution u{1, |y |},pθ (y|x) becomes.
proof.
this is described in appendix c of the sup-plemental material..dyer (2014) indicated that ns is equal to ncewhen ν = |y | and pn(y|x) is uniform.
however,as we showed, in terms of the objective distribu-tion, the value of ν is not related to the objectivedistribution because eq.
(9) is independent of ν..3.1.2 ns with frequency-based noise.
in the original setting of ns (mikolov et al., 2013),the authors chose as pn(y|x) a unigram distributionof y, which is independent of x. such a frequency-based distribution is calculated in terms of frequen-cies on a corpus and independent of the modelparameter θ .
since in this case, different fromthe case of a uniform distribution, pn(y|x) remainson the right side of eq.
(9), pθ (y|x) decreaseswhen pn(y|x) increases.
thus, we can interpretfrequency-based noise as a type of smoothing forpd(y|x).
the smoothing of ns w/ freq decreasesthe importance of high-frequency labels in the train-ing data for learning more general vector represen-tations, which can be used for various tasks as pre-trained vectors.
since we can expect pre-trainedvectors to work as a prior (erhan et al., 2010) thatprevents models from overﬁtting, we tried to usens w/ freq for pre-training kge models in ourexperiments..sun et al.
(2019) recently proposed sans, whichuses pθ (y|x) for generating negative samples.
byreplacing pn(y|x) with pθ (y|x), the objective distri-bution when using sans is as follows:.
pθ (y|x) =.
,.
(10).
pd(y|x).
p ˆθ (y|x) ∑yi∈y.
pd (yi|x)p ˆθ (yi|x).
where ˆθ is a parameter set updated in the previ-ous iteration.
because both the left and right sidesof eq.
(10) include pθ (y|x), we cannot obtain ananalytical solution of pθ (y|x) from this equation.
however, we can consider special cases of pθ (y|x)to gain an understanding of eq.
(10).
at the begin-ning of the training, pθ (y|x) follows a discrete uni-form distribution u{1, |y |} because θ is randomlyinitialized.
in this situation, when we set p ˆθ (y|x) in.
pθ (y|x) = pd(y|x)..(11).
next, when we set p ˆθ (y|x) in eq.
(10) as pd(y|x),pθ (y|x) becomes.
pθ (y|x) = u{1, |y |}..(12).
in actual mini-batch training, θ is iteratively up-dated for every batch of data.
because pθ (y|x) con-verges to u{1, |y |} when p ˆθ (y|x) is close to pd(y|x)and pθ (y|x) converges to pd(y|x) when p ˆθ (y|x) isclose to u{1, |y |}, we can approximately regard theobjective distribution of sans as a mixture of pdand u{1, |y |}.
thus, we can represent the objectivedistribution of pθ (y|x) as.
pθ (y|x) ≈ (1 − λ )pd(y|x) + λ u{1, |y |}.
(13).
where λ is a hyper-parameter to determine whetherpθ (y|x) is close to pd(y|x) or u{1, |y |}.
assum-ing that pθ (y|x) starts from u{1, |y |}, λ shouldstart from 0 and gradually increase through train-ing.
note that λ corresponds to a temperature αfor p ˆθ (y|x) in sans, deﬁned as.
p ˆθ (y|x) =.
exp(α fθ (x, y))∑y(cid:48)∈y exp(α fθ (x, y(cid:48))).
,.
(14).
where α also adjusts p ˆθ (y|x) to be close to pd(y|x)or u{1, |y |}..4 theoretical relationships among loss.
4.1 corresponding sce form to ns with.
frequency-based noise.
we induce a corresponding cross entropy loss fromthe objective distribution for ns with frequency-pd (yi|x)based noise.
we set tx,y = pn(y|x) ∑pn(yi|x) ,yi∈ylen(z)i=1.
q(y|x) = t −1zi log zi.
x,y pd(y|x), and ψ(z) = ∑under these conditions, following induction fromeq.
(5), we can reformulatebψ(z)(q(y|x), p(y|x)) as follows:.
(4) to eq..bψ(z)(q(y|x), pθ (y|x)).
(cid:35)t −1x,y pd(yi|x) log pθ (yi|x).
pd(x, y).
(cid:34) |y |∑i=1.
= −∑x,y1|d| ∑.
= −.
(x,y)∈d.
t −1x,y log pθ (y|x)..(15).
3.1.3 self-adversarial ns.
functions.
loss.
objective distribution.
ψ(z) or ψ(z).
remarks.
ns w/ unins w/ freq.
pd(y|x)t −1x,y pd(y|x).
ψ(z) = z log(z) − (1 + z) log(1 + z)ψ(z) = z log(z) − (1 + z) log(1 + z).
sans.
(1 − λ )pd(y|x) + λ u{1, |y |} ψ(z) = z log(z) − (1 + z) log(1 + z) approximately derived.
λ increases.
scesce w/ bc.
pd(y|x)t −1x,y pd(y|x).
ψ(z) = ∑ψ(z) = ∑.
zi log zizi log zi.
len(z)i=1len(z)i=1.
len(z)i=1.
sce w/ ls.
(1 − λ )pd(y|x) + λ u{1, |y |}.
ψ(z) = ∑.
zi log zi.
λ is ﬁxed..tx,y = pn(y|x) ∑yi∈y.
pd (yi|x)pn(yi|x).
from zero in training..tx,y = pn(y|x) ∑yi∈y.
pd (yi|x)pn(yi|x).
table 1: summary of our theoretical ﬁndings.
w/ uni denotes with uniform noise, w/ freq denotes with frequency-based noise, w/ bc denotes with backward correction, and w/ ls denotes with label smoothing..except that tx,y is conditioned by x and not nor-malized for y, we can interpret this loss functionas sce with backward correction (sce w/ bc)(patrini et al., 2017).
taking into account that back-ward correction can be a smoothing method forpredicting labels (lukasik et al., 2020), this rela-tionship supports the theoretical ﬁnding that nscan adopt a smoothing to the objective distribution..because the frequency-based noise is used inword2vec as unigram noise, we speciﬁcally con-sider the case in which pn(y|x) is set to unigramnoise.
in this case, we can set pn(y|x) = pd(y).
since relation tuples do not appear twice in aknowledge graph, we can assume that pd(x, y) isuniform.
accordingly, we can change t −1tox,y1pd (y)c , where c.= pd (x).
=.
1pd (y) ∑yi∈y.
pd (yi|x)pd (yi).
pd (y) ∑yi∈y.
pd (yi,x)pd (yi)pd (x).
is a constant value, and we can reformulate eq.
(15)as follows:.
where #x and #y respectively represent frequenciesfor x and y in the training data.
we use eq.
(16) topre-train models for sce-based loss functions..4.2 corresponding sce form to sans.
we induce a corresponding cross entropy loss fromthe objective distribution for sans by settingq(y|x) = (1 − λ )pd(y|x) + λ u{1, |y |} and ψ(z) =zi log zi.
under these conditions, on the ba-∑sis of induction from eq.
(4) to eq.
(5), we can.
len(z)i=1.
reformulate bψ(z)(q(y|x), pθ (y|x)) as follows:.
bψ(z)(q(y|x), pθ (y|x)).
= −∑x,y.
(cid:20) |y |∑i=1.
(1 − λ )pd(yi|x) log pθ (yi|x).
+.
(cid:21)λ u{1, |y |} log pθ (yi|x).
|y |∑i=1(cid:20)(1 − λ ) log pθ (y|x).
pd(x, y).
= −.
1|d| ∑.
(x,y)∈d.
+.
|y |∑i=1.
λ|y |.
(cid:21)log pθ (yi|x)..(17)the equation in the brackets of eq.
(17) is the crossentropy loss that has a corresponding objective dis-tribution to that of sans.
this loss function is sim-ilar in form to sce with label smoothing (sce w/ls) (szegedy et al., 2016).
this relationship alsoaccords with the theoretical ﬁnding that ns canadopt a smoothing to the objective distribution..5 understanding loss functions for fair.
we summarize the theoretical ﬁndings from sec-tions 2, 3, and 4 in table 1. to compare the resultsfrom the theoretical ﬁndings, we need to under-stand the differences in their objective distributionsand divergences..5.1 objective distributions.
the objective distributions for ns w/ uni and sceare equivalent.
we can also see that the objec-tive distribution for sans is quite similar to thatfor sce w/ ls.
these theoretical ﬁndings will beimportant for making a fair comparison betweenscoring methods trained with the ns and sce lossfunctions.
when a dataset contains low-frequency.
log pθ (y|x).
pd(x)pd(y)c.−.
∝ −.
(x,y)∈d.
1|d| ∑1|d| ∑.
(x,y)∈d.
#x#y.log pθ (y|x),.
(16).
comparisons.
ψ(z) = ∑.
len(z)i=1ψ(z) = z log(z) − (1 + z) log(1 + z).
zi log zi.
22.28.
0.4.
0.2.)
p,5.
0(.)
z(.
ψd.20.
10.
0.
9.63.
8.95.
0.
0.
0.2.
0.4.
0.6.
0.8.
1.p.wn18rr fb15k-237 yago3-10 kinship.
umls.
nations.
3.09.
1.87.
0.88.figure 1: divergence between 0.5 and p in dψ(z) foreach ψ(z)..figure 2: kl divergence of pd(y|x) between trainingand test relations for each dataset.
entities, sans and sce w/ ls can improve thelink-prediction performance through their smooth-ing effect, even if there is no performance improve-ment from the scoring method itself.
for compar-ing the sce and ns loss functions fairly, therefore,it is necessary to use the vanilla sce against ns w/uni and use sce w/ ls against sans..however, we still have room to discuss the rela-tionship between sans and sce w/ ls becauseλ in sans increases from zero during training,whereas λ in sce w/ ls is ﬁxed.
to introduce thebehavior of λ in sans to sce w/ ls, we trieda simple approach in our experiments that trainskge models via sce w/ ls using pre-trained em-beddings from sce as initial parameters.
thoughthis approach is not exactly equivalent to sans,we expected it to work similarly to increasing λfrom zero in training..we also discuss the relationship between ns w/freq and sce w/ bc.
while ns w/ freq is oftenused for learning word embeddings, neither ns w/freq nor sce w/ bc has been explored in kge.
we investigated whether these loss functions areeffective in pre-training kge models2.
becausesans and sce w/ ls are similar methods to nsw/ freq and sce w/ bc in terms of smoothing, inour experiments, we also compared ns w/ freqwith sans and sce w/ bc with sce w/ ls aspre-training methods..5.2 divergences.
comparing ψ(z) for ns and sce losses is as im-portant as focusing on their objective distributions.
the ψ(z) determines the distance between model-.
2as a preliminary experiment, we also trained kge mod-els via ns w/ freq and sce w/ bc.
however, these meth-ods did not improve the link-prediction performance becausefrequency-based noise changes the data distribution drasti-cally..predicted and data distributions in the loss.
it hasan important role in determining the behavior ofthe model.
figure 1 shows the distance in eq.
(3)between the probability p and probability 0.5 foreach ψ in table 13. as we can see from the ex-ample, dψ(z)(0.5, p) of the sce loss has a largerdistance than that of the ns loss.
in fact, painskyand wornell (2020) proved that the upper boundof the bregman divergence for binary labels whenzi log zi.
this means that the sceψ(z) = ∑loss imposes a larger penalty on the same predictedvalue than the ns loss when the value of the learn-ing target is the same between the two losses4..len(z)i=1.
however, this does not guarantee that the dis-tance of sce is always larger than ns.
this is be-cause the values of the learning target between thetwo losses are not always the same.
to take intoaccount the generally satisﬁed property, we alsofocus on the convexity of the functions.
in eachtraining instance, the ﬁrst-order and second-orderderivatives of these loss functions indicate that sceis convex, but ns is not in their domains5.
sincethis property is independent of the objective distri-bution, we can consider sce ﬁts the model morestrongly to the training data in general.
because ofthese features, sce can be prone to overﬁtting..whether the overﬁtting is a problem depends onhow large the difference between training and testdata is.
to measure the difference between trainingand test data in a kg dataset, we calculated thekullback-leibler (kl) divergence for p(y|x) be-tween the training and test data of commonly usedkg datasets.
to compute p(y|x), we ﬁrst calculated.
3in this setting, we can expand ψ(z) = ∑.
ψ(z) = z log z + (1 − z) log (1 − z)..len(z)i=1.
zi log zi to.
4see appendix.
e for the further details.
5goldberg and levy (2014) discuss the convexity of theinner product in ns.
different from theirs, our discussion isabout the convexity of the loss functions itself..method.
loss.
fb15k-237.
wn18rr.
mrr hits@1 hits@3 hits@10 mrr hits@1 hits@3 hits@10.tucker.
rescal.
complex.
distmult.
transe.
rotate.
nssans.
scesce w/ ls.
nssans.
scesce w/ ls.
nssans.
scesce w/ ls.
nssans.
scesce w/ ls.
nssans.
scesce w/ ls.
nssans.
scesce w/ ls.
0.2570.330.
0.3380.343.
0.3370.339.
0.3520.363.
0.2960.300.
0.3000.318.
0.3040.320.
0.3420.344.
0.2840.328.
0.3240.323.
0.3010.333.
0.3150.315.
0.1510.238.
0.2460.251.
0.2470.249.
0.2600.269.
0.2110.214.
0.2180.231.
0.2190.234.
0.2520.254.
0.1820.230.
0.2320.231.
0.2030.238.
0.2280.228.
0.2970.365.
0.3720.378.
0.3680.372.
0.3870.400.
0.3240.328.
0.3260.348.
0.3360.352.
0.3740.377.
0.3190.365.
0.3590.359.
0.3330.371.
0.3470.346.
0.4720.512.
0.5210.529.
0.5160.520.
0.5370.548.
0.4680.472.
0.4660.493.
0.4700.489.
0.5210.526.
0.4980.525.
0.5080.508.
0.5050.523.
0.4860.489.
0.4310.445.
0.4530.472.
0.3850.389.
0.4510.469.
0.3940.432.
0.4630.477.
0.3890.410.
0.4380.448.
0.2180.219.
0.2290.229.
0.4690.472.
0.4520.447.
0.4070.421.
0.4240.441.
0.3540.363.
0.4170.435.
0.3730.407.
0.4340.441.
0.3740.386.
0.4070.410.
0.0110.016.
0.0540.054.
0.4290.431.
0.4230.417.
0.4400.455.
0.4650.483.
0.4050.404.
0.4700.485.
0.4030.442.
0.4730.491.
0.3940.419.
0.4470.460.
0.3900.394.
0.3660.369.
0.4840.487.
0.4630.461.
0.4730.489.
0.5070.528.
0.4370.434.
0.5120.529.
0.4320.480.
0.5210.546.
0.4160.452.
0.4970.527.
0.5100.514.
0.5230.522.
0.5470.550.
0.5070.502.table 2: results for each method in fb15k-237 and wn18rr datasets.
notations are same as those in table 1..p(ei|rk, e j) = p(ei|rk) + p(ei|e j) on the basis of fre-quencies in the data then calculated p(e j|rk, ei) inthe same manner.
we treated both p(ei|rk, e j) andp(e j|rk, ei) as p(y|x).
we denote p(y|x) in the train-ing data as p and in the test data as q. with thesenotations, we calculated dkl(p||q) as the kl di-vergence for p(y|x) between the test and trainingdata.
figure 2 shows the results.
there is a largedifference in the kl divergence between fb15k-237 and wn18rr.
we investigated how this dif-ference affects the sce and ns loss functions forlearning kge models..in a practical setting, the loss function’s diver-gence is not the only factor to affect the ﬁt to thetraining data.
model selection also affects the ﬁt-ting.
however, understanding a model’s behav-ior is difﬁcult due to the complicated relationshipbetween model parameters.
for this reason, weexperimentally investigated which combinationsof models and loss functions are suitable for linkprediction..6 experiments and discussion.
we conducted experiments to investigate the valid-ity of what we explained in section 5 through a.comparison of the ns and sce losses..6.1 experimental settings.
we evaluated the following models on the fb15k-237 and wn18rr datasets in terms of the meanreciprocal rank (mrr), hits@1, hits@3, andhits@10 metrics: tucker (balazevic et al., 2019);rescal (bordes et al., 2011); complex (trouil-lon et al., 2016); distmult (yang et al., 2015);transe (bordes et al., 2013); rotate (sun et al.,2019).
we used libkge (broscheit et al., 2020)6as the implementation.
for each model to be able tohandle queries in both directions, we also traineda model for the reverse direction that shares theentity embeddings with the model for the forwarddirection..to determine the hyperparameters of these mod-els, for rescal, complex, distmult, and transewith sce and sce w/ ls, we used the settingsthat achieved the highest performance in a previ-ous study (rufﬁnelli et al., 2020) for each lossfunction as well as the settings from the originalpapers for tucker and rotate.
in transe withns and sans, we used the settings used by sun.
6https://github.com/uma-pi1/kge.
et al.
(2019).
when applying sans, we set α toan initial value of 1.0 for libkge for all modelsexcept transe and rotate, and for transe and ro-tate, where we followed the settings of the originalpaper since sans was used in it.
when applyingsce w/ ls, we set λ to the initial value of libkge,0.3, except on transe and rotate.
in the originalsetting of rotate, because the values of sans fortranse and rotate were tuned, we also selected λfrom {0.3, 0.1, 0.01} using the development datain transe and rotate for fair comparison.
ap-pendix d in the supplemental material details theexperimental settings..6.2 characteristics of loss functions.
table 2 shows the results for each loss and modelcombination.
in the following subsections, we dis-cuss investigating whether our ﬁndings work in apractical setting on the basis of the results..6.2.1 objective distributions.
in terms of the objective distribution, when scew/ ls improves performance, sans also improvesperformance in many cases.
moreover, it accordswith our ﬁnding that sce w/ ls and sans havesimilar effects.
for transe and rotate, the rela-tionship does not hold, but as we will see later, thisis probably because transe with sce and rotatewith sce did not ﬁt the training data.
if the scedoes not ﬁt the training data, the effect of scew/ ls is suppressed as it has the same effect assmoothing..6.2.2 divergences.
next, let us focus on the distance of the loss func-tions.
a comparison of the results of wn18rrand fb15k-237 shows no performance degrada-tion of sce compared with ns.
this indicates thatthe difference between the training and test datain wn18rr is not so large to cause overﬁttingproblems for sce..in terms of the combination of models and lossfunctions, the results of ns are worse than thoseof sce in tucker, rescal, complex, and dist-mult.
because the four models have no constraintto prevent ﬁtting to the training data, we con-sider that the lower scores are caused by under-ﬁtting.
this conjecture is on the basis that the nsloss weakly ﬁts model-predicted distributions totraining-data distributions compared with the sceloss in terms of divergence and convexity..in contrast, the performance gap between ns.
method.
pre-train mrr hits@1 hits@3 hits@10.fb15k-237.
0.363-rescal0.363sce+sce w/ ls sce w/ bc 0.3610.364.sce w/ ls.
rescal+sans.
-nsns w/ freqsans.
0.3390.3420.3430.345.wn18rr.
0.477-complex0.477sce+sce w/ ls sce w/ bc 0.4690.481.sce w/ ls.
rotate+sans.
-nsns w/ freqsans.
0.4720.4700.4700.471.
0.2690.2680.2660.269.
0.2490.2510.2510.254.
0.4410.4390.4330.444.
0.4310.4330.4280.429.
0.4000.4000.3980.402.
0.3720.3760.3780.380.
0.4910.4930.4860.496.
0.4870.4830.4840.488.
0.5480.5520.5470.550.
0.5200.5240.5240.525.
0.5460.5500.5330.553.
0.5500.5480.5530.552.method.
pre-train mrr hits@1 hits@3 hits@10.table 3: results of pre-training methods.
+ denotescombination of model and loss function.
other nota-tions are same as those in table 1..and sce is smaller in transe and rotate.
thisis because the score functions of transe and ro-tate have bounds and cannot express minus values.
since sce has a normalization term, it is difﬁcultto represent values close to 1 when the score func-tion cannot represent negative values.
this featureprevents transe and rotate from completely ﬁttingto the training data.
therefore, we can assume thatns can be a useful loss function when the scorefunction is bounded..6.3 effectiveness of pre-training methods.
we also explored pre-training for learning kgemodels.
we selected the methods in table 2 thatachieved the best mrr for each ns-based loss andeach sce-based loss in each dataset.
in accordancewith the success of word2vec, we chose unigramnoise for both ns w/ freq and sce w/ bc..table 3 shows the results.
contrary to our ex-pectations, sce w/ bc does not work well as apre-training method.
because the unigram noisefor sce w/ bc can drastically change the originaldata distribution, sce w/ bc is thought to be effec-tive when the difference between training and testdata is large.
however, since the difference is notso large in the kg datasets, as discussed in the pre-vious subsection, we believe that the unigram noisemay be considered unsuitable for these datasets..compared with sce w/ bc, both sce w/ lsand sans are effective for pre-training.
this isbecause the hyperparameters of sce w/ ls and.
sans are adjusted for kg datasets..when using vanilla sce as a pre-trainingmethod, there is little improvement in predictionperformance, compared with other methods.
thisresult suggests that increasing λ in training is notas important for improving task performance..for rotate, there is no improvement in pre-training.
because rotate has strict constraints onits relation representation, we believe it may de-grade the effectiveness of pre-training..7 related work.
mikolov et al.
(2013) proposed the ns loss func-tion as an approximation of the sce loss functionto reduce computational cost and handle a largevocabulary for learning word embeddings.
ns isnow used in various nlp tasks, which must handlea large amount of vocabulary or labels.
melamudet al.
(2017) used the ns loss function for training alanguage model.
trouillon et al.
(2016) introducedthe ns loss function to kge.
in contextualized pre-trained embeddings, clark et al.
(2020a) indicatedthat electra (clark et al., 2020b), a variant ofbert (devlin et al., 2019), follows the same man-ner of the ns loss function..ns is frequently used to train kge mod-els.
kge is a task to complement a knowledgegraph that describes relationships between enti-ties.
knowledge graphs are used in various im-portant downstream tasks because of its conve-nience in incorporating external knowledge, suchas in a language model (logan et al., 2019), di-alogue (moon et al., 2019), question-answering(lukovnikov et al., 2017), natural language infer-ence (k m et al., 2018), and named entity recog-nition (he et al., 2020).
thus, current kge isimportant in nlp..due to the importance of kge, various scoringmethods including rescal (bordes et al., 2011),transe (bordes et al., 2013), distmult (yanget al., 2015), complex (trouillon et al., 2016),tucker (balazevic et al., 2019), and rotate (sunet al., 2019) used in our experiment, have been pro-posed.
however, the relationship between thesescore functions and loss functions is not clear.
sev-eral studies (rufﬁnelli et al., 2020; ali et al., 2020)have investigated the best combinations of scoringmethod, loss function, and their hyperparametersin kg datasets.
these studies differ from ours inthat they focused on empirically searching for goodcombinations rather than theoretical investigations..as a theoretical study, levy and goldberg (2014)showed that ns is equivalent to factorizing a matrixfor pmi when a unigram distribution is selected asa noise distribution.
dyer (2014) investigated thedifference between nce (gutmann and hyv¨arinen,2010) and ns.
gutmann and hirayama (2011) re-vealed that nce is derivable from bregman diver-gence.
our derivation for ns is inspired by theirwork.
meister et al.
(2020) proposed a frameworkto jointly interpret label smoothing and conﬁdencepenalty (pereyra et al., 2017) through investigatingtheir divergence.
yang et al.
(2020) theoreticallyinduced that a noise distribution that is close tothe true distribution behind the training data is suit-able for training kge models in ns.
they alsoproposed a variant of sans in the basis of theirinvestigation..different from these studies, we investigated thedistributions at optimal solutions of sce and nsloss functions while considering several types ofnoise distribution in ns..8 conclusion.
we revealed the relationships between sce andns loss functions in kge.
through theoreticalanalysis, we showed that sce and ns w/ uni areequivalent in objective distribution, which is thepredicted distribution of a model at an optimal so-lution, and that sce w/ ls and sans have similarobjective distributions.
we also showed that scemore strongly ﬁts a model to the training data thanns due to the divergence and convexity of sce..the experimental results indicate that the differ-ences in the divergence of the two losses were notlarge enough to affect dataset differences.
the re-sults also indicate that sce works well with highlyﬂexible scoring methods, which do not have anybound of the scores, while ns works well withrotate, which cannot express minus values dueto its bounded scoring.
moreover, they indicatethat sce and sans work better in pre-trainingthan ns w/ uni, commonly used for learning wordembeddings..for future work, we will investigate the proper-.
ties of loss functions in out-of-domain data..acknowledgements.
this work was partially supported by jsps kakenhigrant nos.
19k20339, 21h03491, and 21k17801..references.
mehdi ali, max berrendorf, charles tapley hoyt, lau-rent vermue, sahand sharifzadeh, volker tresp, andjens lehmann.
2020. pykeen 1.0: a python libraryfor training and evaluating knowledge graph emebd-dings.
arxiv preprint arxiv:2007.14175..ivana balazevic, carl allen, and timothy hospedales.
2019. tucker: tensor factorization for knowledgegraph completion.
in proceedings of the 2019 con-ference on empirical methods in natural languageprocessing and the 9th international joint confer-ence on natural language processing (emnlp-ijcnlp), pages 5185–5194, hong kong, china.
as-sociation for computational linguistics..arindam banerjee, srujana merugu,.
inderjit s.dhillon, and joydeep ghosh.
2005. clustering withbregman divergences.
journal of machine learningresearch, 6(58):1705–1749..antoine bordes, nicolas usunier, alberto garcia-duran,jason weston, and oksana yakhnenko.
2013. translating embeddings for modeling multi-relational data.
in advances in neural informationprocessing systems, volume 26, pages 2787–2795.
curran associates, inc..antoine bordes, jason weston, ronan collobert, andyoshua bengio.
2011. learning structured embed-in proceedings of thedings of knowledge bases.
twenty-fifth aaai conference on artiﬁcial intelli-gence, aaai’11, page 301–306.
aaai press..l.m.
bregman.
1967. the relaxation method of ﬁndingthe common point of convex sets and its applicationto the solution of problems in convex programming.
ussr computational mathematics and mathemati-cal physics, 7(3):200 – 217..samuel broscheit, daniel rufﬁnelli, adrian kochsiek,patrick betz, and rainer gemulla.
2020. libkge- a knowledge graph embedding library for repro-ducible research.
in proceedings of the 2020 con-ference on empirical methods in natural languageprocessing: system demonstrations, pages 165–174..kevin clark, minh-thang luong, quoc le, andchristopher d. manning.
2020a.
pre-training trans-formers as energy-based cloze models.
in proceed-ings of the 2020 conference on empirical methodsin natural language processing (emnlp), pages285–294, online.
association for computationallinguistics..kevin clark, minh-thang luong, quoc v. le, andelectra: pre-christopher d. manning.
2020b.
training text encoders as discriminators rather thanin international conference on learn-generators.
ing representations..tim dettmers, minervini pasquale, stenetorp pon-tus, and sebastian riedel.
2018. convolutional 2din proceedings ofknowledge graph embeddings..the 32th aaai conference on artiﬁcial intelligence,pages 1811–1818..jacob devlin, ming-wei chang, kenton lee, andkristina toutanova.
2019. bert: pre-training ofdeep bidirectional transformers for language under-in proceedings of the 2019 conferencestanding.
of the north american chapter of the associationfor computational linguistics: human languagetechnologies, volume 1 (long and short papers),pages 4171–4186, minneapolis, minnesota.
associ-ation for computational linguistics..chris dyer.
2014. notes on noise contrastive es-arxiv preprint.
timation and negative sampling.
arxiv:1410.8251..dumitru erhan, yoshua bengio, aaron courville,pierre-antoine manzagol, pascal vincent, and samybengio.
2010. why does unsupervised pre-trainingj. mach.
learn.
res.,help deep learning?
11:625–660..yoav goldberg and omer levy.
2014. word2vec ex-plained: deriving mikolov et al.’s negative-samplingword-embedding method.
corr, abs/1402.3722..michael gutmann and aapo hyv¨arinen.
2010. noise-contrastive estimation: a new estimation principlefor unnormalized statistical models.
in proceedingsof the thirteenth international conference on artiﬁ-cial intelligence and statistics, pages 297–304..michael u. gutmann and jun-ichiro hirayama.
2011.bregman divergence as general framework to es-in pro-timate unnormalized statistical models.
ceedings of the twenty-seventh conference on un-certainty in artiﬁcial intelligence, uai’11, page283–290, arlington, virginia, usa.
auai press..qizhen he, liang wu, yida yin, and heming cai.
2020. knowledge-graph augmented word represen-tations for named entity recognition.
proceedingsof the aaai conference on artiﬁcial intelligence,34(05):7919–7926..annervaz k m, somnath basu roy chowdhury, andambedkar dukkipati.
2018.learning beyonddatasets: knowledge graph augmented neural net-works for natural language processing.
in proceed-ings of the 2018 conference of the north ameri-can chapter of the association for computationallinguistics: human language technologies, vol-ume 1 (long papers), pages 313–322, new orleans,louisiana.
association for computational linguis-tics..rudolf kadlec, ondrej bajgar, and jan kleindienst.
2017. knowledge base completion: baselines strikeback.
in proceedings of the 2nd workshop on rep-resentation learning for nlp, pages 69–74, vancou-ver, canada.
association for computational linguis-tics..omer levy and yoav goldberg.
2014. neural wordembedding as implicit matrix factorization.
in pro-ceedings of the 27th international conference onneural information processing systems - volume 2,nips’14, page 2177–2185, cambridge, ma, usa.
mit press..giorgio patrini, alessandro rozza, aditya kr-ishna menon, richard nock, and lizhen qu.
2017.making deep neural networks robust to label noise:in proceedings of thea loss correction approach.
ieee conference on computer vision and patternrecognition (cvpr)..gabriel pereyra, george tucker,.
jan chorowski,lukasz kaiser, and geoffrey e. hinton.
2017. regu-larizing neural networks by penalizing conﬁdent out-put distributions.
corr, abs/1701.06548..daniel rufﬁnelli, samuel broscheit, and rainergemulla.
2020. you can teach an old dog newtricks!
on training knowledge graph embeddings.
in international conference on learning represen-tations..zhiqing sun, zhi-hong deng, jian-yun nie, and jiantang.
2019. rotate: knowledge graph embeddingby relational rotation in complex space.
in interna-tional conference on learning representations..christian szegedy, vincent vanhoucke, sergey ioffe,jon shlens, and zbigniew wojna.
2016. rethinkingthe inception architecture for computer vision.
inproceedings of the ieee conference on computer vi-sion and pattern recognition, pages 2818–2826..kristina toutanova and danqi chen.
2015. observedversus latent features for knowledge base and textin proceedings of the 3rd workshop oninference.
continuous vector space models and their composi-tionality, pages 57–66, beijing, china.
associationfor computational linguistics..th´eo trouillon, johannes welbl, sebastian riedel, ´ericgaussier, and guillaume bouchard.
2016. complexin icml,embeddings for simple link prediction.
pages 2071–2080..bishan yang, wen-tau yih, xiaodong he, jianfenggao, and li deng.
2015. embedding entities andrelations for learning and inference in knowledgebases.
in 3rd international conference on learningrepresentations, iclr 2015, san diego, ca, usa,may 7-9, 2015, conference track proceedings..zhen yang, ming ding, chang zhou, hongxia yang,jingren zhou, and jie tang.
2020. understandingnegative sampling in graph representation learning.
in proceedings of the 26th acm sigkdd inter-national conference on knowledge discovery anddata mining, kdd ’20, page 1666–1676, new york,ny, usa.
association for computing machinery..robert logan, nelson f. liu, matthew e. peters, mattgardner, and sameer singh.
2019. barack’s wifehillary: using knowledge graphs for fact-aware lan-guage modeling.
in proceedings of the 57th annualmeeting of the association for computational lin-guistics, pages 5962–5971, florence, italy.
associa-tion for computational linguistics..michal lukasik, srinadh bhojanapalli, aditya menon,and sanjiv kumar.
2020. does label smoothing mit-igate label noise?
in proceedings of the 37th inter-national conference on machine learning, volume119 of proceedings of machine learning research,pages 6448–6458.
pmlr..denis lukovnikov, asja fischer, jens lehmann, ands¨oren auer.
2017. neural network-based questionanswering over knowledge graphs on word and char-acter level.
in proceedings of the 26th internationalconference on world wide web, www ’17, page1211–1220, republic and canton of geneva, che.
international world wide web conferences steeringcommittee..clara meister, elizabeth salesky, and ryan cot-terell.
2020. generalized entropy regularization or:inthere’s nothing special about label smoothing.
proceedings of the 58th annual meeting of the asso-ciation for computational linguistics, pages 6870–6886, online.
association for computational lin-guistics..oren melamud, ido dagan, and jacob goldberger.
2017. a simple language model based on pmi ma-in proceedings of the 2017trix approximations.
conference on empirical methods in natural lan-guage processing, pages 1860–1865, copenhagen,denmark.
association for computational linguis-tics..tomas mikolov, ilya sutskever, kai chen, greg s cor-rado, and jeff dean.
2013. distributed representa-tions of words and phrases and their compositional-ity.
in advances in neural information processingsystems, volume 26. curran associates, inc..seungwhan moon, pararth shah, anuj kumar, and ra-jen subba.
2019. opendialkg: explainable conver-sational reasoning with attention-based walks overknowledge graphs.
in proceedings of the 57th an-nual meeting of the association for computationallinguistics, pages 845–854, florence, italy.
associ-ation for computational linguistics..a. painsky and g. w. wornell.
2020. bregman diver-gence bounds and universality properties of the loga-rithmic loss.
ieee transactions on information the-ory, 66(3):1658–1673..a proof of proposition 1, 2, and 3.we can reformulate (cid:96)ns as follows:.
(cid:32).
(cid:96)ns(θ ) = −.
log(p(c = 1, y|x; θ )) +.
log(p(c = 0, yi|x; θ )).
(cid:33).
1|d| ∑.
(x,y)∈d.
1|d| ∑.
(x,y)∈d.
1|d| ∑.
(x,y)∈d.
= −.
= −.
=.
1|d| ∑.
(x,y)∈d.
ν∑i=1,yi∼pn.
1|d| ∑.
(x,y)∈d.
ν∑i=1,yi∼pn.
log(p(c = 1, y|x; θ )) −.
log(p(c = 0, yi|x; θ )).
log(.
11 + g(y|x; θ ).)
−.
1|d| ∑.
(x,y)∈d.
ν∑i=1,yi∼pn.
log(.
g(yi|x; θ )1 + g(yi|x; θ ).
).
log(1 + g(y|x; θ )) +.
νν|d| ∑.
(x,y)∈d.
ν∑i=1,yi∼pn.
log(1 +.
1g(yi|x; θ ).
).
= ∑x,y.
pd(y|x) log(1 + g(y|x; θ ))pd(x) +∑x,y.
ν pn(y|x) log(1 +.
)pd(x).
(18).
1g(y|x; θ ).
letting u = (x, y), f (u) = ν pn(y|x)eq.
(18) as:.
pd (y|x) , g(u) = g(y|x; θ ), and pd(x) = 1.pd (y|x) pd(x, y), we can reformulate.
(cid:32).
∑x,y(cid:20).
(cid:96)ns(θ ) =.
pd(y|x) log(1 + g(u)).
ν pn(y|x) log(1 +.
pd(x, y) +∑x,y(cid:21).
1pd(y|x).
1g(u).)
f (u).
pd(x, y).
log(1 + g(u)) + log(1 +.
=∑x,y(cid:105)(cid:104)=∑log(1 + g(u)) − log(g(u)) f (u) + log(1 + g(u)) f (u)x,y(cid:104)=∑−g(u) log(1 + g(u)) + (1 + g(u)) log(1 + g(u))x,y.
pd(x, y).
1g(u).
).
1pd(y|x).
pd(x, y).
(cid:33).
(cid:105)+ log(g(u))g(u) + log(1 + g(u))g(u) − log(g(u)) f (u) + log(1 + g(u)) f (u).
pd(x, y).
(19).
with ψ(g(u)) = g(u) log(g(u)) − (1 + g(u)) log(1 + g(u)) and ∇ψ(g(u)) = log(g(u)) − log(1 + g(u)),we can reformulate eq.
(19) as:(cid:105)(cid:104)(cid:96)ns(θ ) =∑−ψ(g(u)) + ∇ψ(g(u))g(u) − ∇ψ(g(u)) f (u).
pd(x, y).
x,y.
(20)from eq.
(20), when (cid:96)ns(θ ) is minimized, g(u) = f (u) is satisﬁed.
in this condition, g(y|x; θ ) becomespd (y|x) , and exp( fθ (x, y)) becomes pd (y|x)ν pn(y|x).
=bψ(z)( f (u), g(u))..pd(y|x)ν pn(y|x)based on the eq.
(1) and eq.
(21), the objective distribution for pθ (y|x) is as follows:.
g(u) = f (u) ⇔ g(y|x; θ ) =.
⇔ exp( fθ (x, y)) =.
..ν pn(y|x) as follows:ν pn(y|x)pd(y|x).
(21).
(22).
pθ (y|x) =.
pd(y|x).
pn(y|x) ∑yi∈y.
..pd (yi|x)pn(yi|x).
b proof of proposition 4.pmi is induced by multiplying pd(x) to the right-hand side of eq.
(8) and then computing logarithm forboth sides as follows:pn(y|x)pd(y|x).
pd(x, y)pd(x)pd(y).
⇔ exp( fθ (x, y)) =.
⇔ fθ (x, y) = log.
pd(y|x)pd(y).
pd(y|x)pn(y|x).
g(y|x; θ ) =.
=.
=.
pd(x, y)pd(y)pd(y)(23).
when pn(y|x) is a uniform distribution, pn(y|x) ∑yi∈y.
pd (yi|x)pn(yi|x) = ∑yi∈y.
pd(yi|x) = 1, and thus, eq.
(9) becomes.
c proof of proposition 5.pd(y|x)..d experimental details.
dataset: we use fb15k-237 (toutanova and chen, 2015)7 and wn18rr (dettmers et al., 2018)8 datasetsin the experiments.
we followed the standard split in the original papers for each dataset.
table 4 lists thestatistics for each dataset..dataset.
entities.
relations.
tuples.
train.
valid.
test.
wn18rrfb15k-237.
40,94314,541.
11237.
86,835272,115.
3,03417,535.
3,13420,466.table 4: the numbers of each instance for each dataset..metric: we evaluated the link prediction performance of models with mrr, hits@1, hits@3, andhits@10 by ranking test triples against all other triples not appeared in the training, valid, and test datasets.
we used libkge for calculating these metric scores.
model: we compared the following models: tucker (balazevic et al., 2019); rescal (bordes et al.,2011); complex (trouillon et al., 2016); distmult (yang et al., 2015); transe (bordes et al., 2013);rotate (sun et al., 2019).
for each model, we also trained a model for the reverse direction that sharesthe entity embeddings with the model for the forward direction.
thus, the dimension size of subject andobject embeddings are the same in all models.
implementation: we used libkge (broscheit et al., 2020)9 as the implementation.
we used its 1vsallsetting for sce-based loss functions and negative sampling setting for ns-based loss functions.
wemodiﬁed libkge to be able to use label smoothing on the 1vsall setting.
we also incorporated ns w/freq and sce w/ bc into the implementation.
hyper-parameter: table 5 and 6 show the hyper-parameter settings of each method for each dataset.
in rescal, complex, and distmult we used the settings that achieved the highest performance foreach loss function in the previous study (rufﬁnelli et al., 2020)10. in tucker and rotate, we follow thesettings from the original paper.
when applying sans, we set α to an initial value of 1.0 for libkgefor all models except transe and rotate, and for transe and rotate, where we followed the settingsof the original paper of sans since sans was used in it.
when applying sce w/ ls, we set λ to theinitial value of libkge, 0.3, except on transe and rotate.
in the original setting of transe and rotate,because the value of sans was tuned for comparison, for fairness, we selected λ from {0.3, 0.1, 0.01} byusing the development data through a single run for each value.
we set the maximum epoch to 800. wecalculated mrr every ﬁve epochs on the developed data, and the training was terminated when the highestvalue was not updated ten times.
we chose the best model by using the mrr score on the developmentdata.
these hyperparameters were also used in the pre-training step.
validation score table 7, 8, and 9 show the best mrr scores of each loss for each model on the validationdataset.
device: in all models, we used a single nvidia rtx2080ti for training.
except for rotete withsce-based loss functions, all models ﬁnished the training in one day.
the rotete with sce-based lossfunction ﬁnished the training in at most one week..7https://www.microsoft.com/en-us/download/confirmation.aspx?id=523128https://github.com/timdettmers/conve9https://github.com/uma-pi1/kge10https://github.com/uma-pi1/kge-iclr20.
model.
batch dim initialize.
regularize.
dropout.
optimizer.
sample.
λ.α.type entity.
relation.
entity.
rel..type.
lr.
decay p..sub..obj..fb15k-237.
tucker.
rescal.
comlex.
distmult.
transe.
rotate.
128scesce w/ ls 128128ns128sans.
sce512sce w/ ls 512256ns256sans.
sce512sce w/ ls 512512ns512sans.
scescenssans.
51251210241024.sce128sce w/ ls 1281024ns1024sans.
sce1024sce w/ ls 10241024ns1024sans.
200200200200.
128128128128.
128128256256.
128128256256.
12812810001000.
1000100010001000.xn: 1.0xn: 1.0xn: 1.0xn: 1.0.n: 0.123n: 0.123xn: 1.0xn: 1.0.u: 0.311u: 0.311n: 4.81e-5n: 4.81e-5.
n: 0.806n: 0.806u: 0.848u: 0.848.u: 1.0e-5u: 1.0e-5xu: 1.0xu: 1.0.xu: 1.0xu: 1.0xu: 1.0xu: 1.0.
----.
----.
----.
--lp: 3lp: 3.
--lp: 2lp: 2.
--lp: 3lp: 3.
--1.22e-121.22e-12.
--6.34e-96.34e-9.
--1.55e-101.55e-10.
--4.80e-144.80e-14.
--9.08e-189.08e-18.
--3.93e-153.93e-15.
0.30.30.30.3.
0.4270.4270.3470.347.
0.04760.04760.1820.182.
0.3700.3700.4550.455.
----.
----.
0.40.40.40.4.
0.1590.159--.
adamadamadamadam.
adamadamadagradadagrad.
0.00050.00050.00050.0005.
7.39e-57.39e-50.01700.0170.adagrad0.4430.443adagrad0.0437 adagrad0.0437 adagrad.
0.5030.5030.2410.241.
0.2800.2800.3600.360.adamadamadagradadagrad.
----.
----.
adamadamadamadam.
adamadamadamadam.
0.000630.000630.1410.141.
0.00030.00030.000050.00005.
0.000050.000050.000050.00005.
----.
0.950.950.950.95.
0.950.950.950.95.
0.950.950.950.95.
0.950.950.950.95.
0.950.950.950.95.
----.
all allall allall allall all.
1 all all1 all all15551555.
2222.
7 all all7 all all484484.
11.
1 all all1 all all36755793675579.
5 all all5 all all25625652562565.
5 all all5 all all25625652562565.
-0.3--.
-0.3--.
-0.3--.
-0.3--.
-0.01--.
-0.01--.
table 5: the hyper-parameters for each model in fb15k-237.
rel.
denotes relation, p. denotes patience, sub.
denotes subjective, obj.
denotes objective, xn denotes xavier normal, n denotes normal, xu denotes xavier uniform,and u denotes uniform..model.
batch dim initialize.
regularize.
dropout.
optimizer.
sample.
λ.α.type entity.
relation.
entity.
rel..type.
lr.
decay p..sub..obj..wn18rr.
tucker.
rescal.
comlex.
distmult.
transe.
rotate.
sce128sce w/ ls 128128ns128sans.
512scesce w/ ls 512512ns512sans.
sce512sce w/ ls 5121024ns1024sans.
sce512sce w/ ls 5121024ns1024sans.
sce128sce w/ ls 128512ns512sans.
512scesce w/ ls 512512ns512sans.
200200200200.
256256128128.
128128128128.
128128128128.
512512500500.
500500500500.xn: 1.0xn: 1.0xn: 1.0xn: 1.0.xn: 1.0xn: 1.0n: 1.64e-4n: 1.64e-4.
u: 0.281u: 0.281xn: 1.0xn: 1.0.u: 0.311u: 0.311xn: 1.0xn: 1.0.xn: 1.0xn: 1.0xu: 1.0xu: 1.0.xu: 1.0xu: 1.0xu: 1.0xu: 1.0.
----.
----.
----.
lp: 2lp: 2--.
lp: 2lp: 2--.
lp: 2lp: 2--.
4.52e-64.52e-6--.
1.44e-181.44e-18--.
2.13e-72.13e-7--.
4.19e-104.19e-10--.
1.44e-181.44e-18--.
8.99e-138.99e-13--.
adamadamadamadam.
adamadamadamadam.
0.20.20.20.2.
----.
0.3590.3590.04660.0466.
0.04760.04760.04660.0466.
0.2520.252--.
----.
0.20.20.20.2.
----.
----.
----.
adagradadagrad.
0.3110.3110.0826 adam0.0826 adam.
adagradadagrad.
0.4430.4430.0826 adam0.0826 adam.
adagradadagradadamadam.
adamadamadamadam.
0.00050.00050.00050.0005.
0.002460.002460.001520.00152.
0.5260.5263.32e-53.32e-5.
0.5030.5033.32e-53.32e-5.
0.2530.2530.000050.00005.
0.000050.000050.000050.00005.
----.
0.950.950.950.95.
0.950.950.950.95.
0.950.950.950.95.
0.950.950.950.95.
0.950.950.950.95.
----.
allallallall.
9 all9 all11.
66.
5 all5 all77.
66.
7 all7 all77.
66.allallallall.
allall88.allall66.allall66.
-0.3--.
-0.3--.
-0.3--.
-0.3--.
5 all5 all55.
10241024.
5 all5 all55.
10241024.allall10241024.allall10241024.
-0.01--.
-0.01--.
table 6: the hyper-parameters for each model in wn18rr.
the notations are the same as table 5..----.
----.
----.
----.
----.
----.
----.
----.
----.
----.
----.
----.
---1.0.
---1.0.
---1.0.
---1.0.
---1.0.
---1.0.
---1.0.
---1.0.
---1.0.
---1.0.
---0.5.
---0.5.model.
loss.
fb15k-237 wn18rr.
tucker.
rescal.
complex.
distmult.
transe.
rotate.
scesce w/ lsnssans.
scesce w/ lsnssans.
scesce w/ lsnssans.
scesce w/ lsnssans.
scesce w/ lsnssans.
scesce w/ lsnssans.
0.3450.3500.2610.337.
0.3590.3690.3440.344.
0.3040.3240.3020.308.
0.3500.3510.3080.326.
0.3280.3220.2890.333.
0.3200.3200.3060.340.
0.4510.4700.4330.441.
0.4610.4740.3890.390.
0.4680.4780.3990.433.
0.4410.4510.3910.412.
0.2270.2200.2160.218.
0.4520.4490.4720.475.table 7: the best mrr scores on validation data..dataset.
mehotd.
fb15k-237.
rescal+sce w/bcrescal+ns w/ freq.
mrr.
0.1490.171.wn18rr.
complex+sce w/ bc 0.3610.469rotate+ns w/ freq.
fb15k-237.
method.
rescal+sce w / ls.
rescal+sans.
wn18rr.
method.
complex+sce w/ ls.
rotate+sans.
pretrain.
mrr.
sce0.369sce w/ bc 0.3690.371sce w/ ls.
nsns w/ freqsans.
0.3490.3480.350.pretrain.
mrr.
sce0.483sce w/ bc 0.4690.481sce w/ ls.
nsns w/ freqsans.
0.4720.4740.475.table 8: the best mrr scores of pre-trained models on validation data..table 9: the best mrr scores of models initialized with pre-trained embeddings on validation data..e note: the divergence between the ns and sce loss functions.
painsky and wornell (2020) proved that the upper bound of the bregman divergence for binary labelszi log zi.
however, to compare the sce and ns loss functions in general, we need towhen ψ(z) = ∑zi log zi, we can derive the followingconsider the divergence of multi labels in sce.
when ψ(z) = ∑inequality by using the log sum inequality:.
len(z)i=1.
len(z)i=1.
dψ(z)(pd(y|x), pθ (y|x))|y |∑i=1.
pd(yi|x)pθ (yi|x).
pd(yi|x) log.
=.
=pd(y j|x) log.
+.
pd(yi|x) log.
(cid:61)pd(y j|x) log.
+ (.
pd(yi|x)) log.
|y |∑i(cid:54)= j.
|y |∑i(cid:54)= j.pd(y j|x)pθ (y j|x).
pd(y j|x)pθ (y j|x).
pd(y j|x)pθ (y j|x).
pd(yi|x)pθ (yi|x).
(∑.
|y |i(cid:54)= j pd(yi|x))|y |i(cid:54)= j pθ (yi|x))(∑(1 − pd(y j|x))(1 − pθ (y j|x)).
..=pd(y j|x) log.
+ (1 − pd(y j|x)) log.
(24).
eq.
(24) shows that the divergence of multi labels is larger than that of binary labels in sce.
as weexplained, dψ(z)( f , g) of sce is larger than dψ(z)( f , g) of ns in binary labels.
therefore, the sce lossimposes a larger penalty on the same predicted value than the ns loss when the value of the learningtarget is the same between the two losses..