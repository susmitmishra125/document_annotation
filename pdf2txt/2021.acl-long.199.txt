multi-stage pre-training over simpliﬁed multimodal pre-training models.
tongtong liubeijing university of postsand telecommunicationsttliu@bupt.edu.cn.
fangxiang fengbeijing university of postsand telecommunicationsfxfeng@bupt.edu.cn.
xiaojie wangbeijing university of postsand telecommunicationsxjwang@bupt.edu.cn.
abstract.
multimodal pre-training models,such aslxmert, have achieved excellent results indownstream tasks.
however, current pre-trained models require large amounts of train-ing data and have huge model sizes, whichmake them difﬁcult to apply in low-resourcesituations.
how to obtain similar or even bet-ter performance than a larger model under thepremise of less pre-training data and smallermodel size has become an important problem.
in this paper, we propose a new multi-stagepre-training (msp) method, which uses infor-mation at different granularities from word,phrase to sentence in both texts and imagesto pre-train the model in stages.
we also de-sign several different pre-training tasks suit-able for the information granularity in dif-ferent stage in order to efﬁciently capturethe diverse knowledge from a limited corpus.
we take a simpliﬁed lxmert (lxmert-s), which has only 45.9% parameters of theoriginal lxmert model and 11.76% of theoriginal pre-training data as the testbed of ourmsp method.
experimental results show thatour method achieves comparable performanceto the original lxmert model in all down-stream tasks, and even outperforms the origi-nal model in image-text retrieval task..1.introduction.
self-attention based transformer (vaswani et al.,2017) effectively overcomes the problem of rnnbeing difﬁcult to run in parallel, and greatly pro-motes the development of large-scale pre-trainingmodels.
the pre-training language models, suchas bert (devlin et al., 2019), have achievedexcellent performance in many natural languageprocessing tasks.
with their big success, re-searchers have also developed pre-training mod-els on multimodal tasks.
a series of multimodalpre-training models have been proposed, such as.
vilbert (lu et al., 2019), lxmert (tan andbansal, 2019), uniter (chen et al., 2019) etc.,and have achieved excellent results in language-vision multimodal tasks..however, the current pre-training models arenormally with large-scale parameters, require hugepre-training data and have very high demands oncomputational resources.
for example, the gptmodel (radford et al., 2018) has 110 million pa-rameters, gpt-2 (radford et al., 2019) has 1.5 bil-lion parameters, and gpt-3 (brown et al., 2020)has a staggering 175 billion parameters.
the sameis true for multimodal pre-trained models.
for ex-ample, lxmert (tan and bansal, 2019) has 183.5million parameters and requires 816 titanx gpuhours for training on 9.18 million text-image pairs.
the sizes of these models are too huge for them tobe deployed in many real-world scenarios.
there-fore, the study of lightweight pre-training models,which can achieve similar performances to large-scale models with smaller parameter scales andtraining costs, is signiﬁcantly valuable..there are some types of work on developinglightweight pre-trained models, including the de-sign of the model structure, quantization, pruningand distillation.
for example, albert (lan et al.,2020) is a lightweight model through structuraldesign such as parameter sharing and parameterdecomposition, and achieves better performancethan original models; q8bert (zafrir et al., 2019)compresses the model to 1/4 of the original modelbut with no more than 1% performance loss byquantizing 32bit ﬂoating point into 8bit; (michelet al., 2019) used bert weight pruning to com-press the model and found that removing a largenumber of attention heads would not have a majorimpact on the model performance; tinybert (jiaoet al., 2020) reduced the model size by 7.5 timesbut with no more than 4% performance loss bydesigning a teacher-student distillation model..proceedingsofthe59thannualmeetingoftheassociationforcomputationallinguisticsandthe11thinternationaljointconferenceonnaturallanguageprocessing,pages2556–2565august1–6,2021.©2021associationforcomputationallinguistics2556all above works are on language pre-trainingmodels, and most of them concern scales of modelparameters.
there are few works on cuttingtraining data and light weighing multimodal pre-training model.
in fact, compared with languagemodel, multimodal pre-training models should dealwith data from both language and visual modal,which demand larger amounts of data and morecomputational resources.
meanwhile, collectionsof training data are more difﬁcult.
taking for exam-ple the size of text-image pairs used for multimodalpre-training, the frequently used ms coco (linet al., 2014) is a high quality dataset with only0.82m pairs, while lait (qi et al., 2020) is al-ready a big data with 10m pairs but with averagequality.
therefore, it is signiﬁcantly valuable to de-velop lightweight multimodal pre-training modelswhich can make use of limited data efﬁciently..existing research on curriculum learning (ben-gio et al., 2009) has shown that imitating the pro-cess of human learning by gradually increasingthe difﬁculty of a task from simple to complex instages helps to make better use of different typesof data and effectively improve the performanceof learning.
many models (qi et al., 2020) use asmuch as data available but few works have beendone on how to arrange the tasks for better mak-ing use of limited data.
we therefore borrow theidea of curriculum learning on training pre-trainingmodels.
we construct a pre-training process whichmakes use of data from smaller units to bigger unitsin stages, and design appropriate pre-training tasksfor each corresponding stage..speciﬁcally, we propose a new multi-stage pre-training (msp) method.
the ﬁrst pre-training stageis on the token units, where the text input is thecategory labels of the objects in the images, andthe image input is the object features.
an imagefeatures random shufﬂe (ifrs) is designed asa pre-training task for this stage.
ifrs randomlyshufﬂes the object features, and the model predictsthe original object order based on the text infor-mation.
the second stage focuses on phrase units.
phrase-level descriptions of the image are input onthe text side and image features are input on theimage side.
a topic of image and text for phrase(titp) task is designed for it.
the third stage issentence-based pre-training.
sentence-level cap-tions are input on the text side, and image featuresare input on the image side.
a topic of image andtext for sentence (tits) task is designed for it.
we.
take a simpliﬁed lxmert (lxmert-s) whichhas fewer parameters and less pre-training data asthe testbed of our msp method.
experimental re-sults show that our method achieves comparableperformance to the original lxmert model indownstream tasks..the main contributions of our work are as fol-lows: (1) we propose a new msp method thatallows the model to learn different granularities oftext-image correspondence information at differentstages; (2) for each stage, we design pre-trainingtasks suitable for that stage, ifrs task for token-based pre-training, titp task for phrase-based pre-training, and tits task for sentence-based pre-training; (3) with less pre-trained data (11.76%),fewer model parameters (45.9%), less resource con-sumption (25%) and less training time (46.57%),the performances of downstream tasks are compa-rable to or even exceed that of the original model..2 related works.
multimodal pre-training models multimodalpre-training models are mainly divided into twocategories: single-stream models and two-streammodels.
single-stream models such as b2t2 (al-berti et al., 2019), oscar (li et al., 2020), etc.,fuse image and text information at the beginningof the input;two-stream models such as vil-bert (lu et al., 2019), lxmert(tan and bansal,2019), etc., encode the image and text informa-tion alone ﬁrst and then fuse them later.
gener-ally two-stream models will have more parame-ters than single-stream models, but whether thesingle-stream model or the two-stream model hasbetter performance or is related to the speciﬁctasks require more rigorous experimental proof.
we conduct follow-up experiments based on thetwo-stream model lxmert by removing the cod-ing layer of the individual modalities and keepingonly the fusion coding layer, so that the simpliﬁedlxmert model is more like the single-streammodel..multimodal pre-training data there are sev-eral different considerations on making use ofdata.
visualbert (li et al., 2019) believes thatpre-training on the target dataset can improve theperformance of the model, so visualbert ﬁrstpre-trains on coco caption and then continuespre-training on the target dataset (e.g.
vqa).
im-agebert (qi et al., 2020), on the other hand, istrained on the out-of-domain lait dataset and.
2557figure 1: overview of our proposed msp method, including three stages from token, phrase to sentence-basedpre-training, with appropriate pre-training tasks for each stage of pre-training..then on the in-domain datasets, such as concep-tual captions(cc) (sharma et al., 2018) and sbucaptions (ordonez et al., 2011).
it can be saidthe dataset that is most similar to the downstreamtask is used for training at last, and the generaldata is used ﬁrstly.
clearly, this way of using datais directly related to the downstream tasks.
dif-ferent downstream tasks might lead to differentorder of data usage.
in this paper, we design astaged pre-training from word-level to phrase-levelto sentence-level, which is related to the size ofinformation units.
we also design suitable pre-training tasks for different phases to fully exploitthe text-image information correspondence of dif-ferent units in each phase, which has consistenteffectiveness for different downstream tasks..multimodal pre-training tasks the mostlyemployed language pre-training task is maskedlanguage modeling (mlm) (chen et al., 2019),where tokens are masked with a probability andthose masked tokens are predicted by the model.
masked region feature regression (mrfr) (chenet al., 2019), which is similar to the mlm task, isa popular image pre-training task.
masked ob-ject classiﬁcation (moc) (qi et al., 2020) taskcan be regarded as a multimodal pre-training task,which is to predict the category label of eachmasked object feature.
another popular multi-modal pre-training task called image-text match-ing (itm) (chen et al., 2019) is similar to the nextsentence prediction (nsp) task in bert (devlinet al., 2019), where an image corresponding to atext is randomly replaced with a probability of 50%,and the task is to discriminate whether the imagematches the text.
the existing pre-training tasks formultimodal data are limited.
we design new pre-training tasks with the aim of making full use of theexisting training dataset at different granularities..3 method.
the overall structure of our msp method is shownin figure 1. the pre-training process is dividedinto three stages based on different granularitiesof text-image correspondence from token, phraseto sentence.
we design corresponding pre-trainingtasks for the three stages..we perform the above three-stage pre-trainingon a simpliﬁed model of lxmert (lxmert-s).
the simpliﬁed process of the lxmert model isshown in figure 2. the cross-modality encoder oflxmert-s is identical to the lxmert.
we obtainthe simpliﬁed lxmert (lxmert-s) by remov-ing the object-relationship encoder and languageencoder.
the image features and text features aredirectly input to the cross-modality encoder in thelxmert-s..by removing the single modal coding layer inlxmert, the 12-layer lxmert is simpliﬁed toa 5-layer lxmert-s. the amounts of parametersin simpliﬁed lxmert-s are only 45.9% of theoriginal model, and the whole experiment can becompleted on a single gpu.
the three-stage pre-training method is also fully applicable to otherpre-training models..3.1 stage 1: word-based pre-training.
the ﬁrst stage of pre-training focuses on learningthe correspondence between text token units andimage objects to help the model mine ﬁne-grainedinformation.
to this end, we design the appropriatepre-training tasks and corresponding dataset forthis phase of pre-training..pre-training tasks we design an image fea-tures random shufﬂe (ifrs) pre-training taskto enhance the pre-training of the token layer,based on the existing masked language model-ing (mlm) (chen et al., 2019), masked region.
2558lxmert-s  data：token-image object (category)      task:    ifrs and others[cls]shirtpan[mask][cls]mlmtowelmug[cls][cls]mrfrrandom shuffleifrsmanshirtpantowelmugmoclxmert-s[cls]pan[mask]yellow[cls]mlmsliver,mrfryellowpanmugsliver,moctithlxmert-s[cls]man[mask]a[cls]mlmadryingmrfrapanmanadryingmoctits[cls][cls]itm_hs[cls][cls]  data：phrase-image object (a ribute)      task:    tith and othersstage2  data：sentence-image object (seman c)  task:    tits and othersstage3stage1based on the text-side category label and informa-tion around the object.
moc predicts the categoryand attribute labels of the masked object features..training data we extract training data for ifrstask from caption-image pairs directly.
for eachimage, 36 object features and their corresponding36 category labels are provided by faster-rcnn.
these category labels have been uniﬁed with thetext vocabulary, so they are all included in the textvocabulary.
during training, the image side inputsthe image features in sequence, and the text side in-puts the category labels in the corresponding order.
in the ifrs task, when the image side is shufﬂed,the order of the text side remains unchanged..3.2 stage 2: phrase-based pre-training.
the previous stage explores the correspondencebetween the image objects and their category.
thisstage mines the correspondence between the im-age object and the phrase describing of the ob-ject.
since the phrase description usually containsricher information about the attributes of the object,such as ”green old car”, building a pre-training taskbased on the correspondence between the phraseand the object allows the model to obtain rich in-formation about the attributes..pre-training tasks we deﬁne a topic of imageand text for phrase (titp) pre-training task thatmore directly supports phrase-based informationmining..topic of image and text for phrase (titp):given a token sequence of image phrase-level de-scription w = {w1, w2, w3.
.
.
wn}, object featuresequence r = {r1, r2, r3.
.
.
rm}, and correspon-dent category label sequence l = {l1, l2, l3.
.
.
lm}extracted by faster-rcnn.
let topic set is topic =w ∩l = {p1, p2.
.
.
pq}, and label set y ={y1, y2.
.
.
yv}, where v is the size of the vocabu-lary.
if yi∈topic, then yi is 1, otherwise yi is 0. weadd a fc layer to the multimodal representation toget sθ(w, r), predict the correct topic from the vo-cabulary size v categories, and use bceloss to cal-culate the gap between the model output sθ(w, r)and the label y..v−1(cid:88).
i=0.
l = e(w,r)∼d [1/v.
(yilogsθ (w,r)+(1−yi)log(1−sθ (w,r)).
(2)other pre-training tasks: we add mlm, mrfrand moc tasks to the phrase-based pre-training.
mlm masks the attribute or category information.
overview of.
figure 2:the simpliﬁed processof lxmert.
we obtained a simpliﬁed lxmert(lxmert-s) by removing the object-relationship en-coder and language encoder in the dotted box andkeeping only the cross-modality encoder..feature regression (mrfr) (chen et al., 2019)and masked object classiﬁcation (moc) (qi et al.,2020)..j = (ri+1, ri+2, ri) = (r[s].
image features random shufﬂe (ifrs): given aset of image regions r = {r1, r2, r3.
.
.
rm}, whichare obtained by adding a fully-connected (fc) layerto the regions of interest (rois) and projectingthem to the hidden size, a feature triplet is threeconsecutive features in r, e.g.
tj = (ri, ri+1, ri+2).
a shufﬂe on a triplet is to randomly change theorder of features in the triplet with a probabil-ity of 5%.
for example, the triplet tj is shufﬂedas t[s], r[s]i+2).theshufﬂed triplet t[s]is used as input for the net-jwork, and the corresponding output is convertedto the dimensionality of rois to obtain hθ(t[s]j ) =(hθ(r[s]i+2)).
the rois extractedby faster-rcnn corresponding to the original tjis fθ(tj) = (fθ(ri), fθ(ri+1), fθ(ri+2)),we use thel2 loss to calculate the distance between the net-work output hθ(t[s]j ) and fθ(tj) as in the followingequation..i+1), hθ(r[s].
i ), hθ(r[s].
i+1, r[s].
i.l = e(w,r)∼d.
[s]||hθ (ri.
)−fθ (ri)||22.
(1).
k=k(cid:88).
i=k(cid:48)+2(cid:88).
k=0.
i=k(cid:48).
where k is the number of shufﬂed triples..other pre-training tasks: we add the existingmlm, mrfr and moc tasks to the token-basedpre-training.
mlm masks the token-level categorylabels of objects with a certain probability p, andthe model predicts the masked category label basedon the corresponding object feature on the imageside.
mrfr masks the object features, and themodel predicts the original object-level features.
2559cross-modality encoder object-relationshipencoderlanguage encoder[cls]tokentokentoken[cls]tokentokentokentoken[cls]token[cls]tits，itm_hs...of the phrase with a certain probability p, and themodel predicts the masked information based onthe corresponding object features.
mrfr masksthe object features of the image, and the model pre-dicts the original object based on the phrase-leveldescription on the text side and the surroundingobject information, and moc predicts the categoryand attribute of the object being masked based onthe surrounding image features and the phrase-leveldescription on the text side..training data: we obtain the correspondingtraining data based on the visual genome (vg) (kr-ishna et al., 2017) dataset, which contains a largenumber of phrases.
we eliminate the phrases con-taining verbs.
the remaining phrases are concate-nated with commas to obtain a phrase-level descrip-tion of the image.
during training, the spliced vgphrase is used as input on the text side and 36 ob-ject features extracted by faster-rcnn are inputon the image side..3.3 stage 3: sentence-based pre-training.
on the basis of the above token and phrase train-ing, this stage uses the overall sentence-image cor-respondence relationship for pre-training to minelarger unit text-image related information..pre-training tasks we design two sentence-level pre-training tasks, image-text matchingbased on hard sample (itm hs) and topic ofimage and text for sentence (tits) described asfollows..image-text matching based on hard sample(itm hs): the purpose of this task is to reduce thenoise brought to the model when the text-imagepair does not match.
we retrieve the top m mostsimilar images for each image from difﬁcult sam-ples ﬁle1 as the hard sample set.
in the itm hstask, each image is replaced with a randomly se-lected hard sample with probability of 50% if thehard sample sets is not empty.
if the set of cur-rent sample is empty, an image in the trainingset is randomly selected.
let the token sequencew = {w1, w2, w3.
.
.
wn} and the image feature se-quence r = {r1, r2, r3.
.
.
rm}, the label y∈{0, 1}indicates whether the input image-text pair matcheseach other.
we apply the fc layer on top of themultimodal representation to get sθ(t, r), whichis the matching score of the image and text..l = e(w,r)∼d[ylogsθ(w,r)+(1−y)log(1−sθ(w,r))].
(3).
1the difﬁcult sample comes from the difﬁcult sample ﬁle.
in vilbert’s image-text retrieval task..topic of image and text for sentence (tits):the purpose of this task is to jointly predictthe content described by both image and sen-tence information.
given a token sequence w ={w1, w2, w3.
.
.
wn}, an image feature sequencer = {r1, r2, r3.
.
.
rm}, category labels for objectfeatures l = {l1, l2, l3.
.
.
lm}, topic = w ∩l ={p1, p2.
.
.
pq}, and label y = {y1, y2.
.
.
yv}, wherev is the size of the vocabulary.
if yi∈topic, thenyi is 1, otherwise yi is 0. we apply the fc layeron top of the multimodal representation, convert itsdimension to the vocabulary size v to get sθ(w, r),and use bceloss to calculate the gap between themodel output sθ(w, r) and the label y..k=k(cid:88).
k=0.
l = e(w,r)∼d [1/v.
(yilogsθ (w,r)+(1−yi)log(1−sθ (w,r))).
(4)other pre-training tasks: we add the existingmlm, mrfr and moc tasks to the sentence-based pre-training.
mlm masks the informationin the sentence and the model predicts the maskedinformation based on the all information on theimage side.
mrfr masks the object features of theimage and the model predicts the original objectbased on the overall information at the sentencelevel on the text side and the surrounding object in-formation.
moc predicts the category and attributeof the masked object based on the image featuresand the text-side sentence-level description..training datain this stage, the image and itscorresponding caption are directly used as input,the sentence level information caption is input onthe text side, and the 36 object features providedby faster-rcnn are input on the image side..4 experiments.
4.1 pre-training dataset.
in this paper, the model is pre-trained using thecoco dataset and part of the vg dataset, andonly 1.08m text-image pairs are used, where0.12m image-text pairs are used in token-basedpre-training stage, 0.34m image-text pairs are usedin phrase-based pre-training stage, and 0.62mimage-text pairs are used in the sentence-basedpre-training stage.
all datasets we used are alsoused in initial lxmert.
table 1 gives a compari-son of the pre-training data, model parameters2 and.
2we exclude the parameters of the word embedding andpre-training task and only count the number of parameters inthe transform part..2560model.
parameter.
training data.
text-image pairs.
resource consumption.
vl-bert.
134.8m.
uniﬁed vlpvilbertlxmertvisualbertours.
-218.9m183.5m85.05m84.3m.
3.3m.
3.3m3.3m9.18m1.28m1.08m.
text corpuswikipediabookscorpus-----.
16 v100 gpus.
8 v100 gpus8 titanx gpus4 titanx gpus-1 titanx gpus.
table 1: comparison of parameter size, training data and resource consumption between the model in this paperand some pre-trained models..computational resources with other models..4.3 baselines.
4.2 downstream tasks and data sets.
visual question answering (vqa): there aremultiple datasets for vqa.
we use three com-mon used datasets: vqa v2.0 (goyal et al.,2017), gqa (hudson and manning, 2019), andnlvr2 (suhr et al., 2019).
accuracy is used as tomeasure model performance..cross-modal retrieval.
task: we chooseflickr30k (young et al., 2014) dataset as the re-trieval task data, and evaluate the performance ofthe model in image retrieval (ir), text retrieval(tr), zero shot image retrieval (zs-ir), and zeroshot text retrieval (zs-tr) respectively, and theperformance metric is the matching score of textand image pairs.
zero shot is to evaluate the per-formance of the pre-trained model directly on thetest set without ﬁne-tuning, and is used to evalu-ate the effect of the pre-trained model.
thereforezs-ir and zs-tr are directly loaded with modelparameters to perform ir and tr tasks withoutﬁne-tuning..in the ﬁne-tuning stage, the multimodal repre-sentation of the model is passed through a fc layeras a joint representation of image and text to solvedownstream tasks.
for vqa tasks, we linearizethe multimodal representation into the answer cat-egory dimension through the fc layer to predictthe answer of each question.
for the image-textretrieval (young et al., 2014) task, we randomlyreplace the image or text, construct three negativeexamples for an image-text pair, including two ran-dom negative examples and a hard sample, and usebceloss to calculate the difference between thematching score and the text-image matching label ..we compare our model with both single-streammultimodal pre-training models including uniﬁedvlp (zhou et al., 2020), visualbert (li et al.,2019) and vl-bert (su et al., 2020) and two-stream models including vilbert (lu et al., 2019)and lxmert (tan and bansal, 2019)..uniﬁed vlp uniﬁed vlp uses a 12 layersof shared multi-layer transformer network forboth encoding and decoding, which differs frommany existing methods where the encoder anddecoder are implemented using separate models.
it conducts pre-training on the conceptual cap-tions(cc) (sharma et al., 2018) which has around3.3 million image-text pairs, and requires 150 hoursof training on the 8x v100 gpus.
uniﬁed vlpincludes only the mlm task when processing thecomprehension tasks..visualbert visualbert contains 12 layers oftransformer with 85.05m parameters.
it ﬁrst pre-trains on coco caption (lin et al., 2014) withmlm and itm tasks and then continues pre-training on the target dataset with mlm task.
thepre-training data sizes for visuabert on the vqav2.0 task are shown in table 1. for different down-stream tasks, the second stage of pre-training needsto be re-trained..vl-bert vl-bert contains 12 layers of trans-former with 134.8m parameters.
it pre-trainson both visual-linguistic and text-only datasets.
samples are randomly drawn from both ccand bookscorpus (zhu et al., 2015) & englishwikipedia (at a ratio of 1:1) in each mini-batch.
vl-bert considers itm to be harmful to down-stream tasks and therefore only includes mlm andmoc tasks..2561vqa v2.0.
gqa.
nlvr2.
model.
test-dev.
uniﬁed vlp 70.5vilbert70.55visualbert 70.871.16vl-bert72.42lxmert71.1(98.18%)ours.
test-std70.770.9271-72.5471.18(98.13%).
test-dev----59.858.7(98.16%).
test-std----60.3359.12(97.99%).
val--67.4-74.974.03(98.84%).
test-p--67-74.574.72(↑ 0.22%).
table 2: lxmert-s results on vqa v2.0, gqa and nlvr2..model.
ir(zero-shot).
tr(zero-shot).
vilbert 31.86 61.12 72.8lxmert 24ours.
r@1 r@5 r@10 r@1 r@5 r@10 r@158.2-57.9(99.4%) 83(97.8%) 88.7(97.0%) 64.6.r@10 r@1 r@5 r@1091.52-.
47.38 58.22 23.6.
-61.381.8.
--90.4.
--87.5.
-51.575.
42.42 68.7.
77.92 49.
--.
-.
irr@584.9-.
tr.
table 3: lxmert-s results on image-text retrieval task..vilbert vilbert extends the popular bertarchitecture to a multi-modal two-stream model,processing both visual and textual inputs in sep-arate streams that interact through co-attentionaltransformer layers.
it trains on cc with mlm,moc and itm tasks..lxmert lxmert has a large-scale trans-that consists of three encodersformer modeland a large-scale pre-training data, including mscoco, visual genome, vqa v2.0, gqa and vg-qa (zhu et al., 2016).
the pre-training requires8.5 days on the 4x titanx gpus.
it also has manypre-training tasks, including mlm, mrfr, moc,itm and image question answering (qa) (tanand bansal, 2019), and has achieved good resultsin downstream tasks, especially vqa tasks..4.4.implementation details.
our transformer backbone is the same aslxmert, where each transformer block has 768hidden units and 12 attention heads.
image fea-tures are extracted by faster-rcnn (ren et al.,2015) model (with resnet-101 (he et al., 2016)backbone) trained on visual genome (vg)..during pre-training, our model is trained forabout 95 hours on 1 titanx gpu, and takesadam (kingma and ba, 2015) as the optimizerwith a learning rate of 1e-5.
we train the token-based model for 10 epochs with a batch size of 64,phrase-based model for 20 epochs with a batch sizeof 128 and sentence-based model for 20 epochswith a batch size of 128..during fine-tuning, the learning rate of all down-stream tasks is 5e-5, and the batch size is 32. weﬁne-tune 6 epochs for vqa v2.0, 5 epochs forgqa, and 8 epochs for nlvr2 and image-textretrieval tasks..for hard samples in itm hs task, we retrievethe top 100 most similar images from difﬁcultsamples ﬁle.
for the masking strategies, we ran-domly mask 15% tokens, 15% object features.
the codes of our models are available at https://github.com/lttsmn/lxmert-s..4.5 experimental results.
table 2 gives the results of the model on the threevqa datasets, and table 3 gives the results ofthe model on the flickr30k image-text retrievaldataset..it can be seen from both table 2 and 3 thatthe pre-training model proposed in this paper hasachieved comparable performances with the ex-isting large models under the condition of lesstraining data, fewer parameters and less comput-ing resource occupation.
in some cases, our smallmodel even outperforms the big one.
for exam-ple, nlvr2 task is 0.22 higher than lxmert ontest-p, and zs-ir is 18.42 higher than lxmert inr@1 under the premise that the model parametersare reduced by 54.1% and the training data set isreduced by 88.24%..4.6 ablation study.
table 4 gives results of lxmert-s on differenttasks with different pre-training setting.
the ﬁrst.
2562stage(s)countnone.
stage(s)usedvanilla.
tasks used.
vqatest dev68.1.gqatest dev55.71.nlvr2test-p51.07.zs-iravg.
iravg55.27 -.
zs-travg.
travg58.07 -.
single.
s.two.
three.
t + p + s.t → s.p → s.t → p → s.s → p → t.p → t → s.70.25.nonemlm mrfr moc titsitm hs69.87- itm hs- itm hs - tits69.79mlm mrfr moc itm 70.1mlm mrfr moc titsitm hs ifrs- ifrsmlm mrfr moc titsitm hs titp- titpmlm mrfr moc titsitm hs ifrs titp- titpmlm mrfr moc titsitm hs ifrs titp.
70.71.
70.54.
70.52.
70.58.
71.01.
69.43.
71.1.mlm mrfr moc titsitm hs ifrs titp.
57.66.
70.23.
73.86 54.64 78.3.
59.73.
57.4857.4757.58.
70.9870.7372.24.
53.3371.46 51.79 75.670.17 49.38 74.150.374.31 59.31 77.87 63.33.
58.39.
73.85.
75.68 61.53 80.27 65.47.
58.4.
73.93.
76.08 60.5.
80.6.
65.03.
57.96.
72.96.
74.81 57.66 79.5.
61.13.
58.17.
71.18.
75.49 59.28 80.4.
62.73.
58.7.
58.3.
74.72.
76.55 63.01 80.83 68.6.
74.48.
76.07 63.07 80.96 67.77.
57.98.
56.75.
71.03 -.
74.87 -.
70.92.
58.05.
73.62.
76.69 61.29 81.63 67.table 4: use vqa, gqa, nlvr2, image-text retrieval (flickr30k) downstream tasks to evaluate the mspmethod and pre-training tasks.
image-text retrieval uses the average value of r@1, r@5, r@10..column gives the number of stage(s) in pre-training.
the second column gives the stage(s) used, where sfor sentence stage, p for phrase stage, and t for to-ken stage, t →s means there are two stages includ-ing token-based pre-training ﬁrst and then sentence-based pre-training.
t →p →s means there arethree stages including token-based pre-training ﬁrstand then phrase-based pre-training and sentence-based pre-training last.
t+p+s means to train allstages together.
the third column gives the pre-training tasks used in the pre-training.
we ﬁrstgive all the pre-training tasks used in the train-ing stages used, then verify the validity of the pre-training tasks by removing a task based on all thepre-training tasks, “-” indicates that a pre-trainingtask is removed..figure 3: vqa examples.
we show the distributionof answers with top 3 scores at different pre-trainingstages..figure 4: image-text retrieval examples.
we show thedistribution of caption with top 1 score at different pre-training stages..from table 4, we can ﬁnd: (1) with the orderlyincrease of the training phase, the performance ofthe model on downstream tasks is gradually im-proving; (2) the training granularity from small tolarge is the most effective training sequence; (3)the pre-training tasks we propose for each stageof pre-training can improve the performance ofthe model on downstream tasks, such as titp im-proves vqa performance by 0.09, gqa perfor-mance by 0.4, nlvr2 performance by 0.24, irperformance by 0.48, and zs-tr by 0.83..5 qualitative analysis.
we visualize the impact of different pre-trainingstages on vqa and image-text retrieval task byshowing the answers probability distribution.
foreach example in figure 3, the left side is the inputimage of the model, and the right side is the prob-ability distribution of the top3 scoring answers indifferent pre-training stages..for image-text retrieval task, we select the top1 caption for visualization.
for each sample infigure 4, the left side is the input image and the.
25630.00.20.60.8yst->synt->p->snz44question: is the horse on a path?
answer: yesn1.00.4yw0.00.20.40.60.81.0twst->sbgt->p->swggt t: tan  w:white  g: gray  b: bluequestion: what color are blankets on this bed?
answer:whitew n: no  y: yes  z: zebra  4: 4 top1:a young man sitting on a rock above abody of water, fishing rod in hand  (yes,score=0.9956) top1:a person with a backpack stands ona rocky bank beside a body of water  (no,score=0.9989) top1:a young man sitting on a rock abovea body of water, fishing rod in hand   (yes,score=0.9899) top1:an old women in pink and wearing hat issqueezing her eyes while looking atsomething  (yes,score=0.9843) top1:one man in a hooded sweatshirt pickingup articles of clothing while a woman in a blueshirt looks on (no,score=0.9982)  top1:a young man sitting on a rock above abody of water, fishing rod in hand  (no,score=0.9899)  st->st->p->sst->st->p->sright side is the highest scoring caption predictedby the model..references.
from both figure 3 and 4, we can ﬁnd: (1)token-based pre-training (s vs t →s) helps themodel to learn object information in the images.
for example, in the left sample in figure 3 and4, the model improves its performance on down-stream tasks by adding token-based pre-trainingthat makes the model focus on object informa-tion such as horses, man and rocks in the images;(2) phrase-based pre-training (t →s vs t →p →s)helps the model to learn information about the at-tributes of the objects.
as shown in right-hand im-age in figure 3 and 4, the model pays attention toattribute information, i.e.
blanket is white, clothesare pink, etc..6 conclusion.
in this paper, inspired by the idea of curriculumlearning, we propose a msp method, which usesinformation at different granularities from word,phrase to sentence in both texts and images topre-train a model in stages, we also design pre-training tasks suitable for each stage of pre-training,ifrs task for word-based pre-training, titp taskfor phrase-based pretraining, and tits task forsentence-based pretraining.
experimental resultson several vqa datasets as well as one cross-modalretrieval dataset show that our method achievessimilar or even better performance than a largermodel in terms of accuracy in all downstream tasksunder the premise that the model parameters arereduced by 54.1% and the training data set is re-duced by 88.24%.
in future work, we will addthe above training method to other simpliﬁed pre-trained models to further explore the effectivenessof msp method..acknowledgments.
we would like to thank anonymous reviewers fortheir suggestions and comments.
the work wassupported by the national natural science founda-tion of china (nsfc62076032), the cooperationpoject with beijing sankuai technology co., ltdand the national key research and developmentprogram of china (2020yff0305302).
we wouldlike to thank dr. huixing jiang and his colleagues..chris alberti, jeffrey ling, michael collins, and davidreitter.
2019. fusion of detected objects in textfor visual question answering.
in emnlp-ijcnlp2019, pages 2131–2140..yoshua bengio, j´erˆome louradour, ronan collobert,and jason weston.
2009. curriculum learning.
inproceedings of the 26th annual international confer-ence on machine learning, pages 41–48..tom b brown, benjamin mann, nick ryder, melaniesubbiah, jared kaplan, prafulla dhariwal, arvindneelakantan, pranav shyam, girish sastry, amandaaskell, et al.
2020. language models are few-shotlearners.
arxiv preprint arxiv:2005.14165..yen-chun chen, linjie li, licheng yu, ahmed elkholy, faisal ahmed, zhe gan, yu cheng, andjingjing liu.
2019. uniter: learning universalimage-text representations.
corr, abs/1909.11740..jacob devlin, ming-wei chang, kenton lee, andkristina toutanova.
2019. bert: pre-training ofdeep bidirectional transformers for language under-standing.
in naacl-hlt 2019, pages 4171–4186..yash goyal, tejas khot, douglas summers-stay,dhruv batra, and devi parikh.
2017. making thev in vqa matter: elevating the role of image un-derstanding in visual question answering.
in cvpr2017, pages 6325–6334.
ieee computer society..kaiming he, xiangyu zhang, shaoqing ren, and jiansun.
2016. deep residual learning for image recog-nition.
in cvpr 2016, pages 770–778.
ieee com-puter society..drew a. hudson and christopher d. manning.
2019.gqa: a new dataset for real-world visual rea-soning and compositional question answering.
incvpr2019,, pages 6700–6709..xiaoqi jiao, yichun yin, lifeng shang, xin jiang,xiao chen, linlin li, fang wang, and qun liu.
2020. tinybert: distilling bert for natural lan-guage understanding.
in emnlp 2020, pages 4163–4174..diederik p. kingma and jimmy ba.
2015. adam: amethod for stochastic optimization.
in iclr 2015..ranjay krishna, yuke zhu, oliver groth, justin john-son, kenji hata, joshua kravitz, stephanie chen,yannis kalantidis, li-jia li, david a. shamma,michael s. bernstein, and li fei-fei.
2017. vi-sual genome: connecting language and vision usingcrowdsourced dense image annotations.
int.
j. com-put.
vis., 123(1):32–73..zhenzhong lan, mingda chen, sebastian goodman,kevin gimpel, piyush sharma, and radu soricut.
2020. albert: a lite bert for self-supervisedlearning of language representations.
in iclr 2020..2564liunian harold li, mark yatskar, da yin, cho-juihsieh, and kai-wei chang.
2019. visualbert: asimple and performant baseline for vision and lan-guage.
corr, abs/1908.03557..hao tan and mohit bansal.
2019. lxmert: learningcross-modality encoder representations from trans-in emnlp-ijcnlp 2019, pages 5099–formers.
5110..ashish vaswani, noam shazeer, niki parmar, jakobuszkoreit, llion jones, aidan n. gomez, lukaszkaiser, and illia polosukhin.
2017. attention is allyou need.
in nips 2017, pages 5998–6008..peter young, alice lai, micah hodosh, and julia hock-enmaier.
2014. from image descriptions to visualdenotations: new similarity metrics for semantic in-ference over event descriptions.
trans.
assoc.
com-put.
linguistics, 2:67–78..oﬁr zafrir, guy boudoukh, peter izsak, and moshewasserblat.
2019. q8bert: quantized 8bit bert.
corr, abs/1910.06188..luowei zhou, hamid palangi, lei zhang, houdonghu, jason j. corso, and jianfeng gao.
2020. uni-ﬁed vision-language pre-training for image caption-ing and vqa.
in aaai 2020, pages 13041–13049.
aaai press..yuke zhu, oliver groth, michael s. bernstein, andli fei-fei.
2016. visual7w: grounded questionanswering in images.
in cvpr 2016, pages 4995–5004. ieee computer society..yukun zhu, ryan kiros, richard s. zemel, ruslansalakhutdinov, raquel urtasun, antonio torralba,and sanja fidler.
2015. aligning books and movies:towards story-like visual explanations by watchingmovies and reading books.
corr, abs/1506.06724..xiujun li, xi yin, chunyuan li, pengchuan zhang, xi-aowei hu, lei zhang, lijuan wang, houdong hu,li dong, furu wei, yejin choi, and jianfeng gao.
2020. oscar: object-semantics aligned pre-trainingfor vision-language tasks.
in eccv 2020, volume12375, pages 121–137..tsung-yi lin, michael maire, serge j. belongie, jameshays, pietro perona, deva ramanan, piotr doll´ar,and c. lawrence zitnick.
2014. microsoft coco:common objects in context.
in eccv 2014, volume8693, pages 740–755.
springer..jiasen lu, dhruv batra, devi parikh, and stefanlee.
2019. vilbert: pretraining task-agnostic visi-olinguistic representations for vision-and-languagetasks.
in neurips 2019, pages 13–23..paul michel, omer levy, and graham neubig.
2019.in.
are sixteen heads really better than one?
neurips 2019, pages 14014–14024..vicente ordonez, girish kulkarni, and tamara l. berg.
2011. im2text: describing images using 1 millioncaptioned photographs.
in nips 2011, pages 1143–1151..di qi, lin su,.
jia song, edward cui, taroonbharti, and arun sacheti.
2020. imagebert: cross-modal pre-training with large-scale weak-supervisedimage-text data.
corr, abs/2001.07966..alec radford, karthik narasimhan, tim salimans, andimproving language under-.
ilya sutskever.
2018.standing by generative pre-training (2018)..alec radford, jeffrey wu, rewon child, david luan,dario amodei, and ilya sutskever.
2019. languagemodels are unsupervised multitask learners.
openaiblog, 1(8):9..shaoqing ren, kaiming he, ross b. girshick, and jiansun.
2015. faster r-cnn: towards real-time objectin nipsdetection with region proposal networks.
2015, pages 91–99..piyush sharma, nan ding, sebastian goodman, andradu soricut.
2018.conceptual captions: acleaned, hypernymed, image alt-text dataset for auto-matic image captioning.
in acl 2018, pages 2556–2565. association for computational linguistics..weijie su, xizhou zhu, yue cao, bin li, lewei lu,furu wei, and jifeng dai.
2020. vl-bert: pre-training of generic visual-linguistic representations.
in iclr 2020..alane suhr, stephanie zhou, ally zhang, iris zhang,huajun bai, and yoav artzi.
2019. a corpus forreasoning about natural language grounded in pho-tographs.
in acl 2019,, pages 6418–6428.
associa-tion for computational linguistics..2565