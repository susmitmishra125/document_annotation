all that’s ‘human’ is not gold:evaluating human evaluation of generated text.
elizabeth clark1.
tal august1suchin gururangan1.
soﬁa serrano1.
nikita haduong1.
noah a. smith1,2.
1paul g. allen school of computer science & engineering, university of washington2allen institute for artiﬁcial intelligence{eaclark7,taugust,sofias6,qu,sg01,nasmith}@cs.washington.edu.
abstract.
human evaluations are typically consideredthe gold standard in natural language gener-ation, but as models’ ﬂuency improves, howwell can evaluators detect and judge machine-generated text?
we run a study assessing non-experts’ ability to distinguish between human-and machine-authored text (gpt2 and gpt3)in three domains (stories, news articles, andrecipes).
we ﬁnd that, without training, evalua-tors distinguished between gpt3- and human-authored text at random chance level.
weexplore three approaches for quickly trainingevaluators to better identify gpt3-authoredtext (detailed instructions, annotated examples,and paired examples) and ﬁnd that while eval-uators’ accuracy improved up to 55%, it didnot signiﬁcantly improve across the three do-mains.
given the inconsistent results acrosstext domains and the often contradictory rea-sons evaluators gave for their judgments, weexamine the role untrained human evaluationsplay in nlg evaluation and provide recom-mendations to nlg researchers for improv-ing human evaluations of text generated fromstate-of-the-art models..figure 1: excerpts from human evaluators’ explana-tions for why they believe a gpt3-generated story (alsoexcerpted) was written by a human (left) or a machine(right).
the evaluators point to a wide range of textattributes to make their decisions, sometimes using thesame aspect of the text to come to opposite conclusions..1.introduction.
human-quality text has long been a holy grail forthe output of natural language generation (nlg)systems, serving as an upper bound on their per-formance.
since we lack a good way of encodingmany aspects of what constitutes human-qualityoutput in an automated method, we often must relyon human evaluation for our models.
though eval-uations with end-users in an applied setting are en-couraged (belz and reiter, 2006), in practice, mosthuman evaluations instead ask people to rate gener-ated text’s intrinsic quality (van der lee et al., 2019;howcroft et al., 2020).
sometimes the generatedtext is explicitly compared to human-authored text(e.g., liu et al., 2016; zellers et al., 2021; zhang.
et al., 2020), but even when no human-authoredtext is evaluated, evaluators implicitly compare thegenerated text to their knowledge of language andnorms within speciﬁc domains..evaluators are often asked to assess a text holis-tically, e.g., based on its overall quality, natural-ness, or humanlikeness (van der lee et al., 2021;howcroft et al., 2020), where the exact evaluationcriteria is left to the discretion of the evaluator.
though other evaluations are broken down alongspeciﬁc dimensions of text quality (e.g., grammat-icality, coherence, etc.
), novikova et al.
(2017,2018) and callison-burch et al.
(2007) found thatthese dimensions are often correlated and may beconﬂated in some evaluation settings.
this is con-.
proceedingsofthe59thannualmeetingoftheassociationforcomputationallinguisticsandthe11thinternationaljointconferenceonnaturallanguageprocessing,pages7282–7296august1–6,2021.©2021associationforcomputationallinguistics7282cerning because, as nlg models improve, eval-uators are asked to read longer passages of textconditioned on large amounts of context.
in thesecases, ﬂuency-related aspects of quality (i.e., theones that don’t require careful reading of the con-text and meaning of the passage) are the easiest toassess, particularly in small-batch evaluations withnon-expert evaluators where speed is incentivized.
this poses a challenge when collecting human eval-uations for state-of-the-art language models, as er-rors are often content-based (e.g., factual inaccu-racies or inconsistencies with the context) ratherthan ﬂuency-based (brown et al., 2020), so a su-perﬁcial read may not be sufﬁcient to catch modelerrors.
for accurate assessments of generated text,we need human evaluations that are designed toencourage a sufﬁciently careful reading of the textto examine these subtler aspects of text quality..we asked non-expert evaluators to assess thehumanlikeness (operationalized as how believablyhuman an evaluator ﬁnds a text) of text generatedby current nlg models (gpt2 and gpt3) to testwhat current human evaluation practices can revealabout the models’ quality (§2).
we found that eval-uators were unable to distinguish between gpt3-and human-authored text across story, news, andrecipe domains.
however, when we categorizedthe aspects of text the evaluators used to make theirjudgments, we found they primarily focused onthe grammar, spelling, and style of the text.
theevaluators’ responses also indicated that they un-derestimated the quality of text current models arecapable of generating (as seen in figure 1).
to ourknowledge, this paper is the ﬁrst to evaluate humanevaluations of gpt3-generated text across multipledomains..we then looked at three different evaluatortraining methods—providing detailed instructions,annotated examples, and human-machine pairedexamples—to test whether we could improve eval-uators’ accuracy (§3).
while we found includingexamples in the task increased the set of texts eval-uators thought could be machine-generated andincreased their focus on textual content, no trainingmethod signiﬁcantly increased evaluators’ perfor-mance consistently across domains..based on our results (discussed in §4), we recom-mend moving away from small-batch evaluationswith little training when collecting human evalua-tions of nlg models (§5).
we also encourage prac-titioners to consider alternative evaluation frame-.
works that capture the usefulness of generated textin downstream settings rather than its humanlike-ness..2 how well can untrained evaluatorsidentify machine-generated text?.
in our ﬁrst study, we ask how well untrained evalu-ators can distinguish between human- and machine-generated text.
this task format, inspired by theturing (1950) test, is used to compare the qualityof machine-generated text to human-authored textand, as models’ ﬂuency improves, to analyze nlgmodels’ ability to “fool” readers (ippolito et al.,2020; brown et al., 2020)..by asking evaluators to assess the humanlikenessof the text with only minimal instructions (see fig-ure 2), we observe how well untrained evaluatorscan detect state-of-the-art machine-generated textand which attributes evaluators focus on and thinkare important for detecting machine-generated text..2.1 the task.
we gave evaluators 5 text passages, some of whichwere written by people and some generated by amodel.
we asked them to rate the text on a 4-pointscale (ippolito et al., 2020):.
1. deﬁnitely human-written2. possibly human-written3. possibly machine-generated4. deﬁnitely machine-generated.
if they selected option 1, we asked them: “why didyou select this rating?” otherwise, they were asked,“what would you change to make it seem morehuman-like?” the interface is shown in figure 2..2.2 data.
we considered human- and machine-generated textin three different domains: stories, news articles,and recipes.
in all three cases, we collected 50human-authored texts in english and generated 50texts from both the 175b parameter gpt3 model(also known as davinci; brown et al., 2020)1 andgpt2-xl (radford et al., 2019).2 evaluators wereassigned to one domain and one model; the textsread by any given evaluator included some human-authored texts and some texts generated by their as-signed model.
we only considered texts 100 words.
1beta.openai.com/2huggingface.co/gpt2-xl.
7283a time (255 stories total) and randomly chose 50human-authored stories from this set.
for themachine-generated text, we conditioned the mod-els on the three priming texts and on the phraseonce upon a time.
we removed generated storiesthat directly copied a priming text (with > 80%overlap) and regenerated those texts (9 instanceswith gpt2, 2 with gpt3)..this is the most open-ended of the three domains,as the story’s content is virtually unrestricted, andthe only creative domain.
it is also the noisiest ofthe human-authored datasets, as the stories wereoriginally collected from social media commentswith no quality-based ﬁltering..2.2.2 news articles.
we collected 2,111 recent local news articles from15 different newspapers using newspaper3k5 (de-tails in appendix a.1).
after ﬁltering out articlesunder 100 words, we manually ﬁltered out articlesthat weren’t local news or that referenced the coro-navirus pandemic.
we randomly chose 50 articlesto use as our human-authored news articles andanother 50 to use as prompts for our generationmodels.
we conditioned each generated text on theheadline and ﬁrst sentence from the prompt articles,along with the three priming texts..because the title and the ﬁrst sentence of a newsarticle often summarize its contents, the generatedcontent must adhere to the topics they introduce.
by using local, recent news, we also limit the mod-els’ ability to copy from their training data.
themodels seemed to have the most trouble with thisdataset structurally, e.g., generating new headlineswithout ending the current article or outputting in-valid end-of-ﬁle tags..2.2.3 recipes.
we collected 50 human-authored recipes from therecipenlg dataset (bie´n et al., 2020), which con-tains 2,231,142 recipes scraped from the web.
werandomly chose an additional 50 recipes and usedtheir titles and ingredient lists as prompts, append-ing them to the end of the priming texts..this is the most closed of the three domains, asthe recipe must incorporate the listed ingredientsand result in the dish described by the title.
recipesare typically written in clear commands, leavinglittle room for surprising or unexpected text..5github.com/codelucas/newspaper.
figure 2: the task interface (story domain).
or longer, and after reaching 100 words, all textswere truncated at the end of the next sentence.3.
to generate text, we used the “three-shot” set-ting described in brown et al.
(2020), conditioningthe text on three additional samples of in-domain,human-authored text, which we refer to as the prim-ing texts (all priming texts are in the supplemen-tary materials and at ark.cs.washington.edu/human_evals_acl21).
while this setting is not typ-ically how gpt2 is used in practice, we held thisapproach constant to directly compare how modelquality changes evaluators’ ability to distinguishbetween texts.
for each domain, each generatedtext was conditioned on the same set of primingtexts.
the texts were delimited with an (cid:104)eos(cid:105)token and generated using the default gpt3 gen-eration settings (i.e., sampling with temperature= 0.7)..2.2.1 stories.
the human-authored texts came from the redditwritingprompts dataset (fan et al., 2018).4 wecollected all the stories that began with once upon.
3using nltk; www.nltk.org/4github.com/pytorch/fairseq/tree/.
master/examples/stories.
72842.3 participants.
we used amazon mechanical turk (amt) to col-lect the text evaluations with non-expert evaluators,commonly used in nlg evaluations (van der leeet al., 2019).
to have adequate power in our analy-ses (based on a power analysis with β = 0.8; cardet al., 2020), we had 130 different evaluators foreach of the 6 task settings (3 domains × 2 models).
each participant evaluated 5 texts each, giving us atotal of 780 participants and 3,900 text evaluations.
we paid evaluators us$1.25 for completingthe task.
following common best practice onamt (berinsky et al., 2012), evaluators had tohave over a 95% acceptance rate, be in the unitedstates, and have completed over 1,000 hits (amttasks).
we excluded evaluators’ work if their ex-planations were directly copied text from the task,did not match their responses, did not follow theinstructions, or were short, vague, or otherwise un-interpretable.
across experiments, 445 participants(18.6%) were rejected and not included in the §2results (780 approved participants) and §3 results(1,170 approved participants)..2.4 results.
overall, evaluators choosing between human andgpt2-generated text correctly identiﬁed the authorof the text 57.9% of the time,6 but the evaluatorschoosing between human- and gpt3-generatedtext only guessed correctly 49.9% of the time (ta-ble 1), compared to 50% random chance.
while theaccuracy of classifying gpt2- vs. human-authoredtext is signiﬁcantly7 different from chance, evalu-ators’ accuracy distinguishing gpt3- and human-authored text is not.8 this remains the case regard-less of text domain; we failed to ﬁnd any evidencethat evaluators’ accuracy on any one domain forgpt3 differs from the overall gpt3 accuracy of≈ 50%.9 the story texts saw the biggest dropin evaluator accuracy from gpt2 to gpt3 (62%to 48%, cohen’s d = 0.57).
the distribution ofevaluators’ scores are shown in appendix a.2..in table 1, we see other statistics worsen as wellbetween gpt2 and gpt3: how well evaluatorsidentiﬁed the machine-generated text (f1, preci-sion, and recall), evaluators’ agreement (krippen-dorff’s α, a measure of annotator agreement that.
6unless otherwise noted, all analyses binned the responses.
into 2 categories (human and machine)..7t388 = 6.58, p < 0.00018t388 = −0.09, p = 0.939anova with f2,390 = 0.78, p = 0.46.corrects for the probability of random agreement),and the percent of guesses that the text was human-written (% human).
given that the texts are equallylikely to be human- and machine-written, there aredisproportionately many human guesses, makingup two thirds of the responses in the gpt3 ex-periments.
despite the signiﬁcantly lower scores,evaluators’ conﬁdence (the percent of deﬁnitely re-sponses) remains fairly constant across conditions..2.5 analysis.
taken on its own, the evaluators’ difﬁculty identify-ing gpt3-generated text compared to gpt2 pointsto the improvement of new nlg models.
however,it also points to concerns about extending currenthuman evaluation methodologies to state-of-the-art text generation.
in particular, the evaluators’explanations reveal underlying confusion and mis-conceptions about state-of-the-art nlg..to better understand what untrained evaluatorsfocused on in the text to make their decisions, theauthors annotated 150 random responses from theevaluators who distinguished between human- andgpt3-generated text (see appendix a.3 for annota-tion details).
we divided the text annotation labelsinto three categories: form, content, and machinecapabilities.
form qualities focus on the format,style, and tone of the text, while content focuses onthe text’s meaning.
we also coded for commentsthat explicitly referenced people’s perceptions ofwhat types of language machines are capable (orincapable) of generating (machine capabilities)..we found nearly twice as many comments aboutthe form of the text than the content (form: 47%of labels, content: 25%).
evaluators in our samplefocused most on the spelling, grammar, or punctua-tion of the texts (45 out of 150 comments) and thestyle or tone of the text (24 out of 150 comments).
however, these dimensions of text are unlikely tobe helpful in identifying text generated by currentmodels, considering that gpt3 has already beenshown to generate ﬂuent text and to adapt easily tonew generation domains (brown et al., 2020)..we also found that the reasons evaluators gavefor their answers often contradicted each other.
theformality of the text, spelling and grammar errors,and clarity were all cited to justify both humanand machine judgments.
this was also reﬂected inthe low agreement scores between evaluators, withkrippendorff’s α ≈ 0 across domains..evaluators’ expectations about what nlg mod-.
7285model.
overallacc..gpt2.
*0.58.gpt3.
0.50.domain.
acc..f1.
prec.
recall kripp.
α.storiesnewsrecipes.
storiesnewsrecipes.
*0.62*0.570.55.
0.480.510.50.
0.600.520.48.
0.400.440.41.
0.640.600.59.
0.470.540.50.
0.560.470.40.
0.360.370.34.
%human.
%conﬁdent.
0.100.090.03.
0.030.050.00.
55.2360.4665.08.
62.1565.5466.15.
52.0051.3850.31.
47.6952.4650.62.table 1: §2 results, broken down by domain and model, along with the f1, precision, and recall at identifyingmachine-generated text, krippendorff’s α, % human-written guesses, and % conﬁdent guesses (i.e., deﬁnitelymachine- or human-authored).
* indicates the accuracies signiﬁcantly better than random (two-sided t-test, forbonferroni-corrected p < 0.00333)..els are capable of ranged from thinking their textis already indistinguishable from human-authoredtext (“i have no idea if a human wrote anythingthese days.
no idea at all.”) to doubting machines’ability to use basic language (“usually ai has ter-rible grammer [sic] and messes up.”).
but overallwe found most evaluators’ beliefs about generatedlanguage underestimated or misunderstood currentnlg models, as seen in appendix a.4..3 can we train evaluators to betteridentify machine-generated text?.
given evaluators’ inability to distinguish gpt3-and human-authored text and their inconsistent rea-soning for their decisions, we investigated whetherthere were simple ways of improving evaluators’ability to spot attributes of gpt3-generated text.
inspired by crowdsourcing research on guidingworkers on writing or other subjective tasks (kimet al., 2017; mitra et al., 2015), we tested threelightweight evaluator-training methods to see if wecould improve people’s ability to identify machine-generated text while maintaining the short, low-cost nature of the evaluations..3.1 evaluator training methods.
we considered 3 evaluator trainings that can beadded to the beginning of a human evaluation task,at most requiring only 3 extra samples of human-and machine-generated text.
to test the effective-ness of each type of training, we re-ran the ex-periments from §2, but this time, we prependedone of three evaluator-training methods to theevaluation task: an instruction-based training, anexample-based training, and a comparison-basedtraining.
screenshots of the training interfaces.
are in appendix a.6; the full set of training ma-terials are in the supplementary materials and atark.cs.washington.edu/human_evals_acl21..other than the training, the task setup was iden-tical to the gpt3-based tasks in §2.
we againran the task on amazon mechanical turk acrossthree domains (stories, news, and recipes), usingthe same texts.
as each individual participant wasonly permitted to complete one set of evaluations,the set of evaluators who received these trainingswas completely disjoint from the set of evaluatorsfrom our ﬁrst study.
the participants were subjectto the same restrictions described in §2.3 and ex-cluded according the same criteria; we did not usethe trainings to ﬁlter out evaluators.
for each do-main and training method pair, we had 130 uniqueevaluators complete the task, giving us 5,850 textannotations from 1,170 evaluators..3.1.1 training with instructions.
to give evaluators a better sense of which parts ofthe text to pay attention to, we extended the origi-nal task instructions to include dimensions of thetext that could be helpful for identifying machine-generated text (repetition and factuality) and onesthat could be misleading (grammar, spelling, andstyle).
we chose these dimensions based on pre-vious work (ippolito et al., 2020) and evaluators’comments in a pilot study (see appendix a.5)..the instructions training was the simplest ofour 3 evaluator training methods.
it was generalenough to be applied across the 3 domains butprovided little information about the quality anddomain of text the evaluator would be rating.
itdid not increase the cost of collecting evaluations(us$1.25 per hit) because it does not require anyextra work on the part of the evaluator, though this.
7286also made it the easiest training to ignore.
theinstruction-based training is the most prescriptiveof the training methods, as the researcher has tochoose the dimensions they want the evaluators tofocus on..3.1.2 training with examples.
our examples training consisted of 3 practicerounds of the actual task: given a text, guess ifit is machine- or human-authored.
we collected 3additional texts in the same manner described in§2.2 and wrote a short explanation of which aspectsof the text hinted at its source.
after an evaluatormakes their guess, the correct answer and explana-tion are shown.
each domain had its own set ofexamples and explanations..by showing examples, this training helps setthe evaluators’ expectations about the quality ofthe human- and machine-generated text.
we paidevaluators more for completing this task (us$1.75per hit) to compensate for the extra texts theyneeded to read.
as with the instruction-based train-ing, while pointing out speciﬁc text dimensions canhelp evaluators focus on important features, it mayalso restrict their search space..3.1.3 training with comparison.
in the comparison training, we took the examplepassages from the examples training and pairedthem with a text from the opposite source (machineor human) that began with the same prompt.
weasked evaluators to guess which of the two textswas the machine-generated one.
we then providedthe correct answer to the evaluator, along with thesame explanations used in the examples training.
this training allows evaluators to directly com-pare human and machine texts written from thesame prompt.
it is also the most expensive train-ing, as it required evaluators to read three morepassages than the examples training; we paid eval-uators us$2.25 per hit..increase over the no-training accuracy when consid-ering any of the three domains individually.
evenso, the signiﬁcant difference in overall performanceis mainly contributed by the story domain; whencomparing evaluators’ performance with no train-ing to its examples training counterpart, we seea change of 0.019 and 0.062 mean accuracy inthe news and recipe domains, respectively, versus0.086 on the story domain.
this is perhaps due tothe examples helping override the preconceptionthat machines cannot generate “creative” text..across all 3 domains, the examples and com-parison trainings produced the highest recall andf1 scores for evaluators’ judgments and decreasedthe percentage of texts they guessed were human-written, which indicate that evaluators were willingto consider a broader set of texts to be machine-generated than the evaluators in §2.
however, de-spite the trainings and the increased proportionof conﬁdent responses, evaluator agreement re-mained low across domain and training settings(α ≤ 0.11), and higher agreement did not corre-spond to higher accuracy..3.3 analysis.
we again annotated 150 comments along the di-mensions listed in appendix a.3, divided intoform, content, and machine capabilities categories,this time from evaluators who received the best-performing examples training.
as shown in ta-ble 3, we found that the proportion of form com-ments dropped in the sample of evaluators whowent through the examples training, while the pro-portion of content comments doubled.
we alsosaw a drop in the number of comments mention-ing evaluators’ expectations of machine-generatedtext.
while this change in focus doesn’t necessarilycorrespond to correct judgments, content reasonsare more in-line with current nlg model capabili-ties (brown et al., 2020)..3.2 results.
4 discussion.
we found that while all 3 training methods im-proved evaluators’ accuracy at identifying machine-vs. human-authored text over the no-training accu-racy, the examples training was the only one thatshowed signiﬁcant improvement (see table 2).10.breaking down the results by domain, however,we ﬁnd the examples accuracy did not signiﬁcantly.
10tukey’s hsd adjusted p < 0.003 for distinguishing be-.
tween the examples training and no training, d = 0.25.overall, none of our three training methods sig-niﬁcantly improved evaluators’ ability to detectmachine-generated text reliably across text do-mains while still maintaining the small-batch na-ture of amazon mechanical turk.
this speaksto the improving quality of nlg models, but wealso found that untrained evaluators mainly fo-cused on the format of the text, deciding if it washuman or machine-generated based on whether.
7287training.
domain acc..f1.
prec.
recall kripp.
α.overallacc..%human.
%conﬁdent.
none.
0.50.instructions.
0.52.examples.
*0.55.comparison.
0.53.storiesnewsrecipes.
storiesnewsrecipes.
storiesnewsrecipes.
storiesnewsrecipes.
0.480.510.50.
0.500.560.50.
0.570.530.56.
0.560.520.51.
0.400.440.41.
0.450.480.41.
0.550.480.56.
0.560.510.49.
0.470.540.50.
0.490.550.52.
0.580.520.61.
0.550.530.52.
0.360.370.34.
0.420.430.33.
0.530.450.51.
0.570.480.46.
0.030.050.00.
0.110.050.07.
0.060.050.06.
0.070.080.06.
62.1565.5466.15.
57.6962.7767.69.
53.6958.0055.23.
48.4653.8554.31.
47.6952.4650.62.
45.5452.1549.85.
64.3165.6964.00.
56.6250.3153.54.table 2: §3 results, broken down by domain and training method, along with the f1, precision, and recall atidentifying machine-generated text, krippendorff’s α, % human-written guesses, and % conﬁdent guesses (i.e.,deﬁnitely machine- or human-authored).
“none” training refers to the gpt3 results from §2.
* indicates accuraciessigniﬁcantly better than none (no training; two-sided t-test, for bonferroni-corrected p < 0.00333)..training.
form content.
machinecapabilities.
noneexamples.
47.132.5.
24.650.0.
28.317.5.table 3: % of annotation labels that reference the text’sform and content and the evaluator’s perception of ma-chines’ capabilities.
the text was grammatically or stylistically correct.
this, combined with the high percentage of hu-man guesses, the low recall scores for the machineguesses, and the evaluators’ comments on their ex-pectations of nlg models, indicates a systematicunderestimation by the evaluators of the qualityof machine-generated text.
evaluators who weretrained with examples had higher expectations ofmachine-generated text and focused more on thetext’s content; however, the training was not sufﬁ-cient to signiﬁcantly raise evaluators’ scores acrossall three domains..many of the explanations given by evaluatorsincluded references to the text that reﬂected humanattributes or intent that they suspected machinescould not generate (e.g., “personal description amachine wouldn’t understand, [like a pirate] want-ing to be home with his wife and son” from fig-ure 1 and the examples in appendix a.4).
how-ever, current nlg models are capable of generating.
text with at least superﬁcial reference to human at-tributes or intent, as seen in the generated storyin figure 1. this assumption that machines can’tgenerate text with these aspects of humanlikenessled many evaluators astray, and we suspect it is onecause of the low accuracy we found..crowdsourcing studies dealing only with human-authored texts often include extensive training,quality checks, or coordination (kittur and kraut,2008; kim et al., 2017; bernstein et al., 2010).
nlg evaluations usually forego such structures,based, we suspect, on the assumption that evaluat-ing machine-generated text requires only ﬂuencyin the language the text is generated in.
our re-sults suggest otherwise.
evaluators often mistookmachine-generated text as human, citing superﬁcialtextual features that machine generation has sur-passed (brown et al., 2020).
one potential remedyfor this is to focus evaluator training on debunkingthis misconception.
we did see evidence that theincrease in accuracy we saw with our examplestraining was associated with fewer explanationsmistakenly referencing machine capabilities, eventhough the training did not speciﬁcally focus onthis..5 recommendations.
based on our ﬁndings, if nlg researchers mustrun human evaluations as small-batch evaluations.
7288on amazon mechanical turk or similar platforms,we recommend they train evaluators with examples.
this will help calibrate the evaluators’ expectationsof generated text and indicate the careful readingthey may need to do to properly assess the text’squality.
our experiments also indicate the impor-tance of conﬁrming with evaluators why they havemade the decisions they have, as the criteria theymight implicitly be evaluating may be mismatchedwith researchers’ intended criteria.
however, otherevaluation setups may be more successful on ama-zon mechanical turk, such as long-term evalu-ations with qualiﬁed evaluators who have gonethrough an extended training (like those in kitturand kraut, 2008; zellers et al., 2019a) or third-party evaluator quality tools (e.g., positly, used bybrown et al., 2020)..however, given the increasing length of textnlg models can handle and the careful readingneeded to detect many errors in generated text, weencourage nlg researchers to move away fromstandalone, intrinsic human evaluation tasks.
wefound that, by default, our evaluators in this evalu-ation setting were most likely to focus on surface-level, ﬂuency-related aspects of quality.
we joinpast work (belz and reiter, 2006; van der lee et al.,2021) in recommending a move towards evaluationsettings where evaluators are better motivated tocarefully consider the content and usefulness ofgenerated text.
for example, turingadvice (zellerset al., 2021) asks evaluators to rate nlg models bytheir ability to generate helpful advice, and roft(dugan et al., 2020) engages evaluators through aguessing game to determine the boundary betweenhuman- and machine-generated text.
other evalua-tion methods ask the evaluators to directly interactwith the generated text; for example, choose yourown adventure (clark and smith, 2021) and sto-rium (akoury et al., 2020) evaluate story generationmodels by having people write stories with the helpof generated text.11 we see that gpt3 can success-fully mimic human-authored text across severaldomains, renewing the importance of evaluationsthat push beyond surface-level notions of qualityand consider whether a text is helpful in a down-.
11note that we initially tried a fourth training conditionalong these lines, where we asked evaluators to directly inter-act with the generated text by rewriting it be more humanlike.
we found we were unable to successfully recruit evaluators tocomplete this task.
the rate of retention was less than 30%,and the rejection rate was over 50%.
we found amt was nota good platform for this type of task, at least not for the formatand the price point we explored in this work..stream setting or has attributes that people wouldwant from machine-generated text..finally, given the mixed effect we found dif-ferent trainings can have on evaluators’ perfor-mance and the lack of human evaluation detailstypically presented in nlg papers (van der leeet al., 2019; howcroft et al., 2020), we encouragenlg researchers to include details of any instruc-tions and training they gave evaluators in their pub-lications.
this, along with efforts to standardize hu-man evaluation design (belz et al., 2020; howcroftet al., 2020) and deployment (khashabi et al., 2021;gehrmann et al., 2021), will support future devel-opment of evaluator training procedures and thecomparison of human evaluation results in futurenlg evaluation work..6 related work.
a subﬁeld of nlg analyzes the role of human eval-uations, including discussions of the tradeoffs ofhuman and automatic evaluations (belz and re-iter, 2006; hashimoto et al., 2019).
there are cri-tiques and recommendations for different aspectsof human evaluations, like the evaluation design(novikova et al., 2018; santhanam and shaikh,2019), question framing (schoch et al., 2020), andevaluation measures like agreement (amidei et al.,2018), as well as analyses of past nlg papers’ hu-man evaluations (van der lee et al., 2021; howcroftet al., 2020).
additionally, crowdsourcing litera-ture has work on effectively using platforms likeamazon mechanical turk (e.g., daniel et al., 2018;oppenheimer et al., 2009; weld et al., 2014; mitraet al., 2015).
in this work, we focus on the roleevaluator training can play for producing betteraccuracy at distinguishing human- and machine-generated text, though other quality control meth-ods are worth exploring..previous work has asked evaluators to distin-guish between human- and machine-authored text;for example, ippolito et al.
(2020) found thattrained evaluators were able to detect open-endedgpt2-l-generated text 71.4% of the time andbrown et al.
(2020) found evaluators could guessgpt3-davinci-generated news articles’ source with52% accuracy, though these results are not directlycomparable to ours due to differences in the evalu-ation setup, data, and participants..finally, our ﬁndings that untrained evaluators arenot well equipped to detect machine-generated textpoint to the importance of researching the safe de-.
7289ployment of nlg systems.
gehrmann et al.
(2019)proposed visualization techniques to help readersdetect generated text, and work like zellers et al.
(2019b), ippolito et al.
(2020), and uchendu et al.
(2020) investigated large language models’ abilityto detect generated text..7 conclusion.
we found that untrained evaluators were unable todistinguish between human- and gpt3-generatedtext from three domains.
however, we also foundthat the evaluators focused on surface-level textqualities to make these decisions and underesti-mated current nlg models’ capabilities.
we exper-imented with three methods for training evaluators,and while example-based trainings led to increasesin recall and the amount of content-based evalua-tions, they did not lead to signiﬁcant improvementsin accuracy across all domains.
given that evalua-tors struggled to distinguish between human- andmachine-generated text in this setting, we shouldshift how we think about collecting human evalua-tions for current nlg models..acknowledgments.
this research was supported in part by the ofﬁce ofnaval research under the muri grant n00014-18-1-2670. the authors would like to thank katharinareinecke, the members of the cse 599 crowdsourc-ing class, and the ark group for their feedback,the reviewers for their helpful comments, and theparticipants who took part in our study..ethical considerations all experiments in thispaper were approved by our institution’s internal re-view board.
evaluators’ responses were collectedand stored anonymously.
evaluators were paidbased on an estimated us$10 per hour rate; weraised the price of the task in proportion to theadded difﬁculty of our 3 training methods.
for eachdataset we considered, its source and language areincluded, along with any other details we believedwould be relevant to evaluators’ ability to read andunderstand the text.
evaluators were warned aboutpossible risks before starting the task, namely thatnlg models can generate text with harmful lan-guage or themes, and were able to leave commentsabout their experience at the end of the study..references.
nader akoury, shufan wang, josh whiting, stephenhood, nanyun peng, and mohit iyyer.
2020. sto-rium: a dataset and evaluation platform formachine-in-the-loop story generation.
in proceed-ings of the 2020 conference on empirical methodsin natural language processing (emnlp), pages6470–6484, online.
association for computationallinguistics..jacopo amidei, paul piwek, and alistair willis.
2018.rethinking the agreement in human evaluation tasks.
in proceedings of the 27th international conferenceon computational linguistics, pages 3318–3329,santa fe, new mexico, usa.
association for com-putational linguistics..anja belz and ehud reiter.
2006. comparing auto-matic and human evaluation of nlg systems.
in11th conference of the european chapter of theassociation for computational linguistics, trento,italy.
association for computational linguistics..anya belz, simon mille, and david m. howcroft.
2020. disentangling the properties of human eval-uation methods: a classiﬁcation system to supportcomparability, meta-evaluation and reproducibilityin proceedings of the 13th internationaltesting.
conference on natural language generation, pages183–194, dublin, ireland.
association for computa-tional linguistics..adam j. berinsky, gregory a. huber, and gabriel s.lenz.
2012. evaluating online labor markets forexperimental research: amazon.com’s mechanicalturk.
in political analysis, volume 20, pages 351–368. cambridge university press..michael bernstein, greg little, robert miller, bj¨ornhartmann, mark ackerman, david karger, davidcrowell, and katrina panovich.
2010. soylent: aword processor with a crowd inside.
in uist 2010- 23rd acm symposium on user interface softwareand technology, volume 58, pages 313–322..michał bie´n, michał gilski, martyna maciejewska,wojciech taisner, dawid wisniewski, and ag-nieszka lawrynowicz.
2020. recipenlg: a cook-ing recipes dataset for semi-structured text genera-tion.
in proceedings of the 13th international con-ference on natural language generation, pages 22–28, dublin, ireland.
association for computationallinguistics..tom brown, benjamin mann, nick ryder, melaniesubbiah,jared d kaplan, prafulla dhariwal,arvind neelakantan, pranav shyam, girish sastry,amanda askell, sandhini agarwal, ariel herbert-voss, gretchen krueger, tom henighan, rewonchild, aditya ramesh, daniel ziegler, jeffrey wu,clemens winter, chris hesse, mark chen, ericsigler, mateusz litwin, scott gray, benjamin chess,jack clark, christopher berner, sam mccandlish,alec radford, ilya sutskever, and dario amodei..72902020. language models are few-shot learners.
inadvances in neural information processing systems,volume 33, pages 1877–1901.
curran associates,inc..chris callison-burch, cameron fordyce, philippkoehn, christof monz, and josh schroeder.
2007.in pro-(meta-) evaluation of machine translation.
ceedings of the second workshop on statistical ma-chine translation, pages 136–158, prague, czechrepublic.
association for computational linguis-tics..dallas card, peter henderson, urvashi khandelwal,robin jia, kyle mahowald, and dan jurafsky.
2020.inwith little power comes great responsibility.
proceedings of the 2020 conference on empiricalmethods in natural language processing (emnlp),pages 9263–9274, online.
association for computa-tional linguistics..elizabeth clark and noah a. smith.
2021. chooseyour own adventure: paired suggestions in collabo-rative writing for evaluating story generation models.
in proceedings of the 2021 conference of the northamerican chapter of the association for computa-tional linguistics: human language technologies,pages 3566–3575, online.
association for compu-tational linguistics..florian daniel, pavel kucherbaev, cinzia cappiello,boualem benatallah, and mohammad allahbakhsh.
2018. quality control in crowdsourcing: a surveyof quality attributes, assessment techniques, and as-in acm computing surveys, vol-surance actions.
ume 51. association for computing machinery..liam dugan, daphne ippolito, arun kirubarajan, andchris callison-burch.
2020. roft: a tool for eval-uating human detection of machine-generated text.
in proceedings of the 2020 conference on empiri-cal methods in natural language processing: sys-tem demonstrations, pages 189–196, online.
asso-ciation for computational linguistics..angela fan, mike lewis, and yann dauphin.
2018. hi-in proceedingserarchical neural story generation.
of the 56th annual meeting of the association forcomputational linguistics (volume 1: long papers),pages 889–898, melbourne, australia.
associationfor computational linguistics..sebastian gehrmann, tosin adewumi, karmanya ag-garwal, pawan sasanka ammanamanchi, aremuanuoluwapo, antoine bosselut, khyathi raghavichandu, miruna clinciu, dipanjan das, kaustubh d.dhole, wanyu du, esin durmus, ondˇrej duˇsek,chris emezue, varun gangal, cristina garbacea,tatsunori hashimoto, yufang hou, yacine jernite,harsh jhamtani, yangfeng ji, shailza jolly, mihirkale, dhruv kumar, faisal ladhak, aman madaan,mounica maddela, khyati mahajan, saad ma-hamood, bodhisattwa prasad majumder, pedro hen-rique martins, angelina mcmillan-major, simonmille, emiel van miltenburg, moin nadeem, shashi.
narayan, vitaly nikolaev, rubungo andre niy-ongabo, salomey osei, ankur parikh, laura perez-beltrachini, niranjan ramesh rao, vikas raunak,juan diego rodriguez, sashank santhanam, jo˜aosedoc, thibault sellam, samira shaikh, anasta-sia shimorina, marco antonio sobrevilla cabezudo,hendrik strobelt, nishant subramani, wei xu, diyiyang, akhila yerukola, and jiawei zhou.
2021. thegem benchmark: natural language generation, itsevaluation and metrics.
arxiv, abs/2102.01672..sebastian gehrmann, hendrik strobelt, and alexanderrush.
2019. gltr: statistical detection and visu-in proceedings of thealization of generated text.
57th annual meeting of the association for compu-tational linguistics: system demonstrations, pages111–116, florence, italy.
association for computa-tional linguistics..tatsunori hashimoto, hugh zhang, and percy liang.
2019. unifying human and statistical evaluation fornatural language generation.
in proceedings of the2019 conference of the north american chapter ofthe association for computational linguistics: hu-man language technologies, volume 1 (long andshort papers), pages 1689–1701, minneapolis, min-nesota.
association for computational linguistics..david m. howcroft, anya belz, miruna-adrianaclinciu, dimitra gkatzia, sadid a. hasan, saadmahamood, simon mille, emiel van miltenburg,sashank santhanam, and verena rieser.
2020.twenty years of confusion in human evaluation:nlg needs evaluation sheets and standardised def-in proceedings of the 13th internationalinitions.
conference on natural language generation, pages169–182, dublin, ireland.
association for computa-tional linguistics..daphne ippolito, daniel duckworth, chris callison-burch, and douglas eck.
2020. automatic detec-tion of generated text is easiest when humans arein proceedings of the 58th annual meet-fooled.
ing of the association for computational linguistics,pages 1808–1822, online.
association for computa-tional linguistics..daniel khashabi, gabriel stanovsky, jonathan bragg,nicholas lourie, jungo kasai, yejin choi, noah a.smith, and daniel s. weld.
2021. genie: a leader-board for human-in-the-loop evaluation of text gen-eration.
arxiv, abs/2101.06561..joy kim, sarah sterman, allegra argent beal co-hen, and michael s bernstein.
2017. mechanicalnovel: crowdsourcing complex work through re-in proceedings of the 2017ﬂection and revision.
acm conference on computer supported cooper-ative work and social computing, pages 233–245.
association for computing machinery..aniket kittur and robert e. kraut.
2008. harnessingthe wisdom of crowds in wikipedia: quality throughcoordination.
in proceedings of the 2008 acm con-ference on computer supported cooperative work,.
7291international conference on natural language gen-eration, pages 88–94, tokyo, japan.
association forcomputational linguistics..stephanie schoch, diyi yang, and yangfeng ji.
2020.
“this is a problem, don’t you agree?” framing andbias in human evaluation for natural language gener-ation.
in proceedings of the 1st workshop on evalu-ating nlg evaluation, pages 10–16, online (dublin,ireland).
association for computational linguistics..alan turing.
1950. computing machinery and intelli-gence.
in mind, volume lix, pages 433–460.
ox-ford university press (oup)..adaku uchendu, thai le, kai shu, and dongwon lee.
2020. authorship attribution for neural text gener-in proceedings of the 2020 conference onation.
empirical methods in natural language process-ing (emnlp), pages 8384–8395, online.
associa-tion for computational linguistics..daniel s. weld, mausam, christopher h. lin, andjonathan bragg.
2014. artiﬁcial intelligence andin handbook of collectivecollective intelligence.
intelligence, chapter 3. mit press..rowan zellers, ari holtzman, yonatan bisk, alifarhadi, and yejin choi.
2019a.
hellaswag: canin pro-a machine really ﬁnish your sentence?
ceedings of the 57th annual meeting of the asso-ciation for computational linguistics, pages 4791–4800, florence, italy.
association for computationallinguistics..rowan zellers, ari holtzman, elizabeth clark, lianhuiqin, ali farhadi, and yejin choi.
2021. turingad-vice: a generative and dynamic evaluation of lan-guage use.
in proceedings of the 2021 conference ofthe north american chapter of the association forcomputational linguistics: human language tech-nologies, pages 4856–4880, online.
association forcomputational linguistics..rowan zellers, ari holtzman, hannah rashkin,yonatan bisk, ali farhadi, franziska roesner, andyejin choi.
2019b.
defending against neural fakenews.
in advances in neural information process-ing systems, volume 32. curran associates, inc..jingqing zhang, yao zhao, mohammad saleh, andpeter liu.
2020. pegasus: pre-training with ex-tracted gap-sentences for abstractive summarization.
in proceedings of the 37th international conferenceon machine learning, volume 119, pages 11328–11339. pmlr..cscw ’08, page 37–46, new york, ny, usa.
as-sociation for computing machinery..chris van der lee, albert gatt, emiel van miltenburg,and emiel krahmer.
2021. human evaluation of au-tomatically generated text: current trends and bestpractice guidelines.
computer speech & language,67:101151..chris van der lee, albert gatt, emiel van miltenburg,sander wubben, and emiel krahmer.
2019. bestpractices for the human evaluation of automaticallygenerated text.
in proceedings of the 12th interna-tional conference on natural language generation,pages 355–368, tokyo, japan.
association for com-putational linguistics..chia-wei liu, ryan lowe, iulian serban, mike nose-worthy, laurent charlin, and joelle pineau.
2016.how not to evaluate your dialogue system: anempirical study of unsupervised evaluation metricsfor dialogue response generation.
in proceedings ofthe 2016 conference on empirical methods in natu-ral language processing, pages 2122–2132, austin,texas.
association for computational linguistics..tanushree mitra, c.j.
hutto, and eric gilbert.
2015.comparing person- and process-centric strategiesfor obtaining quality data on amazon mechanicalturk.
in proceedings of the 33rd annual acm con-ference on human factors in computing systems,pages 1345–1354.
association for computing ma-chinery..jekaterina novikova, ondˇrej duˇsek, amanda cer-cas curry, and verena rieser.
2017. why we needin proceedingsnew evaluation metrics for nlg.
of the 2017 conference on empirical methods innatural language processing, pages 2241–2252,copenhagen, denmark.
association for computa-tional linguistics..jekaterina novikova, ondˇrej duˇsek, and verena rieser.
2018. rankme: reliable human ratings for naturalin proceedings of the 2018language generation.
conference of the north american chapter of theassociation for computational linguistics: humanlanguage technologies, volume 2 (short papers),pages 72–78, new orleans, louisiana.
associationfor computational linguistics..daniel m. oppenheimer, tom meyvis, and nico-instructional manipulationlas davidenko.
2009.checks: detecting satisﬁcing to increase statisticalpower.
in journal of experimental social psychol-ogy, volume 45, pages 867–872.
elsevier..alec radford, jeff wu, rewon child, david luan,dario amodei, and ilya sutskever.
2019. languagemodels are unsupervised multitask learners.
openaiblog..sashank santhanam and samira shaikh.
2019. to-wards best experiment design for evaluating dia-in proceedings of the 12thlogue system output..7292often justiﬁed their choices by explaining whattypes of human language they believed machinescould (or could not) generate.
we took note ofthese comments and annotated for them in our dataannotation process (appendix a.3) because theydemonstrate the expectations evaluators have forthe quality of machine-generated text.
some exam-ple comments shown in table 5..a.5 pilot study.
before running the experiments described in thepaper, we ran a smaller-scale version with bothamazon mechanical turk (n = 22) and “expert”evaluators (nlp graduate students; n = 11).
weasked the evaluators to distinguish between storiesauthored by humans, gpt2, and gpt3 and to ex-plain their reasoning.
when we coded and analyzedtheir responses, we found that the most accurateevaluators focused on textual aspects like repeti-tion and were less likely to mention aspects likestyle.
the amt evaluators mentioned grammarand spelling far more frequently than the expertevaluators, who were more likely to mention therepetition, factuality, and commonsense of the pas-sage..figure 5 shows the basic instructions that wereshown to all evaluators, in both §2 and §3, regard-less of training or domain.
all training informationoccurred after receiving the basic instructions..instruction training.
a.6.1the training shown to evaluators in the instructiontraining condition is shown in figure 6..a.6.2 example traininga screenshot of the examples and comparisontraining is in figure 7. the full set of examplesand annotations used in the examples and com-parison trainings can be found in the supplemen-tary materials and at ark.cs.washington.edu/human_evals_acl21..a appendices.
a.1 newspapers.
each newspaper came from a randomly chosen u.s.state and was selected from wikipedia’s lists ofnewspapers by state (en.wikipedia.org/wiki/list_of_newspapers_in_the_united_states#by_state_and_territory).
the human-authorednews articles and prompts came from the followingstates and websites:.
• hi: www.westhawaiitoday.com• ct: www.greenwichtime.com/• wa: www.vashonbeachcomber.com/• sd: www.argusleader.com/• ca: www.redding.com/• ma: www.lowellsun.com/• ne: starherald.com/• va: dailyprogress.com/• wv: www.theintermountain.com/• nm: www.lcsun-news.com/• la: www.nola.com/• ia: qctimes.com/• ny: www.pressconnects.com/• in: www.pal-item.com/• nj: www.northjersey.com/.
the frequency of the scores (out of 5) receivedby evaluators is shown in figures 3 (for gpt2experiments) and 4 (for gpt3 experiments)..a.3 annotation details.
the authors annotated 300 comments (150 fromthe no training experiment and 150 from the ex-amples experiment).
for each experiment, we ran-domly chose 50 authors from each setting and ran-domly added 1 of their responses to the annotationset.
each comment was annotated by 2 of the au-thors.
the annotation labels are shown in table 4.to create the set of annotation labels, the authorscreated a candidate list of labels, annotated a subsetof the data collected in the pilot study (appendixa.5) together, then another subset separately, andﬁnally reﬁned the labels based on feedback fromthat process.
because evaluators’ responses oftencontained more than one reason for their choice,comments could receive more than one label..a.4 evaluators’ expectations of generated.
text.
because we asked evaluators whether they thoughtthe text was human- or machine-authored, they.
a.2 score frequencies.
a.6 training and instructions.
7293(a) gpt2 overall.
(b) gpt2 story.
(c) gpt2 news.
(d) gpt2 recipe.
figure 3: histogram of scores classifying human and gpt2 texts..(a) gpt3 overall.
(b) gpt3 story.
(c) gpt3 news.
(d) gpt3 recipe.
figure 4: histogram of scores classifying human and gpt3 texts..category.
label.
description.
example.
form.
grammar.
level of de-tail.
the spelling and grammar of the text,punctuation/formatting issuesis the text simple or does it go morein-depth?.
content.
factuality.
genre.
the.
ifthetextgenre/domain/style/formality that thereader expects, adheres to style norms.
is.
repetition.
words/phrases/content repeated itself.
consistency.
commonsense.
coherence.
the accuracy of the text, whether itdescribes things that are “true.”how the text relates to the context andother pieces of the textwhether the text “makes sense” withinthe world that it is written.
the structure and coherence of the text.
order issues go here..i would make the text more grammatical by addingmore punctuation where necassary.
i would include more examples and explanations ofthe statements.
the author need to elaborate moreon the topic.
written exactly the way a human will tell a story.
repeating “or some would say” seemed very unnat-ural.
the article lists many facts that make the informa-tion seem like it was machine-generated.
the subject of the article follows the headline wellwithout repeating it exactlychange the “bake in the preheated oven for 20 min-utes on top of the stove.” you can’t bake on top ofthe stove but to bake in the oven.
more cohesion between sentences.
feel looselyrelated, but wording is strange..machinecapabili-ties.
null.
writer intentand expres-sion.
speculating about writer’s intent or ca-pabilities (e.g., ability to express emo-tions).
the text is thorough and tries to cover all basis ofthe situation.
it is very inclusive and humans worryabout being inclusive not machines..miscellaneous everything else.
null/vague.
no reasons given, or too vague to beconsidered a real reason.
too many dialogue-like things, and make it lessgender-dicey.
i selected this rating because it is deﬁnitely writtenby human.
table 4: the annotation labels, along with an example of each label.
note that some example sentences would alsobe labeled with additional labels.
we did not use the null category in the paper’s analyses..7294punctuation is perfect as well as the ﬂow of the text.
there is also more complex punctuation, such as quotes, that i think acomputer would get wrong..“fried anyone to a crisp.” that is a human if i’ve ever seen one.
a bot or ai is more proper, they wouldn’t write so casual..because it talked about love which robots know nothing about..lack of oxford comma.
a computer would know better..the article ﬂows properly, has appropriate english and multiple quotes.
this would seem to be more than a bot could create.
how would a bot create random quotes?.
this was more of a ramble which humans do, not computers..there are details and key phrases used in this article that computer generated text would not have in it, such as “came up short”,“put together a solid drive”, “put up any points”.
these are human speciﬁc terms and are not generally able to be programmedinto a text program..this piece quotes the host and i don’t believe ai can interview people yet so this has to be human written..it has a lot of detail in an emotional description that a machine isn’t capable of giving to its readers..the way some words are phrased here again shows the human uncertainty, “let the apples marinate for about 30 minutes”.
if thiswas machine-generated, it would most likely just say marinate for 30 minutes..it seems to know when to use semicolns very well.
this could be a human or a really smart computer..i don’t think ais are capable of writing recipes on their own just yet..i don’t believe a machine could come up with this level of whimsy or creativity and have it make sense..i don’t think ai would use the term ‘literally’..there is a lot of every day language written in this recipe that i couldn’t see a machine possibly replicating..it adds that she is both nervous and excited whereas a machine wouldn’t care what emotions are involved..the writer used proper grammar and punctuation.
no bot could write this,.
i’m not sure if a computer would get the concept or use the word “your” where the recipe begins with “start by doing your prep.”.
table 5: example reasons evaluators gave for their decisions that spoke to their beliefs about current nlg capabil-ities..figure 5: basic instructions shown to all evaluators..figure 6: the instruction training..7295figure 7: the example training (left) and comparison training (right) in the story domain.
the instructions are thesame for both, except “choose the one you think was written by a machine.” was in comparison only..7296