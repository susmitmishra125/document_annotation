determinantal beam search.
clara meister martina forster ryan cotterell.
,.
eth zürich university of cambridgemeistecl@inf.ethz.ch martfors@ethz.chryan.cotterell@inf.ethz.ch.
abstract.
reﬂect.
function does not.
beam search is a go-to strategy for decodingneural sequence models.
the algorithm cannaturally be viewed as a subset optimizationproblem, albeit one where the correspondingsetinteractionsbetween candidates.
empirically, this leads tosets often exhibiting high overlap, e.g., stringsmay differ by only a single word.
yet inuse-cases that call for multiple solutions, a di-verse or representative set is often desired.
toaddress this issue, we propose a reformulationof beam search, which we call determinantalbeam search.
determinantal beam search has anatural relationship to determinantal point pro-cesses (dpps), models over sets that inherentlyencode intra-set interactions.
by posing itera-tions in beam search as a series of subdetermi-nant maximization problems, we can turn thealgorithm into a diverse subset selection pro-cess.
in a case study, we use the string subse-quence kernel to explicitly encourage n-gramcoverage in text generated from a sequencemodel.
we observe that our algorithm offerscompetitive performance against other diverseset generation strategies in the context oflanguage generation, while providing a moregeneral approach to optimizing for diversity..1.introduction.
the decoding of neural sequence models is a fun-damental component of many tasks in nlp.
yet,many proposed decoding methods aim to produceonly a single solution; further, decoding strategiesthat provide a set, such as beam search, admit highoverlap between solutions.
such approaches fail toreﬂect that for many nlp tasks,1 there can be mul-tiple correct solutions—or that we may desire a di-verse set of solutions.
as it stands, standard beamsearch chooses items based purely on individual.
1as concrete examples,.
in machine translation therealmost always exist multiple ways to translate a sentence; instory generation, we often seek creative language or multipleoptions to choose from..scores, with no means for encoding interaction be-tween candidates; this is the limitation which weattempt to address in this work..we derive determinantal beam search, a novelgeneralization of beam search that casts subsetselection asthe subdeterminant optimizationproblem.
speciﬁcally, we formulate each iterationof beam search as a subdeterminant maximizationproblem parameterized by a positive semi-deﬁnitematrix that encodes interactions between the pos-sible candidates; standard beam search is recov-ered by a speciﬁc diagonal matrix.
this framingcreates a natural paradigm for taking the rela-tionships between candidates during the decodingprocess, and can thus assign higher scores to di-versiﬁed sets; we show how this approach relatesto k-determinantal point processes (dpps).
giventhe wealth of research on efﬁcient kernel com-putation (rousu and shawe-taylor, 2005; farhanet al., 2017) and dpp inference strategies (li et al.,2016; han et al., 2017; chen et al., 2018), we ﬁndthe impact on runtime to be quite reasonable incomparison to standard decoding techniques..in a case study on neural machine translation(nmt), we demonstrate how to make use of thestring subsequence kernel (lodhi et al., 2002) toencode the notion of n-gram diversity in the lan-guage generation process, allowing us to derive anelegant diverse beam search.
under this scheme,we observe that determinantal beam search gener-ates more diverse sets than standard beam searchwith minimal trade-off in terms of bleu.
wesee improved performance over stochastic beamsearch (sbs; kool et al., 2019), which is reportedto encourage diversity, and a slight improvementover vijayakumar et al.
(2018)’s diverse beamsearch (dbs) while providing a more general ap-proach to optimizing for intra-set diversity..2 neural sequence models.
neural sequence models are probability distribu-tions p(y | x) over sequences y in an output.
proceedingsofthe59thannualmeetingoftheassociationforcomputationallinguisticsandthe11thinternationaljointconferenceonnaturallanguageprocessing,pages6551–6562august1–6,2021.©2021associationforcomputationallinguistics6551space y conditioned on an input x.2 here wedeﬁne y as the set of all valid sequences derivedfrom a vocabulary v that are bookended by distin-guished bos and eos tokens, indicating the begin-ning and end of the sequence, respectively.
typ-ically, the sequence length is upper-bounded bysome value nmax ∈ z+, which may depend onx. in this work, we consider locally normalizedmodels, i.e.
where p is a probability distributionover ¯v def= v ∪ {eos} conditioned on previouslygenerated tokens y<t.
the probability of the fullsequence y = (cid:104)y1, y2, .
.
.
(cid:105) is then calculated viathe chain rule of probability:.
degenerate objective.
it is important to notethat the highest-probability solutions under neuralsequence models are not always high-quality;speciﬁcally for tasks involving language gener-ation, e.g., machine translation, prior work hasshown the tendency for map decoding to leadto generic or degenerate solutions (stahlberg andbyrne, 2019; meister et al., 2020; eikema andaziz, 2020) while superior solutions assignedonly slightly lower probability are often over-looked (holtzman et al., 2020).
consequently,heuristic search methods or alternative objectivesare frequently employed for decoding languagegenerators..p(y | x) =.
p(yt | y<t, x).
(1).
2.1 beam search.
|y|(cid:89).
t=1.
def= bos.
our model p is typicallywhere y<1 = y0parameterized by a neural network with weights θ.as we do not focus on the underlying model itselfin this work, we omit the dependence of p on theparameters θ..we deﬁne the decoding problem as the searchfor the highest-scoring y among all sequences iny according to the model p(y | x), which is alsocalled maximum-a-posteriori (map) inference:.
y(cid:63) = argmax.
log p(y | x).
(2).
y∈y.
where the log transform of p is used by conven-tion.
we further deﬁne the set decoding problemas the search for a set y (cid:63) of a speciﬁed cardinalityk among all valid subsets {y (cid:48) ⊆ y | |y (cid:48)| = k}that has the highest score where, by overloading,we deﬁne.
p(y | x) def=.
p(y | x).
(3).
(cid:89).
y∈y.
similarly to eq.
(2), the set-decoding problem isthen deﬁned as:.
y (cid:63) = argmaxy (cid:48)⊆y,|y (cid:48)|=k.
log p(y (cid:48) | x).
(4).
a common heuristic to approximate the decodingproblem in eq.
(2) is to sequentially choosethe token yt at each time step t that maximizesp(yt | y<t, x) until the eos token is generated orthe maximum sequence length nmax is reached.
this procedure is known as greedy search.
beamsearch is an oft-employed generalization of greedysearch that returns k candidates and explores moreof the search space.3 in this work, we focus ona framing of beam search as iterative subsetselection, which allows for a remarkably conciseformulation of the algorithm.
given an initialset y0 containing only the bos token, we choosesubsequent yt for t ∈ {1, .
.
.
, nmax} according tothe following recursion:.
standard beam search.
y0 ← {bos}yt ← argmaxy (cid:48)t ⊆bt,|y (cid:48)t |=k.
(5).
log p(y (cid:48).
t | yt−1, x).
where we are constrained to only extending candi-dates present in the beam set, which we deﬁne as.
bt.
def= {y<t ◦ y | y<t ∈ yt−1 and y ∈ ¯v}.
(6).
however, as has been noted in the literature, thereare a number of issues with both eq.
(2) and (4).
first, as y may be an exponentially large (in v)space and p is typically non-markovian, we can-not efﬁciently search over y, much less over y k.second, speciﬁcally for language generation tasks,these might not be useful objectives..2x may be, e.g., a source sentence or an image..where ◦ is used to indicate string concatenations.
note that candidates in yt−1 already endingin eos are simply added directly to bt,i.e.,eos ◦ eos = eos.
under this deﬁnition, we havethe cardinality constraint |bt| ≤ | ¯v| · k..3a number of nlp tasks only take the highest-scoring el-ement of the returned set y while other tasks utilize the entireset of solutions..65522.2 a determinantal reformulation.
full determinantal beam search.
we now introduce an alternative, equivalent no-tation for eq.
(5) using matrices and determi-nants that will shed light on the straightforwardgeneralization of beam search that we present asthe primary contribution of this paper.
we de-ﬁne a timestep-dependent4 diagonal matrix d ∈r|bt|×|bt| where we take the diagonal entry.
dii.
def= p(y(i).
≤t | x).
(7).
here y(i)≤t is the ith candidate in bt according toa unique mapping of every element y≤t ∈ bt toan integer between 1 and |bt|.
furthermore, weuse the notation dyt where yt ⊆ bt, to indicatethe submatrix that only contains those rows andcolumns corresponding to the elements of yt.
wemay now rewrite eq.
(5) as.
determinantal standard beam search.
y0 ← {bos}yt ← argmaxy (cid:48)t ⊆bt,|y (cid:48)t |=k.
(8).
log det(dy (cid:48).
).
t.where equivalence follows from the deﬁnition ofthe determinant for diagonal matrices.
formally,eq.
(8) is known as the subdeterminant maxi-mization problem5 (klee et al., 1995; ebrahimiet al., 2017), which—as the name suggests—refers to the problem of ﬁnding the determinantmaximizing subset of a matrix.
while the notationintroduced in eq.
(8) may seem contrived, it al-lows us to perform the subsequent generalization..3 determinantal beam search.
we are now in a position to ask the fundamentalquestion of this work: what happens if we re-place the diagonal matrix d with a non-diagonalmatrix?
this substitution allows us to accountfor interactions between the elements in the beam.
formally, we consider a timestep-dependent posi-tive semi-deﬁnite (psd) matrix d + w · k wherethe off-diagonal matrix k indicates the strengthof the interactions between candidates.
the non-negative weight w ≥ 0 controls the importance ofthese interactions during the decoding process.
inthis case, the beam search recursion becomes:.
4we have omitted the time-step dependence of d for no-.
tational brevity as it is always clear from context..5albeit with a cardinality constraint..y0 ← {bos}yt ← argmaxy (cid:48)t ⊆bt,|y (cid:48)t |=k.
log det(dy (cid:48).
+ w · ky (cid:48).
t.(9)).
t.clearly, we recover beam search when w = 0;however, we can now select subsets based addi-tionally on candidate interactions.
that is, eq.
(9)now has an interpretation as a diversity objectivefunction (indyk et al., 2014) when k is chosenwisely.
due to the presence of the log, eq.
(9) isonly well deﬁned when the matrix dy + w · kyis psd.6.
3.1 constructing k.one simple way to construct k is as a gram ma-trix, where each i, j element of k is computedvia a kernel function k : s × s → r that mapstwo items in a space s to a real number.
speciﬁ-cally, we deﬁne kij = k(si, sj) where si, sj ∈ sare the ith and jth elements of s, respectively.
inslight abuse of notation, we overload the kernelfunction k to take a set s such that k = k(s) isthe kernel matrix resulting from pairwise compu-tation over elements of s.7 following from mer-cer’s theorem, the matrix k = k(s) is necessarilypsd and, thus the matrix dy + w · ky is psd forany y ⊆ s.8.
the efﬁcient computation of kernel functionsis a well-studied problem—largely due to theprevalence of kernels in various machine learningtechniques.
for example, dynamic programmingtechniques are often employed in computationof k(s) (rousu and shawe-taylor, 2005) orapproximate low-rank kernel matrices can be usedin place of k(s) (si et al., 2017)..3.2 relation to a dpps.
one interpretation of eq.
(9) is as a determinan-tal point process (dpp).
speciﬁcally, it is a k-dpp.
6to see this, recall that the determinant is the product ofthe eigenvalues.
to ensure that the determinant is strictly pos-itive, we can simply enforce that all the eigenvalues are pos-itive, which is necessarily the case for psd matrices.
notethat in the case where any of the eigenvalues of a submatrixare zero, we take log det(·) = −∞..7in machine learning literature, the term “kernel” is oftenused to refer to both the function k and the kernel matrix k.8to see this, note that the matrix d is necessarily psd.
since psd matrices are closed under addition and multiplica-tion by a positive scalar, then necessarily d + w · k is psd.
lastly, any submatrix of a psd matrix is also psd, whichmakes dy + w · ky a psd matrix..6553(kulesza and taskar, 2011) in the l-ensemble pa-rameterization where we have l = d+w·k.
thisinterpretation as a k-dpp gives us a very clear un-derstanding of why eq.
(8) yields a diverse beamsearch.
the diagonal entries encode quality, whichtells how “good” each candidate on the beam is,while the off-diagonal entries encode how similartwo elements are and, thus, how much they shouldbe repulsed.
for an overview of dpps we refer thereader to kulesza and taskar (2012)..3.3 computing log-determinantsunfortunately, computing the argmax9 in eq.
(9)is an np-hard problem (ko et al., 1995).
however,as the subdeterminant maximization problem hasmany applications, there has been much researchon efﬁcient algorithms for approximating log-determinants in the context of, e.g., determinantalpoint processes (gillenwater et al., 2012; hanet al., 2017).10one such algorithm uses aﬁrst-order approximation of the log-determinantfunction (han et al., 2017).
the work of chenet al.
(2018) uses a greedy, iterative approach; byupdating the cholesky factorization of the matrixkernel incrementally, the algorithm reduces infer-ence time to o(k2|s|) to return k candidates fromset s. pseudocode for the latter approach canbe found in chen et al.
(2018); pseudocode forthe algorithm in log-space—since probabilisticmodels are often worked with in log-space fornumerical stability—can be found in app.
a..3.4 runtime analysis.
we consider the runtime of selecting k candidatesat any given time step in the recursion of eq.
(9).
at each time step, we must ﬁrst construct thematrix k. this computation is highly dependenton the set interactions being modeled; as such, leto(c(k)) be a runtime bound for k’s computationwhen our search uses a beam size of k. oncewe have constructed our matrix d + w · k, wemust next select k items.
the set of hypotheses atany time step is at most k| ¯v|.
while as discussedin §3.3, ﬁnding the size-k subset that exactly.
9we may also sample from the k-dpp modeled by eq.
(9)rather than taking the approximate mode; this would only re-quire changing the inference algorithm and can be done ina similarly efﬁcient manner (li et al., 2016).
we focus ondeterministic methods in this work as we aim to ﬁnd the ob-jective maximizing set..10as beam search is already a heuristic approach, such anapproximation does not have any theoretical implications forthe results of our algorithm..maximizes eq.
(9) has exponential runtime, weassume approximate methods are employed.
using the method given by chen et al.
(2018),approximate map inference takes k3| ¯v|timeto return k items from a set of size k| ¯v|.
thus,the runtime at each iteration of determinantalbeam search under these conditions would beo(c(k) + k3| ¯v|).
note that standard beam searchruns in o(k| ¯v| log(k| ¯v|)) time at each iteration.
as k is generally small (≤ 20) and the impact ofc(k) can be made reasonable (§3.1), the practicalincrease in runtime is typically only moderate..4 case study: diverse beam search.
we now consider the task of language generation,where our vocabulary ¯v is a set of words and y isthe set of all valid strings derived from ¯v.
whenthe space of our kernel function s = bt, onesimple way of modeling interactions is through astring subsequence kernel (lodhi et al., 2002)..4.1 computing the string kernel.
the string subsequence kernel, proposed by lodhiet al.
(2002), is a function over two strings s andt computed as:.
k(s, t) =.
(cid:88).
(cid:88).
λl(i) (cid:88).
λl(j).
(10).
u∈v n.i:u=s[i].
j:u=t[j].
where v n is the set of all ﬁnite strings of lengthn over the alphabet v; i (or j) denotes a vector ofindices i = (i1, .
.
.
, i|u|) where 1 < i1 < i|u| ≤|s|; l(i) def= i|u|−i1+1 is the length of the substringu in s; λ ∈ (0, 1] is a decay factor which serves asa penalty for gaps within a compared subsequence.
direct computation of eq.
(10) is exponentialin |v|, but efﬁcient dynamic programs can be uti-in this work, we employ the trie-basedlized:methods of rousu and shawe-taylor (2005) tocompute eq.
(10).
under this scheme, the com-putation of the kernel between two strings s and tis o(n · m · log(max(|s|, |t|)), where n is the cho-sen subsequence length (a hyperparameter) and mis the number of words that strings s and t have incommon.
note that |s|, and thus m , are boundedby the time step t. further, we can reuse manyof the computations between subsequent decod-ing rounds due to the iterative nature of both beamsearch and the subsequence kernel computations.
additionally, since the magnitude of eq.
(10) is in-ﬂuenced by the lengths of s and t, we normalize.
6554the kernel as follows:.
knorm(s, t) =.
k(s, t)(cid:112)k(s, s) · k(t, t).
(11).
4.2.integration into detbs.
the string subsequence kernel gives us a straight-forward method for decoding diverse sets ofstrings from language generators.
we constructthe matrix.
d + w · k(bt).
(12).
the.
objective-maximizing.
using the dynamic program mentioned aboveto compute k(bt).
intuitively, we can expectthe size k set correspondingthe argmax—i.e.,submatrix—oftod + w · k(bt) to have higher subsequence diver-sity as w is increased.
this is perhaps most easilyseen when viewing our problem as a k-dpp:if strings y(i) and y(j) have high overlap, thiswill be reﬂected in the matrix k(bt) at positioni, j. higher values of k(bt)i,j = k(y(i), y(j))lead to lower probability of both y(i) and y(j)being in the set drawn according to the k-dppparameterized by d + w · k(bt), which followsfrom the properties of dpps outlined in §2.2.
inshort, higher values of k(y(i), y(j)) decrease thevalue of log det(dy + w · k(bt)y ) for sets ycontaining both y(i) and y(j), which makes y lesslikely to be chosen in the recursion of eq.
(9)..5 experiments.
in our experiments, we explore the use of determi-nantal beam search as a diverse decoding strategyfor language generation..5.1 baselines.
various diverse decoding strategies exist in thenlp literature.
we ﬁrst discuss those strategiesthat we employ as baselines in our experiments..standard beam search.
beam search is one ofthe most widely used decoding algorithms in nlp,where many problems require efﬁcient strategiesfor decoding solutions from structured predic-tors.
speciﬁcally, for language generation tasks,beam search has repeatedly proved its effective-ness at decoding state-of-the-art solutions (wuet al., 2016; serban et al., 2017; edunov et al.,2018; yang et al., 2019).
we refer back to §2.1for the algorithm..stochastic beam search.
kool et al.
(2019)propose stochastic beam search (sbs), a decod-ing technique that samples without replacementfrom sequence models according to their distribu-tion over the entire space y. for random samplingmethods such as sbs, it is customary to use a sam-pling temperature t > 0 at generation time to con-trol for the peakiness of the sampling distribution.
this results in the generalized softmax:.
pt (y | y<t, x).
(cid:16).
(13).
(cid:17).
exp.
log p(y | y<t, x)/t.
=.
(cid:80).
y(cid:48)∈v exp.
(cid:16).
log p(y(cid:48) | y<t, x)/t.
(cid:17).
where larger t may lead to more diverse sets sim-ply due to additional smoothing..t , .
.
.
, bg.
diverse beam search.
vijayakumar et al.
(2018) propose a modiﬁcation to the standardbeam search algorithm—which they term diversebeam search (dbs)—to alleviate lack of diversity.
the algorithm further divides the beam into ggroups b1t , where g is a hyperparameterof the algorithm, and optimizes for diversitybetween the different groups by subtracting asimilarity term ∆(y≤t, bgt ) from the decodingobjective.11 speciﬁcally, ∆(y≤t, bgt ) representsthe degree of similarity between a hypothesis y≤tand a group of hypotheses bgt .
they ﬁnd g = k,i.e., each group contains a single hypothesis,and the hamming distance similarity metric leadto the best results; we use these settings in ourexperiments.
note that under this scheme, thesolution set may have duplicates if the diversitypenalty is not large enough..notably, under the above experimental settings,the runtimes of diverse beam search and our algo-rithm are the same, up to computation of the ham-ming loss and string kernel, respectively.
how-ever, while string kernel computations in our algo-rithm can be done in parallel, the diversity penaltyin diverse beam search must be computed sequen-tially for each hypothesis, as it is based on the pre-viously chosen groups..5.2 setup.
we run experiments on neural machine transla-tion (nmt) models trained on the wmt’14 (bo-jar et al., 2014) en–fr and the wmt’19 (barrault.
11the diversity term has coefﬁcient w to determine thestrength of the penalty.
when this weight is 0 or sufﬁcientlysmall, all groups will return the same solution(s)..6555figure 1: averaged n-gram diversity vs. minimum, median, and maximum bleu score for beam sizes k = 5, 10,20 on wmt’14 en–fr and wmt’19 de–en newstest using various decoding strategies.
the free parameterfor each strategy is either the softmax temperature or the weight of the diversity parameter (see §5.2)..et al., 2019) de–en datasets; for reproducibility,we use the pretrained models made available byfairseq12 (ott et al., 2019).
we evaluate on thenewstest set from the respective datasets, eachcontaining 3003 sentences.
further details can befound in app.
b..for determinantal beam search (detbs), weperform a hyperparameter search (precise detailslikewise in app.
b) over λ and n, the decay factorand subsequence length, respectively.
search isperformed for ﬁxed w = 0.1 and k = 10 onvalidation sets for both languages; we omit asearch over the entire space of w, k, λ, n so asto not create an unfair advantage for detbs incomparison with the other decoding strategies,for which no hyperparameters are tuned.
we usesubsequence length n = 2 and λ ∈ {0.1, 0.3} forde–en and en–fr, respectively..we decode sets of size k ∈ {5, 10, 20}with each strategy, comparing sentence-levelbleu and n-gram coverage dn averaged acrossn ∈ {1, 2, 3, 4} in the decoded sets, where wedeﬁne dn as.
dn =.
#of unique n-grams in k strings#of n-grams in k strings.
(14).
12https://github.com/pytorch/fairseq/.
tree/master/examples/translation.
13for each decoding strategy, we choose the diversity pa-rameter corresponding to the most diverse set that had medianbleu 28.5 ± 0.05..while dn has a more natural interpretation ascoverage of different n-grams, the above quantityis often referred to as n-gram diversity in theliterature and so we transition to this term forconsistency.
following the experimental setup ofkool et al.
(2019), we vary sampling temperaturet ∈ {0.1, 0.2, .
.
.
, 0.8} in the case of beamsearch and stochastic beam search and diversityweight w ∈ {0.1, 0.2, .
.
.
, 0.8} in the case ofdiverse beam search.
for detbs, we observe thatlarger sets require a smaller diversity penalty toachieve good n-gram diversity: in fig.
1 we showresults for detbs with the string subsequencekernel for w ∈ {0.01, 0.02, · · · , 0.1, 0.2, 0.3, 0.4}for k = 5, w ∈ {0.01, 0.02, · · · , 0.15] for k = 10,and w ∈ {0.01, 0.02, · · · , 0.05} for k = 20.14 toobserve how bleu is affected by larger diversitycoefﬁcients under detbs, we explore a ﬁner grainof weights for detbs in app.
c..5.3 results.
fig.
1 shows the sentence-level bleu score andaveraged n-gram diversity on the newstest setfor different decoding strategies; tab.
2 showsexplicit coverage of 1, 2, 3, 4-grams and aver-aged across 1, 2, 3, 4-grams for different decodingstrategies when bleu is controlled for.
the 3 lines.
14recall w = 0 recovers standard beam search with a tem-.
perature of t = 1..6556zum abschluss wurde eine tombola verlost.
die wahrheit zu sagen ist aber kein verbrechen..source sentence.
beam search (t = 0.6).
• a rafﬂe was held to close the event.
• a rafﬂe was held to conclude the event.
• a rafﬂe was held at the end.
• at the end a rafﬂe was held.
• a rafﬂe was held to close the draw..• but telling the truth is not a crime.
• telling the truth is not a crime.
• but telling the truth isn’t a crime.
• but telling the truth is no crime.
• however, telling the truth is not a crime..diverse beam search (w = 0.4).
• to conclude, a rafﬂe was held.
• a rafﬂe was held to close the event.
• a rafﬂe was held to close the event.
• a rafﬂe was held to close the event.
• at the end of the event, a rafﬂe was held..• but telling the truth is not a crime.
• but telling the truth is not a crime.
• but telling the truth is not a crime.
• but telling the truth is not a crime.
• telling the truth, however, is not a crime..determinantal beam search (w = 0.12).
• finally, a rafﬂe was held.
• a rafﬂe was held at the end.
• at the end a rafﬂe was held.
• to conclude, a rafﬂe was held.
• a rafﬂe was held to close the event..• but telling the truth is not a crime.
• but telling the truth isn’t a crime.
• telling the truth is not a crime.
• however, telling the truth is not a crime.
• but to tell the truth is not a crime..table 1: generated translations for given source sentence from wmt’19 de–en dataset using different decodingstrategies.
all use a beam size of ﬁve.13 we see large overlap in beam search results while dbs actually returnsseveral of the same results.
in comparision, detbs turns qualitatively diverse results even for simple sentences..per decoding strategy in fig.
1 represent the mini-mum, median, and maximum sentence-level bleuscore out of the k translation options, averagedacross the corpus.
we consider median bleu tobe the best metric of set text-quality, as a good di-verse decoding algorithm should not completelysacriﬁce bleu for the sake of diversity.
the plotsare analogous to those in kool et al.
(2019)..on both datasets and across different set sizes,results indicate that detbs generates diverse setsof strings while maintaining high median andmaximum bleu scores.
we see similar or highern-gram diversity in comparison to dbs for thesame median bleu and a notably better n-gramdiversity vs. bleu trade-off than standard beamsearch and sbs.
further, the highest quality trans-lation (shown by max bleu) does not appear tobe sacriﬁced when the diversity parameter is in-creased for detbs.
in contrast, there is a notabledrop-off for generation strategies in which diver-sity is controlled for using temperature.
we showsamples of generated text in tab.
1..6 related work.
our work is built upon much of the subset op-timization literature in machine learning.
webase our algorithm off the subdeterminant maxi-mization problem (agarwal et al., 2004), which.
has been used to ﬁnd core sets—a concept origi-nating in computational geometry concerning theexistence of a small, representative set of coreitems—in data summarization problems (mirza-soleiman et al., 2013), nearest neighbor search(abbar et al., 2013) and streaming algorithms (in-dyk et al., 2014) inter alia.
informally, we canconnect our problem to the notion of decoding acore set from sequence models.
to the best of ourknowledge, our work is the ﬁrst to use this conceptwhen decoding sequence models..wang and chan (2019) incorporate dpps intoa reinforcement learning objective to optimizefor diverse text when training image captioningmodels.
we optimize for diversity during de-coding, rather than training, which makes ourmethods applicable with out-of-the-box modelsand allows us to avoid highly hyperparameter-sensitive techniques, like minimum-risk trainingor reinforcement learning-based algorithms, whileachieving the same goal.
while the applicationof our methods at training times is an interestingresearch direction, we foresee technical chal-lenges corresponding to such approaches that mayoutweigh their beneﬁts..as a decoding method, our work is closest.
15in the case that not all strategies had such a set, we in-stead bounded bleu by the lowest of the median bleu acrossdecoding strategies..6557de–en.
en–fr.
1-gram 2-gram 3-gram 4-gram avg..1-gram 2-gram 3-gram 4-gram avg..detbsdbsbeam searchsbs.
bleu.
26.2426.5226.1525.95.
0.110.090.080.08.
0.170.140.140.14.
0.210.180.190.19.
0.260.220.230.24.bleu.
23.2924.2623.2923.08.
0.190.160.160.16.
0.110.090.090.1.
0.170.140.140.16.
0.210.170.180.21.
0.250.20.220.25.
0.180.150.160.18.table 2: we report the coverage (as deﬁned in eq.
(14)) of 1, 2, 3, 4-grams and averaged across 1, 2, 3, 4-grams aswell as median bleu for k = 20 on the newstest dataset.
for each decoding strategy, we report metrics on thegenerated set that has highest (average) dn, where we set the constraint that median bleu for the set is still within1 point of the highest median bleu (across decoding strategies and diversity parameters).15.to that of vijayakumar et al.
(2018), who pro-pose a variation of beam search (described in§5.3).
however, their algorithm lacks theoreticalmotivation and is not guaranteed to provide a non-overlapping set;the same solution may appearmultiple times in the decoded set if the diversitypenalty is not large enough, as shown in tab.
2.additionally, groups at each time step t must beprocessed in order since the score of all hypothe-ses considered for group g + 1 depend on hypothe-ses in groups 1, .
.
.
, g, which creates a large bottle-neck under the recommended settings of g = k..random sampling strategies for decoding neu-ral sequence models have received much attentionin recent years.
while techniques such as stochas-tic beam search and the uniquerandomizer (shiet al., 2020) are convenient for creating statis-tical estimators and have uses in reinforcementlearning techniques due to their clear probabilisticinterpretation, there are no diversity guaranteesfor the set of generated sequences..tam (2020).
likewise adapts beam search,proposing a k-means clustering version that clus-ters solutions by averaged word embeddings.
asthere lacks an interpretation of distance betweenaveraged word embeddings though, it is unclearif the method can explicitly optimize for anytangible notion of coverage or diversity..7 conclusion.
we propose determinantal beam search (detbs):a new way of framing beam search that allowsus to optimize set generation for diversity andcoverage rather than simply individual scores.
formally, we redeﬁne beam search as an iterativesubdeterminant maximization problems wherewe selectthe approximately maximizing setaccording to the psd matrix parameterizing ourscore function.
this gives us the ability to encodethe notion of intra-set diversity into the beam.
search optimization problem.
we discuss and ex-periment with efﬁcient methods for inference andkernel computation that make detbs an efﬁcientdecoding strategy in practice.
we use detbs inthe context of language generation, where weexplicitly encourage n-gram coverage throughthe string subsequence kernel.
in our nmtexperiments, we ﬁnd detbs generates much morediverse sets of strings than standard beam searchand stochastic beam search with a small trade-off in median bleu.
we observe competitiveperformance compared with diverse beam search..acknowledgements.
we would like to thank the anonymous reviewersfor their helpful feedback and recommendations..ethical considerations.
while language generation can be used for mali-cious purposes, e.g., to propagate misinformationor offensive text, we do not foresee any speciﬁcethical concerns with the techniques in this work..references.
soﬁane abbar, sihem amer-yahia, piotr indyk, sepi-deh mahabadi, and kasturi r. varadarajan.
2013.diverse near neighbor problem.
in proceedings ofthe twenty-ninth annual symposium on computa-tional geometry.
association for computing ma-chinery..pankaj k. agarwal, sariel har-peled, and kasturi r.varadarajan.
2004. approximating extent measuresof points.
journal of the association for computingmachinery, 51(4):606–635..loïc barrault, ondˇrej bojar, marta r. costa-jussà,christian federmann, mark fishel, yvette gra-ham, barry haddow, matthias huck, philipp koehn,shervin malmasi, christof monz, mathias müller,santanu pal, matt post, and marcos zampieri.
2019.findings of the 2019 conference on machine trans-lation.
in proceedings of the fourth conference on.
6558machine translation, pages 1–61.
association forcomputational linguistics..ondˇrej bojar, christian buck, christian federmann,barry haddow, philipp koehn, johannes leveling,christof monz, pavel pecina, matt post, hervesaint-amand, radu soricut, lucia specia, and aleštamchyna.
2014. findings of the 2014 workshopon statistical machine translation.
in proceedings ofthe ninth workshop on statistical machine trans-lation, pages 12–58.
association for computationallinguistics..laming chen, guoxin zhang, and eric zhou.
2018.fast greedy map inference for determinantal pointinprocess to improve recommendation diversity.
advances in neural information processing sys-tems, pages 5622–5633..j. b. ebrahimi, d. straszak, and n. k. vishnoi.
2017.subdeterminant maximization via nonconvex relax-in 2017 ieee 58thations and anti-concentration.
annual symposium on foundations of computerscience (focs), pages 1020–1031.
ieee computersociety..sergey edunov, myle ott, michael auli, and davidgrangier.
2018. understanding back-translation atin proceedings of the 2018 conference onscale.
empirical methods in natural language process-ing, pages 489–500.
association for computationallinguistics..bryan eikema and wilker aziz.
2020..is map de-coding all you need?
the inadequacy of the modein proceedings ofin neural machine translation.
the 28th international conference on computationallinguistics, pages 4506–4520.
international com-mittee on computational linguistics..muhammad farhan, juvaria tariq, arif zaman, mu-dassir shabbir, and imdad ullah khan.
2017. ef-ﬁcient approximation algorithms for strings kernelbased sequence classiﬁcation.
in advances in neu-ral information processing systems, volume 30,pages 6935–6945.
curran associates, inc..jonas gehring, michael auli, david grangier, de-nis yarats, and yann n. dauphin.
2017. convolu-in proceed-tional sequence to sequence learning.
ings of the 34th international conference on ma-chine learning, volume 70, pages 1243–1252..jennifer gillenwater, alex kulesza, and ben taskar.
2012. near-optimal map inference for determinan-tal point processes.
in advances in neural informa-tion processing systems, volume 25, pages 2735–2743. curran associates, inc..ari holtzman, jan buys, li du, maxwell forbes, andyejin choi.
2020. the curious case of neural textin proceedings of the internationaldegeneration.
conference on learning representations..piotr indyk, sepideh mahabadi, mohammad mahdian,and vahab s. mirrokni.
2014. composable core-setsin pro-for diversity and coverage maximization.
ceedings of the thirty-third association for com-puting machinery sigmod-sigact-sigart sym-posium on principles of database systems, page100–108.
association for computing machinery..v. klee, p. gritzmann, and d. larman.
1995. largestj-simplices in n-polytopes.
discrete and computa-tional geometry, 13(3-4):477–516..chun-wa ko, jon lee, and maurice queyranne.
1995.an exact algorithm for maximum entropy sampling.
operations research, 43(4):684–691..wouter kool, herke van hoof, and max welling.
2019. stochastic beams and where to ﬁnd them:the gumbel-top-k trick for sampling sequencesin proceedings of the inter-without replacement.
national conference on machine learning, pages3499–3508..alex kulesza and ben taskar.
2011. k-dpps: ﬁxed-size determinantal point processes.
in proceedingsof the 28th international conference on machinelearning, pages 1193–1200..alex kulesza and ben taskar.
2012. determinantalpoint processes for machine learning.
now pub-lishers inc..chengtao li, stefanie jegelka, and suvrit sra.
2016.efﬁcient sampling for k-determinantal point pro-volume 51 of proceedings of machinecesses.
learning research, pages 1328–1337..zhifei li and jason eisner.
2009. first- and second-order expectation semirings with applications toinminimum-risk training on translation forests.
proceedings of the 2009 conference on empiricalmethods in natural language processing, pages40–51.
association for computational linguistics..huma lodhi, craig saunders, john shawe-taylor,nello cristianini, and chris watkins.
2002. textclassiﬁcation using string kernels.
journal of ma-chine learning research, 2:419–444..clara meister, ryan cotterell, and tim vieira.
2020.if beam search is the answer, what was the ques-in proceedings of the 2020 conference ontion?
empirical methods in natural language processing(emnlp), pages 2173–2185.
association for com-putational linguistics..insu han, prabhanjan kambadur, kyoungsoo park, andjinwoo shin.
2017. faster greedy map inferencefor determinantal point processes.
volume 70 ofproceedings of machine learning research, pages1384–1393..baharan mirzasoleiman, amin karbasi, rik sarkar,and andreas krause.
2013. distributed submodularmaximization: identifying representative elementsin massive data.
in advances in neural informationprocessing systems, volume 26, pages 2049–2057..6559yonghui wu, mike schuster, zhifeng chen, quoc v.le, mohammad norouzi, wolfgang macherey,maxim krikun, yuan cao, qin gao, klausmacherey, jeff klingner, apurva shah, melvinjohnson, xiaobing liu, lukasz kaiser, stephangouws, yoshikiyo kato, taku kudo, hidetokazawa, keith stevens, george kurian, nishantpatil, wei wang, cliff young, jason smith, jasonriesa, alex rudnick, oriol vinyals, gregory s.corrado, macduff hughes, and jeffrey dean.
2016.google’s neural machine translation system: bridg-ing the gap between human and machine translation..zhilin yang, zihang dai, yiming yang, jaime car-bonell, rusland salakhutdinov, and quoc v le.
2019. xlnet: generalized autoregressive pretrain-in advances ining for language understanding.
neural information processing systems, volume 32..nathan ng, kyra yee, alexei baevski, myle ott,michael auli, and sergey edunov.
2019. facebookfair’s wmt19 news translation task submission.
in proceedings of the fourth conference on ma-chine translation (volume 2: shared task papers,day 1), pages 314–319.
association for computa-tional linguistics..myle ott, sergey edunov, alexei baevski, angelafan, sam gross, nathan ng, david grangier, andfairseq: a fast, extensiblemichael auli.
2019.in proceedings oftoolkit for sequence modeling.
the 2019 conference of the north american chap-ter of the association for computational linguis-tics (demonstrations), pages 48–53.
association forcomputational linguistics..juho rousu and john shawe-taylor.
2005. efﬁcientcomputation of gapped substring kernels on largealphabets.
journal of machine learning research,6:1323–1344..iulian vlad serban, tim klinger, gerald tesauro, kar-tik talamadupula, bowen zhou, yoshua bengio,and aaron courville.
2017. multiresolution recur-rent neural networks: an application to dialogueresponse generation.
in proceedings of the thirty-first aaai conference on artiﬁcial intelligence,page 3288–3294.
aaai press..kensen shi, david bieber, and charles sutton.
2020.incremental sampling without replacement for se-in proceedings of the 37th inter-quence models.
national conference on machine learning, volume119 of proceedings of machine learning research,pages 8785–8795.
pmlr..si si, cho-jui hsieh, and inderjit s. dhillon.
2017.memory efﬁcient kernel approximation.
journal ofmachine learning research, 18(20):1–32..felix stahlberg and bill byrne.
2019. on nmt searchinerrors and model errors: cat got your tongue?
proceedings of the 2019 conference on empiricalmethods in natural language processing and the9th international joint conference on natural lan-guage processing (emnlp-ijcnlp), pages 3356–3362. association for computational linguistics..yik-cheung tam.
2020. cluster-based beam search forpointer-generator chatbot grounded by knowledge.
computer speech and language, 64:101094..ashwin vijayakumar, michael cogswell, ramprasaathselvaraju, qing sun, stefan lee, david crandall,and dhruv batra.
2018. diverse beam search forin aaaiimproved description of complex scenes.
conference on artiﬁcial intelligence, pages 7371–7379..qingzhong wang and antoni b. chan.
2019. to-wards diverse and accurate image captions viareinforcing determinantal point process.
corr,abs/1908.04919..6560a log-space computations.
algorithm 1 fast greedy map inference withlog-space parameterization (chen et al., 2018).
we transform computations according to (li andeisner, 2009).
input: l: log of psd matrix.
k: desired set size.
ci = [ ], di = lii, si = [ ]j = argmaxi∈s diyg = {j}while |yg| !
= k :for i ∈ s\yg :.
1: function greedy_map_inference( )2:3:4:5:6:7:8:9:10:11:12:13:14:15:16:.
ci = [ci.
else.
s, log_inner ← logsumexp(cj, ci, si, sj)func ← log_add if s < 0 else log_minusif lji > log_inner :.
s ← 1ei = func(lji, log_inner) − 0.5 · dj.
s ← −sei = func(log_inner, lji) − 0.5 · djei], di = di − 2 · ei, si = [si.
s].
j = argmaxi∈s\yg di, yg = yg ∪ {j}.
return yg.
s, log_inner ← 1, −∞for (cid:104)c1, c2, s1, s2(cid:105) ∈ (cid:104)c1, c2, s1, s2(cid:105) :.
17:18: function logsumexp(c1, c2, s1, s2)19:20:21:22:23:24:25:26:27:.
else.
s = s(cid:48)log_inner ← func(c1 + c2, log_inner).
s(cid:48) ← s1 · s2func ← log_add if s == s(cid:48) else log_minusif log_inner > c1 + c2 :.
log_inner ← func(log_inner, c1 + c2).
return s, log_inner.
b experimental setup.
hyperparameters.
as we use the string subse-quence kernel of section §4 in detbs, there are anumber of hyperparameters that can be adjustedbeyond the diversity weight w: the decay factorλ indicates the degree to which interior gaps arepenalized and subsequence length n indicates thelength of the considered substrings u. for eachlanguage, we perform a search over these twohyperparameters for set size k = 10 and diver-sity coefﬁcient w = 0.1 on validation sets.
weuse a grid search over n = [2, 3, 4, 5, 6, 7, 8] andλ = [0.1, 0.3, 0.5, 0.7, 1.0].
we choose the conﬁg-uration that yields the highest (average n-gram di-versity)*bleu, using this conﬁguration in all sub-sequent experiments.
while there may be betterperforming hyperparameters under different k andw, we omit searching over the entire space to cre-ate a fairer comparison with the other decodingstrategies..dataset.
n decay (λ).
wmt’14 en–frwmt’19 de–en.
22.
0.30.1.table 3: best performing conﬁgurations found insearch over string subsequence kernel parameters..interestingly, larger values of n did not improveperformance, and were more computationally ex-pensive; small values of n and decay λ appear tooffer the best bleu vs. n-gram diversity trade-off..dataset and model statistics we use a con-volutional sequence-to-sequence modeltrainedaccording to gehring et al.
(2017) on the wmt’14en–fr dataset.16 data preprocessing steps, modelhyperparameters and baseline performances canbe found in their work.
we use the pre-trainedmodel checkpoints made available by fairseqathttps://github.com/pytorch/fairseq/tree/master/examples/translation.
we use atransformer-based model trained according to nget al.
(2019) on the wmt’19 de–en dataset.17likewise, data preprocessing steps, model hy-perparameters and baseline performances can befound in ng et al.
(2019).
we similarly use thepretrained model checkpoints made available byfairseq..c additional results.
figure 2: n-gram diversity vs. minimum, median andmaximum bleu score for beam sizes k = 5, 10, 20 onwmt’19 de–en newstest using a larger range ofthe diversity weight w..16available.
at.
http://statmt.org/wmt14/.
translation-task.html.
17available at http://www.statmt.org/wmt19/.
translation-task.html.
6561figure 3: n-gram diversity vs. minimum, median andmaximum bleu score for beam sizes k = 5, 10, 20 onwmt’14 en–fr newstest using a larger range of thediversity weight w..6562