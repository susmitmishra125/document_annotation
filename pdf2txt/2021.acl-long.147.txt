poisoning knowledge graph embeddings via relation inference patterns.
peru bhardwaj1.
john kelleher2∗ luca costabello3∗ declan o’sullivan1∗.
1 adapt centre, trinity college dublin, ireland2 adapt centre, tu dublin, ireland3 accenture labs, irelandperu.bhardwaj@adaptcentre.ie.
abstract.
we study the problem of generating data poi-soning attacks against knowledge graph em-bedding (kge) models for the task of link pre-diction in knowledge graphs.
to poison kgemodels, we propose to exploit their inductiveabilities which are captured through the rela-tionship patterns like symmetry, inversion andcomposition in the knowledge graph.
speciﬁ-cally, to degrade the model’s prediction conﬁ-dence on target facts, we propose to improvethe model’s prediction conﬁdence on a set ofdecoy facts.
thus, we craft adversarial addi-tions that can improve the model’s predictionconﬁdence on decoy facts through differentinference patterns.
our experiments demon-strate that the proposed poisoning attacks out-perform state-of-art baselines on four kgemodels for two publicly available datasets.
wealso ﬁnd that the symmetry pattern based at-tacks generalize across all model-dataset com-binations which indicates the sensitivity ofkge models to this pattern..1.introduction.
knowledge graph embeddings (kge) are increas-ingly deployed in domains with high stake deci-sion making like healthcare and ﬁnance (noy et al.,2019), where it is critical to identify the potential se-curity vulnerabilities that might cause failure.
butthe research on adversarial vulnerabilities of kgemodels has received little attention.
we study theadversarial vulnerabilities of kge models throughdata poisoning attacks.
these attacks craft inputperturbations at training time that aim to subvertthe learned model’s predictions at test time..poisoning attacks have been proposed for mod-els that learn from other graph modalities (xu et al.,2020) but they cannot be applied directly to kgemodels.
this is because they rely on gradients of.
∗equal contribution by last authors..figure 1: composition based adversarial attack on frauddetection.
the knowledge graph consists of two types of en-tities - person and bankaccount.
the target triple to predictis (karl, affiliated with, joe the mobster).
originalkge model predicts this triple as true.
but a malicious at-tacker adds adversarial triples (in purple) that connect karlwith a non-suspicious person bob through composition pat-tern.
now, the kge model predicts the target triple as false..all possible entries in a dense adjacency matrixand thus, do not scale to large knowledge graphswith multiple relations.
the main challenge in de-signing poisoning attacks for kge models is thelarge combinatorial search space of candidate per-turbations which is of the order of millions forbenchmark knowledge graphs with thousands ofnodes.
two recent studies (zhang et al., 2019a;pezeshkpour et al., 2019) attempt to address thisproblem through random sampling of candidateperturbations (zhang et al., 2019a) or through avanilla auto-encoder that reconstructs discrete enti-ties and relations from latent space (pezeshkpouret al., 2019).
however, random sampling dependson the number of candidates being sampled and theauto-encoder proposed in pezeshkpour et al.
(2019)is only applicable to multiplicative kge models..in this work, we propose to exploit the inductiveabilities of kge models to craft poisoned examplesagainst the model.
the inductive abilities of kgemodels are expressed through different connectiv-.
proceedingsofthe59thannualmeetingoftheassociationforcomputationallinguisticsandthe11thinternationaljointconferenceonnaturallanguageprocessing,pages1875–1888august1–6,2021.©2021associationforcomputationallinguistics1875ity patterns like symmetry, inversion and compo-sition between relations in the knowledge graph.
we refer to these as inference patterns.
we focuson the task of link prediction using kge modelsand consider the adversarial goal of degrading thepredicted rank of target missing facts.
to degradethe ranks of target facts, we propose to carefullyselect a set of decoy facts and exploit the inferencepatterns to improve performance on this decoy set.
figure 1 shows an example of the use of composi-tion pattern to degrade kge model’s performance.
we explore a collection of heuristic approachesto select the decoy triples and craft adversarial per-turbations that use different inference patterns toimprove the model’s predictive performance onthese decoy triples.
our solution addresses the chal-lenge of large candidate space by breaking downthe search space into smaller steps - (i) determin-ing adversarial relations; (ii) determining the decoyentities that most likely violate an inference pat-tern; and (iii) determining remaining adversarialentities in the inference pattern that are most likelyto improve the rank of decoy triples..we evaluate the proposed attacks on four state-of-art kge models with varied inductive abilities- distmult, complex, conve and transe.
we usetwo publicly available benchmark datasets for linkprediction - wn18rr and fb15k-237.
compar-ison against the state-of-art poisoning attacks forkge models shows that our proposed attacks out-perform them in all cases.
we ﬁnd that the attacksbased on symmetry pattern perform the best andgeneralize across all model-dataset combinations.
thus, the main contribution of our research is aneffective method to generate data poisoning attacks,which is based on inference patterns captured bykge models.
through a novel reformulation of theproblem of poisoning kge models, we overcomethe existing challenge in the scalability of poison-ing attacks for kge models.
furthermore, theextent of effectiveness of the attack relying on aninference pattern indicates the kge model’s sensi-tivity to that pattern.
thus, our proposed poisoningattacks help in understanding the kge models..2 problem formulation.
for a set of entities e and a set of relations r,a knowledge graph is a collection of triples rep-resented as kg = {(s, r, o) | s, o ∈ e and r ∈r}, where s, r, o represent the subject, relationand object in a triple.
a knowledge graph em-.
modeldistmultcomplexconvetranse.
scoring function(cid:104)es, er, eo(cid:105)(cid:60)((cid:104)es, er, eo(cid:105))(cid:104)σ(vec(σ([er, es] ∗ ω))w), eo(cid:105)− (cid:107)es + er − eo(cid:107).
table 1: scoring functions fsro of the kge modelsused in this research.
for complex, es, er, eo ∈ ck;for the remaining models es, er, eo ∈ rk.
here, (cid:104)·(cid:105)denotes the tri-linear dot product; σ denotes sigmoid· de-activation function, ∗ denotes 2d convolution;notes conjugate for complex vectors, and 2d reshapingfor real vectors in conve model; (cid:107)·(cid:107) denotes l-p norm.
bedding (kge) model encodes entities and rela-tions to a low-dimensional continuous vector spacees, er, eo ∈ rk where k is the embedding di-mension.
to do so, it uses a scoring functionf : e × r × e → r which depends on the en-tity and relation embeddings to assign a score toeach triple fsro = f (es, er, eo).
table 1 shows thescoring functions of state-of-art kge models stud-ied in this research.
the embeddings are learnedsuch that the scores for true (existing) triples inthe knowledge graph are higher than the scores forfalse (non-existing) triples in the knowledge graph.
multiplicative vs additive interactions: thescoring functions of kge models exhibit mul-tiplicative or additive interactions (chandrahaset al., 2018).
the multiplicative models scoretriples through multiplicative interactions of sub-ject, relation and object embeddings.
the scor-ing function for these models can be expressed asfsro = e(cid:62)r f(es, eo) where the function f mea-sures the compatibility between the subject andobject embeddings and varies across different mod-els within this family.
distmult, complex andconve have such interactions.
on the other hand,additive models score triples through additive in-teractions of subject, relation and object embed-dings.
the scoring function for such models can beexpressed as fsro = − (cid:13)r(eo)(cid:13)(cid:13)m1(cid:13)where es, eo ∈ rke , er ∈ rkr and mr ∈ rke ×kris the projection matrix from entity space rke to re-lation space rkr.
transe has additive interactions.
inductive capacity of kge models: the gen-eral intuition behind the design of the scoringfunctions of kge models is to capture logicalproperties between relations from the observedfacts in the knowledge graph.
these logical prop-erties or inference patterns can then be used tomake downstream inferences about entities and re-lations.
for example, the relation is owned by.
r(es) + er − m2.
1876is inverse of the relation owns, and when the fact(account42, is owned by, karl) is true, then thefact (karl, owns, account42) is also true andvice versa.
a model that can capture inversionpattern can thus predict missing facts about ownsbased on observed facts about is owned by.
themost studied inference patterns in the current litera-ture are symmetry, inversion and composition sincethey occur very frequently in real-world knowledgegraphs.
in this work, we use these patterns to inves-tigate the adversarial vulnerability of kge models.
link prediction: since most of the existingknowledge graphs are incomplete, a standard usecase of kge models is to predict missing triples inthe kg.
this task is evaluated by an entity rankingprocedure.
given a test triple (s, r, o), the subjectentity is replaced by each entity from e in turn.
these replacements are referred to as synthetic neg-atives.
the kge model’s scoring function is usedto predict scores of these negative triples.
thescores are then sorted in descending order and therank of the correct entity is determined.
these stepsare repeated for the object entity of the triple..the state-of-art evaluation metrics for this taskare (i) mr which is the mean of the predicted ranks,(ii) mrr which is the mean of the reciprocals ofpredicted ranks and (iii) hits@n which count theproportion of correct entities ranked in top-n. inthe ﬁltered setting (bordes et al., 2013), negativetriples that already exist in the training, validationor test set are ﬁltered out.
that is, their scores areignored while computing the ranks.
depending onthe domain of use, either subject or object or bothranks of the test triple are used to determine themodel’s conﬁdence1 in predicting a missing link..poisoning attacks on kge models: we studypoisoning attacks for the task of link predictionusing kge models.
we focus on targeted attackswhere the attacker targets a speciﬁc set of missingtriples instead of the overall model performance.
we use the notation (s, r, o) for the target triple;in this case, s, o are the target entities and r isthe target relation.
the goal of an adversarial at-tacker is to degrade the ranks of missing tripleswhich are predicted highly plausible by the model.
the rank of a highly plausible target triple can bedegraded by improving the rank of less plausibledecoy triples.
for a target triple (s, r, o), the decoytriple for degrading the rank on object side wouldbe (s, r, o(cid:48)) and the decoy triple for degrading the.
1kge models do not provide model uncertainty estimates..rank on subject side would be (s(cid:48), r, o).
thus, theaim of the adversarial attacker is to select decoytriples from the set of valid synthetic negatives andcraft adversarial edits to improve their ranks.
theattacker does not add the decoy triple itself as anadversarial edit, rather chooses the adversarial ed-its that would improve the rank of a missing decoytriple through an inference pattern..threat model: to ensure reliable vulnerabilityanalysis, we use a white-box attack setting wherethe attacker has full knowledge of the target kgemodel (joseph et al., 2019).
they cannot manipu-late the model architecture or learned embeddingsdirectly; but only through addition of triples to thetraining data.
we focus on adversarial additionswhich are more challenging to design than adver-sarial deletions for sparse knowledge graphs2..as in prior studies (pezeshkpour et al., 2019;zhang et al., 2019a), the attacker is restricted tomaking edits only in the neighbourhood of targetentities.
they are also restricted to 1 decoy triplefor each entity of the target triple.
furthermore,because of the use of ﬁltered settings for kgeevaluation, the attacker cannot add the decoy tripleitself to the training data (which intuitively wouldbe a way to improve the decoy triple’s rank)..3 poisoning knowledge graph.
embeddings through relationinference patterns.
since the inference patterns on the knowledgegraph specify a logic property between the rela-tions, they can be expressed as horn clauses whichis a subset of fol formulae.
for example, a prop-erty represented in the form ∀x, y : (x, owns, y) ⇒(y, is owned by, x) means that two entities linkedby relation owns are also likely to be linked by theinverse relation is owned by.
in this expression,the right hand side of the implication ⇒ is referredto as the head and the left hand side as the body ofthe clause.
using such expressions, we deﬁne thethree inference patterns used in our research..deﬁnition 3.1. the symmetry pattern ps is ex-pressed as ∀x, y : (x, r, y) ⇒ (y, r, x).
here, therelation r is symmetric relation..2for every target triple, the possible number of adversarialadditions in the neighbourhood of each entity are e × r.for the benchmark dataset fb15k-237, this is of the orderof millions; whereas the maximum number of candidates foradversarial deletion are of the order of thousands..1877deﬁnition 3.2. the inversion pattern pi is ex-pressed as ∀x, y : (x, ri, y) ⇒ (y, r, x).
here,the relations ri and r are inverse of each other..deﬁnition 3.3. the composition pattern pc isexpressed as ∀x, y, z : (x, r1, z) ∧ (z, r2, y) ⇒(x, r, y).
here, the relation r is a composition ofr1 and r2 ; and the ∧ is the conjunction operatorfrom relational logic..the mapping g : v → e of variables v in theabove expressions to entities e is called a ground-ing.
for example, we can map the logic expres-sion ∀x, y : (x, owns, y) ⇒ (y, is owned by, x)to the grounding (karl, owns, account42) ⇒(account42, is owned by, karl).
thus, a kgemodel that captures the inversion pattern will as-sign a high prediction conﬁdence to the head atomwhen the body of the clause exists in the graph..in the above expressions, the decoy triple be-comes the head atom and adversarial edits are thetriples in the body of the expression.
since thedecoy triple is an object or subject side negativeof the target triple, the attacker already knows therelation in the head atom.
they now want to de-termine (i) the adversarial relations in the body ofthe expression; (ii) the decoy entities which willmost likely violate the inference pattern for thechosen relations and; (iii) the remaining entities inthe body of the expression which will improve theprediction on the chosen decoy triple.
notice thatthe attacker needs all three steps for compositionpattern only; for inversion pattern, only the ﬁrsttwo steps are needed; and for symmetry pattern,only the second step is needed.
below we describeeach step in detail.
a computational complexityanalysis of all the steps is available in appendix a..3.1 step1: determine adversarial relations.
expressing the relation patterns as logic expres-sions is based on relational logic and assumes thatthe relations are constants.
thus, we use an al-gebraic approach to determine the relations in thehead and body of a clause.
given the target relationr, we determine the adversarial relations using analgebraic model of inference (yang et al., 2015)..inversion: if an atom (x, r, y) holds true, thenfor the learned embeddings in multiplicative mod-els, we can assume ex ◦ er ≈ ey; where ◦ denotesthe hadamard (element-wise) product.
if the atom(y, ri, x) holds true as well, then we can also as-sume ey ◦ eri ≈ ex.
thus, er ◦ eri ≈ 1 for inverserelations r and ri when embeddings are learned.
from multiplicative models.
we obtain a similarexpression er + eri ≈ 0 when embeddings arelearned from additive models..thus, to determine adversarial relations for in-version pattern, we use the pre-trained embeddingsto select ri that minimizes | erietr − 1 | for multi-plicative models; and ri that minimizes | eri +er |for additive models.
composition:.
if two atoms (x, r1, y) and(y, r2, z) hold true, then for multiplicative mod-els, ex ◦ er1 ≈ ey and ey ◦ er2 ≈ ez.
therefore,ex ◦ (er1 ◦ er2) ≈ ez.
hence, relation r is a compo-sition of r1 and r2 if er1 ◦ er2 ≈ er.
similarly, forembeddings from additive models, we can modelcomposition as er1 + er2 ≈ er..thus, to determine adversarial relations for com-position pattern, we use pre-trained embeddings toobtain all possible compositions of (r1, r2).
formultiplicative models, we use er1 ◦ er2 and for ad-ditive models we use er1 + er2.
from these, wechoose the relation pair for which the euclidean dis-tance between the composed relation embeddingsand the target relation embedding er is minimum..3.2 step2: determine decoy entities.
we consider three different heuristic approachesto select the decoy entity - soft truth score, rankspredicted by the kge model and cosine distance..soft logical modelling of inference patternsonce the adversarial relations are determined, wecan express the grounding for symmetry, inversionand composition patterns for the decoy triples.
wediscuss only object side decoy triple for brevity -.
gs : (o(cid:48), r, s) ⇒ (s, r, o(cid:48))gi : (o(cid:48), ri, s) ⇒ (s, r, o(cid:48))gc : (s, r1, o(cid:48)(cid:48)) ∧ (o(cid:48)(cid:48), r2, o(cid:48)) ⇒ (s, r, o(cid:48)).
if the model captures ps, pi or pc to assign highrank to the target triple, then the head atom (s, r, o(cid:48))of a grounding that violates this pattern is a suitabledecoy triple.
adding the body of this groundingto the knowledge graph would improve the modelperformance on decoy triple through ps, pi or pc.
to determine the decoy triple this way, we needa measure of the degree to which a grounding sat-isﬁes an inference pattern.
we call this measurethe soft truth score φ : g → [0, 1] - it provides thetruth value of a logic expression indicating the de-gree to which the expression is true.
we model thesoft truth score of grounded patterns using t-normbased fuzzy logics (h´ajek, 1998)..1878the score fsro of an individual atom (i.e.
triple)is computed using the kge model’s scoring func-tion.
we use the sigmoid function σ(x) = 1/(1 +exp(−x)) to map this score to a continuous truthvalue in the range (0, 1).
hence, the soft truth scorefor an individual atom is φ(s, r, o) = σ(fsro).
thesoft truth score for the grounding of a pattern canthen be expressed through logical composition (e.g.
∧ and ⇒) of the scores of individual atoms in thegrounding.
we follow (guo et al., 2016, 2018)and deﬁne the following compositions for logicalconjunction (∧), disjunction (∨), and negation (¬):.
adversarial attack step.
sym inv.
com.
determine adversarial relations.
determine decoy entities.
determine adversarial entities.
n/a.
sftrnkcos.n/a.
alg.
sftrnkcos.n/a.
alg.
sftrnkcos.sft.
table 2: a summary of heuristic approaches used fordifferent steps of the adversarial attack with symmetry(sym), inversion (inv) and composition (com) pattern.
alg denotes the algebraic model for inference patterns;sft denotes the soft truth score; rnk denotes the kgeranks; and cos denotes the cosine distance..φ(a ∧ b) = φ(a) · φ(b),.
φ(a ∨ b) = φ(a) + φ(b) − φ(a) · φ(b),.
φ(¬a) = 1 − φ(a)..here, a and b are two logical expressions, whichcan either be single triples or be constructed bycombining triples with logical connectives.
if a isa single triple (s, r, o), we have φ(a) = φ(s, r, o).
given these compositions, the truth value of anylogical expression can be calculated recursively(guo et al., 2016, 2018)..thus, we obtain the following soft truth scoresfor the groundings of symmetry, inversion and com-position patterns gs, gi and gc -.
φ(gs) = φ(o(cid:48), r, s) · φ(s, r, o(cid:48)) − φ(o(cid:48), r, s) + 1φ(gi) = φ(o(cid:48), ri, s) · φ(s, r, o(cid:48)) − φ(o(cid:48), ri, s) + 1.φ(gc) = φ(s, r1, o(cid:48)(cid:48)) · φ(o(cid:48)(cid:48), r2, o(cid:48)) · φ(s, r, o(cid:48))− φ(s, r1, o(cid:48)(cid:48)) · φ(o(cid:48)(cid:48), r2, o(cid:48)) + 1.to select the decoy triple (s, r, o(cid:48)) for symmetryand inversion, we score all possible groundings us-ing φ(gs) and φ(gi).
the head atom of groundingwith minimum score is chosen as decoy triple..for composition pattern, the soft truth scoreφ(gc) for candidate decoy triples (s, r, o(cid:48)) containstwo entities (o(cid:48), o(cid:48)(cid:48)) to be identiﬁed.
thus, we usea greedy approach to select the decoy entity o(cid:48).
weuse the pre-trained embeddings to group the enti-ties o(cid:48)(cid:48) into k clusters using k-means clustering anddetermine a decoy entity with minimum soft truthscore for each cluster.
we then select the decoyentity o(cid:48) with minimum score across the k clusters..kge ranks: we use the ranking protocol fromkge evaluation to rank the target triple againstvalid subject and object side negatives (s(cid:48), r, o) and(s, r, o(cid:48)).
for each side, we select the negativetriple that is ranked just below the target triple (that.
is, negative rank = target rank + 1).
theseare suitable as decoy because their predicted scoresare likely not very different from the target triple’sscore.
thus, the model’s prediction conﬁdencefor these triples might be effectively manipulatedthrough adversarial additions.
this is in contrast tovery low ranked triples as decoy; where the modelhas likely learnt a low score with high conﬁdence..cosine distance: a high rank for the targettriple (s, r, o) against queries (s, r, ?)
and (?, r, o)indicates that es, eo are similar to the embeddingsof other subjects and objects related by r in thetraining data.
thus, a suitable heuristic for select-ing decoy entities s(cid:48) and o(cid:48) is to choose ones whoseembeddings are dissimilar to es, eo.
since theseentities are not likely to occur in the neighbourhoodof o and s, they will act adversarially to reduce therank of target triple.
thus, we select decoy entitiess(cid:48) and o(cid:48) that have maximum cosine distance fromtarget entities s and o respectively..3.3 step3: determine adversarial entities.
this step is only needed for the composition patternbecause the body for this pattern has two adversar-ial triples.
given the decoy triple in the head ofthe composition expression, we select the body ofthe expression that would maximize the rank of thedecoy triple.
we use the soft-logical model deﬁnedin step 2 for selecting decoy triples.
the soft truthscore for composition grounding of decoy tripleis given by φ(gt) = φ(s, r1, o(cid:48)(cid:48)) · φ(o(cid:48)(cid:48), r2, o(cid:48)) ·φ(s, r, o(cid:48)) − φ(s, r1, o(cid:48)(cid:48)) · φ(o(cid:48)(cid:48), r2, o(cid:48)) + 1. weselect the entity o(cid:48)(cid:48) with maximum score becausethis entity satisﬁes the composition pattern for thedecoy triple and is thus likely to improve the decoytriple’s ranks on addition to the knowledge graph..1879wn18rr fb15k-237.
entitiesrelationstrainingvalidationtest.
target.
distmultcomplexconvetranse.
40,5591186,8352,8242,924.
1,3151,3691,2471,195.
14,505237272,11517,52620,438.
3,3423,9304,7115,359.table 3: statistics for the datasets wn18rr and fb15k-237. we removed triples from the validation and test set thatcontained unseen entities to ensure that we do not add newentities as adversarial edits.
the numbers above (includingthe number of entities) reﬂect this ﬁltering..4 evaluation.
the aim of our evaluation is to assess the effective-ness of proposed attacks in degrading the predictiveperformance of kge models on missing triples thatare predicted true.
we use the state-of-art evalua-tion protocol for data poisoning attacks (xu et al.,2020).
we train a clean model on the original data;then generate the adversarial edits and add them tothe dataset; and ﬁnally retrain a new model on thispoisoned data.
all hyperparameters for training onoriginal and poisoned data remain the same..we evaluate four models with varying inductiveabilities - distmult, complex, conve and transe;on two publicly available benchmark datasets forlink prediction3- wn18rr and fb15k-237.
weﬁlter out triples from the validation and test setthat contain unseen entities.
to assess the attackeffectiveness in degrading performance on triplespredicted as true, we need a set of triples that arepredicted as true by the model.
thus, we select astarget triples, a subset of the original test set whereeach triple is ranked ≤ 10 by the original model.
table 3 provides an overview of dataset statisticsand the number of target triples selected..baselines: we compare the proposed methodsagainst the following baselines -.
random n: random edits in the neighbourhood.
of each entity of the target triple..random g1: global random edits in the knowl-edge graph which are not restricted to the neigh-bourhood of entities in the target triple and have 1edit per decoy triple (like symmetry and inversion).
random g2: global random edits in the knowl-edge graph which are not restricted to the neigh-.
bourhood of entities in the target triple and have 2edits per decoy triple (like composition)..zhang et al.
: poisoning attack from (zhang et al.,2019a) for edits in the neighbourhood of subject ofthe target triple.
we extend it for both subject andobject to match our evaluation protocol.
furtherimplementation details available in appendix b.2.
criage: poisoning attack from (pezeshkpouret al., 2019).
we use the publicly available im-plementation and the default attack settings4.
themethod was proposed for edits in the neighbour-hood of object of the target triple.
we extend it forboth entities to match our evaluation protocol andto ensure fair evaluation..implementation: for every attack, we ﬁlter outadversarial edit candidates that already exist in thegraph.we also remove duplicate adversarial editsfor different targets before adding them to the orig-inal dataset.
for step 2 of the composition attackwith ground truth, we use the elbow method to de-termine the number of clusters for each model-datacombination.
further details on kge model train-ing, computing resources and number of clustersare available in appendix b. the source code toreproduce our experiments is available on github5..4.1 results.
table 4 and 5 show the reduction in mrr andhits@1 due to different attacks on the wn18rrand fb15k-237 datasets.
we observe that the pro-posed adversarial attacks outperform the randombaselines and the state-of-art poisoning attacks forall kge models on both datasets..we see that the attacks based on symmetry in-ference pattern perform the best across all model-dataset combinations.
this indicates the sensitivityof kge models to symmetry pattern.
for dist-mult, complex and conve, this sensitivity can beexplained by the symmetric nature of the scoringfunctions of these models.
that is, the models as-sign either equal or similar scores to triples that aresymmetric opposite of each other.
in the case oftranse, the model’s sensitivity to symmetry patternis explained by the translation operation in scoringfunction.
the score of target (s, r, o) is a transla-tion from subject to object embedding through therelation embedding.
symmetry attack adds the ad-versarial triple (o(cid:48), r, s) where the relation is same.
4https://github.com/pouyapez/criage5https://github.com/perubhardwaj/.
3https://github.com/timdettmers/conve.
inferenceattack.
1880distmult.
complex.
conve.
transe.
hits@1 mrr.
hits@1 mrr.
hits@1 mrr.
hits@1.
0.89.
0.84.
0.92.original.
baselineattacks.
proposedattacks.
mrr.
0.90.
0.86 (-4%).
random nrandom g1 0.88random g2 0.88zhang et al.
0.82 (-8%)criage.
0.87.
0.85.
0.830.830.830.810.84.sym truthsym ranksym cosinv truthinv rankinv coscom truthcom rankcom cos.0.660.400.320.610.57 (-36%) 0.320.830.870.830.860.820.83 (-8%)0.830.860.800.85 (-5%)0.770.86.
0.84 (-6%)0.800.880.830.830.880.76 (-14%) 0.74-.
-.
0.56 (-33%) 0.240.56 (-33%) 0.240.430.620.800.860.850.800.80 (-10%) 0.790.810.860.770.830.700.82 (-8%).
0.90 (-2%)0.920.910.90 (-2%)0.90.
0.89.
0.880.890.890.870.88.
0.61 (-34%) 0.280.310.620.440.670.870.900.850.89 (-4%)0.880.900.860.890.840.890.830.88(-4%).
0.36.
0.03.
0.28 (-20%)0.010.350.020.020.340.24 (-33%) 0.01-.
-.
0.570.360.020.250.24 (-33%) 0.010.030.340.020.250.010.25 (-30%)0.53 (+49%) 0.270.570.320.53 (+49%) 0.27.table 4: reduction in mrr and hits@1 due to different attacks on the target split of wn18rr.
first block of rows are thebaseline attacks with random edits; second block is state-of-art attacks; remaining are the proposed attacks.
for each block, wereport the best relative percentage difference from original mrr; computed as (original − poisoned)/original ∗ 100. lowervalues indicate better results; best results for each model are in bold.
statistics on the target split are in table 3..as the target relation, but target subject is the objectof adversarial triple.
now, the model learns theembedding of s as a translation from o(cid:48) through re-lation r. this adversarially modiﬁes the embeddingof s and in turn, the score of (s, r, o)..we see that inversion and composition attacksalso perform better than baselines in most cases, butnot as good as symmetry.
this is particularly truefor fb15k-237 where the performance for thesepatterns is similar to random baselines.
for thecomposition pattern, it is likely that the model hasstronger bias for shorter and simpler patterns likesymmetry and inversion than for composition.
thismakes it harder to deceive the model through com-position than through symmetry or inverse.
further-more, fb15k-237 has high connectivity (dettmerset al., 2018) which means that a kge model relieson a high number of triples to learn target triples’ranks.
thus, poisoning kge models for fb15k-237 will likely require more adversarial triples pertarget triple than that considered in this research..the inversion pattern is likely ineffective on thebenchmark datasets because these datasets do nothave any inverse relations (dettmers et al., 2018;toutanova and chen, 2015).
this implies that ourattacks cannot identify the inverse of the targettriple’s relation in step 1. we investigate this hy-pothesis further in appendix d, and evaluate theattacks on wn18 dataset where the inverse rela-tions have not been ﬁltered out.
this means thatthe kge model can learn the inversion pattern and.
the inversion attacks can identify the inverse ofthe target relation.
in this setting, we ﬁnd that theinversion attacks outperform other attacks againstcomplex on wn18, indicating the sensitivity ofcomplex to the inversion pattern when the datasetcontains inverse relations..an exception in the results is the compositionpattern on transe where the model performanceimproves instead of degrading on the target triples.
this is likely due to the model’s sensitivity to com-position pattern such that adding this pattern im-proves the performance on all triples, including tar-get triples.
to verify this, we checked the change inranks of decoy triples and found that compositionattacks on transe improve these ranks too.
re-sults for this experiment are available in appendixc. this behaviour of composition also indicatesthat the selection of adversarial entities in step 3of the composition attacks can be improved.
italso explains why the increase is more signiﬁcantfor wn18rr than fb15k-237 - wn18rr doesnot have any composition relations but fb15k-237does; so adding these to wn18rr shows signif-icant improvement in performance.
we aim toinvestigate these and more hypotheses about theproposed attacks in future work..5 related work.
kge models can be categorized into tensor factor-ization models like distmult (yang et al., 2015)and complex (trouillon et al., 2016), neural archi-.
1881original.
baselineattacks.
proposedattacks.
distmult.
complex.
conve.
transe.
mrr.
0.61.hits@1 mrr.
hits@1 mrr.
hits@1 mrr.
hits@1.
0.38.
0.61.
0.45.
0.61.
0.45.
0.63.
0.48.random n0.54 (-11%) 0.40random g1 0.540.40random g2 0.550.41zhang et al.
0.53 (-13%) 0.390.410.54criage.
sym truthsym ranksym cosinv truthinv rankinv coscom truthcom rankcom cos.0.510.360.390.530.46 (-25%) 0.310.410.550.560.430.54 (-11%) 0.400.420.560.420.56 (-8%)0.430.56 (-8%).
0.54 (-12%) 0.400.550.410.400.550.51 (-16%) 0.38-.
-.
0.560.410.380.530.51 (-17%) 0.380.400.540.550.400.53 (-14%) 0.390.550.410.55 (-11%) 0.400.420.56.
0.56 (-8%)0.410.570.430.420.570.54 (-11%) 0.390.410.56.
0.51 (-17%) 0.340.380.550.370.520.410.560.400.55 (-9%)0.420.560.430.570.410.56 (-8%)0.420.56.
0.60 (-4%)0.450.620.460.460.610.57 (-10%) 0.42-.
-.
0.620.480.53 (-16%) 0.360.400.550.460.620.420.58 (-8%)0.440.590.510.650.480.690.490.63 (0%).
table 5: reduction in mrr and hits@1 due to different attacks on the target split of fb15k-237.
for each block of rows, wereport the best relative percentage difference from original mrr; computed as (original − poisoned)/original ∗ 100. lowervalues indicate better results; best results for each model are in bold.
statistics on the target split are in table 3..tectures like conve (dettmers et al., 2018) andtranslational models like transe (bordes et al.,2013).
we refer the reader to (cai et al., 2018)for a comprehensive survey.
due to the black-boxnature of kge models, there is an emerging litera-ture on understanding these models.
(pezeshkpouret al., 2019) and (zhang et al., 2019a) are mostclosely related to our work as they propose otherdata poisoning attacks for kge models..minervini et al.
(2017) and cai and wang (2018)use adversarial regularization in latent space andadversarial training to improve predictive perfor-mance on link prediction.
but these adversarialsamples are not in the input domain and aim toimprove instead of degrade model performance.
poisoning attacks have also been proposed for mod-els for undirected and single relational graph datalike graph neural networks (z¨ugner et al., 2018;dai et al., 2018) and network embedding mod-els (bojchevski and g¨unnemann, 2019).
a surveyof poisoning attacks for graph data is available in(xu et al., 2020).
but the attacks for these modelscannot be applied directly to kge models becausethey require gradients of a dense adjacency matrix.
in the literature besides adversarial attacks,lawrence et al.
(2020), nandwani et al.
(2020)and zhang et al.
(2019b) generate post-hoc ex-planations to understand kge model predictions.
trouillon et al.
(2019) study the inductive abilitiesof kge models as binary relation properties forcontrolled inference tasks with synthetic datasets.
allen et al.
(2021) interpret the structure of knowl-.
edge graph embeddings by comparison with wordembeddings.
on the theoretical side, wang et al.
(2018) study the expressiveness of various bilinearkge models and guti´errez-basulto and schockaert(2018) study the ability of kge models to learnhard rules expressed as ontological knowledge..the soft-logical model of inference patterns inthis work is inspired by the literature on injectinglogical rules into kge models.
guo et al.
(2016)and guo et al.
(2018) enforce soft logical rules bymodelling the triples and rules in a uniﬁed frame-work and jointly learning embeddings from them.
additionally, our algebraic model of inference pat-terns, which is used to select adversarial relations,is related to approaches for graph traversal in latentvector space discussed in yang et al.
(2015); guuet al.
(2015); arakelyan et al.
(2021)..6 conclusion.
we propose data poisoning attacks against kgemodels based on inference patterns like symmetry,inversion and composition.
our experiments showthat the proposed attacks outperform the state-of-art attacks.
since the attacks rely on relation infer-ence patterns, they can also be used to understandthe kge models.
this is because if a kge modelis sensitive to a relation inference pattern, then thatpattern should be an effective adversarial attack.
we observe that the attacks based on symmetrypattern generalize across all kge models whichindicates their sensitivity to this pattern..1882in the future, we aim to investigate hypothesesabout the effect of input graph connectivity andexistence of speciﬁc inference patterns in datasets.
we note that such investigation of inference pat-tern attacks will likely be inﬂuenced by the choiceof datasets.
in this paper, we have used bench-mark datasets for link prediction.
while there areintuitive assumptions about the inference patternson these datasets, there is no study that formallymeasures and characterizes the existence of thesepatterns.
this makes it challenging to verify theclaims made about the inductive abilities of kgemodels, not only by our proposed attacks but alsoby new kge models proposed in the literature..thus, a promising step in understanding knowl-edge graph embeddings is to propose datasets andevaluation tasks that test varying degrees of speciﬁcinductive abilities.
these will help evaluate newmodels and serve as a testbed for poisoning attacks.
furthermore, speciﬁcations of model performanceon datasets with different inference patterns willimprove the usability of kge models in high-stakedomains like healthcare and ﬁnance..in addition to understanding model behaviour,the sensitivity of state-of-art kge models to simpleinference patterns indicates that these models canintroduce security vulnerabilities in pipelines thatuse knowledge graph embeddings.
thus, anotherpromising direction for future work is towards mit-igating the security vulnerabilities of kge models.
some preliminary ideas for this research can lookinto adversarial training; or training an ensembleof different kge scoring functions; or trainingan ensemble from subsets of the training dataset.
since our experiments show that state-of-art kgemodels are sensitive to symmetry pattern, we callfor future research to investigate neural architec-tures that generalize beyond symmetry even thoughtheir predictive performance for link prediction onbenchmark datasets might not be the best..acknowledgements.
this research was conducted with the ﬁnancial sup-port of accenture labs and science foundationireland (sfi) at the adapt sfi research centreat trinity college dublin.
the adapt sfi cen-tre for digital content technology is funded byscience foundation ireland through the sfi re-search centres programme and is co-funded underthe european regional development fund (erdf)through grant no.
13/rc/2106 p2..broader impact.
we study the problem of generating data poisoningattacks on kge models.
data poisoning attacksidentify the vulnerabilities in learning algorithmsthat could be exploited by an adversary to manip-ulate the model’s behaviour (joseph et al., 2019;biggio and roli, 2018).
such manipulation canlead to unintended model behaviour and failure.
identifying these vulnerabilities for kge modelsis critical because of their increasing use in do-mains that need high stakes decision making likeheathcare (bendtsen and petrovski, 2019) and ﬁ-nance (hogan et al., 2020; noy et al., 2019).
in thisway, our research is directed towards minimizingthe negative consequences of deploying state-of-art kge models in our society.
this honours theacm code of ethics of contributing to societalwell-being and acknowledging that all people arestakeholders in computing.
at the same time, weaim to safeguard the kge models against potentialharm from adversaries and thus honour the acmcode of avoiding harm due to computing systems.
arguably, because we study vulnerabilities byattacking the kge models, the proposed attackscan be used by an actual adversary to manipulatethe model behaviour of deployed systems.
thisparadox of an arms race is universal across securityresearch (biggio and roli, 2018).
for our research,we have followed the principle of proactive secu-rity as recommended by joseph et al.
(2019) andbiggio and roli (2018).
as opposed to reactivesecurity measures where learning system design-ers develop countermeasures after the system isattacked, a proactive approach anticipates such at-tacks, simulates them and designs countermeasuresbefore the systems are deployed.
thus, by reveal-ing the vulnerabilities of kge models, our researchprovides an opportunity to ﬁx them..besides the use case of security, our researchcan be used in understanding the inductive abilitiesof kge models, which are black-box and hard tointerpret.
we design attacks that rely on the induc-tive assumptions of a model to be able to deceivethat model.
thus, theoretically, the effectiveness ofattacks based on one inference pattern over anotherindicates the model’s reliance on one inference pat-tern over another.
however, as we discussed in ourpaper, realistically, it is challenging to make suchclaims about the inductive abilities of kge mod-els because the inference patterns in benchmarkdatasets are not well deﬁned..1883thus, we would encourage further work to eval-uate our proposed attacks by designing benchmarktasks and datasets that measure speciﬁc inductiveabilities of models.
this will not only be usefulfor evaluating the proposed attacks here, but alsofor understanding the inductive abilities of existingkge models.
this in turn, can guide the commu-nity to design better models.
in this direction, weencourage researchers proposing new kge modelsto evaluate not only the predictive performance onbenchmark datasets, but also the claims made oninductive abilities of these models and their robust-ness to violations of these implicit assumptions..references.
carl allen, ivana balazevic, and timothy hospedales.
2021. interpreting knowledge graph relation repre-sentation from word embeddings.
in internationalconference on learning representations..erik arakelyan, daniel daza, pasquale minervini, andmichael cochez.
2021. complex query answeringwith neural link predictors.
in international confer-ence on learning representations..claus bendtsen and slav´e petrovski.
2019. how dataand ai are helping unlock the secrets of disease.
inastrazeneca blog..battista biggio and fabio roli.
2018. wild patterns:ten years after the rise of adversarial machine learn-ing.
pattern recognition, 84:317–331..aleksandar bojchevski and stephan g¨unnemann.
2019.adversarial attacks on node embeddings via graphpoisoning.
in international conference on machinelearning, pages 695–704.
pmlr..antoine bordes, nicolas usunier, alberto garcia-jason weston, and oksana yakhnenko.
duran,2013. translating embeddings for modeling multi-relational data.
in c. j. c. burges, l. bottou,m. welling, z. ghahramani, and k. q. weinberger,editors, advances in neural information processingsystems 26, pages 2787–2795.
curran associates,inc..hongyun cai, vincent w zheng, and kevin chen-chuan chang.
2018. a comprehensive survey ofgraph embedding: problems, techniques, and appli-cations.
ieee transactions on knowledge and dataengineering..liwei cai and william yang wang.
2018. kbgan:adversarial learning for knowledge graph embed-dings.
in proceedings of the 2018 conference of thenorth american chapter of the association for com-putational linguistics: human language technolo-gies, volume 1 (long papers), pages 1470–1480,new orleans, louisiana.
association for computa-tional linguistics..chandrahas, aditya sharma, and partha talukdar.
towards understanding the geometry of2018.in proceedings ofknowledge graph embeddings.
the 56th annual meeting of the association for com-putational linguistics (volume 1: long papers),pages 122–131, melbourne, australia.
associationfor computational linguistics..hanjun dai, hui li, tian tian, xin huang, lin wang,jun zhu, and le song.
2018. adversarial attack onin international conferencegraph structured data.
on machine learning, pages 1115–1124.
pmlr..tim dettmers, pasquale minervini, pontus stenetorp,convolutional 2dand sebastian riedel.
2018.in proceedings ofknowledge graph embeddings.
the aaai conference on artiﬁcial intelligence, vol-ume 32..shu guo, quan wang, lihong wang, bin wang, andli guo.
2016. jointly embedding knowledge graphsand logical rules.
in proceedings of the 2016 con-ference on empirical methods in natural languageprocessing, pages 192–202, austin, texas.
associa-tion for computational linguistics..shu guo, quan wang, lihong wang, bin wang, andli guo.
2018. knowledge graph embedding with it-erative guidance from soft rules.
in proceedings ofthe aaai conference on artiﬁcial intelligence, vol-ume 32..v´ıctor guti´errez-basulto and steven schockaert.
2018.from knowledge graph embedding to ontology em-bedding?
an analysis of the compatibility betweenvector space representations and rules.
in principlesof knowledge representation and reasoning: pro-ceedings of the sixteenth international conference,kr 2018, tempe, arizona, 30 october - 2 november2018, pages 379–388.
aaai press..kelvin guu, john miller, and percy liang.
2015.traversing knowledge graphs in vector space.
inproceedings of the 2015 conference on empiricalmethods in natural language processing, pages318–327, lisbon, portugal.
association for compu-tational linguistics..petr h´ajek.
1998. metamathematics of fuzzy logic,volume 4. springer science & business media..aidan hogan, eva blomqvist, michael cochez, clau-dia d’amato, gerard de melo, claudio guti´errez,jos´e emilio labra gayo, sabrina kirrane, sebas-tian neumaier, axel polleres, roberto navigli, axel-cyrille ngonga ngomo, sabbir m. rashid, anisarula, lukas schmelzeisen, juan f. sequeda, steffenstaab, and antoine zimmermann.
2020. knowledgegraphs.
corr, abs/2003.02320..anthony d. joseph, blaine nelson, benjamin i. p. ru-binstein, and j. d. tygar.
2019. adversarial ma-chine learning.
cambridge university press..carolin lawrence, t. sztyler, and mathias niepert.
2020. explaining neural matrix factorization withgradient rollback.
arxiv, abs/2010.05516..1884pasquale minervini,.
timthomas demeester,rockt¨aschel, and sebastian riedel.
2017.ad-versarial sets for regularising neural link predictors.
in proceedings of the thirty-third conference onuncertainty in artiﬁcial intelligence, uai 2017,sydney, australia, august 11-15, 2017. auai press..yatin nandwani, ankesh gupta, aman agrawal,mayank singh chauhan, parag singla, and mausam.
2020. oxkbc: outcome explanation for factoriza-in auto-tion based knowledge base completion.
mated knowledge base construction..natasha noy, yuqing gao, anshu jain, anantnarayanan, alan patterson, and jamie taylor.
2019.industry-scale knowledge graphs: lessons and chal-lenges.
commun.
acm, 62(8):36–43..pouya pezeshkpour, yifan tian, and sameer singh.
2019.investigating robustness and interpretabilityof link prediction via adversarial modiﬁcations.
inconference of the north american chapter of the as-sociation for computational linguistics (naacl)..daniel rufﬁnelli, samuel broscheit, and rainergemulla.
2020. you can teach an old dog newtricks!
on training knowledge graph embeddings.
in international conference on learning represen-tations..kristina toutanova and danqi chen.
2015. observedversus latent features for knowledge base and textin proceedings of the 3rd workshop oninference.
continuous vector space models and their composi-tionality, pages 57–66, beijing, china.
associationfor computational linguistics..th´eo trouillon, ´eric gaussier, christopher r. dance,and guillaume bouchard.
2019. on inductive abili-ties of latent factor models for relational learning.
j.artif.
int.
res., 64(1):21–53..th´eo trouillon, johannes welbl, sebastian riedel, ´ericgaussier, and guillaume bouchard.
2016. com-in in-plex embeddings for simple link prediction.
ternational conference on machine learning, pages2071–2080..yanjie wang, rainer gemulla, and hui li.
2018. onmulti-relational link prediction with bilinear models.
in proceedings of the aaai conference on artiﬁcialintelligence, volume 32..han xu, yao ma, hao-chen liu, debayan deb, huiliu, ji-liang tang, and anil k jain.
2020. adversar-ial attacks and defenses in images, graphs and text:a review.
international journal of automation andcomputing, 17(2):151–178..bishan yang, wen-tau yih, xiaodong he, jianfenggao, and li deng.
2015. embedding entities andrelations for learning and inference in knowledgebases.
in 3rd international conference on learningrepresentations, iclr 2015, san diego, ca, usa,may 7-9, 2015, conference track proceedings..hengtong zhang, tianhang zheng, jing gao, chenglinmiao, lu su, yaliang li, and kui ren.
2019a.
datapoisoning attack against knowledge graph embed-ding.
in international joint conference on artiﬁcialintelligence..wen zhang, bibek paudel, wei zhang, abraham bern-stein, and huajun chen.
2019b.
interaction embed-dings for prediction and explanation in knowledgegraphs.
in proceedings of the twelfth acm interna-tional conference on web search and data mining,pages 96–104..daniel z¨ugner, amir akbarnejad,.
and stephang¨unnemann.
2018. adversarial attacks on neuralin international con-networks for graph data.
ference on knowledge discovery & data mining,pages 2847–2856..appendix.
a computational complexity analysis.
lets say e is the set of entities and r is the set ofrelations.
the number of target triples to attack is tand the speciﬁc target triple is (s, r, o).
here, wediscuss the computational complexity of the threesteps of the proposed attacks -.
determine adversarial relations:in this step,we determine the inverse relation or the composi-tion relation of a target triple.
to select inverserelation, we need r computations for every targettriple.
selecting composition relation requires thecomposition operation r2 times per target triple.
to avoid repetition, we pre-compute the inverseand composition relations for all target triples.
thisgives the complexity o(r2) for inverse relation.
for composition relation, we compute composi-tions of all relation pairs and then select the adver-sarial pair by comparison with target relation.
thisgives o(r2 + r) complexity for composition..determine decoy entity: the three heuristicsto compute the decoy entity are soft-truth score,kge ranks and cosine distance.
for symmetryand inversion, the soft truth score requires 2 for-ward calls to the model for one decoy entity.
forcomposition, if the number of clusters is k, thesoft truth score requires 3k forward calls to themodel.
to select decoy entities based on kgeranks, we require one forward call for each decoyentity.
for cosine distance, we compute the simi-larity of s and o to all entities via two calls to py-torch’s f.cosine similarity.
once the heuristicscores are computed, there is an additional com-plexity of o(e) to select the entity with minimum.
1885score.
thus, the complexity for decoy selection iso(te) for all heuristics except soft truth score oncomposition where it is o(kte)..determine adversarial entity: this step re-quires three forward calls to the kge model be-cause the ground truth score needs to be computed.
thus, the complexity for this step is o(te)..based on the discussion above, the overall com-putational complexity is o(te) for symmetry at-tacks and o(r2 + te) for inversion attacks.
forcomposition attacks, it is o(r2 +r+kte) for softtruth score and o(r2 + r + te) for kge ranksand cosine distance..b implementation details.
b.1 training kge modelsour codebase6 for kge model training is based onthe codebase from (dettmers et al., 2018)7. we usethe 1-k training protocol but without reciprocalrelations.
each training step alternates throughbatches of (s,r) and (o,r) pairs and their labels.
themodel implementation uses an if-statement for theforward pass conditioned on the input batch mode.
for transe scoring function, we use l2 normand a margin value of 9.0. the loss function usedfor all models is pytorch’s bcelosswithlogits.
for regularization, we use label smoothing and l2regularization for transe; and input dropout withlabel smoothing for remaining models.
we also usehidden dropout and feature dropout for conve..we do not use early stopping to ensure same hy-perparameters for original and poisoned kge mod-els.
we used an embedding size of 200 for all mod-els on both datasets.
for complex, this becomesan embedding size of 400 because of the real andimaginary parts of the embeddings.
all hyperpa-rameters are tuned manually based on suggestionsfrom state-of-art implementations of kge models(rufﬁnelli et al., 2020; dettmers et al., 2018).
thehyperparameter values for all model dataset com-binations are available in the codebase.
table 6shows the mrr and hits@1 for the original kgemodels on wn18rr and fb15k-237..for re-training the model on poisoned dataset,we use the same hyperparameters as the originalmodel.
we run all model training, adversarial at-tacks and evaluation on a shared hpc cluster withnvidia rtx 2080ti, tesla k40 and v100 gpus..6https://github.com/perubhardwaj/.
inferenceattack.
7https://github.com/timdettmers/conve.
wn18rr.
fb15k-237.
mrr hits@1 mrr hits@1.distmultcomplexconvetranse.
0.420.430.430.19.
0.390.400.400.02.
0.270.240.320.34.
0.190.200.230.25.table 6: mrr and hits@1 results for original kgemodels on wn18rr and fb15k-237.
wn18rr.
original high.
low.
distmultcomplexconvetranse.
distmultcomplexconvetranse.
0.900.890.920.36.
0.610.610.610.63.
0.820.760.900.25.
0.830.790.900.24.
0.550.510.540.57.
0.530.520.540.57.fb15k-237.
original high.
low.
table 7: mrr of kge models trained on originaldatasets and poisoned datasets from the attack in zhanget al.
(2019a).
high, low indicate the high and low per-centage of candidates used for attack..b.2 baseline implementation details.
one of the baselines in our evaluation is the attackfrom (zhang et al., 2019a).
it proposed edits in theneighbourhood of subject of the target triple.
weextend it for both subject and object to match ourevaluation protocol.
since no public implementa-tion is available, we implement our own..the attack is based on computing a perturbationscore for all possible candidate additions.
since thesearch space for candidate additions is of the or-der e × r, the attack uses random down samplingto ﬁlter out the candidates.
the percent of triplesdown sampled are not reported in the original paperand the implementation is not available.
so, in thispaper, we pick a high and a low value of the per-centage of triples down sampled and generate ad-versarial edits for both fractions.
the high and lowpercent values that were used to select candidateadversarial additions for wn18rr are distmult:(20.0, 5.0); complex: (20.0, 5.0); conve: (2.0,0.1); transe: (20.0, 5.0).
for fb15k-237, thesevalues are distmult: (20.0, 5.0); complex: (15.0,5.0); conve: (0.3, 0.1); transe: (20.0, 5.0).
thus, we generate two poisoned datasets fromthe attack - one that used a high number of candi-.
1886figure 2: mean of the relative increase in mrr of object and subject side decoy triples due to proposed attacks on wn18rrand fb15k-237.
the increase is computed relative to original mrr of decoy triples as (poisoned − original)/original.
thescale on y-axis is symmetric log scale.
higher values are better; as they show the effectiveness of attack in improving decoytriples’ ranks relative to their original ranks..dates and another that used a low number of can-didates.
we train two separate kge models onthese datasets to assess attack performance.
ta-ble 7 shows the mrr of the original model; andpoisoned kge models from attack with high andlow downsampling percents.
the results reportedfor this attack’s performance in section 4.1 are thebetter of the two results (which show more degra-dation in performance) for each combination..b.3 attack implementation details.
our proposed attacks involve three steps to gener-ate the adversarial additions for all target triples.
for step1 of selection of adversarial relations, wepre-compute the inversion and composition rela-tions for all target triples.
step2 and step3 arecomputed for each target triple in a for loop.
thesesteps involve forward calls to kge models to scoreadversarial candidates.
for this, we use a vector-ized implementation similar to kge evaluation pro-tocol.
we also ﬁlter out the adversarial candidatesthat already exist in the training set.
we furtherﬁlter out any duplicates from the set of adversarialtriples generated for all target triples..for the composition attacks with soft-truth score,we use the kmeans clustering implementation fromscikit − learn.
we use the elbow method on thegrid [5, 20, 50, 100, 150, 200, 250, 300, 350, 400,450, 500] to select the number of clusters.
thenumber of clusters selected for wn18rr are dist-mult: 300, complex: 100, conve: 300, transe:50. for fb15k-237, the numbers are distmult:.
200, complex: 300, conve: 300, transe: 100..c analysis on decoy triples.
the proposed attacks are designed to generate ad-versarial triples that improve the kge model per-formance on decoy triples (s, r, o(cid:48)) and (s(cid:48), r, o).
in this section, we analyze whether the perfor-mance of kge models improves or degrades overdecoy triples after poisoning.
for the decoy tripleson object side (s, r, o(cid:48)), we compute the changein object side mrr relative to the original objectside mrr of these triples.
similarly, for the decoytriples on subject side (s(cid:48), r, o), we compute thechange in subject side mrr relative to the originalsubject side mrr of these decoy triples.
figure 2shows plots for the mean change in mrr of objectand subject side decoy triples..we observed in section 4.1 that the compositionattacks against transe on wn18rr improved theperformance on target triples instead of degradingit.
in figure 2, we notice that composition attacksagainst transe are effective in improving the ranksof decoy triples on both wn18rr and fb15k-237.
this evidence supports the argument made in themain paper - it is likely that the composition at-tack does not work against transe for wn18rrbecause the original dataset does not contain anycomposition relations; thus adding this pattern im-proves model’s performance on all triples insteadof just the target triples because of the sensitivityof transe to composition pattern..1887distmult.
complex.
conve.
transe.
hits@1 mrr.
hits@1 mrr.
hits@1 mrr.
hits@1.original.
baselineattacks.
proposedattacks.
mrr.
0.82.
0.80 (-2%).
random nrandom g1 0.82random g2 0.81zhang et al.
0.77 (-6%)criage.
0.78.
0.67.
0.630.660.650.590.61.sym truthsym ranksym cosinv truthinv rankinv coscom truthcom rankcom cos.0.620.300.270.590.50 (-38%) 0.170.660.810.660.820.640.79 (-3%)0.620.790.640.800.610.78 (-5%).
0.99.
0.99 (0%)0.990.990.97 (-3%)-.
0.99.
0.980.980.980.95-.
0.900.820.89 (-10%) 0.790.850.920.860.740.84 (-16%) 0.680.750.870.970.980.960.980.950.97 (-2%).
0.80.
0.79 (-2%)0.800.790.77 (-3%)0.78.
0.63.
0.610.620.620.610.63.
0.58 (-17%) 0.270.330.620.350.600.610.78 (-3%)0.610.790.630.800.620.770.580.75 (-6%)0.620.77.
0.65.
0.45.
0.46 (-29%) 0.180.570.330.220.500.43 (-33%) 0.16-.
-.
0.740.600.340.520.41 (-37%) 0.130.340.590.550.340.51 (-22%) 0.250.53 (-18%) 0.250.470.670.320.58.table 8: reduction in mrr and hits@1 due to different attacks on the target split of wn18.
for each block of rows, we reportthe best relative percentage difference from original mrr; computed as (original − poisoned)/original ∗ 100. lower valuesindicate better results; best results for each model are in bold..d analysis on wn18.
the inversion attacks identify the relation that thekge model might have learned as inverse of thetarget triple’s relation.
but the benchmark datasetswn18rr and fb15k-237 do not contain inverserelations, and a kge model trained on these cleandatasets would not be vulnerable to inversion at-tacks.
thus, we perform additional evaluation onthe wn18 dataset where triples with inverse rela-tions have not been removed.
table 8 shows theresults for different adversarial attacks on wn18.
we see that the symmetry based attack is most ef-fective for distmult, conve and transe.
this indi-cates the sensitivity of these models to the symme-try pattern even when inverse relations are presentin the dataset.
for distmult and conve, this islikely due to the symmetric nature of their scoringfunctions; and for transe, this is likely because ofthe translation operation as discussed in section4.1. on the complex model, we see that thoughthe symmetry attacks are more effective than ran-dom baselines, the inversion attacks are the mosteffective.
this indicates that the complex modelis most sensitive to the inversion pattern when theinput dataset contains inverse relations..e analysis of runtime efﬁciency.
in this section, we compare the runtime efﬁciencyof the baseline and proposed attacks.
table 9 showsthe time taken (in seconds) to select the adversar-ial triples using different attack strategies for all.
distmult complex conve transe.
random nrandom g1random g2zhang et al.
criage.
sym truthsym ranksym cosinv truthinv rankinv coscom truthcom rankcom cos.10.088.2816.0194.4821.77.
19.6323.4722.5211.4315.2714.962749.6022.0434.78.
10.698.1615.82255.53-.
8.767.6418.72666.8521.96.
35.4027.2528.6215.6918.1420.47.
22.7625.8225.6924.1330.9923.021574.44 6069.7937.8132.37.
31.5368.06.
7.836.4913.3381.96-.
31.5925.0323.1331.8921.8220.63470.3420.8819.86.table 9: time taken in seconds to generate adversarialtriples using baseline and proposed attacks on wn18.
models on wn18 dataset.
similar patterns wereobserved for attack execution on other datasets..for criage, the reported time does not includethe time taken to train the auto-encoder model.
sim-ilarly, for soft-truth based composition attacks, thereported time does not include the time taken topre-compute the clusters.
we observe that the pro-posed attacks are more efﬁcient than the baselinezhang et al.
attack which requires a combinatorialsearch over the canidate adversarial triples; andhave comparable efﬁciency to criage.
amongthe different proposed attacks, composition attacksbased on soft-truth score take more time than othersbecause they select the decoy entity by computingthe soft-truth score for multiple clusters..1888