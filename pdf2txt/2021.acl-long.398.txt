pre-training universal language representation.
yian li1,2,3, hai zhao1,2,3,∗1 department of computer science and engineering, shanghai jiao tong university2 key laboratory of shanghai education commission for intelligent interactionand cognitive engineering, shanghai jiao tong university, shanghai, china3moe key lab of artiﬁcial intelligence, ai institute, shanghai jiao tong universityliya19@sjtu.edu.cn,zhaohai@cs.sjtu.edu.cn.
abstract.
despite the well-developed cut-edge represen-tation learning for language, most languagerepresentation models usually focus on spe-ciﬁc levels of linguistic units.
this workintroduces universal language representationlearning, i.e., embeddings of different levelsof linguistic units or text with quite diverselengths in a uniform vector space.
we pro-pose the training objective misad that utilizesmeaningful n-grams extracted from large un-labeled corpus by a simple but effective algo-rithm for pre-trained language models.
thenwe empirically verify that well designed pre-training scheme may effectively yield univer-sal language representation, which will bringgreat convenience when handling multiple lay-ers of linguistic objects in a uniﬁed way.
es-pecially, our model achieves the highest accu-racy on analogy tasks in different language lev-els and signiﬁcantly improves the performanceon downstream tasks in the glue benchmarkand a question answering dataset..1.introduction.
in this paper, we propose universal language repre-sentation (ulr) that uniformly embeds linguisticunits in different hierarchies in the same vectorspace.
a universal language representation modelencodes linguistic units such as words, phrases orsentences into ﬁxed-sized vectors and handles mul-tiple layers of linguistic objects in a uniﬁed way.
ulr learning may offer a great convenience whenconfronted with sequences of different lengths, es-pecially in tasks such as natural language under-standing (nlu) and question answering (qa),.
∗ corresponding author.
this paper was partially sup-ported by national key research and development pro-gram of china (no.
2017yfb0304100), key projects ofnational natural science foundation of china (u1836222and 61733011), huawei-sjtu long term ai project, cutting-edge machine reading comprehension and language model.
this work was supported by huawei noah’s ark lab..hence it is of great importance in both scientiﬁcresearch and industrial applications..as is well known, embedding representationfor a certain linguistic unit (i.e., word) en-ables linguistics-meaningful arithmetic calculationamong different vectors, also known as word anal-ogy (mikolov et al., 2013).
for example:.
king − man = queen − woman.
in fact, manipulating embeddings in the vectorspace reveals syntactic and semantic relations be-tween the original symbol sequences and this fea-ture is indeed useful in true applications.
for ex-ample, “london is the capital of england” can beformulized as:.
england + capital ≈ london.
then given two documents one of which contains“england” and “capital”, the other contains “lon-don”, we consider them relevant.
while a ulrmodel may generalize such good analogy featuresonto free text with all language levels involved to-gether.
for example, eat an onion : vegetable ::eat a pear : fruit..ulr has practical values in dialogue systems,by which human-computer communication will gofar beyond executing instructions.
one of the mainchallenges of dialogue systems is dialogue statetracking (dst).
it can be formulated as a semanticparsing task (cheng et al., 2020), namely, convert-ing natural language utterances with any lengthinto uniﬁed representations.
thus this is essen-tially a problem that can be conveniently solvedby mapping sequences with similar semantic mean-ings into similar representations in the same vectorspace according to a ulr model..another use of ulr is in the frequently askedquestions (faq) retrieval task, where the goal isto answer a user’s question by retrieving question.
proceedingsofthe59thannualmeetingoftheassociationforcomputationallinguisticsandthe11thinternationaljointconferenceonnaturallanguageprocessing,pages5122–5133august1–6,2021.©2021associationforcomputationallinguistics5122paraphrases that already have an answer from thedatabase.
such task can be accurately done by onlymanipulating vectors such as calculating and rank-ing vector distance (i.e., cosine similarity).
thecore is to embed sequences of different lengths inthe same vector space.
then a ulr model retrievesthe correct question-answer pair for the user queryaccording to vector distance..in this paper, we propose a universal languagerepresentation learning method that generates ﬁxed-sized vectors for sequences of different lengthsbased on pre-trained language models (devlin et al.,2019; lan et al., 2019; clark et al., 2020).
weﬁrst introduce an efﬁcient approach to extract andprune meaningful n-grams from unlabeled cor-pus.
then we present a new pre-training objective,minimizing symbol-vector algorithmic difference(misad), that explicitly applies a penalty over dif-ferent levels of linguistic units if their representa-tions tend not to be in the same vector space..to investigate our model’s ability of capturingdifferent levels of language information, we intro-duce an original universal analogy task derivedfrom google’s word analogy dataset, where ourmodel signiﬁcantly improves the performance ofprevious pre-trained language models.
evaluationon a wide range of downstream tasks also demon-strates the effectiveness of our ulr model.
over-all, our ulr-bert reaches the highest averageaccuracy on the universal analogy dataset and ob-tains 1.1% gain over google bert on the gluebenchmark.
extensive experimental results on aquestion answering task veriﬁes that our model canbe easily applied to real-world applications in anextremely convenient way..2 related work.
previous language representation learning methodssuch as word2vec (mikolov et al., 2013), glove(pennington et al., 2014), laser (artetxe andschwenk, 2019), infersent (conneau et al., 2017)and use (cer et al., 2018) focus on speciﬁc granu-lar linguistic units, e.g., words or sentences.
laterproposed elmo (peters et al., 2018), openai gpt(radford et al., 2018), bert (devlin et al., 2019)and xlnet (yang et al., 2020) learns contextual-ized representation for each input token.
althoughsuch pre-trained language models (prlms) more orless are capable of offering universal language rep-resentation through their general-purpose trainingobjectives, all the prlms devote into the contex-.
tualized representations from a generic text back-ground and pay little attention on our concerneduniversal language presentation..as a typical prlm, bert is trained on a largeamount of unlabeled data including two trainingtargets: masked language model (mlm), andnext sentence prediction (nsp).
albert (lanet al., 2019) is trained with sentence-order predic-tion (sop) as a replacement of nsp.
structbert(wang et al., 2020) combines nsp and sop to learninter-sentence structural information.
nevertheless,roberta (liu et al., 2019) and spanbert (joshiet al., 2020) show that single-sequence trainingis better than the sentence-pair scenario.
besides,bert-wwm (cui et al., 2019), structbert (joshiet al., 2020), spanbert (wang et al., 2020) per-form mlm on higher linguistic levels, augmentingthe mlm objective by masking whole words, tri-grams or spans, respectively.
electra (clarket al., 2020) further improves pre-training througha generator and discriminator architecture.
theaforementioned models may seemingly handle dif-ferent sized input sequences, but all of them focuson sentence-level speciﬁc representation still foreach word, which may cause unsatisfactory perfor-mance in real-world situations..there are a series of downstream nlp tasksespecially on question answering which may beconveniently and effectively solved through ulrlike solution.
actually, though in different forms,these tasks more and more tend to be solved byour suggested ulr model, including dialogue ut-terance regularization (cao et al., 2020), questionparaphrasing (bonadiman et al., 2019), measuringqa similarities in faq tasks (damani et al., 2020;sakata et al., 2019)..3 model.
as pre-trained contextualized language modelsshow their powerfulness in generic language rep-resentation for various downstream nlp tasks, wepresent a bert-style ulr model that is especiallydesigned to effectively learn universal, ﬁxed-sizedrepresentations for input sequences of any granu-larity, i.e., words, phrases, and sentences.
our pro-posed pre-training method is furthermore strength-ened in three-fold.
first, we extract a large numberof meaningful n-grams from monolingual corpusbased on point-wise mutual information to lever-age the multi-granular structural information.
sec-ond, inspired by word and phrase representation.
5123and their compositionality, we introduce a novelpre-training objective that directly models the in-put sequences and the extracted n-grams throughmanipulating their representations.
finally, we im-plement a normalized score for each n-gram toguide their sampling for training..3.1 n-gram extracting.
given a symbol sentence, joshi et al.
(2020) uti-lize span-level information by randomly maskingand predicting contiguous segments.
differentfrom such random sampling strategy, our methodis based on point-wise mutual information (pmi)(church and hanks, 1989) that makes efﬁcient useof statistics and automatically extracts meaningfuln-grams from unlabeled corpus..mutual information (mi) describes the associa-tion between two tokens by comparing the proba-bility of observing them together with the proba-bilities of observing them independently.
highermutual information indicates stronger associationbetween the tokens.
to be speciﬁc, an n-gram isdenoted as w = (x1, .
.
.
, x|w|), where |w| is thenumber of tokens in w and |w| > 1. therefore, wepresent an extended pmi formula as follows:.
p m i(w) =.
logp (w) −.
logp (xk).
.
.
1|w|.
.
|w|(cid:88).
k=1.
where the probabilities are estimated by countingthe number of observations of each token and n-gram in the corpus, and normalizing by the cor-1pus size.
|w| is an additional normalization fac-tor which avoids extremely low scores for longn-grams..we ﬁrst collect all n-grams with lengths up to nusing the srilm toolkit1 (stolcke, 2002), and com-pute pmi scores for all the n-grams based on theiroccurrences.
then, only n-grams with pmi scoreshigher than the chosen threshold are selected andinput sequences are marked with the correspondingn-grams..3.2 training objective.
while the mlm training objective as in bert (de-vlin et al., 2019) and its extensions (cui et al., 2019;joshi et al., 2020; wang et al., 2020) are widelyused for pre-trained contextualized language mod-eling, they do not focus on our concerned ulr,.
1http://www.speech.sri.com/projects/srilm/download.html.
which demands an arithmetic corresponding rela-tionship between the symbol and its representedvector.
in order to directly model such demand,we propose a novel training target – minimizingsymbol-vector algorithmic difference (misad)– that leverages the vector space regularity of dif-ferent granular linguistic units.
for example, thefollowing symbol sequence equation.
“london is” + “the capital of england”.
=“london is the capital of england”.
(1).
indicates a vector algorithmic equation accordingto our ulr goal,.
vector(“london is”) + vector(“the capital of.
england”).
=vector(“london is the capital of england”).
(2).
thus, if the symbol equation (1) cannot imply therespective vector equation (2), we may set a train-ing objective to let the ulr model forcedly learnsuch relationship..formally, we denote the input sequence bys = {x1, .
.
.
, xm}, where m is the number oftokens in s. after n-gram extracting and pruningby means of pmi, each sequence is marked withseveral n-grams.
during pre-training, only oneof them is selected by the n-gram scoring func-tion, which will be introduced in detail in sec-tion 3.3, and the input sequence is represented ass = {x1, .
.
.
, xi−1, w, xj+1, .
.
.
, xm}, where then-gram w = {xi, .
.
.
, xj} (1 ≤ i < j ≤ m) is asub-sequence of s. then we convert s into twoindependent parts – the n-gram w and the rest ofthe tokens r = {x1, .
.
.
, xi−1, xj+1, .
.
.
, xm} –which are fed into the model separately along withthe original complete sequence..the transformer encoder generates a contextual-ized representation for each token in the sequence.
to derive ﬁxed-sized vectors for sequences of dif-ferent lengths, we use the pooled output of the[cls] token as sequence embeddings.
the modelis trained to minimize the following mean squareerror (mse) loss:.
lm isad = m se(ew + er, es).
where ew, er and es are representations of w, rand s, respectively, and are all normalized to unitlengths.
to enhance the robustness of the model,we jointly train misad and the mlm objective.
5124lm lm as in bert with equal weights.
since theinput sentence s is split into w + r, we must avoidmasking out the n-gram w in the original sentencein order not to affect the semantics after vectorspace combination.
however, tokens in n-gramsother than w have equal weights of being replacedwith [mask] as other tokens.
the ﬁnal loss func-tion is as follows:.
l = lm isad + lm lm.
3.3 n-gram sampling.
for a given sequence, the importance of differentn-grams and the degree to which the model un-derstands their semantics are different.
instead ofsampling n-grams at random, we let the model de-cide which n-gram to choose based on the knowl-edge learned in the pre-training stage.
followingtamborrino et al.
(2020), we employ a normalizedscore for each n-gram in the input sequence usingthe masked language modeling head..we mask one n-gram at a time and the modeloutputs probabilities of the masked tokens giventheir surrounding context.
the score of an n-gramw is calculated as the average probabilities of alltokens in it..scorew =.
p (xk|s\w).
1|w|.
|w|(cid:88).
k=1.
where |w| is the length of w and s\w is the notationof an input sequence s with all tokens within wreplaced by the special token [mask].
finally, wechoose the n-gram with the lowest score for ourtraining target..4.implementation of ulr pre-training.
this section introduces our ulr pre-training de-tails..as for the pre-training corpus, we downloadthe english wikipedia corpus2 and pre-processwith process wiki.py3, which extracts textfrom xml ﬁles.
when processing paragraphs fromwikipedia, we ﬁnd that a large number of enti-ties are annotated with special marks, which maybe useful for our task.
therefore, we identify allthe entities and treat them as high-quality n-grams.
then, we remove punctuation marks and characters.
in other languages based on regular expressions,and ﬁnally get a corpus of 2,266m words..as for n-gram pruning, pmi scores of all n-grams with a maximum length of n = 6 are calcu-lated for each document.
we manually evaluate theextracted n-grams and ﬁnd more than 50% of thetop 2000 n-grams contain 2 ∼ 3 words, and onlyless than 3% n-grams are longer than 4. althougha larger n-gram vocabulary can cover longer n-grams, it will cause too many meaningless n-gramsat the same time.
therefore, we empirically retainthe top 3000 n-grams for each document.
finally,we randomly sample 10m sentences from the entirecorpus to reduce training time..during pre-training, bert packs sentence pairsinto a single sequence and use the special [cls]token as sentence-pair representation.
however,our misad training objective requires single-sentence inputs.
thus in our experiments, eachinput is an n-ngram or a single sequence with amaximum length of 128. special tokens [cls]and [sep] are added at the front and end of eachinput, respectively.
instead of training from scratch,we initialize our model with the ofﬁcially releasedcheckpoints of bert (devlin et al., 2019), al-bert (lan et al., 2019) and electra (clarket al., 2020).
we use adam optimizer (kingmaand ba, 2017) with initial learning rate of 5e-5 andlinear warmup over the ﬁrst 10% of the trainingsteps.
batch size is 64 and dropout rate is 0.1. eachmodel is trained for one epoch over 10m trainingexamples on four nvidia tesla p40 gpus..5 experimental setup.
5.1 tasks.
we construct a universal analogy dataset in terms ofwords, phrases and sentences and experiment withmultiple representation models to examine theirability of representing different levels of linguisticunits through a task-independent evaluation4.
fur-thermore, we conduct experiments on a wide rangeof downstream tasks from the glue benchmarkand a question answering task..5.1.1 universal analogy.
our universal analogy dataset is based on google’sword analogy dataset and contains three levels oftasks: words, phrases and sentences..2https://dumps.wikimedia.org/enwiki/latest3https://github.com/panyang/wikipedia word2vec/blob/.
master/v1/process wiki.py.
liyianan/ulr..4code and dataset are available at: https://github.com/.
5125girlworsechinachilean − chile.
− boy− bad− beijing + paris+ china.
+ brother = daughter+ big.
sisterlarger= biggereurope= france= japanese chinese.
wifesmallergermanyrussian.
sonbetter.
fatherbiggestbelgium londonkorean.
ukrainian.
table 1: examples from our word analogy dataset.
the correct answers are in bold..word-level recall that in a word analogy task(mikolov et al., 2013), two pairs of words thatshare the same type of relationship, denoted as a :b :: c : d, are involved.
the goal is to retrieve thelast word from the vocabulary given the ﬁrst threewords.
to facilitate comparison between modelswith different vocabularies, we construct a closed-vocabulary analogy task based on google’s wordanalogy dataset through negative sampling.
con-cretely, for each original question, we use gloveto rank every word in the vocabulary and the top5 results are considered to be candidate words.
ifglove fails to retrieve the correct answer, we man-ually add it to make sure it is included in the candi-dates.
during evaluation, the model is expected toselect the correct answer from 5 candidate words.
table 1 shows examples from our word anlogydataset.
phrase-/sentence-level to derive higher levelanalogy datasets, we put word pairs from the word-level dataset into contexts so that the resultingphrase and sentence pairs also have linear rela-tionships.
phrase and sentence templates are ex-trated from the english wikipedia corpus.
bothphrase and sentence datasets have four types ofsemantic analogy and three kinds of syntactic anal-ogy.
please refer to appendix a for details aboutour approach of constructing the universal analogydataset..5.1.2 glue.
the general language understanding evaluation(glue) benchmark (wang et al., 2018) is a collec-tion of tasks that are widely used to evaluate theperformance of a model in language understanding.
we divide nlu tasks from the glue benchmarkinto three main categories.
single-sentence classiﬁcation single-sentenceclassiﬁcation tasks includes sst-2 (socher et al.,2013), a sentiment classiﬁcation task, and cola(warstadt et al., 2019), a task that is to determinewhether a sentence is grammatically acceptable.
natural language inference glue containsfour nli tasks: mnli (williams et al., 2018),qnil (rajpurkar et al., 2016), rte (bentivogli.
et al., 2009) and wnli (levesque et al., 2012).
however, we exclude the problematic wnli inaccordance with devlin et al.
(2019).
semantic similarity mrpc (dolan and brockett,2005), qqp (chen et al., 2018) and sts-b (ceret al., 2017) are semantic similarity tasks, where themodel is required to either determine whether thetwo sentences are equivalent or assign a similarityscore for them..in the ﬁne-tuning stage, pairs of sentences areconcatenated into a single sequence with a specialtoken [sep] in between.
for both single sentenceand sentence pair tasks, the hidden state of the ﬁrsttoken [cls] is used for softmax classiﬁcation.
we use the same sets of hyperparameters for all theevaluated models.
experiments are ran with batchsizes in {8, 16, 32, 64} and learning rate of 3e-5for 3 epochs..5.1.3 geogranno.
geogranno (herzig and berant, 2019) containsnatural language paraphrases paired with logicalforms.
the dataset is manually annotated: for eachnatural language utterance, a correct canonical ut-terance paraphrase is selected.
the train/dev setshave 487 and 59 paraphrase pairs, respectively.
inour experiments, we focus on question paraphraseretrieval, whose task is to retrieve the correct para-phrase from all 158 different sentences when givena question.
most of the queries have only one cor-rect answer while some have two or more matches.
evaluation metrics are top-1/5/10 accuracy..for geogranno and the universal analogytask, we apply three pooling strategies on top ofthe prlm: using the vector of the [cls] token,mean-pooling of all token embeddings and max-pooling over time of all embeddings.
the defaultsetting is mean-pooling..5.2 baselines.
on the universal analogy task, we adopt three typesof baselines including bag-of-words (bow) modelfrom pre-trained word embeddings: glove (pen-nington et al., 2014), sentence embedding models:infersent (conneau et al., 2017), gensen (subra-.
5126model.
word.
phrase.
sentence.
avg.
gain.
sem.
syn.
avg..sem.
syn.
avg..sem.
syn.
avg..word & sentence representation modelsgloveinfersentgensenuselaser.
82.668.844.573.026.9.
78.088.784.483.178.2.
80.378.864.578.052.6.pre-trained contextualized language modelsbertbasebertlargealbertbasealbertxxlargeelectrabaseelectralarge.
51.349.733.738.222.920.4.
60.246.638.135.632.424.7.
55.848.235.936.927.722.6.our universal language representation models70.8ulr-bertbase73.5ulr-bertlarge49.9ulr-albertbase28.9ulr-albertxxlargeulr-electrabase29.526.5ulr-electralarge.
71.780.843.526.824.422.0.
70.066.256.331.034.631.0.
0.00.00.01.80.0.
0.30.10.10.82.22.9.
1.18.40.33.61.72.9.
40.954.154.463.163.3.
69.367.453.652.357.149.8.
66.860.558.255.056.556.7.
20.527.027.232.531.7.
34.833.926.726.629.726.4.
34.034.529.329.329.129.8.
0.20.00.00.61.6.
0.10.50.10.40.41.4.
1.54.70.30.70.90.8.
39.850.844.944.155.4.
68.361.260.949.439.552.0.
63.054.360.960.357.652.9.
20.025.422.422.428.5.
34.230.930.524.920.026.7.
32.329.530.630.529.326.9.
40.343.738.044.337.6.
41.637.731.029.525.825.2.
45.745.836.629.629.327.7.
-----.
------.
4.18.15.60.13.52.5.table 2: performance of different models on the universal analogy dataset.
“sem” = semantic.
“syn” = syntactic..manian et al., 2018), use (cer et al., 2018) andlaser (artetxe and schwenk, 2019), and pre-trained contextualized language models: bert,albert and electra..on glue and geogranno, we especially.
evaluate our model and two baseline models:.
bert the ofﬁcially released pre-trained bertmodels (devlin et al., 2019)..mlm-bert bert models trained with thesame additional steps with our model on wikipediausing only the mlm objective..ulr-bert our universal language representa-tion model trained on wikipedia with mlm andmisad..6 results.
6.1 universal analogy.
and electra hardly exhibit arithmetic character-istics and increasing the model size usually leadsto a decrease in accuracy..however, training models with our properly de-signed misad objective greatly improves the per-formance.
especially, ulr-bert obtains 15% ∼25% absolute gains on word-level analogy, suchresults are so strong to be comparable to glove,which especially focuses on the linear word anal-ogy feature from its training scheme.
mean-while glove performs far worse than our modelon higher-level analogies.
overall, ulr-bertachieves the highest average accuracy (45.8%), anabsolute gain of 8.1% over bert, indicating thatit has indeed more effectively learned universallanguage representations across different linguisticunits.
it demonstrates that our pre-training methodis effective and can be adapted to different prlms..6.2 glue.
results on our universal analogy dataset are re-ported in table 2. generally, semantic analogiesare more challenging than the syntactic ones andhigher-level relationships between sequences aremore difﬁcult to capture, which is observed in al-most all the evaluated models.
on the word analogytask, glove achieves the highest accuracy (80.3%)while its performance drops sharply on higher-leveltasks.
all well trained prlms like bert, albert.
5https://gluebenchmark.com.
table 3 shows the performance on the glue bench-mark.
our model improves the bertbase andbertlarge by 1.1% and 0.7% on average, re-spectively.
since our model is established on thereleased checkpoints of google bert, we makeadditional comparison with mlm-bert that istrained under the same procedure as our modelexcept for the pre-training objective.
while themodel trained with more mlm updates may im-prove the performance on some tasks, it underper-.
5127model.
bertbasebertlarge.
bertbasemlm-bertbaseulr-bertbasebertlargemlm-bertlargeulr-bertlarge.
batch size: 8, 16, 32, 64; length: 128; epoch: 3; lr: 3e-5.
single sentence.
natural language inference.
semantic similarity.
cola(mc).
sst-2(acc).
mnlim/mm(acc).
qnli(acc).
rte(acc).
mrpc qqp(f1).
(f1).
sts-b(pc).
avg.
gain.
52.160.5.
53.551.956.560.562.661.8.
93.594.9.
92.194.094.394.994.595.0.in literature.
our implementation.
84.6/83.486.7/85.9.
84.5/83.784.5/83.984.6/84.086.1/85.686.6/85.686.7/86.0.
90.592.7.
90.690.491.092.892.893.0.
66.470.1.
67.166.668.068.867.171.0.
88.989.3.
87.588.189.089.688.990.2.
71.272.1.
71.671.671.672.172.372.3.
87.187.6.
85.386.286.687.387.288.2.
79.782.2.
79.579.780.682.082.082.7.
--.
-0.21.1-.
00.7.table 3: test results on the glue benchmark scored by the evaluation server5.
we exclude the problematic wnlidataset and recalculate the “avg.” score.
results for bertbase and bertlarge are obtained from devlin et al.
(2019).
“mc” and “pc” are matthews correlation coefﬁcient (matthews, 1975) and pearson correlation coefﬁcient,respectively..model.
glovelaserbm25.
bertbasemlm-bertbaseulr-bertbasebertlargemlm-bertlargeulr-bertlarge.
top-1 top-5 top-10.
0.36.327.1.
29.637.039.715.924.535.1.
2.79.562.5.
58.966.866.042.757.868.8.
7.412.776.4.
67.172.677.354.270.777.3.table 4: question paraphrase retrieval accuracy of dif-ferent models on the train-dev set of geogranno..forms bert on datasets such as mrpc, rte andsst-2.
our model exceeds mlm-bertbase andmlm-bertlarge by 0.9% and 0.7% on averagerespectively.
the main gains from the base modelare in cola (+4.6%) and rte (+1.4%), whichare entirely contributed by our misad trainingobjective.
overall, our model improves the perfor-mance of its baseline on every dataset in the gluebenchmark, demonstrating its effectiveness in realapplications of natural language understanding..6.3 geogranno.
table 4 shows the performance on geogranno.
as we can see, 4 out of 6 evaluated pre-trainedlanguage models signiﬁcantly outperform bm25for top-1 accuracy, indicating the superiority ofcontextualized embedding-based models over thestatistical method.
among all the evaluated mod-els, ulr-bert yields the highest accuracies(39.7%/68.8%/77.3%).
to be speciﬁc, our ulrmodels exceeds bertbase and bertlarge by.
10.1% and 19.2% and obtains 2.7% and 10.6%improvements compared with mlm-bertbaseand mlm-bertlarge in terms of top-1 accu-racy, respectively, which are consistent with theresults on the glue benchmark.
since n-gramsand sentences of different lengths are involved inthe pre-training of our model, it is especially betterat understanding the semantics of input sequencesand mapping queries to their paraphrases accordingto the learned sense of semantic equality..7 ablation study.
in this section, we explore to what extent doesour model beneﬁt from the misad objective andsampling strategy, and further conﬁrm that our pre-training procedure improves the model’s ability ofencoding variable-length sequences..7.1 effect of training objectives.
to make a fair comparison, we train bert withthe same additional updates using different combi-nations of training tasks:.
nsp-bert is trained with mlm and nsp,whose goal is to distinguish whether two inputsentences are consecutive.
for each sentence, wechoose its following sentence 50% of the time andrandomly sample a sentence 50% of the time..sop-bert is trained with mlm and sop, asubstitute of the nsp task that aims at better mod-eling the coherence between sentences.
consistentwith lan et al.
(2019), we sample two consecu-tive sentences in the same document as a positive.
5128model.
single sentence.
natural language inference.
semantic similarity.
cola(mc).
sst-2(acc).
mnlim/mm(acc).
qnli(acc).
mrpc qqp(f1).
(f1).
sts-b(pc).
avg.
gain.
bertmlm-bertnsp-bertsop-bertulr-bert.
53.551.953.550.956.5.
92.194.093.292.794.3.
84.5/83.784.5/83.984.1/83.584.0/83.184.6/84.0.
90.690.490.590.791.0.
87.588.187.785.089.0.
71.671.672.170.971.6.
85.386.284.583.986.6.
79.579.779.578.680.6.
-0.20-0.91.1.rte(acc).
67.166.666.166.568.0.table 5: comparison of base models using different training objectives on the glue test set..model.
colastd.
mean max.
rtestd.
mean max.
mrpcstd.
mean max.
bertulr-bert.
1.511.31.
57.059.3.
58.360.2.
1.921.83.
68.169.3.
70.472.6.
0.520.61.
90.490.8.
90.991.5.table 6: standard deviation, mean, and maximum performance on the glue dev set when ﬁntuing bert andulr-bert with 5 random seeds..sample, and reverse their order 50% of the time tocreate a negative sample..for both baselines and ulr, we use the same setof parameters for 5 runs, and average scores on theglue test set are reported in table 5. althoughwe expect nsp and sop to help the model bet-ter understand the relationship between sentencesand beneﬁt tasks like natural language inference,they hardly improve the performance on glue ac-cording to our strict implementation.
speciﬁcally,nsp-bert outperforms mlm-bert on datasetssuch as cola, qnli and qqp while less satisfac-tory on other tasks.
sop-bert is on a par withmlm-bert on three nli tasks but it sharply de-creases the score on other datasets.
in general,single-sentence training with only the mlm objec-tive accounts for better performance as describedby liu et al.
(2019); joshi et al.
(2020).
besides,our training strategy which combines mlm andmisad yields the most considerable gains com-pared with other training objectives..table 6 shows standard deviation, mean and max-imum performance on cola/rte/mrpc dev setwhen ﬁne-tuning bert and ulr-bert over 5random seeds, which clearly shows that our modelis generally more stable and yields better resultscompared with bert..7.2 effect of sampling strategies.
we compare our pmi-based n-gram samplingscheme with two alternatives.
speciﬁcally, we trainthe following two baseline models under the samemodel settings except for the sampling strategy.
random spans we replace our n-gram module.
with the masking strategy as proposed by joshiet al.
(2020), where the sampling probability ofspan length l is based on a geometric distributionl ∼ geo(p).
the parameter p is set to 0.2 andmaximum span length lmax = 6.named entities we only retain named entities thatare annotated in the wikipedia corpus..table 7 shows the effect of different samplingschemes on the glue dev set.
as we can see,our pmi-based n-gram sampling is preferable toother strategies on 6 out of 8 tasks.
cola and rteare more sensible to sampling strategies than othertasks.
on average, using named entities and mean-ingful n-grams is better than randomly sampledspans.
we attribute the source to the reason is thatrandom span sampling ignores important semanticand syntactic structure of a sequence, resulting in alarge number of meaningless segments.
comparedwith using only named entities, our pmi-based ap-proach automatically discovers structures withinany sequence and is not limited to any granularity,which is critical to pre-training universal languagerepresentation..7.3 application to different models.
experiments on the universal analogy task revealthat our proposed training scheme can be adaptedin thisto various pre-trained langauge models.
subsection, we compare our model with bert,albert and electra on geogranno andthe glue benchmark..table 8 shows the results on geogranno andthe glue dev set, where our approach can en-hance the performance of all three pre-trained mod-.
5129single sentence.
natural language inference.
semantic similarity.
model.
cola(mc).
sst-2(acc).
mnlim/mm(acc).
qnli(acc).
random spansnamed entitiespmi n-grams.
56.157.159.3.
93.193.193.6.
84.5/84.984.4/84.784.7/84.9.
91.591.691.8.rte(acc).
66.167.569.3.mrpc qqp(f1).
(f1).
sts-b(pc).
91.590.890.8.
87.987.987.8.
89.889.989.9.avg..82.883.083.6.table 7: comparison of base models using different sampling strategies on the glue dev set..model.
bertalbertelectra.
ulr-bertulr-albertulr-electra.
geogranno glue dev.
29.6/58.9/67.118.4/41.1/52.611.2/21.1/26.6.
39.7/66.0/77.324.9/44.7/55.926.8/51.8/65.5.
82.683.086.5.
83.683.486.9.table 8: comparison of different base models on ge-ogranno and glue.
we report the top-1/5/10 accu-racy on geogranno..group by query length |q|.
model.
bertulr-bert.
1∼6(32.6%).
7∼8(36.7%).
9∼15(30.7%).
73.979.8+5.9.
64.976.1+11.2.
62.575.9+13.4.
group by abs(|q| − |q|).
model.
bertulr-bert.
≥0(100%).
≥2(62.2%).
≥3(43.3%).
67.177.3+10.2.
63.076.2+13.2.
57.070.3+13.3.
table 9: comparison of top-10 accuracy of bert andulr-bert on different subsets of geogranno..els.
among all the evaluated models, ulr-bertachieves the largest gains on glue while ulr-electra obtains the most signiﬁcant improve-ment on geogranno.
it further veriﬁes the effec-tiveness and universality of our model..7.4 effect of sequence length.
in previous experiments on geogranno, ourmodel has shown considerable improvement overall three evaluated prlms.
the task involves textmatching between linguistic units at different lev-els where queries are sentences and labels are oftenphrases.
thus the performance on such task highlydepends on the model’s ability to uniformly dealwith linguistic units of different granularities..in the following, we explore deeper details andinterpretability of how our proposed objective actat different levels of linguistic units.
speciﬁcally,.
we intuitively show the consistency of the repre-sentations learned by ulr-bert by grouping thedataset according to query length |q| and the abso-lute difference between query length and questionlength abs(|q| − |q|), respectively..results are shown in table 9, which clearlyshows that as the length of the query increases,the performance of bert drops sharply.
sim-ilarly, bert is more sensible to the differencebetween query length and question length.
incontrast, ulr-bert is more stable when dealingwith sequences of different lengths and is superiorto bert in terms of representation consistency,which we speculate is due to the interaction be-tween different levels of linguistic units in the pre-training procedure..8 conclusion.
this work formally introduces universal languagerepresentation learning to enable uniﬁed vectoroperations among different language hierarchies.
for such a purpose, we propose three highlightedulr learning enhancement, including the newlydesigned training objective, minimizing symbol-vector algorithmic difference (misad).
in de-tailed model implementation, we extend bert’spre-training objective to a more general level,which leverages information from sequences ofdifferent lengths in a comprehensive way.
in ad-dition, we provide a universal analogy dataset asa task-independent evaluation benchmark.
over-all experimental results show that our proposedulr model is generally effective in a broad rangeof nlp tasks including natural language questionanswering and so on..references.
mikel artetxe and holger schwenk.
2019. mas-sively multilingual sentence embeddings for zero-shot cross-lingual transfer and beyond.
transac-tions of the association for computational linguis-tics, 7:597–610..5130luisa bentivogli, peter clark, dagan ido, and giampic-colo danilo.
2009. the ﬁfth pascal recognizing tex-tual entailment challenge.
in proceedings of tac..daniele bonadiman, anjishnu kumar, and arpit mittal.
2019. large scale question paraphrase retrieval withsmoothed deep metric learning.
in proceedings ofthe 5th workshop on noisy user-generated text (w-nut 2019), pages 68–75..ruisheng cao, su zhu, chenyu yang, chen liu, raoma, yanbin zhao, lu chen, and kai yu.
2020. un-supervised dual paraphrasing for two-stage semanticin proceedings of the 58th annual meet-parsing.
ing of the association for computational linguistics,pages 6806–6817..daniel cer, mona diab, eneko agirre, i˜nigo lopez-gazpio, and lucia specia.
2017. semeval-2017task 1: semantic textual similarity multilingual andcrosslingual focused evaluation.
in proceedings ofthe 11th international workshop on semantic evalu-ation (semeval-2017), pages 1–14..daniel cer, yinfei yang, sheng-yi kong, nan hua,nicole limtiaco, rhomni st. john, noah constant,mario guajardo-cespedes, steve yuan, chris tar,brian strope, and ray kurzweil.
2018. universalin proceedings ofsentence encoder for english.
the 2018 conference on empirical methods in nat-ural language processing: system demonstrations,pages 169–174..zihan chen, hongbo zhang, xiaoji zhang, and leqi.
zhao.
2018. quora question pairs..jianpeng.
cheng,.
devang agrawal,.
h´ectormart´ınez alonso, shruti bhargava, joris driesen,federico flego, dain kaplan, dimitri kartsaklis,lin li, dhivya piraviperumal, jason d. williams,hong yu, diarmuid ´o s´eaghdha, and andersjohannsen.
2020. conversational semantic parsingin proceedings of thefor dialog state tracking.
2020 conference on empirical methods in naturallanguage processing (emnlp), pages 8107–8117..kenneth ward church and patrick hanks.
1989. wordassociation norms, mutual information, and lexicog-in 27th annual meeting of the associationraphy.
for computational linguistics, pages 76–83..kevin clark, minh-thang luong, quoc v. le, andchristopher d. manning.
2020. electra: pre-training text encoders as discriminators rather thangenerators..alexis conneau, douwe kiela, holger schwenk, lo¨ıcbarrault, and antoine bordes.
2017. supervisedlearning of universal sentence representations fromnatural language inference data.
in proceedings ofthe 2017 conference on empirical methods in natu-ral language processing, pages 670–680..yiming cui, wanxiang che, ting liu, bing qin,ziqing yang, shijin wang, and guoping hu.
2019.pre-training with whole word masking for chinesebert..sonam damani, kedhar nath narahari, ankush chat-terjee, manish gupta, and puneet agrawal.
2020.optimized transformer models for faq answering.
in pakdd 2020, lecture notes in computer sci-ence..jacob devlin, ming-wei chang, kenton lee, andkristina toutanova.
2019. bert: pre-training ofdeep bidirectional transformers for language under-standing.
in proceedings of the 2019 conference ofthe north american chapter of the association forcomputational linguistics: human language tech-nologies, volume 1 (long and short papers), pages4171–4186..william b. dolan and chris brockett.
2005. automati-cally constructing a corpus of sentential paraphrases.
in proceedings of the third international workshopon paraphrasing (iwp2005)..jonathan herzig and jonathan berant.
2019. don’tparaphrase, detect!
rapid and effective data collec-in proceedings of thetion for semantic parsing.
2019 conference on empirical methods in natu-ral language processing and the 9th internationaljoint conference on natural language processing(emnlp-ijcnlp), pages 3810–3820..mandar joshi, danqi chen, yinhan liu, daniel s.weld, luke zettlemoyer, and omer levy.
2020.spanbert: improving pre-training by representingand predicting spans.
transactions of the associa-tion for computational linguistics, 8:64–77..diederik p. kingma and jimmy ba.
2017. adam: a.method for stochastic optimization..zhenzhong lan, mingda chen, sebastian goodman,kevin gimpel, piyush sharma, and radu soricut.
2019. albert: a lite bert for self-supervisedlearning of language representations..hector j. levesque, ernest davis, and leora mor-genstern.
2012. the winograd schema challenge.
kr’12, page 552–561..yinhan liu, myle ott, naman goyal, jingfei du, man-dar joshi, danqi chen, omer levy, mike lewis,luke zettlemoyer, and veselin stoyanov.
2019.roberta: a robustly optimized bert pretrainingapproach..b.w.
matthews.
1975..the pre-dicted and observed secondary structure of t4 phagelysozyme.
biochimica et biophysica acta (bba) -protein structure, 405(2):442 – 451..comparison of.
tomas mikolov, kai chen, greg corrado, and jeffreydean.
2013. efﬁcient estimation of word represen-tations in vector space..jeffrey pennington, richard socher, and christophermanning.
2014. glove: global vectors for wordrepresentation.
in proceedings of the 2014 confer-ence on empirical methods in natural languageprocessing (emnlp), pages 1532–1543..5131matthew peters, mark neumann, mohit iyyer, mattgardner, christopher clark, kenton lee, and lukezettlemoyer.
2018. deep contextualized word repre-sentations.
in proceedings of the 2018 conferenceof the north american chapter of the associationfor computational linguistics: human languagetechnologies, volume 1 (long papers), pages 2227–2237..alec radford, karthik narasimhan, tim salimans, andimproving language under-ilya sutskever.
2018.standing by generative pre-training.
url https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/languageunsupervised/language understand-ing paper.pdf..pranav rajpurkar, jian zhang, konstantin lopyrev, andpercy liang.
2016. squad: 100,000+ questions formachine comprehension of text.
in proceedings ofthe 2016 conference on empirical methods in natu-ral language processing, pages 2383–2392..alex warstadt, amanpreet singh, and samuel r. bow-man.
2019. neural network acceptability judgments.
transactions of the association for computationallinguistics, 7:625–641..adina williams, nikita nangia, and samuel bowman.
2018. a broad-coverage challenge corpus for sen-tence understanding through inference.
in proceed-ings of the 2018 conference of the north americanchapter of the association for computational lin-guistics: human language technologies, volume 1(long papers), pages 1112–1122..zhilin yang, zihang dai, yiming yang, jaime car-bonell, ruslan salakhutdinov, and quoc v. le.
2020.xlnet: generalized autoregressive pretraining forlanguage understanding..a universal analogy.
wataru sakata, tomohide shibata, ribeka tanaka, andsadao kurohashi.
2019. faq retrieval using query-question similarity and bert-based query-answerrelevance.
in proceedings of the 42nd internationalacm sigir conference on research and develop-ment in information retrieval, pages 1113–1116..as a new task, universal representation has to beevaluated in a multiple-granular analogy dataset.
in this section, we introduce the procedure of con-structing different levels of analogy datasets basedon google’s word analogy dataset..richard socher, alex perelygin, jean wu, jasonchuang, christopher d. manning, andrew ng, andchristopher potts.
2013. recursive deep modelsfor semantic compositionality over a sentiment tree-in proceedings of the 2013 conference onbank.
empirical methods in natural language processing,pages 1631–1642..andreas stolcke.
2002. srilm - an extensible lan-in seventh international.
guage modeling toolkit.
conference on spoken language processing..sandeep subramanian, adam trischler, yoshua ben-gio, and christopher j pal.
2018. learning gen-eral purpose distributed sentence representations viain internationallarge scale multi-task learning.
conference on learning representations..alexandre tamborrino, nicola pellican`o, baptiste pan-nier, pascal voitot, and louise naudin.
2020. pre-training is (almost) all you need: an applicationin proceedings of theto commonsense reasoning.
58th annual meeting of the association for compu-tational linguistics, pages 3878–3887..alex wang, amanpreet singh, julian michael, fe-lix hill, omer levy, and samuel bowman.
2018.glue: a multi-task benchmark and analysis plat-in pro-form for natural language understanding.
the 2018 emnlp workshop black-ceedings ofboxnlp: analyzing and interpreting neural net-works for nlp, pages 353–355..wei wang, bin bi, ming yan, chen wu, jiangnan xia,zuyi bao, liwei peng, and luo si.
2020. struct-bert: incorporating language structures into pre-training for deep language understanding.
in inter-national conference on learning representations..a.1 word-level analogy.
the goal of a word analogy task is to solve ques-tions like “a is to b as c is to ?”, which is to re-trieve the last word from the vocabulary given theﬁrst three words.
the objective can be formulatedas maximizing the cosine similarity between thetarget word embedding and the linear combinationof the given vectors:.
d∗ = arg max.
cosine(c + b − a, d).
d∗.
cosine(u, v) =.
u · v(cid:107)u(cid:107)(cid:107)v(cid:107).
where a, b, c, d represent embeddings of the cor-responding words and are all normalized to unitlengths..we construct a closed-vocabulary analogy taskbased on google’s word analogy dataset throughnegative sampling.
during evaluation, the modelis expected to select the correct answer from 5candidate words..a.2 phrase/sentence-level analogy.
to investigate the arithmetic properties of vectorsfor higher levels of linguistic units, we presentphrase and sentence analogy tasks based on the pro-posed word analogy dataset.
statistics are shownin table 10..5132a.2.2 syntacticwe consider three typical syntactic analogies:tense, comparative and negation, correspondingto three subsets: “present-participle”, “positive-comparative”, “positive-negative”, where themodel needs to distinguish the correct answerfrom “past tense”, “superlative” and “positive”,respectively.
for example, given phrases.
pigs are bright : pigs are brighter than goats ::.
the train is slow,.
the model need to give higher similarity score to thesentence that contains “slower” than the one thatcontains “slowest”.
similarly, we add synonymsand synonymous phrases for each question to eval-uate the model ability of learning context-awareembeddings rather than interpreting each word inthe question independently.
for instance, “pleas-ant” ≈ “not unpleasant” and “unpleasant” ≈ “notpleasant”..dataset.
#q.
#c.#l (p/s).
capital-commoncapital-worldcity-statemale-femalepresent-participlepositive-comparativepositive-negative.
all.
#p.231166723333729.
328.
5064524246750610561322812.
11193.
5555222.
-.
6.0/12.06.0/12.06.0/12.04.1/10.14.8/8.83.4/6.14.4/9.2.
5.4/10.7.
table 10: statistics of our analogy datasets.
#p and #qare the number of pairs and questions for each category.
#c is the number of candidates for each dataset.
#l (p/s)is the average sequence length in phrase/sentence-levelanalogy datasets..a.2.1 semantic.
semantic analogies can be divided into four sub-sets: “capital-common”, “capital-world”, “city-state” and “male-female”.
the ﬁrst two sets canbe merged into a larger dataset: “capital-country”,which contains pairs of countries and their capi-tal cities; the third involves states and their cities;the last one contains pairs with gender relations.
considering glove’s poor performance on word-level “country-currency” questions (<32%), wediscard this subset in phrase and sentence-levelanalogies.
then we put words into contexts so thatthe resulting phrases and sentences also have linearrelationships.
for example, based on relationship.
athens : greece :: baghdad : iraq,.
we select phrases and sentences that contain theword “athens” from the english wikipedia corpus.
we manually modify some words to ensure textcoherence: “he was hired as being professor ofphysics by the university of athens.” and createexamples:.
hired by ... athens : hired by ... greece :: hired.
by ... baghdad : hired by ... iraq..however, we found that such a question isidentical to word-level analogy for bow methodslike averaging glove vectors, because they treatembeddings independently despite the content andword order.
to avoid lexical overlapping betweensequences, we replace certain words and phraseswith their synonyms and paraphrases, e.g.,.
hired by ... athens : employed by ... greece ::.
employed by ... baghdad : hired by ... iraq..5133