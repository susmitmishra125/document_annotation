voxpopuli: a large-scale multilingual speech corpus for representationlearning, semi-supervised learning and interpretation.
changhan wang(cid:63), morgane rivi`ere(cid:63), ann lee, anne wu, chaitanya talnikar,daniel haziza, mary williamson, juan pino, emmanuel dupoux.
facebook ai.
{changhan,mriviere,annl,annewu,talnikar,dhaziza,marywilliamson,juancarabina,dpx}@fb.com.
abstract.
we introduce voxpopuli, a large-scale multi-lingual corpus providing 400k hours of un-it islabeled speech data in 23 languages.
the largest open data to date for unsuper-vised representation learning as well as semi-supervised learning.
voxpopuli also con-tains 1.8k hours of transcribed speeches in15 languages and their aligned oralinter-pretations into 15 target languages totaling17.3k hours.
we provide speech recogni-tion (asr) baselines and validate the versa-tility of voxpopuli unlabeled data in semi-supervised asr and speech-to-texttransla-tion under challenging out-of-domain settings.
the corpus is available at https://github.
com/facebookresearch/voxpopuli..1.introduction.
recent progress in speech-to-text tasks such asautomatic speech recognition (asr) and speechtranslation (st) has been achieved by the devel-opment and application of unsupervised speechpre-training methods (oord et al., 2018; schnei-der et al., 2019; baevski et al., 2020; conneauet al., 2020; wu et al., 2020; nguyen et al., 2020),with semi-supervised learning (self-training) (kahnet al., 2020a; pino et al., 2020; zhang et al., 2020b;xu et al., 2020) or a combination of both meth-ods (xu et al., 2020).
this line of research lever-ages large amounts of unlabeled english speechdata (kahn et al., 2020b) that enable improve-ments in english asr or out-of-english st. largeamounts of multilingual audio data are needed inorder to achieve similar progress for multilingualasr and st. similarly, most asr and st researchis currently conducted on the librispeech (panay-otov et al., 2015) and must-c benchmarks (cattoniet al., 2020; di gangi et al., 2019).
as a result, the.
(cid:63).
equal contribution..research community has been mostly focused onspeech-to-text tasks with english as input.
whilemultilingual asr (pratap et al., 2020; ardila et al.,2020) and st datasets (wang et al., 2020b; iranzo-s´anchez et al., 2020) have recently been made avail-able, the amount of data available quickly dropsbeyond the top few high-resource languages..simultaneous speech translation (interpretation)has witnessed a resurgence with the applicationsof end-to-end encoder-decoder models.
most ofthe recent studies focus on text output and leveragest corpora that are translated ofﬂine in the writ-ten form.
there are differences, however, betweentranslationese and interpretese (sridhar et al., 2013;he et al., 2016), where interpreters develop a vari-ety of strategies to improve simultaneity.
modelstrained on translation corpora are unlikely to learnfrom these interpretation skills to achieve betterquality-latency trade-offs.
finally, there has beenlittle research (jia et al., 2019; tjandra et al., 2019;zhang et al., 2020a) into speech output due to thelack of open data.
existing corpora (tohyama et al.,2004; bendazzoli et al., 2005) are either of limitedsize or no longer publicly available..in this paper, we introduce voxpopuli, a large-scale multilingual speech corpus for representationlearning, semi-supervised learning and interpreta-tion.
it contains the largest open unlabeled speechdata to date, totaling 400k hours in 23 languages:bulgarian (bg), czech (cs), croatian (hr), dan-ish (da), dutch (nl), english (en), estonian (et),finnish (fi), french (fr), german (de), greek (el),hungarian (hu), italian (it), latvian (lv), lithua-nian (lt), maltese (mt), polish (pl), portuguese(pt), romanian (ro), slovak (sk), slovene (sl),spanish (es) and swedish (sv).
voxpopuli alsoprovides a total of 1.8k hours of transcribed speechin 16 languages (en, de, fr, es, pl, it, ro, hu, cs,nl, fi, hr, sk, sl, et and lt) and their aligned oralinterpretations into 15 target languages (en, de, fr,.
proceedingsofthe59thannualmeetingoftheassociationforcomputationallinguisticsandthe11thinternationaljointconferenceonnaturallanguageprocessing,pages993–1003august1–6,2021.©2021associationforcomputationallinguistics993es, pl, it, ro, hu, cs, nl, fi, sk, sl, lt and da)totaling 17.3k hours..we describe our corpus creation methodology insection 2 and analyze the created corpus in sec-tion 3. we provide asr baselines and demonstratethe value of our multilingual unlabeled data as wellas weakly labeled data on several non-english lan-guages in section 4..2 corpus creation.
2.1 data acquisition.
voxpopuli sources data from 2009-2020 europeanparliament (ep) event recordings, which includeplenary sessions, committee meetings and otherevents.
in each event, speakers give speeches inturn in different european union (eu) languages.
these speeches are partially transcribed (for ple-nary sessions only) and interpreted into 24 eu lan-guages.
the interpretations are only oral withoutany transcription.
in the following part, we refer tothe original speech as “source speech” and to theinterpreted one as “target speech”.
we downloadaudio clips for both source and target speechesfrom the ofﬁcial website1.
we also crawl thetranscript, speaker information and starting/endingtimestamps for each speech (for plenary sessionsonly) from that source, with which we later alignthe speech to its transcript and interpretation ut-terance by utterance.
the acquired raw data suf-fers from missing audios, incomplete transcriptsand inaccurate timestamps.
we build data process-ing pipelines to segment speech paragraphs intoutterances and ﬁlter out the ones with erroneoustranscriptions..2.2 data processing.
2.2.1 unlabeled speech.
we construct voxpopuli unlabeled set from allsource and target speeches in 23 eu languages(excluding irish because of very limited data avail-ability).
we segment full-event audios into shortclips of 15-30 seconds using an energy-based voiceactivity detection (vad) algorithm1.
each audioclip has a maximum of 2 seconds of continuoussilence, and silent clips are discarded.
around 16%of the data is dropped after silence removal, whichleads to a ﬁnal overall duration of around 400khours..1https://multimedia.europarl.europa.eu1https://github.com/amsehili/auditok.
unlab.
hrs.
24.1k23.2k22.8k21.4k21.2k21.9k17.9k17.7k18.7k19k14.2k8.1k12.1k11.3k10.6k14.4k17.5k17.6k17.7k13.1k9.1k16.3k13.6k.
endefresplitrohucsnlfihrsksletltptbgellvmtsvda.
transcribedspkrs (f%).
tkns.
lmtkns.
1313 (29.6)531 (30.6)534 (38.6)305 (40.6)282 (23.7)306 (33.8)164 (27.6)143 (30.3)138 (24.9)221 (39.3)84 (56.8)83 (33.1)96 (33.8)45 (43.9)29 (43.7)21 (14.8)-------.
4.8m 60.1m2.3m 50.0m2.1m 58.6m1.6m 57.4m802k 13.6m757k 52.1m739k 10.3m431k 13.0m461k 13.5m488k 54.6m160k 34.5m337k285k270k 13.3m12.6m76k11.3m18k11.5m10k--------------.
hrs.
54328221116611191896362532743351032-------.
all.
384k.
1791.
4295.
15m 467m.
table 1: statistics for unlabeled (“unlab.”) and tran-scribed speech data in voxpopuli: duration in hours(“hrs”), number of speakers (“spkrs”), percentageof female speakers (“f%”) and number of tokens(“tkns”).
durations are calculated on segmented au-dios where leading and trailing silence is trimmed.
thetranscriptionlm data is a combination of voxpopuliand sentences from europarl (koehn, 2005)..2.2.2 transcribed speechthe voxpopuli transcribed set comes from aligningthe full-event source speech audio with the tran-scripts for plenary sessions.
ofﬁcial timestampsare available for locating speeches by speaker inthe full session, but they are frequently inaccurate,resulting in truncation of the speech or mixtureof fragments from the preceding or the succeed-ing speeches.
to calibrate the original timestamps,we perform speaker diarization (sd) on the full-session audio using pyannote.audio (bredin et al.,2020) and adopt the nearest sd timestamps (byl1 distance to the original ones) instead for seg-mentation.
full-session audios are segmented intospeech paragraphs by speaker, each of which has atranscript available..the speech paragraphs have an average dura-tion of 197 seconds, which leads to signiﬁcantmemory usage and prevents efﬁcient parallelism(batching) during model training.
we hence furthersegment these paragraphs into utterances with amaximum duration of 20 seconds.
we leverage.
994source.
endefresplitrohucsnlfihrsksllt.en.
-18716913068696030393115312161.de.
463-18713866775938354318272261.fr.
427196-13554765925293515271441.es.
441204187-55795827302913241651.pl.
432214172118-724929362713271951.target (oral interpretation)hu.
ro.
cs.
nl.
it.
46121719714867-6130323813281661.
4571981951285575-27312413241651.
38220514493436138-232512221441.
42721417011867685027-2513243251.
4001961581154264432023-11221341.fi.
442217168124557148312932-241651.sk.
4332081681146266502955251426-61.sl.
434218156108577046262923123717-1.lt.398164139835053382125191121134-.
da.
370179134863460291818259201030.total.
6.0k2.8k2.3k1.6k7759616883784344011823842396813.total.
857 1.2k 1.1k 1.2k 1.2k 1.3k 1.2k 1.1k 1.2k 1.1k 1.3k 1.3k 1.2k 1.0k 995.
17.3k.
table 2: duration statistics (hours) of aligned speech-to-speech data in voxpopuli between 15 source languagesand 15 target languages..speech recognition (asr) systems to force-alignspeech paragraphs to the given transcripts and cutthe utterances by ending punctuation or the longestsilence inside the sentence if it exceeds 20 seconds.
the asr systems are tds models (hannun et al.,2019) trained with asg criterion (collobert et al.,2016) on audio tracks from in-house de-identiﬁedvideo data.
the resulting utterance segments mayhave incorrect transcriptions due to incomplete rawtranscripts or inaccurate asr force-alignment.
weuse the predictions from the same asr systemsas references and ﬁlter the candidate segments bya maximum threshold of 20% character error rate(cer)..we split the ﬁltered utterances into train, devel-opment and test sets with disjoint speakers andtarget duration ratio (18:1:1).
to determine theassignments, we group utterances by speaker andsort them by overall duration in ascending order.
we assign the sorted groups to the test set in or-der until it reaches 20 speakers or the target dura-tion (whichever comes later).
the same processis repeated on the remaining utterance groups toconstruct the development set (with minimum 10speakers instead).
finally, the rest of utterancesmake up the train set.
this approach ensures higherspeaker diversity in the test and development sets..2.2.3 speech-to-speech alignment.
even though every source speech is associated withcorresponding simultaneous interpretations in tar-get languages, considerable preprocessing and ﬁl-tering is necessary to make this dataset usable.
our.
strategy is to align source and target at the sentencelevel using asr..we ﬁrst compare the spectrogram of the sourceand the target speech to remove the identical partsand segment the target speech into paragraphs.
these identical speech are due to either the shortdelay between the time the source speaker and theinterpreter started, or the fact that the source lan-guage is the same as the target one, and thus nointerpretation is needed.
for long target paragraphs,we further segment them by silence into audio clipsof at most 15 minutes long.
we use the same asrmodel described in section 2.2.2 and a languagemodel (section 2.2.4) to decode the segmented tar-get audio.
the decoded text is also forced alignedwith the target audio, so that we have the times-tamps of every decoded word..for each source segment produced in sec-tion 2.2.2, we locate all decoded words that arewithin a window of ﬁve seconds to its start andend.
a set of candidate target segments can begenerated from all possible combinations of thestarting and ending decoded words.
we computethe cosine similarity between the laser represen-tation (artetxe and schwenk, 2019) of the sourcetext and each decoded text in the candidate set toﬁnd the best target segment, i.e.
the one with thehighest score.
we ﬁrst carry out this process forall source segments, respectively, and then ﬁnetunethe boundaries of overlapping target segments forconsecutive source segments.
finally, a thresholdof 0.75 is applied on the similarity score to ﬁlterout low-quality alignments, which can be due to.
995original(french).
trans-lation.
inter-pretation.
vous le savez tous, la forˆet recule.
toutesles deux secondes dans le monde, c’estl’´equivalent d’un terrain de football qui estd´etruit, c’est en un an l’´equivalent du terri-toire de la gr`ece qui est d´eforest´e et c’est´evidemment dramatique..as you all know, the forest is receding.
every two seconds, across the world, theequivalent of a football pitch is destroyed;within a year, an area the size of greece isdeforested.
clearly, this is a tragic situation..you all know that we are losing forests ev-ery second, the surface the size area of afootball ﬁeld is lost in the forest.
this isreally tragic..table 3: an example from voxpopuli for interpretesevs. translationese.
translationese is verbatim and exact,while interpretese tends to be more general and summa-rizing with unimportant details dropped..asr errors..in addition to asr output, we also collect humantranscription on 400 hours of english target speech.
the human annotators were asked to provide times-tamps for each word while transcribing, and thuswe can apply the same alignment process describedabove on human transcription and generate a set ofground truth speech-to-speech alignment data..as a by-product from this alignment process,source text and target speech is aligned, which pro-vides speech-to-text “translation” data in the re-versed direction.
this data is weakly labeled—thelabel (text) may contain more information than thespeech data (interpretation is likely to drop unim-portant details) and hence is not exact.
however, itis still useful for st model training as an additionto labeled data..2.2.4 language modeling data.
to train language models (lm) for asr decoding,we combine voxpopuli transcription in the trainingset with the europarl corpus (koehn, 2005), whichis from the proceedings of the european parlia-ment from 1996 to 2011. to process the europarldata, we ﬁrst apply the sentence segmentation toolprovided with the corpus.
we remove all texts inthe parentheses, replace hyphens and slashes withspace, and remove all other punctuation exceptapostrophes.
all digits are converted into words,and all texts are normalized into lowercase.
table 1shows the statistics of the lm data..3 data analysis.
unlabeled speech as we can see from table 1,voxpopuli has a total of 400k hours of unlabeleddata well-distributed across 23 eu languages, re-sulting in 8k-24k hours of data for each language.
this ensures adequate data on languages with lowerasr resource, which are likely to beneﬁt morefrom semi-supervised learning.
it also facilitatesmultilingual model training since there is not muchdata imbalance and little need for tuning data sam-pling strategy..transcribed speech the voxpopuli transcribeddata contains 16 languages totaling 1.8k hoursand 4.3k speakers, whose detailed statistics canbe found in table 1, including duration (hours) bylanguage, number of speakers, percentage of fe-male speakers and number of tokens.
the datadistribution is imbalanced and reﬂects the naturaldistribution of the number of native speakers.
theremaining 7 languages (pt, bg, el, lv, mt, sv andda) are not covered due to either limited data vol-ume or the availability of processing pipelines..speech-to-speech alignment the statistics ofthe speech-to-speech alignment between all sourcelanguages and 15 target languages are shown intable 2. compared with the total amount of dataavailable for each source language (“transcribedhours” in table 1), we obtain target alignments formore than 70% of the source sentences in en, de,fr, es and it, more than 50% for pl, ro, cs, nland hr, and the rest has at least 40% of source seg-ments aligned.
to examine the quality of our asrsystem, we align the asr output with the humantranscription we collect on english target speechand see a word error rate (wer) of 31.7. withthe human transcription, we can produce groundtruth speech-to-speech alignment data that is 1.1times larger than the size of the alignment data cre-ated from using asr output, indicating that around12% of the low-quality alignments are ﬁltered dueto asr errors.
if we compare the asr-based andthe ground truth alignment data, there is on averagea 0.75-second shift in the target segment bound-aries..interpretese vs. translationese we exemplifythe differences between simultaneous oral interpre-tation and ofﬂine written translation using voxpop-uli in table 3. the latter is verbatim and exactcompared to the original speech, while the former.
996en.
de.
it.
fr.
es.
pl.
ro.
hu.
nl.
cs.
sl.
fi.
hr.
sk.
avg.
↓.
sup.
baseline.
devtest.
30.1 29.0 41.6 28.6 27.4 27.1 28.5 27.4 35.7 27.8 95.7 45.7 44.9 30.230.0 29.3 45.2 30.5 31.4 25.6 27.7 27.9 38.3 27.7 96.5 41.6 40.2 32.7.vp-10k devtest.
+ ft.15.5 17.2 19.1 13.912.816.2 16.2 21.5 15.4 11.0 12.5.
8.6.
8.39.4.
11.5 18.5 11.1 20.6 21.1 15.6 10.412.0 19.7 11.8 26.1 17.1 14.1 11.1.
37.137.5.
14.615.3.table 4: voxpopuli asr baselines and in-domain unsupervised pre-training.
we report voxpopuli dev andtest wer for languages with ≥10 hours of data.
top: supervised monolingual transformer baselines.
bottom:wav2vec 2.0 base model pre-trained on 10k-hour voxpopuli unlabeled data (23 languages) and ﬁne-tuned on vox-populi asr data.
as we can see, pre-training with in-domain unlabeled data substantially improves performanceespecially for low-resource languages..within/across speaker ↓.
en.
fr.
zh.
mfcc 12.1/23.4sup.†6.2/8.0ll-6k‡4.5/6.2.
12.6/25.58.7/10.88.4/12.7.
11.5/21.37.9/10.38.2/8.2.
std.
↓.
--1.8/2.7.
voxpopuli.
en-500fr-500en+fr-500.
6.9/9.98.1/12.16.9/9.8.
9.6/14.59.1/13.89.0/13.1.
8.7/9.79.2/10.18.6/9.6.
1.1/2.20.5/1.50.9/1.6.
table 5: phoneme discriminability of unsupervisedfeatures across languages.
we report abx discrim-inability score on the 10s test set from zerospeech2017† for english (“en”), french (“fr”) and man-darin (“zh”).
we compare our models with the mfccbaseline, the supervised topline and the state-of-the-art monolingual (english) model‡.
we measure thegenerality of the representations by standard deviation(“std.”) of the scores across the 3 languages.
we seethat multilingual representations generalize better andare more robust on unseen languages.
† dunbar et al.
(2017).
‡ riviere and dupoux (2020)..tends to be more general and summarizing withunimportant details dropped.
human interpretersregularly apply these tactics to make better quality-latency trade-offs.
speech-to-speech translationmodels may beneﬁt from these tactics if they aretrained on interpretation data that voxpopuli pro-vides..4 experiments & results.
we provide voxpopuli asr baselines and vali-date the versatility of voxpopuli unlabeled datain unsupervised representation learning and semi-supervised learning for asr as well as st. wealso evaluate the quality of speech-to-speech align-ment indirectly via the weakly labeled st data itproduces..4.1 experimental setup.
for representation learning, we perform speakerdiarization before vad-based segmentation so thateach utterance contains exactly one speaker.
weaugment the data with time dropout, pitch modiﬁ-cation and reverberation (kharitonov et al., 2020)during model training..for non-wav2vec models, we extract 80-dimensional log-mel ﬁlterbank speech featureswith 25ms windows size and 10ms shift.
we applyper-utterance cmvn (cepstral mean and variancenormalization) to the extracted features.
for gpumemory efﬁciency, we remove training samplesthat have more than 60 seconds of speech or havemore than 1024 characters..we train wav2vec 2.0 (baevski et al., 2020) mod-els with original hyper-parameter settings usingfairseq (ott et al., 2019), except for table 7 wherewe use wav2letter (pratap et al., 2018) and fol-low talnikar et al.
(2020) to do ﬁnetuning usingboth supervised ctc (graves et al., 2006) lossand unsupervised wav2vec 2.0 loss.
the largestmodel (“vp-100k”) takes 10 days on 128 v100gpus for 1m updates.
for non-wav2vec models,we train transformer (vaswani et al., 2017) withcross-entropy criterion using fairseq s2t (wanget al., 2020a).
for section 4.2 and section 4.4.1,we use phoneme vocabularies for models that weevaluate with per (phone error rate) and charactervocabularies for the other.
for section 4.4.2, weuse unigram (kudo and richardson, 2018) vocabu-laries with 2k subwords for all models.
to improvest model training, we pre-train the encoder on thelibrispeech (panayotov et al., 2015) asr task..we use the best checkpoint by validation lossfor evaluation, except for section 4.4.2 where weaverage the 10 best checkpoints.
we build n-gramlanguage models for decoding (when speciﬁed) us-ing kenlm (heaﬁeld, 2011)..997pt.
pt.
domain hours.
langs.
in out.
per ↓ (voxpopuli langs.)
svites.
nl.
fr.
per ↓ (other langs.)
tr.
ky ru.
tt.
zh.
peravg.
↓ std.
↓.
m-cpc†.
out.
60k 0.
1.
36.4 44.3 37.8 43.1 46.5.
37.5 42.4 45.7 40.6 53.2.
42.7.
4.8.wav2vec 2.0 base (95m).
xlsr-mono‡xlsr-10‡vp-mono-5kvp-10kvp-100k.
ininoutoutout.
<0.4k 11.4k 104.5k 110k 5100k 5.
0101818.
6.8 10.4 10.9 37.4 63.69.4 13.4 13.8 16.3 21.06.89.37.58.5 11.9 11.0 13.6 15.07.6 10.3 9.7 12.2 13.0.
8.6.
9.7.
29.6 11.6 44.0 21.4 31.48.6 11.2 11.7 8.3 24.5--10.9 12.4 13.1 8.8 19.39.4 10.7 11.7 8.0 17.5.
-.
-.
-.
wav2vec 2.0 large (317m).
xlsr-10‡xlsr-53‡vp-mono-5kvp-10kvp-100k.
inin+outoutoutout.
1.4k 10156k 10 4304.5k 11810k 518100k 5.
7.9 12.6 11.7 14.0 20.65.8 12.22.96.76.36.17.25.59.79.37.96.38.38.06.55.4.
5.07.08.97.7.
7.06.1-9.38.5.
9.77.1-.
7.2 22.85.1 18.3-.
9.38.1-9.2 11.3 7.6 18.86.9 17.38.0.
9.8.
-.
26.713.8-12.511.0.
12.37.6-9.88.6.
17.25.1-3.02.7.
5.24.2-3.23.1.table 6: few-shot asr with out-of-domain out-of-language unsupervised pre-training.
we adopt the com-mon voice (cv) few-shot phoneme recognition setup† and report test per (phone error rate).
our wav2vec 2.0models are pre-trained on voxpopuli (out-of-cv-domain) either with 4.5k-hour monolingual data (“vp-mono-5k”) or 10k-hour/100k-hour multilingual data (“vp-10k” and “vp-100k”).
pre-training languages may includethe ones being evaluated (“in”) and others (“out”).
our models outperform xlsr-mono and xlsr-10 (samearchitecture as ours but using in-domain cv data) on most languages with out-of-domain and (partially) out-of-language pre-training.
our best model (vp-100k large) performs competitively to xlsr-53, which leverages52k-hour out-of-cv-domain data in addition to the cv data.
† rivi`ere et al.
(2020) ‡ conneau et al.
(2020).
train hoursfr.
de.
es.
660.test wer ↓fr.
es.
de.
12.8 19.4 16.5.baseline†.
1582.
787.vp-50k.
314+ lm (20%).
364(46%).
203(31%).
17.0 18.8 11.910.09.67.8.table 7: asr with out-of-domain unsupervised pre-training and less supervision.
we report test wer oncommon voice (cv).
top: supervised baseline trainedon the combination of an extended cv train set and sev-eral other corpora (decoding with lm).
bottom: ourwav2vec 2.0 base model pre-trained on 50k-hour vox-populi data (out-of-cv-domain) and ﬁne-tuned on thestandard cv train set (a subset of the baseline’s one).
we optionally use 4-gram lms trained on cv for de-coding.
our model outperforms the baseline (evenwithout lm) while using less supervised train data.
†deepspeech polyglot..4.2 speech recognition (asr) baselines.
we provide monolingual transformer baselines forthe 14 languages that have more than 10 hours oftranscribed data (see table 1).
both developmentand test wer are reported in table 4. we seethat several low-resource languages (fi, it, hr, skand sl) suffer from high recognition errors (>40%wer) due to the lack of training data.
even thehighest resource one (en) has a high wer ofaround 30%..4.3 unsupervised representation learning.
we follow the setting in rivi`ere et al.
(2020) toevaluate unsupervised speech representations byphoneme discriminability on 3 languages (english,french and mandarin), and report abx discrim-inability score (schatz et al., 2013) on the 10s testset from zerospeech 2017 (dunbar et al., 2017).
standard deviation (“std.”) of the scores acrossthe 3 languages is also reported as a measure forthe generality of the representations.
as previousstudies focus on monolingual representations, weexplore multilingual representations and examinetheir generality across languages.
we train cpc-based models (riviere and dupoux, 2020) on 500-hour english and 500-hour french unlabeled datafrom voxpopuli, respectively.
and we combine en-glish and french data with 50% sampling (so thatthe total duration remains the same) for the multi-lingual setting.
we observe from table 5 that themultilingual model (“en+fr-500”) performs com-parably to the monolingual ones (“en-500” and“fr-500”) on their seen languages and performs bet-ter on unseen language (“zh”).
its scores vary lessacross languages (lower “std.”) compared to “en-500”.
the variance of the scores is comparable to“fr-500” while the average is lower.
we concludethat multilingual representations generalize betteracross languages and are more robust on unseen.
998fr→en ↑.
es→en ↑.
de→en ↑.
fr ↓.
es ↓.
de ↓.
train hours (ep+cv)test set.
38+264.
ep.
cv.
32+113.
ep.
cv.
42+184.
ep.
cv.
38+264.
ep.
cv.
32+113.
ep.
cv.
42+184.
ep.
cv.
(cascaded) baseline†our end-to-end baselinewith 800h self-trainingwith 3000h self-training.
400h weakly labeled+ labeled.
25.424.526.727.4.
22.931.1.
27.627.028.628.9.
10.130.3.
26.520.522.422.7.
22.228.4.
27.426.626.827.3.
10.929.7.
21.317.518.819.6.
18.024.4.
21.020.020.120.0.
8.823.4.
24.320.819.519.0.
18.318.817.317.0.
15.017.215.615.3.
21.414.113.713.2.
19.823.221.821.4.
16.018.417.517.3.table 8: st and asr using voxpopuli data for self-training or weak supervision.
left: test bleu for stmodels.
right: test wer for asr models.
we evaluate in-voxpopuli-domain performance with europarl-st(ep) and the out-of-domain performance with covost 2 (cv).
we combine both corpora to train our baselineand pseudo-label 3k-hour monolingual voxpopuli unlabeled data for self-training.
for st training with weaksupervision, we combine ep, cv and 300h weakly labeled data from voxpopuli.
both approaches for leveragingvoxpopuli data improve in-domain (ep) and out-of-domain (cv) performance simultaneously.
† ep baselinesfrom iranzo-s´anchez et al.
(2020) and cv baselines from wang et al.
(2020b)..languages.
for quick exploration, we leverage onlypart of the voxpopuli unlabeled data and leave thevalidation on more data to future work..4.4 semi-supervised learning.
we explore two semi-supervised learning settingsfor the application of voxpopuli unlabeled data:unsupervised pre-training followed by supervisedﬁne-tuning for asr and self-training for asr aswell as st..4.4.1 asr with unsupervised pre-trainingself-supervised (unsupervised) pre-training suchas wav2vec 2.0 (baevski et al., 2020) substan-tially reduces the need of labeled data in asr.
furthermore, multilingual pre-training (conneauet al., 2020) allows cross-lingual transfer, whichbrings extra gains especially to low-resource lan-guages.
pre-training wav2vec 2.0 models is, how-ever, resource-intensive and hence re-training mod-els for each task with different domains is imprac-tical.
with the large-scale multilingual data invoxpopuli, we explore if scaling multilingual pre-training can take us towards the one-model-ﬁts-allparadigm by alleviating the impacts of domain orlanguage mismatch between pre-training and ﬁne-tuning.
we train wav2vec 2.0 models 1 on 10k-hour, 50k-hour and 100k-hour voxpopuli data in23 languages (denoted as “vp-10k”, “vp-50k”and “vp-100k”, respectively).
we also train mod-els with 4.5k-hour monolingual data (denoted as“vp-mono-5k”) for comparison.
for quick veriﬁ-cation, we use only part of the voxpopuli unlabeleddata for pre-training.
we leave training the models.
1wav2vec 2.0 base (95m) unless speciﬁed otherwise..on the full 400k-hour data to future work, whichis supposed to achieve even better performance..in-domain pre-training we examine the con-ventional in-domain pre-training setting on the vox-populi asr benchmark.
we evaluate the vp-10kmodel, where the pre-training data is ﬁltered so thatit has no overlaps with the transcribed developmentand test set.
from table 4, we see that pre-trainingusing unlabeled data brings signiﬁcant gains to allthe languages (average 59% test wer reduction).
the gains are most signiﬁcant on the low-resourcelanguages, where improvements are qualitative (forexample, from nearly 100% test wer on sl downto around 30%)..out-of-domain pre-training we examine theout-of-domain pre-training setting using the com-mon voice (cv) asr corpus (ardila et al., 2020).
in contrast with the political domain oral speechin voxpopuli, they are more ﬂuent read speech ofno copyright sentences (for example, wikipediaarticles).
we adopt the few-shot phoneme recog-nition setup on cv v3 from rivi`ere et al.
(2020),with which domain adaptation is limited duringﬁne-tuning due to the small data volume — ithas 1-hour train set, 20-minute development setand 1-hour test set for 10 languages including 5voxpopuli ones.
we present the performance ofvp-mono-5k, vp-10k and vp-100k with the m-cpc (rivi`ere et al., 2020) and xlsr (conneauet al., 2020) baselines in table 6, where phone errorrate (per) is reported.
the xlsr baselines sharethe same wav2vec 2.0 architecture as our modelsbut are trained with in-domain cv data.
vp-mono-5k outperforms xlsr-mono and xlsr-10 on all.
9995 voxpopuli languages (except for a tie on es withxlsr-mono).
vp-100k outperforms xlsr-10on 8 (9) out of the 10 languages.
vp-100k (large)overall performs competitively to xlsr-53, whichleverages 52k-hour out-of-domain data in additionto the in-domain cv data.
notably, it outperformsxlsr-53 on zh, which is covered by xlsr-53 butremote from the eu languages in vp-100k.
thissuggests the high generality of the speech represen-tations vp-100k learned..we also evaluate our multilingual model (vp-50k) under the normal setup (cv v5.1) and reporttest wer in table 7. they are compared withsupervised baselines from deepspeech-polyglot1,which leverage extended cv train sets and severalother corpora for training as well as lm for de-coding.
our model outperforms the baseline withﬁne-tuning on the standard cv train set (a subsetof the baseline’s one), even when not using lm indecoding..out-of-language pre-training in the few-shotphoneme recognition setup (table 6), vp-100kdoes not cover 5 of the 10 cv languages (ky, ru,tr, tt and zh) in pre-training, but leverages datafrom 18 additional eu languages.
it outperformsthe in-domain in-language xlsr baselines on mostof the uncovered languages (except ky which is aremote central asian language).
moreover, it per-forms more stably across all the 10 languages witha smaller variance (standard deviation) on per..4.4.2 self-training for asr and st.self-training (scudder, 1965) is a classical semi-supervised learning approach, where unlabeleddata is equipped with pseudo-labels from a su-pervised model and then combined with labeleddata for model training.
we use the combinationof europarl-st (iranzo-s´anchez et al., 2020) andcovost 2 (wang et al., 2020b) for both asr andst labeled data in 3 languages (directions).
theformer is created from 2009-2012 ep plenary ses-sions and hence has the same domain as voxpop-uli.
the latter is based on common voice v4,which has different domain than voxpopuli anddominates the combined train set.
we train trans-former base (vaswani et al., 2017) supervised base-lines and use 0.8k/3k-hour monolingual voxpop-uli unlabeled data (from 2013-2020 sessions onlyto avoid overlaps with europarl-st) to self-traintransformer large models.
we upsample labeled.
data in self-training so that it has the same durationas the unlabeled one.
we observe from table 8that self-training on voxpopuli improves both in-domain (“ep”) and out-of-domain (“cv”) perfor-mance with similar magnitude most of the time.
for st, self-training helps to narrow the gap be-tween end-to-end models and the cascaded ones(more labeled data available) without the additionof expensive labeled data..4.5 weakly supervised st.we evaluate the quality of the weakly labeled stdata from our speech-to-speech alignment on thesame benchmark as the self-training experiments.
this also provides an indirect evaluation for ouralignment pipeline since imprecise alignments hurtthe st label quality.
we examine the performanceof weakly supervised training as well as joint train-ing using both labeled and weakly labeled data.
wesee from table 8 that the former is on par with (orbetter than) the supervised baseline in the voxpop-uli domain (“ep”) with 0.3x-1.8x more trainingdata than the baseline.
joint training brings sub-stantial gains to both in-domain (“ep”) and out-of-domain (“cv”) performance, and it outperformsself-training.
this suggests that our weakly labeleddata (0.4k hours) is much more informative andefﬁcient than the pseudo-labeled data (3k hours)when combined with labeled data..5 related work.
multilingual speech corpora librilight (kahnet al., 2020b) currently represents the largest scaleunlabeled speech corpus but it is limited to english.
mls (pratap et al., 2020) is a recently releasedlarge-scale multilingual corpus of read speech in8 languages, derived from librivox.
mailabs1is also derived from librivox and has about 1000hours available in 9 languages.
while mls andmailabs are derived from audiobooks, vox-forge1 and common voice (ardila et al., 2020)gather data via crowd-sourcing.
voxforge col-lected data in about 15 different languages withabout 300 hours of speech in total; common voicecurrently supports 60 languages for a total of 7327validated hours available.
the cmu wildernessdataset (black, 2019) collects readings from thenew testament, with 700 different languages avail-.
1https://www.caito.de/2019/01/the-m-ailabs-speech-.
dataset.
1https://gitlab.com/jaco-assistant/deepspeech-polyglot.
1http://www.voxforge.org.
1000able.
iarpa babel program1 collected data for 24languages, mostly from conversational telephonespeech.
the dataset is however not released andunder an open license, and focused on low-resourcelanguages, with labeled data ranging between 25to 65 hours per language..speech-to-text and speech-to-speech transla-tion apart from machine translation (koehn,2005), the european parliament open data has fos-tered the development of corpora for speech-to-text translation and for simultaneous interpreta-tion.
europarl-st (iranzo-s´anchez et al., 2020) is amultilingual speech-to-text translation corpus withtranslations between 6 european languages (en, fr,de, es, it and pt).
similarly, epic (bendazzoliet al., 2005) is derived from the european parlia-ment with simultaneous interpretation speeches initalian, english and spanish.
ciair (tohyamaet al., 2004) and stc (shimizu et al., 2014) are si-multaneous interpretation corpora between englishand japanese with a total of about 180 hours for theformer, while the latter is currently unavailable fordownload.
the mass dataset (zanon boito et al.,2020) also provides speech to speech alignmentsfor about 8k utterances across 8 languages, for atotal of about 23h of speech..6 conclusion.
in this paper, we introduce a large-scale multilin-gual speech corpus, voxpopuli, for representationlearning, semi-supervised learning and interpreta-tion.
voxpopuli provides the largest open unla-beled speech data to date, which has broad applica-tions including unsupervised pre-training and self-training.
voxpopuli is also the ﬁrst corpus for largeamounts of open speech-to-speech interpretationdata.
we provide voxpopuli asr baselines and val-idate the versatility of voxpopuli unlabeled data insemi-supervised learning under challenging out-of-domain settings.
the corpus is available at https://github.com/facebookresearch/voxpopuli..7 acknowledgements.
we thank gabriel synnaeve, tatiana likhoma-nenko, jade copet, vineel pratap, jiatao gu andalexis conneau for helpful discussions on theproject..1https://www.iarpa.gov/index.php/research-.
programs/babel.
8 ethical considerations.
we acknowledge the european union (eu) for cre-ating and publishing the materials used by vox-populi.
we will add citations as well as acknowl-edgements in our release.
we paid the marketprice to transcription vendors for the human an-notations we collected.
voxpopuli includes allavailable speeches from the 2009-2020 ep eventswithout any selections on the topics or speakers.
the speech contents represent the standpoints ofthe speakers in the ep events, many of which areeu ofﬁcials..references.
rosana ardila, megan branson, kelly davis, michaelkohler, josh meyer, michael henretty, reubenmorais, lindsay saunders, francis tyers, and gre-gor weber.
2020. common voice: a massively-multilingual speech corpus.
in proceedings of the12th language resources and evaluation confer-ence, pages 4218–4222, marseille, france.
euro-pean language resources association..mikel artetxe and holger schwenk.
2019. mas-sively multilingual sentence embeddings for zero-shot cross-lingual transfer and beyond.
transac-tions of the association for computational linguis-tics, 7:597–610..alexei baevski, yuhao zhou, abdelrahman mohamed,and michael auli.
2020. wav2vec 2.0: a frame-work for self-supervised learning of speech represen-tations.
advances in neural information processingsystems, 33..claudio bendazzoli, annalisa sandrelli, et al.
2005.an approach to corpus-based interpreting studies:developing epic (european parliament interpretingcorpus).
proceedings of challenges of multidimen-sional translation..alan w black.
2019. cmu wilderness multilingualspeech dataset.
in icassp 2019-2019 ieee interna-tional conference on acoustics, speech and signalprocessing (icassp), pages 5971–5975.
ieee..herv´e bredin, ruiqing yin, juan manuel coria, gre-gory gelly, pavel korshunov, marvin lavechin,diego fustes, hadrien titeux, wassim bouaziz, andmarie-philippe gill.
2020. pyannote.audio: neuralbuilding blocks for speaker diarization.
in icassp2020, ieee international conference on acoustics,speech, and signal processing, barcelona, spain..roldano cattoni, mattia antonino di gangi, luisabentivogli, matteo negri, and marco turchi.
2020.must-c: a multilingual corpus for end-to-endspeech translation.
computer speech & language,66:101155..1001ronan collobert, christian puhrsch, and gabriel syn-naeve.
2016. wav2letter: an end-to-end convnet-based speech recognition system.
arxiv preprintarxiv:1609.03193..alexis conneau, alexei baevski, ronan collobert, ab-delrahman mohamed, and michael auli.
2020. un-supervised cross-lingual representation learning forspeech recognition..mattia a di gangi, roldano cattoni, luisa bentivogli,matteo negri, and marco turchi.
2019. must-c: amultilingual speech translation corpus.
in 2019 con-ference of the north american chapter of the asso-ciation for computational linguistics: human lan-guage technologies, pages 2012–2017.
associationfor computational linguistics..ewan dunbar, xuan nga cao,.
juan benjumea,julien karadayi, mathieu bernard, laurent besacier,xavier anguera, and emmanuel dupoux.
2017. thezero resource speech challenge 2017. in 2017 ieeeautomatic speech recognition and understandingworkshop (asru), pages 323–330.
ieee..alex graves, santiago fern´andez, faustino gomez,and j¨urgen schmidhuber.
2006.connectionisttemporal classiﬁcation: labelling unsegmentedse-quence data with recurrent neural networks.
icmlthe2006 - machine learning, proceedings oftwenty-third international conference..awni hannun, ann lee, qiantong xu, and ronan col-lobert.
2019. sequence-to-sequence speech recogni-tion with time-depth separable convolutions.
corr,abs/1904.02619..he he, jordan boyd-graber, and hal daum´e iii.
2016.interpretese vs. translationese: the uniqueness ofhuman strategies in simultaneous interpretation.
inproceedings of the 2016 conference of the northamerican chapter of the association for computa-tional linguistics: human language technologies,pages 971–976..kenneth heaﬁeld.
2011. kenlm: faster and smallerlanguage model queries.
in proceedings of the sixthworkshop on statistical machine translation, pages187–197..javier iranzo-s´anchez, joan albert silvestre-cerd`a,javier jorge, nahuel rosell´o, adri`a gim´enez, al-bert sanchis, jorge civera, and alfons juan.
2020.europarl-st: a multilingual corpus for speech trans-in icassp 2020-lation of parliamentary debates.
2020 ieee international conference on acoustics,speech and signal processing (icassp), pages8229–8233.
ieee..ye jia, ron j. weiss, fadi biadsy, wolfgang macherey,melvin johnson, zhifeng chen, and yonghui wu.
2019. direct speech-to-speech translation with ain interspeech 2019,sequence-to-sequence model.
20th annual conference of the international speechcommunication association, graz, austria, 15-19september 2019, pages 1123–1127.
isca..jacob kahn, ann lee, and awni hannun.
2020a.
self-training for end-to-end speech recognition.
inicassp 2020-2020 ieee international confer-ence on acoustics, speech and signal processing(icassp), pages 7084–7088.
ieee..jacob kahn, morgane rivi`ere, weiyi zheng, evgenypierre-emmanuelkharitonov, qiantong xu,mazar´e,julien karadayi, vitaliy liptchinsky,ronan collobert, christian fuegen, et al.
2020b.
libri-light: a benchmark for asr with limitedin icassp 2020-2020 ieeeor no supervision.
international conference on acoustics, speech andsignal processing (icassp), pages 7669–7673.
ieee..eugene kharitonov, morgane rivi`ere, gabriel syn-pierre-emmanuel mazar´e,naeve, lior wolf,matthijs douze, and emmanuel dupoux.
2020.data augmenting contrastive learning of speechrepresentations in the time domain..philipp koehn.
2005. europarl: a parallel corpus forstatistical machine translation.
in mt summit, vol-ume 5, pages 79–86.
citeseer..taku kudo and john richardson.
2018. sentencepiece:a simple and language independent subword tok-enizer and detokenizer for neural text processing.
inproceedings of the 2018 conference on empiricalmethods in natural language processing: systemdemonstrations, pages 66–71, brussels, belgium.
association for computational linguistics..ha nguyen,.
fethi bougares, n. tomashenko,y. est`eve, and l. besacier.
2020.investigatingself-supervised pre-training for end-to-end speechtranslation.
in interspeech..aaron van den oord, yazhe li, and oriol vinyals.
2018. representation learning with contrastive pre-dictive coding.
arxiv preprint arxiv:1807.03748..myle ott, sergey edunov, alexei baevski, angelafan, sam gross, nathan ng, david grangier, andfairseq: a fast, extensiblemichael auli.
2019.in proceedings oftoolkit for sequence modeling.
naacl-hlt 2019: demonstrations..vassil panayotov, guoguo chen, daniel povey, andlibrispeech: an asrsanjeev khudanpur.
2015.corpus based on public domain audio books.
in2015 ieee international conference on acoustics,speech and signal processing (icassp), pages5206–5210.
ieee..j. pino, qiantong xu, xutai ma, mohammad javaddousti, and yun tang.
2020. self-training for end-to-end speech translation.
in interspeech..vineel pratap, awni hannun, qiantong xu, jeff cai,jacob kahn, gabriel synnaeve, vitaliy liptchin-sky, and ronan collobert.
2018. wav2letter++:the fastest open-source speech recognition system.
corr, abs/1812.07625..1002you need.
in advances in neural information pro-cessing systems, volume 30, pages 5998–6008.
cur-ran associates, inc..changhan wang, yun tang, xutai ma, anne wu,dmytro okhonko, and juan pino.
2020a.
fairseq s2t:in pro-fast speech-to-text modeling with fairseq.
ceedings of the 2020 conference of the asian chap-ter of the association for computational linguistics(aacl): system demonstrations..changhan wang, anne wu, and juan pino.
2020b.
covost 2 and massively multilingual speech-to-texttranslation.
arxiv e-prints, pages arxiv–2007..anne wu, changhan wang, juan pino, and jiatao gu.
2020. self-supervised representations improve end-to-end speech translation.
in interspeech..qiantong xu, alexei baevski, tatiana likhoma-nenko, paden tomasello, alexis conneau, ronancollobert, gabriel synnaeve, and michael auli.
2020. self-training and pre-training are comple-arxiv preprintmentary for speech recognition.
arxiv:2010.11430..marcely zanon boito, william havard, mahault gar-nerin, ´eric le ferrand, and laurent besacier.
2020.mass: a large and clean multilingual corpus ofsentence-aligned spoken utterances extracted fromin proceedings of the 12th languagethe bible.
resources and evaluation conference, pages 6486–6493, marseille, france.
european language re-sources association..chen zhang, xu tan, yi ren, tao qin, kejun zhang,and tie-yan liu.
2020a.
uwspeech: speech tospeech translation for unwritten languages.
arxivpreprint arxiv:2006.07926..yu zhang, james qin, daniel s park, wei han, chung-cheng chiu, ruoming pang, quoc v le, andyonghui wu.
2020b.
pushing the limits of semi-supervised learning for automatic speech recogni-tion.
arxiv preprint arxiv:2010.10504..vineel pratap, qiantong xu, anuroop sriram, gabrielsynnaeve, and ronan collobert.
2020. mls: alarge-scale multilingual dataset for speech re-in proc.
interspeech 2020, pages 2757–search.
2761..morgane riviere and emmanuel dupoux.
2020. to-wards unsupervised learning of speech features inthe wild.
slt 2020: ieee spoken language tech-nology workshop..morgane rivi`ere, armand joulin, pierre-emmanuelmazar´e, and emmanuel dupoux.
2020. unsuper-vised pretraining transfers well across languages.
in icassp 2020-2020 ieee international confer-ence on acoustics, speech and signal processing(icassp), pages 7414–7418.
ieee..thomas schatz, vijayaditya peddinti, francis bach,aren jansen, hynek hermansky, and emmanueldupoux.
2013. evaluating speech features withthe minimal-pair abx task: analysis of the classi-cal mfc/plp pipeline.
in interspeech 2013: 14thannual conference of the international speech com-munication association, pages 1–5..steffen schneider, alexei baevski, ronan collobert,and michael auli.
2019. wav2vec: unsupervisedpre-training for speech recognition.
in proc.
inter-speech 2019, pages 3465–3469..h scudder.
1965. probability of error of some adap-ieee transac-.
tive pattern-recognition machines.
tions on information theory, 11(3):363–371..hiroaki shimizu, graham neubig, sakriani sakti,tomoki toda, and satoshi nakamura.
2014. collec-tion of a simultaneous translation corpus for compar-ative analysis.
in lrec, pages 670–673.
citeseer..vivek kumar rangarajan sridhar, john chen, andsrinivas bangalore.
2013. corpus analysis of simul-taneous interpretation data for improving real timespeech translation.
in interspeech, pages 3468–3472..chaitanya talnikar, tatiana likhomanenko, ronanjointcollobert, and gabriel synnaeve.
2020.masked cpc and ctc training for asr.
arxiv preprintarxiv:2011.00093..andros tjandra, sakriani sakti, and satoshi nakamura.
2019. speech-to-speech translation between untran-in 2019 ieee auto-scribed unknown languages.
matic speech recognition and understanding work-shop (asru), pages 593–600.
ieee..hitomi tohyama, shigeki matsubara, koichiro ryu,n kawaguch, and yasuyoshi inagaki.
2004. ciairsimultaneous interpretation corpus.
in proc.
orien-tal cocosda..ashish vaswani, noam shazeer, niki parmar, jakobuszkoreit, llion jones, aidan n gomez, ł ukaszkaiser, and illia polosukhin.
2017. attention is all.
1003