timedial: temporal commonsense reasoning in dialog.
lianhui qin♦.
aditya gupta♠.
shyam upadhyay♠.
∗.
luheng he♠.
yejin choi♦ manaal faruqui♠♠google assistant♦paul g. allen school of computer science & engineering, university of washington{gaditya, shyamupa, luheng, mfaruqui}@google.com{lianhuiq, yejin}@cs.washington.edu.
abstract.
everyday conversations require understandingeveryday events, which in turn, requires un-derstanding temporal commonsense conceptsinterwoven with those events.
despite re-cent progress with massive pre-trained lan-guage models (lms) such as t5 and gpt-3,their capability of temporal reasoning in di-alogs remains largely under-explored.
in thispaper, we present the ﬁrst study to investi-gate pre-trained lms for their temporal rea-soning capabilities in dialogs by introducinga new task and a crowd-sourced english chal-lenge set, timedial.
we formulate time-dial as a multiple choice cloze task with over1.1k carefully curated dialogs.
empirical re-sults demonstrate that even the best perform-ing models struggle on this task compared tohumans, with 23 absolute points of gap in ac-curacy.
furthermore, our analysis reveals thatthe models fail to reason about dialog contextcorrectly; instead, they rely on shallow cuesbased on existing temporal patterns in context,motivating future research for modeling tem-poral concepts in text and robust contextualreasoning about them.
the dataset is pub-licly available at: https://github.com/google-research-datasets/timedial..1.introduction.
humans can effortlessly reason about temporal con-cepts of everyday events such as their duration, fre-quency, or relative ordering (allen, 1984; radvan-sky and zacks, 2014) based on rich commonsenseknowledge about how the world works, especiallyin relation to time.
however, reasoning about suchconcepts has been challenging for machines (kahnand gorry, 1977; kozareva and hovy, 2011) sinceit requires both understanding the local temporalexpressions and reasoning about their global con-texts such as their relative ordering and relations.
∗work done during an internship at google..a: may we see the wine list please.
b: sure.
our special wine today is a 1989 chardonnay.
a: i’d like a bottle please.
b: i’ll need to see your id please.
a: here you go.
b: sorry about the inconvenience, you look so young.
ihad to make sure you are over.
..a) 21 years old (cid:51)c) 4 years old (cid:55).
b) 30 years old (cid:55)d) 18 years old (cid:51).
a: good morning!
may i help you?
b: yes.
my wife and i are interested in renting a housefor the summer.
a: very well.
how long do you want the house?
allsummer?
b: no, not all summer.
just for six weeks .
a: i am afraid i can only rent it for two months .
, but i think my brotherb: my holiday is onlyand his family would take it for the other two weeks ..a) six decades (cid:55)c) six weeks (cid:51).
b) 45 days (cid:51)d) two months (cid:55).
table 1: examples from our timedial challenge set,demonstrating the need for commonsense knowledgeand arithmetic reasoning over the context to infer thecorrect answers.
key contextual information for rea-soning success is highlighted..(uzzaman et al., 2013; ning et al., 2018b; puste-jovsky, 2017).
the problem becomes even morechallenging in dialogs, where explicit and implicitinter-dependencies among temporal concepts canappear across conversation turns..for instance, for the ﬁrst dialog in table 1, onemust understand the context, i.e., selling wine, anduse world knowledge of minimum legal drinkingage in order to reason about correct answers to ﬁllin the blank.
similarly, in the second conversation,commonsense about the durations summer, month,week, day and their relations, plus numerical rea-soning, are necessary to make the inference..although previous works have studied tempo-ral reasoning in natural language, they have ei-ther focused on speciﬁc time-related concepts in.
proceedingsofthe59thannualmeetingoftheassociationforcomputationallinguisticsandthe11thinternationaljointconferenceonnaturallanguageprocessing,pages7066–7076august1–6,2021.©2021associationforcomputationallinguistics7066isolation, such as temporal ordering and relationextraction (leeuwenberg and moens, 2018; ninget al., 2018a), and/or dealt with limited context,such as single-sentence-based question answering(zhou et al., 2019) and natural language infer-ence (vashishtha et al., 2020; mostafazadeh et al.,2016)..in this work, we make the ﬁrst systematic studyof temporal commonsense reasoning in a multi-turndialog setting.
the task involves complex reason-ing that requires operations like comparison andarithmetic reasoning over temporal expressions andthe need for commonsense and world knowledge.
we design a new task for dialog-based tempo-ral reasoning and present a new challenge set inenglish, called timedial, to evaluate languageunderstanding models on the task.
we formulatethe problem as a crowd-sourced cloze task withmultiple choices based on dialogs in the dailydi-alog dataset (li et al., 2017).
given a dialog withone temporal span masked out, the model is askedto ﬁnd all correct answers from a list of four op-tions to ﬁll in the blank (table 1)..the challenge set requires the models to demon-strate understanding of the context and use tempo-ral commonsense to make right choices.
our ﬁnalchallenge set consists of 1.1k carefully curateddialog instances..we then study the performance of several state-of-the-art pre-trained language models on time-dial along several dimensions including modelingparadigms (classiﬁcation, mask ﬁlling, and gener-ation), the scope of dialog contexts, in-domain vs.out-of-domain training, dependence on shallow textmatching for reasoning, and the types of reasoningrequired.
our experiments demonstrate that off-the-shelf, pre-trained language models cannot ef-fectively reason about temporal aspects in a dialog,even with domain-speciﬁc ﬁnetuning.
our ﬁndingsindicate that large-scale pre-trained models evenafter ﬁne-tuning may not be sufﬁcient for robusttemporal reasoning in dialogs, and motivate futureresearch toward modeling temporal concepts overdiverse everyday events, and contextual reasoningabout them..2 task: temporal reasoning in dialog.
we formulate the dialog-based temporal common-sense reasoning problem as a cloze task (taylor,1953).
formally, given a multi-turn dialog contextof n conversational turns between two speakers a.and b, where a temporal words span within the con-text is masked out, the task is to predict the suitabletemporal expression(s) for the masked-out spanfrom a list of options.
that is, we want the conver-sation model to select all the correct answers fromthe options based on the dialog context.
followingsimilar cloze-style challenge datasets, we use accu-racy as the evaluation metric (mostafazadeh et al.,2016; onishi et al., 2016; mihaylov and frank,2018)..having a non-trivial set of options is crucial tobuild a challenge set and to avoid accidental spuri-ous biases (geirhos et al., 2020; gururangan et al.,2018; le bras et al., 2020).
we ensure this via thefollowing ﬁltering process.
(1) for each maskedspan, there is more than one correct answer in theoptions.
this makes the task more challenging formodels since more comprehensive understandingof the context is required to recognize all the cor-rect choices.
in our dataset (§3) we guarantee twocorrect answers for each masked span.
(2) someincorrect options are selected to be spuriously cor-related with the dialog context.
for example, weinclude temporal spans in the dialog context as neg-ative options, which will challenge models that relyprimarily only on shallow pattern matching with-out correct temporal reasoning.
we present moreinformation in §3 about how the negative optionswere created by human annotators..3 dataset: timedial.
the timedial dataset is derived from dailydi-alog data (li et al., 2017), which is a multi-turndialog corpus containing over 13k english dialogs.
dialogs in this dataset consist of turn-taking be-tween two people on topics over 10 broad cate-gories, ranging from daily lives to ﬁnancial topics..3.1 data collection.
our data collection process involves two steps: (1)identifying dialogs that are rich in temporal expres-sions, and (2) asking human annotators to providecorrect and incorrect options for cloze instancesderived from these dialogs.
we now describe thesesteps in detail..temporal expression identiﬁcation.
here, weselect dialogs that are rich with temporal informa-tion, in order to focus on complex temporal rea-soning that arises in natural dialogs.
temporalexpressions are automatically identiﬁed with su-time (chang and manning, 2012), an off-the-shelf.
7067category.
dialog.
worldknowledge(5%).
comparison(24%).
arithmetic(5%).
a: may we see the wine list ?
b: sure .
our special wine today is a 1989 chardonnay .
a: that sounds pretty good!
how much is it ?
b: it’s $4.25 cents by the glass .
thewhole bottle is $22.25 .
a: i’d like a bottle please .
b: i’ll need to see your id please .
a: here you go .
b: sorry about the inconvenience, i had make sure you are over.
..a: yes , sir.
may i help you?
b: please i’d like a ticket to new york.
a: for today?
b: no, early saturday morning .
a: we have a ﬂight that we’ll put you there atnothing earlier?
i prefer ﬂight at 9 thirty.
a: i’m afraid not , unless you want a night ﬂight.
b: no, exactly not...
is that ok?
b:.
a: how long do you want the house ?
all summer ?
b: no , just for six weeks.
a: i’m afraid i can only rent it for two months .
b: my holiday is onlythe other two weeks .., but i think my brother and his family would take it for.
generalcommonsense(60%).
a: do you get up early every morning ?
b: about 6 in the morning.
i like to walk to the ofﬁce .
a: good habit.
how long does it take ?
b:a: no , my little sister lives with me .
.
...
do you live alone ?.
options.
(cid:51) 21 years old(cid:55) 30 years old(cid:55) 4 years old(cid:51) 18 years old.
(cid:51) ten am(cid:55) 9:30 pm(cid:51) eleven am(cid:55) four am.
(cid:55) six decades(cid:51) 45 days(cid:51) six weeks(cid:55) two months.
(cid:51) 20 minutes(cid:55) 10 seconds(cid:51) 15 minutes(cid:55) 20 hours.
others(6%).
a: how long does a facial service take?
b: we have half-hour and one-hour treatments.
a: what’s the regular price?
b: well , the half-hour facial costs $50 andthe one-hour costs $80.
a: good , i will take.
facial.
b: that’s ﬁne , madam..(cid:51) the one hour(cid:55) the 20 hour(cid:55) the 80 second(cid:51) the half hour.
table 2: example dialogs and answer options from the timedial dataset, categorized by the nature of reasoningrequired to correctly answer them, along with the percentage of each reasoning category in the set of 100 sampledexamples.
the relevant key information in the dialog context is highlighted..temporal expression detector.1 we keep only thedialogs with more than 3 temporal expressions andat least one expression that contains numerals like“two weeks” (as opposed to non-numeric spans, like“summer”, “right now”, and “later”).
in our initialexperiment, we observe that language models canoften correctly predict these non-numerical tempo-ral phrases..we note that temporal expressions containingnumerals serve as more challenging sets of optionsthan non-numerical ones.
this ﬁltering step resultsin 1,127 unique dialogs for further processing..annotators2 were asked to (1) come up with an al-ternative correct answer that makes sense in thedialog adhering to commonsense, and (2) formu-late two incorrect answers that have no possibilityof making sense in the dialog context.
we high-light all time expressions in the context to makeit easier for annotators to select reasonable timeexpressions..to ensure that the annotated incorrect optionsare not too trivially distinguishable by the models(as discussed in §2), we deﬁne three rules for theannotators to follow..human annotated options.
next, we makespans in the dialogs.
for a dialog, we mask outeach temporal expression that contains numerals,each resulting in a cloze question that is then sentfor human annotation..this resulted in 1,526 instances for annotation.
for each masked span in each dialog, we obtainhuman annotation to derive a ﬁxed set of correctand incorrect options given the context.
concretely,given a masked dialog and a seed correct answer(i.e., the original text) for the masked span, the.
• rule 1: phrase matching.
the rater shouldﬁrst try to pick another temporal span from thedialog context that makes syntactic/semanticsense (e.g., when the span is of the appro-priate type, such as duration, for the maskedspan) but is still incorrect according to com-monsense..• rule 2: numeral matching.
if rule 1 doesnot apply, raters should follow a relaxed ver-sion of rule 1, whereby the incorrect optionshould contain any numeral occurring in thedialog context..1https://nlp.stanford.edu/software/.
sutime.shtml.
2who are english linguists..7068# dialog instances# temporal expressions# avg.
turns per dialog# avg.
words per turn# avg.
time spans per dialog.
1, 1041, 98511.716.53.0.incorrect options.
% phrase matching% numeral matching% open-ended.
16.3 %49.6 %45.4 %.
instance can involve multiple reasoning types, weassociate it with one predeﬁned category label thatindicates the primary type of reasoning it requires.
table 2 shows the category distribution and exam-ples in each of the category.
we observe that thedataset requires general commonsense for 60% ofthe dialogs, making it the most common reasoningtype..table 3: statistics of our timedial challenge set..4 modeling.
• rule 3: open-ended.
if neither of the aboverules is applicable, then raters can come upwith an incorrect option using their own judg-ment.
the two incorrect options are requiredto differ from each other as much as possible..rules-1&2 are designed to confuse models thatrely on shallow pattern matching.
finally, to en-sure the quality of the human-annotated options,we perform a subsequent round of human valida-tion on the gathered data.
the validators identifyand ﬁx issues such as duplicate options, unreason-able or obscure annotations w.r.t natural usage, orungrammatical annotations that do not ﬁt in thecontext..3.2 properties of timedial.
table 3 shows statistics of timedial.
the datasetcontains over 1.1k test instances.
each dialog con-tains 11.7 turns and 3 temporal expressions on av-erage, presenting richer and more complex contextcompared to the recent single-sentence-based tem-poral question answering benchmarks (e.g., zhouet al., 2019; vashishtha et al., 2020).
as above,each test instance contains two correct answers andtwo incorrect ones.3 over half of the incorrect op-tions are annotated based on phrase and numeralmatching from context, which pose a signiﬁcantchallenge for models relying on shallow text match-ing, as we show in our experimental analysis (§5).
answering different instances in the dataset re-quires different types of core reasoning abilities,such as comparison, arithmetic inference, or rea-soning based on world knowledge or general com-monsense.
to facilitate ﬁne-grained analysis, wealso annotate the reasoning categories for a ran-domly sampled set of 100 dialogs.
though each.
3we also collected 342 extra instances for which the an-notators deem there is only one unique correct answer for thecontext.
thus, each of those instances contains one correctoption and two incorrect ones.
we release those instancesalong with the dataset, though we did not include them inempirical study in this paper..we consider a broad set of methods and evalu-ate their performance on our challenge timedialdataset.
these methods vary in terms of the mod-eling paradigms, the scope of the dialog contexts,and training settings.
in particular, they encompassthe major ways pre-trained lms are currently usedin downstream tasks (§4.1) which often outperformearlier specialized non-pretrained models.
we alsoconsider different lengths of context used in reason-ing, varying by their vicinity to the masked span(§4.2).
finally, we study different training settings,including zero-shot, in-domain, and out-of-domaintraining (§4.3)..4.1 modeling paradigms.
we experiment across three major modelingparadigms: (i) binary classiﬁcation, (ii) mask fill-ing, and (iii) generation.
figure 1 shows the differ-ent architectures.
for each test instance, the modeltakes as input a pair of (masked dialog context, can-didate), and outputs a score measuring how likelythe candidate being a correct answer.
based onthe prediction scores of all options, the model thenchooses the top two positive candidates as the pre-dicted answer for the instance.
each paradigm ofmodels is ﬁnetuned using training data from differ-ent domains, as discussed in §4.3..4.1.1 binary classiﬁcation.
in this setting, we formulate the task as a binaryclassiﬁcation problem, i.e., we use a classiﬁer tomeasure the probability of the candidate in the(masked dialog context, candidate) pair being a cor-rect answer.
any powerful lm — e.g., bert (de-vlin et al., 2019), albert (lan et al., 2019),roberta (liu et al., 2019), etc.
can be used tobuild the classiﬁer..this method’s key challenge is the lack of an-notated training data for direct supervision.
wegenerate weak supervision training data as follows.
in an unlabeled corpus, we use the sutime tool.
7069figure 1: we study three modeling paradigms for the task, based on bert and t5, including (1) classiﬁcation, (2)mask filling, and (3) generation (§4.1).
the models are ﬁnetuned with various training data, as discussed in §4.3..to annotate temporal spans.
we mask each tempo-ral span in this corpus and use the masked text asone positive example for binary classiﬁcation.
togenerate negative example, we randomly sampleanother temporal span from the dialog context anduse it as a negative example for the masked tempo-ral span.
the resulting data is noisy because therandomly sampled temporal span can also logicallyﬁt in the masked span in the given context; how-ever, we assume the likelihood of that happeningis low.
we leave drawing harder negative instancesusing heuristics to future work..4.1.2 mask filling.
we also use the mask ﬁlling approach of bert-like mask language models (mlms).
for eachdialog context and a candidate temporal span of mtokens, we replace the blank in the dialog contextwith m masked tokens.
we then evaluate the like-lihood of predicting the temporal span tokens forthose masked positions, and make average acrossthe positions.
a key advantage of this methodis that we can directly apply a bert model inthe zero-shot manner since the model was pre-trained in the same way, as for accommodating for[mask] ﬁllings.
additionally, we also ﬁnetunebert’s mlm for learning task speciﬁc properties..4.1.3 generation.
the third method is a fully generative approachusing the text-to-text paradigm of t5 (raffel et al.,2020).
given a masked dialog context, the modelis trained to generate the masked text in an encoder-decoder framework.
as a result, evaluating thelikelihood of generating the given temporal span(normalized with the length of the span) is used asthe probability of it being correct.
similar to mask.
ﬁlling, we use t5 either in a zero-shot manner orwith additional ﬁne-tuning..4.2 dialog context.
we aim to study the inﬂuence of context on amodel’s temporal reasoning in dialog by incorpo-rating varying scopes of dialog context based ontheir vicinity to the target span.
since the dialogsin timedial are rich in temporal concepts, wewant to evaluate lms’ dependence on shallow textmatching vs. the ability to accurately understandthe causal relations between those concepts (seetable 6).
we use the following three settings:.
• full context, where the model is presentedwith the complete available dialog to reasonon.
due to our design of challenging nega-tives, the full context can often confuse mod-els that rely on shallow cues..• local context, where we provide only withthe utterances that immediately precede andfollow the target utterance..• target context, where the context is restrictedto only the particular utterance that containsthe masked span..4.3 training details.
for all models, we consider two common trainingsettings, e.g., in-domain data, which is typicallysmall, and out-of-domain training where a largeamount of data is available.
table 4 shows trainingdata statistics.
for mask-ﬁlling and generation, wealso evaluate in a zero-shot setup with no ﬁnetun-ing..in-domain training.
our challenge timedialtest set is derived from contextually rich dialogs.
7070input: [cls] a: a: i’m … b: my holiday is only [mask] … [sep] six weeksclassiﬁcation layerbertoutput:1(1) classificationinput:  [cls] a: i’m … b: my holiday is only [mask] [mask] ….bertoutput:six       weeks(2) mask fillingt5 encodert5 decoderinput:  a: i’m … b: my holiday is only [mask] ….output:(3) generationsix weeksinput:…… b:  no, not all summer.
just for six weeks.
a:  i am afraid i can only rent it for two months.
b:  my holiday is only _______, but i think my brother and his family would take it for the other two weeks.options:a)six decades   b) 45 days c) six weeks   d) two monthsmask filling and generation.
size-train.
2-best acc (%).
# train.
# dev.
classiﬁcation (bert).
in-domain (daily)out-domain (meena).
14.5k1.26m.
2.4k23k.
classiﬁcation.
# train.
# dev.
# spans.
in-domain (daily)out-domain (meena).
58.0k5.04m.
9.6k92k.
2,15338,750.table 4: number of training and development instancesfor different settings.
an instance is derived by mask-ing one temporal span of a dialog.
for classiﬁca-tion, we draw 3 negative samples per positive sample.
“# spans” is the size of temporal span pool from whichnegative samples are drawn for weak supervision..from the dailydialog dataset, based on the num-ber of temporal spans.
however, this still leavesremaining data with less than 3 temporal spans orwith no numeric span.
by masking each tempo-ral span in each dialog, we obtain 14.5k traininginstances to use in our domain speciﬁc ﬁne-tuning..out-of-domain training.
in this setting, we con-sider a much larger corpus from a general do-main.
speciﬁcally, we use the large scale trainingset based on the meena dataset adiwardana et al.
(2020), which is mined and ﬁltered from publicdomain social media conversations over 341gbof text (40b words).4 compared to the above in-domain data from dailydialog which were man-ually written by human annotators in a clean andconsistent way, the dialogs in the meena corpustend to be noisy, casual, and usually short.
like ourdailydialog processing, we identify all temporalexpressions for dialogs in meena using sutime..5 experiments and analyses.
using the proposed timedial challenge set, wenext conduct extensive experiments and analyseson the different model variants and context settings.
we use either 4x4 or 8x8 cloud tpus v3 podslices5 for ﬁne-tuning and one v100 gpu for infer-ence.
we provide more details of the experimentconﬁgurations in the appendix..evaluation.
since each example of timedialcontains two correct answers, we report the met-ric 2-best accuracy, which measures whether bothof the model’s top-ranked answers are correct.
in.
4we acquired a trimmed down version of the meena.
dataset by contacting the authors..5https://cloud.google.com/tpu.
mask filling (bert).
generation (t5).
base-outbase-inlarge-outlarge-in.
base-zerobase-outbase-inlarge-zerolarge-outlarge-in.
base-zerobase-outbase-inlarge-zerolarge-outlarge-in.
human.
43.151.148.753.2.
44.847.467.447.754.870.0.
39.850.659.239.161.974.8.
97.8.table 5: model and human performance on timedial.
base and large denote the size of the pre-trainedbert and t5; zero, in, and out denote that themodel is zero-shot (with no ﬁnetuning), ﬁntuned usingthe in-domain dailydialog data, or ﬁnetuned using theout-of-domain meena data, respectively.
the full dia-log context is used for all models..other words, if the model erroneously ranks an in-correct answer over a correct one, we consider it tobe an error case.
note that we use the ranking-based metric as opposed to classiﬁcation-basedones (for example, by asking the model to clas-sify whether each individual candidate answer iscorrect or not (e.g., zhou et al., 2019)) and becauseit presents a stricter measure that penalizes any in-correct answers being ranked over correct answers,and the ranking metric is not inﬂuenced by speciﬁcchoices of the threshold hyperparameter that cutsoff positive and negative predictions..5.1 model performance.
table 5 shows model results and human perfor-mance.
human performance achieves a near-perfect level (97.80, with cohen’s kappa score of0.86 showing almost perfect inter-rater agreement(landis and koch, 1977))..overall.
the generation model based on t5-large and ﬁnetuned on the in-domain dailydia-log data achieves the best performance.
however,its 2-best accuracy (74.8) lagged far behind the hu-man performance, demonstrating the difﬁculty ofthe timedial challenge set..7071dialog context.
options.
gold cls.
mf.
gen.a: what’s the date today?
b: today is september 28th, 2007.a: i have a meeting this afternoon.
b: when will it begin?
a: it will begin at three o’clock.
what’s the time now?
b: it isa: i have to go now.
i don’t want to be late.
b: don’t worry, time is enough....a: doctor, i feel much better now.
will i be able to go homesome time this week?
b: that’s good to hear.
you’ve had an ideal recovery from youroperation.
we’re going to send you home tomorrow.
a: do you think i can get back to work very soon?
b:don’t be in such a hurry.
i’m conﬁdent that you’ll becompletely recovered in.
a: is there anything i should do?
b: you’d better have a good rest for a week..half past one.
quarter to two.
half past three.
half past nine.
4 to 6 weeks.
5 to 7 weeks.
a week.
a day.
(cid:51).
(cid:51).
(cid:55).
(cid:55).
(cid:51).
(cid:51).
(cid:55).
(cid:55).
(cid:51).
(cid:55).
(cid:51).
(cid:55).
(cid:55).
(cid:51).
(cid:51).
(cid:55).
(cid:55).
(cid:55).
(cid:51).
(cid:51).
(cid:55).
(cid:55).
(cid:51).
(cid:51).
(cid:51).
(cid:55).
(cid:51).
(cid:55).
(cid:51).
(cid:55).
(cid:51).
(cid:55).
table 6: example prediction errors made by different models for cases with challenging options, based on thephrase and numeral matching rules (§3).
gold denotes the true labels.
the model predictions show that themodels get confused by learning shallow text matching in terms of pre-existing temporal concepts (marked bybold faced text) in the context..out-of-domain vs..in-domain.
zero-shot vs.when comparing the different training data setup,we observe that models with in-domain training us-ing the dailydialog data (e.g., large-in) consis-tently outperforms those trained on the large out-of-domain meena dataset (e.g., large-out).
bothsetups outperform the zero-shot models (withoutany ﬁne-tuning) (e.g., large-zero).
the resultsshow that the large lms still highly depend on in-domain or at least dialog data to grasp and enhancetheir temporal reasoning ability in dialog context.
further, we see increasing performance with in-creasing model size, which is not unexpected giventhe complexity of the task..5.2 error analysis.
figure 2: percentage of errors on options created by dif-ferent rules.
cls, mf, and gen represent classiﬁca-tion, mask-ﬁlling, and generation models, respectively;and in and out denote in-domain and out-of-domaintraining.
all models are of large size..next, we analyze the different types of errors basedon different rules for negative option creation inthe annotation process.
in particular, the phrasematching rule picks an exact time span from thedialog context, and numeral matching picks numer-als from the dialog context.
thus, models pickingthose incorrect options imply reliance on spuriousshallow text matching features..figure 2 shows the percentage of errors in termsof the different rules.
for example, the bert-based classiﬁcation model cls-in erroneouslypicks 52% of negative options created by the phrasematching rule as correct answers (i.e., by rankingthose negative options over the true correct options).
we observe that the various models are all most vul-nerable to the phrase matching options compared.
to other types of negative options, showing thatthey rely on spurious text matching to a signiﬁcantextent.
between bert and t5, we ﬁnd t5 beingmore robust to shallow text matching..table 6 provides further examples of predictionerrors, illustrating confusions due to shallow textmatching.
in the ﬁrst dialog, both incorrect an-swers already partially occur in the context or arerelated to preexisting concepts (i.e., “three” to“three o’clock”, and “nine” to “september”).
allthe three models were confused and chose eitherof the two as the top prediction for the blank, eventhough the options clearly violate the context.
in-terestingly, the mask ﬁlling model was completelyconfused and ranked both incorrect answers overthe correct ones.
similarly in the second example,.
707200.150.30.450.6cls-incls-outmf-inmf-outgen-ingen-outerror-phrase matchingerror-numeral matchingerror-open-endedtable 1modelerror 1error 2error 3cls-in0.524663680.196734690.20594634cls-out0.551569510.180408160.2559826mf-in0.186098650.10775510.15663524mf-out0.26905830.178775510.24873096gen-in0.195067260.062857140.14503263gen-out0.298206280.124897960.21392313bertt51sizetraining.
basein.
out.
largein.
out.
classiﬁcation (bert).
targetlocalfull.
40.0.
50.5.
47.5+ 3.4 + 3.3 + 7.5 + 2.0− 0.6 − 0.1 + 2.7 + 1.2.
50.5.mask filling (bert).
targetlocalfull.
44.3.
57.8.
46.8+ 5.4 + 3.0 + 8.1 + 4.9+ 9.6 + 3.1 + 9.6 + 8.0.
60.3.generation (t5).
targetlocalfull.
55.5.
45.9.
56.1+ 3.7 + 2.7 + 6.1 + 3.7+ 3.7 + 4.7 + 8.2 + 5.8.
66.7.table 7: impact of dialog context on reasoning accu-racy.
in and out denote in-domain and out-of-domaintraining, respectively.
we use 2-best accuracy of targetcontext as reference and report the absolute changes inperformance of local and full context, respectively.
lo-cal dialog context results in better performance to fulldialog context on 5 of the 12 cases, which are high-lighted in the table..the models fail to capture the contextual semantics..5.3.inﬂuence of dialog context.
table 7 shows how different scopes of dialog con-text (§4.2) affect model performance.
first, themost restrictive target-only context is insufﬁcientfor accurate reasoning, by producing the weakestperformance of most models.
this highlights theimportance of context information for temporalcommonsense reasoning in dialog, which differsfrom previous temporal reasoning studies based onlimited context (e.g., single-sentence question an-swering).
second, we note that the full dialog con-text does not always lead to the best performance.
in 5 out of the 12 cases, using the local contextyields equal or higher reasoning accuracy.
theresults show that the lms still fall short of prop-erly modeling the rich dialog contexts and makingeffective use of all information to do reasoning..5.4 errors of reasoning categories.
figure 3 shows the percentage of errors in eachreasoning category.
we observe that the modelstend to make non-trivial portions of errors on com-monsense/world knowledge questions.
for exam-ple, the strongest model, t5 gen-in, failed on18% of the instances that require commonsense orworld knowledge, while bert cls-in made er-rors on 48% of such instances.
the performance.
figure 3: percentage of errors on different reasoningtypes.
cls, mf, and gen represent classiﬁcation,mask-ﬁlling, and generation models, respectively.
allmodels are of large size..on comparison-based instances seems similar..6 related work.
temporal commonsense reasoning.
early stud-ies related to temporal analysis deﬁne time in thecontext of sets and relations (bruce, 1972; allen,1983).
more recent works often associate timewith events and focus on identifying time expres-sions (chang and manning, 2012; angeli et al.,2012; lee et al., 2014), extracting temporal re-lations among events(setzer and gaizauskas,2000; pustejovsky et al., 2005; lapata and las-carides, 2006; chambers et al., 2007; ning et al.,2018b), and timeline construction (do et al., 2012;leeuwenberg and moens, 2018)..some recent work has focused on building chal-lenging benchmarks for temporal commonsensereasoning.
story cloze test focuses on stereotyp-ical causal temporal and causal relations betweenevents (mostafazadeh et al., 2016).
vashishtha et al.
(2020) recast temporal reasoning datasets for eventduration and event ordering into the natural lan-guage inference (nli) format.
turque (ning et al.,2020) is an reading comprehension dataset wherethe model needs to answer questions such as “whathappens before/after [event]”.
most related to ourwork is mctaco (zhou et al., 2019), a dataset forevaluating temporal commonsense in the form ofmultiple-choice reading comprehension, where thecontext usually consists of a single sentence.
ourwork instead studies temporal commonsense rea-soning in dialogs which often require signiﬁcantcommonsense and world knowledge to reason overrich context (qin et al., 2019b; dinan et al., 2018)..commonsense reasoning with lms.
with therecent success of large pre-trained language models.
7073table 1commonsense/world knowledge                        comparisonarithmetic/otherscls-in0.47692307690.54166666670.6363636364cls-out0.41538461540.45833333330.2727272727mf-in0.24615384620.20833333330.09090909091mf-out0.35384615380.33333333330.3636363636gen-in0.18461538460.16666666670.1818181818gen-out0.23076923080.41666666670.27272727270.000.180.350.530.70cls-incls-outmf-inmf-outgen-ingen-outcommonsense/world knowledgecomparisonarithmetic/othersbertt51(lms) (devlin et al., 2019; brown et al., 2020), it isan open question whether these models, pretrainedon large amounts of data, capture commonsenseknowledge.
several works have been proposed toassess the ability of lms for commonsense or nu-merical reasoning (zhang et al., 2020; bouraouiet al., 2020), or to mine commonsense knowledgefrom lms (davison et al., 2019).
lin et al.
(2020)showed that state-of-the-art lms such as bertand roberta performs poorly on numerical rea-soning tasks without any ﬁnetuning.
works havealso been proposed to improve language model’scommonsense reasoning (qin et al., 2020, 2019a;zhou et al., 2020) and numerical reasoning abil-ities (geva et al., 2020).
in our work, we studyseveral modeling approaches and ﬁnetuning set-tings of large lms, and establish strong baselinesfor temporal commonsense reasoning in dialogs..7 conclusions.
we introduced timedial, a challenge set consist-ting of 1.1k multiple-choice cloze questions fortemporal commonsense reasoning in dialog.
thedataset is carefully curated to evaluate a models’ability to do temporal commonsense/numerical rea-soning over dialog context.
in order to establishstrong baselines and provide information on futuremodel development, we conducted extensive exper-iments with state-of-the-art language models withdifferent settings: the scope of context, weak su-pervision strategies, and learning objectives.
whilehumans can easily answer these questions (97.8%accuracy), even our best model variant (t5-largewith in-domain training) struggles on this challengeset (73%).
moreover, our qualitative error analy-ses show that these large language models oftenrely on shallow, spurious features (particularly textmatching) when answering these questions, insteadof truly doing reasoning over the context..references.
daniel adiwardana, minh-thang luong, david r so,jamie hall, noah fiedel, romal thoppilan, zi yang,apoorv kulshreshtha, gaurav nemade, yifeng lu,et al.
2020. towards a human-like open-domainchatbot.
arxiv preprint arxiv:2001.09977..james f allen.
1983. maintaining knowledge abouttemporal intervals.
communications of the acm,26(11):832–843..james f allen.
1984. towards a general theory ofaction and time.
artiﬁcial intelligence, 23(2):123–154..gabor angeli, christopher d manning, and dan juraf-sky.
2012. parsing time: learning to interpret timeexpressions.
in proc.
of naacl..zied bouraoui,.
ands. schockaert.
2020. inducing relational knowledgefrom bert.
in proc.
of aaai..jos´e camacho-collados,.
tom b. brown, benjamin mann, nick ryder, melaniesubbiah, jared kaplan, prafulla dhariwal, arvindneelakantan, pranav shyam, girish sastry, amandaaskell, sandhini agarwal, ariel herbert-voss,gretchen krueger, tom henighan, rewon child,aditya ramesh, daniel m. ziegler, jeffrey wu,clemens winter, christopher hesse, mark chen,eric sigler, mateusz litwin, scott gray, benjaminchess, jack clark, christopher berner, sam mc-candlish, alec radford, ilya sutskever, and darioamodei.
2020. language models are few-shot learn-ers..bertram c bruce.
1972. a model for temporal refer-ences and its application in a question answering pro-gram.
artiﬁcial intelligence, 3:1–25..nathanael chambers, shan wang, and dan juraf-sky.
2007. classifying temporal relations betweenevents.
in proc.
of acl..angel x chang and christopher d manning.
2012. su-time: a library for recognizing and normalizing timeexpressions.
in proc.
of lrec..joe davison, joshua feldman, and alexander rush.
2019. commonsense knowledge mining from pre-trained models.
in proc.
of emnlp..jacob devlin, ming-wei chang, kenton lee, andkristina toutanova.
2019. bert: pre-training ofdeep bidirectional transformers for language under-standing.
in proc.
of naacl..emily dinan, stephen roller, kurt shuster, angelafan, michael auli, and jason weston.
2018. wizardof wikipedia: knowledge-powered conversationalagents.
in proc.
of iclr..quang do, wei lu, and dan roth.
2012. joint infer-in proc.
of.
ence for event timeline construction.
emnlp..robert geirhos,.
j¨orn-henrik jacobsen, claudiomichaelis, richard zemel, wieland brendel,matthias bethge, and felix a wichmann.
2020.shortcut learning in deep neural networks.
naturemachine intelligence, 2(11):665–673..mor geva, ankit gupta, and jonathan berant.
2020.injecting numerical reasoning skills into languagemodels.
in proc.
of acl..suchin gururangan, swabha swayamdipta, omerlevy, roy schwartz, samuel bowman, and noah asmith.
2018. annotation artifacts in natural lan-in proc.
of naacl, pagesguage inference data.
107–112..7074kenneth kahn and g.anthony gorry.
1977. mecha-nizing temporal knowledge.
artiﬁcial intelligence,9(1):87 – 108..z. kozareva and e. hovy.
2011. learning temporalin 2011 ieeeinformation for states and events.
fifth international conference on semantic comput-ing, pages 424–429..zhenzhong lan, mingda chen, sebastian goodman,kevin gimpel, piyush sharma, and radu soricut.
2019. albert: a lite bert for self-supervised learn-arxiv preprinting of language representations.
arxiv:1909.11942..qiang ning, hao wu, rujun han, nanyun peng, mattgardner, and dan roth.
2020. torque: a readingcomprehension dataset of temporal ordering ques-tions.
in proc.
of emnlp..qiang ning, hao wu, haoruo peng, and dan roth.
2018b.
improving temporal relation extraction witha globally acquired statistical resource.
in proc.
ofnaacl..takeshi onishi, hai wang, mohit bansal, kevin gim-pel, and david mcallester.
2016. who did what: ain proc.
large-scale person-centered cloze dataset.
of emnlp..j richard landis and gary g koch.
1977. the mea-surement of observer agreement for categorical data.
biometrics, pages 159–174..james pustejovsky.
2017. iso-timeml and the annota-tion of temporal information.
in handbook of lin-guistic annotation, pages 941–968.
springer..mirella lapata and alex lascarides.
2006. learningsentence-internal temporal relations.
journal of ar-tiﬁcial intelligence research, 27:85–117..ronan le bras, swabha swayamdipta, chandra bhaga-vatula, rowan zellers, matthew peters, ashish sab-harwal, and yejin choi.
2020. adversarial ﬁlters ofdataset biases.
in proc.
of icml, pages 1078–1088.
pmlr..kenton lee, yoav artzi, jesse dodge, and luke zettle-moyer.
2014. context-dependent semantic parsingfor time expressions.
in proc.
of acl..artuur leeuwenberg and marie francine moens.
2018.temporal information extraction by predicting rela-tive time-lines.
in proc.
of emnlp..yanran li, hui su, xiaoyu shen, wenjie li, ziqiangcao, and shuzi niu.
2017. dailydialog: a manuallylabelled multi-turn dialogue dataset.
arxiv preprintarxiv:1710.03957..bill yuchen lin, seyeon lee, rahul khanna, and xi-ang ren.
2020. birds have four legs?!
numersense:probing numerical commonsense knowledge ofpre-trained language models.
in proc.
of emnlp..yinhan liu, myle ott, naman goyal, jingfei du, man-dar joshi, danqi chen, omer levy, mike lewis,luke zettlemoyer, and veselin stoyanov.
2019.roberta: a robustly optimized bert pretraining ap-proach.
arxiv preprint arxiv:1907.11692..todor mihaylov and anette frank.
2018. knowledge-able reader: enhancing cloze-style reading compre-hension with external commonsense knowledge.
inproc.
of acl..nasrin mostafazadeh, nathanael chambers, xiaodonghe, devi parikh, dhruv batra, lucy vanderwende,pushmeet kohli, and james allen.
2016. a cor-pus and cloze evaluation for deeper understandingof commonsense stories.
in proc.
of naacl..qiang ning, zhili feng, hao wu, and dan roth.
2018a.
joint reasoning for temporal and causal relations.
inproc.
of acl..james pustejovsky, robert knippen, jessica littman,and roser saur´ı.
2005. temporal and event informa-tion in natural language text.
language resourcesand evaluation, 39(2):123–164..lianhui qin, antoine bosselut, ari holtzman, chan-dra bhagavatula, elizabeth clark, and yejin choi.
2019a.
counterfactual story reasoning and genera-tion.
in proc.
of emnlp..lianhui qin, michel galley, chris brockett, xiaodongliu, xiang gao, william b dolan, yejin choi, andjianfeng gao.
2019b.
conversing by reading: con-tentful neural conversation with on-demand machinereading.
in proc.
of acl..lianhui qin, vered shwartz, peter west, chandra bha-gavatula, jena d hwang, ronan le bras, antoinebosselut, and yejin choi.
2020. back to the future:backpropagation-based decoding for unsupervisedcounterfactual and abductive reasoning.
in proc.
ofemnlp..gabriel a radvansky and jeffrey m zacks.
2014..event cognition.
oxford university press..colin raffel, noam shazeer, adam roberts, kather-ine lee, sharan narang, michael matena, yanqizhou, wei li, and peter j. liu.
2020. exploringthe limits of transfer learning with a uniﬁed text-to-text transformer.
journal of machine learning re-search, 21(140):1–67..andrea setzer and robert j gaizauskas.
2000. anno-tating events and temporal information in newswiretexts.
in proc.
of lrec..wilson l taylor.
1953..“cloze procedure”: a newtool for measuring readability.
journalism quarterly,30(4):415–433..naushad uzzaman, hector llorens, leon derczyn-ski, james allen, marc verhagen, and james puste-jovsky.
2013. semeval-2013 task 1: tempeval-3:evaluating time expressions, events, and temporalrelations.
in proc.
of semeval..7075siddharth vashishtha, adam poliak, yash kumar lal,benjamin van durme, and aaron steven white.
2020. temporal reasoning in natural language in-ference.
in proc.
of findings of emnlp..ben zhou, daniel khashabi, qiang ning, and danroth.
2019.
“going on a vacation” takes longer than“going for a walk”: a study of temporal common-sense understanding.
in proc.
of emnlp..xikun zhang, deepak ramachandran, ian tenney,yanai elazar, and dan roth.
2020. do language em-in proc.
of findings ofbeddings capture scales?
emnlp..ben zhou, qiang ning, daniel khashabi, and danroth.
2020. temporal common sense acquisitionwith minimal supervision.
in proc.
of acl..7076