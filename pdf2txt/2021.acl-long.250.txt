human-in-the-loop for data collection: a multi-target counternarrative dataset to fight online hate speech.
margherita fanton1,2, helena bonaldi1,2, serra sinem tekiro˘glu2, marco guerini2,1university of trento, italy2fondazione bruno kessler, via sommarive 18, povo, trento, italymfanton@fbk.eu, hbonaldi@fbk.eu, tekiroglu@fbk.eu, guerini@fbk.eu.
abstract.
undermining the impact of hateful contentwith informed and non-aggressive responses,called counter narratives, has emerged as apossible solution for having healthier onlinecommunities.
thus, some nlp studies havestarted addressing the task of counter narra-tive generation.
although such studies havemade an effort to build hate speech / counternarrative (hs/cn) datasets for neural gener-ation, they fall short in reaching either high-quality and/or high-quantity.
in this paper,we propose a novel human-in-the-loop datacollection methodology in which a generativelanguage model is reﬁned iteratively by us-ing its own data from the previous loops togenerate new training samples that experts re-view and/or post-edit.
our experiments com-prised several loops including dynamic vari-ations.
results show that the methodologyis scalable and facilitates diverse, novel, andcost-effective data collection.
to our knowl-edge, the resulting dataset is the only expert-based multi-target hs/cn dataset available tothe community..1.introduction.
the proliferation of online hatred has becamean alarming issue (williams, 2019) threateningnot only the well-being of target individuals andgroups, but also of society as a whole.
whileauthorities establish regulations and policies, so-cial media platforms take actions against hatespeech mostly through moderation activities, suchas content removal, account suspension, or shadow-banning, at the risk of hindering the freedom ofexpression.
meanwhile, non-governmental orga-nizations are qualifying volunteers for respondingto online hate to promote human dignity and under-standing in society.
such responses, i.e., counter-narratives (cn), are non-aggressive textual feed-back using credible evidence, factual arguments,.
alternative viewpoints, and are considered as an ef-fective strategy (benesch, 2014; schieb and preuss,2016) to confront hate speech while respecting thehuman rights (kiritchenko et al., 2020)..however, the vast amount of online hate speechmakes an effective manual intervention impossible,which motivates a line of nlp research focusingon semi or fully automatized cn generation so-lutions1.
in recent years, several cn collectionstrategies and datasets have been proposed address-ing the data-hungry nature of current state of theart generation technologies (mathew et al., 2018;qian et al., 2019; chung et al., 2019)..considering the shortcomings of the existingcollection strategies (that grant either quality orquantity, but not both), we present an approach toproduce high quality cns for multiple hate targetswhile reducing the need for expert intervention.
tothis end, we build on top of the previous hybriddata collection strategies, aiming to increase efﬁ-ciency while maintaining the requirements of dataquality, novelty and diversity.
in particular, we startfrom the work by tekiro˘glu et al.
(2020) that usesan author-reviewer framework in which the author –a generative language model – is tasked with gener-ating hs/cn pairs while a pool of human reviewersﬁlter and possibly post-edit the produced output.
inthe present work we propose to further reduce thedata collection effort by closing the pipeline andfeeding the post-edited output back to the languagemodel in order to regularly update it and improve.
1in our view the generation process can be fully automaticbut generation systems need human supervision and shouldnot be fully autonomous, at least for delicate tasks such ashate countering on social media platforms.
for this reason weadvocate that generation systems should be used as suggestingtool for ngo operators, to make their countering work moreeffective.
in this way there is always a “human moderator”taking the ﬁnal decision (chung et al., 2019).
furthermore,this approach is also in line with de lima salge and berente(2017)’s ethical framework, since this “suggesting tool” con-ﬁguration grants compliance with their rules..proceedingsofthe59thannualmeetingoftheassociationforcomputationallinguisticsandthe11thinternationaljointconferenceonnaturallanguageprocessing,pages3226–3240august1–6,2021.©2021associationforcomputationallinguistics3226the quality of the generated pairs.
our experimentscomprised of two sessions, spanning a period of 6months.
in the ﬁrst session we set up a ‘simple’human-in-the-loop (hitl henceforth) procedureand iterated it several times, measuring at each loopthe performance of the whole framework accordingto relevant metrics.
in the second session we runseveral additional loops in which we test differentstrategies (i.e.
author conﬁgurations) to improvethe data collection according to the given metrics.
findings show that the hitl framework is scal-able, allowing to obtain datasets that are adequatein terms of diversity, novelty, and quantity.
more-over, this framework improves on previous hybriddata collection strategies, reducing at each loop thepost-editing effort of the human reviewers or thenumber of discarded examples (session one).
onthe other hand, with dynamic adaptation, possibleunwanted behaviors or ﬂaws of the data collectioncan be handled at each loop by simply varying theauthor conﬁguration (session 2).
the ﬁnal datasetcontains 5000 hs/cn pairs in english language,covering multiple hate targets, in terms of race,religion, country of origin, sexual orientation, dis-ability, or gender.
to the best of our knowledge,this is the ﬁrst multi-target expert-based hs/cndataset constructed through a semi-automatic mech-anism and can be downloaded at the following link:https://github.com/marcoguerini/conan..2 related work.
with regard to hatred countering, we will focus onthree research aspects relevant for the present work,i.e.
(i) publicly available datasets for detection,(ii) publicly available datasets for countering, (iii)approaches for hybrid data collection..hate detection datasets.
several datasets forhate detection have been presented, most of whichrely on material collected from smps, such as twit-ter (waseem and hovy, 2016; waseem, 2016; rosset al., 2017), facebook (kumar et al., 2018), what-sapp (sprugnoli et al., 2018), and forums (de gib-ert et al., 2018).
while the above datasets focus ona classiﬁcation task, mathew et al.
(2020) releaseda dataset annotated with rationales to improve hatespeech interpretability and sap et al.
(2020) pro-posed the social bias inference corpus (sbic)annotated with the description of the biases implic-itly present in the language.
for a more extensivereview, we refer the reader to poletto et al.
(2020)and vidgen and derczynski (2020)..hate countering datasets.
while several socialstudies proved that counter-narratives are effec-tive in hate countering (benesch, 2014; silvermanet al., 2016; schieb and preuss, 2016; stroud andcox, 2018; mathew et al., 2019), only few workshave focused on data collection for cn genera-tion.
mathew et al.
(2018) focus on crawling,following the intuition that cns can be found onsmps as responses to hateful expressions.
qianet al.
(2019) propose a crowdsourcing methodologywhere crowd-workers (non-expert) are instructedto write responses to hate content collected fromsmps.
the study by chung et al.
(2019) also relieson outsourcing cns writing, but via nichesourcing,using ngo operators expert in cn production..hybrid models for data collection.
given thedata-hungry nature of current nlp technologies,one line of research has recently focused on ad-vanced hybrid models for data collection.
wallaceet al.
(2019) proposed using model interpretation toguide humans in the creation of adversarial exam-ples for factoid question-answering systems.
dinanet al.
(2019) and vidgen et al.
(2020) perform a datacollection with hitl for detecting offensive lan-guage.
in both studies, the dynamic procedure isshown to be successful in reducing model error rateacross rounds.
vidgen et al.
(2020) point out thatthe hitl approach has multiple advantages overthe static data collection: design ﬂaws can be ad-dressed during the construction of the dataset andannotators’ work is optimized, since it is guidedby the feedback from the model.
finally tekiro˘gluet al.
(2020) propose a hybrid approach where anlm is trained on a seed datasets of hs/cn pairsto generate new pairs that are then validated andpost-edited by annotators..3 methodology.
in figure 1 we present the pipeline of our method-ology.
following the idea presented by tekiro˘gluet al.
(2020), we have an author module built usinggpt-2 language model (radford et al., 2019) andﬁne-tuned on a seed dataset of hs/cn pairs.
theauthor produces novel hs/cn candidates whilethe reviewer(s) ﬁlter and eventually post-edit them.
we iterate this data collection several times, at eachloop reviewed examples are added to training dataand the author is ﬁne-tuned from scratch again onall available data.
in the following sections wedescribe the main elements used in our procedures..3227v1...vi−1 as training data and administered thegenerated samples to reviewers until the targetnumber was reached.
in total we iterated the pro-cedure 4 times reaching v5 for a total of 3000 pairs..in the second session, we tested several alternativeauthor conﬁgurations to ameliorate some unwantedbehaviors/trends that emerged during the ﬁrst ses-sion.
we ran 4 additional data collection loops, thistime in parallel (i.e.
all starting from v5 dataset)instead of an iteration.
for each loop, representedas v6,{conf ig name}, we collected 500 hs/cn pairsreaching a total of 5000 examples..3.3 author models.
in our experiments all models are variants of theauthor (gpt-2), obtained by changing the wayit is ﬁne-tuned or conditioned.
for consistency,each model is trained using the same hyperparam-eter conﬁgurations.
in particular, we used gpt-2 medium model, ﬁne-tuned for 3 epochs witha batch size of 1024 tokens and a learning rateof 2e-5.
each pair has been represented as <|startof hs|>hs<|endof hs|> <|startof cn|>cn <|endof cn|> for the training.
at the gen-eration time, nucleus sampling (holtzman et al.,2019) has been utilized with a p value of 0.9.for the standard conﬁgurations we use only <|startof hs|> for conditioning.
given an hs tag,the models produce a chunk of text, which is alist of hs/cn pairs.
these pairs are then cleanedfrom the special tokens and administered to thereviewers for evaluation and possible post-editing..3.4 reviewers.
we recruited 3 annotators, from a pool of internshipstudents, as reviewers over a period of 18 weeksto ﬁlter and post-edit the generated pairs after anextensive training procedure..training.
annotators underwent a training for 2weeks, so that they became “experts” on hs/cnpost-editing.
the training included: (i) reading anddiscussing ngo guidelines and public documenta-tion describing the activity of cn writing for hatecountering, (ii) reading all v1 pairs to better com-prehend the attributes of counter narratives, (iii)reading a sample of 100 hs/cn pairs that havebeen post-edited by an expert to see concrete ex-amples of post-editing activity, (iv) performing apractice session of cn post-editing and discussingit with an expert ngo operator..figure 1: the author-reviewer in the loop conﬁgura-tion.
the author module produces hs/cn candidatesand the reviewer(s) validates and eventually post-editsthem.
at each loop new examples are added to trainingdata and the author is ﬁne-tuned from scratch..3.1 seed dataset.
to start the process, we built a seed dataset of880 hs/cn pairs by nichesourcing its collection to20 experts from two different ngos.
we namedthis dataset v1.
the methodology for collectingv1 closely replicates the one presented by chunget al.
(2019).
in particular we ﬁrst created a list ofprototypical hate texts – with the help of an ngoexpert – for the following hate targets: disabled,jews, overweight, lgbt+, muslim, women,people of color, romani, migrants.
wethen prepared two online data collection forms: inthe ﬁrst, ngo operators were asked to respond toexamples selected from the prototypical hate textlist, in the second they were asked to write theirown hs/cn pairs.
this data collection sessionlasted roughly one month..3.2 sessions.
our experiments were run in two separate andsubsequent sessions, meant to explore differentaspects of the hitl approach..in the ﬁrst session, after using v1 for the ini-tial ﬁne-tuning of gpt-2, we iterated the datacollection 4 times, keeping the author-reviewerconﬁguration as close as possible to the orig-inal one presented by tekiro˘glu et al.
(2020).
loops are numbered sequentially as v2...vn.
at each loop, we acquired 500 examples ofaccepted and eventually post-edited hs/cnpairs2.
to obtain a new set of 500 pairs (vi) weﬁne-tuned gpt-2 every time from scratch using.
2the only exception is v2 that accounts for 620 pairs to.
have a round number of examples by reaching 1500..3228instructions.
we adapted the reviewing instruc-tions from tekiro˘glu et al.
(2020).
in particular, foreach pair, we asked the operators: (a) to approve itwithout any modiﬁcations if it was a valid pair, (b)if the pair was not perfect, but easily amendable, tomodify it, (c) if the cn is completely irrelevant, ordoes not follow ngo’s guidelines, to discard thepair regardless of hs quality, (d) whenever thereare facts or statistics in the cn, check veracity ofthe information to avoid possible lm hallucinationeffects.
we further instructed the annotators to pro-vide a hate target label for each accepted pair.
thelabels were useful both for analysis and for the sub-sequent label-based generation strategies present inv6.
in table 7 we give an example of gpt-2 outputand its post-edited version..hs: transgenders should rape our childrencn: this is not true.
maybe they are worriedbecause of the rise in hate crimes, incidents ofwhich are down to 28 percent, since 2014.hspe: transgenders want to rape our children.
cnpe: this is not true.
maybe you should beworried about the rise in hate crimes againstqueers, incidents of which are almost doubledsince 2014.target: lgbt+.
table 1: an hs/cn example generated by gpt-2 andthe post-edited version with hate target annotation..mitigation procedure.
we applied an adaptedversion of the guidelines by vidgen et al.
(2019)to safeguard the annotators’ well-being against therisk of harmful consequences of working with abu-sive content (present in the hss and possibly ingenerated, not well-formed cns).
to this end weﬁrst made sure that annotators understood the pro-social aspects of the research and explained themthe purpose of their annotation activity in details.
then we instructed the annotators to work no morethan 2/3 hours per day and take regular breaks, byadjusting their workload as needed.
finally, wehad meetings and feedback from the annotators ona weekly basis to let possible problems or distressemerge.
this procedure was repeated throughoutthe whole data collection campaign..4 metrics.
to understand the ‘diachronic’ behavior of ourhitl methodology across iterations, the following.
metrics have been computed at the end of each loopover the newly obtained pairs..imbalance degree measures the difference be-tween a perfectly-balanced distribution of thehate target categories and the actual unbalanceddatasets; we use imbalance degree (id) since itis speciﬁcally devoted to the multi-class scenario(ortigosa-hern´andez et al., 2017).
datasets thatare balanced over multiple hate targets could al-low building more representative cn generationmodels..acceptance rateis the percentage of pairs ac-cepted by the reviewers (either untouched or post-edited) over the total number they scrutinised.
itrepresents an overall estimate of the ability of theframework to produce reasonable-quality material..hter is originally a measure of post-editingeffort at sentence level translations (specia andfarzindar, 2010).
we adopted it to the measurereviewers’ effort in terms of the average numberof edits over the accepted pairs.
an upper-boundthreshold value of 0.4 is used to account for easilypost-editable pairs (turchi et al., 2013)..novelty measures how different two collectionsof texts are from each other, and it is grounded onjaccard similarity.
we utilized it to compute theoriginality present in vi with respect to the trainingdata collected in previous loops (dziri et al., 2019;wang and wan, 2018)..repetition rate measures the intra-corporaquality in terms of language diversity by consider-ing the rate of non-singleton ngram types it con-tains (cettolo et al., 2014; bertoldi et al., 2013).
we use it to measure the ability of the frameworkto provide diverse and varied examples.
repetitionrate (rr) has the advantage of being independentfrom corpus size, so it can be used to directly com-pare different versions of our dataset..vocabulary expansion is a measure we intro-duce to serve two main objectives: (i) quantifyingthe contribution of the author and the reviewers, byfocusing on new tokens appeared at each loop (e.g.
the term “peace” was introduced for the ﬁrst timeby annotators in v2), (ii) quantifying the presenceof cross-fertilization, i.e.
tokens that appear for theﬁrst time in version vn for a particular target, butthey were present in a version antecedent to vn forthe other targets (e.g.
the term “peace” for the tar-get jews appears at v4 but it was already present.
3229for the target muslim in v2).
the algorithm forcomputing vocabulary expansion is described inappendix a.1..5 session one.
in session one, all the versions of the datasetv2...v5 are generated using gpt-2vi, where theﬁne-tuning is performed on all previous versionsof the dataset v1...vi−1 as explained earlier..to produce hs/cn pairs, the author condition-ing is performed using only <|startofhs|> tagand collecting all the generated material providedthat each pair is encapsulated with the proper tags.
for the analysis, we computed the metrics de-scribed in section 4 on the hs/cn pairs obtained ineach loop using micro-averaging (in appendix a.4,table 5 we report all results in detail).
to isolatethe possible effect of target-class imbalance, macroaverages were also calculated; similarly, to accountfor element-wise differences we calculated microaverages for hs and cn sets separately3..discussion.
considering our objective of collect-ing quality material in an efﬁcient way, we ﬁrstfocus on the ratio of accepted pairs and the post-editing effort in each loop.
as shown in figure 2,the percentage of accepted pairs tends to increaseacross the loops, for both the pairs that are post-edited (“modiﬁed”) from 35.8 in v2 to 50.1 in v5and the ones accepted without post-editing (“un-touched”) from 1.5 in v2 to 10.9 in v5..figure 2: on the left: percentage of pairs accepted (i)modiﬁed and (ii) untouched.
on the right: id calcu-lated over the 7 main target classes..at the same time, the average post-editing effortof the reviewers tend to decrease across the ver-sions, as depicted in figure 3. to ensure that thedecrease in hter is not due to the increasing ratioof untouched pairs to the total number of accepted.
3these results are in line with the ones showed in thepaper, and do not change the discussion.
they are reported inappendix a.4, table 6.figure 3: on the left: evolution of the post-editing ef-fort in terms of hter across loops both for all pairsand modiﬁed only.
on the right: micro average of rep-etition rate (rr) across loops for the hs+cn pairs..pairs, we computed the hter for the modiﬁedpairs alone.
consistently with the overall trend,hter for modiﬁed pairs also declines, indicatingthat the data collection loops succeeded not only inreducing the reviewer effort, but also in improvingthe quality of the generated material to be post-edited.
notably, after v3 the hter falls below the0.4 acceptability threshold as deﬁned in (turchiet al., 2013) for the amt scenario (figure 3).
inview of this analysis, we can conclude that the efﬁ-ciency of data collection is increased by hitl ascompared to a static approach that does not retrainthe author module (that can be represented by v2).
regarding the evaluations with the quality met-ric repetition rate (figure 3), it increases fromv2 on signifying a decrease in the lexical diversityof the generated data.
moreover, we observed aconsistent trend for the scores of the second qualitymetric, i.e.
novelty (figure 4).
similar to the di-versity, novelty of the collected data also decreasesacross the versions, regardless of the dataset againstwhich the novelty is computed.
particularly, thechange in the cumulative novelty represents howthe vocabulary becomes less and less enrichableas the loop number increases, indicating a possi-ble saturation point where novel material is highlydifﬁcult to obtain.
finally, the distribution of hatetargets shows a worsening also in terms of id thatincreases from a score of 2.2 in v1 to 4.5 in v5 (seefigure 2) with some targets becoming predominantwhile others slowly disappearing.
more detailson each target distribution per loop are given inappendix a.2, figure 11..as for pair length, throughout the loops wefound that “untouched” pairs are usually shorter(30.7 tokens on average) than the other acceptedpairs (37.3 tokens on average before post-editing).
during the discussion sessions, annotators reported.
3230that the “untouched” pairs are not only shorter butalso somewhat stereotypical, with a small noveltyadded to the overall dataset (e.g.
“you cannot saythis about an entire religion”, “it’s unfair to saythis about an entire religion”)..figure 4: novelty: (i) vi with respect to v1 seed dataset,(ii) vi with respect to the previous version vi−1.
(iii)cumulative novelty, i.e.
vi vs. v1...vi−1..6 session two.
given the problems emerged during the loops ofthe ﬁrst session (i.e.
higher efﬁciency but lowerquality at each loop), we organized an additionalsession to test several parallel methodologiesto ameliorate them.
the description of the v6conﬁgurations are as follows:.
v6,sbf : the model gpt-2v5 is conditioned withnovel offensive speeches extracted from sbic cor-pus (sap et al., 2020).
we chose this resourcesince: (i) it contains several thousand of social me-dia posts containing biases and stereotypes span-ning the same target categories with our study, (ii)for each post it provides an ‘implied statement’that closely resembles a ‘prototypical hate speech’on which we trained our system.
we sampled thesame number of ‘implied statements’ for each tar-get that maps to our labels4 among the ones an-notated with ‘the intent behind the statement wasto offend’ and/or ’the post could be offensive tosomeone’.
we provide the statements as conditionsby appending them to <|startof hs|>.
v6,lab : the model is conditioned specifyingon which hate target it should focus on.
in thisconﬁguration, we trained a variant of gpt-2v5that takes into account the target label, and mod-iﬁed the original representation of our trainingdata accordingly.
in particular we accommodatehate target information within the starting token:<|startof hs: target label|>..4in table 4 in appendix we provide the mapping we used..v6,arg : we ﬁne-tuned gpt-2 on a dataset ofargumentative pairs collected from kialo5, an on-line debate platform for constructive and rationaldiscussions among peers that has been exploitedrecently by the nlp community (durmus et al.,2019a,b; scialom et al., 2020).
each discussion inkialo is represented as a tree of arguments in whicha child node is connected to its parent via a “pro”or “con” relation.
extracting all the claims con-nected by a “con” relation, we obtained a dataset of128178 argument pairs covering a broader domainas compared to hs/cn pairs.
we then ﬁne-tunedgpt-2 for 1 epoch over the argumentation datasetwith the standard hyperparameters.
preliminaryexperiments showed that the best strategy was torepresent these pairs with the same format as oursto facilitate transfer of task characteristics and argu-mentative knowledge.
then this model was againﬁne-tuned using the standard v1...v5 data.
at infer-ence time, conditioning has been performed usinglists of unique hss from the v1...v5 data.
v6,m ix : the last model is obtained by blendingthe three previous versions together, i.e.
ﬁrst ﬁne-tuning on kialo dataset, second ﬁne-tuning usingtarget label notation on v1...v5 data, conditioningusing sbic offensive speeches..bearing in mind the problems emerged duringsession one, our ﬁrst goal in session two wasto balance the dataset with respect to the hatereducing id score).
to this endtargets (i.e.
the conditioning always takes into account thehate target label (with respect to 7 targets: jews,lgbt+, muslim, women, disabled,peopleof color, migrants) either explicitly as inv6,lab or v6,m ix , or implicitly as in v6,sbf andv6,arg.
in addition, to better balance the numberof pairs for each target, we administered only theﬁrst 5 pairs of each generated chunk to the review-ers..discussion.
all the applied methodologies allowfor a better balancing of data in terms of hate tar-gets, yielding an average id score of 2.3 for thev6 conﬁgurations in comparison to the id score of6. as shown in figure 5 - left, all v6 con-4.5 for v5ﬁgurations have a slightly higher acceptance rate7. thus introducing novel material or datathan v5.
5www.kialo.com6in appendix, table 3, we provide the target distribution.
over the ﬁnal dataset..7in order to estimate the trend of each metric after v5, wecalculated also v6,p redict ed, shown as a dashed line in.
3231representation in ﬁne-tuning stages has no strongperturbation effect.
second, and more interestingly,we observe a signiﬁcant variation in the ratio ofuntouched and modiﬁed pairs to all the reviewedsamples: for all v6 approaches while there is astrong decrease in ratio of untouched pairs (fig-ure 5, right), there is a signiﬁcant increase in thosemodiﬁed (see figure 5, left).
in other words thesemodels were able to produce a higher amount ofsuitable, albeit non perfect, pairs.
in particular,comparing v6 conﬁgurations we can observe thatfor the untouched pairs the highest acceptance rateis achieved via v6,arg with 6.37% accepted pairs,whereas for the modiﬁed pairs v6,m ix yields thehighest percentage, with 66.15% of the pairs ac-cepted..concerning the reviewer’s effort, we see that theoverall hter increases for the all v6 approaches(figure 6, left).
considering that we had a lowernumber of untouched and a higher number of mod-iﬁed pairs this was expected, and if we turn to thehter of modiﬁed pairs alone we see that thereis a smaller difference between v5 and v6 hter.
even more interestingly, the hter scores of allv6 conﬁgurations, even if higher than v5, are stillbelow the acceptability threshold value of 0.4 de-ﬁned earlier.
going into details, amongst the v6conﬁgurations, hter reaches its lowest value inv6,arg, for both the modiﬁed and untouched pairs:since it was conditioned using gold hs material,this result is expected.
as opposed to the othermodels, v6,lab is conditioned only with a labelrepresentation and not with actual hss.
this af-fected negatively the post-editing effort, as we cannotice a higher hter for this conﬁguration.
more-over, v6,lab has a smaller amount of untouchedpairs, so we expected hter to spike up..figure 6: v6 conﬁgurations hter, for all pairs on theleft, modiﬁed pairs on the right..elty both with respect to v5 and expected v6 (thedashed line) , except for v6,arg, possibly due to itsconditioning with hss from v1 ... v5.
therefore,we also computed the novelty for cn set alone todiscard the effect of hs on the metric.
in this set-ting, all v6 conﬁgurations reach a novelty between0.741 and 0.745, as compared to a cn novelty in v5of 0.737 (as in appendix a.3).
the effect of goldhs conditioning in v6,arg can also be spotted inthe lowest hter results in figure 6. the highestincrease in novelty is recorded for v6,m ix , reach-ing a score of 0.76; also novelty scores computedwith respect to v5 and v1 conﬁrm the result..all v6 conﬁgurations succeeded in reaching anrr lower than both v5 and expected v6 (the dashedline).
it is interesting that v6,lab has the highestrr among the v6 conﬁgurations, possibly becauseit was not built using any external knowledge,but only with a different label representation.
onthe other hand, v6,arg conﬁguration, for whichan initial argumentation ﬁne-tuning has beenperformed, has the lowest rr (5.474)..figure 7: v6 conﬁgurations.
cumulative novelty (onthe left), repetition rate (on the right)..figure 5: acceptance rate for v6 conﬁgurations: modi-ﬁed pairs on the left, untouched pairs on the right..with regard to data quality (see figure 7), we seethat all v6 strategies succeed in increasing the nov-the plots, using a linear regression model over v1...v5..from this analysis we can conclude that v6 con-ﬁgurations are better at producing sub-optimal ma-terial but worse at producing perfect material.
stillthe general quality of the pairs (in terms of nov-elty and rr) in session two is much higher than.
3232before, exhibiting the desired behavior for whichthese strategies were introduced..figure 8: vocabulary expansion throughout loops (per-centage of words) ..7 vocabulary analysis..we report vocabulary expansion ﬁndings in fig-ure 8. for each loop v2...v5 the average percent-age of new words injected into the dataset by theauthor model (gpt-2) is higher than the averagepercentage of new words inserted by the three re-viewers during post-editing.
both trend-lines, evenif slightly decreasing are not converging, implyingthat ﬁne-tuned gpt-2 is not reaching a “saturationpoint” and is continuously adding new material.
this trend is in line with the decrease in novelty.
on the other hand, instructions asked for a minimalpost-edit, so the reviewers have less opportunity toinject new material than the author and the decreaseis consistent with the decreasing hter..as for the percentage of words generated bythe author model pertaining to the same target, wesee an increasing trend throughout the generationsdue to the cumulative nature of the metric.
still,the presence of words ﬁrst observed in other tar-gets throughout the loops, shows that the cross-fertilization phenomenon generated by gpt-2 per-sists.
this desired feature shows the ability of gpt-2 to learn the ‘domain’ vocabulary of other targetsand to use it effectively for the target under analysis,in a way that is approved by the humans - review-ers8.
finally, we can remark that v6 versions areable to increase both the injection of novel wordsfrom gpt-2 and also its cross-fertilization ability..8 qualitative analysis.
during our exploratory experiments and thediscussion sessions with the annotators, several.
8even though we opted for a distinction in terms of source(same target, other target) for the ‘not-new’ words generatedby the author module, we chose not to have the same dis-tinction for the reviewers’ ‘not-new’ words, since we cannotassess if the reviewer was (or not) aware of the presence of aword in previous versions of the dataset..hs: “we should kill all the jews.”cn: “there are many alternatives to removingjews, such as converting them to another religion(e.g.
buddhism).”hs: “all homosexuals are trying to underminethe family by trying to impose their sexual pref-erences.”cn: “thus far, people of various sexual orien-tations have attempted suicide twice as often asheterosexuals.
among youth, those that seriouslycontemplate suicide are at almost three times therate of heterosexual.
lgb youth are almost ﬁvetimes as likely to have attempted suicide com-pared to heterosexual youth.
tell me honestly:who do you think is imposing sexual orientationto whom?”hs: “i don’t like muggleborns, they are likely todo violence.”cn: “we do not say that muggleborns are lesslikely to commit crimes.
we are saying that theyare almost certainly not the case.
”.
table 2: hs/cn examples generated by gpt-2..interesting subjects have emerged, which caninitiate future work..argumentation and counter narratives.
in or-der to obtain even more novelty in produced pairs,v6,arg model could be used without ﬁne-tuningon the hs/cn dataset under the assumption that acounter argument is the same as a counter narra-tive.
still, the ability to argument on a variety oftopics is not enough to provide a meaningful cnwhen prompted with an hs.
a cn also presupposevalues, so - for example - a logically valid argu-ment is not necessarily an acceptable cn, as theﬁrst example in table 2 shows (produced by gpt-2ﬁne-tuned only on kialo arguments)..new arguments or new paraphrases.
onequestion that emerged is whether gpt-2 is able toproduce novel arguments or it is just a very sophis-ticated paraphrasing tool.
during the discussionsessions with annotators and also by manual anal-ysis, we could ﬁnd cns that contained genuinelynovel arguments, which were not present in thetraining data but produced by gpt-2.
in the secondexample in table 2, the novel argument is aboutcapsizing the “imposing the homosexual agenda”argument by providing data on “suicidal attemptsamong homosexual youth”..3233novel hate targets and general knowledge.
gpt-2 proved to be able to generate hs/cn pairsalso for unseen targets, including intersectionalones (e.g.
“black women”).
still the lack of a“commonsense knowledge” can produce funny re-sults that are beyond the scope of hallucination(zellers et al., 2019; solaiman et al., 2019), suchas the third example in table 2, where gpt-2 ad-dresses muggleborns (target of hate in harry potterbooks)..9 conclusions.
in this paper we presented a novel hitl methodol-ogy for data collection based on an author-reviewerframework.
this methodology puts together anlm and a set of human reviewers, where the lm isreﬁned iteratively, using data from previous loopsthat have been validated by experts.
experimentsshow that as loops are iterated, efﬁciency in datacollection increases (acceptance rate and htermetrics) while the dataset quality decreases interms of novelty and diversity metrics.
for thisreason we experimented with additional dynamicloop adaptation that are able to increase the overallquality of the dataset without hindering the efﬁ-ciency signiﬁcantly..acknowledgments.
this work was partly supported by the hateme-ter project within the eu rights, equality andcitizenship programme 2014-2020. we are deeplygrateful to stop hate uk and its volunteers fortheir help and effort in preparing the seed dataset(version v1) necessary for this work..references.
susan benesch.
2014. countering dangerous speech:new ideas for genocide prevention.
washington,dc: united states holocaust memorial museum..nicola bertoldi, mauro cettolo, and marcello federico.
2013. cache-based online adaptation for machinetranslation enhanced computer assisted translation.
in mt-summit, pages 35–42..mauro cettolo, nicola bertoldi, and marcello federico.
2014. the repetition rate of text as a predictor of theeffectiveness of machine translation adaptation.
inproceedings of the 11th biennial conference of theassociation for machine translation in the americas(amta 2014), pages 166–179..yi-ling chung, elizaveta kuzmenko, serra sinemtekiro˘glu, and marco guerini.
2019. conan -.
counter narratives through nichesourcing: a mul-tilingual dataset of responses to ﬁght online hatein proceedings of the 57th annual meet-speech.
ing of the association for computational linguis-tics, pages 2819–2829, florence, italy.
associationfor computational linguistics..emily dinan, samuel humeau, bharath chintagunta,and jason weston.
2019. build it break it ﬁx it fordialogue safety: robustness from adversarial humanin proceedings of the 2019 conference onattack.
empirical methods in natural language processingand the 9th international joint conference on natu-ral language processing (emnlp-ijcnlp), pages4529–4538..esin durmus, faisal ladhak, and claire cardie.
2019a.
determining relative argument speciﬁcity and stancein proceed-for complex argumentative structures.
ings of the 57th annual meeting of the associationfor computational linguistics, pages 4630–4641..esin durmus, faisal ladhak, and claire cardie.
2019b.
the role of pragmatic and discourse context in de-in proceedings of thetermining argument impact.
2019 conference on empirical methods in natu-ral language processing and the 9th internationaljoint conference on natural language processing(emnlp-ijcnlp), pages 5672–5682..nouha dziri, ehsan kamalloo, kory mathewson, andosmar r zaiane.
2019. augmenting neural re-sponse generation with context-aware topical atten-tion.
in proceedings of the first workshop on nlpfor conversational ai, pages 18–31..ona de gibert, naiara perez, aitor garcıa-pablos, andmontse cuadros.
2018. hate speech dataset from awhite supremacy forum.
emnlp 2018, page 11..ari holtzman, jan buys, maxwell forbes, and yejinchoi.
2019. the curious case of neural text degener-ation.
corr, abs/1904.09751..svetlana kiritchenko, isar nejadgholi, and kathleen c.fraser.
2020. confronting abusive language online:a survey from the ethical and human rights perspec-tive.
corr, abs/2012.12305..ritesh kumar, atul kr ojha, shervin malmasi, andmarcos zampieri.
2018. benchmarking aggressionidentiﬁcation in social media.
in proceedings of thefirst workshop on trolling, aggression and cyber-bullying (trac-2018), pages 1–11..carolina alves de lima salge and nicholas berente.
2017. is that social bot behaving unethically?
com-munications of the acm, 60(9):29–31..binny mathew, navish kumar, ravina, pawan goyal,and animesh mukherjee.
2018. analyzing the hateand counter speech accounts on twitter.
corr,abs/1812.02712..3234binny mathew, punyajoy saha, hardik tharad, sub-ham rajgaria, prajwal singhania, suman kalyanmaity, pawan goyal, and animesh mukherjee.
2019.thou shalt not hate: countering online hate speech.
in proceedings of the international aaai confer-ence on web and social media, volume 13, pages369–380..binny mathew, punyajoy saha, seid muhie yi-mam, chris biemann, pawan goyal, and ani-mesh mukherjee.
2020. hatexplain: a bench-mark dataset for explainable hate speech detec-tion.
arxiv:2012.10289 [cs].
arxiv: 2012.10289..jonathan ortigosa-hern´andez, i˜naki inza, and jose alozano.
2017. measuring the class-imbalance ex-tent of multi-class problems.
pattern recognitionletters, 98:32–38..fabio poletto, valerio basile, manuela sanguinetti,cristina bosco, and viviana patti.
2020. resourcesand benchmark corpora for hate speech detection: asystematic review.
language resources and evalu-ation..jing qian, anna bethke, yinyin liu, elizabeth beld-ing, and william yang wang.
2019. a bench-mark dataset for learning to intervene in online hatespeech.
in proceedings of the 2019 conference onempirical methods in natural language processingand the 9th international joint conference on natu-ral language processing (emnlp-ijcnlp), pages4757–4766, hong kong, china.
association forcomputational linguistics..alec radford, jeffrey wu, rewon child, david luan,dario amodei, and ilya sutskever.
2019. languagemodels are unsupervised multitask learners.
openaiblog, 1(8)..bj¨orn ross, michael rist, guillermo carbonell, ben-jamin cabrera, nils kurowsky, and michael wo-jatzki.
2017. measuring the reliability of hatespeech annotations: the case of the europeanrefugee crisis.
corr, abs/1701.08118..maarten sap, saadia gabriel, lianhui qin, dan ju-rafsky, noah a. smith, and yejin choi.
2020. so-cial bias frames: reasoning about social and powerin proceedings of theimplications of language.
58th annual meeting of the association for compu-tational linguistics, pages 5477–5490, online.
as-sociation for computational linguistics..carla schieb and mike preuss.
2016. governing hatespeech by means of counterspeech on facebook.
in66th ica annual conference, at fukuoka, japan, pages1–23..thomas scialom, serra sinem tekiro˘glu, jacopo sta-iano, and marco guerini.
2020. toward stance-in pro-based personas for opinionated dialogues.
ceedings of the 2020 conference on empirical meth-ods in natural language processing: findings,pages 2625–2635..tanya silverman, christopher j stewart, jonathanbirdwell, and zahed amanullah.
2016. the im-institute for strate-pact of counter-narratives.
gic dialogue, london.
https://www.
strategicdi-alogue.
org/wp-content/uploads/2016/08/impact-of-counter-narratives online.
pdf–73..irene solaiman, miles brundage, jack clark, amandaaskell, ariel herbert-voss, jeff wu, alec radford,and jasmine wang.
2019. release strategies andthe social impacts of language models.
corr,abs/1908.09203..lucia specia and atefeh farzindar.
2010. estimatingmachine translation post-editing effort with hter.
inproceedings of the second joint em+/cngl work-shop bringing mt to the user: research on integrat-ing mt in the translation industry (jec 10), pages33–41..rachele sprugnoli, stefano menini, sara tonelli, fil-ippo oncini, and enrico piras.
2018. creating awhatsapp dataset to study pre-teen cyberbullying.
inproceedings of the 2nd workshop on abusive lan-guage online (alw2), pages 51–59..scott r stroud and william cox.
2018. the varietiesof feminist counterspeech in the misogynistic on-line world.
in mediating misogyny, pages 293–310.
springer..serra sinem tekiro˘glu, yi-ling chung, and marcoguerini.
2020.generating counter narrativesagainst online hate speech: data and strategies.
inproceedings of the 58th annual meeting of the asso-ciation for computational linguistics, pages 1177–1190, online.
association for computational lin-guistics..marco turchi, matteo negri, and marcello federico.
2013. coping with the subjectivity of human judge-in proceedings ofments in mt quality estimation.
the eighth workshop on statistical machine trans-lation, pages 240–251..bertie vidgen and leon derczynski.
2020. direc-tions in abusive language training data, a system-atic review: garbage in, garbage out.
plos one,15(12):e0243300..bertie vidgen, alex harris, dong nguyen, rebekahtromble, scott hale, and helen margetts.
2019.challenges and frontiers in abusive content detec-tion.
in proceedings of the third workshop on abu-sive language online, pages 80–93, florence, italy.
association for computational linguistics..bertie vidgen, tristan thrush, zeerak waseem, anddouwe kiela.
2020. learning from the worst: dy-namically generated datasets to improve online hatedetection.
corr, abs/2012.15761..eric wallace, pedro rodriguez, shi feng, ikuya ya-mada, and jordan boyd-graber.
2019. trick meif you can: human-in-the-loop generation of adver-sarial question answering examples.
transactions.
3235of the association for computational linguistics,7(0):387–401..ke wang and xiaojun wan.
2018. sentigan: gener-ating sentimental texts via mixture adversarial net-works.
in ijcai, pages 4446–4452..zeerak waseem.
2016. are you a racist or am i seeingthings?
annotator inﬂuence on hate speech detectionon twitter.
in proceedings of the ﬁrst workshop onnlp and computational social science, pages 138–142..zeerak waseem and dirk hovy.
2016. hateful sym-bols or hateful people?
predictive features for hatein proceedings of thespeech detection on twitter.
naacl student research workshop, pages 88–93..matthew williams.
2019. hatred behind the screens:.
a report on the rise of online hate speech..rowan zellers, ari holtzman, hannah rashkin,yonatan bisk, ali farhadi, franziska roesner, andyejin choi.
2019. defending against neural fakenews.
in advances in neural information process-ing systems, pages 9051–9062..3236a appendix.
a.1 vocabulary expansion algorithm.
the pseudo-code for the vocabulary expansion met-ric described in section 4 can be found in algo-rithm 1. for each version and target, we deﬁne twofollowing sets of words:.
v ocabpe: words from the post-edited pairs.
v ocabgen: words from the generated pairs.
a word is considered novel when it is not present inthe collective vocabulary of the previous versions:v ocab(v1,...,i−1)..algorithm 1: vocabulary expansion foreach targetfor each version vi do.
for each word w in vi do.
if w in v ocabpe and w in v ocabgenthenauthor w ←wif author w in v ocab(v1,...,i−1) thenif author w in same target v ocabthensame target author w ←author w.else.
else.
else.
other target author w ←author w.novel author w ←author w.reviewer w ←wif reviewer w in v ocab(v1,...,i−1)thennot novel reviewer w ←reviewer w.else.
novel reviewer w ←reviewer w.figure 9: session one.
hter scores on the left.
rron the rigth..results for hs alone are less consistent yieldinghigher scores for v3 and v4.
this can be mostlyexplained with the different approaches of post-editing the hss by the annotators, which includethe possibility to rewrite it entirely when needed.
on the other hand, the decreasing trend of hterfor hs starting from v3, resulting in a lower scorein v5 than the one calculated on cn only, couldbe due to the increasing frequency of prototypicalhss.
this implication is conﬁrmed by the higherrr scores for hss as compared to cns, whichgrow faster for the former than the latter (figure 9on the right).
moreover, the increasing number ofprototypical hss contributes to the novelty scoresfor hss only being lower than those of cns anddecreasing more rapidly (figure 10)..each word is assigned to one of the followingsets: author-novel, author-same-target, author-other-target, reviewer-novel, reviewer-not-novel.
considering the size in terms of words of each set,we calculate the percentages for each target andversion, so that we are able to obtain the vocabularyexpansion scores as macro average percentages..a.2 additional material for session one.
in this section, we present the most interesting re-sults that we have obtained by analysing only thehs or the cn sets..while hter calculated on cn alone shows aclear decreasing trend (figure 9 on the left), the.
figure 10: session one.
novelty scores (hs on the left,cn on the right)..in figure 11 the target distribution at each loopof session one is shown, in table 3 the frequenciesof targets in the ﬁnal dataset are displayed.
themuslims target covers a signiﬁcant percentage ofthe generations in every loop and consists of morethan the half of the pairs v5.
in fact it is expectedto cause even more imbalanced productions in thenext loops.
jews, migrants and disabledtargets diminish over the loops, while the othertargets can be considered as stable..3237v6,sbfdisabled.
jews.
lgbt+.
migrants.
muslim.
poc.
women*overweight*romani.
lesbian women,.
labels from sap et al.
(2020)mentally disabled folks, physicallydisabled folks, autistic folks, blindpeople, folks with down syndrome,autisticjewish folks, jews, holocaust, holo-caust victimsgay men,transwomen, trans men, nonbinary folks,gay folks, bisexual women, transpeopleimmigrants,refugeesmuslim folks, islamic folks, mus-lims, islamicblack folks, africans, africa, peopleof color, african folks african, pocwomen, feminists, feministfat folksgypsies.
immigrants,.
illegal.
table 4: label mapping for v6,sbf .
starred items areconsidered as “other targets” in figure 11..figure 12: hter for hs and cn, computed on allpairs..figure 13: cumulative novelty, i.e.
vi vs..vx for.
i−1(cid:83)x=1.
hs and cn, computed on all pairs.
..figure 14: repetiton rate for hs and cn, computedon all pairs..figure 11: the targets distributions for the loops of ses-sion one..targetdisabledjewslgbt+migrantsmuslimspocwomenothertotal.
coverage4.4011.8712.3319.1326.687.0413.235.32100.pairs22059461795713353526622665003.table 3: target distribution over the ﬁnal dataset..a.3 additional material for session two.
concerning session two, the results for cns arein line with the conclusions drawn in the paperfor hs/cn pairs.
the same holds for hss, theonly exception being for the cumulative noveltyof v6,arg hss, as can be seen in figure 13 andin table 6. as explained earlier in section 6, thiseffect is due to the use of hate speeches from thetraining set for conditioning gpt-2.
this resultalso corresponds to hss from v6,arg having lowerhter (figure 12) and a higher rr (figure 14)..a.4 tables.
in table 5, the main results calculated on thein table 6, respec-hs/cn pairs are displayed.
tively, the results calculated on hs only and cnonly are shown..3238versionsimbalance degreeacceptance rate (untouched)acceptance rate (modiﬁed)discarded pairs ratehter (all pairs)hter (modiﬁed)vi vs. cumulative noveltyvi vs. v1 noveltyvi vs. vi−1 noveltyrrvocab.
gpt-2: newvocab.
gpt-2: same targetvocab.
gpt-2: other targetsvocab.
human: newvocab.
human: not new.
v23.2221.47535.82062.7050.4440.4620.8180.8180.8183.75318.89727.99727.31610.37315.417.v33.2141.94134.00464.0550.4060.4290.7920.8120.8004.9999.06042.01724.1435.61019.170.v43.3195.68447.05347.2630.3470.3890.7660.8060.7775.8767.25649.70321.0395.23016.773.v5 v6,sbf2.1435.14653.09941.7550.3340.3670.7550.8120.7775.4916.92456.41919.3533.04014.264.
4.48510.93650.06139.0030.2710.3300.7380.7990.7567.9625.30357.13716.1643.34318.053.v6,arg v6,lab3.0983.30860.30536.3870.3660.3860.7520.8090.7705.9935.11158.65316.6072.84416.784.
2.0956.36756.05537.5780.3130.3490.7280.7950.7755.4745.40762.55014.5743.22714.241.v6,m ix2.0574.21366.15229.6350.3500.3720.7600.8130.7815.5856.85958.10417.3463.44014.251.table 5: all results for hs/cn pairs..versionshs metricshter (all pairs)rrvi vs. cumulative noveltyvi vs. v1 noveltyvi vs. vi−1 noveltycn metricshter (all pairs)rrvi vs. cumulative noveltyvi vs. v1 noveltyvi vs. vi−1 novelty.
v2.
v3.
v4.
v5 v6,sbf.
v6,arg v6,lab.
v6,m ix.
0.3996.5080.7570.7570.757.
0.5272.4430.8140.8140.814.
0.4979.4960.6970.7610.713.
0.3833.6920.7880.8060.798.
0.4059.1010.6240.7310.662.
0.3514.2360.7600.8000.779.
0.18915.5760.4630.6780.522.
0.3125.6900.7370.7950.758.
0.4249.0620.6180.7600.673.
0.3294.4280.7430.8050.771.
0.30110.4790.4360.6890.652.
0.3424.1250.7430.8010.771.
0.38510.7000.5820.7430.645.
0.3824.5210.7410.8010.768.
0.44411.3610.6060.7580.666.
0.3414.4280.7450.8020.774.table 6: all results for hs and cn sets alone..3239hs: every muslim is a potential terrorist, even if they are not born here!
cn: a person’s place of birth is important.
some people prefer to identify as non-muslim, some asmuslim, and still others as christian.
i prefer to call them what they are: people.
hspe: every muslim is a potential terrorist, even if they are born here!
cnpe: a person’s place of birth is not relevant, but neither is a person’s religion.
some people aremuslims.
i prefer to call them what they are: people, not potential terrorists.
target: muslimshs: men are more smart than womencn: it is about time women are given a chance to prove that they are intelligent as well.
hspe: men are smarter than women, who should be excluded from education.
cnpe: this is not true: it is about time women are given a chance to prove that they are intelligentas well... and it is sad that they still need to prove it.
target: womenhs: gays are just a bunch of disgusting animals trying to ruin other animals’ lives.
cn: being gay is not a problem, it is about orientation.
hspe: gays are just a bunch of disgusting animals trying to ruin other people’s lives.
cnpe: being gay is not a problem, being homophobic is.
target: lgbt+hs: jews are responsible for the 9/11 attacks.
cn: i don’t think this idea is true: since the 9/11 attacks were committed by muslims, it doesn’tseem fair to blame jews.
hspe: jews are responsible for the 9/11 attacks.
cnpe: i don’t think this idea is true: since the 9/11 attacks were committed by al qaeda extremists itdoesn’t seem fair to blame jews.
target: jews.
table 7: examples of hs/cn pairs before and after post-editing with assigned target labels..3240