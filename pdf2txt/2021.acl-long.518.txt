learning dense representations of phrases at scale.
jinhyuk lee1,2∗ mujeen sung1.
jaewoo kang1 danqi chen2.
korea university1 princeton university2{jinhyuk_lee,mujeensung,kangj}@korea.ac.krdanqic@cs.princeton.edu.
abstract.
open-domain question answering can be refor-mulated as a phrase retrieval problem, withoutthe need for processing documents on-demandduring inference (seo et al., 2019).
however,current phrase retrieval models heavily dependon sparse representations and still underper-form retriever-reader approaches.
in this work,we show for the ﬁrst time that we can learndense representations of phrases alone thatachieve much stronger performance in open-domain qa.
we present an effective methodto learn phrase representations from the super-vision of reading comprehension tasks, cou-pled with novel negative sampling methods.
we also propose a query-side ﬁne-tuning strat-egy, which can support transfer learning andreduce the discrepancy between training andinference.
on ﬁve popular open-domain qadatasets, our model densephrases improvesover previous phrase retrieval models by 15%–25% absolute accuracy and matches the perfor-mance of state-of-the-art retriever-reader mod-els.
our model is easy to parallelize due topure dense representations and processes morethan 10 questions per second on cpus.
finally,we directly use our pre-indexed dense phraserepresentations for two slot ﬁlling tasks, show-ing the promise of utilizing densephrases as adense knowledge base for downstream tasks.1.
1.introduction.
open-domain question answering (qa) aims toprovide answers to natural-language questions us-ing a large text corpus (voorhees et al., 1999; fer-rucci et al., 2010; chen and yih, 2020).
while adominating approach is a two-stage retriever-readerapproach (chen et al., 2017; lee et al., 2019; guuet al., 2020; karpukhin et al., 2020), we focus on.
a recent new paradigm solely based on phrase re-trieval (seo et al., 2019; lee et al., 2020).
phraseretrieval highlights the use of phrase representa-tions and ﬁnds answers purely based on the similar-ity search in the vector space of phrases.2 withoutrelying on an expensive reader model for process-ing text passages, it has demonstrated great runtimeefﬁciency at inference time..despite great promise, it remains a formidablechallenge to build vector representations for ev-ery single phrase in a large corpus.
since phraserepresentations are decomposed from question rep-resentations, they are inherently less expressivethan cross-attention models (devlin et al., 2019).
moreover, the approach requires retrieving answerscorrectly out of billions of phrases (e.g., 6 × 1010phrases in english wikipedia), making the scale ofthe learning problem difﬁcult.
consequently, ex-isting approaches heavily rely on sparse represen-tations for locating relevant documents and para-graphs while still falling behind retriever-readermodels (seo et al., 2019; lee et al., 2020)..in this work, we investigate whether we can buildfully dense phrase representations at scale for open-domain qa.
first, we aim to learn strong phraserepresentations from the supervision of readingcomprehension tasks.
we propose to use data aug-mentation and knowledge distillation to learn betterphrase representations within a single passage.
wethen adopt negative sampling strategies such as in-batch negatives (henderson et al., 2017; karpukhinet al., 2020), to better discriminate the phrases ata larger scale.
here, we present a novel methodcalled pre-batch negatives, which leverages preced-ing mini-batches as negative examples to compen-sate the need of large-batch training.
lastly, wepresent a query-side ﬁne-tuning strategy that dras-.
∗work partly done while visiting princeton university.
1our code is available at https://github.com/.
princeton-nlp/densephrases..2following previous work (seo et al., 2018), ‘phrase’ de-notes any contiguous segment of text up to l words (includingsingle words), which is not necessarily a linguistic phrase..proceedingsofthe59thannualmeetingoftheassociationforcomputationallinguisticsandthe11thinternationaljointconferenceonnaturallanguageprocessing,pages6634–6647august1–6,2021.©2021associationforcomputationallinguistics6634category.
model.
sparse?.
storage(gb).
#q/sec(gpu, cpu).
nq squad(acc).
(acc).
retriever-reader.
phrase retrieval.
drqa (chen et al., 2017)bertserini (yang et al., 2019)orqa (lee et al., 2019)realmnews (guu et al., 2020)dpr-multi (karpukhin et al., 2020).
denspi (seo et al., 2019)denspi + sparc (lee et al., 2020)densephrases (ours).
(cid:51)(cid:51)(cid:55)(cid:55)(cid:55).
(cid:51)(cid:51)(cid:55).
2621181876.
1,2001,547320.
1.8, 0.62.0, 0.48.6, 1.28.4, 1.20.9, 0.04.
2.9, 2.42.1, 1.720.6, 13.6.
--33.340.441.5.
8.114.540.9.
29.838.620.2-24.1.
36.240.738.0.table 1: retriever-reader and phrase retrieval approaches for open-domain qa.
the retriever-reader approachretrieves a small number of relevant documents or passages from which the answers are extracted.
the phraseretrieval approach retrieves an answer out of billions of phrase representations pre-indexed from the entire corpus.
appendix b provides detailed benchmark speciﬁcation.
the accuracy is measured on the test sets in the open-domain setting.
nq: natural questions..tically improves phrase retrieval performance andallows for transfer learning to new domains, with-out re-building billions of phrase representations.
as a result, all these improvements lead to amuch stronger phrase retrieval model, without theuse of any sparse representations (table 1).
weevaluate our model, densephrases, on ﬁve standardopen-domain qa datasets and achieve much bet-ter accuracies than previous phrase retrieval mod-els (seo et al., 2019; lee et al., 2020), with 15%–25% absolute improvement on most datasets.
ourmodel also matches the performance of state-of-the-art retriever-reader models (guu et al., 2020;karpukhin et al., 2020).
due to the removal ofsparse representations and careful design choices,we further reduce the storage footprint for the fullenglish wikipedia from 1.5tb to 320gb, as wellas drastically improve the throughput..finally, we envision that densephrases acts as aneural interface for retrieving phrase-level knowl-edge from a large text corpus.
to showcase thispossibility, we demonstrate that we can directlyuse densephrases for fact extraction, without re-building the phrase storage.
with only ﬁne-tuningthe question encoder on a small number of subject-relation-object triples, we achieve state-of-the-artperformance on two slot ﬁlling tasks (petroni et al.,2021), using less than 5% of the training data..2 background.
we ﬁrst formulate the task of open-domain ques-tion answering for a set of k documents d ={d1, .
.
.
, dk}.
we follow the recent work (chenet al., 2017; lee et al., 2019) and treat all of englishwikipedia as d, hence k ≈ 5 × 106. however,.
most approaches—including ours—are generic andcould be applied to other collections of documents.
the task aims to provide an answer ˆa for the in-put question q based on d. in this work, we focuson the extractive qa setting, where each answer isa segment of text, or a phrase, that can be found ind. denote the set of phrases in d as s(d) and eachphrase sk ∈ s(d) consists of contiguous wordswstart(k), .
.
.
, wend(k) in its document ddoc(k).
inpractice, we consider all the phrases up to l = 20words in d and s(d) comprises a large number of6 × 1010 phrases.
an extractive qa system returnsa phrase ˆs = argmaxs∈s(d) f (s|d, q) where f isa scoring function.
the system ﬁnally maps ˆs toan answer string ˆa: text(ˆs) = ˆa and the evalua-tion is typically done by comparing the predictedanswer ˆa with a gold answer a∗..although we focus on the extractive qa setting,recent works propose to use a generative model asthe reader (lewis et al., 2020; izacard and grave,2021), or learn a closed-book qa model (robertset al., 2020), which directly predicts answers with-out using an external knowledge source.
the ex-tractive setting provides two advantages: ﬁrst, themodel directly locates the source of the answer,which is more interpretable, and second, phrase-level knowledge retrieval can be uniquely adaptedto other nlp tasks as we show in §7.3..retriever-reader.
a dominating paradigm inopen-domain qa is the retriever-reader ap-proach (chen et al., 2017; lee et al., 2019;karpukhin et al., 2020), which leverages a ﬁrst-stage document retriever fretr and only reads topk(cid:48) (cid:28) k documents with a reader model fread.
the scoring function f (s | d, q) is decomposed as:.
6635f (s | d, q) = es(s, d)(cid:62)eq(q),.
(2).
3.2 base architecture.
f (s | d, q) = fretr({dj1, .
.
.
, djk(cid:48) } | d, q)× fread(s | {dj1, .
.
.
, djk(cid:48) }, q),.
(1).
where {j1, .
.
.
, jk(cid:48)} ⊂ {1, .
.
.
, k} and if s /∈s({dj1, .
.
.
, djk(cid:48) }), the score will be 0. it can eas-ily adapt to passages and sentences (yang et al.,2019; wang et al., 2019).
however, this approachsuffers from error propagation when incorrect docu-ments are retrieved and can be slow as it usually re-quires running an expensive reader model on everyretrieved document or passage at inference time..phrase retrieval.
seo et al.
(2019) introduce thephrase retrieval approach that encodes phrase andquestion representations independently and per-forms similarity search over the phrase representa-tions to ﬁnd an answer.
their scoring function f iscomputed as follows:.
where es and eq denote the phrase encoder andthe question encoder respectively.
as es(·) andeq(·) representations are decomposable, it cansupport maximum inner product search (mips)and improve the efﬁciency of open-domain qamodels.
previous approaches (seo et al., 2019;lee et al., 2020) leverage both dense and sparsevectors for phrase and question representationsby taking their concatenation: es(s, d) =[esparse(s, d), edense(s, d)].3 however, since thesparse vectors are difﬁcult to parallelize with densevectors, their method essentially conducts sparseand dense vector search separately.
the goal ofthis work is to only use dense representations,i.e., es(s, d) = edense(s, d), which can modelf (s | d, q) solely with mips, as well as close thegap in performance..3 densephrases.
3.1 overview.
we introduce densephrases, a phrase retrievalmodel that is built on fully dense representations.
our goal is to learn a phrase encoder as well as aquestion encoder, so we can pre-index all the pos-sible phrases in d, and efﬁciently retrieve phrasesfor any question through mips at testing time.
weoutline our approach as follows:.
• we ﬁrst learn a high-quality phrase encoderand an (initial) question encoder from thesupervision of reading comprehension tasks(§4.1), as well as incorporating effective nega-tive sampling to better discriminate phrases atscale (§4.2, §4.3)..• then, we ﬁx the phrase encoder and encodeall the phrases s ∈ s(d) and store the phraseindexing ofﬂine to enable efﬁcient search (§5).
• finally, we introduce an additional strategycalled query-side ﬁne-tuning (§6) by furtherupdating the question encoder.4 we ﬁnd thisstep to be very effective, as it can reducethe discrepancy between training (the ﬁrststep) and inference, as well as support transferlearning to new domains..before we present the approach in detail, we ﬁrst.
describe our base architecture below..our base architecture consists of a phrase encoderes and a question encoder eq.
given a passagep = w1, .
.
.
, wm, we denote all the phrases up to ltokens as s(p).
each phrase sk has start and end in-dicies start(k) and end(k) and the gold phraseis s∗ ∈ s(p).
following previous work on phraseor span representations (lee et al., 2017; seo et al.,2018), we ﬁrst apply a pre-trained language modelmp to obtain contextualized word representationsfor each passage token: h1, .
.
.
, hm ∈ rd.
then,we can represent each phrase sk ∈ s(p) as the con-catenation of corresponding start and end vectors:.
es(sk, p) = [hstart(k), hend(k)] ∈ r2d..(3).
a great advantage of this representation is that weeventually only need to index and store all the wordvectors (we use w(d) to denote all the words ind), instead of all the phrases s(d), which is atleast one magnitude order smaller..similarly, we need to learn a question encodereq(·) that maps a question q = ˜w1, .
.
.
, ˜wn to avector of the same dimension as es(·).
since thestart and end representations of phrases are pro-duced by the same language model, we use an-other two different pre-trained encoders mq,startand mq,end to differentiate the start and end po-sitions.
we apply mq,start and mq,end on q sep-arately and obtain representations qstart and qend.
3seo et al.
(2019) use sparse representations of both para-graphs and documents and lee et al.
(2020) use contextualizedsparse representations conditioned on the phrase..4in this paper, we use the term question and query inter-changeably as our question encoder can be naturally extendedto “unnatural” queries..6636figure 1: an overview of densephrases.
(a) we learn dense phrase representations in a single passage (§4.1) alongwith in-batch and pre-batch negatives (§4.2, §4.3).
(b) with the top-k retrieved phrase representations from theentire text corpus (§5), we further perform query-side ﬁne-tuning to optimize the question encoder (§6).
duringinference, our model simply returns the top-1 prediction..taken from the [cls] token representations re-spectively.
finally, eq(·) simply takes their con-catenation:.
eq(q) = [qstart, qend] ∈ r2d..(4).
note that we use pre-trained language models toinitialize mp, mq,start and mq,end and they areﬁne-tuned with the objectives that we will deﬁnelater.
in our pilot experiments, we found that span-bert (joshi et al., 2020) leads to superior perfor-mance compared to bert (devlin et al., 2019).
spanbert is designed to predict the informationin the entire span from its two endpoints, thereforeit is well suited for our phrase representations.
inour ﬁnal model, we use spanbert-base-cased asour base lms for es and eq, and hence d = 768.5see table 5 for an ablation study..4 learning phrase representations.
in this section, we start by learning dense phraserepresentations from the supervision of readingcomprehension tasks, i.e., a single passage p con-tains an answer a∗ to a question q. our goal is tolearn strong dense representations of phrases fors ∈ s(p), which can be retrieved by a dense rep-resentation of the question and serve as a direct.
5our base model is largely inspired by denspi (seo et al.,2019), although we deviate from theirs as follows.
(1) weremove coherency scalars and don’t split any vectors.
(2)denspi uses a shared encoder for phrases and questions whilewe use 3 separate language models initialized from the samepre-trained model.
(3) we use spanbert instead of bert..answer (§4.1).
then, we introduce two differentnegative sampling methods (§4.2, §4.3), which en-courage the phrase representations to be better dis-criminated at the full wikipedia scale.
see figure 1for an overview of densephrases..4.1 single-passage training.
to learn phrase representations in a single passagealong with question representations, we ﬁrst max-imize the log-likelihood of the start and end posi-tions of the gold phrase s∗ where text(s∗) = a∗.
the training loss for predicting the start position ofa phrase given a question is computed as:.
zstart1., .
.
.
, zstart.
m = [h(cid:62).
1 qstart, .
.
.
, h(cid:62).
p start = softmax(zstart1lstart = − log p start.
start(s∗)..mqstart],, .
.
.
, zstartm ),.
(5).
we can deﬁne lend in a similar way and the ﬁnalloss for the single-passage training is.
lsingle =.
lstart + lend2.
..(6).
this essentially learns reading comprehension with-out any cross-attention between the passage and thequestion tokens, which fully decomposes phraseand question representations..data augmentation since the contextualizedword representations h1, .
.
.
, hm are encoded ina query-agnostic way, they are always inferior to.
6637(cid:475)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:476)(cid:17)(cid:35)(cid:45)(cid:28)(cid:46)(cid:32)(cid:1)(cid:6)(cid:41)(cid:30)(cid:42)(cid:31)(cid:32)(cid:45)(cid:454)(cid:5)(cid:42)(cid:41)(cid:453)(cid:47)(cid:1)(cid:20)(cid:47)(cid:28)(cid:41)(cid:31)(cid:1)(cid:20)(cid:42)(cid:1)(cid:4)(cid:39)(cid:42)(cid:46)(cid:32)(cid:1)(cid:47)(cid:42)(cid:1)(cid:14)(cid:32)(cid:454)(cid:1)(cid:36)(cid:46)(cid:1)(cid:28)(cid:1)(cid:35)(cid:36)(cid:47)(cid:1)(cid:46)(cid:42)(cid:41)(cid:34)(cid:1)(cid:29)(cid:52)(cid:1)(cid:47)(cid:35)(cid:32)(cid:1)(cid:3)(cid:45)(cid:36)(cid:47)(cid:36)(cid:46)(cid:35)(cid:1)(cid:45)(cid:42)(cid:30)(cid:38)(cid:1)(cid:29)(cid:28)(cid:41)(cid:31)(cid:1)(cid:47)(cid:35)(cid:32)(cid:1)(cid:17)(cid:42)(cid:39)(cid:36)(cid:30)(cid:32)(cid:1)(cid:442)(cid:442)(cid:442)(cid:475)(cid:4)(cid:13)(cid:20)(cid:476)(cid:1)(cid:24)(cid:35)(cid:42)(cid:1)(cid:46)(cid:36)(cid:41)(cid:34)(cid:46)(cid:1)(cid:5)(cid:42)(cid:41)(cid:453)(cid:47)(cid:1)(cid:20)(cid:47)(cid:28)(cid:41)(cid:31)(cid:1)(cid:20)(cid:42)(cid:1)(cid:4)(cid:39)(cid:42)(cid:46)(cid:32)(cid:1)(cid:47)(cid:42)(cid:1)(cid:14)(cid:32)(cid:450)(cid:446)(cid:3)(cid:5)(cid:42)(cid:41)(cid:453)(cid:47)(cid:14)(cid:32)(cid:47)(cid:35)(cid:32)(cid:17)(cid:42)(cid:39)(cid:36)(cid:30)(cid:32)(cid:18)(cid:48)(cid:32)(cid:46)(cid:47)(cid:36)(cid:42)(cid:41)(cid:1)(cid:6)(cid:41)(cid:30)(cid:42)(cid:31)(cid:32)(cid:45)(cid:3)(cid:28)(cid:45)(cid:28)(cid:30)(cid:38)(cid:1)(cid:16)(cid:29)(cid:28)(cid:40)(cid:28)(cid:446)(cid:3)(cid:17)(cid:35)(cid:45)(cid:28)(cid:46)(cid:32)(cid:1)(cid:6)(cid:41)(cid:30)(cid:42)(cid:31)(cid:32)(cid:45)(cid:446)(cid:3)(cid:7)(cid:42)(cid:45)(cid:50)(cid:28)(cid:45)(cid:31)(cid:1)(cid:3)(cid:28)(cid:30)(cid:38)(cid:50)(cid:28)(cid:45)(cid:31)(cid:47)(cid:35)(cid:32)(cid:17)(cid:42)(cid:39)(cid:36)(cid:30)(cid:32)(cid:11)(cid:28)(cid:40)(cid:32)(cid:46)(cid:3)(cid:45)(cid:42)(cid:50)(cid:41)(cid:475)(cid:4)(cid:13)(cid:20)(cid:476)(cid:1)(cid:5)(cid:42)(cid:41)(cid:453)(cid:47)(cid:1)(cid:20)(cid:47)(cid:28)(cid:41)(cid:31)(cid:1)(cid:20)(cid:42)(cid:1)(cid:4)(cid:39)(cid:42)(cid:46)(cid:32)(cid:1)(cid:47)(cid:42)(cid:1)(cid:14)(cid:32)(cid:1)(cid:475)(cid:20)(cid:6)(cid:17)(cid:476)(cid:1)(cid:46)(cid:48)(cid:41)(cid:34)(cid:1)(cid:29)(cid:52)(cid:17)(cid:42)(cid:46)(cid:36)(cid:47)(cid:36)(cid:49)(cid:32)(cid:1)(cid:39)(cid:28)(cid:29)(cid:32)(cid:39)(cid:15)(cid:32)(cid:34)(cid:28)(cid:47)(cid:36)(cid:49)(cid:32)(cid:1)(cid:39)(cid:28)(cid:29)(cid:32)(cid:39)(cid:473)(cid:28)(cid:474)(cid:1)(cid:20)(cid:36)(cid:41)(cid:34)(cid:39)(cid:32)(cid:465)(cid:43)(cid:28)(cid:46)(cid:46)(cid:28)(cid:34)(cid:32)(cid:1)(cid:47)(cid:45)(cid:28)(cid:36)(cid:41)(cid:36)(cid:41)(cid:34)(cid:1)(cid:50)(cid:479)(cid:1)(cid:28)(cid:31)(cid:31)(cid:36)(cid:47)(cid:36)(cid:42)(cid:41)(cid:28)(cid:39)(cid:1)(cid:41)(cid:32)(cid:34)(cid:28)(cid:47)(cid:36)(cid:49)(cid:32)(cid:46)(cid:473)(cid:29)(cid:474)(cid:1)(cid:18)(cid:48)(cid:32)(cid:45)(cid:52)(cid:465)(cid:46)(cid:36)(cid:31)(cid:32)(cid:1)(cid:415)(cid:41)(cid:32)(cid:465)(cid:47)(cid:48)(cid:41)(cid:36)(cid:41)(cid:34)(cid:1)(cid:421)(cid:1)(cid:10)(cid:41)(cid:33)(cid:32)(cid:45)(cid:32)(cid:41)(cid:30)(cid:32)(cid:446)(cid:3)(cid:475)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:476)(cid:18)(cid:48)(cid:32)(cid:46)(cid:47)(cid:36)(cid:42)(cid:41)(cid:1)(cid:6)(cid:41)(cid:30)(cid:42)(cid:31)(cid:32)(cid:45)(cid:5)(cid:42)(cid:41)(cid:453)(cid:47)(cid:1)(cid:20)(cid:47)(cid:28)(cid:41)(cid:31)(cid:1)(cid:20)(cid:42)(cid:1)(cid:4)(cid:39)(cid:42)(cid:46)(cid:32)(cid:1)(cid:47)(cid:42)(cid:1)(cid:14)(cid:32)(cid:2)(cid:31)(cid:31)(cid:36)(cid:47)(cid:36)(cid:42)(cid:41)(cid:28)(cid:39)(cid:1)(cid:41)(cid:32)(cid:34)(cid:28)(cid:47)(cid:36)(cid:49)(cid:32)(cid:46)(cid:473)(cid:36)(cid:41)(cid:465)(cid:29)(cid:28)(cid:47)(cid:30)(cid:35)(cid:1)(cid:479)(cid:1)(cid:43)(cid:45)(cid:32)(cid:465)(cid:29)(cid:28)(cid:47)(cid:30)(cid:35)(cid:474)(cid:47)(cid:35)(cid:32)(cid:1)(cid:17)(cid:42)(cid:39)(cid:36)(cid:30)(cid:32)(cid:475)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:476)(cid:475)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:476)(cid:475)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:476)(cid:6)(cid:39)(cid:39)(cid:36)(cid:41)(cid:34)(cid:47)(cid:42)(cid:41)(cid:16)(cid:45)(cid:30)(cid:35)(cid:32)(cid:46)(cid:47)(cid:45)(cid:28)(cid:5)(cid:48)(cid:38)(cid:32)(cid:35)(cid:36)(cid:47)(cid:46)(cid:42)(cid:41)(cid:34)(cid:475)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:476)(cid:446)(cid:3)(cid:35)(cid:36)(cid:47)(cid:1)(cid:46)(cid:42)(cid:41)(cid:34)(cid:21)(cid:42)(cid:43)(cid:465)(cid:423)(cid:21)(cid:42)(cid:43)(cid:465)(cid:424)query-dependent representations in cross-attentionmodels (devlin et al., 2019), where passages arefed along with the questions concatenated by a spe-cial token such as [sep].
we hypothesize that onekey reason for the performance gap is that readingcomprehension datasets only provide a few anno-tated questions in each passage, compared to the setof possible answer phrases.
learning from this su-pervision is not easy to differentiate similar phrasesin one passage (e.g., s∗ = charles, prince of walesand another s = prince george for a question q =who is next in line to be the monarch of england?).
following this intuition, we propose to use a sim-ple model to generate additional questions for dataaugmentation, based on a t5-large model (raf-fel et al., 2020).
to train the question genera-tion model, we feed a passage p with the goldanswer s∗ highlighted by inserting surroundingspecial tags.
then, the model is trained to max-imize the log-likelihood of the question words ofq. after training, we extract all the named enti-ties in each training passage as candidate answersand feed the passage p with each candidate an-swer to generate questions.
we keep the question-answer pairs only when a cross-attention readingcomprehension model6 makes a correct predictionon the generated pair.
the remaining generated qapairs {(¯q1, ¯s1), (¯q2, ¯s2), .
.
.
, (¯qr, ¯sr)} are directlyaugmented to the original training set..distillation we also propose improving thephrase representations by distilling knowledgefrom a cross-attention model (hinton et al., 2015).
we minimize the kullback–leibler divergence be-tween the probability distribution from our phraseencoder and that from a standard spanbert-baseqa model.
the loss is computed as follows:.
c.c.).
kl(p start||p start.)
+ kl(p end||p end2.ldistill =.
,(7)where p start (and p end) is deﬁned in eq.
(5) andp startdenote the probability distributionscused to predict the start and end positions of an-swers in the cross-attention model..and p end.
c.4.2.in-batch negatives.
eventually, we need to build phrase representationsfor billions of phrases.
therefore, a bigger chal-lenge is to incorporate more phrases as negativesso the representations can be better discriminated.
6spanbert-large, 88.2 em on squad..figure 2: two types of negative samples for the ﬁrstbatch item (qstart) in a mini-batch of size b = 4 andc = 3. note that the negative samples for the endrepresentations (qend) are obtained in a similar manner.
isee §4.2 and §4.3 for more details..1.at a larger scale.
while seo et al.
(2019) simplysample two negative passages based on questionsimilarity, we use in-batch negatives for our densephrase representations, which has been shown to beeffective in learning dense passage representationsbefore (karpukhin et al., 2020)..i., qendi., qstarti., g endi.as shown in figure 2 (a), for the i-th exam-ple in a mini-batch of size b, we denote thehidden representations of the gold start and endpositions hstart(s∗) and hend(s∗) as gstartandgend, as well as the question representation asi[qstart].
let gstart, gend, qstart, qend be the, qendiib × d matrices and each row corresponds togstartrespectively.
basically, weican treat all the gold phrases from other pas-sages in the same mini-batch as negative exam-ples.
we compute sstart = qstartgstart(cid:124)and send =qendgend(cid:124)and the i-th row of sstart and send returnb scores each, including a positive score and b−11 , .
.
.
, send, .
.
.
, sstartnegative scores: sstartb .
similar to eq.
(5), we can compute the loss func-tion for the i-th example as:= softmax(sstart= softmax(sendlog p start_ibi., .
.
.
, sstart11 , .
.
.
, send.
p start_ibip end_ib.
b ),b ),+ log p end_ib2.b and send.
lneg = −.
(8).
1.,.
i.i.we also attempted using non-gold phrases fromother passages as negatives but did not ﬁnd a mean-ingful improvement..4.3 pre-batch negatives.
the in-batch negatives usually beneﬁt from a largebatch size (karpukhin et al., 2020).
however, it ischallenging to further increase batch sizes, as theyare bounded by the size of gpu memory.
next,we propose a novel negative sampling method.
6638positive(a) in-batch negatives ()b−1(b) pre-batch negatives ( )b×cdetached  in recent c batchesgstartinegativegstart1gstart2gstart3gstart4qstart1qstart2qstart3qstart4qstart1qstart2qstart3qstart4called pre-batch negatives, which can effectivelyutilize the representations from the preceding cmini-batches (figure 2 (b)).
in each iteration, wemaintain a fifo queue of c mini-batches to cachephrase representations gstart and gend.
the cachedphrase representations are then used as negativesamples for the next iteration, providing b × cadditional negative samples in total.7.
these pre-batch negatives are used together within-batch negatives and the training loss is the sameas eq.
(8), except that the gradients are not back-propagated to the cached pre-batch negatives.
afterwarming up the model with in-batch negatives, wesimply shift from in-batch negatives (b − 1 nega-tives) to in-batch and pre-batch negatives (hence atotal number of b × c + b − 1 negatives).
for sim-plicity, we use lneg to denote the loss for both in-batch negatives and pre-batch negatives.
since wedo not retain the computational graph for pre-batchnegatives, the memory consumption of pre-batchnegatives is much more manageable while allowingan increase in the number of negative samples..4.4 training objective.
finally, we optimize all the three losses together, onboth annotated reading comprehension examplesand generated questions from §4.1:.
l = λ1lsingle + λ2ldistill + λ3lneg,.
(9).
where λ1, λ2, λ3 determine the importance of eachloss term.
we found that λ1 = 1, λ2 = 2, and λ3 =4 works well in practice.
see table 5 and table 6for an ablation study of different components..5.indexing and search.
indexing after training the phrase encoder es,we need to encode all the phrases s(d) in the en-tire english wikipedia d and store an index ofthe phrase dump.
we segment each documentdi ∈ d into a set of natural paragraphs, fromwhich we obtain token representations for eachparagraph using es(·).
then, we build a phrasedump h = [h1, .
.
.
, h|w(d)|] ∈ r|w(d)|×d bystacking the token representations from all the para-graphs in d. note that this process is computation-ally expensive and takes about hundreds of gpuhours with a large disk footprint.
to reduce the.
7this approach is inspired by the momentum contrast ideaproposed in unsupervised visual representation learning (heet al., 2020).
contrary to their approach, we have separateencoders for phrases and questions and back-propagate to bothduring training without a momentum update..size of phrase dump, we follow and modify severaltechniques introduced in seo et al.
(2019) (see ap-pendix e for details).
after indexing, we can usetwo rows i and j of h to represent a dense phraserepresentation [hi, hj].
we use faiss (johnsonet al., 2017) for building a mips index of h.8.
search for a given question q, we can ﬁnd theanswer ˆs as follows:ˆs = argmax.
es(s(i,j), d)(cid:62)eq(q),.
= argmax.
(hqstart)i + (hqend)j,.
(10).
s(i,j).
s(i,j).
where s(i,j) denotes a phrase with start and endindices as i and j in the index h. we can com-pute the argmax of hqstart and hqend efﬁcientlyby performing mips over h with qstart and qend.
in practice, we search for the top-k start and top-kend positions separately and perform a constrainedsearch over their end and start positions respec-tively such that 1 ≤ i ≤ j < i + l ≤ |w(d)|..6 query-side fine-tuning.
so far, we have created a phrase dump h that sup-ports efﬁcient mips search.
in this section, we pro-pose a novel method called query-side ﬁne-tuningby only updating the question encoder eq to cor-rectly retrieve a desired answer a∗ for a questionq given h. formally speaking, we optimize themarginal log-likelihood of the gold answer a∗ for aquestion q, which resembles the weakly-supervisedqa setting in previous work (lee et al., 2019; minet al., 2019).
for every question q, we retrieve topk phrases and minimize the objective:.
lquery = − log.
(cid:80).
s∈ ˜s(q),text(s)=a∗ exp (cid:0)f (s|d,q)(cid:1)s∈ ˜s(q) exp (cid:0)f (s|d,q)(cid:1).
(cid:80).
,.
(11)where f (s|d, q) is the score of the phrase s(eq.
(2)) and ˜s(q) denotes the top k phrases forq (eq.
(10)).
in practice, we use k = 100 for allthe experiments..there are several advantages for doing this: (1)we ﬁnd that query-side ﬁne-tuning can reduce thediscrepancy between training and inference, andhence improve the ﬁnal performance substantially(§8).
even with effective negative sampling, themodel only sees a small portion of passages com-pared to the full scale of d and this training objec-tive can effectively ﬁll in the gap.
(2) this train-ing strategy allows for transfer learning to unseen.
8we use ivfsq4 with 1m clusters and set n-probe to 256..6639domains, without rebuilding the entire phrase in-dex.
more speciﬁcally, the model is able to quicklyadapt to new qa tasks (e.g., webquestions) whenthe phrase dump is built using squad or naturalquestions.
we also ﬁnd that this can transfers tonon-qa tasks when the query is written in a dif-ferent format.
in §7.3, we show the possibility ofdirectly using densephrases for slot ﬁlling tasksby using a query such as (michael jackson, is asinger of, x).
in this regard, we can view our modelas a dense knowledge base that can be accessedby many different types of queries and it is able toreturn phrase-level knowledge efﬁciently..7 experiments.
7.1 setup.
datasets.
we use two reading comprehensiondatasets: squad (rajpurkar et al., 2016) and nat-ural questions (nq) (kwiatkowski et al., 2019) tolearn phrase representations, in which a single goldpassage is provided for each question.
for the open-domain qa experiments, we evaluate our approachon ﬁve popular open-domain qa datasets: natu-ral questions, webquestions (wq) (berant et al.,2013), curatedtrec (trec) (baudiš and šediv`y,2015), triviaqa (tqa) (joshi et al., 2017), andsquad.
note that we only use squad and/or nqto build the phrase index and perform query-sideﬁne-tuning (§6) for other datasets..we also evaluate our model on two slot ﬁllingtasks, to show how to adapt our densephrases forother knowledge-intensive nlp tasks.
we focuson using two slot ﬁlling datasets from the kiltbenchmark (petroni et al., 2021): t-rex (elsaharet al., 2018) and zero-shot relation extraction (levyet al., 2017).
each query is provided in the formof “{subject entity} [sep] {relation}" and theanswer is the object entity.
appendix c providesthe statistics of all the datasets..implementation details.
we denote the trainingdatasets used for reading comprehension (eq.
(9))as cphrase.
for open-domain qa, we train two ver-sions of phrase encoders, each of which are trainedon cphrase = {squad} and {nq, squad}, re-spectively.
we build the phrase dump h for the2018-12-20 wikipedia snapshot and perform query-side ﬁne-tuning on each dataset using eq.
(11).
forslot ﬁlling, we use the same phrase dump for open-domain qa, cphrase = {nq, squad} and performquery-side ﬁne-tuning on randomly sampled 5k.
model.
query-dependent.
bert-basespanbert-base.
query-agnostic.
squad.
nq (long).
em f1.
em f1.
80.885.7.
88.592.2.
69.973.2.
78.281.0.dilbert (siblini et al., 2020)deformer (cao et al., 2020)denspi†denspi + sparc†densephrases (ours).
63.0-73.676.478.3.
72.072.181.784.886.3.
--68.2-71.9.
--76.1-79.6.table 2: reading comprehension results, evaluated onthe development sets of squad and natural ques-tions.
underlined numbers are estimated from the ﬁg-ures from the original papers.
†: bert-large model..or 10k training examples to see how rapidly ourmodel adapts to the new query types.
see ap-pendix d for details on the hyperparameters andappendix a for an analysis of computational cost..7.2 experiments: question answering.
reading comprehension.
in order to show theeffectiveness of our phrase representations, we ﬁrstevaluate our model in the reading comprehensionsetting for squad and nq and report its perfor-mance with other query-agnostic models (eq.
(9)without query-side ﬁne-tuning).
this problem wasoriginally formulated by seo et al.
(2018) as thephrase-indexed question answering (piqa) task..compared to previous query-agnostic models,our model achieves the best performance of 78.3em on squad by improving the previous phraseretrieval model (denspi) by 4.7% (table 2).
al-though it is still behind cross-attention models, thegap has been greatly reduced and serves as a strongstarting point for the open-domain qa model..open-domain qa.
experimentalresults onopen-domain qa are summarized in table 3. with-out any sparse representations, densephrases out-performs previous phrase retrieval models by alarge margin and achieves a 15%–25% absoluteimprovement on all datasets except squad.
train-ing the model of lee et al.
(2020) on cphrase ={nq, squad} only increases the result from14.5% to 16.5% on nq, demonstrating that it doesnot sufﬁce to simply add more datasets for train-ing phrase representations.
our performance isalso competitive with recent retriever-reader mod-els (karpukhin et al., 2020), while running muchfaster during inference (table 1)..6640model.
retriever-reader.
cretr: (pre-)training.
nq wq trec tqa squad.
drqa (chen et al., 2017)bert + bm25 (lee et al., 2019)orqa (lee et al., 2019)realmnews (guu et al., 2020)dpr-multi (karpukhin et al., 2020).
--{wiki.
}†{wiki., cc-news}†{nq, wq, trec, tqa}.
-26.533.340.441.5.
20.717.736.440.742.4.phrase retrieval.
denspi (seo et al., 2019)denspi + sparc (lee et al., 2020)denspi + sparc (lee et al., 2020)densephrases (ours)densephrases (ours).
cphrase: training.
{squad}{squad}{nq, squad}{squad}{nq, squad}.
8.1∗14.5∗16.531.240.9.
11.1∗17.3∗-36.337.5.
25.421.330.142.949.4.
31.6∗35.7∗-50.351.0.
-47.145.0-56.8.
30.7∗34.4∗-53.650.7.
29.833.220.2-24.1.
36.240.7-39.438.0.table 3: open-domain qa results.
we report exact match (em) on the test sets.
we also show the additionaltraining or pre-training datasets for learning the retriever models (cretr) and creating the phrase dump (cphrase).
∗:no supervision using target training data (zero-shot).
†: unlabeled data used for extra pre-training..t-rex.
zsre.
model m share split qg distill em.
model.
acc.
f1.
acc.
f1.
dpr + bertdpr + bartrag.
densephrases5kdensephrases10k.
-11.1223.12.
25.3227.84.
-11.4123.94.
29.7632.34.
4.4718.9136.83.
40.3941.34.
27.0920.3239.91.
45.8946.79.table 4: slot ﬁlling results on the test sets of t-rexand zero shot re (zsre) in the kilt benchmark.
wereport kilt-ac and kilt-f1 (denoted as acc and f1in the table), which consider both span-level accuracyand correct retrieval of evidence documents..7.3 experiments: slot filling.
table 4 summarizes the results on the two slot ﬁll-ing datasets, along with the baseline scores pro-vided by petroni et al.
(2021).
the only extractivebaseline is dpr + bert, which performs poorlyin zero-shot relation extraction.
on the other hand,our model achieves competitive performance on alldatasets and achieves state-of-the-art performanceon two datasets using only 5k training examples..8 analysis.
ablation of phrase representations.
table 5shows the ablation result of our model on squad.
upon our choice of architecture, augmenting train-ing set with generated questions (qg = (cid:51)) andperforming distillation from cross-attention mod-els (distill = (cid:51)) improve performance up to em =78.3. we attempted adding the generated questionsto the training of the spanbert-qa model butﬁnd a 0.3% improvement, which validates that datasparsity is a bottleneck for query-agnostic models..denspi bb.
sb.
bl..densebb.
phrases bb.
sb.
sb.
sb..(cid:51)(cid:51)(cid:51).
(cid:51)(cid:55)(cid:55)(cid:55)(cid:55).
(cid:51)(cid:51)(cid:51).
(cid:55)(cid:55)(cid:55)(cid:55)(cid:55).
(cid:55)(cid:55)(cid:55).
(cid:55)(cid:55)(cid:55)(cid:51)(cid:51).
(cid:55)(cid:55)(cid:55).
(cid:55)(cid:55)(cid:55)(cid:55)(cid:51).
70.268.573.6.
70.271.973.276.378.3.table 5: ablation of densephrases on the developmentset of squad.
bb: bert-base, sb: spanbert-base,bl: bert-large.
share: whether question and phraseencoders are shared or not.
split: whether the fullhidden vectors are kept or split into start and end vec-tors.
qg: question generation (§4.1).
distill: distilla-tion (eq.(7)).
denspi (seo et al., 2019) also included acoherency scalar and see their paper for more details..effect of batch negatives.
we further evaluatethe effectiveness of various negative samplingmethods introduced in §4.2 and §4.3.
since it iscomputationally expensive to test each setting atthe full wikipedia scale, we use a smaller text cor-pus dsmall of all the gold passages in the develop-ment sets of natural questions, for the ablationstudy.
empirically, we ﬁnd that results are gener-ally well correlated when we gradually increase thesize of |d|.
as shown in table 6, both in-batchand pre-batch negatives bring substantial improve-ments.
while using a larger batch size (b = 84)is beneﬁcial for in-batch negatives, the number ofpreceding batches in pre-batch negatives is optimalwhen c = 2. surprisingly, the pre-batch negativesalso improve the performance when d = {p}..6641b c d = {p} d = dsmall.
qs nq wq trec tqa squad.
type.
none.
+ in-batch.
+ pre-batch.
48.
4884.
848484.
-.
--.
124.
70.4.
70.570.3.
71.671.971.2.
35.3.
52.454.2.
59.860.459.8.table 6: effect of in-batch negatives and pre-batch neg-atives on the development set of natural questions.
b:batch size.
c: number of preceding mini-batches usedin pre-batch negatives.
dsmall: all the gold passages inthe development set of nq.
{p}: single passage..effect of query-side ﬁne-tuning.
we summa-rize the effect of query-side ﬁne-tuning in table 7.for the datasets that were not used for training thephrase encoders (tqa, wq, trec), we observea 15% to 20% improvement after query-side ﬁne-tuning.
even for the datasets that have been used(nq, squad), it leads to signiﬁcant improvements(e.g., 32.6%→40.9% on nq for cphrase = {nq})and it clearly demonstrates it can effectively reducethe discrepancy between training and inference..9 related work.
learning effective dense representations of wordsis a long-standing goal in nlp (bengio et al., 2003;collobert et al., 2011; mikolov et al., 2013; peterset al., 2018; devlin et al., 2019).
beyond words,dense representations of many different granular-ities of text such as sentences (le and mikolov,2014; kiros et al., 2015) or documents (yih et al.,2011) have been explored.
while dense phrase rep-resentations have been also studied for statisticalmachine translation (cho et al., 2014) or syntacticparsing (socher et al., 2010), our work focuses onlearning dense phrase representations for qa andany other knowledge-intensive tasks where phrasescan be easily retrieved by performing mips..this type of dense retrieval has been also stud-ied for sentence and passage retrieval (humeauet al., 2019; karpukhin et al., 2020) (see lin et al.,2020 for recent advances in dense retrieval).
whiledensephrases is explicitly designed to retrievephrases that can be used as an answer to givenqueries, retrieving phrases also naturally entails re-trieving larger units of text, provided the datastoremaintains the mapping between each phrase andthe sentence and passage in which it occurs..cphrase = {squad}.
(cid:55)12.3(cid:51) 31.2.
11.836.3.
36.950.3.
34.653.6.cphrase = {nq}.
(cid:55)32.6(cid:51) 40.9.
21.137.1.
32.349.7.
32.449.2.cphrase = {nq, squad}.
(cid:55)28.9(cid:51) 40.9.
18.937.5.
34.951.0.
31.950.7.
35.539.4.
20.725.7.
33.238.0.table 7:effect of query-side ﬁne-tuning indensephrases on each test set.
we report em ofeach model before (qs = (cid:55)) and after (qs = (cid:51)) thequery-side ﬁne-tuning..10 conclusion.
in this study, we show that we can learn dense repre-sentations of phrases at the wikipedia scale, whichare readily retrievable for open-domain qa andother knowledge-intensive nlp tasks.
we learnboth phrase and question encoders from the supervi-sion of reading comprehension tasks and introducetwo batch-negative techniques to better discrimi-nate phrases at scale.
we also introduce query-sideﬁne-tuning that adapts our model to different typesof queries.
we achieve strong performance on ﬁvepopular open-domain qa datasets, while reducingthe storage footprint and improving latency signif-icantly.
we also achieve strong performance ontwo slot ﬁlling datasets using only a small numberof training examples, showing the possibility ofutilizing our densephrases as a knowledge base..acknowledgments.
we thank sewon min, hyunjae kim, gyuwankim, jungsoo park, zexuan zhong, dan fried-man, chris sciavolino for providing valuable com-ments and feedback.
this research was supportedby a grant of the korea health technology r&dproject through the korea health industry develop-ment institute (khidi), funded by the ministry ofhealth & welfare, republic of korea (grant num-ber: hr20c0021) and national research foun-dation of korea (nrf-2020r1a2c3010638).
itwas also partly supported by the james mi *91 re-search innovation fund for data science and anamazon research award..6642ethical considerations.
our work builds on standard reading comprehen-sion datasets such as squad to build phrase rep-resentations.
squad, in particular, is createdfrom a small number of wikipedia articles sampledfrom top-10,000 most popular articles (measuredby pageranks), hence some of our models trainedonly on squad could be easily biased towards thesmall number of topics that squad contains.
wehope that excluding such datasets during training orinventing an alternative pre-training procedure forlearning phrase representations could mitigate thisproblem.
although most of our efforts have beenmade to reduce the computational complexity ofprevious phrase retrieval models (further detailedin appendices a and e), leveraging our phrase re-trieval model as a knowledge base will inevitablyincrease the minimum requirement for the addi-tional experiments.
we plan to apply vector quanti-zation techniques to reduce the additional cost ofusing our model as a kb..references.
akari asai, kazuma hashimoto, hannaneh hajishirzi,richard socher, and caiming xiong.
2020. learn-ing to retrieve reasoning paths over wikipedia graphfor question answering.
in international conferenceon learning representations (iclr)..petr baudiš and jan šediv`y.
2015. modeling of thequestion answering task in the yodaqa system.
in international conference of the cross-languageevaluation forum for european languages (clef)..yoshua bengio, réjean ducharme, pascal vincent, andchristian jauvin.
2003. a neural probabilistic lan-guage model.
the journal of machine learning re-search (jmlr)..jonathan berant, andrew chou, roy frostig, and percyliang.
2013. semantic parsing on freebase fromquestion-answer pairs.
in empirical methods in nat-ural language processing (emnlp)..qingqing cao, harsh trivedi, aruna balasubramanian,and niranjan balasubramanian.
2020. deformer:decomposing pre-trained transformers for fasterin association for computa-question answering.
tional linguistics (acl)..danqi chen, adam fisch, jason weston, and antoinebordes.
2017. reading wikipedia to answer open-in association for computa-domain questions.
tional linguistics (acl)..danqi chen and wen-tau yih.
2020. open-domainin association for computa-.
question answering.
tional linguistics (acl)..kyunghyun cho, bart van merriënboer, caglar gul-cehre, dzmitry bahdanau, fethi bougares, holgerschwenk, and yoshua bengio.
2014.learningphrase representations using rnn encoder-decoderin empiricalfor statistical machine translation.
methods in natural language processing (emnlp)..ronan collobert, jason weston, léon bottou, michaelkarlen, koray kavukcuoglu, and pavel kuksa.
2011.natural language processing (almost) from scratch.
jmlr..jacob devlin, ming-wei chang, kenton lee, andkristina toutanova.
2019. bert: pre-training ofdeep bidirectional transformers for language under-standing.
in north american chapter of the associ-ation for computational linguistics (naacl)..hady elsahar, pavlos vougiouklis, arslen remaci,jonathon hare, frederiquechristophe gravier,laforest, and elena simperl.
2018. t-rex: a largescale alignment of natural language with knowledgein international conference on lan-base triples.
guage resources and evaluation (lrec)..david ferrucci, eric brown, jennifer chu-carroll,james fan, david gondek, aditya a kalyanpur,adam lally, j william murdock, eric nyberg, johnprager, et al.
2010. building watson: an overviewof the deepqa project.
ai magazine, 31(3)..kelvin guu, kenton lee, zora tung, panupong pa-supat, and ming-wei chang.
2020.realm:retrieval-augmented language model pre-training.
in international conference on machine learning(icml)..kaiming he, haoqi fan, yuxin wu, saining xie, andross girshick.
2020. momentum contrast for un-supervised visual representation learning.
in ieeeconference on computer vision and pattern recog-nition (cvpr)..matthew henderson, rami al-rfou, brian strope, yun-hsuan sung, lászló lukács, ruiqi guo, sanjiv ku-mar, balint miklos, and ray kurzweil.
2017. efﬁ-cient natural language response suggestion for smartreply.
arxiv preprint arxiv:1705.00652..geoffrey hinton, oriol vinyals, and jeff dean.
2015.distilling the knowledge in a neural network.
arxivpreprint arxiv:1503.02531..samuel humeau, kurt shuster, marie-anne lachaux,and jason weston.
2019. poly-encoders: architec-tures and pre-training strategies for fast and accuratemulti-sentence scoring.
in international conferenceon learning representations (iclr)..gautier izacard and edouard grave.
2021. leveragingpassage retrieval with generative models for openin european chap-domain question answering.
ter of the association for computational linguistics(eacl)..6643jeff johnson, matthijs douze, and hervé jégou.
2017.billion-scale similarity search with gpus.
arxivpreprint arxiv:1702.08734..mandar joshi, danqi chen, yinhan liu, daniel s weld,luke zettlemoyer, and omer levy.
2020. span-bert: improving pre-training by representing andpredicting spans.
transactions of the association ofcomputational linguistics (tacl)..mandar joshi, eunsol choi, daniel s weld, and lukezettlemoyer.
2017. triviaqa: a large scale dis-tantly supervised challenge dataset for reading com-prehension.
in association for computational lin-guistics (acl)..patrick lewis, ethan perez, aleksandara piktus, fabiopetroni, vladimir karpukhin, naman goyal, hein-rich küttler, mike lewis, wen-tau yih, tim rock-täschel, et al.
2020. retrieval-augmented generationin advances infor knowledge-intensive nlp tasks.
neural information processing systems (neurips)..jimmy lin, rodrigo nogueira, and andrew yates.
textrank-pretrained transformers forarxiv preprint.
bert and beyond..2020.ing:arxiv:2010.06467..lucian vlad lita, abe ittycheriah, salim roukos, andtruecasing.
in associa-.
nanda kambhatla.
2003.tion for computational linguistics (acl)..vladimir karpukhin, barlas o˘guz, sewon min, patricklewis, ledell wu, sergey edunov, danqi chen,and wen-tau yih.
2020. dense passage retrievalfor open-domain question answering.
in empiricalmethods in natural language processing (emnlp)..tomas mikolov, ilya sutskever, kai chen, greg s cor-rado, and jeff dean.
2013. distributed representa-tions of words and phrases and their compositional-ity.
in advances in neural information processingsystems (nips)..diederik p kingma and jimmy ba.
2015. adam: amethod for stochastic optimization.
in internationalconference on learning representations (iclr)..ryan kiros, yukun zhu, russ r salakhutdinov,richard zemel, raquel urtasun, antonio torralba,and sanja fidler.
2015. skip-thought vectors.
ad-vances in neural information processing systems(nips)..tom kwiatkowski, jennimaria palomaki, olivia red-ﬁeld, michael collins, ankur parikh, chris alberti,danielle epstein, illia polosukhin, jacob devlin,kenton lee, et al.
2019. natural questions: a bench-transac-mark for question answering research.
tions of the association of computational linguis-tics (tacl)..quoc le and tomas mikolov.
2014. distributed repre-sentations of sentences and documents.
in interna-tional conference on machine learning (icml)..jinhyuk lee, minjoon seo, hannaneh hajishirzi, andjaewoo kang.
2020. contextualized sparse repre-sentations for real-time open-domain question an-swering.
in association for computational linguis-tics (acl)..kenton lee, ming-wei chang, and kristina toutanova.
2019. latent retrieval for weakly supervised opendomain question answering.
in association for com-putational linguistics (acl)..kenton lee, shimi salant, tom kwiatkowski, ankurparikh, dipanjan das, and jonathan berant.
2017.learning recurrent span representations for extrac-tive question answering.
in iclr..omer levy, minjoon seo, eunsol choi, and lukezettlemoyer.
2017. zero-shot relation extraction viareading comprehension.
in computational naturallanguage learning (conll)..sewon min, danqi chen, hannaneh hajishirzi, andluke zettlemoyer.
2019. a discrete hard em ap-proach for weakly supervised question answering.
in empirical methods in natural language process-ing (emnlp)..matthew e peters, mark neumann, mohit iyyer, mattgardner, christopher clark, kenton lee, and lukezettlemoyer.
2018. deep contextualized word repre-sentations.
in north american chapter of the asso-ciation for computational linguistics (naacl)..fabio petroni, aleksandra piktus, angela fan, patricklewis, majid yazdani, nicola de cao, jamesthorne, yacine jernite, vassilis plachouras, timrocktäschel, et al.
2021. kilt: a benchmark forknowledge intensive language tasks.
in north amer-ican chapter of the association for computationallinguistics (naacl)..colin raffel, noam shazeer, adam roberts, katherinelee, sharan narang, michael matena, yanqi zhou,wei li, and peter j liu.
2020. exploring the lim-its of transfer learning with a uniﬁed text-to-texttransformer.
journal of machine learning research,21(140)..pranav rajpurkar, jian zhang, konstantin lopyrev, andpercy liang.
2016. squad: 100,000+ questions formachine comprehension of text.
in empirical meth-ods in natural language processing (emnlp)..adam roberts, colin raffel, and noam shazeer.
2020.how much knowledge can you pack into the param-eters of a language model?
in empirical methods innatural language processing (emnlp)..minjoon seo, tom kwiatkowski, ankur parikh, alifarhadi, and hannaneh hajishirzi.
2018. phrase-indexed question answering: a new challenge forin empiricalscalable document comprehension.
methods in natural language processing (emnlp)..6644minjoon seo,.
jinhyuk lee, tom kwiatkowski,ankur p parikh, ali farhadi, and hannaneh ha-jishirzi.
2019. real-time open-domain question an-swering with dense-sparse phrase index.
in associa-tion for computational linguistics (acl)..wissam siblini, mohamed challal, and charlottepasqual.
2020.delaying interaction layersin transformer-based encoders for efﬁcient openarxiv preprintdomain question answering.
arxiv:2010.08422..richard socher, christopher d manning, and an-drew y ng.
2010. learning continuous phrase repre-sentations and syntactic parsing with recursive neu-ral networks.
in proceedings of the nips-2010 deeplearning and unsupervised feature learning work-shop..ellen m voorhees et al.
1999. the trec-8 question.
answering track report.
in trec..zhiguo wang, patrick ng, xiaofei ma, ramesh nallap-ati, and bing xiang.
2019. multi-passage bert: aglobally normalized bert model for open-domainquestion answering.
in empirical methods in natu-ral language processing (emnlp)..wei yang, yuqing xie, aileen lin, xingyu li, luchentan, kun xiong, ming li, and jimmy lin.
2019.end-to-end open-domain question answering withbertserini.
in north american chapter of the asso-ciation for computational linguistics (naacl)..wen-tau yih, kristina toutanova, john c platt, andchristopher meek.
2011. learning discriminativeprojections for text similarity measures.
in compu-tational natural language learning (conll)..6645a computational cost.
we describe the resources and time spent dur-ing inference (table 1 and a.1) and indexing (ta-ble a.1).
with our limited gpu resources (24gb× 4), it takes about 20 hours for indexing the entirephrase representations.
we also largely reduced thestorage from 1,547gb to 320gb by (1) removingsparse representations and (2) using our sharing andsplit strategy.
see appendix e for the details on thereduction of storage footprint and appendix b forthe speciﬁcation of our server for the benchmark..indexing.
resources.
storage.
time.
dprdenspi + sparcdensephrases.
32gb gpu × 824gb gpu × 424gb gpu × 4.
76gb17h1,547gb 85h20h320gb.
inference.
ram / gpu.
#q/sec (gpu, cpu).
dprdenspi + sparcdensephrases.
86gb / 17gb27gb / 2gb12gb / 2gb.
0.9, 0.042.1, 1.720.6, 13.6.table a.1: complexity analysis of three open-domainqa models during indexing and inference.
for infer-ence, we also report the minimum requirement of ramand gpu memory for running each model with gpu.
for computing #q/s for cpu, we do not use gpus butload all models on the ram..b server speciﬁcations for benchmark.
to compare the complexity of open-domain qamodels, we install all models in table 1 on thesame server using their public open-source code.
our server has the following speciﬁcations:.
hardware.
intel xeon cpu e5-2630 v4 @ 2.20ghz128gb ram12gb gpu (titan xp) × 22tb 970 evo plus nvme m.2 ssd × 1.table b.2: server speciﬁcation for the benchmark.
for dpr, due to its large memory consumption,we use a similar server with a 24gb gpu (titanrtx).
for all models, we use 1,000 randomly sam-pled questions from the natural questions devel-opment set for the speed benchmark and measure#q/sec.
we set the batch size to 64 for all modelsexcept bertserini, orqa and realm, whichdo not allow a batch size of more than 1 in theiropen-source implementations.
#q/sec for dpr in-cludes retrieving passages and running a reader.
dataset.
train.
dev.
test.
natural questionswebquestionscuratedtrectriviaqasquad.
79,1683,4171,35378,78578,713.t-rexzero-shot re.
2,284,168147,909.
8,7573611338,8378,886.
5,0003,724.
3,6102,03269411,31310,570.
5,0004,966.table c.3: statistics of ﬁve open-domain qa datasetsand two slot ﬁlling datasets.
we follow the same splitsin open-domain qa for the two reading comprehensiondatasets (squad and natural questions)..model and the batch size for the reader model is setto 8 to ﬁt in the 24gb gpu (retriever batch sizeis still 64).
for other hyperparameters, we use thedefault settings of each model.
we also exclude thetime and the number of questions in the ﬁrst ﬁveiterations for warming up each model.
note thatdespite our effort to match the environment of eachmodel, their latency can be affected by various dif-ferent settings in their implementations such as thechoice of library (pytorch vs. tensorﬂow)..c data statistics and pre-processing.
in table c.3, we show the statistics of ﬁve open-domain qa datasets and two slot ﬁlling datasets.
pre-processed open-domain qa datasets are pro-vided by chen et al.
(2017) except natural ques-tions and triviaqa.
we use a version of naturalquestions and triviaqa provided by min et al.
(2019); lee et al.
(2019), which are pre-processedfor the open-domain qa setting.
slot ﬁllingdatasets are provided by petroni et al.
(2021).
weuse two reading comprehension datasets (squadand natural questions) for training our model oneq.
(9).
for squad, we use the original datasetprovided by the authors (rajpurkar et al., 2016).
for natural questions (kwiatkowski et al., 2019),we use the pre-processed version provided by asaiet al.
(2020).9 we use the short answer as a groundtruth answer a∗ and its long answer as a gold pas-sage p. we also match the gold passages in naturalquestions to the paragraphs in wikipedia wheneverpossible.
since we want to check the performancechanges of our model with the growing numberof tokens, we follow the same split (train/dev/test)used in natural questions-open for the readingcomprehension setting as well.
during the valida-.
9https://github.com/akariasai/learning_to_retrieve_reasoning_paths.
6646positions (trained together with eq.
(9)).
we tunethe threshold for the ﬁlter logits on the readingcomprehension development set to the point wherethe performance does not drop signiﬁcantly whilemaximally ﬁltering tokens.
in the full wikipediasetting, we ﬁlter about 75% of tokens and store770m token representations..second, in our architecture, we use a base model(spanbert-base) for a smaller dimension of tokenrepresentations (d = 768) and does not use anysparse representations including tf-idf or contex-tualized sparse representations (lee et al., 2020).
we also use the scalar quantization for storingfloat32 vectors as int4 during indexing..lastly, since the inference in eq.
(10) is purelybased on mips, we do not have to keep the originalstart and end vectors which takes about 500gb.
however, when we perform query-side ﬁne-tuning,we need the original start and end vectors for re-constructing them to compute eq.
(11) since (theon-disk version of) mips index only returns thetop-k scores and their indices, but not the vectors..tion of our model and baseline models, we excludesamples whose answers lie in a list or a table froma wikipedia article..d hyperparameters.
we use the adam optimizer (kingma and ba, 2015)in all our experiments.
for training our phrase andquestion encoders with eq.
(9), we use a learningrate of 3e-5 and the norm of the gradient is clippedat 1. we use a batch size of b =84 and train eachmodel for 4 epochs for all datasets, where the lossof pre-batch negatives is applied in the last twoepochs.
we use squad to train our qg model10and use spacy11 for extracting named entities ineach training passage, which are used to generatequestions.
the number of generated questions is327,302 and 1,126,354 for squad and naturalquestions, respectively.
the number of precedingbatches c is set to 2..for the query-side ﬁne-tuning with eq.
(11), weuse a learning rate of 3e-5 and the norm of the gra-dient is clipped at 1. we use a batch size of 12and train each model for 10 epochs for all datasets.
the top k for the eq.
(11) is set to 100. while weuse a single 24gb gpu (titan rtx) for train-ing the phrase encoders with eq.
(9), query-sideﬁne-tuning is relatively cheap and uses a single12gb gpu (titan xp).
using the developmentset, we select the best performing model (based onem) for each dataset, which are then evaluated oneach test set.
since spanbert only supports casedmodels, we also truecase the questions (lita et al.,2003) that are originally provided in the lowercase(natural questions and webquestions)..e reducing storage footprint.
as shown in table 1, we have reduced the stor-age footprint from 1,547gb (lee et al., 2020) to320gb.
we detail how we can reduce the storagefootprint in addition to the several techniques intro-duced by seo et al.
(2019)..first, following seo et al.
(2019), we apply alinear transformation on the passage token repre-sentations to obtain a set of ﬁlter logits, which canbe used to ﬁlter many token representations fromw(d).
this ﬁlter layer is supervised by applyingthe binary cross entropy with the gold start/end.
10the quality of generated questions from a qg modeltrained on natural questions is worse due to the ambiguity ofinformation-seeking questions.
11https://spacy.io/.
6647