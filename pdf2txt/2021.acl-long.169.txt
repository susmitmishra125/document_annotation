plotcoder: hierarchical decoding for synthesizing visualization codein programmatic context.
xinyun chen, linyuan gong, alvin cheung and dawn songuc berkeley{xinyun.chen, gly, akcheung, dawnsong}@berkeley.edu.
abstract.
creating effective visualization is an impor-tant part of data analytics.
while there aremany libraries for creating visualizations, writ-ing such code remains difﬁcult given the myr-iad of parameters that users need to provide.
in this paper, we propose the new task of syn-thesizing visualization programs from a com-bination of natural language utterances andcode context.
to tackle the learning prob-lem, we introduce plotcoder, a new hierar-chical encoder-decoder architecture that mod-els both the code context and the input ut-terance.
we use plotcoder to ﬁrst deter-mine the template of the visualization code,followed by predicting the data to be plotted.
we use jupyter notebooks containing visual-ization programs crawled from github to trainplotcoder.
on a comprehensive set of testsamples from those notebooks, we show thatplotcoder correctly predicts the plot typeof about 70% samples, and synthesizes the cor-rect programs for 35% samples, performing 3-4.5% better than the baselines.1.
1.introduction.
visualizations play a crucial role in obtaininginsights from data.
while a number of li-braries (hunter, 2007; seaborn, 2020; bostocket al., 2011) have been developed for creating vi-sualizations that range from simple scatter plots tocomplex 3d bar charts, writing visualization coderemains a difﬁcult task.
for instance, drawing ascatter plot using the python matplotlib library canbe done using both the scatter and plot methods,and the scatter method (matplotlib, 2020) takesin 2 required parameters (the values to plot) alongwith 11 other optional parameters (marker type,color, etc), with some parameters having numerictypes (e.g., the size of each marker) and some being.
1our code and data are available at https://github..com/jungyhuk/plotcoder..arrays (e.g., the list of colors for each collection ofthe plotted data, where each color is speciﬁed as astring or another array of rgb values).
looking upeach parameter’s meaning and its valid values re-mains tedious and error-prone, and the multitude oflibraries available further compounds the difﬁcultyfor developers to create effective visualizations..in this paper, we propose to automatically syn-thesize visualization programs using a combina-tion of natural language utterances and the pro-grammatic context that the visualization programwill reside (e.g., code written in the same ﬁle asthe visualization program to load the plotted data),focusing on programs that create static visualiza-tions (e.g., line charts, scatter plots, etc).
whilethere has been prior work on synthesizing codefrom natural language (zettlemoyer and collins,2012; oda et al., 2015; wang et al., 2015; yinet al., 2018), and with addition information such asdatabase schemas (zhong et al., 2017; yu et al.,2018, 2019b,a) or input-output examples (polo-sukhin and skidanov, 2018; zavershynskyi et al.,2018), synthesizing general-purpose code from nat-ural language remains highly difﬁcult due to theambiguity in the natural language input and com-plexity of the target.
our key insight in synthe-sizing visualization programs is to leverage theirproperties: they tend to be short, do not use com-plex programmatic control structures (typically afew lines of method calls without any control ﬂowor loop constructs), with each method call restrictedto a single plotting command (e.g., scatter, pie)along with its parameters (e.g., the plotted data).
this inﬂuences our model architecture design aswe will explain..to study the visualization code synthesis prob-lem, we use the python jupyter notebooks fromthe juice dataset (agashe et al., 2019), whereeach notebook contains the visualization programand its programmatic context.
these notebooks.
proceedingsofthe59thannualmeetingoftheassociationforcomputationallinguisticsandthe11thinternationaljointconferenceonnaturallanguageprocessing,pages2169–2181august1–6,2021.©2021associationforcomputationallinguistics2169are crawled from github and written by vari-ous programmers, thus a main challenge is un-derstanding the complexity and the noisiness ofreal-world programmatic contexts and the hugevariance in the quality of natural language com-ments.
unfortunately, using standard lstm-basedmodels and transformer architectures (vaswaniet al., 2017) fails to solve the task, as noted in priorwork (agashe et al., 2019)..we observe that while data to be plotted is usu-ally stored in pandas dataframes (pandas, 2020),they are not explicitly annotated in juice.
hence,unlike prior work, we augment the programmaticcontext with dataframe names and their schemawhen available in predicting the plotted data..we next utilize our insight above and designa hierarchical deep neural network code genera-tion model called plotcoder that decomposessynthesis into two subtasks: generating the plotcommand, then the parameters to pass in given thecommand.
plotcoder uses a pointer networkarchitecture (vinyals et al., 2015), which allowsthe model to directly select code tokens in the pre-vious code cells in the same notebook as the plot-ted data.
meanwhile, inspired by the schema link-ing techniques proposed for semantic parsing withstructured inputs, such as text to sql tasks (iyeret al., 2017; wang et al., 2019a; guo et al., 2019),plotcoder’s encoder connects the embeddingof the natural language descriptions with their cor-responding code fragments in previous code cellswithin each notebook.
although the constructedlinks can be noisy because the code context is lessstructured than the database tables in text-to-sqlproblems, we observe that our approach results insubstantial performance gain..we evaluate plotcoder’s ability to synthesizevisualization programs using jupyter notebooks ofhomework assignments or exam solutions.
on thegold test set where the notebooks are ofﬁcial so-lutions, our best model correctly predicts the plottypes for over 80% of samples, and precisely pre-dicts both the plot types and the plotted data forover 50% of the samples.
on the more noisy testsplits with notebooks written by students, whichmay include work-in-progress code, our model stillachieves over 70% plot type prediction accuracy,and around 35% accuracy for generating the entirecode, showing how plotcoder’s design deci-sions improve our prediction accuracy..figure 1: an example of plot code synthesisproblem studied in this work.
given the naturallanguage, code context within a few code cellsfrom the target code, and other code snippets re-lated to dataframes, plotcoder synthesizes thedata visualization code..2 related work.
there has been work on translating natural lan-guage to code in different languages (zettlemoyerand collins, 2012; wang et al., 2015; oda et al.,2015; yin et al., 2018; zhong et al., 2017; yuet al., 2018; lin et al., 2018).
while the in-put speciﬁcation only includes the natural lan-guage for most tasks, prior work also uses ad-ditional information for program prediction, in-cluding database schemas and contents for sqlquery synthesis (zhong et al., 2017; yu et al., 2018,2019b,a), input-output examples (polosukhin andskidanov, 2018; zavershynskyi et al., 2018), andcode context (iyer et al., 2018; agashe et al., 2019).
there has also been work on synthesizing datamanipulation programs only from input-output ex-amples (drosos et al., 2020; wang et al., 2017).
in this work, we focus on synthesizing visualiza-tion code from both natural language descriptionand code context, and we construct our benchmarkbased on the python jupyter notebooks from thejuice (agashe et al., 2019).
compared to juice’sinput format, we also annotate dataframe schemaif available, which is especially important for visu-alization code synthesis..prior work has studied generating plots fromother speciﬁcations.
falx (wang et al., 2019b,.
2170plotcoder:hierarchicaldecodingforsynthesizingvisualizationcodeinprogrammaticcontextanonymousnaacl-hlt2021submissionabstract001naturallanguageexploretherelationshipbetweenrarityandaskillofyourchoice.chooseoneskill(‘attack’,‘defense’or‘speed’)anddothefollowing.usethescipypackagetoassesswhethercatchratepredictstheskill.createascatterplottovisualizehowtheskilldependsupontherarityofthepokemon.overlayabestﬁtlineontothescatterplot.localcodecontextslope,intercept,r_value,p_value,std_err=linregress(df['catch_rate'],df['speed'],)x=np.arange(256)y=slope*x+interceptdistantdataframecontextdf['weight_kg'].describe()df['color'].value_counts().plot(kind='bar')df['body_style'].value_counts().plot(kind='bar')grouped=df.groupby(['body_style','hasgender',]).mean()df.groupby('color')['attack'].mean()df.groupby('color')['pr_male'].mean()df.sort_values('catch_rate',ascending=false).head()dataframeschemadf:['catch_rate','speed','weight_kg','color','body_style']groundtruthplt.scatter(df['catch_rate'],df['speed'])plt.plot(x,y)002(a)naturallanguagecreateascatterplotoftheobservationsinthe‘credit’datasetfortheattributes‘duration’and‘age’(ageshouldbeshownonthexaxis).localcodecontextduration=credit['duration'].valuesage=credit['age'].valuesgroundtruthplt.scatter(age,duration)predictionplt.scatter(duration,age)003(b)naturallanguagethisgraphprovidesmoreevidencethatthehigherastate’sparticipationrates,thelowerthatstate’saveragesscoresarelikelytobe.thehighertheparticipationrate,thelowertheexpectedaverageverbalscores.localcodecontextplt.plot(sat_data['math'],sat_data['verbal'])dataframeschemasat:['rate','math','verbal']groundtruthplt.plot(sat_data['rate'],sat_data['math'])plt.plot(sat_data['rate'],sat_data['verbal'])predictionplt.plot(sat_data['math'],sat_data['verbal'])plt.plot(sat_data['rate'],sat_data['verbal'])004(a)naturallanguageplotagaussianbyloopingthrougharangeofxvaluesandcreatingaresultinglistofgaussianvalues,glocalcodecontextx_axis=np.arange(-20,20,0.1)g=[]forxinx_axis:g.append(f(mu,sigma2,x))groundtruth&predictionplt.plot(x_axis,g)005(b)naturallanguagelikeinq9,let’sstartbythinkingabouttwodicelocalcodecontextresults=[]foriinrange(1,7):forjinrange(1,7):print((i,j),max(i,j))results.append(max(i,j))groundtruth&predictionplt.hist(results)00612021) synthesizes plots from input-output exam-ples, but do not use any learning technique, andfocuses on developing a domain-speciﬁc languagefor plot generation instead.
in (dibia and demiralp,2019), the authors apply a standard lstm-basedsequence-to-sequence model with attention for plotgeneration, but the model takes in only raw data tobe visualized with no natural language input.
thevisualization code synthesis problem studied in ourwork is much more complex, where both the natu-ral language and the code context can be long, andprogram speciﬁcations are implicit and ambiguous.
our design of hierarchical program decodingis inspired by prior work on sketch learning forprogram synthesis, where various sketch represen-tations have been proposed for different applica-tions (solar-lezama, 2008; murali et al., 2018;dong and lapata, 2018; nye et al., 2019).
com-pared to other code synthesis tasks, a key differ-ence is that our sketch representation distinguishesbetween dataframes and other variables, which isimportant for synthesizing visualization code..our code synthesis problem is also related tocode completion, i.e., autocompleting the programgiven the code context (raychev et al., 2014; liet al., 2018; svyatkovskiy et al., 2020).
however,standard code completion only requires the modelto generate a few tokens following the code con-text, rather than entire statements.
in contrast, ourtask requires the model to synthesize complete andexecutable visualization code.
furthermore, unlikestandard code completion, our model synthesizescode from both the natural language descriptionand code context.
nevertheless, when the preﬁxof the visualization code is given, our model couldalso be used for code completion, by including thegiven partial code as part of the code context..3 visualization code synthesis problem.
we now discuss our problem setup of synthesiz-ing visualization code in programmatic context,where the model input includes different types ofspeciﬁcations.
we ﬁrst describe the model inputs,then introduce our code canonicalization processto make it easier to train our models and evaluatethe accuracy, and ﬁnally our evaluation metrics..3.1 program speciﬁcation.
the natural language description and code from thepreceding cells.
to do so, our model takes in thefollowing inputs:• the natural language description for the visual-ization, which we extract from the natural lan-guage markdown above the target code cell con-taining the gold program in the notebook..• the local code context, deﬁned as a few codecells that immediately precede the target codecell.
the number of cells to include is a tunablehyper-parameter to be described in section 5.
• the code snippets related to dataframe manipu-lation that appear before the target code cell inthe notebook, but are not included in the localcode context.
we refer to such code as the distantdataframe context.
when such context containscode that uses dataframes, they are part of themodel input by default.
as mentioned in section 1, unlike juice, we alsoextract the code snippets related to dataframes, andannotate the dataframe schemas according to theirsyntax trees.
as shown in figure 1, knowing thecolumn names in each dataframe is important forour task, as dataframes are often used for plotting..3.2 code canonicalization.
one way to train our models is to directly utilize theplotting code in jupyter notebooks as the groundtruth.
however, due to the variety of plotting apisand coding styles, such a model rarely predictsexactly the same code as written in jupyter note-books.
for example, there are at least four ways inmatplotlib (and similar in other libraries) to createa scatter plot for columns ‘y’ against ‘x’ from adataframe df: plt.scatter(df[’x’],df[’y’]),plt.plot(df[’x’],df[’y’],’o’),df.plot.scatter(x=’x’,y=’y’),df.plot(kind=’scatter’,x=’x’,y=’y’).
moreover, given that the natural language de-scription is ambiguous, many plot attributes arehard to precisely predict.
for example, from thecontext shown in figure 1, there are many validways to specify the plot title, the marker style,axis ranges, etc.
in our experiments, we ﬁnd thatwhen trained on raw target programs, fewer than5% predictions are exactly the same as the groundtruth, and a similar phenomenon is also observedearlier (agashe et al., 2019)..we illustrate our program speciﬁcation in figure 1,which represents a jupyter notebook fragment.
ourtask is to synthesize the visualization code given.
therefore, we design a canonical representationfor plotting programs, which covers the core of plotgeneration.
speciﬁcally, we convert the plotting.
2171code into one of the following templates:• lib.plot type(x,{y}∗), where lib is a plot-ting library, and plot type is the plot type to becreated.
the number of arguments may vary fordifferent plot type, e.g., 1 for histograms andpie charts, and 2 for scatter plots..(a.used.
commonly.
matplotlib.pyplot),.
example, when using plt as.
theabbreviationconvert.
• l0 \n l1 \n ... lm, where each li is aplotting command in the above template, and \nare separators.
forlibraryofdf.plot(kind=’scatter’,x=’x’,y=’y’)into plt.scatter(df[’x’],df[’y’]), wherelib = plt and plot type = scatter.
plottingcode in other libraries could be converted similarly.
the tokens that represent the plotted data, i.e., xand y, are annotated in the code context as follows:• var, when the token is a variable name, e.g., x.we.
and y in figure 1..• df, when the token is a pandas dataframe or a.python dictionary, e.g., df in figure 1..• str, when the token is a column name of adataframe, or a key name of a python dictionary,such as ‘catch rate’ and ‘speed’ in figure 1.the above annotations are used to cover differenttypes of data references.
for example, a columnin a dataframe is usually referred to as df[str],and sometimes as df[var] where var is a string.
in section 4.2, we will show how to utilize these an-notations for hierarchical program decoding, whereour decoder ﬁrst generates a program sketch thatpredicts these token types without the plotted data,then predicts the actual plotted data subsequently..3.3 evaluation metrics.
plot type accuracy.
to compute this metric, wecategorize all plots into several types, and a predic-tion is correct when it belongs to the same type asthe ground truth.
in particular, we consider the fol-lowing categories: (1) scatter plots (e.g., generatedby plt.scatter); (2) histograms (e.g., generatedby plt.hist); (3) pie charts (e.g., generated byplt.pie); (4) a scatterplot overlaid by a line (e.g.,such as that shown in figure 1, or generated bysns.lmplot); (5) a plot including a kernel densityestimate (e.g., plots generated by sns.distplotor sns.kdeplot); and (6) others, which are mostlyplots generated by plt.plot..plotted data accuracy.
this metric measureswhether the predicted program selects the same.
data to plot as the ground truth.
unless otherwisespeciﬁed, the ordering of variables must match theground truth as well, i.e., swapping the data usedto plot x and y axes result in different plots.
program accuracy.
we consider a predicted pro-gram to be correct if both the plot type and plotteddata are correct.
as discussed in section 3.2, we donot evaluate the correctness of other plot attributesbecause they are mostly unspeciﬁed..4 plotcoder model architecture.
in this section, we present plotcoder, a hier-archical model architecture for synthesizing vi-sualization code from natural language and codecontext.
plotcoder includes an lstm-basedencoder (hochreiter and schmidhuber, 1997) tojointly embed the natural language and code con-text, as well as a hierarchical decoder that generatesapi calls and selects data for plotting.
we providean overview of our model architecture in figure 2..4.1 nl-code context encoder.
plotcoder’s encoder computes a vector repre-sentation for each token in the natural languagedescription and the code context, where the codecontext is the concatenation of the code snippetsdescribing dataframe schemas and the local codecells, as described in section 3.1.nl encoder.
we build a vocabulary for the natu-ral language tokens, and train an embedding matrixfor it.
afterwards, we use a bi-directional lstmto encode the input natural language sequence (de-noted as lstmnl), and use the lstm’s output ateach timestep as the contextual embedding vectorfor each token.
code context encoder.
we build a vocabulary vcfor the code context, and train an embedding matrixfor it.
vc also includes the special tokens {var,df, str} used for sketch decoding in section 4.2.we train another bi-directional lstm (lstmc),which computes a contextual embedding vector foreach token in a similar way to the natural languageencoder.
we denote the hidden state of lstmc atthe last timestep as hc.
nl-code linking.
capturing the correspondencebetween the code context and natural language iscrucial in achieving a good prediction performance.
for example, in figure 2, plotcoder infers thatthe dataframe column “age” should be plotted, asthis column name is mentioned in the natural lan-guage description.
inspired by this observation, we.
2172figure 2: overview of the plotcoder architecture.
the nl-code linking component connects the embeddingvectors for underscored tokens in natural language and code context, i.e., “age”..design the nl-code linking mechanism to explic-itly connect the embedding vectors of code tokensand their corresponding natural language words.
speciﬁcally, for each token in the code contextthat also occurs in the natural language, let hc andhnl be its embedding vectors computed by lstmcand lstmnl, respectively, we compute a new codetoken embedding vector as:.
h(cid:48)c = wl([hc; hnl])where wl is a linear layer, and [hc; hnl] is the con-catenation of hc and hnl.
when no natural languageword matches the code token, hnl is the embeddingvector of the [eos] token at the end of the natu-ral language description.
when we include thisnl-code linking component in the model, h(cid:48)c re-places the original embedding hc for each token inthe code context, and the new embedding is usedfor decoding.
we observe that many informativenatural language descriptions explicitly state thevariable names and dataframe columns for plotting,which makes our nl-code linking effective.
more-over, this component is especially useful when thevariable names for plotting are unseen in the train-ing set, thus nl-code linking provides the only cueto indicate that these variables are relevant..4.2 hierarchical program decoder.
we train another lstm to decode the visualizationcode sequence, denoted as lstmp.
our decodergenerates the program in a hierarchical way.
ateach timestep, the model ﬁrst predicts a token fromthe code token vocabulary that represents the pro-gram sketch.
as shown in figure 2, the programsketch does not include the plotted data.
after that,the decoder predicts the plotted data, where it em-ploys a copy mechanism (gu et al., 2016; vinyalset al., 2015) to select tokens from the code context.
first, we initiate the hidden state of lstmp withhc, the ﬁnal hidden state of lstmc, and the starttoken is [go] for both sketch and full program de-coding.
at each step t, let st−1 and ot−1 be the.
sketch token and output program token generatedat the previous step.
note that st−1 and ot−1 aredifferent only when st−1 ∈ {var, df, str}, whereot−1 is the actual data name with the correspond-ing type.
let est−1 and eot−1 be the embeddingvectors of st−1 and ot−1 respectively, which arecomputed using the same embedding matrix for thecode context encoder.
the input of lstmp is theconcatenation of the two embedding vectors, i.e.,[est−1; eot−1].
attention.
to compute attention vectors over thenatural language description and the code context,we employ the two-step attention in (iyer et al.,2018).
speciﬁcally, we ﬁrst use hpt to computethe attention vector over the natural language inputusing the standard attention mechanism (bahdanauet al., 2015), and we denote the attention vector asattnt.
then, we use attnt to compute the attentionvector over the code context, denoted as attpt.
sketch decoding.
for sketch decoding,themodel computes the probability distribution amongall sketch tokens in the code token vocabulary vc:.
p r(st) = softmax(ws(hpt + attnt + attpt))here ws is a linear layer.
for hierarchical decod-ing, we do not allow the model to directly decodethe names of the plotted data during sketch decod-ing, so st is selected only from the valid sketchtokens, such as library names, plotting functionnames, and special tokens for plotted data represen-tation in templates discussed in section 3.2.data selection.
for st ∈ {var, df, str}, we usethe copy mechanism to select the plotted data fromthe code context.
speciﬁcally, our decoder includes3 pointer networks (vinyals et al., 2015) for select-ing data with the type var, df, and str respectively,and they employ similar architectures but differentmodel parameters..we take variable name selection as an instance toillustrate our data selection approach using the copy.
2173split.
train dev (gold) test (gold) dev (hard) test (hard).
5.2 evaluation setup.
allscatterhistpiescatter+plotkdeothers.
389711189588565741533260913504.
57191413317.
48171111513.
827254182143451292.
894276175135764309.table 1: dataset statistics.
the description of the dif-ferent plot categories is in section 3.3..mechanism.
we ﬁrst compute vt = wv(attnt),where wv is a linear layer.
for the i-th token ci inthe code context, let hci be its embedding vector,we compute its prediction probability as:.
p r(ci) =.
exp vtt hcij exp vt.t hcj.
(cid:80).
after that, the model selects the token with thehighest prediction probability as the next programtoken ot, and uses the corresponding embeddingvectors for st and ot as the input for the next de-coding step of lstmp..the decoding process terminates when the.
model generates the [eof] token..5 experiments.
in this section, we ﬁrst describe our dataset forvisualization code synthesis, then introduce ourexperimental setup and discuss the results..5.1 dataset construction.
those.
pandas.dataframe.plot,.
we build our benchmark upon the juicedataset, and select those that call plotting apis,from matplotlib.pyplotincluding(plt),seaborn(sns), ggplot, bokeh, plotly, geoplotlib,pygal.
over 99% of the samples use plt,pandas.dataframe.plot, or sns.
we ﬁrst extractplot samples from the original dev and test splitsof juice to construct dev (gold) and test (gold).
however, the gold splits are too small to obtainquantitative results.
therefore, we extract around1,700 jupyter notebooks of homeworks and examsfrom juice’s training set, and split them roughlyevenly into dev (hard) and test (hard).
allremaining plot samples from the juice trainingsplit are included in our training set.
the length ofthe visualization programs to be generated variesbetween 6 and 80 tokens, but the code context istypically much longer.
we summarize the datasetstatistics in table 1..implementation details.
unless otherwise spec-iﬁed, for the input speciﬁcation we include k = 3previous code cells as the local context, which usu-ally provides the best accuracy.
we set 512 as thelength limit for both the natural language and thecode context.
for all model architectures, we trainthem for 50 epochs, and select the best checkpointbased on the program accuracy on the dev (hard)split.
more details are deferred to appendix a..baselines.
we compare the full plotcoderagainst the following baselines: (1) - hierarchy:the encoder is the same as in the full plotcoder,but the decoder directly generates the full programwithout predicting the sketch.
(2) - link: the en-coder does not use nl-code linking, and the de-coder is not hierarchical.
(3) lstm: the modeldoes not use nl-code linking, copy mechanism,and hierarchical decoding.
the encoder still usestwo separate lstms to embed the natural lan-guage and code context, which performs betterthan the lstm baseline in prior work (agasheet al., 2019).
(4) + bert: we use the same hier-archical decoder as the full model, but replace theencoder with a transformer architecture (vaswaniet al., 2017) initialized from a pre-trained model,and we ﬁne-tune the encoder with other part ofthe model.
we evaluated two pre-trained models.
one is roberta-base (liu et al., 2019), an im-proved version of bert-base (devlin et al., 2018)pre-trained on a large text corpus.
another is code-bert (feng et al., 2020), which has the same ar-chitecture as roberta-base, but is pre-trained ongithub code in several programming languagesincluding python, and has demonstrated good per-formance on code retrieval tasks.
to demonstratethe effectiveness of target code canonicalizationdiscussed in section 3.2, we also compare withmodels that are directly trained on the raw groundtruth code from the same set of jupyter notebooks..5.3 results.
we present the program prediction accuracies intable 2. first, training on the canonicalized codesigniﬁcantly boosts the performance for all mod-els, suggesting that canonicalization improves dataquality and hence prediction accuracies.
whentrained with target code canonicalization,thefull plotcoder signiﬁcantly outperforms othermodel variants on different data splits.
on the harddata splits, the hierarchical plotcoder predicts.
217435% of the samples correctly, improving over thenon-hierarchical model by 3 − 4.5%.
meanwhile,nl-code linking enables the model to better cap-ture the correspondence between the code contextand the natural language, and consistently improvesthe performance when trained on canonicalized tar-get code.
without the copy mechanism, the base-line lstm cannot predict any token outside of thecode vocabulary.
therefore, this model performsworse than other lstm-based models, especiallyon plotted data accuracies, as shown in table 3..interestingly, while our hierarchical decoding,nl-code linking, and copy mechanism are mainlydesigned to improve the prediction accuracy of theplotted data, as shown in table 4, we observe thatthe plot type accuracies of our full model are alsomostly better, especially on the hard splits.
tobetter understand this, we break down the resultsby plot type, and observe that the most signiﬁcantimprovement comes from the predictions of scatterplots (“s”) and plots in “others” category.
we positthat these two categories constitute the majority ofthe dataset, and the hierarchical model learns tobetter categorize plot types from a large numberof training samples.
in addition, we observe thatthe full model does not always perform better thanother baselines on data splits of small sizes, andthe difference mainly comes from the ambiguity inthe natural language description.
we defer morediscussion to section 5.4..also, using bert-like encoders does not im-prove the results.
this might be due to the dif-ference in data distribution for pre-training andvocabularies.
speciﬁcally, roberta is pre-trainedon english passages, which does not include manyvisualization-related descriptions and code com-ments.
therefore, the subword vocabulary utilizedby roberta breaks down important keywords forvisualization, e.g., “scatterplots” and “histograms”into multiple words, which limits model perfor-mance, especially for plot type prediction.
usingcodebert improves the performance of roberta,but it still does not improve over the lstm-basedmodels, which may again due to vocabulary mis-match.
as a result, in table 4, the plot type accura-cies of both models using bert-like encoders areconsiderably lower than the lstm-based models..to better understand the plotted data predictionperformance, in addition to the default plotted dataaccuracy that requires the data order to be the sameas the ground truth, we also evaluate a relaxed.
47.37%47.37%45.61%40.35%35.09%26.32%.
28.07%26.32%24.56%24.56%24.56%24.56%.
49.12%47.37%47.37%40.35%40.35%29.82%.
33.33%31.58%28.07%26.32%31.58%28.07%.
model.
test (hard) dev (hard) test (gold) dev (gold).
full model− hierarchy− linklstm+ codebert+ roberta.
full model− hierarchy− linklstm+ codebert+ roberta.
with code canonicalization.
34.79%30.20%29.98%26.17%33.11%32.77%.
34.70%31.56%28.05%24.67%34.58%33.37%.
56.25%45.83%43.75%41.67%54.17%50.00%.
without code canonicalization20.58%20.25%20.02%16.22%20.92%20.47%.
22.73%22.85%21.77%16.93%22.61%22.37%.
22.92%18.75%20.83%16.67%22.92%20.83%.
table 2: evaluation on program accuracy..model.
test (hard) dev (hard) test (gold) dev (gold).
full model− hierarchy− linklstm+ codebert+ roberta.
full model− hierarchy− linklstm+ codebert+ roberta.
with code canonicalization.
40.16%35.91%35.46%29.87%38.14%37.47%.
38.69%37.00%35.67%28.05%38.33%38.33%.
60.42%47.92%47.92%43.75%58.33%58.33%.
without code canonicalization24.94%26.73%25.39%18.90%26.85%25.28%.
27.69%27.93%27.21%21.04%27.21%27.81%.
29.17%31.25%25.00%18.75%29.17%27.08%.
table 3: evaluation on plotted data accuracy..version without ordering constraints.
note that theordering includes two factors: (1) the ordering ofthe plotted data for the different axes; and (2) theordering of plots when multiple plots are included.
we observe that the ordering issue happens foraround 1.5% of samples, and is more problematicfor scatter plots (“s”) and “others.” figure 3 showssample predictions where the model selects thecorrect set of data to plot, but the ordering is wrong.
although sometimes the natural language explicitlyspeciﬁes which axes to plot (e.g., figure 3 (a)),such descriptions are mostly implicit (e.g., figure 3(b)), making it hard for the model to learn.
fullresults on different plot types are in section 5.4..5.3.1 the effect of different model inputs.
to evaluate the effect of including different inputspeciﬁcations, we present the results in table 5.speciﬁcally, - nl means the model input does notinclude the natural language, and - distant dfsmeans the code context only includes the local codecells.
interestingly, even without the natural lan-guage description, plotcoder correctly predictsa considerable number of samples.
figure 4 showssample correct predictions without relying on thenatural language description.
to predict the plotted.
2175model.
test (hard) dev (hard) test (gold) dev (gold).
full model− hierarchy− linklstm+ codebert+ roberta.
full model− hierarchy− linklstm+ codebert+ roberta.
with code canonicalization.
70.58%64.65%65.32%66.67%65.44%65.21%.
71.46%68.92%64.09%67.47%67.96%66.38%.
83.33%87.50%81.25%85.42%75.00%66.67%.
without code canonicalization63.53%61.41%61.30%64.65%56.04%61.30%.
65.66%67.47%63.72%65.78%57.07%61.91%.
72.92%66.67%64.58%81.25%60.42%68.75%.
78.95%82.46%73.68%85.96%57.89%54.39%.
80.70%73.68%77.19%70.18%56.14%49.12%.
table 4: evaluation on plot type accuracy..figure 4: examples of model predictions evenwithout the natural language input..figure 3: examples of predictions where themodel selects the correct set of data to plot, butthe order is wrong..data, a simple yet effective heuristic is to selectvariable names appearing in the most recent codecontext.
this is also one possible reason that causesthe wrong data ordering prediction in figure 3(a);in fact, the prediction is correct if we change theorder of assignment statements for variables ageand duration in the code context..input.
test (hard) dev (hard) test (gold) dev (gold).
full input− distant dfs− nl.
34.79%34.34%27.52%.
34.70%34.10%28.42%.
56.25%52.08%43.75%.
47.37%45.61%21.05%.
table 5: evaluation on the full hierarchical modelwith different inputs..meanwhile, we evaluated plotcoder by vary-ing the number of local code cells k. the resultsshow that the program accuracies converge or start.
figure 5: a sample prediction that requires a goodunderstanding of the code context..to decrease when k > 3 for different models, asobserved in (agashe et al., 2019).
however, the ac-curacy drop of our hierarchical model is much lessnoticeable than the baselines, suggesting that ourmodel is more resilient to the addition of irrelevantcode context.
see appendix b for more discussion..5.4 prediction results per plot type.
we present the breakdown results per plot type intables 6 and 7. to better understand the plotteddata prediction performance, in addition to the de-fault plotted data accuracy that requires the dataorder to be the same as the ground truth, we alsoevaluate a relaxed version without ordering con-straints, described as permutation invariant in ta-ble 7. we compute the results on test (hard), which.
2176plotcoder:hierarchicaldecodingforsynthesizingvisualizationcodeinprogrammaticcontextanonymousnaacl-hlt2021submissionabstract001naturallanguageexploretherelationshipbetweenrarityandaskillofyourchoice.chooseoneskill(‘attack’,‘defense’or‘speed’)anddothefollowing.usethescipypackagetoassesswhethercatchratepredictstheskill.createascatterplottovisualizehowtheskilldependsupontherarityofthepokemon.overlayabestﬁtlineontothescatterplot.localcodecontextslope,intercept,r_value,p_value,std_err=linregress(df['catch_rate'],df['speed'],)x=np.arange(256)y=slope*x+interceptdistantdataframecontextdf['weight_kg'].describe()df['color'].value_counts().plot(kind='bar')df['body_style'].value_counts().plot(kind='bar')grouped=df.groupby(['body_style','hasgender',]).mean()df.groupby('color')['attack'].mean()df.groupby('color')['pr_male'].mean()df.sort_values('catch_rate',ascending=false).head()dataframeschemadf:['catch_rate','speed','weight_kg','color','body_style']groundtruthplt.scatter(df['catch_rate'],df['speed'])plt.plot(x,y)002(a)naturallanguageplotagaussianbyloopingthrougharangeofxvaluesandcreatingaresultinglistofgaussianvalues,glocalcodecontextx_axis=np.arange(-20,20,0.1)g=[]forxinx_axis:g.append(f(mu,sigma2,x))groundtruth&predictionplt.plot(x_axis,g)003(b)naturallanguagelikeinq9,let’sstartbythinkingabouttwodicelocalcodecontextresults=[]foriinrange(1,7):forjinrange(1,7):print((i,j),max(i,j))results.append(max(i,j))groundtruth&predictionplt.hist(results)004(a)naturallanguagecreateascatterplotoftheobservationsinthe‘credit’datasetfortheattributes‘duration’and‘age’(ageshouldbeshownonthexaxis).localcodecontextduration=credit['duration'].valuesage=credit['age'].valuesgroundtruthplt.scatter(age,duration)predictionplt.scatter(duration,age)005(b)naturallanguagethisgraphprovidesmoreevidencethatthehigherastate’sparticipationrates,thelowerthatstate’saveragesscoresarelikelytobe.thehighertheparticipationrate,thelowertheexpectedaverageverbalscores.localcodecontextplt.plot(sat_data['math'],sat_data['verbal'])dataframeschemasat:['rate','math','verbal']groundtruthplt.plot(sat_data['rate'],sat_data['math'])plt.plot(sat_data['rate'],sat_data['verbal'])predictionplt.plot(sat_data['math'],sat_data['verbal'])plt.plot(sat_data['rate'],sat_data['verbal'])0061plotcoder:hierarchicaldecodingforsynthesizingvisualizationcodeinprogrammaticcontextanonymousnaacl-hlt2021submissionabstract001naturallanguageexploretherelationshipbetweenrarityandaskillofyourchoice.chooseoneskill(‘attack’,‘defense’or‘speed’)anddothefollowing.usethescipypackagetoassesswhethercatchratepredictstheskill.createascatterplottovisualizehowtheskilldependsupontherarityofthepokemon.overlayabestﬁtlineontothescatterplot.localcodecontextslope,intercept,r_value,p_value,std_err=linregress(df['catch_rate'],df['speed'],)x=np.arange(256)y=slope*x+interceptdistantdataframecontextdf['weight_kg'].describe()df['color'].value_counts().plot(kind='bar')df['body_style'].value_counts().plot(kind='bar')grouped=df.groupby(['body_style','hasgender',]).mean()df.groupby('color')['attack'].mean()df.groupby('color')['pr_male'].mean()df.sort_values('catch_rate',ascending=false).head()dataframeschemadf:['catch_rate','speed','weight_kg','color','body_style']groundtruthplt.scatter(df['catch_rate'],df['speed'])plt.plot(x,y)002(a)naturallanguagecreateascatterplotoftheobservationsinthe‘credit’datasetfortheattributes‘duration’and‘age’(ageshouldbeshownonthexaxis).localcodecontextduration=credit['duration'].valuesage=credit['age'].valuesgroundtruthplt.scatter(age,duration)predictionplt.scatter(duration,age)003(b)naturallanguagethisgraphprovidesmoreevidencethatthehigherastate’sparticipationrates,thelowerthatstate’saveragesscoresarelikelytobe.thehighertheparticipationrate,thelowertheexpectedaverageverbalscores.localcodecontextplt.plot(sat_data['math'],sat_data['verbal'])dataframeschemasat:['rate','math','verbal']groundtruthplt.plot(sat_data['rate'],sat_data['math'])plt.plot(sat_data['rate'],sat_data['verbal'])predictionplt.plot(sat_data['math'],sat_data['verbal'])plt.plot(sat_data['rate'],sat_data['verbal'])004(a)naturallanguageplotagaussianbyloopingthrougharangeofxvaluesandcreatingaresultinglistofgaussianvalues,glocalcodecontextx_axis=np.arange(-20,20,0.1)g=[]forxinx_axis:g.append(f(mu,sigma2,x))groundtruth&predictionplt.plot(x_axis,g)005(b)naturallanguagelikeinq9,let’sstartbythinkingabouttwodicelocalcodecontextresults=[]foriinrange(1,7):forjinrange(1,7):print((i,j),max(i,j))results.append(max(i,j))groundtruth&predictionplt.hist(results)00611000001002003004005006007008009010011012013014015016017018019020021022023024025026027028029030031032033034035036037038039040041042043044045046047048049050051052053054055056057058059060061062063064065066067068069070071072073074075076077078079080081082083084085086087088089090091092093094095096097098099acl-ijcnlp2021submission***.conﬁdentialreviewcopy.donotdistribute.plotcoder:hierarchicaldecodingforsynthesizingvisualizationcodeinprogrammaticcontextanonymousacl-ijcnlpsubmissionabstractnaturallanguageexploretherelationshipbetweenrarityandaskillofyourchoice.chooseoneskill(‘attack’,‘defense’or‘speed’)anddothefollowing.usethescipypackagetoassesswhethercatchratepredictstheskill.createascatterplottovisualizehowtheskilldependsupontherarityofthepokemon.overlayabestﬁtlineontothescatterplot.localcodecontextslope,intercept,r_value,p_value,std_err=linregress(df['catch_rate'],df['speed'],)x=np.arange(256)y=slope*x+interceptdistantdataframecontextdf['weight_kg'].describe()df['color'].value_counts().plot(kind='bar')df['body_style'].value_counts().plot(kind='bar')grouped=df.groupby(['body_style','hasgender',]).mean()df.groupby('color')['attack'].mean()df.groupby('color')['pr_male'].mean()df.sort_values('catch_rate',ascending=false).head()dataframeschemadf:['catch_rate','speed','weight_kg','color','body_style']groundtruthplt.scatter(df['catch_rate'],df['speed'])plt.plot(x,y)(a)naturallanguageplotagaussianbyloopingthrougharangeofxvaluesandcreatingaresultinglistofgaussianvalues,glocalcodecontextx_axis=np.arange(-20,20,0.1)g=[]forxinx_axis:g.append(f(mu,sigma2,x))groundtruth&predictionplt.plot(x_axis,g)(b)naturallanguagelikeinq9,let’sstartbythinkingabouttwodicelocalcodecontextresults=[]foriinrange(1,7):forjinrange(1,7):print((i,j),max(i,j))results.append(max(i,j))groundtruth&predictionplt.hist(results)(a)naturallanguagecreateascatterplotoftheobservationsinthe‘credit’datasetfortheattributes‘duration’and‘age’(ageshouldbeshownonthexaxis).localcodecontextduration=credit['duration'].valuesage=credit['age'].valuesgroundtruthplt.scatter(age,duration)predictionplt.scatter(duration,age)(b)naturallanguagethisgraphprovidesmoreevidencethatthehigherastate’sparticipationrates,thelowerthatstate’saveragesscoresarelikelytobe.thehighertheparticipationrate,thelowertheexpectedaverageverbalscores.localcodecontextplt.plot(sat_data['math'],sat_data['verbal'])dataframeschemasat:['rate','math','verbal']groundtruthplt.plot(sat_data['rate'],sat_data['math'])plt.plot(sat_data['rate'],sat_data['verbal'])predictionplt.plot(sat_data['math'],sat_data['verbal'])plt.plot(sat_data['rate'],sat_data['verbal'])naturallanguageproblem5.agegroups(1point)createahistogramofallpeople’sages.usethedefaultsettings.addthelabel”age”onthex-axisand”count”onthey-axis.localcodecontextincome_data.columns=["age","workclass","fnlwgt","education","education_num","marital_status","occupation","relationship","race","sex","capital_gain","capital_loss","hours_per_week","native_country","income_class"]...married_af_peoples=\\income_data[income_data["marital_status"].str.contains("married-af-spouse")].shape[0]...dataframeschemaincome_data:['age','workclass',...,'income_class']married_af_peoples:['age','workclass',...,'income_class']groundtruthplt.hist(income_data.age)predictionplt.hist(married_af_peoples.age)has more samples per plot type than the gold splits.
compared to the non-hierarchical models, the mostsigniﬁcant improvement comes from the predic-tions of scatter plots (“s”) and plots in “others”category.
we posit that these two categories consti-tute the majority of the dataset, and the hierarchicalmodel learns to better categorize plot types from alarge number of training samples.
the accuracy ofthe hierarchical model on some categories is lowerthan the baseline’s, but the difference is not statisti-cally signiﬁcant since those categories only containa few examples.
a more detailed discussion isincluded in appendix c..model.
s.h.pie.
s+p.
kde.
others.
with code canonicalization.
77.17% 70.86% 61.54% 12.28% 29.69% 84.14%full model− hierarchy70.65% 68.00% 76.92% 15.79% 39.06% 71.20%− link73.55% 68.00% 69.23% 21.05% 35.94% 70.55%73.91% 71.43% 69.23% 21.05% 28.13% 73.79%lstm+ codebert 67.39% 66.29% 76.92% 21.05% 35.94% 77.02%+ roberta61.59% 62.29% 61.54% 10.53% 34.38% 80.58%.
without code canonicalization.
71.01% 74.29% 76.92% 12.28% 37.50% 65.05%full model− hierarchy75.00% 72.00% 61.54% 14.04% 31.25% 58.25%− link72.10% 60.57% 69.23% 22.81% 37.50% 63.75%lstm74.64% 74.29% 69.23% 19.30% 29.69% 65.70%+ codebert 71.01% 56.00% 46.15% 14.04% 35.94% 55.02%+ roberta73.91% 47.13% 46.15% 10.53% 29.69% 74.43%.
table 6: plot type accuracy on test (hard) per type..model.
all.
s.h.pie.
s+p.
kde.
others.
plotted data accuracy.
40.16% 42.39% 41.14% 61.54% 10.53% 21.88% 45.95%full model− hierarchy35.91% 35.87% 40.00% 69.23% 8.77% 21.88% 40.13%− link35.46% 36.96% 39.43% 53.85% 8.77% 14.06% 40.45%lstm29.87% 30.43% 33.14% 61.54% 8.77% 12.50% 33.66%+ codebert 38.14% 38.41% 39.43% 61.54% 8.77% 20.31% 44.98%+ roberta37.47% 39.13% 36.57% 69.23% 3.51% 17.19% 45.63%.
plotted data accuracy (permutation invariant).
full model− hierarchy− linklstm.
41.50% 44.57% 41.14% 61.54% 12.28% 21.88% 47.57%37.47% 38.04% 40.00% 69.23% 10.53% 21.88% 42.39%41.05% 40.58% 39.43% 53.85% 8.77% 15.62% 43.04%30.65% 31.88% 33.14% 61.54% 10.53% 12.50% 34.30%.
table 7: plotted data accuracy on test (hard) per type.
all models are trained with canonicalized target code..5.4.1 error analysis.
to better understand the challenges of our task, weconduct a qualitative error analysis and categorizethe main reasons of error predictions.
we investi-gate all error cases on test (gold) split for the fullhierarchical model, and present the results in ta-ble 8. we summarize the key observations below,and defer more discussion to appendix e.• around half of error cases are due to the ambigu-ity of the natural language description.
(1-3)• about 10% samples require longer code contextfor prediction, because the program selects theplotted data from distant code context that ex-ceeds the input length limit.
(4).
• besides understanding complex natural.
• sometimes the model generates semanticallysame but syntactically different programs fromthe ground truth, which can happen when twovariables or data frames contain the same data.
(5)lan-guage description, as shown in figure 3, an-other challenge is to understand the code con-text and reason about the data stored in dif-in figure 5,ferent variables.
for example,although both dataframes income data andmarried af peoples include the age column,the model must infer that married af peoplesis a subset of income data, thus it should selectincome data to plot the statistics of people fromall groups.
(6-7).
error category.
(1) nl only suggests the plot type(2) nl only suggests the plotted data(3) nl has no plotting information(4) need more code context(5) semantically correct(6) challenging nl understanding(7) challenging code context understanding.
%.
28.579.529.529.5214.2919.059.52.table 8: error analysis on test (gold) with the hierar-chical model..6 conclusion.
in this paper, we conduct the ﬁrst study of visual-ization code synthesis from natural language andprogrammatic context.
we describe plotcoder,a model architecture that includes an encoder thatlinks the natural language description and codecontext, and a hierarchical program decoder thatsynthesizes plotted data from the code context anddataframe items.
results on real-world jupyternotebooks show that plotcoder can synthesizevisualization code for different plot types, and out-performs various baseline models..acknowledgments.
this material is in part based upon work sup-ported by the national science foundation un-der grant no.
twc-1409915, iis-1546083, iis-1955488, iis-2027575, ccf-1723352, doe awardde-sc0016260, darpa d3m under grant no.
fa8750-17-2-0091; berkeley deepdrive, the intel-nsf capa center, and gifts from adobe, face-book, google, and vmware.
xinyun chen is sup-ported by the facebook fellowship..2177references.
rajas agashe, srinivasan iyer, and luke zettlemoyer.
2019.juice: a large scale distantly superviseddataset for open domain context-based code gener-in proceedings of the 2019 conference onation.
empirical methods in natural language processingand the 9th international joint conference on natu-ral language processing (emnlp-ijcnlp), pages5439–5449..dzmitry bahdanau, kyunghyun cho, and yoshua ben-gio.
2015. neural machine translation by jointlylearning to align and translate.
in iclr..michael bostock, vadim ogievetsky, and jeffrey heer.
2011. d3 data-driven documents.
ieee trans-actions on visualization and computer graphics,17(12):2301–2309..jacob devlin, ming-wei chang, kenton lee, andkristina toutanova.
2018. bert: pre-training of deepbidirectional transformers for language understand-ing.
arxiv preprint arxiv:1810.04805..victor dibia and c¸ a˘gatay demiralp.
2019. data2vis:automatic generation of data visualizations us-ing sequence-to-sequence recurrent neural net-ieee computer graphics and applications,works.
39(5):33–46..li dong and mirella lapata.
2018. coarse-to-ﬁne de-coding for neural semantic parsing.
in proceedingsof the 56th annual meeting of the association forcomputational linguistics (volume 1: long papers),pages 731–742..ian drosos, titus barik, philip j. guo, robert de-line, and sumit gulwani.
2020. wrex: a uni-ﬁed programming-by-example interaction for syn-thesizing readable code for data scientists.
in chi’20: chi conference on human factors in comput-ing systems, honolulu, hi, usa, april 25-30, 2020,pages 1–12.
acm..zhangyin feng, daya guo, duyu tang, nan duan, xi-aocheng feng, ming gong, linjun shou, bing qin,ting liu, daxin jiang, et al.
2020. codebert: apre-trained model for programming and natural lan-guages.
arxiv preprint arxiv:2002.08155..jiatao gu, zhengdong lu, hang li, and victor okincorporating copying mechanism inli.
2016.in proceedings ofsequence-to-sequence learning.
the 54th annual meeting of the association for com-putational linguistics (volume 1: long papers),pages 1631–1640..jiaqi guo, zecheng zhan, yan gao, yan xiao,jian-guang lou, ting liu, and dongmei zhang.
2019. towards complex text-to-sql in cross-domainin pro-database with intermediate representation.
ceedings of the 57th annual meeting of the asso-ciation for computational linguistics, pages 4524–4535..sepp hochreiter and j¨urgen schmidhuber.
1997.neural computation,.
long short-term memory.
9(8):1735–1780..john d hunter.
2007. matplotlib: a 2d graphics en-vironment.
computing in science & engineering,9(3):90–95..srinivasan iyer, ioannis konstas, alvin cheung, jayantkrishnamurthy, and luke zettlemoyer.
2017. learn-ing a neural semantic parser from user feedback.
inproceedings of the 55th annual meeting of the as-sociation for computational linguistics (volume 1:long papers), pages 963–973..srinivasan iyer, ioannis konstas, alvin cheung, andluke zettlemoyer.
2018. mapping language to codein proceedings of thein programmatic context.
2018 conference on empirical methods in naturallanguage processing, pages 1643–1652..jian li, yue wang, michael r lyu, and irwin king.
2018. code completion with neural attention andpointer networks.
in proceedings of the 27th inter-national joint conference on artiﬁcial intelligence,pages 4159–25..xi victoria lin, chenglong wang, luke zettlemoyer,and michael d ernst.
2018. nl2bash: a corpusand semantic parser for natural language interfaceto the linux operating system.
in proceedings of theeleventh international conference on language re-sources and evaluation (lrec 2018)..yinhan liu, myle ott, naman goyal, jingfei du,mandar joshi, danqi chen, omer levy, mikelewis, luke zettlemoyer, and veselin stoyanov.
2019. roberta: a robustly optimized bert pre-training approach.
arxiv:1907.11692 [cs].
arxiv:1907.11692..matplotlib.
2020. matplotlib scatter method doc-umentation.
https://matplotlib.org/3.3.3/api/as gen/matplotlib.pyplot.scatter.html..vijayaraghavan murali, letao qi, swarat chaudhuri,and chris jermaine.
2018. neural sketch learningfor conditional program generation.
in internationalconference on learning representations..maxwell nye, luke hewitt, joshua tenenbaum, andarmando solar-lezama.
2019. learning to inferin international conference onprogram sketches.
machine learning, pages 4861–4870..yusuke oda, hiroyuki fudaba, graham neubig,hideaki hata, sakriani sakti, tomoki toda, andsatoshi nakamura.
2015.learning to generatepseudo-code from source code using statistical ma-in 2015 30th ieee/acm in-chine translation (t).
ternational conference on automated software en-gineering (ase), pages 574–584.
ieee..pandas.
2020..pandas dataframe documentation..https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.dataframe.html..2178in 2018 ieee/acm 15th interna-stack overﬂow.
tional conference on mining software repositories(msr), pages 476–486.
ieee..tao yu, rui zhang, heyang er, suyi li, eric xue,bo pang, xi victoria lin, yi chern tan, tianze shi,zihan li, et al.
2019a.
cosql: a conversationaltext-to-sql challenge towards cross-domain naturallanguage interfaces to databases.
in proceedings ofthe 2019 conference on empirical methods in nat-ural language processing and the 9th internationaljoint conference on natural language processing(emnlp-ijcnlp), pages 1962–1979..tao yu, rui zhang, kai yang, michihiro yasunaga,dongxu wang, zifan li, james ma, irene li, qingn-ing yao, shanelle roman, et al.
2018. spider: alarge-scale human-labeled dataset for complex andcross-domain semantic parsing and text-to-sql task.
in proceedings of the 2018 conference on empiri-cal methods in natural language processing, pages3911–3921..tao yu, rui zhang, michihiro yasunaga, yi cherntan, xi victoria lin, suyi li, heyang er, irene li,bo pang, tao chen, et al.
2019b.
sparc: cross-domain semantic parsing in context.
in proceedingsof the 57th annual meeting of the association forcomputational linguistics, pages 4511–4523..maksym zavershynskyi, alex skidanov, and illia polo-sukhin.
2018. naps: natural program synthesisdataset.
arxiv preprint arxiv:1807.03168..luke s zettlemoyer and michael collins.
2012. learn-ing to map sentences to logical form: structuredclassiﬁcation with probabilistic categorial grammars.
arxiv preprint arxiv:1207.1420..victor zhong, caiming xiong, and richard socher.
2017.seq2sql: generating structured queriesfrom natural language using reinforcement learning.
arxiv preprint arxiv:1709.00103..illia polosukhin and alexander skidanov.
2018. neu-ral program search: solving programming tasksarxiv preprintfrom description and examples.
arxiv:1802.04335..veselin raychev, martin vechev, and eran yahav.
2014.code completion with statistical language models.
in proceedings of the 35th acm sigplan confer-ence on programming language design and imple-mentation, pages 419–428..seaborn.
2020. mwaskom/seaborn library documenta-tion.
https://doi.org/10.5281/zenodo.592845..armando solar-lezama.
2008. program synthesis by.
sketching.
ph.d. thesis, uc berkeley..alexey svyatkovskiy, shao kun deng, shengyu fu,and neel sundaresan.
2020.intellicode compose:code generation using transformer.
arxiv preprintarxiv:2005.08025..ashish vaswani, noam shazeer, niki parmar, jakobuszkoreit, llion jones, aidan n gomez, łukaszkaiser, and illia polosukhin.
2017. attention is allin advances in neural information pro-you need.
cessing systems, pages 5998–6008..oriol vinyals, meire fortunato, and navdeep jaitly.
2015. pointer networks.
in advances in neural in-formation processing systems, pages 2692–2700..bailin wang, richard shin, xiaodong liu, olek-sandr polozov, and matthew richardson.
2019a.
rat-sql: relation-aware schema encoding andarxiv preprintlinking for text-to-sql parsers.
arxiv:1911.04942..chenglong wang, alvin cheung, and rastislav bod´ık.
2017. synthesizing highly expressive sql queriesfrom input-output examples.
in proceedings of the38th acm sigplan conference on programminglanguage design and implementation, pldi, pages452–466.
acm..chenglong wang, yu feng, rastislav bodik, alvincheung, and isil dillig.
2019b.
visualization by ex-ample.
proceedings of the acm on programminglanguages, 4(popl):1–28..chenglong wang, yu feng, rastislav bod´ık, isil dil-lig, alvin cheung, and amy j. ko.
2021. falx:synthesis-powered visualization authoring.
in chi’21: chi conference on human factors in comput-ing systems, pages 106:1–106:15. acm..yushi wang, jonathan berant, and percy liang.
2015.in proceed-building a semantic parser overnight.
ings of the 53rd annual meeting of the associationfor computational linguistics and the 7th interna-tional joint conference on natural language pro-cessing (volume 1: long papers), pages 1332–1342..pengcheng yin, bowen deng, edgar chen, bogdanvasilescu, and graham neubig.
2018. learning tomine aligned code and natural language pairs from.
2179a implementation details.
for the model input, we select the sufﬁx of the codesequence when it exceeds the length limit, and weselect the preﬁx for the natural language.
to con-struct the vocabularies, we include natural languagewords that occur at least 15 times in the trainingset, and code tokens that occur at least 1,000 times,so that each vocabulary includes around 10, 000tokens.
we include an [unk] token in both vocabu-laries, which is used to encode all input tokens notappeared in our vocabularies..the model parameters are randomly initializedwithin [−0.1, 0.1].
each lstm has 2 layers, anda hidden size of 512. the embedding size of allembedding matrices is 512, and the hidden sizeof the linear layers is 512. for training, the batchsize is 32, the initial learning rate is 1e-3, with adecay rate of 0.9 after every 6, 000 batch updates.
the dropout rate is 0.2, and the norm for gradientclipping is 5.0..for models using the transformer architecture asthe encoder, we use the pre-trained roberta-baseand codebert from their ofﬁcial repositories.2the hyper-parameters are largely the same as thelstm-based models, except that we added a linearlearning rate warmup for the ﬁrst 6, 000 trainingsteps, which is the common practice for ﬁne-tuningbert-like models..b training with varying number of.
contextual code cells.
as discussed in section 5.3.1, we provide the re-sults of including different number of local codecells as the model input in figure 6. we also eval-uated the upper bounds of program accuracies fordifferent values of k, where we consider an exam-ple to be predictable if all plotted data in the tar-get program are covered in the input code context.
we observe that including dataframe manipulationcode in distant code cells improves the coverage,especially when k is small..c detailed analysis on results per plot.
type.
in section 5.4, we present the breakdown resultsper plot type in tables 6 and 7, where we observedthat “scatter” and “others” constitute the majority.
2roberta: https://github.com/pytorch/fairseq/.
tree/master/examples/robertacodebert: https://github.com/microsoft/codebert.
(a).
(b).
figure 6: program accuracy with different number ofinput code cells.
(a) results of different model archi-tectures.
(b) the comparison between the accuracy ofthe hierarchical model and the upper bounds..of the dataset, and the hierarchical model learns tobetter categorize plot types from a large number oftraining samples..note that for categories that the hierarchicalmodel does not perform better than baselines, evenif the accuracy differences are noticeable, the num-bers of correct predictions do not differ much.
forexample, among the 13 samples in the “pie” cate-gory, the hierarchical model correctly classiﬁes 8samples, while the non-hierarchical version makes10 correct predictions.
when looking at the predic-tions, we observe that the 2 different predictions aremainly due to the ambiguity of the natural languagedescriptions.
speciﬁcally, the text descriptions are“the average score of group a is better than aver-age score of group b in 51% of the state” and “iam analyzing the data of all male passengers”.
infact, for both examples, the hierarchical model stillgenerates a program including the plotted data inthe ground truth.
however, the hierarchical modelwrongly selects plt.bar as the plotting api forthe former sample, and selects plt.scatter forthe latter sample, where it additionally selects an-other variable for the x-axis.
for these 2 samples,we observe that the code context includes plottingprograms that use other data to generate pie charts,.
2180we consider several directions to address differ-ent error categories as future work.
to mitigatethe ambiguity of natural language descriptions, wecould incorporate additional program speciﬁcationssuch as input-output examples.
input-output exam-ples are also helpful for evaluating the executionaccuracy, which considers all semantically correctprograms as correct predictions even if they differfrom the ground truth.
most jupyter notebooksfrom github do not contain sufﬁcient executioninformation, e.g., many of them load external datafor plotting, and the data sources are not public.
therefore, developing techniques to automaticallysynthesize input-output examples is a promisingfuture direction.
designing new models for coderepresentation learning is another future direction,which could help address the challenge of embed-ding long code context..and the non-hierarchical model picks a heuristic toselect the same plot type in the code context whenthere is no cue provided in the natural language de-scription, while the hierarchical model selects plottypes that happen more frequently in the trainingdistribution.
a similar phenomenon holds for othercategories or data splits with a small number ofexamples..d other plot types.
in the “others” category discussed in section 3.3,besides the plots generated by plt.plot, there arealso other plot types, with much smaller data sizesthan plt.plot.
in table 9, we present the break-down accuracies of some plot types, which consti-tute the largest percentages in the “others” categoryexcluding plt.plot samples.
speciﬁcally, around4% samples use boxplot, and each of the other3 plot types include around 1% samples.
due tothe lack of data for such plot types, the results aremuch lower than the overall accuracies of all plotcategories, but still non-trivial..plot type plot type acc plotted data acc program acc.
boxplotpairplotjointplotviolinplot.
51.04%42.31%36.36%47.06%.
10.42%34.62%9.09%5.88%.
7.29%23.00%4.55%5.88%.
table 9: breakdown accuracies of plots in “others” cat-egory on test (hard), using the full hierarchical model..e more discussion of error analysis.
as discussed in section 5.4.1, the lack of informa-tion in natural language descriptions is the mainreason for a large proportion of wrong predictions(categories 1-3 in table 8).
• many natural language descriptions only mentionthe plot type, e.g., “make a scatter plot,” which isone reason that the plot type accuracy is generallymuch higher than the plotted data accuracy.
(1)• sometimes the text only mentions the plot-ted data without specifying the plottype,e.g., “plot the data x1 and x2,” where bothplt.plot(x1,x2) and plt.scatter(x1,x2)are possible predictions, and the model needsto infer the plot type from the code context.
(2)• the text description includes no plotting informa-tion at all, e.g., “localize your search around thevalue you found above,” where the model needsto infer which variables are search results andcould be plotted.
(3).
2181