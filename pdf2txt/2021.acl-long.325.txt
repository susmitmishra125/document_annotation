how is bert surprised?
layerwise detection of linguistic anomalies.
bai li1,4, zining zhu1,4 guillaume thomas2, yang xu1,3,4, frank rudzicz1,4,51 university of toronto, department of computer science2 university of toronto, department of linguistics3 university of toronto, cognitive science program4 vector institute for artiﬁcial intelligence 5 unity health toronto{bai, zining, yangxu, frank}@cs.toronto.eduguillaume.thomas@utoronto.ca.
abstract.
transformer language models have shown re-markable ability in detecting when a word isanomalous in context, but likelihood scoresoffer no information about the cause of theanomaly.
in this work, we use gaussianmodels for density estimation at intermedi-ate layers of three language models (bert,roberta, and xlnet), and evaluate ourmethod on blimp, a grammaticality judge-ment benchmark.
in lower layers, surprisalis highly correlated to low token frequency,but this correlation diminishes in upper layers.
next, we gather datasets of morphosyntactic,semantic, and commonsense anomalies frompsycholinguistic studies; we ﬁnd that the bestperforming model roberta exhibits surprisalin earlier layers when the anomaly is mor-phosyntactic than when it is semantic, whilecommonsense anomalies do not exhibit sur-prisal at any intermediate layer.
these resultssuggest that language models employ separatemechanisms to detect different types of lin-guistic anomalies..1.introduction.
transformer-based language models (lms) haveachieved remarkable success in numerous naturallanguage processing tasks, prompting many prob-ing studies to determine the extent of their linguis-tic knowledge.
a popular approach is to formulatethe problem as a multiple-choice task, where thelm is considered correct if it assigns higher like-lihood to the appropriate word than an inappro-priate one, given context (gulordava et al., 2018;ettinger, 2020; warstadt et al., 2020).
the like-lihood score, however, only gives a scalar valueof the degree that a word is anomalous in context,and cannot distinguish between different ways thata word might be anomalous..it has been proposed that.
ent.
types of linguistic anomalies..there are differ-chomsky.
figure 1: example sentence with a morphosyntacticanomaly (left) and semantic anomaly (right) (anoma-lies in bold).
darker colours indicate higher surprisal.
we investigate several patterns: ﬁrst, surprisal at lowerlayers corresponds to infrequent tokens, but this effectdiminishes towards upper layers.
second, morphosyn-tactic violations begin to trigger high surprisals at anearlier layer than semantic violations..(1957) distinguished semantic anomalies (“color-less green ideas sleep furiously”) from ungram-maticality (“furiously sleep ideas green color-less”).
psycholinguistic studies initially suggestedthat different event-related potentials (erps) areproduced in the brain depending on the type ofanomaly; e.g., semantic anomalies produce nega-tive erps 400 ms after the stimulus, while syn-tactic anomalies produce positive erps 600 msafter (kutas et al., 2006).
here, we ask whethertransformer lms show different surprisals intheir intermediate layers depending on the type ofanomaly.
however, lms do not compute likeli-hoods at intermediate layers – only at the ﬁnallayer..proceedingsofthe59thannualmeetingoftheassociationforcomputationallinguisticsandthe11thinternationaljointconferenceonnaturallanguageprocessing,pages4215–4228august1–6,2021.©2021associationforcomputationallinguistics4215thecatwon'teatingthefood0123456789101112layertheplanelaughedattherunway0123456789101112in this paper, we introduce a new tool to probefor surprisal at intermediate layers of bert (de-vlin et al., 2019), roberta (liu et al., 2019), andxlnet (yang et al., 2019), formulating the prob-lem as density estimation.
we train gaussian mod-els to ﬁt distributions of embeddings at each layerof the lms.
using blimp (warstadt et al., 2020)for evaluation, we show that this model is effec-tive at grammaticality judgement, requiring only asmall amount of in-domain text for training.
fig-ure 1 shows the method using the roberta modelon two example sentences..we apply our model to test sentences drawnfrom blimp and 7 psycholinguistics studies, ex-hibiting morphosyntactic, semantic, and common-sense anomalies.
we ﬁnd that morphosyntacticanomalies produce out-of-domain embeddings atearlier layers, semantic anomalies at later lay-ers, and no commonsense anomalies, even thoughthe lm’s ﬁnal accuracy is similar.
we showthat lms are internally sensitive to the type oflinguistic anomaly, which is not apparent if weonly had access to their softmax probability out-puts.
our source code and data are availableat: https://github.com/spoclab-ca/layerwise-anomaly..2 related work.
mantic roles.
miaschi et al.
(2020) examined thelayerwise performance of bert for a suite of lin-guistic features, before and after ﬁne tuning.
ourwork further investigates what linguistic informa-tion is contained in different layers, with a focuson anomalous inputs..2.2 neural grammaticality judgements.
many recent probing studies used grammatical-ity judgement tasks to test the knowledge of spe-ciﬁc phenomena in lms.
warstadt et al.
(2019)gathered sentences from linguistic publications,and evaluated by matthews correlation with theground truth.
more commonly, the model is pre-sented with a binary choice between an accept-able and unacceptable sentence: blimp (warstadtet al., 2020) used templates to generate 67k suchsentence pairs, covering 12 types of linguistic phe-nomena.
similarly, hu et al.
(2020) created syn-tactic tests using templates, but deﬁned successcriteria using inequalities of lm perplexities..in contrast with artiﬁcial templates, gulordavaet al.
(2018) generated test cases by perturbingnatural corpus data to test long-distance depen-dencies.
most grammaticality studies focused onsyntactic phenomena, but rabinovich et al.
(2019)tested lms’ sensitivity to semantic infelicities in-volving indeﬁnite pronouns..2.1 probing lms for linguistic knowledge.
2.3 tests of selectional restrictions.
soon after bert’s release, many papers inventedprobing techniques to discover whatlinguisticknowledge it contains, and how this informationis distributed between layers (e.g., rogers et al.
(2021) provides a comprehensive overview).
ten-ney et al.
(2019) used “edge probing” to determineeach layer’s contribution to a task’s performance,and discovered that the middle layers contributedmore when the task was syntactic, and the upperlayers more when the task was semantic..several papers found that bert’s middle layerscontain the most syntactic information.
kelly et al.
(2020) found that bert’s middle layers are best atdistinguishing between sentences with direct andindirect object constructions.
hewitt and manning(2019) used a structural probe to recover syntaxtrees from contextual embeddings, and found theperformance peaked in middle layers..probing results are somewhat dependent on thechoice of linguistic formalism used to annotate thedata, as kulmizev et al.
(2020) found for syntax,and kuznetsov and gurevych (2020) found for se-.
violations of selectional restrictions are one typeof linguistic unacceptability, deﬁned as a seman-tic mismatch between a verb and an argument.
sasano and korhonen (2020) examined the ge-ometry of word classes (e.g., words that can bea direct object of the verb ‘play’) in word vec-tor models;they compared single-class modelsagainst discriminative models for learning wordclass boundaries.
chersoni et al.
(2018) testeddistributional semantic models on their abilityto identify selectional restriction violations usingstimuli from two psycholinguistic datasets.
fi-nally, metheniti et al.
(2020) tested how muchbert relies on selectional restriction informationversus other contextual information for makingmasked word predictions..2.4 psycholinguistic tests for lms.
the n400 response is a negative event-related po-tential that occurs roughly 400ms after a stimulusin human brains, and is generally associated withthe stimulus being semantically anomalous with.
4216respect to the preceding context (kutas and fed-ermeier, 2011).
although many studies have beenperformed with a diverse range of linguistic stim-uli, exactly what conditions trigger the n400 re-sponse is still an open question.
frank et al.
(2015)found that the n400 response is correlated withsurprisal, i.e., how unlikely an lm predicts a wordgiven the preceding context..recently, several studies have investigated re-lationships between surprisal in neural lms andthe n400 response.
michaelov and bergen(2020) compared human n400 amplitudes withlstm-based models using stimuli from severalpsycholinguistic studies.
ettinger (2020) useddata from three psycholinguistic studies to probebert’s knowledge of commonsense and nega-tion.
our work is similar to the latter – we lever-age psycholinguistic studies for their stimuli, butwe do not use the their n400 amplitude results..3 model.
we use the transformer language model as a con-textual embedding extractor (we write this asbert for convenience).
let l be the layer index,which ranges from 0 to 12 on all of our models.
using a training corpus {w1, · · · , wt }, we extractcontextual embeddings at layer l for each token:.
x(l)1., · · · , x(l).
t = bertl(w1, · · · , wt )..(1).
next, we ﬁt a multivariate gaussian on the ex-tracted embeddings:.
x(l)1., · · · , x(l).
µl,.
t ∼ n (for evaluating the layerwise surprisal of a newsentence s = [t1, · · · , tn], we similarly extractcontextual embeddings using the language model:.
(2).
b.b.σl)..y1, · · · , yn = bertl(t1, · · · , tn).
the surprisal of each token is the negative log like-lihood of the contextual vector according to themultivariate gaussian:.
(3).
gi = − log p(yi |.
µl,.
σl).
for i = 1 .
.
.
n..(4)finally, we deﬁne the surprisal of sentence s as thesum of surprisals of all of its tokens, which is alsothe joint log likelihood of all of the embeddings:.
b.b.surprisall(t1, · · · , tn) =.
gi.
n.xi=1.
= − log p(y1, · · · , yn |.
µl,.
σl)..(5).
b.b.
3.1 connection to mahalanobis distance.
the theoretical motivation for using the sum of loglikelihoods is that when we ﬁt a gaussian modelwith full covariance matrix, low likelihood corre-sponds exactly to high mahalanobis distance fromthe in-distribution points.
the score given by thegaussian model is:.
g = − log p(y |.
µl,.
σl).
= − log.
1b(2π)d/2|.
.
bσl|1/2.
exp(−.
d2).
,.
(6).
!.
where d is the dimension of the vector space, andd is the mahalanobis distance:.
b.d =.
(y −.
µl)t.σ.
−1l (y −.
µl)..(7).
q.rearranging, we get:.
b.b.
12.b.d2 = 2g − d log(2π) − log |.
σl|,.
(8).
thus the negative log likelihood is the squared ma-halanobis distance plus a constant..b.various methods based on mahalanobis dis-tance have been used for anomaly detection inneural networks; for example, lee et al.
(2018)proposed a similar method for out-of-domain de-tection in neural classiﬁcation models, and caoet al.
(2020) found the mahalanobis distancemethod to be competitive with more sophisticatedmethods on medical out-of-domain detection.
intransformer models, podolskiy et al.
(2021) usedmahalanobis distance for out-of-domain detec-tion, outperforming methods based on softmaxprobability and likelihood ratios..gaussian assumptions.
our model assumesthat the embeddings at every layer follow a multi-variate gaussian distribution.
since the gaussiandistribution is the maximum entropy distributiongiven a mean and covariance matrix, it makes thefewest assumptions and is therefore a reasonabledefault.
hennigen et al.
(2020) found that embed-dings sometimes do not follow a gaussian distri-bution, but it is unclear what alternative distribu-tion would be a better ﬁt, so we will assume agaussian distribution in this work..3.2 training and evaluation.
for all of our experiments, we use the ‘base’ ver-sions of pretrained language models bert (de-vlin et al., 2019), roberta (liu et al., 2019), and.
4217ycarucca.
0.6.
0.4.
1.0.
0.8.
0.2.
0.0.ycarucca.
0.8.
0.7.
0.6.
10.
20.
50.
100.
200.
500.
1000 2000.
5000 10000.
0.
1.
2.
3.
4.
5.
7.
8.
9.
10.
11.
12.bert.
roberta.
xlnet.
training sentences.
(a).
bert.
roberta.
xlnet.
6layer.
(b).
figure 2: blimp accuracy different amounts of training data and across layers, for three lms.
about 1000sentences are needed before a plateau is reached (mean tokens per sentence = 15.1)..xlnet (yang et al., 2019), provided by hugging-face (wolf et al., 2020).
each of these modelshave 12 contextual layers plus a 0th static layer,and each layer is 768-dimensional..we train the gaussian model on randomly se-lected sentences from the british national cor-pus (leech, 1992), representative of acceptableenglish text from various genres.
we evaluateon blimp (warstadt et al., 2020), a dataset of67k minimal sentence pairs that test acceptabilityjudgements across a variety of syntactic and se-mantic phenomena.
in our case, a sentence pair isconsidered correct if the sentence-level surprisalof the unacceptable sentence is higher than that ofthe acceptable sentence..how much training data is needed?
we ex-periment with training data sizes ranging from 10to 10,000 sentences (figure 2a).
compared tothe massive amount of data needed for pretrain-ing the lms, we ﬁnd that a modest corpus suf-ﬁces for training the gaussian anomaly model, anda plateau is reached after 1000 sentences for allthree models.
therefore, we use 1000 trainingsentences (unless otherwise noted) for all subse-quent experiments in this paper..which layers are sensitive to anomaly?
wevary l from 0 to 12 in all three models (figure2b).
the layer with the highest accuracy differsbetween models: layer 9 has the highest accuracyfor bert, 11 for roberta, and 6 for xlnet.
allmodels experience a sharp drop in the last layer,likely because the last layer is specialized for themlm pretraining objective..comparisons to other models.
our best-performing model is roberta, with an accuracyof 0.830. this is slightly higher the best model re-ported in blimp (gpt-2, with accuracy 0.801)..we do not claim to beat the state-of-the-art onblimp: salazar et al.
(2020) obtains a higheraccuracy of 0.865 using roberta-large.
eventhough the main goal of this paper is not to max-imize accuracy on blimp, our gaussian anomalymodel is competitive with other transformer-basedmodels on this task..in appendix a, we explore variations of thegaussian anomaly model, such as varying thetype of covariance matrix, gaussian mixture mod-els, and one-class svms (sch¨olkopf et al., 2000).
however, none of these variants offer a signiﬁcantimprovement over a single gaussian model withfull covariance matrix..3.3 lower layers are sensitive to frequency.
we notice that surprisal scores in the lower layersare sensitive to token frequency: higher frequencytokens produce embeddings close to the center ofthe gaussian distribution, while lower frequencytokens are at the periphery.
the effect graduallydiminishes towards the upper layers..to quantify the sensitivity to frequency, wecompute token-level surprisal scores for 5000 sen-tences from bnc that were not used in training.
we then compute the pearson correlation betweenthe surprisal score and log frequency for each to-ken (figure 3).
in all three models, there is a highcorrelation between the surprisal score and log fre-quency at the lower layers, which diminishes at theupper layers.
a small positive correlation persistsuntil the last layer, except for xlnet, in which thecorrelation eventually disappears..there does not appear to be any reports of thisphenomenon in previous work.
for static wordvectors, gong et al.
(2018) found that embeddingsfor low-frequency words lie in a different region of.
42181.0.
0.8.
0.6.
0.4.
0.2.
0.0.l.noitaerroc nosraep.bert.
roberta.
xlnet.
0.
1.
2.
3.
4.
5.
7.
8.
9.
10.
11.
12.
6layer.
figure 3: pearson correlation between token-level sur-prisal scores (equation 4) and log frequency.
the cor-relation is highest in the lower layers, and decreases inthe upper layers..the embedding space than high-frequency words.
we ﬁnd evidence that the same phenomenon oc-curs in contextual embeddings (appendix b).
inthis scenario, the gaussian model ﬁts the high-frequency region and assigns lower likelihoods tothe low-frequency region, explaining the positivecorrelation at all layers; however, it is still unclearwhy the correlation diminishes at upper layers..4 levels of linguistic anomalies.
we turn to the question of whether lms exhibitdifferent behaviour when given inputs with dif-ferent types of linguistic anomalies.
the task ofpartitioning linguistic anomalies into several dis-tinct classes can be challenging.
syntax and se-mantics have a high degree of overlap – thereis no widely accepted criterion for distinguishingbetween ungrammaticality and semantic anomaly(e.g., abrus´an (2019) gives a survey of currentproposals), and poulsen (2012) challenges this di-chotomy entirely.
similarly, warren et al.
(2015)noted that semantic anomalies depend somewhaton world knowledge..within a class, the anomalies are also heteroge-neous (e.g., ungrammaticality may be due to vio-lations of agreement, wh-movement, negative po-larity item licensing, etc), which might each affectthe lms differently.
thus, we deﬁne three classesof anomalies that do not attempt to cover all pos-sible linguistic phenomena, but captures differentlevels of language processing while retaining in-ternal uniformity:.
1. morphosyntactic anomaly:.
an error inthe inﬂected form of a word,for exam-ple, subject-verb agreement (*the boy eat the.
sandwich), or incorrect verb tense or aspectinﬂection (*the boy eaten the sandwich).
ineach case, the sentence can be corrected bychanging the inﬂectional form of one word..2. semantic anomaly:.
a violation of a se-lectional restriction, such as animacy (#thehouse eats the sandwich).
in these cases, thesentence can be corrected by replacing one ofthe verb’s arguments with another one in thesame word class that satisﬁes the verb’s se-lectional restrictions..3. commonsense anomaly: sentence describesan situation that is atypical or implausiblein the real world but is otherwise acceptable(#the customer served the waitress)..4.1 summary of anomaly datasets.
we use two sources of data for experiments on lin-guistic anomalies: synthetic sentences generatedfrom templates, and materials from psycholinguis-tic studies.
both have advantages and disadvan-tages – synthetic data can be easily generated inlarge quantities, but the resulting sentences maybe odd in unintended ways.
psycholinguistic stim-uli are designed to control for confounding fac-tors (e.g., word frequency) and human-validatedfor acceptability, but are smaller (typically fewerthan 100 sentence pairs)..we curate a set of 12 tasks from blimp and 7psycholinguistic studies1.
each sentence pair con-sists of a control and an anomalous sentence, sothat all sentences within a task differ in a consis-tent manner.
table 1 shows an example sentencepair from each task.
we summarize each dataset:.
1. blimp (warstadt et al., 2020): we usesubject-verb and determiner-noun agreementtests as morphosyntactic anomaly tasks.
forsimplicity, we only use the basic regular sen-tences, and exclude sentences involving ir-regular words or distractor items.
we alsouse the two argument structure tests involv-ing animacy as a semantic anomaly task.
allthree blimp tasks therefore have 2000 sen-tence pairs..1several of these stimuli have been used in natural lan-guage processing research.
chersoni et al.
(2018) used thedata from pylkk¨anen and mcelree (2007) and warren et al.
(2015) to probe word vectors for knowledge of selectionalrestrictions.
ettinger (2020) used data from federmeier andkutas (1999) and chow et al.
(2016), which were referred toas cprag-102 and role-88 respectively..4219type.
taskblimp (subject-verb).
correct examplethese casseroles disgust kayla..incorrect examplethese casseroles disgusts kayla..morphosyntax.
blimp (det-noun).
craig explored that grocery store..craig explored that grocery stores..osterhout and nicol(1999).
blimp (animacy).
the cats won’t eat the food thatmary gives them.
amanda was respected by somewaitresses..the cats won’t eating the food thatmary gives them.
amanda was respected by somepicture..pylkk¨anen and mcelree(2007).
the pilot ﬂew the airplane afterthe intense class..the pilot amazed the airplane afterthe intense class..semantic.
warren et al.
(2015).
corey’s hamster explored a nearbybackpack and ﬁlled it with sawdust..corey’s hamster entertained a nearbybackpack and ﬁlled it with sawdust..osterhout and nicol(1999).
osterhout and mobley(1995).
warren et al.
(2015).
federmeier and kutas(1999).
chow et al.
(2016).
the cats won’t eat the food thatmary gives them..the cats won’t bake the food thatmary gives them..the plane sailed through the air andlanded on the runway.
corey’s hamster explored a nearbybackpack and ﬁlled it with sawdust..the plane sailed through the air andlaughed on the runway.
corey’s hamster lifted a nearbybackpack and ﬁlled it with sawdust..“checkmate,” rosalie announcedwith glee.
she was getting to bereally good at chess.
the restaurant owner forgot whichcustomer the waitress had served..“checkmate,” rosalie announcedwith glee.
she was getting to bereally good at monopoly.
the restaurant owner forgot whichwaitress the customer had served..urbach and kutas (2010).
prosecutors accuse defendants ofcommitting a crime..prosecutors accuse sheriffs ofcommitting a crime..commonsense.
table 1: example sentence pair for each of the 12 tasks.
the 3 blimp tasks are generated from templates; theothers are stimuli materials taken from psycholinguistic studies..2. osterhout and nicol (1999): contains 90 sen-tence triplets containing a control, syntactic,and semantic anomaly.
syntactic anomaliesinvolve a modal verb followed by a verb in-ing form; semantic anomalies have a selec-tional restriction violation between the sub-ject and verb.
there are also double anoma-lies (simultaneously syntactic and semantic)which we do not use..3. pylkk¨anen and mcelree (2007): contains 70sentence pairs where the verb is replacedin the anomalous sentence with one that re-quires an animate object, thus violating theselectional restriction.
in half the sentences,the verb is contained in an embedded clause..4. warren et al.
(2015): contains 30 sentencetriplets with a possible condition, a selec-tional restriction violation between the sub-ject and verb, and an impossible conditionwhere the subject cannot carry out the action,i.e., a commonsense anomaly..5. osterhout and mobley (1995): we use datafrom experiment 2, containing 90 sentencepairs where the verb in the anomalous sen-tence is semantically inappropriate.
the ex-.
periment also tested gender agreement errors,but we do not include these stimuli..6. federmeier and kutas (1999): contains 34sentence pairs, where the ﬁnal noun in eachanomalous sentence is an inappropriate com-pletion, but in the same semantic category asthe expected completion..7. chow et al.
(2016): contains 44 sentencepairs, where two of the nouns in the anoma-lous sentence are swapped to reverse theirroles.
this is the only task in which the sen-tence pair differs by more than one token..8. urbach and kutas (2010): contains 120 sen-tence pairs, where the anomalous sentence re-places a patient of the verb with an atypicalone..4.2 quantifying layerwise surprisallet d = {(s1, s′1), · · · , (sn, s′n)} be a dataset ofsentence pairs, where si is a control sentence ands′i is an anomalous sentence.
for each layer l, wedeﬁne the surprisal gap as the mean difference ofsurprisal scores between the control and anoma-.
4220lous sentences, scaled by the standard deviation:.
morphosyntax  −  blimp (subject−verb).
pag. lasirprus.2.
1.
0.
2.
1.
0.
2.
1.
0.
2.
1.
0.
2.
1.
0.
2.
1.
0.
2.
1.
0.
2.
1.
0.
2.
1.
0.
2.
1.
0.
2.
1.
0.
2.
1.
0.surprisal gapl(d) =e{surprisall(s′σ{surprisall(s′.
i) − surprisall(si)}ni) − surprisall(si)}n.i=1.
i=1.
(9).
the surprisal gap is a scale-invariant measureof sensitivity to anomaly, similar to a signal-to-noise ratio.
while surprisal scores are unitless,the surprisal gap may be viewed as the number ofstandard deviations that anomalous sentences trig-ger surprisal above control sentences.
this is ad-vantageous over accuracy scores, which treats thesentence pair as correct when the anomalous sen-tence has higher surprisal by any margin; this hardcutoff masks differences in the magnitude of sur-prisal.
the metric also allows for fair comparisonof surprisal scores across datasets of vastly differ-ent sizes.
figure 4 shows the surprisal gap for all12 tasks, using the roberta model; the resultsfor bert and xlnet are in the appendix c..next, we compare the performance of the gaus-sian model with the masked language model(mlm).
we score each instance as correct if themasked probability of the correct word is higherthan the anomalous word.
one limitation of themlm approach is that it requires the sentencepair to be identical in all places except for onetoken, since the lms do not support modelingjoint probabilities over multiple tokens.
to ensurefair comparison between gm and mlm, we ex-clude instances where the differing token is out-of-vocabulary in any of the lms (this excludesapproximately 30% of instances).
for the gaus-sian model, we compute accuracy using the best-performing layer for each model (section 3.2).
the results are listed in table 2..5 discussion.
5.1 anomaly type and surprisal.
morphosyntactic anomalies generally appear ear-lier than semantic anomalies (figure 4).
the sur-prisal gap plot exhibits different patterns depend-ing on the type of linguistic anomaly: morphosyn-tactic anomalies produce high surprisal relativelyearly (layers 3-4), while semantic anomalies pro-duce low surprisals until later (layers 9 and above).
commonsense anomalies do not result in sur-prisals at any layer: the surprisal gap is near zerofor all of the commonsense tasks.
the observeddifference between morphosyntactic and semantic.
morphosyntax  −  blimp (det−noun).
morphosyntax  −  osterhout and nicol.
semantic  −  blimp (animacy).
semantic  −  pylkkänen and mcelree.
semantic  −  warren et al..semantic  −  osterhout and nicol.
semantic  −  osterhout and mobley.
commonsense  −  warren et al..commonsense  −  federmeier and kutas.
commonsense  −  chow et al..commonsense  −  urbach and kutas.
0.
1.
2.
3.
4.
8.
9.
10 11 12.
5.
7.
6layer.
figure 4: layerwise surprisal gaps for all tasks usingthe roberta model.
generally, a positive surprisalgap appears in earlier layers for morphosyntactic tasksthan for semantic tasks; no surprisal gap appears at anylayer for commonsense tasks..4221type.
task.
morphosyntax.
semantic.
commonsense.
blimp (subject-verb)blimp (det-noun)osterhout and nicol (1999)blimp (animacy)pylkk¨anen and mcelree (2007)warren et al.
(2015)osterhout and nicol (1999)osterhout and mobley (1995)warren et al.
(2015)federmeier and kutas (1999)chow et al.
(2016)urbach and kutas (2010).
size.
2000200090200070309090303444120.bert.
gm mlm0.9550.9991.0000.7870.9551.0000.9571.000∗0.550∗0.708n/a0.924.
0.9530.9701.0000.6440.727∗0.5560.681∗0.528∗0.600∗0.458∗0.591∗0.470.
robertagm mlm0.9570.9991.0000.7540.9551.0001.0000.981∗0.4500.875n/a0.939.
0.9710.9831.0000.7670.9320.9440.8410.9060.750∗0.583∗0.432∗0.485.
xlnet.
gm mlm0.5840.5910.7180.6570.727∗0.5560.7830.774∗0.600∗0.667n/a0.712.
0.8270.8940.9010.675∗0.636∗0.667∗0.507∗0.302∗0.300∗0.625∗0.568∗0.500.
table 2: comparing accuracy scores between gaussian anomaly model (gm) and masked language model (mlm)for all models and tasks.
asterisks indicate that the accuracy is not better than random (0.5), using a binomial testwith threshold of p < 0.05 for signiﬁcance.
the mlm results for chow et al.
(2016) are excluded because thecontrol and anomalous sentences differ by more than one token.
the best layers for each model (section 3.2) areused for gm, and the last layer is used for mlm.
generally, mlm outperforms gm, and the difference is greaterfor semantic and commonsense tasks..anomalies is consistent with previous work (ten-ney et al., 2019), which found that syntactic in-formation appeared earlier in bert than semanticinformation..one should be careful and avoid drawing con-clusions from only a few experiments.
a simi-lar situation occurred in psycholinguistics research(kutas et al., 2006): early results suggested thatthe n400 was triggered by semantic anomalies,while syntactic anomalies triggered the p600 – adifferent type of erp.
however, subsequent ex-periments found exceptions to this rule, and nowit is believed that the n400 cannot be catego-rized by any standard dichotomy, like syntax ver-sus semantics (kutas and federmeier, 2011).
inour case, pylkk¨anen and mcelree (2007) is an ex-ception: the task is a semantic anomaly, but pro-duces surprisals in early layers, similar to the mor-phosyntactic tasks.
hence it is possible that thedichotomy is something other than syntax versussemantics; we leave to future work to determinemore precisely what conditions trigger high sur-prisals in lower versus upper layers of lms..5.2 comparing anomaly model with mlm.
the masked language model (mlm) usually out-performs the gaussian anomaly model (gm), butthe difference is uneven.
mlm performs muchbetter than gm on commonsense tasks, slightlybetter on semantic tasks, and about the same orslightly worse on morphosyntactic tasks.
it is notobvious why mlm should perform better thangm, but we note two subtle differences betweenthe mlm and gm setups that may be contributing.
factors.
first, the gm method adds up the sur-prisal scores for the whole sequence, while mlmonly considers the softmax distribution at one to-ken.
second, the input sequence for mlm alwayscontains a [mask] token, whereas gm takes theoriginal unmasked sequences as input, so the rep-resentations are never identical between the twosetups..mlm generally outperforms gm, but it doesnot solve every task: all three lms fail to per-form above chance on the data from warren et al.
(2015).
this set of stimuli was designed sothat both the control and impossible completionsare not very likely or expected, which may havecaused the difﬁculty for the lms.
we excludedthe task of chow et al.
(2016) for mlm becausethe control and anomalous sentences differed bymore than one token2..5.3 differences between lms.
roberta is the best-performing of the three lmsin both the gm and mlm settings: this is expectedsince it is trained with the most data and performswell on many natural language benchmarks.
sur-prisingly, xlnet is ill-suited for this task and per-forms worse than bert, despite having a similarmodel capacity and training data..the surprisal gap plots for bert and xl-.
2sentence pairs with multiple differing tokens are incon-venient for mlm to handle, but this is not a fundamental lim-itation.
for example, salazar et al.
(2020) proposed a modiﬁ-cation to mlm to handle such cases: they compute a pseudo-log-likelihood score for a sequence by replacing one token ata time with a [mask] token, applying mlm to each maskedsequence, and summing up the log likelihood scores..4222net (appendix c) show some differences fromroberta: only morphosyntactic tasks produceout-of-domain embeddings in these two models,and not semantic or commonsense tasks.
ev-idently, how lms behave when presented withanomalous inputs is dependent on model architec-ture and training data size; we leave exploration ofthis phenomenon to future work..6 conclusion.
we use gaussian models to characterize out-of-domain embeddings at intermediate layers oftransformer language models.
the model re-quires a relatively small amount of in-domain data.
our experiments reveal that out-of-domain pointsin lower layers correspond to low-frequency to-kens, while grammatically anomalous inputs areout-of-domain in higher layers.
furthermore, mor-phosyntactic anomalies are recognized as out-of-domain starting from lower layers compared tosyntactic anomalies.
commonsense anomalies donot generate out-of-domain embeddings at anylayer, even when the lm has a preference for thecorrect cloze completion.
these results show thatdepending on the type of linguistic anomaly, lmsuse different mechanisms to produce the outputsoftmax distribution..acknowledgements.
we thank julian salazar and our anonymous re-viewers for their helpful suggestions.
yx isfunded through an nserc discovery grant, asshrc insight grant, and an ontario era award.
fr is supported by a cifar chair in artiﬁcial in-telligence..references.
m´arta abrus´an.
2019. semantic anomaly, pragmaticinfelicity, and ungrammaticality.
annual review oflinguistics, 5:329–351..tianshi cao, chinwei huang, david yu-tung hui, andjoseph paul cohen.
2020. a benchmark of med-arxiv preprintical out of distribution detection.
arxiv:2007.04250..emmanuele chersoni, adri`a torrens urrutia, philippeblache, and alessandro lenci.
2018. modeling vio-lations of selectional restrictions with distributionalsemantics.
in proceedings of the workshop on lin-guistic complexity and natural language process-ing, pages 20–29..noam chomsky.
1957. syntactic structures.
mouton.
and co..wing-yee chow, cybelle smith, ellen lau, and colinphillips.
2016. a “bag-of-arguments” mechanismfor initial verb predictions.
language, cognitionand neuroscience, 31(5):577–596..jacob devlin, ming-wei chang, kenton lee, andkristina toutanova.
2019. bert: pre-training ofdeep bidirectional transformers for language under-standing.
in proceedings of the 2019 conference ofthe north american chapter of the association forcomputational linguistics: human language tech-nologies, volume 1 (long and short papers), pages4171–4186..allyson ettinger.
2020. what bert is not: lessonsfrom a new suite of psycholinguistic diagnostics forlanguage models.
transactions of the associationfor computational linguistics, 8:34–48..kara d federmeier and marta kutas.
1999. a rose byany other name: long-term memory structure andsentence processing.
journal of memory and lan-guage, 41(4):469–495..stefan l frank, leun j otten, giulia galli, andgabriella vigliocco.
2015. the erp response to theamount of information conveyed by words in sen-tences.
brain and language, 140:1–11..chengyue gong, di he, xu tan, tao qin, liweiwang, and tie-yan liu.
2018. frage: frequency-agnostic word representation.
in advances in neuralinformation processing systems, pages 1334–1345..kristina gulordava, piotr bojanowski, ´edouard grave,tal linzen, and marco baroni.
2018. colorlessingreen recurrent networks dream hierarchically.
proceedings of the 2018 conference of the northamerican chapter of the association for computa-tional linguistics: human language technologies,volume 1 (long papers), pages 1195–1205..lucas torroba hennigen, adina williams, and ryanintrinsic probing through dimen-cotterell.
2020.in proceedings of the 2020 con-sion selection.
ference on empirical methods in natural languageprocessing (emnlp), pages 197–216..john hewitt and christopher d manning.
2019. astructural probe for ﬁnding syntax in word represen-in proceedings of the 2019 conference oftations.
the north american chapter of the association forcomputational linguistics: human language tech-nologies, volume 1 (long and short papers), pages4129–4138..jennifer hu, jon gauthier, peng qian, ethan wilcox,and roger levy.
2020. a systematic assessment ofsyntactic generalization in neural language models.
in proceedings of the 58th annual meeting of theassociation for computational linguistics, pages1725–1744.
association for computational linguis-tics..4223ma kelly, yang xu, jes´us calvillo, and david reitter.
2020. which sentence embeddings and which layersin cognitive science,encode syntactic structure?
pages 2375–2381..lee osterhout and janet nicol.
1999. on the distinc-tiveness, independence, and time course of the brainresponses to syntactic and semantic anomalies.
lan-guage and cognitive processes, 14(3):283–317..artur kulmizev, vinit ravishankar, mostafa abdou,and joakim nivre.
2020. do neural language mod-els show preferences for syntactic formalisms?
inproceedings of the 58th annual meeting of the asso-ciation for computational linguistics, pages 4077–4091..marta kutas and kara d federmeier.
2011. thirtyyears and counting: ﬁnding meaning in the n400component ofthe event-related brain potential(erp).
annual review of psychology, 62:621–647..marta kutas, cyma k van petten, and robert kluen-der.
2006. psycholinguistics electriﬁed ii (1994–in handbook of psycholinguistics, pages2005).
659–724.
elsevier..ilia kuznetsov and iryna gurevych.
2020. a matterof framing: the impact of linguistic formalism onin proceedings of the 2020 con-probing results.
ference on empirical methods in natural languageprocessing (emnlp), pages 171–182.
associationfor computational linguistics..kimin lee, kibok lee, honglak lee, and jinwoo shin.
2018. a simple uniﬁed framework for detecting out-of-distribution samples and adversarial attacks.
ad-vances in neural information processing systems,31:7167–7177..geoffrey neil leech.
1992.
100 million words of en-the british national corpus (bnc).
lan-.
glish:guage research, 28:1–13..yinhan liu, myle ott, naman goyal, jingfei du, man-dar joshi, danqi chen, omer levy, mike lewis,luke zettlemoyer, and veselin stoyanov.
2019.roberta: a robustly optimized bert pretrainingapproach.
arxiv preprint arxiv:1907.11692..eleni metheniti, tim van de cruys, and nabil hathout.
2020. how relevant are selectional preferences fortransformer-based language models?
in proceed-ings of the 28th international conference on com-putational linguistics, pages 1266–1278..felicealessio miaschi, dominique brunato,dell’orletta, and giulia venturi.
2020.lin-guistic proﬁling of a neural language model.
the28th international conference on computationallinguistics, pages 745–756..james michaelov and benjamin bergen.
2020. howwell does surprisal explain n400 amplitude underdifferent experimental conditions?
in proceedingsof the 24th conference on computational naturallanguage learning, pages 652–663..lee osterhout and linda a mobley.
1995. event-related brain potentials elicited by failure to agree.
journal of memory and language, 34(6):739–773..alexander podolskiy, dmitry lipin, andrey bout, eka-terina artemova, and irina piontkovskaya.
2021.revisiting mahalanobis distance for transformer-based out-of-domain detection.
in 35th aaai con-ference on artiﬁcial intelligence (aaai 2021)..mads poulsen.
2012..thegrammaticality–acceptability distinction in func-tional approaches to language.
acta linguisticahafniensia, 44(1):4–21..the usefulness of.
liina pylkk¨anen and brian mcelree.
2007. an megstudy of silent meaning.
journal of cognitive neuro-science, 19(11):1905–1921..ella rabinovich, julia watson, barend beekhuizen,and suzanne stevenson.
2019. say anything: auto-matic semantic infelicity detection in l2 english in-deﬁnite pronouns.
in proceedings of the 23rd con-ference on computational natural language learn-ing (conll), pages 77–86..anna rogers, olga kovaleva, and anna rumshisky.
2021. a primer in bertology: what we knowabout how bert works.
transactions of the asso-ciation for computational linguistics, 8:842–866..julian salazar, davis liang, toan q. nguyen, andkatrin kirchhoff.
2020. masked language modelin proceedings of the 58th annual meet-scoring.
ing of the association for computational linguis-tics, pages 2699–2712.
association for computa-tional linguistics..ryohei sasano and anna korhonen.
2020. investigat-ing word-class distributions in word vector spaces.
in proceedings of the 58th annual meeting of theassociation for computational linguistics, pages3657–3666..bernhard sch¨olkopf, robert c williamson, alex jsmola, john shawe-taylor, and john c platt.
2000.support vector method for novelty detection.
inadvances in neural information processing systems,pages 582–588..ian tenney, dipanjan das, and ellie pavlick.
2019.bert rediscovers the classical nlp pipeline.
inproceedings of the 57th annual meeting of the asso-ciation for computational linguistics, pages 4593–4601..thomas p urbach and marta kutas.
2010. quantiﬁersmore or less quantify on-line: erp evidence for par-tial incremental interpretation.
journal of memoryand language, 63(2):158–179..tessa warren, evelyn milburn, nikole d patson, andmichael walsh dickey.
2015. comprehending theimpossible: what role do selectional restriction vi-language, cognition and neuro-olations play?
science, 30(8):932–939..4224alex warstadt, alicia parrish, haokun liu, anhad mo-hananey, wei peng, sheng-fu wang, and samuel rbowman.
2020. blimp: the benchmark of linguis-tic minimal pairs for english.
transactions of theassociation for computational linguistics, 8:377–392..alex warstadt, amanpreet singh, and samuel r bow-man.
2019. neural network acceptability judg-ments.
transactions of the association for compu-tational linguistics, 7:625–641..thomas wolf, julien chaumond, lysandre debut, vic-tor sanh, clement delangue, anthony moi, pier-ric cistac, morgan funtowicz, joe davison, samshleifer, et al.
2020. transformers: state-of-the-art natural language processing.
in proceedings ofthe 2020 conference on empirical methods in nat-ural language processing: system demonstrations,pages 38–45..zhilin yang, zihang dai, yiming yang, jaime car-bonell, russ r salakhutdinov, and quoc v le.
2019.xlnet: generalized autoregressive pretraining forlanguage understanding.
in advances in neural in-formation processing systems, pages 5753–5763..4225genreacademicfictionnewsspokenall.
accuracy0.7970.8400.8280.7950.830.kernelrbflinearpolynomial.
score0.7380.7260.725.table 5: effect of the genre of training data..with three kernels (table 6), but it performs worsethan the gaussian model on all settings..table 6: using 1-svm instead of gmm, with variouskernels..sentence aggregation..instead of equation5, we try deﬁning sentence-level surprisal as themaximum surprisal among all tokens (table 7):.
surprisal(s1, · · · , sn) = maxn.
i=1gi;.
(10).
however, this performs worse than using the sumof token surprisals..aggregation accuracy0.830sum0.773max.
table 7: two sentence-level aggregation strategies.
a ablation experiments on gaussian.
model.
we compare some variations to our methodologyof training the gaussian model.
all of these vari-ations are evaluated on the full blimp dataset.
ineach experiment, (unless otherwise noted) the lan-guage model is roberta-base, using the second-to-last layer, and the gaussian model has a full co-variance matrix trained with 1000 sentences fromthe bnc corpus..covariance matrix.
we vary the type of co-variance matrix (table 3).
diagonal and sphericalcovariance matrices perform worse than with thefull covariance matrix; this may be expected, asthe full matrix has the most trainable parameters..covariance accuracy0.830full0.755diagonal0.752spherical.
table 3: varying the type of covariance matrix in thegaussian model..gaussian mixture models.
we try gmms withup to 16 mixture components (table 4).
we ob-serve a small increase in accuracy compared to asingle gaussian, but the difference is too small tojustify the increased training time..components accuracy0.83010.84120.83640.84980.82716.table 4: using gaussian mixture models (gmms) withmultiple components..genre of training text.
we sample from gen-res of bnc (each time with 1000 sentences) totrain the gaussian model (table 5).
the modelperformed worse when trained with the academicand spoken genres, and about the same with theﬁction and news genres, perhaps because their vo-cabularies and grammars are more similar to thosein the blimp sentences..one-class svm.
we try replacing the gaussianmodel with a one-class svm (sch¨olkopf et al.,2000), another popular model for anomaly detec-tion.
we use the default settings from scikit-learn.
4226b pca plots of infrequent tokens.
we feed a random selection of bnc sentences intoroberta and use pca to visualize the distribu-tion of rare and frequent tokens at different layers(figure 5).
in all cases, we ﬁnd that infrequent to-kens occupy a different region of the embeddingspace from frequent tokens, similar to what gonget al.
(2018) observed for static word vectors.
thisis consistent with the correlation between token-level surprisal and frequency (figure 3), althoughthe decrease in correlation towards upper layers isnot apparent in the pca plots..c surprisal gap for bert and xlnet.
figures 6 and 7 plot the surprisal gaps using thebert and xlnet models; data and algorithms areidentical to the roberta model (figure 4).
thegaussian model is only sensitive to morphosyntac-tic anomalies, and not to semantic and common-sense ones..figure 5: pca plot of randomly sampled robertaembeddings at layers 1, 4, 7, and 10. points are col-ored by token frequency: “rare” means the 20% leastfrequent tokens, and “frequent” is the other 80%..4227layer: 1frequentrarelayer: 4frequentrarelayer: 7frequentrarelayer: 10frequentraremorphosyntax  −  blimp (subject−verb).
morphosyntax  −  blimp (subject−verb).
morphosyntax  −  blimp (det−noun).
morphosyntax  −  blimp (det−noun).
morphosyntax  −  osterhout and nicol.
morphosyntax  −  osterhout and nicol.
semantic  −  blimp (animacy).
semantic  −  blimp (animacy).
semantic  −  pylkkänen and mcelree.
semantic  −  pylkkänen and mcelree.
semantic  −  warren et al..semantic  −  warren et al..semantic  −  osterhout and nicol.
semantic  −  osterhout and nicol.
semantic  −  osterhout and mobley.
semantic  −  osterhout and mobley.
commonsense  −  warren et al..commonsense  −  warren et al..commonsense  −  federmeier and kutas.
commonsense  −  federmeier and kutas.
commonsense  −  chow et al..commonsense  −  chow et al..commonsense  −  urbach and kutas.
commonsense  −  urbach and kutas.
pag. lasirprus.−1.
−1.
−1.
−1.
−1.
1.
0.
1.
0.
1.
0.
1.
0.
1.
0.
1.
0.
1.
0.
1.
0.
1.
0.
1.
0.
1.
0.
1.
0.
−1.
−1.
−1.
−1.
−1.
−1.
−1.
pag. lasirprus.1.
0.
−1.
1.
0.
−1.
1.
0.
−1.
1.
0.
−1.
1.
0.
−1.
1.
0.
−1.
1.
0.
−1.
1.
0.
−1.
1.
0.
−1.
1.
0.
−1.
1.
0.
−1.
1.
0.
−1.
0.
1.
2.
3.
4.
8.
9.
10 11 12.
0.
1.
2.
3.
4.
8.
9.
10 11 12.
7.
5.
6layer.
7.
5.
6layer.
figure 6: surprisal gap plot using bert..figure 7: surprisal gap plot using xlnet..4228