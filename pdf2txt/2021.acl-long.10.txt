generalising multilingual concept-to-text nlgwith language agnostic delexicalisation.
giulio zhouhuawei noah’s ark lablondon, ukgiuliozhou@huawei.com.
gerasimos lampourashuawei noah’s ark lablondon, ukgerasimos.lampouras@huawei.com.
abstract.
concept-to-text natural language generationis the task of expressing an input meaning rep-resentation in natural language.
previous ap-proaches in this task have been able to gener-alise to rare or unseen instances by relying on adelexicalisation of the input.
however, this of-ten requires that the input appears verbatim inthe output text.
this poses challenges in mul-tilingual settings, where the task expands togenerate the output text in multiple languagesin this paper, we ex-given the same input.
plore the application of multilingual models inconcept-to-text and propose language agnos-tic delexicalisation, a novel delexicalisationmethod that uses multilingual pretrained em-beddings, and employs a character-level post-editing model to inﬂect words in their correctform during relexicalisation.
our experimentsacross ﬁve datasets and ﬁve languages showthat multilingual models outperform monolin-gual models in concept-to-text and that ourframework outperforms previous approaches,especially in low resource conditions..1.introduction.
recently, neural approaches to language generationhave become predominant in various tasks suchas concept-to-text natural language generation(nlg), summarisation, and machine translationthanks to their ability to achieve state-of-the-artperformance through end-to-end training (duˇseket al., 2018; chandrasekaran et al., 2019; barraultet al., 2019).
speciﬁcally in machine translation,deep learning models have proven easy to adapt tomultilingual output (johnson et al., 2017) and havebeen demonstated to successfully transfer knowl-edge between languages, beneﬁting both the lowand high resource languages (dabre et al., 2020).
in the concept-to-text nlg task, the languagegeneration model has to produce a text that is an.
figure 1: delexicalisation on webnlg challenge2020 with target output in english and russian.
doubleunderlining marks text missed by delexicalisation..accurate realisation of the abstract semantic infor-mation given in the input (meaning representation,mr; see figure 1).
it is common practice to per-form a delexicalisation (wen et al., 2015) of themr, in order to facilitate the nlg model’s gen-eralisation to rare and unseen input; lack of gen-eralisation is a main drawback of neural models(goyal et al., 2016) but is particularly prominentin concept-to-text.
delexicalisation consists of apreprocessing and a postprocessing step.
in prepro-.
proceedingsofthe59thannualmeetingoftheassociationforcomputationallinguisticsandthe11thinternationaljointconferenceonnaturallanguageprocessing,pages114–127august1–6,2021.©2021associationforcomputationallinguistics114inputmr:x1x2x3x4broadcastedbyﬁrstairedlastairedwhere:x1=bananaman|бананамен,x2=bbc|би_би_си,x3=1983_10_03,x4=1986_04_15goldtargetreferences:bananamanﬁrstairedonthebbconoctober3rd,1983andbroadcastitslastepisodeonapril15th,1986.бананаменвпервыевышелвэфирнаbbc3октября1983года,аегопоследнийэпизодвышел15апреля1986года.таблица1:delexicalisationandrelexicalisationstrategiesonenglishandrussianexamplesfromthewebnlg2020challenge.exactdelexicalisation:x1ﬁrstairedonthex2onoctober3rd,1983andbroadcastitslastepisodeonapril15th,1986.x1впервыевышелвэфирнаbbc3октября1983года,аегопоследнийэпизодвышел15апреля1986года.languageagnosticdelexicalisation(lad):x1ﬁrstairedonthex2onoctoberx3andbroadcastitslastepisodeonx4.x1впервыевышелвэфирнаx2x3года,аегопослед-нийэпизодвышелx4года.таблица2:delexicalisationandrelexicalisationstrategiesonenglishandrussianexamplesfromthewebnlg2020challenge.generatedoutputbeforerelexicalisation:(assumingtrainingwithlad)x1ﬁrstairedonthex2onoctoberx3andbroadcastitslastepisodeonx4.x1впервыевышелвэфирнаx2x3года,аегопослед-нийэпизодвышелx4года.exactrelexicalisation:bananamanﬁrstairedonthebbconoctober19831003andbroadcastitslastepisodeon19860415.бананаменвпервыевышелвэфирнабибиси19831003года,аегопоследнийэпизодвышел19860415года.automaticvaluepost-editing(vape):bananamanﬁrstairedonthebbconoctober3rd,1983andbroadcastitslastepisodeonapril15th,1986.бананаменвпервыевышелвэфирнабибиси3октяб-ря1983года,аегопоследнийэпизодвышел15апреля1986года.таблица3:delexicalisationandrelexicalisationstrategiesonenglishandrussianexamplesfromthewebnlg2020challenge.inputmr:x1x2x3x4countryleadernamedemonymwhere:x1=amdavadnigufa|амдаваднигуфа,x2=india|индия,x3=t.s.thakur|т.с.тхакур(муж),x4=indianpeople|индийцыgoldtargetreferences:amdavadnigufaislocatedinindia,wheretheleaderiststhakurandthedemonymforpeoplelivingthereisindian.амдаваднигуфанаходитсявиндии,гделидерт.с.тхакурилюди,проживающиетам,называютсяиндий-цами.exactdelexicalisation:x1islocatedinx2,wheretheleaderiststhakurandthedemonymforpeoplelivingthereisindian.x1находитсявиндии,гделидерт.с.тхакурилюди,проживающиетам,называютсяиндийцами.languageagnosticdelexicalisation(lad):x1islocatedinx2,wheretheleaderisx3andthedemonymforpeoplelivingthereisx4.x1находитсявx2,гделидерx3илюди,проживающиетам,называютсяx4.таблица4:delexicalisationandrelexicalisationstrategiesonenglishandrussianexamplesfromthewebnlg2020challenge.generatedoutputbeforerelexicalisation:x1islocatedinx2wherex3istheleaderandthepeopleareknownasx4.x1находитсявx2,гделидеромявляетсяx3.местныежителиизвестныкакx4.exactrelexicalisation:amdavadnigufaislocatedinindiawheret.s.thakuristheleaderandthepeopleareknownasindianpeople.амдаваднигуфанаходитсявиндия,гделидеромяв-ляетсят.с.тхакур(муж).местныежителиизвестныкакиндийцы.automaticvaluepost-editing(vape):amdavadnigufaislocatedinindiawheret.s.thakuristheleaderandthepeopleareknownasindians.амдаваднигуфанаходитсявиндии,гделидеромяв-ляетсят.с.тхакур.местныежителиизвестныкакин-дийцами.таблица5:delexicalisationandrelexicalisationstrategiesonenglishandrussianexamplesfromthewebnlg2020challenge.1cessing, all occurrences of mr values in the textare replaced with placeholders.
this way the modellearns to generate text that is abstracted away fromactual values.
in postprocessing (relexicalisation),placeholders are re-ﬁlled with values..the main shortcoming of delexicalisation is thatits efﬁcacy is bounded by the number of valuesthat are correctly identiﬁed.
in fact, a naive imple-mentation of “exact” delexicalisation (see figure 1)requires the values provided by the mr to appearverbatim in the text, which is often not the case.
this shortcoming is more prominent when expand-ing concept-to-text to the multilingual setting, asmr values in the target language are often onlypartially provided.
additionally, mr values areusually in their base form, which makes it harder toﬁnd them verbatim in text of morphologically richlanguages.
finally, relexicalisation also remains anaive process (see figure 2) that ignores how con-text should effect the morphology of the mr valuewhen it is added to the text (goyal et al., 2016)..we propose language agnostic delexicalisation(lad), a novel delexicalisation method that aimsto identify and delexicalise values in the text in-dependently of the language.
lad expands overprevious delexicalisation methods and maps inputvalues to the most similar n-grams in the text, byfocusing on semantic similarity, instead of lexicalsimilarity, over a language independent embeddingspace.
this is achieved by relying on pretrainedmultilingual embeddings, e.g.
laser (artetxeand schwenk, 2019).
in addition, when relexicalis-ing the placeholders, the values are processed witha character-level post editing model that modiﬁesthem to ﬁt their context.
speciﬁcally in morpholog-ically rich languages, this post editing results in thevalue exhibiting correct inﬂection for its context..our goal is to explore the application of multi-lingual models with a focus on their generalisationcapability to rare or unseen inputs.
in this paper,we (i) apply multilingual models and show thatthey outperform monolingual models in concept-to-text, especially in low resource conditions; (ii)propose lad and show that it achieves state-of-the-art results, especially on unseen input; (iii) provideexperimental analysis across 5 datasets and 5 lan-guages over models with and without pre-training..2 related work.
multilingual generation techniques have mostlybeen the focus of machine translation (mt) as.
figure 2: relexicalisation examples; double underlin-ing marks errors that ignore context..the appropriate data (multilingual parallel sourceand target sentences) are more readily availablethere.
earlier research enabled multilingual gener-ation with no and partial parameter sharing (luonget al., 2016; firat et al., 2016), while johnson et al.
(2017) explored many-to-many translation with fullparameter sharing in a universal encoder-decoderframework.
despite the successes of this many-to-many framework, the improvements were mainlyattributed to the model’s multilingual input.
wanget al.
(2018) improved on one-to-many translation(i.e.
the input is always on a single language, whilethe output is on many) by introducing special labelinitialisation, language-dependent positional em-beddings and a new parameter-sharing mechanism.
in other language generation tasks, the vast ma-jority of datasets are only available with englishoutput.
to enable output in a different language, anumber of zero-shot methods have been proposedwith the most common practice being to directlyuse an mt model to translate the output into thetarget language (wan et al., 2010; shen et al., 2018;duan et al., 2019).
the mt model can be ﬁne-tuned on task-speciﬁc data when those are available(miculicich et al., 2019).
for the purposes of thispaper, we do not consider these previous works asmultilingual, as the language generation model isdisjoint from the multilingual component, i.e.
thepipelined mt model.
contrary to this, chi et al.
(2020) proposed a cross-lingual pretrained maskedlanguage model to generate in multiple languages,outperforming pipeline models on question gener-ation and abstractive summarisation..115inputmr:x1x2x3x4broadcastedbyﬁrstairedlastairedwhere:x1=bananaman|бананамен,x2=bbc|би_би_си,x3=1983_10_03,x4=1986_04_15goldtargetreferences:bananamanﬁrstairedonthebbconoctober3rd,1983andbroadcastitslastepisodeonapril15th,1986.бананаменвпервыевышелвэфирнаbbc3октября1983года,аегопоследнийэпизодвышел15апреля1986года.таблица1:delexicalisationandrelexicalisationstrategiesonenglishandrussianexamplesfromthewebnlg2020challenge.exactdelexicalisation:x1ﬁrstairedonthex2onoctober3rd,1983andbroadcastitslastepisodeonapril15th,1986.x1впервыевышелвэфирнаbbc3октября1983года,аегопоследнийэпизодвышел15апреля1986года.languageagnosticdelexicalisation(lad):x1ﬁrstairedonthex2onoctoberx3andbroadcastitslastepisodeonx4.x1впервыевышелвэфирнаx2x3года,аегопослед-нийэпизодвышелx4года.таблица2:delexicalisationandrelexicalisationstrategiesonenglishandrussianexamplesfromthewebnlg2020challenge.generatedoutputbeforerelexicalisation:(assumingtrainingwithlad)x1ﬁrstairedonthex2onoctoberx3andbroadcastitslastepisodeonx4.x1впервыевышелвэфирнаx2x3года,аегопослед-нийэпизодвышелx4года.exactrelexicalisation:bananamanﬁrstairedonthebbconoctober19831003andbroadcastitslastepisodeon19860415.бананаменвпервыевышелвэфирнабибиси19831003года,аегопоследнийэпизодвышел19860415года.automaticvaluepost-editing(vape):bananamanﬁrstairedonthebbconoctober3rd,1983andbroadcastitslastepisodeonapril15th,1986.бананаменвпервыевышелвэфирнабибиси3октяб-ря1983года,аегопоследнийэпизодвышел15апреля1986года.таблица3:delexicalisationandrelexicalisationstrategiesonenglishandrussianexamplesfromthewebnlg2020challenge.inputmr:x1x2x3x4countryleadernamedemonymwhere:x1=amdavadnigufa|амдаваднигуфа,x2=india|индия,x3=t.s.thakur|т.с.тхакур(муж),x4=indianpeople|индийцыgoldtargetreferences:amdavadnigufaislocatedinindia,wheretheleaderiststhakurandthedemonymforpeoplelivingthereisindian.амдаваднигуфанаходитсявиндии,гделидерт.с.тхакурилюди,проживающиетам,называютсяиндий-цами.exactdelexicalisation:x1islocatedinx2,wheretheleaderiststhakurandthedemonymforpeoplelivingthereisindian.x1находитсявиндии,гделидерт.с.тхакурилюди,проживающиетам,называютсяиндийцами.languageagnosticdelexicalisation(lad):x1islocatedinx2,wheretheleaderisx3andthedemonymforpeoplelivingthereisx4.x1находитсявx2,гделидерx3илюди,проживающиетам,называютсяx4.таблица4:delexicalisationandrelexicalisationstrategiesonenglishandrussianexamplesfromthewebnlg2020challenge.generatedoutputbeforerelexicalisation:x1islocatedinx2wherex3istheleaderandthepeopleareknownasx4.x1находитсявx2,гделидеромявляетсяx3.местныежителиизвестныкакx4.exactrelexicalisation:amdavadnigufaislocatedinindiawheret.s.thakuristheleaderandthepeopleareknownasindianpeople.амдаваднигуфанаходитсявиндия,гделидеромяв-ляетсят.с.тхакур(муж).местныежителиизвестныкакиндийцы.automaticvaluepost-editing(vape):amdavadnigufaislocatedinindiawheret.s.thakuristheleaderandthepeopleareknownasindians.амдаваднигуфанаходитсявиндии,гделидеромяв-ляетсят.с.тхакур.местныежителиизвестныкакин-дийцами.таблица5:delexicalisationandrelexicalisationstrategiesonenglishandrussianexamplesfromthewebnlg2020challenge.1an adaptation of puduppully et al.
(2019) wasapplied to multilingual concept-to-text nlg andparticipated in the document-level generation andtranslation shared task (hayashi et al., 2019, dgt).
however, this shared task, and in extension thedataset and participating systems, heavily focus oncontent selection and document generation.
addi-tionally, the input’s attributes are constant acrosstrain and testing, so there are no unseen data andno need to improve on the model’s generalisationcapability.
as the goal of this paper is multilingual-ity (content selection is a language agnostic task)and generalisation, we opt to not use this dataset..multilinguality has also been explored in the re-lated tasks of morphological inﬂection and surfacerealisation in sigmorphon (mccarthy et al.,2019) and msr (mille et al., 2020) challenges.
however, our automatic value post-editing ap-proach focuses mostly on adapting values to con-text and does not assume additional input such asdependency trees, pos tags or morphological infor-mation that surface realisation and morphologicalinﬂection often requires..particularly for concept-to-text nlg, notableprevious works includes the approach of fan andgardent (2020) who make use of pretrained lan-guage models through the transformer architecturefor amr-to-text generation in multiple languages,and the webnlg challenge 2020 (castro ferreiraet al., 2020).
the goal of webnlg 2020 was togenerate output in both english and russian butmost of the participants focused on monolingualrather than multilingual approaches..3 rare and unseen inputs in nlg.
due to the existence of open-set and numericalattributes in the aforementioned datasets, it is com-mon during testing for mrs to contain rare or un-seen values.
certain datasets are even more chal-lenging in this regard (e.g.
webnlg challenge2020) as they also contain unseen relations in thedevelopment and test subsets.
several techniqueshave been proposed to mitigate this problem..delexicalisation, also known as anonymisationor masking, is a pre/post-processing procedure thatattempts to mitigate problems with data sparsity.
in preprocessing, all values in the mr that appearverbatim in the target sentence are replaced in bothinput and output with speciﬁc placeholders, e.g.
“x-” followed by the corresponding attribute (e.g.
“x-type”) so that the placeholder still captures rele-.
vant semantic information.
in figure 1 we use num-bered placeholders instead, for clarity and space.
the model is trained to generate the target text con-taining these placeholders, which are subsequentlyreplaced with the corresponding true values (i.e.
relexicalised) in post-processing.
see figures 1and figure 2 for examples; we mark this strategyas exact due to the exact matching of the valueswith the text.
to improve delexicalisation accuracy,n-gram matching (trisedya et al., 2018) has beenproposed as an alternative.
thanks to its simplic-ity and efﬁcacy, delexicalisation is widely used bymany systems, including the winning systems ofmajor concept-to-text nlg shared tasks (gardentet al., 2017; duˇsek et al., 2018; castro ferreiraet al., 2020).
mapping the values as such can besufﬁcient for simple datasets, but otherwise, in-correct or incomplete delexicalisation will lead toinconsistent input and deteriorate performance..lastly, problems may also occur during relexical-isation as it does not take into account the contextin which the placeholders are situated and mayresult in disﬂuent sentence.
for a simpliﬁed exam-ple, observe how placing the unedited dates in theplaceholders leads to disﬂuent output in figure 2..segmentation strategies are commonly used inneural machine translation to improve the gen-eralisation ability of models.
the objective is tobreak down words into smaller units, reducing thevocabulary and the number of unseen tokens (sen-nrich et al., 2016).
unfortunately, applying seg-mentation in concept-to-text nlg, e.g.
using byte-pair-encoding (bpe) subword units (gardent et al.,2017; zhang et al., 2018) or using characters as ba-sic units (goyal et al., 2016; agarwal and dymet-man, 2017; deriu and cieliebak, 2018), underper-forms against delexicalisation.
challenges includecapturing long dependencies between segmentedwords, and generating non-existing words..copy mechanism is another method to ad-dress unseen input, by allowing the decoder ofan encoder-decoder model to draw a token di-rectly from the input sequence instead of gener-ating it from the decoder vocabulary (see et al.,2017).
while applications of the copy mechanismin concept-to-text nlg have achieved overall goodresults (chen, 2018; elder et al., 2018; gehrmannet al., 2018), when dealing with rare and unseen in-puts delexicalisation is still preferable (shimorinaand gardent, 2018).
to improve the generalisationability of copy mechanism models, roberti et al..116should generate output in.
in addition, we followwang et al.
(2018) and initialise the decoder withthe language token.
the rest of the components (i.e.
delexicalisation, ordering, and value post-editing)are orthogonal to the model’s architecture..4.1 value matching.
as discussed in section 3, one of the challengesof delexicalisation is matching the mr values withcorresponding words in the text, especially in themultilingual setting.
even when the mr valuesare in the same language as the target, we observefrom the examples in figure 1 that token overlap-ping methods (i.e.
exact and n-gram matching) arenot sufﬁcient to generate a complete and accuratedelexicalisation as values may appear differently..to counter this problem, lad performs match-ing by mapping mr values to n-grams based on thesimilarity of their representations.
speciﬁcally, itcalculates the similarity between a value v and allword n-grams wi .
.
.
wj in the text, with ji < nand n set to the maximum value length observed inthe training data.
lad employs laser (artetxeand schwenk, 2019) to generate language agnosticsentence embeddings of the values and n-grams,and calculates their distance via cosine similarity.
given an mr and text, all possible value and n-gram comparisons are calculated and the matchesare determined in a greedy fashion..−.
4.2 generic placeholders and ordering.
in section 3, we discussed how the webnlgdatasets are more challenging because they containunseen attributes in the development and test sub-sets, in addition to unseen values.
this is problem-atic when we use attribute-bound placeholders (e.g.
“x-type”) as unseen attributes will result in unseenplaceholders.
following trisedya et al.
(2018), forthe webnlg datasets, lad uses numbered genericplaceholders “x#” (e.g.
“x1”).
unfortunately, theadoption of generic placeholders creates problemsfor relexicalisation as it becomes unclear whichinput value should replace which placeholder.
weaddress this by ordering the model’s input basedon the graph formed by its rdf triples, again byfollowing trisedya et al.
(2018).
we traverse ev-ery edge in the graph, starting from the node withthe least incoming edges (or randomly in case ofties) and then visit all nodes via bfs (breadth-ﬁrstsearch).
we then trust that the model will learn torespect the input order when generating, and followthe order to relexicalise the placeholders..figure 3: language agnostic delexicalisation outline..(2019) propose applying the copy mechanism tocharacter-level nlg systems.
this is combinedwith an additional optimisation phase during train-ing where the encoder and decoder are switched..4 language agnostic delexicalisation.
in order to address the shortcomings of previousapproaches to generalise over rare or unseen inputs,especially in cases of multilingual output, we pro-pose language agnostic delexicalisation (lad).
figure 3 shows an overview of lad; the inputand output are ﬁrst delexicalised using pretrainedlanguage-independent embeddings, and (option-ally) ordered.
the multilingual generation modelis trained on the delexicalised training data, andthe output is relexicalised using automatic valuepost-editing to ensure that the values ﬁt the context.
each component is described in more detail bellow.
to enable multilingual generation, we adapt theuniversal encoder-decoder framework via “targetforcing” (johnson et al., 2017) since it can be di-rectly applied to any nlg model without the needto modify the latter’s architecture.
to do so, weextend the input mr in the encoder with a lan-guage token that signals which language the model.
117we note that this is only required for models thatemploy delexicalisation strategies and for datasetswith unseen attributes (i.e.
the webnlg challengedatasets).
concept-to-text nlg systems do notgenerally require ordered input (wen et al., 2015)..4.3 automatic value post-editing.
as discussed in section 3, a naive relexicalisationof the placeholders may lead to disﬂuent sentences,as the procedure does not take into account the con-text in which the placeholders have been placed.
for example, in the sentence “there are 2 x thathave free parking”, if we need to replace the place-holder “x” with the mr value “guesthouse” , thevalue should be pluralised to ﬁt the context.
thisproblem is more evident in morphologically richlanguages, where more factors affect the value’sform.
to alleviate this, the lad framework incor-porates an automatic value post-editing compo-nent, consisting of a character-level seq2seq modelthat iterates over values as they are placed in thetext and modiﬁes them to ﬁt the context of their re-spective placeholders.
anastasopoulos and neubig(2019) has already shown the beneﬁts of charactermodels on morphological inﬂection generation, butno previous work has addressed how relexicalisa-tion should adapt to context..our proposed vape model requires as input themr placeholder ei, original value vi and corre-sponding nlg output w(cid:48)n for context; theseare serialised and passed to the encoder.
similar tothe multilingual model, we add an appropriate lan-guage token l before the nlg output.
the outputof vape is the mr value v(cid:48).
i in the proper form..1 .
.
.
w(cid:48).
ei vi [sep] l w(cid:48).
1 .
.
.
w(cid:48).
v(cid:48)i.n} →.
{.
the training signal for vape is obtained duringdelexicalisation.
for a given delexicalisation strat-egy, we obtain all pairs of mr values and matchingn-grams in the training data, and subsequently trainvape using these n-grams as the targets.
there-fore, the vape model is dependent on the qualityof the delexicalisation strategy; speciﬁcally for ex-act delexicalisation, vape cannot be trained as themr values and matching n-grams are the same..most edits vape performs concern incorrectinﬂections, but it is not limited to morphologicaledits and has the potential to deal with varioustypes of modiﬁcations.
during our experimentswe observed vape performing value re-formatting“april 15th 1986”), syn-(e.g.
“1986 04 15”.
→.
onym generation (e.g.
“east”“oriental”) andvalue translation (e.g.
“bbc” from latin to cyrillic)..→.
5 experiments.
↑.
).
↓.
for our experiments we use ﬁve datasets and calcu-late bleu-4 (papineni et al., 2002,), meteor↑), chrf++ (popovi´c,(banerjee and lavie, 2005,2015,.
), and ter (snover et al., 2006,.
↑.
the webnlg challenge 2017 (gardent et al.,2017, webnlg17) data consists of sets of rdftriple pairs and corresponding english texts in 15dbpedia categories.
for our purposes, we will beusing a later work (shimorina et al., 2019) that in-troduced a machine translated russian version ofwebnlg17, a part of which was post edited byhumans.
due to the limited amount of human cor-rected russian sentences, and to facilitate the mostaccurate evaluation, we use these solely for testing.
to ensure that half of the domains in the new testset remain unseen during training, we create ourown train/dev/test split by retaining the followingdbpedia categories from training and developmentsets: astronaut, monument and university..the latest incarnation of the webnlg challenge(castro ferreira et al., 2020, webnlg20) is fullyhuman annotated for both english and russian.
weuse this as the main dataset in our experiments, asit is designed to promote multilinguality.
however,due to the fact that the provided test set does notcontain unseen russian instances, we perform ourexperiment on a custom split (webnlg20*) ensur-ing that part of the domains in the test data remainunseen during training.
the split was performedsimilarly to the previously described webnlg17.
multiwoz 2.1 (eric et al., 2020) and cross-woz (zhu et al., 2020) are datasets of dialogue actsand corresponding utterances in english and chi-nese respectively.
the two datasets share the samestructure, with multiwoz covering 7 domains and25 attributes, and crosswoz covering 5 domainsand 72 attributes; 4 of the domains are common inboth datasets though crosswoz has more attachedattributes.
multilingual woz 2.0 (mrkˇsi´c et al.,2017) is also a dialogue dataset with utterancesavailable in three languages: english, italian andgerman.
its scope is more limited than multiwozand crosswoz as it only covers a single domain.
for all models in our experiments, the input con-sists of a simple linearisation of the mrs. partic-ularly, for the delexicalisation based models, thevalues are extended with their respective placehold-.
118ers as shown in the following example: “entity 1meyer werft location entity 2 germany”..5.1 ablation study.
first we perform an ablation study to determinehow the different components of lad (orderingand vape) affect its performance; lad being ourfull language agnostic delexicalisation model asdescribed in section 4. in addition to lad, wherethese components are incrementally removed, weexplore how their addition would inﬂuence exactand n-gram delexicalisation (trisedya et al., 2018).
we do not explore adding vape to exact delexical-isation (there is no exact + o + v variant), as itcannot be trained in this setting (see section 4.3).
in table 1, we observe that both components arebeneﬁcial, but less so for seen english data.
forthe more morphologically rich and lower resourcedrussian, the components are helpful for both seenand unseen.
vape leads to an improvement in per-formance in almost all cases and even when addedon ngram.
an exception is unseen english data,where removing vape is beneﬁcial; this suggeststhat vape is overeager to make edits in english..→.
by studying the output, we observe that vapemodiﬁed 20% of values in english, and 66% inrussian; directly copying the value was insufﬁcientin russian where proper inﬂection is needed.
weidentiﬁed three consistent errors where copying theoriginal value would be preferable to using vape:the removal of date information (e.g.
“1969-09-011st, 1969”), misspelling of proper nouns (e.g.
→“atat¨urk monument”“atat erk monument”), andmishandling of long values (e.g.
“ottoman army“ottomansoldiers killed in the battle of baku”army soldiers killed in the batttle of kiled in thebathe batom”).
we observe that these errors oc-cur more frequently for english unseen cases, butcould be reduced by extending vape with a con-trol mechanism that decides whether copying thevalues themselves is preferable.
such errors occurin part because vape, as a character-level model,suffers from the same challenges as other segmen-tation methods (see section 3).
however, sincevape’s input is much shorter, the problem is not asprevalent.
overall, lad outperforms the previousdelexicalisation strategies exact and ngram, andvape is shown to be integral to its performance..→.
5.2 monolingual vs multilingual.
here we explore the performance of monolin-gual and multilingual models on concept-to-text.
a0.56exact0.52exact + o0.56ngram0.53ngram + ongram + o+v 0.540.59lad - o-v0.60lad - v0.62lad.
englishs0.620.540.620.550.570.650.630.66.u0.180.380.160.400.330.180.390.32.russians0.250.200.270.250.350.280.260.42.u0.030.140.040.150.160.030.160.21.a0.210.190.220.230.310.230.240.37.table 1: bleu on webnlg20* for delexicalisationmodels augmented with generic placeholders+ordering,and value post-edit.
a = all categories; s = seen cate-gories; u = unseen categories; o = generic placehold-ers+ordering; v = value post-edit..woz 2.0.en0.660.65.
0.660.68.it0.560.57.
0.580.59.de0.590.61.
0.560.57.webnlg2020*.
en0.570.57.
0.580.62.ru0.310.33.
0.320.37.multiwoz+ crosswozen0.560.57.zh0.680.66.
0.560.58.
0.680.68.word.
lad.
monomulti.
monomulti.
table 2: bleu for mono- and multilingual models..datasets.
the word model has the exact same archi-tecture as lad but no delexicalisation is performed,and consequently no automatic value post-editingand no ordering.
since there is no relexicalisationthat needs to occur during post-processing, the in-put to the word model needs not be speciﬁcallyordered, and is just a concatenation of the rdftriples as they appear in the original dataset.
formultilingual, we add the appropriate language to-kens on the input of word, in the same manner weadded them to lad.
for the monolingual (mono)conﬁguration we train the models to produce a sin-gle language, while for multilingual (multi) wetrain them to produce all languages available inthat dataset.
please refer to table 2 for the results.
we observe that the multilingual models out-perform their monolingual counterpart in mostdatasets and languages, especially with lad asits delexicalisation and relexicalisation modulesare more robust to multilingual input and output.
speciﬁcally for the multiwoz and crosswozdatasets, in the monolingual setting the modelsare trained exclusively on the respective dataset,i.e.
multiwoz for english, and crosswoz forchinese.
for multilingual, we take advantage ofthe fact that these datasets share the same structure,and train the models on both datasets.
for english,.
119english.
wordcharbpespcopy.
lad.
wordcharbpespcopy.
lad.
russian.
all categories.
seen categoriesbleu meteor chrf++ ter bleu meteor chrf++ ter bleu meteor chrf++ ter0.900.570.890.540.970.540.960.580.610.57.
0.260.240.240.230.51.
0.640.610.620.660.59.
0.430.410.420.440.38.
0.720.700.720.740.65.
0.360.400.420.340.42.
0.110.090.090.090.27.
0.440.470.490.420.45.
0.360.350.350.370.36.
0.630.520.630.640.63.
0.100.050.070.070.38.unseen categories.
0.62.
0.42.
0.71.
0.71.
0.66.
0.45.
0.75.
0.31.
0.32.
0.30.
0.54.
0.61.all categories.
seen categoriesbleu meteor chrf++ ter bleu meteor chrf++ ter bleu meteor chrf++ ter0.940.330.910.300.970.250.980.340.910.24.
0.510.500.480.540.42.
0.630.670.710.620.74.
0.540.530.540.560.46.
0.560.610.650.530.72.
0.020.010.020.010.02.
0.130.130.110.100.14.
0.200.200.190.180.20.
0.410.410.380.410.35.
0.440.440.440.450.39.
0.420.380.320.440.29.unseen categories.
0.37.
0.51.
0.55.
0.55.
0.42.
0.57.
0.60.
0.51.
0.21.
0.34.
0.42.
0.71.table 3: webnlg20* results for multilingual models..english.
splad.
all categories.
seen categoriesbleu meteor chrf++ ter bleu meteor chrf++ ter bleu meteor chrf++ ter0.960.580.610.62.unseen categories.
0.640.71.
0.420.36.
0.370.42.
0.660.66.
0.440.45.
0.340.31.
0.070.32.
0.090.30.
0.230.54.
0.740.75.mbartmb-ladmb-lad+mb-lad-spe.
0.660.660.670.66.
0.740.740.750.74.
0.330.310.310.31.
0.670.680.680.67.
0.750.750.750.75.
0.320.300.300.30.
0.580.520.610.59.
0.410.380.420.41.
0.700.680.710.70.
0.440.440.370.38.russian.
splad.
all categories.
seen categoriesbleu meteor chrf++ ter bleu meteor chrf++ ter bleu meteor chrf++ ter0.980.340.710.37.unseen categories.
0.450.55.
0.620.55.
0.410.51.
0.440.42.
0.540.57.
0.560.60.
0.530.51.
0.010.21.
0.100.34.
0.180.42.mbartmb-ladmb-lad+mb-lad-spe.
0.370.410.420.46.
0.510.580.580.59.
0.570.510.510.47.
0.430.450.410.42.
0.570.610.600.60.
0.520.490.500.49.
0.150.290.410.44.
0.330.440.520.52.
0.350.480.550.55.
0.780.590.520.49.
0.450.460.460.45.
0.560.580.570.57.
0.440.440.450.44.
0.500.540.550.57.table 4: webnlg20* results for pretrained multilingual models..we observe that the multilingual model improves,suggesting that domain knowledge is transferredfrom crosswoz.
for chinese however, the mul-tilingual word model underperforms.
this is notvery surprising, as the overlap between the datasetsis favourable to multiwoz, i.e.
most of the at-tributes of multiwoz also appear in crosswoz,while the majority of crosswoz’s attributes do notappear in multiwoz..5.3 multilingual generalisation.
tables 3 contains full results for english and rus-sian on webnlg20* respectively.
we include theword conﬁguration (see section 5.2), as well aschar, bpe, and sp, which are variations that usecharacters, byte-pair-encoding, and sentencepiece.
as subword units respectively.
copy refers to thecopy mechanism model by roberti et al.
(2019).
the sp model performs very well for seen cate-gories, but fails to generalise on unseen data.
thecopy model performs well for unseen categories inenglish, but underperforms in russian as values forit are only partially translated, i.e.
some values inthe mr may appear in english while others appearin russian.
this is challenging for copy modelsas the target reference does not closely match theinput, but lad can handle it more robustly..observing the output, lad’s main advantage isthat it avoids under- and over-generating valuesas they are being controlled by the placeholders.1sp is often the most ﬂuent of the models, but for.
1we provide output examples in the appendix..120english.
mbartmb-ladmb-lad+mb-lad-sl.
all categories.
seen categoriesbleu meteor chrf++ ter bleu meteor chrf++ ter bleu meteor chrf++ ter0.500.490.480.490.460.500.490.48.
0.630.670.680.66.
0.550.560.550.54.
0.680.710.710.70.
0.410.400.440.40.
0.330.360.380.36.
0.570.620.640.61.
0.460.440.430.45.
0.370.390.390.39.
0.400.410.410.41.
0.420.410.410.41.unseen categories.
table 5: ofﬁcial webnlg20 testset results for pretrained multilingual models on english text..longer input it tends to under-generate and missvalues.
the copy model tends to repeat values,which can be attributed to the fact that it is basedon characters where long-distance dependenciesare hard to maintain.
on the other hand, copy canpotentially generate more relevant output since itcan copy words from attributes as well as values..overall, lad helps the multilingual model out-perform all other models in both english and rus-sian.
it is especially beneﬁcial in generalising tounseen data, as was its main objective after all..5.4 generalising with pretrained models.
here we explore the generalisation capabilities ofmultilingual pretrained models, by replacing theunderlying nlg model with mbart (liu et al.,2020), a multilingual denoising autoencoder pre-trained on a large-scale dataset containing 25 lan-guages (cc25).
similarly to kasner and duˇsek(2020), we ﬁne-tune mbart with the default en-ro conﬁguration for up to 10000 updates.
usingmbart as the underlying model also helps facil-itate a comparison against a conﬁguration that issimilar to many of the state of the art participantsin the webnlg 2020 challenge, although some ofthem used different pretrained models..table 4 shows the performance of the ﬁne-tunedmodels on the webnlg20* dataset.
the mbart-based model outperforms the non-delexicalisationsp, and non-pretrained lad in english.
how-ever, lad still performs better in russian.
thismakes sense as the cc25 dataset is heavily biasedtowards the english language and contains dou-ble the amount of tokens compared to russian,and much more compared to other lower-resourcelanguages.
combining the lad framework withmbart (mb-lad) resulted in a general improve-ment in performance, especially for lower-resourceunseen data.
however, as discussed in section 5.1,the vape component remains to some degrees sus-ceptible to unseen contexts.
to tackle this issue, weimprove vape by pre-loading mbart and ﬁne-tuning it for value post-editing as well (mb-lad+),.
russian.
mbartmb-ladmb-lad+mb-lad-sl.
all/seen categoriesbleu meteor chrf++ ter0.520.430.500.420.530.380.480.44.
0.550.610.590.61.
0.560.640.610.63.table 6: ofﬁcial webnlg20 testset results for pre-trained multilingual models on russian text..russian.
wordcharbpesplad.
all categoriesbleu meteor chrf++ ter0.950.020.900.010.970.020.930.020.840.04.
0.210.190.200.200.26.
0.160.140.150.160.22.table 7: webnlg17 results for multilingual models..achieving 3 and 29 points increase in bleu scorefor unseen english over the vanilla mbart andlad models, and 26 and 20 points for unseen rus-sian.
additionally, to take advantage of mbart’sdenoising ability, we extend the ﬁne-tuned vapeto edit the “exact” relexicalised nlg output andprovide a sentence-level output (mb-lad-spe),i.e.
edits are not exclusively focused on the values.
results show that mb-lad-spe improves furthermb-lad+ on russian in both seen and unseen..table 5 and 6 also shows the automatic evalua-tion of the ﬁne-tuned mbart models on the ofﬁ-cial webnlg20 challenge testset; the ofﬁcial testset had no unseen subset of russian.
the resultsare consistent with the ﬁndings in our previous ex-periments, with small improvements of lad-basedmbart models over the mbart-base..5.5 synthetic data.
we use the webnlg17 automatically translatedrussian “silver” data, to determine how useful theyare for training multilingual concept-to-text nlg.
as preliminary results were not promising, we limitthe scope of the experiment to only a few systems.
table 7 gathers the results it is apparent that au-tomatically translated data are insufﬁcient; lad.
121seems to more consistently achieve higher perfor-mance than other models, but all scores are too lowto draw any sufﬁciently supported conclusions..6 conclusion.
we proposed language agnostic delexicalisation,a novel delexicalisation framework that matchesand delexicalises mr values in the text indepen-dently of the language.
for relexicalisation, anautomatic value post editing model adapts the val-ues to their context.
results show that multilingualmodels outperform monolingual models, and thatlad outperforms previous work in improving theperformance of multilingual models, especially inlow resource conditions.
lad also improves onthe performance of pre-trained language modelsachieving state-of-the-art results.
the automaticvalue post editing component is especially beneﬁ-cial in morphologically rich languages..references.
shubham agarwal and marc dymetman.
2017. a sur-prisingly effective out-of-the-box char2char modelon the e2e nlg challenge dataset.
in proceedingsof the 18th annual sigdial meeting on discourseand dialogue, pages 158–163, saarbr¨ucken, ger-many.
association for computational linguistics..antonios anastasopoulos and graham neubig.
2019.pushing the limits of low-resource morphological in-ﬂection.
in proceedings of the 2019 conference onempirical methods in natural language processingand the 9th international joint conference on natu-ral language processing (emnlp-ijcnlp), pages984–996, hong kong, china.
association for com-putational linguistics..mikel artetxe and holger schwenk.
2019. mas-sively multilingual sentence embeddings for zero-shot cross-lingual transfer and beyond.
transac-tions of the association for computational linguis-tics, 7:597–610..satanjeev banerjee and alon lavie.
2005. meteor:an automatic metric for mt evaluation with im-proved correlation with human judgments.
in pro-ceedings of the acl workshop on intrinsic and ex-trinsic evaluation measures for machine transla-tion and/or summarization, pages 65–72, ann ar-bor, michigan.
association for computational lin-guistics..lo¨ıc barrault, ondˇrej bojar, marta r. costa-juss`a,christian federmann, mark fishel, yvette gra-ham, barry haddow, matthias huck, philipp koehn,shervin malmasi, christof monz, mathias m¨uller,santanu pal, matt post, and marcos zampieri.
2019..findings of the 2019 conference on machine transla-tion (wmt19).
in proceedings of the fourth con-ference on machine translation (volume 2: sharedtask papers, day 1), pages 1–61, florence, italy.
as-sociation for computational linguistics..thiago castro ferreira, claire gardent, nikolaiilinykh, chris van der lee, simon mille, diegomoussallem, and anastasia shimorina.
2020. the2020 bilingual, bi-directional webnlg+ sharedtask: overview and evaluation results (webnlg+2020).
in proceedings of the 3rd international work-shop on natural language generation from the se-mantic web (webnlg+), pages 55–76, dublin, ire-land (virtual).
association for computational lin-guistics..muthu kumar chandrasekaran, michihiro yasunaga,dragomir r. radev, dayne freitag, and min-yenkan. 2019. overview and results: cl-scisummin proceedings of the 4th jointshared task 2019.workshop on bibliometric-enhanced information re-trieval and natural language processing for digitallibraries (birndl 2019) co-located with the 42ndinternational acm sigir conference on researchand development in information retrieval (sigir2019), paris, france, july 25, 2019..shuang chen.
2018. a general model for neural textgeneration from structured data.
e2e nlg chal-lenge system descriptions..zewen chi, li dong, furu wei, wenhui wang, xian-ling mao, and heyan huang.
2020. cross-lingualnatural language generation via pre-training.
pro-ceedings of the aaai conference on artiﬁcial intel-ligence, 34(05):7570–7577..raj dabre, chenhui chu, and anoop kunchukuttan.
2020. a survey of multilingual neural machinetranslation.
acm comput.
surv., 53(5)..jan milan deriu and mark cieliebak.
2018. end-to-end trainable system for enhancing diversity in nat-ural language generation.
e2e nlg challenge sys-tem descriptions..xiangyu duan, mingming yin, min zhang, boxingchen, and weihua luo.
2019. zero-shot cross-lingual abstractive sentence summarization throughteaching generation and attention.
in proceedings ofthe 57th annual meeting of the association for com-putational linguistics, pages 3162–3172, florence,italy.
association for computational linguistics..ondˇrej duˇsek, jekaterina novikova, and verena rieser.
2018. findings of the e2e nlg challenge.
inproceedings of the 11th international conferenceon natural language generation, pages 322–328,tilburg university, the netherlands.
association forcomputational linguistics..henry elder,.
sebastian gehrmann, alexandero’connor, and qun liu.
2018. e2e nlg challengesubmission: towards controllable generation of.
122in proceedings of thediverse natural language.
11th international conference on natural languagegeneration, pages 457–462, tilburg university,the netherlands.
association for computationallinguistics..fernanda vi´egas, martin wattenberg, greg corrado,macduff hughes, and jeffrey dean.
2017. google’smultilingual neural machine translation system: en-abling zero-shot translation.
transactions of the as-sociation for computational linguistics, 5:339–351..mihail eric, rahul goel, shachi paul, abhishek sethi,sanchit agarwal, shuyang gao, adarsh kumar,anuj goyal, peter ku, and dilek hakkani-tur.
2020.multiwoz 2.1: a consolidated multi-domain dia-logue dataset with state corrections and state track-in proceedings of the 12th lan-ing baselines.
guage resources and evaluation conference, pages422–428, marseille, france.
european language re-sources association..angela fan and claire gardent.
2020. multilingualamr-to-text generation.
in proceedings of the 2020conference on empirical methods in natural lan-guage processing (emnlp), pages 2889–2901, on-line.
association for computational linguistics..orhan firat, kyunghyun cho, and yoshua bengio.
2016. multi-way, multilingual neural machine trans-in pro-lation with a shared attention mechanism.
ceedings of the 2016 conference of the north amer-ican chapter of the association for computationallinguistics: human language technologies, pages866–875, san diego, california.
association forcomputational linguistics..claire gardent, anastasia shimorina, shashi narayan,and laura perez-beltrachini.
2017. the webnlgchallenge: generating text from rdf data.
in pro-ceedings of the 10th international conference onnatural language generation, pages 124–133, san-tiago de compostela, spain.
association for compu-tational linguistics..sebastian gehrmann, falcon dai, henry elder, andalexander rush.
2018. end-to-end content and planin proceed-selection for data-to-text generation.
ings of the 11th international conference on natu-ral language generation, pages 46–56, tilburg uni-versity, the netherlands.
association for computa-tional linguistics..natural.
raghav goyal, marc dymetman, and eric gaussier.
language generation through2016.character-based rnns with ﬁnite-state prior knowl-in proceedings of coling 2016, the 26thedge.
international conference on computational linguis-tics: technical papers, pages 1083–1092, osaka,japan.
the coling 2016 organizing committee..hiroaki hayashi, yusuke oda, alexandra birch, ioan-nis konstas, andrew finch, minh-thang luong,graham neubig, and katsuhito sudoh.
2019. find-ings of the third workshop on neural generation andin proceedings of the 3rd workshoptranslation.
on neural generation and translation, pages 1–14,hong kong.
association for computational linguis-tics..melvin johnson, mike schuster, quoc v. le, maximkrikun, yonghui wu, zhifeng chen, nikhil thorat,.
zdenˇek kasner and ondˇrej duˇsek.
2020. train hard,ﬁnetune easy: multilingual denoising for rdf-to-text generation.
in proceedings of the 3rd interna-tional workshop on natural language generationfrom the semantic web (webnlg+), pages 171–176,dublin, ireland (virtual).
association for computa-tional linguistics..taku kudo and john richardson.
2018. sentencepiece:a simple and language independent subword tok-enizer and detokenizer for neural text processing.
inproceedings of the 2018 conference on empiricalmethods in natural language processing: systemdemonstrations, pages 66–71, brussels, belgium.
association for computational linguistics..yinhan liu, jiatao gu, naman goyal, xian li, sergeyedunov, marjan ghazvininejad, mike lewis, andluke zettlemoyer.
2020. multilingual denoisingpre-training for neural machine translation.
transac-tions of the association for computational linguis-tics, 8:726–742..minh-thang luong, quoc v. le,.
ilya sutskever,oriol vinyals, and lukasz kaiser.
2016. multi-in 4th inter-task sequence to sequence learning.
national conference on learning representations,iclr 2016, san juan, puerto rico, may 2-4, 2016,conference track proceedings..arya d. mccarthy, ekaterina vylomova, shijie wu,chaitanya malaviya, lawrence wolf-sonkin, gar-rett nicolai, christo kirov, miikka silfverberg, sab-rina j. mielke, jeffrey heinz, ryan cotterell, andmans hulden.
2019. the sigmorphon 2019shared task: morphological analysis in context andcross-lingual transfer for inﬂection.
in proceedingsof the 16th workshop on computational research inphonetics, phonology, and morphology, pages 229–244, florence, italy.
association for computationallinguistics..lesly miculicich, marc marone, and hany hassan.
2019. selecting, planning, and rewriting: a mod-ular approach for data-to-document generation andtranslation.
in proceedings of the 3rd workshop onneural generation and translation, pages 289–296,hong kong.
association for computational linguis-tics..simon mille, anya belz, bernd bohnet, thiago cas-tro ferreira, yvette graham, and leo wanner.
2020.the third multilingual surface realisation shared taskin pro-(sr’20): overview and evaluation results.
ceedings of the third workshop on multilingual sur-face realisation, pages 1–20, barcelona, spain (on-line).
association for computational linguistics..123nikola mrkˇsi´c, diarmuid ´o s´eaghdha, tsung-hsienwen, blaise thomson, and steve young.
2017. neu-ral belief tracker: data-driven dialogue state track-ing.
in proceedings of the 55th annual meeting ofthe association for computational linguistics (vol-ume 1: long papers), pages 1777–1788, vancouver,canada.
association for computational linguistics..myle ott, sergey edunov, alexei baevski, angelafan, sam gross, nathan ng, david grangier, andfairseq: a fast, extensiblemichael auli.
2019.in proceedings oftoolkit for sequence modeling.
the 2019 conference of the north american chap-ter of the association for computational linguistics(demonstrations), pages 48–53, minneapolis, min-nesota.
association for computational linguistics..kishore papineni, salim roukos, todd ward, and wei-jing zhu.
2002. bleu: a method for automatic eval-in proceedings ofuation of machine translation.
the 40th annual meeting of the association for com-putational linguistics, pages 311–318, philadelphia,pennsylvania, usa.
association for computationallinguistics..maja popovi´c.
2015. chrf: character n-gram f-scorefor automatic mt evaluation.
in proceedings of thetenth workshop on statistical machine translation,pages 392–395, lisbon, portugal.
association forcomputational linguistics..ratish puduppully, jonathan mallinson, and mirellalapata.
2019. university of edinburgh’s submis-sion to the document-level generation and transla-in proceedings of the 3rd work-tion shared task.
shop on neural generation and translation, pages268–272, hong kong.
association for computa-tional linguistics..marco roberti, giovanni bonetta, rossella cancel-liere, and patrick gallinari.
2019. copy mechanismand tailored training for character-based data-to-textin machine learning and knowledgegeneration.
discovery in databases - european conference,ecml pkdd 2019, w¨urzburg, germany, septem-ber 16-20, 2019, proceedings, part ii, pages 648–664..abigail see, peter j. liu, and christopher d. manning.
2017. get to the point: summarization with pointer-generator networks.
in proceedings of the 55th an-nual meeting of the association for computationallinguistics (volume 1: long papers), pages 1073–1083, vancouver, canada.
association for computa-tional linguistics..rico sennrich, barry haddow, and alexandra birch.
2016. neural machine translation of rare wordswith subword units.
in proceedings of the 54th an-nual meeting of the association for computationallinguistics (volume 1: long papers), pages 1715–1725, berlin, germany.
association for computa-tional linguistics..shi-qi shen, yun chen, cheng yang, zhi-yuan liu,and mao-song sun.
2018. zero-shot cross-lingual.
neural headline generation.
dio, speech and lang.
proc., 26(12):2319–2327..ieee/acm trans.
au-.
anastasia shimorina and claire gardent.
2018. han-dling rare items in data-to-text generation.
inproceedings of the 11th international conferenceon natural language generation, pages 360–370,tilburg university, the netherlands.
association forcomputational linguistics..anastasia shimorina, elena khasanova, and clairegardent.
2019. creating a corpus for russian data-to-text generation using neural machine translationin proceedings of the 7th work-and post-editing.
shop on balto-slavic natural language processing,pages 44–49, florence, italy.
association for com-putational linguistics..matthew snover, bonnie dorr, richard schwartz, lin-nea micciulla, and john makhoul.
2006. a studyof translation edit rate with targeted human annota-tion.
in in proceedings of association for machinetranslation in the americas, pages 223–231..bayu distiawan trisedya, jianzhong qi, rui zhang,and wei wang.
2018. gtr-lstm: a triple encoderfor sentence generation from rdf data.
in proceed-ings of the 56th annual meeting of the associationfor computational linguistics (volume 1: long pa-pers), pages 1627–1637, melbourne, australia.
as-sociation for computational linguistics..xiaojun wan, huiying li, and jianguo xiao.
2010.cross-language document summarization based onmachine translation quality prediction.
in proceed-ings of the 48th annual meeting of the associationfor computational linguistics, pages 917–926, up-psala, sweden.
association for computational lin-guistics..yining wang, jiajun zhang, feifei zhai, jingfang xu,and chengqing zong.
2018. three strategies to im-prove one-to-many multilingual translation.
in pro-ceedings of the 2018 conference on empirical meth-ods in natural language processing, pages 2955–2960, brussels, belgium.
association for computa-tional linguistics..tsung-hsien wen, milica gaˇsi´c, nikola mrkˇsi´c, pei-hao su, david vandyke, and steve young.
2015.semantically conditioned lstm-based natural lan-guage generation for spoken dialogue systems.
inproceedings of the 2015 conference on empiricalmethods in natural language processing, pages1711–1721, lisbon, portugal.
association for com-putational linguistics..biao zhang, jing yang, qian lin, and jinsong su.
2018. attention regularized sequence-to-sequencelearning for e2e nlg challenge.
e2e nlg challengesystem descriptions..qi zhu, kaili huang, zheng zhang, xiaoyan zhu, andminlie huang.
2020. crosswoz: a large-scale chi-nese cross-domain task-oriented dialogue dataset.
transactions of the association for computationallinguistics, 8:281–295..1247 appendices.
a conﬁgurations.
the multilingual nlg and vape use a transformeras underlying architecture.
we use the fairseqtoolkit for our experiments (ott et al., 2019).
themodels are trained with shared embeddings, 8 at-tention heads, 6 layers, 512 hidden size, 2048 sizefor the feed forward layers.
we trained with 0.3dropout, adam optimiser with a learning rate of0.0005. the nlg are trained with early stoppingand patience set to 20. automatic value post editmodels are trained with the same conﬁguration butpatience was set to 6. for the copy mechanism-based model we use the eda-cs implementationprovided by roberti et al.
(2019) with the defaultconﬁguration.
due to its extremely high compu-tational training cost, the models are trained for15 epochs.
bpe and sentencepiece (kudo andrichardson, 2018) models are trained with a vocab-ulary size set to 12000 tokens..for all models in our experiments, the input con-sists of a simple linearisation of the mrs. partic-ularly, for the delexicalisation based models, thevalues are extended with their respective placehold-ers as shown in the following example: “entity 1meyer werft location entity 2 germany..b input examples.
figure 6 shows some examples of how, during train-ing, lad maps mr values to n-grams of the targetreference, based on the similarity of their represen-tations.
we can observe that these values could nothave been matched by exact and n-gram delexicali-sation as they constitute signiﬁcant paraphrases ofthe value..figure 4 and 5 show some additional exam-ples of delexicalisation and relexialisation for thevarious approaches from the webnlg challenge2020. table 8 shows more delexicalisation exam-ples from webnlg, multiwoz and crosswozdatasets, where we can observe the shortcomingsof exact and n-gram delexicalisation..c output examples.
table 9 and 10 present some examples for englishand russian output respectively.
the examplesinclude output from sentencepiece (sp), copy, andlad systems..figure 4: delexicalisation on webnlg challenge2020 with target output in english and russian.
doubleunderlining marks text missed by delexicalisation..figure 5: relexicalisation examples; double underlin-ing marks errors that ignore context..125inputmr:x1x2x3x4broadcastedbyﬁrstairedlastairedwhere:x1=bananaman|бананамен,x2=bbc|би_би_си,x3=1983_10_03,x4=1986_04_15goldtargetreferences:bananamanﬁrstairedonthebbconoctober3rd,1983andbroadcastitslastepisodeonapril15th,1986.бананаменвпервыевышелвэфирнаbbc3октября1983года,аегопоследнийэпизодвышел15апреля1986года.exactdelexicalisation:x1ﬁrstairedonthex2onoctober3rd,1983andbroadcastitslastepisodeonapril15th,1986.x1впервыевышелвэфирнаbbc3октября1983года,аегопоследнийэпизодвышел15апреля1986года.languageagnosticdelexicalisation(lad):x1ﬁrstairedonthex2onoctoberx3andbroadcastitslastepisodeonx4.x1впервыевышелвэфирнаx2x3года,аегопоследнийэпизодвышелx4года.inputmr:x1x2x3x4broadcastedbyﬁrstairedlastairedwhere:x1=bananaman|бананамен,x2=bbc|би_би_си,x3=1983_10_03,x4=1986_04_15goldtargetreferences:bananamanﬁrstairedonthebbconoctober3rd,1983andbroadcastitslastepisodeonapril15th,1986.бананаменвпервыевышелвэфирнаbbc3октября1983года,аегопоследнийэпизодвышел15апреля1986года.таблица1:delexicalisationandrelexicalisationstrategiesonenglishandrussianexamplesfromthewebnlg2020challenge.exactdelexicalisation:x1ﬁrstairedonthex2onoctober3rd,1983andbroadcastitslastepisodeonapril15th,1986.x1впервыевышелвэфирнаbbc3октября1983года,аегопоследнийэпизодвышел15апреля1986года.languageagnosticdelexicalisation(lad):x1ﬁrstairedonthex2onoctoberx3andbroadcastitslastepisodeonx4.x1впервыевышелвэфирнаx2x3года,аегопослед-нийэпизодвышелx4года.таблица2:delexicalisationandrelexicalisationstrategiesonenglishandrussianexamplesfromthewebnlg2020challenge.generatedoutputbeforerelexicalisation:(assumingtrainingwithlad)x1ﬁrstairedonthex2onoctoberx3andbroadcastitslastepisodeonx4.x1впервыевышелвэфирнаx2x3года,аегопослед-нийэпизодвышелx4года.exactrelexicalisation:bananamanﬁrstairedonthebbconoctober19831003andbroadcastitslastepisodeon19860415.бананаменвпервыевышелвэфирнабибиси19831003года,аегопоследнийэпизодвышел19860415года.automaticvaluepost-editing(vape):bananamanﬁrstairedonthebbconoctober3rd,1983andbroadcastitslastepisodeonapril15th,1986.бананаменвпервыевышелвэфирнабибиси3октяб-ря1983года,аегопоследнийэпизодвышел15апреля1986года.таблица3:delexicalisationandrelexicalisationstrategiesonenglishandrussianexamplesfromthewebnlg2020challenge.1table 8: dataset examples and delexicalisation output; double underlining marks text that was missed..figure 6: examples of lad’s value mapping to targetreference n-grams..table 9: output text from three different systems inenglish..126exampletakenfromthewebnlg2020dataset.mr(cid:104)x1=bananaman,broadcastedby,x2=bbc(cid:105)(cid:104)x1=бананамен,broadcastedby,x2=би_би_си(cid:105)(cid:104)x1=bananaman,ﬁrstaired,x3=1983_10_03(cid:105)(cid:104)x1=бананамен,ﬁrstaired,x3=1983_10_03(cid:105)(cid:104)x1=bananaman,lastaired,x3=1986_04_15(cid:105)(cid:104)x1=бананамен,lastaired,x4=1986_04_15(cid:105)referencebananamanﬁrstairedonthebbconoctober3rd,1983andbroadcastitslastepisodeonapril15th,1986.бананаменвпервыевышелвэфирнаbbc3октября1983года,аегопоследнийэпизодвышел15апреля1986года.exactx1ﬁrstairedonthex2onoctober3rd,1983andbroadcastitslastepisodeonapril15th,1986.x1впервыевышелвэфирнаbbc3октября1983года,аегопоследнийэпизодвышел15апреля1986года.ngramx1ﬁrstairedonthex2onoctober3rd,x3andbroadcastitslastepisodeonapril15thx4.x1впервыевышелвэфир3октябряx3года,аегопоследнийэпизодвышел15апреляx4года.ladx1ﬁrstairedonthex2onx3andbroadcastitslastepisodeonx4.x1впервыевышелвэфирнаx2x3года,аегопоследнийэпизодвышелx4года.exampletakenfromthemultiwozdataset.mrhotel-inform{type:x1=“guesthouse”,parking:none,choice:x2=“5”}booking-inform{none}referencethereare5guesthousesthathavefreeparking.shouldibookoneofthemforyou?exacttherearex2guesthousesthathavefreeparking.shouldibookoneofthemforyou?ngramtherearex2guesthousesthathavefreeparking.shouldibookoneofthemforyou?ladtherearex2x1thathavefreeparking.shouldibookoneofthemforyou?exampletakenfromthecrosswozdataset.mrattraction-inform{duration:x1=“1小时”,rating:x2=“5分”}attraction-request{name:none}reference吃完饭，我想去一个评分5分的景点，转上1个来小时，你能给我推荐一个吗exact吃完饭，我想去一个评分x2的景点，转上1个来小时，你能给我推荐一个吗ngram吃完饭，我想去一个评分x2的景点，转上1个来x1，你能给我推荐一个吗lad吃完饭，我想去一个评分x2的景点，转上x1，你能给我推荐一个吗table1:examplesofdatasetinstanceswithdiﬀerentdelexicalizationstrategies.1generatedoutputbeforerelexicalisation:(assumingtrainingwithlad)x1islocatedinx2wherex3istheleaderandthepeopleareknownasx4.x1находитсявx2,гделидеромявляетсяx3.местныежителиизвестныкакx4.exactrelexicalisation:amdavadnigufaislocatedinindiawheret.s.thakuristheleaderandthepeopleareknownasindianpeople.амдаваднигуфанаходитсявиндия,гделидеромяв-ляетсят.с.тхакур(муж).местныежителиизвестныкакиндийцы.automaticvaluepost-editing(vape):amdavadnigufaislocatedinindiawheret.s.thakuristheleaderandthepeopleareknownasindians.амдаваднигуфанаходитсявиндии,гделидеромяв-ляетсят.с.тхакур.местныежителиизвестныкакин-дийцами.таблица5:delexicalisationandrelexicalisationstrategiesonenglishandrussianexamplesfromthewebnlg2020challenge.target:aarhusairportislocatedintirstrup,partofthecentralregionofdenmarkwhichhasthecapitalcityofcopenhagen.value:centraldenmarkregioncosn-gram0.95thecentralregionofdenmark......0.73thecapitalcityofcopenhagen......0.25whichhasthetarget:alanshepardisdead.value:deceasedcosn-gram0.84dead......0.47alanshepard......0.40alanshepardisdead.goldtargetreferences:amdavadnigufaislocatedinindia,wheretheleaderiststhakurandthedemonymforpeoplelivingthereisindian.амдаваднигуфанаходитсявиндии,гделидерт.с.тхакурилюди,проживающиетам,называютсяиндий-цами.exactdelexicalisation:x1islocatedinx2,wheretheleaderiststhakurandthedemonymforpeoplelivingthereisindian.x1находитсявиндии,гделидерт.с.тхакурилюди,проживающиетам,называютсяиндийцами.languageagnosticdelexicalisation(lad):x1islocatedinx2,wheretheleaderisx3andthedemonymforpeoplelivingthereisx4.x1находитсявx2,гделидерx3илюди,проживающиетам,называютсяx4.таблица6:delexicalisationandrelexicalisationstrategiesonenglishandrussianexamplesfromthewebnlg2020challenge.2mr:htrane,revenue,1.0264e10ihtrane,netincome,5.563e8ihtrane,numberofemployees,29000isp:tranehasarevenueof$10,264,000,000,withanetincomeof$556,300,000andarevenueof$10,264,000,000.copy:trane,acompanywith29,000employees,has29,000employeesandwasconnectedat$556,300,000.lad:trane,whichhasarevenueof$10,264,000,000,hasanetincomeof$556,300,000andemploys29,000people.mr:hwilliam_anders,dateofretirement,"1969-09-01"ihwilliam_anders,occupation,fighter_pilotihwilliam_anders,birthplace,british_hong_kongihwilliam_anders,wasacrewmemberof,apollo_8isp:thebirthplaceofgreekborn,adonisgeorgiadis,isthecompany,ofwhichwasinoﬃceatthesametimethatmogenenenenenenenenville,newbritain,connecticut,isamemberoftheorderofpoalesandadivisionof45000kilometres.copy:williamanderswasborninbritishhongkongandhasacrewmewmemberoftheﬁghterpilot.thewasacrewmemberofthewasacrewmemberofthewasacrewmemberofthewasacrewmemberoflad:williamanders,whichwasfollowedby1st,1969andﬁghterpilot,wasborninbritishhongkongandhasbeenanumberofapollo8.
table 10: output text from three different systems inrussian..127mr:htrane,revenue,1.0264e10ihtrane,netincome,5.563e8ihtrane,numberofemployees,29000isp:tranehasarevenueof$10,264,000,000,withanetincomeof$556,300,000andarevenueof$10,264,000,000.copy:trane,acompanywith29,000employees,has29,000employeesandwasconnectedat$556,300,000.lad:trane,whichhasarevenueof$10,264,000,000,hasanetincomeof$556,300,000andemploys29,000people.mr:hwilliam_anders,dateofretirement,"1969-09-01"ihwilliam_anders,occupation,fighter_pilotihwilliam_anders,birthplace,british_hong_kongihwilliam_anders,wasacrewmemberof,apollo_8isp:thebirthplaceofgreekborn,adonisgeorgiadis,isthecompany,ofwhichwasinoﬃceatthesametimethatmogenenenenenenenenville,newbritain,connecticut,isamemberoftheorderofpoalesandadivisionof45000kilometres.copy:williamanderswasborninbritishhongkongandhasacrewmewmemberoftheﬁghterpilot.thewasacrewmemberofthewasacrewmemberofthewasacrewmemberofthewasacrewmemberoflad:williamanders,whichwasfollowedby1st,1969andﬁghterpilot,wasborninbritishhongkongandhasbeenanumberofapollo8.