towards user-driven neural machine translation.
huan lin1,2 liang yao3 baosong yang3 dayiheng liu3 haibo zhang3.
weihua luo3 degen huang4.
jinsong su1,2,5∗.
1school of informatics, xiamen university2institute of artiﬁcial intelligence, xiamen university3alibaba group 4dalian university of technology 5pengcheng lab, shenzhenhuanlin@stu.xmu.edu.cn.
{yaoliang.yl,yangbaosong.ybs,liudayiheng.ldyh,zhanhui.zhb}@alibaba-inc.com.
weihua.luowh@alibaba-inc.com.
huangdg@dlut.edu.cn.
jssu@xmu.edu.cn.
abstract.
a good translation should not only translatethe original content semantically, but also in-carnate personal traits of the original text.
fora real-world neural machine translation (nmt)system, these user traits (e.g., topic preference,stylistic characteristics and expression habits)can be preserved in user behavior (e.g., histor-ical inputs).
however, current nmt systemsmarginally consider the user behavior due to:1) the difﬁculty of modeling user portraits inzero-shot scenarios, and 2) the lack of user-behavior annotated parallel dataset.
to ﬁll thisgap, we introduce a novel framework calleduser-driven nmt.
speciﬁcally, a cache-basedmodule and a user-driven contrastive learningmethod are proposed to offer nmt the abilityto capture potential user traits from their histor-ical inputs under a zero-shot learning fashion.
furthermore, we contribute the ﬁrst chinese-english parallel corpus annotated with userbehavior called udt-corpus.
experimentalresults conﬁrm that the proposed user-drivennmt can generate user-speciﬁc translations.
1.
1.introduction.
in recent years, neural machine translation (nmt)models (sutskever et al., 2014; luong et al., 2015;vaswani et al., 2017) have shown promising qual-ity and thus increasingly attracted users.
whendrawing on a translation system, every user hashis own traits, including topic preference, stylisticcharacteristics, and expression habits, which canbe implicitly embodied in their behavior, e.g., thehistorical inputs of these users.
a good transla-tion should implicitly mirror user traits rather than.
∗ jinsong su is the corresponding author.
this workwas done when huan lin was interning at damo academy,alibaba group..1we release our source code and the associated bench-https://github.com/deeplearnxmu/.
at.
markuser-driven-nmt..figure 1: an example in which user traits leads tosynonymous yet stylistically different translations..merely translate the original content, as the exam-ple shown in figure 1. however, current nmtmodels are mainly designed for the semantic trans-formation between the source and target sentencesregardless of subtle traits with respect to user be-havior.
it can be said that the effect of user behavioron translation modeling is still far from utilization,which, to some extent, limits the applicability ofnmt models in real-world scenarios..more recently, several studies have shown thatthe prominent signals in terms of personal char-acteristics can be served as inductive biases andreﬂected in translation results using domain adapta-tion approaches, such as personality (mirkin et al.,2015), gender (rabinovich et al., 2017), and po-liteness (sennrich et al., 2016a).
however, previ-ously explored signals characterize users from asingle dimension, which insufﬁciently representﬁne-grained user traits.
furthermore, michel andneubig (2018) pay their attention to personalizedted talk translation, in which they train a speaker-speciﬁc bias to revise the prediction distribution.
incontrast with these studies, our work investigates amore realistic online scenario: a real-world mt sys-tem serves extensive users, where the user-behaviorannotated data covering all users is unavailable.
previous methods (mirkin et al., 2015; michel andneubig, 2018) require the users in the training setand the test set to be consistent, therefore can not.
proceedingsofthe59thannualmeetingoftheassociationforcomputationallinguisticsandthe11thinternationaljointconferenceonnaturallanguageprocessing,pages4008–4018august1–6,2021.©2021associationforcomputationallinguistics4008that is amazing !cool !
[cheerful, outgoing, active][polite, formal, gentle]太棒了！太棒了！abdeal with this zero-shot issue..starting from this concern, we explore user-driven nmt that generates personalized transla-tions for users unseen in the training dataset ac-cording to their behavior.
speciﬁcally, we choosethe historical inputs to represent user behaviorsince they can not only be easily obtained in thereal-world scenarios, but also reﬂect the topicpreference, stylistic characteristic, and context ofuser.
moreover, compared with pre-deﬁned or user-speciﬁc labels, historical inputs can be updatedwith current source sentences, which is also in linewith realistic scenario..in this work, we propose a novel framework forthis task, where the nmt model is equipped witha cache module to restore and update historicalinputs.
besides, in order to further transfer thetraits from the seen users to the unseen ones, wedesign a regularization framework based on con-trastive learning (bose et al., 2018; yang et al.,2019), which forces our model to decrease thedivergence between translations of similar userswhile increasing the diversity on dissimilar users.
in order to further train and assess the pro-posed framework, we construct a new user-drivenmachine translation dataset called udt-corpus.
this corpus consists of 6,550 users with totally57,639 chinese sentences collected from a real-world online mt system.
among them, 17,099chinese sentences are annotated with their englishtranslations by linguistic experts according to theuser-speciﬁc historical inputs.
experimental resultsdemonstrate that the proposed framework facili-tates the translation quality, and exactly generatesdiverse translations for different users..to summarize, major contributions of our work.
are four-fold:.
• we introduce and explore user-driven nmttask that leverages user behavior to enhancetranslation model.
we hope our study canattract more attention to explore techniqueson this topic..• we propose a novel framework for user-drivennmt based on cache module and contrastivelearning, which is able to model user traits inzero-shot scenarios..• we collect udt-corpus and make it publiclyavailable, which may contribute to the subse-quent researches in the communities of nmtand user-driven models..• extensive analyses indicate the effectiveness.
of our work and verify that nmt can proﬁtfrom user behavior to generate diverse trans-lations conforming to user traits..2 related work.
this section mainly includes the related studiesof personalized machine translation, cache-basednmt and contrastive learning for nmt..personalized machine translation recently,some researchers have employed domain adapta-tion (zhang et al., 2019; gururangan et al., 2020;yao et al., 2020) to generate personalized transla-tions.
for example, mirkin et al.
(2015) show thatthe translation generated by the smt model has anadverse effect on the prediction of author personal-ities, demonstrating the necessity of personalizedmachine translation.
furthermore, sennrich et al.
(2016a) control the politeness in the translation byadding a politeness label on the source side.
rabi-novich et al.
(2017) explore a gender-personalizedsmt system that retains the original gender traits.
these domain labels represent users in single di-mension separately, which are insufﬁcient to distin-guish large-scale users in a ﬁne-grained way.
themost correlated work to ours is michel and neu-big (2018) which introduces a speaker-speciﬁc biasinto the conventional nmt model.
however, thesemethods are unable to deal with users unseen atthe training time.
different from them, user-drivennmt can generate personalized translations forthese unseen users in a zero-shot manner..cache-based machine translation inspired bythe great success of cache on language model-ing (kuhn and de mori, 1990; goodman, 2001;federico et al., 2008), nepveu et al.
(2004) proposea cache-based adaptive smt system.
tiedemann(2010) explore a cache-based translation modelthat ﬁlls the cache with bilingual phrase pairs ex-tracted from previous sentence pairs in a document.
bertoldi et al.
(2013) use a cache mechanism toachieve online learning in phrase-based smt.
gonget al.
(2011), kuang et al.
(2018), and tu et al.
(2018) further exploit cache-based approaches toleverage contextual information for document-levelmachine translation.
contrast with the document-level nmt that learns to capture contextual infor-mation, our study aims at modeling user traits, suchas, topic preference, stylistic characteristics, and ex-pression habits.
moreover, historical inputs of userhas relatively fewer dependencies than the contexts.
4009used in document-level translation..contrastive learning for nmt contrastivelearning has been extensively applied in the com-munities of computer vision and natural languageprocessing due to its effectiveness and generalityon self-supervised learning (vaswani et al., 2013;mnih and kavukcuoglu, 2013; liu and sun, 2015;bose et al., 2018).
towards raising the ability ofnmt in capturing global dependencies, wisemanand rush (2016) ﬁrst introduce contrastive learn-ing into nmt, where the ground-truth translationand the model output are considered as the positiveand contrastive samples, respectively.
yang et al.
(2019) construct contrastive examples by deletingwords from ground-truth translation to reduce wordomission errors in nmt.
contrast to these studies,we employ contrastive learning to create broaderlearning signals for our user-driven nmt model,where the prediction distribution of translationswith respect to similar users and dissimilar usersare considered as positive and contrastive samples,respectively.
thus, our model can better transferthe knowledge of the seen users to the unseen ones..3 user-driven translation dataset.
in order to build a user-driven nmt system, weconstruct a new dataset called udt-corpus con-taining 57,639 inputs of 6,550 users, 17,099 amongthem are chinese-to-english translation examples..3.1 data collection and preprocessingwe collect raw examples from alibaba translate2which contain the user inputs and the translationsgiven by the translation system..for data preprocessing, we ﬁrst anonymize dataand perform data deduplication within each user.
then, we utilize a pre-trained n-gram languagemodel kenlm3 to ﬁlter out translation exampleswith low-quality source data.
moreover, we removesuch pairs whose source sentence is shorter than 2words or longer than 100 words..3.2 data annotation.
in the corpus, we represent each translation ex-ample as a triplet (cid:104)x (u), y (u), h (u)(cid:105), where h (u)is the historical inputs of the user u, x (u) is thecurrent source sentence and y (u) is the target trans-lation sentence annotated with h (u).
to obtain.
2https://www.aliyun.com/product/ai/.
base_alimt.
3https://github.com/kpu/kenlm..such a triplet, we ﬁrst sequentially sample up to 10source sentences which are the historical inputs ofeach user.
then, for the given historical inputs, wecollect their followed source input paired with thepseudo translation given by the translation system.
afterwards, we assign these historical inputs andthe current input pairs to two professional anno-tators and ask them to revise the pseudo transla-tion according to the source sentence and historicalinputs.
speciﬁcally, we ﬁrst ask one of them toannotate and the other to evaluate, and then resolveannotation disagreements by reviewing.
duringannotation, 91.8% of the original data are revised.
moreover, annotators are asked to record whethertheir revision is affected by user history.
the resultshows that 76.25% of the sentences are impacted..4 user-driven nmt framework.
in this section, we ﬁrst give a brief descriptionabout the problem formulation of user-driven nmt,and then introduce our proposed framework in de-tail.
we choose transformer (vaswani et al., 2017)as the basic nmt model due to its competitive per-formance.
in fact, our framework is transparentand applicable to other nmt models..figure 2 illustrates the basic framework of theproposed user-driven nmt.
most typically, weequip the nmt model with two user-speciﬁccaches to exploit user behavior for better translation(see section § 4.2).
besides, we augment the con-ventional nmt training objective with contrastivelearning, which allows the model to learn transla-tion diversity across users (see section § 4.3)..4.1 problem formulation.
(cid:16).
y(u)i.
<i , u; θ.
|x (u), y (u).
given the source sentence x and the previouslygenerated words y<i = y1, ..., yi−1, the conven-tional nmt model with parameter θ predicts thecurrent target word yi by p (yi|x, y<i; θ).
as asigniﬁcant extension of conventional nmt, user-driven nmt with parameter θ aims to model(cid:17)p, that is, generates thetranslation that can reﬂect the traits of user u. un-like previous studies (mirkin et al., 2015; micheland neubig, 2018) only caring for generating trans-lations for users seen at the training time, our user-driven nmt mainly focuses on a more realisticonline mt scenario, where the users for testingare unseen in the training dataset.
moreover, theconventional domain adaptation methods can notbe directly applied to this zero-shot scenario..4010t.and “corpus”.
speciﬁcally, when constructing topiccache c(u), we treat the historical inputs h (u) ofthe user u as the “document” and the historicalinputs h (u) of all users u as the “corpus”, thendeﬁne topic cache c(u)as an embedding sequenceof historical keywords.
unlike the topic cache,for context cache c(u), we individually considerthe current source sentence x (u) and historical in-puts h (u) as the tf-idf “document” and “corpus”,deﬁning c(u)as an embedding sequence of currentckeywords..c.t.besides, in the real-world mt scenario, thereexists a large number of users without any historicalinput.
for these users, we ﬁnd the most similaruser according to the cosine similarity based ontheir tf-idf bag-of-word representations of topickeywords, and initialize the corresponding topiccache with that of the most similar user..updating caches when using an online mt sys-tem, users often continuously input multiple sen-tences.
thus, our caches should be dynamicallyupdated to ensure the accurate encoding of userbehavior..to update topic cache, we ﬁrst recalcualte thetf-idf values of all historical input words, so as toredetermine the keywords stored in this cache.
asfor context cache, we consider it as a ﬁlter windowsliding across historical inputs, and apply ﬁrst-in-ﬁrst-out rule to replace its earliest keywords withthe recently input ones..c.and c(u).
reading from caches during the translation ofthe nmt model, we perform a gating operation onc(u), producing a vector r(u) that reﬂectstuser behavior as follows:r(u) = αc(u)α = sigmoid(wtc(u)(cid:104)c(u)t = meanpooling.
ct + wrc(u)(cid:105).
t + (1 − α)c(u).
c(u)t.c ),.
(1).
(2).
(3).
,.
c(u)c = meanpooling.
c(u)c.(cid:104).
(cid:105).
,.
(4).
where both wt and wr are learnable parameter ma-trices.
then, we directly add r(u) into the embed-ding sequence of original current source sentencex (u), forming a source embedding sequence withuser behavior as follows:.
ˆx (u) = {x(u).
i + r(u)}1≤i<|x (u)|..(5).
finally, the nmt model is fed with ˆx(u) to gen-erate the translation for u. due to the limitation.
figure 2: the architecture of our user-driven nmtmodel.
we use the topic cache and context cache to cap-ture the long-term and short-term user traits for user ufrom corresponding historical inputs h (u), respectively.
then, we combine the representations of two caches toget a user behavior representation r(u), which is fedinto the nmt model for personalized translation.
fur-thermore, we use contrastive learning involving similaruser u+ and dissimilar user u− to increase the transla-tion diversity among different users..4.2 cache-based user behavior modeling.
due to the advantages of cache mechanism on dy-namic representations (gong et al., 2011; kuanget al., 2018; tu et al., 2018), we equip the con-ventional transformer-based nmt model with twouser-speciﬁc caches to leverage user behavior fornmt: 1) topic cache c(u)that aims at capturingtthe global and long-term traits of user u; and 2)context cache c(u), which is introduced to capturethe short-term traits from the recent source inputsof user u. during this process, we focus on thefollowing three operations on cache:.
c.cache representation in order to facilitate theefﬁcient computation of the user behavior encodedby our caches, we deﬁne each cache as an em-bedding sequence of keywords.
we ﬁrst calculatetf-idf values of input words, and then extractwords with tf-idf weights higher than a prede-ﬁned threshold to represent user behavior..note that the calculation of tf-idf value of aword mainly depends on its frequency in the docu-ment and inverse document frequency in the corpus.
since two caches play different roles in the user-driven nmt model, we identify keywords for twocaches based on different deﬁnitions of “document”.
4011topic cache𝐜!
(#)historical inputs⊕context cache𝐜%(#)𝐻(#)𝑟(")𝑟("!)𝑟("")𝑋(")𝑌(")𝑃(𝑦!"|𝑋",𝑌#!",𝐻")𝑃(𝑦!"|𝑋",𝑌#!",𝐻"!)𝑃(𝑦!"|𝑋",𝑌#!
",𝐻"")𝐿$%+𝐿&%’user-driven nmt modelof pages, we omit the detailed descriptions of thenmt model.
please refer to vaswani et al.
(2017)for the details..4.3 model training with a contrastive lossgiven training instances (cid:104)x (u), y (u), h (u)(cid:105), wetrain the user-driven nmt model using the follow-ing objective function:.
l = lmle + lcl..(6).
here, lmle is the maximum likelihood translationloss extended from the conventional nmt trainingobjective.
formally, it is deﬁned as:.
lmle =.
− log p (y(u).
i.
|x (u), y (u).
<i , h (u); θ)..(cid:88).
i.
(7)lclis a triplet-margin-based constrastive loss,which allows the nmt model to learn the trans-lation diversity across users..speciﬁcally, for an input sentence, an ideal user-driven nmt model should be able to generate trans-lations with non-divergent user traits for similarusers, while producing translations with diverseuser traits for dissimilar users.
however, usingonly lmle cannot guarantee this since it separatelyconsiders each training instance during the modeltraining.
to deal with this issue, for each traininginstance (cid:104)x (u), y (u), h (u)(cid:105), we ﬁrst determine themost similar user u+ according to the cosine sim-ilarity based on their bag-of-keyword representa-tions, and randomly select a user without any samekeyword as the dissimilar user u− of u. finally,using historical inputs of u+ and u−, we constructseveral pseudo training instances to deﬁne lcl asfollows:.
(cid:88).
lcl =.
max[d(x (u), y (u), h (u), h (u+)) (8).
u∈u− d(x (u), y (u), h (u), h (u−)) + η, 0],.
where d.(cid:16).
x (u), y (u), h (u), h (u+)(cid:17).
= ||.
1|y (u)|.
−.
1|y (u)|.
i(cid:88).
i.
(cid:88).
log p.(cid:16).
y(u)i.
|x (u), y (u).
<i , h (u)(cid:17).
log p.(cid:16).
y(u)i.
|x (u), y (u).
<i , h (u+)(cid:17).
||2.
#user#historical input#current sentence pairs.
train5,35033,44114,006.dev6003,6291,557.test6003,4701,536.table 1: dataset for ﬁne-tuning experiments....d.d.(cid:16).
(cid:16).
, which is similar to.
x (u), y (u), h (u), h (u−)(cid:17)x (u), y (u), h (u), h (u+)(cid:17)formally, lcl will encourage the nmt modelto minimize the prediction difference betweenthe training instances (cid:104)x (u), y (u), h (u)(cid:105) and(cid:104)x (u), y (u), h (u+)(cid:105), and maximize the differencebetween the training instances (cid:104)x (u), y (u), h (u)(cid:105)and (cid:104)x (u), y (u), h (u−)(cid:105).
in this way, the nmtmodel can not only exploit pesudo training in-stances, but also produce more consistent trans-lations with user traits..5 experiments.
in this section, we carry out several groups of ex-periments to investigate the effectiveness of ourproposed framework on udt-corpus..5.1 setup.
we develop the user-driven nmt model based onopen-nmt transformer (klein et al., 2017), andadopt a two-stage strategy to train this model: weﬁrst pre-train a transformer-based nmt modelon the wmt2017 chinese-to-english dataset, andthen ﬁne-tune this model to our user-driven nmtmodel using udt-corpus..datasets the wmt2017 chinese-to-englishdataset is composed of the news commentary v12,un parallel corpus v1.0, and cwmt corpora, withtotally 25m parallel sentences.
to ﬁne-tune ourmodel, we split udt-corpus into training, valida-tion and test set, respectively.
table 1 providesmore detailed statistics of these datasets.
to im-prove the efﬁciency of model training, we train themodel using only parallel sentences with no morethan 100 words.
following common practices, weemploy byte pair encoding (sennrich et al., 2016b)with 32k merge operations to deal with all sen-tences..and η is a predeﬁned threshold, which is set to 2 inour experiments.
here, we omit the deﬁnition of.
(9).
training details following vaswani et al.
(2017), we use the following hyper-parameters: theword embedding dimension is set to 512, the hid-den layer dimension is 2048, the layer numbers of.
4012both encoder and decoder are set to 6, and the num-ber of attention heads is set to 8. besides, we use4 gpus for training.
at the pre-training stage, weemploy the adam optimizer with β2 = 0.998. weuse the batch size of 16,384 tokens and pre-trainthe model for 200,000 steps.
particularly, we adoptthe dropout strategy (srivastava et al., 2014) withrate 0.1 to enhance the robustness of our model.
when ﬁne-tuning the model, we keep the othersettings consistent with the pre-training stage, butreduce the batch size to 2048 tokens and ﬁne-tunethe model with early-stopping strategy..evaluation we assess the translation qualitywith two metrics: one is case-insensitive bleu(mteval-v13a.pl, papineni et al., 2002)4 and theother is meteor5 (denkowski and lavie, 2011)..5.2 baselines.
we represent our user-driven nmt model as ud-nmt and compare it with the following baselines:• tf.
it is a transformer-based nmt model pre-trained on the wmt2017 corpus.
this modelyields 24.61 bleu score on wmt2017chinese-to-english translation task, whichis comparable with reported results in (wanet al., 2020; zhou et al., 2020), which makesour subsequent experiments convincing..• tf-ft. this model is also a transformer-based nmt model that is further ﬁne-tunedon the parallel sentences of udt-corpus.
• tf-ft + pesudata.
this model is a variantof tf-ft. when constructing it, we pair his-torical inputs with their translations producedby our online translation system, forming ad-ditional data for ﬁne-tuning tf-ft..• tf-ft + conchist (tiedemann and scher-rer, 2017).
in this model, we introduce userbehavior into tf-ft by concatenating eachinput sentence with several historical inputs.
we mark all tokens in historical inputs witha special preﬁx to indicate that they are addi-tional information..• tf-ft + userbias (michel and neubig,2018).
it introduces user-speciﬁc biases toreﬁne softmax-based predictions of trans-former nmt model.
we change it to a zero-shot method similar to (farajian et al., 2017).
4https://github.com/moses-smt/mosesdecoder/blob/master/scripts/generic/multi-bleu.perl.
5https://github.com/cmu-mtlab/meteor.
figure 3: effects of cache size on translation quality..model.
bleu meteor.
w/o user behavior.
tftf-fttf-ft + pesudata.
tf-ft + conchisttf-ft + userbiasud-nmt.
27.5228.6129.02.
30.8531.3632.35.w/ user behavior.
44.0545.3545.40.
46.0846.7948.20.table 2: main results on udt-corpus.
“w/o”, “w/”denote “without” and “with”, respectively..since (michel and neubig, 2018) can not bedirectly applied to our scenario.
in particular,we replace the user id in the test set with thatof the most similar user in the training set.
note that the ﬁrst two baselines, e.g., tf and tf-ft, are conventional nmt models without exploit-ing user behavior..5.3 effect of cache sizes.
since cache size directly determines the utility ofuser behavior, we investigate its effect on the perfor-mance of ud-nmt.
we denote the sizes of topiccache and context cache as st and sc for simplicity.
figure 3 lists the performance of our model withdifferent st and sc on validation set.
we observethat st larger than 25 and sc larger than 35 do notlead to signiﬁcant improvements.
for this result,we speculate that small cache sizes are unable tocapture sufﬁcient user behavior for nmt.
however,since the number of keywords are limited, largercache sizes only bring limited information gain.
therefore, we directly use st = 25 and sc = 35 inthe subsequent experiments..5.4 main results.
from table 2, we observe that our ud-nmt modelconsistently outperforms all baselines in terms oftwo metrics.
moreover, we draw several interestingconclusions:.
4013515253545bleu33.032.932.832.732.632.532.432.3515253545bleu(a) topic cache size 𝒔𝒕(𝒔𝒄=35)(b) context cache size 𝒔𝒄(𝒔𝒕=25)33.032.932.832.732.632.532.432.3modelud-nmtw/o topic cachew/o context cachew/o similar user initializationw/o contrastive learning.
bleu↑ meteor↑32.3531.88†31.86†32.0232.00.
48.2048.0047.84†48.1448.09.s-bleu↑ d-bleu↑.
32.17–31.94†31.86†31.88†.
32.23–31.58†31.13‡31.94.s-sim.↓ d-sim.↓93.18–88.6193.54†93.49†.
80.10–69.3280.1681.59†.
table 3: ablation study.
↑: higher is better, ↓: lower is better.
since the user similarity is calculated based onthe topic keywords, the model can not ﬁnd similar user and dissimilar user without it.
thus w/o topic cache doesnot have the s-bleu, s-sim., d-bleu and d-sim.. ‡/†: indicates the drop of translation quality is statisticallysigniﬁcant comparing to “ud-nmt” (p<0.01/0.05)..1) all nmt models leveraging user behavior sur-pass vanilla models, including tf, tf-ft, showingthat user behavior is useful for nmt..2) ud-nmt exhibits better than tf-ft + pesu-data, which uses the same training data as ours.
the underlying reason is that ud-nmt can lever-age user traits to generate better translations..3) although both tf-ft + userbias and ud-nmt exploit user behavior for nmt, ud-nmtachieves better performance than tf-ft + user-bias without introducing extra parameters.
thisresult demonstrates the advantage of cache on mod-eling user behavior than introducing user-speciﬁcbiases into model parameters..5.5 ablation study.
to explore the effectiveness of different compo-nents in our model, we further compare ud-nmtwith its several variants, as shown in table 3..particularly, we propose to evaluate translationsusing the following variant metrics: s-bleu, s-sim., d-bleu and d-sim.. when using s-bleu,we replace the topic cache of current user withthat of his most similar user.
keeping the samecurrent input, we calculate the bleu score withground-truth as reference and the translation forthis similar user as hypothesis.
as for s-sim., weadopt the same strategy as s-bleu, but use thetranslation for original user as reference to evaluatethe bleu score.
in other words, s-bleu and d-bleu assesses the translation quality given unsuit-able user.
therefore, higher s-bleu and d-bleuindicates better model robustness, while s-bleuand d-bleu measures how much the translationchanges given different user.
thus lower s-sim.
and d-sim.
show larger translation diversity.
our conclusions are shown as follows:1) w/o topic cache.
to build this variant, weremove topic cache from our model.
the result inline 2 indicates that removing topic cache leads to.
a performance drop, suggesting that topic cache isuseful for modeling user behavior..2) w/o context cache.
unlike the above variant,we only use topic cache to represent user traits inthis variant.
according to the results shown in line3, we observe that this change results in a signif-icant performance decline of our model, demon-strating that context cache also effectively capturesuser behavior for nmt.
however, the translationdiversity among users increases since the modelwill not be affected by the context cache in thisvariant, which is the same between different userswhen calculating s-sim.
and d-sim...3) w/o similar user initialization.
in this variant,we do not initialize topic caches of the users with-out historical inputs using that of the most similarusers.
from line 4, we observe that the perfor-mance of our model degrades without similar userinitialization..4) w/o contrastive learning..in this variant,we remove the contrastive learning from thewhole training objective to inspect the performancechange of our model.
as shown in line 4, theperformance of our model drops, proving that thecontrastive learning is important for the training ofour model..moreover, we can infer from column 6 and 7that our model can generate diverse translations.
speciﬁcally, the translations of dissimilar users haslarger diversity than that of similar ones.
further-more, we conclude that our model is robust, sinceit still performs well when we replace the topiccache of current user with those of other users (seecolumn 4 and 5)..5.6 analysis of contrastive margin.
inspired by yang et al.
(2019), we argue that thecontrastive learning may increase the prediction di-versity of our model between users compared withusing the mle loss.
to conﬁrm this, we randomly.
4014figure 4: two examples of user-driven machine translation..sample 300 examples from the training dataset, andcompute the following margin:.
∆ =.
(cid:104).
(cid:105)d(u+) (·)−d(u−) (·).
−.
(cid:104)mle (·)−d(u−)d(u+).
mle (·).
(cid:105).
,.
where d(u+)(·) is deﬁned in equation 9. the deﬁni-tion of d(u+)mle (·) is the same with d(·), the only dif-ference lies in that the nmt model is only trainedby the conventional mle loss.
we ﬁnd that d(·)has a larger margin than dmle(·) on 88% of sam-pled sentence pairs, with an average margin of 0.19.the results indicate again that the contrastive learn-ing increases the translation diversity..5.7 qualitative analysis.
in order to intuitively understand how our cachemodule exactly affects the translations, we feedour model with the same current source sentencebut different users, and display the 1-best transla-tions generated by our model.
as shown in thefigure 4 (a), our model is able to produce correctbut diverse translations according to different topiccaches.
moreover, it is interesting to observe thatspeciﬁc topic keywords such as “type b arr”, “neg-atively regulated” and “modulators” are translatedto synonymous but “out-of-domain” phrases if thetopic cache does not conform to input sentence.
onthe contrary, the model conversely generates “in-domain” translation if the topic cache comes fromthe same topic of input sentence..correlation orderud-nmt > tf-ft + pesudataud-nmt > tf-ft + userbias.
proportion86%74%.
table 4: the proportion of translations more related tohistorical inputs assessed by human translators.
a >b indicates the translations generated by a system ismore correlated to history inputs..besides, to further reveal the effect of user behav-ior, we provide an example in figure 4 (b), whichlists different translations by compared models forthe same inputs.
the historical inputs indicate thatthis user may be an apparel seller, since his histori-cal inputs contain the product titles and descriptionsof clothing.
thus, the keywords “wear resistant”in the source sentence are correlated with this user.
however, two baselines translate it to “waterproof”and “resistant”, respectively.
moreover, tf-ft+ userbias generates a subject–verb–object struc-tured sentence by adding the auxiliary verb “is”,which does not conform to the expression habit ofthe product title.
by contrast, with the hint of thekeywords in historical inputs, our ud-nmt is ableto produce suitable translation consistent with thetopic preference of this user..5.8 manual evaluation.
to further ﬁnd out weather the improvements ofour model are contributed by user traits, we ran-.
4015historical inputs面料成分氨纶风格性感款式连体裤颜色白色, 黑色(fabric composition spandex style sexy type jumpsuit color white, black)2020 秋冬季新款港风复古落肩外套女宽松学生毛绒短款上衣(2020 autumn and winter new hong kong style retro drop sleeves jacket female loose student plush crop top)src牛津纺面料防水耐磨, 15英寸大小refoxford fabric waterproof and wear resistant, 15 inch in sizetf-ft + pesudataoxford textile fabrics waterproof and waterproof, 15 inches sizetf-ft + userbiasoxford woven fabric iswaterproof and resistant, 15 inches in sizeud-ftoxford woven fabric waterproof and wear -resistant, 15 inch size(a) translations of ud-nmt given different user traits.
wordshighlighted in greenindicate the unnatural or awkward translations.
(b) translations of different models.
words highlighted in redare incorrect translations.
src基因芯片分析发现, 在crf多突变体中, 许多受b型arrs调控的基因同时也受crfs的调制剂起负反馈调节作用。refgene chip analysis found out that in multiple crf mutants, many genes regulated by type b arrswere also negatively regulated by the modulatorsof crfs.useratopic cache木马（trojan）| 渔夫（fisherman）|百洁布（cleaning cloth）| 耳机（earphone）|蓝牙（bluetooth）translationgene chip analysis found that in the crf mutant , many genes regulated by b arrsare also negatively fed by crfs toning.user btopic cache乙烯（ethylene）| 伸长（extension）| 促进（promote）| 基因（gene）|生长素（auxin）| 调控（regulate）translationgene chip analysis found that in the crf mutant , many genes regulated by type b arrsare also subject to negative feedback adjustmentby crfs modulatingagents .
domly sample 100 examples from the test datasetand ask the linguist experts to sort different systemsaccording to the relevance between the generatedtranslations and the historical input.
the results intable 4 show that our model can generate transla-tions more in line with history inputs than baselinemodels in most cases, proving that our method canmake better use of user traits..6 conclusion.
we propose user-driven nmt task, which aimsto leverage user behavior to generate personalizedtranslations.
with the help of cache module andcontrastive estimation, we successfully build anend-to-end nmt model that is able to capturepotential user traits from their historical inputsand generate diverse translations under a zero-shotlearning fashion.
furthermore, we contribute udt-corpus, which is the ﬁrst chinese-english parallelcorpus annotated with user behavior.
we expectour study can attract more attention towards thistopic.
it is a promising direction to explore otherbehavior in future, such as clickthrough and editingoperations.
moreover, following recent advance-ments in domain adaptation for nmt, we plan tofurther improve our model via adversial trainingbased knowledge transfer (zeng et al., 2018; yaoet al., 2020; su et al., 2021) and dual knowledgetransfer (zeng et al., 2019)..acknowledgments.
the project was supported by national key re-search and development program of china (no.
2020aaa0108004 and no.
2018yfb1403202),national natural science foundation of china (no.
61672440), natural science foundation of fujianprovince of china (no.
2020j06001), youth inno-vation fund of xiamen (no.
3502z20206059), andthe fundamental research funds for the centraluniversities (no.
zk20720200077).
we also thankthe reviewers for their insightful comments..references.
nicola bertoldi, mauro cettolo, and marcello federico.
2013. cache-based online adaptation for machinetranslation enhanced computer assisted translation.
in proceedings of the 2013 machine translationsummit, pages 35–42..avishek joey bose, huan ling, and yanshuai cao.
in pro-2018. adversarial contrastive estimation.
ceedings of the 56th annual meeting of the asso-.
ciation for computational linguistics, pages 1021–1032..michael j. denkowski and alon lavie.
2011. meteor1.3: automatic metric for reliable optimization andevaluation of machine translation systems.
in pro-ceedings of the 6th workshop on statistical machinetranslation, pages 85–91..m. amin farajian, marco turchi, matteo negri, andmarcello federico.
2017. multi-domain neural ma-chine translation through unsupervised adaptation.
in proceedings of the second conference on ma-chine translation, pages 127–137..marcello federico, nicola bertoldi, and mauro cettolo.
2008. irstlm: an open source toolkit for handlinglarge scale language models.
in proceedings of the9th annual conference of the international speechcommunication association, pages 1618–1621..zhengxian gong, min zhang, and guodong zhou.
2011. cache-based document-level statistical ma-chine translation.
in proceedings of the 2011 con-ference on empirical methods in natural languageprocessing, pages 909–919..joshua t. goodman.
2001. a bit of progress in lan-guage modeling.
computer speech and language,15(4):403–434..suchin gururangan, ana marasovic,.
swabhaswayamdipta, kyle lo, iz beltagy, doug downey,and noah a. smith.
2020. don’t stop pretraining:adapt language models to domains and tasks.
inthethe 58th annual meeting ofproceedings ofassociation for computational linguistics, pages8342–8360..guillaume klein, yoon kim, yuntian deng, jean senel-lart, and alexander rush.
2017. opennmt: open-source toolkit for neural machine translation.
in pro-ceedings of the 55th annual meeting of the associa-tion for computational linguistics, pages 67–72..shaohui kuang, deyi xiong, weihua luo, andguodong zhou.
2018. modeling coherence forneural machine translation with dynamic and topicin proceedings of the 27th internationalcaches.
conference on computational linguistics, pages596–606..roland kuhn and renato de mori.
1990. a cache-based natural language model for speech recogni-ieee transactions on pattern analysis andtion.
machine intelligence, 12(6):570–583..yang liu and maosong sun.
2015. contrastive unsu-pervised word alignment with non-local features.
inproceedings of the 29th association for the advance-ment of artiﬁcial intelligence, pages 2295–2301..thang luong, hieu pham, and christopher d. man-ning.
2015. effective approaches to attention-basedin proceedings of theneural machine translation.
2015 conference on empirical methods in naturallanguage processing, pages 1412–1421..4016paul michel and graham neubig.
2018. extreme adap-tation for personalized neural machine translation.
in proceedings of the 56th annual meeting of the as-sociation for computational linguistics, pages 312–318..ilya sutskever, oriol vinyals, and quoc v. le.
2014.sequence to sequence learning with neural net-in proceedings of the 28th annual con-works.
ference on neural information processing systems,pages 3104–3112..shachar mirkin, scott nowson, caroline brun, andjulien perez.
2015. motivating personality-awarein proceedings of the 2015machine translation.
conference on empirical methods in natural lan-guage processing, pages 1102–1108..j¨org tiedemann.
2010. context adaptation in statisticalmachine translation using models with exponentiallydecaying cache.
in proceedings of the 2010 work-shop on domain adaptation for natural languageprocessing, pages 8–15..andriy mnih and koray kavukcuoglu.
2013. learningword embeddings efﬁciently with noise-contrastiveestimation.
in proceedings of the 27th annual con-ference on neural information processing systems,pages 2265–2273..laurent nepveu, guy lapalme, philippe langlais, andgeorge f. foster.
2004. adaptive language andtranslation models for interactive machine transla-in proceedings of the 2004 conference ontion.
empirical methods in natural language processing,pages 190–197..kishore papineni, salim roukos, todd ward, and wei-jing zhu.
2002. bleu: a method for automatic eval-uation of machine translation.
in proceedings of the40th annual meeting of the association for compu-tational linguistics, pages 311–318..ella rabinovich, raj nath patel, shachar mirkin, lu-personal-cia specia, and shuly wintner.
2017.ized machine translation: preserving original authortraits.
in proceedings of the 15th conference of theeuropean chapter of the association for computa-tional linguistics, pages 1074–1084..rico sennrich, barry haddow, and alexandra birch.
2016a.
controlling politeness in neural machinein proceedings oftranslation via side constraints.
the 2016 conference of the north american chap-ter of the association for computational linguistics,pages 35–40..rico sennrich, barry haddow, and alexandra birch.
2016b.
neural machine translation of rare wordswith subword units.
in proceedings of the 54th an-nual meeting of the association for computationallinguistics, pages 1715–1725..nitish srivastava, geoffrey hinton, alex krizhevsky,ilya sutskever, and ruslan salakhutdinov.
2014.dropout: a simple way to prevent neural networksfrom overﬁtting.
the journal of machine learningresearch, 15(1):1929–1958..jinsong su,.
jiali zeng,.
jun xie, huating wen,yongjing yin, and yang liu.
2021. exploring dis-criminative word-level domain contexts for multi-ieee trans.
domain neural machine translation.
pattern anal.
mach.
intell., 43:1530–1545..j¨org tiedemann and yves scherrer.
2017. neural ma-chine translation with extended context.
in proceed-ings of the 3rd workshop on discourse in machinetranslation, pages 82–92..zhaopeng tu, yang liu, shuming shi, and tong zhang.
2018. learning to remember translation history witha continuous cache.
transactions of the associationfor computational linguistics, 6:407–420..ashish vaswani, noam shazeer, niki parmar, jakobuszkoreit, llion jones, aidan n. gomez, lukaszkaiser, and illia polosukhin.
2017. attention is allyou need.
in proceedings of the 31th annual con-ference on neural information processing systems,pages 5998–6008..ashish vaswani, yinggong zhao, victoria fossum, anddavid chiang.
2013. decoding with large-scale neu-in pro-ral language models improves translation.
ceedings of the 2013 conference on empirical meth-ods in natural language processing, pages 1387–1392..yu wan, baosong yang, derek f. wong, yikai zhou,lidia s. chao, haibo zhang, and boxing chen.
2020. self-paced learning for neural machine trans-in proceedings of the 2020 conference onlation.
empirical methods in natural language processing,pages 1074–1080..sam wiseman and alexander m. rush.
2016.sequence-to-sequence learning as beam-search opti-mization.
in proceedings of the 2016 conference onempirical methods in natural language processing,pages 1296–1306..zonghan yang, yong cheng, yang liu, and maosongsun.
2019. reducing word omission errors in neu-ral machine translation: a contrastive learning ap-proach.
in proceedings of the 57th conference of theassociation for computational linguistics, pages6191–6196..liang yao, baosong yang, haibo zhang, boxingchen, and weihua luo.
2020. domain transferbased data augmentation for neural query translation.
in proceedings of the 28th international confer-ence on computational linguistics, coling 2020,barcelona, spain (online), december 8-13, 2020,pages 4521–4533.
international committee on com-putational linguistics..4017jiali zeng, yang liu, jinsong su, yubin ge, yaojie lu,iterative dualyongjing yin, and jiebo luo.
2019.domain adaptation for neural machine translation.
in proceedings of the 2019 conference on empiri-cal methods in natural language processing, pages845–855..jiali zeng, jinsong su, huating wen, yang liu, junxie, yongjing yin, and jianqiang zhao.
2018. multi-domain neural machine translation with word-levelin proceedings ofdomain context discrimination.
the 2018 conference on empirical methods in nat-ural language processing, brussels, belgium, octo-ber 31 - november 4, 2018, pages 447–457..xuan zhang, pamela shapiro, gaurav kumar, paul mc-namee, marine carpuat, and kevin duh.
2019. cur-riculum learning for domain adaptation in neural ma-chine translation.
in proceedings of the 2019 con-ference of the north american chapter of the asso-ciation for computational linguistics, pages 1903–1915..yikai zhou, baosong yang, derek f. wong, yu wan,and lidia s. chao.
2020. uncertainty-aware cur-riculum learning for neural machine translation.
inproceedings of the 58th annual meeting of the asso-ciation for computational linguistics, pages 6934–6944..4018