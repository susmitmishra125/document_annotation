deep differential ampliﬁer for extractive summarization.
ruipeng jia1,2, yanan cao1,2, fang fang1,2∗, yuchen zhou1,zheng fang1, yanbing liu1,2 and shi wang3∗1institute of information engineering, chinese academy of sciences2school of cyber security, university of chinese academy of sciences3institute of computing technology, chinese academy of sciences1,2{jiaruipeng, caoyanan, fangfang0703, zhouyuchen, fangzheng,liuyanbing}@iie.ac.cn3wangshi@ict.ac.cn.
abstract.
for sentence-level extractive summarization,there is a disproportionate ratio of selected andunselected sentences, leading to ﬂatting thesummary features when optimizing the clas-siﬁcation.
the imbalanced sentence classi-ﬁcation in extractive summarization is inher-ent, which can’t be addressed by data sam-pling or data augmentation algorithms easily.
in order to address this problem, we inno-vatively consider the single-document extrac-tive summarization as a rebalance problemand present a deep differential ampliﬁer frame-work to enhance the features of summary sen-tences.
speciﬁcally, we calculate and amplifythe semantic difference between each sentenceand other sentences, and apply the residualunit to deepen the differential ampliﬁer archi-tecture.
furthermore, the corresponding objec-tive loss of the minority class is boosted by aweighted cross-entropy.
in this way, our modelpays more attention to the pivotal informationof one sentence, that is different from previ-ous approaches which model all informativecontext in the source document.
experimen-tal results on two benchmark datasets showthat our summarizer performs competitivelyagainst state-of-the-art methods.
our sourcecode will be available on github..1.introduction.
single-document extractive summarization formssummary by copying and concatenating the mostimportant spans (usually sentences) in a document.
sentence-level summarization is a very challeng-ing task, because it arguably requires an in-depthunderstanding of the source document sentences,and current automatic solutions are still far fromhuman performance.
recent approaches frame thetask as a sequence labeling problem, taking advan-tage of the success of neural network architectures..∗ corresponding authors: fang fang and shi wang.
figure 1: rouge score for documents with differ-ent length.
the result is calculated on the test set ofcnn/dm and the trained model is based on bert..however, there are still two inherent obstacles forsentence-level extractive summarization:.
1) it should be detrimental to keep tangentialinformation (west et al., 2019).
the intuitive limi-tation of those approaches is that they always preferto model and retain all informative content from thesource document.
this goes against the fundamen-tal goal of summarization, which crucially needs toforget all but the “pivotal” information.
recently,the information bottleneck principle (tishby et al.,2000; west et al., 2019) is introduced to incorpo-rate a tradeoff between information selection andpruning.
length penalty and the topic loss (bazio-tis et al., 2019) are used in the autoencoding systemto augment the reconstruction loss.
however, thesemethods require external variables or augmenta-tive terms, without enhancing the representation ofpivotal information..2) imbalanced classes inherently result inmodels that have poor predictive performance,speciﬁcally for the minority class.
the distribu-tion of examples across the known classes can varyfrom a slight bias to a severe imbalance, wherethere is one example in the minority class fordozens of examples in the majority class.
for in-stance, according to the statistics on the popularsummarization dataset, only 7.33% sentences of.
proceedingsofthe59thannualmeetingoftheassociationforcomputationallinguisticsandthe11thinternationaljointconferenceonnaturallanguageprocessing,pages366–376august1–6,2021.©2021associationforcomputationallinguistics3661020304050numberofsentences2025303540rouge-1rouge-2rouge-lcnn/dm (hermann et al., 2015) are labeled as “1”and others are “0”, indicating whether this sentenceshould be selected as summary or not.
conversely,most machine learning algorithms for classiﬁcationpredictive models are designed and demonstratedon problems that assume an equal distribution ofclasses.
this means that a naive application of amodel may only focus on learning the character-istics of the abundant observations, neglecting theexamples from the minority class.
furthermore,as shown in figure 1, the rouge score graduallydeclines along with the number of sentences accu-mulating, since the valuable summary sentencesis generally a tiny minority (with the quantity of1-4), while more and more majority sentences willswamp the minority ones.
unfortunately, the imbal-ance in summarization is inherent, which can’t beaddressed by common data augmentation (he andma, 2013; asai and hajishirzi, 2020; min et al.,2020; zoph et al., 2019; xie et al., 2020), for thereis a rare inﬂuence on the 0/1 distribution by addingor deleting the entire document..these two obstacles are interrelated and inter-act with each other.
highlighting the pivotal in-formation will strengthen the unique semantic andweaken the common informative content.
addition-ally, a more balanced distribution would make mi-nority class more attractive.
if we can’t resolve thecategory imbalance problem in extractive summa-rization by data augmentation, how to make the mi-nority class more attractive?
inspired by the differ-ential ampliﬁer of analog electronics1, we proposea heuristic model, differsum, as shorthand for dif-ferential ampliﬁer for extractive summarizationto enhance the representation of the summary sen-tences.
speciﬁcally, we calculate and amplify thesemantic difference between each sentence andother sentences, by the subtraction operation.
theoriginal differential ampliﬁer consists of two termsand the second term is used to avoid making theﬁnal output zero.
in our model, we use the residualunit instead of the second term to make the archi-tecture deeper.
we further design a more appropri-ate objective function to avoid biasing the data, bymaking the loss of a minority much greater than themajority.
differsum shows superiority over otherextractive methods in two aspects: 1) enhancingthe representation of the pivotal information and 2)compensating the minority class and penalizing themajority ones..1https://en.wikipedia.org/wiki/differential ampliﬁer.
experimental results validate the effectiveness ofdiffersum.
the human evaluation also shows thatour model is better in relevance compared with oth-ers.
our contributions in this work are concludedas follows:.
• we propose a novel conceptualization of ex-tractive summarization as rebalance problem..• we introduce a heuristic approach, calculat-ing and amplifying the semantic representationof pivotal information by integrating both thedifferential ampliﬁer and residual learning..• our proposed framework has achieved superiorperformance compared with strong baselines..2 related work.
2.1 extractive summarization.
recent research work on extractive summariza-tion spans a large range of approaches.
theseworks usually instantiate their encoder-decoderarchitecture by choosing rnn (nallapati et al.,2017; zhou et al., 2018), transformer (wang et al.,2019; zhong et al., 2019b; liu and lapata, 2019;zhang et al., 2019b) or gnn (wang et al., 2020;jia et al., 2020b) as encoder, autoregressive (jad-hav and rajan, 2018; liu and lapata, 2019) orrl-based (narayan et al., 2018; arumae and liu,2018; luo et al., 2019) decoders.
for two-stagesummarization, chen and bansal (2018) and baeet al.
(2019) follow a hybrid extract-then-rewritearchitecture, with policy-based rl to bridge thetwo networks together.
lebanoff et al.
(2019), xuand durrett (2019) and mendes et al.
(2019) focuson the extract-then-compress learning paradigm,which will ﬁrst train an extractor for content selec-tion.
zhong et al.
(2020) introduces extract-then-match framework, which employs bertsumext(liu and lapata, 2019) as ﬁrst-stage to prune un-necessary information.
however, these above ex-tractive approaches prefer to model all source in-formative context and they pay little attention tothe imbalance problem..2.2 deep residual learning.
the original deep residual learning is introducedin image recognition (he et al., 2016a) for the no-torious degradation problem.
then, residual is in-troduced to the natural language process by trans-former (vaswani et al., 2017).
essentially, we can-not determine the depth of the network very well.
367according to the probability of p(1|si, d, θ)..3.2 sentence encoder.
the sentence encoder in extractive summarizationmodels is usually a recurrent neural network withlong-short term memory (hochreiter and schmid-huber, 1997) or gated recurrent units (cho et al.,2014).
in this paper, our sentence encoder buildson the bert architecture (devlin et al., 2019),a recently proposed highly efﬁcient model whichis based on the deep bidirectional transformer(vaswani et al., 2017) and has achieved state-of-the-art performance in many nlp tasks.
the trans-former aims at reducing the fundamental constraintof sequential computation which underlies mostarchitecture (liu et al., 2019).
it eliminates recur-rence in favor of applying a self-attention mecha-nism which directly models relationships betweenall words in a sentence..our extractive model is composed of a sentence-level transformer (ts) and a document-level trans-former (td) (liu et al., 2019).
for each sentencesi in the input document, ts is applied to obtain acontextual representation for each word:.
[u11, u12, ..., um n ] = ts([w11, w12, ..., wm n ])(1)and the representation of a sentence is acquired.
by applying weighted-pooling:.
aij = w0utijn(cid:88).
si =.
1n.j=1.
aijuij.
(2).
figure 2: overview of differsum..when building a deep network.
there will be opti-mal layers in the network, and outside the optimallayer is the redundant layer.
we expect the redun-dant layer to correspond to the input and output,namely identity mapping (he et al., 2016a,b; veitet al., 2016; balduzzi et al., 2018).
resnet (heet al., 2016a) addresses the degradation problemby introducing a deep residual learning framework.
if an identity mapping were optimal, it would beeasier to push the residual to zero than to ﬁt an iden-tity mapping by a stack of nonlinear layers (huangand wang, 2017).
in this paper, the residual unitserves as the second item of the differential am-pliﬁer to keep our architecture deep enough andcapture pivotal information..3 methodology.
3.1 problem deﬁnition.
we model the sentence extraction task as a se-quence tagging problem (kedzie et al., 2018).
given a document d consisting of a sequence of msentences [s1, s2, ..., sm ] and a sentence si consist-ing of a sequence of n words [wi1, wi2, ..., win ].
we denote by hi and hij the embedding of sen-tences and words in a continuous space.
the ex-tractive summarizer aims to produce a summary sby selecting m sentences from d (where m ≤ m ).
for each sentence si ∈ d, there is ground-truthyi ∈ {0, 1} and we will predict a label ˆyi ∈ {0, 1},where 1 means that si should be included in thesummary.
we assign a score p(ˆyi|si, d, θ) to quan-tify si’s relevance to the summary, where θ is theparameters of neural network model.
finally, weassemble a summary s by selecting m sentences,.
document-level transformer td takes si as in-put and yields a contextual representation for eachsentence:.
[v1, v2, ..., vm ] = td([s1, s2, ..., sm ]).
(3).
3.3 deep differential ampliﬁer.
in the transformer model sketched above, inter-sentence relations are modeled by multi-head at-tention based on softmax functions, which onlycapture shallow structural information (liu et al.,2019)..a differential ampliﬁer is a type of electronicampliﬁer that ampliﬁes the difference between twoinput voltages but suppresses any voltage common.
368 1 2 3         3 2 1 1 2 3     11 12 13  1  1 11 12 13  1  1weighted pooling 1 2 3     1 2 3     ff & sigmoid 1 2 3    to the two inputs.
the output of an ideal differentialampliﬁer is given by:.
vout = ad(v +.
in − v −in ).
(4).
in are the input voltage; ad is the.
where v +differential-mode gain..in and v −.
in and v −.
in practice, the gain should not be quite equalfor the two inputs, v +in and v −in .
for instance, evenif v +in are equal, the output vout should notbe zero.
so, modern differential ampliﬁers are usu-ally implemented with a more realistic expression,which includes a second term:.
vout = ad(v +.
in − v −.
in ) + ac.
v +in + v −in2.
(5).
where ac is called the common-mode gain of theampliﬁer..inspired by the differential ampliﬁer above, wecalculate and amplify the semantic difference be-tween each sentence and other sentences by thesubtraction operation of the sentence representa-tions [v1, v2, ..., vm ].
particularly, for sentence si,in and v −v +in are calculated as follows:.
v +in = vi(cid:80).
v −in =.
j∈{1,2,...,m }\{i} vjm − 1.
(6).
the original differential ampliﬁer consists of twoterms and the second one avoids making the ﬁnaloutput zero.
while for the deep neural network:1) inputs of the differential ampliﬁer are vector in-stances in the high dimensional space, which ispractically impossible for the zero output, com-pared with scalar; 2) the second term of the differ-ential ampliﬁer is not suitable for the deep iterativearchitecture, since it is exposed to the degradationproblem..notably, residual learning is introduced in deeplearning as shortcut connections to skip one ormore layers, which is naturally an alternative tothe second item of the differential ampliﬁer.
theadvantages of this method are: 1) the residual ar-chitecture will highlight the pivotal informationas well as reserving the original sentence repre-sentation; 2) it is easier to optimize the residualmapping than to optimize the original (he et al.,2016a).
hence, the residual unit is employed asthe second item, along with an iterative reﬁnementalgorithm to enhance the ﬁnal representation ofsentences..3.4 residual representation for sentence.
the differential ampliﬁer in our architecture con-sists of a few stacked layers to iteratively reﬁne thepivotal representation.
let us consider h(x) as anunderlying mapping to be ﬁt, with x denoting theinputs to the ﬁrst of these layers.
since multiplenonlinear layers can asymptotically approximatecomplicated functions (he et al., 2016a; mont´ufaret al., 2014), the differential ampliﬁer mappingh(x) is recast into a residual mapping f(x) andan identity mapping x:.
h(x) = f(x) + x.
(7).
obviously, residual learning is just a variant of.
the differential ampliﬁer:.
h(x) := voutf(x) := ad(v +.
in − v −in ).
(8).
where the output voltage vout thus becomes theoriginal mapping h(x) and the ﬁrst item of am-pliﬁer ad(v +in ) equals to residual mappingf(x),.
in − v −.
in our model, the second item of the differen-tial ampliﬁer is replaced by the identity mappingx, which is the shortcut connection and the outputis added to the outputs of f(x).
furthermore, 1)the identity shortcut connections advance the ar-chitecture without extra parameter; 2) the identityshortcut doesn’t add the computational complexity(he et al., 2016a);.
thus, for sentence respresentation vi, the deep.
differential ampliﬁer is:.
h(vi) = ad(vi −.
(cid:80).
j∈{1,2,...,m }\{i} vjm − 1.)
+ vi (9).
3.5.iterative structure reﬁnement.
the differential ampliﬁer and residual unit spe-cialize in modeling the pivotal information, whiledeeper neural networks with more parameters areable to infer semantic more accurately.
so, an itera-tive reﬁnement algorithm is introduced to enhancethe ﬁnal representation of pivotal information.
forsentence vi, the fundamental iterative unit is:.
h(vi) = f(vi) + vivi = h(vi).
(10).
369where we iteratively reﬁne the representation vi fork times; and thanks to the built-in residual mecha-nism, most shorter paths are needed during training,as longer paths do not contribute any gradient..along with the supervision, each iteration willpay more attention to the key semantic differencef(vi) of sentences with label 1, while trying tozero other f(vj).
conversely, previous extractiveapproaches without differential ampliﬁer can onlyclassify those sentences by compensating or penal-izing vi / vj, which is more difﬁcult to model..following previous work (nallapati et al., 2017;liu et al., 2019), we use a sigmoid function after alinear transformation to calculate the probability riof selecting si as a summary sentence:.
ri = sigmoid(w1vti ).
(11).
3.6 weighted objective function.
to rebalance the bias of minority 1-class and major-ity 0-class, we have built a deep differential ampli-ﬁer to amplify and capture the unique informationfor summary sentences.
besides, another heuristicmethod is to make our model pay more attention to1-class: a weighted cross-entropy function..particularly, we further design a more appropri-ate objective function to avoid biasing the data, bymaking the loss of a minority much greater thanthe majority.
the weight we employed is to rebal-ance the observations for each class, so the sum ofobservations for each class are equal.
finally, wedeﬁne the model’s loss function as the summationof the losses of all iterations:.
l =.
(cid:40).
k(cid:88).
k=1.
1m.m(cid:88).
i=1.
(cid:34) (cid:80).
(cid:80).
sj ∈d.
sj ∈d.
i(sj /∈ s)i(sj ∈ s).
y log(rki ).
+(1 − y) log(1 − rki ).
(cid:35)(cid:41).
(12).
where i(·) is an indicator function and k is thenumber of iterations..4 experiments.
4.1 datasets.
as shown in table 1, we employ two datasetswidely-used with multiple sentences summary:cnn and dailymail (cnn/dm) (hermann et al.,2015) and new york times (nyt) (sandhaus,2008)..table 1: data statistics: cnn/daily mail and nyt..datasets.
avg.doc length.
words.
sentences.
avg.summary lengthsentenceswords.
cnndailymailnyt.
760.50653.33800.04.
33.9829.3335.55.
45.7054.6545.54.
3.593.862.44.cnn/dm we used the standard split (her-mann et al., 2015)training, validation,forand test(90,266/1,220/1,093 for cnn and196,96/12.148/10,397 for daily mail), with split-ting sentences by stanford corenlp (manninget al., 2014) toolkit and pre-processing the datasetfollowing (see et al., 2017) and (zhong et al., 2020).
this dataset contains news articles and several as-sociated abstractive highlights.
we use the un-anonymized version as in previous summarizationwork and each document is truncated to 800 bpetokens..nyt following previous work (zhang et al.,2019b; xu and durrett, 2019), we use 137,778,17,222 and 17,223 samples for training, validation,and test, respectively.
we also followed their ﬁl-tering procedure, documents with summaries lessthan 50 words were removed from the dataset.
sen-tences were split with the stanford corenlp toolkit(manning et al., 2014).
input documents were trun-cated to 800 bpe tokens too..4.2 parameters.
our code is based on pytorch (paszke et al., 2019)and the pre-trained model employed in differsumis ‘albert-xxlarge-v2’, which is based on the hug-gingface/transformers2.
we train differsum twodays for 100,000 steps on 2gpus(nvidia teslav100, 32gb) with gradient accumulation everytwo steps.
adam with β1 = 0.9, β2 = 0.999 isused as optimizer.
learning rate schedule followsthe strategy with warming-up on ﬁrst 10,000 steps.
we have tried the iteration steps of 2/4/6/8 foriterative reﬁnement, and k = 4 is the best choicebased on the validation set.
we select the top-3checkpoints based on the evaluation loss on thevalidation set, and report the averaged results onthe test set..following jia et al.
(2020a) and jia et al.
(2021),we employ the greedy algorithm for the sentence-level soft labels, which falls under the umbrella.
2https://github.com/huggingface/transformers.
370table 2: rouge f1 on cnn/dm..table 3: rouge f1 on nyt..models.
abstractive.
abs (2015)pgc (2017)transformerabs (2017)t5large (2020)bartlarge (2019a)pegasuslarge (2019a)prophetnetlarge (2020)extractive.
lead-3oracle (sentence).
summarunner (2017)exconsumm (2019)pnbertbase (2019a)hibertlarge (2019b)bert-ext+rlbase (2019)bertsumextbase (2019)bertsumextlarge (2019)discobertbase (2020)hsgbase (2020)etcsumbase (2020)aredsumbase (2020)matchsumbase (2020)differsumlarge.
cnn/dmr-2.
r-1.
r-l.35.4639.5340.2143.5244.1644.1744.20.
40.4255.61.
39.6041.7042.6942.3742.7643.2543.8543.7742.9543.8443.4344.4144.70.
13.3017.2817.7621.5521.2821.4721.17.
17.6232.84.
16.2018.6019.6019.9519.8720.2420.3420.8519.7620.8020.4420.8621.36.
32.6536.3837.0940.6940.9041.1141.30.
36.6751.88.
35.3037.8038.8538.8339.1139.6339.9040.6739.2339.7739.8340.5540.83.of subset selection.
besides, we employ the tri-gram blocking strategy for decoding, which is asimple but powerful version of maximal marginalrelevance (carbonell and goldstein, 1998).
specif-ically, when predicting summaries for a new doc-ument, we ﬁrst use the model to obtain the prob-ability score p(1|si, d, θ) for each sentence, andthen we rank sentences by their scores and discardthose which have trigram overlappings with theirpredecessors..4.3 metric.
rouge (lin, 2004) is the standard metric forevaluating the quality of summaries.
we reportthe rouge-1, rouge-2, and rouge-l of dif-fersum by rouge-1.5.5.pl, which calculates theoverlap lexical units of extracted sentences andground-truth..5 results and analysis.
5.1 results on cnn/dm.
table 2 shows the results on cnn/dailymail.
allof these scores are in accordance with original pa-pers.
following nallapati et al.
(2017); liu andlapata (2019), we compare extractive summariza-.
r-1.
r-l.models.
abstractive.
abs (2015)pgc (2017)transformerabs (2017)bartlarge (2019a)extractive.
lead-3oracle (sentence).
summarunner (2017)exconsumm (2019)jecs (2019)bertsumextbase (2019)hibertlarge (2019b)differsumlarge.
42.7843.9345.3648.73.
41.8064.22.
42.3743.1845.5046.6649.4749.52.nytr-2.
25.6126.8527.3429.25.
22.6044.57.
23.8924.4325.3026.3530.1129.78.
35.2638.6739.5344.48.
35.0057.27.
38.7438.9238.2042.6241.6343.86.tion models against abstractive models, and it iscertainly that the abstractive paradigm is still on thefrontier of summarization.
the ﬁrst part of extrac-tive approaches is the lead-3 baseline and oracleupper bound, while the second part includes otherextractive summarization models.
we present ourmodels ﬁnally at the bottom.
it is obvious that ourdiffersum outperforms all extractive baseline mod-els.
compared with large version bertsumext,our differsum achieves 0.85/1.02/0.93 improve-ments on r-1, r-2, and r-l, which indicates thepivotal information captured by the differential am-pliﬁer is more powerful than the other structures.
compared with early approaches, especially forbertsumext, we observe that bert outper-forms all previous non-bert-based summarizationsystems, and trigram-blocking leads to a greatimprovement on all rouge metrics.
match-sum is a comparable competitor to our differ-sum, which formulates the extractive summariza-tion task as a two-step problem and extract-then-match summary based on a well-trained bert-sumext.
therefore, we only train a large versiondiffersum for a fair comparison..5.2 results on nyt.
results on nyt are summarized in table 3. notethat we use limited-length rouge recall as dur-rett et al.
(2016), where the selected sentences aretruncated to the length of the human-written sum-maries.
the parts of table 3 is similar to table 2.the ﬁrst four lines are abstractive models, and thenext two lines are our golden baselines for extrac-.
371table 4: ablation study on cnn/dm..table 5: human evaluation on cnn/dm..models.
r-1.
r-2.
r-l.models.
1st.
2nd.
3rd.
4th meanr.
44.70differsumdiffersum w/o albert 44.4144.17differsum w/o ampliﬁer44.32differsum w/o iteration.
21.3620.8020.7421.02.
40.8340.5740.4240.48.summarunner0.20bertsumext 0.250.48differsum0.68ground-truth.
0.270.300.270.22.
0.300.280.200.07.
0.230.170.050.03.
2.562.371.821.45.tive summarization.
the third part reports the per-formance of other extractive works and our modelrespectively.
again, we observe that our differen-tial ampliﬁer modeling performs better than bothlstm and bert.
meanwhile, we ﬁnd that extrac-tive approaches show superiority over abstractivemodels, and the rouge scores are higher thancnn/dailymail..5.3 ablation studies.
we propose several strategies to improve the per-formance of extractive summarization, includingdifferential ampliﬁer (vs. normal residual network),pre-trained albert(vs. bert), and iterative re-ﬁnement (vs. none).
to investigate the inﬂuenceof these factors, we conduct experiments and listthe results in table 4. signiﬁcantly, 1) differen-tial ampliﬁer is more critical than albert, forthe reason that the pivotal information is essentialand difﬁcult for albert to model; 2) iterative re-ﬁnement mechanism enlarges the advantage of thedifferential ampliﬁer, demonstrating the superiorityof deep architecture..5.4 human evaluation for summarization.
it is not enough to only rely on the rouge eval-uation for a summarization system, although therouge correlates well with human judgments(owczarzak et al., 2012).
therefore, we design anexperiment based on a ranking method to evaluatethe performance of differsum by humans.
fol-lowing cheng and lapata (2016), narayan et al.
(2018) and zhang et al.
(2019b), ﬁrstly, we ran-domly select 40 samples from cnn/dm test set.
then the human participants are presented withone original document and a list of correspondingsummaries produced by different model systems.
participants are requested to rank these summaries(ties allowed) by taking informativeness (can thesummary capture the important information fromthe document) and ﬂuency (is the summary gram-matical) into account.
each document is annotatedby three different participants separately..the input article and ground truth summaries are.
also shown to the human participants in additionto the three model summaries (summarunner,bertsumext, and differsum).
from the resultsshown in table 5, it is obvious that differsum isbetter in relevance compared with others..5.5 trigram blocking strategy.
trigram blocking leads to a great improvement onall rouge metrics for many extractive approaches(liu and lapata, 2019; wang et al., 2020).
it ishas become a fundamental module in extractivesummarization.
in this paper, differsum extractssummary sentences with the trigram-blocking al-gorithm, but whether there is a great improvementalong with it, like in summarunner or bert-sumext?.
it has been explained by nallapati et al.
(2017);liu and lapata (2019), that picking all sentences bycomparing the predicted probability with a thresh-old may not be an optimal strategy since the train-ing data is very imbalanced in terms of summary-membership of sentences.
therefore, the trigram-blocking algorithm is introduced to select top-ksentences and reduce the redundancy..coincidentally, our differsum is designed to 1)rebalance the distribution of majority and minorityand 2) ﬁlter the tangential and redundant informa-tion.
thus, the trigram-blocking algorithm maybe useless for our differsum..table 6 further summarizes the performance gainof trigram-blocking strategy.
it is obvious thatthis strategy is essential for bertsumext orsummarunner, achieving more than 2.68 / 0.98improvements on r-1 separately, for that thereis no enough redundancy modeling for both ofthem.
while on the other hand, the efﬁciency of thetrigram-blocking strategy is weak for differsum..5.6 documents with a different number of.
sentences.
in this paper, we emphasize the inherent imbal-ance problem of the majority 0-class and the mi-nority 1-class.
in fact, in cnn/dailymail dataset,there are plenty of documents with a different num-.
372table 6: rouge scores about trigram-blocking oncnn/dm test set..models.
differsum (with trigram-blocking)differsumbertsumext (with trigram-blocking)bertsumextsummarunner (with trigram-blocking)summarunner.
r-1.
r-l.44.7044.3643.8541.1740.5839.60.
40.8340.4339.9036.5236.6135.30.
(a) bertsumext.
(b) differsum.
figure 3: comparison between the rouge scorestendencies of bertsumext and differsum.
ber of sentences, ranging from 3-sentences to 100-sentences.
while the number of summary sen-tences, labeled with 1, is from 1-sentences to 5-sentences, and the average number of sentenceslabeled 1 in cnn/dailymail is only 7.33%.
whatis worse is that the distribution of the number ofsentences for documents is a uniform distribution,thus we could not avoid the imbalance by cleaningthe data..in this paper, we design another experiment toanalysis the harmful effect of imbalance classes.
we train the bertsumext (12-layers) fromscratch on cnn/dailymail, and evaluate the modelon the test set to check the tendency of rougescores, along with the number of sentences accu-mulating.
the result is shown in the line chart offigure 1 and figure 3a, and obviously we only payattention to the document in which the number ofsentences less than 55. speciﬁcally, each docu-ment is truncated to 2000 bpe tokens to involvemore sentences, but this can not cover those wholedocuments with more than 55-sentences.
there-fore, we choose to calculate the rouge scores fordocuments with sentences from 3 to 55..for comparison, we train our differsum (12-layers) from scratch, and each document is trun-cated to 2000 bpe tokens too.
the tendency ofour differsum is as figure 3b.
compared with thetendency of bertsumext, there is no obviousrouge decrease, demonstrating that our approachhas strengthened the representation of pivotal and.
rebalanced the disproportionate ratio of summarysentences and other sentences..note that more truncated bpe tokens will in-crease the ﬁnal average rouge slightly, for it maylose some summary sentences when truncating toomany tokens.
unfortunately, our 24-layers differ-sum can only be trained with 800 bpe tokens forthe limitation of gpu source..5.7 map words representation into sentence.
representation.
a key issue motivating the sentence-level trans-former (ts) and the document-level transformer(td) is that the features for words after the tsmight be at different scales or magnitudes.
thiscan be due to some words having very sharp orvery distributed attention weights when summingover the features of the other words..in this paper, we apply two ways to map thewords representation into its sentence representa-tion: weighted-pooling at equation 2 and picking[cls] token as sentence (liu and lapata, 2019).
table 7 shows that [cls] is not enough to con-vey enough informative information of words forboth our differsum and bertsumext.
espe-cially, differsum is more sensitive to the word fea-tures since our differential ampliﬁer may amplifythe semantic features effectively..table 7: rouge scores about sentence representa-tion on cnn/dm test set..models.
differsum (weighted-pooling)differsum ([cls])bertsumext (weighted-pooling)bertsumext ([cls]).
r-1.
r-l.44.7044.4143.9243.85.
40.8340.4340.0839.90.
6 conclusion.
in this paper, we introduce a heuristic model, dif-fersum, 1) to calculate and ampliﬁer the pivotalinformation and 2) to rebalance the distribution ofminority 1-class and majority 0-class.
besides, weemploy another weighted cross-entropy functionto compensate for the imbalance.
experimentalresults show that our method signiﬁcantly outper-forms previous models.
in the future, we wouldlike to generalize differsum to other ﬁelds..acknowledgements.
this research is supported by the national keyresearch and development program of china.
3731020304050numberofsentences2025303540rouge-1rouge-2rouge-l1020304050numberofsentences2025303540rouge-1rouge-2rouge-l(no.2017yfc0820700) and national natural sci-ence foundation of china (no.61902394).
wethank all authors for their contributions and allanonymous reviewers for their constructive com-ments..references.
kristjan arumae and fei liu.
2018. reinforced extrac-tive summarization with question-focused rewards.
in acl, pages 105–111..akari asai and hannaneh hajishirzi.
2020. logic-guided data augmentation and regularization for con-in acl, pages 5642–sistent question answering.
5650..sanghwan bae, taeuk kim, jihoon kim, and sanggoo lee.
2019. summary level training of sentencein arxivrewriting for abstractive summarization.
preprint arxiv:1909.08752..david balduzzi, marcus frean, lennox leary,and brianjp lewis, kurt wan-duo ma,mcwilliams.
2018.the shattered gradientsproblem: if resnets are the answer, then what is thequestion?
in icml, pages 342–350..christos baziotis, ion androutsopoulos, ioannis kon-stas, and alexandros potamianos.
2019. seq3: dif-ferentiable sequence-to-sequence-to-sequence au-for unsupervised abstractive sentencetoencodercompression.
in naacl-hlt, pages 673–681..keping bi, rahul jha, w. bruce croft, and asli celiky-ilmaz.
2020. aredsum: adaptive redundancy-awareiterative sentence ranking for extractive documentsummarization..greg durrett, taylor berg-kirkpatrick, and dan klein.
2016. learning-based single-document summariza-tion with compression and anaphoricity constraints.
in arxiv preprint arxiv:1603.08887..haibo he and yunqian ma.
2013. imbalanced learn-foundations, algorithms, and applications..ing:john wiley & sons..kaiming he, xiangyu zhang, shaoqing ren, and jiansun.
2016a.
deep residual learning for image recog-nition.
in cvpr, pages 770–778..kaiming he, xiangyu zhang, shaoqing ren, and jiansun.
2016b.
identity mappings in deep residual net-works.
in eccv, pages 630–645..karl moritz hermann, tomas kocisky, edward grefen-stette, lasse espeholt, will kay, mustafa suleyman,and phil blunsom.
2015. teaching machines to readand comprehend.
in nips, pages 1693–1701..sepp hochreiter and j¨urgen schmidhuber.
1997. longshort-term memory.
neural computation, pages1735–1780..yi yao huang and william yang wang.
2017. deepresidual learning for weakly-supervised relation ex-traction.
in emnlp, pages 1803–1807..aishwarya jadhav and vaibhav rajan.
2018. extrac-tive summarization with swap-net: sentences andin acl,words from alternating pointer networks.
pages 142–151..ruipeng jia, yanan cao, haichao shi, fang fang,cong cao, and shi wang.
2021.flexible non-autoregressive extractive summarization with thresh-old: how to extract a non-ﬁxed number of summarysentences.
in aaai..jaime carbonell and jade goldstein.
1998. the use ofmmr, diversity-based reranking for reordering docu-in sigir, pagesments and producing summaries.
209–210..ruipeng jia, yanan cao, haichao shi, fang fang, yan-bing liu, and jianlong tan.
2020a.
distilsum: dis-tilling the knowledge for extractive summarization.
in cikm, pages 2069–2072..yen-chun chen and mohit bansal.
2018. fast abstrac-tive summarization with reinforce-selected sentencerewriting.
in acl, pages 675–686..jianpeng cheng and mirella lapata.
2016. neural sum-marization by extracting sentences and words.
inacl..kyunghyun cho, bart van merri¨enboer, caglar gul-cehre, dzmitry bahdanau, fethi bougares, holgerschwenk, and yoshua bengio.
2014.learningphrase representations using rnn encoder-decoderfor statistical machine translation.
emnlp, pages1724–1734..jacob devlin, ming-wei chang, kenton lee, andkristina toutanova.
2019. bert: pre-training of deepbidirectional transformers for language understand-ing.
in naacl-hlt, pages 4171–4186..ruipeng jia, yanan cao, hengzhu tang, fang fang,cong cao, and shi wang.
2020b.
neural extrac-tive summarization with hierarchical attentive het-erogeneous graph network.
in emnlp, pages 3622–3631..chris kedzie, kathleen mckeown, and hal daum´e iii.
2018. content selection in deep learning models ofsummarization.
in emnlp, pages 1818–1828..logan lebanoff, kaiqiang song, franck dernoncourt,doo soon kim, seokhwan kim, walter chang, andfei liu.
2019. scoring sentence singletons and pairsfor abstractive summarization.
acl, pages 2175–2189..chin-yew lin.
2004. rouge: a package for automaticin text summarization.
evaluation of summaries.
branches out, pages 74–81..374yang liu and mirella lapata.
2019. text summariza-in emnlp, pages.
tion with pretrained encoders.
3728–3738..yang liu, ivan titov, and mirella lapata.
2019. sin-gle document summarization as tree induction.
innaacl-hlt, pages 1745–1755..ling luo, xiang ao, yan song, feiyang pan, minyang, and qing he.
2019. reading like her: hu-man reading inspired extractive summarization.
inemnlp, pages 3033–3043..christopher manning, mihai surdeanu, john bauer,jenny finkel, steven bethard, and david mcclosky.
2014. the stanford corenlp natural language pro-cessing toolkit.
in acl, pages 55–60..afonso mendes, shashi narayan, sebasti˜ao miranda,zita marinho, andr´e ft martins, and shay b co-hen.
2019. jointly extracting and compressing doc-uments with summary state representations.
innaacl-hlt, pages 3955–3966..junghyun min, r. thomas mccoy, dipanjan das,syntacticemily pitler, and tal linzen.
2020.data augmentation increases robustness to inferenceheuristics.
in acl, pages 2339–2352..guido mont´ufar, razvan pascanu, kyunghyun cho,and yoshua bengio.
2014. on the number of lin-ear regions of deep neural networks.
in nips, pages2924–2932..ramesh nallapati, feifei zhai, and bowen zhou.
2017.summarunner: a recurrent neural network based se-quence model for extractive summarization of docu-ments.
in aaai, pages 3075–3081..shashi narayan, shay b cohen, and mirella lapata.
2018. ranking sentences for extractive summariza-in naacl-hlt,tion with reinforcement learning.
pages 1747–1759..shashi narayan, joshua maynez, jakub adamek,daniele pighin, blaˇz brataniˇc, and ryan mcdon-ald.
2020. stepwise extractive summarization andplanning with structured transformers.
in emnlp,pages 4143–4159..karolina owczarzak, john m conroy, hoa trang dang,and ani nenkova.
2012. an assessment of the accu-racy of automatic evaluation in summarization.
inproceedings of workshop on evaluation metrics andsystem comparison for automatic summarization,pages 1–9..adam paszke, sam gross, francisco massa, adamlerer, james bradbury, gregory chanan, trevorkilleen, zeming lin, natalia gimelshein, lucaantiga, alban desmaison, andreas k¨opf, edwardyang, zach devito, martin raison, alykhan tejani,sasank chilamkurthy, benoit steiner, lu fang, jun-jie bai, and soumith chintala.
2019. pytorch: animperative style, high-performance deep learning li-brary.
in nips, pages 8024–8035..colin raffel, noam shazeer, adam roberts, katherinelee, sharan narang, michael matena, yanqi zhou,wei li, and peter j liu.
2020. exploring the limitsof transfer learning with a uniﬁed text-to-text trans-former.
j. mach.
learn.
res., pages 140:1–140:67..alexander m. rush, sumit chopra, and jason weston.
2015. a neural attention model for abstractive sen-tence summarization.
in emnlp, pages 379–389..evan sandhaus.
2008. the new york times annotatedin linguistic data consortium, philadel-.
corpus.
phia..abigail see, peter j. liu, and christopher d. manning.
2017. get to the point: summarization with pointer-generator networks.
in acl, pages 1073–1083..naftali tishby, fernando c pereira, and williambialek.
2000. the information bottleneck method.
arxiv preprint physics/0004057..ashish vaswani, noam shazeer, niki parmar, jakobuszkoreit, llion jones, aidan n gomez, łukaszkaiser, and illia polosukhin.
2017. attention is allyou need.
in nips, pages 5998–6008..andreas veit, michael wilber, and serge belongie.
2016. residual networks behave like ensembles ofin nips, pages 550–relatively shallow networks.
558..danqing wang, pengfei liu, yining zheng, xipengqiu, and xuanjing huang.
2020. heterogeneousgraph neural networks for extractive document sum-marization.
in acl, pages 6209–6219..danqing wang, pengfei liu, ming zhong, jie fu,xipeng qiu, and xuanjing huang.
2019. explor-ing domain shift in extractive text summarization.
inarxiv preprint arxiv:1908.11664..peter west, ari holtzman, jan buys, and yejin choi.
2019. bottlesum: unsupervised and self-supervisedsentence summarization using the information bot-tleneck principle.
in emnlp, pages 3750–3759..qizhe xie, zihang dai, eduard hovy, minh-thang lu-ong, and quoc v. le.
2020. unsupervised data aug-mentation for consistency training.
in nips..jiacheng xu and greg durrett.
2019. neural extrac-tive text summarization with syntactic compression.
emnlp, pages 3290–3301..jiacheng xu, zhe gan, yu cheng, and jingjing liu.
2020. discourse-aware neural extractive text sum-marization.
in acl, pages 5021–5031..yu yan, weizhen qi, yeyun gong, dayiheng liu,nan duan, jiusheng chen, ruofei zhang, and mingzhou.
2020. prophetnet: predicting future n-gramin arxivfor sequence-to-sequence pre-training.
preprint arxiv:2001.04063, pages 2401–2410..375jingqing zhang, yao zhao, mohammad saleh, and pe-ter j liu.
2019a.
pegasus: pre-training with ex-tracted gap-sentences for abstractive summarization.
in arxiv preprint arxiv:1912.08777, pages 11328–11339..xingxing zhang, furu wei, and ming zhou.
2019b.
hibert: document level pre-training of hierarchicalbidirectional transformers for document summariza-tion.
in acl, pages 5059–5069..ming zhong, pengfei liu, yiran chen, danqing wang,xipeng qiu, and xuanjing huang.
2020. extrac-tive summarization as text matching.
in acl, pages6197–6208..ming zhong, pengfei liu, danqing wang, xipeng qiu,and xuanjing huang.
2019a.
searching for effectiveneural extractive summarization: what works andwhat’s next.
in acl, pages 1049–1058..ming zhong, danqing wang, pengfei liu, xipeng qiu,and xuanjing huang.
2019b.
a closer look at databias in neural extractive summarization models.
inarxiv preprint arxiv:1909.13705..qingyu zhou, nan yang, furu wei, shaohan huang,ming zhou, and tiejun zhao.
2018. neural docu-ment summarization by jointly learning to score andselect sentences.
in acl, pages 654–663..barret zoph, ekin d. cubuk, golnaz ghiasi, tsung-yilin, jonathon shlens, and quoc v. le.
2019. learn-ing data augmentation strategies for object detection.
in eccv, pages 566–583..376