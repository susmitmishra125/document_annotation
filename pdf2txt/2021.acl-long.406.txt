word sense disambiguation: towards interactive  context exploitation from both word and sense perspectives   ming wang1 and yinglin wang2, * school of information management and engineering shanghai university of finance and economics, shanghai, china 1wangming@163.sufe.edu.cn, 2wang.yinglin@shufe.edu.cn    .
abstract .
these .
systems .
document .
proposed  word .
sense lately disambiguation (wsd)  systems  have approached  the  estimated  upper  bound  of the task on standard evaluation benchmarks.
however, typically implement the disambiguation of words in independently, almost a underutilizing sense and word dependency in  context.
in  this  paper,  we  convert  the nearly  isolated  decisions  into  interrelated ones  by  exposing  senses  in  context  when learning sense embeddings in a similarity-based  sense  aware  context  exploitation (sace)  architecture.
meanwhile,  we enhance  the  context  embedding  learning with  selected  sentences  from  the  same document,  rather  than  utilizing  only  the sentence  where  each  ambiguous  word appears.
experiments on both english and multilingual wsd datasets have shown the effectiveness  of  our  approach,  surpassing previous  state-of-the-art  by  large  margins (3.7%  and  1.2%  respectively),  especially on few-shot (14.3%) and zero-shot (35.9%) scenarios.
.
1 .
introduction .
word sense disambiguation (wsd) is the task of determining  a  word’s  sense  given  its  context.
recently,  contextualized  representation  learning (devlin  et  al.,  2019;  liu  et  al.,  2019)  have accelerated the advancement of wsd, raising the performance on a standard evaluation framework (raganato et al., 2017a) from slightly higher than 70%  (raganato  et  al.,  2017b;  luo  et  al.,  2018; kumar et al., 2019) to about 80% (vial et al., 2019; blevins  and  zettlemoyer,  2020;  bevilacqua  and .
* corresponding author .
.
navigli, 2020).
this is an estimated upper bound of  the  task,  which  is  from  the  inter-annotator agreement:  the  percentage  of  words  that  are annotated with the same meaning by two or more annotators (navigli, 2009).
there is a clear trend that supervised systems tend to incorporate sense knowledge  into  their  architecture,  ranging  from sense definition, usage examples to sense relation.
however,  the  disambiguation  of  words  in  a document  is  almost  independent  of  each  other, especially  from  the  perspective  of  senses  in context.
the  connection  of  each  word’s disambiguation  is  limited  to  the  utilization  of  a sentence (loureiro and jorge, 2019; huang et al., 2019;  hadiwinoto  et  al.,  2019;  scarlini  et  al., 2020a) or a small window of text (bevilacqua and navigli,  2020)  because  of  computation  cost  or model restriction.
more severely, the interaction of senses  in  context  is  barely  explored.
similar  to word  cooccurrence, the  appearance of one  sense can  sometimes  dominate  the  choice  of  another sense  in  the  same  sentence  (agirre  et  al.,  2014; maru et al., 2019).
.
in this paper, we introduce sace, a similarity-based wsd approach.
precisely, we transform the previously  almost  isolated  disambiguation  of words  in  a  document  into  interrelated  ones  to maximize  the  contribution  of  context  from  both word  and  sense  perspectives.
we  summarize  our contributions as follows: .
1. we  devise  an  interactive  sense  embedding learning  technique  that  takes  into  account senses  in  context  via  a  selective  attention layer  in  a  neural  architecture.
it  connects senses via their appearance in a piece of text rather than using manually constructed sense relations, being less costly.
.
2. we introduce a method to  better exploit the .
proceedingsofthe59thannualmeetingoftheassociationforcomputationallinguisticsandthe11thinternationaljointconferenceonnaturallanguageprocessing,pages5218–5229august1–6,2021.©2021associationforcomputationallinguistics5218context  sentences  of  an  ambiguous  word  in the neural architecture by selecting important sentences from the same document according to sentence relatedness.
.
3. with experiments on corresponding datasets, the proposed architecture is proved to have an overwhelming  advantage  of  few-shot  and zero-shot  wsd  learning  ability  compared with other strong baselines.
.
4. we  show  that  the  proposed  architecture  is portable  to  multilingual  scenarios  when trained  merely on an english dataset  with a multilingual  pre-trained  model,  achieving new tested on  most benchmarks and the combined one.
.
state-of-the-art .
2  related work .
and .
knowledge-based .
there are mainly two alternatives for solving wsd, supervised namely approaches.
while  the  former  mainly  relies  on  a sense  inventory  for  disambiguation,  the  latter  is dependent  on  sense-annotated  corpora  to  train  a sense classifier, either for each word or the whole vocabulary.
however,  many  recently  proposed systems  combine two  strategies, injecting  sense  knowledge  into  their  supervised models while somehow inadequately modeling the provided  context  in  a  document  from  both  word and sense perspectives.
.
the  above .
2.1  supervised method .
early supervised approaches model the relational pattern  between  an  ambiguous  word’s  local features  and  its  gold  sense  from  sense-annotated data.
ims  (zhong and  ng, 2010)  was one of the most  prevalent  systems  that  trained  a  sense classifier  for  each  lemma  in  training  data.
in comparison,  raganato  et  al.
(2017b)  unified  the disambiguation  of  words  into  a  single  sequence labeling architecture, relieving the efficiency issue.
many this systems architecture by incorporating sense knowledge.
.
following .
improved .
for unseen lemmas, these systems require most frequent  sense  (mfs)  fallback  (select  the  most frequent candidate  sense  in the training  data).
to tackle  this  problem,  lmms  (loureiro  and  jorge 2019) in  a implements similarity-based  manner.
learns  a  sense embedding  for  each  labeled  sense  in  semcor (miller  et  al.,  1994)  and  maps  them  to  full coverage of wordnet (miller, 1995) senses using .
the  disambiguation .
it .
sense relations.
bert (devlin et al., 2019) is used as a feature-extraction module for both gloss and context  encoding.
further,  bem  (blevins  and zettlemoyer,  2020)  utilizes  two  encoders  for  the above approach in a fine-tuning manner.
although the  model is  more  effective  even  without exploiting sense knowledge other than glosses, it takes around 2.5 days for training.
.
the employment of sense relations in previous supervised systems is mostly limited to explicitly defined  sense  relations  including hypernymy  and hyponymy  relation,  severely  neglecting  how senses  in  context  contribute  to  the  selection  of  a word’s sense.
.
2.2  context exploitation .
for supervised wsd approaches, it is typical to use a small fraction of the whole context to carry out disambiguation,  such  as  a  sentence,  or  a  sliding window  of  text.
in  contrast,  knowledge-based wsd approaches tend to more sufficiently exploit a word’s context, ranging from a sentence (lesk, 1986;  wang  and  wang,  2020),  a  few  sentences (agirre et al., 2018, wang et al., 2020) to even the whole  document  (chaplot  and  salakhutdinov, 2018).
some studies draw in out-of-dataset context (ponzetto and navigli, 2010; scarlini et al., 2020a) for including  wikipedia documents.
therefore,  it  is  worth  exploring whether  the  disambiguation  of  words  within  the same  document  can  benefit  from  each  other  in  a supervised system.
.
disambiguation, .
on .
sense  graph .
the  utilization  of  senses  in  context  is  far  less investigated  compared  with  words  in  context.
ukb  (agirre  et  al.,  2014,  a  knowledge-based system)  is  one  of  the  related  systems  that  model sense relations in context.
it first connects senses in  context  via  wordnet  sense  relations  and the personalized  pagerank operates constructed sense importance.
for  each  word,  the  most  important potential sense is considered as the correct sense.
syntagnet (maru et al.,  2019) improves the idea by  introducing  manually  disambiguated  sense pairs  in  context  during  sense  graph  construction.
although  the  system  was  able  to  challenge supervised systems at the time, it relied on human labor to obtain sense pairs in context.
there was no attempt on integrating the utilization of senses in context into a supervised architecture.
.
to  decide .
5219scores.
𝒗�𝒃𝒆𝒍𝒍.𝒏.𝟎𝟏𝒗�𝒃𝒆𝒍𝒍.𝒏.𝟎𝟐𝒗�𝒃𝒆𝒍𝒍.𝒏.𝟎𝟑….
sense-level context.
𝒕𝒉𝒆𝒓𝒆.
𝒈𝒐.
….
….
𝒃𝒆𝒍𝒍.
𝒗�𝒃𝒆𝒍𝒍roberta.
word-level context.
𝑺𝒊−𝟏𝟐.
𝑺𝒊−𝟏.
𝑺𝒊.
𝑺𝒊+𝟏 𝑺𝒊+𝟕.
𝑻𝒉𝒆𝒓𝒆.
𝒈𝒐𝒆𝒔.
𝒃𝒆𝒍𝒍.
gloss(.
….
roberta.
predicted sense from the last epoch.
𝒃𝒆𝒍𝒍.
𝒏.
𝟎𝟑):.
the sound of a bell being struck..figure 1: sace framework.
.
𝒗𝒃𝒆𝒍𝒍.𝒏.𝟎𝟏𝒗𝒃𝒆𝒍𝒍.𝒏.𝟎𝟐𝒗𝒃𝒆𝒍𝒍.𝒏.𝟎𝟑.
….
.
3  preliminary .
.
𝑗𝑗.
𝑠𝑠̃𝑗𝑗𝑡𝑡ℎ.
𝑤𝑤𝑖𝑖𝑗𝑗.
, and .
is  the .
sentence .
𝑆𝑆𝑖𝑖 = {𝑤𝑤𝑖𝑖1, 𝑤𝑤𝑖𝑖2, … , 𝑤𝑤𝑖𝑖𝑗𝑗, … , 𝑤𝑤𝑖𝑖𝑖𝑖}𝐷𝐷 = {𝑆𝑆1, 𝑆𝑆2, … , 𝑆𝑆𝑖𝑖, … , 𝑆𝑆𝑚𝑚}..wsd is to select the correct sense given  its  context.
.
of a word   word  in  the 𝑤𝑤𝑖𝑖𝑗𝑗  of  a document  the 𝑡𝑡ℎ𝑖𝑖candidate  senses  are from a sense inventory such as wordnet.
here, 𝑠𝑠�𝑤𝑤𝑖𝑖𝑗𝑗� = {𝑠𝑠𝑗𝑗1, 𝑠𝑠𝑗𝑗2, … , 𝑠𝑠𝑗𝑗𝑗𝑗, … , 𝑠𝑠𝑗𝑗𝑗𝑗} denote the index of sentence, word, and , sense respectively.
𝑖𝑖𝑗𝑗𝑗𝑗in  a  similarity-based  wsd  approach,  the disambiguation  of  a  word  is  determined  by  the  similarity  between  its  context  representation and  each  candidate  sense  representation  .
in 𝑣𝑣𝑤𝑤𝑖𝑖𝑗𝑗many  cases,  both  representations  are  vectors  and the similarity is measured by their dot product after normalization.
then,  the  sense  with  the  highest similarity is selected as the correct sense.
.
𝑣𝑣𝑠𝑠𝑗𝑗𝑗𝑗.
𝑆𝑆𝑖𝑖.
𝐺𝐺𝑠𝑠𝑗𝑗𝑗𝑗.
typically,  a  word’s  context  representation  is   where  the  word learned  using  the  sentence appears (loureiro and jorge, 2019; scarlini et al., 2020a;  scarlini et al., 2020b).
the  representation of  a  candidate  sense  is  obtained  using  its  defined in wordnet (blevins gloss/definition and  zettlemoyer,  2020).
a  common  approach  of encoding these two sequences in recent research is to  utilize  pre-trained  models  such  as  bert, roberta (liu et al., 2019), and so on, taking the sum of the outputs of the last four layers as encoded features (loureiro and jorge, 2019; scarlini et al.,   and 2020a),  as  in  (1)  and  (2).
before  feeding  to the models, a special token [cls]/[sep] is added  to  the  beginning/end  of  the  sequence, 𝐺𝐺𝑠𝑠𝑗𝑗𝑗𝑗modifying them into  and .
, respectively.
.
𝑆𝑆𝑖𝑖.
𝑆𝑆̅𝑖𝑖.
𝐺𝐺̅𝑠𝑠𝑗𝑗𝑗𝑗.
.
.
.
.
(1) .
𝑤𝑤𝑖𝑖𝑗𝑗.
(𝑆𝑆̅𝑖𝑖).
), using .
for each .
(−4,−1)𝑗𝑗𝑣𝑣𝑤𝑤𝑖𝑖𝑗𝑗 = ∑𝑧𝑧𝐵𝐵𝐵𝐵𝐵𝐵𝐵𝐵𝑧𝑧’s context representation, a normal choice  is  to  utilize  the  model’s  output  at  the position of the word ( as input, shown in equation (1).
if the word is tokenized into several pieces,  their  mean  is  taken.
in  contrast,  for  each sense representation, when it is fine-tuning a pre-trained model, the sense embedding is the output at the  position  of  [cls]  (blevins  and  zettlemoyer, 2020), with the modified gloss as input, as in (2).
.
𝑆𝑆̅𝑖𝑖.
𝑗𝑗.
.
(2) .
[𝐶𝐶𝐶𝐶𝐶𝐶].
[𝐶𝐶𝐶𝐶𝐶𝐶].
(−4,−1)𝑧𝑧= ∑.
𝑣𝑣𝑠𝑠𝑗𝑗𝑗𝑗.
(𝐺𝐺̅𝑠𝑠𝑗𝑗𝑗𝑗).
𝐵𝐵𝐵𝐵𝐵𝐵𝐵𝐵𝑧𝑧.
to utilize the supervision from a training corpus, a  cross-entropy  loss  is  implemented  against  the similarity  distribution  of  candidate  senses  (the   in  (3))  and  the softmax  product  without  index one-hot  ground-truth  distribution,  shown in   is  a  matrix  of equation  (4).
.
𝑗𝑗.
�𝑠𝑠�𝑤𝑤𝑖𝑖𝑗𝑗��×ℎ.
𝑉𝑉𝑠𝑠�𝑤𝑤𝑖𝑖𝑗𝑗� ∈ ℝ.concatenated sense embeddings arranged in rows.
is  the  dimension  of  the  pre-trained  model’s  is equal hidden states (768 or 1024 of bert).
ℎto 1 when ) is the correct sense,  otherwise  0,  representing  each  element  in the ground-truth one-hot vector.
for prediction, the model selects the sense with the largest dot product for each word.
.
sense of .
(the .
𝑤𝑤𝑖𝑖𝑗𝑗.
𝑦𝑦𝑗𝑗𝑗𝑗.
𝑠𝑠𝑗𝑗𝑗𝑗.
𝑡𝑡ℎ.
𝑗𝑗.
(3) .
(4) .
[𝐶𝐶𝐶𝐶𝐶𝐶].
𝑠𝑠𝑖𝑖𝑠𝑠 �𝑣𝑣𝑤𝑤𝑖𝑖𝑗𝑗 ∙ 𝑣𝑣𝑠𝑠𝑗𝑗𝑗𝑗.
𝑗𝑗� = 𝑠𝑠𝑗𝑗𝑠𝑠𝑠𝑠𝑠𝑠𝑠𝑠𝑠𝑠(𝑉𝑉𝑠𝑠�𝑤𝑤𝑖𝑖𝑗𝑗�𝑣𝑣𝑤𝑤𝑖𝑖𝑗𝑗)[𝐶𝐶𝐶𝐶𝐶𝐶].
�𝑠𝑠(𝑤𝑤𝑖𝑖𝑗𝑗)�𝑗𝑗=1.
ℒ�𝑤𝑤𝑖𝑖𝑗𝑗, 𝑠𝑠𝑗𝑗� = − ∑in the above approach (from bem, blevins and ))learning zettlemoyer,  2020), process of different senses is independent of each other, relying merely on sense gloss.
besides, the .
𝑦𝑦𝑗𝑗𝑗𝑗𝑙𝑙𝑗𝑗𝑙𝑙(𝑠𝑠𝑖𝑖𝑠𝑠(𝑣𝑣𝑤𝑤𝑖𝑖𝑗𝑗 ∙ 𝑣𝑣𝑠𝑠𝑗𝑗𝑗𝑗the  embedding .
5220between .
different .
words’ interaction disambiguation  is  limited  to  the  utilization  of  a sentence, leading to inadequate exploitation of the words  in  context.
therefore,  we  transform  the above  almost  isolated  decisions  into  interrelated ones by learning the sense and context embeddings interactively.
.
4  sace: .
sense  aware  context .
exploitation in supervised wsd .
4.1  sense-level context (slc) .
the interactive sense embedding learning mainly involves  a  selective  attention  layer  upon  the original  sense  embeddings  from  the  pre-trained model.
the goal of this interaction is to assist the learning of one sense’s embedding to be aware of the others in the same context.
it is supported by the fact that many sense pairs are more commonly used than the others.
.
in practice, each of the ambiguous words in the document  has  several  candidate  senses,  which poses  questions  about  which  senses  should  be attended in the selective attention layer.
to address this  problem,  we  make  use  of  the  iterative characteristic of the model training.
in other words, the system’s predicted senses of each word within a  particular  context  from the  former  iteration  are attended.
for  the  first  iteration,  the  first  sense  of each word in context is attended.
in such a strategy, the  senses  of  monosemous  words  (has  a  single sense) can be exploited at all iterations.
.
to  enhance  that  of  each  sense .
for  convenient  demonstration,  we  use  the   of  the  context   of  can be a larger context.
.
in .
embedding  of  predicted  senses words  in word in equation (5), 𝑤𝑤𝑖𝑖𝑗𝑗(6), .
is the number of words in  is a learnable weight matrix.
𝑛𝑛.
.
we note that, 𝑆𝑆𝑖𝑖.
𝑠𝑠𝑗𝑗𝑗𝑗.
𝑠𝑠̂𝑝𝑝.
𝑆𝑆𝑖𝑖.
𝑆𝑆𝑖𝑖.
ℎ×ℎ.
𝑊𝑊 ∈ ℝ. .
.
𝑣𝑣̅𝑠𝑠𝑗𝑗𝑗𝑗 = 𝑣𝑣𝑠𝑠𝑗𝑗𝑗𝑗.
[𝐶𝐶𝐶𝐶𝐶𝐶].
𝑖𝑖𝑝𝑝=1(𝑝𝑝≠𝑗𝑗)+ ∑.
𝛼𝛼(𝑠𝑠𝑗𝑗𝑗𝑗, 𝑠𝑠̂𝑝𝑝)[𝐶𝐶𝐶𝐶𝐶𝐶].
[𝐶𝐶𝐶𝐶𝐶𝐶].
∙ 𝑊𝑊𝑣𝑣𝑠𝑠̂𝑝𝑝.
𝛼𝛼(𝑠𝑠𝑗𝑗𝑗𝑗, 𝑠𝑠̂𝑝𝑝) = 𝑊𝑊𝑣𝑣𝑠𝑠𝑗𝑗𝑗𝑗.
the  attention  score  in  (6)  only  takes  into consideration the representation at [cls] position (sentence  level  representation)  for  each  gloss, neglecting  the  relatedness  between  each  gloss word  of  two  senses.
to  tackle  this,  we  devise  a combined  attention  score  by  considering  both  [cls] and gloss word relevance, in equation (7).
is  a  predefined  gloss  length  of  all  senses  for 𝑙𝑙   is  obtained  with normalization.
.
𝑎𝑎𝑣𝑣𝑠𝑠𝑗𝑗𝑗𝑗.
∈ ℝ.ℎ×1.
(5) .
[𝐶𝐶𝐶𝐶𝐶𝐶]𝑣𝑣𝑠𝑠̂𝑝𝑝  .
(6) .
.
equation (2) by changing the output position to if the length (e.g., .
. )
of a sense gloss is smaller than .
is a zero vector where 𝑙𝑙.
is larger than .
.
.
𝑠𝑠.
, .
𝑎𝑎𝑣𝑣𝑠𝑠𝑗𝑗𝑗𝑗.
𝑙𝑙.
𝑠𝑠.
.
𝑙𝑙.
[𝐶𝐶𝐶𝐶𝐶𝐶].
[𝐶𝐶𝐶𝐶𝐶𝐶].
𝛼𝛼�𝑠𝑠𝑗𝑗𝑗𝑗, 𝑠𝑠̂𝑝𝑝� = 𝑊𝑊𝑣𝑣𝑠𝑠𝑗𝑗𝑗𝑗∙ 𝑊𝑊𝑣𝑣𝑠𝑠̂𝑝𝑝1𝑔𝑔𝑔𝑔𝑎𝑎2 ∑∑ (𝑊𝑊𝑣𝑣𝑠𝑠𝑗𝑗𝑗𝑗𝑎𝑎=1𝑏𝑏=1𝑔𝑔4.2  word-level context (wlc) .
𝑏𝑏∙ 𝑊𝑊𝑣𝑣𝑠𝑠̂𝑝𝑝.
).
+.
(7) .
in  many  previous  supervised  systems, the disambiguation  of  one  word  in  a  sentence  is isolated from the words in  the other sentences of the  same  document.
we  convert  the  isolated disambiguation  into  interactive  ones  by  utilizing several  highly  related  sentences  within  the  same document for context embedding learning.
.
for  each  sentence .
,  we  select  its  related sentences  under  two  criteria,  with  one  being  the 𝑆𝑆𝑖𝑖 ,  and  the  other  being  the  semantic distance  to .
the first criterion can be regarded relatedness to 𝑆𝑆𝑖𝑖as  local  features  and  the  second  one  is  aimed  at injecting global features  while maintaining a low noise level.
.
𝑆𝑆𝑖𝑖.
from the perspective of local features, directly surrounding sentences within a window are used as related  sentences.
for  global  features,  we  score context  sentences  and  utilize  the  top  related sentences learning.
for  context  embedding  ,  we  regard  each precisely,  in  a  document   and  calculate  the  tf-sentence  as  a  document  idf score of each word in the vocabulary 𝑑𝑑for  all  sentences.
the  intuition  behind  modeling 𝐷𝐷sentences with tf-idf is that we find the average length  of  semcor  sentences  is  22,  which  is reasonably  long.
this  represents  the  original  ,  where  each document  as  a  matrix row  and  column  indicate  sentence  and  word dimension  respectively.
for  instance,   is  the tf-idf  score  of concerning .
.
the  score  of .
𝑉𝑉𝐷𝐷 ∈ ℝ.𝑚𝑚×|𝑣𝑣|.
of .
in .
𝐷𝐷.
𝑣𝑣.
is shown as follows: 𝑤𝑤𝑖𝑖𝑗𝑗.
𝑆𝑆𝑖𝑖.
𝑉𝑉𝐷𝐷(𝑆𝑆𝑖𝑖, 𝑤𝑤𝑖𝑖𝑗𝑗)𝑆𝑆𝑗𝑗 (8) .
.
𝑆𝑆𝑖𝑖.
sentence .
𝑆𝑆𝑖𝑖𝑠𝑠𝑠𝑠𝑗𝑗𝑠𝑠𝑠𝑠𝐶𝐶𝑖𝑖�𝑆𝑆𝑗𝑗� = 𝑉𝑉𝐷𝐷(𝑆𝑆𝑖𝑖) ∙ 𝑉𝑉𝐷𝐷(𝑆𝑆𝑗𝑗)after  scoring  all  context  sentences  for  each , we concatenate related sentences with  and utilize them as an input to bert for context } embedding learning.
as an example, {𝑆𝑆𝑖𝑖are  related  sentences  from  local  features,  and  if } are top-scored sentences from global { features,  we  use 𝑆𝑆𝑖𝑖+7𝑆𝑆𝑖𝑖−12as  an  input  to  equation  (1)  and  retrieve  the 𝐶𝐶𝑖𝑖 = {𝑆𝑆� 𝑖𝑖−12, 𝑆𝑆̅𝑖𝑖−1, 𝑆𝑆̅𝑖𝑖, 𝑆𝑆̅𝑖𝑖+1, 𝑆𝑆̅𝑖𝑖+7}  of  each  word enhanced  context  embedding .
𝑆𝑆𝑖𝑖+1.
𝑆𝑆𝑖𝑖−1.
, .
, .
𝑣𝑣̅𝑤𝑤𝑖𝑖𝑗𝑗.
5221𝑆𝑆𝑖𝑖.
in .
.
in such a way, different .
is retrieved for each sentence in the document.
we note that, when 𝑤𝑤𝑖𝑖𝑗𝑗the  total  sequence  length  is  longer  than  512,  we  .
for remove  the  furthest  sentences  away  from instance,   and  so  on  in  the  above  , example will be removed in order.
𝑆𝑆̅𝑖𝑖+7  and .
in  equation  (4)  are .
finally, .
𝑆𝑆̅𝑖𝑖−12.
𝐶𝐶𝑖𝑖.
𝑆𝑆̅𝑖𝑖.
[𝐶𝐶𝐶𝐶𝐶𝐶]   and 𝑣𝑣𝑠𝑠𝑗𝑗𝑗𝑗.
to replaced  with 𝑣𝑣𝑤𝑤𝑖𝑖𝑗𝑗calculate the loss, with which to update the weights of the pre-trained model and the selective attention layer.
.
respectively .
𝑣𝑣̅𝑤𝑤𝑖𝑖𝑗𝑗.
𝑣𝑣̅𝑠𝑠𝑗𝑗𝑗𝑗.
4.3  try-again mechanism (tam) .
in  a  previous  similarity-based  wsd  approach, wang  and  wang  (2020)  proposed  a  try-again mechanism (tam) that takes into account not only the  similarity  of  ’s  context  embedding  to  the sense  embedding  of  ,  but  also  to  the  sense 𝑤𝑤𝑖𝑖𝑗𝑗  during  evaluation.
embedding  of 𝑠𝑠𝑗𝑗𝑗𝑗 are connected by either wordnet here, 𝑠𝑠𝑟𝑟 ∈ 𝑠𝑠𝑟𝑟𝑟𝑟𝑟𝑟𝑎𝑎𝑡𝑡𝑟𝑟𝑟𝑟relations  or  the  super-sense  relation  (i.e.,  senses that  belong  to  the  same  super-sense  category  in wordnet).
this  mechanism  in  (9)  manages  to boost  the  performance  of  its  knowledge-based system by a relatively large margin.
.
and .
𝑠𝑠𝑗𝑗𝑗𝑗.
𝑠𝑠𝑟𝑟.
(9) .
𝑠𝑠𝑟𝑟∈𝑠𝑠𝑟𝑟𝑟𝑟𝑟𝑟𝑟𝑟𝑟𝑟𝑟𝑟𝑟𝑟 (𝑣𝑣𝑤𝑤𝑖𝑖𝑗𝑗 ∙ 𝑣𝑣𝑠𝑠𝑟𝑟) .
𝑠𝑠𝑖𝑖𝑠𝑠�𝑤𝑤𝑖𝑖𝑗𝑗, 𝑠𝑠𝑗𝑗𝑗𝑗� = 𝑣𝑣𝑤𝑤𝑖𝑖𝑗𝑗 ∙ 𝑣𝑣𝑠𝑠𝑗𝑗𝑗𝑗 + maxin this subsection, we reconstruct tam so that it becomes effective in our model.
this process helps the  disambiguation  of  words  to  be  even  more interactive since it considers an increased number of senses by utilizing sense relation knowledge.
.
in  our  implementation,  we  replace  the  above relations  with  only  those  derived  from  coarse sense  inventory  (csi,  lacerra  et  al.,  2020).
similar to the utilization of super-sense categories, we connect senses that belong to the same label in csi as related senses.
also, we change the direct sum of the above two similarities into a weighted sum using a hyperparameter .
.
.
𝛽𝛽.
𝑠𝑠𝑖𝑖𝑠𝑠�𝑤𝑤𝑖𝑖𝑗𝑗, 𝑠𝑠𝑗𝑗𝑗𝑗� = (1 − 𝛽𝛽) ∗ 𝑣𝑣̅𝑤𝑤𝑖𝑖𝑗𝑗 ∙ 𝑣𝑣̅𝑠𝑠𝑗𝑗𝑗𝑗 + 𝛽𝛽 ∗.
(10) .
𝑠𝑠𝑟𝑟∈𝑠𝑠𝑟𝑟𝑟𝑟𝑟𝑟𝑟𝑟𝑟𝑟𝑟𝑟𝑟𝑟 (𝑣𝑣̅𝑤𝑤𝑖𝑖𝑗𝑗 ∙ 𝑣𝑣̅𝑠𝑠𝑟𝑟)                                   maxin  addition,  our  approach  only  learns  a  sense embedding for the candidate senses whose lemma is annotated in training data.
therefore, in tam, we save  sense  embeddings  from  training  for  each .
.
epoch  and  use  them  to  implement  tam  during evaluation.
it is worth mentioning that for senses , we that do not have a sense embedding in neglect their calculation in equation (10).
.
𝑠𝑠𝑟𝑟𝑟𝑟𝑟𝑟𝑎𝑎𝑡𝑡𝑟𝑟𝑟𝑟.
5  experiment settings .
5.1  datasets .
report .
to validate the effectiveness of our approach, we use semcor and an evaluation framework† to train and  evaluate  our  model,  sacebase,  respectively.
the evaluation framework contains 5 english all-words  wsd  benchmarks.
we the experimental  results  on  each  dataset  including senseval-2 (se2, palmer et al., 2001), senseval-3 (se3, snyder and palmer, 2004), semeval-2007 task-17  (se07,  pradhan  et  al., 2007),  semeval-2013  (se13,  navigli et  al., 2013)  and  semeval-2015  (se15,  moro  and navigli, 2015).
also, the results from part-of-speech (pos) perspectives on their  combined  dataset  (all)  are  reported.
following previous works, we train large models, sacelarge on semcor and sacelarge+ on semcor, wordnet  gloss  tagged  (wngt),  and  wordnet examples (wne) for fair comparisons.
here, wne is  regarded  as  an  extra  sense  gloss  and  is concatenated  after  the  original  sense  gloss  for sense embedding learning, which is similar to the implementation in sref (wang and wang, 2020).
for few-shot wsd, we partition all according to the gold label of each annotation into allwn_1st and  allwn_others.
besides,  according  to  whether senses  and  lemmas  of  all  instances  appear  in semcor,  we  extract  two  subsets,  allzss  and allzsl, to evaluate the zero-shot learning ability of our model.
.
for cross-lingual datasets, we use the wordnet version of the latest evaluation framework‡ which contains test datasets for spanish, italian, french, and german.
these datasets are preprocessed data from  semeval-2013  (navigli  et  al.,  2013)  and semeval-2015  (moro  and  navigli,  2015).
the former only disambiguates nouns while the latter covers  words  in  four  pos  (noun-n,  verb-v, adjective-a, adverb-r).
.
we  note  that  the  performance  in  each  table  is .
reported with f1 in percentage.
.
5.2  model design .
† http://lcl.uniroma1.it/wsdeval/home .
‡ https://github.com/sapienzanlp/mwsd-datasets .
5222respectively,  which .
our base and large model utilize robertabase and robertalarge perform relatively  better  than  bert  models.
for  cross-lingual evaluation, we fine-tune xlm-roberta-base  (sacemul,  conneau  et  al.,  2020)  with  the same  training  data  as  sacelarge+,  following  the setting in ewiser.
in each system, two encoders are adopted, with one being a context encoder and the  other  being  a  sense  gloss  encoder.
this  is identical  to  the  setting  in  bem.
we  note  that  a major  difference  is  that  the  pre-trained  model adopted in the above papers is bert.
.
the hyperparameters of our model are selected the  number  of include using  se07.
they , the surrounding sentences (2) on both sides of  number  of  top  related  sentences  (2)  of (0.1)  in  tam.
the  learning  rate  for  sacebase, 𝛽𝛽sacelarge, sacelarge+, and sacemul is 1e-5, 1e-6, 1e-6, and 5e-6 respectively.
.
and 𝑆𝑆𝑖𝑖.
𝑆𝑆𝑖𝑖.
to  accelerate  the  model  training,  we  organize the sentences in a document into batches according to  the  total  number  of  candidate  senses  (400  for sacebase  and  sacemul,  150  for  sacelarge  and sacelarge+),  i.e.,  if  the  total  number  of  candidate senses exceeds 400 or 150 when adding a sentence, then  the  sentence  belongs  to  the  next  batch.
for each batch, the gloss and context encoders are only called  once.
the  context  and  gloss  length  is normalized to the maximal sequence length within each  batch  to  reduce  unnecessary  padding  and computation.
also,  apex  is  employed  for  mixed-precision  computing.
more  details  are  shown  in appendix a. .
5.3  baselines .
from .
systems .
state-of-the-art .
we  compare  the  proposed  model  with  previous supervised different perspectives.
these include  sense vocabulary compression (svc, vial et al., 2019), ewise  (kumar  et  al.,  2019),  lmms  (loureiro and jorge, 2019), glu (hadiwinoto et al., 2019), glossbert  (huang  et  al.,  2019),  ewiser (bevilacqua  and  navigli,  2020),  bem  (blevins and  zettlemoyer,  2020),  ares  (scarlini  et  al., 2020b) and sref (wang and wang, 2020).
bem is our direct baseline, which utilizes two encoders to  learn  context  and  sense  embedding  separately and achieves state-of-the-art with only semcor.
.
for  cross-lingual  evaluation,  we  compare  our results with those reported in syntagnet, ewiser, ares, mulan (barba et al., 2020).
these systems are all recently proposed systems with state-of-the-.
ablation study .
sacebase .
all .
80.9 .
∆ .
0 .
-w/o wlc 79.4 -w/o slc([cls]+word)  79.7 80.3 -w/o tam 80.4 -w/o slc(word) 78.4 -w/o all .
-1.5 -1.2 -0.6 -0.5 -2.5 .
table 1: ablation study of sacebase on all .
art performance.
.
6  results .
6.1  ablation analysis .
illustrated between .
that  enhancing different .
in  this  subsection,  we  demonstrate  how  each component  of  our  model  benefits  wsd performance.
in table 1, the system’s performance on  all  has the words’ interaction disambiguation in the same document (wlc) can raise  the  system’s  performance  by  the  largest margin,  1.5  f1.
this  promotion  is  slightly  larger than that (1.2 f1) provided by the interactive sense embedding learning  (slc).
the  gloss  word attention  in  slc  is  also  proved  effective,  which helps increase the system’s performance by 0.5 f1, similar  to  the  contribution  of tam,  0.6  f1.
most importantly, when all components are removed, the performance on all decreases to 78.4 f1.
we note that the baseline here is different from bem since we  remove  unnecessary  padding  and  utilize roberta.
this  has  dramatically  accelerated  the training  process  from  3.5  hours  to  0.5  hour  per epoch  while  achieved  similar  performance.
we also note that the experimental results reported in this paper are obtained using the same random seed as  bem.
with  different  random  seeds,  the performance gap on all between sacebase and its baseline (-w/o all) ranges from 1.7 f1 to 2.7 f1.
.
6.2  all-words wsd .
table 2 demonstrates how our systems and lately proposed baselines perform on different partitions of all.
when it is trained on semcor, sacebase has already outperformed all its competitors by at least  1.9  f1,  on  all.
this  is  obtained  without utilizing  prior  sense  relation  knowledge.
it  is  the first  system  that  surpasses  the  estimated  upper bound (80 f1) of the task using only semcor.
.
except glossbert and bem, the other systems adopt bertlarge as their pre-trained model.
when .
5223training data .
systems .
datasets .
concatenation of all datasets v .
n .
a .
r .
svc (gwnc2019) ewise (acl2019) lmms (acl2019) .
se2  se3  se07  se13  se15  all 76.7 77.5  77.4  69.5 79.6  65.9  79.5  85.5 76.0 71.8*  74.0  60.2  78.0  82.1 73.8  71.1  67.3*  69.4 75.4 78.0  64.0  80.5  83.5 75.1 76.3  75.6  68.1 76.8* glossbert (emnlp2019)  77.7  75.2  72.5*  76.1 73.7* 75.5  73.6  68.1*  71.1 77.9 77.3 78.0  77.1  71.0 78.0 78.6  76.6  72.1 77.8 78.9 78.9  78.4  71.0 79.4  77.4  74.5*  79.7 80.9  79.1  74.7*  82.4 82.4  81.1  76.3*  82.5 78.7 79.7  77.8  73.4 80.7 80.8  79.0  75.2 82.4 83.6  81.4  77.8 .
78.3 74.5 77.0 80.4 76.2 80.6  68.3  80.5  83.5 83.2 80.5 80.6  66.5  82.6  84.4 79.3*  78.3*  81.7  66.3  81.2  85.8 79.0*  81.4  68.5  83.0  87.9 81.7 80.9*  83.2  71.1  85.4  87.9 84.6 81.9*  84.1  72.2  86.4  89.0 83.7 82.6 81.4  68.7  83.7  85.5 79.0 81.8*  80.1*  82.9  69.4  83.6  87.3 87.3*  82.9*  85.3  74.2  85.9  87.3 .
glu (emnlp2019) ares (emnlp2020) sref (emnlp2020) ewiser (acl2020) bem (acl2020) sacebase sacelarge svc (gwnc2019) ewiser (acl2020) sacelarge+ .
- - .
- - .
- - .
- - .
semcor .
semcor +wngt +wne .
table 2: english all-words wsd performance on different partitions of all utilizing two sets of training data.
following sref, those marked with * are (partially) obtained as a validation set.
sota is in bold.
.
systems  also  obtain .
we use robertalarge, sacelarge can further reach 81.9 f1 on all, surpassing the previous state-of-the-art  by  2.9  (3.7%  of  79.0)  f1.
this  is  a  large margin  given  that  bem  and  ewiser  are  strong baselines.
when extra training data and wne are employed, a similar margin, 2.8 f1, is attained on all.
our .
state-of-the-art performance  on  each  dataset,  with  the  margin ranging from 0.2 to 2.9 f1 for sacebase and 1.8 to 3.0 f1 for  sacelarge, in the first category.
as for sacelarge+,  the  margin  above  the  previous  best system for each dataset is even larger, varying from 1.7  to  5.5  f1.
it  is  noteworthy  that  sacebase outperforms sacelarge by 0.9 f1 on se15 and they obtain  similar  performance  on  se13.
these  two datasets are less ambiguous since each lemma has fewer candidate senses on average.
this illustrates the  competitive  disambiguation  capability  of sacebase on easier instances.
we also note that the development set in two categories is different, with the first being se07  and the  second being se15.
this is because we follow most systems’ setting in the first category and follow ewiser’s setting in the second category for better comparison.
.
for  the  performance  on  different  pos,  our systems set new lines for all of them in all.
the largest  advancement  comes  from  the  higher disambiguation ability of verbs, making our system the first to reach the line of 70 f1.
the systems also obtain  unprecedented  performance  on  noun disambiguation,  surpassing  the  previous  best system by 1.5, 2.4, and 2.4 for sacebase, sacelarge, and sacelarge+ respectively.
sacelarge+ is the only system that exceeds 85 f1 on noun disambiguation.
.
6.3  rare and unseen sense disambiguation .
rare  sense  disambiguation  table  3  reports different  systems’  performance  on allwn_1st  and allwn_others, which has 4278 and 2525 annotations respectively.
compared  with  previous  well-performing systems including lmms  and sref, our systems achieve much better performance on both datasets, with the major contribution coming from  wordnet  1st  sense  disambiguation.
on  the contrary,  sace  and  bem  obtain  similar performance  on  allwn_1st  while  sace  can disambiguate  rare  senses  with  higher  accuracy.
this  shows  a  better  few-shot  learning  ability  of sace  in  comparison  to  bem  because  the allwn_others dataset only contains the words whose correct sense appears infrequently in semcor.
.
here,  sense  disambiguation  is  defined  as whether a system can select the sense as the correct sense, which is viewed from a sense perspective.
in comparison, word or lemma disambiguation is to determine the  correct  sense  of  a  word  or  lemma, which is viewed from a word perspective.
.
unseen  sense  disambiguation  in  the  second column of table 4, different system’s performance on allzss (691 polysemous instances) is provided.
this  dataset  only  contains  polysemous  words whose gold label is not in semcor, which evaluate the  zero-shot  sense  disambiguation  ability  of different systems.
it is shown that lately proposed systems have an overwhelming advantage of zero-shot sense disambiguation over ordinary baselines including  wordnet  s1  and  bert-base,  with  the margin ranging from about 12 f1 to about 42 f1.
its specifically,  although  bem  outperforms .
5224  models .
wordnet 1st lmms sref bem sacebase sacelarge sacelarge+ .
allwn_1st (n=4728) 100 87.6 91.0 93.6 94.2 94.1 94.7 .
allwn_other (n=2525) 0 52.6 53.2 51.7 56.1 59.0 60.8 .
.
table 3: rare sense disambiguation on all .
baselines  by  around  25  f1,  our  base  and  large system  still  beat  bem  by  almost  12  and  18  f1 respectively.
.
in the third column, we follow previous works and  show  how  different  systems  perform  on allzss*  (1139  instances  including  monosemous ones).
the aforementioned gaps become narrower since  each  system  can  correctly  disambiguate monosemous instances.
.
these .
two  datasets .
unseen lemma disambiguation in the last two columns  of  table  4,  the  systems’  performance  on zero-shot  lemmas  is  presented.
the  difference between is  whether monosemous lemmas are included.
we believe it is to  focus  on  allzsl  (222 more  reasonable polysemous instances) since monosemous lemmas do  not  require  disambiguation  and  thus  the statistics  on  allzsl*  cannot  fully  reveal  the systems’ zero-shot disambiguation ability of words.
similarly, it shows that lately proposed systems tend to outperform the baselines by large margins, varying  from  19  to  almost  36  f1.
among  them, bem  performs  the  worst  on  this  dataset,  2.2  f1 lower  than  a  similar  system,  glossbert.
in contrast, after incorporating both word and sense level context, our system obtains an unprecedented performance on this dataset, being the first system to  reach  the  line  of  90  f1  and  beating  bem  by almost  16  f1.
also,  different  from  sref  and ares,  our  systems  do  not  rely  on  wordnet  or syntagnet sense relation knowledge.
.
6.4  cross-lingual all-words wsd .
subsets) .
to  evaluate .
we  utilize  two  multilingual  datasets  (including french-fr,  german-de,  italian-it,  and  spanish-es the  multilingual transferability of our method.
table 5 presents the performance of some lately proposed systems and ours.
for our system, the baseline is trained with the same training data as sacelarge+ using xlm-roberta-base,  while  removing  all  the  proposed .
models .
wordnet 1st  bert-base lmms glossbert ares sref bem sacebase sacelarge .
allzss (n=691) 24.0 23.5 36.7 37.4 42.6 46.1 48.7 60.4 66.2 .
allzss* (1139) 53.9 53.6 61.6 62.0 65.2 67.3 68.9 76.0 79.5 .
allzsl (222) 54.4 54.4 74.8 75.6 81.1 82.4 73.4 90.0 90.0 .
allzsl* (670) 84.9 84.9 91.7 91.9 93.7 94.2 91.2 96.7 96.7 .
.
table 4: zero-shot lemma and sense disambiguation.
the datasets marked with * include monosemous instances.
.
components including slc, wlc, and tam.
for the systems under comparison, all but ukb+syn utilizes english training data.
also, ewiser and mulan further  employ  semcor  and  wngt  as  their training data, being the same as sacemul.
.
it shows that sacemul has obtained a new state-of-the-art on both the combined dataset and most individual  datasets,  surpassing  its  direct  baseline by 2.4 f1.
in detail, the largest margin, about 5.5 f1  on  its  spanish  and  italian  subset,  above  the previous best system is acquired on se15, which covers instances in all pos.
this has revealed the advantage  of  sacemul  on overwhelming disambiguating instances of other pos.
in contrast, sacemul  performs  6.5  f1  lower  than  mulan  on the  spanish  subset  of  se13,  which  only  covers noun  instances.
in  a  word,  sacemul  is  more compatible with real cross-lingual scenarios since it has a strong disambiguation ability of words in different pos.
.
6.5  analysis .
error analysis by comparing the disambiguation results  of  sacebase  and  its  baseline  (all  factors removed),  it  is  revealed  that  both  systems  have correctly  disambiguated  5346  instances  in  all while  525  and  339  instances  are  only  correctly disambiguated  by  sacebase  and  its  baseline respectively.
in other words, sacebase has falsely .
it  average .
se15 .
it  es .
se13 de  es  fr .
ukb+syn  76.4  74.1  70.3  72.1  63.4  69.0 ewiser  80.9  78.8  83.6  77.7  69.5  71.8 mulan  82.3  81.1  81.6  77.9  69.4  71.8 ares 79.6  75.3  81.2  77.0  70.1  71.4 baseline  80.5  74.9  80.7  73.6  72.7  74.9 sacemul  82.6  74.6  83.0  78.1  75.6  77.3 .
71.1 77.5 77.8 76.2 76.3 78.7 .
table 5: multilingual all-words wsd .
5225/ .
10 .
id  score .
47  0.969 .
19  0.807 .
sentence they  belong  to  a  group  of  ringers  who  drive  every  sunday  from  church  to  church  in  a sometimes-exhausting effort to keep the bells sounding in the many belfries of east anglia.
"the sound of bells is a net to draw people into the church," he says.
proper english bells are started off in "rounds, " from the highest-pitched bell to the lowest -- a simple descending scale using, in larger churches, as many as 12 bells.
immigration policy under nicolas sarkozy was criticized from various aspects a congestion of  police,  legal  and  administrative  services  subjected  to  a  policy  of  numbers  and  the compatibility of that policy with the self-proclaimed status of the country as the country of french human rights.
0 is immigration a burden or an opportunity for the economy?
13  0.476  restraining immigration leads to anaemic growth and harms employment.
.
0.384 .
1 .
/ .
table 6: two examples of wlc .
synset-1 family.n.01 pilot_burner.n.01 cruise.v.01 republican.a.01 time.n.02 sport.n.05 .
synset-2 member.n.01 burner.n.01 travel.v.01 democratic.a.02 take.v.02 player.n.01 .
table 7: related synsets by slc .
predicted 339 examples that are correctly predicted by  its  baseline.
this  indicates  the  proposed methods  might  have  injected  excessive  noise  for the  disambiguation  of  these  instances.
therefore, selective  exploitation  of  context  for  different instances might be beneficial.
.
the  bottom  half  of  table  6  shows  an  example (country)  that  sacebase  falsely  predicted.
it  is shown  that  the wlc  does  not  manage  to  retrieve valuable information for disambiguating the word while injecting some irrelevant context.
.
case  study  table  6  gives  an  example  of  top related  sentences  (#47  and  #19)  of  a  particular sentence (#10) under disambiguation.
here, church is falsely predicted when wlc is disabled.
it shows that wlc has detected similar sentences in the same document  and  incorporated  valuable  context  for context embedding learning.
.
table  7  provides  some  examples  regarding synsets that are connected by the selective attention layer,  indicating  its  ability  of  detecting  some syntagmatic  sense  relations  and  senses  of  close meaning.
the  connection  is  established  by  using   in  a  batch the  largest  attention  score after filtering self-connection.
.
𝛼𝛼�𝑠𝑠𝑗𝑗𝑗𝑗, 𝑠𝑠̂𝑝𝑝�.
7  conclusion .
in  this  paper,  we  propose  an  interactive  context .
exploitation  method  from  both  word  and  sense perspectives in a supervised similarity-based wsd architecture.
experiments  on  english  and  cross-lingual  all-words  wsd  datasets  verify the effectiveness of our approach, surpassing previous state-of-the-art by large margins.
it also shows that the  proposed  method  has  an  overwhelming advantage of learning few-shot and zero-shot wsd ability.
for  future  work,  we  intend  to  utilize to  enhance  current reinforcement interactive  wsd  by  customizing  the  context exploitation  for  different  instances.
the  source code at: is https://github.com/lwmlyy/sace.
.
available .
learning .
8  ethics impact statement .
this paper does not involve the presentation of a new dataset, an nlp application and the utilization of  demographic  or  identity  characteristics  in formation.
for compute time/power, the proposed system  requires  less  gpu  amount  (1  versus  2 gpus)  and  time  (10  versus  about  70  hours)  for training compared with its direct baseline (blevins and zettlemoyer, 2020).
.
acknowledgments .
we thank the anonymous reviewers and jianzhang zhang  for  their  insightful  comments.
this  work was  supported  by  the  national  natural  science foundation of china (under project no.
61375053) and  the  graduate  innovation  fund  of  shanghai university  of  finance  and  economics  (under project no.
cxjj-2019-395).
.
references  .
eneko agirre, oier lópez de lacalle and aitor soroa.
2014.  random  walks  for  knowledge-based  word sense  disambiguation.
computational  linguistics, .
522640(1): 57-84. .
eneko agirre, oier lópez de lacalle, and aitor soroa.
2018. the risk of sub-optimal use of open source nlp software: ukb is inadvert-ently state-of-the-art  in  knowledge-based  wsd.
in  proceedings  of workshop for nlp open source software, pages 29-33,  melbourne,  australia:  association for computational linguistics.
.
edoardo barba, luigi procopio, niccolò campolungo, tommaso  pasini  and  roberto  navigli.
2020. mulan:  multilingual  label  propagation  for  word sense disambiguation.
in ijcai-2020, pages 3837-3844. .
pierpaolo  basile,  annalina  caputo,  and  giovanni semeraro.
2014.  an  enhanced  lesk  word  sense disambiguation  algorithm  through  a  distributional semantic  model.
in  coling  2014,  pages  1591-1600, dublin, ireland.
.
michele  bevilacqua  and  roberto  navigli.
2020. breaking through the 80% glass ceiling: raising the state of the art in word sense disambiguation by incorporating knowledge graph information.
in acl  2020,  pages  2854-2864.  association  for computational linguistics.
.
terra  blevins  and  luke  zettlemoyer.
2020.  moving down the long tail of word sense disambiguation with  gloss  informed  bi-encoders.
in  acl  2020, pages  1006-1017.  association  for  computational linguistics.
.
alexis conneau, kartikay khandelwal, naman goyal, vishrav chaudhary, guillaume wenzek, francisco guzmán,  edouard  grave,  myle  ott,  luke zettlemoyer, veselin stoyanov.
2020. unsupervised cross-lingual  representation  learning  at  scale.
in acl  2020,  pages  8440-8451.  association  for computational linguistics.
.
devendra  singh  chaplot  and  ruslan  salakhutdinov.
2018. sense disambiguation using topic models.
in aaai 2018. .
knowledge-based .
word .
jacob  devlin,  ming-wei  chang,  kenton  lee,  and kristina  toutanova.
2019.  bert:  pre-training  of deep  bidirectional language understanding.
in naacl 2019, pages 4171-4186, minneapolis, minnesota.
.
transformers .
for .
christian hadiwinoto, hwee tou ng and wee chung gan.
improved word  sense  disambiguation  using pre-trained  contextualized  word  representations.
in emnlp-ijcnlp 2019, pages 3507-3512, hong kong, china.
.
luyao  huang,  chi  sun,  xipeng  qiu  and  xuanjing huang.
glossbert:  bert  for  word  sense in disambiguation  with  gloss  knowledge.
.
emnlp-ijcnlp  2019,  pages  3507-3512,  hong kong, china.
.
sawan  kumar,  sharmistha  jat,  karan  saxena  and partha  talukdar.
2019.  zero-shot  word  sense definition disambiguation embeddings.
in  proceedings  of  the  57th  annual meeting  of  the  association  for  computational linguistics,  pages  5670-5681,  florence,  italy.
association for computational linguistics.
.
sense .
using .
caterina  lacerra,  michele  bevilacqua,  tommaso pasini, roberto navigli.
2020. csi: a coarse sense inventory  for  85%word  sense  disambiguation.
in proceedings of the thirty-fourth aaai conference on  artificial intelligence,  pages  8123-8130. association  for  the  advancement  of  artificial intelligence.
.
michael lesk.
1986. automatic sense disambiguation using machine readable dictionaries: how to tell a pine cone from an ice cream cone.
in sigdoc ’86, pages 24-26, new york, ny, usa.
acm.
.
yinhan  liu,  myle  ott,  naman  goyal,  jingfei  du, mandar  joshi,  danqi  chen,  omer  levy,  mike lewis, luke zettlemoyer, veselin stoyanov.
2019. roberta: a robustly optimized bert pretraining approach.
in arxiv:1907.11692. .
daniel  loureiro  and  alípio  mário  jorge.
2019. language  modelling  makes  sense:  propagating representations through wordnet for full coverage word  sense  disambiguation.
in  acl  2019,  pages 5682-5691, florence, italy.
.
fuli luo, tianyu liu, qiaolin xia, baobao chang, and zhifang sui.
2018. incorporating glosses into neural word  sense  disambiguation.
in  acl  2018,  pages 2473-2482,  melbourne, australia.
association  for computational linguistics.
.
marco maru, federico scozzafava, federico martelli, and roberto navigli.
2019. syntagnet: challenging supervised word sense disambiguation with lexical-semantic combinations.
in proc.
of emnlp, pages 3525-3531.  association for  computational linguistics.
.
george  a.  miller,  martin  chodorow,  shari  landes, claudia  leacock,  and  robert  g.  thomas.
1994. using sense identification.
language technology:  proceedings  of  a  workshop  held at plainsboro, new jersey, march 8-11, 1994. .
concordance human .
semantic .
for .
in .
a .
george a. miller.
1995. wordnet: a lexical database for  english.
communications  of  the  acm,  41(2): 39-41. .
andrea  moro  and  roberto  navigli.
2015.  semeval-2015 task  13:  multilingual  all-words  sense disambiguation and entity linking.
in semeval 2015, .
5227pages 288-297, denver, colorado.
.
roberto navigli, david jurgens, and daniele vannella.
2013.  semeval-2013  task  12:  multilingual  word sense  disambiguation.
in  semeval  2013  *sem, pages 222-231, atlanta, georgia, usa.
.
roberto  navigli,  kenneth  c.  litkowski,  and  orin hargraves.
2007.  semeval-2007  task  07:  coarse grained  english  all-words  task.
in  proceedings  of the  4th  international  workshop  on  semantic evaluations,  semeval  ’07,  pages  30–35,  prague, czech republic.
.
roberto navigli.
2009. word sense disambiguation: a survey.
acm computing surveys, 41(2):10:1-10:69. .
martha  palmer,  christiane  fellbaum,  scott  cotton, lauren delfs, and hoa trang dang.
2001. english tasks:  all-words  and  verb  lexical  sample.
in proceedings  of  senseval-2,  pages  21-24, toulouse, france.
.
simone  paolo  ponzetto  and  roberto  navigli.
2010. knowledge-rich  word  sense  disambiguation rivaling supervised systems.
in proceedings of the 48th  annual  meeting  of for computational  linguistics,  pages  1522-1531. association for computational linguistics.
.
the  association .
navigli.
.
alessandro  raganato,  jose  camacho-collados,  and roberto sense disambiguation:  a  unified  evaluation  framework and empirical comparison.
in eacl 2017, pages 99-110, valencia, spain.
.
2017a.
word .
alessandro raganato, claudio delli bovi, and roberto navigli.
2017b.
neural  sequence  learning  models for  word  sense  disambiguation.
in  proceedings  of the  2017  conference  on  empirical  methods  in natural  language  processing,  pages  1156-1167, for denmark.
copenhagen, computational linguistics.
.
association .
bianca  scarlini,  tommaso  pasini,  roberto  navigli.
2020a.
sensembert:  context-enhanced  sense sense embeddings disambiguation.
in aaai 2020. .
for  multilingual  word .
contexts .
bianca scarlini, tommaso pasini and roberto navigli.
comes  better 2020b.
with  more performance: contextualized sense embeddings for all-round  word  sense  disambiguation.
in  the  2020 conference  on  empirical  methods  in  natural for processing.
language computational linguistics.
.
association .
benjamin  snyder  and  martha  palmer.
2004.  the english all-words task.
in senseval-3, pages 41-43, barcelona, spain.
.
loïc  vial,  benjamin  lecouteux  and  didier  schwab.
the .
sense  vocabulary  compression .
through .
semantic  knowledge  of  wordnet  for  neural  word sense  disambiguation.
in  proceedings  of  the  10th global wordnet conference.
.
ming wang and yinglin wang.
2020. a synset relation-enhanced  framework  with  a  try-again  mechanism for  word  sense  disambiguation.
in  the  2020 conference  on  empirical  methods  in  natural language for processing.
computational linguistics.
.
association .
yinglin wang, ming wang and hamido fujita.
2020. word  sense  disambiguation:  a  comprehensive knowledge  exploitation  framework.
knowledge-based systems, 10530. .
zhi zhong and hwee tou ng.
2010. it makes sense: a wide-coverage  word  sense  disambiguation  system for free text.
in acl 2010 system demonstrations, pages 78-83, uppsala, sweden.
.
appendix .
a  experimental setting .
computing infrastructure we use pytorch deep learning  infrastructure  along  with  transformers and apex to implement our model.
other required packages  can  be  found  in  readme.md  file  in  the source code.
.
runtime the average training time for sacebase, sacelarge,  sacelarge+  and  sacemul  is  10  hours, 20 hours, 59 hours and 17 hours, respectively.
.
parameters  the  parameters  include  those  from the  pre-trained  models  such  as  roberta-base, roberta-large  and  xlm-roberta-base,  and those from the selective attention layer (6 heads * 768/1024 * 768/1024).
.
evaluation metrics we use f1-measure to report the  evaluation  results.
for  systems  that  can provide  sense  predictions  for  each  lemma,  f1-measure is equal to accuracy, which is the number of  instances  that  are  correctly  predicted  by  the model.
see navigli, 2009 for details.
.
β in tam .
0.1, 0.2, 0.3, 0.4, 0.5 .
wic local sentences .
wic global sentences lr .
gloss_batch-size .
1, 2, 3, 4, 5 .
1, 2, 3, 4, 5 .
1e-5, 5e-5, 1e-6, 5e-6 .
150, 200, 250, 300, 350, 400 .
table 1: hyperparameter bounds and optimal setting .
5228f1 on dev set (se07)76.3 75.8 75.4.
73.6.
74.5.
74.7.
74.5.
73.2.
72.7.
73.
75.2.
74.5.
74.5 74.7.
74.3.
71.4 72.3.
71.9.
77.
75.
73.
69.
67.
71.
69.9.
67.9.
1.
2.
3.
4.
5.
6.
7.
8.
9.
10.sacebase.
sacelarge.
.
figure 1: f1 on se07 of sacebase and sacelarge .
in .
hyperparameter  search  the  bounds  for  each hyperparameter  are table  1,  with listed configurations for  best  performing  models underlined.
we  use  the  f1-measure  on  se07  to select the values.
all the details are shown in the source code.
for those that have two underlined numbers,  they  are  the  best  setting  for  base  and large models.
.
b  experimental results .
in figure 1, we show how sacebase and sacelarge perform on se07 at each epoch during training.
it is  shown  that  both  systems  reach  their  optimal performance on se07 at early epoch, 3rd or 4th epoch.
this indicates if we utilize the method of early stopping during training, its time efficiency can further be enlarged.
.
5229