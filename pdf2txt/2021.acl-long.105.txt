exploiting language relatedness for low web-resource languagemodel adaptation: an indic languages study.
yash khemchandani1∗ sarvesh mehtani1∗ vaidehi patil1sunita sarawagi1partha talukdar2abhijeet awasthi11indian institute of technology bombay, india2google research, india{yashkhem,smehtani,awasthi,sunita}@cse.iitb.ac.in.
vaidehipatil@ee.iitb.ac.in,.
partha@google.com.
abstract.
recent research in multilingual language mod-els (lm) has demonstrated their ability toeffectively handle multiple languages in asingle model.
this holds promise for lowweb-resource languages (lrl) as multilingualmodels can enable transfer of supervision fromhigh resource languages to lrls.
however, in-corporating a new language in an lm still re-mains a challenge, particularly for languageswith limited corpora and in unseen scripts.
inthis paper we argue that relatedness among lan-guages in a language family may be exploitedto overcome some of the corpora limitations oflrls, and propose relatelm.
we focus on in-dian languages, and exploit relatedness along(1) script (since many in-two dimensions:dic scripts originated from the brahmic script),and (2) sentence structure.
relatelm usestransliteration to convert the unseen script ofinto the script of a re-limited lrl textlated prominent language (rpl) (hindi in ourcase).
while exploiting similar sentence struc-tures, relatelm utilizes readily available bilin-gual dictionaries to pseudo translate rpl textinto lrl corpora.
experiments on multiplereal-world benchmark datasets provide valida-tion to our hypothesis that using a related lan-guage as pivot, along with transliteration andpseudo translation based data augmentation,can be an effective way to adapt lms for lrls,rather than direct training or pivoting throughenglish..1.introduction.
bert-based pre-trained language models (lms)have enabled signiﬁcant advances in nlp (devlinet al., 2019; liu et al., 2019; lan et al., 2020).
pre-trained lms have also been developed for the mul-tilingual setting, where a single multilingual modelis capable of handling inputs from many different.
∗authors contributed equally.
figure 1: number of wikipedia articles for top-few in-dian languages and english.
the height of the englishbar is not to scale as indicated by the break.
number ofenglish articles is roughly 400x more than articles inoriya and 800x more than articles in assamese..languages.
for example, the multilingual bert(mbert) (devlin et al., 2019) model was trainedon 104 different languages.
when ﬁne-tuned forvarious downstream tasks, multilingual lms havedemonstrated signiﬁcant success in generalizingacross languages (hu et al., 2020; conneau et al.,2019).
thus, such models make it possible totransfer knowledge and resources from resourcerich languages to low web-resource languages(lrl).
this has opened up a new opportunity to-wards rapid development of language technologiesfor lrls..however, there is a challenge.
the currentparadigm for training mutlilingual lm requirestext corpora in the languages of interest, usually inlarge volumes.
however, such text corpora is oftenavailable in limited quantities for lrls.
for exam-ple, in figure 1 we present the size of wikipedia,a common source of corpora for training lms, fortop-few scheduled indian languages1 and english.
the top-2 languages are just one-ﬁftieth the size of.
1according to indian census 2011, more than 19,500 lan-guages or dialects are spoken across the country, with 121 ofthem being spoken by more than 10 thousand people..proceedingsofthe59thannualmeetingoftheassociationforcomputationallinguisticsandthe11thinternationaljointconferenceonnaturallanguageprocessing,pages1312–1323august1–6,2021.©2021associationforcomputationallinguistics1312figure 2: pre-training with mlm and alignment loss in relatelm with lrl l as punjabi (pa) in gurumukhiscript and rpl r as hindi (hi) in devanagari script.
relatelm ﬁrst transliterates lrl text in the monolingualcorpus (dl) and bilingual dictionaries (bl→r and br→l) to the script of the rpl r. the transliterated bilingualdictionaries are then used to pseudo translate the rpl corpus (dr) and transliterated lrl corpus (dlr ).
thispseudo translated data is then used to adapt the given lm m for the target lrl l using a combination of maskedlanguage model (mlm) and alignment losses.
for notations and further details, please see section 3..english, and yet hindi is seven times larger thanthe o(20,000) documents of languages like oriyaand assamese which are spoken by millions of peo-ple.
this calls for the development of additionalmechanisms for training multilingual lms whichare not exclusively reliant on large monolingualcorpora..recent methods of adapting a pre-trained mul-tilingual lm to a lrl include ﬁne-tuning the fullmodel with an extended vocabulary (wang et al.,2020), training a light-weight adapter layer whilekeeping the full model ﬁxed (pfeiffer et al., 2020b),and exploiting overlapping tokens to learn embed-dings of the lrl (pfeiffer et al., 2020c).
these aregeneral-purpose methods that do not sufﬁcientlyexploit the speciﬁc relatedness of languages withinthe same family..we propose relatelm for this task.
relatelmexploits relatedness between the lrl of interestand a related prominent language (rpl).
wefocus on indic languages, and consider hindi asthe rpl.
the languages we consider in this pa-per are related along several dimensions of linguis-tic typology (dryer and haspelmath, 2013; lit-tell et al., 2017): phonologically, phylogenetically.
as they are all part of the indo-aryan family, ge-ographically, and syntactically matching on keyfeatures like the subject-object-verb (sov) orderas against the subject-verb-object (svo) orderin english.
even though the scripts of several in-dic languages differ, they are all part of the samebrahmic family, making it easier to design rule-based transliteration libraries across any languagepair.
in contrast, transliteration of indic languagesto english is harder with considerable phoneticvariation in how words are transcribed.
the geo-graphical and phylogenetic proximity has lead tosigniﬁcant overlap of words across languages.
thisimplies that just after transliteration we are ableto exploit overlap with a related prominent lan-guage (rpl) like hindi.
on three indic languageswe discover between 11% and 26% overlappingtokens with hindi, whereas with english it is lessthan 8%, mostly comprising numbers and entitynames.
furthermore, the syntax-level similaritybetween languages allows us to generate high qual-ity data augmentation by exploiting pre-existingbilingual dictionaries.
we generate pseudo paralleldata by converting rpl text to lrl and vice-versa.
these allow us to further align the learned embed-.
1313…ਇਹ :    एक हैਬਾਂਦਰ : बंदर, वानर...…   ਇਹਇਕ ਬਾਂਦਰ ਹੈਬਾਂਦਰਹੈ...…इक है :    एक हैबांदर : बंदर, वानर...…   इहइक हैबांदरहै...transliteration into r’s script…   इहएक हैबंदरहै...…   राम अच्छा बालक हैचँगाम अच्छा बालक हैुँडाहै...pseudo translationइहइक हैहैइहएक हैबंदरहैराम अच्छा बालक हैचँगाम अच्छा बालक हैुँडाहैराम अच्छा बालक हैहैइहहैबालक हैmaskइक हैmlm trainingalignment lossalignment loss…अच्छा : ਚੰਗਾ, ਵਧੀਆबालक है : ਮੁੰਡਾ...pre-training with mlm and alignment…अच्छा : चँगा, वधीआबालक है : म अच्छा बालक हैुँडा...अच्छा…   राम अच्छा बालक हैअच्छाबालक हैहै...बांदरबांदरdings across the two languages using the recentlyproposed loss functions for aligning contextual em-beddings of word translations (cao et al., 2020; wuand dredze, 2020).
in this paper, we make the following contributions:• we address the problem of adding a low web-resource language (lrl) to an existing pre-trained lm, especially when monolingual cor-pora in the lrl is limited.
this is an impor-tant but underexplored problem.
we focuson indian languages which have hundred ofmillions of speakers, but traditionally under-studied in the nlp community..• we propose relatelm which exploits relat-edness among languages to effectively in-corporate a lrl into a pre-trained lm.
wehighlight the relevance of transliteration andpseudo translation for related languages, anduse them effectively in relatelm to adapt apre-trained lm to a new lrl..• through extensive experiments, we ﬁnd thatrelatelm is able to gain signiﬁcant improve-ments on benchmark datasets.
we demon-strate how relatelm adapts mbert to oriyaand assamese, two low web-resource indianlanguages by pivoting through hindi.
via ab-lation studies on bilingual models we showthat relatelm is able to achieve accuracy ofzero-shot transfer with limited data (20k doc-uments) that is not surpassed even with fourtimes as much data in existing methods..the source code for our experiments is available.
at https://github.com/yashkhem1/relatelm..2 related work.
transformer (vaswani et al., 2017) based languagemodels like mbert (devlin et al., 2019), muril(khanuja et al., 2021), indicbert (kakwani et al.,2020), and xlm-r (conneau et al., 2019), trainedon massive multilingual datasets have been shownto scale across a variety of tasks and languages.
the zero-shot cross-lingual transferability offeredby these models makes them promising for low-resource domains.
pires et al.
(2019) ﬁnd thatcross-lingual transfer is even possible across lan-guages of different scripts, but is more effective fortypologically related languages.
however, recentworks (lauscher et al., 2020; pfeiffer et al., 2020b;hu et al., 2020) have identiﬁed poor cross-lingualtransfer to languages with limited data when jointlypre-trained.
a primary reason behind poor transfer.
is the lack of model’s capacity to accommodateall languages simultaneously.
this has led to in-creased interest in adapting multilingual lms tolrls and we discuss these in the following twosettings..lrl adaptation using monolingual data foreleven languages outside mbert, wang et al.
(2020) demonstrate that adding a new target lan-guage to mbert by simply extending the embed-ding layer with new weights results in better per-forming models when compared to bilingual-bertpre-training with english as the second language.
pfeiffer et al.
(2020c) adapt multilingual lms tothe lrls and languages with scripts unseen duringpre-training by learning new tokenizers for the un-seen script and initializing their embedding matrixby leveraging the lexical overlap w.r.t.
the lan-guages seen during pre-training.
adapter (pfeifferet al., 2020a) based frameworks like (pfeiffer et al.,2020b; artetxe et al., 2020; ¨ust¨un et al., 2020) ad-dress the lack of model’s capacity to accommodatemultiple languages and establish the advantages ofadding language-speciﬁc adapter modules in thebert model for accommodating lrls.
thesemethods generally assume access to a fair amountof monolingual lrl data and do not exploit relat-edness across languages explicitly.
these methodsprovide complimentary gains to our method of di-rectly exploiting language relatedness..lrl adaptation by utilizing parallel datawhen a parallel corpus of a high resource languageand its translation into a lrl is available, con-neau and lample (2019) show that pre-trainingon concatenated parallel sentences results inimproved cross-lingual transfer.
methods likecao et al.
(2020); wu and dredze (2020) discussadvantages of explicitly bringing together thecontextual embeddings of aligned words in atranslated pair.
language relatedness has beenexploited in multilingual-nmt systems in variousways (neubig and hu, 2018; goyal and durrett,2019; song et al., 2020).
these methods typicallyinvolve data augmentation for a lrl with helpof a related high resource language (rpl) or toﬁrst learn the nmt model for a rpl followed byﬁnetuning on the lrl.
wang et al.
(2019) proposea soft-decoupled encoding approach for exploitingsubword overlap between lrls and hrls toimprove encoder representations for lrls.
gaoet al.
(2020) address the issue of generating ﬂuent.
1314lrl.
punjabigujaratibengali.
percentage overlap of wordsrelated prominent distant prominent.
(hindi)25.523.310.9.
(english)7.54.55.5.bleu scores.
lrl(target)punjabigujaratibengali.
related prominent distant prominent(english) (source)(hindi) (source)24.616.520.312.919.312.4.table 1: motivation for transliteration: % over-lapping words between transliterated lrl (in promi-nent language’s script) and prominent language text.
% overlap is deﬁned as the number of common dis-tinct words divided by number of distinct words inthe transliterated lrl.
overlap is much higher withhindi, the related prominent language (rpl), com-pared to english, the distant language.
overlappingwords act as anchors during multilingual pre-trainingin relatelm(section 3.1).
table 2: motivation for pseudo translation: bleuscores between pseudo translated prominent languagesentences and lrl sentences.
bleu with hindi, therpl, is much higher than with english, the distantprominent language highlighting the effectiveness ofpseudo translation from a rpl (section 3.2).
englishand hindi dictionary sizes same.
for these experiments,we used a parallel corpus across these 5 languages ob-tained from tdil (section 4.1).
lrl sentences in nmt by extending the soft-decoupled encoding approach to improve decoderrepresentations for lrls.
xia et al.
(2019) utilizedata augmentation techniques for lrl-englishtranslation using rpl-english and rpl-lrlparallel corpora induced via bilingual lexiconsand unsupervised nmt.
goyal et al.
(2020)utilize transliteration and parallel data from relatedindo-aryan languages to improve nmt systems.
similar to our approach they transliterate all theindian languages to the devanagri script.
similarly,song et al.
(2020) utilize chinese-english parallelcorpus and transliteration of chinese to japanesefor improving japanese-english nmt systems viadata augmentation..to the best of our knowledge no earlier work hasexplored the surprising effectiveness of translitera-tion to a related existing prominent language, forlearning multilingual lms, although some workexists in nmt as mentioned above..3 low web-resource adaptation in.
relatelm.
problem statement and notations our goalis to augment an existing multilingual languagemodel m, for example mbert, to learn repre-sentations for a new lrl l for which availablemonolingual corpus dl is limited.
we are also toldthat the language to be added is related to anotherlanguage r on which the model m is already pre-trained, and is of comparatively higher resource.
however, the script of dl may be distinct from thescripts of existing languages in m. in this sectionwe present strategies for using this knowledge to.
better adapt m to l than the existing baseline ofﬁne-tuning m using the standard masked languagemodel (mlm) loss on the limited monolingual datadl (wang et al., 2020).
in addition to the monolin-gual data dr in the rpl and dl in the lrl, wehave access to a limited bilingual lexicon bl→rthat map a word in language l to a list of synonymsin language r and vice-versa br→l..we focus on the case where the rpl, lrl pairsare part of the indo-aryan language families whereseveral levels of relatedness exist.
our proposed ap-proach, consists of three steps, viz., transliterationto rpl’s script, pseudo translation, and adaptationthrough pre-training.
we describe each of thesesteps below.
figure 2 presents an overview of ourapproach..3.1 transliteration.
first, the scripts of indo-aryan languages are partof the same brahmic script.
this makes it easier todesign simple rule-based transliterators to convert acorpus in one script to another.
for most languagestransliterations are easily available.
example, theindic-trans library 2 (bhat et al., 2015).
we usedlr to denote the lrl corpus after transliteratingto the script of the rpl.
we then propose to furtherpre-train the model m with mlm on the translit-erated corpus dlr instead of dl.
such a strategycould provide little additional gains over the base-line, or could even hurt accuracy, if the two lan-guages were not sufﬁciently related.
for languagesin the indo-aryan family because of strong phy-logenetic and geographical overlap, many wordsacross the two languages overlap and preserve the.
2https://github.com/libindic/.
indic-trans.
1315same meaning.
in table 1 we provide statistics ofthe overlap of words across several transliteratedindic languages with hindi and english.
note thatfor hindi the fraction of overlapping words is muchhigher than with english which are mostly num-bers, and entity names.
these overlapping wordsserve as anchors to align the representations forthe non-overlapping words of the lrl that sharesemantic space with words in the rpl..hindi and from english.
we observe much highbleu for translation from hindi highlighting thesyntactic relatedness of the languages..let (dr, br→lr(dr)) denote the parallel cor-pus formed by pseudo translating the rpl corpusvia the transliterated rpl to lrl lexicon.
likewiselet (dlr, blr→r(dlr)) be formed by pseudotranslating the transliterated low web-resource cor-pus via the transliterated lrl to rpl lexicon..3.2 pseudo translation with lexicons.
3.3 alignment loss.
parallel data between a rpl and lrl languagepair has been shown to be greatly useful for ef-ﬁcient adaptation to lrl (conneau and lample,2019; cao et al., 2020).
however, creation of par-allel data requires expensive supervision, and isnot easily available for many low web-resourcelanguages.
back-translation is a standard methodof creating pseudo parallel data but for low web-resource languages we cannot assume the presenceof a well-trained translation system.
we exploitthe relatedness of the indic languages to design apseudo translation system that is motivated by twofactors:.
• first, for most geographically proximal rpl-lrl language pairs, word-level bilingualdictionaries have traditionally been avail-able to enable communication.
when theyare not, crowd-sourcing creation of word-level dictionaries3 requires lower skill andresources than sentence level parallel data.
also, word-level lexicons can be created semi-automatically (zhang et al., 2017) (artetxeet al., 2019) (xu et al., 2018)..• second, indic languages exhibit common syn-tactic properties that control how words arecomposed to form a sentence.
for exam-ple, they usually follow the subject-object-verb (sov) order as against the subject-verb-object (svo) order in english..we therefore create pseudo parallel data betweenr and l via a simple word-by-word translationusing the bilingual lexicon.
in a lexicon a word canbe mapped to multiple words in another language.
we choose a word with probability proportional toits frequency in the monolingual corpus dl.
weexperimented with a few other methods of selectingwords that we discuss in section 4.4. in table 2we present bleu scores obtained by our pseudotranslation model of three indic languages from.
3wiktionary is one such effort.
the union of the two pseudo parallel corpora above,collectively called p, is used for ﬁne-tuning musing an alignment loss similar to the one pro-posed in (cao et al., 2020).
this loss attemptsto bring the multilingual embeddings of differentlanguages closer by aligning the correspondingword embeddings of the source language sentenceand the pseudo translated target language sentence.
let c be a random batch of source and (pseudotranslated) target sentence pairs from p, i.e.
c =((s1, t1), (s2, t2), ..., (sn , tn )), where s and t arethe source and target sentences respectively.
sinceour parallel sentences are obtained via word-leveltranslations, the alignment among words is knownand monotonic.
alignment loss has two terms:.
l = lalign + lreg where lalign is used to bringthe contextual embeddings closer and lreg is theregularization loss which prevents the new embed-dings from deviating far away from the pre-trainedembeddings.
each of these are deﬁned below:.
lalign =.
(cid:88).
#word(s)(cid:88).
(s,t)∈c.
i=1.
lreg =.
(cid:88).
.
.
#tok(s)(cid:88).
(s,t)∈c.
j=1.
||f (s, ls(i))−f (t, lt(i))||22.
||(f (s, j) − f0(s, j)||22.
.
||f (t, j) − f0(t, j)||22.
.
#tok(t)(cid:88).
+.
j=1.
where ls(i) is the position of the last token of i-th word in sentence s and f (s, j) is the learnedcontextual embedding of token at j-th position insentence s, i.e, for lalign we consider only the lasttokens of words in a sentence, while for lreg weconsider all the tokens in the sentence.
f0(s, j)denotes the ﬁxed pre-trained contextual embeddingof the token at j-th position in sentence s. #word(s)and #tok(s) are the number of (whole) words andtokens in sentence s respectively..13164 experiments.
dataset split.
lang.
we carry out the following experiments to evaluaterelatelm’s effectiveness in lrl adaptation:.
• first, in the full multilingual setting, we eval-uate whether relatelm is capable of extend-ing mbert with two unseen low-resourceindic languages: oriya (unseen script) andassamese (seen script).
(section 4.2).
• we then move to the bilingual setting wherewe use relatelm to adapt a model trained ona single rpl to a lrl.
this setting allowed usto cleanly study the impact of different adap-tation strategies and experiment with manyrpl-lrl language pairs.
(section 4.3).
• finally, section 4.4, presents an ablationstudy on dictionary lookup methods, align-ment losses, and corpus size..we evaluate by measuring the efﬁcacy of zero-shot transfer from the rpl on three different tasks:ner, pos and text classiﬁcation..4.1 setup.
lm models we take m-bert as the model mfor our multilingual experiments.
for the bilingualexperiments, we start with two separate monolin-gual language models on each of hindi and englishlanguage to serve as m. for hindi we trainedour own hi-bert model over the 160k mono-lingual hindi wikipedia articles using a vocabsize of 20000 generated using wordpiece tokenizer.
for english we use the pre-trained bert modelwhich is trained on almost two orders of magnitudewikipedia articles and more.
when the lrl isadded in its own script, we use the bert-base-casedmodel and when the lrl is added after translit-eration to english, we use the bert-base-uncasedmodel..lrls, monolingual corpus, lexicon aslrls we consider ﬁve indic languages spanningfour different scripts.
monolingual data was ob-tained from wikipedia as summarized in table 4.we extend m-bert with two unseen low web-resource languages: assamese and oriya.
since itwas challenging to ﬁnd indic languages with task-speciﬁc labeled data but not already in m-bert,we could not evaluate on more than two languages.
for the bilingual model experiments, we adapt eachof hi-bert and english bert with three differ-ent languages: punjabi, gujarati and bengali.
forthese languages we simulated the lrl setting by.
train data rpl.
val data rpl.
test data lrl.
number of sentencesner pos textc.
27.056.020.025.053.05.03.814.010.04.013.01.07.913.40.28.014.00.35.89.71.08.014.0-7.64.00.2.enhienhipagubnasor.
table 3: statistics of task-speciﬁc datasets.
all num-bers are in thousands..lrl #docs.
scripts.
pagubnoras.
20 gurumukhi20gujarati20 as-bangla207 as-bangla.
oriya.
hi-lexiconfw5329231819.bw fw18651843124018181917.en-lexiconbw1510101817.table 4: statistics of resources used for lrls in the ex-periments.
all the numbers are in thousands.
#docsrepresents number of documents for each language.
for each language, hi-lexicon and en-lexicon reportsizes of bilingual hindi and english dictionaries respec-tively in either direction.
fw represents the directionfrom a lrl to hi or en.
hindi uses the devanagriscript with a vocab size of 20k.
for all other languagesthe vocab size is ﬁxed at 10k.
as-bangla refers to thebengali-assamese script..downsampling their wikipedia data to 20k doc-uments.
for experiments where we require en-glish monolingual data for creating pseudo trans-lations, we use a downsampled version of englishwikipedia having the same number of documentsas the hindi wikipedia dump..the addition of a new language to m was doneby adding 10000 tokens of the new language gen-erated by wordpiece tokenization to the existingvocabulary, with random initialization of the newparameters.
for all the experiments, we use li-bindic’s indictrans library (bhat et al., 2015) fortransliteration.
for pseudo translation we use theunion of bilingual lexicons obtained from cfilt4 and wiktionary 5 and their respective sizes foreach language are summarized in table 4.tasks for zero-shot transfer evaluation afteradding a lrl in m, we perform task-speciﬁc ﬁne-.
4https://www.cfilt.iitb.ac.in/5https://hi.wiktionary.org/wiki/.
1317lrl adaptation.
mbertebert (wang et al., 2020)relatelm−pseudotebert (wang et al., 2020)relatelm−pseudotrelatelm.
prominentlanguage-.
en.
hi.
bengali.
punjabi.
gujaratiner pos textc.
ner pos textc.
ner pos textc.
87.841.756.619.458.538.669.028.280.265.182.366.9.
86.348.658.178.677.381.3.
39.814.515.314.839.639.7.
64.233.654.751.476.178.6.
75.932.758.645.677.578.7.
65.837.857.248.179.179.8.
83.450.759.873.269.971.7.
70.831.268.834.056.357.3.table 5: different adaptation strategies evaluated for zero-shot transfer (f1-score) on ner, pos tagging and textclassiﬁcation after ﬁne-tuning with the prominent language (english or hindi).
mbert, which is trained withmuch larger datasets and more languages is not directly comparable, and is presented here just for reference..tuning on the rpl separately for three tasks: ner,pos and text classiﬁcation.
table 3 presents a sum-mary of the training, validation data in rpl and testdata in lrl on which we perform zero-shot evalu-ation.
we obtained the ner data from wikiann(pan et al., 2017) and xtreme (hu et al., 2020)and the pos and text classiﬁcation data from thetechnology development for indian languages(tdil)6. we downsampled the tdil data for eachlanguage to make them class-balanced.
the postagset used was the bis tagset (sardesai et al.,2012).
for the english pos dataset, we had tomap the penn tagset in to the bis tagset.
we haveprovided the mapping that we used in the appendix(b).
methods compared we contrast relatelmwith three other adaptation techniques:(1)ebert (wang et al., 2020) that extends the vo-cabulary and tunes with mlm on dl as-is, (2)relatelm without pseudo translation loss, and (3)m-bert when the language exists in m-bert..training details for pre-training on mlm wechose batch size as 2048, learning rate as 3e-5 andmaximum sequence length as 128. we used wholeword masking for mlm and bertwordpiecetok-enizer for tokenization.
for pre-training hi-bertthe duplication was taken as 5 with training donefor 40k iterations.
for all lrls where monolin-gual data used was 20k documents, the duplicationfactor was kept at 20 and and training was done for24k iterations.
for assamese, where monolingualdata was just 6.5k documents, a duplication factorof 60 was used with the same 24k training itera-tions.
the mlm pre-training was done on googlev3-8 cloud tpus..for alignment loss on pseudo translation wechose learning-rate as 5e-5, batch size as 64 and.
6https://www.tdil-dc.in.
lrl adaptation.
oriyarelatelm−pseudotrelatelmebert (wang et al., 2020)relatelm−pseudotrelatelmassameserelatelm−pseudotrelatelmebert (wang et al., 2020)relatelm−pseudotrelatelm.
prominent ner pos textc.
language.
en.
hi.
en.
hi.
14.216.410.822.724.7.
-----.
72.174.171.774.775.2.
78.277.471.979.479.3.
63.262.753.176.576.7.
74.874.778.679.880.2.table 6: mbert+lrl with different adaptation strate-gies evaluated on ner, pos tagging and text classi-ﬁcation with both english and hindi as the ﬁne-tuninglanguages.
accuracy metric is f1..maximum sequence length as 128. the train-ing was done for 10 epochs also on google v3-8cloud tpus.
for task-speciﬁc ﬁne-tuning we usedlearning-rate 2e-5 and batch size 32, with train-ing duration as 10 epochs for ner, 5 epochs forpos and 2400 iterations for text classiﬁcation.
the models were evaluated on a separate rpl val-idation dataset and the model with the minimumf1-score, accuracy and validation loss was selectedfor ﬁnal evaluation for ner, pos and text classiﬁ-cation respectively.
all the ﬁne-tuning experimentswere done on google colaboratory.
the resultsreported for all the experiments are an average of 3independent runs..4.2 multilingual language models.
we evaluate relatelm’s adaptation strategy onmbert, a state of the art multilingual model withtwo unseen languages: oriya and assamese.
thescript of oriya is unseen whereas the script of as-samese is the same as bengali (already in m-bert).
table 6 compares different adaptation strategies in-.
1318(a) punjabi.
(b) gujarati.
(c) bengali.
figure 3: comparison of f1-score between relatelm-20k, ebert-20k and ebert-80k, where the number aftermethod name indicates pre-training corpus size.
we ﬁnd that relatelm-20k outperforms ebert-20k in 8 out of9 settings, and even outperforms ebert-80k, which is trained over 4x more data, in 7 out of 9 settings..cluding the option of treating each of hindi and en-glish as rpl for transliteration into.
for both lrls,transliterating to hindi as rpl provides gains overebert that keeps the script as-is and englishtransliteration.
we ﬁnd that these gains are muchmore signiﬁcant for oriya than assamese, whichcould be because oriya is a new script.
furtheraugmentation with pseudo translations with hindias rpl, provides signiﬁcant added gains.
we havenot included the ner results for assamese due tothe absence of good quality evaluation dataset..4.3 bilingual language models.
for more extensive experiments and ablation stud-ies we move to bilingual models.
table 5 showsthe results of different methods of adapting m toa lrl with hi-bert and bert as two choicesof m. we obtain much higher gains when thelrl is transliterated to hindi than to english orkeeping the script as-is.
this suggests that translit-eration to a related language succeeds in parametersharing between a rpl and a lrl.
note that theenglish bert model is trained on a much largerenglish corpus than the hi-bert model is trainedon the hindi corpus.
yet, because of the related-ness of the languages we get much higher accuracywhen adding transliterated data to hindi rather thanto english.
next observe that pre-training withalignment loss on pseudo translated sentence pairsimproves upon the results obtained with translitera-tion.
this shows that pseudo translations is a decentalternative when a parallel translation corpora isnot available..overall, we ﬁnd that relatelm provides sub-stantial gains over the baseline.
in many cases re-latelm is even better than mbert which was pre-trained on a lot more monolingual data in that lan-guage.
among the three languages, we obtain low-est gains for bengali since the phonetics of bengali.
loss dict lookuppunjabimse ﬁrstmse maxmse root-weightedmse weightedcstvweightedgujaratimse ﬁrstmse maxmse root-weightedmse weightedweightedcstvbengalimse ﬁrstmse maxmse root-weightedmse weightedweightedcstv.
ner pos text c..62.468.264.966.968.2.
39.239.139.739.740.2.
55.556.256.457.356.6.
80.081.378.981.380.8.
83.382.582.682.384.0.
68.070.369.371.767.6.
77.677.676.978.679.4.
78.680.479.979.881.6.
74.079.776.578.776.5.table 7: usefulness of bilingual dictionaries withmse(mean squared error loss) and cstv(contrastiveloss) evaluated on ner, pos tagging and text classi-ﬁcation in relatelm..varies to some extent from other indo-aryan lan-guages, and bengali shows inﬂuence from tibeto-burman languages too (kunchukuttan and bhat-tacharyya, 2020).
this is also evident in the lowerword overlap and lower bleu in table 1 and ta-ble 2 compared to other indic languages.
we fur-ther ﬁnd that in case of bengali, the ner results arebest when bengali is transliterated to english ratherthan hindi, which we attribute to the presence ofenglish words in the ner evaluation dataset..4.4 ablation study.
methods of dictionary lookups we experi-mented with various methods of choosing the trans-lated word from the lexicon which may have mul-tiple entries for a given word.
in table 7 we com-pare four methods of picking entries: ﬁrst - en-.
1319try at ﬁrst position, max-entry with maximum fre-quency in the monolingual data, weighted - entrywith probability proportional to that frequency androot-weighted - entry with probability proportionalto the square root of that frequency.
we ﬁnd thatthese four methods are very close to each other,with the weighted method having a slight edge..alignment loss we compare the mse-basedloss we used with the recently proposed contrastiveloss (wu and dredze, 2020) for lalign but did notget any signiﬁcant improvements.
we have pro-vided the results for additional experiments in theappendix (a).
increasing monolingual sizein figure 3 we in-crease the monolingual lrl data used for adaptingebert four-fold and compare the results.
we ob-serve that even on increasing monolingual data, inmost cases, by being able to exploit language relat-edness, relatelm outperforms the ebert modelwith four times more data.
these experiments showthat for zero-shot generalization on nlp tasks, itis more important to improve the alignment amonglanguages by exploiting their relatedness, than toadd more monolingual data..5 conclusion and future work.
we address the problem of adapting a pre-trainedlanguage model (lm) to a low web-resource lan-guage (lrl) with limited monolingual corpora.
we propose relatelm, which explores relatednessbetween the lrl and a related prominent lan-guage (rpl) already present in the lm.
relatelmexploits relatedness along two dimensions – scriptrelatedness through transliteration, and sentencestructure relatedness through pseudo translation.
we focus on indic languages, which have hundredsof millions of speakers, but are understudied inthe nlp community.
our experiments provide evi-dence that relatelm is effective in adapting mul-tilingual lms (such as mbert) to various lrls.
also, relatelm is able to achieve zero-shot trans-fer with limited lrl data (20k documents) whichis not surpassed even with 4x more data by exist-ing baselines.
together, our experiments establishthat using a related language as pivot, along withdata augmentation through transliteration and bilin-gual dictionary-based pseudo translation, can bean effective way of adapting an lm for lrls, andthat this is more effective than direct training orpivoting through english..integrating relatelm with other complementarymethods for adapting lms for lrls (pfeiffer et al.,2020b,c) is something we plan to pursue next.
weare hopeful that the idea of utilizing relatedness toadapt lms for lrls will be effective in adaptinglms to lrls in other languages families, such assouth-east asian and latin american languages.
we leave that and exploring other forms of related-ness as fruitful avenues for future work..acknowledgements we thank technology de-velopment for indian languages (tdil) pro-gramme initiated by the ministry of electronicsinformation technology, govt.
of india for provid-ing us datasets used in this study.
the experimentsreported in the paper were made possible by a ten-sor flow research cloud (tfrc) tpu grant.
theiit bombay authors thank google research indiafor supporting this research.
we thank dan gar-rette and slav petrov for providing comments onan earlier draft..references.
mikel artetxe, gorka labaka, and eneko agirre.
2019.bilingual lexicon induction through unsupervisedmachine translation.
in proceedings of the 57th an-nual meeting of the association for computationallinguistics, pages 5002–5007, florence, italy.
asso-ciation for computational linguistics..mikel artetxe, sebastian ruder, and dani yogatama.
2020. on the cross-lingual transferability of mono-lingual representations.
in proceedings of the 58thannual meeting of the association for computa-tional linguistics, pages 4623–4637, online.
asso-ciation for computational linguistics..irshad ahmad bhat, vandan mujadia, aniruddha tam-mewar, riyaz ahmad bhat, and manish shrivastava.
2015. iiit-h system submission for ﬁre2014 sharedtask on transliterated search.
in proceedings of theforum for information retrieval evaluation, fire’14, pages 48–53, new york, ny, usa.
acm..steven cao, nikita kitaev, and dan klein.
2020. mul-tilingual alignment of contextual word representa-tions.
in 8th international conference on learningrepresentations, iclr 2020, addis ababa, ethiopia,april 26-30, 2020. openreview.net..alexis conneau, kartikay khandelwal, naman goyal,vishrav chaudhary, guillaume wenzek, franciscoguzm´an, edouard grave, myle ott, luke zettle-moyer, and veselin stoyanov.
2019. unsupervisedcross-lingual representation learning at scale.
arxivpreprint arxiv:1911.02116..alexis conneau and guillaume lample.
2019. cross-lingual language model pretraining.
in advances in.
1320neural information processing systems, volume 32,pages 7059–7069.
curran associates, inc..jacob devlin, ming-wei chang, kenton lee, andkristina toutanova.
2019. bert: pre-training ofdeep bidirectional transformers for language under-in proceedings of the 2019 conferencestanding.
of the north american chapter of the associationfor computational linguistics: human languagetechnologies, volume 1 (long and short papers),pages 4171–4186, minneapolis, minnesota.
associ-ation for computational linguistics..matthew s. dryer and martin haspelmath, editors.
2013. wals online.
max planck institute for evo-lutionary anthropology, leipzig..luyu gao, xinyi wang, and graham neubig.
2020.improving target-side lexical transfer in multilingualin findings of the as-neural machine translation.
sociation for computational linguistics: emnlp2020, pages 3560–3566, online.
association forcomputational linguistics..tanya goyal and greg durrett.
2019. embedding timeexpressions for deep temporal ordering models.
inproceedings of the 57th annual meeting of the asso-ciation for computational linguistics, pages 4400–4406, florence, italy.
association for computationallinguistics..vikrant goyal, sourav kumar, and dipti misra sharma.
2020. efﬁcient neural machine translation for low-resource languages via exploiting related languages.
in proceedings of the 58th annual meeting of theassociation for computational linguistics: studentresearch workshop, pages 162–168, online.
associ-ation for computational linguistics..junjie hu, sebastian ruder, aditya siddhant, gra-ham neubig, orhan firat, and melvin johnson.
2020. xtreme: a massively multilingual multi-taskbenchmark for evaluating cross-lingual generaliza-tion.
corr, abs/2003.11080..divyanshu kakwani, anoop kunchukuttan, satishgolla, nc gokul, avik bhattacharyya, mitesh minlpsuite:khapra, and pratyush kumar.
2020.monolingual corpora, evaluation benchmarks andpre-trained multilingual language models for indianlanguages.
in proceedings of emnlp 2020..simran khanuja, diksha bansal, sarvesh mehtani,savya khosla, atreyee dey, balaji gopalan,dilip kumar margam, pooja aggarwal, rajiv tejanagipogu, shachi dave, shruti gupta, subhashchandra bose gali, vish subramanian, and parthatalukdar.
2021. muril: multilingual represen-arxiv preprinttations forarxiv:2103.10730..indian languages..anoop kunchukuttan and pushpak bhattacharyya.
2020. utilizing language relatedness to improve ma-chine translation: a case study on languages of theindian subcontinent..zhenzhong lan, mingda chen, sebastian goodman,kevin gimpel, piyush sharma, and radu soricut.
2020. albert: a lite bert for self-supervised learn-ing of language representations..anne lauscher, vinit ravishankar, ivan vuli´c, andgoran glavaˇs.
2020. from zero to hero: on thelimitations of zero-shot language transfer with mul-tilingual transformers.
in proceedings of the 2020conference on empirical methods in natural lan-guage processing (emnlp), pages 4483–4499, on-line.
association for computational linguistics..patrick littell, david r. mortensen, ke lin, kather-ine kairis, carlisle turner, and lori levin.
2017.uriel and lang2vec: representing languages astypological, geographical, and phylogenetic vectors.
in proceedings of the 15th conference of the euro-pean chapter of the association for computationallinguistics: volume 2, short papers..yinhan liu, myle ott, naman goyal, jingfei du, man-dar joshi, danqi chen, omer levy, mike lewis,luke zettlemoyer, and veselin stoyanov.
2019.roberta: a robustly optimized bert pretraining ap-proach..graham neubig and junjie hu.
2018. rapid adapta-tion of neural machine translation to new languages.
in proceedings of the 2018 conference on empiri-cal methods in natural language processing, pages875–880, brussels, belgium.
association for com-putational linguistics..xiaoman pan, boliang zhang, jonathan may, joelnothman, kevin knight, and heng ji.
2017. cross-lingual name tagging and linking for 282 languages.
in proceedings of acl 2017, pages 1946–1958..jonas pfeiffer, andreas r¨uckl´e, clifton poth, aish-ivan vuli´c, sebastian ruder,warya kamath,kyunghyun cho,and iryna gurevych.
2020a.
adapterhub: a framework for adapting transform-ers.
in proceedings of the 2020 conference on em-pirical methods in natural language processing:system demonstrations, pages 46–54, online.
asso-ciation for computational linguistics..jonas pfeiffer, ivan vuli´c, iryna gurevych, and se-bastian ruder.
2020b.
mad-x: an adapter-basedframework for multi-task cross-lingual transfer.
in proceedings of the 2020 conference on empiricalmethods in natural language processing (emnlp),pages 7654–7673, online.
association for computa-tional linguistics..jonas pfeiffer, ivan vulic, iryna gurevych, and sebas-tian ruder.
2020c.
unks everywhere: adapting mul-tilingual language models to new scripts.
corr,abs/2012.15562..telmo pires, eva schlinger, and dan garrette.
2019.in pro-how multilingual is multilingual bert?
ceedings of the 57th annual meeting of the asso-ciation for computational linguistics, pages 4996–5001, florence, italy.
association for computa-tional linguistics..1321madhavi sardesai, jyoti pawar, shantaram walawa-likar, and edna vaz.
2012. bis annotation standardsin proceed-with reference to konkani language.
ings of the 3rd workshop on south and southeastasian natural language processing, pages 145–152, mumbai, india.
the coling 2012 organizingcommittee..meng zhang, yang liu, huanbo luan, and maosongsun.
2017. adversarial training for unsupervisedin proceedings of thebilingual lexicon induction.
55th annual meeting of the association for com-putational linguistics (volume 1: long papers),pages 1959–1970, vancouver, canada.
associationfor computational linguistics..haiyue song, raj dabre, zhuoyuan mao, fei cheng,sadao kurohashi, and eiichiro sumita.
2020. pre-training via leveraging assisting languages for neuralmachine translation.
in proceedings of the 58th an-nual meeting of the association for computationallinguistics: student research workshop, pages 279–285, online.
association for computational linguis-tics..ahmet ¨ust¨un, arianna bisazza, gosse bouma, andgertjan van noord.
2020. udapter: language adap-intation for truly universal dependency parsing.
proceedings of the 2020 conference on empiricalmethods in natural language processing (emnlp),pages 2302–2315, online.
association for computa-tional linguistics..ashish vaswani, noam shazeer, niki parmar, jakobuszkoreit, llion jones, aidan n gomez, lukaszkaiser, and illia polosukhin.
2017. attention is allyou need.
arxiv preprint arxiv:1706.03762..xinyi wang, hieu pham, philip arthur, and grahamneubig.
2019. multilingual neural machine transla-tion with soft decoupled encoding.
arxiv preprintarxiv:1902.03499..zihan wang, karthikeyan k, stephen mayhew, anddan roth.
2020. extending multilingual bert tolow-resource languages.
in findings of the associ-ation for computational linguistics: emnlp 2020,pages 2649–2656, online.
association for computa-tional linguistics..shijie wu and mark dredze.
2020. do explicit align-ments robustly improve multilingual encoders?
inproceedings of the 2020 conference on empiricalmethods in natural language processing (emnlp),pages 4471–4482, online.
association for computa-tional linguistics..mengzhou xia, xiang kong, antonios anastasopou-los, and graham neubig.
2019. generalized datain pro-augmentation for low-resource translation.
ceedings of the 57th annual meeting of the asso-ciation for computational linguistics, pages 5786–5796, florence, italy.
association for computa-tional linguistics..ruochen xu, yiming yang, naoki otani, and yuexinwu.
2018. unsupervised cross-lingual transfer ofword embedding spaces.
in proceedings of the 2018conference on empirical methods in natural lan-guage processing, pages 2465–2474, brussels, bel-gium.
association for computational linguistics..appendix.
a additional experiments with contrastiveloss.
apart from mse loss, we also experimented withthe recently proposed contrastive loss.
we presentthe results of using contrastive loss with variousmethods of dictionary lookups as described in sec-tion 4 of the paper, in table 8.loss dict lookup.
ner pos text c..cstvﬁrstcstv maxcstvcstv weighted.
root-weighted.
ﬁrstcstvcstv maxcstvcstv weighted.
root-weighted.
punjabi73.162.172.168.2gujarati39.938.939.940.2.bengali.
cstvﬁrstcstv maxcstvcstv weighted.
root-weighted.
56.256.958.556.6.
80.779.878.580.8.
83.384.183.184.0.
67.769.271.167.6.
75.573.477.979.4.
80.480.876.081.6.
77.276.970.976.5.table 8: evaluations on ner, pos tagging and textclassiﬁcation in relatelm using contrastive loss withdifferent methods of dictionary lookup.
b pos tagset mapping between penntreebank tagset and bis tagset.
for the pos experiments involving m-bert as thebase model, we ﬁne-tune our trained model withboth english and hindi training data and calculatezero-shot results on the target language.
however,the english dataset that we used was annotatedusing penn treebank tagset while the rest of thelanguages were annotated using bis tagset.
wecame up with a mapping between the penn tagsand the bis tags so that the english pos datasetbecomes consistent with the hindi counterpart.
ta-ble 9 contains the mapping that we used for thesaid conversion.
note that since we are using top-level tags (e.g pronouns) instead of sub-level tags.
1322(e.g personal pronouns, possessive pronouns) forthe pos classiﬁcation, the mapping is also done toreﬂect the same..penn tagsetccexinjjrlsnnnnpposprp$rbrrptovbvbgvbpwpafx.
pdt.
bis tagsetccrdpspjjqtnnpspprrbrprpvvvprrd.
all, half: qtsuch: dm”default”: qtsome, every,both, all,another, a,an: qtthis, these,the: dmthose, that: pr”default”: qt.
penn tagsetcdfwjjjjsmdnnsnnpsprprbrbssymuhvbdvbnvbzwp$-lrb-# .
, $ “ () : - ‘’ ‘.
wdt.
bis tagsetqtrdjjjjvnnprrbrbrdrpvvvprrd.
rd.
which, that : prwhatever: rp”default”: pr.
how,wherever,when, where: prwhenever: rbwhy: rb”default” : pr.
dt.
wrb.
-rrb-.
rd.
table 9: tagset mapping between penn treebank andbis.
for some tags in penn treebank (e.g.
dt), wedecided that a one-to-many mapping was appropriatebased on a word-level division.
1323