common sense beyond english: evaluating and improvingmultilingual language models for commonsense reasoning.
bill yuchen lin.
seyeon lee xiaoyang qiao xiang ren.
{yuchen.lin, seyeonle, xiaoyanq, xiangren}@usc.edudepartment of computer science and information sciences institute,university of southern california.
abstract.
commonsense reasoning research has so farbeen limited to english.
we aim to evalu-ate and improve popular multilingual languagemodels (ml-lms) to help advance common-sense reasoning (csr) beyond english.
wecollect the mickey corpus, consisting of 561ksentences in 11 different languages, whichcan be used for analyzing and improving ml-lms.
we propose mickey probe, a language-agnostic probing task for fairly evaluating thecommon sense of popular ml-lms across dif-ferent languages.
in addition, we also createtwo new datasets, x-csqa and x-codah,by translating their english versions to 15other languages, so that we can evaluate pop-ular ml-lms for cross-lingual commonsensereasoning.
to improve the performance be-yond english, we propose a simple yet effec-tive method — multilingual contrastive pre-training (mcp).
it signiﬁcantly enhances sen-tence representations, yielding a large perfor-mance gain on both benchmarks (e.g., +2.7%accuracy for x-csqa over xlm-rl)1..1 introduction.
understanding natural language relies heavily oncommonsense reasoning (csr), which is the pro-cess of making inferences with commonsenseknowledge.
commonsense knowledge is the set ofgeneral facts that reﬂect our natural understandingof the physical world and human behavior, whichare usually seen as an implicit background whenpeople communicate with each other using lan-it is thus of vital importance to evalu-guages.
ate and improve the commonsense reasoning ca-pability of language models (lms), towards build-ing general natural language understanding (nlu)systems (davis and marcus, 2015)..1we release our code and data at the project website:.
https://inklab.usc.edu/xcsr/..figure 1: commonsense reasoning is well-studied withbenchmarks and lms in english.
can we advancecommonsense reasoning beyond english?.
many recent benchmark datasets and probingmethods have been proposed to evaluate ma-chine common sense.
as shown in figure 1,the lama probe (petroni et al., 2019) is for an-alyzing lms’ zero-shot commonsense recallingability; commonsenseqa (csqa) (talmor et al.,2019) is instead a multiple-choice qa task thatneeds ﬁne-tuning; codah (chen et al., 2019)and swag (zellers et al., 2018) focus on the abil-ity to complete the most plausible scenes.
how-ever, all these works have been limited only toenglish.
consequently, follow-up analysis andreasoning methods developed (lin et al., 2019;feng et al., 2020; lin et al., 2020) also focus onlyon english lms like bert (devlin et al., 2019).
such english-centric trend of commonsense rea-soning studies not only limits our research scope,but also tends to exacerbate english-speciﬁc biasthat might prevent future methods from generaliz-ing beyond english (ponti et al., 2020)..proceedingsofthe59thannualmeetingoftheassociationforcomputationallinguisticsandthe11thinternationaljointconferenceonnaturallanguageprocessing,pages1274–1287august1–6,2021.©2021associationforcomputationallinguistics1274birds have [mask] .lama probewhere do adults usually use glue sticks?a) school b) drawer c) officecommonsenseqaswag/codahthe chef drops the piece of shrimp in the fryer.→a) the chef chops the pan.b) the chef watches it sizzle.c) the chef likes fried chicken.english lmmultilingual lmenarzhfrhiitdeesjaplnlpturviruswwingcbit is of pressing urgency for the community todevelop nlu systems that can serve all languagesin the world to bridge the gap between differentcultures and eliminate language barriers (hu et al.,2020), and multilingual language models (ml-lms), such as xlm-r (conneau et al., 2020),are among the most promising tools to achievethis ambitious goal.
although ml-lms have beenevaluated in a few nlu tasks, e.g., xnli (con-neau et al., 2018) and xtemre (hu et al., 2020),it is still relatively unclear how ml-lms per-form in commonsense reasoning tasks, due to thelack of 1) dedicated methods for probing commonsense in ml-lms and 2) multilingual benchmarkdatasets for commonsense reasoning..to analyze how much common sense ml-lms already have without any tuning, we pro-pose mickeyprobe, a zero-shot probing task.
ittasks a ml-lm to rank a set of contrastive as-sertions (i.e., declarative sentences) in the samelanguage by their commonsense plausibility, forwhich we use pseudo-likelihood (pll)(salazaret al., 2020) as a proxy.
unlike the lama probe,it can study multi-token concepts which are ubiq-uitous in some non-english languages.
in addi-tion, it fairly compares performance across differ-ent languages via a language-invariant evaluationprotocol.
alongside the probing task, we also cre-ate mickeycorpus, a large-scale multilingualdataset, consisting of 561k sentences in 11 differ-ent languages.
our experiments reveal that thereare always large discrepancies across different lan-guages in the tested ml-lms, and different ml-lms show very different language preferences..beyond supervision-free analysis of ml-lms,we also study their performance in commonsensereasoning tasks, such as csqa and codah,within a cross-lingual transfer setting (i.e., trainedon english data and tested on other languages).
we ﬁnd that existing ml-lms tend to have muchlower accuracy in commonsense reasoning beyondenglish.
we conjecture a major common weak-ness of existing ml-lms is that their pretrain-ing stages do not have a proper sentence-level ob-jective.
therefore, we propose multilingual con-trastive pre-training (mcp), which tasks a ml-lm to select the correct assertion out of a setof n contrastive assertions in n different lan-guages.
we re-format mickeycorpus by sam-pling across languages and thus form a dedicatedpre-training corpus for the mcp task.
to fairly.
evaluate different ml-lms and validate the ef-fectiveness of mcp, we create x-csqa and x-codah, two cross-lingual commonsense reason-ing datasets by translating their english versions to15 other languages2, including low-resource onessuch as swahili (sw) and urdu (ur).
experimentsshow that the proposed mcp objective indeed sig-niﬁcantly improves the performance of state-of-the-art ml-lms in cross-lingual commonsensereasoning.
our contributions are as follows:.
• resources.
we collect a large multilin-gual parallel corpus, mickeycorpus, con-sisting of 561k sentences in 11 languages,which can be used for analyzing and improv-ing ml-lms.
we also create x-csqa andx-codah,two cross-lingual csr bench-marks in 16 languages, for question answer-ing and scene completion, respectively..• evaluation and analysis.
we analyze mul-tiple popular ml-lms with mickeyprobe,a language-invariant, zero-shot task for prob-ing common sense in ml-lms; we also eval-uate them on x-csqa and x-codah in across-lingual transfer setting..• method to improve ml-lms.
we proposemultilingual contrastive pretraining, a sim-ple and effective sentence-level pretext taskfor enhancing ml-lms in cross-lingual com-monsense reasoning, which signiﬁcantly im-proves the state-of-the-art ml-lms in cross-lingual commonsense reasoning..2 background and related work.
in this section, we introduce important concepts,background knowledge, and related work beforewe present our work in following sections..2.1 multilingual language models.
a multilingual language model (ml-lm) aimsto produce text representations for multiple lan-guages in a uniﬁed embedding space.
one ofthe unique advantages of ml-lms is their po-tential ability to perform zero-shot cross-lingualtransfer — a model trained (or ﬁne-tuned) ondata in one language (usually english) can be di-rectly used in other languages as well without fur-ther ﬁne-tuning.
improving ml-lms is thus be-lieved as one of the most promising approach to-wards multilingual nlu at scale.
mbert (devlin.
2the 16 languages for x-csqa and x-codah: {en, zh,.
de, es, fr, it, jap, nl, pl, pt, ru, ar, vi, hi, sw, ur}..1275et al., 2019) is simply the bert model (devlinet al., 2019) trained on multilingual corpora with-out speciﬁc designs about multilinguality.
thedistil-mbert (d-mbert) (sanh et al., 2019) isa smaller mbert trained by knowledge distil-lation.
conneau and lample (2019) proposedxlm(-100), which is pretrained with both maskedlanguage modeling (mlm) and translation lan-(2020)guage modeling (tlm).
conneau et al.
further proposed xlm-r, which improves thexlm with a better sub-token vocabulary and high-quality multilingual corpora (cc100).
we leavethe analysis of recent seq2seq ml-lms, such asmbart (liu et al., 2020) and mt5 (xue et al.,2021), as future work, because their architecturesare signiﬁcantly different from the other ml-lms.
note that the above ml-lms are pretrainedonly with token-level training objectives such asmlm (i.e., recovering masked tokens in monolin-gual text) and tlm (i.e., recovering masked to-kens in a pair of parallel sentences in two differ-ent languages).
however, most nlu tasks, in-cluding commonsense reasoning, highly rely onsentence-level representations.
we argue that awell-designed sentence-level pre-training objec-tive should improve ml-lms for nlu tasks.
thisintuition motivates us to propose a sentence-levelpre-training objective — mcp (section 5)..2.2 cross-lingual language understanding.
there are a few recent multilingual benchmarksfor nlu tasks, e.g., xtreme(hu et al., 2020),tydi qa(clark et al., 2020), and xglue(lianget al., 2020).
xtreme and xglue are uniﬁedlarge-scale multilingual multitask benchmarks,while ty-di qa focuses on the qa.
these existingcross-lingual benchmarks have not covered com-monsense reasoning tasks, such as csqa (talmoret al., 2019), swag (zellers et al., 2018), and co-dah (chen et al., 2019)..csqa is a question answering task and theother two are scene completion tasks, while allhave a multiple-choice selection objective, asshown in figure 1. these benchmarks are widelyused to evaluate lms for commonsense reasoning.
unfortunately, they are limited to english, not ap-plicable to evaluate models of multilingual com-monsense knowledge, which motivates us to cre-ate x-csqa and x-codah.
the goal of the re-cent xcopa (ponti et al., 2020) dataset shares asimilar goal, but it only focused on event-based.
causal reasoning in the scope of humans’ socialbehavior, which is thus arguably more culturallybiased.
in contrast, the x-csqa and x-codahare mainly for evaluating general world knowl-edge and cover more ﬁne-grained types of reason-ing (e.g., quantitative, negation), and thus engagea more language-agnostic, comprehensive under-standing of ml-lms about common sense..2.3 the lama probe and its limitations.
the lama probe (petroni et al., 2019) is theseminal work on probing for common sense in(english) language models.
it has a straightfor-ward intuition:if a pretrained language modelthencontains more commonsense knowledge,it should be better at recalling a masked to-ken in a commonsense assertion (e.g.,“birds have[mask]”).
speciﬁcally, given a lama-probe sen-tence s and its masked token wt, a lm undertesting uses all past and future tokens — st :=1, wt+1, .
.
.
, ww1, .
.
.
, wtas the input torank all tokens in the vocabulary with the prob- ability pvia zero-shot inference.
onecan evaluate the performance of recalling commonsense by measuring the position of a correct to-ken “wing” in the ranked list.
that is, the lamaprobe method uses token-level probability as aproxy to probe for common sense in lms via rank-ing all tokens in their vocabularies..wt |.
s. .
 .
 .
 .
s.\.
\.
..t.|.
|.
this intuitive method, however, has several in-herent limitations.
first, in many other languages,multi-token concepts are ubiquitous, for exam-ple, “˛fü” (“library” in simpliﬁed chinese).
jiang et al.
(2020) present several methods to de-code multi-token entities so that they can adapt thelama probe to probe a lm for language-speciﬁcit is however infeasible to use token-analysis.
level probing tasks if we want to analyze ml-lmsacross languages.
in addition, the evaluation met-ric of the lama probe could be unfair, becausethere can be many correct words for a maskedposition (e.g., “birds have legs/eyes”).
the rank-ing metrics of the lama probe, however, tend toignore these facts, resulting in a less trustworthyanalysis.
the vocabulary-speciﬁc ranking is un-fair when comparing across different languages,so they can have very different label space.
theselimitations of the lama probe prevent us fromanalyzing common sense in ml-lm across topo-logically diverse languages..12763 the mickey probe.
the challenges of using the lama probe forprobing common sense in ml-lms motivate us topropose a more suitable method for analyzing ml-lms, one that can fairly compare across a diverseset of languages.
we present mickeyprobe,a multilingualtask for probing commonsenseknowledge and analysis.
we design a language-agnostic probing task with a sentence-selectionobjective for analyzing common sense of a ml-lm: given a set of assertions (i.e., declarative sen-tences) that have similar words and syntactic fea-tures, select the one with highest commonsenseplausibility.
we present the task formulation inthis section and then introduce how we collect thededicated dataset in section 4..{.
l.=.
 .
notations.
we deﬁne a mickey probe m as a setof k assertions in the same language, where oneand only one of them (say, mi) is the truth asser-tion with better commonsense plausibility than theother k1 ones.
each mickey probe m has mul-tiple semantically equivalent versions in differentlanguages.
let us denote a language by l2len, f r, ru, zh, .
.
.
whereis thenumber of languages of interest.
then, m l is theprobe m in the language l. for example, m en andm fr denote the probes with the same meaning butin english (en) and french (fr) respectively.
weto denote a multilingual parallel dataset forusek as-mickeyprobe, which consists of tsertions.
t is the number of mickeyprobe itemsand each item has k assertions andlanguage.
finally, we can formally describe a multilingualparallel dataset.
for mickeyprobe:.
⇥|l|⇥.
and.
|l|.
|l|.
m.}.
m.8.
2m.
,.
m.(lx, ly)82li ./ mlym lx.
i.
2,.
..i.
8.
2.k,.
n..
(1).
we use the notation ./ to indicate two assertionsin different languages (e.g., lx and ly) are semanti-cally equivalent to each other.
we leave the detailsof creating such an.
in section 4..m.commonsense probing task.
given a mickyprobe m in the dataset, and suppose the indexmof the truth assertion to be t, a perfect multilinguallanguage model would produce sentence probabil-ities such that it always gives the truth assertionm lt the highest probability among other candidatesfor every language..l.,.
i.
8.
2l.
8.
2.n..
k, p (m li ).
p (m l.t )..(2).
.
figure 2: a mickey probe example m has a set ofprobes in different languages (e.g., m en/zh), and eachof them is a set of 5 assertions.
we rank assertionsin the same language by their plls to probe commonsense in ml-lms across different languages..it is still an open problem to properly com-pute sentence probabilities from masked lan-guage models, the recently proposed pseudo-log-likelihood scoring (plls) (salazar et al., 2020)has shown promising results in many downstreamnlp applications that need sentence re-ranking(e.g., speech recognition, and translation), sug-gesting it is a promising proxy of sentence prob-ability.
given a sentence s, its pll is deﬁned as:.
log p (s) = pll(s) :=.
log p.s.|.
|.
xi=1.
wi |.
s.i.
\.
(3).
 .
 .
that is, we individually mask each token wi at atime and use the remaining context si to get theprobability of a word wi in the sentence s. finally,we aggregate them to approximate p (s)..\.
m.is deﬁned as.
evaluation metric.
theric for mickeyprobeparallel datasetlracy of.
evaluation met-over a multilingualin a speciﬁc languagethe overall hit@k accu-the selection results hit@ k (l) =truth-rank(m l)wheretruth-rank(m l) means the the position of theptruth assertion m lin m l sorted by their prob-tabilities deﬁned in eq.
(3).
the hit@1 is justequivalent to the conventional accuracy..|m|.
2m.
.
1.m.k./.
{.
}.
advantages of mickeyprobe.
there are twokey advantages of the mickeyprobe for evalu-ating ml-lms: (1) the sentence-level probabil-ity can be more generally applied in languages be-sides english, comparing with the lama probewhich only studies single-token english words..1277ranking by plls mickeyprobeml-lmthe effectof readingthe newsis lyingabout the world.… of interviewingthe deceasedis learningabout the world.… oftrackingthe dragonis learningabout the world.
… ofreadingthe newsis learningabout the world.… of readingthe newsis sayingabout the world.阅读新闻的效果是对世界撒谎。采访死者的效果是了解世界。追踪龙的效果是了解世界。阅读新闻的效果是了解世界。阅读新闻的效果是描述世界。en: [4,3,1,5,2]….zh: [2,4,3,1,5]𝑀4𝑒𝑛𝑀4𝑧ℎmodels \.
lbt-cosinecc-size (gb).
en.
de.
it.
es.
fr.
nl.
ru.
bg.
vi.
zh.
hi.
avg.
1.0300.8.
0.93766.6.
0.93630.2.
0.93553.3.
0.93456.8.
0.93329.3.
0.901278.0.
0.90157.5.
0.882137.3.
0.87946.9.
0.86920.2.
0.91997.9.shortest.
23.17.
27.21.
29.93.
31.00.
35.84.
31.68.
18.55.
22.01.
15.46.
25.07.
20.66.
25.51.d-mbertmbertxlm-100xlm-rbxlm-rl.
62.9563.5660.5789.6990.03.
34.5635.5836.3358.9461.98.
25.2629.1326.4953.4553.42.
34.8544.7043.3960.8863.68.
50.4642.5832.5349.1259.47.
32.3935.1536.2459.9963.12.
21.4928.3032.9045.7450.03.
29.1436.0339.7145.2647.01.
19.7724.0425.7941.6545.30.
32.5728.1533.0151.0255.93.
25.8827.8531.4940.7343.98.
33.5735.9236.2254.2257.63.table 1: the hit@1 accuracy (%) of the ﬁve ml-lms for the mickeyprobe task..(2) the task formulation creates a relativelyclosed-ended setting, such that we can use alanguage-independent evaluation metric to fairlycompare across various languages within a ml-lm and compare across various ml-lms fora particular language.
in addition, we can seelama probe as a monolingual, word-level ver-thesion of the more general mickeyprobe:m en=enlama probe is when{{mis a huge number of k assertions (i.e.,the vocabulary size) — a ﬁxed [mask] is re-placed by all tokens in the vocabulary.., and}.
2m.
=.
l.}.
4 the mickey corpus and evaluation.
we present a procedure for automatically creat-ing a multilingual parallel datasetfor the prob-ing task mickeyprobe.
our collected corpus,named mickeycorpus , has 561k sentences in11 languages (t =10.2k, k=5,.
=11)..m.|l|.
4.1 creating english probes.
for the correct commonsense assertions in en-glish, we have an existing resource, the omcscorpus (singh et al., 2002) which contains human-written sentences in english that describe com-monsense facts.
each assertion can be used as am enand we perform perturbations on it to cre-tate the other k1 distractor assertions (i.e., falsecandidates), yielding an m en example.. .
inspired by bert-attack method (li et al.,2020), we use a simple method to generate falseassertions that are semantically related and syn-tactically similar to the truth assertions.
given acorrect assertion, we ﬁrst randomly sample a few(13) words with a part-of-speech tag as noun,verb, or adjective, and replace them with [mask].
then, we use a beam-search style method to de-code the [mask] tokens one by one from left toright.
to ensure that the distractors are less plau-.
⇠.
figure 3: the mickeyprobe results in hit@1-acc.
alarger version of this ﬁgure is in appendix (fig.
6)..sible, we limit the decoding steps to only sam-300th.
weple tokens that ranks between 200threpeat the above procedure multiple times withdifferent sets of [mask] tokens.
then, we usestanza (qi et al., 2020) to remove distractors thathave sequences of pos tags or morphological fea-tures different from the truth assertions.
finally,we sample k.1 of them as the distractors..⇠.
 .
4.2 scaling to ten other languages..we use bidirectional translation with the mar-ianmt models (junczys-dowmunt et al., 2018)pretrained on the opus corpora (tiedemann,2016).
we translate all english probes to the 25languages that has models in both directions andthen translate them back to english.
as the outputsfrom these models might contain noise and errors,we compute the semantic similarities (i.e., cosinesimilarity) between the original m en and the back-translated m x-en via the sentencebert (reimersand gurevych, 2019) model..to ensure the quality and fair comparisons, weset a similarity threshold as 0.75 and keep theintersections of probes in all languages.
con-sidering some languages tend to have transla-tions of lower quality, we ﬁnally choose thebest 10 languages to build the mickey probedataset for our analysis, yielding 10k exam-561kples in each language and 10.2k*5*11=sentences in total..the language set.
⇡.
l.1278{.
.
}.
en, de, f r, ru, es, hi, vi, bg, zh, nl, itnote that our purpose of checking the back-translation quality here is mainly to only keepthe high-quality translations for all language pairsthat we considered.
conventional metrics, e.g.,blue score (papineni et al., 2002), which focuson the exact word match, are thus less suitable:given the original sentence “i have a book”, thetranslation results “i have a novel” and “i have atool” will be seen as equally wrong.
inspired bybertscore (zhang et al., 2020), the bt-cosine isbased on sentencebert, which efﬁciently gives ahigher score for the former and a lower score forthe latter, due to the semantic relatedness between“novel” and “book.” we observed that most of ourback-translations are in similar situations, and thusdecide to use bt-cosine instead of others..4.3 analyzing ml-lms with mickey probes.
we now use the mickeycorpus to evaluate the5 pre-trained ml-lms introduced in section 2.1:d-mbert (sanh et al., 2019), mbert (devlinet al., 2019), xlm (conneau and lample, 2019),xlm-rbase, and xlm-rlarge (conneau et al.,2020).
all these ml-lms pretraining objectivescontain masked-word-prediction tasks, so we caneasily use ppls (eq.
3) to probe them a zero-shot, supervision-free manner with hit@1 accu-racy.
(the hit@2 results are shown in appendix.)
we present a histogram in figure 3 and show theconcrete results in table 1. we ﬁnd that thereare always large discrepancies across different lan-guages in all tested ml-lms, which motivates usto analyze the following questions..q1: do different ml-lms have similar lan-guage preferences?
no.
we arrange the lan-guages in all ml-lms with the same order forfigure 3 — the monotonically descending orderof xlm-rl.
interestingly, we ﬁnd that differentml-lms are good for different languages, result-ing in a very diverse set of trends.
for example,xlm-rb, has a higher performance in it than zhl which are pre-trainedand fr, unlike xlm-r on the same corpora with the same objectives.
mbert and d-mbert has stronger performancein fr than nl and de, unlike xlm and xlm-r..q2: does length inﬂuence pll ranking?
notmuch.
the pll computation indeed tends to pre-fer shorter sequences (see eq.
3), so one may won-der if the length of assertions would inﬂuence theprobing results.
the “shortest” row in table 1.presents the results when we always select theshortest assertion within a probe, instead of pllranking.
the gaps between these scores and xlm-r-l’s suggest that the probing task indeed usespll as a valid proxy for evaluating common sensebased on sentence-level semantics..q3: is the translation quality a key factor?
weshow “bt-cosine”, the mean of the cosine scoresbetween the original english sentences and theback-translated ones, and sort the table by thesenumbers.
the ﬁrst 5 languages, {de, it, es, fr, nl}have the largest bt-cosine, i.e., the best transla-tion quality, and they indeed have better perfor-mances in general for xlm-r models.
however,although zh has a worse bt-score than vi, all ml-lms perform better in zh than vi.
thus, we be-lieve the translation quality of mickeycorpuswill not be a factor to inﬂuence our understandingof ml-lms.
consequently, this suggests that fur-ther study must depend on pre-training corpora ofeach ml-lm in different languages..q4: does the size of pre-training corpora mat-ter?
we list the size of the monolingual corpusin each language for cc-100 that xlm-r are pre-trained on (i.e., the cc-size row).
although ru hasa much larger corpus than de, it, etc., the xlm-r performance in ru is much worse.
in addition,fr and nl have almost the same translation qualitywhile fr’s cc-size is twice the size of nl, but theperformance in fr is still much worse than nl.
weconjecture this would be either due to the design ofsub-token vocabulary or the text quality (instead ofthe size) of the cc-100 corpora..further implications.
the benchmark resultsof ﬁve popular ml-lms on the mickeyprobetask over the mickeycorpus offer the initialand valuable understanding with a closer lookat the commonsense knowledge of ml-lms byprobing them in a uniﬁed evaluation protocol.
onecan either compare a ml-lm across different lan-guages or compare a certain language across ml-lms in table 1. these comparable results sup-port further analysis that can beneﬁt the develop-ment of ml-lms in the future.
after all, eventhe best ml-lm xlm-rl also degrades much inother languages, and also perform slightly worsethan robertal in en (93.4%).
we argue (culture-invariant) common sense knowledge should beseen as an important way to connect multiple lan-guages and thus better align them in a shared em-bedding space induced by a ml-lm..12795 multilingual contrastive pre-training.
in this section, we reformat the mickeyprobeso that we can reuse the mickeycorpus forimproving the pre-trained ml-lms for common-sense reasoning beyond english.
we propose amultilingual contrastive pre-training (mcp) taskthat focuses on enhancing the sentence-level rep-resentation of ml-lms.
mcp improves a ml-lm in a multilingual, contrastive environment,where the model learns to select the assertion withthe best commonsense plausibility from a set ofcontrastive sentences in different languages.
eachmcp example is a set of multilingual assertionswhile each mickey probe is a monolingual set..m..
we createmcp dataset creation frompretraining examples for the mcp task by con-verting mickeyprobe examples, as shown in thesteps illustrated in algorithm 1. simply put, wereformat a k-way mickey probe m (kas-sertions) to a mcp example by sampling a set ofv candidate assertions in v different languages.
we convert all examples in the mickeycorpusto build a new cross-lingual sentence-selection.
⇥|l|.
mdataset.
c.for learning the mcp task..mcp learning.
given a mcp example c,we append one dense linear layer f on top of aml-lm with parameters denoted as ⇥ml-lm forlearning to predict the commonsense plausibilityscore of each assertion ci 2.c as follows:.
2c.
hi = ml-lm(ci).
[cls]oi = f (hi;⇥ f ).
zi =.
eoic.v =|j=1.
eoj.
|.
⇢ =.
1i log (zi).
vp.xi=1.
 .
(4).
(5).
(6).
(7).
we ﬁrst get the logit oi of each assertion by pro-jecting its [cls] embeddings hi to a logit oi viaa dense layer f with parameters ⇥f ; then, weuse softmax to normalize the logits as plausibilityscores zi; finally, we compute the cross-entropyloss ⇢ where 1i=1 if ci is a correct assertion and0 otherwise.
we ﬁne-tune⇥ml-lm, ⇥f }to mini-.
mize the overall loss over the mcp dataset.
c6 evaluation for cross-lingual csr.
{.
in this section, we introduce the datasets, experi-mental setup, results, and our analysis..|l|.
2m.
.
m lxt.is always the truth.
*/.
algorithm 1: convert a mickey probe mto an example for the mcp task.
in: msub-sets;/* is a probe that haseach sub-set m lx is a set of k assertions in thesame language lx2lout: c /* a set of v assertions in differentlanguages.
*/remarks:  n(x) is a function to randomlysample n unique elements from a set x.
1 la   2 c. 1(lm lat }   {3 foreach li 2l /* sample an index of distractor assertion.
*/j)t 1(n}/* add a distractor assertion as a candidate.
*/c.add(m lij ).
/* iterate each sampled distractor language li.
*/1(.)
/* pick an anchor language.
*/.
/* initiate w/ the truth assertion.
*/.
k  {.
la) do.
 v.
 .
 .
4.
5.
6.1 x-csqa & x-codah: two newbenchmarks for evaluating xcsr.
to evaluate ml-lms for commonsense reason-ing in a cross-lingual zero-shot transfer setting, wecreate two benchmark datasets, namely x-csqaand x-codah.
table 3 shows the statistics of thetwo datasets.
speciﬁcally, we use online commer-cial services such as deepl pro translate to col-lect high-quality translations of the examples incsqa and codah for 15 languages other thanenglish.
the size of codah is small (only 2.7k),so we use 7k swag validation examples as addi-tional training data which share the same formu-lation.
we discuss the reduction of cultural dif-ferences and quality control of automatic transla-tions as well as other details in ethical consider-ations (the paragraph for cultural bias reduction)and appendix (a).
as our goal is to evaluate differ-ent ml-lms (instead of different languages) in auniﬁed evaluation protocol for cross-lingual com-monsense reasoning, we argue that such automati-cally translated examples, although might containnoise, can serve as a starting benchmark for us toobtain meaningful analysis before more human-translated datasets will be available in the future..6.2 setup.
we focus on 4 popular ml-lms that we intro-duced in section 2.1: mbert, xlm-100, xlm-rb and xlm-rl as well as our proposed mcpmethod.
for both tasks, we concatenate eachprompt (the question or ﬁrst sentence) and each.
1280en.
de.
it.
es.
fr.
nl.
ru.
vi.
zh.
hi.
pl.
ar.
ja.
pt.
cc-size (gb).
300.8.
66.6.
30.2.
53.3.
56.8.
29.3.
278.0.
137.3.
46.9.
20.2.
44.6.
28.0.
69.3.
49.1.sw.1.6.ur.
5.7.avg.
76.10.x-codah [task: scene completion; random guess: 25.0; robertal for en: 81.6 ].
mbertxlm-100xlm-r-bxlm-r-l.mcp(xlm-rb)mcp(xlm-rl).
 (xlm-rl).
mbertxlm-100xlm-rbxlm-rl.
mcp(xlm-rb)mcp(xlm-rl).
 (xlm-rl).
+3.5.
+1.1 +2.0.
-0.2.
+1.3 +1.4.
+2.3.
+4.9.
+4.6.
+4.6.
+1.5 +2.9 +0.9 +2.6 +2.4 +1.4.
+2.3.
42.942.750.166.4.
52.269.9.
38.834.351.566.7.
52.169.5.
33.131.545.859.6.
47.660.7.
29.626.744.156.1.
46.259.3.
33.532.244.459.9.
46.261.9.
36.428.542.158.2.
45.660.3.
33.830.744.260.9.
44.460.7.
35.329.344.859.5.
44.361.4.
35.234.945.260.1.
48.161.4.
33.828.344.060.3.
44.760.0.
33.732.642.059.3.
44.860.7.
32.627.243.356.8.
45.361.1.
31.930.944.156.3.
42.958.6.
32.729.939.552.1.
42.857.5.
22.824.743.257.4.
43.262.3.
22.221.142.651.4.
45.355.7.
38.031.444.657.3.
45.761.9.
37.828.640.652.7.
44.356.7.
26.526.838.149.1.
37.853.7.
21.122.134.648.7.
36.851.3.
31.027.041.957.5.
41.859.0.
27.226.640.253.9.
41.456.1.
34.830.037.851.2.
41.854.1.
27.726.338.448.4.
36.852.3.
34.027.442.053.8.
42.954.7.
31.425.137.550.0.
37.550.2.
37.233.244.158.2.
44.760.8.
34.130.943.459.9.
44.960.7.
30.825.335.642.2.
37.244.6.
21.820.129.641.6.
28.143.3.
31.524.934.646.6.
36.448.0.
23.721.733.045.2.
33.448.8.x-csqa [task: question answering; random guess: 20.0; robertal for en: 70.4 ].
33.230.442.456.0.
43.658.3.
30.426.740.653.8.
41.956.5.
+2.8.
+3.3 +2.2 +1.9.
-0.4.
+4.3.
+5.4.
+4.3.
+4.0.
+2.6.
+2.1 +3.9 +0.2 +0.8 +1.7 +3.6.
+2.7.
table 2: benchmark results for different ml-lms and mcp-enhanced models for x-csqa and x-codah in azero-shot cross-lingual setting.
  is the improvement of mcp.
{pl,ar,ja,pt,sw,ur} are unseen in mcp..stat..dataset.
#.
!.
task format# languages# options per example.
# training (en)# dev per lang.
# test per lang..x-csqa x-codah.
qa155.
8,8881,0001,074.scenecomp.
154.
8,4763001,000.
# total instances.
80,550.
60,000.table 3: statistics of the two x-csr datasets..of its options individually in the form of “[cls]prompt [sep] optioni [sep]”.
then, we ﬁne-tuneml-lms over the english training dataset and testthem on other languages..why zero-shot cross-lingual transfer?
it is al-most impossible to collect data in all languagesthat an nlu system might be used for.
there-fore, prior works mainly focus on zero-shot cross-lingual transfer (conneau et al., 2018), which ismore meaningful and can offer lower-bound per-formance analysis.
it is also an ideal settingfor studying csr because most commonsensefacts are language-invariant.
thus, an english-ﬁnetuned ml-lm for csr should be able to trans-fer its ability to a wide range of other languages aswell.
furthermore, our goal of this paper is to eval-uate and improve ml-lms, so translating back toenglish and then use an english-only lm is alsonot helpful towards to this end..figure 4: categorized accuracy in for mcp(xlm-rl)on x-codah.
each box is for 15 languages..6.3 experiments for cross-lingual csr.
in table 2, we present the empirical results overx-codah and x-csqa for the ml-lms as wellas two models enhanced by our proposed mcpmethod.
on both tasks, the xlm-rl performs thebest with a large margin.
enhanced by the mcpmethod, both xlm-rb and xlm-rl see signif-icant improvement (e.g., 2.7% absolute improve-ment for xlm-rl on x-csqa-avg)..can mcp’s improvement generalize to un-seen, low-resource languages?
note that mcpdataset only involves 9 languages here, and thereare 6 languages that are totally unseen in the mcptraining (i.e., {pl, ar, ja, pt, sw, ur}).
the largestperformance gain is in ru on x-csqa and vi on x-codah.
surprisingly, we ﬁnd the improvementson them are also large for xlm-rl (e.g., 48.4!
52.3 for ar).
in addition, for the two low-resourcelanguages sw and ur, mcp also brings 23 per-centage points of improvement for xlm-rl.
it is,however, not always the case for xlm-rb, whichwe conjecture tends to be more likely to overﬁt..⇠.
1281mcp(xlm-r-b).
mcp(xlm-r-l).
xlm-r-l.xlm-r-b.
0.7.
0.6.
0.5.
0.4.
0.3.
0.2.
100.
200.
300.
400.
500.
600.
700.
800.figure 5: dev acc v.s.
learning steps on x-csqa..step.
although ml-lms enjoy the merits of zero-shot cross-lingual transfer, their performances areusually worse than the english-only robertalon the en-test (70.4% vs 66.7% for x-csqa).
although mcp can mitigate the gap (70.4% vs69.5%) for x-csqa, there is still a large gap(81.6% vs 69.9%) for x-codah.
we use fig.
4to analyze how different categories of common-sense reasoning in codah (chen et al., 2019)are diverse in different languages.
we ﬁnd thatothers, reference, and negation have relativelysmaller variances across different languages, asthey are more language-invariant.
however, afew polysemous, idioms examples can be english-speciﬁc which may not generalize to other lan-guages.
more detailed analysis is in appendix..from the curve of dev accuracy in figure 5,we see that mcp-enhanced xlm-r models aremuch more sample efﬁcient and converge muchfaster than vanilla versions.
this suggests that themcp, if used on a larger corpus with broader top-ics, can potentially produce a better ml-lm withmore general usage, especially when only limitedlabelled is available.
our results on xnli-10%(using 10% of the training data) (conneau et al.,2018) show that mcp-enhanced xlm-rl has 1.2percent accuracy improvement on the average of15 languages.
as our focus in this paper is com-monsense reasoning, we leave the study on othercross-lingual nlu tasks as future work.
impor-tantly, our experiments imply that a proper (con-tinual) pre-training task that has a (contrastive)sentence-level objective could improve both the ﬁ-nal performance as well as learning efﬁciency..7 conclusion.
we evaluate and improve popular multilingual lan-guage models (ml-lms) for advancing common-sense reasoning beyond english.
we proposethe mickeyprobe, a language-agnostic probingtask for analyzing common sense of ml-lms in a.zero-shot manner.
with our proposed new bench-mark datasets via automatic translation, x-csqaand x-codah, we evaluate ml-lms in a cross-lingual transfer setting for commonsense reason-ing.
we also improve the state-of-the-art ml-lmwith a simple yet effective method — multilingualcontrastive pre-training, which uses a sentence-level objective to enhance sentence representa-tions, yielding a signiﬁcant performance gain.
allabove work is based on mickeycorpus, whichcan be used as both a probing dataset and a pre-training corpus for analyzing and improving ml-lms.
we hope our resources and pre-trainingmethod for ml-lms can help the community ad-vance commonsense reasoning beyond english..acknowledgements.
this research is supported in part by the ofﬁceof the director of national intelligence (odni),intelligence advanced research projects activity(iarpa), via contract no.
2019-19051600007,the darpa mcs program under contract no.
n660011924033 with the united states ofﬁce ofnaval research, the defense advanced researchprojects agency with award w911nf-19-20271,and nsf sma 18-29268. the views and con-clusions contained herein are those of the authorsand should not be interpreted as necessarily rep-resenting the ofﬁcial policies, either expressed orimplied, of odni, iarpa, or the u.s. govern-ment.
we would like to thank all the collabora-tors in usc ink research lab and the reviewersfor their constructive feedback on the work..* ethical considerations.
resource copyright this work presents threenew resources: mickeycorpus, x-codah,and x-csqa, which are multilingual extension ofthe omcs (singh et al., 2002) 3, csqa (talmoret al., 2019)4, and codah (chen et al., 2019)5 re-spectively.
all these three original sources of thedata are publicly available for free, and we do notadd any additional requirement for accessing ourresources.
we will highlight the original sourcesof our data and ask users to cite the original paperswhen they use our extended versions for research..3https://github.com/commonsense/.
conceptnet5/wiki/downloads.
4https://www.tau-nlp.org/commonsenseqa5https://github.com/websail-nu/codah.
1282cultural bias reduction like most most mul-tilingual parallel resources, especially in generalnlu domain, there exists potential data bias dueto the barrier of languages as well as cultural dif-ferences (acharya et al., 2020; lin et al., 2018),which could induce the labeling differences on thesame situation.
for example, a question like “whatdo people usually drink in the morning?
(cof-fee/tea/milk)” or “when does a wedding usuallystart?
(morning/afternoon/evening)” might be an-swered very differently by people from differentbackgrounds and cultures, not to mention differ-ent languages.
the prior english commonsenseresources which our datasets are built on are al-ready possess such inherent bias, even with in theenglish language.
therefore, before we translatecsqa and codah, we intentionally remove theexamples that are either labeled as non-neutral bya pre-trained sentiment classiﬁer, or contained anykeywords that are relevant to social behavior (e.g.,weddings).
we manually inspect test examples inx-csqa and x-codah in the english and chi-nese versions and have a strong conﬁdence there isfew strongly controversial example.
however, weadmit that such reduction of cultural differences incommon sense has not been systematically mea-sured in this work for other languages..application risks of cross-lingual csr..the work also evaluates a few multilingual lan-guage models (ml-lms) for cross-lingual com-monsense reasoning (xcsr), and introduced anew model which outperforms them.
this raisesthe question of whether harm might arise fromapplications of xcsr—or more generally, sincexcsr is intended as a step toward makingenglish-only csr more applicable in other lan-guages, whether harm might arise more generallyfrom existing ml-lms.
among the risks that needto be considered in any deployment of nlp tech-nology are that responses may be wrong or biased,in ways that would lead to improperly justiﬁed de-cisions.
although in our view the current technol-ogy is still relatively immature, and unlikely to beﬁelded in applications that would cause harm ofthis sort, it is desirable that ml-lms provide au-dit trails, and recourse so that their predictions canbe explained to and critiqued by affected parties..references.
a. acharya, kartik talamadupula, and mark a. fin-layson.
2020. an atlas of cultural commonsense formachine reasoning.
arxiv, abs/2009.05664..michael chen, mike d’arcy, alisa liu, jared fer-nandez, and doug downey.
2019. codah: anadversarially-authored question answering datasetfor common sense.
in proceedings of the 3rd work-shop on evaluating vector space representationsfor nlp, pages 63–69, minneapolis, usa.
associa-tion for computational linguistics..jonathan h. clark, eunsol choi, michael collins, dangarrette, tom kwiatkowski, vitaly nikolaev, andjennimaria palomaki.
2020. tydi qa: a bench-mark for information-seeking question answering intypologically diverse languages.
transactions of theassociation for computational linguistics, 8:454–470..alexis conneau, kartikay khandelwal, naman goyal,vishrav chaudhary, guillaume wenzek, franciscoguzmán, edouard grave, myle ott, luke zettle-moyer, and veselin stoyanov.
2020. unsupervisedcross-lingual representation learning at scale.
inproceedings of the 58th annual meeting of the asso-ciation for computational linguistics, pages 8440–8451, online.
association for computational lin-guistics..alexis conneau and guillaume lample.
2019. cross-lingual language model pretraining.
in advances inneural information processing systems 32: annualconference on neural information processing sys-tems 2019, neurips 2019, december 8-14, 2019,vancouver, bc, canada, pages 7057–7067..alexis conneau, ruty rinott, guillaume lample, ad-ina williams, samuel bowman, holger schwenk,and veselin stoyanov.
2018. xnli: evaluatingcross-lingual sentence representations.
in proceed-ings of the 2018 conference on empirical methodsin natural language processing, pages 2475–2485,brussels, belgium.
association for computationallinguistics..ernest davis and gary marcus.
2015. commonsensereasoning and commonsense knowledge in artiﬁ-cial intelligence.
communications of the acm,58(9):92–103..jacob devlin, ming-wei chang, kenton lee, andkristina toutanova.
2019. bert: pre-training ofdeep bidirectional transformers for language under-in proceedings of the 2019 conferencestanding.
of the north american chapter of the associationfor computational linguistics: human languagetechnologies, volume 1 (long and short papers),pages 4171–4186, minneapolis, minnesota.
associ-ation for computational linguistics..yanlin feng, xinyue chen, bill yuchen lin, peifengwang, jun yan, and xiang ren.
2020. scalablemulti-hop relational reasoning for knowledge-aware.
1283in proceedings of the 2020question answering.
conference on empirical methods in natural lan-guage processing (emnlp), pages 1295–1309, on-line.
association for computational linguistics..the 2020 conference on empirical methods in nat-ural language processing (emnlp), pages 6862–6868, online.
association for computational lin-guistics..junjie hu, sebastian ruder, aditya siddhant, gra-ham neubig, orhan firat, and melvin johnson.
2020. xtreme: a massively multilingual multi-task benchmark for evaluating cross-lingual gen-eralization.
technical report..zhengbao jiang, antonios anastasopoulos, jun araki,haibo ding, and graham neubig.
2020. x-factr:multilingual factual knowledge retrieval from pre-in proceedings of thetrained language models.
2020 conference on empirical methods in naturallanguage processing (emnlp), pages 5943–5959,online.
association for computational linguistics..marcin junczys-dowmunt, roman grundkiewicz,tomasz dwojak, hieu hoang, kenneth heaﬁeld,tom neckermann, frank seide, ulrich germann,alham fikri aji, nikolay bogoychev, andré f. t.martins, and alexandra birch.
2018. marian: fastneural machine translation in c++.
in proceedingsof acl 2018, system demonstrations, pages 116–121, melbourne, australia.
association for compu-tational linguistics..linyang li, ruotian ma, qipeng guo, xiangyang xue,and xipeng qiu.
2020. bert-attack: adversar-ial attack against bert using bert.
in proceed-ings of the 2020 conference on empirical methodsin natural language processing (emnlp), pages6193–6202, online.
association for computationallinguistics..yaobo liang, nan duan, yeyun gong, ning wu, fen-fei guo, weizhen qi, ming gong, linjun shou,daxin jiang, guihong cao, xiaodong fan, ruofeizhang, rahul agrawal, edward cui, sining wei,taroon bharti, ying qiao, jiun-hung chen, win-nie wu, shuguang liu, fan yang, daniel cam-pos, rangan majumder, and ming zhou.
2020.xglue: a new benchmark datasetfor cross-lingualpre-training, understanding and generation.
in pro-ceedings of the 2020 conference on empirical meth-ods in natural language processing (emnlp),pages 6008–6018, online.
association for compu-tational linguistics..bill yuchen lin, xinyue chen, jamin chen, and xiangren.
2019. kagnet: knowledge-aware graph net-works for commonsense reasoning.
in proceedingsof the 2019 conference on empirical methods innatural language processing and the 9th interna-tional joint conference on natural language pro-cessing (emnlp-ijcnlp), pages 2829–2839, hongkong, china.
association for computational lin-guistics..bill yuchen lin, seyeon lee, rahul khanna, and xi-ang ren.
2020. birds have four legs?!
numersense:probing numerical commonsense knowledge ofin proceedings ofpre-trained language models..bill yuchen lin, frank f. xu, kenny zhu, and seung-won hwang.
2018. mining cross-cultural differ-ences and similarities in social media.
in proceed-ings of the 56th annual meeting of the associationfor computational linguistics (volume 1: long pa-pers), pages 709–719, melbourne, australia.
asso-ciation for computational linguistics..yinhan liu, jiatao gu, naman goyal, xian li, sergeyedunov, marjan ghazvininejad, mike lewis, andluke zettlemoyer.
2020. multilingual denoisingpre-training for neural machine translation.
trans-actions of the association for computational lin-guistics, 8:726–742..kishore papineni, salim roukos, todd ward, and wei-jing zhu.
2002. bleu: a method for automatic eval-uation of machine translation.
in proceedings of the40th annual meeting of the association for com-putational linguistics, pages 311–318, philadelphia,pennsylvania, usa.
association for computationallinguistics..fabio petroni, tim rocktäschel, sebastian riedel,patrick lewis, anton bakhtin, yuxiang wu, andalexander miller.
2019.language models asknowledge bases?
in proceedings of the 2019 con-ference on empirical methods in natural languageprocessing and the 9th international joint confer-ence on natural language processing (emnlp-ijcnlp), pages 2463–2473, hong kong, china.
as-sociation for computational linguistics..edoardo maria ponti, goran glavaš, olga majewska,qianchu liu, ivan vuli´c, and anna korhonen.
2020.xcopa: a multilingual dataset for causal com-in proceedings of the 2020monsense reasoning.
conference on empirical methods in natural lan-guage processing (emnlp), pages 2362–2376, on-line.
association for computational linguistics..peng qi, yuhao zhang, yuhui zhang, jason bolton,and christopher d. manning.
2020.stanza: apython natural language processing toolkit for manyin proceedings of the 58th an-human languages.
nual meeting of the association for computationallinguistics: system demonstrations, pages 101–108, online.
association for computational lin-guistics..nils reimers and iryna gurevych.
2019. sentence-bert: sentence embeddings using siamese bert-networks.
in proceedings of the 2019 conference onempirical methods in natural language process-ing and the 9th international joint conference onnatural language processing (emnlp-ijcnlp),pages 3982–3992, hong kong, china.
associationfor computational linguistics..1284julian salazar, davis liang, toan q. nguyen, and ka-trin kirchhoff.
2020. masked language model scor-in proceedings of the 58th annual meetinging.
of the association for computational linguistics,pages 2699–2712, online.
association for compu-tational linguistics..victor sanh, lysandre debut, julien chaumond, andthomas wolf.
2019. distilbert, a distilled versionof bert: smaller, faster, cheaper and lighter.
arxiv,abs/1910.01108..push singh, thomas lin, erik t mueller, grace lim,travell perkins, and wan li zhu.
2002. open mindcommon sense: knowledge acquisition from thegeneral public.
in otm confederated internationalconferences" on the move to meaningful internetsystems", pages 1223–1237.
springer..alon talmor, jonathan herzig, nicholas lourie, andjonathan berant.
2019. commonsenseqa: a ques-tion answering challenge targeting commonsenseknowledge.
in proceedings of the 2019 conferenceof the north american chapter of the associationfor computational linguistics: human languagetechnologies, volume 1 (long and short papers),pages 4149–4158, minneapolis, minnesota.
associ-ation for computational linguistics..jörg tiedemann.
2016. opus – parallel corpora foreveryone.
in proceedings of the 19th annual con-ference of the european association for machinetranslation: projects/products, riga, latvia.
balticjournal of modern computing..linting xue, noah constant, adam roberts, mi-hir kale, rami al-rfou, aditya siddhant, adityabarua, and colin raffel.
2021. mt5: a massivelymultilingual pre-trained text-to-text transformer.
inproceedings of the 2021 conference of the northamerican chapter of the association for computa-tional linguistics: human language technologies,pages 483–498, online.
association for computa-tional linguistics..rowan zellers, yonatan bisk, roy schwartz, andyejin choi.
2018. swag: a large-scale adversar-ial dataset for grounded commonsense inference.
in proceedings of the 2018 conference on empiri-cal methods in natural language processing, pages93–104, brussels, belgium.
association for compu-tational linguistics..tianyi zhang, varsha kishore, felix wu, kilian q.weinberger, and yoav artzi.
2020. bertscore: eval-in 8th inter-uating text generation with bert.
national conference on learning representations,iclr 2020, addis ababa, ethiopia, april 26-30,2020. openreview.net..1285appendix.
a details for dataset construction.
before we start the translation procedure, we ﬁrstre-split the datasets of csqa and codah suchthat the test set examples in the english languagedo not contain controversial examples or culture-related examples that would potentially cause cul-tural bias in our dataset.
please refer to the sectionof ethical considerations (following the conclu-sion) in the main paper for more details.
then, weuse the deepl pro translation service to translatethe 10 languages: {de, fr, es, pt, it, nl, pl, ru, jap,zh} and use google translation api to translatethe others {ar, sw, ur, vi, hi}..we agree that ideally we should use human ex-perts to translate the examples in csqa and co-dah, but the cost or building a large-scale multi-lingual dataset with the same scale of our datasetsis extremely high – around 10k usd.
as a mat-ter of fact, most of the examples in csqa andcodah are very easy and short sentences, andmost of them can be well translated by moderncommercial translation apis, because they usuallyhave a hybrid system.
moreover, we choose thedeepl online service because it has been reportedby many individual media as the best choice.
toensure the quality of the translation, we performthe translation for both directions and then usethe same quality control method as we discussedin section 4 for removing the examples that havelower cosine similarity between original englishversion and back-translated examples.
during theprocess, we manually went through the chineseversions to ﬁnd a suitable threshold for taking theintersection — 0.85, which results in a comparablebt-cosine mean to the xnli dataset 6..models.
#lgs.
tnz.
l hm.
hf f.v.#para.
mbertxlm-100xlm-rbxlm-rl.
104 wp12100 bpe 16spm 12100spm 24100.
76812807681024.
3072512030724096.
110k200k250k250k.
172m570m270m550m.
a.
12161216.table 4: model architectures..b hyper-parameters.
we summarize hyper-parameters that we used fortraining ml-lms on x-codah and x-csqa in.
6we sampled 1k examples in the test set and follow the.
same procedure for the intersection language set..⇥.
# gpus.
table 7. evaluation steps are equally 100 for allmodels and datasets.
maximum sequence lengthis 100 for x-codah and 64 for x-csqa.
thebatch size here refers to “train batch size per de-# gradient accumulation steps”.
vicenote that the mcp methods use the exactly thesame hyper-parameters which we have found op-timal by tuning over the dev set.
the learning rateswe tried for all models are from the range {3e-5,2e-5, 1e-5, 8e-6, 6e-6, 5e-6}.
the warm up stepsare selected from {50, 100, 200, 300, 500}..⇥.
c details of ml-lms.
table 4 shows the model architectures and sizesthat we used from (conneau et al., 2020).
weshow the tokenization (tnz) used by each trans-former model, the number of layers l, the numberof hidden states of the model hm, the dimensionof the feed-forward layer hf f , the number of at-tention heads a, the size of the vocabulary v andthe total number of parameters #params..d additional experimental results.
d.1 hit@1 accuracy in histogram.
d.2 hit@k accuracy of mickey probes.
table 5 shows the hit@2 accuracy of the ﬁve ml-lms for the mickeyprobe.
hit@2 accuracy eval-uates whether the models can rank the correct as-sertion within top 2. unlike hit@1 which onlyaccepts best predictions, hit@2 is more ﬂexible.
thus, the performances in hit@2 increase com-pared to the ones in hit@1. we can see that thediscrepancies across languages still exist..d.3 categorized x-codah analysis.
please refer the codah (chen et al., 2019) pa-per for the deﬁnition and concrete examples ineach category.
we show benchmark results ofmcp(xlm-rl) on x-codah within differentcarriages in table 6. the rb stands for using theroberta-large model to ﬁne-tune on the englishx-codah dataset.
we ﬁnd that the largest gapsin en are in the idioms and the others.
interest-ingly, we ﬁnd that the quantities category is wheremcp performs better than the roberta large..1286figure 6: the mickeyprobe results in hit@1-acc.
(an enlarged version of figure 3.).
models \.
l.d-mbertmbertxlm-100xlm-rbxlm-rl.
en.
de.
it.
es.
fr.
nl.
ru.
bg.
vi.
zh.
hi.
avg.
shortest.
42.20.
50.91.
52.49.
56.06.
57.30.
55.95.
40.96.
45.86.
35.64.
47.67.
43.81.
48.08.
87.0687.3885.1797.7797.83.
61.4862.3063.9683.6485.57.
47.7052.0247.0578.2176.73.
62.3073.0171.6184.7385.56.
76.1770.4155.9972.7783.71.
59.0362.4263.1484.0886.09.
45.7156.8358.7374.0477.74.
55.4762.3465.8971.6772.55.
42.5349.7750.2968.7972.01.
60.2453.8160.5377.8981.32.
52.5653.9958.0868.2770.78.
59.1162.2161.8678.3580.90.table 5: the hit@2 accuracy of the ﬁve ml-lms for the mickey probe task..category.
rb.
en.
de.
it.
es.
fr.
nl.
ru.
vi.
zh.
hi.
pl.
ar.
ja.
pt.
sw.ur.
idiomsneg.
poly.
ref.
quant.
others.
79.5275.6179.1786.4961.2982.89.
69.8875.6175.0078.3867.7468.95.
61.4565.8558.3362.1645.1661.05.
56.6365.8566.6767.5745.1662.37.
60.2470.7368.7567.5751.6159.74.
73.4970.7370.8364.8654.8459.08.
60.2458.5460.4264.8661.2960.66.
57.8370.7366.6767.5751.6157.37.
50.665.8568.7562.1661.2963.03.
55.4270.7356.2554.0545.1663.55.
45.7863.4154.1767.5754.8453.29.
59.0465.8560.4272.9758.0657.89.
50.660.9845.8375.6841.9454.08.
50.658.5466.6745.9541.9455.13.
56.6370.7368.7554.0554.8460.79.
44.5841.4645.8362.1651.6143.55.
40.9658.545056.7651.6147.5.avg.
55.8764.6361.4664.0252.4258.00.table 6: benchmark results for mcp(xlm-r-l) on x-codah in different categories.
rb = roberta-large..model.
lr.
# epoch # wus.
bsz.
x-codah.
mbertxlm-100xlm-r-bxlm-r-l.mcp(xlm-r-b)mcp(xlm-r-l).
mbertxlm-100xlm-r-bxlm-r-l.mcp(xlm-r-b)mcp(xlm-r-l).
3e-051e-051e-056e-06.
1e-056e-06.
3e-051e-051e-056e-06.
1e-056e-06.
x-csqa.
20202010.
2010.
30203010.
3010.
100100100100.
100100.
100300100100.
100100.
1286412864.
12864.
646414464.
14464.table 7: the optimal hyper-parameters for ﬁne-tuning.
(lr represents ‘learning rate’; training # epoch ; # wus= ‘# warm up steps’; bsz = ‘batch size’).
1287