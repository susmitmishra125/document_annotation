structural knowledge distillation: tractably distilling informationfor structured predictor.
xinyu wang(cid:5)‡♠, yong jiang†∗♠, zhaohui yan(cid:5)♠, zixia jia(cid:5)♠, nguyen bach†, tao wang†,zhongqiang huang†, fei huang†, kewei tu(cid:5)∗(cid:5)school of information science and technology, shanghaitech universityshanghai engineering research center of intelligent vision and imagingshanghai institute of microsystem and information technology, chinese academy of sciencesuniversity of chinese academy of sciences†damo academy, alibaba group{wangxy1,jiazx,yanzhh,tukw}@shanghaitech.edu.cn{yongjiang.jy,nguyen.bach}@alibaba-inc.com{leeo.wangt,z.huang,f.huang}@alibaba-inc.com.
abstract.
in size;.
knowledge distillation is a critical techniqueto transfer knowledge between models, typi-cally from a large model (the teacher) to amore ﬁne-grained one (the student).
the objec-tive function of knowledge distillation is typ-ically the cross-entropy between the teacherand the student’s output distributions.
how-ever, for structured prediction problems, theoutput space is exponentialthere-fore, the cross-entropy objective becomes in-tractable to compute and optimize directly.
inthis paper, we derive a factorized form of theknowledge distillation objective for structuredprediction, which is tractable for many typicalchoices of the teacher and student models.
inparticular, we show the tractability and empir-ical effectiveness of structural knowledge dis-tillation between sequence labeling and depen-dency parsing models under four different sce-narios: 1) the teacher and student share thesame factorization form of the output struc-ture scoring function; 2) the student factoriza-tion produces more ﬁne-grained substructuresthan the teacher factorization; 3) the teacherfactorization produces more ﬁne-grained sub-structures than the student factorization; 4) thefactorization forms from the teacher and thestudent are incompatible.1.
1.introduction.
deeper and larger neural networks have led to sig-niﬁcant improvement in accuracy in various tasks,but they are also more computationally expensiveand unﬁt for resource-constrained scenarios such.
∗ yong jiang and kewei tu are the corresponding authors.
♠: equal contributions.
‡: this work was conducted whenxinyu wang was interning at alibaba damo academy..1our code is publicly available at https://github..com/alibaba-nlp/structuralkd..as online serving.
an interesting and viable solu-tion to this problem is knowledge distillation (kd)(buciluˇa et al., 2006; ba and caruana, 2014; hin-ton et al., 2015), which can be used to transferthe knowledge of a large model (the teacher) to asmaller model (the student).
in the ﬁeld of natu-ral language processing (nlp), for example, kdhas been successfully applied to compress massivepretrained language models such as bert (devlinet al., 2019) and xlm-r (conneau et al., 2020)into much smaller and faster models without sig-niﬁcant loss in accuracy (tang et al., 2019; sanhet al., 2019; tsai et al., 2019; mukherjee and has-san awadallah, 2020)..a typical approach to kd is letting the stu-dent mimic the teacher model’s output probabil-ity distributions on the training data by using thecross-entropy objective.
for structured predictionproblems, however, the output space is exponen-tially large, making the cross-entropy objective in-tractable to compute and optimize directly.
takesequence labeling for example.
if the size of thelabel set is l, then there are ln possible label se-quences for a sentence of n words and it is infeasi-ble to compute the cross-entropy by enumeratingthe label sequences.
previous approaches to struc-tural kd either choose to perform kd on localdecisions or substructures instead of on the full out-put structure, or resort to top-k approximation ofthe objective (kim and rush, 2016; kuncoro et al.,2016; wang et al., 2020a)..in this paper, we derive a factorized form of thestructural kd objective based on the fact that al-most all the structured prediction models factorizethe scoring function of the output structure intoscores of substructures.
if the student’s substruc-ture space is polynomial in size and the teacher’s.
proceedingsofthe59thannualmeetingoftheassociationforcomputationallinguisticsandthe11thinternationaljointconferenceonnaturallanguageprocessing,pages550–564august1–6,2021.©2021associationforcomputationallinguistics550marginal distributions over these substructures canbe tractably estimated, then we can tractably com-pute and optimize the factorized form of the struc-tural kd objective.
as will be shown in the paper,many widely used structured prediction models sat-isfy the assumptions and hence are amenable totractable kd.
in particular, we show the feasibilityand empirical effectiveness of structural kd withdifferent combinations of teacher and student mod-els, including those with incompatible factorizationforms.
we apply this technique to structural kdbetween sequence labeling and dependency parsingmodels under four different scenarios..1. the teacher and student share the same factor-ization form of the output structure scoring func-tion..2. the student factorization produces more ﬁne-grained substructures than the teacher factoriza-tion..3. the teacher factorization produces more ﬁne-grained substructures than the student factoriza-tion..4. the factorization forms from the teacher and the.
student are incompatible..in all the cases, we empirically show that ourstructural kd approaches can improve the studentmodels.
in the few cases where previous kd ap-proaches are applicable, we show our approachesoutperform these previous approaches.
with un-labeled data, our approaches can further improvestudent models’ performance.
in a zero-shot cross-lingual transfer case, we show that with sufﬁcientunlabeled data, student models trained by our ap-proaches can even outperform the teacher models..2 background.
2.1 structured prediction.
structured prediction aims to predict a structuredoutput such as a sequence, a tree or a graph.
inthis paper, we focus on structured prediction prob-lems with a discrete output space, which includemost of the structured prediction tasks in nlp(e.g., chunking, named entity recognition, and de-pendency parsing) and many structured predictiontasks in computer vision (e.g., image segmentation).
we further assume that the scoring function of theoutput structure can be factorized into scores of apolynomial number of substructures.
consequently,.
we can calculate the conditional probability of theoutput structure y given an input x as follows:.
p (y|x) =.
(cid:80).
(cid:81).
exp (score(y, x))y(cid:48)∈y(x) exp (score(y(cid:48), x))u∈y exp (score(u, x))z(x).
=.
(1).
where y(x) represents all possible output struc-tures given the input x, score(y, x) is the scoringfunction that evaluates the quality of the output y,z(x) is the partition function, and u ∈ y denotesthat u is a substructure of y. we deﬁne the sub-structure space u(x) = (cid:83)y∈y(x){u|u ∈ y}as theset of substructures of all possible output structuresgiven input x..take sequence labeling for example.
given asentence x , the output space y(x) contains allpossible label sequences of x. in linear-chain crf,a popular model for sequence labeling, the scor-ing function score(y, x) is computed by summingup all the transition scores and emission scoreswhere i ranges over all the positions in sentence x,and the substructure space u(x) contains all pos-sible position-speciﬁc labels {yi} and label pairs{(yi−1, yi)}..2.2 knowledge distillation.
knowledge distillation is a technique that trains asmall student model by encouraging it to imitatethe output probability distribution of a large teachermodel.
the typical kd objective function is thecross-entropy between the output distributions pre-dicted by the teacher model and the student model:.
lkd = −.
pt(y|x) log ps(y|x).
(2).
(cid:88).
y∈y(x).
where pt and ps are the teacher’s and the student’sdistributions respectively..during training, the student jointly learns fromthe gold targets and the distributions predicted bythe teacher by optimizing the following objectivefunction:.
lstudent = λlkd + (1 − λ)ltarget.
where λ is an interpolation coefﬁcient between thetarget loss ltarget and the structural kd loss lkd.
following clark et al.
(2019); wang et al.
(2020a),one may apply teacher annealing in training bydecreasing λ linearly from 1 to 0. because kddoes not require gold labels, unlabeled data canalso be used in the kd loss..5513 structural knowledge distillation.
when performing knowledge distillation on struc-tured prediction, a major challenge is that the struc-tured output space is exponential in size, leading tointractable computation of the kd objective in eq.
2. however, if the scoring function of the studentmodel can be factorized into scores of substructures(eq.
1), then we can derive the following factorizedform of the structural kd objective..lkd = −.
pt(y|x)logps(y|x).
(cid:88).
y∈y(x).
pt(y|x).
scores(u, x)+logzs(x).
=−.
pt(y|x).
1u∈yscores(u, x)+logzs(x).
(cid:88).
=−.
y∈y(x)(cid:88).
(cid:88).
u∈y(cid:88).
y∈y(x).
u∈us(x).
(cid:88) (cid:88).
=−u∈us(x),y∈y(x).
(cid:88).
=−.
u∈us(x).
pt(y|x)1u∈yscores(u, x)+logzs(x).
pt(u|x)scores(u, x)+logzs(x).
(3).
where 1condition is 1 if the condition is true and 0otherwise.
from eq.
3, we see that if us(x) ispolynomial in size and pt(u|x) can be tractablyestimated, then the structural kd objective can betractably computed and optimized.
in the rest ofthis section, we will show that this is indeed thecase for some of the most widely used models insequence labeling and dependency parsing, tworepresentative structured prediction tasks in nlp.
based on the difference in score factorization be-tween the teacher and student models, we divideour discussion into four scenarios..3.1 teacher and student share the same.
factorization form.
case 1a: linear-chain crf ⇒ linear-chaincrf in this case, both the teacher and the studentare linear-chain crf models.
an example appli-cation is to compress a state-of-the-art crf modelfor named entity recognition (ner) that is basedon large pretrained contextualized embeddings to asmaller crf model with static embeddings that ismore suitable for fast online serving..for a crf student model described in sec-tion 2.1,if we absorb the emission scorese(yi, x) into the transition score st((yi−1, yi), x)at each position i,then the substructurespace us(x) contains every two adjacentla-bels {(yi−1, yi)} for i=1, .
.
.
, n, with n be-.
ing the sequence length, and the substruc-ture score is deﬁned as score((yi−1, yi), x) =st((yi−1, yi), x) + se(yi, x).
the substructuremarginal pt((yi−1, yi)|x) of the teacher model canbe computed by:.
pt((yi−1, yi)|x) ∝ α(yi−1) × β(yi).
× exp(score((yi−1, yi), x)).
(4).
where α(yi−1) and β(yi) are forward and back-ward scores that can be tractably calculated usingthe classical forward-backward algorithm..comparing with the posterior kd and top-kkd of linear-chain crfs proposed by wang et al.
(2020a), our approach calculates and optimizesthe kd objective exactly, while their two kd ap-proaches perform kd either heuristically or approx-imately.
at the formulation level, our approach isbased on the marginal distributions of two adja-cent labels, while the posterior kd is based on themarginal distributions of a single label..case 1b: graph-based dependency parsing ⇒dependency parsing as sequence labelinginthis case, we use the biafﬁne parser proposed bydozat et al.
(2017) as the teacher and the sequencelabeling approach proposed by strzyz et al.
(2019)as the student for the dependency parsing task.
thebiafﬁne parser is one of the state-of-the-art models,while the sequence labeling parser provides a goodspeed-accuracy tradeoff.
there is a big gap in accu-racy between the two models and therefore kd canbe used to improve the accuracy of the sequencelabeling parser..here we follow the head-selection formulationof dependency parsing without the tree constraint.
the dependency parse tree y is represented by(cid:104)y1, .
.
.
, yn(cid:105), where n is the sentence length andyi = (hi, li) denotes the dependency head of thei-th token of the input sentence, with hi being theindex of the head token and li being the dependencylabel.
the biafﬁne parser predicts the dependencyhead for each token independently.
it models sepa-rately the probability distribution of the head indexpt(hi|x) and the probability distribution of the la-bel pt(li|x).
the sequence labeling parser is amaxent model that also predicts the head of eachtoken independently.
it computes score((hi, li), x)for each token and applies a softmax function toproduce the distribution ps((hi, li)|x)..therefore, these two models share the same fac-torization in which each substructure is a depen-.
552dency arc speciﬁed by yi.
us(x) thus containsall possible dependency arcs among tokens of theinput sentence x. the substructure marginal pre-dicted by the teacher can be easily derived as:.
pt((hi, li)|x) = pt(hi|x) × pt(li|x).
(5).
note that in this case, the sequence labeling parseruses a maxent decoder, which is locally normal-ized for each substructure.
therefore, the structuralkd objective in eq.
3 can be reduced to the fol-lowing form without the need for calculating thestudent partition function zs(x)..lkd = −.
pt(u|x) × logps(u|x).
(6).
(cid:88).
u∈us(x).
in all the cases except case 1a and case 3, thestudent model is locally normalized and hence wecan follow this form of objective..3.2 student factorization produces more.
fine-grained substructures than teacherfactorization.
case 2a: linear-chain crf ⇒ maxentinthis case, we use a linear-chain crf model as theteacher and a maxent model as the student.
pre-vious work (yang et al., 2018; wang et al., 2020a)shows that a linear-chain crf decoder often leadsto better performance than a maxent decoder formany sequence labeling tasks.
still, the simplicityand efﬁciency of the maxent model is desirable.
therefore, it makes sense to perform kd from alinear-chain crf to a maxent model..as mentioned in case 1a, the substructures ofa linear-chain crf model are consecutive labels{(yi−1, yi)}.
in contrast, a maxent model pre-dicts the label probability distribution ps(yi|x) ofeach token independently and hence the substruc-ture space us(x) consists of every individual label{yi}.
to calculate the substructure marginal of theteacher pt(yi|x), we can again utilize the forward-backward algorithm:.
pt(yi|x) ∝ α(yi) × β(yi).
(7).
where α(yi) and β(yi) are forward and backwardscores..case 2b: second-order dependency parsing⇒ dependency parsing as sequence labelingthe biafﬁne parser is a ﬁrst-order dependencyparser, which scores each dependency arc in a.parse tree independently.
a second-order depen-dency parser scores pairs of dependency arcs witha shared token.
the substructures of second-orderparsing are therefore all the dependency arc pairswith a shared token.
it has been found that second-order extensions of the biafﬁne parser often havehigher parsing accuracy (wang et al., 2019; zhanget al., 2020; wang et al., 2020d; wang and tu,2020).
therefore, we may take a second-orderdependency parser as the teacher to improve a se-quence labeling parser..here we consider the second-order dependencyparser of wang and tu (2020).
it employs meanﬁeld variational inference to estimate the probabili-ties of arc existence pt(hi|x) and uses a ﬁrst-orderbiafﬁne model to estimate the probabilities of arc la-bels pt(li|x).
therefore, the substructure marginalcan be calculated in the same way as eq.
5..3.3 teacher factorization produces more.
fine-grained substructures than studentfactorization.
case 3: maxent ⇒ linear-chain crf herewe consider kd in the opposite direction of case2a.
an example application is zero-shot cross-lingual ner.
previous work (pires et al., 2019;wu and dredze, 2019) has shown that multilin-gual bert (m-bert) has strong zero-shot cross-lingual transferability in ner tasks.
many suchmodels employ a maxent decoder.
in scenarios re-quiring fast speed and low computation cost, how-ever, we may want to distill knowledge from suchmodels to a model with much cheaper static mono-lingual embeddings while compensating the perfor-mance loss with a linear-chain crf decoder..as described in case 1a, the substructures ofa linear-chain crf model are consecutive labels{(yi−1, yi)}.
because of the label independenceand local normalization in the maxent model, thesubstructure marginal of the maxent teacher is cal-culated by:.
pt((yi−1, yi)|x) = pt(yi−1|x)pt(yi|x).
(8).
3.4 factorization forms from teacher and.
student are incompatible.
case 4: ner as parsing ⇒ maxent veryrecently, yu et al.
(2020) propose to solve thener task as graph-based dependency parsing andachieve state-of-the-art performance.
they repre-sent each named entity with a dependency arc from.
553the ﬁrst token to the last token of the named en-tity, and represent the entity type with the arc label.
however, for the ﬂat ner task (i.e., there is nooverlapping between entity spans), the time com-plexity of this method is higher than commonlyused sequence labeling ner methods.
in this case,we take a parsing-based ner model as our teacherand a maxent model with the bioes label schemeas our student..the two models adopt very different representa-tions of ner output structures.
the parsing-basedteacher model represents an ner output of a sen-tence with a set of labeled dependency arcs anddeﬁnes its score as the sum of arc scores.
the max-ent model represents an ner output of a sentencewith a sequence of bioes labels and deﬁnes itsscore as the sum of token-wise label scores.
there-fore, the factorization forms of these two modelsare incompatible..computing the substructure marginal of theteacher pt(yi|x), where yi ∈ {bl, il, el, sl, o|l ∈l} and l is the set of entity types, is much morecomplicated than in the previous cases.
takeyi = bl for example.
pt(yi = bl|x) represents theprobability of the i-th word being the beginning ofa multi-word entity of type ‘l’.
in the parsing-basedteacher model, this probability is proportional tothe summation of exponentiated scores of all theoutput structures that contain a dependency arc oflabel ‘l’ with the i-th word as its head and with itslength larger than 1. it is intractable to computesuch marginal probabilities by enumerating all theoutput structures, but we can tractably computethem using dynamic programming.
see supple-mentary material for a detailed description of ourdynamic programming method..4 experiments.
we evaluate our approaches described in section3 on ner (case 1a, 2a, 3, 4) and dependencyparsing (case 1b, 2b)..4.1 settings.
datasets we use conll 2002/2003 datasets(tjong kim sang, 2002; tjong kim sang andde meulder, 2003) for case 1a, 2a and 4, anduse wikiann datasets (pan et al., 2017) for case1a, 2a, 3, and 4. the conll datasets containthe corpora of four indo-european languages.
weuse the same four languages from the wikianndatasets.
for cross-lingual transfer in case 3, we.
use the four indo-european languages as the sourcefor the teacher model and additionally select fourlanguages from different language families as thetarget for the student models.2.
we use the standard training/development/testsplit for the conll datasets.
for wikiann, wefollow the sampling of wang et al.
(2020a) with12000 sentences for english and 5000 sentences foreach of the other languages.
we split the datasetsby 3:1:1 for training/development/test.
for case1b and 2b, we use penn treebank (ptb) 3.0 andfollow the same pre-processing pipeline as in maet al.
(2018).
for unlabeled data, we sample sen-tences that belong to the same languages of thelabeled data from the wikiann datasets for case1a, 2a and 4 and we sample sentences from thetarget languages of wikiann datasets for case 3.we use the bllip corpus3 as the unlabeled datafor case 1b and 2b..models for the student models in all the cases,we use fasttext (bojanowski et al., 2017) word em-beddings and character embeddings as the wordrepresentation.
for case 1a, 2a and 4, we con-catenate the multilingual bert, flair (akbik et al.,2018), fasttext embeddings and character embed-dings (santos and zadrozny, 2014) as the wordrepresentations for stronger monolingual teachermodels (wang et al., 2020c).
for case 3, we use m-bert embeddings for the teacher.
also for case3, we ﬁne-tune the teacher model on the trainingset of the four indo-european languages from thewikiann dataset and train student models on thefour additional languages.
for the teacher modelsin case 1b and 2b, we simply use the same em-beddings as the student because there is alreadyhuge performance gap between the teacher and stu-dent in these settings and hence we do not needstrong embeddings for the teacher to demonstratethe utility of kd..baselines we compare our structural kd (struct.
kd) with training without kd (w/o kd) as well asexisting kd approaches.
in case 1a, the pos.
kdbaseline is the posterior kd approach for linear-chain crfs proposed by wang et al.
(2020a).
they.
2the four languages from the conll datasets are dutch,english, german and spanish and the four target languages forcase 3 are basque, hebrew, persian and tamil.
we use iso639-1 language codes (https://en.wikipedia.org/wiki/list_of_iso_639-1_codes) to represent eachlanguage..3brown laboratory for linguistic information processing.
(bllip) 1987-89 wsj corpus release 1..5541a.
2b.
1b.
2acon wiki ptb con wiki ptb con wiki89.15 88.52 95.96 89.15 88.52 96.04 88.57 88.3884.70 83.31 89.85 83.87 80.86 89.85 83.87 80.86.caselabeledteacherw/o kdpos.
kd 85.27 83.73struct.
kd 85.35 84.12 91.83 84.50 82.23 91.78 84.28 81.45.
4.
-.
-.
-.
-.
-.
-.
case 2a.
case 3conll wikiann wiki+u wiki u56.0188.65-89.1538.4284.2545.2884.50.
87.4188.5283.0783.34.
87.4188.5282.0982.23.maxent teachercrf teachertoken.
kdstruct.
kd.
table 1: averaged f1 scores for ner and labeled at-tachment scores (las) for dependency parsing on la-beled datasets.
con: conll datasets..1a.
2a.
2b.
1bcaselabeled+unlabeled wiki ptb wiki ptb wiki u wiki88.52 95.96 88.52 96.04 56.01 88.38teacher84.19 90.03 82.40 90.03 41.11 82.10top-1pos.
kd + top-1-84.91struct.
kd + top-1 85.24 91.98 85.24 91.94 45.28 82.44.
3.
4.
-.
-.
-.
-.
table 2: average f1 score of ner and labeled attach-ment scores (las) for dependency parsing with bothlabeled and unlabeled data.
wiki u means that the train-ing data of this case contains only the unlabeled data..also propose top-k kd but have shown that it is in-ferior to pos.
kd.
for experiments using unlabeleddata in all the cases, in addition to labeled data, weuse the teacher’s prediction on the unlabeled dataas pseudo labeled data to train the student mod-els.
this can be seen as the top-1 kd method4.
in case 2a and 3, where we perform kd betweencrf and maxent models, we run a reference base-line that replaces the crf teacher or student modelwith a maxent model and performs token-level kd(token kd) of maxent models that optimizes thecross entropy between the teacher and student labeldistributions at each position..training for maxent and linear-chain crf mod-els, we use the same hyper-parameters as in akbiket al.
(2018).
for dependency parsing, we use thesame hyper-parameters as in wang and tu (2020)for teacher models and strzyz et al.
(2019) for stu-dent models.
for m-bert ﬁne-tuning in case 3,we mix the training data of the four source datasetsand train the teacher model with the adamw opti-mizer (loshchilov and hutter, 2018) with a learn-ing rate of 5×10−5 for 10 epochs.
we tune thekd temperature in {1, 2, 3, 4, 5} and the loss inter-polation annealing rate in {0.5, 1.0, 1.5}.
for allexperiments, we train the models for 5 runs with aﬁxed random seed for each run..table 3: comparing with reference baselines on nertask.
wiki+u means the training data comprises la-beled and unlabeled wikiann data and wiki u meansthat the training data of this case contains only the un-labeled data..w/o kd†top-wk kd†pos.
kd†pos.+top-wk†struct.
kd.
w/o kd†token kd†struct.
kd.
de.
82.1682.1582.2282.3182.28.
81.4081.3081.27.encase 1a90.1390.5290.6890.5390.86case 2a90.0890.0290.25.es.
nl.
avg..88.0688.6488.5788.6688.67.
87.7288.2488.64.
89.1189.2489.4189.5890.07.
88.9988.8789.14.
87.3687.6487.7287.7787.97.
87.0587.1187.32.table 4: a comparison of kd approaches for multilin-gual ner.
†: results are from wang et al.
(2020a)..4.2 results.
table 1 shows the experimental results with labeleddata only and 2 shows the experimental results withadditional 3000 unlabeled sentences.
the resultsshow that our structural kd approaches outper-form the baselines in all the cases.
table 3 com-pares struct.
kd with token kd, the referencebaseline based on maxent models.
for case 2a,which involves a maxent student, struct.
kd witha crf teacher achieves better results than tokenkd with a maxent teacher.
for case 3, which in-volves a maxent teacher, struct.
kd with a crfstudent achieves better results than token kd witha maxent student.
these results are to be expectedbecause struct.
kd makes it possible to applyexact knowledge distillation with a more capableteacher or student.
in all the experiments, we runalmost stochastic dominance proposed by droret al.
(2019) with a signiﬁcance level of 0.05 andﬁnd that the advantages of our structural kd ap-proaches are signiﬁcant.
please refer to appendixfor more detailed results..4.3 multilingual ner experiments.
4we do not predict pseudo labels for the labeled data,because we ﬁnd that the teacher models’ predictions on thelabeled training data have approximately 100% accuracy inmost of the cases..there is a recent increase of interest in training mul-tilingual ner models (tsai et al., 2019; mukherjeeand hassan awadallah, 2020) because of the strong.
555generalizability of m-bert on multiple languages.
existing work explored knowledge distillation ap-proaches to train fast and effective multilingualner models with the help of monolingual teachers(wang et al., 2020a).
to show the effectivenessof structural kd in the multilingual ner setting,we compare our approaches with those reported bywang et al.
(2020a).
speciﬁcally, the monolingualteachers are always crf models, and the multilin-gual student is either a crf model (case 1a) or amaxent model (case 2a).
wang et al.
(2020a) re-port results of the top-wk kd (a weighted versionof top-k kd) and pos.
kd approaches for case1a and the reference baseline token kd (with amaxent teacher) for case 2a.
we follow their ex-perimental settings when running our approach..the experimental results in table 4 show theeffectiveness of struct.
kd in both cases.
in case1a, our approach is stronger than both top-wk kdand pos.
kd as well as the mixture of the twoapproaches on average.
in case 2a, struct.
kdnot only outperforms token kd, but also makes themaxent student competitive with the crf studentwithout kd (87.32 vs. 87.36)..5 analysis.
5.1 amount of unlabeled data.
we compare our approaches with the baselines withdifferent amounts of unlabeled data for case 1a,1b and 3, which are cases that apply in-domainunlabeled data for ner and dependency parsing,and cross-lingual unlabeled data for ner.
we ex-periment with more unlabeled data for case 1bthan for the other two cases because the labeledtraining data of ptb is more than 10 times largerthan the labeled ner training data in case 1a and3. results are shown in figure 1. the experimentalresults show that our approaches consistently out-perform the baselines, though the performance gapsbetween them become smaller when the amountof unlabeled data increases.
comparing the perfor-mance of the students with the teachers, we cansee that in case 1a and 1b, the gap between theteacher and the student remains large even with thelargest amount of unlabeled data.
this is unsurpris-ing considering the difference in model capacitybetween the teacher and the student.
in case 3,however, we ﬁnd that when using 30,000 unlabeledsentences, the crf student models can even out-perform the maxent teacher model, which showsthe effectiveness of crf models on ner..crfgloballocal.
de75.3775.6776.61.en91.2191.1191.41.es86.5586.7287.20.nl85.6785.9286.19.avg.
84.7084.8585.35.table 5: comparison of the global and local tempera-ture application approaches on conll ner..crfcrf-mrg.
ner-par.
ner-par.-mrg.
maxent.
conll wikiann89.1589.0888.5787.4088.65.
88.5288.4188.3886.8287.41.table 6: averaged f1 score of teachers and it’smarginal distributions.
-mrg.
: marginal distribution,ner-par.
: ner as parsing (yu et al., 2020)..5.2 temperature in structural knowledge.
distillation.
a frequently used kd technique is dividing the log-its of probability distributions of both the teacherand the student by a temperature in the kd ob-jective (hinton et al., 2015).
using a higher tem-perature produces softer probability distributionsand often results in higher kd accuracy.
in struc-tural kd, there are two approaches to applying thetemperature to the teacher model, either globallyto the logit of pt(y|x) (i.e., scoret(y, x)) of thefull structure y, or locally to the logit of pt(u|x)of each student substructure u. we empiricallycompare these two approaches in case 1a withthe same setting as in section 4.1. table 5 showsthat the local approach results in better accuracyfor all the languages.
therefore, we use the localapproach by default in all the experiments..5.3 comparison of teachers.
in case 2a and case 4, we use the same max-ent student model but different types of teachermodels.
our structural kd approaches in bothcases compute the marginal distribution pt(yi|x)of the teacher at each position i following the sub-structures of the maxent student, which is thenused to train the student substructure scores.
wecan evaluate the quality of the marginal distribu-tions by taking their modes as label predictions andevaluating their accuracy.
in table 6, we comparethe accuracy of the crf teacher and its marginaldistributions from case 2a, the ner-as-parsingteacher and its marginal distributions from case4, and the maxent teacher which is the kd base-line in case 2a.
first, we observe that for both.
556top-1 kd.
token kd.
struct.
kd.
30.top-1 kd.
struct.
kd.
pos.
kd.
88.
86.
84.
94.
92.
90.
60.
50.
40.top-1 kd.
struct.
kd.
0 3.
10(a) case 1a.
30.
0 10.
100.
3.
10.
5030(b) case 1b.
(c) case 3.figure 1: the accuracy of structural kd and the baselines on different amounts of unlabeled data in three cases.
the x-axis represents the amount of unlabeled data in thousand and the y-axis represents the accuracy.
the dashedlines are the accuracy of the teacher models.
the dotted lines are the accuracy of the baseline models without anyknowledge from the teachers..crf and ner-as-parsing, predicting labels fromthe marginal distributions leads to lower accuracy.
this is to be expected because such predictions donot take into account correlations between adjacentlabels.
while predictions from marginal distribu-tions of the crf teacher still outperform maxent,those of the ner-as-parsing teacher clearly under-perform maxent.
this provides an explanation asto why struct.
kd in case 4 has equal or evenlower accuracy than the token kd baseline in case2a in table 3..6 related work.
6.1 structured prediction.
in this paper, we use sequence labeling and depen-dency parsing as two example structured predictiontasks.
in sequence labeling, a lot of work appliedthe linear-chain crf and achieved state-of-the-artperformance in various tasks (ma and hovy, 2016;akbik et al., 2018; liu et al., 2019b; yu et al., 2020;wei et al., 2020; wang et al., 2021a,b).
meanwhile,a lot of other work used the maxent layer insteadof the crf for sequence labeling (devlin et al.,2019; conneau et al., 2020; wang et al., 2020b)because maxent makes it easier to ﬁne-tune pre-trained contextual embeddings in training.
anotheradvantage of maxent in comparison with crf isits speed.
yang et al.
(2018) showed that modelsequipped with the crf are about two times slowerthan models with the maxent layer in sequence la-beling.
in dependency parsing, recent work showsthat second-order crf parsers achieve signiﬁcantlyhigher accuracy than ﬁrst-order parsers (wanget al., 2019; zhang et al., 2020).
however, theinference speed of second-order parsers is muchslower.
zhang et al.
(2020) showed that second-order parsing is four times slower than the sim-ple head-selection ﬁrst-order approach (dozat and.
manning, 2017).
such speed-accuracy tradeoff asseen in sequence labeling and dependency pars-ing also occurs in many other structured predictiontasks.
this makes kd an interesting and very use-ful technique that can be used to circumvent thistradeoff to some extent..6.2 knowledge distillation in structured.
prediction.
kd has been applied in many structured predic-tion tasks in the ﬁelds of nlp, speech recognitionand computer vision, with applications such as neu-ral machine translation (kim and rush, 2016; tanet al., 2019), sequence labeling (tu and gimpel,2019; wang et al., 2020a), connectionist temporalclassiﬁcation (huang et al., 2018), image semanticsegmentation (liu et al., 2019a) and so on.
in kdfor structured prediction tasks, how to handle theexponential number of structured outputs is a mainchallenge.
to address this difﬁcult problem, recentwork resorts to approximation of the kd objective.
kim and rush (2016) proposed sequence-level dis-tillation through predicting k-best sequences ofthe teacher in neural machine translation.
kun-coro et al.
(2016) proposed to use multiple greedyparsers as teachers and generate the probability dis-tribution at each position through voting.
very re-cently, wang et al.
(2020a) proposed structure-levelknowledge distillation for linear-chain crf modelsin multilingual sequence labeling.
during the dis-tillation process, teacher models predict the top-klabel sequences as the global structure informationor the posterior label distribution at each positionas the local structural information, which is thenused to train the student.
besides approximate ap-proaches, an alternative way is using models thatmake local decisions and performing kd on theselocal decisions.
anderson and g´omez-rodr´ıguez(2020) formulated dependency parsing as a head-.
557selection problem and distilled the distribution ofthe head node at each position.
tsai et al.
(2019)proposed minibert through distilling the outputdistributions of m-bert models of the maxentclassiﬁer.
besides the output distribution, mukher-jee and hassan awadallah (2020) further distilledthe hidden representations of teachers..7 conclusion.
in this paper, we propose structural knowledge dis-tillation, which transfers knowledge between struc-tured prediction models.
we derive a factorizedform of the structural kd objective and make ittractable to compute and optimize for many typicalchoices of teacher and student models.
we applyour approach to four kd scenarios with six casesfor sequence labeling and dependency parsing.
em-pirical results show that our approach outperformsbaselines without kd as well as previous kd ap-proaches.
with sufﬁcient unlabeled data, our ap-proach can even boost the students to outperformthe teachers in zero-shot cross-lingual transfer..acknowledgmentsthis work was supported by the national natu-ral science foundation of china (61976139) andby alibaba group through alibaba innovative re-search program..references.
alan akbik, duncan blythe, and roland vollgraf.
2018. contextual string embeddings for sequencein proceedings of the 27th internationallabeling.
conference on computational linguistics, pages1638–1649, santa fe, new mexico, usa.
associ-ation for computational linguistics..mark anderson and carlos g´omez-rodr´ıguez.
2020.distilling neural networks for greener and faster de-in proceedings of the 16th in-pendency parsing.
ternational conference on parsing technologies andthe iwpt 2020 shared task on parsing into en-hanced universal dependencies, pages 2–13, on-line.
association for computational linguistics..jimmy ba and rich caruana.
2014. do deep nets reallyneed to be deep?
in z. ghahramani, m. welling,c. cortes, n. d. lawrence, and k. q. weinberger,editors, advances in neural information processingsystems 27, pages 2654–2662.
curran associates,inc..cristian buciluˇa, rich caruana,.
and alexandruniculescu-mizil.
2006. model compression.
in pro-ceedings of the 12th acm sigkdd internationalconference on knowledge discovery and data min-ing, kdd ’06, pages 535–541, new york, ny, usa.
acm..kevin clark, minh-thang luong, urvashi khandel-wal, christopher d. manning, and quoc v. le.
2019.bam!
born-again multi-task networks for naturallanguage understanding.
in proceedings of the 57thannual meeting of the association for computa-tional linguistics, pages 5931–5937, florence, italy.
association for computational linguistics..alexis conneau, kartikay khandelwal, naman goyal,vishrav chaudhary, guillaume wenzek, franciscoguzm´an, edouard grave, myle ott, luke zettle-moyer, and veselin stoyanov.
2020. unsupervisedincross-lingual representation learning at scale.
proceedings of the 58th annual meeting of the asso-ciation for computational linguistics, pages 8440–8451, online.
association for computational lin-guistics..jacob devlin, ming-wei chang, kenton lee, andkristina toutanova.
2019. bert: pre-training ofdeep bidirectional transformers for language under-in proceedings of the 2019 conferencestanding.
of the north american chapter of the associationfor computational linguistics: human languagetechnologies, volume 1 (long and short papers),pages 4171–4186, minneapolis, minnesota.
associ-ation for computational linguistics..timothy dozat and christopher d manning.
2017.deep biafﬁne attention for neural dependency pars-ing.
in international conference on learning rep-resentations..timothy dozat, peng qi, and christopher d. manning.
2017. stanford’s graph-based neural dependencyparser at the conll 2017 shared task.
in proceed-ings of the conll 2017 shared task: multilingualparsing from raw text to universal dependencies,pages 20–30, vancouver, canada.
association forcomputational linguistics..rotem dror, segev shlomov, and roi reichart.
2019.deep dominance - how to properly compare deepneural models.
in proceedings of the 57th annualmeeting of the association for computational lin-guistics, pages 2773–2785, florence, italy.
associa-tion for computational linguistics..geoffrey hinton, oriol vinyals, and jeffrey dean.
2015. distilling the knowledge in a neural network.
in nips deep learning and representation learn-ing workshop..piotr bojanowski, edouard grave, armand joulin, andtomas mikolov.
2017. enriching word vectors withsubword information.
transactions of the associa-tion for computational linguistics, 5:135–146..mingkun huang, yongbin you, zhehuai chen, yanminqian, and kai yu.
2018. knowledge distillation forsequence model.
in proc.
interspeech 2018, pages3703–3707..558yoon kim and alexander m. rush.
2016. sequence-level knowledge distillation.
in proceedings of the2016 conference on empirical methods in natu-ral language processing, pages 1317–1327, austin,texas.
association for computational linguistics..adhiguna kuncoro, miguel ballesteros, lingpengkong, chris dyer, and noah a. smith.
2016. distill-ing an ensemble of greedy dependency parsers intoone mst parser.
in proceedings of the 2016 con-ference on empirical methods in natural languageprocessing, pages 1744–1753, austin, texas.
asso-ciation for computational linguistics..yifan liu, ke chen, chris liu, zengchang qin,zhenbo luo, and jingdong wang.
2019a.
struc-tured knowledge distillation for semantic segmen-in 2019 ieee/cvf conference on com-tation.
puter vision and pattern recognition (cvpr), pages2599–2608.
ieee..yijin liu, fandong meng, jinchao zhang, jinan xu,yufeng chen, and jie zhou.
2019b.
gcdt: a globalcontext enhanced deep transition architecture for se-quence labeling.
in proceedings of the 57th annualmeeting of the association for computational lin-guistics, pages 2431–2441, florence, italy.
associa-tion for computational linguistics..ilya loshchilov and frank hutter.
2018. decoupledin international con-.
weight decay regularization.
ference on learning representations..xuezhe ma and eduard hovy.
2016..end-to-endsequence labeling via bi-directional lstm-cnns-crf.
in proceedings of the 54th annual meeting ofthe association for computational linguistics (vol-ume 1: long papers), pages 1064–1074, berlin, ger-many.
association for computational linguistics..xuezhe ma, zecong hu, jingzhou liu, nanyun peng,graham neubig, and eduard hovy.
2018. stack-in pro-pointer networks for dependency parsing.
ceedings of the 56th annual meeting of the associa-tion for computational linguistics (volume 1: longpapers), pages 1403–1414, melbourne, australia.
association for computational linguistics..subhabrata mukherjee and ahmed hassan awadallah.
2020. xtremedistil: multi-stage distillation for mas-in proceedings of thesive multilingual models.
58th annual meeting of the association for compu-tational linguistics, pages 2221–2234, online.
as-sociation for computational linguistics..xiaoman pan, boliang zhang, jonathan may, joelnothman, kevin knight, and heng ji.
2017. cross-lingual name tagging and linking for 282 languages.
in proceedings of the 55th annual meeting of theassociation for computational linguistics (volume1: long papers), pages 1946–1958, vancouver,canada.
association for computational linguistics..telmo pires, eva schlinger, and dan garrette.
2019.in pro-.
how multilingual is multilingual bert?.
ceedings of the 57th annual meeting of the asso-ciation for computational linguistics, pages 4996–5001, florence, italy.
association for computa-tional linguistics..victor sanh, lysandre debut, julien chaumond, andthomas wolf.
2019. distilbert, a distilled versionof bert: smaller, faster, cheaper and lighter.
the5th workshop on energy efﬁcient machine learningand cognitive computing - neurips 2019..cicero d santos and bianca zadrozny.
2014. learningcharacter-level representations for part-of-speechin proceedings of the 31st internationaltagging.
conference on machine learning (icml-14), pages1818–1826..michalina strzyz, david vilares, and carlos g´omez-rodr´ıguez.
2019. viable dependency parsing as se-in proceedings of the 2019 con-quence labeling.
ference of the north american chapter of the asso-ciation for computational linguistics: human lan-guage technologies, volume 1 (long and short pa-pers), pages 717–723, minneapolis, minnesota.
as-sociation for computational linguistics..xu tan, yi ren, di he, tao qin, and tie-yan liu.
2019. multilingual neural machine translation withknowledge distillation.
in international conferenceon learning representations..raphael tang, yao lu, linqing liu, lili mou, olgavechtomova, and jimmy lin.
2019. distilling task-speciﬁc knowledge from bert into simple neural net-works.
arxiv preprint arxiv:1903.12136..erik f. tjong kim sang.
2002..introduction to theconll-2002 shared task: language-independentin coling-02: thenamed entity recognition.
6th conference on natural language learning 2002(conll-2002)..erik f. tjong kim sang and fien de meulder.
2003. introduction to the conll-2003 shared task:language-independent named entity recognition.
inproceedings of the seventh conference on natu-ral language learning at hlt-naacl 2003, pages142–147..henry tsai, jason riesa, melvin johnson, naveen ari-vazhagan, xin li, and amelia archer.
2019. smalland practical bert models for sequence labeling.
in proceedings of the 2019 conference on empiricalmethods in natural language processing and the9th international joint conference on natural lan-guage processing (emnlp-ijcnlp), pages 3632–3636, hong kong, china.
association for computa-tional linguistics..lifu tu and kevin gimpel.
2019. benchmarking ap-proximate inference methods for neural structuredprediction.
in proceedings of the 2019 conferenceof the north american chapter of the associationfor computational linguistics: human languagetechnologies, volume 1 (long and short papers),.
559for computational linguistics and the 10th interna-tional joint conference on natural language pro-cessing, pages 93–99, suzhou, china.
associationfor computational linguistics..zhenkai wei, yu hong, bowei zou, meng cheng, andjianmin yao.
2020. don’t eclipse your arts due tosmall discrepancies: boundary repositioning within pro-a pointer network for aspect extraction.
ceedings of the 58th annual meeting of the asso-ciation for computational linguistics, pages 3678–3684, online.
association for computational lin-guistics..shijie wu and mark dredze.
2019. beto, bentz, be-cas: the surprising cross-lingual effectiveness ofbert.
in proceedings of the 2019 conference onempirical methods in natural language processingand the 9th international joint conference on natu-ral language processing (emnlp-ijcnlp), pages833–844, hong kong, china.
association for com-putational linguistics..jie yang, shuailong liang, and yue zhang.
2018. de-sign challenges and misconceptions in neural se-in proceedings of the 27th inter-quence labeling.
national conference on computational linguistics,pages 3879–3889, santa fe, new mexico, usa.
as-sociation for computational linguistics..juntao yu, bernd bohnet, and massimo poesio.
2020.named entity recognition as dependency parsing.
inproceedings of the 58th annual meeting of the asso-ciation for computational linguistics, pages 6470–6476, online.
association for computational lin-guistics..yu zhang, zhenghua li, and min zhang.
2020. efﬁ-cient second-order treecrf for neural dependencyin proceedings of the 58th annual meet-parsing.
ing of the association for computational linguistics,pages 3295–3305, online.
association for computa-tional linguistics..pages 3313–3324, minneapolis, minnesota.
associ-ation for computational linguistics..xinyu wang, jingxian huang, and kewei tu.
2019.second-order semantic dependency parsing withend-to-end neural networks.
in proceedings of the57th annual meeting of the association for com-putational linguistics, pages 4609–4618, florence,italy.
association for computational linguistics..xinyu wang, yong jiang, nguyen bach, tao wang,fei huang, and kewei tu.
2020a.
structure-levelknowledge distillation for multilingual sequence la-in proceedings of the 58th annual meet-beling.
ing of the association for computational linguistics,pages 3317–3330, online.
association for computa-tional linguistics..xinyu wang, yong jiang, nguyen bach, tao wang,zhongqiang huang, fei huang, and kewei tu.
2020b.
ain: fast and accurate sequence labelingin proceed-with approximate inference network.
ings of the 2020 conference on empirical methodsin natural language processing (emnlp), pages6019–6026, online.
association for computationallinguistics..xinyu wang, yong jiang, nguyen bach, tao wang,zhongqiang huang, fei huang, and kewei tu.
2021a.
automated concatenation of embeddingsin the joint confer-for structured prediction.
ence of the 59th annual meeting of the associationfor computational linguistics and the 11th interna-tional joint conference on natural language pro-cessing (acl-ijcnlp 2021).
association for com-putational linguistics..xinyu wang, yong jiang, nguyen bach, tao wang,zhongqiang huang, fei huang, and kewei tu.
2021b.
improving named entity recognition by ex-ternal context retrieving and cooperative learning.
in the joint conference of the 59th annual meet-ing of the association for computational linguisticsand the 11th international joint conference on natu-ral language processing (acl-ijcnlp 2021).
as-sociation for computational linguistics..xinyu wang, yong jiang, nguyen bach, tao wang,huang zhongqiang, fei huang, and kewei tu.
2020c.
more embeddings, better sequence labelers?
in findings of emnlp, online..xinyu wang, yong jiang, and kewei tu.
2020d.
en-hanced universal dependency parsing with second-order inference and mixture of training data.
in pro-ceedings of the 16th international conference onparsing technologies and the iwpt 2020 sharedtask on parsing into enhanced universal dependen-cies, pages 215–220, online.
association for com-putational linguistics..xinyu wang and kewei tu.
2020. second-order neuraldependency parsing with message passing and end-in proceedings of the 1st confer-to-end training.
ence of the asia-paciﬁc chapter of the association.
560a dynamic programming for case 4.we describe how the marginal distribution overbioes labels at each position of the input sen-tence can be tractably computed based on the ner-as-parsing teacher model using dynamic program-ming..given an input sentence x with n words, we ﬁrst.
deﬁne the following functions..•.
•.
•.
•.
−→dp(i, l) represents the summation of scoresof all possible labeling sequences of the sub-sentence from the ﬁrst token to the i-th tokenwhile a span ends with the i-th token with alabel l.−→dp(i, f) represents the summation of scoresof all possible labeling sequences of the sub-sentence from the ﬁrst token to the i-th tokenwhile there is no arc pointing to the i-th token.
←−dp(i, l) represents the summation of scoresof all possible labeling sequences of the sub-sentence from the i-th toke to the last tokenwhile a span starts with the i-th token with alabel l.←−dp(i, f) represents the summation of scoresof all possible labeling sequences of the sub-sentence from the i-th toke to the last tokenwhile there is no arc coming from the i-thtoken..we can compute the values of these functions forall values of i and l using dynamic programming.
the base cases are:.
where.
−→dp(1, f) = 1.
←−dp(n, f) = 1.the recursive formulation of these functions are:.
−→dp(i, l) =.
i(cid:88).
exp(score(yk,i = l)) ∗.
−→dp(k, f).
−→dp(i, f) =.
k=1−→dp(i − 1, f) +.
(cid:88).
−→dp(i − 1, l).
←−dp(i, l) =.
n(cid:88).
exp(score(yi,j = l)) ∗.
←−dp(j, f).
←−dp(i, f) =.
j=i←−dp(i + 1, f) +.
(cid:88).
←−dp(i + 1, l).
l∈l.
l∈l.
where score(yi,j = l) is the score assigned by theteacher model to the dependency arc from i to j.with label l. after dynamic programming, we cancompute the substructure marginals of the teacherpt(yi|x) as follows:.
pt(yi = bl|x) = dp(bl, i)/z(x)n(cid:88).
−→dp(i, f) ∗.
=.
exp(score(yi,j = l)).
j=i+1.
←−dp(j, f)/z(x).
∗.
pt(yi = il|x) = dp(il, i)/z(x).
i−1(cid:88).
n(cid:88).
=.
exp(score(yk,j = l)) ∗.
−→dp(k, f).
j=i+1.
k=1←−dp(j, f)/z(x).
∗.
pt(yi = el|x) = dp(el, i)/z(x)i−1(cid:88).
←−dp(i, f) ∗.
exp(score(yk,i = l)).
=.
k=1.
−→dp(k, f)/z(x).
∗.
pt(yi = o|x) = dp(o, i)/z(x)←−dp(i, f)/z(x).
−→dp(i, f) ∗.
=.
pt(yi = sl|x) = dp(sl, i)/z(x).
−→dp(i, f)∗ exp(score(yi,i = l))∗.
←−dp(i, f)/z(x).
=.
• dp(x, i) represents the summation of scoresof all possible labeling sequences in which thei-th token is labeled as x. x can be one of‘bl, il, el, o, sl’..• z(x) represents the summation of scores ofall possible labeling sequences given the inputsentence x. yi,j = l represents that there is adependency arc of label ‘l’ from the i-th wordto the j-th word.
we can calculate z(x) by←−−→dp(1, f )dp(n, l) +.
−→dp(n, f ) or.
←−dp(1, l) +.
the edge cases are:.
pt(yn = bl|x) = 0pt(y1 = il|x) = pt(yn = il|x) = 0pt(y1 = el|x) = 0.
561c.2 results of parsing task.
tabel 11 and 12 represent the results of experi-ments of parsing.
our structural kd approachessigniﬁcantly outperform the other approaches in allcases.
uas and las in these tables were depen-dency parsing metrics, and they refer to unlabeledattachment score and labeled attachment score re-spectively..teacherstudent.
speed (sentences/second)27.76672.20.
# param (m)233.409.46.table 7: running speed and model sizes of the teacherand student models in case 2a..b additional analysis.
b.1 comparison of speed and model size.
an important goal of kd is to produce faster andsmaller models.
in table 7, we show a comparisonon the running speed and model size between theteacher and student models on the conll englishtest set from case 2a.
it can be seen that the studentmodel is about 24 times faster and 25 times smallerthan the teacher model..c detailed experimental results.
in this section, we present detailed experimentalresults.
table 8, 9 and 10 show the results of nertask, while table 11 and 12 show the results ofparsing.
we evaluate the signiﬁcance based onalmost stochastic dominance (asd) (dror et al.,2019), which is a high quality comparison betweendeep neural networks.
we evaluate with a signiﬁ-cance level of 0.05. for the signiﬁcance test overaveraged scores, we averaged over the same ran-dom seed of each language as a sample of aver-aged score.
in tables, we use † to represent ourapproaches are signiﬁcantly stronger than the mod-els training without kd or with top-1 kd.
we use‡ to represent that our approaches are signiﬁcantlystronger than other kd approaches..c.1 results of ner task.
table 8, 9 and 10 represent the kd results of ex-periments with labeled and unlabeled datasets.
ourapproaches outperform the baselines signiﬁcantlyin most of the cases.
note that in some cases, ourapproaches perform slightly inferior to other ap-proaches (for example, de dataset in case 1a intable 9 with 30k unlabeled sentences) while ourapproaches are still stronger than these approachesaccording to the asd test.
the possible reasonis that the variances of our approaches are muchlarger than the other approaches and asd indicatesour approaches is possibly better than the otherapproaches..562datasetscenario.
teacherw/o kdpos.
kdstruct.
kdcrf teachermaxent teacherw/o kdtoken-level kdstruct.
kdteacherw/o kdstruct.
kd.
conlles89.2986.5587.33.wikiannes91.8585.8485.98.en83.8080.0981.76.de86.9880.1280.02.en92.2591.2191.38.nl91.5685.6785.92.avg.
89.1584.7085.27.nl91.4687.1987.15.de83.4875.3776.4676.61†‡ 91.41†‡ 87.20† 86.19†‡ 85.35†‡ 80.64†‡ 81.37† 87.29†‡ 87.19†‡ 84.12†‡83.4882.8374.4475.0875.41†‡ 91.04†‡ 86.25†‡ 85.28†‡ 84.50†‡ 78.49†‡ 79.48† 85.28†‡ 85.66†‡ 82.23†‡88.3882.3874.4480.8674.90† 91.21† 85.82† 85.20† 84.28† 78.66† 78.97† 83.83† 83.34† 81.45†.
avg.
88.5283.3183.73.
88.5287.4180.8682.09.
91.8590.8183.7384.92.
91.4690.3983.1985.50.
89.1588.6583.8784.25.
92.2592.0390.7890.95.
86.9885.9877.9878.40.
89.2988.4985.4285.88.
91.5691.2684.8385.10.
83.8082.4678.5279.52.
88.5783.87.
92.4190.78.
90.7284.83.
88.7785.42.
83.1178.52.
91.4183.73.
86.9677.98.
92.0583.19.case 1a.
case 2a.
case 4.table 8: results of f1 scores for ner task on labeled datasets.
datasetscenario.
teachertop-1pos.
kd + top-1struct.
kd + top-1top-1pos.
kd + top-1struct.
kd + top-1top-1pos.
kd + top-1struct.
kd + top-1teachertop-1token-level kd + top-1struct.
kd + top-1top-1token-level kd + top-1struct.
kd + top-1top-1token-level kd + top-1struct.
kd + top-1teachertop-1struct.
kd + top-1top-1struct.
kd + top-1top-1struct.
kd + top-1.
case 1a.
case 2a.
case 4.
# unlabeled sent..81.1982.56.
80.3281.53.
89.3689.53.
86.2486.51.
88.2388.99.
84.9085.45.
83.8078.4879.18.en83.8079.8581.40.nl91.4688.4488.55.avg88.5284.1984.91.wikiann with unlabeled datadees86.9891.8580.6687.7988.1081.5681.88†‡ 81.23† 88.66†‡ 89.20†‡ 85.24†‡88.7882.2782.0189.2882.34†‡ 81.27† 89.85†‡ 89.19†‡ 85.66†‡84.2090.2184.1289.8284.17†‡ 82.14† 90.41†‡ 89.84†‡ 86.64†‡91.8586.9885.5478.8279.8485.8979.82† 79.41†‡ 86.36†‡ 87.75†‡ 83.34†‡86.9380.7587.8280.7181.07†‡ 79.41†‡ 87.77†‡ 87.99†‡ 84.06†‡88.7882.4989.3282.3583.06†‡ 80.43†‡ 89.02† 88.62† 85.28†‡88.3892.0583.1191.4186.9685.8286.9482.1078.4177.2287.22† 82.44†78.80† 78.00† 85.7587.5183.1279.5987.8577.5383.38†80.04† 78.06† 88.03† 87.4081.4784.5888.8089.4681.85† 79.57† 89.55† 89.13† 85.03†.
91.4686.7787.36.
88.5282.4083.07.
84.8685.23.
87.3087.80.
83.3883.89.
79.4380.42.
78.5379.23.
88.7488.84.
78.59.
3k.
10k.
30k.
3k.
10k.
30k.
3k.
10k.
30k.
table 9: results of f1 scores for ner task on unlabeled datasets.
563case 3.
#unlabeled sent..teachertop-1token-level kd + top-1struct.
kd + top-1top-1token-level kd + top-1struct.
kd + top-1top-1token-level kd + top-1struct.
kd + top-1.
3k.
10k.
30k.
fa40.3037.8826.32.avg.
56.0141.1138.42.ta57.1443.4638.45.wikiannheeu58.6867.9241.3241.7752.6736.2253.69†‡ 42.02†‡ 42.75†‡ 42.66†‡ 45.28†‡43.3758.6358.8741.6262.50†‡ 39.72†‡ 46.22†‡ 58.27†‡ 51.68†‡55.1274.3770.9855.5075.66†‡ 38.08†‡ 58.52†‡ 64.69†‡ 59.24†‡.
48.4645.87.
57.1854.35.
34.6528.63.
63.7863.39.
57.2454.83.
35.7029.44.table 10: result of f1 scores of zero shot transfer experiment on ner task.
metric.
uas.
las.
teacherw/o kdstruct.
kdteacherw/o kdstruct.
kd.
case 1b case 2b.
ptb95.9691.7893.56†94.2489.8591.83†.
ptb96.0491.7893.56†94.2989.8591.78†.
table 11: result of f1 scores of parsing task with labeled dataset.
note that all our approaches are signiﬁcantlystronger than the baseline..case 1bptb with unlabeled data.
case 2bptb with unlabeled data.
metric.
uas.
las.
10k.
3k100k avg.
top-192.00 92.52 93.22 93.69 94.25 93.14 91.99 92.44 93.16 93.69 94.26 93.11struct.
kd + top-1 93.71† 93.93† 94.26† 94.58† 94.84† 94.26† 93.67† 93.90† 94.30† 94.64† 94.89† 94.28†top-190.03 90.62 91.44 91.99 92.61 91.34 90.03 90.59 91.41 91.98 92.66 91.33struct.
kd + top-1 91.98† 92.24† 92.63† 93.00† 93.28† 92.63† 91.94† 92.18† 92.66† 93.04† 93.31† 92.63†.
100k avg..50k.
50k.
30k.
10k.
30k.
3k.
table 12: the accuracy of parsing task with unlabeled dataset (in thousand).
note that all our approaches aresigniﬁcantly stronger than the baseline..564