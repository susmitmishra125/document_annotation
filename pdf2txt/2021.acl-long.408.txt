obtaining better static word embeddingsusing contextual embedding models.
prakhar guptaepfl, switzerlandprakhar.gupta@epfl.ch.
martin jaggiepfl, switzerlandmartin.jaggi@epfl.ch.
abstract.
the advent of contextual word embeddings—representations of words which incorporate se-mantic and syntactic information from theircontext—has led to tremendous improvementson a wide variety of nlp tasks.
however,recent contextual models have prohibitivelyhigh computational cost in many use-casesand are often hard to interpret.
in this work,we demonstrate that our proposed distilla-tion method, which is a simple extension ofcbow-based training, allows to signiﬁcantlyimprove computational efﬁciency of nlp ap-plications, while outperforming the qualityof existing static embeddings trained fromscratch as well as those distilled from previ-ously proposed methods.
as a side-effect, ourapproach also allows a fair comparison of bothcontextual and static embeddings via standardlexical evaluation tasks..1.introduction.
word embeddings—representations of wordswhich reﬂect semantic and syntactic informationcarried by them are ubiquitous in natural languageprocessing.
static word representation modelssuch as glove (pennington et al., 2014), cbow,skipgram (mikolov et al., 2013) and sent2vec(pagliardini et al., 2018) obtain stand-alone rep-resentations which do not depend on their sur-rounding words or sentences (context).
contex-tual embedding models (devlin et al., 2019; peterset al., 2018; liu et al., 2019; radford et al., 2019;schwenk and douze, 2017) on the other hand, em-bed the contextual information as well into theword representations making them more expressivethan static word representations in most use-cases.
while recent progress on contextual embeddingshas been tremendously impactful, static embed-dings still remain fundamentally important in manyscenarios as well:.
• even when ignoring the training phase, thecomputational cost of using static word em-beddings is typically tens of millions timeslower than using standard contextual embed-ding models1, which is particularly importantfor latency-critical applications and on low-resource devices, and in view of environmen-tal costs of nlp models (strubell et al., 2019)..• many nlp tasks inherently rely on static wordembeddings (shoemark et al., 2019), for ex-ample for interpretability, or e.g.
in researchin bias detection and removal (kaneko andbollegala, 2019; gonen and goldberg, 2019;manzini et al., 2019) and analyzing word vec-tor spaces (vulic et al., 2020) or other metricswhich are non-contextual by choice..• static word embeddings can complement con-textual word embeddings, for separating staticfrom contextual semantics (barsalou, 1982;rubio-fern´andez, 2008), or for improvingjoint embedding performance on downstreamtasks (alghanmi et al., 2020)..we also refer the reader to this article2 illustratingseveral down-sides of using bert-like models overstatic embedding models for non-specialist users.
indeed, we can see continued prevalence of staticword embeddings in industry and research areasincluding but not limited to medicine (zhang et al.,2019; karadeniz and ¨ozg¨ur, 2019; magna et al.,2020) and social sciences (rheault and cochrane,2020; gordon et al., 2020; farrell et al., 2020; lucyet al., 2020)..from a cognitive science point of view, humanlanguage has been hypothesized to have both con-.
1bert base (devlin et al., 2019) produces 768 dimen-sional word embeddings using 109m parameters, requiring29b flops per inference call (clark et al., 2020)..2do.
bert?
tedunderwood.com/2019/07/15/).
humanists.
need.
(https://.
proceedingsofthe59thannualmeetingoftheassociationforcomputationallinguisticsandthe11thinternationaljointconferenceonnaturallanguageprocessing,pages5241–5253august1–6,2021.©2021associationforcomputationallinguistics5241textual as well as context-independent properties(barsalou, 1982; rubio-fern´andez, 2008) underlin-ing the need for continued research in studying theexpressiveness context-independent embeddingson the level of words..most existing word embedding models, whetherstatic or contextual, follow firth (1957)’s famoushypothesis - “you shall know a word by the com-pany it keeps” , i.e., the meaning of a word arisesfrom its context.
during training existing staticword embedding models, representations of con-texts are generally approximated using averagingor sum of the constituent word embeddings, whichdisregards the relative word ordering as well asthe interplay of information beyond simple pairsof words, thus losing most contextual information.
ad-hoc remedies attempt to capture longer con-textual information per word using higher ordern-grams like bigrams or trigrams, and have beenshown to improve the performance of static wordembedding models (gupta et al., 2019; zhao et al.,2017).
however, these methods are not scalable tocover longer contexts..in this work, we obtain improved static wordembeddings by leveraging recent contextual em-bedding advances, namely by distilling existingcontextual embeddings into static ones.
our pro-posed distillation procedure is inspired by existingcbow-based static word embedding algorithms,but during training plugs in any existing contextualrepresentation to serve as the context element ofeach word..our resulting embeddings outperform the cur-rent static embedding methods, as well as the cur-rent state-of-the-art static embedding distillationmethod on both unsupervised lexical similaritytasks as well as on downstream supervised tasks,by a signiﬁcant margin.
the resulting static em-beddings remain compatible with the underlyingcontextual model used, and thus allow us to gaugethe extent of lexical information carried by staticvs contextual word embeddings.
we release ourcode and trained embeddings publicly on github3..formed by using these embeddings as rows and useit as a static embedding.
however, this method isnot scalable in terms of memory (the embeddingmatrix scaling with the number of contexts) andcomputational cost (pca)..bommasani et al.
(2020) propose two differentapproaches to obtain static embeddings from con-textual models..1. decontextualized static embeddings - theword w alone without any context, after tok-enization into constituents w1, .
.
.
, wn is fedto the contextual embedding model denotedby m and the resulting static embedding isgiven by g(m (w1), .
.
.
, m (wn)) where g isa pooling operation.
it is observed that theseembeddings perform dismally on the standardstatic word embedding evaluation tasks..2. aggregated static embeddings - since con-textual embedding models are not trained ona single word (without any context) as input,an alternative approach is to obtain the con-textual embedding of the word w in differentcontexts and then pool(max, min or average)the embeddings obtained from these differentcontexts.
they observe that average poolingleads to the best performance.
we refer tothis method (with average pooling) as asethroughout the rest of the paper.
as we seein our experiments, the performance of aseembeddings saturates quickly with increasingsize of the raw text corpus and is therefore notscalable..other related work includes distillation of con-textual word embeddings to obtain sentence em-beddings (reimers and gurevych, 2019).
we alsorefer the reader to mickus et al.
(2020) for a dis-cussion on the semantic properties of contextualmodels (primarily bert) as well as rogers et al.
(2020), a survey on different works exploring theinner workings of bert including its semanticproperties..2 related work.
3 proposed method.
a few methods for distilling static embeddingshave already been proposed.
ethayarajh (2019)propose using contextual embeddings of the sameword in a large number of different contexts.
theytake the ﬁrst principal component of the matrix.
3https://github.com/epfml/x2static.
to distill existing contextual word representationmodels into static word embeddings, we augmenta cbow-inspired static word-embedding methodas our anchor method to accommodate additionalcontextual information of the (contextual) teachermodel.
sent2vec (pagliardini et al., 2018) is a.
5242modiﬁcation of the cbow static word-embeddingmethod which instead of a ﬁxed-size context win-dow uses the entire sentence to predict the maskedword.
it also has the ability to learn n-gram rep-resentations along with unigram representations,allowing to better disentangle local contextual in-formation from the static unigram embeddings.
sent2vec, originally meant to obtain sentenceembeddings and later repurposed to obtain wordrepresentations (gupta et al., 2019) was shown tooutperform competing methods including glove(pennington et al., 2014), cbow, skipgram(mikolov et al., 2013) and fasttext (bojanowskiet al., 2016) on word similarity evaluations.
fora raw text corpus c (collection of sentences), thetraining objective is given by.
minu ,v.(cid:88).
(cid:88).
s∈c.
wt∈s.
f (uwt, ectx(s, wt)).
(1).
w(cid:48)∈n (cid:96)(−u(cid:62).
where f (u, v) := (cid:96)(u(cid:62)v) + (cid:80)w(cid:48)v).
here, wt is the masked target word, u and v arethe target word embedding and the source n-grammatrices respectively, n is the set of negative targetsamples and, (cid:96) : x (cid:55)→ log (1 + e−x) is the logisticloss function..for sent2vec, the context encoder ectx usedin optimizing (1) is simply given by the (static,non-contextual) sum of all vectors in the sentencewithout the target word,.
ectx(s, wt) :=.
1|r(s\{wt})|.
(cid:88).
vw ,.
(2).
w∈r(s\{wt}).
where r(s) denotes the optional expansion of thesentence s from words to short n-grams, i.e., thecontext sentence embedding is obtained by aver-aging the embeddings of word n-grams in the sen-tence s..we will now generalize the objective (1) by al-lowing the use of arbitrary modern contextual rep-resentations ectx instead of the static context repre-sentation as in (2).
this key element will allow usto translate quality gains from improved contextualrepresentations also to better static word embed-ding in the resulting matrix u .
we propose twodifferent approaches of doing so, which differ inthe granularity of context used for obtaining thecontextual embeddings..target word) allows for a more reﬁned representa-tion of the context, and to take in account the wordorder as well as the interplay of information amongthe words of the context..more formally, let m (s, w) denote the outputof a contextual embedding-encoder, e.g.
bert,corresponding to the word w when a piece of text scontaining w is fed to it as input.
we let ectx(s, w)to be the average of all contextual embeddings ofwords w returned by the encoder,.
ectx(s, wt) := 1|s|.
m (s, w).
(3).
(cid:88).
w∈s.
this allows for a more reﬁned representation of thecontext as the previous representation did not takein account neither the word order nor the interplayof information among the words of the context.
cer-tainly, using smwt(s with wt masked) and w wouldmake for an even better word-context pair butthat would amount to one contextual embedding-encoder inference per word instead of one inferenceper sentence as is the case in (3) leading to a drasticdrop in computational efﬁciency..3.2 approach 2 - paragraphs as context.
since contextual models are trained on large piecesof texts (generally ≥ 512 tokens), we instead useparagraphs instead of sentences to obtain the con-textual representations.
however, in order to pre-dict target words, we use the contextual embed-dings within the sentence only.
consequently, forthis approach, we have.
ectx(s, wt) := 1|s|.
m (ps, w),.
(4).
(cid:88).
w∈s.
where ps is the paragraph containing sentence s.in the transfer phase, this approach is more com-putationally efﬁcient than the previous approach,as we have to invoke the contextual embeddingmodel m only once for each paragraph as opposedto once for every constituent sentence.
moreover,it encapsulates the related semantic information inparagraphs in the contextual word embeddings..we call our models x2staticsent in the sen-tence case (3), and x2staticpara in the paragraphcase (4) respectively where x denotes the parentmodel..4 experiments and discussion.
3.1 approach 1 - sentences as context.
4.1 corpus preprocessing and training.
using contextual representations of all words in thesentence s (or the sentence s \ {wt} without the.
we use the same english wikipedia dump aspagliardini et al.
(2018); gupta et al.
(2019) to.
5243epoch(s)trained.
maxvocab.
size.
target wordsubsamplinghyperparameter.
minimumword count.
initiallearningrate.
batchsize.
1.
750000.
5e-6.
10.
0.001.
128.numberofnegativessampled10.table 1: training hyperparameters used for training x2static models.
model.
epoch(s)trained.
maxvocab.
size.
numberofnegativessampled.
target wordsubsamplinghyperparameter.
min.
wordcount.
initiallearningrate.
wordn-grams.
charactern-grams.
windowsize.
sent2vec {5,10,15} 750000 {5,8,10} {1e-4, 5e-6, 1e-5, 5e-6}{5,8,10} {1e-4, 5e-6, 1e-5, 5e-6}skipgram {5,10,15} n.a.
{5,8,10} {1e-4, 5e-6, 1e-5, 5e-6}cbow {5,10,15} n.a..101010.
0.20.050.05.
{1,2,3}n.a.
n.a..n.a..n.a..{n.a.,3-6} {2,5,10}{n.a.,3-6} {2,5,10}.
table 2: hyperparameter search space description for the training of sent2vec, skipgram and cbowmodels: best hyperparameters for the chosen model in our experiments are shown in bold.
n.a.
indicates notapplicable..generate distilled x2static representations.
asour corpus for training static word embedding base-lines as well as for distilling static word embed-dings from pre-trained contextual embedding mod-els.
we remove all paragraphs with less than 3sentences or 140 characters, lowercase the char-acters and tokenize the corpus using the stanfordnlp library (manning et al., 2014) resulting in acorpus of approximately 54 million sentences and1.28 billion words.
we then use the transform-ers library4 (wolf et al., 2020) to generate repre-sentations from existing transformer models.
ourx2static representations are distilled from thelast representation layers of these models..we use the same hyperparameter set for train-ing all x2static models, i.e., no hyperparametertuning is done at all.
we use 12-layer as well as 24-layer pre-trained models using bert (devlin et al.,2019), roberta (liu et al., 2019) and gpt2(radford et al., 2019) architectures as the teachermodel to obtain x2static word embeddings.
allthe x2static models use the same set of train-ing parameters except the parent model.
traininghyperparameters are provided in table 1. the dis-tillation/training process employs the lazy versionof the adam optimizer (kingma and ba, 2015a),suitable for sparse tensors.
we use a subsam-pling parameter similar to fasttext (bojanowskiet al., 2016) in order to subsample frequent targetwords during training.
each x2static model wastrained using a single v100 32 gb gpu.
obtainingx2static embeddings from 12-layer contextualembedding models took 15-18 hours while it took.
35-38 hours to obtain them from their 24-layercounterparts..to ensure a fair comparison, we also evaluatesent2vec, cbow and skipgram models thatwere trained on the same corpus.
we do an exten-sive hyperparameter tuning for these models andchoose the one which shows best average perfor-mance on the 5 word similarity datasets used insubsection 4.2. these hyperparameter sets can beaccessed in table 2 where the chosen hyperparam-eters are shown in bold.
we set the number of di-mensions to be 768 to ensure parity between themand the x2static models compared.
we usedthe sent2vec library5 for training sent2vecand the fasttext library6 for training cbowand skipgram models.
we also evaluate somepre-trained 300 dimensional glove (penningtonet al., 2014) and fasttext (bojanowski et al.,2016) models in table 3. the glove model wastrained on common-crawl corpus of 840 billiontokens (approximately 650 times larger than ourcorpus) while the fasttext vectors were trainedon a corpus of 16 billion tokens (approximately12 times larger than our corpus)).
we also extractase embeddings from each layer using the samewikipedia corpus..we perform two different sets of evaluations.
the ﬁrst set corresponds to unsupervised word sim-ilarity evaluations to gauge the quality of the ob-tained word embeddings.
however, we recognizethat there are concerns regarding word-similarity.
5https://github.com/epfml/sent2vec6https://github.com/facebookresearch/.
4https://huggingface.co/transformers/.
fasttext/.
5244evaluation tasks (faruqui et al., 2016) as they areshown to exhibit signiﬁcant difference in perfor-mance when subjected to hyperparameter tuning(levy et al., 2015).
to address these limitationsin the evaluation, we also evaluate the x2staticembeddings on a standard set of downstream su-pervised evaluation tasks used in pagliardini et al.
(2018)..4.2 unsupervised word similarity evaluation.
to assess the quality of the lexical information con-tained in the obtained word representations, weuse the 4 word-similarity datasets used by (bom-masani et al., 2020), namely wordsim353 (353word-pairs) (agirre et al., 2009) dataset; simlex-999 (999 word-pairs) (hill et al., 2014) dataset;rg-65 (65 pairs) (joubarne and inkpen, 2011);and simverb-3500 (3500 pairs) (gerz et al., 2016)dataset as well as the rare words rw-2034 (2034pairs) (luong et al., 2013) dataset.
to calculatethe similarity between two words, we use the co-sine similarity between their word embeddings.
these similarity scores are compared to the hu-man ratings using spearman’s ρ (spearman, 1904)correlation scores.
we use the tool7 provided bybommasani et al.
(2020) to report these resultson ase embeddings.
it takes around 3 days toobtain ase representations of the 2005 words inthese word-similarity datasets for 12-layer modelsand around 5 days to obtain them for their 24-layercounterparts on the same machine used for learningx2static representations.
all other embeddingsare evaluated using the muse repository evalua-tion tool8 (lample et al., 2018)..we perform two sets of experiments concerningthe unsupervised evaluation tasks.
the ﬁrst setis the comparison of our x2static models withcompeting models.
for ase, we report two sets ofresults, one which per task reports the best resultamongst all the layers and other, which reports theresults obtained on the best performing layer onaverage..we report our observations in table 3. we pro-vide additional results for larger models in ap-pendix b. we observe that x2static embeddingsoutperform competing models on most of the tasks.
moreover, the extent of improvement on simlex-999 and simverb-3500 tasks compared to the pre-.
7https://github.com/rishibommasani/.
contextual2static.
8https://github.com/facebookresearch/.
muse.
vious models strongly highlights the advantage ofusing improved context representations for trainingstatic word representations..second, we study the performance of the bestase embedding layer with respect to the size ofcorpus used.
bommasani et al.
(2020) report theirresults on a corpus size of only up to n = 100, 000sentences.
in order to measure the full potentialof the ase method, we obtain different sets ofase embeddings as well as x2staticpara embed-dings from small chunks of the corpus to the fullwikipedia corpus itself and compare their perfor-mance on simlex-999 and rw-2034 datasets.
wechoose simlex-999 as it captures true similarity in-stead of relatedness or association (hill et al., 2014)and rw-2034 to gauge the robustness of the embed-ding model on rare words.
we report our observa-tions in figure 1. we observe that the performanceof the ase embeddings tends to saturate with theincrease in the corpus size while x2staticparaembeddings are either signiﬁcantly outperformingthe ase embeddings or still show a signiﬁcantlygreater positive growth rate in performance w.r.t.
the corpus size.
thus, the experimental evidencesuggests that on larger texts, x2static embed-dings will have an even better performance andhence, x2static is a better alternative than aseembeddings from any of the layers of the contex-tual embedding model, and obtains improved staticword embeddings from contextual embedding mod-els..4.3 downstream supervised evaluation.
we evaluate the obtained word embeddings on var-ious sentence-level supervised classiﬁcation tasks.
six different downstream supervised evaluationtasks namely classiﬁcation of movie review sen-timent(mr) (pang and lee, 2005), product re-views(cr) (hu and liu, 2004), subjectivity classi-ﬁcation(subj) (pang and lee, 2004), opinion po-larity (mpqa) (wiebe et al., 2005), question typeclassiﬁcation (trec) (voorhees, 2002) and ﬁne-grained sentiment analysis (sst-5) (socher et al.,2013) are employed to gauge the performance ofthe obtained word embeddings..we use a standard cnn based architecture onthe top of our embeddings to train our classiﬁer.
we use 100 convolutional ﬁlters with a kernel sizeof 3 followed by a relu activation function.
aglobal max-pooling layer follows the convolutionlayer.
before feeding the max-pooled output to a.
5245300300.
768768768.
768768.
768768.
768768.
768768.
768768.
768768.parent model \other details.
size of thetraining corpusrelative to ours.
model \distilled model.
existing pre-trained models.
models trained by us.
fasttextglove.
skipgramcbowsent2vec.
12x650x.
n.a.
n.a.
n.a..models distilled by us.
parent model.
ase - best layer per taskase - best overall layer.
bert-12bert-12.
bert2staticsentbert2staticpara.
bert-12bert-12.
dim.
rg-65 ws-353.
sl-999.
sv-3500 rw-2034 average.
0.76690.6442.
0.5960.5791.
0.4160.3764.
0.32740.2625.
0.52260.4607.
0.52760.4646.
0.82590.83480.7811.
0.71410.49990.7407.
0.40640.40970.5034.
0.27220.26260.3297.
0.48490.40430.4248.
0.54070.48230.55594.
0.7449(1) 0.7012(1) 0.5216(4) 0.4151(5) 0.4577(5) 0.5429(3)0.6948(3) 0.6768(3) 0.5195(3) 0.3889(3) 0.4343(3) 0.5429(3).
0.74210.7555.
0.72970.7598.
0.54610.5384.
0.44370.4317.
0.54690.5299.
0.60170.6031.ase - best layer per taskase - best overall layer.
roberta-12roberta-12.
0.673(0) 0.7023(0) 0.554(5) 0.4602(4) 0.5075(3) 0.5600(0)0.673(0) 0.7023(0) 0.5167(0) 0.4424(0) 0.4657(0) 0.5600(0).
roberta2staticsentroberta2staticpara.
roberta-12roberta-12.
0.79990.8057.
0.74520.7638.
0.55070.5544.
0.46580.4717.
0.54960.5501.
0.62220.6291.ase - best layer per task gpt2-12ase - best overall layergpt2-12.
gpt22staticsentgpt22staticpara.
gpt2-12gpt2-12.
0.7013(1) 0.6879(0) 0.4972(2) 0.3905(2) 0.4556(2) 0.5365(2)0.6833(2) 0.6560(2) 0.4972(2) 0.3905(2) 0.4556(2) 0.5365(2).
0.74840.7881.
0.71510.7267.
0.53970.5417.
0.46760.4733.
0.57600.5668.
0.60940.6193.table 3: comparison of the performance of different embedding methods on word similarity tasks.
modelsare compared using spearman correlation for word similarity tasks.
all x2static method performances whichimprove over all ase methods on their parent model as well as all static models are shown in bold.
best perfor-mance in each task is underlined.
for all ase methods, the number in parentheses for each dataset indicates whichlayer was used for obtaining the static embeddings..classiﬁer, it is passed through a dropout layer withdropout probability of 0.5 to prevent overﬁtting.
we use adam (kingma and ba, 2015b) to train ourclassiﬁer.
to put the performance of these staticmodels into a broader perspective, we also ﬁne-tunelinear classiﬁers on the top of their parent mod-els as well as sentence-transformers (reimers andgurevych, 2019) obtained from roberta-12 andbert-12.
for the sentence-transformer models,we use the sentence-transformer models obtainedby ﬁne-tuning their parent models on the naturallanguage inference(nli) task using the combina-tion of stanford nli (bowman et al., 2015) and themulti-genre nli (williams et al., 2018) datasets.
the models are refered to as sbert-base-nliand sroberta-base-nli in the rest of the pa-per..the hyperparameter search space for the ﬁne-tuning process involves the number of epochs (8-.
16) and the learning rates[1e-4,3e-4,1e-3].
wher-ever train, validation, and test split is not given, weuse 60% of the data as the training data, 20% of thedata as validation data and the rest as the test data.
after obtaining the best hyperparameters, we trainon the train and validation data together with thesehyperparameters and predict the results on the testset.
for the linear classiﬁers on the top of parentmodels, we set the number of epochs and learningrate search space for parent model + linear classiﬁercombination to be [3,4,5,6] and [2e-5,5e-5] respec-tively.
the learning rates in the learning rate searchspace are lower than those for static embeddings asthe contextual embeddings are also ﬁne-tuned andfollow the recommendation of devlin et al.
(2019).
for the sentence-transformer models, we only trainthe linear classiﬁer and set the number of epochsand learning rate search space to be [3,4,5,6] and[1e-4,3e-4,1e-3] respectively.
we use cross-entropy.
5246figure 1: effect of corpus size on the word-embedding quality for ase best task independent layer andx2staticpara : in the legend, parent model is indicated in subscript..loss for training all the models.
we use macro-f1score and accuracy to gauge the quality of ourpredictions.
we compare x2static models withall other static models trained from scratch on thesame corpus as well as the glove and fasttextmodels used in the previous section.
we also useexisting glove embeddings trained on tweets(27billion tokens - 20 times larger than our corpus)(pennington et al., 2014) to make the comparisoneven more extensive.
we report our observations intable 4. for ase embeddings, we take the layerwith best average macro-f1 performance..we.
observe.
that when measuring.
theoverall performance, with the exception ofroberta2staticsent which has similar av-.
erage f-1 score to ase owing to its dismalperformance on the cr task, all x2staticembeddings outperform their competitors by asigniﬁcant margin.
even though the gloveand fasttext embeddings were trained oncorpora of one to two magnitudes larger andhave a larger vocabulary, their performance lagsbehind that of the x2static embeddings.
toensure statistical soundness, we measure meanand standard deviation of the performance on 6runs of x2staticpara model training followed bydownstream evaluation along with 6 runs of aseembedding downstream evaluation with differentrandom seeds in table 5 in the appendix.
we seethat x2staticpara embeddings outperform ase.
5247102101100fraction of the full wikipedia dataset used0.350.400.450.500.55spearman0s  performance of the models on simlex-999asebert12aseroberta12asegpt212bert2staticpararoberta2staticparagpt22staticpara101100fraction of the full wikipedia dataset used0.250.300.350.400.450.500.55spearman0s  performance of the models on rw-2034asebert12aseroberta12asegpt212bert2staticpararoberta2staticparagpt22staticparaexisting pre-trained models.
gloveglove (twitter)fasttext.
models trained by us.
skipgramcbowsent2vec.
models distilled by us.
ase - bert-12 (5)bert2staticsentbert2staticpara.
ase - gpt2-12 (4)gpt22staticsentgpt22staticpara.
parent contextualmodels and derivatives.
embeddings \task.
dim.
crf1 / acc..mrf1 / acc..mpqaf1 / acc..subjf1 / acc..trecf1 / acc..sst-5f1 / acc..averagef1 / acc..300 81.6/83.2 78.2/78.2 85.1/87.6 90.9/90.9 45.4/86.2 15.5/43.2 66.1/78.1200 79.0/80.9 74.1/74.2 82.1/85.0 89.6/89.7 49.1/87.8 13.1/37.5 64.5/75.9300 80.3/81.9 78.3/78.4 86.5/88.1 90.9/90.9 45.3/85.9 13.9/43.9 66.2/78.2.
768 78.4/80.9 75.2/75.2 83.1/85.8 91.5/91.5 50.2/88.6 13.9/39.0 65.4/76.8768 75.9/78.5 72.6/72.7 83.3/86.0 85.5/85.5 43.2/85.7 13.4/38.9 62.0/74.6768 79.8/81.2 74.1/74.1 81.0/84.5 89.4/89.4 42.9/84.1 13.2/38.6 63.4/75.3.
768 81.5/83.0 78.5/78.5 86.0/86.0 91.0/91.0 48.3/87.6 15.0/42.1 66.7/78.0768 80.1/82.0 78.9/78.9 87.4/89.1 91.8/91.8 50.6/88.7 16.1/43.7 67.5/79.0768 81.1/83.6 80.8/80.8 87.3/89.3 91.6/91.6 51.8/89.2 16.1/44.9 68.1/79.9.
ase - roberta-12 (2)roberta2staticsentroberta2staticpara.
768 78.4/81.2 78.3/78.3 86.4/88.5 89.5/89.5 52.0/89.1 15.2/43.0 66.6/78.3768 76.5/79.6 80.2/80.2 85.6/88.0 92.2/92.2 49.7/89.1 15.7/43.8 66.7/78.8768 80.9/82.3 80.0/80.1 87.3/89.4 92.4/92.4 49.3/88.8 16.3/43.4 67.7/79.4.
768 81.0/82.1 80.1/80.1 84.8/86.2 91.2/91.2 51.0/88.8 15.5/42.0 67.3/78.4768 81.5/83.5 79.5/79.5 86.5/88.5 91.8/91.8 51.8/89.2 16.2/43.8 67.9/79.4768 81.0/82.6 79.7/79.7 86.9/88.8 92.1/92.1 53.0/89.1 16.2/44.1 68.1/79.4.
bert-12sbert-base-nli.
768 89.6/90.6 87.4/87.4 89.4/90.8 96.7/96.7 77.6/94.7 30.7/54.0 78.6/85.7768 87.4/88.7 83.3/83.3 86.8/88.2 93.6/93.6 41.6/72.2 25.3/48.2 69.7/79.1.
roberta-12sroberta-base-nli.
768 90.0/90.8 90.1/90.1 89.1/90.6 96.3/96.3 95.1/99.2 34.0/57.6 82.4/87.4768 87.6/88.6 86.3/86.3 86.8/88.8 94.6/94.6 52.4/80.6 23.7/53.5 72.7/82.1.
gpt2-12.
768 88.5/89.5 87.1/87.1 87.3/89.1 96.1/96.1 76.8/94.3 30.8/54.5 77.8/85.1.
table 4: comparison of the performance of different static embeddings on downstream tasks.
all x2staticmethod performances which improve or are at par over all other static embedding methods and the best ase layeron their parent model are shown in bold.
best static embedding performance for each task is underlined.
for eachase method, the number in brackets indicates the layer with best average performance.
we use macro-f1 scoresand accuracy as the metrics to gauge the performance of models on these downstream tasks.
note: contextualembeddings for bert-12, roberta-12 and gpt2-12 in the sota section are also ﬁne-tuned while sbert-base-nli and sroberta-base-nli are not..by a signiﬁcant margin..for both word similarity evaluations anddownstream supervised tasks, we observe thatx2staticpara embeddings perform slightly betterthan x2staticsent embeddings.
however, sinceno hyperparameter tuning was performed on thedistillation of x2static embeddings, it is hardto discern which x2static variant shows betterperformance.
moreover, owing to the same factconcerning hyperparameter tuning, we expect to.
see even larger improvements with proper hyperpa-rameter tuning as well as training on larger data..5 conclusion and future work.
to.
proposes.
augmentby.
earlierthis workword2vec-based methodsleveragingrecent more expressive deep contextual embeddingmodels to extract static word embeddings.
theresulting distilled static embeddings, on an average,outperform their competitors on both unsupervised.
5248as well downstream supervised evaluations andthus can be used to replace compute-heavycontextual embedding models (or existing staticembedding models) at inference time in manycompute-resource-limited applications.
the result-ing embeddings can also be used as a task-agnostictool to measure the lexical information conveyedby contextual embedding models and allow a faircomparison with their static analogues..further work can explore extending this dis-tillation framework into cross-lingual domains(schwenk and douze, 2017; lample and conneau,2019) as well as using better pooling methods in-stead of simple averaging for obtaining the con-text representation, or joint ﬁne-tuning to obtaineven stronger static word embeddings.
anotherpromising avenue is the use of a similar approachto learn sense embeddings from contextual embed-ding models.
we would also like to investigate theperformance of these embeddings when distilledon a larger corpus along with more extensive hyper-parameter tuning.
last but not the least, we wouldlike to release x2static models for different lan-guages for further public use..kevin clark, minh-thang luong, quoc v. le, andchristopher d. manning.
2020. electra: pre-training text encoders as discriminators rather thangenerators.
in iclr..jacob devlin, ming-wei chang, kenton lee, andkristina toutanova.
2019. bert: pre-training ofdeep bidirectional transformers for language un-derstanding.
in naacl-hlt..kawin ethayarajh.
2019. how contextual are contex-tualized word representations?
comparing the ge-ometry of bert, elmo, and gpt-2 embeddings.
in emnlp-ijcnlp - proceedings of the 2019 con-ference on empirical methods in natural languageprocessing and the 9th international joint confer-ence on natural language processing, pages 55–65.
acl..t. farrell,.
´oscar araque, miriam fern´andez, andh. alani.
2020. on the use of jargon and wordembeddings to explore subculture within the redditsmanosphere.
12th acm conference on web sci-ence..manaal faruqui, yulia tsvetkov, pushpendre rastogi,and chris dyer.
2016. problems with evaluation ofword embeddings using word similarity tasks.
inrepeval@acl..j. r. firth.
1957. a synopsis of linguistic theory, 1930-.
1955..references.
eneko agirre, enrique alfonseca, keith b. hall, janakravalova, marius pasca, and aitor soroa.
2009.a study on similarity and relatedness using distri-in hlt-butional and wordnet-based approaches.
naacl..daniela gerz, ivan vuli´c, felix hill, roi reichart, andanna korhonen.
2016.simverb-3500: a large-scale evaluation set of verb similarity.
in proceed-ings of the 2016 conference on empirical methodsin natural language processing, pages 2173–2182..israa alghanmi, luis espinosa anke, and stevenschockaert.
2020. combining bert with staticword embeddings for categorizing social media.
in proceedings of the sixth workshop on noisy user-generated text (w-nut 2020), pages 28–33..l. barsalou.
1982. context-independent and context-dependent information in concepts.
memory & cog-nition, 10:82–93..piotr bojanowski, edouard grave, armand joulin, andtomas mikolov.
2016. enriching word vectors withsubword information.
transactions of the associa-tion for computational linguistics, 5:135–146..rishi bommasani, kelly davis, and claire cardie.
interpreting pretrained contextualized repre-in.
2020.sentations via reductions to static embeddings.
acl..samuel r bowman, gabor angeli, christopher potts,and christopher d manning.
2015. a large anno-tated corpus for learning natural language inference.
in emnlp..hila gonen and yoav goldberg.
2019. lipstick on apig: debiasing methods cover up systematic genderbiases in word embeddings but do not remove them.
in naacl-hlt..joshua gordon, marzieh babaeianjelodar, and jeannamatthews.
2020. studying political bias via wordin www ’20 - companion proceed-embeddings.
ings of the web conference 2020, page 760764..prakhar gupta, matteo pagliardini, and martin jaggi.
2019. better word embeddings by disentanglingcontextual n-gram information.
in naacl-hlt..felix hill, roi reichart, and anna korhonen.
2014.simlex-999: evaluating semantic models with (gen-uine) similarity estimation.
computational linguis-tics, 41:665–695..minqing hu and bing liu.
2004. mining and summa-rizing customer reviews.
in proceedings of the tenthacm sigkdd international conference on knowl-edge discovery and data mining, pages 168–177.
acm..5249colette joubarne and diana inkpen.
2011. compari-son of semantic similarity for different languages us-ing the google n-gram corpus and second-order co-in canadian conference onoccurrence measures.
ai..thomas manzini, lim yao chong, alan w. black, andyulia tsvetkov.
2019. black is to criminal as cau-casian is to police: towards detecting, evaluatingand removing multiclass bias in word embeddings.
in naacl 2019..masahiro kaneko and danushka bollegala.
2019.gender-preserving debiasing for pre-trained wordin proceedings of the 57th annualembeddings.
meeting of the association for computational lin-guistics, pages 1641–1650, florence, italy.
acl..timothee mickus, denis paperno, mathieu constant,and kees van deemter.
2020. what do you mean,bert?
assessing bert as a distributional seman-tics model.
proceedings of the society for computa-tion in linguistics, 3(1):350–361..ilknur karadeniz and arzucan ¨ozg¨ur.
2019. linkingentities through an ontology using word embeddingsand syntactic re-ranking.
bmc bioinformatics, 20..diederik p kingma and jimmy ba.
2015a.
adam: a.method for stochastic optimization.
in iclr..diederik p. kingma and jimmy ba.
2015b.
adam: amethod for stochastic optimization.
in iclr - inter-national conference on learning representations..guillaume lample and alexis conneau.
2019. cross-in neuripslingual language model pretraining.
2019 - advances in neural information processingsystems..guillaume lample, alexis conneau, marc’aurelioranzato, ludovic denoyer, and herv´e j´egou.
2018.in interna-word translation without parallel data.
tional conference on learning representations..omer levy, y. goldberg, and i. dagan.
2015..im-proving distributional similarity with lessons learnedfrom word embeddings.
transactions of the associ-ation for computational linguistics, 3:211–225..yinhan liu, myle ott, naman goyal, jingfei du, man-dar joshi, danqi chen, omer levy, mike lewis,luke zettlemoyer, and veselin stoyanov.
2019.roberta: a robustly optimized bert pretraining ap-proach.
arxiv, abs/1907.11692..li lucy, dorottya demszky, patricia bromley, and danjurafsky.
2020. content analysis of textbooks vianatural language processing: findings on gender,race, and ethnicity in texas u.s. history textbooks.
aera open, 6..thang luong, richard socher, and christopher d.manning.
2013. better word representations with re-cursive neural networks for morphology.
in conll..andr´es alejandro ramos magna, h´ector allende-cid,carla taramasco, c. becerra, and r. figueroa.
2020.application of machine learning and word embed-dings in the classiﬁcation of cancer diagnosis usingpatient anamnesis.
ieee access, 8:106198–106213..christopher d. manning, mihai surdeanu, john bauer,jenny rose finkel, steven bethard, and david mc-closky.
2014. the stanford corenlp natural lan-guage processing toolkit.
in acl..tomas mikolov, kai chen, gregory s. corrado, andjeffrey dean.
2013. efﬁcient estimation of word rep-resentations in vector space.
in iclr - internationalconference on learning representations..matteo pagliardini, prakhar gupta, and martin jaggi.
2018. unsupervised learning of sentence embed-indings using compositional n-gram features.
naacl-hlt..bo pang and lillian lee.
2004. a sentimental educa-tion: sentiment analysis using subjectivity summa-rization based on minimum cuts.
in proceedings ofthe 42nd annual meeting on association for compu-tational linguistics, page 271. association for com-putational linguistics..bo pang and lillian lee.
2005. seeing stars: ex-ploiting class relationships for sentiment categoriza-tion with respect to rating scales.
in proceedings ofthe 43rd annual meeting on association for compu-tational linguistics, pages 115–124.
association forcomputational linguistics..jeffrey pennington, richard socher, and christopher d.manning.
2014. glove: global vectors for word rep-resentation.
in emnlp..matthew peters, mark neumann, mohit iyyer, mattgardner, christopher clark, kenton lee, and lukezettlemoyer.
2018. deep contextualized word repre-sentations.
in proceedings of the 2018 conferenceof the north american chapter of the associationfor computational linguistics: human languagetechnologies, volume 1 (long papers), pages 2227–2237, new orleans, louisiana.
acl..alec radford, jeffrey wu, rewon child, david luan,dario amodei, and ilya sutskever.
2019. languagemodels are unsupervised multitask learners..nils reimers and iryna gurevych.
2019. sentence-bert: sentence embeddings using siamese bert-networks.
in emnlp-ijcnlp - proceedings of the2019 conference on empirical methods in naturallanguage processing and the 9th international jointconference on natural language processing, pages3982–3992, hong kong, china.
acl..l. rheault and c. cochrane.
2020. word embeddingsfor the analysis of ideological placement in parlia-mentary corpora.
political analysis, 28:112–133..5250quentin lhoest, and alexander rush.
2020. trans-formers: state-of-the-art natural language process-in emnlp - proceedings of the 2020 con-ing.
ference on empirical methods in natural languageprocessing: system demonstrations, pages 38–45,online.
acl..yijia zhang, qingyu chen, z. yang, h. lin, and zhiy-ong lu.
2019. biowordvec, improving biomedi-cal word embeddings with subword information andmesh.
scientiﬁc data, 6..zhe zhao, tao liu, shen li, bofang li, and xiaoyongdu.
2017. ngram2vec: learning improved wordrepresentations from ngram co-occurrence statistics.
in emnlp..anna rogers, olga kovaleva, and anna rumshisky.
2020. a primer in bertology: what we know abouthow bert works.
transactions of the association forcomputational linguistics, 8:842–866..paula rubio-fern´andez.
2008. concept narrowing:the role of context-independent information.
j. se-mant., 25:381–409..holger schwenk and matthijs douze.
2017. learn-ing joint multilingual sentence representations within proceedings of theneural machine translation.
2nd workshop on representation learning for nlp,pages 157–167, vancouver, canada.
acl..philippa shoemark, farhana ferdousi liza, dongnguyen, scott hale, and barbara mcgillivray.
2019.room to glo: a systematic comparison of semanticchange detection approaches with word embeddings.
in emnlp-ijcnlp - proceedings of the 2019 con-ference on empirical methods in natural languageprocessing and the 9th international joint confer-ence on natural language processing, pages 66–76,hong kong, china.
acl..r. socher, alex perelygin, j. wu, jason chuang,christopher d. manning, a. ng, and christopherpotts.
2013. recursive deep models for seman-tic compositionality over a sentiment treebank.
inemnlp..charles spearman.
1904. the proof and measurementof association between two things.
the americanjournal of psychology, 15(1):72–101..emma strubell, ananya ganesh, and andrew mccal-lum.
2019. energy and policy considerations fordeep learning in nlp.
in acl..ellen m voorhees.
2002. overview of the trec 2001question answering track.
in nist special publica-tion, pages 42–51..ivan vulic, sebastian ruder, and anders søgaard.
2020. are all good word vector spaces isomorphic?
in emnlp..janyce wiebe, theresa wilson, and claire cardie.
2005. annotating expressions of opinions and emo-tions in language.
language resources and evalua-tion, 39(2):165–210..adina williams, nikita nangia, and samuel bowman.
2018. a broad-coverage challenge corpus for sen-tence understanding through inference.
in proceed-ings of the 2018 conference of the north americanchapter of the association for computational lin-guistics: human language technologies, volume 1(long papers), pages 1112–1122..thomas wolf, lysandre debut, victor sanh, julienchaumond, clement delangue, anthony moi, pier-ric cistac, tim rault, remi louf, morgan funtow-icz, joe davison, sam shleifer, patrick von platen,clara ma, yacine jernite, julien plu, canwen xu,teven le scao, sylvain gugger, mariama drame,.
5251a comparison of multiple downstream.
runs.
embeddings \task.
averagemean f1 / acc..ase - bert-12 (5)bert2staticpara.
67.0 ± 0.2/78.1 ± 0.268.3 ± 0.3/79.9 ± 0.2.ase - roberta-12 (2) 67.0 ± 0.2/78.2 ± 0.367.9 ± 0.2/79.6 ± 0.3roberta2staticpara.
ase - gpt2-12 (4)gpt22staticpara.
67.4 ± 0.3/78.3 ± 0.368.4 ± 0.2/80.0 ± 0.4.table 5: comparison of the overall performanceof x2staticpara with ase on downstream tasks.
mean and standard deviation of performance on eachtask over six runs is shown..b experiments on larger models.
in addition to the smaller 12-layer contextual em-bedding models, we also obtain x2static wordvectors from larger 24-layer contextual embeddingmodels, once again outperforming their ase coun-terparts by a signiﬁcant margin.
the evaluationresults can be accessed in the table 6..5252model \distilled model.
existing models.
models trained by us.
fasttextglove.
skipgramcbowsent2vec.
parent model \other details.
size of thetraining corpusrelative to ours.
12x650x.
n.a.
n.a.
n.a..models distilled by us.
parent model.
ase - best layer per taskase - best overall layer.
bert2staticsentbert2staticpara.
bert-12bert-12.
bert-12bert-12.
dim..rg-65.
ws-353.
sl-999.
sv-3500 rw-2034 average.
300300.
768768768.
768768.
768768.
0.76690.6442.
0.5960.5791.
0.4160.3764.
0.32740.2625.
0.52260.4607.
0.52760.4646.
0.82590.83480.7811.
0.71410.49990.7407.
0.40640.40970.5034.
0.27220.26260.3297.
0.48490.40430.4248.
0.54070.48230.55594.
0.7449(1)0.6948(3).
0.7012(1)0.6768(3).
0.5216(4)0.5195(3).
0.4151(5)0.3889(3).
0.4577(5)0.4343(3).
0.5429(3)0.5429(3).
0.74210.7555.
0.72970.7598.
0.54610.5384.
0.44370.4317.
0.54690.5299.
0.60170.6031.ase - best layer per taskbert-24ase - best task independent layer bert-24.
1024 0.7745(9)1024 0.7677(7).
0.7267(6) 0.5404(15) 0.4364(10) 0.4735(6)0.4665(7)0.7052(7).
0.4307(7).
0.5209(7).
0.5782(7)0.5782(7).
bert2staticsentbert2staticpara.
ase - best layer per taskase - best overall layer.
roberta2staticsentroberta2staticpara.
roberta2staticsentroberta2staticpara.
ase - best layer per taskase - best overall layer.
gpt22staticsentgpt22staticpara.
bert-24bert-24.
10241024.
0.80310.8085.
0.72390.7652.
0.56750.5607.
0.46920.4543.
0.55950.5504.
0.62470.6278.roberta-12roberta-12.
roberta-12roberta-12.
768768.
768768.
0.673(0)0.673(0).
0.7023(0)0.7023(0).
0.554(5)0.5167(0).
0.4602(4)0.4424(0).
0.5075(3)0.4657(0).
0.5600(0)0.5600(0).
0.79990.8057.
0.74520.7638.
0.55070.5544.
0.46580.4717.
0.54960.5501.
0.62220.6291.roberta-24roberta-24.
10241024.
0.76770.7939.
0.73360.7523.
0.53970.5476.
0.45760.4663.
0.57200.5739.
0.61410.6268.gpt2-12gpt2-12.
gpt2-12gpt2-12.
768768.
768768.
0.7013(1)0.6833(2).
0.6879(0)0.6560(2).
0.4972(2)0.4972(2).
0.3905(2)0.3905(2).
0.4556(2)0.4556(2).
0.5365(2)0.5365(2).
0.74840.7881.
0.71510.7267.
0.53970.5417.
0.46760.4733.
0.57600.5668.
0.60940.6193.ase - best layer per taskroberta-24ase - best task independent layer roberta-24.
1024 0.6782(8)1024 0.6738(6).
0.6736(6) 0.5526(18) 0.4571(9)0.4571(9)0.5437(9)0.6270(9).
0.5385(9)0.5385(9).
0.5680(9)0.5680(9).
ase - best layer per taskgpt2-24ase - best task independent layer gpt2-24.
1024 0.6574(1)0.6957(0) 0.4988(13) 0.4226(12) 0.4566(12) 0.5155(13)1024 0.5773(13) 0.6242(13) 0.4988(13) 0.4210(13) 0.4561(13) 0.5155(13).
gpt22staticsentgpt22staticpara.
gpt2-24gpt2-24.
10241024.
0.78150.7907.
0.73110.7331.
0.55370.5488.
0.47740.4850.
0.59390.5828.
0.62750.6281.table 6: comparison of the performance of different embedding methods on word similarity tasks.
modelsare compared using spearman correlation for word similarity tasks.
all x2static method performances whichimprove over all ase methods on their parent model as well as all static models are shown in bold.
best perfor-mance in each task is underlined.
for all ase methods, the number in parentheses for each dataset indicates whichlayer was used for obtaining the static embeddings..5253