guiding teacher forcing with seer forcingfor neural machine translation.
yang feng1,2 shuhao gu1,2 dengji guo1,2 zhengxin yang1,2 chenze shao1,2 ∗1 key laboratory of intelligent information processinginstitute of computing technology, chinese academy of sciences (ict/cas)2 university of chinese academy of sciences, beijing, china.
{.
fengyang, gushuhao19b, guodengji19s}yangzhengxin17z, shaochenze18z@ ict.ac.cn}.
{.
@ ict.ac.cn.
abstract.
although teacher forcing has become the maintraining paradigm for neural machine transla-tion, it usually makes predictions only condi-tioned on past information, and hence lacksglobal planning for the future.
to addressthis problem, we introduce another decoder,called seer decoder, into the encoder-decoderframework during training, which involves fu-ture information in target predictions.
mean-while, we force the conventional decoder tosimulate the behaviors of the seer decoderin this way, atvia knowledge distillation.
test the conventional decoder can perform likethe seer decoder without the attendance ofit.
experiment results on the chinese-english,english-german and english-romanian trans-lation tasks show our method can outper-form competitive baselines signiﬁcantly andachieves greater improvements on the biggerdata sets.
besides, the experiments also proveknowledge distillation the best way to trans-fer knowledge from the seer decoder to theconventional decoder compared to adversariallearning and l2 regularization..1.introduction.
neural machine translation (nmt) (kalchbrennerand blunsom, 2013; sutskever et al., 2014; bah-danau et al., 2014; gehring et al., 2017; vaswaniet al., 2017) has achieved great success and is draw-ing larger attention recently.
most nmt models areunder the attention-based encoder-decoder frame-work which assumes there is a common seman-tic space between the source and target languages.
the encoder encodes the source sentence to thecommon space to get its meaning, and the decoderprojects the source meaning to the target space togenerate corresponding target words.
whenevergenerating a target word at a time step, the decoder.
∗the code: https://github.com/ictnlp/seerforcingnmt.
needs to retrieve the attended source informationand then decodes into a target word.
the underlineprinciple which makes sure the framework worksis that the information hold by the source sentenceand its target counterpart is equivalent.
thus thetranslation procedure can be considered to decom-pose source information into different pieces andthen to convert each piece to a proper target wordaccording to bilingual context.
when all the infor-mation encoded in the source sentence is throughlyprocessed, the whole translation has been gener-ated..neural machine translation models are usuallytrained via maximum likelihood estimation (mle)(johansen and juselius, 1990) and the operationform is known as teacher forcing (williams andzipser, 1989).
the teacher forcing strategy per-forms one-step-ahead predictions with the pastground truth words fed as context and forces thedistribution of the next prediction to approach a0-1 distribution where the probability of the nextground truth word corresponds to 1 and others to0. in this way, the predicted sequence is trained tobe close to the ground truth sequence.
from theperspective of information division, the functionof teacher forcing is to teach the translation modelhow to segment source information and derive theground truth word from the source information at amaximum probability..however, teacher forcing can only provide up-to-now ground truth words for one-step-ahead pre-dictions and hence lacks global planning for thefuture.
this will result in local optimization espe-cially when the next prediction is highly related tothe future.
besides, as the translation grows, theprevious prediction errors will be accumulated andaffect later predictions (zhang et al., 2019c).
thisis the important reason why nmt models cannotalways produce the ground truth sequence duringtraining.
therefore, it is more possible to achieve.
proceedingsofthe59thannualmeetingoftheassociationforcomputationallinguisticsandthe11thinternationaljointconferenceonnaturallanguageprocessing,pages2862–2872august1–6,2021.©2021associationforcomputationallinguistics2862figure 1: the architecture of the proposed method.
global optimization by getting to know the futureground truth words.
this can lead to better cross-attention to the source sentence and thus betterinformation devision.
but unfortunately, groundtruth can be only obtained during training and wecannot inference with future ground truth at test..to address this problem, we introduce an addi-tional seer decoder into the encoder-decoder frame-work to integrate future information.
during train-ing, the seer decoder is used to guide the behaviorsof the conventional decoder while at test the trans-lation model only inferences with the conventionaldecoder without introducing any extra parametersand calculation cost.
speciﬁcally, the conventionaldecoder only gets past information participating inthe next prediction, while the seer decoder has boththe past and future ground truth words engagedin the next prediction.
both decoders are trainedto generate ground truth via mle and meanwhilethe conventional decoder is forced to simulate thebehaviors of the seer decoder via knowledge distil-lation (buciluˇa et al., 2006; hinton et al., 2015).
inthis way, at test the conventional decoder can per-form like the seer decoder as if it knew the futuretranslation..we conducted experiments on two small datasets (chinese-english and english-romanian) andtwo big data sets (chinese-english and english-german) and the experiment results show that ourmethod can outperform strong baselines on all thedata sets.
in addition, we also compared differentmechanisms of transferring knowledge and foundthat knowledge distillation is more effective thanadversarial learning and l2 regularization.
to thebest of our knowledge, this paper is the ﬁrst toexplore the effects of the three mechanisms simul-taneously in machine translation..figure 2: the architecture of the seer decoder.
2 the proposed method.
we introduce our method on the basis of trans-former which is under the encoder-decoder frame-work (vaswani et al., 2017).
our model consistsof three components: the encoder, the conventionaldecoder and the seer decoder.
the architecture isshown in figure 1. the encoder and the conven-tional decoder work in the same way as the corre-sponding components of transformer do.
the seerdecoder integrates future ground truth informationinto its self-attention representation and calculatescross-attention over source hidden states with theself-attention representation as the query.
duringtraining, the encoder is shared by the two decodersand both decoders perform predictions to generateground truth.
the behaviors of the conventional de-coder are guided by the seer decoder via knowledgedistillation.
if the conventional decoder can predicta similar distribution as the seer decoder, we thinkthe conventional decoder performs like the seerdecoder.
then we can only use the conventionaldecoder for test..the details of the encoder and the conventionaldecoder can be got from vaswani et al.
(2017).
assume the input sequence is x = (x1, ..., xj ), theground truth sequence is y∗ = (y∗i ) and thegenerated translation is y = (y1, ..., yi ).
we willgive more description to the seer decoder and thetraining in what follows..1, ..., y∗.
2863ground truth wordsmasked multi-head attentionmasked multi-head attentionfuture mask multi-headattention multi-headattention  masked multi-head attentionmasked multi-head attentionpast masklinear transformationmulti-head attention output probabilitiesfuture subdecoderpast subdecoder(n-1)target embeddingencoder outputencoder outputffnffnencoder outputffnlinear & softmaxfusion layer⇥<latexit sha1_base64="c5v4yvckqxajveef+dg0ckj6lyw=">aaacyxicjvhlssnafd2nr1pfvzdugkvwvrirdfl0i7ipyb/qfknsar3ny8xermwvp+bwf0z8a/0l74wjqev0qpiz595zzu69fhpyir3npwbntc/mzhxnswuls8sr5dw1pkjylgcniamtro17gou8zg3jzcjaaca8ya9zy788vphwncset+jtoupzl/kgmr/wwjnenbusr0yclsto1dhlngsuarwyvu/kz+iijwqbckrgicejh/ag6onahyouub7gxgweui4z3kfe2pyyggv4xf7sd0i7jmfj2itpodubnrlsm5hsxhzpesrlckvtbb3ptbnif/mea091txh9femvestxtuxfus/m/+pulrid7osaonwuakzvfxixxhdf3dz+upukh5q4hfsuzwghwvnzz1trhk5d9dbt8vedqvi1d0xujjd1sxqw+3ock6c5u3wdqnuyw6kdmfexsyfnbnm891ddeepokpcfhvcij+vyurjurnupvktgnov4tqz7d/yrkbu=</latexit><latexit sha1_base64="c5v4yvckqxajveef+dg0ckj6lyw=">aaacyxicjvhlssnafd2nr1pfvzdugkvwvrirdfl0i7ipyb/qfknsar3ny8xermwvp+bwf0z8a/0l74wjqev0qpiz595zzu69fhpyir3npwbntc/mzhxnswuls8sr5dw1pkjylgcniamtro17gou8zg3jzcjaaca8ya9zy788vphwncset+jtoupzl/kgmr/wwjnenbusr0yclsto1dhlngsuarwyvu/kz+iijwqbckrgicejh/ag6onahyouub7gxgweui4z3kfe2pyyggv4xf7sd0i7jmfj2itpodubnrlsm5hsxhzpesrlckvtbb3ptbnif/mea091txh9femvestxtuxfus/m/+pulrid7osaonwuakzvfxixxhdf3dz+upukh5q4hfsuzwghwvnzz1trhk5d9dbt8vedqvi1d0xujjd1sxqw+3ock6c5u3wdqnuyw6kdmfexsyfnbnm891ddeepokpcfhvcij+vyurjurnupvktgnov4tqz7d/yrkbu=</latexit><latexit sha1_base64="c5v4yvckqxajveef+dg0ckj6lyw=">aaacyxicjvhlssnafd2nr1pfvzdugkvwvrirdfl0i7ipyb/qfknsar3ny8xermwvp+bwf0z8a/0l74wjqev0qpiz595zzu69fhpyir3npwbntc/mzhxnswuls8sr5dw1pkjylgcniamtro17gou8zg3jzcjaaca8ya9zy788vphwncset+jtoupzl/kgmr/wwjnenbusr0yclsto1dhlngsuarwyvu/kz+iijwqbckrgicejh/ag6onahyouub7gxgweui4z3kfe2pyyggv4xf7sd0i7jmfj2itpodubnrlsm5hsxhzpesrlckvtbb3ptbnif/mea091txh9femvestxtuxfus/m/+pulrid7osaonwuakzvfxixxhdf3dz+upukh5q4hfsuzwghwvnzz1trhk5d9dbt8vedqvi1d0xujjd1sxqw+3ock6c5u3wdqnuyw6kdmfexsyfnbnm891ddeepokpcfhvcij+vyurjurnupvktgnov4tqz7d/yrkbu=</latexit><latexit sha1_base64="c5v4yvckqxajveef+dg0ckj6lyw=">aaacyxicjvhlssnafd2nr1pfvzdugkvwvrirdfl0i7ipyb/qfknsar3ny8xermwvp+bwf0z8a/0l74wjqev0qpiz595zzu69fhpyir3npwbntc/mzhxnswuls8sr5dw1pkjylgcniamtro17gou8zg3jzcjaaca8ya9zy788vphwncset+jtoupzl/kgmr/wwjnenbusr0yclsto1dhlngsuarwyvu/kz+iijwqbckrgicejh/ag6onahyouub7gxgweui4z3kfe2pyyggv4xf7sd0i7jmfj2itpodubnrlsm5hsxhzpesrlckvtbb3ptbnif/mea091txh9femvestxtuxfus/m/+pulrid7osaonwuakzvfxixxhdf3dz+upukh5q4hfsuzwghwvnzz1trhk5d9dbt8vedqvi1d0xujjd1sxqw+3ock6c5u3wdqnuyw6kdmfexsyfnbnm891ddeepokpcfhvcij+vyurjurnupvktgnov4tqz7d/yrkbu=</latexit>future maskpast mask⇥<latexit sha1_base64="c5v4yvckqxajveef+dg0ckj6lyw=">aaacyxicjvhlssnafd2nr1pfvzdugkvwvrirdfl0i7ipyb/qfknsar3ny8xermwvp+bwf0z8a/0l74wjqev0qpiz595zzu69fhpyir3npwbntc/mzhxnswuls8sr5dw1pkjylgcniamtro17gou8zg3jzcjaaca8ya9zy788vphwncset+jtoupzl/kgmr/wwjnenbusr0yclsto1dhlngsuarwyvu/kz+iijwqbckrgicejh/ag6onahyouub7gxgweui4z3kfe2pyyggv4xf7sd0i7jmfj2itpodubnrlsm5hsxhzpesrlckvtbb3ptbnif/mea091txh9femvestxtuxfus/m/+pulrid7osaonwuakzvfxixxhdf3dz+upukh5q4hfsuzwghwvnzz1trhk5d9dbt8vedqvi1d0xujjd1sxqw+3ock6c5u3wdqnuyw6kdmfexsyfnbnm891ddeepokpcfhvcij+vyurjurnupvktgnov4tqz7d/yrkbu=</latexit><latexit sha1_base64="c5v4yvckqxajveef+dg0ckj6lyw=">aaacyxicjvhlssnafd2nr1pfvzdugkvwvrirdfl0i7ipyb/qfknsar3ny8xermwvp+bwf0z8a/0l74wjqev0qpiz595zzu69fhpyir3npwbntc/mzhxnswuls8sr5dw1pkjylgcniamtro17gou8zg3jzcjaaca8ya9zy788vphwncset+jtoupzl/kgmr/wwjnenbusr0yclsto1dhlngsuarwyvu/kz+iijwqbckrgicejh/ag6onahyouub7gxgweui4z3kfe2pyyggv4xf7sd0i7jmfj2itpodubnrlsm5hsxhzpesrlckvtbb3ptbnif/mea091txh9femvestxtuxfus/m/+pulrid7osaonwuakzvfxixxhdf3dz+upukh5q4hfsuzwghwvnzz1trhk5d9dbt8vedqvi1d0xujjd1sxqw+3ock6c5u3wdqnuyw6kdmfexsyfnbnm891ddeepokpcfhvcij+vyurjurnupvktgnov4tqz7d/yrkbu=</latexit><latexit sha1_base64="c5v4yvckqxajveef+dg0ckj6lyw=">aaacyxicjvhlssnafd2nr1pfvzdugkvwvrirdfl0i7ipyb/qfknsar3ny8xermwvp+bwf0z8a/0l74wjqev0qpiz595zzu69fhpyir3npwbntc/mzhxnswuls8sr5dw1pkjylgcniamtro17gou8zg3jzcjaaca8ya9zy788vphwncset+jtoupzl/kgmr/wwjnenbusr0yclsto1dhlngsuarwyvu/kz+iijwqbckrgicejh/ag6onahyouub7gxgweui4z3kfe2pyyggv4xf7sd0i7jmfj2itpodubnrlsm5hsxhzpesrlckvtbb3ptbnif/mea091txh9femvestxtuxfus/m/+pulrid7osaonwuakzvfxixxhdf3dz+upukh5q4hfsuzwghwvnzz1trhk5d9dbt8vedqvi1d0xujjd1sxqw+3ock6c5u3wdqnuyw6kdmfexsyfnbnm891ddeepokpcfhvcij+vyurjurnupvktgnov4tqz7d/yrkbu=</latexit><latexit sha1_base64="c5v4yvckqxajveef+dg0ckj6lyw=">aaacyxicjvhlssnafd2nr1pfvzdugkvwvrirdfl0i7ipyb/qfknsar3ny8xermwvp+bwf0z8a/0l74wjqev0qpiz595zzu69fhpyir3npwbntc/mzhxnswuls8sr5dw1pkjylgcniamtro17gou8zg3jzcjaaca8ya9zy788vphwncset+jtoupzl/kgmr/wwjnenbusr0yclsto1dhlngsuarwyvu/kz+iijwqbckrgicejh/ag6onahyouub7gxgweui4z3kfe2pyyggv4xf7sd0i7jmfj2itpodubnrlsm5hsxhzpesrlckvtbb3ptbnif/mea091txh9femvestxtuxfus/m/+pulrid7osaonwuakzvfxixxhdf3dz+upukh5q4hfsuzwghwvnzz1trhk5d9dbt8vedqvi1d0xujjd1sxqw+3ock6c5u3wdqnuyw6kdmfexsyfnbnm891ddeepokpcfhvcij+vyurjurnupvktgnov4tqz7d/yrkbu=</latexit>1(n-1)⇥<latexit sha1_base64="c5v4yvckqxajveef+dg0ckj6lyw=">aaacyxicjvhlssnafd2nr1pfvzdugkvwvrirdfl0i7ipyb/qfknsar3ny8xermwvp+bwf0z8a/0l74wjqev0qpiz595zzu69fhpyir3npwbntc/mzhxnswuls8sr5dw1pkjylgcniamtro17gou8zg3jzcjaaca8ya9zy788vphwncset+jtoupzl/kgmr/wwjnenbusr0yclsto1dhlngsuarwyvu/kz+iijwqbckrgicejh/ag6onahyouub7gxgweui4z3kfe2pyyggv4xf7sd0i7jmfj2itpodubnrlsm5hsxhzpesrlckvtbb3ptbnif/mea091txh9femvestxtuxfus/m/+pulrid7osaonwuakzvfxixxhdf3dz+upukh5q4hfsuzwghwvnzz1trhk5d9dbt8vedqvi1d0xujjd1sxqw+3ock6c5u3wdqnuyw6kdmfexsyfnbnm891ddeepokpcfhvcij+vyurjurnupvktgnov4tqz7d/yrkbu=</latexit><latexit sha1_base64="c5v4yvckqxajveef+dg0ckj6lyw=">aaacyxicjvhlssnafd2nr1pfvzdugkvwvrirdfl0i7ipyb/qfknsar3ny8xermwvp+bwf0z8a/0l74wjqev0qpiz595zzu69fhpyir3npwbntc/mzhxnswuls8sr5dw1pkjylgcniamtro17gou8zg3jzcjaaca8ya9zy788vphwncset+jtoupzl/kgmr/wwjnenbusr0yclsto1dhlngsuarwyvu/kz+iijwqbckrgicejh/ag6onahyouub7gxgweui4z3kfe2pyyggv4xf7sd0i7jmfj2itpodubnrlsm5hsxhzpesrlckvtbb3ptbnif/mea091txh9femvestxtuxfus/m/+pulrid7osaonwuakzvfxixxhdf3dz+upukh5q4hfsuzwghwvnzz1trhk5d9dbt8vedqvi1d0xujjd1sxqw+3ock6c5u3wdqnuyw6kdmfexsyfnbnm891ddeepokpcfhvcij+vyurjurnupvktgnov4tqz7d/yrkbu=</latexit><latexit sha1_base64="c5v4yvckqxajveef+dg0ckj6lyw=">aaacyxicjvhlssnafd2nr1pfvzdugkvwvrirdfl0i7ipyb/qfknsar3ny8xermwvp+bwf0z8a/0l74wjqev0qpiz595zzu69fhpyir3npwbntc/mzhxnswuls8sr5dw1pkjylgcniamtro17gou8zg3jzcjaaca8ya9zy788vphwncset+jtoupzl/kgmr/wwjnenbusr0yclsto1dhlngsuarwyvu/kz+iijwqbckrgicejh/ag6onahyouub7gxgweui4z3kfe2pyyggv4xf7sd0i7jmfj2itpodubnrlsm5hsxhzpesrlckvtbb3ptbnif/mea091txh9femvestxtuxfus/m/+pulrid7osaonwuakzvfxixxhdf3dz+upukh5q4hfsuzwghwvnzz1trhk5d9dbt8vedqvi1d0xujjd1sxqw+3ock6c5u3wdqnuyw6kdmfexsyfnbnm891ddeepokpcfhvcij+vyurjurnupvktgnov4tqz7d/yrkbu=</latexit><latexit sha1_base64="c5v4yvckqxajveef+dg0ckj6lyw=">aaacyxicjvhlssnafd2nr1pfvzdugkvwvrirdfl0i7ipyb/qfknsar3ny8xermwvp+bwf0z8a/0l74wjqev0qpiz595zzu69fhpyir3npwbntc/mzhxnswuls8sr5dw1pkjylgcniamtro17gou8zg3jzcjaaca8ya9zy788vphwncset+jtoupzl/kgmr/wwjnenbusr0yclsto1dhlngsuarwyvu/kz+iijwqbckrgicejh/ag6onahyouub7gxgweui4z3kfe2pyyggv4xf7sd0i7jmfj2itpodubnrlsm5hsxhzpesrlckvtbb3ptbnif/mea091txh9femvestxtuxfus/m/+pulrid7osaonwuakzvfxixxhdf3dz+upukh5q4hfsuzwghwvnzz1trhk5d9dbt8vedqvi1d0xujjd1sxqw+3ock6c5u3wdqnuyw6kdmfexsyfnbnm891ddeepokpcfhvcij+vyurjurnupvktgnov4tqz7d/yrkbu=</latexit>2.1 the seer decoder.
although we feed the future ground truth words tothe seer decoder, we will not tell it the next groundtruth word to be generated, in case it will only learna copy operation, not how to derive a word.
consid-ering efﬁciency, the seer decoder does not integratethe past and future ground truth information with aunique decoder , but two separate subdecoders.
asa result, the seer decoder consists of three compo-nents: the past subdecoder, the future subdecoderand the fusion layer.
the architecture of the seerdecoder is given in figure 2. the past and futuresubdecoders are employed to decode the past andfuture ground truth information into hidden statesrespectively and the fusion layer is used to fuse theoutput of the past and future subdecoders and cal-culate the ﬁnal hidden state for the next prediction.
1 layersand each layer has three sublayers which are themulti-head sublayer, the cross-attention sublayerand the feed-forward network (fnn) sublayer, thesame as transformer.
the multi-attention sublayeraccepts the whole ground truth sequence as theinput and applies a mask matrix mp to make sureonly the past ground truth words attend the self-attention.
speciﬁcally, to generate the i-th targetword, its corresponding mask vector in the maskmatrix mp is set to mask the words y∗i+1, ..., y∗i .
then after the cross-attention sublayer and the ffnsublayer, the past subdecoder output a sequence ofpast hidden states, the packed matrix of which isdenoted as hp..the past subdecoder is composed of n.i , y∗.
−.
the future subdecoder has the same structureas the past subdecoder except for the mask matrix.
the future subdecoder also has the whole groundtruth sequence as the input but employs a differentmask matrix mf to only remain the future groundtruth information.
to generate the i-th target word,the corresponding mask vector in mf masks thewords y∗i .
the packed matrix of thefuture hidden states generated by the future subde-coder is denoted as hf ..1, ..., y∗.
i−1, y∗.
the fusion layer is composed of four sublay-ers: the multi-head sublayer, the linear sublayer,the cross-attention sublayer and the ffn sublayer.
except the linear sublayer, the rest three sublay-ers works in the same way as transformer does.
the multi-head sublayer encodes the outputs ofthe past and future subdecoders separately with themask matrix mp and mf , and the packed matrixof their output are denoted as h(cid:48)f respec-.
p and h(cid:48).
f , so that the same index in h(cid:48).
tively.
then we reverse the order of the vectors inf to get h(cid:48)(cid:48)h(cid:48)p andh(cid:48)(cid:48)f can correspond to the past and future repre-sentation needed for the same prediction.
assumeh(cid:48)f i ], then its reversed matrixis h(cid:48)(cid:48)f 1].
the linear sublayerfuses h(cid:48).
f i ; ...; h(cid:48).
f 2; ...; h(cid:48).
f = [h(cid:48).
f = [h(cid:48).
f 2; h(cid:48).
f 1; h(cid:48).
p and h(cid:48)(cid:48).
f via a linear transformation asp + wf h(cid:48)(cid:48)f.a = wph(cid:48).
(1).
now we can think each representation in the matrixa incorporates the past and future information forits corresponding prediction.
then after the cross-attention sublayer over the outputs of the encoderand then the ffn sublayer, we can get the targethidden states produced by the seer decoder as ss =[ss1; ...; ssi ]t .
then the probability to generate thetarget word yi is.
ps(yi.
>i, y∗y∗.
<i, x).
|.
∝.
exp (wossi).
(2).
note that the past and the future subdecodersshare the same set of parameters, and the samelinear transformation matrix wo is applied to theoutputs of the conventional and seer decoders..3 training.
|.
(cid:105).
(cid:104).
y∗.
x, y∗.
<i, x) and ps(yi.
in our method, only the conventional decoder isemployed for test and the seer decoder is onlyused to guide the conventional decoder during train-ing.
given a sentence pairin the trainingset, the conventional decoder and the seer decodercan predict a distribution for target position i as>i, y∗y∗pc(yi<i, x), respectively.
|the two decoders are both trained by comparing itspredicted distribution with the 0-1 distribution ofthe ground truth word by minimizing the cross en-tropy, that is to maximize the likelihood of the cor-responding ground truth word.
as the two decodersinvolve different information for next prediction,we call the training strategy teacher forcing andseer forcing, respectively.
the cross-entropy lossfor the conventional decoder is.
c =.
l.−.
k(cid:88).
ik(cid:88).
k=1.
i=1.
log pc(y∗i |.
y∗.
<i, x),.
(3).
and the cross-entropy loss for the seer decoder is.
s =.
l.−.
k(cid:88).
ik(cid:88).
k=1.
i=1.
log ps(y∗i |.
>i, y∗y∗.
<i, x)..(4).
where k is the size of the training set and ik is thelength of the k-th target sentence..2864the conventional decoder is further trained toget close to the distribution of the seer decoder viaknowledge distillation.
in knowledge distillation,the conventional decoder (the student) has to notonly match the one-hot ground truth word, but ﬁtthe distribution over the target vocabulary v drawnby the seer decoder (the teacher).
the knowledgedistillation loss can be formalized asik(cid:88).
|v|(cid:88).
k(cid:88).
kd =.
l.−.
k=1.
i=1.
l=1.
ps(yi = l.>i, y∗y∗|.
<i, x).
log pc(yi = l.×.
<i, x).
y∗|.
(5).
where.
v.is the size of the target vocabulary..|.
|.
the ﬁnal training loss is.
=.
s + λ.c + (1.l.l.l.λ).
kd ..−.
l.(6).
different from the conventional knowledge distilla-tion which ﬁrst trains the teacher via cross entropyagainst ground truth, then ﬁxes the teacher and onlytrains the student, we train all the parameters fromthe scratch, but we still follow the above rule tokeep the teacher (i.e.
the seer decoder) unchangedin the process of distillation.
to do this, we do notupdate the parameters of the seer decoder throughkd, that is, we only back propagate gradi-the losss, but not throughents to the seer decoder through.
l.l.kd..l4 related work.
reinforcement-learning-based methods also en-code future information in the rewards to superviseﬁne-tuning of the translation model.
the rewardsare worked out either by sampling future transla-tion with the reinforce algorithm (williams,1992; yu et al., 2017; yang et al., 2018; shao et al.,2019), or by directly calculating a value with theactor-critic algorithm (bahdanau et al., 2016; liet al., 2017).
this set of methods only give a weaksupervision to the nmt model through rewardsand suffer from unstable training.
in contrast, shaoet al.
(2018) propose to train autoregressive nmtwith the probabilistic n-gram based gleu (wuet al., 2016) and shao et al.
(2020) propose tominimize the bag-of-ngrams difference for non-autoregressive nmt so that the two methods canabandon reinforcement learning and perform train-ing directly by gradient descent..another set of methods introduce future infor-mation into inference with additional pass of de-coding or extra components at test.
niehues et al..(2016), xia et al.
(2017), hassan et al.
(2018) andzhang et al.
(2018) proposed a two-pass decodingalgorithm to ﬁrst generate a draft translation andthen generate ﬁnal translation referring to the draft.
geng et al.
(2018) expand this line of methods byperforming an adaptive multi-pass decoding wherethe number of decoding passes is determined bya policy network.
liu et al.
(2016a), liu et al.
(2016b), hoang et al.
(2017), zhang et al.
(2019d)and he et al.
(2019) perform bidirectional decod-ing simultaneously and the two decoders correlateto each other via an agreement term or a regu-larization term in the loss.
zhou et al.
(2019a) ,zhou et al.
(2019b) and zhang et al.
(2019b) alsomaintain a forward decoder and a backward de-coder to decode simultaneously but they interactto each other when making predictions.
zhanget al.
(2019a) introduce a future-aware vector attest which is learned via the knowledge distilla-tion framework during training.
the differencebetween this set of methods and our method is thatour method does not require any other cost at testand is easy to use..there are some other works which integrate fu-ture information during training while only performone-pass decoding.
serdyuk et al.
(2018) introducea twin network to perform bidirectional decodingsimultaneously during training and force the hid-den states generated by the two decoders to beconsistent, then at inference it can only use theforward decoder.
but in this method the two de-coders act as a counterpart to each other and nodecoder plays a role of teacher, which determinesthat it can only be trained via l2 regularization,not knowledge distillation which has proven in theexperiments more effective than l2 regularization.
feng et al.
(2020) introduce an evaluation moduleto give each translation more reasonable evaluationwhen it cannot match the ground truth.
the evalua-tion is conducted from the perspective of ﬂuencyand faithfulness which both need the participationof past and future information.
the difference fromthe method proposed in this paper is their methoduses self-generated translation as past informationand does not train with knowledge distillation..some researchers work in another perspectiveby introducing future information.
zhang et al.
(2020b) propose to employ future source informa-tion to guide simultaneous machine translation withknowledge distillation, so that the incompletenessof source can be mitigated.
zheng et al.
(2018) and.
2865zheng et al.
(2019) propose to model past and fu-ture information for the source to help the decoderfocus on untranslated source information..5 experiments.
5.1 settings.
5.1.1 data preparation.
→.
we conducted experiments on two small data setsand two big data sets.
small data setschinese.
english the training set consists ofabout 1.25m sentence pairs from ldc corpora with27.9m chinese words and 34.5m english wordsrespectively1.
we used mt02 for validation andmt03, mt04, mt05, mt06, mt08 for test.
wetokenized and lowercased english sentences usingthe moses scripts2, and segmented the chinesesentences with the stanford segmentor3.
the twosides were further segmented into subword unitsusing byte-pair encoding(bpe) (sennrich et al.,2016) with 30k merge operations.
32k size ofthe chinese dictionary and 29k size of the englishdictionary were built for the two sides..→.
english.
romanian we used the preprocessedversion of wmt16 en-ro dataset released by leeet al.
(2018) which includes 0.6m sentence pairs.
we used news-dev 2016 for validation and news-test 2016 for test.
the two languages share the 35ksize of the joint vocabulary generated with 40kmerge operations of bpe on the combined data..big data setschinese.
→.
english the training data is fromwmt 2017 zh-en translation tasks that contains20.18m sentence pairs after deleting duplicate ones.
the newsdev2017 was used as the developmentset and newstest2017 was used as the test set.
toavoid the effects of the translationese (grahamet al., 2019), we also tested the methods on thenewstest2019 test set.
we tokenized and truecasedthe english sentences with moses scripts.
for thechinese data, we performed word segmentation byusing stanford segmenter.
32k bpe sizes wereapplied to the training data seperately and then weﬁltered out the sentences which are longer than 128sub-words.
44k size of the chinese dictionary and.
1the corpora include ldc2002e18, ldc2003e07,of ldc2004t07,.
portion.
ldc2003e14, hansardsldc2004t08 and ldc2005t06.
2http://www.statmt.org/moses/3https://nlp.stanford.edu/.
33k size of the english dictionary were built basedon the corresponding data..→.
english.
german the training data is fromwmt2016 which consists of about 4.5m sentencespairs with 118m english words and 111m germanwords.
the newstest2014 was used as the develop-ment set and newstest2016 and newstest2019 wereused as the test sets.
the two languages share the32k size of the joint vocabulary generated with30k merge operations of bpe on the combineddata..5.1.2 systems.
transformer we used an open-sourcetoolkit called fairseq-py released by facebook (ottet al., 2019) which was implemented strictly fol-lowing vaswani et al.
(2017)..rl-nmt we trained transformer under thereinforcement learning framework using the re-inforce algorithm (williams, 1992) with thebleu as the rewards.
the implementation detailsfor the rl part is the same as yang et al.
(2018)..abdnmt our implementation of zhang et al..(2018) based on transformer..twinnet our implementation of serdyuk et al.
(2018) based on transformer.
the weight of l2loss was 0.2 ..evanmt our implementation of feng et al..(2020)..l.−.
k=1.
(cid:80)ik.
i=1 (cid:107).
g(sti).
tion.
(cid:80)k.seer+l2 seer forcing with l2 regulariza-2 =similar to twinnet, we setssi)2 where g is a linear(cid:107)transformation.
we ﬁrst pretrained the two de-s, then trainedcoders together only withl=them with the loss of2 whereα = 0.2, too.
please note that the l2 loss did notupdate the seer decoder and the encoder so thatthe conventional decoder would approach the seerdecoder, which followed serdyuk et al.
(2018)..ls + α.lt +.
t+.
=.
l.l.l.l.seer+al seer forcing with adversarial learn-ing.
a discriminator is employed to distinguish thehidden state sequences generated by the conven-tional decoder and the seer decoder.
the discrim-inator is based on cnn, implemented accordingto gu et al.
(2019).
the translation model andthe discriminator are trained jointly via a gradientreversal layer just like our method.
the loss isd is the loss of thed wherero data set.
ldiscriminator and α = 0.3 on the enand α = 0.2 on the other data sets..s + α.t +.
→.
=.
l.l.l.l.2866mt03 mt04 mt05 mt06 mt08.
cn.
en.
→.
transformer 46.54rl-nmt45.75abdnmt47.16twinnet47.78evanmt47.05seer+l247.98**seer+al47.91**48.12**our method.
46.9547.4147.5848.7447.7648.66**48.38**48.85**.
46.3946.4446.7748.5946.5948.16**47.97**48.25**.
45.3947.0845.9746.6546.5847.02**47.04**47.25**.
36.7537.6536.4338.8037.3938.64**38.18**38.71**.
avg ∆44.4044.87 +0.4744.78 +0.3846.11 +1.7145.07 +0.6746.09 +1.6945.89 +1.4946.24+1.84.
en.
ro.
→time wmt16 ∆32.601.032.791.7033.802.7833.792.5633.292.1933.55**1.9033.59**2.6433.86**1.92.
+0.19.
+1.20.
+1.19.
+0.69.
+0.95.
+1.04.
+1.26.
time.
1.02.383.362.622.911.832.351.86.table 1: bleu scores on small data sets.
** mean the improvements over transformer is statistically signiﬁ-cant (collins et al., 2005) (ρ < 0.01, respectively)..∆.
time.
2016.
∆.
∆.
time.
2017transformer 23.75twinnet23.39evanmt–seer+l223.95seer+al24.0124.35*our method.
∆.
-0.36.
–.
+0.20.
+0.26.
+0.60.
en.
cn→2019.
26.0026.09–25.8226.47*26.80**.
1.02.58–1.932.291.97.
+0.09.
–.
-0.18.
+0.47.
+0.80.
33.4933.0534.0033.5834.0334.25**.
en.
de.
→2019.
36.2035.6937.2536.6536.8137.34*.
-0.44.
+0.51.
+0.09.
+0.54.
+0.76.
1.02.572.481.532.391.57.
-0.51.
+1.05.
+0.45.
+0.61.
+1.14.
table 2: bleu scores on big data sets.
* and ** mean the improvements over transformer is statisticallysigniﬁcant (collins et al., 2005) (ρ < 0.05 and ρ < 0.01, respectively)..our method implemented based on fairseq-py.
the weight λ in equation 6 for the smallchineseenglish data set is set to 0.25, and forother data sets is set to 0.5..→.
all the transformer-based systems have thesame conﬁguration as the base model describedin vaswani et al.
(2017) except that dropout rateis 0.3. the translation quality was evaluated withbleu (papineni et al., 2002) with n=4 using thesacrebleu tool (post, 2018)4, where small datasets employ case-insensitive bleu while big datasets use case-sensitive bleu..5.2 main results.
we compare our method with other methodsthat can make global planning,including thereinforcement-based method (rl-nmt), the two-pass decoding method (abdnmt), twin net-works which match past and future information(twinnet) and the nmt model with an evalu-ate module to evaluate ﬂuency and faithfulness(evanmt).
in addition, we also explore learningmechanisms which can transfer knowledge fromthe seer decoder to the conventional decoder, in-cluding l2 regularization (seer+l2), adversariallearning (seer+al) and knowledge distillation(our method)..→.
we report results together with training time onthe small and big data sets in table 1 and table 2,respectively.5 as for different methods, in the smalldata sets, rl-nmt can only get small improve-ments over transformer which are in line with theresults reported in wu et al.
(2018), and abdnmtcannot get consistent improvements over trans-roformer with an obvious difference on the en→data set and a small difference on the cnen dataset.
twinnet can get comparable bleu scoreswith our method on the small data sets but mostlynegative difference on the big data sets.
evanmtcan achieve consistent improvements and greaterimprovements on the ende data set.
for thelearning mechanisms, knowledge distillation showconsistent superiority over l2 regularization and ad-versarial learning, which is remarkable especiallyon the big data sets.
adversarial learning can bringimprovements over transformer on all the data setswhile l2 regularization acts unstable on the bigdata sets.
in summary, our method proved to beeffective not only in the term of the architecture butalso in the learning mechanism..→.
4bleu+case.mixed+numrefs.1+smooth.exp+tok.13a+version.1.3.6.
5please note that there is no comparability between our re-sults and that of zhang et al.
(2019a) because we used differentvalidation and test sets..2867mt03 mt04 mt05 mt06 mt08.
cd w/o ca6.24sd w/o ca16.39cd with ca 29.45sd with ca52.61.
6.6816.7025.0345.57.
6.7016.6430.1452.02.
6.7717.2132.0752.68.
4.4911.9723.3944.14.avg.
6.1215.7828.0249.40.table 3: bleu scores of teacher forcing and seer forcing with and without cross-attention on nist cnentranslation.
cd and sd denote the conventional decoder and the seer decoder, respectively.
ca represents cross-attention..→.
5.3 the superiority of the seer decoder.
l.l.l.=.
t +.
to use seer forcing to guide teacher forcing, itshould be ensured that the seer decoder can out-perform the conventional decoder.
to verify this,we trained the two decoders together with thelosss without knowledge distillation.
then we evaluated their performance on the smallchinese-english translation task as follows.
bothdecoders are fed with ground truth words as contextat test so that they can inference in the same way asat training, where the conventional decoder uses thepast ground truth as context and the seer decoderemploys the past and future ground truth words ascontext in the past and future subdecoders..besides translation performance, we also checkthe superiority of seer decoder in target languagemodeling.
we do this by dropping out cross-attention so that the decoder can only generatetranslation based on target language model.
inthis way, the translation performance without cross-attention can demonstrate the ability of the twodecoders in target language modeling..we used the ﬁrst reference of the test set asground truth and calculated bleu scores only withthis reference.
from the results in table 3, we cansee that whether with or without cross-attention theseer decoder can make super large improvementsover the conventional decoder consistently on allthe test sets.
however, without cross-attention, thebleu scores of both decoders decrease dramati-cally which means language model information isnot enough for the translation task.
therefore, wecan conclude the seer decoder acts much better intarget language modeling and cross-language pro-jection and it is reasonable to use the seer decoderas the guider..5.4 the distillation of future information.
as the seer decoder achieves its superiority with thehelp of future target information, we hope that theconventional decoder can learn future informationfrom the seer decoder with knowledge distillation..accuracy recall f1-score.
transformerour method.
47.2352.24.
40.9142.10.
43.8446.63.table 4: comparison on the predicted bag of words be-tween the conventional decoders.
to check this, we tested whether the hidden statesof the conventional decoder could derive more fu-ture ground truth words after knowledge distilla-tion.
the underlying belief is that the future groundinformation transferred from the seer decoder canhelp the conventional decoder derive more futureground truth words..assuming the hidden states generated by theconventional decoder are st = [st1; ...; sti ]t , thefuture words for each target position i can be pre-dicted with the distribution.
pwi.
softmax(wwsti).
(7).
∼.
where ww is the weight matrix.
during training,we can get the bag of ground truth words for po-sition i as y∗i+1, ..., y∗y∗and train ww withi }other parameters ﬁxed by maximizing the likeli-hood of y∗.
i =.
{.
i as.
w =.
l.−.
k(cid:88).
ik(cid:88).
(cid:88).
k=1.
i=1.
w∈y∗i.log pwi(w).
(8).
where k is the size of training sentences, ik is thelength of the target sentence and log pwi(w) is theprobability of the word w in equation 7..at test, we select the top best ibi words ac-cording to equation 7 as the bag of future wordsbi for position i. as we cannot get the groundtruth, the size of bi is calculated approximately aswhere j is the length2ibi = max}of source sentence.
as we do not know the tar-get length during prediction, it may occur that i isgreater than j and calculating ibi in this way canensure bi contains 2 words at least..2, (j{.
−.
×.
i).
we conducted experiments on chinese-englishtranslation and used mt02 as the test set only.
2868figure 3: the similarity of the past and future informa-tion to the fused information.
avg.
∆.
our method-future-past-kd.
46.2445.3845.4244.84transformer 44.40.
-0.86.
-0.82.
-1.40.
-1.84.en transla-table 5: ablation study on nist cntion.
-future : dropping the future subdecoder; -past: dropping the past subdecoder; -kd: droppingknowledge distillation..→.
with the ﬁrst reference as ground truth.
we cal-culated the accuracy and recall by comparing eachbi against each y∗i .
the results in table 4 show theconventional decoder in our method can achievehigher accuracy and recall compared to the decoderof transformer.
this means knowledge distillationdoes transfer future information from the seer de-coder to the conventional decoder..5.5 the contribution of subdecoders.
in the seer decoder of our method, the informationfrom the past and future subdecoders is fused (asshown in equation 1) to get the ﬁnal cross-attention.
the intuition is that at the beginning stage, the pastsubdecoder contains less information than the fu-ture subdecoder, so the fused information shouldrely more on the future subdecoder.
as the transla-tion gets longer, the information embodied in thepast subdecoder grows, and the fused informationshould depend more on the past subdecoder.
toconﬁrm this hypothesis, we calculate the cosinesimilarity of the vectors in a given in equation 1with the corresponding weighted vectors of wph(cid:48)pand wf h(cid:48)(cid:48)f ..we selected 205 sentences the length of which.
figure 4: the bleu scores on sentence bins with dif-ferent lengths..ranges [15, 25], then calculated the cosine similar-ities word by word.
then the similarities at thesame target position will be averaged and the chartover all the target positions is given in figure 3.the ﬁgure conﬁrms our conjecture that at ﬁrst, thefused information is highly related to the futureinformation, and over time the similarity to past in-formation increases gradually while the similarityto future information decreases faster..5.6 ablation study.
we have proven that in our method the past andfuture information collaborate to achieve betterglobal planning.
in this section, we will explorethe inﬂuence of past and future information by sep-arately deleting the future and past subdecodersfrom the seer decoder.
in both cases, only the struc-ture of the seer decoder changes and the wholemodel is trained with knowledge distillation in thesame way.
we also remove knowledge distillationloss in which case the seer and conventional de-coders only interact via the shared encoder andonly optimize their own cross-entropy losses dur-ing training.
the results are given in table 5..when we exclude future or past information, thetranslation performance decreases dramatically atalmost the same extent, but they still have an ob-vious gain compared to transformer.
this demon-strates that both the past and future informationare necessary for global planning.
it is interestingthat the translation performance still rise withoutfuture subdecoder where there is no additional in-formation fed compared to transformer.
the rea-son may be the conventional and seer decoder canrestrict each other to avoid bad behaviors.
whenknowledge distillation is dropped, the performancedecline greatly which means only communicatingvia the encoder the conventional and seer decoders.
2869is not enough.
hence we need to introduce knowl-edge distillation to reinforce the inﬂuence of theseer decoder to the conventional decoder..dzmitry bahdanau, kyunghyun cho, and yoshua ben-gio.
2014. neural machine translation by jointlyarxiv preprintlearning to align and translate.
arxiv:1409.0473..5.7 performance with sentence length.
as the translation is generated word by word, thetranslation errors will be accumulated while the thetranslation grows, which will inﬂuence the later pre-diction.
in our method, the conventional decodercan learn future information from the seer decoderand hence it should make better global planningfor the whole sequence.
from this, we deduce thatour method performs better on long sentences thantransformer..→.
we checked this on the nist cn.
en trans-lation task and split the sentences in all the testsets into 8 bins according to their length.
then wetranslated for each bin and tested the bleu scores.
the results in figure 4 show that our method canachieve bigger improvements on longer sentences,especially in the last three bins..6 conclusion.
in order to help the nmt model to make goodglobal planning at inference, we propose to intro-duce a seer decoder which embodies future groundtruth to guide the behaviors of the conventional de-coder.
to this end, we employ the method of knowl-edge distillation to transfer future information fromthe seer decoder to the conventional decoder.
attest, the conventional decoder can perform trans-lation on its own as if it knew some future infor-mation.
the experiments indicate our method canoutperform strong baselines signiﬁcantly on fourdata sets.
we are also the ﬁrst to explore learningmechanisms of knowledge distillation, adversar-ial learning and l2 regularization and knowledgedistillation has proven to be the most effective one..acknowledgement.
this paper was supported by national keyr&d program of china (no.
2017yfe0192900).
thank wanying xie for running experiments ofevanmt.
thank all the anonymous reviewers forthe insightful and valuable comments..references.
dzmitry bahdanau, philemon brakel, kelvin xu,anirudh goyal, ryan lowe, joelle pineau, aaroncourville, and yoshua bengio.
2016. an actor-criticalgorithm for sequence prediction..cristian buciluˇa, rich caruana,.
and alexandruniculescu-mizil.
2006. model compression.
in pro-ceedings of the 12th acm sigkdd internationalconference on knowledge discovery and data min-ing, pages 535–541..michael collins, philipp koehn, and ivona kucerova.
2005. clause restructuring for statistical machinetranslation.
in proceedings of acl 2015, pages 531–540..yang feng, wanying xie, shuhao gu, chenze shao,wen zhang, zhengxin yang, and dong yu.
2020.modeling ﬂuency and faithfulness for diverse neu-ral machine translation.
in proceedings of the aaaiconference on artiﬁcial intelligence, volume 34,pages 59–66..jonas gehring, michael auli, david grangier, denisyarats, and yann n dauphin.
2017. convolutionalin proceedingssequence to sequence learning.
of the 34th international conference on machinelearning-volume 70, pages 1243–1252.
jmlr.
org..xinwei geng, xiaocheng feng, bing qin, and tingliu.
2018. adaptive multi-pass decoder for neuralin proceedings of the 2018machine translation.
conference on empirical methods in natural lan-guage processing, pages 523–532..yvette graham, barry haddow, and philipp koehn.
2019. translationese in machine translation evalu-ation.
corr, abs/1906.09833..shuhao gu, yang feng, and qun liu.
2019. improvingdomain adaptation translation with domain invariantand speciﬁc information.
in proceedings of the 2019conference of the north american chapter of theassociation for computational linguistics: humanlanguage technologies, volume 1 (long and shortpapers), pages 3081–3091..hany hassan, anthony aue, chang chen, vishalchowdhary,jonathan clark, christian feder-mann, xuedong huang, marcin junczys-dowmunt,william lewis, mu li, et al.
2018. achieving hu-man parity on automatic chinese to english newstranslation.
arxiv preprint arxiv:1803.05567..zhongjun he, hua wu, haifeng wang, et al.
2019.multi-agent learning for neural machine translation.
in proceedings of the 2019 conference on empiricalmethods in natural language processing and the9th international joint conference on natural lan-guage processing (emnlp-ijcnlp), pages 855–864..geoffrey hinton, oriol vinyals, and jeff dean.
2015.distilling the knowledge in a neural network.
arxivpreprint arxiv:1503.02531..2870cong duy vu hoang, gholamreza haffari, and trevorcohn.
2017. towards decoding as continuous opti-misation in neural machine translation.
in proceed-ings of the 2017 conference on empirical methodsin natural language processing, pages 146–156..søren johansen and katarina juselius.
1990. maxi-mum likelihood estimation and inference on coin-tegration with applications to the demand formoney.
oxford bulletin of economics and statistics,52(2):169–210..nal kalchbrenner and phil blunsom.
2013. recurrentin proceedings ofcontinuous translation models.
the 2013 conference on empirical methods in natu-ral language processing, pages 1700–1709..jason lee, elman mansimov, and kyunghyun cho.
2018. deterministic non-autoregressive neural se-in pro-quence modeling by iterative reﬁnement.
ceedings of the 2018 conference on empirical meth-ods in natural language processing, pages 1173–1182..jiwei li, will monroe, and dan jurafsky.
2017. learn-ing to decode for future success.
arxiv preprintarxiv:1701.06549..lemao liu, andrew finch, masao utiyama, andagreement on target-eiichiro sumita.
2016a.
bidirectional lstms for sequence-to-sequence learn-ing.
in thirtieth aaai conference on artiﬁcial in-telligence..lemao liu, masao utiyama, andrew finch, andagreement on target-eiichiro sumita.
2016b.
in pro-bidirectional neural machine translation.
ceedings of the 2016 conference of the north amer-ican chapter of the association for computationallinguistics: human language technologies, pages411–416..jan niehues, eunah cho, thanh-le ha, and alexwaibel.
2016. pre-translation for neural machinein proceedings of coling 2016, thetranslation.
26th international conference on computationallinguistics: technical papers, pages 1828–1836..myle ott, sergey edunov, alexei baevski, angelafan, sam gross, nathan ng, david grangier, andfairseq: a fast, extensiblemichael auli.
2019.in proceedings oftoolkit for sequence modeling.
naacl-hlt 2019: demonstrations..kishore papineni, salim roukos, todd ward, and wei-jing zhu.
2002. bleu: a method for automatic eval-uation of machine translation.
in proceedings of the40th annual meeting of the association for compu-tational linguistics, pages 311–318..weizhen qi, yu yan, yeyun gong, dayiheng liu,nan duan, jiusheng chen, ruofei zhang, and mingzhou.
2020. prophetnet: predicting future n-gramfor sequence-to-sequence pre-training.
in proceed-ings of the 2020 conference on empirical methodsin natural language processing: findings, pages2401–2410..rico sennrich, barry haddow, and alexandra birch.
2016. neural machine translation of rare words withsubword units.
in proceedings of the 54th annualmeeting of the association for computational lin-guistics (volume 1: long papers), volume 1, pages1715–1725..dmitriy serdyuk, nan rosemary ke, alessandro sor-doni, adam trischler, chris pal, and yoshua ben-gio.
2018. twin networks: matching the future forsequence generation..chenze shao, xilin chen, and yang feng.
2018.greedy search with probabilistic n-gram matchingin proceedings offor neural machine translation.
the 2018 conference on empirical methods in natu-ral language processing, pages 4778–4784..chenze shao, yang feng, jinchao zhang, fandongmeng, xilin chen, and jie zhou.
2019. retrievingsequential information for non-autoregressive neuralmachine translation.
in proceedings of the 57th an-nual meeting of the association for computationallinguistics, pages 3013–3024..chenze shao, jinchao zhang, yang feng, fandongmeng, and jie zhou.
2020. minimizing the bag-of-ngrams difference for non-autoregressive neural ma-chine translation.
in proceedings of the aaai con-ference on artiﬁcial intelligence, volume 34, pages198–205..ilya sutskever, oriol vinyals, and quoc v le.
2014.sequence to sequence learning with neural networks.
in advances in neural information processing sys-tems, pages 3104–3112..ashish vaswani, noam shazeer, niki parmar, jakobuszkoreit, llion jones, aidan n gomez, łukaszkaiser, and illia polosukhin.
2017. attention is allin advances in neural information pro-you need.
cessing systems, pages 5998–6008..ronald j williams.
1992. simple statistical gradient-following algorithms for connectionist reinforce-ment learning.
machine learning, 8(3-4):229–256..ronald j williams and david zipser.
1989. a learn-ing algorithm for continually running fully recurrentneural networks.
neural computation, 1(2):270–280..matt post.
2018. a call for clarity in reporting bleuscores.
in proceedings of the third conference onmachine translation: research papers, pages 186–191, belgium, brussels.
association for computa-tional linguistics..lijun wu, fei tian, tao qin, jianhuang lai, and tie-yan liu.
2018. a study of reinforcement learningin proceedings offor neural machine translation.
the 2018 conference on empirical methods in natu-ral language processing, pages 3612–3621..2871yonghui wu, mike schuster, zhifeng chen, quoc vle, mohammad norouzi, wolfgang macherey,maxim krikun, yuan cao, qin gao, klausmacherey, et al.
2016. google’s neural machinetranslation system: bridging the gap between hu-arxiv preprintman and machine translation.
arxiv:1609.08144..in proceedings of the aaai conference on artiﬁcialintelligence, volume 33, pages 443–450..zaixiang zheng, shujian huang, zhaopeng tu, xin-yudai, and jiajun chen.
2019. dynamic past and fu-ture for neural machine translation.
arxiv preprintarxiv:1904.09646..yingce xia, fei tian, lijun wu, jianxin lin, tao qin,nenghai yu, and tie-yan liu.
2017. deliberationnetworks: sequence generation beyond one-pass de-coding.
in advances in neural information process-ing systems, pages 1784–1794..zaixiang zheng, hao zhou, shujian huang, lili mou,xinyu dai, jiajun chen, and zhaopeng tu.
2018.modeling past and future for neural machine trans-lation.
transactions of the association for computa-tional linguistics, 6:145–157..long zhou, jiajun zhang, and chengqing zong.
2019a.
synchronous bidirectional neural machine transla-tion.
transactions of the association for computa-tional linguistics, 7:91–105..long zhou, jiajun zhang, chengqing zong, and hengyu.
2019b.
sequence generation: from both sides tothe middle.
proceedings of the twenty-eighth inter-national joint conference on artiﬁcial intelligence..zhen yang, wei chen, feng wang, and bo xu.
2018.improving neural machine translation with condi-tional sequence generative adversarial nets.
in pro-ceedings of the 2018 conference of the north amer-ican chapter of the association for computationallinguistics: human language technologies, vol-ume 1 (long papers), pages 1346–1355..lantao yu, weinan zhang, jun wang, and yong yu.
2017. seqgan: sequence generative adversarial netsin thirty-first aaai confer-with policy gradient.
ence on artiﬁcial intelligence..biao zhang, deyi xiong, jinsong su, and jiebo luo.
future-aware knowledge distillation for2019a.
ieee/acm transac-neural machine translation.
tions on audio, speech, and language processing,27(12):2278–2287..jiajun zhang, long zhou, yang zhao, and chengqingzong.
2019b.
synchronous bidirectional inferencearxiv preprintfor neural sequence generation.
arxiv:1902.08955..ruiyi zhang, changyou chen, zhe gan, wenlin wang,dinghan shen, guoyin wang, zheng wen, andlawrence carin.
2020a.
improving adversarial textin pro-generation by modeling the distant future.
ceedings of the 58th annual meeting of the asso-ciation for computational linguistics, pages 2516–2531..shaolei zhang, yang feng, and liangyou li.
2020b.
future-guided incremental transformer for simulta-neous translation.
arxiv preprint arxiv:2012.12465..wen zhang, yang feng, fandong meng, di you, andqun liu.
2019c.
bridging the gap between train-ing and inference for neural machine translation.
inproceedings of the 57th annual meeting of the asso-ciation for computational linguistics, pages 4334–4343..xiangwen zhang, jinsong su, yue qin, yang liu, ron-grong ji, and hongji wang.
2018. asynchronousbidirectional decoding for neural machine transla-in thirty-second aaai conference on artiﬁ-tion.
cial intelligence..zhirui zhang, shuangzhi wu, shujie liu, mu li, mingzhou, and tong xu.
2019d.
regularizing neural ma-chine translation by target-bidirectional agreement..2872