generating landmark navigation instructions from maps as agraph-to-text problem.
raphael schumanncomputational linguisticsheidelberg university, germany.
stefan riezlercomputational linguistics & iwrheidelberg university, germany.
{rschuman|riezler}@cl.uni-heidelberg.de.
abstract.
car-focused navigation services are based onturns and distances of named streets, whereasnavigation instructions naturally used by hu-mans are centered around physical objectscalled landmarks.
we present a neural modelthat takes openstreetmap representations asinput and learns to generate navigation in-structions that contain visible and salient land-marks from human natural language instruc-tions.
routes on the map are encoded in alocation- and rotation-invariant graph repre-sentation that is decoded into natural languageinstructions.
our work is based on a noveldataset of 7,672 crowd-sourced instances thathave been veriﬁed by human navigation instreet view.
our evaluation shows that the nav-igation instructions generated by our systemhave similar properties as human-generated in-structions, and lead to successful human navi-gation in street view..1.introduction.
current navigation services provided by the au-tomotive industry or by google maps generateroute instructions based on turns and distances ofnamed streets.
in contrast, humans naturally usean efﬁcient mode of navigation based on visibleand salient physical objects called landmarks.
asshown by tom and denis (2004), route instructionsbased on landmarks are easier processed and mem-orized by humans.
may et al.
(2003) recommendthat in pedestrian navigation systems, ”landmarksshould be used as the primary means of providingdirections”.
another navigation scenario wherelandmarks are useful is if gps tracking is poor ornot available, and if information is inexact regard-ing distances (e.g., in human estimates) or streetnames (e.g., for users riding a bicycle).
we presenta neural model that takes a real-world map repre-.
sentation from openstreetmap1 as input and gen-erates navigation instructions that contain salientlandmarks, learned directly from human naturallanguage instructions..in our framework, routes on the map are learnedby discretizing the street layout, connecting streetsegments with adjacent points of interest, thus en-coding visibility of landmarks, and encoding theroute and surrounding landmarks in a location- androtation-invariant graph.
based on crowd-sourcednatural language instructions for such map rep-resentations, a graph-to-text mapping is learnedthat decodes graph representations into natural lan-guage route instructions that contain salient land-marks.
our work is accompanied by a dataset of7,672 instances of routes in openstreetmap andcorresponding crowd-sourced natural language in-structions.
the navigation instructions were gen-erated by workers on the basis of maps includingall points of interest, but no street names.
theywere veriﬁed by different workers who followedthe navigation instructions on google street view2.
experimental results on randomly sampled testroutes show that our graph-to-text model produceslandmarks with the same frequency found in hu-man reference instructions.
furthermore, the time-normalized success rate of human workers ﬁndingthe correct goal location on street view is 0.664.since these routes can have a partial overlap withroutes in the training set, we further performed anevaluation on completely unseen routes.
the rateof produced landmarks drops slightly comparedto human references, and the time-normalized suc-cess rate also drops slightly to 0.629. while thereis still room for improvement, our results show-case a promising direction of research, with a widepotential of applications in various existing map.
1www.openstreetmap.org2www.google.com/streetview.
proceedingsofthe59thannualmeetingoftheassociationforcomputationallinguisticsandthe11thinternationaljointconferenceonnaturallanguageprocessing,pages489–502august1–6,2021.©2021associationforcomputationallinguistics489figure 1: the data collection is split into two tasks.
in the navigation instructions task (top) annotators see arendered map and write instructions to follow the route.
the navigation run task (bottom) is used to validatenavigation instructions.
a different annotator tries to ﬁnd the goal location in street view..applications and navigation systems..the main contributions of this paper are:.
• we collect and publish a large scale dataset ofnatural language landmark navigation instruc-tions that are validated by human navigation runsin street view..• we present a method to represent geospatialroutes as a graph and propose an appropriategraph-to-text architecture that learns to generatenavigation instructions from real-world data..street view in order to ﬁnd a hidden teddy bear.
the data for that task is obtained from annotationworkers that follow a predeﬁned route in streetview and write down navigation instructions alongthe way.
a central difference between touchdownand our dataset is the annotation modality: touch-down annotators use panorama images along theroute, while our instruction writers only see therendered route on a map.
see section 4.3 for amore detailed discussion..2 related work and datasets.
mirowski et al.
(2018) published a subset of streetview covering parts of new york city and pitts-burgh.
street view is a navigable environment thatis build from connected real-world 360◦panoramas.
this data is used by hermann et al.
(2020) to traina visual agent to follow turn-by-turn instructionsgenerated by google maps api.
chen et al.
(2019)published a street view dataset3 with more recentand higher resolution panorama images that coversthe lower half of manhattan.
they further introducethe touchdown task that has the goal to navigate.
3www.streetlearn.cc.
our work puts the task of natural language navi-gation upside down by learning to generate human-like navigation instructions from real-world mapdata instead of training an agent to follow humangenerated instructions.
prior work in this areahas used rule-based systems to identify landmarks(rousell and zipf, 2017) or to generate landmark-based navigation instructions (dr¨ager and koller,2012; cercas curry et al., 2015).
despite havingall points of interest on the map available, our ap-proach learns to verbalize only those points of inter-est that have been deemed salient by inclusion in ahuman navigation instruction.
previous approachesthat learn navigation instructions from data have.
490been conﬁned to simpliﬁed grid-based representa-tions of maps for restricted indoor environments(daniele et al., 2017).
de vries et al.
(2018) tacklesthe problem in a more sophisticated outdoor en-vironment but the model fails to verbalize usefulinstructions when conditioned on more than onepossible landmark.
other work generates naviga-tion instructions from indoor panoramas along apath but provides no explicit evaluation like humannavigation success.
they rather use the instruc-tions to augment the training routes for a visionand language navigation agent (fried et al., 2018)..3 task.
the task addressed in our work is that of au-tomatically generating natural language land-mark navigation instructions (nllni) from real-world open-source geographical data from open-streetmap.
the instructions are generated a pri-ori (janarthanam et al., 2012) for the whole route.
training data for nllni was generated by humancrowdsourcing workers who were given a route onan openstreetmap rendering of lower manhattan,with the goal of producing a succinct natural lan-guage instruction that does not use street names orexact distances, but rather is based on landmarks.
landmarks had to be visible on the map and in-cluded, e.g., churches, cinemas, banks, shops, andpublic amenities such as parks or parking lots.
eachgenerated navigation instruction was validated byanother human crowdsourcing worker who had toreach the goal location by following the instructionon google street view..nllni outputs are distinctively different fromnavigation instructions produced by openroute-service, google maps, or car navigation systems.
while these systems rely on stable gps signalssuch that the current location along a grid of streetscan be tracked exactly, we aim at use cases wheregps tracking is not available, and knowledge ofdistances or street names is inexact, for example,pedestrians, cyclists, or users of public transporta-tion.
the mode of nllni is modeled after humannavigation instructions that are naturally based on asmall number of distinctive and visible landmarksin order to be memorizable while still being in-formative enough to reach the goal.
a further ad-vantage of nllni is that they are based on mapinputs which are more widely available and lesstime dependent than street view images..4 data collection.
because there is no large scale dataset for nllnithat is generated from map information only, wecollect data via crowdsourcing.
the annotator isshown a route on the map and writes navigation in-structions based on that information (figure 1, top).
we take the approach of chen et al.
(2019) anddetermine correctness of navigation instructions byshowing them to other annotators that try to reachthe goal location in street view (figure 1, bottom)..4.1 resources and preparation.
we use the static street view dataset provided bychen et al.
(2019).
this allows us to make theexperiments in this work replicable.
because thepanorama pictures were taken at the end of 2017,we export an openstreetmap extract of manhat-tan from that time.
openstreetmap (osm) is anopen source collection of geodata that can be usedto render maps of the world.
it features detailedstreet layouts and annotations for points of interest(poi) like amenities, infrastructure or land use4.
we discretize the street layout by creating a nodeevery ten meters along the roads.
the resultingstructure is further referenced to as the osm graphwith nodes consisting of street segments.
based onthat graph, we sample routes of length between 35and 45 nodes.
a route is the shortest path betweenits start and end node.
it includes a minimum ofthree intersections (i.e., a node with more than twoedges) and ends in proximity to a poi.
we furtherassure that it is possible to follow the route in streetview by verifying that a corresponding subgraphexists in the street view graph..4.2 crowdsourcingwe use amazon mechanical turk (amt)5 to ac-quire annotators.
before working on the actualtasks, workers were required to pass a tutorial andqualiﬁcation test.
the tutorial introduces the tasks,teaches basic mechanics of street view and ex-plains meaning of map icons.
a feature of amtand additional ip address6 lookup ensures that an-notators are located in the united states.
this in-creases the probability of working with native en-glish speakers and people familiar with us streetenvironments.
we paid $0.35 per navigation in-structions task and $0.20 for the navigation run.
4openstreetmap.org/wiki/map_features5www.mturk.com6ip addresses were not saved and are not part of the dataset..491datasettalk the walkroom-to-roomtouchdowntalk2navroom-x-roommap2seq.
#instructions environment data source3d renderingpanoramaspanoramaspanoramas and mappanoramasmap.
gridworldindooroutdooroutdoorindooroutdoor.
78621,5679,32610,714126,0697,672.
#nodes avg.
length vocabulary avg.
tokens34.529.089.668.878.055.1.
10010,80029,64121,23310,80029,641.
5873,1564,9995,240388k3,826.
6.86.035.240.07.040.0.table 1: overview of natural language navigation instructions datasets.
the instructions in our dataset rely solelyon information present in openstreetmap.
dataset: talk the walk (macmahon et al., 2006); room-to-room (an-derson et al., 2018b); touchdown (chen et al., 2019); talk2nav (vasudevan et al., 2020); room-x-room (kuet al., 2020); map2seq (this work).
#instructions: number of instructions in the dataset.
environment: type ofnavigation environment.
data source: type of information the annotator uses to write the navigation instructions.
#nodes: number of nodes in the discretized environment.
avg.
length: average number of nodes per route.
vocabulary: number of unique tokens in the instructions.
avg.
tokens: number of tokens per route instruction..phenomenon.
reference to unique entitycoreferencecomparisonsequencingcountallocentric spatial relationegocentric spatial relationimperativedirectiontemporal conditionstate veriﬁcation.
r-to-r touchdown map2seqc258144520252272.µ9.21.10.11.60.41.23.65.23.71.91.5.µ6.30.50.01.80.60.53.25.33.50.30.6.µ3.70.50.00.20.20.21.24.02.80.40.1.c25153219172325242118.c258024119252525712.example.
... turn right where dough boys is on the corner ...... is a bar, landmark tavern, stop outside of it ...... there are two lefts, take the one that is not sharp ...... continue straight at the next intersection ...... go through the next two lights ...... go through the next light with citibank at the corner.
...... at the end of the park on your right...... head down the block and go through the double lights ...... head straight to the light and make a right ...... go straight until you come to the end of a garden area ...... you should see bike rentals on your right ....table 2: linguistic analysis of 25 randomly sampled navigation instructions.
numbers for room-to-room (an-derson et al., 2018b) and touchdown (chen et al., 2019) taken from the latter.
c is the number of instructions outof the 25 which contain the phenomenon at least once.
µ is the mean number of times each phenomenon occurs..task.
furthermore, we paid a bonus of $0.15 forsuccessfully reaching the goal location and $0.25for validated navigation instructions.
the amountswere chosen on the basis of $10/hour.
the anno-tation procedure involved two phases.
first, anannotator wrote navigation instructions for a givenroute.
afterwards, a different annotator used the in-structions to navigate to the goal location.
if one oftwo annotators did so successfully, the navigationinstructions were considered valid..navigation instructions task as shown in fig-ure 1 (top), the annotator sees a route on a mapwhich is rendered without street names.
workerswere told to write navigation instructions as if ”atourist is asking for directions in a neighborhoodyou are familiar with” and to ”mention landmarksto support orientation”.
the navigation instructionswere written in a text box below the map which islimited to 330 characters..navigation run task figure 1 (bottom) showsthe street view interface with navigation instruc-tions faded-in at the bottom.
it is possible to lookaround 360◦ and movement is controlled by the.
white arrows.
in addition there is a button on thebottom left to backtrack which proved to be veryhelpful.
the initial position is the start of the routefacing in the correct direction.
the annotators ﬁn-ish the navigation run with the bottom right buttoneither when they think the goal location is reachedor if they are lost.
the task is successful if theannotator stops the run within a 25 meter radiusaround the goal location..4.3 dataset.
the data collection resulted in 7,672 navigationinstructions that were manually validated instreet view.
for additional 1,059 instructions, thevalidation failed, which amounts to a validationrate of 88%.
of the validated instructions, 1,033required a second try in the navigation run task.
onaverage, instructions are 257 characters long, witha minimum length of 110, and a maximum of 330characters.
we release the segmented osm graph,the routes in that graph paired with the collectednavigation instructions, and the data split used in.
492figure 2: graph representation of the route in figure 3. the framed middle part is magniﬁed for readability.
somenodes are left out for sake of clear visualization.
also, node colors are for visualization only and not encoded inthe graph.
green nodes are part of the route.
blue nodes are neighboring street segments.
orange nodes belong toosm points of interest.
angles are relative to route direction and start clockwise at 0◦ which is facing forward..the starting direction and can infer the facing di-rection for the rest of the route.
because the initialfacing direction in touchdown is random, the ﬁrstpart of their instructions is about rotating the agent.
this explains the higher number of occurrences ofthe state veriﬁcation phenomenon.
in our dataset,state veriﬁcation is usually used to ensure the cor-rect stopping position.
the different setting of datacollection is also reﬂected by the temporal condi-tion phenomenon.
annotators of touchdown writedown instructions while navigating street view andthus experience the temporal component ﬁrst hand,while our annotators have a time independent lookat the route..5 method.
the underlying osm geodata of the rendered mapis an xml tree of nodes located in the latitude-longitude coordinate system.
the nodes are com-posed into ways and polygons8.
these elementsin connection with their annotations are used torender the visual map.
in the next subsection wepropose our approach to represent a route and itssurrounding map features as a graph that includesall necessary information for generating landmarknavigation instructions.
the second subsection de-scribes the neural graph-to-text architecture thatis trained to learn inductive representations of theindividual route graphs and to decode navigationinstructions from them..5.1 map-to-graph representation.
the basis of the graph for a single route is theosm subgraph (section 4.1) that includes the ac-.
8www.openstreetmap.org/wiki/elements.
figure 3: route rendered on the map with street seg-ments and landmark visibility..our experiments7.
table 1 gives a comparison ofdifferent datasets with natural language landmarknavigation instructions.
our dataset is the only onethat uses only map information to generate naviga-tion instructions.
the advantage of relying solelyon map data is the global availability and longevityof the encoded features.
in contrast, navigationinstructions written from street view include tem-porary features like construction utilities, streetadvertisements, or passing vehicles.
table 2 showsa qualitative linguistic analysis of the navigationinstructions of different datasets.
in general, navi-gation instructions are driven by giving directionsin imperative formulation while referencing to en-tities along the route.
in contrast to the touchdowntask where including store names was prohibited,the entities in our instructions are often referencedto by their name.
although the instruction writersin our setting did not see the route in ﬁrst personperspective, objects are vastly referenced to in ego-centric manner (egocentric with respect to the navi-gating agent).
this is because the annotator knows.
7www.cl.uni-heidelberg.de/.
statnlpgroup/map2seq/.
493<street><8><street><9><street><neighbor><street><neighbor><street><10><street><11><street><last><street><neighbor><street><neighbor>359°0°0°6°91°271°359°0°88°<street><5><street><7><street><6><street><neighbor><poi><poi><k_name_1>gramercy<k_name_2>park1°3°<tag_key>leisure<tag_value>park269°90°133°90°90°90°35°270°<k_name_1>big<poi><poi><tag_key>amenity<k_name_2>daddy's<tag_value>restaurant332°214°<street><1><street><2><street><3><street><4><street><neighbor><poi><poi><tag_key>amenity0°1°358°<tag_value>place_of_worship89°0°<k_name_1>the<k_name_2>brotherhood<k_name_3>synagogue270°321°219°<poi><poi><tag_key>amenity<k_name_1>barﬂy<tag_value>bar63°90°<poi><poi><tag_key>amenity<tag_value>bicycle_rental90°90°90°node tokennode typedirected edge90°directed edge with angletual route nodes.
further, neighboring street seg-ment nodes are added.
this is depicted in figure 3as green and blue circles, respectively.
in order todecide on the visibility of the pois, we employ atechnique similar to that of rousell and zipf (2017).
for each street segment, the pois in a radius of 30meters are identiﬁed.
if a line drawn between thestreet segment and the poi is not interrupted bya building polygon, the poi is considered visiblefrom that particular street segment.
if the poi it-self is (inside) a polygon, then the line is drawn tothe closest point on the poi polygon.
the orangecircles in figure 3 show the results of the visibil-ity check and how they naturally ﬁt into the graphstructure.
each point of interest in osm has one ormore tags in the form of key and value pairs.
theystore properties like type or name.
note that weonly determine the geometric visibility of the poisand do not incorporate any hand-crafted saliencescores as to what would be a good landmark.
in-stead, saliency of a landmark is implicitly learnedfrom natural language verbalization of the poi inthe human-generated instruction..an example graph representation of the route infigure 3 is given in figure 2. formally, a route rep-resentation is a directed graph g = (v, e), wherev denotes the set of nodes and e the set of edges.
anode v consists of a node type vt and a node tokenvw.
there are v t node types and v w node tokens.
street segments are of type <street>.
a point ofinterest has the node type <poi>.
an osm tagkey has the node type <tag key> and an osm tagvalue has the node type <tag value>.
the nodetoken further speciﬁes nodes in the graph.
streetsegments that belong to the route have a node token<p> according to their sequential position p. thelast route segment has the special token <last>.
other street segment nodes have the <neighbor>token.
the actual key and value literals of an osmtag are the node tokens of the respective node.
theosm name tag is split into multiple nodes withtype <k name n> where n is the word positionand the node token is the word at that position..all adjacent street segment nodes are connectedwith an edge in both directions.
if a poi is visiblefrom a particular street segment, there is an edgefrom the corresponding poi node to that streetsegment node.
each poi node is connected withtheir tag key nodes.
a tag value node is connectedto its corresponding tag key node.
the name tagnodes of the same poi are connected with each.
other.
some edges have a geometric interpretation.
this is true for edges connecting a street segmentwith either a poi or with another street segment.
these edges (u, v) ∈ ea, ea ⊂ e have a labelattached.
the label ang(u, v) is the binned anglebetween the nodes relative to route direction.
thecontinuous angle [0◦, 360◦) is assigned to one of 12bins.
each bin covers 30◦ with the ﬁrst bin startingat 345◦.
the geometric distance between nodes isnot modeled explicitly because street segments areequidistant and poi visibility is determined with amaximum distance.
the proposed representation ofa route and its surroundings as a directed graph withpartially geometric edges is location- and rotation-invariant, which greatly beneﬁts generalization..5.2 graph-to-text architecture.
by representing a route as a graph, we can framethe generation of nllni from maps as a graph-to-text problem.
the encoder learns a neural rep-resentation of the input graph and the sequencedecoder generates the corresponding text.
the ar-chitecture follows the transformer (vaswani et al.,2017) but uses graph attentional layers (veliˇckovi´cet al., 2018) in the encoder.
graph attention in-jects the graph structure by masking (multi-head)self-attention to only attend to nodes that are ﬁrst-order neighbors in the input graph.
the geomet-ric relations between some nodes are treated asedge labels which are modeled by distinct fea-ture transformation matrices during node aggre-gation (schlichtkrull et al., 2018)..the input to a layer of the encoder is a set of noderepresentations, x = {x1, x2, .
.
.
, xn }, xi ∈rdm, where n is the number of nodes and dmis the model size.
each layer l : rdm → rdmtakes x and produces new node representations x(cid:48).
the input to the ﬁrst layer is constructed from theconcatenation of type and token embedding: xi =]) where w f ∈ r2dm×dmrelu (w f [etvtiis a weight matrix, et ∈ rdm and ew ∈ rdmare embedding matrices for node types and nodetokens, respectively..||ewvwi.the output of a single graph attention head is theweighted sum of neighboring node representations:.
(cid:88).
¯xi =.
j|(vj ,vi)∈e.
αij(w u.r(i,j)xj).
(1).
the weight coefﬁcient is computed as αij =k|(vk ,vi)∈e exp (eik) where eijsoftmaxj(eij) =.
exp (eij ).
(cid:80).
494bleu len.
landm.
sdtw sr snt200 instances test set.
bleu len.
landm.
sdtw sr snt200 instances test set.
referencerule basedseq2seqgraph2textg2t+pretrain.
-0.7113.1218.6018.81.referenceg2t+pretrain.
-17.39.
53.553.152.952.652.5.
53.553.0.
2.7612.441.952.412.44.
2.722.41.
.728.405.139.475.471.
.726.475.
.855.460.160.540.540.
.861.551.
.878.455.206.676.537.
.830.664.referencerule basedseq2seqgraph2textg2t+pretrain.
-0.6711.1214.0715.64.referenceg2t+pretrain.
-16.27.
57.552.351.850.550.3.
54.253.2.
2.6810.961.581.742.33.
2.692.30.
.725.472.074.344.367.
.727.407.
.824.525.100.400.429.
.843.473.
.791.512.137.534.530.
.807.629.
700 instances test set.
700 instances test set.
table 3: evaluation of navigation instructions producedby models and human reference on partially seen testroutes.
evaluation metrics are explained in section 6.3..table 4: evaluation of navigation instructions producedby models and human reference on unseen test routes..measures the compatibility of two node representa-tions:.
eij = leakyrelu (at [w v xi||w u.r(i,j)xj]).
(2).
where a ∈ r2dh, w v ∈ rdm×dh, dh = dm/h isthe attention head dimension and h is the numberof heads.
in the case of a geometric relation be-r(i,j) ∈ rdm×dhtween nodes, the weight matrix w uis selected according to the angle label between thenodes: r(i, j) = ang(ui, uj), otherwise r(i, j) =unlabeled.
the output of each head is concate-nated and after a skip connection forwarded to thenext encoder layer.
the encoder layer is appliedl times and the ﬁnal node representations x∗ areused in the decoder context attention mechanism.
thus, no modiﬁcation of the transformer decoderis necessary and l decoder layers are used.
further,the decoder can copy node tokens from the inputinto the output sequence (see et al., 2017)..the described architecture is able to model allaspects of the input graph.
graph attention modelsdirected edges.
edge labels model the geometricrelation between nodes.
heterogeneous nodes arerepresented by their type embedding and token em-bedding.
the sequentiality of the route is encodedby tokens (<1>, <2>, ...) of the respective nodes.
this is analogous to absolute position embeddingswhich provide word order information for text en-coding (vaswani et al., 2017; devlin et al., 2019)..6 experiments.
6.1 baselines.
we consider two baselines.
a rule based systemthat uses a single heuristic to construct instructionsby stringing together all pois and intersectionsalong the route, and following each intersectionby the turning direction.
similar, pois are fol-lowed by ’left’ or ’right’ depending on which side.
of the street they appear.
the end of the routeis signaled by the ’stop’ token.
the second base-line is a seq2seq (sequence-to-sequence) modelthat is trained on pairs of rule based navigationinstructions and crowdsourced instructions.
theseq2seq model follows the transformer architec-ture (vaswani et al., 2017) with copy mechanismand is trained with the same hyperparameters asthe graph-to-text model.
examples are given infigure 4..6.2 experimental setup.
we construct a graph for each route as describedabove.
on average there are 144 nodes in a graphand 3.4 edges per node.
there are 8 different nodetypes and a vocabulary of 3,791 node tokens.
thehyperparameters for the graph-to-text architectureare set as follows: the embedding and hidden sizeis set to 256. we use 6 encoder and decoder lay-ers with 8 attention heads.
cross entropy loss isoptimized by adam (kingma and ba, 2015) with alearning rate of 0.5 and batch size of 12. the em-bedding matrix for node tokens and output tokens isshared.
additionally we experiment with pretrain-ing the graph-to-text model with above mentionedrule based instructions as target.
this teaches themodel sequentiality of route nodes and basic in-terpretation of the angle labels.
we generate 20kinstances for pretraining and further ﬁne tune onthe human generated instances.
both models andthe seq2seq baseline are trained on 5,667 instancesof our dataset.
the best weights for each model areselected by token accuracy based early stopping onthe 605 development instances..6.3 evaluation metrics.
bleu is calculated with sacrebleu (post, 2018)on lower-cased and tokenized text.
length is the average length in number of tokens.
landmarks is the number of landmark occur-.
495reference: at the light with fridays on the corner, turn right.
continue down the long street to the next light with nine west on the right corner, then turn left.
goto the next light with brooks brothers on the right corner, then turn right and stop.
rule based: starbucks coffee left subway entrance right best buy mobile left yankees right bus stop left bus stop left light right the michelangelo left tgi fridaysleft pizza hut left bobby van ’s left park right men ’s wearhouse left fountain left fountain left subway entrance left light left nine west right rockefeller centerleft subway entrance right brooks brothers right light right stopseq2seq: go straight to the light and make a left.
go straight to the next light and make a left.
go straight to the light and make a right.
stop one step after turningwith brooks brothers to your right.
graph2text: walk to the light with tgi fridays on the corner and turn right.
walk down the long block to the next light with nine west on the left corner, then turnleft.
walk to the next light with brooks brothers on the far right corner, then turn right.
g2t+pretrain: turn right at the ﬁrst set of lights with tgi fridays on the left corner.
pass a park on the right and turn left at the lights.
pass the fountain on theright and turn right at the lights.
take two steps and stop.
brooks brothers is on the right corner..figure 4: route from partially seen test set paired with instructions generated by different systems..rences per instance.
occurrences are identiﬁedby token overlap between navigation text and tagvalues of pois along the route.
e.g., landmarksin the instructions in figure 1 are: dunkin donuts,bubble tea & crepes, chipotle, broadway hotel.
sdtw is success weighted by normalized dy-namic time warping (ilharco et al., 2019).
dis-tance between two nodes is deﬁned as meters alongthe shortest path between the two nodes and thresh-old distance is 25 meters.
sr is the ﬁrst try success rate in the navigation runtask.
success is achieved if the human navigatorstops within a radius of 25 meters around the goal.
snt is success weighted by navigation time:1, where si is a binary success indi-ncator that is 1 if the annotator stops within a 25meter radius around the goal.
ti is the time un-til the navigation run is ﬁnished.
we empiricallyestimate the expected navigation time ¯ti as 1.3 sec-onds9 per node in the route.
this estimation rangesfrom 45.5 seconds for routes with 35 nodes to 58.5seconds for routes with 45 nodes.
snt is inspiredby spl (anderson et al., 2018a) but considers tra-jectory time instead of trajectory length..i=1 si.
(cid:80)n.¯titi.
6.4 experimental results and analysis.
results of our experimental evaluation are shownin table 3 and 4. we evaluate on unseen data,i.e., routes without any overlap with routes in thetraining set, and on partially seen data, i.e., routes.
randomly sampled from the training area with par-tial overlaps.10 for the baseline models we performthe human evaluation on a 200 instances subset ofthe full 700 instances test set..on the partially seen test set with 200 instances,our proposed graph-to-text models outperform thebaseline models in terms of the success based met-rics.
in the unseen setup, the rule based baselineachieves a better success rate, but falls short whensuccess is weighted by navigation time.
this resultshows that the instructions generated by the rulebased system are exact by including all possiblelandmarks, but obviously do not resemble naturallanguage and high evaluation time suggests thatthey are hard to read.
despite moderate bleuscores and reasonable amount of produced land-marks, the seq2seq baseline fails to generate usefulnavigation instructions.
the pretrained graph-to-text model performs better than its plain counter-part in the unseen setup.
it produces more correctlandmarks and higher success rates.
in the extendedevaluation the pretrained graph-to-text model iscompared with the reference on 700 instances ineach test set.
under the central evaluation metricof success normalized by time (snt), our modelreaches .664 and .629 on partially seen and unseentest data, respectively..an example output for each system togetherwith the input map is shown in figure 4. the rulebased instruction is complete, but ignores saliency.
9average over all successful navigation runs in the dataset..10the data split is shown in the appendix..496of landmarks and is hard to read.
the seq2seq base-line generates a navigation instruction that soundshuman-like and also includes salient landmarksfound on the map.
however, the directions are in-correct in this example.
the graph-to-text basedmodels get the directions right and produce ﬂuentnatural language sentences.
they include land-marks at the correct sequential position.
a furtherqualitative evaluation of instructions generated bythe graph-to-text models is given in the appendix..7 conclusion.
we presented a dataset and suitable graph-to-text ar-chitecture to generate landmark navigation instruc-tions in natural language from openstreetmap ge-ographical data.
our neural model includes novelaspects such as a graphical representation of a routeusing angle labels.
our dataset consists of a fewthousand navigation instructions that are veriﬁedfor successful human navigation.
the dataset islarge enough to train a neural model to producenavigation instructions that are very similar in sev-eral aspects to human-generated instructions onpartially seen test data.
however, performance nat-urally drops on unseen data including new types oflandmarks in new combinations..acknowledgments.
we would like to thank christian buck and mas-similiano ciaramita for initial fruitful discussionsabout this work.
the research reported in this pa-per was supported by a google focused researchaward on ”learning to negotiate answers in multi-pass semantic parsing”..references.
peter anderson, angel x. chang, devendra singhchaplot, alexey dosovitskiy, saurabh gupta,jitendra ma-vladlen koltun,lik, roozbeh mottaghi, manolis savva,andamir roshan zamir.
2018a.
on evaluation of em-bodied navigation agents.
corr, abs/1807.06757..jana kosecka,.
peter anderson, qi wu, damien teney, jake bruce,mark johnson, niko s¨underhauf, ian reid, stephengould, and anton van den hengel.
2018b.
vision-interpreting visually-and-language navigation:grounded navigation instructions in real environ-ments.
in proceedings of the ieee conference oncomputer vision and pattern recognition (cvpr)..amanda cercas curry, dimitra gkatzia, and verenarieser.
2015. generating and evaluating landmark-based navigation instructions in virtual environ-.
ments.
in proceedings of the 15th european work-shop on natural language generation (enlg),pages 90–94, brighton, uk.
association for com-putational linguistics..h. chen, a. suhr, d. misra, n. snavely, and y. artzi.
2019. touchdown: natural language navigationand spatial reasoning in visual street environments.
in 2019 ieee/cvf conference on computer vi-sion and pattern recognition (cvpr), pages 12530–12539..andrea f. daniele, mohit bansal, and matthew r.walter.
2017. navigational instruction generationas inverse reinforcement learning with neural ma-2017 12th acm/ieee inter-chine translation.
national conference on human-robot interaction(hri, pages 109–118..jacob devlin, ming-wei chang, kenton lee, andkristina toutanova.
2019. bert: pre-training ofdeep bidirectional transformers for language under-in proceedings of the 2019 conferencestanding.
of the north american chapter of the associationfor computational linguistics: human languagetechnologies, volume 1 (long and short papers),pages 4171–4186, minneapolis, minnesota.
associ-ation for computational linguistics..markus dr¨ager and alexander koller.
2012. genera-tion of landmark-based navigation instructions fromopen-source data.
in proceedings of the 13th con-ference of the european chapter of the associationfor computational linguistics, pages 757–766, avi-gnon, france.
association for computational lin-guistics..daniel fried, ronghang hu, volkan cirik, annarohrbach, jacob andreas, louis-philippe morency,taylor berg-kirkpatrick, kate saenko, dan klein,and trevor darrell.
2018. speaker-follower modelsfor vision-and-language navigation.
in advances inneural information processing systems, volume 31,pages 3314–3325.
curran associates, inc..karl moritz hermann, mateusz malinowski, piotrmirowski, andras banki-horvath, keith anderson,and raia hadsell.
2020. learning to follow di-rections in street view.
proceedings of the aaaiconference on artiﬁcial intelligence, 34(07):11773–11781..gabriel ilharco, vihan jain, alexander ku, eugeneie, and jason baldridge.
2019. effective and gen-eral evaluation for instruction conditioned naviga-tion using dynamic time warping.
neurips visuallygrounded interaction and language workshop..srinivasan janarthanam, oliver lemon, and xingkunliu.
2012. a web-based evaluation framework forin proceedingsspatial instruction-giving systems.
of the acl 2012 system demonstrations, pages 49–54, jeju island, korea.
association for computa-tional linguistics..497arun balajee vasudevan, dengxin dai, and luc vantalk2nav: long-range vision-and-gool.
2020.language navigation with dual attention and spatialmemory..ashish vaswani, noam shazeer, niki parmar, jakobuszkoreit, llion jones, aidan n gomez, ł ukaszkaiser, and illia polosukhin.
2017. attention is allyou need.
in i. guyon, u. v. luxburg, s. bengio,h. wallach, r. fergus, s. vishwanathan, and r. gar-nett, editors, advances in neural information pro-cessing systems 30, pages 5998–6008.
curran asso-ciates, inc..petar veliˇckovi´c, guillem cucurull, arantxa casanova,adriana romero, pietro li`o, and yoshua bengio.
in international2018. graph attention networks.
conference on learning representations..harm de vries, kurt shuster, dhruv batra, devi parikh,jason weston, and douwe kiela.
2018. talk thewalk: navigating new york city through groundeddialogue.
corr, abs/1807.03367..diederik p. kingma and jimmy ba.
2015. adam:in iclr.
a method for stochastic optimization.
(poster)..alexander ku, peter anderson, roma patel, eugeneie, and jason baldridge.
2020. room-across-room:multilingual vision-and-language navigation withdense spatiotemporal grounding.
in conference onempirical methods for natural language process-ing (emnlp)..matt macmahon, brian stankiewicz, and benjaminkuipers.
2006. walk the talk: connecting lan-guage, knowledge, and action in route instructions.
in proceedings of the 21st national conference onartiﬁcial intelligence - volume 2, aaai’06, page1475–1482.
aaai press..andrew j. may, tracy ross, steven h. bayer, andpedestrian naviga-mikko j. tarkiainen.
2003.tion aids: information requirements and design im-plications.
personal and ubiquitous computing,7(6):331–338..piotr mirowski, matt grimes, mateusz malinowski,karl moritz hermann, keith anderson, denisteplyashin, karen simonyan, koray kavukcuoglu,andrew zisserman, and raia hadsell.
2018. learn-ing to navigate in cities without a map.
in s. bengio,h. wallach, h. larochelle, k. grauman, n. cesa-bianchi, and r. garnett, editors, advances in neu-ral information processing systems 31, pages 2419–2430. curran associates, inc..matt post.
2018. a call for clarity in reporting bleuscores.
in proceedings of the third conference onmachine translation: research papers, pages 186–191, belgium, brussels.
association for computa-tional linguistics..adam rousell and alexander zipf.
2017. towardsa landmark-based pedestrian navigation service us-ing osm data.
isprs international journal of geo-information, 6(3):64..michael schlichtkrull, thomas n. kipf, peter bloem,rianne van den berg, ivan titov, and max welling.
2018. modeling relational data with graph convolu-tional networks.
in the semantic web, pages 593–607, cham.
springer international publishing..abigail see, peter j. liu, and christopher d. manning.
2017. get to the point: summarization with pointer-generator networks.
in proceedings of the 55th an-nual meeting of the association for computationallinguistics (volume 1: long papers), pages 1073–1083, vancouver, canada.
association for computa-tional linguistics..ariane tom and michel denis.
2004. language andspatial cognition: comparing the roles of landmarksand street names in route instructions.
applied cog-nitive psychology, 18(9):1213–1230..498appendices.
a dataset split.
figure 6: navigation success rate in respect of routelength.
length is measured in number of nodes in aroute..shows that the length of the route has little inﬂu-ence on the navigation success rate on the partiallyseen test set.
on the unseen data there is tendencyin favor of shorter routes for the g2t+pretrain model.
the reference instructions do not show such bias.
figure 8 shows navigation success with respect tonumber of turns in a route which is another com-plexity indicator.
the success rate drops with anincreasing number of turns for all systems but notfor the reference instructions.
the analysis revealsthat performance of our model drops with increas-ing route complexity while it is stable for referenceinstructions.
the rule based system appears to bemore stable with increasing number of turns incomparison to the learned models..c landmarks.
table 5 and 6 presents a scoring of types of land-marks produced by our pretrained model.
a com-parison of landmarks produced in human-generatedreference instructions to those produced in model-generated instructions shows a large overlap onpartially seen data, and ranking is similar to hand-crafted salient scores used in work in geoinformat-.
figure 5: dataset splits.
all 700 routes that are exclusively in the greenrectangle are in the unseen test set.
all 605 routesthat cross the green border are in the developmentset.
none of those development set routes extendfurther than the blue rectangle.
the training setconsists of routes within the red rectangle but out-side of the green rectangle.
the partially seen testset consists of 700 randomly sampled routes fromthe training set (and removed from the training set).
partially seen means that subsequences of thoseroutes can be present in the training set..b evaluation navigation success rate.
analysis.
we analyze the navigation success rate with respectto properties of the corresponding routes.
figure 6.
49935-36 (40)37-38 (36)39-41 (51)42-43 (42)44-45 (36)route length in number of nodes (#instances)0.00.20.40.60.81.0navigation success ratetest unseen 200 instancesreferencerule_basedgraph2textg2t+pretrain35-36 (41)37-38 (28)39-41 (47)42-43 (42)44-45 (42)route length in number of nodes (#instances)0.00.20.40.60.81.0navigation success ratetest partially seen 200 instancesreferencerule_basedgraph2textg2t+pretrainreference.
osm tagbankparkpharmacyfurnitureburgergardencoffee shopplace of worshipamericanbicycle rental.
amenity:leisure:amenity:shop:cuisine:leisure:cuisine:amenity:cuisine:amenity:.
top12345678910.model.
osm tag.
amenity:shop:amenity:leisure:cuisine:shop:cuisine:cuisine:shop:cuisine:.
pharmacyfurniturebankgardenburgersupermarketcoffee shopamericanconvenienceitalian.
score0.410.350.320.300.290.290.260.250.230.23.score0.390.380.370.290.280.250.250.240.220.21.table 5: frequency of osm tags of landmark occur-rences in the instructions for the partially seen test set,normalized by the number of occurrences in the inputgraph..reference.
osm tag.
amenity:shop:shop:amenity:cuisine:tourism:shop:shop:shop:amenity:.
cinemawinecomputerpharmacycoffee shophotelconveniencehousewaresupermarketbank.
top12345678910.model.
osm tagjuicepharmacyconveniencecinemacoffee shopcomputerhotelpetbeautywine.
cuisine:amenity:shop:amenity:cuisine:shop:tourism:shop:shop:shop:.
score0.580.530.530.510.490.440.420.310.310.28.score0.640.550.500.460.460.450.410.390.380.38.table 6: frequency of osm tags of landmark occur-rences in the instructions for the unseen test set, nor-malized by the number of occurrences in the inputgraph..ics (rousell and zipf, 2017).
the distribution oflandmarks in the unseen test data is different fromthe partially seen data.
to some extent, the modelis able to adapt to the unseen environment..d annotation instructions.
the amt workers got the following instructionsfor the writing task:.
the goal of this task is to write navigation in-structions for a given route.
imagine a tourist isasking for directions in a neighborhood you arefamiliar with and try to mention useful landmarksto support orientation.
another annotator will laterread your instructions in order to ﬁnd the goal lo-cation in streetview (navigation run task).
if theother annotator successfully navigates to the goallocation, your instruction is validated..e examples.
figure 7: navigation success rate in respect of numberof intersections in a route.
each node in the route withmore than two neighbors is counted as an intersection..figure 8: navigation success rate in respect of numberof turns in a route.
a turn is deﬁned as an intersectionthat isn’t crossed in straight direction (345◦to 15◦)..5003 (51)4 (94)5 (46)>=6 (14)number of intersections in route (#instances)0.00.20.40.60.81.0navigation success ratetest unseen 200 instancesreferencerule_basedgraph2textg2t+pretrain3 (69)4 (74)5 (35)>=6 (22)number of intersections in route (#instances)0.00.20.40.60.81.0navigation success ratetest partially seen 200 instancesreferencerule_basedgraph2textg2t+pretrain1 (61)2 (100)3 (36)>=4 (8)number of turns in route (#instances)0.00.20.40.60.81.0navigation success ratetest unseen 200 instancesreferencerule_basedgraph2textg2t+pretrain1 (60)2 (98)3 (30)>=4 (12)number of turns in route (#instances)0.00.20.40.60.81.0navigation success ratetest partially seen 200 instancesreferencerule_basedgraph2textg2t+pretrainreference: head straight past the playground and turn left at the light.
turn right at the next light with fantastic cafe on the right.
continue down the very longstreet and stop at the next light with turntable 5060 on the near left corner.
rule based: parking left pitch left playground left pitch left bus stop right light left citibank left light right fantastic cafe left church of the right dorian gray tapleft perbacco left matcha cafe wabi left b4 left stopseq2seq: go straight to the light and turn left.
go straight to the ﬁrst light and turn left.
go straight to the ﬁrst light and stop a couple of steps before the next light.
you should see perbacco on your left.
graph2text: head to the light and turn left.
at the next light with fantastic cafe on your near right corner, turn right.
go to the next light and stop in the middle ofthe intersection.
b4 gray tap should be on your left.
g2t+pretrain: head to the light and turn left.
at the following light with fantastic cafe on the far left corner, turn right.
go to the next light with 4th street deli onthe near left corner and stop in the middle of the intersection..figure 9: route from partially seen test set with successful navigation for g2t+pretrain..reference: turn right at the light right ahead of you.
go through the following light.
the next light is a double intersection.
turn left at the ﬁrst of the lights, andgo through the second one.
pass a graveyard on the right and stop at the end of it, right as the church starts.
it’s about 2/3 down the block.
rule based: garden left light right chef restaurant supplies right light straight bus stop left light left light straight grave yard right stop saint patrick’s rightseq2seq: go to the light and turn right.
go through the following light, restaurant on the corner.
pass a bus stop on the right, and go through the light.
at thefollowing light, bus stop on the corner, turn left.
go through the following light, and stop after a step or two.
saint mary’s church will be on the right.
graph2text: turn right at the ﬁrst set of lights.
go through the next set of lights and then turn left at the following lights.
pass second time around half way downthe block.
stop at saint’s around saint’s church on the right.
g2t+pretrain: go to the light and turn right.
go straight through 1 light and at the following light, there should be a bus stop on the far left corner.
turn left andgo about 1/2 to the next light, stopping in front of saint patrick’s church on the right and graveyard memorial’s on the right..figure 10: route from unseen test set with successful navigation for g2t+pretrain..501reference: go to the light and turn right.
go through the following light with optical 88. the next light will have da gennaro on the far right corner, go throughthe light.
go through the following light as well, with a school on the corner.
turn right at the following light.
take the ﬁrst left possible and stop after a few steps.
rule based: hong kong supermarket left light right sweet moment left light straight 12 corners right light straight da gennaro right public school 130 rightlight straight parking left may wah vegetarian right hair lounge right light right intersection left stopseq2seq: go straight and take a right at the intersection where hong kong supermarket is.
go through the next three intersections and at the fourth one take aright and stop at hair lounge.
graph2text: go to the light and turn right.
go through the following light, optical 88 on the corner.
go through the following light as well, da gennaro on thecorner.
at the following light, hair lounge on the corner, turn right.
take a step and stop.
g2t+pretrain: head to the light and turn right.
go past the next 2 lights with da gennaro on the right corner.
at the 3rd light with may wah vegetarian on the farright corner, turn left.
take one step and stop..figure 11: route from partially seen test set with unsuccessful navigation for g2t+pretrain..reference: go straight to the light and turn right.
go through the next 3 lights.
at the 4th, there is a chase and nike store on the corners.
go through theintersection and stop on the other side at your destination.
rule based: public school 21 left light right lombardi’s pizza left park left rice to riches right drinking water left playground left ta¨ım right light straighttartinery left duane reade left light straight restaurant left premium laces right starbucks left light straight muji at moma right balthazar left soho left lightstraight niketown right stop chase left chase left shelter leftseq2seq: head to the light and turn right.
go past the next 3 lights with starbucks on the left corner.
at the 4th light with chase bank on the near left corner, crossthe street and stop.
graph2text: head to the light and turn right.
go past the next 2 lights with starbucks on the far left corner.
at the 3rd light with chase on the far left corner, crossthe street and stop.
chase should be on your left.
g2t+pretrain: head to the light and turn right.
go past the next 3 lights with starbucks on the left corner.
at the 4th light with chase on the near left corner, crossthe street.
take one step and stop..figure 12: route from unseen test set with unsuccessful navigation for g2t+pretrain..502