investigating label suggestions for opinion mining in german covid-19social media.
tilman beck1, ji-ung lee1, christina viehmann2,marcus maurer2, oliver quiring2 and iryna gurevych1.
1ubiquitous knowledge processing lab, technical university of darmstadt2institut f¨ur publizistik, johannes gutenberg-university mainzwww.ukp.tu-darmstadt.de.
www.ifp.uni-mainz.de.
abstract.
this work investigates the use of interactivelyupdated label suggestions to improve upon theefﬁciency of gathering annotations on the taskof opinion mining in german covid-19 socialmedia data.
we develop guidelines to conducta controlled annotation study with social sci-ence students and ﬁnd that suggestions froma model trained on a small, expert-annotateddataset already lead to a substantial improve-ment – in terms of inter-annotator agreement(+.14 fleiss’ κ) and annotation quality – com-pared to students that do not receive any labelsuggestions.
we further ﬁnd that label sug-gestions from interactively trained models donot lead to an improvement over suggestionsfrom a static model.
nonetheless, our analy-sis of suggestion bias shows that annotators re-main capable of reﬂecting upon the suggestedlabel in general.
finally, we conﬁrm the qual-ity of the annotated data in transfer learning ex-periments between different annotator groups.
to facilitate further research in opinion miningon social media data, we release our collecteddata consisting of 200 expert and 2,785 studentannotations.1.
1.introduction.
the impact analysis of major events like the covid-19 pandemic is fundamental to research in socialsciences.
to enable more socially sensitive pub-lic decision making, researchers need to reliablymonitor how various social groups (e.g., politicalactors, news media, citizens) communicate aboutpolitical decisions (jungherr, 2015).
the increas-ing use of social media especially allows socialscience researchers to conduct opinion analysis ona larger scale than with traditional methods, e.g..1code and data can be found on github:.
https://github.com/ukplab/acl2021-label-suggestions-german-covid19.
interviews or questionnaires.
however, the publi-cation of research results is often delayed or tem-porally transient due to limitations of traditionalsocial science research, i.e.
prolonged data gather-ing processes or opinion surveys being subject toreactivity.
given the increasing performance of lan-guage models trained on large amounts of data in aself-supervised manner (devlin et al., 2019; brownet al., 2020), one fundamental question that arisesis how nlp systems can contribute to alleviate ex-isting difﬁculties in studies for digital humanitiesand social sciences (risch et al., 2019)..one important approach to make data annota-tion more efﬁcient is the use of automated labelsuggestions.
in contrast to active learning, thataims to identify a subset of annotated data whichleads to optimal model training, label suggestionsalleviate the annotation process by providing anno-tators with pre-annotations (i.e., predictions) froma model (ringger et al., 2008; schulz et al., 2019).
to enable the annotation of large amounts of datawhich are used for quantitative analysis by disci-plines such as social sciences, label suggestions area more viable solution than active learning..one major difﬁculty with label suggestions is thedanger of biasing annotators towards (possibly erro-neous) suggestions.
so far, researchers have inves-tigated automated label suggestions for tasks thatrequire domain-speciﬁc knowledge (fort and sagot,2010; yimam et al., 2013; schulz et al., 2019); andhave shown that domain experts successfully iden-tify erroneous suggestions and are more robust topotential biases.
however, the limited availabil-ity of such expert annotators restricts the use oflabel suggestions to small, focused annotation stud-ies.
for tasks that do not require domain-speciﬁcknowledge and can be conducted with non-expertannotators – such as crowd workers or citizen sci-ence volunteers – on a large scale, label sugges-tions have not been considered yet.
this leads to.
proceedingsofthe59thannualmeetingoftheassociationforcomputationallinguisticsandthe11thinternationaljointconferenceonnaturallanguageprocessing,pages1–13august1–6,2021.©2021associationforcomputationallinguistics1two open questions.
first, if non-expert annotatorsthat do not receive any training besides annota-tion guidelines beneﬁt from label suggestions at all.
second, if existing biases are ampliﬁed especiallywhen including interactively updated suggestionsthat have been shown to be advantageous over staticones (klie et al., 2020)..we tackle these challenges by conducting a com-parative annotation study with social science stu-dents using a recent state-of-the-art model to gener-ate label suggestions (devlin et al., 2019).
ourresults show that a small set of expert-labeleddata is sufﬁcient to improve annotation qualityfor non-expert annotators.
in contrast to schulzet al.
(2019), we show that although interactive andnon-interactive label suggestions substantially im-prove the agreement, we do not observe signiﬁcantdifferences between both approaches.
we furtherconﬁrm this observation with experiments usingmodels trained on (and transferred to) individualannotator groups.
our contributions are:.
c1: an evaluation of label suggestions in terms ofannotation quality for non-expert annotators..c2: an investigation of label suggestion bias forboth static and interactively updated sugges-tions..c3: a novel corpus of german twitter posts thatcan be used by social science researchers tostudy the effects of governmental measuresagainst covid-19 on the public opinion..finally, we also publish 200 expert and 2,785individual student annotations of our dataset to fa-cilitate further research in this direction..2 related work.
label suggestions.
in an early work, rehbeinet al.
(2009) study the effects of label sugges-tions on the task of word sense disambiguationand observe a positive effect on annotation quality.
with the introduction of annotation tools such asbrat (stenetorp et al., 2012), webanno (yimamet al., 2013), or inception (klie et al., 2018),the use of label suggestions became more feasi-ble; leading to an increased investigation of labelsuggestions in the context of nlp.
for instance,yimam et al.
(2014) investigate label suggestionsfor amharic pos tagging and german named en-tity recognition and show with expert annotators.
that label suggestions signiﬁcantly reduce the an-notation time.
other works further investigate in-teractively updated label suggestions and come to asimilar conclusion (klie et al., 2020).
label sugges-tions have also been shown to be effective in non-nlp annotation tasks that require domain-speciﬁcknowledge such as in medical (lingren et al., 2014)or educational (schulz et al., 2019) use cases..bias.
annotations from untrained human anno-tators may introduce biases that are conveyed tomachine learning models (gururangan et al., 2018).
one possible source of bias may be due to the differ-ent decision making process triggered by label sug-gestions – namely, ﬁrst deciding if the suggestedlabel is correct and only if not, considering differentlabels (turner and schley, 2016).
hence, the keyquestion that arises is to what extent annotators areinﬂuenced by such suggestions.
although fort andsagot (2010) identify an inﬂuence on annotationbehaviour when providing pre-annotated data forpos-tagging, they do not measure any clear biasin the annotated labels.
rosset et al.
(2013) cometo a similar conclusion when investigating the biasintroduced by label suggestions in a cross-domainsetup, i.e., when using label suggestions from amodel that is trained on data from a different do-main than the annotated data.
they conduct theirexperiments with eight annotators from varyinglevels of expertise and report considerable anno-tation performance gains while not ﬁnding con-siderable biases introduced by label suggestions.
most similar to our work is the setup from schulzet al.
(2019).
the authors investigate interactivelabel suggestions for expert annotators across twodomains and study the effects of using existingand newly annotated data for training different sug-gestion models.
they compare personalised usermodels against a universal model which has accessto all annotated data and show that the latter pro-vides suggestions with a higher acceptance rate.
this seems less surprising due to the substantiallylarger training set.
further, they do not identify anybias introduced by pre-annotating data..whereas existing work reports no measurablebias for expert annotators (fort and sagot, 2010;lingren et al., 2014; schulz et al., 2019), it re-mains unclear for annotators who have no priorexperience in similar annotation tasks; especiallyfor scenarios where – besides annotation guide-lines – no further training is provided.
however,the use of novice annotators is common for sce-.
2we made use of the twitter streaming api andgathered only those tweets which were classiﬁed asgerman by the twitter language identiﬁer.
this re-sulted in a set of approximately 16.5 million tweets.
we retained only tweets that contain key termsreferring to measures related to the covid-19 pan-demic and removed all duplicates, retweets andall tweets with text length less than 30 characters.
after ﬁltering, 237,616 tweets remained and theirdaily temporal distribution is visualized in figure 1.we sample uniformly at random from the remain-ing tweets for all subsequent annotation tasks.2.
annotation scheme.
we developed annotationguidelines together with three german-speakingresearchers from social sciences and iteratively re-ﬁned them in three successive rounds.
our goalfrom a social science perspective is to analyze thepublic perception of measures taken by the govern-ment.
therefore, the resulting dataset should helpin (1) identifying relevant tweets for governmentalmeasures and if relevant, (2) detecting what stanceis expressed.
we follow recent works on stance de-tection and twitter data (hanselowski et al., 2018;baly et al., 2018; conforti et al., 2020) and usefour distinct categories for our annotation.
theyare deﬁned as follows:.
unrelated: no measures related to the contain-.
ment of the pandemic are mentioned.
comment: measures are mentioned, but not as-.
sessed or neutral.
support: measures are assessed positively.
refute: measures are assessed negatively.
the four label annotation scheme allows us todistinguish texts that are related to the pandemicbut do not talk about measures (i.e., unrelated)..4 study setup.
our goal is to study the effects of interactively up-dated and static label suggestions in non-expertannotation scenarios.
non-experts such as crowdworkers or student volunteers have no prior ex-perience in annotating comparable tasks and onlyreceive annotation guidelines for preparation.3 oursecondary goal is to collect a novel dataset that canbe used by social science researchers to study the.
figure 1: number of tweets per day collected from de-cember 2019 to april 2020..narios where no linguistic or domain expertise isrequired.
hence, we present a ﬁrst case-study forthe use of interactive label suggestions with non-expert annotators.
furthermore, we ﬁnd that re-cent state-of-the-art models such as bert (devlinet al., 2019) can provide high-quality label sugges-tions with already little training data and hence,are important for interactive label suggestions innon-expert annotation tasks..3 annotation task.
our task is inspired by social science researchon analyzing public opinion using social me-dia (jungherr, 2015; mccormick et al., 2017).
thegoal is to identify opinions in german-speakingcountries about governmental measures establishedto contain the spread of the corona virus.
we usetwitter due to its international and widespread us-age that ensures a sufﬁcient database and the sev-eral challenges for the automatic identiﬁcation ofopinions and stance it poses from an nlp perspec-tive (imran et al., 2016; mohammad et al., 2016;gorrell et al., 2019; conforti et al., 2020).
for ex-ample, the use of language varies from colloquialexpressions to well-formed arguments and news-spreading statements due to its heterogeneous userbase.
additionally, hashtags are used directly aspart of text but also to embed the tweet itself inthe broader discussion on the platform.
finally,the classiﬁcation of a tweet is particularly challeng-ing given the character limitation of the platform,i.e., at the date of writing twitter allows for 280characters per tweet..data collection.
initially, we collected tweetsfrom december 2019 to the end of april 2020.using a manually chosen set of search queries(‘corona’, ‘pandemie’, ‘covid’, ‘socialdistance’),.
2we provide additional information about data collectionin appendix a and discuss ethical concerns regarding the useof twitter data after the conclusion..3we provide the original german guidelines along with thedataset.
an english summary is provided in the appendix b.
301101001.00010.0002019-12-012020-01-012020-02-012020-03-012020-04-01number of tweetsfigure 2: design of the annotation setup for each of the three user groups.
the 30 quality control instances (red)were inserted at random positions but are visualized at the end for presentation purpose..effects of governmental measures for preventingthe spread of covid-19 on the public opinion..the resulting labels were then re-evaluated by theexperts and agreed upon..to train a model that provides label suggestionsto our non-expert annotators, we ﬁrst collect a smallset of 200 expert-annotated instances.
we thensplit our non-expert annotators into three differentgroups that receive (g1) no label suggestions, (g2)suggestions from a model trained on expert anno-tations, and (g3) suggestions from a model that isretrained interactively using both expert-annotatedand interactively annotated data..4.1 expert annotations.
the expert annotations were provided by the re-searchers (three social science researchers and onenlp researcher) that created the annotation guide-lines and who are proﬁcient in solving the task.
intotal, 200 tweets were sampled uniformly at ran-dom and annotated by all four experts.
the inter-annotator agreement (iaa) across all 200 tweetslies at 0.54 fleiss’s κ (moderate agreement) andis comparable to previously reported annotationscores in the ﬁeld of opinion and argument min-ing (bar-haim et al., 2020; schaefer and stede,2020; boltuˇzi´c and ˇsnajder, 2014).
overall, inmore than 50% of the tweets all four experts se-lected the same label (respectively, in ∼75% of thetweets at least three experts selected the same label).
the disagreement on the remaining ∼25% of thetweets furthermore shows the increased difﬁcultyof our task due to ambiguities in the data source,e.g., ironical statements or differentiating govern-mental measures from non-governmental ones likehome-ofﬁce.
to compile gold standard labels forinstances that the experts disagreed upon, we applymace (hovy et al., 2013) using a threshold of 1.0..4.2 student annotations.
the annotations were conducted with a group of 21german-speaking university students.
to ensure abasic level of comparability for our student anno-tators, we recruited all volunteers from the samesocial science course at the same university.
theannotators received no further training apart fromthe annotation guidelines.
we randomly assignedthem to three different groups (g1, g2, and g3),each consisting of seven students.
to investigatethe effects of interactive label suggestions, we de-ﬁned different annotation setups for each group.
the annotations were split into two rounds.
ateach round of annotation, students were providedwith 100 tweets consisting of 70 new tweets and30 quality control tweets from the expert-labeleddata which are used to compare individual groups.
across both rounds, we thus obtain a total of 140unique annotated tweets per student and use 60tweets for evaluation.
the annotation setup of eachgroup including the individual data splits is visual-ized in figure 2.4.no label suggestions (g1).
the ﬁrst groupserves as a control group and receives no labelsuggestions..static label suggestions (g2).
the secondgroup only receives label suggestions based ona model which was trained using the 200 expert-labeled instances described in section 4.1..4note that the control instances were distributed uniformlyat random within a round to mitigate any interdependencyeffects between different tweets..4interactive label suggestions (g3).
the lastgroup of students receives expert label suggestionsin the ﬁrst round and interactively updated labelsuggestions in the second round.
in contrast to ex-isting work (schulz et al., 2019), this setup allowsus to directly quantify effects of bias ampliﬁcationthat may occur with interactive label suggestions..4.3 label suggestion model.
system setup.
we conduct our annotation exper-iments using inception (klie et al., 2018) whichallows us to integrate label suggestions using rec-ommendation models.
to obtain label suggestions,we use a german version of bert (ger-bert) thatis available through the huggingface library (wolfet al., 2020).5 we perform a random hyperparame-ter search (cf.
appendix b.3) and train the modelon the expert annotated data for 10 epochs witha learning rate of 8e-5 and a batch size of 8. weselect the model that performed best in terms off1-score on a held-out stratiﬁed test set (20% of thedata) across ten runs with different random seeds.
all experiments were conducted on a desktop ma-chine with a 6-core 3.8 ghz cpu and a geforcertx 2060 gpu (8gb)..model.
macro-f1 accuracy.
majorityrandombilstm (schulz et al., 2019)sbert+lgbm (klie et al., 2020)ger-bert (this work).
.15.23.47.50.66.
.45.27.53.55.68.table 1: performance of various label suggestion mod-els on expert-labeled dataset..model comparison.
to assess the label sugges-tion quality of our model, we report the predictiveperformance on the expert-labeled dataset (setupas described above) in table 1. we compare ourmodel with baselines6 which have been used inrelated work (schulz et al., 2019; klie et al., 2020)for label suggestions.
as expected, ger-bertachieves superior performance and the results arepromising for using label suggestions..interactive training routine.
to remedy thecold-start problem, g3 receives label sugges-tions from the model trained only on the expert-annotated data in round 1. afterwards, we retrainthe model with an increasing number of instances.
5https://deepset.ai/german-bert6we adapted the respective architectures to our setup..using both, the expert annotations and the g3 dataof individual students from round 1.7 to avoidunnecessary waiting times for our annotators dueto the additional training routine, we always col-lect batches of 10 instances before re-training ourmodel.
we then repeatedly train individual modelsfor each student in g3 with an increasing amount ofdata of up to 70 instances.
the 30 expert-annotatedquality control tweets were excluded in this step toavoid conﬂicting labels and duplicated data..5 study evaluation.
table 2 shows the overall statistics of our resultingcorpus consisting of 200 expert and 2,785 student-annotated german tweets.
note that we removed60 expert-annotated instances that we included forannotation quality control for each student, result-ing in 140 annotated tweets per student..outliers.
a ﬁne-grained analysis of annotationtime is not possible due to online annotations athome.
however, one student in g3 had, on average,spent less than a second for each annotation andaccepted almost all suggested labels.
this student’sannotations were removed from the ﬁnal datasetand assumed as faulty labels considering the shortamount of time spent on this task in comparison tothe minimum amount of seven seconds per tweetand annotation for all other students..5.1 annotation quality.
to assess the overall quality of our collected studentannotations, we investigate annotator consistencyin terms of inter-annotator-agreement (iaa) as wellas the annotator accuracy on our quality assuranceinstances..table 3 shows fleiss’ κ (fleiss, 1971) and theaccuracy computed for the quality control instancesin gen-that were consistent across all groups.
eral, we observe a similar or higher agreement forour students compared to the expert annotations(κ = 0.54) showing that the guidelines were ableto convey the task well.
we also ﬁnd that groupsthat receive label suggestions (g2 and g3) achievea substantially larger iaa as opposed to g1.
mostinterestingly, we observe a substantial increase iniaa for both g2 and g3 in the second annota-tion round, whereas the iaa in g1 remains stable..7note that using all previously annotated data of g3 wouldimpair the comparability between individual students as thedata was collected asynchronously to allow students to picktheir best suited timeslot.
further, a synchronization stepbetween users would impair the applicability of the approach..5n annotator avg.
length.
unrelated.
comment.
support.
refute.
2002,785.expertstudent.
965980840.g1g2g3.
189 (±75)185 (±75).
185 (±76)185 (±73)184 (±75).
53 (26.5%)1,003 (36.0%).
89 (44.5%)1,055 (37.9%).
43 (21.5%)425 (15.3%).
15 (7.5%)302 (10.8%).
387 (40.1%)320 (32.7%)296 (35.2%).
334 (34.6%)407 (41.5%)314 (37.4%).
128 (13.3%)152 (15.5%)145 (17.3%).
116 (12.0%)101 (10.3%)85 (10.1%).
table 2: our twitter dataset on public opinion about containment measures during the corona pandemic..acc.
iaa acc.
iaa acc.
iaa.
round 1round 2total.
.48.47.48.
.76.81.78.
.62.67.65.g3.
.84.82.83.g2.
.90.92.91.g1.
.74.68.71.table 3: annotation accuracy (acc) and iaa (fleiss’κ) on the quality control instances for each annotatorgroup and round..analyzing our models’ predictions shows that thesuggested labels for the 60 quality control samplesmostly conform with the label given by the expert(97% for g2 and 94% for g3).
therefore, annota-tors are inclined to accept the label suggested bythe model.
we can further conﬁrm this observationwhen investigating the number of instances that thestudents labeled correctly (accuracy).
the highestaccuracy is observed for the group that receivedthe highest quality suggestions (g2).
furthermore,both groups that received label suggestions (g2,g3) express an increased accuracy over the controlgroup (g1).
in general, for both rounds the accu-racy remains similarly high across all groups (±.02difference) with only a slight decrease (−.04) forg1.
hence, we conjecture that the resulting annota-tions provide satisfying quality given the challeng-ing task and annotator proﬁciency..5.2 suggestion bias.
one major challenge in using label suggestions isknown in psychology as the anchoring effect (tver-sky and kahneman, 1974; turner and schley,2016).
it describes the concept that annotators whoare provided a label suggestion follow a differentdecision process compared to a group that does notreceive any suggestions and tend to accept the sug-gestions.
as we observe larger iaa and accuracyfor groups receiving label suggestions, we lookat the label suggestion acceptance rate and which.
figure 3: the number of rejected label suggestions.
the x-axis displays the corrected label and the y-axisthe label suggestion.
for example, the upper left cor-ner shows that ten suggestions of label refute werecorrected as unrelated by the users..labels have been corrected by the annotators..acceptance rate.
one way to quantify possiblebiases is to evaluate if annotators tend to acceptmore suggestions with an increasing number ofinstances (schulz et al., 2019).
this may be thecase when annotators increasingly trust the modelwith consistently good suggestions.
consequently,with increasing trust towards the model’s predic-tions, non-expert annotators may tend to acceptmore model errors.
to investigate if annotatorsremain capable of reﬂecting on instance and labelsuggestion, we compute the average acceptancerate for g2 and g3 in both rounds.
we ﬁnd thatfor both groups, the acceptance rate remains stable(g2: 73% and 72%, g3: 68% and 69%) and con-clude that annotators receiving high quality labelsuggestions remain critical while producing moreconsistent results..6unrelatedcommentsupportrefuterefutesupportcommentunrelated10251605051027990464201462number of label suggestion corrections (g2)020406080numberfigure 4: number of label suggestions that diverge fromthe model trained on expert-data with increasing numberof annotations (for each student)..figure 5: number of label suggestions in g3 that havebeen accepted in the second round of annotations (foreach student)..label corrections.
to further evaluate if stu-dents are vulnerable to erroneous label suggestionsfrom a model, we speciﬁcally investigate labels thathave been corrected.
figure 3 shows our results forg2.8 as can be seen, the most notable number oflabel corrections were made by students for unre-lated tweets that were classiﬁed as comments bythe model.
additionally, we ﬁnd a large number ofcorrections that have been made with respect to thestance of the presented tweet.
we will discuss bothtypes of corrections in the following.
unrelated tweets.
the label suggestion modelmakes the most errors for unrelated tweets (i.e.,tweets that are corrected as unrelated) by mis-classifying them as comment (99).
in contrast,instances that are identiﬁed as unrelated tweetsare only seldomly corrected.
this indicates an in-creased focus on recall at the expense of precisionfor related tweets, most likely due to commentbeing the largest class in the training data (see ta-ble 2, expert data).
we ﬁnd possible causes forsuch wrong predictions when we look at exampleswhere comment was suggested for unrelatedinstances9:.
example 1: the corona virus also requires spe-cial protective measures for naomi campbell.
the top model wears a protective suit duringa trip..example 2: extraordinary times call for ex-traordinary measures: the ”elbschlosskeller”now has a functioning door lock.
#hamburg#corona #covid-19.
8note that analyzing g3 shows similar observations (cf..appendix c)..9note that we present translations of the original german.
texts for better readability and to protect user privacy.
clearly, these examples are fairly easy to annotatefor humans but are difﬁcult to predict for a modeldue to speciﬁc cue words being mentioned, e.g.,measures.
similar results have also been reportedin previous work (hanselowski et al., 2018;conforti et al., 2020)..stance.
in figure 3, we can also see thatthe model makes mistakes regarding the stance ofa tweet.
especially, 101 support suggestionshave been corrected as either being unrelated orneutral and 88 comment suggestions have beencorrected to either support or refute.
forthe second case, we often discover tweets thatimplicitly indicate the stance – for example, bycomplaining about people ignoring the measures:.
example 3: small tweet aside from xr: col-league drags himself into the ofﬁce this morn-ing with ﬂu symptoms ( ¨od) the other col-leagues still have to convince him to please gohome immediately.
only then does he showsome understanding.
unbelievable.
#covid#socialdistancing.
such examples demonstrate the difﬁculty of thetask and seem to be difﬁcult to recognize for themodel.
however, given the large amount of labelcorrections, the non-expert annotators seem to beless susceptible to accept such model errors..5.3 bias ampliﬁcation.
the high number of label corrections for speciﬁctypes of tweets shows that our annotators of g2remained critical towards the suggested label.
withinteractively updated suggestions however, thismay not be the case.
especially annotators thataccept erroneous suggestions may lead to reinforc-.
7 0 20 40 60 80 100 100 120 140 160 180 200accumulative differenceiterations21s22s23s24s25s27 0 10 20 30 40 50 60 70 80 90 100 100 110 120 130 140 150 160 170 180 190 200total accepted suggestionsiterations21s22s23s24s25s27ing a model in its prediction; hence, leading toamplifying biases..diverging suggestions.
to study such effects,we ﬁrst identify if the interactively updated modelsexpress a difference in terms of predictions com-pared to the static model.
in figure 4 we can ob-serve that with already 40 instances (iteration 140),the number of differently predicted instances is tenor higher across all personalized models.
this di-vergence is highly correlated with the number ofchanges a student provides (see figure 5).
we thuscan conclude that the interactively trained modelsare able to adapt to the individual annotations foreach annotator..figure 6: average number of accepted label sugges-tions across all instances for g2 and g3.
the shared ar-eas display the upper and lower quartile for each group..comparison to g2.
figure 6 shows the averagenumber of accepted suggestions for g2 and g3 aswell as the upper and lower quartiles, respectively.
the vertical line separates the ﬁrst and the secondround of annotations.
we ﬁnd that especially in theﬁrst round of annotations, both groups have a verysimilar acceptance rate of suggested labels.
onlywith interactively updated suggestions we ﬁnd anincreasing divergence in g3 with respect to theupper and lower quartiles..individual acceptance rate.
to assess the im-pact of interactive label suggestions, we furtherinvestigate how many suggestions were acceptedby each annotator.
figure 5 shows the number ofaccepted label suggestions for each student in g3in the second round of annotations.
although weobserve that the average number of accepted la-bel suggestions remains constant across g2 andg3, we can see substantial differences between in-dividual students.
for instance, we can observethat for s21, the increased model adaptivity leads.
figure 7: transfer learning performance of modelstrained on individual annotator groups.
the x-axispresents the dataset which is used for model training,the y-axis lists the dataset used for model testing..to an overall decrease in the number of acceptedlabels.
moreover, s24 who received predictionsthat diverge less from the static model predictionaccepted the most suggestions in the second round.
this shows that interactive label suggestions doesnot necessarily lead to a larger acceptance rate –possibly amplifying biases – but instead, variesfor each annotator and needs to be investigated infuture work..5.4 cross-group transfer.
finally, we investigate how well models trainedon different annotator groups transfer to each other.
we hence conduct transfer learning experiments forwhich we remove the quality control instances inour student groups and train a separate ger-bertmodel using the same hyperparameters as for theexpert model.
we use 80% of the data for trainingand the remaining 20% to identify the best modelwhich we then transfer to another group.
figure 7shows the macro-f1 scores averaged across tenindependent runs, diagonal entries are the scoreson the 20%.
most notably, models trained on thegroups with label suggestions (g2, g3) do in factperform comparable or better on the expert-labeleddata and outperform models trained on the groupnot receiving any suggestions (g1).
the highercross-group performance for models trained ongroups that received label suggestions shows thatthe label suggestions successfully conveyed knowl-edge from the expert annotated data to our stu-dents..8 0 20 40 60 80 100 120 140 160 180 200 0 20 40 60 80 100 120 140 160 180 200mean accepted suggestionsiterationg2g3expertg1g2g3g3g2g1expert55.9344.1449.3147.9652.3841.4848.2244.8840.3839.4940.1340.6452.1538.3845.8644.10transfer learning experiments using expert and student data4042444648505254macro f16 conclusion.
in this work, we analysed the usefulness of pro-viding label suggestions for untrained annotatorsto identify opinions in a challenging text domain(i.e., twitter).
we generated suggestions usingexpert-labeled training data as well as interac-tively training models using data annotated by un-trained students.
our results show that label sugges-tions from a state-of-the-art sentence classiﬁcationmodel trained on a small set of expert annotationshelp improving annotation quality for untrainedannotators.
in terms of potential biases that mayoccur with untrained annotators we observe that thestudents retained their capability to reﬂect on thesuggested label.
we furthermore do not observe ageneral ampliﬁcation in terms of bias with interac-tively updated suggestions; however, we ﬁnd thatsuch effects are very speciﬁc to individual annota-tors.
we hence conclude that interactively updatedlabel suggestions need to be considered carefullywhen applied to non-expert annotation scenarios.
for future work, we plan to leverage our setupto annotate tweets from a larger time span.
in ger-many, the measures taken by the government havebeen met with divided public reaction – startingwith reactions of solidarity and changing towardsa more critical public opinion (viehmann et al.,2020a,b).
in particular, we are interested if ourlabel suggestion model is robust enough to accountfor such a shift in label distribution..acknowledgements.
this work has been supported by the german re-search foundation (dfg) as part of the researchtraining group kritis no.
grk 2222/2.
further,this work is supported by the european regionaldevelopment fund (erdf) and the hessian statechancellery – hessian minister of digital strategyand development under the promotional reference20005482 (texprax).
we thank the student volun-teers from the university of mainz for their anno-tations as well as johannes daxenberger, mohsenmesgar, ute winchenbach, kevin stowe and theanonymous reviewers for their valuable feedback..ethical considerations.
data collection and annotation.
the tools weuse to collect tweets are in compliance with twit-ter’s terms of service.
we only release the set ofidentiﬁers (tweet ids) for the texts used in this.
research project.
thereby, we adhere to the twitterdeveloper policy10 and give users full control oftheir privacy and data as they can delete or privatizetweets so that they cannot be collected..we asked student annotators for voluntary partic-ipation in the annotation study.
all students havebeen informed about the goal of the conducted re-search and the purpose of the collected annotations.
during annotation no information about the tweet’sauthor or any other additional metadata was madeavailable to the annotators.
we did not collect anypersonal data from the students before, after, orduring the annotation task..data usage.
this work presents an investigationof efﬁcient data annotation methods in a case studyon social media data.
the results of this workallow social science researchers to apply their anal-ysis on a larger scale.
in the case of analyzingpublic opinion on governmental measures, the re-sulting analysis allows politicians to make moresocially sensitive public decisions.
this informa-tion is useful in aggregated form, without the needfor information about individual users.
however,we want to point out that users of social media(particularly twitter) do not constitute a represen-tative sample of the general population, especiallyin germany (newman et al., 2020).
therefore, ourgoal is not to foster public decision-making solelybased upon analysis of twitter but to provide anadditional supporting tool..dual use.
further, we acknowledge the potentialof misuse of our dataset: the annotated data allowsanyone, including both individuals and organiza-tions, for training models to identify individuals ex-pressing their consent or dissent with governmentalactions.
to this end, we follow the argumentationby (benton et al., 2017) that in general we cannotprevent publicly available data from being misusedbut we want to make both researchers and the gen-eral public aware of the possible malicious use..references.
ramy baly, mitra mohtarami, james glass, llu´ısm`arquez, alessandro moschitti, and preslav nakov.
2018.integrating stance detection and fact check-ing in a uniﬁed corpus.
in proceedings of the 2018conference of the north american chapter of theassociation for computational linguistics: human.
10https://developer.twitter.com/en/developer-terms/agreement-and-policy.
9language technologies, volume 2 (short papers),pages 21–27, new orleans, louisiana.
associationfor computational linguistics..roy bar-haim, lilach eden, roni friedman, yoavkantor, dan lahav, and noam slonim.
2020. fromarguments to key points: towards automatic argu-ment summarization.
in proceedings of the 58th an-nual meeting of the association for computationallinguistics, pages 4029–4039, online.
associationfor computational linguistics..adrian benton, glen coppersmith, and mark dredze.
2017. ethical research protocols for social mediain proceedings of the first aclhealth research.
workshop on ethics in natural language process-ing, pages 94–102, valencia, spain.
association forcomputational linguistics..filip boltuˇzi´c and jan ˇsnajder.
2014. back up yourstance: recognizing arguments in online discus-sions.
in proceedings of the first workshop on ar-gumentation mining, pages 49–58, baltimore, mary-land.
association for computational linguistics..tom b brown, benjamin mann, nick ryder, melaniesubbiah, jared kaplan, prafulla dhariwal, arvindneelakantan, pranav shyam, girish sastry, amandaaskell, et al.
2020. language models are few-shotlearners.
arxiv preprint arxiv:2005.14165..costanza conforti, jakob berndt, mohammad taherpilehvar, chryssi giannitsarou, flavio toxvaerd,and nigel collier.
2020. will-they-won’t-they: avery large dataset for stance detection on twitter.
inproceedings of the 58th annual meeting of the asso-ciation for computational linguistics, pages 1715–1724, online.
association for computational lin-guistics..jacob devlin, ming-wei chang, kenton lee, andkristina toutanova.
2019. bert: pre-training ofdeep bidirectional transformers for language under-in proceedings of the 2019 conferencestanding.
of the north american chapter of the associationfor computational linguistics: human languagetechnologies, volume 1 (long and short papers),pages 4171–4186, minneapolis, minnesota.
associ-ation for computational linguistics..joseph l fleiss.
1971. measuring nominal scale agree-ment among many raters.
psychological bulletin,76(5):378..kar¨en fort and benoˆıt sagot.
2010..inﬂuence ofpre-annotation on pos-tagged corpus development.
in proceedings of the fourth linguistic annotationworkshop, pages 56–63, uppsala, sweden.
associa-tion for computational linguistics..845–854, minneapolis, minnesota, usa.
associa-tion for computational linguistics..suchin gururangan, swabha swayamdipta, omerlevy, roy schwartz, samuel bowman, and noah a.smith.
2018. annotation artifacts in natural lan-in proceedings of the 2018guage inference data.
conference of the north american chapter of theassociation for computational linguistics: humanlanguage technologies, volume 2 (short papers),pages 107–112, new orleans, louisiana.
associa-tion for computational linguistics..andreas hanselowski, avinesh pvs, benjaminschiller, felix caspelherr, debanjan chaudhuri,christian m. meyer, and iryna gurevych.
2018. aretrospective analysis of the fake news challengein proceedings of the 27thstance-detection task.
international conference on computational lin-guistics, pages 1859–1874, santa fe, new mexico,usa.
association for computational linguistics..dirk hovy, taylor berg-kirkpatrick, ashish vaswani,and eduard hovy.
2013. learning whom to trustwith mace.
in proceedings of the 2013 conferenceof the north american chapter of the associationfor computational linguistics: human languagetechnologies, pages 1120–1130, atlanta, georgia.
association for computational linguistics..muhammad imran, prasenjit mitra, and carlos castillo.
2016. twitter as a lifeline: human-annotated twit-ter corpora for nlp of crisis-related messages.
inproceedings of the tenth international conferenceon language resources and evaluation (lrec’16),pages 1638–1643, portoroˇz, slovenia.
europeanlanguage resources association (elra)..andreas jungherr.
2015. analyzing political commu-nication with digital trace data.
springer, cham,switzerland..jan-christoph klie, michael bugert, beto boullosa,richard eckart de castilho, and iryna gurevych.
2018. the inception platform: machine-assistedand knowledge-oriented interactive annotation.
inproceedings of the 27th international conference oncomputational linguistics: system demonstrations,pages 5–9, santa fe, new mexico.
association forcomputational linguistics..jan-christoph klie, richard eckart de castilho, andiryna gurevych.
2020. from zero to hero: human-in-the-loop entity linking in low resource do-in proceedings of the 58th annual meet-mains.
ing of the association for computational linguistics,pages 6982–6993, online.
association for computa-tional linguistics..genevieve gorrell, elena kochkina, maria liakata,ahmet aker, arkaitz zubiaga, kalina bontcheva,and leon derczynski.
2019. semeval-2019 task 7:rumoureval, determining rumour veracity and sup-port for rumours.
in proceedings of the 13th inter-national workshop on semantic evaluation, pages.
todd lingren, louise deleger, katalin molnar, haijunzhai, jareen meinzen-derr, megan kaiser, laurastoutenborough, qi li, and imre solti.
2014. eval-uating the impact of pre-annotation on annotationspeed and potential bias: natural language process-ing gold standard development for clinical named.
10entity recognition in clinical trial announcements.
journal of the american medical informatics asso-ciation, 21(3):406–413..tyler h mccormick, hedwig lee, nina cesare, alishojaie, and emma s spiro.
2017. using twitter fordemographic and social science research: tools fordata collection and processing.
sociological meth-ods & research, 46(3):390–421..saif mohammad, svetlana kiritchenko, parinaz sob-hani, xiaodan zhu, and colin cherry.
2016. ain proceed-dataset for detecting stance in tweets.
ings of the tenth international conference on lan-guage resources and evaluation (lrec’16), pages3945–3952, portoroˇz, slovenia.
european languageresources association (elra)..nic newman, richard fletcher, anne schulz, simgeandi, and rasmus-kleis nielsen.
2020. digitalnews report 2020. reuters institute for the study ofjournalism, pages 2020–06..ines rehbein,.
and carolinejosef ruppenhofer,sporleder.
2009. assessing the beneﬁts of partial au-tomatic pre-labeling for frame-semantic annotation.
in proceedings of the third linguistic annotationworkshop (law iii), pages 19–26, suntec, singa-pore.
association for computational linguistics..eric ringger, marc carmen, robbie haertel, kevinseppi, deryle lonsdale, peter mcclanahan, jamescarroll, and noel ellison.
2008. assessing thecosts of machine-assisted corpus annotation throughin proceedings of the sixth interna-a user study.
tional conference on language resources and eval-uation (lrec’08), marrakech, morocco.
europeanlanguage resources association (elra)..julian risch, anke stoll, marc ziegele, and ralf kres-tel.
2019. hpidedis at germeval 2019: offensivelanguage identiﬁcation using a german bert model.
in proceedings of the 15th conference on naturallanguage processing (konvens 2019), pages 405–410, erlangen, germany.
german society for com-putational linguistics & language technology..sophie rosset, cyril grouin, thomas lavergne, mo-hamed ben jannet, j´er´emy leixa, olivier galibert,and pierre zweigenbaum.
2013. automatic namedentity pre-annotation for out-of-domain human an-in proceedings of the 7th linguistic an-notation.
notation workshop and interoperability with dis-course, pages 168–177, soﬁa, bulgaria.
associationfor computational linguistics..robin schaefer and manfred stede.
2020. annotationin proceed-and detection of arguments in tweets.
ings of the 7th workshop on argument mining, pages53–58, online.
association for computational lin-guistics..claudia schulz, christian m. meyer, jan kiesewetter,michael sailer, elisabeth bauer, martin r. fischer,frank fischer, and iryna gurevych.
2019. anal-ysis of automatic annotation suggestions for hard.
discourse-level tasks in expert domains.
in proceed-ings of the 57th annual meeting of the associationfor computational linguistics, pages 2761–2772,florence, italy.
association for computational lin-guistics..pontus stenetorp, sampo pyysalo, goran topi´c,tomoko ohta, sophia ananiadou, and jun’ichi tsu-jii.
2012. brat: a web-based tool for nlp-assistedin proceedings of the demonstra-text annotation.
tions at the 13th conference of the european chap-ter of the association for computational linguistics,pages 102–107, avignon, france.
association forcomputational linguistics..brandon m turner and dan r schley.
2016. the an-chor integration model: a descriptive model of an-choring effects.
cognitive psychology, 90:1–47..amos tversky and daniel kahneman.
1974. judgmentscience,.
under uncertainty: heuristics and biases.
185(4157):1124–1131..christina viehmann, marc ziegele, and oliver quir-ing.
2020a.
gut informiert durch die pandemie?
nutzung unterschiedlicher informationsquellen inder corona-krise.
ergebnisse einer dreiwelligenpanelbefragung im jahr 2020. media perspektiven,10-11:2556–577..christina viehmann, marc ziegele, and oliver quiring.
2020b.
in der krise r¨ucken alle etwas mehr zusam-men?
fast!
– wie medien und andere faktoren zurgef¨uhlten integration und spaltung beitragen.
kom-munikationsmanager, 2:32–36..thomas wolf, lysandre debut, victor sanh, julienchaumond, clement delangue, anthony moi, pier-ric cistac, tim rault, r´emi louf, morgan funtow-icz, joe davison, sam shleifer, patrick von platen,clara ma, yacine jernite, julien plu, canwen xu,teven le scao, sylvain gugger, mariama drame,quentin lhoest, and alexander m. rush.
2020.transformers: state-of-the-art natural language pro-cessing.
in proceedings of the 2020 conference onempirical methods in natural language processing:system demonstrations, pages 38–45, online.
asso-ciation for computational linguistics..seid muhie yimam, chris biemann, richard eckart decastilho, and iryna gurevych.
2014. automatic an-notation suggestions and custom annotation layersin webanno.
in proceedings of 52nd annual meet-ing of the association for computational linguis-tics: system demonstrations, pages 91–96, balti-more, maryland.
association for computational lin-guistics..seid muhie yimam,.
iryna gurevych, richardeckart de castilho, and chris biemann.
2013.webanno: a ﬂexible, web-based and visuallyinsupported system for distributed annotations.
proceedings ofthethe 51st annual meeting ofsys-association for computational linguistics:tem demonstrations, pages 1–6, soﬁa, bulgaria.
association for computational linguistics..11a data crawl details.
a.1 filter terms.
we crawled twitter posts using live streamingapi (twitter).
based on a preliminary examinationof the data, we selected the following set of termsfor ﬁltering tweets: [’stayhomesavelifes’,’wirbleibenzuhause’,’bleibdaheim’, ’abstandhalten’,’flatthecurve’, ’flattenthecurve’,’sperre’, ’verbot’,’beschraenkung’, ’quarant¨ane’,’quarantaene’, ’wirvsvirus’,’schließung’, ’homeoffice’,’infektionsschutz’,’ansteckungsrisiko’, ’notbetrieb’,’bleibtzuhause’, ’stayhome’].
a.2 additional data.
unlike related work (schaefer and stede, 2020), wedo not investigate reply structures of tweets.
in pre-liminary experiments we found that our collectionmethod provides a large enough amount of relevanttweets which can be annotated without context..b annotation.
in this section we provide more detailed informa-tion on the annotation guidelines and the annotationplatform we used..b.1 annotation guidelines.
we ﬁrst provide background context about the mea-sures for containing the spread of covid-19.
af-terwards, we provide a deﬁnition for measures tobe considered during the annotation study: weconsider all measures which are taken by the gov-ernment to contain the pandemic (e.g., closing ofschools)..next, we introduce the annotation task as a two-step process.
first, the annotator has to decideif the text actually does mention measures as de-ﬁned as above (some examples are provided forclariﬁcation).
if not, the annotator selects the labelnomeasure11.
in the opposite case, the annotatordecides in a second step if the text contains a posi-tive position (proopinion), a negative position(conopinion) or if there is no stance expressed.
11please note that we use a different notation in the mainpaper.
the label nomeasure corresponds to unrelated,label noopinion corresponds to comment, proopinioncorresponds to support and conopinion corresponds torefute.
(noopinion).
we provide examples for each la-bel to our annotators..during our preliminary studies, we identiﬁedseveral ambiguities regarding the stance annotationwhich is in the nature of the source of the texts(twitter) and the subject of the annotation (mea-sures regarding the covid-19 pandemic):.
• a tweet discusses (positive/negative) conse-quences or by-products of measures : weregard those as (positive/negative) statementsas the author implicitly states their opinion byreﬂecting upon the measures.
• a tweet reﬂects the opinion of another ac-tor: this is considered as an opinion as deﬁnedabove.
it is assumed that the author posts thisopinion because they identify themselves withthe original opinion..• a tweet makes an unagitated observationwhether measures are functioning: this isnot to be taken as an opinion for or against themeasures per se.
only if an explicit assess-ment of the observation is made, the positioncan be derived..• the role of hashtags: hashtags are often am-biguous and the respective context needs tobe taken into account.
therefore, in our anno-tation hashtags are only considered as contextto what is said; they never stand for them-selves.
hashtags can be used to determinewhether a measure is being addressed.
todo this, the hashtag must contain a measure.
further, hashtags can be used as context tosupport the position in a tweet..these decisions are reﬂected at the correspond-ing positions in the annotation guidelines, alongwith several example tweets.
in the end we pro-vide a note that twitter posts may contain mali-cious, suggestive, offensive, or potentially sensitivecontent and that the annotation can be paused andresumed at any time..b.2 annotation interface.
in figure 8 a screenshot of the annotation interfaceis depicted.
it is taken from the group were labelrecommendations are provided.
the twitter poststo be annotated are shown in the center where eachline corresponds to a single tweet.
for the sake ofclarity, only ﬁve texts are shown simultaneously.
12figure 8: a screenshot of the annotation interface using inception (klie et al., 2018).
0.00001, 0.00005, 0.00008].
• batch size: [4,8,16].
c label rejections.
and the user navigates through all texts using thenavigation bar above the text window..the label recommendations are displayed usinga green box above the corresponding text and thecurrently selected recommendation is highlightedin orange.
if the user agrees with the providedlabel, nothing needs to be changed.
in the op-posite case, the user can click on the recommen-dation and select another label on the right-handside (annotation panel) using the opiniondropdown ﬁeld.
the annotators receiving no labelsuggestions (g1) do not see any recommendationduring annotation.
they create an annotation foreach sentence by double-clicking on the sentence.
once the user has ﬁnished annotating all samples,the annotation session is ﬁnished by clicking thelock symbol in the navigation bar.
the technicalprocedure of the annotation has been explained toall annotators beforehand..b.3 label suggestion model.
we used the german-bert-cased bertbase model which was pretrained on a germanwikipedia dump (6gb), an openlegaldata dump(2.4gb) and news articles (3.6gb).
it was trainedfor 810k steps with a batch size of 1024 for se-quence length 128 and 30k steps with sequencelength 512. it outperformed the multilingual ver-sion of bert on several downstream tasks usinggerman data (germeval-201812, germeval-2014ner13, 10kgnad14).
more information can befound at the corresponding website15..for our setup, we performed a random hyperpa-rameter search using the following combinations:.
• learning rate:.
[0.01, 0.1, 0.001, 0.0001,.
12https://projects.fzai.h-da.de/iggsa/.
germeval-2018/.
13https://sites.google.com/site/.
germeval2014ner/data.
14https://tblock.github.io/10kgnad/15https://deepset.ai/german-bert.
figure 9: number of rejected label suggestions forgroup g3.
the x-axis displays the corrected label andthe y-axis the label suggestion.
for example, the upperleft corner shows that 8 suggestions of label refutewere corrected as unrelated by the users..figure 9 displays how student annotators fromg3 corrected label suggestions, per category.
asdiscussed in section 5.2 we observe a similarpattern as for annotator group g2.
the major-ity of label corrections are for the predicted cat-egory comment or corrections for a wrongly pre-dicted stance (e.g., predictions of support orrefute)..13unrelatedcommentsupportrefuterefutesupportcommentunrelated82310059410219903337024174number of label suggestion corrections (g3)020406080number