readonce transformers:reusable representations of text for transformers.
shih-ting lin∗ ashish sabharwal†.
tushar khot†.
∗ university of texas, austin, u.s.a.† allen institute for ai, seattle, u.s.a.j0717lin@utexas.edu, {ashishs,tushark}@allenai.org.
abstract.
we present readonce transformers, an ap-proach to convert a transformer-based modelinto one that can build an information-capturing, task-independent, and compressedrepresentation of text.
the resulting repre-sentation is reusable across different exam-ples and tasks, thereby requiring a documentshared across many examples or tasks to onlybe read once.
this leads to faster trainingand evaluation of models.
additionally, weextend standard text-to-text transformer mod-els to representation+text-to-text models, andevaluate on multiple downstream tasks: multi-hop qa, abstractive qa, and long-documentsummarization.
our one-time computed repre-sentation results in a 2x-5x speedup comparedto standard text-to-text models, while the com-pression also allows existing language modelsto handle longer documents without the needfor designing new pre-trained models..1.introduction.
transformer-based large scale language models(lms) (radford et al., 2018; devlin et al., 2019)are task-independent models that are surprisinglyeffective when directly ﬁne-tuned on many differ-ent end-tasks (rajpurkar et al., 2016; wang et al.,2019b,a).
however, this approach relies heavilyon using end-task supervision to learn to solve twosub-problems simultaneously: extract information1from an input document d and solve the end-task(e.g., answer a question about d).
this incentivizeslm-based models to learn to extract only task-speciﬁc—and even example-speciﬁc—informationwhen ﬁne-tuned on the end-task.
for example, aquestion answering (qa) model may learn to onlyextract the answer from d given the input question..∗the author’s work was primarily done during an intern-.
ship at the allen institute for ai..1by “extract information”, we mean implicitly or explicitly.
compute some representation of the document..figure 1: readonce transformers: rather thanlearning to extract information speciﬁc to each end-task, we use transformer-based encoders to build task-independent, reusable document representations once,and feed them into various representation+text trans-former models trained for end-tasks..this strategy, while effective on many datasets,is also inefﬁcient.
first, it requires model’s pre-trained weights to be ﬁne-tuned separately for eachend-task, even though the sub-problem of gather-ing the information content of the input documentd is shared across tasks.
second, each d mustbe re-read from scratch in the context of each ex-ample (e.g., once for each question) even whenmany examples share d. not only is this computa-tional redundancy undesirable, slow inference canquickly become a bottleneck in deployed, real-timesystems if models with billions of parameters mustre-read d for every input query..inspired by humans’ ability to read a documentand extract key information from it without hav-ing to know the use case in advance, we ask thefollowing question: can we use transformer-basedlms to build compressed representations of textthat are example- and task-independent, and hencereusable?
further, can we extend text-to-text trans-former architectures to consume such representa-.
proceedingsofthe59thannualmeetingoftheassociationforcomputationallinguisticsandthe11thinternationaljointconferenceonnaturallanguageprocessing,pages7129–7141august1–6,2021.©2021associationforcomputationallinguistics7129text-to-textmodeltext-to-textmodeltext-to-textmodeld1d1d1documentencoderd1...task-independentrepresentationstask-speciﬁcmodelsreadoncetransformersreadingcomprehensionsummarizationabstractive qarepr.
+ textmodelrepr.
+ textmodeltions in conjunction with text?.
prior representation learning approaches attemptto capture the meaning of sentences into a continu-ous vector (conneau et al., 2017; kiros et al., 2015;reimers and gurevych, 2019).
while they havebeen effective on downstream classiﬁcation tasks,it is unclear whether they can capture the informa-tion content of entire paragraphs.
moreover, theseapproaches focus on building ﬁxed-length repre-sentations that are used as the input features fortask-speciﬁc classiﬁers.
in contrast, our goal is to(a) use transformer-based lms to build compressedrepresentations that scale with the document size,and (b) combine them with example-speciﬁc textinputs to produce the more general text output..to this end, we propose an approach to convertany encoder-decoder based transformer lm (suchas bart (lewis et al., 2020)) into a new archi-tecture termed readonce transformer, with twokey parts: (1) a document encoder that reads docu-ments only once to create compressed, information-capturing, reusable representations that we refer toas readonce representations (2) a representa-tion+text model that consumes these documentrepresentations together with task- and example-speciﬁc plain text (e.g., a question) to producetext output (e.g.
an answer).
to ensure that ourcompressed representations capture the key facts,we use supervision from two factoid qa datasets,squad (rajpurkar et al., 2016) and unsuper-visedqa (lewis et al., 2019) to train readoncetransformers.
to solve an end-task, we only needto compute the readonce representations ofthe documents once and only train the representa-tion+text model to perform the end-task..our experiments demonstrate that these represen-tations are more effective at capturing informationcompared to baseline approaches.
our representa-tions also generalize to other tasks such as multi-hop qa (yang et al., 2018), abstractive qa (ko-cisk´y et al., 2018), and summarization (narayanet al., 2018).
since readonce representationsare computed only once, we can train and inferwith models 2x-5x faster than standard approaches,with only a marginal drop in accuracy (about 3 f1points on qa and 4 rouge-l points on summariza-tion for a 2x speedup).
moreover, the compressionratio parameter k of our representations providesan easy way to trade off computation time with ac-curacy.
speciﬁcally, our analysis suggests that theresulting model has a computation cost of roughly.
1/2r + 3/4k2 of the base lm, where r is thefrequency of document reuse..additionally, our compressed representation en-ables us to efﬁciently combine information fromlong (or multiple) documents enabling more accu-rate long-document summarization (cohan et al.,2018) without needing costly pre-training of newlms (beltagy et al., 2020; zaheer et al., 2020)..2 related work.
representation learning approaches are commonlyused to extract ﬁxed-length sentence embed-dings (conneau et al., 2017; kiros et al., 2015;wang et al., 2020) from variable-length text inputs.
such ﬁxed length representations have enabled thedevelopment of simpler downstream models thatdo not have to deal with the variable-lengths oftextual inputs.
however, these representations havemainly been used for simple classiﬁcation taskson short input texts (bowman et al., 2015; wanget al., 2019b).
the word-level representations fromrnns or transformers are also variable-length, butuncompressed.
while such representations havebeen re-used with rnns (peters et al., 2018) andare easy to combine with text input, it is not imme-diately clear how to combine representations fromtransformers with text, which is what we propose.
recent work (reimers and gurevych, 2019;he et al., 2020; artetxe and schwenk, 2019;karpukhin et al., 2020) has tried buildingdocument-embedding using large-scale languagemodels as well.
however these ﬁxed-length rep-resentations have mostly been built to identifysimilar documents (reimers and gurevych, 2019;karpukhin et al., 2020) and are not used directly forqa.
quase (he et al., 2020), also used question-answering supervision for transfer learning but donot produce re-usable representations.
artetxe andschwenk (2019) learned multi-lingual sentence em-beddings that may be able to capture the knowledgepresent in a sentence but they were designed forbilstms.
some large-scale lms have been espe-cially designed to handle long documents (yanget al., 2019; beltagy et al., 2020; zaheer et al.,2020) too but need to be pre-trained on large cor-pora, whereas we can use any pre-trained lm..aspects of our work also bears resemblanceto domain adaptation (daume iii and marcu,2006), transfer learning (pan and yang, 2010)and multi-task learning (caruana, 1993) but fo-cuses on learning information-capturing represen-.
7130tations from transformer-based models that has notbeen explored by prior work.
while model dis-tillation (hinton et al., 2015) can also result inspeedups, these techniques are orthogonal and canbe easily incorporated in our framework (as weshow in our experiments)..3 readonce transformers.
our goal in this work is to identify the optimalarchitecture to extract information-capturing re-usable representations.
at the same time, we alsoneed to ﬁnd the optimal architecture to use such rep-resentation in conjunction with text inputs.
so at ahigh level (as shown in fig.
1), we need to developtwo systems: (1) a model to compute the represen-tation, document encoder and (2) a general modelfor tasks that can consume vector representationsand text, representation+text model.
given therecent success and generality of encoder-decodermodels (radford et al., 2018; raffel et al., 2020;lewis et al., 2020), we focus on developing modelsfor such an architecture.
we present the potentialchoices for each model, with the ﬁnal model usedin our system indicated by a *..3.1 document encoder.
given an encoder-decoder model, there are dif-ferent ways to compute representations for a doc-ument d with tokens {t1, .
.
.
, tn}.
we focus onusing the output representation generated by theencoder, represented with hi for each token ti..fixed length aggregation.
the most commonapproach is to extract a single representation froma sequence of vector (kiros et al., 2015; conneauet al., 2017).
while this can be a very compact rep-resentation of a document, it tends to be very lossy,especially when dealing with large documents.
asa result, these representations are mainly used forclassiﬁcation (conneau et al., 2017; reimers andgurevych, 2019) or retrieval (karpukhin et al.,2020), and have not been shown to capture thecontent of the document.
e.g, infersent (conneauet al., 2017) presented a self-attentive approach toextract sentence embedding using:uθ(hi)hi.
r =.
(cid:88).
(1).
i.where uθ is a function that computes a scalar atten-tion over each hi.
to reduce information loss, weextend these models to produce m representationvectors by learning m sets of parameters θj for.
j ∈ {1, .
.
.
, m }, i.e., rj = (cid:80)uθj (hi) = eθj hi/ (cid:80).
i eθj hi..i uθj (hi)hi where.
special token representations.
with the ad-vent of transformer models, another common ap-proach is adding a special [cls] (radford et al.,2018; devlin et al., 2019) or <s> (liu et al., 2019)token to the context.
the output representation ofthis special token can then be used as inputs toclassiﬁers and other down-stream models.
again,a single representation can be lossy, so we gener-ate m representations by inserting multiple specialtokens.
we can dynamically adjust the numberof special tokens based on the input length to pro-duce a variable-length representation.
to achieve acompression-ratio of 1k special tokensand use their representations..k , we insert n.we consider two ways2 of inserting special to-kens into the context: (1) sufﬁx: add them at theend of the context3 (2) interleave: add them afterevery k tokens.
while the ﬁrst approach preservescontext continuity, the latter might more directlyincentivize the model to capture local context..sliding window aggregation*.
we apply theidea of aggregating single-vector representations togenerate a variable-length representation.
we applyan aggregation function f over sliding windows ofsize w tokens to capture the local context of thewindow (akin to cnns).
for a stride length of s,this would result in representation vectors:.
rj = f ({hs·j, · · · , hs·j+w })(2)where f ∈ {µ, α, ω} corresponds to mean-pooling,linear weighting (as described in eqn.
(1)), andmax-pooling, respectively..figure 2 shows how we would compute theserepresentations using a window-size of w=2 withno overlap (i.e.
s=2) and the linear weighting func-tion.
the resulting readonce representationswould have m = n/2 vectors where n is the num-ber of tokens in the input..sentencebert baseline.
for completeness, wealso use an existing transformer-based sentence-bert model (reimers and gurevych, 2019)4 to com-pute the representation of each sentence in the docu-ment.
since the space of these representation might.
2more complex designs such as special token embeddings,position embeddings, and indicator features are left as futurework..3preﬁxing special tokens generally worsened performance.
4we use the bert-large nli tokens which performedbetter than the nli-stsb representations in our experiments.
7131figure 2: sliding window aggregation approach to ex-tract meaning representations from a transformer-basedencoder.
linear weighted sum is used to aggregateeach w=2 vectors from the ﬁnal output layer into asingle vector, resulting in the readonce represen-tations with n/2 vectors..be different, we learn a single-layer feedforwardnetwork to project the representations into the rightspace.
for fair comparison to models with variablecompression ratio k, we also use sentencebertrepresentations for a sliding window of k tokens..3.2 representation+text model.
next, we present our modiﬁcation to downstreamtask models to use both text and our generatedreadonce representations.
since most nlptasks can be re-formulated as a text-to-text prob-lem (radford et al., 2018; raffel et al., 2020), wefocus on extending text-to-text encoder-decodermodels to a (vec+text)-to-text model..append to encoder*.
since the transformerblock in an encoder can handle any input lengthin each layer, one possible approach is to appendthe representations to the lth layer of the encoder.
this allows the model to focus on parsing the inputexample text(e.g., question) in the l-1 layers fol-lowed by focusing on answering the question in theremaining layers.
we show this model in figure 3where the encoder only processes the q tokens ofthe question for the ﬁrst l layers.
once the mreadonce representations are added to the lthlayer, all the subsequent layers produce m + qvectors by attending over both the representationsand text.
finally an unmodiﬁed decoder producesthe output answer..modify transformer block attention.
ratherthan just modifying the input, we consider an alter-nate approach of modifying the transformer blockitself.
similar to plotmachines (rashkin et al.,2020), we view the representation as a memory thatthe self-attention block can attend over (in addition.
figure 3: appending the readonce representationsto the lth layer of the encoder to extend standardencoder-decoder models to handle text+vector inputs..to the input text).
we modify the self-attentionblocks in both the encoder and the decoder5 to usetwo separate attention modules for both of theseinput types and averages the vectors.6 with thisdesign, ideally the representation+text model willgain extra capacity to model the interaction be-tween the representation and the input text..3.3 training readonce via qa.
given the overall architecture of such a system(shown in fig.
4), we next focus on training thismodel to produce readonce representations thatcapture the information present in the document.
while prior representation learning models haveoften focused on classiﬁcation tasks, we insteaduse the reading comprehension qa task to ensurethis information-capturing property.
if a model isable to use just the readonce representationsto answer the questions grounded in the document,the representations would contain the informationneeded to answer such questions..the key question here is: which qa datasetsare most suitable for training a compact yetinformation-capturing document representation?.
low-level semantic qa datasets (michael et al.,2018; he et al., 2015) don’t allow for any compres-sion as the questions require the knowledge aboutevery word in the input sentence.
more complexmulti-hop qa datasets such as hotpotqa (yanget al., 2018) are also not appropriate, as they focuson learning to reason in addition to capturing theinformation.
shallow reading comprehension tasksprovide a sweet spot between these two extremes,as extracting key information from the given docu-ment is sufﬁcient to answer the questions.
further,unlike semantic qa tasks, the questions only focuson the key facts mentioned in a document, whichcan be captured in a compressed representation.
we.
5only modifying the encoder or decoder resulted in.
slightly lower performance..6see app.
a.1 for the detailed formulas..
7132kiss(cid:29288)(cid:29288)(cid:29288)(cid:29288)(cid:29288)(cid:29288)andtellisaarcher.
(cid:29288)(cid:29288)(cid:29288)readonce representationlinear weightinglinear weightinglinear weightinglinear weightingencoder(cid:29288)(cid:29288)(cid:29288)layer lwhichencoder(cid:29288)(cid:29288)(cid:29288)(cid:29288)(cid:29288)(cid:29288)(cid:29288)(cid:29288)(cid:29288)readonce representationmovie(cid:29288)(cid:29288)(cid:29288)?decoderansweret al., 2019).
this increases the size of the trainingdataset while also introducing question diversity.
to avoid these automatically generated questionsoverwhelming training, we ensure that the samenumber of questions are selected from both thedatasets in each batch (by duplicating squad ques-tions).
in the same vein, we evaluate each modelbased on their performance on the squad task.7.
unless otherwise mentioned, we use the bart-large model in all our experiments, and optimizethe model with cross-entropy loss.
we set the learn-ing rate to 1e-5 for the weights initialized from thebart model, and to 1e-4 for randomly initializednewly added weights, which is shown beneﬁcial inpeters et al.
(2019).
for other hyper-parameters,we follow lewis et al.
(2020).
we ran all the ex-periments on rtx 8000 with 48gb gpu memory.
all experiments did not use the complete gpumemory, e.g.
experim we kept the batch size andgradient accumulation steps constant (both at 8)across different compression ratios..4.2 architecture evaluation.
to be able to evaluate the representations, we needto ﬁrst select the architecture of the model consum-ing these representations..4.2.1 representation+text model.
we explore the different choices for the represen-tation+text model model discussed in §3.2, as-suming the representation is generated by a simpledocument encoder model: mean aggregation overa sliding window with both window size and stridebeing 8 tokens.
the results are shown in table 1..architecture.
appendappend*appendmodifyatt.
designparameters.
squad.
l=1l=6l=12–.
em.
35.057.453.555.3.f1.
52.674.569.271.7.table 1: comparing bart-based architectures forjointly processing continuous representations and text..figure 4: readonce transformers architecture.
weuse aggregated sliding window representations (fig.
2)as document encoder to compute the readoncerepresentations.
we append these representations tothe lth layer of the encoder in our representation+textmodel (fig.
3).
this end-to-end model is ﬁne-tunedon qa tasks to train the document encoder to extractinformation-capturing representations..use two such datasets to train our models: squadand unsupervised qa..3.4 downstream usage of readonce.
to verify the generality of the readonce repre-sentations, we train models to perform multi-hopreasoning, abstractive qa and summarization us-ing our learned representations.
speciﬁcally, wefreeze the document encoder model and use it togenerate the representations for documents.
wefurther ﬁne-tune the representation+text modelon the downstream task to produce the output la-bel given the readonce representations and anyexample-speciﬁc input..4 representation learning experiments.
we ﬁrst evaluate the different potential architec-tural choices for extracting and using documentrepresentations discussed in §3.1 and §3.2, respec-tively.
while our main interest is in learning ef-fective representations, we also need to ﬁnd theoptimal representation+text model architecturethat can consume the representation..4.1 training setup.
we train the entire model on the factoid qa taskto ensure that the document representations docapture factual knowledge.
we primarily use thesquad reading-comprehension dataset (rajpurkaret al., 2016) containing more than 100,000 crowd-sourced factoid questions.
we further augment thisdataset with about 500,000 rule-based questionsfrom the unsupervisedqa (uqa) dataset (lewis.
we see that appending readonce representa-tions too early (l=1) or too late (l=12) in the en-coder stack is not as effective as appending abouthalf-way (l=6).8 we suspect that appending too.
7the scores on uqa correlate well with the scores on.
squad, with close to 90 f1 for most models..8we also experimented with l=3 and l=9, and didn’t ﬁnd.
any signiﬁcant gains..7133early does not allow the model to focus on under-standing the question, whereas appending too latedoes not leave enough room for cross-attention be-tween the question and representations..modifying the transformer block to attend overthese representations results in a reasonable f1score on squad, but it is still outperformed byour simple append architecture.
hence, for the restof this work, we stick to the simpler architectureof appending the representation at the 6th layer,denoted append(l=6)..4.2.2 document encoder.
given the representation+text model model ar-chitecture chosen above, we now explore potentialdocument encoder architectures to extract read-once representations.
for a fair comparison, weensure that all our evaluated representations use,on average across a dataset, the same number ofvectors to represent documents.
table 2 presentsem and f1 scores on squad for the various archi-tectural choices discussed in §3.1..architecture.
designparameters.
squad.
slidingwindow* w=8, s=8, f=αw=8, s=8, f=µslidingwindoww=8, s=8, f=ωslidingwindoww=16, s=8, f=αslidingwindowm=n/8sufﬁxm=n/8interleavem=n/8sentencebertm=#sent.
sentencebertm=21fixedlength.
em.
58.357.448.853.144.619.017.815.245.6.f1.
74.774.564.669.658.731.029.625.259.3.table 2: a comparison of different architectures forextracting continuous representations using bart en-coder.
each approach extracts representations of thesame length, namely 1/8th of the document length ei-ther for each document or on average across the dataset.
since sentencebert was trained on sentences, we alsoshow results for sentencebert(m=#sent) with n/32representations per document on average..the top 3 rows explore the sliding window ar-chitecture with both window size and stride lengthof 8 (i.e., no overlap between windows), with thethree different aggregation functions mentioned ear-lier.
we see that both the mean µ and the learnedweighted sum α have comparable performance onthis task, and outperform the max-pooling func-tion ω. we also evaluate the impact of increasingthe overlap between windows by increasing thewindow size (not changing the stride length keepsthe average number of vectors constant).
for the.
learned weighted sum function, this results in a5 point f1 drop, possibly due to the aggregationfunction having to operate over a larger window.9we next evaluate the approaches inspired byprior work where we add special tokens and usethe representations of these tokens.
for the bartmodel, we use a newly added [cls] token as ourspecial token.
we see from table 2 that neitherappending these tokens at the end nor interleavingthem in the input results in representations compa-rable to the sliding window based approaches.10the sliding window representations outperform thepre-trained sentence-based representations fromsentencebert irrespective of the number of vec-tors used.11 finally, if we ﬁx the representationlength to 21 vectors (computed based on the aver-age token length of squad: 163.7), the learnedrepresentations are still not as effective..4.3 final readonce architecture.
based on this set of experiments, we use the slid-ing window architecture for the document en-coder with learned weighted sum as the aggregationfunction, and append these representations to the6th layer in the ﬁnal task-dependent representa-tion+text model..5 downstream task experiments.
next, we evaluate the quality of our representa-tions by using them on three downstream tasks,different from the tasks readonce transformersare trained on, demonstrating faster training andinference.
we then show the beneﬁt of using ourrepresentation when documents are much longerthan the token limit of the underlying lm..5.1 experimental setup.
tasks: we consider three end-tasks, extractiveqa, summarization, and abstractive qa, to eval-uate our system using the following datasets: (1)hotpotqa (yang et al., 2018), a multi-hop reason-ing extractive qa dataset.
(2) xsum (narayanet al., 2018), an abstractive news summarizationdataset (3) narrativeqa (kocisk´y et al., 2018), anabstractive qa dataset where answers are not spans.
9we also compared w=8, s=2 with w=2, s=2 in our earlyexperiments and notice a similar trend—the smaller slidingwindow performs better..10special token preﬁx scored similar to the sufﬁx model.
11even when the slidingwindow approach is limited tom = n/32 vectors, it achieves a higher f1 score (52.4) thansentencebert..7134from the input document.
more details about thesedatasets and metrics provided in app.
b.
5.2 representation quality.
baselines: we compare readonce transform-ers to bart-based qa models that use the doc-ument text directly to answer the given question.
since these models use text directly without anylossy compression, their score is best viewed as anupper bound for any representation-based bartmodel, including ours.
we train the bart modelto generate the answer given the entire documentand question (we use “summary” as question forxsum).
in addition to bart-large, we evalu-ate two smaller models: bart-base and distil-bart (shleifer and rush, 2020).
since our repre-sentations were trained on squad and uqa, wealso ﬁrst ﬁne-tune all our bart models on thesame datasets..readonce models: we freeze the parametersof the document encoder to generate the represen-tations for all the documents in the datasets.
wethen use these representations with our represen-tation+text model, which is further ﬁne-tuned oneach end-task.
to evaluate the impact of our pre-training on qa datasets, we compare our modelto the readonce architecture initialized with thebart model weights, readonceφ.
to illustratethe architecture-independence of our approach andorthogonality to traditional compression methods,we also train and evaluate readonce models us-ing the bart-base and distilbart models.
thesemodels were also ﬁrst trained on squad +uqadatasets to learn the document representation.
seeapp.
c for more details..since our representation+text model can han-dle a variable number of representation vectors,we can change this compression ratio, on-the-ﬂy,without having to change the model architecture.
speciﬁcally, we can use a stride-length of k inour document encoder to generate representationsthat are 1/kth of the input length, and then feedthem to a downstream model.
by reducing k, wecan reduce the compression ratio and improve themodel accuracy, at the cost of increased runtime..interestingly, we discovered that we don’t evenneed to re-train document encoder for each valueof k. we can achieve a performance comparable toencoders trained individually for each value of k,by using the document encoder trained on k = 8and only varying k during the ﬁne-tuning step..first, we assess the ability of readonce rep-resentations to capture document information ascompared to using the original document text.
asshown in table 3, our framework at k=2 is about2x faster than bart-large while being only 3 f1and 4 rouge-l points behind this model with fullaccess to the text.
this demonstrates that read-once representations do capture most of the rel-evant information in the document.
the differentcompressed models can also result in smaller (dis-tilbart) or comparable (bart-base) speed-ups,but (1) our accuracy vs speed trade-off is moreeasily controllable via k and (2) we can applyour framework on these models to achieve simi-lar speedups.12.
hotpotqa narr.qa.
xsum.
f1 | sec.
r-l | sec.
r-l | sec..architecture.
bart-large.
bart-base.
distilbart.
readonceφ(k=8)readonce (k=8)readonce (k=2)ubsquad +uqa.
64.8 | 1.070.9 | 1.077.2 | 2.080.1 | 4.0.
41.9 | 1.155.7 | 1.166.7 | 2.170.5 | 5.7.
32.6 | 1.131.9 | 1.135.4 | 2.037.2 | 4.0.readonce (k=2)ubsquad +uqa.
71.5 | 0.676.5 | 1.3.
61.3 | 0.865.8 | 1.9.
31.6 | 0.932.2 | 1.5.readonce (k=2)ubsquad +uqa.
75.5 | 1.580.5 | 3.7.
65.1 | 1.870.5 | 5.3.
33.4 | 1.636.5 | 3.6.table 3: performance of readonce transformerson three datasets, vs. corresponding text-to-text trans-former models with full access to document text (i.e.
upper bounds).
we also show the training time (secs)per batch for each model.
our representations areable to reduce the training time compared to the upper-bounds at the cost of small drops in accuracy..lastly, we note that the readonceφ system,which simply uses the bart model parameters,is about 6 f1 and 14 rouge-l points behind ourmodel with learned representations.
this showsthat our model does utilize the factoid questionsto learn to extract meaningful representations —without our training, the representations obtainedfrom the pre-trained models are not as effective.13.
12while more recent lms can outperform bart (e.g.
pe-gasus (zhang et al., 2020) for summarization), we believesimilar tradeoffs can be achieved by applying our frameworkon these newer models..13we also observe drops in score when using the bartmodel parameters in only the document encoder or only therepresentation+text model..71355.3 model efﬁciency.
one key advantage of readonce representa-tions is that the model needs to read the documentonly once, and can reuse pre-computed representa-tions for multiple examples or even multiple tasks.
speciﬁcally, if a document is repeated across rexamples (the replication factor) and we use acompression ratio of k, our computation cost perquestion is roughly only (1/2r + 3/4k2) relativeto a baseline seq2seq model (cf.
app.
c.3 for ananalysis).
in other words, the higher the replicationfactor r or the compression ratio k, the higherthe speedup achieved via readonce representa-tions..our model exhibits a speedup of 2x-5x in train-ing time compared to the different bart architec-tures (figure 5).
similarly, we observe a 2x-3xspeedup in the inference time (as shown in fig-ure 6), which again plateaus out at k=8..note that the time reported for our model in-cludes the cost of reading readonce representa-tions from disk as well as some ﬁxed costs.
thesecosts form a larger fraction of the overall time forfaster models.
hence, while our speedups do notexactly match up to the theoretical analysis, theempirical trends are as expected: we see largerspeedups on the narrativeqa dataset which has ahigher replication factor r. in general, the r valuefor our datasets (e.g., r=29.7 for narrativeqa) iswithin the range of other datasets (e.g., r=9.4 fornewsqa and r=13.9 for drop).
note that evenwhen r=1 (e.g., xsum), we observe a speedupdue to the compression ratio k..figure 6:inference time (seconds) per batch.
forreadonce models, document representations are pre-computed and cached, resulting in a 2-3x speedup..varying the values of the compression ratio k. asshown in figure 7, across all three of our datasets,as the value of k increases, the model’s accuracygoes down due to increased compression but sodoes the training time.
as compared to the upper-bound bart-large model, we see a large gainin speed when k=2 with diminishing gains as kreaches 8..figure 7: the training time (seconds per batch) vs per-formance trade-off achieved by the readonce modelwith different values of k on our three evaluation tasks.
the points annotated with “k=i” indicate the read-once models and “ub” indicate their correspondingupper bound.
all evaluations use the bart-largemodel.
as k increases, the readonce model can betrained faster at the cost of accuracy with diminishinggains after k=4..5.5 handling long documents.
compressing document representations also en-ables the downstream model to reason over docu-ments longer than its maximum token length limitt. for example, we can compute representationsof document chunks with upto t tokens each andconcatenate them together.
since these represen-tations do not rely on any position embeddings inrepresentation+text model, theoretically we canuse as many representation vectors as needed..figure 5: training time (seconds) per batch.
forreadonce models, document representations are pre-computed and cached, resulting in a 2-5x speedup..5.4 efﬁciency-accuracy tradeoff.
we also perform a more ﬁne-grained analysis ofthe efﬁciency-accuracy tradeoff in readonce by.
given gpu memory limits, lets assume we canonly accommodate documents upto length t. given.
71361.02.03.04.05.06.0training time per batch2030405060708090f1k=2k=4k=8k=16ubk=2k=4k=8k=16ubk=2k=4k=8k=16ubhotpotqa (f1)narr.qa (r-l)xsum (r-l)2030405060708090r - la compression ratio k, we can compute read-once representations for k such length-t chunks,increasing the capacity of our downstream modelto t*k.14 for simplicity, we ignore the question asit tends to be much shorter than t..to assess the impact of increased model capac-ity, we evaluate our learned representations on thelong document summarization task pubmed (co-han et al., 2018).15 we follow cohan et al.
(2018)and only include the ﬁrst 4 sections from each doc-ument (average length=2270 tokens).
we vary thememory budget from t=512 to t=256 and com-pare our approach to two bart seq2seq baselines:a simple truncation baseline with t /4 tokens fromeach section, and a sliding-window baseline oftenused in qa models for summarization extendedhere by concatenating summaries from length-tchunks of the input document.
for the readoncetransformer with a compression ratio of k, we canaccommodate k*t/4 tokens per section, resultingin a total of t representations from the 4 sections.
we choose to obtain these t representations usingk/2 chunks from each section, with each chunkcontaining t/2 tokens.16.
figure 8: accuracy of models under different maxi-mum window length assumptions on pubmed dataset.
readonce transformers stay substantially more ac-curate as the maximum window length decreases..rouge-l scores of these models are depictedin figure 8. as we reduce t for the underlyingtransformer model from 512 to 256, the score ofthe baseline bart model drops to 35.5 rouge-l.when used with the sliding window technique, theperformance is even worse, likely due to the naiveaggregation of the summaries.
our approach, on.
14if we allow overlap of o tokens between chunks, the.
capacity changes to t*k – o*(k–1).
15we also evaluate narrativeqa, see app.
c.216see appendix c.1 for more details..the other hand, concatenates document representa-tions, allowing the downstream model to build a co-herent summary.
we see the rouge-l score onlydrops to 36.6 when k=2 (with model capacity drop-ping from 1024 to 512 tokens) and a much smallerdrop from 37.0 to 36.5 when k=8 (with model ca-pacity dropping from 3520 to 1472 tokens).
thissimulation shows that concatenating readoncerepresentations is a simple yet effective way toincrease the capacity of existing models..6 conclusion.
this work introduced readonce transformers,a novel approach for using large scale transformer-based language models to both build and con-sume reusable document representations.
akinto humans’ ability to read a document and ex-tract useful information without knowing the end-use, readonce representations are compact,information-capturing document representationsthat can be pre-computed once, in a task- andexample-independent fashion..our results on extractive qa, summarization,and abstractive qa tasks demonstrate that usingreadonce representations, in lieu of re-readingdocument text in the context of every example, re-sults in substantially faster training and inference,at a modest cost in accuracy.
the readonceframework also offers an easy way to control thetrade off between speed and accuracy (via the com-pression ratio parameter), and enables the use ofstandard transformer architectures on long docu-ments beyond the model’s token limit..identifying the ideal compact document repre-sentations in our controlled setting opens up thepossibility of efﬁcient open-domain qa, wheremodels retrieve and reason directly over these rep-resentations.
we leave an exploration of the train-ing of the retrieval function, often with only answersupervision and ideally in an end-to-end setting, tofuture work..acknowledgements.
we thank dirk groeneveld for providing the outputof the quark system for hotpotqa and the beakerteam for their support with the experiments..references.
m. artetxe and holger schwenk.
2019. massively mul-tilingual sentence embeddings for zero-shot cross-lingual transfer and beyond.
tacl, 7:597–610..713737.835.533.837.036.836.436.634.036.536.934.637.2iz beltagy, matthew e. peters, and arman cohan.
2020. longformer: the long-document transformer.
arxiv:2004.05150..samuel r. bowman, gabor angeli, christopher potts,and christopher d. manning.
2015. a large anno-tated corpus for learning natural language inference.
in emnlp..r. caruana.
1993. multitask learning: a knowledge-.
based source of inductive bias.
in icml..arman cohan, franck dernoncourt, doo soon kim,trung bui, seokhwan kim, w. chang, and nazligoharian.
2018. a discourse-aware attention modelfor abstractive summarization of long documents.
innaacl-hlt..alexis conneau, douwe kiela, holger schwenk, lo¨ıcbarrault, and antoine bordes.
2017. supervisedlearning of universal sentence representations fromnatural language inference data.
in emnlp..hal daume iii and daniel marcu.
2006. domain adap-tation for statistical classiﬁers.
journal of artiﬁcialintelligence research, 26:101–126..jacob devlin, ming-wei chang, kenton lee, andkristina toutanova.
2019. bert: pre-training ofdeep bidirectional transformers for language under-standing.
in naacl..dirk groeneveld, tushar khot, ashish sabharwal, et al.
2020. a simple yet strong pipeline for hotpotqa.
in emnlp..hangfeng he, qiang ning, and dan roth.
2020.quase: question-answer driven sentence encoding.
in acl..luheng he, m. lewis, and luke zettlemoyer.
2015.question-answer driven semantic role labeling: us-ing natural language to annotate natural language.
inemnlp..geoffrey hinton, oriol vinyals, and jeffrey dean.
2015. distilling the knowledge in a neural net-work.
in neurips deep learning and representa-tion learning workshop..v. karpukhin, barlas o˘guz, sewon min, patrick lewis,ledell yu wu, sergey edunov, danqi chen, andwen tau yih.
2020. dense passage retrieval for open-domain question answering.
in emnlp..ryan kiros, yukun zhu, russ r salakhutdinov,richard zemel, raquel urtasun, antonio torralba,and sanja fidler.
2015. skip-thought vectors.
inneurips..tom´as kocisk´y, jonathan schwarz, p. blunsom, chrisdyer, k. hermann, g´abor melis, and edwardgrefenstette.
2018. the narrativeqa reading com-prehension challenge.
tacl, 6:317–328..mike lewis, yinhan liu, naman goyal, mar-jan ghazvininejad, abdelrahman mohamed, omerlevy, ves stoyanov,and luke zettlemoyer.
2020. bart: denoising sequence-to-sequence pre-training for natural language generation, translation,and comprehension.
in acl..patrick lewis, ludovic denoyer, and sebastian riedel.
2019. unsupervised question answering by clozetranslation.
in acl..yinhan liu, myle ott, naman goyal, jingfei du, man-dar joshi, danqi chen, omer levy, mike lewis,luke zettlemoyer, and veselin stoyanov.
2019.roberta: a robustly optimized bert pretraining ap-proach.
arxiv preprint arxiv:1907.11692..julian michael, gabriel stanovsky, luheng he, i. da-gan, and luke zettlemoyer.
2018. crowdsourc-ining question-answer meaning representations.
naacl..s. narayan, shay b. cohen, and mirella lapata.
2018.don’t give me the details, just the summary!
topic-aware convolutional neural networks for extremesummarization.
in emnlp..sinno jialin pan and qiang yang.
2010. a survey ontransfer learning.
ieee transactions on knowledgeand data engineering, 22:1345–1359..matthew e. peters, mark neumann, mohit iyyer, mattgardner, christopher clark, kenton lee, and lukezettlemoyer.
2018. deep contextualized word repre-sentations.
in naacl-hlt..matthew e. peters, mark neumann, robert l lo-gan, roy schwartz, vidur joshi, sameer singh, andnoah a. smith.
2019. knowledge enhanced contex-tual word representations.
in emnlp..alec radford, karthik narasimhan, tim salimans, andimproving language under-ilya sutskever.
2018.standing with unsupervised learning.
technical re-port, openai..colin raffel, noam shazeer, adam roberts, katherinelee, sharan narang, michael matena, yanqi zhou,wei li, and peter j liu.
2020. exploring the limitsof transfer learning with a uniﬁed text-to-text trans-former.
jmlr, 21(140):1–67..pranav rajpurkar, jian zhang, konstantin lopyrev, andpercy liang.
2016. squad: 100,000+ questions formachine comprehension of text.
in emnlp..hannah rashkin, a. c¸ elikyilmaz, yejin choi, and jian-feng gao.
2020. plotmachines: outline-conditionedgeneration with dynamic plot state tracking.
inemnlp..nils reimers and iryna gurevych.
2019. sentence-bert: sentence embeddings using siamese bert-networks.
in emnlp/ijcnlp..7138sam shleifer.
and alexander m. rush.
2020.arxiv,.
pre-trained summarization distillation.
abs/2010.13002..alex wang, yada pruksachatkun, nikita nangia,amanpreet singh, julian michael, felix hill, omerlevy, and samuel bowman.
2019a.
superglue: astickier benchmark for general-purpose language un-derstanding systems.
in neurips..alex wang, amanpreet singh, julian michael, felixhill, omer levy, and samuel r bowman.
2019b.
glue: a multi-task benchmark and analysis plat-form for natural language understanding.
in iclr..shuohang wang, yuwei fang, siqi sun, zhe gan,yu cheng, jingjing liu, and jing jiang.
2020.cross-thought for sentence encoder pre-training.
inproceedings of the 2020 conference on empiricalmethods in natural language processing (emnlp),pages 412–421, online.
association for computa-tional linguistics..zhilin yang, zihang dai, yiming yang, jaime car-bonell, russ r salakhutdinov, and quoc v le.
2019.xlnet: generalized autoregressive pretraining forlanguage understanding.
in neurips..zhilin yang, peng qi, saizheng zhang, yoshua ben-gio, william w. cohen, ruslan salakhutdinov, andchristopher d. manning.
2018.hotpotqa: adataset for diverse, explainable multi-hop questionanswering.
in emnlp..manzil zaheer, guru prashanth guruganesh, avidubey, joshua ainslie, chris alberti, santiago on-tanon, philip minh pham, anirudh ravula, qifanwang, li yang, and amr mahmoud el houssienyahmed.
2020. big bird: transformers for longer se-quences.
in neurips..jingqing zhang, yao zhao, mohammad saleh, and pe-ter j. liu.
2020. pegasus: pre-training with ex-tracted gap-sentences for abstractive summarization.
in icml..7139a model details.
a.1 the detailed architecture of modiﬁed.
transformer block attention.
in this variation of representation+text model,the modiﬁed self-attention block uses two separateattention modules for both of these input types andaverages the vectors.
speciﬁcally, let hlenc be thematrix of hidden states generated from the lth layerof a standard transformer:enc = attn(hl−1.
(3)where attn(q, k, v) is the attention module usedin the transformer that takes q, k, v as the query,key, and value matrices.
to take extra readoncerepresentations as an input, we instead computehl.
enc , hl−1enc ).
enc , hl−1.
hl.
enc =(attn(hl−1(hl−1attn(cid:48).
enc , hl−1enc , r, r))/2.
(cid:48).
enc , hl−1.
enc )+.
(4).
where attn(·) is a separate attention module toinclude the readonce representations r in ourrepresentation+text model, whose weights areinitialized by the corresponding weights in attn(·)to speed up the training.
for the decoder in therepresentation+text model, we also compute thehidden states of each layer as per eqn.
4 so thatthe model can attend over the extracted documentinformation during the decoding process too..enc as:hl.
b dataset details.
(1) hotpotqa (yang et al., 2018) is a multi-hopreasoning dataset that requires models to aggregateinformation from two paragraphs to produce theanswer (a span from the input paragraphs).
we fo-cus on their distractor setting where they addition-ally provide models with 8 distractor paragraphs.
for efﬁciency, we use the output of the quark sys-tem (groeneveld et al., 2020) which selects up to512 tokens (including the question) from the inputparagraphs.
we use the answer em and f1 scoresas the metrics..(2) xsum (narayan et al., 2018) is an abstrac-tive news summarization dataset that requires mod-els to generate summaries that go beyond simply ex-tracting key sentences.
we use the rouge-l summ.
score17 commonly used for summarization datasets,which computes the union-lcs of the longest com-mon subsequences (lcs) between each pair of ref-erence and hypothesis sentences.
in contrast, the.
17https://github.com/google-research/.
google-research/tree/master/rouge.
standard rouge-l score computes lcs betweenthe reference and hypothesis, treating both of themas one sentence..(3) narrativeqa (kocisk´y et al., 2018) is anabstractive qa dataset where answers may not beextractive spans in the input document.
modelswould need to understand the content of the docu-ment to generate such answers.
we use the samerouge-l summ.
score as for the summarizationtask.18.
(4) pubmed (cohan et al., 2018) is an abstrac-tive long-document summarization dataset speciﬁ-cally focusing on the scientiﬁc publications.
thelarge number of tokens in each document makes ithard for standard pretrained transformers to dealwith.
we use the same rouge-l summ.
score asfor xsum..c experiment setting for distilbart.
we follow shleifer and rush (2020) to obtain ourdistilbart model used in §5.
speciﬁcally, we ﬁrstcreate a student model with 12-layer encoder and 6-layer decoder from bart-largesquad +uqa usingthe “shrink and fine-tune” distillation describedin shleifer and rush (2020), which has been shownto be effective for bart model on summarizationtasks.
we then further ﬁnetune the student modelon squad+uqa, and exploit the resulting modelas our distilbart..c.1 setup for the pubmed dataset in thelong-document experiment.
we follow cohan et al.
(2018) and only include4 sections from document.
after the truncation,the average number of tokens in the documents inthis dataset is 2270, with 90% of the documents be-ing under 4253 tokens.
to include the informationfrom each section, we evenly distribute the lengthbudget t across the sections.
this would mean, forthe baseline bart seq2seq model, each sectionis ﬁrst truncated to t/4 tokens, then the 4 sectionsare concatenated as the input.
as for readoncetransformers, we ﬁrst compute representations fork/2 chunks of each section with length t/2 tokens,then aggregate them as the ﬁnal readonce rep-resentations for the input document.
in this case,even when k equals 2, we are allowed to includeone chunk from each section without exceeding the.
18in our experiments, we did not notice any substan-tial difference between the simple rouge-l metric and thissummarization-based metric..7140backward pass) as ten 2 and that of the decoder astdn 2, where te, td capture the complexity of theencoder and decoder model respectively and n 2captures the self-attention over the input context.
we ignore the self-attention over the decoded textas it is unchanged across models....12.
(cid:1) (cid:0) c.for any baseline seq2seq model,.
the com-putational cost of processing an example (con-text+question) can be captured by (te + td)(c +q)2. for readonce transformers, the cost ofcomputing a context’s representation, amortizedover the r questions that share this context, isc2ter .
once the compressed context representa-tion is appended to the question encoder at layerl (out of 12), the rest of the encoder computation(cid:16) l12 q2 + (cid:0)1 − lcosts te ·.
the de-coder’s computation cost is td · (cid:0) cwhen l = 6 and q (cid:28) c.k + q(cid:1)2(cid:17)k + q(cid:1)2k , the net com-putational cost (without caching) simpliﬁes totec2r + tec2k2 .
assuming te ≈ td = t , thisequals t c2 (cid:0) 1(cid:1).
in contrast, the baselinemodel’s cost simpliﬁes to 2t c2.
the efﬁciencygain of readonce transformers over the base-line encoder-decoder model is therefore roughly12.
(cid:0)1/r + 3/2k2(cid:1).
additionally, when we use these representationsfor multiple training runs, inferences, downstreamtasks, etc., the cost of computing the ﬁxed repre-sentations is basically amortized to a constant term.
as a result, over multiple runs, using readoncerepresentations now reduces the cost of the build-ing and using models to just t c2 32k2 .
so usingthese cached representations amortized over mul-tiple epochs/runs, improves the efﬁciency gainsfurther to 3/4k2..2k2 + tdc2r + 32k2.
length limit.
for the bart + slidingwindow base-line, we concatenate summaries from 16 chunks oflength t with 4 chunks from each section..c.2 handling long documents: narr.qa.
aside from pubmed, we also evaluate the abilityof readonce transformers to handle long doc-uments on the narrativeqa dataset.
the averagenumber of tokens in the documents in this datasetis 668.6, with 90% of the documents being under981 tokens.
the results are depicted in figure 9..figure 9: accuracy of models under different max-imum window length assumptions on narrativeqadataset.
readonce transformers stay substantiallymore accurate as the maximum window length de-creases..as we reduce t for the underlying transformermodel from 384 to 128, the score of the baselinebart model drops signiﬁcantly from 59.1 to 38.8.with k=2, our model consistently outperforms thebaseline model but exhibits a similar drop in scoreas the maximum token limit of this model witht=128 is still only 208 tokens (as per the equationabove).
on the other hand, with k=8 and t=128,we can handle documents up to 688 tokens, therebyrequiring no truncation on 50% of the exampleseven in this extreme scenario.
as a result, we onlysee a 5.2 point drop in score as t is decreased from384 to 128. these simulations provide strong evi-dence of the ability of readonce transformersto handle long documents more effectively thanstandard transformer based models..c.3 read-once efﬁciency gains.
let c denote context length (as #tokens), q thequestion length, r the repetition factor (i.e., #ques-tions per context), and k the readonce com-pression factor.
for an input of n tokens, we treatthe computational cost of the encoder (forward or.
714166.159.157.757.856.050.852.543.238.8384256128