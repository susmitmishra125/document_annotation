every bite is an experience: key point analysis of business reviews.
roy bar-haim, lilach eden, yoav kantor∗, roni friedman, noam slonimibm research{roybar,lilache,yoavka,roni.friedman-melamed,noams}@il.ibm.com.
abstract.
previous work on review summarization fo-cused on measuring the sentiment toward themain aspects of the reviewed product or busi-ness, or on creating a textual summary.
theseapproaches provide only a partial view of thedata: aspect-based sentiment summaries lacksufﬁcient explanation or justiﬁcation for theaspect rating, while textual summaries do notquantify the signiﬁcance of each element, andare not well-suited for representing conﬂict-ing views.
recently, key point analysis (kpa)has been proposed as a summarization frame-work that provides both textual and quantita-tive summary of the main points in the data.
we adapt kpa to review data by introduc-ing collective key point mining for better keypoint extraction; integrating sentiment analy-sis into kpa; identifying good key point can-didates for review summaries; and leverag-ing the massive amount of available reviewsand their metadata.
we show empirically thatthese novel extensions of kpa substantiallyimprove its performance.
we demonstrate thatpromising results can be achieved without anydomain-speciﬁc annotation, while human su-pervision can lead to further improvement..1.introduction.
with their ever growing prevalence, online opinionsand reviews have become essential for our every-day decision making.
we turn to the wisdom ofthe crowd before buying a new laptop, choosing arestaurant or planning our next vacation.
however,this abundance is often overwhelming: reading hun-dreds or thousands of reviews on a certain busi-ness or product is impractical, and users typicallyhave to rely on aggregated numeric ratings, com-plemented by reading a small sample of reviews,which may not be representative.
the vast majorityof available information is left unexploited..∗first three authors equally contributed to this work..opinion summarization is a long-standing chal-lenge, which has attracted a lot of research interestover the past two decades.
early works (hu andliu, 2004; gamon et al., 2005; snyder and barzi-lay, 2007; blair-goldensohn et al., 2008; titov andmcdonald, 2008) aimed to extract, aggregate andquantify the sentiment toward the main aspects orfeatures of the reviewed entity (e.g., food, price,service, and ambience for restaurants).
such aspect-based sentiment summaries provide a high-level,quantitative view of the summarized opinions, butlack explanations and justiﬁcations for the assignedscores (ganesan et al., 2010)..an alternative line of work casts this problem asmulti-document summarization, aiming to create atextual summary from the input reviews (careniniet al., 2006; ganesan et al., 2010; chu and liu,2019; braˇzinskas et al., 2020b).
while such sum-maries provide more detail, they lack a quantitativeview of the data.
the salience of each element inthe summary is not indicated, making it difﬁcult toevaluate their relative signiﬁcance.
this is particu-larly important for the common case of conﬂictingopinions.
in order to fully capture the controversy,the summary should ideally indicate the propor-tion of favorable vs. unfavorable reviews for thecontroversial aspect..recently, key point analysis (kpa) has been pro-posed as a novel extractive summarization frame-work that addresses the limitations of the aboveapproaches (bar-haim et al., 2020a,b).
kpa ex-tracts the main points discussed in a collection oftexts, and matches the input sentences to these keypoints (kps).
the salience of each kp correspondsto the number of its matching sentences.
the setof key points is selected out of a set of candidates -short input sentences with high argumentative qual-ity, so that together they achieve high coverage,while aiming to avoid redundancy.
the resultingsummary provides both textual and quantitative.
proceedingsofthe59thannualmeetingoftheassociationforcomputationallinguisticsandthe11thinternationaljointconferenceonnaturallanguageprocessing,pages3376–3386august1–6,2021.©2021associationforcomputationallinguistics3376positive key pointsamazingly helpful and friendly staff.
modern furnishings and very clean.
the views are incredible.
the historic building is beautiful.
rooms are nice and comfortable.
the rooftop pool/patio is superb.
luxurious and spacious rooms.
the decor is very elegant.
the food here is excellent.
great location - walkable to anything..% reviews.
negative key points.
8.6% cons: poor customer service6.3% food is way over priced.
5.2% buffet was extremely disappointing.
4.9% plus it’s disgusting and unsanitary.
3.8% employees are rude.
3.6% rooms had a foul odor.
2.7% check-in took an hour.
2.6% staff unhelpful and uncaring.
2.4% building is very dated.
2.2% our room had mechanical issues..% reviews9.8%3.5%3.4%3.3%3.2%3.1%3.0%2.6%2.3%1.8%.
table 1: a sample summary produced by our system: key point analysis of an hotel with 2,662 reviews from theyelp dataset.
top 10 positive and negative key points are shown.
the balanced mixture of positive and negativekey points in this summary correlates with the hotel’s middling rating of 3.25 stars..key point: the views are incredible.
the scenery is amazing.
great view too, of the bellagio fountains.
i love this place for the scenery.
great room overlooking the pool.
all were beautifully appointed and had great views ofthe strip..key point: cons: poor customer serviceservice horrible from start to ﬁnish.
the front desk was so rude to us.
the people that check you in suck.
the guy at check in was far from friendly.
probably one of the worst customer experiences..table 2: sample matches of sentences to key points..views of the data, as illustrated in table 1. table 2shows a few examples of matching sentences tokps..originally developed for argument summariza-tion, kpa has also been applied to user reviewsand municipal surveys, using the same supervisedmodels that were only trained on argumentationdata, and was shown to perform reasonably well.
however, previous work only used kpa “out-of-the-box”, and did not attempt to adapt it to differenttarget domains..in this work we propose several improvementsto kpa, in order to make it more suitable to re-view data, and in particular to large-scale reviewdatasets:.
1. we show how the massive amount of reviewsavailable in datasets like amazon and yelp,as well as their meta-data, such as numericrating, can be leveraged for this task..2. we integrate sentiment classiﬁcation intokpa, which is crucial for analyzing reviews..3. we improve key point extraction by introduc-ing collective key point mining: extractinga large, high-quality set of key points from alarge collection of businesses in a given do-main..4. we deﬁne the desired properties of key pointsin the context of user reviews, and develop aclassiﬁer that detects such key points..we show empirically that these novel extensionsof kpa substantially improve its performance.
wedemonstrate that promising results can be achievedwithout any domain-speciﬁc annotation, while hu-man supervision can lead to further improvement.
overall, this work makes a dual contribution: ﬁrst,it proposes a new framework for review summa-rization.
second, it advances the research on kpa,by introducing novel methods that may be appliednot only to user reviews, but to other use cases aswell..2 background: key point analysis.
kpa was initially developed for summarizing largeargument collections (bar-haim et al., 2020a).
kpa matches the given arguments to a set of keypoints (kps), deﬁned as high-level arguments.
theset of kps can be either given as input, or automat-ically extracted from the data.
the resulting sum-mary includes the kps, along with their salience,represented by the number (or fraction) of match-ing arguments.
the user can also drill down fromeach kp to its associated arguments..bar-haim et al.
(2020b) proposed the followingmethod for automatic extraction of kps from a setof arguments, opinions or views, which they referto as comments:.
1. select short, high quality sentences as kp can-.
didates..33772. map each comment to its best matching kp,if the match score exceeds some thresholdtmatch..3. rank the candidates according to the number.
of their matches..4. remove candidates that are too similar to a.higher-ranked candidate1..5. re-map the removed candidates and theirmatched comments to the remaining candi-dates..6. re-sort the candidates by the number ofmatches and output the top-k candidates..given a set of kps and a set of comments, asummary is created by mapping each comment toits best-matching kp, if the match score exceedstmatch..the above method relies on two models: amatching model that assigns a match score for a(comment, kp) pair, and a quality model, that as-signs a quality score for a given comment.
thematching model was trained on the argkp dataset,which contains 24k (argument, kp) pairs labeledas matched/unmatched.
the quality model wastrained on the ibm-argq-rank-30kargs dataset,which contains quality scores for 30k arguments(gretz et al., 2020)2. the arguments in bothdatasets support or contest a variety of commoncontroversial topics (e.g., “we should abolish cap-ital punishment”), and were collected via crowd-sourcing..bar-haim et al.
showed that models trained onargumentation data not only perform well on argu-ments, but also achieve reasonable results on otherdomains, including survey data and sentences takenfrom user reviews.
however, they did not attemptto adapt kpa to these domains.
in the followingsections we look more closely at applying kpa tobusiness reviews..3 data and task.
in this work we apply kpa to business reviewsfrom the yelp open dataset3.
the dataset con-tains about 8 million reviews for 200k businesses.
each business is classiﬁed into multiple categories..1that is, their match score with that candidate exceeds the.
threshold tmatch..2both.
datasets.
are.
available.
from https:.
//www.research.ibm.com/haifa/dept/vst/debating_data.shtml.
3https://www.yelp.com/dataset.
traindevtest.
businesses (%)25%25%50%.
reviews1,289,7541,338,1232,622,054.table 3: yelp dataset split.
restaurants is by far the most common cate-gory, comprising the majority of the reviews.
be-sides restaurants, the dataset contains a wide vari-ety of other business types, from nail salons todentists.
we focus on two business categories inour experiments: restaurants (4.9m reviews)and hotels (258k reviews).
we will henceforthrefer to these business categories as domains.
eachreview includes, in addition to the review text, sev-eral other attributes, most relevant for our work isthe “star rating” on a 1-5 scale..we ﬁltered and split the dataset as follows.
first,we removed reviews with more than 15 sentences(10% of the reviews).
second, we removed busi-nesses with less than 50 reviews.
the remain-ing businesses were split into train, development(dev) and test set, as detailed in table 3..our goal is to create a summary of the reviewsfor a given business.
the summary would list thetop k positive and top k negative kps, and indicatefor each kp its salience in the reviews, representedby the percentage of reviews that match the kp.
a review is matched to a kp if at least one of itssentences is matched to that kp.
an example ofsuch summary is given in table 1. table 2 shows afew examples of matching sentences to kps..4 classiﬁcation models.
our system employs several classiﬁcation models:in addition to the matching and argument qualitymodels discussed in section 2, in this work we adda sentiment classiﬁcation model and a kp qualitymodel, to be discussed in the next sections..all four classiﬁers were trained by ﬁne-tuninga roberta-large model (liu et al., 2019).
priorto the ﬁne-tuning of each classiﬁer, we adaptedthe model to the business reviews domain, by pre-training on the yelp dataset.
we performed maskedlm pertraining (devlin et al., 2019; liu et al.,2019) on 1.5 million sentences sampled from thetrain set with a length ﬁlter of 20-150 charactersper sentence.
the following parameters were used:learning rate - 1e-5; 2 epochs.
training took twodays on a single v100 gpu..the matching model was then obtained by ﬁne-.
3378tuning the pre-trained model on the argkp dataset,with the parameters speciﬁed by bar-haim et al.
(2020b).
the quality model was ﬁne-tuned follow-ing the procedure described by gretz et al.
(2020),except for using roberta-large instead of bert-base, with learning rate of 1e-5..5.incorporating sentiment into kpa.
previous work on kpa has ignored the issue of sen-timent (or stance) altogether.
when applied to ar-gumentation data, it was assumed that the stance ofthe arguments is known, and kpa was performedseparately for pro and con arguments.
accordingly,the argkp dataset only contains (argument, kp)pairs having the same stance..there are, however, several advantages for in-corporating sentiment into kpa, in particular whenanalyzing reviews:.
1. separating positive kps from negative ones.
makes the summaries more readable..2. filtering neutral sentences, which are mostly.
irrelevant, may improve kpa quality..3. attempting to match only sentences and kpswith the same polarity may reduce both match-ing errors and run time..we developed a sentence-level sentiment clas-siﬁer for yelp data by leveraging the abundanceof available star ratings for short reviews.
we ex-tracted from the entire train set reviews having atmost 3 sentences and 64 tokens.
reviews with 1-2, 3 and 4-5 star rating were labeled as negative(neg, 20% of the reviews), neutral (neut, 11%)and positive (pos, 69%), respectively.
the reviewswere divided into a training set, comprising 235,481reviews, and a held-out set, comprising 26,166 re-views..the sentiment classiﬁer was trained by ﬁne-tuning the pre-trained model on the above trainingdata, for 3 epochs.
the ﬁrst two rows in table 4show the classiﬁer’s performance on the held-outset..since we ultimately wish to apply the classiﬁerto individual sentences, we also annotated a smallsentence-level benchmark of 158 reviews from theheld-out set, which contain 952 sentences.
we se-lected a minimal threshold ts for predicting pos orneg sentiment.
if both pos and neg predictionsare below this threshold, the sentence is predictedas neut.
the threshold was selected so that therecall of both pos and neg is at least 70%, while.
reviews.
sentences.
pos0.96pr 0.97p0.82r 0.88.neg0.860.910.810.70.neut0.580.470.480.47.table 4: sentiment classiﬁcation results on held-outdata.
precision (p) and recall (r) per class are shown,for both complete reviews and individual sentences..aiming to maximize precision4.
sentence-level per-formance on the benchmark using this thresholdis shown in the last two rows of table 4. almostall the errors involved neutral labels - confusionbetween positive and negative labels was very rare.
we integrate sentiment into kpa as follows.
weextract positive kps from a set of sentences classi-ﬁed as positive, and likewise for negative kps.
inorder to further improve precision, positive (neg-ative) sentences are only selected from positive(negative) reviews..when matching sentences to the extracted kpswe ﬁlter out neutral sentences and match sentencesonly to kps with the same polarity.
however, atthis stage we do not ﬁlter by the review polarity,since we would like to allow matching positivesentences in negative reviews and vice versa, aswell as positive and negative sentences in neutralreviews..6 collective key point mining.
kpa is an extractive summarization method: kpsare selected from the review sentences being sum-marized.
when generating a summary for a busi-ness with just a few dozens of reviews, the inputreviews may not have enough good kp candidates -short sentences that concisely capture salient pointsin the reviews.
this is a common problem for ex-tractive summarization methods, where it is oftendifﬁcult to ﬁnd sentences that ﬁt into the summaryin their entirety..we propose to address this problem by miningkps collectively for the whole domain (e.g., restau-rants or hotels).
the extracted set of domain kpsis then matched to the review sentences of eachanalyzed business.
this method can extract kpsfrom reviews of thousands of businesses, ratherthan from a single business, and therefore is muchmore robust.
it overcomes a fundamental limitationof extractive summarization - limited selection ofcandidate sentences, while sidestepping the com-.
4the chosen threshold was 0.79..3379sentencespos49,68549,655.neg48,75159,552.restaurantshotels.
table 5: number of positive and negative sentences ex-tracted for kp mining in each domain..plexity of sentence generation that exists in abstrac-tive summarization.
using the same set of kps foreach business makes it easy to compare differentbusinesses.
for example, we can rank businessesby the prevalence of a certain kp of interest..for each domain, we sampled 12,000 positivereviews and 12,000 negative reviews from the trainset, from which positive and negative kps wereextracted, respectively5.
we extracted positive andnegative sentences from the reviews using the senti-ment classiﬁer, as described in the previous section.
we ﬁltered sentences with less than 3 tokens ormore than 36 tokens (not including punctuation), aswell as sentences with less than 10 characters.
thenumber of positive and negative sentences obtainedfor each domain is detailed in table 5. we ranthe kp extraction algorithm described in section 2separately for the positive and negative sentencesin each domain.
we used a matching thresholdtmatch = 0.99. the length of kp candidates wasconstrained to 3-5 tokens, and their minimal qualityscore was tquality=0.426.
for each run, we selectedthe resulting top 70 candidates..the number of roberta predictions required bythe algorithm is o(#kp-candidates × #sentences).
while the input size in previous work was up toa few thousands of sentences, here we deal with50k-60k sentences per run.
in order to maintainreasonable run time, we had to constrain both thenumber of sentences and the number of kp can-didates.
we selected the top 25% sentences withthe highest quality score.
the maximal number ofkp candidates was 1.5 ×ns, where ns is thenumber of input sentences, and the highest-qualitycandidates were selected.
each run took 3.5-4.5hours using 10 v100 gpus..√.
7.improving key point quality.
previous work did not attempt to explicitly deﬁnethe desired properties kps should have, or to de-.
5to ensure diversity over the businesses, we employed atwo-step sampling process: ﬁrst sampled a business and thensampled a review for the business..6the threshold was selected by inspecting a sample of the.
training data..velop a model that identiﬁes good kp candidates.
instead, kp candidates were selected based ontheir length and argument quality, using the qualitymodel of gretz et al.
(2020).
this quality model,however, is not ideally suited for selecting kp can-didates for review summarization: ﬁrst, it is trainedon crowd-contributed arguments, rather than onsentences extracted from user reviews.
second,quality is determined based on whether the argu-ment should be selected for a speech supportingor contesting a controversial topic, which is quitedifferent from our use case..we ﬁll this gap by deﬁning the following require-.
ments from a kp in review summarization:.
1. validity: the kp should be a valid, under-standable sentence.
this would ﬁlter out sen-tences such as “it’s rare these days to ﬁndthat!”..2. sentiment: it should have a clear sentiment(either positive or negative).
this would ex-clude sentences like “i came for a companyevent”..3. informativeness: it should discuss someaspect of the reviewed business.
statementssuch as “love this place” or “we were verydisappointed”, which merely express an over-all sentiment should be discarded, as this infor-mation is already conveyed in the star rating.
the kp should also be general enough to berelevant for other businesses in the domain.
a common example of sentences that are toospeciﬁc is mentioning the business name or aperson’s name (“byron at the front desk is thebest!”)..4. single aspect: it should not discuss multi-ple aspects (e.g., “decent price, respectableportions, good ﬂavor”)..as we show in section 8, the method presentedin the previous sections extracts many kps that donot meet the above criteria.
in order to improve thissituation, we developed a new kp quality classiﬁer.
we created a labeled dataset for this task, as fol-lows.
we sampled from the restaurant and hotelreviews in the train set 2,000 sentences compris-ing 3-8 tokens and minimal argument quality oftquality.
each sentence was annotated for each ofthe above criteria7 by 10 crowd annotators, usingthe appen platform8.
we took several measures.
7the guidelines are included in the appendix.
8https://appen.com/.
3380to ensure annotation quality, following gretz et al.
(2020) and bar-haim et al.
(2020b).
first, the an-notation was performed by trusted annotators, whoperformed well on previous tasks.
second, we em-ployed the annotator-κ score (toledo et al., 2019),which measures inter annotator agreement, and re-moved annotators whose annotator-κ was too low.
the details are provided in the appendix.
for eachsentence and each criterion, the fraction of positiveannotations was taken to be its conﬁdence..the ﬁnal dataset was created by setting upperand lower thresholds on the conﬁdence value ofeach of the four criteria.
sentences that matchedall the upper thresholds were considered positive.
sentences that matched any of the lower thresholdswere considered negative.
the rest of the sentenceswere discarded.
the threshold values we used aregiven in the appendix.
overall, the dataset contains404 positive examples and 1,291 negative exam-ples..we trained a kp quality classiﬁer by ﬁne-tuningthe pretrained roberta model (cf.
section 4) onthe above dataset (4 epochs, learning rate: 1e-05).
figure 1 shows that this classiﬁer (denoted kp qual-ity ft) performs reasonably well on the dataset, ina 4-fold cross-validation experiment.
unsurpris-ingly, the argument quality classiﬁer trained onargumentation data is shown to perform poorly onthis task..the classiﬁer was used to ﬁlter bad kp candi-dates, as part of the kp mining algorithm (sec-tion 6).
candidates that passed this ﬁltering wereﬁltered and ranked by the argument quality modelas before.
we selected a threshold of 0.4 for theclassiﬁer, which corresponds to keeping 32% ofthe candidates, with precision of 0.62 and recall of0.82..8 evaluation.
8.1 experimental setup.
our evaluation follows bar-haim et al.
(2020b),while making the necessary changes for our setting.
let d be a domain, k a set of positive and nega-tive kps for d, and b a sample of businesses in d.applying kpa to a business b ∈ b using the set ofkps k and a matching threshold tmatch creates amapping from sentences in b’s reviews, denoted rb,to kps in k. by modifying tmatch we can explorethe tradeoff between precision (fraction of correctmatches) and coverage.
bar-haim et al.
performedkpa over individual sentences, and correspond-.
figure 1: kp quality precision vs. recall.
the ﬁne-tuned kp quality model (“kp quality ft”) and the orig-inal argument quality model are evaluated over the kpquality labeled dataset..ingly deﬁned coverage as the fraction of matchedsentences.
we are more interested in review-levelcoverage, since not all the sentences in the revieware necessarily relevant for the summary..given kpa results for b, k and tmatch, we can.
compute the following measures:.
1. review coverage: the fraction of reviews perbusiness that are matched to at least one kp,macro-averaged over the businesses in b..2. mean matches per review:.
the averagenumber of matched kps per review, macro-averaged over the businesses in b..computing precision requires a labeled sample.
we create a sample s by repeating the followingprocedure until n samples are collected:.
1. sample a business b ∈ b; a review r ∈ rb.
and a sentence s ∈ r..2. let the kp k ∈ k be the best match of s in.
k with match score m..3. add the tuple [(s, k), m] to s if m > tmin..the (s, k) pairs in s are annotated as cor-rect/incorrect matches.
we can then compute theprecision for any threshold tmatch > tmin by con-sidering the corresponding subset of the sample..we sampled for each domain 40 businesses fromthe test set, where each business has between 100and 5,000 reviews.
for each domain, and eachevaluated set of kps, we labeled a sample of 400pairs..we experimented with several conﬁgurations ofkpa adapted to yelp reviews, as described in theprevious sections.
these conﬁgurations are de-noted by the preﬁx rkpa.
each conﬁguration onlydiffers in the method it employs for creating the set.
33810.00.10.20.30.40.50.60.70.80.91.0recall0.30.40.50.60.70.80.91.0precisionargument qualitykp quality ftof domain kps (k):.
rkpa-base: this conﬁguration ﬁlters kp can-didates according to their length and quality, usingthe quality model trained on argumentation data.
in each domain, the top 30 mined kps for eachpolarity were selected..rkpa-ft: this conﬁguration applies the ﬁne-tuned kp quality model as an additional ﬁlter forkp candidates.
as with the previous conﬁguration,we take the top 30 kps for each polarity, in eachdomain..rkpa-manual: we also experimented withan alternative form of human supervision, wherethe set of automatically-extracted kps obtainedby the rkpa-base conﬁguration is manually re-viewed and edited.
kps may be rephrased, redun-dancies are removed and bad kps are ﬁltered out.
while this kind of task is less suitable for crowd-sourcing, it can be completed fairly quickly - aboutan hour per domain.
the task was performed bytwo of the authors, each working on one domainand reviewing the results for the other domain.
theﬁnal set includes: 18 positive and 15 negative kpsfor restaurants; 20 positive and 20 negative kps forhotels.9.
in addition to the above conﬁgurations, we alsoexperimented with a “vanilla” kpa conﬁguration(denoted kpa), which replicates the system of bar-haim et al.
(2020b), without any of the adaptationsand improvements introduced in this work.
noyelp data was used for pretraining or ﬁne-tuning themodels; key points were extracted independentlyfor each business in the test set; and no sentimentanalysis was performed.
instead of taking the top30 kps for each polarity, we took the top 60 kps..sample labeling.
similar to the kp qualitydataset, the eight samples of 400 pairs (two do-mains × four conﬁgurations) were annotated inthe appen crowdsourcing platform.
the annota-tion guidelines are included in the appendix.
eachinstance was labeled by 8 trusted annotators, andannotators with annotator-κ < 0.05 were removed(cf.
section 7).
we set a high bar for labeling cor-rect matches: at least 85% of the annotators hadto agree that the match is correct, otherwise it waslabeled as incorrect..we veriﬁed the annotations consistency by sam-pling 250 pairs, and annotating each pair by 16 an-notators.
annotations for each pair were randomlysplit into two sets of 8 annotations, and a binary la-bel was derived from each set, as described above.
the two sets of labels for the sample agreed on85.2% of the pairs, with cohen’s kappa of 0.610..8.2 results.
figure 2 shows the precision/coverage curves forthe four conﬁgurations, where coverage is mea-sured either as review coverage (left) or as meanmatches per review (right).
we ﬁrst note that allthree conﬁgurations developed in this work outper-form vanilla kpa by a large margin..the rkpa-base conﬁguration, which isonly trained on previously-available data, alreadyachieves reasonable performance.
for example, theprecision at review coverage of 0.8 is 0.77 forhotels and 0.83 for restaurants.
applying humansupervision for improving the set of key points,either by training a kp quality model on crowdlabeling (rkpa-ft), or by employing a human-in-the loop approach (rkpa-manual) leads tosubstantial improvement in both domains.
whileboth alternatives perform well, rkpa-ft achievesbetter precision at higher coverage rates..table 6 shows, for each conﬁguration in therestaurants domain, the top 10 kps ranked by theirnumber of matches in the sample.
the matchingthreshold for each conﬁguration corresponds to re-view coverage of 0.75. for the rkpa-base con-ﬁguration, we can see examples of kps that discussmultiple aspects (rows 3, 4), are too general (row8) or too speciﬁc (row 9).
these issues are muchimproved by applying the kp quality classiﬁer, asillustrated by the top 10 kps for the rkpa-ftconﬁguration..table 7 provides a more systematic comparisonof the kp quality in both conﬁgurations, based onthe top 30 kps for each polarity in each domain(120 in total per conﬁguration).
for each domainand conﬁguration, the table shows the fraction ofkps that conform to our guidelines (section 7).
inboth domains, kp quality is much improved for therkpa-ft conﬁguration..error analysis: by analyzing the top matchingerrors of both domains, we found several system-atic patterns of errors.
the most common type of.
9the set of kps for each conﬁguration is provided as sup-.
plementary material..10this result is comparable to (bar-haim et al., 2020b),who reported cohen’s kappa of 0.63 in a similar experiment..3382(a) hotels.
(b) restaurants.
figure 2: kpa precision vs. coverage.
#12.rkpa-basethe food here is superb.
service and quality was excellent..3.
4.large portions and reasonableprices.
fantastic food, location, and am-biance.
staff is interactive and friendly.
56 again, ﬂavorless and poor quality.
ingredients where fresh and tasty.
7.
8 we’ll certainly be back again..9 kevin, was rude and condescending..10 atmosphere is fun and casual..rkpa-ftthe food here is superb.
customer service is consistently ex-ceptional.
service is slow and inattentive..rkpa-manualfresh and tasty ingredientseverything was delicious.
quick and polite service..service is slow and inattentive..staff is interactive and friendly.
very affordable pricesatmosphere is fun and casual..service was friendly and welcom-ing.
the food is very ﬂavorful.
reasonably priced menu items.
the restaurant is beautifully deco-rated.
everything was cooked to perfec-tion.
the overall ambience was pleasing.
a lot of varietystaff are super nice & attentive..the dishes are extremely over-priced..the food was ﬂavorless.
table 6: top 10 key points for each conﬁguration in the restaurants domain, ranked by their number of matches inthe sample.
the matching threshold for each conﬁguration corresponds to review coverage of 0.75..rkpa-base rkpa-ft.hotelsrestaurants.
0.700.62.
0.850.95.the friendliest when she came to help us” and “thewaitress was friendly though.”..table 7: key point quality assessment.
for each do-main and conﬁguration, the table shows the fraction ofkps that conform to our guidelines..9 related work.
error consisted of a kp and a sentence making thesame claim towards different targets, e.g.
“we hadto reﬁll our own wine and ask for reﬁlls of soda.”was matched to “coffee was never even reﬁlled.”.
this usually stemmed from a too speciﬁc kp andwas more common in the restaurants domain..in some cases, a sentence was matched to anunrelated kp with a shared concept or term.
forexample, “cheap, easy, and ﬁlling” was matchedto “ordering is quick and easy”.
polarity errorswere rare but present, e.g.
“however she wasn’t.
previous work on review summarization was dom-inated by two paradigms: aspect-based sentimentsummarization and multi-document opinion sum-marization..aspect-based sentiment summarization.
thisline of work aims to create structured summariesthat assign an aggregated sentiment score or ratingto the main aspects of the reviewed entity (hu andliu, 2004; gamon et al., 2005; snyder and barzi-lay, 2007; blair-goldensohn et al., 2008; titov andmcdonald, 2008).
aspects typically comprise 1-2words (e.g., service, picture quality), and are ei-ther predeﬁned or extracted automatically.
a core.
33830.00.20.40.60.81.0review coverage0.550.600.650.700.750.800.850.900.951.00precision0.00.51.01.52.02.5mean matches per review0.550.600.650.700.750.800.850.900.951.00rkpa-baserkpa-ftrkpa-manualkpa0.00.20.40.60.81.0review coverage0.550.600.650.700.750.800.850.900.951.00precision0.00.51.01.52.02.5mean matches per review0.550.600.650.700.750.800.850.900.951.00rkpa-baserkpa-ftrkpa-manualkpasub-task in this approach is aspect-based senti-ment analysis: identiﬁcation of aspect mentions inthe text, which may be further classiﬁed into high-level aspect categories, and classiﬁcation of thesentiment towards these mentions.
recent exam-ples are (ma et al., 2019; miao et al., 2020; karimiet al., 2020)..the main shortcoming of such summaries is thelack of detail, which makes it difﬁcult for a user tounderstand why an aspect received a particular rat-ing (ganesan et al., 2010).
although some of thesesummaries include for each aspect a few supportingtext snippets as “evidence”, these examples may beconsidered anecdotal rather than representative..multi-document opinion summarization.
thisapproach aims to create a ﬂuent textual summaryfrom the input reviews.
a major challenge here isthe limited amount of human-written summariesavailable for training.
recently, several abstractiveneural summarization methods have shown promis-ing results.
these models require no summariesfor training (chu and liu, 2019; braˇzinskas et al.,2020b; suhara et al., 2020), or only a handful ofthem (braˇzinskas et al., 2020a).
as discussed in theprevious section, textual summaries provide moredetail than aspect-based sentiment summaries, butlack a quantitative dimension.
in addition, the as-sessment of such summaries is known to be difﬁ-cult.
as demonstrated in this work, kpa can beevaluated using straightforward measures such asprecision and coverage..introduced in this work may improve kpa perfor-mance in other domains as well..in future work we would like to generate richersummaries by combining domain level key pointswith “local” key points, individually extracted perbusiness.
it would also be interesting to adapt cur-rent methods for unsupervised abstractive summa-rization to generate key points..ethical considerations.
• our use of the yelp dataset has been reviewedand approved by both the data acquisition au-thority in our organization and the yelp team..• we do not store or use any user information.
from the yelp dataset..• we ensured fair compensation for crowd an-notators as follows: we set a fair hourly rateaccording to our organization’s standards, andderived the payment per task from the hourlyrate by estimating the expected time per taskbased on our own experience..• regarding the potential use of the proposedmethod - one of the advantages of kpa is thatit is transparent, veriﬁable and explainable -the user can drill down from each key point toit matched sentences, which provide justiﬁca-tion and supporting evidence for its inclusionin the summary..10 conclusion.
references.
we introduced a novel paradigm for summarizingreviews, based on kpa.
kpa addresses the limi-tations of previous approaches by generating sum-maries that combine both textual and quantitativeviews of the data.
we presented several extensionsto kpa, which make it more suitable for large-scalereview summarization: collective key point miningfor better key point extraction; integrating senti-ment analysis into kpa; identifying good key pointcandidates for review summaries; and leveragingthe massive amount of available reviews and theirmetadata..we achieved promising results over the yelpdataset without requiring any domain-speciﬁc an-notations.
we also showed that performance canbe substantially improved with human supervision.
while we focused on user reviews, the methods.
roy bar-haim, lilach eden, roni friedman, yoavkantor, dan lahav, and noam slonim.
2020a.
fromarguments to key points: towards automatic argu-ment summarization.
in proceedings of the 58th an-nual meeting of the association for computationallinguistics, pages 4029–4039, online.
associationfor computational linguistics..roy bar-haim, yoav kantor, lilach eden, roni fried-man, dan lahav, and noam slonim.
2020b.
quan-titative argument summarization and beyond: cross-in proceedings of thedomain key point analysis.
2020 conference on empirical methods in naturallanguage processing (emnlp), pages 39–49, on-line.
association for computational linguistics..sasha blair-goldensohn, tyler neylon, kerry hannan,george a. reis, ryan mcdonald, and jeff reynar.
2008. building a sentiment summarizer for local ser-vice reviews.
in nlp in the information explosionera (nlpix)..3384arthur braˇzinskas, mirella lapata, and ivan titov.
2020a.
few-shot learning for opinion summariza-in proceedings of the 2020 conference ontion.
empirical methods in natural language process-ing (emnlp), pages 4119–4135, online.
associa-tion for computational linguistics..arthur braˇzinskas, mirella lapata, and ivan titov.
2020b.
unsupervised opinion summarization asin proceedings of thecopycat-review generation.
58th annual meeting of the association for compu-tational linguistics, pages 5151–5169, online.
as-sociation for computational linguistics..giuseppe carenini, raymond ng, and adam pauls.
2006. multi-document summarization of evaluativein 11th conference of the european chap-text.
ter of the association for computational linguis-tics, trento, italy.
association for computationallinguistics..eric chu and peter liu.
2019. meansum: a neu-ral model for unsupervised multi-document abstrac-tive summarization.
in proceedings of the 36th in-ternational conference on machine learning, vol-ume 97 of proceedings of machine learning re-search, pages 1223–1232.
pmlr..jacob devlin, ming-wei chang, kenton lee, andkristina toutanova.
2019. bert: pre-training ofdeep bidirectional transformers for language under-in proceedings of the 2019 conferencestanding.
of the north american chapter of the associationfor computational linguistics: human languagetechnologies, volume 1 (long and short papers),pages 4171–4186, minneapolis, minnesota.
associ-ation for computational linguistics..akbar karimi, leonardo rossi, and andrea prati.
2020.adversarial training for aspect-based sentiment anal-ysis with bert..yinhan liu, myle ott, naman goyal, jingfei du, man-dar joshi, danqi chen, omer levy, mike lewis,luke zettlemoyer, and veselin stoyanov.
2019.roberta: a robustly optimized bert pretraining ap-proach.
corr, abs/1907.11692..dehong ma, sujian li, fangzhao wu, xing xie,and houfeng wang.
2019. exploring sequence-to-sequence learning in aspect term extraction.
in pro-ceedings of the 57th annual meeting of the asso-ciation for computational linguistics, pages 3538–3547, florence, italy.
association for computa-tional linguistics..zhengjie miao, yuliang li, xiaolan wang, and wang-chiew tan.
2020. snippext: semi-supervised opin-ion mining with augmented data.
proceedings ofthe web conference 2020..benjamin snyder and regina barzilay.
2007. multi-ple aspect ranking using the good grief algorithm.
in human language technologies 2007: the con-ference of the north american chapter of the asso-ciation for computational linguistics; proceedingsof the main conference, pages 300–307, rochester,new york.
association for computational linguis-tics..yoshihiko suhara, xiaolan wang, stefanos angelidis,and wang-chiew tan.
2020. opiniondigest: a sim-ple framework for opinion summarization.
in pro-ceedings of the 58th annual meeting of the asso-ciation for computational linguistics, pages 5789–5798, online.
association for computational lin-guistics..michael gamon, anthony aue, simon corston-oliver,and eric ringger.
2005. pulse: mining customeropinions from free text.
in advances in intelligentdata analysis vi, pages 121–132, berlin, heidel-berg.
springer berlin heidelberg..ivan titov and ryan mcdonald.
2008. a joint modelof text and aspect ratings for sentiment summariza-tion.
in proceedings of acl-08: hlt, pages 308–316, columbus, ohio.
association for computa-tional linguistics..kavita ganesan, chengxiang zhai, and jiawei han.
2010. opinosis: a graph based approach to abstrac-tive summarization of highly redundant opinions.
inproceedings of the 23rd international conferenceon computational linguistics (coling 2010), pages340–348, beijing, china.
coling 2010 organizingcommittee..shai gretz, roni friedman, edo cohen-karlik, as-saf toledo, dan lahav, ranit aharonov, and noamslonim.
2020. a large-scale dataset for argumentinquality ranking: construction and analysis.
aaai..minqing hu and bing liu.
2004. mining and summa-rizing customer reviews.
in proceedings of the tenthacm sigkdd international conference on knowl-edge discovery and data mining, kdd ’04, pages168–177, new york, ny, usa.
acm..assaf toledo, shai gretz, edo cohen-karlik, ronifriedman, elad venezian, dan lahav, michal ja-covi, ranit aharonov, and noam slonim.
2019. au-tomatic argument quality assessment - new datasetsin proceedings of the 2019 confer-and methods.
ence on empirical methods in natural languageprocessing and the 9th international joint confer-ence on natural language processing (emnlp-ijcnlp), pages 5624–5634, hong kong, china.
as-sociation for computational linguistics..appendices.
a key point quality dataset.
a.1 annotation guidelines.
below are the annotation guidelines for the kpquality annotation task:.
3385validitysentiment.
positiveconﬁdence >0.85clear sentiment with conﬁdence >0.6 no sentiment or sentiment conﬁdence <0.5.
negativeconﬁdence <0.8.
informativeness.
informative with conﬁdence >0.6.
multiple aspects conﬁdence <= 0.57.too speciﬁc\not informative;or doesn’t refer to an aspect withconﬁdence >0.6conﬁdence >= 0.85.table 8: criteria for creating the key point quality dataset from crowd annotations.
sentences that match all thepositive criteria are labeled as valid key points; sentences that match any of the negative criteria are labeled asinvalid key points, and the rest are excluded..key points; sentences that match any of the nega-tive criteria are considered invalid key points, andthe rest are excluded.
the conﬁdence of a crite-rion denotes the fraction of positive annotationsin the case of a binary choice, or the fraction ofannotations for a certain label otherwise..b key point matching annotation.
guidelines.
below are the match annotation guidelines for(sentence, kp) pairs:.
in this task you are presented with a business do-main, a sentence taken from a review of a businessin that domain and a key point..you will be asked to answer the following ques-.
tion: does the key point match the sentence?.
a key point matches a sentence if it captures thegist of the sentence, or is directly supported by apoint made in the sentence..the options are:.
• yes.
• no.
clear).
• faulty key point (not a valid sentence or un-.
in the following you will be presented with abusiness category and a sentence extracted froma customer review on a certain business in thatcategory.
you will be asked to answer the followingquestions:.
1. is this a valid, understandable sentence?
(yes.
/ no).
2. what is the sentiment this sentence expressestoward the reviewed business or aspect of thatbusiness?
(positive / negative / mixed senti-ment / neutral or unclear).
3. can this sentence be used to review as-pect(s) of another business under the samecategory?
(no, it is too business speciﬁc /no, it does not refer to certain aspects of thebusiness/ no, it is not informative / yes).
note: an aspect of a business is a single at-tribute of its overall service/product.
in ho-tels, for instance it could be the cleanliness ofthe room.
in most businesses it could be thefriendliness of the staff, the price, the conve-niency of location etc..4. does this sentence discuss more than one in-dependent aspect of the business?
(yes/no).
a.2 quality control.
annotators were excluded if their annotator-κscore (toledo et al., 2019), calculated for each ques-tion, was below any of these thresholds:.
• question #3 (informativeness): 0.3.
• question #4 (multiple aspects): 0.1.a.3 final dataset generation.
table 8 shows the criteria for the inclusion of asentence in the kp quality dataset.
sentences thatmatch all the positive criteria are considered valid.
3386