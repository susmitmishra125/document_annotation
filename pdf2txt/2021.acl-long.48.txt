cosy: counterfactual syntax for cross-lingual understanding.
sicheng yu1, hao zhang2,3, yulei niu2, qianru sun1, jing jiang11singapore management university, singapore2nanyang technological university, singapore3agency for science, technology and research, singaporescyu.2018@phdcs.smu.edu.sg, hao007@e.ntu.edu.sgyn.yuleiniu@gmail.com, {qianrusun,jingjiang}@smu.edu.sg.
abstract.
english:           i      bought      two      new      laptops      yesterday      ..pre-trained multilingual language models, e.g.,multilingual-bert, are widely used in cross-lingual tasks, yielding the state-of-the-art per-formance.
however, such models suffer froma large performance gap between source andtarget languages, especially in the zero-shotsetting, where the models are ﬁne-tuned onlyon english but tested on other languages forthe same task.
we tackle this issue by incorpo-rating language-agnostic information, speciﬁ-cally, universal syntax such as dependency re-lations and pos tags, into language models,based on the observation that universal syn-tax is transferable across different languages.
our approach, named counterfactual syn-tax (cosy), includes the design of syntax-aware networks as well as a counterfactualtraining method to implicitly force the net-works to learn not only the semantics butalso the syntax.
to evaluate cosy, we con-duct cross-lingual experiments on natural lan-guage inference and question answering usingmbert and xlm-r as network backbones.
our results show that cosy achieves the state-of-the-art performance for both tasks, withoutusing auxiliary dataset.1.
1.introduction.
with the emergence of bert (devlin et al.,2019), large-scale pre-trained language modelshave become an indispensable componentinthe solutions to many natural language process-ing (nlp) tasks.
recently, large-scale multilingualtransformer-based models, such as mbert (devlinet al., 2019), xlm (lample and conneau, 2019)and xlm-r (conneau et al., 2020a), have beenwidely deployed as backbones in cross-lingual nlptasks (wu and dredze, 2019; pires et al., 2019; ke-ung et al., 2019).
however, these models trained.
1our code is publicly available on github: https://.
github.com/pluviophileyu/cosy.
chinese:       (cid:6211) (cid:6870)(cid:4787) (cid:7470)(cid:7492) (cid:2921)(cid:3982) (cid:6786)(cid:9431) (cid:17395)(cid:20431) (cid:2059).
shared syntax:.
<root>.
i (cid:6211)[pron].
new (cid:6786)(cid:9431)[adj].
bought(cid:7470)(cid:7492)[verb].
<obj>.
laptops (cid:17395)(cid:20431)[noun].
yesterday (cid:6870)(cid:4787)[noun].
two (cid:2921)(cid:3982)[num].
.
(cid:2059)[punct].
figure 1: examples of two sentences in english andchinese that have the same meaning and share the samesyntax in the format of dependency relations and postags..on a single resource-rich language, e.g., english,all suffer from a large drop of performance whentested on different target languages, e.g., chineseand german—where the setting is called zero-shot cross-lingual transfer.
for example, on thexquad dataset, mbert achieves a 24 percent-age points lower exact match score on the targetlanguage chinese than on the training languageenglish (hu et al., 2020).
this indicates that thismodel has seriously overﬁtted english..an intuitive way to tackle this is to introducelanguage-agnostic information—the most transfer-able feature across languages, which is lacking inexisting multilingual language models (choenniin our work, we proposeand shutova, 2020).
to exploit reliable language-agnostic information—syntax in the form of universal dependency rela-tions and universal pos tags (de marneffe et al.,2014; nivre et al., 2016; zhou et al., 2019, 2021).
as illustrated in figure 1, the sentences in chineseand english share the same meaning but have differ-.
proceedingsofthe59thannualmeetingoftheassociationforcomputationallinguisticsandthe11thinternationaljointconferenceonnaturallanguageprocessing,pages577–589august1–6,2021.©2021associationforcomputationallinguistics577factual syntax.
i[pron].
new[adj].
<root>.
bought[verb].
<obj>.
laptops[noun].
randomize< >.
yesterday[noun].
two[num].
.
[punct].
randomize[ ].
i[pron].
new[adj].
i[sym].
new[cconj].
<dep>.
bought[verb].
<aux>.
laptops[noun].
<root>.
bought[aux].
<obj>.
laptops[adv].
yesterday[noun].
two[num].
.
[punct].
yesterday[pron].
two[det].
.
[verb].
dependency relation-levelcounterfactual syntax.
pos tag-levelcounterfactual syntax.
figure 2: illustration of counterfactual syntax gener-ation.
red color highlights the modiﬁed syntax withrandomized labels..ent word orders.
the order difference hampers thetransferability between english and chinese in con-ventional language models (with sequential wordsas input).
in contrast, it is clear from figure 1 thatthe two sentences share identical dependency rela-tions and pos tags.
thus, we can incorporate suchuniversal syntax2 information to enhance the trans-ferability across different languages.
to achievethis learning objective in deep models, we designsyntax-aware networks that incorporate the encod-ings of dependency relations and pos tags into theencoding of semantics..however, we ﬁnd that empirically the conven-tional attention-based incorporation of syntax, e.g.,relational graph attention networks (ishiwatariet al., 2020), has little effect on improving themodel.
one possible reason is that the learningprocess may be dominated by the pre-trained lan-guage models due to their strength in semantic rep-resentation learning, which leads to an overﬁttedmodel.
this raises the question of how to inducethe model to focus more on syntax while maintain-ing its original capability of representing seman-tics?
to this end, we propose a novel counterfac-tual syntax (cosy) method, inspired by causalinference (roese, 1997; pearl et al., 2009) and con-trastive learning (he et al., 2020)..the intuition behind cosy is to create copies oftraining instances with their syntactic features al-tered (see the “counterfactual” syntax in figure 2),and to force the encodings of the counterfactual in-.
2in the rest of this paper, syntax denotes universal syntax.
for simplicity..stances to be different from the encodings of theircorresponding factual instances.
in this way, themodel would learn to put more emphasis on the syn-tactic information when learning how to encode aninstance, and such encodings are likely to performwell across languages..we evaluate our cosy method on both questionanswering (qa) and natural language inference(nli) under cross-lingual settings.
experimentalresults show that, without using any additional data,cosy is superior to the state-of-the-art methods.
contributions: 1) we develop a syntax-aware net-work that incorporates transferable syntax in lan-guage models; 2) we propose a novel counterfac-tual training method that addresses the technicalchallenge of emphasizing syntax; and 3) extensiveexperiments on three benchmarks demonstrate theeffectiveness of our method for cross-lingual tasks..2 related work.
cross-lingual transfer.
large-scale pre-trainedlanguage models (devlin et al., 2019; liu et al.,2019) have achieved sequential success in variousnatural language processing tasks.
recent stud-ies (lample and conneau, 2019; conneau et al.,2020a) extend the pre-trained language models tomultilingual tasks and demonstrate their promi-nent capability on cross-lingual knowledge trans-fer, even under zero-shot scenario (wu and dredze,2019; pires et al., 2019; hsu et al., 2019)..motivated by the success of multilingual lan-guage models on cross-lingual transfer, severalworks explore how these models work and whattheir bottleneck is.
on the one hand, some studiesﬁnd that the shared sub-words (wu and dredze,2019; dufter and sch¨utze, 2020) and the parame-ters of top layers (conneau et al., 2020b) are cru-cial for cross-lingual transfer.
on the other hand,the bottleneck is attributed to two issues: (i) catas-trophic forgetting (keung et al., 2020; liu et al.,2020), where knowledge learned in the pre-trainingstage is forgotten in downstream ﬁne-tuning; (ii)lack of language-agnostic features (choenni andshutova, 2020; zhao et al., 2020) or linguistic dis-crepancy between the source and the target lan-guages (wu and dredze, 2019; lauscher et al.,2020).
in this work, we aim to tackle zero-shotand few-shot cross-lingual transfer by focusing onthe second issue..existing works can be roughly divided into twogroups.
the ﬁrst proposes to modify the lan-.
578guage model by aligning languages with paralleldata (zhao et al., 2020) or strengthening sentence-level representation (wei et al., 2020).
the secondgroup focuses on the learning paradigm for ﬁne-tuning on downstream tasks.
for instance, somemethods adopt meta-learning (nooralahzadeh et al.,2020; yan et al., 2020) or intermediate tasks train-ing (phang et al., 2020) to learn cross-lingualknowledge.
our cosy belongs to the secondgroup and ﬁlls the blank of using the syntacticinformation in zero-shot (few-shot) cross-lingualunderstanding.
counterfactual analysis.
counterfactual analy-sis aims to evaluate the causal effect of a variableby considering its counterfactual scenario.
counter-factual analysis has been widely studied in epidemi-ology (rothman and greenland, 2005) and socialscience (steel, 2004).
recently, counterfactual rea-soning has motivated studies in applications..in the community of computer vision, counter-factual analysis has been successfully applied inexplanation (goyal et al., 2019a,b), long-tailed clas-siﬁcation (tang et al., 2020a), scene graph gen-eration (tang et al., 2020b), and visual questionanswering (chen et al., 2020; niu et al., 2020; ab-basnejad et al., 2020)..in the community of natural language process-ing, counterfactual methods are also emerging re-cently in text classiﬁcation (choi et al., 2020), storygeneration (qin et al., 2019), dialog systems (zhuet al., 2020), gender bias (vig et al., 2020; shinet al., 2020), question answering (yu et al., 2020),and sentiment bias (huang et al., 2020).
to thebest of our knowledge, we are the ﬁrst to conductcounterfactual analysis in cross-lingual understand-ing.
different from previous works (zhu et al.,2020; qin et al., 2019) that generate word-level orsentence-level counterfactual samples, our coun-terfactual analysis dives into syntax level that ismore controllable than text and free from complexlanguage generation module..3 cosy: counterfactual syntax.
cosy aims to leverage the syntactic information,e.g., dependency relations and pos tags, to in-crease the transferability of cross-lingual languagemodels.
speciﬁcally, cosy implicitly forces thenetworks to learn to encode the input not only basedon semantic features but also based on syntacticfeatures through syntax-aware networks and a coun-terfactual training method..as illustrated in figure 3, cosy consists ofthree branches with each branch based on syntax-aware networks (san) indicated by a distinct color.
the main branch (in black) is the factual branchthat uses factual syntax as input.
the red and bluebranches are counterfactual branches using coun-terfactual dependency relations and counterfactualpos tags as input, respectively.
the counterfac-tual training method guides the black branch to putmore emphasis on syntactic information with thehelp of other two branches.
note that the red andblue branches work for counterfactual training, andonly the prediction from the black branch is usedin testing..below, we ﬁrst elaborate the modules of san insection 3.1, and then introduce the counterfactualtraining method in section 3.2..3.1 syntax-aware networks (san).
as shown in figure 3, san contains four majormodules: a set of feature extractors, a relationalgraph attention network (rgat), fusion projection,and a classiﬁer.
in this section, we use the routein the black branch as an example to elaborateeach module.
the set of feature extractors includethree components: a pre-trained language model,a dependency graph constructor and a pos tagsextractor.
pre-trained language model.
following previ-ous work (hu et al., 2020), we deploy a pre-trainedmulti-lingual language model, e.g., mbert (de-vlin et al., 2019), to encode each input sentence intocontextual features.
given a sequence of tokenswith a length of s, we denote the derived contex-tual features as h = [h1, ..., hs] ∈ rs×d, where dis the dimensionality of each hidden vector.
dependency graph constructor.
we use it toconstruct the (factual) dependency graph for eachinput sentence.
in this work, the stanza toolkit (qiet al., 2020) is used to extract the universal depen-dency relations as the ﬁrst step.
then, the depen-dency graph can be represented as g = {v, r, e},where the nodes v are tokens, the edges e de-note the existence of dependency relations, andthe set r contains the relation types for e. eachedge eij ∈ e consists of a triplet (vi, vj, r) wherev1, v2 ∈ v and r ∈ r..as shown in figure 3, we deﬁne three kinds ofrelation types in r : 1) a forward syntactic relation,obj−−−→ apples; 2) an inverse syntactic re-.
e.g., love.
lation, e.g., apples.
obj−1−−−→ love; and 3) a self loop.
579counterfactualdependencygraph.
randomizer.
factualdependencygraph.
model.language.pre-trained.factualpos tags.
randomizer.
counterfactualpos tags.
rgat.rgat.factualpos tags.
cat.
cat.
cat.
projection.fusion.projection.fusion.projection.fusion.loss.counterfactual.classifier.loss.counterfactual.counterfactualdependency graph&factual pos tags.
prediction.loss.task.factual dependency graph&counterfactual    pos tags.
dependencygraphconstructor.
"i love apples .".
pos tagsextractor.
figure 3: the overall pipeline of our cosy.
we call the architecture as syntax-aware networks (section 3.1) andthe training method as counterfactual training (section 3.2).
in this architecture, there are three branches: black, redand blue.
black branch is just the normal attention-based network with additional syntactic information, and onlyits prediction is used in the testing stage.
red branch and blue branch are novel as they generate the counterfactualsyntax samples and drive the counterfactual losses in the training stage—the key functions in cosy.
rgat standsfor relational graph attention network (ishiwatari et al., 2020; linmei et al., 2019).
the modules of rgatand the modules of fusion projection are shared across branches, e.g., two rgat modules are sharingparameters.
cat denotes concatenation....self that allows the information to ﬂow from anode to itself.
note that we regard the root re-lation as a self-loop.
in this way, we obtain 75different types of relations in total, and thus denotethe embedding matrix as r ∈ r75×d(cid:2)pos tags extractor.
we deploy the same stanzatoolkit (qi et al., 2020) to assign (factual) postags p for all tokens.
we obtain 17 different typesof pos tags and denote the embedding matrix ast ∈ r17×d(cid:2)relational graph attention networks (rgat).
rgat is one of the standard backbones to incorpo-rate the dependency graph (ishiwatari et al., 2020;linmei et al., 2019).
given the (factual) depen-dency graph g with the contextual features of eachnode, rgat can generate the relation-aware fea-tures (for each node).
details are given below.
sup-pose eij is the directed edge from node vi to nodevj and the dependency relation r. the importancescore of vj from vi is computed as:.
..s(vi, vj) = concat(es.
ij, er.
ij) · wattn,.
(1).
where wattn ∈ r(d/2+d(cid:2))×1 maps a vector to a.scalar, erij is the embedding of the dependency rela-tion between vi and vj from r, and esij is computedby element-wise multiplication between vi and vj:.
esij = (hi · wq) ◦ (hj · wk),(2)where wk ∈ rd×d/2 and wq ∈ rd×d/2 arethe learnable parameters for key and query projec-tions (vaswani et al., 2017), and hi and hj denotetheir contextual features extracted from pre-trainedlanguage models.
then, the importance scores arenormalized across nj to obtain the attention scoreof vj from vi:.
α(vi, vj) =.
(cid:2).
exp(s(vi, vj))k∈nj exp(s(vk, vj)).
,.
(3).
where nj denotes the set of nodes pointing tovj.
the relation-aware features of vj is com-puted as the weighted sum of all nodes in njwith corresponding attention scores.
after com-puting all nodes, we get the relation-aware featuresˆh = [ˆh1, ..., ˆhs] ∈ rs×d.
fusion projection.
we fuse the relation-awarefeatures ˆh with the (factual) pos tags informa-.
580tion before feeding them into the classiﬁer.
givenpos tags p , the fused features for each token arerepresented by.
fj = concat(ˆhj, pj) · wf ,.
(4).
where wf ∈ r(d+d(cid:2))×d are learnable parametersof fusion projection and pj is the correspondingembedding of the pos tag of the j-th token fromt. the fused features of the entire sequence aredenoted as f = [f1, ..., fs] ∈ rs×d.
classiﬁer.
it is designed based on the speciﬁc task,such as nli or qa, following devlin et al.
(2019)..3.2 counterfactual training.
recall that the challenge in the effective utiliza-tion of syntax is how to induce the model to focusmore on syntax while maintaining its original rep-resentation capability of semantics.
inspired bycounterfactual analysis (pearl et al., 2009; pearl,2010; pearl and mackenzie, 2018) and contrastivelearning (hadsell et al., 2006), we propose a coun-terfactual training method by incorporating coun-terfactual syntax (counterfactual dependency graphand counterfactual pos tags) on the red and bluebranches in figure 3. each branch is designed toguide the model to focus on one type of syntax, i.e.,dependency graph or pos tags.
counterfactual dependency graph is utilizedon the red branch with factual pos tags in fig-ure 3. we build a counterfactual dependency graphby maintaining graph structure and nodes, and re-placing each type of relation (except for a self-loopself) with a randomized (counterfactual) type.
we name it g−.
we feed g− and h into rgatto obtain the counterfactual relation-aware featuresdenoted as ˆh−.
then, we fuse ˆh− with the fac-tual pos tags to derive the counterfactual featuresfcf 1 = [f cf 1s ] on the red branch.
finally,we can calculate the similarity between the factualand the counterfactual features, by leveraging thedot-product operation, as follows,.
, ..., f cf 1.
1.lcf 1 =.
fi · f cf 1i.
..(5).
1s.s(cid:3).
i.this counterfactual loss forces the model to em-phasize the syntactic information related to depen-dency relations.
counterfactual pos tags are utilized with thefactual dependency graph on the blue branch infigure 3. we create counterfactual pos tags p −.
from factual pos tags p by randomly selecting apos tag for each token.
accordingly, we replaceeach embedding pi by p−i .
given the relation-aware features ˆh from the black branch, we thenfeed the embeddings of counterfactual pos tagsin eq.
4 and get the counterfactual features asfcf 2 = [f cf 2s ].
finally, we can calculatethe similarity between the factual and the counter-factual features (on the blue branch) by leveragingthe dot-product operation, as follows,.
, ..., f cf 2.
1.lcf 2 =.
fi · f cf 2i.
..(6).
1s.s(cid:3).
i.this counterfactual loss forces the model to em-phasize the syntactic information related to postags.
the overall loss function used in training isas follows,.
l = ltask + λ(lcf 1 + lcf 2),.
(7).
where ltask is the task-speciﬁc loss, i.e., a cross-entropy loss, and λ is a scale to balance between thetask-speciﬁc loss and our proposed counterfactuallosses..4 experiments.
in this section, we evaluate our cosy method forcross-lingual understanding under both zero-shotand few-shot settings.
for the zero-shot setting, weuse english for training and evaluate the model ondifferent target languages.
for the few-shot setting,we follow the implementation in (nooralahzadehet al., 2020) and use the development set of thetarget languages for model ﬁne-tuning3..4.1 datasets.
we evaluate our method on the natural languageinference (nli) and the question answering (qa)tasks.
we brieﬂy introduce the datasets used in ourexperiments as follows.
natural language inference (nli).
given twosentences, nli asks for the relationship betweenthe two sentences, which can be entailment, con-tradiction or neutral.
we conduct experimentson xnli (conneau et al., 2018) and evaluate ourmethod on 13 target languages4.
question answering (qa).
in this paper, we con-sider the qa task that asks the model to locate the.
3all the results and analyses are under the zero-shot set-.
tings by default, except for table 2..4we remove thai (th) and swahili (sw) from our experi-ments since these two languages are not supported by stanza..581method.
#t #m a.d..xnli.
mlqa.
xquad.
en..avg..en..avg..en..avg..1.t naive f.t.
rebm.1xmaml-one l o(l) (cid:3) 82.1 69.611.
82.1 68.4 67.0 / 80.2 44.2 / 61.4 72.2 / 83.5 51.0 / 66.7--82.2 70.1 67.2 / 80.4 45.2 / 62.1 72.6 / 83.6 53.2 / 68.1.lakmcosy (ours).
-66.8 / 80.0.
(cid:3)(cid:2).
11.
--.
--.
-.
-.
(cid:2).
esabrx.
-.
egral.-.
rx.naive f.t..(cid:2)xmaml-one l o(l) (cid:3)(cid:2)cosy (ours).
1.
1.
1.
1.
84.6 75.1.
-.
-.
--.
/ 80.1/ 80.2.
--.
/ 65.1 71.6 / 83.1 55.9 / 71.8/ 66.1.
-.
-.
84.3 75.6 67.7 / 80.7 48.5 / 66.5 74.0 / 85.1 57.3 / 73.4.naive f.t.
stilt.
19.
11.xmaml-one l o(l) (cid:3)(cid:2)cosy (ours).
1.
1.
(cid:2)88.7 80.0 70.6 / 83.5 53.2 / 71.6 75.7 / 86.5 60.6 / 76.8(cid:3) 89.6 81.6 70.8 / 84.1 54.4 / 72.8 77.4 / 88.3 63.3 / 78.7-.
/ 73.2.
/ 84.3.
-.
-.
-.
-.
-.
89.2 81.9 70.9 / 84.2 54.7 / 73.2 77.7 / 88.0 64.0 / 79.7.table 1: cross-lingual zero-shot performance comparison between cosy and sota methods on three benchmarkdatasets.
note that we report accuracy for xnli and exact match/f1 scores for mlqa and xquad.
for eachdataset, “en.” denotes the results of english while “avg.” is the average performance over all languages.
x-rmeans xlm-r and naive f.t.
is the abbr.
of naive fine-tuning.
l is the number of target languages.
#t denotesthe number of training turns, e.g., stilt augments its training by using each of nine additional datasets.
#m is thenumber of ﬁnal models, where 1 < o(l) < l, and a.d. denotes using additional datasets..answer from a passage given a question.
we con-duct experiments on mlqa (lewis et al., 2019)and xquad (artetxe et al., 2020).
cosy is eval-uated on 7 languages on mlqa and 10 languageson xquad (with thai excluded)..4.2 implementation.
in data preprocessing, we feed the same syntac-tic information to each of the subwords in thesame word after tokenization.
our implementa-tion of pre-trained language models (mbert andxlm-r) is based on huggingfaces’s transform-ers (wolf et al., 2020).
we select the checkpointand set hyper-parameters, e.g., learning rate andλ in the loss function, based on the performanceon the corresponding development sets.
we selectlearning rate amongst {7.5e−6, 1e−5, 3e−5} andﬁx the batch size to 32. we select dimension d(cid:3)amongst {100, 300}.
λ in counterfactual loss is setto 0.1 (see figure 4).
a linear warm up strategy forlearning rate is adopted with ﬁrst 10% optimizationsteps.
adam (kingma and ba, 2014) is adopted asthe optimizer.
all experiments are conducted on aworkstation with dual nvidia v100 32gb gpus..4.3 results.
we compare our method with naive ﬁne-tuning andthe state-of-the-art methods.
the overall results onthree benchmarks are presented in table 1 (zero-.
en..methodnaive f.t.∗81.9xmaml-one∗ 82.482.6cosy (ours).
non-en.
avg.
avg..70.370.771.9.
71.271.672.7.table 2: results of xnli under the few-shot set-ting (mbert).
we report the testing results of en-glish (“en.”), the average results over all non-englishlanguages (“non-en.
avg.”) and the average results∗ denotes the resultsover all languages (“avg.”).
from nooralahzadeh et al.
(2020).
more details areavailable in appendix..shot) and table 2 (few-shot)..comparison with naive fine-tuning.
naivefine-tuning (wu and dredze, 2019; liang et al.,2020; hu et al., 2020) is to directly ﬁne-tune thepre-trained language model on downstream tasksas in (devlin et al., 2019).
from table 1 andtable 2, we can observe that cosy consistentlyoutperforms the naive ﬁne-tuning method on alldatasets, e.g., by average 1.9 percentage points (ac-curacy) and 2.9 percentage points (f1) on xnliand xquad with xlm-rlarge in the zero-shot set-ting.
these observations demonstrate the effec-tiveness of cosy and suggest that universal syn-tax as language-agnostic features can enhance thetransferability for cross-lingual understanding.
fur-.
582thermore, the results show that cosy is able towork with different backbones and thus is model-agnostic.
comparison with the state of the art.
weﬁrst outline the sota zero-shot (few-shot) cross-lingual methods we compared with as follows: (1)xmaml-one (nooralahzadeh et al., 2020) bor-rows the idea from meta-learning.
speciﬁcally,xmaml-one utilizes an auxiliary language de-velopment data in training, e.g., using the devel-opment set of spanish in training to assist ger-man on mlqa.
xmaml-one reports the resultsbased on the most beneﬁcial auxiliary language.
(2)stilt (phang et al., 2020) augments intermediatetask training before ﬁne-tuning on the target task,e.g., adding training of hellaswag (zellers et al.,2019) before training on the nli task.
stilt alsoreports results with the most beneﬁcial intermedi-ate task.
(3) lakm (yuan et al., 2020) ﬁrst minesknowledge phrases along with passages from theweb.
then these web data are used to enhancethe phrase boundaries through a masked languagemodel objective.
note that lakm is only evalu-ated on three languages of mlqa..on the one hand, we observe that cosy sur-passes the compared sota methods over all eval-uation metrics.
although meta-learning meth-ods (finn et al., 2017; gu et al., 2018; sun et al.,2019) advance the state-of-the-art performance forfew-shot learning, our cosy still outperforms themeta-learning-based method, i.e., xmaml-one,with 1.1 percentage points in the few-shot setting.
on the other hand, the superiority of cosy is alsoreﬂected in other aspects, which are shown in ta-ble 1. speciﬁcally, cosy does not require ad-ditional datasets and cumbersome data selectionprocess, which is more convenient and resourcessaving..4.4 discussion and analysis.
ablation study.
in table 3, we show the mlqa,xquad and xnli results in 4 ablative settings,to evaluate the approach when we (1) only utilizethe san-black branch; (2) utilize the san-blackbranch with an intuitive gate mechanism to controlthe information of pre-trained language model andsyntax; (3) utilize the san-black branch and san-red branch; (4) utilize the san-black branch andsan-blue branch..compared to the ablative results, we can seethat our full method achieves the overall top per-.
ablative setting.
mlqa xquad xnli.
em f1 em f1 acc.
44.2 61.4 51.0 66.7 68.4naive f.t.
44.3 61.4 51.6 66.9 68.7(1) san-black(2) san-black+gate 44.5 61.5 51.9 67.1 68.7(3) san-black, red 44.9 61.7 52.8 67.8 69.9(4) san-black, blue 44.7 61.8 52.2 67.4 69.745.2 62.1 53.2 68.1 70.1(5) cosy.
table 3: the ablation study on mlqa, xquad andxnli (mbert).
we report the average performanceof all languages on the test set..(cid:12)(cid:5)(cid:7)(cid:4).
(cid:6)(cid:10)(cid:7)(cid:12).
(cid:6)(cid:3)(cid:7)(cid:8).
(cid:6)(cid:3)(cid:7)(cid:6).
(cid:6)(cid:3)(cid:7)(cid:9).
(cid:6)(cid:3)(cid:7)(cid:3).
(cid:6)(cid:3)(cid:7)(cid:5).
(cid:13).
(cid:12).
(cid:11)(cid:10)(cid:5)(cid:9)(cid:8)(cid:7)(cid:6)(cid:5)(cid:4)(cid:3)(cid:2)(cid:0).
(cid:6)(cid:3)(cid:7)(cid:2).
(cid:6)(cid:3)(cid:7)(cid:4).
(cid:6)(cid:10)(cid:7)(cid:9).
(cid:6)(cid:8)(cid:7)(cid:10).
(cid:6)(cid:10)(cid:7)(cid:3).
(cid:12)(cid:5)(cid:7)(cid:11).
(cid:6)(cid:10)(cid:7)(cid:12).
(cid:6)(cid:8)(cid:7)(cid:4).
(cid:6)(cid:12)(cid:7)(cid:2).
(cid:13).
(cid:12).
(cid:11)(cid:10)(cid:16)(cid:15)(cid:6)(cid:9)(cid:8)(cid:15)(cid:15)(cid:14).
(cid:6)(cid:3)(cid:7)(cid:6).
(cid:6)(cid:3)(cid:7)(cid:2).
(cid:6)(cid:4)(cid:7)(cid:10).
(cid:5).
(cid:4).
(cid:6)(cid:4)(cid:7)(cid:8).
(cid:0)(cid:2).
(cid:0)(cid:3).
(cid:0)(cid:4)(cid:0)(cid:2)(cid:3)(cid:0).
(cid:6)(cid:6)(cid:7)(cid:11).
(cid:0)(cid:2).
(cid:0)(cid:3).
(cid:0)(cid:4)(cid:0)(cid:2)(cid:3)(cid:0).
(cid:5).
(cid:4).
(cid:6)(cid:6)(cid:7)(cid:10).
figure 4: left: average f1-measure (%) on target lan-guages on mlqa development set (mbert).
right:average accuracy (%) on target languages on xnli de-velopment set (mbert).
red dotted line denotes themodel performance of using naive ﬁne-tuning..formance in all settings.
syntax features are incor-porated into the models in (1)-(5) and all of themoutperform the naive ﬁne-tuning method, whichdemonstrates the effectiveness of universal syntax.
by analyzing the settings one by one, we can ob-serve that san-black only attains limited improve-ment compared to naive ﬁne-tuning since syntaxis incorporated in the model by overlooked.
gatemechanism (2) fails to solve the overlooking issue.
both of (3) and (4) with counterfactual training areable to bring gains compared to (1), and the resultsindicate that dependency relations are more effec-tive compared to pos labels.
we also observe thatour full method (5) does not accumulate the gainsfrom (3) and (4).
one explanation could be thatpart of the information provided by the dependencyrelations and pos labels overlaps.
for instance, ifamod−−−→ wordb, wewe see an edge of relation, wordamay infer that worda is noun and wordb is adj.
effect of λ. we now study the impact of the scalevalue λ with counterfactual losses.
for clarity, weshow the results with different values of logλ infigure 4. we can observe that cosy attains the.
583mlqa.
xquad.
em f1.
em f1.
(1)(2)(3)(4)current.
44.845.144.945.045.2.
61.762.061.962.062.1.
52.253.152.753.253.2.
67.368.167.868.068.1.table 4: results of different generation ways for gener-ating counterfactual syntax with mbert as backbone.
“current” means the current generation way describedin section 3. we report the average performance of alllanguages..figure 5: f1-measure drop δ (%) with a standard nor-mal distribution perturbation on mlqa and xquad(mbert).
two colors denote cosy and san-black..highest results when λ = 0.1 on both mlqa andxnli.
as the value drops, the effect of counter-factual loss is also smaller and the performance isgetting closer to that from naive ﬁne-tuning (reddotted line).
if a large value of λ is applied, e.g.,λ = 1, the model begins to over-emphasize the syn-tax and semantics are overlooked, which leads tosigniﬁcant decrease on performance.
effect of cosy.
in this part, we ﬁrst study whethercounterfactual training method indeed guides themodel to focus more on syntactic information.
weconduct analysis on the cosy and san-black.
since it is non-trivial to measure the utilization ofsyntax in a straightforward way, we adopt a stan-dard way to measure the importance of the neuronsin deep models (k´ad´ar et al., 2017).
speciﬁcally,we perturb the syntactic features with a gaussiannoise to test data and check whether our modelwould be more easily affected by the syntax pertur-bation.
if so, then it veriﬁes that our model indeedrelies more on syntax.. the results are shown infigure 5. we can discover that the performancedrop of cosy is larger compared to that with san-black..meanwhile, we also explore whether cosy isbeneﬁcial for yielding more meaningful syntax em-bedding than san-black.
speciﬁcally, we com-pute the correlation score (absolute cosine similar-ity) between the embedding of syntactic relationand the corresponding inverse relation from the.
same type.
for cosy, we observe that the scoreof the related types are 42.4× larger than that oftwo randomly selected embeddings (average over10000 times).
however, for san-black, its scoreis only 1.4× larger than that of two randomly se-lected embeddings.
it demonstrates that cosy at-tains more meaningful syntax representations thansan-black..counterfactual syntax generation.
here we an-alyze other alternative ways of counterfactual syn-tax generation.
speciﬁcally, we design the follow-ing variants and report the results in table 4: (1)we not only replace edge types, but also replaceconnections for counterfactual dependency graphconstruction; (2) for each input sequence, we cre-ate 5 counterfactual dependency graphs, 5 sets ofcounterfactual pos tags, and the counterfactualloss is the average over the 5 sets; (3) we replacethe factual syntax with a ﬁxed type, e.g., a type ofpadding instead of a random type from all types; (4)in each generating process, we only replace 50%of the factual syntax..comparing (1) with the result of “san-black,blue” in table 3, we can see that (1) doesnot work.
we believe that randomly changing con-nections in g−, e.g., an edge is created from theﬁrst token to the last token in a long passage, mayhave a signiﬁcant effect to ˆh−, it is undesirable forfurther optimization of counterfactual loss.
resultsfrom (2) and (4) suggest that the number of thegenerated counterfactual syntax and ratio of ran-domizing do not play an important role in cosy.
itis also discovered that randomizing with all types isbetter than simple replacement with a ﬁxed type..5845 conclusion.
we study how to effectively plug in syntactic in-formation for cross-lingual understanding.
specif-ically, we propose a novel counterfactual-syntax-based approach to emphasize the importance ofsyntax in cross-lingual models.
we conduct ex-tensive experiments on three cross-lingual bench-marks, and show that our approach can outperformthe sota methods without additional dataset.
forfuture work, we will combine our approach withother orthogonal methods, e.g., meta-learning, tofurther improve its effectiveness..acknowledgments.
this research is supported by the national re-search foundation, singapore under its strategiccapabilities research centres funding initiative,and partially supported by the agency for sci-ence, technology and research (a*star) underits ame yirg grant (project no.
a20e6c0101),and its ame programmatic fund (project no:a18a1b0045 and no: a18a2b0046).
any opin-ions, ﬁndings and conclusions or recommendationsexpressed in this material are those of the author(s)and do not reﬂect the views of national researchfoundation, singapore..references.
ehsan abbasnejad, damien teney, amin parvaneh,javen shi, and anton van den hengel.
2020. coun-terfactual vision and language learning.
in proceed-ings of the ieee/cvf conference on computer vi-sion and pattern recognition..mikel artetxe, sebastian ruder, and dani yogatama.
2020. on the cross-lingual transferability of mono-lingual representations.
in proceedings of the 58thannual meeting of the association for computa-tional linguistics..long chen, xin yan, jun xiao, hanwang zhang, shil-iang pu, and yueting zhuang.
2020. counterfactualsamples synthesizing for robust visual question an-swering.
in proceedings of the ieee/cvf confer-ence on computer vision and pattern recognition..rochelle choenni and ekaterina shutova.
2020. whatdoes it mean to be language-agnostic?
probing mul-tilingual sentence encoders for typological proper-ties.
arxiv preprint arxiv:2009.12862..alexis conneau, kartikay khandelwal, naman goyal,vishrav chaudhary, guillaume wenzek, franciscoguzm´an, ´edouard grave, myle ott, luke zettle-moyer, and veselin stoyanov.
2020a.
unsupervisedcross-lingual representation learning at scale.
inproceedings of the 58th annual meeting of the as-sociation for computational linguistics..alexis conneau, ruty rinott, guillaume lample, ad-ina williams, samuel bowman, holger schwenk,and veselin stoyanov.
2018. xnli: evaluating cross-lingual sentence representations.
in proceedings ofthe 2018 conference on empirical methods in natu-ral language processing..alexis conneau, shijie wu, haoran li, luke zettle-moyer, and veselin stoyanov.
2020b.
emergingcross-lingual structure in pretrained language mod-els.
in proceedings of the 58th annual meeting ofthe association for computational linguistics..jacob devlin, ming-wei chang, kenton lee, andkristina toutanova.
2019. bert: pre-training of deepbidirectional transformers for language understand-ing.
in proceedings of the 2019 conference of thenorth american chapter of the association for com-putational linguistics: human language technolo-gies..philipp dufter and hinrich sch¨utze.
2020. identifyingnecessary elements for bert’s multilinguality.
arxivpreprint arxiv:2005.00396..chelsea finn, pieter abbeel, and sergey levine.
2017.model-agnostic meta-learning for fast adaptation ofdeep networks.
in international conference on ma-chine learning..yash goyal, amir feder, uri shalit, and been kim.
2019a.
explaining classiﬁers with causal concepteffect (cace).
arxiv preprint arxiv:1907.07165..yash goyal, ziyan wu, jan ernst, dhruv batra, deviparikh, and stefan lee.
2019b.
counterfactual vi-sual explanations.
in international conference onmachine learning..jiatao gu, yong wang, yun chen, victor ok li,and kyunghyun cho.
2018. meta-learning for low-resource neural machine translation.
in proceedingsof the 2018 conference on empirical methods innatural language processing..raia hadsell, sumit chopra, and yann lecun.
2006.dimensionality reduction by learning an invariantmapping.
in 2006 ieee computer society confer-ence on computer vision and pattern recognition.
ieee..seungtaek choi, haeju park, jinyoung yeo, and seung-won hwang.
2020. less is more: attention supervi-sion with counterfactuals for text classiﬁcation.
inproceedings of the 2020 conference on empiricalmethods in natural language processing..kaiming he, haoqi fan, yuxin wu, saining xie, andross girshick.
2020. momentum contrast for unsu-pervised visual representation learning.
in proceed-ings of the ieee/cvf conference on computer vi-sion and pattern recognition..585tsung-yuan hsu, chi-liang liu, and hung-yi lee.
2019. zero-shot reading comprehension by cross-lingual transfer learning with multi-lingual languagerepresentation model.
in proceedings of the 2019conference on empirical methods in natural lan-guage processing and the 9th international jointconference on natural language processing..yaobo liang, nan duan, yeyun gong, ning wu, fen-fei guo, weizhen qi, ming gong, linjun shou,daxin jiang, guihong cao, et al.
2020. xglue: anew benchmark datasetfor cross-lingual pre-training,understanding and generation.
in proceedings of the2020 conference on empirical methods in naturallanguage processing..junjie hu, sebastian ruder, aditya siddhant, grahamneubig, orhan firat, and melvin johnson.
2020.xtreme: a massively multilingual multi-task bench-mark for evaluating cross-lingual generalisation.
ininternational conference on machine learning..po-sen huang, huan zhang, ray jiang, robert stan-forth, johannes welbl, jack rae, vishal maini, daniyogatama, and pushmeet kohli.
2020. reducingsentiment bias in language models via counterfac-tual evaluation.
in proceedings of the 2020 con-ference on empirical methods in natural languageprocessing: findings..taichi ishiwatari, yuki yasuda, taro miyazaki, andjun goto.
2020. relation-aware graph attention net-works with relational position encodings for emo-tion recognition in conversations.
in proceedings ofthe 2020 conference on empirical methods in natu-ral language processing..´akos k´ad´ar, grzegorz chrupała, and afra alishahi.
2017. representation of linguistic form and func-tion in recurrent neural networks.
computationallinguistics..phillip keung, vikas bhardwaj, et al.
2019. adver-sarial learning with contextual embeddings for zero-resource cross-lingual classiﬁcation and ner.
in pro-ceedings of the 2019 conference on empirical meth-ods in natural language processing and the 9th in-ternational joint conference on natural languageprocessing..phillip keung, yichao lu, julian salazar, and vikasbhardwaj.
2020. on the evaluation of contex-tual embeddings for zero-shot cross-lingual transferlearning.
in proceedings of the 2020 conferenceon empirical methods in natural language process-ing..diederik p kingma and jimmy ba.
2014. adam: amethod for stochastic optimization.
arxiv preprintarxiv:1412.6980..guillaume lample and alexis conneau.
2019. cross-lingual language model pretraining.
arxiv preprintarxiv:1901.07291..anne lauscher, vinit ravishankar, ivan vuli´c, andgoran glavaˇs.
2020.from zero to hero: onthe limitations of zero-shot cross-lingual transferwith multilingualarxiv preprinttransformers.
arxiv:2005.00633..patrick lewis, barlas o˘guz, ruty rinott, sebastianriedel, and holger schwenk.
2019. mlqa: eval-uating cross-lingual extractive question answering.
arxiv preprint arxiv:1910.07475..hu linmei, tianchi yang, chuan shi, houye ji, andxiaoli li.
2019. heterogeneous graph attention net-works for semi-supervised short text classiﬁcation.
in proceedings of the 2019 conference on empiri-cal methods in natural language processing andthe 9th international joint conference on naturallanguage processing..yinhan liu, myle ott, naman goyal, jingfei du, man-dar joshi, danqi chen, omer levy, mike lewis,luke zettlemoyer, and veselin stoyanov.
2019.roberta: a robustly optimized bert pretraining ap-proach.
arxiv preprint arxiv:1907.11692..zihan liu, genta indra winata, andrea madotto, andpascale fung.
2020. exploring ﬁne-tuning tech-niques for pre-trained cross-lingual models via con-tinual learning.
arxiv preprint arxiv:2004.14218..marie-catherine de marneffe, timothy dozat, na-talia silveira, katri haverinen, filip ginter, joakimnivre, and christopher d manning.
2014. univer-sal stanford dependencies: a cross-linguistic typol-ogy.
in proceedings of the ninth international con-ference on language resources and evaluation..yulei niu, kaihua tang, hanwang zhang, zhiwu lu,xian-sheng hua, and ji-rong wen.
2020. coun-terfactual vqa: a cause-effect look at language bias.
arxiv preprint arxiv:2006.04315..joakim nivre, marie-catherine de marneffe, filip gin-ter, yoav goldberg, jan hajic, christopher d man-ning, ryan mcdonald, slav petrov, sampo pyysalo,natalia silveira, et al.
2016. universal dependenciesv1: a multilingual treebank collection.
in proceed-ings of the tenth international conference on lan-guage resources and evaluation..farhad nooralahzadeh, giannis bekoulis, johannesbjerva, and isabelle augenstein.
2020. zero-shotcross-lingual transfer with meta learning.
in pro-ceedings of the 2020 conference on empirical meth-ods in natural language processing..judea pearl.
2010. causal inference.
causality: objec-.
tives and assessment..judea pearl and dana mackenzie.
2018. the book of.
why: the new science of cause and effect..judea pearl et al.
2009. causal inference in statistics:.
an overview.
statistics surveys..jason phang, phu mon htut, yada pruksachatkun,haokun liu, clara vania, katharina kann, iacer.
586calixto, and samuel r bowman.
2020.en-glish intermediate-task training improves zero-shot cross-lingualarxiv preprinttransfer too.
arxiv:2005.13013..xiangpeng wei, yue hu, rongxiang weng, luxi xing,heng yu, and weihua luo.
2020. on learninguniversal representations across languages.
arxivpreprint arxiv:2007.15960..telmo pires, eva schlinger, and dan garrette.
2019.how multilingual is multilingual bert?
in proceed-ings of the 57th annual meeting of the associationfor computational linguistics..peng qi, yuhao zhang, yuhui zhang, jason bolton,stanza: aand christopher d manning.
2020.python natural language processing toolkit for manyhuman languages.
in proceedings of the 58th an-nual meeting of the association for computationallinguistics: system demonstrations..lianhui qin, antoine bosselut, ari holtzman, chandrabhagavatula, elizabeth clark, and yejin choi.
2019.counterfactual story reasoning and generation.
inproceedings of the 2019 conference on empiricalmethods in natural language processing..neal j roese.
1997. counterfactual thinking.
psycho-.
logical bulletin..kenneth j rothman and sander greenland.
2005. cau-sation and causal inference in epidemiology.
ameri-can journal of public health..seungjae shin, kyungwoo song, joonho jang, hyemikim, weonyoung joo, and il-chul moon.
2020.neutralizing gender bias in word embedding with la-tent disentanglement and counterfactual generation.
in proceedings of the 2020 conference on empiricalmethods in natural language processing: findings..daniel steel.
2004. social mechanisms and causal in-.
ference.
philosophy of the social sciences..qianru sun, yaoyao liu, tat-seng chua, and berntschiele.
2019. meta-transfer learning for few-shotlearning.
in proceedings of the ieee/cvf confer-ence on computer vision and pattern recognition..kaihua tang, jianqiang huang, and hanwang zhang.
2020a.
long-tailed classiﬁcation by keeping thegood and removing the bad momentum causal effect.
arxiv preprint arxiv:2009.12991..kaihua tang, yulei niu, jianqiang huang, jiaxin shi,and hanwang zhang.
2020b.
unbiased scene graphgeneration from biased training.
in proceedings ofthe ieee/cvf conference on computer vision andpattern recognition..ashish vaswani, noam shazeer, niki parmar, jakobuszkoreit, llion jones, aidan n gomez, łukaszkaiser, and illia polosukhin.
2017. attention is allin advances in neural information pro-you need.
cessing systems..jesse vig, sebastian gehrmann, yonatan belinkov,sharon qian, daniel nevo, yaron singer, and stuartshieber.
2020. investigating gender bias in languagemodels using causal mediation analysis.
advancesin neural information processing systems..thomas wolf, julien chaumond, lysandre debut, vic-tor sanh, clement delangue, anthony moi, pier-ric cistac, morgan funtowicz, joe davison, samshleifer, et al.
2020. transformers: state-of-the-artnatural language processing.
in proceedings of the2020 conference on empirical methods in naturallanguage processing: system demonstrations..shijie wu and mark dredze.
2019. beto, bentz, becas:the surprising cross-lingual effectiveness of bert.
inproceedings of the 2019 conference on empiricalmethods in natural language processing and the9th international joint conference on natural lan-guage processing..ming yan, hao zhang, di jin, and joey tianyi zhou.
2020. multi-source meta transfer for low resourcemultiple-choice question answering.
in proceedingsof the 58th annual meeting of the association forcomputational linguistics..sicheng yu, yulei niu, shuohang wang, jing jiang,and qianru sun.
2020. counterfactual variable con-trol for robust and interpretable question answering.
arxiv preprint arxiv:2010.05581..fei yuan, linjun shou, xuanyu bai, ming gong,yaobo liang, nan duan, yan fu, and daxin jiang.
2020. enhancing answer boundary detection formultilingual machine reading comprehension.
inproceedings of the 58th annual meeting of the as-sociation for computational linguistics..rowan zellers, ari holtzman, yonatan bisk, alifarhadi, and yejin choi.
2019. hellaswag: can amachine really ﬁnish your sentence?
arxiv preprintarxiv:1905.07830..wei zhao, steffen eger, johannes bjerva, and is-inducing language-arxiv.
abelle augenstein.
2020.agnostic multilingualpreprint arxiv:2008.09112..representations..joey tianyi zhou, hao zhang, di jin, and xi peng.
2021. dual adversarial transfer for sequence label-ing.
ieee transactions on pattern analysis and ma-chine intelligence..joey tianyi zhou, hao zhang, di jin, hongyuan zhu,meng fang, rick siow mong goh, and kennethkwok.
2019. dual adversarial neural transfer forlow-resource named entity recognition.
in proceed-ings of the 57th annual meeting of the associationfor computational linguistics..qingfu zhu, weinan zhang, ting liu,.
andwilliam yang wang.
2020. counterfactual off-policy training for neural dialogue generation.
inproceedings of the 2020 conference on empiricalmethods in natural language processing..appendix.
587mbertnaive fine-tuning1xmaml-one2cosy.
xlm-rbasenaive fine-tuning3cosy.
xlm-rlargenaive fine-tuning4stilt5cosy.
mbertnaive fine-tuning1lakm3cosy.
xlm-rbasenaive fine-tuning2naive fine-tuning∗xmaml-one4cosy.
xlm-rlargenaive fine-tuning1stilt5xmaml-one4cosy.
methods.
mbert.
1naive fine-tuningcosy.
xlm-rbase.
∗.
xlm-rlarge.
1naive fine-tuning2stiltcosy.
methods.
en.
fr.
es.
de.
el.
bg.
ru.
tr.
ar.
vi.
zh.
hi.
ur.
avg.
82.182.182.2.
73.874.475.2.
74.375.175.5.
71.171.872.2.
66.468.068.9.
68.969.571.1.
69.070.270.1.
61.661.263.1.
64.966.166.7.
69.571.872.4.
69.371.171.3.
60.062.262.4.
58.061.559.7.
68.469.670.1.
84.684.3.
78.278.8.
79.278.6.
77.076.4.
75.976.3.
77.578.4.
75.576.3.
72.973.9.
72.171.1.
74.875.4.
73.775.1.
69.871.1.
65.167.1.
75.175.6.
88.789.689.2.
82.284.183.6.
83.784.585.1.
82.583.783.2.
80.881.883.3.
83.083.584.7.
79.179.980.9.
78.080.180.8.
77.279.380.1.
79.381.381.0.
78.280.780.5.
75.678.277.7.
71.774.574.1.
80.081.681.9.table 5: results on xnli of zero-shot setting.
we report the accuracy on 13 xnli languages and the averageaccuracy.
1: (wu and dredze, 2019); 2: (nooralahzadeh et al., 2020); 3: (liang et al., 2020); 4: (hu et al., 2020);5: (phang et al., 2020)..methods.
en.
es.
de.
ar.
hi.
vi.
zh.
avg.
67.0 / 80.266.8 / 80.067.2 / 80.4.
49.2 / 67.448.0 / 65.948.5 / 66.4.
43.8 / 59.044.5 / 60.547.0 / 61.1.
34.6 / 52.3-35.0 / 52.9.
35.3 / 50.2-35.9 / 51.2.
40.7 / 61.2-43.2 / 63.1.
38.6 / 59.6-39.3 / 59.8.
44.2 / 61.4-45.2 / 62.1.
-.
/ 80.167.1 / 80.1/ 80.267.7 / 80.7.
-.
-.
/ 67.950.3 / 68.0/ 67.550.9 / 68.7.
-.
-.
/ 62.148.3 / 62.9/ 63.649.1 / 63.4.
-.
-.
/ 56.437.2 / 57.0/ 58.038.7 / 57.8.
-.
-.
/ 60.544.5 / 62.4/ 61.745.4 / 62.7.
-.
-.
/ 67.147.1 / 67.4/ 68.047.9 / 68.3.
-.
-.
/ 61.438.4 / 62.0/ 64.039.7 / 63.6.
-.
-.
/ 65.147.6 / 65.7/ 66.148.5 / 66.5.
-.
70.6 / 83.570.8 / 84.1/ 84.370.9 / 84.2.
-.
56.6 / 74.156.8 / 75.3/ 74.356.5 / 74.7.
-.
54.9 / 70.152.9 / 69.6/ 70.855.2 / 70.3.
-.
47.1 / 66.646.4 / 67.4/ 66.646.7 / 66.7.
-.
53.1 / 70.654.8 / 72.5/ 70.953.7 / 72.1.
-.
52.9/ 74.051.7 / 70.9/ 74.853.2 / 74.3.
-.
37.0 / 62.147.0 / 69.4/ 70.746.6 / 70.2.
-.
53.2 / 71.654.4 / 72.8/ 73.254.7 / 73.2.
-.
table 6: results on mlqa of zero-shot setting.
we report the exact match and f1 score (em / f1) on 7 lan-guages.
∗: our implementation by ofﬁcial code; 1: (hu et al., 2020); 2: (liang et al., 2020); 3: (yuan et al., 2020);4: (nooralahzadeh et al., 2020); 5: (phang et al., 2020)..en.
ar.
de.
el.
es.
hi.
ru.
tr.
vi.
zh.
avg.
72.2 / 83.5 45.1 / 61.5 54.0 / 70.6 44.9 / 62.6 56.9 / 75.5 46.0 / 59.2 53.3 / 71.3 40.1 / 55.4 49.6 / 69.5 48.3 / 58.0 51.0 / 66.772.6 / 83.6 47.6 / 63.6 57.2 / 72.3 47.7 / 64.6 58.6 / 76.5 47.5 / 60.7 55.6 / 72.1 42.2 / 56.7 54.0 / 72.4 48.9 / 58.5 53.2 / 68.1.naive fine-tuningcosy.
71.6 / 83.1 49.9 / 66.2 56.6 / 72.5 54.2 / 72.4 58.8 / 76.6 51.3 / 67.7 57.2 / 74.1 52.5 / 68.3 53.8 / 73.6 52.6 / 63.6 55.9 / 71.874.0 / 85.1 51.0 / 67.8 59.2 / 75.4 55.5 / 73.2 59.0 / 77.2 51.5 / 69.1 58.5 / 75.0 52.5 / 69.5 56.0 / 74.2 56.2 / 67.3 57.3 / 73.4.
75.7 / 86.5 49.0 / 68.6 63.4 / 80.4 61.7 / 79.8 63.9 / 82.0 59.7 / 76.7 64.3 / 80.1 59.3 / 75.9 59.0 / 79.1 50.0 / 59.3 60.6 / 76.877.4 / 88.3 59.9 / 75.9 63.6 / 80.3 62.1 / 80.3 63.2 / 81.8 59.2 / 76.1 64.1 / 80.0 59.2 / 75.8 61.2 / 80.5 61.3 / 70.8 63.3 / 78.777.7 / 88.0 58.7 / 76.5 65.1 / 81.4 64.4 / 81.7 64.0 / 82.5 60.6 / 77.1 64.7 / 80.9 60.7 / 76.3 61.5 / 80.7 63.0 / 72.1 64.0 / 79.7.table 7: results on xquad of zero-shot setting.
we report the exact match and f1 score (em / f1) on 10languages.
∗: our implementation by ofﬁcial code; 1: (hu et al., 2020); 2: (phang et al., 2020)..588methods.
en.
fr.
es.
de.
el.
bg.
ru.
tr.
ar.
vi.
zh.
hi.
ur.
avg.
naive fine-tuning 81.9 75.4 75.8 73.3 69.5 71.6 70.8 64.9 67.4 73.2 73.9 64.4 63.7 71.282.4 75.3 76.2 73.5 70.0 71.9 71.5 64.9 68.0 73.5 74.2 65.0 63.8 71.6xmaml-one82.7 76.0 76.5 74.1 70.7 72.8 72.1 65.7 68.4 73.9 74.9 65.8 64.6 72.1xmaml-two82.7 77.2 76.5 74.3 71.1 73.9 72.4 67.6 69.8 74.3 74.7 66.4 63.7 72.7cosy.
table 8: results on xnli of few-shot setting with mbert.
we report the accuracy on 13 xnli languages and theaverage accuracy.
results except our cosy are all from (nooralahzadeh et al., 2020)..589