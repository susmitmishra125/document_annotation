evaluation examples are not equally informative:how should that change nlp leaderboards?.
pedro rodriguezuniversity of marylandfacebook reality labs∗me@pedro.ai.
joe barrowuniversity of marylandjdbarrow@cs.umd.edu.
alexander hoyleuniversity of marylandhoyle@umd.edu.
john p. laloruniversity of notre damejohn.lalor@nd.edu.
robin jiauniversity of southern californiarobinjia@usc.edu.
jordan boyd-graberuniversity of marylandjbg@umiacs.umd.edu.
abstract.
leaderboards are widely used in nlp and pushthe field forward.
while leaderboards are astraightforward ranking of nlp models, thissimplicity can mask nuances in evaluationitems (examples) and subjects (nlp models).
rather than replace leaderboards, we advocatea re-imagining so that they better highlight ifand where progress is made.
building on educa-tional testing, we create a bayesian leaderboardmodel where latent subject skill and latent itemdifficulty predict correct responses.
using thismodel, we analyze the ranking reliability ofleaderboards.
afterwards, we show the modelcan guide what to annotate, identify annotationerrors, detect overfitting, and identify informa-tive examples.
we conclude with recommenda-tions for future benchmark tasks..1 leaderboards are shiny.
leaderboard evaluations—for better or worse—arethe de facto standard for measuring progress inquestion answering (rajpurkar et al., 2016) andin many nlp tasks (wang et al., 2019a).
an un-fortunate side effect of leaderboard popularity issota-chasing, often at the expense of carefullyinspecting data and models (linzen, 2020).
forexample, the same “super-human” models that topquestion answering leaderboards (najberg, 2018)often fail spectacularly (feng et al., 2018; wallaceet al., 2019a) by learning non-generalizable statisti-cal patterns (mccoy et al., 2019; niven and kao,2019).
finally, focusing solely on metrics conflatesprogress on a specific task with progress on real-world nlp problems behind the task (bender andkoller, 2020).
plainly, focusing on headline sotanumbers “provide(s) limited value for scientificprogress absent insight into what drives them” andwhere they fail (lipton and steinhardt, 2019)..figure 1: difficulty and ability discriminating (dad)leaderboards infer the difficulty, discriminativeness, andfeasibility of examples.
negative discriminability sug-gests an annotation error; for example, the question withmost negative discriminability asks “why did demandfor rentals decrease?” when the answer is “demand forhigher quality housing increased.”.
in this work we take leaderboards “as they are,”and imagine how they might better support re-search.
leaderboards establish differences betweenmodels on a fixed task.
hence, leaderboards shouldenable and encourage the comparison of modelsand inspection of examples.
and leaderboardsshould also signal when they have outlived theirusefulness (boyd-graber and börschinger, 2020)..1.1 how to direct leaderboards’ light.
to help focus attention on examples and models ofinterest, we propose difficulty and ability discrim-inating (dad) leaderboards that explicitly modelboth task and submissions jointly, rather than eitherin isolation.1 dad’s underlying model is based on.
∗work completed at university of maryland..1source code, data, and visualizations at irt.pedro.ai..proceedingsofthe59thannualmeetingoftheassociationforcomputationallinguisticsandthe11thinternationaljointconferenceonnaturallanguageprocessing,pages4486–4503august1–6,2021.©2021associationforcomputationallinguistics4486−7.5−5.5−3.5−1.50.52.54.56.58.502,000−8−6−4−20246810diﬃculty ((cid:1))−10−50510discriminability ((cid:2))02,000−10.0−8.0−6.0−4.0−2.00.02.04.06.08.010.012.00.01.0feasibility (λ)annotation errordiscriminativeand harddiscriminativeeasygenerally, given n test items x = (x1, .
.
.
, xn)and m subjects s = (s1, .
.
.
, sm), where eachsubject answers every item, we want to estimatesubject skills and item difficulties.
to discover therandom variables that best explain the data, we turnto probabilistic inference (pearl, 1988)..two additional random variables further improvedad: discriminability γi and feasibility λi.
we firstconsider discriminability and the margin betweena question’s difficulty βi and a subject’s skill θj.
adiscriminative question is challenging but can stillbe answered correctly by a strong subject.
if ken’sability is higher than most items’ difficulty (θj − βiis large), item discriminability multiplies this gapby γi in a model called irt-disc.
questions withlow γi are low quality: they have annotation erroror do not make sense..another way of capturing poor quality questionsis the feasibility λi.
for example, if the question“who was the first president” has the answer rajen-dra prasad, the question has an unstated implicitassumption that subjects must guess what countryor company the question is about.
in the modelirt-feas, if a large fraction of subjects all get anitem wrong, everyone’s probability of getting theitem right is capped at λi.
in nlp terms, 1 − λicorresponds to the prevalence of annotation errorsthat lead to unsolvable items..having introduced all of the constituent elementsof the model, we can now present the full genera-tive model:.
1. for each subject j:.
(a) draw skill θj ∼ n (µθ, τ −1.
).
θ.
2. for each item i:.
(a) draw difficulty βi ∼ n (µβ, τ −1β )(b) draw discriminability γi ∼ n (µγ, τ −1γ )(c) draw feasibility λi ∼ u[0, 1]3. draw subject i response on item j,.
rij ∼ pij(rij | θj, βi, λi) =.
pij(rij = 1|θj) =.
λi1 + e−γi(θj −βi).
..(1).
for irt-base, γi and λi are fixed to 1.0, whilefor irt-disc, only λi is fixed.3.
means µθ, µβ, µγ are drawn from n (0, 106) andτθ, τβ, τγ from a γ(1, 1) prior, as in lalor et al.
(2019) and recommended by natesan et al.
(2016).4.
3in psychometrics, irt-base is called a rasch (rasch,1960) or 1 parameter logistic (1pl) model, irt-disc is a 2plmodel, and irt-feas is a 4pl model with guessing set to zero.
4we differ by allowing γ < 0 to identify bad items..figure 2: a dad leaderboard uses irt to jointly inferitem difficulty βi, discriminability γi, feasibility λi, andsubject skill θj.
these predict the likelihood pij(rij =1) of a correct response rij..item response theory (lord et al., 1968; baker,2001, irt, reviewed in §2), a widely used (van rijnet al., 2016) alternative in educational testing tosimple summary statistics (edgeworth, 1888)..dad can explicitly identify the difficulty anddiscriminability of items (figure 1),2 which inturn can lead to a more nuanced ranking of mod-els, identifying poor items, and better understand-ing of a dataset and task.
throughout the paper,we use the question answering (qa) benchmarksquad 2.0 (rajpurkar et al., 2018).
for example,dad can identify questions that are challenging tomodels and questions that are wrong (incorrectlyannotated).
in addition to better understandingdatasets, it is also helpful for efficiently selectingevaluation items to annotate.
we conclude withrecommendations for future leaderboards (§7) anddiscuss where irt in nlp can go next (§8)..2 a generative story for leaderboards.
leaderboards are a product of the metrics, evalu-ation data, and subjects (machine or human) whoanswer items (figure 2).
for concreteness, let’sassume that we have a question-answering task andtwo subjects: ken, who is good at trivia, and burt,who is not.
in the simplest irt models, each sub-ject j has a random variable θj corresponding totheir skill: ken’s is big, burt’s is small..but you cannot know that until you start ask-ing them questions of varying difficulty βi.
harderquestions have a higher difficulty (“what is theairspeed of an unladen swallow”) than easy ones(“who is buried in grant’s tomb”).
the bigger themargin between a subject’s skill θj and an item’sdifficulty βi, θj − βi, the more likely that subject jresponds correctly pi,j(ri,j = 1).
this is the sim-plest irt model, which we call irt-base..2example and feasibility distribution in appendix a. in-.
teractive visualization linked from http://irt.pedro.ai..4487subjectsitemsresponses2.1 examples are not equally useful.
for all z using adam (kingma and ba, 2015)..because it is difficult to completely codify skilland difficulty into a single number, we can rewritethe exponent in equation 1 as a sum over dimen-sions −γi(∑︁k θj,k − βi,k), where each dimensioncaptures the interaction between an item’s diffi-culty and a subject’s skill.
for example, perhapsburt could better exploit artifacts in one dimension(their skill for θj,k=5 is high but everywhere elseis low) while ken might not know much about aparticular topic like potent potables (θj,k=2 is lowbut everywhere else is high).
we call this modelirt-vec.5 multidimensional irt models (reck-ase, 2009) could—in addition to better modelingdifficulty—also cluster items for interpretation; webriefly experiment with this (appendix f), butleave more to future work (§8)..irt’s fundamental assumption is that not all itemsand subjects are equal.
this explains why leader-boards can fail while having “normal looking” ac-curacies.
as a thought experiment, consider adataset that is one third easy (βi ∈ [0, 1]), one thirdmedium difficulty (βi ∈ [2, 3]), and one third hard(βi ∈ [6, 7]).
suppose that ken has skill θk = 4while burt has skill θb = 2. a standard leader-board would say that ken has higher accuracy thanburt.
but suppose there’s a new subject that wantsto challenge ken; they are not going to reliablydethrone ken until their skill θc is greater than six.
this is a more mathematical formulation of the“easy” and “hard” dataset splits in question answer-ing (sugawara et al., 2018; rondeau and hazen,2018; sen and saffari, 2020).
in irt-feas, thisrecapitulates the observation of boyd-graber andbörschinger (2020) that annotation error can hin-der effective leaderboards.
dad helps systematizethese observations and diagnose dataset issues..2.2.inference.
to estimate the latent parameters of our model,we use mean-field variational inference (jordanet al., 1999).
in variational inference, we pro-pose a distribution over the latent variables, qϕ(·),that approximates the true but intractable poste-rior p(·).
we then minimize the kl-divergencebetween these distributions, equivalent to maximiz-ing the evidence lower-bound (elbo) with respectto the variational parameters..5we do not incorporate feasibility into the irt-vec model.
since it already improves over 1d models without it..in our case, qϕ(·) is a mean-field distribution,which means it factorizes over each of the latentvariables (the product is over the n × m subject-item pairs).
qϕ(θ, β, γ, µ, τ ) = q(µ)q(τ ).
q(θj)q(βi)q(γi).
∏︂.
i,j.
specifically, for our key latent variables z ∈{θ, β, γ}, the associated variational distributionsare of the form q(z) = n (uz, t−1z ).
recall that inthe generative distribution, each latent z is drawnfrom a n (µz, τ −1) whose parameters are also la-tent variables; for these variables, we use the vari-ational distributions q(µz) = n (uµz , t−1µz ) andq(τz) = γ(aτz , bτz ).
we optimize the elbo withrespect to the variational parameters.
z.ϕ = {uz, tz, uµz , tµz , aτz , bτz , λ}.
with dad’s leaderboard irt model introduced,we next discuss how leaderboard subjects are sta-tistically compared and alternative methods—suchas using irt parameters—to evaluate whether twomodels are truly different..3 ranking and comparing subjects.
fundamentally, the objective of comparative eval-uations like leaderboards is to decide whethermodel a is better than model b. a thread of nlphas rightfully advocated for adding rigor to thesedecisions using statistics (traub, 1997, classicaltesting theory) where the objective is to infer atrue score t from the observed test score x =t +e given a measurement error e, uniform acrosssubjects.
however, in educational testing—a fieldmeasuring skill and knowledge in humans—irtis a primary measurement instrument (hamble-ton, 1991, p. 2).
a major motivation for irt isthat subjects of different skill have different errors.
irt explicitly accounts for the bandwidth-fidelitydilemma (mcbride, 1976):items can either ac-curately measure a narrow ability range (fidelity)or inaccurately measure large ability ranges (band-width).6 this section and the next contrast methodsfor identifying the best model and advocate for irt.
implicit in nearly all leaderboard evaluations isranking models by a statistic such as the averageaccuracy.
as we show in §4, naïve rankings arenoisier than irt rankings..6estimation error of θ varies by position (appendix e)..44884.irt for leaderboards.
leaderboards should: (1) reliably and efficientlyrank better models ahead of worse models (tague-sutcliffe, 1992; voorhees, 2003) and (2) guideinspection of items and subjects (§5).
the firstameliorates the unavoidable randomness of finiteevaluations while the second enables error analy-sis (wu et al., 2019) and model probing (belinkovand glass, 2019; zhang et al., 2019).
first we ver-ify that irt models accurately predict the responsesof subjects (§4.2).
next, a ranking stability analysisshows that irt has modestly better reliability thanclassical rankings (§4.2.3).
lastly, using irt to ac-tively sample items for annotation yields rankingswith better correlation to complete test data (§4.4)..4.1 why a linear model baseline.
at first blush, the differences between irt and logis-tic regression are minimal, but we include the com-parison to address natural questions from the nlpcommunity: (1) do the idiosyncrasies of the irt for-mulation hurt accuracy?
(2) should we add featuresto better understand phenomena in the questions?
(3) why not use deep models?.
the next section argues that both irt and logisticregression are accurate even without laboriouslyengineered task-specific features.
adding obviousfeatures such as item words (e.g., questions) onlyminimally improves the accuracy.
we explicitlyomit less interpretable deep models since our goalis to make leaderboards more interpretable..4.2 response prediction is accurate.
just as educational testing researchers validate irtmodels by seeing if they predict subject responsescorrectly (american educational research associ-ation, 2014), we validate how well dad predictswhether squad models get questions right..we compare against a logistic regression lin-ear model (lm) implemented with vowpal wab-bit (agarwal et al., 2014).
since integrating hand-crafted features is easy, we incorporate featuresderived from subject ids; item ids; functions ofthe squad question, answer, and title; and irt pa-rameters (details in appendix b).
as in irt, logis-tic regression predicts whether a subject correctlyresponds to an item.
later, we discuss ways tointegrate more features into irt (§8)..4.2.1 squad leaderboard dataexperiments are on the squad 2.0 leaderboard.
development data are publicly available, and orga-.
nizers provide test set responses.
there are 161 de-velopment subjects, 115 test subjects, and 11,873items (1.9 million total pairs).
experiments that donot need test responses use all development sub-jects; those that do use the smaller test subset..4.2.2 evaluation scheme.
following prior work (wu et al., 2020), we evalu-ate irt and linear models by holding out 10% ofresponses and computing classification metrics.7in squad, predicting whether a response is correctis an imbalanced classification problem (80.4% ofresponses in the development set are correct).
thus,we use roc auc, macro f1, and accuracy..4.2.3.irt response prediction is accurate.
irt models that incorporate more priors into thegenerative story should be better, but are they?
wecompare four irt models: irt-base, irt-disc, irt-feas, and irt-vec (§2).
the more sophisticatedmodels are better and all improve over the lm(figure 3) and correlate well with each other (ap-pendix c).
to be clear, while higher accuracy thanlm is good, our goal is to validate that irt modelsare accurate; later, we inspect model errors andidentify annotation errors (§5)..4.2.4 what model features are predictive?.
integrating additional features into bayesian mod-els is not trivial, so we instead use the flexibility oflinear models to identify useful features.
our leave-one-in ablation compares features (figure 3): thetop ablations both use irt features, further validat-ing irt parameters.
the subject and item identifierfeatures are also strongly predictive, but item isthe stronger of the two.
text-based features areweaker, but this suggests future work to better inte-grate them into irt models (§8)..4.3 ranking with irt.
leaderboards should produce reliable subject rank-ings: can dad rank systems even with a tiny testset?
thus, we compare the correlation both of tra-ditional average accuracy (§3) and irt rankingson the whole test set compared to the rankings ofthe same metric on a smaller test set.
our firstexperiment (§4.3.1) examines the stability of ex-isting items and subjects while the second (§4.4)investigates stability of “new” evaluation data usingsampling strategies..7everywhere else in the paper, we train on all responses..4489figure 3: we compare each irt and linear model (lm) by how well they predict subject responses.
we focus onroc auc since predicting responses is an imbalanced classification problem (most subjects are correct).
under thatmetric, all irt models improve over the best lm, and the strongest lm ablation only uses irt features.
that textualfeatures are predictive in the lm suggests they could improve future models..4.3.1.irt rankings have better reliability.
4.4.irt improves cold start reliability.
rankings should be reliable within the same dataset(e.g., on dev set) and generalize to similar datasets(e.g., with a test dataset).
to test the first, wemeasure the ranking stability of mutually exclu-sive samples of the development data (buckley andvoorhees, 2000).
to test the second, we measurethe correlation between development set samplerankings to test set rankings (voorhees, 1998)..specifically, for a range of sample sizes8 we(1) sample two partitions of the data, (2) computethe classical ranking9 and the irt ranking from arefit irt-feas model, then (3) compute kendall’scorrelation (kendall, 1938) between the samplesfor each ranking (details in appendix d).
in bothcases irt rankings have higher correlation thanclassical rankings (figure 4, left).
since the benefitis strongest at low sample sizes, irt can improvethe reliability of small-scale evaluations..the second experiment examines ranking gener-alization: irt yields more reliable measures of sub-ject skill, implying a greater consistency in subjectrankings across evaluation settings.
figure 4 com-pares the development set sample rankings com-puted above to rankings obtained using subjects’test set responses (with the same irt model)..across all sample sizes, subjects’ irt abilityestimated on the development set correlates welltest set ability.
crucially, this is better than thecorresponding classical metrics like accuracy (ap-pendix d quantifies the statistical significance ofthe difference), supporting our original motivationfor using irt.10.
8the sample size must be less than half the size of the.
development data so that we can obtain two samples.
9for squad, ordering by mean exact match score.
10since the maximum trial size was limited, we train onefinal model with the full data, see table 3 in the appendix d..irt can also guide the construction of tests.
just asirt practitioners prepare tests for humans, we tooconstruct tests for machines.
in educational testing,collecting responses from humans is expensive;likewise, although questions are cheap in search-based qa tasks (nguyen et al., 2016; kwiatkowskiet al., 2019), annotating answers is expensive.
like-wise, “grading” machine dialog responses is expen-sive and irt helps (sedoc and ungar, 2020).
toemulate this setting, we use computerized adaptivetesting (weiss and kingsbury, 1984) to iterativelyselect squad items to “annotate.”.
as in human test preparation, we use existingannotations to infer item parameters and iterativelyinfer the ability of new subjects.
this experimentsplits m subjects into a training group (80%) and atesting group (20%).
the training group representssubjects for which we have full item predictionsand annotations; the testing group represents a newgroup of subjects that we need to rank.
to effi-ciently rank, we should iteratively choose items toannotate that yield the most information about theranking if all the data were annotated..this experiment compares how well severalitem selection strategies work.
for each selectionmethod, we (1) choose a sample size, (2), samplefrom the development set, (3) compute the rank-ing of subjects, and (4) compute kendall’s rankcorrelation (figure 5).11.which item selection strategies should we com-pare?
as a baseline, we use naïve random sampling.
like prior work, we compare selecting items withthe highest difficulty and the highest discriminabil-ity (lalor et al., 2019) as well as the sum of the.
11we compute correlations with the complete development.
set on ten trials to build 95% confidence intervals..44900.00.10.20.30.40.50.60.70.80.91.0roc auc0.00.10.20.30.40.50.60.70.80.91.0macro f10.00.10.20.30.40.50.60.70.80.91.0accuracyirt-veclm +questionirt-feaslm +contextirt-disclm +statsirt-baselm +subj & item idlm alllm +topics 1klm +irtlm +titlelm +item idlm +baselinelm +subject idfeaturesfigure 4: compared to the final ranking over a large test set, how well does a small test set correlate?
theleft shows correlation between mutually exclusive development set samples and the right between developmentsamples and the full test set.
in both experiments (panes), ranking systems by irt ability is more stable—across allsample sizes—than mean accuracy and thus more reliable (kendall’s rank correlation is higher).
bands show 95%confidence intervals of rank correlations across ten trials per sample size..sponse pij is the same as the likelihood of an in-correct response 1 − pij, which corresponds to themaximal value of ii(θj); it is also sensible thisvalue increases as discriminability γi increases..to infer the maximally informative items, weestimate the ability θj of each subject using thecurrently selected items, use the ability to computethe information of each yet-to-be-annotated itemfor each subject, and then aggregate the informa-tiveness.
info(i) =.
ii(θj).
(3).
∑︂.
j.by item i summed over subjects j. this approachis similar to uncertainty sampling and reduces toit for the irt-base model (lewis and gale, 1994).
we initially seed with the twenty-five most discrim-inative items (details in appendix d)..like computerized adaptive testing (morenoet al., 1984), figure 5 shows that at lower samplesizes three of the irt sampling methods are bet-ter than random sampling—difficulty does worse.
the other irt methods have comparable correla-tion.
thus, by using irt, dad can both improverankings and guide annotation..5 qualitative insights on leaderboards.
dad also helps qualitative analysis of items andsubjects.
first, irt identifies overfitting and gener-alizes partitioning datasets by difficulty.
then weshow that—like in educational testing—irt identi-fies good and bad items..figure 5: suppose we need to cold start and collectannotations for a new subject: what order would mostrapidly increase correlation to the full test data?
as weexpect, the correlations eventually converge, but withlittle data, irt has better correlation than other methods.
we suspect that the irt information underperforms earlyon when the subject ability estimate is unstable..two.12 we propose that items should be selected ac-cording to their fisher information content (weiss,1982).
(p′.
ii(θj) =.
ij)2pij(1 − pij)as derived by lord et al.
(1968, p. 70)..= γ2.
i pij(1 − pij).
(2).
intuitively, if we do not yet know the true skill θj,we should pick items whose expected response weare most uncertain about.
our uncertainty (entropy)is maximized when the likelihood of a correct re-.
12we train an irt-disc model to simplify sampling (e.g.,avoiding a tradeoff between feasibility and discriminability)..44911632641282565121,0242,0484,096development set sample size1632641282565121,0242,0484,096development set sample size0.40.50.60.70.80.91.0kendall rank correlationdev sample to dev sample0.40.50.60.70.80.91.0kendall rank correlationdev sample to testirt to irtacc to acccorrelation1020301002003001,0002,00010,000development set sample size0.500.550.600.650.700.750.800.850.900.951.00correlation to test rankhigh informationhigh discriminationhigh disc + diﬀhigh diﬃcultyrandomsampling methodfigure 6: we partition evaluation data by irt difficulty and discriminability with accuracy in each quartile.
most improvements in high-accuracy systems come from getting high-difficulty questions right.
items with lowdiscriminability (and thus prone to annotation errors) are difficult for all subjects except the overfit args-bertmodel.
we include top-performing squad subjects, several notable subjects (systems), and a pair from the bottomof the leaderboard..5.1 guiding analysis with irt.
5.2.identifying annotation error.
several works curate easy and hard qa subsetsbased on how many models answer correctly (ron-deau and hazen, 2018) or heuristics (sugawaraet al., 2018).
irt can create similar subsets usingirt-feas, the best 1d model.
difficulty finds wheresubjects improve while discriminability and feasi-bility can surface items that may be invalid.
forexample, one low feasibility question (figure 9)asks “what are two examples of types of turingmachines?” which has two problems: (1) the an-swer omits five types and (2) span-based evaluationprecludes selecting non-contiguous types..after.
excluding.
items with.
negativediscriminability—they are likely erroneous—we sort items into bins.
we break both difficultyand discriminability into four bins—taking the 25th,50th, and 75th percentiles—creating eight total bins.
then we select representative squad subjectswith their exact match scores (figure 6).
let’sexamine a feasible item with positive difficulty anddiscriminability like “what reform was attemptedfollowing the nice treaty?”13in this case, theannotator’s span is too long—resulting in almostno correct answers and a low fuzzy match (tokenf1).
in contrast, one highly discriminative questionsucceeds because there are multiple plausibleguesses to “who did the normans team up with inanatolia?”14 while both the armenian state andturkish forces are superficially plausible answers,only turkish forces is correct; nonetheless, somemodels are fooled.
using irt to guide subjectanalysis is helpful; next, we test how efficient it isin identifying annotation error..to test if irt can identify annotation error, we in-spect sixty squad development set items.
we se-lect ten items from each of these groups: the mostnegative discriminability, discriminability nearestto zero, the highest discriminability, the least diffi-cult, most difficult, and irt model errors.
for each,we annotate whether the item was correct, was “cor-rect” yet flawed in some way, or simply wrong (fig-ure 7).15 inter-annotator agreement between threeauthors on this three-way annotation with krippen-dorff’s α (krippendorff, 2004; artstein and poesio,2008) is 0.344. despite only modest agreement,just as in the development of education tests, neg-ative discriminability is predictive of bad items.
when discriminability is negative, then the prob-ability of getting the answer right is higher whenability is lower, which is undesirable: ken con-sistently loses to burt on those items.
this couldidentify bad items in evaluation sets for removal..6 related work.
dad draws together two primary threads: we useirt to understand datasets, which has been appliedto other nlp tasks, and apply it to improving leader-boards.
finally, we explore how the insights of irtcan improve not just the analysis of test sets but toimprove the construction of test sets..irt in nlpirt is gaining traction in machinelearning research (martínez-plumed et al., 2016,2019) where automated metrics can be mislead-ing (sedoc et al., 2019): machine translation (hop-kins and may, 2013) and chatbot evaluation (sedoc.
13a: “there was an attempt to reform the constitutional lawof the eu and make it more transparent.” (appendix figure 10).
14example with statistics in appendix figure 11..15annotation guidelines provided in supplementary materi-als; figure 7 uses the first set of annotations which were lateraugmented by two additional sets of annotations..4492test accsa-net on albertrobertabert + synthetic self-trainingarsg-berttree-lstm + bidaf + elmoname7591588783bottom modelhallmarkoverﬁttop modeldescription4295919679959799199042778598979963949498diﬃcultylowmed-lowmed-highhigh9460917998789896624667529889989498659690discriminabilitylowmed-lowmed-highhigh0.21.0dev accfigure 7: we annotate squad items by discriminability, difficulty, and irt prediction errors.
for example, onequestion with negative discriminability was classified as “wrong” with the explanation that the annotated answerindicates it is not answerable, but the question actually is answerable.
items with negative discriminability orwhere irt’s prediction is wrong have a much higher rate of annotation error (“flawed” or “wrong”).
using similarmethodology, errors in datasets could be more rapidly identified..and ungar, 2020).
concurrent with our work, va-nia et al.
(2021) compare nlp test sets with irt.
closest to our work in nlp is otani et al.
(2016),who rank machine translation subjects and computecorrelations with gold scores.
similarly, martínez-plumed and hernández-orallo (2020) use irt onnon-language ai video game benchmarks.
just aswe use irt to identify difficult or easy items, laloret al.
(2016) create challenge sets for textual entail-ment.
we test irt as a way to guide annotation,but it can also train nlp models; for example, deepmodels learn “easy” examples faster (lalor et al.,2018) and maintain test accuracy when trainingdata are down-sampled (lalor et al., 2019)..improving leaderboards the rise nlp leader-boards has encouraged critical thought into im-proving them (linzen, 2020), improving evalua-tion more broadly (eger et al., 2020), and thought-ful consideration of their influence on the direc-tion of research (sculley et al., 2018; dotan andmilli, 2020).
dad aims make leaderboard yard-sticks (hernandez-orallo, 2020) more reliable, in-terpretable, and part of curating the benchmarkitself.
in line with our reliability goal, just as statis-tical tests should appear in publications (dror et al.,2018; dodge et al., 2019), they should be “freebies”for leaderboard participants (ethayarajh and juraf-sky, 2020).
alternatively, hou et al.
(2019) positthat leaderboards could be automatically extractedfrom publications.
how to aggregate multi-taskbenchmarks (wang et al., 2019b,a; fisch et al.,2019) and multi-metric benchmarks (ma et al.,2021) is an open question which—although wedo not address—is one use for irt..this work implicitly argues that leaderboardsshould be continually updated.
as a (static) leader-board ages, the task(s) overfit (recht et al., 2019)which—although mitigable (blum and hardt, 2015;anderson-cook et al., 2019)—is best solved bycontinually collecting new data (kiela et al.,2021).
ideally, new data should challenge mod-els through adversarial collection (wallace et al.,2019b; nie et al., 2020) and related methods (gard-ner et al., 2020).
however, if making an easyleaderboard more difficult is possible, the leader-board has outlived its helpfulness and should beretired (voorhees, 2000)..part of our work centers on alternate task effi-cacy rankings, but this naïvely assumes that taskefficacy is the sole use case of leaderboards.
in-deed, focusing solely these factors can mislead thepublic (paullada et al., 2020) and may not reflecthuman language capabilities (schlangen, 2020).
leaderboards are also well positioned to provideincentive structures for participants to prioritizefairness (bender and friedman, 2018) and effi-ciency (strubell et al., 2019; schwartz et al., 2020;min et al., 2021) or incorporate testing of specificcapabilities (ribeiro et al., 2020; dunietz et al.,2020).
to enable these more nuanced analyses,leaderboards should accept runnable models ratherthan static predictions (ma et al., 2021)..active learning beyond irt, the analysis oftraining dynamics and active learning (settles,2009) is helpful for actively sampling specificitems or identifying low-quality items (brodleyand friedl, 1999).
for example, swayamdipta et al.
(2020) and pleiss et al.
(2020) propose alternative.
4493048countdiﬀ: highdiﬀ: lowdisc: highdisc: negdisc: ≈0irt valcorrectflawedwrongcorrectflawedwrongcorrectflawedwrongcorrectflawedwrongcorrectflawedwrongcorrectflawedwrongnoneis answerableis answerable + misleadingone answer wronganswer partially correctambiguousincomplete answermisleadingambiguous + missing answerbad questionbad question + bad answersno answerhigh probability + no answerlow probabilityanswer set incompleteexplanationtraining dynamics-based methods for identifyingdifficult items as well annotation errors.
evencloser to goals, rahman et al.
(2020) use activelearning to build a test collection.
explicitly mea-suring how effectively examples separate the bestsubject from the rest allows test set curators to “fo-cus on the bubble” (boyd-graber and börschinger,2020), prioritizing examples most likely to revealinteresting distinctions between submitted systems..alternate formulationsirt is an example ofconvergent evolution of models that predict subjectaction given an item.
ideal point models (pooleand rosenthal, 2017) consider how a legislator(subject) will vote on a bill (item) and use a simi-lar mathematical formulation.
the venerable elomodel (glickman and jones, 1999) and modernextensions (herbrich et al., 2007) predict whethera player (subject) will defeat an opponent (item)with, again, a similar mathematical model.
certainirt models can also be formulated as nonlinearmixed models (rijmen et al., 2003), where theitem parameters are fixed effects and the latent sub-ject parameters are random effects.
this allows forcomparisons between irt models and other mixedeffects models under a consistent framework.
irt-base and irt-disc can be formulated as nonlinearmixed models, and irt-feas can be formulated asa discrete mixture model over items.
as we discussfurther in the next section, dad’s application of irtcan further be improved by adopting interpretableextensions of these models..7 conclusion.
this paper advocates incorporating decades of re-search in crafting education tests to improve howwe evaluate the capabilities of nlp models.
we pro-pose and validate an alternate irt ranking methodfor leaderboard evaluations, show it can guide an-notation, detect annotation error, and naturally par-tition evaluation data.
just as educators movedfrom classical testing to irt, the nlp communityshould consider future evaluations with irt..7.1 limitations.
although there is much to gain through irt evalu-ation, there are limitations which make it hard toimplement.
first, it requires access to item-levelresponses for all examples for all subjects whichare often only available to organizers.
second, ur-bano (2016) notes that sampling mutually exclusivesubsets has drawbacks—samples are not entirely.
independent.
lastly, our work is a proof of conceptusing squad 2.0 as a test bed and our results maynot generalize..8 future work.
we see a few directions for future work.
first, thispaper is intended to validate irt and its usefulnessas an active part of the leaderboard lifecycle; thenatural next step is to implement it in a leaderboard.
second, our irt models do not incorporate the itemcontent (e.g., example text) to predict responses,but in principle could; bayesian models with meta-data (card et al., 2018) and ideal point models frompolitical science (poole and rosenthal, 1985) thatincorporate bills and speeches do exactly this (ger-rish and blei, 2011; nguyen et al., 2015; kraftet al., 2016).
analogously, irt for leaderboardscan and should also incorporate text from passages,questions, and answers to better model what makesquestions difficult.
such a model can also predictwhich characteristics would create discriminatingor difficult items.
lastly, multidimensional irtmodels to evaluate multiple skills could aid multi-task or multi-metric leaderboards like mrqa (fischet al., 2019) and dynaboard (ma et al., 2021)..acknowledgements.
for their work on early iterations of leaderboardvisualizations, we thank jacob bremerman and weiwei chi.
for insightful discussions and ideas wethank shi feng, doug oard, joão sedoc, mikewu, and patrick lewis.
we thank peter rankelfor recommendations on statistical testing methods.
for discussion and feedback on visualizations, wethank leo zhicheng liu, calvin bao, and class-mates in umd’s fall 2020 “information visualiza-tion” course.
for suggestions on topic modeling,we thank philip resnik and maria antoniak.
forfeedback on prior versions of this paper, we thankour anonymous acl reviewers and members ofthe umd clip lab.
boyd-graber and rodriguez’swork at umd were supported by nsf grant iis-1822494. the views and conclusions containedherein are those of the authors and should not beinterpreted as necessarily representing the officialpolicies, either expressed or implied, of the sponsor.
the u.s. government is authorized to reproduceand distribute reprints for governmental purposesnotwithstanding any copyright annotation therein..4494references.
alekh agarwal, olivier chapelle, miroslav dudík, andjohn langford.
2014. a reliable effective terascalelinear learning system.
journal of machine learningresearch, 15:1111–1133..national council on measurement in education, jointcommittee on standards for educational and psy-chological testing (u.s.) american educational re-search association, american psychological asso-ciation.
2014. standards for educational and psy-chological testing.
american educational researchassociation, washington, dc..christine m anderson-cook, kary l myers, lu lu,michael l fugate, kevin r quinlan, and normapawley.
2019. how to host an effective data compe-tition: statistical advice for competition design andanalysis.
statistical analysis and data mining: theasa data science journal, 12(4):271–289..ron artstein and massimo poesio.
2008. inter-coderagreement for computational linguistics.
computa-tional linguistics, 34(4):555–596..frank b baker.
2001. the basics of item response.
theory.
eric..jordan boyd-graber and benjamin börschinger.
2020.what question answering can learn from trivia nerds.
in proceedings of the association for computationallinguistics.
association for computational linguis-tics..carla e brodley and mark a friedl.
1999. identifyingmislabeled training data.
the journal of artificialintelligence research, 11(1):131–167..chris buckley and ellen m voorhees.
2000. evaluatingevaluation measure stability.
in proceedings of theacm sigir conference on research and develop-ment in information retrieval..dallas card, chenhao tan, and noah a smith.
2018.neural models for documents with metadata.
in pro-ceedings of the association for computational lin-guistics.
association for computational linguistics..rafael jaime de ayala.
2013. the theory and practiceof item response theory.
guilford publications..jesse dodge, suchin gururangan, dallas card, royschwartz, and noah a. smith.
2019. show yourwork: improved reporting of experimental results.
association for computational linguistics..yonatan belinkov and james glass.
2019. analysismethods in neural language processing: a survey.
intransactions of the association for computationallinguistics, pages 49–72..ravit dotan and smitha milli.
2020. value-laden dis-ciplinary shifts in machine learning.
in proceedingsof the 2020 conference on fairness, accountability,and transparency..emily m bender and batya friedman.
2018. datastatements for natural language processing: towardmitigating system bias and enabling better science.
transactions of the association for computationallinguistics, 6:587–604..emily m bender and alexander koller.
2020. climbingtowards nlu: on meaning, form, and understandingin the age of data.
in proceedings of the associa-tion for computational linguistics.
association forcomputational linguistics..james bergstra, daniel yamins, and david cox.
2013.making a science of model search: hyperparameteroptimization in hundreds of dimensions for visionarchitectures.
in proceedings of the 30th interna-tional conference on machine learning, volume 28of proceedings of machine learning research, pages115–123.
pmlr..eli bingham, jonathan p. chen, martin jankowiak, fritzobermeyer, neeraj pradhan, theofanis karaletsos,rohit singh, paul szerlip, paul horsfall, and noah d.goodman.
2018. pyro: deep universal probabilis-tic programming.
journal of machine learning re-search..avrim blum and moritz hardt.
2015. the ladder: a re-liable leaderboard for machine learning competitions.
in proceedings of the international conference ofmachine learning.
pmlr..rotem dror, gili baumer, segev shlomov, and roireichart.
2018. the hitchhiker’s guide to testing sta-tistical significance in natural language processing.
in proceedings of the association for computationallinguistics.
association for computational linguis-tics..jesse dunietz, gregory burnham, akash bharadwaj,owen rambow, jennifer chu-carroll, and david fer-rucci.
2020. to test machine comprehension, startby defining comprehension.
in proceedings of theassociation for computational linguistics..f y edgeworth.
1888. the statistics of examinations.
journal of the royal statistical society, 51(3):599–635..bradley efron.
1994. an introduction to the bootstrap..chapman & hall, new york..steffen eger, yang gao, maxime peyrard, wei zhao,and eduard hovy, editors.
2020. proceedings of thefirst workshop on evaluation and comparison ofnlp systems.
association for computational lin-guistics..kawin ethayarajh and dan jurafsky.
2020. utility is inthe eye of the user: a critique of nlp leaderboards.
in proceedings of empirical methods in natural lan-guage processing.
association for computationallinguistics..4495shi feng, eric wallace, alvin grissom ii, mohit iyyer,pedro rodriguez, and jordan boyd-graber.
2018.pathologies of neural models make interpretationsdifficult.
in proceedings of empirical methods innatural language processing.
association for com-putational linguistics..adam fisch, alon talmor, robin jia, minjoon seo,eunsol choi, and danqi chen.
2019. mrqa 2019shared task: evaluating generalization in readingin proceedings of the 2nd work-comprehension.
shop on machine reading for question answering.
association for computational linguistics..matt gardner, yoav artzi, victoria basmova, jonathanberant, ben bogin, sihao chen, pradeep dasigi,dheeru dua, yanai elazar, ananth gottumukkala,nitish gupta, hanna hajishirzi, gabriel ilharco,daniel khashabi, kevin lin, jiangming liu, nel-son f liu, phoebe mulcaire, qiang ning, sameersingh, noah a smith, sanjay subramanian, reuttsarfaty, eric wallace, ally zhang, and ben zhou.
2020. evaluating models’ local decision boundariesvia contrast sets.
in findings of the association forcomputational linguistics: emnlp.
association forcomputational linguistics..sean m gerrish and david m blei.
2011. predictinglegislative roll calls from text.
in proceedings of theinternational conference of machine learning..mark e glickman and albyn c jones.
1999. rating the.
chess rating system.
chance, 12..ronald hambleton.
1991. fundamentals of item re-sponse theory.
sage publications, newbury park,calif..ralf herbrich, tom minka, and thore graepel.
2007.trueskill™: a bayesian skill rating system.
in pro-ceedings of advances in neural information process-ing systems..jose hernandez-orallo.
2020. ai evaluation: on brokenyardsticks and measurement scales.
in workshop onevaluating evaluation of ai systems at aaai..mark hopkins and jonathan may.
2013. models oftranslation competitions.
in proceedings of the asso-ciation for computational linguistics.
associationfor computational linguistics..yufang hou, charles jochim, martin gleize, francescabonin, and debasis ganguly.
2019. identificationof tasks, datasets, evaluation metrics, and numericscores for scientific leaderboards construction.
inproceedings of the association for computationallinguistics.
association for computational linguis-tics..m g kendall.
1938. a new measure of rank correlation..biometrika, 30(1/2):81–93..douwe kiela, max bartolo, yixin nie, divyanshkaushik, atticus geiger, zhengxuan wu, bertie vid-gen, grusha prasad, amanpreet singh, pratik ring-shia, zhiyi ma, tristan thrush, sebastian riedel,zeerak waseem, pontus stenetorp, robin jia, mohitbansal, christopher potts, and adina williams.
2021.dynabench: rethinking benchmarking in nlp.
inconference of the north american chapter of the as-sociation for computational linguistics.
associationfor computational linguistics..diederik p. kingma and jimmy ba.
2015. adam: amethod for stochastic optimization.
in proceedingsof the international conference on learning repre-sentations..peter kraft, hirsh jain, and alexander m rush.
2016.an embedding model for predicting roll-call votes.
in proceedings of empirical methods in natural lan-guage processing.
association for computationallinguistics..klaus krippendorff.
2004. content analysis: an intro-duction to its methodology.
sage: thousand oaks,ca.
chapter 11..tom kwiatkowski, jennimaria palomaki, olivia red-field, michael collins, ankur parikh, chris alberti,danielle epstein, illia polosukhin, jacob devlin, ken-ton lee, kristina toutanova, llion jones, matthewkelcey, ming-wei chang, andrew m dai, jakobuszkoreit, quoc le, and slav petrov.
2019. natu-ral questions: a benchmark for question answeringresearch.
transactions of the association for compu-tational linguistics, 7:453–466..john p lalor, hao wu, tsendsuren munkhdalai, andhong yu.
2018. understanding deep learning per-formance through an examination of test set diffi-culty: a psychometric case study.
in proceedings ofempirical methods in natural language processing.
association for computational linguistics..john p lalor, hao wu, and hong yu.
2016. buildingan evaluation scale using item response theory.
inproceedings of empirical methods in natural lan-guage processing.
association for computationallinguistics..john p lalor, hao wu, and hong yu.
2019. learninglatent parameters without human response patterns:item response theory with artificial crowds.
in pro-ceedings of empirical methods in natural languageprocessing.
association for computational linguis-tics..michael i. jordan, zoubin ghahramani, tommi s.jaakkola, and lawrence k. saul.
1999. an intro-duction to variational methods for graphical models.
machine learning, 37(2):183–233..david d lewis and william a gale.
1994. a sequentialalgorithm for training text classifiers.
in proceedingsof the acm sigir conference on research and de-velopment in information retrieval.
springer-verlag..4496tal linzen.
2020. how can we accelerate progressintowards human-like linguistic generalization?
proceedings of the association for computationallinguistics.
association for computational linguis-tics..zachary c. lipton and jacob steinhardt.
2019. trou-bling trends in machine learning scholarship.
queue,17(1)..f m lord, m r novick, and allan birnbaum.
1968..statistical theories of mental test scores..zhiyi ma, kawin ethayarajh, tristan thrush, somyajain, ledell wu, robin jia, christopher potts, ad-ina williams, and douwe kiela.
2021. dynaboard:an evaluation-as-a-service platform for holistic next-generation benchmarking..f martínez-plumed and j hernández-orallo.
2020.dual indicators to analyze ai benchmarks: diffi-culty, discrimination, ability, and generality.
ieeetransactions on computational intelligence in ai andgames, 12(2):121–131..fernando martínez-plumed, ricardo b c prudêncio,adolfo martínez-usó, and josé hernández-orallo.
2016. making sense of item response theory in ma-chine learning.
in proceedings of the twenty-secondeuropean conference on artificial intelligence..fernando martínez-plumed, ricardo b c prudêncio,adolfo martínez-usó, and josé hernández-orallo.
2019. item response theory in ai: analysing ma-chine learning classifiers at the instance level.
artifi-cial intelligence, 271:18–42..james r. mcbride.
1976. bandwidth, fidelity, and adap-tive tests.
t. j. mcconnell, jr.
(ed.
), cat/c 2 1975:the second conference on computer-assisted test con-struction.
atlanta ga: atlanta public schools..andrew kachites mccallum.
2002..a machinehttp://mallet.cs.umass.edu..learning.
for.
language.
mallet:toolkit..tom mccoy, ellie pavlick, and tal linzen.
2019. rightfor the wrong reasons: diagnosing syntactic heuris-tics in natural language inference.
in proceedings ofthe association for computational linguistics.
asso-ciation for computational linguistics..sewon min, jordan l boyd-graber, c alberti, danqichen, eunsol choi, m collins, kelvin guu, han-naneh hajishirzi, kenton lee, j palomaki, colin raf-fel, a roberts, t kwiatkowski, patrick lewis, y wu,heinrich kuttler, l liu, pasquale minervini, pontusstenetorp, sebastian riedel, sohee yang, minjoonseo, gautier izacard, f petroni, l hosseini, nicolade cao, e grave, ikuya yamada, sonse shimaoka,masatoshi suzuki, shumpei miyawaki, s sato, ryotakahashi, j suzuki, martin fajcik, martin docekal,karel ondrej, p smrz, hao cheng, y shen, x liu,pengcheng he, w chen, jianfeng gao, barlas o˘guz,xilun chen, v karpukhin, stan peshterliev, dmytrookhonko, m schlichtkrull, sonal gupta, yashar.
mehdad, and wen-tau yih.
2021. neurips 2020 effi-cientqa competition: systems, analyses and lessonslearned..kathleen e moreno, c douglas wetzel, james rmcbride, and david j weiss.
1984. relationshipbetween corresponding armed services vocationalaptitude battery (asvab) and computerized adap-tive testing (cat) subtests.
applied psychologicalmeasurement, 8(2):155–163..adam najberg.
2018. alibaba ai model tops humans.
in reading comprehension..prathiba natesan, ratna nandakumar, tom minka, andjonathan d rubright.
2016. bayesian prior choice inirt estimation using mcmc and variational bayes.
frontiers in psychology, 7:1422..tri nguyen, mir rosenberg, xia song, jianfeng gao,saurabh tiwary, rangan majumder, and li deng.
2016. ms marco: a human generated machinein proceedingsreading comprehension dataset.
of the nips workshop on cognitive computation:integrating neural and symbolic approaches..viet-an nguyen, jordan boyd-graber, philip resnik,and kristina miler.
2015. tea party in the house: ahierarchical ideal point topic model and its applica-tion to republican legislators in the 112th congress.
in proceedings of the association for computationallinguistics.
association for computational linguis-tics..yixin nie, adina williams, emily dinan, mohit bansal,jason weston, and douwe kiela.
2020. adversar-ial nli: a new benchmark for natural language un-derstanding.
in proceedings of the association forcomputational linguistics.
association for compu-tational linguistics..timothy niven and hung-yu kao.
2019. probing neu-ral network comprehension of natural language argu-ments.
in proceedings of the association for compu-tational linguistics.
association for computationallinguistics..naoki otani, toshiaki nakazawa, daisuke kawahara,and sadao kurohashi.
2016.irt-based aggrega-tion model of crowdsourced pairwise comparison forevaluating machine translations.
in proceedings ofempirical methods in natural language processing.
association for computational linguistics..adam paszke, sam gross, francisco massa, adamlerer, james bradbury, gregory chanan, trevorkilleen, zeming lin, natalia gimelshein, lucaantiga, alban desmaison, andreas kopf, edwardyang, zachary devito, martin raison, alykhan te-jani, sasank chilamkurthy, benoit steiner, lu fang,junjie bai, and soumith chintala.
2019. pytorch:an imperative style, high-performance deep learn-ing library.
in proceedings of advances in neuralinformation processing systems..4497amandalynne paullada,.
inioluwa deborah raji,emily m bender, emily denton, and alex hanna.
2020. data and its (dis)contents: a survey of datasetdevelopment and use in machine learning research.
in proceedings of the neurips 2020 workshop: mlretrospectives, surveys and meta-analyses..marco tulio ribeiro, tongshuang wu, carlos guestrin,and sameer singh.
2020. beyond accuracy: be-havioral testing of nlp models with checklist.
inproceedings of the association for computationallinguistics.
association for computational linguis-tics..judea pearl.
1988. probabilistic reasoning in intelli-gent systems: networks of plausible inference.
mor-gan kaufmann publishers inc., san francisco, ca,usa..frank rijmen, francis tuerlinckx, paul de boeck, andpeter kuppens.
2003. a nonlinear mixed modelframework for item response theory.
psychologicalmethods, 8(2):185..geoff pleiss, tianyi zhang, ethan r elenberg, and kil-ian q weinberger.
2020. identifying mislabeled datausing the area under the margin ranking.
in proceed-ings of advances in neural information processingsystems..peter w van rijn, sandip sinharay, shelby j haber-man, and matthew s johnson.
2016. assessmentof fit of item response theory models used in large-scale educational survey assessments.
large-scaleassessments in education, 4(1):10..pavlin g. poliˇcar, martin stražar, and blaž zupan.
2019.opentsne: a modular python library for t-sne dimen-sionality reduction and embedding..keith t poole and howard rosenthal.
1985. a spatialmodel for legislative roll call analysis.
americanjournal of political science, 29(2):357–384..keith t poole and howard rosenthal.
2017. ideology& congress: a political economic history of roll callvoting, 2 edition.
routledge, london, england..md mustafizur rahman, mucahid kutlu, tamer el-sayed, and matthew lease.
2020. efficient test col-lection construction via active learning.
in proceed-ings of the 2020 acm sigir on international confer-ence on theory of information retrieval.
associationfor computing machinery..pranav rajpurkar, robin jia, and percy liang.
2018.know what you don’t know: unanswerable ques-tions for squad.
in proceedings of the associationfor computational linguistics.
association for com-putational linguistics..pranav rajpurkar, jian zhang, konstantin lopyrev, andpercy liang.
2016. squad: 100,000+ questions formachine comprehension of text.
in proceedings ofempirical methods in natural language processing.
association for computational linguistics..georg rasch.
1960. studies in mathematical psychol-ogy: i. probabilistic models for some intelligenceand attainment tests.
studies in mathematical psy-chology: i. probabilistic models for some intelli-gence and attainment tests.
nielsen & lydiche, ox-ford, england..benjamin recht, rebecca roelofs, ludwig schmidt,and vaishaal shankar.
2019. do imagenet classi-in proceedings offiers generalize to imagenet?
the international conference of machine learning.
pmlr..mark d reckase.
2009. multidimensional item re-in reckase, editor, multi-sponse theory models.
dimensional item response theory, pages 79–112.
springer new york, new york, ny..marc-antoine rondeau and t j hazen.
2018. system-atic error analysis of the stanford question answeringdataset.
in proceedings of the workshop on machinereading for question answering.
association forcomputational linguistics..david schlangen.
2020. targeting the benchmark: onmethodology in current natural language processingresearch..roy schwartz, jesse dodge, noah a smith, and orenetzioni.
2020. green ai.
communications of theacm, 63(12):54–63..d sculley, jasper snoek, alexander b wiltschko, anda rahimi.
2018. winner’s curse?
on pace, progress,and empirical rigor.
in proceedings of the interna-tional conference on learning representations..joao sedoc, daphne ippolito, arun kirubarajan, jaithirani, lyle ungar, and chris callison-burch.
2019.chateval: a tool for chatbot evaluation.
in confer-ence of the north american chapter of the associa-tion for computational linguistics.
association forcomputational linguistics..joão sedoc and lyle ungar.
2020. item response the-ory for efficient human evaluation of chatbots.
inproceedings of the first workshop on evaluationand comparison of nlp systems.
association forcomputational linguistics..priyanka sen and amir saffari.
2020. what do modelsin pro-learn from question answering datasets?
ceedings of empirical methods in natural languageprocessing.
association for computational linguis-tics..burr settles.
2009. active learning literature sur-vey.
technical report 1648, university of wisconsin–madison..emma strubell, ananya ganesh, and andrew mccal-lum.
2019. energy and policy considerations fordeep learning in nlp.
in proceedings of the associa-tion for computational linguistics.
association forcomputational linguistics..4498alex wang, yada pruksachatkun, nikita nangia, aman-preet singh, julian michael, felix hill, omer levy,and samuel bowman.
2019a.
superglue: a stick-ier benchmark for general-purpose language under-in proceedings of advances instanding systems.
neural information processing systems..alex wang, amapreet singh, julian michael, felix hill,omer levy, and samuel r. bowman.
2019b.
glue:a multi-task benchmark and analysis platform fornatural language understanding.
in proceedings ofthe international conference on learning represen-tations..david j weiss.
1982. improving measurement qualityand efficiency with adaptive testing.
applied psycho-logical measurement, 6(4):473–492..david j weiss and g gage kingsbury.
1984. appli-cation of computerized adaptive testing to educa-tional problems.
journal of educational measure-ment, 21(4):361–375..m wu, r davis, b domingue, c piech, and noah dgoodman.
2020. variational item response theory:fast, accurate, and expressive.
in 13th internationalconference on educational data mining..tongshuang wu, marco tulio ribeiro, jeffrey heer, anddaniel weld.
2019. errudite: scalable, reproducible,and testable error analysis.
in proceedings of the as-sociation for computational linguistics.
associationfor computational linguistics..jiawei zhang, yang wang, piero molino, lezhi li, anddavid s ebert.
2019. manifold: a model-agnosticframework for interpretation and diagnosis of ma-chine learning models.
ieee transactions on visual-ization and computer graphics, 25(1):364–373..saku sugawara, kentaro inui, satoshi sekine, andakiko aizawa.
2018. what makes reading compre-hension questions easier?
in proceedings of empiri-cal methods in natural language processing.
asso-ciation for computational linguistics..saku sugawara, yusuke kido, hikaru yokono, andakiko aizawa.
2017. evaluation metrics for ma-chine reading comprehension: prerequisite skills andreadability.
in proceedings of the association forcomputational linguistics.
association for compu-tational linguistics..swabha swayamdipta, roy schwartz, nicholas lourie,yizhong wang, hannaneh hajishirzi, noah a smith,and yejin choi.
2020. dataset cartography: mappingand diagnosing datasets with training dynamics.
inproceedings of empirical methods in natural lan-guage processing.
association for computationallinguistics..jean tague-sutcliffe.
1992. the pragmatics of informa-tion retrieval experimentation, revisited.
informationprocessing & management, 28(4):467–490..ross e traub.
1997. classical test theory in historicalperspective.
educational measurement, 16:8–13..julián urbano.
2016. test collection reliability: a studyof bias and robustness to statistical assumptions viastochastic simulation.
information retrieval journal,19(3):313–350..clara vania, phu mon htut, william huang, dharamungra, richard yuanzhe pang, jason phang,haokun liu, kyunghyun cho, and samuel r. bow-man.
2021. comparing test sets with item responsetheory.
in proceedings of the association for compu-tational linguistics.
association for computationallinguistics..ellen m voorhees.
1998. variations in relevance judg-ments and the measurement of retrieval effectiveness.
in proceedings of the acm sigir conference onresearch and development in information retrieval.
association for computing machinery..ellen m voorhees.
2000. the trec-8 question answer-.
ing track report..ellen m voorhees.
2003. evaluating the evaluation: acase study using the trec 2002 question answeringtrack.
in conference of the north american chapterof the association for computational linguistics..eric wallace, shi feng, nikhil kandpal, matt gard-ner, and sameer singh.
2019a.
universal adversarialtriggers for attacking and analyzing nlp.
in pro-ceedings of empirical methods in natural languageprocessing.
association for computational linguis-tics..eric wallace, pedro rodriguez, shi feng, and jordanboyd-graber.
2019b.
trick me if you can: human-in-the-loop generation of adversarial question answeringin transactions of the association forexamples.
computational linguistics, pages 387–401..4499description.
emdev.
emtest abilitydev abilitytest.
feature.
allirt.
item idsubject idquestioncontextstats.
subject & item idtopics 1ktitlebaseline.
all the featuresirt values for difficulty, discrim-inability, feasibility, and abilitythe item’s idthe subject’s idquestion wordscontext wordsquestion & context lengths; answer-ability, answer position & length; dif-ficulty from sugawara et al.
(2017)item and subject idtopic weights of question wordswikipedia page title wordsno features, majority class baseline.
table 1: the linear model integrates a variety of featuresto determine which are most predictive of a subjectresponding correctly to an item..ability.
irt-feas.
irt-disc.
irt-base.
irt-feasirt-discirt-base.
1.000.9470.895.
0.9471.000.907.
0.8950.9071.00.table 2: table entries are kendall’s τ rank correlationof irt subject ability between rows and columns.
gen-erally, the models agree on the ranking with the irt-feas and irt-disc having the strongest correlation..a squad item examples.
figures 8, 9, 10, and 11 show previously discussedsquad examples (§5) in full.
the squad annota-tions from figure 7 are included in supplementarymaterials and at irt.pedro.ai.
on the same page,we provide a web interface for inspecting the pa-rameters of the irt models.
figure 12 shows thefeasibility distribution corresponding to figure 1..b logistic regression features.
the linear model (§4.2) includes features based onitem ids, subject ids, textual features of the ques-tion, context, and answer, and topic model features.
table 1 lists the feature names from figure 3 withdescriptions of each.
when irt features or thestatistics features are used, they include interactionterms with themselves..c irt model type correlation.
emdevemtestabilitydevabilitytest.
1.000.9530.9540.931.
0.9531.000.9440.947.
0.9540.9441.000.950.
0.9310.9470.9501.00.table 3: entries are kendall’s rank correlation betweenrows and columns.
scores are squad exact match (em)and irt-disc ability..d ranking stability experiments.
here we provide further details for the rankingstability experiments (§4.2.3).
first, we filter fromthe 161 subjects that have development set scoresto the 115 that also have test set scores.16 in oursimulation, we run 10 trials for every sample size;sample size begins at 100 and with steps of 100. inaddition to these, we also run trials for sample sizes25, 50, and 75. since each sample can be no largerthan half the dataset, we stop at half the dataset..d.1 development and test set correlations.
table 3 uses a irt-disc model since we noticed thatin comparison irt-feas overfit the data, yieldingworse results.
the correlations with the full data areall strong, but not the same.
we conclude that—atleast on squad—irt rankings are modestly morereliable than classical rankings..d.2 statistical significance of difference in.
kendall tau coefficients.
while figure 4 shows a consistent difference incorrelation between ranking methods, it is unclearwhether this difference is statistically significant.
we estimate the statistical significance of the dif-ference through bootstrap sampling (efron, 1994).
since the null case is no difference in correla-tion coefficients, we seek a symmetric samplingdistribution centered at zero that represents a re-alistic density function.
each ranking stabilityexperiment17 trial results in two lists of numberpairs.
the lists correspond to subject scores ontwo datasets;18 each number pair is the subject’saccuracy and irt score.
to create the bootstrapdistribution, we (1) sample with replacement pairsfrom one list, (2) compute the correlation between.
although each irt model differs in expressiveness,they should—in general—produce similar results.
this is confirmed by computing the kendall’s rankcorrelation between the subject abilities and itemdifficulties (table 2)..16the squad organizers curate the test set subjects to avoid.
overfit, garbage, or duplicate submissions..17one experiment for development sample to development.
sample and one for development sample to test set..18in the first experiment, development set samples; in the.
second, a development set sample and the full test set..4500discriminability: -9.63 difficulty: -0.479 feasibility: 0.614 mean exact match: 0.472wikipedia page: economic inequality question id: 572a1c943f37b319004786e3question: why did the demand for rentals decrease?
official answer: demand for higher quality housingcontext: a number of researchers (david rodda, jacob vigdor, and janna matlack), argue that a shortage of affordablehousing – at least in the us – is caused in part by income inequality.
david rodda noted that from 1984 and 1991, thenumber of quality rental units decreased as the demand for higher quality housing increased (rhoda 1994:148).
throughgentrification of older neighbourhoods, for example, in east new york, rental prices increased rapidly as landlords foundnew residents willing to pay higher market rate for housing and left lower income families without rental units.
the advalorem property tax policy combined with rising prices made it difficult or impossible for low income residents to keeppace..figure 8: the example from squad with the lowest discriminability.
surprisingly, it had a negative discriminability,implying that the less skilled a subject is, the more likely their response is to be correct..discriminability: 3.24 difficulty: 3.86 feasibility: 0 mean exact match: 0wikipedia page: computational complexity theory question id: 56e1b00ce3433e14004230a1question: in the determination of complexity classes, what are two examples of types of turing machines?
official answer: probabilistic turing machines, non-deterministic turing machinescontext: many types of turing machines are used to define complexity classes, such as deterministic turing machines,probabilistic turing machines, non-deterministic turing machines, quantum turing machines, symmetric turing machinesand alternating turing machines.
they are all equally powerful in principle, but when resources (such as time or space)are bounded, some of these may be more powerful than others..figure 9: this question is regarded as infeasible by the irt model.
upon further inspection, the answer omits fiveacceptable answers, but more importantly does not permit all combinations of turing machines..the resampled ranking and unused ranking whenusing accuracy versus irt score, and (3) computeand store the irt correlation score minus the accu-racy correlation score.
we repeat this process 1000times for each of the 10 trials in the original ex-periment and aggregate all the differences to buildthe bootstrap distribution.
for each sample size wecompute the empirical p-value on each trial whichwe show in box and whisker plots (figure 13)..e the irt statistical test.
the irt test differs in two substantial ways fromother tests: (1) it does not assume that items areequally informative and (2) it does assume thatthe informativeness of items is a function of thesubject’s skill θj.
in the literature, this is closelyconnected to reliability (tague-sutcliffe, 1992) andeach item provides information about the locationof θj; as we accumulate more evidence for thelocation of θj the error of estimation decreases.
itis a well known result in irt that standard errorof estimate (see) σ(θˆ|θ) varies with respect to theagent location parameter θ (de ayala, 2013, p. 30)and is connected to the fisher information.
ii(θ) =.
(p′i)2pi(1 − pi).
(4).
of each item.
for a 2pl model, information.
is maximized when pi = (1 − pi).
since fisherinformation is additive, the information of the eval-uation set is maximal when items have a 50%chance of being responded to correctly.
as derivedby de ayala (2013, p. 102), the standard error ofestimation.
see(θ) =.
(6).
√︄.
1i ii(θ).
..∑︁.
is computed by accumulating the informationgained from each item.
given two subjects x andy , one can use the probability distribution of scoredifferences.
n (θy − θx , see(θx )2 + see(θy )2)to compute the probability that the difference inskill is greater than two standard errors which cor-responds to an α ≤ .05 significance level..(7).
f multidimensional irt clustering.
while we achieve strong held-out accuracy with10 dimensional irt (irt-vec), we had limited suc-cess in interpreting parameters.
we use tsne 19plots overlayed with features like item accuracy,the question’s wikipedia page, if the question wasanswerable, length of questions, and topic modelweights.
of these, item accuracy and answerabilityshowed the most obvious patterns (figure 14)..19we use opentsne (poliˇcar et al., 2019) with default.
ii(θ) = γ2pi(1 − pi).
(5).
parameters..4501discriminability: 2.1 difficulty: 2.38 feasibility: 0.995 mean exact match: 0.00621 mean f1: 0.546wikipedia page: european union law question id: 57268f2bf1498d1400e8e3c4question: what reform was attempted following the nice treaty?
official answer: an attempt to reform the constitutional law of the european union and make it more transparentcontext: following the nice treaty, there was an attempt to reform the constitutional law of the european union and makeit more transparent; this would have also produced a single constitutional document.
however, as a result of the referendumin france and the referendum in the netherlands, the 2004 treaty establishing a constitution for europe never came intoforce.
instead, the lisbon treaty was enacted.
its substance was very similar to the proposed constitutional treaty, but itwas formally an amending treaty, and – though it significantly altered the existing treaties – it did not completely replacethem..figure 10: this example shows that the answer span is likely too large, causing models to fail in both squad’sexact match and f1 metrics..discriminability: 8.01 difficulty: -1.41 feasibility: 0.939 mean exact match: 0.64 mean f1: 0.667wikipedia page: normas question id: 56de10b44396321400ee2595question: who did the normans team up with in anatolia?
official answer: turkish forcescontext: some normans joined turkish forces to aid in the destruction of the armenians vassal-states of sassoun andtaron in far eastern anatolia.
later, many took up service with the armenian state further south in cilicia and the taurusmountains.
a norman named oursel led a force of "franks" into the upper euphrates valley in northern syria.. .
..figure 11: this highly discriminative question succeeds because there are many plausible answers.
for example,although only “turkish forces” is correct, some models answer “the armenian state.”.
g reproducibility checklist.
here we provide reproducibility details to comple-ment our source code (https://irt.pedro.ai)..g.1 software and parameters.
are.
irt models.
allimplemented in py-torch (paszke et al., 2019) and pyro (binghamet al., 2018).
linear models are trained withvowpal wabbit (agarwal et al., 2014).
the topicmodel that generates features for the linear modeluses mallet (mccallum, 2002)..the number of irt model parameters is propor-tional to the number of subjects m and the numberof items n. the irt-base has one parameter persubject and one per item.
the irt-disc has oneparameter per subject and two per item.
the irt-feas has one parameter per subject and three peritem.
the irt-vec has ten parameters per subjectand thirty per item..g.2 hyperparameters.
invest significant effort.
in hyper-we did notparameter tuning the irt models and instead usedthe defaults in the py-irt software20 providedby lalor et al.
(2019).
the irt-base, irt-disc, andirt-feas models were trained for 1000 epochs withno early stopping conditions and a learning rateof 0.1 with adam (kingma and ba, 2015).
theirt-vec model was trained for 2500 epochs andused 10 dimensions..20github.com/jplalor/py-irt.
figure 12: the feasibility parameter λ of our irt modelrepresents the probability that an example is unsolvable.
for example, annotation error could lead to an exam-ple always being scored incorrectly—regardless of howgood the model is.
in squad 2.0, λ < .434 in the 5%percentile, λ < .698 for the 7.5%, and λ < .931 in the10% percentile..we repeated this approach with the multi-taskquestion answering shared task mrqa (fisch et al.,2019).
however, instead of using 10 dimensionswe use 6 to match the number of development settasks in mrqa.
although questions in narrativeqastandout (figure 15), there is not a discernible pat-tern amongst the other tasks.
we leave more so-phisticated methods for making multidimensionalirt models interpretable to future work..45020.000.100.200.300.400.500.600.700.800.901.00probability of feasibility (λ)10201002001,0002,00010,00020,000100,000count0.1%0.2%0.3%1%2%3%10%20%30%100%percentagefigure 13: p-values of the rank correlation difference for each sample size and trial in figure 4. the inherent noisein dev set sampling makes inferring significance difficult (left); test set driven results (right) are more significant..figure 15: in mrqa, tsne shows a relationship be-tween whether the task is narrativeqa with respect tomultidimensional difficulty and discriminability..eters.22 specifically, we used an optimization in-terval of 10, removed stop words, trained for 1000iterations, and used a document-topic thresholdof 0.05. each document was comprised of thewikipedia page title and the question text..g.3 computational resources.
the majority of experiments were conducted ona single workstation with an intel i7-7700k cpu,47gb of ram, and an nvidia 1080ti.
the aver-age runtime for the irt-feas model on cpu is 113seconds with a standard deviation of 2.31 over 5trials.
the average runtime of the irt-vec modelon gpu is 110 seconds with a standard deviation of0.5 over 5 trials..since each ranking stability experiment required(§4.3.1) re-training an irt-feas model on each sub-set, we parallelized this experiment on a cpu clus-ter where each trial received two cpu cores and16gb of ram.
in total, this included 520 trialswhich corresponds to twice that many trained irtmodels since one model is trained on each subsetof the data..figure 14: in squad, tsne shows a relationship be-tween mean exact match (item accuracy) and answer-ability with respect to multidimensional difficulty anddiscriminability..in the linear model, we used a hyperopt-based (bergstra et al., 2013) tool provided by vow-pal wabbit21 for hyper parameter search.
for eachlm, the tool spent 20 iterations optimizing thelearning rate, l2 regularization, and number ofbits against the logistic loss function.
the learningrate was searched from 0.001 to 10 with loguni-form sampling, l2 regularization from 1e − 8 to 1,and bits from 20 to 23 as categorical variables..the topic model that generated features for thelinear model used mallet, and we followed the rec-ommendations of the software to set hyper param-.
21github.com/vowpalwabbit/vowpal_wabbit.
22mallet.cs.umass.edu/topics.php.
45030.00.10.20.30.40.50.60.70.80.91.0p-value01,0002,0003,0004,0005,000sample size01,0002,0003,0004,0005,000sample sizedev sample to dev sampledev sample to test−60−40−200204060tsne dimension 1−60−40−200204060tsne dimension 1−70−50−30−1010305070tsne dimension 0answerablenot answerable0.00.20.40.60.8mean item accuracy−80−60−40−20020406080tsne dimension 0−60−40−200204060tsne dimension 1bioasqdropduorcracerelationextractiontextbookqamrqa task