reliability testing for natural language processing systems.
samson tan§(cid:92)∗ shaﬁq joty§‡ kathy baxter§araz taeihagh♦♣ gregory a. bennett§ min-yen kan(cid:92).
§salesforce research‡nanyang technological university(cid:92)school of computing, national university of singapore♦lee kuan yew school of public policy, national university of singapore♣centre for trusted internet and community, national university of singapore.
abstract.
questions of fairness, robustness, and trans-parency are paramount to address before de-ploying nlp systems.
central to these con-cerns is the question of reliability: can nlpsystems reliably treat different demographicsfairly and function correctly in diverse andnoisy environments?
to address this, we arguefor the need for reliability testing and contextu-alize it among existing work on improving ac-countability.
we show how adversarial attackscan be reframed for this goal, via a frameworkfor developing reliability tests.
we argue thatreliability testing — with an emphasis on inter-disciplinary collaboration — will enable rigor-ous and targeted testing, and aid in the enact-ment and enforcement of industry standards..1.introduction.
rigorous testing is critical to ensuring a programworks as intended (functionality) when used un-der real-world conditions (reliability).
hence, it istroubling that while natural language technologiesare becoming increasingly pervasive in our every-day lives, there is little assurance that these nlpsystems will not fail catastrophically or amplify dis-crimination against minority demographics whenexposed to input from outside the training distribu-tion.
recent examples include gpt-3 (brown et al.,2020) agreeing with suggested suicide (rousseauet al., 2020), the mistranslation of an innocuoussocial media post resulting in a minority’s arrest(hern, 2017), and biased grading algorithms thatcan negatively impact a minority student’s future(feathers, 2019).
additionally, a lack of rigoroustesting, coupled with machine learning’s (ml) im-plicit assumption of identical training and testingdistributions, may inadvertently result in systemsthat discriminate against minorities, who are oftenunderrepresented in the training data.
this can take.
∗correspondence to: samson.tan@salesforce.com.
figure 1: how doctor can integrate with existingsystem development workﬂows.
test (left) and sys-tem development (right) take place in parallel, separateteams.
reliability tests can thus be constructed inde-pendent of the system development team, either by aninternal “red team” or by independent auditors..the form of misrepresentation of or poorer perfor-mance for people with disabilities, speciﬁc gender,ethnic, age, or linguistic groups (hovy and spruit,2016; crawford, 2017; hutchinson et al., 2020)..amongst claims of nlp systems achieving humanparity in challenging tasks such as question answer-ing (yu et al., 2018), machine translation (has-san et al., 2018), and commonsense inference (de-vlin et al., 2019), research has demonstrated thesesystems’ fragility to natural and adversarial noise(goodfellow et al., 2015; belinkov and bisk, 2018)and out-of-distribution data (fisch et al., 2019).
it is also still common practice to equate “test-ing” with “measuring held-out accuracy”, even asdatasets are revealed to be harmfully biased (wag-ner et al., 2015; geva et al., 2019; sap et al., 2019)..many potential harms can be mitigated by detect-ing them early and preventing the offending modelfrom being put into production.
hence, in additionto being mindful of the biases in the nlp pipeline(bender and friedman, 2018; mitchell et al., 2019;.
proceedingsofthe59thannualmeetingoftheassociationforcomputationallinguisticsandthe11thinternationaljointconferenceonnaturallanguageprocessing,pages4153–4169august1–6,2021.©2021associationforcomputationallinguistics4153waseem et al., 2021) and holding creators account-able via audits (raji et al., 2020; brundage et al.,2020), we argue for the need to evaluate an nlpsystem’s reliability in diverse operating conditions..initial research on evaluating out-of-distributiongeneralization involved manually-designed chal-lenge sets (jia and liang, 2017; nie et al., 2020;gardner et al., 2020), counterfactuals (kaushiket al., 2019; khashabi et al., 2020; wu et al., 2021),biased sampling (søgaard et al., 2021) or toolk-its for testing if a system has speciﬁc capabilities(ribeiro et al., 2020) or robustness to distributionshifts (goel et al., 2021).
however, most of theseapproaches inevitably overestimate a given sys-tem’s worst-case performance since they do notmimic the nlp system’s adversarial distribution1..a promising technique for evaluating worst-caseperformance is the adversarial attack.
however, al-though some adversarial attacks explicitly focus onspeciﬁc linguistic levels of analysis (belinkov andbisk, 2018; iyyer et al., 2018; tan et al., 2020; egerand benz, 2020), many often simply rely on wordembeddings or language models for perturbationproposal (see §4).
while the latter may be useful toevaluate a system’s robustness to malicious actors,they are less useful for dimension-speciﬁc testing(e.g., reliability when encountering grammaticalvariation).
this is because they often perturb theinput across multiple dimensions at once, whichmay make the resulting adversaries unnatural..hence, in this paper targeted at nlp researchers,practitioners, and policymakers, we make the casefor reliability testing and reformulate adversarialattacks as dimension-speciﬁc, worst-case tests thatcan be used to approximate real-world variation.
we contribute a reliability testing framework —doctor — that translates safety and fairness con-cerns around nlp systems into quantitative tests.
we demonstrate how testing dimensions for doc-tor can be drafted for a speciﬁc use case.
finally,we discuss the policy implications, challenges, anddirections for future research on reliability testing..2 terminology deﬁnitions.
let’s deﬁne key terms to be used in our discussion..nlp system.
the entire text processing pipelinebuilt to solve a speciﬁc task; taking raw text as inputand producing predictions in the form of labels.
1the distribution of adversarial cases or failure proﬁle..(classiﬁcation) or text (generation).
we excluderaw language models from the discussion since itis unclear how performance, and hence worst-caseperformance, should be evaluated.
we do includenlp systems that use language models internally(e.g., bert-based classiﬁers (devlin et al., 2019))..reliability.
deﬁned by ieee (2017) as the “de-gree to which a system, product or component per-forms speciﬁed functions under speciﬁed condi-tions for a speciﬁed period of time”.
we preferthis term over robustness2 to challenge the nlpcommunity’s common framing of inputs from out-side the training distribution as “noisy”.
the notionof reliability requires us to explicitly consider thespeciﬁc, diverse environments (i.e., communities)a system will operate in.
this is crucial to reducingthe nlp’s negative impact on the underrepresented..dimension.
an axis along which variation canoccur in the real world, similar to plank (2016)’svariety space.
a taxonomy of possible dimensionscan be found in table 1 (appendix)..adversarial attack.
a method of perturbing theinput to degrade a target model’s accuracy (good-fellow et al., 2015).
in computer vision, this isachieved by adding adversarial noise to the image,optimized to be maximally damaging to the model.
§4 describes how this is done in the nlp context..stakeholder.
a person who is (in-)directly im-pacted by the nlp system’s predictions..actor.
someone who has inﬂuence over a) thedesign of an nlp system and its reliability testingregime; b) whether the system is deployed; andc) who it can interact with.
within the context ofour discussion, actors are likely to be regulators,experts, and stakeholder advocates..expert.
an actor who has specialized knowl-edge, such as ethicists, linguists, domain experts,social scientists, or nlp practitioners..3 the case for reliability testing in nlp.
the accelerating interest in building nlp-basedproducts that impact many lives has led to ur-gent questions of fairness, safety, and accountabil-ity (hovy and spruit, 2016; bender et al., 2021),.
2the “degree to which a system or component can func-tion correctly in the presence of invalid inputs or stressfulenvironmental conditions” (ieee, 2017)..4154prompting research into algorithmic bias (boluk-basi et al., 2016; blodgett et al., 2020), explainabil-ity (ribeiro et al., 2016; danilevsky et al., 2020),robustness (jia and liang, 2017), etc.
research isalso emerging on best practices for productizingml: from detailed dataset documentation (benderand friedman, 2018; gebru et al., 2018), modeldocumentation for highlighting important but of-ten unreported details such as its training data, in-tended use, and caveats (mitchell et al., 2019), anddocumentation best practices (partnership on ai,2019), to institutional mechanisms such as audit-ing (raji et al., 2020) to enforce accountability andred-teaming (brundage et al., 2020) to address de-veloper blind spots, not to mention studies on theimpact of organizational structures on responsibleai initiatives (rakova et al., 2020)..calls for increased accountability and transparencyare gaining traction among governments (116thu.s. congress, 2019; nist, 2019; european com-mission, 2020; smith, 2020; california state leg-islature, 2020; fda, 2021) and customers increas-ingly cite ethical concerns as a reason for not en-gaging ai service providers (eiu, 2020)..while there has been signiﬁcant discussion aroundbest practices for dataset and model creation, workto ensure nlp systems are evaluated in a man-ner representative of their operational conditionshas only just begun.
initial work in constructingrepresentative tests focuses on enabling develop-ment teams to easily evaluate their models’ lin-guistic capabilities (ribeiro et al., 2020) and ac-curacy on subpopulations and distribution shifts(goel et al., 2021).
however, there is a clear needfor a paradigm that allows experts and stakeholderadvocates to collaboratively develop tests that arerepresentative of the practical and ethical concernsof an nlp system’s target demographic.
we arguethat reliability testing, by reframing the concept ofadversarial attacks, has the potential to ﬁll this gap..3.1 what is reliability testing?.
despite the recent advances in neural architecturesresulting in breakthrough performance on bench-mark datasets, research into adversarial examplesand out-of-distribution generalization has foundml systems to be particularly vulnerable to slightperturbations in the input (goodfellow et al., 2015)and natural distribution shifts (fisch et al., 2019).
while these perturbations are often chosen to max-.
imize model failure, they highlight serious reliabil-ity issues for putting ml models into productionsince they show that these models could fail catas-trophically in naturally noisy, diverse, real-worldenvironments (saria and subbaswamy, 2019).
ad-ditionally, bias can seep into the system at multiplestages of the nlp lifecycle (shah et al., 2020), re-sulting in discrimination against minority groups(o’neil, 2016).
the good news, however, is thatrigorous testing can help to highlight potential is-sues before the systems are deployed..the need for rigorous testing in nlp is reﬂected inacl 2020 giving the best paper award to check-list (ribeiro et al., 2020), which applied the idea ofbehavior testing from software engineering to test-ing nlp systems.
while invaluable as a ﬁrst steptowards the development of comprehensive test-ing methodology, the current implementation ofchecklist may still overestimate the reliability ofnlp systems since the individual test examples arelargely manually constructed.
importantly, with thecomplexity and scale of current models, humanscannot accurately determine a model’s adversarialdistribution (i.e., the examples that cause modelfailure).
consequently, the test examples they con-struct are unlikely to be the worst-case examplesfor the model.
automated assistance is needed..therefore, we propose to perform reliability test-ing, which can be thought of as one componentof behavior testing.
we categorize reliability testsas average-case tests or the worst-case tests.
astheir names suggest, average-case and worst-casetests estimate the expected and lower-bound per-formance, respectively, when the nlp system isexposed to the phenomena modeled by the tests.
average-case tests are conceptually similar to wuet al.
(2021)’s counterfactuals, which is contem-poraneous work, while worst-case tests are mostsimilar to adversarial attacks (§4)..our approach parallels boundary value testing insoftware engineering: in boundary value testing,tests evaluate a program’s ability to handle edgecases using test examples drawn from the extremesof the ranges the program is expected to handle.
similarly, reliability testing aims to quantify thesystem’s reliability under diverse and potentiallyextreme conditions.
this allows teams to performbetter quality control of their nlp systems and in-troduce more nuance into discussions of why andwhen models fail (§5).
finally, we note that reliabil-.
4155ity testing and standards are established practicesin engineering industries (e.g., aerospace (nelson,2003; wilkinson et al., 2016)) and advocate for nlengineering to be at parity with these ﬁelds..3.2 evaluating worst-case performance in a.label-scarce world.
a proposed approach for testing robustness to nat-ural and adverse distribution shifts is to constructtest sets using data from different domains or writ-ing styles (miller et al., 2020; hendrycks et al.,2020), or to use a human vs. model method of con-structing challenge sets (nie et al., 2020; zhanget al., 2019b).
while they are the gold standard,such datasets are expensive to construct,3 makingit infeasible to manually create worst-case test ex-amples for each nlp system being evaluated.
con-sequently, these challenge sets necessarily overesti-mate each system’s worst-case performance whenthe inference distribution differs from the train-ing one.
additionally, due to their crowdsourcednature, these challenge sets inevitably introducedistribution shifts across multiple dimensions atonce, and even their own biases (geva et al., 2019),unless explicitly controlled for.
building individ-ual challenge sets for each dimension would beprohibitively expensive due to combinatorial ex-plosion, even before having to account for conceptdrift (widmer and kubat, 1996).
this couplingcomplicates efforts to design a nuanced and com-prehensive testing regime.
hence, simulating vari-ation in a controlled manner via reliability testscan be a complementary method of evaluating thesystem’s out-of-distribution generalization ability..4 adversarial attacks as reliability tests.
we ﬁrst give a brief introduction to adversarialattacks in nlp before showing how they can beused for reliability testing.
we refer the reader tozhang et al.
(2020b) for a comprehensive survey..existing work on nlp adversarial attacks perturbsthe input at various levels of linguistic analysis:phonology (eger and benz, 2020), orthography(ebrahimi et al., 2018), morphology (tan et al.,2020), lexicon (alzantot et al., 2018; jin et al.,2020), and syntax (iyyer et al., 2018)..early work did not place any constraints on theattacks and merely used the degradation to a tar-.
3dua et al.
(2019) reports a cost of 60k usd for 96k.
question–answer pairs..c ← samplecandidates(x )switch testtype do.
algorithm 1 general reliability testrequire: data distribution dd = {x , y} modeling the di-mension of interest d, nlp system m, source datasetx ∼ x , desired labels y (cid:48) ∼ y, scoring function s.ensure: average- or worst-case examples x (cid:48), result r.1: x (cid:48) ← {∅}, r ← 02: for x, y(cid:48) in x, y (cid:48) do3:4:5:6:7:8:9:10:11:12: end for13: r ← r|x|14: return x (cid:48), r.s ← mean(s(y(cid:48), m(c)))x (cid:48) ← x (cid:48) ∪ ccase worstcasetest.
x(cid:48), s ← arg minxc∈c s(y(cid:48), m(xc))x (cid:48) ← x (cid:48) ∪ {x(cid:48)}.
case averagecasetest.
r ← r + s.get model’s accuracy as the measure of success.
however, this often resulted in the semantics andexpected prediction changing, leading to an over-estimation of the attack’s success.
recent attacksaim to preserve the original input’s semantics.
apopular approach has been to substitute words withtheir synonyms using word embeddings or a lan-guage model as a measure of semantic similarity(alzantot et al., 2018; ribeiro et al., 2018; michelet al., 2019; ren et al., 2019; zhang et al., 2019a;li et al., 2019; jin et al., 2020; garg and ramakr-ishnan, 2020; li et al., 2020a)..focusing on maximally degrading model accuracyoverlooks the key feature of adversarial attacks: theability to ﬁnd the worst-case example for a modelfrom an arbitrary distribution.
many recent attacksperturb the input across multiple dimensions atonce, which may make the result unnatural.
byconstraining our sample perturbations to a distribu-tion modeling a speciﬁc dimension of interest, theperformance on the generated adversaries is a validlower bound performance for that dimension.
saidanother way, adversarial attacks can be reframed asinterpretable reliability tests if we constrain themto meaningful distributions..this is the key element of our approach as detailedin alg.
1. we specify either an average (lines 5–7)or worse case test (lines 8–10), but conditionedon the data distribution d that models a particulardimension of interest d. the resultant reliabilityscore gauges real-world performance and the worst-case variant returns the adversarial examples thatcause worst-case performance.
when invariance toinput variation is expected, y(cid:48) is equivalent to the.
4156source label y. note that by ignoring the average-case test logic and removing d, we recover thegeneral adversarial attack algorithm..however, the key difference between an adversar-ial robustness mindset and a testing one is the lat-ter’s emphasis on identifying ways in which naturalphenomena or ethical concerns can be operational-ized as reliability tests.
this change in perspectiveopens up new avenues for interdisciplinary researchthat will allow researchers and practitioners to havemore nuanced discussions about model reliabilityand can be used to design comprehensive reliabilitytesting regimes.
we describe such a framework forinterdisciplinary collaboration next..5 a framework for reliability testing.
we introduce and then describe our general frame-work, doctor, for testing the reliability of nlpsystems.
doctor comprises six steps:.
1. deﬁne reliability requirements.
2. operationalize dimensions as distributions.
3. construct tests.
4. test system and report results.
5. observe deployed system’s behavior.
6. reﬁne reliability requirements and tests.
deﬁning reliability requirements.
before anytests are constructed, experts and stakeholder advo-cates should work together to understand the demo-graphics and values of the communities the nlpsystem will interact with (friedman and hendry,2019) and the system’s impact on their lives.
thelatter is also known as algorithmic risk assess-ment (ada lovelace institute and datakind uk,2021).
there are three critical questions to address:1) along what dimensions should the model betested?
2) what metrics should be used to mea-sure system performance?
3) what are acceptableperformance thresholds for each dimension?.
question 1 can be further broken down into: a) gen-eral linguistic phenomena, such as alternativespellings or code-mixing; b) task-speciﬁc quirks,e.g., an essay grading system should not use textlength to predict score; c) sensitive attributes, suchas gender, ethnicity, sexual orientation, age, or dis-ability status.
this presents an opportunity for inter-disciplinary expert collaboration: linguists are bestequipped to contribute to discussions around (a),.
domain experts to (b), and ethicists and social sci-entists to (c).
however, we recognize that suchcollaboration may not be feasible for every nlpsystem being tested.
it is more realistic to expectethicists to be involved when applying doctor atthe company and industry levels, and ethics-trainednlp practitioners to answer these questions withinthe development team.
we provide a taxonomy ofpotential dimensions in table 1 (appendix)..since it is likely unfeasible to test every possible di-mension, stakeholder advocates should be involvedto ensure their values and interests are accuratelyrepresented and prioritized (hagerty and rubinov,2019), while experts should ensure the dimensionsidentiﬁed can be feasibly tested.
a similar ap-proach to that of community juries4 may be taken.
we recommend using this question to evaluate thefeasibility of operationalizing potential dimensions:“what is the system’s performance when exposedto variation along dimension d?”.
for example,rather than simply “gender”, a better-deﬁned di-mension would be “gender pronouns”.
with thisunderstanding, experts and policymakers can thencreate a set of reliability requirements, comprisingthe testing dimensions, performance metric(s), andpassing thresholds..next, we recommend using the same metricsfor held-out, average-case, and worst-case perfor-mance for easy comparison.
these often vary fromtask to task and are still a subject of active research(novikova et al., 2017; reiter, 2018; kryscinskiet al., 2019), hence the question of the right met-ric to use is beyond the scope of this paper.
fi-nally, ethicists, in consultation with the other afore-mentioned experts and stakeholders, will determineacceptable thresholds for worst-case performance.
the system under test must perform above saidthresholds when exposed to variation along thosedimensions in order to pass.
for worst-case perfor-mance, we recommend reporting thresholds as rel-ative differences (δ) between the average-case andworst-case performance.
these questions may helpin applying this step and deciding if speciﬁc nlpsolutions should even exist (leins et al., 2020):.
• who will interact with the nlp system, in whatcontext, and using which language varieties?.
• what are the distinguishing features of these va-.
rieties compared to those used for training?.
4docs.microsoft.com/en-us/azure/.../community-jury.
4157• what is the (short- and long-term) impact on thecommunity’s most underrepresented members ifthe system performs more poorly for them?.
we note that our framework is general enough tobe applied at various levels of organization: withinthe development team, within the company (com-pliance team, internal auditor), and within the in-dustry (self-regulation or independent regulator).
however, we expect the exact set of dimensions,metrics and acceptable thresholds deﬁned in step 1to vary depending on the reliability concerns ofthe actors at each level.
for example, independentregulators will be most concerned with establishingminimum safety and fairness standards that all nlpsystems used in their industries must meet, whilecompliance teams may wish to have stricter andmore comprehensive standards for brand reasons.
developers can use doctor to meet the othertwo levels of requirements and understand theirsystem’s behaviour better with targeted testing..operationalizing dimensions.
while the ab-stractness of dimensions allows people who arenot nlp practitioners to participate in drafting theset of reliability requirements, there is no way totest nlp systems using fuzzy concepts.
therefore,every dimension the system is to be tested alongmust be operationalizable as a distribution fromwhich perturbed examples can be sampled in orderfor nlp practitioners to realize them as tests..since average-case tests attempt to estimate a sys-tem’s expected performance in its deployed envi-ronment, the availability of datasets that reﬂectreal-world distributions is paramount to ensure thatthe tests themselves are unbiased.
this is less of anissue for worst-case tests; the tests only needs toknow which perturbations that are possible, but nothow frequently they occur in the real world.
figur-ing out key dimensions for different classes of nlptasks and exploring ways of operationalizing themas reliability tests are also promising directions forfuture research.
such research would help nlppractitioners and policymakers deﬁne reliabilityrequirements that can be feasibly implemented..constructing tests.
next, average- and worst-case tests are constructed (alg.
1).
average-casetests can be data-driven and could take the formof manually curated datasets or model-based per-turbation generation (e.g., polyjuice (wu et al.,2021)), while worst-case tests can be rule-based.
(e.g., morpheus (tan et al., 2020)) or model-based(e.g., bert-attack (li et al., 2020a)).
we recom-mend constructing tests that do not require accessto the nlp model’s parameters (black-box assump-tion); this not only yields more system-agnostictests, but also allows for (some) tests to be createdindependently from the system development team.
if the black-box assumption proves limiting, thecommunity can establish a standard set of itemsan nlp system should export for testing purposes,e.g., network gradients if the system uses a neuralmodel.
regardless of assumption, keeping the reg-ulators’ test implementations separate and hiddenfrom the system developers is critical for stake-holders and regulators to trust the results.
thisseparation also reduces overﬁtting to the test suite..testing systems.
a possible model for test own-ership is to have independently implemented testsat the three levels of organization described above(team, company, industry).
at the developmentteam level, reliability tests can be used to diag-nose weaknesses with the goal of improving thenlp system for a speciﬁc use case and set of targetusers.
compared to unconstrained adversarial ex-amples, contrasting worst-case examples that havebeen constrained along speciﬁc dimensions withnon-worst-case examples will likely yield greaterintuition into the model’s inner workings.
study-ing how modiﬁcations (to the architecture, trainingdata and process) affect the system’s reliability oneach dimension will also give engineers insight intothe factors affecting system reliability.
these testsshould be executed and updated regularly duringdevelopment, according to software engineeringbest practices such as agile (beck et al., 2001)..red teams are company-internal teams tasked withﬁnding security vulnerabilities in their developedsoftware or systems.
brundage et al.
(2020) pro-pose to apply the concept of red teaming to surfaceinﬂaws in an ai system’s safety and security.
companies that maintain multiple nlp systems,we propose employing similar, specialized teamscomposed of nlp experts to build and maintainreliability tests that ensure their nlp systems ad-here to company-level reliability standards.
thesetests will likely be less task-/domain-speciﬁc thanthose developed by engineering teams due to theirwider scope, while the reliability standards maybe created and maintained by compliance teamsor the red teams themselves.
making these stan-.
4158dards available for public scrutiny and ensuringtheir products meet them will enable companiesto build trust with their users.
to ensure all nlpsystems meet the company’s reliability standards,these reliability tests should be executed as a partof regular internal audits (raji et al., 2020), inves-tigative audits after incidents, and before majorreleases (especially if it is the system’s ﬁrst releaseor if it received a major update).
they may also beregularly executed on randomly chosen productionsystems and trigger an alert upon failure..at the independent regulator level, reliability testswould likely be carried out during product certiﬁ-cation (e.g., ansi/iso certiﬁcation) and externalaudits.
these industry-level reliability standardsand tests may be developed in a similar manner tothe company-level ones.
however, we expect themto be more general and less comprehensive thanthe latter, analogous to minimum safety standardssuch as iec 60335-1 (iec, 2020).
naturally, highrisk applications and nlp systems used in regu-lated industries should comply with more stringentrequirements (european commission, 2021)..our proposed framework is also highly compatiblewith the use of model cards (mitchell et al., 2019)for auditing and transparent reporting (raji et al.,2020).
in addition to performance on task-relatedmetrics, model cards surface information and as-sumptions about a machine learning system andtraining process that may not be readily availableotherwise.
when a system has passed all tests andis ready to be deployed, its average- and worst-caseperformance on all tested dimensions can be in-cluded as an extra section on the accompanyingmodel card.
in addition, the perturbed examplesgenerated during testing and their labels (x(cid:48), y(cid:48))can be stored for audit purposes or examined toensure that the tests are performing as expected..observing and reﬁning requirements.
it iscrucial to regularly monitor the systems’ impactpost-launch and add, update, or re-prioritize di-mensions and thresholds accordingly.
monitoringlarge-scale deployments can be done via commu-nity juries, in which stakeholders who will be likelyimpacted (or their advocates) give feedback on theirpain points and raise concerns about potential neg-ative effects.
smaller teams without the resourcesto organize community juries can set up avenues(e.g., online forms) for affected stakeholders to givefeedback, raise concerns, and seek remediation..6 from concerns to dimensions.
we now illustrate how reliability concerns can beconverted into concrete testing dimensions (step 1)by considering the scenario of applying automatedtext scoring to short answers and essays from stu-dents in the multilingual population of singapore.
we study a second scenario in appendix a. au-tomated text scoring (ats) systems are increas-ingly used to grade tests and essays (markoff, 2013;feathers, 2019).
while they can provide instantfeedback and help teachers and test agencies copewith large loads, studies have shown that they oftenexhibit demographic and language biases, such asscoring african- and indian-american males loweron the gre argument task compared to humangraders (bridgeman et al., 2012; ramineni andwilliamson, 2018).
since the results of some testswill affect the futures of the test takers (salaky,2018), the scoring algorithms used must be sufﬁ-ciently reliable.
hence, let us imagine that singa-pore’s education ministry has decided to create astandard set of reliability requirements that all atssystems used in education must adhere to..linguistic landscape.
a mix of language vari-eties are used in singapore: a prestige english vari-ety, a colloquial english variety, three other ofﬁciallanguages (chinese, malay, and tamil), and a largenumber of other languages.
english is the linguafranca, with ﬂuency in the prestige variety corre-lating with socioeconomic status (vaish and tan,2008).
a signiﬁcant portion of the population doesnot speak english at home.
subjects other thanlanguages are taught in english..stakeholder impact.
the key stakeholders af-fected by ats systems would be students inschools and universities.
the consequences oflower scores could be life-altering for the stu-dent who is unable to enroll in the major of theirchoice.
at the population level, biases in an atssystem trained on normally sampled data wouldunfairly discriminate against already underrepre-sented groups.
additionally, biases against dis-ﬂuent or ungrammatical text when they are notthe tested attributes would result in discriminationagainst students with a lower socioeconomic statusor for whom english is a second language..finally, nlp systems have also been known to beoverly sensitive to alternative spellings (belinkovand bisk, 2018).
when used to score subject tests,this could result in the ats system unfairly penaliz-.
4159ing dyslexic students (coleman et al., 2009).
sinceeducation is often credited with enabling socialmobility,5 unfair grading may perpetuate systemicdiscrimination and increase social inequality..dimension.
we can generally categorize writtentests into those that test for content correctness(e.g., essay questions in a history test), and thosethat test for language skills (e.g., proper use ofgrammar).
while there are tests that simultane-ously assess both aspects, modern ats systemsoften grade them separately (ke and ng, 2019).
we treat each aspect as a separate test here..when grading students on content correctness, wewould expect the ats system to ignore linguisticvariation and sensitive attributes as long as they donot affect the answer’s validity.
hence, we wouldexpect variation in these dimensions to have no ef-fect on scores: answer length, language/vocabularysimplicity, alternative spellings/misspellings ofnon-keywords, grammatical variation, syntacticvariation (especially those resembling transfer froma ﬁrst language), and proxies for sensitive attributes.
on the other hand, the system should be able todifferentiate proper answers from those aimed atgaming the test (chin, 2020; ding et al., 2020)..when grading students on language skills, however,we would expect ats systems to be only sensitiveto the relevant skill.
for example, when assessinggrammar use, we would expect the system to besensitive to grammatical errors (from the perspec-tive of the language variety the student is expectedto use), but not to the other dimensions mentionedabove (e.g., misspellings)..actors.
relevant experts include teachers of thesubjects where the ats systems will be deployed,linguists, and computer scientists.
the stakeholders(students) may be represented by student unions (atthe university level) or focus groups comprising arepresentative sample of the student population..7.implications for policy.
there is a mounting effort to increase accountabil-ity and transparency around the development anduse of nlp systems to prevent them from ampli-fying societal biases.
doctor is highly comple-mentary to the model card approach increasinglyadopted6 to surface oft hidden details about nlp.
5www.encyclopedia.com/.../education-and-mobility6huggingface.co/models;github.com/ivylee/model-cards-and-datasheets;.
models: developers simply need to list the testeddimensions, metrics, and score on each dimensionin the model card.
crucially, reliability tests canbe used to highlight fairness issues in nlp sys-tems by including sensitive attributes for the targetpopulation, but it is paramount these requirementsreﬂect local concerns rather than any prescriptivistperspective (sambasivan et al., 2021)..at the same time, the ability to conduct quantitative,targeted reliability testing along speciﬁable dimen-sions paves the way for reliability standards to beestablished, with varying levels of stringency andrigor for different use cases and industries.
we envi-sion minimum safety and fairness standards beingestablished for applications that are non-sensitive,not safety-critical, and used in unregulated indus-tries, analogous to standards for household appli-ances.
naturally, applications at greater risks (liet al., 2020b) of causing harm upon failure shouldbe held to stricter standards.
policymakers are start-ing to propose and implement regulations to en-force transparency and accountability in the use ofai systems.
for example, the european union’sgeneral data protection regulation grants data sub-jects the right to obtain “meaningful informationabout the logic involved” in automated decisionsystems (eu, 2016).
the eu is developing ai-speciﬁc regulation (european commission, 2020):e.g., requiring developers of high-risk ai systemsto report their “capabilities and limitations, ... [and]the conditions under which they can be expected tofunction as intended”.
in the u.s., a proposed billof the state of washington will require public agen-cies to report “any potential impacts of the auto-mated decision system on civil rights and libertiesand potential disparate impacts on marginalizedcommunities” before using automated decision sys-tems (washington state legislature, 2021)..one may note that language in the proposed regula-tion is intentionally vague.
there are many ways tomeasure bias and fairness, depending on the typeof model, context of use, and goal of the system.
today, companies developing ai systems employthe deﬁnitions they believe most reasonable (orperhaps easiest to implement), but regulation willneed to be more speciﬁc for there to be meaningfulcompliance.
doctor’s requirement to explicitlydeﬁne speciﬁc dimensions instead of a vague no-tion of reliability will help policymakers in this.
blog.einstein.ai/model-cards-for-ai-model-transparency.
4160regard, and can inform the ongoing development ofnational (nist, 2019) and international standards7..while external algorithm audits are becoming pop-ular, testing remains a challenge since companieswishing to protect their intellectual property maybe resistant to sharing their code (johnson, 2021),and implementing custom tests for each systemis unscalable.
our approach to reliability testingoffers a potential solution to this conundrum bytreating nlp systems as black boxes.
if reliabil-ity tests become a legal requirement, regulatoryauthorities will be able to mandate independentlyconducted reliability tests for transparency.
suchstandards, combined with certiﬁcation programs(e.g., ieee’s ethics certiﬁcation program for au-tonomous and intelligent systems8), will furtherincentivize the development of responsible nlp, asthe companies purchasing nlp systems will insiston certiﬁed systems to protect them from both le-gal and brand risk.
to avoid confusion, we expectcertiﬁcation to occur for individual nlp systems(e.g., an end-to-end question answering system forcustomer enquiries), rather than for general pur-pose language models that will be further trainedto perform some speciﬁc nlp task.
while con-crete standards and certiﬁcation programs that canserve this purpose do not yet exist, we believe thatthey eventually will and hope our paper will informtheir development.
this multi-pronged approachcan help to mitigate nlp’s potential harms whileincreasing public trust in language technology..8 challenges and future directions.
while doctor is a useful starting point to im-plement reliability testing for nlp systems, weobserve key challenges to its widespread adoption.
first, identifying and prioritizing the dimensionsthat can attest a system’s reliability and fairness.
the former is relatively straightforward and canbe achieved via collaboration with experts (e.g., aspart of the u.s. nist’s future ai standards (nist,2019)).
the latter, however, is a question of valuesand power (noble, 2018; mohamed et al., 2020;leins et al., 2020), and should be addressed via acode of ethics and ensuring that all stakeholdersare adequately represented at the decision table..second, our proposed method of reliability testingmay suffer from similar issues plaguing automatic.
7ethicsstandards.org/p70008standards.ieee.org/industry-connections/ecpais.html.
evaluation metrics for natural language generation(novikova et al., 2017; reiter, 2018; kryscinskiet al., 2019): due to the tests’ synthetic nature theymay not fully capture the nuances of reality.
forexample, if a test’s objective were to test an nlpsystem’s reliability when interacting with africanamerican english (aae) speakers, would it bepossible to guarantee (in practice) that all gener-ated examples fall within the distribution of aaetexts?
potential research directions would be todesign adversary generation techniques that canoffer such guarantees or incorporate human feed-back (nguyen et al., 2017; kreutzer et al., 2018;stiennon et al., 2020)..9 conclusion.
once language technologies leave the lab and startimpacting real lives, concerns around safety, fair-ness, and accountability cease to be thought ex-periments.
while it is clear that nlp can havea positive impact on our lives, from typing auto-completion to revitalizing endangered languages(zhang et al., 2020a), it also has the potential toperpetuate harmful stereotypes (bolukbasi et al.,2016; sap et al., 2019), perform disproportionatelypoorly for underrepresented groups (hern, 2017;bridgeman et al., 2012), and even erase alreadymarginalized communities (bender et al., 2021)..trust in our tools stems from an assurance thatstakeholders will remain unharmed, even in theworst-case scenario.
in many mature industries,this takes the form of reliability standards.
how-ever, for standards to be enacted and enforced, wemust ﬁrst operationalize “reliability”.
hence, weargue for the need for reliability testing (especiallyworst-case testing) in nlp by contextualizing itamong existing work on promoting accountabilityand improving generalization beyond the trainingdistribution.
next, we showed how adversarial at-tacks can be reframed as worst-case tests.
finally,we proposed a possible paradigm, doctor, forhow reliability concerns can be realized as quantita-tive tests, and discussed how this framework can beused at different levels of organization or industry..acknowledgements.
samson is supported by salesforce and singapore’seconomic development board under the industrialpostgraduate programme.
araz is supported by thenus centre for trusted internet and communitythrough project ctic-rp-20-02..4161broader impact.
much like how we expect to not be exposed toharmful electric shocks when using electrical ap-pliances, we should expect some minimum levelsof safety and fairness for the nlp systems we in-teract with in our everyday lives.
as mentioned in§1, §3, and §7, standards and regulations for aisystems are in the process of being developed forthis purpose, especially for applications deemed“high-risk”, e.g., healthcare (european commis-sion, 2020).
reliability testing, and our proposedframework, is one way to approach the problem ofenacting enforceable standards and regulations..however, the ﬂip side of heavily regulating ev-ery single application of nlp is that it may slowdown innovation.
therefore, it is important thatthe level of regulation for a particular applicationis proportionate to its potential for harm (datenethik kommission, 2019).
our framework can beadapted to different levels of risk by scaling downthe implementation of some steps (e.g., the methodand depth in which stakeholder consultation hap-pens or the comprehensiveness of the set of testingdimensions) for low-risk applications..finally, it is important to ensure that any tests, stan-dards, or regulations developed adequately repre-sents the needs of the most vulnerable stakeholders,instead of constructing them in a prescriptivist man-ner (hagerty and rubinov, 2019).
hence, doc-tor places a strong emphasis on involving stake-holder advocates and analyzing the impact of anapplication of nlp on the target community..references.
116th u.s. congress.
2019. algorithmic accountabil-ity act of 2019..ada lovelace institute and datakind uk.
2021. ex-amining the black box: tools for assessing algorithmicsystems.
technical report..moustafa alzantot, yash sharma, ahmed elgohary,bo-jhang ho, mani srivastava, and kai-wei chang.
2018. generating natural language adversarial exam-ples.
in proceedings of the 2018 conference on empir-ical methods in natural language processing, pages2890–2896, brussels, belgium.
association for com-putational linguistics..amnesty international.
2018. toxic twitter - triggersof violence and abuse against women on twitter..kent beck, mike beedle, arie van bennekum, alistaircockburn, ward cunningham, martin fowler, james.
grenning, jim highsmith, andrew hunt, ron jeffries,jon kern, brian marick, robert c. martin, steve mel-lor, ken schwaber, jeff sutherland, and dave thomas.
2001. manifesto for agile software development..yonatan belinkov and yonatan bisk.
2018. syntheticand natural noise both break neural machine translation.
in 6th international conference on learning represen-tations, vancouver, bc, canada..emily m bender and batya friedman.
2018. datastatements for natural language processing: towardmitigating system bias and enabling better science.
transactions of the association for computational lin-guistics, 6:587–604..emily m. bender, timnit gebru, angelina mcmillan-major, and shmargaret shmitchell.
2021. on the dan-gers of stochastic parrots: can language models be tooin proceedings of the conference on fairness,big?
accountability, and transparency..su lin blodgett, solon barocas, hal daumé iii, andhanna wallach.
2020.language (technology) ispower: a critical survey of “bias” in nlp.
in proceed-ings of the 58th annual meeting of the association forcomputational linguistics, pages 5454–5476, online.
association for computational linguistics..tolga bolukbasi, kai-wei chang, james y zou,venkatesh saligrama, and adam t kalai.
2016. manis to computer programmer as woman is to home-maker?
debiasing word embeddings.
in d. d. lee,m. sugiyama, u. v. luxburg, i. guyon, and r. gar-nett, editors, advances in neural information process-ing systems 29, pages 4349–4357.
curran associates,inc..brent bridgeman, catherine trapani, and yigal attali.
2012. comparison of human and machine scoring ofessays: differences by gender, ethnicity, and country.
applied measurement in education, 25(1):27–40..tom b. brown, benjamin mann, nick ryder,melanie subbiah, jared kaplan, prafulla dhariwal,arvind neelakantan, pranav shyam, girish sastry,amanda askell, sandhini agarwal, ariel herbert-voss, gretchen krueger, tom henighan, rewonchild, aditya ramesh, daniel m. ziegler, jeffrey wu,clemens winter, christopher hesse, mark chen, ericsigler, mateusz litwin, scott gray, benjamin chess,jack clark, christopher berner, sam mccandlish, alecradford, ilya sutskever, and dario amodei.
2020. lan-in advances inguage models are few-shot learners.
neural information processing systems 33..miles brundage, shahar avin, jasmine wang, haydnbelﬁeld, gretchen krueger, gillian hadﬁeld, heidykhlaaf, jingying yang, helen toner, ruth fong, et al.
2020. toward trustworthy ai development: mecha-nisms for supporting veriﬁable claims.
arxiv preprintarxiv:2004.07213..california state legislature.
2020. california privacyrights act..minhao cheng, wei wei, and cho-jui hsieh.
2019..4162evaluating and enhancing the robustness of dialoguesystems: a case study on a negotiation agent.
in pro-ceedings of the 2019 conference of the north ameri-can chapter of the association for computational lin-guistics: human language technologies, volume 1(long and short papers), pages 3325–3335, minneapo-lis, minnesota.
association for computational linguis-tics..monica chin.
2020. these students ﬁgured out theirtests were graded by ai — and the easy way to cheat.
the verge..katie cohen, fredrik johansson, lisa kaati, andjonas clausen mork.
2014. detecting linguistic mark-ers for radical violence in social media.
terrorism andpolitical violence, 26(1):246–256..chris coleman, noël gregg, lisa mclain, andleslie w bellair.
2009. a comparison of spellingperformance across young adults with and with-out dyslexia.
assessment for effective intervention,34(2):94–105..kate crawford.
2017. the trouble with bias (keynote).
advances in neural information processing systems30..marina danilevsky, kun qian, ranit aharonov, yanniskatsis, ban kawas, and prithviraj sen. 2020. a surveyof the state of explainable ai for natural language pro-in proceedings of the 1st conference of thecessing.
asia-paciﬁc chapter of the association for computa-tional linguistics and the 10th international joint con-ference on natural language processing, pages 447–459, suzhou, china.
association for computationallinguistics..daten ethik kommission.
2019. opinion of the dataethics commission.
technical report, data ethics com-mission of the federal government (germany)..thomas davidson, debasmita bhattacharya, and ing-mar weber.
2019. racial bias in hate speech and abu-sive language detection datasets.
in proceedings of thethird workshop on abusive language online, pages25–35, florence, italy.
association for computationallinguistics..jacob devlin, ming-wei chang, kenton lee, andkristina toutanova.
2019.bert: pre-training ofdeep bidirectional transformers for language under-standing.
in proceedings of the 2019 conference of thenorth american chapter of the association for com-putational linguistics: human language technolo-gies, volume 1 (long and short papers), pages 4171–4186, minneapolis, minnesota.
association for com-putational linguistics..yuning ding, brian riordan, andrea horbach, aoifecahill, and torsten zesch.
2020. don’t take “nswvt-nvakgxpm” for an answer –the surprising vulnerabil-ity of automatic content scoring systems to adversarialin proceedings of the 28th international con-input.
ference on computational linguistics, pages 882–892,barcelona, spain (online).
international committee oncomputational linguistics..dheeru dua, yizhong wang, pradeep dasigi, gabrielstanovsky, sameer singh, and matt gardner.
2019.drop: a reading comprehension benchmark requiringdiscrete reasoning over paragraphs.
in proceedings ofthe 2019 conference of the north american chapter ofthe association for computational linguistics: humanlanguage technologies, volume 1 (long and short pa-pers), pages 2368–2378, minneapolis, minnesota.
as-sociation for computational linguistics..michael dunn.
2014. gender determined dialect vari-in the expression of gender, pages 39–68.
deation.
gruyter..javid ebrahimi, anyi rao, daniel lowd, and dejingdou.
2018. hotflip: white-box adversarial examplesfor text classiﬁcation.
in proceedings of the 56th an-nual meeting of the association for computational lin-guistics (volume 2: short papers), pages 31–36, mel-bourne, australia.
association for computational lin-guistics..steffen eger and yannik benz.
2020. from hero tozéroe: a benchmark of low-level adversarial attacks.
in proceedings of the 1st conference of the asia-paciﬁcchapter of the association for computational linguis-tics and the 10th international joint conference onnatural language processing, pages 786–803, suzhou,china.
association for computational linguistics..eiu.
2020. staying ahead of the curve: the busi-ness case for responsible ai.
technical report, theeconomist intelligence unit..eu.
2016. general data protection regulation..european commission.
2020. on artiﬁcial intelligence- a european approach to excellence and trust.
techni-cal report, european commission..european commission.
2021. proposal for a regulationlaying down harmonised rules on artiﬁcial intelligence(artiﬁcial intelligence act).
technical report, europeancommission..fda.
2021. artiﬁcial intelligence/machine learning(ai/ml)-based software as a medical device (samd) ac-tion plan.
technical report, u.s. food & drug admin-istration..todd feathers.
2019. flawed algorithms are gradingmillions of students’ essays.
vice..adam fisch, alon talmor, robin jia, minjoon seo, eu-nsol choi, and danqi chen.
2019. mrqa 2019 sharedtask: evaluating generalization in reading comprehen-sion.
in proceedings of the 2nd workshop on machinereading for question answering, pages 1–13, hongkong, china.
association for computational linguis-tics..batya friedman and david g hendry.
2019. value sen-sitive design: shaping technology with moral imagi-nation.
mit press..bharath ganesh.
2018. the ungovernability of digi-tal hate culture.
columbia journal of international af-fairs..4163matt gardner, yoav artzi, victoria basmov, jonathanberant, ben bogin, sihao chen, pradeep dasigi,dheeru dua, yanai elazar, ananth gottumukkala,nitish gupta, hannaneh hajishirzi, gabriel ilharco,daniel khashabi, kevin lin, jiangming liu, nelson f.liu, phoebe mulcaire, qiang ning, sameer singh,noah a. smith, sanjay subramanian, reut tsarfaty,eric wallace, ally zhang, and ben zhou.
2020. eval-uating models’ local decision boundaries via contrastsets.
in findings of the association for computationallinguistics: emnlp 2020, pages 1307–1323, online.
association for computational linguistics..siddhant garg and goutham ramakrishnan.
2020.bae: bert-based adversarial examples for text clas-siﬁcation.
in proceedings of the 2020 conference onempirical methods in natural language processing,online.
association for computational linguistics..timnit gebru, jamie morgenstern, briana vecchione,jennifer wortman vaughan, hanna wallach, haldaumé iii, and kate crawford.
2018. datasheets fordatasets.
arxiv preprint arxiv:1803.09010..mor geva, yoav goldberg, and jonathan berant.
2019.are we modeling the task or the annotator?
an investi-gation of annotator bias in natural language understand-in proceedings of the 2019 conferenceing datasets.
on empirical methods in natural language processingand the 9th international joint conference on naturallanguage processing (emnlp-ijcnlp), pages 1161–1166, hong kong, china.
association for computa-tional linguistics..karan goel, nazneen rajani, jesse vig, samson tan,jason wu, stephan zheng, caiming xiong annd mo-hit bansal, and christopher ré.
2021. robustnessgym: unifying the nlp evaluation landscape.
arxivpreprint arxiv:2101.04840..ian j. goodfellow, jonathon shlens, and christianszegedy.
2015. explaining and harnessing adversarialexamples.
in 3rd international conference on learn-ing representations, san diego, california..alexa hagerty and igor rubinov.
2019. global aiethics: a review of the social impacts and ethicalimplications of artiﬁcial intelligence.
arxiv preprintarxiv:1907.07892..hany hassan, anthony aue, chang chen, vishalchowdhary, jonathan clark, christian federmann,xuedong huang, marcin junczys-dowmunt, williamlewis, mu li, et al.
2018. achieving human parity onautomatic chinese to english news translation.
arxivpreprint arxiv:1803.05567..dan hendrycks, xiaoyuan liu, eric wallace, adamdziedzic, rishabh krishnan, and dawn song.
2020.pretrained transformers improve out-of-distribution ro-in proceedings of the 58th annual meet-bustness.
ing of the association for computational linguistics,pages 2744–2751, online.
association for computa-tional linguistics..alex hern.
2017. facebook translates ‘good morning’into ‘attack them’, leading to arrest.
the guardian..dirk hovy, federico bianchi, and tommaso fornaciari.
2020.
“you sound just like your father” commercialmachine translation systems include stylistic biases.
inproceedings of the 58th annual meeting of the associ-ation for computational linguistics, pages 1686–1690,online.
association for computational linguistics..dirk hovy and shannon l spruit.
2016. the social im-pact of natural language processing.
in proceedings ofthe 54th annual meeting of the association for com-putational linguistics (volume 2: short papers), pages591–598..ben hutchinson, vinodkumar prabhakaran, emilydenton, kellie webster, yu zhong, and stephen de-nuyl.
2020. social biases in nlp models as barriers forin proceedings of the 58thpersons with disabilities.
annual meeting of the association for computationallinguistics, pages 5491–5501, online.
association forcomputational linguistics..iec.
2020. household and similar electrical appliances– safety – part 1: general requirements.
iec 60335-1:2020..stan-ieee.
2017.dard - systems and software engineering–vocabulary.
iso/iec/ieee 24765:2017(e), pages 1–541..iso/iec/ieee international.
mohit iyyer, john wieting, kevin gimpel, and lukezettlemoyer.
2018. adversarial example generationwith syntactically controlled paraphrase networks.
inproceedings of the 2018 conference of the north amer-ican chapter of the association for computationallinguistics: human language technologies, volume1 (long papers), pages 1875–1885, new orleans,louisiana.
association for computational linguistics..robin jia and percy liang.
2017. adversarial ex-amples for evaluating reading comprehension systems.
in proceedings of the 2017 conference on empiricalmethods in natural language processing, pages 2021–2031, copenhagen, denmark.
association for compu-tational linguistics..di jin, zhijing jin, joey tianyi zhou, and peterszolovits.
2020.is bert really robust?
a strongbaseline for natural language attack on text classiﬁ-in the thirty-fourth aaaication and entailment.
conference on artiﬁcial intelligence, aaai 2020, thethirty-second innovative applications of artiﬁcial in-telligence conference, iaai 2020, the tenth aaai sym-posium on educational advances in artiﬁcial intelli-gence, eaai 2020, new york, ny, usa, february 7-12,2020, pages 8018–8025.
aaai press..khari johnson.
2021. what algorithm auditing startupsneed to succeed.
venturebeat..divyansh kaushik, eduard hovy, and zachary lipton.
2019. learning the difference that makes a differ-in inter-ence with counterfactually-augmented data.
national conference on learning representations..zixuan ke and vincent ng.
2019. automated essayin interna-scoring: a survey of the state of the art.
tional joint conference on artiﬁcial intelligence.
inter-.
4164national joint conferences on artiﬁcial intelligence or-ganization..daniel khashabi, tushar khot, and ashish sabharwal.
2020. more bang for your buck: natural perturba-tion for robust question answering.
in proceedings ofthe 2020 conference on empirical methods in naturallanguage processing (emnlp), pages 163–170, on-line.
association for computational linguistics..kate klonick.
2018. the new governors: the people,rules, and processes governing online speech.
harvardlaw review, 131(6):1598..julia kreutzer, shahram khadivi, evgeny matusov,and stefan riezler.
2018. can neural machine transla-tion be improved with user feedback?
in proceedingsof the 2018 conference of the north american chapterof the association for computational linguistics: hu-man language technologies, volume 3 (industry pa-pers), pages 92–105..wojciech kryscinski, nitish shirish keskar, bryan mc-cann, caiming xiong, and richard socher.
2019. neu-ral text summarization: a critical evaluation.
in pro-ceedings of the 2019 conference on empirical methodsin natural language processing and the 9th interna-tional joint conference on natural language process-ing (emnlp-ijcnlp), pages 540–551, hong kong,china.
association for computational linguistics..zachary laub.
2019. hate speech on social media:global comparisons.
council on foreign relations..kobi leins, jey han lau, and timothy baldwin.
2020.give me convenience and give her death: who shoulddecide what uses of nlp are appropriate, and on whatin proceedings of the 58th annual meet-basis?
ing of the association for computational linguistics,pages 2908–2913, online.
association for computa-tional linguistics..jinfeng li, shouling ji, tianyu du, bo li, and tingwang.
2019. textbugger: generating adversarial textin 26th annual net-against real-world applications.
work and distributed system security symposium..linyang li, ruotian ma, qipeng guo, xiangyang xue,and xipeng qiu.
2020a.
bert-attack: adversarial at-in proceedings oftack against bert using bert.
the 2020 conference on empirical methods in naturallanguage processing, online.
association for compu-tational linguistics..yanwei li, araz taeihagh, martin de jong, and an-dreas klinke.
2020b.
toward a commonly shared pub-lic policy perspective for analyzing risk coping strate-gies.
risk analysis..john markoff.
2013. essay-grading software offersprofessors a break.
the new york times..alexandria marsters.
2019. when hate speech leadsto hateful actions: a corpus and discourse ana-lytic approach to linguistic threat assessment of hatespeech.
ph.d. thesis, georgetown university, wash-ington, d.c..paul michel, xian li, graham neubig, and juan pino.
2019. on evaluation of adversarial perturbations forin proceedings of thesequence-to-sequence models.
2019 conference of the north american chapter ofthe association for computational linguistics: humanlanguage technologies, volume 1 (long and short pa-pers), pages 3103–3114, minneapolis, minnesota.
as-sociation for computational linguistics..john miller, karl krauth, benjamin recht, and lud-wig schmidt.
2020. the effect of natural distributionshift on question answering models.
arxiv preprintarxiv:2004.14444..margaret mitchell, simone wu, andrew zaldivar,parker barnes, lucy vasserman, ben hutchinson,elena spitzer, inioluwa deborah raji, and timnit ge-bru.
2019. model cards for model reporting.
in pro-ceedings of the conference on fairness, accountability,and transparency, pages 220–229..shakir mohamed, marie-therese png, and williamisaac.
2020. decolonial ai: decolonial theory as so-ciotechnical foresight in artiﬁcial intelligence.
philoso-phy & technology, 33(4):659–684..stacy nelson.
2003. certiﬁcation processes for safety-critical and mission-critical aerospace software.
tech-nical report, nasa technical reports server..khanh nguyen, hal daumé iii, and jordan boyd-graber.
2017. reinforcement learning for bandit neu-ral machine translation with simulated human feedback.
in proceedings of the 2017 conference on empiricalmethods in natural language processing, pages 1464–1474..yixin nie, adina williams, emily dinan, mohitbansal, jason weston, and douwe kiela.
2020. adver-sarial nli: a new benchmark for natural language un-derstanding.
in proceedings of the 58th annual meet-ing of the association for computational linguistics,pages 4885–4901, online.
association for computa-tional linguistics..nist.
2019. u.s. leadership in ai: a plan for federalengagement in developing technical standards and re-lated tools.
technical report, national institute of stan-dards and technology..saﬁya umoja noble.
2018. algorithms of oppression:how search engines reinforce racism.
nyu press..jekaterina novikova, ondˇrej dušek, amanda cer-cas curry, and verena rieser.
2017. why we needin proceedings ofnew evaluation metrics for nlg.
the 2017 conference on empirical methods in naturallanguage processing, pages 2241–2252, copenhagen,denmark.
association for computational linguistics..cathy o’neil.
2016. weapons of math destruc-tion: how big data increases inequality and threatensdemocracy.
crown..partnership on ai.
2019. about ml.
technical report,partnership on ai..4165barbara plank.
2016. what to do about non-standard(or non-canonical) language in nlp.
proceedings ofthe 13th conference on natural language processing(konvens 2016)..megha rajagopalan, lam thuy vo, and aung naingsoe.
2018. how facebook failed the rohingya inmyanmar.
buzzfeed news..inioluwa deborah raji, andrew smart, rebecca nwhite, margaret mitchell, timnit gebru, ben hutchin-son, jamila smith-loud, daniel theron, and parkerbarnes.
2020. closing the ai accountability gap:deﬁning an end-to-end framework for internal algorith-mic auditing.
in proceedings of the 2020 conferenceon fairness, accountability, and transparency, pages33–44..bogdana rakova, jingying yang, henriette cramer,and rumman chowdhury.
2020. where responsibleai meets reality: practitioner perspectives on enablersfor shifting organizational practices.
arxiv preprintarxiv:2006.12358..chaitanya ramineni and david williamson.
2018. un-derstanding mean score differences between the e-rater® automated scoring engine and humans for de-mographically based groups in the gre® general test.
ets research report series, 2018(1):1–31..ehud reiter.
2018. a structured review of the validityof bleu.
computational linguistics, 44(3):393–401..shuhuai ren, yihe deng, kun he, and wanxiang che.
2019. generating natural language adversarial exam-inples through probability weighted word saliency.
proceedings of the 57th annual meeting of the associa-tion for computational linguistics, pages 1085–1097..marco tulio ribeiro, sameer singh, and carlosguestrin.
2016.
"why should i trust you?
": explain-in proceedingsing the predictions of any classiﬁer.
of the 22nd acm sigkdd international conferenceon knowledge discovery and data mining, kdd ’16,page 1135–1144, new york, ny, usa.
association forcomputing machinery..marco tulio ribeiro, sameer singh, and carlosguestrin.
2018. semantically equivalent adversarialin proceedings ofrules for debugging nlp models.
the 56th annual meeting of the association for com-putational linguistics (volume 1: long papers), pages856–865, melbourne, australia.
association for com-putational linguistics..marco tulio ribeiro, tongshuang wu, carlos guestrin,and sameer singh.
2020. beyond accuracy: behav-in pro-ioral testing of nlp models with checklist.
ceedings of the 58th annual meeting of the associationfor computational linguistics, pages 4902–4912, on-line.
association for computational linguistics..anne-laure rousseau, clément baudelaire, and kevinriera.
2020. doctor gpt-3: hype or reality?
nablatechnologies blog..kristin salaky.
2018. what standardized tests look likein 10 places around the world.
insider..nithya sambasivan, erin arnesen, ben hutchinson,tulsee doshi, and vinodkumar prabhakaran.
2021. re-imagining algorithmic fairness in india and beyond.
inproceedings of the 2021 conference on fairness, ac-countability, and transparency..maarten sap, dallas card, saadia gabriel, yejin choi,and noah a. smith.
2019. the risk of racial bias inhate speech detection.
in proceedings of the 57th an-nual meeting of the association for computational lin-guistics, pages 1668–1678, florence, italy.
associationfor computational linguistics..suchi saria and adarsh subbaswamy.
2019. safe andreliable machine learning (tutorial).
acm conferenceon fairness, accountability, and transparency..deven santosh shah, h. andrew schwartz, and dirkhovy.
2020.predictive biases in natural languageprocessing models: a conceptual framework andin proceedings of the 58th annual meet-overview.
ing of the association for computational linguistics,pages 5248–5264, online.
association for computa-tional linguistics..andrew smith.
2020. using artiﬁcial intelligence andalgorithms..anders søgaard, sebastian ebert, jasmijn bastings,and katja filippova.
2021. we need to talk about ran-dom splits.
in proceedings of the 16th conference ofthe european chapter of the association for compu-tational linguistics: main volume, pages 1823–1832,online.
association for computational linguistics..nisan stiennon, long ouyang, jeff wu, daniel mziegler, ryan lowe, chelsea voss, alec radford,dario amodei, and paul christiano.
2020. learningto summarize from human feedback.
arxiv preprintarxiv:2009.01325..samson tan and shaﬁq joty.
2021. code-mixing onsesame street: dawn of the adversarial polyglots.
inproceedings of the 2021 conference of the north amer-ican chapter of the association for computational lin-guistics: human language technologies, online.
as-sociation for computational linguistics..samson tan, shaﬁq joty, min-yen kan, and richardsocher.
2020. it’s morphin’ time!
combating linguis-tic discrimination with inﬂectional perturbations.
inproceedings of the 58th annual meeting of the associ-ation for computational linguistics, pages 2920–2935,online.
association for computational linguistics..deborah tannen.
1991. you just don’t understand:women and men in conversation.
ballantine booksnew york..deborah tannen et al.
2005. conversational style: an-alyzing talk among friends.
oxford university press..viniti vaish and teck kiang tan.
2008. language andsocial class: linguistic capital in singapore.
in annualmeeting of the american educational research associ-ation.
american educational research association..4166claudia wagner, david garcia, mohsen jadidi, andmarkus strohmaier.
2015. it’s a man’s wikipedia?
as-sessing gender inequality in an online encyclopedia.
inproceedings of the international aaai conference onweb and social media, volume 9..wei emma zhang, quan z. sheng, ahoud alhazmi,and chenliang li.
2020b.
adversarial attacks on deep-learning models in natural language processing: a sur-vey.
acm transactions on intelligent systems andtechnology, 11(3)..yuan zhang, jason baldridge, and luheng he.
2019b.
paws: paraphrase adversaries from word scrambling.
in proceedings of the 2019 conference of the northamerican chapter of the association for computa-tional linguistics: human language technologies,volume 1 (long and short papers), pages 1298–1308,minneapolis, minnesota.
association for computa-tional linguistics..lily wakeﬁeld.
2020. queer people are being forcedoff social media by trolling and online abuse, searinglyobvious report conﬁrms.
pinknews..eric wallace, shi feng, nikhil kandpal, matt gardner,and sameer singh.
2019. universal adversarial triggersin proceedings offor attacking and analyzing nlp.
the 2019 conference on empirical methods in naturallanguage processing and the 9th international jointconference on natural language processing (emnlp-ijcnlp), pages 2153–2162, hong kong, china.
asso-ciation for computational linguistics..zeerak waseem, smarika lulz, joachim bingel, and is-abelle augenstein.
2021. disembodied machine learn-arxiving: on the illusion of objectivity in nlp.
preprint arxiv:2101.11974..washington state legislature.
2021. senate bill sb5116..gerhard widmer and miroslav kubat.
1996. learningin the presence of concept drift and hidden contexts.
machine learning, 23:69–101..chris wilkinson, jonathan lynch, raj bharadwaj, andkurt woodham.
2016. veriﬁcation of adaptive sys-tems.
technical report, federal aviation administra-tion william j. hughes technical center..tongshuang wu, marco tulio ribeiro, jeffrey heer,polyjuice: automated,and daniel s weld.
2021.arxivgeneral-purpose counterfactual generation.
preprint arxiv:2101.00288..adams wei yu, david dohan, quoc le, thang luong,rui zhao, and kai chen.
2018. fast and accurate read-ing comprehension by combining self-attention andconvolution.
in international conference on learningrepresentations..yuan zang, fanchao qi, chenghao yang, zhiyuan liu,meng zhang, qun liu, and maosong sun.
2020. word-level textual adversarial attacking as combinatorial op-timization.
in proceedings of the 58th annual meetingof the association for computational linguistics, pages6066–6080..huangzhao zhang, hao zhou, ning miao, and lei li.
2019a.
generating ﬂuent adversarial examples for nat-in proceedings of the 57th annualural languages.
meeting of the association for computational linguis-tics, pages 5564–5569, florence, italy.
association forcomputational linguistics..shiyue zhang, benjamin frey, and mohit bansal.
2020a.
chren: cherokee-english machine translationfor endangered language revitalization.
in proceedingsof the 2020 conference on empirical methods in nat-ural language processing (emnlp), pages 577–595,online.
association for computational linguistics..4167appendix.
a testing dimensions: detecting violent.
content on social media.
in this second case study, we apply doctor formeasuring the reliability of a violent content de-tection system for english social media posts.
al-though we limit this discussion to the u.s., this is agrowing global problem (laub, 2019) that can leadto deadly outcomes (rajagopalan et al., 2018).
inthis hypothetical use case, the nlp system may au-tomatically remove violent content or alert contentmoderators to potential violations of the social me-dia company’s acceptable use policy.
moderatorscan decide if speciﬁc content should be removed,and if necessary, notify law enforcement to avertpending violence (e.g., threats against individuals,planned violent events).
as a result of the 1996communications decency act9, social media plat-forms have broad latitude (klonick, 2018) to de-velop their own policies for acceptable content andhow they handle it.
in this scenario, the complianceofﬁcer of the company developing the system is re-sponsible for making sure it does not discriminateagainst speciﬁc user demographics..research has shown that hate speech can lead tohateful actions (marsters, 2019).
in many cases,individuals posted their intents online prior to com-mitting violence (cohen et al., 2014).
when iden-tifying content to remove and especially when in-volving law enforcement, it is important to distin-guish between “hunters" — those who act — and“howlers" — those who do not (marsters, 2019).
this is to avoid wrongly detaining individuals whohave no intention of committing violence, evenif their words are indefensible.
between these ex-tremes, posters may harass, stalk, dox, or otherwiseabuse victims from a distance, therefore it is stillnecessary to ﬂag, remove, and potentially track ordocument violent content..linguistic landscape.
we focus solely on en-glish speakers, but we acknowledge that the actuallinguistic landscape is much more complex (over350 languages).
posters on social media may speakenglish as their ﬁrst language or as a second lan-guage and they often code-switch/-mix.
standardamerican english is used for business purposesin the u.s. but there are other frequently used lan-guage varieties including african american en-glish (aae), cajun vernacular english, and three.
9fcc.gov/general/telecommunications-act-1996.
different latinx (hispanic) vernacular englishes..stakeholder impact.
the key stakeholders thatwill be impacted are those most often facing violentthreats online: minorities, women, immigrants, andthe lgbtq community (amnesty international,2018; ganesh, 2018; davidson et al., 2019; wake-ﬁeld, 2020).
additionally, anyone that posts con-tent on the social media site is a stakeholder.
un-fortunately, the very communities that are oftenthe target of violent posts are also often wronglyﬂagged as posting toxic content themselves due toracial biases present in the training data (sap et al.,2019; davidson et al., 2019).
given the risk ofharm to victims if the system misses violent postsfrom hunters or misidentiﬁes legitimate content asviolent and notiﬁes law enforcement, it is criticalthe right balance of false positives and false nega-tives is achieved in ﬂagging content..dimensions.
there are two tasks under consider-ation here: identifying violent content and identi-fying hunters who “truly intend to use lethal vio-lence” (marsters, 2019).
in the ﬁrst task, the sys-tem is looking for content that negatively targets asocially deﬁned group.
additionally, the contentincludes not only hate speech (e.g., profanity, epi-thets, vulgarity) but also content that incites othersto hatred or violence.
since content written in aaehas been shown to be ﬂagged as toxic more often(sap et al., 2019; davidson et al., 2019), we mustensure that the system is reliable when encounter-ing dialectal variation.
additionally, due to thecasual environment of social media, multilingualspeakers often code-switch and code-mix.
hence,we expect variation in these dimensions to haveno effect on the system’s predictions: alternativespellings, morphosyntactic variation, word choice,code-mixing, idioms, and references to and mani-festations of sensitive attributes and their proxies.
however, we must expect the system to be sensitiveto in-group and out-group usage of reclaimed slursso that the in-group usage does not result in a ﬂagwhile out-group usage result in ﬂagged posts..when identifying hunters, we may expect the sys-tem to be sensitive to uses of ﬁrst person pronouns,certainty adverbs, negative evaluative adjectives,and modiﬁers (marsters, 2019).
however, in or-der to avoid unfairly penalizing vernacular englishspeakers we should expect the system’s predictionsto be equally unaffected by variation in the dimen-sions listed for the ﬁrst task..4168semantics.
idioms (e.g., ﬁner than frog hair).
hyphenationcapitalizationpunctuationreduplication of lettersemojis/emoticonshomonymsdisemvoweling (eger and benz, 2020)homophones (e.g., accept vs. except) (eger and benz, 2020)accidental misspellings (belinkov and bisk, 2018)intentional alternative spellings (e.g., yas, thru, startin)open compound concatenation (e.g., couch potato/couchpotato)dialectal differences (e.g., favor vs. favour) (ribeiro et al., 2018)mixing writing scripts (tan and joty, 2021)transliteration.
grammatical gender shiftsgrammatical category (tan et al., 2020)dialectal differences (tan et al., 2020)clitics.
dialectal variation (e.g., fries vs. chips)synonyms/sememes (zang et al., 2020)vocabulary simplicity/complexitycross-lingual synonyms (tan and joty, 2021)loanwords.
matching number and tenseword/phrase order (especially for languages without strict word ordering)prepositional variation (e.g., stand on line vs. stand in line)syntactic variation (iyyer et al., 2018)sentence simplicity/complexitycode-mixing (tan and joty, 2021).
register (e.g., formality)conversational style (involvement/considerateness) (tannen et al., 2005)discourse markers / connector wordscross-cultural differencescode-switching.
gender pronounsnamesreclaimed slursgenderlects (tannen, 1991; dunn, 2014).
namesreclaimed slursrace-aligned language varieties.
age/generation-aligned language styles (hovy et al., 2020).
namesreclaimed slurs.
reclaimed slurs.
associated adjectives (hutchinson et al., 2020).
location names (e.g., cities, countries)figures of speech.
geographic locations (for ethnicity, socioeconomic status).
rule-based (alzantot et al., 2018; jin et al., 2020)model-based (garg and ramakrishnan, 2020; li et al., 2020a).
orthography.
morphology.
lexicon.
syntax.
discourse&pragmatics.
gender identity.
race.
age.
religion.
sexual orientation.
disability status.
place of origin.
proxies.
black-box.
linguistic.
phenomena.
sensitive attributes.
malicious attacks.
gradient-based.
hotflip (ebrahimi et al., 2018), universal triggers (wallace et al., 2019).
policy-based.
adversarial negotiation agent (cheng et al., 2019).
table 1: taxonomy of possible dimensions with references to linguistics literature and existing adversarial attacksthat could be used as worst-case tests.
linguists are best equipped to decide which linguistic phenomena are highpriority for each use case, ethicists for sensitive attributes, and nlp practitioners for malicious attacks..4169