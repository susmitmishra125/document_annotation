exploring the representation of word meanings in context: a casestudy on homonymy and synonymy.
marcos garciacitius – research center in intelligent technologiesuniversidade de santiago de compostela, galizamarcos.garcia.gonzalez@usc.gal.
abstract.
this paper presents a multilingual study ofword meaning representations in context.
weassess the ability of both static and contextual-ized models to adequately represent differentlexical-semantic relations, such as homonymyand synonymy.
to do so, we created a newmultilingual dataset that allows us to performa controlled evaluation of several factors suchas the impact of the surrounding context or theoverlap between words, conveying the same ordifferent senses.
a systematic assessment onfour scenarios shows that the best monolingualmodels based on transformers can adequatelydisambiguate homonyms in context.
however,as they rely heavily on context, these modelsfail at representing words with different senseswhen occurring in similar sentences.
experi-ments are performed in galician, portuguese,english, and spanish, and both the dataset(with more than 3,000 evaluation items) andnew models are freely released with this study..1.introduction.
contrary to static vector models, which representthe different senses of a word in a single vector(erk, 2012; mikolov et al., 2013), contextualizedmodels generate representations at token-level (pe-ters et al., 2018; devlin et al., 2019), thus beingan interesting approach to model word meaning incontext.
in this regard, several studies have shownthat clusters produced by some contextualized wordembeddings (cwes) are related to different sensesof the same word (reif et al., 2019; wiedemannet al., 2019), or that similar senses can be aligned incross-lingual experiments (schuster et al., 2019)..however, more systematic evaluations of pol-ysemy (i.e., word forms that have different re-lated meanings depending on the context (apres-jan, 1974)), have shown that even though cwespresent some correlations with human judgments.
(nair et al., 2020), they fail to predict the similarityof the various senses of a polysemous word (haberand poesio, 2020)..as classical datasets to evaluate the capabilitiesof vector representations consist of single wordswithout context (finkelstein et al., 2001) or heavilyconstrained expressions (kintsch, 2001; mitchelland lapata, 2008), new resources with annotationsof words in free contexts have been created, includ-ing both graded similarities (huang et al., 2012;armendariz et al., 2020) or binary classiﬁcationof word senses (pilehvar and camacho-collados,2019; raganato et al., 2020).
however, as thesedatasets largely include instances of polysemy, theyare difﬁcult to solve even for humans (in fact, thehighest reported human upper bound is about 80%)as the nuances between different senses dependon non-linguistic factors such as the annotator pro-cedure or the target task (tuggy, 1993; kilgarriff,1997; hanks, 2000; erk, 2010)..in this paper, we rely on a more objectiveand simple task to assess how contextualized ap-proaches (both neural network models and con-textualized methods of distributional semantics)represent word meanings in context.
in particu-lar, we observe whether vector models can iden-tify unrelated meanings represented by the sameword form (homonymy) and the same sense con-veyed by different words (synonymy).
in contrastto polysemy, there is a strong consensus concern-ing the representation of homonymous senses inthe lexicon, and it has been shown that homonymsare cognitively processed differently than polyse-mous words (klepousniotou et al., 2012; macgre-gor et al., 2015).
in this regard, exploratory experi-ments in english suggest that some cwes correctlymodel homonymy, approximating the contextual-ized vectors of a homonym to those of its para-phrases (lake and murphy, 2020), and showingstronger correlation with human judgments to those.
proceedingsofthe59thannualmeetingoftheassociationforcomputationallinguisticsandthe11thinternationaljointconferenceonnaturallanguageprocessing,pages3625–3640august1–6,2021.©2021associationforcomputationallinguistics3625of polysemous words (nair et al., 2020).
however,as homonyms convey unrelated meanings depend-ing on the context, it is not clear whether the goodperformance of cwes actually derives from thecontextualization process or simply from the useof explicit lexical cues present in the sentences..taking the above into account, we have createda new multilingual dataset (in galician, portuguese,english, and spanish) with more than 3,000 evalu-ation items.
it allows for carrying out more than 10experiments and controlling factors such as thesurrounding context, the word overlap, and thesense conveyed by different word forms.
we usethis resource to perform a systematic evaluationof contextualized word meaning representations.
we compare different strategies using both staticembeddings and current models based on deep ar-tiﬁcial neural networks.
the results suggest thatthe best monolingual models based on transform-ers (vaswani et al., 2017) can identify homonymshaving different meanings adequately.
however,as they strongly rely on the surrounding context,words with different meanings are represented veryclosely when they occur in similar sentences.
apartfrom the empirical conclusions and the dataset, thispaper also contributes with new bert and fasttextmodels for galician.1.
section 2 presents previous studies about wordmeaning representation.
then, section 3 introducesthe new dataset used in this paper.
in section 4we describe the models and methods to obtain thevector representations.
finally, the experiments andresults are discussed in section 5, while section 6draws some conclusions of our study..2 related work.
a variety of approaches has been implementedto compute word meaning in context by meansof standard methods of distributional semantics(schütze, 1998; kintsch, 2001; mcdonald andbrew, 2004; erk and padó, 2008).
as composi-tional distributional models construct sentence rep-resentations from their constituents vectors, theytake into account contextualization effects on mean-ing (mitchell and lapata, 2008; baroni and zam-parelli, 2010; baroni, 2013).
however, these ap-proaches often have scalability problems as theirrepresentations grow exponentially with the sizeof the sentences.
therefore, the datasets used to.
1dataset, models, and code are available at https://.
github.com/marcospln/homonymy_acl21/..evaluate them are composed of highly restrictedphrases (grefenstette and sadrzadeh, 2011)..the rise of artiﬁcial neural networks on naturallanguage processing popularized the use of vectorrepresentations, and the remarkable performance ofneural language models (melamud et al., 2016; pe-ters et al., 2018) led to a productive line of researchexploring to what extent these models represent lin-guistic knowledge (rogers et al., 2020).
however,few of these works have focused on lexical seman-tics, and most of the relevant results in this ﬁeldcome from evaluations in downstream tasks.
in thisregard, wiedemann et al.
(2019) found that clustersof bert embeddings (devlin et al., 2019) seemto be related to word senses, while schuster et al.
(2019) observed that clusters of polysemous wordscorrespond to different senses in a cross-lingualalignment of vector representations..probing lstms on lexical substitution tasks,aina et al.
(2019) showed that these architecturesrely on the lexical information from the input em-beddings, and that the hidden states are biased to-wards contextual information.
on an exploration ofthe geometric representations of bert, reif et al.
(2019) found that different senses of a word tendto appear separated in the vector space, while sev-eral clusters seem to correspond to similar senses.
recently, vuli´c et al.
(2020) evaluated the perfor-mance of bert models on several lexical-semantictasks in various languages, including semantic sim-ilarity or word analogy.
the results show that usingspecial tokens ([cls] or [sep]) hurts the quality ofthe representations, and that these tend to improveacross layers until saturation.
as this study usesdatasets of single words (without context), type-level representations are obtained by averaging thecontextualized vectors over various sentences..there are several resources to evaluate wordmeaning in free contexts, such as the stanford con-textual word similarity (huang et al., 2012) andcosimlex (armendariz et al., 2020), both repre-senting word similarity on a graded scale, or theword-in-context datasets (wic), focused on binaryclassiﬁcations (i.e., each evaluation item containstwo sentences with the same word form, having thesame or different senses) (pilehvar and camacho-collados, 2019; raganato et al., 2020).
thesedatasets include not only instances of homonymybut mostly of polysemous words.
in this regard,studies on polysemy using transformers have ob-tained diverse results: haber and poesio (2020).
3626found that bert embeddings correlate better withhuman ratings of co-predication than with similar-ity between word senses, thus suggesting that theserepresentations encode more contextual informa-tion than word sense knowledge.
nevertheless, theresults of nair et al.
(2020) indicate that bert rep-resentations are correlated with human scores ofpolysemy.
an exploratory experiment of the latterstudy also shows that bert discriminates betweenpolysemy and homonymy, which is also suggestedby other pilot evaluations reported by lake andmurphy (2020) and yu and ettinger (2020)..our study follows this research line pursuing ob-jective and unambiguous lexical criteria such as therepresentation of homonyms and synonyms.
in thiscontext, there is a broad consensus in the psycholin-guistics literature regarding the representation ofhomonyms as different entries in the lexicon (incontrast to polysemy, for which there is a long dis-cussion on whether senses of polysemous wordsare stored as a single core representation or as in-dependent entries (hogeweg and vicente, 2020)).
in fact, several studies have shown that homonymsare cognitively processed differently from polyse-mous words (klepousniotou et al., 2012; rabagliatiand snedeker, 2013).
in contrast to the differentsenses of polysemous words, which are simulta-neously activated, the meanings of homonyms arein conﬂict during processing, with the not relevantones being deactivated by the context (macgre-gor et al., 2015).
to analyze how vector modelsrepresent homonymy and synonymy in context, wehave built a new multilingual resource with a stronginter-annotator agreement, presented below..3 a new multilingual resource of.
homonymy and synonymy in context.
this section brieﬂy describes some aspects oflexical semantics relevant to our study, and thenpresents the new dataset used in the paper..homonymy and homography: homonymy is awell-known type of lexical ambiguity that can bedescribed as the relation between distinct and unre-lated meanings represented by the same word form,such as match, meaning for instance ‘sports game’or ‘stick for lighting ﬁre’.
in contrast to polysemy(where one lexeme conveys different related sensesdepending on the context, e.g., newspaper as anorganization or as a set of printed pages), it is of-ten assumed that homonyms are different lexemesthat have the same lexical form (cruse, 1986), and.
therefore they are stored as independent entries inthe lexicon (pustejovsky, 1998)..there are two main criteria for homonymy iden-tiﬁcation: diachronically, homonyms are lexicalitems that have different etymologies but are acci-dentally represented by the same word form, whilea synchronic perspective strengthens unrelatednessin meaning.
even if both approaches tend to iden-tify similar sets of homonyms, there may be am-biguous cases that are diachronically but not syn-chronically related (e.g., two meanings of banco–‘bench’ and ‘ﬁnancial institution’– in portugueseor spanish could be considered polysemous as theyderive from the same origin,2 but as this is a purelyhistorical association, most speakers are not awareof the common origin of both senses).
in this study,we follow the synchronic perspective, and considerhomonymous meanings those that are clearly unre-lated (e.g., they unambiguously refer to completelydifferent concepts) regardless of their origin..it is worth mentioning that as we are dealingwith written text we are actually analyzing homo-graphs (different lexemes with the same spelling)instead of homonyms.
thus, we discard instancesof phonologically identical words which are writtendifferently, such as the spanish hola ‘hello’ and ola‘wave’, both representing the phonological form/ola/.
similarly, we include words with the samespelling representing different phonological forms,e.g., the galician-portuguese sede, which corre-sponds to both /sede/ ‘thirst’, and /sede/ ‘head-quarters’..in this paper, homonymous senses are those unre-lated meanings conveyed by the same (homonym)word form.
for instance, coach may have twohomonymous senses (‘bus’ and ‘trainer’), whichcan be conveyed by other words (synonyms) indifferent contexts (e.g., by bus or trainer)..structure of the dataset: we have created a newresource to investigate how vector models representword meanings in context.
in particular, we wantto observe whether they capture (i) different sensesconveyed by the same word form (homonymy), and(ii) equivalent senses expressed by different words(synonymy).
the resource contains controlled sen-tences so that it allows us to observe how the con-text and word overlap affect word representations.
to allow for different comparisons with the same.
2in fact, several dictionaries organize them in a single en-try: https://dicionario.priberam.org/banco,https://dle.rae.es/banco..3627sense sentences 1-3.sentence 4.sentence 5.
(1).
(2).
we’re going to the airport by coach..we’re going to the airport by bus.
we’re going to the airport by bicycle..that man was appointed as the new coach..that man was appointed as the new trainer.
that man was appointed as the new president..[.
.
. ]
the coach was badly.
they had to travel.
delayed by roadworks..everywhere by bus..she has recently joined.
they need a new trainer.
the amateur team as coach..for the young athletes..table 1: example sentences for two senses of coach in english (‘bus’ and ‘trainer’).
sentences 1 to 3 include, inthe same context, the target word, a synonym, and a word with a different sense (in italic), respectively.
sentences4 and 5 contain the target word and a synonym in different contexts, respectively..and different contexts, we have included ﬁve sen-tences for each meaning (see table 1 for examples):three sentences containing the target word, a syn-onym, and a word with a different sense, all ofthem in the same context (sentences 1 to 3), andtwo additional sentences with the target word and asynonym, representing the same sense (sentences 4and 5, respectively).
thus, for each sense we havefour sentences (1, 2, 4, 5) with a word conveyingthe same sense (both in the same and in differentcontexts) and another sentence (3) with a differentword in the same context as sentences 1 and 2..from this structure, we can create datasets ofsentence triples, where the target words of two ofthem convey the same sense, and the third one has adifferent meaning.
thus, we can generate up to 48triples for each pair of senses (24 in each direction:sense 1 vs. sense 2, and vice-versa).
these datasetsallow us to evaluate several semantic relations atthe lexical level, including homonymy, synonymy,and various combinations of homonymous senses.
interestingly, we can control for the impact of thecontext (e.g., are contextualized models able todistinguish between different senses occurring inthe same context, or do they incorporate excessivecontextual information into the word vectors?
), theword overlap (e.g., can a model identify differentsenses of the same word form depending on thecontext, or it strongly depends on lexical cues?
),or the pos-tag (e.g., are homonyms with differentpos-tags easily disambiguated?)..
construction of the dataset: we compiled datafor four languages: galician, portuguese, spanish,and english.3 we tried to select sentences compati-ble with the different varieties of the same language.
3galician is generally considered a variety of a single(galician-)portuguese language.
however, they are dividedin this resource, as galician has recently been standardizedusing a spanish-based orthography that formally separates itfrom portuguese (samartim, 2012)..
(e.g., with the same meaning in uk and us english,or in castilian and mexican spanish).
however,we gave priority to the european varieties whennecessary (e.g., regarding spelling variants)..the dataset was built using the following pro-cedure: first, language experts (one per language)compiled lists of homonyms using dedicated re-sources for language learning, together with word-net and other lexicographic data (miller, 1995;montraveta and vázquez, 2010; guinovart, 2011;rademaker et al., 2014).
only clear and unam-biguous homonyms were retained (i.e., those inthe extreme of the homonymy-polysemy-vaguenessscale (tuggy, 1993)).
these homonyms werethen enriched with frequency data from large cor-pora: wikipedia and sli galweb (agerri et al.,2018) for galician, and a combination of wikipediaand europarl for english, spanish and portuguese(koehn, 2005).
from these lists, each linguist se-lected the most frequent homonyms, annotatingthem as ambiguous at type or token level (absolutehomonymy and partial homonymy in lyons’ terms(lyons, 1995)).
as a substantial part were noun-verb pairs, only a few of these were included.
foreach homonym, the language experts selected fromcorpora two sentences (1 and 4) in which the targetwords were not ambiguous.4 they then selecteda synonym that could be used in sentence 1 with-out compromising grammaticality (thus generatingsentence 2), and compiled an additional sentencefor it (5), trying to avoid further lexical ambiguitiesin this process.5 for each homonym, the linguistsselected a word with a different meaning (for sen-.
4sentences were selected, adapted, and simpliﬁed usinggdex-inspired constraints (kilgarriff et al., 2008) (i.e., avoid-ing high punctuation ratios, unnecessary subordinate clauses,etc.
), which resulted in the creation of new sentences..5in most cases, this synonym is the same as that of sentence2, but this is not always the case.
besides, in some cases wecould not ﬁnd words conveying the same sense, for which wedo not have sentences 2 and 5..3628language hom.
senses sent.
triples pairs wic197galician129english81portuguese101spanish508total.
47 (4)30 (5)22 (1)23 (3)122.
13657093586453077.
8234632733911950.
22713894105564.
2214111057.κ0.940.960.960.950.94.table 2: characteristics of the dataset.
first three columns display the number of homonyms (hom), senses, andsentences (sent), respectively.
senses in parentheses are the number of homonymous pairs with different pos-tags).
center columns show the size of the evaluation data in three formats: triples, pairs, and wic-like pairs,followed by the cohen’s κ agreements and their micro-average.
the total number of homonyms and senses is thesum of the language-speciﬁc ones, regardless of the fact that some senses occur in more than one language..tence 3), trying to maximize the following criteria:(i) to refer unambiguously to a different concept,and to preserve (ii) semantic felicity and (iii) gram-maticality.
the size of the ﬁnal datasets variesdepending on the initial lists and on the ease ofﬁnding synonyms in context..results: apart from the sentence triples ex-plained above, the dataset structure allows us tocreate evaluation sets with different formats, suchas sentence pairs to perform binary classiﬁcationsas in the wic datasets.
table 2 shows the num-ber of homonyms, senses, and sentences of themultilingual resource, together with the size of theevaluation datasets in different formats..as the original resource was created by one an-notator per language, we ensured its quality as fol-lows: we randomly extracted sets of 50 sentencepairs and gave them to other annotators (5 for gali-cian, and 1 for each of the other three varieties, allof them native speakers of the target language).
wethen computed the cohen’s κ inter-annotator agree-ment (cohen, 1960) between the original resourceand the outcome of this second annotation (see theright column of table 2).
we obtained a micro-average κ = 0.94 across languages, a result whichsupports the task’s objectivity.
nevertheless, it isworth noting that few sentences have been carefullymodiﬁed after this analysis, as it has shown thatseveral misclassiﬁcations were due to the use ofan ambiguous synonym.
thus, it is likely that theﬁnal resource has higher agreement values..4 models and methods.
this section introduces the models and proceduresto obtain vector representations followed by theevaluation method..4.1 models.
we have used static embeddings and cwes basedon transformers, comparing different ways of ob-taining the vector representations in both cases:.
static embeddings: we have used skip-gramfasttext models of 300 dimensions (bojanowskiet al., 2017).6 for english and spanish, we haveused the ofﬁcial vectors trained on wikipedia.
forportuguese, we have used the model provided byhartmann et al.
(2017), and for galician we havetrained a new model (see appendix c for details).7.contextualized embeddings: we have evalu-ated multilingual and monolingual models:8.multilingual models: we have used the ofﬁcialmultilingual bert (mbert cased, 12 layers) (de-vlin et al., 2019), xlm-roberta (base, 12 layers)(conneau et al., 2020), and distilbert (distilm-bert, 6 layers) (sanh et al., 2019)..monolingual models: for english, we haveused the ofﬁcial bert-base model (uncased).
forportuguese and spanish, bertimbau (souza et al.,2020) and beto (cañete et al., 2020) (both cased).
for galician, we trained two bert models (with 6and 12 layers; see appendix c)..4.2 obtaining the vectors.
static models: these are the methods used toobtain the representations from the static models:word vector (wv): embedding of the targetword (homonymous senses with the same wordform will have the same representation)..6in preliminary experiments we also used word2vec andglove models, obtaining slightly lower results than fasttext.
7these portuguese and galician models obtained better.
results (0.06 on average) than the ofﬁcial ones..8to make a fair comparison we prioritized base models(12 layers), but we also report results for large (24 layers) and6 layers models when available..3629languagegalicianenglishportuguesespanish.
exp1.
exp2.
exp3.
exp4.
total.
full.
122774565.
105524149.
183893787.
149583771.
27814480146.
2299174110.
135684159.
135684159.
718378203357.
618269193289.
1365709358645.
1157494342517.table 3: number of instances of each experiment and language.
numbers on the right of each column are thosetriples where the three target words belong to the same morphosyntactic category (left values are the total numberof triples).
total are the sums of the four experiments, while full refers to all the instances of the dataset..sentence vector (sent): average embedding of.
the whole sentence..syntax (syn): up to four different representa-tions obtained by adding the vector of the targetword to those of their syntactic heads and depen-dents.
this method is based on the assumptionthat the syntactic context of a word characterizesits meaning, providing relevant information for itscontextualized representation (e.g., in ‘he swims tothe bank’, bank may be disambiguated by combin-ing its vector with the one of swim).9 appendix ddescribes how heads and dependents are selected..contextualized models: for these models, wehave evaluated the following approaches:.
sentence vector (sent): vector of the sentencebuilt by averaging all words (except for the specialtokens [cls] and [sep]), each of them representedby the standard approach of concatenating the last4 layers (devlin et al., 2019)..word vector (wv): embedding of the targetword, combining the vectors of the last 4 layers.
we have evaluated two operations: vector concate-nation (cat), and addition (sum)..word vector across layers (lay): vector of thetarget word on each layer.
this method allows us toexplore the contextualization effects on each layer.
vectors of words split into several sub-wordsare obtained by averaging the embeddings of theircomponents.
similarly, mwes vectors are the av-erage of the individual vectors of their components,both for static and for contextualized embeddings..4.3 measuring sense similarities.
given a sentence triple where two of the targetwords (a and b) have the same sense and the third(c) a different one, we evaluate a model as fol-lows (in a similar way as other studies (kintsch,2001; lake and murphy, 2020)): first, we obtain.
9we have also evaluated a contextualization method usingselectional preferences inspired by erk and padó (2008), butthe results were almost identical to those of the wv approach..three cosine similarities between the vector repre-sentations: sim1 = cos(a, b); sim2 = cos(a, c);sim3 = cos(b, c).
then, an instance is labeled ascorrect if those words conveying the same sense(a and b) are closer together than the third one (c).
in other words, sim1 > sim2 and sim1 > sim3:otherwise, the instance is considered as incorrect..5 evaluation.
this section presents the experiments performedusing the new dataset and discusses their results..5.1 experiments.
among all the potential analyses of our data, wehave selected four evaluations to assess the behav-ior of a model by controlling factors such as thecontext and the word overlap:.
homonymy (exp1): the same word form inthree different contexts, two of them with the samesense (e.g., coach in sentences [1:1, 1:4, 2:1]10 intable 1).
this test evaluates if a model correctlycaptures the sense of a unique word form in con-text.
hypothesis: static embeddings will fail asthey produce the same vector in the three cases,while models that adequately incorporate contex-tual cues should correctly identify the outlier sense..synonyms of homonymous senses (exp2): aword is compared with its synonym and with thesynonym of its homonym, all three in different con-texts (e.g., coach=bus(cid:54)=trainer in [1:1, 1:5, 2:2]).
this test assesses if there is a bias towards one ofthe homonymous senses, e.g., the most frequentone (macgregor et al., 2015).
hypothesis: mod-els with this type of bias may fail, so as in exp1,they should also appropriately incorporate contex-tual information to represent these examples..synonymy vs homonymy (exp3): we comparea word to its synonym and to a homonym, all in.
10first and second digits refer to the sense and sentence ids..3630different contexts (e.g., coach=bus(cid:54)=coach in [1:1,1:5, 2:1]).
here we evaluate whether a model ad-equately represents both (i) synonymy in context–two word forms with the same sense in differentcontexts– and (ii) homonymy –one of the formerword forms having a different meaning.
hypothe-sis: models relying primarily on lexical knowledgeare likely to represent homonyms closer than syn-onyms (giving rise to an incorrect output), but thoseintegrating contextual information will be able tomodel the three representations correctly..synonymy (exp4): two synonyms vs. a differ-ent word (and sense), all of them in the same con-text (e.g., [2:1, 2:2, 2:3]).
it assesses to what extentthe context affects word representations of differ-ent word forms.
hypothesis: static embeddingsmay pass this test as they tend to represent type-level synonyms closely in the vector space.
highlycontextualized models might be puzzled as differ-ent meanings (from different words) occur in thesame context, so that the models should have anadequate trade-off between lexical and contextualknowledge..table 3 displays the number of sentence triplesfor each experiment as well as the total numberof triples of the dataset.
to focus on the semanticknowledge encoded in the vectors –rather than onthe morphosyntactic information–, we have evalu-ated only those triples in which the target words ofthe three sentences have the same pos-tag (num-bers on the right).11 besides, we have also carriedout an evaluation on the full dataset..5.2 results and discussion.
table 4 contains a summary of the results of eachexperiment in the four languages.
for reasonsof clarity, we include only fasttext embeddingsand the best contextualized model (bert).
resultsfor all models and languages can be seen in ap-pendix a. bert models have the best performanceoverall, both on the full dataset and on the selectedexperiments, except for exp4 (in which the threesentences share the context) where the static mod-els outperform the contextualized representations.
in exp1 and exp2, where the context plays acrucial role, fasttext models correctly labeled be-tween 50%/60% of the examples (depending onthe language and vector type, with better results.
11on average, bert-base models achieved 0.24 higher re-sults (add) when tested on all the instances (including differentpos-tags) of the four experiments..for sent and syn).
for bert, the best accuracysurpasses 0.98 (exp1 in english), with an averageacross languages of 0.78, and where word vectorsoutperform sentence representations.
these highresults and the fact that wvs work better in generalthan sent may be indicators that transformers areproperly incorporating contextual knowledge..solving exp3 requires both dealing with contex-tual effects and homonymy (as two words have thesame form but different meaning) so that static em-beddings hardly achieve 0.5 accuracy (sent, withlower results for both wv and syn).
bert’s per-formance is also lower than in exp1 and exp2,with an average of 0.67 and sent beating wvs inmost cases, indicating that the word vectors are notadequately representing the target senses..finally, fasttext obtains better results than berton exp4 (where the three instances have the samecontext), reaching 0.81 in spanish with an aver-age across languages of 0.64 (always with wvs).
bert’s best performance is 0.41 (in two lan-guages) with an average of 0.42, suggesting thatvery similar contexts may confound the model..to shed light on the contextualization processof transformers, we have analyzed their perfor-mance across layers.
figure 1 shows the accuracycurves (vs. the macro-average sent and wv vec-tors of the contextualized and static embeddings)for ﬁve transformers models on galician, the lan-guage with the largest dataset (see appendix a forequivalent ﬁgures for the other languages)..in exp1 to exp3 the best accuracies are obtainedat upper layers, showing that word vectors appro-priately incorporate contextual information.
this istrue especially for the monolingual bert versions,as the multilingual models’ representations showhigher variations.
except for galician, exp1 hasbetter results than exp2, as the former primarilydeals with context while the latter combines contex-tualization with lexical effects.
in exp3 the curvestake longer to rise as initial layers rely more onlexical than on contextual information.
further-more, except for english (which reaches 0.8), theperformance is low even in the best hidden layers(≈ 0.4).
in exp4 (with the same context in thethree sentences), contextualized models cannot cor-rectly represent the word senses, being surpassedin most cases by the static embeddings..finally, we have observed how transformers rep-resentations vary across the vector space.
figure 2shows the umap visualizations (mcinnes et al.,.
3631model.
vec..exp1 exp2 exp3 exp4 macro micro.
full.
bert-base.
fasttext.
bert-base.
fasttext.
bert-base.
fasttext.
bert-base.
fasttext.
sentcatsentwvsyn (3).
sentaddsentwvsyn (3).
sentaddsentwvsyn (3).
sentaddsentwvsyn (3).
0.6950.7050.5620.210.533.
0.7880.9810.5960.3080.442.
0.6830.8540.610.0240.659.
0.7550.8570.4490.1220.367.galician0.7510.2930.47600.197.
0.7580.7990.6850.5640.658.
0.6550.810.50.5520.69.english.
0.7360.7580.5050.0330.231portuguese0.6350.3780.52700.176spanish0.5360.4090.4450.0180.173.
0.4320.5410.6220.5410.459.
0.5920.7040.3380.620.577.
0.1780.4220.1410.5260.185.
0.2210.4410.1470.5740.176.
0.220.3660.1710.6340.195.
0.1860.4410.0850.8140.237.
0.5960.5550.4660.3250.393.
0.60.7480.4370.3660.385.
0.4930.5350.4820.30.372.
0.5170.6030.3290.3930.339.
0.6180.5130.4680.2860.362.
0.5990.7320.4310.3350.357.
0.5180.5080.4870.2440.337.
0.5160.5640.3460.3460.318.
0.7270.6990.6180.4610.567.
0.70.8390.5430.480.546.
0.5640.670.550.4530.508.
0.5950.740.4290.4790.553.table 4: summary of the bert and fasttext results.
macro and micro refer to the macro-average and micro-average results across the four experiments, respectively.
full are the micro-average values on the whole dataset..figure 1: results across layers and models for galician.
sent and wv (dashed) are macro-average values.
macroavg|syn is the macro-average per layer (transformers) and the macro-average of the syn strategy (fasttext)..2018) of the contextualization processes of exp1and exp3 examples in english.
in 2a, the similarvectors of match in layer 1 are being contextualized.
across layers, producing a suitable representationsince layer 7. however, 2b shows how the model isnot able to adequately represent match close to its.
3632(a) exp1:sentence 2: “chelsea have a match with united next week.”.
sentence 3: “you should always strike a match away fromyou.”.
(b) exp3:sentence 2: “a game consists of two halves lasting 45 minutes,meaning it is 90 minutes long.”.
sentence 3: “he was watching a football stadium.”.
figure 2: umap visualizations of word contextualization across layers (1 to 12) in exp1 and exp3 in english(bert-base).
in both cases, sentence 1 is “he was watching a football match.”, and the target word in sentence 3is the outlier..synonym game, as the vectors seem to incorporateexcessive information (or at least limited lexicalknowledge) from the context.
additional visualiza-tions in galician can be found in appendix b..in sum, the experiments performed in this studyallow us to observe how different models generatecontextual representations.
in general, our resultsconﬁrm previous ﬁndings which state that trans-formers models increasingly incorporate contextualinformation across layers.
however, we have alsofound that this process may deteriorate the rep-resentation of the individual words, as it may beincorporating excessive contextual information, assuggested by haber and poesio (2020)..6 conclusions and further work.
this paper has presented a systematic study of wordmeaning representation in context.
besides staticword embeddings, we have assessed the abilityof state-of-the-art monolingual and multilingualmodels based on the transformers architecture toidentify unambiguous cases of homonymy and syn-onymy.
to do so, we have presented a new datasetin four linguistic varieties that allows for controlledevaluations of vector representations..the results of our study show that, in most cases,the best contextualized models adequately identifyhomonyms conveying different senses in variouscontexts.
however, as they strongly rely on the sur-rounding contexts, they misrepresent words havingdifferent senses in similar sentences..with multiword expressions of different degrees ofidiomaticity and to include less transparent –butstill unambiguous– contexts of homonymy.
finally,we also plan to systematically explore how multilin-gual models represent homonymy and synonymyin cross-lingual scenarios..acknowledgments.
we would like to thank the anonymous review-ers for their valuable comments, and nvidiacorporation for the donation of a titan xp gpu.
this research is funded by a ramón y cajal grant(ryc2019-028473-i) and by the galician govern-ment (erdf 2014-2020: call ed431g 2019/04)..references.
rodrigo agerri, xavier gómez guinovart, germanrigau, and miguel anxo solla portela.
2018. de-veloping new linguistic resources and tools for thegalician language.
in proceedings of the eleventhinternational conference on language resourcesand evaluation (lrec 2018), miyazaki, japan.
eu-ropean language resources association (elra)..laura aina, kristina gulordava, and gemma boleda.
2019. putting words in context: lstm languagein proceedings ofmodels and lexical ambiguity.
the 57th annual meeting of the association for com-putational linguistics, pages 3342–3348, florence,italy.
association for computational linguistics..ju d apresjan.
1974. regular polysemy.
linguistics,.
in further work, we plan to extend our dataset.
12(142):5–32..3633carlos santos armendariz, matthew purver, matejulˇcar, senja pollak, nikola ljubeši´c, and markgranroth-wilding.
2020. cosimlex: a resourcefor evaluating graded word similarity in context.
in proceedings of the 12th language resourcesand evaluation conference, pages 5878–5886, mar-seille, france.
european language resources asso-ciation..marco baroni.
2013. composition in distributionallanguage and linguistics compass,.
semantics.
7(10):511–522..marco baroni and roberto zamparelli.
2010. nounsare vectors, adjectives are matrices: representingadjective-noun constructions in semantic space.
inproceedings of the 2010 conference on empiricalmethods in natural language processing, pages1183–1193, cambridge, ma.
association for com-putational linguistics..piotr bojanowski, edouard grave, armand joulin, andtomas mikolov.
2017. enriching word vectors withsubword information.
transactions of the associa-tion for computational linguistics, 5:135–146..josé cañete, gabriel chaperon, rodrigo fuentes, jou-hui ho, hojin kang, and jorge pérez.
2020. span-ish pre-trained bert model and evaluation data.
in pml4dc at iclr 2020..jacob cohen.
1960. a coefﬁcient of agreement fornominal scales.
educational and psychological mea-surement, 20(1):37–46..alexis conneau, kartikay khandelwal, naman goyal,vishrav chaudhary, guillaume wenzek, franciscoguzmán, edouard grave, myle ott, luke zettle-moyer, and veselin stoyanov.
2020. unsupervisedcross-lingual representation learning at scale.
inproceedings of the 58th annual meeting of the asso-ciation for computational linguistics, pages 8440–8451, online.
association for computational lin-guistics..david alan cruse.
1986. lexical semantics.
cam-.
bridge university press..jacob devlin, ming-wei chang, kenton lee, andkristina toutanova.
2019. bert: pre-training ofdeep bidirectional transformers for language under-in proceedings of the 2019 conferencestanding.
of the north american chapter of the associationfor computational linguistics: human languagetechnologies, volume 1 (long and short papers),pages 4171–4186, minneapolis, minnesota.
associ-ation for computational linguistics..katrin erk.
2010. what is word meaning, really?
(andhow can distributional models help us describe it?).
in proceedings of the 2010 workshop on geometri-cal models of natural language semantics, pages17–26, uppsala, sweden.
association for computa-tional linguistics..katrin erk.
2012. vector space models of word mean-ing and phrase meaning: a survey.
language andlinguistics compass, 6(10):635–653..katrin erk and sebastian padó.
2008. a structuredvector space model for word meaning in context.
in proceedings of the 2008 conference on empiri-cal methods in natural language processing, pages897–906, honolulu, hawaii.
association for com-putational linguistics..lev finkelstein, evgeniy gabrilovich, yossi matias,ehud rivlin, zach solan, gadi wolfman, and ey-tan ruppin.
2001. placing search in context: theconcept revisited.
in proceedings of the 10th inter-national conference on world wide web, pages 406–414..marcos garcia and pablo gamallo.
2010. análisemorfossintáctica para português europeu e galego:problemas, soluções e avaliação.
linguamática,2(2):59–67..edward grefenstette and mehrnoosh sadrzadeh.
2011.experimental support for a categorical composi-tional distributional model of meaning.
in proceed-ings of the 2011 conference on empirical methodsin natural language processing, pages 1394–1404,edinburgh, scotland, uk.
association for computa-tional linguistics..xavier gómez guinovart.
2011. galnet: wordnet 3.0.do galego.
linguamática, 3(1):61–67..janosch haber and massimo poesio.
2020. assessingpolyseme sense similarity through co-predication ac-ceptability and contextualised embedding distance.
in proceedings of the ninth joint conference on lex-ical and computational semantics, pages 114–124,barcelona, spain (online).
association for compu-tational linguistics..patrick hanks.
2000. do word meanings exist?
com-.
puters and the humanities, 34:205–215..nathan hartmann, erick fonseca, christopher shulby,marcos treviso, jéssica silva, and sandra aluísio.
2017. portuguese word embeddings: evaluatingon word analogies and natural language tasks.
inproceedings of the 11th brazilian symposium in in-formation and human language technology, pages122–131, uberlândia, brazil.
sociedade brasileirade computação..lotte hogeweg and agustin vicente.
2020. on the na-ture of the lexicon: the status of rich lexical mean-ings.
journal of linguistics, 56(4):865–891..eric huang, richard socher, christopher manning,and andrew ng.
2012.improving word represen-tations via global context and multiple word proto-types.
in proceedings of the 50th annual meeting ofthe association for computational linguistics (vol-ume 1: long papers), pages 873–882, jeju island,korea.
association for computational linguistics..3634adam kilgarriff.
1997. i don’t believe in word senses..computers and the humanities, 31(2):91–113..adam kilgarriff, milos husák, katy mcadam,michael rundell, and pavel rychl`y.
2008. gdex:automatically ﬁnding good dictionary examples in acorpus.
in proceedings of the xiii euralex inter-national congress, pages 425–432.
documenta uni-versitaria barcelona, spain..walter kintsch.
2001. predication.
cognitive science,.
25(2):173–202..ekaterini klepousniotou, g bruce pike, karsten stein-hauer, and vincent gracco.
2012. not all ambigu-ous words are created equal: an eeg investigationof homonymy and polysemy.
brain and language,123(1):11–21..philipp koehn.
2005. europarl: a parallel corpusfor statistical machine translation.
in conferenceproceedings: the tenth machine translation summit,volume 5, pages 79–86.
aamt..yuri kuratov and mikhail arkhipov.
2019. adaptationof deep bidirectional multilingual transformers forrussian language.
computational linguistics andintellectual technologies, 18:333–339..brenden m. lake and gregory l. murphy.
2020. wordmeaning in minds and machines.
arxiv preprint:2008.01766..john lyons.
1995. linguistic semantics: an introduc-.
tion.
cambridge university press..lucy j macgregor, jennifer bouwsema, and ekateriniklepousniotou.
2015. sustained meaning activationfor polysemous but not homonymous words: evi-dence from eeg.
neuropsychologia, 68:126–138..scott mcdonald and chris brew.
2004. a distribu-tional model of semantic context effects in lexicalin proceedings of the 42nd annualprocessing.
meeting of the association for computational lin-guistics (acl-04), pages 17–24, barcelona, spain..leland mcinnes, john healy, nathaniel saul, andlukas großberger.
2018. umap: uniform man-journal ofifold approximation and projection.
open source software, 3(29):861..oren melamud, jacob goldberger, and ido dagan.
2016. context2vec: learning generic context em-in proceedingsbedding with bidirectional lstm.
of the 20th signll conference on computationalnatural language learning, pages 51–61, berlin,germany.
association for computational linguis-tics..tomas mikolov, kai chen, greg corrado, and jef-frey dean.
2013. efﬁcient estimation of word rep-in workshop pro-resentations in vector space.
ceedings of the international conference on learn-ing representations (iclr) 2013. arxiv preprintarxiv:1301.3781..george a miller.
1995..a lexicaldatabase for english.
communications of the acm,38(11):39–41..wordnet:.
jeff mitchell and mirella lapata.
2008. vector-basedmodels of semantic composition.
in proceedings ofacl-08: hlt, pages 236–244, columbus, ohio.
as-sociation for computational linguistics..ana maría fernández montraveta and gloria vázquez.
2010. la construcción del wordnet 3.0 en español.
la lexicografía en su dimensión teórica, pages 201–220..sathvik nair, mahesh srinivasan, and stephan meylan.
2020. contextualized word embeddings encode as-pects of human-like word sense knowledge.
in pro-ceedings of the workshop on the cognitive aspectsof the lexicon, pages 129–141, online.
associationfor computational linguistics..lluís padró and evgeny stanilovsky.
2012. freelingin proceed-3.0: towards wider multilinguality.
ings of the eighth international conference on lan-guage resources and evaluation (lrec’12), pages2473–2479, istanbul, turkey.
european languageresources association (elra)..matthew peters, mark neumann, mohit iyyer, mattgardner, christopher clark, kenton lee, and lukezettlemoyer.
2018. deep contextualized word rep-in proceedings of the 2018 confer-resentations.
ence of the north american chapter of the associ-ation for computational linguistics: human lan-guage technologies, volume 1 (long papers), pages2227–2237, new orleans, louisiana.
associationfor computational linguistics..mohammad taher pilehvar and jose camacho-collados.
2019. wic: the word-in-context datasetfor evaluating context-sensitive meaning represen-in proceedings of the 2019 conferencetations.
of the north american chapter of the associationfor computational linguistics: human languagetechnologies, volume 1 (long and short papers),pages 1267–1273, minneapolis, minnesota.
associ-ation for computational linguistics..james pustejovsky.
1998..the generative lexicon..the mit press..hugh rabagliati and jesse snedeker.
2013. the truthabout chickens and bats: ambiguity avoidance dis-tinguishes types of polysemy.
psychological sci-ence, 24(7):1354–1360..alexandre rademaker, valeria de paiva, gerardde melo, livy real, and maira gatti.
2014.openwordnet-pt: a project report.
in proceedingsof the seventh global wordnet conference, pages383–390, tartu, estonia.
university of tartu press..colin raffel, noam shazeer, adam roberts, katherinelee, sharan narang, michael matena, yanqi zhou,wei li, and peter j liu.
2020. exploring the lim-its of transfer learning with a uniﬁed text-to-text.
3635ashish vaswani, noam shazeer, niki parmar, jakobuszkoreit, llion jones, aidan n. gomez, lukaszkaiser, and illia polosukhin.
2017. attention is allyou need.
arxiv preprint arxiv:1706.03762..ivan vuli´c, edoardo maria ponti, robert litschko,goran glavaš, and anna korhonen.
2020. probingpretrained language models for lexical semantics.
inproceedings of the 2020 conference on empiricalmethods in natural language processing (emnlp),pages 7222–7240, online.
association for computa-tional linguistics..guillaume wenzek, marie-anne lachaux, alexis con-neau, vishrav chaudhary, francisco guzmán, ar-mand joulin, and edouard grave.
2020. ccnet:extracting high quality monolingual datasets fromin proceedings of the 12th lan-web crawl data.
guage resources and evaluation conference, pages4003–4012, marseille, france.
european languageresources association..gregor wiedemann, steffen remus, avi chawla, andchris biemann.
2019. does bert make anysense?
interpretable word sense disambiguationin proceedingswith contextualized embeddings.
of the 15th conference on natural language pro-cessing (konvens 2019): long papers, pages 161–170, erlangen, germany.
german society for com-putational linguistics & language technology..thomas wolf, lysandre debut, victor sanh, julienchaumond, clement delangue, anthony moi, pier-ric cistac, tim rault, remi louf, morgan funtow-icz, joe davison, sam shleifer, patrick von platen,clara ma, yacine jernite, julien plu, canwen xu,teven le scao, sylvain gugger, mariama drame,quentin lhoest, and alexander rush.
2020. trans-formers: state-of-the-art natural language process-ing.
in proceedings of the 2020 conference on em-pirical methods in natural language processing:system demonstrations, pages 38–45, online.
asso-ciation for computational linguistics..lang yu and allyson ettinger.
2020. assessing phrasalrepresentation and composition in transformers.
inproceedings of the 2020 conference on empiricalmethods in natural language processing (emnlp),pages 4896–4907, online.
association for computa-tional linguistics..transformer.
journal of machine learning research,21(140):1–67..alessandro raganato, tommaso pasini, jose camacho-collados, and mohammad taher pilehvar.
2020.xl-wic: a multilingual benchmark for evaluatingin proceedings of thesemantic contextualization.
2020 conference on empirical methods in naturallanguage processing (emnlp), pages 7193–7206,online.
association for computational linguistics..emily reif, ann yuan, martin wattenberg, fernanda bviegas, andy coenen, adam pearce, and been kim.
2019. visualizing and measuring the geometry ofbert.
in advances in neural information process-ing systems, volume 32, pages 8594–8603.
curranassociates, inc..anna rogers, olga kovaleva, and anna rumshisky.
2020. a primer in bertology: what we know abouthow bert works.
transactions of the association forcomputational linguistics, 8:842–866..roberto samartim.
2012. língua somos: a construçãoda ideia de língua e da identidade coletiva na gal-in novas achegas ao es-iza (pré-) constitucional.
tudo da cultura galega ii: enfoques socio-históricose lingüístico-literarios, pages 27–36..victor sanh, lysandre debut, julien chaumond, andthomas wolf.
2019. distilbert, a distilled ver-sion of bert: smaller, faster, cheaper and lighter.
in proceedings of the 5th workshop on energy ef-ﬁcient machine learning and cognitive computingat neurips 2019, vancouver, canada..tal schuster, ori ram, regina barzilay, and amirgloberson.
2019. cross-lingual alignment of con-textual word embeddings, with applications to zero-in proceedings of theshot dependency parsing.
2019 conference of the north american chapter ofthe association for computational linguistics: hu-man language technologies, volume 1 (long andshort papers), pages 1599–1613, minneapolis, min-nesota.
association for computational linguistics..hinrich schütze.
1998. automatic word sense discrim-ination.
computational linguistics, 24(1):97–123..fábio souza, rodrigo nogueira, and roberto lotufo.
2020. bertimbau: pretrained bert models forin 9th brazilian conferencebrazilian portuguese.
on intelligent systems, bracis, rio grande do sul,brazil, october 20-23 (to appear)..milan straka, jana straková, and jan hajic.
2019. ud-pipe at sigmorphon 2019: contextualized em-beddings, regularization with morphological cate-gories, corpora merging.
in proceedings of the 16thworkshop on computational research in phonetics,phonology, and morphology, pages 95–103, flo-rence, italy.
association for computational linguis-tics..david tuggy.
1993. ambiguity, polysemy, and vague-.
ness.
cognitive linguistics, 4(3):273–290..3636appendices.
a complete results.
figure 3 and table 5 include the results for all languages and models.
we also include large variants(bert and xlm-roberta) when available.
for static embeddings, we report results for the best synsetting, which combines up to three syntactically related words with the target word (see appendix d)..figure 3: results across layers and models for english (top), portuguese (middle), and spanish (bottom).
sentand wv (dashed) are macro-average values.
macroavg|syn is the macro-average per layer (transformers) and themacro-average of the syn strategy (fasttext)..3637480.
..480.
..780.
..780.
..160.
..370.
..370.
..360.
..960.
..860.
..850.
..260.
..460.
..360.
..360.
..360.
..450.
..840.
..550.
..70.
..70.
..37.
..0.
37.
..0.
46.
..0.
870.
..87.
..0.
55.
..0.
45.
..0.
45.
..0.
45.
..0.
34.
..0.
34.
..0.
54.
..0.
45.
..0.
65.
..0.
85.
..0.
14.
..0.
34.
..0.
43.
..0.
63.
..0.
4.
..0.
6.
..0.
470.
..570.
..360.
..970.
..80.
..450.
..650.
..650.
..350.
..840.
..740.
..540.
..550.
..750.
..650.
..340.
..540.
..440.
..730.
..930.
..60.
..220.
..34.
..0.
440.
..720.
..750.
..75.
..0.
610.
..720.
..720.
..220.
..720.
..920.
..220.
..140.
..640.
..420.
..820.
..730.
..510.
..750.
..810.
..47.
..0.
670.
..67.
..0.
87.
..0.
80.
..77.
..0.
35.
..0.
45.
..0.
96.
..0.
32.
..0.
32.
..0.
550.
..940.
..150.
..97.
..0.
42.
..0.
12.
..0.
150.
..30.
..0.
32.
..0.
80.
..66.
..0.
380.
..18.
..0.
95.
..0.
180.
..180.
..75.
..0.
26.
..0.
26.
..0.
26.
..0.
95.
..0.
55.
..0.
5.
..0.
6.
..0.
6.
..0.
55.
..0.
25.
..0.
35.
..0.
55.
..0.
96.
..0.
5.
..0.
970.
..690.
..890.
..980.
..1.
1.
560.
..380.
..380.
..380.
..180.
..450.
..170.
..370.
..760.
..960.
..960.
..130.
..440.
..60.
..60.
..470.
..470.
..60.
..25.
..0.
65.
..0.
65.
..0.
250.
..60.
..60.
..910.
..44.
..0.
44.
..0.
450.
..140.
..140.
..95.
0.
7.
0.
7.
0.
67.
0.
68.
0.
68.
0.
–.
–.
–.
360.
..360.
..150.
..960.
..960.
..740.
..670.
..770.
..150.
..350.
..450.
..340.
..840.
..550.
..50.
..–.
–.
–.
83.
..0.
53.
..0.
53.
..0.
14.
..0.
64.
..0.
74.
..0.
93.
..0.
26.
..0.
26.
..0.
93.
..0.
13.
..0.
53.
..0.
53.
..0.
23.
..0.
3.
..0.
–.
–.
–.
830.
..830.
..930.
..250.
..250.
..930.
..660.
..760.
..830.
..630.
..830.
..330.
..930.
..430.
..40.
..–.
–.
–.
910.
..420.
..520.
..910.
..450.
..650.
..170.
..170.
..210.
..430.
..730.
..900.
..180.
..420.
..20.
..–.
–.
–.
14.
..0.
32.
..0.
22.
..0.
64.
..0.
32.
..0.
32.
..0.
93.
..0.
64.
..0.
64.
..0.
54.
..0.
10.
..0.
10.
..0.
54.
..0.
20.
..0.
71.
..0.
–.
–.
–.
14.
0.
54.
0.
44.
0.
44.
0.
26.
0.
26.
0.
84.
0.
36.
0.
66.
0.
44.
0.
94.
0.
25.
0.
43.
0.
26.
0.
85.
0.
–.
–.
–.
15.
0.
16.
0.
36.
0.
15.
0.
76.
0.
76.
0.
94.
0.
48.
0.
48.
0.
15.
0.
16.
0.
16.
0.
54.
0.
21.
0.
73.
0.
65.
0.
66.
0.
76.
0.
86.
0.
86.
0.
45.
0.
45.
0.
55.
0.
54.
0.
16.
0.
16.
0.
44.
0.
85.
0.
85.
0.
6.
0.
74.
0.
74.
0.
55.
0.
54.
0.
15.
0.
5.
0.
25.
0.
5.
0.
15.
0.
55.
0.
45.
0.
35.
0.
74.
0.
43.
0.
43.
0.
63.
0.
93.
0.
93.
0.
53.
0.
74.
0.
74.
0.
93.
0.
52.
0.
62.
0.
94.
0.
42.
0.
43.
0.
94.
0.
35.
0.
45.
0.
25.
0.
85.
0.
75.
0.
54.
0.
83.
0.
83.
0.
53.
0.
24.
0.
34.
0.
53.
0.
84.
0.
94.
0.
83.
0.
13.
0.
23.
0.
84.
0.
73.
0.
3.
0.
22.
0.
73.
0.
73.
0.
22.
0.
64.
0.
64.
0.
71.
0.
51.
0.
71.
0.
43.
0.
43.
0.
51.
0.
23.
0.
23.
0.
2.
0.
22.
0.
22.
0.
71.
0.
36.
0.
46.
0.
83.
0.
83.
0.
96.
0.
83.
0.
73.
0.
75.
0.
61.
0.
41.
0.
14.
0.
42.
0.
22.
0.
53.
0.
93.
0.
83.
0.
35.
0.
0.
0.
0.
1.
0.
74.
0.
2.
0.
81.
0.
34.
0.
15.
0.
45.
0.
94.
0.
15.
0.
94.
0.
34.
0.
64.
0.
94.
0.
64.
0.
15.
0.
34.
0.
94.
0.
15.
0.
34.
0.
23.
0.
53.
0.
26.
0.
45.
0.
64.
0.
3.
0.
86.
0.
58.
0.
58.
0.
86.
0.
59.
0.
59.
0.
36.
0.
37.
0.
37.
0.
15.
0.
36.
0.
36.
0.
94.
0.
37.
0.
37.
0.
15.
0.
86.
0.
17.
0.
16.
0.
20.
0.
66.
0.hsilgne.hsinaps.eseugutrop.37.
0.
7.
0.
7.
0.
46.
0.
56.
0.
56.
0.
35.
0.
26.
0.
16.
0.
45.
0.
65.
0.
65.
0.
44.
0.
94.
0.
5.
0.
75.
0.
15.
0.
15.
0.
26.
0.
64.
0.
75.
0.
26.
0.
15.
0.
15.
0.
34.
0.
34.
0.
93.
0.
83.
0.
73.
0.
34.
0.
93.
0.
83.
0.
43.
0.
73.
0.
73.
0.
34.
0.
92.
0.
13.
0.
74.
0.
92.
0.
63.
0.
5.
0.
65.
0.
55.
0.
94.
0.
64.
0.
54.
0.
83.
0.
14.
0.
4.
0.
24.
0.
24.
0.
24.
0.
33.
0.
6.
0.
4.
0.
4.
0.
24.
0.
43.
0.
63.
0.
74.
0.
33.
0.
93.
0.naicilag.81.
0.
24.
0.
24.
0.
57.
0.
92.
0.
82.
0.
61.
0.
95.
0.
2.
0.
2.
0.
61.
0.
22.
0.
22.
0.
61.
0.
83.
0.
93.
0.
61.
0.
24.
0.
34.
0.
61.
0.
42.
0.
62.
0.
41.
0.
35.
0.
91.
0.
92.
0.
94.
0.
32.
0.
12.
0.
94.
0.
22.
0.
2.
0.
24.
0.
22.
0.
3.
0.
2.
0.
5.
0.
70.
0.
70.
0.
84.
0.
2.
0.
0.
67.
0.
8.
0.
8.
0.
6.
0.
17.
0.
17.
0.
16.
0.
26.
0.
15.
0.
45.
0.
45.
0.
43.
0.
15.
0.
94.
0.
25.
0.
65.
0.
96.
0.
65.
0.
66.
0.
5.
0.
4.
0.
17.
0.
7.
0.
7.
0.
16.
0.
26.
0.
16.
0.
84.
0.
75.
0.
75.
0.
25.
0.
65.
0.
55.
0.
24.
0.
84.
0.
64.
0.
15.
0.
25.
0.
45.
0.
65.
0.
12.
0.
35.
0.tnes.tac.dda.tnes.tac.dda.tnes.tac.dda.tnes.tac.dda.tnes.tac.dda.tnes.dda.tnes.vw.treb.
2treb.trebm.b-mlx.l-.
mlx.f.i.m.am.4e.3e.2e.1e.f.i.m.am.4e.3e.2e.1e.f.i.m.am.4e.3e.2e.1e.f.i.m.am.4e.3e.2e.1e..
cev.ledom.egral.dna.,.
naicilag.rof.llams(.
egaugnal.hcae.rof.ledomtrebdnoces.a.ot.srefer.2trebdna.,sledomesab-treberatreb..
segaugnal.ruof.eht.rof.stluser.etelpmoc.:5.elbat.eht.ttsaf.dna.,.
treb.litsid.fo.noisrev.laugnilitlumeht.si.trebmd..
ylevitcepser.,sledomegral.dna.esab.atrebormlxera.
-.
l-.
mlxdna.b-mlx..)
hsilgnedna.eseugutrop.rof.elohweht.no.seulav.egareva-orcimeht.era.f...ylevitcepser.,stnem.irepxe.ruof.eht.ssorca.stluser.egareva-orcimdna.egareva-orcameht.ot.refer.i.mdna.am..
sgniddebme.txettsaf.)3(.
nys.ttsaf.tac.trebmd..
tesatad.3638b contextualization process.
(a) sent.
1: “ten que haber algún erro nos cálculos porque oresultado non é correcto.”sent.
2: “segundo os meus cálculos acabaremos en tres días.”sent.
3: “tivo varios cálculos biliares.”.
(b) sent.
1: “de sobremesa tomou queixo con marmelo.”sentence 2: “fomos a unhas xornadas gastronómicas doqueixo.”sentence 3: “achegouse a ela e pasoulle a man polo queixo.”.
(c) sentence 1: “eran tantos que parecían un banco de xurelos.”sent.2: “desde a rocha víanse pequenos cardumes de robaliza.”sentence 3: “este asento de pedra é algo incómodo.”.
(d) sent.1: “apuntou todos os números de teléfono na axenda.”sentence 2: “anotou todos os números de teléfono na axenda.”sentence 3: “riscou todos os números de teléfono na axenda.”..(e) sent.
1: “vai ter lugar a elección da próxima sede dosxogos olímpicos.”sent.
2: “a localización do evento será decidida esta semana.”sent.
3: “vou á fonte por auga, que teño sede.”.
(f) sentence 1: “encántalle comer o bolo de pan antes da sopa.”sentence 2: “o molete tiña a codia un pouco dura.”sentence 3: “para atraeren as robalizas iscaban bolo vivo.”.
figure 4: examples in galician using bert-base (english translations of the sentences in appendix e).
first row shows examples of ex1.
in figure 4a cálculos is correctly contextualized since layer 3. in figure 4b, theoutlier sense of queixo is not correctly contextualized in any layer.
second row shows examples of exp2 (4c) and exp4 (4d).
in figure 4c, the synonymys banco and cardume arecloser to the outlier asento in layer 1 (and from 4 to 7), but the contextualization process is not able to correctlyrepresent the senses in the vector space.
in figure 4d, the result is correct from layer 7 to 11, but in general therepresentations of words in similar sentences point towards a similar region.
third row incudes examples of exp3.
in figure 4e, the occurrences of the homonym sede are correctly contextual-ized as the one in the ﬁrst sentence approaches its synonym localización in upper layers.
the equivalent exampleof figure 4f is not adequately solved by the model, as both senses of bolo are notoriously distanct from molete,synonym of the ﬁrst homonymous sense..3639these hierarchies we have evaluated representationsbuilt by adding from 1 to 4 vectors to the one ofeach target word.
as shown in table 5, combin-ing 3 syntactically related words to the target oneobtains the best results..for the experiments, we have parsed the datasetsusing the 2.5 universal dependencies models pro-vided by udpipe (straka et al., 2019)..e english translations (figure 4).
figure 4a, sentence 1: “there must be some errorin the calculations because the result is incorrect”.
sentence 2: “according to my calculations we willﬁnish in three days”.
sentence 3: “[he/she] hadseveral gallstones”..figure 4b, sentence 1: “for dessert [he/she] atecheese with quince”.
sentence 2: “we went to acheese gastronomy days”.
sentence 3: “[he/she]approached her and ran his hand over her chin”..figure 4c, sentence 1: “they were so many thatthey looked like a school of mackerel”.
sentence 2:“from the rock small shoals of sea bass could beseen”.
sentence 3: “this stone seat is somewhatuncomfortable”..figure 4d, sentences 1 and 2: “[he/she] wrotedown all the phone numbers on the phone book.”sentence 3: “[he/she] crossed out all the phonenumbers on the phone book”..figure 4e, sentence 1: “the choice of the nextvenue for the olympics will take place”.
sentence2: “the location of the event will be decided thisweek”.
sentence 3: “i’ll get water from the spring,i am thirsty”..figure 4f, sentence 1: “[he/she] loves to eat thebread cake before soup”.
sentence 2: “the breadhad a slightly hard crust”.
sentence 3: “they usedlive sand lance to attrack sea bass”..c galician models.
training corpus: we combined the sli galweb(agerri et al., 2018), cc-100 (wenzek et al., 2020),the galician wikipedia (april 2020 dump), andother news corpora crawled from the web.
fol-lowing raffel et al.
(2020), sentences with a highratio of punctuation and symbols, and duplicateswere removed.
the ﬁnal corpus has 555m words(633m tokens tokenized with freeling (padró andstanilovsky, 2012; garcia and gamallo, 2010)).
the corpus was divided into 90%/10% splits fortrain and development..fasttext model: we trained a fasttext skip-grammodel for 15 iterations with 300 dimensions, win-dow size of 5, negative sampling of 25, and a min-imum word frequency of 5. we used the same90% split used to train the bert models, but withautomatic tokenization (≈ 600m tokens)..bert models: we used the 90% train split ofthe corpus (with the original tokenization) to traintwo bert models, with 6 and 12 layers:.
bert-small (6 layers): this model has beentrained from scratch using a vocabulary of 52,000(sub-)words and a batch size of 208. it has beentraining during 1m steps (≈ 20 epochs) in 14 days..bert-base (12 layers): following kuratovand arkhipov (2019), we initialized the model fromthe ofﬁcial pre-trained mbert, therefore havingthe same vocabulary size (119,547).
we trainedit on the galician corpus during 600k steps (≈ 13epochs in 28 days) with a batch size of 198..both models were trained with the transform-ers library (wolf et al., 2020) on a single nvidiatitan xp gpu (12gb), a block size of 128, a learn-ing rate of 0.0001, a masked language modeling(mlm) probability of 0.15, and a weight decay of0.01. they have been trained only with the mlmobjective..d syntax (syn method).
to get the heads and dependents of each target wordwe have used the following hierarchies: for nouns:headv erb (the head verb, if any)> depv erb (de-pendents of the head verb with one of the followingrelations: obj, nmod, obl)> depadj (a dependentadjective)> depn oun (a dependent noun).
forverbs: head (only if it is a verb or a noun)> obj(its direct object, if any)> arg (a dependent withone of these relations: nsubj, nmod, obl).
using.
3640