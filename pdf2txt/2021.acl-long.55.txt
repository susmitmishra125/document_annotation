tickettalk: toward human-level performance with end-to-end,transaction-based dialog systems.
bill byrne*, karthik krishnamoorthi*, saravanan ganesh*, mihir kalegoogle, mountain view, ca{billb,krishnamoorthi,srrvnn,mihirkale}@google.com.
abstract.
1.introduction.
we present a data-driven, end-to-end approachto transaction-based dialog systems that per-forms at near-human levels in terms of ver-bal response quality and factual grounding ac-curacy.
we show that two essential compo-nents of the system produce these results: asufﬁciently large and diverse, in-domain la-beled dataset, and a neural network-based, pre-trained model that generates both verbal re-sponses and api call predictions.
in terms ofdata, we introduce tickettalk, a movie ticket-ing dialog dataset with 23,789 annotated con-versations.
the movie ticketing conversationsrange from completely open-ended and unre-stricted to more structured, both in terms oftheir knowledge base, discourse features, andnumber of turns.
in qualitative human evalu-ations, model-generated responses trained onjust 10,000 tickettalk dialogs were rated to“make sense” 86.5% of the time, almost thesame as human responses in the same contexts.
our simple, api-focused annotation schemaresults in a much easier labeling task makingit faster and more cost effective.
it is also thekey component for being able to predict apicalls accurately.
we handle factual ground-ing by incorporating api calls in the trainingdata, allowing our model to learn which ac-tions to take and when.
trained on the same10,000-dialog set, the model’s api call predic-tions were rated to be correct 93.9% of thetime in our evaluations, surpassing the ratingsfor the corresponding human labels.
we showhow api prediction and response generationscores improve as the dataset size incremen-tally increases from 5000 to 21,000 dialogs.
our analysis also clearly illustrates the bene-ﬁts of pre-training.
to facilitate future workon transaction-based dialog systems, we havepublished the tickettalk dataset at https://git.io/jl8an..*equal contribution.
building a dialog system that handles human con-versational behavior is challenging because it mustrespond sensibly and relevantly to a wide variety ofcontext-sensitive user input over multiple conver-sation turns.
task-based systems, e.g.
those usedfor ticket booking, food ordering, etc., face furtherhurdles to incorporate ever changing, real-worldknowledge into the dialog and execute transactions.
recently, there has been growing interest in theso-called end-to-end approach to task-based dia-log systems (peng et al., 2020; hosseini-asl et al.,2020; lin et al., 2020; wen et al., 2017; bordeset al., 2016) due to its relatively simple and scal-able architecture, and promising results in chatbotapplications (vinyals and le, 2015; serban et al.,2015b).
inspired by sequence-to-sequence learn-ing (sutskever et al., 2014), this approach trains asingle model on a dialog dataset to form the basisfor a given application.
for each dialog turn, themodel effectively takes the conversation history asits input and generates an appropriate response..to gain wider adoption, the end-to-end approachmust overcome challenges with respect to trainingdata and factual grounding.
in terms of trainingdata, there is already general concern in the nlpcommunity about the lack of quality, task-orienteddialog datasets, especially domain-speciﬁc collec-tions (wen et al., 2017; bordes et al., 2016).
thisproblem is compounded for end-to-end approachessince they typically require a large amount of in-domain data to generate competitive results.
withrespect to grounding, since the end-to-end ap-proach is based on a single neural network, it musteither incorporate the knowledge base (kb) intothe model itself, or the model must be able to accu-rately predict which api calls to make and when.
in addition, details returned from the api callsmust be accurately incorporated in conversational.
proceedingsofthe59thannualmeetingoftheassociationforcomputationallinguisticsandthe11thinternationaljointconferenceonnaturallanguageprocessing,pages671–680august1–6,2021.©2021associationforcomputationallinguistics671responses.
this is contrasted with modular archi-tectures where the user’s intent is derived from astructured representation and then used to deter-mine which api calls to make such as in rastogiet al.
(2020) and madotto (2020)..in this work we promote an end-to-end approachto single-domain, transaction-based dialog systemsand describe how we overcome both data andgrounding challenges described above.
in quali-tative evaluations, our models perform on par withhumans in generating verbal responses as well aspredicting api calls.
just two components formthe basis for this system: a sufﬁciently large, in-domain, labeled dataset and a pre-trained trans-former model.
combining natural language outputand structured api calls into a uniﬁed text-to-text-format allows us to leverage general purpose text-to-text transformers to train models.
speciﬁcally,we use the t5 infrastructure (raffel et al., 2019)and show that its pre-training feature has a signiﬁ-cant impact on evaluations, boosting scores by 30percent..models were trained on our tickettalk dataset(aka taskmaster-3), a movie ticketing dialog corpuswith 23,789 conversations labeled with a simpleyet unique api-based annotation schema.
thismakes it one of the largest single-domain datasetsto date.
a public release of the dataset accompaniesthis paper.
we chose movie ticketing since it isboth transaction-based and relatively complex, butour overall approach to dialog systems applies toany task-based domain.
while there is a lot ofrecent work on multi-domain task-based dialogsystems, human-like interaction for even single-domain tasks has yet to be demonstrated.
by ﬁrstsolving the problem for a single domain, we arguethat replicating the process for multiple domainswill be achievable by simply training on additionalhigh-quality datasets labeled with the same api-focused strategy..2 related work and background.
2.1 datasets.
over the past few years the nlp community hasresponded to the lack of dialog data with larger,publicly released task-oriented datasets spanningmultiple domains (wu et al., 2020; budzianowskiand vuli´c, 2019).
this underscores the crucial roledata plays in any approach to task-based dialogsystems.
multiwoz (budzianowski et al., 2018)consists of 10,420 dialogs in multiple domains and.
has become a popular benchmarking corpus forstate tracking.
it has also undergone a series ofsubsequent reﬁnements.
msr-e2e, featured in themicrosoft dialog challenge (li et al., 2018), has10,087 dialogues in three domains, movie-ticketbooking, restaurant reservation, and taxi booking.
taskmaster-1 (byrne et al., 2019) offers 13,215dialogs in six domains and has been updated witha second installment, taskmaster-2 (byrne et al.,2020), which adds 17,289 more dialogs totallingover 30,000. the schema guided dialogue dataset(rastogi et al., 2020) has 22,825 dialogs in multipledomains.
metalwoz (lee et al., 2019) has 37,884dialogs in 227 domains and is aimed at helpingmodels more accurately predict user responses innew domains.
both schema and metalwoz areused in dstc8 (kim et al., 2019).
in additionto these, serban et al.
(2018) provides a thoroughsurvey of dialog corpora released in previous years..2.2 modular vs. end-to-end architecturesin contrast to the end-to-end 1 approach, tradi-tional, modular strategies employ a division of la-bor among the components, e.g.
understanding,state tracking, dialog policy, generation, etc., whichare either largely hand-crafted or derived from train-ing individual models on labeled datasets (wenet al., 2017; young et al., 2013).
this architectureis inherently more complex than the single-modelend-to-end strategy we propose and can requiresigniﬁcantly more design and engineering.
more-over, since each module requires its own supervisedtraining dataset, it is harder to apply to differentdomains (serban et al., 2015a)..figure 1: traditional modular system.
however, the separation of functions makes themodular approach more transparent and in some re-spects easier to debug.
it has also been consideredby some to be better equipped to interact with exter-nal apis (sukhbaatar et al., 2015; wen et al., 2017).
1the term “end-to-end” is sometimes also used when de-scribing parts of modular systems (li et al., 2017; wen et al.,2017) but it is fundamentally different from the single text-to-text transformer model approach we present here..672and therefore might be better suited for task-baseddialogs.
as mentioned above, we show that our sin-gle model-based approach can accurately generateboth the appropriate response as well as predict thecorrect api call at the right time.
earlier work byandreas et al.
(2020) and hosseini-asl et al.
(2020)employs a similar modeling approach to predict di-alog state in task-based dialogs, which can be seenas a precursor to our api call prediction strategy..figure 2: simpliﬁed end-to-end system.
3 the tickettalk dataset.
3.1 overview.
the tickettalk movie ticketing dataset was createdusing the self-dialog collection method (krauseet al., 2017; moghe et al., 2018; byrne et al., 2019)in which a paid crowd-sourced worker writes bothsides of the dialog (i.e.
both customer and tick-eting agent turns) based on a particular scenarioand set of instructions.
following the annotationstrategy used for taskmaster-1 (byrne et al., 2019),labels are limited to basic entities and events (i.e.
api calls).
the dataset was created by over 4000unique, native or near-native us english speak-ers.
further demographic information (e.g.
gender,dialect, etc.)
is not known, and no personal identiﬁ-able information was gathered..stat type.
dialogs.
total turns.
unique tokens.
avg.
turns per dialog.
avg.
tokens per turn.
value.
23,789.
481,632.
62,868.
20.25.
10.35.unique named entities.
57,285.table 1: tickettalk dataset statistics.
the rationale for limiting dialogs to a single do-main (movie ticketing) is based on our hypothesisthat human-level performance in terms of both re-sponse generation and api call prediction for aparticular task requires larger (i.e.
10,000+), morediverse datasets than are currently available.
in.
other words, carefully curated, annotated datasetsthat cover all the idiosyncrasies of a single task ortransaction are a key factor in model performance.
concern about the cost and efﬁciency of creatingthese larger corpora has led some researchers tolook for approaches that alleviate dependencieson annotated data (budzianowski and vuli´c, 2019;wen et al., 2017).
however, signiﬁcant time and ex-pense can be saved when assembling these corporaby simplifying the collection and annotation proce-dures.
in addition, little to no training is requiredfor workers to be able to perform consistently well..3.2 collection methodology.
using self-dialogs (where a worker creates thewhole conversation, both user and agent turns) facil-itates building large and linguistically rich datasetssince it is both simple and cost effective, and al-lows users to draw on their lifetime of conversa-tional experiences.
this in turn ensures the modelcan handle the wide range of human conversationalbehaviors that emerge in natural dialog.
for thisproject we extended the self-dialog to include overthree dozen sets of user instructions to generate awider variety of conversations, from open-endedprompts to more speciﬁc instructions that requirespeciﬁc types of exchanges.
for example, one setsimply instructs workers to “write the transcrip-tion of a conversation” in which a person makes asuccessful ticket transaction with a booking agent.
this allows dialog creators to express their uniqueview of what a typical movie ticketing transactionwould be, structuring each conversation how theysee ﬁt.
they are also instructed to ﬁnd real valuesfor required details (i.e.
slots) such as time, date,theater, movie, etc.
using a movie or theater siteof their choice for a speciﬁc location.
this ensuresthe dataset has a large and diverse kb.
in contrast,the more restrictive sets of instructions focus onspeciﬁc sub-dialogs for error handling, changinga detail, entity resolution, and the like.
in suchcases we often provide a limited kb with one ormore values for all the details so the worker canfocus on the primary task of creating a realisticset of exchanges for this type of interaction.
in athird type of scenario, the conversation is partiallycompleted and the user’s task is focused on a veryspeciﬁc part of the exchange.
this allows us to “ﬁllholes” in the data quickly and cost effectively.
thatis, we can create large numbers of short, conver-sational examples that the model does not handle.
673adequately and then retrain for better results..3.3 annotation.
dialog data annotation can be complex and timeconsuming even for trained linguists as it typicallyinvolves carefully and consistently labeling dia-log states, user intents, and dialog acts, amongothers (henderson et al., 2013; wen et al., 2017;budzianowski et al., 2018).
the api-targeted ap-proach is far more straightforward since only basicentities (e.g.
name, time, number of tickets, the-ater, movie attributes, etc.)
and api calls (e.g.
toﬁnd theaters, movies, and show times, book tick-ets, etc.)
are labeled.
the task is therefore easierto learn, faster to complete, and cheaper to run.
moreover, as we discuss below, it ﬁts well withthe text-to-text format we use in our approach totransaction-based dialog systems.
fifteen workersperformed the annotations using a web-based toolthat allows for only well-formed labels.
to labelan api call, the api name is ﬁrst selected which inturn creates the correct set of possible (arg name,arg value) pairs to choose from, both for inputs andresponses.
this ensures that the model is trainedon syntactically well formed api calls.
no anno-tations were removed from the dialogs.
the fullannotation schema is included with the dataset re-lease at https://git.io/jl8an..4 a novel end-to-end approach.
4.1 overview.
we implement a new approach to end-to-end dia-log systems by combining natural language outputand structured api calls into a uniﬁed text-to-textformat where the input and output are always textstrings.
this allows us to leverage widely available,state of the art, general purpose text-to-text trans-formers as the foundation of our system.
speciﬁ-cally, we used the publicly available text-to-texttransfer transformer (t5) (raffel et al., 2019) totrain our models.
the t5 framework was designedspeciﬁcally to explore transfer learning techniquesfor nlp and includes pre-training on the colossalclean crawled corpus (c4), composed of hun-dreds of gigabytes of web-based english text (raf-fel et al., 2019).
the original pre-training objectivefor the c4 corpus in the t5 framework was a de-noising task, i.e.
recovering missing words fromthe input.
since this type of task scales well tomultiple downstream tasks, we used our custominputs/targets from the tickettalk dataset to repre-.
sent an end-to-end task based dialog system andultimately achieve positive results..4.2 setup.
we use t5-base (raffel et al., 2019) as our pre-trained model, which follows the transformer archi-tecture (vaswani et al., 2017) and consists of 220mparameters.
it was pre-trained on the large scalec4 dataset mentioned above for 1m steps with aspan corruption objective.
we ﬁne-tune this modelon the taskmaster-3 dataset for 40000 steps witha constant learning rate of 0.001 using 16 tpu v3chips.
the batch size was set to 131,072 tokensper batch.
the maximum input sequence lengthand output length were set to 1024 and 256 tokensrespectively..4.3 model and implementation.
the goal of our model is to generate a text stringthat either serves as a verbal response to the user orthat contains one or more api calls with the datarequired at the current stage of the conversation.
verbal responses come in two ﬂavors: those thatdepend on a particular api call details and thosethat do not.
for example, when an api is invokedto ﬁnd theater names for a given movie and loca-tion, the details returned from the api call mustbe correctly incorporated into the system’s next re-sponse, e.g.
“i found two theaters, amc 20 andcentury city 16.” in contrast, other verbal outputs,e.g.
“what city do you plan to see the movie in?”are derived from the overall conversation history.
given the required text-to-text format used inour approach, we identify the type and function ofeach string by converting the annotations to a set oftokens.
as shown in table 2 and 3, tokens identifythe speaker, i.e.
user vs. agent, the string type i.e.
utterance vs. api call, and the details of each apicall, both names as well as input parameters andvalues, and response parameters and values.
wealso tag the conversation “context” which separatesthe most recent turn from previous turns.
our tokenkey is shown in table 2..the ﬁrst step is to use tokens to represent theuser and agent interactions, providing speaker in-formation to the model by the use of ”<u>” and”<a>”.
we then convert any api invocations intotheir text equivalent using tokens for marking apinames, argument types and values, i.e.
”<pn>”,”<pan>”, etc.
the results of these two steps areshown in table 3..674useragentprogram nameprogram argument nameprogram argument valueprogram response.
uapnpanpavprpran program response argument namepravprogram response argument valuecconversation context.
table 2: tokens identifying string type and function.
<u><a>.
<u>api call:.
i’d like to watch a movie.
sure.
i can help you with that.
what kind of movies are you interested in?
are there any good action movies?
<pn>ﬁnd movies<pan>name.genre<pav>action.
response: <pr>ﬁnd movies.
<pran>name.movie<prav>john wick<prav>jack ryani found john wick and jack ryan..<a>.
table 3: speaker turns and api calls identiﬁed with tokens.
the next step is to create the model inputs andtargets.
we use the following algorithm to accom-plish this:.
1. initialize conversation context to an empty.
(a) if the sentence is a user utterance (<u>)or a program response(<pr>), add it tothe model input along with the conversa-tion context (if present)..(b) if the sentence is an agent utterance(<a>) or program invocation (<pn>),add it to the model target..(c) if both model input and target have beencreated, output the (input, target) pairand update the conversation context toreﬂect this..(d) continue (2) to generate the next input,.
target pair..using the these rules, the model inputs and tar-.
gets are generated as in table 4..once the model has been trained on inputs andtargets, we can use the system to accomplish tasksin the following manner:.
3..inputs.
targets.
<u>i’d like to watch amovie..<a>sure.
i can help youwith that.
what kind ofmovies are you interestedin?.
<pn>ﬁnd movies<pan>name.genre<pav>action.
<a>i found john wickand jack ryan..<u>are there any goodaction movies?
<c><u>i’d like to watch amovie.
<a>sure.
i can help youwith that.
what kind ofmovies are you interestedin?.
wick.
<pr>ﬁnd movies<pran>name.movie<prav>john<prav>jack ryan<c><u>i’d like to watch amovie.
<a>sure.
i can help youwith that.
what kind ofmovies are you interestedin?
<u>are there anyaction movies?
good<pn>ﬁnd movies<pan>name.genre<pav>action.
table 4: generating inputs vs. targets.
1. obtain user utterance and format it by adding.
the speaker token..2. provide the formatted utterance to the model..(a) if the model prediction contains the agent(<a>) token, format it and show it to theuser..i. update conversation context and.
start again from (1)..(b) if the model prediction contains the pro-.
gram (<pn>) token:.
i. extract program argument name(<pan>) and value (<pav>).
ii.
issue the api call by providing it to.
the api adapter..iii.
format api results and provide it tothe model along with the conversa-tion context.
iv.
start from (3)..this interaction lifecycle is illustrated in figure.
string..lowing:.
2. iterate through the interactions and do the fol-.
3. obtain model prediction.
675natural language responses..5 experiments.
5.1 overview.
in this section, we show how our end-to-end ap-proach to transaction-based dialog systems pro-duces verbal responses and predicts api calls withnear human-level quality and accuracy.
throughhuman qualitative evaluations, we show that twoaspects in particular, dataset size and pre-training,signiﬁcantly affect performance.
below we de-scribe our evaluation methodology followed by adetailed discussion of the experiment results..5.2 evaluation methodology.
dataset size and pre-training are key factors in cre-ating models for end-to-end dialog systems.
tounderstand the amount of data required for ourapproach, we trained four models, each on a dif-ferent number of randomly selected subsets of thetickettalk dataset, namely 5000, 7500, 10,000 and21,000 dialogs.
to measure the effect of transferlearning, we trained a second 10,000-dialog modelwithout the t5 framework’s pre-training compo-nent, setting up an a-b comparison with the pre-trained model..as mentioned earlier, our models generate threetypes of output: api calls, verbal responses basedon the results of an api call, and “plain” verbalresponses based on the conversation context (i.e.
not dependent on a particular api call response).
we set up a pair of evaluations for each type.
theﬁrst evaluation asked human raters to evaluate themodel’s output given a speciﬁc conversation his-tory (i.e.
context) while the second asked raters toevaluate the human’s response for the same set ofcontexts.
each experiment included 1000 context-response pairs of varying lengths, i.e.
some con-versation histories might have just one exchange(a user and agent turn) while others could have upto nine exchanges.
we requested three ratings foreach question distributed among a pool of about900 paid raters for a total of 3000 data points perexperiment.
table 5 and table 6 below shows asample context-response pair presented to humanraters for each type of model output..we use our “makes-sense” metric to evaluatethe model-generated responses and api call pre-dictions against the human standard.
for verbalresponses, we ask one question:.
• does the agent’s next response make sense?.
figure 3: system interaction life cycle.
4.4.invoking apis.
when we detect an api call in the output, we in-voke the api, retrieve the results, and embed theresponses in the next model input.
as shown infigure 4, each api call predicted by the modeltypically contains a generic api name, such as”ﬁnd-movies”, or ”ﬁnd-theaters”, and a list of keyvalue pairs that detail the speciﬁc parameters to beused while invoking the api, as shown in figure 4..figure 4: example api invocation (outside model).
the api call, while structured, may still includepronouns or other co-referential phrases as inputparameters.
for example, the date parameter foran api call might contain the value “tonight”, andthe location value might be “nearby”.
the reso-lution of these entities happens outside the coreinteraction layer in what can be understood as the“api adapter” (and not the actual api itself).
thisnot only helps simplify annotation, but also helpsleverage existing solutions to these well deﬁnedproblems.
this separation of the api layer is alsouseful for encapsulating all api speciﬁc artifacts,like authentication tokens, endpoint addresses anddata formatters.
in this way, the end-to-end systemis able to interact with the user to solicit detailsrelevant to the task, generate api calls to fetchdata from external knowledge sources, and use theresponses provided by the api call to construct.
676table 6: context paired with predicted api call.
5.3 results.
next responseagent: ok. do you haveany theaters in mind?.
contextcust: can you help mebook a movie ticket?
agent: yes i can.
cust: can you ﬁnd ticketsfor the movie knives out?
agent: sure!
what timedid you want to book?
cust: 5 pm would bebest..contextcust: i would like to see amovie tonight.
agent: sure.
what moviewould you like to see?
cust: i’m not really sure.
can you help me pick some-thing?
agent: no problem.
i cangive you the names of acouple of movies playing inyour area.
what city are yougoing to see the movie in?.
table 5: context paired with generated verbal response.
actionfind movies location:oak valley arkansas.
for negative answers, we give a list of reasonsraters believe it does not make sense (i.e.
off topic,repeated information, incorrect details, grammarmistakes, other).
for api call predictions there aretwo questions:.
1. do all the action types, their details, and theirorder make sense at this point in the conversa-tion?.
2. are there any actions that should be listedhere but that are missing (either as additionsor replacements)?.
again, raters are given options to choose for nega-tive answers..the ofﬂine evaluation strategy described aboveoffers scalability and minimal rater training.
how-ever, an online, interactive setup would further al-low us to evaluate the ability of the model to handleerrors in its own output (from previous predictions)and its robustness while dealing with novel inputs.
we have begun to build an interactive ui to facili-tate such evaluations and show promising results ofsuch an interaction in table 7 below.
the authorsof this paper played the user role.
the t5 modelwas trained on the full tickettalk dataset which.
includes nearly 24k dialogs.
if the model gener-ates an api call, we create a value that mimics theresponse from the api adapter and provide it to themodel before the next prediction.
we also providethe model with fake api responses (for calls likeﬁnd movies and ﬁnd theaters) containing entitiesthat have never been used in the conversations inthe tickettalk dataset..the conversation in 7 includes the exact apiresponses with intentionally made up movie the-ater names that have been provided to the model toensure they were not part of the training set.
themodel behaves correctly when provided with themade up api responses that are not in the dataset.
when the dialog ﬂow closely matches the datasetﬂows, which are signiﬁcantly diverse and varied,we can recreate interactions like this relatively eas-ily.
this particular example took two attempts togenerate.
future evaluation of our approach will in-clude this type of interactive task where testers rateboth individual as well as the overall conversation..comparing the “makes-sense” scores for model-generated vs. human-generated responses, a clearpattern of improvement emerges based on datasetsize.
table 8 presents the three types of model-generated responses evaluated: ”plain responses”(not strictly based on api results), ”responses toapis” (based on api results), and ”api calls” them-selves.
when 5k and 7.5k dialogs are used for thetraining set, scores for model-generated responseslag behind the human-generated scores by up to5.5%.
at 10k dialogs, the response scores differ byless than 2% and model-generated api predictionsoutperform human labels by 2.5%.
at 21k di-alogs, model-generated responses improve to nearhuman-level performance.
the 10k model’s apicall prediction fares better than 21k model for apilabeling, which is likely due to the fact that, asmore api call combinations are introduced, theyare harder for the model to interpret.
in contrast,adding general dialog data along with pre-trainingwill improve the model’s predictions of english ut-terances which gives the 21k model an advantagein ”plain response” scores.
as an automatic metric,we also provide the bleu score generated for eachmodel.
maximum n-gram order for computingbleu score was set to 4. the unrestricted natureof the entities in the datasets makes it much harderto create a robust automatic metric for api call.
677usermodeluserapi callapi response.
modeluserapi call.
api responsemodeluserapi call.
api responsemodeluserapi call.
api responsemodelusermodel.
usermodel.
userapi call.
api responsemodel.
[newmovie 2021, old-.
i’d like to book some tickets.
okay, what movie would you like to see?
whats in theaters?
ﬁnd movies(location: ’nearby’)name.movie:movie 2020]newmovie 2021 and oldmovie 2020what’s the rating for newmovie?
get movie attribute(name.movie: ’new-movie 2021’, attribute: ’rating.movie’)rating.movie: [7.7]7.7ok wheres it playing?
ﬁnd theaters(name.movie: ’newmovie2021’, location: ’nearby’)name.theater: [classictheater 2000]classictheater 2000are there any shows this evening?
’new-ﬁnd showtimes(name.movie:movie 2021’, name.theater:’clas-sictheater 2000’, date.showing: ’today’,time.preference: ’this evening’)time.showing: [7:10 pm, 9:40 pm]yes, there is a 7:10 pm and a 9:40 pm.
book two tickets for the 7 o clock show.
okay, i have you down for two tick-ets for the 7:10 pm show of newmovie2021 at classictheater 2000. is that cor-rect?
yes, that is correct.
okay, would you like to use your creditcard on ﬁle?
yeahbook tickets(name.movie: ’newmovie’classictheater2021’, name.theater:2000’, num.tickets: ’two’, time.showing:’7:10 pm’, date.showing:’today’,type.screening: ’any’)booking.status: [success]your tickets are conﬁrmed and they’vebeen sent to your email address..table 7: model performance in an online interactive session..predictions.
this is compounded by the fact thatany given dialog context may allow for differentsets of api calls..the effect of pre-training is also very clear.
aftertraining a ﬁfth model, this time without the t5framework’s pre-training feature, we see a hugedrop in evaluation scores.
as shown at the bottomof table 8, we see a decrease of 30% in modelperformance for verbal responses and about a 25%drop in api call prediction accuracy..finally, the quality of the model’s predictionstays on par with human scores throughout the.
plain resp..resp.
to apis api calls.
86.9% -5.5% 92.3% -3.9% 95.2% -2.2%.
96.2%.
97.4%.
93.8% -2.4% 95.2% -2.3%96.2%.
97.7%.
86.5% -1.9% 91.8% -1.4% 97.1% +2.5%.
93.2%.
94.6%.
size5kmodel:human: 92.4%bleu:.
56.
87.8% -3%.
7.5kmodel:human: 90.8%bleu:.
59.
10kmodel:human: 88.4%bleu:.
61.
21kmodel:human: 91.2%bleu:.
60.
10kmodel:bleu:.
89.8% -1.4% 95.3% -0.3% 93.9% +0.3%.
95.6%.
93.6%.
no pre-training.
55.8% -32.6% 63.1% -30.1% 72.8% -21.8%51.table 8: effects of training set size and pre-training on modelaccuracy.
conversation as the context grows.
figure 5 showshow the model’s ”makes sense” score stay on thesame path after each exchange..figure 5: model accuracy per dialog exchange.
6 conclusion.
we have described an end-to-end dialog sys-tem approach that shows promising potential fortransaction-based dialog applications.
in ofﬂine hu-man evaluations, our single-domain models trainedon just 10,000 dialogs generate responses and pre-dict api calls with near-human level accuracy.
a.
678key aspect of this strategy is combining naturallanguage output and structured api calls into a uni-ﬁed text-to-text format in order to leverage generalpurpose text-to-text transformers, such as the t5framework.
in this way, predicting which api callto make and when is essentially the same as generat-ing the appropriate utterance at a given point in theconversation.
the pre-training component signiﬁ-cantly boosts performance on our downstream taskof ﬁne tuning models on the our datasets.
thesecarefully curated and sufﬁciently large datasetsare also core to this strategy, and creating them isstraightforward using the self-dialog technique andsimple, api-focused annotation.
the tickettalkdataset released with this paper is one such exam-ple.
when compared with more traditional, modu-lar system architectures, our end-to-end approachshould signiﬁcantly reduce design and engineeringtime and resources needed to build task-based dia-log systems.
future work will include interactiveevaluation of current models as well as an applica-tion of this approach to multiple-domain systems..acknowledgments.
we would like to thank our colleagues danielde freitas adiwardana, noam shazeer, filipradlinksi, and pedro moreno for their discussionand insights through several iterations of this pa-per.
we thank hadar shemtov for his guidance andsupport of the overall project..references.
jacob andreas, john bufe, david burkett, charleschen, josh clausman, jean crawford, kate crim,jordan deloach, leah dorner, jason eisner, et al.
2020. task-oriented dialogue as dataﬂow synthesis.
transactions of the association for computationallinguistics, 8:556–571..antoine bordes, y-lan boureau, and jason weston.
2016. learning end-to-end goal-oriented dialog.
arxiv preprint arxiv:1605.07683..paweł budzianowski and ivan vuli´c.
2019. hello, it’sgpt-2–how can i help you?
towards the use of pre-trained language models for task-oriented dialoguesystems.
arxiv preprint arxiv:1907.05774..paweł budzianowski, tsung-hsien wen, bo-hsiangtseng, inigo casanueva, stefan ultes, osman ra-madan, and milica gaˇsi´c.
2018. multiwoz-alarge-scale multi-domain wizard-of-oz dataset forarxiv preprinttask-oriented dialogue modelling.
arxiv:1810.00278..bill byrne, karthik krishnamoorthi, saravananganesh, amit dubey, andy cedilnik, and kyu-young kim.
2020.https://github.com/google-research-datasets/taskmaster/tree/master/tm-2-2020.
sec-ond dataset in series of three..taskmaster-2..bill byrne, karthik krishnamoorthi, chinnadhuraisankar, arvind neelakantan, daniel duckworth,semih yavuz, ben goodrich, amit dubey, andycedilnik, and kyu-young kim.
2019. taskmaster-1:toward a realistic and diverse dialog dataset.
arxivpreprint arxiv:1909.05358..matthew henderson, blaise thomson, and steveyoung.
2013. deep neural network approach for thein proceedings ofdialog state tracking challenge.
the sigdial 2013 conference, pages 467–471..ehsan hosseini-asl, bryan mccann, chien-sheng wu,semih yavuz, and richard socher.
2020. a simplelanguage model for task-oriented dialogue.
arxivpreprint arxiv:2005.00796..seokhwan kim, michel galley, chulaka gunasekara,sungjin lee, adam atkinson, baolin peng, hannesschulz, jianfeng gao, jinchao li, mahmoud adada,minlie huang, luis lastras, jonathan k. kummer-feld, walter s. lasecki, chiori hori, anoop cherian,tim k. marks, abhinav rastogi, xiaoxue zang,srinivas sunkara, and raghav gupta.
2019. theeighth dialog system technology challenge..ben krause, marco damonte, mihai dobre, danielduma, joachim fainberg, federico fancellu, em-manuel kahembwe, jianpeng cheng, and bonniewebber.
2017.edina: building an open do-main socialbot with self-dialogues.
arxiv preprintarxiv:1709.09816..s lee, h schulz, a atkinson, j gao, k suleman,l el asri, m adada, m huang, s sharma, w tay,et al.
2019. multi-domain task-completion dialogchallenge.
dialog system technology challenges,8..xiujun li, yun-nung chen, lihong li, jianfeng gao,end-to-end task-and asli celikyilmaz.
2017.completion neural dialogue systems.
arxiv preprintarxiv:1703.01008..xiujun li, sarah panda, jj (jingjing) liu, and jianfenggao.
2018. microsoft dialogue challenge: buildingend-to-end task-completion dialogue systems.
inslt 2018..zhaojiang lin, andrea madotto, genta indra winata,and pascale fung.
2020. mintl: minimalist transferlearning for task-oriented dialogue systems.
arxivpreprint arxiv:2009.12005..andrea madotto.
2020. language models as few-shotlearner for task-oriented dialogue systems.
arxivpreprint arxiv:2008.06239..679nikita moghe, siddhartha arora, suman banerjee, andmitesh m khapra.
2018. towards exploiting back-ground knowledge for building conversation sys-tems.
arxiv preprint arxiv:1809.08205..the european chapter of the association for compu-tational linguistics: volume 1, long papers, pages438–449, valencia, spain.
association for computa-tional linguistics..baolin peng, chunyuan li, jinchao li, shahin shayan-deh, lars liden, and jianfeng gao.
2020. soloist:few-shottask-oriented dialog with a single pre-trained auto-regressive model..chien-sheng wu, steven hoi, richard socher, andcaiming xiong.
2020. tod-bert: pre-trained naturallanguage understanding for task-oriented dialogues.
arxiv preprint arxiv:2004.06871..steve young, milica gaˇsi´c, blaise thomson, and ja-son d williams.
2013. pomdp-based statistical spo-ken dialog systems: a review.
proceedings of theieee, 101(5):1160–1179..colin raffel, noam shazeer, adam roberts, katherinelee, sharan narang, michael matena, yanqi zhou,wei li, and peter j liu.
2019. exploring the limitsof transfer learning with a uniﬁed text-to-text trans-former.
arxiv preprint arxiv:1910.10683..abhinav rastogi, xiaoxue zang, srinivas sunkara,raghav gupta, and pranav khaitan.
2020. towardsscalable multi-domain conversational agents: theschema-guided dialogue dataset.
in proceedings ofthe aaai conference on artiﬁcial intelligence, vol-ume 34, pages 8689–8696..iulian v serban, alessandro sordoni, yoshua bengio,aaron courville, and joelle pineau.
2015a.
build-ing end-to-end dialogue systems using generative hi-erarchical neural network models.
arxiv preprintarxiv:1507.04808..iulian v serban, alessandro sordoni, yoshua ben-gio, aaron courville, and joelle pineau.
2015b.
hierarchical neural network generative models formovie dialogues.
arxiv preprint arxiv:1507.04808,7(8):434–441..iulian vlad serban, ryan lowe, peter henderson, lau-rent charlin, and joelle pineau.
2018. a survey ofavailable corpora for building data-driven dialoguesystems: the journal version.
dialogue & dis-course, 9(1):1–49..sainbayar sukhbaatar, jason weston, rob fergus, et al.
2015. end-to-end memory networks.
in advancesin neural information processing systems, pages2440–2448..ilya sutskever, oriol vinyals, and quoc v le.
2014.sequence to sequence learning with neural networks.
advances in neural information processing systems,27:3104–3112..ashish vaswani, noam shazeer, niki parmar, jakobuszkoreit, llion jones, aidan n gomez, łukaszkaiser, and illia polosukhin.
2017. attention is allyou need.
advances in neural information process-ing systems, 30:5998–6008..oriol vinyals and quoc le.
2015. a neural conversa-tional model.
arxiv preprint arxiv:1506.05869..tsung-hsien wen, david vandyke, nikola mrkˇsi´c,milica gaˇsi´c, lina m. rojas-barahona, pei-hao su,stefan ultes, and steve young.
2017. a network-based end-to-end trainable task-oriented dialoguein proceedings of the 15th conference ofsystem..680