a neural model for joint document and snippet ranking in questionanswering for large document collections.
dimitris pappas1,2 and ion androutsopoulos1.
1department of informatics, athens university of economics and business, greece1pappasd@aueb.gr,ion@aueb.gr2institute for language and speech processing, research center ‘athena’, greece2dpappas@athenarc.gr.
abstract.
question answering (qa) systems for largedocument collections typically use pipelinesthat (i) retrieve possibly relevant documents,(ii) re-rank them, (iii) rank paragraphs or othersnippets of the top-ranked documents, and (iv)select spans of the top-ranked snippets as ex-act answers.
pipelines are conceptually simple,but errors propagate from one component tothe next, without later components being ableto revise earlier decisions.
we present an ar-chitecture for joint document and snippet rank-ing, the two middle stages, which leverages theintuition that relevant documents have goodsnippets and good snippets come from rele-vant documents.
the architecture is generaland can be used with any neural text relevanceranker.
we experiment with two main instan-tiations of the architecture, based on posit-drmm (pdrmm) and a bert-based ranker..experiments on biomedical data from bioasqshow that our joint models vastly outperformthe pipelines in snippet retrieval, the main goalfor qa, with fewer trainable parameters, alsoremaining competitive in document retrieval.
furthermore, our joint pdrmm-based modelis competitive with bert-based models, de-spite using orders of magnitude fewer param-eters.
these claims are also supported by hu-man evaluation on two test batches of bioasq.
to test our key ﬁndings on another dataset,we modiﬁed the natural questions dataset sothat it can also be used for document and snip-pet retrieval.
our joint pdrmm-based modelagain outperforms the corresponding pipelinein snippet retrieval on the modiﬁed naturalquestions dataset, even though it performsworse than the pipeline in document retrieval.
we make our code and the modiﬁed naturalquestions dataset publicly available..1.introduction.
question answering (qa) systems that search largedocument collections (voorhees, 2001; tsatsaro-.
nis et al., 2015; chen et al., 2017) typically usepipelines operating at gradually ﬁner text granulari-ties.
a fully-ﬂedged pipeline includes componentsthat (i) retrieve possibly relevant documents typi-cally using conventional information retrieval (ir);(ii) re-rank the retrieved documents employing acomputationally more expensive document ranker;(iii) rank the passages, sentences, or other ‘snip-pets’ of the top-ranked documents; and (iv) selectspans of the top-ranked snippets as ‘exact’ answers.
recently, stages (ii)–(iv) are often pipelined neuralmodels, trained individually (hui et al., 2017; panget al., 2017; lee et al., 2018; mcdonald et al., 2018;pandey et al., 2019; mackenzie et al., 2020; sekuli´cet al., 2020).
although pipelines are conceptuallysimple, errors propagate from one component tothe next (hosein et al., 2019), without later com-ponents being able to revise earlier decisions.
forexample, once a document has been assigned alow relevance score, ﬁnding a particularly relevantsnippet cannot change the document’s score..we propose an architecture for joint documentand snippet ranking, i.e., stages (ii) and (iii), whichleverages the intuition that relevant documents havegood snippets and good snippets come from rele-vant documents.
we note that modern web searchengines display the most relevant snippets of thetop-ranked documents to help users quickly iden-tify truly relevant documents and answers (sultanet al., 2016; xu et al., 2019; yang et al., 2019a).
the top-ranked snippets can also be used as a start-ing point for multi-document query-focused sum-marization, as in the bioasq challenge (tsatsaro-nis et al., 2015).
hence, methods that identify goodsnippets are useful in several other applications,apart from qa.
we also note that many neural mod-els for stage (iv) have been proposed, often calledqa or machine reading comprehension (mrc)models (kadlec et al., 2016; cui et al., 2017; zhanget al., 2020), but they typically search for answers.
proceedingsofthe59thannualmeetingoftheassociationforcomputationallinguisticsandthe11thinternationaljointconferenceonnaturallanguageprocessing,pages3896–3907august1–6,2021.©2021associationforcomputationallinguistics3896only in a particular, usually paragraph-sized snip-pet, which is given per question.
for qa systemsthat search large document collections, stages (ii)and (iii) are also important, if not more important,but have been studied much less in recent years,and not in a single joint neural model..the proposed joint architecture is general andcan be used in conjunction with any neural text rel-evance ranker (mitra and craswell, 2018).
givena query and n possibly relevant documents fromstage (i), the neural text relevance ranker scores allthe snippets of the n documents.
additional neu-ral layers re-compute the score (ranking) of eachdocument from the scores of its snippets.
otherlayers then revise the scores of the snippets takinginto account the new scores of the documents.
theentire model is trained to jointly predict documentand snippet relevance scores.
we experiment withtwo main instantiations of the proposed architec-ture, using posit-drmm (mcdonald et al., 2018),hereafter called pdrmm, as the neural text ranker,or a bert-based ranker (devlin et al., 2019).
weshow how both pdrmm and bert can be used toscore documents and snippets in pipelines, thenhow our architecture can turn them into modelsthat jointly score documents and snippets..experimental results on biomedical data frombioasq (tsatsaronis et al., 2015) show thejoint models vastly outperform the correspondingpipelines in snippet extraction, with fewer train-able parameters.
although our joint architecture isengineered to favor retrieving good snippets (asa near-ﬁnal stage of qa), results show that thejoint models are also competitive in document re-trieval.
we also show that our joint version ofpdrmm, which has the fewest parameters of allmodels and does not use bert, is competitive tobert-based models, while also outperforming thebest system of bioasq 6 (brokos et al., 2018) inboth document and snippet retrieval.
these claimsare also supported by human evaluation on twotest batches of bioasq 7 (2019).
to test our keyﬁndings on another dataset, we modiﬁed naturalquestions (kwiatkowski et al., 2019), which onlyincludes questions and answer spans from a sin-gle document, so that it can be used for documentand snippet retrieval.
again, our joint pdrmm-based model largely outperforms the correspondingpipeline in snippet retrieval on the modiﬁed nat-ural questions, though it does not perform betterthan the pipeline in document retrieval, since the.
joint model is geared towards snippet retrieval, i.e.,even though it is forced to extract snippets fromfewer relevant documents.
finally, we show thatall the neural pipelines and joint models we consid-ered improve the bm25 ranking of traditional ir onboth datasets.
we make our code and the modiﬁednatural questions publicly available.1.
2 methods.
2.1 document ranking with pdrmm.
our starting point is posit-drmm (mcdonaldet al., 2018), or pdrmm, a differentiable extensionof drmm (guo et al., 2016) that obtained the bestdocument retrieval results in bioasq 6 (brokoset al., 2018).
mcdonald et al.
(2018) also reportedit performed better than drmm and several otherneural rankers, including pacrr (hui et al., 2017).
given a query q = (cid:104)q1, .
.
.
, qn(cid:105) of n queryterms (q-terms) and a document d = (cid:104)d1, .
.
.
, dm(cid:105)of m terms (d-terms), pdrmm computes context-sensitive term embeddings c(qi) and c(di) fromthe static (e.g., word2vec) embeddings e(qi) ande(di) by applying two stacked convolutional layerswith trigram ﬁlters, residuals (he et al., 2016), andzero padding to q and d, respectively.2 pdrmmthen computes three similarity matrices s1, s2, s3,each of dimensions n × m (fig.
1).
each elementsi,j of s1 is the cosine similarity between c(qi) andc(dj).
s2 is similar, but uses the static word em-beddings e(qi), e(dj).
s3 uses one-hot vectors forqi, dj, signaling exact matches.
three row-wisepooling operators are then applied to s1, s2, s3:max-pooling (to obtain the similarity of the bestmatch between the q-term of the row and any ofthe d-terms), average pooling (to obtain the aver-age match), and average of k-max (to obtain theaverage similarity of the k best matches).3 we thusobtain three scores from each row of each similaritymatrix.
by concatenating row-wise the scores fromthe three matrices, we obtain a new n × 9 matrixs(cid:48) (fig.
1).
each row of s(cid:48) indicates how well thecorresponding q-term matched any of the d-terms,using the three different views of the terms (one-hot, static, context-aware embeddings).
each rowof s(cid:48) is then passed to a multi-layer perceptron.
1see http://nlp.cs.aueb.gr/publications..html for links to the code and data..2mcdonald et al.
(2018) use a bilstm encoder instead ofconvolutions.
we prefer the latter, because they are faster, andwe found that they do not degrade performance..3we added average pooling to pdrmm to balance the other.
pooling operators that favor long documents..3897figure 1: pdrmm for document scoring.
the samemodel (with different trained parameters) also scoressentences in the pdrmm+pdrmm pipeline and thejoint jpdrmm model (adding the layers of fig.
2)..(mlp) to obtain a single match score per q-term..each context aware q-term embedding is alsoconcatenated with the corresponding idf score(bottom left of fig.
1) and passed to another mlpthat computes the importance of that q-term (wordswith low idfs may be unimportant).
let v be thevector containing the n match scores of the q-terms,and u the vector with the corresponding n impor-tance scores (bottom right of fig.
1).
the initialrelevance score of the document is ˆr(q, d) = vt u.then ˆr(q, d) is concatenated with four extra fea-tures: z-score normalized bm25 (robertson andzaragoza, 2009); percentage of q-terms with exactmatch in d (regular and idf weighted); percentageof q-term bigrams matched in d. an mlp computesthe ﬁnal relevance r(q, d) from the 5 features..neural rankers typically re-rank the top n doc-uments of a conventional ir system.
we use thesame bm25-based ir system as mcdonald et al.
pdrmm is trained on triples (cid:104)q, d, d(cid:48)(cid:105),(2018).
where d is a relevant document from the top nof q, and d(cid:48) is a random irrelevant document fromthe top n .
we use hinge loss, requiring the rele-vance of d to exceed that of d(cid:48) by a margin..2.2 pdrmm-based pipelines for document.
and snippet ranking.
brokos et al.
(2018) used the ‘basic cnn’ (bcnn)of yin et al.
(2016) to score (rank) the sentencesof the re-ranked top n documents.
the resultingpipeline, pdrmm+bcnn, had the best documentand snippet results in bioasq 6, where snippetswere sentences.
hence, pdrmm+bcnn is a rea-sonable document and snippet retrieval baselinepipeline.
in another pipeline, pdrmm+pdrmm,we replace bcnn by a second instance of pdrmmthat scores sentences.
the second pdrmm instance.
figure 2: final layers of jpdrmm and jbert.
the in-put sentence scores are generated by pdrmm (fig.
1)or bert (fig.
3) now applied to document sentences.
the document’s score is obtained from the score of itsbest sentence and external features, and is also used torevise the sentence scores.
training jointly minimizesdocument and sentence loss..is the same as when scoring documents (fig.
1),but the input is now the query (q) and a single sen-tence (s).
given a triple (cid:104)q, d, d(cid:48)(cid:105) used to train thedocument-scoring pdrmm, the sentence-scoringpdrmm is trained to predict the true class (rele-vant, irrelevant) of each sentence in d and d(cid:48) usingcross entropy loss (with a sigmoid on r(q, s)).
aswhen scoring documents, the initial relevance scoreˆr(q, s) is combined with extra features using anmlp, to obtain r(q, s).
the extra features are nowdifferent: character length of q and s, number ofshared tokens of q and s (with/without stop-words),sum of idf scores of shared tokens (with/withoutstop-words), sum of idf scores of shared tokensdivided by sum of idf scores of q-terms, numberof shared token bigrams of q and s, bm25 score ofs against the sentences of d and d(cid:48), bm25 score ofthe document (d or d(cid:48)) that contained s. the twopdrmm instances are trained separately..2.3.joint pdrmm-based models fordocument and snippet ranking.
given a document d with sentences s1, .
.
.
, sk anda query q, the joint document/snippet ranking ver-sion of pdrmm, called jpdrmm, processes sep-arately each sentence si of d, producing a rele-vance score r(q, si) per sentence, as when pdrmmscores sentences in the pdrmm+pdrmm pipeline.
the highest sentence score maxi r(q, si) is con-catenated (fig.
2) with the extra features that areused when pdrmm ranks documents, and an mlpproduces the document’s score.4jpdrmm thenrevises the sentence scores, by concatenating thescore of each sentence with the document score.
4we also tried alternative mechanisms to obtain the doc-ument score from the sentence scores, including average ofk-max sentence scores and hierarchical rnns (yang et al.,2016), but they led to no improvement..3898and passing each pair of scores to a dense layerto compute a linear combination, which becomesthe revised sentence score.
notice that jpdrmmis mostly based on scoring sentences, since themain goal for qa is to obtain good snippets (almostﬁnal answers).
the document score is obtainedfrom the score of the document’s best sentence(and external features), but the sentence scores arerevised, once the document score has been obtained.
we use sentence-sized snippets, for compatibilitywith bioasq, but other snippet granularities (e.g.,paragraph-sized) could also be used..jpdrmm is trained on triples (cid:104)q, d, d(cid:48)(cid:105), whered, d(cid:48) are relevant and irrelevant documents, respec-tively, from the top n of query q, as in the originalpdrmm; the ground truth now also indicates whichsentences of the documents are relevant or irrele-vant, as when training pdrmm to score sentencesin pdrmm+pdrmm.
we sum the hinge loss of dand d(cid:48) and the cross-entropy loss of each sentence.5we also experiment with a jpdrmm version thatuses a pre-trained bert model (devlin et al., 2019)to obtain input token embeddings (of wordpieces)instead of the more conventional pre-trained (e.g.,word2vec) word embeddings that jpdrmm usesotherwise.
we call it bjpdrmm if bert is ﬁne-tuned when training jpdrmm, and bjpdrmm-nfif bert is not ﬁne-tuned.
in another variant of bjp-drmm, called bjpdrmm-adapt, the input em-bedding of each token is a linear combination of allthe embeddings that bert produces for that tokenat its different transformer layers.
the weights ofthe linear combination are learned via backprop-agation.
this allows bjpdrmm-adapt to learnwhich bert layers it should mostly rely on whenobtaining token embeddings.
previous work hasreported that representations from different bertlayers may be more appropriate for different tasks(rogers et al., 2020).
bjpdrmm-adapt-nf is thesame as bjpdrmm-adapt, but bert is not ﬁne-tuned; the weights of the linear combination ofembeddings from bert layers are still learned..2.4 pipelines and joint models based on.
ranking with bert.
the bjpdrmm model we discussed above and itsvariants are essentially still jpdrmm, which in turninvokes the pdrmm ranker (fig.
1, 2); bert isused only to obtain token embeddings that are fed.
5additional experiments with jpdrmm, reported in theappendix, indicate that further performance gains are possibleby tuning the weights of the two losses..figure 3: document scoring with bert.
the samemodel scores sentences in jbert (adding the layers offig.
2), but with an mlp replacing the ﬁnal dense layer..to jpdrmm.
bert as a ranker, replacing pdrmm..instead, in this subsection we use.
for document ranking alone (when not cosider-ing snippets), we feed bert with pairs of questionsand documents (fig.
3).
bert’s top-layer embed-ding of the ‘classiﬁcation’ token [cls] is concate-nated with external features (the same as whenscoring documents with pdrmm, section 2.1), anda dense layer again produces the document’s score.
we ﬁne-tune the entire model using triples (cid:104)q, d, d(cid:48)(cid:105)with a hinge loss between d and d(cid:48), as when train-ing pdrmm to score documents.6.
our two pipelines that use bert for documentranking, bert+bcnn and bert+pdrmm, arethe same as pdrmm+bcnn and pdrmm+pdrmm(section 2.2), respectively, but use the bert ranker(fig.
3) to score documents, instead of pdrmm.
the joint jbert model is the same as jpdrmm,but uses the bert ranker (fig.
3), now applied tosentences, instead of pdrmm (fig.
1), to obtain theinitial sentence scores.
the top layers of fig.
2 arethen used, as in all joint models, to obtain the docu-ment score from the sentence scores and revise thesentence scores.
similarly to bjpdrmm, we alsoexperimented with variations of jbert, which donot ﬁne-tune the parameters of bert (jbert-nf),use a linear combination (with trainable weights)of the [cls] embeddings from all the bert layers(jbert-adapt), or both (jbert-adapt-nf)..2.5 bm25+bm25 baseline pipeline.
we include a bm25+bm25 pipeline to measure theimprovement of the proposed models on conven-tional ir engines.
this pipeline uses the question.
6we use the pre-trained uncased bert base of devlinet al.
(2019).
the ‘documents’ of the bioasq dataset areconcatenated titles and abstracts.
most question-documentpairs do not exceed bert’s max.
length limit of 512 word-pieces.
if they do, we truncate documents.
the same approachcould be followed in the modiﬁed natural questions dataset,where ‘documents’ are wikipedia paragraphs, but we did notexperiment with bert-based models on that dataset..3899as a query to the ir engine and selects the nd docu-ments with the highest bm25 scores.7 the nd doc-uments are then split into sentences and bm25 isre-computed, this time over all the sentences of thend documents, to retrieve the ns best sentences..3 experiments.
3.1 data and experimental setup.
bioasq data and setup following mcdonaldet al.
(2018) and brokos et al.
(2018), we ex-periment with data from bioasq (tsatsaroniset al., 2015), which provides english biomed-ical questions, relevant documents from med-line/pubmed8, and relevant snippets (sentences),prepared by biomedical experts.
this is the onlyprevious large-scale ir dataset we know of that in-cludes both gold documents and gold snippets.
weuse the bioasq 7 (2019) training dataset, whichcontains 2,747 questions, with 11 gold documentsand 14 gold snippets per question on average.
weevaluate on test batches 1–5 (500 questions in to-tal) of bioasq 7.9 we measure mean averageprecision (map) (manning et al., 2008) for docu-ment and snippet retrieval, which are the ofﬁcialbioasq evaluation measures.
the document col-lection contains approx.
18m articles (concatenatedtitles and abstracts only, discarding articles with noabstracts) from the medline/pubmed ‘baseline’2018 dataset.
in pdrmm and bcnn, we use thebiomedical word2vec embeddings of mcdonaldet al.
(2018).
we use the galago10 ir engine toobtain the top n = 100 documents per query.
af-ter re-ranking, we return nd = 10 documents andns = 10 sentences, as required by bioasq.
wetrain using adam (kingma and ba, 2015).
hyper-parameters were tuned on held-out validation data..natural questions data and setup even thoughthere was no other large-scale ir dataset providingmultiple gold documents and snippets per ques-tion, we needed to test our best models on a seconddataset, other than bioasq.
therefore we modi-ﬁed the natural questions dataset (kwiatkowskiet al., 2019) to a format closer to bioasq’s.
eachinstance of natural questions consists of an html.
7in each experiment, the same ir engine and bm25 hyper-parameters are used in all other methods.
all bm25 hyper-parameters are tuned on development data..8https://www.ncbi.nlm.nih.gov/pubmed9 bioasq 8 (2020) was ongoing during this work, hence wecould not use its data for comparisons.
see also the discussionof bioasq results after expert inspection in section 3.2.
10www.lemurproject.org/galago.php.
document of wikipedia and a question.
the an-swer to the question can always be found in thedocument as if a perfect retrieval engine were used.
a short span of html source code is annotatedby humans as a ‘short answer’ to the question.
alonger span of html source code that includes theshort answer is also annotated, as a ‘long answer’.
the long answer is most commonly a paragraph ofthe wikipedia page.
in the original dataset, morethan 300,000 questions are provided along withtheir corresponding wikipedia html documents,short answer and long answer spans.
we modiﬁednatural questions to ﬁt the bioasq setting.
fromevery wikipedia html document in the originaldataset, we extracted the paragraphs and indexedeach paragraph separately to an elasticsearch11 in-dex, which was then used as our retrieval engine.
we discarded all the tables and ﬁgures of the htmldocuments and any question that was answered bya paragraph containing a table.
for every question,we apply a query to our retrieval engine and re-trieve the ﬁrst n = 100 paragraphs.
we treat eachparagraph as a document, similarly to the bioasqsetting.
for each question, the gold (correct) docu-ments are the paragraphs (at most two per question)that were included in the long answers of the origi-nal dataset.
the gold snippets are the sentences (atmost two per question) that overlap with the shortanswers of the original dataset.
we discard ques-tions for which the retrieval engine did not manageto retrieve any of the gold paragraphs in its top 100paragraphs.
we ended up with 110,589 questionsand 2,684,631 indexed paragraphs.
due to lack ofcomputational resources, we only use 4,000 ques-tions for training, 400 questions for development,and 400 questions for testing, but we make the en-tire modiﬁed natural questions dataset publiclyavailable.
hyper-parameters were again tuned onheld-out validation data.
all other settings were asin the bioasq experiments..3.2 experimental results.
bioasq results table 1 reports document andsnippet map scores on the bioasq dataset, alongwith the trainable parameters per method.
for com-pleteness, we also show recall at 10 scores, but webase the discussion below on map, the ofﬁcial mea-sure of bioasq, which also considers the rankingof the 10 documents and snippets bioasq allowsparticipants to return.
the oracle re-ranks the n.11www.elastic.co/products/elasticsearch.
3900methodbm25 +bm25pdrmm+bcnnpdrmm+pdrmmjpdrmmbert+bcnnbert+pdrmmbjpdrmmbjpdrmm-adaptbjpdrmm-nfbjpdrmm-adapt-nfjbertjbert-adaptjbert-nfjbert-adapt-nforaclesentence pdrmm.
params421.83k11.39k5.79k109.5m109.5m88.5m88.5m3.5m3.5m85m85m6.3k6.3k05.68k.
doc.
map (%)6.867.477.476.698.798.797.596.936.847.427.937.817.907.8419.246.39.snip.
map (%)4.295.679.1615.726.079.6316.8215.7015.7717.3516.2915.9915.9916.5325.188.73.doc.
recall@10(%)48.6552.9752.9753.6855.7355.7352.2148.7748.8152.1253.4452.9452.7853.1872.6748.60.snip.
recall@10(%)4.9312.4318.4318.8313.0519.3019.5719.3817.9519.6619.8719.8719.6419.6441.1418.57.table 1: parameters learned, document and snippet map on bioasq 7, test batches 1–5, before expert inspection.
systems in the 2nd (or 3rd) zone use (or not) bert.
in each zone, best scores shown in bold.
in the 2nd and 3rdzones, we underline the results of the best pipeline, the results of jpdrmm, and the best results of the bjpdrmmand jbert variants.
the differences between the underlined map scores are statistically signiﬁcant (p ≤ 0.01)..= 100 documents (or their snippets) that bm25retrieved, moving all the relevant documents (orsnippets) to the top.
sentence pdrmm is an ab-lation of jpdrmm without the top layers (fig.
2);each sentence is scored using pdrmm, then eachdocument inherits the highest score of its snippets..pdrmm+bcnn and pdrmm+pdrmm use thesame document ranker, hence the document mapof these two pipelines is identical (7.47).
however,pdrmm+pdrmm outperforms pdrmm+bcnn insnippet map (9.16 to 5.67), even though pdrmmhas much fewer trainable parameters than bcnn,conﬁrming that pdrmm can also score sen-tences and is a better sentence ranker than bcnn.
pdrmm+bcnn was the best system in bioasq 6for both documents and snippets, i.e., it is a strongbaseline.
replacing pdrmm by bert for docu-ment ranking in the two pipelines (bert+bcnnand bert+pdrmm) increases the document mapby 1.32 points (from 7.47 to 8.79) with a marginalincrease in snippet map for bert+pdrmm (9.16 to9.63) and a slightly larger increase for bert+bcnn(5.67 to 6.07), at the expense of a massive increasein trainable parameters due to bert (and com-putational cost to pre-train and ﬁne-tune bert).
we were unable to include a bert+bert pipeline,which would use a second bert ranker for sen-tences, with a total of approx.
220m trainable pa-rameters, due to lack of computational resources..the main joint models (jpdrmm, bjpdrmm,jbert) vastly outperform the pipelines in snippetextraction, the main goal for qa (obtaining 15.72,16.82, 16.29 snippet map, respectively), though.
their document map is slightly lower (6.69, 7.59,7.93) compared to the pipelines (7.47, 8.79), butstill competitive.
this is not surprising, since thejoint models are geared towards snippet retrieval(they directly score sentences, document scores areobtained from sentence scores).
human inspectionof the retrieved documents and snippets, discussedbelow (table 2), reveals that the document mapof jpdrmm is actually higher than that of the bestpipeline (bert+pdrmm), but is penalized in ta-ble 1 because of missing gold documents..jpdrmm, which has the fewest parameters ofall neural models and does not use bert at all, iscompetitive in snippet retrieval with models thatemploy bert.
more generally, the joint modelsuse fewer parameters than comparable pipelines(see the zones of table 1).
not ﬁne-tuning bert(-nf variants) leads to a further dramatic decreasein trainable parameters, at the expense of slightlylower document and snippet map (7.59 to 6.84,and 16.82 to 15.77, respectively, for bjpdrmm,and similarly for jbert).
using linear combina-tions of token embeddings from all bert layers(-adapt variants) harms both document and snip-pet map when ﬁne-tuning bert, but is beneﬁcial inmost cases when not ﬁne-tuning bert (-nf).
thesnippet map of bjpdrmm-nf increases from 15.77to 17.35, and document map increases from 6.84to 7.42. a similar increase is observed in the snip-pet map of jbert-nf (15.99 to 16.53), but mapdecreases (7.90 to 7.84).
in the second and thirdresult zones of table 1, we underline the results ofthe best pipelines, the results of jpdrmm, and the.
3901results of the best bjpdrmm and jbert variant.
ineach zone and column, the differences between theunderlined map scores are statistically signiﬁcant(p ≤ 0.01); we used single-tailed approximaterandomization (dror et al., 2018), 10k iterations,randomly swapping in each iteration the rankingsof 50% of queries.
removing the top layers ofjpdrmm (sentence pdrmm), clearly harms perfor-mance for both documents and snippets.
the oraclescores indicate there is still scope for improvementsin both documents and snippets..bioasq results after expert inspection at theend of each bioasq annual contest, the biomedicalexperts who prepared the questions and their golddocuments and snippets inspect the responses ofthe participants.
if any of the documents and snip-pets returned by the participants are judged relevantto the corresponding questions, they are added tothe gold responses.
this process enhances the goldresponses and avoids penalizing participants forresponses that are actually relevant, but had beenmissed by the experts in the initial gold responses.
however, it is unfair to use the post-contest en-hanced gold responses to compare systems thatparticipated in the contest to systems that did not,because the latter may also return documents andsnippets that are actually relevant and are not in-cluded in the gold data, but the experts do not seethese responses and they are not included in thegold ones.
the results of table 1 were computedon the initial gold responses of bioasq 7, beforethe post-contest revision, because not all of themethods of that table participated in bioasq 7.12in table 2, we show results on the revised post-contest gold responses of bioasq 7, for those ofour methods that participated in the challenge.
weshow results on test batches 4 and 5 only (out of 5batches in total), because these were the only twobatches were all three of our methods participatedtogether.
each batch comprises 100 questions.
wealso show the best results (after inspection) of ourcompetitors in bioasq 7, for the same batches..a ﬁrst striking observation in table 2 is thatall results improve substantially after expert in-spection, i.e., all systems retrieved many relevantdocuments and snippets the experts had missed.
again, the two joint models (jpdrmm, bjpdrmm-nf) vastly outperform the bert+pdrmm pipeline.
12results without expert inspection can be obtained at anytime, using the bioasq evaluation platform.
results withexpert inspection can only be obtained during the challenge..in snippet map.
as in table 1, before expert in-spection the pipeline has slightly better documentmap than the joint models.
however, after expertinspection jpdrmm exceeds the pipeline in doc-ument map by almost two points.
bjpdrmm-nfperforms two points better than jpdrmm in snippetmap after expert inspection, though jpdrmm per-forms two points better in document map.
afterinspection, the document map of bjpdrmm-nf isalso very close to the pipeline’s.
table 2 conﬁrmsthat jpdrmm is competitive with models that usebert, despite having the fewest parameters.
all ofour methods clearly outperformed the competition..natural questions results table 3 reports re-sults on the modiﬁed natural questions dataset.
weexperiment with the best pipeline and joint modelof table 1 that did not use bert (and are compu-tationally much cheaper), i.e., pdrmm+pdrmmand jpdrmm, comparing them to the more con-ventional bm25+bm25 baseline.
since there are atmost two relevant documents and snippets per ques-tion in this dataset, we measure mean reciprocalrank (mrr) (manning et al., 2008), and recall attop 1 and 2. both pdrmm+pdrmm and jpdrmmclearly outperform the bm25+bm25 pipeline inboth document and snippet retrieval.
as in ta-ble 1, the joint jpdrmm model outperforms thepdrmm+pdrmm pipeline in snippet retrieval, butthe pipeline performs better in document retrieval.
again, this is unsurprising, since the joint modelsare geared towards snippet retrieval.
we also notethat jpdrmm uses half of the trainable parametersof pdrmm+pdrmm (table 1).
no comparison toprevious work that used the original natural ques-tions is possible, since the original dataset providesa single document per query (section 3.1)..4 related work.
neural document ranking (guo et al., 2016; huiet al., 2017; pang et al., 2017; hui et al., 2018;mcdonald et al., 2018) only recently managed toimprove the rankings of conventional ir; see lin(2019) for caveats.
document or passage rankingmodels based on bert have also been proposed,with promising results, but most use only simplis-tic task-speciﬁc layers on top of bert (yang et al.,2019b; nogueira and cho, 2019), similar to ouruse of bert for document scoring (fig.
3).
anexception is the work of macavaney et al.
(2019),who explored combining elmo (peters et al., 2018)and bert (devlin et al., 2019) with complex neu-.
3902before expert inspection.
after expert inspection.
methodbert+pdrmmjpdrmmbjpdrmm-nfbest bioasq 7 competitor.
document map7.295.166.18n/a.
snippet map7.5812.4513.89n/a.
document map14.8616.5514.6513.18.snippet map15.6121.9823.9614.98.table 2: document and snippet map (%) on bioasq 7 test batches 4 and 5 before and after post-contestexpert inspection of system responses, for methods that participated in bioasq 7. we also show the results (afterinspection) of the best other participants of bioasq 7 for the same batches..document retrieval.
snippet retrieval.
methodbm25+bm25pdrmm+pdrmmjpdrmm.
mrr recall@1 recall@2 mrr recall@1 recall@230.1840.3336.50.
29.7538.5036.00.
16.5028.2524.50.
3.7513.7519.00.
8.1922.8626.92.
7.1322.7525.25.table 3: mrr (%) and recall at top 1 and 2 (%) on the modiﬁed natural questions dataset..ral ir models, namely pacrr (hui et al., 2017),drmm (guo et al., 2016), knrm (dai et al., 2018),convknrm (xiong et al., 2017), an approachthat we also explored here by combining bertwith pdrmm in bjpdrmm and jbert.
however,we retrieve both documents and snippets, whereasmacavaney et al.
(2019) retrieve only documents..models that directly retrieve documents by in-dexing neural document representations, ratherthan re-ranking documents retrieved by conven-tional ir, have also been proposed (fan et al., 2018;ai et al., 2018; khattab and zaharia, 2020), butnone addresses both document and snippet retrieval.
yang et al.
(2019a) use bert to encode, index, anddirectly retrieve snippets, but do not consider doc-uments; indexing snippets is also computationallycostly.
lee et al.
(2019) propose a joint model fordirect snippet retrieval (and indexing) and answerspan selection, again without retrieving documents..no previous work combined document and snip-pet retrieval in a joint neural model.
this maybe due to existing datasets, which do not provideboth gold documents and gold snippets, with theexception of bioasq, which is however small bytoday’s standards (2.7k training questions, sec-tion 3.1).
for example, pang et al.
(2017) usedmuch larger clickthrough datasets from a chinesesearch engine, as well as datasets from the 2007 and2008 trec million query tracks (qin et al., 2010),but these datasets do not contain gold snippets.
squad (rajpurkar et al., 2016) and squad v.2 (ra-jpurkar et al., 2018) provide 100k and 150k ques-tions, respectively, but for each question they re-quire extracting an exact answer span from a singlegiven wikipedia paragraph; no snippet retrieval is.
performed, because the relevant (paragraph-sized)snippet is given.
ahmad et al.
(2019) provide mod-iﬁed versions of squad and natural questions,suitable for direct snippet retrieval, but do not con-sider document retrieval.
searchqa (dunn et al.,2017) provides 140k questions, along with 50 snip-pets per question.
the web pages the snippetswere extracted from, however, are not included inthe dataset, only their urls, and crawling themmay produce different document collections, sincethe contents of web pages often change, pages areremoved etc.
ms-marco (nguyen et al., 2016)was constructed using 1m queries extracted frombing’s logs.
for each question, the dataset includesthe snippets returned by the search engine for thetop-10 ranked web pages.
however the gold an-swers to the questions are not spans of particular re-trieved snippets, but were freely written by humansafter reading the returned snippets.
hence, gold rel-evant snippets (or sentences) cannot be identiﬁed,making this dataset unsuitable for our purposes..5 conclusions and future work.
our contributions can be summarized as follows:(1) we proposed an architecture to jointly rankdocuments and snippets with respect to a ques-tion, two particularly important stages in qa forlarge document collections; our architecture can beused with any neural text relevance model.
(2) weinstantiated the proposed architecture using a re-cent neural relevance model (pdrmm) and a bert-based ranker.
(3) using biomedical data (frombioasq), we showed that the two resulting jointmodels (pdrmm-based and bert-based) vastly out-perform the corresponding pipelines in snippet re-.
3903trieval, the main goal in qa for document collec-tions, using fewer parameters, and also remainingcompetitive in document retrieval.
(4) we showedthat the joint model (pdrmm-based) that does notuse bert is competitive with bert-based models,outperforming the best bioasq 6 system; our jointmodels (pdrmm- and bert-based) also outper-formed all bioasq 7 competitors.
(5) we providea modiﬁed version of the natural questions dataset,suitable for document and snippet retrieval.
(6) weshowed that our joint pdrmm-based model alsolargely outperforms the corresponding pipeline onopen-domain data (natural questions) in snippetretrieval, even though it performs worse than thepipeline in document retrieval.
(7) we showed thatall the neural pipelines and joint models we consid-ered improve the traditional bm25 ranking on bothdatasets.
(8) we make our code publicly available.
we hope to extend our models and datasets forstage (iv), i.e., to also identify exact answer spanswithin snippets (paragraphs), similar to the answerspans of squad (rajpurkar et al., 2016, 2018).
this would lead to a multi-granular retrieval task,where systems would have to retrieve relevant doc-uments, relevant snippets, and exact answer spansfrom the relevant snippets.
bioasq already in-cludes this multi-granular task, but exact answersare provided only for factoid questions and they arefreely written by humans, as in ms-marco, withsimilar limitations.
hence, appropriately modiﬁedversions of the bioasq datasets are needed..acknowledgements.
we thank ryan mcdonald for his advice in earlierstages of this work..references.
george brokos, polyvios liosis, ryan mcdonald,dimitris pappas, and ion androutsopoulos.
2018.aueb at bioasq 6: document and snippet re-trieval.
in proceedings of the 6th bioasq workshop,pages 30–39, brussels, belgium..danqi chen, adam fisch, jason weston, and antoinebordes.
2017. reading wikipedia to answer open-in proceedings of the 55th an-domain questions.
nual meeting of the association for computationallinguistics (volume 1: long papers), pages 1870–1879, vancouver, canada..yiming cui, zhipeng chen, si wei, shijin wang,ting liu, and guoping hu.
2017. attention-over-attention neural networks for reading comprehen-sion.
in proceedings of the 55th annual meeting ofthe association for computational linguistics (vol-ume 1: long papers), pages 593–602, vancouver,canada..zhuyun dai, chenyan xiong, jamie callan, andzhiyuan liu.
2018. convolutional neural networksfor soft-matching n-grams in ad-hoc search.
in pro-ceedings of the eleventh acm international confer-ence on web search and data mining, pages 126–134, marina del rey, ca..jacob devlin, ming-wei chang, kenton lee, andkristina toutanova.
2019. bert: pre-training ofdeep bidirectional transformers for language un-derstanding.
in proceedings of the 2019 conferenceof the north american chapter of the associationfor computational linguistics: human languagetechnologies, volume 1 (long and short papers),pages 4171–4186..rotem dror, gili baumer, segev shlomov, and roi re-ichart.
2018. the hitchhiker’s guide to testing sta-tistical signiﬁcance in natural language processing.
in proceedings of the 56th annual meeting of theacl (volume 1: long papers), pages 1383–1392..matthew dunn, levent sagun, mike higgins, v. ugurg¨uney, volkan cirik, and kyunghyun cho.
2017.searchqa: a new q&a dataset augmentedarxiv,with contextabs/1704.05179..from a search engine..amin ahmad, noah constant, yinfei yang, and danielcer.
2019. reqa: an evaluation for end-to-end an-in proceedings of the 2ndswer retrieval models.
workshop on machine reading for question answer-ing, pages 137–146, hong kong, china..yixing fan, jiafeng guo, yanyan lan, jun xu, chengx-iang zhai, and xueqi cheng.
2018. modeling di-verse relevance patterns in ad-hoc retrieval.
inthe 41st international acm sigir conference onresearch & development in information retrieval..qingyao ai, brendan o’connor, and w. bruce croft.
2018. a neural passage model for ad-hoc doc-in advances in information re-ument retrieval.
trieval, cham..lisa bauer, yicheng wang, and mohit bansal.
2018.commonsense for generative multi-hop question an-swering tasks.
in proceedings of the 2018 confer-ence on empirical methods in natural languageprocessing, pages 4220–4230, brussels, belgium..jiafeng guo, yixing fan, qingyao ai, and w. brucecroft.
2016. a deep relevance matching modelin proceedings of the 25thfor ad-hoc retrieval.
acm international on conference on informationand knowledge management, pages 55–64, indi-anapolis, indiana, usa..kaiming he, xiangyu zhang, shaoqing ren, and jiansun.
2016. deep residual learning for imagerecognition.
in proceedings of the ieee conference.
3904on computer vision and pattern recognition, pages770–778..stefan hosein, daniel andor, and ryan mcdonald.
2019. measuring domain portability and error prop-agation in biomedical qa.
in joint european con-ference on machine learning and knowledge dis-covery in databases, pages 686–694, wurzburg,germany..kai hui, andrew yates, klaus berberich, and gerardde melo.
2017. pacrr: a position-aware neu-ral ir model for relevance matching.
in proceed-ings of the 2017 conference on empirical methodsin natural language processing, pages 1049–1058,copenhagen, denmark..kai hui, andrew yates, klaus berberich, and gerardde melo.
2018. co-pacrr: a context-aware neuralir model for ad-hoc retrieval.
in proceedings of the11th acm international conference on web searchand data mining, pages 279–287, marina del rey,ca..rudolf kadlec, martin schmid, ondrej bajgar, and jankleindienst.
2016. text understanding with the at-tention sum reader network.
in proceedings of the54th annual meeting of the association for compu-tational linguistics (volume 1: long papers), pages908–918, berlin, germany..omar khattab and matei zaharia.
2020. colbert:efﬁcient and effective passage search via con-arxiv,textualized late interaction over bert.
abs/2004.12832..tushar khot, ashish sabharwal, and peter clark.
2019.what’s missing: a knowledge gap guided approachfor multi-hop question answering.
in proceedings ofthe 2019 conference on empirical methods in nat-ural language processing and the 9th internationaljoint conference on natural language processing(emnlp-ijcnlp), pages 2814–2828, hong kong,china.
association for computational linguistics..diederik p. kingma and jimmy ba.
2015. adam:corr,.
a method for stochastic optimization.
abs/1412.6980..tom kwiatkowski, jennimaria palomaki, olivia red-ﬁeld, michael collins, ankur parikh, chris alberti,danielle epstein, illia polosukhin, matthew kelcey,jacob devlin, kenton lee, kristina n. toutanova,llion jones, ming-wei chang, andrew dai, jakobuszkoreit, quoc le, and slav petrov.
2019. natu-ral questions: a benchmark for question answeringresearch.
transactions of the association of com-putational linguistics..jinhyuk lee, seongjun yun, hyunjae kim, miyoungko, and jaewoo kang.
2018. ranking paragraphsfor improving answer recall in open-domain ques-in proceedings of the 2018 con-tion answering.
ference on empirical methods in natural languageprocessing, pages 565–569, brussels, belgium.
as-sociation for computational linguistics..kenton lee, ming-wei chang, and kristina toutanova.
2019. latent retrieval for weakly supervised opendomain question answering.
in proceedings of the57th annual meeting of the association for com-putational linguistics, pages 6086–6096, florence,italy..jimmy lin.
2019. the neural hype and compar-sigir forum,.
isons against weak baselines.
52(2):40–51..sean macavaney, andrew yates, arman cohan,and nazli goharian.
2019. cedr: contextual-ized embeddings for document ranking.
corr,abs/1904.07094..joel mackenzie, zhuyun dai, luke gallagher, andjamie callan.
2020. efﬁciency implications of termweighting for passage retrieval, page 1821–1824.
association for computing machinery, new york,ny, usa..christopher d. manning, prabhakar raghavan, andhinrich sch¨utze.
2008. introduction to informationretrieval.
cambridge university press..ryan mcdonald, george brokos, and ion androut-sopoulos.
2018. deep relevance ranking using en-in proceed-hanced document-query interactions.
ings of the 2018 conference on empirical methodsin natural language processing, pages 1849–1860,brussels, belgium..bhaskar mitra and nick craswell.
2018. an introduc-tion to neural information retrieval.
now publish-ers..tri nguyen, mir rosenberg, xia song, jianfeng gao,saurabh tiwary, rangan majumder, and li deng.
2016. ms marco: a human generated ma-chine reading comprehension dataset.
corr,abs/1611.09268..rodrigo nogueira and kyunghyun cho.
2019. passagere-ranking with bert.
corr, abs/1901.04085..s. pandey, i. mathur, and n. joshi.
2019. informationretrieval ranking using machine learning techniques.
in 2019 amity international conference on artiﬁcialintelligence (aicai), pages 86–92..liang pang, yanyan lan, jiafeng guo, jun xu, jing-fang xu, and xueqi cheng.
2017. deeprank: anew deep architecture for relevance ranking ininformation retrieval.
in proceedings of the 2017acm on conference on information and knowledgemanagement..matthew peters, mark neumann, mohit iyyer, mattgardner, christopher clark, kenton lee, and lukezettlemoyer.
2018. deep contextualized word rep-in proceedings of the 2018 confer-resentations.
ence of the north american chapter of the associ-ation for computational linguistics: human lan-guage technologies, pages 2227–2237, new or-leans, louisiana..3905tao qin, tie-yan liu, jun xu, and hang li.
2010.letor: a benchmark collection for research on learn-ing to rank for information retrieval.
inf.
retrieval..peng xu, xiaofei ma, ramesh nallapati, and bing xi-ang.
2019. passage ranking with weak supervsion.
arxiv..pranav rajpurkar, robin jia, and percy liang.
2018.know what you don’t know: unanswerable ques-tions for squad.
in proceedings of the 56th annualmeeting of the association for computational lin-guistics (volume 2: short papers), pages 784–789,melbourne, australia..pranav rajpurkar, jian zhang, konstantin lopyrev, andpercy liang.
2016. squad: 100,000+ questions formachine comprehension of text.
in proceedings ofthe 2016 conference on empirical methods in natu-ral language processing, pages 2383–2392, austin,texas..stephen robertson and hugo zaragoza.
2009. theprobabilistic relevance framework: bm25 and be-yond.
foundations and trends in information re-trieval, 3(4):333–389..anna rogers, olga kovaleva, and anna rumshisky.
2020. a primer in bertology: what we knowabout how bert works.
transactions of the associ-ation for computational linguistics, 8..apoorv saxena, aditay tripathi, and partha taluk-dar.
2020. improving multi-hop question answeringover knowledge graphs using knowledge base em-beddings.
in proceedings of the 58th annual meet-ing of the association for computational linguistics,pages 4498–4507, online..ivan sekuli´c, amir soleimani, mohammad alianne-jadi, and fabio crestani.
2020. longformer forms marco document re-ranking task.
arxiv,abs/2009.09392..md arafat sultan, vittorio castelli, and radu florian.
2016. a joint model for answer sentence rankingand answer extraction.
transactions of the associ-ation for computational linguistics, 4:113–125..g. tsatsaronis, g. balikas, p. malakasiotis, i. parta-las, m. zschunke, m.r.
alvers, d. weissenborn,a. krithara, s. petridis, d. polychronopoulos,y. almirantis, j. pavlopoulos, n. baskiotis, p. galli-nari, t. artieres, a. ngonga, n. heino, e. gaussier,l. barrio-alvers, m. schroeder, i. androutsopou-los, and g. paliouras.
2015. an overview of thebioasq large-scale biomedical semantic index-ing and question answering competition.
bmcbioinformatics, 16(138)..ellen m. voorhees.
2001. the trec question an-natural language engineering,.
swering track.
7(4):361–378..chenyan xiong, zhuyun dai, jamie callan, zhiyuanliu, and russell power.
2017. end-to-end neuralad-hoc ranking with kernel pooling.
in proceedingsof the 40th international acm sigir conference onresearch and development in information retrieval,pages 55–64, shinjuku, tokyo, japan..wei yang, yaxiong xie, aileen lin, xingyu li,luchen tan, kun xiong, ming li, and jimmy lin.
2019a.
end-to-end open-domain question answer-ing with bertserini.
corr, abs/1902.01718..wei yang, haotian zhang, and jimmy lin.
2019b.
simple applications of bert for ad hoc docu-ment retrieval.
corr, abs/1903.10972..zhilin yang, peng qi, saizheng zhang, yoshua bengio,william cohen, ruslan salakhutdinov, and christo-pher d. manning.
2018. hotpotqa: a dataset fordiverse, explainable multi-hop question answering.
in proceedings of the 2018 conference on empiri-cal methods in natural language processing, pages2369–2380, brussels, belgium..zichao yang, diyi yang, chris dyer, xiaodong he,alex smola, and eduard hovy.
2016. hierarchicalattention networks for document classiﬁcation.
inproceedings of the 2016 conference of the northamerican chapter of the association for computa-tional linguistics: human language technologies..wenpeng yin, hinrich sch¨utze, bing xiang, andbowen zhou.
2016. abcnn: attention-based con-volutional neural network for modeling sentencepairs.
transactions of the association for compu-tational linguistics, 4..zhuosheng zhang, jun jie yang, and hai zhao.
2020.retrospective reader for machine reading compre-hension.
arxiv..appendix.
tuning the weights of the two losses and theeffect of extra features in jpdrmm.
in table 1, all joint models used the sum of thedocument and snippet loss (l = ldoc + lsnip).
bycontrast, in table 4 we use a linear combinationl = ldoc+λsniplsnip and tune the hyper-parameterλsnip ∈ {10, 1, 0.1, 0.01}.
we also try removingthe extra document and/or sentence features (fig.
1–3) to check their effect.
this experiment was per-formed only with jpdrmm, which is one of our bestjoint models and computationally much cheaperthan methods that employ bert.
as in table 1, weuse the bioasq data, but here we perform a 10-foldcross-validation on the union of the training anddevelopment subsets.
this is why the results forλsnip = 1 when using both the sentence and docu-ment extra features (row 4, in italics) are slightlydifferent than the corresponding jpdrmm resultsof table 1 (6.69 and 15.72, respectively)..3906“the most famous fountains of this kind were foundin the villa d’este, at tivoli near rome, whichfeatured a hillside of basins, fountains and jets ofwater, as well as a fountain which produced musicby pouring water into a chamber, forcing air into aseries of ﬂute-like pipes.”..to prefer the gold sentence, the model needs toknow that fontana di trevi is also very famous, butthis information is not included in the gold sentenceitself, though it is included in the next sentence:“standing 26.3 metres (86 ft) high and 49.15 metres(161.3 ft) wide, it is the largest baroque fountainin the city and one of the most famous fountains inthe world.”.
hence, some form of multi-hop qa (yang et al.,2018; bauer et al., 2018; khot et al., 2019; saxenaet al., 2020) seems to be needed to combine theinformation that fontana di trevi is in rome (ex-plicitly mentioned in the gold sentence) with infor-mation from the next sentence and, more generally,other sentences even from different documents..in the case of the question “what part of thebody is affected by mesotheliomia?” of the bioasqdataset, the gold sentence is:‘’malignant pleural mesothelioma (mpm) is a hardto treat malignancy arising from the mesothelialsurface of the pleura.”.
instead, the top sentence of bjpdrmm-adapt-nfis the following, which contains several words ofthe question, but not ‘mesothelioma’, which is themost important question term.
“for pts specialized in acute care, geriatrics and pe-diatrics, the body part most commonly affected wasthe low back, while for pts specialized in orthope-dics and neurology, the body part most commonlyaffected was the neck.”.
in this case, the gold sentence does not explicitlyconvey that the pleura is a membrane that envelopseach lung of the human body and, therefore, a partof the body.
again, this additional information canbe found in other sentences..sent.
doc.
extra extrayesyesnoyesyesnoyesyesnoyesyesnoyesyesnoyesyesnoyesyesnoyesyesno.
doc..λsnip map (%)6.23 ± 0.14101.20 ± 0.14101.18 ± 0.23106.80 ± 0.0711.35 ± 0.2417.35 ± 0.1617.85 ± 0.080.16.77 ± 0.250.17.59 ± 0.120.17.83 ± 0.070.016.61 ± 0.190.017.65 ± 0.100.01.snip.
map (%)14.73 ± 0.323.59 ± 0.452.19 ± 0.2915.42 ± 0.233.77 ± 0.7314.58 ± 0.8817.28 ± 0.2613.86 ± 1.1015.77 ± 0.6017.34 ± 0.3712.96 ± 0.2914.24 ± 1.63.table 4: jpdrmm results on bioasq 7 data for tunedweights of the two losses, with and without the ex-tra sentence and document features.
the 4th row (initalics) corresponds to the jpdrmm conﬁguration of ta-ble 1, but the results here are slightly different, becausewe used a 10-fold cross-validation on the training anddevelopment data.
the map scores are averaged overthe 10 folds.
we also report standard deviations (±)..table 4 shows that further performance gains(6.80 to 7.85 document map, 15.42 to 17.34 snip-pet map) are possible by tuning the weights ofthe two losses.
the best scores are obtained whenusing both the extra sentence and document fea-tures.
however, the model performs reasonablywell even when one of the two types of extra fea-tures is removed, with the exception of λsnip = 10.the standard deviations of the map scores overthe folds of the cross-validation indicate that theperformance of the model is reasonably stable..error analysis and limitations.
we conducted an exploratory analysis of the re-trieved snippets in the two datasets.
for eachdataset, we used the model with the best snippet re-trieval performance, i.e., jpdrmm for the modiﬁednatural questions (table 3) and bjpdrmm-adapt-nf for bioasq (table 1)..both models struggle to retrieve the gold sen-tences when the answer is not explicitly mentionedin them.
for example, the gold sentence for thequestion “what is the most famous fountain inrome?” of the natural questions dataset is:.
“the trevi fountain (italian: fontana di trevi) isa fountain in the trevi district in rome, italy, de-signed by italian architect nicola salvi and com-pleted by giuseppe pannini.”.
instead, the top sentence of jpdrmm is the follow-ing, which looks reasonably good, but mentionsfamous fountains (of a particular kind) near rome..3907