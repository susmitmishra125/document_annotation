leebert: learned early exit for bert with cross-level optimization.
wei zhu1,2 ∗1 east china normal university, shanghai, china2 pingan health tech, shanghai, china.
abstract.
pre-trained language models like bert areperformant in a wide range of natural languagethey are resource exhaus-tasks.
however,tive and computationally expensive for indus-trial scenarios.
thus, early exits are adopt-ed at each layer of bert to perform adap-tive computation by predicting easier sampleswith the ﬁrst few layers to speed up the in-ference.
in this work, to improve efﬁciencywithout performance drop, we propose a noveltraining scheme called learned early exitingfor bert (leebert).
first, we ask each exitto learn from each other, rather than learningonly from the last layer.
second, the weight-s of different loss terms are learned, thus bal-ancing off different objectives.
we formulatethe optimization of leebert as a bi-level op-timization problem, and we propose a novelcross-level optimization (clo) algorithm toimprove the optimization results.
experimentson the glue benchmark show that our pro-posed methods improve the performance ofthe state-of-the-art (sota) early exiting meth-ods for pre-trained models..1.introduction.
the last couple of years have witnessed the riseof pre-trained language models (plms), such asbert (devlin et al., 2018), gpt (radford et al.,2019), xlnet (yang et al., 2019), and albert(lan et al., 2020), etc.
by pre-training on the un-labeled corpus and ﬁne-tuning on labeled ones,bert-like models achieved considerable improve-ments in many natural language processing (nlp)tasks, such as text classiﬁcation and natural lan-guage inference (nli), sequence labeling, etc..however, these plms suffer from two problems.
the ﬁrst problem is efﬁciency.
the state-of-the-art(sotas) achievements of these models usually rely.
on very deep model architectures accompanied byhigh computational demands, impairs their prac-ticalities.
like general search engines or onlinemedical consultation services, industrial settingsprocess generally millions of requests per minute.
what makes efﬁciency more critical is that the traf-ﬁc of online services varies drastically with time.
for example, during the ﬂu season, the search re-quests of dingxiangyuan1 are ten times more thanusual.
and the number of claims during the holi-days is ﬁve to ten times more than that of the work-days for online shopping.
many servers need tobe deployed to enable bert in industrial settings,which is unbearable for many companies..second, previous literature (fan et al., 2020;michel et al., 2019; zhou et al., 2020) pointedout that large plms with dozens of stacked trans-former layers are over-parameterized and could suf-fer from the “overthinking” problem (kaya et al.,2019).
that is, for many input samples, their shal-low representations at a shallow layer are enoughto make a correct classiﬁcation.
in contrast, theﬁnal layer’s representations may be overﬁtting ordistracted by irrelevant features that do not gener-alize.
the overthinking problem leads to not onlypoor generalization but also wasted computation.
to address these issues, both the industry andacademia have devoted themselves to acceleratingplms at inference time.
standard methods includedirect network pruning (zhu and gupta, 2018; xuet al., 2020; fan et al., 2020; michel et al., 2019),knowledge distillation (sun et al., 2019; sanh et al.,2019; jiao et al., 2020), weight quantization (zhanget al., 2020; bai et al., 2020; kim et al., 2021) andadaptive inference (zhou et al., 2020; xin et al.,2020; geng et al., 2021; liu et al., 2020).
amongthem, adaptive inference has attracted much atten-tion.
given that real-world data is usually com-.
∗contact: 52205901018@stu.ecnu.edu.cn..1https://search.dxy.cn/.
proceedingsofthe59thannualmeetingoftheassociationforcomputationallinguisticsandthe11thinternationaljointconferenceonnaturallanguageprocessing,pages2968–2980august1–6,2021.©2021associationforcomputationallinguistics2968posed of easy samples and difﬁcult samples, adap-tive inference aims to deal with simple exampleswith only a small part of a plm, thus speedingup inference time on average.
the speed-up ratiocan be controlled with certain hyper-parameters tocope with drastic changes in request trafﬁc.
what’smore, it can address the over-thinking problem andimprove the model’s generalization ability..early exiting is one of the most crucial adap-tive inference methods (bolukbasi et al., 2017).
itimplements adaptive inference by installing exits,or intermediate prediction layer, at each layer ofbert and exiting ”easy” samples at exits of theshallow layers to speed up inference (figure 1).
strategies for early exiting are designed (teerapit-tayanon et al., 2016; kaya et al., 2019; xin et al.,2020; zhou et al., 2020), which decides when toexit given the current obtained predictions (fromprevious and current layers)..early exiting architectures’ training procedureis essentially a multi-objective problem since eachexit is trying to improve its performance.
differentobjectives from different classiﬁers may conﬂictand interfere with one-another (phuong and lam-pert, 2019; yu et al., 2020).
thus they incorporatedistillation loss to improve the training procedureby encouraging early exits to mimic the output dis-tributions of the last exit.
the motivation is that thelast exit has the maximum network capacity andshould be more accurate than the earlier exits.
intheir work, only the last exit can act as a teacherexit.
besides, the multiple objectives are uniformlyweighted..in this work, we propose a novel training mecha-nism called learned early exiting for bert (lee-bert).
our contributions are three folded.
first,instead of learning from the last exit, leebertasks each exit to learn from each other.
the mo-tivation is that different layers extract features ofvarying granularity.
thus they have different per-spectives of the sentence.
distilling knowledgefrom each other improves the expressiveness oflower exits and alleviates the overﬁttng of the laterexits.
second, to achieve the optimal trade-offs be-tween different loss terms, their weights are treatedas parameters and are learned along with model pa-rameters.
the optimization of the learnable weightsand model parameters is formulated as a bi-leveloptimization problem, optimized with gradient de-scent.
built upon previous literature (liu et al.,2019), we propose a novel cross-level optimization.
(clo) algorithm to solve the bilevel optimizationbetter..extensive experiments are conducted on theglue benchmark (wang et al., 2018), and showthat leebert outperforms existing sota bertearly exiting methods, sometimes by a large mar-gin.
ablation study shows that: (1) knowledgedistillation among all the exits can improve theirperformances, especially for the shallow ones; (2)our novel clo algorithm is useful in learning moresuitable weights and brings performance gains..our contributions are integrated into our lee-bert framework, which can be summarized asfollows:.
• we propose a novel training method for earlyexiting plms to ask each exit to learn fromeach other..• we propose to ﬁnd the optimal trade-off ofdifferent loss terms by assigning learnableweights..• we propose a novel cross-level optimization(clo) algorithm to learn the loss term weight-s better..2 preliminaries.
in this section, we introduce the necessary back-ground for bert early exiting.
throughout thiswork, we consider the case of multi-class classi-ﬁcation with samples {(xn, yn), xn ∈ x , yn ∈y, i = 1, 2, ..., n }, e.g., sentences, and the num-ber of classes is k..2.1 backbone models.
in this work, we adopt bert and albert asbackbone models.
bert is a multi-layer trans-former (vaswani et al., 2017) network, which ispre-trained in a self-supervised manner on a largecorpus.
albert is more lightweight than bertsince it shares parameters across different layers,and the embedding matrix is factorized..2.2 early exiting architecture.
as depicted in figure 1, early exiting architecturesare networks with exits at different transformer lay-ers.
with m exits, m classiﬁers pm : x → ∆k(m = 1, 2, ..., m ) are designated at m layers ofbert, each of which maps its input to the proba-bility simplex ∆k, i.e., the set of probability dis-tributions over the k classes.
previous literature(phuong and lampert, 2019; liu et al., 2020) think.
2969figure 1: the training procedure of leebert, which differs from the previous literature in two aspects.
first, welet exits learn from each other, instead of only asking shallow exits to learn from the deepest exit.
second, theimportance of each distillation loss term are retained along with the learning of model parameters..of p1, ..., pm as being ordered from least to mostexpressive.
however, in terms of generalization a-bility, due to the over-thinking problem, later layersmay not be superior to shallow layers..in principle, the classiﬁers may or may not shareweights and computation, but in the most interest-ing and practically useful case, they share both..2.3 early exiting strategies.
there are mainly three early exiting strategies forbert early exiting.
branchynet (teerapittayanonet al., 2016), fastbert (liu et al., 2020) and dee-bert (xin et al., 2020) calculated the entropy ofthe prediction probability distribution as a proxy forthe conﬁdence of exiting classiﬁers to enable ear-ly exiting.
shallow-deep nets (kaya et al., 2019)and righttool (schwartz et al., 2020) leveragedthe softmax scores of predictions of exiting clas-siﬁers, that is, if the score of a particular class isdominant and large enough, the model will exit.
recently, pabee (zhou et al., 2020) propose apatience based exiting strategy analogous to earlystopping model training, that is, if the exits’ predic-tions remain unchanged for a pre-deﬁned numberof times (patience), the model will stop inferenceand exit.
pabee achieves sotas results for bertearly exiting..in this work, we mainly adopt the pabee’s pa-tience based early exiting strategy.
however, inablation studies, we will show that our leebertframework can improve the inference performance.
of other exiting strategies..3 our leebert framework.
in this section, we introduce the proposed lee-bert framework.
first, we present our distillationbased loss design, and then we elaborate on howto optimize with learnable weights.
our main con-tribution is a novel training mechanism for bertearly exiting, which extends liu et al.
(2020) andphuong and lampert (2019) via mutual distillationand learned weights..3.1 loss objectives.
3.1.1 classiﬁcation losswhen receiving an input sample (xn, yn), each ex-it will calculate the cross-entropy loss based onits predicted, and all the exits are simultaneouslyoptimized with a summed loss, i.e.,.
lce(xn, yn) =.
lce(pm(xn), yn)..(1).
m(cid:88).
m=1.
note that the above objective directly assumes uni-form weights for all m loss terms..3.1.2 distillation lossto introduce our contribution, we ﬁrst remind thereader of the classical distillation framework as in-troduced in hinton et al.
(2015): assume we want aprobabilistic classiﬁer s (student) to learn from an-other classiﬁer t (teacher).
this can be achieved by.
2970minimizing the (temperature-scaled) cross-entropybetween their prediction distributions,.
3.2 weighted loss.
k(cid:88).
k=1.
lkd(t, s) = −τ 2.
[t1/τ (xn)]k log[[s1/τ (xn)]k],.
(2)where τ ∈ r+ is the distillation temperature, and.
[t1/τ (x)]k =.
tk(x)1/τk(cid:48) =1 tk(cid:48) (x)1/τ.
,.
(cid:80)k.(3).
is the distribution obtained from the distributiont(x) by temperature-scaling, and [t1/τ (x)]k is de-ﬁned analogously..the temperature parameter allows controllingthe softness of the teachers’ predictions: the higherthe temperature, the more suppressed is the differ-ence between the largest and the smallest value ofthe probability vector.
the temperature scaling al-lows compensating for the over-conﬁdence of thenetwork’s outputs, i.e., they put too much probabil-ity mass on the top predicted class and too little onthe others.
the factor τ 2 in eq 2 ensures that thetemperature scaling does not negatively affect thegradient magnitude..returning to the early exiting architecture, wefollow the same strategy as classical distillation butuse exits of different layers both as students andteachers.
for any exit m, let t (m) ⊂ 1, ..., m(which could be empty) be the set of teacher exits itis meant to learn from.
then we deﬁne the overalldistillation loss as.
lkd(xn) =.
m(cid:88).
(cid:88).
m=1.
t∈t (m).
lkd(pt(xn), pm(xn))m ∗ |t (m)|.
..(4)previous work (phuong and lampert, 2019; li-u et al., 2020) considers using only the last exitas as the teacher and all exits learn from it.
theusual belief is that deeper exits have more networkcapacity and more accurate than the early exits.
however, the over-thinking phenomenon revealsthat later exits may not be superior to earlier ones.
the more shallow exit may provide different per-spectives in semantic understanding of the inputsentences.
thus, to fully learn from available infor-mation, later exits can beneﬁt from learning fromearly exits.
with this motivation, we consider twosettings:.
learn from later exits (lle).
in this setting,.
early exits learn from all its later exits..learn from all exits (lae).
in this setting, an.
exit learns from all other exits..previous work considers uniform weights for thedistillation loss terms or classiﬁcation loss term,which does not effectively take the trade-off amongmultiple objectives.
first, from the perspectiveof knowledge distillation, intuitively, later exitsshould place little weights on the very early exitssince they have less to offer.
and all exits shouldplace higher importance on exits that are perfor-mant and not overﬁtting.
second, different lossobjectives are usually competing, which may hurtthe ﬁnal results..to address these issues, we propose to assigna set of learnable weights to our loss objective,which are updated via gradient descent along withthe model parameters.
we give weight wi for eachclassiﬁcation loss term and wm,t for the distillationloss term coming from exit m learning from exit t,and the overall loss objective becomes.
l(xn, yn) =.
wilce(pm(xn), yn).
m(cid:88).
m=1.
m(cid:88).
(cid:88).
+.
m=1.
t∈t (m).
wm,t.
lkd(pt(xn), pm(xn))m ∗ |t (m)|.
..(5).
note that ω = {wi, wm,t} can be understood as aset of learnable training hyper-parameter..3.3 optimization of learned weights.
3.3.1 single vs. bi-level optimization.
assume we have two datasets d1 and d2, whichusually are both subsets of the training set dtr.
d1 can be equal to d2.
for a given set of ω ={wi, wm,t}, the optimal solution θ∗(ω) of networkparameters θ are derived from d1, and the optimalω∗ are determined on d2.
we denote the loss ondataset d as ld(θ, ω), a function of two sets ofparameters for convenience.
then the optimizationproblem becomes.
minωld2(θ∗(ω), ω),s.t.,θ∗(ω) = arg minθ.ld1(θ, ω).
(6).
though the above bi-level optimization can ac-curately describe our problem, it is generally dif-ﬁcult to solve.
one heuristic simpliﬁcation of theabove equation is to let d1 = d2 = dtr, and.
2971the optimization problem in eq 16 reduces to thesingle-level optimization (slo),.
minθ,ωldtr (θ, ω),.
(7).
which can be solved directly by stochastic gradi-ent descent.
this reduced formulation treats thelearnable weights ω just as a part of the modelparameters.
despite its efﬁciency, compared withθ, the number of parameters in ω is almost ne-glectable.
thus optimization will need to ﬁt θwell for gradient descent, resulting in inadequatesolutions of ω..the most widely adopted optimization algorithmfor eq 16 is the bi-level optimization (blo) algo-rithm liu et al.
(2019), which asks d1 and d2 to bea random split of dtr.2 and the gradient descentis done following:.
θ = θ − λ1∇θld1,ω = ω − λ2∇ωld2..(8).
that is, updating the parameters in an interleavingfashion: one-step gradient descent of θ on d1 fol-lowed by one step gradient descent of ω on d2.
note that θ∗(ω) in eq 16 is not satisﬁed in blodue to ﬁrst-order approximation, leading gradientupdates of ω into wrong directions, collapsing thebi-level optimization..3.4 cross-level optimization.
we now propose our cross-level optimization algo-rithm.
the gradient descent updating of θ and ωfollows.
θ = θ − λ1∇θld1,ω = ω − λ1∇ωld1 − λ2∇ωld2..(9).
the above equation is the core of our clo algo-rithm, which we will refer to as clo-v1, which arederived and demonstrated in detail in the appendix.
we can see that our cross-level optimization’s coreidea is to draw gradient information from both s-plits of the training set, thus making the updatingof ω more reliable..note that updating ω requires its gradients onboth the d1 set and d2 set.
thus its computationcomplexity is higher than the blo algorithm.
wepropose a more efﬁcient version of cross-level opti-mization (clo-v2), which can also be found in theappendix.
we divide the training procedure into.
2note that on each epoch start, the split of dtr can be.
re-generated..groups, each group containing c steps, θ is updat-ed solely on the training set for c − 1 steps, andupdated following eq 9 for the remaining one step.
we will call the hyper-parameter c as the cross-level cycle length.
clo-v2 is more efﬁcient thanclo-v1, and our experiments show that clo-v2works well and is comparable with clo-v1..4 experiments.
4.1 tasks and datasets.
we evaluate our proposed approach to the classiﬁca-tion tasks on glue benchmark.
we only excludethe sts-b task since it is a regression task, and weexclude the wnli task following previous work(devlin et al., 2018; jiao et al., 2020; xu et al.,2020)..4.2 backbone models.
backbone models.
all of the experiments are builtupon the google bert, albert.
we ensure faircomparison by setting the hyper-parameters relatedto the plm backbones the same with huggingfacetransformers (wolf et al., 2020)..4.3 baseline methods.
we compare with the previous bert early exitingmethods and compare other methods that speed upbert inference..directly reducing layers.
we experiment withdirectly utilizing the ﬁrst 6 and 9 layers of the orig-inal (al)bert with a single output layer on thetop, denoted by (al)bert-6l and (al)bert-9l,respectively.
these two baselines serve as a lowerbound for performance metrics since it does notemploy any technique..static model compression approaches.
formodel parameter pruning, we include the result-s of layerdrop (fan et al., 2020) and attentionhead pruning (michel et al., 2019) on albert.
for knowledge distillation, we include distillbert(sanh et al., 2019), bert-pkd (sun et al., 2019).
3 for module replacing, we include bert-of-theseus (xu et al., 2020)..input-adaptive inference.
this category in-cludes entropy-based method deebert, score-based method shallow-deep, and patience-basedexiting method pabee as our baselines.
we also.
3note that the two methods consider knowledge distillationon the ﬁne-tuning stage, whereas tinybert (jiao et al., 2020)and turc et al.
(2019) investigate knowledge distillation duringboth the pre-training stage and ﬁne-tuning stage..2972method.
#param speed-up cola mnli mrpc qnli qqp.
rte.
sst-2.
albert-basealbert-6lalbert-9llayerdropheadprunedeebertshallow-deeppabeefastbertfastbert-clo-v2leebert-lleleebert-randleebert-uniformleebert-sloleebert-bloleebert-clo-v1leebert.
albert-basepabeefastbertleebert.
12m12m12m12m12m12m12m12m12m12m12m12m12m12m12m12m12m.
12m12m12m12m.
1.00x1.96x1.30x1.96x1.22x1.88x1.95x1.91x1.94x1.95x1.96x1.95x1.95x1.94x1.93x1.95x1.96x.
1.00x1.89x1.95x1.96x.
dev set.
57.451.953.852.252.653.754.156.457.157.257.557.057.157.257.457.957.8.
84.680.281.279.880.381.781.583.984.785.085.184.884.985.085.185.485.4.test set.
54.153.554.054.6∗.
84.383.684.484.8∗.
89.585.887.185.986.287.287.188.789.189.289.589.289.189.289.589.989.7.
87.086.586.787.2.
89.284.786.284.584.386.486.788.689.089.389.489.189.089.389.489.789.7.
88.388.188.388.6.
89.686.888.387.388.087.487.888.989.389.589.889.289.389.689.890.390.2.
75.670.672.971.372.172.472.274.475.676.376.775.875.976.076.476.976.8.
91.888.890.389.789.589.689.790.590.991.191.391.091.090.991.391.891.8.
71.169.870.571.4∗.
73.472.873.774.6∗.
92.892.092.593.1∗.
table 1: experimental results of models with albert backbone on the development set and glue test set.
if notspeciﬁed, leebert and its variants (e.g., leebert-lle) are optimized using clo-v2.
the mean performancescores of 5 runs are reported.
the speed-up ratio is averaged across 7 tasks.
best performances are bolded, ”*”indicates the performance gains are statistically signiﬁcant..include the results of fastbert when it adopts thepabee’s exiting strategy..4.4 experimental settings.
we implement leebert on the base of hugging-face’s transformers.
we conduct our experimentson a single nvidia v100 16gb gpu..training.
we add a linear output layer af-ter each intermediate layer of the pre-trainedbert/albert model as the internal classiﬁer.
the hyperparameter tuning is done in a cross-validation fashion on the training set so that thedev set information of glue tasks are not re-vealed.
we perform grid search over batch sizes of16, 32, 128, and learning rates of {1e-5, 2e-5, 3e-5,5e-5} for model parameters θ, and learning ratesof {1e-5, 1e-4, 1e-3, 5e-3} for learnable weightsω. the cross-level cycle length c will be selectedfrom 2, 4, 8. we will adopt the adam optimizer.
at each epoch, the training set is randomly splitinto d1 and d2 with a ratio 5 : 5. we apply anearly stopping mechanism with patience 5 and e-valuate the model on dev set at each epoch end.
and we deﬁne the dev performance of our early.
exiting architecture as the average performance ofall the exits.
we will select the model with the bestaverage performance in cross validation..we set clo-v2 as the main optimization algo-rithm of leebert, and lae as the main distilla-tion strategy.4 to demonstrate leebert’s ditilla-tion objectives are beneﬁcial, we train leebertwith the lle strategy (leebert-lle).
we alsolet the loss term weights in fastbert to be learn-able and train with our clo-v2 algorithm, i.e.,fastbert-clo-v2..to compare our leebert’s clo optimizationprocedure with baselines, we also train leebertwith (1) single level algorithm (leebert-slo);(2) bi-level algorithm (leebert-blo).
to com-pare clo-v1 and clo-v2, we also train the lee-bert with clo-v1, i.e., leebert-clo-v1.
be-sides, we also include leebert with randomlyassigned discrete weights (leebert-rand) and u-niform weights (leebert-uniform) as baselines,which will serve to demonstrate that our optimiza-tion procedure is beneﬁcial.
the discrete weights.
4henceforth, unless otherwise speciﬁed, our leebert.
method will be the one with lae and clo-v2..2973are randomly selected from {1, 2, ..., 50}, and arenormalized so that the loss terms at each exit haveweights summed to 1..inference.
following prior work, inference withearly exiting is on a per-instance basis, i.e., thebatch size for inference is set to 1. we believethis setting mimics the common latency-sensitiveproduction scenario when processing individualrequests from different users.
we report the meanperformance over 5 runs with different randomseeds.
for deebert and shallow-deep, we set thethreshold for entropy or score, such that the speed-up ratio is between 1.80x to 2.1x.
for fastbertand our leebert, we mainly adopt the pabee’spatience based exiting strategy, and we comparethe results when the patience is set at 4. how thepatience parameter affects the inference efﬁciencyis also investigated for pabee, fastbert, andleebert..4.5 overall comparison.
table 1 reports the main results on glue withalbert as the backbone model.
albert is pa-rameter and memory-efﬁcient due to its cross-layerparameter sharing strategy, however, it still has highinference latency.
from table 1 we can see thatour approach outperforms all compared methodsto improve inference efﬁciency while maintaininggood performances, demonstrating the proposedleebert framework’s effectiveness.
note that oursystem can effectively enhance the original al-bert and pabee by a relatively large marginwhen speeding-up inference by 1.97x.
we alsoconduct experiments on the bert backbone withthe mnli, mrpc, and sst-2 tasks, which can befound in the appendix.
to give more insight intohow early exits perform under different efﬁciencysettings, we illustrate how the patience parame-ter affect the average number of inference layers(which is directly related to speed-up ratios) (fig-ure 2), and prediction performances (figure 3).
wealso show that one can easily apply our leebertframework to image classiﬁcation tasks in the ap-pendix..4.6 analysis.
we now analyze more deeply the main take-awaysfrom table 1 and our experiments..our leebert can speed up inference.
fig-ure 2 shows that on the mrpc task, with the samepatience parameter, leebert usually goes throughfewer layers (on average) than pabee and fast-.
figure 2: the curve of patience vs. avg inference lay-ers for pabee, fastbert and leebert.
the task ismrpc..bert, showing the leebert can improve the efﬁ-ciency of plms’ early exiting..our knowledge distillation strategies are ben-eﬁcial.
table 1 reveals that our lae setting pro-vides the best overall performances on glue interms of distillation strategies.
leebert outper-forms fastbert-clo-v2 on all tasks and exceedsleebert-lle on 6 of the seven tasks, and thescores on qnli the results are comparable.
thisresult proves that exits learning from each other aregenerally beneﬁcial..our clo algorithm brings performancegains.
as a sanity check, leebert-rand performsworse than all optimized leebert models.
table1 also shows that the slo and blo algorithm-s perform worse than our clo.
and we can seethat clo-v1 and clo-v2 have comparable results.
clo-v1 seems to have slight advantages on taskswith few samples, but the performance gaps seemto be marginal.
since clo-v2 is more efﬁcien-t, we will use clo-v2 as our main optimizationalgorithm..the patience-score curves are different for d-ifferent plms.
figures 3(a) and 3(b) show thatdiffernt plms have quite different patience-scorecurves.
for albert, early exiting with pabee’sstrategy can improve upon the albert-base ﬁne-tuning, and the best performance is obtained withpatience 6. with patience 6, the average num-ber of inference layers is 8.11. this phenomenonshows that albert base may suffer from the over-thinking problem.
with the help of our distillationstrategy and clo optimization, the performancegain is considerable.
note that: (a) without distilla-.
2974(a) albert backbone.
(b) bert backbone.
figure 3: patience-performance curves for albert and bert on the mrpc task..tion, shallow exits’ performances are signiﬁcantlyworse, and our distillation can help these exits toimprove; (b) with leebert, the performances ofthe later exits are comparable to the earlier ones,since the over-thinking problem is alleviated bydistillation.
however, the patience-score curve forbert is quite monotonic, suggesting that over-thinking problem is less severe.
note that bert’sshallow exits are signiﬁcantly worse than that ofalbert, and with leebert, the shallow exits’performances are improved..training time costs.
table 2 presents the pa-rameter numbers and time costs of training for lee-bert compared with the original (al)bert, andpabee, fastbert.
we can see that although exitsneed extra time for training, early exiting architec-tures actually can reduce the training time.
intu-itively, additional loss objectives can be regardedas additional parameter updating steps for lowerlayers, thus speeding up the model convergence.
leebert-clo-v1 requires a longer time for train-ing.
notably, our leebert’s time costs are com-parable with pabee and fastbert, even thoughit has more complicated gradient updating steps..working with different exiting strategies.
re-call that our results are mainly obtained by adopt-ing the pabee’s patience based exiting strategies.
however, our leebert framework is quite off-the-shelf, and can be integrated with many otherexiting strategies.
our framework can work underdifferent exiting strategies.5 when using entropy-based strategy, leebert outperforms deebert.
5due to length limitation, we will leave the detailed results.
of this ablation study in the appendix..method-w/o early exitingw pabeew fastbertw leebert-clo-v1w leebert(-clo-v2).
#params.
training timemrpc sst-2 mrpc sst-211312m109+18k102+18k226+18k118+18k.
12m+18k+18k+18k+18k.
6.46.26.013.26.5.table 2: comparison of parameter numbers and train-ing time costs.
the training time is the time cost (inminutes) until until the best performing checkpoint (onthe dev set) with and without early exiting strategies onalbert as the backbone model..by a large margin.
when using shallow-deep’smax probability strategy, leebert outperformsshallow-deep on all glue tasks..5 conclusion and discussions.
in this work, we propose a new framework forimproving plms’ early exiting.
our main contri-butions lie in two aspects.
first, we argue that exitsshould learn and distill knowledge from each otherduring training.
second, we propose that early ex-iting networks’ training objectives be weighted dif-ferently, where the weights are learnable.
the learn-able weights are optimized with the cross-level op-timization we propose.
experiments on the gluebenchmark datasets show that our framework canimprove plms’ early exiting performances, espe-cially under high latency requirements.
our frame-work is easy to implement and can be adapted tovarious early exiting strategies.
we want to explorenovel exiting strategies that better guarantee exitingperformances in the future..2975references.
haoli bai, wei zhang, l. hou, l. shang, jing jin,x. jiang, qun liu, michael r. lyu, and irwin king.
2020. binarybert: pushing the limit of bert quanti-zation.
arxiv, abs/2012.15701..tolga bolukbasi, j. wang, o. dekel, and venkateshsaligrama.
2017. adaptive neural networks for ef-ﬁcient inference.
in icml..jacob devlin, ming-wei chang, kenton lee, andkristina toutanova.
2018. bert: pre-training of deepbidirectional transformers for language understand-ing.
arxiv preprint arxiv:1810.04805..angela fan, e. grave, and armand joulin.
2020. re-ducing transformer depth on demand with structureddropout.
arxiv, abs/1909.11556..shijie geng, peng gao, z. fu, and yongfeng zhang.
2021. romebert: robust training of multi-exit bert..kaiming he, x. zhang, shaoqing ren, and jian sun.
2016. deep residual learning for image recognition.
2016 ieee conference on computer vision and pat-tern recognition (cvpr), pages 770–778..geoffrey e. hinton, oriol vinyals, and j. dean.
2015.distilling the knowledge in a neural network.
arxiv,abs/1503.02531..xiaoqi jiao, y. yin, l. shang, xin jiang, x. chen, lin-lin li, f. wang, and qun liu.
2020. tinybert: dis-tilling bert for natural language understanding.
arx-iv, abs/1909.10351..y. kaya, sanghyun hong, and t. dumitras.
2019.shallow-deep networks: understanding and mitigat-ing network overthinking.
in icml..se-hoon kim, amir gholami, zhewei yao, m. w. ma-i-bert: integer-only.
honey, and k. keutzer.
2021.bert quantization.
arxiv, abs/2101.01321..a. krizhevsky.
2009. learning multiple layers of fea-.
tures from tiny images..zhenzhong lan, mingda chen, sebastian goodman,kevin gimpel, piyush sharma, and radu sori-cut.
2020. albert: a lite bert for self-supervisedlearning of language representations.
arxiv, ab-s/1909.11942..hanxiao liu, k. simonyan, and yiming yang.
2019.darts: differentiable architecture search.
arxiv, ab-s/1806.09055..weijie liu, p. zhou, zhe zhao, zhiruo wang, haotangdeng, and q. ju.
2020. fastbert: a self-distillingarxiv, ab-bert with adaptive inference time.
s/2004.02178..mary phuong and christoph h. lampert.
2019.distillation-based training for multi-exit architec-tures.
2019 ieee/cvf international conference oncomputer vision (iccv), pages 1355–1364..alec radford, jeff wu, rewon child, david luan,dario amodei, and ilya sutskever.
2019. languagemodels are unsupervised multitask learners..victor sanh, lysandre debut, julien chaumond, andthomas wolf.
2019. distilbert, a distilled versionof bert: smaller, faster, cheaper and lighter.
arxiv,abs/1910.01108..roy schwartz, gabi stanovsky, swabha swayamdipta,jesse dodge, and n. a. smith.
2020. the right toolfor the job: matching model and instance complexi-ties.
in acl..s. sun, yu cheng, zhe gan, and jingjing liu.
2019.patient knowledge distillation for bert model com-pression.
in emnlp/ijcnlp..surat teerapittayanon, bradley mcdanel, and h. t.kung.
2016. branchynet: fast inference via earlyexiting from deep neural networks.
2016 23rd inter-national conference on pattern recognition (icpr),pages 2464–2469..iulia turc, ming-wei chang, kenton lee, and kristi-na toutanova.
2019. well-read students learn better:on the importance of pre-training compact models.
arxiv: computation and language..ashish vaswani, noam shazeer, niki parmar, jakobuszkoreit, llion jones, aidan n. gomez, l. kaiser,and illia polosukhin.
2017. attention is all you need.
arxiv, abs/1706.03762..alex wang, amanpreet singh, julian michael, felixhill, omer levy, and samuel r. bowman.
2018.glue: a multi-task benchmark and analysis plat-form for natural language understanding.
in black-boxnlp@emnlp..thomas wolf, lysandre debut, victor sanh, julienchaumond, clement delangue, anthony moi, pier-ric cistac, tim rault, remi louf, morgan funtow-icz, joe davison, sam shleifer, patrick von platen,clara ma, yacine jernite, julien plu, canwen xu,teven le scao, sylvain gugger, mariama drame,quentin lhoest, and alexander rush.
2020. trans-formers: state-of-the-art natural language process-ing.
in proceedings of the 2020 conference on em-pirical methods in natural language processing:system demonstrations, pages 38–45, online.
asso-ciation for computational linguistics..j. xin, raphael tang, j. lee, y. yu, and jimmy lin.
2020. deebert: dynamic early exiting for accelerat-ing bert inference.
arxiv, abs/2004.12993..paul michel, omer levy, and graham neubig.
2019.are sixteen heads really better than one?
in neurip-s..canwen xu, wangchunshu zhou, tao ge, furu wei,and m. zhou.
2020. bert-of-theseus: compressingbert by progressive module replacing.
in emnlp..2976z. yang, zihang dai, yiming yang, j. carbonell,r. salakhutdinov, and quoc v. le.
2019. xlnet:generalized autoregressive pretraining for languageunderstanding.
in neurips..tianhe yu, saurabh kumar, a. gupta, s. levine, karolhausman, and chelsea finn.
2020. gradient surgeryfor multi-task learning.
arxiv, abs/2001.06782..w. zhang, l. hou, y. yin, l. shang, x. chen, x. jiang,and qun liu.
2020. ternarybert: distillation-awareultra-low bit bert.
arxiv, abs/2009.12812..wangchunshu zhou, canwen xu, tao ge, julian m-cauley, ke xu, and furu wei.
2020. bert losespatience: fast and robust inference with early exit.
arxiv, abs/2006.04152..m. zhu and s. gupta.
2018. to prune, or not to prune:exploring the efﬁcacy of pruning for model compres-sion.
arxiv, abs/1710.01878..a derivation of our cross-leveloptimization algorithm..we now derive our cross-level optimization (clo)methods.
our objective is.
minωld2(θ∗(ω), ω),s.t.,θ∗(ω) = arg minθ.ld1(θ, ω).
(15).
assume the optimal solution is θ∗ and ω∗.
theobjective in eq 16 can be viewed as minimiz-ing the gap between ld1(θ, ω) and ld1(θ∗, ω∗),and minimizing the gap between ld2(θ∗, ω) andld2(θ∗, ω∗).
thus, introducing slack variables δ1and δ2, eq 16 can be reformulated as.
minθ,ωδ2.
1 + δ22,.s.t.,ld1(θ, ω) <= ld1(θ∗, ω∗) + δ1,ld2(θ∗, ω) <= ld2(θ∗, ω∗) + δ2,δ1 >= 0, δ2 >= 0..using the lagrangian multiplier method, the la-grangian function is.
1 + δ22.lg(δ1, δ2, θ, ω, λ) = δ2− λ1(ld1(θ, ω) − ld1(θ∗, ω∗) − δ1)− λ2(ld2(θ∗, ω) − ld2(θ∗, ω∗) − δ2)− λ3δ1 − λ4δ2..(17).
to solve this lagrangian function, the gradient de-scent updating of θ and ω becomes.
θ = θ − λ1∇θld1,ω = ω − λ1∇ωld1 − λ2∇ωld2..(18).
now we formally illustrate the clo-v1 algorith-m, which is in algorithm 1. we also ofﬁcially givethe clo-v2 algorithm in algorithm 2..b hyper-parameters for each tasks.
table 3 reports the important hyper-parametersof leebert for each task.
note that our hyper-parameter search was done on the training set withcross-validation so that the glue benchmarks’ devset information was not revealed during training..c results with bert backbone.
we conduct experiments with the bert backboneon three representative tasks of glue, mnli, m-rpc, and sst-2.
the results are reported in table5. the results show that our leebert frameworkworks well with different types of plms..d patience-performance curves on sst-2.
we also provide the patience-performance curves(figure 4) on the sst-2 task, with albert andbert backbones..e working with different exiting.
strategies.
our results are mainly obtained by adopting thepabee’s patience based exiting strategies.
nowwe demonstrate that leebert can work with otherexiting strategies.
table 4 shows that leebertcan help improve deebert with its entropy-basedexiting method and outperforms shallow-deep withits max-prediction-based approach..to demonstrate the effectiveness of leebert onthe image classiﬁcation task, we follow the ex-perimental settings in shallow-deep (kaya et al.,2019).
we conduct experiments on two imageclassiﬁcation datasets, cifar-10 and cifar-100(krizhevsky, 2009).
and resnet-56 (he et al.,2016) serves as the backbone and we compare lee-bert with pabee, dbt from phuong and lam-pert (2019).
after every two convolutional layers,an exiting classiﬁer is added.
we set the batch sizeto 128 and use sgd optimizer with learning rateof 0.1. we set the cross level sycle to be 4, andlearning rate of the learnable weights ω are 0.01..table 6 reports the results.
leebert outper-forms the full resnet-56 on both tasks even whenit provides 1.3x speed-up.
besides, it outperformspabee and dbt..f leebert are effective for image.
(16).
classiﬁcation.
2977algorithm 1: leebert-clo-v1.
parameters: θ, ω;return: the converged early exiting model; while not converge do.
for t=1, ..., t do.
sample batch b1 and b2 from d1 and d2, respectivelyupdate θ with.
θ = θ − λ1∇θlb1,.
calculate lb1 and lb2 with the updated θ, and update ω with:.
ω = ω − λ1∇ωlb1 − λ2∇ωlb2,.
end.
end.
algorithm 2: leebert-clo-v2.
parameters: θ, ω, c;return: the converged early exiting model; while not converge do.
for t=1, ..., t do.
for c = 1, 2, ..., c doif c != c then.
sample batch b1 from d1, respectively update θ and with.
θ = θ − λ1∇θlb1,ω = ω − λ1∇ωlb1,.
sample batch b1 and b2 from d1 and d2, respectivelyupdate θ with.
θ = θ − λ1∇θlb1,.
calculate lb1 and lb2 with the updated θ, and update ω with:.
ω = ω − λ1∇ωlb1 − λ2∇ωlb2,.
endelse.
end.
end.
end.
end.
(10).
(11).
(12).
(13).
(14).
2978task.
colamnlimrpcqnliqqprtesst-2.
lr ofmodel params2e-51e-51e-51e-51e-52e-52e-5.
lr oflearnable weights1e-31e-51e-41e-51e-51e-31e-4.
batchsize161283212812816128.cross-levelsteps2448444.table 3: hyper-parameter settings for each task..method.
#param speed-up mnli mrpc sst-2.
deebertleebert (ours).
12m12m.
with deebert’s entropy-based exiting strategy87.21.88x88.61.92xwith shallow-deep’s max-prob based exiting strategy1.95x2.04x.
12m108m.
87.188.9.
81.583.7.
81.783.9.shallow-deepleebert (ours).
90.691.8.
90.791.7.method.
#param speed-up mnli mrpc sst-2.
bert-basebert-6lbert-9ldistillbertbert-pkdbert-of-theseuspabeefastbertleebert (ours).
bert-basepabeefastbertleebert (ours).
108m66m87m66m66m66m108m108m108m.
108m108m108m108m.
dev set1.00x1.96x1.30x1.96x1.96x1.96x1.86x1.95x1.97xtest set1.00x1.86x1.96x1.97x.
83.579.180.479.880.680.781.582.183.1.
83.381.682.083.1.
88.383.985.885.385.585.486.286.788.5.
87.285.285.787.1.
91.589.690.589.389.789.690.490.891.8.
92.791.391.792.6.table 4: experimental results of leebert when using different early exiting strategies..table 5: experimental results of models with bert backbone on the development set and glue test set.
themean performance scores of 5 runs are reported.
the speed-up ratio is averaged across 3 tasks.
best performancesare bolded..method-resnet-56pabeedbtleebert.
cifar-10speed-up acc.
91.891.992.192.5.
1.00x1.26x1.281.30x.
cifar-100speed-up acc.
68.669.069.369.6.
1.00x1.22x1.25x1.27x.
table 6: experimental results of leebert when applied in the image classiﬁcation tasks..2979(a) albert.
(b) bert.
figure 4: patience-performance curves for albert and bert on the mrpc task..2980