beyond ofﬂine mapping:learning cross-lingual word embeddings through context anchoring.
aitor ormazabal1, mikel artetxe2, aitor soroa1, gorka labaka1, eneko agirre11hitz center, university of the basque country (upv/ehu)2facebook ai research{aitor.ormazabal,a.soroa,gorka.labaka,e.agirre}@ehu.eusartetxe@fb.com.
abstract.
recent research on cross-lingual word embed-dings has been dominated by unsupervisedmapping approaches that align monolingualembeddings.
such methods critically relyon those embeddings having a similar struc-ture, but it was recently shown that the sepa-rate training in different languages causes de-in this pa-partures from this assumption.
per, we propose an alternative approach thatdoes not have this limitation, while requiringa weak seed dictionary (e.g., a list of identicalwords) as the only form of supervision.
ratherthan aligning two ﬁxed embedding spaces, ourmethod works by ﬁxing the target languageembeddings, and learning a new set of embed-dings for the source language that are alignedwith them.
to that end, we use an exten-sion of skip-gram that leverages translated con-text words as anchor points, and incorporatesself-learning and iterative restarts to reducethe dependency on the initial dictionary.
ourapproach outperforms conventional mappingmethods on bilingual lexicon induction, andobtains competitive results in the downstreamxnli task..1.introduction.
cross-lingual word embeddings (clwes) repre-sent words from two or more languages in a sharedspace, so that semantically similar words in dif-ferent languages are close to each other.
earlywork focused on jointly learning clwes in twolanguages, relying on a strong cross-lingual su-pervision in the form of parallel corpora (luonget al., 2015; gouws et al., 2015) or bilingual dic-tionaries (gouws and søgaard, 2015; duong et al.,2016).
however, these approaches were later su-perseded by ofﬂine mapping methods, which sepa-rately train word embeddings in different languagesand align them in an unsupervised manner throughself-learning (artetxe et al., 2018; hoshen and.
wolf, 2018) or adversarial training (zhang et al.,2017; conneau et al., 2018a)..despite the advantage of not requiring any paral-lel resources, mapping methods critically rely onthe underlying embeddings having a similar struc-ture, which is known as the isometry assumption.
several authors have observed that this assumptiondoes not generally hold, severely hindering the per-formance of these methods (søgaard et al., 2018;nakashole and flauger, 2018; patra et al., 2019).
inlater work, ormazabal et al.
(2019) showed that thisissue arises from trying to align separately trainedembeddings, as joint learning methods are not sus-ceptible to it..in this paper, we propose an alternative approachthat does not have this limitation, but can still workwithout any parallel resources.
the core idea ofour method is to ﬁx the target language embed-dings, and learn aligned embeddings for the sourcelanguage from scratch.
this prevents structuralmismatches that result from independently trainingembeddings in different languages, as the learningof the source embeddings is tailored to each partic-ular set of target embeddings.
for that purpose, weuse an extension of skip-gram that leverages trans-lated context words as anchor points.
so as to trans-late the context words, we start with a weak initialdictionary, which is iteratively improved throughself-learning, and we further incorporate a restart-ing procedure to make our method more robust.
thanks to this, our approach can effectively workwithout any human-crafted bilingual resources, re-lying on simple heuristics (automatically generatedlists of numerals or identical words) or an existingunsupervised mapping method to build the initialdictionary.
our experiments conﬁrm the effective-ness of our approach, outperforming previous map-ping methods on bilingual dictionary induction andobtaining competitive results on zero-shot cross-lingual transfer learning on xnli..proceedingsofthe59thannualmeetingoftheassociationforcomputationallinguisticsandthe11thinternationaljointconferenceonnaturallanguageprocessing,pages6479–6489august1–6,2021.©2021associationforcomputationallinguistics64792 related work.
word embeddings.
embedding methods learnstatic word representations based on co-occurrencestatistics from a corpus.
most approaches use twodifferent matrices to represent the words and thecontexts, which are known as the input and outputvectors, respectively (mikolov et al., 2013; pen-nington et al., 2014; bojanowski et al., 2017).
theoutput vectors play an auxiliary role, being dis-carded after training.
our method takes advantageof this fact, leveraging translated output vectors asanchor points to learn cross-lingual embeddings.
to that end, we build on the skip-gram with neg-ative sampling (sgns) algorithm (mikolov et al.,2013), which trains a binary classiﬁer to distinguishwhether each output word co-occurs with the giveninput word in the training corpus or was insteadsampled from a noise distribution..mapping clwe methods.
ofﬂine mappingmethods separately train word embeddings for eachlanguage, and then learn a mapping to align theminto a shared space.
most of these methods alignthe embeddings through a linear map—often en-forcing orthogonality constraints—and, as such,they rely on the assumption that the geometricstructure of the separately learned embeddings issimilar.
this assumption has been shown to failunder unfavorable conditions, severely hinderingthe performance of these methods (søgaard et al.,2018; vuli´c et al., 2020).
existing attempts to mit-igate this issue include learning non-linear mapsin a latent space (mohiuddin et al., 2020), employ-ing maps that are only locally linear (nakashole,2018), or learning a separate map for each word(glavaˇs and vuli´c, 2020).
however, all these meth-ods are supervised, and have the same fundamentallimitation of aligning a set of separately trainedembeddings (ormazabal et al., 2019)..self-learning.
while early mapping methods re-lied on a bilingual dictionary to learn the align-ment, this requirement was alleviated thanks to self-learning, which iteratively re-induces the dictionaryduring training.
this enabled learning clwes ina semi-supervised fashion starting from a weakinitial dictionary (artetxe et al., 2017), or in a com-pletely unsupervised manner when combined withadversarial training (conneau et al., 2018a) or ini-tialization heuristics (artetxe et al., 2018; hoshenand wolf, 2018).
our proposed method also incor-porates a self-learning procedure, showing that this.
technique can also be effective with non-mappingmethods..joint clwe methods.
before the populariza-tion of ofﬂine mapping, most clwe methods ex-tended monolingual embedding algorithms by ei-ther incorporating an explicit cross-lingual term intheir learning objective, or directly replacing wordswith their translation equivalents in the trainingcorpus.
for that purpose, these methods relied onsome form of cross-lingual supervision, rangingfrom bilingual dictionaries (gouws and søgaard,2015; duong et al., 2016) to parallel or document-aligned corpora (luong et al., 2015; gouws et al.,2015; vuli´c and moens, 2016).
more recently,lample et al.
(2018) reported positive results learn-ing regular word embeddings over concatenatedmonolingual corpora in different languages, rely-ing on identical words as anchor points.
wanget al.
(2019) further improved this approach by ap-plying a conventional mapping method afterwards.
as shown later in our experiments, our approachoutperforms theirs by a large margin..freezing.
artetxe et al.
(2020) showed that it ispossible to transfer an english transformer to a newlanguage by freezing all the inner parameters of thenetwork and learning a new set of embeddings forthe new language through masked language mod-eling.
this works because the frozen transformerparameters constrain the resulting representationsto be aligned with english.
similarly, our proposedapproach uses frozen output vectors in the targetlanguage as anchor points to learn aligned embed-dings in the source language..3 proposed method.
let xi and ˜xi be the input and output vectors ofthe ith word in the source language, and yj and˜yj be their analogous in the target language.1 inaddition, let d be a bilingual dictionary, whered(i) = j denotes that the ith word in the sourcelanguage is translated as the jth word in the targetlanguage.
our approach ﬁrst learns the target lan-guage embeddings {yi} and {˜yi} monolinguallyusing regular sgns.
having done that, we learnthe source language embeddings {xi} and {˜xi},constraining them to be aligned with the targetlanguage embeddings according to the dictionaryd. for that purpose, we propose an extension of.
1recall that {˜xi} and {˜yj} are auxiliary, and the goal is.
to learn aligned {xi} and {yj} (see §2)..6480⊲ learn target embedings⊲ iterative restart (§3.3).
algorithm 1 proposed methodinput: d (dictionary), csrc (src corpus), ctgt (tgt corpus)output: {xi}, {yi} (aligned src and tgt embs)hparams: t (updates), r (restarts), k (re-inductions)1: {yi}, {˜yi} ← sgns(ctgt)2: for r ← 1 to r do3:4:5:6:7:8:9:10:11: end for.
(wi, wj) ← next instance(csrc)⊲ core method (§3.1)backprop(l(wi, wj))if it mod (t /k) = 0 then ⊲ self-learn (§3.2).
{xi}, {˜xi} ← random init()for it ← 1 to t do.
d ← reinduce({xi}, {yi}).
end for.
end if.
sgns that replaces the output vectors in the sourcelanguage with their translation equivalents in thetarget language, which act as anchor points (§3.1).
so as to make our method more robust to a weakinitial dictionary, we incorporate a self-learningprocedure that re-estimates the dictionary duringtraining (§3.2), and perform iterative restarts (§3.3).
algorithm 1 summarizes our method..3.1 sgns with cross-lingual anchoring.
given a pair of words (wi, wj) co-occurring in thesource language corpus, we deﬁne a generalizedsgns objective as follows:.
l(wi, wj) = log σ (xwi · ctx(wj)) +.
ewn∼pn(w) [log σ (−xwi · ctx(wn))].
k.i=1x.where k is the number of negative samples, pn(w)is the noise distribution, and ctx(wt) is a functionthat returns the output vector to be used for wt.
inregular sgns, this function would simply returnthe output vector of the corresponding word, so thatctx(wt) = ˜xwt.
in contrast, our approach replacesit with its counterpart in the target language if wtis in the dictionary:.
ctx(wt) =.
˜yd(wt)˜xwt.
(.
if wt ∈ dotherwise.
during training, the replaced vectors {˜yi} arekept frozen, acting as anchor points so that theresulting embeddings {xi} are aligned with theircounterparts {yi} in the target language..3.2 self-learning.
this is not different for conventional mapping meth-ods, which also rely on a bilingual dictionary toalign separately trained embeddings in differentlanguages.
so as to overcome this issue, modernmapping approaches rely on self-learning, whichalternates between aligning the embeddings andre-inducing the dictionary in an iterative fashion(artetxe et al., 2017)..we adopt a similar strategy, and re-induce thedictionary d a total of k times during training,where k is a hyperparameter.
to that end, we ﬁrstobtain the translations for each source word usingcsls retrieval (conneau et al., 2018a):.
d(i) = arg max.
csls(xi, yj).
j.having done that, we discard all entries that donot satisfy the following cyclic consistency condi-tion:2.i = arg max.
k.(cid:0)3.3 iterative restarts.
i ∈ d ⇐⇒cos.xk, yarg maxj cos(xi,yj ).
(cid:1).
while self-learning is able to improve a weak ini-tial dictionary throughout training, the method isstill susceptible to poor local optima.
this can befurther exacerbated by the learning rate decay com-monly used with sgns, which makes it increas-ingly difﬁcult to recover from a poor solution astraining progresses.
so as to overcome this issue,we sequentially run the entire sgns training rtimes, where r is a hyperparameter of the method.
we use the output from the previous run as the ini-tial dictionary, but all the other parameters are resetand the full training process is run from scratch..4 experimental setup.
we next describe the systems explored in our ex-periments (§4.1), the data and procedure used totrain them (§4.2), and the evaluation tasks (§4.3)..4.1 systems.
we compare 3 model families in our experiments:.
ofﬂine mapping.
this approach learns mono-lingual embeddings in each of the languages sepa-rately, which are then mapped into a common space.
as shown later in our experiments, the performanceof our basic method is largely dependent on thequality of the bilingual dictionary itself.
however,.
2we deﬁne our cyclic consistency condition over cosinesimilarity, which we found to be more restrictive than csls(in that it discards more entries) and work better in our prelim-inary experiments..6481en.
de.
es.
fr.
ﬁ.ru.
zh.
de-en es-en fr-en ﬁ-en ru-en zh-en.
tokenssentences.
2,390 860 601 724 91 498 23410.
101.
22.
25.
42.
28.
6.identicalnumeralsmapping.
44.81.453.3.
57.6 63.8 37.71.62.41.667.3 69.7 22.3.
4.31.128.2.
3.30.217.1.table 1: size of the training corpora (millions)..table 2: size of the initial dictionaries (thousands)..through a linear transformation.
we experimentwith 3 popular methods from the literature: muse(conneau et al., 2018a), icp (hoshen and wolf,2018) and vecmap (artetxe et al., 2018).
we usethe original implementation of each method in theirunsupervised mode with default hyperparameters..joint learning + ofﬂine mapping.
this ap-proach jointly learns word embeddings for twolanguages over their concatenated monolingual cor-pora, where identical words act as anchor points(lample et al., 2018).
having done that, the vo-cabulary is partitioned into one shared and two lan-guage speciﬁc subsets, which are further alignedthrough an ofﬂine mapping method (wang et al.,2019).
we use the joint align implementation fromthe authors with default hyperparameters, whichrelies on fasttext for the joint learning step andmuse for the mapping step.3.
anchoring.
our.
cross-lingualproposedmethod, described in section 3. we explore 3 alter-natives to obtain the initial dictionary: (i) identicalwords, where di = j if the ith source word andthe jth target word are identically spelled, (ii)numerals, a subset of the former where identicalwords are further restricted to be sequences ofdigits, and (iii) unsupervised mapping, wherewe use the baseline vecmap system describedabove to induce the initial dictionary.4 the ﬁrsttwo variants make assumptions on the writingsystem of different languages, which is usuallyregarded as a weak form of supervision (artetxeet al., 2017; søgaard et al., 2018), whereas thelatter is strictly unsupervised, yet dependant on anadditional system from a different family..4.2 data and training details.
we learn clwes between english and six otherlanguages: german, spanish, french, finnish, rus-sian and chinese.
following common practice, we.
3the original implementation only supports the supervisedmode with rcsls mapping, so we modiﬁed it to use musein the unsupervised setting as described in the original paper.
4we use csls retrieval and apply the cyclic consistency.
restriction as described in section 3.2..use wikipedia as our training corpus,5 which wepreprocessed using standard moses scripts, and re-strict our vocabulary to the most frequent 200ktokens per language.
in the case of chinese, wordsegmentation was done using the stanford seg-menter.
table 1 summarizes the statistics of theresulting corpora, while table 2 reports the sizesof the initial dictionaries derived from it for ourproposed method..for joint align, we directly run the ofﬁcial imple-mentation over our tokenized corpus as describedabove.
all the other systems take monolingual em-beddings as input, which we learn using the sgnsimplementation in word2vec.6 for our proposedmethod, we set english as the target language, ﬁxthe corresponding monolingual embeddings, andlearn aligned embeddings in the source languageusing our extension of sgns (§3).7 we set thenumber of restarts r to 3, the number of reinduc-tions per restart k to 50, and the number of epochsto 10 #trg sents#src sents , which makes sure that the sourcelanguage gets a similar number of updates to the10 epochs done for english.8 for all the otherhyperparameters, we use the same values as forthe monolingual embeddings.
we made all of ourdevelopment decisions based on preliminary exper-iments on english-finnish, without any systematichyperparameter exploration.
our implementationruns on cpu, except for the dictionary reinductionsteps, which run on a single gpu for around one.
5we extracted the corpus from the february 2019 dump.
using the wikiextractor tool..6we use 10 negative samples, a sub-sampling threshold of1e-5, 300 dimensions, and 10 epochs.
note that joint alignalso learns 300-dimensional vectors, but runs fasttext withdefault hyperparameters under the hood..7in our preliminary experiments, we observed our pro-posed method to be quite sensitive to which language is thesource and which one is the target.
we ﬁnd the language withthe largest corpus to perform best as the target, presumablybecause the corresponding monolingual embeddings are bet-ter estimated, so it is more appropriate to ﬁx them and learnaligned embeddings for the other language.
following thisobservation, we set english as the target language for all pairs,as it is the language with the largest corpus..8for a fair comparison, we also tried using the same num-ber of epochs for the baseline systems, but this performedworse than the reported numbers with 10 epochs..6482de-en.
es-en.
fr-en.
ﬁ-en.
ru-en.
zh-en.
→ ←.
→ ←.
→ ←.
→ ←.
→ ←.
→ ←.
offline mapping.
muse (conneau et al., 2018a)icp (hoshen and wolf, 2018)vecmap (artetxe et al., 2018).
72.9 74.873.9 75.174.5 76.6.
83.5 83.082.5 83.283.5 83.3.
81.7 82.380.5 82.382.7 83.0.
0.3∗ 0.3∗0.3∗ 0.3∗61.9 45.1.
0.0∗ 0.3∗59.5 46.165.7 49.0.
39.5 30.90.1∗ 2.8∗42.4 33.4.joint align (wang et al., 2019).
70.7 68.7.
71.9 69.6.
79.2 78.0.
33.1 29.1.
31.3 25.1.
3.6∗ 18.4.
48.2.joint learning + offline mapping.
cross-lingual anchoring.
ours (identical init)ours (numeral init)ours (mapping init).
76.7 77.976.9 77.776.8 78.1.
86.5 84.186.3 84.186.3 84.2.
85.0 84.885.0 84.984.9 84.9.
63.3 51.363.6 50.664.2 51.5.
65.3 51.664.9 51.465.7 51.5.
42.1 38.91.4∗ 4.9∗42.5 38.8.
67.361.067.5.avg.
45.848.965.1.table 3: main bli results on the muse dataset (p@1).
asterisks denote divergence (< 5% p@1)..hour in total..4.3 evaluation tasks.
as described next, we evaluate our method ontwo tasks: bilingual lexicon induction (bli) andcross-lingual natural language inference (xnli)..bli.
following common practice, we induce abilingual dictionary through csls retrieval (con-neau et al., 2018a) for each set of cross-lingualembeddings, and evaluate the precision at 1 (p@1)with respect to the gold standard test dictionaryfrom the muse dataset (conneau et al., 2018a).
for the few out-of-vocabulary source words, werevert to copying as a back-off strategy,9 so ourreported numbers are directly comparable to priorwork in terms of coverage..xnli.
we train an english natural language in-ference model on multinli (williams et al., 2018),and evaluate the zero-shot cross-lingual transferperformance on the xnli test set (conneau et al.,2018b) for the subset of our languages covered by it.
to that end, we follow glavaˇs et al.
(2019) and trainan enhanced sequential inference model (esim)on top of our original english embeddings, whichare kept frozen during training.
at test time, wetransfer into the rest of the languages by pluggingin the corresponding aligned embeddings.
notethat we use the exact same english model for ourproposed method and the baseline muse and icpsystems,10 which only differ in the set of aligned.
9this has a negligible impact in practice, as it accounts forless than 1.4% of the test cases.
moreover, all of our systemsuse the same underlying vocabulary, so they are affected inthe exact same way..10this is possible because they all ﬁx the target languageembeddings (english in this case) and align the embeddings.
embeddings used for cross-lingual transfer.
in con-trast, vecmap and joint align also manipulate thetarget english embeddings, which would requiretraining a separate model for each language pair,so we decide to exclude them from this set of ex-periments.11.
we next discuss our main results on bli (§5.1) andxnli (§5.2), followed by our ablation study (§5.3)and error analysis (§5.4) on bli..5 results.
5.1 bli.
table 3 comprises our main bli results.
we ob-serve that our method obtains the best results inall directions (matched by vecmap in russian-english), outperforming the strongest baseline by2.4 points on average for the mapping based initial-ization.
our improvements are more pronounced inthe backward direction (3.1 points on average) butstill substantial in the forward direction (1.7 pointson average)..it is worth noting that some systems fail to con-verge to a good solution for the most challeng-ing language pairs.
this includes our proposedmethod in the case of chinese-english when usingthe numeral-based initialization, which we attributeto the smaller size of the initial dictionary (only 244entries, see table 2).
other than that, we observethat our approach obtains very similar results re-gardless of the initial dictionary.
quite remarkably,.
in the source language with them, either through mapping(muse, icp) or learning from scratch (ours)..11in addition to the computational overhead, having sepa-rate models introduces some variance, while our comparisonis more direct..6483de-en.
es-en.
fr-en.
ru-en.
→ ←.
→ ←.
→ ←.
→ ←.
conneau et al.
(2018a)hoshen and wolf (2018)grave et al.
(2018)alvarez-melis and jaakkola (2018)yang et al.
(2018)mukherjee et al.
(2018)alvarez-melis et al.
(2018)xu et al.
(2018)wang et al.
(2019)zhou et al.
(2019)li et al.
(2020).
72.2 74.073.0 74.773.3 75.472.8 71.970.3 71.5.
-.
-.
71.1 73.867.0 69.372.2 74.274.4 77.274.3 75.3.
83.3 81.784.1 82.184.1 82.880.4 81.779.3 79.979.2 84.581.8 81.377.8 79.584.2 81.484.9 82.884.6 82.4.
82.1 82.382.9 82.382.9 82.678.9 81.378.9 78.4.
-.
-.
81.6 82.975.5 77.983.6 82.883.5 83.183.7 82.6.
59.1 44.061.8 47.559.1 43.743.7 45.1.
55.4 41.7.
58.3 45.063.6 49.2.
--.
-.
-.
--.
-.
-.
ours (mapping init).
76.8 78.1.
86.3 84.2.
84.9 84.9.
65.7 51.5.avg.
72.373.673.069.5--71.2-72.774.8-.
76.6.table 4: bli results on muse dataset in comparison with prior published results (p@1).
all systems are fullyunsupervised (except that of zhou et al.
(2019), which uses identical words as a seed dictionary), and use sgnsembeddings trained on wikipedia..museicp.
fr.
es.
de.
enzh73.9 65.0 68.1 67.9 39.1∗ 61.436.1∗73.9 62.2 64.2 65.7 59.4.ru.
ours (identical init) 73.9 65.0 68.7 67.1 63.573.9 65.0 68.6 67.1 63.3ours (numeral init)ours (mapping init) 73.9 65.1 68.6 67.0 63.5.
49.834.9∗49.4.table 5: xnli results (accuracy).
asterisks denote di-vergence (< 5% p@1 in bli)..the variant using vecmap for initialization (map-ping init) is substantially stronger than vecmapitself despite not using any additional training sig-nal..so as to put our results into perspective, table 4compares them to previous numbers reported in theliterature.
note that the numbers are comparable interms of coverage and all systems use wikipediaas the training corpus, although they might differon the speciﬁc dump used and the preprocessingdetails.12 as it can be seen, our approach obtainsthe best results by a substantial margin.13.
5.2 xnli.
basic method (identical init).
+ self-learning.
+ iterative restarts.
basic method (numeral init).
+ self-learning.
+ iterative restarts.
basic method (mapping init).
+ self-learning.
+ iterative restarts.
53.966.967.3.
2.653.961.0.
67.567.567.5.table 6: ablation results on bli (average p@1).
mapping systems, achieving the best results on 3out of the 5 transfer languages by a small margin.
nevertheless, it signiﬁcantly lags behind museon chinese, even if the exact same set of cross-lingual embeddings performed better than museat bli.
while striking, similar discrepancies be-tween bli and xnli performance where also ob-served in previous studies (glavaˇs et al., 2019).
finally, we observe that the initial dictionary hasa negligible impact in the performance of our pro-posed method, which supports the idea that ourapproach converges to a similar solution given anyreasonable initialization..we report our xnli results in table 5. we observethat our method is competitive with the baseline.
5.3 ablation study.
12in particular, most mapping methods use the ofﬁcialwikipedia embeddings from fasttext.
unfortunately, the pre-processed corpus used to train these embeddings is not public,so works that explore other approaches, like ours, need to usetheir own pre-processed copy of wikipedia..13artetxe et al.
(2019) report even stronger results based onunsupervised machine translation instead of direct retrievalwith clwes.
note, however, that their method still relies oncross-lingual embeddings to build the underlying phrase-table,so our improvements should be largely orthogonal to theirs..so as to understand the role of self-learning andthe iterative restarts in our approach, we performan ablation study and report our results in table 6.we observe that the contribution of these compo-nents is greatly dependant on the initial dictionary.
for the numeral initialization, the basic methodworks poorly, and both extensions bring large im-provements.
in contrast, the identical initialization.
6484source−target.
target−source.
40.
1@p.60.
20.
0.ours (identical init).
ours (numeral init).
ours (mapping init).
vecmap.
0.
50.
100.
50.
100.
150.
150.
0.re−induction number.
figure 1: finnish-english learning curves (bli p@1).
the iterative restarts happen at the vertical lines..does not beneﬁt from iterative restarts, but self-learning still plays a major role.
in the case ofthe mapping-based initialization, the basic methodis already very competitive.
this suggests thatboth the self-learning and the iterative restarts arehelpful to make the method more robust to a weakinitialization, and have a minor impact otherwise.
in order to better understand the underlyinglearning dynamics, we analyze the learning curvesfor finnish-english in figure 1. we observe that,when the initial dictionary is strong, our methodsurpasses the baseline and stabilizes early.
in con-trast, convergence is much slower when using theweak numeral-based initialization, and the iterativerestarts are critical to escape poor local optima..5.4 error analysis.
so as to better understand where our improvementsin bli are coming from, we perform an error anal-ysis on the spanish-english direction.
to that end,we manually inspect the 69 instances for which ourmethod (with mapping-based initialization) pro-duced a correct translation while vecmap failedaccording to the gold standard, as well as the 26instances for which the opposite was true.
we thencategorize these errors into several types, which aresummarized in table 7..we observe that, in 52.6% of the 95 analyzedinstances, the translation produced by our methodis identical to the source word, while this percent-age goes down to 4.2% for vecmap.
this tendencyof our approach to copy its input is striking, as themodel has no notion about the words being iden-tically spelled.14 a large portion of these cases.
correspond to named entities, where copying is theright behavior, while vecmap outputs a differentproper noun.
there are also some instances wherethe input word is in the target language,15 whichcan be considered an artifact of the dataset, butcopying also seems the most reasonable behaviorin these cases.
finally, there are also a few caseswhere the input word is present in the target vocab-ulary, which is selected by our method and countedas an error.
once again, we consider these to bean artifact of the dataset, as copying seems a rea-sonable choice if the input word is considered tobe part of the target language vocabulary.
the re-maining cases where neither method copies mostlycorrespond to common errors, where one of the sys-tems (most often vecmap) outputs a semanticallyrelated but incorrect translation.
however, thereare also a few instances where both translationsare correct, but one of them is missing in the goldstandard..with the aim to understand the impact of identi-cal words in our original results, we re-evaluatedthe systems using a ﬁltered version of the musegold standard dictionaries, where we removed allsource words that were included in the set of can-didate translations.
in order to be fair, we ﬁlteredout identical words from the output of the system,reverting to the second highest-ranked translationwhenever the ﬁrst one is identical to the sourceword.
the results for the strongest system in eachfamily are shown in table 8. even if the mar-gin of improvement is reduced compared to table3, the best results are still obtained by our pro-posed method, bringing an average improvement.
14the variants of our system with identical or numeralinitialization do indirectly see this signal, but the one analyzedhere is initialized with the vecmap mapping..15english words will often appear in other languages as partof named entities (e.g., “pink” as part of “pink floyd”), whichexplains the presence of such words in the spanish vocabulary..6485gold standard.
type.
common errors.
ours right–vecmap wrong.
named entity, ours copies.
21.1%.
en word in es vocab, ours copies.
15.8%.
gap in gold standard.
common errors.
vecmap right–ours wrong.
es word in en vocab, ours copies.
7.4%.
gap in gold standard.
4.2%.
cases.
30.5%.
5.3%.
15.8%.
source.
derrotascampeona.
philadelphiasusana.
pinkspace.
examples.
vecmap.
victoriesmedalist.
pittsburghbeatriz.
tangerinesci.
ours.
defeatschampion.
philadelphiasusana.
pinkspace.
adecuadamarquesa.
appropriatemarchioness.
adequatemarquise.
conservadoresnoveno.
conservativesninth.
calzadacantera.
ferroviariosituados.
roadwayquarry.
railwaysituated.
liberalstenth.
calzadacantera.
railpositioned.
table 7: bli error analysis on spanish-english.
see section 5.4 for details..de-en.
es-en.
fr-en.
ﬁ-en.
ru-en.
zh-en.
→ ←.
→ ←.
→ ←.
→ ←.
→ ←.
→ ←.
vecmap (artetxe et al., 2018)joint align (wang et al., 2019).
68.3 70.257.0 53.3.
85.1 79.463.0 57.4.
80.8 78.170.2 64.4.
58.4 38.94.0∗ 0.7∗.
66.1 48.631.3 22.4.
45.0 34.53.5∗ 0.9∗.
ours (identical init)ours (mapping init).
68.9 72.268.9 72.3.
86.0 80.785.8 80.8.
81.5 80.081.4 80.2.
54.0 41.055.4 41.6.
65.7 50.966.1 51.0.
44.6 38.145.1 37.9.avg.
62.835.7.
63.663.9.table 8: bli results on muse with identical words removed (p@1).
asterisks denote divergence (< 5% p@1)..of 1.1 points.
it is also worth noting that joint align,which shares a portion of the vocabulary for bothlanguages (and will thus translate all words in theshared vocabulary identically), suffers a large dropin performance.
this highlights the importanceof accompanying quantitative bli evaluation withan error analysis as urged by previous studies (ke-mentchedjhieva et al., 2019)..6 conclusions and future work.
our approach for learning clwes addresses themain limitations of both ofﬂine mapping and jointlearning methods.
different from mapping ap-proaches, it does not suffer from structural mis-matches arising from independently training em-beddings in different languages, as it works byconstraining the learning of the source embeddingsso they are aligned with the target ones.
at thesame time, unlike previous joint methods, our sys-tem can work without any parallel resources, re-lying on numerals, identical words or an existingmapping method for the initialization.
we achievethis by combining cross-lingual anchoring with.
self-learning and iterative restarts.
while recent re-search on clwes has been dominated by mappingapproaches, our work shows that the fundamentaltechniques that popularized these methods (e.g.,the use of self-learning to relax the need for cross-lingual supervision) can also be effective beyondthis paradigm..despite its simplicity, our experiments on blishow the superiority of our method when com-pared to previous mapping systems.
we comple-ment these results with additional experiments ona downstream task, where our method obtains com-petitive results, as well as an ablation study and asystematic error analysis.
we identify a strikingtendency of our method to translate words identi-cally, even if it has no notion of the words beingidentically spelled.
thanks to this, our methodis particularly strong at translating named entities,but we show that our improvements are not lim-ited to this phenomenon.
these insights conﬁrmthe value of accompanying quantitative results onbli with qualitative evaluation (kementchedjhievaet al., 2019) and/or other tasks (glavaˇs et al., 2019)..6486in the future, we would like to further exploreclwe methods that go beyond the currently dom-inant mapping paradigm.
in particular, we wouldlike to remove the requirement of a seed dictio-nary altogether by using adversarial learning, andexplore more elaborated context translation anddictionary re-induction schemes..acknowledgments.
aitor ormazabal, aitor soroa, gorka labaka andeneko agirre were supported by the basque gov-ernment (excellence research group it1343-19 anddeeptext project kk-2020/00088), project big-knowledge (ayudas fundaci´on bbva a equiposde investigaci´on cient´ıﬁca 2018) and the spanishmineco (project domino pgc2018-102041-b-i00 mciu/aei/feder, ue).
aitor ormazabalwas supported by a doctoral grant from the spanishmecd..references.
david alvarez-melis and tommi jaakkola.
2018.gromov-wasserstein alignment of word embeddingspaces.
in proceedings of the 2018 conference onempirical methods in natural language processing,pages 1881–1890, brussels, belgium.
associationfor computational linguistics..david alvarez-melis,.
andtommi s jaakkola.
2018.towards optimaltransport with global invariances.
arxiv preprintarxiv:1806.09277..stefanie.
jegelka,.
mikel artetxe, gorka labaka, and eneko agirre.
2017.learning bilingual word embeddings with (almost)no bilingual data.
in proceedings of the 55th annualmeeting of the association for computational lin-guistics (volume 1: long papers), pages 451–462,vancouver, canada.
association for computationallinguistics..mikel artetxe, gorka labaka, and eneko agirre.
2018.a robust self-learning method for fully unsupervisedcross-lingual mappings of word embeddings.
in pro-ceedings of the 56th annual meeting of the associa-tion for computational linguistics (volume 1: longpapers), pages 789–798.
association for computa-tional linguistics..mikel artetxe, gorka labaka, and eneko agirre.
2019.bilingual lexicon induction through unsupervisedmachine translation.
in proceedings of the 57th an-nual meeting of the association for computationallinguistics, pages 5002–5007, florence, italy.
asso-ciation for computational linguistics..mikel artetxe, sebastian ruder, and dani yogatama.
2020. on the cross-lingual transferability of mono-lingual representations.
in proceedings of the 58th.
annual meeting of the association for computa-tional linguistics, pages 4623–4637, online.
asso-ciation for computational linguistics..piotr bojanowski, edouard grave, armand joulin, andtomas mikolov.
2017. enriching word vectors withsubword information.
transactions of the associa-tion for computational linguistics, 5:135–146..alexis conneau, guillaume lample, marc’aurelioranzato, ludovic denoyer, and herv´e j´egou.
2018a.
word translation without parallel data.
in proceed-ings of the 6th international conference on learningrepresentations (iclr 2018)..alexis conneau, ruty rinott, guillaume lample, ad-ina williams, samuel bowman, holger schwenk,and veselin stoyanov.
2018b.
xnli: evaluatingcross-lingual sentence representations.
in proceed-ings of the 2018 conference on empirical methodsin natural language processing, pages 2475–2485,brussels, belgium.
association for computationallinguistics..long duong, hiroshi kanayama, tengfei ma, stevenbird, and trevor cohn.
2016. learning crosslingualword embeddings without bilingual corpora.
in pro-ceedings of the 2016 conference on empirical meth-ods in natural language processing, pages 1285–1295, austin, texas.
association for computationallinguistics..goran glavaˇs, robert litschko, sebastian ruder, andivan vuli´c.
2019. how to (properly) evaluate cross-lingual word embeddings: on strong baselines, com-parative analyses, and some misconceptions.
in pro-ceedings of the 57th annual meeting of the associa-tion for computational linguistics, pages 710–721,florence, italy.
association for computational lin-guistics..goran glavaˇs and ivan vuli´c.
2020..non-linearinstance-based cross-lingual mapping for non-in proceedings ofisomorphic embedding spaces.
the 58th annual meeting of the association for com-putational linguistics, pages 7548–7555, online.
association for computational linguistics..stephan gouws, yoshua bengio, and greg corrado.
2015. bilbowa: fast bilingual distributed repre-sentations without word alignments.
in proceedingsof the 32nd international conference on machinelearning, pages 748–756..stephan gouws and anders søgaard.
2015. simplein pro-task-speciﬁc bilingual word embeddings.
ceedings of the 2015 conference of the north amer-ican chapter of the association for computationallinguistics: human language technologies, pages1386–1390, denver, colorado.
association for com-putational linguistics..edouard grave, armand joulin, and quentin berthet.
unsupervised alignment of embeddingsarxiv preprint.
2018.with wasserstein procrustes.
arxiv:1805.11222..6487yedid hoshen and lior wolf.
2018. non-adversarialin proceedings ofunsupervised word translation.
the 2018 conference on empirical methods in nat-ural language processing, pages 469–478, brus-sels, belgium.
association for computational lin-guistics..yova kementchedjhieva, mareike hartmann, and an-ders søgaard.
2019. lost in evaluation: misleadingbenchmarks for bilingual dictionary induction.
inproceedings of the 2019 conference on empiricalmethods in natural language processing and the9th international joint conference on natural lan-guage processing (emnlp-ijcnlp), pages 3336–3341, hong kong, china.
association for computa-tional linguistics..guillaume lample, myle ott, alexis conneau, lu-dovic denoyer, and marc’aurelio ranzato.
2018.phrase-based & neural unsupervised machine trans-in proceedings of the 2018 conference onlation.
empirical methods in natural language processing,pages 5039–5049, brussels, belgium.
associationfor computational linguistics..yanyang li, yingfeng luo, ye lin, quan du, huizhenwang, shujian huang, tong xiao, and jingbo zhu.
2020. a simple and effective approach to robustunsupervised bilingual dictionary induction.
inproceedings of the 28th international conferenceon computational linguistics, pages 5990–6001,barcelona, spain (online).
international committeeon computational linguistics..thang luong, hieu pham, and christopher d. man-ning.
2015. bilingual word representations withmonolingual quality in mind.
in proceedings of the1st workshop on vector space modeling for naturallanguage processing, pages 151–159.
associationfor computational linguistics..tomas mikolov, ilya sutskever, kai chen, greg s cor-rado, and jeff dean.
2013. distributed representa-tions of words and phrases and their compositional-ity.
in advances in neural information processingsystems 26, pages 3111–3119..tasnim mohiuddin, m saiful bari, and shaﬁq joty.
2020. lnmap: departures from isomorphic as-sumption in bilingual lexicon induction through non-linear mapping in latent space.
in proceedings of the2020 conference on empirical methods in naturallanguage processing (emnlp), pages 2712–2723,online.
association for computational linguistics..tanmoy mukherjee, makoto yamada, and timothyhospedales.
2018. learning unsupervised wordtranslations without adversaries.
in proceedings ofthe 2018 conference on empirical methods in nat-ural language processing, pages 627–632, brus-sels, belgium.
association for computational lin-guistics..ndapa nakashole.
2018. norma: neighborhood sen-insitive maps for multilingual word embeddings..proceedings of the 2018 conference on empiricalmethods in natural language processing, pages512–522, brussels, belgium.
association for com-putational linguistics..ndapa nakashole and raphael flauger.
2018. charac-terizing departures from linearity in word translation.
in proceedings of the 56th annual meeting of theassociation for computational linguistics (volume2: short papers), pages 221–227, melbourne, aus-tralia.
association for computational linguistics..aitor ormazabal, mikel artetxe, gorka labaka, aitorsoroa, and eneko agirre.
2019. analyzing the lim-itations of cross-lingual word embedding mappings.
in proceedings of the 57th annual meeting of theassociation for computational linguistics, pages4990–4995, florence, italy.
association for compu-tational linguistics..barun patra, joel ruben antony moniz, sarthak garg,matthew r gormley, and graham neubig.
2019.bliss in non-isometric embedding spaces..jeffrey pennington, richard socher, and christophermanning.
2014. glove: global vectors for wordrepresentation.
in proceedings of the 2014 confer-ence on empirical methods in natural languageprocessing (emnlp), pages 1532–1543, doha,qatar.
association for computational linguistics..anders søgaard, sebastian ruder, and ivan vuli´c.
2018. on the limitations of unsupervised bilingualdictionary induction.
in proceedings of the 56th an-nual meeting of the association for computationallinguistics (volume 1: long papers), pages 778–788. association for computational linguistics..ivan vuli´c and marie-francine moens.
2016. bilingualdistributed word representations from document-aligned comparable data.
journal of artiﬁcial intel-ligence research, 55(1):953–994..ivan vuli´c, sebastian ruder, and anders søgaard.
2020. are all good word vector spaces isomorphic?
in proceedings of the 2020 conference on empiricalmethods in natural language processing (emnlp),pages 3178–3192, online.
association for computa-tional linguistics..zirui wang, jiateng xie, ruochen xu, yiming yang,graham neubig, and jaime carbonell.
2019. cross-lingual alignment vs joint training: a compara-tive study and a simple uniﬁed framework.
arxivpreprint arxiv:1910.04708..adina williams, nikita nangia, and samuel bowman.
2018. a broad-coverage challenge corpus for sen-tence understanding through inference.
in proceed-ings of the 2018 conference of the north americanchapter of the association for computational lin-guistics: human language technologies, volume1 (long papers), pages 1112–1122, new orleans,louisiana.
association for computational linguis-tics..6488ruochen xu, yiming yang, naoki otani, and yuexinwu.
2018. unsupervised cross-lingual transfer ofword embedding spaces.
in proceedings of the 2018conference on empirical methods in natural lan-guage processing, pages 2465–2474, brussels, bel-gium.
association for computational linguistics..pengcheng yang, fuli luo, shuangzhi wu, jingjingxu, dongdong zhang, and xu sun.
2018. learningunsupervised word mapping by maximizing meandiscrepancy.
arxiv preprint arxiv:1811.00275..meng zhang, yang liu, huanbo luan, and maosongsun.
2017. adversarial training for unsupervisedin proceedings of thebilingual lexicon induction.
55th annual meeting of the association for com-putational linguistics (volume 1: long papers),pages 1959–1970, vancouver, canada.
associationfor computational linguistics..chunting zhou, xuezhe ma, di wang, and grahamneubig.
2019. density matching for bilingual wordembedding.
in proceedings of the 2019 conferenceof the north american chapter of the associationfor computational linguistics: human languagetechnologies, volume 1 (long and short papers),pages 1588–1598, minneapolis, minnesota.
associ-ation for computational linguistics..6489