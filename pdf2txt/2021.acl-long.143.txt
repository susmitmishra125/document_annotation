implicit representations of meaning in neural language models.
belinda z. li maxwell nye.
jacob andreas.
massachusetts institute of technology{bzl,mnye,jda}@mit.edu.
abstract.
does the effectiveness of neural language mod-els derive entirely from accurate modeling ofsurface word co-occurrence statistics, or dothese models represent and reason about theworld they describe?
in bart and t5 trans-former language models, we identify contex-tual word representations that function as mod-els of entities and situations as they evolvethroughout a discourse.
these neural represen-tations have functional similarities to linguis-tic models of dynamic semantics: they supporta linear readout of each entity’s current prop-erties and relations, and can be manipulatedwith predictable effects on language genera-tion.
our results indicate that prediction in pre-trained neural language models is supported,at least in part, by dynamic representations ofmeaning and implicit simulation of entity state,and that this behavior can be learned with onlytext as training data.1.
1.introduction.
figure 1: neural language models trained on textalone (a–c) produce semantic representations that en-code properties and relations of entities mentioned ina discourse (a(cid:48)).
representations are updated when thediscourse describes changes to entities’ state (b(cid:48))..neural language models (nlms), which placeprobability distributions over sequences of words,produce contextual word and sentence embeddingsthat are useful for a variety of language process-ing tasks (peters et al., 2018; lewis et al., 2020).
this usefulness is partially explained by the factthat nlm representations encode lexical relations(mikolov et al., 2013) and syntactic structure (ten-ney et al., 2019).
but the extent to which nlmtraining also induces representations of meaningremains a topic of ongoing debate (bender andkoller, 2020; wu et al., 2021).
in this paper, weshow that nlms represent meaning in a speciﬁcsense: in simple semantic domains, they build rep-resentations of situations and entities that encodelogical descriptions of each entity’s dynamic state..1code and data are available at https://github.com/.
belindal/state-probes..consider the text in the left column of fig.
1.sentences (a) describe the contents of a room; thissituation can be formally characterized by the graphof entities, properties, and relations depicted in (a(cid:48)).
sentence (b), you pick up the key, causes the situa-tion to change: a chest becomes empty, and a keybecomes possessed by you rather than containedby the chest (b(cid:48)).
none of these changes are explic-itly described by sentence (b).
nevertheless, theset of sentences that can follow (a)–(b) to form asemantically coherent discourse is determined bythis new situation.
an acceptable next sentencemight feature the person using the key (c1) or per-forming an unrelated action (c2).
but a sentence inwhich the person takes an apple out of the chest (c3)cannot follow (a)–(b), as the chest is now empty..formal models of situations (built, like (a(cid:48))–(b(cid:48)),from logical representations of entities and their.
proceedingsofthe59thannualmeetingoftheassociationforcomputationallinguisticsandthe11thinternationaljointconferenceonnaturallanguageprocessing,pages1813–1827august1–6,2021.©2021associationforcomputationallinguistics1813you see an open chest.
the only thing in the chest is an old key.
there is a locked wooden door leading east.
next, you… (c1)   …use the key to unlock the door.
(c2)   …drop an apple on the ground.
(c3)   …remove an apple from the             chest.you pick up the key.
(a)chestopencontainskeyyou(a )′ (b)lm encodersemantic probelm decoder(b )′ doorlockedchestopenpossesseskeyyoudoorlockedemptyattributes) are central to linguistic theories of mean-ing.
nlms face the problem of learning to generatecoherent text like (a–c) without access to any ex-plicit supervision for the underlying world state(a(cid:48))–(b(cid:48)).
indeed, recent work in nlp points to thelack of exposure of explicit representations of theworld external to language as prima facie evidencethat lms cannot represent meaning at all, and thuscannot in general output coherent discourses like(a)–(c) (bender and koller, 2020)..the present paper can be viewed as an empiricalresponse to these arguments.
it is true that cur-rent nlms do not reliably output coherent descrip-tions when trained on data like (a)–(c).
but fromtext alone, even these imperfect nlms appear tolearn implicit models of meaning that are translat-able into formal state representations like (a(cid:48))–(b(cid:48)).
these state representations capture information likethe emptiness of the chest in (b(cid:48)), which is notexplicitly mentioned and cannot be derived fromany purely syntactic representation of (a)–(b), butfollows as a semantically necessary consequence.
these implicit semantic models are roughly analo-gous to the simplest components of discourse repre-sentation theory and related formalisms: they rep-resent sets of entities, and update the facts that areknown about these entities as sentences are addedto a discourse.
like the nlms that produce them,these implicit models are approximate and error-prone.
nonetheless, they do most of the thingswe expect of world models in formal semantics:they are structured, queryable and manipulable.
inthis narrow sense, nlm training appears to inducenot just models of linguistic form, but models ofmeaning..this paper begins with a review of existing ap-proaches to nlm probing and discourse represen-tation that serve as a foundation for our approach.
we then formalize a procedure for determiningwhether nlm representations encode representa-tions of situations like fig.
1 (a(cid:48))–(b(cid:48)).
finally, weapply this approach to bart and t5 nlms trainedon text from the english-language alchemy andtextworld datasets.
in all cases, we ﬁnd evidenceof implicit meaning representations that:.
1. can be linearly decoded from nlm encodings.
of entity mentions..2. are primarily attributable to open-domain pre-training rather than in-domain ﬁne-tuning..3. inﬂuence downstream language generation..we conclude with a discussion of the implicationsof these results for evaluating and improving factu-ality and coherence in nlms..2 background.
what do lm representations encode?
this pa-per’s investigation of state representations buildson a large body of past work aimed at under-standing how other linguistic phenomena are rep-resented in large-scale language models.
nlmrepresentations have been found to encode syn-tactic categories, dependency relations, and coref-erence information (tenney et al., 2019; hewittand manning, 2019; clark et al., 2019).
withinthe realm of semantics, existing work has identi-ﬁed representations of word meaning (e.g., ﬁne-grained word senses; wiedemann et al.
2019) andpredicate–argument structures like frames and se-mantic roles (kovaleva et al., 2019)..in all these studies,.
the main experimentalparadigm is probing (shi et al., 2016; belinkovand glass, 2019): given a ﬁxed source of repre-sentations (e.g.
the bert language model; devlinet al.
2019) and a linguistic label of interest (e.g.
semantic role), a low-capacity “probe” (e.g a linearclassiﬁer) is trained to predict the label from therepresentations (e.g.
to predict semantic roles frombert embeddings).
a phenomenon is judged tobe encoded by a model if the probe’s accuracy can-not be explained by its accuracy when trained oncontrol tasks (hewitt and liang, 2019) or baselinemodels (pimentel et al., 2020)..our work extends this experimental paradigmto a new class of semantic phenomena.
as in pastwork, we train probes to recover semantic annota-tions, and interpret these probes by comparison tonull hypotheses that test the role of the model andthe difﬁculty of the task.
the key distinction is thatwe aim to recover a representation of the situationdescribed by a discourse rather than representa-tions of the sentences that make up the discourse.
for example, in fig.
1, we aim to understand notonly whether nlms encode the (sentence-level)semantic information that there was a picking upevent whose patient was you and whose agent wasthe key—we also wish to understand whether lmsencode the consequences of this action for all en-tities under discussion, including the chest fromwhich the key was (implicitly) taken..how might lms encode meaning?
like inother probing work, an attempt to identify neural.
1814lms and other semantic phenomena in addi-tion to work on interpretability, a great deal of pastresearch uses language modeling as a pretrainingscheme for more conventional (supervised) seman-tics tasks in nlp.
lm pretraining is useful for se-mantic parsing (einolghozati et al., 2019), instruc-tion following (hill et al., 2020), and even imageretrieval (ilharco et al., 2021).
here, our primaryobjective is not good performance on downstreamtasks, but rather understanding of representationsthemselves.
lm pretraining has also been foundto be useful for tasks like factoid question answer-ing (petroni et al., 2019; roberts et al., 2020).
ourexperiments do not explore the extent to whichlms encode static background knowledge, but in-stead the extent to which they can build representa-tions of novel situations described by novel text..3 approach.
overview we train probing models to testwhether nlms represent the information statesspeciﬁed by the input text.
we speciﬁcally probefor the truth values of logical propositions aboutentities mentioned in the text.
for example, in fig-ure 1, we test whether a representation of sentences(a)–(b) encodes the fact that empty(chest) is trueand contains(chest, key) is false..meanings as information states to formalizethis: given a universe consisting of a set of entities,properties, and relations, we deﬁne a situation asa complete speciﬁcation of the properties and rela-tions of each entity.
for example, the box labeled i0in fig.
2 shows three situations involving a chest, akey, an apple, an eaten property and a containsrelation.
in one situation, the chest contains the keyand the apple is eaten.
in another, the chest containsthe apple, and the apple is not eaten.
in general, asituation assigns a value of true or false to everylogical proposition of the form p (x) or r(x, y)(e.g.
locked(door) or contains(chest, key))..now, given a natural language discourse, we canview that discourse as specifying a set of possiblein fig.
2, the sentence x0 picks outsituations.
the subset of situations in which the chest containsthe key.
a collection of situations is called aninformation state, because it encodes a listener’s.
semantics is a precise treatment of quantiﬁcation and scopeat the discourse level.
the tasks investigated in this paperdo not involve any interesting quantiﬁcation, and rely on thesimplest parts of the formalism.
more detailed exploration ofquantiﬁcation in nlms is an important topic for future study..figure 2: a collection of possible situations is repre-sented as an information state (i0).
information statesassign values to propositions φi,j according to whetherthey are true, false, or undetermined in all the situa-tions that make up an information state.
appendinga new sentence discourse causes the information stateto be updated (i1).
in this case, the sentence the onlything in the chest is an old key causes contains(chest,key) to become true, contains(chest, apple) to be-come false, and leaves eaten(apple) undetermined..encodings of entities and situations must begin witha formal framework for representing them.
thisis the subject of dynamic semantics in linguis-tics (heim, 2008; kamp et al., 2010; groenendijkand stokhof, 1991).
the central tool for represent-ing meaning in these approaches is the informationstate: the set of possible states of the world consis-tent with a discourse (i0 and i1 in fig.
2).
beforeanything is said, all logically consistent situationsare part of the information state (i0).
each newsentence in a discourse provides an update (thatconstrains or otherwise manipulates the set of pos-sible situations).
as shown in the ﬁgure, theseupdates can affect even unmentioned entities: thesentence the only thing in the chest is a key en-sures that the proposition contains(chest, x) isfalse for all entities x other than the key.
this isformalized in §3 below.2.
the main hypothesis explored in this paper isthat lms represent (a particular class of) informa-tion states.
given an lm trained on text alone, anda discourse annotated post-hoc with informationstates, our probes will try to recover these informa-tion states from lm representations.
the semanticsliterature includes a variety of proposals for howinformation states should be represented; here, wewill represent information states logically, and de-code information states via the truth values thatthey assign to logical propositions (φi,j in fig.
2).3.
2see also yalcin (2014) for an introductory survey.
3in existing work, one of the main applications of dynamic.
1815contains: the only thing in the chest is an old key.x0containseatencontainseatencontains: contains(chest, key) = ?
: contains(chest, apple) = ?
: eaten(apple) = ?ϕ0,0ϕ0,1ϕ0,2i0i1: contains(chest, key) = t : contains(chest, apple) = f : eaten(apple) = ?ϕ1,0ϕ1,1ϕ1,2contains…figure 3: overview of the probe model.
left: alchemy.
right: textworld.
the lm is ﬁrst trained to generate thenext instruction from prior context (left side, both ﬁgures).
next, the lm encoder is frozen and a probe is trainedto recover (the truthfulness of) propositions about the current state from speciﬁc tokens of encoder outputs..knowledge of (and uncertainty about) the state ofthe world resulting from the events described in adiscourse.4 in a given information state, the valueof a proposition might be true in all situations, falsein all situations, or unknown:true in some butfalse in others.
an information state (or an nlmrepresentation) can thus be characterized by thelabel it assigns to every proposition..probing for propositions we assume we aregiven:.
• a sequence of sentences x1:n = [x1, .
.
.
, xn]..• for each i, the information state ii that resultsfrom the sentences x1:i. we write i(φ) ∈{t, f, ?}
for the value of the proposition φ inthe information state i..• a language model encoder e that maps sen-tences to sequences of d-dimensional wordrepresentations..to characterize the encoding of semantic informa-tion in e(x), we design a semantic probe thattries to recover the contents of ii from e(x1:i)proposition-by-proposition.
intuitively, this probeaims to answer three questions: (1) how is the truthvalue of a given proposition φ encoded?
(linearly?
nonlinearly?
in what feature basis?)
(2) where isinformation about φ encoded?
(distributed acrossall token embeddings?
local to particular tokens?)
(3) how well is semantic information encoded?
(can it be recovered better than chance?
perfectly?).
4an individual sentence is associated with a context changepotential: a map from information states to information states..the probe is built from three components, eachof which corresponds to one of the questions above:.
1. a proposition embedder embed : l → rd(where l is the set of logical propositions)..2. a localizer loc : l × rd → rd whichextracts and aggregates lm representationsas candidates for encoding φ. the localizerextracts tokens of e(x) at positions corre-sponding to particular tokens in the under-lying text x. we express this in notation ase(x)[*], where * is a subsequence of x.
(forexample, if x = “the third beaker is empty”.
e(x) = [v1, v2, v3, v4, v5] has one vector pertoken.
e(x)[“third beaker”] = [v2, v3].).
3. a classiﬁer clsθ : rd × rd → {t, f, ?
},which takes an embedded proposition and alocalized embedding, and predicts the truthvalue of the proposition..we say that a proposition φ is encoded by e(x) if:.
clsθ(embed(φ), loc(φ, e(x))) = i(φ) ..(1).
given a dataset of discourses d, we attempt to ﬁnda classiﬁer parameters θ from which all proposi-tions can be recovered for all sentences in eq.
(1).
to do so, we label each with the truth/falsehoodof every relevant proposition.
we then train theparameters of a clsθ on a subset of these propo-sitions and test whether it generalizes to held-outdiscourses..1816ttkeychestopenpossessesyouemptyftlm encoderlm decodertextworlddrain 2 from the first beaker.localizerlm encoderlm decoder> unlock wooden door you unlock the wooden door with the old key.
localizerxalchemyxe(x)e(x)classifierclassifierembed?embedthe first beaker has 1 green, the second beaker has 2 red, the third beaker has 3 red.
pour the last red beaker into beaker 1. mix.has-4-brown(beaker 1)has-2-red(beaker 2)matches(old key, wooden door)in(old key, chest)open(chest)you see an open chest.
the only thing in the chest is an old key.
there is a locked wooden door leading east.
> take old key you take the old key from the chest.
4 experiments.
our experiments aim to discover to what extent(and in what manner) information states are en-coded in nlm representations.
we ﬁrst present aspeciﬁc instantiation of the probe that allows us todetermine how well information states are encodedin two nlms and two datasets (§4.2); then providea more detailed look at where speciﬁc propositionsare encoded by varying loc (§4.3).
finally, we de-scribe an experiment investigating the causal roleof semantic representations by directly manipulat-ing e(x) (§4.4).5.is fully speciﬁed, the information state associatedwith each instruction preﬁx consists of only a singlepossible situation, deﬁned by a set of propositions:.
φ =(cid:8)has-v-c(b) :.
b ∈ {beaker 1, beaker 2, .
.
.},.
v ∈ 1..4,c ∈ {red, orange, yellow, .
.
.
}(cid:9) ..(2).
in the experiments below, it will be useful to haveaccess to a natural language representation of eachproposition.
we denote this:.
4.1 preliminaries.
nl(has-v-c(b)) = “the b beaker has v c” ..(3).
in all experiments, the encoder e comesmodelfrom a bart (lewis et al., 2020) or t5 (raffelet al., 2020) model.
except where noted, bartis pretrained on openwebtext, bookcorpus, cc-news, and stories (lewis et al., 2020), t5 is pre-trained on c4 (raffel et al., 2020), and both areﬁne-tuned on the textworld or alchemy datasetsdescribed below.
weights of e are frozen duringprobe training..data: alchemy alchemy,the ﬁrst datasetused in our experiments,is derived from thescone (long et al., 2016) semantic parsing tasks.
we preserve the train / development split from theoriginal dataset (3657 train / 245 development).
every example in the dataset consists of a human-generated sequence of instructions to drain, pour,or mix a beaker full of colored liquid.
each instruc-tion is annotated with the ground-truth state thatresults from following that instruction (figure 3).
we turn alchemy into a language modelingdataset by prepending a declaration of the initialstate (the initial contents of each beaker) to theactions.
the initial state declaration always fol-lows a ﬁxed form (“the ﬁrst beaker has [amount][color], the second beaker has [amount] [color],...”).
including it in the context provides enoughinformation that it is (in principle) possible to deter-ministically compute the contents of each beaker af-ter each instruction.
the nlm is trained to predictthe next instruction based on a textual descriptionof the initial state and previous instructions..the state representations we probe for inalchemy describe the contents of each beaker.
be-cause execution is deterministic and the initial state.
truth values for each proposition in each instruc-tion sequence are straightforwardly derived fromground-truth state annotations in the dataset..data: textworld textworld (cˆot´e et al., 2018)is a platform for generating synthetic worlds fortext-based games, used to test rl agents.
the gamegenerator produces rooms containing objects, sur-faces, and containers, which the agent can interactwith in various predeﬁned ways..we turn textworld into a language modelingdataset by generating random game rollouts follow-ing the “simple game” challenge, which samplesworld states with a ﬁxed room layout but chang-ing object conﬁgurations.
for training, we sample4000 rollouts across 79 worlds, and for develop-ment, we sample 500 rollouts across 9 worlds.
con-texts begin with a description of the room that theplayer currently stands in, and all visible objects inthat room.
this is followed by a series of actions(preceded by >) and game responses (fig.
3)..the nlm is trained to generate both an actionand a game response from a history of interactions.
we probe for both the properties of and relationsbetween entities at the end of a sequence of actions.
unlike alchemy, these may be undetermined, asthe agent may not have explored the entire envi-ronment by the end of an action sequence.
(forexample, in fig.
3, the truth value of matches(oldkey, door) is unknown).
the set of propositionsavailable in the textworld domain has form.
φ ={p(o) : o ∈ o, p ∈ p }.
∪ {r(o1, o2) : o1, o2 ∈ o, r ∈ r}.
(4).
5sections here are also discussed in more detail in ap-.
pendix a.1 (for §4.1), a.2 (for §4.2), and a.3 (for §4.3)..for objects o = {player, chest, .
.
.
}, proper-ties p = {open, edible, .
.
.}
and relations r =.
1817main probe (§4.2).
baselines &model ablations(§4.2).
locality (§4.3).
+pretrain, -ﬁne-tune-pretrain, +ﬁne-tunerandom init.
no changeno lm.
ﬁrstlast.
1.50.40.00.0.
--.
alchemy.
textworld.
state em.
entity em.
state em.
entity em.
bart.
t5.
bart.
t5.
bart.
t5.
bart.
t5.
7.6.
1.1.
14.3.
4.3.
75.0.
69.3.
75.5.
74.1.
48.7.
23.2.
53.8.
38.9.
95.2.
91.1.
96.9.
94.3.
62.864.962.732.4.
--.
14.411.39.731.77.
81.274.574.181.8.
49.655.1.
51.558.6.
93.696.5.
95.997.6.table 1: probing results.
for each dataset, we report entity em, the % of entities for which all propositionswere correct, and state em, the % of states for which all proposition were correct.
for non-pretrained baselines(-pretrain, +ﬁne-tune and random init.
), we report the single best result from all model conﬁgurations examined.
semantic state information can be recovered at the entity level from both language models on both datasets, andsuccessful state modeling appears to be primarily attributable to pretraining rather than ﬁne-tuning..{on, in, .
.
.}.
we convert propositions to naturallanguage descriptions as:.
nl(p(o)) = “the p is o”nl(r(o1, o2)) = “the o1 is r o2” ..(5).
the set of propositions and their natural languagedescriptions are pre-deﬁned by textworld’s sim-ulation engine.
the simulation engine also givesus the set of true propositions, from which we cancompute the set of false and unknown propositions..evaluation we evaluate probes according to twometrics.
entity exact-match (em) ﬁrst aggre-gates the propositions by entity or entity pair, thencounts the percentage of entities for which allpropositions were correctly labeled.
state emaggregates propositions by information state (i.e.
context), then counts the percentage of states forwhich all facts were correctly labeled..4.2 representations encode entities’ ﬁnal.
properties and relations.
with this setup in place, we are ready to ask our ﬁrstquestion: is semantic state information encoded atall by pretrained lms ﬁne-tuned on alchemy andtextworld?
we instantiate the probing experimentdeﬁned in §3 as follows:.
the proposition embedder converts eachproposition φ ∈ φ to its natural language descrip-tion, embeds it using the same lm encoder that isbeing probed, then averages the tokens:.
embed(φ) = mean(e(nl(φ))).
(6).
the localizer associates each proposition φ withspeciﬁc tokens corresponding to the entity or en-tities that φ describes, then averages these tokens..in alchemy, we average over tokens in the initialdescription of the beaker in question.
for example,let x be the discourse in figure 3 (left) and φ be aproposition about the ﬁrst beaker.
then, e.g.,.
loc(has-1-red(beaker 1), e(x)) =mean(e(x)[the ﬁrst beaker has 2 green,])..(7).
in textworld, we average over tokens in all men-tions of each entity.
letting x be the discourse infigure 3 (right), we have:.
loc(locked(wooden door), e(x)) =.
mean(e(x)[wooden door]) ..(8).
relations, with two arguments, are localized bytaking the mean of the two mentions..finally, the classiﬁer is a linear model whichmaps each nlm representation and proposition toa truth value.
in alchemy, a linear transformationis applied to the nlm representation, and then theproposition with the maximum dot product withthat vector is labelled t (the rest are labelled f ).
in textworld, a bilinear transformation maps each(proposition embedding, nlm representation) pairto a distribution over {t, f, ?}..
as noted by liang and potts (2015), it is easyto construct examples of semantic judgments thatcannot be expressed as linear functions of purelysyntactic sentence representations.
we expect (andverify with ablation experiments) that this probeis not expressive enough to compute informationstates directly from surface forms, and only expres-sive enough to read out state information alreadycomputed by the underlying lm..1818state em.
entity em.
bart.
t5.
bart.
t5.
remapmain probe.
50.250.2.
50.453.8.
88.991.3.
93.294.6.table 2: locality of information state in textworld(t5).
entity state information tends to be slightly morepresent in mentions of the target entity (main probe)rather than of other entities (remap), but not by much..performs much worse than our probe) also actsas a lexical overlap baseline—there will be lexi-cal overlap between true propositions and the ini-tial state declaration only if the beaker state is un-changed.
in textworld, each action induces mul-tiple updates, but can at most overlap with one ofits affected propositions (e.g.
you close the chestcauses closed(chest) and ¬open(chest), butonly overlaps with the former).
moreover, only∼50% of actions have lexical overlap with anypropositions at all.
thus, lexical overlap cannotfully explain probe performance in either domain.
in summary, pretrained nlm representationsmodel state changes and encode semantic informa-tion about entities’ ﬁnal states..4.3 representations of entities are local to.
entity mentions.
the experiment in §4.2 assumed that entity statecould be recovered from a ﬁxed set of input tokens.
next, we conduct a more detailed investigationinto where state information is localized.
to thisend, we ask two questions: ﬁrst, can we assumestate information is localized in the correspondingentity mentions, and second, if so, which mentionencodes the most information, and what kind ofinformation does it encode?.
4.3.1 mentions or other tokens?
we ﬁrst contrast tokens within mentions of the tar-get entity to tokens elsewhere in the input discourse.
in alchemy, each beaker b’s initial state declara-tion is tokenized as: toksb = {theb, [position]b,beb, akerb, hasb, [volume]b, [color]b, ,b}, where bsigniﬁes the beaker position.
rather than poolingthese tokens together (as in §4.2), we construct alocalizer ablation that associates beaker b’s statewith single tokens t in either the initial mention ofbeaker b, or the initial mention of other beakers atan integer offset ∆.
for each (t, ∆) pair, we con-struct a localizer that matches propositions aboutbeaker b with tb+∆.
for example, the (has, +1)localizer associates the third beaker’s ﬁnal state.
figure 4: locality of information state in alchemy.
wefocus on the initial state declaration.
linear probesare trained to decode the ﬁnal state of a beaker condi-tioned on the individual contextualized representationsfor each word.
separate probes are trained for eachposition.
tokens in the same relative position (with re-spect to the target beaker) are superimposed and the av-eraged entity em is reported in (b) for bart and (t5)for t5.
(a) shows the paradigm on a concrete example..results results are shown in table 1. a probeon t5 can exactly recover 14.3% of informationstates in alchemy, and 53.8% in textworld.
forcontext, we compare to two baselines: a no lmbaseline, which simply predicts the most frequentﬁnal state for each entity, and a no change baseline,which predicts that the entity’s ﬁnal state in thediscourse will be the same as its initial state.
theno lm baseline is correct 0% / 1.8% of the timeand the no change baseline is correct 0% / 9.7% ofthe time—substantially lower than the main probe.
to verify that this predictability is a propertyof the nlm representations rather than the textitself, we apply our probe to a series of modelablations.
first, we evaluate a randomly initial-ized transformer rather than the pretrained andﬁne-tuned model, which has much lower probeaccuracy.
to determine whether the advantageis conferred by lm pretraining or ﬁne-tuning,we ablate either open-domain pretraining, in a-pretrain,+ﬁne-tune ablation, or in-domain ﬁne-tuning, in a +pretrain,-ﬁne-tune ablation.
(for allexperiments not using a pretrained model check-point, we experimented with both a bart-like andt5-like choice of depth and hidden size, and foundthat the bart-like model performed better.)
whileboth ﬁne-tuning and pretraining contribute to theﬁnal probe accuracy, pretraining appears to play amuch larger role: semantic state can be recoveredwell from models with no in-domain ﬁne-tuning..finally, we note that there may be lexical over-lap between the discourse and natural languagedescriptions of propositions.
how much of theprobe’s performance can be attributed to this over-lap?
in alchemy, the no change baseline (which.
1819tthe[pos]beakerhas[amount][color],the[pos]+1[amount][color]39.441.442.141.558.568.674.364.845.135.035.734.542.158.142.440.764.866.767.963.542.141.932.432.9thethirdbeakerhas4blue,thefourth2red… drain 2 from beaker 3.probelocalizerhas-2-blue(beaker3)58.5% / 64.8% accuracyembed(a)(b)(t5)with the vector in e(x) at the position of the “has”token in the fourth beaker has 2 red..in textworld, which does not have such easilycategorizable tokens, we investigate whether infor-mation about the state of an entity is encoded inmentions of different entities.
we sample a randommapping remap between entities, and construct alocalizer ablation in which we decode propositionsabout w from mentions of remap(w).
for example,we probe the value of open(chest) from mentionsof old key.
these experiments use a different eval-uation set—we restrict evaluation to the subset ofentities for which both w and remap(w) appear inthe discourse.
for comparability, we re-run themain probe on this restricted set.6.
results fig.
4 shows the locality of bart andt5 in the alchemy domain.
entity em is highestfor words corresponding to the correct beaker, andspeciﬁcally for color words.
decoding from anytoken of an incorrect beaker barely outperforms theno lm baseline (32.4% entity em).
in textworld,table 2 shows that decoding from a remapped en-tity is only 1-3% worse than decoding from theright one.
thus, the state of an entity e is (roughly)localized to tokens in mentions of e, though thedegree of locality is data- and model-dependent..4.3.2 which mention?.
to investigate facts encoded in different mentionsof the entity in question, we experiment with decod-ing from the ﬁrst and last mentions of the entitiesin x. the form of the localizer is the same as 4.2,except instead of averaging across all mentions ofentities, we use the ﬁrst mention or the last mention.
we also ask whether relational propositions can bedecoded from just one argument (e.g., in(old key,chest) from just mentions of old key, rather thanthe averaged encodings of old key and chest)..results as shown in table 1, in textworld, prob-ing the last mention gives the highest accuracy.
fur-thermore, as table 3 shows, relational facts can bedecoded from either side of the relation..4.4 changes to entity representations causechanges in language model predictions.
the localization experiments in section 4.3 indi-cate that state information is localized within con-.
relations.
properties.
bart.
t5.
bart.
t5.
1-arg2-argrandom init..41.449.6.
55.554.4.
83.294.5.
90.998.5.
21.9.
35.4.table 3: state em for decoding each type of fact (re-lations vs. properties), with each type of probe (1- vs.2- argument decoding).
though decoding from two en-tities is broadly better, one entity still contains a non-trivial amount of information, even regarding relations..textual representations in predictable ways.
thissuggests that modifying the representations them-selves could induce systematic and predictablechanges in model behavior.
we conduct a seriesof causal intervention experiments in the alchemydomain which measure effect of manipulating en-coder representations on nlm output.
we replacea small subset of token representations with thosefrom a different information state, and show thatthis causes the model to behave as if it were in thenew information state.7.
a diagram of the procedure is shown in fig.
5.we create two discourses, x1 and x2, in whichone beaker’s ﬁnal volume is zero.
both discoursesdescribe the same initial state, but for each xi,we append the sentence drain vi from beaker bi,where vi is the initial volume of beaker bi’s con-tents.
though the underlying initial state tokens arethe same, we expect the contextualized representa-tion c1 = e(x1)[the ith beaker .
.
.]
to differ fromc2 = e(x2)[the ith beaker .
.
.]
due to the differentﬁnal states of the beakers.
let cont(x) denotethe set of sentences constituting semantically ac-ceptable continuations of a discourse preﬁx x.
(infig.
1, cont(a, b) contains c1 and c2 but not c3.
)8in alchemy, cont(x1) should not contain mixing,draining, or pouring actions involving b1 (similarlyfor cont(x2) and b2).
decoder samples given cishould fall into cont(xi)..finally, we replace the encoded description ofbeaker 2 in c1 with its encoding from c2, creatinga new representation cmix.
cmix was not derivedfrom any real input text, but implicitly representsa situation in which both b1 and b2 are empty.
adecoder generating from cmix should generate in-structions in cont(x1) ∩ cont(x2) to be consis-tent with this situation..6the remap and ∆ (cid:54)= 0 probes described here are analo-gous to control tasks (hewitt and liang, 2019).
they measureprobes’ abilities to predict labels that are structurally similarbut semantically unrelated to the phenomenon of interest..7this experiment is inspired by geiger et al.
(2020).
8in order to automate evaluation of consistency, we usea version of alchemy with synthetically generated text.
theunderlying lm has also been ﬁne-tuned on synthetic data..1820tions are imperfect: even in the best case, completeinformation states can only be recovered 53.8% ofthe time in tasks that most humans would ﬁnd verysimple.
(additional experiments described in ap-pendix a.5 offer more detail about these errors.)
the success of our probing experiments should notbe taken to indicate that the discovered semanticrepresentations have anything near the expressive-ness needed to support human-like generation.....of our experimental paradigm: while ourprobing experiments in §4.2 provide a detailed pic-ture of structured state representations in nlms,the intervention experiments in §4.4 explain the re-lationship between these state representations andmodel behavior in only a very general sense.
theyleave open the key question of whether errors inlanguage model prediction are attributable to er-rors in the underlying state representation.
finally,the situations we model here are extremely simple,featuring just a handful of objects.
thought experi-ments on the theoretical capabilities of nlms (e.g.
bender and koller’s “coconut catapult”) involvefar richer worlds and more complex interactions.
again, we leave for future work the question ofwhether current models can learn to represent them..6 conclusion.
even when trained only on language data, nlmsencode simple representations of meaning.
in ex-periments on two domains, internal representationsof text produced by two pretrained language mod-els can be mapped, using a linear probe, to repre-sentations of the state of the world described by thetext.
these internal representations are structured,interpretably localized, and editable.
this ﬁndinghas important implications for research aimed atimproving factuality and and coherence in nlms:future work might probe lms for the the states andproperties ascribed to entities the ﬁrst time they arementioned (which may reveal biases learned fromtraining data; bender et al.
2021), or correct errorsin generation by directly editing representations..acknowledgments.
thanks to ekin aky¨urek, evan hernandez, joeo’connor, and the anonymous reviewers for feed-back on early versions of this paper.
mn is sup-ported by a nsf graduate research fellowship.
this work was supported by a hardware donationfrom nvidia under the nvail program..figure 5: intervention experiments.
construct c1, c2by appending text to empty one of the beakers (e.g.
the ﬁrst and second beakers) and encoding the result.
then, create cmix by taking encoded tokens from c1and replacing the encodings corresponding to the sec-ond beaker’s initial state declaration with those fromc2.
this induces the lm to model both the ﬁrst andsecond beakers as empty, and the lm decoder shouldgenerate actions consistent with this state..% of generations within....cont(x1) ∩ cont(x2).
cont(x1).
cont(x2).
bart.
20.416.157.7.c1c2cmix.
t5.
37.929.175.4.bart.
t5.
bart.
t5.
96.224.186.7.
93.037.986.8.
21.687.764.8.
40.887.284.5.table 4: results of intervention experiments.
thoughimperfect, generations from cmix are more often con-sistent with both contexts compared to those from c1or c2, indicating that its underlying information state(approximately) models both beakers as empty..results we generate instructions conditioned oncmix and check whether they are in the expectedsets.
results, shown in table 4, align with this pre-diction.
for both bart and t5, substantially moregenerations from cmix fall within cont(x1) ∩cont(x2) than from c1 or c2.
though imper-fect (compared to c1 generations within cont(x1)and c2 generations within cont(x2)), this sug-gests that the information state associated with thesynthetic encoding cmix is (approximately) one inwhich both beakers are empty..5 limitations.
...of large nlms:it is important to emphasizethat both lm output and implicit state representa-.
1821(g1) mix the first beaker.
(g3) mix the third beaker.
(g2) mix the second beaker.lm decoder(c2)(c1)lm encoderthe first beaker has 2 green, the second beaker has 2 red, the third beaker has 1 green.
drain 2 from first beaker.the first beaker has 2 green, the second beaker has 2 red, the third beaker has 1 green.
drain 2 from second beaker.inconsistentinconsistentconsistent(cmix)lm encoderimpact statement.
this paper investigates the extent to which neu-ral language models build meaning representationsof the world, and introduces a method to probeand modify the underlying information state.
weexpect this can be applied to improve factuality, co-herence, and reduce bias and toxicity in languagemodel generations.
moreover, deeper insight intohow neural language models work and what exactlythey encode can be important when deploying thesemodels in real-world settings..however, interpretability research is by naturedual-use and improve the effectiveness of modelsfor generating false, misleading, or abusive lan-guage.
even when not deliberately tailored togeneration of harmful language, learned seman-tic representations might not accurately representthe world because of errors both in prediction (asdiscussed in §5) and in training data..references.
yonatan belinkov and james glass.
2019. analysismethods in neural language processing: a survey.
transactions of the association for computationallinguistics, 7:49–72..emily m bender, timnit gebru, angelina mcmillan-major, and shmargaret shmitchell.
2021. on thedangers of stochastic parrots: can language modelsbe too big?
in proceedings of the 2021 acm confer-ence on fairness, accountability, and transparency,pages 610–623..emily m. bender and alexander koller.
2020. climb-ing towards nlu: on meaning, form, and under-in proceedings of thestanding in the age of data.
58th annual meeting of the association for compu-tational linguistics, pages 5185–5198, online.
as-sociation for computational linguistics..kevin clark, urvashi khandelwal, omer levy, andchristopher d. manning.
2019. what does bertin pro-look at?
an analysis of bert’s attention.
ceedings of the 2019 acl workshop blackboxnlp:analyzing and interpreting neural networks fornlp, pages 276–286, florence, italy.
associationfor computational linguistics..marc-alexandre cˆot´e, ´akos k´ad´ar, xingdi yuan, benkybartas, tavian barnes, emery fine, james moore,ruo yu tao, matthew hausknecht, layla el asri,mahmoud adada, wendy tay, and adam trischler.
2018. textworld: a learning environment for text-based games.
corr, abs/1806.11532..jacob devlin, ming-wei chang, kenton lee, andkristina toutanova.
2019. bert: pre-training of.
deep bidirectional transformers for language under-in proceedings of the 2019 conferencestanding.
of the north american chapter of the associationfor computational linguistics: human languagetechnologies, volume 1 (long and short papers),pages 4171–4186, minneapolis, minnesota.
associ-ation for computational linguistics..arash einolghozati, panupong pasupat, sonal gupta,rushin shah, mrinal mohit, mike lewis, and lukezettlemoyer.
2019. improving semantic parsing fortask oriented dialog..atticus geiger, kyle richardson, and christopherpotts.
2020. neural natural language inference mod-els partially embed theories of lexical entailment andnegation.
in proceedings of the third blackboxnlpworkshop on analyzing and interpreting neural net-works for nlp, pages 163–173, online.
associationfor computational linguistics..jeroen groenendijk and martin stokhof.
1991. dy-namic predicate logic.
linguistics and philosophy,14:39–100..irene heim.
2008. file change semantics and the fa-miliarity theory of deﬁniteness, pages 223 – 248..john hewitt and percy liang.
2019. designing andinterpreting probes with control tasks.
in proceed-ings of the 2019 conference on empirical methodsin natural language processing and the 9th inter-national joint conference on natural language pro-cessing (emnlp-ijcnlp), pages 2733–2743, hongkong, china.
association for computational lin-guistics..john hewitt and christopher d. manning.
2019. astructural probe for ﬁnding syntax in word repre-sentations.
in proceedings of the 2019 conferenceof the north american chapter of the associationfor computational linguistics: human languagetechnologies, volume 1 (long and short papers),pages 4129–4138, minneapolis, minnesota.
associ-ation for computational linguistics..felix hill, sona mokra, nathaniel wong, and timharley.
2020. human instruction-following withdeep reinforcement learning via transfer-learningfrom text..gabriel ilharco, rowan zellers, ali farhadi, and han-naneh hajishirzi.
2021.probing contextual lan-guage models for common ground with visual rep-in proceedings of the 2021 confer-resentations.
ence of the north american chapter of the associ-ation for computational linguistics: human lan-guage technologies, pages 5367–5377, online.
as-sociation for computational linguistics..hans kamp, josef genabith, and uwe reyle.
2010.discourse representation theory, pages 125–394..olga kovaleva, alexey romanov, anna rogers, andanna rumshisky.
2019. revealing the dark secretsof bert.
in proceedings of the 2019 conference on.
1822zhou, wei li, and peter j. liu.
2020. exploringthe limits of transfer learning with a uniﬁed text-to-text transformer.
journal of machine learning re-search, 21(140):1–67..adam roberts, colin raffel, and noam shazeer.
2020.how much knowledge can you pack into the param-in proceedings of theeters of a language model?
2020 conference on empirical methods in naturallanguage processing (emnlp), pages 5418–5426,online.
association for computational linguistics..xing shi, inkit padhi, and kevin knight.
2016. doesstring-based neural mt learn source syntax?
in pro-ceedings of the 2016 conference on empirical meth-ods in natural language processing, pages 1526–1534, austin, texas.
association for computationallinguistics..ian tenney, dipanjan das, and ellie pavlick.
2019.inbert rediscovers the classical nlp pipeline.
proceedings of the 57th annual meeting of the asso-ciation for computational linguistics, pages 4593–4601, florence, italy.
association for computationallinguistics..gregor wiedemann, steffen remus, avi chawla, andchris biemann.
2019. does bert make any sense?
interpretable word sense disambiguation with con-textualized embeddings..zhaofeng wu, hao peng, and noah a. smith.
2021.infusing finetuning with semantic dependencies.
transactions of the association for computationallinguistics, 9:226–242..seth yalcin.
2014. introductory notes on dynamic se-.
mantics..empirical methods in natural language processingand the 9th international joint conference on natu-ral language processing (emnlp-ijcnlp), pages4365–4374, hong kong, china.
association forcomputational linguistics..mike lewis, yinhan liu, naman goyal, mar-jan ghazvininejad, abdelrahman mohamed, omerlevy, veselin stoyanov, and luke zettlemoyer.
2020. bart: denoising sequence-to-sequence pre-training for natural language generation, translation,and comprehension.
in proceedings of the 58th an-nual meeting of the association for computationallinguistics, pages 7871–7880, online.
associationfor computational linguistics..percy liang and christopher potts.
2015. bringing ma-chine learning and compositional semantics together.
annu.
rev.
linguist., 1(1):355–376..reginald long, panupong pasupat, and percy liang.
2016. simpler context-dependent logical forms viain proceedings of the 54th an-model projections.
nual meeting of the association for computationallinguistics (volume 1: long papers), pages 1456–1465, berlin, germany.
association for computa-tional linguistics..tomas mikolov, wen-tau yih, and geoffrey zweig.
2013. linguistic regularities in continuous spacein proceedings of the 2013word representations.
conference of the north american chapter of theassociation for computational linguistics: humanlanguage technologies, pages 746–751, atlanta,georgia.
association for computational linguistics..matthew peters, mark neumann, mohit iyyer, mattgardner, christopher clark, kenton lee, and lukezettlemoyer.
2018. deep contextualized word rep-in proceedings of the 2018 confer-resentations.
ence of the north american chapter of the associ-ation for computational linguistics: human lan-guage technologies, volume 1 (long papers), pages2227–2237, new orleans, louisiana.
associationfor computational linguistics..fabio petroni, tim rockt¨aschel, sebastian riedel,patrick lewis, anton bakhtin, yuxiang wu, andalexander miller.
2019. language models as knowl-in proceedings of the 2019 confer-edge bases?
ence on empirical methods in natural languageprocessing and the 9th international joint confer-ence on natural language processing (emnlp-ijcnlp), pages 2463–2473, hong kong, china.
as-sociation for computational linguistics..tiago pimentel, naomi saphra, adina williams, andryan cotterell.
2020. pareto probing: trading offaccuracy for complexity.
in proceedings of the 2020conference on empirical methods in natural lan-guage processing (emnlp), pages 3138–3153, on-line.
association for computational linguistics..colin raffel, noam shazeer, adam roberts, kather-ine lee, sharan narang, michael matena, yanqi.
1823a appendix.
a.1 datasets details (§4.1).
alchemy alchemy is downloadable at https://nlp.stanford.edu/projects/scone/.
alchemy propo-sitions are straightforwardly derived from existinglabels in the dataset.
we preserve the train/devsplit from the original dataset (3657 train/245 dev),which we use for training the underlying lm andthe probe.
in subsequent sections, we include ad-ditional results from a synthetic dataset that wegenerated (3600 train/500 dev), where actions arecreated following a ﬁxed template, making it easyto evaluate consistency..textworld we generate a set of worlds for train-ing, and a separate set of worlds for testing.
weobtain transcripts from three agents playing oneach game: a perfect agent and two (semi-)randomagents, which intersperse perfect actions with sev-eral steps of random actions.
for training, wesample 4000 sequences from the 3 agents across79 worlds.
for development, we sample 500 se-quences from the 3 agents across 9 worlds..during game generation, we are given the setof all propositions that are true in the world, andhow the set updates after each player action.
how-ever, the player cannot infer the full state beforeinteracting with and seeing all objects, and neither(we suspect) can a language model trained on par-tial transcripts.
for example, a player that starts inthe bedroom cannot infer is-in(refrigerator,kitchen) without ﬁrst entering the kitchen.
onesolution would be to hard-code rules–a player canonly know about the states of entities it has directlyaffected or seen.
however, since synthetically-generated worlds might share some commonalities,a player that has played many games before (or anlm trained on many transcripts) might be able todraw particular conclusions about entities in unseenworlds, even before interacting with them..to deal with these factors, we train a labellermodel label to help us classify propositions asknown true, known false, and unknown.
we gener-ate a training set (separate from the training set forthe probe and probed lm) to train the labeller.
weagain use bart, but we give it the text transcriptsand train it to directly decode the full set of truepropositions and false proposition by the end ofthe transcript (recall we have the ground-truth fulltrue set, and we label all propositions that aren’tin the true set as false).
this allows the labeller.
model to pick up both patterns between the dis-course and its information state, as well as infergeneral patterns among various discourses.
thus,on unknown worlds, given text t , if proposition ais true most or all of the time given t , the modelshould be conﬁdent in predicting a. we label a astrue in these cases.
however, if proposition a istrue only half of the time given t , the model is un-conﬁdent.
we label a as unknown in these cases.
thus, we create our unknown set using a conﬁdencethreshold τ on label’s output probability..a.2 probe details + additional results (§4.2).
below, we give a more detailed account of our prob-ing paradigm in each domain, including equations..alchemy probe the proposition embedder con-verts propositions φ to natural language descrip-tions (cid:101)φ (“the bth beaker has v c”) and encodesthem with the bart or t5 lm encoder..given a proposition has-v-c(b), the localizerloc maps has-v-c(b) to tokens in e(x) that cor-responding to the initial state of beaker b. since xalways begins with an initial state declaration ofthe form “the ﬁrst beaker has [amount] [color], thesecond beaker has [amount] [color], ...”, tokens atposition 8b − 8 · · · 8b − 1 of x correspond to theinitial state of beaker b.
(each state has 8 tokens:‘the’, ‘bth’, ‘be’, ‘aker’, ‘has’, ‘[amount]’, ‘[color]’,‘,’).
thus,.
loc(has-v-c(b), e(x)) =.
mean(e(x)[8b − 8], · · · , e(x)[8b − 1]).
we train a linear probe clsθ to predict the ﬁnalbeaker state given the encoded representation e(x)of text x. for our probe, we learn linear projec-tion weights w (d×d) and bias b(d) to maximize thedot product between the lm representation andthe embedded proposition.
formally, it computesv(max)b., c(max)b.as.
v(max)b., c(max)b.
= arg maxv(cid:48),c(cid:48).
(cid:0)embed(has-v(cid:48)-c(cid:48)(b))·.
(w · loc(has-v(cid:48)-c(cid:48)(b), e(x)) + b)(cid:1).
(9).
, c(max)in other words, v(max)are the values of vband c that maximize this dot product.
the probethen returns.
b.clsθ(embed(has-v-c(b)), loc(has-v-c(b), e(x))).
(cid:40).
=.
t if v, c = v(max)f if v, c (cid:54)= v(max).
b.b., c(max)b, c(max)b.
(10).
1824figure 6: alchemy locality - full results.
top: t5, finetuned+probed on real data.
middle: bart, fine-tuned+probed on real data.
bottom: bart, finetuned+probed on synthetic data.
we note that for the syntheticdata, accurate decoding is possible from a much wider set of tokens, but all still correspond to the relevant beaker..note that clsθ selects the optimal ﬁnal state perbeaker, from the set of all possible states of beakerb, taking advantage of the fact that only one propo-sition can be true per beaker..a.3 localization experiment details +.
additional results (§4.3).
below we provide a speciﬁc, formulaic account ofeach of our localizer experiments..textworld probe for textworld, the propositionembedder converts propositions φ to natural lan-guage descriptions (cid:101)φ (“the o is p” for propertiesand “the o1 is r o2” for relations) and encodesthem with the bart or t5 lm encoder..given a proposition p(o) pertaining to an entityor r(o1, o2) pertaining to an entity pair, we deﬁnelocalizer loc to map the proposition to tokens ofe(x) corresponding to all mentions of its argu-ments, and averages across those tokens:.
all idx(e) = set of indices of x correspond.
-ing to all instances of e.loc(r(o1, o2), e(x)) = mean(cid:0)e(x)[all idx(o1)∪all idx(o2)](cid:1)loc(p(o), e(x)) = mean (e(x)[all idx(o)]).
(11).
we train a bilinear probe clsθ that classiﬁeseach (proposition embedding, lm representation)pair to {t , f , ?}.
the probe has parametersw (3×d×d), b(3) and performs the following bilinearoperation:.
scr(φ, e(x)) = embed(φ)t · w · loc(φ, e(x)) + b.mentions vs. other tokens (§4.3.1) – alchemyrecall that we train a probe for each (t, ∆) pairto extract propositions about b from token tb+∆ ∈toksb+∆, where ∆ is the beaker offset.
speciﬁ-cally, the localizer for this probe takes form.
off :.
{‘the’ → 0, [position] → 1, ‘be’ → 2, ‘aker’ → 3,.
‘has’ → 4, [amount] → 5, [color] → 6, ‘,’ → 7}.
loc(t,∆)(has-v-c(b, e(x))) =.
mean(e(x)[8(b + ∆) − 8 + off(t)]).
the full token-wise results for beaker states ina 3-beaker (24-token) window around the targetbeaker is shown in figure 6 (top/middle)..additional localizer ablations results for a bartprobe trained and evaluated on synthetic alchemydata are shown in figures 6 (bottom).
similar tothe non-synthetic experiments, we point the local-izer to just a single token of the initial state.
inter-estingly, bart’s distribution looks very differentin the synthetic setting.
though state informationis still local to the initial state description of thetarget beaker, it is far more distributed within thedescription–concentrated in not just the amountand color tokens, but also the mention tokens..(12).
mentions vs. other tokens (§4.3.1) – textworldthe speciﬁc localizer for this experiment has form.
where scr is a vector of size 3, with one score pert , f , ?
label.
the probe then takes the highest-scoring label.
clsθ(embed(φ), loc(φ, e(x))) =.
t if scr(φ, e(x))[0] > scr(φ, e(x))[1], scr(φ, e(x))[2]f if scr(φ, e(x))[1] > scr(φ, e(x))[2], scr(φ, e(x))[0]?
if scr(φ, e(x))[2] > scr(φ, e(x))[0], scr(φ, e(x))[1](13).
.
loc(r(o1, o2), e(x)) =.
mean(e(x)[all idx(remap(o1))∪all idx(remap(o2))])loc(p(o), e(x)) = mean(e(x)[all idx(remap(o))]).
note the evaluation set for this experiment isslightly different as we exclude contexts which donot mention remap(w)..1825main probe (§4.2).
human-grounded-features (§a.4)synthetic data.
alchemy.
entity em state em.
75.0.
45.788.2.
7.55.
0.7135.9.table 5: additional alchemy results.
we compare ourfull encoded-nl embedder with a featurized embedder(§a.4).
we also report results on synthetic data....which mention?
(§4.3.2) – ﬁrst/last the local-izer for this experiment is constructed by replac-ing all instances of all idx in eq.
10 with eitherfirst idx or last idx, deﬁned as:.
first idx(e) = set of indices of x correspond-.
ing to ﬁrst instance of e.last idx(e) = set of indices of x correspond-.
ing to last instance of e.which mention?
(§4.3.2) – single- vs. both-entity probe.
the speciﬁc localizer for thesingle-entity probe has form.
loc(r(o1, o2), e(x)) =.
(cid:26)mean(e(x)[all idx(o1)]),mean(e(x)[all idx(o2)]).
(cid:27).
loc(p(o), e(x)) = mean(e(x)[all idx(o)]).
note the localizer returns a 2-element set of en-codings from each relation.
we train the probe todecode r(o1, o2) from both elements of this set..the full results are in table 3. as shown, theboth-mentions probe is slightly better at both de-coding relations and properties.
however, this maysimply be due to having less candidate proposi-tions per entity pair, than per entity (which in-cludes relations from every other entity pairedwith this entity).
for example, entity pair (ap-ple, chest) has only three possibilities: in(apple,chest) is true/unknown/false, while singular en-tity (chest) has much more: in(apple, chest),in(key, chest), open(chest), etc.
can eachbe true/unknown/false.
a full set of results bro-ken down by property/relation can be found in ta-ble 6. overall, the single-entity probe outperformsall baselines, suggesting that each entity encodingcontains information about its relation with otherentities..a.4 proposition embedder ablations.
we experiment with a featurized embed functionin the alchemy domain.
recall from section 4.2and a.2 that our main probe uses encoded natural-language assertions of the state of each beaker.
(eq.
6).
we experiment with featurized vectorwhere each beaker proposition is the concatena-tion of a 1-hot vector for beaker position anda sparse vector encoding the amount of eachcolor in the beaker (with 1 position per color).
for example, if there are 2 beakers and 3 col-ors [green,red,brown], has-3-red(2) is repre-sented as [0, 1, 0, 3, 0].
a multi-layer perceptron isused as the embed function to map this featurizedrepresentation into a dense vector, which is used inthe probe as described by eq.
10. in this setting,the embed mlp is optimized jointly with the probe.
results are shown in table 5. using a featur-ized representation (45.7) is signiﬁcantly worsethan using an encoded natural-language represen-tation (75.0), suggesting that the form of the factembedding function is important.
in particular, theencoding is linear in sentence-embedding space,but nonlinear in human-grounded-feature space..a.5 error analysis.
we run error analysis on the bart model.
forthe analysis below, it is important to note that wemake no distinction between probe errors and rep-resentation errors—we do not know whether theerrors are attributable to the linear probe’s lack ofexpressive power, or whether the underlying lmindeed does fail to capture certain phenomena.
wenote that a bart decoder trained to decode theﬁnal information state from e(x) is able to achieve53.5% state em on alchemy (compared to 0% onrandom initialization baseline) whereas the lineardecoder was only able to achieve 7.55% state em—suggesting that certain state information may benon-linearly encoded in nlms..alchemy the average number ofincorrectbeakers per sample is 25.0% (2.7 beakers out of 7).
we note that the distribution is skewed towardslonger sequences of actions, where the % of wrongbeakers increases from 11.3% (at 1 action) to 24.7%(2 actions), 30.4% (3 actions), 33.4% (at 4 actions).
for beakers not acted upon (ﬁnal state unchanged),the error rate is 13.3%.
for beakers acted upon(ﬁnal state changed), the error rate is 44.6%.
thus,errors are largely attributed to failures in reasoningabout the effects of actions, rather than failures indecoding the initial state.
(this is unsurprising, asin alchemy, the initial state is explicitly written inthe text—and we’re decoding from those tokens).
for beakers that were predicted wrong, 36.8%were predicted to be its unchanged initial state and.
1826overall relations.
properties.
true facts.
false facts.
em.
48.7.
23.214.411.31.779.73.
49.655.1.em.
49.6.
32.726.821.924.830.1.
50.956.6.em.
94.5.
75.444.035.433.49.73.
88.596.5.pre.
95.1.
93.287.391.588.377.9.
95.496.1.rec.
98.1.
94.986.683.880.973.0.
97.298.7.f1.
96.4.
93.886.387.284.475.3.
96.197.3.pre.
99.6.
97.093.391.888.879.1.
99.199.7.rec.
98.9.
96.193.084.186.961.8.
98.798.9.f1.
99.2.
96.492.986.587.668.9.
98.999.3.
+pretrain, -ﬁnetune-pretrain, +ﬁnetunerandomly initializedno lmno change.
locality (ﬁrst)locality (last).
table 6: textworld probing.
metrics are reported on whole state.
precision is computed against all gold, ground-truth true facts in the state.
recall is computed against the label-model-generated true facts in the state.
allnumbers reported are averaged across all samples.
relations are overall much harder to probe than properties..proposition type error rate.
{north|south|east|west}-of(a,b)is-on(a,b)is-in(a,b)locked(a)closed(a)eaten(a)open(a).
11.8%.
6.17%1.20%0.47%0.35%0.049%0.039%.
table 7: error rate per proposition type in textworld..> go eastyou enter the kitchen..where the ellipses possibly encompass a long se-quence of other actions..a.6.
infrastructure and reproducibility.
we run all experiments on a single 32gb nvidiatesla v100 gpu.
on both alchemy and textworld,we train the language models to convergence, thentrain the probe for 20 epochs.
in alchemy andtextworld, both training the language model andthe probe takes approximately a few (less than 5)hours.
we probe bart-base, a 12-layer encoder-decoder transformer model with 139m parameters,and t5-base, a 24-layer encoder-decoder trans-former model which has 220m parameters.
ourprobe itself is a linear model, with only two param-eters (weights and bias)..the remaining 63.2% were predicted to be empty— thus, probe mistakes are largely attributable toa tendency to over-predict empty beakers.
thissuggests that the downstream decoder may tendto generate actions too conservatively (as emptybeakers cannot be acted upon).
correcting thiscould encourage lm generation diversity..finally, we examine what type of action tends tothrow off the probe.
when there is a pouring or mix-ing-type action present in the sequence, the modeltends to do worse (25.3% error rate for drain-typevs. 31.4 and 33.3% for pour- and mix-type), thoughthis is partially due to the higher concentration ofdrain actions in short action sequences..textworld textworld results, broken down byproperties/relations, are reported in table 6. theprobe seems to be especially bad at classifyingrelations, which make sense as relations are of-ten expressed indirectly.
a breakdown of errorrate for each proposition type is shown in table 7,where we report what % of that type of propo-sition was labelled incorrectly, each time it ap-peared.
this table suggests that the probe consis-tently fails at decoding locational relations, i.e.
fail-ing to classify east-of(kitchen,bedroom) andwest-of(kitchen,bedroom) as true, despite thelayout of the simple domain being ﬁxed.
onehypothesis is that location information is mademuch less explicit in the text, and usually re-quire reasoning across longer contexts and ac-tion sequences.
for example, classifying in(key,drawer) as true simply requires looking at a singleaction: > put key in drawer.
however, classifyingeast-of(kitchen,bedroom) as true requires rea-soning across the following context:.
you are in the bedroom [.
.
.
].
1827